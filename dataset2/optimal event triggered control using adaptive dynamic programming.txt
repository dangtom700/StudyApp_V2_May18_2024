Optimal Event-Triggered Control
Using Adaptive Dynamic
Programming
Sarangapani Jagannathan, Vignesh Narayanan, and Avimanyu SahooSystem Modeling and Control with Resource-Oriented Petri Nets
MengChu Zhou, Naiqi Wu
Deterministic Learning Theory for Identification, Recognition, and Control 
Cong Wang, David J. Hill
Optimal and Robust Scheduling for Networked Control Systems 
Stefano Longo, Tingli Su, Guido Herrmann, Phil Barber
Electric and Plug-in Hybrid Vehicle Networks
Optimization and Control
Emanuele Crisostomi, Robert Shorten, Sonja Stüdli, Fabian Wirth
Adaptive and Fault-Tolerant Control of Underactuated Nonlinear Systems
Jiangshuai Huang, Yong-Duan Song
Discrete-Time Recurrent Neural Control
Analysis and Application
Edgar N. Sánchez
Control of Nonlinear Systems via PI, PD and PID 
Stability and Performance
Yong-Duan Song
Multi-Agent Systems
Platoon Control and Non-Fragile Quantized Consensus
Xiang-Gui Guo, Jian-Liang Wang, Fang Liao, Rodney Swee Huat Teo
Classical Feedback Control with Nonlinear Multi-Loop Systems
With MATLAB® and Simulink®, Third Edition
Boris J. Lurie, Paul Enright
Motion Control of Functionally Related Systems
Tarik Uzunović, Asif Sabanović
Intelligent Fault Diagnosis and Accommodation Control
Sunan Huang, Kok Kiong Tan, Poi Voon Er, Tong Heng Lee
Nonlinear Pinning Control of Complex Dynamical Networks
Edgar N. Sanchez, Carlos J. Vega, Oscar J. Suarez, Guanrong Chen
Adaptive Control of Dynamic Systems with Uncertainty and Quantization
Jing Zhou, Lantao Xing, Changyun Wen 
Robust Formation Control for Multiple Unmanned Aerial Vehicles
Hao Liu, Deyuan Liu, Yan Wan, Frank L. Lewis, Kimon P. Valavanis
Variable Gain Control and Its Applications in Energy Conversion
Chenghui Zhang, Le Chang, Cheng Fu
For more information about this series, please visit: https://www.crcpress.com/Automation-and￾Control-Engineering/book-series/CRCAUTCONENGDedication
The first author would like to dedicate the book to his mother
Janaki and spouse Sandhya
The second author would like to dedicate the book to
KA Kamakshi, R Jayalakshmi, VEV Ramani, KA Gopal, T Vijayalakshmi
Krishnan, K Jagadisan, and R Narayanan
The third author would like to dedicate the book to his mother Kumudini, spouse
Praveena, and daughter AnaayaMATLAB® is a trademark of The MathWorks, Inc. and is used with permission. The MathWorks does 
not warrant the accuracy of the text or exercises in this book. This book’s use or discussion of 
MATLAB® software or related products does not constitute endorsement or sponsorship by The 
MathWorks of a particular pedagogical approach or particular use of the MATLAB® software. 
First edition published 2024
by CRC Press
2385 NW Executive Center Drive, Suite 320, Boca Raton FL 33431
and by CRC Press
4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN
CRC Press is an imprint of Taylor & Francis Group, LLC
© 2024 Sarangapani Jagannathan, Vignesh Narayanan, and Avimanyu Sahoo
Reasonable efforts have been made to publish reliable data and information, but the author and pub￾lisher cannot assume responsibility for the validity of all materials or the consequences of their use. 
The authors and publishers have attempted to trace the copyright holders of all material 
reproduced in this publication and apologize to copyright holders if permission to publish in this 
form has not been obtained. If any copyright material has not been acknowledged please write and 
let us know so we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, 
transmitted, or utilized in any form by any electronic, mechanical, or other means, now known or 
hereafter invented, including photocopying, microfilming, and recording, or in any information stor￾age or retrieval system, without written permission from the publishers.
For permission to photocopy or use material electronically from this work, access www.copyright.com 
or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 01923, 
978-750-8400. For works that are not available on CCC please contact mpkbookspermissions@tandf.co.uk
Trademark notice: Product or corporate names may be trademarks or registered trademarks and 
are used only for identification and explanation without intent to infringe.
ISBN: 978-1-032-46865-5 (hbk)
ISBN: 978-1-032-79150-0 (pbk) 
ISBN: 978-1-003-49075-3 (ebk)
DOI: 10.1201/9781003
Publisher's note: This book has been prepared from camera-ready copy provided 
by the authors.
Access the Instructor and Student Resources/Support Material: 
www.routledge.com/9781032468655$XWKRU%LRV
%S4BSBOHBQBOJ+BHBOOBUIBOJTB$VSBUPST%JTUJOHVJTIFE1SPGFTTPSBOE3VUMFEHF&NFSTPODIBJSPG&MFDUSJDBM
BOE$PNQVUFS&OHJOFFSJOHBUUIF.JTTPVSJ6OJWFSTJUZPG4DJFODFBOE5FDIOPMPHZ	GPSNFS6OJWFSTJUZPG
.JTTPVSJ3PMMB
)FIBTBKPJOU1SPGFTTPSBQQPJOUNFOUJOUIF%FQBSUNFOUPG$PNQVUFS4DJFODF)FTFSWFEBT
B%JSFDUPSGPSUIF/4'*OEVTUSZ6OJWFSTJUZ$PPQFSBUJWF3FTFBSDI$FOUFSPO*OUFMMJHFOU.BJOUFOBODF4ZTUFNT
GPSZFBST)JTSFTFBSDIJOUFSFTUTJODMVEFMFBSOJOHBEBQUBUJPOBOEDPOUSPMTFDVSFIVNBODZCFSQIZTJDBM
TZTUFNTQSPHOPTUJDTBOEBVUPOPNPVTTZTUFNTSPCPUJDT1SJPSUPIJT.JTTPVSJ45BQQPJOUNFOUIFTFSWFE
BTBGBDVMUZBU6OJWFSTJUZPG5FYBTBU4BO"OUPOJPBOEBTBTUBGGFOHJOFFSBU$BUFSQJMMBS1FPSJB
)FIBTDPBVUIPSFEPWFSSFGFSFFE*&&&5SBOTBDUJPOKPVSOBMBOEDPOGFSFODFBSUJDMFTXSJUUFOCPPL
DIBQUFSTBVUIPSFEDPFEJUFECPPLTSFDFJWFE64QBUFOUTBOEPOFQBUFOUEFGFOTFQVCMJDBUJPO)F
EFMJWFSFEBSPVOEQMFOBSZBOELFZOPUFUBMLTJOWBSJPVTJOUFSOBUJPOBMDPOGFSFODFTBOETVQFSWJTFEUP
HSBEVBUJPOEPDUPSBMBOE.4UIFTJTTUVEFOUT)FXBTBDPFEJUPSGPSUIF*&5CPPLTFSJFTPODPOUSPMGSPN
VOUJMBOETFSWFEPONBOZFEJUPSJBMCPBSETJODMVEJOH*&&&4ZTUFNT.BOBOE$ZCFSOFUJDTBOEIBT
CFFOPOPSHBOJ[JOHDPNNJUUFFTPGTFWFSBM*&&&$POGFSFODFT)FJTDVSSFOUMZBOBTTPDJBUFFEJUPSGPS*&&&
5SBOTBDUJPOTPO/FVSBM/FUXPSLTBOE-FBSOJOH4ZTUFNTBOEPUIFST
)FSFDFJWFENBOZBXBSETJODMVEJOHUIF#FTU"TTPDJBUF&EJUPS"XBSE*&&&$445SBOTJUJPOUP
1SBDUJDF"XBSE#PFJOH1SJEF"DIJFWFNFOU"XBSE$BUFSQJMMBS3FTFBSDI&YDFMMFODF"XBSE
6OJWFSTJUZPG.JTTPVSJ1SFTJEFOUJBM"XBSEGPSTVTUBJOFEDBSFFSFYDFMMFODF6OJWFSTJUZPG5FYBT
1SFTJEFOUJBM"XBSEGPSFBSMZDBSFFSFYDFMMFODFBOE/4'$BSFFS"XBSE)FBMTPSFDFJWFETFWFSBMGBDVMUZ
FYDFMMFODFBOEUFBDIJOHFYDFMMFODFBOEDPNNFOEBUJPOBXBSET"TQBSUPGIJT/4'*6$3$IFUSBOTJUJPOFE
NBOZUFDIOPMPHJFTBOETPGUXBSFQSPEVDUTUPJOEVTUSJBMFOUJUJFTTBWJOHNJMMJPOTPGEPMMBST)FJTB'FMMPXPGUIF
*&&&/BUJPOBM"DBEFNZPG*OWFOUPSTBOE*OTUJUVUFPG.FBTVSFNFOUBOE$POUSPM6,*OTUJUVUJPOPG
&OHJOFFSJOHBOE5FDIOPMPHZ	*&5
6,BOE"TJB1BDJGJD"SUJGJDJBM*OUFMMJHFODF"TTPDJBUJPO
%S7JHOFTI/BSBZBOBOJTBO"TTJTUBOU1SPGFTTPSJOUIF"*JOTUJUVUFBOEUIF%FQBSUNFOUPG$PNQVUFS4DJFODF
BOE&OHJOFFSJOHBU6OJWFSTJUZPG4PVUI$BSPMJOB	64$
$PMVNCJB)FJTBMTPBGGJMJBUFEXJUIUIF$BSPMJOB
"VUJTNBOE/FVSPEFWFMPQNFOUSFTFBSDIDFOUFSBU64$)JTSFTFBSDIJOUFSFTUTJODMVEFEZOBNJDBMTZTUFNTBOE
OFUXPSLTBSUJGJDJBMJOUFMMJHFODFEBUBTDJFODFMFBSOJOHUIFPSZBOEDPNQVUBUJPOBMOFVSPTDJFODF
)FSFDFJWFEIJT#5FDI&MFDUSJDBMBOEFMFDUSPOJDTFOHJOFFSJOHBOE.5FDI&MFDUSJDBMFOHJOFFSJOHEFHSFFT
GSPN4"453"6OJWFSTJUZ5IBOKBWVSBOEUIF/BUJPOBM*OTUJUVUFPG5FDIOPMPHZ,VSVLTIFUSB*OEJB
SFTQFDUJWFMZJOBOEBOEIJT1I%EFHSFFGSPN.JTTPVSJ6OJWFSTJUZPG4DJFODFBOE5FDIOPMPHZ
3PMMB.0JO)FXBTBQPTUEPDUPSBMSFTFBSDIBTTPDJBUFBU8BTIJOHUPO6OJWFSTJUZJO4U-PVJTCFGPSF
KPJOJOHUIF"*JOTUJUVUFPG64$
"WJNBOZV4BIPPSFDFJWFEIJT1I%JO&MFDUSJDBM&OHJOFFSJOHGSPN.JTTPVSJ6OJWFSTJUZPG4DJFODFBOE
5FDIOPMPHZ3PMMB.064"JOBOEB.BTUFSPG5FDIOPMPHZ	.5FDI
GSPNUIF*OEJBO*OTUJUVUFPG
5FDIOPMPHZ	#)6
7BSBOBTJ*OEJBJO)FJTDVSSFOUMZBO"TTJTUBOU1SPGFTTPSJOUIF&MFDUSJDBMBOE
$PNQVUFS&OHJOFFSJOH%FQBSUNFOUBUUIF6OJWFSTJUZPG"MBCBNBJO)VOUTWJMMF	6")
"-#FGPSFKPJOJOH
6")%S4BIPPXBTBO"TTPDJBUF1SPGFTTPSJOUIF%JWJTJPOPG&OHJOFFSJOH5FDIOPMPHZBU0LMBIPNB4UBUF
6OJWFSTJUZ4UJMMXBUFS0,
%S4BIPPTSFTFBSDIJOUFSFTUTJODMVEFMFBSOJOHCBTFEDPOUSPMBOEJUTBQQMJDBUJPOTJOMJUIJVNJPOCBUUFSZQBDL
NPEFMJOHEJBHOPTUJDTQSPHOPTUJDTDZCFSQIZTJDBMTZTUFNT	$14
BOEFMFDUSJDNBDIJOFSZIFBMUINPOJUPSJOH
$VSSFOUMZIJTSFTFBSDIGPDVTFTPOEFWFMPQJOHJOUFMMJHFOUCBUUFSZNBOBHFNFOUTZTUFNT	#.4
GPSMJUIJVNJPO
CBUUFSZQBDLTVTFEPOCPBSEFMFDUSJDWFIJDMFTDPNQVUBUJPOBOEDPNNVOJDBUJPOFGGJDJFOUEJTUSJCVUFE
JOUFMMJHFOUDPOUSPMTDIFNFTGPSDZCFSQIZTJDBMTZTUFNTVTJOHBQQSPYJNBUFEZOBNJDQSPHSBNNJOH
SFJOGPSDFNFOUMFBSOJOHBOEEJTUSJCVUFEBEBQUJWFTUBUFFTUJNBUJPO)FIBTQVCMJTIFEPWFSKPVSOBMBOE
DPOGFSFODFBSUJDMFTJODMVEJOH*&&&5SBOTBDUJPOTPO/FVSBM/FUXPSLTBOE-FBSOJOH4ZTUFNT$ZCFSOFUJDTBOE
*OEVTUSJBM&MFDUSPOJDT)FJTBMTPBO"TTPDJBUF&EJUPSJO*&&&5SBOTBDUJPOTPO/FVSBM/FUXPSLTBOE-FBSOJOH
4ZTUFNTBOE'SPOUJFSTJO$POUSPM&OHJOFFSJOH/POMJOFBS$POUSPMContents
Chapter 1 Background and Introduction to Event-triggered Control....................................... 1
Chapter 2 Adaptive Dynamic Programming and Optimal Control........................................ 47
Chapter 3 Linear Discrete-time and Networked Control Systems......................................... 93
Chapter 4 Nonlinear Continuous-time Systems................................................................... 129
Chapter 5 Co-optimization of Event-triggered Sampling and Control ................................ 161
Chapter 6 Linear Interconnected Systems............................................................................ 199
Chapter 7 Nonlinear Interconnected Systems ...................................................................... 221
Chapter 8 Exploration and Hybrid Learning for Nonlinear Interconnected Systems.......... 257
Chapter 9 Event-Triggered Control Applications ................................................................ 275
Bibliography ................................................................................................................................. 317
viiPreface
Modern feedback control systems have resulted in major successes in the fields of mechanical and
aerospace engineering, automotive, space, defense, and industrial systems. A feedback controller
proactively alters the system behavior to meet a desired level of performance by optimizing a per￾formance index. Modern control techniques, whether linear or nonlinear, were developed using state
space or frequency domain techniques. These techniques were responsible for effective flight con￾trol systems, engine and emission controllers, space shuttle controllers, and for industrial systems.
In the past decade, significant advances in theoretical and applied research have occurred in com￾putation, communication, and control to address performance in cyber-physical systems wherein
cyber and physical system components interact through a communication network. Wireless com￾munication is preferred over wired communication as it allows mobility though maintaining control
performance for such systems is a challenge. Recently, a communication network is combined with
the modern control system to form a networked control system (NCS) and this novel cyber-physical
system (CPS) concept is considered a 3rd generation control system. In NCS, a communication
packet carries the reference input, plant output, and control input which are exchanged by using a
communication network among control system components such as sensors, controllers, and actua￾tors.
Compared with traditional control systems, an NCS reduces system wiring with ease of system
diagnosis and maintenance while increasing the system agility and is therefore considered as part
of Industry 4.0. Because of these advantages, the NCS/CPS concepts have been considered in man￾ufacturing and power industries despite the increasing complexity of the system. The complexity
of such man-made systems has placed severe constraints on existing feedback design techniques.
More stringent performance requirements in the face of system uncertainties and unknown envi￾ronments such as a communication network within the control loop have challenged the limits of
modern control. Operating a complex system with network imperfections such as random delays,
packet losses, and quantization errors within the feedback loop and functioning in different operat￾ing regimes requires that the controller be intelligent with adaptive and learning capabilities in the
presence of unknown disturbances, unmodeled dynamics, and unstructured uncertainties. A novel
framework, referred to as performance-based sampling and control, or event-triggered control, has
been proposed and the control design has to be developed to address these challenges.
Recently, neural network-based optimal adaptive controllers that function forward-in-time man￾ner, to perform regulation and trajectory tracking, have been developed using reinforcement learning
and adaptive dynamic programming both in continuous and discrete time. Controllers designed in
discrete time have the important advantage that they can be directly implemented in digital form
on modern-day embedded hardware. Unfortunately, the discrete-time design is far more complex
than the continuous-time design when Lyapunov stability analysis is used since the first difference
in the Lyapunov function is quadratic in the states not linear as in the case of continuous-time. This
coupled with uncertainty in system dynamics along with event-triggered sampling and control for
uncertain linear and nonlinear systems require an advanced optimal adaptive controller design.
This book presents the neural networks and Q-learning-based event-triggered control design to
address regulation and tracking for linear and nonlinear dynamical systems. Several powerful mod￾ern control techniques are used in the book for the design of such controllers. Thorough devel￾opment, rigorous analysis, and simulation examples are presented in each case. Proof sketch are
provided for stability.
The first chapter introduces an introduction to event-triggered and traditional, fixed, and adap￾tive model-based event-triggered control methods designed after providing background on stability.
Chapter 2 discusses the background of adaptive dynamic programming (ADP), optimal control of
ixx Contents
dynamic systems using value and policy iterations, and fundamental aspects of ADP for continuous
and discrete-time systems including Q-learning and actor-critic neural network-based frameworks.
Chapter 3 lays the foundation of the traditional Q-learning-based optimal adaptive event￾triggered control scheme design for uncertain linear discrete-time dynamic systems using infinite
time horizon with state and output feedback. The case of NCS is also attempted with the Q-learning￾based event-triggered design. In Chapter 4, we introduce adaptive event-triggered state feedback
design for uncertain nonlinear continuous-time systems without and with a communication network
within the control loop by using the function approximation property of neural networks with event
sampling. Minimum intersample time and Zeno behavior are also presented.
Chapter 5 confronts the additional complexity introduced by optimizing both the event trigger
sampling instants and uncertain dynamical systems by treating sample selection and control input
as a co-optimization problem. Both linear and nonlinear dynamical systems are considered. Chap￾ter 6 presents the optimal control of linear interconnected systems with hybrid parameter updates.
In Chapter 7, we discuss optimal event-triggered control design for nonlinear interconnected sys￾tems using output feedback and non-zero-sum games. Chapter 8 covers the event-triggered design
of nonlinear interconnected systems with exploration through NN identifier. Chapter 9 treats the
applications of event-triggered control of robot manipulators, unmanned aerial vehicles, and cyber￾physical systems. Since Lyapunov analysis is done within the event triggered instants, the control
schemes covered in the book from chapter 3 and beyond do not require Input to State (ISS) assump￾tion. The MATLAB code for the examples included in the book can be downloaded.
This book has been written for senior undergraduate and graduate students in a college cur￾riculum, for practicing engineers in industry, and for university researchers. Detailed derivations,
stability analysis, and computer simulations show how to design optimal event-triggered controllers
as well as how to build them.
Acknowledgments and grateful thanks are due to our former doctoral students who have forced
us to take our work seriously and be part of it.
This research work is supported by the National Science Foundation under grant ECCS#1406533
and CMMI#1547042 and Missouri University of Science and Technology.
Jagannathan Sarangapani
Rolla, Missouri
Vignesh Narayanan
Columbia, South Carolina
Avimanyu Sahoo
Huntsville, AlabamaList of Figures
1.1 Adaptive model-based event-triggered control system........................................................... 34
1.2 Performance of ZOH-based event-triggering mechanism. ..................................................... 40
1.3 Performance of the fixed model-based event-triggering mechanism. ................................... 40
1.4 Performance of the adaptive model-based event-triggering mechanism. .............................. 41
1.5 Comparison of network traffic for the event-triggered schemes............................................. 42
2.1 Computational model of a single neuron inspired by the anatomy of a biological neuron. .. 50
2.2 One-layer neural network. ...................................................................................................... 51
2.3 Two-layer neural network....................................................................................................... 53
2.4 Hopfield neural network. ........................................................................................................ 55
2.5 Generalized discrete-time dynamical NN............................................................................... 57
2.6 A multilayer neural network................................................................................................... 59
2.7 Convergence of action errors and state trajectories. ............................................................... 82
2.8 (a) NN weights and (b) control signal error for the nonlinear system.................................... 83
2.9 (a) Convergence of state trajectories and (b) comparison of near-optimal cost functions. .... 83
3.1 Event-triggered optimal state feedback regulator. .................................................................. 97
3.2 Evolution of the Lyapunov function during event and inter-event times.............................. 104
3.3 Performance of the event-based controller with state feedback: (a) convergence of state
vector, (b) evolution of the event-trigger threshold and the event-trigger error, and (c)
the total number of trigger instants. ..................................................................................... 107
3.4 (a) Control policy, (b) QFE errors, and (c) parameter estimation error................................ 108
3.5 The event-based controller performance of output feedback design: (a) Convergence of
the system states, (b) evolution of both the event-trigger threshold and the error, (c) the
cumulative number of triggers. ............................................................................................ 113
3.6 The performance of the optimal controller: (a) Convergence of observer state error; (b)
optimal control input; (c) value function estimation error. .................................................. 114
3.7 Block diagram of event-triggered NCS. .............................................................................. 115
3.8 Timing diagram of transmission with time-varying delay. .................................................. 115
3.9 Convergence of (a) closed-loop event-triggered state vector; (b) event-based optimal
control input; and (c) Bellman error. .................................................................................... 126
3.10 (a) Transmission instants and inter-event times; and (b) cumulative number of triggering
instants .................................................................................................................................. 126
4.1 NN structure with an event-based activation function.......................................................... 132
4.2 Structure of the adaptive state feedback ETC system........................................................... 133
4.3 (a) Evolution of the event-trigger threshold. (b) Cumulative number of events................... 140
4.4 Existence of a nonzero positive lower bound on inter-event times. ..................................... 140
4.5 Convergence of (a) system states and (b) approximated control input................................. 140
4.6 Convergence of the NN weight estimates............................................................................. 141
4.7 (a) Evolution of the event-trigger threshold. (b) Cumulative number of events................... 142
4.8 Existence of a nonzero positive lower bound on inter-event times and gradual increase
with the convergence of NN weight estimates to target. ...................................................... 142
4.9 Cumulative number of events with different NN initial weights.......................................... 143
4.10 Convergence of (a) system state vectors and (b) control input............................................. 143
xixii List of Figures
4.11 Convergence of the NN weight estimates............................................................................. 143
4.12 Data rates with periodic as well as event-based sampling.................................................... 144
4.13 Near-optimal event-sampled control system. ....................................................................... 146
4.14 Convergence of (a) system state, (b) control input, and (c) HJB error. ................................ 154
4.15 NN performance. (a) Approximation error ˜f . (b) Approximation error (g˜). ....................... 154
4.16 Evolution of (a) event sampling condition, (b) cumulative number of event-sampled
instants, and (c) inter-sample times. ..................................................................................... 156
4.17 Convergence of the norm of the NN weight estimates. (a) 
Wˆ f

; (b) Wˆ g and (c) Wˆ v ...... 156
5.1 Evolution of (a) the state trajectory; (b) optimal event-based control policy; (c) control
input error and sampling threshold; and (d) cumulative number of sampling instants. ....... 170
5.2 Aperiodic inter-sample times with a minimum inter-sample time of 0.011 s for γ = 0.25.. 171
5.3 Optimal cost surface and comparison of optimal cost trajectory between the game-based
method (Sahoo et al., 2018) and existing method (Wang and Lemmon, 2009b) (a) for
state x1 and x2 with x3 = x4 = 0.; and (b) for state x3 and x4 with x1 = x2 = 0. .................. 171
5.4 Comparison of cumulative cost between game-based method presented in this section
based on (Sahoo et al., 2018) and the method presented by Wang and Lemmon (2009b)... 171
5.5 Self-triggered implementation: (a) evolution of the state trajectory; (b) optimal event￾based control policy; and (c) inter-sample time with γ = 0.25. ........................................... 172
5.6 (a) Evolution of the state trajectory; (b) optimal event-based control policy; (c) trigger￾ing condition; and (d) inter-sample times............................................................................. 172
5.7 Comparison of (a) cumulative sampling instants; and (b) cumulative cost between the
method in (Sahoo et al., 2018) and the method in (Wang and Lemmon, 2009b)................. 173
5.8 Networked control system and event-triggered feedback..................................................... 174
5.9 Approximate optimal event sampled control system............................................................ 177
5.10 Comparison of state trajectories ........................................................................................... 181
5.11 Comparison of control policies............................................................................................. 182
5.12 Comparison of (top) inter-event time; and (bottom) cumulative cost. ................................. 183
5.13 Implementation of the game-based algorithm presented in this section............................... 193
5.14 Plot showing trajectories of (a) cumulative events and (b) inter-sample time...................... 194
5.15 Convergence of tracking errors............................................................................................. 194
5.16 Convergence of event-based Bellman errors. ...................................................................... 194
5.17 Comparison of cumulative costs .......................................................................................... 195
5.18 Cumulative number of sampling instants and inter-sample times........................................ 195
5.19 Convergence of estimated NN weights................................................................................. 196
6.1 Block diagram illustrating distributed control scheme in a large-scale interconnected
system. .................................................................................................................................. 202
6.2 Evolution of the Lyapunov function (a) with Hybrid learning scheme. (b) without Hy￾brid learning scheme. ........................................................................................................... 212
6.3 Estimation error comparison between event-triggered PI, hybrid learning, and event￾triggered TD algorithms when the network is lossless......................................................... 216
6.4 Controller performance with delays. .................................................................................... 216
6.5 Estimation error comparison (a) with 10% packet loss. (b) without packet loss.................. 217
6.6 (Left panel)(a) Inter-event time and (b) cumulative trigger instants (Right panel) Con￾troller performance with packet-loss. (a) State trajectories and (b) control inputs .............. 218
7.1 State trajectories (Linear example)....................................................................................... 235
7.2 Event-triggering mechanism................................................................................................. 235
7.3 State trajectories (Example - 7.2). ........................................................................................ 235
7.4 HJB error (Example - 7.2). ................................................................................................... 236List of Figures xiii
7.5 Observer performance: Example 7.2. .................................................................................. 236
7.6 Observer performance: Example 7.3. ................................................................................... 237
7.7 State trajectory of walking robot. ......................................................................................... 237
7.8 Event-triggered control. ....................................................................................................... 238
7.9 Number of iterations in the inter-event period...................................................................... 238
7.10 Cost comparison (Example 7.3). .......................................................................................... 238
7.11 Convergence of system states and control inputs for continuous (solid lines) and event￾sampled system (dotted lines)............................................................................................... 251
7.12 Convergence of event-sampled error and threshold and inter-event times showing the
asynchronous transmission instants...................................................................................... 252
7.13 Convergence of Bellman error and cumulative cost for continuous and event-sampled
designs. ................................................................................................................................. 252
7.14 Comparison of Bellman error and cumulative cost for the hybrid method with NZS
game and existing method (Narayanan and Jagannathan, 2017).......................................... 253
8.1 Block diagram representation of exploration strategy. ......................................................... 266
8.2 State Trajectories (Dotted (red) Lines – Hybrid Algorithm vs Enhanced hybrid algo￾rithm) (Time in 10−2s).......................................................................................................... 269
8.3 Control torques. .................................................................................................................... 270
8.4 Identifier approximation error............................................................................................... 271
8.5 Comparison of HJB error...................................................................................................... 271
8.6 Comparison of cost............................................................................................................... 271
8.7 Cost function trajectories...................................................................................................... 272
9.1 Controller block diagram - (a) Configuration 1. (b) Configuration 2................................... 279
9.2 Tracking error (a) PD control. (b) NN control. Observer estimation error (c) PD control.
(d) NN control....................................................................................................................... 289
9.3 Case 1: (a) Tracking error. (b) Observer estimation error. (c) Cumulative number of
events. (d) Event-triggering error and threshold................................................................... 290
9.4 Case 2: (a) Tracking error. (b) Observer estimation error. (c) Cumulative number of
events. (d) Event-triggering error and threshold................................................................... 291
9.5 UAV Trajectory Tracking...................................................................................................... 306
9.6 Control Inputs. ...................................................................................................................... 306
9.7 Effectiveness of Event-Sampling.......................................................................................... 307
9.8 State regulation errors........................................................................................................... 314
9.9 Event-triggered estimation error. .......................................................................................... 314
9.10 The cost function comparison............................................................................................... 314
9.11 The fairness comparison. ...................................................................................................... 315List of Tables
1.1 Commonly used p-norms on Rn ............................................................................................... 8
1.2 ZOH-based Event-Triggered Control ..................................................................................... 27
1.3 Model-based Event-Triggered Control................................................................................... 31
1.4 ZOH-based Event-Triggered Control of Linear Discrete-time System.................................. 35
1.5 Fixed Model-based Event-Triggered Control of Discrete-time System ................................. 37
1.6 Adaptive Model-based Event-Triggered Control of Linear System....................................... 39
2.1 Backpropogation Algorithm ................................................................................................... 63
3.1 State feedback control scheme................................................................................................ 98
3.2 Observer-based feedback controller design.......................................................................... 110
3.3 Co-optimization of sampling and control for NCS............................................................... 125
4.1 Event-triggered control of nonlinear system ........................................................................ 139
4.2 Comparison of computational load between periodic and event-sampled system. .............. 144
4.3 Event-triggered optimal control of nonlinear system ........................................................... 155
5.1 Co-optimization of sampling instants and control policy of linear systems......................... 170
5.2 Co-optimization of sampling instants and control policy for nonlinear systems ................. 180
5.3 Co-optimization of sampling instants and control policy for trajectory tracking................. 192
6.1 Distributed hybrid Q-Learning ............................................................................................. 213
6.2 Comparison of parameter error convergence time................................................................ 217
7.1 State feedback controller design equations........................................................................... 228
7.2 Event-triggered state feedback controller design equations ................................................. 229
7.3 Design equations for hybrid learning with output feedback................................................. 231
7.4 Numerical analysis................................................................................................................ 239
7.5 Design equations based on distributed Nash games ............................................................. 244
7.6 Design equations based on distributed Nash games using event-triggered feedback........... 247
8.1 Distributed NN identifier design equations .......................................................................... 262
8.2 Distributed NN control design equations ............................................................................. 264
9.1 Output feedback controller design using Configuration 1.................................................... 284
9.2 Output feedback controller design using Configuration 2.................................................... 288
9.3 Configuration 1: Analysis with σ ......................................................................................... 289
9.4 Configuration 2: Analysis with σ ......................................................................................... 290
9.5 Output feedback controller design for a quadrotor UAV...................................................... 305
9.6 Effects of Event-Sampling on Mean Squared Errors and Control Effort Means ................. 307
9.7 Effects of Event-Sampling on Mean Squared Errors and Control Effort Means ................. 308
xv1 Background and
Introduction to
Event-Triggered Control
CONTENTS
1.1 Introduction ............................................................................................................................... 1
1.1.1 Continuous-Time Systems............................................................................................ 2
1.1.2 Discrete-Time Systems ................................................................................................. 4
1.1.3 Sampled-Data Control Systems.................................................................................... 5
1.1.4 Impusive Hybrid Dynamical Systems .......................................................................... 6
1.2 Mathematical Preliminaries....................................................................................................... 7
1.2.1 Vector Norms................................................................................................................ 7
1.2.2 Matrix Norms................................................................................................................ 8
1.2.3 Quadratic Forms and Definiteness.............................................................................. 10
1.2.4 Properties of Functions and their Norms.................................................................... 11
1.3 Stability.................................................................................................................................... 14
1.3.1 Equilibrium Point........................................................................................................ 14
1.3.2 Stability Definitions.................................................................................................... 15
1.3.3 Lyapunov Theorems for Stability ............................................................................... 16
1.4 Event-triggered Control Systems ............................................................................................ 22
1.4.1 Background and History............................................................................................. 23
1.4.2 Event-triggered Control of Continuous-Time Systems .............................................. 26
1.4.3 Event-triggered Control of Discrete-time Systems..................................................... 32
1.5 Closing Remarks ..................................................................................................................... 41
1.6 Problems.................................................................................................................................. 42
This chapter provides a concise overview of dynamical systems, including ordinary differential
and difference equations representing continuous-time and discrete-time systems, respectively. Ad￾ditionally, it introduces hybrid systems that combine characteristics of both differential and dif￾ference equations. It then delves into fundamental stability principles, utilizing Lyapunov-based
methodologies, essential for analyzing these systems. Furthermore, it explores feedback control
systems, emphasizing the practical implementation of control policies using embedded processors
as sampled-data controllers employing periodic sampling of system states or outputs. Alternatively,
aperiodic sampling-based control frameworks, also referred to as the event-triggered control, is in￾troduced later in the chapter, where we shall review some of the basic event-based sampling and
controller execution techniques that are aimed at optimizing system performance while minimizing
computational overhead. The final section introduces conventional event-triggered control strate￾gies documented in the existing literature with the necessary mathematical prerequisites provided
for completeness.
12 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
1.1 INTRODUCTION
During the early 1960s, the field of control systems underwent a profound transformation with
the advent of state space methods. This paradigm shift marked a departure from the conventional
approaches grounded in Laplace transforms and transfer functions. Instead, it ushered in a new era
centered around vector spaces, as well as the study of differential and difference equations. This piv￾otal period also cast a bright spotlight on the concept of stability, particularly within the framework
of Lyapunov theory (Kalman, 1963; Kalman and Bertram, 1960; Brockett, 2014). Furthermore, it
witnessed significant advancements in optimization theory, most notably Pontryagin’s maximum
principle (Bittner, 1963), along with the emergence of innovative adaptive control techniques, in￾cluding model-reference adaptive control, gain-scheduling, etc. (Annaswamy and Fradkov, 2021).
In parallel, developments in reinforcement learning theory for solving sequential decision making
problems along with the emergence of artificial neural networks as universal function approximators
spurred the development of data-driven control methodologies (Sutton and Barto, 1998; Narendra
and Parthasarathy, 1990; Lewis et al., 1998). These methodologies proved exceptionally well-suited
for managing complex dynamical systems that often defy concise modeling using traditional first￾principle techniques.
Fast forward to the present day, and we find ourselves in an era characterized by substantial
progress in computing power, communication technologies, and control systems. These advance￾ments have paved the way for cyber-physical systems (CPS), which integrates cyber and physi￾cal system components, and are increasingly used in safety- or infrastructure-critical applications
(e.g., manufacturing units, smart grids, traffic networks). These developments have enhanced the
design and deployment of sophisticated systems in domains such as manufacturing, energy, and
transportation, wherein complex networks of interconnected physical systems enabled by cyber in￾frastructures are commonplace. These systems are often described as ‘smart’ because they possess
the capability to self-monitor, communicate, and autonomously govern their operations. Given the
intricacy of tasks performed by these CPS, efficient control components are often needed to ensure
efficiency and robustness. This has contributed to the advancements in resource-efficient control
design techniques and data-driven optimal control methods. These methodologies play a crucial
role in minimizing communication overhead, computational costs, and ensuring a predetermined
level of performance. For instance, event-triggered control techniques and adaptive or approximate
dynamic programming-based controllers are valuable tools developed to address the demands of
CPS.
In the following chapters, we will systematically delve into these tools and dissect their design
principles. In this chapter, we shall lay the foundation by providing a brief review of fundamental
building blocks necessary for constructing efficient controllers capable of managing large and com￾plex dynamical systems. We shall start by reviewing various system descriptions, followed by essen￾tial mathematical preliminaries. Subsequently, we shall review stability concepts and conclude with
an overview of event-triggered control techniques. Several excellent expositions on dynamical sys￾tems and control are available, for instance, in Kwakernaak and Sivan (1972); Luenberger (1979);
Chen (1984); Dorf and Bishop (2000); Ogata (2011); Astr ˚ om and Murray (2021). For more infor- ¨
mation on mathematical preliminaries, see Royden and Fitzpatrick (1968), and on event-triggered
control, see Lemmon (2010).
1.1.1 CONTINUOUS-TIME SYSTEMS
Engineered and naturally occurring systems, encompassing physical, chemical, and biological do￾mains, often exhibit dynamic behavior. These systems possess internal memory, respond to external
perturbations or stimuli, and evolve over time based on governing first-principles and external in￾fluences. The formalization of the concept of a system was initiated in the early 1900s, and in this
context, a system is regarded as a distinct entity from its environment, defined by its interactionsBackground and Introduction to Event-Triggered Control 3
through input and output signals.
In the realm of feedback control, dynamical systems assume a pivotal role. These systems, be
they drones, robots, or temperature regulation setups, possess a state evolving over time, governed
by differential or difference equations. The essence of feedback emerges when we measure the cur￾rent system state or a derived output and employ this information to shape the system’s behavior
in line with our objectives. Through judicious manipulation of these governing principles via feed￾back mechanisms, we seek to exert control over the system, achieving desired outcomes such as
guiding a drone along a predefined trajectory or maintaining a specific room temperature. Some
of these systems exhibit continuous state evolution, characterized by differential equations, and are
thus classified as continuous-time dynamical systems or continuous systems.
A continuous-time dynamical system is typically defined by a differential equation of the form
dx(t)
dt = x˙(t) = f(x(t),t), x(t0) = x0, (1.1)
where x(t) ∈ Rn is the state of the system at time t, t0 is the initial time, and x0 is the initial state of
the system at t0. The vector function f : Rn ×R → Rn specifies the rate of change of the system’s
state. This function is often referred to as the dynamics of the system in (1.1) or the drift vector field.
For a system to be well-defined, f is typically required to be Lipschitz continuous, which guarantees
the existence and uniqueness of solutions (1.1) (Khalil, 2002).
In a more formal way, we can define a dynamical system as a tuple {T,X,Φ}, where T ⊆ R is
the time set. This set could represent continuous time (for instance, T = R or T = [0,∞)) or discrete
time (such as T = Z or T = N). The set X is a nonempty set known as the state space. This set
contains all possible states that the system can occupy. Usually, X ⊆ Rn for some positive integer
n, but X can be any set in general. The function Φ : T × T × X → X is a function describing the
evolution of the system over time. For a given time t ∈ T and a state x ∈ X, Φ(t,t0, x) returns the
state of the system at time t if the system started in the state x at time t0. Without loss of generality,
we may set t0 = 0 and represent Φ(t,t0, x) as Φ(t,x) with the initial time argument suppressed.
The function Φ must satisfy the following two conditions for all t,s ∈ T and x ∈ X
1. Φ(0,x) = x,
2. If t +s ∈ T, then Φ(t +s,x) = Φ(s,Φ(t,x)).
The first condition indicates that if no time has passed, the state of the system remains unchanged.
The second condition, known as the semigroup property (Layek et al., 2015), represents the idea
that to find the state of the system at time t + s, one may first track the system evolution from
state x for time t units so that state becomes Φ(t, x), and then let it evolve from this state for an
additional time s units. These two properties are fundamental axioms of dynamical systems and
ensure a degree of consistency and predictability in the system’s time evolution. We can verify that
the system represented in (1.1) satisfies the above two conditions (Layek et al., 2015).
Controlled dynamical systems are defined by a set of n coupled first-order ordinary differential
equations, which can be compactly represented as
x˙(t) = f(t, x(t),u(t)) (1.2)
where x(t)=(x1(t),..., xn(t))T is the state vector, u(t)=(u1(t),...,um(t))T is the input vector or
control, f : R×Rn ×Rm → Rn is a vector function with n components, i.e., f = (f1,..., fn)T , and
x˙(t)=(x˙1(t),..., x˙n(t))T denotes the derivative of x(t) with respect to time. Each function fi for
i = 1,...,n, gives the rate of change of the corresponding state variable xi as a function of time t,
the current state x, and the current input u.
The equation in (1.2) is also referred to as the state equation. The output equation can be ex￾pressed as
y(t) = h(t,x(t),u(t)) (1.3)4 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
where y(t) ∈ Rp is the p-dimensional output vector and the function h : R × Rn × Rm → Rp is a
vector-valued function. The equations (1.2) and (1.3) are known as the state-space model or state
model. Specifically, the state equation is a non-autonomous (the evolution explicitly depends on
time) and non-affine form. An autonomous system takes the form
x˙(t) = f(x(t)), x(t0) = x0, (1.4)
where the function f(·) does not explicitly depend on time t.
A control system in an affine form is a dynamical system expressed as
x˙(t) = f(x(t)) +
m
∑
i=1
gi(x(t))ui(t), x(t0) = x0, (1.5)
where x(t) ∈ Rn, u(t)=(u1(t),...,um(t))T ∈ Rm is the input (or control) vector, f : Rn → Rn is the
drift vector field representing the system’s internal dynamics in the absence of control, gi : Rn → Rn
for i = 1,...,m are control vector fields or control coefficients modeling the influence of the control
inputs on the system dynamics. Each of these functions, i.e., f,g1,...,gm, can either be a linear or a
nonlinear function of the system state. Each control input ui(t) influences the state dynamics through
the corresponding control vector field gi(x), and these influences are added together to give the total
rate of change of the system’s state. The term “affine” here refers to the fact that the dependence
of the system dynamics on the control inputs ui(t) is affine (linear with a constant offset due to the
internal drift). We can write the state equation in (1.5) in a compact form as
x˙(t) = f(x(t)) +g(x(t))u(t), x(t0) = x0, (1.6)
where the control coefficient g : Rn → Rn×m is a matrix-valued function and the control input u(t) ∈
Rm for all time t.
In the state feedback control framework, the goal is to design a control input for the system that
renders the closed-loop system stable. This control input is designed as a continuous function of
time, taking the form
u(t) = μ(x(t)), (1.7)
where μ : Rn → Rm is the control policy or control law that is dependent on the system state obtained
as feedback. Then the controlled system or the closed-loop system can be represented as
x˙(t) = f(x(t),μ(x(t))), x(t0) = x0. (1.8)
There are several approaches available to design the state feedback nonlinear controllers to synthe￾size control laws when the system dynamics (i.e., the drift and control vector fields) are known.
Examples range from PID controllers to feedback linearization, backstepping, sliding mode con￾trollers, and so on, and can be found in several nonlinear control system texts such as Khalil (2002);
Lewis et al. (2012b); Astr ˚ om and Murray (2021). ¨
As a special case of the nonlinear time-invariant affine system (1.6), we have the linear time￾invariant (LTI) system expressed as
x˙(t) = Ax(t) +Bu(t), x(t0) = x0, (1.9)
y(t) = Cx(t) +Du(t), (1.10)
where A ∈ Rn×n and B ∈ Rn×m are the internal dynamics and control coefficient matrices, respec￾tively. The matrices C ∈ Rp×n and D ∈ Rp×m are the output matrices. Here the drift vector field
f(x(t)) is a linear function of the state Ax(t) and the control vector field g(x(t)) is a constant matrix
B. The state feedback control input in the case of a linear system can be expressed as
u(t) = Kx(t), (1.11)
where K ∈ Rm×n is the control gain and can be designed using techniques such as pole placement
(Chen, 1984) or linear quadratic regulation (LQR) approaches (Lewis et al., 2012b; Kwakernaak
and Sivan, 1972).Background and Introduction to Event-Triggered Control 5
1.1.2 DISCRETE-TIME SYSTEMS
A discrete dynamical system is typically defined by a difference equation capturing the evolution of
the system state and the output equation, taking the form
xk+1 = f(k,xk,uk), x0 is the initial state,
yk = h(xk,uk), (1.12)
where xk ∈ Rn is the state, uk ∈ Rm is the control input, yk is the output of the system at the discrete
time steps k ∈ Z. The function f : Z×Rn ×Rm → Rn specifies how the state of the system changes
from one-time step to the next and the function h : Rn ×Rm → Rp maps the states and control to the
output. Without loss of generality, we shall consider the initial time to be 0 and the initial state is
defined as x0. Similar to the continuous case, a minimum that f be Lipschitz continuous can ensure
the existence and uniqueness of solutions to the difference equation (1.12). Note that the time index
in a discrete-time system is an integer k instead of a real number t in a continuous-time system.
In the state feedback control framework, the goal is to design a sequence of control inputs
uk = μ(xk), (1.13)
which renders the closed-loop system
xk+1 = f(xk,μ(xk)), (1.14)
stable. Similar to the continuous-time case, a linear time-invariant (LTI) discrete-time system can
be represented by a linear difference equation as
xk+1 = Axk +Buk, (1.15)
yk = Cxk +Duk, (1.16)
where A ∈ Rn, B ∈ Rn×m, C ∈ Rp×n and D ∈ Rp×m are all constant matrices. The discrete-time state
feedback control input can be defined as
uk = Kxk (1.17)
where K is the control gain matrix.
The discrete-time dynamics of a system as expressed in the equation (1.12) may be derived in
two ways. They might originate directly from analyzing the dynamical system or process under
consideration, where the system state evolves naturally in discrete time-steps. For example, the time
evolution of a counter, population of bacterial cells (broadly, population models), epidemic models,
etc. Alternatively, they could represent discretized or sampled versions of continuous-time dynam￾ical system. In contemporary settings, controllers are predominantly implemented in a digital form
due to the widespread use of embedded hardware. This necessitates a discrete-time representation
of the controller, which could be established through design based on the discrete-time system dy￾namics.
1.1.3 SAMPLED-DATA CONTROL SYSTEMS
In many practical applications, the system or the plant operates in continuous time but the controller
operates in discrete time. This is often the case when the controller is implemented digitally us￾ing embedded processors. The controller samples the output of the plant at discrete time intervals,
and these samples are used to compute control inputs. The system dynamics are described by a
combination of differential equations (for the plant) and difference equations (for the controller).6 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
To understand the implementation, we start by considering a nonlinear continuous-time system
in nonaffine form, represented as
x˙(t) = f(x(t),u(t)), x(t0) = x0, (1.18)
where x : R → Rn is the state with x(t0) = x0 ∈ Rn being the initial condition and u : R → Rm
is the control input. In order to implement the controller u, we resort to an embedded processor
that inherently operates in a digital domain. This entails utilizing a sampled version of the state for
computing the control input.
To formalize this, we can define the sequence of sampling instants as {tk}∞
k=0 such that tk+1 > tk
for k = 1,2,.... The kth sampling instant is denoted by tk ∈ R. The sampler output yields a sequence
of sampled states, {xˆk}, where ˆxk = x (tk). A state-feedback controller K : Rn → Rm then maps this
sampled state onto a control vector ˆuk ∈ Rm. The sequence {uˆk}∞
k=0 of these controls is transformed
into a continuous-time signal via a zero-order hold without delay. This renders the control signal,
u : R → Rm, employed by the plant as a piecewise constant function.
Although both the discrete-time systems and the sampled-data control systems have components
of discreteness, they represent different concepts. In the discrete-time systems, all variables change
at certain discrete moments in time. The system’s state is updated at these discrete instants based
on a discrete-time state-transition function. The changes in the system are only registered at these
specific instants and the changes at any other time are ignored or assumed non-existent. On the other
hand, in the sampled-data control systems, the control input to a continuous-time plant or system
is updated only at discrete time instants, with the control input held constant (or varying according
to some specified rule) between these instants. The system’s dynamics are continuous in time, but
the controller only “observes” and “acts” on the system at discrete instants. This is a form of hybrid
system, as it has both continuous and discrete elements. Sampled-data control systems are common
in practice due to the prevalence of digital control implementations.
1.1.4 IMPUSIVE HYBRID DYNAMICAL SYSTEMS
An impulsive hybrid dynamical system exhibits a mix of continuous and discrete (impulsive) be￾haviors. A general impulsive hybrid system can be represented as
x˙(t) = f(x(t),u(t)), x(0) = x0, (t,x(t)) ∈/ D,
x(t
+) = F(x(t)), (t, x(t)) ∈ D, (1.19)
where x(t) ∈ C ⊆ Rn represents the state of the system at time t with initial state x(t0) = x0, C is
an open set containing the origin, and u(t) ∈ Rm represents the control input. The first equation
describes the flow dynamics, where f : C → Rn is continuous and the second equation describes the
systems’ jump dynamics with a continuous function F : D → Rn. The set D ⊂ [0,∞)× Z , referred
to as jump set or resetting set, represents the set of times and the associated states when impulsive
changes occur. Here x(t
+) = F(x(t)) = limε→0 x(t +ε) and Z ⊂ C denotes the set of resetting time
instants.
In some cases, C and D can intersect, meaning the system may either flow or jump. The specific
behavior, in this case, depends on the exact form of the hybrid system model. These sets can be
characterized based on the physical properties of the system. For instance, in a bouncing ball system,
the flow set consists of times during which ball is in the air or at rest on the ground, while the jump
set consists of times when the ball is impacting the ground. For more information on the hybrid
dynamical systems see Bainov and Simeonov (1993); Goebel et al. (2009).
The impulsive hybrid dynamical systems and sampled data systems have distinct characteristics
and uses. They differ primarily in the nature of their discrete and continuous dynamics and their
typical application domains. An impulsive hybrid dynamical system exhibits behavior that is a mix
of continuous dynamics and instantaneous, discrete changes (impulses). The impulsive actions canBackground and Introduction to Event-Triggered Control 7
occur at fixed times (as in clock-driven systems), at variable times determined by the state of the
system (as in event-driven systems), or in a manner that is a combination of the two. The defining
characteristic of impulsive hybrid systems is that they can change their state instantly in response
to certain events, as determined by the jump set and map. These systems are used to model various
real-world phenomena, such as bouncing balls, circuit switchings, or biological systems with sudden
events. On the other hand, in a sampled data system, the control input is updated only at discrete
time instants resulting from a sampling process. The system evolves continuously between these
updates, typically driven by a held (constant) input. This type of system is common in digital control
implementations, where the controller reads sensor data (sampling), computes control actions, and
updates the control inputs only at discrete times. The controller may use a zero-order hold or some
other method to maintain the control input between updates. As we shall see later in the chapter,
event-triggered control systems can be seen as both sampled data systems with aperiodic sampling
intervals as well as impulsive dynamical system.
1.2 MATHEMATICAL PRELIMINARIES
In this section, some of the mathematical tools required for the analysis and design of optimal
event-triggered control systems are reviewed. We restrict our focus to the most important tools and
techniques that are applied throughout the rest of the book. To gain a broader perspective and a more
comprehensive understanding of these topics, please refer to Rudin (1953), Royden and Fitzpatrick
(1968), and Strang (2022).
1.2.1 VECTOR NORMS
The norm of a vector in an n-dimensional real space is a non-negative value that, in some sense,
measures the length (or size) of the vector. Consider Rn, the set of all n-dimensional vectors x =
(x1,x2,··· , xn)T , where x1,··· , xn−1, and xn are real numbers.
Definition 1.1. The norm · of vectors in Rn is a real-valued function with the following proper￾ties:
1. x ≥ 0 for all x ∈ Rn, with x = 0 if and only if x = 0
2. x+y≤x+y, for all x,y ∈ Rn (triangle inequality)
3. αx = |α|x, for all α ∈ R and x ∈ Rn
The concept of a norm can be extended to measure the distance between two vectors. Let x and
y be two vectors in Rn. The distance between these two vectors can be defined as x − y. There
are several commonly used metrics that satisfy the Definition 1.1. For instance, a class of p-norm,
where p ≥ 1, can be defined as
xp = (|x1|
p +|x2|
p +···+|xn|
p)
1/p, ∀x ∈ Rn.
For p = 1,2,∞, the corresponding norms x1, x2, and x∞ are defined as shown in Table 1.1.
Listed below are some frequently employed inequalities involving various vector norms (p￾norms) in Rn, which we will frequently refer to in this book.
1. Triangle Inequality: For any vectors x and y, and any p ≥ 1,
x+yp ≤ xp +yp.
2. Reverse Triangle Inequality: For vectors x, y and any p ≥ 1,
|xp − yp|≤x−yp.8 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Table 1.1
Commonly used p-norms on Rn
p-norm on Rn Expression
x1 |x1|+|x2|+···+|xn|
x2 (Euclidean norm) (|x1|
2 +|x2|
2 +···+|xn|
2)1/2 = (xT x)1/2
x∞ (infinity norm) maxi |xi|
3. Holder’s Inequality: For any vectors x = (x1,x2,...,xn)T and y = (y1,y2,...,yn)T , and any p,
q ≥ 1 such that 1
p + 1
q = 1,
|x1y1 +x2y2 +...+xnyn| = |xT y|≤xpyq.
4. Cauchy-Schwarz Inequality: This is a special case of Holder’s inequality when p = q = 2. For
any vectors x and y,
|x1y1 +x2y2 +...+xnyn| = |xT y|≤x2y2.
All p-norms are equivalent in the sense that if ·α and ·β are two different p-norms, then
there exists positive constants c1 and c2 such that
c1xα ≤ xβ ≤ c2xα
for all x ∈ Rn. For the norms x1, x2, and x∞, these inequalities take the forms of
x2 ≤ x1 ≤ √nx2 and x∞ ≤ x2 ≤ √nx∞.
1.2.2 MATRIX NORMS
For x ∈ Rn, an m×n matrix A of real elements defines a linear mapping y = Ax from Rn to Rm. The
norm of a matrix is a non-negative value that can represent the “size”, “length”, or “magnitude” of
the matrix.
Definition 1.2. A function · : M → R, where M is a set of matrices in Rm×n, is called a matrix
norm if it satisfies the following properties for all matrices A,B ∈ M and all scalars λ
1. A ≥ 0 (non-negativity)
2. A = 0 ⇔ A = 0 (definiteness)
3. λA = |λ|·A (homogeneity or absolute scalability)
4. A+B≤A+B (subadditivity or triangle inequality)
These four properties are the defining characteristics of a matrix norm. Different norms might
weigh the elements of the matrix differently, but they all have to satisfy these properties. There are
three more commonly used matrix norms.
1. Frobenius Norm: This is the direct analogue to the Euclidean norm for vectors. For an m × n
matrix A = [ai j], the Frobenius norm is defined as
AF =
 m
∑
i=1
n
∑
j=1
|ai j|
2.Background and Introduction to Event-Triggered Control 9
2. Induced matrix or Operator Norm: This is induced by the vector p-norm. For any matrix
A ∈ Rm×n and x ∈ Rn, the p-norm (or operator norm) is defined as
Ap = sup
x=0
Axp
xp
,
where “sup” denotes the supremum over all non-zero vectors x. In other words, it is the maximum
ratio of the norm of the output vector Ax ∈ Rm to the norm of the input vector x, over all non-zero
input vectors. For p = 1,2, and ∞, the corresponding induced norms are given as follows:
a. 1-Norm (or Maximum Absolute Column Sum Norm): This is the maximum absolute
column sum of the matrix. If A = [ai j] is a matrix, then the 1-norm of A is defined as
A1 = max
j ∑
i
|ai j|.
b. Induced 2-Norm (or Spectral Norm): The 2-norm (Euclidean norm) of a matrix is the
square root of the largest eigenvalue of the matrix product AT and A, given as
A2 = [λmax(ATA)] 1
2 .
It is also equal to the largest singular value of the matrix.
c. Infinity Norm (or Maximum Absolute Row Sum Norm): This is the maximum absolute
row sum of the matrix. If A = [ai j] is a matrix, then the infinity norm of A is defined as
A∞ = max
i ∑
j
|ai j|.
There are several matrix inequalities we will use in this book and are listed below.
1. Triangle Inequality: The triangle inequality applies to any matrix norm (by definition). For any
matrices A and B
A+B≤A+B.
2. Inverse of a Matrix: If A is invertible, then the norm of the inverse matrix satisfies
A−1 ≥ 1
A
.
3. Submultiplicativity: For any two matrices A and B
AB≤AB.
4. Spectral Radius: The spectral radius ρ(A) of a matrix A (the maximum absolute value of its
eigenvalues) is always less than or equal to any matrix norm
ρ(A) ≤ A.
5. Consistency of Norms: All matrix norms are equivalent in the sense that for any two norms
·α and ·β , there exist positive constants c1 and c2 such that
c1Aα ≤ Aβ ≤ c2Aα
for all matrices A.
6. Cauchy-Schwarz Inequality: For matrices A, B with compatible dimensions
|A,B| ≤ AFBF,
where A,B represents the Frobenius inner product of A and B, defined as the trace of the matrix
product BT and A, and ·F is the Frobenius norm.10 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
7. Norm of a Transpose: The norm of a matrix and its transpose are equal
AT  = A.
These equalities and inequalities provide useful ways to estimate the magnitude of matrices,
compare different matrices, and analyze the stability and convergence of numerical algorithms in￾volving matrices. The interpretation of the matrix norms depends on the type of norm being used.
For example, the Frobenius norm can be interpreted as a measure of “magnitude” or “size” of the
matrix. The induced p-norm (or operator norm) measures the maximum amount by which the matrix
can “stretch” a vector when the vector norm is measured using the corresponding vector p-norm.
Example 1.1. Let x = 
1 −10 2 0
. Compute the vector p-norms for p = 1,2, and ∞.
The vector norms can be found as
x∞ = | −10| = 10
x1 = 13
x2 = √
105
Example 1.2. Given a matrix A =
⎡
⎣
4 −4 2
−123
−210
⎤
⎦. Compute the matrix p-norms for p = 1,∞, and
the Frobenius norm.
Solution: The norms can be calculated as follows: The 1-norm of a matrix is the maximum of the
absolute column sum, which is computed as
A1 = max{4+1+2,4+2+1,2+3+0} = max{7,7,5} = 7
The ∞-norm of a matrix is the maximum of the absolute row sum, which is computed as
A∞ = max{4+4+2,1+2+3,2+1+0} = max{10,6,3} = 10
The Frobenius norm of a matrix is the square root of the sum of squares of all the elements of the
matrix and it is computed as
AF =


42 + (−4)2 +22 + (−1)2 +22 +33 + (−2)2 +12 +02 = √
55
1.2.3 QUADRATIC FORMS AND DEFINITENESS.
Given a real symmetric matrix A ∈ Rn×n (i.e., AT = A) and a vector x ∈ Rn, the quadratic form is
defined as
f(x) = xTAx,
where xT denotes the transpose of x, and the result of this expression is a scalar. For example, given
a 2×2 matrix A =

a b
b d
and a two dimensional vector x =

x1
x2

, the quadratic form can be written
as
f(x) = 
x1 x2


a b
b d x1
x2

= ax2
1 +2bx1x2 +dx2
2.Background and Introduction to Event-Triggered Control 11
Definition 1.3. A symmetric matrix A ∈ Rn×n is said to be
1. Positive definite: if f(x) > 0 for all x ∈ Rn with x = 0.
2. Negative definite: if f(x) < 0 for all x ∈ Rn with x = 0.
3. Positive semi-definite: if f(x) ≥ 0 for all x ∈ Rn.
4. Negative semi-definite: if f(x) ≤ 0 for all x ∈ Rn.
5. Indefinite: if it is neither positive semi-definite nor negative semi-definite.
In the above definition, we assumed A is symmetric. However, the notion of the definiteness is not
restricted to symmetric matrices. Any square matrix can be decomposed into a sum of a symmetric
and a skew symmetric matrices, i.e., for any A ∈ Rn×n, we have A = 1
2 (A−AT )+ 1
2 (A+AT ), where
the second term yields a symmetric matrix and the first term yields a skew-symmetric matrix. It
can be shown that for a nonzero vector x ∈ Rn and a nonzero matrix A ∈ Rn×n the quadratic form
xTAx = 0 if and only if A is a skew-symmetric matrix. Determining the positive definiteness of a
matrix from the above definition is cumbersome. Alternatively, we can use the following definition
to evaluate the positive definiteness of a symmetric matrix.
Definition 1.4. Let A ∈ Rn×n be a symmetric matrix. The matrix A is positive definite if and only if
any of the following conditions hold
1. λi(A) > 0, i = 1,2,...,n, where λi(·) is the ith eigenvalue of A. Note that the eigenvalues of A are
real since the matrix is symmetric.
2. Every principal minor of A is positive.
3. There exists a nonsingular matrix A1 such that A = A1AT
1 .
4. xTAx ≥ αx2 for some α > 0 and ∀x ∈ Rn.
These properties are crucial in many fields, including optimization, as they help determine the
nature of a function. For example, if the Hessian of a function evaluated at a critical point, which is
a symmetric matrix, is positive definite, it means that the critical point is a local minimum.
A symmetric matric A ∈ Rn×n can be decomposd as
A = VTΛV
where V is a orthogonal matrix (i.e., VTV = I) composed of n-orthogonal eignvectors of A, and Λ
is a digonal matrix with eigenvalues of A as the diagonal elements.
Example 1.3. Let x =

x1
x2

∈ R2 and V(x) = x2
1 +x2
2 ∈ R. Show that V(x) is positive definite (PD).
Solution One can check that
V(x) = x2
1 +x2
2 = 0
when x1 = 0 and x2 = 0 and x2
1 +x2
2 > 0 for all x1 = 0 or x2 = 0. Alternatively, we can rewrite V(x)
in a quadratic form as V(x) = xT

1 0
0 1
x. Since the matrix 
1 0
0 1
x has positive eigenvalues, the
function V(x) is PD.
Example 1.4. Let x =

x1
x2

∈ R2 and V(x) = x2
1.
For x =

0
x2

with x2 = 0, we have V(x) = x2
1 = 0. Therefore, the function V(x) is not positive
definite. It is positive semidefinite.12 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
1.2.4 PROPERTIES OF FUNCTIONS AND THEIR NORMS
Before discussing the properties of the functions, we will review the definitions of the various sets
we will use throughout the book.
An open ball B centered at x ∈ Rn with a radius of r ∈ R>0 is the collection of points y ∈ Rn such
that y−x < r. Alternatively, we can express this as
B(x,r) := {y ∈ Rn | y−x < r}.
A closed ball B¯ centered at x ∈ Rn with a radius of r ∈ R>0 is the collection of points y ∈ Rn
such that y−x ≤ r, and is expressed as
B¯(x,r) := {y ∈ Rn | y−x ≤ r}.
A sphere S centered at x ∈ Rn with a radius of r ∈ R>0 is the collection of points y ∈ Rn such
that y−x = r. This set is given by
S(x,r) := {y ∈ Rn | y−x = r}.
To generalize the above definitions, we can use the distance function d(x, y) defined in a metric
space instead of the · to define these sets.
The ε-neighborhood of point x ∈ Rn is the open ball of radius ε centered at x, i.e., B(x, ε).
A set D ⊆ Rn is called open iff
∀x ∈ D,∃ε > 0 such that B(x, ε) ⊆ D.
A set D ⊂ Rn is called closed iff its complement Dc is open.
A set D ∈ Rn is bounded if there is r > 0 such that x ≤ r for all x ∈ D.
A set D ∈ Rn is compact if it is closed and bounded.
Some of the important properties of functions are introduced next.
Definition 1.5. (Continuity) A function f : D → Rm is called continuous at x ∈ D ⊆ Rn iff
∀ε > 0,∃δ > 0 such that x−y < δ =⇒  f(x)− f(y) < ε. (1.20)
A function f is continuous on a set D iff it is continuous at x for all x ∈ D.
Definition 1.6. (Uniform continuity) A function is uniformly continuous on D, if given ε > 0 there is
δ > 0 (dependent only on ε) such that for all x, y ∈ D, we have x−y < δ =⇒  f(x)− f(y) < ε.
Note that uniform continuity is defined on a set and continuity is defined at a point. For uniform
continuity, the same constant δ works for all pair of points in D. If f is uniformly continuous on a
set D, then it is continuous on D. If the set D is compact (closed and bounded), then continuity and
uniform continuity are equivalent.
Example 1.5. There are many examples of functions that are both continuous and uniformly con￾tinuous. A very common one is the linear function f(x) = mx+b, where m and b are constants.
Solution: This function is defined for all real numbers. The function f(x) = mx + b is continuous
everywhere in its domain (R). This is because for any ε > 0, we can always choose δ = ε
|m| (assum￾ing m = 0, if m = 0, then f(x) = b which is constant and hence continuous). Then, if |x − c| < δ,
we have
| f(x)− f(c)| = |mx+b−mc−b| = |m||x−c| < |m|δ = ε.
So, the function f(x) = mx+b is continuous at every point in its domain. The function f(x) = mx+b
is also uniformly continuous. This is because the δ that we chose above works for all pair of points
(x, c) in the domain of the function.Background and Introduction to Event-Triggered Control 13
Example 1.6. Consider the function f(x) = x2 on the set of real numbers.
Solution: This function is continuous at every point in its domain, which includes all real numbers.
This can be confirmed using the ε −δ definition of continuity. Given any point c ∈ R and any ε > 0,
choose a δ = ε
|x+c|+1 such that if |x − c| < δ, then | f(x) − f(c)| = |x2 − c2| = |(x + c)(x − c)| ≤
|x + c||x − c|. Substituting the choice of δ, we have | f(x) − f(c)|≤|x + c||x − c| < |x + c|δ = |x+c|
|x+c|+1 ε < ε. So, f(x) = x2 is continuous. However, f(x) = x2 is not uniformly continuous on the
set of all real numbers. This is because the function becomes increasingly steep as x increases or
decreases, so there is no single δ that works for all x. To see this more explicitly, consider two points
x = n and x = n+1/n, where n is any positive integer. The difference in the function values at these
points is
f(n+1/n)− f(n)=(n+1/n)
2 −n2 = 2+1/n2.
So even though the difference |x − n| = 1/n can be made arbitrarily small by choosing a large
enough n, the difference | f(x)− f(n)| = 2+1/n2 can never be made less than 2. Thus, there is no
δ such that | f(x)− f(n)| < ε whenever |x−n| < δ for all x and n, as would be required for uniform
continuity. So f(x) = x2 is continuous but not uniformly continuous on the set of all real numbers.
Example 1.7. Consider another common example of a function f(x) = sin(x2) on the interval [0,∞)
that is continuous but not uniformly continuous.
Solution: This function is continuous on the interval [0,∞). This is because the sine function is
continuous for all real numbers and the composition of continuous functions is also continuous.
However, f(x) = sin(x2) is not uniformly continuous on [0,∞). To check this, consider two se￾quences of points, xn = √2πn and yn = 2πn+ π
2 , where n ≥ 0. The distance between xn and yn
is |xn −yn| = 2πn+ π
2 − √2πn, which tends to 0 as n approaches infinity. However, the distance
between the function values at these points, | f(xn) − f(yn)| = |sin((2πn)2) − sin((2πn + π
2 )2)| =
|0−1| = 1, which does not tend to 0 as n approaches infinity. Therefore, there is no δ > 0 such that
|x−y| < δ implies | f(x)− f(y)| < ε for all x, y in [0,∞) and all ε > 0. This shows that the function
is not uniformly continuous on the interval [0,∞).
Definition 1.7. (Absolutely Continuous) A function ψ : [a,b] → Rn is absolutely continuous if, for
all ε > 0, there exists δ > 0 such that, for each finite collection {(a1,b2),...,(an,bn)} of disjoint
open intervals contained in [a,b] with ∑n
i=1(bi −ai) < δ, it follows that
n
∑
i=1
|ψ(bi)−ψ(ai)| < ε.
Definition 1.8. (Lipschitz Continuity) A function f : I ×D → Rn, where I = [a,b], is called Lipschitz
continuous over D, uniformly in I, if ∃ L > 0 such that
∀x,y ∈ D and ∀t ∈ I, f(t,x)− f(t,y) ≤ Lx−y.
Definition 1.9. (Locally Lipschitz Continuity) A function f : I ×D → Rn, where I = [a,b], is called
locally Lipschitz continuous in x over D, uniformly in t, if ∃ L > 0 and δ > 0 such that
∀z, y ∈ B(x,δ) and ∀t ∈ I, f(t,z)− f(t,y) ≤ Lz−y
Definition 1.10. (Global Lipschitz Continuity) A function f : I ×D → Rn, where I = [a,b], is called
globally Lipschitz continuous in x over D, uniformly in t, if it is Lipschiz continuous in x over D
uniformly in t and D = Rn.14 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Definition 1.11. (Piecewise Continuity) A function f : I ×D → Rn is piecewise continuous in t for
all x ∈ D ⊂ Rn if for every fixed x ∈ D ⊂ Rn and every bounded interval J ⊂ I the function t → f(t, x)
is continuous on J at all but a finite number of points, and at every point of discontinuity td, the left
and right-sided limits, i.e., limh↓td f(t
d +h, x) and limh↑td f(t
d −h, x), exists and are finite.
Function Norms Given a function f(t) : [0,∞) → Rn, its Lp (function) norm is given in terms of
the vector norm  f(t)p at each value of t by
 f(·)p =
 ∞
0
 f(t)p
pdt1
p
,
and for p = ∞, i.e., L∞ norm, is given by
 f(·)∞ = sup
t≥0
 f(t)∞.
If the Lp norm exists, we say f(t) ∈ Lp. Note that a function is in L∞ if and only if it is bounded.
If we have a function defined over the positive intergers including zero, i.e., Z+ = {0,1,2,...}
and f(k) : Z+ → Rn, then the p (function) norm is given in terms of the vector  f(k)p at each
value of k by
 f(·)p =
 ∞
∑
k=0
 f(k)p
p
1
p
,
and if p = ∞, i.e., ∞ norm, given by
 f(.)∞ = sup
k
 f(k)∞.
If the p norm is finite, we say f(k) ∈ p. Note that a function is in ∞ if and only if it is bounded.
1.3 STABILITY
In this section, we will review various stability concepts used in the book for developing event￾triggered, adaptive, and neural network control for linear and nonlinear systems. In particular, we
focus on stability concepts based on Lypunov stability theory, such as local and global Lyapunov
stability, local and global asymptotic stability, input-to-state stability, uniformly ultimately bounded
stability, and L2 stability. More details on these stability concepts can be found in Khalil (2002).
1.3.1 EQUILIBRIUM POINT
A point x∗ is called an equilibrium point of
x˙(t) = f(x(t)) (1.21)
implies that if x(τ) = x∗ for some τ then x(t) = x∗ for all t ≥ τ, i.e., ˙x(t) = 0. In other words, if the
state of a system starts at x∗, it will remain at x∗. For an autonomous system (1.21), the equilibrium
points are the real roots of the equation
f(x) = 0.
Example 1.8. Find the equilibrium points of the linear time-invariant system
x˙(t) = Ax(t). (1.22)
Solution:Background and Introduction to Event-Triggered Control 15
1. It has a single equilibrium point (the origin, 0) if A is nonsingular;
2. if A is singular, it has infinitely many equilibrium points, which are contained in the null space
of the matrix.
A linear system can not have multiple isolated equilibrium points.
Example 1.9. For the system
x¨(t) +x˙(t) = 0, (1.23)
find the equilibrium points.
Solution: In state space representation with x1(t) = x(t) and x2(t) = x˙(t)
x˙(t) = 
0 1
0 −1

x(t).
Alternatively, we can write each state equation as
x˙1(t) = x2(t),
x˙2(t) = −x2(t).
Therefore, it can be concluded that x2 = 0 for any x1 is an equilibrium point. The equilibrium points
are not isolated.
Example 1.10. Consider the dynamics of a pendulum given by a nonlinear autonomous equation
MR2θ¨(t) +bθ˙(t) + MgRsinθ(t) = 0,
where R is the pendulum length, M its mass, b the friction coefficient at the hinge, and g is the
gravity constant. Compute the equilibrium points.
Solution: A state space representation for this system is given by
x˙1(t) = x2(t)
x˙2(t) = − b
MR2 x2(t)− g
R
sinx1(t).
Therefore, the equilibrium points can be obtained as
x2 = 0 and sinx1 = 0,
i.e., the points (nπ,0) with n = 0,±1,±2,···.
1.3.2 STABILITY DEFINITIONS
In this section, several definitions characterizing the stability of equilibrium points of a system are
introduced.
Definition 1.12. Let the origin be the equilibrium point of the system x˙(t) = f(x(t)), where f(x) is
locally Lipschitz over the domain D ⊂ Rn that contains the origin. Then equilibrium point x∗ = 0
is locally stable if, for each ε > 0, there exists a δ > 0, such that if x0 < δ, then x(t) < ε for
all t ≥ 0. Unstable, if the equilibrium point is not stable. Locally asymptotically stable if it is stable
and ∃δ > 0 such that x0 < δ implies that x(t) exists for all t ≥ 0 and limt→∞ x(t) = 0.
Definition 1.13. (Globally Asymptotically Stable) An equilibrium point of the dynamical system
x˙(t) = f(x(t)) is globally asymptotically stable if it is asymptotically stable and its domain of at￾traction is Rn.16 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Definition 1.14. (Exponential Stability) The equilibrium point x = 0 is said to be exponentially
stable if there exist three strictly positive numbers c, α, and λ such that for x0 < c
x(t) ≤ αx0e−λt
for all t ∈ R≥0. It is said to be globally exponentially stable if the above inequality holds for x0 ∈ Rn.
1.3.3 LYAPUNOV THEOREMS FOR STABILITY
1.3.3.1 Lyapunov Stability Theory for Continuous-Time Systems
We shall begin with some definitions.
Definition 1.15. (Positive and Negative Definite) A function V : D → R is positive definite if
V(0) = 0 and V(x) > 0 in D\ {0}.
The function is negative definite if −V(x) is positive definite.
Definition 1.16. (Positive and Negative Semidefinite) A function V : D → R is positive (negative)
semidefinite if
V(0) = 0 and V(x) ≥ 0 (V(x) ≤ 0) in D\ {0}.
Definition 1.17. (Radialy Unbounded) A function V : Rn → R is radially unbounded if it satisfies
the following condition:
V(x) → ∞ as x → ∞.
Definition 1.18. (Invariant Set) A set M ⊂ Rn is invariant with respect to x˙(t) = f(x(t)) with x0 ∈ M
implies that the solution x(t) = φ(t, x0) ∈ M for every t ∈ R.
Definition 1.19. (Positively Invariant Set) A set M ⊂ Rn is forward (or positively) invariant with
respect to x˙(t) = f(x(t)) of x0 ∈ M implies that the solution x(t) = φ(t, x0) ∈ M for every positive
interval of existence of φ.
Now we are ready to review the Lyapunov theorems to assess the stability of an equilibrium point
directly without solving for the system dynamics. A detailed proof of the theorems can be found in
(Khalil, 2002).
Theorem 1.1. Let x = 0 be an equilibrium point for the system x˙(t) = f(x(t)), where f is locally
Lipschitz continuous, and D ⊂ Rn contains the origin. Let V : D → R be a continuously differentiable
function and positive definite, Then, the following statements are true:
1. If V˙(x(t)) = ∂V(x)T
∂ x f(x) ≤ 0,∀x ∈ D, then equilibirum point x = 0 is stable.
2. If V ∈ C1(D) and V˙(x(t)) = ∂V(x)T
∂ x f(x) < 0,∀x ∈ D, then equilibrium point x = 0 is asymptoti￾cally stable, where C1 is the set of continuous functions with continuous first derivatives.
Theorem 1.2. Let x = 0 be an equilibrium point for the system x˙(t) = f(x(t)), where f is locally
Lipschitz continuous. Let V : Rn → R be a continuously differentiable function and radially un￾bounded. If V˙(x(t)) = ∂V(x)T
∂ x f(x) < 0,∀x ∈ Rn \ {0}. Then, the equilibrium point x = 0 is globally
asymptotically stable (GAS).Background and Introduction to Event-Triggered Control 17
Theorem 1.3. (Lyapunov’s Indirect Theorem) Let x = 0 be an equilibrium point of the nonlinear
system x˙(t) = f(x(t)), where f : D → Rn is continuously differentiable, f(0) = 0 and D is a neigh￾borhood of the origin (0 ∈ D). Let
A = ∂ f
∂ x
(x)




x=0
is Hurwitz. Then, the origin is locally asymptotically (exponentially) stable.
Example 1.11. For the system given below
x˙1(t) = −x3
1(t) +x2(t)
x˙2(t) = −x1(t)−x2(t), (1.24)
find if the eqilliborum point x = 0 is stable or unstable. Comment on the type of stability.
Solution: Choose a Lyapunov function candidate
V(x) = 1
2
(x2
1 +x2
2).
Note that the function V(x) is positive definite. The first derivative can be computed as
V˙ (x) = x1x˙1 +x2x˙2 = x1

−x3
1 +x2

+x2 (−x1 −x2)
= −x4
1 +x1x2 −x2x1 −x2
2 = −x4
1 −x2
2 = α(x).
One needs to check for the following conditions to determine the stability of the equilibrium point
x = 0. If
α(x) = −x4
1 −x2
2 ≤ 0 (PSD) ⇒ x = 0 is stable.
α(x) = −x4
1 −x2
2 < 0 (PD) ⇒ x = 0 is AS.
α(x) = −x4
1 −x2
2 < 0 (PD) and V(x) → ∞ as x → ∞ (radially unbounded) ⇒ x = 0 is GAS.
To check the radial unboundedness of V(x), we have
V(x) = 1
2
(x2
1 +x2
2) → ∞ as x → ∞.
From the above, V(x) is radially unbounded, and V˙(x) is PD. Therefore, the equilibrium point x = 0
is globally asymptotically stable (GAS).
A nonautonomous system is represented in the form
x˙ = f(t, x).
The vector field is time-varying. In the time-invariant case, we saw the solution of ˙x = f(x) starting
from any time t0 only depends on t −t0. Without loss of generality, we assumed t0 = 0. In nonau￾tonomous cases, when the vector field depends on time, then the solution depends on t0 in addition
to t −t0. Therefore, the concept of stability will also have some dependence on t0.
Definition 1.20. (Khalil (2002)) An equilibrium point x = 0 of x˙(t) = f(t, x) is said to be
1. locally stable if and only if ∀ε > 0 and ∀t0 ∈ R≥0, ∃δ(t0, ε) > 0 such that ∀t > t0, x0 < δ =⇒
the solutionφ(t,t0, x0) < ε;
2. locally uniformly stable if and only if ∀ε > 0, ∃δ(ε) > 0 (δ is independent of t0) such that
∀t0 ∈ R≥0 and ∀t > t0, x0 < δ =⇒ φ(t,t0, x0) < ε;18 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
3. uniformly globally stable if and only if it is uniformly stable and δ can be selected such that
limε→∞ δ(ε) = ∞;
4. locally asymptotically stable (AS) if and only if it is stable and for all t0 ∈ R≥0, ∃c = c(t0) > 0
a constant such that ∀ε > 0, ∃T(c, ε,t0) ≥ 0 such that x0 < c =⇒ φ(t,t0,x0) < ε for
t ≥ t0 +T ;
5. locally uniformly asymptotically stable (UAS) if and only if it is uniformly stable and ∃c > 0, c
is independent of t0, such that ∀ε > 0, ∃T(c, ε) ≥ 0, T is independent of t0, such that ∀t0 ∈ R≥0,
x0 < c =⇒ φ(t,t0,x0) < ε for t ≥ t0 +T ;
6. globally asymptotically stable (GAS) if and only if it is asymptotically stable for all t0 ∈ R≥0
c > 0, ε > 0, ∃T(c, ε,t0) ≥ 0 such that x0 < c =⇒ φ(t,t0,x0) < ε for t ≥ t0 +T .
7. uniformly globally asymptotically atable (UGAS) if and only if it is uniformly globally sta￾ble and for all c > 0 and ε > 0, ∃T(c, ε) ≥ 0, T independent of t0, such that x0 < c =⇒
φ(t,t0, x0) < ε for t ≥ t0 +T .
Definition 1.21. (Class K and K∞ Functions) A function α : [0,a) → R is said to be belong to
1. class K (α ∈ K ([0,a),R)) if and only if it is strictly increasing and α(0) = 0, and
2. class K∞ if and only if α ∈ K ([0,a),R), with a = ∞, and α(r) → ∞ as r → ∞.
Definition 1.22. (Class K L Functions) A function β : [0,a)×R≥0 → R is said to be class K L
(β ∈ K L ([0,a) × R≥0,R)) if and only if for every fixed s, β(r,s) ∈ K ([0,a),R) and for every
fixed r, β(r,s) is nonincreasing with lims→∞ β(r,s) = 0.
Definition 1.23. (Khalil (2002)) A system x˙(t) = f(t,x,u) is said to be input-to-state stable if there
exist a class K L function β and a class K function γ such that for any initial state x0 and any
bounded input u(t), the solution x(t) exists for all t ≥ t0 and satisfies
x(t) ≤ β(x(t0),t −t0) +γ

sup
t0≤τ≤t
u(τ)

.
We shall now look at some of the definitions concerning the stability of nonautonomous systems.
Theorem 1.4. Let the origin x = 0 be an equilibrium point of the system x˙(t) = f(t, x) and D ⊂ Rn
be a domain (open and connected set) containing x = 0. Suppose f(t, x) is piecewise continuous in
t and locally Lipschitz in x for all t ≥ 0 and x(t) ∈ D. Let V(t, x) be a continuously differentiable
function such that
W1(x) ≤ V(t, x) ≤ W2(x)
∂V
∂t
+
∂V
∂ x f(t,x) ≤ 0
for all t ≥ 0 and x ∈ D, where W1(x) and W2(x) are continuous positive definite functions on D.
Then, the origin is uniformly stable.
Theorem 1.5. Let the hypothesis of Theorem 1.4 be satisfied
1. If a stronger inequality
∂V
∂t
+
∂V
∂ x f(t, x) ≤ −W3(x)
for all t ≥ 0 and x ∈ D, where W3(x) is continuous positive definite function on D. Then, the
origin is uniformly asymptotically stable (UAS).Background and Introduction to Event-Triggered Control 19
Theorem 1.6. (Khalil, 2002) (Input-to-State Stability Let V : [0,∞) × Rn → R be continuosly dif￾ferentiable function such that
α1(x) ≤ V(t, x) ≤ α2(x)
∂V
∂t
+
∂V
∂ x f(t, x) ≤ −W3(x), ∀x ≥ ρ(u) ≥ 0
for all (t,x,u) ∈ [0,∞)×Rn ×Rm, where α1 and α2 are class K∞ functions, ρ is a class K function
and W3(x) is a continuous positive definite function on Rn. Then the system x˙(t) = f(t, x,u) is input￾to-state stable (ISS) with γ = α−1
1 ◦α2 ◦ ρ.
Example 1.12. (Khalil, 2002) The system
x˙(t) = −x3(t) +u(t)
has a globally asymptotically stable origin when u = 0. Show that it is ISS.
Soluton: Select a Lyapunov function candidate V = 1
2 x2, the derivative of V along the trajectory
of the system is given by
V˙(t) = −x4 +xu = −(1−θ)x4 −θx4 +xu
≤ −(1−θ)x4,∀|x| ≥ |u|
θ
1
3
,
where 0 < θ < 1. Thus, the system is ISS.
Definition 1.24. (Boundedness) Let f be piecewise continuous in t and locally Lipschitz in x for
all t ≥ 0 and x(t) ∈ D for some domain D ⊂ Rn that contains the origin, then the solutions of the
system x˙(t) = f(t,x) with x(t0) = x0 are
1. uniformly bounded if there exists c > 0, independent of t0, and for every a ∈ (0, c), there is β > 0,
dependent on a but independent of t0, such that
x(t0) ≤ a =⇒ x(t) ≤ β,∀t ≥ t0
2. globally uniformly bounded if the above condition holds for arbitrary large a.
3. uniformly ultimately bounded with ultimate bound b if there exists a positive constant c, inde￾pendent of t0, and for every a ∈ (0,c), there is T ≥ 0, dependent on a and b, but independent of
t0, such that
x(t0) ≤ a =⇒ x(t) ≤ b,∀t ≥ t0 +T
4. globally uniformly ultimately bounded if the above inequality holds for arbitrary large a.
Theorem 1.7. (Boundedness and Ultimate Boundedness) Let D ⊂ Rn be a domain that contains the
origin and V : [0,∞)×D → R be continuously differentiable function such that
α1(x) ≤ V(t, x) ≤ α2(x)
for all t ≥ 0 and ∀x ∈ D, where α1 and α2 are class K functions and W3(x) is a continuous positive
definite function. Take r ≥ 0 such that Br ⊂ D and suppose that
μ < α−1
2 (α1(r)).
Then there exists a class K L function β for every initial state x(t0), satisfying x(t0) ≤
α−1
2 (α1(r)), there is T ≥ 0, such that the solution of x˙(t) = f(t, x) satisfies
x(t) ≤ β(x(t0),t −to), ∀t0 ≤ t ≤ t0 +T
x(t) ≤ α−1
1 (α2(μ)), ∀t ≥ t0 +T.
Moreover, if D = Rn and α1 belongs to class K∞, then the above inequalities hold for any initial
state x(t0), with no restriction on how large μ is.20 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
1.3.3.2 Lyapunov Stability Theory for Discrete-time Systems
Consider a nonlinear autonomous discrete-time system given by
xk+1 = f(xk) (1.25)
where f : D → D is a nonlinear function with D ⊆ Rn. Analogous to the equilibrium point defined for
a continuous time system, a fixed-point x∗ for the discrete-time system is one where xk+1 = xk = x∗.
In other words, for a given κ ∈ Z if xκ = x∗, then xκ+1 = f(xκ ) = f(x∗) = x∗.
The following theorem guarantees the stability of the system.
Theorem 1.8. Let x = 0 be a fixed point of the time-invariant autonomous discrete-time system in
(1.25), where f : D → D is locally Lipschitz with f(0) = 0 and D ⊆ Rn contains the origin. Let
V : D → R satisfies
V(0) = 0 and V(x(k)) > 0,∀x(k) ∈ D\ {0},
then
1. if ΔV(xk) ≤ 0,∀xk ∈ D implies x = 0 is stable,
2. if x = 0 is stable and ΔV(xk) < 0,∀x ∈ D\ {0} implies x = 0 is locally asymptotically stable.
3. if x = 0 is stable and ΔV(xk) < 0,∀x ∈ D\ {0}, D = Rn and V(xk) is radially unbounded V(xk) →
∞ as xk → ∞ implies x = 0 is globally asymptotically stable.
Here ΔV(xk) is the first difference operator, i.e., ΔV(xk) = V(xk+1)−V(xk).
Definition 1.25. A function V : D → R satisfying the condition of positive definiteness, i.e.,
V(0) = 0 and v(x) > 0,∀x ∈ D\ 0
and
ΔV(xk) ≤ 0,∀x ∈ D
is called a Lyapunov function for the system (1.25).
Definition 1.26. (Jiang and Wang, 2001) The control system
xk+1 = f(xk,uk) (1.26)
is input-to-state stable (ISS) if there exists a K L -function β : R≥0×R≥0 → R≥0 and a K -function
γ such that, for each u ∈ m
∞ and each ξ ∈ Rn, it holds that
|x(k,ξ ,u)| ≤ β(|ξ |, k) +γ(u) (1.27)
for each k ∈ Z+.
Definition 1.27. (Jiang and Wang, 2001) A continuous function V : Rn → R≥0 is called an ISS￾Lyapunov function for the system (1.26) if the following holds:
1. There exists a K∞- functions α1,α2 such that
α1(|ξ |) ≤ V(ξ ) ≤ α2(|ξ |), ∀ξ ∈ Rn. (1.28)
2. There exists a K∞-function α3 and a K -function σ, such that
V(f(ξ ,μ))−V(ξ ) ≤ −α3(|ξ |) +σ(|μ|), ∀ξ ∈ Rn,∀μ ∈ Rm. (1.29)
Theorem 1.9. Consider system (1.26). The following are equivalent:Background and Introduction to Event-Triggered Control 21
1. It is ISS.
2. It admits a smooth ISS-Lyapunov function.
Theorem 1.10. The fixed point x = 0 of the linear time-invariant discrete-time system
xk+1 = Axk (1.30)
where A ∈ Rn×n is
1. stable iff all the eigenvalues of A defined as λ1,...,λn satisfy |λi| ≤ 1 for i = 1,...,n, and the
algebraic and geometric multiplicity of the eigenvalues with absolute value 1 coincide,
2. globally asymptotically stable iff all the eigenvalue of A are such that |λi| < 1.
Remark 1.1. A matrix A with all the eigenvalues with an absolute value less than 1 is called a
Schur Matrix.
Lyapunov function for linear systems. Consider a Lyapunov function candidate V : Rn → R given
by
V(xk) = xT
k Pxk,
where P ∈ Rn×n is a symmetric positive definite matrix. The first difference
ΔV(xk) = V(xk+1)−V(xk).
Along the system dynamics (1.30), the first derivative
ΔV(xk) = xT
k+1Pxk+1 −xT
k Pxk
= (Axk)
TPxk −xT
k Pxk
= xT
k ATPAxk −xT
k Pxk
= xT
k (ATPA−P)xk = −xT
k Qxk
For the Lyapunov function candidate
V(xk) = xT
k Pxk,
where P ∈ Rn×n is a symmetric positive definite matrix, the first difference
ΔV(xk) = −xT
k Qxk
1. If Q ≤ 0 (PSD), the fixed point x = 0 is stable.
2. If Q < 0 (PD), the fixed point x = 0 is asymptotically stable.
Theorem 1.11. A matrix A is Shur iff for any Q = QT > 0 there exists a unique matrix P = PT > 0
that satisfies
ATPA−P = −Q.
The proof for the theorem can be found in Ogata (2011).
1.3.3.3 Lyapunov Stability Theory for Impulsive Hybrid Systems
Recall the dynamics of an impulsive hybrid system defined in (1.19). A function x : Ix0 → C is a
solution to the impulsive dynamical system (1.19) on the interval Ix0 with initial condition x(0) = x0,
if x(·) is left-continuous and x(t) satisfies (1.19) for all t ∈ Ix0 . In addition, we use the notation
φ(t, τ,x0) to denote the solution x(t) of (1.19) at time t ≥ τ with initial condition x(τ) = x0. Finally,
a point xe ∈ C is an equilibrium point of (1.19) if and only if φ(t, τ, xe) = xe for all τ ≥ 0 and t ≥ τ.
Note that xe ∈ C is an equilibrium point of (1.19) if and only if f(xe) = 0 and F(xe) = xe.22 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Definition 1.28. (Haddad et al., 2006) The nonlinear impulsive dynamical system (1.19) is locally
bounded if there exists a γ > 0 such that, for every δ ∈ (0, γ), there exists ε = ε(δ) > 0 such that
x(0) < δ implies x (t) < ε, t ≥ 0.
Definition 1.29. (Haddad et al., 2006) The nonlinear state-dependent impulsive dynamical system
(1.19) is UB with bound ε if there exists γ > 0 such that, for every δ ∈ (0, γ), there exists T =
T(δ, ε) > 0 such that ξ (0) < δ implies ξ (t) < ε, for t ≥ T , and globally UB with bound ε if,
for every δ ∈ (0,∞), there exists, T = T(δ, ε) > 0, such that x(0) < δ implies x(t) < ε, t ≥ T .
The following theorem will be used to prove the ultimate boundedness of the impulsive dynami￾cal systems.
Theorem 1.12. (Haddad et al., 2006) Consider the impulsive dynamical system (1.19). Assume
that the jumps occur at distinct time instants and there exists a continuously differentiable function
V : C → R and class K functions α () and β () such that
α (x) ≤ V (x) ≤ β (x), x ∈ C, (1.31)
∂V (x)
∂ x f (x,u) < 0, x ∈ C, x ∈/ Z , x ≥ χ , (1.32)
V (F (x))−V (x) ≤ 0, x ∈ C, x ∈ Z , x ≥ χ , (1.33)
where χ > 0 is such that Bα−1(β(χ))(0) = {ξ ∈ Rn : x <α−1 (β (χ))
⊂ C with η > β (χ). Fur￾ther, assume θ  sup
ξ∈B¯χ (0)∩Z
V (F (x)) exists. Then the nonlinear state-dependent impulsive dynami￾cal system (1.19) is UB with bound Ξ  α−1 (η), where η  max{β (χ),θ}. The sets Bα−1(β(χ))(0)
and B¯χ (0) denotes an open ball centered at 0 with radius α−1(β(χ)) and a closed ball centered at
0 with radius χ.
So far, we have explored essential mathematical tools and theorems relevant to the design and
analysis of closed-loop control systems within the event-triggered control framework. We shall now
explore event-triggered control and review traditional event-triggered control techniques.
1.4 EVENT-TRIGGERED CONTROL SYSTEMS
The advent of embedded processors spurred research on the digital implementation of controllers,
leading to the development of foundational theories of sampled data and discrete-time control sys￾tems. In the sampled-data systems approach, a discrete-time controller controls a continuous-time
plant, whereas in a discrete-time control system, the system and controllers operate in discrete-time
(Kuo, 2012; Ogata, 2011). In both schemes, a periodic, fixed sampling time is determined a priori
and used to sample the feedback signals and execute the controller. This fixed sampling time is
generally governed by the well-known Nyquist sampling criterion, which considers the worst-case
scenario. However, this periodic sampling scheme leads to ineffective resource utilization (Astrom
and Bernhardsson, 2002; Tabuada, 2007; Dong et al., 2017) and increases the computational burden
on the controller. The problem aggravates in the case of systems with shared digital communica￾tion networks in the feedback loop, referred to as networked control systems (NCS) (Shousong and
Qixin, 2003; Walsh et al., 2002; Liou and Ray, 1991), due to limited bandwidth. In this case, peri￾odic sampling and transmission exacerbates the problem of network congestion, leading to longer
network-induced delays.Background and Introduction to Event-Triggered Control 23
Furthermore, the periodic sampling and transmission of feedback data and control execution can
be redundant when there is no significant change in the system performance, and when the sys￾tem is operating within a desired operating envelope. As an alternative, various sampling schemes
were proposed to alleviate the burden of needless computational load and network congestion (El￾lis, 1959; Dorf et al., 1962; Phillips and Tomizuka, 1995; Hristu-Varsakelis and Kumar, 2002; Rabi
and Baras, 2007). In recent times, performance-based sampling schemes have been developed to re￾duce the computational cost and are formally referred to as “event-triggered control” (Arzen, 1999;
Tabuada, 2007; Mazo and Tabuada, 2011; Heemels et al., 2008; Lunze and Lehmann, 2010; Tal￾lapragada and Chopra, 2013; Heemels et al., 2012; Tallapragada and Chopra, 2012; Cogill, 2009;
Wang and Lemmon, 2008; Donkers and Heemels, 2012; Heemels and Donkers, 2013; Garcia and
Antsaklis, 2012; Eqtami et al., 2010; Stocker and Lunze, 2011; Mazo Jr. and Cao, 2011; Mazo and
Tabuada, 2011). In this sampling scheme, the determination to transmit and execute control updates
takes place when a significant alteration in the system state or output errors is identified. These al￾terations have the potential to jeopardize stability or diminish the desired performance. Therefore,
the control is updated using the latest state or output information. To facilitate this sampling and
transmission processes, an additional hardware device, referred to as the event-triggering mecha￾nism, has been developed. This mechanism evaluates event-triggering conditions to orchestrate the
timing of sampling instants or the events. Since the objective of this sampling is to facilitate con￾troller execution and not signal reconstruction, this is equally applicable for both continuous (Wang
and Lemmon, 2008; Tabuada, 2007; Mazo and Tabuada, 2011; Heemels et al., 2008; Lunze and
Lehmann, 2010; Tallapragada and Chopra, 2013; Heemels et al., 2012; Cogill, 2009; Donkers and
Heemels, 2012) and discrete-time systems (Heemels and Donkers, 2013; Eqtami et al., 2010) to
either regulate the system (Wang and Lemmon, 2008; Mazo and Tabuada, 2011; Heemels et al.,
2008) or enable the system state to track a desired trajectory (Tallapragada and Chopra, 2013).
In the case of a continuous-time system, the sensor measures the system state or output vec￾tors continuously, and the triggering mechanism determines the sampling instants by evaluating the
event-triggering condition (Wang and Lemmon, 2008; Mazo and Tabuada, 2011; Heemels et al.,
2008; Cogill, 2009; Lunze and Lehmann, 2010). The event-triggering condition is usually defined
using a state or output error function, referred to as the event-triggering error, and a suitably de￾signed state-dependent threshold (Tabuada, 2007). The feedback signals are transmitted and the
control is executed when the event-triggering error breaches the threshold. Typically, stability tech￾niques (e.g., Lyapunov analysis, small-gain theorem, etc.) are used to design the event-triggering
condition. In between events, various techniques are used to preserve some form of continuity in
the feedback signal and the control signal. We shall see some of these methods later in this chapter.
Similarly, in the discrete-time case (Heemels and Donkers, 2013; Eqtami et al., 2010), the sensor
measures the system state or output, and the triggering mechanism evaluates the event-triggering
condition periodically, and a decision is made whether to transmit or not. In both continuous and
discrete-time cases, the event-triggering instants turn out to be aperiodic and, hence, save computa￾tional load and bandwidth usage. These inherent advantages of event-triggered control are proven
to be more beneficial in large-scale systems such as decentralized systems (Mazo Jr. and Cao, 2011;
Mazo and Tabuada, 2011; Tabuada, 2007), distributed, multi-agent systems (Mazo Jr. and Dimarog￾onas, 2010), and cyber-physical systems (Mazo Jr. and Dimarogonas, 2010).
A related approach called self-triggered control (Wang and Lemmon, 2009b; Mazo Jr. and Di￾marogonas, 2010; Sahoo et al., 2018; Gommans et al., 2014) is also developed for systems where the
extra hardware for realizing the trigger mechanism can be obviated. This software-based scheme,
a special case of the event-triggered control, predicts the sampling instants by using the previously
sampled data and the system’s dynamics, i.e., using the complete model of the system. Hence, a
continuous evaluation of the event-triggering condition is not necessary. For more information, see
(Wang and Lemmon, 2009b; Mazo Jr. and Dimarogonas, 2010; Sahoo et al., 2018; Gommans et al.,
2014) .24 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
1.4.1 BACKGROUND AND HISTORY
The study of aperiodic sampling for sampled-data control dates back to the late fifties and early six￾ties (Ellis, 1959; Dorf et al., 1962; Phillips and Tomizuka, 1995; Hristu-Varsakelis and Kumar, 2002;
Rabi and Baras, 2007) and was first studied in (Ellis, 1959) for quantized systems to share the com￾munication channel without increasing its bandwidth. Moreover, a state-based adaptive sampling
method for sampled data servo-mechanisms was proposed by Dorf et al. (1962), where the abso￾lute value of the first derivative of the error signal controls the adaptive sampling rate. Later, this
aperiodic state-dependent sampling was studied under various names, such as multi-rate sampling
(Phillips and Tomizuka, 1995), interrupt-driven triggering (Hristu-Varsakelis and Kumar, 2002), and
level-triggered sampling (Rabi and Baras, 2007). In the last two decades, this scheme has been stud￾ied under the name of event-triggered sampling (Tabuada, 2007). Various theoretical (Astrom and
Bernhardsson, 2002; Tabuada, 2007) and experimental (Arzen, 1999; Heemels et al., 2008) results
emphasizing its inherent advantages in computation and communication-saving have been reported
in the literature. In the last few years, theoretical results started to appear in the literature for both
deterministic (Tabuada, 2007; Mazo and Tabuada, 2011; Heemels et al., 2008; Lunze and Lehmann,
2010; Tallapragada and Chopra, 2013; Heemels et al., 2012; Tallapragada and Chopra, 2014; Cogill,
2009; Wang and Lemmon, 2008; Donkers and Heemels, 2012; Heemels and Donkers, 2013; Garcia
and Antsaklis, 2012) and stochastic (Henningsson et al., 2008; Astrom and Bernhardsson, 2002; Sa￾hoo and Jagannathan, 2016) event-triggered control. Thereafter, various controller design strategies
incorporating event-triggered feedback were introduced.
Emulation-based approach (Tabuada, 2007; Tallapragada and Chopra, 2012) is one of the design
strategies used for the event-triggered system design. In the emulation-based design, the continuous
controller is presumed to be stabilizing and an event-triggering condition is developed to aperiod￾ically update the controller to maintain stability and a certain level of performance. In the earlier
works (Tabuada, 2007; Mazo and Tabuada, 2011), the system was assumed to be ISS (Khalil, 2002)
with respect to the measurement error, and event-triggering conditions were designed to reduce
computation and guarantee asymptotic stability. This assumption primarily serves to preclude sys￾tem instability in the inter-event time, ensuring that the states do not grow unbounded with a finite
escape time. A non-zero positive lower bound on the inter-event times was also guaranteed to avoid
accumulation of events leading to Zeno behavior. The event-triggered control approach was also ex￾tended to accommodate other design considerations, such as output feedback design (Tallapragada
and Chopra, 2012; Donkers and Heemels, 2010), decentralized designs (Si et al., 2004; Lehmann
and Lunze, 2011), and trajectory tracking control (Hu and Yue, 2012).
The event-triggered control approach was also studied in the context of discrete-time systems
(Heemels and Donkers, 2013; Garcia and Antsaklis, 2012; Eqtami et al., 2010), where the sen￾sor senses the system state periodically in a time-triggered approach and the transmission of the
feedback signals and controller executions are carried out at the event-triggering instants. In the
discrete-time event-triggered control, the minimum inter-event time is the periodic sampling inter￾val of the discrete-time system (Lehmann and Lunze, 2011). Similar to the event-triggered control
in a discrete-time domain, a periodic event-triggered control approach for continuous-time systems
was presented by Donkers and Heemels (2010), where the sensor sampled periodically and the
events were determined as in the case of discrete-time case (Eqtami et al., 2010). The triggering
condition was evaluated periodically with a fixed sampling interval, and the transmission decision
was made at the violation of a trigger condition. This design framework enforces a positive lower
bound on the minimum inter-event times. The stability analysis was carried out using three different
modeling techniques for hybrid systems: impulsive systems (Haddad et al., 2006), piecewise linear
systems, and perturbed linear systems. In all the above design approaches, the system state or out￾put and the control input are held between two consecutive events by a zero-order hold (ZOH) for
implementation.
In the second event-triggered control design strategy (Lunze and Lehmann, 2010; Garcia andBackground and Introduction to Event-Triggered Control 25
Antsaklis, 2013), a model of the system is used to reconstruct the system state vector and, subse￾quently, used for designing the control input. As the control input is based on the model states, no
feedback transmission is required unless there is a significant change in the system performance due
to external disturbance or internal parameter variation. In the area of model-based event-triggered
control design, Lunze and Lehmann (2010) used an input generator as a model to predict the system
state and compute the control. Further, Garcia and Antsaklis (2013) considered the nominal dynam￾ics of the system with uncertainty, usually of smaller magnitude and bounded, to form a model. The
asymptotic stability was guaranteed by designing the event-triggering condition. A discrete-time
model-based approach was also presented by Heemels and Donkers (2013) for systems subjected
to disturbance. Two modeling approaches (perturbed linear and piecewise linear system) were used
to analyze the stability and sufficient conditions for global exponential stability were derived in
terms of linear matrix inequalities (LMI). It is observed that the model-based approach reduces the
number of events or transmissions more effectively when compared to the ZOH-based approach but
with a higher computational load due to the induction of the model.
The event-triggered control scheme was also extended to NCS with inherent network constraints
(Wang and Lemmon, 2011b; Hu and Yue, 2012; Wang and Lemmon, 2009a) such as constant or
time-varying delays, packet losses, and quantization errors (Sarangapani and Xu, 2018). In these
design approaches, the event-triggering condition was tailored (Wang and Lemmon, 2011b, 2009a)
to handle the maximum allowable delays, packet losses (Wang and Lemmon, 2009a), and quantiza￾tion error for both state and control input (Hu and Yue, 2012) so as to ensure stability. Furthermore,
information-theoretic perspective of event-triggered control techniques was explored in a series of
efforts (Tallapragada and Cortes, 2015; Khojasteh et al., 2019). Event-triggered control was also ap- ´
plied in the context of optimal control, both for deterministic and stochastic systems (Cogill, 2009;
Molin and Hirche, 2013; Rabi et al., 2008; Imer and Basar, 2006). Molin and Hirche (2013) ex￾tended the optimal control designs to the event-triggered control framework and characterized the
certainty equivalence principle-based controller as optimal in a linear quadratic Gaussian (LQG)
framework. The optimal control input and the optimal event-triggering instants were designed us￾ing the separation principle. The optimal solution of the event-based control (Molin and Hirche,
2013) used the system dynamics to solve the Riccati equation backward-in-time. Later, the optimal
event-triggered control for stochastic continuous-time NCS was formulated as an optimal stopping
problem (Rabi et al., 2008), and an analytical solution was obtained. The optimal control in a con￾strained networked environment was also studied by researchers (Imer and Basar, 2006). Learning
and adaptation with sparse, aperiodic feedback data are challenging tasks due to the need for sta￾bilization under system uncertainty, dependence on adaptation rules tied to aperiodicity, and the
impact of channel losses. Consequently, two primary challenges have emerged, leading to efforts
that prioritize stability without emphasizing the convergence of parameter errors (Karafyllis and
Krstic, 2018; Karafyllis et al., 2019). Alternatively, there are efforts that opt to sacrifice feedback
sparsity to facilitate both learning and system stabilization (Sahoo, 2015). In the later chapters, we
will explore data-driven optimal control strategies that enable learning and stabilization at the ex￾pense of frequent triggering. Additionally, we will observe that a better trade-off between learning
and stabilization can be achieved with game-theoretic techniques.
We will begin with the review of data-driven optimal control design strategies in Chapter 2. In
general, these strategies build-upon classical dynamic programming and reinforcement learning￾based approaches to obtain optimal solutions forward-in-time. These techniques, often referred to
as the adaptive dynamic programming (Watkins, 1989; Si et al., 2004), approximate dynamic pro￾gramming (ADP) (Werbos, 1991a), and neuro-dynamic programming (NDP) (Bertsekas, 2012), use
online approximator-based parameterization and value and/or policy iterations (Wang et al., 2011;
Werbos, 1991a) to solve the Bellman or Hamilton-Jacobi-Bellman (HJB) equation to obtain the
optimal control. However, these approaches are computationally intensive due to iterative learning
and function approximation. Event-sampled ADP and Q-learning schemes have also been proposed26 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
in the literature to implement these computationally intensive learning-based near-optimal control
approaches with limited resources (Sahoo, 2015; Vamvoudakis et al., 2017b; Narayanan and Ja￾gannathan, 2016b). We shall learn about these data-driven approaches in Chapters 3 to 8. In the
following, we shall review some of the basic event-triggered control strategies and their implemen￾tation in continous- and discrete- time systems. For more information on the event-triggered control
techniques, see Lemmon (2010).
1.4.2 EVENT-TRIGGERED CONTROL OF CONTINUOUS-TIME SYSTEMS
In this chapter, we shall discuss the traditional ZOH-based, model-based, and adaptive event￾triggered control schemes both in continuous- and discrete-time domains. The event-triggered op￾timal control approaches using ADP are discussed in Chapters 3 to 8, along with applications in
Chapter 9.
1.4.2.1 ZOH-based Event-Triggered Control
In this section, the ZOH-based event-triggered control, first introduced by Tabuada (2007), is dis￾cussed. Consider a nonlinear system in the nonaffine form
x˙(t) = f(x(t),u(t)), x(0) = x0, (1.34)
where x(t) ∈ Rn and u(t) ∈ Rm are the state and control input vectors. The internal dynamics is the
vector function f : Rn ×Rm → Rn. Let the feedback-based control input
u(t) = μ(x) (1.35)
guarantees ISS of the controlled system with respect to the event-triggering error e(t) defined as
e(t) = xˆ(t)−x(t), tk ≤ t < tk+1, (1.36)
where {tk}∞
k=0 is the sequence of the sampling instants, referred to as events. At timestk, k = 0,1,···,
the system states are sampled and sent to the controller. The sampled states {xˆk}∞
k=0 form a sequence
in which ˆxk = x (tk) held by a ZOH to compute control input. In certain following chapters in the
book, we employ xe(t) to denote the sampled states, while ˆx(t) is reserved for denoting states es￾timated through observers or identifiers. The output of the ZOH is a piecewise continuous signal
defined as
xˆ(t) = x(tk), tk ≤ t < tk+1. (1.37)
The event-based control input is computed as
ue(t) = μ(xˆ). (1.38)
Then the event-triggered closed-loop system can be represented as
x˙(t) = f(x(t),μ(x(t) +e(t))). (1.39)
Since the control input u renders the system ISS, there exists a Lyapunov functionV : Rn → R (by
converse Lyapunov theorem (Khalil, 2002)). Alternatively, there exists class K functions α,α¯ , γ,
and β such that
α(x) ≤ V(x) ≤ α¯(x), (1.40)
∂V
∂ x f(x,μ(x+e)) ≤ −γ(x) +β(e). (1.41)Background and Introduction to Event-Triggered Control 27
Table 1.2
ZOH-based Event-Triggered Control
System dynamics x˙(t) = f(x(t),u(t)), x(0) = x0
Control input u(t) = μ(x)
Event-triggering error e(t) = xˆ(t)−x(t), tk ≤ t < tk+1
Feedback at the controller ˆx(t) = x(tk), tk ≤ t < tk+1
Control input at the actuator ue(t) = μ(xˆ(t))
Event-triggering conditions β(e(t)) ≤ σ γ(x(t))
The first inequality in (1.40) states that V is positive definite, and the second inequality in (1.41)
ensures the directional derivative is dissipative. Further, we have the directional derivative of the
Lyapunov function V upper bounded as
V˙(t) ≤ −γ(x(t)) +β (e(t)). (1.42)
If we restrict the evolution of the event-triggering error e(t) so that for some σ ∈ (0,1)
β (e(t)) ≤ σ γ(x(t)), (1.43)
for all t ≥ 0, the first derivative of the Lyapunov function is upper bounded by
V˙ ≤ −(1−σ)γ(x(t)). (1.44)
By Lyapunov direct theorem, the event-triggered system is asymptotically stable.
Remark 1.2. The event-triggering condition is the restriction on the function β in (1.43). The
events are triggered upon the violation of this condition. Note that the term σ γ(x(t)) in (1.43)
is referred to as threshold and is a function of system state. Therefore, it is often referred to as the
state-dependent threshold condition (Tabuada, 2007).
The evolution of the event-triggering error is important to get a better insight into the behavior of
the closed-loop event-triggered control. The event-triggering error is reset to zero at the beginning of
the interval [tk,tk+1), i.e., e (tk) = 0 for all k = 0,1,··· ,∞. The system is fed with a piecewise constant
control input as in (1.38). Therefore, the event-triggering error increases when β (the function of
the norm of e(t)) satisfies the inequality β (ek(t)) ≤ σ γ(x(t)), the system state is sampled, and
the state at the controller ˆx is updated as ˆx = x(tk), when this inequality is violated, forcing the error
e(t) to zero again. To implement this event-triggered control scheme, a triggering mechanism is
employed to evaluate the inequality (1.43) continuously. Upon detecting a violation of the inequality,
the sampler in the mechanism triggers an event.
A Special Case of Linear System
Consider the special case of a linear system represented by
x˙(t) = Ax(t) +Bu(t), (1.45)28 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
where x(t) ∈ Rn and u(t) ∈ Rm are the state and control input vector. The matrics A ∈ Rn×n and
B ∈ Rn×m are the system matrices. The pair (A,B) is assumed to be controllable. Therefore, there
exists a control gain matrix K ∈ Rm×n, such that the control input
u(t) = Kx(t) (1.46)
renders the closed-loop system asymptotically stable. We can write the closed-loop dynamics of the
system with event-based control u(t) = Kxˆ(t) as
x˙(t) = Ax(t) +Bu(t) = Ax(t) +BK(x(t) +e(t)),
= Ax(t) +BKx(t) +BKe(t). (1.47)
In the case of a linear system, since the control gain matrix K renders the closed-loop system
asymptotically stable, it also ensures that the system is ISS with respect to the measurement er￾ror. Therefore, by the ISS Lyapunov theorem for the linear system, there exists a Lyapunov function
V : Rn → R≥0 satisfying
ax2 ≤ V(x) ≤ a¯x2, (1.48)
∂V
∂ x (Ax+BKx+BKe) ≤ −ax2 +bex, (1.49)
where V(x) can be selected as a quadratic functions V(x) = xTPx. Then we have a = λmin(P) and
a¯ = λmax(P), where λmin(P) and λmax(P) are the minimum and maximum eigenvalues of the matrix
P and the variable a = λmin(Q), where the positive definite matrix Q satisfies the Lyapunov equation
(A+BK)
TP+P(A+BK) = −Q. (1.50)
The variable b in (1.49) is given as b = KTBTP+PBK. From inequality (1.49), we have
V˙(x) ≤ −ax2 +bex.
Selecting an event-triggering condition to restrict the evolution of the event-triggering error
e(t) ≤ σx, (1.51)
where 0 < σ < 1, we have
V˙(x) ≤ −ax2 +σbx2 = −(a−σb)x2.
Consequently, by ensuring that a > σb, the closed-loop event-triggered linear system can be made
asymptotically stable.
1.4.2.2 Zeno-free Behavior of Event-triggered System
One of the challenges in event-triggered control of continuous-times systems is to ensure that the
events are not accumulated. In other words, it is important to avoid triggering an ‘infinite’ number
of events in a finite interval. Triggering very large (in the limit, infinite) number of events in finite
time is referred to as Zeno behavior. To avoid this Zeno behavior, a nonzero positive lower bound
on the inter-event times, i.e., δtk = tk+1 −tk, should be guaranteed.
A common approach to derive a lower bound on the inter-event time is to examine the evolution
of the event-triggering error e(t). Assume that the closed-loop system is Lipschitz with respect to
the state x and the event-triggering error e, i.e., there exists a positive constant L such that for all x
and e in Rn,
 f(x,μ(x+e)) ≤ Lx+Le. (1.52)Background and Introduction to Event-Triggered Control 29
For the kth event, the event-triggerining error e violates the event-triggering condition (1.43) at
time tk+1, then
β (e(t)) > σ γ (x (t)). (1.53)
Assume there exists a positive constant P such that
Pe(t) ≥ γ−1
 1
σ β1 (e(t))

≥ x (t). (1.54)
From this relation, the ratio of the event-triggering error with respect to the system state must be
greater than a positive constant 1/P. In other words, an event occurs when
1
P ≤ e(t)
x(t)
. (1.55)
This is a more conservative condition than the original event-triggering condition in (1.43). Since the
goal is to show the existence of the nonzero positive δtk, we shall examine this condition and derive
the bounds on inter-event time. Then it should also be a bound for the event-triggering condition in
(1.43). In addition, this also provides a tractable approach to solve for tk.
During an inter-event time tk ≤ t < tk+1 for k = 0,1,··· ,∞, the evolution of the ratio of the
trajectories for e(t)/x(t) satisfies
d
dt
e(t)
x(t) ≤

1+ e(t)
x(t)
 Lx(t)+Le(t)
x(t) = L

1+ e(t)
x(t)
2
. (1.56)
By Comparison principle (Khalil, 2002), the solution to the differential inequality, for any δt ∈
(0,tk+1 −tk), satisfies
e(t)
x(t) ≤
δtL
1−δtL (1.57)
for all k = 0,1,··· ,∞. Furthermore, using (1.55), at the next trigger instant t = tk+1, we have
1
P ≤ e (tk+1)
x (tk+1) ≤
δtkL
1−δtkL
. (1.58)
From the above relation, the inter-event times δtk is lower bounded as
tk+1 −tk = δtk ≥
1
L+LP (1.59)
Remark 1.3. The inter-event times δtk for k = 0,1,··· ,∞ is greater than a nonzero positive value.
Alternatively, its lower bound is non-zero, positive constant. Therefore, the Zeno-free behavior is
guaranteed. Note that the lower bound approaches to zero when the Lipschitz constant L approaches
infinity. This would happen only when the function f is not Lipschitz.
1.4.2.3 Model-based Event-triggered Control
The ZOH event-triggered control scheme can save network resources when compared with the pe￾riodic sampling and control scheme. An alternative to this approach is to use a model of the system
and implement this model along with the controller to estimate the system state and update control
inputs between any two events using the model states. In other words, instead of using the state
and control transmitted during an event and holding this value at the controller and at the actuator,
in the model-based event-triggered control scheme introduced by Garcia and Antsaklis (2013), the
estimated states obtained from the model are used to continuously update the control input.30 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Consider the model of a linear time-invariant system (1.45) given by
˙
xˆ(t) = Aˆxˆ(t) +Buˆ (t), (1.60)
where u(t) = Kxˆ(t) ∈ Rm represents the control input, which is a function of the estimated system
state ˆx(t) ∈ Rn and K is the constant feedback control gain matrix obtained by using (Aˆ,Bˆ). In this
case, if the control inputs are computed using the model states, the lack of actual state information
from the sensors to the controllers will lead to an error. Unlike the measurement error in the ZOH￾based event-triggered controller, in the model-based controllers, the error is due to the discrepency
between the system state and the model state and is called as the state estimation error, which is
defined as
e(t) = x(t)−xˆ(t).
The model uncertainties will cause this error to grow. Hence, in this control scheme, the events
are designed to keep this estimation error e(t) under a predefined threshold. During an event, the
controller resets the estimated state ˆx equal to the received system state x (i.e., ˆx(t) = x(t) at t = tk)
and estimation error e(t) to zero, i.e., e(t) = 0 at t = tk.
It is important to note that the model uncertainties of fixed model-based event-triggered control
system (i.e., ΔA = A−Aˆ,ΔB = B−Bˆ

need to be small for maintaining the stability, and for
fully exploiting the model for reducing potentially redundant events. Consider the fixed model in
(1.60) with the control input
u(t) =  Kx(t), t = tk
Kxˆ(t), tk ≤ t < tk+1. (1.61)
By using u(t) and (1.60), the fixed model-based event-triggered closed-loop system dynamics can
be represented as
x˙(t)=(Aˆ +BKˆ )x(t)+(A+BK )x(t)−BKe(t), (1.62)
where A = A−Aˆ,B = B−Bˆ represent model uncertainties between actual system dynamics (A,B)
and constant model matrices (Aˆ,Bˆ) with the state estimation error
e(t) =  0 t = tk
x(t)−xˆ(t) tk ≤ t < tk+1
. (1.63)
Both state estimation error e(t) and system uncertainties (A,B) will affect system stability.
Theorem 1.13. (Garcia and Antsaklis, 2013) Given a linear event-triggered continuous-time system
(1.45) and a model with constant matrices (1.60), the closed-loop system with feedback selected as
(1.61) is asymptotically stable when the following relation defined as the event-triggering condition
e(t) ≤
σ(λmin(Q)−Δ)
b¯ x(t) (1.64)
is violated, where 0 < σ < 1, b¯ = 2PBKˆ +2ηPK with B˜ ≤ η, (A˜ +BK˜ )TP+P(A˜ +BK˜ ) ≤
Δ < λmin(Q) with Q, P, and K are positive definite matrices, and constant feedback control gain
matrix respectively. Here the feedback gain matrix K is obtained by using
(Aˆ +BKˆ )
TP+P(Aˆ +BKˆ ) = −Q. (1.65)Background and Introduction to Event-Triggered Control 31
Table 1.3
Model-based Event-Triggered Control
System dynamics x˙(t) = Ax(t) +Bu(t), x(0) = x0
System model ˙
xˆ(t) = Aˆxˆ(t) +Buˆ (t), ˆx(0) = xˆ0
Control input u(t) =  Kx(t) t = tk
Kxˆ(t) tk ≤ t < tk+1
Event-triggering error e(t) = x(t)−xˆ(t), tk ≤ t < tk+1
Event-triggering conditions e(t) ≤ σ(λmin(Q)−Δ)
b¯ x(t)
1.4.2.4 Event-based Trajectory Tracking
Consider a nonlinear system
x˙(t) = f(x(t),u(t)), x(0) = x0, (1.66)
where x(t) ∈ Rn and u(t) ∈ Rm. The control objective is to track a feasible reference trajectory
xd(t) ∈ Rn generated by a reference system represented by
x˙d(t) = fd(xd(t),v(t)), xd(0) = xd0, (1.67)
where xd(t) ∈ Rn is the reference state with xd(0) = 0, v(t) is the external input to the reference sys￾tem, and fd(xd,v) ∈ Rn is the internal dynamics. Following standard characteristics of the systems
in (1.66) and (1.67) are assumed.
Assumption 1.1. System (1.66) is stabilizable and the system states are available for measurement.
The feasible reference trajectory xd(t) ∈ Ωxd , where Ωxd is a compact set, is bounded such that
xd(t) ≤ bxd , where bxd > 0 is a constant.
Define the error between the system state and the reference state as the tracking error, er(t) 
x(t) − xd(t). A control signal that enables the system to track the reference trajectory depends on
the tracking error and can be represented in the form
u(t) = γ(ξ (t)), (1.68)
where ξ = [er xd v]
T . Then the tracking error system, utilizing (1.66) and (1.67), can be defined by
e˙r(t) = x˙(t)−x˙d(t) = f(er +xd, γ(ξ ))− fd(xd,v). (1.69)
In the event-triggered control framework, the controller (1.68) is executed at time instants tk for
k = 0,1,···, and can be expressed as
u(t) = γ(ξ (tk)), tk ≤ t < tk+1. (1.70)
Using this control signal, the tracking error dynamics can be rewritten as
e˙r(t) = f(er +xd, γ(ξ (tk)))− fd(xd,v), tk ≤ t < tk+1. (1.71)32 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
We can also define the measurement errors due to event-based sampling as
eξ (t) =
⎡
⎣
eer(t)
exd (t)
ev(t)
⎤
⎦ =
⎡
⎣
er(tk)−er(t)
xd(tk)−xd(t)
v(tk)−v(t)
⎤
⎦, tk ≤ t < tk+1. (1.72)
Using the measurement errors, the tracking error dynamics can be expressed as
e˙r(t) = f(er +xd, γ(ξ +eξ ))− fd(xd,v). (1.73)
Adding and subtracting f(er +xd, γ(ξ )), we have
e˙r(t) = f(er +xd, γ(ξ ))− fd(xd, v) + f(er +xd, γ(ξ +eξ ))− f(er +xd, γ(ξ )). (1.74)
Note that the reference trajectory generated by (1.67) should be a feasible trajectory for the system
(1.66) to track. To characterize feasible reference trajectories, authors Tallapragada and Chopra
(2013) introduced a family of compact sets and assumed that f, fd, and γ are Lipschitz continuous
on compact sets. Using this assumption, yields the following inequality
f(er +xd, γ(ξ +eξ ))− f(er +xd, γ(ξ )) ≤ Leξ , (1.75)
where L is a (local) constant that is dependent on the compact set. It can then be shown that if the
continuously implemented control policy γ(ξ ) admits a Lyapunov function V satisfying
α1(er) ≤ V(er) ≤ α2(er), (1.76)
∂V
∂ er
[ f(er +xd, γ(ξ ))− fd(xd,v)] ≤ −α3(er), (1.77)
where α1(·), α2(·), and α3(·) are class K∞ functions, then the closed-loop tracking controller ren￾ders the tracking error uniformly ultimately bounded provided the events are triggered when the
inequality condition Leξ  < σ α3(er)
β(er) with σ ∈ (0,1) and β(er) ≥ maxw ∂V(w)
∂w  is violated.
It should be noted that with the tracking error dynamics defined as in (1.74), the tracking control
problem associated with the system (1.66) can be viewed as a regulation problem for the tracking
error system. However, unlike the regulation problem, where a continously implemented asymptot￾ically stabilizing control input implemented aperiodically in a ZOH-based event-triggered control
scheme retains asymptotically stability, the regulation problem associated with the tracking error
system does not retain asymptotic convergence of the tracking error to zero. Rather, the tracking er￾ror signals are only guaranteed to be uniformly ultimately bounded with the event-triggered tracking
control implementation. For more information on event-triggered tracking controllers, see Tallapra￾gada and Chopra (2013).
1.4.3 EVENT-TRIGGERED CONTROL OF DISCRETE-TIME SYSTEMS
We shall now review the event-triggered control strategies for discrete-time systems.
1.4.3.1 ZOH-based Event-Triggered Control Design
Consider a nonlinear system
xk+1 = f(xk,uk), (1.78)
where xk ∈ Rn and uk ∈ Rm are the state and control input vectors. The internal dynamics is the
vector function f : Rn ×Rm → Rn. Let the feedback-based control input
uk = μ(x) (1.79)Background and Introduction to Event-Triggered Control 33
guarantees ISS of the controlled system with respect to the event-triggering error ek defined as
ek = xˆk −xk, kl ≤ k < kl+1, (1.80)
where {kl}∞
l=0 is the sequence of the sampling instants, referred to as events and ˆxk is the previously
defined sampled state as defined next. At times kl, l = 0,1,···, the system states are sampled and
sent to the controller. The sampled states {xˆl}∞
l=0 form a sequence in which ˆxl = x (kl) is used to
compute the control input that is held at the actuator until a new event. The output of the ZOH is a
piecewise continuous signal defined as
xˆk = xkl
, kl ≤ k < kl+1. (1.81)
The event-based control input is computed as
uk = μ(xˆ). (1.82)
Then the event-triggered closed-loop system can be represented as
xk+1 = f(xk,μ(xk +ek)). (1.83)
Since the control policy μ renders the system ISS, there exists an ISS-Lyapunov function V : Rn →
R≥0. Additionally, there exists class K -functions α,α¯ , γ, and β such that
α(xk) ≤ V(xk) ≤ α¯(xk), ∀xk ∈ Rn, (1.84)
V(f(xk, e))−V(xk) ≤ −γ(xk) +β(ek), ∀xk ∈ Rn,∀ek ∈ Rn. (1.85)
From these results, we have the first-difference of the Lyapunov function V upper bounded as
ΔVk ≤ −γ(xk) +β (ek). (1.86)
If we restrict the evolution of the event-triggering error ek so that for some σ ∈ (0,1)
β (ek) ≤ σ γ(xk), (1.87)
for all k ≥ 0, the first difference of the Lyapunov function is upper bounded by
ΔVk ≤ −(1−σ)γ(xk). (1.88)
By Lyapunov direct theorem, the event-triggered control system is asymptotically stable.
A Special Case of Linear System
This section introduces a traditional ZOH event-triggered control scheme and an event-triggering
condition. Different from the periodic sampling scheme, the ZOH event-triggered controller might
not receive the system state at every sampling time instant. Hence the controller will hold the latest
received system state vector for control input design until a new state vector is received due to an
event. Now, consider the linear discrete-time
xk+1 = Axk +Buk (1.89)
where xk ∈ Rn,uk ∈ Rm are the system states and control inputs respectively, and A ∈ Rn×n, B ∈
Rn×m denote the system matrices. Let the event-triggered control input be given by
uk = Kxˆk, (1.90)34 Optimal Event-Triggered Control Using Adaptive Dynamic Programming

    	
	


	


	


   
  	





	  




    

 
 	 


 



 
 

Figure 1.1 Adaptive model-based event-triggered control system.
where ˆxk is the latest state measurement at the time kl and K is a stabilizing control gain matrix for
the system given by (1.89) with known system matrices (A,B). The latest state measurement is not
updated for any k such that kl ≤ k < kl+1 and it is updated at the occurence of the next event at kl+1.
Due to this, the control input will introduce a measurement error ek = xˆk −xk into the closed-loop
system. After substituting control input uk from (1.90), the closed-loop system dynamics can be
expressed as
xk+1 = (A+BK)xk +BKek. (1.91)
As the inter-event time is elongated, the effect of the state measurement error ek in (1.91) might
accumulate and affect the stability of the system. Therefore, a threshold (Eqtami et al., 2010) is
derived for ek to ensure stability. Note, however, that the delay between the sensor, controller, and
actuator is assumed to be negligible.
Theorem 1.14. For the linear discrete-time event-triggered control system (1.89), let an event be
triggered when the event-triggering condition
ek2 ≤ Γxk2 (1.92)
is violated, where Γ = σ Q
BK2P ,0 < σ < 1, and Q,P,K are positive definite matrices with the
feedback control gain matrix selected as the solution of
2(A+BK)
TP(A+BK)−P = −Q. (1.93)
Then the closed-loop system is asymptotically stable.
Proof. Consider a Lyapunov function candidate as V = xT
k Pxk. Using system representation (1.91)
and Cauchy-Schwartz inequality, the first difference of the Lyapunov function candidate can be
derived as
ΔV ≤ −Qxk2 +2BK2Pek2 ≤ −(1−σ)Qxk2 (1.94)
after applying the event-trigger condition (1.92). Then ΔV is negative definite while V is positive
definite when the state of the system is nonzero. Therefore, the event-triggered closed-loop control
system is globally asymptotically stable. In other words, as k → ∞, xk → 0.Background and Introduction to Event-Triggered Control 35
Table 1.4
ZOH-based Event-Triggered Control of Linear Discrete-time System
System dynamics xk+1 = Axk +Buk
Control input uk =
 Kxk k = kl
Kxˆk kl ≤ k < kl+1
Event-triggering error ek = xˆk −xk, kl ≤ k < kl+1
Event-triggering conditions ek2 ≤ Γxk2
, Γ = σ Q
BK2P ,
0 < σ < 1 and 2(A+BK)TP(A+BK)−P = −Q
1.4.3.2 Fixed Model-based Event-triggered Control Design
Although ZOH event-triggered control can save the network resource compared with periodic time￾triggered control, its efficiency is low since the measured state and control inputs are not updated
between any two events. As an alternative, the model-based event-triggered control scheme maybe
used to provide state estimates between events to update the control input. This section presents a
(fixed) model-based event-triggered control scheme and the corresponding event-trigger condition
design procedure for linear time-invariant discrete-time systems. We shall assume that the commu￾nication delay (within the closed-loop) is negligible, the system’s initial conditions are known, the
uncertainities in the system dynamics are bounded above, and the bounds are known.
Consider the linear discrete-time system as in (1.89). Let a model for this system be defined as
xˆk+1 = Aˆkxˆk +Bˆkuk, xˆ0 = x0, (1.95)
with uk = Kkxˆk ∈ Rm denoting the control input vector based on estimated states, Kk ∈ Rm×n is the
time-dependent feedback control gain matrix which is derived based on estimated system matrices,
i.e., Aˆk ∈ Rn×n,Bˆk ∈ Rn×m.
We shall assume that the model is fixed, i.e., Aˆk,Bˆk, and Kk are fixed for all k. The event-triggered
control input is designed as
uk =
 Kxk, k = kl
Kxˆk, kl ≤ k < kl+1. (1.96)
where Kk = K is a feedback control gain matrix selected using fixed model parameters (Aˆk = Aˆ,Bˆk =
Bˆ). The model-based event-triggered closed-loop system can be represented as
xk+1 = (Aˆ +BKˆ )xk + (A+BK )xk +BKek (1.97)
where A = A−Aˆ,B = B−Bˆ represent model uncertainties between actual system dynamics (A,B)
and constant model matrices (Aˆ,Bˆ) with the state estimation error
ek = xˆk −xk. (1.98)
The state estimation error ek is reset to zero whenever an event occurs, setting the estimated state
equal to the actual state of the system. One of the key assumptions to designing the event-triggering
condition to ensure system stability is that the constants Am and Bm that satisfy A < Am and
B < Bm are known.36 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Theorem 1.15. Consider a linear discrete-time system as in (1.89) with event-triggered control
input (1.96), where xˆk is the model states updated via (1.95). Assume that the constants Am and
Bm satisfying A < Am and B < Bm are known. Let an event be triggered when the inequality
condition
ek2 ≤ Γxk2 (1.99)
is violated, where Γ = σ Q−4Δ2
mP
4BK2P with 0 < σ < 1, Am +BmK = Δm, Q,P are positive definite
matrices, and K is constant feedback control gain matrix. Here the feedback gain matrix is obtained
by using
2(Aˆ +BKˆ )
TP(Aˆ +BKˆ )−P = −Q. (1.100)
Then the closed loop system is asymptotically stable if Q and P satisfy Δ2
mP < Q
4 .
Proof. Consider the Lypaunov function candidate as Vk = xT
k Pxk. Using system representation
(1.97) and Cauchy-Schwartz inequality, the first difference of Lyapunov function candidate can
be expressed as
ΔVk = xT
k+1Pxk+1 −xT
k Pxk
=

(Aˆ +BKˆ )xk + (A˜ +BK )xk +BKek
T
P

(Aˆ +BKˆ )xk + (A˜ +BK )xk +BKek

−xT
k Pxk
≤ xT
k

2(Aˆ +BKˆ )
TP(Aˆ +BKˆ )−P

xk +2

(A+BK )xk +BKek
T
P

(A˜ +BK )xk +BKek

≤ −xT
k Qxk +4xT
k (A˜ +BK )
TP(A˜ +BK )xk +4eT
k KTBTPBKek
≤ −Qxk2 +4A+BK 2Pxk


2 +4

BK

2

Pek2.
Define Δm such that A+BK ≤A+BK  ≤ Am +BmK = Δm. Then we have
ΔVk ≤ −Qxk2 +4Δ2
mPxk2 +4BK2Pek2 . (1.101)
Applying event-triggering condition (1.99) into (1.101) , we get
ΔVk ≤ −(1−σ)

Q −4Δ2
mP

xk2 . (1.102)
With 0 < σ < 1, assuming Δ2
mP < Q
4 and Q a positive definite matrix, ΔVk is negative definite.
Hence, using (1.97), the model-based event-triggered closed-loop system is globally asymptotically
stable.
Remark 1.4. Compared with the ZOH event-triggered scheme, the fixed model-based approach can
reduce network traffic since the model can estimate the system state vector between two events, and
the control input is updated accordingly. However, the control gain matrix K is fixed. The model
uncertainties (A,B) need to be small satisfying A+BK 2P < Q
4 in order to maintain stability.
In other words, large model uncertainties can affect performance and stability.
1.4.3.3 Adaptive Model-based Event-triggered Control Design
In the adaptive model-based event-triggered control scheme, the model parameters are constantly
adjusted once per event when the controller receives the system state vector from the sensor. At the
update times, the state vector of the model is also updated with the measurements obtained from
the system; the update times are generally non-periodic and are triggered by the size of the state
estimation error. Consequently, the model update is also not periodic in contrast with traditional
adaptive control literature (Luders and Narendra, 1973). We shall employ estimation (Luders andBackground and Introduction to Event-Triggered Control 37
Table 1.5
Fixed Model-based Event-Triggered Control of Discrete-time System
System dynamics xk+1 = Axk +Buk
System model xˆk+1 = Aˆxˆk +Buˆ k, ˆx0 = x0
Control input uk =
 Kxk k = kl
Kxˆk kl ≤ k < kl+1
Event-triggering error ek = xˆk −xk, kl ≤ k < kl+1
Event-triggering conditions ek2 ≤ Γxk2
, Γ = σ Q−4Δ2
mP
4BK2P , Δm = Am +BmK
0 < σ < 1 and 2(Aˆ +BKˆ )TP(Aˆ +BKˆ )−P = −Q
Narendra, 1973) and adaptation (Werbos, 1983) techniques to derive an event-triggered control
scheme with unknown system dynamics. First, system states are estimated by using an adaptive
state estimator or adaptive model. By using the current model parameters, the feedback gain matrix
is generated during an event. In the inter-event times, the model will generate the state estimates for
updating the control input, but the control gain matrix is fixed between the events. Then an adaptive
model-based event-triggered control scheme is derived based on the estimated state vector, which
can maintain stability even with unknown system dynamics.
Adaptive State Estimator Design
Based on event-triggered control schemes (Anta and Tabuada, 2010; Eqtami et al., 2010; Tabuada,
2007; Sontag, 2008; Dimarogonas and Johanson, 2009a,b; Rabi et al., 2008; Pekir and Shiryaev,
2006; Garcia and Antsaklis, 2011)), the parameters of the adaptive state estimator will be updated
only when an event is triggered and sensed system states are received at the controller. Recalling
(1.89) and (1.95), event-triggered control system can be represented as
xk+1 = Axk +Buk = θ T zk (1.103)
and the adaptive state estimator as
xˆk+1 = Aˆkxk +Bˆkuk = θˆT
k zk, (1.104)
where θ =  A B T and θˆ
k =  Aˆk Bˆk
T represent the target and estimated system matrices,
respectively, and zk =  xT
k uT
k
T denotes the augmented state and control vector. The state esti￾mation error dynamics ek+1 can be derived as
ek+1 = xk+1 −xˆk+1 = θ T zk −θˆT
k zk = θ
T
k zk, (1.105)
with θ
k = θ −θˆ
k =  Ak Bk
T is the parameter estimation error. Now define the update law for
the unknown parameters θˆ
k as
θˆ
k+1 = θˆ
k +αeγk+1zkeT
k+1, (1.106)38 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
where αe is the tuning parameter satisfying 0 < αe < 1 and γk+1 is an indicator for the event trigger
condition, i.e.,
γk+1 =
 1 event is initiated
0 event is not initiated. (1.107)
Meanwhile, adaptive parameter estimation error dynamics θ
k can be expressed as
θ˜
k+1 = θ˜
k −αeγk+1zkeT
k+1. (1.108)
Compared with traditional adaptive estimator schemes (Luders and Narendra, 1973), where the
updates are done periodically, event-based nonperiodic tuning law (1.106) used here updates the
parameters aperiodically. As a result, convergence of the parameters is coupled with the frequency
of events. Let the proposed adaptive estimator be defined as (1.104) and the adaptive estimator
parameter update law be given by (1.106). Then using Taylor’s theorem (Goodwin and Sin, 2014)
or by using Lyapunov theory (Xu, 2012), sufficient conditions for designing the positive constant
tuning parameter αe can be derived such that the parameter estimation errors θ
k (1.108) converge to
zero asymptotically as time k → ∞.
Event-triggered Controller Design
Consider the linear discrete-time system and adaptive estimator represented by (1.103) and (1.104),
respectively, with adaptive model-based event-triggered control input uk = Kkxˆk, where Kk is a time￾varying feedback gain matrix. The closed-loop system can be represented after applying the control
input as
xk+1 = Axk +Buk = (A+BK)xk −BKkek, (1.109)
where the feedback gain matrix is based on adaptive estimator parameter at time k (i.e., θˆ
k =  Aˆk Bˆk
T
) and estimated state error vector. It is important to note that the control gain ma￾trix Kk can be time-dependent until the model parameters attain their target values (i.e., when
θˆ
k → θ,Aˆk → A,Bˆk → B, then Kk → K). In an adaptive model-based event-triggered scheme, the
designed control input uk = Kkxˆk, which is based on adaptive the model 
Aˆk,Bˆk

, can render the state
estimator ˆxk+1 = 
Aˆk +BˆkKk

xˆk asymptotically stable. Any such Kk can also render the closed-loop
adaptive model-based event-triggered system ISS (Eqtami et al., 2010; Garcia and Antsaklis, 2011)
with respect to the estimation error ek.
Remark 1.5. Given symmetric positive definite matrices P and Q, the feedback gain matrix uk =
Kkxˆk can be selected such that the following holds
2

Aˆk +BˆkKk
T P

Aˆk +BˆkKk

−P = −Q. (1.110)
In the model-based event-triggered linear discrete-time system, the adaptive state estimator and
feedback gain matrix should be updated when
ek2 ≤ Γxk2 , (1.111)
is satisfied where
Γ = σ Q
4PBKk2 (1.112)
with Q,P are symmetric positive definite solutions of (1.110) and 0 < σ < 1.
Remark 1.6. It is important to note the difference between adaptive model 
Aˆk,Bˆk
 and actual
system matrixes (A,B) or parameter estimation errors will converge to zero with the adaptive model￾based scheme when compared to a fixed model scheme. Therefore, an adaptive model-based event￾triggered control scheme can reduce more network traffic than a fixed model-based scheme when
the uncertainties are compensated via adaptation.Background and Introduction to Event-Triggered Control 39
Table 1.6
Adaptive Model-based Event-Triggered Control of Linear System
System dynamics xk+1 = Axk +Buk
System model xˆk+1 = Aˆkxˆk +Bˆkuk, ˆx0 = x0
Control input uk =
 Kkxk k = kl
Kkxˆk kl ≤ k < kl+1
Event-triggering error ek = xk −xˆk, kl ≤ k < kl+1
Event-triggering conditions ek2 ≤ Γxk2
, Γ = σ Q
4BKk2P ,
0 < σ < 1, 2(Aˆk +BˆkKk)TP(Aˆk +BˆkKk)−P = −Q
Model parameterization θˆ
k = 
Aˆk Bˆk
T
Model parameter adaptation rule θˆ
k+1 = θˆ
k +αeγk+1zkeT
k+1
Remark 1.7. From event triggering condition (1.111), ek2 ≤ Γxk2
. Thus, when xk →
0,ek2 → 0 and ek → 0.
Example 1.13. Consider the linear discrete-time system
xk+1 =
 1.0559 −0.0397
0.0298 0.9318 
xk +
 0.1008
0.0981 
uk
with initial system states given by x0 =  −0.2 0.5 T and the event-triggering condition param￾eter σ = 0.8.
First, the performance of the ZOH event-triggered control scheme is evaluated. As shown in
Figure 1.2, the measurement error will be reset to zero when the norm of measurement error exceeds
the threshold, and the sensor will transmit sensed system state to the controller since the event is
triggered. In Figure 1.2, compared with periodic control, ZOH event-triggered control significantly
reduces network traffic due to the increased time intervals between consecutive events. Figure 1.2
also shows that the measurement error and its threshold (1.92) will converge to zero.
Next, the fixed model-based event-triggered control scheme is considered. This scheme, as de￾scribed by Garcia and Antsaklis (2011), uses the model
Aˆ =
 1.0503 −0.0358
0.0268 0.9385 
,Bˆ =
 0.1009
0.0886 
,
where the uncertainties are given by ΔA = 0.01 and ΔB = 0.01. As shown in Figure 1.3, fixed model￾based event-triggered control can reduce more network traffic than ZOH event-triggered scheme40 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Figure 1.2 Performance of ZOH-based event-triggering mechanism.
Figure 1.3 Performance of the fixed model-based event-triggering mechanism.Background and Introduction to Event-Triggered Control 41
Figure 1.4 Performance of the adaptive model-based event-triggering mechanism.
shown in Figure 1.2 due to elongated update interval. Since the model can estimate system states,
events were triggered less frequently than the ZOH-based event-triggered control scheme. However,
the drawback of a fixed model-based event-triggered scheme is that uncertainties (i.e., ΔA,ΔB) need
to be small and more computations are required. If the uncertainties are increased to ΔA = 0.3324
and ΔB = 0.6295, then the fixed model-based event-triggered control cannot even maintain the sys￾tem stability since the constant control gain matrix K is developed from inaccurate model parameters
(Aˆ,Bˆ). If model(Aˆ,Bˆ) is far away from the actual system parameters (A,B), control gain matrix K
might not be stabilizing.
In the adaptive model-based event-triggered control scheme, the model estimates A0,B0 were
initialized randomly and tuned subsequently. The model parameters are tuned online to attain their
target values (A,B) over time. The adaptive tuning parameter was selected as αe = 0.01. Figure
1.4 shows the performance of event-triggering mechanism. At the beginning, the estimation error
is large and events are triggered more frequently since the model needs to be tuned. After a short
period, the events are triggered less frequently, which reduced the network traffic much more than
both the ZOH- and the fixed model- based event-triggered control schemes. In the end, to demon￾strate the benefit of all the event-triggered control schemes (i.e., ZOH, fixed model-based with
small uncertainties, and adaptive model-based) from the network side, the network performance of
all the three schemes are compared in Figure 1.5. Assume that every system states need 20 bits for
transmission. From this figure, it can be seen that the network traffic of all event-triggered control
schemes decreased significantly compared with periodic time-based sampling. However, the advan￾tage of the adaptive model-based event-triggered control scheme is that it reduces network traffic
and relaxes the requirement of system dynamics or the assumption that the uncertainties are small
with the fixed-model-based scheme.42 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Figure 1.5 Comparison of network traffic for the event-triggered schemes.
1.5 CLOSING REMARKS
This chapter briefly introduced event-triggered control with necessary concepts of dynamical con￾trol systems operating in continuous, discrete, sampled-data, and hybrid time domains. Mathemati￾cal preliminaries are also introduced, which will be used throughout the book. In the second part of
the chapter, the traditional event-triggered control designs both in continuous- and discrete- time
frameworks, are also introduced. The three main architectures are introduced, i.e., ZOH-based,
fixed-model-based, and adaptive model-based designs. It is observed that fixed model-based event￾triggered control can save more computational resources when compared to their ZOH-based coun￾terparts. In addition, the adaptive model-based event-triggered control can save more computation
when compared with the other two approaches without complete knowledge of the system dynam￾ics.
1.6 PROBLEMS
1.6.1 Show that the following inequalities hold for any x ∈ Rn:
x2 ≤ x1 ≤ √nx2 and x∞ ≤ x2 ≤ √nx∞,
where n is a scalar.
1.6.2 Check if the following inequalities are true. For two real matrices A and B of dimension m×n
and n×l, the following inequities hold
a. √
1
n A∞ ≤ A2 ≤ √mA∞
b. √
1
m A1 ≤ A2 ≤ √nA1
c. A2 ≤ A1A∞
d. ABp ≤ ApBp
1.6.3 Explain the nature of the equilibrium point(s) of the following autonomous nonlinear systems.
a. ˙x(t) = x2(t)Background and Introduction to Event-Triggered Control 43
b. ˙x(t) = sin(x(t))
c. ˙x(t) = sin( 1
x(t))
1.6.4 (Khalil, 2002) Consider a function
V(x) = x2
1
1+x2
1
+x2
2,
where x = (x1 x2)T ∈ R2. Is this function radially unbounded?
1.6.5 Show that for a linear time-invariant dynamical system state x(t) ∈ Rn and control u(t) ∈ Rm,
if the control input renders the closed-loop system asymptotically stable, then the system is
ISS with respect to the measurement error.
1.6.6 (Lewis et al., 2012b) The longitudinal dynamics of an F-16 aircraft in straight and level flight
at 502 ft/sec are given by
x˙ = Ax+Bu =
−2.0244×10−2 7.8761 −3.2169×101 −6.502×10−1
−2.5373×10−4 −1.0189 0 9.0484×10−1
0 00 1
7.9472×10−11 −2.498 0 −1.3861

x (1.113)
+
 −1×10−2
−2.09×10−3
0
−1.99×10−1

u (1.114)
The state is x =  vT α θ q T
, with vT the forward velocity, α the angle of attack, θ
the pitch angle, and q the pitch rate. The control input u(t) is the elevator deflection δe.
(a) Plot the response of the open-loop system to initial conditions of x0 =  0 0.101 T
(note that the angular units are in radians).
1.6.7 The dynamics of the Lorenz attractor system are given by
x˙1 = −σ (x1 −x2)
x˙2 = rx1 −x2 −x1x3
x˙3 = −bx3 +x1x2.
This system exhibits chaotic behavior. Simulate the trajectories using MATLAB with σ =
10,r = 28,b = 8/3. Use initial conditions of x1 = 0.1, x2 = 0.1,x3 = 0.1, and a time window
of 200sec. Plot the states versus time, and also the phase-plane plot in (x1,x2, x3)-space.
1.6.8 The dynamical interaction of two populations, one predatory on the other (e.g. foxes and
rabbits) is described by (Luenberger, 1979)
x˙1 = ax1 −bx1x2
x˙2 = −cx2 +dx1x2
with a,b,c,d positive constants. The number of prey at time t is described by x1(t) and of
predators by x2(t). This model exhibits oscillatory behavior corresponding to alternating pe￾riods where the prey is scarce then plentiful. In the first equation, the first term reveals that
x1(t) will increase if x2 is zero; the second term shows the effect of encounters between preda￾tor and prey, indicating that a positive x2 causes x1(t) to decrease. In the second equation, the
first term reveals that x2(t) will decrease if x1 is zero; the second term shows that a positive
x1 causes x2(t) to increase.
Simulate this system in MATLAB using different values of the constants (begin on the first
run with all values equal to 1). Plot x1(t),x2(t) versus t, and also the phase-plane plot x1
versus x2. Be sure to use a sufficiently long simulation run time to observe all effects. Start
with nonzero initial conditions.44 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
1.6.9 The effects of overcrowding of prey in the presence of scarce food resources can be included
to the model described in the previous problem by adding a term so that the model becomes
x˙1 = ax1 −bx1x2 −ex2
1
x˙2 = −cx2 +dx1x2
with e > 0. Simulate this system in MATLAB and compare its behavior to the model in the
previous problem.
1.6.10 The development of an epidemic can be described by (Luenberger, 1979)
x˙1 = −βx1x2
x˙2 = βx1x2 −γx2
x˙3 = γx2
with x1 the number of susceptible individuals, x2 the number of infected individuals, and
x3 the number of individuals who are either immune or removed by isolation or death. The
infection rate constant is β > 0, and the removal rate constant is γ > 0. Simulate this system
in MATLAB using different values of the constants. Plot x1(t),x2(t), x3(t) versus t, and also
phase-plane plots xi versus xj. Plot the 3-D plot in x1,x2,x3-space. Be sure to use a sufficiently
long simulation run time to observe all effects. Start with nonzero initial conditions.
1.6.11 Many congenital diseases can be explained as the result of both genes at a single location
being the same recessive gene (Luenberger, 1979). Under some assumptions, the frequency
of the recessive gene at generation k is given by the recursion
x(k +1) = x(k)
1+x(k)
.
Simulate in MATLAB using x(0) = 80. Observe that x(k) converges to zero, but very slowly.
This explains why deadly genetic diseases can remain active for hundreds of generations.
Simulate the system starting for a small negative value of x(0) and observe that it tends away
from zero.
1.6.12 Simulate the system
x1(k +1) = x2(k)
1+x2(k)2
x2(k +1) = x1(k)
1+x2(k)2
using MATLAB. Plot x1(k), x2(k) versus k and the phase-plane plot.
1.6.13 The system
x˙1 = x2
x˙2 = x3
x˙3 = −2x1x3 +sinx2 +5u
is in Brunovsky form. If x1(t) is required to track a desired trajectory yd(t) the feedback
linearization design is very easy. However, in this example it is desired for y(t) ≡ x2(t) to
track yd(t). Perform the design and study the zero dynamics. Simulate the closed-loop system
using MATLAB with initial conditions of x1(0) = 1,x2(0) = 1, x3(0) = 1.
1.6.14 For the system
x˙1 = x1x2 +x3
x˙2 = −2x2 +x1
x˙3 = sinx1 +cx1x2Background and Introduction to Event-Triggered Control 45
Find the equilibrium points.
1.6.15 Using Lyapunov techniques examine stability for the following systems. Plot time histories
to substantiate your conclusions.
(a) ˙x1 = x1x2
2 −x1 and ˙x2 = −x2
1x2 −x2
(b) ˙x1 = x2 sinx1 −x1 and ˙x2 = −x1 sinx1 −x2.
(c) ˙x1 = x2 +x1

x2
1 −2

and ˙x2 = −x1.
1.6.16 Using Lyapunov techniques design controllers to stabilize the following systems. Plot time
histories, both open-loop and closed-loop.
(a) ˙x1 = x1x2 and ˙x2 = x2
1 −sinx1 +u
(b) ˙x1 = x1 +
1+x2
2

u and ˙x2 = x2 sinx1.
(c) ˙x1 = x1

x2
1 −2

and ˙x2 = cos(x1)+(2+sin(x1))u.
1.6.17 (Bounded Stability) Using Lyapunov Extensions. Use the UUB extension to show that the
Van der Pol oscillator
y¨+α 
y2 −γ

y˙+y = u
is UUB. Find the radius of the region of boundedness. Simulate the system in MATLAB and
plot phase-plane trajectories to verify the result.
1.6.18 Consider the system
x˙1 = x1x2
2 −x1

x2
1 +x2
2 −3

x˙2 = −x2
1x2 −x2

x2
1 +x2
2 −3

.
Show that the system is UUB. Select the Lyapunov function
L = 
x2
1 +x2
2 −3
2
to demonstrate that this system has two equilibria: a stable limit cycle and an unstable equi￾librium point at the origin. Simulate the system using MATLAB and make phase-plane plots
for several initial conditions to convince yourself that, in this example, the limit cycle is the
cause of the UUB nature of the stability.
1.6.19 The system
x˙ = Ax+Bu+d
has a disturbance d(t) that is unknown but bounded so that d < dM, with the bound dM
known. Show that by selecting the control input as u(t) = −Kx(t) it is possible to improve
the UUB stability properties of the system by making the bound on x smaller. In fact,
if feedback is allowed, the initial system matrix A need not be stable as long as (A,B) is
stabilizable.2 Adaptive Dynamic
Programming and Optimal
Control
CONTENTS
2.1 Neural Network (NN) Control: Introduction........................................................................... 48
2.1.1 History of Neural Network ......................................................................................... 49
2.1.2 Neural Network Architectures.................................................................................... 49
2.1.3 Function Approximation Property of NN................................................................... 56
2.1.4 Neural Network Training Methods............................................................................. 60
2.2 Optimal Control and Dynamic Programming ......................................................................... 62
2.2.1 Discrete-time Optimal Control ................................................................................... 63
2.2.2 Dynamic Programming............................................................................................... 65
2.2.3 Limitations of Dynamic Programming....................................................................... 67
2.3 Adaptive Dynamic Programming for Discrete-Time Systems................................................ 68
2.3.1 Value and Policy Iteration-based Online Solution...................................................... 69
2.3.2 Temporal Difference and Value Function Approximation ......................................... 70
2.3.3 Online Policy and Value Iteration with Approximated Value Function ..................... 72
2.3.4 Actor-Critic Architecture............................................................................................ 73
2.3.5 Q-learning................................................................................................................... 73
2.3.6 Q- learning for Linear Quadratic Regulator ............................................................... 75
2.3.7 Q-learning for Nonlinear System................................................................................ 76
2.4 Time-based ADP for Discrete-time Systems without Iteration............................................... 76
2.4.1 Approximation of Value Function via Critic NN ....................................................... 78
2.4.2 Approximation of Control Policy via Actor NN ........................................................ 79
2.4.3 A Special Case of Linear System ............................................................................... 79
2.5 Adaptive Dynamic Programming for Continuous-Time Systems........................................... 83
2.5.1 Continuous-Time Policy Iteration (PI) Algorithm...................................................... 85
2.5.2 Continuous-Time Value Iteration (VI) Algorithm ...................................................... 85
2.6 Adaptive Dynamic Programming for Optimal Tracking......................................................... 86
2.6.1 Optimal Tracking for Continuous-time Systems........................................................ 86
2.6.2 Optimal Tracking for Discrete-time Systems ............................................................. 88
2.6.3 Optimal Tracking for Continuous-time Systems Using Discounted Cost.................. 89
2.7 Concluding Remarks ............................................................................................................... 90
2.8 Problems.................................................................................................................................. 90
This chapter delves deep into the foundational concepts of neural network (NN)-based approxima￾tion and its application in optimal control. The core focus here is the integration of these fundamen￾tals into the broader framework of event-based optimal control, a topic to be thoroughly explored
in the subsequent chapters. We shall begin with the notion of function approximation, a significant
4748 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
feature of artificial NNs. Alongside this, the principles of dynamic programming and its adaptive
variant, referred to as adaptive/approximate dynamic programming (ADP), shall be examined in
both discrete and continuous time domains. Neural networks, particularly those with multiple layers,
are universally acknowledged for their capacity to approximate nonlinear functions. This inherent
property, when applied to learning dynamical systems, facilitates the computation of control policy
(a process known as indirect adaptive control of nonlinear systems) or enables the direct approxi￾mation of control policies. We shall briefly review the journey of neural network evolution, taking a
deep dive into their mathematical structures, universal approximation property, and training mech￾anisms. Following the discourse on neural networks, we shall focus on optimal control theory. This
field of mathematical optimization strives to synthesize a control policy that optimizes a specified
objective function and helps steer dynamical systems over time. We shall see that the “curse of di￾mensionality,” a notorious issue that precludes dynamic programming from being employed in large
state space, has led to the advent of ADP. We shall see that, as a unifying approach, ADP allows for
the design of optimal control solutions without comprehensive knowledge of system dynamics by
combining adaptive control, dynamic programming, and reinforcement learning techniques.
2.1 NEURAL NETWORK (NN) CONTROL: INTRODUCTION
In this section, we will provide a concise overview of artificial neural networks (NN), emphasizing
aspects most pertinent to their applications in closed-loop control of dynamical systems. The ma￾terial related to NN presented in this chapter are based on earlier texts by Lewis et al. (1998) and
Jagannathan (2006). Specifically, the discussion will encompass various NN topologies, key prop￾erties, and essential training techniques, all contributing to their versatility in controlling complex
systems. Control architectures will also be elaborated, elucidating how NN can be integrated into
existing control paradigms. Among the many applications, particular attention will be given to the
function approximation capability of NN. This property enables the creation of adaptive/learning￾based controllers that can model intricate, nonlinear system behaviors, making it a vital tool for
effective control. The section will equip the reader with the foundational understanding required to
explore and exploit NN’s potential in modern control systems by focusing on these elements.
In the landscape of NN, a multitude of surveys and texts have shaped the field, including those
by Lippmann (1987), Simpson (1992), Narendra and Parthasarathy (1990), and Hush and Horne
(1993). Some of the contributions are also found in the works of Haykin (1994), Kosko (1992), Kung
(1993), Levine (1991), Peretto (1992), Goodfellow et al. (2016) among others too extensive to list
here. However, a comprehensive mastery of NN’s pattern recognition and classification applications
is not indispensable for the understanding of feedback control. The focus here is narrowed to select
network topologies, fine-tuning methods, and essential properties, underscoring the importance of
the NN function approximation characteristic, as illuminated by (Lewis et al., 1999). These elements
serve as the core topics of this chapter, offering a tailored introduction to NN. For those seeking a
more in-depth exploration of the background on NN, additional insights can be sought from (Lewis
et al., 1999; Haykin, 1994; Goodfellow et al., 2016), and related sources.
In the realm of digital control, the use of NN presents a sharp contrast between closed-loop and
open-loop applications, with the latter predominantly focused on digital signal processing (DSP).
Within DSP, tasks like pattern recognition, classification, and the approximation of static functions
(possibly involving time delays) are common. Over the years, a robust methodology has been de￾veloped in DSP to choose suitable network topologies and weight configurations, ensuring reliable
performance, and the nuances of weight-training algorithms are now well understood (LeCun et al.,
2002). On the other hand, the approach to employing NN in closed-loop control for dynamic systems
has been more exploratory and less standardized. Techniques adapted from open-loop systems, such
as backpropagation for weight adjustments, are often applied in a somewhat unsophisticated but op￾timistic fashion. The challenges here are multifaceted, involving the need to dynamically evolve NN
within a feedback loop, ensuring both system stabilization and convergence of weights. LiteratureAdaptive Dynamic Programming and Optimal Control 49
on this subject typically contains limited analysis, supported mainly by simulation examples, and
practical demonstrations on hardware remain notably scarce.
Over the years, the study of NN in closed-loop control applications has matured, with an increas￾ing number of researchers offering rigorous mathematical analyses (Narendra and Parthasarathy,
1990; Lewis et al., 1998; Jagannathan, 2006). The foundation for this sophisticated exploration was
laid by Narendra and Parthasarathy (1990), Werbos (1991b), and several follow-up efforts in the
early 1990s. Lewis and associates subsequently expanded upon this ground-breaking research dur￾ing the early to mid-1990s (Lewis et al., 1998). A significant discovery emanating from this body
of work is that conventional open-loop weight-tuning algorithms, such as backpropagation or Heb￾bian tuning, need to be restructured to ensure stability and tracking in feedback control systems
(Lewis et al., 1999). This realization has deepened our understanding of NN and spurred further
investigations into their potential applications within the field of closed-loop control.
2.1.1 HISTORY OF NEURAL NETWORK
The roots of neural networks can be traced back to 1943 when McCulloch and Pitts proposed the
first mathematical model of a biological neuron, establishing a basic concept for artificial neural
networks (McCulloch and Pitts, 1943). This early work laid the groundwork for Rosenblatt’s intro￾duction of the perceptron in 1958, a model that enabled simple binary pattern recognition (Rosen￾blatt, 1958). Interest in neural networks grew in the 1960s, but was stymied by Minsky and Papert’s
“Perceptrons” in 1969. This book highlighted the limitations of single-layer perceptrons, casting
doubts on the potential of neural networks (Minsky and Papert, 1969). During the 1970s, Werbos
introduced the backpropagation algorithm for training multi-layer networks but remained largely
unnoticed until the next decade (Werbos, 1974).
The 1980s marked a resurgence in neural network research. Rumelhart, Hinton, and Williams
popularized the backpropagation algorithm, leading to new possibilities in training complex, multi￾layered networks (Rumelhart et al., 1986). LeCun and his team further extended this by apply￾ing backpropagation to convolutional neural networks (CNNs), initiating a new era in handwriting
recognition (LeCun et al., 1989). The 1990s saw the exploration of neural networks in closed-loop
control systems. Narendra and Parthasarathy began to investigate the application of neural networks
to dynamical systems (Narendra and Parthasarathy, 1990). Lewis and his team delved into neural
network control of nonlinear systems, setting a precedent for further exploration in this field (Lewis
et al., 1999).
The 2000s heralded the age of deep learning, with Hinton and Salakhutdinov’s introduction of
deep belief networks in 2006 (Hinton and Salakhutdinov, 2006). The watershed moment came in
2012 when Krizhevsky, Sutskever, and Hinton demonstrated the efficacy of deep CNNs in the Ima￾geNet competition (Krizhevsky et al., 2012). This inspired further innovations, including sequence￾to-sequence learning with recurrent neural networks in 2014 (Sutskever et al., 2014) and the de￾velopment of residual networks (ResNets) in 2015 (He et al., 2016a). Numerous other neural net￾work architectures, such as Long Short-Term Memory (LSTMs) networks, autoencoders, Genera￾tive Adversarial Networks (GANs), transformers, graph neural networks, reservoir networks, graph
convolutional networks, and more advanced generative networks like diffusion models, have been
introduced. The history of neural networks is rich and marked by periods of intense innovation,
skepticism, and resurgence. From the foundational ideas of McCulloch and Pitts to the present era
of deep learning and diffusion models, neural networks have evolved into a powerful and versatile
tool, finding applications in diverse domains.
2.1.2 NEURAL NETWORK ARCHITECTURES
Artificial neural networks (ANNs) draw inspiration from biological information processing systems,
specifically the nervous system and its fundamental unit, the neuron. Within these networks, signals50 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
are transmitted as potential differences between the interior and exterior of cells. In a neuronal cell,
where dendrites carry signals from other neurons into the soma (cell body), potentially multiply￾ing each incoming signal by a transfer weighting coefficient. The soma integrates these signals,
accumulating them at the axon hillock. When the combined signal surpasses a specific cell thresh￾old, it triggers an action potential that travels through the axon. Nonlinearities within the cell cause
the composite action potential to be a nonlinear function of the incoming signal combination. The
axon forms connections, or synapses, with the dendrites of subsequent neurons. Synapses operate
by releasing neurotransmitter chemicals across intercellular gaps, and they can either be excitatory,
promoting the firing of the next neuron, or inhibitory, hindering the firing of the next neuron.



 








Figure 2.1 Computational model of a single neuron inspired by the anatomy of a biological neuron.
2.1.2.1 Mathematical Model of a Neuron
A mathematical model of the neuron is depicted in Figure 2.1, which shows the dendrite weights v j,
the firing threshold v0 (also called the “bias”), and the nonlinear function σ(·). The cell inputs are
the n signals at the time instant k, i.e., x1(k), x2(k),x3(k),..., xn(k) and the output is the scalar y(k),
which can be expressed as the summation of weighted input signals including the bias as
y(k) = σ
 n
∑
j=1
vjxj(k) +v0

. (2.1)
Positive weights vj correspond to excitatory synapses and negative weights to inhibitory synapses.
This network was called the perceptron by Rosenblatt in 1959 (Haykin, 1994).
In continuous time the output can be expressed as a function of time y(t) and written as
y(t) = σ
 n
∑
j=1
vjxj(t) +v0

. (2.2)
The nonlinear function is known as the activation function. The activation functions are selected
specific to the applications though some common choices include sigmoid, radial basis, tangent hy￾perbolic tangent, and rectified linear functions. The activation function introduces nonlinearities to
the neuron and often used to generate outputs between [0,1], giving a probabilistic interpretation to
the NN. Sigmoid functions are a general class of monotonically nondecreasing functions taking on
bounded values between −∞ and +∞. As the threshold or bias v0 changes, the activation functions
shift left or right. For many NN training algorithms (including backpropagation), the derivative of
σ(·) is needed to make the activation function selected differentiable.
The expression for the neuron output y(k) at the time instant k (or y(t) in the case of continuous￾time) can be streamlined by defining the column vector of NN weights ¯v(k) ∈ Rn as
x¯(k) =  x1 x2 ··· xn
T , v¯(t) =  v1 v2 ··· vn
T (2.3)Adaptive Dynamic Programming and Optimal Control 51












 
 
 


 
 


Figure 2.2 One-layer neural network.
Then, it is possible to write in matrix notation
y = σ 
v¯
Tx¯

+v0 (2.4)
Note that while defining the NN output equation y in (2.4), the arguments k for discrete time and
t for continuous time are committed. Defining the augmented input column vector x ∈ Rn+1 as
x =  1 ¯xT T =  1 x1 x2 ··· xn

v =  v0 v¯
T T =  v0 v1 v2 ··· vn
T (2.5)
one may write
y = σ 
vTx

. (2.6)
Though the input vector ¯x ∈ Rn and the weight vector ¯v ∈ Rn have been augmented by 1 and v0,
respectively, to include the threshold, we may at times loosely say that x and v are elements of Rn.
The neuron output expression vector y is referred to as the cell recall mechanism. They describe
how the output is reconstructed from the input signals and the values of the cell parameters.
2.1.2.2 Single-layer Neural Network with Vector Output
Figure 2.2 shows an NN consisting of L cells, all fed by the same input signals x j, j = 1,...,n, and
producing one output y per neuron. We call this a one-layer NN. The recall equation for this network
is given by
yl = σ
 n
∑
j=1
vl jxj +vl0

l = 1,2,...,L. (2.7)
It is convenient to write the weights and the thresholds in a matrix and vector forms, respectively.
By defining the matrix of weights and the vector of thresholds as
V¯ T ≡
⎡
⎢
⎢
⎢
⎣
v11 v12 ··· v1n
v21 v22 ··· v2n
.
.
. .
.
. .
.
.
vL1 vL2 ··· vLn
⎤
⎥
⎥
⎥
⎦
bv =
⎡
⎢
⎢
⎢
⎣
v10
v20
.
.
.
vL0
⎤
⎥
⎥
⎥
⎦. (2.8)52 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
One may write the output vector y =  y0 y1 y2 ··· yL
T
as
y = σ¯

V¯ Tx¯+bv

. (2.9)
The vector activation function is defined for a vector w ≡ [w1w2 ···wL]
T
as
σ¯(w) ≡ [σ¯(w1)σ¯(w2)···σ¯(wL)]T . (2.10)
A further refinement may be achieved by inserting the threshold vector as the first column of the
augmented matrix of weights as
VT ≡
⎡
⎢
⎢
⎢
⎣
v10 v11 ··· v1n
v20 v21 ··· v2n
.
.
. .
.
. .
.
.
vL0 vL1 ··· vLn
⎤
⎥
⎥
⎥
⎦. (2.11)
Then, the NN outputs may be expressed in terms of the augmented input vector x as
y = σ¯

VTx

. (2.12)
2.1.2.3 Multi-layer Perceptron (MLP)
A two-layer ANN consists of two neuron layers (or computational layers), where the first layer
containing L neurons, known as the hidden layer, feeds into a second layer made up of m neu￾rons, known as the output layer (see Figure 2.3). This architecture is part of a category known
as multilayer perceptrons, distinguished by their enhanced computational abilities compared to the
one-layer NN. The one-layer NN is capable of performing basic digital operations like AND, OR,
and COMPLEMENT but fell out of favor in the research community after it was proven incapable
of handling the EXCLUSIVE OR (X-OR) operation, a fundamental logic operation in digital logic
design (Minsky and Papert, 1969). This limitation led to a temporary halt in NN research. Later,
it was found that the two-layer NN could effectively implement the X-OR operation, rejuvenating
interest in NNs during the early 1980s (Rumelhart et al., 1986). Notably, solutions to the X-OR
problem utilizing sigmoid activation functions were proposed by several scholars such as Hush and
Horne (1993). The diagnosis of the X-OR problem and the proposed solution approach highlighted
the concept of separability, a crucial characteristic that influences a model’s ability to categorize
inputs into distinct clusters or groups.
The output of the two-layer NN is given by the recall equation
yi = σ
 L
∑
l=1
wilσ
 n
∑
j=1
vl jx j +vl0

+wi0

i = 1,2,...,m. (2.13)
Defining the hidden-layer outputs zl allows one to write
zl = σ
 n
∑
j=1
vl jxj +vl0

l = 1,2,...,L,
yi = σ
 L
∑
l=1
wilzl +wi0

i = 1,2,...,m.
(2.14)Adaptive Dynamic Programming and Optimal Control 53

 




 





	



 

	




Figure 2.3 Two-layer neural network.
Defining first-layer weight matrices V¯ and V as in the previous subsection, and second-layer
weight matrices as
W¯ T ≡
⎡
⎢
⎢
⎢
⎣
w11 w12 ··· w1L
w21 w22 ··· w2L
.
.
. .
.
. .
.
.
wm1 wm2 ··· wmL
⎤
⎥
⎥
⎥
⎦
bw =
⎡
⎢
⎢
⎢
⎣
w10
w20
.
.
.
wm0
⎤
⎥
⎥
⎥
⎦,
WT ≡
⎡
⎢
⎢
⎢
⎣
w10 w11 w12 ··· w1L
w20 w21 w22 ··· w2L
.
.
. .
.
. .
.
.
wm0 wm1 wm2 ··· wmL
⎤
⎥
⎥
⎥
⎦,
(2.15)
one may write the NN output as,
y = σ¯

W¯ Tσ¯

V¯ Tx¯+bv

+bw

, (2.16)
or, in streamlined form as
y = σ 
WTσ 
VTx
. (2.17)
In these equations, the notation σ¯ means the vector is defined in accordance with (2.10). In (2.17),
it is necessary to use the augmented vector.
σ(w) = 
1 σ¯(w)T T = 
1 σ(w1) ··· σ(wL)
T (2.18)
where a “1 ” is placed as the first entry to allow the incorporation of the thresholds wi0 as the first
column of WT. In terms of the hidden-layer output vector z ∈ RL one may write
z¯ = σ 
VTx

y = σ 
WTz

, (2.19)
where z ≡ 
1 ¯zTT
.54 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
In the remainder of this book, we shall not show the overbar on vectors. The reader can determine
by the context whether the leading “1” is required. We shall generally be concerned in later chapters
with two-layer NN with linear activation functions in the output layer so that
y = WTσ 
VTx

. (2.20)
2.1.2.4 Linear-in-the Parameter NN
If the first-layer weights and the thresholds V in (2.20) are predetermined by some a priori method,
then only the second-layer weights and thresholds W are considered to define the NN, so that the
NN has only one layer of weights. One may then define the fixed function φ(x) = σ 
VTx

so that
such a one-layer NN has the recall equation
y = WTφ(x), (2.21)
where x ∈ Rn (recall that technically x is augmented by ” 1 ”), y ∈ Rm,φ(·) : R → RL, and L is
the number of hidden-layer neurons. This NN is linear in the NN parameters W. It is easier to train
such NN as they often lead to linear regression. This one-layer having only output-layer weights W
should be contrasted with the one-layer NN discussed in (2.12), which consists of only input-layer
weights V.
More generality is gained if σ(·) is not diagonal, for example, as defined in (2.10), but φ(·) is
allowed to be a general function from Rn to RL. This is called a functional link neural net (FLNN)
(Sadegh, 1993). We often use σ(·) in place of φ(·), with the understanding that, for linear-in-the￾parameter (LIP) nets, this activation function vector is not diagonal but is a general function from
Rn to RL. Some special FLNNs are now discussed. It is important to mention that in a class of
neural networks, called the random vector functional link (RVFL) networks (Lewis et al., 1998),
the input-to-hidden-layer weights will be selected randomly and held fixed, whereas the hidden￾to-output-layer weights will be tuned. This will minimize the computational complexity associated
with using NN in feedback control applications while ensuring that one can use NN in control.
Gaussian or Radial Basis Function Networks. The selection of a suitable set of activation
functions is considerably simplified in various sorts of structured nonlinear networks, including
radial basis functions (RBFs) and cerebellar model articulation controllers (CMAC). We shall see
here that the key to the design of such structured nonlinear networks lies in a more general set of
NN thresholds than allowed in the standard equation (2.13), and in the Gaussian or RBF (Sanner
and Slotine, 1991), given when x is a scalar
σ(x) = e−(x−μ)2/2p, (2.22)
where μ is the mean and p the variance. RBF NN can be written as (2.21), but have an advan￾tage over the usual sigmoid NN in that the n-dimensional Gaussian function is well understood
from probability theory, Kalman filtering, and elsewhere, making the n-dimensional RBF easier to
conceptualize.
The j-th activation function can be written as
σj(x) = e
−1/2

(x−μj)
T
P−1
j (x−μj)

(2.23)
with x,μj ∈ Rn. Define the vector of activation functions as σj(x) ≡ Pj = diag
pjk
, then (1.24)
becomes separable and may be decomposed into components as
σj(x) = e−1/2∑n
k=1 −(xk−μjk)
2
/Pjk =
n
∏
k=1
e
−1/2

(xk−μjk)
2
Pjk
, (2.24)Adaptive Dynamic Programming and Optimal Control 55
where xj,μjk are the k-th components of x,μj. Thus, the n-dimensional activation functions are the
product of n scalar functions. Note that this equation is of the form of the activation functions in
(2.13), but with more general thresholds, as a threshold is required for each different component of
x at each hidden-layer neuron j; that is, the threshold at each hidden-layer neuron in Figure 2.3 is
a vector. The variances pjk are typically identically selected and the offsets μjk are usually selected
with different values while designing the RBF NN and they are all left fixed; only the output-layer
weights WT are generally updated during training. Therefore, the RBF NN is a special sort of FLNN
(1.22), where φ(x) = σ(x).
2.1.2.5 Dynamic NN
The NNs that have been discussed so far contain no time-delay elements or integrators. Such NNs
are called static or non-dynamic networks as they do not have any memory. Further, the networks
that we have seen so far have connections that are unidirectional, i.e., they have signals going from
input to output via hidden layers and do not have connections that create a feedback path or loop
for signals to flow backwards toward the input. There are many different dynamic NN, or recurrent
NN, where some signals in the NN are either integrated or delayed and fed back into the network.
Hopfield Network. One well-known instance of dynamic NN is the Hopfield net, as depicted in
Figure 2.4. This specific two-layer NN design has a feedback loop, where the output, represented by
yi, is looped back into the hidden layer’s neurons (Haykin, 1994). Its first-layer weights vii are char￾acterized as the identity elemants, while the second-layer weight matrix composed of wi j assumes
a square form. The activation function employed in the output layer is linear. The hidden layer of
the Hopfield net is notable for its enhanced functionalities, including memory capabilities. These
neurons, capable of internal signal processing, are often referred to as neuronal processing elements
(NPEs) (Simpson, 1992).

 	


	





 



 



	 
Figure 2.4 Hopfield neural network.
In the continuous-time case, the internal dynamics of each hidden-layer NPE contains an integra￾tor 1/s and a time constant τi in addition to the usual nonlinear activation function σ(·). The internal
state of the NPE is described by the signal xi(t). The ordinary differential equation describing the56 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
continuous-time Hopfield net is given as
τix˙i(t) = −xi(t) +
n
∑
j=1
wi jσj(xj(t)) +ui(t) (2.25)
with the output equation
yi =
n
∑
j=1
wi jσj(xj(t)). (2.26)
This is a dynamical system of special form that contains the weights wi j as adjustable parameters
and positive time constants τi. The activation function has a subscript to allow, for instance, for
scaling terms gj as in σj(xj) ≡ σ (gjxj), which can significantly improve the performance of the
Hopfield net. In the traditional Hopfield net the threshold offsets ui are constant bias terms. It can be
seen that (2.25) has the form of a state equation in control system theory, where the internal state is
labeled as x(t). It is for this reason that we have named the offsets as ui. The biases play the role of
the control input term, which is labeled as u(t). In traditional Hopfield NN, the term “input pattern”
refers to the initial state components xi(0). In the discrete-time case, the internal dynamics of each
hidden-layer NPE contains a time delay instead of an integrator. The NN is now described by the
difference equation
xi(k +1) = pixi(k) +
n
∑
j=1
wi jσj(xj(k)) +ui(k) (2.27)
with |pi| < 1. This is a discrete-time dynamical system with time index k.
Defining the NN weight matrix WT, vectors x ≡ [x1 x2 x3 ··· xn]
T and u ≡  u1 u2 u3 ···un
T
,
and the matrices Γ ≡ diag 1
τ1
1
τ2 ··· 1
τn
!T
, one may write the continuous-time Hopfield network dy￾namics as
x˙(t) = −Γx(t) +ΓWTσ(x(t)) +Γu(t). (2.28)
Generalized Recurrent NN. A generalized dynamical system is shown in Figure 2.5. In this
figure, H(z) = C(zI − A)−1B represents the transfer function of linear dynamical system or plant
given by
x(k +1) = Ax(k) +Bu(k)
y(k) = Cx(k) (2.29)
with internal state x(k) ∈ Rn, control input u(k), and output y(k). An additional feedback of the
output via a two-layer net described by (2.16) and (2.17) can be augmented to (2.29). This dynamic
NN is described by the equation
x(k +1) = Ax(k) +B

σ 
WTσ 
VT (Cx(k) +u1(k))+Bu2(k), (2.30)
where u2(k) = u(k) and u1(k) models an input fed to the network in its feedback path. From
examination of (2.27) it is plain to see that the Hopfield net is a special case of this equation, which
is also true of many other dynamical NN in the literature. A analogous version for the continuous￾time case can also be defined as
x˙(t) = Ax(t) +B[σ(WTσ(VT (Cx(t) +u1(t)))] +Bu2(t). (2.31)
In these two cases, the properties of the matrix A are different, and often, chosen to stabilize the
respective dynamical systems.Adaptive Dynamic Programming and Optimal Control 57
 




 


 	 
  
Figure 2.5 Generalized discrete-time dynamical NN.
2.1.3 FUNCTION APPROXIMATION PROPERTY OF NN
The potential of NN in closed-loop control applications is rooted in their universal function approx￾imation capability. This fundamental attribute pertains specifically to NNs with at least two layers.
Various researchers, such as Cybenko (1989), Hornik et al. (1989), and Park and Sandberg (1991),
have extensively analyzed the approximation functionalities of NNs.
The universal approximation theorem posits that a two-layer NN can approximate any smooth
function f(x) to any desired degree of accuracy within a compact set, given the appropriate weights.
This principle has been proven to be true for different types of activation functions, including sig￾moid and hardlimit. More formally, let f(x) : Rn → Rm be a smooth function, and given a compact
set S ∈ Rn along with a positive number εN, there exists a two-layer NN such that it approximates
the function such
f(x) = WTσ 
VTx

+ε(x) (2.32)
with ε(x) = εN for all x ∈ S, for some sufficiently large number (L) of hidden layer neurons. The
value ε (generally a function of x ) is called the NN function approximation error or the reconstruc￾tion error, and it decreases as the hidden layer size L increases. We say that, on the compact set
S, as S becomes larger, the required L generally increases correspondingly. Approximation results
have also been shown for smooth functions with a finite number of discontinuities. Even though the
results says that there exists an NN that approximates f(x), it should be noted that it does not show
how to determine the required weights. It is in fact not an easy task to determine the weights so that
an NN does indeed approximate a given function f(x) closely enough. In the next section we shall
show how to accomplish this using backpropagation tuning.
2.1.3.1 Functional Link Neural Networks (FLNNs)
In Section 2.1.2.4, a special class of one-layer of NN known as Functional-Link NN (FLNN) written
as
y = WTφ(x) (2.33)
with NN output weights W (including the thresholds) and a general function φ(·) from Rn to RL was
discussed. FLNN maintain the ability to approximate functions as long as their activation functions,
denoted by φ(·), adhere to two essential requirements within a compact, simply connected set S of
Rn (Sadegh, 1993):58 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
1. A constant function on set S can be described by equation (2.33) for a finite number L of hidden￾layer functions.
2. The functional range of equation (2.33) is dense within the continuous function space from S to
Rm for a countable L.
In other words, if φ(·)forms a basis set, then a smooth function f(x): Rn → Rm can be approximated
within a compact set S of Rn, represented as
f(x) = WT φ(x) +ε(x) (2.34)
with some optimal weights and thresholds W and a specific number of hidden-layer neurons
L. Indeed, for any chosen positive number εN, a feedforward NN can be constructed to ensure
ε(x) < εN for all x in S. Barron (1993) established an essential restriction on all LIP approxima￾tors, highlighting that the error ε possesses a fundamental lower limit, constrained by expressions
on the magnitude of 1/L2/n. Consequently, the effectiveness of enlarging the number of NN inputs,
denoted by n, to boost the precision of the approximation becomes diminished as L increases. This
limitation, interestingly, is not applicable to multilayer nonlinear-in-the-parameters networks, where
the constraints of lower bounds do not impose the same restrictions.
2.1.3.2 Multilayer NN
Presently, there exists a variety of NN architectures that are adept at approximating unknown nonlin￾ear functions. Cybenko (1989) demonstrated that a M dimensional continuous function f(x) ∈C(S),
lying within a compact subset S of RN, can be closely represented by an n-layer feedforward NN.
This can be visualized in Figure 2.6, where the approximation is given by the expression:
y = f(x) = WT
n φ

WT
n−1φ(···φ(x)] +ε(x) , (2.35)
where ε(x) represents the approximation error, y = (y1,y2,..., yM)T , and φ denotes the activation
function used within the layers of the network, Wn,Wn−1,...,W2,W1 are target weights of the hidden￾to-output- and input-to-hidden-layers, respectively, and x is the input vector. The target weights are
the weights that are needed to faithfully represent the function f(x) within a desired accuracy in the
domain of approximation. The actual NN output (i.e., untrained NN output) is defined as
ˆf(x) = Wˆ T
n φˆ
n, (2.36)
where Wˆ n is the actual output-layer weight matrix. For simplicity, φ

Wˆ T
n−1φn−1(·)

is denoted as
φˆ
n. For an NN architecture with fixed number of layers and fixed number of neurons, if there exists
constant ideal weights Wn,Wn−1,...,W2,W1 such that ε(x) = 0 for all x ∈ S, then f(x) is said to be
in the functional range of the NN.
2.1.3.3 Random Vector Functional-Link Networks
The challenge of choosing activation functions in LIP NNs to serve as a basis can be tackled, as
specified earlier, by randomizing the selection of the matrix V as outlined in equation (2.21). Igel￾nik and Pao (1995) demonstrated that for RVFL nets, the resultant function φ(x) = σ 
VTx

indeed
functions as a stochastic basis. Consequently, the RVFL network is endowed with the universal
approximation property, showcasing its ability to mimic any given continuous function over a par￾ticular domain. Within this methodology, the function σ(·) may be represented by conventional
sigmoid functions. Specifically, the technique involves the random selection of activation function
scaling parameters vl j and shift parameters vl0 within the expression σ 
∑j vl jxj +vl0

. As a result,Adaptive Dynamic Programming and Optimal Control 59











 


















  




Figure 2.6 A multilayer neural network.
a family of L distinct activation functions emerges, each characterized by unique scaling and shifting
parameters. This property was elucidated by Kim (1996), who provided insights into the underlying
mechanics of such activation functions.
2.1.3.4 Event-based Approximation Properties of NNs
When the NNs are employed in a feedback control system, they are often used in the controllers
to learn the system dynamics, “value function” (introduced later in this chapter), and/or the control
inputs. These functions are typically state-dependent functions and their domain is specified by the
state space of the dynamical system. Hence, the data required to learn these functions are dependent
on the sampling and feedback availabililty, especially in the context of online learning and control.
With intermittent event-based transmission of the system state vector x, introduced in Chapter 1,
the universal NN approximation property can be extended to achieve a desired level of accuracy
by properly designing a trigger condition. The trigger condition will generate required number of
events for the availability of system state vector for approximation. The theorem introduced next
extends the approximation property of NN for event-based sampling.
Theorem 2.1. (Sahoo, 2015) Let f(x) : S → Rn, be smooth and uniformly continuous function in
a compact set for all x ∈ S ⊂ Rn. Then, there exists a single layer NN with sufficient number of
neurons such that the function f(x) can be approximated with constant weights and event-driven
activation function, such that
f(x) = WTϕ(xˆ) +εe(xˆ, es), (2.37)
where W ∈ Rl×n is the target NN weight matrix with l being the number of hidden-layer neurons,
ϕ(xˆ) is the bounded event-driven activation function, and εe(xˆ, es) is the event-driven NN recon￾struction error with x representing the last event-based sampled state held at the ZOH. ˆ
Remark 2.1. The event-based reconstruction error εe(xˆ, es) is a function of the traditional recon￾struction error ε(·) and event-trigger error es due to aperiodic feedback. A event-based recon￾struction error εe(xˆ,es) can be made small by increasing the frequency of event-based samples in60 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
addition to increasing the hidden layer neurons. This requires a suitable event-trigger condition
for obtaining both approximation accuracy and a reduction in computation. A small event-based
reconstruction error means a higher number of events, which results in more computations and
transmissions. Hence, a trade-off exists between reconstruction error and transmission when NN
are used in an event-based control scheme as function approximators.
2.1.4 NEURAL NETWORK TRAINING METHODS
In this section, we shall review the gradient-based weight tuning methods for NN. The gradient de￾scent algorithm is a one of the widely-used optimization technique to solve nonlinear optimization
problems. In the context of machine learning, it is often used to tune parameters of a model using
data in a supervised, unsupervised, or reinforcement learning paradigm. It operates by iteratively
adjusting the model’s parameters to minimize a given loss or cost or objective function. The process
begins with an initial guess for the model’s parameters and then proceeds to calculate the gradient of
the loss function with respect to the model parameters, which points in the direction of the steepest
increase in the loss. The algorithm updates the parameters in the opposite direction of the gradient,
with the learning rate controlling the step size. This process repeats until a termination criterion
is met, such as a predefined number of iterations or a desired level of loss. Geometrically, gradi￾ent descent can be envisioned as traversing a multi-dimensional loss surface, where each dimension
represents a model parameter, in search of the lowest point on the loss surface, which corresponds to
the optimal parameters for minimizing the loss. As the algorithm iteratively descends along the loss
surface, the steps become smaller, eventually converging to the minimum as the gradient approaches
zero. Gradient-based algorithm in the context of neural network training leads to the backpropoga￾tion algorithm that uses chain rule for computing the required gradients.
This part of the section is a brief summary of the material from Lewis et al. (1998). Several
variations to gradient-based optimization algorithms are available. These include variations in the
choice of learning rate, batch size (i.e., the number of data samples used during each step of the
algorithm), and the stopping criterion. For more information, see Boyd and Vandenberghe (2004);
Lewis et al. (1998); LeCun et al. (2002).
2.1.4.1 Gradient Descent Tuning in Continuous Time
In continuous time, the gradient descent algorithm for a single-layer neural network Lewis et al.
(1998) can be expressed as
V˙(t) = −η
∂E(t)
∂V(t)
, (2.38)
where E(t) denotes the defined cost function at time t and η > 0 is the learning rate or step-size.
Given an input-output pair (X,Y) targeted by the NN, the output error can be formulated as
e(t) = Y −y(t) = Y −σ 
VT (t)X

. (2.39)
Using this error, a cost function can be formulated, for example, as the least-squares output-error
cost given by
E(t) = 1
2
eT (t)e(t) = 1
2
tr
e(t)eT (t)

. (2.40)
The trace of a square matrix tr{·} is defined as the sum of the diagonal elements. Disregarding
the derivative of σ(·) (or equivalently, assuming linear activation functions), the continuous-time
perceptron training rule emerges as
V˙(t) = ηXeT (t). (2.41)Adaptive Dynamic Programming and Optimal Control 61
Continuous time batch updating techniques are recommended in scenarios involving multiple in￾put/output patterns.
2.1.4.2 Gradient Descent Tuning in Discrete Time
Given the input-output pair (X,Y) that the NN should associate, define the NN output error vector
as
e(k) = Y −y(k) = Y −σ 
VT(k)X
 (2.42)
and the least-squares output-error cost as
E(k) = 1
2
eT(k)e(k)− 1
2
tr
e(k)eT(k)
 (2.43)
In terms of matrices, the gradient descent algorithm is
V(k +1) = V(k)−η
∂E(k)
∂V(k)
. (2.44)
Writing the cost function explicitly, we have
E(k) = 1
2
tr 
Y −σ 
VT(k)X
Y −σ 
VT(k)X
T!
, (2.45)
where e(k) is the NN output error associated with input vector X using the weights V(k) determined
at iteration k. Assuming linear activation functions σ(·), one has
E(k) = 1
2
tr 
Y −VT(k)X
Y −VT(k)X
T!
. (2.46)
Then the gradient can be computed as
∂E(k)
∂V(k) = −XeT(k), (2.47)
So that the gradient descent tuning algorithm is written as
V(k +1) = V(k) +ηXeT(k), (2.48)
which updates both the weights and the thresholds.
2.1.4.3 Backpropagation Training
Backpropagation training refers to a fundamental algorithm for training ANNs. It is used to optimize
the parameters of the NN, such as weights and biases, to minimize a cost function. It is a gradient￾based optimization algorithm and uses chain rule to compute the gradient of the cost or loss function
with respect to weights in the hidden layers. The algorithm works in a two-phase process. In the
forward pass, the input data is fed through the network to make predictions. In the backward pass, the
gradient of the loss function with respect to the network’s parameters are computed using the chain
rule, beginning from the output layer and moving backward through the hidden layers. This gradient
represents the direction and magnitude of parameter adjustments needed to reduce the prediction
error. The parameters are updated using gradient descent method. This process is iteratively repeated
for numerous training examples until the network’s performance converges to a satisfactory level.62 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Backpropagation essentially distributes the error signals backward through the network, enabling it
to learn and adapt its internal representations to make more accurate predictions.
Consider a two-layer NN as in (2.13). The forward recursion becomes
z = σ 
VTX

y = σ 
WTz
 (2.49)
and the backward recursion is
e = Y −y
δ 2 = diag{y}(1−diag{y})e
δ 1 = diag{z}(1−diag{z})Wδ 2
(2.50)
Then the computation of the updates for the NN weights and thresholds for the two-layer NN can
be obtained as
W = W +ηz(δ 2
i )
T , (2.51)
V = V +ηX(δ 1
l )
T . (2.52)
As with the gradient-descent algorithm, the NN weights and thresholds are typically initialized
to small random (positive and negative) values. The backpropogation training is only guaranteed
to find a local minimum, provided the step size or the learning rate parameter η is chosen ap￾propriately. The backpropogation algorithm in continuous time can be derived similar to the dis￾crete time algorithm presented in this section. For example, if we index the recursive relation as
Wk+1 = Wk +ηkz(δ 2
i )T , we may view this recursive equation as a first-order Euler approximation of
the update rule represented in continuous time, i.e., the first-order approximation of the differential
equation W˙ (t) = ηz(δ 2
i )T . For details, see Lewis et al. (1998).
Up to this point, we have explored fundamental concepts related to ANNs and their training
processes. Moving forward, we will delve into optimal control techniques, encompassing dynamic
programming and reinforcement learning methods. The subsequent content in this chapter draws
upon Lewis et al. (2012a,b); Bertsekas (2012) for its foundation.
2.2 OPTIMAL CONTROL AND DYNAMIC PROGRAMMING
Optimal control theory is a branch of mathematical optimization that deals with finding a control
for a dynamical system over a period of time such that an objective function is optimized. It has
numerous applications in science, engineering, economics, and more. The development of optimal
control theory dates back to the 17th century. One of the first problems that led to the concept of
optimal control was the brachistochrone problem posed by Johann Bernoulli in 1696 (Struik, 1986).
In this problem, one must find the shape of the curve down which a bead sliding from rest and
accelerated by gravity will slip (without friction) from one point to another in the least time. In the
18th century, optimal control took a significant step forward with the development of the calculus
of variations by mathematicians like Euler and Lagrange (Kline, 1990). The calculus of variations
(Liberzon, 2011; Boltyanskii et al., 1961; Weinstock, 1974) is a field of mathematical analysis that
uses variations to find maxima and minima of functionals.
The modern development of optimal control theory started in the 1950s with the work of
Lev Pontryagin (Pontryagin et al., 1962), Richard Bellman (Bellman, 1966), and Rudolf Kalman
(Kalman, 1960a). Russian mathematician Lev Pontryagin and his students developed the maximum
principle (referred to as Pontryagin’s maximum principle) (Pontryagin et al., 1962), which is con￾sidered one of the cornerstones of optimal control theory. On the other hand, Richard Bellman
introduced the principle of optimality and dynamic programming (Bellman, 1966), a method forAdaptive Dynamic Programming and Optimal Control 63
Table 2.1
Backpropogation Algorithm
Forward Recursion to Compute NN Output z = σ
"
∑n
j=0 v jXj
#
;  = 1,2,...,L
yi = σ 
∑L
=0wiz

;i = 1,2,...,m
with x0 = z0 = 1 and Yi is the desired output at the output neuron i
Backward Recursion for Backpropagated Errors ei = Yi −yi;i = 1,2,...,m
δ 2
i = yi(1−yi) ei; i = 1,2,...,m
δ 1
 = z (1−z)∑m
i=1wiδ 2
i ;  = 1,2,...,L
Computation of the NN Weight and Threshold Updates wi = wi +ηzδ 2
i ;i = 1,2,...,m;  = 0,1,...,L
v j = v j +ηXjδ 1
 ;  = 1,2,...,L; j = 0,1,...,n
solving complex problems by breaking them down into simpler subproblems. This approach forms
the basis for many algorithms in optimal control and search, including those used to control discrete
event systems. In the early 1960s, Rudolf Kalman introduced the Kalman filter, which is an optimal
recursive Bayesian filter for estimating the internal state of a linear dynamic system from a series
of noisy measurements (Kalman, 1960b). In the latter half of the 20th century, optimal control the￾ory was further developed and refined and saw an increasing range of applications from economics
(optimal growth theory, optimal saving, etc.) (Lucas and Stokey, 1983; Ramsey, 1928), engineering
(optimal control of industrial processes, etc.) (Kirk, 2004; Lewis et al., 2012b; Liberzon, 2011; Ku￾mar and Varaiya, 1986) to computer science (optimal routing, resource allocation, etc.) (Bertsekas,
2012). Optimal control theory continues to be an area of active research with recent developments,
including in stochastic control, robust control, and applications to complex systems. Despite its
theoretical complexity, it is a fundamental tool for engineers, economists, and computer scientists
alike.
A significant amount of research on optimal control using dynamic programming and approx￾imate dynamic programming has been conducted in the discrete-time domain. Therefore, in this
section, we shall start with the discrete-time case and then review the continuous-time case. For
detailed treatments, see Bertsekas (2012); Lewis et al. (2012b,a).
2.2.1 DISCRETE-TIME OPTIMAL CONTROL
To formalize the concept of dynamic programming from a dynamical system perspective, let us
consider a discrete-time dynamical system described by the equation
xk+1 = f(xk,uk), (2.53)
where k is the discrete time step, xk ∈ Rn is the state at time k, uk ∈ Rm is the control input at time
k, and f : Rn × Rm → Rn is a function describing the dynamics of the system. A special class of
nonlinear dynamics can be expressed in the affine state space difference equation form, given by
xk+1 = f (xk) +g(xk)uk. (2.54)64 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
The goal of the optimal control problem is to find a sequence of control inputs u0,u1,...,uN−1 that
minimizes the cost functional
J(x0) = Φ(xN) +
N−1
∑
k=0
r(xk,uk), (2.55)
subject to the dynamics of the system (2.54), where Φ : Rn → R is the terminal cost function,
r : Rn × Rm → R is the stage cost function, and N is the horizon length (since N is finite, it is
referred to as a finite-horizon optimal control problem). As a special case, by letting N → ∞, the
performance index can be rewritten as
J(x0) =
∞
∑
k=0
r(xk,uk), (2.56)
which leads to infinite horizon optimal control problem. The function r(xk,uk) (in both finite and
infinite horizon cases), often referred to as the utility function, serves as a metric to evaluate the
one-step expense of control. It can be tailored to reflect various optimization objectives, such as
minimizing fuel consumption, reducing energy usage, mitigating risk, and so forth. For example,
one common representation of this function is the quadratic energy form given by
r(xk,uk) = xT
k Qxk +uT
k Ruk. (2.57)
In some cases, it might be expressed more generally as
r(xk,uk) = Q(xk) +uT
k Ruk. (2.58)
It is necessary to ensure that Q(xk) and R are positive definite to ascertain that the cost function
remains properly defined. The system is presumed to be stabilizable within a specified set Ω ∈ Rn.
In other words, there exists a control policy
uk = μ (xk), (2.59)
ensuring that the closed-loop system defined by
xk+1 = f (xk) +g(xk)μ (xk) (2.60)
exhibits asymptotic stability within Ω.
Remark 2.2. A control strategy is referred to as admissible if it accomplishes stabilization and
results in a determinate cost J (xk).
We refer to the cost or value of any admissible strategy uk = μ (xk) as V (xk). Strategies yielding
lower values are considered superior. It is worth highlighting that the value of any given admissible
strategy can be ascertained by examining the infinite sum denoted as (2.56). Optimal control theory
revolves around the determination of a control policy μ(xk) that yields the minimum possible cost,
leading to
V∗ (xk) = min
μ(·)
 ∞
∑
i=k
r(xi,μ (xi))
, (2.61)
referred to as the optimal cost or optimal value. The associated control policy that achieves this
minimal cost is represented by:
u∗
k = μ∗ (xk) = argmin
μ(·)
 ∞
∑
i=k
r(xi,μ (xi))
. (2.62)Adaptive Dynamic Programming and Optimal Control 65
where u∗
k = μ∗(xk) is the optimal control policy. In other words, the goal is to identify a function that
maps states to control actions such that the sum of one-step costs is minimized across the system’s
evolution.
A planner with a short-sighted or myopic perspective would focus solely on reducing the imme￾diate one-step cost or utility. Yet, the actual challenge extends beyond minimizing this immediate
cost to include the minimization of the cumulative sum of all the one-step costs, commonly re￾ferred to as the cost-to-go. Addressing this challenge precisely for general nonlinear systems can
often be very difficuly, and in some cases, unattainable. To alleviate the complexities of this opti￾mization problem, diverse methods and approaches have been devised. In the following sections,
we will discuss two approaches: 1) dynamic programming and 2) adaptive/approximate dynamic
programming.
2.2.2 DYNAMIC PROGRAMMING
Mathematician Richard Bellman developed the concept of dynamic programming in the 1950s
(Bellman, 1966). He described the method as follows: “An optimal policy has the property that
whatever the initial state and initial decision are, the remaining decisions must constitute an opti￾mal policy with regard to the state resulting from the first decision.” In simpler terms, the principle
says that if a particular route is optimal, then every sub-route must also be optimal. For example, if
the fastest route from city A to city C is via city B, then the fastest route from city B to city C must
be a part of the fastest route from city A to city C. In the context of an optimal control problem with
the cost-to-go defined as in (2.54), if u∗ = {u∗
0,u∗
1,...,u∗
N−1} is an optimal control sequence for this
problem with the associated states at each k given by {x0,x∗
1,...,x∗
N−1, x∗
N}, then for every k and for
every x∗
k , the remaining part of the sequence u∗ = {u∗
k ,u∗
k+1,...,u∗
N−1} must be an optimal control
sequence for the problem starting from state x∗
k at time k.
Additionally, the Bellman’s principle of optimality naturally leads to a recursive formulation
of the value or the cost-to-go function. If V∗(x) is the optimal value function which gives us the
minimum cost-to-go from state x, and u is the control input, then
V∗(xk) = min
uk
{r(xk,uk) +V∗(xk+1)}, (2.63)
where r(xk,uk) is the cost at time k, and xk+1 is the state at the next time step, which results from
applying control uk at state xk. This equation says that the optimal cost-to-go from a state xk under
an optimal policy is the optimal one-step-cost at this state plus the cost-to-go from the next state
under the optimal policy.
This principle leads to the definition of the value function V(xk) recursively, giving rise to the
optimization problem for each time k. The value function can be computed recursively, backward￾in-time, using dynamic programming with the relation
V(xk) = min
uk
{r(xk,uk) +V(xk+1)} (2.64)
In particular, if we have the solution to the subproblems, often referred to as the tail subproblems,
i.e., the optimal policy from xk+1, xk+2, ..., x∞, then the optimal policy from xk to x∞ can be com￾puted using (2.63). The dynamic programming algorithm uses the solutions to the tail subproblems,
that is, the value functions V(xk+N), to compute the solution of the subproblem from xk+N−1. This
solution is then used step by step, working backward in time, to address the current problem, which
is to determine the value function V(xk) from xk. Given the value functions, the optimal control
sequence can be computed by choosing the control input at each time step that minimizes the right￾hand side of the dynamic programming equation
u∗
k = argmin uk
{r(xk,uk) +V(xk+1)}. (2.65)66 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Example 2.1. (Lewis et al., 2012b) Consider a linear discrete-time plant given by
xk+1 = xk +uk. (2.66)
The finite horizon performance index is defined as
J0 = x2
N +
N−1
∑
k=0
u2
k , (2.67)
where the final time index is N = 2. The feasible/admissible control inputs are
uk ∈ {−1,−0.5,0,0.5,1}, (2.68)
and the feasible states are given by
xk ∈ {0,0.5,1.0,1.5}. (2.69)
Find an admissible control sequence u∗
0 and u∗
1, that minimizes J0 while resulting in an admissible
state trajectory x∗
0, x∗
1, and x∗
2.
Solution: We will use the principle of optimality in (2.63) to solve this problem. We will consider
only the admissible control inputs in (2.68) that result in admissible states xk+1 in (2.69).
If x0 = 0, the admissible u0 are u0 = 0,0.5, and 1. To see this, for x0 = 1 let us evaluate x1 for
each of these control inputs as follows
x1 = 0+0 = 0,
x1 = 0+0.5 = 0.5,
x1 = 0+1 = 1.
Note that u0 = −1 and u0 = −0.5 are not admissible control since they results in states x1 = −1 and
x1 = −0.5, respectively, which are not admissible.
If x0 = 0.5, the admissible u0 are −0.5,0, and 0.5. To see this, for x0 = 0.5 let’s evaluate x1 as
follows x1 = 0.5−0.5 = 0
x1 = 0.5+0 = 0.5
x1 = 0.5+0.5 = 1
The control input u0 = −1 is not an admissible control input since it results in the state x1 = −0.5
that is not feasible.
For x0 = 1, the admissible u0 are −1,−0.5,0, and 0.5. For x0 = 1, the feasible next state x1 are
computed for each feasible control input as follows
x1 = x0 +u0 = 1−1 = 0
x1 = 1−0.5 = 0.5
x1 = 1+0 = 1
x1 = 1+0.5 = 1.5
The control input u0 = 1 is not admissible.
Similarly, if x0 = 1.5, the admissible u0 are −1,−0.5,0, and 0.5, and the possible next states x1
are x1 = x0 +u0 = 1.5−1 = 0.5
x1 = 1.5−0.5 = 1
x1 = 1.5+0 = 1.5Adaptive Dynamic Programming and Optimal Control 67
The control inputs u0 = 0.5,1 are not admissible.
Using the Bellman equation, an admissible cost can be expressed as
Jk = 1
2
u2
k +J∗
k+1. (2.70)
Then, the optimal cost by the Bellman principle of optimality is given by
J∗
k = min
uk
(Jk). (2.71)
To compute the optimal policy u∗
k and J∗
k for each xk using (2.71), we need to solve it backward from
k = N.
Step 1: Given N = 2, the final cost
J∗
N = x2
N = x2
2 (2.72)
must be calculated for each admissible state with the value xN. For the final states x2 =
0,0.5,1.0, and 1.5, the terminal costs respectively are J∗
2 (x2 = 0) = 0, J∗
2 (x2 = 0.5) = 0.25, J∗
2 (x2 =
1.0) = 1, and J∗
2 (x2 = 1.5) = 2.25.
Step 2: One step backward, i.e., k = 1, for each possible state x1 and for each admissible cost u1,
we will use (2.70) to compute the incurred cost as follows
J1(x1 = 1.5,u1 = −1) = 1
2
u2
1 +J∗
2 (x2 = x1 +u1) = 1
2
(−1)
2 +J∗
2 (x2 = 0.5) = 0.5+0.25 = 0.75
J1(x1 = 1.5,u1 = −0.5) = 1
2
(−0.5)
2 +J∗
2 (x2 = 1) = 0.125+1 = 1.125
J1(x1 = 1.5,u1 = 0) = 1
2
(0)
2 +J∗
2 (x2 = 1.5) = 0+2.25 = 2.25
From the above J1, the value of u∗ with x1 = 1.5 is u∗ = −1 and J∗
1 = 0.75. Now for x1 = 1.0
and admissible control policies u1 = 0.5,0,−0.5,−1, we can compute the J1 similarly. The smallest
value of J1 = 0.375, which occurs for u1 = −0.5. Hence, if x1 = 1.0, then u∗ = −0.5 and J∗
1 = 0.375.
Computing in a similar fashion, if x1 = 0.5 , then u∗ = −0.5 and J∗
1 = 0.125 and if x1 = 0, then u∗ = 0
and J∗
1 = 0.
Step 3: Decrement k = 0. For x0 = 1.5 and admissible control u0 = 0,−0.5,−1, the associated
cost J0 = 1
2 u2
0 +J∗
1 , where J∗
1 is the optimal cost calculated in the Step 2 for x1 = 1.5+u0. Then, we
have for x0 = 1.5, J∗
0 = 0.5 and u∗
0 = −0.5
x0 = 1.0, J∗
0 = 0.25 and u∗
0 = −0.5
x0 = 0.5, J∗
0 = 0.125 and u∗
0 = 0 or −0.5
x0 = 0.0, J∗
0 = 0 and u∗
0 = 0
Note that the optimal control for the state x0 = 0.5 is not unique.
From the above computation, the optimal control sequence that minimizes the cost J and results
in admissible states can be easily found. For example, if x0 = 1, then the optimal control sequence
will be u∗
0 = −0.5 and u∗
1 −0.5, which will result in an optimal state trajectory of x∗
0 = 1.0, x∗
1 = 0.5,
and x∗
2 = 0. The optimal cost will be J∗
0 = 0.25.
The principle laid down by Bellman establishes a backward-in-time procedure that is essential for
solving the optimal control problem, as the optimal policy at time k+1 must be known to determine
the optimal policy at time k as per equation (2.64). This algorithm, by its very nature, functions as
offline planning methods. A prominent example of such a method within feedback control design
is employing the Riccati equation in solving the Linear Quadratic Regulator (LQR) problem. This
process demands an offline resolution of the Riccati equation, assuming the known dynamics of the
system.68 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
2.2.3 LIMITATIONS OF DYNAMIC PROGRAMMING
Dynamic programming is a powerful methodology for solving a variety of optimization problems,
particularly in the realm of control theory and operations research. However, it also comes with
several limitations:
1. The Curse of Dimensionality: The computational and memory requirements of dynamic
programming grow exponentially with the dimensionality of the state space and action space (or
the set of admissible control inputs). This is known as Bellman’s “curse of dimensionality”. For a
system with n state variables, discretizing each variable into m points would require mn points in the
state space. Thus, the number of computations and memory required scales exponentially, making
dynamic programming computationally intractable for high-dimensional problems.
2. Discretization: Dynamic programming typically requires discretizing the state and/or control
spaces. This can introduce approximation errors. Additionally, choosing the right discretization can
be problem-specific and challenging.
3. Full Knowledge Requirement: Dynamic programming assumes that the model of the system
(the transition function f and the cost function L) are known perfectly. In many real-world problems,
this assumption is not valid. The model might be unknown, partially known, or it might change over
time.
Despite these limitations, there are ways to address or mitigate them:
1. Model-free methods: If the model of the system is not known perfectly, one can use model￾free methods, which do not require a system model. Examples of such methods include reinforce￾ment learning techniques like Q-learning and SARSA.
2. Approximation Methods: These methods try to reduce the impact of the curse of dimen￾sionality by approximating the value function or the policy with simpler functions. Some common
approximation methods include linear function approximation, neural networks, and other machine
learning techniques. These methods are often referred to as adaptive/approximate dynamic program￾ming.
2.3 ADAPTIVE DYNAMIC PROGRAMMING FOR DISCRETE-TIME SYSTEMS
Dynamic programming provides a backward-in-time optimal solution procedure, making it suitable
for offline planning but unsuitable for online learning. Derived from the Bellman equation (2.63),
several iterative methods have been developed for learning the solution to the optimal control equa￾tion without the need to solve the so-called Hamilton-Jacobi-Bellman (HJB) equation. These meth￾ods include Policy Iteration and Value Iteration. In this section we shall see how these methods
can be transformed into online real-time reinforcement learning techniques. We shall see that these
methods can efficiently tackle the optimal control problem using data collected along system trajec￾tories. Key references in this domain include works by Sutton and Barto (1998), Bertsekas (2012),
Werbos (1974, 1989, 1991a, 1992), and others (see Lewis et al. (2012a)).
ADP or neurodynamic programming (NDP) is an extension of dynamic programming (DP),
addressing DP’s limitations by employing approximation structures and real-time learning. It origi￾nated in the 1960s with the concept of reinforcement learning, a notion proposed by Minsky (Min￾sky and Papert, 1969) in the field of artificial intelligence. The integration of DP and reinforcement
learning into neurocontrol during the late 1980s laid the groundwork for ADP, culminating in the
development of deep reinforcement learning in the 2010s (Werbos, 1991a). ADP’s unique capability
to handle systems with uncertainties or unknown dynamics makes it a significant advancement in
optimal control, leveraging function approximators like NN (Sutton and Barto, 1998).
Reinforcement Learning (RL), a machine learning paradigm where an agent learns through in￾teractions with its environment, serves as the foundational learning mechanism in ADP (Sutton
and Barto, 1998). In the context of ADP, the RL agent learns optimal control policies by approxi￾mating an optimal value function, adapting to feedback, and maximizing cumulative rewards. ThisAdaptive Dynamic Programming and Optimal Control 69
powerful amalgamation of DP, RL, and function approximation makes ADP a versatile and promis￾ing methodology with extensive applications in robotics, power systems, traffic control, and more
(Bertsekas, 2012; Werbos, 1992).
2.3.1 VALUE AND POLICY ITERATION-BASED ONLINE SOLUTION
Unlike traditional DP methods that require offline design and knowledge of the system dynamics
f(x), g(x), RL allows for online learning in real-time, making them particularly useful when the
exact system dynamics are not known. By leveraging the fundamental concept that the Bellman
optimality equation act as fixed point equations, we can develop methods that operate forward-in￾time. This direction facilitates the solution of the optimal control problem via learning, without
utilizing or needing the system models.
To see this, let us start with a given admissible control policy uk = μ (xk) that results in a value
V (xk). Guided, by equation (2.64), we can derive a new policy from this value by employing the
following operation:
μ
(xk) = argmin
μ(·)
(r(xk,μ (xk)) +V (xk+1)). (2.73)
Bertsekas (2012) demonstrated that the new policy μ(xk) represents an enhancement as it has
a value V(xk) that is either less than or equal to the original value V (xk). This phenomenon is
referred to as the one-step improvement property of rollout algorithms, meaning that (2.73) is an
improved policy. Repeating this procedure iterative can systematically improve the control strategy
and move closer to an optimal solution. This leads to an iterative method known as Policy Iteration
for determining the optimal control policy. This concept has been extensively discussed in works
such as Sutton and Barto (1998), and Bertsekas (2012).
2.3.1.1 Policy iteration
The policy iteration algorithm is an iterative procedure that involves two key steps: policy evaluation
and policy improvement. The algorithm is presented below.
1. Initialization: Start with an arbitrary control policy μ0(xk).
2. Policy Evaluation: For the current policy μj(xk), j denotes the iteration number, compute
the value function Vj(xk) by using the Bellman equation, which is also a consistency condition that
satisfies
Vj(xk) = r(x(t),μj(xk)) +Vj(xk+1). (2.74)
3. Policy Improvement: Compute an improved policy μj+1(xk) by solving the following mini￾mization problem
μj+1(xk) = argmin
μ(·)
{r(xk,μ(xk)) +Vj+1(xk+1)}. (2.75)
If the system dynamics is in affine form as in (2.54) and utility function r(xk,uk) is in quadratic
form as in (2.57), the policy improvent can be computed in one-shot as
μj+1(xk) = −1
2
R−1gT (xk)
∂Vj+1(xk+1)
∂ xk+1
. (2.76)
4. Check for Convergence: If the policy μj+1(xk) is the same as the policy μj(xk) (or close
enough), then stop the iterative process. The current policy and value function are optimal in the
sense that they cannot be improved further to yield a better cost. Otherwise, go back to step 2
(Policy Evaluation) with the updated policy.
Remark 2.3. The policy iteration method requires an initial admissible policy (μ0(xk)). The con￾vergence of this algorithm to the optimal value and control policy—equivalently, to the solution of70 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
equations (2.63) and (2.65) has been established under certain conditions. This finding has been
corroborated by several researchers, as detailed in Bertsekas (2015).
This algorithm provides an effective way to solve the optimal control problem. However, it is
worth mentioning that it can be computationally expensive since it requires solving the system of
Bellman equations in the policy evaluation step for each policy.
2.3.1.2 Value Iteration
The value iteration algorithm is an iterative method that computes the value function and the optimal
control policy by solving the Bellman equation.
1. Initialization: Begin by choosing any control policy μ0 (xk). This policy can be chosen arbi￾trariy and does not need to be a stabilizing policy.
2. Value Update Step: Update the value by employing the following equation
Vj+1 (xk) = r(xk,μj(xk)) +Vj(xk+1). (2.77)
3. Policy Improvement Step: Identify an enhanced policy utilizing
μj+1 (xk) = argmin
μ(·)

r(xk,μ (xk)) +Vj+1 (xk+1)

. (2.78)
An interesting aspect to observe is that, unlike the policy iteration, the old value is applied on
the right-hand side of the equation in (2.74). It has been theoretically demonstrated that the value
iteration converges under certain conditions. One essential difference between value iteration and
policy iteration is that value iteration does not necessitate an initial stabilizing policy. Value itera￾tion’s foundation rests on the idea that the Bellman optimality equation in (2.63) also represents a
fixed point equation. The alternating steps of value updating and policy improvement serve as the
iterative method for the contraction map associated with the equation (2.63).
A critical distinction between the two iterative algorithms lies in the complexity of their respec￾tive solutions. Policy iteration mandates the resolution of the equation in (2.74) at every stage, a
nonlinear Lyapunov equation, posing challenges for general nonlinear systems. Conversely, value
iteration relies on solving equation (2.77), a mere recursion, making it more computationally ac￾cessible. Lastly, it is worth noting that the fixed-point equation can be the underlying principle
for online RL algorithms, given appropriate structuring. These algorithms have the ability to learn
through observation of data gathered along the system’s trajectories.
2.3.2 TEMPORAL DIFFERENCE AND VALUE FUNCTION APPROXIMATION
To realize RL-based algorithms for solving optimal control problems, one must deal with two key
ideas: temporal difference (TD) error and value function approximation (VFA).
2.3.2.1 Temporal Difference (TD) error
We have seen that given an infinite horizon cost function (2.56), we can rewrite the value function
as
V(xk) = r(xk,uk) +
∞
∑
i=k+1
r(xi,ui), (2.79)
where r(xk,uk) is the cost to go at time instant k and the second term involving the summation in
(2.79) is the cost from k +1 to infinity. We have also seen that this can be rewritten as a difference
equation given by
V(xk) = r(xk,μ(xk) +V(xk+1), V(0) = 0, (2.80)Adaptive Dynamic Programming and Optimal Control 71
where uk = μ(xk) is the current control policy. The Bellman equation (2.80) is a version of the
Lyapunov equation. From (2.80), the discrete-time Hamiltonian can be expressed as
H(xk,μ(xk),ΔVk) = r(xk,μ(xk) +ΔVk, (2.81)
where ΔVk = V(xk+1)−V(xk).
Remark 2.4. The Hamiltonian function, commonly encountered in the context of Pontryagin’s max￾imum principle, captures the cost function along with the constraint expression, which is often intro￾duced by the system dynamics, using a Lagrangian multiplier or costate. In (2.81), the Hamiltonion
captures the one-step-cost, r(xk,μ(xk)), and the constraint, which is given by the first difference of
the value function ΔVk = V(xk+1)−V(xk). We shall see that in the continuous-time, this translates
to ∂V(x(t))
∂t = ∂VT (x(t))
∂ x(t)
dx(t)
dt , where ∂VT (x(t))
∂ x(t) forms the Lagrange multiplier.
As discussed in the previous section, the Bellman equation provides a backward-in-time solution
approach. To solve the problem online, forward-in-time, one can define an error equation using the
Hamiltonian function (2.81) given by
ek = r(xk,μ (xk)) +V (xk+1)−V (xk). (2.82)
The error ek is referred to as the temporal difference (TD) error. The TD error becomes zero when the
Bellman equation is satisfied, which implies that the Hamiltonian becomes zero or the consistency
condition is satisfied. Algorithmically, this also implies that, given a fixed control policy u = μ(x),
one could seek to solve the equation ek = 0 for each time step k. This results in the determination of
a value function V(·), which represents the solution to the TD equation satisfying
0 = r(xk,μ (xk)) +V (xk+1)−V (xk). (2.83)
Solving (2.83) leads to the best estimate of the value related to the current policy application equiv￾alently to the evaluation of the infinite sum in the cost function (2.56). The TD error is thus inter￾pretable as a measure of discrepancy between anticipated and actual system performance following
a chosen action. It acts as a metric to assess the prediction error in estimating the system’s future
states under the influence of the current policy. This error can be used to guide how the policy is
updated to better align with the optimal trajectory. However, this needs to be accomplished with￾out detailed knowledge of system dynamics, relying solely on the data gathered along the system
trajectories. This task presents a significant challenge, particularly for general nonlinear systems,
where the complexity of the system dynamics makes the TD equation notoriously difficult to solve.
Note that in the policy iteration, the TD error is completely reduced to zero in the policy evaluation
step while in the value iteration, the TD error is reduced (but not eliminated) in each step before the
policy is updated.
2.3.2.2 Value Function Approximation
A practical approach to solving the TD equation involves approximating the value function V(·)
through a parametric approximator. This method is often synonymously called approximate dy￾namic programming (ADP) (Werbos, 1974, 1989, 1991a, 1992) and neurodynamic programming
(NDP) (Bertsekas, 2012). An NN-based function approximator, introduced in Section 2.1.3, is uti￾lized to approximate the value function V(xk) in ADP/NDP algorithms.
Recalling the universal approximation property of NNs, the value function in a compact set can
be written as
V(xk) = WT φ(xk) +ε(xk), (2.84)
where W ∈ Rl is the unknown target NN weights, φ(xk) ∈ Rl is the activation function which forms
a basis for approximation, and ε(xk) ∈ R is the approximation error with l denoting the number of72 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
neurons in the output layer. Note that we can use a NN with three or more layers for the approxima￾tion, and it is a design choice.
The estimated value function can be represented as
Vˆ(xk) = Wˆ T φ(xk), (2.85)
where Wˆ is the estimated weights, tuned during the learning process. Substituting the value function
estimates from (2.85), the TD error (2.82) becomes
ek = r(xk,μ (xk)) +Wˆ T φ (xk+1)−Wˆ T φ (xk). (2.86)
The equation ek = 0 represents a fixed point equation, acting as a consistency equation that holds
true at every time instance k for the value V(·), which corresponds to the prevailing policy u = μ(x).
Consequently, iterative methods can be employed to solve the TD equation, by updating the weights
(Wˆ ) using optimization algorithms including the backpropogation to minimize the error ek.
2.3.3 ONLINE POLICY AND VALUE ITERATION WITH APPROXIMATED VALUE
FUNCTION
By utilizing the TD error and the value function approximation, we shall see that the policy and
value iteration algorithms can be formulated so as to obtain the optimal policies online and forward￾in-time.
2.3.3.1 Online Policy Iteration
1. Initialize. Choose any admissible control policy μ0 (xk).
2. Policy Evaluation Step. Calculate the least-squares solution Wˆ j+1 with the following equation
Wˆ T
j+1 (φ (xk)−φ (xk+1)) = r(xk,μj(xk)). (2.87)
3. Policy Improvement Step. Ascertain an improved policy using the equation
μj+1 (xk) = argmin
μ(.)

r(xk,μ (xk)) +Wˆ T
j+1φ (xk+1)

. (2.88)
If the utility has the special quadratic form and the dynamics are given by an affine expression, then
the policy improvement step looks like
μj+1 (xk) = −1
2
R−1gT (xk)∇φ T (xk+1)Wˆ j+1, (2.89)
where ∇φ(x) = ∂ φ(x)
∂ x ∈ Rl×n is the Jacobian of the activation function vector.
Remark 2.5. The estimated weights for the value function Wˆ j+1 can be computed using least
squares (LS) (Ljung, 1999; Soderstr ¨ om and Stoica, 1989; Goodwin and Sin, 2014), recursive LS ¨
(Ljung, 1999; Soderstr ¨ om and Stoica, 1989; Goodwin and Sin, 2014), or gradient-based algorithms ¨
(Astrom and Wittenmark, 1995). For recursive least squares (RLS) and gradient-based algorithms, ¨
the regression vector must satisfy the persistency of excitation (PE) condition (Narendra and An￾naswamy, 2012) to ensure convergence.
After the weights for the value function are estimated, the control policy is updated according
to equations (2.88) or (2.89). Subsequently, the procedure (policy evaluation and improvement) is
repeated again for the next step, j+1. This entire process continues until convergence to the optimal
control solution, which is the approximate solution to equations (2.63) and (2.65). This results in an
online RL algorithm capable of solving the optimal control problem while gathering data along the
system trajectories. In a similar fashion, an online RL algorithm can also be derived based on value
iteration.Adaptive Dynamic Programming and Optimal Control 73
2.3.3.2 On-Line Value Iteration Algorithm
1. Initialize. Select any control policy μ0 (xk), not necessarily admissible or stabilizing.
2. Value Update Step. Determine the least-squares solution Wˆ j+1 to
Wˆ T
j+1φ (xk) = r(xk,μj(xk)) +Wˆ T
j φ (xk+1). (2.90)
3. Policy Improvement Step. Determine an improved policy using
μj+1 (xk) = −γ
2
R−1gT (xk)∇φ T (xk+1)Wˆ j+1. (2.91)
Note that the old weight parameters are on the right-hand side of (2.90). Thus, the regression vector
is now φ (xk), which must be persistently exciting for convergence of RLS.
Additionally, Werbos (1974, 1989, 1991a, 1992) introduced four foundational methods of ADP.
He defined a version of RL that learns the scalar value function Vh (xk) via heuristic dynamic pro￾gramming (HDP). The extension of HDP, known as action-dependent HDP (AD-HDP), was pre￾sented as Q-learning for discrete-state Markov decision processes (MDP) by Watkins (Watkins,
1989). AD-HDP learns the Q-function, which is also a scalar function, and enables RL without
using the model of the system dynamics. Dual heuristic programming (DHP) involves the online
learning of the costate function λk = ∂V(xk)
∂ xk , which is an ‘n’-vector gradient and carries more infor￾mation than the scalar value. The final method, action-dependent DHP (AD-DHP), is founded on
learning the gradients of the Q-function.
2.3.4 ACTOR-CRITIC ARCHITECTURE
In the previous section, the we reviewed how a value function can be approximated using an NN.
This NN is often referred to as the critic network, in the online policy iteration and value iteration.
However, the implementation of the equation (2.88) for a nonlinear system can be challenging, as
the control is implicitly integrated, given that xk+1 is reliant on μ(·) and is used as an argument for
a nonlinear activation function. These issues are addressed by incorporating a second NN dedicated
to the control policy, commonly referred to as the actor network (Werbos, 1974, 1989, 1991a, 1992;
Barto et al., 1983). Consequently, the actor-network has a parametric approximator structure given
by
uk = μ (xk) = Vˆ Tσ (xk), (2.92)
where σ(x) : Rn → RM represents a vector of M activation functions and Vˆ ∈ RM×m is a matrix of
weights or unidentified parameters.
Once the critic NN parameters converge to Wj+1 in the policy iteration or value iteration steps,
it becomes necessary to execute the policy update steps denoted by equations (2.88) and (2.91). A
gradient descent method (Lewis et al., 2012b) can be employed to update the actor weights Vˆ , as
given below
Vˆ i+1
j+1 =Vˆ i
j+1 −β σ (xk)
"
2R

Vˆ i
j+1
T
σ (xk) +g(xk)
T ∇φ T (xk+1)Wj+1
#T
. (2.93)
Here, β > 0 serves as a tuning parameter, and the tuning index i increases along with the time index
k.
2.3.5 Q-LEARNING
In the realm of HDP or value function learning, knowledge of system dynamics is required, as shown
by equations (2.89) and (2.91). At a minimum, the knowledge of the input coupling function g(·)74 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
(the matrix B in the case of the linear system) is necessary. This need arises from the minimization
operation shown in equation (2.88)
μj+1(xk) = argmin
μ(·)

r(xk,μ(xk)) +WT
j+1φ(xk+1)

. (2.94)
Given the cost-to-go r(xk,uk) = Q(xk) + uTRuk, to compute ‘argmin’ in (2.94), differentiate with
respect to the control to yield
0 = ∂
∂uk
[Q(xk) +uT
k Ruk] + ∂
∂uk
WT
j+1φ(xk+1)
=2Ruk +
∂ φ(xk+1)
∂uk
T
Wj+1
=2Ruk +
∂ xk+1
∂uk
T
∇φ(xk+1)
TWj+1.
(2.95)
However, when evaluating ∂ xk+1
∂uk = g(xk), the system input matrix g(·) is required. Even when a
secondary actor NN is used, the function g(·) is needed to adjust the actor NN weights, as per
(2.93).
To avoid the need for knowledge of the system dynamics, an alternative method for taking partial
derivatives with respect to the control input must be conceived, without going through the system.
Werbos employed the concept of backpropagation through time using AD-HDP to achieve this.
Watkins (1989) introduced similar concepts for discrete-space MDPs, termed as Q-learning. Given
the Bellman equation (2.80), the value of any given admissible policy μ(·) can be computed. The
optimal control is determined using equations (2.65). Thus, the Q-function or the quality function
associated with policy u = μ(x) is defined as
Q(xk,uk) = r(xk,uk) +V(xk+1). (2.96)
The optimal Q-function is then given by
Q∗(xk,uk) = r(xk,uk) +V∗(xk+1). (2.97)
Utilizing Q∗, the Bellman optimality equation can be expressed in a simplified form
V∗(xk) = min
u Q∗(xk,u), (2.98)
and the optimal control policy can be written as
μ∗(xk) = argminu Q∗(xk,u). (2.99)
The minimum value can be found by solving
∂
∂u
Q∗(xk,uk) = 0. (2.100)
Unlike (2.95), (2.100) does not require any derivatives involving system dynamics. Assuming
the Q-function is known for all (xk,uk), there is no need to compute ∂ xk+1
∂uk . Value function learning
(or HDP) necessitates learning and storing the optimal value for all possible states xk. On the other
hand, Q-learning requires storing the optimal Q-function for all possible values of (xk,uk), i.e., for
all possible control actions taken in each possible state, demanding more information storage. The
following discussion will address how to manage this using Q-function approximation for applying
online RL techniques.Adaptive Dynamic Programming and Optimal Control 75
Applying online RL techniques to learn the Q-function requires
1. A fixed point equation in terms of the Q function to use the TD learning, and
2. An appropriate parametric approximator structure for the Q-function approximation (QFA).
To establish a fixed point equation for the Q-function, note that Q(xk,μ(xk)) =V(xk). This allows
the construction of a Bellman equation for Q-function as
Q(xk,uk) = r(xk,uk) +Q(xk+1,μ(xk+1)), (2.101)
or equivalently,
Q(xk,μ(xk)) = r(xk,μ(xk)) +Q(xk+1,μ(xk+1)). (2.102)
The optimal Q-value, denoted by Q∗, satisfies
Q∗(xk,uk) = r(xk,uk) +Q∗(xk+1,μ∗(xk+1)). (2.103)
Thus, (2.101) serves as a fixed point equation or the ‘Bellman equation’ in terms of the Q-function,
similar to (2.80). With this, any online RL method previously discussed can be used as the founda￾tion for realizing AD-HDP, including the policy iteration and value iteration.
2.3.6 Q- LEARNING FOR LINEAR QUADRATIC REGULATOR
We shall now derive a Q-learning based optimal controller for a linear system. Consider the linear
quadratic regulator (LQR), where the one-step-cost is defined as the sum of quadratic function of
state and control inputs. With this, we shall examine the choice of suitable approximator structures
for the QFA.
The Q-function for the LQR is given by equation (2.96)
QK (xk,uk) = xT
k Qxk +uT
k Ruk +xT
k+1Pxk+1, (2.104)
where P is the solution to an associated Lyapunov equation for the policy K (Lewis et al., 2012b).
Therefore, we have
QK (xk,uk) = xT
k Qxk +uT
k Ruk + (Axk +Buk)
T P(Axk +Buk), (2.105)
which can be rewritten in matrix form as
QK (xk,uk) =  xk
uk
T  Q+ATPA BTPA
ATPB R+BTPB   xk
uk

≡ z
T
k Hzk. (2.106)
This equation represents the Q-function for the LQR case. It is quadratic in (xk,uk). Using the
Kronecker product, we can rewrite this equation as
QK (xk,uk) = H¯ T z¯k (2.107)
where H¯ = vec(H) and ¯zk = zk ⊗ zk =
 xk
uk

⊗
 xk
uk

. The term ¯zk is the quadratic basis set
composed of the components of the state and the control input. Then, the fixed point equation
(2.101) becomes
H¯ T zk = xT
k Qxk +uT
k Ruk +H¯ T zk+1 (2.108)
with uk = −Kxk.
Define the Q-function in terms of state xk and control uk as
QK (xk,uk) = z
T
k Hzk =
 xk
uk
T  Hxx Hxu
Hux Huu   xk
uk

. (2.109)76 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
This decomposes the matrix H into four submatrices: Hxx, Hxu, Hux, and Huu. Then equation (2.100)
gives us
0 = Huxxk +Huuuk, (2.110)
which can be rearranged to solve for uk, the control action at step k,
uk = −(Huu)
−1 Huxxk. (2.111)
Here, the matrix H can be acquired or learned through online RL, and thus this step does not ne￾cessitate knowledge of the system dynamics. Interestingly, if we apply (2.100) to (2.106), we get K
as
K = 
R+BTPB−1
BTPA, (2.112)
which is the same as a traditional LQR controller (Lewis et al., 2012b).
2.3.7 Q-LEARNING FOR NONLINEAR SYSTEM
In nonlinear systems, we can make informed assumptions about the structure of our Q-function ap￾proximation. The Q-function can be perceived as a parametric mapping of state and action variables
(x, u), characterized by a set of weights W and a chosen basis function φ. This can be implemented
using NN or other approximation schemes. The approximated Q-function can therefore be repre￾sented as
Q(xk,uk) = WT φ(xk,uk) = WT φ(zk), (2.113)
where zk = [xT
k uT
k ]
T . Building on this approximation, we can calculate the TD error. The TD error
can be expressed as
ek = r(xk,μ (xk)) +WT φ (zk+1)−WT φ (zk). (2.114)
With this TD error, we can apply policy iteration or value iteration to continuously learn the weight
vector in an online manner.
Furthermore, with the critic NN defined as in (2.113), our aim is to identify the point at which
the derivative of the Q-function with respect to the control action u equals zero
∂
∂u
Q(xk,u) = ∂
∂u
WT φ (xk,u) = 0. (2.115)
Again, due to the nature of the NN structure that inherently includes the control action u (a feature
of AD-HDP), these derivatives can be calculated without the knowledge of the system dynamics.
Nonetheless, formulating an explicit policy uk = μ (xk) by finding a solution for u is not a straightfor￾ward task. This necessitates the application of the implicit function theorem tailored to this specific
neural network structure. This framework is flexible and facilitates the use of both policy itera￾tion and value iteration techniques for Q-learning, thus offering diverse pathways to learn optimal
solutions.
2.4 TIME-BASED ADP FOR DISCRETE-TIME SYSTEMS WITHOUT ITERA￾TION
In the previous section, the approximate solution to optimal control problems using iterative ap￾proaches (policy and value iteration) using fixed-point equations is presented. Although these it￾erative approaches guarantee optimality at every time instant, one of the limitations is the number
of iterations required (to solve policy evaluation and/or policy improvement steps) within a sam￾pling instant to converge to the fixed-point solution. This renders the real-time implementation of
these schemes difficult. To address this issue, Dierks and Jagannathan (2012b) presented an optimal
control framework for nonlinear discrete-time systems, which learns the Hamilton-Jacobi-BellmanAdaptive Dynamic Programming and Optimal Control 77
(HJB) equation and the corresponding optimal control policy truly online and forward in-time using
NN without incorporating policy and value iterations.
Consider an affine nonlinear discrete-time system that can be characterized by
xk+1 = f(xk) +g(xk)u(xk) (2.116)
where xk ∈ Rn, f(xk) ∈ Rn, and g(xk) ∈ Rn×m, adhering to the condition g(xk)F ≤ gM. Here,
·F is the Frobenius norm. The control input is represented as u(xk) ∈ Rm. The internal dynamics,
denoted by f(xk), are treated as unknown, whereas the input coefficient matrix, g(xk), is known.
An optimal control policy for (2.116) should result in a control sequence u(xk) that minimizes the
infinite horizon cost function defined by
J(xk) =
∞
∑
i=0
r(xk+i,u(xk+i)) (2.117)
for all xk, where r(xk,u(xk)) = Q(xk) +u(xk)TRu(xk), and Q(xk) ≥ 0 and R ∈ Rm×m is a symmetric
positive definite matrix. Here Q(xk) is a penalty function on the state and is not related to the Q￾function.
The Bellman equation (2.80), using the value function, can be written as
V(xk) = r(xk,u(xk)) +V(xk+1) = r(xk,u(xk)) +V(f(xk) +g(xk)u(xk)) (2.118)
It is necessary for J(xk) = 0 for xk = 0, which allows J(xk) to serve as a Lyapunov function. Fur￾thermore, the control policy u(xk) must assure that (2.117) is finite, making u(xk) admissible. By
leveraging the Bellman’s principle of optimality (2.63), it can be established that the infinite horizon
optimal cost function, denoted by V∗(xk), is time-invariant and satisfies the discrete-time Hamilton￾Jacobi-Bellman (HJB) equation (Lewis et al., 2012b)
V∗(xk) = min
u(xk)
(r(xk,u(xk)) +V∗(f(xk) +g(xk)u(xk))). (2.119)
By invoking the stationarity condition (Lewis et al., 2012b), the optimal control u∗(xk) that mini￾mizes V∗(xk) can be computed. The stationarity condition can be expressed as
∂V∗(xk)
∂u(xk) = 2Ru(xk) +g(xk)
T
∂V∗(xk+1)
∂ xk+1

= 0, (2.120)
which leads to the optimal control expressed as
u∗(xk) = −1
2
R−1g(xk)
T
∂V∗(xk+1)
∂ xk+1

. (2.121)
Despite the complete knowledge of the system dynamics, the optimal control (2.121) generally
remains inaccessible for nonlinear discrete-time systems due to its reliance on the future state vector
xk+1 at the current time instant k. As discussed in the previous sections, value and policy iteration￾based strategies are often employed to overcome this limitation. The actor-critic approach employs
two NNs: a critic NN to learn the HJB equation and an action NN to learn the control policy that
minimizes the estimated cost function. Leveraging the approximation property of NNs, Jagannathan
(2006), representations for the cost function (2.118) and feedback control policy (2.121) can be
expressed using NNs (over a compact set S) as follows
V(xk) = ΦTσ(xk) +εJk(xk) (2.122)
and
u(xk) = ΘTϑ(xk) +εuk(xk), (2.123)78 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
where Φ and Θ represent the constant target NN weights, while εJk(xk) and εuk(xk) are the bounded
state-dependent approximation errors for all xk ∈ S. The activation function vectors for the critic
and action NN are denoted by σ(•) and ϑ(•), respectively, and they are independent. In the time￾based ADPs, the authors Dierks and Jagannathan (2012b) defined the upper bounds for the target
NN weights are set as Φ ≤ ΦM and ΘF ≤ ΘM, where ΦM and ΘM are positive constants. The
approximation errors were constrained to be |εJk| ≤ εJM and εuk ≤ εuM, where εJM and εuM are
positive constants (Jagannathan, 2006). Lastly, the gradient of the approximation error was also
assumed to be bounded above as ∂ εJk/(∂ xk+1)F ≤ ε
JM, where ε
JM is another positive constant.
These bounds were used to analyze the convergence properties of the time-based ADP algorithm
in control applications. See Dierks and Jagannathan (2012b); Xu (2012); Sahoo (2015) for more
details.
2.4.1 APPROXIMATION OF VALUE FUNCTION VIA CRITIC NN
Similar to other approximation-based methods, the time-based ADP algorithm also uses the value
function approximation using a critic NN and is denoted as
Vˆ
k (xk) = Φˆ T
k σ (xk), (2.124)
where Vˆ
k (xk) is the approximation of the cost function and Φˆ k represents the estimate of the target
NN weight vector, Φ. The subscript k indicates the time index, denoting the association between
the approximated cost function Vˆ
k (xk) and Φˆ k within the same time frame. To ensure Jˆ(0) = 0 is
satisfied, the basis function must fulfill the condition σ(0) = 0 when x = 0.
Next, define the residual or cost-to-go (CTG) error as
eVk = r(xk−1,u(xk−1)) +Φˆ T
k Δσ (xk−1), (2.125)
where Δσ (xk−1) is σ (xk)−σ (xk−1). This CTG error (2.125) can be understood as the time-shifted
TD error, discussed in the previous section, with its dynamics expressed as
eVk+1 = r(xk,u(xk)) +Φˆ T
k+1 (σ (xk+1)−σ (xk)). (2.126)
Following this, an auxiliary CTG error vector can be defined as
EVk = Yk−1 +Φˆ T
k Xk−1 ∈ R1×(1+j)
, (2.127)
where Yk−1 consists of r(xk−1,u(xk−1)) through r(xk−1−j,u(xk−1−j)) and Xk−1 consists of Δσ(xk−1)
through Δσ(xk−1−j) with j < k−1. The auxiliary vector (2.127) represents the previous j+1 CTG
errors, recalculated using the most recent Φˆ k. The parameter j is a design choice, but it must satisfy
j ≤ L−1, where σ (xk) ∈ RL. Then the dynamics of the auxiliary vector are defined as
ET
Vk+1 = YT
k +XT
k Φˆ k+1. (2.128)
Now define the cost function NN weight update to be
Φˆ k+1 = Xk

XT
k Xk
−1 
αJET
Vk −YT
k

, (2.129)
where 0 < αJ < 1, and substituting (2.129) into (2.128) reveals
ET
Vk+1 = αJET
Vk
. (2.130)
It can be observed that the NN weight update reveals that the time-evolution of the CTG error can
be controlled by the choice of the parameterAdaptive Dynamic Programming and Optimal Control 79
Remark 2.6. Observing the definition of the cost function (2.122) and its NN approximation
(2.124), it is evident that both become zero only when xk = 0. Thus, once the system states become
zero, the cost function approximation is no longer updated. This can be viewed as a PE requirement
for the inputs to the cost function NN wherein the system states must be persistently exiting long
enough for the NN to learn the optimal cost function. This PE requirement ensures the existence of
a constant α, such that α ≤ 
XT
k Xk


F (Jagannathan, 2006).
Remark 2.7. Implementation of the time-driven ADP-based optimal control scheme does not use
policy or value iterations, which are commonly used in offline optimal control training (Al-Tamimi
et al., 2008). Instead, online learning and time-based policy updates are used here by using both
state measurements and its time history.
2.4.2 APPROXIMATION OF CONTROL POLICY VIA ACTOR NN
To derive the control policy that minimizes the approximated cost function (2.124), an action NN is
employed to approximate (2.121), and is given by
uˆ(xk) = Θˆ T
k ϑ (xk), (2.131)
where Θˆ k is the estimated value of the ideal weight matrix Θk. The basis function, ϑ(•), is designed
to satisfy ϑ(0) = 0, which guarantees uˆ(0) = 0, a requirement for admissibility. Subsequently,
the action error, the difference between the feedback control applied to (2.116) and the control signal
that minimizes the estimated cost function (2.124), is defined as
u˜(xk) = Θˆ T
k ϑ (xk) + 1
2
R−1gT (xk)
∂ σ (xk+1)
∂ xk+1
T
Φˆ k. (2.132)
To drive the action error to zero, the control NN weight update was proposed by Dierks and Jagan￾nathan (2012b) as
Θˆ k+1 = Θˆ k −αu
ϑ (xk)u˜
T
k
ϑT (xk)ϑ (xk) +1
, (2.133)
where 0 < αu < 1 is a positive design parameter, controlling the rate of change of the control NN
weight matrix.
Remark 2.8. To calculate the action error (2.132) and implement the NN weight update (2.133),
knowledge of the input transformation matrix g(xk) is required. However, knowledge of the internal
dynamics f (xk) can be avoided.
Dierks and Jagannathan (2012b) also derived sufficient condition to warrant the uniform ultimate
boundedness of the critic and action NN weight estimation errors using Lyapunov theory. Further,
the estimated control input (2.131) was shown to approache the optimal control signal with a small
bounded error. This error is expected to be a function of the NN reconstruction errors εJ and εu.
If both the NN approximation errors are considered to be negligible (Chen and Jagannathan, 2008;
Al-Tamimi et al., 2008) as in the case of standard adaptive control (Jagannathan, 2006) or when
the number of hidden-layer neurons is significantly increased (Jagannathan, 2006), the estimated
control policy will approach the optimal control asymptotically as shown by Dierks and Jagannathan
(2012b). In summary, time-based ADP facilitates real-time online learning of the optimal control
policy. It accomplishes this by integrating historical data throughout the system’s trajectory.80 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
2.4.3 A SPECIAL CASE OF LINEAR SYSTEM
Consider a linear discrete-time system as the special case of the nonlinear system in (2.116) given
by
xk+1 = Axk +Bu(xk). (2.134)
For the infinite horizon cost function (2.117), the Hamiltonion function reveals a discrete algebraic
Riccati equation (ARE) given by
P = ATPA+Q−ATPB
R+BTPB−1
BTPA (2.135)
with J∗ (xk) = xTPx (Lewis et al., 2012b). In addition, the optimal control policy (2.121) is obtained
as
u∗ (xk) = −K∗xk = −
R+BTPB−1
BTPAxk. (2.136)
In order to show that the time-driven ADP is useful for linear systems, the cost function J (xk) =
xT
k Pxk and the feedback control policy (2.136) have NN representations as
J (xk) = ΦTσ (xk) = ΦT x¯k (2.137)
and
u(xk) = ΘTϑ (xk) = ΘT xk (2.138)
respectively, where Φ, Θ, σ(•), and ϑ(•) are as defined in (2.122) and (2.123). Further, Φ =
vec(P) and Θ = −K∗T with vec(•) denoting a vector function that transforms an n×n matrix into a
column vector by stacking the columns of the square matrix into a one column with the off-diagonal
elements summed as Pi j +Pji. The term ¯xk = 
x2
1,..., x1xn,x2
2,x2x3,...,xn−1xn,x2
n

is the Kronecker
product quadratic polynomial basis vector. Note that the NN reconstruction errors, εJk and εuk, are
negligible for the case of linear systems as the baisis functions are the exact regression functions.
It is demonstrated by Dierks and Jagannathan (2012b) that the NN estimation errors, Φ˜ and Θ˜ ,
and the CTG error and action error, eJk and ˜u(xk), respectively, converge to zero asymptotically
as k → ∞ when εJk and εuk are zero. This implies that there exists a time, ka, such that for all
k > ka,Φˆ = Φ = vec(P),Θˆ = Θ = −K∗T , EJk = 0, and Jˆ(xk) = Φˆ T x¯k = xTPx = J (xk). Under
these conditions, the NN cost function update (2.129) is rewritten as
Φˆ k+1 = Φ = −Xk

XT
k Xk
−1
YT
k . (2.139)
Therefore, multiplying both sides of (2.139) by XT
k and taking the transpose of both sides yields
ΦTXk = −Yk which can be expanded as
ΦT 
x¯k+1 −x¯k x¯k −x¯k−1 ... x¯k+1−j −x¯k−j

= 
r(xk,u(xk)) r(xk−1,u(xk−1)) ... r

xk−j,u

xk−j
 . (2.140)
Then, by using ΦT x¯k = xT
k Pxk and r(xk,u(xk)) = xT
k Qxk+ uT (xk)Ru(xk), (2.140) can be rewritten
as
⎡
⎢
⎢
⎢
⎣
xT
k Pxk
xT
k−1Pxk−1
.
.
.
xT
k−j
Pxk−j
⎤
⎥
⎥
⎥
⎦ =
⎡
⎢
⎢
⎢
⎣
xT
k Qxk +uT (xk)Ru(xk) +xT
k+1Pxk+1
xT
k−1Qxk−1 +uT (xk−1)Ru(xk−1) +xT
k Pxk
.
.
.
xT
k−j
Qxk−j +uT 
xk−j

Ru
xk−j

+xT
k+1−j
Pxk+1−j
⎤
⎥
⎥
⎥
⎦. (2.141)
Now, using (2.134) and Θ = −K∗T allows (2.141) to take the form of the equation (as shown
at the bottom of the page), or zT diag
P−Q−K∗TRK∗ −
AT −K∗TBT 
P(A−BK∗)

z = 0 where
z =  xT
k xT
k−1 ··· xT
k−j
T
. Since z > 0 by the PE condition, it follows that
P−Q−K∗TRK∗ −
AT −K∗TBT 
P(A−BK∗) = 0. (2.142)Adaptive Dynamic Programming and Optimal Control 81
After rearranging this expression and recalling K∗ = 
R+BT PB)−1BTPA, (2.142) can be rewritten
as
P = Q+K∗TRK∗ +
AT −K∗TBT 
P(A−BK∗)
= ATPA+Q−ATPB
R+BTPB−1
BTPA, (2.143)
which is nothing but the discrete-time ARE (DARE) shown in (2.135). Therefore, the NN update
(2.129) indeed solves the matrix Lyapunov equation for linear systems.
⎡
⎢
⎢
⎢
⎣
xT
k Pxk
xT
k−1Pxk−1
.
.
.
xT
k−j
Pxk−j
⎤
⎥
⎥
⎥
⎦ =
⎡
⎢
⎢
⎢
⎣
xT
k Qxk +xT
k K∗TRK∗xk +xT
k

AT −K∗TBT 
P(A−BK∗)xk
xT
k−1Qxk−1 +xT
k−1K∗TRK∗xk−1 +xT
k−1

AT −K∗TBT 
P(A−BK∗)xk−1
.
.
.
xT
k−j
Qxk−j +xT
k−j
K∗TRK∗xk−j +xT
k−j

AT −K∗TBT 
P(A−BK∗) xk−j
⎤
⎥
⎥
⎥
⎦
(2.144)
Example 2.2. Consider the linear system whose dynamics are given by
xk+1 =
 x1k+1
x2k+1

=
 0 −0.8
0.8 1.8

xk +
 0
−1

u(xk). (2.145)
Design and simulate an optimal control policy that minimizes the performance index (2.117), where
Q(xk) = xT
k xk and R = 1 using time-based ADP.
Using a quadratic cost function (2.117) with Q(xk) = xT
k xk and R = 1, the optimal control input
can be found by solving the associated DARE and revealed to be u∗ (xk)=[0.6239 1.2561] xk
while the optimal cost function is found to be J∗ (xk) = 1.5063x2
1k +2.0098x1kx2k +3.7879x2
2k. To
obtain this solution through time-based ADP, we shall begin by selecting the initial stabilizing policy
for the algorithm to be u0 (xk) =  0.5 1.4 
xk. Generate the basis functions for the critic from a
sixth-order polynomial as
 
x2
1,x1x2,x2
2, x4
1, x3
1x2,...,x6
2
!
and the action network basis functions can be chosen as the system states as we are working with a
linear system.
The initial critic NN weight estimates may be set to zero at the beginning of the simulation while
the initial weight estimates of the action network should reflect the initial stabilizing control. For the
results presented in the following, the simulation was run for 240 time steps. The design parameters
for the critic and action networks were selected as aJ = 10−6, au = 0.1. With these parameters, the
time-based ADP algorithm was implemented to learn the optimal policy and value for the linear
quadratic regulation problem and the final values of the critic and actor NN weights are shown as
follows
Φˆ =  1.5071 2.0097 3.7886 −0.0082 −0.0015
0.0025 0.0030 −0.0014 0.0020 0.0000···
0.0000 0.0008 −0.0003 0.0009 −0.0002 
and
Θˆ =  0.6208 1.2586 0.0589 −0.0338 0.0095 ···
0.0092 −0.0049 0.0074 0.0050 0.0075 −0.0054 
.
Examining the final values for the NN weights and comparing them to the target parameters shown
above in u∗ (xk) and J∗ (xk), it is clear that they have successfully learned the optimal values with
small bounded error. Additionally, the difference between the optimal control law obtained from
the DARE and the optimal control learned online is shown in Figure 2.7. This figure shows that82 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
control input generated by the time-based online ADP scheme converges to the target optimal con￾trol scheme. It can be observed that the NN-based adaptive optimal control law converged to the
optimal value with a small bounded error within the first 200 time steps. Finally, Figure 2.7 (right
panel) illustrates that the system state trajectories for the initial stabilizing control that is not optimal
and the final online optimal control law.
Figure 2.7 Convergence of action errors and state trajectories.
Example 2.3. (Dierks and Jagannathan, 2012b) Consider a nonlinear system defined by
xk+1 =
 x1k+1
x2k+1

=
 −x1kx2k
x2
1k +1.8x2k

+
 0
−1

u(xk). (2.146)
Design and simulate an optimal control policy that minimizes the performance index (2.117) as
defined in Example 2.2 using time-based ADP.
The initial stabilizing policy for the algorithm was selected to be u0 (xk) =  −0.4 1.24 
xk,
while the basis functions for the cost function approximator were generated as a sixth-order polyno￾mial, and the control network basis functions were generated from the gradient of the cost function
basis vector. The design parameters for the cost function and control NN were selected as aJ = 10−6
and au = 0.1, while the cost function approximator NN weights were set to zero at the beginning
of the simulation. The initial weights of the action control network were chosen to reflect the initial
stabilizing control. The simulation was run for 375 time steps, and the time history of the NN weight
estimates are shown in Figure 2.8 (a), and the action error is shown in Figure 2.8 (b). Examining
Figure 2.8 (a), it is clear that all NN weights remain bounded while Figure 2.8 (b) shows that the
control signal error converges to a small bounded region around the origin.
As a comparison, the SDRE algorithm was implemented along with the offline training algorithm
presented in (Chen and Jagannathan, 2008). Figure 2.9 (a) shows the state trajectories when the
final optimal control policies learned online, trained offline, and using the discrete SDRE solution,
respectively, are applied to the nonlinear system. From the plot shown in Figure 2.9 (a), it is clear
that the resulting state trajectories for the online learning and offline training solutions are identical.
However, the SDRE solution differs from online and offline HJB-based solutions since SDRE is a
suboptimal approximation. Thus, SDRE is an attractive alternative for nonlinear optimal control,
however, the resulting control laws are suboptimal even when the exact dynamics are known.
Figure 2.9 (b) displays the cost functions associated with the final optimal control policy learned
online, the final optimal control policy found via offline training (Chen and Jagannathan, 2008), and
the SDRE control policy, which confirms that the cost associated with the final control policy learned
online is on par with the final control policy trained offline as the two curves are indistinguishableAdaptive Dynamic Programming and Optimal Control 83
Figure 2.8 (a) NN weights and (b) control signal error for the nonlinear system.
Figure 2.9 (a) Convergence of state trajectories and (b) comparison of near-optimal cost functions.
from each other. In addition, the sub-optimality of the SDRE approach is once again illustrated.
Also, this figure indicates the total cost for each policy, which was calculated according to
JTotal =
kfinal
∑
k=0
J (xk).
Examining the total cost values, the total costs for the online and offline policies are nearly the same,
while the total cost of the SDRE approach is higher, as expected.
2.5 ADAPTIVE DYNAMIC PROGRAMMING FOR CONTINUOUS-TIME SYS￾TEMS
The task of applying RL to continuous-time (CT) systems is a significantly more challenging en￾deavor compared to its discrete-time counterparts. To understand this, we shall consider a CT non￾linear dynamical system in an affine form
x˙(t) = f(x) +g(x)u(t), x(0) = x0, (2.147)
where x(t) ∈ Rn is the state vector and the control input is denoted by u(t) ∈ Rm. To solve an
optimal control problem for the CT system, we shall begin by making the standard assumptions84 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
to ensure the existence of unique solution to the initial value problem and an equilibrium point at
x = 0. This includes the requirement for f(0) = 0 and the Lipschitz continuity of f(x) +g(x)u on a
subset Ω ⊆ Rn that includes the origin. We further assume that the system can be stabilized on Ω by
a continuous control function u(t), rendering the closed-loop system asymptotically stable on Ω.
We represent the concept of goal-oriented optimal behavior by defining a cost function or per￾formance measure associated with the feedback control policy u = μ(x) as
V(x(t)) =  ∞
t
r(x(τ),u(τ))dτ, (2.148)
where the utility function r(x,u) = Q(x)+uTRu is employed. The function Q(x) is positive definite,
that is, Q(x) > 0 for all x = 0 and Q(x) = 0 for x = 0. The matrix R ∈ Rm×m is also positive definite.
Here a control input is said to be an admissible control if it stabilizes the system and ensure that the
integral (2.148) is well-defined.
If the cost is smooth, it leads to a nonlinear Lyapunov equation, an infinitesimal equivalent to
equation (2.148), which can be derived by differentiation as
0 = r(x,μ(x)) + (∇V)
T (f(x) +g(x)μ(x)),V(0) = 0, (2.149)
where ∇V (a column vector) represents the gradient of the cost function V with respect to x. The
equation (2.149) known as the CT Bellman equation, is defined based on the CT Hamiltonian func￾tion
H (x,μ(x),∇V) = r(x,μ(x)) + (∇V)
T (f(x) +g(x)μ(x)). (2.150)
Now using the stationarity condition, we may defined the feedback control policy by solving
∂H(x,μ(x),∇V)
∂ μ(x) = 0 for μ(x) using (2.150). Substituting the μ(x) back into (2.150) will yield the
so-called Hamilton-Jacobi-Bellman (HJB) equation.
At this juncture, we can see the immediate challenge with CT systems. When we compare the
CT Bellman Hamiltonian (2.150) to the discrete-time Hamiltonian (2.81), we notice that the former
includes the full system dynamics f(x) +g(x)u, which is not the case with the latter. Therefore, the
CT Bellman equation (2.149) cannot be leveraged as the basis for RL unless the full dynamics are
available. Observing that
0 = r(x,μ(x)) + (∇V)
T (f(x) +g(x)μ(x)) = r(x,μ(x)) +V˙ . (2.151)
Euler’s method can be employed (Baird, 1994) to discretize this to obtain
0 = r(xk,uk) + V (xk+1)−V (xk)
T ≡ rS (xk,uk)
T +
V (xk+1)−V (xk)
T . (2.152)
Here, the sample period is represented by T, such that t = kT. The discrete sampled utility is
rS (xk,uk) = r(xk,uk)T, where it is important to multiply the CT utility by the sample period. It
is alo noteworthy that the discretized CT Bellman equation (2.152) bears the same structure as the
DT Bellman equation (2.80). Consequently, many early RL methods developed for discrete-time
systems can be deployed. Baird (1994), based on this, defined advantage learning as a way of
enhancing the conditioning of RL for sampled CT systems. He observed that if the utility is not
discretized appropriately, the discrete-time solutions do not converge to the CT solutions as T ap￾proaches zero. However, it’s important to recognize this as only an approximation. Alternatively,
in many works that followed (Lewis et al., 2012a), the HJB equation was treated as a consistency
condition that must be satisfied by the approximated value function and control policy to learn
the optimal value function iteratively. Doya (2000) studied the CT RL problem directly using the
Hamiltonion expression (2.150). An alternate exact method for CT RL was proposed by Vrabie
et al. (2009a). One can write the cost in the integral reinforcement form
V(x(t)) =  t+T
t
r(x(τ),u(τ))dτ +V(x(t +T)). (2.153)Adaptive Dynamic Programming and Optimal Control 85
This equation forms the exact CT analogue of the DT Bellman equation (2.80) for any T > 0.
Bellman’s principle of optimality specifies that the optimal value is structured as follows (Lewis
et al., 2012b)
V∗(x(t)) = min
u¯(t:t+T)
 t+T
t
r(x(τ),u(τ))dτ +V∗(x(t +T))
, (2.154)
where ¯u(t : t +T) signifies the control set {u(τ) : t ≤ τ < t +T}. The optimal control can be repre￾sented as
μ∗(x(t)) = argmin
u¯(t:t+T)
 t+T
t
r(x(τ),u(τ))dτ +V∗(x(t +T))
. (2.155)
Vrabie et al. (2009a) demonstrated that the nonlinear Lyapunov equation (2.149) aligns exactly
with the integral reinforcement form (2.153). This implies that the positive definite solution to both
equations gives the value (2.148) of the policy u = μ(x). The integral reinforcement form is a
manifestation of the Bellman equation for CT systems and functions as a fixed point equation.
Accordingly, we can describe the temporal difference error for CT systems as follows
e(t : t +T) = t+T
t
r(x(τ),u(τ))dτ +V(x(t +T))−V(x(t)). (2.156)
This equation is decoupled from the system dynamics. As such, policy iteration and value itera￾tion for CT systems can be formulated directly from this definition.
2.5.1 CONTINUOUS-TIME POLICY ITERATION (PI) ALGORITHM
Similar to the dicrete-time policy iteration, the algorithm in CT demands an initial stabilizing policy
and involves the policy evaluation and improvement phases. If the control vector-field or the control
coofficient for a nonlinear input-affine system is known, then the PI algorithm can be implemented
as described below.
1. Initialization. Start with any admissible, i.e., stabilizing, control policy μ(0)
(x).
2. Policy Evaluation Phase. Calculate V(i)
(x(t)) through the following relation:
V(i)
(x(t)) =  t+T
t
r
"
x(s),μ(i)
(x(s))#
ds+V(i)
(x(t +T)),
V(i)
(0) = 0.
(2.157)
3. Policy Improvement Phase. Find an enhanced policy using
μ(i+1) = argmin u

H
"
x,u,∇V(j)
x
#, (2.158)
which is specifically
μ(i+1)
(x) = −1
2
R−1gT (x)∇V(i) x . (2.159)
Here the superscript i denotes the iteration index and ∇V(i) x is the gradient of the value function
with respect to the state.
2.5.2 CONTINUOUS-TIME VALUE ITERATION (VI) ALGORITHM
Similar to the PI, the continuous-time analogue for the value iteration algorithm is summarized as
follows:
1. Initialization. Begin with any control policy μ(0)
(x), not necessarily a stabilizing one.86 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
2. Policy Evaluation Phase. Solve for V(i)
(x(t)) using the following:
V(i)
(x(t)) =  t+T
t
r
"
x(s),μ(i)
(x(s))#
ds+V(i−1)
(x(t +T)),
V(i)
(0) = 0.
(2.160)
3. Policy Improvement Phase. Determine a superior policy using
μ(i+1) = argmin u

H
"
x,u,∇V(i)
x
#, (2.161)
which specifically is
μ(i+1)
(x) = −1
2
R−1gT (x)∇V(i)
x . (2.162)
Crucially, these algorithms do not demand explicit knowledge about the internal system dy￾namics function f(·), thus making them suitable for partially unknown systems. The PI and VI
algorithms can be executed online by using the value function approximation and the temporal
difference error along with the data collected at each time increment (x(t),x(t + T),ρ(t : t + T)),
where
ρ(t : t +T) =  t+T
t
r(x(τ),u(τ))dτ (2.163)
represents the reinforcement gathered in each time span. With the TD error defined for the CT case,
algorithms including the Q-learning and the actor-critic techniques can be analogously implemented
in the CT case. See Lewis et al. (2012a); Kiumarsi et al. (2017) for more information. The RL time
interval T is not required to be constant at each iteration. Adjusting T based on the time required
to glean meaningful insights from the observations is permissible. We shall see that a time-varying
sampling period Tk can designed, wherein the value of Tk is dynamically determined by the event￾triggering mechanism in an event-triggering control framework in Chapter 4.
2.6 ADAPTIVE DYNAMIC PROGRAMMING FOR OPTIMAL TRACKING
We shall now revisit the tracking control problem considered in Chapter 1. Unlike the regulation
problem, the difficulty for designing closed-loop optimal tracking control lies in solving the time￾varying Hamilton-Jacobi-Bellman (HJB) equation, which is usually too hard to solve analytically.
The time-varying nature arises due to the dependence of the HJB equation on the desired or refer￾ence trajectory. As a consequence, the value function and control policy are typically obtained as
time-varying functions (Dierks and Jagannathan, 2010a, 2009b; Zhang et al., 2011a, 2008a). Typ￾ical approaches to handle this challenge have been to make assumptions on the desired trajectory,
which is usually assumed to be a state of a dynamic system, referred as generator dynamics. Addi￾tionally, assumptions on the boundedness of the reference trajectory and steady-state stability of the
generator dynamics are made inorder to keep the tracking control problem tractable. In this section,
we briefly review two popular approaches reported in the literature to address the optimal track￾ing control problem. Both these methods focus on transforming the nonautonomous tracking error
system into an augmented autonomous system, which will result in well-defined infinite horizon
performance index. For more information on the methods described in this section, see Zhang et al.
(2008b, 2011b); Kamalapurkar et al. (2015); Modares and Lewis (2014b) and the references therein.
2.6.1 OPTIMAL TRACKING FOR CONTINUOUS-TIME SYSTEMS
Consider an affine nonlinear controllable continuous-time system given by
x˙(t) = f(x(t)) +g(x(t))u(t), x(0) = x0, (2.164)Adaptive Dynamic Programming and Optimal Control 87
where x : [0,∞) → Rn and u : Rn → Rm are state and control input vectors, respectively. The nonlin￾ear vector function f : Rn → Rn and matrix function g : Rn → Rn×m are, respectively, the internal
dynamics and the input gain function with f(0) = 0. The control objective is to track a feasible
reference trajectory xd(t) ∈ Rn generated by a reference system represented by
x˙d(t) = ζ (xd(t)), xd(0) = xd0, (2.165)
where xd(t) ∈ Rn is the reference state, and ζ : Rn → Rn is the internal dynamics with ζ (0) = 0.
To address the tracking control problem, the following assumptions characterizing the systems in
(2.164) and (2.165) are essential.
Assumption 2.1. System (2.164) is controllable and the system states are available for measure￾ment.
Assumption 2.2. The functions f(x) and g(x) are Lipschitz continuous for all x ∈ Ωx where Ωx
is a compact set containing the origin. Further, the function g(x) has a full column rank for all
x ∈ Ωx and satisfies g(x) ≤ gM for some constant gM > 0. In addition, g(xd)g+(xd) = I where
g+ = (gT g)−1gT .
Assumption 2.3. The feasible reference trajectory xd(t) ∈ Ωxd , where Ωxd is a compact set, is
bounded such that xd(t) ≤ bxd where bxd > 0 is a constant.
Define the error between the system state and the reference state as tracking error, given by
er(t)  x(t) − xd(t). Then, the tracking error system, utilizing (2.164) and (2.165), can be defined
by
e˙r(t) = x˙(t)−x˙d(t) = f(er +xd) +g(er +xd)u−ζ (xd), er(0) = x(0)−xd(0). (2.166)
Unlike the non-optimal case discussed in Chapter 1, to design optimal tracking controller, a steady￾state feed-forward control policy as a function of the reference trajectory is needed (Zhang et al.,
2011a). The feed-forward component of the control policy can be expressed as
ud = g+(xd)(ζ (xd))− f(xd), (2.167)
where ud : Rn → Rm. By augmenting the tracking error er and desired trajectory xd, the dynamics
of the augmented tracking error system can be represented as
χ˙(t) = F(χ) +G(χ)w(t), (2.168)
where χ  [eT
r xT
d ]
T ∈ R2n is the augmented state with χ(0)=[eT
r (0) xT
d (0)]T = χ0, F : R2n →
R2n is given by F(χ) 

f(er +xd) +g(er +xd)ud −ζ (xd)
ζ (xd)

, G : R2n → R2n×m given by G(χ) 

g(er +xd)
0

, and the mismatched control policy w  u−ud ∈ Rm. The infinite horizon performance
index with state constraint enforced by the dynamical system in (2.168) can be defined as
J(χ,w) =  ∞
0
[χT (τ)Q¯χ(τ) +w(τ)
TRw(τ)]dτ (2.169)
where Q¯ 
 Q 0n×n
0n×n 0n×n

∈ R2n×2n with Q ∈ Rn×n and R ∈ Rm×m are symmetric positive definite
matrices. The matrix 0n×n is a matrix with all elements zero. Note that the performance index is
defined using the mismatched policy w and, therefore, the cost functional is finite for any admissible
control policy w ∈ Ωw, where Ωw is the set of all admissible policies (Lewis et al., 2012b). With
the above-reformulated cost function (2.169), one can design the optimal control policy w∗(t) by
treating it as an optimal regulator using methods discussed in Section 2.5.88 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
2.6.1.1 A Special Case of Linear System
Consider a linear continuous-time system given in (1.45). The control objective is to track a feasible
reference trajectory xd(t) ∈ Rn generated by a reference system represented by
x˙d(t) = Adxd(t), xd(0) = xd0, (2.170)
where xd(t) ∈ Rn is the reference state, and Ad ∈ Rn×n is the internal dynamics with xd(0) = 0.
Define the error between the system state and the reference state as the tracking error, given by
er(t)  x(t)−xd(t). Then, the tracking error system, utilizing (1.45) and (2.170), can be defined by
e˙r(t) = x˙(t)−x˙d(t) = A(er +xd) +Bu−Adxd. (2.171)
The steady-state feed-forward control policy for the reference trajectory can be expressed as
ud = B+(Adxd −Axd), (2.172)
where ud : Rn → Rm is the steady state control policy corresponding to the reference trajectory. By
augmenting the tracking error er and desired trajectory xd, the dynamics of the augmented tracking
error system can be represented as
χ˙(t) = Gχ(t) +Hw(t), (2.173)
where χ  [eT
r xT
d ]
T ∈ R2n is the augmented state with χ(0)=[eT
r (0) xT
d (0)]T = χ0, G ∈ R2n is
given by G 

A A−Ad
0 Ad

, H ∈ R2n×m given by H 

B
0

, and the mismatched control policy
w  u − ud ∈ Rm. The augmented error system in (2.173) transforms the tracking problem into a
regulation problem, which can then be solved to stabilize the augmented error system using any of
the methods described in Section 2.5.
2.6.2 OPTIMAL TRACKING FOR DISCRETE-TIME SYSTEMS
Consider an affine nonlinear controllable discrete-time system given by
xk+1 = f(xk) +g(xk)uk, x0 ∈ Rn, (2.174)
where x : [0,∞) → Rn and u : Rn → Rm are state and control input vectors, respectively. The nonlin￾ear vector function f : Rn → Rn and matrix function g : Rn → Rn×m are, respectively, the internal
dynamics and the input gain function. The control objective is to track a feasible reference trajectory
xk,d ∈ Rn generated by a reference system represented by
xk+1,d = ζ (xk,d), x0,d ∈ Rn, (2.175)
where xk,d ∈ Rn is the reference state, and ζ : Rn → Rn is the internal dynamics. To address the track￾ing control problem, the following assumptions characterizing the systems in (2.174) and (2.175)
are essential. Similar to the assumptions used for the continuous-time systems, the system (2.174)
should be controllable. Further, it is important that the function g−1(xk,d) exists, the reference tra￾jectory is feasible and bounded.
Define the error between the system state and the reference state as tracking error, given by
ek,r  xk −xk,d. Then, the tracking error system, utilizing (2.164) and (2.175), can be defined by
ek+1,r = xk+1 −xk+1,d = f(ek,r +xk,d) +g(ek,r +xk,d)uk −ζ (xk,d), ek,r = x0 −xk,d. (2.176)Adaptive Dynamic Programming and Optimal Control 89
For time-invariant optimal tracking problems in linear systems, the performance index is typically
defined as the following quadratic form
J(e0,r,uk) =
∞
∑
k=0
{eT
k,rQek,r +uT
k Ruk}, (2.177)
where Q,R are positive definite penalty matrices. Sometimes the control penalty function in the cost
is defined as (uk −uk−1)TR(uk −uk−1) (Zhang et al., 2008a). However, in the case of time-variant
tracking problems within a nonlinear environment, the issue becomes considerably more complex.
The aforementioned performance index, i.e., J(e0,r,uk) calculated by (2.177), may become invalid,
leading to potential infinity values, as the control uk depends on the desired trajectory xk,d. To solve
this problem, several approaches are presented in the literature. In particular, derived from Zhang
et al. (2008a) the following performance index can be used
J(e0,r,uk) =
∞
∑
k=0
{eT
k,rQek,r +wT
k Rwk}, (2.178)
where wk = uk −uk,d. To keep the change in controls small, Zhang et al. (2008a) used the penalty
term (wk −wk−1)TR(wk −wk−1) with wk = 0 for k < 0. The steady-state feed-forward control policy
as a function of the reference trajectory, following (Zhang et al., 2008a), is expressed as
uk,d = g−1(xk,d)(ζ (xk,d))− f(xk,d), (2.179)
where ud : Rn → Rm. In equation (2.178), the first term represents the tracking error, while the
second term corresponds to the difference in feedback and feedforward control components. Nev￾ertheless, a concern persists that if the performance index considers only these two aspects, the
system may exhibit oscillations. For instance, the feedback and feedforward control difference may
be small, yet a significant error could exist between the these two control components. To address
this, Zhang et al. (2008a) introduced an additional penalty term (uk −uk,d)T S(uk −uk,d) in the cost
function to suppress oscillatory transients.
The transformation of the tracking control problem to a regulation problem will be complete if
we define the augmented system and the associated cost function based on the previous develop￾ments. By augmenting the tracking error er and desired trajectory xd, the dynamics of the augmented
tracking error system can be represented as
χk+1 = F(χk) +G(χk)wk, (2.180)
where χ  [eT
r xT
d ]
T ∈ R2n is the augmented state with χ0 = [eT
0,r xT
0,d]
T , F : R2n → R2n is given
by F(χ) 

f(er +xd) +g(er +xd)ud −ζ (xk,d)
ζ (xk,d)

, G : R2n → R2n×m given by G(χ) 

g(er +xd)
0

,
and the mismatched control policy w  u−ud ∈ Rm. The infinite horizon performance index with
state constraint enforced by the dynamical system in (2.168) can be defined as
J(χ,w) =  ∞
0
[χT (τ)Q¯χ(τ) +w(τ)
TRw(τ)]dτ (2.181)
where Q¯ 
 Q 0n×n
0n×n 0n×n

∈ R2n×2n with Q ∈ Rn×n and R ∈ Rm×m are symmetric positive definite
matrices. The matrix 0n×n is a matrix with all elements zero. Note that the performance index is
defined using the mismatched policy w and, therefore, the cost functional is finite for any admissible
control policy w ∈ Ωw, where Ωw is the set of all admissible policies (Lewis et al., 2012b). With
the above-reformulated cost function (2.181), one can design the optimal control policy w∗(t) by
treating it as an optimal regulator using methods discussed in Section 2.5.90 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
2.6.3 OPTIMAL TRACKING FOR CONTINUOUS-TIME SYSTEMS USING DIS￾COUNTED COST
The conventional tracking control design techniques, including the one presened in Section 2.6.1,
find the feedback and feedforward parts of the control input separately using complete knowledge of
the system dynamics. In addition, with the augmented system dynamics, they employ ADP methods
to learn the control components. An alternative method to learn the tracking control input with both
the feedback and feedforward parts of the control input simultaneously was reported in the work by
Modares and Lewis (2014a,b). In their approach, the considered the generated model as in (2.165)
and constructed an augmented system
χ˙(t) =  e˙r(t)
x˙d (t)

=
 f(er+xd )−ζ (xd )
ζ (xd )

+

g(er+xd )
0

u(t) ≡ F(χ) +G(χ)u(t), (2.182)
with the augmented states and their initial conditions defined as in (2.168) and the functions F and
G redefined based on (2.182). In this case, the cost function, characterizing the performance of the
tracking controller is defined as
J(χ(t),w(t)) =  ∞
t
exp−λ(τ−t)
[χT (τ)Q¯χ(τ) +u(τ)
TRu(τ)]dτ, (2.183)
where λ ≥ 0 is the discount factor. It is important to note that both the steady-state (feed-forward)
and feedback components of the control input are obtained simultaneously through the minimization
of the discounted performance function (2.183) along the trajectories of the augmented system
(2.182). The incorporation of a discount factor in the performance index is crucial. This is due
to the presence of a steady-state component in the control input, which typically renders the integral
in (2.183) unbounded if a discount factor is not utilized.
In this case, the continuous-time equivalent of the Bellman equation can be obtained as
[χT (τ)Q¯χ(τ) +u(τ)
TRu(τ)]−λV +
∂VT
∂ χ [F(χ) +G(χ)u] = 0, V(0) = 0, (2.184)
and the Hamiltonial function given by
H(χ,u,
∂VT
∂ χ )=[χT (τ)Q¯χ(τ) +u(τ)
TRu(τ)]−λV +
∂VT
∂ χ [F(χ) +G(χ)u]. (2.185)
The optimal control policy is then obtained via stationarity condition and is given by u∗(t) =
−R−1GT (χ) ∂V∗
∂ χ] , with V∗ denoting the optimal value function for the tracking problem. As de￾scribed earlier, the augmented error system transforms the tracking problem into a regulation prob￾lem, which can then be solved to stabilize the augmented error system using any of the ADP methods
described in Section 2.5.
2.7 CONCLUDING REMARKS
In this chapter, a brief introduction to neural networks and their approximation property, dynamic
programming-based backward-in-time optimal control, and online iterative and time-based solu￾tions to approximate optimal control using adaptive dynamic programming both in discrete and
continuous time domains are presented. The universal approximation property of the NNs is lever￾aged to approximate the value function (solution to the HJB equation) and optimal control policy
using actor-critic architecture. First, an online iterative approach using the policy and value iteration
is presented and then a non-iterative approach using the time history of the cost-to-go and system
trajectory is presented. This time-based approach is utilized as the backbone for the development of
event-based ADP and Q-learning presented in the next chapters (Chapters 3-8).Adaptive Dynamic Programming and Optimal Control 91
2.8 PROBLEMS
2.8.1 A neuron with linear activation function is described by y = v1x1 + v2x2 + v0. Select the
weights to design one-layer NN that implement the following Logic operations:
(a) AND
(b) OR
(c) COMPLEMENT
What about the Exclusive-OR (XOR) operation? Plot the decision boundary learned by the
NN.
2.8.2 Perform a MATLAB simulation of the Hopfield nets (both continuous and discrete time)
using their equations provided in 2.1.2.5. Select ui = 0, τi, pi ∈ (0,1), and wi j ∈ [0,1] for all
i, j. Make phase-plane plots for representative initial conditions x(0).
2.8.3 Build and train a two-layer NN that implements the X-OR operation. Begin with random
weights and train using backpropagation. The input vectors x are {(0 0)T ,(0 1)T ,(1 0)T ,(1 1)T },
and the associated desired outputs y are given by the definition of X-OR. Plot the decision
boundary learned by the network.
2.8.4 Perform a detailed derivation of backpropogation algorithm using matrix techniques.
2.8.5 Let P desired input-output pairs 
X1,Y1

,

X2,Y2

,...,
XP,Y P
be prescribed for the NN.
In batch updating, all P pairs are presented to the NN and a cumulative error is computed.
At the end of this procedure, the NN weights are updated once using the backpropogation
algorithm. Derive this update law by defining the cumulative error for one epoch as
E(k) =
P
∑
p=1
Ep(k)
with Ep(k) given as the squared difference between network output and the target output. Use
matrix calculus for the derivation. Implement the resulting algorithm for the X-OR problem.
Alternatively, in a stochastic gradient-based updating, the p pairs are sampled randomly and
weights of the NN are updated using the sampled data. Assign uniform probability distribu￾tion to the P pairs of data and sample them with equal probability. Implement the resulting
algorithm for the X-OR problem. Is there any difference in the convergence time between the
two algorithms?
2.8.6 The activation function derivatives are required to implement backpropagation training. Com￾pute the derivatives of the following: (a) sigmoid activation function (b) tanh activation func￾tion (c) RBF.
2.8.7 Derive the backpropagation algorithm using: (a) Tanh activation functions. (b) RBF activation
functions.
2.8.8 (Lewis et al. (1998)) A modified backpropagation algorithm is obtained by changing the order
of the operations in the standard backpropogation algorithm. Thus, suppose the backpropa￾gated error and updated weights are computed in the interleaved fashion
δ 2
i = yi(1−yi)ei;i = 1,2,...,m
wi = wi +ηzδ 2
i ; i = 1,2,...,m;  = 0,1,...,L
δ 1
 = z (1−z)
m
∑
i=1
wiδ 2
i .;  = 1,2,...,L
v j = v j +ηXjδ 1
 ;  = 1,2,...,L; j = 0,1,...,n
where the new layer-2 weights wi are used to compute the layer-1 backpropagated error δ 1
 .
Justify this algorithm (or argue against it) using partial derivative/chain rule arguments. Would
you expect this algorithm to perform better or worse than standard backpropagation?92 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
2.8.9 Write a MATLAB M file to implement the modified backpropagation training algorithm given
in the previous problem . Using your program, and any example, answer the following ques￾tions: Does the modified algorithm converge faster than standard backpropagation? Can this
behavior be generalized to any example?
2.8.10 The longitudinal dynamics of an F-16 aircraft in straight and level flight at 502 ft/sec are given
by
x˙ = Ax+Bu =
−2.0244×10−2 7.8761 −3.2169×101 −6.502×10−1
−2.5373×10−4 −1.0189 0 9.0484×10−1
0 00 1
7.9472×10−11 −2.498 0 −1.3861

x (2.186)
+
 −1×10−2
−2.09×10−3
0
−1.99×10−1

u
The state is x =  vT α θ q T
, with vT the forward velocity, α the angle of attack, θ
the pitch angle, and q the pitch rate. The control input u(t) is the elevator deflection δe.
(a) Try MATLAB function LQR to perform linear quadratic regulator design. Use Q = I,R =
0.1.
2.8.11 Consider the scalar bilinear system
xk+1 = xkuk +u2
k (2.187)
with the cost index
J0 = x2
N +
N−1
∑
k=0
xkuk. (2.188)
Let N = 2 and the control is constrained to take values of uk = −1,0,1 and the state to take
values of xk = −2,−1,0,1,2. Use dynamic programming to find an optimal state feedback
control law. Let x0 = −2. Find the optimal cost, control sequence, and state trajectory.
2.8.12 Write a MATLAB program to implement the dynamic programming algorithm to solve the
previous problem.
2.8.13 For the system in (2.186), implement the value iteration, policy iteration algorithms to learn
the optimal control policy.
2.8.14 For the system in (2.186), implement the time-based ADP algorithm to learn the optimal
control policy.3 Linear Discrete-time and
Networked Control
Systems
CONTENTS
3.1 Introduction ............................................................................................................................. 94
3.2 State Feedback Design ............................................................................................................ 95
3.2.1 Problem statement ...................................................................................................... 95
3.2.2 The Q-function setup .................................................................................................. 98
3.2.3 Parametric form and learning of optimal gain............................................................ 99
3.2.4 Closed-loop stability analysis................................................................................... 102
3.3 Output Feedback Design ....................................................................................................... 107
3.3.1 Adaptive observer design.......................................................................................... 108
3.3.2 Observer-based controller design and closed-loop stability ..................................... 109
3.4 Event-sampled Networked Control Systems......................................................................... 112
3.4.1 NCS Reformulation and Problem Statement............................................................ 113
3.4.2 Co-optimization under Uncertain NCS Dynamics ................................................... 117
3.4.3 Main Results and Stability Analysis......................................................................... 122
3.5 Optimal Tracking Control ..................................................................................................... 124
3.6 Concluding Remarks ............................................................................................................. 126
3.7 Problems................................................................................................................................ 127
The resource-aware event-triggered control, discussed in Chapter 1, can considerably save the com￾putational and communication costs for implementing the control schemes in systems that use
embedded processors with limited computational capabilities and communication networks in the
feedback loop. The primary focus of Chapter 1 is to introduce the traditional event-based controller
design techniques (for regulation and tracking problems) via an emulation-based approach, which
can guarantee the stability of the systems with less frequent computation of the control input. The
event-triggering mechanism or condition for determining the sampling instants is designed using
complete knowledge of the system dynamics and Lyapunov stability. On the other hand, in Chapter
2, we focused on optimizing the system performance by minimizing a performance index by ex￾ploiting adaptive control, optimal control, and reinforcement learning (RL), referred to as adaptive
dynamic programming (ADP). One of the challenges we encountered in Chapter 2 is the require￾ment of higher computation due to the value function and control policy approximation using neural
networks. To alleviate the challenges in deploying the ADP-based near-optimal control schemes due
to the computational demand, in this chapter, we shall explore the domain of event-driven ADP for
linear systems with state and output feedback. We will also explore networked systems where the
feedback data and control input are communicated via a communication network.
In the first part of the chapter, we shall focus on designing near-optimal adaptive regulators
for uncertain linear discrete-time systems utilizing ADP (dynamic programming and Q-learning
9394 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
techniques). We shall start with the assumption that the communication channels in the control
loop are lossless. In the second part, we shall extend the Q-learning techniques to the networked
control system (NCS), where an imperfect communication network closes the feedback loop. In
both the cases, we shall utilize the feedback data that are sampled and communicated at event-based
aperiodic time instants. To solve the feedback control problem in this setting, we shall incorporate
event-driven ADP-based Q-learning technique with both state and output feedback and learn the
infinite horizon near-optimal control policy. We shall develop aperiodic or intermittent update laws
for tuning the Q-function parameters only at the event-triggering instants. For both the state and the
output feedback cases, event-triggering conditions, asymptotic stability and convergence results,
and existence of non-trivial minimum inter-event times shall be analyzed. Finally, we shall see that
these developments are also useful in the context of developing near-optimal tracking controllers for
linear discrete-time systems with unknown models. The techniques and analysis presented in this
chapter are based on the developments in (Sahoo, 2015).
3.1 INTRODUCTION
Event-driven sampling and feedback control frameworks (Astrom and Bernhardsson, 2002; Cogill,
2009; Donkers and Heemels, 2010; Eqtami et al., 2010; Garcia and Antsaklis, 2011; Heemels et al.,
2008; Lunze and Lehmann, 2010; Mazo and Tabuada, 2011; Molin and Hirche, 2013; Tabuada,
2007; Wang and Lemmon, 2008), called event-triggered control (ETC), have been introduced to re￾duce the computational burden. The ETC framework is instrumental in the area of networked control
systems (NCS) (Xu et al., 2012). It saves bandwidth due to the non-periodic transmission of feed￾back signals through the communication network included within the loop. The main idea behind
the ETC design is determining the aperiodic sampling or event-triggering instants. A state or output￾dependent condition called an event-triggering condition is generally employed to determine these
triggering instants. Further, the event-triggering condition also ensures the desired performance and
closed-loop stability of the physical systems. As discussed in Chapter 1, the initial development
of the event-triggered control schemes using state and output feedbacks are designed with the as￾sumption of complete knowledge of system dynamics (Donkers and Heemels, 2010; Eqtami et al.,
2010; Heemels et al., 2008; Lunze and Lehmann, 2010) or a model of a physical system with small
uncertainties (Garcia and Antsaklis, 2011). Later, Sahoo et al. (2013b) presented an adaptive control
scheme in an event-triggered context. The system dynamics were estimated using adaptive control
(Narendra and Annaswamy, 2012) techniques with an event-based sampling of state and output
measurements.
In the earlier works on event-triggered optimal control, Molin and Hirche (2013) examined tradi￾tional optimal control (Lewis et al., 2012b) in the context of event-based transmission of the system
states. The Riccati equation (RE), with complete knowledge of the system dynamics, was used to
solve the finite horizon Linear Quadratic Regulator (LQR) problem in a backward-in-time manner.
In general, the optimal control schemes use Q-learning-based iterative techniques (Bradtke et al.,
1994; Watkins, 1989) were proposed for periodically sampled linear systems to obtain iteratively
optimal control using dynamic programming in a forward-in-time manner, which also relaxes the
assumption of complete knowledge of system dynamics. However, with these iterative techniques
(Cheng et al., 2007; Wang et al., 2011), a significant number of iterations are needed within a sam￾pling interval to obtain a solution to the dynamic programming, and therefore limits its applicability
(Dierks and Jagannathan, 2009b) in real-world systems. Dierks and Jagannathan (2009b) proposed
a time-based approach, discussed in Chapter 2, to obtain the optimal solution forward in time.
Although the approach in (Dierks and Jagannathan, 2009b) alleviates the issue of iteration, the ap￾proximation of the value function still uses period sampling and control updates, demanding higher
computation. In this chapter, we will focus on an approach that alleviates the issue of computational
demand called event-driven Q-learning.
In the first part of the chapter, taken from (Sahoo, 2015) (Chapter 1), an event-driven ADP-basedLinear Discrete-time and Networked Control Systems 95
state feedback control scheme for uncertain linear discrete-time systems is introduced. The event￾driven ADP (Xu et al., 2012; Bradtke et al., 1994; Watkins, 1989) technique solves the infinite
horizon optimal control problem under both the state and the output feedback measurements. In
event-driven ADP, the cost function is parametrized by introducing the Q-function, and the con￾trol input is computed from the learned parameters of the Q-function using the event-based state
measurements. This complete data-driven Q-learning technique relaxes the assumption of complete
knowledge of system dynamics for both designs. Instead of the iterative approaches (value/policy
iterations), the time history of the cost-to-go function at the event-triggering instants is used (Dierks
and Jagannathan, 2009b) for Q-function parameter update. A non-periodic parameter tuning law is
used to tune the parameters of the Q-function.
However, the state vector is not always available for measurement in many practical applica￾tions. Therefore, the above approach is extended for output feedback event-triggered optimal con￾trol design. An adaptive observer to estimate the system state vector was included at the sensor
and tuned periodically for the output feedback design. The estimated state vector is sampled at the
event-triggering instants and used for estimating the Q-function for the output feedback case. The
adaptive event-triggering conditions for the state and output feedback are designed as a function
of the estimated Q-function parameters and with either measured or estimated state vectors. We
will also demonstrate that the adaptive event-triggering condition ensured the convergence of pa￾rameters by creating a sufficient number of events during the initial adaptation. The stability of the
closed-loop event-triggered system was demonstrated using the Lyapunov direct method, as dis￾cussed by the authors in Wang and Lemmon (2008). It is shown in Wang and Lemmon (2008) that
the Lyapunov function need not converge monotonically. In the case ETC, as discussed in Wang
and Lemmon (2008), it may increase during the inter-event times but remains bounded. Therefore,
the convergence of the Lyapunov function is shown using a decrescent function that upper bounds
the Lyapunov function, provided the regression vectors satisfy the PE condition (Green and Moore,
1985). Finally, the existence of non-trivial inter-event times for the state feedback case is shown by
deriving an explicit formula.
3.2 STATE FEEDBACK DESIGN
In this section, the event-triggered optimal adaptive state feedback-based regulation is presented.
The event-based estimation and event-based stability issues are precisely addressed while formulat￾ing the problem. Subsequently, a solution is introduced along with the design procedure.
3.2.1 PROBLEM STATEMENT
Consider the linear time-invariant (LTI) discrete-time system
xk+1 = Axk +Buk
yk = Cxk, (3.1)
where xk ∈ Ωx ⊆ Rn and uk ∈ Ωu ⊆ Rm represent the state and the control input vectors, respectively,
with initial state x0. The system matrices, A ∈ Rn×n and B ∈ Rn×m, are considered unknown. The
system (3.1) satisfies the following assumption.
Assumption 3.1. The system is considered controllable with the control coefficient matrix satisfying
B ≤ Bmax, where Bmax > 0 is a known constant. Further, the order of the system is known, and the
state vector is considered measurable.
The system states xk, in an event-triggered framework (see Figure 3.1), is sent to the controller
at the event sampled instants only. Let a subsequence {ki}∞
i=0 of time instants k ∈ N∪ {0} satisfying
ki+1 > ki and ki → ∞ as i → ∞ referred to as event-sampled instants with k0 = 0 as the initial event96 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
sampled instant. Alternatively, the system state vector (xki
) is sent to the controller at the time instant
ki for i = 0,1,2,···. The controller holds it until the next trigger instant for the controller execution.
The state vector at the controller can be defined as
xˆk =
$
xk, k = ki,
xki
, ki < k < ki+1, (3.2)
where ˆxk is the state at the controller between two triggering instants with an initial value ˆx0 = x0
due to the initial trigger at k0.
The event-triggering condition (defined later) is typically a function of the event-triggering error
and a state-dependent threshold (Tabuada, 2007). The event-triggering error, ek, is defined as the
difference between the current, xk, and the last transmitted and held system state vector, ˆxk, at the
controller, i.e.,
ek = xk −xˆk, ki ≤ k < ki+1. (3.3)
The event-based state feedback controller can be written as
uk = μ (xˆk), ki ≤ k < ki+1, (3.4)
where μ (xˆk) is a function of the event-based state (3.2).
The primary objective here is to design an event-based optimal controller uk with without the
knowledge of the system dynamics (A and B) while minimizing the performance index defined as
Jk = ∑∞
j=k r(xj,uj), (3.5)
where r(xk,uk) = xT
k Pxk + uT
k Ruk is a quadratic cost-to-go function at the time instant k where
the matrix P ∈ Rn×n is positive semi-definite that penalizes the system state (xk) and the matrix
R ∈ Rm×m that is positive definite and penalizes the control input. The initial control input u0 is
assumed to be admissible, i.e., the control input u0 is stabilizing, and the integral in the performance
index exists.
According to Bellman’s principle of optimality (Bellman, 1966), discussed in Chapter 2, the
optimal value function can be defined as
V∗(xk) = min
uk
(r(xk,uk) +V∗(xk+1)), (3.6)
where V∗(xk) is the optimal value function at time instant k, and V∗(xk+1) is the optimal value
function from time k +1 onwards. The Hamiltonian is given by
H (xk,uk) = r(xk,uk) +V(xk+1)−V(xk). (3.7)
The state feedback optimal control input u∗
k satisfies the stationarity condition given by ∂H(xk,uk)
∂uk =
0. Using the stationarity condition, the optimal control input with periodic state xT
k for ∀k can be
expressed as
u∗
k = μ∗ (xk) = −K∗
k xk, (3.8)
Using the emulation-based approach, the event-triggered optimal input u∗
k with the event-sampled
state vector at the controller, ˆxk, can now be written as
u∗
k = μ∗ (xˆk) = −K∗
k xˆk, (3.9)
where K∗ = (R+BT SB)
−1
BT SA is the optimal control gain matrix and the matrix S satisfies the
algebraic Riccati equation (ARE) (Lewis et al., 2012b) given by
S = AT [S−SB(BT SB+R)
−1
BT S]A+P. (3.10)Linear Discrete-time and Networked Control Systems 97
Using the solution, S, to the ARE, the optimal value function can be expressed in a quadratic form
(Lewis et al., 2012b) as
V∗(xk) = xT
k Sxk. (3.11)
Note that the computation of optimal control gain matrix K∗ in (3.9) requires the system matri￾ces, A and B, which are considered unknown in this case. Hence, the ARE (3.10) cannot be solved
to compute the control gain K∗; they must be estimated online. The event-based availability of the
state vector precludes the use of periodic time-based approaches for parameter estimation. There￾fore, the parameters must be estimated using the state and control input information available and
computed, respectively, at the triggering instances ki for i = 0,1,2,···. This requires the adaptive
(update) laws or tuning rules for parameters to be aperiodic while ensuring the convergence of the
parameter estimation error. Therefore, the main objective is to design an adaptive event-triggering
condition under unknown system dynamics to reduce computation and facilitate the optimal control
gain matrix estimation.
As a solution, the event-driven Q-learning-based technique is presented for designing the near￾optimal adaptive regulator whose structure is illustrated in Figure 3.1.
	 	
 	


 	 

	



 
Figure 3.1 Event-triggered optimal state feedback regulator.
In the Q-learning approach (Starr and Ho, 1969), the control gain matrix K∗ can be computed
from the action-dependent value function, usually referred to as the Q-function. Therefore, in this
approach, an event-driven adaptive Q-function estimator (QFE) is utilized at the controller to learn
the Q-function online. An aperiodic tuning law, which updates only at the event-triggering instances,
is used to tune the estimated Q-function parameters. A smart sensor and a triggering mechanism are
included near the plant to determine the event-triggering instants. At the triggering mechanism,
the event-triggering condition is evaluated at every sampling instant k, and a decision is made to
release the system state vector both for time instant ki and ki − 1 (xki and xki−1, i = 1,2,...) to
the controller only at the violation of the triggering condition. Note that the threshold in the event￾triggering condition becomes a function of both the estimated Q-function parameters and the system
state. Therefore, the event-triggering condition becomes adaptive, unlike the ZOH and model-based
triggering conditions discussed in Chapter 1 for known system dynamics (Tabuada, 2007; Garcia
and Antsaklis, 2011; Wang and Lemmon, 2008). The adaptive triggering condition not only ensures
stability but facilitates the estimation process.
The Q-function parameters are also estimated locally at the triggering mechanism using a mirror
QFE to evaluate the triggering condition. This will save transmissions of Q-function parameters
between the QFE at the controller and the triggering mechanism in case of an NCS (discussed in
the second part of this chapter). The mirror at the triggering mechanism and the actual QFE at the
controller operate in synchronism and are initialized with the same initial conditions. It is to be
noted that the mirror QFE at the triggering mechanism demands additional computational power.98 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
However, the overall computational power required for the execution of the event-driven Q-leaning￾based controller is found to be lesser than its traditional counterpart (Barto et al., 2004) with one
QFE.
Table 3.1
State feedback control scheme
System dynamics xk+1 = Axk +Buk
Event-triggering error ek = xk −xˆk, ki ≤ k < ki+1
Q-function estimator (QFE) Q(xk,uk) = zT
k Gzk = ΘT ξk
QFE residual error eV
k =
⎧
⎪⎨
⎪⎩
r(xk,uk) +Θˆ T
k Δξk, k = ki,
r(xˆk,uk) +Θˆ T
k Δ ˆ
ξk, ki < k < ki+1,
Auxilary QFE error ΞV
k = Πk +Θˆ T
k Zk, k = ki
Event-based update law Θˆ k =
⎧
⎪⎨
⎪⎩
Θˆ k−1 − Wk−1Zk−1ΞVT
k−1 
I +ZT
k−1Wk−1Zk−1

, k = ki
Θˆ k−1, ki−1 < k < ki,
Wk =
⎧
⎪⎨
⎪⎩
Wk−1 − Wk−1Zk−1ZT
k−1Wk−1 
I +ZT
k WkZk

 , k = ki
Wk, ki−1 < k < ki
Control input uk = −Kˆkxˆk, ki ≤ k < ki+1
Kˆk = (Gˆ uu
k )
−1
Gˆ ux
k
Event-triggering Condition ek ≤ σETC
k xk
σETC
k =


Γ(1−3μ¯)/3B2
max

Kˆk

2
3.2.2 THE Q-FUNCTION SETUP
The action-dependent value function or the Q-function as a function of the system state vector and
control input can be represented as
Q(xk,uk) = r(xk,uk) +V∗(xk+1). (3.12)
The value function for the time instant k + 1, taken from (3.11), can be expressed as V∗(xk+1) =
xT
k+1Sxk+1. Substituting V∗(xk+1) along with the system dynamics from (3.1) into (3.12) generates
the following

xT
k uT
k

G

xT
k uT
k
T =

xk
uk
T 
P+AT SA AT SB
BT SA R+BT SB xk
uk

. (3.13)
Defining
G =

P+AT SA AT SB
BT SA R+BT SB
=

Gxx Gxu
Gux Guu
, (3.14)Linear Discrete-time and Networked Control Systems 99
where Gxx = P+AT SA, Gxu = AT SB, Gux = BT SA, and Guu = R+BT SB are the block matrices,
we can express the Q-function in a compact form as
Q(xk,uk) = 
xT
k uT
k

G

xT
k uT
k
T
. (3.15)
The optimal action dependent value function or the Q-function Q∗(xk,u∗
k ) in (3.12) is equal to
the optimal value function V∗(xk) in (3.6) when the control policy, uk = u∗
k . i.e., optimal. Thus,
V∗(xk) = min{ uk
Q(xk,uk)} = min
uk
{[xT
k uT
k ]G[xT
k uT
k ]
T
} = Q∗(xk,u∗
k ). (3.16)
The action-dependent value function or the Q-function using (3.16) can be expressed as
Q(xk,uk) = r(xk,uk) +Q∗(xk+1,u∗
k ). (3.17)
The equation (3.17) is the action-dependent version of the Bellman equation of optimality in (3.6).
Therefore, the last row of the G matrix in (3.14) can be used to express the optimal control gain as
K∗ = (R+BT SB)
−1
BT SA = (Guu)
−1
Gux. (3.18)
Therefore, the estimation of matrix G in the action-dependent value function or Q-function will
include all the information needed to compute the optimal control gain matrix, K∗. Alternatively,
the estimation of the G matrix using data is equivalent to solving the ARE (3.10) without information
of A and B matrics. This allows the computation of the optimal control input online in a forward￾in-time manner without explicit knowledge of the system dynamics. The next subsection details
the procedure for the computation of the optimal control policy using the event-based Q-learning
scheme.
3.2.3 PARAMETRIC FORM AND LEARNING OF OPTIMAL GAIN
In this subsection, we express the action-dependent value function or Q-function (3.12) in a para￾metric form. The Q-function parameters are learned using the time history of the system state and
input at triggering instants.
Rewriting the optimal Q-function (3.13) in a parametric form as
Q(xk,uk) = z
T
k Gzk = ΘT ξk, (3.19)
where zk = [xT
k uT
k ]
T
∈ Rn+m=l is the column vector obtained through concatenation of xk and uk.
The regression vector
ξk = zk ⊗zk = [z
2
k1,··· ,zk1zkl,z
2
k2,··· ,zkl−1zkl,z
2
kl)] ∈ Rlg
is a quadratic polynomial vector and ⊗ denotes the Kronecker product. The vector Θ ∈ Rl(l+1)/2=lg
is a vector representation of the Q-function parameter matrix G. The vector Θ is formed by stacking
the columns of the square matrix G as a column vector with the off-diagonal elements summed as
Gab +Gba. The subscript represents the ath row and the bth column where a = b and a,b = 1,··· ,l.
Now, to estimate the Q-function (3.19), the QFE with the event-based availability of the system
state vector can be defined as
Qˆ(xˆk,uk) = zˆ
T
k Gˆ kzˆk = Θˆ T
k ˆ
ξk, ki ≤ k < ki+1, (3.20)
where Θˆ k ∈ Rlg is the estimate of the target parameter Θ, and ˆ
ξk = zˆk ⊗zˆk is the event-based regres￾sion vector with ˆzk = [xˆ
T
k uT
k ]
T constructed at the controller.100 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
We can express the QFE explicitly at the triggering instants (ki) and between two triggers (ki <
k < ki+1) using the event-based state vector at the controller (3.2) as
Qˆ(xˆk,uk) = $
z
T
k Gˆ kzk = Θˆ T
k ξk, k = ki,
zˆ
T
k Gˆ kzˆk = Θˆ T
k ˆ
ξk, ki < k < ki+1. (3.21)
One can reconstruct the estimated Q-function matrix Gˆ k =

Gˆ xx
k Gˆ xu
k
Gˆ ux
k Gˆ uu
k

using Θˆ . The estimated
gain matrix Kˆk can be obtained from the Q-function estimated parameter vector Θˆ k or, alternatively,
Gˆ k. The estimated event-based control input can now be written as
uk = −Kˆkxˆk, ki ≤ k < ki+1, (3.22)
where the estimated control gain Kˆk can be expressed as
Kˆk = (Gˆ uu
k )
−1
Gˆ ux
k . (3.23)
The Bellman equation of Q-function in (3.17) with (3.19), can be rewritten as
0 = r(xk,uk) +Q∗(xk+1,uk+1)−Q∗(xk,uk) = r(xk,uk) +ΘTΔξk, (3.24)
where Q∗(xk+1,uk+1) = ΘT ξk+1 and Δξk = ξk+1 −ξk.
The estimated QFE in (3.21) is not optimal. Thus, Bellman equation (3.24) does not hold and
leads to an error referred to as Q-function estimation error (QFE error). It is denoted by
e
V
k =
$
r(xk,uk) +Θˆ T
k Δξk, k = ki,
r(xˆk,uk) +Θˆ T
k Δ ˆ
ξk, ki < k < ki+1, (3.25)
where Δ ˆ
ξk = ˆ
ξk+1 − ˆ
ξk and r(xˆk,uk) = xˆ
T
k Pxˆ
T
k + uT
k Ruk. This error is similar to the temporal dif￾ference (TD) error used for the iteration-based Q-learning schemes. Therefore, minimization of the
QFE error online will ensure the estimated control input converges to the optimal value.
Define an auxiliary QFE error vector as
ΞV
k = Πk +Θˆ T
k Zk, k = ki (3.26)
where
Πk = [r(xki
,uki
) r(xki−1 ,uki−1 ) ··· r(xki−1−j
,uki−j
)]
and
Zk = [Δξki Δξki−1 ··· Δξki−1−j
]
with 0 < j < ki.
Remark 3.1. The auxiliary QFE error is formed by using the aperiodic time history of the system
state and control input and the current Q-function parameter vector Θˆ k. The auxiliary error elimi￾nates the use of policy iteration, and a near-optimal control gain can be achieved by updating the
Q-function parameters at the aperiodic sampling instants.
The QFE parameters are tuned only at the sampling instants (ki, i = 0,1,···) with the updated
state information and held during the inter-sampling times, ki−1 < k < ki, i = 0,1,··· as follows
Θˆ k =
⎧
⎪⎨
⎪⎩
Θˆ k−1 − Wk−1Zk−1ΞVT
k−1

I +ZT
k−1Wk−1Zk−1

, k = ki,
Θˆ k−1, ki−1 < k < ki,
(3.27)Linear Discrete-time and Networked Control Systems 101
Wk =
⎧
⎪⎨
⎪⎩
Wk−1 − Wk−1Zk−1ZT
k−1Wk−1

I +ZT
k WkZk

 , k = ki
Wk−1, ki−1 < k < ki
, (3.28)
where W0 = βI with β > 0 a larger positive value and I is the identity matrix.
Remark 3.2. The Q-function parameters tuning law (3.27) requires the state vectors xki and xki−1
for the computation of Δξki−1 defined in (3.26). Thus, the system state vectors xki and xki−1, to￾gether, are sent to the controller at the aperiodic sampling instants, as proposed. This tuning rule
saves the computation when compared to a traditional adaptive control technique (Narendra and
Annaswamy, 2012).
Remark 3.3. The estimation stops as the QFE in (3.21) becomes zero when the system state vector
converges to zero before the Q-function parameters Θˆ k converge to the target parameters Θ. Hence,
the regression vector ξk must satisfy the PE condition (Green and Moore, 1985) for the Q-function
parameters to converge to its target values. The PE condition is a necessary condition for parameter
convergence in traditional adaptive control (Narendra and Annaswamy, 2012). The definition of the
PE condition (Green and Moore, 1985) is presented next for completeness.
Definition 3.1. A vector ϕ (xk) is said to be persistently exciting over an interval if there exist
positive constants δ, α
−
, α¯ and kd ≥ 1, such that
α
−
I ≤ ∑k+δ
k=kd
ϕ (xk)ϕT (xk) ≤ α¯ I, (3.29)
where I is the identity matrix of the appropriate dimension.
A PE-like condition can be achieved by adding an exploration noise to the control input uk during
the estimation process (Xu et al., 2012).
The Q-function parameter estimation error dynamics from (3.27), by forwarding one time step,
can be represented as
Θ˜ k+1 =
⎧
⎪⎨
⎪⎩
Θ˜ k +
WkZkΞVT
k

I +ZT
k WkZk

, k = ki,
Θ˜ k, ki < k ≤ ki+1,
(3.30)
where Θ˜ k = Θ−Θˆ k is the Q-function parameter estimation error.
Since we only update the QFE parameters at the triggering instants, the QFE error eV
k for k = ki
can expressed in terms of the Q-function parameter estimation error Θ˜ k, by subtracting (3.24) from
(3.25), as
e
V
k = −Θ˜ T
k Δξk, k = ki. (3.31)
The auxiliary QFE error (3.26) in terms of the Q-function parameter estimation error can be
represented by
ΞV
k = −Θ˜ T
k Zk, k = ki. (3.32)
Next, the lemma guarantees the boundedness of the Q-function parameter estimation error.
Lemma 3.1. Consider both the QFE (3.21) and the tuning law (3.27) and (3.28) with an initial
admissible control policy u0 ∈ Rm. Let Q-function parameter vector Θˆ 0 is initialized with nonzero
and finite value in a compact set. Then, the Q-function parameter estimation error Θ˜ k is bounded
during inter-event times and converges to zero asymptotically when the triggering instants ki → ∞
as i → ∞.102 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Sketch of Proof: We will analyze the convergence of Q-function parameter estimation error using
Lyapunov stability theory. The proof is carried out by considering both the cases of triggering con￾dition, i.e., at the event sampled instants (k = ki) and inter-event times (ki < k < ki+1), because of
the aperiodic tuning of the QFE parameters. A common Lyapunov function is used to evaluate both
cases, and the asymptotic stability is shown by combining both cases.
Select a Lyapunov function candidate given as
LΘ˜ k = Θ˜ T
k W−1
k Θ˜ k (3.33)
where Wk is a positive definite matrix as defined in (3.28).
Case I: At the event sampled instants (k = ki). In this case, the QFE parameters are tuned by using
(3.27) and (3.28) for the case k = ki. The QFE parameter estimation error dynamics (3.30) with the
augmented Bellman error (3.32), can be written as
Θ˜ k+1 =

I − WkZkZT
k

I +ZT
k WkZk



Θ˜ k. (3.34)
The equation (3.28) can also be expressed as
Wk+1W−1
k = I − WkZkZT
k

I +ZT
k WkZk

. (3.35)
Substituting (3.35) in (3.34), the QFE parameter estimation error dynamics become
Θ˜ k+1 = Wk+1W−1
k Θ˜ k. (3.36)
With the above results the first difference ΔLΘ˜ ,k along (3.36) and using (3.34), is upper bounded
as
ΔLΘ˜ ,k ≤ −Z¯2
min

Θ˜ k


2
< 0. (3.37)
Since the regression vector Zk satisfies the PE condition, it holds that 0 < Z¯min ≤ ZkZT
k 
I+ZT
k WkZk < 1.
This implies, the Lyapunov function LΘ˜ ,k decreases i.e., LΘ˜ ,ki+1 < LΘ˜ ,ki
.
Case II: During the inter-event times (ki < k < ki+1). In this case, the QFE parameters are not
tuned and held at their previous values. Consider the same Lyapunov function in (3.33). The first
difference along (3.28) and (3.34) for ki < k < ki+1 is given by
ΔLΘ˜ ,k = Θ˜ T
k+1W−1
k+1Θ˜ k+1 −Θ˜ T
k W−1
k Θ˜ k = 0,ki < k < ki+1. (3.38)
By Lyapunov theorem, the QFE parameter estimation error Θ˜ k remains constant during the inter￾event times.
By combining both the cases for the interval ki ≤ k < ki+1, the QFE parameter estimation error
remains constant during the inter-event intervals and decreases at the triggering instant. Note that the
Lyapunov function is positive definite and lower bounded by zero. Therefore the Lyapunov function
decreases to to zero as the triggering instants ki → ∞, i → ∞, i.e., asymptotically. For the detailed
steps of the proof, refer to (Sahoo, 2015) Proof of Lemma 4.4. The convergence of the estimated
control input uk to near-optimal value and closed-loop stability is presented in the next subsection.
3.2.4 CLOSED-LOOP STABILITY ANALYSIS
The closed-loop system dynamics, using the system in (3.1) and the control input in(3.22), can be
expressed as
xk+1 = Axk −BKˆkxˆk
= Axk −BKˆkxk +BKˆkek, k ≤ ki < ki+1. (3.39)Linear Discrete-time and Networked Control Systems 103
To ensure the closed-loop system is stable, the condition
ek ≤ σETC
k xk, (3.40)
is selected as the event-triggering condition, where σETC
k =
) Γ(1−3μ¯)
3B2
maxKˆ
k
2 is the threshold coeffi￾cient, 0 < Γ < 1, and μ¯ < 1/3 . The event-triggering instants are decided at the violation of the
inequality (3.40). The threshold coefficient σETC
k uses the estimated control gain matrix Kˆk. Thus,
the event-triggering condition (3.40) is adaptive in nature, as proposed earlier. This adaptive na￾ture of the triggering condition helps estimate the Q-function parameters by generating a suitable
number of events during the initial learning phase.
The adaptive event-triggering condition (3.40) becomes equal to the traditional triggering condi￾tion (Tabuada, 2007; Garcia and Antsaklis, 2011; Wang and Lemmon, 2008), discussed in Chapter
1, for systems with known dynamics once the Q-function parameters converge to their target values.
Now we are ready to show the stability of the closed-loop system. We will use the following
lemma to prove the stability.
Lemma 3.2. Consider the controllable linear discrete-time system (3.1). Then there exists an opti￾mal control input, u∗
k , such that the closed-loop dynamics are expressed as
Axk +Bu∗
k2 ≤ ν¯ xk2
, (3.41)
where 0 < ν¯ < 1 is a constant.
Proof. This can be seen by selecting a Lyapunov function L(xk) = xT
k xk. The first difference along
the system dynamics (3.1) can be expressed as ΔL(xk) ≤ 
Axk +Bu∗
k


2
−xk2 ≤ −(1−ν¯)xk2
.
Since the system is controllable and the optimal control input u∗
k is stabilizing (Lewis et al., 2012b),
the first difference ΔL(xk) < 0 provided the parameter ν¯ satisfies 0 < ν¯ < 1.
Theorem 3.1. Consider the closed-loop event-triggered system (3.39), Q-function parameter es￾timation error dynamics (3.30) and assume Assumption 3.1 holds. Let u0 ∈ Ωu ⊆ Rm is an initial
admissible control policy and the Q-function parameter estimate Θˆ 0 initialized in a compact set.
Suppose the last held state vector, xˆk, and the Q-function parameter vector, Θˆ k are updated by using
(3.2), (3.27) and (3.28), respectively, at the violation of the event-triggering condition (3.40). Then,
the control input (3.22) ensures the closed-loop event-triggered system state vector xk and the Q￾function parameter estimation error Θ˜ k converges to zero asymptotically, provided the regression
vector ξk satisfies PE condition and the following inequality

K˜kxk


2 ≤ l
Θ˜

Θ˜ T
k ZkZT
k Θ˜ k



I +ZT
k WkZk


(3.42)
holds where lΘ˜ > 0 is a positive constant. Moreover, the estimated control input converges to its
optimal value, i.e., uk → u∗
k as k → ∞.
Sketch of the Proof. Note that in an event-based control framework, the Lyapunov function need not
decrease monotonically during both the events and inter-event times (Wang and Lemmon, 2008).
The Lyapunov function may increase during the inter-event times, as shown in Figure 3.2. We only
need to show the existence of a piecewise continuous function h(k) ∈ R>0, such that
h(k) ≥ Ls
k for all k ∈ N and lim
k→∞
h(k) = 0, (3.43)
where Ls
k is a common Lyapunov function from both the event and inter-event times.104 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Figure 3.2 Evolution of the Lyapunov function during event and inter-event times
Consider the Lyapunov function candidate given by
Ls
k = Λ1Lx,k +Λ2LΘ˜ ,k, (3.44)
where Lx,k = xT
k xk and LΘ˜ ,k = Θ˜ T
k W−1
k Θ˜ k with Λ1 = π
2B2
maxl
Θ˜ and Λ2 = 2π with 0 < π < 1 and l
Θ˜
is a positive constant.
Case 1. At the sampled instants (k = ki). For simplicity, we will evaluate each term in the Lyapunov
function candidate (3.44) and combine them to get the overall first difference.
At the event-sampled instants with ek = 0, along the closed-loop system dynamics xk+1 = Axk −
BKˆkxk, k = ki, the first difference of the first term, ΔLx,k = xT
k+1xk+1 −xT
k xk with the relation K˜k =
K∗ −Kˆk and Cauchy-Schwartz (C-S) inequality, can be expressed as
ΔLx,k ≤ −(1−2ν¯)xk2 +2B2
max

K˜kxk


2
. (3.45)
The first difference of the second term, LΘ˜ ,k, in (3.44), the first difference is the same as in
(3.37) of Lemma 3.1. Combining the individual first differences (3.45) and (3.37), the overall first
difference ΔLs
k = Λ1ΔLx,k +Λ2ΔLΘ˜ ,k as
ΔLs
k ≤ −(1−2ν¯)xk2 +2B2
max

K˜kxk


2
− Θ˜ T
k ZkZT
k Θ˜ k

I +ZT
k WkZk

. (3.46)
From the hypothesis of the theorem 
K˜kxk


2 ≤ l
Θ˜
Θ˜ T
k ZkZT
k Θ˜ k
I+ZT
k WkZk , and the definitions of Λ1 and Λ2 in
(3.44), the overall first difference can be written as
ΔLs
k ≤ −(1−2ν¯)Λ1 xk2 −πZ¯2
min

Θ˜ k


2
< 0, (3.47)
where 0 < π < 1,0 < ν¯ < 1
2 . By Lyapunov theorem, the Lyapunov function is a decreasing function,
i.e., Ls
ki+1 < Ls
ki
.
Case 2. During the inter-event times (ki < k < ki+1). Consider the same Lyapunov function (3.44)
as in Case 1. The system dynamics during the inter-event times become
xk+1 = Axk −BKˆkxk +BKˆkek, ki < k < ki+1. (3.48)
The first difference of the first term, with system dynamics (3.48), Lemma 3.2, and Cauchy￾Swartz inequality, can be expressed as
ΔLx,k ≤ −(1−3ν¯)xk2 +3B2
max

K˜kxk


2
+3B2
max

Kˆk


2
ek2 . (3.49)Linear Discrete-time and Networked Control Systems 105
Recalling the triggering condition (3.40) with μ¯ = ν¯, one can reach at
ΔLx,k ≤ −(1−Γ)(1−3ν¯)xk2 +3B2
maxl
Θ˜

Θ˜ T
k ZkZT
k Θ˜ k



I +ZT
k WkZk

. (3.50)
The first difference of the second term for ki < k < ki+1 remains the same as in (3.34) in Lemma
3.1. Combing the individual first differences (3.50) and (3.34), the overall first difference ΔLs
k =
Λ1ΔLx,k +Λ2ΔLΘ˜ ,k, using the fact Θ˜ T
k ZkZT
k Θ˜ k
I+ZT
k WkZk ≤ Θ˜ k2, is expressed as
ΔLs
k ≤ −(1−Γ)(1−3ν¯)Λ1 xk2 +3Λ1B2
maxl
Θ˜

Θ˜ k


2
. (3.51)
From (3.34) in Lemma 3.1, Θ˜ k remains constant for ki < k < ki+1. Thus, 
Θ˜ k


2 = 
Θ˜ ki+1


2 ≤
BΘ˜ ,ki+1 for ki < k < ki+1. Substituting the inequality in (3.51), the first difference
ΔLs
k ≤ −(1−Γ)(1−3ν¯)Λ1 xk2 +Bs
Θ˜ ,ki+1, (3.52)
where Bs
Θ˜ ,ki+1 = 3Λ1B2
maxl
Θ˜ BΘ˜ ,ki+1. From (3.52), the first difference of the Lyapunov function
ΔLs
k < 0, as long as
xk >


Bs
Θ˜ ,ki+1
/(1−Γ)(1−3ν¯)Λ1 = Bc
x,ki+1. (3.53)
By Lyapunov theorem, the system state, xk and QFE parameter error Θ˜ k are bounded. Further,
system state xk converges to the ball of radius Bc
x,ki+1 in a finite time and Θ˜ k remains bounded.
The bound for the Lyapunov function (3.44) for ki < k < ki+1 can obtained by using the bounds
for xk and Θ˜ k as
BL,k = Λ1

Bc
x,ki+1
2
+Λ2
"
BΘ˜ ,ki+1
#2
for ki < k < ki+1. (3.54)
It follows that the Lyapunov function Ls
k for ki < k < ki+1 converges to the bound BL,k in a finite
time and stay within BL,k.
Now, from Case I and Case II, we will show the existence of a function h(k) such that (31) holds
to prove the asymptotic convergence of xk and Θ˜ k. With this effect, define a piecewise continuous
function
h(k) = max
Ls
k,BL,k

,k ∈ N. (3.55)
It is clear that h(k) ≥ Ls
k for all k ∈ N. From Lemma 3.1 BΘ˜ ,ki+1 → 0 with event sampled
instants ki → ∞. Therefore, from (3.53), Bc
x,ki+1 → 0 as ki → ∞. It follows that the bound BL,k → 0
as ki → ∞. Since, the Lyapunov function Ls
ki+1 < Ls
ki for k = ki and Ls
k → BL,k, ki < k < ki+1,Ls
k → 0
as ki → ∞. Consequently, the upper bound functions h(k) → 0 as ki → ∞. Since ki is a subsequence
of k ∈ N, by extension h(k) → 0 as k → ∞. Finally, since the QFE error converges to zero as ki → ∞,
u(t) → u∗(t) as ki → ∞.
The event-triggering condition can also be represented alternatively as a function of the last held
state ( ˆxk) by the ZOH. It is given in the following corollary.
Corollary 3.1. Consider the closed-loop event-triggered system (3.39), Q-function parameter es￾timation error dynamics (3.30) and assume Assumption 3.1 holds. Let u0 ∈ Ωu ⊆ Rm is an initial
admissible control policy and the Q-function parameter estimate Θˆ 0 initialized in a compact set.
Suppose the event-triggering condition is rewritten as
ek ≤
σETC
k
1+σETC
k
xˆk, (3.56)
where threshold σETC
k is defined in (3.40). Then, the closed-loop event-triggered system state vector
xk and the Q-function parameter estimation error Θ˜ k converge to zero asymptotically provided the
regression vector ξk satisfies PE condition.106 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Proof Sketch: It is routine to check that the triggering threshold in (3.56) is smaller than that in
(3.40). Therefore, Theorem 3.1 holds.
The minimum inter-event time, implicitly defined by either the event-triggering condition (3.40)
or, alternatively, (3.56), is the minimum time required for the event-triggering error to reach the
threshold value over all inter-trigger intervals (ki < k < ki+1), i ∈ N. In a discrete-time system the
minimum inter-event time is trivial and is equal to the sampling time (Ts) or, alternatively, δkmin =
min
i∈N {δki} = 1 where δki = ki+1 − ki, i ∈ N are the inter-event times. Hence, to demonstrate the
efficacy of the event-triggered control of a discrete-time system, the inter-event times must be non￾trivial, i.e., δki > 1, ∀i ∈ N).
Proposition 3.1. Consider the linear discrete-time system in (3.1), the controller (3.22), the event￾triggering condition in (3.56). Let the initial Q-function parameter estimate Θˆ 0 be initialized in a
compact set. Then, the inter-event times (δki) implicitly defined by (3.56), are given as
δki ≥
ln"
1+ 1
Mi
(F −1)σi
#
ln(F) , (3.57)
where σi = σETC
ki
1+σETC
ki
is the event-trigger threshold for ith inter-event time, σETC
ki =

Γ(1−3μ¯)
3Bmax

Kˆki


2
,
F = √μ¯ +BmaxKM, Mi = (√μ¯ +Bmax

K˜ki

+1),

K∗
k

 ≤ KM, and K˜k = K∗ −Kˆk. The inter-event
times becomes non-trivial when σi
Mi > 1.
Sketch of the Proof: The formula for the inter-trigger times can be derived by solving for the
dynamics of ek within an inter-trigger period of ki < k < ki+1 and comparing the solution with the
threshold.
Remark 3.4. The threshold σETC
ki is dependent on upon the estimated control gain Kˆk (i.e., implic￾itly on the control gain estimation error K˜k via the relation Kˆk = K∗ − K˜k and Mi, which is also a
function of K˜k). Thus, the inter-event times (δki) in (3.57) is a function of the parameter estimation
error K˜k. Alternatively, the convergence of the Q-function parameter estimation error to its ultimate
bound, as proven in Lemma (3.1), will increase the threshold value σETC
ki and reduce Mi in (3.57).
This will increase the inter-event times δki and leads to non-trivial values, i.e., δki > 1, k ∈ N.
Example 3.1. Consider the benchmark example of the batch reactor whose discrete-time with a
sampling interval of Ts = 0.01 s is given as
xk+1 = Axk +Buk, yk = Cxk, (3.58)
where A =
⎡
⎢
⎢
⎣
1.0142 −0.0018 0.0651 −0.0546
−0.0057 0.9582 −0.0001 0.0067
0.0103 0.0417 0.9363 0.0563
0.0004 0.0417 0.0129 0.9797
⎤
⎥
⎥
⎦
, B =
⎡
⎢
⎢
⎣
4.7798×10−6 −0.0010
0.0556 1.5316×106
0.0125 −0.0304
0.0125 −0.0002
⎤
⎥
⎥
⎦
and C =

101 −1
010 0 
. Assume the system states are available for measurement and design and
simulate a near-optimal controller and associated parameter update laws using the event-triggered
Q-learning approach in Tables 3.1.
Solution: We can use Table 3.1 for the controller and update laws. The quadratic cost function was
chosen as in (3.5) with the penalty matrices P = I4×4 and R = I2×2 where I denotes the identity
matrix. The initial system states were selected x0 = 
0.1 −0.1 0.3 −0.5
T
. The initial param￾eter vector Θˆ 0 ∈ Rlg=21 was chosen at random from a uniform distribution in the interval [0, 1].Linear Discrete-time and Networked Control Systems 107
Figure 3.3 Performance of the event-based controller with state feedback: (a) convergence of state vector,
(b) evolution of the event-trigger threshold and the event-trigger error, and (c) the total number of trigger
instants.
The design parameters were β = 2×105, μ¯ = 0.3, and Γ = 0.1. The PE condition was satisfied by
adding a zero mean Gaussian noise with the control input. The simulation was conducted for 10 sec
with a sampling time of 0.01 sec or 1000 sampling instants.
The event-based optimal controller’s performance is illustrated in Figure 3.3. The convergence
of the system state close to zero and the event-trigger threshold are depicted in Figure 3.3 (a) and
(b), respectively. The threshold converged as the state converged. The event-trigger error shown in
Figure 3.3(b) evolved during the inter-event times; it resets to zero at the triggered instants. The
cumulative number of trigger instants plotted in Figure 3.3 (c) was found to be 108 out of 1000
sampling instants. This implies the controller and the Q-function parameters were updated only 108
times. Thus, the computation was reduced when compared to the traditional discrete-time systems.
The inter-event times plotted in Figure 3.3(d) shows the aperiodic occurrence of events validated by
the design. The QFE error is illustrated in Figure 3.4 (b). The QFE error converged close to zero.
This implies the near optimality of the adaptive controller is achieved in finite time.
3.3 OUTPUT FEEDBACK DESIGN
This section extends the state-feedback event-triggered optimal control design to the output feed￾back design.
Assumption 3.2. The system (3.1) is observable and the output matrix C ∈ Rpo×n is considered
known.
The performance index can be redefined as a function of the system output, given by
Jk =
∞
∑
j=k
r
y
(yj,uj), (3.59)
where ry(yj,uj) = yT
j Pyyj + uT
j Ruj and uj is the sequence of event-based control input that min￾imizes the value function. The performance index can be rewritten by using the output equation108 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Figure 3.4 (a) Control policy, (b) QFE errors, and (c) parameter estimation error.
as
Jk =
∞
∑
j=k
xT
j CTPy
Cxj +uT
j Ruj =
∞
∑
j=k
xT
j Px j +uT
j Ruj, (3.60)
where P = CTPy
C. Note that if the system state vector can be reconstructed, then the state feedback
design can be used for the output feedback case. Hence, an adaptive observer-based approach is
proposed next.
3.3.1 ADAPTIVE OBSERVER DESIGN
An adaptive observer is included at the sensor node to estimate the system states. The observer has
access to the system output (yk) at every periodic time instant k. Therefore, the adaptive observer
parameters can be tuned using a traditional time-triggered tuning law at every time instant k, unlike
the QFE tuning law, as discussed in the previous section. This architecture needs additional compu￾tational power at the sensor node when compared to the state feedback design and can be considered
as a trade-off.
3.3.1.1 Adaptive Observer Design
Consider the adaptive observer dynamics given by
xo
k+1 = Aˆkxo
k +Bˆkuk +Lˆ k (yk −yo
k ),
yo
k = Cxo
k , (3.61)
where xo
k ∈ Rn and yo
k ∈ Rpo are the observer state and the output vectors, respectively. The matri￾ces Aˆk ∈ Rn×n and Bˆk ∈ Rn×m are the estimated observer system matrices, and Lˆ k ∈ Rn×po is the
estimated observer gain matrix.
In a parametric form, the observer dynamics can be represented as
xo
k+1 = ψˆ T
k φk, (3.62)Linear Discrete-time and Networked Control Systems 109
where ψˆ k = [Aˆk Bˆk Lˆ k]
T
∈ R(n+m+po)×n is the estimated observer parameter matrix. The regres￾sion vector for the observer is denoted as φk = [xoT
k uT
k e
yT
k ]
T
∈ R(n+m+po)
, where e
y
k = yk −yo
k is
the output error vector. The observer state estimation error can be defined as
ex
k = xk −xo
k , (3.63)
and the state estimation error dynamics from (3.63) and (3.61) become
ex
k+1 = Axk +Buk −Aˆkxo
k −Bˆkuk −Lˆ k (yk −yo
k )=(A−LC) ex
k +ψ˜ T
k φk, (3.64)
where ψ˜ k = [A˜k B˜k L˜ k]
T
∈ R(n+m+po)×n is the observer parameter estimation error with ψ˜ k =
ψ − ψˆ k, A˜k = A − Aˆk, B˜k = B − Bˆk, and L˜ k = L − Lˆ k. The observer ideal target parameters ψ = 
ABLT
∈ R(n+m+p)×n and L is the ideal observer gain matrix. The observability of the system
in Assumption 3.2 guarantees the existence of an ideal observer gain matrix L such that A−LC is
Schur stable.
The observer output error dynamics can be written by using (3.64) as
e
y
k+1 = C(A−LC) ex
k +Cψ˜ T
k φk. (3.65)
A time-triggered periodic tuning law
ψˆ k+1 = ψˆ k +αo
ψ
φke
yT
k+1X
1+φ T
k φk
, (3.66)
can be selected to estimate the observer parameters, thereby guaranteeing that the observer state
estimation error converges to zero asymptotically. The learning gainαo
ψ > 0 and X ∈ Rpo×n is a
constant matrix to match the dimension and selected such that 
XTC

 ≤ 1. The observer parameter
estimation error dynamics can be computed from (3.66) as
ψ˜ k+1 = ψ˜ k −αo
ψ
φke
yT
k+1X
1+φ T
k φk
,∀k. (3.67)
The boundedness of the observer parameter estimation error is presented in the following lemma.
Lemma 3.3. Consider the adaptive observer (3.61) in a parametric form (3.62) and let Assumption
3.2 holds. Assume the initial observer parameters ψˆ0 are initialized in a compact set Ωψ. Suppose
the observer parameters are updated by the tuning law (3.66). Then, the observer state estima￾tion error ex
k and the parameter estimation error ψ˜ k converge asymptotically to zero provided the
regression vector φk satisfies PE condition and the learning gain satisfies 0 < αo
ψ < cmin
2(1+φk2
)
.
Proof. The proof is similar to the proof of Lemma 3.1. For a detailed proof, refer to (Sahoo, 2015)
(Chapter 1, proof of Lemma 5.1).
3.3.2 OBSERVER-BASED CONTROLLER DESIGN AND CLOSED-LOOP STABILITY
The event-based observer state vector will be used to design the near-optimal control input. There￾fore, the observer states are sent to the controller at the triggered instants, similar to the state feed￾back case. The event-triggering error now can be redefined as the difference between the current
observer state and the last released observer state as
e
o,ET
k = xo
k −x
o,c
k , ki ≤ k < ki+1. (3.68)110 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Table 3.2
Observer-based feedback controller design
System dynamics xk+1 = Axk +Buk, yk = Cxk
Observer dynamics xo
k+1 = Aˆkxo
k +Bˆkuk +Lˆ k

yk −yo
k

, yo
k = Cxo
k
Adaptive observer update ψˆ k+1 = ψˆ k +αo
ψ
φke
yT
k+1X
1+φT
k φk
,
Event-triggering error e
o,ET
k = xo
k −x
o,c
k , ki ≤ k < ki+1.
Q-function estimator (QFE) Qˆ(x
o,c
k ,uk) = $
z
oT
k Gzz ˆ o
k = Θˆ T
k ξ o
k , k = ki,
z
o,cT
k Gˆ kz
o,c
k = Θˆ T
k ξ o,c
k , ki < k < ki+1,
QFE residual error e
o,V
k =
$
xoT
k Pxo
k +uT
k Ruk +Θˆ T
k Δξ o
k , k = ki,
x
o,cT
k Pxo,c
k +uT
k Ruk +Θˆ T
k Δξ o,c
k , ki < k < ki+1,
Auxilary QFE error Ξo,V
k = Πo
k +Θˆ T
k Zo
k , k = ki
Q-function estimator update Θˆ k =
⎧
⎪⎪⎨
⎪⎪⎩
Θˆ k−1 − αo
V Zo
k−1Ξo,VT
k−1



I +ZoT
k−1Zo
k−1



, k = ki,
Θˆ k−1, ki−1 < k < ki.
Control input uk = −Kˆkx
o,c
k = −

Gˆ uu
k
T
Gˆ ux
k x
o,c
k , ki ≤ k < ki+1
Event-triggering condition


e
0,ET
k


 ≤ σo,ETC
k

xo
k

, σo,ETC
k =


Γo
ET (1−4μ¯)/4B2
max

Kˆk


2
The event-sampled observer state vector at the controller x
o,c
k can be redefined as
x
o,c
k =
$
xo
k , k=ki,
xo
ki
, ki < k < ki+1. (3.69)
The action-dependent value function or the Q-function for the output feedback design with value
function (3.60) is the same as that in the state feedback case. Therefore, the optimal control input
can be computed by estimating the Q-function parameter matrix G or, alternatively, the parameter
vector Θ defined in (3.19). The event-based QFE with observer’s state can be written as
Qˆ(x
o,c
k ,uk) =
⎧
⎨
⎩
z
oT
k Gzz ˆ o
k = Θˆ T
k ξ o
k , k = ki,
z
o,cT
k Gˆ kz
o,c
k = Θˆ T
k ξ o,c
k , ki < k < ki+1,
(3.70)
where the event-based regression vectors ξ o
k = zo
k ⊗zo
k , ξ o,c
k = z
o,c
k ⊗z
o,c
k , zo
k = [xoT
k uT
k ]
T and z
o,c
k =Linear Discrete-time and Networked Control Systems 111
[x
o,cT
k uT
k ]
T
.
The Q-function estimation error, based on the observer states, becomes
e
o,V
k =
⎧
⎨
⎩
xoT
k Pxo
k +uT
k Ruk +Θˆ T
k Δξ o
k , k = ki,
x
o,cT
k Pxo,c
k +uT
k Ruk +Θˆ T
k Δξ o,c
k , ki < k < ki+1,
(3.71)
where Δξ o
k = ξ o
k+1 −ξ o
k and Δξ o,c
k = ξ o,c
k+1 −ξ o,c
k .
Similar to the state feedback case, the augmented error using the time history of the observer
states and the control input at the trigger instant can be defined as
Ξo,V
k = Πo
k +Θˆ T
k Zo
k , k = ki (3.72)
where Πo
k = [r(xo
ki
,uki
) r(xo
ki−1
,uki−1 ) ··· r(xo
ki−1−j
,uki−j
] and Zo
k = [Δξ o
ki Δξ o
ki−1 ··· Δξ o
ki−1
] for k =
ki with 0 < j < ki. A tuning law can be defined to tune the Q-function parameter estimates as
Θˆ k =
⎧
⎪⎪⎨
⎪⎪⎩
Θˆ k−1 − αo
V Zo
k−1Ξo,VT
k−1



I +ZoT
k−1Zo
k−1



, k = ki,
Θˆ k−1, ki−1 < k < ki.
(3.73)
The parameter estimation error dynamics by using (3.73) with a forwarded time step can be
represented as
Θ˜ k+1 =
⎧
⎪⎪⎨
⎪⎪⎩
Θ˜ k +
αo
V Zo
kΞ0,VT
k



I +ZoT
k Zo
k



, k = ki,
Θ˜ k, ki < k < ki+1,
(3.74)
The observer-based QFE error (e
o,V
k ) from (3.72) and (3.24) at the triggering instants k = ki is
expressed in terms of the parameter estimation error Θ˜ k as
e
o,V
k = −Θ˜ T
k Δξ o
k +ΘT (Δξ o
k −Δξk) + f(xo
k )− f(xk), k = ki (3.75)
where f(xo
k ) = xoT
k Pkxo
k and f(xk) = xT
k Pkxk. The function f () is a continuous function. Therefore it
satisfies the Lipschitz continuity given as 
 f

xo
k

− f (xk)

 ≤ Lf ex
k. The observer-based augmented
QFE error (3.72) using (3.75) can be rewritten as
Ξo,V
k = −Θ˜ T
k Zo
k +ΘT (Zo
k −Zk) +Fo
k −Fk, k = ki, (3.76)
where Fo
k = [xoT
ki Pxo
ki
,xoT
ki−1
Pxo
ki−1
,··· , xoT
ki−j−1
Pxo
ki−j−1
] and Fk = [xT
ki
Pxki
,xT
ki−1
Pxki−1 ,··· , xT
ki−j−1
Pxki−j−1 ].
The estimated control (3.22) with the event-based observer state vector and the Q-function esti￾mated parameters can be written as
uk = −Kˆkx
o,c
k = −

Gˆ uu
k
T
Gˆ ux
k x
o,c
k , ki ≤ k < ki+1. (3.77)
The closed-loop dynamics of the observer-based event-triggered system from (3.1) and control
input (3.77) can be described as
xk+1 = Axk −BKˆkx
o,c
k = Axk −BKˆkxo
k +BKˆke
o,ET
k
= (A−BKˆk)xk +BKˆkex
k +BKˆke
o,ET
k , k ≤ k < ki+1. (3.78)112 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Consider the event-triggering error (3.68). The event-triggering condition as a function of the event￾triggering error, the Q-function estimated parameter, and the observer state vector is selected as


e
0,ET
k


 ≤ σo,ETC
k xo
k, (3.79)
where σo,ETC
k =
) Γo
ET (1−4μ¯)
4B2
maxKˆ
k
2 is the event-trigger threshold, 0 < Γo
ET < 1, μ¯ < 1/4. The main
results of the output feedback design are claimed in the next theorem.
Theorem 3.2. Consider the uncertain LTI discrete-time system (3.1), the adaptive observer (3.61),
and the observer-based controller (3.77) represented as a closed-loop event-triggered system (3.78).
Let the Assumption 3.2 holds and the regression vectors φk and ξ o
k satisfy the PE condition. Sup￾pose u0 ∈ Ωu ⊂ Rmis the initial admissible control policy and the initial parameters Θˆ 0 and ψˆ0
initialized in compact sets. Let the state vector (xo,c
k ) and the Q-function parameter vector (Θˆ k)
are updated, respectively, by (3.69) and (3.73) at the violation of the triggering condition (3.79).
Then, the closed-loop event-triggered system state vector xk , the Q-function parameter estima￾tion error Θ˜ k, the observer state estimation error ex
k, and the observer parameter estimation error
ψ˜ k converge to zero asymptotically. Further, the estimated control input converge to the optimal
control input, i.e., uk → u∗
k , as k → ∞ provided the design parameters satisfy 0 < αo
V < 1/3 and
0 < αo
ψ < cmin/2
"
1+φk2
#
.
Proof. The proof is similar to the proof of Theorem 3.1 in the state feedback case. The detailed
proof of the theorem can be found in (Sahoo, 2015) (Chapter 1, Theorem 5.2)
Example 3.2. Consider the benchmark example of the batch reactor as given in (3.58). Assume the
system states are not available for measurement, and the only available signal to measure is the
output of the system. Design and simulate a near-optimal controller using the output measurement
and associated parameter update laws using the event-triggered Q-learning approach in Tables 3.2.
Select the parameters of the performance index, learning gains, and initial values of the state and
parameters. Plot all the signals, including event-trigger condition, state, and parameter estimation
errors. Comment on the convergence of the system states and QFE error convergence.
Solution: The simulation parameters were chosen as follows. The adaptive gains for the observer
were α0
Ψ = 0.01, αo
V = 0.01, μ¯ = 0.2, and Γo
ET = 0.1. The observer parameters were initialized
randomly from the uniform distribution in the interval [0,0.5]. The initial observer states were given
as xo
0 = [.02, −.02, .03, −0.1]
T . The penalty matrices were P = I4×4 and R = I2×2. The remaining
parameters for the state feedback in Example 3.1 are used here as well.
The observer-based output feedback controller performance is illustrated in Figures 3.5 and 3.6.
The system states, and the event-triggering threshold are converged close to zero, as shown in Figure
3.5 (a) and (b). It was observed that the number of cumulative trigger instants was increased to 115,
as shown in Figure 3.5 (c) when compared to the state feedback case. This is due to the additional
uncertainty introduced by the adaptive observer. The inter-event times are plotted in Figure 3.5
(d). The QFE and the TC errors are converged to zero (shown in Figure 3.6 (a) and (b)), implying
optimality was achieved in the final time. The convergence observer state estimation error is shown
in Figure 3.6.
3.4 EVENT-SAMPLED NETWORKED CONTROL SYSTEMS
In the previous event-based state and output feedback schemes, the communication network is not
considered. This section introduces an event-triggered design for networked control systems (NCS)
(Walsh et al., 2002; Xu et al., 2012) consisting of a communication network enclosed within theLinear Discrete-time and Networked Control Systems 113
Figure 3.5 The event-based controller performance of output feedback design: (a) Convergence of the sys￾tem states, (b) evolution of both the event-trigger threshold and the error, (c) the cumulative number of triggers.
feedback loop. Despite the advantages, such as flexibility in remote control and spatial distribution
of the systems, the communication network introduces imperfections, such as time-varying delays,
random packet losses, and quantization errors (Walsh et al., 2002) that significantly affect the sys￾tem’s stability and performance. Traditional control design approaches use a fixed sampling interval
to transmit the feedback information (Walsh et al., 2002; Xu et al., 2012) and execute the con￾trol policy. This periodic transmission requires a large bandwidth, which is often challenging when
shared communication networks are employed for control applications.
In the past decade, various stable (Astr ˚ om and Bernhardsson, 1999; Wang and Lemmon, 2011b) ¨
and optimal (Narayanan and Jagannathan, 2016b) event-triggered control schemes have been pro￾posed for linear and nonlinear systems in the presence of delays and packet losses (Wang and Lem￾mon, 2011b) for systems with known and unknown dynamics. Since the randomness of delays and
packet losses are encountered in NCS, the optimal controller design for uncertain NCS is presented
using stochastic control theory in (Xu et al., 2012, 2014b). In this section, a co-design approach
from (Sahoo et al., 2017a), to optimize both the transmission instants and the control policy for
uncertain NCS, is proposed to regulate the system optimally. The NCS is represented as a stochastic
time-varying discrete-time system by including random delays and packet losses. A stochastic cost
function is defined to co-optimize the control input and transmission intervals. The problem is rep￾resented as a discrete-time min-max problem (Basar and Bernhard, 2008) where the optimal control
policy minimizes the cost function, and the control input error maximizes it.
3.4.1 NCS REFORMULATION AND PROBLEM STATEMENT
Consider an NCS represented by a continuous time linear time-invariant (LTI) system given by
x˙(t) = Ax(t) +α(t)Bu(t −τ(t)), x(0) = x0, (3.80)
where x(t) ∈ Rn and u(t) ∈ Rm are the system state and control input vectors and A ∈ Rn×n and
B ∈ Rn×m, are uncertain time-invariant system and input matrices, respectively. The random time
delay τ(t) = τsc(t)+τca(t) where τsc(t) and τca(t) are mutually independent delays from the sensors
to controller (S-C) and controller to actuator (C-A) channels, respectively. The random packet losses114 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Figure 3.6 The performance of the optimal controller: (a) Convergence of observer state error; (b) optimal
control input; (c) value function estimation error.
α(t) = αsc(t)αca(t) is the packet loss indicator defined as
α(t) = $
1, control input received at t;
0, control input not received at t. (3.81)
where αsc(t) and αca(t) are the S-C channel and C-A channel mutually independent packet loss
indicators.
Assumption 3.3. The pair (A,B) is controllable and the state vector is measurable. Further, the
order of the system is known.
Since a packet switch communication network is used in the feedback loop, as shown in Fig. 3.7,
the controller is implemented in a sampled data framework, i.e., the sensor samples the system state
at time instants kTs, where k ∈ N and Ts is the fixed sampling period. The control input remains
constant between two consecutive sampling instants. To make the design tractable, the following
standard properties of the communication network are assumed (Liou and Ray, 1991).
Assumption 3.4. (Liou and Ray, 1991) (i) the initial value of the system states are deterministic;
(ii) The time-varying random delays are bounded satisfying τk
sc ≤ Δs and τ k
ca ≤ dTs where Δs < Ts
is a fixed skew between the sensor and controller sampling instants, d is a positive integer and
superscript k denotes the variable at time instant k. Further τ k
sc and τ k
ca are mutually independent
process with known statistics; and (iii) The random packet losses αk
sc and αk
ca are i.i.d and follows
Bernoulli distribution with P(αk = 1) = P(αk
sc = 1)P(αk
ca = 1) = αp with P(·) is the probability of
the random variable.
From Assumption 3.4 (ii), the upper bound of the C-A channel delays τ k
ca is d. Therefore, a
maximum of d +1 delayed control inputs can be received at the actuator within a sampling interval
[kTs,(k + 1)Ts], without any packet loss, i.e., αk = 1. The control inputs are applied to the plantLinear Discrete-time and Networked Control Systems 115
Plant
Update Mechanism
Sensor
Controller
L N [
[  %X$[ Triggering 
Mechanism
Q-function 
estimator
d
Q-function
estimator
N [
Network
Actuator 
with ZOH
Controller to actuator channel Sensor to controller channel
Figure 3.7 Block diagram of event-triggered NCS.

N
W 
N
GW  
N W 
N
W
Sensor
Controller
Actuator
  V N G7 
VF
   V N 7     V N 7  V N7    V N 7 
Figure 3.8 Timing diagram of transmission with time-varying delay.
instantly once they are received. A packet reordering mechanism is used such that the latest control
input is held at the actuator till the next is available and applied to the system at time kTs +t
k
l where
t
k
l , l = 0,1,2,··· ,d is the time after kTs satisfying t
k
l > t
k
l+1 and t
k
−1 = Ts and Tk
d = 0, as shown in
Fig. 3.8. In case of multiple packets arriving simultaneously, the one with the latest timestamp is
used, and old ones are discarded.
A discrete time version of system in (3.80), by incorporating the delays τk and packet losses αk,
can be obtained by integrating (3.80) (Liou and Ray, 1991) and is given by
xk+1 = As
xk +
d
∑
l=0
αk−lBk
l uk−l, (3.82)
where xk = x(kTs), As = eATs, Bk
l = * t
k
l−1
t
k
l
eA(Ts−w)
dwB with t
k
−1 = Ts and t
k
d = 0.
In the event-based control framework, the state vector received at the controller time instants is
given by xc
k = xki
, ki ≤ k < ki+1,i = 1,2,··· . (3.83)
where xc
k is the event-based received state at the controller.
Remark 3.5. Since the delay τ k
sc < Δs, the system states are available at the controller before the
skewed controller sampling instants for controller execution when no packet loss occurs in the S-116 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
C channel. Therefore, the mirror estimator at the triggering mechanism, discussed in the previous
sections, is synchronized to evaluate the triggering condition and execute the controller.
The event-based control input, with state xc
k at the controller, can be expressed as
us
k = μ(xc
k) = μ(xki
), ki ≤ k < ki+1,i = 1,2,··· , (3.84)
where μ : Rn → Rm. The error introduced in the control input due to the event-based transmission
is defined as eu
k = us
k −uk = μ(xk)− μ(xc
k), (3.85)
where eu
k is the control input error. With event-based current and delayed control inputs us
k, ···, us
k−d
the system (3.82) can be expressed as
xk+1 = As
xk +
d
∑
l=0
αk−lBk
l us
k−l. (3.86)
Replacing event-based current and delayed control inputs us
k−l = uk−l + eu
k−l
, l = 0,1,··· ,d from
(3.85), the event based system (3.86) can be expressed as
xk+1 = As
xk +
d
∑
l=0
αk−lBk
l uk−l +
d
∑
l=0
αk−lBk
l eu
k−l. (3.87)
For the co-optimization of control policy and transmission intervals, consider the infinite horizon
stochastic cost function
J(x0) = E
τ,α
$ ∞
∑
k=0

xT
k Hxk +uT
k Ruk −γ2euT
k eu
k

+
(3.88)
where H ∈ Rn×n and R ∈ Rm×m are constant symmetric positive definite user defined matrices and
γ > γ∗ is the penalty for the control input error, where γ∗ is the minimum value of γ such that the
cost function (3.88) is finite. The operator E
τ,α {} is the expected value of (·).
Defining the augmented state zk = [xT
k uT
k−1 ··· uT
k−d euT
k−1 ··· euT
k−d]
T ∈ Rn+2dm, the discretized
system in (3.87) can be expressed as
zk+1 = Aτ,α
k zk +Bτ,α
k uk +Eτ,α
k eu
k (3.89)
where Aτ,α
k ∈ R(n+2dm)×(n+2dm)
, Bτ,α
k ∈ R(n+2dm)×m, and Eτ,α
k ∈ R(n+2dm)×m, are defined in (3.90).
The cost function for the augmented NCS (3.89) can be defined by redefining the weight matrices
in (3.88) [2] as
J (zk) = E
τ,α
$ ∞
∑
j=k
r(zj,uj,eu
j)
+
(3.91)
where r(zk,uk, eu
k ) = zT
k Hzzk +uT
k Rzuk −γ2
z euT
k eu
k with Hz = diag{H,R/(d+1),··· ,R/(d+1)}, Rz =
R/(d +1) are positive definite matrices, and γ2
z = γ2/(d +1).
The co-optimization of the event-triggered NCS in (3.86) with performance index (3.88) is the
same as the co-optimization of the augmented NCS (3.89) with performance index (3.91). Since
the objective is to co-optimize the transmission intervals δki = ki+1 −ki,i = 1,2,··· and the control
policy uk, the problem can be formulated as a two-player zero-sum game where the control input uk
is the player I and the input error eu
k is the player II. The Player I’s objective is to minimize the cost
function (3.91) while the player II attempts to maximize it.
Therefore, the optimization problem leads to a min-max problem where the optimal value V∗(zk)
of the cost function (3.91) is given by
V∗(zk) = min
uk
max
eu
k
E
τ,α {r(zk,uk,eu
k ) +V(zk+1)} (3.92)Linear Discrete-time and Networked Control Systems 117
Aτ,α
k =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
As αk−1Bk
1 ··· αk−(d−1)Bk
d αk−dBk
d αk−1Bk
1 ··· αk−(d−1)Bk
d αk−dBk
d
0 0 ··· 0 00 ··· 0 0
0 Im ··· 0 00 ··· 0 0
.
.
.
.
.
. .
.
. .
.
.
.
.
. 0 ··· 0 0
0 0 ··· Im 0 0 ··· 0 0
0 0 ··· 0 00 ··· 0 0
0 0 ··· 0 0 Im ··· 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 0 .
.
. 0 0
0 0 ··· 0 00 ··· Im 0
,
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
Bτ,α
k =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
αkBk
0
Im
0
···
0
0
0
···
0
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
, and Eτ,α
k =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
αkBk
0
0
0
···
0
Im
0
···
0
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
(3.90)
where V(zk+1) is the cost-to-go from time k + 1 onward. For a given optimal sequence (u∗
k ,eu∗
k ),
observable pair ( E
τ,α
{Aτ,α
k },
√Hz), and γz > γ∗
z a saddle point solution exists (Basar and Bernhard,
2008) i.e.,
min
uk
max
eu
k
J = max
eu
k
min
uk
J.
The solution to the min-max problem can be obtained by solving the stochastic game Ricatti
(SGR)-like equation with known system matrices Aτ,α
k , Bτ,α
k , and Eτ,α
k . Since the matrices Aτ,α
k ,
Bτ,α
k , and Eτ,α
k are stochastic and uncertain, one needs to develop solution approach that is com￾pletely data-driven, as presented in the next section.
3.4.2 CO-OPTIMIZATION UNDER UNCERTAIN NCS DYNAMICS
In this section, a model-free solution to the co-optimization problem for the stochastic uncertain
system is presented using event-based stochastic Q-learning.
Define the Hamiltonian for the cost function (3.91) as
H (zk,uk, eu
k ,V∗(zk+1)) = E
τ,α {r(zk,uk,eu
k ) +V∗(zk+1)}. (3.93)
Assuming the stochastic game has a solution, the stochastic optimal value function (3.92) in a
quadratic form (Lewis et al., 2012b) is given by V∗(zk) = E
τ,α
(zT
k Skzk), where Sk is the time-varying
positive definite symmetric matrix solution of the SGR-like equation given by.
E
τ,α
(Sk) = E
τ,α
(Aτ,αT
k Pk+1Aτ,α
k )−Hz −

E
τ,α
(Aτ,αT
k Pk+1Bτ,α
k ) E
τ,α
(Aτ,αT
k Pk+1Eτ,α
k )

×
⎡
⎣
Rz + E
τ,α
(Bτ,αT
k Pk+1Bτ,α
k ) E
τ,α
(Bτ,αT
k Pk+1Eτ,α
k )
E
τ,α
(Eτ,αT
k Pk+1Bτ,α
k ) E
τ,α
(Eτ,αT
k Pk+1Eτ,α
k )−γ2
z I
⎤
⎦
⎡
⎣
E
τ,α
(Bτ,αT
k Pk+1Aτ,α
k )
E
τ,α
(Eτ,αT
k Pk+1Aτ,α
k )
⎤
⎦
(3.94)
The optimal control input u∗
k using the stationarity condition can be found as
u∗
k = K∗
k zk, (3.95)118 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
where
K∗
k =−

Rz + E
τ,α
(Bτ,αT
k Sk+1Bτ,α
k )− E
τ,α
(Bτ,αT
k Sk+1Eτ,α
k )

E
τ,α
(Eτ,αT
k Sk+1Eτ,α
k )−γ2
z I
−1
× E
τ,α
(Eτ,αT
k Sk+1Bτ,α
k )
−1

E
τ,α
(Bτ,αT
k Sk+1Eτ,α
k )

E
τ,α
(Eτ,αT
k Sk+1Eτ,α
k )−γ2
z I
−1
× E
τ,α
(Eτ,αT
k Sk+1Aτ,α
k )− E
τ,α
(Bτ,αT
k Sk+1Aτ,α
k )

(3.96)
is the optimal control input gain, and the optimal control input error eu∗
k is given by
eu∗
k = L∗
k zk, (3.97)
where L∗
k the optimal control input gain given by
L∗
k =−

E
τ,α
(Eτ,αT
k Sk+1Eτ,α
k )−γ2
z I − E
τ,α
(Eτ,αT
k Sk+1Bτ,α
k )

E
τ,α
(Bτ,αT
k Sk+1Bτ,α
k ) +Rz
−1
× E
τ,α
(Bτ,αT
k Sk+1Eτ,α
k )
−1

E
τ,α
(Eτ,αT
k Sk+1Bτ,α
k )

E
τ,α
(Bτ,αT
k Sk+1Bτ,α
k ) +Rz
−1
× E
τ,α
(Bτ,αT
k Sk+1Aτ,α
k )− E
τ,α
(Eτ,αT
k Sk+1Aτ,α
k )

.
(3.98)
Alternatively, we can state that the optimal control input u∗
k is stabilizing under the worst-case
error eu∗
k . Therefore, the optimal triggering instants can be determined by using the worst-case con￾trol input error L∗
k zk as a threshold of the triggering condition. Thus, the triggering condition can be
defined as
euT
k eu
k = E
τ,α
{z
T
k L∗T
k L∗
k zk}. (3.99)
Further, by emulation-based approach, the event-based optimal control input can be expressed as
us∗
k = K∗
k z
s
k, ki ≤ k < ki+1 (3.100)
where us∗
k is the event-based optimal control input and zs
k = zki
,ki ≤ k < ki+1 is the event-based aug￾mented state transmitted at the triggering instants. With the NCS dynamics uncertain, the gains K∗
k
and L∗
k can not be computed analytically. Therefore, implementing the controller in (3.100) and trig￾gering condition (3.99) is impossible. Therefore, an estimation-based approach using stochastic Q￾learning is developed. Similar to the deterministic case in the previous section, the action-dependent
value function or the Q-function can be written as
Q(zk,uk,eu
k ) = E
τ,α {(r(zk,uk,eu
k ) +V∗(zk+1))} (3.101)
where Q(zk,uk,eu
k ) is the Q-function and V∗(zk+1) is the optimal cost for k + 1 onward. The opti￾mal value function for the time instant k + 1 in a quadratic form can be expressed as V∗(zk+1) =
E
τ,α {zT
k+1Sk+1zk+1}.
Substitute V∗(zk+1) in (3.101) to get Q(zk,uk, eu
k ) = E
τ,α {zT
k Pzzk +uT
k Rzuk +zT
k+1Sk+1zk+1}. Along
the system dynamics (3.89), the Q-function,further, leads to
Q(zk,uk,eu
k ) = E
τ,α
{[zT
k uT
k euT
k ]Gk[zT
k uT
k euT
k ]
T
} = E
τ,α {wT
k Gkwk},∀k (3.102)Linear Discrete-time and Networked Control Systems 119
where wk = [zT
k uT
k euT
k ]
T ∈ Rn+(d+2)m = lmn, and Gk =
⎡
⎣
Gzz
k Gzu
k Gze
k
Guz
k Guu
k Gue
k
Gez
k Geu
k Gee
k
⎤
⎦ ∈ Rlmn×lmn .
Q(zk,uk,eu
k ) = E
τ,α
{[zT
k uT
k euT
k ]Gk[zT
k uT
k euT
k ]
T
} = E
τ,α {wT
k Gkwk},∀k (3.103)
where wk = [zT
k uT
k euT
k ]
T ∈ ℜn+(d+2)m = lmn, and Gk =
⎡
⎣
Gzz
k Gzu
k Gze
k
Guz
k Guu
k Gue
k
Gez
k Geu
k Gee
k
⎤
⎦ ∈ ℜlmn×lmn where the
sub matrices G(·)
k defined as
Gk =
⎡
⎢
⎢
⎢
⎢
⎣
Hz + E
τ,α
(Aτ,αT
k Sk+1Aτ,α
k ) E
τ,α
(Aτ,αT
k Sk+1Bτ,α
k ) E
τ,α
(Aτ,αT
k Sk+1Eτ,α
k )
E
τ,α
(Bτ,αT
k Sk+1Aτ,α
k ) Rz + E
τ,α
(Bτ,αT
k Sk+1Bτ,α
k ) E
τ,α
(Bτ,αT
k Sk+1Eτ,α
k )
E
τ,α
(Eτ,αT
k Sk+1Aτ,α
k ) E
τ,α
(Eτ,αT
k Sk+1Bτ,α
k ) E
τ,α
(Eτ,αT
k Sk+1Eτ,α
k )−γ2
z I
⎤
⎥
⎥
⎥
⎥
⎦
.
(3.104)
The optimal Q-function Q∗(zk,u∗
k , eu∗
k ) is equals the optimal value function V∗(zk) when both
uk = u∗
k and eu
k = eu∗
k , i.e., E
τ,α
{Q∗(zk,u∗
k , eu∗
k )} = E
τ,α
{V∗(zk)}. Therefore, the optimal control input
gain K∗
k in terms of the Q-function parameters Gk can be written as
K∗
k = (Guu
k −Gue
k Gee−1
k Geu
k )
−1(Gue
k Gee−1
k Gez
k −Guz
k ). (3.105)
Similarly, the control input error gain L∗
k is given by
L∗
k = (Gee
k −Geu
k Guu−1
k Gue
k )
−1(Geu
k Guu−1
k Guz
k −Gez
k ). (3.106)
Remark 3.6. The gains K∗
k and L∗
k are well defined, i.e., the inverses in (3.105) and (3.106) exist
if γ2
z I − E
τ,α
{Eτ,αT
k Sk+1Eτ,α
k } > 0. This condition is a required condition for the existence of the
saddle point solution (Basar and Bernhard, 2008) and ensures the optimal control input u∗
k is strictly
feedback stabilizing in the presence of worst-case control input error eu∗
k .
Assumption 3.5. Xu et al. (2012) The Q-function is slowly time-varying and can be expressed as
linear in the unknown parameters.
In a parametric form the Q-function can be represented as
Q(zk,uk, eu
k ) = E
τ,α
{gT
k ξk},∀k (3.107)
The Q-function estimator (QFE) at the controller sampling instants can be represented as
Qˆ(z
s
k,us
k,eu
k ) = E
τ,α
{gˆ
T
k ξ s
k }, ki ≤ k < ki+1, (3.108)
where Qˆ(zs
k,us
k, eu
k ) is the estimate of Q-function in (3.107), ˆgk ∈ Rlg is the estimate of gk. The event￾based regression vector ξ s
k = ξki
, ki ≤ k < ki+1,i = 1,2,···; evaluated at the transmission instants.
The estimates of the Q-function parameter matrix Gˆ k can be reconstructed from ˆgk to compute the
estimated control gain matrix Kˆk given by
Kˆk = (Gˆ uu
k −Gˆ ue
k Gˆ ee−1
k Gˆ eu
k )
−1(Gˆ ue
k Gˆ ee−1
k Gˆ ez
k −Gˆ uz
k ) (3.109)120 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
and the estimated control input error gain matrix Lˆ k given by
Lˆ k = (Gˆ ee
k −Gˆ eu
k Gˆ uu−1
k Gˆ ud
k )
−1(Gˆ eu
k Gˆ uu−1
k Gˆ uz
k −Gˆ ez
k ). (3.110)
where Gˆ(·)
k is the estimated block matrix element of Gˆ k.
The event-based estimated control input can be computed as
us
k = Kˆkz
s
k, ki ≤ k < ki+1. (3.111)
Similarly, the estimated control input error eu
k is given by
eu
k = Lˆ kzk, ki ≤ k < ki+1. (3.112)
The Bellman equation for the stochastic discrete-time system (3.89) satisfies
V∗(zk) = E
τ,α
{r(zk,uk, eu
k )}+ E
τ,α
{V∗(zk+1)}. (3.113)
With a one-time step backward, the Bellman equation becomes
0 = E
τ,α
{r(zk−1,uk−1,eu
k−1) + E
τ,α
{V∗(zk)} − E
τ,α
{V∗(zk−1)}. (3.114)
Since the optimal Q-function E
τ,α
{Q∗(zk,u∗
k ,eu∗
k )} = E
τ,α
{V∗(zk)}, the Bellman equation (3.114)
in terms of the Q-function parameters can be expressed as
0 = E
τ,α
{r(zk−1,uk−1, eu
k−1)}+ E
τ,α
{Q∗(zk,u∗
k , eu∗
k )} − E
τ,α
{Q∗(zk−1,u∗
k−1,eu∗
k−1)}. (3.115)
Using the parametric form (3.107), the Bellman equation is represented as
0 = E
τ,α
{r(zk−1,uk−1, eu
k−1)}+ E
τ,α
{gT
k Δξk−1}, (3.116)
where Δξk−1 = ξk − ξk−1. With the estimated Q-function parameters, the resulting Bellman error
can be represented as
eB
k = E
τ,α
{r
s
(zk−1,uk−1,eu
k−1) + E
τ,α
{gˆ
T
k Δξ s
k−1,zki−1}, ki ≤ k < ki+1, (3.117)
where rs
(zk−1,uk−1,eu
k−1) = r(zki−1,uki−1, eu
ki−1) and Δξ s
k−1 = ξ s
ki
−ξ s
ki−1, ki ≤ k < ki+1, i = 1,2,···.
Remark 3.7. The computation of the Bellman error at the controller requires the state information
at time ki and ki − 1 for all i = 1,2,···. Therefore, the trigger mechanism transmits both the state
information zki and zki−1 at transmission instants ki,i = 1,2,···.
The objective of the Q-function parameter estimation is to drive the Bellman error (3.117) to
zero by designing an update law such that the optimal solution is reached. Before defining the
update law for the interval [ki, ki+1) the following notations are introduced. The time instants
k ∈ [ki,ki+1) are denoted by k = ki + j, j = 0,1,2,··· , ki+1 − ki − 1. The recursive least square
(Goodwin and Sin, 2014) update law for the estimated Q-function parameter for time instants
ki + j, j = 0,1,2,··· , ki+1 −ki −1, is
gˆki+j = E
τ,α
⎧
⎨
⎩
gˆki−1+j −αg
Wki−2+jΔξ s
ki−1e
s,BT
ki−1+j
1+Δξ sT
ki−1Wki−2+jΔξ s
ki−1
⎫
⎬
⎭, (3.118)Linear Discrete-time and Networked Control Systems 121
for j = 0,1,2,···, ki+1 − ki − 1, where the dynamics of Bellman error (3.117) for ki + j, j =
0,1,2,··· , ki+1 −ki −1 are represented as
e
s,B
ki−1+j = E
τ,α
{r(zki−1,uki−1, eu
ki−1) + E
τ,α
{gˆ
T
ki−1+jΔξ s
ki−1}, (3.119)
for j = 0,1,2,···, ki+1 −ki −1,and the dynamics of the gain matrix are given by
Wki+j =

Wki−1+j −αw
Wki−1+jΔξ s
ki−1Δξ sT
ki−1Wki−1+j
1+Δξ sT
ki−1Wki−1+jΔξ s
ki−1

1{Wki+j≥Wmin}, (3.120)
for j = 0,1,2,··· ,ki+1 − ki + 1, where Wki+j ∈ Rlg×lg is the positive definite time-varying gain
matrix with W0 = βwI where βw > 0 is a constant, Wmin minimum value of the gain matrix indicated
by the indicator function 1(·). The learning gains αg and αw are positive constants.
The rationale behind the parameter update law (3.118) with (3.120), defined for all time instants
in the interval [ki,ki+1) for all i = 1,2,···, is to update the parameters at the trigger instants k = ki
with the new event-based information and, further, utilize the time between the triggering-instants
ki < k < ki+1 to accelerate the parameter update using the information available at the previous
triggering instants. Thus, the parameter update scheme (3.118) with (3.119) can be considered as
a hybrid update scheme (Narayanan and Jagannathan, 2016b). This is different from the parameter
update law introduced in the previous sections for systems without a network. This update law is
referred to as the hybrid update law.
Remark 3.8. From (3.120), the gain matrix may converge to zero during the update process. There￾fore, the gain matrix Wki+j resets to W0 when Wki+j ≤ Wmin (Goodwin and Sin, 2014). In the re￾cursive least square approach, this is referred to as covariate resetting (Goodwin and Sin, 2014),
and the proof of stability holds when the gain is reset to its initial value.
Defining the Q-function parameter estimation error E
τ,α
{g˜k} = E
τ,α
{gk} − E
τ,α
{gˆk}, the Bellman
error by subtracting (3.116) from (3.117) can be expressed as
e
s,B
k = E
τ,α
{g˜
T
k Δξ s
k−1},k = ki. (3.121)
The dynamics of Bellman error for k = ki + j, j = 0,1,2,··· , ki+1 − ki − 1, from (3.121), can be
expressed as
e
s,B
ki−1+j = E
τ,α
{g˜
T
ki−1+jΔξ s
ki−1}. (3.122)
From (3.119), the Bellman error for time ki + j becomes
e
s,B
ki+j = E
τ,α
{r(zki−1,uki−1, eu
ki−1) + E
τ,α
{gˆ
T
ki+jΔξ s
ki−1}, (3.123)
for j = 0,1,2,··· ,ki+1 −ki −1. Substitute the update law (3.118) in (3.123) to get
e
s,B
ki+j = 
1−αgΓki−1

e
s,B
ki−1+j
. (3.124)
where Γki−1 = Δξ sT
ki−1Wki−2+jΔξ s
ki−1
1+Δξ sT
ki−1Wki−2+jΔξ s
ki−1
. Again, substituting e
s,B
ki−1+j from (3.122) in (3.124) leads
to
e
s,B
ki+j = 
1−αgΓki−1

E
τ,α
{g˜
T
ki−1+jΔξ s
ki−1}. (3.125)
Further, from (3.122), the dynamics of Bellman error e
s,B
ki+j = E
τ,α
{g˜
T
ki+j
Δξ s
ki−1}, j = 0,1,··· ,(ki+1 −
ki −1). Inserting in (3.125), reveals that
E
τ,α
{g˜
T
ki+jΔξ s
ki−1} = E
τ,α
{

1−αgΓki−1

g˜
T
ki−1+jΔξ s
ki−1}. (3.126)
The next lemma guarantees the convergence of the parameter estimation error.122 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Lemma 3.4. Consider the Q-function in (3.107) and the event-based estimation in (3.108). Let
Assumption 3.3 - 3.5 hold and the Q-function parameters gˆ0 is initialized with nonzero values in
a compact set Ωg ⊂ ℜlg×lg and updated using (3.118) and (3.120). Then, for constants 0 < αg <
2, Q-function parameter estimation error g˜k converges to zero asymptotically in the mean square
provided the regression vector satisfies the PE condition.
Sketch of the proof. Consider a candidate Lyapunov function given by L(g˜ki−1+j) =
E
τ,α
{g˜
T
ki−1+j
Δξ s
ki−1Δξ sT
ki−1g˜ki−1+j}. The first difference of the Lyapunov function
ΔL(g˜ki−1+j) = E
τ,α
{g˜
T
ki+jΔξ s
ki−1Δξ sT
ki−1g˜ki+j} − E
τ,α
{g˜
T
ki−1+jΔξ s
ki−1Δξ sT
ki−1g˜ki−1+j}. (3.127)
Substituting (3.126) in the first difference leads to
ΔL(g˜ki−1+j) = − E
τ,α
{

αgΓki−1(2−αgΓki−1)

g˜
T
ki−1+jΔξ s
ki−1Δξ sT
ki−1g˜ki−1+j}. (3.128)
By definition of Γki−1 in (3.124) and persistency of excitation condition (Green and Moore, 1985)
of the regression vector, it is clear that 0 < ξ 2
min ≤ Γki−1 < 1 where ξmin > 0 is a constant. Utilizing
the above fact, the first difference is upper bounded by
ΔL(g˜ki−1+j) ≤ − E
τ,α
{

αgξ 2
min(2−αg)

g˜
T
ki−1+jΔξ s
ki−1Δξ sT
ki−1g˜ki−1+j}. (3.129)
The Lyapunov first difference (3.129) ΔL(g˜ki−1+j) < 0 by selecting 0 < αg < 2. Therefore,
the Lyapunov function decreases in the interval ki ≤ k < ki+1, i = 0,1,···. Consequently,
lim
ki→∞ E
τ,α
{g˜
T
ki−1+j
Δξ s
ki−1Δξ sT
ki−1g˜ki−1+j} = 0. Since the regression vector Δξ s
ki−1 satisfies PE condition
(Green and Moore, 1985), the Q- function parameter estimation error ˜gk converges to zero asymp￾totically in the mean square, i.e, limki→∞ E
τ,α
{g˜
T
ki−1+j
g˜ki−1+j} = 0 or alternatively, limk→∞ E
τ,α
{g˜
T
k g˜k} = 0
since ki is a subsequence of k ∈ ℜ.
Next, the main results are presented by designing the event-triggering condition to decide the
transmission instants.
3.4.3 MAIN RESULTS AND STABILITY ANALYSIS
The closed-loop dynamics of the system (3.89) with estimated control input (3.111) both at trigger
instants and during inter-event times are given by
zk+1 = Aτ,α
k zk +Bτ,α
k Kzˆ s
k,k = ki, (3.130)
and
zk+1 = Aτ,α
k zk +Bτ,α
k Kzˆ k +Eτ,α
k Lzˆ k,ki < k < ki+1 (3.131)
Since Kˆk and Lˆ k are computed from the estimated Gˆ k, the following assumption is necessary to
ensure the estimated control gain matrix is well defined.
Assumption 3.6. The estimated Q-function submatrices Gˆ uu
k and Gˆ ee
k are full rank during the esti￾mation process.
This assumption can be satisfied by initializing the Q-function parameters with non-zero values in
a compact set and observed heuristically with numerical simulation. Before presenting the theorem,
the following technical lemma is necessary.Linear Discrete-time and Networked Control Systems 123
Lemma 3.5. Consider the augmented NCS (3.89) and estimated Q-function (3.108) along with the
estimated control input (3.111) and control input error (3.112). Let the Q-function parameter laws
be as given in (3.120) and (3.122). Then, the inequalities
WminE
τ,γ
{z
T
k K˜ T
k RzK˜kzk} ≤ ϖKλmax(Rz)αwE
τ,γ
{g˜k}2, (3.132)
and
WminE
τ,γ
{z
T
k L˜ T
k L˜ kzk} ≤ ϖLαwE
τ,γ
{g˜k}2 (3.133)
hold, where E
τ,α
{K˜k} = E
τ,α
{K∗
k −Kˆk}, E
τ,α
{L˜ k} = E
τ,α
{L∗
k −Lˆ k} are the errors in gain matrices, ϖK =
C2
g ¯
ξ 2
c > 0 and ϖL =C2
L ¯
ξ 2
c > 0 with Cg > 0, CL > 0 and ¯
ξc > 0 are constants, Wmin defined in (3.120),
and λmax(Rz) is the maximum eigenvalue of Rz.
Sketch of Proof: By definition E
τ,γ
{zk} ≤ E
τ,γ
{ξk}. Therefore, there exists a constant ¯
ξc > 0
such that E
τ,γ
{zk} ≤ ¯
ξcE
τ,γ
{Δξ s
ki−1} holds.
Further, defining G˜ k = Gk − Gˆ k and using Frobenius norm for matrices, it is clear that the
block matrices G˜(·)
k ≤G˜ k. Further, from (3.105), (3.106), (3.109) and (3.110), K˜k = K∗
k −Kˆk =
f(G˜ eu
k ,G˜ uu
k ,G˜ ud
k ,G˜ uz
k ,G˜ ez
k ). Since the inverse of the estimated block matrices Gˆ uu
k and Gˆ ee
k exists from
Assumption 3.6, it holds that
K˜k =  f(G˜ eu
k ,G˜ uu
k ,G˜ ud
k ,G˜ uz
k ,G˜ ez
k ) ≤ CgG˜ k = Cgg˜k
whereCg > 0 is a constant. With similar arguments the matrix L˜ k satisfies L˜ k ≤CLG˜ k =CLg˜k
where CL > 0 is a constant.
With the above results, consider the left side term in (3.132)
WminE
τ,γ
{z
T
k K˜ T
k RzK˜kzk} ≤ Wminλmax(Rz)E
τ,γ
{zk}2E
τ,γ
{K˜k}2. (3.134)
From update law for the gain matrix Wki+j in (3.120) Wmin ≤ Wki+j. By matrix inverse lemma
Goodwin and Sin (2014), it holds that
Wki+j =αw
Wki−1+j
1+Δξ sT
ki−1Wki−1+jΔξ s
ki−1
. (3.135)
By generalizing the equation (3.135) for time instants k and j = 1, it holds that Wmin ≤
αw
Wk
1+Δξ sT
ki−1WkΔξ s
ki−1
. Substituting the inequality in (3.134) leads to
WminE
τ,γ
{z
T
k K˜ T
k RzK˜kzk} ≤ C2
g ¯
ξ 2
c λmax(Rz)αwE
τ,γ
{g˜k}2, (3.136)
where
WkE
τ,γ
{Δξ s
ki−1}2
1+Δξ sT
ki−1WkΔξ s
ki−1
 < 1. Replacing ϖK = C2
g ¯
ξ 2
c one can reach the inequality in (3.132).
Further, with the similar argument, it can also be shown that the inequality in (3.133) also holds.
Remark 3.9. The constants ϖK > 0 and ϖL > 0 in inequality (3.132) and (3.133) are not necessary
for the implementation of the controller and only used for stability proof.
Theorem 3.3. Consider the continuous time system (3.80) represented as a sampled data system
(3.86) and an augmented NCS (3.89) along with the estimated control input (3.111) and control124 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
input error (3.112). Let the Assumptions 1 to 4 hold, the Q-function parameters are initialized with
non-zero values in a compact set Ωg ⊂ Rlg×lg , and the parameters are updated using (3.118) and
(3.120). Then, the closed-loop event-triggered NCS is asymptotically stable in the mean square
provided the regression vector satisfies PE, the inequality
euT
k eu
k ≤ E
τ,α
{z
T
k Lˆ T
k Lˆ kzk}, ki ≤ k < ki+1 (3.137)
holds, and the parameters satisfy 0 < αg < 2, 0 < c1 < 1 and √
2γz > γ∗
z . Further, the estimated
control input us
k converges to it optimal value us∗
k and the set of transmission instants determined by
the equality condition in (3.137), i.e., {ki|i ∈ N,euT
k eu
k = zT
k Lˆ T
k Lˆ kzk converges to the optimal sampling
set {ki|i ∈ N, euT
k eu
k = E
τ,α
{zT
k L∗T
k L∗
k zk}}.
Sketch of the Proof. The closed-loop stability of the event-triggered NCS is demonstrated by
proving the Lyapunov function L ∈ R+ decreases for ki ≤ k < ki+1 for i = 1,2,···. Since the in￾terval [ki,ki+1)=[ki, ki +1)∪[ki +1, ki+1) it suffices to demonstrate that the first difference of the
Lyapunov function ΔLk < 0, k = ki and ΔLk < 0,ki < k < ki+1.
With this effect, we can select a Lyapunov function candidate given by
L(zk,g˜k) = E
τ,α
{g˜
T
k Δξ s
ki−1Δξ sT
ki−1g˜k}+νxWmin E
τ,γ
{V∗(zk)} = L(zk) +L(g˜k). (3.138)
Note that the first term in the Lyapunov function is the Q-function parameter estimation error term
in Lemma 3.5, and the second term is the optimal value function. Since the Optimal value function is
positive definite, we can use it as the Lyapunov function. By using the results from Lemma 3.5 and
evaluating the second both at the triggering instant k = ki and inter-triggering times ki < k < ki+1
(in two cases) along the NCS dynamics, update law, and event-triggering condition, we can show
that ΔLk < 0 for both the cases.
Example 3.3. The benchmark example of batch reactor Xu et al. (2014b) is selected for simulation
whose continuous-time dynamics are represented by x˙ = Ax + Bu, where A and B matrices are
defined in Xu et al. (2014b).
Solution: Select the following parameters for the simulation. The initial state vector of the NCS was
taken as [2 −2 6 −10]
T . The delay bound d = 2 with a mean of 0.1s, and the packet loss follows
the Bernoulli distribution. The sensor sampling time is selected to be Ts = 0.1 sec. A quadratic
cost function is selected with Hz = 0.2I and Rz = 0.2I, where I is the identity matrix. The penalty
for the control input error γz = 5. The learning gains were αg = 0.5 and αw = 1. Initial value of
the gain matrix W0 = 9999I. To ensure the initial value of the Q-function parameters are non-zero
and in a compact set, the values are selected as 60% of the actual values. An exploration noise of
small magnitude was added to the regression vector to satisfy the PE condition, and a Monte Carlo
simulation was run for 5 sec or sampling instants.
First, the performance of the event-based optimal regulator is studied. The state vector is reg￾ulated to zero by the proposed event-based optimal regulator, as shown in Fig. 3.9(a). It is clear
that this event-based regulator is able to handle random delays and packet losses in the presence of
uncertain system dynamics. The optimal control input, shown in Fig. 3.9(b), also converges to zero
as the system state converges. The convergence Bellman error to zero shown in Fig. 3.9(c) implies
the optimal control is attained.
Second, the event-based regulator performance to save the bandwidth and computation is shown
in Fig. 3.10. The events or the transmission instants are illustrated in 3.10 (a). The cumulative
number of events that occurred during the simulation time is shown in Fig. 3.10 (b). The horizontal
axis indicates the total number of sampling instants. This shows that a fewer number of times the
data is being transmitted when compared to the traditional periodic transmission.Linear Discrete-time and Networked Control Systems 125
Table 3.3
Co-optimization of sampling and control for NCS
System dynamics ˙x(t) = Ax(t) +α(t)Bu(t −τ(t)), x(0) = x0,
Event-triggering error eu
k = us
k −uk = μ(xk)− μ(xc
k)
Q-function estimator (QFE) Qˆ(zs
k,us
k, eu
k ) = E
τ,α
{gˆ
T
k ξ s
k }, ki ≤ k < ki+1
QFE residual error e
s,B
k = E
τ,α
{g˜
T
k Δξ s
k−1}, k = ki
Auxilary QFE error Ξo,V
k = Πo
k +Θˆ T
k Zo
k , k = ki
Q-function estimator update ˆgki+j = E
τ,α
⎧
⎨
⎩
gˆki−1+j −αg
Wki−2+jΔξ s
ki−1e
s,BT
ki−1+j
1+Δξ sT
ki−1Wki−2+jΔξ s
ki−1
⎫
⎬
⎭
Wki+j =

Wki−1+j −αw
Wki−1+jΔξ s
ki−1Δξ sT
ki−1Wki−1+j
1+Δξ sT
ki−1Wki−1+jΔξ s
ki−1

1{Wki+j≥Wmin}
j = 0,1,2,···, ki+1 −ki −1
Control input us
k = Kˆkzs
k, ki ≤ k < ki+1
Event-triggering condition euT
k eu
k ≤ E
τ,α
{zT
k Lˆ T
k Lˆ kzk}, ki ≤ k < ki+1
3.5 OPTIMAL TRACKING CONTROL
Consider a linear continuous-time system given in (3.80). The control objective is to track a feasible
reference trajectory xd(t) ∈ Rn generated by a reference system represented by
x˙d(t) = Adxd(t), xd(0) = xd0 (3.139)
where xd(t) ∈ Rn is the reference state, and Ad ∈ Rn×n is the internal dynamics with xd(0) = 0.
Following standard characteristics of the systems in (3.80) and (3.139) are assumed.
Assumption 3.7. System (3.80) is stabilizable, and the system states are available for measurement.
Assumption 3.8. The feasible reference trajectory xd(t) ∈ Ωxd , where Ωxd is a compact set, is
bounded such that xd(t) ≤ bxd where bxd > 0 is a constant. The control coefficient matrix B has
full column rank.
Define the error between the system state and the reference state as the tracking error, given by
er(t)  x(t)−xd(t). Then, the tracking error system, utilizing (3.80) and (3.139), can be defined by
e˙r(t) = x˙(t)−x˙d(t) = A(er(t) +xd(t)) +α(t)Bu(t −τ)−Adxd(t). (3.140)
Following the derivations in Section 2.6, the steady-state feed-forward control policy for the refer￾ence trajectory can be expressed as
ud = B+(Adxd −Axd) (3.141)126 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Figure 3.9 Convergence of (a) closed-loop event-triggered state vector; (b) event-based optimal control
input; and (c) Bellman error.
Figure 3.10 (a) Transmission instants and inter-event times; and (b) cumulative number of triggering instants
where ud : Rn → Rm is the steady state control policy corresponding to the reference trajectory and
B+ is the (left) pseudoinverse of B. Note that here we use the assumption that B has full column rank,
and therefore, the pseudoinverse we need, exists (Strang, 2022). By augmenting the tracking error er
and desired trajectory xd, the dynamics of the augmented tracking error system can be represented
as
χ˙(t) = Gχ(t) +α(t)Hw(t −τ) (3.142)
where χ  [eT
r xT
d ]
T ∈ R2n is the augmented state with χ(0)=[eT
r (0) xT
d (0)]T = χ0, G ∈ R2n is
given by G 

A A−Ad
0 Ad

, H ∈ R2n×m given by H 

B
0

, and the mismatched control policy
w(t −τ)  u(t −τ)−ud(t) ∈ Rm.
The augmented error system in (3.142) transforms the tracking problem into a regulation prob￾lem. We can then follow the discretization procedure and the control synthesis technique developed
in Section 3.4.
3.6 CONCLUDING REMARKS
This chapter presents three event-based infinite-time horizon near-optimal control techniques for
uncertain linear discrete-time systems with and without communication networks in the feedbackLinear Discrete-time and Networked Control Systems 127
loop. Both the state and the output feedback-based near-optimal controllers for the system without
a communication network are designed using the system state and output vectors without needing
the system dynamics. The event-triggering conditions are designed as a function of the estimated
Q-function parameters and the measured or observer system state vector, making it adaptive in na￾ture. In both cases, these triggering conditions ensured sufficient events to estimate the Q-function
parameters. The aperiodic tuning laws guaranteed the convergence of the Q-function parameter es￾timation errors; the Lyapunov technique was used to prove them. For NCS, the delays and packet
losses are integrated to reformulate the system, and Q-learning-based design is extended to the
stochastic domain. The simulation results using benchmark examples also corroborated the ana￾lytical results by revealing the convergence of the closed-loop parameters and the reduction in the
computation. It was observed that the cumulative number of triggers was dependent on the initial
Q-function parameters.
3.7 PROBLEMS
3.7.1 The longitudinal dynamics of an F-16 aircraft in straight and level flight at 502 ft/sec are given
by
x˙ = Ax+Bu =
−2.0244×10−2 7.8761 −3.2169×101 −6.502×10−1
−2.5373×10−4 −1.0189 0 9.0484×10−1
0 00 1
7.9472×10−11 −2.498 0 −1.3861

x (3.143)
+
 −1×10−2
−2.09×10−3
0
−1.99×10−1

u (3.144)
The state is x =  vT α θ q T
, with vT the forward velocity, α the angle of attack, θ
the pitch angle, and q the pitch rate. The control input u(t) is the elevator deflection δe.
(a) Plot the response of the open-loop system to initial conditions of x0 =  0 0.101 T
(note that the angular units are in radians).
(b) Try MATLAB function lqr to perform linear quadratic regulator design. Use Q = I,R = 0.1.
(c) Use the Q-learning algorithm (Table 3.1) with τ = 0 and γca = 1 to learn the control inputs
for Q = I,R = 0.1.
3.7.2 Consider a of linear dynamical system given by
d
dt x(t) = Ax(t) +Bu(t), (3.145)
where A =  0 −1
1 0 
and B =  1 0
0 1 
.
(a) Use the Q-learning algorithm (Table 3.1) with τ = 0 and γca = 1 to learn the control inputs
for Q = I,R = 0.1.
(b) Now consider the channel losses. The new dynamics will take the form d
dt x(t) = Ax(t) +
γcaBu(t −τ), τ = 0.1 and γca is a random variable characterized by Bernoulli distribution
with the probability of data lost is 10%. Use the Q-learning algorithm (3.3) to learn the
control inputs for Q = I,R = 0.1I.
(c) What happens when τ is a random variable following a Gaussian distribution with mean
10ms and 1ms standard deviation?
3.7.3 Consider the system of an inverted pendulum. The dynamics are
x˙(t) =  0 1
g
l − aik
ml2 0

xi(t) +γca  0
1
ml2

ui(t −τ),
where l = 5, g = 10, m = 5, k = 5 and hi j = 1 for ∀ j ∈ {1,2,..,N}. The system is open loop
unstable.128 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
(a) Use the Q-learning algorithm (Table 3.3) with τ being a random variable following a Gaus￾sian distribution with mean 100ms and 20ms standard deviation and γ is a random variable
characterized by Bernoulli distribution with the probability of data lost is 10% to learn the
control inputs for Q = 10×I,R = 0.001.
(b) What if the cost function is defined with R = 10 and Q = 0.1×I.
3.7.4 Use the system defined in the Problems 3.7.1-3. Consider the first state as the only measured
quantity (output) of the system. Use the Q-learning algorithm (Table 3.2) to design feedback
control inputs to steer the system to the equilibrum point at origin.
3.7.5 Use the system defined in the previous problem and do not use the data history in the state￾feedback-based Q-learning algorithm described in this chapter. Does the Q-function param￾eters converge? Try using the gradient descent algorithm in place of RLS-based parameter
updates. Does the Q-function parameters converge?
3.7.6 Consider a linear discrete-time system given by
xk+1 =
 −1 2
2.2 1.7

xk +
 2
1.6

uk
yk =  1 2 
xk
(3.146)
The open-loop poles are z1 = −2.1445 and z2 = 2.8445, so the system is unstable. Use the
Q-learning algorithm in Table 3.1, with Q = 6 and R = 1, to design a feedback control to
optimally stabilize the system.
3.7.7 For the system described in the previous problem, consider the trajectory tracking control
task. Design a Q-learning-based trajectory tracking controller to track the reference trajectory
generated by the command generator dynamics given by rk+1 = −rk, with r(0) = −1.
3.7.8 For the system used in the previous two examples, use the output matrix to C = [0 1] and
implement the learning algorithm in Table 3.1. What is the minimum value for the weighting
parameter Q for R = 1 for which the learning algorithm converges.
3.7.9 For the Example 3.1, select the parameters of the performance index, learning gains, and
initial values of the state and parameters. Use the equations in Table 3.1 to design feedback
controller. Plot all the signals, including event-trigger condition, state, and parameter estima￾tion errors. Comment on the convergence of the system states and QFE error convergence
with PE signal selected as
a. Sinusoid signal.
b. Pulse train with 50% duty cycle.
3.7.10 For the Example 3.1, select the parameters of the performance index, learning gains, and
initial values of the state and parameters. Use the equations in Table 3.2 to design feedback
controller. Start with τ = 0 and use the Bernoulli distribution to characterize γca with a prob￾ability of packet loss set at 10% and increase the τ by 0.05. Plot the parameter estimation
error convergence time versus τ. Repeat the experiments with a τ fixed at 0.1 and vary the
probability of packet loss with 5% increments. Comment on the convergence of the system
states and QFE error convergence with respect to channel losses.4 Nonlinear
Continuous-time Systems
CONTENTS
4.1 Introduction ........................................................................................................................... 129
4.2 Control of Uncertain Nonlinear System Using Event-based Approximation ....................... 131
4.2.1 Function Approximation........................................................................................... 132
4.2.2 Adaptive Event-triggered State Feedback Control ................................................... 133
4.2.3 Lower Bound on Inter-event Times .......................................................................... 137
4.3 Optimal Control using Event-based Neuro-dynamic Programming ..................................... 144
4.3.1 Background and Problem Statement ........................................................................ 145
4.3.2 Approximate Optimal Controller Design ................................................................. 146
4.3.3 Identifier Design ....................................................................................................... 147
4.3.4 Controller Design...................................................................................................... 148
4.3.5 Stability of the Closed-loop System ......................................................................... 150
4.3.6 Minimum Intersample Time ..................................................................................... 152
4.4 Optimal Event-based Tracking Control................................................................................. 155
4.5 Concluding Remarks ............................................................................................................. 157
4.6 Problems................................................................................................................................ 158
In the previous chapter, we studied Q-learning-based optimal control synthesis techniques suitable
for uncertain linear discrete-time systems using state and output feedback when the communication
network in the feedback loop is lossless as well as when it is lossy. On the other hand, in the first part
of this chapter, based on the work in (Sahoo et al., 2016), we shall consider multi-input-multi-output
(MIMO) uncertain nonlinear continuous-time (CT) systems in affine form. In particular, we shall de￾velop an approximation-based event-triggered controller suitable for MIMO nonlinear CT systems
whose dynamics are represented in affine form. The controller utilizes a linearly parameterized neu￾ral network (NN) whose weights are tuned using data that are sampled based on aperiodic events. In
this context, we shall revisit the NN approximation property with event-based sampling, develop an
event-triggering condition by using the Lyapunov technique to reduce the network resource utiliza￾tion, and generate the required number of events for the NN approximation. In the second part of
this chapter, based on the work in (Sahoo et al., 2017b), we shall develop approximate optimal con￾trollers for the uncertain nonlinear CT systems using adaptive dynamic programming (ADP) with
event-sampled state and input vectors. In this case, we shall incorporate NNs to not only mitigate
the need for an accurate model of the system dynamics but also learn the optimal value function,
which becomes an approximate solution to the Hamilton-Jacobi-Bellman (HJB) equation associated
with the optimal control problem. For both the non-optimal and optimal controllers presented in this
chapter, we shall also develop weight tuning rules to train the NNs online with aperiodic feedback
data, design event-triggering conditions, derive sufficient conditions for closed-loop stability, and
develop arguments to compute a positive lower bound on the minimum inter-sample time.
129130 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
4.1 INTRODUCTION
Among the earlier works on event-triggered control, Tabuada (2007) presented an event-triggered
control for nonlinear systems by assuming the input-to-state stability (ISS) of the system with re￾spect to measurement error. Further, an event-triggering condition is developed for deciding the
triggering instants to execute the controller with a desired closed-loop performance. A lower bound
on the inter-triggering times is also guaranteed to avoid the accumulation point. In these early works
(Tabuada, 2007; Donkers and Heemels, 2012; Postoyan et al., 2011; Heemels and Donkers, 2013;
Lunze and Lehmann, 2010; Stocker and Lunze, 2011), the complete knowledge system dynam- ¨
ics have been considered for the ETC design both for linear and nonlinear systems with a few
exceptions (Garcia and Antsaklis, 2013; Wang and Hovakimyan, 2010). In (Garcia and Antsaklis,
2013), the authors considered known uncertainty for the system and developed a model-based event￾triggered control scheme. Further, in (Wang and Hovakimyan, 2010), an L1 adaptive control scheme
is proposed where the nominal system dynamics are considered known, and uncertainties are com￾pensated for by using an adaptive term tuned with a projection-based tuning law. On the other hand,
in (Sahoo et al., 2013a), the complete knowledge of the system dynamics was relaxed by approxi￾mating the dynamics using a neural network (NN) based approximations for SISO systems, while a
zero-order hold (ZOH) was used for the controller implementation.
On the other hand, as discussed in Chapter 2, optimal control (Lewis et al., 2012b; Bertsekas,
2012) of nonlinear dynamic systems in continuous time is a challenging problem due to the difficulty
involved in obtaining a closed-form solution to the Hamilton-Jacobi-Bellman (HJB) (Bertsekas,
2012) equation. Adaptive dynamic programming (ADP) (Lewis et al., 2012a; Bertsekas, 2012; Chen
and Jagannathan, 2008; Si et al., 2004; Vrabie et al., 2009b; Jiang and Jiang, 2015; Prokhorov and
Wunsch, 1997; Liu and Wei, 2013; Zhang et al., 2009; Dierks and Jagannathan, 2010b; Xu et al.,
2012; Dierks and Jagannathan, 2012b; Zhang et al., 2008a) techniques are used to solve the optimal
control online of uncertain dynamic systems by finding an approximated value function, which
becomes a solution to the HJB equation.
The first section of the chapter, taken from (Sahoo et al., 2016), introduces the development of
event-triggered control (ETC) of MIMO nonlinear continuous-time systems in affine form when
the system and the controller are separated by an ideal communication network with no delays and
packet losses. Instead of approximating the unknown nonlinear functions of the system dynamics by
using two NNs (Sahoo et al., 2013a), the controller is approximated by using a linearly parameter￾ized NN in the event-triggered context under the assumption that the system states are measurable.
An event-triggering condition based on the system state and estimated NN weight is designed to
orchestrate the transmission of the state vector and control input between the plant and controller.
Since the approximation is carried out using the event-based state vector, the event-triggering con￾dition is made adaptive to attain a tradeoff between resource utilization and function approxima￾tion. In addition, the NN weights and the control inputs are only updated at the triggering instants,
which are aperiodic in nature and held until the next update. Consequently, the proposed scheme
reduces the overall computation when compared to the traditional NN schemes (Lewis et al., 1998),
where weights are updated periodically. In addition, to analyze the system stability and design the
event-triggering condition, the nonlinear impulsive dynamical model of the closed-loop dynamics is
considered. The well-developed Lyapunov approach for the nonlinear impulsive dynamical system
(Michel, 2008; Hayakawa and Haddad, 2005) is utilized to study inter-event and event time behav￾ior and used to prove the local ultimate boundedness (UB) of the system state and the NN weight
estimation errors.
In the second part of the paper, the event-triggered stabilizing controller design in the first part
is extended to event-based optimal control design and taken from (Sahoo et al., 2016) using event￾sampled actor-critic NN. Here, the NN weights are updated at the event-sampled instants and, hence,
aperiodic in nature to save computation. The event-triggering conditions are also made adaptive to
orchestrate the sampling instants, such that the approximation accuracy is maintained along withNonlinear Continuous-time Systems 131
the system performance. The closed-loop stability and convergence are guaranteed by using the
extension of the Lyapunov approach for switched systems.
4.2 CONTROL OF UNCERTAIN NONLINEAR SYSTEM USING EVENT-BASED
APPROXIMATION
Consider the multi-input-multi-output (MIMO) nonlinear uncertain continuous-time system repre￾sented in the affine form as
x˙ = f(x) +g(x)u, x(0) = x0, (4.1)
where x = [x1 x2 ··· xn]
T
∈ Rnx and u ∈ Rmu are the state and input vectors of the system (4.1),
respectively. The nonlinear vector function, f(x) ∈ Rnx , and the matrix function, g(x) ∈ Rnx×mu ,
represent the internal dynamics and control coefficient function, respectively. We assume the system
satisfies the properties as stated below.
Assumption 4.1. The system (4.1) is controllable and input-to-state linearizable. The internal dy￾namics, f(x) and control coefficient g(x) are considered unknown with the control coefficient matrix,
g(x), bounded above in a compact set for all x ∈ Ωx ⊂ Rnx , satisfying g(x) ≤ gmax with gmax > 0
being a known positive constant (Lewis et al., 1998).
The input-to-state linearizable assumption is satisfied by a wide variety of practical systems such
as a robot manipulator, mass damper system, and many others. For these classes of controllable
nonlinear systems in affine form (4.1) with complete knowledge of system dynamics, f(x) and
g(x), there exists an ideal feedback linearizable controller ud, given by
ud = K(x), (4.2)
which renders the closed-loop system asymptotically stable, where K(x) is a function of the system
state vector. The linear closed-loop dynamics can be represented by
x˙ = Ax, (4.3)
where A is a Hurwitz matrix and can be designed as per the closed-loop performance requirement.
By converse Lyapunov theorem (Drazin and Drazin, 1992), an asymptotically stable system admits
a Lyapunov function, V(x) : Ωx → R, which satisfies the following inequalities
α1 (x) ≤ V (x) ≤ α2 (x), (4.4)
V˙ (x) ≤ −α3 (x), (4.5)
where α1, α2 and α3 are class K functions.
Moreover, considering a standard quadratic Lyapunov function, V(x) = xTPx, for the closed￾loop system (4.3), the class K functions are expressed as α1 (x) = λmin (P)x2
, α2 (x) =
λmax (P)x2 and α3 (x) = λmin (Q)x2
. The matrices P ∈ Rnx×nx and Q ∈ Rnx×nx are symmetric,
positive definite, and satisfy the Lyapunov equation given by
ATP+PA = −Q. (4.6)
In the case of a traditional NCS, the state vector, x, and the control input, u, are transmitted with a
fixed sampling interval Ts. On the other hand, in an event-triggering context, the system state vector
is sampled and transmitted at the triggering instants only.
Let {tk}, for k = 0,2,··· be a monotonically increasing sequence of time instants with t0 = 0 such
that tk+1 > tk and tk → ∞ as k → ∞ represent the event-triggering instants. The system states, x(tk),
and control inputs, u(tk), are transmitted at these time instants, rather continuously. The event-based132 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
transmitted state and the control input vectors are held, respectively, at the controller and plant by
the zero-order-holds (ZOH). It is important to note that tk is a function of current system state x and
the last transmitted system state, 
x = x(tk), tk < t ≤ tk+1, and is aperiodic in nature.
Define the event-triggering error, es ∈ Rnx , as
es = x− 
x,tk < t ≤ tk+1. (4.7)
The triggering instants, tk, k = 1,2,3,···, are determined by an event-triggering condition con￾sisting of the event-triggering error (4.7) and a state-dependent threshold. Once the event-triggering
error exceeds the threshold (time instant, t = tk), the sensor and triggering mechanism initiates the
transmission of the current state vector x. The last held event-based state vector 
x updated to the
new value, i.e., 
x
+ = x, for t = tk and held for tk < t ≤ tk+1 where 
x
+ = 
x

t
+
k

and t
+
k is the time
instant just after tk. The event-triggering error is then reset to zero for the next event to occur, i.e.,
e+
s = 0, t = tk. (4.8)
Since the system dynamics f(x) and g(x) are considered unknown, the implementation of the
controller (4.2) is not possible. Further, in the event-based sampling and transmission context, the
intermittent availability of the system state vector at the controller precludes the traditional neural
network (NN)-based approximation with a periodic update of the NN weights. Therefore, the NN
function approximation property is revisited under the event-based sampling and transmission next.
4.2.1 FUNCTION APPROXIMATION
Figure 4.1 NN structure with an event-based activation function.
As discussed in Chapter 2 Section 2.1.3.1, by the universal approximation property of NN, any
continuous function f(x) can be approximated over a compact set for all x ∈ Ωx ⊂ Rnx up to a
desired level of accuracy ε f by the selection of suitable activation functions and an adequate number
of hidden layer neurons. Alternatively, there exists an unknown target weight matrix W such that
f(x) in a compact set can be written as
f (x) = WTϕ(VT x) +ε f(x), (4.9)
where W ∈ Rl×b and V ∈ Ra×l represent the target NN weight matrix for the output and input layers,
respectively, and defined as
(W,V) = arg min (W, V)
[sup
x∈Ωx

WTϕ(VT x)− f(x)

]. (4.10)Nonlinear Continuous-time Systems 133
where the activation function is denoted by ϕ() : Ra → Rl and ε f(x) ∈ Rnx denotes the traditional
reconstruction error. The constants l, a, and b are the number of neurons in the hidden layer, num￾ber of input and output of the NN, respectively.
We will consider the linearly parameterized (Lewis et al., 1998) NNs, as shown in Figure 4.1,
for approximating the unknown function as in (4.9) where the output layer weights W ∈ Rl×b are
updated while the input layer weight matrix V ∈ Ra×l is initialized at random and held. This linearly
parameterized NN is also known as random vector functional link networks (RVFL) (Lewis et al.,
1998). The activation function ϕ() : Ra → Rl is a hyperbolic tangent activation function and given
by ϕ() = e2 ¯x −1/e2 ¯x +1 with ¯x = VT x. The activation function ϕ(VT x) forms a basis for the
unknown function, and the universal approximation property is retained. The output layer activation
functions, ϕo(), are selected to be purely linear.
Recall the universal approximation property of the NN under intermittent event-based transmis￾sion of the system states, as discussed in Chapter 2 Section 2.1.3.4. The approximation error is a
function of the event-triggering error. Therefore, it must be handled effectively to ensure that the
NNs approximate the desired functions at the controller using aperiodic feedback data. Next, the
adaptive event triggered control (ETC) scheme development is introduced.
4.2.2 ADAPTIVE EVENT-TRIGGERED STATE FEEDBACK CONTROL
This section proposes a state-feedback design of the NN-based adaptive ETC. The structure of the
proposed adaptive ETC scheme with a communication network between the plant and the controller
is depicted in Figure 4.2. Further, for simplicity, the following assumption regarding the network is
considered.
  
 

 


	
 






Figure 4.2 Structure of the adaptive state feedback ETC system.
Assumption 4.2. The communication network between the plant and the controller is ideal
(Donkers and Heemels, 2012), i.e., the networked induced delays, including the computational delay
and the packet losses, are not present.
In the proposed scheme, a smart sensor and triggering mechanism are included at the plant to
decide the event-triggering instants by evaluating the event-triggering condition continuously. At
the violation of the event-triggering condition, the state vector is transmitted first, and then the
controller is updated and transmitted to the plant. The ZOHs are used to hold the last transmitted
state and control input, respectively, at the controller and the plant until the next transmission is
received.
Since the system dynamics are considered unknown, the control input is approximated by using
a NN in an event-sampling context. Further, the NN weights are updated in an aperiodic manner
at every triggering instant only and held during the inter-event durations. In order to achieve the
desired approximation accuracy, an adaptive event-triggering condition is designed to generate the
required number of events during the learning phase. Thus, the event-triggering condition becomes134 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
a function of the NN weight estimates and the system state vector, whereas, in the traditional ETC
design, it is a function of system state only (Tabuada, 2007; Donkers and Heemels, 2012) with a
constant threshold. Therefore, to evaluate the event-triggering condition locally, the NN weights are
updated at both the triggering mechanism and controller in synchronism without transmitting the
estimated NN weights. This increases the computation, but the overall computation reduces due to
the event-based aperiodic update at both places.
By the universal approximation property of the NNs, the ideal control input (4.2) is written as
ud = WT
u ϕu (x¯) +εu (x), (4.11)
where Wu ∈ Rlu×mu is the output layer unknown ideal NN weight matrix, and ϕu(x¯) ∈ Rlu is the
tangent hyperbolic activation function with ¯x =VT
u x. The function εu(x) ∈ Rmu is the traditional NN
reconstruction error, Vu ∈ Rnx×lu is the input layer weight matrix and l = lu, a = nx, and b = mu are
the number of neurons in the hidden layer, number of inputs and outputs of the NN, respectively.
Before presenting the approximation-based controller design, the following standard assump￾tions are introduced for the NN.
Assumption 4.3. The target weights, Wu, the activation function, ϕu(), and the reconstruction error
εu() of the NN are upper bounded in compact set such that Wu ≤ Wu,max, ϕu() ≤ ϕu,max and
εu() ≤ εu,max where Wu,max, ϕu,max, and εu,max are positive constants.
Assumption 4.4. The NN activation function, ϕu(x¯), is considered Lipschitz continuous in a com￾pact set for all x ∈ Ωx ⊂ Rnx . Alternatively, for every x ∈ Ωx ⊂ Rnx , there exists a constant Lϕu > 0
such that


ϕu (x¯)−ϕu
"¯
x
#

 ≤ Lϕu


x− 
x


 is satisfied.
In the event-triggered control context, the actual controller uses the event-based state vector 
x
held at the ZOH. Hence, by Theorem 1, the actual event-based control input is represented as
u = Wˆ T
u ϕu(
¯
x), tk < t ≤ tk+1, (4.12)
where Wˆ u ∈ Rlu×mu is the estimated NN weight matrix, ϕu
"¯
x
#
∈ Rlu is the event-based NN activa￾tion function where ¯
x = VT
u

x is the scaled input to the NN. Since the last held state, 
x and the NN
weights are updated at the event-triggering instants, t = tk, the control input is also updated at the
triggering instants, and, then, transmitted to the plant and held by the ZOH until the next update is
received.
Further, as proposed, the estimated NN weights, Wˆ u ∈ Rlu×mu , are held during inter-event dura￾tions tk < t ≤ tk+1 and updated at the triggering instants or referred to as jumps at t = tk. Therefore,
the NN update law during inter-event durations is defined as
˙
Wˆ u = 0, tk < t ≤ tk+1. (4.13)
Further, at the event-triggering instants, the update law is selected as
Wˆ +
u = Wˆ u − αu
c+es2 ϕu (x¯) eT
s L−κWˆ u,t = tk, (4.14)
where Wˆ +
u ∈ Rlu×mu is the updated NN weight estimate just after the trigger instant with αu > 0
being the NN learning rate, c > 0 is a positive constant, L ∈ Rnx×mu is a design matrix to match the
dimension, and κ > 0 is a positive constant serving the same role as the sigma-modification (Ioannou
and Fidan, 2006) in the traditional adaptive control. Note that the update law (4.14) uses traditional
activation ϕu (x¯) since the system state vector, x, is available for the update at the triggering instants.Nonlinear Continuous-time Systems 135
Next, define the NN weight estimation error as W˜ u = Wu − Wˆ u. The weight estimation error
dynamics during the flow, by using (4.13), can be written as
˙
W˜ u = W˙ u − ˙
Wˆ u = 0, tk < t ≤ tk+1, (4.15)
while for the jump instant, t = tk, the NN weight estimation error dynamics derived from (4.14)
becomes
W˜ +
u = Wu −Wˆ +
u = W˜ u +αuχsϕu (x¯) eT
s L+κWˆ u, t = tk, (4.16)
with χs = 1/
"
c+es2
#
.
As per the proposed scheme, the last transmitted state and the NN weights are updated at the
triggering instants only. Hence, the closed-loop event-triggered system behaves as an impulsive
dynamical system. Assuming the event instants are distinct, i.e., there exists a non-zero lower bound
on the inter-event times, δtk = tk+1 −tk > 0, which is proven later in Section 4.2.3, the closed loop
dynamics can be formulated in two steps.
The first step towards impulsive system modeling is to formulate the flow dynamics. The closed￾loop system dynamics during the flow interval for t ∈ (tk,tk+1] can be derived by using both (4.1)
and (4.12), and represented as
x˙ = f(x) +g(x)Wˆ T
u ϕu(
¯
x), t ∈ (tk,tk+1]. (4.17)
Adding and subtracting the ideal control input ud yields
x˙ = f(x) +g(x)
"
Wˆ T
u ϕu(
¯
x) +ud −ud
#
, t ∈ (tk,tk+1]. (4.18)
Recalling the NN approximation of the ideal controller (4.12) and the ideal closed-loop dynamics
(4.3), (4.18) becomes
x˙ = Ax+g(x)
"
Wˆ T
u ϕu
"¯
x
#
−WT
u ϕu (x¯)−εu(x)

, t ∈ (tk,tk+1]. (4.19)
From the definition, Wu = Wˆ u +W˜ u, the closed loop dynamics (4.19) can be written as
x˙ = Ax−g(x)

W˜ T
u ϕu (x¯) +εu(x)

+g(x)
"
Wˆ T
u ϕu
"¯
x
#
−Wˆ T
u ϕu (x¯)), t ∈ (tk,tk+1]. (4.20)
Similarly, the dynamics of the last transmitted state vector, 
x, held by the ZOH, during the flow
interval becomes ˙
x = 0, t ∈ (tk, tk+1]. (4.21)
Further, the flow dynamics of the NN weight estimation error is given by (4.15).
In the second and final step, it only remains to formulate the reset dynamics to complete the
impulsive modeling of the event-triggered system. This consists of the jumps in the system state,
i.e.,
x+ = x, t = tk, (4.22)
the last transmitted state held by the ZOH,

x
+ = x, t = tk, (4.23)
and NN weight estimation error dynamics (4.16).
From (4.22), (4.23) and (4.16), the reset dynamics for the system are given by
Δx = x+ −x = 0, t = tk, (4.24)136 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Δ

x = 
x
+ − 
x = x− 
x = es, t = tk, (4.25)
and
ΔW˜ u = W˜ +
u −W˜ u = αuχsϕu (x¯) eT
s L+κWˆ u , t = tk. (4.26)
For formulating the impulsive dynamical system, we consider ξs =

xT 
x
T
vec
W˜ u
T

∈ Rnξs
as the augmented states where vec
W˜ u

∈ Rlumu is the vector form of the NN weight estimation error
matrix and nξs = nx +nx +lumu.
Now combine (4.20), (4.21) and (4.15) to obtain the flow dynamics as
˙
ξs = Fs
c (ξs), ξs ∈ C ⊂ Ds, ξs ∈/ Zs. (4.27)
Next combine (4.24), (4.25) and (4.26) to get the reset dynamics as
Δξs = Fs
d (ξs), ξs ∈ Ds, ξs ∈ Zs, (4.28)
for the impulsive dynamical nonlinear system where the nonlinear functions, Fs
c (ξs) and Fs
d (ξs),
are defined as Fs
c (ξs) =
⎡
⎣
H (ξs)
0
0
⎤
⎦ and Fs
d (ξs) =
⎡
⎣
0
es
vec
αuχsϕu (x¯) eT
s L+κWˆ u

⎤
⎦ with H (ξs) = Ax−
g(x)

W˜ T
u ϕu (x¯) +εu(x)

+g(x)
"
Wˆ T
u ϕu
"¯
x
#
−Wˆ T
u ϕu (x¯)
#
and Δξs = ξ +
s −ξs. The set Ds ⊂ Rnξ is
an open set with 0 ∈ Ds. The flow set Cs ⊂ Ds is defined as Cs = {ξs ∈ Ds : es ≤ σs x}, Zs ⊂ Ds
is the jump set and defined as Zs = {ξs ∈ D : es > σs x} where σs x is the event-triggering
threshold to be designed next.
The following lemma guarantees the boundedness of the NN weight estimation error both during
the flow and the jump instants.
Lemma 4.1. (Boundedness of the NN weight estimation error): Consider the nonlinear continuous￾time system (4.1) and the controller (4.12) expressed as a nonlinear impulsive dynamical system
(4.27) and (4.28). Let Assumptions 4.1 through 4.4 be satisfied while the initial NN weights, Wˆ u (0),
are initialized in the compact set ΩWu . Under the assumption that a non-zero positive lower bound
on the inter-event times, δtk = tk+1 −tk > 0, k ∈ N exists, there exist positive constants αu > 0,
0 < κ < 1/2 , T and T such that the weight estimation error, ¯ W˜ u, is bounded during the flow period
and ultimately bounded for all tk > T or, alternatively t ¯ > T when the NN weights are updated by
using (4.13) and (4.14).
Proof: Refer to (Sahoo et al., 2016).
The event-triggering condition, given by
D(es) ≤ σs x, (4.29)
where
σs = Γsqmin
4gmaxLϕu

Wˆ u

P , (4.30)
is the threshold coefficient with 0 < Γs < 1 and Lϕu is the Lipschitz constants for the activation
functions, qminis the minimum eigenvalue of Q, P is a symmetric positive definite matrix with P and
Q satisfying (4.6), and D() is a dead-zone operator defined as
D(es) = $es , ifx > Bx
s,max,
0, otherwise, (4.31)Nonlinear Continuous-time Systems 137
where Bx
s,max is the bound for the system state vector x. The system state vector is transmitted to
the controller, and the updated control input is transmitted to the plant by the violation of the event￾triggering condition (4.29). The adaptive event-triggering condition (4.29) guarantees the stability
of the equilibrium points of the closed-loop impulsive dynamical system, as stated in the theorem
below.
Theorem 4.1. (Closed-loop stability): Consider the nonlinear system (4.1), the control input (4.12),
NN update laws (4.13) and (4.14), expressed as an impulsive dynamical system (4.27) and (4.28).
Let Assumptions 4.1 through 4.4 hold. Assume there exists a non-zero positive lower bound on the
inter-event times given by δtk = tk+1 −tk > 0, k ∈ N and the initial NN weight, Wˆ u (0), is initialized
in the compact set ΩWu . Then, the closed-loop system state vector ξs for any initial condition ξs (0) ∈
Ds ⊂ Rnξ is locally ultimately bounded with a bound ξs ≤ Ξ provided the events are triggered at
the violation of the condition (4.29). Further, the ultimate bound is given by
Ξ =


η/λmin (P¯) , (4.32)
where P¯ = diag{P,P,I} is a positive definite matrix whereI is the identity matrix with ap￾propriate dimension and η = max
λmax (P¯)μ2
s ,θ
 with θ = sup
ξs∈B¯μs∩Zs
Vs

ξs +Fs
d (ξs)

, μs =
max 
Bx
s,max,B

x
s,max,BW˜
s,max!
where Bx
s,max, B
x
s,max, and BW˜
s,max are the bounds for the system state,
x, the last transmitted state, 
x, and the NN weight estimation error, W˜ u, respectively.
Proof: For detailed proof, refer to (Sahoo et al., 2016).
Remark 4.1. The threshold coefficient σs of the event-triggering condition (4.29) is a function of the
norm of NN weight estimates 
Wˆ u

and, hence, adaptive in nature. Since the weights are updated
only at the triggering instants, 
Wˆ u

 is piecewise constant and jumps at the triggering instants
t = tk, according to the update law (4.14). This implies that σs is also a piecewise constant function
and changes at the triggering instants. This variation in σs, implicitly depends on the NN weight
estimation error, W˜ u, which generates the required number of triggers for the NN approximation of
the control input during the learning phase. Once the NN weight matrix, Wˆ u, converges close to the
unknown constant target weight matrix, Wu, the weight estimates, Wˆ u becomes steady; in turn, σs
becomes a constant like the traditional event-triggered control with known knowledge of the system
dynamics Tabuada (2007); Tallapragada and Chopra (2013).
Remark 4.2. The dead-zone operator D() is used to stop the unnecessary triggering of events
due to the NN reconstruction error once the state vector reaches and stays within the ultimately
bounded (UB) region. This implies that, for an event to trigger, the following two conditions need to
be satisfied:
1) The system state vector is outside the bound, i.e.x > Bx
s,max, and
2) the event-trigger condition (4.29) is violated, i.e., es > σs x.
Remark 4.3. The assumption on the non-zero positive lower bound on inter-event times in Theorem
4.1 is relaxed by guaranteeing a non-zero positive value in Theorem 4.2, which is discussed in detail
in Section 4.2.3. In addition, an explicit formula for analyzing the lower bound on the inter-event
times when the system state vector x > Bx
s,max to avoid accumulation point is also derived.
Remark 4.4. From the proof, the system state vector, x, and NN weight estimation error, W˜ u, remain
locally ultimately bounded (UB) for all tk > T or alternatively, for all t ¯ > T where the time T
depends on T . This implies that the control input and the event-triggering error are also locally UB. ¯
Consequently, all the closed-loop system parameters remain UB for all time t > T .138 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
4.2.3 LOWER BOUND ON INTER-EVENT TIMES
In this section, the existence of non-zero positive lower bound on inter-event times is presented in
the following theorem. In addition, an explicit formula for the s is derived.
Theorem 4.2. Consider the event-triggered system (4.1) along with the controller (4.12) repre￾sented as an impulsive dynamical system (4.27) and (4.28). Let Assumptions 4.1 through 4.4 hold
and NN weights, Wˆ u (0)is initialized in a compact set ΩWu and updated using (4.13) and (4.14) by
the violation of event-triggering condition (4.29). Then, the lower bound on the inter-event times
δtk = tk+1 −tk for all k ∈ N implicitly defined by (4.29) is bounded away from zero and is given by
δtk ≥
1
A ln
1+ A
ν1,k
σs,minBx
s,max
> 0, (4.33)
where σs,min is the minimum value of the threshold coefficient over all inter-trigger times. Further,
ν1,k = gmax 
W˜ u,k

ϕu,max +εu,max
+2gmaxLϕuϕu,max

Wˆ u,k

 with W˜ u,k and Wˆ u,k are the NN weight
estimation error and weight estimate for kth flow interval.
Proof. For detailed proof, refer to (Sahoo et al., 2016).
Furthermore, it is interesting to study the effect of NN weight estimation error W˜ u on the inter￾event times. The following proposition defines a relation between the lower bound on inter-event
times δtk and the NN weight estimation error, W˜ u.
Proposition 4.1. Assume the hypothesis in Theorem 3 holds. Then, the lower bound on inter-event
times also satisfies
δtk ≥
1
A ln
1+ A
νM,k
σs,minBx
s,max
, (4.34)
where vM,k = gmax 
ϕu,max(1+2Lϕu )

W˜ u,k

+εu,max
+2gmaxLϕuϕu,maxWu,max.
Proof. For detailed proof, refer to (Sahoo et al., 2016).
Remark 4.5. It is clear from (4.34) that the lower bound on inter-event times depends on vM,k
which is a function of NN weight estimation error W˜ u. During the initial learning phase of the NN,
the term vM,k in (4.34) might become larger for certain initial value Wˆ u (0) and lead to smaller
inter-event times closer to zero. A proper initialization of the NN weights, Wˆ u(0), close to the target
will reduce the weight estimation error, W˜ u, and in turn vM,k in (4.34). This will keep the inter-event
times away from zero and reduce the number of transmissions in the initial phase. In addition, as per
Lemma 4.1, the convergence of the NN weight estimation errors to the bound will further increase
the inter-event times, leading to less resource utilization.
We will now consider some examples to implement the control algorithms. The first example
considers a second-order system and is an academic example providing an intuitive idea of the
analytical design. Tthe second example emphasizes the practical application point of view by con￾sidering a practical industrial example of a two-link robot manipulator.
Example 4.1. Consider the single-input second-order nonlinear dynamics given by
x˙1 = x2,
x˙2 = −x3
1 −x2 +u. (4.35)
The simulation parameters include the initial state vector as [5 −1]
Twhereas the closed-loop sys￾tem matrix is given by A = [0 1;−3 −4] and the positive definite matrix, Q = diag{0.1, 0.1}.Nonlinear Continuous-time Systems 139
Table 4.1
Event-triggered control of nonlinear system
System dynamics x˙ = f(x) +g(x)u, x(0) = x0
Event-triggering error es = x−x , t
k < t ≤ t
k+1
Control input u = Wˆ T
u ϕu(
¯
x ), t
k < t ≤ t
k+1
NN weight update law ˙
Wˆ u = 0, for t
k < t ≤ t
k+1
Wˆ +
u = Wˆu − αu
c+es2 ϕu (x¯) eT
s L−κWˆu,t = t
k ,
Event-triggering condition D(es) ≤ σs x
D(es) = $es , ifx > Bx
s,max,
0, otherwise,
σs = Γsqmin
4gmaxLϕu

Wˆu

P
The learning gain, αu = 0.001, κ = 0.001Γs = 0.9, c = 1 and L ∈ R2×1 with elements are all
one. The ultimate bound for the system state vector was chosen as 0.001. The tangent hyperbolic
activation function, tanh
VT
u x

, was used in the NN hidden layer with a randomly initialized fixed
input weight, Vu, from the uniform distribution in the interval [0, 1]. The Lipschitz constant for the
activation function was computed to be Lϕu= Vu = 4.13. The number of neurons in the hidden
layer was chosen as lu = 15. The NN weight Wˆ u (0) was initialized at random from the uniform
distribution in the interval [0, 0.01]. The sampling time chosen for simulation was 0.001 sec.
Figure 4.3 (a) illustrates the evolution of the state-dependent event error and threshold, and in
Figure 4.3 (b), the cumulative number of events occurred. The total number of events triggered is
found to be 645, and the events frequently occurred during the initial NN learning phase. This is
due to a large initial NN weight estimation error, W˜ u as discussed in Remark 4.5. Alternatively, the
event-triggering condition generates the required number of triggers for the NN to approximate the
control input. A proper selection of the initial weights, Wˆ u (0), will further reduce the number of
initial triggers.
Furthermore, the lower bound on the inter-event times is observed to be 0.002 sec, as shown
in Figure 4.4, implying the existence of a non-zero lower bound on the inter-event times to avoid
accumulation points. It is clear from Figure 4.4 that the inter-event times are gradually increasing
along with the convergence of the weight estimation error, W˜ u, to its ultimate bound, as presented
in Proposition 4.1 and discussed in Remark 4.5. This elongated inter-event time reduces resource
utilization, which is one of the primary objectives of the design.
Figures 4.5 (a) and (b) depict the convergence of the closed-loop ETC system state vector and
approximated control input. This implies the event-based control input with reduced computation
can regulate the system state close to zero. Figure 4.6 shows the convergence of the estimated NN
weights with an aperiodic weight update.140 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Figure 4.3 (a) Evolution of the event-trigger threshold. (b) Cumulative number of events.
Figure 4.4 Existence of a nonzero positive lower bound on inter-event times.
Figure 4.5 Convergence of (a) system states and (b) approximated control input.Nonlinear Continuous-time Systems 141
Figure 4.6 Convergence of the NN weight estimates.
Next, we consider the benchmark example of a MIMO system to evaluate the design.
Example 4.2. A two-link robot manipulator is considered whose dynamics are given by
x˙ = f (x) +g(x)u, (4.36)
where f(x) =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
x3
x4

−(2x3x4 +x2
4 −x2
3 −x2
3 cosx2)sinx2
+20 cosx1 −10 cos(x1 +x2)cosx2

cos2 ⎛ x2−2
⎜⎜⎝
(2x3x4 +x2
4 +2x3x4 cosx2 +x2
4 cosx2 +3x2
3
+2x2
3 cosx2 +20(cos(x1 +x2)−cosx1)×
(1+cosx2)−10 cosx2 cos(x1 +x2)
⎞
⎟⎟⎠
cos2x2−2
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
and g(x) =
⎡
⎢
⎢
⎢
⎣
0 0
0 0
1
2−cos2x2
−1−cos x2
2−cos2x2 −1−cos x2
2−cos2x2
3+2 cos x2
2−cos2x2
⎤
⎥
⎥
⎥
⎦
.
The following simulation parameters were selected for the simulation. The initial state vector is
given by x = 
π/3 −π/10 0 0T while the closed-loop matrix A = diag{−3, −4, −6, −8}
and the positive definite matrix was chosen as Q = diag{0.1, 0.1, 0.1, 0.1}. The learning gain
was selected as αu = 0.5, Γs = 0.9, κ = 0.0015, L ∈ R4×2 with elements all one, gmax = 3 and
c = 1. The bound for system state vector was chosen as 0.001. The tangent hyperbolic activation
function was used in the hidden layer of the NN with a randomly initialized fixed input weight Vu
from the uniform distribution in the interval[0, 1]. The Lipschitz constant for the activation function
was computed to be Lϕu = Vu = 3.42. The number of neurons in the hidden layer was selected as
l = 15. The NN weight Wˆ u (0) was initialized at random from the uniform distribution in the interval
[0, 0.01]. The sampling time chosen for simulation was 0.001 sec.142 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Figure 4.7 (a) Evolution of the event-trigger threshold. (b) Cumulative number of events.
The event-triggering threshold is shown in Figure 4.7 (a), along with the event-triggering error.
The cumulative number of triggered events is illustrated in Figure 4.7(b), which shows the state
vector is only transmitted 2000 times, indicating the reduction in communication bandwidth usage
when compared to a continuous transmission. Further, the lower bound on the inter-event times is
found to be 0.002 sec, proven in Theorem 4.2. In addition, as per Proposition 4.1, the inter-event
times increase with the convergence of the NN weight estimates to the target, as shown in Figure
4.8.
Figure 4.8 Existence of a nonzero positive lower bound on inter-event times and gradual increase with the
convergence of NN weight estimates to target.
Further, from Theorem 4.1, the cumulative number of events depends upon the initial NN
weights. The histogram in Figure 4.9 shows the plot between the norm of initial weights and the
cumulative number of events. The cumulative number of events varies with weight initialization.
Convergence of the system state and control input is shown in Figures 4.10 (a) and (b), re￾spectively, implying the event-based controller-regulated system states close to zero. Further, the
convergence of the estimated NN weights to the target value with aperiodic event-based update law
is shown in Figure 4.11.
Finally, comparison results in terms of computation and the network traffic between a sampled￾data system with a fixed periodic sampling and event-based sampling are presented in Table 4.2 and
Figure 4.12, respectively.
Table 4.2 gives the number of computations observed in terms of addition and multiplications
that are needed for realizing both methods. It is evident that with the event-based system, a 48%Nonlinear Continuous-time Systems 143
Figure 4.9 Cumulative number of events with different NN initial weights.
Figure 4.10 Convergence of (a) system state vectors and (b) control input.
Figure 4.11 Convergence of the NN weight estimates.144 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Figure 4.12 Data rates with periodic as well as event-based sampling.
Table 4.2
Comparison of computational load between periodic and event-sampled system.
System
Traditional
periodic
sampled data
system
Event-based
non-periodic
sampled
Samping instants 30,000 2000
Number of
additions and
Multiplications
at every
sampling
instant
NN update law at
the controller 10 10
Update law at the
trigger mechanism 0 10
Trig. Condition
executed at every
periodic sampled
instant
0 6
Total number of Computation 226000
reduction in computation when compared to the sample data approach is observed. Further, consid￾ering each packetized transmission is of 8-bit data through the ideal network, Figure 4.12 compares
the data rates in both the cases. It is clear that the data rate in the case of event-based sampling is
lower, implying that the needed network bandwidth is less. This verifies the resourcefulness of the
event-triggered control design.
4.3 OPTIMAL CONTROL USING EVENT-BASED NEURO-DYNAMIC PRO￾GRAMMING
In the previous sections, an event-based approximation of the controller using NN is presented
to design a stabilizing controller with less computation and communication. In this section, we
will discuss an ADP-based optimal control scheme for nonlinear continuous-time systems with
completely uncertain dynamics and event-based sampling. The ADP scheme uses two NNs, oneNonlinear Continuous-time Systems 145
for approximating the uncertain system dynamics and the other for the value function. Similar to
the stable controller design in the previous section, an adaptive sampling condition is derived via
Lyapunov techniques, the NN weight estimates are tuned at the event sampled instants, and the
closed-loop system is formulated as a nonlinear impulsive dynamical system (Haddad et al., 2006;
Goebel et al., 2009; Hayakawa and Haddad, 2005) to leverage the extension of the Lyapunov direct
method (Haddad et al., 2006) for guaranteeing the locally UB of all signals.
4.3.1 BACKGROUND AND PROBLEM STATEMENT
Consider the controllable nonlinear continuous-time systems given in (4.1) satisfying the following
assumption.
Assumption 4.5. (Dierks and Jagannathan, 2010b) The nonlinear system is controllable and ob￾servable. The nonlinear matrix function g(x) for all x ∈ Ωx satisfies gm ≤ g(x) ≤ gM, where gM
and gm are the known positive constants, with Ωx is a compact set.
In an optimal control setting, the goal is to design a control input that minimizes the value func￾tion given by
V(t) =  ∞
t
r(x(t),u(t))dt (4.37)
where r(x,u) = Q(x) +uTRu is the cost-to-go function, Q(x) ∈ R and R ∈ Rm×m represent positive
definite quadratic function and matrix, respectively, to penalize the system state vector and the
control input. The initial control input u0 must be admissible to keep the infinite horizon value
function (4.37) finite.
The Hamiltonian for the cost function (4.37) can be given by
H(x,u) = Q(x) +uTRu+∇xVT [ f(x) +g(x)u],
where ∇xV = ∂V
∂ x . The optimal control policy u∗(x) that minimizes the value function (4.37) can be
obtained using the stationarity condition as
u∗(x) = −1
2
R−1gT (x)
∂V∗
∂ x (4.38)
where V∗ ∈ R is the optimal value function. Then, substituting the optimal control input into the
Hamiltonian, the HJB equation becomes
H∗ (x,u∗) =Q(x) +∇xV∗(x)f(x)− 1
4
∇xV∗T (x)g(x)R−1gT (x)∇xV∗(x) = 0 (4.39)
where ∇xV∗(x) = ∂V∗
∂ x . It is extremely difficult to obtain an analytical solution to the HJB (4.39).
Therefore, the ADP techniques (Dierks and Jagannathan, 2010b) are utilized to generate an approx￾imate solution in a forward-in-time manner by using periodically sampled state vectors with the
assumption that the system state or output vectors are available continuously for measurement.
For optimal policy generation using ADP in an event-sampled framework, the value function
and the system dynamics need to be approximated with intermittently available system state vector.
The optimal value function using NN-based approximation with an event-sampled state vector, as
discussed in the previous section, can be written as
V∗(x) = WT
V φ(x˘) +εe,V (x˘, es), tk < t ≤ tk+1 (4.40)
where WV ∈ RlV is the unknown constant target NN weights, φ(x˘) ∈ RlV is the event-sampled ac￾tivation function, and εe,V (x˘, es) = WT
V (φ(x)−φ(x˘)) +εV (x˘+es) is the eventbased reconstruction
error, where εV (x˘+es) = εV (x) ∈ R is the traditional reconstruction error.146 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
The HJB equation (4.39) with event-sampled approximation of the value function (4.40) can be
expressed as
H∗ (x,u∗) =Q(x) +
WT
V ∇xφ(x˘) +∇xεe,V (x˘, es)

f(x)− 1
4

WT
V ∇xφ(x˘) +∇xεe,V (x˘, es)

×D(x)

∇xφ T (x˘)WV +∇xεe,V (x˘,es)

(4.41)
where D(x) = g(x)R−1gT (x),∇xφ(x˘) = ∂ φ(x˘)
∂ x , and ∇xεe,V (x˘, es) = ∂ εe,V (x˘,es)
∂ x . It is clear from (4.41)
that similar to the NN approximation, the HJB equation is also a function of the event sampling
error es. In other words, the optimal controller performance is governed by the design of the event
sampling condition.
Thus, the event-sampled optimal control problem in an approximate optimal control framework
can be defined more precisely as follows.
1. Approximate the unknown system dynamics f(x) and g(x), and the value function, V, in an
event-sampled context with a desired level of accuracy.
2. Design the event sampling condition not only to reduce computation but also to minimize ap￾proximation error.
3. Guarantee a positive lower bound on the inter-sample times.
	

!
 
 

		 

 
	 
		

"# 

 

 
Figure 4.13 Near-optimal event-sampled control system.
4.3.2 APPROXIMATE OPTIMAL CONTROLLER DESIGN
The structure of the event-sampled approximate optimal controller is shown in Fig. 4.13, and the
design will be carried out using two NNs with an event-sampled state vector. One NN is used as an
identifier to approximate the unknown system dynamics, and the second one is used to approximate
the solution of the HJB equation, which is the value function. Now, to reduce the computation and
ensure the accuracy of the approximation, we propose an adaptive event-triggering or sampling
condition as a function of event-sampling error, the estimated NN weights, and the system state
vector (defined later). The system state is sent to the controller at the event-sampled instants andNonlinear Continuous-time Systems 147
used to tune the NN weights. The weights are held during the inter-sample times, so the tuning
scheme becomes aperiodic.
Remark 4.6. To evaluate the proposed adaptive event sampling condition at the triggering mecha￾nism, in the case of networked control systems (NCSs) Xu et al. (2012), it will require the transmis￾sion of the NN weight estimates from the controller. To mitigate this additional transmission cost,
mirror identifier and value function approximator NNs are used at the triggering mechanism to es￾timate the NN weights locally. Both the actual and mirror NNs operate in synchronism. Thus, this
design can be considered as an event-sampled NCS with negligible delays and packet losses.
4.3.3 IDENTIFIER DESIGN
The knowledge of the system dynamics, f(x) and g(x), is needed for the computation of the optimal
control policy (4.38). To relax this, an event-sampled NN-based identifier design is presented in this
section. By using the event-based approximation (Sahoo et al., 2015), the nonlinear continuous-time
system in (3) can be represented as
x˙ = f(x) +g(x)u = WT
I σI(x˘)u¯+εe,I (4.42)
where WI =  WT
f WT
g
T
∈ R(lf +mlg)×n
, where Wf ∈ Rlf ×n and Wg ∈ Rmlg×n are the un￾known target NN weight matrices, and σI(x˘) =  σf(x˘) 0
0 σg(x˘)

, where σf(x˘) ∈ Rlf and σg(x˘) ∈
Rmlg×m are the event-sampled activation functions. The error εe,I = 
εe, f (x˘, es) εe,g (x˘, es)

u¯ is the
eventbased reconstruction error with εe, f (x˘,es) = WT
f

σf(x)− σf(x˘)

+ ε f (x˘+es), εe,g (x˘, es) =
WT
g (σg(x)−σg(x˘))+ εg (x˘+es), and ¯u =  1 uT T
. The subscripts f and g denote the param￾eters corresponding to the functions f(·) and g(·), respectively. The event-sampled reconstruction
error εe,I can also be written as εe,I = WT
I σ˜I(x,x˘)u¯ +εI, where σ˜I(x, x˘) = σI(x)−σI(x˘) is the acti￾vation function error and εI =  ε f εgu 
is the augmented traditional reconstruction error. The
constants lf and lg denote the number of neurons of the NNs. The following assumption holds for
the NN (Jagannathan, 2006).
Assumption 4.6. The target weight vector, WI, the activation function, σI(·), and the augmented
reconstruction error εI(·) are bounded above in a compact set, such that WI ≤ WI,M, σI(·) ≤
σI,M, and εI(·) ≤ εI,M satisfied, where WI,M, σI,M, εI,M are positive constants. Furthermore, it is
assumed that the activation function σI(x) is Lipschitz continuous in the compact set for all x ∈ Ωx
and satisfies σI(x)−σI(x˘) ≤ CσI x−x˘, where CσI > 0 being a constant.
Since the system state vector is only available at event sampled instants, the identifier dynamics
is defined as
˙
xˆ = A(xˆ−x˘) + ˆf(x˘) +gˆ(x˘)u, tk < t ≤ tk+1 (4.43)
where ˆx ∈ Rn is the identifier estimated state vector, A is a user-defined Hurwitz matrix and satisfies
the Lyapunov equation ATP+PA = −Π, where P and Π are positive definite matrices. The matrix
A ensures the stability of the identifier. The functions ˆf(x˘) ∈ Rn and ˆg(x˘) ∈ Rn×m are the estimated
system dynamics. By using the event-sampled NN approximation for the system as in (4.42), the
estimated value of the identifier dynamics is represented as
˙
xˆ = A(xˆ−x˘) +Wˆ T
I (t)σI(x˘)u¯, tk < t ≤ tk+1 (4.44)
where WˆI(t) is the estimated NN weight matrices and σI(x˘) being event sampled the activation
function.148 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Defining eI = x−xˆ as the identification error, the identifier error dynamics, from (4.42) and (4.44)
can be expressed as
e˙I = AeI −Aes +W˜ T
I σI(x˘)u¯+Wˆ T
I σ˜I(x, x˘)u¯+εI (4.45)
where W˜I = WI −WˆI is the NN identifier weight estimation error. Since eI can only be computed at
the event-sampled instants with the current sampled state at the identifier, the NN identifier weight
matrices are tuned at the event-sampled instants only. This can be considered as a jump in the
identifier NN weights, which is given by
Wˆ +
I = WˆI +
αIσI(x)ue¯ T
I
"
c+u¯2 eI2
# −αIWˆI, t = tk (4.46)
where αI > 0 denotes the learning rate, and c > 0 is a positive constant. During the inter-sample
times referred as flow duration, i.e., tk < t ≤ tk+1, the weights are held at the previously tuned values.
Therefore, the tuning law for the flow duration is given by
˙
WˆI = 0, tk < t ≤ tk+1. (4.47)
From (4.46) and (4.47), it is clear that the NN weights are tuned at aperiodic instants saving
computation when compared with the traditional NN [10], [19] control. The approximated control
coefficient function in (4.43) is held to ensure the ultimate boundedness of the closed-loop system
parameters once it becomes less than equal to the lower bound. It can be expressed as
gˆ(·) =
$
gmin, if gˆ(x) ≤ gmin
gˆ(·), otherwise. (4.48)
The NN identifier weight estimation error dynamics from (4.46) and (4.47) at both jump and flow
durations can be expressed as
W˜ +
I = W˜I − αIσI(x)ue¯ T
I
c+u¯2 eI2 +αIWˆI, t = tk (4.49)
˙
W˜I = W˙I − ˙
WˆI = 0, tk < t ≤ tk+1. (4.50)
We will use the identifier dynamics to design an event-sampled near-optimal controller.
4.3.4 CONTROLLER DESIGN
In this section, the solution to the HJB equation, essentially the value function, is approximated by
using a second NN with an event-sampled state vector. The approximated value function is utilized
to obtain the near-optimal control input. Consider the event-sampled approximation of the optimal
value function in (4.40). The following assumptions hold in a compact set.
Assumption 4.7. The target NN weights, activation functions, and the traditional reconstruction er￾rors of the value function approximation NN are bounded above satisfying WV  ≤ WV,M,φ(·) ≤
φM, and |εV (·)| ≤ εV,M, where WV,M,φM, and εV,M are the positive constants. It is further assumed
that the gradient of the activation function and the reconstruction error is bounded by a positive
constant, i.e., ∇xφ(·) = ∂ φ(·)/∂ x ≤ φ
M and ∇xεV (·) = ∂ εV (·)/ ∂ x ≤ ε
V,M.
The activation function and its gradient are Lipschitz continuous in a compact set, such that for
x ∈ Ωx, there exist positive constants Cφ > 0 and C∇φ > 0, satisfying φ(x) − φ(x˘) ≤ Cφ x − x˘
and ∇xφ(x)−∇xφ(x˘) ≤ C∇φ x−x˘.Nonlinear Continuous-time Systems 149
The optimal control policy (4.38) in terms of event-sampled NN approximation of the value
function becomes
u∗ = −1
2
R−1gT (x)

∇xφ T (x˘)WV +∇xεe,V (x˘, es)

. (4.51)
The estimated value function in the context of an event-sampled state can be represented as
Vˆ(x˘) = Wˆ T
V φ(x˘), tk < t ≤ tk+1. (4.52)
Therefore, the actual control policy by using the estimated value function (4.52) and the identifier
dynamics is given by
u(x) = −1
2
R−1gˆ
T (x˘)∇xφ T (x˘)WˆV
= −1
2
R−1 
Wˆ T
g σg(x˘)
T ∇xφ T (x˘)WˆV , tk < t ≤ tk+1.
(4.53)
Now, with the estimated value function (4.52) and approximated system dynamics (4.43), the
error introduced in the HJB equation (4.41), referred to as temporal difference (TD) or HJB equation
error, can be expressed as
Hˆ(x˘,u) = Q(x˘) +uTRu+∇xVˆ T (x˘)[ ˆf(x˘) +g(x˘)u] (4.54)
for tk < t ≤ tk+1, where ∇xVˆ(x˘) = ∂Vˆ(x˘)/∂ x. Substituting the actual control policy (4.53) in (4.54),
the TD or HJB equation error can further be expressed as
Hˆ

x˘,WˆV

=Q(x˘) +Wˆ T
V ∇xφ(x˘) ˆf(x˘)− 1
4
Wˆ T
V ∇xφ(x˘)D(x˘)∇T
x φ(x˘)WˆV , (4.55)
for tk < t ≤ tk+1, where Dˆ(x˘) = gˆ(x˘)R−1gˆ
T (x˘).
Similar to the NN identifier, the value function NN weight is updated at the event-sampled in￾stants with the updated HJB error. The HJB error (4.55) with event-sampled state at the sampled
instants with ˘x+ = x,t = tk can be written as
Hˆ + 
x,WˆV

= Q(x) +Wˆ T
V ∇xφ(x) ˆf(x)− 1
4
Wˆ T
V ∇xφ(x)D(x)∇T
x φ(x)WˆV , t = tk. (4.56)
The NN tuning law at the event-sampled instants is selected as
Wˆ +
V = WˆV −αV
ωˆ Hˆ +T 
x,WˆV

(1+ωˆ Tωˆ)
2 , t = tk (4.57)
where α1 > 0 is the NN learning gain parameter and
ωˆ = ∇xφ(x) ˆf(x)− 1
2
∇xφ(x)Dˆ(x)∇T
x φ(x)WˆV . (4.58)
During the inter-sample times or flow period, the tuning law for the value function NN is given
as ˙
WˆV = 0, tk < t ≤ tk+1. (4.59)
Define the value function NN weight estimation error as W˜V = WV −WˆV . The NN weight esti￾mation error dynamics by using (4.57) and (4.59) can be expressed as
W˜ +
V = W˜V +
"
αVωˆ Hˆ +T 
x,WˆV
1+ωˆ Tωˆ
2
#
, t = tk, (4.60)
˙
W˜V = 0, tk < t ≤ tk+1. (4.61)150 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
The HJB or TD error in terms of the value function NN weight estimation error W˜V , using (4.41)
and (4.55) can be expressed as
Hˆ

x˘,WˆV

= Q(x˘)−Q(x) +Wˆ T
V ∇xφ(x˘) ˆf(x˘)−WT
V ∇xφ(x)f(x)−(1/4)Wˆ T
V ∇xφ(x˘)D(x˘)∇T
x φ(x˘)WˆV
−(1/4)WT
V ∇xφ(x)D(x)∇xφ T (x)WV −εH, tk < t ≤ tk+1
(4.62)
where εH = ∇T
x εV (x) (f(x) +g(x)u∗) − (1/4)∇T
x εV (x) D(x)∇xεV (x). It is routine to check that
εH ≤ εH,M, where εH,M is a positive constant.
Similarly, the HJB equation error at the event-sampled instants with ˘x+ = x can be computed
from (4.62) as
Hˆ + 
x,WˆV

= −W˜ T
V ∇xφ(x) ˆf(x)−WT
V ∇xφ(x) ˜f(x) + 1
2
W˜ T
V ∇xφ(x)D(x)∇T
x φ(x)WˆV
+
1
4
W˜ T
V ∇xφ(x)D(x)∇T
x φ(x)W˜V + (1/4)WT
V ∇xφ(x)D(x)∇xφ T (x)WV −εH t = tk.
(4.63)
4.3.5 STABILITY OF THE CLOSED-LOOP SYSTEM
Now, consider the augmented state vector ζ = 
xT eT
I xˇ
T vec(W˜I)T W˜ T
V

.
Flow Dynamics: The closed-loop system dynamics during the flow, tk < t ≤ tk+1, can be repre￾sented by
x˙ = f(x) +g(x)

−1
2
R−1gˆ
T (x˘)∇xφ T (x˘)WˆV

. (4.64)
Adding and subtracting g(x)u∗ in (4.64) and with some simple mathematical operations, the
closed-loop dynamics during the flow can be written as
x˙ =f(x) +g(x)u∗ + (1/2)g(x)R−1g˜
T (x)∇xφ T (x)WV + (1/2)g(x)R−1gˆ
T (x)∇xφ T (x)W˜V
+ (1/2)g(x)R−1gˆ
T (x˘)×
∇xφ T (x)−∇xφ T (x˘)

WˆV + (1/2)g(x)R−1gT (x)∇xεV (x) (4.65)
for tk < t ≤ tk+1.
The flow dynamics for the identification error eI is the same as in (4.45). The last held state, ˘x,
during the flow period remains constant. Thus, the dynamics can be represented as
˙
x˘ = 0, tk < t ≤ tk+1. (4.66)
Finally, the dynamics of the NN weight estimation errors vec
W˜I

and W˜V during the flow are as
in (4.50) and (4.61) represented in vector form. Combining (4.45), (4.50), (4.61), (4.65), and (4.66),
the flow dynamics of the impulsive dynamical system as
˙
ζ = Fc(ζ ), ζ ∈ Ωζ , ζ ∈ C, ζ ∈/ Z (4.67)
where Fc(ζ ) = 
FsT
c FeT
c 0T 0T 0T
T
with
Fs
c = f(x) +g(x)u∗ +
1
2
g(x)R−1g˜
T (x)∇xφ T (x)WV +
1
2
g(x)R−1gˆ
T (x)∇xφ T (x)

WV −W˜V

+
1
2
g(x)R−1gˆ
T (x˘)

∇xφ T (x)−∇xφ T (x˘)
WV −W˜V

+
1
2
g(x)R−1gT (x)∇xεV (x),FeI
c
= AeI −Aes +W˜ T
I σI(x˘)u¯+Wˆ T
I σ˜I(x, x˘)u¯+εI
and 0T s are the null vectors of appropriate dimensions.Nonlinear Continuous-time Systems 151
Jump Dynamics: The jump dynamics of the system state vector and the identification error are
given by
x+ = x, t = tk (4.68)
e+
I = x+ −xˆ
+ = x−xˆ = eI, t = tk. (4.69)
The jump dynamics of the last held state, ˘x, is given by
x˘
+ = x, t = tk. (4.70)
Furthermore, the jumps in the NN weight estimation errors are given by (4.49) and (4.60).
Defining the first difference Δζ = ζ + −ζ and using (4.49), (4.60), and (4.68)-(4.70), the differ￾ence equation for the reset/jump dynamics can be written as
Δζ = Fd(ζ ), ζ ∈ Ωζ , ζ ∈ Z (4.71)
where Fd(ζ ) = 
0T 0T eT
s vec
ΔW˜I
T 
ΔW˜V
T T
with
ΔW˜I = − αI
c+u¯2 eI2 σI(x)ue¯ T
I +αI

WI −W˜I

and
ΔW˜V = αV
ωˆ Hˆ + 
x,WˆV

(1+ωˆ Tωˆ)
2 .
The set C ⊂ Ωζ is the flow set, Z ⊂ Ωζ is the jump set, and Ωζ ⊂ Rnζ is an open set with 0 ∈
Ωζ ,nζ = n + n + n + lI + lV and lI = 
lf +mlg

n. The event sampling condition decides the flow
and jump sets introduced next.
Consider the event sampling error in (4.7). Define a condition given by
D(es) > σETC 
x,

WˆV

,

WˆI


 (4.72)
where σETC 
x,

WˆV

,

WˆI


 = min
ΓQ(x)
η1 ,

2ΓQ(x)
η2
5
is the threshold with 
WˆV

 ≥ κ,

WˆI

 ≥ κ,η1 = gMλmax 
R−1

σI,Mφ
M

WˆV


WˆI

C∇φ and η2 =C∇φ gMλmax 
R−1

σI,Mφ
M

WˆI

+
32
Πmin "
CσI

WˆI


2
u¯2P2 +AP2
#
. The constant κ is a small positive constant to ensure that the
threshold is well-defined. The previous value of the NN weight estimates is used to compute (4.72)
of the estimated values 
WˆV

 < κ or 
WˆI

 < κ. The design parameter Γ satisfies 0 < Γ < 1, the
constants C∇φ and CσI are the Lipschitz constants, and Πmin = λmin(Π). The dead-zone operator
D(·) is defined as
D(·) = $
·, x > Bx
ub
0 (4.73)
where Bx
ub is the desired ultimate bound for the closed-loop system and can be selected arbitrarily
close to zero. The event-sampled instants are decided when the condition (4.72) is satisfied.
Remark 4.7. The main advantage of the adaptive event sampling condition (4.72) is that the thresh￾old σETC(·) gets tuned with NN weight estimates in addition to the system state. Therefore, the sys￾tem states will be sampled based on the NN weight estimation errors and the system performance,
ensuring the accuracy of approximation and stability. Once the NN weights converged close to their
target values and became constant, the threshold became similar to those used in the traditional
event sampling conditions (Tabuada, 2007; Postoyan et al., 2011).152 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Remark 4.8. The dead-zone operator D(·) prevents unnecessary triggering of events due to the NN
reconstruction error once the system state is inside the ultimate bound.
To show the locally ultimately boundedness of the closed-loop event-sampled system, we will
use the extension of the Lyapunov direct method of stability from (Haddad et al., 2006) for the
nonlinear impulsive dynamical systems. Before claiming the main results, the following technical
results are necessary.
Lemma 4.2. Consider the definition of ωˆ given in (4.58). For any positive number N > 0, the
following inequality holds:
− W˜ T
V ωˆωˆ TW˜V
(1+ωˆ Tωˆ)
2 ≤

1
N (1+ωˆ Tωˆ)
2

W˜ T
V
1
2
∇xφ(x)Dˆ(x)∇T
x φ(x)WV −∇xφ(x) ˆf(x)

×
1
2
∇xφ(x)Dˆ(x)∇T
x φ(x)WV −∇xφ(x) ˆf(x)
T
W˜V − 1
"
4(N +1) (1+ωˆ Tωˆ)
2
#
×
W˜ T
V ∇xφ(x)Dˆ(x)∇T
x φ(x)W˜V
W˜ T
V ∇xφ(x)Dˆ(x)∇T
x φ(x)W˜V
T .
(4.74)
Proof. Refer to (Sahoo et al., 2017b).
We will use the above results to show the local UB of the NN weight estimation errors in Lemma
4.3, using which the closed-loop system is shown to be bounded.
Lemma 4.3. Consider the nonlinear continuous-time system (4.1) along with the NN-based iden￾tifier (4.44) and the value function approximator (4.52) with an event-sampled state vector. Let
Assumptions 4.5 to 4.7 hold and the initial identifier and value function NN weights, WˆI(0) and
WˆV (0), respectively, be initialized with nonzero finite values. Suppose there exists a nonzero posi￾tive inter-sample time between two consecutive event sampling instants, δtk = tk+1 −tk > 0, and the
value function activation function φ(x) and its gradient ∇xφ(x) satisfy the persistency of excitation
(PE) condition. Then, the weight estimation errors W˜I and W˜V are locally UB for all event sampling
instant tk > T or t ¯ > T for T > T provided that the NN weights are tuned by using ¯ (4.46) and (4.47),
and the learning gains satisfy 0 < αI < 1
3 , 0 < αV < min{ 1
40 , 2N(1−N)
2(N+1)(16+N)} with 0 < N < 1.
Proof. Refer to (Sahoo et al., 2017b).
Theorem 4.3. Consider the nonlinear continuous-time system (4.1), identifier (4.44), and the value
function approximator (4.52) represented as an impulsive dynamical system (4.67) and (4.71). Let
u0 be an initial stabilizing control policy for (4.1). Let Assumptions 4.5 to 4.7 hold, and assume there
exists a minimum inter-sample time mink{δtk} > 0. Suppose the value function activation function
φ(x) and its gradient ∇xφ(x) satisfy the PE condition, and the system states are transmitted at
the violation of the event sampling condition (4.72). Let the initial identifier and value function NN
initial weights, WˆI(0) and WˆV (0), are nonzero and finite and updated according to (4.57) and (4.59).
Then, the closed-loop impulsive dynamical system is locally UB for all event sampling instant tk > T¯
or t > T for T > T . Furthermore, the estimated value function satisfies ¯ 
V∗ −Vˆ

 ≤ BV , where BV
is a small positive constant provided that the design parameters are selected as in Lemma 4.3.
Proof. Refer to (Sahoo et al., 2017b).
The assumption on the inter-sample time in Theorem 4.3 is relaxed by guaranteeing the existence
of a positive minimum inter-sample time in the next section.Nonlinear Continuous-time Systems 153
4.3.6 MINIMUM INTERSAMPLE TIME
The following theorem guarantees the existence of the nonzero positive minimum inter-sample time
mink∈N{δtk} = mink∈N {tk+1 −tk}.
Theorem 4.4. Consider the continuous-time system (4.1) represented as impulsive dynamical sys￾tems (4.67) and (4.71) along with the event sampling condition (4.72). Then, the minimum inter￾sample time minkN{δtk} implicitly defined by (4.72) is lower bounded by a nonzero positive constant
and given by
δtmin = min
kN {δtk} ≥ min
k∈N
 1
K
ln
1+
K
Mk
σETC,min5 > 0 (4.75)
where σETC,min is the minimum threshold coefficient value, K > 0 is a constant, and
Mk = gMλmax 
R−1
σI,Mφ
M

WˆI,k


WˆV,k

+
1
2
gMλmax 
R−1
φ
MWV,M

σI,M

W˜I,k

+ε¯I

+
1
2
gMλmax 
R−1
σI,Mφ
M

WˆI,k


W˜V,k

+
1
2
g2
Mλmax 
R−1
ε
V,M,
where the subscript k represents the k th inter-sample time.
Proof. For the proof, refer to (Sahoo et al., 2017b).
Remark 4.9. The constant K satisfies the inequality  f(x) +g(x)u∗ ≤ Kx. This inequality holds
since the ideal optimal control input is stabilizing.
Remark 4.10. It is clear from (4.75) that the lower bound on inter-sample times depends on Mk or,
alternatively, on the NN weight estimation errors W˜V , and W˜I by the definitions WˆI = W −W˜I and
WˆV = W −W˜V . During the initial learning phase of the NN, the term Mk in (4.75) may become large
for certain initial values WˆV (0) and WˆI(0), which may lead to shorter nonzero inter-sample times.
Hence, proper initialization of the NN weights is necessary to keep the inter-sample time away from
zero during the learning phase. In addition, with an update in NN weights, the convergence of the
NN weight estimation errors will elongate the inter-sample times, leading to fewer sampled events
and less resource utilization.
We will use the example of a two-link robot manipulator for implementing the optimal control
design to investigate its effectiveness via simulation.
Example 4.3. Consider the two-link robot dynamics given in Example 4.2 and implement the event￾based optimal control design presented in the previous sections.
The following simulation parameters were chosen for simulation. The initial system state vector
was chosen as  π/6 −π/600 T
. The cost function was selected as in (4.37) with Q = I4×4
and R = I2×2. The learning parameters are chosen as αV = 0.005 and αI = 0.055, and other design
parameters were gM = 3,gm = 1, Γ = 0.99,A = −10I, and P = I, where I is the identify matrix
satisfying the intervals as in Lemma 1. The basis function for approximating the value function is
given by the expansion of φ(x) = 
x2
1,x2
2, x2
3, x2
4,x1x2,...,x4
1,x4
2,...,x3
1x2,...,x2
1x2x3,...,x1x2x3x4

.
The activation functions for the identifier σI(·) = diag{tanh(·)tanh(·)}. A number of hidden layer
neurons for identifier and value function NN are selected as 25 and 39, respectively. All the NN
weight estimates are initialized at random from a uniform distribution in the interval (0,1). The
ultimate bound for the system state is chosen as 0.001.
The performance of the event-sampled control system is shown in Fig. 4.14. The system state is
regulated close to zero, as shown in Fig. 4.14(a), along with the control input in Fig. 4.14(b). The
HJB equation error converges close to zero [shown in Fig. 4.14(c)], confirming that a near-optimal154 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Figure 4.14 Convergence of (a) system state, (b) control input, and (c) HJB error.
Figure 4.15 NN performance. (a) Approximation error ˜f . (b) Approximation error (g˜).Nonlinear Continuous-time Systems 155
Table 4.3
Event-triggered optimal control of nonlinear system
System dynamics ˙x = f(x) +g(x)u, x(0) = x0
Identifier dynamics ˙
xˆ = A(xˆ−x˘) +Wˆ T
I (t)σI(x˘)u¯, tk < t ≤ tk+1
Value function V(t) = * ∞
t r(x(τ),u(τ))dτ
Identifier NN weight update ˙
WˆI = 0, tk < t ≤ tk+1
Wˆ +
I = WˆI + αIσI(x)ue¯ T
I
(c+u¯2eI2
) −αIWˆI, t = tk
Event-triggering error es(t) = x(t)−x˘(t),tk < t ≤ tk+1 ∀k = 1,2,...
Value function approximation Vˆ(x˘) = Wˆ T
V φ(x˘), tk < t ≤ tk+1
Optimal Control input u = −1
2R−1 
Wˆ T
g σg(x˘)
T ∇xφ T (x˘)WˆV , tk < t ≤ tk+1
Critic NN weight update law ˙
WˆV = 0, tk < t ≤ tk+1
Wˆ +
V = WˆV −
"
αVωˆ Hˆ +T 
x,WˆV

/

1+ωˆ Tωˆ
2
#
, t = tk
Event-triggering condition D(es) > σETC 
x,

WˆV

,

WˆI



σETC 
x,

WˆV

,

WˆI



= min
ΓQ(x)
η1 ,

2ΓQ(x)
η2
5
control policy is achieved with an event-sampled implementation. This further implies that the value
function is approximated satisfactorily with the event-sampled NN. The approximation error for the
system dynamics is shown in Fig. 4.15, which appears to be bounded.
The event sampling threshold’s evolution and the event sampling error are shown in Fig. 4.16(a).
Cumulative number of event-sampled instants is shown in Fig. 4.16(b), and the inter-sample times
are shown in Fig. 4.16(c). From the cumulative number of event-sampled instances, it is evident
that fewer sampled instances occurred when compared with the traditional periodic sampled data
system. The number of event-sampled instants is found to be 15783 for a simulation duration of
50 s with a sampling interval of 0.001 s or 50000 sampling instants for the traditional sampled data
system.
Furthermore, it is clear from Fig. 4.16(b) that the event sampling condition generated a large
number of sampled instants at the initial NN online learning phase. This is due to the large weight
estimation error and makes the NN learn the unknown system dynamics and the value function to
achieve near optimality. Over time, as the NN approximates the system dynamics and value function,
the inter-sample times increase, thereby reducing the number of sampled events. The convergence
of all the NN weight estimates is shown in Fig. 4.17 (a)-(c).156 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Figure 4.16 Evolution of (a) event sampling condition, (b) cumulative number of event-sampled instants,
and (c) inter-sample times.
Figure 4.17 Convergence of the norm of the NN weight estimates. (a) 
Wˆ f

; (b) Wˆ g and (c) Wˆ vNonlinear Continuous-time Systems 157
4.4 OPTIMAL EVENT-BASED TRACKING CONTROL
Consider an affine nonlinear continuous-time system given in (4.1) The control objective is to track
a feasible reference trajectory xd(t) ∈ Rn generated by a reference system represented by
x˙d(t) = ζ (xd(t)), xd(0) = xd0 (4.76)
where xd(t) ∈ Rn is the reference state, and ζ : Rn → Rn is the internal dynamics with ζ (0) = 0.
Assumption 4.8. The functions f(x) and g(x) are Lipschitz continuous for all x ∈ Ωx where Ωx
is a compact set containing the origin. Further, the function g(x) has a full column rank for all
x ∈ Ωx and satisfies g(x) ≤ gM for some constant gM > 0. In addition, g(xd)g+(xd) = I where
g+ = (gT g)−1gT .
Assumption 4.9. The feasible reference trajectory xd(t) ∈ Ωxd , where Ωxd is a compact set, is
bounded such that xd(t) ≤ bxd where bxd > 0 is a constant.
Define the error between the system state and the reference state as tracking error, given by
er(t)  x(t)−xd(t). Then, the tracking error system, utilizing (4.1) and (4.76), can be defined by
e˙r = x˙−x˙d = f(er +xd) +g(er +xd)u−ζ (xd). (4.77)
The steady-state feed-forward control policy for the reference trajectory Kamalapurkar et al. (2015)
can be expressed as
ud = g+(xd)(ζ (xd))− f(xd) (4.78)
where ud : Rn → Rm is the steady state control policy corresponding to the reference trajectory. By
augmenting the tracking error er and desired trajectory xd, the dynamics of the augmented tracking
error system can be represented as
χ˙ = F(χ) +G(χ)w (4.79)
where χ  [eT
r xT
d ]
T ∈ R2n is the augmented state with χ(0)=[eT
r (0) xT
d (0)]T = χ0, F : R2n →
R2n is given by F(χ) 

f(er +xd) +g(er +xd)ud −ζ (xd)
ζ (xd)

, G : R2n → R2n×m given by G(χ) 

g(er +xd)
0

, and the mismatched control policy w  u−ud ∈ Rm. It is routine to check that F(0) =
0.
The infinite horizon performance index with state constraint enforced by the dynamical system
in (4.79) can be defined as
J(χ,w) =  ∞
0
[χT (τ)Q¯χ(τ) +w(τ)
TRw(τ)]dτ (4.80)
where Q¯ 
 Q 0n×n
0n×n 0n×n

∈ R2n×2n with Q ∈ Rn×n and R ∈ Rm×m are symmetric positive definite
matrices. The matrix 0n×n is a matrix with all elements zero. Note that the performance index
is defined using the mismatched policy w in Kamalapurkar et al. (2015) and, therefore, the cost
functional is finite for any admissible control policy w ∈ Ωw where Ωw is the set of all admissible
policies Lewis et al. (2012b).
Observe that the augmented system dynamics in (4.79) and the performance index (4.80) are in a
similar form as in (4.1) and (4.37). Therefore, we can use Table 4.3,mutatis mutandis, for designing
the optimal event-based tracking controller for the nonlinear system (4.79).158 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
4.5 CONCLUDING REMARKS
This chapter presented an event-triggered stabilization of MIMO uncertain nonlinear continuous￾time systems. The control input was directly approximated by using an NN in the context of event￾based transmission. A novel event-triggering condition was developed based on the system state
vector and NN weight estimate to ensure the reduction in transmission of the feedback control
signal. The weights were updated in a non-periodic manner at the triggering instants. The controller
design guaranteed the desired performance while relaxing the need for system dynamics. Lyapunov
analysis confirmed the closed-loop stability. Simulation results confirmed the validity of the control
design and reduction in resource utilization.
The second part of the chapter presented an event-sampled near-optimal control of an uncertain
continuous-time system. A near-optimal solution of the HJB equation was achieved with event sam￾pled approximation of the value function and system dynamics. The NN weight tuning at the event￾sampled instants with adaptive event sampling conditions was found to ensure the convergence of
the NN weight estimates to their respective target values. It was observed that the inter-sample times
depend upon the initial values of the NN weight estimates. Furthermore, the inter-sample times were
found to increase with the convergence of the parameters. The simulation results validated the ana￾lytical design. The cost function considered in this paper only optimizes the control policy. Finally,
a framework for extending the optimal regulator design for event-based trajectory tracking control
is presented.
4.6 PROBLEMS
4.6.1 Consider an inverted pendulum (Spooner and Passino, 1999), which can be represented as an
input-affine nonlinear dynamical system. The dynamics of the system are given by
x˙1 = x2,
x˙2 = (mgr
J − kr2
4 J)sinx1 +
kr
2J
(l −b) + u
J
. (4.81)
Design feedback control policies based on the control algorithm using equations in Table 4.1.
4.6.2 Design near-optimal control policies based on the learning algorithm using equations in Table
4.3 for the Problem 4.6.1.
4.6.3 For the same system as in Problem 4.6.1., consider the linearized dynamics as in
(Guinaldo et al., 2011). The parameters in the system dynamics are m1= 2, m2= 2.5, J1= 5,
J2= 6.25,k = 10,r = 0.5,l = 0.5, and g = 9.8,b = 0.5. Pick the initial conditions of the sys￾tem states in the interval [0,1] and the initial weights of the NN from [−1,1], randomly.
Design control policies based on the algorithms using equations in Table 4.1.
4.6.4 For the linearized system used in Problem 4.6.3., select the parameters in the system dynamics
as m1= 20, m2= 25, J1= 5, J2= 6.25, k = 10,r = 0.5,l = 0.5, and g = 9.8,b = 0.5. Pick the
initial conditions of the system states in the interval [0,1] and the initial weights of the NN
from [−1,1], randomly. Design control policies based on the algorithms using equations in
Table 4.3.
4.6.5 Design feedback control policies based on the learning algorithm using equations in Table 4.3
for the Example 4.1. Use RVFL network with sigmoid activation function.
4.6.6 Design feedback control policies based on the learning algorithm using equations in Table 4.1
for the Example 4.2. Choose the event-triggering condition with Γs ∈ (0,1) and comment on
the performance of the event-triggering mechanism.
4.6.7 Design feedback control policies based on the learning algorithm using equations in Table
4.3 for the Example 4.2. Choose Q = 0.4I and vary the parameter values for R. The penalty
on control torques and the events generated are interdependent. Comment on the number of
events generated versus the control penalty function.Nonlinear Continuous-time Systems 159
4.6.8 Design feedback control policies based on the learning algorithm using equations in Table 4.3
for the Example 4.2. What if the regulation task is replaced with tracking problem, where the
goal is to track a sinusoidal reference trajectory?
4.6.9 Consider the nonlinear system represented by
x˙1 = −x1 +x2
x˙2 = −1
2 (x1 +x2) + 1
2
x2 sin2 (x1)+(sin(x1))u (4.82)
Using the controller equations in Table 4.1 to design an event-based feedback control policy.
4.6.10 For the system given in the previous problem, design a learning-based controller us￾ing equations given in Table 4.3. Select the infinite horizon cost function as Vu(x(t)) =
* ∞
t

Q(x) +u2

dτ with Q(x) = x2
1 +x2
2 to be minimized.5 Co-optimization of
Sampling and Control
using Differential Games
CONTENTS
5.1 Linear Systems ...................................................................................................................... 161
5.1.1 Problem Statement.................................................................................................... 162
5.1.2 Co-optimization Scheme .......................................................................................... 163
5.1.3 Self-triggered Implementation.................................................................................. 167
5.2 Nonlinear Systems................................................................................................................. 173
5.2.1 Problem Statement.................................................................................................... 173
5.2.2 Co-optimization Methodology.................................................................................. 174
5.2.3 Neural Network Controller Design........................................................................... 177
5.3 Co-optimized Tracking Control of Nonlinear Systems......................................................... 181
5.3.1 Background and Problem Definition ........................................................................ 182
5.3.2 Event-based Tracking Problem................................................................................. 184
5.3.3 Performance Index and Solution to the Min-max Optimization ............................. 184
5.3.4 Optimal Event-sampling Condition and Stability Analysis...................................... 186
5.3.5 Zeno-free Behavior of System ................................................................................. 187
5.3.6 Approximate Solution for the Min-max Optimization Problem............................... 188
5.3.7 Adaptive Event-sampling Condition and Stability ................................................... 190
5.4 Concluding Remarks ............................................................................................................. 195
5.5 Problems................................................................................................................................ 196
In Chapters 3 and 4, we covered ADP-based controllers for linear and nonlinear systems in the
event-triggered control framework. In both these chapters, we have seen that the event-triggering
conditions can be derived using Lyapunov stability analysis. By contrast, in this chapter, we shall
see that the design of event-triggering condition can be coupled with the control synthesis by uti￾lizing game-theoretic techniqes. To this end, this chapter presents an event and a self-triggered
sampling and regulation scheme for linear and nonlinear continuous-time dynamic systems, as well
as tracking control problems. We shall develop these schemes based on a zero-sum game formu￾lation, wherein the control policy is treated as the first player, and the threshold for control input
error due to aperiodic dynamic feedback is treated as the second player. The optimal control policy
and sampling intervals are generated using the saddle point or Nash equilibrium solution, which is
obtained from the corresponding game algebraic Riccati equation. The event- and self-triggering
control schemes presented in this chapter are based on Sahoo et al. (2018, 2017a) and the second
part of the chapter dealing with nonlinear systems is based on Narayanan et al. (2018b) with the last
part on tracking control is based on Sahoo and Narayanan (2019).
161162 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
5.1 LINEAR SYSTEMS
In this section, a co-optimization approach for both the control policy and the sampling intervals,
taken from (Sahoo et al., 2018), is presented. Unlike the emulation-based methods in previous chap￾ters, which involved designing a control policy to guide the system with continuous feedback and
later creating an event-triggering condition based on stability conditions, here, the control policy
and event-triggering condition are concurrently designed. The optimal event-based sampling and
control problem is formulated as a two-player zero-sum game where the control policy is treated
as the first player, and the threshold for the error between continuous and event-triggered control
policy due to the aperiodic sampling is viewed as the second player. A performance index is defined
using the system state and the players. Player I’s objective is to minimize the cost function, while
Player II attempts to maximize it. The problem leads to a min-max problem (Basar and Bernhard,
2008) where the objective is to reach a saddle point solution or Nash equilibrium (Basar and Bern￾hard, 2008). The Nash equilibrium solution is obtained using the game-theoretic algebraic Riccati
equation (GARE) (Basar and Bernhard, 2008). The event-based optimal control policy is designed
using the solution of GARE and the optimal sampling condition is derived using the worst-case
threshold to maximize the sampling periods.
To eliminate the requirement of the additional piece of hardware for triggering mechanisms in the
event-based implementation, a self-triggered control scheme is also presented. A weaker triggering
condition, derived from the event-based implementation, is utilized to determine the future sampling
instants for the self-triggered design. Thus, the self-triggered sampling condition leads to a subop￾timal solution. We shall see that these controllers lead to asymptotic stability of the closed-loop
system and the Zeno-free behavior of the triggering mechanism is also guaranteed.
The next section briefly describes the background on event-based control and formulates the
problem for linear systems.
5.1.1 PROBLEM STATEMENT
Consider a linear continuous-time system given by
x˙(t) = Ax(t) +Bu(t), x(0) = x0, (5.1)
where x(t) ∈ Rn and u(t) ∈ Rm denote the system state and control input vector, respectively. The
matrices A ∈ Rn×n and B ∈ Rn×m are the internal dynamics and input coefficient matrices, respec￾tively.
Assumption 5.1. The pair (A,B) is controllable, and the state vector x(t) is measurable.
Consider a traditional infinite horizon performance index for the system in (5.1) given by
J(x(0)) =  ∞
0
(xT (τ)Hx(τ) +uT (τ)Ru(τ))dτ, (5.2)
where H ∈ Rn×n and R ∈ Rm×m are symmetric positive definite matrices, and u(t) = μ(x(t)), where
μ : Rn → Rm is an admissible control policy. The objective is to minimize the performance index J
by designing an optimal control policy u∗(t) = μ∗(x(t)). The solution of the optimal control problem
can be obtained by solving the algebraic Riccati equation (ARE) (Lewis et al., 2012b). The inherent
assumption of the execution of the optimal control policy is the continuous availability of the system
state vector.
As discussed in Chapter 1, in the event-triggered and self-triggered control formalism, the system
state vector is sampled, and the controller is executed aperiodically by using a sampling condition.
Define the aperiodic sampling instants as {tk}∞
k=0 with t0 = 0. The sampled state sent to the controller
at the aperiodic sampling instants can be expressed as
xˆ(t) = x(tk), tk ≤ t < tk+1 (5.3)Co-optimization of Sampling and Control using Differential Games 163
where ˆx(t) ∈ Rn is the sampled state at the controller for controller execution. The resulting state
measurement error can be expressed as
es,x(t) = xˆ(t)−x(t) = x(tk)−x(t), tk ≤ t < tk+1 (5.4)
where es,x(t) ∈ Rn is the state measurement error. Now, the control input with the sampled state ˆx(t)
is expressed as ˆu(t) = μ(xˆ(t)), which is held at the actuator by a zero-order-hold (ZOH) and applied
to the system till the next update is reached. The system in (5.1) with sampled state based input ˆu(t)
can be expressed as
x˙(t) = Ax(t) +Buˆ(t). (5.5)
Note that the sampled control input ˆu(t) = μ(xˆ(t)) ∈ Rm is a piecewise constant input signal due
to ZOH. Define the error between the continuous control input, u(t) = μ(x(t)), and the sampled
control input, ˆu(t), as
es,u(t) = uˆ(t)−u(t), (5.6)
where es,u(t) ∈ Rm, the control input error due to the aperiodic feedback, is a piecewise continuous
function. The sampled system (5.5) with (5.6) can be expressed as
x˙(t) = Ax(t) +Bu(t) +Bes,u(t). (5.7)
The aperiodic sampling and controller execution introduced an error term, Bes,u(t), in the system
dynamics (5.1), which can be considered as an exogenous input to the system. It is clear that larger
the magnitude of es,u(t), larger the event-triggering error is allowed to grow, resulting in longer
inter-event time.
To obtain an optimal threshold for es,u(t) such that the sampling intervals can be maximized, we
introduce a dynamical system
x˙(t) = Ax(t) +Bu(t) +Beˆs,u(t), (5.8)
where ˆes,u is an exogenous independent signal, whose optimal value will be used as a threshold for
es,u in (5.7). Therefore, the objective is to, first, design the optimal control policy u∗ for the system in
(5.8) in the presence of the worst case exogenous signal e∗
s,u (worst case value for ˆes,u) such that the
system in (5.8) mimics the optimal target system, ˙x(t) = Ax(t) +Bu∗(t) +Be∗
s,u(t). Second, design
the sampling instants and the control policy for the system in (5.7) such that the resulting closed￾loop system emulates the target system. Therefore, the traditional performance index in (5.2) for
the system in (5.8) must be redefined to include the additional term ˆes,u, which can be maximized.
This clearly leads to a zero-sum min-max game formulation. In the next section, a solution to the
dynamic game is presented.
5.1.2 CO-OPTIMIZATION SCHEME
Redefine the infinite horizon performance index (5.2) for an event-based control system (5.8) as
J(x(0),u,eˆs,u) = 1
2
 ∞
0

xT (τ)Hx(τ) +u(τ)
TRu(τ)−γ2eˆs,u(τ)
T eˆs,u(τ)

dτ, (5.9)
where H ∈ Rn×n and R ∈ Rm×m are constant symmetric positive definite user-defined matrices and
γ > γ∗, with γ∗ denoting the minimum value of γ, is a constant such that the performance index in
(5.9) is finite.
Remark 5.1. In the traditional H∞ optimal control (Basar and Bernhard, 2008; Wei and Guo, 2010)
approach, solution of the optimal control problem leads to a saddle point optimal control input u∗
and worst case exogenous signal e∗
s,u (Basar and Bernhard, 2008). In the event-triggered formalism,164 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
the control input error es,u is a function of u, unlike the independent disturbance in H∞ optimal
control (Basar and Bernhard, 2008; Wei and Guo, 2010). However, the event-triggering threshold
is an independent time-dependent signal. In the co-design scheme, the performance index (5.8)
is defined using an independent continuous exogenous signal, eˆs,u, based on (5.8). Minimization
of (5.9) will lead to the optimal exogenous signal, e∗
s,u, which is utilized as the threshold for the
triggering mechanism (defined in (5.20)).
To design optimal control policy, u∗ and the sampling instants {tk}∞
k=0, which will maximize the
sampling intervals, δtk = tk+1 −tk, a saddle point solution to the min-max optimization problem
(Basar and Bernhard, 2008), defined by the cost function (5.9) with dynamic state constraint (5.8),
needs to be obtained. This means, we should find the optimal value function given by
V∗(x(t)) = min
u max
eˆs,u
J(x,u,eˆs,u)
= min
u max
eˆs,u
 ∞
t
1
2
(xT (τ)Hx(τ) +uT (τ)Ru(τ)−γ2eˆ
T
s,u(τ)eˆs,u(τ))dτ,
(5.10)
such that, given the pair (u∗,e∗
s,u), the saddle point solution is reached, i.e., minu max
eˆs,u
J(x,u,eˆs,u) =
max
eˆs,u
min
u J(x,u,eˆs,u). To solve the optimal control problem, begin by defining the Hamiltonian
for the optimal value function (5.10) along the system dynamics (5.8). The Hamiltonian can be
expressed as
H (x,u,eˆs,u,
∂V∗
∂ x ) = 1
2
xTHx+
1
2
uTRu− 1
2
γ2eˆ
T
s,ueˆs,u +
∂V∗T
∂ x (Ax+Bu+Beˆs,u), (5.11)
where ∂V∗T
∂ x = ∂V∗T (x(t))
∂ x(t) .
The optimal control input can be computed by using the stationarity condition ( ∂H (x,u,eˆs,u, ∂V∗
∂ x )
∂u =
0) and given by
u∗ = μ∗(x) = arg minu H (·) = −R−1BT ∂V∗
∂ x , (5.12)
and the threshold policy, with ∂H (x,u,eˆs,u, ∂V∗
∂ x )
∂ eˆs,u = 0, is given by
e∗
s,u = arg max eˆs,u
H (·) = 1
γ2 BT ∂V∗
∂ x . (5.13)
The optimal value for linear systems with quadratic performance index (5.9) can be represented as
V∗ = 1
2 xTPx, where P ∈ Rn×n is a symmetric positive definite kernel matrix of the GARE (to be
derived in (5.17)). Then, the optimal control input (5.12) can be expressed as
u∗(t) = −R−1BTPx(t), (5.14)
whereas the threshold policy (5.13) is given by
e∗
s,u(t) = 1
γ2 BTPx(t). (5.15)
By substituting the optimal control input (5.14) and the worst-case control-input input error (5.15)
or the threshold policy, the Hamilton-Jacobi-Isaacs (HJI) equation can be expressed as
H (x,u∗, e∗
s,u,
∂V∗
∂ x ) = −1
2
xTHx− 1
2
u∗TRu∗ +
1
2
γ2e∗T
s,ue∗
s,u +
∂V∗T
∂ x (Ax+Bu∗ +Be∗
s,u).Co-optimization of Sampling and Control using Differential Games 165
Rearranging the equation reveals
H (x,u∗,e∗
s,u,
∂V∗
∂ x ) = 1
2
xT (ATP+PA+H −PBR−1BTP+
1
γ2 PBBTP)x, (5.16)
where the GARE is given by
ATP+PA+H −PBR−1BTP+
1
γ2 PBBTP = 0. (5.17)
The next theorem states the existence of a unique solution of the GARE in the context of aperi￾odic sampling.
Theorem 5.1. (Existence of the solution of GARE) Consider the linear system (5.1). Let As￾sumption 5.1 holds. Then, there exists a unique symmetric positive definite kernel matrix P sat￾isfying the GARE (5.17) provided the condition R−1 > 1
γ2 I is satisfied. Further, the Hamiltonian
H (x,u∗,e∗
s,u, ∂V∗
∂ x ) = 0.
Proof. The GARE (5.17) can be rewritten as
ATP+PA+H −PB(R−1 − 1
γ2 I)BTP = 0. (5.18)
By selecting γ properly one can find a positive definite matrix R¯ such that R−1 −(1/γ2)I = R¯−1.
Substituting R¯−1, the GARE (5.18) becomes an algebraic Riccati equation (ARE) (Lewis et al.,
2012b). Since, the pair (A,
√H) is detectable, the matrix P is unique symmetric and positive definite
solution of (5.17) (Lewis et al., 2012b). Since P satisfies the GARE in (5.17), the Hamiltonian
H (x,u∗,e∗
s,u, ∂V∗
∂ x ) = 0.
The sampled optimal control input, using the solution of the GARE, with sampled state (5.3) can
be expressed as
uˆ
∗(t) = −R−1BTPxˆ(t). (5.19)
Next, define the event-triggering condition for the system in (5.7) to determine sampling instants
using the threshold policy in (5.15) as
eT
s,u(t)es,u(t) ≤
1
γ4 xT (t)PBBTPx(t), ∀t ∈ R≥0. (5.20)
The triggering-mechanism evaluates the condition (5.20) and samples the system state vector when
the inequality is violated. The following theorem guarantees the asymptotic stability of the sampled
system.
Theorem 5.2. (Asymptotic stability of optimal event-triggered system) Consider the linear sys￾tem (5.1) represented as event-sampled system in (5.5). Let Assumption 5.1 holds, γ satisfies
γ2 > λmax(R) and the symmetric positive definite matrix P satisfies (5.17). Then, the sampled sys￾tem (5.5) with control policy (5.19) is asymptomatically stable for any initial state x0 ∈ Rn if the
inequality in (5.20) holds.
Proof.  Let L : Rn → R≥0 be a continuously differentiable positive definite Lyapunov can￾didate function given by
L(x(t)) = 1
2
xT (t)Px(t) = V∗(x),
where V∗(x) is the optimal value function and P is a symmetric positive definite matrix that
satisfies GARE (5.17).166 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
 The first derivative of L(x) along the system dynamics (5.5) is given by
L˙(t) = ∂V∗T
∂ x x˙(t) = ∂V∗T
∂ x (Ax+Bus) = ∂V∗T
∂ x (Ax+Bu+Bes,u).
 Replacing with the optimal value of control input u∗ from (5.14), the first difference leads
to
L˙(t) = ∂V∗T
∂ x [(Ax+Bu∗ +Be∗
s,u)−B(e∗
s,u −es,u)] (5.21)
 Given u∗ and e∗
s,u, the HJI equation in (5.16) satisfies H (x,u∗,e∗
s,u, ∂V∗
∂ x ) = 0. Then, the HJI
equation can be rearranged as
∂V∗T
∂ x (Ax+Bu∗ +Be∗
s,u) = −1
2
xTHx− 1
2
u∗TRu∗ +
1
2
γ2e∗T
s,ue∗
s,u.
 Further, from (5.13) we have γ2e∗
s,u = BT ∂V∗
∂ x . Utilizing these facts and inserting in (5.21),
the first derivative
L˙(t) = −1
2
xTHx− 1
2
u∗TRu∗ +
1
2
γ2e∗T
s,ue∗
s,u −γ2e∗T
s,ue∗
s,u +γ2e∗T
s,ues,u. (5.22)
 Applying Young’s inequality the first derivative is further simplified as
L˙(t) ≤ −
1
2
xTHx− 1
2
u∗TRu∗ +
1
2
γ2e∗T
s,ue∗
s,u −γ2e∗T
s,ue∗
s,u +
1
2
γ2e∗T
s,ue∗
s,u +
1
2
γ2eT
s,ues,u. (5.23)
 Recalling the triggering condition (5.20), the condition can be rewritten as eT
s,ues,u ≤ e∗T
s,ue∗
s,u.
Substituting this triggering condition in (5.23), the first derivative is upper bounded as
L˙ ≤ −
1
2
xTHx− 1
2
u∗TRu∗ +
1
2
γ2e∗T
s,ue∗
s,u. (5.24)
 Inserting u∗ from (5.14) and e∗
u in (5.15), the first derivative of the Lyapunov function can
be simplified as
L˙(t) ≤ −
1
2
xTHx− 1
2
xTPBR−1BTPx+
1
2
γ2( 1
γ4 xTPBBTPx) = −1
2
xT Mx, (5.25)
where M = H + PBR−1BTP − 1
γ2 PBBTP is a positive definite matrix since H is positive
definite and by selecting γ such that γ2 > λmax(R), the matrix R−1− 1
γ2 I will also be positive
definite.
 From (5.25), the Lyapunov first derivative L˙(x) is negative definite. By the Lyapunov sta￾bility theorem, the system states x(t) → 0 as t → ∞.
Remark 5.2. Note that the set of sampling instants is determined using the equality condition in
(5.20), i.e., {tk}∞
k=0 = {t ∈ R≥0|eT
s,ues,u = 1
γ4 xTPBBTPx}. Since the threshold for the sampled policy
error is selected as its worst-case value, the sampling intervals δtk = tk+1 −tk are maximized with
respect to the performance index (5.9)
The optimal event-triggering condition (5.20) is a function of the control input error and the state
vector of the original system. The following corollary presents a traditional triggering condition
(Tabuada, 2007) using the state error (5.4) and the sampled state vector (5.3).Co-optimization of Sampling and Control using Differential Games 167
Corollary 5.1. Let the hypothesis of Theorem 5.2 hold. Then, the sampled system (5.5) with control
policy (5.19) is asymptomatically stable for any initial state x0 ∈ Rn if the inequality
eT
s,xΠes,x ≤
1
2
xˆ
TΩxˆ,∀t ∈ [tk,tk+1), k ∈ N, (5.26)
holds, where Π = ψπ I + Ω + KTK ∈ Rn×n with Ω = 1
γ4 PBBTP ∈ Rn×n, K = R−1BTP ∈ Rm×n,
ψπ > 0 is a constant and I is the identity matrix with appropriate dimension.
Proof.  To complete the proof, it suffices to show that given the inequality (5.26) the sam￾pling condition (5.20) holds. By the definition of Ω in (5.26), the inequality (5.20) can be
rewritten as eT
s,ues,u ≤ xTΩx. Further, the control input error
es,u = uˆ−u = Kxˆ−Kx = K(xˆ−x) = Kes,x.
 Expansion of the expression in (5.26) leads to
eT
s,xΠes,x = ψπ eT
s,xes,x +eT
s,xΩes,x +eT
s,xKTKes,x
= ψπ eT
s,xes,x +eT
s,xΩes,x +eT
s,ues,u ≤
1
2
xˆ
TΩxˆ. (5.27)
 Rearrangement of the equation (5.27) reveals that
eT
s,ues,u ≤
1
2
xˆ
TΩxˆ−eT
s,xΩes,x −ψπ eT
s,xes,x.
 Adding and subtracting similar terms and rearranging the equation again, we get
eT
s,ues,u ≤ xˆ
T
s Ωxˆs +eT
s,xΩes,x −2 ˆxT
s Ωes,x −
1
2
xˆ
TΩxˆ+2eT
s,xΩes,x −2 ˆxTΩes,x

−ψπ eT
s,xes,x.
 By completion of the square, we have
eT
s,ues,u ≤ (xˆ−es,x)
TΩ(xˆ−es,x)−( 1
√
2
xˆ− √
2es,x)
TΩ( 1
√
2
xˆ− √
2es,x)−ψπ eT
s,xes,x
≤ xTΩx. (5.28)
 From (5.28) it is clear that given (5.26) the inequality (5.20) holds. Consequently, by The￾orem 5.2, the event-sampled system (5.5) is asymptotically stable.
Remark 5.3. The condition in (5.26) uses the sampled state vector x and can also be used as an ˆ
event-triggering condition. The sampling instants can be determined by the violation of the inequal￾ity. However, the condition in (5.26) is a weaker condition and results in sub-optimal triggering.
The main advantage of the inequality in (5.26) is that it only uses sampled signals and can be used
to predict the next sampling instants at the controller using the current sampled state without any
triggering mechanism as presented in the next section.
5.1.3 SELF-TRIGGERED IMPLEMENTATION
In this section, we shall extend the event-based design to the self-triggered case by deriving a sam￾pling condition with current states to determine the sampling instants.168 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
The inequality in (5.26) can be equivalently written as

√
Πes,x ≤ 1
√
2

√
Ωxˆ, ∀t ∈ [tk,tk+1), k ∈ N, (5.29)
where √
Π and √
Ω are the matrix square roots of the matrices Π and Ω, respectively. By definition,
the inverse of Π and, hence, √
Π exists by properly selecting the value of ψπ .
The most important condition for developing the self-triggered scheme is that the sampling peri￾ods must be bounded from below by a nonzero positive constant (Wang and Lemmon, 2009b). The
following lemma states the lower boundedness of the sampling period or the inter-event time.
Lemma 5.1. Consider the linear system (5.1) represented as sampled data system in (5.5) along
with the sampled optimal control input (5.19). Let γ satisfies γ2 > λmax(R), Π is invertible, and there
exists a symmetric positive definite matrix P satisfy (5.17). Then, the sampled system is asymptot￾ically stable if the sampling is carried out when the condition in (5.29) is violated. Further, the
inter-sample times δtk are lower bounded by a non-zero positive constant.
Proof.  From Corollary 5.1, the inequality in (5.26) or equivalently in (5.29) is weaker than
(5.20). Consequently, by Theorem 5.2, the sampled system is asymptotically stable.
 It only remains to show the lower boundedness inter-sample times. Since the inequality in
(5.29) is a weaker condition of (5.20), it suffices to show that the inter-sample times δtk
determined by (5.29) are lower bounded.
 Define an error ¯es,x = √
Πes,x. The inter-sampling period δtk =tk+1−tk is implicitly defined
by the equality in (5.29). The error e¯s,x(t) evolves from zero at time tk for k ∈ N to the
threshold value of √
1
2 
√
Ωxs at time tk+1.
 The time derivative of the measurement error e¯s,x becomes
d
dt e¯s,x(t)≤e˙¯s,x(t) = 
√
Π(˙
xˆ(t)−x˙(t)) = 
√
Πx˙(t) = 
√
Π(Ax+Buˆ)
= 
√
Π(A(xˆ−es,x) +BKxˆ)
≤ √
Π(A+BK)xˆ+
√
ΠAes,x≤√
ΠA
√
Π−1√
Πes,x+
√
Π(A+BK)xˆ
.
 Defining αs = 
√
ΠA
√
Π−1
 and βs = 
√
Π(A+BK), we have.
d
dt e¯s,x ≤ αse¯s,x+βsxˆ. (5.30)
 By comparison lemma (Khalil, 2002), the solution of the differential inequality in (5.30) is
upper bounded by
e¯s,x(t)≤e¯s,x(tk)eαs(t−tk) +
 t
tk
βsxˆ(t)eαs(t−τ)
dτ
= βsxˆ(t)
αs
"
eαs(t−tk) −1
#
. (5.31)
 With e¯s,x(tk) = 0,∀k ∈ N, at the next sampling instant tk+1,∀k ∈ N, the error
e¯s,x(tk+1) = √
1
2 
√
Ωxˆ.
 From (5.31) it holds that √
1
2 
√
Ωxˆ = e¯s,x(tk+1) ≤ βsxˆ
αs (eαs(tk+1−tk) −1), which leads to
βsxˆ
αs (eαs(tk+1−tk) −1) > √
1
2 
√
Ωxˆ.Co-optimization of Sampling and Control using Differential Games 169
 Solving for the δtk = tk+1 −tk reveals
δtk >
1
αs
ln
1+
αs √
2βsxˆ

√
Ωxˆ

. (5.32)
 From (5.32), the inter-sample duration δtk = tk+1 −tk > 0, ∀k ∈ N and, hence, the sampling
periods are bounded from below by a non-zero positive constant.
With the guarantee of lower boundedness of the sampling periods, the following theorem conso￾lildates the stability results for the sub-optimal self-triggered control scheme.
Theorem 5.3. Consider the linear continuous-time system (5.1) represented as a sampled data
system in (5.5). Let the Assumption 5.1 hold, γ satisfies γ2 > λmax(R), and the symmetric positive
definite matrix P satisfies (5.17). Then the self-triggered system (5.5) with optimal control policy
(5.19) is asymptomatically stable for any initial state x0 ∈ Rn if systems states are sampled, and the
controller is executed at the time-instants determined by the following inequality
tk+1 ≥ tk +
1
αs
ln
1+
αs √
2βsxˆ

√
Ωxˆ

. (5.33)
Further, the self-triggered system is Zeno-free.
Sketch of proof:
 The next sampling instant tk+1 for the self-triggered condition (5.33) uses the expression of
inter-sample time δtk in (5.32). Alternatively, enforcing the self-triggered sampling condi￾tion (5.33) ensures the event-triggering condition (5.29) is satisfied. Therefore, by Lemma
5.1, the self-triggered system is asymptotically stable.
 Further, from (5.32) of Lemma 5.1, δtk ≥ 1
αs
ln"
1+ √ αs
2βsxˆ 
√
Ωxˆ
#
> 0. This implies
tk+1 − tk > 0, i.e., the sampling periods are bounded from below by a non-zero positive
number, and the self-triggered system does not exhibit Zeno behavior.
Remark 5.4. The sampling intervals determined by the self-triggering scheme use a weaker con￾dition (5.29) when compared to the event-triggering condition (5.20). Therefore, the self-triggered
control results have a known degree of sub-optimality from the sampling period point of view.
Example 5.1. The unstable batch reactor example (Walsh et al., 2002; Heemels et al., 2010) is
considered for simulation whose dynamics are given by
x˙(t) =
⎡
⎣
1.38 −0.207 6.715 −5.676
−0.581 −4.29 0 0.675
1.067 4.273 −6.654 5.893
0.048 4.273 1.343 −2.104
⎤
⎦x(t) +
⎡
⎣
0 0
5.679 0
1.136 −3.146
1.136 0
⎤
⎦u(t).
The simulation parameters were selected as follows: initial state vector x0 = [1.8, −2.4, −4.6, 4]
T ,
the matrices for the cost function H = I4×4 and R = 0.05I2×2. The value of γ = 0.25 was selected
such that γ2 > λmax(R), as in Theorem 5.1. The simulation is run for 5 seconds and the results are
shown in Figure 5.1 through Figure 5.5.
Event-triggered control: The evolution of the system state vector and the control inputs are shown
in Figures 5.1 (a) and (b). The optimal event-sampled control input in Figure 5.1 (b) is a piecewise
constant function due to the event-based aperiodic execution of the control input. The control input
error es,u used as the event-triggering error is plotted in Figure 5.1 (c) along with the threshold.170 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Table 5.1
Co-optimization of sampling instants and control policy of linear systems
System dynamics ˙x(t) = Ax(t) +Bu(t), x(0) = x0
Event-triggering error es,u(t) = uˆ(t)−u(t),
Performance index J(x(0),u,eˆs,u) = 1
2
* ∞
0

xT (τ)Hx(τ) +u(τ)TRu(τ)−γ2eˆs,u(τ)T eˆs,u(τ)

dτ
Game Riccati equation ATP+PA+H −PBR−1BTP+ 1
γ2 PBBTP = 0
Control input ˆu∗ = −R−1BTPxˆ
Event-triggering condition eT
s,ues,u ≤ 1
γ4 xTPBBTPx,∀t
Self-triggering condition tk+1 ≥ tk + 1
αs
ln"
1+ √ αs
2βsxˆ 
√
Ωxˆ
#
012345
(a)
-5
0
5
States
x1
x2
x3
x4
012345 (b)
-100
-50
0
50
Control inputs
u1s
u2s
012345 (c)
Time (s)
0
1000
2000
3000
Trig. Condition
1
γ4xT PBBT P x eT
suesu
012345 (d)
Time (s)
0
50
100
150
Cumul. samples
Number of samples
0 0.05 0.1
0
1000
2000
3000
Figure 5.1 Evolution of (a) the state trajectory; (b) optimal event-based control policy; (c) control input error
and sampling threshold; and (d) cumulative number of sampling instants.
The plot is zoomed out for clarity of the evolution of the control input error, which resets at every
sampling instants. A total of 123 sampling instants occurred during the simulation time of 5 sec
for γ = 0.25, as shown in Fig. 5.1 (d). A minimum sampling time of 0.011s is observed, and the
aperiodic sampling instants during the simulation are plotted in Figure 5.2.
Next, the comparison results with the traditional scheme developed by Wang and Lemmon
(2009b) is presented. Note that a direct comparison is not possible due to the difference between
the approaches and performance requirements. Therefore, to have a fair comparison in terms of
the cost, same value of γ = 0.25 is used for both the approaches, and β = 0.76 was selected for
evaluating the triggering condition (5.20) developed by Wang and Lemmon (2009b) such that equal
numbers of triggering occurs for both the methods. This results in 122 events which is close to the
game-based method, i.e., 123. The optimal cost trajectory for both the approaches are computed as
V∗(x) = xTPx with the same P matrix, which is the solution of the GARE in (5.17). The cumulativeCo-optimization of Sampling and Control using Differential Games 171
Figure 5.2 Aperiodic inter-sample times with a minimum inter-sample time of 0.011 s for γ = 0.25.
Figure 5.3 Optimal cost surface and comparison of optimal cost trajectory between the game-based method
(Sahoo et al., 2018) and existing method (Wang and Lemmon, 2009b) (a) for state x1 and x2 with x3 = x4 = 0.;
and (b) for state x3 and x4 with x1 = x2 = 0.
Figure 5.4 Comparison of cumulative cost between game-based method presented in this section based on
(Sahoo et al., 2018) and the method presented by Wang and Lemmon (2009b).
optimal costs are computed by integrating the current values.
The optimal cost surface and the optimal cost trajectory for the game-based and a classical
method (Wang and Lemmon, 2009b) are plotted in Figure 5.3. Since the batch reactor is a fourth￾order system, only the first two states are considered at a time for plotting the costs, while the other172 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
two are made zero. The optimal cost in the game-based method uses the shortest path resulting in
lower cumulative cost, which, compared to the existing method (Wang and Lemmon, 2009b), is
shown in Figure 5.4.
Self-triggered control: The sampling instants are determined by the controller using the self￾triggering condition in (5.33) with ψπ = 0.001 and all other parameters are the same as in the case of
event-triggered control. The convergence of the system states and control input are shown in Figures
012345 (a)
Time (s)
-5
0
5
System states
x1
x2
x3
x4
012345 (b)
Time (s)
-100
-50
0
50
Control inputs
u1s u2s
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
(c)
Time (s)
0
1
2
3
Inter-sample times
×10−3
Tk
Figure 5.5 Self-triggered implementation: (a) evolution of the state trajectory; (b) optimal event-based con￾trol policy; and (c) inter-sample time with γ = 0.25.
5.5 (a) and (b). The inter-sample times are shown in Figure 5.5 (c). Recall that the self-triggering
condition (5.33) is derived using a weaker condition than the event-triggering condition given by
(5.20). Therefore, inter-sample times are small when compared to the event-based inter-sample
times. A minimum inter-sample time of 0.001s is observed with an averaged sampling interval of
0.0011s.
Figure 5.6 (a) Evolution of the state trajectory; (b) optimal event-based control policy; (c) triggering condi￾tion; and (d) inter-sample times
Example 5.2. In this example, the inverted pendulum on a cart with dynamics as described by Wang
and Lemmon (2009b) is considered. The parameters used for the simulation were m = 1, g = 10,
M = 10, and l = 3 with initial state x0 = [0.98, 0, 0.2, 0]
T , H = I, and R = 0.05.
All other parameters are selected as in Example 5.1 to show the efficacy of the design for both
sampling intervals and cost. The convergence of the system states and the optimal control input are
shown in Figures 5.6 (a) and (b), respectively. The evolution of the triggering condition is shownCo-optimization of Sampling and Control using Differential Games 173
Figure 5.7 Comparison of (a) cumulative sampling instants; and (b) cumulative cost between the method in
(Sahoo et al., 2018) and the method in (Wang and Lemmon, 2009b)
in Figure 5.6 (c), and inter-sample times with an average inter-sample time of 0.156 s are shown in
Figure 5.6 (d). The comparison results for the number of samples and cumulative cost are shown in
Figures 5.7 (a) and (b), respectively.
5.2 NONLINEAR SYSTEMS
In this section, the co-design approach is extended to control systems governed by nonlinear
continuous-time systems. First, the performance index is redefined as a two-player zero-sum game
where the control policy is one player and the threshold policy to bound the event-triggering error is
the second. The solution to this game is obtained as the saddle point solution to the min-max opti￾mization problem (Basar and Bernhard, 2008). The maximizing policy obtained as a solution to the
game is utilized as the dynamic threshold to determine the sampling instants, which optimizes the
sampling intervals, while the minimizing control policy, which accounts for the lack of continuous
feedback is applied to the system.
Since the solution to the optimization problem is difficult to obtain, an approximate solution
using ADP and RL techniques is presented. A functional-link linearly-parametrized artificial NN
is employed to learn the optimal value function and, further, used to determine the optimal con￾trol policy and worst-case error in control policy for designing the threshold in the event-triggering
mechanism. We shall use a forward-in-time hybrid learning scheme from Narayanan and Jagan￾nathan (2016a), which is elaborated in Chapters 6-8, to estimate the NN weights online. We shall
use Lyapunov stability analysis to guarantee the local ultimate boundedness of the state vector and
the NN weight estimation errors.
5.2.1 PROBLEM STATEMENT
Consider a continuous time nonlinear dynamical system represented in an input affine form given
by
x˙(t) = f(x) +g(x)u(t), x(0) = x0, (5.34)
where x(t) ∈ Ωx ⊂ Rn and u(t) ∈ Rm are, respectively, the state and the control input; Ωx is a
compact set in the n-dimensional Euclidean space. The maps f : Ωx → Rn, and g : Ωx → Rn×m are
the internal dynamics and input gain functions with f(0) = 0. The feedback control input for (5.34)
is of the form
u(t) = μ(x(t)), (5.35)
where μ : Ωu → Rm is a nonlinear map satisfying μ(0) = 0m and Ωu is a compact subset of Ωx. The
vector 0m denotes the zero vector in Rm.174 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
In an event-triggered control framework the system states are sampled, and the control policies
are updated at the instant tk for k = 0,1,···. The control policy is held at the actuator using a zero￾order hold and satisfies
ue(t) = μ(xe(t)),∀t ∈ [tk,tk+1), (5.36)
where ue(t) is the event-sampled control policy and xe(t) = x(tk), ∀t ∈ [tk,tk+1) is the event-sampled
state received at the controller. Hence, the control input ue(t) applied to the system is a piecewise
constant function. The error between the continuous state x(t) and the intermittently available state
xe(t), referred to as event-triggering error (or measurement error), is given by
e(t) = xe(t)−x(t), ∀t ∈ [tk,tk+1). (5.37)
Note that, with the newly received state information at the sampling instants, the error (5.37) resets
to zero, i.e., e(tk) = 0. The event-triggering error leads to an error in the control input. Define the
error between the continuous control input (5.35) and the event-sampled control control input ue(t)
in (5.36), referred to as sampling error policy es,u(t), given as
es,u(t) = ue(t)−u(t) = μ(xe)− μ(x). (5.38)
Remark 5.5. Unlike the linear case, in the case of nonlinear systems, the policy (5.35) cannot be
represented as a linear function of the event-triggering error. Therefore, for clarity of exposition,
the error es,u(t) in (5.38), is defined as a sampling error policy.
The nonlinear system given by (5.34) with event-based control policy (5.36) can be re-written as
x˙(t) = f(x) +g(x)ue(t). (5.39)
Define the performance measure ζ (t) for the system (5.34) as
ζ (t)2 = Q(x(t)) +uT (t)Ru(t) (5.40)
where Q(•) is a positive definite function satisfying Q(0) = 0 and R is a positive definite matrix. The
function Q and the matrix R penalize the states and the control policy, respectively. A block diagram
		
 

 
 	

 


  
Figure 5.8 Networked control system and event-triggered feedback.
for the traditional optimal state feedback control with event-based availability of the state vector is
shown in Figure 5.8. The control policy in (5.35) is designed by minimizing the performance index
(5.40) and the triggering condition is designed to retain the stability by forming an upped bound to
the measurement error (5.37). The event-triggering mechanism monitors the sensor measurements
and evaluates the triggering condition and dynamically determines the time-instants {tk}, ∀k =
1,2,··· to close the feedback loop.
Although the control policy minimizes the cost when continuously implemented, the event￾triggering instants are not optimal. Therefore, the problem at hand is to design an optimal control
policy that minimizes (5.40) and an event-triggering condition that maximizes the inter-sampling
interval. In the next section, a min-max optimization problem is presented to achieve this design
objective.Co-optimization of Sampling and Control using Differential Games 175
5.2.2 CO-OPTIMIZATION METHODOLOGY
In this section, similar to the linear case, a min-max optimization problem is developed for the
nonlinear system and the saddle-point solution to the min-max co-optimization problem is derived.
Further, the resulting worst-case sampling error policy is utilized to design the event-triggering
condition.
Utilizing the definition of the sampling error policy (5.38), one can express the system dynamics
(5.39) as
x˙(t) = f(x) +g(x)u(x(t)) +g(x)es,u(t). (5.41)
To optimize the ever-triggering instants, the sampling error policy es,u(t) must be maximized. Keep￾ing this in mind, redefine the infinite horizon performance index using (5.40) as
J(x, es,u,u) =  ∞
t
[ζ (τ)2 −σ2eT
s,ues,u]dτ, (5.42)
where σ > 0 represents the attenuation constant (Basar and Bernhard, 2008). The above optimal
control problem is clearly a two-player zero-sum game-based min-max problem, where the control
policy u(t) is the minimizing player, and the sampling error policy is the maximizing player. The
objective here is to find a saddle-point solution (u∗,e∗
s,u) such that these policies yield an optimal
value satisfying
V∗(x(t)) = min
u max
es,u
J(u, es,u) = max
es,u
min
u J(u, es,u). (5.43)
For a reachable and zero-state observable system (Basar and Bernhard, 2008) with Q(x) =
C(x)TC(x), with a nonlinear map C, there exists a minimum positive definite solution, the opti￾mal value function (5.43), for the associated HJI equation when σ > σ∗, where σ∗ is the H∞ gain
as described earlier for the case of linear systems using the variable γ∗ (Basar and Bernhard, 2008).
To solve this optimal control problem, define the Hamiltonian function using the infinitesimal
version of the cost function. For an admissible control policy (Dierks and Jagannathan, 2010b),
consider the cost function defined using (5.42) and the dynamic state constraint in (5.41) as
H(x,u, es,u) = Q(x) +uTRu−σ2eT
s,ues,u +V∗T
x [ f(x) +g(x)u(t) +g(x)es,u(t)], (5.44)
where V∗
x = ∂V∗
∂ x with V∗(x) denoting the value-function defined in (5.43). The optimal control
policies, by using the stationarity condition ∂H(x,u,es,u)
∂u = 0, is given by
u∗(x(t)) = −1
2
R−1gT (x)V∗
x . (5.45)
Similarly, the worst-case sampling error policy, with ∂H(x,u,es,u)
∂ es,u = 0, is given by
e∗
s,u(x(t)) = 1
2σ2 gT (x)V∗
x . (5.46)
The event-sampled optimal control policy u∗
e (t) = μ∗(xe) is given by
u∗
e (t) = −1
2
R−1gT (xe)V∗
xe , (5.47)
where V∗
xe = ∂V∗(x)
∂ x |x=xe .
Inserting the optimal control policy (5.45) and worst-case sampling error policy (5.46) in the
Hamiltonian (5.44) results in the HJI equation given by
H = Q(x) +V∗T
x f(x)− 1
4
V∗T
x g(x)R−1gT (x)V∗
x +
1
4σ2V∗T
x g(x)gT (x)V∗
x . (5.48)176 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Assumption 5.2. The functions f(x) and g(x) are Lipschitz continuous in the compact set Ωx and
the function g(x) satisfies g(x) ≤ gM, where gM > 0 is a constant.
Assumption 5.3. The control policy u(t) is locally Lipschitz continuous and satisfies es,u(t) =
μ(xe)− μ(x) = ue(t)−u(t) ≤ Lu e(t), where Lu > 0 is the Lipschitz constant.
A solution to the HJI equation (5.48), i.e., the optimal value function V∗, exists for σ > σ∗ and
is positive definite when the system and the performance function satisfy certain properties related
to the controllability and observability of the system (Basar and Bernhard (2008)). If we assume
that such a solution exists in the domain of interest and that it is smooth, the resulting optimal cost
(5.42) will be finite and the optimal control policy (5.45) will be asymptotically stabilizing in the
presence of the worst-case sampling error policy (5.46).
The following lemma establishes the ISS of the system (5.41) with respect to the ever-triggering
error.
Lemma 5.2. (Narayanan et al., 2018b) Consider the nonlinear system (5.41) and the performance
index (5.42). Let V∗ be the positive definite solution for the HJI equation (5.48) and Assumptions
5.2 and 5.3 hold. Then the optimal policy (5.45) renders the closed-loop system (5.41) is locally ISS
with respect to the event-triggering error e(t).
Next, the main results of this section are presented.
Theorem 5.4. (Narayanan et al., 2018b) Consider the affine nonlinear system dynamics (5.41)
along with the infinite horizon performance index (5.42). Suppose the Assumptions 5.2 and 5.3
hold. Let V∗ be the positive definite solution for the HJI equation (5.48) and there exists a σ > 0
such that the inequality
δ1(x) > (δ2(x))2
holds, where
δ1(x) = Q(x) + 1
2
V∗T
x g(x)R−1gT (x)V∗
x +
1
4σ2V∗T
x g(x)gT (x)V∗
x > 0
and δ2(x) = V∗T
x g. Then sthe event-sampled optimal control policy in (5.47) with the event￾triggering condition given by
es,u(t) ≤ 
e∗
s,u(t)

, t ∈ [tk,tk+1), ∀k ∈ {0,N} (5.49)
renders the closed-loop system asymptotically stable when Q,R, and σ are selected such that
δ3(x) = δ1(x)−(δ2(x))2 > 0.
From the triggering condition (5.49) the events are triggered when the sampling error reaches the
worst case sampling error policy with respect to the performance index (5.42). Consequently, the
sequence of sampling instant is optimal.
Corollary 5.2. (Narayanan et al., 2018b) Let the hypothesis of the Theorem 5.4 hold. Then, if the
following inequality given by
Lue(t) ≤ 1
2
e∗
s,u(tk) (5.50)
holds, the system (5.41) is asymptotically stable. Further, the minimum inter-sampling time
δtm = in f
k∈{0,N}
(δtk) = in f
k∈{0,N}
(tk+1 −tk) > 0 where δtk is given by
δtk = tk+1 −tk ≥
1
g ln(
g
XM
e∗
s,u(tk)+1) > 0. (5.51)
is lower bounded by a nonzero positive constant with XM > 0 such that  f(x) +g(x)u∗ ≤ XM.Co-optimization of Sampling and Control using Differential Games 177
Remark 5.6. The expression for inter-event times (5.51) is a function of the previous sampling
instant. Therefore, can be utilized to determine the next sampling instant without using the event￾triggering mechanism. The sensor sampling instants can be pre-scheduled which will save the addi￾tional computation power required at the sensor. This autonomous sampling scheme is leads to the
self-triggering scheme for the case of nonlinear systems considered in this section.
As discussed earlier, a closed-form solution to the HJI equation is almost impossible to compute
analytically even with the complete knowledge of the system dynamics. Hence, we shall utilize ADP
methods to obtain an approximate solution with NN-based approximation and a learning scheme.
5.2.3 NEURAL NETWORK CONTROLLER DESIGN
In this section, an approximate solution to the HJI equation is obtained by approximating the value
function with the help of a NN-based value function approximator (VFA), which is the solution
of the HJI equation. The control policy and the event-triggering condition is designed using the
approximate value. Thus, the saddle point solution to the min-max problem is learned online and in
a forward-in-time manner.
A schematic of the NN-based learning scheme is given in Figure 5.9. Since, the estimated solu￾tion is utilized for designing the event-triggering condition, the event-trigger mechanism is equipped
with a VFA as the one at the controller. This is similar to the mirror estimators introduced in Chap￾ter 4. Both the value function approximators are synchronized by initializing with equal NN weight
matrices and subsequent updates.


		
	
 	



 

	


    

   
Figure 5.9 Approximate optimal event sampled control system.
In an event-based sampling framework the value function (5.42) can be expressed as
V(x(0)) =
∞
∑
k=0
1
2
 t+Tk
tk
(Q(x) +uTRu−σ2eT
s,ues,u)dτ

.
Using the infinitesimal version of the cost function (5.42), we can write
V˙(τ) = −Q(x)−uT (τ)Ru(τ) +σ2eT
s,u(τ)es,u(τ). (5.52)
By integrating (5.52) in the interval [tk,tk+1), the Bellman equation in the integral form is obtained
as
V∗(tk+1)−V∗(tk) =  tk+1
tk
(−Q(x(τ))−uT (τ)Ru(τ) +σ2eT
s,u(τ)es,u(τ))dτ. (5.53)
Assuming that the solution to the HJI equation is a smooth function, a two-layer linearly parameter￾ized NN with a fixed input layer can be used to approximate the optimal value function. This means,178 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
there exist constant weight matrices W and ω such that the value function in parametric form can
be represented as V∗(x) = WT φ(ωT x) +ε(x) (5.54)
where W ∈ ΩW ⊂ RNo×1 is the unknown target weight matrix, φ(ωT x) ∈ RNo is the smooth activa￾tion function satisfying φ(0) = 0 and ω is the randomly selected constant input layer weight matrix.
Recall from Chapter 2 that the random selection of the input layer weights form a stochastic basis
(Lewis et al. (1998)). The function ε(x) is the reconstruction error and No is the number of hidden
layer neurons. Since ω is not updated online, for brevity we express φ(ωT x) = φ(x) in the rest of
this section.
To approximate the optimal value function using a NN, the following standard assumptions are
needed.
Assumption 5.4. The optimal value function V∗(x) exists and is smooth in Ωx. The target weight
vector W satisfies the bound W ≤ WM. The set of activation functions [φ1 φ2 ...φNo ]
T form a basis
on the compact set Ωx with φ(x) ≤ No. The reconstruction error satisfies ε(x) ≤ εM.
The Bellman equation (5.53) in terms of the NN approximation, by inserting (5.54), yields
WTΔφ(τ) +Δε(τ) = * tk+1
tk (−Q(x)−uT (τ)Ru(τ) +σ2eT
s,u(τ)es,u(τ))dτ (5.55)
where Δφ(τ) = φ(tk+1)−φ(tk) and Δε(τ) = ε(x(tk+1))−ε(x(tk)). Defining the optimal value func￾tion estimate as Vˆ and substituting it in the Bellman equation (5.53), the Bellman error/TD error can
be expressed as
χk+1 = * tk+1
tk (Q(x) +uT (τ)Ru(τ)−σ2eT
s,u(τ)es,u(τ))dτ +Vˆ(tk+1)−Vˆ(tk), (5.56)
where χk+1 is calculated at the occurrence of k + 1 event. The estimated value function in a para￾metric form using NN weights, Wˆ ∈ RNo is given by
Vˆ(x) = Wˆ T φ(x). (5.57)
Substituting the estimate of the approximated optimal value function from (5.57) in (5.56) yields
χk+1 =
 tk+1
tk
(Q(x) +uT (τ)Ru(τ)−σ2eT
s,u(τ)es,u(τ))dτ +Wˆ TΔφ(τ), (5.58)
where Δφ(τ) = φ(tk+1)−φ(tk) and χk+1 is the residual error calculated at the event-sampling instant
tk+1. Define the NN weight estimation error as W˜ = W −Wˆ . Then, substituting for the right hand
side of (5.55) in (5.58), we shall get
−χk+1 = WT (t)Δφ(τ)−Wˆ T (t)Δφ(τ) +Δε = W˜ T (t)Δφ(τ) +Δε. (5.59)
Note that, TD error (5.59) is expressed as a function of NN weight estimation error and used for
demonstrating the stability of the system.
Now, with the estimated value function, the control policy can be expressed as
u(t) = −1
2
R−1gT (x)Vˆ
x = −1
2
R−1gT (x)∇T φ(x)Wˆ , (5.60)
where Vˆ
x = ∂Vˆ(x)
∂ x , ∇φ is the partial derivative of φ with respect to x. The event-sampled control
input with event-based state xe can be written from (5.60) as
ue(t) = −1
2
R−1gT (xe)Vˆ
xe = −1
2
R−1gT (xe)∇T φ(xe)Wˆ , (5.61)Co-optimization of Sampling and Control using Differential Games 179
where Vˆ
xe = ∂Vˆ(x)
∂ x |xe . Similarly, the event-triggering condition using the estimated values is ex￾pressed as
es,u(t) ≤ eˆs,u(t), t ∈ [tk,tk+1), ∀k ∈ {0,N}, (5.62)
where the estimated worst-case sampled error or the threshold policy ˆes,u = 1
2σ2 gT (x)Vˆ
x(t). Since
the objective is to estimate the optimal value function online, this can be achieved by minimizing
the TD error (5.58). Thus, the following NN weight adaptation rule can be used.
˙
Wˆ =
⎧
⎪⎪⎨
⎪⎪⎩
−α [Δφ(τ)]
(1+[Δφ(τ)]T [Δφ(τ)])2 χT
k , t = tk
−α [Δφ(τ)]
(1+[Δφ(τ)]T [Δφ(τ)])2 χT (tk), t ∈ (tk,tk+1).
(5.63)
The derivative of Wˆ represent the right derivative at {tk} for k = 0,1,....
Remark 5.7. In contrast to the traditional policy or value iteration schemes described earlier in
Chapter 2, the parameter tuning rule presented in (5.63) is a hybrid learning scheme developed
by Narayanan and Jagannathan (2016a), which can be implemented online. The Bellman error,
χk, is calculated at every event-triggering instant, tk, and the parameters are updated continuously
using (5.63) both at the event-sampling and inter-event intervals. The update rule utilizes the new
information obtained at the event-triggering instant to calculate the Bellman error, χk(t), and this
error is reduced by weight updates in the inter-event period, [tk,tk+1).
Next the main result of the section is summarized.
Theorem 5.5. (Narayanan et al., 2018b) Consider the nonlinear input affine system dynamics (5.41)
along with the performance index (5.42), control policy (5.61) and NN weight update rule (5.63)
with a persistently exciting regression vector. With the Assumptions 5.2-5.4 satisfied, let the NN
initial weights Wˆ (0) be defined in a compact set ΩW and let the initial control policy be admissible.
Then, there exists an N > 0 such that the closed-loop state vector and the NN weight estimation
error are locally ultimately bounded with the event-triggering instants k > N, provided the event￾triggering condition given in (5.62) is satisfied and the design parameters α,Q,R, and σ are chosen
such that ¯
δx > L2
u, α
ρ > 1
2 + g2
M
2 , where ρ = (1+ [Δφ(τ)]T [Δφ(τ)])2 and ¯
δx = Q(x) + u∗TRu∗ −
σ2e∗T
s,ues,u with the learning rate α > 0. The bounds for states and NN weight estimation error
are Bx =
) 1
2 g2
M∇ε2
M+α2ε2
M
(¯
δx−L2
u) and BW =
 1
2 g2
M∇ε2
M+α2ε2
M
( α
ρ − g2
M
2 − 1
2 )
, respectively, as defined in the proof with
g(x) ≤ gM, ∇ε ≤ ∇εM, ε ≤ εM where ∇εM,gM, εM are positive constants.
Sketch of proof:
 Consider a continuously differentiable candidate Lyapunov function L : Bx × ΩW → R+
given by
L(x,W˜ ) = Lx(t) +LW (t) (5.64)
where Lx(t) = V∗(x) and LW (t) = 1
2W˜ TW˜ with W˜ = W −Wˆ .
 The directional derivative of the Lyapunov can be expressed as
L˙(x,W˜ ) = L˙ x(t) +L˙W (t). (5.65)
 Substituting the system dynamics and the weight estimation error dynamics using the up￾date rule, we obtain the expression for the slope of the Lyapunov candidate function.180 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
 Using the optimal Hamiltonian H(x,u∗,e∗
s,u) = 0 and simplification using standard vector
norm inequalities, the bounds Bx and BW can be obtained.
Remark 5.8. Note that the bounds on the closed loop signals are obtained as a function of the
NN reconstruction error. It is known that as the number of hidden layer neurons are increased,
the reconstruction error converges to zero. In this special case, by appropriate design of the NN
approximator, one can bring the state vector and the NN weight estimation error arbitrarily close
to zero asymptotically.
Table 5.2
Co-optimization of sampling instants and control policy for nonlinear systems
System dynamics x˙(t) = f(x(t)) +g(x(t))u(t),x(0) = x0
Event-triggering error es,u(t) = ue(t)−u(t)
Performance index J(x,es,u,u) = * ∞
t [ζ (τ)2 −σ2eT
s,ues,u]dτ
Value function approximation Vˆ(x) = Wˆ T φ(x)
Critic NN update law ˙
Wˆ =
⎧
⎪⎪⎨
⎪⎪⎩
−α [Δφ(τ)]
(1+[Δφ(τ)]T [Δφ(τ)])2 χT
k , t = tk
−α [Δφ(τ)]
(1+[Δφ(τ)]T [Δφ(τ)])2 χT (tk), t ∈ (tk,tk+1).
Optimal control input ue(t) = −1
2R−1gT (xe)Vˆ
xe = −1
2R−1gT (xe)∇T φ(xe)Wˆ
Event-triggering condition es,u(t) ≤ eˆs,u(t), t ∈ [tk,tk+1),∀k ∈ {0,N}
Example 5.3. Consider the unstable continuous-time nonlinear system dynamics as in Dierks and
Jagannathan (2010b) given by
x˙1 = −(29x1 +87x1x2
2)/8−(2x2 +3x2x2
1)/4+u1
x˙2 = −(x1 +3x1x2
2)/4+3u2.
The analytical solution to the HJI equation (optimal value function) was calculated as V∗(x) =
x2
1 + 2x2
2 + 3x1x2 by Dierks and Jagannathan (2010b). For the example results presented here, the
simulation parameters were selected as described by Dierks and Jagannathan (2010b), with σ = 0.7.
The NN weights are initialized with random values in the interval [-2, 2]. The first layer weights are
fixed at the random values and sigmoid activation function is utilized in the hidden layer neurons.
To verify if the game-theoretic method developed in this section offers any advantage, the re￾sults obtained with this method are compared with that of an event-triggered optimal approximate
controller with the event-triggering condition developed by Narayanan and Jagannathan (2016a).Co-optimization of Sampling and Control using Differential Games 181
Figure 5.10 Comparison of state trajectories
The convergence of the closed-loop system state is shown in Figure 5.10 and control inputs in Fig￾ure 5.11. It can be observed that the state trajectories are satisfactory with similar control efforts.
The comparison between the inter-sampling times is shown in Figure 5.12 (top). Further, the lower
bound on the inter-event times is observed to be 1 ms. It is clear from Figure 5.12 (top) that the inter￾event times are elongated, reducing resource utilization, which is one of the primary objectives of
the design. Further, it is observed that the average inter-event time is increased considerably. The
comparison of the cumulative cost function in Figure 5.12 (bottom) reveals the benefit of using the
game-theoretic scheme resulting in lower cost.
5.3 CO-OPTIMIZED TRACKING CONTROL OF NONLINEAR SYSTEMS
We shall now expand the game-theoretic co-design technique for designing a tracking controller
for nonlinear dynamical systems. Trajectory tracking control (Tallapragada and Chopra, 2013; Pos￾toyan et al., 2015; Cheng and Ugrinovskii, 2016; Peng et al., 2016; Gao and Chen, 2008), which
concerns with the problem of steering the system state or the output of a system along a desired
trajectory, has a wide range of practical applications. Examples include leader-follower architecture
of mobile robots (Cheng and Ugrinovskii, 2016; Han et al., 2015), autonomous systems employed
for surveillance (Postoyan et al., 2015), and target tracking radars. In a trajectory tracking control,
the control policy is, in general, time-varying which leads to a continuous control effort to keep
the tracking error minimum. Therefore, a time-based periodic implementation of a tracking con￾trol policy requires significantly higher computations when compared to a state regulation problem.
We have reviewed the event-based tracking controllers (Tallapragada and Chopra, 2013; Cheng and
Ugrinovskii, 2016; Postoyan et al., 2015) in Chapter 1 while their optimal counterpart was discussed
in Chapter 2.
This section, based on the work by Sahoo and Narayanan (2019), presents a performance-based
sampling scheme for trajectory tracking control. A performance index is introduced, which aids in
designing the optimal triggering threshold. The event-based tracking error system is formulated by
incorporating the error between the continuous control policy and the event-based control policy,
referred to as sampled error policy or the threshold policy. An augmented tracking error system,182 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Figure 5.11 Comparison of control policies
similar to the one developed in Chapter 2, in an event-based formalism, is defined.
The solution to the HJI equation, i.e., the optimal value function, is approximated using a func￾tional link critic neural network. Event-based intermittently available state information is used as
the input of the NN. The estimated optimal control policy and the triggering threshold are computed
from the approximated value function. Since the triggering threshold is a function of the estimated
NN weights, which are updated with an impulsive update scheme, the triggering condition becomes
adaptive. To avoid the Zeno behavior of the closed-loop system during the learning period, a lower
bound on the adaptive threshold is enforced. We shall see that for the event-based tracking problem,
the tracking error and the NN weight estimation errors are ultimately bounded.
5.3.1 BACKGROUND AND PROBLEM DEFINITION
Recall the problem setup for a tracking control design task from Chapter 2. We begin with an
affine nonlinear continuous-time system given in (5.34). The control objective is to track a feasible
reference trajectory xd(t) ∈ Rn generated by a reference system
x˙d(t) = ζ (xd(t)), xd(0) = xd0, (5.66)
where xd(t) ∈ Rn is the reference state and ζ : Rn → Rn is the internal dynamics with ζ (0) = 0.
Following standard characteristics of the systems in (5.34) and (5.66) are assumed.
Assumption 5.5. The system described by (5.34) is controllable and its states are available for
measurement.
Assumption 5.6. The functions f(x) and g(x) are Lipschitz continuous for all x ∈ Ωx, where Ωx
is a compact set containing the origin. Further, the function g(x) has a full column rank for all
x ∈ Ωx and satisfies g(x) ≤ gM for some constant gM > 0. In addition, g(xd)g+(xd) = I, where
g+ = (gT g)−1gT .
Assumption 5.7. The feasible reference trajectory xd(t) ∈ Ωxd , where Ωxd is a compact set, is
bounded such that xd(t) ≤ bxd , where bxd > 0 is a constant.Co-optimization of Sampling and Control using Differential Games 183
Figure 5.12 Comparison of (top) inter-event time; and (bottom) cumulative cost.
Define the error between the system state and the reference state as the tracking error, given by
er(t)  x(t)−xd(t). Then, the tracking error system, utilizing (2.164) and (5.66), can be defined by
e˙r(t) = x˙(t)−x˙d(t) = f(er +xd) +g(er +xd)u(t)−ζ (xd). (5.67)
The steady-state feed-forward control policy for the given reference trajectory (see Chapter 2) can
be expressed as
ud(t) = g+(xd)(ζ (xd))− f(xd), (5.68)
where ud : Rn → Rm is the steady state control policy corresponding to the reference trajectory. By
augmenting the tracking error er and desired trajectory xd, the dynamics of the augmented tracking
error system can be represented as
χ˙(t) = F(χ(t)) +G(χ(t))w(t), (5.69)
where χ(t)  [eT
r (t) xT
d (t)]T ∈ R2n is the augmented state with χ(0)=[eT
r (0) xT
d (0)]T = χ0, F :
R2n → R2n is given by F(χ) 

f(er +xd) +g(er +xd)ud −ζ (xd)
ζ (xd)

, G : R2n → R2n×m given by
G(χ) 

g(er +xd)
0

, and the mismatched control policy w(t)  u(t) − ud(t) ∈ Rm. Further, we
require that F(0) = 0.
The infinite horizon performance index with state constraint enforced by the dynamical system
in (5.69) can be defined as
J(χ(0),w(0)) =  ∞
0
[χT (τ)Q¯χ(τ) +w(τ)
TRw(τ)]dτ (5.70)184 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
where Q¯ 
 Q 0n×n
0n×n 0n×n

∈ R2n×2n with Q ∈ Rn×n and R ∈ Rm×m are symmetric positive definite
matrices. The matrix 0n×n is a matrix with all elements zero. Note that the performance index is
defined using the mismatched policy w (Sahoo and Narayanan, 2019; Kamalapurkar et al., 2015)
and, therefore, the cost functional is finite for any admissible control policy w ∈ Ωw, where Ωw is
the set of all admissible policies.
5.3.2 EVENT-BASED TRACKING PROBLEM
In an ETC formalism, the system state x(tk) is sent to the controller at the time instants tk,∀k ∈
{0,N}. The sampled state at the controller can be written as
xs(t) = x(tk), ∀t ∈ [tk,tk+1). (5.71)
In this section, note that we use a different notation (xs) to represent event-triggered state feedback
information. This is to prevent any potential ambiguity with notations related to reference trajectory
(indicated by subscript notation) and NN estimates (denoted by the ˆ(·) notation).
The event-based control policy, executed at the sampling instants tk,∀k ∈ {0,N} with the state
information xs(t) and reference trajectory xds = xd(tk),∀t ∈ [tk,tk+1), can be represented as
us(t) = μ(xs(t)−xds(t)) = μ(er(t),tk), ∀t ∈ [tk,tk+1), (5.72)
where μ : Rn ×R≥0 → Rm. The control policy us(t) is held at the actuator using a zero-order hold
and applied to the system till the next update. Therefore, the event-based control policy is a piece￾wise constant function.
The event-based nonlinear system can be rewritten with event-based control policy us as
x˙(t) = f(x) +g(x)us(t). (5.73)
We define the error between the continuous control input u(t)  μ(x(t),xd(t)) = μ(er(t),t) and the
event-based control input us(t) in (5.72) as sampled error policy, eu(t) ∈ Rm, which is given by
eu(t) = us(t)−u(t). (5.74)
The event-triggered system dynamics in (5.73), with sampled error policy in (5.74), lead to
x˙(t) = f(x) +g(x)u(t) +g(x)eu(t). (5.75)
The tracking error dynamics, with the event-triggered system (5.75) and the reference system (5.66),
becomes
e˙r(t) = f(er +xd) +g(er +xd)u(t) +g(er +xd)eu(t)−ζ (xd). (5.76)
The event-based augmented tracking error system, from (5.76) and (5.66), can be represented as
χ˙(t) = F(χ) +G(χ)w(t) +G(χ)eu(t). (5.77)
Our main objective is to develop a unified design scheme by minimizing a performance index
such that both the sampling intervals and the control policy are optimized. Therefore the problem
in hand is threefold: 1) redefinition of the time-invariant performance index in (5.70) to obtain the
worst case threshold for the sampled error policy, eu in (5.77), which in turn determines the event￾based sampling intervals; 2) design of the event-based sampling condition such that the sampling
intervals are maximized; and 3) design of the NN weight update law for approximation of the
solution of the corresponding HJI equation in an ETC framework. While at this point, one may
follow the developments from Section 5.2.2, there are subtle differences in developing a tracking
controller. We shall develop the solution to the tracking control problem next.Co-optimization of Sampling and Control using Differential Games 185
5.3.3 PERFORMANCE INDEX AND SOLUTION TO THE MIN-MAX OPTIMIZATION
In this section, the co-design problem is formulated as a min-max optimization problem by intro￾ducing a performance index. A saddle point solution to the optimization problem is obtained by
solving the associated HJI equation.
Reformulation of Performance Index
The event-based sampling instants can be designed by triggering the events when the sampled
error policy, eu, in (5.74) reaches a maximum value without jeopardizing the stability. Alternatively,
the design is to obtain an optimal threshold for eu(t), to determine the triggering instants with respect
to the desired performance of the system. With this effect, redefine the cost functional (5.70) with
the dynamic constraint (5.77) as
J(χ,w, eˆu) =  ∞
0
[χT (τ)Q¯χ(τ) +w(τ)
TRw(τ)−γ2eˆ
T
u (τ)eˆu(τ)]dτ, (5.78)
where γ > γ∗ represents the penalizing factor for the threshold ˆeu of sampled error policy eu with
γ∗ > 0 such that the performance index is finite for all admissible w.
Note that the performance index (5.78) is minimized by maximizing the threshold ˆeu and mini￾mizing the control policy w. Therefore, the optimization problem leads to a min-max problem hav￾ing two players. The mismatch control policy, w, acts as player 1, i.e., the minimizing player, and the
threshold, ˆeu, as player 2, i.e., the maximizing player. The objective, now, can be redefined as solv￾ing the two-player zero-sum-game (Basar and Bernhard, 2008) to reach at the saddle-point optimal
value, V∗ : R2n → R≥0, i.e., the optimal value where min
w max
eˆu
J(χ,w,eˆu) = max
eˆu
min
w J(χ,w,eˆu).
Remark 5.9. The traditional min-max problem (Basar and Bernhard, 2008) optimizes the control
policy in the presence of an independent disturbance, explicitly added to the system dynamics, by
using a performance index similar to (5.78). However, the threshold eˆu in (5.78) is implied in the
system dynamics (5.76). The main advantage of the proposed performance index is to obtain the
worst case value of eˆu in terms of system state χ, which can be used as the threshold for sampled
error policy eu. Note that this is a function of the reference trajectory.
From (5.78) the saddle-point optimal value, V∗, can be written as
V∗(χ) = min w(τ)|τ∈R≥t
max eˆu(τ)|τ∈R≥t
 ∞
t

χT (τ)Q¯χ(τ) +w(τ)
TRw(τ)−γ2eˆ
T
u (τ)eˆu(τ)

dτ. (5.79)
Define the Hamiltonian, with the admissible control policy and dynamic constraint (5.77), as
H (χ,w,eˆu) = χT (t)Q¯χ(t) +wT (t)Rw(t)−γ2eˆ
T
u (t)eˆu(t) +V∗T
χ [F(χ) +G(χ)w(t) +Geu(t)],
(5.80)
where V∗
χ = ∂V∗/∂ χ. By using the stationarity conditions, ∂H (χ,w,eˆu)
∂w = 0 and ∂H (χ,w,eˆu)
∂ eˆu = 0, the
optimal mismatch control policy
w∗(χ) = −1
2
R−1GT (χ)V∗
χ (χ) (5.81)
and the worst-case threshold value
e∗
u(χ) = 1
2γ2 GT (χ)V∗
χ (χ). (5.82)
From (5.81) and (5.68) the continuous optimal control policy u∗ is given by
u∗(t) = −1
2
R−1GT (χ)V∗
χ (χ) +g+(xd)(ζ (xd)− f(xd)). (5.83)186 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
To implement the control policy (5.83) in the ETC framework, the augmented state at the con￾troller can be expressed as
χs(t) = χ(tk), ∀t ∈ [tk,tk+1), (5.84)
where χ(tk)=[eT
r (tk), xT
d (tk)]T ,∀k ∈ {0,N}. The event-sampled optimal control policy u∗
s with the
augmented sampled state can be expressed as
u∗
s(t) = −1
2
R−1GT (χs)V∗
χs +g+(xds)(ζ (xds)− f(xds)). (5.85)
where V∗
χs = ∂V∗(χ)
∂ χ |χ=χs. The HJI equation with optimal policies (5.81) and (5.82) is given by
H ∗ = χT (t)Q¯χ(t) +w∗T (t)Rw∗(t)−γ2e∗T
u (t)e∗
u(t) +V∗T
χ [F(χ) +G(χ)w∗(t) +G(χ)e∗
u(t)] = 0
(5.86)
for all χ with V∗(0) = 0. The following assumption regarding the HJI equation is essential to pro￾ceed further.
Assumption 5.8. The solution to the HJI equation, i.e., the optimal value function V∗, exists and is
continuously differentiable.
Remark 5.10. Note that the solution of the HJI equation (5.86), i.e., the optimal value function, V∗,
exists for a reachable and zero-state observable system for γ > γ∗, where γ∗ is the H∞ gain (Basar
and Bernhard, 2008).
5.3.4 OPTIMAL EVENT-SAMPLING CONDITION AND STABILITY ANALYSIS
The sampling condition for the triggering mechanism can be defined using worst-case sampled error
e∗
u as the threshold. Define the sampling condition as
tk+1 = inf{t > tk|eu(t)
T eu(t) = max{r
2, 1
4γ4V∗T
χ GGTV∗
χ }} (5.87)
with t0 = 0. The parameter r > 0 is a design choice.
Remark 5.11. The parameter r > 0 is introduced in the triggering condition to enforce the positive
minimum inter-sample time, i.e., Zeno-free behavior of the system, and can be arbitrarily selected
close to zero. However, this minimum threshold on triggering condition leads to bounded stability
of the ETC system.
Next, the stability results of the optimal event-triggered system are presented in the theorem.
The transformation of the time-varying tracking control problem to a time-invariant problem re￾sults in a positive semidefinite optimal value function (5.79) (Kamalapurkar et al., 2015), rendering
it unsuitable to be treated as a Lyapunov candidate function. This shortcoming is overcome by
considering the time-variant counterpart of the time-invariant optimal value function V∗ given by
V∗
t : Rn × R≥0 → R, where V∗
t (er,t) = V∗([eT
r , xT
d (t)]T ) for all er ∈ Rn and for all t ∈ R≥0. It
was shown by Kamalapurkar et al. (2015) that V∗
t : Rn × R≥0 → R is a valid candidate Lyapunov
function.
Before presenting the stability results, the following technical lemma, which also establishes the
ISS of the system (5.77) with respect to the sampled error policy, is presented.
Lemma 5.3. (Sahoo and Narayanan, 2019) Consider the augmented tracking error system (5.77)
and the performance index (5.78). Let the solution to the HJI equation (5.86) be V∗, and the As￾sumptions 5.5-5.8 hold. Then the mismatch optimal policy (5.81) renders the augmented system
(5.77) local ISS with respect to eu(t).Co-optimization of Sampling and Control using Differential Games 187
Theorem 5.6. (Sahoo and Narayanan, 2019) Consider the affine nonlinear system (2.164) and the
reference system (5.66), reformulated as event-sampled augmented tracking error system (5.77). Let
V∗ be the solution of the HJI equation (5.86), and the Assumptions 5.5 - 5.8 hold. Then, with the
event-based optimal control policy (5.85) and event-based sampling condition (5.87), the tracking
error is ultimately bounded provided γ satisfies γ2 > λmax(R).
From the optimal value perspective, the next corollary quantifies the degree of optimality by com￾puting the optimal value for the event-triggered implementation when compared to the continuous
saddle point optimal value.
Corollary 5.3. (Sahoo and Narayanan, 2019) Let the hypothesis of Theorem 5.6 hold. Then, the
optimal cost for the event-triggered implementation with policy w∗
s is given by
J(;,w∗
s) = V∗(χ) + ∞
t
[eT
u (R−γ2I)eu −γ2e∗T
u e∗
u]dτ. (5.88)
Note that the cost due to event-triggering introduces the second term in (5.88). Although not sur￾prising, the event-based implementation can perform no better than the continuous implementation
in terms of the performance cost. This also indicates that the cost can be adjusted by parameters R
and γ.
Remark 5.12. The performance index in (5.78) provides an extra degree of freedom, in terms of the
parameter γ, in optimizing the value when compared to the optimal value in (Vamvoudakis et al.,
2017b). In addition, when compared to the threshold coefficient parameter σ in the traditional
event-triggering condition in (Tabuada, 2007), this penalizing term γ as threshold coefficient in the
sampling condition (5.87) also determines the degree of optimality.
5.3.5 ZENO-FREE BEHAVIOR OF SYSTEM
To show the sampling instants are not accumulated, i.e., Zeno-free behavior of the system, we will
use a more conservative event-sampling condition. Define the augmented state sampling error es as
the error between the continuous augmented state χ(t) and the sampled augmented state χs(t). It
can be expressed as
es(t) = χs(t)− χ(t), ∀t ∈ [tk,tk+1). (5.89)
The following standard assumption and the technical lemma are necessary to proceed.
Assumption 5.9. The optimal policies u∗ and e∗
u are locally Lipschitz in a compact set, such that,
u∗
s −u∗ = eu ≤ Lu es and e∗
u(χ)−e∗
u(χs) ≤ Lu es where Lu > 0 is the Lipschitz constant.
Lemma 5.4. If the inequality Lues(t) ≤ (1/4γ2)GT (χs)V∗
χs
 holds, then the inequality
eT
u (t)eu(t) ≤ (1/4γ4)V∗T
χ G(χ)GT (χ)V∗
χ also holds.
Proof.  By definition we have 1
4γ2 GT (χs)V∗
χs
 = 1
2 e∗
u(tk) and, therefore, the expression
Lues(t) ≤ (1/4γ2)GT (χs)V∗
χs

can be rewritten as
2Lues(t)≤e∗
u(tk) = e∗
u(t)+e∗
u(tk)−e∗
u(t)≤e∗
u(t)+e∗
u(tk)−e∗
u(t)≤e∗
u(t)+Lues(t).
 Rearranging the expression leads to
Lues(t)≤e∗
u(t). (5.90)188 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
 By Assumption 5.9, eu(t) < Lues(t), comparing with (5.90), it holds that eu(t) ≤
e∗
u(t). Squaring both side it holds that
eT
u (t)eu(t) ≤ (1/4γ4)V∗T
χ G(χ)GT (χ)V∗
χ .
Corollary 5.4. (Sahoo and Narayanan, 2019) Let the hypothesis of the Theorem 5.6 holds. Then,
the sampling condition defined by
tk+1 = inf{t > tk | Lues(t) = max{r, 1
4γ2 GT (χs)V∗
χs
}}, (5.91)
ensures the tracking error convergence to it’s ultimate bound. Further, the minimum inter-sample
time τm = inf
k∈{0,N}
(τk) = inf
k∈{0,N}
(tk+1 −tk) > 0 where τk is given by
τk >
1
κ1
lnκ1r
κ2
+1

, (5.92)
where κ1 = GMLu +Luρ and κ3 = Luρχs.
Since, an analytical closed-form solution to the HJI equation (5.86) is difficult to compute (Bell￾man, 1966), the solution is approximated using a NN and is presented in the next section.
5.3.6 APPROXIMATE SOLUTION FOR THE MIN-MAX OPTIMIZATION PROBLEM
In this section, the optimal value function, which is the solution of the HJI equation is approximated
using a functional link NN to design the optimal control policy and the sampling condition.
Value Function Approximation and Event-sampled Bellman Error. Recalling the Assump￾tion 5.8, the solution to the HJI equation, i.e., the optimal value function V∗(χ) is smooth and
continuous. By the universal approximation property (Lewis et al., 1998), the value function can be
approximated using neural network, referred to as critic NN, in a compact set Ωχ ⊂ R2n. Alterna￾tively, in a compact set Ωχ , there exists an ideal weight vector W and a basis function φ(χ) such
that the value function can be expressed as
V∗(χ) = WT φ(χ) +ε(χ) (5.93)
where W ∈ Rlo is the target weight vector, which is unknown, φ : R2n → Rlo is the activation func￾tion, and ε : R2n → R is the approximation error. Further, the activation function satisfy φ(0) = 0
and lo is the number of neurons in the hidden layer. The following standard assumption for the NN
is used for analysis.
Assumption 5.10. The unknown target weight vector W, the activation function φ, and the recon￾struction error ε are bounded in the compact sets. This means, there exists constants WM > 0, φM > 0
and εM > 0 such that, W ≤ WM, supχ∈Ωχ φ(χ) ≤ φM, and supχ∈Ωχ ε(χ) ≤ εM. Further, the
gradient of approximation error satisfies supχ∈Ωχ ∇ε(χ) ≤ ε¯M, where ε¯M > 0 is a constant and
∇ε(χ) = ∂ ε(χ)
∂ χ .
The optimal mismatch control policy in a parametric form using NN approximation can be com￾puted as
w∗(χ) = −(1/2)R−1GT (χ)(∇φ T (χ)W +∇ε(χ)), (5.94)
and, similarly, the worst case threshold becomes
e∗
u(χ) = −(1/2γ2)GT (χ)(∇φ T (χ)W +∇ε(χ)), (5.95)Co-optimization of Sampling and Control using Differential Games 189
where ∇φ(χ) = ∂ φ(χ)
∂ χ . The estimated value function Vˆ(χ) is given by
Vˆ(χ) = Wˆ T φ(χs), ∀t ∈ [tk,tk+1), (5.96)
where Wˆ ∈ Rlo is NN weight estimates and φ(χs) ∈ Rlo is the activation function with sampled
augmented state χs as input. Then, estimated mismatch control policy
w(χ) = −(1/2)R−1GT (χ)(∇φ T (χs)Wˆ ) (5.97)
and the estimated control input can be expressed as
u(t) = −1
2
R−1GT (χ)∇φ T (χs)Wˆ +g+(xd)(ζ (xd)− f(xd)). (5.98)
The estimated sampled control policy now can be expressed as
us(t) = −(1/2)R−1GT (χs)∇φ T (χs)Wˆ +g+(xds)(ζ (xds)− f(xds)),∀t ∈ [tk,tk+1). (5.99)
Further, the estimated threshold is given by
eˆu(t)=(1/2γ2)GT (χ)∇φ T (χs)Wˆ . (5.100)
To update the weights of the critic NN we will use the Bellman principle of optimality. Note that,
the value function (5.78) in the ETC frame work can equivalently be expressed as
J(χ(0)) =
∞
∑
k=0
 tk+τk
tk
(χTQ¯χ +wTRw−γ2eˆ
T
u eˆu)dτ

.
By Bellman principle of optimality, the Bellman equation in an integral from is given by
V∗(tk+1)−V∗(tk) =  tk+1
tk

−χT Q¯χ −wTRw+γ2eˆ
T
u eˆu

dτ, (5.101)
where tk+1 = tk +τk. The event-based integral Bellman equation (5.101), with the critic NN (5.93),
yields
WTΔφ(τk) +Δε(τk) = * tk+1
tk (−χT Q¯χ −wTRw+γ2eˆ
T
u eˆu)dτ, (5.102)
where Δφ(τk) = φ(χ(tk+1))−φ(χ(tk)) and Δε(τk) = ε(χ(tk+1))−ε(χ(tk)). With estimated critic
NN weights (5.96), the Bellman error can be expressed as
δk+1 = * tk+1
tk (χT Q¯χ +wTRw−γ2eˆ
T
u eˆu)dτ +Vˆ(tk+1)−Vˆ(tk)
= * tk+1
tk (χT Q¯χ +wTRw−γ2eˆ
T
u eˆu)dτ +Wˆ TΔφ(τk), (5.103)
where δk+1 is the Bellman residual error or temporal difference error calculated at the occurrence
of k +1 event.
The critic NN weights are learned online such that the Bellman residual error (5.103) is mini￾mized. Since the augmented system states at the controller are updated only at the sampling instants,
the Bellman residual error (5.103) can only be computed at the sampling instants. Therefore, the NN
weights are updated as a jump in the weight at the triggering instants tk,∀k ∈ {0,N} with the new
state feedback information, given as
Wˆ + = Wˆ −α2
Δφ(τk−1)
(1+Δφ T (τk−1)Δφ(τk−1))2 δ T
k , t = tk, (5.104)
where Wˆ + = Wˆ (t
+
k ) and t
+
k is the time instant just after tk. Further, to utilize the inter-sample times
and accelerate the convergence of NN weights, the Bellman residual error computed at the previous190 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
sampling instants tk is used to update the critic NN weight estimates. The continuous update (flow)
during the inter-sample times is defined as
˙
Wˆ = −α1
Δφ(τk−1)
(1+Δφ T (τk−1)Δφ(τk−1))2 δ T
k , t ∈ (tk,tk+1), (5.105)
where δk = * tk
tk−1 (χT Q¯χ +wTRw−γ2eˆ
T
u eˆu)dτ +Wˆ TΔφ(τk−1) is the Bellman residual error at tk,
derived from (5.103) with α1 > 0, and α2 > 0 are learning gains.
Remark 5.13. Note that the integration * tk
tk−1
(·)dτ and the difference Δφ(τk) can be computed
using the augmented state χ and mismatch control input w information at two consecutive sampling
instants tk and tk−1, which are available at the controller at the kth-time instant.
Remark 5.14. The update laws in (5.104) and (5.105) can be referred to as impulsive parameter
update scheme with (5.104) as jump and (5.105) as flow dynamics. Further, the update during
the inter-sample times is motivated by the traditional value/policy iteration based ADP schemes
(Bertsekas, 2012). This ensures boundedness of the closed-loop system parameters during the flow
periods when the control policy is not updated.
5.3.7 ADAPTIVE EVENT-SAMPLING CONDITION AND STABILITY
The event-based sampling condition with estimated ˆeu in (5.100) can be defined as
tk+1 = inf{t > tk | eu(t)
T eu(t) = max(r
2, 1
4γ4Wˆ T ∇φ(χs)G(χ)GT (χ)∇φ T (χs)Wˆ )}. (5.106)
Defining the NN weight estimation error W˜ = W −Wˆ , the Bellman residual error, by subtracting
(5.103) from (5.102) with a event-step backward, can be represented as
δk = −W˜ TΔφ(τk−1)−Δε(τk−1). (5.107)
The Bellman residual error δk using (5.107) is not computable since the target NN weight W is
unknown and will only be used for demonstrating the stability; presented in the next theorem.
The event-sampled augmented tracking error system, by defining a concatenated state vector
ξ = [χT , W˜ T ]
T ∈ R2n+lo , can be expressed as a nonlinear impulsive dynamical system as
˙
ξ (t) = 
F(χ) +G(χ)w(t) +G(χ)eu(t)
α1
Δφ(τk−1)
(1+ΔφT (τk−1)Δφ(τk−1))2 δ T
k

, ξ ∈ C , t ∈ (tk,tk+1) (5.108)
and
ξ + =
 χ
W˜ +α2
Δφ(τk−1)
(1+ΔφT (τk−1)Δφ(τk−1))2 δ T
k

, ξ ∈ D, t = tk, (5.109)
where (5.108) are the dynamics of the system during the inter-sample times, referred to as flow
dynamics, and (5.109) are the dynamics at the sampling instants referred to as jump dynamics. The
sets
C  {ξ ∈ R2n+lo | eu(t)
T eu(t) < max(r
2, 1
4γ4Wˆ T ∇φ(χs)G(χ)GT (χ)∇φ T (χs)Wˆ )}
and
D  {ξ ∈ R2n+lo | eu(t)
T eu(t) ≥ max(r
2, 1
4γ4Wˆ T ∇φ(χs)G(χ)GT (χ)∇φ T (χs)Wˆ )}Co-optimization of Sampling and Control using Differential Games 191
are the flow and jump sets, respectively.
For brevity and to facilitate the proof of the theorem, presented next, the following variables are
defined.
gr = GR−1GT , gφ = ∇φGR−1GT ∇T φ, gγ = ∇φGGT ∇T φ. (5.110)
Further, using the Assumptions 5.6 and 5.10, the following bounds are are developed in compact
set Ωχ :
1
2WT gφ + 1
2∇εT gr∇φ T  = ι1, gγ || = ι2, 1
2 WT ∇φgr∇ε+ 1
2 ∇εT gr∇ε = ι3,gr = ι4,
gφ  = ι5, (ι1 + ι2
2γ2 ) = ϖ1, ι2
2γ2 W2 +ι3 = ϖ2,
ϖ3 = ϖ2 + α
4 Δε2
M, ϖ5 = 1
2α2Δε2
M + 1
2αΔε2
M + 1
2α2ΔφMΔε2
M.
(5.111)
Moreover, the inequalities
 Δφ(τk−1)
1+ΔφT (τk−1)Δφ(τk−1)
 ≤ 1
2 , 1
1+ΔφT (τk−1)Δφ(τk−1) ≤ 1 (5.112)
for every vector Δφ are also utilized to claim the practical stability using Lyapunov analysis by the
authors Sahoo and Narayanan (2019).
Before presenting the main results, the following standard assumption on the PE condition for
parameter estimation is presented for completeness.
Assumption 5.11. The vector ϕ(τ)  Δφ(τ)
1+ΔφT (τ)Δφ(τ) ∈ Rlo is persistently exciting, i.e., there exist a
time period T > 0 and a constant ϕδ > 0 such that over the interval [t,t +T] the regressor vector
satisfies * t+T
t ϕ(τ)ϕT (τ)dτ ≥ ϕδ I, where I is the identity matrix of appropriate dimensions.
Theorem 5.7. (Sahoo and Narayanan, 2019) Consider the event-triggered system represented as
a nonlinear impulsive hybrid dynamical system in (5.108) and (5.109) with control policy (5.99).
Suppose the Assumptions 5.5-5.11 hold, the NN initial weights Wˆ (0) initialized in a compact set
ΩW and the initial control policy be admissible. Then, there exists a positive integer N > 0, such
that, the tracking error er and the NN weight estimation error W, is locally ultimately bounded for ˜
all sampling instants tk > tN, provided event-based sampling instants are obtained using (5.106)
and the weight tuning gains are selected as 0 < α1 < 1 and 0 < α2 < 1
2 . Further, V∗ −Vˆ  and
ws −w∗
s  are also ultimately bounded.
Corollary 5.5. Let the hypothesis of Theorem 5.7 hold. Then, the set of the sequence of event
sampling instants {tk | k ∈ {0,N}, eT
u eu = max(r2, 1
4γ4Wˆ T ∇φ(χs)GGT (χ)
∇T φ(χs)Wˆ )} determined by (5.106) converges the closed neighborhood of the actual sampling
sequence {tk | k ∈ {0,N},eu(t)T eu(t) = max{r2,(1/4γ4)V∗T
χ GGTV∗
χ }} determined in (5.87).
Corollary 5.6. (Sahoo and Narayanan, 2019) Let the hypothesis of the Theorem 5.7 hold. Then,
with the event-based sampling condition
tk+1 = inf{t > tk | Lues(t) = max{r, 1
4γ2 GT (χs)Vˆχs}} (5.113)
the tracking error er and NN weight estimation error W are locally ultimately bounded. Further, the ˜
minimum inter-sample time δtm = inf
k∈{0,N}
(δtk) > 0, where δtk is given by
δtk >
1
κ1
lnκ1r
κ2
+1

. (5.114)
The implementation of the proposed analytical design is detailed in the flow chart shown in
Figure 5.13.192 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Table 5.3
Co-optimization of sampling instants and control policy for trajectory tracking
System dynamics ˙x(t) = f(x(t)) +g(x(t))u(t),x(0) = x0
Reference system dynamics ˙xd(t) = ζ (xd(t)), xd(0) = xd0
Augmented error system χ˙ = F(χ) +G(χ)w
Event-triggering error es,u(t) = us(t)−u(t),
Performance index J(χ,w, eˆu) =  ∞
0
[χT (τ)Q¯χ(τ) +w(τ)
TRw(τ)−γ2eˆ
T
u (τ)eˆu(τ)]dτ
Value function approximation Vˆ(χ) = Wˆ T φ(χs), ∀t ∈ [tk,tk+1)
Critic NN update law ˙
Wˆ = −α1
Δφ(τk−1)
(1+ΔφT (τk−1)Δφ(τk−1))2 δ T
k , t ∈ (tk,tk+1)
Wˆ + = Wˆ −α2
Δφ(τk−1)
(1+ΔφT (τk−1)Δφ(τk−1))2 δ T
k , t = tk
Optimal control input us(t) = −(1/2)R−1GT (χs)∇φ T (χs)Wˆ
+g+(xds)(ζ (xds)− f(xds)),∀t ∈ [tk,tk+1).
Event-triggering condition tk+1 = inf{t > tk|eu(t)T eu(t) = max{r2, 1
4γ4V∗T
χ GGTV∗
χ }}
Example 5.4. A numerical example of Van-der-Pol oscillator, whose dynamics is given by
x˙(t) = f(x) +g(x)u(t),
with f = 
x2, x2(x2
1 −1) +x1
T and g = [0, 1]
T , is considered for the validation using simulation.
The reference trajectory considered is given by xd = [0.5cos(2t), −sin(2t)]T with ζ (xd) =
[xd2, −4xd1]
T . The penalty matrices in the performance index (5.78) are selected as Q = 50 ×
diag[1,0.1], R = 0.1, γ = 0.35. The NN weights are initialized randomly from a uniform distribution
in the interval [0, 2]. The learning gains are selected as α1 = 0.045, and α2 = 0.08. A polynomial re￾gression vector φ(χ)=(1/8)[χ2
1 ,χ1χ2,χ1χ3,χ1χ4,χ2
2 ,χ2χ3,χ2χ4,χ2
3 ,χ3χ4,χ2
1 χ2,χ2
1 χ3,χ2
1 χ4,χ2
2 ,
χ2
2 χ3,χ2
2 χ4,χ2
3 χ4,χ2
1 χ2
2 ,χ2
1 χ2
3 ,χ2
1 χ2
4 ,χ2
2 χ2
3 ,χ2
2 χ2
4 ,χ2
3 χ2
4 ,χ2
4 ]
T is used for approximating the solu￾tion of the HJI equation. A normally distributed probing noise in the interval [0,1] is added to the
regressor vector to satisfy PE condition to ensure the convergence of the NN weights. The simula￾tion is run for 35 s with initial states x0 = [2, 1.5]
T .
The results of the proposed near-optimal event-based ADP-based scheme is compared with sam￾pled data implementation of the scheme, where the feedback signals are sampled and the controller
is executed periodically. The plots are superimposed to verify the effectiveness of the proposed
approach. The notation (e) and (c) next to the simulation variables in the figures indicate event￾based implementation and sampled data implementation, respectively. All simulation parameters,
including the initial conditions, are kept the same for both cases. Learning gain for the NN weight
update during the flow period, which is not available in the sampled data case, is adjusted to obtainCo-optimization of Sampling and Control using Differential Games 193
Start
Initialize states x(0), critic FLNN weights
(W(0)) and initial stabilizing control us(0)
Run the system
x˙ = f(x) + g(x)us
Check trig. cond.
eT
u eu ≤
max"
r2, 1
4γ4 Wˆ T ∇φ(χs)G(χ)GT (χ)∇φ T (χs)Wˆ
#
Update FLNN weights (flow)
˙
Wˆ = −α1
Δφ(τk−1)
(1+ΔφT (τk−1)Δφ(τk−1))2 δ T
k
Update FLNN weight (jump)
Wˆ + = Wˆ − α2
Δφ(τk−1)
(1+ΔφT (τk−1)Δφ(τk−1))2 δ T
k
Execute control policy
us(t) = −(1/2)R−1GT (χs)∇φ T (χs)Wˆ +
g+(xds)(ζ (xds) − f(xds))
Yes
No
Figure 5.13 Implementation of the game-based algorithm presented in this section.
a comparable state convergence rate for fair comparison in terms of computation and performance
cost.
The event-sampling condition in (5.104) with r = 0.005 is utilized for the sampling and controller
execution. The states and the desired trajectory are shown in Figure 5.14. It clear from the figure
that with the event based implementation of the control input closely follows the sampled data
implementation with less frequent control execution. The system states converge arbitrarily close
to the desired states in about 20 second. The large initial tracking error is due to the approximated
controller with randomly initialized NN weights. As the NN weight estimates converge close to the
target values, the system states converge close to the desired trajectories. The convergence of the
tracking errors for both event-based and sampled data implementation to their respective bounds
are shown in Figure 5.15. Note that the event-based tracking error closely follows the sampled￾data tracking error. This is achieved by selecting the NN weight tuning gain during the flow period
appropriately, which accelerate the convergence rate of the weight updates.
The convergence of the event-based Bellman residual error close to zero is shown in Figure 5.16.
Note that the Bellman error converges earlier than the state convergence in both cases. This implies194 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Figure 5.14 Plot showing trajectories of (a) cumulative events and (b) inter-sample time.
Figure 5.15 Convergence of tracking errors.
0 5 10 15 20 25 30 35
T ime (s)
-0.5
0
0.5
1
1.5
2
Bellman errors
δk (e) δk (c)
Figure 5.16 Convergence of event-based Bellman errors.
the approximate value function converges to arbitrarily close neighborhood of their optimal values
before the states converge to their respective desired states. It is observed that the Bellman error
for the event-based implementation case spikes before convergence when compared to the smooth
plot of sampled data implementation. This is due to the aperiodic control execution. However, the
Bellman error converges close to zero as in the case of sampled data implementation. The cost
comparison is shown in Figure 5.17. The cumulative cost for the event-based approach is close to
each other. Note that the penalty factor γ in (5.76) determines the degree of optimality and differentCo-optimization of Sampling and Control using Differential Games 195
Figure 5.17 Comparison of cumulative costs
Figure 5.18 Cumulative number of sampling instants and inter-sample times.
values of γ will lead to different costs.
From the reduction of computation point of view, the cumulative number of sampling instants
and the inter-sampling times are shown in Figure 5.18 (a) and (b). It is observed that the control
is executed 51.18% of times when compared to the periodic sampled data execution for γ = 0.35
during the simulation time of 35 s. This shows a 48.82% reduction of feedback communication and
computational load. Note that γ is selected such that γ2 > λmax(R) where λmax = 0.1. Different value
of γ and r will result in different number of sampling. From Figure 5.18 (b) it is evident that the
sampling instants are aperiodic and the events are crowded till 2.5 secs. This is due to the high initial
tracking error in the learning phase of the NN. Once the NN estimated weights converge close to
the target values, shown in Figure 5.19, the sampling instants are reduced.
Since the ideal NN weights are unknown, it is not possible to show convergence of the NN
estimated weight to the target values. However, from Figure 5.19 (a) and (b), it can be observed
that the NN weight estimates reaches their steady state, i.e., become constant, approximately about
18 seconds for both cases. Therefore, it can be concluded that the NN weights converge to a close
neighborhood of the target weights. Alternatively, the NN weight estimation error is ultimately
bounded. Note that the target values for both cases are different since the optimal value function for
sampled data case is different from the proposed event-based case.196 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Figure 5.19 Convergence of estimated NN weights.
5.4 CONCLUDING REMARKS
In conclusion, this chapter presents event-based control strategies, each tailored to optimize system
performance based on game-theoretic techniques. First, we looked at an event-triggered and self￾triggered controllers for linear dynamical systems to co-design the optimal control policy and trigger
conditions. The second part extended this co-design approach, leveraging game theory to jointly
optimize sampling instants and control policy for nonlinear dynamical system. Finally, we focused
on near-optimal adaptive sampling for state trajectory tracking, with significant reductions in control
execution. The key takeaway from these techniques is that the co-design method using the zero-sum
game formulation offers an extra design variable to optimize the performance cost. This degree of
freedom can be leveraged to optimize event-triggered controllers.
5.5 PROBLEMS
5.5.1 Consider an inverted pendulum (Spooner and Passino, 1999), which can be represented as an
input-affine nonlinear dynamical system. The dynamics of the system are given by
x˙1 = x2,
x˙2 = (mgr
J − kr2
4 J)sinx1 +
kr
2J
(l −b) + u
J
. (5.115)
Design feedback control policies based on the control algorithm using equations in Table 5.2.
5.5.2 Design near-optimal control policies based on the learning algorithm using equations in Table
5.3 for the Problem 5.5.1.
5.5.3 For the same system as in Problem 5.5.1., consider the linearized dynamics as in
(Guinaldo et al., 2011). The parameters in the system dynamics are m1= 2,m2= 2.5,
J1= 5,J2= 6.25, k = 10,r = 0.5,l = 0.5, and g = 9.8,b = 0.5. Pick the initial conditions of
the system states in the interval [0,1] and the initial weights of the NN from [-1,1], randomly.
Design event- and self-triggered control policies based on the algorithms using equations in
Table 5.1.Co-optimization of Sampling and Control using Differential Games 197
5.5.4 Consider the system of an inverted pendulum. The dynamics are
x˙(t) =  0 1
g
l − aik
ml2 0

xi(t) +γca  0
1
ml2

ui(t −τ),
where l = 5, g = 10, m = 5, k = 5 and hi j = 1 for ∀ j ∈ {1,2,..,N}. The system is open
loop unstable. Design event- and self-triggered control policies based on the algorithms using
equations in Table 5.1.
5.5.5 For the system described in the previous problem, design feedback control policies using
equations in Table 5.2. Compare results with the solution obtained using 5.1.
5.5.6 Design feedback control policies using equations in Table 5.1 for the Problem 2.8.1.
5.5.7 Consider the nonlinear system represented by
x˙1 = −x1 +x2
x˙2 = −1
2 (x1 +x2) + 1
2
x2 sin2 (x1)+(sin(x1))u (5.116)
Design feedback control policies using equations in Table 5.2.
5.5.8 Consider the two-link robot manipulator system with the dynamics described in Example 4.3.
Design feedback control policies using equations in Table 5.2.
5.5.9 For the system defined in the previous problem, Design feedback control policies using equa￾tions in Table 5.3. Use the generator system with dynamics ˙xd(t) = 0, with initial conditions
(0.1,0.1).
5.5.10 Consider the two-link robot manipulator system with the dynamics described in Example 4.3.
Design feedback control policies using equations in Table 5.3. Compare the performance of
the event-triggering mechanisms with that given in Example 4.3.6 Large-scale Linear
Systems
CONTENTS
6.1 Introduction ........................................................................................................................... 199
6.2 Large-scale Systems and Periodic Controllers ...................................................................... 201
6.2.1 Large-scale systems with linear dynamics ............................................................... 201
6.2.2 Periodically-sampled time-driven Q-learning .......................................................... 203
6.3 Event-based Hybrid Q-learning Scheme ............................................................................... 205
6.3.1 Time-driven Q-learning with intermittent feedback ................................................. 206
6.3.2 Hybrid Q-Learning with Intermittent Feedback....................................................... 207
6.3.3 Tracking control problem for linear interconnected systems. .................................. 214
6.4 Closing Remarks ................................................................................................................... 217
6.5 Problems................................................................................................................................ 218
So far in the book, we have focused on isolated dynamical systems. We have considered the design
of control inputs for these systems along with an event-triggering scheme to specify the sampling
and communication instants and determine when the sensor data and the control inputs are transmit￾ted from the sensor to the controller and the controller to the actuator, respectively. In this chapter,
we shall expand our focus to large-scale interconnected systems govered by linear dynamics. These
systems are composed of multiple subsystems each of which is governed by the drift dynamics,
dynamics due to control inputs, and an interconnection dynamics that model the effect of inter￾actions/coupling between the subsystems. For such large-scale systems, we shall first extend the
Q-learning algorithm described in Chapter 3 to develop a distributed learning algorithm for synthe￾sizing distributed control policies. The distributed learning algorithm, called the hybrid Q-learning
algorithm introduced in (Narayanan and Jagannathan, 2016b, 2015), shall be used for the design of
a linear adaptive optimal regulator for a large-scale interconnected system with event-sampled input
and state vector. We shall see that the extension of Q-learning-based controllers to such large-scale
systems with event-triggered distributed control execution introduces significant challenges in data
sampling and communication protocol design due to network losses. To accommodate these losses,
we shall utilize a stochastic dynamic modelling approach (Xu et al., 2012) for the large-scale sys￾tem and use this model to design the Q-learning algorithm. We shall see that by embedding iterative
parameter learning updates within the event-sampled instants along with the time-driven Q-learning
algorithm introduced in Chapter 3, the efficiency of the optimal regulator is improved considerably.
6.1 INTRODUCTION
The control of large-scale interconnected systems has been a prominent research area for the past
five decades (Jamshidi, 1996). These systems are characterized by their complexity, comprising
geographically distributed subsystems connected with each other through either structural interac￾tions (e.g., power grid, Venkat et al., 2008), communication network (e.g., wireless sensor networks,
Mazo and Tabuada, 2011), or through control inputs in applications modeled as ensemble control
199200 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
systems (Li and Khaneja, 2006; Yu et al., 2023). Traditional centralized controller design techniques
for such systems often face challenges related to computational limitations, control integrity (Ioan￾nou, 1986), demands high communication cost, is prone to single-point-of-failure, and requires an
accurate model of the structural interconnections (Siljak, 2011). To address these issues, decentral￾ized and distributed control schemes have been extensively explored in the literature, empowering
each subsystem with an independent local controller (Ioannou, 1986; Siljak and Zecevic, 2005;
Mehraeen and Jagannathan, 2011; Liu et al., 2014; Narendra and Mukhopadhyay, 2010; Venkat
et al., 2008; Camponogara and de Lima, 2012; Song and Fang, 2014; Zhou et al., 2015; Zheng
et al., 2012; Wang et al., 2010; Chen et al., 2015; Wang and Lemmon, 2011b; Guinaldo et al., 2012).
The design of local controllers in these schemes becomes complex due to constraints imposed by
the interconnections, specifically the coupling matrix (Jamshidi, 1996; Ioannou, 1986; Siljak and
Zecevic, 2005; Mehraeen and Jagannathan, 2011; Liu et al., 2014; Narendra and Mukhopadhyay,
2010; Venkat et al., 2008; Camponogara and de Lima, 2012; Song and Fang, 2014; Zhou et al.,
2015; Zheng et al., 2012; Wang et al., 2010; Chen et al., 2015) in systems with linear dynamics.
This matrix determines how the states and/or the control of one subsystem influence the dynamics
of the other subsystems.
In a decentralized control scheme, each subsystem is equipped with independent controllers and
are controlled using locally available feedback information (Siljak, 2011; Bakule, 2008; Antonelli,
2013). In other words, in these schemes, the control feedback loop at each subsystem is closed lo￾cally, and hence, a dedicated communication network to share information among the subsystems is
not required. However, decentralized controllers may lead to unsatisfactory performance, and even
instability, especially either if the interconnection strength is strong (Antonelli, 2013; Gusrialdi and
Hirche, 2011) or in the presence of unstable fixed modes (Wang and Davison, 1973). To system￾atically deal with the stability issues arising in decentralized controllers, robust control approaches
based on small-gain theorem were introduced in a series of works by Jiang et al. (1994, 1996); Liu
and Jiang (2015); Dashkovskiy et al. (2010). In these approaches, an ISS gain operator was derived,
and conditions were imposed on this ISS gain operator to guarantee the existence of an ISS Lya￾punov function for the overall system. However, they did not consider the task of optimizing the
transient response performance of the subsystems. Optimality was later included as a design crite￾rion in the decentralized control framework (Bakule, 2008; Liu et al., 2014; Jiang and Jiang, 2012;
Gao et al., 2016).
Over the years, significant advancements have been made in the design of controllers for large￾scale systems to ensure the stabilization of subsystems despite uncertain interconnection matrices
and limited communication (Ioannou, 1986; Jamshidi, 1996; Siljak and Zecevic, 2005). Promi￾nently, adaptive controllers were introduced with the aim of learning the interconnection terms
and providing appropriate compensation (Ioannou, 1986; Siljak and Zecevic, 2005; Mehraeen and
Jagannathan, 2011; Liu et al., 2014), although their effectiveness was found to be limited when
dealing with strong interconnections. As an alternative, the utilization of reference models emerged
as a method to gather information about other subsystems, but it was reported by Narendra and
Mukhopadhyay (2010) that relying solely on reference models without direct communication of
state information leads to unsatisfactory transient performance. Furthermore, leveraging the com￾munication network that connects subsystems, several distributed control algorithms have been pro￾posed (e.g., Venkat et al. (2008); Hirche et al. (2009); Camponogara and de Lima (2012); Song and
Fang (2014); Zhou et al. (2015); Zheng et al. (2012), and the references therein).
Alternatively, works employing model-predictive control (MPC) to effectively address optimiza￾tion problems for large-scale systems while leveraging the network have been popular.MPC-based
algorithms typically rely on accurate system models to predict future outputs and iteratively mini￾mize a cost function over a limited time horizon (Venkat et al., 2008; Camponogara and de Lima,
2012; Song and Fang, 2014; Zhou et al., 2015; Zheng et al., 2012). In contrast, we shall see that the
Q-function-based distributed control algorithm developed in this chapter offers distinct advantages.Large-scale Linear Systems 201
It does not necessitate an accurate system model and avoids extensive iterations for optimization.
As demonstrated in previous chapters, the utilization of aperiodic event-triggered control offers
significant advantages in reducing communication and computational overhead, making it particu￾larly well-suited for large-scale interconnected systems. This is relevant when considering systems
that share a communication network with limited bandwidth, where minimizing network usage be￾comes crucial. Event-triggered control has been extensively studied in the context of large-scale
interconnected systems, as evident in various works (Wang and Lemmon, 2011b; Guinaldo et al.,
2012; Chen et al., 2015). Notably, these advancements often incorporate assumptions on intercon￾nection strengths (Wang and Lemmon, 2011b; Chen et al., 2015) or control coefficients (Guinaldo
et al., 2012) to achieve subsystem decoupling. Moreover, the presence of a communication network
among subsystems and within the feedback loop introduces random time delays and data dropouts
(Halevi and Ray, 1988; Zhang et al., 2001; Shousong and Qixin, 2003), which can significantly
degrade control performance. Therefore, it is essential to incorporate these network losses in the
control design process to ensure realistic control signals for controlling large-scale systems. In the
approach presented in this chapter, we incorporate these network losses using a well-established
sampled-data modeling approach based on the developments outlined in Xu et al. (2012) and Halevi
and Ray (1988).
Learning control signals for isolated system when the dynamics are not fully known is already
challenging. In the event-triggered framework with aperiodic feedback, we have seen that the learn￾ing process is slowed down considerably. We have also seen in the previous chapters that the inter￾event time was dependent on the learning error and the use of mirror estimator in the event-triggering
mechanism resulted in frequent events when the learning errors were large. We shall see that a hybrid
model-free Q-learning scheme, introduced in this chapter, uses event-sampled state and input vector
to improve the learning time. This hybrid algorithm embeds a finite number of Q-function param￾eter updates within the inter-event period to facilitate faster learning without explicitly increasing
the events as discussed in Chapter 3. In the hybrid algorithm, we shall also see that the temporal￾difference based ADP schemes (Xu et al., 2012; Sahoo and Jagannathan, 2014) and the policy/value
iterations based ADP schemes (Lewis et al., 2012a; Zhong et al., 2014) become special cases. Since
the Q-function parameters at each subsystem are estimated online with event-sampled input, state
information along with past history, and the data obtained from other subsystems through the com￾munication network, an overall system model is not required. This makes the control scheme data￾driven (Hou and Wang, 2013). In this chapter, we will cover the development of a hybrid Q-learning
scheme from (Narayanan and Jagannathan, 2016b) using event-sampled states, input vectors, and
their history. Additionally, we will discuss the derivation of a time-driven and hybrid Q-learning
scheme for an uncertain large-scale interconnected system governed by linear dynamics and en￾closed by a communication network.
6.2 LARGE-SCALE SYSTEMS AND PERIODIC CONTROLLERS
In this section, we shall develop a time-driven Q-learning scheme for large-scale interconnected
systems with periodic feedback.
6.2.1 LARGE-SCALE SYSTEMS WITH LINEAR DYNAMICS
Consider a linear time-invariant continuous-time system having N interconnected subsystems as
shown in Figure 6.1 with subsystem dynamics described by
x˙i(t) = Aixi(t) +Biui(t) +∑N
j=1
j=i
Ai jxj(t), xi(0) = xi0, (6.1)
where xi(t), x˙i(t) ∈ Rni×1 represent the state vector and state derivatives, respectively, ui(t) ∈
Rmi ,Ai ∈ Rni×ni , and Bi ∈ Rni×mi denote control input, internal dynamics, and control gain ma-202 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Figure 6.1 Block diagram illustrating distributed control scheme in a large-scale interconnected system.
trices of the i
th subsystem, Ai j ∈ Rni×nj represents the interconnection matrix between the i
th and
j
th subsystem for i ∈ 1,2,..N. The system matrices Ai,Bi, and the interconnection matrix Ai j are
considered uncertain, which implies that the entries in these matrices are not accurately known. The
overall system description can be expressed in a compact form as
X˙(t) = AX(t) +BU(t), X(0) = X0, (6.2)
where X(t) ∈ Rn, U(t) ∈ Rm, B ∈ Rn×m, A ∈ Rn×n, X˙ = [x˙
T
1 ,..., x˙
T
N]
T
, A =
⎛
⎝
A1 ... A1N
: ... :
AN1 ... AN
⎞
⎠,
B = diag[B1,.,BN] (block diagonal matrix), and U = [uT
1 ,...,uT
N]
T
. In the large-scale interconnected
system, the subsystems communicate with each other via Network-0 (shown in Figure 6.1), while
each subsystem is also enclosed by their local networks. Effects of the network-induced losses can
be modeled along with the system dynamics by utilizing the discretization technique previously
seen in Chapter 3. Detailed exposition on the assumptions and the modeling approach can be seen
in works by Halevi and Ray (1988); Xu et al. (2012). The assumptions in this context cover various
aspects related to traffic patterns, message lengths, data latencies, sampling intervals, time skew,
control signal processing delay, network conditions, and queue capacity.
They can be summarized as follows: The communication network traffic follows a periodic pat￾tern with deterministic (time-dependent) queueing delays and data latencies. The message lengths
for sensor and control signals are identical, indicating that their latencies share similar characteris￾tics. The sampling intervals for the sensor and controller are the same. A constant time skew exists
between the sampling instants of the sensor and controller over a finite time window. The control
signal processing delay is constant and shorter than the sampling interval. The network is assumed
to be non-overloaded and free from transmission errors. Additionally, the receiver queue at the con￾troller has a capacity of one (Halevi and Ray, 1988; Xu et al., 2012). These assumptions provide
specific conditions for analyzing and designing controls in the context of CPS with communication
networks in their feedback loops.
Assumption 6.1. The system (6.2) is considered controllable and the states are measurable. Fur￾ther, the order of subsystems is considered known.Large-scale Linear Systems 203
With the network-induced delays and data-dropout, the dynamics of the plant can be rewritten as
X˙(t) = AX(t) +γca(t)BU(t −τ(t)), X(0) = X0, (6.3)
where γca(t) is the data-dropout indicator, which becomes In×n when the control input is received at
the actuator and 0n×n when the control input is lost at time t. This only includes the data loss in Net￾work 2 and τ(t) is the total delay. Now, integrating the system dynamics with network parameters
over the sampling interval (Halevi and Ray, 1988; Xu et al., 2012), we get
Xk+1 = AdXk +γca,kBk
0Uk +γca,k−1Bk
1Uk−1 +...+γca,k−d¯Bk
d¯
Uk−d¯, X(0) = X0, (6.4)
where Xk = X(kTs),Ad = eATs, d¯ is the delay bound, Uk is the control input. The matrices Bk
0 and
Bk
i for i = {1,2,...d¯} are all defined as in (Xu et al., 2012). From the discretized system repre￾sentation, we can define an augmented state vector consisting of state and past control inputs as
X¯(k)=[XT
k UT
k−1 ...UT
k−d¯
]
T ∈ Rn+dm¯ . The new augmented system representation is given by
X¯
k+1 = Axk¯ X¯
k +Bxk¯ Uk, X¯(0) = X¯
0, (6.5)
with the system matrices given by Axk¯ =
⎡
⎢
⎢
⎢
⎣
Ad γca,k−1Bk
1 ··· γca,k−d¯Bk
d¯
0 ··· 0 0
.
.
. Im
.
.
. .
.
.
0 ··· Im 0
⎤
⎥
⎥
⎥
⎦
and Bxk¯ =
⎡
⎢
⎢
⎢
⎣
γca,kBk
0
Im
.
.
.
0
⎤
⎥
⎥
⎥
⎦
.
Note that the system dynamics in (6.5) are stochastic due to the network-induced delays and data￾dropouts. The assumptions regarding the controllability, observability, and the existence of unique
solution for the stochastic Riccati equation (SRE) are now dependent on the Grammian functions
associated with this stochastic system (Lewis et al., 2012b). Hence, inorder to proceed further toward
developing a Q-learning-based control scheme, the following assumption is needed.
Assumption 6.2. We need the system defined in (6.5) to be both uniformly completely observable
and controllable (Lewis et al., 2012b).
The time-driven Q-learning and adaptive optimal regulation of such stochastic linear time￾varying interconnected system is presented next. We shall begin by considering the case when the
feedback is periodic.
6.2.2 PERIODICALLY-SAMPLED TIME-DRIVEN Q-LEARNING
For the system dynamics (6.5), we can define an infinite horizon cost function as
Jk = E
τ,γ

1
2 ∑∞
t=k X¯ T
k Px¯X¯
k +UT
k Rx¯Uk

, (6.6)
where Px¯ = diag(P, R
d¯,.., R
d¯) and Rx¯ = R
d¯. The penalty matrices P and R are positive semidefinite and
positive definite, respectively. E denotes the expected value operator. The cost function (6.6) can
also be represented as
Jk = E
τ,γ
[X¯ T
k SkX¯
k]
with Sk being the symmetric positive semi-definite solution of the SRE (Lewis et al., 2012b). The
next step is to define the action-dependent Q-function for the stochastic system (6.5) with the cost￾to-go function (6.6) as
Q(X¯
k,Uk) = E
τ,γ
[r(X¯
k,Uk) +Jk+1 |X¯
k ] = E
τ,γ
{[X¯ T
k UT
k ]Gk[X¯ T
k UT
k ]
T
}, (6.7)204 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
where r(X¯
k,Uk) = X¯ T
k Px¯X¯
k +UT
k Rx¯Uk and Gk is a time-varying matrix. In the Bellman equation, we
have the matrix
E
τ,γ
(Gk) =
⎡
⎣
Px¯ + E
τ,γ
(AT
xk¯ Sk+1Axk¯ ) E
τ,γ
(AT
xk¯ Sk+1Bxk¯ )
E
τ,γ
(BT
xk¯ Sk+1Axk¯ ) Rx¯ + E
τ,γ
(BT
xk¯ Sk+1Bxk¯ )
⎤
⎦
=
⎡
⎣
E
τ,γ
(Gx¯x¯
k ) E
τ,γ
(GxU¯
k )
E
τ,γ
(GUx¯
k ) E
τ,γ
(GUU
k )
⎤
⎦. (6.8)
From the matrix equation (6.8), the time-varying control gain can be expressed as
Kk = E
τ,γ
{[Rx¯ + BT
xk¯ Sk+1Bxk¯ ]
−1
BT
xk¯ Sk+1Axk¯ } = E
τ,γ
{(GUU
k )
−1
GUx¯
k }. (6.9)
The Q-function (6.7) in parametric form is given by
Q(X¯
k,Uk) = E
τ,γ
(z
T
k Gkzk) = E
τ,γ
(ΘT
k ξk), (6.10)
where zk = [(γsc,kX¯
k)T UT
k ]
T
∈ R¯l with ¯l = m+n+md¯ and ξk = zT
k ⊗zk is the regression vector.
The symbol ⊗ denotes the Kronecker product, Θk ∈ ΩΘ ⊂ Rlg is formed by vectorization of the
parameter matrix Gk, and γsc,k is a packet loss indicator, defined similar to γca,k. The estimate of the
optimal Q-function is expressed as
Qˆ(X¯
k,Uk) = E
τ,γ
(z
T
k Gˆ kzk) = E
τ,γ
(Θˆ T
k ξk), (6.11)
where Θˆ k ∈ Rlg is the estimate of expected target parameter Θk. By Bellman’s principle of optimal￾ity, the optimal value function satisfies
0 = E
τ,γ
(J∗
k+1 |X¯
k )− E
τ,γ
(J∗
k ) + E
τ,γ
(r(X¯
k,Uk)) = E
τ,γ
(r(X¯
k,Uk)) + E
τ,γ
(ΘT
k Δξk), (6.12)
where Δξk = ξk+1 −ξk and E
τ,γ
(J∗
k+1 |X¯
k ) is the expected cost-to-go at k +1st instant given the state
information at the kth instant. Since the estimated Q-function does not satisfy (6.12), the temporal
difference (TD) error will be observed as
eB(k) = E
τ,γ
(r(X¯
k,Uk) +Θˆ T
k Δξk). (6.13)
Remark 6.1. In the iterative learning schemes (Bradtke et al., 1994; Lewis et al., 2012a) the pa￾rameters of the Q-function estimator (QFE) are updated to minimize the error in (6.13) until the
error converges to a small value for every time step k. On the contrary, time-driven ADP schemes
(Xu et al., 2012; Sahoo and Jagannathan, 2014; Dierks and Jagannathan, 2009a) calculate the Bell￾man error at each step and update once at the sampling instant and the stability of the closed-loop
system is established under certain mild assumptions on the estimated control policy.
The overall cost function (6.6) for the large-scale system (6.5), can be represented as the sum
of the individual cost of all the subsystems as, Jk = ∑N
i=1 Ji,k, where Ji,k = E
τ,γ
{1
2 ∑∞
s=k x¯
T
i,kPx¯,ix¯i,k
+uT
i,kRx¯,iui,k} is the quadratic cost function for i
th subsystem with ¯xi representing the augmented
states of the i
th subsystem, Px¯ = diag{Px¯,1 ···Px¯,N} and Rx¯ = diag{Rx¯,1 ···Rx¯,N}.
The optimal control sequence to minimize the quadratic cost function (6.6) in a decentralized
framework is not straightforward because of the interconnection dynamics. Alternatively, the dis￾tributed optimal control policy for each subsystem, which minimizes the cost function (6.6), isLarge-scale Linear Systems 205
obtained by using the SRE of the overall system given the system dynamics Axk¯ and Bxk¯ . It is given
by
u∗
i,k = E
τ,γ
{−K∗
i,kx¯i,k −∑N
j=1, j=i
K∗
i j,kx¯j,k}, (6.14)
where K∗
i,k are the diagonal elements and K∗
i j,k are the off-diagonal elements of K∗
k in (6.9). In the
following lemma, we shall see that with the control law (6.14) designed at each subsystem, the
overall system is asymptotically stabilized in the mean square.
Lemma 6.1. Consider the ith subsystem of the large-scale interconnected system (6.5). Let Assump￾tion 6.2 hold and let the system matrices Axk¯ and Bxk¯ be known. The optimal control policy obtained
as in (6.14) renders the individual subsystems asymptotically stable in the mean square.
Proof:
 By definition, the optimal control input is stabilizing. The closed-loop system ma￾trix (Axk¯ − Bxk¯ K∗
k ) is Schur stable (Kailath, 1980). Therefore, the Lyapunov equation
(Axk¯ −Bxk¯ K∗
k )
TP¯(Axk¯ −Bxk¯ K∗
k )−P¯ = −F¯, has a positive definite solution F¯.
 Consider the Lyapunov function candidate Lk = E
τ,γ
(X¯ T
k P¯X¯
k), with P¯ being positive definite.
 The first difference, using the overall system dynamics with optimal control input is ΔLk =
−E
τ,γ
(X¯ T
k F¯X¯
k). Since, F¯ can be chosen as a diagonal matrix, the first difference in terms of
the subsystems can be expressed as
ΔLk = −∑N
i=1 E
τ,γ
(x¯
T
i,kF¯
ix¯i,k) ≤ −∑N
i=1 q¯min E
τ,γ

x¯i,k


2
, (6.15)
where ¯qmin is the minimum singular value of F¯.
 This implies the subsystems are asymptotically stable in the mean square. The results of
this lemma plays a key role in the stability analysis of the interconnected system, where the
need for the accurate knowledge of Axk¯ and Bxk¯ can be relaxed.
The controller design using a hybrid Q-learning based ADP approach for such large-scale intercon￾nected system in the presence of network-induced losses and with intermittent feedback is discussed
next.
6.3 EVENT-BASED HYBRID Q-LEARNING SCHEME
In this section, we present the distributed hybrid learning scheme reported in (Narayanan and Jagan￾nathan, 2016b), which builds upon the time-driven Q-learning-based ADP approach for the control
of large-scale interconnected system to improve the convergence time with event-sampled state and
input vectors. In the hybrid algorithm, the idle-time between any two successive events are utilized
to perform limited parameter updates iteratively in order to minimize the Bellman error. The finite
number of iterations between any two events may vary, and hence, the resulting control policy need
not necessarily converge to an admissible policy. In this case, the stability of the closed-loop sys￾tem cannot be established following the arguments used in traditional iterative ADP schemes (Lewis
et al., 2012a) or the time-driven Q-learning schemes (Sahoo and Jagannathan, 2014; Xu et al., 2012).
An additional challenge is to estimate the Q-function parameters in (6.11) for the system defined
in (6.5) with intermittent feedback and in the presence of network-induced losses. Since subsystems
broadcast their states via the communication network, each local subsystem can estimate the Q￾function of the overall system so that a predefined reference model is not needed. Subsequently, the
optimal control gains and the decoupling gains for each subsystem can be computed without using
the complete knowledge of the system dynamics and interconnection matrix. Although, the esti￾mation of the Q-function at each subsystem increases the computation, this additional computation206 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
overhead can be considered as trade-off for relaxing the assumption on the strength of interconnec￾tion terms and estimating optimal control policy.
Assumption 6.3. The target parameters are assumed to be slowly-varying (Goodwin and Sin,
2014).
In adaptive identification, the estimation of unknown parameters is carried out adaptively. When
these parameters exhibit time-dependent behavior, the adaptive learning scheme must track a mov￾ing target. Consequently, the assumption sets the constraints on the target parameters that enable an
adaptive scheme to learn them in real-time.
6.3.1 TIME-DRIVEN Q-LEARNING WITH INTERMITTENT FEEDBACK
In the case of an event-sampled system, the system state vector X¯
k is sent to the controller at event￾sampled instants. To denote the event-sampling instants, we define a sub-sequence {kl}l∈N,∀k ∈
0,1,..., with k0 = 0 being the initial sampling instant and N is the set of natural numbers. The
system state vector X¯
kl sent to the controller is held by ZOH until the next sampling instant, and it
is expressed as X¯ e
k = X¯
kl
, kl ≤ k < kl+1. The error in measurements due to this aperiodic feedback
transmission is referred to as the event-sampling error. Precisely, this error can be expressed as
eET (k) = X¯
k −X¯ e
k , kl ≤ k < kl+1, l = 1,2,··· . (6.16)
Since the estimation of expected value of the parameter matrix Gk must use X¯ e
k , the Q-function
estimate can be expressed as
Qˆ(X¯ e
k ,Uk) = E
τ,γ
(z
e,T
k Gˆ kz
e
k) = E
τ,γ
(Θˆ T
k ξ e
k ), kl ≤ k < kl+1, (6.17)
where ze
k = [(γsc,kX¯ e
k )T UT
k ]
T
∈ R¯l
, ξ e
k = z
e,T
k ⊗ze
k being the event-sampled regression vector, and Θˆ
is the result of vectorization of the matrix Gˆ k. The Bellman error with event-sampled state is given
by
eB(k) = E
τ,γ

r(X¯ e
k ,Uk) +Θˆ T
k Δξ e
k

, kl ≤ k < kl+1. (6.18)
Here r(X¯ e
k ,Uk) = X¯ e,T
k Px¯X¯ e
k +UT
k Rx¯Uk and Δξ e
k = ξ e
k+1 − ξ e
k . The Bellman error in (6.18) can be
rewritten as
eB(k) = E
τ,γ
{r(X¯
k,Uk) +Θˆ T
k Δξk +Ξs

X¯
k, eET (k),Θˆ k

}, (6.19)
where Ξs

X¯
k,eET (k),Θˆ k

= r(X¯
k −eET (k),Uk)−r(X¯
k,Uk) +Θˆ T
k (Δξ e
k −Δξk).
Remark 6.2. By comparing (6.19) with (6.13), one can see that the Bellman error in (6.19) has
an additional error term, Ξs

X¯
k, eET (k),Θˆ k

. This additional error consists of errors in cost-to-go
and the regression vector, which are driven by eET (k). Hence, the estimation of QFE parameters
depends upon the frequency of the event-sampling instants. We have seen this in Chapters 3-5. One
of the consequence of this was that the learning process was tied with the event-triggering, and
often, faster parameter convergence required frequent events.
The parameter vector estimated via the QFE, Θˆ i
k, is tuned only at the event-sampling instants.
The superscript i denotes the overall system parameters at the i
th subsystem. Using these parameters,
the estimated control policy at each subsystem can be computed as
Ui
k = −Kˆi
kX¯i,e
k = −(Gˆi,uu
k )
−1
(Gˆi,ux
k )X¯i,e
k . (6.20)
By using (6.20), the event-based estimated control input for the i
th subsystem is given by
ui,k = −Kˆi,kx¯
e
i,k −∑N
j=1, j=i
Kˆi j,kx¯
e
j,k, kl ≤ k < kl+1, ∀i ∈ {1,2,..N}. (6.21)Large-scale Linear Systems 207
Remark 6.3. It should be noted that the optimal controllers designed at each subsystem takes into
account the structural constraint that are present in the form of the interconnection matrix. However,
the consideration of input, state, and time constraints (Lewis et al., 2012b) as a part of the optimal
control problem is not dealt with in this book.
With the following assumption, the parameter update rule for the Q-function estimator is pre￾sented.
Assumption 6.4. The target parameter vector Θk is assumed to be bounded by positive constant
such that Θk ≤ ΘM. The regression function Zi
(X¯
k) is locally Lipschitz for all X¯
k ∈ Ωx.
6.3.2 HYBRID Q-LEARNING WITH INTERMITTENT FEEDBACK
In the time-driven Q-learning scheme (Sahoo and Jagannathan, 2014) discussed in Chapter 3, the
parameters of the QFE are not updated during the inter-event time interval. On the contrary, in the
hybrid learning algorithm, the parameters are also tuned during the inter-event interval thus making
the Q-function to converge faster and the actual control input to attain optimality quicker.
6.3.2.1 Parameter update at event-sampling instants
The QFE parameter vector Θˆ i
k is tuned by using the past data of the Bellman error (6.19) that is
available at the event-sampling instants. Therefore, the auxiliary Bellman error at the event-sampling
instants is expressed as
ΞB
i,e
(k) = Πi,e
k +Θˆ i,T
k Zi,e
k , for k = kl,
where
Πi,e
k = [r(X¯i
ki
l
,Ui
ki
l
) r(X¯i
ki
l−1
,Ui
ki
l−1
··· r(X¯i
ki
l−v−1
,Ui
ki
l−v−1
)] ∈ R1×ν
and
Zi,e
k = [Δξ i
ki
l
, Δξ i
ki
l−1
, ··· Δξ i
ki
l−v−1
] ∈ Rlg×ν .
Remark 6.4. A larger time history may lead to faster convergence but results in higher computation.
The (optimal or minimum) number of historic states and input values ν is not fixed is a design choice.
Next, to reduce this error, select the update law (Goodwin and Sin, 2014) for the QFE parameter
vector Θˆ i
k tuned only at the event-sampling instants, as
Θˆ i
k = Θˆ i
k−1 +
Wi
k−2Zi,e
k−1Ξi,eT
B (k −1)
1+Zi,eT
k−1Wi
k−2Zi,e
k−1
, k = kl, (6.22)
where
Wi
k = Wi
k−1 − Wi
k−1Zi,e
k−1Zi,eT
k−1Wi
k−1
1+Zi,eT
k−1Wi
k−1Zi,e
k−1
, k = kl. (6.23)
Here Wi
0 = βI, with β > 0, a large positive value. The aperiodic execution of (6.22), saves computa￾tion, when compared to the traditional adaptive Q-learning techniques. The superscript i indicating
the overall system parameters at the i
th subsystem, will be dropped from hereon. The update rules
for tuning the parameters within the inter-event periods are presented next.208 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
6.3.2.2 Iterative parameter update
The Recursive Least Squares (RLS) algorithm was employed in the work presented in (Bradtke
et al., 1994; Lewis et al., 2012a) to iteratively update the parameters of the Q-function within pe￾riodic sampling instants for performing the policy evaluation, i.e., to estimate the value function
satisfying the Bellman equation for the given policy. The update equations aim to minimize the
Bellman error, and analytical findings in (Bradtke et al., 1994; Lewis et al., 2012a) demonstrate that
each iteration produces a control policy that is at least as good as the existing policy in minimizing
the Bellman error.
However, performing a large number of iterative updates in real-time control is not practical. To
address this, the time-driven Q-learning approach (Xu et al., 2012; Sahoo and Jagannathan, 2014)
was developed. To compensate for the lack of sufficent updates in the time-driven learning, Dierks
and Jagannathan (2009b) proposed update rules that included data batches consisting of historic
data. Studies in (Xu et al., 2012; Sahoo and Jagannathan, 2014) reveal that as the sampling instants
increase, the parameter estimation error converges to zero. To enhance the convergence rate of the
estimation error, the RLS update equations (6.22) and (6.23) were applied at the sampling instants
and additional iterative parameter updates were introduced by Narayanan and Jagannathan (2016b)
between two event-triggering instants.
In other words, to utilize the time between two event-sampling instants, Narayanan and Jagan￾nathan (2016b) proposed to update the parameters iteratively to minimize the error that was calcu￾lated during the previous event, which is expressed as ΞB j,e
(k) = Πj,e
k +Θˆ j,T
k Z j,e
k , k = kl, where j
is the iteration index. In this scheme, the Q-function parameters are updated using the equations
Θˆ (k j
l ) = Θˆ (k j−1
l ) +
W(k j−2
l−2 )Z(k j−1
l−1 )ΞT
B (k j−1
l−1 )
1+ZT (k j−1
l−1 )W(k j−1
l−2 )Z(k j−1
l−1 )
, (6.24)
W(k j
l ) = W(k j−1
l−1 )− W(k j−1
l−1 )Z(k j−1
l−1 )ZT (k j−1
l−1 )W(k j−1
l−1 )
1+ZT (k j−1
l−1 )W(k j−1
l−1 )Z(k j−1
l−1 ) , j = 0,1,.... (6.25)
These updates start at k j
l with j = 0 and ends at k j
l+1 for some j > 0 for every l > 0. Hence, whenever
there is an event, the Q-function parameter vector, which is updated iteratively using (6.24) and
(6.25) is passed on to the QFE to calculate the new Bellman error. The estimated control gain matrix
can be obtained from the estimated parameter vector Θˆ k in (6.22) at each event-sampled instants. In
terms of the estimated parameters, the control gains are given by (6.20), where
Kˆk = (Gˆ uu
k )
−1
Gˆ ux
k =
⎡
⎢
⎣
Kˆ1 ··· Kˆ1N
.
.
. ... .
.
.
KˆN1 ··· KˆN
⎤
⎥
⎦ (6.26)
is the estimated control gain. It is important to note that this control gain is obtained directly from
the Q-function parameters, which are constructed with the past data and the current feedback infor￾mation, and without using the system dynamics.
In the hybrid algorithm, the update equations (6.24) and (6.25) together with (6.18) search for
an improved Q-function during every inter-event period. This is done by utilizing the Bellman error
equation (6.18) to evaluate the existing control policy, the Q-function is iteratively updated between
two event-sampling instants. However, in contrast to the algorithms in (Bradtke et al., 1994; Lewis
et al., 2012a), the iteration index j in (6.24) and (6.25) depends on the event-sampling mechanism,
resulting in finite, varying number of iterative updates between any two events.
Remark 6.5. The control policy for the individual subsystem is given by (6.21). Since it is possible
that Gˆ uu
k might be rank-deficient during the learning phase, Narayanan and Jagannathan (2016b)Large-scale Linear Systems 209
proposed the following conditions to check before the control law is updated. If Gˆ uu
k−1 is singular or
if Gˆ uu
k−1−Rx¯ is not positive definite, then, Gˆ uu
k−1 is replaced by Rx¯ in the control policy. The conditions
can be checked easily by calculating the eigenvalues of Gˆ uu
k−1.
Remark 6.6. The QFE parameter update rules given by (6.22) and (6.23) requires the state vectors
Xkl to Xkl−v−1 for the computation of regression vector at k = kl. Therefore, the past values are
required to be stored at the QFE.
With the update rules presented in this section and the control gains selected from (6.23), the
assumption in (Xu et al., 2012; Sahoo and Jagannathan, 2014) that the inverse of Gˆ uu
k exists when
the updates utilize the time history of the regression function and Bellman error is no longer needed.
The analytical results for the learning algorithm are summarized next.
6.3.2.3 Stability Analysis
Defining the QFE’s parameter estimation error E
τ,γ
(Θ˜ k) = E
τ,γ
(Θk −Θˆ k), the error dynamics for the
parameters at event-triggering instants and the inter-event times can be represented using (6.22) and
(6.24) as
E
τ,γ
(Θ˜ 0
kl+1
) = E
τ,γ
(Θ˜ j
k +
W j
k Z j,e
k Ξj,eT
B (k)
1+Ze j,T
k W j
k Z j,e
k
), k = k0
l , (6.27)
E
τ,γ
(Θ˜ j+1
kl ) = E
τ,γ
(Θ˜ j
k +
W j
k Z j,e
k Ξj,eT
B (k)
1+Ze j,T
k W j
k Z j,e
k
), k0
l < k < k0
l+1. (6.28)
Remark 6.7. When there is no data loss, the Q-function estimator is updated and the control policy
is updated as soon as it is computed. This requires the broadcast scheme to generate an acknowl￾edgment signal whenever the packets are successfully received at the subsystems (Guinaldo et al.,
2012). A suitable scheduling protocol has to ensure that the data lost in the network is kept minimal.
Next, an event-sampling condition is selected for the Q-learning scheme to work execute these
parameter and control updates and sampling instants. We can develop event-triggering conditions
as in Chapters 3 and 4. The hybrid learning algorithm introduced in this chapter is independent of
the event-sampling condition. Here, we consider a quadratic function f i
(k) = x¯i(k)
TΓix¯i(k), with
Γi > 0, for the i
th subsystem. The event-sampling condition should satisfy
f i
(k) ≤ λ f i
(kl +1),∀k ∈ [kl +1, kl+1), (6.29)
for stability, when λ < 1, as shown in the next section.
Remark 6.8. The event-sampling condition presented here depends only on the local subsystem
state information. The function f i
(k) can be selected as the Lyapunov function and such an event￾sampling condition is also presented in (Meng and Chen, 2013) for a single system.
The following technical lemma will be used to prove the stability of the closed-loop system
during the learning period. Detailed proofs for the Lemmas and Theorems presented in this chapter
are available in (Narayanan and Jagannathan, 2016b).
Lemma 6.2. Consider the system in (6.5) and the QFE (6.17). Define U˜(kl−1) =U(kl−1)−Uˆ(kl−1)
and G˜ ux
kl−1 = Gux
kl−1 − Gˆ ux
kl−1
. If the control policy is updated such that, whenever Gˆ uu
kl−1 − Rx¯ is not
positive definite or Gˆ uu
kl−1 is singular, Gˆ uu
kl−1 is replaced by Rx¯ in the control policy, then
E
τ,γ
(U˜(kl−1)) ≤ E
τ,γ
{2

R−1
x¯





Gux
kl−1




X¯
kl−1

+
R−1
x¯





G˜ ux
kl−1




X¯
kl−1

} (6.30)210 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Key Ideas of the Proof:
 This Lemma establishes an upper bound for the error in the control inputs applied to the
system based on the QFE.
 In particular, this bound holds for the cases when Gˆ uu
kl−1 is either singular or nonsingular.
The upper bound is used in the main Theorem to demonstrate the closed-loop stability of
the controlled system.
 The proof is a straight-forward application of the given definitions, the use of triangle in￾equality, and basic properties of norm operators given in Chapter 1.
 Together with the requirement of persistency of excitation, conditions for stability of the
closed loop system can be derived.
Recall the definition of a persistently exciting (PE) signal from Chapter 3. A regression vector
ϕ(xk) is said to be PE if there exists positive constants δ,α
− ,α¯ , and kd ≥ 1 such that
α
− I ≤ ∑k+δ
k=kd
ϕ(xk)ϕT (xk) ≤ α¯ I,
where I is the identity matrix of appropriate dimension.
Lemma 6.3. Consider the QFE in (6.17) with an initial admissible control policy U0 ∈ Rm. Let all
the aforementioned Assumptions (3.1-3.4) hold, and the QFE parameter vector Θˆ (0) be initialized in
a compact set ΩΘ. When the QFE is updated at the event-sampling instants using (6.22) and (6.23)
and during the inter-sampling period using (6.24) and (6.25), the QFE parameter estimation error
E
τ,γ
(Θ˜ j
kl
) is bounded. Under the assumption that the regression vector ξ j
kl is PE, the QFE parameter
estimation error Θ˜ j
kl for all Θˆ (0) ∈ ΩΘ converges to zero asymptotically in the mean square as the
event instants kl → ∞.
Sketch of Proof:
 The proof is composed of two parts. The first part considers the event-sampling instants and
the second part considers the inter-event time. A common Lyapunov function for both the
cases are considered to show that this function values decrease as the events increases.
 The Lyapunov candidate function is chosen to be
Li,Θ˜ (k j
l ) = E
τ,γ
Θ˜ T (k j
l )W−1(k j
l−1)Θ˜ (k j
l ) (6.31)
where j is the iteration index.
 Using the definition of Θ˜ and matrix inversion lemma (Goodwin and Sin, 2014), the first
difference for all the Q function estimators can be obtained as
∑N
i=1 ΔLi,Θ˜ (k j
l ) ≤ −N E
τ,γ
Θ˜ T (k j
l−1)Z(k j
l−1)ZT (k j
l−1)Θ˜ (k j
l−1)
1+ZT (k j
l−1)W(k j
l−1)Z(k j
l−1) . (6.32)
 Similarly, during the inter-sampling instants, since the parameters are updated iteratively
using (6.24) and (6.25). The first difference of the Lyapunov candidate function, using sim￾ilar arguments as in the previous case, can be shown to be negative semi-definite.
 If the regression vector satisfies PE condition, we can show that the Lyapunov first differ￾ences satisfies the following bound
∑N
i=1 ΔLi,Θ˜ (k j
l ) ≤ −Nκmin E
τ,γ



Θ˜ T (k j
l−1)



2
(6.33)
with 0 < κmin ≤ 1.Large-scale Linear Systems 211
 Thus, with the regression vector satisfying PE condition, the parameter estimation error is
strictly decreasing both during the event-sampling instants and the inter-event period. This
implies that as k j
l → ∞, the QFE parameter estimation error converges to zero asymptotically
in the mean-square.
Remark 6.9. Covariance resetting technique (Goodwin and Sin, 2014) can be used to reset W
whenever W ≤ Wmin. This condition was also used in the Lyapunov analysis to ensure stability of
the closed-loop system (Narayanan and Jagannathan, 2016b). With the covariance resetting, the
parameter convergence proof in Lemma 6.3 will still be valid (Goodwin and Sin, 2014).
Next, the Lyapunov analysis is used to derive the conditions for the stability of the closed-loop
system, with the controller designed in this section.
Theorem 6.1. Consider the closed-loop system (6.5), parameter estimation error dynamics (6.23)
along with the control input (6.20). Let the Assumptions 6.1-6.4 hold, and let U(0) ∈ Ωu be an initial
admissible control policy. Suppose the last held state vector, X¯ e, j
kl , and the QFE parameter vector,
Θˆ j
kl are updated by using, (6.22),(6.23) at the event-sampled instants, and (6.24),(6.25) during the
inter-sampling period. Then, there exists a constant γmin > 0 such that the closed-loop system state
vector X¯ j
kl for all X¯(0) ∈ Ωx converges to zero asymptotically in the mean square and the QFE
parameter estimation error Θ˜ j
kl for all Θˆ (0) ∈ ΩΘ remains bounded. Further, under the assump￾tion that the regression vector ξ j
kl satisfies the PE condition, the QFE parameter estimation error
Θ˜ j
kl for all Θˆ (0) ∈ ΩΘ converges to zero asymptotically in the mean square, with event-sampled in￾stants kl → ∞, provided the inequality γmin > μ +ρ1 is satisfied. Further, the estimated Q-function
Qˆ(X¯(k),U(k)) →E
τ,γ
{Q∗(X¯(k),U(k))} and estimated control input U(k) → E
τ,γ
{U∗(k)}. μ,ρ1 are
positive constants.
Sketch of Proof: The proof step considers two cases. In the first case, the periodic feedback case is
be analysed. In the second case, the aperiodic feedback is considered.
 Case 1: The Lyapunov function considered is
L(X¯,Θ˜ ) = E
τ,γ
X¯ T
k−1ΓX¯
k−1 + E
τ,γ
Π¯ ∑N
i=1 Li,Θ˜ (6.34)
Π¯ = η W0ρ2
N with η > 1.
 For the first term, the first difference can be obtained as
ΔLx ≤ −(γmin − μ −ρ1)E
τ,γ
X¯
k−1
2 +ρ2 Γ E
τ,γ

Θ˜ k−1


2
X¯
k−1
2
, (6.35)
where ρ1 =


(Γ+ Γ2
ε2 )Bmax



2
(4GM

R−1
x¯


2
+ 2ε), ρ2 = Γ


(1+ Γ
ε2
)Bmax



2
(

R−1
x¯


2
+
2G2
MR−1
x¯ 
4
ε ), μ = ε2Ac2
. To obtain this inequality, the results of Lemma 6.2, Assumption
6.4, and the bounds GM are used along with the Young’s inequality (Young, 1912).
 For the second term in the Lyapunov function, we use Lemma 6.3, when 0 < Γ ≤ Wmin
(from Remark 6.9), and since the history values are used, Zk−12 ≥ X¯
k−1
2
, then the first
difference becomes
ΔL ≤ −(γmin − μ −ρ1)E
τ,γ
X¯(k −1)
2 −(Π¯ N −W0ρ2)κmin E
τ,γ

Θ˜ (k −1)


2
, (6.36)
with 0 ≤ α ≤ 1. Substituting the value of Π¯ , the second term is always negative. Therefore,
L(k +1) < L(k),∀k ∈ N.212 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Figure 6.2 Evolution of the Lyapunov function (a) with Hybrid learning scheme. (b) without Hybrid learning
scheme.
 Case 2: To extend the stability results for the event-based control scheme, it is required
to prove that between any two aperiodic sampling instants, the Lyapunov function is non￾increasing. Let the Lyapunov function be given by (6.34). Taking the first difference to get
ΔLk = E
τ,γ
{X¯ T
k ΓX¯
k −X¯ T
k−1ΓX¯
k−1 +Π¯ ∑N
i=1 ΔLi,Θ˜ } kl ≤ k < kl+1,∀l ∈ N (6.37)
 When the events occurring at kl and kl+1 = kl +1, the Lyapunov function is decreasing due
to (6.36).
 When the event-sampling does not occur consecutively at kl,kl +1, the interval [kl,kl+1) =
[kl, kl + 1) ∪ [kl + 1, kl+1). During [kl,kl + 1), the Lyapunov function is decreasing be￾cause of the control policy updated at kl. In the interval [kl + 1,kl+1) due to the event￾sampling algorithm the inequality in (6.29) is satisfied. Therefore, ΔL(X¯,Θ˜ ) = E
τ,γ
{X¯ T
k ΓX¯
k−
λX¯ T
kl+1ΓX¯
kl+1}+ΔLi,Θ˜ . Using the results from Lemma 6.3 and for ¯
λ < 1, we get
ΔLk = −(1− ¯
λ)E
τ,γ
{X¯ T
kl
ΓX¯
kl
} −Nκmin E
τ,γ



Θ˜ T (k j
l−1)



2
(6.38)
 Therefore, ΔL(x¯,Θ˜ ) < 0 during the inter-sampling period. From Lemma 6.1, ∑N
i=1 ΔL(x¯i,Θ˜ i
) < 0.
Combining Case 1 and Case 2, the Lyapunov equation satisfies the following inequality,
L(kl+1) < L(kl +1) < L(kl),∀{kl}l∈N (6.39)
The evolution of the Lyapunov function is depicted in Figure 6.2. During the event-sampling in￾stant, due to the updated control policy (6.21), the Lyapunov function decreases. Due to the event￾sampling condition (6.29) and the iterative learning within the event-sampling instants, the Lya￾punov function decreases during the inter-sampling period. Since the iterative learning does not
take place in the time-driven Q-learning (Sahoo and Jagannathan, 2014), the first difference of the
parameter estimation error is zero for the inter-event period. This makes the Lyapunov function
negative semi-definite during this period.
Remark 6.10. The design constants Rx¯,Wmin, and W0 are selected based on the inequalities that
are analytically derived in the Theorem 6.1 using the bounds on Axk¯ ,Bxk¯ ,Sk (see Proof). Then, the
constants Γ and Π¯ can be found to ensure closed-loop system stability. The requirement of PE
condition is necessary so that the regression vector is non-zero until the parameter error goes to
zero. By satisfying the PE condition in the regression vector, the expected value of the parameter
estimation error Θ˜
k will converge to zero. This PE signal is viewed as the exploration signal in the
reinforcement learning literature (Lewis et al., 2012a).
Remark 6.11. An initial identification process can be used to obtain the nominal values of Axk¯ ,Bxk¯
which can be used to initialize the Q-function parameters. The algorithm presented in this sectionLarge-scale Linear Systems 213
Table 6.1
Distributed hybrid Q-Learning
Subsystem dynamics x˙i(t) = Aixi(t) +Biui(t) +∑N
j=1
j=i
Ai jx j(t), xi(0) = xi0,
Overall system dynamics X˙(t) = AX(t) +γcaBU(t −τ)
Sampled stochastic dynamics X¯
k+1 = Axk¯ X¯
k +Bxk¯ Uk, X¯(0) = X¯0
Temporal difference (TD) error eB(k) = E
τ,γ
(r(X¯
k,Uk) +Θˆ T
k Δξk)
Event-driven TD error eB(k) = E
τ,γ

r(X¯ e
k ,Uk) +Θˆ T
k Δξ e
k

, kl ≤ k < kl+1
TD Learning Θˆ i
k = Θˆ i
k−1 +
Wi
k−2Zi,e
k−1Ξi,eT
B (k−1)
1+Zi,eT
k−1Wi
k−2Zi,e
k−1
, k = kl
Wi
k = Wi
k−1 − Wi
k−1Zi,e
k−1Zi,eT
k−1Wi
k−1
1+Zi,eT
k−1Wi
k−1Zi,e
k−1
, k = kl
Hybrid learning Θˆ (k j
l ) = Θˆ (k j−1
l ) + W(k j−2
l−2 )Z(k j−1
l−1 )ΞT
B (k j−1
l−1 )
1+ZT (k j−1
l−1 )W(k j−1
l−2 )Z(k j−1
l−1 )
W(k j
l ) = W(k j−1
l−1 )− W(k j−1
l−1 )Z(k j−1
l−1 )ZT (k j−1
l−1 )W(k j−1
l−1 )
1+ZT (k j−1
l−1 )W(k j−1
l−1 )Z(k j−1
l−1 )
Distributed control law ui,k = −Kˆi,kx¯
e
i,k −∑N
j=1, j=iKˆi j,kx¯
e
j,k, kl ≤ k < kl+1, ∀i ∈ {1,2,..N}
can be used as a time-driven Q-learning scheme by not performing the iterative learning between
the event-sampling instants. Also, if the iteration index, j → ∞, for each kl, the algorithm becomes
the traditional value iteration-based ADP scheme.
The event-sampling and broadcast algorithm for the subsystems followed by the hybrid learning
algorithm is summarized next.
6.3.2.4 Hybrid Q-learning Algorithm
For estimating the overall Q-function locally, we shall use the following request-based event￾sampling algorithm. Consider an event occurring at the i
th subsystem at the sampling instant kl.
This subsystem generates a request signal and broadcasts it with its state information to the other
subsystems. Upon receiving the broadcast request, the other subsystems broadcast their respective
state information to all the subsystems. This can be considered as a forced event at the other sub￾systems.
Remark 6.12. The events at all the subsystem occur asynchronously based on the local event￾sampling condition, whereas the Q-function estimator and control policy remain synchronized at214 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
each subsystem due to the forced event. The request signal is considered to be broadcasted without
any delay in Network-0 in Figure 6.1.
The algorithm for the hybrid learning scheme is summarized as Algorithm 6.1.
Algorithm 6.1 Hybrid Q-Learning for Intermittent feedback
1: Initialize Θˆ j
0,W j
0 ,U0
2: for Event-sampling instants: l = 0 → ∞ do
3: if Event = Yes then
4: Calculate Bellman Error eB(k j
l )
5: Update Θˆ j
kl
,W j
kl
6: Update the control input at the actuator Ukl
7: Pass the parameters Θˆ 0
kl
,W0
kl
,eB(k0
l ) for iterations
8: else
9: for Iterative Index: j = 0 → ∞ do
10: Update Θˆ j
kl
,W j
kl with eB(k j
l )
11: Calculate eB(k j+1
l )
12: if eB(k j+1
l )-eB(k j
l ) < ε or Event = Yes then
13: Pass the Parameters Θˆ j
kl
,W j
kl to QFE
14: Goto 4:
15: end if
16: j = j +1
17: end for
18: end if
19: if eB(k0
l+1)-eB(k0
l ) < ε then
20: Stop PE Condition
21: end if
22: l = l +1
23: end for
6.3.3 TRACKING CONTROL PROBLEM FOR LINEAR INTERCONNECTED SYS￾TEMS.
Converting a tracking problem into a regulation problem with augmented system dynamics is a
common approach used in control theory to simplify the control design process. An overview of
this conversion process is given in Chapters 1 and 2. Here we shall briefly look at this conversion
for the interconnected system so that the Q-learning algorithm developed in this chapter can be used
to address the tracking problem for linear interconnected systems. In a tracking problem, the goal is
to make the system output (or states) follow a desired reference trajectory or signal. We begin with
the definition of a feasible reference trajectory for the i
th subsystem as xi,d(t) ∈ Rn
i . Assume that the
feasible reference trajectory xi,d(t) ∈ Rn
i for the i
th subsystem is generated by a reference system
represented by
x˙i,d(t) = Ai,dxi,d(t) +
N
∑
j=1, j=i
Ai j,dx j,d(t), xi,d(0) = xid0, (6.40)
where xi,d(t) ∈ Rni is the reference state of the i
th subsystem, Ai j,d ∈ Rni×nj is the coupling matrix,
and Ai,d ∈ Rni×ni is the internal dynamics with xi,d(0) = 0 for the generator system.Large-scale Linear Systems 215
Define the error between the system state and the reference state as the tracking error, given by
ei,r(t)  xi(t)−xi,d(t). Then, the tracking error system, utilizing (6.1) and (6.40), can be defined by
e˙i,r(t) = x˙i(t)−x˙i,d(t) = Aixi(t) +Biui(t) +∑N
j=1
j=i
Ai jxj(t)−Ai,dxi,d(t)−
N
∑
j=1, j=i
Ai j,dxj,d(t).
(6.41)
The steady-state control inputs corresponding to the desired trajectory is given by
ui,d(t) = B+
i (Ai,dxid +∑
j
Ai j,dxj,d −Aixi,d −∑
j
Ai jxj,d),
where B+
i is defined as (BT
i Bi)−1BT
i and is assumed to have full-column rank. By augmenting the
tracking error ei,r and desired trajectory xi,d, the dynamics of the augmented tracking error system
can be represented as
χ˙i(t) = Giχi(t) +Hiui,d(t) +Hiωi(t) +
N
∑
j=1, j=i
Ki jχj(t), (6.42)
where χi  [eT
i,r xT
i,d]
T ∈ R2ni is the augmented state with χi(0)=[eT
i,r(0) xT
i,d(0)]T = χi0,
Gi ∈ R2ni×2ni is given by 
Ai Ai −Ai,d
0 Ai,d

, Hi ∈ R2ni×mi given by 
Bi
0

, and Ki j is given by

Ai j Ai j −Ai j,d
0 Ai j,d

.
The augmented error system in (6.42) transforms the tracking problem into a regulation prob￾lem. We can follow the steps discussed earlier in this chapter to introduce the network losses and
derive a sampled data system, which can then be used to synthesize the tracking control using the
Q-learning-based control scheme presented in this chapter. With the augmented system, we can de￾sign a feedback controller that stabilizes the augmented system at the desired operating point. This
controller will implicitly regulate the original system to track the reference trajectory. The gains of
the controller need to be tuned to achieve the desired tracking performance and stability.
Next, we shall see some examples where the hybrid learning algorithm is used in the event-based
Q-learning framework online to synthesize distributed control policies.
Example 6.1. A system of N interconnected inverted pendulums coupled by a spring is considered
for the verification of the analytical design presented in this chapter. The dynamics are
x˙i(t) =  0 1
g
l − aik
ml2 0

xi(t) + 0
1
ml2

ui(t) + ∑
j∈Ni
 0 0
hi jk
ml2 0

xj(t),
where l = 2, g = 10, m = 1, k = 5 and hi j = 1 for ∀ j ∈ {1,2,..,N}. The system is open loop unstable.
Ideal network: The system is discretized with a sampling time of 0.1s. With Pi = I2×2 and Ri =
1, ∀i = 1,2,3, the initial states for the system was selected as x1 = [2 −3]
T , x2 = [−1 2]
T and
x3 = [−1 1]
T and W(0) = 500,λ = 0.6,Wmin = 250. For the PE condition, Gaussian white noise
with zero mean and 0.2 standard deviation was added to the control inputs. The initial parameters
of the QFE is obtained by solving the SRE of the nominal model of the system by introducing a
multiplicative uncertainity factor (e.g., Ai = εAi and Bi = εBi for all i) with ε = (0.2,1). Under
the ideal case, without network-induced losses, the comparisons between the Q-learning using PI
versus hybrid learning scheme and time-driven Q-learning versus the hybrid learning scheme are
shown in Figures 6.3, respectively. Both these comparisons demonstrate that the convergence rate216 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Figure 6.3 Estimation error comparison between event-triggered PI, hybrid learning, and event-triggered
TD algorithms when the network is lossless.
Figure 6.4 Controller performance with delays.
is fastest for the PI while the hybrid learning scheme with event-sampled feedback is faster when
compared with the time-driven Q-learning. The advantage of hybrid algorithm over the time-driven
Q-learning is due to the iterative parameter updates embedded within the inter-event period. On the
other hand, unlike hybrid learning scheme, the PI cannot be implemented in real-time.
Monte-Carlo analysis: The simulation is carried out with random delays (d¯= 2) introduced by
the network. The delay is characterized by normal distribution with 80 ms mean and 10 ms standard
deviation and a Monte-Carlo analysis is carried out for 500 iterations. In the case where the random
delays are considered, the state and control trajectories are stable during the learning period as seen
in Figure 6.4. The comparison between the time-driven Q-learning and the hybrid learning schemes
as seen in Figure 6.5(b) shows that parameter error convergence in the hybrid scheme is much faster,
which shows that the hybrid learning algorithm is more robust than the time-driven Q-learning in
the presence of delays. This is partly due to augmented state vector and iterative parameter learning
within the event-sampled instants.
Random packet-losses characterized with Bernoulli distribution is introduced keeping the prob￾ability of data lost at 10%. All design parameters are kept the same. Table 6.2, lists the convergenceLarge-scale Linear Systems 217
Time-driven Q-learning
Hybrid learning algorithm
Time-driven Q-learning
Hybrid learning algorithm
Figure 6.5 Estimation error comparison (a) with 10% packet loss. (b) without packet loss.
Table 6.2
Comparison of parameter error convergence time
Mean-delay (in
ms)
% Data-drop out Convergence time (in sec)
Time-driven
Q-learning
Hybrid Learning
algorithm
0 0 13.7 10.6
30 10 61.0 36.9
25 246 190.0
80 10 632.0 317.0
25 486.5 269.0
100 10 239.0 198.0
25 637.3 239.8
time for the parameter estimation error for the existing time-driven Q-learning algorithm and the
hybrid learning algorithm. The error threshold was defined as 10−2 and the design parameters were
unchanged. In the ideal case, when there are no network losses, the difference in the convergence
time for the two algorithms is small. As the network losses are increased, the parameter error con￾verges to the threshold much faster with the hybrid learning algorithm. It is clear that with the hy￾brid learning scheme the estimation error converges much quicker than the time-driven Q-learning
scheme per the information given in the Table 6.2. The total number of events, the state, and the
control policy during the learning period are shown in Figures 6.6(a) and 6.6(b) respectively. With
the hybrid learning algorithm, the stability of the system is not affected during the learning period.
As the events are spaced out, more number of iterative parameter updates take place within the
inter-event period.
6.4 CLOSING REMARKS
The hybrid Q-learning-based scheme for a large-scale interconnected system appears to guarantee a
desired performance. The stability conditions for the closed-loop system during the learning period
is derived using the Lyapunov stability analysis and the asymptotic results hold when we move from
isolated systems (Chapter 3) to large-scale systems with event-triggered Q-learning-based online
control scheme. The Q-function parameters for the entire system are estimated at each subsystem
with the event-sampled inputs, states, and past state vectors, which is computationally not very
efficient. This control scheme does not impose any assumptions on the interconnection strengths.
The mirror estimator is not used in the event-sampling mechanism and reference models for each
subsystems are also not needed. Finally, with embedded iterative updates for parameters in the218 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Figure 6.6 (Left panel)(a) Inter-event time and (b) cumulative trigger instants (Right panel) Controller per￾formance with packet-loss. (a) State trajectories and (b) control inputs
hybrid learning scheme, the learning process is decoupled from the event-triggering instants.
6.5 PROBLEMS
6.5.1 (Lewis et al., 2012b) The longitudinal dynamics of an F-16 aircraft in straight and level flight
at 502 ft/sec are given by
x˙(t) = Ax(t) +Bu(t) =−2.0244×10−2 7.8761 −3.2169×101 −6.502×10−1
−2.5373×10−4 −1.0189 0 9.0484×10−1
0 00 1
7.9472×10−11 −2.498 0 −1.3861

x (6.43)
+
 −1×10−2
−2.09×10−3
0
−1.99×10−1

u (6.44)
The state is x =  vT α θ q T
, with vT the forward velocity, α the angle of attack, θ
the pitch angle, and q the pitch rate. The control input u(t) is the elevator deflection δe.
(a) Plot the response of the open-loop system to initial conditions of x0 =  0 0.101 T
(note that the angular units are in radians).
(b) Try MATLAB function lqr to perform linear quadratic regulator design. Use Q = I,R = 0.1.
6.5.2 For the system described in the previous problem, use the Q-learning algorithm (Algorithm
6.1) with τ = 0 and γca = 1 to learn the control inputs for Q = I,R = 0.1. Compare the per￾formance of the controlled system with the event-triggered control algorithm given in Table
3.3.
6.5.3 Consider a of linear dynamical system given by
x˙(t) = Ax(t) +Bu(t), (6.45)
where A =  0 −1
1 0 
and B =  1 0
0 1 
. Use the Q-learning algorithm (Algorithm 6.1) with τ = 0
and γca = 1 to learn the control inputs for Q = I,R = 0.1.
6.5.4 For a linear system taking the form d
dt x(t) = Ax(t)+γcaBu(t −τ), where τ = 0.05 and γca is a
random variable characterized by Bernoulli distribution with the probability of data lost is 5%.
Use the Q-learning algorithm (Algorithm 6.1) to learn the control inputs for Q = I,R = 0.1I.
Use the matrices A and B as given in Problem 6.5.1.
6.5.5 Now consider the channel losses for the system described in 6.5.3. The new dynamics will
take the form d
dt x(t) = Ax(t) +γcaBu(t −τ), where τ and γca are random variables character￾ized by Gaussian distribution with mean 10 ms and 1 ms standard deviation, and BernoulliLarge-scale Linear Systems 219
distribution with the probability of data lost is 10%, respectively. Use the Q-learning algo￾rithm (Algorithm 6.1) to learn the control inputs for Q = I,R = 0.1I. What happens when τ
is a random variable following a Gaussian distribution with mean 30 ms and 1 ms standard
deviation?
6.5.6 Consider the large-scale system composed of N interconnected inverted pendulums coupled
by a spring. The dynamics are
x˙(t) =  0 1
g
l − aik
ml2 0

xi(t) +γca  0
1
ml2

ui(t −τ) + ∑
j∈Ni
 0 0
hi jk
ml2 0

xj(t),
where l = 5, g = 10, m = 5, k = 5 and hi j = 1 for ∀ j ∈ {1,2,..,N}. The system is open-loop
unstable.
(a) Use the Q-learning algorithm (Algorithm 6.1) with τ being a random variable following
a Gaussian distribution with mean 100ms and 20ms standard deviation and γ is a random
variable characterized by Bernoulli distribution with the probability of data lost is 10% to
learn the control inputs for Q = 10×I,R = 0.001.
(b) Analyze the performance when the cost function is redefined with R = 10 and Q = 0.1×I.
6.5.7 Use the system defined in the previous problem and do not use the data history in the hybrid
Q-learning algorithm described in this chapter. Do the Q-function parameters converge? Try
using the gradient descent algorithm in place of RLS-based parameter updates. Do the Q￾function parameters converge?
6.5.8 Use the system defined in the Example 6.1. Do not use the covariance resetting technique to
reset the learning gain in the hybrid Q-learning algorithm described in this chapter. Does the
Q-function parameters converge?
6.5.9 Consider a three-area interconnected power system with HVDC links described in Pham et al.
(2019). This system can be represented as
x˙(t) = Ax(t) +
3
∑
i=1
Aix(t −hi) +
2
∑
i=1
Biu(t −di) +Γw(t),
y(t) = Cx(t),
(6.46)
where x(t)=[xT
1 , xT
2 , xT
3 ]
T ∈ Rn, u(t)=[uT
1 ,uT
2 ]
T ∈ Rm, w(t)=[wT
1 ,wT
2 ,wT
3 ]
T ∈ Rq, and y(t) =
[yT
1 ,yT
2 , yT
3 ]
T ∈ Rp with
A =
⎡
⎣
A11 A12 A13
A21 A22 A23
A31 A32 A33
⎤
⎦, C = diag(C1, C2 C3),
B1 =
⎡
⎣
Bd1 0n1×m2
0n2×m1 0n2×m2
0n3×m1 0n3×m2
⎤
⎦, B2 =
⎡
⎣
0n1×m1 0n1×m2
0n2×m1 Bd2
0n3×m1 0n3×m2
⎤
⎦,
A1 = diag(A11h , 0n2×n2 , 0n3×n3 ), A2 = diag(0n1×n1 , A22h , 0n3×n3 ),
and A3 = diag(0n1×n1 , 0n2×n2 , A33h ). The system matrices are defined as
Aii =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
−Di
Mi 0 1
Mi
1
Mi
−1
Mi 0 −1
RgiTgi
−1
Tgi
0 0 00
0 1
Tti
−1
Tti 0 00
0 00 −1
Tei 0 0
0 000 −1
Tdci 0
1 0 0 0 00
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
, i = 1,2, A12 = A21 = [0],220 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Ai3 =
 05×1 05×3
−1 01×3

, Aiih =
⎡
⎣
03×1 03×5
−Kei
Tei ρei 01×5
02×1 02×5
⎤
⎦, Bdi =

01×4
Kdci
Tdci 0
T
, i = 1,2,
Ci =
⎡
⎣
1 01×3 0 0
0 01×3 1 0
0 01×3 0 1
⎤
⎦, i = 1,2, A31 = A32 =
 01×4
1
M3 0
03×4 03×1 03×1

,
A33 =
⎡
⎢
⎢
⎢
⎣
−D3
M3 0 1
M3
1
M3 −1
Rg3Tg3
−1
Tg3 0 0
0 1
Tt3
−1
Tt3 0
0 00 −1
Te3
⎤
⎥
⎥
⎥
⎦
, C3 =  1 01×(n3−1)

, Γi =
 −1
Mi 01×(ni−1)
T
,
A33h =
 03×1 03×3
−Ke3
Te3 ρe3 01×3

, Γ3 =
 −1
M3 01×(n3−1)
T
.
For this system, select the system parameters as Tti = 0.3, Tgi = 0.08, Rgi = 2.4, ρe1 = 0.9/2.4,
ρe2 = 1.1/2.4, ρe3 = 1/2.4, Mi = 0.1667, Tdci = 0.1, Tei = 1, Kei = 1, Kdc1 = 0.0191, Kdc2 =
0.0164, Di = 0.0083, αi j = 1 , the rated power is 2000 MW, and design a Q-learning-based
control policy using Algorithm 6.1. Assume hi = 0 and di is a random variable sampled from
Gaussian distribution with mean 0.1 and variance 0.05.
6.5.10 For the system described in the previous problem, can the Algorithm 6.1 be applied to the
system defined in (6.46) when the delays modeled by hi are given by Gaussian distribution?7 Nonlinear Interconnected
Systems
CONTENTS
7.1 Introduction ........................................................................................................................... 221
7.2 Nonlinear Interconnected Systems ........................................................................................ 223
7.2.1 System dynamics ...................................................................................................... 223
7.2.2 Event-based optimal control policy .......................................................................... 224
7.3 Distributed Controller Design Using State and Output Feedback......................................... 225
7.3.1 State feedback controller design............................................................................... 226
7.3.2 Event-triggered output feedback controller .............................................................. 228
7.3.3 Stability Analysis...................................................................................................... 231
7.4 Distributed Controller Design Using Nonzero Sum Games.................................................. 239
7.4.1 Problem Reformulation ............................................................................................ 240
7.4.2 Game-Theoretic Solution.......................................................................................... 241
7.4.3 Approximate Solution to the NZS Game.................................................................. 242
7.4.4 Event-based Approximate Nash Solution................................................................. 245
7.4.5 Distributed Control of Linear Interconnected Systems ............................................ 250
7.5 Tracking Control of Nonlinear Interconnected Systems ....................................................... 253
7.6 Concluding Remarks ............................................................................................................. 254
7.7 Problems................................................................................................................................ 255
This chapter, based on the work presented in Narayanan and Jagannathan (2017) and Narayanan
et al. (2018c), introduces approximate optimal distributed control schemes for large-scale intercon￾nected systems governed by nonlinear dynamcis. We shall leverage event-triggered state feedback
control scheme and the hybrid learning approach introduced in Chapter 6 to design distributed
nonlinear controllers that learn optimal policies online. Here we shall explore the learning-based
synthesis of distributed optimal adaptive controllers with both state and output feedback. To allevi￾ate the need for complete state measurements, an extended nonlinear observer is introduced at each
subsystem to estimate the system’s internal states using measurable feedback data. We shall also see
that the optimal control design for the interconnected system can be reformulated as an N-player
dynamic game. The game-theoretic formulation for large-scale interconnected systems, presented
in Narayanan et al. (2018c), introduces a systematic way to optimize local (subsystem level) as well
as global (overall system level) objectives. An ADP-based approach with critic NNs is introduced
to approximate the Nash equilibrium solution of the game by learning the solution to the coupled
Hamilton-Jacobi (HJ) equations for the continuous and the event-sampled closed-loop systems. We
shall see that modeling the distributed optimal control in a game-theoretic setting offers additional
flexibility in terms of fine-tuning the performance of the interconnected systems.
221222 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
7.1 INTRODUCTION
In this chapter, we shall investigate the learning-based control problem for nonlinear interconnected
systems, an area of active research in the control community (Mehraeen and Jagannathan, 2011;
Wang et al., 2014; Liu et al., 2014; Spooner and Passino, 1999; Huang et al., 2003; Narendra and
Mukhopadhyay, 2010). As we navigate the realm of cyber-physical systems (CPS) and internet-of￾things (IoT), we encounter intricate networks of interacting dynamic units in diverse applications.
Controlling these interconnected subsystems presents unique challenges in designing distributed
control algorithms, which we will continue to explore in this chapter, for nonlinear interconnected
systems. When the interconnection or the coupling dynamics are modeled by nonlinear functions,
distributed control policies using adaptive terms have been reported in the literature (Huang et al.,
2003; Narendra and Mukhopadhyay, 2010). These control policies incorporate additional compen￾sation terms in the control input, finely tuned to learn interconnection dynamics and effectively
cancel their effects.
One of the impediments common to these approaches is in implementing the distributed con￾trol algorithm, as it introduces communication overhead due to sharing of measurement (outputs or
states) information among subsystems. As we have seen in the earlier chapters, to mitigate these
costs, event-triggered controllers are proposed (Guinaldo et al., 2011; Wang and Lemmon, 2011a;
Tallapragada and Chopra, 2013; Mazo and Tabuada, 2011; Garcia and Antsaklis, 2013; Sahoo et al.,
2016; Zhong and He, 2016). In order to extend the learning-based optimal control scheme pre￾sented for linear interconnected system in Chapter 6 to the nonlinear setting, we should consider the
(coupled) Hamilton-Jacobi-Bellman (HJB) equation associated with the nonlinear interconnected
system. When the system dynamics are nonlinear, since the HJB equation does not have a closed￾form solution (Werbos, 1995), inspired by the reinforcement RL techniques (Sutton and Barto,
1998), a suite of learning algorithms based on dynamic programming have been proposed, which
we have reviewed in Chapter 2. These learning ADP algorithms generate an approximation of the
optimal value function and an approximate optimal control policy (Werbos, 1995; Liu et al., 2014;
Mehraeen and Jagannathan, 2011; Wang et al., 2014; Prokhorov et al., 1995; Xu and Jagannathan,
2013; Lewis et al., 2012a; Barto et al., 2004; Dierks and Jagannathan, 2010a; Sahoo et al., 2016;
Chen and Jagannathan, 2008; Wang et al., 2015; Zhong and He, 2016; Vignesh and Jagannathan,
2016; Liu et al., 2012; Xu et al., 2014a).
Typically, as discussed in Chapter 4, the optimal value function is approximated by using an
artificial NN without solving the HJB equation directly. In order to learn the NN weights, the HJB
residual error, which is a continuous time equivalent of the Bellman error, is used. The RL-based
online ADP methods (Liu et al., 2014; Mehraeen and Jagannathan, 2011) applied to interconnected
systems typically requires extensive computations and exchange of feedback information among
subsystems through a communication network. Comparing with the traditional ADP design, we
have seen in the earlier chapters that the event-based method samples the state and updates the
controller only when it is necessary.
On the other hand, in the realm of RL theory, the Generalized Policy Iteration (GPI) algorithm,
inspired by classical dynamic programming, was introduced to mitigate the requirement for precise
knowledge of state transition probabilities and reward distributions, as outlined by Sutton (1998)
(Sutton and Barto, 1998). In the GPI algorithm, policy evaluation and improvement are two iterative
steps. Various RL schemes are developed based on the number of iterations in each step, aimed at
generating a sequence of control actions to maximize a specific reward function. In Chapter 2, we
reviewed examples of these algorithms, such as policy and value iteration schemes. The policy
evaluation step learns the optimal value function and the policy improvement step learns the greedy
action. For online control algorithms, the temporal difference learning (TDL)-based RL schemes
with one-step policy evaluation are more suitable. In TDL methods, using the one step feedback
and the estimated future cost (bootstrapping), the value function parameters are updated (Sutton
and Barto, 1998). Inspired by the TD ADP design in (Dierks and Jagannathan, 2010a) and based onNonlinear Interconnected Systems 223
the developments in (Narayanan and Jagannathan, 2017), in the first part of this chapter, we shall
develop an online learning framework for interconnected systems by using event-triggered state and
output information. Several NNs (Lewis et al., 1998) will be used for estimating the optimal value
functions by minimizing the HJB error (Sutton and Barto, 1998). To overcome the requirement
of larger inter-event time as demanded by the event-based PI or VI algorithm, and to reduce the
convergence time of the event-based TD learning algorithm, a TD-ADP scheme combined with
iterative learning between two event sampling instants is developed, which is similar to the hybrid
algorithm in Chapter 6.
As the event-triggering instants are decided based on a dynamic condition, the time between
any consecutive events is not fixed. Therefore, embedding finite number of iterations to tune the
NN weights while assuring stable operation is non-trivial, especially due to the fact that the initial
NN parameters and the initial control policy play a vital part in determining the stability during
the learning phase. We shall first see the design of learning-based controllers when the state vector
of each subsystem is communicated to others. Next, to relax the requirement of measuring the
entire state vector, we will design nonlinear observers at each subsystem to estimate the overall
system state vector using outputs that are communicated only at event-based sampling instants from
the other subsystems. In the second part of the chapter, we will revisit the design of the objective
function for the subsystems based on which the local as well as the global system objectives are
specified to the learning mechanism.
7.2 NONLINEAR INTERCONNECTED SYSTEMS
7.2.1 SYSTEM DYNAMICS
Consider a nonlinear input-affine system composed of N interconnected subsystems. Let the dy￾namics of each subsystem be represented as
x˙i(t) = fi(xi) +gi(xi)ui(t) +∑N
j=1, j=i
Δi j(xi,x j),
xi(0) = xi0, yi(t) = Cixi(t),
(7.1)
where xi(t) ∈ Si ⊆ Rni×1 represents the state vector, ˙xi(t) ∈ Rni×1 represents the state derivative
with respect to time for the i
th subsystem, ui(t) ∈ Rmi represents the control action, fi : Rni → Rni ,
gi : Rni → Rni×mi , and Δi j : Rni×nj → Rni , represents the nonlinear dynamics, input gain function,
and the interconnection map between the i
th and j
th subsystems, respectively; yi(t) ∈ Rpi is the
output vector with Ci ∈ Rpi×ni , a constant matrix and Si is a compact set. The dynamics of the
augmented system are expressed as
X˙(t) = F(X) +G(X)U(t), X(0) = X0, (7.2)
where X(t) ∈ S ⊆ Rn×1, U(t) ∈ Rm, G : Rn → Rn×m, F : Rn → Rn, U = [u1
T ,.,uN
T ]
T ,
m = ∑N
i=1 mi, n = ∑N
i=1 ni, X˙ = [x˙
T
1 ,.,x˙
T
N]
T , G(X) = diag(g1(x1),.,gN(xN)), F(X) = [(f1(x1) +
∑N
j=2 Δ1 j)T ,.,(fN(xN) + ∑N−1
j=1 ΔN j)T ]
T , and S is a compact set obtained as a result of finite union
of Si. The following assumptions on the system dynamics will be made in the analysis presented in
this section.
Assumption 7.1. Each subsystem described by (7.1) and the interconnected system (7.2) are con￾trollable.
Assumption 7.2. The nonlinear maps F(X) and G(X) are Lipschitz continuous functions in the
compact set S.
Assumption 7.3. There exists gim and giM > 0 such that gim < gi(xi) ≤ giM, ∀i ∈ {1,..,N}.224 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Assumption 7.4. We shall begin with the assumption that the states are measurable. This will be
relaxed in the subsequent design using outputs and extended nonlinear observers. Delay and packet
loss in the communication network are assumed to be absent.
Define a subsequence {tk}k∈N ⊂ t to represent the event-triggering time-instants. The state of
the i
th subsystem at the sampling instant t
i
k is denoted as xi(t
i
k). During the inter-event period, latest
sensor measurements are not updated at the controller. The difference between the actual state and
the states available at the controller results in an event-sampling error given by
ei(t) = xi(t)−xi(t
i
k), t
i
k ≤ t < t
i
k+1. (7.3)
This error is reset to zero at the sampling instants due to the feedback update. A brief background on
the design of optimal controller using aperiodic, event-triggered feedback is presented in the next
subsection.
7.2.2 EVENT-BASED OPTIMAL CONTROL POLICY
Let the performance of the interconnected system (7.2), be evaluated using the following function
V(X(t),U(t)) =  ∞
t
[Q(X) +UT (τ)RU(τ)]dτ, (7.4)
where R ∈ Rm×m is a positive definite penalty matrix, penalizing control actions and Q : Rn → R
with Q(0) = 0, represent positive definite functions that penalize the states. Use the integral in (7.4)
to denote the infinite horizon value function V(X(t)) defined in S. If V(X(t)) and its derivative
are continuous in its domain, the time-derivative of the V(X(t)) can be obtained as described in
(Bertsekas, 2012; Dierks and Jagannathan, 2010a) and it is given by
V˙(X(t)) = −
Q(X) +UT (t)RU(t)

= ∂VT (t)
∂X(t) X˙(t). (7.5)
Assuming that a minimum of the value function exists and it is unique (Lewis et al., 2012a), the
optimal control policy can be obtained as
U∗(t) = −R
2
−1
GT (X(t)) ∂V(t)
∂X(t)
∗
. (7.6)
Substituting (7.6) in (7.5), the HJB equation is obtained as
H = Q(X) + ∂V∗
∂X
T
F(X)− 1
4
∂V
∂X
∗T
G(X)R−1GT (X)
∂V
∂X
∗
. (7.7)
When the feedback is aperiodic and event-based, the Hamiltonian in (7.7) between events can be
represented using a piecewise continuous control input as
H =
"
Q(X) +U∗T
(tk)RU∗(tk)
#
+
∂V∗
∂X
T
X˙(t). (7.8)
The piecewise continuous control policy that minimizes the Hamiltonian in (7.8) is defined as
U∗(tk) = −R−1
2 GT (Xe)
∂V∗
∂Xe
, (7.9)
with Xe = X(tk), the state held at the actuator using a ZOH circuit between tk and tk+1 for all k ∈ N.
The function approximation property of NNs with with event-triggered feedback is recalled next.Nonlinear Interconnected Systems 225
Neural network approximation using event-based feedback
With the following standard assumption, recall the effect of the aperiodic event-based feedback on
the approximation property of the NN from Chapter 4.
Assumption 7.5. The NN reconstruction error and its derivative, εi(x),∇xεi(x), the constant target
weights θ∗
i , and the activation function φ(x), which satisfies φ(0) = 0, are all bounded.
Remark 7.1. The NN approximation with state vector sampled at event-triggering instants as input
is a function of event-sampling error. Since the reconstruction error εe depends on the error due to
event sampling, a direct relationship between approximation accuracy and the frequency of events is
revealed. One of the motivations behind the hybrid learning algorithm is to decouple the relationship
between the accuracy of approximation and the sampling frequency. In chapter 4, this trade-off is
handled by designing the event-triggering condition based on the estimated weights and the states
of the system. This resulted in an inverse relationship between the inter-event time and the weight
estimation error, thereby forcing more events when the difference between the estimated NN weight
and the target weights is large.
7.3 DISTRIBUTED CONTROLLER DESIGN USING STATE AND OUTPUT
FEEDBACK
In this section, we shall begin with the design of distributed approximately optimal controller with
state feedback using a hybrid learning scheme. Later, we shall derive nonlinear observers to relax the
requirement of full state measurements. Before learning an optimal value function, it is important to
assess the existence of such a function, which is related to the existence of solution to an associated
partial differential (HJB) equation. Therefore, the following assumption is needed to proceed further.
Assumption 7.6. V∗(X) ∈C1(S) is a unique solution to the HJB equation introduced in the previous
section, where C1(S) represents the class of continuous functions defined in a closed and bounded
set S and have continuous derivatives in S.
Proposition 7.1 (Vignesh and Jagannathan (2016)). Consider the augmented system dynamics in
(7.2) with the individual subsystems (7.1), ∀i ∈ {1,2,..,N}, ∃u∗
i (t) for t ∈ [0,∞), which is a function
of X(t), such that the cost function (7.4) is minimized.
Proof: First, consider the infinite horizon value function defined by (7.4) for the augmented
system in (7.2). Define
R = diag(R1,R2,..,RN), Q(X) = ∑N
i=1 Qi(X),
U(X(t)) = [uT
1 ,.,uT
N]
T
, ∂VT
∂X =

∂VT
1
∂ x1
,
∂VT
2
∂ x2
.......∂VT
N
∂ xN

,
and
V(X(t)) =  ∞
t
 N
∑
i=1

Qi(X) +ui
TRiui


dτ =
 ∞
t
N
∑
i=1
Vi(X(τ))dτ. (7.10)
The Hamiltonian (7.8) becomes
H(X,U,
∂V
∂X ) = 
∂VT
1
∂ x1
,..., ∂VT
N
∂ xN

[x˙
T
1 (t),..., x˙
T
N(t)]T
+
 N
∑
i=1

Qi(X) +ui
TRiui


=
N
∑
i=1
Hi(X,ui,
∂VT
i
∂ xi
), where Hi(X,ui,
∂VT
i
∂ xi
) = ∂VT
i
∂ xi
x˙i +Qi(X) +ui
TRiui

.
(7.11)226 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
For optimality, each subsystem should generate a control policy from Hi(X,ui,
∂VT
i
∂ xi ) as
ui
∗(t) = −1
2
Ri
−1gi
T (xi)
∂Vi
∗
∂ xi
, ∀i ∈ 1,2,..N. (7.12)
By designing controllers at each subsystem to generate (7.12), the cost function (7.4) of the aug￾mented system is minimized.
Remark 7.2. A strictly decentralized controller can be realized by designing (7.12) as a function
of xi(t). Despite the simplicity of such controller, the work by Narendra and Mukhopadhyay (2010)
highlighted the unacceptable performance observed, especially in the transient period, as a result of
such design approach. Therefore, (7.12) is desired to be a function of X(t) and it can be considered
as
ui
∗(t) = −1
2
Ri
−1gi
T (xi)
∂V∗
i,i
∂ xi
− 1
2
Ri
−1gi
T (xi)∑N
j=1
j=i
∂V∗
i, j
∂ xi
, (7.13)
where V∗
i, j is the cost due to interconnections and V∗
i,i is the optimal cost of the ith subsystem when
the interconnections are absent and V∗
i = V∗
i,i +V∗
i, j
. The control policy as expressed in (7.13) is
composed of two parts. The first part denotes the optimal control policy for a decoupled subsystem
wherein the interconnections are absent while the second part compensates for the interconnections.
Remark 7.3. Note that the control policy (7.12) is considered to be distributed and it is equivalent
to (7.13). In the decentralized control policy, the second term in (7.13) is zero (Mehraeen and Ja￾gannathan, 2011). This term explicitly takes into account the interconnection terms in the subsystem
dynamics and it is expected to compensate for the interconnections. An equivalent control policy for
the linear interconnected system using Riccati solution can be obtained as
u∗
i (t) = −kiixi(t)−∑N
i=1
i=j
ki jxj(t).
Here, kii and ki j are the diagonal and off-diagonal entries of the Kalman gain matrix corresponding
to the optimal controller for the interconnected system (7.2) with linear dynamics.
7.3.1 STATE FEEDBACK CONTROLLER DESIGN
We shall use the artificial NNs to represent the optimal value function in a parametric form using NN
weights and a set of basis function with a bounded approximation error. Using the parameterized
representation, the value function can be represented as
Vi(X) = θ T
i φ(X) +εi(X),
where φ(X) is a basis function and εi(X) is the bounded approximation/reconstruction error. Let the
target NN weights be θ∗
i and the estimated NN weights be θˆ
i at the i
th subsystem. The parameterized
HJB equation with approximate optimal value function can be obtained as
Qi(X) +θ∗T
i ∇xφ(x)
˙
f i(x)− 1
4
θ∗T
i ∇xφ(x)Di∇T
xφ(x)θ∗
i +εiHJB = 0, (7.14)
where
εiHJB = ∇xεi
T (
˙
f i(x)− Di
2 (∇T
xφ(x)θ∗
i +∇xεi)) + 1
4
∇xεi
TDi∇xεi,
˙
f i(x) = fi(xi) +∑N
j=1
j=i
Δi j(xi,x j),Nonlinear Interconnected Systems 227
the partial derivative of the optimal cost function V∗
i
T with respect to xi is ∇T xφ(x)θ∗
i , and Di =
Di(xi) = gi(xi)Ri
−1gi
T (xi). Let ∇T xφ(x)θ∗
i  ≤ VxiM and Di ≤ DiM. Now, using the estimated
weights θˆ
i, the control input (7.12), can be written as
uˆi(t) = −1
2
R−1
i gT
i θˆT
i ∇xφ(x)
and the parameterized Hamiltonian equation is
Hˆi = Qi(X) +θˆT
i ∇xφ(x)
˙
fi(x)− 1
4
θˆT
i ∇xφ(x)Di∇T
xφ(x)θˆ
i. (7.15)
Equation (7.15) can be used to evaluate the value function for the given policy. Since it is a con￾sistency condition, if the estimated value function is the true optimal value function for the control
policy (7.12), then Hˆi = 0. Due to the estimated quantity θˆ
i, the value function calculated using the
estimated weights is not equal to the optimal value function. This will result in a HJB residual error
and Hˆi = 0 is no longer true. The estimates θˆ
i are now updated such that the HJB residual error is
minimized. Consider the Levenberg-Marquardt algorithm (Lewis et al., 2012a) to update the NN
weights. In this case, the weight estimates evolve based on the dynamic equation given by
˙
θˆi(t) = −αi1σiHˆi
(σT
i σi +1)
2 ,
where αi1 is the learning step and σi = ∇xφ(x)
˙
fi(x) − 1
2∇xφ(x)Di∇T
x φ(x)θˆ
i. This weight tuning
rule ensures the HJB residual error convergence while stability of the closed-loop system when
the estimated weights are used in the control policy is not a given, especially, if the initial control
policy is not stabilizing. Therefore, to relax the dependence on the initial control policy in dictating
the stability of the closed-loop system, a conditional stabilizing term was appended in the weight
update rule proposed by Dierks and Jagannathan (2010a). Together with this term, we may use the
following weight update rule
˙
θˆi(t) = − αi1
(σT
i σi +1)
2 σiHˆi +
1
2
βi∇xφ(x)DiLix(xi)−κiθˆ
i, (7.16)
where κi,βi are positive design parameters and Lix(xi) is the partial derivative of a positive definite
Lyapunov function for the i
th subsystem with respect to the state. Since the controller has access to
the feedback information only when an event is triggered, (7.16) will have to be slightly modified
and this will be presented in the next subsection.
Remark 7.4. By utilizing the nonlinear maps gi, the stabilizing term in (7.16) is appended to the NN
weight tuning rule to relax the requirement of initial stabilizing control (Dierks and Jagannathan,
2010a). In the event-triggered implementation of the controller, the stabilizing term in the update
rule ensures stability of the closed-loop system at the event-based sampling instants and the sigma￾modification term ensures that the weights are bounded.
The event-triggered state feedback controller design is introduced next.
7.3.1.1 Event-triggered state feedback controller
For the near optimal distributed control design with event-triggered state feedback, the error (7.3)
introduced due to aperiodic feedback will drive the control policy between two event-based sam￾pling instants. With the estimated optimal value function and the estimated optimal control policy,
the Hamiltonian is represented as
Hˆi(X,uˆi,e,
∂Vˆ T
i
∂ xi
) = ∂Vˆ T
i
∂ xi
x˙i,e(t)+[Qi(Xe) +uˆ
T
i,e(t)Riuˆi,e(t)], (7.17)228 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Table 7.1
State feedback controller design equations
Subsystem dynamics ˙xi(t) = fi(xi) +gi(xi)ui(t) +∑N
j=1, j=i Δi j(xi, x j), xi(0) = xi0
HJB residual error Hˆi = Qi(X) +θˆT
i ∇xφ(x)
˙
fi(x)− 1
4θˆT
i ∇xφ(x)Di∇T xφ(x)θˆ
i
TD Learning ˙
θˆi(t) = − αi1
(σT
i σi+1)
2 σiHˆi + 1
2βi∇xφ(x)DiLix(xi)−κiθˆ
i
Value function Vˆ(X) = θˆT φ(X),
Distributed control law ui = −1
2Ri
−1gi
T (xi)
∂Vˆ
i,i
∂ xi − 1
2Ri
−1gi
T (xi)∑N
j=1
j=i
∂Vˆ
i, j
∂ xi
where (.)e denotes the influence of (7.3) due to event-based feedback and this notation will be
followed henceforth. Using the parameterized representation of the approximate value function, we
get
Hˆi = Qi(Xe) +θˆT
i ∇xφ(xe)
˙
fi(xe)− 1
4
θˆT
i ∇xφ(xe)Di,ε∇T
xφ(xe)θˆ
i, (7.18)
where Di,ε = Di(xi,e). Finally, we can use a NN weight tuning rule to minimize the HJB residual
error. For instance,
˙
θˆi(t) = $
−αi1
ρ2 σiHˆi + 1
2βi∇xφ(x)DiLix(xi)−κiθˆ
i, t = t
i
k
0, t ∈ (t
i
k,t
i
k+1), (7.19)
where ρ = (σT
i,eσi,e +1). The estimated NN weights, θˆ
i, at each subsystem are not updated between
events. To determine the time instants tk, a decentralized event-triggering condition is required. De￾fine a locally Lipschitz Lyapunov candidate function, Li(xi) for the i
th subsystem such that Li(xi) > 0
for all xi ∈ S\{0}, with {0} denoting the singleton set containing zero vector. Events are generated
such that the following condition is satisfied
Li(xi(t)) ≤ (1+tk −t)ΓiLi(xi(tk)), tk ≤ t < tk+1, (7.20)
with 0 < Γi < 1. Note that the event-triggering condition (7.20) requires only the local states. Also
note that the kth event sampling instant at any two subsystems need not be the same and t
i
k used in the
equations above represents the time instant of the occurrence of the kth event at the i
th subsystem.
Since the estimated weights are not used in (7.20) a mirror estimator is not required (Sahoo et al.,
2016). Next, the nonlinear observer which utilizes the output from the subsystems obtained at event￾based sampling instants to reconstruct the internal state information is presented, which requires the
following standard assumption.
Assumption 7.7. The subsystems are assumed to be observable. This is required to enable recon￾struction of the states from the measured outputs.Nonlinear Interconnected Systems 229
Table 7.2
Event-triggered state feedback controller design equations
Subsystem dynamics ˙xi(t) = fi(xi) +gi(xi)ui,e(t) +∑N
j=1, j=i Δi j(xi, x j), xi(0) = xi0
HJB residual error Hˆi = Qi(Xe) +θˆT
i ∇xφ(xe)
˙
fi(xe)− 1
4θˆT
i ∇xφ(xe)Di,ε∇T xφ(xe)θˆ
i
TD Learning ˙
θˆi(t) = $
−αi1
ρ2 σiHˆi + 1
2βi∇xφ(x)DiLix(xi)−κiθˆ
i, t = t
i
k
0, t ∈ (t
i
k,t
i
k+1).
Value function Vˆ(X) = θˆT φ(X),
Distributed control law ui,e(t) = −1
2Ri
−1gi
T (xi,e)
∂Vˆ
i,i
∂ xi,e − 1
2Ri
−1gi
T (xi,e)∑N
j=1
j=i
∂Vˆ
i, j
∂ xi,e
7.3.2 EVENT-TRIGGERED OUTPUT FEEDBACK CONTROLLER
Output feedback controllers use the measured quantity to estimate the internal system states using
observers. The estimated states are then utilized to design the controllers. Since it is desired that
the outputs be communicated among subsystems, the observers at each subsystem are designed so
that they estimate the state vector of all the subsystems using the event-triggered outputs. To avoid
redundancy, all the equations for the controller are not explicitly presented for output feedback￾based design. For the implementation of output feedback controller, estimated states will replace
the actual states in the design equations presented in the previous subsection. In order to develop
an event-triggering condition, we could substitute the outputs in place of the states in (7.20). In
the analysis, the event-triggering condition can be represented in terms of the state vector using the
linear map Ci. In order to estimate the system state vector using the output information obtained at
the event-based sampling instants, consider the observer at i
th subsystem with dynamics
˙
Xˆi(t) = F(Xˆi) +G(Xˆi)Ui,e(t) + μi[Yi,e(t)−CXˆi(t)], (7.21)
where Xˆi,μi, and Yi,e represent the overall estimated state vector, observer gain matrix, and event￾triggered output vector of the overall system, respectively, at the i
th subsystem, C is the augmented
matrix composed of Ci, each with appropriate dimensions. The output vector is a function of the
measurement error since the output from each subsystem is shared only when an event is triggered.
Defining the difference between the actual state and the estimated state vectors at the i
th subsys￾tem as the state estimation error
X˜i(t) = Xi(t)−Xˆi(t),
the evolution of the state estimation error is described by the differential equation
˙
X˜i(t) = F(Xi) +G(Xi)Ui,e(t)−[F(Xˆi) +G(Xˆi)Ui,e(t)]− μi[Yi,e(t)−CXˆi(t)]. (7.22)
Next, the boundedness of the state estimation error with event-triggered output feedback is pre￾sented assuming the distributed control policy is admissible. The detailed proofs for the results
presented next are available in (Narayanan and Jagannathan, 2017).230 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Lemma 7.1. For the augmented system given in (7.2) composed of interconnected subsystems given
in (7.1), consider the observer (7.21) at each subsystem with the error dynamics (7.22), and let the
measurement error (7.3) be bounded. The observer estimation error is locally UUB, provided the
control policy is optimal and the observer gains are chosen such that ηi,o1,ηi,o2 > 0, where the
design variables ηi,o1,ηi,o2 are defined in the proof.
Sketch of Proof:
 Since the separation principle does not hold for nonlinear systems, the stability of the con￾trollers together with the observers, operating online, should be analyzed.
 The Lyapunov candidate function used in this proof is
Li(X˜i(t)) = 1
2
X˜ T
i (t)γiX˜i(t) + 1
4
(X˜ T
i (t)γiX˜i(t))2
.
 Using the estimation error dynamics (7.22) and Assumptions 7.3-7.4, the time-derivative of
the Lyapunov function can be obtained to as
L˙i(t) ≤ −ηi,o1

X˜i(t)


2
−ηi,o2

X˜i(t)


4
+ξi1,obs, (7.23)
where ηi,o1 = γi μi C − γiLf − 1.5, ηi,o2 = γi2 μi C − γi2
Lf − 3, and
ξi1,obs = 1
8G4
Mγi8
U∗
i 4 + 1
2G2
Mγi2
U∗
i 2
.
 Note that these bounds are only valid if the control policy is optimal. Further, the constants
γi,μi, and C, are all design variables that can be independently chosen to ensure that these
bounds hold.
Recall that the convergence of the NN weights is coupled with the number of events when the weight
update rule (7.19) is used. This significantly reduces the convergence time (Sahoo et al., 2016). To
decouple this relationship between the number of events and the learning time, a hybrid NN weight
adaption rule, from Narayanan and Jagannathan (2017), is introduced in the next subsection.
7.3.2.1 Hybrid learning algorithm
The results of event-based function approximation shows that the approximation error in the optimal
value function and the optimal control action generated will depend on the frequency of events. The
TD ADP scheme developed by Sahoo et al. (2016) presents an NN approximator wherein the NN
weight updates occur only at the event triggering instants t
i
k. In contrast, classical ADP schemes
(e.g., PI and VI) performs iterative learning, assuming significant iterations could be carried out
during the inter-event period.
Following the hybrid algorithm in Chapter 6, the learning scheme presented here is inspired by
the GPI and introduced in (Narayanan and Jagannathan, 2017). Specifically, the NN weights are
tuned using the tuning rule
˙
θˆi(t) = $−αi1
ρˆ 2 σˆiHˆi + 1
2βi∇xφ(xˆ)DˆiLix(xˆi)−κiθˆ
i, t = t
i
k
− αi1
ρˆ 2(t
i
k)
σˆi,e(t
i
k)Hˆi,e(θˆ
i(t))−κiθˆ
i(t), t
i
k < t < t
i
k+1.
(7.24)
To denote the use of estimated states from the observer, (ˆ.) notation is used for the functions Di,ρi,
and σi. Whenever an event occurs, new feedback information is updated at the controller and broad￾cast to the neighboring subsystems. The weights are tuned with the new feedback information and
the updated weights are used to generate the control action, which is applied at the actuator. In the
inter-event period, past feedback values are used to evaluate the value function and the policy using
the HJB equation. This is done by adjusting the estimated weights in the inter-event period accord￾ing to (7.24) so that θˆ
i moves towards θ∗
i . It was shown in (Narayanan and Jagannathan, 2017) thatNonlinear Interconnected Systems 231
Table 7.3
Design equations for hybrid learning with output feedback
Observer dynamics ˙
Xˆi(t) = F(Xˆi) +G(Xˆi)Ui,e(t) + μi[Yi,e(t)−CXˆi(t)]
HJB residual error Hˆi = Qi(Xˆe) +θˆT
i ∇xφ(xˆe)
˙
fi(xˆe)− 1
4θˆT
i ∇xφ(xˆe)Di,ε∇T xφ(xˆe)θˆ
i
Hybrid learning ˙
θˆi(t) = $ −αi1
ρˆ 2 σˆiHˆi + 1
2βi∇xφ(xˆ)DˆiLix(xˆi)−κiθˆ
i, t = t
i
k
− αi1
ρˆ 2(t
i
k)
σˆi,e(t
i
k)Hˆi,e(θˆ
i(t))−κiθˆ
i(t), t
i
k < t < t
i
k+1.
Value function Vˆ(Xˆ) = θˆT φ(Xˆ),
Distributed control law ui,e = −1
2Ri
−1gi
T (xˆi,e)
∂Vˆ
i,i
∂ xˆi,e − 1
2Ri
−1gi
T (xˆi,e)∑N
j=1
j=i
∂Vˆ
i, j
∂ xˆi,e
the stability of the system is preserved as a consequence of the additional stabilizing term in (7.24).
Using the actual states in place of the estimated states, the update rules for the hybrid learning
scheme can be derived for the state feedback controller.
Remark 7.5. As the time between two successive events increases, more time is available for the
iterative weight updates. Therefore, HJB residual error is reduced considerably resulting in an
approximately optimal control action at every event-triggering instant.
Remark 7.6. In the traditional RL literature, the GPI is used and a family of TD algorithms are
presented, such as TD(0), n-TD, TD(λ) (Sutton and Barto, 1998). All these learning algorithms
have a fixed number of iterative weight updates for policy evaluation. In contrast, the event-triggered
control framework cannot ensure fixed inter-event time. Hence, the hybrid algorithm is most relevant
and applicable in the event-based online learning control framework.
For the stability analysis, first, using the fact that the optimal control policy results in a
stable closed-loop system, a time-varying bound on the closed-loop dynamics can be defined
as F(X) +G(X)U∗(t) ≤ ψ X(t), with ψ > 0. It was also shown by Dierks and Jagan￾nathan (2010a) that there exists positive constant ζ1 such that Lx(X)  f(X) +g(X)U∗(t) ≤
−ζ1Lx(X)2
, with the Lyapunov function L(X), its gradient Lx(X) with respect to the state vector.
Example 7.1. We can consider the L(X(t)) = 1
2 (XT (t)X(t)) and linear system dynamics X˙(t) =
AX(t) +BU∗(t), where U∗(t) = −KX(t). Computing the time derivative of this function and using
the system dynamics, we get 
XT (t)

AX(t) +BU∗(t) ≤ −ζ1X(t)2
, where ζ1 can be selected
as the spectral-radius of the closed-loop system matrix.
With these results, the stability analysis of the state-feedback controller and the output feedback
controller with event-triggered feedback are summarized in the next section.232 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
7.3.3 STABILITY ANALYSIS
For the analysis of the event-triggered controller, first, we shall prove that the distributed controller
admits a Lyapunov function for the closed loop system which satisfies a local input-to-state stability￾like condition, resulting in local UUB of all the states, weight estimation error, and state estimation
error. Let the error in the NN weight estimate be defined as θ˜
i = θ∗
i −θˆ
i and the target weights be
constants and bounded by θiM. Consider the Hamiltonian (7.18), and the ideal HJB equation given
in (7.14), adding and subtracting Qi(X) in (7.18) and rewriting the Hamiltonian in terms of θ˜
i, we
get the following equations:
Hˆi,e = −θ˜T
i σi,e +
1
4
θ˜T
i ∇xφ(xe)Di,ε∇T
xφ(xe)θ˜
i +Qi(xe)
+θ∗T
i [∇xφ(xe)
˙
fi(xe)−∇xφ(x) ¯fi(x)]−εiHJB −Qi(X)
+
1
4
θ∗T
i [∇xφ(x)Di∇T
x φ(x)−∇xφ(xe)Di,ε∇T
xφ(xe)]θ∗
i
(7.25)
and, for the case of output feedback, we have
Hˆi = −θ˜T
i σˆi,e +
1
4
θ˜T
i ∇xφ(xˆe)Dˆi,ε∇T
xφ(xˆe)θ˜
i +Qi(Xˆe)
+
1
4
θ∗T
i [∇xφ(x)Di∇T
xφ(x)−∇xφ(xˆe)Dˆi,ε∇T
xφ(xˆe)]θ∗
i
−εiHJB +θ∗T
i (∇xφ(xˆe)
˙
fi(xˆe)−∇xφ(x)
˙
f i(x))−Qi(X).
(7.26)
For all the stability results presented in this section, detailed proofs are available in (Narayanan
and Jagannathan, 2017).
Theorem 7.1. Consider the nonlinear dynamics of the augmented system (7.2) with the equilibrium
point at origin. Let the initial states xi0,Xˆi0 ∈ S and let θˆ
i(0) be defined in a compact set Ωiθ . Use the
update rule defined in (7.16), with the estimated states, to tune the NN weights. With the estimated
states evolving according to the observer dynamics given by (7.21) and measurement error set to
zero, there exists ηi
s > 0 such that θ˜
i, X(t) and the observer error dynamics are locally uniformly
ultimately bounded (UUB) by ξicl in the presence of a bounded external input. The constants, ηi
s
and the bound, ξicl, are defined in the proof.
Sketch of Proof:
 In this proof, the Lyapunov function for the closed-loop system is selected as
Li(xi,θ˜
i,X˜i) = Li1(xi) +Li2(θ˜
i) +Li3(X˜i). (7.27)
 The proof involdes demonstrating that each term in the Lyapunov function has a negative
time-derivative outside of the bound defined by certain constants.
 This bound is defined as ξicl = ξi1,obs + 1
16R−4
i G4
M, where ξi1,obs is the bound contributed by
the observer (Lemma 7.1). Note here that this bound will increase when the measurement
error is nonzero. Since the magnitude of the measurement error is kept under a pre-defined
threshold, we shall see that the closed-loop system is locally UUB.
This analytical result in Theorem 7.1 is equivalent to the local ISS condition (Khalil, 2002). The
reconstruction and the measurement errors can be considered as external inputs to the system. How￾ever, the boundedness of the event-based measurement error is summarized in the next theorem
using the decentralized event-triggering condition introduced earlier in the section.Nonlinear Interconnected Systems 233
Theorem 7.2. Consider the nonlinear interconnected system described by (7.2) wherein the initial
states xi0,Xˆi0 ∈ S. Let the NN weights be initialized in a compact set Ωiθ . Consider the weight tun￾ing rule defined in (7.24) using the estimated states and the event-triggering mechanism satisfying
(7.20) with the measured outputs at each subsystem. With the estimated states evolving according
to the observer dynamics given by (7.21), there exists ηi
s > 0 such that θ˜
i, X(t), and the observer
error dynamics are locally uniformly ultimately bounded by ξicl wherein the bound is obtained in￾dependent of the measurement error. The constants, ηi
s and the bound, ξicl, are defined in the proof.
Sketch of Proof:
 Recall that the results from the previous theorem, it can be observed that when the event￾sampling error is zero, the bounds can obtained as in Theorem 7.1. To establish similar
results with event-triggered feedback, the event-triggering condition is used to bound this
error.
Corollary 7.1. Consider the nonlinear interconnected system given by (7.2) with origin being the
equilibrium point and the initial states xi0,Xˆ
i0 ∈ S. Let θˆ
i(0) be defined in a compact set Ωiθ . Use
the update rule defined in (7.16) to tune the NN weights at each subsystem. Then, there exists com￾putable positive constants αi1,βi,κi such that θ˜
i and X(t) are locally uniformly ultimately bounded
with the bounds ξθ ,ξx, respectively, when there is a non-zero bounded measurement error. Further,
using the event-sampling condition (7.20), it can be shown that the closed-loop system is locally
UUB when the NN weights are tuned using (7.19) and (7.24).
Sketch of Proof:
 Since the stability results for the state feedback controller can be obtained from Theorem
7.1 and Theorem 7.2 by setting the observer estimation error to zero.
Remark 7.7. Results from Theorems 7.1 and 7.2 can be used along with Assumption 7.2 to es￾tablish the non-zero minimum inter-event time (Wang and Lemmon, 2011a; Guinaldo et al., 2011).
However, since the inter-event time is dynamically changing, ensuring sufficient time availability to
carry out significantly large number of weight updates between any successive events is not feasible.
Therefore, algorithms like PI and VI are restrictive for event-based control implementation. Redun￾dant events can be prevented by using a dead-zone operator as soon as the states of each subsystem
converge to their respective bounds. The learning algorithm and the corresponding stability results
derived for the closed-loop nonlinear system can be easily extended for linear interconnected sys￾tem. The event-sampling mechanism at each subsystem operates asynchronously, resulting in lower
network congestion. However, suitable communication protocol is required to be utilized along with
the controller to minimize the packet losses due to collision and other undesired network perfor￾mance (Guinaldo et al., 2011).
Remark 7.8. The weight tuning rules for the online approximator in (7.24) are used for event￾triggered implementation of state and output feedback controllers. The bounds ξicl can be made
arbitrarily small by appropriate choice of αi1,βi, and κi in the weight update rule satisfying the
Lyapunov stability results. The iterative learning, presented in (Liu et al., 2014; Zhong and He,
2016; Lewis et al., 2012a), results in the value function approximate that yield approximately opti￾mal, hence, stabilizing control input at each time step. This yields θ˜
i = 0 for each of the algorithms
(Liu et al., 2014; Zhong and He, 2016; Lewis et al., 2012a), at each event-triggering instant, which
reduces the complexity of analysis. The stabilizing term 1
2βi∇xφ(xˆ)DˆiLix(xˆi) in the weight tuning
rule (7.24) ensures stability of the closed loop system in the presence of non-zero θ˜
i.234 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Remark 7.9. In the adaptive control theory, the sigma/epsilon modification (Narendra and An￾naswamy, 2012) terms in the adaptation rule ensures that the actual weights are bounded in the
presence of bounded disturbances. It also helps in avoiding the parameter drift and relaxes the PE
condition. In all the ADP designs (Lewis et al., 2012a), the PE condition is required for conver￾gence of the weight estimation errors and it is achieved by adding random signal to the control
policy (Dierks and Jagannathan, 2010a; Xu and Jagannathan, 2013; Sahoo et al., 2016). This also
has an additional benefit of being an exploratory signal. In RL literature, the dilemma of explo￾ration versus exploitation is greatly discussed (Sutton and Barto, 1998). For a learning problem,
the exploratory noise signal helps the learning mechanism to explore the search space to find the
exact solution and ensures observability conditions while learning (Lewis et al., 2012a). However,
for the online control problem, stability is more important and is given priority. Therefore, explicitly
adding random exploratory signal to the control policy is undesirable.
Remark 7.10. In the RL literature, the one step TD algorithm is proven to have convergence issues
(Sutton and Barto, 1998) due to bootstrapping. This occurs as the parameter values that approxi￾mate the value function grow unbounded (Sutton and Barto, 1998). However, convergence results
for online one-step TD algorithms are presented in (Dierks and Jagannathan, 2010a; Xu and Jagan￾nathan, 2013; Sahoo et al., 2016) under certain conditions. These algorithms utilize the stabilizing
terms in the parameter update rule and present local convergence.
Remark 7.11. For the output feedback controller, an additional uncertainty due to estimated states
is introduced during the learning period. Moreover, the computations are increased due the observer
present at each subsystem. The state estimation error forces frequent events when compared to
the state feedback controller, where the state estimation error is absent. However, for practical
applications, all the states are not measured and with output feedback, only the output vector is
broadcast through the network when compared to the entire state vector. The location of the observer
is crucial and there are several locations which are feasible to place an observer operating with
event-triggered feedback, as discussed in the literature (Tallapragada and Chopra, 2013; Zhong and
He, 2016). For the interconnected system, the extended observers discussed here are placed along
with the controller for the following reasons – a) only the output from each subsystem is broadcast
through the network; b) using the outputs from all the subsystem, the overall state vector can be
reconstructed at each subsystem, as required by the distributed controllers. These advantages are
lost when the observers are placed along with the sensors at each subsystem. In order to eliminate
an additional event-sampling mechanism at each subsystem, the observer states are held constant
between the event sampling instants. We shall, nevertheless, explore these design configurations in
much more detail in Chapter 9.
In this section, two examples are discussed in the context of the analytical design presented thus
far in this chapter. The first example includes a system of two inverted pendulums connected by
spring. We first consider a linear system and then a system with nonlinear dynamics is considered.
In the second example, a more practical nonlinear system with three interconnected subsystems is
considered.
Example 7.2. The example used here has two inverted pendulum connected by spring (Spooner
and Passino, 1999), which can be represented of the form (7.2). A NN with one layer and 5 neurons
together with polynomial basis set wherein the control variables α1 = 25,β = 0.01, Li(x) = 1
2 xT
i βxi
and φ(x) = 
x1,1
2,x1,2
2,x2,1
2, x2,2
2,xT x
T
; the initial conditions are defined in the interval [0,1] and
the initial weights of the NN are chosen randomly from [-1,1]. The dynamics of the system are given
by
x˙i1(t) = xi2, x˙i2(t)=(migr
Ji
− kr2
4Ji
)sinxi1 +
kr
2Ji
(l −b) + ui
Ji
+
kr2
4Ji
sinxj1.Nonlinear Interconnected Systems 235
0 1 2 3 4 5
−10
0
10
Time in sec
x
State trajectory
x11 x12 x21 x22
Figure 7.1 State trajectories (Linear example).
For the linear dynamics, refer (Guinaldo et al., 2011). The parameters in the system dynamics
are m1= 2,m2= 2.5,J1= 5,J2= 6.25,k = 10,r = 0.5,l = 0.5, and g = 9.8,b = 0.5. The controller
design parameters are chosen as R1=.03,R2= 0.03,Qi = 0.1XTX.
0 5 10 15
0
200
400
Time in sec
Events
Cumulative sampling instants
SS1
SS2
0 0.5 1 1.5 2 2.5 3
0
100
200
300
400
Event−triggering mechanism
Time in sec
Threshold vs Indicator
Threshold
Event error
Figure 7.2 Event-triggering mechanism.
The results in Figure 7.1 shows the distributed controller performance for the linear system for
various initial conditions. Figure 7.2 shows the cumulative events, event-triggering error, and its
threshold for the linear interconnected system, which demonstrates the sparsity of events in the
event-based feedback. Next, the results for the event-triggered controller are presented with the
distributed control scheme for the nonlinear dynamical system. For the event-triggered controller,
the initial states and the weights are chosen as in the previous case. The design parameters are
Γ = 0.95,α1 = 20, β = 0.01, Ri = 0.03, Q = 2XTX.
0 2 4 6 8 10
−0.2
−0.1
0
0.1
0.2
Time in sec
x
State trajectory
x1 x2 x3 x4
Figure 7.3 State trajectories (Example - 7.2).236 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
0 2 4 6 8 10
0
0.02
0.04
Time in sec
HJB error
H
TD algorithm
Hybrid algorithm
Figure 7.4 HJB error (Example - 7.2).
The system state trajectories with event-triggered controller are stable during the learning phase.
This can be verified from Figure 7.3 for both the subsystems. The results in Figure 7.3 include the
state trajectories for various initial states. The HJB residual error for the TD ADP-based controller
and the hybrid learning based controller are compared. It is evident from the results in Figure 7.4 that
the iterative weight updates between event-triggering instants seems to reduce the learning time. The
observer performance is presented in Figure 7.5. The plots of estimated and actual outputs with the
event-triggered feedback are compared when the hybrid learning algorithm is employed to generate
the control policy online. The event triggered feedback and aperiodic update of the observer results
in a piecewise continuous estimate of the actual states. The observer error convergence is essential
for the stability of the controlled system. Efficiency of the event-triggering condition designed for
the two subsystems, SS1 for subsystem 1 and SS2 for subsystem 2; the convergence time for the
observer estimation error and the HJB error for various initial conditions are recorded in Table 7.4.
0 2 4 6 8 10 12
−1
−0.5
0
0.5
1
Time in sec
y, ˆy
Actual and estimated outputs
yˆ1 yˆ2 y1 y2
Figure 7.5 Observer performance: Example 7.2.
Example 7.3. For the second example, a system composed of three interconnected subsystems is
considered. The three subsystems describe the dynamics of knee and thigh in a walking robot (Dun￾bar, 2007). Let γ1(t) be the relative angle between the two thighs, γ2(t) and γ3(t) be the right and left
knee angles relative to the right and the left thigh. The dynamical equations of motion (in rad/sec)
are
γ¨1(t) = 0.1[1−5.25γ2
1 (t)]γ˙1(t)−γ1(t) +u1(t)Nonlinear Interconnected Systems 237
0 10 20 30
−1
−0.5
0
0.5
1
Time in sec
y
Observer error
Figure 7.6 Observer performance: Example 7.3.
0 5 10
−0.5
0
0.5
1
Time in sec
x
State Trajectory
γ
1 γ
2 γ
3
Figure 7.7 State trajectory of walking robot.
γ¨2(t) = 0.01
1− p2(γ2(t)−γ2e)
2

γ˙2(t)−4(γ2(t)−γ2e)
+0.057γ1(t)γ˙1(t) +0.1(γ˙2(t)−γ˙3(t)) +u2(t)
γ¨3(t) = 0.01
1− p3(γ3(t)−γ3e)
2

γ˙3(t)−4(γ3(t)−γ3e)
+0.057γ1(t)γ˙1(t) +0.1(γ˙3(t)−γ˙2(t)) +u3(t).
The parameter values used to generate the figures given in this example are (γ2e, γ3e, p2 p3)
= (−0.227,0.559,6070,192). The control objective is to design torque commands and bring the
robot to a halt.
The distributed control scheme with an NN to approximate V∗
i (X) at each subsystem is designed.
The angles were initialized as 40◦ ±3◦,3◦ ±1◦,−3◦ ±1◦ and the angular velocities were initialized
at random to take values between 0 and 1. Two layer NNs with 12 neurons in the hidden layer are
used at each subsystem. The NN weights of the input layer were initialized at random to form ran￾dom vector functional link network (Lewis et al., 1998) and the second layer weights are initialized
to take values between 0 and 1. The states of each subsystem generated using the hybird learning
approach for different initial conditions are recorded. It can be observed that the states reach their
equilibrium point (0,-0.227,0.559) every time, ensuring stable operation, for both state and output
feedback control implementation (Figure 7.7). The convergence of the observer estimation error can
be verified from Figure 7.6. The hybrid algorithm converges faster and reaches steady state before
the time driven ADP. The observer estimation error converged to a neighborhood of origin. In the
analysis, different initial values for xi(0) and Xˆi(0) were chosen to test the algorithm and the results
are tabulated. It is observed that whenever the observer error persists, performing iterative weight
updates did not improve the learning rate. Therefore, the observer should be designed in such a way
that the observer error converges faster and in this case the hybrid algorithm with output feedback238 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
0 5 10
0
0.2
0.4
Control input for subsystem 1
Time in sec
τ
1
TD algorithm
Hybrid algorithm
0 2 4 6 8
−1
−0.5
0
Control input for subsystem 3
Time in sec
τ
3
TD algorithm
Hybrid algorithm
0 2 4 6 8
0
0.2
0.4
0.6
Control input for subsystem 2
Time in sec
τ
2
TD algorithm
Hybrid algorithm
0 100 200 300 400 500
0.1
0.2
0.3
0.4
0.5
Initial conditions
Feedback utilization
Number of Events
Figure 7.8 Event-triggered control.
Figure 7.9 Number of iterations in the inter-event period.
0 100 200 300 400 500
0.8
0.85
0.9
0.95
1
Initial conditions
Ratio of Cumulative costs
Hybrid / TD
Figure 7.10 Cost comparison (Example 7.3).
controller outperformed the time driven ADP (Table 7.4).
The control torques generated using the hybrid learning algorithm with event-triggered feedbackNonlinear Interconnected Systems 239
Table 7.4
Numerical analysis
Example Algorithm
Cumulative cost
(Normalized)
Convergence time
in sec Feedback utilization
State feedback
(SF)
Output feedback
(OF)
HJB error Observer error SF OF SF OF
1 TD 1 1 10.13 12.89 3.13 0.3716 0.8
Hybrid 0.988 0.912 6.35 10.74 2.90 0.398 0.7824
2 TD 1 1 4.8 37.20 30.654 0.2 0.4825
Hybrid 0.86 0.5916 4.1 31.62 27.13 0.3 0.55
and TD ADP are presented in Figure 7.8. Also, the feedback utilization (ratio the event-triggered
feedback instants and the sensor samples) are presented for simulations carried out for 500 different
initial conditions (Figure 7.8). The cumulative cost is calculated using the cost function defined in
(7.4). The comparison of the cumulative cost calculated for the hybrid learning approach with that
of the TD ADP reveals that the hybrid scheme results in a lower cumulative cost. Figure 7.10 shows
the ratio of costs due to hybrid algorithm over TD algorithm for different initial conditions. For
the output feedback case, due to the presence of the observer estimation error, the convergence of
the HJB error takes more time when compared to state feedback. The improvement in the learning
scheme is due to the learning process in the inter-sampling period. For analysis, the sensor sampling
time was fixed at 10ms and the control scheme was simulated to record the number of times the
weight update rule was executed in the inter-event period (Figure 7.9). It can be seen that the inter￾event time is not uniform and hence, the number of weight updates are varying. Initially, the events
are not spaced out and therefore, the iterative updates do not take place, but with time, the events
become spaced out, but still with varying intervals. This results in a varying number of iterative
weight updates. The comparison of HJB residual error for TD ADP and the learning scheme reveals
that the hybrid learning scheme requires less time for convergence. Table 7.4 summarizes the com￾parison of the two learning algorithms. Feedback utilization is the ratio of events with respect to the
sensor samples, when the sensor operates with a sampling period of 10 ms.
So far, this chapter presents an approximation based distributed controller with event triggered
state and output feedback that seeks optimality for a class of nonlinear interconnected system. In the
next section, we will present dynamic game formulation of the distributed optimal control problem.
7.4 DISTRIBUTED CONTROLLER DESIGN USING NONZERO SUM GAMES
As we have seen in Chapter 3 to 5, the performance of an isolated system, i.e., without intercon￾nection with other systems, can be optimized by solving the associated Riccati equation (RE) or the
HJB equation with linear or nonlinear dynamics, respectively. For the nonlinear system, obtaining
a closed-form analytical solution for the HJB partial differential equation is involved (Lewis et al.,
2012a), and hence, earlier in this chapter, we have sought an ADP-based near optimal solution using
a data-driven control scheme. In addition to the network related constraints, designing optimal dis￾tributed control for interconnected systems is a challenging problem due to the difficulty involved
in determining a trade-off between the performance of the overall system and the subsystems.
On the one hand, the decentralized optimal control techniques optimize local subsystem perfor￾mance and the resulting control policies may not optimize the overall system performance since all
the local controllers are designed independently. On the other hand, the distributed optimal control
scheme discussed in the earlier section lumps the overall system objective and the control policies
at each subsystem strive to optimize a distributed cost with little flexibility over the control syn-240 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
thesis at each subsystem. In a nutshell, the optimal control problems for interconnected systems
are formulated either based on a global objective (Mehraeen and Jagannathan, 2011; Narayanan
and Jagannathan, 2017) function or a bunch of independent local objective functions (Liu et al.,
2014). This leads to optimization of either the local subsystem or the overall system performance.
In practice, the control scheme for a large-scale interconnected system needs to be flexible such
that different performance criterion for individual subsystems are met to achieve a global objective.
The theory of nonzero-sum (NZS) differential games (Starr and Ho, 1969), where each player has
different performance index, is apt to meet the control design requirements for an interconnected
system.
In recent times, several multi-player game theory (Starr and Ho, 1969) based online control de￾sign schemes are presented in the literature (Vamvoudakis and Lewis, 2011; Johnson et al., 2015;
Vamvoudakis et al., 2017a) using ADP/RL for isolated systems. In these approaches (Vamvoudakis
and Lewis, 2011; Johnson et al., 2015; Vamvoudakis et al., 2017a), each player strives to optimize a
performance index such that a Nash-equilibrium is achieved. The authors Vamvoudakis and Lewis
(2011) proposed an online approximate optimal control scheme for both linear and nonlinear sys￾tems. Policy iteration based ADP scheme (Bertsekas, 2012) with actor-critic network is utilized to
approximate the Nash equilibrium solution of the coupled Hamilton-Jacobi (HJ) equation. It was
shown that the resulting approximate optimal controller guarantees UUB of all closed-loop system
signals with the convergence of the ADP algorithm. The requirement of complete knowledge of
the system dynamics for implementing the algorithm developed by Vamvoudakis and Lewis (2011)
was relaxed by Johnson et al. (2015). Further, the authors Song et al. (2017) presented an off-policy
integral RL method to solve the multi-player game problem with an assumption that the control co￾efficient maps for all the players are same, limiting its applicability. All the above multi-player game
based optimal control designs (Vamvoudakis and Lewis, 2011; Johnson et al., 2015; Vamvoudakis
et al., 2017a) are presented for multi-input isolated systems.
In the rest of this chapter, we shall focus on the multi-player game-theoretic method, based on
(Narayanan et al., 2018c), for designing distributed optimal controllers to attain optimal perfor￾mance both at the subsystem and the overall system levels, i.e., Nash-equilibrium, with limited and
intermittent communication among the subsystems. For the ease of exposition, the distributed op￾timal control design with continuous transmission of state information among the subsystems is
presented first, and then extended to the case of event-based sampling, transmission, and control
execution. The interconnected system is first reformulated and treated as an N-player NZS game. A
multi-player cost function, utilizing the control inputs as players, is defined at each subsystem. ADP
based approximate solution for the game is presented for both continuous and event-based system
wherein NNs introduced at each subsystem learn the solution to the coupled HJ equation online.
7.4.1 PROBLEM REFORMULATION
Consider an interconnected system consisting of N subsystems. The dynamics of the i
th subsystem,
represented in a nonlinear affine form, are given by
x˙i(t) = ¯fi(xi) +g¯i(xi)ui(t) +∑N
j=1
j=i
Δi j(xi,xj), (7.28)
for i = 1,2,··· ,N, where xi(t) ∈ Rni with xi(0) ∈ Rni , and ui(t) ∈ Rmi are the state and control
input vectors of i
th subsystem, respectively. The functions ¯fi : Rni → Rni and ¯gi : Rni → Rni×mi are
the internal dynamics and control coefficient of the i
th subsystem, respectively, with ¯f(0) = 0. The
function Δi j : Rni ×Rnj → Rni represents the interconnection between the i
th and the j
th subsystem.
As we have seen earlier, to stabilize the i
th subsystem effectively, the control policy ui(t) is
desired to be a function of both xi(t) and xj(t), ∀i, j = {1,··· ,N}, i.e., distributed control policy.
Therefore, the control objective is to design N distributed optimal control policies (u∗
1, u∗
2,··· , u∗
N)Nonlinear Interconnected Systems 241
at each subsystem such that the following performance index is optimized.
Ji(X(t)) = * ∞
t (Qi(X) +∑N
j=1 uT
j (τ)Ri juj(τ))dτ, (7.29)
where X = [xT
1 ,...,xT
N]
T is the overall system state vector, Qi(·) ∈ R, i = 1,2,··· ,N is a positive
definite function with Qi(0) = 0, Rii ∈ Rmi×mi , and Ri j ∈ Rmj×mj are symmetric postive definite
matrices.
The rationale behind the performance index (7.29) is to obtain an optimal equilibrium solution
for each subsystem while ensuring the Nash equilibrium solution to the overall system (Starr and
Ho, 1969). This is different from the traditionally considered decentralized local cost function (Liu
et al., 2014), i.e., Ji(xi) = * ∞
t (Q(xi)+uT
i (τ)Riui(τ))dτ with local system states xi and control policy
ui, or the overall system cost function (7.4). A major limitation of these local or global performance
indices is that the influence of the neighboring subsystem’s control policies are not included while
designing the distributed control policy. Although the performance index (7.29) at each subsystem
is well defined in the sense that it accounts for the overall system state vector and neighboring
control policies, it requires a reformulation for the interconnected system dynamics to derive the
corresponding HJ equations.
Moreover, the solution to the HJ equation at each subsystem requires continuous availability of
the state information from local and neighboring subsystems. In an event-based control paradigm,
recall that each subsystem determines the aperiodic sampling and transmission instants to broadcast
the state information asynchronously. With this effect, define the sequences of time instants, {t
i
k}∞
k=0
and {t
j
k}∞
k=0, for all i = 1,2,··· ,N with t
i
0 = 0 and t
j
0 = 0 as event-sampling instants at the i
th and
j
th subsystem, respectively. The states xi(t
i
k) and xj(t
j
k ) of i
th and j
th subsystems are transmitted to
their respective local controller and broadcasted to neighboring subsystems at time instant t
i
k and
t
j
k , respectively. Therefore, for solving the HJ equation and to obtain the optimal control policies at
each subsystem, the state information xi(t
i
k) and x j(t
j
k ) are available at each subsystem which are
intermittent and asynchronous.
7.4.2 GAME-THEORETIC SOLUTION
In this section, first, the subsystem dynamics as in (7.28) are reformulated to represent an N-player
interconnected system and the corresponding HJ equation for (7.29) is derived. Then, we shall de￾velop the solution to the resulting optimal control problem using game theory and ADP techniques.
The interconnected system, by augmenting the subsystem dynamics, can be represented as an
N-player interconnected system given by
X˙(t) = f(X) +∑N
j=1 gj(xj)uj(t), X(0) = X0, (7.30)
where X(t)=[xT
1 (t),··· ,xT
N(t)]T
∈ Bx ⊆ Rn with n = ∑N
i=1 ni is the augmented state of the large￾scale system and Bx is a compact set. The map f(X) = [( ¯f1(x1) +∑N
j=2 Δ1 j(x2))T
,··· ,( ¯fN(xN) +
∑N−1
j=1 ΔN j(x))T ]
T with f(0) = 0 and gj(xj)=[0,··· ,g¯
T
j (xj),··· ,0]
T
∈ Rn×mj , j = 1,··· ,N are the
known overall internal dynamics and the control coefficient function where 0’s are matrices with all
elements zero of appropriate dimensions.
Assumption 7.8. The individual subsystems (7.28) and augmented system in (7.30) are stabilizable
and the state vector, x(t), is available for measurement. In addition, the control coefficient matrix
gi(xi) satisfies 0 < gim < gi(xi) ≤ giM, i = 1,2,··· ,N for some constants gim > 0 and giM > 0.
Assumption 7.9. The functions ¯fi(xi), Δi j(xi, x j), and g¯i(xi) are locally Lipschitz continuous in their
respective domains.242 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
The optimal value function, V∗
i , is given by
V∗
i (X(t)) = min
ui
Ji(X(t)) = min
ui
 ∞
t
(Qi(X) +∑N
j=1 uT
j (τ)Ri juj(τ))dτ, (7.31)
for i = 1,..,N. As per the Nash equilibrium strategy (Starr and Ho, 1969), given the optimal N-tuple
{u∗
1,··· ,u∗
N}, the optimal value at each subsystem satisfies
V∗
i = V∗
i (u∗
1,u∗
2,...,u∗
N) ≤ V∗
i (u∗
1,...,ui,...,u∗
N), for i ∈ {1,...,N}.
Using the infinitesimal version of (7.29), the Hamiltonian function Hi is defined as
Hi(X,u1,··· ,uN,V∗
ix) = Qi(X) +∑N
j=1 uT
j (t)Ri juj(t) +V∗T
ix [ f(X) +∑N
j=1 gj(xj)uj], (7.32)
where Vix = ∂Vi/∂X and i = 1,2,··· ,N. Applying the stationarity condition ∂Hi/∂ui = 0, i =
1,2,··· ,N, reveals the greedy distributed optimal control policy u∗
i given by
u∗
i (t) = −1
2
R−1
ii gT
i V∗
ix, ∀i = {1,2,··· ,N}. (7.33)
Inserting the optimal control policy (7.33) into the Hamiltonian function (7.32), the HJ equation is
given by
Hi(X,u∗
1,··· ,u∗
N,V∗
ix) = Qi(X) +V∗T
ix [ f(X)− 1
2 ∑N
j=1 gjR−1
j j gT
j V∗
jx] (7.34)
+
1
4 ∑N
j=1V∗
jxgjR−1
j j Ri jR−1
j j gT
j V∗T
jx = 0, ∀i = {1,2,··· ,N}.
Note that the HJ equation (7.34) includes the interconnection dynamics, neighboring subsystem
control policy, and hence, coupled. The optimal control policy for the large-scale system (7.30)
can be obtained using (7.33) at each subsystem, given the system dynamics and the optimal value
function. However, with complete information of the system dynamics in (7.30), a closed form
solution of the coupled HJ partial differential equation (7.34), i.e., the optimal value function, is
difficult to compute (Vamvoudakis et al., 2017a). In general, approximate solution to the HJ equation
is sought using the ADP-based approach by approximating the optimal value function using NNs.
Therefore, in the next two sections, solutions for the N-player cooperative game based optimal
controller using ADP is presented. The continuous ADP based solution with overall system state is
presented in the next followed by the event-based solution.
7.4.3 APPROXIMATE SOLUTION TO THE NZS GAME
In this section, an approximate solution to the N-player game is presented using ADP based ap￾proach. Continuous availability of the subsystem state information at every subsystem is assumed,
which is relaxed in the next section.
7.4.3.1 Value Function Approximation (critic design)
The optimal value function, which is the solution to the coupled HJ equation (7.34), is approximated
using a linearly parametrized dynamic NN at each subsystem. The following standard assumption
is required for the universal approximation property of the NN to hold.
Assumption 7.10. The solution to the HJ equation (7.34), i.e., the optimal value function, V∗
i (·),
exists and is unique, real-valued, and continuously differentiable on the compact set Bx ⊆ Rn.Nonlinear Interconnected Systems 243
By the universal approximation property of the NNs (Lewis et al., 1998), there exists an ideal
weight matrix θ∗
i ∈ Rlθ and a basis function φ(X) ∈ Rlθ such that the smooth optimal value function
V∗
i (X) can be represented as
V∗
i (X) = θ∗T
i φ(X) +εi(X), ∀i = {1,2,··· ,N}, (7.35)
where θ∗
i ∈ Rlθ is the unknown target constant weight matrix, φ(X) ∈ Rlθ is the activation function
and εi(X) is the approximation error. Note that, each subsystem has a value function approximator
(critic NN) to approximate the solution of HJ equation using the augmented state X. The optimal
control policy in a parametric form using (7.35) can be expressed as
u∗
i (t) = −1
2
R−1
ii gT
i (∇T
x φ(X)θ∗
i +∇T
x εi(X)) (7.36)
for i = 1,2,··· ,N, where ∇x = ∂ (·)/∂X denotes the gradient with respect to X. The standard as￾sumptions that are used in the analysis are stated next.
Assumption 7.11. (Lewis et al., 1998) NN target weight matrix θ∗
i , activation function φ(·), gra￾dient of activation function ∇xφ(·), reconstruction error εi(·), and the gradient of the reconstruc￾tion error ∇xεi(·), ∀i = {1,··· ,N} are bounded satisfying θ∗
i  ≤ θiM, φ(·) ≤ φM, ∇xφ(·) ≤ ∇φM,
εi(·) ≤ εiM, and ∇xεi(·) ≤ ∇εiM where θiM, φM, ∇φM, and εiM and ∇εiM are positive constants.
Since the target weight of the NN approximation in (7.35) is unknown, one needs to estimate the
NN weights online. The estimate of the optimal value function can be expressed as
Vˆ
i(X) = θˆT
i (t)φ(X), ∀i = {1,··· ,N}, (7.37)
where θˆ
i(t) ∈ Rlθ is the estimate of the target weight θ∗
i . The estimated control policy using (7.37)
is represented as
uˆi(t) = −1
2
R−1
ii gT
i Vˆ
ix = −1
2
R−1
ii gT
i ∇T
x φ(X)θˆ
i. (7.38)
To update the weights of the NN, we shall avail the Bellman optimality principle. From (7.31), and
for T > 0, we have the recursive Bellman equation,
V∗
i (X(t)) =  t+T
t
(Qi(X) +
N
∑
j=1
uT
j (τ)Ri juj(τ))dτ +V∗
i (X(t +T)). (7.39)
Taking one step backwards, we can rewrite (7.39) using the estimated value as
Vˆ
i(X(t −T)) =  t
t−T
(Qi(X) +
N
∑
j=1
uT
j (τ)Ri juj(τ))dτ +Vˆ
i(X(t))−Eib, (7.40)
where Eib is the temporal difference/ Bellman residual error due to the estimated values. The NN
weights can then be updated using a gradient based algorithm for minimizing the function EiHJ = 1
2E2
ib. For example, the NN weight update law is given by
˙
θˆi(t) = −αi1(∂Eib/∂θˆ
i )Eib, (7.41)
where αi1 = αi/ρ2
i with αi > 0 is the learning gain and ρi = 1 + (∂Eib/∂θˆ
i)
T
(∂Eib/∂θˆ
i) is the
normalization term. To facilitate the stability proof presented in the next subsection, define the NN
weight estimation error θ˜
i(t) = θ∗
i −θˆ
i. The weight estimation error dynamics from (7.41) can be
expressed as ˙
θ˜i(t) = αi1(∂Eib/∂θˆ
i )Eib. (7.42)
Next, the stability results for the closed-loop system are summarized from (Narayanan et al.,
2018c).244 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Table 7.5
Design equations based on distributed Nash games
Integral Bellman error Eib = * t
t−T (Qi(X) +∑N
j=1 uT
j Ri juj)dτ +Vˆ
i(X(t))−Vˆ
i(X(t −T))
Update rule ˙
θˆi = −αi1(∂Eib/∂θˆ
i )Eib
Value function Vˆ
i(Xˆ) = θˆ
i
T
φ(Xˆ),
Distributed control law ˆui(t) = −1
2R−1
ii gT
i Vˆ
ix = −1
2R−1
ii gT
i ∇T
x φ(X)θˆ
i
7.4.3.2 Stability Analysis
To claim UUB of the closed-loop parameters, the following technical lemmas are required.
Lemma 7.2. Consider the NN tuning rule (7.41) and the weight estimation error dynamics (7.42).
With the Bellman equation (7.40), the weight estimation error dynamics satisfies
L˙i(θ˜
i) ≤ −
αi1
2 θ˜T
i (t)Δφ(X(t))Δφ T (X(t))θ˜
i(t) + αi1
2 εT
ib(X)εib(X), (7.43)
where, i = {1,··· ,N} and Li(θ˜
i(t)) = 1
2θ˜T
i (t)θ˜
i(t).
Sketch of Proof:
 Consider the smooth, positive definite function Li(θ˜
i(t)) = 1
2θ˜T
i (t)θ˜
i(t). Substituting the
parameter error dynamics in the time-derivative of the Lyapunov function and using the
Young’s inequality will lead to (7.43).
Note that as the number of hidden layer neurons, l, is increased, the approximation error εib will
approach zero (Lewis et al., 1998).
Lemma 7.3. Given the interconnected system (7.28) with the optimal control policy u∗
j , j =
1,2,··· ,N in (7.33). Let Assumptions 7.8 and 7.9 hold. Then, for continuously differentiable, radi￾ally unbounded Lyapunov candidate functions Li(xi) for the subsystem (7.28), there exist constants
Ci2 > 0 such that the inequality
LT
ixi
(xi)( ¯¯f i(X) +giu∗
i (t)) ≤ −Ci2

Lixi
(xi)


2 (7.44)
holds; where ¯¯fi(X) = ¯fi(xi) +∑N
j=1,i=j Δi j(xi, xj) and Lixi
(xi) = ∂Li(xi)/∂ xi
.
Proof. The proof is a direct result of the stabilizing property of the optimal policies u∗
i (x) and for
details refer to (Dierks and Jagannathan, 2012a, 2010b).
Recall the PE condition (Ioannou and Fidan, 2006), which is used for ensuring the stability of
the closed-loop system presented in the theorem. A signal Δφ(x,T)
1+ΔT φ(x,T)Δφ(x,T) is said to be persistently
exciting over an interval [t −T,t], if ∀t ∈ R+, there exists a T > 0, τ1 > 0, τ2 > 0 such that
τ1I ≤
 t
t−T
Δφ(x,T)ΔT φ(x,T)
(1+ΔT φ(x,T)Δφ(x,T))2 dτ ≤ τ2I,
where I is the identity matrix of appropriate dimension.Nonlinear Interconnected Systems 245
Theorem 7.3. Consider the interconnected system with the ith subsystem dynamics given by (7.28)
and represented as a N-player large-scale system in (7.30). Let the Assumptions 7.8 to 7.11 hold,
the value function approximator NN weights are initialized in a compact set Ωθ ⊆ Rlθ and up￾dated according to (7.41). Then the estimated control input (7.38) renders the closed-loop system
locally uniformly ultimately bounded provided the design parameters βi >
D2
iM
2Ci2
, αi >
∇φMβ2
i
2 , and
the regression vector is persistently exciting, where DiM,βi are positive constants.
Sketch of Proof:
 Since the control policy is continuously updated, we only have to consider one case,
where we pick a continuously differentiable positive definite candidate Lyapunov function
L : Rlθ ×Bx → R, given as
L(θ˜,X) = ∑N
i=1 Li(θ˜
i,xi), (7.45)
where Li(θ˜
i, xi) = βiLi(xi) + Li(θ˜
i) is the Lyapunov function of i
th subsystem with βi > 0
and Li(θ˜
i)=(1/2)θ˜T
i θ˜
i.
 The Lyapunov function is a sum of Lyapunov functions corresponding to the N subsystems
and each of the subsystem Lyapunov function is composed of two terms, each of which
accounts for the NN weight estimation error at a subsystem and the subsystem states, re￾spectively.
 Substituting the time derivatives of the weight estimation errors and the subsystem dynam￾ics, we arrive at the desired bounds. Here βi and αi are independent design parameters that
can be chosen to satisfy the (sufficiency) conditions defined in the Theorem statement.
Corollary 7.2. Let the hypothesis of Theorem 7.3 holds. Then the estimated value function (7.37)
and control policy (7.38) converge to an arbitrarily close neighborhood of the optimal value function
(7.35) and control policy (7.36), respectively.
Sketch of Proof:
 The proof constitutes considering the error between the estimated value function and the op￾timal value function and the the error between the optimal control policy and the estimated
control policy.
 From the proof of Theorem 7.3, the NN weight estimation error θ˜
i is bounded and with As￾sumption 7.11, 
V˜
i

 and u˜i are UUB. Since the bound can be made smaller by increasing
the number of neurons, the estimated value and the estimated Nash solution converge to the
neighborhood of their optimal values, respectively.
Next, the Nash solution in the context of an event-based sampling scheme to reduce the usage of
computation and communication bandwidth is presented.
7.4.4 EVENT-BASED APPROXIMATE NASH SOLUTION
In this section, the assumption of continuous availability of the system state vector is relaxed by in￾troducing an asynchronous event-based sampling and transmission scheme among the subsystems.
Before presenting the event-sampled formulation, the following property of the communication net￾work is assumed.
Assumption 7.12. The communication network used for the exchange of feedback information is
ideal, i.e., there are no delays and packet losses occurring during transmission.246 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
7.4.4.1 Event-sampled Optimal Control Policy
Define the local state information received at the i
th controller at the occurrence of an event as
xie(t) = xi(t
i
k), t
i
k ≤ t < t
i
k+1, ∀k ∈ N, (7.46)
where xie(t) is the event-sampled state vector of i
th subsystem. This intermittent transmission of the
system state vector introduces an error, referred to as event-sampling error, and is defined as
ei(t) = xi(t)−xie(t), t
i
k ≤ t < t
i
k+1, ∀k ∈ N, (7.47)
where ei(t) is the event-sampling error of the i
th subsystem. The sampling instantst
i
k and t
j
k are asyn￾chronous and solely governed by local event-sampling condition at each subsystem defined later in
(7.55). To implement the distributed controller in the event-based sampling context, the broadcasted
state information are stored at each subsystem controller and updated as soon as new state infor￾mation is received. However, each subsystem controller is executed only at the local subsystem
sampling instants with the latest information available from the local and neighboring subsystems.
The optimal control policy in the event-sampling context can be rewritten from (7.33) using the
event-based states as
u∗
ie(t) = −1
2
R−1
ii gT
i (xie)V∗
ixe , ∀i = 1,2,··· ,N, (7.48)
where Xe = [xT
1e, ··· , xT
Ne]
T is event-based states of the large-scale system received at the controller
of each sub-system and V∗
ixe = ∂V∗
i (X)/∂X |
X=Xe is the gradient evaluated at Xe. Event-sampled
approximation (Sahoo et al., 2013a) of the optimal value function V∗
i (X) using the NNs can be
represented as
V∗
i (X) = θ∗T
i φ(Xe) +εie(Xe,e), i = 1,2,··· ,N, (7.49)
where φ(Xe) ∈ Rlθ is the event-sampled activation function and εie(Xe,e) = θi
∗T
(φ(Xe + e) −
φ(Xe))+εi(Xe+e), i = 1,2,··· ,N is the event-sampled approximation error with e = [eT
1 , ··· , eT
N]
T
is the event-sampling error of the large-scale system.
The optimal control policy with event-based state information in a parametric form can be ex￾pressed as
u∗
ie(t) = −1
2
R−1
ii gT
ie[∇T
xeφ(Xe)θ∗
i +∇T
xe εie(Xe,e)], (7.50)
for i = 1,2,··· ,N, where gje = gj(xje) and ∇xe = ∂ (·)/∂X |
X=Xe
. The estimation of the optimal
value function in an event-sampling context can be expressed as
Vˆ
i(X) = θˆT
i (t)φ(Xe), i = 1,2,··· ,N. (7.51)
The estimated control policy ˆuie, i = 1,2,··· ,N, from (7.51) is represented as
uˆie(t) = −1
2
R−1
ii gT
ieVˆ
ixe = −1
2
R−1
ii gT
ie∇T
xeφ(Xe)θˆ
i, (7.52)
for i = 1,2,··· ,N. To derive the NN learning rule, rewrite the Bellman equation (7.40) in the
event-driven framework as
Vˆ
i(X(t
i
k−1)) =  t
i
k
t
i
k−1
(Qi(X) +
N
∑
j=1
uˆ
T
je(τ)Ri juˆje(τ))dτ +Vˆ
i(X(t
i
k))−Eib,e, (7.53)
where Eib,e is the event-driven temporal difference error/Bellman residual error.Nonlinear Interconnected Systems 247
Table 7.6
Design equations based on distributed Nash games using event-triggered feedback
Integral Bellman error Eib,e = * t
i
k
t
i
k−1
(Qi(X) +∑N
j=1 uT
j Ri juj)dτ +Vˆ
i(X(t
i
k))−Vˆ
i(X(t
i
k−1))
Update rules
θˆ+
i (t) = θˆ
i(t)−αi1(∂Eib,e/∂θˆ
i )Eib,e, t = t
i
k,
˙
θˆi(t) = −αi1(∂Eib,e/∂θˆ
i )Eib,e, t
i
k < t < t
i
k+1,
Value function Vˆ
i(Xˆ) = θˆ
i
T
φ(Xˆe),
Distributed control law ˆuie(t) = −1
2R−1
ii gT
ieVˆ
ixe = −1
2R−1
ii gT
ie∇T
xeφ(Xe)θˆ
i
Remark 7.12. Note that the states of the ith subsystem are accessed by the controller located at
the ith subsystem at event-triggering instants ti
k only, therefore, it is natural to consider calculating
the temporal difference/Bellman residual error at the event-triggering instants, i.e., the fixed T in
(7.39) becomes time-varying Ti
k = t
i
k+1 −t
i
k. This requires that the event-triggering mechanism is
Zeno free, i.e., the inter-event times Ti
k is positive. Furthermore, at the triggering instant ti
k only the
local states, xi, are updated, and the last received states of the jth subsystem should be used. This
requires the Bellman error to be redefined as the event-driven Bellman error, Eib,e.
The NN weight update law with event-driven Bellman equation (7.53) is given by
θˆ+
i (t) = θˆ
i(t)−αi1(∂Eib,e/∂θˆ
i )Eib,e, t = t
i
k,
˙
θˆi(t) = −αi1(∂Eib,e/∂θˆ
i )Eib,e, t
i
k < t < t
i
k+1, (7.54)
for i = 1,2,··· ,N and k ∈ N, where αi1 = αi/ρ2
ie with αi > 0 is the learning gain and ρie = 1 +
(∂Eib,e/∂θˆ
i)
T
(∂Eib,e/∂θˆ
i) is the normalization term and the notation θˆ+
i (t) = θˆ
i(t
+) evaluated at
time t = t
i
k,k ∈ N, and defined as lims→t
θˆ
i(s). The event-triggering condition for the asynchronous
sampling and broadcasting is discussed next.
7.4.4.2 Event-triggering Condition and Stability
An event-sampling mechanism is required at each sub-system to determine the broadcast instants for
the system states. Consider a positive definite, locally Lipschitz continuous function Li(xi) ∈ R as
the Lyapunov function of i
th subsystem (7.28). The event-triggering condition using the Lyapunov
function can be selected as
e¯i(t) ≤ ((2−eLi fΔ(t−t
i
k)
)βi −1)Li(xi(t
i
k)), t ∈ [t
i
k,t
i
k+1), (7.55)
where βi > 1, e¯i(t) = PiL ei(t), and LfΔ = PiLPi fΔ with PiL > 0 and PfΔ > 0 being the Lipschitz
constants satisfying 
Li(xi(t
i
k))−Li(xi(t))

 ≤ PiL ei(t) and


 ¯¯f i(X)− ¯¯f i(Xe)


 ≤ Pi fΔ ei(t). The
states are sampled using the equality condition in (7.55).248 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Remark 7.13. The event-sampling condition in (7.55) is decentralized and a function of local sub￾system information and can be evaluated independently at each subsystem. This leads to an asyn￾chronous sampling and transmission scheme.
Lemma 7.4. If the inequality in (7.55) hold, then
Li(xi(t)) ≤ (2−eLi fΔ(t−t
i
k)
)βiLi(xi(t
i
k)), t ∈ [t
i
k,t
i
k+1). (7.56)
Sketch of Proof:
 By Lipschitz continuity of the Lyapunov function Li(xi(t)), it holds that 
Li(xi(t))−Li(xi(t
i
k))

 ≤
PiL

xi(t)−xi(t
i
k)

 = PiL ei(t), t ∈ [t
i
k,t
i
k+1).
 As per the Lemma statement, since the inequality (7.55) holds, the above inequality satisfies

Li(xi(t))−Li(xi(t
i
k))

 ≤ (2−eLfΔ(t−t
i
k)
)βiLi(xi(t
i
k))−Li(xi(t
i
k)) .
 Adding Li(xi(t
i
k)) on the right hand side, we get the desired bounds.
Define the augmented states χi = [xT
i xT
ie θ˜T
i ]
T for the closed-loop system. The closed-loop event￾triggered system can be formulated as a nonlinear impulsive hybrid dynamical system due to in￾volved flow dynamics during inter event-times t
i
k < t < t
i
k+1 and the jump dynamics at triggering
instants t = t
i
k, ∀k,i. Using (7.28), (7.46), and (7.54), the closed-loop impulsive dynamics can be
obtained as
χ˙i(t) =
⎡
⎢
⎣
¯fi(xi) +g¯i(xi)uˆie(t) +∑N
j=1
j=i
Δi j(xi,x j),
0
αi1(∂Eib,e/∂θˆ
i )Eib,e
⎤
⎥
⎦ (7.57)
for t
i
k < t < t
i
k+1 and
χi
+(t) =
⎡
⎣
xi(t)
xi(t)
θ˜
i(t) +αi1(∂Eib,e/∂θˆ
i )Eib,e
⎤
⎦ (7.58)
for t = t
i
k, ∀k,i. The closed-loop stability results are summarized next.
Theorem 7.4. Consider the nonlinear continuous-time interconnected system (7.30), with event￾based impulsive dynamics (7.57) and (7.58). Let the Assumptions 7.8 to 7.12 hold, the initial control
policy ui(0) be admissible, the critic NN weights initialized in a compact set, and updated according
to (7.54). Suppose the initial event occurs at ti
0 = 0 and there exists a positive minimum inter￾event time between the triggering instants. Then, the event-sampled control policy (7.52) renders
the closed-loop impulsive dynamical system locally UB provided activation function and its gradient
satisfy the PE condition and the system states are transmitted at the violation of the event sampling
condition (7.55) with the design parameter 0 < αi < 1.
Sketch of Proof:
 The proof constitutes considering two cases corresponding to the flow dynamics and the
jump dynamics and demonstrating that the first derivative of the Lyapunov function is ulti￾mately bounds.
 Flow dynamics: A continuously differentiable positive definite candidate Lyapunov func￾tion is chosen to demonstrate the results as stated. Specifically, the Lyapunov candidate
function, L(χ), is chosen as
L(χ) = ∑N
i=1 Li(χi), (7.59)
where Li(χi) = Li(θ˜
i) +βiLi(xi) +βiLi(xi,e) is the Lyapunov function of i
th subsystem with
Li(θ˜
i)=(1/2)θ˜T
i θ˜
i, Li(xi) = xT
i xi, Li(xi,e) = xT
i,exi,e, and βi > 1.Nonlinear Interconnected Systems 249
 Jump dynamics: The same Lyapunov function from the flow period is chosen as a candidate
for analyzing the jump dynamics as well.
 The time-derivative of the Lyapunov candidate function during the flow and the first￾difference of the Lyapunov candidate function during jump dynamics can be computed
by using weight estimation error dynamics and the system dynamics during both the event￾triggering and inter-event times. Furthermore, by appropriate choice of the learning parame￾ter αi and NN design, the bounds can be reduced when the regression function is persistently
exciting. The overall states and weight estimation error bounds can be obtained similar to
Theorem 7.3.
Corollary 7.3. Let the hypothesis of Theorem 7.4 holds. Then the estimated value function (7.51)
and control policy (7.52), respectively, converge arbitrarily close neighborhood of the optimal value
function (7.49) and control policy (7.50).
Proof: The proof follows arguments similar to that of Corollary 7.2. In addition to the Assumptions
stated in Corollary 7.2, in the case of event-triggered control scheme considered here, the estimated
value function (7.51) and control policy (7.52), respectively, converge arbitrarily close neighborhood
of the optimal value function (7.49) and control policy (7.50) as number of event-triggering instant
increases. The assumption for the minimum inter-event time between the events is guaranteed in
Theorem presented next.
Theorem 7.5. Let the hypothesis of Theorem 7.4 holds. Then the inter-event times Ti
k = t
i
k+1 −t
i
k ,
i = 1,2,··· ,N , ∀k ∈ N are lower bounded by a non-zero positive number and
Ti
k = t
i
k+1 −t
i
k ≥
1
LfΔ
ln
"
1+
LfΔ
κ (2βi −1)Li(xi(t
i
k))#
"
(1+
LfΔ
κ βiLi(xi(t
i
k))# > 0, (7.60)
where κ > 0 is defined in the proof.
Proof:
 The dynamics of the event-trigger error satisfies
d e¯i(t)
dt ≤ e˙¯i(t) = PiLx˙i(t)
=



PiL ¯¯f i(X)−(1/2)PiLD¯i∇T
x φ(Xe)θˆ
i


 ≤ LfΔ e¯i+κ,
where LfΔ = PiLPi fΔ, κ =  ¯¯f (Xe) + (1/2)PiLD¯iM∇T
x φMθˆ
iM. The above inequality is
reached using the Lipschitz continuity of the function ¯¯f i(X).
 By comparison lemma, the solution of e˙¯i(t) with 
e¯i(t
i
k)

 = 0 is upper bounded by
e¯i(t) ≤
 t
t
i
k
κeLfΔ(t−τ)
dτ = (κ/LfΔ )(eLfΔ(t−t
i
k) −1). (7.61)
 At the next sampling instant 
ei(t
i
k+1)

 = ((1 − eLfΔ(t
i
k+1−t
i
k)
)βi − 1)Li(xi(t
i
k)). Comparing
both the equations

ei(t
i
k+1)

 = ((2−eLfΔ(t
i
k+1−t
i
k)
)βi −1)Li(xi(t
i
k))
≤ (κ/LfΔ )(eLfΔ(t
i
k+1−t
i
k) −1).250 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
 Solving the equation for the inter-sample time leads to (7.60).
Ti
k = t
i
k+1 −t
i
k =
1
LfΔ
ln
1+
LfΔ
κ (2βi −1)Li(xi(t
i
k))
/

(1+
LfΔ
κ βiLi(xi(t
i
k))
.
The inter-sample times Ti
k > 0, i = 1,2,··· ,N by selecting βi > 1, implies the sampling instants are
distinct and not accumulated. Next, the approximate Nash solution for linear system is presented.
7.4.5 DISTRIBUTED CONTROL OF LINEAR INTERCONNECTED SYSTEMS
In this section, we extended the results to linear systems. The dynamics of i
th subsystem, represented
in a continuous time with linear dynamics, are given by
x˙i(t) = A¯ixi(t) +B¯iui(t) +∑N
j=1
j=i
Ai jxj(t), (7.62)
for i = 1,2,··· ,N, where the linear map Ai : Rni → Rni and B¯i : Rni → Rni×mi are the internal
dynamics and input gain of the i
th subsystem, respectively. The matrix function Ai j : Rni → Rni×nj
represents the interconnection between i
th and j
th subsystem. The interconnected system can be
represented as a N-player large scale system given by
X˙(t) = AX(t) +∑N
j=1 Bjuj(t), X(0) = X0, (7.63)
where the map A = [A¯i,Ai2,...,AiN] for i = 1,..,N and Bj = [0,...,B¯T
j ,...,0]
T
∈ Rn×mj , j = 1,..,N
are the known overall internal dynamics and the control coefficient function with 0’s representing
the zero matrices of appropriate dimensions. The cost functional Ji associated with each subsystem
for the admissible policies ui(t), i = 1,2,··· ,N is defined as
Ji(X) =  ∞
t
(XT (τ)QiX(τ) +∑N
j=1 uT
j (τ)Ri juj(τ))dτ,
where Qi > 0, i = 1,2,··· ,N is a positive definite matrix. The optimal value V∗
i at each subsystem
is given by
V∗
i (X) = min
ui
Ji(X) = min
ui
 ∞
t
(XT (τ)QiX(τ) +∑N
j=1 uT
j (τ)Ri juj(τ))dτ.
Let the candidate optimal value function be of the formV∗
i (x) = 1
2 xT (t)P∗
i x(t), i = 1,2,··· ,N, where
P∗
i > 0 is a symmetric positive definite matrix. The Hamiltonian function Hi is defined as
Hi(X,u1,··· ,uN,V∗
ix) = XTQiX +∑N
j=1 uT
j Ri juj +V∗T
ix [AX(t) +∑N
j=1 Bjuj], (7.64)
where V∗
ix = ∂Vi/∂X = P∗
i X, i = 1,2,··· ,N. Applying the stationarity condition ∂Hi/∂ui = 0, i =
1,2,··· ,N reveals the greedy optimal control policy u∗
i given by
u∗
i (t) = −1
2
R−1
ii BT
i P∗
i X(t), i = 1,2,··· ,N. (7.65)
Inserting the optimal control policy (7.65) in the Hamiltonian function (7.64), reveals the coupled
Riccati equation (Song et al., 2017). Using an approach similar to the approximation used in the
previous section, forward-in-time solution to the coupled Riccati equation can be adaptively learnt
online using the ADP-based approach by representing the optimal value function as
V∗
i (X) = θ∗
i
T φ¯(X), (7.66)
where θ∗
i = vec(P∗
i ) is obtained using vectorization of the matrix P∗
i and φ¯(X) = vec(XT ⊗ X),
where ⊗ is the Kronecker product.Nonlinear Interconnected Systems 251
Remark 7.14. Note that the regression vector is formed using Kronecker product of the system
states, and, therefore, different than the bounded activation functions. Further, the approximation
error is not present.
Example 7.4. Consider three coupled nonlinear subsystems, where the three subsystems represent
are the thigh and knee dynamics of a walking robot experiment (Dunbar, 2007). In the following,
x1(t) is the relative angle between the two thighs, x2(t) is the right knee angle (relative to the right
thigh), and x3(t) is the left knee angle (relative to left thigh). The controlled equations of motion in
units of (rad/s) are
x¨1(t) = 0.1[1−5.25x2
1(t)]x˙1(t)−x1(t) +u1(t)
x¨2(t) = 0.01
1− p2(x2(t)−x2eq)
2

x˙2(t)−4(x2(t)−x2eq)
+0.057x1(t)x˙1(t) +0.1(x˙2(t)−x˙3(t)) +u2(t)
x¨3(t) = 0.01[1− p3(x3(t)−x3eq)
2
]x˙3(t)−4(x3(t)−x3eq)
+0.057x1(t)x˙1(t) +0.1(x˙3(t)−x˙2(t)) +u3(t),
where x¨i correspond to the dynamics of the ith subsystem. The control objective is to bring the robot
to a stop in a stable manner (xi → xieq), where xieq are the equilibrium points with x1eq = 0, x2eq =
−0.227,x3eq = 0.559.
The parameters p2 and p3 are 6070, and 192, respectively. The NN to approximate V∗
i were
designed with 6 hidden layer neurons. The initial conditions xi(0) and θˆ
i(0) were selected randomly
in the intervals [−1,1] and [0,1], respectively. The learning gains were selected for continuous time
simulations as α1 = 25, α2 = 35, α3 = 35, and for the event-triggering case, α1 = 0.4, α2 = 0.85,
and α3 = 0.85, βi = 1.086,∀i ∈ {1,2,3}. A quadratic performance index with Qi(X) = XT Q¯iX
is selected, where the parameters Q¯ 1 = 0.02, Q¯ 2 = 0.075, Q¯ 3 = 0.075, R11 = 0.04, R22 = 0.01,
R33 = 0.01, and Ri j = Rj j, for i = j, i, j ∈ {1,2,3}. The robotic system is simulated with the torques
generated using the control algorithm with hybrid leaning scheme.
0 1 2 3 4 5 6 7 8 9 10
-0.2
0
0.2
0.4
0.6
0 1 2 3 4 5 6 7 8 9 10
-0.5
0
0.5
Figure 7.11 Convergence of system states and control inputs for continuous (solid lines) and event-sampled
system (dotted lines).
The time history of the large-scale interconnected system states for both continuous (dotted) and
event-based (solid) implementation is plotted in Figure 7.11 (top). The system states both for the
continuous and event-based designs converge to the equilibrium state with the approximated con￾trol policies. The control inputs for both the designs also converge to zero as shown in Figure 7.11
(bottom). The event-triggered control policies (dotted lines) are piecewise constant functions, since
they are only updated at the triggering instants, and closely approximates the continuous policies.
To show the effectiveness of the event-based implementation in reducing communication and com-252 Optimal Event-Triggered Control Using Adaptive Dynamic Programming








    




         



Figure 7.12 Convergence of event-sampled error and threshold and inter-event times showing the asyn￾chronous transmission instants.
putation, the event-trigger error ¯ei(t) and the threshold in condition (7.55) for all the subsystems are
shown in Figure 7.12 (a) - (c). Note that the event-triggering conditions are local and decentralized
and, therefore, the thresholds are different for each subsystem leading to different number of sam￾pling instants. In terms of number of events, highest number of triggering observed at the subsystem
1 and lowest at subsystem 2; implying asynchronous triggering at subsystems. The inter-event times
Ti
k are shown in Figure 7.12 (d). It is clear that the inter-event times are different between two con￾secutive sampling instants leading to aperiodic sampling. This further implies that the computational
and the communication resources are saved by reducing the transmission and controller execution
when compared to periodic execution schemes.
		




Figure 7.13 Convergence of Bellman error and cumulative cost for continuous and event-sampled designs.
From the optimality perspective, the Bellman residual errors for both continuous and event-based
are shown in Figure 7.13 (top) indicating the convergence of the errors to zero at the same time. This
implies the estimated value function converged to its optimal value and Nash equilibrium solution
is reached for both cases. As expected, the time history of the event-based Bellman residual error is
oscillating before convergence. This is due to the NN parameter update using the aperiodic feedback
information with event-based sampling and transmission. The comparison between the cumulative
cost for continuous and event-triggered solution is shown in Figure 7.13 (bottom). It is clear that
the event-based cost is slightly higher than the continuous implementation which demonstrates the
trade-off between the system performance and communication resource savings. Finally, to compare
the effectiveness of the algorithm with the work in (Narayanan and Jagannathan, 2017), the con￾vergence of Bellman error and cumulative costs for the interconnected system are plotted in FigureNonlinear Interconnected Systems 253
Nash game-based algorithm Hybrid Algorithm
Figure 7.14 Comparison of Bellman error and cumulative cost for the hybrid method with NZS game and
existing method (Narayanan and Jagannathan, 2017).
7.14. The parameters in this experiment were chosen as follows: α1 = 0.4, α2 = 0.85, and α3 = 0.85,
βi = 1.086,∀i ∈ {1,2,3}. A quadratic performance index with Qi(X) = XT Q¯iX is selected where
the parameters Q¯ 1 = 0.05, Q¯ 2 = 0.085, Q¯ 3 = 0.085, R11 = 0.06, R22 = 0.07, R33 = 0.04, and
R12 = −0.01, R13 = −0.002, R21 = −0.025, R23 = −0.003, R31 = −0.025, R32 = −0.01. Note
that the flexibility of the distributed control framework lies in the additional term ∑3
j=1, j=i uT
i jRi jui j,
that is included in the cost function (7.31) of the i
th subsystem, as compared to the results in Sec￾tion 7.3 based on (Narayanan and Jagannathan, 2017), wherein Ri j = 0, ∀i, j ∈ {1,2,3}. In other
words, due to the NZS framework, a trade-off between the performance of different subsystems
can be achieved by explicitly accounting for control policies of other neighboring subsystems while
computing the control policy. From Figure 7.14, the Bellman residual error for both the designs
converge to zero at the same time. Further, it is clear that the method incur in lower cumulative cost
when compared to the method in the Section 7.3.
7.5 TRACKING CONTROL OF NONLINEAR INTERCONNECTED SYSTEMS
This section concerns with a brief summary of tracking control problem for nonlinear interconnected
systems using the learning-based control algorithms presented in this chapter. In a tracking problem,
the goal is to make the system output (or states) follow a desired reference trajectory or signal. We
begin with the definition of a feasible reference trajectory for the i
th subsystem as xi,d(t) ∈ Rni .
Assume that the feasible reference trajectory xi,d(t) ∈ Rni for the i
th subsystem is generated by a
reference system represented by
x˙i,d(t) = fi,d(xi,d(t)) +
N
∑
j=1, j=i
Δi j,d(xi,d,xj,d), xi,d(0) = xid0, (7.67)
where xi,d(t) ∈ Rni is the reference state of the i
th subsystem, fi,d and Δi j,d are nonlinear functions,
and xi,d(0) = 0. The system in (7.67) is often referred to as the generator system that generates
feasible reference trajectories for the system to track.
Define the error between the system state and the reference state as the tracking error, given by
ei,r(t)  xi(t)−xi,d(t). Then, the tracking error system, utilizing (7.1) and (7.67), can be defined by
e˙i,r(t) =x˙i(t)−x˙i,d(t) = fi(xi(t)) +gi(xi)ui(t) +∑N
j=1
j=i
Δi j(xi,x j)−x˙i,d(t),
=fi(ei,r, xi,d) +gi(ei,r, xi,d)ui(t) +
N
∑
j=1, j=i
Δi j(ej,r,ei,r,x j,d,xi,d)− fi,d(xi,d)−
N
∑
j=1, j=i
Δi j,d(xi,d,x j,d).
(7.68)254 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Recall the following assumptions on the control coefficient and the reference trajectory.
Assumption 7.13. The function gi is bounded, the matrix gi(x) has full column rank for all
xi(t) ∈ Rni , the function g+
i : Rni → Rmi×ni defined as g+
i  (gT
i gi)−1gT
i is bounded and lo￾cally Lipschitz. The reference trajectory is bounded, the functions fi,d and Δi j,d are locally Lip￾schitz functions, and gi(xi,d)g+
i (xi,d)(fi,d(xi,d) + ∑j Δi j,d(xi,d,xj,d) − fi(xi,d) − ∑j Δi j(xi,d,x j,d)) =
fi,d(xi,d) +∑j Δi j,d(xi,d, x j,d)− fi(xi,d)−∑j Δi j(xi,d,x j,d).
The steady-state control policy ui,d : Rni → Rmi corresponding to the desired trajectory xi,d is
ui,d(xi,d) = g+
i,d(xi,d)(fi,d(xi,d) +∑
j
Δi j,d(xi,d,x j,d)− fi(xi,d)−∑
j
Δi j(xi,d,x j,d)). (7.69)
By augmenting the tracking error ei,r and desired trajectory xi,d, the dynamics of the augmented
tracking error system can be represented as
χ˙i(t) = Gi(χi(t)) +Hi(χi(t))ωi(t) +
N
∑
j=1, j=i
Ki j(χi(t),χj(t)), (7.70)
where χi  [eT
i,r xT
i,d]
T ∈ R2ni is the augmented state with χi(0)=[eT
i,r(0) xT
i,d(0)]T = χi0, Gi is given
by 
fi(ei,r +xi,d)− fi,d(xi,d) +gi(ei,r +xi,d)ui,d(xi,d)
fi,d(xi,d)

, Hi given by 
gi(ei,r +xi,d)
0

, Ki j is given by

∑j Δi j(ei,r +xi,d, e j,r +xj,d)−∑j Δi j,d(xi,d,xj,d)
∑j Δi j,d(xi,d,xj,d)

,
and ωi(t) = ui(t)−ui,d(t). The augmented error system in (7.70) associated with the i
th subsystem
enables transforming the tracking problem into a regulation problem. We can further augment the
error subsystems for i = 1,...,N to define the overall tracking error system similar to (7.2).
Further, the cost function for the overall tracking error system can be redefined as
V(χ,ω) =  ∞
t
[Q¯(χ) +ωT (τ)R¯ω(τ)]dτ, (7.71)
subject to dynamic constraints (7.70) for i = 1,...,N. Since the desired trajectories are not con￾trollable with ωi(t), the penalty for the desired trajectories are kept as zero. For example, Q¯(χ) =
[eT
r xT
d ]
 Q 0
0 0

[eT
r xT
d ]
T , where er, xd are the vectors of the overall error, desired trajectories for all the
subsystems concatenated into a single vector, respectively, and Q is a positive definite matrix and 0
represent zero matrices of appropriate dimensions. Using (7.70) and (7.71), we can follow the steps
discussed earlier in this chapter in Sections 7.3 and 7.4 to design learning-based tracking control
schemes for nonlinear interconnected systems.
7.6 CONCLUDING REMARKS
This chapter covered an approximation based distributed controller with event-triggered state and
output feedback that seeks optimality for a class of nonlinear interconnected system. The event￾triggered control execution significantly reduces the communication and computational resource
utilization by reducing the frequency of feedback instants. The hybrid learning scheme accelerates
the learning of the NN weights with event-triggered feedback while reducing the communication
costs. The event-triggering condition is independent of the estimated parameters and an additional
estimator at the event-triggering mechanism is not required. The event-triggering mechanism is
decentralized, asynchronous, and ensures that the system is stable during the inter-event period. The
requirement of initial stabilizing control policy is relaxed by utilizing the dynamics of the system.Nonlinear Interconnected Systems 255
Next a distributed optimal controller design for large-scale interconnected systems is reformu￾lated using N-player NZS game. Nash equilibrium solution is obtained using ADP based scheme
for both continuous and event-sampled implementation of the system. Local ultimate boundedness
of the system states and NN weight estimation errors are demonstrated analytically. It is observed
in the example that the hybrid leaning scheme closely approximates the continuous time solution.
From the performance point of view, the event-based implementation closely followed the contin￾uously updated solution while saving the communication bandwidth. The distributed optimization
problem when formulated using the NZS game offers flexibility in terms of the achievable trade-off
between the performance of various subsystems due to the Nash equilibrium solution.
7.7 PROBLEMS
7.7.1 Consider N inverted pendulum connected by spring (Spooner and Passino, 1999), which can
be represented of the form (7.2). The dynamics of the system are given by
x˙i1 = xi2,
x˙i2 = (migr
Ji
− kr2
4Ji
)sinxi1 +
kr
2Ji
(l −b) + ui
Ji
+
N
∑
j=1
kr2
4Ji
sinxj1. (7.72)
Treat xi2(t) as the measured variable, N = 10, and let all the subsystems be connected such that
they form a chain network. Design distributed control policies based on the hybrid learning
algorithm using equations in Table 7.2.
7.7.2 For the system described in the previous problem, consider the outputs from each subsystem
to defined as yi(t) = xi1(t). Use the hybrid learning algorithm with distributed observers (see
Table 7.3) to design output-feedback controllers.
7.7.3 Design distributed control policies based on the hybrid learning algorithm using equations in
Table 7.6 for the Problem 7.7.1.
7.7.4 For the same system as in Problem 7.7.1., consider the linearized dynamics as in
(Guinaldo et al., 2011). The parameters in the system dynamics are m1= 2,m2= 2.5,
J1= 5,J2= 6.25,k = 10,r = 0.5,l = 0.5, and g = 9.8,b = 0.5. Pick the initial conditions of
the system states in the interval [0,1] and the initial weights of the NN from [-1,1], randomly.
Treat xi2(t) as the measured variable, N = 10, and let all the subsystems be connected such
that they form a ring network. Design distributed control policies based on the hybrid learning
algorithm using equations in Table 7.3.
7.7.5 Design distributed control policies based on the hybrid learning algorithm using equations in
Table 7.6 for the Problem 7.7.3.
7.7.6 Consider the system of three interconnected systems modeling the robot walking gait. The
controlled equations of motion in units of (rad/s) are
x¨1(t) = 0.1[1−5.25x2
1(t)]x˙1(t)−x1(t) +u1(t)
x¨2(t) = 0.01
1− p2(x2(t)−x2eq)
2

x˙2(t)−4(x2(t)−x2eq)
+0.057x1(t)x˙1(t) +0.1(x˙2(t)−x˙3(t)) +u2(t)
x¨3(t) = 0.01[1− p3(x3(t)−x3eq)
2
]x˙3(t)−4(x3(t)−x3eq)
+0.057x1(t)x˙1(t) +0.1(x˙3(t)−x˙2(t)) +u3(t),
where ¨xi correspond to the dynamics of the i
th subsystem. The control objective is to bring the
robot to a stop in a stable manner (xi → xieq), where xieq are the equilibrium points with x1eq =
0, x2eq = −0.227, x3eq = 0.559. The parameters p2 and p3 are 6070, and 192, respectively.
Design distributed control policies based on the hybrid learning algorithm using equations in
Table 7.3.256 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
7.7.7 Repeat Problem 7.7.5. using Table 7.3 and Table 7.6 with the parameters x1eq = 0, x2eq =
0.559, x3eq = 0.226, p2 = 226, and p3 = 5240.
7.7.8 Consider a nonlinear interconnected system governed by
x˙1 =
 −x11 +x12
−0.5(x11 +x12) +0.5x2
11x12
+
 0
sin(x11)

u1 (7.73)
+

1
0

(x11 +x22)sin2 (ρ1x12)cos(0.5x21) (7.74)
x˙2 =
 0.5x22
−x21 −0.5x22 +0.5x21 cos2 (x22)

+
 0
x21
u2 (7.75)
+

1
0
"
0.5(x12 +x22)cos"
ρ2ex2
21## (7.76)
where x1 = [x11,x12]
T ∈ R2 and x2 = [x21,x22]
T ∈ R2 are the state vectors of subsystems 1
and 2, respectively, u1 ∈ R and u2 ∈ R are the control inputs of subsystems 1 and 2, respec￾tively, and ρ1 ∈ R and ρ2 ∈ R are the unknown parameters. To simplify discussion, randomly
chooseρi ∈ [−1,1],i = 1,2 as in (Yang and He, 2020). Use the state-feedback based controller
with the equations given in Table 7.2.
7.7.9 Use the game-theoretic approach to design control inputs for the system described in the
previous problem. Use the equations given in Table 7.6.
7.7.10 Design distributed controls using the equations given in Table 7.5 for the system described
in (7.76). Now, what happens if the term ∑2
j=1 uT
j Ri juj used in the equation to compute the
integral Bellman error is replaced by uT
i Ri jui −uT
j Ri juj for the i
th subsystem.8 Exploration and Hybrid
Learning for
Interconnected Systems
CONTENTS
8.1 Introduction ........................................................................................................................... 257
8.2 Background and Problem Statement ..................................................................................... 258
8.2.1 System description.................................................................................................... 258
8.2.2 Event-triggered feedback and Optimal control......................................................... 258
8.2.3 Event-sampled NN approximation ........................................................................... 259
8.3 Event-driven Adaptive Dynamic Programming .................................................................... 260
8.3.1 Identifier design for the interconnected system........................................................ 260
8.4 Learning with Exploration for Online Control...................................................................... 263
8.4.1 Enhanced hybrid learning......................................................................................... 263
8.4.2 Identifiers for the enhanced hybrid learning scheme................................................ 263
8.4.3 Role of identifiers and exploration in online control ................................................ 264
8.4.4 Exploration using identifiers..................................................................................... 265
8.5 Stability Analysis................................................................................................................... 267
8.6 Summary and Concluding Remarks...................................................................................... 272
8.7 Problems................................................................................................................................ 273
In this chapter, we shall explore the possibility of systematically improving the distributed con￾trol performance using RL techniques. Specifically, we shall develop a distributed control scheme
for an interconnected system composed of uncertain input-affine nonlinear subsystems with event￾triggered state feedback using an enhanced hybrid learning scheme-based ADP with online explo￾ration. The enhanced hybrid learning scheme was introduced in (Narayanan and Jagannathan, 2017).
In this scheme, the NN weight tuning rules for learning the approximate the optimal value function
is appended with information from NN identifiers that are used to reconstruct the system dynamics
using feedback data. Considering the effects of NN approximation of the system dynamics and the
boot-strapping to extrapolate the optimal values, we shall see the the NN weight update rules in￾troduced in this chapter learns the approximate optimal value function faster when compared to the
algorithms developed in earlier chapters. Finally, we shall also consider incorporating exploration
in the online control framework using the NN identifiers to reduce the overall cost at the expense of
additional computations during the initial online learning phase.
8.1 INTRODUCTION
Advanced control schemes are necessary for the efficient and cost-effective operation of CPS in
diverse applications, e.g., industrial systems with uncertain dynamics. As we have seen thus far in
257258 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
this book, the ADP schemes aim to address the problem of optimization over time through learn￾ing without needing apriori knowledge of the system dynamics. The event-triggered ADP schemes
while reducing computational and communication burden, are still inefficient due to the following
reasons: a) the learning time is increased due to intermittent feedback as the frequency of events de￾cides the approximation accuracy; b) the sampling instants are dynamic, hence, the inter-sampling
intervals are time-varying, restricting the use of iterative learning schemes. The flexible hybrid learn￾ing framework presented in the Chapters 6 and 7 accelerate the learning process with event-triggered
feedback and ensure online, real-time implementation, .
In this chapter, we shall see that the RL techniques together with NN identifiers can be employed
to further improve the efficiency of the control scheme for facilitating real-time implementation.
An improved weight update rule, based on (Narayanan and Jagannathan, 2017), for learning and
enhancing the approximate optimal value function with an online exploration strategy by using the
identifiers, is presented in this chapter. We shall see that the cumulative cost during the learning
phase can be reduced further with the enhanced learning technique presented in this chapter when
compared to the cost incurred with the hybrid learning algorithm at the expense of additional com￾putations.
8.2 BACKGROUND AND PROBLEM STATEMENT
To keep the chapter self-contained, we briefly revisit the system description and optimal control
problem for nonlinear interconnected systems.
8.2.1 SYSTEM DESCRIPTION
Consider a nonlinear input-affine continuous-time system composed of N interconnected subsys￾tems, described by the differential equation
x˙i(t) = fi(xi) +gi(xi)ui(t) +∑N
j=1
j=i
Δi j(xi,xj), xi(0) = xi0, (8.1)
where xi(t) ∈ Bi ⊆ Rni×1 represents the state vector of the i
th subsystem and ˙xi(t) its time derivative,
Bi is a compact set, ui(t) ∈ Rmi is the control input, fi : Bi → Rni , gi : Bi → Rni×mi are uncertain
nonlinear maps, and Δi j : Rni×nj → Rni is the uncertain nonlinear interconnection between the i
th
and the j
th subsystems. The augmented system dynamics are
X˙(t) = F(X) +G(X)U(t), x(0) = x0, (8.2)
where F = [(f1 +∑N
j=2 Δ1 j)
T ,.,(fN +∑N−1
j=1 ΔN j)
T ]
T
, X(t)=[xT
1 (t),., xT
N(t)]T
∈ B ⊆ Rn, B = 6
N
i=1
Bi,
U(t)=[uT
1 (t),..,uT
N(t)]T
∈ Rm, m = ∑N
i=1 mi, n = ∑N
i=1 ni, and G = diag([g1(x1)..,gN(xN)]). The
following assumptions are needed for the control design.
Assumption 8.1. The dynamics (8.1) and (8.2) are stabilizable with equilibrium point at the origin.
Full-state information is available as measurements. The communication network that facilitates
information sharing among subsystems is lossless.
Assumption 8.2. The nonlinear map gi(xi) is bounded such that 0 < gim < gi(xi) ≤ giM in Bi for
every subsystem.
Assumption 8.3. The functions fi(xi), Δi j(xi, x j), gi(xi) are locally Lipschitz continuous on com￾pacts.
In the next subsection, the notion of event-triggered feedback and greedy policy design with
aperiodic event-based feedback is presented.Exploration and Hybrid Learning for Interconnected Systems 259
8.2.2 EVENT-TRIGGERED FEEDBACK AND OPTIMAL CONTROL
Let xi(t
i
k) be the state of the i
th subsystem at time instant t
i
k. Between any successive event-sampling
instantst
i
k,t
i
k+1, the state vector is denoted as 
xi(t) = xi(t
i
k), ∀k ∈ N. Using the ZOH, the last updated
states and control are held at actuators and controllers between events. To denote the difference
between the actual system states and the state available at the controller, an event-sampling error is
defined as
ei(t) = xi(t)−xi(t
i
k), t
i
k ≤ t < t
i
k+1. (8.3)
By rewriting 
xi(t) using (8.3), the feedback between events can be defined as 
xi(t) = xi(t)−ei(t).
Next, define the infinite horizon cost function of the augmented system (8.2), as
V(X(t)) =  ∞
t

Q(X) +UT (τ)RU(τ)

dτ, (8.4)
where Q(X) > 0, ∀X ∈ B\{0}, Q(0) = 0, R > 0 are the penalty functions of appropriate dimen￾sions. Let V(.) and its time-derivative be continuous on a compact set B. Then,
V˙(X(t)) = −
Q(X) +UT (t)RU(t)

.
Using the infinitesimal version of (8.4), define the Hamiltonian function
H(X,U,
∂V
∂X ) = 
Q(X) +UT (t)RU(t)

+ (∂VT /∂X)X˙(t).
The optimal control policy which minimizes (8.4) (assuming a unique minimum exists) is obtained
by using the stationarity condition as U∗(t) = −1
2R−1GT (X)∂V∗/∂X and it is called greedy pol￾icy with respect to (8.4). The Hamiltonian function can be defined between two event-triggering
instants, [tk,tk+1), as
H(X(t),U(tk),
∂V
∂X ) = 
Q(X) +UT (t)RU(t)

+ (∂VT /∂X)X˙(t). (8.5)
The greedy policy with event-triggered state becomes
U∗(t) = −1
2R−1GT (

X)(∂V∗/∂

X) (8.6)
with 
X(t) = X(tk), t ∈ [tk,tk+1). Now, for the interconnected system (8.2) under consideration, the i
th
subsystem dynamics (8.1) are influenced by the states of the j
th subsystem satisfying Δi j(xi,x j) = 0.
Consider the i
th subsystem in (8.1) and the cost function (8.4) for (8.2), then ∃u∗
i (t) ∈ Rmi , given by
ui
∗(t) = −1
2
Ri
−1gi
T (xi)(∂Vi
∗(X)/∂ xi), ∀i ∈ 1,2,..N. (8.7)
as a function of xi(t), x j(t), for all j ∈ 1,2,..,N, : Δi j = 0, where Vi
∗(X) represent the optimal value
function of the i
th subsystem, Ri is a positive definite matrix, such that the cost function (8.4) is
minimized. Chapter 7 presents a learning algorithm to synthesize this distributed control policy
(8.7).
8.2.3 EVENT-SAMPLED NN APPROXIMATION
With the objective of finding the approximate optimal value function as an approximate solution to
the HJB using aperiodic event-triggered feedback, we have utilized event-based NN approximation
in the prior chapters. To recall, we define a smooth function, χ : B → R, in a compact set B ⊆ Rn.260 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Given εM > 0 there exists θ∗ ∈ Rp×1 such that χ(X) = θ∗T
φ(Xe) +εe. The event-triggered approx￾imation error εe is defined as εe = θ∗T
(φ(Xe +e)−φ(Xe)) +ε(X), satisfying εe < εM, ∀Xe ∈ B,
where X,Xe are continuous and event triggered variables, e is the measurement error due to event
sampling, ε(X) is the bounded NN reconstruction error, and φ(Xe) is an appropriately chosen basis
function. The following assumption is required for the ADP design.
Assumption 8.4. The solution for the HJB (8.5) is unique, real-valued, smooth and satisfies
V∗(X) = ∑N
i=1V∗
i (X). Further, φ(X) is chosen such that φ(0) = 0, the activation function and its
derivative, and the target NN weights are assumed to be bounded.
The parameterized representation of the optimal value function using NN weights θ∗ and basis
function φ(Xe) with event-based inputs is given as
V∗(X) = θ∗T
φ(Xe) +ε(Xe), (8.8)
where ε(Xe) is the event-driven reconstruction error. Define the target NN weights as θ∗
i at the i
th
subsystem. Using a parameterized representation (8.8) for V∗
i (X), HJB equation can be derived as
θ∗T
i ∇xφ(X)
˙
f i − θ∗T
i ∇xφ(X)Di∇T xφ(X)θ∗
i
4 +εiHJB +Qi(X) = 0 (8.9)
where Qi(x) > 0, Di = gi(xi)Ri
−1gi
T (xi), ˙
f i = fi(xi) + ∑N
j=1
j=i
Δi j, and εiHJB = ∇xεT
i (
˙
f i −
1
2Di(∇T xφ(X)θ∗
i + ∇xεi) + 1
4Di∇xεi). The estimated value function is given by Vˆ
i(X) = θˆT
i φ(X),
where θˆ
i is the NN estimated weights and its gradient along the states is given by ∂Vˆ
i/∂ xi =
θˆT
i ∇xφ(X) and ∇xφ(X) is the gradient of the activation function φ(X) along X. The Hamiltonian
function using Vˆ
i(Xe) = θˆT
i φ(Xe) reveals
Hˆ =Qi(Xe) +θˆT
i ∇xφ(Xe)
˙
f i − 1
4
θˆT
i ∇xφ(Xe)Di,ε∇T
xφ(Xe)θˆ
i, (8.10)
where Di,ε = Di,ε (xi,e) = gi(xi,e)Ri
−1gi
T (xi,e). The estimated optimal control input is obtained from
(8.10) as
ui,e(t) = −1
2
Ri
−1gT
i (xi,e)θˆT
i ∇xφ(Xe), ∀i ∈ 1,2,..N. (8.11)
Note that (8.10) is used as the forcing function to tune θˆ
i. The NN identifier design with event￾triggered feedback is presented in the next subsection. The identifiers are utilized to generate uncer￾tain nonlinear functions and also for the purpose of exploration, which will be discussed in Section
8.4.
8.3 EVENT-DRIVEN ADAPTIVE DYNAMIC PROGRAMMING
In this section, first, we shall design NN identifiers at each subsystem to approximate the uncertain
nonlinear functions in (8.1). These NN identifiers will be used to design an event-triggered hybrid
learning algorithm for constructing an approximately optimal control sequence.
8.3.1 IDENTIFIER DESIGN FOR THE INTERCONNECTED SYSTEM
For approximating the subsystem dynamics, consider a distributed identifier at each subsystem,
which operates with event-triggered feedback information
˙
xˆi(t) = ˆfi(xˆi) +gˆi(xˆi)ui,e(t) +∑N
j=1
j=i
Δˆi j(xˆi,xˆj)−Aix˜i,e(t), (8.12)Exploration and Hybrid Learning for Interconnected Systems 261
where ˜xi,e(t) = xi,e(t)−xˆi(t), is the event-driven state estimation error and Ai > 0 is a positive def￾inite matrix which stabilizes the NN identifier. To distinguish the identifiers developed in the next
section, we use x(t) to denote the states of the overall system instead of X(t). Using NN approxima￾tion, the parametric equations for the nonlinear functions in (8.1) are gi(xi) = Wigσig(xi) +εig(xi), ¯fi(x) = Wi f σi f(x) +εi f(x); where Wi• denotes the target NN weights, σi• denotes the bounded NN
activation functions, and εi• denotes the bounded reconstruction errors. Using the estimate of the
NN weights, Wˆi•, define ˆ¯f i(x) = Wˆi f σi f(x) and ˆgi(xˆi) = Wˆigσig(xˆi). Now, to analyze the stability
of (8.12), define the state estimation error ˜xi(t) = xi(t)−xˆi(t). Using (8.12) and (8.1), the dynamic
equation describing the evolution of ˜xi(t) is revealed as
˙
x˜i(t) = W˜i f σi f(x) +Wi f σ˜i f −W˜i f σ˜i f + [W˜igσig(xi)+
Wigσ˜ig −W˜igσ˜ig]ui,e(t) +εigui,e(t) +εi f +Aix˜i +Aiei(t), (8.13)
with σ˜i• = σi•(x)−σi•(xˆ) and W˜i• = Wi• −Wˆi•.
Remark 8.1. Note that the approximation of ¯f(x) requires the states of the ith and jth subsystem
satisfying Δi j(xi, x j) = 0. Therefore, the inputs to the NN are xˆi,x j,e, and x˜i. Due to the presence of
x j,e as input, the identifier is considered to be distributed.
With the NN identifiers at each subsystem, the control design equations (8.10) and (8.11) can be
re-derived as
Hˆ = Qi(xe) +θˆT
i ∇xφ(xe) ˆ¯f i − 1
4
θˆT
i ∇xφ(xe)Dˆi,ε∇T
xφ(xe)θˆ
i
and
ui,e(t) = −1
2
Ri
−1gˆ
T
i (xi,e)θˆT
i ∇xφ(xe),
with Dˆi,ε = gˆi(xi,e)Ri
−1gˆ
T
i (xi,e). All the design equations to learn the greedy policy u∗
i (t) without
requiring the nonlinear functions ¯fi,gi and V∗
i are presented next.
Lemma 8.1. Consider the identifier dynamics (8.12). Using the estimation error, x˜i(t), as a forcing
function, define NN weight tuning using the Levenberg-Marquardt scheme with sigma modification
term to avoid parameter drift as
˙
Wˆ i f(t) =
αi f σi f x˜
T
i,e
ci f +


x˜
T
i,e



2 −κi fWˆi f , ˙
Wˆ ig(t) =
αigσigui,ex˜
T
i,e
ci f +


x˜
T
i,e



2

uT
i,e



2 −κigWˆig, (8.14)
where αi f ,αig,κi f ,κig, ci f are positive design constants. The error dynamics using (8.14) are ob￾tained as
˙
W˜ i f(t) = −αi f σi f x˜
T
i,e
ci f +


x˜
T
i,e



2 +κi fWˆi f , ˙
W˜ ig(t) = −αigσigui,ex˜
T
i,e
ci f +


x˜
T
i,e



2

uT
i,e



2 +κigWˆig. (8.15)
Let all the assumptions introduced in the previous sections hold. If ui,e(t) = u∗
i,e , then
αi f ,αig,κi f ,κig,Ai > 0 can be chosen such that (8.13) and (8.15) are stable and x˜i(t),W˜i•(t) are
locally UUB.
Sketch of Proof: The proof considers a Lyapunov candidate function and the result is established by
showing that the time-derivative of the function is negative outside a bound.262 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Table 8.1
Distributed NN identifier design equations
Identifier dynamics ˙
xˆi(t) = ˆfi(xˆi) +gˆi(xˆi)ui,e(t) +∑N
j=1
j=i
Δˆi j(xˆi, xˆj)−Aix˜i,e
Event-driven state estimation error ˜xi,e = xi,e −xˆi
NN approximations ˆ¯f i(x) = Wˆi f σi f(x)
gˆi(xˆi) = Wˆigσig(xˆi)
Weight update rules ˙
Wˆ i f(t) = αi f σi f x˜
T
i,e
ci f +


x˜
T
i,e



2 −κi fWˆi f
˙
Wˆ ig(t) = αigσigui,ex˜
T
i,e
ci f +


x˜
T
i,e



2

uT
i,e



2 −κigWˆig
 The following Lyapunov candidate function is considered
JiI(x˜i,W˜i f ,W˜ig) = Jix˜ +Ji ˜f +Jig˜,
with Jix˜ = 1
2 μi1x˜
T
i Pix˜i, Ji ˜f = 1
2 μi2W˜ T
i f W˜i f + 1
4 μi4(W˜ T
i f W˜i f)
2 and Jig˜ = 1
2 μi3W˜ T
igW˜ig +
1
4 μi5(W˜ T
igW˜ig)
2 + 1
8 μi6(W˜ T
igW˜ig)
4
. Here the variables μi j,Pi, j = 1,2,.,6, are positive con￾stants of appropriate dimensions.
 For the optimal control policy, the bounds are obtained as a function of reconstruction error
associated with the NNs approximating the unknown functions fi, gi, and the upper-bound
on the ideal NN weights.
 The bounds can be reduced by tuning the parameters αi f , αig, and the reconstruction error
can be reduced with appropriate choice of the NN architecture.
Remark 8.2. The assumption that the control input is optimal and the measurement error acting
as an input ei(t) is bounded will be relaxed in the closed loop stability analysis. The stability of the
identifier in the presence of measurement errors is required to employ the identifiers for the pur￾pose of exploration, wherein the measurement errors in (8.13) are replaced by bounded exploratory
signals.
Recall the Lyapunov function-based event-triggering condition used in Chapter 7. Using similar
approach, an event triggering mechanism can be designed at each subsystem to determine the dis￾crete time instants. At these events - 1) the i
th subsystem controller receives xi(t), 2) ui(t) is updated
with the latest states at the actuator, and 3) xi(t) is broadcast to the neighboring subsystems. Define a
positive definite, continuous function Ji(xi) = xT
i Γixi, with Γi > 0. For 0 < αi < 1 and k ∈ N, design
the event-triggering mechanism to satisfy the condition
Jix(xi(t)) ≤ (1+t
i
k −t)αiJix(xi(t
i
k)), t ∈ [t
i
k,t
i
k+1). (8.16)Exploration and Hybrid Learning for Interconnected Systems 263
with t
i
0 = 0, ∀i ∈ 1,2,..,N. The hybrid learning scheme presented in Chapter 7 is best suitable
for online implementation. Nevertheless, the hybrid learning scheme does not utilize the feedback
information and the reward signal available during the inter-event period. The classical problem of
exploration vs exploitation and a modified/enhanced learning algorithm, which strives to exploit the
under-utilized information are presented next.
8.4 LEARNING WITH EXPLORATION FOR ONLINE CONTROL
The basic idea behind the enhanced hybrid learning scheme is presented first and the role of the
identifiers will be highlighted following that. The identifiers presented in the previous section are
used to approximate the subsystem dynamics. Together with these identifiers, the distributed con￾trol schemes presented in Chapter 7 can be modified to accommodate uncertainity in the system
dynamics. In this section, the NN identifiers that approximate the overall system dynamics at each
subsystem will be presented. We shall see that these identifiers aid in the implementation of the
enhanced weight update rule presented in this section.
8.4.1 ENHANCED HYBRID LEARNING
The algorithms presented in the previous chapters lack efficiency in learning as they do not utilize
the state and control information along the state trajectory during the inter-event period. To address
this issue, we may consider storing and using this information to update the weights of the value
function NN at the event-sampling instant. However, it should be noted that while the state infor￾mation during the inter-event period is measured and utilized at the event triggering mechanism, it
is not available at the controller/learning mechanism. To overcome this limitation, we may consider
storing the state and control information at the trigger mechanism and transmitting it to the controller
at the event-sampling instants. This approach requires the transmission of states from the sensor to
the controller at each subsystem and broadcasting to other subsystems in the interconnected system.
Consequently, this increases the communication overhead, as the packet size will increase due to
fewer events. Nevertheless, this approach enhances the learning efficiency and ensures that valuable
information is utilized for updating the value function NN.
Building on this basic observation, we may incorporate the identifier located at each subsystem
in the controller to generate the data (corresponding to the inter-event time) and can be used in the
learning process. However, the use of the online identifier and the controller together will lead to
unreliable set of data for the value function estimator. As we shall see later in this chapter, by tuning
the identifier weights in such a way that they converge faster that the value function NN, the data
generated by the identifier can be utilized for learning the optimal value function. Let the sensor
sampling frequency be defined as τs. Consider the weight-tuning rule
˙
θˆi(t) =
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
− αi1vψˆiHˆi
(1+ψˆ T
i ψˆi)
2 − αi2vΨˆ iη¯i
(1+Ψˆ T
i Ψˆ i)
2 −κ3θˆ
i +
1
2
αiv∇xφDˆixi
+
1
2
μi1∇xφ(x)Dˆ T
i Pix˜i, t = t
i
k,
−(αivψˆiHˆi)/(1+ψˆ T
i ψˆi)
2
, t
i
k < t < t
i
k+1, ∀k ∈ N,
(8.17)
with η¯i,Ψˆ i are the estimated Hamiltonian and its derivative with respect to the NN weights calcu￾lated using the estimated states during the inter-event period. Since η¯i is a function of the overall
states, a NN identifier that approximates the overall system can provide the overall state estimate at
each subsystem and the design of such an identifier is briefly presented next.264 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Table 8.2
Distributed NN control design equations
Identifier dynamics ˙
Xˆi(t) = Fˆ
i(Xˆi) +Gˆi(Xˆi)Ui,e(t)−AiX˜i,e(t)
HJB error Hˆ = Qi(xe) +θˆT
i ∇xφ(xe)
˙
f i −0.25θˆT
i ∇xφ(xe)Di,ε∇T xφ(xe)θˆ
i
Estimated value function Vˆ
i(x) = θˆT
i φ(x), Di,ε = Di,ε (xi,e) = gi(xi,e)Ri
−1gi
T (xi,e)
Hybrid learning ˙
θˆi(t) = $
−(αivψˆiHˆi)/(1+ψˆ T
i ψˆi)
2
+0.5μi1∇xφDˆ T
i Pix˜i −κ3θˆ
i +0.5αiv∇xφDˆixi, t = t
i
k
−(αivψˆiHˆi)/(1+ψˆ T
i ψˆi)
2
, t
i
k < t < t
i
k+1, ∀k ∈ N.
Enhanced hybrid learning ˙
θˆi(t) = $
−(αivψˆiHˆi)/(1+ψˆ T
i ψˆi)
2
+0.5μi1∇xφDˆ T
i Pix˜i −κ3θˆ
i +0.5αiv∇xφDˆixi, t = t
i
k
−(αivψˆiHˆi)/(1+ψˆ T
i ψˆi)
2
, t
i
k < t < t
i
k+1, ∀k ∈ N.
8.4.2 IDENTIFIERS FOR THE ENHANCED HYBRID LEARNING SCHEME
Consider the NN identifier at each subsystem as
˙
Xˆi(t) = Fˆ
i(Xˆi) +Gˆi(Xˆi)Ui,e(t)−AiX˜i,e(t), (8.18)
where the subscript i indicates variables available at the i
th subsystem, Fˆ
i,Gˆi are the approximated
functions of the overall dynamics F,G, Xˆ is the estimate of X in (8.2), and U is the augmented
control ui. In contrast to (8.12), the identifier described by (8.18) estimates the states of the inter￾connected system (8.2) to collect the state information and calculate the reinforcement signal for
the inter-event period. The actual and estimated weights for the functions Fi,Gi can be defined as in
Section 8.3.1 and equations similar to (8.13)-(8.15) can be derived for the observer in (8.18).
Remark 8.3. The observer design procedure for (8.18) is similar to that in (8.12). Therefore, all
the details are not included. However, there are a few subtle differences in the NN design. Since the
observer in (8.18) approximates the nonlinear mapping of the overall system, first, the NN takes as
input, the vector [xˆ
T
i xˆ
T
j ]
T
,∀ j = 1,2,..N instead of xˆi. Secondly, the number of neurons in the hidden
layer should be increased as the domains of the nonlinear maps being approximated are of higher
dimensions.
The local UUB stability of the identifier presented in the previous section is applicable to the
identifier designed in this section as well.
Remark 8.4. The use of function approximators to learn the optimal value function and system
dynamics adds to the uncertainty of bootstrapping in finding the optimal control inputs. In addi￾tion, since the learning scheme is based on asynchronous GPI, the initial weights of the function
approximators affect the state trajectory and cumulative cost (return).
8.4.3 ROLE OF IDENTIFIERS AND EXPLORATION IN ONLINE CONTROL
One of the classical problems in the RL literature (Sutton and Barto, 1998; da Silva and Barto,
2012) is the dilemma of exploration vs exploitation. To understand this problem, let us consider
the RL decision-making problem. The decision-making process consists of constructing maps ofExploration and Hybrid Learning for Interconnected Systems 265
states to expected future rewards using reinforcement signals (Sutton and Barto, 1998). The future
actions are influenced by this prediction of future reward, i.e. using the feedback signal, the HJB
error is computed and the approximate optimal value function is updated based on the HJB error; the
estimated value function is then used to obtain the future control action. If the control action is of the
form (8.11), then it is a greedy policy and hence, exploitative. This is due to the fact that the control
policy exploits the current knowledge of the optimal value function and minimizes the Hamiltonian
(8.10). In contrast, if a control policy that is not greedy is applied to the system, then the control
policy is said to be explorative. One has to ensure stability when such a policy is used in online
control. The PE condition is an important requirement for the ADP control methods in (Lewis et al.,
2012a) for the convergence of the estimated parameters to its target values. This condition ensures
that sufficient data is collected to learn the unknown function before the system states settle at an
equilibrium point. Adaptive control theorists developed sigma and epsilon modification techniques
(Sastry and Bodson, 2011; Lewis et al., 1998) to prevent parameter drift and relax PE condition
requirement. However, from a learning perspective, the sigma and epsilon modification techniques
inhibit the learning algorithm from exploring.
To perturb the system and to satisfy the PE condition a control policy of the form ϖe(t) =
u(t) + ξ (t) was used in the learning algorithms presented in (Lewis et al., 2012a; Dierks and Ja￾gannathan, 2010a) and the references therein, where ξ (t) is seen as an exploratory signal and u
is a stabilizing/greedy control policy. For example, random noise signal was used as ξ (t) in the
simulations, while (Lee et al., 2015) explicitly considered the control law with ξ (t) to develop an
actor-critic based ADP design. To relax the PE condition, sufficient data can be collected to satisfy
the rank condition, as indicated in traditional adaptive control (Sastry and Bodson, 2011). It should
also be considered that exploration signal ξ (t) is not easy to design. Although several exploration
policies are investigated for finite Markov decision processes (Sutton and Barto, 1998) and offline
learning schemes (Sutton and Barto, 1998; da Silva and Barto, 2012), an exploration policy that can
provide guaranteed time for convergence to a near-optimal policy for an online control problem are
not available.
Also, in control, issues of stability and robustness are non-trivial. The system can become unsta￾ble in the process of exploration due to the application of ξ (t) in the control action. Inspired by the
work on efficient exploration in (da Silva and Barto, 2012), a novel technique to incorporate explo￾ration in the learning controller is developed in (Narayanan and Jagannathan, 2017). This technique
is presented next.
8.4.4 EXPLORATION USING IDENTIFIERS
The TD learning (Sutton and Barto, 1998; Dierks and Jagannathan, 2010a; Sahoo et al., 2016) and
the hybrid learning schemes (Narayanan and Jagannathan, 2016b,a) reduce the HJB error. But this
error is not reduced to zero, Hˆi(x,ui) = 0, whenever the control action is updated, i.e., optimality is
achieved only in the limit (t → ∞,Vˆ → V∗). Further, in asynchronous learning (Sutton and Barto,
1998), the optimal value function is learned only along the state trajectory and not the entire state
space. Therefore, the initial weights of the value function approximator affect the cumulative cost of
operating the system. To minimize the cost during the learning period an exploration strategy using
identifiers is presented next.
First, consider the identifier described by (8.18). To illustrate the idea, we will consider two
sets of initial weights, one of which will be used by the controller to generate the control ac￾tion ϖ(1)
ie (t) = u
(1)
i (t) + ξ (1)
i (t), such that ξ (1)
i (t) = 0; the other one will be an exploratory policy
ϖ(2)
ie (t) = u
(2)
i (t) +ξ (2)
i (t) with ξ (2)
i (t) = 0, used with the identifier. Figure 8.1 is a simplified block
diagram representation for implementing the exploration strategy. It can be observed that in order to
incorporate exploration without affecting the performance of the existing controller, an additional
identifier and value function estimator are required.266 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Let Θˆ 1i,Θˆ 2i be the weight vectors at the i
th subsystem. Calculate the Hamiltonian as Hˆ (p)
i (xˆe) =
Qi +Θˆ T
pi∇xφ(xˆe) ˆ¯f i − 1
4Θˆ T
pi∇xφ(xˆe)Dˆi∇T xφ(xˆe)Θˆ pi, where p = 1,2 for each initial weights. We can
construct the cost function trajectory with the value function estimator using the NN weights Θˆ 1i,Θˆ 2i
for both the policies ϖ(1)
ie ,ϖ(2)
ie . Similar to (8.7), the stationarity condition can provide the ui,e from
Hˆ (p)
i . Using the Hamiltonian error, the NN weights are tuned using the weight update rule (8.17).
Thus, we can obtain two policies, one exploitative and the other using an exploration policy. For
Figure 8.1 Block diagram representation of exploration strategy.
example, a random exploration policy can be used. For each initial NN weight, a cost function,
control policy, Hamiltonian error, and state trajectory are generated. During the learning period,
using the performance index, the cumulative cost be calculated, for p ∈ {1,2}, using the integral
V(p)
i (t) =  tswitch
t

Qi(x) +ϖ(p)
T
ie (τ)Riϖ(p)
ie (τ)

dτ. (8.19)
Note that the value function trajectories for the two policies start at the same initial cost and
evolve based on the function Qi(x) +ϖ(p)
T
ie (τ)Riϖ(p)
ie (τ). Let the time instant t = tswitch denote the
time at which the difference between the cumulative rewards due to the two control policies start
to increase steadily. Define Vˆ ∗
i (Θ) = min{V1
i (t),V2
i (t)}. Using the value function approximator NN
that corresponds to the estimate Vˆ ∗
i (Θ) generate the greedy policy at the event-based sampling in￾stants t
i
k ≥ tswitch, ∀k ∈ N. If both the policies result in the same cumulative cost V1
i (t),V2
i (t), the
reliability of the cost function estimate can be evaluated by using their HJB error. Choose the esti￾mated value function Vˆ ∗
i such that θˆ
i satisfies the condition θˆ
i = min(argmin
Θ1
(Hˆ 1
i ), argmin
Θ2
(Hˆ 2
i ) ).
Thus, Vˆ ∗
i which is close to the optimal value function is used to generate the control action and po￾tentially minimize the cost during the learning period. Note that the exploration policy need not nec￾essarily yield a reduced cost function trajectory during the learning period. However, it is observed
during the simulation analysis that the appropriate choice of exploration policy can significantly
reduce the cost during the learning period.
Remark 8.5. The sigma/epsilon modification term (κ3θˆ
i) added in the learning rule (8.17) ensures
that the approximated value function reach a neighborhood of the optimal value function withoutExploration and Hybrid Learning for Interconnected Systems 267
compromising the stability. Further, the control action ϖie generated using the learning algorithm
without the exploration strategy (ξi = 0) is always exploitative as ϖie = ui,e, minimizing the cost
function (8.4). Therefore, injecting exploratory signal ξi, to the identifier and searching for a better
policy using the exploration strategy is not going to affect the system performance or stability. In
contrast, it can only improve the optimality of the control action. Therefore, it is a very efficient tool
for online learning and control applications.
Using Lyapunov-based analysis, the stability results for the closed-loop system are summarized,
from (Narayanan and Jagannathan, 2017), next.
8.5 STABILITY ANALYSIS
In this section, first, a more general result establishes the fact that the continuously updated closed￾loop system admits a local input-to-state practically stable Lyapunov function in the presence
of bounded external input (measurement error). This result is required to ensure that the event￾triggering mechanism does not exhibit Zeno behavior. Further, it is shown using two cases that
as the event-sampling instants increase, the states, weight estimation errors, and identifier errors
reach a neighborhood of origin. Using the fact that the optimal controller renders the closed-loop
dynamics bounded reveals that there exists a function δ and a constant C such that
 f(x) +g(x)u∗(t) ≤ δ(x) = C1 x(t), (8.20)
where δ(x) ∈ Rn, C1 ∈ R. This fact is used in the derivations to demonstrate system stability.
Theorem 8.1. Consider the subsystem dynamics (8.1). Define the NN weight update rule (8.17)
for the value function approximator and (8.14) for the identifiers. Let Assumptions 8.1 to 8.4 hold.
Then, αiv,μi,κ3 > 0 can be designed such that θ˜
i,W˜i f ,W˜ig, and x,x˜i are locally uniformly ultimately
bounded when a bounded measurement error is introduced in the feedback.
Sketch of Proof: The object of the proof is to establish the local ISS of the subsystems and the overall
system.
 Consider the Lyapunov function for the interconnected system
J = ∑N
i=1 Ji(xi,θ˜
i,X˜i,W˜i f ,W˜ig),
with the individual terms defined as Ji = Jix + Jiθ˜ + JiI(X˜i,W˜i f ,W˜ig) and Jix = 1
2αivxT
i xi,
Jiθ˜ = 1
2θ˜T
i γiθ˜
i.
 The time-derivative of the term JiI(X˜i,W˜i f ,W˜ig) can be bounded using arguments from
Lemma 8.1.
 The bounds on the time-derivative of the other terms can be computed similarly for
Lyapunov functions of each subsystem. However, the error dynamics dues to the state￾estimation error and the subsystem dynamics yields a positive term that increases the ulti￾mate bounds.
 The second and fourth terms in the weight update rule (8.17) are used to compensate and
cancel these positive terms, leading to reduced bounds on the closed-loop signals.
Theorem 8.2. Consider the augmented nonlinear system (8.2) and its component subsystems (8.1).
Define the NN weight update rule (8.17) for the value function approximator and (8.14), for the
identifiers. Let events be generated based on the event-triggering condition defined by (8.16). Then,
the control parameters can be designed such that θ˜
i,W˜i f ,W˜ig and x, x˜i are locally uniformly ulti￾mately bounded.268 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
The proof of Theorem 8.2 is a special case of Theorem 8.3.
Remark 8.6. From the results of Theorem 8.1, the closed-loop system admits a Lyapunov function
which satisfies the local input-to-state practical stability (ISpS) conditions when the measurement
error is bounded. By analyzing the same Lyapunov function during the inter-event period, using the
event-triggering condition, the boundedness of the measurement can be established.
Remark 8.7. Appropriate choice of design parameters will result in lower bounds on x, x˜i and
θ˜
i,W˜i f ,W˜ig. Redundant events can be avoided using a dead-zone operator (Sahoo et al., 2016).
Remark 8.8. Define the minimum time between two events as δtmin = min{tk+1−tk}, ∀k ∈ N. Then
δtmin > 0 as a result of Assumption 8.3, Theorems 8.1 and 8.2 (Sahoo et al., 2016).
Now the close-loop stability results with the enhanced learning algorithm and exploration are
presented.
Theorem 8.3. Consider the augmented nonlinear system (8.2) and its component subsystems (8.1).
Define the NN weight update rule (8.14) for the identifiers (8.18). Define the event-triggering con￾dition (8.16), further, let the NN weights of the value function estimator be tuned based on the rule
(8.17). Under the assumptions prescribed in the previous sections, the control parameters can be
designed such that the NN weight estimation errors θ˜
i,W˜i f ,W˜ig, the interconnected system states,
and x˜(t) are locally uniformly ultimately bounded.
Sketch of Proof: Since local ISS of the subsystems and the overall system are established in Theorem
8.1, we just demonstrate that since the measurement errors are bounded by the event-triggering
condition, the UUB results still hold.
 Consider the Lyapunov candidate function
J(x,Θ˜ ,X˜,W˜ ) = ∑N
i=1 Ji(xi,θ˜
i,X˜i,W˜i f ,W˜ig),
with Ji(xi,θ˜
i,X˜i,W˜i f ,W˜ig) = Jix +Jiθ˜ +JiI(X˜i,W˜i f ,W˜ig).
 The proof consists of two cases corresponding to measurement error being zero and non￾zero. This set of arguments are consistent with the proofs used to demonstrate the closed￾loop stability of systems implemented in emulation-based event-triggered control frame￾work, where ISS assumptions are used to ensure that the measurement error does not cause
finite-escape time, keeping the closed-loop signals from growing unbounded.
Example 8.1. In this example, three coupled nonlinear subsystems are considered. The three sub￾systems are physically meaningful in that they capture the thigh and knee dynamics of a walking
robot experiment (Dunbar, 2007). In the following, γ1(t) is the relative angle between the two thighs,
γ2(t) is the right knee angle (relative to the right thigh), and γ3(t) is the left knee angle (relative to
left thigh). The controlled equations of motion in units of (rad/sec) are
γ¨1(t) = 0.1[1−5.25γ2
1 (t)]γ˙1(t)−γ1(t) +u1(t),
γ¨2(t) = 0.01
1− p2(γ2(t)−γ2e)
2

γ˙2(t)−4(γ2(t)−γ2e)
+0.057γ1(t)γ˙1(t) +0.1(γ˙2(t)−γ˙3(t)) +u2(t),
γ¨3(t) = 0.01
1− p3(γ3(t)−γ3e)
2

γ˙3(t)−4(γ3(t)−γ3e)
+0.057γ1(t)γ˙1(t) +0.1(γ˙3(t)−γ˙2(t)) +u3(t),
where γ¨i(t) correspond to the dynamics of the ith subsystem (SSi). The control objective is to bring
the robot to a stop in a stable manner. The parameter values (γ2e, γ3e, p2, p3)(t) can be considered
in the model taking on the values (−0.227,0.559,6070,192).Exploration and Hybrid Learning for Interconnected Systems 269
Figure 8.2 State Trajectories (Dotted (red) Lines – Hybrid Algorithm vs Enhanced hybrid algorithm) (Time
in 10−2s).270 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
0
0.5
Control input
es
0 500 1000 1500 2000
−1
−0.5
Time in (x10−
2) s
Torqu
ss1
ss2
ss3
Figure 8.3 Control torques.
The control scheme presented in this chapter requires three NNs at every subsystem. For this ex￾ample, all the NNs were designed to have two layers and formed random vector functional link net￾works (Lewis et al., 1998). The NN that approximated fi(x)+Δi j(x), was designed with 25 neurons
in the hidden layer. The other two NNs that approximated gi,V∗
i were designed with 7,6 hidden layer
neurons respectively. The following initial conditions were set for the simulation: xi(0) ∈ [−1,1],
xˆ(0) = 0, θˆ
i(0),Wˆi f(0),Wˆig(0) ∈ [0,1]. The controller parameters are selected as follows: αi1v = 40,
αi2v = 0.03, μi = 1.95, Pi = 2,κ3 = 0.001, Qi = 20, Ri = 1,Ai = 80, Ci f = 0.5, κi f = κig = 0.0001,
αi f = αig = 100 and Γi = 0.99.
The robotic system is simulated with the torques generated using the control algorithm with hy￾brid and enhanced (modified) hybrid approach and exploration. It can be observed that the states
reach their equilibrium point faster in the modified hybrid approach (Figure 8.2). The magnitude
of the control torque for the hybrid ADP-based learning scheme and the enhanced hybrid approach
are compared in Figure 8.3 using the event-triggered feedback. The enhanced hybrid scheme con￾verges faster due to the improved learning as a result of using the reinforcement signals during the
inter-event period for tuning the NN weights. Convergence of the identification error ensures that
the reinforcement signals used to learn optimal value function and policy are reliable. To test the an￾alytical results for the identifier, 500 different initial conditions and exploration signals like random
noise and trigonometric functions of different frequencies but restricted in magnitude to 0.1 were
used. The state estimation errors converged on each of these simulations as seen in Figure 8.4.
The optimal value function is learned using the consistency condition dictated by the HJB equa￾tion. A lower Hamiltonian/HJB residual error implies that the value function weight estimate is
close to the target weights. Evidently, from Figure 8.5, the enhanced/modified weight tuning rule
improves the optimality due to faster convergence of the HJB residual error. The cumulative cost
calculated for the hybrid learning algorithm and the modified update rule taking into account the
states and reinforcement evaluated in the inter-event period are compared in Figure 8.6. For 500
randomly chosen initial values of states of the system and identifier, the ratio of the cumulative
cost at the end of 20s for hybrid and the learning algorithm dicussed in this chapter is recorded in
Figure 8.6. Due to the dependence of the learning scheme on the identifier, the convergence of the
identification errors should precede the convergence of the controller.
The improvement in the learning scheme is a result of the weights updated between events using
the past data and the exploration strategy. Finally, four additional NN approximators were utilized,
each initialized with the weights randomly selected in [0,2]. To demonstrate the efficiency of the
exploration strategy in off-setting the effects of initial NN weights, each of the randomly picked
weights was used to generate a control policy and the cost function over time using additionalExploration and Hybrid Learning for Interconnected Systems 271
0 500 1000 1500 2000 2500 3000
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
Time (x10−
2 sec)
˜x
Identification error
Figure 8.4 Identifier approximation error.
Figure 8.5 Comparison of HJB error.
0 100 200 300 400 500
0.9
0.92
0.94
0.96
0.98
1
Initial conditions
Proposed learning/hybrid algorithm
Ratio of cumulative cost for different initial conditions
Figure 8.6 Comparison of cost.272 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
0 1 2 3 4 5
0
0.2
0.4
0.6
0.8
1
Effect of Exploration
time in sec
Cumulative cost
Θ*
1
(0)
Θ1
(0)
Θ2
(0)
Θ3
(0)
Θ4
(0)
Figure 8.7 Cost function trajectories.
identifiers for each NN weight. These cost trajectories are compared with the cost function trajectory
of the system with the exploration strategy presented in the previous section. The variable Θ∗
1(t) is
the estimated NN weights that were used to generate the control action sequence, selected by the
exploration strategy, online. This seemed to optimize the performance of the system better than the
other policies as seen in Figure 8.7.
Since multiple NNs were used to generate the cost function trajectories using the identifier states,
computations increased. However, the effect of the initial weights of the NN approximator on the
cost function trajectory is reduced and the learning algorithm eventually used the optimal approxi￾mated value function, which yields the best sequence of control policy, when compared in terms of
the resulting cost functions. To test the event-triggering mechanism, the sensors were sampled at 1
ms and the number of events generated are recorded. The ratio of the total number of events from
the 3 subsystems with the total number of sensor samples collected is computed as 0.5108 for the
enhanced hybrid learning scheme and 0.4981 for the hybrid learning scheme. This demonstrates the
benefits of the enhanced NN weight update rule when compared with the hybrid learning rule as
almost 51% of the sensor information sampled at the event-triggering mechanism is not used by the
learning algorithm in the hybrid learning scheme.
Remark 8.9 (Tracking control tasks). Recall that converting a tracking control problem into a
regulation problem with augmented system dynamics, presented in Chapter 7 in Section 7.5, for
nonlinear interconnected system required that the system dynamics are accurately know. We can
follow the steps discussed earlier in Section 8.4.2 to learn the unknown dynamics Gi,Fi, and Ki j in
the tracking error dynamics (7.70), which can then be used to synthesize the tracking control using
the enhanced hybrid-learning-based control scheme presented in this chapter. With the augmented
system, we can design a feedback controller that stabilizes the augmented system at the desired
operating point. This controller will implicitly regulate the original system to track the reference
trajectory. The gains of the controller need to be tuned to achieve the desired tracking performance
and stability.
8.6 SUMMARY AND CONCLUDING REMARKS
The enhanced hybrid learning scheme with exploration uses a system model, which in turn is recon￾structed using NN identifiers. Local UUB stability of the controlled system ensures that the system
states, NN weights estimation errors, and identification errors are all bounded. The NN identifiers
approximated the system nonlinearities and also aided in evaluating the exploration signals to gatherExploration and Hybrid Learning for Interconnected Systems 273
useful information about the system dynamics which improved the optimality of the control actions.
This learning scheme seems to match and better the performance of the continuous-time TD-ADP
learning scheme with limited feedback information. This is achieved with significantly more com￾putations.
8.7 PROBLEMS
8.7.1 Consider N inverted pendulum connected by spring (Spooner and Passino, 1999). The dy￾namics of the system are given by
x˙i1 = xi2,
x˙i2 = (migr
Ji
− kr2
4Ji
)sinxi1 +
kr
2Ji
(l −b) + ui
Ji
+
N
∑
j=1
kr2
4Ji
sinxj1. (8.21)
Treat xi2(t) as the measured variable, N = 10, and let all the subsystems be connected such
that they form a chain network.
(a) Design distributed identifiers for the system using equations in Table 8.1 with a simple
decentralized control applied with linear negative feedback with gain 0.1.
(b) Design distributed control policies based on the hybrid learning algorithm using equations
in Table 8.2.
8.7.2 For the same system, consider the linearized dynamics as in (Guinaldo et al., 2011). The pa￾rameters in the system dynamics are m1= 2,m2= 2.5, J1= 5,J2= 6.25, k = 10,r = 0.5,l = 0.5,
and g = 9.8,b = 0.5. Pick the initial conditions of the system states in the interval [0,1] and
the initial weights of the NN from [-1,1], randomly. Treat xi2(t) as the measured variable,
N = 10, and let all the subsystems be connected such that they form a ring network.
(a) Design distributed control policies based on the hybrid learning algorithm using equations
in Table 8.2.
8.7.3 Consider the system of three interconnected systems modeling the robot walking gait. The
controlled equations of motion in units of (rad/s) are
x¨1(t) = 0.1[1−5.25x2
1(t)]x˙1(t)−x1(t) +u1(t)
x¨2(t) = 0.01
1− p2(x2(t)−x2eq)
2

x˙2(t)−4(x2(t)−x2eq)
+0.057x1(t)x˙1(t) +0.1(x˙2(t)−x˙3(t)) +u2(t)
x¨3(t) = 0.01[1− p3(x3(t)−x3eq)
2
]x˙3(t)−4(x3(t)−x3eq)
+0.057x1(t)x˙1(t) +0.1(x˙3(t)−x˙2(t)) +u3(t),
where ¨xi correspond to the dynamics of the i
th subsystem. The control objective is to bring the
robot to a stop in a stable manner (xi → xieq), where xieq are the equilibrium points with x1eq =
0, x2eq = −0.227, x3eq = 0.559. The parameters p2 and p3 are 6070, and 192, respectively.
Design distributed control policies based on the hybrid learning algorithm using equations in
Table 8.2.
8.7.4 For the system described in Problems 8.7.3., design NN identifiers using Table 8.1. Use
bounded control inputs, e.g., constant, sine, etc.
8.7.5 Repeat the Problems 8.7.3. and 8.7.4. using Table 8.2 and 8.1 with the parameters x1eq =
0, x2eq = 0.559,x3eq = 0.226, p2 = 226, and p3 = 5240.
8.7.6 Consider the system described in Example 3.1. Implement the enhanced hybrid learning al￾gorithm, equations of which are given in Table 8.2, to design feedback control policy to sta￾bilize the system. Compare the transient performance of the system with the hybrid learning
algorithm-based control scheme.274 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
8.7.7 Consider the system described in Example 4.3. Implement the enhanced hybrid learning algo￾rithm, equations of which are given in Table 8.2, to design feedback control policy to stabilize
the system around origin.
8.7.8 For the two-link robot manipulator system given in Example 4.3, implement an NN identifier
using the equations given in Table 8.1 to reconstruct the manipulator dynamics.
8.7.9 Use the exploration strategy described in the Section 8.4.4 and construct 4 independent NN
controllers using Table 8.2 for the two link robot manipulator given in Example 4.3. Use the
following exploration signals given by ξ (t) as sin(2πt),∑5
i=0 cos(iπt), (unit) pulse signal with
50% duty cycle, (unit) pulse signal with 30% duty cycle, and Gaussian white noise.
8.7.10 Consider 5 inverted pendulums connected by spring, each with the dynamics given as in
(8.21). Assume that they are interconnections, when represented as a graph, forms a ring
topology, i.e., subsystem i is connected with subsystems i + 1 and i − 1 for i = 2,3,4. The
subsystem i = 1 is connected with subsystems j = 2,5 while the subsystem 5 is connected
with subsystems j = 1,4. Compare the cost of the closed-loop system, whose controllers are
designed using the equations given in Table 8.2, for the cases when the system dynamics are
known and when the system dynamics are unknown, in which case, NN identifiers Table 8.1
are used to reconstruct the system.
8.7.11 Design a tracking controller using the enhanced hybrid learning scheme so that the two￾link robot manipulator system (given in Example 4.3) tracks the reference trajectory xd(t) =
sin(5πt)9 Event-Triggered Control
Applications
CONTENTS
9.1 Output Feedback Control of Robot Manipulator: Introduction............................................. 276
9.1.1 Background and Problem Formulation..................................................................... 277
9.1.2 NN Observer and controller design .......................................................................... 279
9.1.3 Example .................................................................................................................... 287
9.2 Unmanned Aerial Vehicle: Introduction................................................................................ 289
9.2.1 Background and Problem Statement ........................................................................ 291
9.2.2 Observer Design ....................................................................................................... 293
9.2.3 Control of Quadrotor UAV ....................................................................................... 298
9.2.4 Quadrotor UAV Stability .......................................................................................... 303
9.2.5 Numerical Example .................................................................................................. 304
9.2.6 Effects of Event-Sampling........................................................................................ 306
9.3 Distributed Scheduling Protocol and Controller Design for Cyber Physical Systems.......... 308
9.3.1 Background............................................................................................................... 308
9.3.2 The Scheduling and Control Problems in CPS......................................................... 309
9.3.3 Cross-Layer Design for CPS .................................................................................... 310
9.3.4 Example .................................................................................................................... 313
9.4 Summary and Concluding Remarks...................................................................................... 315
In this chapter, we shall first explore the utilization of adaptive NNs within the event-triggered feed￾back controller framework to achieve precise trajectory tracking in two robotic applications. In the
first application, we consider the robotic manipulators and detail the design procedure as presented
in (Narayanan et al., 2018a). Specifically, we shall explore the output feedback control scheme that
incorporates a nonlinear NN observer to reconstruct the joint velocities of the manipulator using
joint position measurements. In addition to the observer NN, a second NN is employed to com￾pensate for nonlinearities in the robot dynamics. We will consider two distinct configurations for
the control scheme, depending on whether the observer is co-located with the sensor or the con￾troller within the feedback loop. For both configurations, the controller computes the torque input
by leveraging the observer NN and the second NN. We shall derive the event-triggering condition
and weight update rules for the controller and observer using the Lyapunov stability method.
In the second application, we shall see the event-sampled output-feedback NN-based controller
for steering a quadrotor Unmanned Aerial Vehicle (UAV). The controller design encompasses mul￾tiple components to ensure precise flight control. Firstly, an observer is devised to estimate the
UAV’s state-vector from its outputs. Subsequently, a kinematic controller is developed to determine
the desired translational velocity, which is used in conjunction with a virtual controller to establish
the desired rotational velocity for the UAV’s orientation convergence. The signals generated by the
dynamic controller enable the tracking of the desired lift velocity and rotational velocities. Through￾out the designs, the impact of sampling errors is emphasized. This part of the chapter is based on
275276 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
the results presented in (Szanto et al., 2017a,b).
Finally, we shall explore the application of event-triggered ADP-based controllers in CPS
wherein multiple real-time dynamic systems are connected to their corresponding controllers
through a shared communication network. In particular, we shall look at a distributed scheduling
protocol design via cross-layer approach to optimize the performance of CPS by maximizing the
utility function that is generated by using the information collected from both the application and
network layers. We shall see that when compared with traditional scheduling algorithms, the ap￾plication of event-triggered control synthesis together with a distributed scheduling scheme via the
cross-layer approach presented here can not only allocate the network resources efficiently but also
improve the performance of the overall real-time dynamic system. This last application is based on
the work in Xu (2012); Xu and Jagannathan (2012).
9.1 OUTPUT FEEDBACK CONTROL OF ROBOT MANIPULATOR: INTRO￾DUCTION
The design of torque input for robot manipulators has been one of the widely studied problems
among the control researchers (Kim and Lewis, 1999; Lewis et al., 1998; Lee and Harris, 1998).
For example, in the areas ranging from nano/micro-scale (Cheah et al., 2014) to large-scale appli￾cations (Kermani et al., 2007), and for medical environments (Cheah et al., 2015; Sfakiotakis et al.,
2015), the robotic manipulators are extensively used. The computed torque (CT) controllers, one
of the basic nonlinear controllers for robotic manipulators, convert the nonlinear robot manipulator
dynamics into a linear system via the feedback-linearization technique. In the CT control scheme,
a filtered tracking error (Lewis et al., 1998) is defined and the controller design process is restated
using the filtered tracking error dynamics under the assumption that the dynamics are accurately
known. To obviate the requirement of the complete knowledge of the manipulator dynamics, adap￾tive controllers (Lewis et al., 1998) are introduced under the assumption that the effects of nonlinear
dynamics can be represented as a linear combination of known regression functions (linear in the
unknown parameters (Lewis et al., 1998)).
With the incorporation of Neural Networks (NNs) (Kim and Lewis, 1999; Lewis et al., 1998;
Ge et al., 2013) as function approximators, adaptive NN controllers have emerged to replace known
regression functions with NN activation functions. In these works, continuous or periodic measure￾ments of joint positions and velocities are required for the controllers, although the need for joint
velocity measurements is later relaxed (Kim and Lewis, 1999). Literature presents various control
schemes for robot manipulators, accounting for factors such as network delays (Liu and Chopra,
2012), finite-time control tasks (Galicki, 2015), learning and adaptation for complex tasks (Visioli
et al., 2010; Sun et al., 2006, 2018; Pan et al., 2016; de Jesus Rubio, 2016, 2018), input constraint ´
with saturation nonlinearity (He et al., 2016b), time-varying output constraints (He et al., 2017), and
fast adaptation with performance guarantee (Pekarovskiy et al., 2018).
In addition to the application specific requirements, robotic manipulator systems with remote
interoperability, enabling network integration is desired in many engineering applications wherein
continuous feedback information to compute appropriate torque input is not always desirable. In
this context, event-based sampling and control implementation has become relevant (Sahoo et al.,
2016; Tabuada, 2007; Tallapragada and Chopra, 2012; Zhong and He, 2016; Wang et al., 2017;
Yang et al., 2017). However, event-sampled implementation of observer together with the controller
for a robot manipulator with uncertain dynamics using NNs is nontrivial. Furthermore, with the
aperiodic estimation of states, we shall see that the location of observers plays an important role in
the tracking performance, and therefore, needs a careful consideration.
In the first part of this chapter, we shall study an adaptive NN-based control scheme for a robot
manipulator system utilizing event-sampled measurements of joint position vector. An NN observer
is utilized to reconstruct the joint velocity vector. The nonlinear uncertainties capturing the effectsEvent-Triggered Control Applications 277
of inertia, Coriolis/centripetal forces, frictional forces, and gravitational forces on the robotic ma￾nipulator are learned by using a second NN for generating the torque input. With these components,
we shall look at two controller configurations based on the location of the observer in the feedback
loop. The first configuration considers that the NN observer is co-located with the sensor whereas in
the second configuration, the NN observer is co-located with the controller. Due to the location of
the NN observer, we shall see that the convergence properties of the closed-loop system are not iden￾tical for the two configurations, and more, the ETM has to be redesigned for these configurations
based on the information available at the ETM.
9.1.1 BACKGROUND AND PROBLEM FORMULATION
9.1.1.1 Event-based sampling and control implementation
To denote the time instants when the controller has access to the sensor samples, a sequence, {tk}∞
k=0
with t0 = 0, is defined. These time instants are referred to as the event-sampling or event-triggering
instants. At the time instant tk, the sensor measurement denoted by γ(tk) is updated at the controller
and between any two sampling instants, a zero-order-hold (ZOH) is utilized at the controller to hold
the feedback information. This is represented as γ˘(t) = γ(tk), tk ≤ t < tk + 1, ∀k ∈ {0,N}. Lack
of current sensor measurement at the controller leads to the measurement error or event-sampling
error, ¯eET (t) = γ(t)−γ˘(t), tk ≤ t < tk+1, with ¯eET (tk) = 0, ∀k ∈ {0,N}.
Remark 9.1. Similar to the ZOH use at the controller, the actuator is also equipped with a ZOH so
that a piecewise constant control input is applied to the dynamic system.
As we have seen in this book, the design of controller in the event-triggered feedback frame￾work requires the design of an ETM that constructs the sequence {tk}, dynamically, along with the
controller. Requirement of an efficient ETM is further reinforced by the introduction of a learning
mechanism in the controller. In this case, in addition to considering the stability of the system, the
learning mechanism and its efficiency should be taken into account when the ETM is developed. In
the following Lemma, the event-sampled NN approximation property (Sahoo et al., 2016) is recalled
from Chapter 4.
Lemma 9.1. Given χ : A → R, a smooth, real-valued function with a compact set, A ⊂ Rn and a
positive constant εM, there exists W∗ ∈ Rp×1 so that
χ(x) = W∗T φ(x˘) +ε˘(x, x˘), (9.1)
holds, where ε˘ < εM, ∀x˘ ∈ A, x, x are the continuous and event-sampled variables, ˘ φ(x˘) is the
bounded event-sampled activation function, which forms a basis set of functions in A (Lewis et al.,
1998) and p is the number of hidden layer neurons, and ε˘ = W∗T (φ(x˘+ e¯ET ) − φ(x˘)) + ε is the
event-driven reconstruction error with ε function approximation error (Sahoo et al., 2016).
Remark 9.2. Note that the activation function φ(x) is used to denote the term φ(ωT x), where ω
is the input to hidden layer weights which are randomly set to form a stochastic basis, forming a
random vector functional link network (Lewis et al., 1998). Since the values of ω are fixed upon
initialization, they are not explicitly stated. The approximation of a nonlinear function using an NN
given in (9.1) reveals that when the input to the NN is corrupted by the measurement error, the
accuracy of the approximation is affected creating a trade-off between the estimation accuracy and
the frequency of events.
Next the robot manipulator dynamics are introduced followed by a brief background on event￾triggered tracking control problem.278 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
9.1.1.2 Robot manipulator system
The n−link robot manipulator dynamics with rigid links (Lewis et al., 1998) are expressed as
M(q)q¨(t) +Vm(q,q˙)q˙(t) +G(q) +F(q˙) +τd(t) = τ(t) (9.2)
where q(t) ∈ Rn is the joint variable vector, Vm(q,q˙) is the Coriolis/centripetal matrix, M(q) is the
inertia matrix, F(q˙) models the effects of friction, G(q) is the gravity vector, and τd(t) represents
disturbances. The control input vector τ(t) has components of torque for revolute joints and force
for prismatic joints. With the following facts, the tracking control problem for the robot dynamics
as in (9.2) is introduced.
Some useful facts regarding the dynamics of the robot manipulator is listed here (Lewis et al.,
1998): The matrix M(q) satisfies Bm1 I ≤ M(q) ≤ Bm2 I, for some known positive constants Bm1 ,Bm2 ,
where I is the identity matrix of appropriate dimension, and the Coriolis/centripetal matrix Vm(q,q˙)
is bounded such that Vm(q,q˙) ≤ Vb q˙ with Vb > 0. There exists constants gB,BF,Bf > 0 such
that G(q) ≤ gB and F(q˙) ≤ BF q˙+Bf . The matrix M˙ −2Vm is skew- symmetric, and finally,
there exists a positive constant, τdM > 0, such that τd(t) ≤ τdM, uniformly for all t ∈ R+.
9.1.1.3 Tracking control problem
By defining the joint positions and velocities as the state vector, the robot manipulator system dy￾namics can be represented in a compact form (Lewis et al., 1998) as
 q˙(t)
q¨(t)

=
 q˙(t)
−M−1(q)(Vm(q,q˙)q˙(t) +F(q˙) +G(q)) 
+
 0
M−1(q)

τ(t) + 0
−M−1(q)

τd(t). (9.3)
The objective in a tracking control problem is to design the torque input τ(t) such that the joint
positions track a desired trajectory qd(t). The desired trajectory is restricted to satisfy the following
standard assumption.
Assumption 9.1. (Kim and Lewis, 1999; Lewis et al., 1998) The desired trajectories qd(t) ∈ Rn
satisfy Qd ≤ qB, with Qd(t)=[qd(t) q˙d(t) q¨d(t)]T , and qB > 0.
In real-world applications, for example in manufacturing plants, the robotic manipulator is ex￾pected to perform tasks which are characterized by position, velocity and acceleration that are
bounded. Therefore, it is reasonable to make the assumption stated above in the design and analysis.
To develop a controller, which enables the robot manipulator to track a desired trajectory, we shall
begin by defining the tracking error as
e(t) = qd(t)−q(t), (9.4)
and the filtered tracking error (Lewis et al., 1998) as
r(t) = e˙(t) +λe(t), (9.5)
with a symmetric matrix λ > 0 of compatible dimension.
Observe from (9.5) that a positive choice of λ ensures the stability of ˙e(t) if the filtered tracking
error is finite and bounded. To design the torque input and to ensure boundedness of the filtered
tracking error, differentiate (9.5) with respect to time to reveal, ˙r(t) = e¨(t)+λe˙(t). By incorporating
the dynamics of the robotic manipulator (9.2) and utilizing (9.4) and (9.5), the dynamics of the
filtered tracking error is obtained (Lewis et al., 1998) as
Mr˙(t) = −Vmr(t)−τ(t) + f(z) +τd(t) (9.6)Event-Triggered Control Applications 279
with f(z) = M(q)(q¨d + λe˙) +Vm(q,q˙)(q˙d + λe) + F(q˙) + G(q) and z = [eT e˙
T qT
d q˙
T
d q¨
T
d ]
T . If the
function f(z) is known, it is straight-forward to generate a control input, τ(t) = f(z)+kvr(t), where
kv ∈ Rn×n is a positive definite design matrix, such that the filtered tracking error dynamics in (9.6)
is stable. This leads to a computed torque controller that assumes complete and accurate knowledge
of the robot manipulator dynamics. However, the nonlinear function f(z) is often unknown due to
the nonlinearities such as friction, inertia matrix, and so on.
Further, in the event-triggered feedback framework, the torque input using the event-sampled
state, is designed as
τ(t) = ˆf(

z) +kv

r(t), tk ≤ t < tk+1, (9.7)
where 
r(t) = r(t)−eET (t) is the event-sampled filtered tracking error, eET is the error due to event￾based sampling, and ˆf(

z) denotes an estimate of f(z) with 
z(t) = z(tk) in the interval [tk,tk+1). By
using (9.7) in (9.6), we get
Mr˙(t) = −Vmr(t)− ˆf(

z)−kv

r(t) + f(z) +τd(t). (9.8)
The filtered tracking error dynamics (9.8) reveals that the tracking performance of the robotic
manipulator is influenced by the measurement error due to event-based sampling, the mismatch
between f(z), and its estimate. Moreover, all the controller equations (9.4)-(9.8) assume the avail￾ability of both the joint position and velocities. In the next section, an observer is designed first to
generate the estimate of joint velocities, and then, a controller that utilizes the estimated state vector
is developed to design (9.7).
9.1.2 NN OBSERVER AND CONTROLLER DESIGN
Before developing the controller, two output feedback control schemes, which are represented using
block diagrams in Figure 9.1, are introduced. In the first architecture (Figure 9.1. (a)), the observer is
co-located with the sensor. This ensures that the observer has access to the sensor data continuously,
and hence, the convergence of the observer estimation error will be fast. When an event is triggered,
Robot 
NN weight update rule
Sensor
 >    @ Ö Ö N N TW TW W   NW ZOH
Sampling 
Mechanism
Neural 
Network 
 >    @ TW TW Ö Ö
ZOH Actuator Observer
Controller
NN weight update rule Neural Network Robot 
NN weight update rule
Sensor
>    @ Ö Ö N N TW TW
   NW
ZOH
Sampling 
Mechanism
Neural 
Network 
T W  ZOH Actuator
Controller
Neural 
Network 
NN weight 
update rule Observer
Figure 9.1 Controller block diagram - (a) Configuration 1. (b) Configuration 2.
the estimated joint velocities and the joint positions are fed back to the controller. The controller has
access to the event-sampled estimated states to generate the required torque. Moreover, the ETM
can be designed as a function of all the estimated states.
In the second architecture (Figure 9.1. (b)), the observer is co-located with the controller. In
this configuration, only the output information is transmitted from the sensor to the controller. The
observer NN and the controller are updated with current sensor readings only when there is an event.
This configuration introduces two issues. Firstly, the observer is injected with an additional error due
to event-triggered feedback. Secondly, the event-triggering mechanism now cannot be a function of
the estimated state vector as only the outputs are continuously available. Next, the analytical design
of the controller and the observer are presented for Configuration 1.280 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
9.1.2.1 Observer and controller dynamics (Configuration 1)
The robot manipulator dynamics (9.3) can be represented as
x˙1(t) = x2(t)
x˙2(t) = ¯f(x(t), τ(t)), (9.9)
where x1,x2 denote the joint variables and joint velocities, respectively, x = [xT
1 ,xT
2 ]
T , and ¯f(x, τ) =
−M−1(q)(Vm(q,q˙)q˙ + F(q˙) + G(q) + τd(t) − τ(t)). Using the function approximation property of
the NN (Lewis et al., 1998), the nonlinear map ¯f(.) can be represented as ¯f(x, τ) = WT
o σo(x, τ) +
εo(x), where Wo is the unknown, constant NN weights, σo(·) is the activation function, and εo(x) is
the reconstruction error.
Assumption 9.2. (Lewis et al., 1998) The NN approximation/reconstruction error (εo(.)) is
bounded such that εo ≤ εoM for εoM > 0. The nonlinear neural network activation function, σo(.),
is chosen such that σo(•) ≤ √No, where No denotes the number of neurons in the hidden layer.
Also, there exists WoM > 0 such that Wo ≤ WoM, holds. Finally, the nonlinear activation function
satisfies, σo(α)−σo(β) ≤ Lσ α −β, ∀α,β ∈ A, where A is a compact set.
In the analysis, standard assumptions (Lewis et al., 1998) are made as listed in Assumption 9.2.
It is proven in (Lewis et al., 1998) that as the number of hidden layer neurons are increased, i.e.,
as No → ∞, the reconstruction error, ε → 0. In addition, commonly used activation functions like
the sigmoidal function, radial basis functions, and so on, satisfy the bounds and the smoothness
properties stated above (Lewis et al., 1998).
Let Wˆ o represent the weight estimates of Wo. The approximated nonlinear function is given by ˆ¯f(x) = Wˆ T
o σo(xˆ, τ). Let the estimated joint variable be represented by ˆx1 and the estimated joint
velocity be represented using the vector ˆx2 and let ˆx = [xˆ
T
1 xˆ
T
2 ]
T . Using Wˆ o, define the NN observer
as
˙
xˆ1(t) = xˆ2(t) +Kdx˜1(t)
z˙2(t) = Wˆ oσo(xˆ, τ) +Kx˜1(t),
xˆ2(t) = z2(t) +Kpx˜1(t). (9.10)
Now, rewriting (9.10) using the estimated state variables ˆx1 and ˆx2, one has
˙
xˆ1(t) = xˆ2(t) +Kdx˜1(t)
˙
xˆ2(t) = Wˆ oσo(xˆ, τ) +Kx˜1(t) +Kp ˙
x˜1(t), (9.11)
where Kd,K, and Kp are observer gains, the output estimation error ˜x1(t) = x1(t)−xˆ1(t), and ˆx(0)
is the initial state estimate. Note that the dynamics in terms of a new variable ˙z2(t) is introduced in
(9.10) and is utilized to get the estimate of x1 and x2 as in (Kim and Lewis, 1999). This change of
variable enables both the implementation of the observer using (9.10) and facilitates the convergence
and stability analysis using (9.11) as was demonstrated in (Lewis et al., 1998).
To analyze the stability of the observer, first, define the state estimation error as ˜x = [x˜
T
1 x˜
T
2 ]
T ,
where ˜x2 = x2 −xˆ2. Since the observer is co-located with the sensor, the observer dynamics are not
injected with the measurement error and the resulting estimation error dynamics are
˙
x˜1 = x˜2 −Kdx˜1
˙
x˜2 = −Kpx˜2 −(K −KpKd)x˜1 +W˜ oσo(xˆ) +ε1(x˜), (9.12)
where ε1(x˜) = Wo(σo(x)−σo(xˆ)) +εo(x). The stability results of the observer are presented next.Event-Triggered Control Applications 281
Lemma 9.2. (Configuration 1): Consider the dynamics of the robotic manipulator (9.9) and the
observer dynamics (9.10). Let the weight tuning rule for the observer NN be defined as
˙
Wˆ o(t) = −ΓoKdσo(xˆ)x˜
T
1 (t)−ΓoκoWˆ o(t) (9.13)
where Γo,κo > 0 are design parameters. Then the error dynamics (9.12) and the weights of the NN
observer are locally UUB when KdK > 1
2No + 1
2K2
p, Kp > 1
2K2
d + 1
2No + 1
2 , κo > 1
2 + 1
2K2
d , and the
bounds are functions of the approximation error and target NN observer weights given by Bo = 1
2 ε12 +κ2
oW2
oM.
Sketch of Proof:
 Consider the Lyapunov candidate function as
Lo(x˜,W˜ o) = 1
2
x˜
T
1 (t)Kx˜1(t) + 1
2
x˜
T
2 (t)x˜2(t) + 1
2
tr(W˜ T
o (t)Γ−1
o W˜ o(t)).
 Taking the first derivative, we get L˙ o(x˜,W˜ o) = ˙
x˜
T
1 (t)Kx˜1(t)+x˜
T
2 (t)˙
x˜2(t)+tr(W˜ T
o (t)Γ−1
o ˙
W˜ o(t)).
Using the error dynamics (9.12) and the weight update rule (9.13) along with the fact that
˙
W˜ o(t) = − ˙
Wˆ o(t), we obtain the following inequality
L˙ o(t) ≤ −KdKx˜12 −Kpx˜22 −κo

W˜ o


2
+x˜2(KpKd)x˜1+x˜2

W˜ o

√No +x˜2 ε1
+
W˜ o

Kd
√No x˜1+
W˜ o

κoWoM, (9.14)
with No being the number of hidden layer neurons.
 Applying Young’s inequality, we get
L˙ o(t) ≤ −η1 x˜12 −η2x˜22 −η3W˜ o2 +Bo, (9.15)
where η1 = (KdK − 1
2No − 1
2K2
p), η2 = (Kp − 1
2K2
d − 1
2No − 1
2 ),η3 = (κo − 1
2 − 1
2K2
d ), and
Bo = 1
2 ε1(x˜)2 + κ2
oW2
oM. From (9.15), it can be observed that as long as η1,η2,η3 > 0
and x˜1 > Bo/η1 or x˜2 > Bo/η2 or W˜ o > Bo/η3, the Lyapunov function is
negative definite. As a consequence, it can be concluded that the observer estimation errors
and the observer NN weights are UUB.
To develop the control equations, begin by defining the tracking error using the estimated states
as ˆe(t) = xd(t) − xˆ1(t) and ˙
eˆ(t) = x˙d(t) − xˆ2(t). Now the estimated filtered tracking error can be
defined as ˆr(t) = ˙
eˆ(t) +λe(t). Note that the filtered tracking error is calculated using the estimated
joint velocity tracking error and the actual tracking error (9.4).Next, the nonlinear map in (9.6), is
parameterized using an NN as f(z) = WT φ(z) +ε(z), where the NN weights W denotes the target
parameter for the learning scheme, the activation function φ(.), and the reconstruction error ε(z)
with f(z) defined as in (9.6). Using the estimated states, the tracking errors are redefined and the
input to the NN is redefined as ˆz = [eˆ
T ˙
eˆ
T qT
d q˙
T
d q¨
T
d ]
T . The estimate of the nonlinear map in the
event-triggered feedback framework is given as
ˆf(

z) = Wˆ T φ(

z), (9.16)
with the estimated NN weights Wˆ and 
z = zˆ(tk). Let the weight estimation error be defined as
the difference between the target weights and the estimated weights, W˜ T = WT −Wˆ T . Using the
estimated states, the estimated filtered tracking error dynamics (Kim and Lewis, 1999) are revealed
as
M˙
rˆ(t) =−Vmrˆ(t)−τ(t) + f(z) +τd(t) (9.17)282 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
with f(z) = M(q)(q¨d + ˙
x˜2 +λe˙) +Vm(q,q˙)(q˙d +x˜2 +λe) +F(q˙) +G(q). Using the control torque
(9.7), we get
M˙
rˆ =(−Vm −kv)rˆ+τd +W˜ T φ(zˆe) +WT [φ(z)−φ(zˆe)] +ε +kveET . (9.18)
The subscript (.)e indicates that the variable is event-sampled. The slope of the filtered tracking
error (9.18) reveals that the measurement error introduced by event-based sampling and the error
due to the observer affects the tracking performance of the manipulator. The NN is designed by
taking into account the standard assumptions as stated in Assumption 9.2. Specifically, there exists
positive constants εM, WM > 0 so that ε ≤ εM, W ≤ WM holds. The activation function, φ(.),
satisfies φ(•) ≤ √N0, φ(α)−φ(β) ≤ Lφ α −β, ∀α,β ∈ A, where N0 denotes the number of
neurons in the hidden layer. The stability results of the closed loop system with continuous feedback
is briefly presented next.
Theorem 9.1. Let the estimated filtered tracking error dynamics for the robot manipulator, using
the approximation based torque input τ(t) = ˆf(z) + kvrˆ(t) and the NN approximation (9.16), be
defined as M˙
rˆ(t)=(−Vm − kv)rˆ(t) + τd(t) +W˜ T φ(zˆ) +WT [φ(z) − φ(zˆ)] + ε. Let the dynamics of
the observer be given by (9.11). Select the NN weight tuning rule as
˙
Wˆ (t) = Γφ(zˆ)rˆ
T (t)−κΓWˆ (t), (9.19)
with the design parameters Γ > 0,κ > 1
2 , and λmin(kv) > 1
2 . Let the NN observer weight update rule
be given by (9.13) with the observer design parameters satisfying the conditions given in Lemma
9.2. Under the Assumptions 9.1 and 9.2, the NN observer, tracking errors, and the error in the NN
weight estimates are locally uniformly ultimately bounded.
Sketch of Proof: Here we just analyze the continuous time implementation of the output feedback
controller using Lyapunov stability theory.
 The Lyapunov candidate function L = Lo + L with L(rˆ,W˜ ) = 1
2 rˆ
T Mrˆ + 1
2 tr(W˜ TΓ−1W˜ )
consists of two terms. The first term Lo is concernced with the observer variables as defined
in Lemma 9.2.
 Taking the first derivative of L and substituting the error dynamics and the weight estimation
error dynamics using (9.19), where ˙
W˜ (t) = − ˙
Wˆ (t) and φ˜ = φ(z)−φ(zˆ), reveals
L˙(r,W˜ ) =−rˆ
T kvrˆ+rˆ
T (ε +τd +WT (φ(z)−φ(zˆ)))
+tr(W˜ T κW −W˜ T κW˜ +
1
2
rˆ
T (M˙ −2Vm)rˆ
T . (9.20)
 Using the skew-symmetry property, ˆrT (M˙ −2Vm)rˆ
T = 0, applying Young’s inequality, and
using the results from Lemma 9.2,
L˙ ≤ −(λmin(kv)− 1
2
)rˆ2 −(κ − 1
2
)

W˜ T 

2
−η1 x˜12 −η2x˜22 −η3W˜ o2 +B1, (9.21)
where B1 = B1a + Bo, Bo is the bounds from Lemma 9.2, and B1a = 1
2κ2W2
M + 1
2 ε +
τd + WT (φ(z) − φ(zˆ))2. From (9.21), it can be observed that as long as η1,η2,η3 >
0,λmin(kv) > 1
2 ,κ > 1
2 , and x˜1 > B1/η1 or x˜2 > B1/η2 or W˜ o > B1/η3
or W˜  >


B1/(κ − 1
2 ) or rˆ >


B1/(λmin(kv)− 1
2 ), the Lyapunov function is nega￾tive definite. Hence, it can be concluded that the observer estimation errors, the NN weights,
and the estimated tracking error are UUB.Event-Triggered Control Applications 283
In the following theorem, the ETM is designed and the stability results for the event sampled
controller will be presented in two cases following an approach similar to (Sahoo et al., 2016).
Theorem 9.2. (Configuration 1): Consider the estimated filtered tracking error dynamics given by
(9.18) and the observer dynamics given by (9.11). Define the approximated torque input (9.7) and
the weight tuning law
Wˆ + = Wˆ (t) +Γˆ φ(zˆ)rˆ
T −κΓWˆ (t), t = tk, (9.22)
for k ∈ N. Let the Assumptions 9.1-9.2 hold. Let an event be triggered when the following condition
is not satisfied in the inter-sampling period
eET 2 ≤ σ μkrˆ2
. (9.23)
Then the observer estimation error, NN weight estimation error, and the tracking errors are locally
ultimately bounded provided the design parameters satisfy σ,μk ∈ (0,1), κ > 0,Γ > 0,λmin(kv) ≥
3
2 +σ, and Γˆ = Γ
1+rˆ2 .
Sketch of Proof:
 The proof can be considered in two cases. Case 1, which involves analysis in the inter￾sampling period, is considered first. Consider the Lyapunov function L = L(rˆ,W˜ ) + Lo,
where L(rˆ,W˜ ) = 1
2 rˆ
T Mrˆ+ 1
2 tr(W˜ TW˜ ).
 Since the NN weights in the controller are not updated during the inter-sampling period, we
have ˙
W˜ = 0 between event-triggering instants.
 Applying the norm operator, the facts stated in Section 9.1.1.2, and using Young’s inequal￾ity, we get
L˙(rˆ,W˜ ) ≤ −λmin(kv)rˆ2 +
rˆ
T 

2
+
1
2
kveET 2
+
1
2

(WT φ(z)−Wˆ T φ(zˆe) +ε +τd)


2
. (9.24)
Choose eET 2 ≤ μkrˆ2 with μk = 2σ/k2
v to get
L˙(rˆ,W˜ ) ≤ −(λmin(kv)−1−σ)rˆ2 +B2a, (9.25)
where B2a = 1
2

W˜ T φ(zˆe) +WT (φ(z)−φ(zˆe)) +ε +τd


2
.
 Alternatively, choosing eET 2 ≤ μkrˆ2 with μk = 2σ/[

Wˆ T 

2
L2
φ +k2
v ] yields
L˙ ≤ −(λmin(kv)− 3
2 −σ)rˆ2 +B2b, (9.26)
with B2b = 1
2 WT φ(z)−Wˆ T φ(zˆ) +τd +ε2.
 Finally, the observer Lyapunov function derivative can be combined from Lemma 9.2 to
reveal the overall bound.
 For Case 2, we consider the event triggering instants. Consider the Lyapunov function L .
The first difference ΔL = ΔL(rˆ,W˜ ) +ΔLo. Observe that at t = tk for all k ∈ {0,N}, ˆr+ = rˆ
and ˜x+ = x˜. Therefore, ΔL = 1
2 tr(W˜ +TW˜ +)− 1
2 tr(W˜ TW˜ ). From the definition W˜ + = W −
Wˆ +, we have W˜ + = W˜ (t)−Γˆ φ(z)rˆ
T +κΓWˆ (t). Substituting the error dynamics in the first
difference reveals
ΔL =tr(−W˜ Γˆ φ(z)rˆ
T +
1
2
rˆφ T (z)Γˆ TΓˆ φ(z)rˆ
T
+κW˜ ΓWˆ (t)−κrˆφ T (z)Γˆ TΓWˆ +
1
2
κ2
Wˆ TΓTΓWˆ ). (9.27)284 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Table 9.1
Output feedback controller design using Configuration 1
Observer dynamics ˙
xˆ1(t) = xˆ2(t) +Kdx˜1(t), ˙z2(t) = Wˆ oσo(xˆ, τ) +Kx˜1(t), and
xˆ2(t) = z2(t) +Kpx˜1(t).
Filtered tracking error dynamics Mr˙(t) = −Vmr(t)−τ(t) + f(z) +τd(t)
Control torque τ(t) = ˆf(

z) +kv

r(t)
Observer NN weight update rule ˙
Wˆ o(t) = −ΓoKdσo(xˆ)x˜
T
1 (t)−ΓoκoWˆ o(t)
Controller NN weight update rule Wˆ + = Wˆ (t) +Γˆ φ(zˆ)rˆ
T −κΓWˆ (t), t = tk
Event-triggering condition - 1 eET 2 ≤ μkrˆ2 with μk = 2σ/k2
v
Event-triggering condition - 2 eET 2 ≤ μkrˆ2 with μk = 2σ/[

Wˆ T 

2
L2
φ +k2
v ]
Using the fact that 0 ≤ rˆ
1+rˆ2 ≤ 1 and on simplification (Sahoo et al., 2016), we get
ΔL ≤ −
1
8
(κλmin(Γ)−8κ2Γ2)W˜ 2 +B2, (9.28)
where B2 = 1
2κ ΓN0+ 1
2N0ΓTΓ+κΓW2
M +κ
√N0ΓTΓWM +2κN0Γ3+ 1
2κ2WM
2ΓTΓ+
4
κ Γ3W2
M. It can be observed from Case 1 and Case 2 that all the errors are UB. Further￾more, as the events increase, the weight estimation errors decrease and converge to their
bounds. As a consequence, the bounds calculated for Case 1 decreases.
Remark 9.3. In Theorem 9.2, two different values for μk in (9.23) are derived. In the first condition,
μk = 2σ/kv2 is a constant and it does not require the information regarding the NN weights to
determine the event-based sampling instants. In contrast, the second sampling condition uses μk =
2σ/[

Wˆ T 

2
L2
φ +kv2]. This condition, similar to (Sahoo et al., 2016), utilizes the NN weights.
9.1.2.2 Observer and controller dynamics (Configuration 2)
Consider the second control configuration as represented in Figure 9.1 (b). To reconstruct the joint
velocities when the observer is co-located with the controller, define event-driven observer dynamics
given by
˙
xˆ1(t) = xˆ2(t) +Kdx˜1,e(t) (9.29)
z˙2(t) = Wˆ oσo(xˆe, τ) +Kx˜1,e(t), with ˆx2(t) = z2(t) +Kpx˜1,e(t).
Rewriting the observer dynamics (9.29) using the co-ordinate variables ˆx1(t),xˆ2(t), we get
˙
xˆ1(t) = xˆ2(t) +Kdx˜1,e(t),
˙
xˆ2(t) = Wˆ oσo(xˆe) +Kx˜1,e(t) +Kp ˙
x˜1,e(t). (9.30)Event-Triggered Control Applications 285
Note that the observer given by (9.29) has access to the outputs only at the event-triggering instants,
introducing an error in (9.30). From (9.30), the error dynamics are obtained as
˙
x˜1(t) = x˜2(t)−Kdx˜1,e(t)
˙
x˜2(t) = W˜ oσo(xˆ)−Kx˜1,e(t)−Kp ˙
x˜1,e(t) +ε1(x˜), (9.31)
where ε1(x˜) = Wo(σo(x)−σo(xˆ)) +εo(x). The stability results of the observer (9.29) are presented
next.
Lemma 9.3. (Configuration 2): Consider the dynamics of the robot manipulator defined by (9.3)
and the observer dynamics given by (9.29). Let the weight tuning rule for the NN observer be defined
as
Wˆ +
o = Wˆ o(t)−KdΓˆ oσo(xˆ)x˜
T
1 −κoΓoWˆ o(t), t = tk, (9.32)
for k ∈ N. Then the observer estimation error and the NN observer weights are locally UB when
KdK > K2
p,Kp > K2
d +1, Γˆ o = Γo
1+x˜12 , κo > 0 with the adjustable bounds defined by B3oa+ψ(eET ),
and B3ob is a function of the ε, the measurement errors, the WM, and the design constants.
Sketch of Proof: The structure of the proof consists of two parts as in the case of Theorem 9.2. This
is because the observer weights are updated only at the event-triggering instants using (9.32).
 The Lyapunov candidate function is chosen as Lo(x˜,W˜ o) = 1
2 x˜
T
1 Kx˜1 + 1
2 x˜
T
2 x˜2 + 1
2 tr(W˜ T
o W˜ o).
 First, the inter-sampling period is considered. Since the NN weights are not updated in the
inter-sampling period, we have ˙
W˜ o = 0. Using this, the Lyapunov time-derivative is given
by L˙ o(x˜,W˜ o) = ˙
x˜
T
1 Kx˜1 +x˜
T
2 ˙
x˜2. Using the definition of the event-triggering error and ˙
x˜1(t) in
(9.31) reveals
L˙ o(x˜,W˜ o) ≤ −η1x˜12 −η2x˜22 +B3oa +ψ(eET ), (9.33)
where η1 = 1
2 (KdK −K2
p),η2 = 1
2 (Kp −K2
d −1),B3oa = 1
2 W˜ oσo(xˆ)+ε12, and ψ(eET ) =
1
2KTKde1,ET 2 + 1
2 (K − KpKd)2e1,ET 2 + 1
2Kpe2,ET 2. Note that ˜x1,e = x1,e − xˆ1 =
x˜1 −e1,ET . Similarly, e2,ET (t) = e˙1,ET (t).
 Now, for the second case, consider the event-triggering instants. The function Lo(x˜,W˜ o) as
in the previous case is picked as the Lyapunov candidate function. Taking its first-difference,
we get ΔLo = 1
2 x˜
+T
1 Kx˜
+
1 + 1
2 x˜
+T
2 x˜
+
2 + 1
2 tr(W˜ +T o W˜ +
o )− 1
2 x˜
T
1 Kx˜1 − 1
2 x˜
T
2 x˜2 − 1
2 tr(W˜ T
o W˜ o). Sub￾stituting the jump dynamics and using the fact that 0 ≤ x˜1
1+x˜12 ≤ 1, and on simplification
(Sahoo et al., 2016), we get
ΔLo ≤ −
1
2
κoη3W˜ o2 +B3ob, (9.34)
where B3ob = 1
2 KdΓo2No+ 1
2κ2
oW2
oMΓT
o Γo+κo
√NoΓo2KdWoM + 1
2NoΓT
o 2KT
d 2+
1
2κoΓoW2
oM +κ2
o Γo2W2
oM + 1
2κ2
o
NoK2
d and η3 = λmin(Γo)−4κoΓT
o Γo. It can be observed
from Cases 1 and 2 that the observer estimation errors and the NN weight estimation errors
are UB provided the event-triggering mechanism ensures the boundedness of the measure￾ment errors.
The closed loop results of Configuration 2 are presented next wherein the event-triggering con￾dition is developed with only the joint position vector and event-triggering errors as opposed to the
estimated joint positions and velocities in (9.23).286 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Theorem 9.3. (Configuration 2): Consider the estimated filtered tracking error for the robot ma￾nipulator system defined as (9.18) and the observer dynamics from (9.29). Define the torque input
(9.7) by utilizing the NN approximation (9.16). Consider the weight tuning laws (9.22) and (9.32),
where Γ > 0,κ > 0, kv ≥ 3. If the Assumptions 1-2 are satisfied and when the measurement error
satisfies, in the inter-event period,
eET 2 ≤ σ μke2
. (9.35)
where μk,σ ∈ (0,1), eET = e(t)−e(tk). Then, the tracking error, the observer estimation error, and
the NN weight estimation errors are locally UB.
Sketch of Proof: The proof consists of two parts and the arguments are similar to that used in Theo￾rem 9.2.
 Consider the Lyapunov function L = L(rˆ,W˜ )+Lo, where L(rˆ,W˜ ) = 1
2 rˆ
T Mrˆ+ 1
2 tr(W˜ TW˜ ).
Since the NN weights are not updated during the inter-sampling period, the derivative
˙
W˜ (t) = 0. Therefore, the time-derivative of the Lyapunov function along the estimated fil￾tered tracking error dynamics (9.18) and using the results from (9.33) is given by
L˙ ≤ −(λmin(kv)−1)rˆ2 +
1
2
kv2eET 2
+B3a −η1x˜12 −η2x˜22 +LψeET 2, (9.36)
where B3a = 1
2

(WT φ(z)−Wˆ T φ(zˆe) +ε +τd)


2
+B3oa and Lψ > 0 such that ψ(eET ) ≤
LψeET 2. Using the definition ˆr(t) = ˙
eˆ(t)+λe(t) and applying triangle inequality, we have
L˙ ≤ −2(λmin(kv)−1)

˙
eˆ(t)


2
−2(λmin(kv)−1)λe(t)2
−η1x˜12 −η2x˜22 +
1
2
kv2eET 2 +LψeET 2 +B3a. (9.37)
 Choose eET 2 ≤ μkλe2 with μk = σ/( 1
2 kv2 +Lψ), to get
L˙ ≤ −2(λmin(kv)−1)

˙
eˆ(t)


2
−2(λmin(kv)−1−σ)λe(t)2
−η1x˜12 −η2x˜22 +B3a. (9.38)
 Alternatively, another event-sampling condition can be computed by modifying the proof to
allow a larger bound. From (9.33), the observer Lyapunov function derivative can be used
to obtain
L˙ ≤ −(λmin(kv)− 3
2
)rˆ2 −η1x˜12 −η2x˜22
+
1
2
[

Wˆ T 

2
L2
φ +kv2 +Lψ]eET 2 +B3b, (9.39)
where B3b = 1
2 WT φ(z)−Wˆ T φ(zˆ)+τd +ε2 +B3oa. Similar to the simplification process
preceding (9.38), we get
L˙ ≤ −(2λmin(kv)−3)

˙
eˆ(t)


2
−(2λmin(kv)−3−σ)λe(t)2
−η1x˜12 −η2x˜22 +B3b. (9.40)
To obtain (9.40), choose eET 2 ≤ μkλe2 with μk = 2σ/[

Wˆ T 

2
L2
φ +kv2 +Lψ].Event-Triggered Control Applications 287
 At the event-triggering instants, consider the Lyapunov function L . The first difference
ΔL = ΔL(rˆ,W˜ ) +ΔLo. Observe that at t = tk for all k ∈ {0,N}, ˆr+ = rˆ and ˜x+ = x˜. There￾fore, ΔL = 1
2 tr(W˜ +TW˜ +) − 1
2 tr(W˜ TW˜ ) + ΔLo. From the definition W˜ + = W −Wˆ +, we
have W˜ + = W˜ (t)−Γˆ φ(z)rˆ
T +κΓWˆ (t).
 Substituting the first difference of the controller NN weight estimation error from (9.28) and
the first difference ΔLo from Lemma 9.3, reveals ΔL ≤ −κ
8 (λmin(Γ) − 8κΓ2)W˜ 2 − 1
2κoη3W˜ o2 +B3, where B3 = B2 +B3ob. It can be observed during the event-triggering
instants and between events that the estimated tracking errors, the observer estimation er￾rors, and the NN weight estimation errors are UB.
Remark 9.4. As r and ˆ x converges to their bounds, the actual filtered tracking error converges to its ˜
bound. The proofs of Theorems 9.2-9.3, and Lemma 9.3 consider two cases which correspond to the
event-triggering instants and inter-event period to ensure boundedness of the closed loop signals.
The event-triggering mechanism presented in this section does not exhibit zeno behavior. This is a
consequence of the stability results presented in this section and its proof follows arguments similar
to that presented in (Sahoo et al., 2016).
A brief summary of results for the state feedback case is presented next. As a matter of fact, the
derivations for the state-feedback case become simpler since an observer becomes unnecessary and
the dynamics that result from one’s incorporation vanish from the analysis.
Corollary 9.1. (State-feedback controller) Consider the filtered tracking error dynamics (9.8). In
addition to the Assumptions 9.1 and 9.2, let the joint velocities be measured. Generate the torque
input (9.7) by utilizing the approximated dynamics of the robotic manipulator (9.6), where the es￾timated joint velocities are replaced by the actual joint velocities. Consider the weight tuning law
(9.19), where Γ > 0,κ > 0, and kv ≥ 3. Let the trigger condition (9.23) be satisfied during the inter￾event period with μk,σ ∈ (0,1). The tracking error and the NN weight estimation error are locally
UB (Narayanan and Jagannathan, 2016c).
A detailed presentation for the derivation of the state-feedback controller would be highly redun￾dant. To construct the proof for the Corollary 9.1, the estimated values used in the arguments of the
proof to Theorem 9.3 should be replaced with the measured values.
9.1.3 EXAMPLE
In this section, an example involving the event-sampled control of an n-link robotic manipulator
(Lewis et al., 1998) are presented. The parameters of the robot manipulator dynamics used in the
simulation include (Kim and Lewis, 1999) : m1 = 1, m2 = 2.3, g = 9.8, Li = 1 for i = {1,2}. The
controller design variables are selected as kv = 30I, λ = 5, κ = 0.0001,Γ = 500,Lφ = 1, and σ = 0.1.
The observer design parameters are selected as K = 500,Kp = 2,Kd = 0.2, and Γo = 6,κo = 0.1.
The initial values of the manipulator system states and the NN weights are chosen at random from
[0,1]. The trajectory to be tracked by the manipulator system is given by sin(ωt),cos(ωt) with
ω = 1. The NN observer is designed with 26 hidden layer neurons and sigmoid activation and the
controller NN is designed with 30 hidden layer neurons with sigmoid activation function. The input
to hidden layer weights are randomly selected in the interval [0,0.001] and held to form an RVFL
network. The design parameters for the controller and the observer are similar to those reported in
(Kim and Lewis, 1999) and τd(t)=[8 sin(2t) 5 sin(2t)].
First, a comparison between the standard event-driven PD control (without the NN compensation
in the torque) and the event-driven NN control are presented in Figure 9.2. These can be used to an￾alyze the advantages of the event-sampled NN controller. The Figure 9.2, as expected, demonstrates
that the event-driven NN controller improves the performance of the controlled robotic system over288 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Table 9.2
Output feedback controller design using Configuration 2
Observer dynamics ˙
xˆ1(t) = xˆ2(t) +Kdx˜1,e(t), ˙z2(t) = Wˆ oσo(xˆe, τ) +Kx˜1,e(t),
and ˆx2(t) = z2(t) +Kpx˜1,e(t).
Filtered tracking error dynamics Mr˙(t) = −Vmr(t)−τ(t) + f(z) +τd(t)
Control torque τ(t) = ˆf(

z) +kv

r(t)
Observer NN weight update rule Wˆ +
o = Wˆ o(t)−KdΓˆ oσo(xˆ)x˜
T
1 −κoΓoWˆ o(t), t = tk
Controller NN weight update rule Wˆ + = Wˆ (t) +Γˆ φ(zˆ)rˆ
T −κΓWˆ (t), t = tk
Event-triggering condition - 1 eET 2 ≤ μkλe2 with μk = σ/( 1
2 kv2 +Lψ)
Event-triggering condition - 2 eET 2 ≤ μkλe2 with μk = 2σ/[

Wˆ T 

2
L2
φ +kv2 +Lψ]
the event driven PD control similar to the continuous-time NN control as reported in (Lewis et al.,
1998).
Case 1: First, the event-triggering condition, which is a function of NN weights is chosen with
μk = 2σ/[

Wˆ T 

2
L2
φ + kv2], σ = 0.45,Lφ = 1, kv = 30I, and the figures correspond to the NN
tuning parameters Γ = 0.5 and Γo = 0.6 with κ,κo as chosen above. The initial conditions are
same as those stated before. The actual and desired joint variables, and the event-based observer
estimation error are plotted in Figure 9.3. Due to the location of the observer, the observer estimation
error with event-based feedback remains unaffected. The tracking performance on the other hand
converges after the observer error reaches its bound.
Figures 9.3(c) and (d) presents the performance of the event-sampling mechanism. The thresh￾old function, which is defined using the Lyapunov analysis, ensures that the event-triggering error
remains bounded. It can also be observed that a total number of events (NE) is 540 demonstrating
the efficacy of the event-sampling and control. The important difference between this ETM with the
ETM in Case 2 is that the number of events initially is more in Case 1 and becomes sporadic later,
facilitating the NN learning.
Case 2: In the second case, the event-triggering condition, which is not a function of NN weights
is evaluated with μk = 2σ/kv2, σ = 0.0006, and kv = 30I. In contrast to Case 1, the event￾triggering condition does not require an NN at the trigger mechanism, and hence, reduces the com￾putations. The initial conditions were kept the same. The tracking performance and the observer
estimation error trajectories are recorded in Figure 9.4. Since the design parameter σ, which affects
the event-triggering threshold is adjusted to get a similar cumulative event as in Case 1, similar
number of events are recorded. However, it is observed that, in contrast to the Case 1, more events
are not triggered in the transient NN learning period. The variation in the number of events with
the design parameter σ is summarized in Table 9.3 (Configuration 1). It can be observed that for
both the cases, similar cumulative events can result from appropriate choice of σ. The observer
convergence time to is not affected much, as it has continuous access to the sensor measurements.
To avoid redundancy, the results for Configuration 2 are summarized in Table 9.4. In this example,
the parameter Lψ = 1 is chosen.Event-Triggered Control Applications 289
    






    





    







    






Figure 9.2 Tracking error (a) PD control. (b) NN control. Observer estimation error (c) PD control. (d) NN
control.
Table 9.3
Configuration 1: Analysis with σ
μk = 2σ
[Wˆ T 
2
L2
φ +kv2] μk = 2σ
kv2
σ NE to (s) σ NE to (s)
0.05 865 4.257 0.0002 790 4.27
0.1 762 4.244 0.0006 672 4.25
0.3 743 4.466 0.001 636 4.47
0.4 620 4.525 0.003 583 4.54
0.5 513 4.538 0.0045 572 4.321
Thus far we have seen the application of event-triggered control design techniques for robot
manipulator. The manipulator dynamics are nonlinear but conform to input-affine structure, which
together with the properties of the dyanmics, allowed for designing controllers using a feedback￾linearlization using NNs with aperiodic feedback. In the next section, we shall explore the applica￾tion of event-based NN controller and observer synthesis for an unmanned aerial vehicle. We shall
see that this system is governed by nonlinear dynamics and has a strict-feedback form, which re￾quires utilization of backstepping control technique together with NNs and aperiodic feedback data.
9.2 UNMANNED AERIAL VEHICLE: INTRODUCTION
The emergence of quadrotors as unmanned aerial vehicles (UAVs) has resulted in a significant
amount of research in efforts to develop effective means by which they can be controlled (Dierks
and Jagannathan, 2009c; Voos, 2006; Madani and Benallegue, 2007; Issam and Qingbo, 2014; Lee
et al., 2007). In particular, the work in (Dierks and Jagannathan, 2009c) presented a novel output￾feedback controller with the use of NNs. The objectives of the proposed controller in (Dierks and290 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
    





    





     











 
         
Figure 9.3 Case 1: (a) Tracking error. (b) Observer estimation error. (c) Cumulative number of events. (d)
Event-triggering error and threshold.
Table 9.4
Configuration 2: Analysis with σ
μk = 2σ
[Wˆ T 
2
L2
φ +kv2+Lψ] μk = σ
( 1
2 kv2+Lψ)
σ NE to (s) σ NE to (s)
0.05 850 4.240 0.0002 801 4.283
0.1 772 4.242 0.0006 720 4.522
0.3 750 4.457 0.001 663 6.367
0.4 564 6.257 0.003 591 6.518
0.5 525 6.268 0.0045 561 8.576
Jagannathan, 2009c) were to alleviate the need for unnecessary sensors by introducing an observer
and to compensate for unknown nonlinear dynamics by making use of the universal approximation
property of the NNs. The use of NNs has proven to be a very powerful in the control for a quadrotor
UAV. By implementing an NN observer, thereby relaxing the need for full knowledge of the state
vector, and by using NNs to compensate for uncertainties, a greater degree of flexibility is made
available for engineers. In this section, we shall consider designing such a controller in an event￾triggered feedback framework.
In order to do this, we shall revisit the output feedback control design process as presented by
Dierks and Jagannathan (2009c). First an observer design is briefly presented, allowing the need
for a full knowledge of the state-vector to be avoided. Next, a kinematic controller is designed
in order to find a desired translational velocity such that the UAV’s position converges to a desired
trajectory, which is selected as an external input; additionally, it is in the kinematic controller that the
quadrotor’s desired orientation is found. Then, the information provided by the kinematic controller
is used in the design of a virtual controller wherein a desired rotational velocity is determined such
that the UAV’s orientation converges to its desired value. In these developments, we shall see that theEvent-Triggered Control Applications 291
    







    





     











 
     
Figure 9.4 Case 2: (a) Tracking error. (b) Observer estimation error. (c) Cumulative number of events. (d)
Event-triggering error and threshold.
effects of event-based sampling is injected either implicitly through the NN approximation errors or
explicitly through an intermittently updated state-vector.
Finally, we shall see that for a complex quadrotor UAV system, the benefits of event-sampling are
two-fold: first, computational costs would be reduced due to aperiodic tuning of NN weights. With
fewer computations being performed, battery life and, subsequently, flight time, could be extended.
Secondly, event-sampling may also save in communications costs. In a quadrotor UAV system, the
regular transmission of data from external sensors, such as GPS and gyro readings, is essential for
stable flight and, in these transmissions, packet-losses are inevitable. A reduction in the number of
samples being used would minimize the effects of these losses and save in communication costs.
In order to accomplish the incorporation of event-sampling in the control of a quadrotor UAV,
first, it is shown how the system exhibits ISS-like behavior with respect to bounded measurement
errors; this result is a necessary requirement in order to implement the event-sampled controller
because it ensures the existence of nonzero inter-event times. Next, it is necessary to demonstrate
that the measurement errors remain bounded for all time. This is demonstrated by considering the
dynamics of the system at event-sampling instants as well as during inter-event periods. The bound￾edness of the measurement errors during inter-event periods is guaranteed by the implementation of
an event-execution law, which we shall derive as well.
9.2.1 BACKGROUND AND PROBLEM STATEMENT
In this section, we begin with an introduction on the notations used in the rest of this chapter.
9.2.1.1 Notations and Background
The measurement errors that result from event-sampling is denoted by Ξ. In order to distinguish
event-sampled variables with their time-sampled counterparts, this symbol will appear as a sub￾script; for example, the event-sampled position of the quadrotor is denoted by ρΞ. A formal defini￾tion for Ξ is presented in the background section.
The states of the quadrotor UAV are given by its measured coordinate position, ρ = [x,y,z]
T ; its292 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
orientation Θ = [φ,θ,ψ]
T (roll, pitch, yaw), which are measured with respect to the inertial fixed
frame; its translational velocity in the body fixed frame, v = 
vxb,vyb,vzbT
; and its rotational velocity
in the body fixed frame, ω = 
ωxb,ωyb,ωzbT
. With these, the kinematics of the quadrotor can be
written as (Dierks and Jagannathan, 2009c)
ρ˙ = Rv (9.41)
and
Θ˙ = Tω. (9.42)
The translational rotation matrix relating a vector in the body fixed frame to the inertial coordi￾nate frame is defined by
R(Θ) = R =
⎡
⎣ cθ cψ sφ sθ cψ −cφ sψ cφ sθ cψ +sφ sψ
cθ sψ sφ sθ sψ −cφ cψ cφ sθ sψ −sφ cψ
−sθ sφ cθ cφ cθ
⎤
⎦,
where s(•) and c(•) are used as abbreviations for sin(•) and cos(•), respectively. Moreover, the
rotational transformation matrix from the fixed body to the inertial coordinate frame is defined with
its inverse as
T (Θ) = T =
⎡
⎣
1 sφ tθ cφ tθ
0 cφ −sφ
0 sφ
cθ
cφ
cθ
⎤
⎦, T −1 =
⎡
⎣
1 0 −sθ
0 cφ sφ cθ
0 −sφ cφ cθ
⎤
⎦,
where t(•) is used as an abbreviation for tan(•). Lastly, define the augmented transformation matrix
A = diag{R, T}.
9.2.1.2 Problem Statement
The dynamics of the quadrotor UAV in the body fixed frame are given by (Dierks and Jagannathan,
2009c)
M
 v˙(t)
ω˙(t)

= S¯(ω)
 v(t)
ω(t)

+
 N1 (v)
N2 (ω)

+
 G(R)
03×1

+U(t) +τd(t), (9.43)
where M = diag{mI3×3, J}, with m being a positive scalar representing the total mass of the UAV
and J being a positive definite inertia matrix; S¯(ω) = diag{−mS(ω), S(Jω)}, where S(•) is a skew
symmetric matrix satisfying hT Sh = 0 for any appropriately dimensioned vector h; N1 (v) and N2 (ω)
are nonlinear aerodynamic effects; G(R) = mgRT (Θ)Ez is the gravity vector, with g = 9.8 m/s2 and
Ez = [0, 0, 1]
T
; U = [0, 0, u1, u2]
T is an augmented vector containing the control inputs correspond￾ing to the total thrust, u1, and to the rotational torques, u2 = [u21, u22, u23]
T , corresponding to roll,
pitch, and yaw, respectively; and τd = 
τT
d1, τT
d2
T represents unknown, bounded disturbances such
that τd ≤ τM for a known positive constant, τM.
Before giving the control objective, a definition of the measurement errors that result from event￾sampling is introduced. Define the measurement errors to be
Ξχ (t) = χΞ (tl)− χ (t), ∀t ∈ [tl,tl+1), (9.44)
where, in general, χ (t), represents a time-sampled state variable. Moreover, χΞ (tl) denotes the
event-sampled state variable that was measured at the event-sampling instant tl; it is this event￾sampled variable that is stored in the controller during the inter-event period [tl,tl+1). Finally, Ξχ (t)
is the measurement error that results from intermittent sampling. The introduction of this error
presents an additional challenge in the control objective in that it necessitates additional consid￾erations that warrant the controller’s stable performance. In particular, it is necessary to design anEvent-Triggered Control Applications 293
event-triggering mechanism that ensures that the values that are stored are being updated frequently
enough for the controller to be able to achieve acceptable performance while reducing the number
of computations.
The control objective is to design an event-sampled output-feedback controller for (9.43) such
that the UAV follows a desired trajectory given by ρd = [xd,yd,zd]
T and a desired yaw, ψd, while
maintaining stable flight. This requires knowledge of the quadrotor’s dynamics as well as knowledge
of the UAV’s translational and rotational velocities. However, these requirements are relaxed by
utilizing the universal approximation property of NNs (Lewis et al., 1998) in order to estimate the
uncertainties and unknown quantities. In particular, in order to avoid the measurement of the UAV’s
state-vector, an NN observer is implemented to estimate the translational and rotational velocity
vector, which are not measured. The estimated values are used in the kinematic controller, the NN
virtual controller, and the NN dynamic controller. With these, we shall see that it is possible to
design an event-sampled control law that achieves the desired control objective. In order to ensure
that the control law is being updated frequently enough for the UAV to achieve its tracking objective,
an event-execution law is derived such that the measurement errors remain bounded for all time.
In the analyses, the following assumptions are made.
Assumption 9.3. The states of the reference trajectory, ρd and ψd, remain bounded (Dierks and
Jagannathan, 2009c). The state-vector corresponding to the UAV’s velocity is not available whereas
the system (9.43) is observable (Dierks and Jagannathan, 2009c). There are no transmission or
computation time delays (Tabuada, 2007). The dynamics of the system (9.43) are locally Lipschitz.
With these considerations in mind, the derivation of the event-sampled quadrotor UAV controller
is presented next. The derivation of the event-sampled controller is presented as two sections: In the
first section, the observer design is considered and, in the following section, the controller design is
presented.
9.2.2 OBSERVER DESIGN
In order to relax the need for state-vector measurability, an observer is designed to estimate un￾known values. Subsequently, the estimated values are used in the controller. Since the stability of
the controller relies on accurate sensor readings, the observer’s quick convergence is imperative. The
introduction of event-sampling only adds to the challenge of designing an observer that performs
well enough for the control objectives to be accomplished. Additionally, the implementation of the
event-triggering mechanism must also be taken into consideration. For these reasons, the placement
of the observer is taken to be at the sensor (similar to the Configuration 1 presented for the case
of the robot manuipulator application); practically, this means that, even with event-sampling, the
observer can estimate the state-vector continuously, allowing for quicker convergence as well as for
sufficient information for the implementation of the event-execution law.
Although the NN observer estimates the state-vector continuously, the NN itself is only up￾dated at event-sampled instants. With a continuously updated state-vector, it would not be correct
to directly assign measurement errors to the estimated states. However, since the NN is only being
updated intermittently, there is an approximation error due to the event-sampling. Using the fact that
the inputs of the NN are only be updated at events, the ideal NN approximation can be manipulated
in a manner that allows event-sampling measurement errors to be extracted from the approxima￾tion error, resulting from intermittent updates. The measurement errors that are introduced in this
section include ΞSX , Ξˆ SX , and Ξˇ SV , correpsonding to the quadrotor’s measured position and orien￾tation, estimated position and orientation, and measured and estimated velocity, respectively. Once
the measurement errors are extracted, it is not difficult to account for their effects when the event￾execution law is designed.294 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Begin by defining the augmented vectors X = 
ρT , ΘT T and V = 
vT , ωT T
. Using these aug￾mented vectors, the dynamics (9.41), (9.42), and (9.43) can be rewritten with
X˙(t) = AV(t) +ξ1(t)
V˙(t) = fo (xo) +G¯ + M−1
U(t) +τ¯d(t), (9.45)
where ξ1 represents bounded sensor measurement noise such that ξ1 ≤ ξ1M; τ¯ 
d =
τd1/m,

J−1τd1
T 
satisfies τ¯d ≤ MMτM with MM = 
M−1

; G¯ = M−1G(R); and UΞ =
[0, 0, u1Ξ, u2Ξ]
T is the event-sampled control input which is addressed in later derivations. Note
that there is an explicit measurement error present in UΞ, however, since this term is canceled
out in the observer analysis, it is not considered. Furthermore, by observation of (9.43), fo (xo) =
M−1

S¯(ω)V + [N1 (v), N2 (ω)]T

are unknown dynamics.
Next, introduce the change of variables, Z = V, and denote the observer estimates for X and V
with hats, specifically Xˆ and Vˆ , respectively. Finally, the observer estimation error is denoted with
a tilde, X˜ = X −Xˆ. With these, the observer takes the form (Dierks and Jagannathan, 2009c)
˙
Xˆ(t) = AZˆ +Ko1X˜
˙
Zˆ(t) = ˆfo1Ξ +G¯ +Ko2A−1X˜ + M−1
UΞ
Vˆ(t) = Zˆ +Ko3A−1X˜, (9.46)
where Ko1, Ko2, and Ko3 are positive design constants. Here A−1 is bounded by 
A−1

 ≤ AI
M,
where AI
M is a positive constant. Additionally, ˆfo1Ξ is the event-sampled NN estimate of the un￾known function, fo1 (xo) = fo (xo) + 
AT −Ko3A˙−1

X˜; the second term of the unknown function,

AT −Ko3A˙−1

X˜, arised in the derivation for the observer estimation error dynamics.
Recalling the universal approximation property of NNs (Lewis et al., 1998), for an unknown,
smooth function, fN (xN), its NN approximation is denoted by fN (xN) = WT
N σ 
VT
N xN

+εN, where
WN is the ideal NN weights matrix that is bounded such that WNF ≤ WM; σ (•) is the activation
function in the hidden layers, which is chosen to be the logarithmic sigmoid function and has the
property σ ≤ √N, with N being the number of hidden layer neurons in the NN; VN consists
of randomly selected constant weights; and εN is the NN reconstruction error, which is bounded
such that εN ≤ εM. Since the ideal NN weights are not available, it becomes necessary to intro￾duce NN weight estimates, WˆN, for which an acceptable tuning law is derived later in this section.
Specifically, the ideal, continuousl updated approximation for the unknown function correspond￾ing to the observer is given by fo1 (xo) = WT
o σ 
VT
o xo

+ εo = WT
o σo + εo, where the target NN
weights are bounded by Wo ≤ WMo and the approximation error is bounded by εo ≤ εMo. More￾over, using estimated weights, the approximation for the unknown function in (9.46) is given by
ˆfo1Ξ = Wˆ oΞσo

VT
o xˆo

= Wˆ oΞσˆo and its input is given with ˆxo = 
1, Xˆ T , Vˆ T , X˜ T T
.
In order to account for the effects of intermittent NN updates, begin by adding and subtracting
WT
o σ (xoΞ) = WT
o σoΞ to the ideal approximation
fo1 (xo) =WT
o σoΞ −WT
o σoΞ +WT
o σo +εo
=WT
o σoΞ +WT
o [σo −σoΞ] +εo. (9.47)
The expression given by (9.47) can be interpreted as the ideal approximation given by an intermit￾tently updated NN and WT
o [σo −σoΞ] can be viewed as the approximation error that results from
the intermittent updating. As the frequency of events increases, the values of the event-sampled
variables approach those of their continuously sampled counterparts. As a result, with a very large
number of events, the approximation error caused by event-sampling begins to vanish and the idealEvent-Triggered Control Applications 295
NN approximation is eventually reverted back to its original form. With this, the engineering trade￾off is clear: Fewer events will yield more computational efficiency, however, the efficiency comes
at the expense of accuracy. With regards to its effects on stability, the approximation error that re￾sults from intermittent NN updates is addressed by designing an event-execution law that ensures
acceptable behavior in the closed-loop dynamics; the details for this are be shown in Theorem 9.6,
Case 2.
Next, by adding and subtracting 
AT −Ko3A˙−1

X˜ and using the information in (9.45) and (9.46),
the estimation error dynamics are found to be
˙
X˜(t) = AV˜(t)−[Ko1 −Ko3]X˜(t) +ξ1(t)
˙
Z˜(t) = 
fo +
AT −Ko3A˙−1
X˜(t)

− ˆfo1Ξ −Ko2A−1X˜(t)−
AT −Ko3A˙−1
X˜(t)−εoΞ +τ¯d(t). (9.48)
Next, observe that, from (9.46), V˜ = V −Vˆ = Z˜ −Ko3A−1X˜. The derivative of this expression can
be taken and, by adding and subtracting WT
o σ (xˆoΞ) = WT
o σˆoΞ as well as using (9.47) and (9.48), the
estimation error dynamics corresponding to V are found to be
˙
V˜ =
fo +
AT −Ko3A˙−1
X˜

−WT
o σ 
VT
o xˆoΞ

+WT
o σ 
VT
o xˆoΞ

− ˆfo1Ξ −Ko2A−1X˜
−
AT −Ko3A˙−1
X˜ +τ¯d −Ko3A˙−1X˜ −Ko3A−1 
AV˜ −[Ko1 −Ko3]X˜ +ξ1

=−Ko3V˜ + ˜foΞ −A−1 [Ko2 −Ko3 [Ko1 −Ko3]]X˜ −ATX˜ +εoΞ +ξ2, (9.49)
where ˜foΞ = 
WT
o −Wˆ T
oΞ

σˆoΞ and εoΞ = WT
o [σo −σoΞ] + WT
o [σoΞ −σˆoΞ], which is equal to
WT
o [σo −σˆoΞ], contains the approximation error due to event-sampling, and ξ2 = εo + τ¯d −
Ko3A−1ξ1 is bounded such that ξ2 ≤ ξ2M, where ξ2M = εMo +MMτM +Ko3AI
Mξ1M. These dynam￾ics given by (9.48) and (9.49) are used in demonstrating the boundedness of the observer subsystem
when the NN is updated intermittently.
The following theorem is given in order to demonstrate that the observer described in this section
generates an ISS-like Lyapunov function with respect to bounded measurement errors. The results
for Theorem 9.4 are needed in order to make conclusions on the inter-event periods being bounded
away from zero.
Theorem 9.4. (NN Observer Boundedness): Consider the observer given by (9.46) with estimation
error dynamics described by (9.48) and (9.49). Furthermore, let the NN weights be updated with
˙
Wˆ oΞ = FoσˆoX˜ T −κo1FoWˆ oΞ, (9.50)
where Fo = FT
o > 0 and κo1 > 0 are constant design parameters, and let the initial weights be in
a compact set. Moreover, let the activation function for the NN be Lipschitz. Finally, let the event￾sampling measurement errors corresponding to X, X, and ˆ X be assumed to be bounded such that ˙
ΞSX ≤ ΞX max, Ξˆ SX ≤ Ξˆ X max, and Ξˇ SV ≤ Ξˇ V max, respectively. Then, there exist design constants, Ko1,
Ko2, and Ko3, such that the observer estimation errors, X and ˜ V , as well as the observer NN weight ˜
estimation errors, W˜ oΞ, are locally universally ultimately bounded (UUB).
Sketch of Proof:
 We shall consider the positive-definite Lyapunov candidate
VoΞ = 1
2
X˜ TX˜ +
1
2
V˜ TV˜ +
1
2
tr
W˜ T
oΞF−1 o W˜ oΞ

. (9.51)296 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
 By invoking known bounding conditions, and by using properties of the matrix trace oper￾ation, it is discovered that
V˙
oΞ ≤ −
[Ko1 −Ko3]
2 − No
κo1

X˜


2
−

Ko3
2 − No
κo1
− 1
2

V˜


2
− κo1
4

W˜ oΞ


2
F
+κo1W2
Mo + ξ 2
1M
2[Ko1 −Ko3]
+ ξ 2
2M
2Ko3
+
1
2
ε2
oΞ (9.52)
after completion of squares with respect to 
X˜

,

V˜

, and 
W˜ oΞ

.
 It is at this point that the measurement errors are extracted from the approximation error
caused by intermittent NN updates. In order to do this, the Lipschitz condition on the NN
activation function is to be invoked.
 Additionally, observe that the unknown function that is approximated by the NN is defined
in terms of V and X˜. With these in consideration, it is discovered
ε2
oΞ ≤
1
2
W4
Mo +
1
2
Lσ ΞSX 4 +
1
2
Lσ

Ξˆ SX


4
+
1
2
Lσ

Ξˇ SV


4
, (9.53)
where Lσ is the Lipschitz constant and Ξˇ SV , ΞSX , and Ξˆ SX are the event-sampling-driven
measurement errors corresponding to V, Vˆ and X, Xˆ, respectively.
 Note that the measurement error corresponding to V is rewritten in terms of X˙; this is done
because, in practice, the measured velocity is not available and, therefore, it would not be
possible to implement an execution law directly for V. This problem can be circumvented
by placing a differentiator at the sensor and considering a measurement error in terms of X˙.
Finally, (9.52) and (9.53) are combined to get
V˙
oΞ ≤ −KX˜

X˜


2
−KV˜

V˜


2
−KWo

W˜ oΞ


2
F +Bo, (9.54)
where Bo = ηo + 1
4LσΞ4
X max + 1
4LσΞˆ 4
X max + 1
4LσΞˇ V max with ηo = κo1W2
Mo + 1
4W4
Mo +
ξ 2
1M/[2[Ko1 −Ko3]] + ξ 2
2M/[2Ko3]; KX˜ = [Ko1 −Ko3]/2 − No/κo1; KV˜ = [Ko1 −1]/2 −
No/κo1; and KWo = κo1/4. Finally, (9.54) is less than zero provided that Ko1 > Ko3 +
[2No]/κo1 and Ko3 > [2No]/κo1 +1 and the following inequalities hold:

X˜

 >

Bo
KX˜
or 
V˜

 >

Bo
KV˜
or
W˜ oΞ


F >

Bo
K2
Smax
. (9.55)
 Note that the bounds are primarily dependent on the observer NN. Consequently, by ap￾propriate choice of the observer NN (e.g., number of neurons), the bounds on the observed
states will reduce.
Remark 9.5. The results from Theorem 9.4 can be easily used to demonstrate how the observer
generates an ISS-like Lyapunov function with respect to bounded measurement errors. This result,
along with the assumption that the dynamics (9.43) are Lipschitz, fulfills the necessary requirements
needed to show the existence of nonzero inter-event periods (Tabuada, 2007). However, rather than
showing how the observer generates an ISS-like Lyapunov function by itself, (9.54) will be used later
when the closed-loop dynamics are considered and it will be demonstrated how the whole system
exhibits ISS-like behavior.
In Theorem 9.4, the measurement errors were assumed to be bounded. However, in order to
implement the event-sampled controller, it must be demonstrated that the measurement errors are,
in fact, bounded for all time. This can be done by, first, considering the system dynamics when event￾sampling measurement errors are zero and, second, considering the system dynamics with nonzeroEvent-Triggered Control Applications 297
measurement errors. With nonzero measurement errors, an event-execution law can be designed in
order to ensure that the system dynamics remain stable. In this chapter, a single event-execution law
will be designed when the closed-loop dynamics are considered in Theorem 9.6. However, before
proceeding, the following lemma is given in order to show that the observer with an intermittently
updated NN is eligible for implementation in the event-sampled controller.
Lemma 9.4. Consider the observer given by (9.46) with estimation error dynamics described by
(9.48) and (9.49). Furthermore, let the NN weights be updated with (9.50) with initial weights in a
compact set. Then, there exist design constants, Ko1, Ko2, and Ko3, such that the observer estimation
errors, X and ˜ V , as well as the observer NN weight estimation errors, ˜ W˜ oΞ, are locally UUB for all
time.
Sketch of Proof: Here the proof is constructed in two steps.
 Case 1. In this case, the NN weights are updated using (9.50) and, furthermore, all ap￾proximation and measurement errors that are caused by event-sampling are taken to be
zero. As a result, εoΞ is absent from the observer estimation error dynamics for ˙
V˜ . Be￾cause of this, the coefficient defined in Theorem 9.4 corresponding to 
V˜


2 needs to
be changed to KV˜ = [Ko1]/2 − No/κo1 and the bounded term needs to be revised to
Bo = κo1W2
Mo + ξ 2
1M/[2[Ko1 −Ko3]] + ξ 2
2M/[2Ko3]. Additionally, the event-sampled term
that was previously combined with the approximation error due to intermittent updates no
longer needs to be considered in the presence of measurement errors. Therefore, the ex￾pression for ξ2 becomes ξ2 = εo +τ¯d −Ko3A−1ξ1 +WT
o [σo −σˆo] with ξ2 ≤ ξ2M, where
ξ2M = εMo + MMτM +Ko3AI
Mξ1M +2WMo√No. With these changes in mind, and by select￾ing gains satisfying Ko1 > Ko3 + [2No]/κo1 and Ko3 > [2No]/κo1, it can be concluded that
(9.54) is less than zero and that all signals in the observer are locally UUB.
 Case 2. In this case, the NN weights are held between events, and therefore, the effects
of the third term in (9.51) vanish when the derivative is taken. However, since the ap￾proximation error caused by event-sampling is injected through the estimation error dy￾namics, ˙
V˜ , it becomes necessary to account for the nonzero measurement errors. Us￾ing an approach similar to what was done for Case 1, the coefficients are updated with
Bo = W4
Mo/4+ξ 2
1M/[2[Ko1 −Ko3]] +ξ 2
2M/[2Ko3]; KX˜ = [Ko1 −Ko3]/2; KV˜ = [Ko1 −2]/2;
and KWo = −No/2. With these changes in mind, the expression for V˙
oΞ is rewritten with
V˙
oΞ ≤ −KX˜

X˜


2
−KV˜

V˜


2
−KWo

W˜ oΞ


2
F +Bo
+
1
4
Lσ ΞSX 4 +
1
4
Lσ

Ξˆ SX


4
+
1
4
Lσ

Ξˇ SV


4
. (9.56)
In this case, the measurement errors cannot be assumed to be bounded and it becomes nec￾essary to address their effects. This could be accomplished by designing an event-execution
law that takes the form

Ξχ


4 ≤ γχμχ

eχ


2
, (9.57)
where, in general, 0 < γχ ,μχ < 1 are design constants and eχ = χd −χ is the tracking error
corresponding to the measurement error. An execution-law bearing strong resemblance to
(9.57) is what is presented in this chapter. However, the benefits of this design cannot be
easily seen by considering the observer dynamics alone and (9.57) is given here only for
illustration purposes. When the tracking errors are addressed in the closed-loop dynamics,
we shall demonstrate how an execution law in the form of (9.57) can be used to eliminate
the measurement errors from the observer subsystem.
 The results for Case 1 and Case 2 can be combined in order to make conclusions which
apply for all time. However, since the dynamics in Case 2 can only be fully assessed in the298 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
closed-loop, this final combination is not be done here, but, instead, is shown for the entire
UAV system. From the results that are presented, however, it can be concluded that, with
an appropriately designed event-execution law, the observer with an intermittently updated
NN qualifies as a candidate for an event-sampled output feedback controller.
Remark 9.6. As we shall see in the subsequent sections, a single event-execution law is designed
for the whole system. In other words, the measurement errors that originate in the observer are
combined with the measurement errors in the controller and a single condition is used as the ba￾sis by which events occur. Moreover, when an event does occur, both the observer NN as well as
the controller are updated with the most recent position and orientation sensor measurements and
velocity observer estimates.
Next, the event-sampled controller design is presented. First, the virtual controller is briefly
addressed, then the kinematic and dynamic controllers is designed under the influence of event￾sampling.
9.2.3 CONTROL OF QUADROTOR UAV
A natural progression for the derivation of the controller would be to begin with the design of the
kinematic controller, to proceed with the virtual controller, and to conclude with an analysis for the
dynamic controller. This progression is followed in (Dierks and Jagannathan, 2009c). However, in
order to incorporate the effects of event-sampling, it is, perhaps, easier to first address the stability
of the virtual controller and then consider the kinematic and dynamic controllers. The reason for this
is that the assessment of the virtual controller’s stability does not involve any explicit presence of
event-sampling measurement errors and its analysis can be quickly summarized. An important con￾sideration that needs to be made in the development of event-sampled controllers is in determining
how and where the controller injects measurement errors into the system. In the development for the
virtual controller, only the stability of the virtual control estimates is considered and the injection
of any term into the system is altogether absent. Instead, it is only through the analysis for the kine￾matic and dynamic controllers, where there is an injection of errors caused by event-sampling. For
this reason, the stability of the virtual control estimates is considered first and then the kinematic
and dynamic controllers is considered under the influence of event-sampling.
9.2.3.1 Virtual Control Design
In the developments made in this subsection, the stability of the desired virtual control estimates, Θˆ d
and ωˆd, as well as the boundedness of the virtual control NN weight estimates, WˆΩ, are considered.
Since the desired virtual controls are written in terms of the UAV’s measured position and orienta￾tion, there is an explicit presence of event-sampling measurement errors. However, the derivations
made for the virtual controller here can only assess the stability of the estimates and not how they
are injected into the system.
Begin by defining the virtual controller (Dierks and Jagannathan, 2009c)
˙
Θˆ dΞ(t) = TΩˆ dΞ +KΩ1Θ˜ dΞ
˙
Ωˆ dΞ(t) = ˆfΩ1Ξ +KΩ2T −1Θ˜ dΞ
ωˆdΞ(t) = Ωˆ dΞ +T −1KΘeΘΞ +KΩ3T −1Θ˜ dΞ, (9.58)
where ˆfΩ1Ξ is the event-sampled NN estimation of the unknown function, fΩ1Ξ (xΩ) = fΩΞ +
TTΘ˜ dΞ − KΩ3T˙ −1Θ˜ dΞ with fΩΞ = T˙ −1Θ˙ dΞ + T −1Θ¨ dΞ. Specifically, its estimation is given by
ˆfΩ1Ξ =Wˆ T
ΩΞσ 
VT
Ω xˆΩΞ
=Wˆ T
ΩΞσˆΩΞ, where its input is ˆxΩΞ = 
1, ρd, ρ˙ T
d , ρ¨ T
d , ...
ρ T
d , ΘT
dΞ, Ωˆ dΞ, Vˆ T , Θ˜ dΞ
T
.
Here, the subscript Ξ is used to reinforce the idea that the virtual control NN weight estimates areEvent-Triggered Control Applications 299
updated only at event-sampling instants. Additionally, eΘΞ is the UAV’s event-sampled orientation
tracking error, which is addressed later, and KΩ1, KΩ2, KΩ3, and KΘ are positive design constants.
Next, choose the NN weight update law
˙
WˆΩΞ(t) = FΩσˆΩΞΘ˜ T
dΞ −κΩ1FΩWˆΩΞ, (9.59)
where FΩ = FT
Ω > 0 and κΩ1 > 0 are constant design parameters. Then, using (9.58) and (9.59), the
virtual control estimation error dynamics can be determined and the first derivative of the Lyapunov
candidate describing the virtual control system, VΩΞ = 1
2Θ˜ T
dΞΘ˜ dΞ+ 1
2ω˜ T
dΞω˜dΞ+ 1
2 tr
W˜ T
ΩΞF−1
Ω W˜ΩΞ
,
can be found to satisfy (Dierks and Jagannathan, 2009c)
V˙
ΩΞ ≤ −
KΩ1 −KΩ3 − NΩ
κΩ1

Θ˜ dΞ


2
−

KΩ3
2 − NΩ
κΩ1

ω˜dΞ2 − κΩ1
4

W˜ ΩΞ


2
F +ηΩ, (9.60)
where ηΩ = κΩ1W2
MΩ + ξ 2
ΩM/[2KΩ3] and NΩ is the number of hidden layer neurons in the virtual
control NN. By observation of (9.60), it can be seen that, with appropriate selection of design param￾eters, all signals in the virtual controller remain bounded (Dierks and Jagannathan, 2009c). Next,
the kinematic control and dynamic control is considered under the influence of event-sampling.
9.2.3.2 Injection of Event-Sampled Desired Virtual Control
The tracking errors that correspond to the desired virtual control inputs are with respect to position
and orientation. Begin by defining the position tracking error
eρ (t) = ρd(t)−ρ(t). (9.61)
The dynamics of (9.61) are found to be
e˙ρ (t) = ρ˙d(t)−Rv(t). (9.62)
In order to stabilize (9.62), the desired velocity is selected to be
vd(t) = RT 
ρ˙d(t) +Kρ eρ (t)

, (9.63)
where Kρ = diag
kρx, kρy,kρz
 is a design matrix with all positive constants.
Since the desired velocity is a term that is injected into the system by the event-sampled con￾troller, it becomes necessary to consider (9.63) in the presence of measurement errors. Observe that
the measurement error that is injected into the system can be introduced into the analysis by noting
that, from (9.44), the event-sampled position measurement is given by ρΞ = ρ +Ξρ , which allows
(9.61) to be rewritten as
eρΞ = ρd −
ρ +Ξρ

= eρ −Ξρ , (9.64)
where Ξρ = [Ξx,Ξy,Ξz]
T is the vector of measurement errors corresponding to the quadrotor’s mea￾sured position. Using (9.64) in (9.63) reveals
vdΞ = RT 
ρ˙d +Kρ

eρ −Ξρ

= RT 
ρ˙d +Kρ eρ

−RTKρΞρ . (9.65)
Next, define ev = vdΞ −v and note that, from this, v = vdΞ −ev. Then, using vdΞ as a virtual control
input in the tracking error dynamics and substituting (9.65) into (9.62) gives
e˙ρΞ(t) = −Kρ eρ (t) +Rev(t) +KρΞρ (t). (9.66)300 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Next, the desired virtual control input corresponding to the quadrotor’s orientation is considered.
Begin by defining the orientation tracking error
eΘ(t) = Θd(t)−Θ(t), (9.67)
where Θd = [φd,θd,ψd]
T is the desired orientation. Recall that ψd is an external input to be selected
by the designer. Furthermore, it is shown in (Dierks and Jagannathan, 2009c) how φd and θd can be
calculated in terms of ρ˙d, ρ¨d, ψd, Kρ , and the unknown function fc1 (xc1). The NN approximation
for fc1 is given by
ˆfc1 = Wˆ T
c1σ 
VT
c1xˆc1

= Wˆ T
c1σˆc1 =  ˆfc11, ˆfc12, ˆfc13T ,
where the input is ˆxc1 = 
1, ρ˙ T
d , ρ¨ T
d , Vˆ , ΘT , X˜ T T
. These estimates can then used in the derivation
for the actual dynamic control.
Moving on, the dynamics of (9.67) are found to be
e˙Θ(t) = Θ˙ d(t)−Tω(t). (9.68)
In order to stabilize (9.68), the desired angular velocity is selected to be
ωd(t) = T −1 
Θ˙ d(t) +KΘeΘ(t)

, (9.69)
where KΘ = diag{kΘ1,kΘ2, kΘ3} is a design matrix with all positive constants. Since the desired
angular velocity is a term that is injected into the system by the event-sampled controller, it be￾comes necessary to consider (9.69) in the presence of measurement errors. This is accomplished by
observing that the event-sampled tracking error can be expressed as
eΘΞ(t) = Θd(t)−[Θ(t) +ΞΘ(t)] = eΘ(t)−ΞΘ(t), (9.70)
where ΞΘ = 
Ξφ ,Ξθ ,Ξψ
T is the vector of measurement errors corresponding to the quadrotor’s
measured orientation. Using (9.70) in (9.69) reveals
ωdΞ(t) = T −1 
Θ˙ d(t) +KΘ [eΘ(t)−ΞΘ(t)]
= T −1 
Θ˙ d(t) +KΘeΘ(t)−KΘΞΘ(t)

. (9.71)
Next, define eω = ωdΞ − ω and note that, from this, ω = ωdΞ − eω. Then, using ωdΞ as a virtual
control input in the tracking error dynamics and substituting (9.71) into (9.68) gives
e˙ΘΞ(t) = −KΘeΘ(t) +Teω(t) +KΘΞΘ(t). (9.72)
It is the tracking error dynamics given by (9.66) and (9.72) that are used when the closed-loop
dynamics are considered.
Remark 9.7. In contrast to the results in (Dierks and Jagannathan, 2009c), the tracking error
dynamics, (9.66) and (9.72), contain additional terms, KρΞρ and KΘΞΘ, respectively. These terms
are the artifacts that result from event-based sampling.
9.2.3.3 Event-Sampled Output Feedback Dynamic Control
The actual control input consists of two parts: u1 is a scalar corresponding to the total thrust and
u2 = [u21,u22,u23]
T gives the rotational torques corresponding to roll, pitch, and yaw directions,
respectively. These two parts are considered separately in the following.Event-Triggered Control Applications 301
9.2.3.4 Total Thrust
The time-sampled total thrust control input is given by (Dierks and Jagannathan, 2009c)
u1(t) =mkv3eˆvz
+m
cφdsθdcψd +sφdsψd
 x¨d +kρxx˙d −vˆR1 + ˆfc11
+m
cφdsθdsψd −sφdcψd
 y¨d +kρyy˙d −vˆR2 + ˆfc12
+mcφdcθd

z¨d +kρzz˙d −vˆR3 + ˆfc13 −g

, (9.73)
where the gain, kv3, is an element in the design matrix, Kv = diag{kv1, kv2, kv3}, with all positive
elements; ˆev = [eˆvx,eˆvy,eˆvz]
T = vd − vˆ with ˆv being the translational velocity observer estimate;
vˆR = [vˆR1,vˆR2,vˆR3]
T = KρRvˆ; and ˆfc1 =  ˆfc11, ˆfc12, ˆfc13
is the NN estimate introduced in the pre￾vious subsection. Since the estimates from the observer are being stored in the controller and only
being updated intermittently, it becomes necessary here to consider explicit measurement errors
corresponding to the estimated state-vector. As a result, the terms ˆev and ˆvR both have an explicit
presence of measurement errors.
In order to incorporate the measurement errors in the analysis, it becomes necessary to expand
certain computations so that the terms ˆevz, ˆvR1, ˆvR2, and ˆvR3 can be extracted. First, in order to be able
to consider ˆevz = vdz −vˆz, it is necessary to consider the desired velocity (9.65) under the influence
of event-sampling. By expanding the matrix multiplications, the expression for vdzΞ is obtained as
vdzΞ = vdz +R¯3RKρΞρ , (9.74)
where R¯3R is the third row of RT . Next, a similar procedure is used to find ˆvR1, ˆvR2, and ˆvR3 under the
influence of event-sampling. Using ˆvxΞ = vˆx+Ξˆ vx, ˆvyΞ = vˆy+Ξˆ vy, and ˆvzΞ = vˆz+Ξˆ vz, it is discovered
that
vˆR1Ξ = vˆR1 +kρxR1RΞˆ v, vˆR2Ξ = vˆR2 +kρyR2RΞˆ v,
vˆR3Ξ = vˆR3 +kρzR3RΞˆ v, (9.75)
where R1R, R2R, and R3R are the first, second, and third rows of R, respectively, and Ξˆ v = 
Ξˆ vx,Ξˆ vy,Ξˆ vzT is the vector of measurement errors corresponding to the translational velocity esti￾mates. Finally, using (9.74) and (9.75) in (9.73) reveals
u1Ξ = ufΞ
1 +mkv3

RT
3RKρ

Ξρ−
m
kρxR1R +kρyR2R +kρzR3R + 0 0 kv3
Ξˆ v, (9.76)
where ufΞ
1 is identical to u1 (9.73), but assumes the event-sampled NN estimates, ˆfc1Ξ; the implicitly
affected NN estimation has no effect on further analyses. The control input, u1Ξ, is designed in order
to stabilize the translational velocity tracking error dynamics, which are given by
e˙vΞ(t) = −S(ω) ev − 1
mG(Rd)−τ¯d1
+RT
d

ρ¨d +Kρ ρ˙d −KρRvˆ+ fc1 (xc1)

− 1
mu1ΞEz. (9.77)
Since the event-sampled control input is the sum of the time-sampled control input and the measure￾ment error terms, the results of substituting the expression for u1Ξ into the tracking error dynamics
(9.77) can be used to easily find that (Dierks and Jagannathan, 2009c)
e˙vΞ(t) = −[S(ω) +Kv] ev −Kvv˜+RT
dW˜ c1Ξσˆ T
c1Ξ +ξc1
+
kv3

RT
3RKρ

Ξρ −
kρxR1R +kρyR2R
+kρzR3R + 0 0 kv3
Ξˆ v

Ez, (9.78)302 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
where ξc1 = RT
dWT
c1Ξσ˜ T
c1Ξ +RT
d εc1 −τ¯d1, W˜ c1Ξ = Wc1 −Wˆ c1Ξ, and σ˜c1Ξ = σc1Ξ −σˆc1Ξ. In contrast to
(Dierks and Jagannathan, 2009c), the dynamics (9.78) have the additional measurement error terms
that result from event-sampling.
Later, the tracking error dynamics given by (9.78) are combined in an augmented vector along
with the angular velocity tracking error dynamics, where they are both considered together. Before
doing that, however, the event-sampled control inputs corresponding to the rotational torques and
the angular velocity tracking errors are presented.
9.2.3.5 Rotational Torques
First, consider the angular velocity tracking error dynamics given by
Je˙ω(t) = fc2 (xc2)−u2(t)−τd2(t)−TT eΘ(t), (9.79)
where fc2 (xc2) is an unknown function whose NN approximation is given by ˆfc2 =
Wˆ T
c2σc2

VT
c2xˆc2

= Wˆ T
c2σˆc2 with an input ˆxc2 =

1, ωˆ T , ˙
Ωˆ T
d , Θ˜ T
d , eT
Θ

. Next, the rotational torque con￾trol input is given by (Dierks and Jagannathan, 2009c)
u2(t) = ˆfc2 +Kωeˆω(t),
where Kω = diag{kω1,kω2, kω3} is a design matrix with all positive constants, and ˆeω = ωˆd − ωˆ .
The event-sampled control input u2Ξ injects explicit measurement errors into the system through the
eˆω term. Observing that
ωˆd(t) = Ωˆ d +T −1KΘeΘ(t) +KΩ3T −1Θ˜ d(t),
it can be seen that, here, a measurement error corresponding to the measured orientation is injected
into the system. Using (9.70) it is found that
ωˆdΞ = ωˆd −T −1KΘΞΘ.
Now, noting that ωˆΞ = ωˆ +Ξˆ ω, it is revealed that the event-sampled control signal
u2Ξ(t) = ˆfc2Ξ +Kωeˆω(t)−Kω

T −1KΘΞΘ(t)−Ξˆ ω(t)

, (9.80)
where ˆfc2Ξ is the event-sampled NN estimate given earlier and Ξˆ ω = 
Ξˆ ωx,Ξˆ ωy,Ξˆ ωz
T is the vector
of measurement errors corresponding to the rotational velocity estimates. Next, making use of the
fact that ˆeω = eω −ω˜d +ω˜ , as well as adding and subtracting WT
c2σˆcΞ, the event sampled tracking
error dynamics (9.79) can be rewritten as
Je˙ωΞ(t) = W˜ T
c2Ξσˆc2Ξ −Kωeω +Kωω˜d −Kωω˜ −TT eΘ +ξc2
−Kω

T −1KΘΞΘ −Ξˆ ω

, (9.81)
where W˜ T
c2Ξ = WT
c2 −Wˆ T
c2Ξ, ξc2 = εc2 +WT
c2σ˜cΞ −τd2, and σ˜c2Ξ = σc2 −σˆc2Ξ.
Next, define the augmented vector, eSΞ = 
eT
vΞ,eT
ωΞ
T
, with which the translation and angu￾lar velocities can be considered together. With this, it becomes necessary to also define J¯ =
[I3×3,03×3;03×3,J], a constant matrix; KS = [Kv,03×3;03×3,Kω] > 0, a positive definite design matrix;
SS (ω)=[S(ω),03×3;03×3,03×3], where eT
SΞSS (ω) eSΞ = 0; T¯ = [03×6;03×3,T]; ¯eΘ = 
01×3,eT
Θ
T
;
ω˜¯d = 
01×3,ω˜ T
d
T
; ξc = 
ξ T
c1,ξ T
c2
T
; and Ad = diag{Rd,I3×3}, with Rd = R(Θd). Additionally,
˜fcΞ = W˜ T
cΞσˆcΞ with W˜ cΞ = diag
W˜ c1Ξ,W˜ c2Ξ
 and σˆcΞ = 
σˆ T
c1Ξ,σˆ T
c2Ξ
T and V˜ is the velocity track￾ing error vector defined in the observer development. Finally, in order to make provisions for theEvent-Triggered Control Applications 303
effects of event-sampling, define the augmented measurement error vector, Ξˆ SV = 
Ξˆ T
v ,Ξˆ T
ω
T
. The
augmented coefficient matrix, KVΞ, corresponding to ΞSV can be found in terms of the gain matrices
Kρ and Kω as well as the elements of R and the gain kv3. With these, the tracking error dynamics
described by (9.78) and (9.81) can be rewritten with the single expression
J¯e˙SΞ(t) = AT
d ˜fcΞ −[KS +SS (ω)] eS −KSV˜ −T¯ T e¯Θ +KSω˜¯d
+ξc +KVΞΞˆ SV +KV
XΞΞSX , (9.82)
where ΞSX =  ΞT
ρ , ΞT
Θ
T is the augmented measurement error vector corresponding to the posi￾tion and orientation measurements and the coefficient matrix KV
XΞ accounts for the Ξρ and ΞΘ from
(9.78) and (9.81); the augmented coffecient matrix KV
XΞ can be found in terms of gain matrices Kρ ,
KΘ, and Kω as well as the elements of T −1.
With these results, the final two results may be presented. In Theorem 9.5, for the purposes of
analysis, the time-sampled controller is considered and the explicit measurement errors that would
be injected into the system by event-sampling are viewed as bounded inputs. In this way, the ISS￾like behavior of the system is demonstrated. In Theorem 9.6, the assumption on the boundedness of
the measurement errors is relaxed and it is shown that the measurement errors are, in fact, bounded
with the implementation of an appropriately selected event-execution law.
9.2.4 QUADROTOR UAV STABILITY
Theorem 9.5. (ISS-like Behavior of Quadrotor System): Consider the dynamics described by (9.43).
Let the NN observer be defined by (9.46) and let the observer NN weights be updated at event￾sampling instants with (9.50) with initial weights in a compact set; additionally, let the event￾sampled virtual controller be defined by (9.58) and let the virtual control NN weights be updated
at event-sampling instants with (9.59) with initial weights in a compact set. Moreover, consider the
event-sampled desired virtual control inputs and actual control inputs, (9.65), (9.71), (9.76), and
(9.80), respectively, that are designed to stabilize the event-sampled tracking error dynamics given
by (9.66), (9.72), and (9.82). Additionally, let the NN weights corresponding to the actual control
be updated at event-sampling instants with
˙
Wˆ cΞ = FcσˆcΞ [AdeˆS]
T −κc1FcWˆ cΞ, (9.83)
with initial weights in a compact set, and where κc1 > 0 and Fc = FT
c > 0 are constant design
parameter. Finally, let the measurement errors that would be injected into the system as a result
of intermittent sampling be bounded such that ΞSX  ≤ ΞX max,

Ξˆ SX

 ≤ Ξˆ X max,

Ξ˙ SX

 ≤ Ξ˙ X max,
and 
Ξˆ SV

 ≤ Ξˆ V max. Then, there exist positive design constants Ko1, Ko2, Ko3, KΩ1, KΩ2, and KΩ3,
and positive-definite design matrices Kρ , KΘ, Kv, and Kω, such that the observer estimation errors,
X and ˜ V , the NN weight estimation errors, ˜ W˜ oΞ, the desired virtual control estimation errors, Θ˜ dΞ
and ω˜dΞ, the virtual control NN estimation errors, W˜ ΩΞ, the actual control NN weight estimation
errors, W˜ cΞ, and the position, orientation, and translational and rotational velocity tracking errors,
eρ , eΘ, and eS, respectively, are all locally UUB.
The key idea of the proof is to construct a Lyapunov candidate function and analyze its time￾derivative. In particular, let the positive-definite Lyapunov candidate incorporating all the signals of
the closed-loop system is given by
VUAV Ξ = K2
SmaxVoΞ +K2
SmaxVΩΞ +VcΞ,
where KSmax is the maximum singular value of KS and
VcΞ = 1
2
eT
ρ eρ +
1
2
eT
ΘeΘ +
1
2
eT
S Je¯ S +
1
2
tr
W˜ T
c F−1 c W˜ c

. (9.84)304 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
The first derivative of this Lyapunov candidate function can be shown to be negative semi-definite,
and hence, it can be concluded that all signals in the closed-loop are locally UUB. Detailed proof
can be seen in Szanto (2016).
Remark 9.8. By defining the augmented vector
ζ = 
X˜ ,V˜ ,Θ˜ dΞ,ω˜dΞ,eK,...eS,W˜ oΞF,W˜ ΩΞF,W˜ cΞF
T ,
the final time-derivative of the Lyapunov function can be written in the form V˙ (ζ ) < −Δ(ζ ) +
Λ(ηUAV ,ΞUAV ), where the positive part, Λ, is viewed as an input to the closed-loop system and
is a function of bounded measurement and NN reconstruction errors. It can, therefore, be concluded
that the continuously sampled, closed-loop system has a local ISS-like Lyapunov function associated
with it. Together with the assumption that the system (9.43) is locally Lipschitz, this result satisfies
all conditions necessary to show that there exists positive, nonzero inter-event periods (Tabuada,
2007). This provision is necessary in order to ensure the avoidance of Zeno behavior.
As previously mentioned, the results of Theorem 9.5 are contingent on the boundedness of the
event-sampling measurement errors. However, in order to implement the event-sampled controller,
these results, by themselves, are not sufficient. In addition, it is necessary to demonstrate the bound￾edness of the measurement errors. The following theorem addresses the boundedness of measure￾ment errors by considering two cases: the first case analyzes the dynamics and the influences of
V˙
cΞ(t) and V˙
oΞ(t) when the measurement errors are zero and the second case considers the system
with nonzero measurement errors. It is in the analysis of the second case, an event-execution law is
designed using Lyapunov stability analysis.
Theorem 9.6. (Boundedness of Measurement Errors (Szanto, 2016)): Consider the dynamics de￾scribed by (9.43). Let the NN observer be defined by (9.46) and let the observer NN weights be
updated at event-sampling instants with (9.50) with initial weights in a compact set; addition￾ally, let the event-sampled virtual controller be defined by (9.58) and let the virtual control NN
weights be updated at event-sampling instants with (9.59), initialized within a compact set. More￾over, consider the event-sampled desired virtual control inputs and actual control inputs, (9.65),
(9.71), (9.76), and (9.80), respectively, that are designed to stabilize the event-sampled track￾ing error dynamics given by (9.66), (9.72), and (9.82). Furthermore, let the NN weights cor￾responding to the actual control be updated at event-sampling instants with (9.83) with initial
weights in a compact set. Finally, let the event-sampling measurement errors satisfy the condition
ΞXUAV +ΞVUAV  ≤ γSX μSX eK2+γSV μSV eS2
, where ΞXUAV and ΞVUAV are augmented mea￾surement error vectors corresponding to the quadrotor’s measured output and its estimated velocity,
respectively; eK and eS are the augmented tracking error vectors corresponding to the quadrotor’s
measured output and its estimated velocity; and 0 < γSX , γSV ,μSX ,μSV < 1 are all design constants.
Then, there exist positive design constants Ko1, Ko2, Ko3, KΩ1, KΩ2, and KΩ3, and positive-definite
design matrices Kρ , KΘ, Kv, and Kω, such that the observer estimation errors, X and ˜ V , the NN ˜
weight estimation errors, W˜ oΞ, the desired virtual control estimation errors, Θ˜ dΞ and ω˜dΞ, the vir￾tual control NN estimation errors, W˜ΩΞ, the actual control NN weight estimation errors, W˜ cΞ, and
the position, orientation, and translational and rotational velocity tracking errors, eρ , eΘ, and eS,
respectively, are all locally UUB.
The key idea of this proof is to use the event-triggering conditions to bound the measurement
errors in the Lyapunov time-derivative expression obtained in the results of Theorem 9.5.
9.2.5 NUMERICAL EXAMPLE
The objective of this simulation example is to illustrate the effects of event-sampling. With this in
mind, simulations were performed using the event-sampled controller presented in this section asEvent-Triggered Control Applications 305
Table 9.5
Output feedback controller design for a quadrotor UAV
Observer dynamics ˙
Xˆ(t) = AZˆ +Ko1X˜, ˙
Zˆ(t) = ˆfo1Ξ +G¯ +Ko2A−1X˜ + M−1UΞ,
Vˆ(t) = Zˆ +Ko3A−1X˜
Virtual controller ˙
Θˆ dΞ(t) = TΩˆ dΞ +KΩ1Θ˜ dΞ, ˙
Ωˆ dΞ(t) = ˆfΩ1Ξ +KΩ2T −1Θ˜ dΞ,
ωˆdΞ(t) = Ωˆ dΞ +T −1KΘeΘΞ +KΩ3T −1Θ˜ dΞ
Desired virtual control inputs vdΞ = RT 
ρ˙d +Kρ eρ

−RTKρΞρ
ωdΞ(t) = T −1 
Θ˙ d(t) +KΘeΘ(t)−KΘΞΘ(t)

Observer NN weight update rule ˙
Wˆ oΞ(t) = FoσˆoX˜ T −κo1FoWˆ oΞ
Virtual controller NN weight update rule ˙
WˆΩΞ(t) = FΩσˆΩΞΘ˜ T
dΞ −κΩ1FΩWˆΩΞ
Actual controller NN weight update rule ˙
Wˆ cΞ = FcσˆcΞ [AdeˆS]
T −κc1FcWˆ cΞ
Event-triggering condition ΞXUAV +ΞVUAV  ≤ γSX μSX eK2 +γSV μSV eS2
well as its time-sampled counterpart presented in (Dierks and Jagannathan, 2009c). In order to eval￾uate the controller’s performance, the averages of the control inputs, tracking errors, and observer
estimation errors for both time- and event-sampled controllers are summarized. Additionally, the
effects of varrying the parameters γSX and γSV are considered.
The simulations performed in (Dierks and Jagannathan, 2009c) took into account disturbances
such as unknown aerodynamic effects, blade flapping, and signal noise. Moreover, it introduced a
parameter, α, which describes how the thrust is redirected as a result of, in part, wind conditions;
by taking this parameter to be initially zero and then suddenly increasing it to 200 at t = 20 s, the
effects of an external influence on the system, such as a gust of wind, can be illustrated. All of the
considerations that were taken in (Dierks and Jagannathan, 2009c) are taken here. In general, event￾triggering mechanisms for strict-feedback systems can be implemented in numerous ways. In this
Chapter, the event-sampling scheme employed is one such that the measurement errors and tracking
errors corresponding to the UAV’s position and velocity are combined and the combinations become
the basis by which an event occurs; specifically, an event takes place when there is a failure in the
condition
ΞXUAV 2 +ΞVUAV 2 ≤ γSX μSX eK2 +γSV μSV eS2 .
Identical simulation parameters and controller gains were used in both simulations. The gains
selected in (Dierks and Jagannathan, 2009c) were used here with the following exceptions: The
orientation control gains were chosen to be KΘ = diag{40,40,40} and the angular velocity control
gains were selected as Kω = diag{35,35,35}. Additionally, the desired trajectory remained identical
to what was considered in (Dierks and Jagannathan, 2009c), with the only changes being ωx = ωy =
0.06π, rx = ry = 0.1, rz = 0.5, and ωψ = 0.03π. Finally, for the results that are illustrated in the
figures, the event-execution parameters were chosen to be γSX = γSV = 0.95.306 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
í
í
í 


í
í









X(m)
Quadrotor Tra jectory
Y(m)
Z(m)
Actual Desired Observer Estimate
       









Time (sec)
UAV Position Error
eρΞ
Figure 9.5 UAV Trajectory Tracking.
       










u1Ξ(N)
Total Thrust Control Input
Time (sec)
      
í


u21Ξ(Nm)
Rotational Torque Control Inputs
      
í


u22Ξ(Nm)
      
í


u23Ξ(Nm)
Time (sec)
Figure 9.6 Control Inputs.
The event-sampled quadrotor UAV trajectory is shown in Figure 9.5. The effects of the sudden
disturbance at t = 20 s can be clearly seen in the UAV position error. Although the effects of the
disturbances are clearly visible, the controller’s ability to compensate and recover is also evident.
The recovery, however, is not without expense: Figure 9.6 clearly shows an increase in the total
thrust and the rotational torque control inputs when the value of α jumps from 0o to 200.
The results shown in Figures 9.5 and 9.6 demonstrate the stability of the event-sampled con￾troller. However, with regard to the effects of event-based sampling, these results are not tremen￾dously revealing. In an effort to illustrate these effects, consider the results shown in Figure 9.7. The
occurrence of events is shown. By normalizing the total number of available samples on the x−axis
to one and by scaling the number of events with an equivalent factor, it can be seen that about 60%
of the total samples are event-sampling instants. In other words, the remaining 40% of the sam￾ples are instants when it was unnecessary to update the controller. These results can be summarized
by stating that the implementation of the event-sampled controller yielded a 40% reduction in the
number of sampling instants used by the controller.
9.2.6 EFFECTS OF EVENT-SAMPLING
As a final assessment of the effectiveness of event sampling, consider the information provided in
Tables 9.6 and 9.7. Firstly, the effects of changing the event-execution parameters, γSX and γSV , are
explored and, secondly, the relationship between the number of events and controller performance
is assessed. Whereas previous results spoke only to the stability of the controller, the information
in Tables 9.6 and 9.7 will allow the performance of the event-sampled controller to be compared to
that of the time-sampled controller.Event-Triggered Control Applications 307
      







Total Samples Available
Occurrence of Events
Occurrence of Events out of Available Samples
Figure 9.7 Effectiveness of Event-Sampling
Table 9.6
Effects of Event-Sampling on Mean Squared Errors and Control Effort Means
γSX , γSV Γ ℵeρ ℵeΘ ℵev ℵeω ℵρ˜
0 1 0.0026 0.0019 0.2608 6.094 3.29×10−4
0.01 0.8050 0.0293 0.0218 2.936 95.93 2.89×10−4
0.1 0.7003 0.0249 0.0236 2.448 92.69 3.06×10−4
0.5 0.6254 0.0152 0.0189 1.516 48.72 3.35×10−4
0.95 0.5705 0.0155 0.0252 1.630 38.97 3.08×10−4
Before considering the data, a few remarks are made concerning the notations in Tables 9.6
and 9.7. Firstly, the selection of γSX = γSV = 0 is equivalent to implementing the time-sampled
controller; hence, the first row will serve as a standard to which the event-sampled cases can be
compared. Secondly, the number of events that occur out of the total available samples is given
by Γ = (Number o f Events)/(Total Samples Available); for the time-sampled case, the value for
this parameter is unity. Thirdly, as a basis for comparisons, the mean squared errors, MSE (•), are
considered for the tracking and observer estimation errors corresponding to the position, orientation,
and translational and angular velocities. The values summarized in Tables 9.6 and 9.7 are calculated
with ℵj = ∑
i=x,y,z
MSE (ji) for j = eρ , eΘ, ev,eω,ρ˜,Θ˜ ,v˜,ω˜ . Finally, in order to analyze control efforts
under the influence of event-sampling, the means of the control inputs, ¯u, are considered.
First, observe that the number of events increases as γSX and γSV are increased; in other words,
the number of computations executed by the controller decreases with increasing γ’s. This behavior
is explained by noting that, with smaller γ’s, the upper bounding threshold on the measurement
errors is decreased; with smaller thresholds, it takes less amount of time for the measurement errors
to grow, reach its threshold and trigger an event. If the threshold is made zero by selecting γSX =
γSV = 0, an event is triggered every instant that finite measurement errors exist and, practically, the
event-sampled controller exhibits time-sampled behavior.
Next, concerning tracking errors, there appear to be marginal differences in the mean squared
errors corresponding the position and orientation. With respect to translational and angular veloci￾ties, it is clear that the performance of the time-sampled controller is better; this, however, is not to
say that the event-sampled controller performs poorly in these areas. As a matter of fact, given the
exceptional position and orientation tracking error performances, it would seem that the effects of308 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
Table 9.7
Effects of Event-Sampling on Mean Squared Errors and Control Effort Means
ℵΘ˜ ℵv˜ ℵω˜ u¯1 u¯21 u¯22 u¯23
0.0029 0.0033 0.3117 9.430 0.7554 1.014 1.155
0.0127 0.0040 3.787 9.430 0.7554 1.015 1.136
0.0106 0.0040 2.470 9.426 0.6790 1.005 1.089
0.0081 0.0042 1.467 9.425 0.6090 0.9772 1.002
0.0103 0.0042 1.115 9.426 0.6618 0.9345 0.9668
event-sampling on the velocities are inconsequential. These conclusions can be very easily made for
the observer estimation errors as well. Especially in the case of the observer estimation errors corre￾sponding to translational velocity, it appears that event-sampling has very little effect. Finally, it can
be seen that, with event-sampling, the amount of control effort that is needed does not change sub￾stantially. It is evident that, especially with the rotational torques, greater control effort is required,
but the additional amount is insignificant relative to the total.
The effects of event sampling are summarized by the information in Tables 9.6 and 9.7: The
use of event-sampling gives flexibility in the amount of computations executed by the controller;
moreover, while the reduction in computations does come at a cost with regards to performance, the
fidelity of the controller is not significantly compromised.
9.3 DISTRIBUTED SCHEDULING PROTOCOL AND CONTROLLER DESIGN
FOR CYBER PHYSICAL SYSTEMS
In CPS, multiple real-time dynamic systems are connected to their respective controllers through a
shared communication network, as opposed to a dedicated line. Existing centralized or distributed
scheduling schemes prove unsuitable for such CPS, as they overlook the behavior of real-time dy￾namic systems in the network protocol design. As highlighted in Chapters 6-8, large-scale systems
adhere to asynchronous sampling conditions, yet multiple subsystems might concurrently trigger
events, attempting to utilize the shared network for information transmission. In cases where the
network bandwidth falls short, packet collisions can result in data loss. Therefore, in this part of the
chapter, a distributed scheduling protocol design via cross-layer approach is presented. This proto￾col, as we shall see, seeks to optimize the performance of CPS by maximizing a utility function,
which is generated by using the information collected from both the application and network lay￾ers. Subsequently, an adaptive model based optimal event-triggered control scheme is designed for
each real-time dynamic system with unknown system dynamics in the application layer. Compared
with traditional scheduling algorithms, the distributed scheduling scheme via cross-layer approach,
developed by Xu (2012) and (Xu and Jagannathan, 2012), not only allocates the network resources
efficiently but also improves the performance of the overall real-time dynamic system.
9.3.1 BACKGROUND
Several researchers, as noted by Tian et al. (2010), recognize the potential advantages of integrating
control and networking protocol designs, offering benefits such as cost savings, enhanced adapt￾ability, reliability, and usability. This integration has given rise to the CPS (Xia et al., 2011a,b). In
CPS, where control and communication subsystems are closely intertwined, specialized control and
communication schemes must be devised, taking into account their interdependence. Notably, Xia
et al. (Xia et al., 2011a) proposed a cyber-physical control scheme that considers the effects of aEvent-Triggered Control Applications 309
fixed communication network to maintain the stability of the CPS’s control system. Additionally,
in their work (Xia et al., 2011b), the authors evaluated the performance of the widely used IEEE
802.11 protocol in the context of CPS. Nevertheless, a recurring theme in these studies is the lack
of consideration for real-time interactions between control and communication subsystems. An op￾timal algorithm for CPS should harness these real-time interactions to enhance the performance of
both subsystems. To address the intricate interplays among different layers effectively, a crucial ap￾proach is cross-layer design (Srivastava and Motani, 2005). Cross-layer design refers to an approach
where communication protocols from different components of open systems interconnection (OSI)
layers collaborate or interact to achieve specific optimization or performance goals. The OSI model
is a conceptual framework developed by the International Organization for Standardization (ISO),
aiming to provide a shared foundation for coordinating the development of standards to facilitate
system interconnection. Within the OSI reference model, the communication between systems is
segmented into distinct abstraction layers. It is noteworthy, however, that most cross-layer designs
are predominantly implemented for data link and physical layers (Srivastava and Motani, 2005),
often neglecting the application layer. In the context of CPS, it becomes imperative to jointly con￾sider control design at the application layer and protocol design for communication at the data link
layer. According to the IEEE 802.11 standard, the carrier sense multiple access (CSMA) protocol is
introduced for scheduling communication links in a distributed manner (Jiang and Walrand, 2009).
In this protocol, a communication link wishing to transmit does so only if it does not detect an on￾going transmission from the network. However, the prevalent use of random access schemes in most
CSMA-based distributed scheduling, focusing solely on enhancing data link layer performance, can
adversely impact the application layer (i.e., the control system). Consequently, these protocols are
deemed suboptimal and unsuitable within the context of CPS as they have the potential to degrade
CPS performance. Simultaneously, as we have seen in the earlier chapters, at the application layer,
achieving optimal control design is a formidable challenge, particularly in the presence of uncertain
real-time system dynamics. Traditional optimal control schemes, sampled periodically, necessitate
substantial network resources.
In this section, we shall explore the application of event-triggered learning-based controllers
in a cross layer design scheme in the context of CPS that includes an event-triggered controller
design in the application layer and a distributed scheduling algorithm in the data-link layer. The key
components of this application include:
1. distributed scheduling via cross layer approach that improves performance of CPS by minimizing
the cost function from both data link and application layers
2. an adaptive model-based optimal event-triggered control scheme that is designed in a forward￾in-time manner and without using the knowledge of system dynamics
Different from the discussions earlier in this chapter as well as in the earlier chapters, we shall see
that the event-triggered control scheme presented in this section is based on events that are initiated
not only by the control system but also via the shared network performance.
9.3.2 THE SCHEDULING AND CONTROL PROBLEMS IN CPS
Consider the case where multiple subsystems or CPS, controlled in real-time, try to communicate
to their respective controllers through a shared communication network. Without loss of generality,
we shall assume that these systems are homogeneous (structurally similar). On the other hand,
to save the network resources, event-triggered system is used instead of a traditional time-driven
sampling. Since the all the CPS are homogeneous, consider a CPS pair (cyber and physical system
components) represented as
xi,k+1 = Aixi,k +Biui,k, (9.85)310 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
where xi,k and ui,k are CPS system states and control inputs, respectively, and Ai,Bi denote system
matrices for i
th CPS pair. However, the error between the system states used in the controller and
actual values might increase quickly in ZOH event-triggered control scheme. For overcoming this
drawback, we shall develop an adaptive model-based event-triggered system. Besides event-trigger
mechanism, an adaptive model (Aˆi,k,Bˆi,k) is used for the controller to estimate the system state
vector when controller has not received any information from the sensor. The estimated system
state vector can be represented as
xˆi,k+1 = Aˆi,kxˆi,k +Bˆi,kui,k, (9.86)
for each i. Recall that the adaptive model will be updated once when the most recent system state
vector is received at the controller. Eventually, the adaptive model and estimated system states will
converge close to the actual system states respectively, which in turn will improve the performance
of the event-triggered control system.
9.3.3 CROSS-LAYER DESIGN FOR CPS
First, the event-triggered control design and distributed scheduling protocol are implemented at
all CPS pairs, which are sharing the communication network. Each CPS pair tunes its adaptive
model-based optimal event-triggered controller design by using a distributed scheduling algorithm,
computes its value function based on tuned control design, and transmits the information to the data
link layer. Second, data link layer can update the scheduling of the CPS pair based on network traffic
payload from data link layer and the value function information received from the application layer.
9.3.3.1 Distributed Scheduling Algorithm for CPS
Without loss of generality, we shall assume that a traditional wireless ad-hoc network protocol
is implemented in other layers. For optimizing the performance of multiple CPS pairs, including
the performance from both application layer and data link layer, an optimal cross-layer distributed
scheduling algorithm is developed by incorporating control system information from application
layer. Firstly, the cost function for the CPS pair is represented as
Ji,k = xT
i,kQixi,k +uT
i,kSiui,k +βiRi,k, (9.87)
where Ri,k is i
th CPS pair average traffic payload during [0,kTs] (Ts is the periodic sampling time)
and βi is the weight of average traffic payload for the i
th CPS pair. A large βi indicates that the
average traffic payload will adversely influence the total cost and should be reduced by appropriate
design of the scheduling protocol.
For M CPS pairs, i.e., for i = 1,...,M in (9.86) and (9.87), the overall cost function can be
represented as
Jk =
M
∑
i=1
Ji,k =
M
∑
i=1
(xT
i,kQixi,k +uT
i,kSiui,k +βiRi,k(π)), (9.88)
with π denoting the scheduling policy. Next, the optimal design of multiple CPS pairs should mini￾mize the cost function (9.88), i.e.,
J∗
k = min
u,π
M
∑
i=1
(xT
i,kQixi,k +uT
i,kSiui,k +βiRi,k(π)), (9.89)
where u = (uT
1 ,...,uT
M)T is the control policy and π is the scheduling policy.
Obviously, each pair of CPS has two possible status (for scheduling) at any time: 1) the CPS pair
is scheduled; and 2) the CPS pair is not scheduled. Note that whether or not a CPS pair is scheduled
depends upon this status, which can minimize cost value. For instance,Event-Triggered Control Applications 311
Case 1: The i
th CPS pair has been scheduled
Js,1
i,k = xT
i,kQixi,k +xT
i,kΛixi,k +βiRY
i,k, (9.90)
where Λi = Kˆ T
i,kSiKˆi,k and RY
i,k is the average traffic payload when the i
th CPS pair has been
scheduled.
Case 2: The i
th CPS pair has not been scheduled
Js,2
i,k = xT
i,kQixi,k +xˆ
T
i,kΛixˆi,k +βiRN
i,k, (9.91)
where RN
i,k is the average traffic payload when the i
th CPS pair has not been scheduled.
Then, the difference between these two cases can be considered as utility function and expressed as
ΔJs
i,k = πi,kφ(ei,k)−πi,kβiDi,k, (9.92)
where Di,k = RY
i,k − RN
i,k = (Nk+1
kTs − Nk
kTs
)Ni,bit = 1
kTs
Ni,bit with Ni,bit denoting the number of bits for
packetizing the sensed event at the i
th CPS pair, πi,k is the schedule indicator of the i
th CPS pair,
ei,k = xi,k −xˆi,k, and φ(ei,k)=(xˆ
T
i,kΛixˆi,k −xT
i,kΛixi,k). It can be seen that when ΔJs
i,k > 0, scheduling
the i
th CPS pair can reduce the cost. Therefore, one may schedule the i
th CPS pair when ΔJs
i,k > 0.
It is important to note that there may be multiple CPS pairs eligible to transmit simultaneously,
i.e., the utility functions of several CPS pairs are higher than zero, indicating that all of these CPS
pairs have to be scheduled. However, according to the literature on networking (see Sarangapani
and Xu (2018)), only one CPS pair can access the network. Hence, for optimizing the performance
of network, the optimal scheduling policy should maximize the total utility function, that is
(ΔJs
k)
∗ = maxπ ∑
i∈Gk
ΔJs
i,k, (9.93)
where Gk is the CPS pair set with positive value of utility function at time kTs, i.e., all the i
th
CPS pair with ΔJs
i,k > 0. The main idea behind the distributed scheduling algorithm is to separate
the transmission time of different CPS pairs by using backoff interval (BI) (Mazo and Tabuada,
2011; Xu, 2012), which is designed based on a related utility function in a distributed manner. For
example, to solve the optimal scheduling problem (9.93) for multiple pairs of CPS, the BI can be
designed as
BIi,k = κ ×(exp−ΔJi,k +ni,k) for l ∈ Gk, (9.94)
where κ is a scaling factor and ni,k is a random variable sampled from a Gaussian distribution, and
L = mini, j∈Gk (expΔJi,k −expΔJj,k ) is the range of the random value ni,k.
Result 1: (Xu and Jagannathan, 2012; Xu, 2012) Given the multiple CPS pairs and event￾triggered control scheme, the distributed scheduling scheme selects the adaptive model-based event￾triggered CPS pair with highest utility function value since it has the shortest backoff interval (i.e.
BI) and highest priority to access the shared communication network. In addition, this algorithm
can render best performance schedules for every CPS pair.
9.3.3.2 Adaptive Model-Based Event-Triggering Protocol Design
First, the adaptive state estimator design is presented. For the i
th CPS pair, the event-triggered con￾trol system and adaptive state estimator with received information can be represented as312 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
xi,k+1 = Aixi,k +Biui,k = θ T
i zi,k (9.95)
xˆi,k+1 = Aˆi,kxˆi,k +Bˆi,kui,k = θˆT
i,kzi,k, (9.96)
where θi = [AiBi]
T and θˆ
i,k = [Aˆi,kBˆi,k]
T denote the target and estimated system matrices for the i
th
CPS pair, respectively, and zi,k = [xT
i,k,uT
i,k]
T , represents the augmented state and control vectors. The
update law for the i
th CPS pairs’ estimated parameter vector θˆ
i,k can be designed as
θˆ
i,k+1 = θˆ
i,k +αi,eγi,kzi,keT
i,k, (9.97)
where ei,k+1 = xi,k+1 − xˆi,k+1 is the state estimation error, αi,e < 1 is a positive tuning parameter,
and γi,k is an indicator to the status of the event-triggering condition. Recall that the infinite-horizon
value function of adaptive model-based event-triggered control system for i
th CPS pair can be de￾fined as
V(xi,k) =
∞
∑
l=k
(xT
i,lQixi,l +uT
i,lSiui,l) = xT
i,kPixi,k. (9.98)
Here Pi is the solution to the algebraic Riccati equation corresponding to the i
th CPS pair. Then the
Hamiltonian for the system is represented as
H(xi,k,ui,k) = r(xi,k,ui,k) +V(xi,k+1)−V(xi,k), (9.99)
with r(xi,k,ui,k) = xT
i,kQixi,k + uT
i,kSiui,k is the one-step cost. Based on the standard optimal control
theory, the optimal control input can be represented as
u∗
i,k = K∗
i xk = −(Si +BT
i,kPiBi,k)
−1BT
i,kPiAi,kxi,k, (9.100)
where K∗
i is the optimal feedback gain.
On the other hand, we can define the optimal action-dependent value function as
V(xi,k,ui,k)=[xT
i,k uT
i,k]Θi[xT
i,k uT
i,k]
T (9.101)
with the matrix Θi given by
Θi =

Θxx
i Θxu
i
Θux
i Θuu
i

=

AT
i PiAi +Qi AT
i PiBi
BT
i PiAi BiPiBi +Si

. (9.102)
According to (9.102), the optimal control gain for adaptive model-based event-triggered control
system pair can be represented in terms of value function parameters, Θi, as K∗
i = −(Θuu
i,k)−1Θux
i,k.
Note that if the parameter vector Θi can be estimated online, then system dynamics are not needed
to calculate the optimal control gain for each of the i
th adaptive model-based event-triggered control
system pair. Then the action-dependent value function can be represented as
V(xˆi,k,ui,k) = zˆ
T
i,kΘizˆi,k = ρT
i wi,k, (9.103)
where ˆzi,k = [xT
i,k uT
i,k]
T and wi,k = (zˆ
2
i,k1,...,zˆi,k1zˆi,k j,zˆ
2
i,k2,...,zˆi,k(j−1)zˆi,k j,zˆ
2
i,k j) is the regression
vector obtained as polynomials using the Kronecker product operation, and ρi = vec(Θi), as we
have defined in Chapters 3 and 6 while developing the event-based Q-learning algorithm.
The estimated Bellman equation with estimated state can be represented as Vˆ(xˆi,k+1,ui,k+1) −
Vˆ(xˆi,k,ui,k)+r(xˆi,kui,k) = eT D
i,k , where eT D
i,k is the temporal difference (TD) error. The estimated valueEvent-Triggered Control Applications 313
function for the i
th CPS adaptive model-based event-triggered control system pair can be expressed
as Vˆ(xˆi,k,ui,k) = ρˆ T
i,kwe
i,k. Then the dynamics of the TD error can be rewritten as
eT D
i,k = r(xˆi,k,ui,k) +ρˆ T
i,kΔWe
i,k (9.104)
with ΔWe
i,k−1 = we
i,k −we
i,k−1. Next, an auxiliary error vector incorporating the history of cost-to-go
function is defined as
Πi,k = Γi,k−1 +ρˆ T
i,kΔWe
i,k−1, (9.105)
where Γi,k−1 = [r(xˆi,k−1,ui,k−1),r(xˆi,k−2,ui,k−2),...,r(xˆi,k−1−j,ui,k−1−j)] and the regression matrix
ΔWe
i,k−1 = [ΔWe
i,k−1 ...,ΔWe
i,k−1−j
]. Then the update law for the matrix Θˆ i corresponding to the i
th
CPS pair can be defined as
ρˆi,k+1 = ΔWe
i,k(ΔWeT
i,k ΔWe
i,k)
−1(αi,ρΠT
i,k −ΓT
i,k) (9.106)
with a tuning parameter 0 < αi,ρ < 1. Eventually, recalling (9.102), the optimal control gain can be
developed by using the estimated states as
uˆi,k = Kˆi,kxˆi,k = −(Θˆ uu
i,k)
−1Θˆ ux
i,kxˆi,k. (9.107)
Result 2: (Xu and Jagannathan, 2012; Xu, 2012) Let ui,0 be an initial admissible control policy
for i
th pair CPS adaptive model-based event-triggered control system. Let the adaptive update law
be given by (9.106). Then there exists a positive constant 0 < αi,ρ < 1 such that the estimated
parameter and optimal signal converges to the actual parameter and optimal signal respectively.
9.3.4 EXAMPLE
Example 9.1. In this example, the cross-layer CPS co-design is evaluated. The CPS includes six
pairs that are located within 300m×300m area, randomly. For maintaining the homogeneous prop￾erty, all six pairs are using the similar control system. These examples are obtained from works by
Mazo and Tabuada (2011) and Xu and Jagannathan (2012). The discrete-time model is given as
xk+1 =

1.1138 −0.0790
0.0592 0.8671 
xk +

0.2033
0.1924
uk, (9.108)
with sampling interval Ts = 0.15 seconds, the number of bits for the six quantized sensed data for
the CPS pairs are defined as Nbit = [10 8 6 7 8 4].
First, the performance of the optimal adaptive model- based event-triggered control is shown.
An average value of state regulation errors for the six CPS pairs is shown in the Figure 9.8. The
results indicate that optimal adaptive model-based event-triggered control design presented in this
section can force the regulation errors to zero asymptotically, ensuring all CPS pairs are stable.
Then the performance of adaptive model-based event-triggered CPS’s event-triggering error and
threshold are shown for one of the subsystems. As shown in Figure 9.9, when an event is triggered
and scheduled, the estimation error will be reset to zero since actual CPS state will be received at
the controller. On the other hand, if the event is not triggered and scheduled, the estimation error
increases due to inaccurate adaptive model. Once the adaptive parameters are estimated accurately,
the estimation error converges to zero. Next, the performance of the cross-layer distributed schedul￾ing has been evaluated. For comparison, classical widely used embedded round robin (ERR) (Mazo
and Tabuada, 2011) and Greedy scheduling (Mazo and Tabuada, 2011) are added. In Figure 9.10,
the cost function of multiple pairs of CPS with three different scheduling schemes is compared.
The cross-layer distributed scheduling maintains the lowest value while costs of multiple pairs of314 Optimal Event-Triggered Control Using Adaptive Dynamic Programming
0 7.5 15 22.5 30 37.5 45 -15
-10
-5
0
5
10
Time (Sec)
Regulation error
e1
e2
Figure 9.8 State regulation errors.
0 5 10 15 20 25 30 35 40 45 0
0.5
1
1.5
2
2.5
3
3.5
Time (Sec)
||ek
||2
Figure 9.9 Event-triggered estimation error.
0 5 10 15 20 25 30 35 40 45 0
0.5
1
1.5
2
2.5
3
3.5
4 x 107
Time (Sec)
Cost function
Proposed distributed scheduling
Embedded Round Robin
Greedy Scheduling
Figure 9.10 The cost function comparison.Event-Triggered Control Applications 315
6 8 10 12 14 16 18 20 0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
number of CPS pairs
Fairness index (FI)
Proposed distributed scheduling
Embedded Round Robin
Greedy scheduling
Figure 9.11 The fairness comparison.
CPS with ERR and Greedy scheduling are much higher. This suggests that the distributed schedul￾ing scheme can improve the performance of multiple pairs of CPS. In Figure 9.11, fairness indices
of the distributed scheduling protocol presented in this section and widely used ERR schemes are
recorded. They seem to perform similarly and are very close to one, whereas the same metric for
the Greedy scheduling is much less than one. The ERR method, though is fair, has higher cost than
the distributed scheduling protocol.
9.4 SUMMARY AND CONCLUDING REMARKS
In conclusion, this chapter presented the implementation of an approximation-based control ap￾proach for a robot manipulator using only the joint position vector within an event-driven NN
learning framework. Two control configurations were introduced, along with two event-triggering
conditions in each configuration. The analysis revealed that the bounds on the closed-loop signals
were smaller in the first configuration, where the observer was co-located with the sensor and had
continuous access to the output. Event-triggering conditions were derived based on NN weights,
striking a balance between computational complexity and event generation. Analytical bounds were
calculated using Lyapunov stability theory, ensuring stability of the tracking error system and NN
weights. Furthermore, an event-sampled output-feedback NN controller was developed for an un￾deractuated quadrotor UAV system.The controller performed well compared to its time-sampled
counterpart, offering engineering flexibility by providing a trade-off between computational effi￾ciency and tracking/estimation performance. Depending on the specific requirements, the proposed
event-sampled controller offers a viable solution. Also, in this chapter, we studied the importance of
cross-layer co-design for multiple pairs of CPS. While it is important to optimize the controllers, in
a shared communication network, optimizing and scheduling the transmission times will not only
improve the performance of the control system but also maximize the utility of the shared commu￾nication network. Amid the escalating complexity of CPS within safety- and infrastructure-critical
domains, there arises an expanding demand for co-design techniques rooted in data-driven learning.
These techniques aim to synthesize optimal control and scheduling policies tailored for large-scale
systems. Additionally, the heightened flexibility and interoperability of CPS expose them to in￾creased vulnerability concerning adversarial attacks. While reaping the advantages of data-driven
optimal control techniques, ensuring their safety and resilience against adversarial threats remains
an ongoing challenge.Bibliography
A. Al-Tamimi, F. L. Lewis, and M. Abu-Khalaf. Discrete-time nonlinear hjb solution using approx￾imate dynamic programming: Convergence proof. IEEE Trans. Syst., Man, Cybern., B, 38(4):
943–949, Aug. 2008.
Anuradha M Annaswamy and Alexander L Fradkov. A historical perspective of adaptive control
and learning. Annual Reviews in Control, 52:18–41, 2021.
A. Anta and P. Tabuada. To sample or not to sample: self-triggered control for nonlinear system.
IEEE Trans. on Automat. Contr., 55:2030–2042, 2010.
G. Antonelli. Interconnected dynamic systems: An overview on distributed control. IEEE Control
Systems, 33(1):76–88, 2013.
K. E. Arzen. A simple event-based pid controller. In Proceedings of the 14th World Congress of
IFAC, volume 18, pages 423–428, Beijing, China, Jul. 1999.
Karl Johan Astr ˚ om and Bo Bernhardsson. Comparison of periodic and event-based sampling for ¨
first-order stochastic systems. IFAC Proceedings Volumes, 32(2):5006–5011, 1999.
Karl Johan Astrom and Bo M Bernhardsson. Comparison of Riemann and Lebesgue sampling for
first order stochastic systems. In Proceedings of the 41st IEEE Conference on Decision and
Control, 2002., volume 2, pages 2011–2016. IEEE, 2002.
Karl Johan Astr ˚ om and Richard M Murray. ¨ Feedback systems: an introduction for scientists and
engineers. Princeton university press, 2021.
K.J. Astrom and B. Wittenmark. ¨ Adaptive Control. Dover Publications, 1995.
D. D. Bainov and P. S. Simeonov. Impulsive differential equations: periodic solutions and applica￾tions. Longman scientific & technical, 1993.
Leemon C Baird. Reinforcement learning in continuous time: Advantage updating. In Proceedings
of 1994 IEEE International Conference on Neural Networks (ICNN’94), volume 4, pages 2448–
2453. IEEE, 1994.
L. Bakule. Decentralized control: An overview. Annual Reviews in Control, 32(1):87–98, 2008.
Andrew R. Barron. Universal approximation bounds for superpositions of a sigmoidal function.
IEEE Transactions on Information Theory, 39(3):930–945, 1993.
A. G. Barto, R. S. Sutton, and C. W. Anderson. Neuronlike adaptive elements that can solve difficult
learning control problems. IEEE Transactions on Systems, Man, and Cybernetics, SMC-13(5):
834–846, Sept 1983. ISSN 0018-9472. doi: 10.1109/TSMC.1983.6313077.
Andrew G Barto, Warren Buckler Powell, Jennie Si, and Donald C Wunsch. Handbook of learning
and approximate dynamic programming. Wiley-Interscience, 2004.
Tamer Basar and Pierre Bernhard. H-infinity optimal control and related minimax design problems:
a dynamic game approach. Springer Science & Business Media, 2008.
Richard Bellman. Dynamic programming. Science, 153(3731):34–37, 1966.
Dimitri Bertsekas. Dynamic programming and optimal control: Volume I, volume 4. Athena scien￾tific, 2012.
Dimitri P Bertsekas. Value and policy iterations in optimal control and adaptive dynamic program￾ming. IEEE transactions on neural networks and learning systems, 28(3):500–509, 2015.
L Bittner. Ls pontryagin, vg boltyanskii, rv gamkrelidze, ef mishechenko, the mathematical theory
of optimal processes. viii+ 360 s. new york/london 1962. john wiley & sons. preis 90/-. Zeitschrift
Angewandte Mathematik und Mechanik, 43(10-11):514–515, 1963.
V. G. Boltyanskii, R. V. Gamkrelidze, and L. S. Pontryagin. The Mathematical Theory of Optimal
Processes. Interscience Publishers, 1961.
Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
Steven J Bradtke, B Erik Ydstie, and Andrew G Barto. Adaptive linear quadratic control using
317318 BIBLIOGRAPHY
policy iteration. In Proceedings of 1994 American Control Conference-ACC’94, volume 3, pages
3475–3479. IEEE, 1994.
Roger Brockett. The early days of geometric nonlinear control. Automatica, 50(9):2203–2224,
2014.
Eduardo Camponogara and Marcelo Lopes de Lima. Distributed optimization for MPC of linear
networks with uncertain dynamics. Automatic Control, IEEE Transactions on, 57(3):804–809,
2012.
CC Cheah, X Li, X Yan, and D Sun. Simple pd control scheme for robotic manipulation of biological
cell. IEEE Transactions on Automatic Control, 60(5):1427–1432, 2015.
Chien Chern Cheah, Xiang Li, Xiao Yan, and Dong Sun. Observer-based optical manipulation of
biological cells with robotic tweezers. IEEE Transactions on Robotics, 30(1):68–80, 2014.
Chi-Tsong Chen. Linear system theory and design. Saunders college publishing, 1984.
M.Z.Q. Chen, Liangyin Zhang, Housheng Su, and Chanying Li. Event-based synchronisation of
linear discrete-time dynamical networks. Control Theory Applications, IET, 9(5):755–765, 2015.
ISSN 1751-8644. doi: 10.1049/iet-cta.2014.0595.
Zheng Chen and S Jagannathan. Generalized hamilton–jacobi–bellman formulation-based neural
network control of affine nonlinear discrete-time systems. IEEE Transactions on Neural Net￾works, 19(1):90–106, 2008.
Tao Cheng, Frank L Lewis, and Murad Abu-Khalaf. A neural network solution for fixed-final time
optimal control of nonlinear systems. Automatica, 43(3):482–490, 2007.
Yi Cheng and Valery Ugrinovskii. Event-triggered leader-following tracking control for multivari￾able multi-agent systems. Automatica, 70:204–210, 2016.
Randy Cogill. Event-based control using quadratic approximate value functions. In Proceedings of
the 48h IEEE Conference on Decision and Control (CDC) held jointly with 2009 28th Chinese
Control Conference, pages 5883–5888. IEEE, 2009.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Con￾trol, Signals and Systems, 2(4):303–314, 1989.
Bruno da Silva and Andrew Barto. Td-deltapi: A model-free algorithm for efficient exploration. In
Proceedings of the AAAI Conference on Artificial Intelligence, pages 886–892, 2012.
S. N. Dashkovskiy, B. Ruffer, and F. R. Wirth. Small gain theorems for large scale systems and ¨
construction of ISS Lyapunov functions. SIAM Journal on Control and Optimization, 48(6):
4089–4118, 2010.
Jose de Jes ´ us Rubio. Sliding mode control of robotic arms with deadzone. ´ IET Control Theory &
Applications, 11(8):1214–1221, 2016.
Jose de Jes ´ us Rubio. Discrete time control based in neural networks for pendulums. ´ Applied Soft
Computing, 68:821–832, 2018.
T. Dierks and S. Jagannathan. Optimal control of affine nonlinear discrete-time systems. In Proc.
Medit. Conf. Control Autom., pages 1390–1395, Jun. 2009a.
T. Dierks and S. Jagannathan. Optimal control of affine nonlinear continuous-time systems. In Proc.
IEEE Amer. Control Conf., pages 1568–1573, Jun. 2010a.
T. Dierks and S. Jagannathan. A self-tuning optimal controller for affine nonlinear continuous-time
systems with unknown internal dynamics. In 2012 IEEE 51st IEEE Conference on Decision and
Control (CDC), pages 5392–5397, Dec 2012a. doi: 10.1109/CDC.2012.6425986.
Travis Dierks and S Jagannathan. Optimal tracking control of affine nonlinear discrete-time systems
with unknown internal dynamics. In Decision and Control, 2009 held jointly with the 2009 28th
Chinese Control Conference. CDC/CCC 2009. Proceedings of the 48th IEEE Conference on,
pages 6750–6755. IEEE, 2009b.
Travis Dierks and S Jagannathan. Output feedback control of a quadrotor uav using neural networks.
IEEE transactions on neural networks, 21(1):50–66, 2009c.
Travis Dierks and S Jagannathan. Online optimal control of affine nonlinear discrete-time systemsBIBLIOGRAPHY 319
with unknown internal dynamics by using time-based policy update. IEEE Transactions on Neu￾ral Networks and Learning Systems, 23(7):1118–1129, 2012b.
Travis Dierks and Sarangapani Jagannathan. Optimal control of affine nonlinear continuous-time
systems using an online hamilton-jacobi-isaacs formulation. In 49th IEEE Conference on Deci￾sion and Control (CDC), pages 3048–3053. IEEE, 2010b.
D. V. Dimarogonas and K. H. Johanson. Event-triggered control for multi-agent systems. In Proc.
IEEE Conf. Decision and Contr., pages 7131–7136, 2009a.
D. V. Dimarogonas and K. H. Johanson. Event-triggered cooperative control. In Proc. Europ. Contr.
Conf., pages 3015–3020, 2009b.
Lu Dong, Xiangnan Zhong, Changyin Sun, and Haibo He. Event-triggered adaptive dynamic pro￾gramming for continuous-time systems with control constraints. IEEE Transactions on Neural
Networks and Learning Systems, 28(8):1941–1952, 2017.
M. C. F. Donkers and W. P. M. H. Heemels. Output-based event-triggered control with guaranteed
l-infinity-gain and improved and decentralized event-triggering. IEEE Transactions on Automatic
Control, 57(6):1362–1376, Jun. 2012.
MCF Donkers and WPMH Heemels. Output-based event-triggered control with guaranteed L∞-
gain and improved event-triggering. In Proceedings of 49th IEEE Conference on Decision and
Control (CDC), pages 3246–3251, 2010.
R. Dorf, M. Farren, and C. Phillips. Adaptive sampling frequency for sampled data control systems.
IRE Transactions on Automatic Control, 7(1):38–47, Jan. 1962.
RC Dorf and RH Bishop. Modern control systems, pearsons, 2000.
Kenji Doya. Reinforcement learning in continuous time and space. Neural computation, 12(1):
219–245, 2000.
Philip G Drazin and Philip Drazin Drazin. Nonlinear systems. Cambridge University Press, 1992.
W. B. Dunbar. Distributed receding horizon control of dynamically coupled nonlinear systems.
IEEE Transactions on Automatic Control, 52(7):1249–1263, July 2007. ISSN 0018-9286. doi:
10.1109/tac.2007.900828.
P. Ellis. Extension of phase plane analysis to quantized systems. IRE Transactions on Automatic
Control, 4(2):43–54, Nov. 1959.
A. Eqtami, D. V. Dimarogonas, and K. J. Kyriakopoulos. Event-triggered control for discrete-time
systems. In Proc. Amer. Contr. Conf., pages 4719–4724, 2010.
Mirosław Galicki. Finite-time control of robotic manipulators. Automatica, 51:49–54, 2015.
Huijun Gao and Tongwen Chen. Network-based h∞ output tracking control. IEEE Transactions on
Automatic control, 53(3):655–667, 2008.
W. Gao, Y. Jiang, Z.P. Jiang, and T. Chai. Output-feedback adaptive optimal control of intercon￾nected systems based on robust adaptive dynamic programming. Automatica, 72:37–45, 2016.
E. Garcia and P. J. Antsaklis. Model-based event-triggered control with time-varying network de￾lays. In Proc. IEEE Conf. Decision and Contr., pages 1650–1655, 2011.
E. Garcia and P. J. Antsaklis. Parameter estimation in time-triggered and event-triggered model￾based control of uncertain systems. International Journal of Control, 85(9):1327–1342, Apr.
2012.
E. Garcia and P. J. Antsaklis. Model-based event-triggered control for systems with quantization
and time-varying network delays. IEEE Transactions on Automatic Control, 58(2):422–434, Feb
2013. ISSN 0018-9286. doi: 10.1109/tac.2012.2211411.
Shuzhi Sam Ge, Chang C Hang, Tong H Lee, and Tao Zhang. Stable adaptive neural network
control, volume 13. Springer Science & Business Media, 2013.
Rafal Goebel, Ricardo G Sanfelice, and Andrew R Teel. Hybrid dynamical systems. IEEE control
systems magazine, 29(2):28–93, 2009.
Tom Gommans, Duarte Antunes, Tijs Donkers, Paulo Tabuada, and Maurice Heemels. Self￾triggered linear quadratic control. Automatica, 50(4):1279–1287, 2014.320 BIBLIOGRAPHY
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.
Graham C Goodwin and Kwai Sang Sin. Adaptive filtering prediction and control. Courier Corpo￾ration, 2014.
Michael Green and John B Moore. Persistence of excitation in linear systems. In American Control
Conference, 1985, pages 412–417. Ieee, 1985.
M. Guinaldo, D. V. Dimarogonas, K. H. Johansson, J. Sanchez, and S. Dormido. Distributed ´
event-based control for interconnected linear systems. In 2011 50th IEEE Conference on De￾cision and Control and European Control Conference, pages 2553–2558, Dec 2011. doi:
10.1109/cdc.2011.6160580.
Mar´ıa Guinaldo, Daniel Lehmann, J Sanchez, Sebastian Dormido, and Karl Henrik Johansson. Dis- ´
tributed event-triggered control with network delays and packet losses. In 2012 IEEE 51st IEEE
Conference on Decision and Control (CDC), pages 1–6. IEEE, 2012.
A. Gusrialdi and S. Hirche. Communication topology design for large-scale interconnected systems
with time delay. In American Control Conference (ACC), 2011, pages 4508–4513. IEEE, 2011.
Wassim M Haddad, VijaySekhar Chellaboina, and Sergey G Nersesov. Impulsive and hybrid dy￾namical systems. Princeton Series in Applied Mathematics, 2006.
Yoram Halevi and Asok Ray. Integrated communication and control systems: Part i—analysis.
Journal of Dynamic Systems, Measurement, and Control, 110(4):367–373, 1988.
Yujuan Han, Wenlian Lu, and Tianping Chen. Consensus analysis of networks with time-varying
topology and event-triggered diffusions. Neural Networks, 71:196–203, 2015.
Tomohisa Hayakawa and Wassim M Haddad. Stable neural hybrid adaptive control for nonlinear
uncertain impulsive dynamical systems. In Proceedings of the 44th IEEE Conference on Decision
and Control, pages 5510–5515. IEEE, 2005.
Simon Haykin. Neural Networks: A Comprehensive Foundation. Macmillan College Publishing
Company, 1994.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pages 770–778, 2016a.
Wei He, Yiting Dong, and Changyin Sun. Adaptive neural impedance control of a robotic manip￾ulator with input saturation. IEEE Transactions on Systems, Man, and Cybernetics: Systems, 46
(3):334–344, 2016b.
Wei He, Haifeng Huang, and Shuzhi Sam Ge. Adaptive neural network control of a robotic ma￾nipulator with time-varying output constraints. IEEE Transactions on Cybernetics, 47(10):3136–
3147, 2017.
W. P. M. H. Heemels and M. C. F. Donkers. Model-based periodic event-triggered control of linear
systems. Automatica, 49(3):698–711, Mar. 2013.
W. P. M. H. Heemels, K. H. Johansson, and P. Tabuada. An introduction to event-triggered and self￾triggered control. In Proceedings of the 51st IEEE Conference on Decision and Control, pages
3270–3285, Maul, Hawaii, USA, Dec. 2012.
WP Maurice H Heemels, Andrew R Teel, Nathan Van de Wouw, and Dragan Nesic. Networked
control systems with communication constraints: Tradeoffs between transmission intervals, de￾lays and performance. IEEE Transactions on Automatic control, 55(8):1781–1796, 2010.
WPMH Heemels, JH Sandee, and PPJ Van Den Bosch. Analysis of event-driven controllers for
linear systems. International journal of control, 81(4):571–590, 2008.
T. Henningsson, E. Johannesson, and A. Cervin. Sporadic event-based control of first-order linear
stochastic systems. Automatica, 44(11):2890–2895, Nov. 2008.
Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural
networks. Science, 313(5786):504–507, 2006.
Sandra Hirche, Tilemachos Matiakis, and Martin Buss. A distributed controller approach for delay￾independent stability of networked control systems. Automatica, 45(8):1828–1836, 2009.BIBLIOGRAPHY 321
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni￾versal approximators. Neural Networks, 2(5):359–366, 1989.
Zhong-Sheng Hou and Zhuo Wang. From model-based control to data-driven control: Survey,
classification and perspective. Information Sciences, 235:3–35, 2013.
D. Hristu-Varsakelis and P. R. Kumar. Interrupt-based feedback control over a shared communi￾cation medium. In Proceedings of the 41st IEEE Conference on Decision and Control, pages
3223–3228, Las Vegas, Nevada, USA, Dec. 2002.
S. L. Hu and D. Yue. Event-triggered control design of linear networked systems with quantization.
ISA Transactions, 51(1):153–162, Jan. 2012.
Sunan Huang, Kok Kiong Tan, and Tong Heng Lee. Decentralized control design for large-scale
systems with strong interconnections using neural networks. IEEE Transactions on Automatic
Control, 48(5):805–810, May 2003. ISSN 0018-9286. doi: 10.1109/tac.2003.811258.
Don R. Hush and Barry G. Horne. Progress in supervised neural networks. IEEE Signal processing
magazine, 10(1):8–39, 1993.
Boris Igelnik and Yoh-Han Pao. Stochastic choice of basis functions in adaptive function approx￾imation and the functional-link net. IEEE Transactions on Neural Networks, 6(6):1320–1329,
1995.
O. C. Imer and T. Basar. To measure or to control: optimal control with scheduled measurements
and controls. In Proceedings of the American Control Conference, pages 14–16, Minneapolis,
MN, USA, Jul. 2006.
P Ioannou. Decentralized adaptive control of interconnected systems. IEEE Transactions on Auto￾matic Control, 31(4):291–298, 1986.
Petros Ioannou and Baris Fidan. Adaptive control tutorial. SIAM, 2006.
Kaabche Issam and Geng Qingbo. Research on control strategies for the stabilization of quadrotor
uav. In Fifth International Conference on Intelligent Control and Information Processing, pages
286–292. IEEE, 2014.
S. Jagannathan. Neural Network Control of Nonlinear Discrete-time Systems. CRC Press, Boca
Raton, FL, 2006.
Mohammad Jamshidi. Large-scale systems: modeling, control, and fuzzy logic. Prentice-Hall, Inc.,
1996.
Libin Jiang and Jean Walrand. A distributed csma algorithm for throughput and utility maximization
in wireless networks. IEEE/ACM Transactions on Networking, 18(3):960–972, 2009.
Y. Jiang and Z.P. Jiang. Robust adaptive dynamic programming for large-scale systems with an
application to multimachine power systems. IEEE Transactions on Circuits and Systems II:
Express Briefs, 59(10):693–697, 2012.
Yu Jiang and Zhong-Ping Jiang. Global adaptive dynamic programming for continuous-time non￾linear systems. IEEE Transactions on Automatic Control, 60(11):2917–2929, 2015.
Z. P. Jiang and Y. Wang. Input-to-state stability for discrete-time nonlinear system. Automatica, 37:
857–869, 2001.
Z.P. Jiang, A. R. Teel, and L. Praly. Small-gain theorem for ISS systems and applications. Mathe￾matics of Control, Signals, and Systems (MCSS), 7(2):95–120, 1994.
Z.P. Jiang, I.M.Y. Mareels, and Y. Wang. A Lyapunov formulation of the nonlinear small-gain
theorem for interconnected ISS systems. Automatica, 32(8):1211–1215, 1996.
Marcus Johnson, Rushikesh Kamalapurkar, Shubhendu Bhasin, and Warren E Dixon. Approxi￾mate n-player nonzero-sum game solution for an uncertain continuous nonlinear system. IEEE
Transactions on Neural Networks and Learning Systems, 26(8):1645–1658, 2015.
Thomas Kailath. Linear systems, volume 156. Prentice-Hall Englewood Cliffs, NJ, 1980.
Rudolf E. Kalman. Contributions to the theory of optimal control. Boletin de la Sociedad Matem￾atica Mexicana, 1960a.
Rudolf E. Kalman. A new approach to linear filtering and prediction problems. Journal of Basic322 BIBLIOGRAPHY
Engineering, 82(1):35–45, 1960b.
Rudolf E Kalman and John E Bertram. Control system analysis and design via the “second method”
of lyapunov: I—continuous-time systems. Journal of Basic Engineering, 1960.
Rudolf Emil Kalman. Mathematical description of linear dynamical systems. Journal of the Society
for Industrial and Applied Mathematics, Series A: Control, 1(2):152–192, 1963.
Rushikesh Kamalapurkar, Huyen Dinh, Shubhendu Bhasin, and Warren E Dixon. Approximate
optimal trajectory tracking for continuous-time nonlinear systems. Automatica, 51:40–48, 2015.
Iasson Karafyllis and Miroslav Krstic. Adaptive certainty-equivalence control with regulation￾triggered finite-time least-squares identification. IEEE Transactions on Automatic Control, 63
(10):3261–3275, 2018.
Iasson Karafyllis, Maria Kontorinaki, and Miroslav Krstic. Adaptive control by regulation-triggered
batch least squares. IEEE Transactions on Automatic Control, 65(7):2842–2855, 2019.
Mehrdad R Kermani, Rajni V Patel, and Mehrdad Moallem. Multimode control of a large-scale
robotic manipulator. IEEE Transactions on Robotics, 23(6):1264–1270, 2007.
H. K. Khalil. Nonlinear Systems. Prentice Hall, Upper Saddle River, NJ, 2002.
Mohammad Javad Khojasteh, Pavankumar Tallapragada, Jorge Cortes, and Massimo Franceschetti. ´
The value of timing information in event-triggered control. IEEE Transactions on Automatic
Control, 65(3):925–940, 2019.
Minho Kim. A simple and fast approach to design neural network approximators using the ran￾dom vector functional link network. Proceedings of IEEE International Conference on Neural
Networks, pages 2136–2140, 1996.
Young Ho Kim and Frank L Lewis. Neural network output feedback control of robot manipulators.
IEEE Transactions on Robotics and Automation, 15(2):301–309, 1999.
Donald E. Kirk. Optimal Control Theory: An Introduction. Dover Publications, 2004.
Bahare Kiumarsi, Kyriakos G Vamvoudakis, Hamidreza Modares, and Frank L Lewis. Optimal
and autonomous control using reinforcement learning: A survey. IEEE Transactions on Neural
Networks and Learning Systems, 29(6):2042–2062, 2017.
Morris Kline. Mathematical Thought from Ancient to Modern Times. Oxford University Press,
1990.
Bart Kosko. Neural Networks and Fuzzy Systems: A Dynamical Systems Approach to Machine
Intelligence. Prentice Hall, 1992.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet large scale visual recognition
challenge. In Advances in neural information processing systems, volume 25, 2012.
P. R. Kumar and P. Varaiya. Stochastic Systems: Estimation, Identification, and Adaptive Control.
Prentice-Hall, 1986.
Sun Yuan Kung. Digital Neural Networks. Prentice Hall, 1993.
B. C. Kuo. Analysis and Synthesis of Sampled Data Control Systems. Prentice Hall, Englewood
Cliffs, NJ, 2012.
Huibert Kwakernaak and Raphael Sivan. Linear optimal control systems, volume 1. Wiley￾interscience New York, 1972.
GC Layek et al. An introduction to dynamical systems and chaos, volume 449. Springer, 2015.
Yann LeCun, Bernhard E Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne
Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition.
In Neural Computation, pages 541–551, 1989.
Yann LeCun, Leon Bottou, Genevieve B Orr, and Klaus-Robert M ´ uller. Efficient backprop. In ¨
Neural networks: Tricks of the trade, pages 9–50. Springer, 2002.
DongBin Lee, Timothy C Burg, Bin Xian, and Darren M Dawson. Output feedback tracking control
of an underactuated quad-rotor uav. In 2007 American Control Conference, pages 1775–1780.
IEEE, 2007.
J. Y. Lee, J. B. Park, and Y. H. Choi. Integral reinforcement learning for continuous-time input-BIBLIOGRAPHY 323
affine nonlinear systems with simultaneous invariant explorations. IEEE Transactions on Neu￾ral Networks and Learning Systems, 26(5):916–932, May 2015. ISSN 2162-237X. doi:
10.1109/TNNLS.2014.2328590.
Tong Heng Lee and Christopher John Harris. Adaptive neural network control of robotic manipula￾tors, volume 19. World Scientific, 1998.
D. Lehmann and J. Lunze. Event-based output-feedback control. In Proceedings of the 19th
Mediterranean Conference on Control and Automation, pages 982–987, Corfu, Greece, Jun.
2011.
Michael Lemmon. Event-triggered feedback in control, estimation, and optimization. Networked
control systems, pages 293–358, 2010.
Martin D. Levine. Vision in Man and Machine. McGraw-Hill, 1991.
F. L. Lewis, S Jagannathan, and A Yesildirak. Neural network control of robot manipulators and
non-linear systems. CRC Press, 1998.
F. L. Lewis, D. Vrabie, and K. G. Vamvoudakis. Reinforcement learning and feedback control:
Using natural decision methods to design optimal adaptive controllers. IEEE Control Systems,
32(6):76–105, Dec 2012a. ISSN 1066-033x. doi: 10.1109/mcs.2012.2214134.
Frank L. Lewis, Darren M. Dawson, and Chaouki T. Abdallah. Neural network control of robot
manipulators and nonlinear systems. Series in Systems and Control, 1999.
Frank L Lewis, Draguna Vrabie, and Vassilis L Syrmos. Optimal control. John Wiley & Sons,
2012b.
Jr-Shin Li and Navin Khaneja. Control of inhomogeneous quantum ensembles. Physical review A,
73(3):030302, 2006.
Daniel Liberzon. Calculus of Variations and Optimal Control Theory: A Concise Introduction.
Princeton University Press, 2011.
Luen-Woei Liou and Asok Ray. A stochastic regulator for integrated communication and control
systems: Part i—formulation of control law. Journal of dynamic systems, measurement, and
control, 113(4):604–611, 1991.
Richard P. Lippmann. An introduction to computing with neural nets. IEEE ASSP Magazine, 4(2):
4–22, 1987.
D. Liu, D. Wang, D. Zhao, Q. Wei, and N. Jin. Neural-network-based optimal control for a class
of unknown discrete-time nonlinear systems using globalized dual heuristic programming. IEEE
Transactions on Automation Science and Engineering, 9(3):628–634, July 2012. ISSN 1545-
5955. doi: 10.1109/tase.2012.2198057.
Derong Liu and Qinglai Wei. Finite-approximation-error-based optimal control approach for
discrete-time nonlinear systems. IEEE Transactions on Cybernetics, 43(2):779–789, 2013.
Derong Liu, Ding Wang, and Hongliang Li. Decentralized stabilization for a class of continuous￾time nonlinear interconnected systems using online learning optimal control approach. IEEE
Transactions on Neural Networks and Learning Systems, 25(2):418–428, 2014.
T. Liu and Z.P. Jiang. A small-gain approach to robust event-triggered control of nonlinear systems.
IEEE Transactions on Automatic Control, 60(8):2072–2085, 2015.
Yen-Chen Liu and Nikhil Chopra. Control of robotic manipulators under input/output communica￾tion delays: Theory and experiments. IEEE Transactions on Robotics, 28(3):742–751, 2012.
L. Ljung. System Identification: Theory for the User. Prentice-Hall, 1999.
Robert E. Lucas and Nancy L. Stokey. Optimal fiscal and monetary policy in an economy without
capital. Journal of Monetary Economics, 12(1):55–93, 1983.
G. Luders and K. Narendra. An adaptive observer and identifier for a linear system. IEEE Trans.
on Automat. Contr., 18:496–499, 1973.
David G Luenberger. Dynamic Systems. J. Wiley Sons, 1979.
Jan Lunze and Daniel Lehmann. A state-feedback approach to event-based control. Automatica, 46
(1):211–215, 2010.324 BIBLIOGRAPHY
Tarek Madani and Abdelaziz Benallegue. Sliding mode observer and backstepping control for a
quadrotor unmanned aerial vehicles. In 2007 American control conference, pages 5887–5892.
IEEE, 2007.
M. Mazo and P. Tabuada. Decentralized event-triggered control over wireless sensor/actuator net￾works. IEEE Transactions on Automatic Control, 56(10):2456–2461, Oct 2011. ISSN 0018-9286.
doi: 10.1109/tac.2011.2164036.
M. Mazo Jr. and M. Cao. Decentralized event-triggered control with asynchronous updates. In
Proceedings of the 50th Decision and Control and European Control Conference, pages 2547–
2552, Orlando, FL, USA, Dec. 2011.
M. Mazo Jr. and D. V. Dimarogonas. On self-triggered control for linear systems. In Proceedings
of the American Control Conference, pages 3371–3376, Baltimore, MD, USA, Jun. 2010.
Warren S McCulloch and Walter Pitts. A logical calculus of the ideas immanent in nervous activity.
The Bulletin of Mathematical Biophysics, 5(4):115–133, 1943.
Shahab Mehraeen and S Jagannathan. Decentralized optimal control of a class of interconnected
nonlinear discrete-time systems by using online hamilton-jacobi-bellman formulation. IEEE
Transactions on Neural Networks, 22(11):1757–1769, 2011.
Xiangyu Meng and Tongwen Chen. Event-driven communication for sampled-data control systems.
In 2013 American Control Conference, pages 3002–3007. IEEE, 2013.
Anthony N Michel. Impulsive and hybrid dynamcial systems: Stability, dissipativity and control
(wm haddad et al.; 2008)[bookshelf]. IEEE Control Systems Magazine, 28(2):87–88, 2008.
Marvin Minsky and Seymour Papert. Perceptrons: An Introduction to Computational Geometry.
MIT Press, 1969.
Hamidreza Modares and Frank L Lewis. Linear quadratic tracking control of partially-unknown
continuous-time systems using reinforcement learning. IEEE Transactions on Automatic control,
59(11):3051–3056, 2014a.
Hamidreza Modares and Frank L Lewis. Optimal tracking control of nonlinear partially-unknown
constrained-input systems using integral reinforcement learning. Automatica, 50(7):1780–1792,
2014b.
Adam Molin and Sandra Hirche. On the optimality of certainty equivalence for event-triggered
control systems. IEEE Transactions on Automatic Control, 58(2):470–474, 2013.
Vignesh Narayanan and S Jagannathan. Distributed adaptive optimal regulation of uncertain large￾scale linear networked control systems using Q-learning. In Proceedings of IEEE Symposium
Series on Computational Intelligence, pages 587–592, 2015.
Vignesh Narayanan and S Jagannathan. Approximate optimal distributed control of uncertain non￾linear interconnected systems with event-sampled feedback. In Proceedings of IEEE 55th Con￾ference on Decision and Control (CDC), pages 5827–5832, 2016a.
Vignesh Narayanan and S Jagannathan. Distributed adaptive optimal regulation of uncertain large￾scale interconnected systems using hybrid q-learning approach. IET Control Theory & Applica￾tions, 10(12):1448–1457, 2016b.
Vignesh Narayanan and S Jagannathan. Event-sampled adaptive neural network control of robot
manipulators. In Neural Networks (IJCNN), 2016 International Joint Conference on, pages 4941–
4946. IEEE, 2016c.
Vignesh Narayanan and S Jagannathan. Event-triggered distributed control of nonlinear intercon￾nected systems using online reinforcement learning with exploration. IEEE transactions on cy￾bernetics, 48(9):2510–2519, 2017.
Vignesh Narayanan, S Jagannathan, and Kannan Ramkumar. Event-sampled output feedback con￾trol of robot manipulators using neural networks. IEEE transactions on neural networks and
learning systems, 30(6):1651–1658, 2018a.
Vignesh Narayanan, Avimanyu Sahoo, and S Jagannathan. Optimal event-triggered control of non￾linear systems: A min-max approach. In 2018 Annual American Control Conference (ACC),BIBLIOGRAPHY 325
pages 3441–3446. IEEE, 2018b.
Vignesh Narayanan, Avimanyu Sahoo, S Jagannathan, and Koshy George. Approximate opti￾mal distributed control of nonlinear interconnected systems using event-triggered nonzero-sum
games. IEEE Transactions on Neural Networks and Learning Systems, 30(5):1512–1522, 2018c.
Kumpati S Narendra and Anuradha M Annaswamy. Stable adaptive systems. Courier Corporation,
2012.
Kumpati S Narendra and Snehasis Mukhopadhyay. To communicate or not to communicate: A
decision-theoretic approach to decentralized adaptive control. In Proceedings of American Con￾trol Conference (ACC), pages 6369–6376. IEEE, 2010.
Kumpati S Narendra and Kannan Parthasarathy. Identification and control of dynamical systems
using neural networks. IEEE Transactions on Neural Networks, 1(1):4–27, 1990.
K. Ogata. Discrete Time Control Systems. Prentice Hall, Englewood Cliffs, NJ, 2011.
Yongping Pan, Yiqi Liu, Bin Xu, and Haoyong Yu. Hybrid feedback feedforward: An efficient
design of adaptive neural network control. Neural Networks, 76:122–134, 2016.
Jaeson Park and I. W. Sandberg. Universal approximation using radial-basis-function networks.
Neural Computation, 3(2):246–257, 1991.
Alexander Pekarovskiy, Thomas Nierhoff, Sandra Hirche, and Martin Buss. Dynamically consistent
online adaptation of fast motions for robotic manipulators. IEEE Transactions on Robotics, 34
(1):166–182, 2018.
G. Pekir and A. Shiryaev. Optimal stopping and free-boundary problems. Lectures in Mathematica.
Birkhauser Verlag Press, Basel, Switzerland, 2006.
Chen Peng, Yang Song, Xiang Peng Xie, Min Zhao, and Mei-Rui Fei. Event-triggered output
tracking control for wireless networked control systems with communication delays and data
dropouts. IET Control Theory & Applications, 10(17):2195–2203, 2016.
Pierre Peretto. An Introduction to the Modeling of Neural Networks. Cambridge University Press,
1992.
Thanh Ngoc Pham, Hieu Trinh, and Amanullah Maung Than Oo. Distributed control of hvdc
links for primary frequency control of time-delay power systems. IEEE Transactions on Power
Systems, 34(2):1301–1314, 2019. doi: 10.1109/TPWRS.2018.2869984.
A. M. Phillips and M. Tomizuka. Multi-rate estimation and control under time-varying data sam￾pling with applications to information storage devices. In Proceedings of the American Control
Conference, pages 4152–4155, Seattle, WA, USA, Jun. 1995.
L. S. Pontryagin, V. G. Boltyanskii, R. V. Gamkrelidze, and E. F. Mishchenko. The Mathematical
Theory of Optimal Processes. Interscience Publishers, 1962.
Romain Postoyan, Adolfo Anta, Dragan Nesiˇ c, and Paulo Tabuada. A unifying lyapunov-based ´
framework for the event-triggered control of nonlinear systems. In 2011 50th IEEE conference
on decision and control and European control conference, pages 2559–2564. IEEE, 2011.
Romain Postoyan, Marcos Cesar Bragagnolo, Ernest Galbrun, Jamal Daafouz, Dragan Nesiˇ c, and ´
Eugenio B Castelan. Event-triggered tracking control of unicycle mobile robots. ˆ Automatica, 52:
302–308, 2015.
Danil V Prokhorov and Donald C Wunsch. Adaptive critic designs. IEEE transactions on Neural
Networks, 8(5):997–1007, 1997.
Danil V Prokhorov, Roberto A Santiago, and Donald C Wunsch. Adaptive critic designs: A case
study for neurocontrol. Neural Networks, 8(9):1367–1372, 1995.
M. Rabi and J. S. Baras. Level-triggered control of scalar linear system. In Proceedings of the
Mediterranean Conference on Control and Automation, pages 1–6, Athens, Greece, Jul. 2007.
M. Rabi, K. H. Johansson, and M. Johansson. Optimal stopping for event-triggered sensing and
actuation. In Proceedings of the 47th IEEE Conference on Decision and Control, pages 3607–
3612, Cancun, Mexico, Dec. 2008.
Frank P. Ramsey. A mathematical theory of saving. The Economic Journal, 38(152):543–559, 1928.326 BIBLIOGRAPHY
Frank Rosenblatt. The perceptron: a probabilistic model for information storage and organization
in the brain. Psychological Review, 65(6):386, 1958.
Halsey Lawrence Royden and Patrick Fitzpatrick. Real analysis, volume 2. Macmillan New York,
1968.
Walter Rudin. Principles of mathematical analysis. New York: McGraw-Hill, 1953.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning representations by
back-propagating errors. Nature, 323(6088):533–536, 1986.
P. M. Sadegh. Functional link net: A nonlinear polynomial neural network. Journal of Intelligent &
Robotic Systems, 7(4):403–423, 1993.
A. Sahoo, H. Xu, and S. Jagannathan. Neural network-based adaptive event-triggered control of
nonlinear continuous-time systems. In 2013 IEEE International Symposium on Intelligent Con￾trol (ISIC), pages 35–40, Aug 2013a. doi: 10.1109/ISIC.2013.6658613.
A. Sahoo, H. Xu, and S. Jagannathan. Neural network-based event-triggered state feedback control
of nonlinear continuous-time systems. IEEE Transactions on Neural Networks and Learning
Systems, 27(3):497–509, March 2016. ISSN 2162-237X. doi: 10.1109/TNNLS.2015.2416259.
Avimanyu Sahoo. Event sampled optimal adaptive regulation of linear and a class of nonlinear
systems. PhD thesis, Missouri University of Science and Technology, 2015.
Avimanyu Sahoo and S Jagannathan. Event-triggered optimal regulation of uncertain linear discrete￾time systems by using q-learning scheme. In 53rd IEEE Conference on Decision and Control,
pages 1233–1238. IEEE, 2014.
Avimanyu Sahoo and Sarangapani Jagannathan. Stochastic optimal regulation of nonlinear net￾worked control systems by using event-driven adaptive dynamic programming. IEEE transac￾tions on cybernetics, 47(2):425–438, 2016.
Avimanyu Sahoo and Vignesh Narayanan. Optimization of sampling intervals for tracking control
of nonlinear systems: A game theoretic approach. Neural Networks, 114:78–90, 2019.
Avimanyu Sahoo, Hao Xu, and S Jagannathan. Adaptive event-triggered control of a uncertain
linear discrete time system using measured input and output data. In 2013 American Control
Conference, pages 5672–5677. IEEE, 2013b.
Avimanyu Sahoo, Hao Xu, and S Jagannathan. Neural network-based event-triggered state feed￾back control of nonlinear continuous-time systems. IEEE Transactions on Neural Networks and
Learning Systems, 27(3):497–509, 2015.
Avimanyu Sahoo, Vignesh Narayanan, and S Jagannathan. Optimal sampling and regulation of
uncertain interconnected linear continuous time systems. In Proceedings of IEEE Symposium
Series on Computational Intelligence (SSCI), pages 1–6, 2017a.
Avimanyu Sahoo, Hao Xu, and S Jagannathan. Approximate optimal control of affine nonlinear
continuous-time systems using event-sampled neurodynamic programming. IEEE Transactions
on Neural Networks and Learning Systems, 28(3):639–652, 2017b.
Avimanyu Sahoo, Vignesh Narayanan, and S Jagannathan. A min–max approach to event-and self￾triggered sampling and regulation of linear systems. IEEE Transactions on Industrial Electronics,
66(7):5433–5440, 2018.
Robert M Sanner and Jean-Jacques E Slotine. Gaussian networks for direct adaptive control. In
1991 American control conference, pages 2153–2159. IEEE, 1991.
Jagannathan Sarangapani and Hao Xu. Optimal Networked Control Systems with MATLAB. CRC
Press, 2018.
Shankar Sastry and Marc Bodson. Adaptive control: stability, convergence and robustness. Courier
Corporation, 2011.
Michael Sfakiotakis, Asimina Kazakidi, Theodoros Evdaimon, Avgousta Chatzidaki, and Dimitris P
Tsakiris. Multi-arm robotic swimmer actuated by antagonistic sma springs. In Intelligent Robots
and Systems (IROS), 2015 IEEE/RSJ International Conference on, pages 1540–1545. IEEE, 2015.
Hu Shousong and Zhu Qixin. Stochastic optimal control and analysis of stability of networkedBIBLIOGRAPHY 327
control systems with long delay. Automatica, 39(11):1877–1884, 2003.
Jennie Si, Andrew G Barto, Warren B Powell, and Don Wunsch. Handbook of learning and approx￾imate dynamic programming, volume 2. John Wiley & Sons, 2004.
Dragoslav D Siljak. Decentralized control of complex systems. Courier Corporation, 2011.
Dragoslav D Siljak and AI Zecevic. Control of large-scale systems: Beyond decentralized feedback.
Annual Reviews in Control, 29(2):169–179, 2005.
P. K. Simpson. Artificial neural systems: A new tool for spatial analysis. Landscape and Urban
Planning, 21:37–51, 1992.
Ruizhuo Song, Frank L Lewis, and Qinglai Wei. Off-policy integral reinforcement learning method
to solve nonlinear continuous-time multiplayer nonzero-sum games. IEEE Transactions on Neu￾ral Networks and Learning Systems, 28(3):704–713, 2017.
Yan Song and Xiaosheng Fang. Distributed model predictive control for polytopic uncertain systems
with randomly occurring actuator saturation and packet loss. IET Control Theory & Applications,
8(5):297–310, 2014.
E. D. Sontag. Input to state stability: basic concepts and results. In Nonlinear and Optimal Control
Theory, pages 163–220. Springer, Berlin/Heidelberg, 2008.
J. T. Spooner and K. M. Passino. Decentralized adaptive control of nonlinear systems using radial
basis neural networks. IEEE Transactions on Automatic Control, 44(11):2050–2057, Nov 1999.
ISSN 0018-9286. doi: 10.1109/9.802914.
Vineet Srivastava and Mehul Motani. Cross-layer design: a survey and the road ahead. IEEE
communications magazine, 43(12):112–119, 2005.
Alan Wilbor Starr and Yu-Chi Ho. Nonzero-sum differential games. Journal of Optimization Theory
and Applications, 3(3):184–206, 1969.
C. Stocker and J. Lunze. Event-based control of nonlinear systems: An input-output linearization
approach. In Proceedings of the 50th Decision and Control and European Control Conference,
pages 2541–2546, Orlando, FL, USA, Dec. 2011.
Gilbert Strang. Introduction to linear algebra. SIAM, 2022.
Dirk J. Struik. A Source Book in Mathematics, 1200-1800. Source Books in the History of the
Sciences. Harvard University Press, 1986.
Christian Stocker and Jan Lunze. Event-based control of nonlinear systems: An input-output lin- ¨
earization approach. In 2011 50th IEEE Conference on Decision and Control and European
Control Conference, pages 2541–2546, 2011. doi: 10.1109/CDC.2011.6160526.
Changyin Sun, Hejia Gao, Wei He, and Yao Yu. Fuzzy neural network control of a flexible robotic
manipulator using assumed mode method. IEEE Transactions on Neural Networks and Learning
Systems, 2018.
Mingxuan Sun, Shuzhi Sam Ge, and Iven MY Mareels. Adaptive repetitive learning control of
robotic manipulators without the requirement for initial repositioning. IEEE Transactions on
Robotics, 22(3):563–568, 2006.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks.
Advances in Neural Information Processing Systems, 27:3104–3112, 2014.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press Cam￾bridge, 1998.
Nathan Szanto. Event-sampled direct adaptive neural network control of uncertain strict-feedback
system with application to quadrotor unmanned aerial vehicle. Masters Theses. 7616., 2016.
Nathan Szanto, Vignesh Narayanan, and S Jagannathan. Event-sampled direct adaptive nn output￾and state-feedback control of uncertain strict-feedback system. IEEE transactions on neural
networks and learning systems, 29(5):1850–1863, 2017a.
Nathan Szanto, Vignesh Narayanan, and S Jagannathan. Event-sampled control of quadrotor un￾manned aerial vehicle using neural networks. In 2017 American Control Conference (ACC),
pages 2956–2961. IEEE, 2017b.328 BIBLIOGRAPHY
T. Soderstr ¨ om and P. Stoica. ¨ System Identification. Prentice-Hall, 1989.
Paulo Tabuada. Event-triggered real-time scheduling of stabilizing control tasks. IEEE Transactions
on Automatic Control, 52(9):1680–1685, 2007.
Pavankumar Tallapragada and Nikhil Chopra. Event-triggered dynamic output feedback control
for lti systems. In 2012 IEEE 51st IEEE Conference on Decision and Control (CDC), pages
6597–6602. IEEE, 2012.
Pavankumar Tallapragada and Nikhil Chopra. On event triggered tracking for nonlinear systems.
IEEE Transactions on Automatic Control, 58(9):2343–2348, Sept 2013. ISSN 0018-9286. doi:
10.1109/tac.2013.2251794.
Pavankumar Tallapragada and Nikhil Chopra. Decentralized event-triggering for control of nonlin￾ear systems. IEEE Transactions on Automatic Control, 59(12):3312–3324, 2014.
Pavankumar Tallapragada and Jorge Cortes. Event-triggered stabilization of linear systems under ´
bounded bit rates. IEEE Transactions on Automatic Control, 61(6):1575–1589, 2015.
Guosong Tian, Yu-Chu Tian, and Colin Fidge. Performance analysis of ieee 802.11 dcf based wncs
networks. In IEEE Local Computer Network Conference, pages 496–503. IEEE, 2010.
Kyriakos G Vamvoudakis and Frank L Lewis. Multi-player non-zero-sum games: Online adaptive
learning solution of coupled hamilton–jacobi equations. Automatica, 47(8):1556–1569, 2011.
Kyriakos G Vamvoudakis, Hamidreza Modares, Bahare Kiumarsi, and Frank L Lewis. Game
theory-based control system algorithms with real-time reinforcement learning: how to solve mul￾tiplayer games online. IEEE Control Systems, 37(1):33–52, 2017a.
Kyriakos G Vamvoudakis, Arman Mojoodi, and Henrique Ferraz. Event-triggered optimal tracking
control of nonlinear systems. International Journal of Robust and Nonlinear Control, 27(4):
598–619, 2017b.
Aswin N Venkat, Ian A Hiskens, James B Rawlings, and Stephen J Wright. Distributed mpc
strategies with application to power system automatic generation control. IEEE transac￾tions on control systems technology, 16(6):1192–1206, Nov 2008. ISSN 1063-6536. doi:
10.1109/tcst.2008.919414.
N. Vignesh and S. Jagannathan. Distributed event-sampled approximate optimal control of intercon￾nected affine nonlinear continuous-time systems. In 2016 American Control Conference (ACC),
pages 3044–3049, July 2016. doi: 10.1109/acc.2016.7525383.
Antonio Visioli, Giacomo Ziliani, and Giovanni Legnani. Iterative-learning hybrid force/velocity
control for contour tracking. IEEE Transactions on Robotics, 26(2):388–393, 2010.
Holger Voos. Nonlinear state-dependent riccati equation control of a quadrotor uav. In 2006 IEEE
Conference on Computer Aided Control System Design, 2006 IEEE International Conference on
Control Applications, 2006 IEEE International Symposium on Intelligent Control, pages 2547–
2552. IEEE, 2006.
D. Vrabie, O. Pastravanu, M. Abu-Khalaf, and F. L. Lewis. Adaptive optimal control for continuous￾time linear systems based on policy iteration. Automatica, 45(2):477–484, 2009a.
Draguna Vrabie, Kyriakos Vamvoudakis, and Frank Lewis. Adaptive optimal controllers based
on generalized policy iteration in a continuous-time framework. In 2009 17th Mediterranean
Conference on Control and Automation, pages 1402–1409. IEEE, 2009b.
Gregory C Walsh, Hong Ye, and Linda G Bushnell. Stability analysis of networked control systems.
Control Systems Technology, IEEE Transactions on, 10(3):438–446, 2002.
D. Wang, D. Liu, Q. Zhang, and D. Zhao. Data-based adaptive critic designs for nonlinear robust
optimal control with uncertain dynamics. IEEE Transactions on Systems, Man, and Cybernetics:
Systems, Pp(99):1–12, 2015. ISSN 2168-2216. doi: 10.1109/tsmc.2015.2492941.
Ding Wang, Derong Liu, Hongliang Li, and Hongwen Ma. Neural-network-based robust optimal
control design for a class of uncertain nonlinear systems via adaptive dynamic programming.
Information Sciences, 282:167–179, 2014.
Ding Wang, Haibo He, and Derong Liu. Improving the critic learning for event-based nonlinearBIBLIOGRAPHY 329
hinfty control design. IEEE transactions on cybernetics, 47(10):3417–3428, 2017.
Fei-Yue Wang, N. Jin, D. Liu, and Q. Wei. Adaptive dynamic programming for finite-horizon
optimal control of discrete-time nonlinear systems with ε-error bound. IEEE Transactions on
Neural Networks, 22(1):24–36, Jan. 2011.
S. Wang and E.J. Davison. On the stabilization of decentralized control systems. IEEE Transactions
on Automatic Control, 18(5):473–478, 1973.
Xiaofeng Wang and Naira Hovakimyan. L1 adaptive control of event-triggered networked systems.
In Proceedings of the 2010 American Control Conference, pages 2458–2463. IEEE, 2010.
Xiaofeng Wang and Michael Lemmon. On event design in event-triggered feedback systems. Au￾tomatica, 47(10):2319–2322, 2011a. ISSN 0005-1098.
Xiaofeng Wang and Michael D Lemmon. Event design in event-triggered feedback control systems.
In Decision and Control, 2008. CDC 2008. 47th IEEE Conference on, pages 2105–2110. Ieee,
2008.
Xiaofeng Wang and Michael D Lemmon. Event-triggering in distributed networked systems with
data dropouts and delays. In International Workshop on Hybrid Systems: Computation and Con￾trol, pages 366–380. Springer, 2009a.
Xiaofeng Wang and Michael D Lemmon. Self-triggered feedback control systems with finite-gain
L2 stability. IEEE Transactions on Automatic Control, 54(3):452–467, 2009b.
Xiaofeng Wang and Michael D Lemmon. Event-triggering in distributed networked control systems.
IEEE Transactions on Automatic Control, 56(3):586–601, 2011b.
Xiaoli Wang, Yiguang Hong, Jie Huang, and Zhong-Ping Jiang. A distributed control approach to a
robust output regulation problem for multi-agent linear systems. IEEE Transactions on Automatic
control, 55(12):2891–2895, 2010.
Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. King’s College, Cam￾bridge United Kingdom, 1989.
Xinjiang Wei and Lei Guo. Composite disturbance-observer-based control and h∞ control for com￾plex continuous models. International Journal of Robust and Nonlinear Control: IFAC-Affiliated
Journal, 20(1):106–118, 2010.
Robert Weinstock. Calculus of Variations: With Applications to Physics and Engineering. Dover
Publications, 1974.
P. J. Werbos. A menu of designs for reinforcement learning over time. J. Neural Networks Contr.,
3:835–846, 1983.
P. J. Werbos. Optimization methods for brain-like intelligent control. In Decision and Control,
1995., Proceedings of the 34th IEEE Conference on, volume 1, pages 579–584 vol.1, Dec 1995.
doi: 10.1109/cdc.1995.478957.
Paul Werbos. Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences.
PhD thesis, Harvard University, 1974.
Paul J Werbos. Backpropagation through time: What it does and how to do it. Proceedings of the
IEEE, 78(10):1550–1560, 1989.
Paul J Werbos. Approximate Dynamic Programming for Real-Time Control and Neural Modeling,
pages 493–525. Van Nostrand Reinhold, 1991a.
Paul J Werbos. Adp: Goals, opportunities and principles. IEEE Transactions on Systems, Man, and
Cybernetics, 22(3):346–357, 1992.
P.J. Werbos. An overview of neural networks for control. IEEE Control Systems Magazine, 11(1):
40–41, 1991b. doi: 10.1109/37.103352.
Feng Xia, Xiangjie Kong, and Zhenzhen Xu. Cyber-physical control over wireless sensor and
actuator networks with packet loss. Wireless networking based control, pages 85–102, 2011a.
Feng Xia, Alexey Vinel, Ruixia Gao, Linqiang Wang, and Tie Qiu. Evaluating ieee 802.15. 4 for
cyber-physical systems. EURASIP Journal on Wireless Communications and Networking, 2011:
1–14, 2011b.330 BIBLIOGRAPHY
B. Xu, C. Yang, and Z. Shi. Reinforcement learning output feedback nn control using deterministic
learning technique. IEEE Transactions on Neural Networks and Learning Systems, 25(3):635–
641, March 2014a. ISSN 2162-237x. doi: 10.1109/tnnls.2013.2292704.
H. Xu and S. Jagannathan. Stochastic optimal controller design for uncertain nonlinear net￾worked control system via neuro dynamic programming. IEEE Transactions on Neural
Networks and Learning Systems, 24(3):471–484, March 2013. ISSN 2162-237x. doi:
10.1109/tnnls.2012.2234133.
Hao Xu. Stochastic optimal adaptive controller and communication protocol design for networked
control systems. ProQuest LLC, 2012.
Hao Xu and S Jagannathan. A cross layer approach to the novel distributed scheduling protocol and
event-triggered controller design for cyber physical systems. In 37th Annual IEEE Conference
on Local Computer Networks, pages 232–235, 2012. doi: 10.1109/LCN.2012.6423616.
Hao Xu, S Jagannathan, and Frank L Lewis. Stochastic optimal control of unknown linear net￾worked control system in the presence of random delays and packet losses. Automatica, 48(6):
1017–1030, 2012.
Hao Xu, S Jagannathan, and FL Lewis. Stochastic optimal design for unknown linear discrete-time
system zero-sum games in input-output form under communication constraints. Asian Journal of
Control, 16(5):1263–1276, 2014b.
X. Yang, H. He, and D. Liu. Event-triggered optimal neuro-controller design with reinforcement
learning for unknown nonlinear systems. IEEE Transactions on Systems, Man, and Cybernetics:
Systems, pages 1–13, 2017. ISSN 2168-2216. doi: 10.1109/TSMC.2017.2774602.
Xiong Yang and Haibo He. Adaptive critic learning and experience replay for decentralized event￾triggered control of nonlinear interconnected systems. IEEE Transactions on Systems, Man, and
Cybernetics: Systems, 50(11):4043–4055, 2020. doi: 10.1109/TSMC.2019.2898370.
William Henry Young. On classes of summable functions and their fourier series. Proceedings
of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical
Character, 87(594):225–229, 1912.
Yao-Chi Yu, Vignesh Narayanan, and Jr-Shin Li. Moment-based reinforcement learning for ensem￾ble control. IEEE Transactions on Neural Networks and Learning Systems, 2023.
Huaguang Zhang, Qinglai Wei, and Yanhong Luo. A novel infinite-time optimal tracking control
scheme for a class of discrete-time nonlinear systems via the greedy hdp iteration algorithm. IEEE
Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 38(4):937–942, 2008a.
Huaguang Zhang, Qinglai Wei, and Yanhong Luo. A novel infinite-time optimal tracking control
scheme for a class of discrete-time nonlinear systems via the greedy hdp iteration algorithm. IEEE
Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 38(4):937–942, 2008b.
Huaguang Zhang, Yanhong Luo, and Derong Liu. Neural-network-based near-optimal control for
a class of discrete-time affine nonlinear systems with control constraints. IEEE Transactions on
Neural Networks, 20(9):1490–1503, 2009.
Huaguang Zhang, Lili Cui, Xin Zhang, and Yanhong Luo. Data-driven robust approximate optimal
tracking control for unknown general nonlinear systems using adaptive dynamic programming
method. IEEE Transactions on Neural Networks, 22(12):2226–2236, 2011a.
Huaguang Zhang, Lili Cui, Xin Zhang, and Yanhong Luo. Data-driven robust approximate optimal
tracking control for unknown general nonlinear systems using adaptive dynamic programming
method. IEEE Transactions on Neural Networks, 22(12):2226–2236, 2011b.
Wei Zhang, Michael S Branicky, and Stephen M Phillips. Stability of networked control systems.
IEEE control systems magazine, 21(1):84–99, 2001.
Yi Zheng, Shaoyuan Li, and Hai Qiu. Networked coordination-based distributed model predictive
control for large-scale system. IEEE Transactions on Control Systems Technology, 21(3):991–
998, 2012.
X. Zhong and H. He. An event-triggered adp control approach for continuous-time system withBIBLIOGRAPHY 331
unknown internal states. IEEE Transactions on Cybernetics, Pp(99):1–12, 2016. ISSN 2168-
2267. doi: 10.1109/tcyb.2016.2523878.
Xiangnan Zhong, Zhen Ni, Haibo He, Xin Xu, and Dongbin Zhao. Event-triggered reinforce￾ment learning approach for unknown nonlinear continuous-time system. In 2014 Interna￾tional Joint Conference on Neural Networks (IJCNN), pages 3677–3684. IEEE, July 2014. doi:
10.1109/ijcnn.2014.6889787.
Xiaojun Zhou, Chaojie Li, Tingwen Huang, and Mingqing Xiao. Fast gradient-based distributed
optimisation approach for model predictive control and application in four-tank benchmark. IET
Control Theory & Applications, 9(10):1579–1586, 2015.
