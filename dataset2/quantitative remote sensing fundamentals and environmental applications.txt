Quantitative Remote Sensing
This book provides comprehensive and in-depth explanations of all topics
related to quantitative remote sensing and its applications in terrestrial,
biospheric, hydrospheric, and atmospheric studies. It elucidates how to
retrieve quantitative information on a wide range of environmental
parameters from various remote sensing data at the highest accuracy
possible and expounds how different aspects of the target of remote sensing
can be quantified using diverse analytical methods at various levels of
accuracy. Written in an easy-to-follow language, logically organized, and
with step-by-step examples, the book assists readers to deepen their
understanding of the theory and cutting-edge research on quantitative
remote sensing.
Features
Explains how to retrieve quantitative information on a wide range of
environmental parameters from various tailored remote sensing data
at the highest accuracy possible.
Manifests the author’s decades of teaching and research in
quantitative remote sensing and approaches the subject from both
theoretical and pragmatic perspectives, informed by the latest
research outcomes.
Includes practical and real-life examples to illustrate how the
quantitative information on a target can be retrieved from a given
type of remote sensing data.
Focuses on the latest developments in the field of quantitative remote
sensing.Introduces sufficient mathematical concepts to reveal how remotely
sensed data are converted to quantitative information while providing
quality assurance of the retrieved results.
This is a suitable textbook for upper-level undergraduate or postgraduate
students and serves as a handy and valuable reference for professionals
working in monitoring the environment. By reading this book, readers can
gain a sound understanding of how to retrieve quantitative information on
the environment from diverse remote sensing data using the most
appropriate cutting-edge methods and software.Quantitative Remote Sensing
Fundamentals and Environmental Applications
Jay GaoDesigned cover image: © Shi Y, J Gao, G Brierley, X Li, GLW Perry, and T
Xu (2023), Improving the accuracy of models to map alpine grassland
above-ground biomass using Google earth engine. Grass and Forage Sci
78(2): 237-253. doi: 10.1111/gfs.12607, CC BY 4.0 Deed,
https://creativecommons.org/licenses/by/4.0/
First edition published 2025by CRC Press
2385 NW Executive Center Drive, Suite 320, Boca Raton FL 33431and by CRC Press
4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN
CRC Press is an imprint of Taylor & Francis Group, LLC
© 2025 Jay Gao
Reasonable efforts have been made to publish reliable data and information,
but the author and publisher cannot assume responsibility for the validity of
all materials or the consequences of their use. The authors and publishers
have attempted to trace the copyright holders of all material reproduced in
this publication and apologize to copyright holders if permission to publish
in this form has not been obtained. If any copyright material has not been
acknowledged please write and let us know so we may rectify in any future
reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be
reprinted, reproduced, transmitted, or utilized in any form by any electronic,
mechanical, or other means, now known or hereafter invented, including
photocopying, microfilming, and recording, or in any information storage
or retrieval system, without written permission from the publishers.
For permission to photocopy or use material electronically from this work,
access www.copyright.com or contact the Copyright Clearance Center, Inc.
(CCC), 222 Rosewood Drive, Danvers, MA 01923, 978-750-8400. For
works that are not available on CCC please contact
mpkbookspermissions@tandf.co.uk
Trademark notice: Product or corporate names may be trademarks or
registered trademarks and are used only for identification and explanation
without intent to infringe.
Library of Congress Cataloging-in-Publication Data Names: Gao, Jay,
author.
Title: Quantitative remote sensing : fundamentals and environmental
applications / Jay Gao.Description: First edition. | Boca Raton, FL : CRC Press, 2025. | Includes
bibliographical references and index.
Identifiers: LCCN 2024022074 (print) | LCCN 2024022075 (ebook) | ISBN
9781032852874 (hardback) | ISBN 9781032852898 (paperback) | ISBN
9781003517504 (ebook) Subjects: LCSH: Environmental sciences–Remote
sensing. | Remote sensing–Data processing.
Classification: LCC GE45.R44 G36 2025 (print) | LCC GE45.R44 (ebook) |
DDC 621.36/78–dc23/eng/20240808
LC record available at https://lccn.loc.gov/2024022074
LC ebook record available at https://lccn.loc.gov/2024022075
ISBN: 978-1-032-85287-4 (hbk) ISBN: 978-1-032-85289-8 (pbk) ISBN:
978-1-003-51750-4 (ebk) DOI: 10.1201/9781003517504Typeset in Times New Romanby Newgen Publishing UKDedication
I would like to dedicate this book to my brothers and sister who
have always supported me wholeheartedly throughout my
academic career and welcomed me with open arms to their homes
as a true family member. I will always cherish the wonderful times I
spent together with them in their homes, which felt like a second
home away from home for me.Part I
Chapter 1
1.1
1.1.1
1.1.2
1.1.3
1.1.4
1.2
1.2.1
1.2.2
1.2.3
1.2.4
1.2.5
1.3
1.3.1
1.3.2
1.4
Contents
Preface
Acknowledgments
About the Author
List of Acronyms
Fundamentals
Introduction
Quantitative Remote Sensing
Static Versus Dynamic Quantification
Target of Quantification – The
Environment
Nature of Quantification
Requirements of Quantification
Field Data Collection
In situ Sampling
Collection of Terrestrial Data
Collection of Biophysical Data
Water Data
Second-hand Ancillary Data
Common Predictor Variables
From Drone Images
From Space-borne Bands
Accuracy of Quantified Results1.4.1
1.4.2
1.5
1.5.1
1.5.2
1.5.3
1.6
Chapter 2
2.1
2.1.1
2.1.2
2.1.3
2.2
2.2.1
2.2.2
2.2.3
2.2.4
2.3
2.3.1
2.3.2
2.3.3
2.3.4
2.4
2.4.1
2.4.2
2.4.3
2.4.4
Validation and Cross-validation
Accuracy Expressions
Challenges Facing Quantification
Limitations of Imagery Data
Retrospective Quantification
Data Mismatch
Organization of this Book
Sensing Platforms and Data
Sensing Platforms
Drones
Aircraft
Satellites
Earth Observation Satellite Data
Landsat 8/9 OLI
Sentinel-2 and -3
RapidEye
MODIS
Atmospheric Satellite Data
VIIRS
Himawari 8
TMI
MISR
Meteorological and Oceanographic Satellites
ASCAT
MERIS
Ocean and Land Color Instrument
Geostationary Ocean Color Imager2.5
2.5.1
2.5.2
2.6
2.6.1
2.6.2
2.6.3
Chapter 3
3.1
3.1.1
3.1.2
3.1.3
3.1.4
3.2
3.2.1
3.2.2
3.2.3
3.3
3.3.1
3.3.2
3.3.3
3.4
3.4.1
3.4.2
3.5
Hyperspectral Data
Airborne Data
Space-borne Sensors
Active Sensing Data
TerraSAR
Sentinel-1 SAR
LiDAR Data
Radiometric Correction
Radiation Interactions
With Atmosphere
Spectral Behavior of Typical
Components
With Terrestrial Surface
With Water
Physical Models of Correction
6S (Vector)
SREM
FLAASH-C/DHL
Semi-analytical Methods
Simplified Method for Atmospheric
Corrections
VNIR Method
VHR Imagery Correction
Image-based Methods
Dark Object Subtraction
Dense Dark Vegetation Algorithm
Correction over Complex Waters3.5.1
3.5.2
3.5.3
3.6
3.6.1
3.6.2
3.6.3
3.7
3.7.1
3.7.2
3.8
3.8.1
3.8.2
Chapter 4
4.1
4.1.1
4.1.2
4.1.3
4.2
4.2.1
4.2.2
4.2.3
4.2.4
4.3
4.3.1
4.3.2
4.3.3
Iterative and Optimized Methods
CAAS
Modified SWIR Method
Computing Platforms
MODTRAN
SeaDAS
ESA SNAP
Comparative Assessment
ACIX
Individual Processors
Topographic Correction
FORCE
ATCOR
Analytical Methods
Non-parametric Methods
Regression Models
(Stepwise) Multiple Linear Regression
PLSR
Non-linear Non-parametric Methods
Decision Tree
Gradient-boosted Decision Trees
Random Forest
Artificial Neural Networks
Kernel-based Methods
Support Vector Machine
Support Vector Regression
Kernel Ridge Regression4.3.4
4.3.5
4.4
4.4.1
4.4.2
4.4.3
Part II
Chapter 5
5.1
5.1.1
5.1.2
5.1.3
5.1.4
5.1.5
5.1.6
5.2
5.2.1
5.2.2
5.2.3
5.2.4
5.3
5.3.1
5.3.2
5.3.3
5.3.4
5.3.5
Gaussian Processes Regression
Bayesian Networks
Miscellaneous Methods
SfM Photogrammetry
Movement Detection Methods
Continuum Removal
Environmental Applications
Quantification in the Terrestrial Sphere
Surface Physical Parameters
Spectral Behavior of Soil
Land Surface Temperature
Urban Heat Flux
Lava Temperature
Surface Albedo
Soil Moisture
Soil Biochemical Qualities
Soil Salinity
Soil Salt Content
Soil Organic Carbon
Soil Contaminants
Debris Thickness and Volume
Landslide Debris Thickness
Debris Volume
Sediment Budget
Volcanic Deposits
Snow Depth and Avalanche Volume5.4
5.4.1
5.4.2
5.4.3
Chapter 6
6.1
6.1.1
6.1.2
6.2
6.2.1
6.2.2
6.2.3
6.3
6.3.1
6.3.2
6.3.3
6.3.4
6.4
6.4.1
6.4.2
6.4.3
6.4.4
6.4.5
6.5
6.5.1
6.5.2
Surface Movement
Horizontal Movement
Vertical Movement
Ground Deformation
Quantification in the Biosphere
Vegetation Spectral Behavior and Indices
Spectral Signature of Vegetation
Affecting Factors
Physical and Semi-physical Models
Commonly Used Models
Hybrid Methods
Model Inversion Strategies
Biophysical Variables
Fractional Vegetation Cover and Tree
Volume
Tree and Crown Height
DBH
Wood Volume
Leaf Area Index
Model-based Methods
Parametric Estimation Models
Non-parametric Regression Methods
A Comparison
Global LAI Product
Chlorophyll Content
Useful Spectral Indices
Methods of Estimation6.6
6.6.1
6.6.2
6.6.3
6.7
6.7.1
6.7.2
6.7.3
6.8
6.8.1
6.8.2
6.8.3
6.8.4
6.9
6.9.1
6.9.2
6.9.3
Chapter 7
7.1
7.1.1
7.1.2
7.1.3
7.2
7.2.1
7.2.2
7.3
7.3.1
Bio-quality Variables
Net Primary Productivity
fPAR
Light Use Efficiency
Aboveground Biomass (Carbon)
Micro-scale Quantification
Catchment-scale Grassland Biomass
Meso-scale Forest Biomass (Carbon)
Crop Yield Estimation
General
Corn
Wheat
Rice
Wild Fire Parameters
Rate of Spread
Fire Intensity
Burn Intensity
Quantification in the Hydrosphere
Fundamentals
Spectral Signature of Water
Suitable Remote Sensing Data
Theoretical Grounding
Water Clarity and Bathymetry
Clarity and Secchi Disc Depth
Bathymetry
Water Surface Features
Sea Surface Temperature7.3.2
7.3.3
7.4
7.4.1
7.4.2
7.5
7.5.1
7.5.2
7.5.3
7.5.4
7.5.5
7.5.6
7.5.7
Chapter 8
8.1
8.2
8.2.1
8.2.2
8.2.3
8.2.4
8.2.5
8.2.6
8.2.7
8.3
Significant Wave Height and Period
Surface Flow Velocity
In-water Inorganic Parameters
Suspended Sediments
Sea Surface Salinity
In-water Biochemical Constituents
Chlorophyll Concentration
Retrieval Methods
Eutrophication
Algal Blooms and Biomass
Colored Dissolved Organic
Matter/Carbon
Phytoplankton Layer Depth
Phycocyanin (Cyanobacterial) Blooms
Atmospheric Quantification
Principle of Quantification
Useful Satellite Data
Total Ozone Mapping Spectrometer
Ozone Monitoring Instruments
Global Ozone Monitoring Experiment￾1 and -2
Earth Polychromatic Imaging Camera
and AIRS
SCIAMACHY
GOSAT
Tropospheric Monitoring Instrument
Meteorological Parameters8.3.1
8.3.2
8.4
8.4.1
8.4.2
8.5
8.5.1
8.5.2
8.6
8.6.1
8.6.2
8.6.3
8.6.4
8.6.5
8.6.6
Wind Speed
Precipitation Rate
Atmospheric Impurities
Aerosols
Aerosol Optical Thickness
Solid Particulates
PM2.5 and PM10
Dust Intensity
Gaseous Components
Ozone
Carbon Dioxide
Carbon Monoxide
Nitrogen Dioxide
Sulphur Dioxide
Methane Emissions
IndexPreface
It is very difficult to trace when exactly quantitative remote sensing came
into existence. When remote sensing as a discipline was developed in the
1970s, it was applied mostly for retrieving qualitative information, either
using manual method or digital data analysis. In my student years, I was
engaged with qualitative remote sensing research exclusively, using aerial
photographs and analogue satellite images. However, how to retrieve
quantitative information from remotely sensed data has always intrigued
me. Soon after starting my career as an academia, I found myself
handicapped by the lack of suitable books on the topic of quantitative
remote sensing. Supervision of my postgraduate student thesis research
landed me the opportunity of gaining an understanding of how to
quantitatively retrieve physical parameter values from aerial photographs in
the early 1990s. Back then it was extremely cumbersome and challenging to
undertake quantitative remote sensing analysis due to the lack of standard
software for atmospheric correction and the absence of powerful computing
packages and platforms. Indeed, the retrieval was a painstakingly slow and
awkward process involving working with several disparate computing
packages to visualize the quantitative outcome graphically.
This situation improved in subsequent years with the advent of advanced
digital image processing software packages that allowed rudimentary
radiometric calibration of satellite data, and digital mosaiking of multiple
images. So far scientists have attempted to retrieve quantitative information
on the target of sensing in widely ranging fields. These attempts have
proved fruitful owing to the advances in computing power and the
emergence of sophisticated computing packages. In the evolutionary
trajectory of quantitative remote sensing, two critical milestones stand out
and merit particular elaboration: the advent of LiDAR data and the
exponentially improved computing power, especially the widespreadavailability of machine learning packages. Dissimilar to imagery data that
may be processed to derive qualitative information, LiDAR data are
processed to unexceptionally to derive quantitative information on the
target that is supplementary to the mostly 2D quantitative information from
remote sensing images. Consequently, LiDAR data have opened the
floodgate of quantitative remote sensing and extended remote sensing
applications to widely ranging fields that were unimaginable prior to the
LiDAR era. Machine learning creates a computing environment in which
multiple co-variables can be considered and analyzed simultaneously in an
effort to increase the reliability of the retrieved quantitative information. It
significantly facilitates the undertaking of quantitative remote sensing by
non-remote sensing specialists. Owing to the powerful analytical capability,
those quantification tasks that cannot be accomplished in rudimentary
statistical analysis can be achieved at a much higher and reputable accuracy.
Powerful computing algorithms have also widened the application areas of
quantitative remote sensing to encompass the four spheres (terrestrial,
biological, hydrosphere, and atmosphere) of the natural environment that
are closely intertwined with human life.
As a discipline, quantitative remote sensing has developed
phenomenally and matured over the last two decades. The large volume of
research outcome generated warrants the necessity of writing a book on the
topic to systematically scrutinize the current state of the discipline. This
book aims to enlighten educators and professionals on the latest
development in the field and inform them how to make use of the latest
technologies and methods in their own work. Manifesting the culmination
of decades of my teaching and research in quantitative remote sensing, this
book is a timely addition to the existing body of literature on remote
sensing.
As a newly emerged fledgling discipline, quantitative remote sensing
lacks firm theoretical grounding. Neither exist standard procedures of
practice (e.g., lack of universal methods of data processing and results
validation). How quantitative remote sensing is practised is dictated by the
field in which the quantitative information is derived. However, there arestill some commonalities to rudimentary data preparation, general
components of data analysis, and quality assurance of the retrieved
outcome. For this reason, the book has been structured in two general parts,
fundamentals and practices, each comprising four chapters. After a general
introduction to the topic of quantitative remote sensing, this part provides
an overview of remotely sensed data that have found wide applications in
quantitative remote sensing. It is hoped that this chapter will equip the
reader with the knowledge of how to select the most appropriate types of
data for his or her own projects. Other topics covered in this part include
atmospheric calibration, a processing unique and essential to quantitative
remote sensing, and common analytical methods that have found
applications in quantitative remote sensing and that are instrumental to the
achievement of reasonable quantification outcomes. In general, this part
contains sufficient mathematical background to reveal how remotely sensed
data are converted to quantitative information while also providing quality
assurance of the retrieved results.
The practice part elucidates how the retrieval of diverse quantitative
information is implemented for different targets in four spheres from
diverse remote sensing data, both graphic and non-graphic. Since the
retrieval of physical parameter values varies widely with the sphere or the
target of quantification in the same sphere, I have decided to organize the
second part into four chapters, each devoted to the quantification in a
unique sphere of land, water, air, and biosphere. The content of each chapter
does not conform to a standard format. Neither does it follow a particular
logic except that it starts from simpler topics at the beginning to more
sophisticated and complex topics later on. Therefore, there is no expectation
that the reader must follow all the chapters sequentially as they are ordered,
or the sequence of each chapter, especially those chapters in the second
part. Since the retrieval in each sphere is so unique in the type of suitable
data used, the methods of data processing, and the models of quantification
used, it is unrealistic to expect a professional to be conversant with the
quantification in all four spheres. Chances are a few chapters may suffice.
Knowledgeable readers may jump to the relevant practice chapters mostrelevant to their field of interest directly after reading the first part. Since
acronyms are widely used in the quantitative remote sensing literature, this
book is no exception. Whenever an acronym is encountered the first time, it
is spelled out fully only once instead of in every chapter. Readers unfamiliar
with the acronyms are encouraged to consult the list of acronyms at the
beginning of the book.
This book aims to inform the reader on how to retrieve information on a
target of interest quantitatively from widely ranging remote sensing data
using different processing methods and models. Its flavor leans heavily
towards pragmatism, namely, how to produce the most reliable quantitative
information from the relevant remote sensing data using the best practice.
As such, it minimizes the theoretical exposition of the atmospheric radiative
transfer process. Instead, it emphasizes how to implement the quantitative
retrieval in a given computing environment. When multiple platforms are
available, the pros and cons of each computing platform are comparatively
evaluated. Wherever relevant, step-by-step examples are supplied to
illustrate how the retrieval is accomplished conceptually. Ample examples
are supplied to deepen the comprehension of the text and to facilitate
maximum reader engagement. Another unique feature of the book is the
lavish attention paid to retrieval accuracy, for the entire effort of
quantification is lost if the retrieved outcome cannot meet the practical
accuracy requirements. In order to understand how the accuracy may be
further improved, all the factors responsible for degrading the quality of the
retrieved information are enumerated and elaborated, with their contribution
to inaccuracy ranked and quantified whenever possible.
This book is written with the assumption that the reader has already
gained some fundamental understanding of remote sensing, such as digital
number and its relationship with the spectral reflectance curves, and the
accuracy (precision) of quantitative retrieval. It can be used as a textbook
for upper-level undergraduate and postgraduate courses, or a handy
reference. Additional teaching-related materials (e.g., lecturing PPTs and
lab assignments) can be obtained by browsing the publisher’s website
(WWW.CRCPRESS.COM). Accompanying this book is a lab manual withstep-by-step instructions for a few lab exercises. They can be accessed
online only for those instructors who will adopt the book for their
classroom teaching. Professionals working in a field related to the natural
environment will find the book useful in enlightening them on how to
derive the quantitative information of their interest from a vast array of
remotely sensed data using the latest computing methods and technologies
available. Through reading this book, the reader can hope to gain an in￾depth understanding and appreciation of the subject matter and cutting-edge
research on quantitative remote sensing that no other books can offer.
Jay Gao
April 2024, AucklandAcknowledgments
This book could not have been written without the assistance of numerous
parties that I would like to acknowledge here. The first and foremost party
is my former employer, the University of Auckland, who bestowed me with
an honorable lecturership position following my earlier-than-expected
retirement, during which this book was written. Under this arrangement, I
am privileged to access the university’s vast digital resources, from which
this book profited handsomely. The next parties I am deeply in debt to are
my former doctoral students whose PhD theses are widely cited, and whose
research outcomes are used as illustrations in the appropriate places; they
are Vincent Wang on undertaking the quantitative assessment of vegetation
carbon using integrated LiDAR and imagery data, and Yan Shi on
quantification of grassland above-ground biomass using both drone images
at the micro-scale and satellite imagery at the catchment scale. Also, Daniel
de le Torre on the estimation of rice yield using machine learning methods.
In particular, Yan also helped to produce high-quality illustrations, the front
cover image of this book. I would like to thank numerous authors who have
made their publications freely accessible to me, and whose graphics and
tables are re-used throughout this book to enhance its quality and ease the
comprehension of the text. Finally, I would like to thank the staff at Taylor
& Francis who guided me through the maze of getting published in this
highly complex process. In particular, I am grateful for the assistance and
immediate attention of Irma Britton and Chelsea Reeves who are always
reliable in answering my queries promptly.About the Author
Jay Gao is an internationally renowned expert and author in the area of
remote sensing, GIS, and spatial analysis. He completed his undergraduate
education at Wuhan Technical University of Surveying and Mapping,
specializing in photogrammetry and remote sensing. Upon graduation in
1984, he continued to pursue a postgraduate qualification in remote sensing
in the Geography Department at the University of Toronto. His doctoral
degree was obtained in Geography from the University of Georgia. Once he
submitted his PhD thesis, he joined the Department of Geography at the
University of Auckland as a lecturer in 1992. Over a career spanning nearly
three decades, he has conducted extensive research in applying remote
sensing and geo-spatial information methods to a wide range of fields,
including quantitative environmental and ecological monitoring, and
resources management. In particular, he has developed a keen interest in
retrieving quantitative information from a variety of remotely sensed data.
Either solely or jointly with his postgraduate students, he has carried out
numerous research projects to explore how different types of quantitative
information on the environment can be derived from various types of
remote sensing data. His research results have appeared in more than 200
international journals. In addition, he has (co-)authored several books,
including Digital Processing of Remotely Sensed Data, Fundamentals of
Spatial Analysis and Modelling, and Remote Sensing of Natural Hazards,
the last two being published by CRC Press.AAI
ABI
AERONET
AFAI
AGB(C)
AHI
AI
AIRS
AISA
ALOS
ALS
AMF
AMSR
ANFIS
ANN
AOD
AOP
AOT
ARVI
ASCAT
ASTER
ATCOR
Acronyms
absorbing aerosol index
algal bloom index
AErosol RObotic NETwork
alternative floating algae index
above-ground biomass (carbon)
Advanced Himawari Imager
aerosol index
Atmospheric Infrared Radiation Sounder
Airborne Imaging Spectrometer for Application
Advanced Land Observing Satellite
airborne laser scanning
air mass factor
Advanced Microwave Scanning Radiometer
adaptive neural fuzzy inference system
artificial neural network
aerosol optical depth
Apparent optical property
atmospheric optical thickness
atmospherically resistant vegetation index
Advanced Scatterometer
Advanced Spaceborne Thermal Emission and
Reflection Radiometer
atmospheric and topographic correctionAVIRIS-NG
BLH
BN
BRDF
BT
CAAS
CALIOP
CALIPSO
CASI
CBI
CC
CC
CCC
CD
CDOM
CFNN
CH
CHM
CHRIS
CI
CIB
CMVS
CNN
CSI
CSM
Airborne Visible InfraRed Imaging Spectrometer - Next
Generation
boundary layer height
Bayesian network
bidirectional reflectance distribution function
brightness temperature
complex atmospheric algorithm scheme
Cloud-Aerosol LiDAR with Orthogonal Polarization
Cloud-Aerosol LiDAR and Infrared Pathfinder Satellite
Observation
Compact Airborne Spectrographic Imager
composite burn index
canopy closure
chlorophyll content
canopy chlorophyll content
canopy diameter
colored (chromophoric) dissolved organic matter
cascade-forward neural network
canopy height
canopy height model
Compact High Resolution Imaging Spectrometer
chlorophyll index
column-integrated biomass
clustering for multi-view stereopsis
convolutional neural network
coherent scatterer InSAR
crop surface modelCTM
DAM
DBH
DBN
DDM
DDV
DEM
DGNSS
DIAL
DInSAR
DN
DNN
DOAS
DOC
DoD
DOM
DOMINO
DOS
DPR
DSCD
DSM
DSSAT
DVI
EBR
EC
EDI
EDII
chemical transport model
dry aerial mass
diameter at breast height
dynamic Bayesian network
delay-Doppler map
dense dark vegetation
digital elevation model
differential global navigation satellite system
differential absorption LiDAR
differential interferometric SAR
digital number
deep neural network
differential optical absorption spectroscopy
dissolved organic carbon
differencing of DEMs
dissolved organic matter
Dutch OMI DOAS
dark object subtraction
dual-frequency precipitation radar
differential slant column density
digital surface model
Decision Support System for Agrotechnology Transfer
difference vegetation index
energy balance residual
electrical conductivity
enhanced dust index
enhanced dust intensity indexELM
EO
EPIC
ESA
ET
EVI
FFNN
FLAASH
FLIM
FORCE
FOV
FPAR
FRED
FRP
FTS
FVC
GAM
GBDT
GBRT
GBT
GCP
GEMI
GIOP
GLASS
GNDVI
extreme learning machine
Earth observation
Earth Polychromatic Imaging Camera
European Space Agency
evapotranspiration
enhanced vegetation index
feed-forward neural network
Fast Line-of-sight Atmospheric Analysis of Spectral
Hypercubes
Forest Light Interaction Model
Framework for Operational Radiometric Correction for
Environmental monitoring
field of view
fraction of photosynthetically active radiation
fire radiative energy density
fire radiative power
Fourier Transform Spectrometer
fractional vegetation cover
generalized additive model
gradient-boosted decision tree
gradient-boosted regression tree
gradient-boosted tree
ground control point
Global Environmental Monitoring Index
generalized IOP
Global Land Surface Satellite
green band NDVIGNSS
GOCI
GOME
GOSAT
GPM
GPR
GPS
GRNN
GRVI
GTWR
GWR
HI
HV
IASI
IFOV
IMU
INFORM
InSAR
IOP
IPDA
IPVI
ISM
KCV
KNN
KRR
LAI
LAR
Global Navigation Satellite System
Geostationary Ocean Color Imager
Global Ozone Monitoring Experiment
Greenhouse gases Observing SATellite
Global Precipitation Measurement
Gaussian processes regression
global positioning system
general regression neural network
green-red vegetation index
geographically and temporally weighted regression
geographically weighted regression
harvest index
holdout validation
Infrared Atmospheric Sounding Interferometer
instantaneous field of view
inertial (independent) measurement (mapping) unit
Invertible Forest Reflectance Model
interferometric SAR
Inherent optical property
integrated path differential absorption
infrared percentage vegetation index
inorganic sediment matter
K-fold cross-validation
K-nearest neighbor
kernel ridge regression
leaf area index
least angle regressionLiDAR
LOOCV
LOS
LST
LSTM
LSWI
LUE
LUT
LWIR
MAE
MAPE
MCI
ME
MERIS
MIR
MISR
MLP
MLR
MLRA
MODIS
MODTRAN
MOPITT
MPE
MSAVI
MSI
MSR
MVS
LIght (laser) Detection and Ranging
leave-one-out cross-validation
line of sight
land surface temperature
long short-term memory
land surface water index
light use efficiency
look-up table
long-wave infrared
mean absolute error
mean absolute percentage error
MERIS maximum chlorophyll index
mean error
MEdium Resolution Imaging Spectrometer
middle infrared
Multi-angle Imaging SpectroRadiometer
multi-layer perceptron
multiple linear regression
machine learning regression analysis
Moderate-resolution Imaging Spectroradiometer
Moderate-resolution Atmospheric Transmission
Measurements of Pollution in the Troposphere
mean prediction error
modified soil-adjusted vegetation index
multispectral imager (instrument)
modified simple ratio
multi-view stereoMWP
NASA
NBR
NBRCS
NDCI
NDI
NDRE
NDSI
NDVI
NGRDI
NN
NPP
NRCS
OBRA
OC
OCM
OCO
OLCI
OLI
OMI
OSAVI
OSCAT
OSM
PFSM
PIV
PLS
PLSR
mean wave period
National Aeronautical and Space Administration
normalized burn ratio
normalized bistatic radar cross-section
normalized difference chlorophyll index
normalized difference index
normalized difference red edge
normalized difference salinity index
normalized difference vegetation index
normalized green-red difference index
neural network
net primary productivity
normalized radar cross-section
optimal band ratio analysis
ocean color
Ocean Colour Monitor
Orbiting Carbon Observatory
Ocean Land Color Instrument
Operational Land Imager
Ozone Monitoring Instrument
optimized soil-adjusted vegetation index
Oceansat Scatterometer
organic sediment matter
parameterized first-guess spectrum method
particle image velocimetry
partial least squares
partial least squares regressionPM
PMVS
PMW
PRI
PS
PSF
PVI
QAA
RBF
REP
RF
RFR
RGB
RGBVI
RH
RMSE
RNN
ROS
RPD
RTK
RTM
RVI
SAA
SAIL
SAR
SAVI
SCD
particulate matter
patch-based multi-view stereo
Global Passive Microwave
photochemical reflectance index
persistent (permanent) scatter
point spread function
perpendicular vegetation index
quasi-analytical algorithm
radial basis function
red edge position
random forest
random forest regression
red, green, blue
red green blue vegetation index
relative humidity
root mean square error
recurrent (recursive) neural network
rate of spread
ratio of prediction to derivation
real-time kinematic
radiative transfer model
ratio vegetation index
semi-analytical algorithm
Scattering by Arbitrarily Inclined Leaves
synthetic aperture radar
soil-adjusted vegetation index
slant column densitySCIAMACHY
SDD
SfM
SIFT
SLC
SMAC
SMAP
SMI
SMLR
SMOS
SNAP
SNR
SOC
SR
SREM
SSC
SSM
SSM/I
SSS
SST
STT
SVM
SVR
SWH
SWIR
Scanning Imaging Absorption Spectrometer for
Atmospheric Chartography
Secchi disc depth
structure from motion
scale invariant feature transform
single look complex
simplified method for atmospheric correction
Soil Moisture Active Passive
soil moisture index
stepwise multiple linear regression
Soil Moisture and Ocean Salinity
Sentinel Application Platform
signal-to-noise ratio
soil organic content (carbon)
simple ratio
simplified and robust surface reflectance estimation
method
suspended sediment (solid) concentration
surface soil moisture
Special Sensor Microwave Imager
sea surface salinity
sea surface temperature
senescence temperature threshold
support vector machine
support vector regression
significant wave height
shortwave infraredSWRF
SZA
TBI
TIN
TIR
TLS
TMI
TN
TOA
TOC
TOMS
TP
TP
TRMM
TROPOMI
TSI
TSM
TSS
UAV
UV
UVAI
VARI
VCD
VFC
VHR
VI
VIF
spatiotemporally weighted random forest
solar zenith angle
three-band index
triangulated irregular network
thermal infrared
terrestrial laser scanning
TRMM Microwave Imager
total nitrogen
top of atmosphere
total organic carbon
Total Ozone Mapping Spectrometer
total phosphorus
total phosphorus
Tropical Rainfall Measuring Mission
TROPOspheric Monitoring Instrument
trophic state index
total suspended matter
total suspended solids
unpiloted aerial vehicle
ultraviolet
UV aerosol index
visible atmospherically resistant index
vertical column density
vegetated fractional cover
very high resolution
vegetation index
variance inflation factorVIIRS
VNIR
VZA
WDVI
Visible Infrared Imaging Radiometer Suite
visible near infrared
viewing zenith angle
weighted difference vegetation indexPart I
Fundamentals1 Introduction
DOI: 10.1201/9781003517504-2
Remote sensing is defined as the science and art of garnering information about
features or phenomena of interest in an area on or near the Earth’s surface via detecting
and analyzing the electromagnetic radiation reflected, scattered, or emitted by them.
The radiation received at the sensor is rendered either as an image or non-imagery
randomly distributed point clouds over the area of study. In the former case, the image
is unexceptionally multispectral and captures the spectral behavior of the target and its
variation with wavelength. In the latter case, the captured data manifest the 3D position
of the target at the sensed spot. Conventionally, imagery data are analyzed digitally to
derive qualitative, categorical information on the target, such as different types of
vegetation or different classes of water (e.g., deep water, shallow water) after the pixel
values of the target in the spectral domain are categorized into a pre-determined number
of groups using analyst-defined criteria under certain assumptions. In this process of
converting continuously varying pixel values to categorical surface covers enumerated
as nominal data, the mapped covers rarely have a one-to-one correspondence to the
input pixel values. Instead, a range of pixel values is likely lumped together and
assigned to a single cover as image classification is virtually a process of simplification
and amalgamation of the original input pixel values. With only occasional exceptions,
images are classified based solely on image-derived inputs without resorting to external
or auxiliary data. The number of classified pixels having a certain specific value may be
expressed quantitatively (e.g., area or percentage in the scene of study), this kind of
information extraction is not considered quantitative sensing as the derived quantity is
not associated directly with a unique pixel value or has a spatial component to it.
Instead, it is based on the count of the pixels having a given range of values. In essence,
it is just a type of qualitative remote sensing (e.g., land cover mapping) at a more
detailed level.1.1
Quantitative sensing refers to the process of converting the sensed imagery data to a
parameter value on the ground in the spatial domain via some kind of mathematical
manipulation or sophisticated modeling. Since the pixel values are translated to the in
situ values either directly or indirectly, the quantified outcome is precise, able to
preserve all the subtle variations in the attribute value of the target parameter in the
output. Non-imagery data themselves may be quantitative, but they do not pertain to
quantification as they simply indicate the 3D position of randomly distributed points, of
which the third dimension (height) is the frequent target of quantification, be it the bare
ground elevation or surface relief. The quantitative information covered in this book
refers to the change in its position (e.g., rate of debris displacement) or height (e.g.,
ground subsidence).
The purpose of this chapter is to present an overarching overview of quantitative
remote sensing and lay the foundation of how to undertake it. It starts with narrating its
definition, nature, and requirements. The second part of this chapter elaborates how to
collect field data, including in situ sampling, spectral data measurement, and gathering
of third-party auxiliary data that serve as an indispensable benchmark of remotely
sensing data against their in situ observed parameter values. This discussion is followed
by a review of image-derived variables that have been commonly used to quantify the
target parameter. The fourth section of this chapter expounds how the quantified results
are evaluated and validated for their reliability and accuracy. The fifth section
elucidates the obstacles commonly facing the quantification process and the factors
degrading the reliability of the quantified results. Finally, this chapter outlines the
organization of this book and briefly introduces the content of the chapters to follow.
QUANTITATIVE REMOTE SENSING
Classically, quantitative remote sensing is defined as the process of converting the
radiometry of a target parameter (e.g., the concentration of suspended sediments) in a
spectral band of remote sensing imagery or the transformation of radiometric values
captured in multispectral bands to its real-world value on the ground. Quantification
represents an attempt to associate a quantitative value to the target of sensing so as to
generate its field view of distribution from remotely sensed data. With the advances in
sensing technology, especially the advent of non-imagery sensing, this definition of
quantitative remote sensing is too narrow and restrictive, and hence needs to be
broadened to encompass the derivation of quantitative and field-view information on
the target’s mobility based on its 3D coordinates (X, Y, Z) and their magnitude and rate
of change from multi-temporal non-imagery data.Quantification means the retrieval of physical parameter values, and is considered
synonymous to estimation but not detection that implies generation of only qualitative
information. Quantitative remote sensing is precipitated by the necessity of generating a
spatial view of certain parameter values vital in some applications, such as modeling of
weather pattern and climate warming. The quantified outcome delivers a precise
account of what is in store (e.g., the amount of carbon stored in a forest). It supplies
critical evidence for decision-makers to take the appropriate action such as planting
trees to counterbalance the carbon emission from the combustion of fossil fuels and for
certain transactions (e.g., carbon trading). As a valuable means of census, quantification
is able to gauge the current state and embody it as a numerical value that can serve as
the baseline, against which the long-term trend of change of the target can be monitored
precisely and compared against each other. With the quantitative information, it is
possible to comparatively assess the eco-service values of diverse ecosystems (e.g.,
grassland vs woodland in sequestering atmospheric carbon), and explore the impact of
environmental changes on ecosystem health and functions, and the influence of
environmental factors on them. Only through monitoring the quantitative change of the
target parameter is it possible to ascertain precisely how it has responded to external
treatments, such as soil nutrient content and farmland crop yield in response to
fertilization. More importantly, quantitative remote sensing is able to predict what is
going to happen in the near future. Such information can guide the adoption of
remedies to prevent foreseeable disasters, such as securing more food supplies in light
of a predicted reduction in crop yield to avoid famine.
Since its inception in the late 1960s and early 1970s, remote sensing has served
mostly as a major data source for acquiring qualitative information on the target of
sensing. Most of the efforts are directed at identifying ground covers on the terrestrial
surface via digital manipulation of imagery data using computer software. The
classified land cover maps illustrate the state of the surface at the time of sensing,
yielding qualitative information on the target’s spatial distribution, a process known as
qualitative remote sensing. It is quite different from quantitative remote sensing. The
differences between qualitative and quantitative remote sensing are summarized and
compared in Table 1.1. Apart from the nature of the derived results, the input data used
also differ between them. For instance, field data are instrumental in quantitative
remote sensing, but they are non-essential in generating qualitative information except
in validating the qualitative results. In general, qualitative remote sensing is much
easier to realize as it can be achieved using imagery data alone without the need to take
into account the atmospheric (and even topographic) effects on remote sensing data, or
make use of external data. Qualitative remote sensing can be regarded mostly as static
as the produced output is considered time-invariable or slightly variable with time. Incontrast, quantitative remote sensing is dynamic, producing outputs that are current at
the time of sensing or that have a strong dependency on time. It is able to generate time￾series outputs in near real time and predict the status of the target parameter in the
future based on what is observed on current images. Nevertheless, the output of
quantitative remote sensing is generally less reliable than its counterpart of qualitative
remote sensing as it is affected by more factors, including the sensing environment.
However, quantitative remote sensing can retrieve much more detailed and valuable
information to meet the needs of more applications in many fields than qualitative
remote sensing. Its wider scope of applications encompasses all the spheres from the
ground level all the way up to the stratosphere. In comparison, qualitative remote
sensing is of little use in some applications. Although there is no direct linkage between
qualitative remote sensing and quantitative remote sensing as the quantitative
information can be derived from pixel values directly without the need to establish its
identity first, in certain cases the latter is built upon the former by going one step
further to derive detailed numerical information for each of the mapped covers. For
instance, instead of classifying the vegetative covers categorically as forest, shrubland,
and grassland as is commonly implemented in qualitative remote sensing, their
aboveground biomass (AGB) or carbon stock can be derived from these covers
quantitatively with the assistance of field data and well-established estimation models if
the input data are imagery (but not so with non-imagery data). Needless to say,
quantitative remote sensing is much more complex and demanding to realize than
qualitative remote sensing. Frequently, this realization is possible only if facilitated by
external data or highly sophisticated physical models that may demand simplification to
ensure model invertibility. Understandably, the accuracy of the quantitative information
retrieved from remotely sensed data is markedly lower than that of qualitative results,
or impossible to validate because of the lack of ground truth data.
Table 1.1
Disparities between qualitative and quantitative remote sensing in various aspects
Features Qualitative sensing Quantitative sensing
Primary input
Spectral bands, facilitated by
image-derived spatial
attributes, nearly all internal
data
Spectral data, field data, and
possibly environmental data;
combined use of both internal and
external data
Field data
Non-essential, only needed in
accuracy assessment
Essential, and must be collected
currently with imagery data
Use of models Not relevant Essential in certain casesFeatures Qualitative sensing Quantitative sensing
Decision rules
Spectral domain partitioned
into non-overlapping spheres,
each corresponding to a unique
category of ground cover;
Almost no assumptions
involved
Mathematical equations or models
to translate pixel values to in situ
observed parameter values;
Assumptions are rife to simplify
quantification
Ease of
implementation
Relatively simple computation;
Very easy using standard
software
Sophisticated analysis and
modeling;
Complex, lengthy, and
challenging;
Niche packages or even scripting
needed
Atmospheric
effects
Safe to ignore without making
noticeable changes to the
output
Imperative to eliminate, together
with topographic effects (if
relevant) to generate authentic
results
Nature of output
Imprecise, categorical, limited
in quantity, mostly one-off;
Static, mostly time-invariable;
Indicative of current state
Precise, continuous values;
Dynamic, current at the time of
sensing, time-series outputs
possible
Able to indicate what is likely
going to happen
Reliability
High accuracy easier to
achieve, affected by classifier
and the homogeneity of the
target
Accuracy tends to be lower, or
even unknown; affected by diverse
factors, including the sensing
environment, or the reliability of
models
Domain of
applications
Limited, mostly in the
terrestrial sphere and biosphere
Widely ranging spheres from the
ground all the way up to the air
To a large degree, the development and sophistication of quantitative remote sensing
have been made possible and even spurred by the rapid advances in sensing technology
and computing power, especially the easy availability of purpose-built computing
platforms and packages. They enable remotely sensed data to be calibrated at an
unprecedented accuracy level and to be analyzed either solely or in conjunction with1.1.1
1.1.2
other non-remote sensing data to derive quantitative information about the target in ever
expanding fields of application.
Static Versus Dynamic Quantification
Both the philosophy and manner of remote quantification vary with its nature that falls
into two broad groups of static and dynamic. Static quantification refers to the
derivation of quantitative information on the target at a specific moment, achievable
from one-off data. The produced quantitative information is valid and current at the
time of sensing or within a short period of imaging if the target is temporally stable.
Certain static parameters change their quantitative information minimally within a short
period of sensing, such as vegetation AGB and landslide debris creeping rate.
Conventionally, static quantification has been the exclusive norm in quantitative remote
sensing. In general, static quantification involves the use of only pixel values in a single
band or the ratio of pixel values in multiple bands that are converted to a parameter
value directly, with or without complex transformation. In static quantification, the
image pixel values after (optional) atmospheric correction are relied on to yield the
quantitative output. However, with the evolution and development of remote sensing,
mountainous multi-temporal remotely sensed data of a given geographic area have
accumulated over decades. Such long-term and copious data have opened the floodgate
for quantifying the target dynamically. Dynamic quantification is defined as the
retrieval of a parameter’s attribute value through comparative analysis of the two static
qualities that have been quantified already. Implicitly, it has a temporal dimension. In
dynamic quantification, the quantified values (including LIght (laser) Detection And
Ranging (LiDAR) point 3D coordinates) at different times are compared with each
other directly. Dynamic quantification is a step further than static quantification and
produces much more insightful results as the quantified attribute can be 3D. For
instance, a comparison (e.g., subtraction) can reveal the dynamic change of the target
over a time period, such as the distance of displacement in surface deformation and the
pace of movement (e.g., velocity and direction of debris creeping). The quantified
outcome may be further processed by the duration of study to derive the rate of change.
It is analogous to quantitative change detection in remote sensing.
Target of Quantification – The Environment
In this book, the environment of remote sensing refers to the domain in which humans
dwell or with which they interact closely and frequently. It encompasses land, air, and
ocean. Although human direct interactions with the atmosphere is confined mostly toonly a thin layer adjoining the Earth’s surface, other layers above it (e.g., the
troposphere and stratosphere) are still covered in this book because atmospheric
pollutants residing in them affect human health and even life expectancy just as they
were distributed in the atmosphere. Also included in this book is the biosphere that is
closely related to human life by producing food for human consumption and by
emitting oxygen for humans to breathe. Apart from physical variables, the properties of
natural phenomena taking place within the biosphere and hydrosphere, such as forest
fire intensity and circulation of ocean currents, are also the targets of study in
quantitative remote sensing.
On the surface, human activities and their impacts on the environment are not
considered parts of the natural environment, and hence are beyond the scope of this
book. In reality, certain human activities may leave lasting legacies or imprints behind
in the environment, such as plantation forests and human-induced landslides. They are
all considered the targets of quantification as natural forest and plantation forest can be
quantitatively assessed from identical data using the same method of data analysis
irrespective of their origin. Similarly, fires can be ignited naturally by lightning or
deliberately lit by humans as a way of clearing the land in swidden agriculture or the
excessive accumulation of flammable fuels to minimize the risk of bush fires. Thus, the
quantification of fire properties such as burning intensity and fire spread rate are also
covered in this book as quantification focuses primarily on the phenomenon itself
irrespective of its causes.
The quantitative information about a parameter that can be retrieved from remotely
sensed data may be temporally stable, ephemeral, or in a state of perpetual change.
Stable parameters tend to be static mostly. Ephemeral parameters last only minutes or
hours, such as forest fires and volcanic eruptions. They require images of a super-fine
temporal resolution to quantify. Some elements of the environment are not stationary
but in a state of constant motion, such as winds and ocean currents. Constantly
changing parameters pertain mostly to the atmosphere (and to a lesser extent, the
troposphere) and hydrosphere, such as air pollutants under the effects of winds and
suspended sediments in coastal waters. It is very crucial to distinguish the temporal
dimension of such features as it affects not only the ease of quantification but also the
accuracy at which the target can be quantified. For instance, it is much easier to collect
a large number of in situ samples to construct the estimation model (see Eq. 1.1) for
static and stationary parameters. Conversely, it is highly demanding and almost
impossible to collect in situ samples to verify the quantification accuracy of fast
changing and quickly moving targets such as air pollutants in a volcano plume.
Certain aspects of the environment, such as the elevation of a topographic surface,
may be quantified from stereoscopic aerial photographs as in conventionalphotogrammetry, although this method has been gradually replaced with LiDAR
sensing. The derivation of precise bare ground elevation from aerial photographs is
considered as the task of photogrammetry, not quantitative remote sensing in this book.
Only its close cousin, the highly automated and computationally intensive Structure
from Motion (SfM) photogrammetry (see Section 4.5.1 for more details) will be
covered in this book. However, the derivation of local relief, such as tree height and
changes in elevation induced by volcano eruptions, landslides, and ground subsidence
from LiDAR data is deemed quantitative remote sensing, and hence extensively
elaborated in this book. The widely ranging parameters of the environment that have
been quantified from remotely sensed data are compared and contrasted in Table 1.2,
together with the best data to use and the requirements of quantification. How exactly
the quantification is achieved will be expounded in detail in the relevant chapters to
follow.
Table 1.2
Summary and comparison of commonly quantified environmental parameters covered in this book, their best
sources of data to use, and requirements
Nature Category Examples Best data to use Requirements
Static
State - surface
Albedo,
temperature,
heat flux,
moisture; SST
VNIR and
TIR bands
In situ
samples,
regression
analysis
Significant
wave height
TerraSAR &
GNSS-R
Complex
inversion of
wind-sea wave
spectrum
Leaf area
index
Multispectral
bands
Use of special
indices or
complex
physical models
State -
height/depth
Tree height,
canopy
height;
Bathymetry
LiDAR data
Optical
bands,
bathymetric
LiDAR
Construction of
CHM;
Correction for
the impact of
in-water
substancesNature Category Examples Best data to use Requirements
Content
Salinity, soil
salt, moisture,
soil
contaminants
Hyperspectral
&
multispectral
data
Use of
numerous
predictor
variables and
complex
algorithms
AGB and
AGC
LiDAR and
imagery
Allumetric
equations; in
situ tree
parameters
In-water
matter, Chl-a,
CDOM
Coarse
resolution
ocean
satellite data
Use of
empirical or
semi-analytical
models
Solid
particles, dust
intensity,
PM2.5, trace
gases (CH4,
O3, CO, NO2,
SO2, CO2)
Coarse
resolution
atmospheric
satellite data
Complex
modeling and
physical
models,
assumptions
(simplifications)
Events
Algal and
phytoplankton
blooms
Ocean color
satellite data,
shortwave
bands
Use of indices
and normalized
water-leaving
radiance
Potential Crop yield
Multispectral
bands
Use of co￾variables and
crop yield
models, harvest
index
Quality
LUE, FPAR,
NPP
Vexcel
UltracamX,
optical data
Use of indices
and regression
models
Dynamic Rate Flow, rain,
wind, ocean
circulation
Two-time
optical and
radar images
Tracking of the
same feature in
both images or1.1.3
1.1.3.1
Nature Category Examples Best data to use Requirements
based on spatial
auto-correlation
Thickness/depth
Lava deposit,
snow, debris
Two-time
LiDAR data,
SAR images
Co-registered
DEMs of same
grid size
Volume
Wood, trees
Airphotos,
drone images,
LiDAR,
InSAR
In situ
samples,
models, and
DSM and CHM
Debris,
sediment,
snow, lahar,
lava
Two-time
photos,
LiDAR, and
SAR images
Precise co￾registration of
DEMs of the
same grid size
Intensity
Burn
intensity, fire
intensity
Paired TIR
and optical
images
Rate of fire
spread &
identification of
fire front
Mobility
Horizontal -
Debris
creeping
InSAR,
LiDAR
Two-time
images of a
short separation,
distinct surface
features
Vertical -
Ground
subsidence
GPS, InSAR,
LiDAR
Two-time data
precisely co￾registered
3D
deformation
InSAR,
DInSAR
A pair of SAR
images with
phase
difference,
phase-stable
targets for co￾registration
Nature of Quantification
General PrincipleHow exactly the quantitative information is retrieved from remotely sensed data
depends ultimately upon the nature of the data used, namely, whether they are imagery
(graphic) or non-imagery (non-graphic). Prior to the advent and wide adoption of
LiDAR technology, all remotely sensed data are exclusively graphic, either in analogue
prints or the digital format. These images may be acquired over different portions of the
spectrum and contain multispectral bands. With the use of imagery data, quantitative
remote sensing refers to the derivation of a quantitative measure for a target parameter
from its pixel value in a single band or the transformed pixel values in multiple bands.
The relationship between the quantitative information to be retrieved (Y) and the pixel
value (X) is expressed conceptually as:
(1.1)
where Xi = pixel value in spectral band i, or transformed ratio of pixel values in
multiple bands, or the ith predictor variable; Y = value of the target parameter to be
quantified; n = total number of bands or predictor variables considered.
Since imagery data contain only spectral information on the target parameter to be
retrieved, the estimation of Y from such data alone may not be adequately feasible. It is
likely that Y may be related to other environmental factors, such as topography that also
exerts an effect on vegetation biomass. Thus, Xi have been expanded to include
environmental variables in sophisticated modeling to predict Y using advanced machine
learning algorithms. With the advent of LiDAR data, non-imagery-based quantification
makes use of multi-temporal data about the same target. The two-time data may be
acquired at a temporal separation as short as a few months or as long as a decade. This
quantification can be mathematically implemented as
(1.2)
where ∆ = difference in the 3D coordinates at time 1 (t1) and time 2 (t2) or quantitative
change. This differencing may be undertaken either for ∆(X,Y) or for ∆Z separately.
The former applies to the quantification of non-stationary targets whose horizontal
position has shifted in the interim of two data acquisitions, and is normally carried out
to quantify the pace or rate of mobility. The target itself does not change its identity or
state, only its location. Neither does its height that is assumed constant in the interim.
Therefore, the difference is meaningful only in the horizontal position (X,Y), or the
displacement from (Xt1,Yt1) to (Xt2, Yt2) over the interval of the two data acquisitions.
It can be turned to the speed of motion, such as the downslope creeping rate of
landslide debris using the following equation:1.1.3.2
(1.3)
where Dt = t2 – t1 or time lapse between the two data acquisitions.
In some cases, the third (Z) coordinate is differenced for features that have a relief,
such as topographic surface and tree height. This differencing effectively yields
quantitative information on surface erosion (or sediment deposition) or tree growth in
relation to the bare ground or reference height. Such quantitative information may
pertain to the subsidence rate of the ground or the surface erosion rate, which is
essential in quantifying the volume of sediment yield in a catchment. Conceptually, the
differencing on Z (or h) can be simplified as
(1.4)
This is based on the understanding that the two layers used for differencing have been
co-registered with each other to a sufficiently high accuracy.
Components
Quantitative remote sensing comprises four main parts: field data collection, remote
sensing data acquisition, data processing and analysis, and result validation, of which
data manipulation and transformation form the core. At first glimpse, these four
essential components may not seem indispensable but they are vital to the success of
quantification, especially image-based quantification. The first component is commonly
known as ground remote sensing during which vital data are collected in the field. It is
a preparatory step for model development and result validation. The collected data
serve as the bridge to link remotely sensed data with the actual value of the target
parameter at the time of sensing. In a sense, they benchmark remotely sensed data (and
their transformations) against the quantitative measure of the target parameter on the
ground. In addition, ground remote sensing also supplies the data needed to verify the
quantified results. This topic is so complex that it will be discussed in Section 1.2
separately later. Remote sensing data, either imagery or non-imagery, are the primary
source from which the target parameter’s value is retrieved. A large variety of space￾borne data has accumulated for quantitative remote sensing (see Chapter 2 for details).
Their properties vary widely, and the best data to use depend on the target of
quantification. If the remotely sensed data are acquired or to be acquired by satellites
routinely, they can be purchased or downloaded from the data supplier’s websites. If
not, plans need to be made to fly over the area of study, usually under calm weather
conditions. In case of drone data, prior flight authorization must be secured first (refer
to Section 2.1.1 for more details).The ultimate objective of data manipulation and analysis is to develop the estimation
model, through which the input data are translated to the quantitative value of the
target. Of the four components of quantitative remote sensing, it is the most complex
involving several steps, one of which is data preparation. It aims to transfer the
remotely sensed data to a usable format. This may involve cloud removal from optical
images, and spatial interpolation of LiDAR data to the desired spatial resolution. Data
preparation also includes unification of coordinate reference systems and vertical
benchmark for LiDAR data. If multi-temporal data are involved in a quantification,
they have to be calibrated and standardized to the same radiometric scale, and geo￾referenced to the same ground coordinate system if necessary. Radiometric calibration
is performed on imagery data to remove the atmospheric effects prior to formal
quantification (see Chapter 3). The type of data analysis and its complexity vary widely
with the target of quantification and its nature. It may mean simple division of one
spectral band by another as in producing a vegetation index or transforming the derived
index into a new format via regression models. Once the estimation model is
constructed and deemed acceptable (as with the training samples), it is applied to the
remotely sensed data to produce a quantitative distribution map of the target parameter.
Result validation does not impact the quantification outcome. Instead, it is merely an
attempt to attach a quantitative measure to the retrieved value, usually via independent
samples collected in the field that have not been used in model construction. The
derived validation measures not only indicate the reliability and accuracy of the
quantification, but also allow the comparison of different retrieval models (and model
parameters) and the assessment of the effectiveness of the considered input variables
and data. Through the generated accuracy indicators it is possible to pinpoint the factors
that have adversely impacted the quantification results and enlighten us about how the
quantification may be improved in future. The actual implementation of validation and
the expression of validation outcome are such a broad topic that they require a full
section (Section 1.5) to explain.
The general procedure of quantification comprises several sequential steps in
transforming the input remotely sensed data to the desired quantitative values of the
target parameter. As illustrated in Figure 1.1, this process can be described as data
collection → data analysis → model construction → model application → results
validation sequentially. In terms of timing, only ground data sampling and remote
sensing data acquisition need to be concurrent. The synchronization of in situ sampling
with air- and space-borne imaging is especially important and imperative in quantifying
ephemeral features that change their properties or values within a short time, such as
concentrations of in-water constituents. For terrestrial features whose property or height
does not experience noticeable temporal changes, a discrepancy of a few days between1.1.4
in situ sampling and image acquisition will not tangibly degrade the quantification
outcome. If exact synchronization is not feasible, then the two should take place as
closely as possible to minimize temporal variation in the target parameter value or the
impact of the changed atmospheric conditions on the quantification outcome. Sufficient
in situ samples must be collected at representative sites to build a sound model to
bridge the two types of data together. They also offer plenty of leverage in splitting
them into two sizable parts randomly, one used to construct the model, and the other for
model validation. Once the model is deemed acceptable, it is then applied to the remote
sensing data to generate the spatial distribution of the parameter value for the target of
interest, followed by validation.
FIGURE 1.1 General procedure of quantification from remotely sensed data involving data
transformation and validation to yield the accuracy indication. (Verrelst et al., 2015a,
used with permission (5751050246505) from Elsevier.)
Requirements of Quantification
The aforementioned quantification procedure cannot be successfully accomplished
unless the following four requirements are fulfilled:
i. Ground data. Ground data may not show up in the final quantitative results
directly, but they are fundamental to assessing the accuracy of the intermediate
land cover maps produced from satellite images. How reliably the quantification
has been achieved and what factors have contributed to its inaccuracy are
ascertained by comparing the quantified outcome with some sort of ground truth
and analyzing the two sets of data statistically. Although quantification is based on
remotely sensed data, remote sensing alone cannot fulfill the whole requirementsof quantification. Ground data play three vital roles in the quantification process.
First, they serve as the bridge to link remotely sensed data with real-world
parameter values. Samples are essential to establish the relationship between
sampled ground properties and those on satellite imagery at the corresponding
locations so that point-observed properties can be upscaled to the spatial extent of
pixels and extrapolated to the entire area of study covered by the image. Without
ground data, remotely sensed results are just abstract numbers devoid of
meaningful values attached to them. Second, they are also needed to parameterize
physical models. The parameters in semi-physical models need to be properly
tuned using ground data. Without ground data, their values cannot be determined,
and the models will probably yield vastly inaccurate simulation results. Third,
ground truth data are essential in evaluating the quality of the retrieved result or in
delivering its quality assurance. Validation is commonly accomplished using the
in situ samples not used in constructing the estimation model to attain
independence.
ii. Powerful analytical algorithms and computing systems. In order for the
quantification to be successful, there must be powerful computing packages to
mine the input data and to determine the most effective predictors to be included
in the prediction model from a large pool of remotely sensed and other auxiliary
data. This task becomes increasingly important if more variables are considered in
the retrieval and they are analyzed using sophisticated machine learning
algorithms. Although diverse metrics can be generated from remotely sensed and
auxiliary data with the assistance of machine learning algorithms, not all of them
are useful or equally effective in predicting the dependent variable. It is hence a
prerequisite to identify the most reliable parameters or metrics, and satellite
images (pixel values) and their transformations for certain features, such as tree
biomass to minimize the computation cost. Even for tree heights, they cannot be
linked directly to a LiDAR data point cloud, so various height metrics of LiDAR
data are derived and compared among themselves to see which one allows the
dependent parameter to be modeled most reliably. Without powerful computation,
it is impossible to identify the most relevant predictor variables and attach an
importance value to each of them. Quantification is feasible only when a reputable
relationship between the imaged or remotely sensed values and the targeted
quality to be quantified can be established, usually through powerful computing
packages or machine learning and deep learning algorithms to be covered in
Section 4.3. They also enable the target parameter to be quantified at higher
accuracy.iii. Target visibility. In order for a target to be quantified from remotely sensed data,
first and foremost, it must be visible on the images or have some traces lingering
in the remote sensing data either directly or indirectly. Thus, nearly all
quantifiable features must lie on or above the ground, or be suspended in a
medium of a sufficiently high transparency. For vegetation, quantification is
limited to aboveground features visible from images (e.g., not roots).
Quantification of below-ground biomass is still possible only indirectly via its
relationship with AGB. As for in-water constituents and soil contents, their
quantification is confined to the skin layer through which the solar radiation used
for sensing is able to penetrate. Therefore, the quantified surface concentration
may not reflect the whole 3D distribution of the feature of interest, such as the
vertical distribution of sediments suspended in a lake. For the quantification of
atmospheric parameters, their concentration is usually integrated over the entire
air column from the sensor all the way down to the ground surface as it is difficult
to separate them into layers at high accuracy from space.
iv. Precise positioning. All quantifications from remotely sensed data are inherently
spatial in nature. In order to produce a field view of the quantified parameter, all
field samples collected on the ground must be precisely geo-referenced. Sample
positional information plays three important roles in the quantification. First, it
enables the association of in situ sampled attribute values with their corresponding
image properties. Through their location on the image, a relationship between
image properties and in situ measured quality is established. Second, they enable
the geo-referencing of images and the mosaicking of multiple UAV images to
cover a large ground area. Finally, they enable multi-temporal remote sensing data
to be co-registered with each other. Image co-registration is vital to dynamic
quantification. Equally critically, the images must also be geo-referenced to the
same system using onboard Global Positioning System (GPS)-logged positional
and orientational data, and the two systems must match so that the two sets of data
can be overlaid with each other spatially.
The location of the ground samples collected is usually determined via a GPS unit.
Needless to say, the accuracy of the logged GPS coordinates also impacts the accuracy
of quantification, especially when the target of quantification is spatially heterogeneous
and the image used has a fine spatial resolution. A smaller pixel size means less room
for geo-location inaccuracy as the in situ sample could correspond geographically to the
neighboring pixels instead of the correct one.
It must be noted that even if all the necessary requirements are met, successful
quantification is not always guaranteed because of the limitations with remote sensing,1.2
1.2.1
1.2.1.1
be it imagery (e.g., poor spatial resolution) or non-imagery LiDAR (e.g., in LiDAR
shadow) or the subtlety of the target parameter (e.g., indistinct from each other or from
other features). Only when the target parameter’s signal exceeds the minimal
radiometric resolution of the image used can it be successfully quantified to a credible
accuracy.
FIELD DATA COLLECTION
The objective of ground data collection is to gather quantitative information on the
target parameter at selective and representative spots to facilitate its quantification from
remotely sensed data. Ground data collection comprises two tasks of in situ sampling
and spectral measurement. What samples are collected and how they should be
collected are both governed by the nature of the target parameter. Some can be obtained
by deploying the measurement device into the target to generate a reading directly or
multiple readings continuously without supervision while others require painstaking
clipping, followed by complex and lengthy processing in the lab. Samples are collected
in the field using different methods and for distinct purposes. Spectral behavior of the
target parameter is measured using a spectrometer to gauge its spectral response and
explore the feasibility of remotely quantifying it in the first place. Apart from such first￾hand data, ground data can also be collected from third-party organizations. How to
collect all of the needed data is explained in this section.
In situ Sampling
Sample Size, Distribution, and Considerations
Collection of ground samples in the field is inherently spatial. Spatial sampling is
virtually a process of determining the value of the target parameter. The tenet of spatial
sampling is to collect a sufficient number of spatially representative samples at the
minimum cost and shortest time possible. It has its own considerations, methods, and
requirements. Common sampling considerations are quantity, location, distribution,
size, and representativeness of samples. In order to be statistically viable and yield
trustworthy accuracy indication, sufficient samples must be collected. Sample size
refers to the number of samples to be collected. It is important to adopt a proper
sampling size as a too small size is unlikely to represent the population at a sufficiently
high confidence, and does not provide leverage in allocating spare samples to validate
the quantified results. There exists an intrinsic association between the number of in
situ samples and model reliability. A large sample size not only boosts model accuracybut also affords the opportunity of validating it. Whenever permissible, an as large as
possible sample size should be adopted, as it offers the flexibility of discarding those
samples with incomplete records or those of a dubious quality in subsequent data
analysis. Nevertheless, a larger sample size will incur excessive efforts, time, and
expense both in the field and in the lab to process the collected samples. The adoption
of a larger sample size also means prolonged sampling and more destruction to the
target of sampling, as in the determination of vegetation biomass through clipping of
grasses to the ground level. Furthermore, collecting a large sample size on the ground
makes the synchronization of in situ sampling with the recording of the remotely sensed
data increasingly difficult to maintain. Therefore, the actually adopted sample size
always represents a compromise between sampling cost and model accuracy, and a
delicate balance should be struck between the two. Ideally, the bare minimum sample
size should not fall below 30 as the estimation model constructed from fewer samples is
not statistically viable. However, this figure does not have provision for model
validation, so the actual number should be slightly higher than this bare minimum.
Where samples are collected in the field affects sampling cost and sample quality.
Sample location falls into two main categories of fixed and ad hoc types. Fixed-location
data are collected at permanent, purposely built monitoring stations over a period up to
decades. Such data are suitable for generic applications, but some may be useful for
validating remotely quantified results, such as temperature and pollution levels. Fixed￾location measurements can be automated, and they are extremely accurate owing to the
use of highly sophisticated electronic monitoring device. Fixed-location observations
are limited by their spatial sparsity and location distant from the area of study, unable to
provide adequate ground truth in most cases. So, samples have to be collected at ad hoc
positions in the field. They may be distributed in the area of study randomly,
systematically, clustered, or stratified (Figure 1.2). Of the four, random sampling is the
most popular (Figure 1.2a). It can ensure a wide spatial distribution of samples all over
the area of study. Systematic sampling guarantees that all the collected samples are
widely distributed all over the area of study at a uniform spatial interval (Figure 1.2b).
This strategy may prove impractical in the field due to site accessibility. In comparison,
clustered sampling has a narrow scope of application, and is not used widely (Figure
1.2c). Stratified sampling is the best and the most efficient sampling strategy in terms of
balancing sample size and sample reliability to yield an unbiased sample (Figure 1.2d).
In this strategy, the study area is partitioned into a number of sub-areas of a uniform
shape and size, and a specific number of samples are then collected from each of them.
The distribution of samples within each sub-area can be random or clustered. Stratified
random sampling guarantees that the collected samples are widely distributed over the
entire area of study, and hence are geographically representative. Apart fromgeographic stratification, samples can also be collected via thematic stratification, in
which the number of samples having a pre-defined attribute value is selected, usually
randomly. Such thematic stratification can guarantee the selection of samples with the
pre-determined values. Which method is the best and should be used is governed by the
nature of the target parameter and its spatial distribution in space.
FIGURE 1.2 Four strategies used in collecting spatially representative samples in
quantitative remote sensing. (Gao, 2022.)
Spatially, field samples should be scattered as widely as possible across the study area
to increase their geographic representativeness. A well-balanced and widespread
distribution all over the study area increases the robustness of the estimation models
between environmental parameters and image properties. A high level of geographical
representativeness is guaranteed if the collected samples are widely scattered all over
the area of study. A more representative sample distribution likely ensures that all the
possible values of the target parameter are captured by the collected samples. A larger
range of parameter value is conducive to a more reliable and applicable estimation
model being established. Its application to all the pixels in the input image is virtually a
process of interpolating the parameter value from pixel values. Conversely, if the
parameter value at a given pixel falls outside the range of the sampled values, then the
application of the model to this pixel represents a case of extrapolation, which produces
a far less reliable estimate than interpolation, or even an erroneous outcome. For
instance, the constructed biomass model of grassland can produce negative AGB values
if applied to bare ground pixels of no vegetation.
In certain applications, none of the sample distributions in Figure 1.2 can effectively
capture the spatial pattern of the target parameter. Even a sufficiently large sample size
of randomly distributed points may fail to encapsulate subtle elevational variations
along certain directions, as in quantifying erosion of sand dunes in foreshore coastal
areas (Figure 1.3a). This may be remedied by sampling elevations linearly along
transects. A reliable picture of dune erosion can be established by recording elevations
along a number of strategically located, parallel transects, all perpendicular to theshoreline (Figure 1.3b). Along each transect, elevations are sampled at critical spots and
variable intervals proportional to the surface complexity with more samples collected in
highly variable sections of a transect. Such distributed samples can reveal the change in
beach morphology economically. Similarly, in quantifying coastal bathymetry and
sediment concentration in a channel, all sample points should be distributed linearly
either perpendicular to the shoreline or parallel to the channel median.
FIGURE 1.3 Three types of sampling units and/or sample distribution. (a) (random) point
sampling; (b) transect sampling ideal for studying elevational profile at strategic
positions. The sampling points are distributed along a transect proportionally to surface
elevation variability; and (c) plot sampling to collect the target enclosed by a square or
circle that can be conveniently subdivided into quadrants (e.g., half or a quarter of a
plot). (Gao, 2022.)
In considering sample representativeness, attention should also be paid to sample site
accessibility. It is challenging or even impossible to collect samples that are located at
inconvenient or inaccessible sites, such as in the middle of a swamp. If relevant, the
vertical distribution of samples also deserves consideration, for instance in studying the
vertical distribution of vegetation over a mountain range. Equally, the slope aspect at
which vegetation is distributed may also need to be taken into consideration in
sampling design. Depth of samples is another consideration in studying the vertical
distribution of parameter values, such as soil moisture and suspended sediment
concentrations at different depths. Depth must be factored in if the target parameter has
a vertically non-uniform value and the sensing radiation is able to penetrate the target of
study. Ideally, samples should be collected at a maximal depth to which the signal of the
target parameter can be recorded by the remotely sensed data. Thus, a larger sampling
depth should be adopted with optical imagery data, especially VIS (visible light) data,
in quantifying bathymetry and suspended sediment concentration. However, when it
comes to temperature, the sampling depth should be confined to the skin depth due to1.2.1.2
the absorption of TIR radiation by water, whereas ocean wave height has to be sampled
on the surface.
Sampling Unit
Once a suitable sampling position has been settled, the next consideration is the
topological dimension of the sampling site or sampling unit. Basically, there are two
fundamental types of sampling units, point samples and plot samples. In point
sampling, the parameter value is measured at a particular location (Figure 1.3a). In plot
sampling, the attribute value of the target parameter is enumerated over an area (Figure
1.3c). Whether samples should be collected at a point or over a plot in the field depends
solely on the nature of the parameter to be sampled. Point sampling is the default
choice if the parameter value is observable at a site whose location can be expressed by
a pair of Cartesian coordinates (x, y). These parameters may encompass, but not limited
to, soil moisture, pH, salinity, organic matter content (SOC), elevation, temperature,
and even in-water sediment concentration determined from point-collected water
samples. Fundamentally, all targets of quantification that are observable and measurable
at points in the field must be sampled as point features.
Point sampling is relatively easy to accomplish as only one measurement is needed
to yield the quantitative result. However, this is much easier said than done for
ephemeral targets, such as atmospheric pollutants at different elevations. The collection
of point samples is relatively easy in the terrestrial sphere in comparison with its
aquatic counterpart, even though it may still be subject to site inaccessibility imposed
by natural barriers such as river channels and wetlands. In oceanographic sampling, the
sampling outcome is severely compromised by the sea conditions that, in turn, are
controlled by the weather conditions, and the cruising ship’s position and orientation in
the open sea. The ship of measurement itself may disturb the target through its waves as
in phytoplankton sampling and introduces an additional influence to its signal (e.g.,
secondary reflection off the ship to the target).
For certain biophysical variables, such as vegetation biomass and plant carbon
density, point sampling is utterly incompetent, as they are available and meaningful
only if enumerated within a spatially aggregated unit. Their sampling unit must be
expanded to encompass an area known as sampling plot. Plot sampling is the norm in
sampling attributes that do not exist at points (Figure 1.3c). Conceptually, plot sampling
is fully compatible with remote sensing data to be translated to the in situ plot-sampled
parameter value as pixel values are derived from the radiative energy originating from
an area on the ground. Their spectral properties on a satellite image are enumerated
over a square-shaped ground area. Theoretically, sampling plot size should be
commensurate with the pixel size or spatial resolution of the image to be analyzed.However, the exact match of the two sizes may not matter much if the attribute value of
the target parameter under study is spatially uniform or roughly scale-invariant (i.e.,
scale-independent), such as grassland AGB. Compared with grassland, trees are much
more heterogeneous spatially in their stature, species composition, and spatial
distribution. Naturally, a much larger plot size should be adopted in sampling
parameters of a higher spatial heterogeneity. Invariably, the nominal square shape of
image pixels is more likely to be a circle (and even elliptical) due to off-nadir scanning.
Correspondingly, the sampling plot should have a square or circular shape with a
certain radius (e.g., r = 1.5 m). Compared to circles, squares are preferable as they can
be conveniently and precisely partitioned into four equal quadrants with the assistance
of the two diagonal axes (Figure 1.3c). This partitioning is quite useful and particularly
important in assessing the change of dynamic parameters (e.g., biomass) in a
longitudinal study. For instance, biomass can be harvested in one of the four quarters of
the sampling plot in the first year, and any of the three remaining quarters in subsequent
years. In the field, plot sampling location is usually finalized by tossing a ring into the
air randomly. Wherever it lands is then sampled. This way of determining the sample
location ensures both randomness and independence of spatial samples.
Compared with point sampling, plot sampling takes much longer to complete and is
more subjective as the results vary with the size of the sampling plot, especially when
the target parameter to be sampled has a high degree of spatial heterogeneity. A small
plot size means less fieldwork as the grasses can be clipped very quickly, and the results
tend to be more reliable. However, a too-small plot may not be representative. On the
other hand, a large plot can be more representative, but the amount of fieldwork
required is going to quadruple. Thus, a delicate balance must be struck between sample
plot size and sufficient representativeness.
Once sampling is completed, it must be followed by the recording of the sample
location. Depending on the measuring device used and the nature of sampling, this
recording can be synchronized with sample data collection. In the case of plot
sampling, the plot’s centroid is logged as the position of the collected sample. In order
to be linked to pixels in a geo-referenced image, their precise location must be
expressed in the same coordinate system as that of the remotely sensed data as
accurately as possible, usually with the assistance of a GPS unit. GPS receivers vary
vastly in their functionality, number of channels, and hence accuracy. Irrespective of the
GPS receiver used, in general, the positioning accuracy at a fixed location is higher than
single, rover coordinate readings, especially if the coordinates have been differentially
corrected as multiple positions can be logged at the same sampling site and averaged to
cancel out the random error. Accurate positioning is particularly critical as the
coordinates are the only clues for linking the sampled parameter values to their spectral1.2.2
1.2.2.1
properties in remote sensing imagery, and when the attribute value varies drastically
within a short spatial range. Inaccurate positions of the collected samples may cause the
in situ sampled attribute values to be misaligned with the spectral properties of the
neighboring pixels. Inevitably, this misalignment between in situ samples and their
corresponding spectral values on the satellite image degrades the reliability of the
estimation model and ultimately the quantified results.
Typical positional accuracy used to be around 3–5 m, but can now be reduced to
sub-meters after post-processing, subject to the logging environment. More accurate
positioning is possible with Ramon filtering of the logged data, but may not prove
proportionally beneficial if the satellite image in use has a spatial resolution of a few
meters (i.e., ⁓3 m). Actually, a positioning inaccuracy of in situ samples comparable to
the spatial resolution of the remote sensing data to be used is adequate enough as it
ensures that the sample position on the image coincides with the position actually
logged on the ground. The misalignment of the logged and genuine positions of
samples by a few meters, however, is inconsequential if the image has a spatial
resolution on the order of tens of meters, as with Earth observation or resources satellite
images, or if the sampled attribute is spatially continuous with little local variation. A
shift in sampling position by a few meters on the ground will not destroy the
correspondence of the sample to the correct pixel value on the image. However, caution
needs to be exercised to extrapolate this claim to drone images that have a spatial
resolution on the order of centimeters. In this case a discrepancy of 1 m on the ground
will cause a misalignment of the sample to its corresponding true location on the image
by a few pixels. Thus, a high positioning accuracy is a highly crucial prerequisite for
minimizing subsequent quantification inaccuracy with drone images.
Collection of Terrestrial Data
Spectral Measurement
Spectral measurement strives to gauge the spectral response of the target and to
pinpoint diagnostic wavelengths for quantification. It is undertaken to fulfill three
purposes: (i) to supply vital information for simulating atmospheric conditions (e.g.,
absorption and scattering) needed for radiometrically correcting the atmospheric effects
on satellite images (see Chapter 3 for more details) and for retrieving in-atmosphere
trace constituents; (ii) to test the feasibility of remotely quantifying the target spectrally.
If the target is spectrally indistinct from each other or from the background on the
ground, then the quantification from remotely sensed data is unlikely to be successful.
There is no need to pursue the matter further; and (iii) to identify the most sensitivespectral wavelengths (range) over which the target is maximally responsive to the solar
radiation. The measured spectral response curves can reveal the spectral region over
which the target is the most distinctive from each other (e.g., at different
concentrations) and from other features. Such information can guide the selection of
spectral bands to serve as the most promising candidates for predicting the dependent
parameter’s value. The measured spectra are especially critical for hyperspectral data
that offer more than a hundred spectral bands to choose from. The derived spectral
response curves are analyzed to establish the relationship between the spectral indices
to be derived from the spectral data and the target of quantification, such as soil-heavy
metal content. The in situ measured spectra can also be used to quantify the target
directly, as in X-ray spectrophotometry of soil element content in the lab, but it does not
yield a spatial perspective of the content.
Spectral measurements are taken either in the laboratory or in the field. Laboratory
measurements can be controlled under exactly the same observation conditions
repeatedly, but the measured outcome may not replicate remotely sensed data because
the sensing environment has been altered and the samples are disturbed during their
removal from the field and transfer to the lab. In contrast, field measurements are more
authentic, but the measured results are subject to the influence of several environmental
factors, such as solar elevation and azimuth, changing solar intensity during the
measurement (especially when repeated measurements need to be carried out for
multiple targets over a large area), and the ambient settings of the target.
In the field, spectral measurements are usually taken using a portable hand-held
multispectral or hyperspectral spectrometer. A field spectroradiometer has a typical
spectral sensitivity range of 325–1075 nm and a spectral resolution of 3 nm. For
instance, the Adjustable speed drives (ASDs) FieldSpec 4 Hi-Res NG
spectroradiometer (Analytical Spectral Devices, Boulder, CO, USA) is able to measure
the hyperspectral reflectance and transmittance of surface sediments, soils, plants, water
bodies, and artificial targets at 1,875 wavelengths over the 350–2500 nm spectral range.
The spectral resolution ranges from 3 nm in the VNIR (350–1000 nm) region to 6 nm in
the SWIR (1000–2500 nm) region. They have a 25° field of view (FOV), for a viewing
area of 58 cm in diameter at the canopy level.
In the field, the sensor heads of the spectroradiometer should be mounted in a pole
and oriented squarely toward the target (e.g., the nadir viewing direction), centered over
the area of measurement to replicate the manner of airborne and space-borne sensing. If
the target is a crop field, the device should be stationed at 1.3 m above the ground (e.g.,
mounted on a commercial tripod). If the measured canopy is high, then the device
should be hoisted some height (e.g., 40 cm) above the target (Figure 1.4). Spectral
reflectance at the canopy level is much more difficult to measure as the hyperspectralradiometers must be mounted on an all-terrain sensor platform (Rundquist et al., 2014).
Both the downwelling and upwelling radiance of the canopy should be measured to
calculate its reflectance, which is best accomplished using a dual-radiometer system
connected with an optical fiber. One of them equipped with a 25° FOV optical fiber is
positioned downward to measure the upwelling radiance of the target. The other is
oriented skywards to simultaneously measure the incident irradiance of the target. The
down viewing spectrometer must be elevated to a sufficient height a few meters above
the canopy to cover a sizable sample area, such as 5.4 m above the canopy, resulting in
a sample area with a diameter of 2.4 m. A reflective reference panel is needed to
calibrate the measured solar radiance, and the measurements are repeated twice. First,
the surface reflectance is measured off the panel and then the radiance reflected off the
target. The measured radiance and irradiance are finally converted to reflectance
expressed as a ratio or percentage.FIGURE 1.4 Field spectral measurement of the vegetative canopy using a
spectroradiometer that must be hoisted some height above the target using a pole.
(Ouyang et al., 2013, open access.)1.2.2.2
Regardless of the target, all spectroradiometric measurements should take place under
clear sky conditions close to solar noon between 11:00 and 13:30 local time, when there
are minimal changes in the solar zenith angle and the solar radiation is at its peak
stability. The measurements may be repeated several times (e.g., >10) at a given spot.
All the spectra measured at the same site are averaged to yield the final site reflectance
spectrum. Field-measured spectral reflectance curves of the target have been analyzed
to demonstrate the feasibility of remote sensing quantification. In a strict sense, it is not
the quantification this book is about because it simply generates non-spatial information
on the best spectral ranges or bands to use and illustrates how they should be processed
to maximize the disparity of the quantitative value of the target. It is of limited utility as
spectral measurements are undertaken at specific spots and they fail to yield spatial
distribution of the quantitative value of the target because no imaging data are involved.
No matter how many spots are measured, they are at most point-based, unable to yield a
field view of the quantified parameter value, the biggest strength of remote sensing.
More importantly, the feasibility may not translate into reality due to the impact of the
atmosphere and the ambient sensing environment. Even if ground spectral
measurements enable the target to be quantified at a satisfactory accuracy, there is no
evidence to suggest that similar results can be replicated from actual space-borne data
as the spectral resolution of available satellite images may not match that of the hand￾held spectrometer. Besides, space-borne remote sensing data are prone to degradation in
quality by the atmosphere. In reality, the target also interacts with its surrounding
environs that further complicate the quantification, or even compromise the feasibility
of quantification. Additionally, the actual concentration of the target parameter may
have a spectral signal with a strength well below the radiometric resolution of satellite
images to reach the minimum quantifiable level.
Surface Quality Data
Surface quality data pertain to land, particularly soil. The former is related to land
physical properties (its biological properties will be covered in Section 1.2.3) instead of
its identity. These properties are either thermal or spectral, including temperature, heat
flux, and albedo. More parameters can be quantified about soil, the exposed earth, all
concerned with its content (Table 1.1). Temperature data are not normally collected in
the field as they are routinely recorded by the local meteorological stations, but they
can be sparse and absent from the study area. Albedo is measured using the method
described in the preceding section, but the measured results vary with the local land
cover and its properties (e.g., leaf area and orientation with broadleaf forest). Instead, it
is soil properties that are frequently measured in the field. Most soil parameters such astemperature, salinity, moisture, and SOC, are measurable at sampling points using
relevant instruments.
Soil moisture may be measured using a variety of meters via a probe inserted into
the soil at a few spots in a sampling plot, and the average reading is used as the final
measurement for this plot. In the measurement, the probe should be inserted into the top
soil layer only as its moisture can be remotely sensed. Soil salinity expressed as the
resistivity of soil may be measured using an earth resistivity meter in combination with
an electrical conductivity (EC) probe, with readings dependent on soil structure and
texture, moisture content, and the salinity of the ground water. Soil salt content is much
harder to measure, and has been quantified using only spectral data.
SOC can only be determined by sampling the soil near the surface (e.g., 0–10 cm),
usually at a few spots in the same sampling plot. The separately collected samples are
mixed to form one composite sample representing that particular plot. The samples are
then taken to the laboratory where they are air- or oven-dried, gently crushed, grounded
if necessary, and sieved at 2 mm. Total SOC may be analyzed using the VarioMax C-N
Analyzer. It requires the soil samples to be dry combusted at 950°C, and the measured
result is equivalent to total carbon if the soil parent material is non-calcareous. Soil
oxidizable carbon may be determined using the Walkley–Black method by treating the
samples with 10% HCl. If they show reaction, their carbonate content is analyzed using
the pressure-calcimeter method. Then, SOC is calculated as the difference between total
carbon and the inorganic carbon content. Alternatively, the SOC content may be
measured using a stable isotope mass spectrometer, and the outcome is expressed as
g•kg-1 or percentage (%).
Soil-heavy metal content is much more difficult to analyze in the lab than SOC due
to their minute concentrations. The collected soil samples may be similarly processed as
in measuring SOC. In addition, they may also be grounded and homogenized into a 32
mm mold to squeeze a tablet under 30-ton pressure. Commonly quantified heavy metals
in polluted soils include Cu, Pb, As, Hg, Cd, and Cr. Ca, Mg, and Pb have been
measured using reflectance or atomic-absorption spectroscopy because of their
differential absorption behavior. Other measurement methods include inductively
coupled plasma-atomic emission spectrometry, flame atomic-absorption spectrometry
(spectrophotometry), or SPECTRO xSORT X-ray fluorescence for determining the Cu
and Pb contents. As (arsenic) concentration can be analyzed using the silver
diethyldithiocarbamate photometric method, while the content of Hg can be measured
using atomic fluorescence spectrometry. In order for these methods to work, the soil
samples must be digested with an electric heating board acid (HCL-HNO-3-HCLO4).
Besides, they may be pre-processed by adding aqua regia (3:1 ratio of HCl to HNO3) to
decompose the greater part of any heavy metals present in the soil samples.1.2.3
1.2.3.1
Collection of Biophysical Data
Biophysical parameters and characteristics data are diverse in their nature, such as stand
height (H), canopy height (CH), canopy closure (CC), canopy diameter (CD), stem
density (SD), crown spread, and live crown length at the plot level. The difficulty and
complexity of their measurement in the field varies enormously. Some commonly
quantified biophysical parameters can be measured directly using a meter without
destroying the target of measurement while others have to be measured only indirectly
via other directly measureable parameters of vegetation. Certain parameters can be
measured only by harvesting the target of measurement, totally destroying it in the
process. For instance, tree height is easily determined using a Nikon Forestry 550 laser
rangefinder. Defined as the distance from the nearest root of a tree/shrub to its highest
visible part, CH can be easily measured using the same laser rangefinder to an accuracy
of ±0.3 m for distance and ±0.25° for inclination. CC can be measured using the
Convex Spherical Crown Densitometer, and CD may be obtained by averaging two
perpendicular projected distances of a tree. In the field, these parameters are measured
for a few representative trees in a sampling plot, and they are averaged to yield mean
results for each of the parameters. In comparison to these parameters, leaf area index
(LAI), photosynthetically active radiation (PAR) or the fraction absorbed by vegetation
(FPAR), chlorophyll content, and biomass are much thornier to measure. Their
measurement is detailed below.
LAI
LAI is defined as the portion of leaf-occupied area vertically projected from the canopy
to the horizon over a fully vegetated area. It is indicative of the surface area available
for plants to exchange light, water, and CO2 with the environment. This dimensionless
index quantitatively indicates the number of leaves in the canopy within unit ground
area, and is expressed as a ratio with a value between 0 and 10. A value of 1 means an
equal amount of leaf area to ground area. LAI can be measured using either direct or
indirect methods (Chason et al., 1991). Grounded on the leaf area per unit dry leaf
mass, the direct method is expensive, laborious, inefficient, and destructive as the plants
have to be harvested within a sampling plot for later indoor measurement based on
fresh and dry matter mass. LAI is calculated from the total dry matter mass of the
harvested sample and the specific leaf area. This method is rather restrictive and time￾consuming, suitable for grassland only. In contrast, the indirect or semi-indirect method
of measurement is non-destructive as the measurement is based on the amount of light
reflected off the target of measurement that can be easily and accurately implemented
non-destructively using several instruments, such as LI-COR 3000C leaf area meter,and LI-COR LAI-2200 and CI-100 Plant Canopy Analyzer. LI-COR3000C measures
leaf area on intact plants via rectangular approximation. It comprises two parts, a
scanning head and a readout control unit (console) that are connected to each other via
a fiber optic cable. Data concerning leaf length, mean width, maximum width, area, and
accumulated area are logged as the leaf is scanned by the head. Their readings are
summed in a secondary register and stored locally in the console, or transferred to a
computer via an interface software that permits real-time data collection in the field.
This device can measure LAI at the leaf level only. Canopy-level measurements can be
gathered using the LI-COR LAI-2200 analyzer. It scans leaves with its sensor head
projecting a nearly hemispheric view onto five concentric silicon ring detectors (Pepper
et al., 1998). The optical sensor is connected to a data logger that records ring detector
readings of above- and below-canopy light conditions using built-in software. The LAI￾2000 canopy analyzer measures transmittance at five solar zenith angles
simultaneously, each elementary sampling unit is assigned one LAI value, obtained as a
statistical mean of multiple measurements (including multiple data readings, and a few
replica) with standard errors ranging estimation of LAI and average leaf angle (Verrelst
et al., 2015b).
Ideally, LAI should be measured under conditions of uniformly overcast skies to
preclude underestimation. Alternatively, measurements may be taken at sunset or
sunrise. If measurements are obtained during daylight on sunny days, the lens must be
shielded using a 90° view cap to restrict direct sunlight from striking the optical sensor.
In measuring LAI in the field, the instrument should be deployed at approximately 0.9–
1.3 m below the bottom of the tree crown at easily accessible locations. All
measurements are taken at the base of each tree bole by leveling the instrument’s probe
placed on top of a stake in each cardinal direction, 20 cm from the base of the bole and
15 cm above the ground (Darvishzadeh et al., 2019). The standard protocol is to log one
reference (above the canopy) reading in the nearest open field, and five below-canopy
readings in each cardinal direction for a full tree crown in a plot. The below-crown
readings are recorded at 90° to each other. The measurement is repeated after the crown
is reduced by ten 0.016 m3 samples of leaves, and finally after the removal of a total of
twenty 0.016 m3 samples (Peper et al., 1998). Throughout the measurement session,
illumination conditions for the above- and below-canopy readings should be maintained
maximally constant.
As the name implies, CI-100 Plant Canopy Analyzer does not measure LAI directly.
Instead, it generates the LAI measurement by analyzing high-resolution images taken
with a digital camera equipped with a 150° fish-eye lens positioned at the end of a
probe to scan plant canopies. Up to 32 measurements can be acquired in the field before
downloading the images. They may be adjusted based on the crown or canopy being1.2.3.2
measured. The instrument is operational under sunny, cloudy, or partly cloudy sky
conditions. The fraction of sky (solar beam transmission coefficient) visible in each
sector is analyzed from the images after they have been partitioned into a user-defined
number of zenith and azimuthal divisions by tallying the blue-colored pixels in a sector.
After all sectors have been analyzed, solar beam transmission coefficients are averaged
by zenith division, from which LAI is calculated for the selected zenith angles instantly,
together with PAR and extinction coefficient.
Canopy and solar radiations may be analyzed using WinSCANOPY (Regent
Instruments Inc., Quebec, Canada) and HemiView (Delta-T Devices Ltd, Cambridge,
UK). They take colored hemispherical images as the input, from which LAI, gap
fraction, canopy openness, site factors, Normalized Difference Vegetation Index
(NDVI), and much more are derived without requiring above-canopy measurements.
The images are taken by a digital camera with a calibrated fish-eye lens of a narrower
view angle in the field, and processed externally using specific software to derive leaf￾angle distribution and mean leaf angle, angular distribution of gap frequencies, and site
factors (direct, diffuse, and global) (Bréda, 2003). It can also predict radiation values
beneath the canopy. Most of the outputs are available by sky sector or aggregated into a
single overall whole sky or annual value.
PAR (FPAR) Measurements
Photosynthesis is a vital process to plant and crop productivity. It can appear in two
forms, PAR and FPAR. The irradiance for a specific spectral function such as PAR can
be determined using a suitably filtered radiometer or spectroradiometer. The former
produces a single integral reading based on the spectral sensitivity of its detector, filter,
and the calibration source employed, whereas the latter yields full spectral power
distribution information for the light source measured. The aforementioned device for
measuring LAI can also be used to measure PAR, in addition to portable plant and tree
canopy layer analyzers or meters, such as CI-100, LI-COR LAI-2000, and AccuPAR.
They have been introduced already, so are not repeated here except the battery-operated
linear AccuPAR Ceptometer. Designed to measure PAR in the 400–700 nm spectral
region, it comprises an integral, programmable data logger and 80 independent sensors
arranged at 1 cm2 spacing along the entire attached probe. The sensors along the probe
are partitioned into groups, each containing 16 equidistant calibrated photodiodes or
sensors. They mimic the clumping effects of a canopy. Data collected by each group of
sensors are averaged to produce LAI. Also logged are latitude, longitude, date, and
local time at each sampling point for calculating zenith angle automatically using the
default leaf distribution parameter of 1.0. Fractional beam data (the amount of diffuse
and direct beam radiation reaching the canopy) are logged for each set of above- and1.2.3.3
below-crown measurements. Developed and optimized for low and regular canopies,
AccuPAR measures fractional beam from which LAI is calculated via ellipsoidal
inversion. This device works with alternative above- and below-canopy PAR
measurements using only a linear probe. Above-crown data are recorded by walking
into an open field in close proximity to the trees of interest. For below-crown
measurements, the instrument must be deployed on the top of stakes. Gap fraction and
LAI are then calculated for each direction (Peper and McPherson, 1998). To increase
the accuracy of the measured LAI, AccuPAR may be sited at five locations within each
plot (center, and the central point of the north-east, north-west, south-west, and south￾east quadrants of the plot) and the five measurements are averaged to derive the final
output for the plot.
Another suitable measurement device is the small diameter LI-COR 191SA line
quantum sensor that can be deployed easily to measure PAR within a plant canopy. This
instrument has a light quantum sensing area of 1 m × 12.7 mm over the sensible
wavelength of 400–700 nm, and a typical sensitivity of 7 µA per 1000 µmol•s-1•m-2.
The measured results are spatially averaged over the 1 m scope of sensing area. The
measurement is composed of four fractions of PAR: PAR canopy incident (PARci),
PAR canopy reflection (PARcr), PAR ground incident (PARgi), and PAR ground
reflection (PARgr). The portion of absorbed PAR is calculated as:
(1.5)
Above-canopy measurements are generated by positioning the device 0.5 m above the
canopy, or 0.15 m above the ground to measure under-canopy PAR. In measuring
planted crops, both ends of the probe sensing part should aim at the middle position
between rows and the probe midpoint at the top of plant row (Tan et al., 2018). In either
measurement, the sensor should be horizontally leveled as judged by the midway
position of the spirit level bubble.
Chlorophyll Measurement
Chlorophyll content (CC) is one of the critical leaf biochemical properties affecting
photosynthetic activities. Chlorophyll pigments are valuable for plants’ energy
conversion. Leaf chlorophyll effectively manifests plant growth and nutritional state.
Changes in plant CC is indicative of plant health and environmental stress. This
biochemical variable is essential for quantifying carbon and water fluxes, primary
productivity, and light use efficiency (LUE). Chl-a can be measured using several
instruments at the leaf level in the field, including FieldScout CM 1000 Chlorophyll
Meter, CCM-300 Chlorophyll Content Meter, LI-600 (600N), and SPAD-502Plus. CM1000 can easily and quickly determine chlorophyll content of plant leaves and turf grass
using the “point en shoot” method accurately and directly. It functions by measuring
reflectance of the ambient and reflected light (700–840 nm) and by calculating the
relative chlorophyll index over a conical viewing area between 30 and 180 cm at a
minimum distance of 30 cm from lens. Since the coordinates of the samples have to be
logged, with this device, stamp GPS coordinates are easily measured in combination
with a GPS device if connected to the CM 1000 software (ST-2950S). CCM-300 can
measure CC in plants and crops accurately, reliably, repeatably, readily, and non￾destructively. It is the most efficient meter owing to its ample onboard data storage,
integral GPS module, and portability (e.g., hand-held design). If integral with GPS, it
can store 160,000 datasets and perform data averaging.
LI-600 and LI-600N are compact photometers with Pulse-Amplitude Modulation
fluorometers that simultaneously measure stomatal conductance, Chl-a fluorescence,
and leaf angle over the same leaf or needle for various leaf sizes and morphologies,
including many needles and narrow leaf grasses. They are embedded with a GPS
receiver to track sampled location and an accelerometer/magnetometer to log data
essential to compute leaf’s angle of incidence to the sun based on measured heading,
pitch, and roll, and record 3D coordinates in seconds. While LI-600 is ideal for quickly
measuring Chl-a fluorescence and stomatal conductance on plants in ambient
conditions, LI-600N does so only in light-controlled environments. LI-600 can also
rapidly screen up to 200 samples per hour to identify candidates for detailed
measurements. It is easy to use but has only the basic configuration options. In contrast,
LI-600N allows multiple independent controls, including light, CO2, H2O, and
temperature. When used jointly, LI-600 and LI-600N yield highly complementary data.
For example, the LI-600 and LI-600N can be used to screen a large population and the
LI-6800 can be used to measure selected individuals from that population in more
detail.
The SPAD-502Plus is a compact, portable, light instrument for quickly and readily
measuring the CC of leaves and has been widely used to optimize the timing and
quantity of fertilization to improve crop yields. It quickly and easily measures the CC
of plant leaves without damaging them. Instead of yielding the desired actual amounts
of chlorophyll per unit area of leaf tissue directly, the SPAD-502 meter provides the
data only in arbitrary units. Thus, the measured results have to be standardized to
determine the amount of chlorophyll in a leaf sample. The SPAD readings are
transformed to absolute leaf CCs according to the following equation (Markwell et al.,
1995):
(1.6)1.2.3.4
Apart from direct measurement, leaf CC can also be determined via spectral
measurement (R750-800/R710-730 – 1) using a portable spectroradiometer at 380–2500 nm
normal to the canopy with an FOV of 25° and a distance of about 1.3 m above the
ground (Gitels et al., 2005). This result can be updated to the canopy level via LAI.
If measured at the leaf level, crop leaves must be collected in situ during the
growing season. In case of trees, sample leaves (needles) are collected from two to
three branches of representative trees in each sampling plot. In order to ensure spatial
representativeness, a certain number of leaves (e.g., 20) must be randomly selected
from each plot. They must total around 30 and have a sufficiently large size (e.g., 90
m×90 m) to ensure the existence of a pure pixel in the plot. Multiple measurements are
usually taken and averaged to iron out the random influence of the ambient
environment. In each plot, a crossbow (e.g., Excalibur Matrix 310 crossbow) may be
used to sample leaves/shoots from the top-of-canopy mature sunlit part. An average of
two to three sunlit leaves are collected from a branch by shooting at it with an arrow
attached to a fishing line (Ali et al., 2016). Leaves/shoots CC is immediately measured
using one of the aforementioned CC meters and averaged to determine leaf CC per plot.
Canopy-level CC of each plot is calculated by multiplying the plot’s average canopy
CC by LAI.
Biomass Measurement
Plant biomass can be classified as aboveground and below-ground. AGB refers to the
biomass stored in live plants growing above the ground that is visible to the human
eyes. It includes the biomass of trunks, branches, twigs, and leaves of plants, but
excludes roots and dead wood, in contrast to below-ground biomass. As indicated in
Section 1.2.1.2, the collection of AGB is always based on plot sampling. In addition to
being arduous and time-consuming, plot sampling can also be destructive as the grasses
must be clipped to determine their weight and fresh AGB. The aboveground live plant
material in each sampling plot is harvested, usually with the assistance of a pair of
scissor. The clipped fresh grass in the field is weighted on site. In the laboratory it may
be necessary to sort the fractions of leaves and stalks and weight them separately. The
collected plant material may be desiccated by drying at 105°C in an oven for 24 h, and
weighted again to find the dry biomass. The total water content of both fractions is
calculated by subtracting the dry weights from the previously measured fresh weights
(Vohland et al., 2010).
The workload of sampling biomass and the destruction it causes to plants may be
minimized by adopting an appropriate plot size that can be subdivided into smaller
quadrants (Figure 1.3c). Another way of minimizing the destruction and expediting the
sampling process is to measure the biophysical parameters of the target, from which the1.2.4
targeted variable is calculated. For instance, it is impossible to harvest trees to weigh
their biomass and carbon that becomes known only after the felled trees have been
combusted. Such a destructive sampling is not permissible, hence is replaced with
measuring tree height and diameter at breast height (DBH). Thus, above-ground
biomass sampling becomes measurements of tree structural parameters in a sampling
plot. The measured tree biophysical parameters are then converted to wood volume and
carbon content using well-established equations (see Section 6.7.3.1 for details).
Given the much larger sample plot size of trees, it is impossible to sample every tree
in a plot. Criteria must be set for minimal tree size. This size depends on the age of the
tree. In the presence of huge trees weighing tens of tons, small trees can be safely
ignored as the inaccuracy in estimating their weight from their biophysical parameters
can easily overwhelm the total biomass of all small trees combined in the same plot.
After the AGB of all trees have been individually calculated from the appropriate
allometric equation, the calculated AGB of individual trees and shrubs is then summed
and aggregated to the plot level to be correlated with their corresponding pixel’s value
on the concurrently acquired satellite image.
Whatever field data are collected, they must be processed indoor to prepare for the
subsequent quantification and presented in the proper geospatial format such as a point
shapefile in a GIS so that they can be overlaid with the remotely sensed data (see
Chapter 2) to identify their corresponding properties on the images in preparation for
constructing the quantification model. The type, difficulty, and requirements of data
processing vary with the nature of the collected samples.
Water Data
In comparison with in situ terrestrial data collection, sampling of water properties has
both its strengths and limitations. It is advantageous in that all samples can be collected
at points or several depths that can be completed within minutes. It is disadvantaged
and more challenging because the sampling points are located offshore, accessible only
via a sailing vessel. It may introduce an extra influence on water through hydrographic
perturbations, such as ship wake, ship hull and propeller-induced mixing, and bow
wave. In measuring the reflectance of offshore waters, the influence of the ship must be
minimized. The measured radiance stems from a few features. Apart from surface
reflection, reflection by surface waves, scattering and absorption by in-water
substances, and potential bottom reflection in case the focus is on in-water constituents,
the measured outcome is also subject to the influence of the nearby shore and the
ambient environment (e.g., winds and waves in an open sea), so it requires more care
and preparation.1.2.4.1
Water parameters fall into two broad categories of general water parameters and
water optical properties. General water parameters that can be remotely quantified are
numerous in number and widely ranging in their nature and species. They can be further
broken down to water quality and water content parameters that are measured
differently. Standard water quality parameters such as temperature, pH, conductivity,
salinity, dissolved oxygen, turbidity or transparency (Secchi depth), can be measured
using a submersible multi-parameter sonde (YSI 6600V2) together with in-vivo Chl-a
and colored dissolved organic matter (CDOM) levels spectrophotometrically at each
anchored station. The determination of in-water constituents is so complex that it will
be deferred to Section 1.2.4.4. Optical properties can be further differentiated into
inherent and apparent. Inherent optical properties (IOPs) are properties of the medium
not affected by the ambient light field, such as absorbance, transmittance, and scattering
coefficient. All of them can be measured in a controlled environment, together with the
concentration of in-water substances. Apparent optical properties (AOPs) have a value
that varies with the medium of light propagation and the viewing geometry of the
radiance distribution, in addition to IOPs. They behave with sufficient regularity and
stability that allow a water body to be studied, such as downwelling and upwelling
radiance, diffuse attenuation, and spectral irradiance reflectance. AOPs must be
measured on site. In order to quantify in-water substances, it is imperative to measure
water spectral behavior. The most commonly measured property is water-leaving
radiance.
Water-leaving Radiance
Although the same measurement protocol of the terrestrial sphere can be applied to the
aquatic environment, caution needs to be exercised to avoid disturbance to water and
reflection off water surface. Besides, water-leaving radiance is more complex to
measure as both normalized upwelling radiance and downwelling irradiance must be
measured to calculate remote sensing reflectance (Rrs). The same visible and near￾infrared (VNIR) spectroradiometer as in terrestrial remote sensing described in Section
1.2.2 can be used, such as the FieldSpec Spectroradiometer with a wavelength range of
350–1075 nm. Hyperspectral reflectance of water may be measured using an ASD field
spectrometer at a spectral resolution of 3 nm, and a sampling interval of 1 nm over the
spectral region of 350–1050 nm.
Water-leaving radiance (Lw) cannot be measured directly because the upwelling
radiance above the sea surface also encompasses solar and sky radiance reflected from
the surface of water (Lsr). It does not contain any information on the seawater content,
so has to be eliminated from Lw after it has been measured. Lw may be measured
indirectly by deploying a sensing device above water or by submerging it into waterunder clear sky conditions. In the “above-water method” (Tang et al., 2004), the water
surface spectral reflectance is measured by a sensor connected with a fiber optic cable.
The sensor should be positioned at nadir, on a mount extending approximately 1 m off
the ship (Figure 1.5a). Spectral measurement should be confined to an azimuth angle of
90°–135° from the sun and a nadir viewing angle <90°, preferably between 30° and 45°
(Figure 1.6a) so as to effectively minimize the impact of the ship-cast shadow and
direct solar radiance (Zhang et al., 2016). The upwelling radiance of the water surface
Lsr(λ, 0+) is measured slightly (e.g., 0.3 m) above the water surface. After the water
radiance measurement, the spectrometer is immediately rotated upward to >90° up to
120° to measure the downwelling sky radiance Lsky(λ) at the same viewing angle as that
adopted in measuring upwelling radiance (Figure 1.6b). The measured upwelling
radiance has to be calibrated with the assistance of the downwelling radiance [Lp(λ, 0+)]
measurements from the reference panel regarded as an optical standard. The total
integration time of measurements should be maintained sufficiently long (e.g., >10s) to
enable fluctuations in the reflectance with surface waves to cancel out. Measurements
are carried out in multiple duplicates repeatedly and averaged to derive the final
outcome.FIGURE 1.5 Two methods of measuring water-leaving radiance. (a) The onboard above￾water method (Lee et al., 2021, open access); (b) The skylight blocking method that
measures only water upwelling radiance. (Shang et al., 2017, reprinted with permission
from © Optical Society of America.)FIGURE 1.6 The reference system of nadir and azimuth viewing angle centered on the
water surface (black dot). (a) Side view showing the viewing nadir angle (θ) referenced
from downward vertical axis: θ < 90° for measuring upwellling radiance, >90° for
downwelling radiance (from sky and sun); (b) Top-down view of azimuth viewing angle
(φ) and relative azimuth viewing angle (Δφ) referenced from viewing direction
clockwise from North and the sun, respectively.(Ruddick et al., 2019, open access.)
In hydrospheric measurements, it is preferable to use spectroradiometers equipped with
two sensors for simultaneously measuring upwelling and downwelling radiance of the
target to increase the measurement efficiency and to minimize the solar radiance
variation, such as the Ocean Optics USB2000 radiometer. This dual-fiber optic system
has a spectral sensitivity range of 400–1100 nm (spectral resolution: ⁓1.5 nm). The
measured radiance and irradiance are then converted to percent reflectance.
The on-water method measures the upwelling radiance using an optical fiber
attached to an extendable pole with the tip submerged just slightly underneath the waterbut oriented in the nadir direction so as to minimize the difference between the
upwelling radiance at depth z and the surface. The device may be tethered to a ship or a
fixed offshore platform or moored buoy, or untethered and horizontally drifting. At a
given sampling site, the spectral signal is measured several times to minimize the
signal-to-noise ratio. This method is subject to the interference from impurities in the
water column, but does not require measuring skylight irradiance (Lsky) as it has been
blocked (Figure 1.5b). It is particularly suited to stratified water, shallow bottoms, or
seagrass/kelp beds.
The underwater methods of measurement can be differentiated as fixed-depth or
vertical profiling. In the fixed-depth method, a radiometer is deployed underwater and
attached to permanent floating structures, to measure nadir upwelling radiance at a
minimum of two fixed depths z1 and z2 (Figure 1.7a). z1 should be set maximally
shallow to reduce errors due to propagation to the surface and the chances, subject to
the sea state at the measurement spot. z2 should differ from z1 maximally to reduce the
uncertainty of the derived diffuse attenuation coefficient for upwelling radiance and
heterogeneity of water column over the measurement depth. The nadir water-leaving
radiance (Lwn) is calculated by first estimating the nadir upwelling radiance just
beneath the water surface, Lun(0−), by extrapolating from, preferably, the two
shallowest measurements at depths of z1 and z2 under the assumption that the depth
variation of Lun(z) between z1 and z2 is exponential with a constant diffuse attenuation
coefficient for upwelling radiance. For this method to work, the water downwelling
irradiance has to be measured to calculate Rrs.FIGURE 1.7 Schematic of two underwater water-leaving radiance measurement methods.
(a) fixed-depth; (b) profiling using a typical free-fall radiometer. (Ruddick et al., 2019,
open access.)
The vertical profiling method is suitable for free-fall radiometers deployed from ships
and fixed platforms (Figure 1.7b). A fixed platform enables the measurements to be
automated and unsupervised. In principle, this method is identical to the fixed-depth
method except that measurements are taken at slightly different times at a range of
depths between z1 and z2 for estimating the vertical variation of Lun(z). The radiance1.2.4.2
1.2.4.3
measurements have to be corrected for variations in above-water downwelling
irradiance. If vertical profile radiometry is measured from winches attached to ships, it
is important to avoid optical (shadow/reflection) and hydrographic perturbations from
the ship, and vertical perturbation of the device. Measurements are made from the
ship’s stern with the sun’s relative bearing aft of the beam at a minimum distance of
1.5/KLu from the ship or further with large vessels (Ruddick et al., 2019).
Absorbance and Scattering Coefficient
In situ measurement of water absorbance [α(λ)] and beam attenuation [c(λ)] can be
achieved using spectrophotometers, such as ac-s 5000 Spectral Absorption and
Attenuation Sensor. It uses a linear variable filter monochromator, and has 80
wavelength outputs from 400 to 700 nm at 4 nm resolution (15 nm bandpass). It
features dual path of 10 or 25 cm in length, flow-through measurement, and improved
SNR α(λ) and c(λ) for λ<550 nm, improving data quality in the blue region. Beam
attenuation c(λ) has an acceptance angle = 0.93°, 80±5 wavelengths of absorption, and
attenuation from 400 nm to 730 nm. It can be deployed at random spots to measure the
profile of absorption and attenuation or at a fixed spot to log data automatically over a
period for longitudinal change studies, but the results may need to be calibrated for
temperature, salinity, and scattering effects to improve the accuracy if deployed in
seawater.
Scattering coefficient can be measured using the ECO BB9 Backscattering Sensor
that has a modular suite of sensors for measuring bio-optical and physical parameters of
water in situ. The WET Labs BB9 resolves the volume scattering coefficient (β) at nine
wavelengths. The instrument illuminates a volume of water using modulated LEDs and
detects scattered light at an acceptance angle of 124° from the source beam. A centroid
angle of 117° is normally maintained to minimize the error of the extrapolated total
backscattering coefficient. Measurements can be made continuously by deploying the
instrument from a cruising ship, or solely at a fixed position for cross-sectional
measurement. The deployed sensor should not be oriented towards the sun or other
strong sources of light. If properly configured (e.g., with the necessary sensors
included), it allows temperature, salinity, depth, absorption/attenuation, and Chl,
phycocyanin, and CDOM fluorescence to be measured concurrently. BB9 outputs the
measured scattering coefficient as β(θ,λ) in (m•sr)-1. This volume scattering also
includes water molecule scatting. It must be subtracted from the calibrated output
β(117°,λ) to derive the scattering of suspended sediments. It can also be converted to
the backscattering coefficient bbp(λ) by multiplying it by a coefficient.
Indoor Optical MeasurementsThe above device and methods of measurement are applicable to in situ measurement
of mostly AOPs of water bodies. They are unable to isolate the IOPs of individual in￾water substances, such as absorption coefficients of CDOM [αg(440)], total sediment
matter or TSM [αp(λ)], phytoplankton particles [αph(λ)], and non-phytoplankton
particles [αd(λ)] when they co-exist in the same water. These properties must be
measured in a controlled environment from water samples collected at representative
spots at ⁓0.5 m below the surface and several other depths to capture the vertical
variability of the target parameters. The collected samples should be stored in dark at
the same temperature as that of the field to prevent photo-degradation. Samples should
be transferred immediately from the in situ sampling bottles to clean bottles using clean
Tygon tubing. Sample preparation and absorption spectra run should immediately
follow sampling, or at most a few hours after vertical profiling has been completed
(Mitchell et al., 2002).
In the laboratory, the IOP parameters (e.g., absorption coefficients) are determined
by filtering and analyzing the collected water samples. The determination of the
spectral absorption by soluble material (αg) dissolved in seawater must be preceded by
removal of particulate matter from the seawater samples via filtration. Water samples
are filtered onto a Whatman fiberglass GF/F filter at a low vacuum pressure. Samples
may be prepared using 0.2 µm membrane filters that have been soaked in 10% HCl and
rinsed copiously with pure water and a small aliquot of sample to prevent
contamination (Mitchell et al., 2002). The prepared samples are then analyzed using a
spectrophotometer. Spectral measurements of absorption of the filtrate by water
constituents is calculated from the optical density of the filtrate measured relative to
distilled water at λ = 440 nm and a path length of 5 cm. The measurements of
phytoplankton pigment absorption [αph(λ)] and non-algal particle absorption [αNAP(λ)]
may follow similar quantitative filter technique.
One method of measuring the absorption coefficients of CDOM is to use a
Shimadzu UV-2550 spectrophotometer in the 240–800 nm region. Anther device for the
measurement includes the compact, mid-range UV-2600i/2700i UV spectrophotometer
(Shimadzu, Japan). The UV-2600i is a single monochromator system whose sensible
wavelength range can be extended to 1400 nm using the optional integrating sphere. As
a double monochromator system, the UV-2700i is optimally positioned for measuring
waters of low transmittance. Both can be used to determine the quantity of total
phosphorus (TP) and total nitrogen (TN) in rivers, lakes, and marshes, iron, copper,
arsenic, ammonia, and other substances in water, as well as turbidity. Nevertheless, they
cannot be used to isolate the absorption by one substance from that of another, which
requires filtering of water samples in the laboratory to be discussed in the following
section.1.2.4.4 Water Content Measurement
Broadly, in-water substances fall into two types of inorganic and organic. The former
includes sediments (total, inorganic, and organic). Organic matters are more diverse,
including Chl-a, phytoplankton, dissolved organic carbon, total organic carbon (TOC),
and eutrophication-related nutrients (TN and TP). Organic matters are analyzed
differently from inorganic particles. Their measurement requires the analysis of water
samples (150–250 ml, but up to 5 l, depending on how it will be used) collected using
Niskin or similar bottles, concurrently with the surface in-water optical measurements.
The sample bottles used to collect sample filtrate must have been thoroughly cleaned to
remove any potential organic contaminants, sequentially soaked, and rinsed in mild
detergent (10% HCl) with a final thorough rinse in purified water. Even plastic caps
must be rinsed with 10% HCl twice with Alpha-Q, then dry at 70°C for 4–6 h. Bottles
must be covered with aluminum foil and combusted at 450°C for 4–6 h.
Prior to any measurement, the water samples must be properly prepared by filtering
them using the binder-free and combustible Whatman GF/F filter with a nominal pore
diameter of 0.7 µm for measuring particle absorption. Finer particle (0.22–0.70 µm)
absorption can be measured by reducing the pore size to 0.22 µm using the 0.22 µm
Millipore cellulose acetate membrane filter. For Chl-a analysis, the collected water
samples must be filtered using 0.45-μm cellulose acetate filters (Millipore), or 47-mm￾diameter Whatman fiberglass GF/F filters with 0.7-μm pores for determining CDOM
concentrations. The Chl-a concentration may be determined using a Shimadzu UV-2550
device, or a bench-top TD-700 fluorometer equipped with a chlorophyll optical kit
(340–500 nm excitation filter and emission filter 665 nm). The last device measures
Pheophytin-corrected Chl-a fluorometrically at two wavelengths of 750 and 665 nm.
The collected water samples (150 to 200 ml) have to be filtered at 0.45 μm pore size
using a filtration manifold. The filters are stored in a falcon tube and kept frozen in a
dark freezer (-9° C) hours after sample collection (but not frozen for longer than three
months). Prior to analysis, filters are dissolved in 10 ml 90% buffered acetone and
allowed to extract in a dark freezer (-9°C) for at least 24–48 hours (Li et al., 2010). All
steps in the Chl-a extraction process should be taken in subdued light conditions.
The content of sediment matter (both organic and inorganic), detritus, and
phytoplankton is determined gravimetrically. The concentrations of TSM, OSM
(organic), and ISM (inorganic) can be determined by simply weighing the solids left
behind the filters. TSM concentration is obtained by dividing the weight by the volume
of the filtered water sample. The previously filtered water is then refiltered through pre￾combusted Whatman GF/F filters at 450°C for hours to remove dissolved organic
matter in suspension, and then dried at 105°C for hours and weighed again to obtain the1.2.5
ISM. OSM is the difference between TSM and ISM obtainable via their subtraction
(Zhang et al., 2016).
Second-hand Ancillary Data
Second-hand data about the target are supplied by the third party either through ground￾based observations or via regional statistics. The former applies to atmospheric
parameters that have been monitored through meteorological stations around the world.
Rainfall and temperature data important to atmospheric correction models are
continuously monitored at these stations. Thus, these data can always be synchronized
with periodically recorded remote sensing data over the study area. Special air-quality
parameters such as aerosols have to be measured at purposely built observation stations,
for instance, the AErosol RObotic NETwork (AERONET), a federated ground-based
network established by NASA and PHOtometrie pour le Traitement Operationnel de
Normalisation Satellitaire. Nevertheless, the density of the established monitoring
stations (sampling sites) may not be high enough to meet the needs of local-scale
studies. Thus, these fixed-location data are ideally suitable for quantification at the
global scale that ensures the enclosure of a sufficient number of stations in the study
area.
Some biophysical parameters are impossible to gather in the field at a lead time of a
few months, such as crop yield. It becomes known only after the crop has been
harvested for financial gains not scientific endeavor except in purposely designed
experimental plots. At the regional scale yield data have to be collected from the third
party, such as relevant government departments, usually enumerated by sub-regions. On
the other hand, the prediction of crop yield is meaningful and valuable only at a lead
time of a few months. The absence of the ground truth data means that it is impossible
to validate the results for a single region or in real-time quantitative sensing. For some
other parameters, such as SO2 concentration in a volcano plume, no ground truth data
can be gathered at all.
Auxiliary data differ from second-hand data in that they do not pertain to the target
of quantification. Instead, they may exert an influence on the target. Although remotely
sensed data serve as the backbone of the input needed in most quantification cases, they
are by no means the exclusive source of data. The quantification of those features that
have a geographic component (e.g., vegetation biomass on different slopes) also
benefits from the consideration of co-variables (e.g., climate, topographic, and
hydrologic) that might help to improve quantification accuracy and reliability. For
instance, crop yield may be affected by temperature, soil fertility, topography, and
irrigation. These factors are related to crop health and potentially grain yield, even1.3
1.3.1
though their exact influence remains mostly unknown. Such ancillary data may be
collected from remote sensing data (product), but their spatial scale and resolution must
match those of the primary data.
COMMON PREDICTOR VARIABLES
Remote sensing-based quantification relies on the radiometric information recorded in
spectral bands, either multispectral or hyperspectral. They are the default predictor
variables in the estimation model. In addition to the spectral data of individual bands,
spectral values in multiple bands may be combined in various ways to derive countless
predictor variables in a manner known as vegetation indexing. It is effective at
estimating vegetation properties. Vegetation indices are preferable to individual bands
because they take advantage of the differential spectral behavior (e.g., both reflection
and absorption) of the target. Given the drastic variation of vegetation spectral behavior
with wavelength, vegetation indexing is a widespread practice to quantify certain
parameters of the environment from multiple bands. Since these bands contain unique
spectral information, they are superior to individual bands in enhancing the spectral
subtlety of vegetation health and conditions. The use of multiple bands also enriches the
evidence of decision making. Initially developed for sensing vegetation, they have
found wider applications beyond vegetation, such as soil salinity and moisture content.
All vegetation indices strive to accentuate the difference between red and NIR
reflectance of ground features. The indices can not only indicate the amount of
vegetation (e.g., %cover, LAI, biomass, etc.), and distinguish soil and vegetation, but
also suppress the atmospheric and topographic effects if present. Some of the
commonly used indices are introduced here. Special indices applicable to a particular
sphere of application (e.g., vegetation canopy chlorophyll content) are presented in
relevant sections elsewhere. Since drone images have only four spectral bands, the
indices produced from them are quite different from their space-borne counterparts, and
they are discussed separately below.
From Drone Images
Drone images captured using ordinary cameras have a notorious spectral paucity.
Lacking the critical infrared bands, drone images do not allow many VIs to be derived
from RGB images. The commonly used indices are visible band VIs such as Green Red
Vegetation Index or GredVI (Adamsen et al., 1999), GreenRVI (Tucker, 1979), and
RGBVI (VIRGB) (Lussem et al., 2018). As illustrated in Table 1.3, of the three bands,
the green and red ones are used more widely than the blue band of a shorter1.3.2
wavelength. They are either differenced, ratioed, or ratioed after differencing, as in
deriving the Normalized Green Red Difference Index (GRVI). Simple to calculate,
drone-derived VIs are effective at revealing the vegetation fraction of crop fields
(Torres-Sánchez et al., 2014). In monitoring grassland forage yield, RGB-based GRVI
is correlated closely to forage yield (Lussem et al., 2018).
Table 1.3
Vegetation indices commonly derived from drone RGB bands that have been used to estimate grassland AGB
VIRGB Name Formula References
GRedVI Green Red Vegetation Index Adamsen et al. (1999)
GreenRVI Green Red Vegetation Index Tucker (1979)
MGRVI
Modified Green Red Vegetation
Index Bendig et al. (2015)
RGBVI Red Green Blue Vegetation Index Bendig et al. (2015)
WI Woebbecke index Woebbecke et al. (1995)
IKAW Kawashima index
Kawashima and Nakatani
(1998)
GLI Green leaf index Louhaichi et al. (2001)
VARI
Visible atmospherically resistance
index Gitelson et al. (2002)
From Space-borne Bands
Compared to drone images, satellite images with more bands allow many more spectral
indices to be calculated for quantifying diverse targets. So far, a vast range of spectral
indices have been developed for different environments (Table 1.4). These vegetation
indices fall into three types: simple and intrinsic indices, indices using a soil line (red
edge), and atmospherically corrected indices. The first group of indices is exemplified
by the difference vegetation index (DVI) () (Crippen, 1990). It is sensitive to the
amount of vegetation, and can distinguish between vegetation and soil, but cannot
differentiate reflectance and radiance caused by the atmosphere or shadows, so it has
been modified to derive the Weighted Difference Vegetation Index (Clevers, 1989)
calculated as follows:
(1.7)where , , and = atmospherically corrected reflectance of the near-infrared (λ = 841–876
nm), red (λ = 620–670 nm), and blue (λ = 459–479 nm) bands of the image in use. Both
DVI and WDVI cannot account for the atmospheric effects, a deficiency that can be
rectified by simple ratio indices. The simplest is called the Simple Ratio (SR) or just
Ratio Vegetation Index (RVI):
(1.8)
SR value is high for vegetation, low for other covers such as soil, ice, and water. It
indicates the amount of vegetation and reduces the atmospheric and topographic effects
that are canceled out via division if they remain constant in the two bands used. Image
ratioing largely eliminates irradiance from Eq. 1.8, and thus the topographic effects,
transmittance, and the atmospheric effects.
Table 1.4
Major VIs widely used in quantitative remote sensing and their calculation formula
Index Calculation formula Authors
NDVI (NIR – R)/(NIR + R) Rouse et al. (1974)
ENDVI
(NIR + SWIR2 – R)/(NIR + SWIR2
+ R) Weng et al. (2008)
EVI 2.5(NIR – R)/(NIR + 6R – 7.5B + 1)
Huete et al. (2002), Liu and Huete
(1995)
SAVI (NIR – R)(1 + L)/(NIR + R + L) Huete (1988)
MSAVI Qi et al. (1994)
DVI NIR – R (R798–R683) Clevers (1989)
RVI NIR/R Jordan (1969)
MSR Chen (1996)
OSAVI Rondeaux et al. (1996)
TSAVI Baret and Guyot (1991)
PVI Crippen (1990)
ARVI Kaufman and Tanré (1992)
GEMI Pinty and Verstraete (1992)
SR has been modified and expanded to form the most common and useful index called
NDVI (Rouse et al., 1974) from pixel values on multispectral bands, or(1.9)
The use of the NIR and red bands is grounded on the fact that vegetation spectral
reflectance peaks at infrared wavelengths but is much subdued at visible light
wavelengths. It is particularly low at red wavelength (0.6–0.7 μm). In contrast, the
reflectance of both soil and water has much less variation in these two wavebands. The
differencing of the NIR and red bands effectively maximizes the spectral disparity
between vegetation and these two covers, thus accentuating the visibility of vegetation
on the derived NDVI layer. NDVI is an effective index able to indicate quantitatively
biomass on the ground and has been widely used to estimate vegetation biomass and
monitor its temporal variation, even though it does not eliminate the atmospheric
effects.
NDVI works well with dense vegetation covers as commonly found in the tropics,
but is less effective with sparse and patchy vegetation intermingled with prevalent bare
ground, and in areas of a low surface biomass such as grassland. In such environs,
NDVI and other biological indicators-based methods tend to overestimate the degree of
bare or desertified ground due to seasonal fluctuation in vegetation cover and the severe
effect of rainfall. Consequently, it has been replaced with several alternative VIs,
including soil-adjusted vegetation index (SAVI) (Huete, 1988) and its variants of
SAVI1 and SAVI2, transformed SAVI, and enhanced VI (EVI). SAVI is calculated as:
(1.10)
where L = soil fudge factor with a value between 0 and 1, depending on the soil. It is
often set to 1.
SAVI minimizes soil brightness influences from vegetation indices involving red and
NIR bands. The origin of reflectance spectra in the NIR-red wavelength scatterplot is
shifted to account for first-order soil-vegetation interactions and differential red and
NIR flux extinction through vegetated canopies. For cotton and range grass canopies,
the transformation nearly eliminates soil-induced variations in vegetation indices. SAVI
can depict dynamic soil-vegetation interactions from remotely sensed data. The L-factor
in Eq. 1.10 has been dynamically adjusted using the image-derived NDVI and WDVI as
(Qi et al., 1994):
(1.11)
where γ = primary soil line parameter or slope of the soil line in the reflectance
scatterplot of red vs NIR bands. Its value is commonly taken as 1.06 (Qi et al., 1994),
and the factor 2 increases the L dynamic range. Furthermore, SAVI has also beenmodified to form a few new indices. Two of them are called Modified Soil-Adjusted
Vegetation Index (MSAVI) (Qi et al., 1994) and optimized soil-adjusted vegetation
index (OSAVI) (Rondeaux et al., 1996), calculated as:
(1.12)
(1.13)
MSAVI is more sensitive to vegetation abundance than SAVI and other indices. It has a
higher SNR than other vegetation indices (including the original version of SAVI)
because the L function not only enlarges the vegetation dynamic responses, but also
further suppresses the influences of the soil background.
SAVI has been further transformed by Baret and Guyot (1991) to form a new index
called TSAVI, calculated as:
(1.14)
where a and b denote, respectively, the slope and intercept of the soil line (NIRsoil =
aRsoil +b), and the coefficient value of 0.08 is adjusted to minimize soil effects.
The enhanced VI (EVI) measures the greenness and health of vegetation and
vegetation productivity using blue, red, and NIR bands, calculated as:
(1.15)
where L = soil adjustment factor or canopy background brightness correction factor
(Eq. 1.11) (default = 1); C1 = atmosphere resistance red correction coefficient (default =
6); C2 = atmosphere resistance blue correction coefficient (default = 7.5). For
hyperspectral data, ρNIR = 800, ρr = 670, and ρb = 445. The blue band in EVI is absent
from some optical satellite images such as SPOT, so it is changed to EVI2 as:
(1.16)
Other less frequently used indices are perpendicular vegetation index (PVI) that is
similar to Tasseled Cap Greenness and the 2nd principal component. It is measured by
the orthogonal distance from a point corresponding to a feature’s reflectance to the soil
line in the spectral domain of red vs NIR bands, calculated as:
(1.17)where a, b = coefficient and offset of the soil line that vary slightly among soils. PVI is
functionally equivalent to DVI, and sensitive to the optical properties of bare soil
background that has a higher index value for a given quantity of incomplete vegetation
cover. All VIs (e.g., PVI, SAVI, TSAVI) devised to minimize the soil background effect
strongly reduce the noise for low leaf area indices (LAI < 2–3) (Baret and Guyot,
1991). Since the red radiance subtraction in the numerator of NDVI is considered
irrelevant (Crippen, 1990), it has been simplified as the infrared percentage vegetation
index (IPVI):
(1.18)
IPVI is functionally equivalent to NDVI and RVI with a narrow range of value (0.0–
1.0). It is less complex mathematically.
Three special indices have been developed to quantify specifically vegetation
properties. They are modified simple ratio (MSR) by Chen (1996), red edge position
index (REP) by Horler et al. (1983), and wide dynamic range vegetation index
(WDRVI) by Gitelson (2004), calculated as:
(1.19)
(1.20)
(1.21)
One member of the atmospherically corrected index family is the Atmospherically
Resistant Vegetation Index (ARVI) (Kaufman and Tanré, 1992) calculated as:
(1.22)
where RB is determined from the reflectance in the blue (B) and red (R) bands as:
(1.23)
where γ depends on the aerosol type (a good value is γ = 1 in the absence of the aerosol
model). This concept can be applied to other indices. For instance, SAVI can be
changed to SARVI by replacing R with RB.
The Global Environmental Monitoring Index (GEMI) proposed by Pinty and
Verstraete (1992) can also suppress the atmospheric effects, calculated as:
(1.24)1.4
1.4.1
(1.25)
This non-linear index can account for soil and atmospheric effects simultaneously.
GEMI, computed from measurements at the top of the atmosphere, is therefore both (i)
more useful to compare observations under varying atmospheric and illumination
conditions, and (ii) more representative of actual surface conditions than SR or NDVI
over the bulk of the range of vegetation conditions. This index is seemingly transparent
to the atmosphere, and represents plant information at least as effective as NDVI, but is
complex and difficult to use and interpret. Since most of the data to be used for
quantitative remote sensing will have to be corrected for the atmospheric effects, these
atmospherically corrected indices have not found wide applications in the retrieval of
quantitative information of environmental parameters.
ACCURACY OF QUANTIFIED RESULTS
Validation and Cross-validation
The quantified results need to be validated against the ground truth to provide accuracy
assurance, and to ascertain that the method of quantification is functional and able to
produce reputable results. Validation is also essential to evaluate the performance of
different methods of retrieval and to identify the variables that are the most critical to
the retrieval accuracy. Without validation, the quantified outcome is almost meaningless
and the applicability of the estimation model (method) remains unknown. Validation is
a process of evaluating the quality of the quantified results by comparing them against
some independent data that can be regarded as the ground truth. It has two
connotations, model validation and validation of the modeled outcome. The former is
achieved much more easily than the latter, especially with machine learning methods of
analysis (see Chapter 4 for more details). Validation can be implemented using several
means, depending on the number of samples collected during ground sensing. The most
common method of validation is to make use of in situ measurements that serve as the
reference to judge the quality of the quantified value. During model construction (or
data analysis), the available field samples are divided into two parts. One portion is
reserved for model construction and another for model validation without
compromising the model reliability. After the model has been properly trained using the
first portion of samples, it is run with the second group of samples to examine how it
can accurately predict them. The in situ observed values are compared with the model￾predicted ones, and the disparity between these two sets of quantitative values is
analyzed statistically to indicate the reliability at which the quantification has been
achieved. This validation can produce accuracy indicators, but the validation results arenot independent and objective because there could be a spatial component in the
estimated variable (e.g., the residuals or discrepancies are somehow spatially
correlated). It does not indicate where inaccurate quantification occurs, either. In
contrast, validation against observed values distributed in different parts of the study
area produces much more authentic indications of accuracy. The generated results show
the accuracy of the quantified parameters, not just the estimation model itself.
However, caution needs to be exercised in interpreting the validation results due to
several challenges imposed by the limited coverage of in situ sites, the large spatial
scale mismatch between field measurements and image pixels (see Section 1.5.3 for
more details), the lack of understanding of the intrinsic heterogeneity of the parameters
being quantified, the defects and deficiency in theories on the scale problem embedded
in the validation, and the unavailability of trustworthy in situ datasets with continuity,
completeness, and consistency (Jin et al., 2016). Validation based on in situ
measurements is complicated by the spatial scale mismatch between satellite imagery￾derived estimates and ground measurements, a challenge in quantitative remote sensing
(refer to Section 1.5.3 for more details). This spatial scale mismatch between satellite￾and ground-based observations creates serious uncertainties when the target of
quantification has a high degree of spatial heterogeneity. The effect of this scale
mismatch is particularly pronounced on the accuracy indicators for spatially
heterogeneous parameters.
In certain applications, validation based on in situ observations is not always feasible
due to the small number of field samples available. They are either expensive or
impossible to collect due to the constraint of data collection being synchronized with
the satellite imagery recording. In certain applications no in situ measurements are
collectable, or it is extremely difficult to collect a sufficient number of samples, such as
aerosol optical thickness (AOT) from the AERONET observation stations in monitoring
atmospheric quality. Their number is likely to be limited if the area under study is
small. Thus, all the available samples must be used to construct the quantification
model without any spare ones reserved for validating model accuracy. Sometimes no
ground truth data are available for validation at all. For instance, the gaseous content of
volcanic plumes cannot be measured in time, either on ground or in the air as flying
through the plume is dangerous and prohibited. This leaves no independent dataset to
verify the remotely quantified concentration levels.
Nevertheless, it is still possible to shed light on model stability or reliability via
cross-validation using bootstrapping, in which in situ samples are randomly removed,
one at a time. It has a number of permutations, including Holdout Validation (HV), K￾fold Cross-Validation (KCV), and Leave-One-Out Cross-Validation (LOOCV). Each
permutation has unique requirements and fulfills different needs. HV requires all1.4.2
available samples to be partitioned into two groups at a ratio of 70% vs 30% or 80% vs
20%. The second and lower portion is used to validate the model developed using the
remaining samples (Kim, 2009). KCV requires all the available samples to be divided
into K groups equally and repeatedly. Each time only one of them is selected as the
testing dataset while the remaining K-1 groups are reserved for model development.
Every group is used just once in turn for validation (Kohavi, 1995).
LOOCV is almost identical to KCV except that each sample in the dataset is used, in
turn, to test the model developed using the remaining samples minus the one in use for
validation each time. A model is constructed from the remaining N-1 observations. The
same regression analysis is run repeatedly, potentially yielding 10s or even 100s of
models. The discrepancy between all the observed and predicted values from each
model run is analyzed statistically. The developed model is thus validated n times (n =
number of samples) by comparing the observed value with the predicted one (Hoek et
al., 2008). The final validation outcome is then calculated by averaging all the
individual evaluations. LOOCV overcomes the drawback associated with a small
training dataset that cannot be sensibly partitioned into two parts, one for model
construction and another for model validation. Instead, all of the samples have to be
used for model construction. Leave-one-out testing is the most commonly used, even
though it is computationally intensive and subject to overfitting as all the training data
except one are fed to the model. In a sense, this method indicates the model accuracy,
not the accuracy at which a parameter’s value is estimated. Thus, cross-validation can
never truly reveal the range of prediction discrepancy, nor the magnitude of incorrect
estimations and their locations.
Accuracy Expressions
The validation outcome is statistically analyzed and summarized using several
indicators. The common ones are accuracy or mean bias, precision or repeatability,
relative difference, uncertainty or root mean square error (RMSE). Accuracy measures
the quality of quantification and indicates the degree at which the estimated values of
the target parameter is in agreement with the observed ones. In the literature, it refers to
model accuracy in some cases, but not the accuracy at which the dependent variables or
target parameters are quantified. Accuracy is commonly treated as a proxy for
consistency (e.g., how the accuracy indicator varies with the number of samples used)
in case of insufficient samples. Since all the quantified results are quantitative, they can
be expressed in a number of ways. The commonly used ones are coefficient of
determination (R2 value), and RMSE, calculated as:(1.26)
(1.27)
where N = sample size, Yi = the measured value of the target to be sensed, = the
predicted value of the sensed target, = the average value of all the collected samples.
The coefficient of determination (R²) is calculated statistically between the observed
and predicted values for all the validation samples. It refers to the portion of variation in
the dependent variable (i.e., the target parameter to be quantified) that can be explained
by the predictor variables (see Section 1.3). Usually, the two are plotted out as a
scatterplot to reveal not only the general R2 (Eq. 1.24) but also how the prediction
behaves (e.g., whether there exists any trend between the two or where the fit is loose)
(Figure 1.8a). It illustrates the degree of fitness between the two sets of data. Inaccurate
quantification can be easily appreciated from the 1:1 trend line (Figure 1.8b). If all the
estimated values and observed ones converge closely along the 1:1 trend line, then the
accuracy of quantification is high. Apart from R² value, the quality of estimation (or
model reliability) is also judged by the p value. It indicates the probability at which the
observed R2 is obtained or is statistically significant. The commonly adopted p value is
p<0.05 or p<0.01. A small p value means the relationship between the two sets of
variables has a high degree of being genuine and repeatable. If the p value is too high,
then the relationship between the two can stem from random match, and the predictor
variables are unable to estimate the dependent variable competently. The constructed
estimation model is not sufficiently reliable or valid for quantification and should be
abandoned.FIGURE 1.8 Scatterplots to illustrate the quality of quantifying suspended sediment
concentration (SSC) from digital number (DN) of a spectral band. (a) Regression model
(solid red line) of in situ measured SSC against DN and its coefficient of determination
(R²); (b) Residuals of the estimated SSC vs the observed SSC along the 1:1 trend line
with wider deviation of more observations from this line suggesting less reliable
quantification.
Dissimilar to the more general R², RMSE is more restrictive, generated from the
independent samples only. It yields more information on the range of inaccuracy, and
can indicate the spatial distribution or location of the independent samples. In order tomake RMSEs in multiple studies and the results quantified at multiple times directly
comparable with each other, they have been expressed in relative terms or normalized
to form rRMSE (%) (Eq. 1.28) and nRMSE (Eq. 1.29). Both of them are expressed as a
percentage between 0 and 100, with a lower value suggesting a higher accuracy of
quantification.
(1.28)
(1.29)
Other accuracy indicators include the ratio of the performance to the derivation (RPD),
and the mean absolute percentage error (MAPE), calculated as:
(1.30)
(1.31)
where SDs = standard deviation of the measured attribute value of the target parameter.
A larger R² and RPD value and a smaller RMSE and MAPE are indicative of a superior
model performance. Finally, mean prediction error (MPE) has been used to calculate
the variance of prediction errors (VAR) as:
(1.32)
where PE = prediction error calculated as yobs – ymeasured.
If the predictions are time-series results, then more indicators are available (Table
1.5), such as temporal smoothness and time-series smoothness (Wolters et al., 2021).
These indices are commonly applied to assessing spatial and temporal consistency of
time-series quantification results, such as satellite data products. In general, spatial and
temporal consistency of satellite products’ uncertainty has not been assessed in
quantitative remote sensing because the in situ data collected at specific times do not
fully match the remotely sensed results. So these accuracy indicators have not been
used as commonly as the ones introduced at the beginning of this section.
Table 1.5
Commonly used validation metrics and their calculation
Validation metric Formula
Accuracy (Acc) or mean bias
Precision (Prec) or repeatability1.5
1.5.1
Validation metric Formula
Relative difference [Δ, %]
Uncertainty (Unc) or Root Mean Squared Difference (RMSD)
Root of the unsystematic mean product difference (RMPDu)
Root of the systematic mean product difference (RMPDs)
Coefficient of determination (R2)
Temporal smoothness (δ)
Time-series smoothness index (TSI)
Source: Wolters et al. (2021), open access.
Note: *: N = number of valid samples used for comparison, σ(X) and σ(Y) = standard deviation of X and Y, σ(X,Y)
the co-variation of X and Y, and and = observed and model estimated true value of the attribute. P(di), P(di+1) and
P(di+2) are three consecutive observations on dates di, di+1, and di+2.
CHALLENGES FACING QUANTIFICATION
As pointed out previously, it is not always feasible or successful to quantify a parameter
from remotely sensed data for various reasons. Even if the quantification is possible,
the quantified results may not be sufficiently accurate to be of any use. In fact,
successful quantification is contingent upon several factors, such as image quality,
target visibility and distinctiveness, and the influence of the sensing environment.
Limitations of Imagery Data
Quantification from remote sensing data faces the issue of data unavailability at the
right time due to orbital periods. Even if the right data are indeed available, they may be
contaminated by clouds. Quantification is challenging in cloudy conditions, as optical
images form the bulk of data sources. These images recorded over the VIS and NIR
portion of the spectrum are vulnerable to obstruction by clouds. Image contamination
by clouds, especially with coarse resolution meteorological satellite data over extensive
areas is rife in the rainy season and in tropical areas. All ground areas blocked by
clouds are treated as missing data and cannot be quantified, leaving only those imagery
data obtained under sunny cloud-free conditions usable. The exclusion of cloud￾contaminated images from use reduces the number of inputs and possibly sample size,
and causes the established estimation model prone to the influences of outliers.
Although active sensing data such as radar and LiDAR data are generally immune from
cloud contamination, radar imagery may be noisy in rainy conditions. Besides, flying in
stormy weather degrades image geometry. Although airborne LiDAR data are not1.5.2
affected by clouds, they are liable for blind spots where no LiDAR pulses can reach,
usually in some background area hidden behind the line of sight (LOS). This issue of
blind spots worsens with terrestrial laser scanning. Related to the use of drone images is
the absence of infrared bands that can capture the most subtle spectral variations in the
target parameter. While this issue does not affect the success/failure of quantification, it
does adversely impact the reliability of the quantified results.
Apart from remote sensing data quality, data availability can also be problematic if
the period of study spans a decade or longer since most satellites have a life expectancy
less than this duration. The data used to be acquired from one sensor can cease to exist,
creating headaches in the long-term monitoring of the quantified parameter. The only
exception is Landsat images that have always maintained a consistent standard over
multiple decades. Whenever new data from a recently launched satellite become
available, the existing estimation models (mostly empirical and semi-empirical) and the
best predictor variables may not be working or optimal any more, so new estimation
models have to be developed from scratch with the newly emerged data. Long-term
quantitative remote sensing may be accomplished using satellite data acquired from
multiple sensors. Even so, it is largely impossible to undertake historic quantification
(e.g., unable to collect historic in situ samples) retrospectively in some cases, so
quantification is mostly restricted to the present time.
Retrospective Quantification
Quantitative remote sensing can be carried out only when both remotely sensed and
ground data are collected concurrently. Since in situ data cannot be collected
retrospectively, it is impossible to quantify the past state in spite of the existence of
historic imagery data. One way of circumventing this problem to accomplish
retrospective quantification is to make use of invariant targets on the image such as
snow. Such covers have a constant spectral reflectance irrespective of the time of
sensing except the influence from the varied atmospheric conditions. Through a mutual
radiometric calibration equation of the current and historic images of the same area, the
pixel values on the historic image can be converted to the ground parameter value
without the need for in situ samples. This method also works for multi-temporal images
and can save the processing of individual images and expedite the quantification
process. This method is underpinned by the assumption that the atmospheric conditions
remain unchanged during multi-temporal sensing. Admittedly, this method of relative
calibration does not work if spectrally invariant ground objects are absent from the
scene of study.1.5.3
If the target of quantification has a rather low value or concentration below the
radiometric resolution of the images used (or falling within the uncertainty range of
LiDAR data), then its signature could be rather indistinct, raising the possibility of
unsuccessful quantification. The accuracy of quantification can be further lowered by
the co-existence of multiple targets in the same sensing environment. Their spectral
signature may interfere with each other, and cause the isolation of the specific signature
of individual targets almost impossible. Similarly, if the target of quantification is
mobile, then the pace of motion must exceed the pixel size of the imagery being used or
the positioning uncertainty of LiDAR data. Naturally, both the minimum size of
movement and the lowest magnitude of vertical shift or pace of movement must be an
order higher than the image spatial resolution or the data inaccuracy to generate a viable
quantitative value.
Data Mismatch
As stated previously, the accuracy of the results quantified from remotely sensed data is
validated via comparison with in situ sampled data. However, the two do not always
match perfectly. In fact, there are potentially four mismatches between them that
degrade the accuracy indicators generated.
i. Temporal mismatch. Temporal mismatch between imagery-derived and field￾collected data may arise from the ill synchronization of in situ sampling with
image acquisition because satellite images can be acquired quickly for a huge
ground area within seconds or at most hours with airborne data. In comparison,
field sampling is a strenuous and time-consuming process, taking up to days and
even weeks to complete in several campaigns. The entire study area has to be
traversed to collect spatially representative samples, as in cruising to the open
ocean of tens of 1,000s of square kilometers to measure bathymetry. Although
bathymetry may not change over a short time, the sensing conditions do (e.g.,
changed wind speed and wave height). The sampling itself may be lengthy under
unfavorable conditions such as inland river waters shrouded in a thickly wooded
area. The synchronization becomes more difficult if both the remotely sensed data
and field data are obtained in the field. It becomes increasingly challenging to
perfectly synchronize the two due to shortage of field personnel. In some
applications, a delay of a few days will not introduce noticeable changes to the
target parameter of quantification on the ground (e.g., forest biomass), but it could
mean drastically altered weather condition of sensing from the ground truthing
conditions. Other target parameters of quantification may experience markedchanges in the interim such as suspended sediments or algal blooms. A delay of a
few hours in sampling means a huge variation to the target state. This temporal
mismatch potentially reduces the reliability of the quantification outcome, or even
thwarts the quantification efforts completely.
ii. Scale mismatch. The spatial scale mismatch between in situ samples and image
pixel size can be as large as tens of square kilometers. The minimal area that can
be resolved on a remote sensing image is dictated by its spatial resolution that is
measured on the order of kilometers to meters for space-borne data, and even to
centimeters with drone images. As discussed in Section 1.2.1, field sampling takes
place mostly on the point scale. Even if on the areal scale, plot size has to be
limited to tens of meters at most to be manageable. The collected ground truth
data enumerated at the sampling points or over the small sample plot can never
match image pixel size. They represent the target parameter value integrated over
the ground area corresponding to the pixel size of the imagery data in use. When
the point-sampled values are upscaled to the pixel size, it is assumed that the
quantified target parameter is spatially uniform within the pixel. A lot of global
data products produced from coarse resolution satellite data have a pixel size
measured by kilometers on an ongoing basis. They are not applicable to the
surface level or on the local scale in validating quantification outcome generated
from medium-resolution images. Besides, ancillary environmental variables, such
as rainfall and temperature, that are commonly used in certain quantitative remote
sensing applications are enumerated at points, not over an area.
iii. Dimension mismatch. Remotely sensed data, especially imagery data, capture the
surface reflectance of the target and render it as 2D images, even though it may be
inherently 3D in nature, such as tree biomass. When ground data are compared
with imagery-derived results to assess their accuracy, it induces a dimensional
mismatch between them. Certain parameters such as air pollutants may be 3D,
they are estimated in the in situ measurements at a dimension different from that
of the remotely quantified results. Namely, the estimates from space-borne
imagery data refer to the atmosphere-column concentration integrated from the
surface to the top-of-atmosphere where the sensor is located. However, air
pollutant concentrations used for accuracy assessment are recorded at points near
the Earth’s surface. This mismatch in the sensible depth is also common with in￾water components. The depth to which the radiation used in sensing can penetrate
the target is a function of the target itself and the wavelength of the captured
radiation. For visible sunlight and infrared radiation, the depth of ground
penetration is rather shallow, especially when the surface is moist. As for water,
the depth to which visible light of sensing can penetrate it may not match exactly1.6
the depth at which in situ water samples are collected. On the other hand, in situ
data that are used to validate the remotely quantified results are obtained at a
certain depth below the surface (e.g., 20 cm in sampling soil properties). Unless
the in-water constituents have a uniform vertical distribution, this mismatch in
depth will instigate errors to the assessed accuracy. This means that it may be
necessary to calibrate the remotely sensed values by the vertical distribution of the
target, such as suspended sediments, in order to generate volume information.
With TIR sensing, as in the quantification of sea surface temperature, the radiation
is mostly absorbed by water, leading to little downward penetration, so satellite
imagery-produced temperature of the sea surface is confined to the top few
microns or slightly deeper. In comparison, it is impossible to measure sea surface
temperature at this depth in the field as the thermometer must be immersed in
water to measure its temperature properly, not the air temperature. This depth
mismatch will introduce uncertainty to the validation accuracy and make it
reflective of the genuine accuracy of the remotely quantified outcome.
All of the mismatches will inevitably create discrepancies between ground data and
remotely quantified results and subvert the quantification accuracy.
ORGANIZATION OF THIS BOOK
This book is organized into two broad parts, each comprising four chapters. The first
part covers the fundamentals of quantitative sensing and lays the physical foundation of
quantification. Presented in the first chapter are such topics as collection of field data,
principles of and challenges facing quantification, variables used to predict the
dependent variable or the parameter value of interest, and accuracy measures of
quantified results. Chapter 2 presents an overview of remote sensing data and compares
their major properties and best uses. The reviewed images are generic in that they have
found applications in more than one sphere. If the data are applied exclusively to one
sphere (e.g., the atmosphere), then they are discussed in the relevant chapters
elsewhere. Chapter 3 discusses how different types of remotely sensed data are
radiometrically corrected for the atmospheric effects and surface reflection in
preparation for the quantitative retrieval of physical parameters. Chapter 4 expounds the
analytical means by which image-derived indices or variables are related to in situ
sampled values of the target parameter. Also introduced in this chapter are physical
models and their assimilation with remotely sensed data for the retrieval of quantitative
information on the target parameters.The second part of the book is devoted to the practice of quantitative remote sensing.
Each chapter covers the retrieval in one of the four spheres mentioned in the preceding
section. Starting from the terrestrial sphere, Chapter 5 elaborates on how to retrieve
physical and chemical properties of the terrestrial surface, particularly soil, such as
temperature and salinity. Also included in the discussion is how to quantify changes in
surface elevation and the volume of debris and its displacement velocity using non￾imagery data. How to quantify the biosphere forms the content of Chapter 6. It
encompasses the estimation of not only plant properties using remotely sensed images,
LiDAR data, and (semi-)physical models, but also the prediction of crop yields, in
which remotely sensed predictor variables play a secondary role only. Chapter 7
elucidates how to retrieve hydrographic parameters, both inorganic and organic, in
oceanic and inland waters and their quality, as well as the physical state of oceans from
microwave and thermal data. Chapter 8 discusses the quantitative sensing of the
atmosphere (and to a lesser degree, the troposphere) and how to retrieve atmospheric
quality and gaseous and solid substances quantitatively from various tailored satellite
data.
In this book, the quantification topics are organized by the four spheres of the
environment. This organization may seem intuitive and convenient, but it is inevitably
problematic in certain ways by creating a degree of repetition. For instance, the same
thermal sensing of sea surface temperature in the hydrosphere is also discussed in
quantifying fire intensity in the biosphere and urban heat island in the terrestrial sphere.
Nevertheless, such a disparate treatment of the same type of quantification is justified
by the fact that the remote sensing images used for the quantification and the method of
quantitative retrieval do not overlap because of the differential scale of the phenomenon
and the range of temperatures to be quantified. Thus, the actual repetition is much less
extensive than the section headings seem to suggest.
REFERENCES
Adamsen FG, PJ Pinter, EM Barnes, RL LaMorte, GW Wall, SW Leavitt, and BA
Kimball (1999) Measuring wheat senescence with a digital camera. Crop Sci 39:
719–724.
Ali AM, R Darvishzadeh, AK Skidmore, I van Duren, U Heiden, and M Heurich
(2016). Estimating leaf functional traits by inversion of PROSPECT: assessing leaf
dry matter content and specific leaf area in mixed mountainous forest. Int. J. Appl.
Earth Obs. Geoinf. 45: 66–76.
Baret F and G Guyot (1991) Potentials and limits of vegetation indices for LAI and
APAR assessment. Rem Sens Environ 35(2–3): 161–173. doi: 10.1016/0034-4257(91)90009-U
Bendig J, K Yu, H Aasen, A Bolten, S Bennertz, J Broscheit, M Gnyp, and G Bareth
(2015). Combining UAV-based plant height from crop surface models, visible, and
near infrared vegetation indices for biomass monitoring in barley. Int J Appl Earth
Obs Geoinfo 39: 79–87. doi: 10.1016/j.jag.2015.02.012
Bréda NJJ (2003) Ground-based measurements of leaf area index: A review of methods,
instruments and current controversies. J Exp Bot 54(392): 2403–2417. doi:
10.1093/jxb/erg263
Chason JW, DD Baldocchi, and MA Huston (1991) A comparison of direct and indirect
methods for estimating forest canopy leaf area. Agri Forest Meteo 57: 107–128.
Chen JM (1996) Evaluation of vegetation indices and a modified simple ratio for boreal
applications. Can J Rem Sens 22(3): 229–242. doi:
10.1080/07038992.1996.10855178
Clevers JGPW (1989) The application of a weighted infrared-red vegetation index for
estimating leaf area index by correcting for soil moisture. Rem Sens Environ 29: 25–
37.
Crippen RE (1990) Calculating the vegetation index faster. Rem Sens Environ 34(1):
71–73. doi: 10.1016/0034-4257(90)90085-Z
Darvishzadeh R, A Skidmore, H Abdullaha, E Cherenet, A Ali, T Wang, W
Nieuwenhuis, M Heurich, A Vrieling, B O’Connor, and M Paganini (2019) Mapping
leaf chlorophyll content from Sentinel-2 and RapidEye data in spruce stands using
the invertible forest reflectance model. Int J Appl Earth Obs Geoinfo. 79: 58–70. doi:
10.1016/j.jag.2019.03.003
Gao J (2022) Fundamentals of Spatial Analysis and Modelling. Boca Raton: CRC
Press, 348 p.
Gitelson AA (2004) Wide dynamic range vegetation index for remote quantification of
biophysical characteristics of vegetation. J Plant Physiol 161: 165–173.
Gitelson AA, YJ Kaufman, R Stark, and D Rundquist (2002) Novel algorithms for
remote estimation of vegetation fraction. Rem Sens Environ 80: 76–87
Gitelson AA, A Viña, V Ciganda, D Rundquist, and TJ Arkebauer (2005) Remote
estimation of canopy chlorophyll content in crops. Geophy Res Lett. 32, L08403.
doi:10.1029/2005GL022688
Hoek G, R Beelen, K De Hoogh, D Vienneau, J Gulliver, P Fischer, and D Briggs
(2008) A review of land-use regression models to assess spatial variation of outdoor
air pollution. Atmos Environ 42(33): 7561–7578.
Horler DNH, M Dockray, and J Barber (1983) The red edge of plant leaf reflectance.
Int J Rem Sens 4(2): 273–288. doi: 10.1080/01431168308948546Huete AR (1988) A soil adjusted vegetation index (SAVI). Rem Sens Environ 25: 295–
309.
Huete A et al. (2002) Overview of the radiometric and biophysical performance of the
MODIS vegetation indices. Rem Sens Environ 83: 195–213.
Jin R, X Li, M Ma, Y Ge, T Che, ... and Q Xiao (2016) Remote sensing products
validation activity and observation network in China. IEEE Int Geosci Rem Sens
Sympo (IGARSS), Beijing, China, pp. 7623–7626. doi:
10.1109/IGARSS.2016.7730988
Jordan CF (1969) Derivation of leaf area index quality of light on the forest floor. Ecol
50(4): 663–666.
Kaufman YJ and D Tanré (1992) Atmospherically resistant vegetation index (ARVI) for
EOS-MODIS. IEEE Trans Geosci Rem Sens 30: 261–270.
Kawashima S and M Nakatani (1998) An algorithm for estimating chlorophyll content
in leaves using a video camera. Ann Bot 81: 49–54.
Kim JH (2009) Estimating classification error rate: Repeated cross-validation, repeated
hold-out and bootstrap. Comput Stat Data Analysis 53(11): 3735–3745.
Kohavi R (1995) A study of cross-validation and bootstrap for accuracy estimation and
model selection. Int Joint Conf on Art Int 14(2): 1137–1145.
Lee M-S, K-A Park, and F Micheli (2021) Derivation of red tide index and density
using Geostationary Ocean Color Imager (GOCI) data. Rem Sens 13: 298. doi:
10.3390/rs13020298
Li L, RE Sengpiel, DL Pascual, LP Tedesco, JS Wilson, and E Soyeux (2010) Using
hyperspectral remote sensing to estimate chlorophyll-A and phycocyanin in a
mesotrophic reservoir. Int J Rem Sens 31(15): 4147–4162. doi:
10.1080/01431161003789549
Liu HQ, and AA Huete (1995) Feedback based modification of the NDVI to minimize
canopy background and atmospheric noise. IEEE Trans Geosci Rem Sens 33: 457–
465. doi: 10.1109/36.377946
Louhaichi M, MM Borman, and DE Johnson (2001) Spatially located platform and
aerial photography for documentation of grazing impacts on wheat. Geocarto Int 16:
65–70.
Lussem U, A Bolten, M Gnyp, J Jasper, and G Bareth (2018). Evaluation of RGB-based
vegetation indices from UAV imagery to estimate forage yield in grassland. ISPRS -
Intern Archives of the Photogram, Rem Sens and Spat Info Sci. XLII-3. 1215–1219.
10.5194/isprs-archives-XLII-3-1215-2018
Markwell J, JC Ostermann, and JJ Mitchell (1995) Calibration of the Minolta SPAD￾502 leaf chlorophyll meter. Photosynth Res 46: 467–472.Mitchell BG, M Kahru, J Wieland, and M Stramska (2002) Determination of spectral
absorption coefficients of particles, dissolved material and phytoplankton for discrete
water samples. Ocean Opt Protocols Satell Ocean Color Sensor Validation 3(2):
231–257.
Ouyang Z, Y Gao, X Xie, H Guo, T-T Zhang, and B Zhao (2013) Spectral
discrimination of the invasive plant Spartina alterniflora at multiple phenological
stages in a saltmarsh wetland. PloS One 8: e67315. doi:
10.1371/journal.pone.0067315
Peper PJ and EG McPherson (1998) Comparison of five methods for estimating leaf
area index of open grown deciduous trees. J Arboricult 24(2): 98–111.
Pinty B and MM Verstraete (1992) GEMI: A non-linear index to monitor global
vegetation from satellites. Vegetatio 101: 15–20.
Qi J, A Chehbouni, AR Huete, YH Kerr, and S Sorooshian (1994) A modified soil
adjusted vegetation index. Rem Sens Environ 48(2): 119–126. doi: 10.1016/0034-
4257(94)90134-1
Rondeaux G, M Steven, and F Baret (1996) Optimization of soil-adjusted vegetation
indices. Rem Sens Environ 55: 95–107.
Rouse JW et al. (1974) Monitoring vegetation systems in the great plains with ERTS.
NASA Spec Publ 351: 309.
Rouse JW, RH Haas, JA Schell, DW Deering, and JC Harlan (1974) Monitoring the
vernal advancement of retrogradation of natural vegetation, NASA/GSFC, Type III,
Final Report, Greenbelt, MD, 371 pp.
Ruddick KG, K Voss, E Boss, A Castagna, R Frouin, A Gilerson, M Hieronymi, BC
Johnson, J Kuusk, Z Lee, M Ondrusek, V Vabson, and R Vendt (2019) A review of
protocols for fiducial reference measurements of water-leaving radiance for
validation of satellite remote-sensing data over water. Rem Sens 11(19): 2198. doi:
10.3390/rs11192198
Rundquist D, A Gitelson, B Leavitt, A Zygielbaum, R Perk, and G Keydan (2014)
Elements of an integrated phenotyping system for monitoring crop status at canopy
level. Agronomy 4(1): 108.
Shang Z, Z Lee, Q Dong, and J Wei (2017) Self-shading associated with a skylight￾blocked approach system for the measurement of water-leaving radiance and its
correction. Appl Optics 56(25): 7033–7040. doi:10.1364/ao.56.007033
Tan C, D Wang, J Zhou, Y Du, M Luo, Y Zhang, and W Guo (2018) Remotely
assessing Fraction of Photosynthetically Active Radiation (FPAR) for wheat canopies
based on hyperspectral vegetation indexes. Front Plant Sci 9: 776. doi:
10.3389/fpls.2018.00776Tang JW, GL Tian, XY Wang, XM Wang, and QJ Song (2004) The methods of water
spectra measurement and analysis I: Above-water method. J Rem Sens 8: 37–44.
Torres-Sánchez J, JM Peña-Barragán, A De Castro, and F López-Granados (2014).
Multi-temporal mapping of the vegetation fraction in early-season wheat fields using
images from UAV. Compu Electr Agric. 103: 104–113.
10.1016/j.compag.2014.02.009
Tucker CJ (1979) Red and photographic infrared linear combinations for monitoring
vegetation. Rem Sens Environ 8: 127–150.
Verrelst J, G Camps-Valls, J Muñoz-Marí, JP Rivera, F Veroustraete, JGPW Clevers,
and J Moreno (2015a) Optical remote sensing and the retrieval of terrestrial
vegetation bio-geophysical properties – A review. ISPRS J Photogram Rem Sens 108:
273–290. doi: 10.1016/j.isprsjprs.2015.05.005
Verrelst J, JP Rivera, F Veroustraete, J Muñoz-Marí, JGPW Clevers, G Camps-Valls,
and J Moreno (2015b) Experimental Sentinel-2 LAI estimation using parametric,
non-parametric and physical retrieval methods – A comparison. ISPRS J Photogram
Rem Sens 108: 260–272. doi: 10.1016/j.isprsjprs.2015.04.013
Vohland M, S Mader, and W Dorigo (2010) Applying different inversion techniques to
retrieve stand variables of summer barley with PROSPECT+SAIL. Int J Appl Earth
Obs Geoinfo 12(2): 71–80. doi: 10.1016/j.jag.2009.10.005
Weng Y, P Gong, and Z Zhu (2008) Soil salt content estimation in the Yellow River
delta with satellite hyperspectral data. Canadian J Rem Sens 34(3): 259–270.
doi.org/10.5589/m08-017
Woebbecke D, G Meyer, K VonBargen, and D Mortensen (1995) Color indices for
weed identification under various soil, residue, and lighting conditions. Trans ASAE
38 (1): 271–281.
Wolters E, C Toté, S Sterckx, S Adriaensen, C Henocq, J Bruniquel, S Scifoni, and S
Dransfeld (2021) iCOR atmospheric correction on Sentinel-3/OLCI over land:
Intercomparison with AERONET, RadCalNet, and SYN level-2. Rem Sens 13(4):
654. doi: 10.3390/rs13040654
Zhang Y, Y Zhang, K Shi, Y Zha, Y Zhou, and M Liu (2016) A Landsat 8 OLI-based,
semi-analytical model for estimating the total suspended matter concentration in the
slightly turbid Xin’anjiang Reservoir (China). IEEE J Sel Top Appl Earth Obs Rem
Sens 9(1): 398–413.2 Sensing Platforms and Data
DOI: 10.1201/9781003517504-3
The remotely sensed data that have found applications in quantitative remote
sensing fall broadly into two types, imagery and non-imagery, with the former
having the absolute dominance over the latter with a much longer history of
existence. Imagery data graphically depict the Earth’s state at the time of sensing,
even though the volume of detail they exhibit varies with the altitude of the sensing
platform and the spectral range of the radiative energy received. They are further
differentiated into optical and microwave by the wavelength of the energy. Optical
imagery is acquired using radiation over the visible light and near infrared (VNIR)
wavelength range while microwave imagery commonly known as radar is captured
using actively transmitted microwave radiation. Optical images are acquired
passively using the solar radiation reflected off or emitted by the target to the sensor
over the ultra-violet (UV), VNIR, shortwave infrared (SWIR), and thermal infrared
(TIR) portions of the spectrum. Optical images are further differentiated as medium
resolution or very high resolution (VHR) in terms of the details they exhibit. Most
space-borne, medium-resolution images have a spatial resolution of 5 m or coarser.
There is no precise definition of very high spatial resolution. In general, it is
implicitly construed to be a resolution ranging from sub-meter to a few meters (e.g.,
< 4 m). Radar sensing is active as radar images are recorded using microwave
radiation supplied by the sensor itself. Since microwave radiation is able to
penetrate clouds, radar sensing is operational under cloudy conditions and radar
images are not easily contaminated by clouds unless they are excessively thick.
Over the years remote sensing images have experienced significant improvements
in their geometric fidelity and radiometric resolution (up to 16 bits) that allow
minute concentration levels of trace elements to be quantified globally. Anothertrend is the increasingly wider applications of drone images that offer centimeter￾level spatial resolutions, much finer than meters of airborne data and 10s–100s of
meters of space-borne data (Figure 2.1).
FIGURE 2.1 Comparison of three major types of sensing platforms and the nature of
the acquired data. (Lechner et al., 2020, open access.)
Non-imagery data are exemplified by LiDAR that gather 3D positions of surface
features. LiDAR data are mostly airborne in most cases of quantification, but in
recent years terrestrial LiDAR has gained popularity in quantifying biophysical
parameters of the biosphere with newly invented and powerful scanners. Both
imagery and LiDAR data must make use of the radiation that can penetrate the
atmosphere, especially if acquired from space-borne sensors. They have
complementary properties and are frequently used in tandem to improve
quantification accuracy. This chapter first compares and contrasts three types of
sensing platforms (drone, airplane, and satellite) and their impact on the properties
of the acquired remote sensing data. Imagery-based sensing data will be introduced
first, separately by the main purposes of application. Some satellite images have
found applications in more than one sphere, with the sphere-specific data presented
in relevant subsequent chapters. The discussion then progresses to active non￾imagery sensing. Finally, this chapter elaborates on hyperspectral data.2.1
2.1.1
SENSING PLATFORMS
Drones
Drone data are acquired tens of meters (40–70 m) above the ground (Figure 2.1), a
limitation imposed by local aviation authority, usually with the assistance of a
digital camera. Drones can be regarded as multi-rotor helicopters. Also known as
unpiloted aerial vehicles (UAV), they are battery-operated, so they cannot be
airborne for hours (e.g., <⁓12 minutes). At a flight altitude of 50 m above the
mean ground, the captured images have a maximal frame of 4,000×3,000 pixels,
corresponding to a ground resolution of approximately 0.02 m×px-1 (Geipel et al.,
2014). Thus, drone images are best for local- or micro-scale quantification because
each frame covers only tens of square meters on the ground. Drone images enjoy
several strengths, such as superior spatial resolution, and the capacity to fly on￾demand flexibly at critical times, to acquire data at a low cost, and to carry multiple
sensors. The type of drone images that can be recorded depends on the sensor
carried onboard. The most commonly used sensors are inbuilt cameras that have the
motion compensation mechanism. They acquire digital photographs in the visible
light and infrared portion of the spectrum. Normally, the acquired drone
photographs comprise three multispectral bands of blue (λ = 400–550 nm), green (λ
= 470–620 nm), and red (λ = 590–700 nm) wavelengths, plus one NIR band. Some
UAV-borne cameras can take pictures in five spectral bands of blue (B, 450 ±16
nm), green (G, 560±16 nm), red (R, 650±16 nm), red edge (RE, 730±16 nm), and
NIR (840±26 nm) with 1600×1300 effective pixels simultaneously, plus one RGB
image. The UAV is also equipped with a top irradiation sensor to record the
incoming radiation for the same five bands for estimating surface reflectance.
Thus, drone images are not spectrally rich, a major drawback in quantifying
biophysical parameters of vegetation, even though they have a superfine spatial
resolution up to 1 cm that allows micro-features on the ground to be resolved
(Figure 2.2). UAV or hexacopter drone has been used to acquire both multispectral
and hyperspectral images if equipped with the corresponding sensors or cameras.
Their sensitivity ranges from 400 to 1000 nm with a trial resolution of 4 nm.
Airborne hyperspectral sensor can have an FOV of 32.2°, and a spatial resolution of
0.4 m (Pyo et al., 2022).FIGURE 2.2 The amount of details about a degraded meadow on the Qinghai-Tibet
Plateau of West China, shown on a drone image (image resolution = 1 cm) taken
with an ordinary camera at a height of 50 m. image size: 9224×7380; blue patches:
poisonous ruderals; bluish patches: exposed soil; whitish patches: pika mounds;
green patches: healthy meadow.
Drone images used to be notoriously difficult to control geometrically, and it is
painfully slow and laborious to process multiple drone images to generate the
desired outcome. The quantification over large study areas requires the use of a
large number of photos that can easily run into 100s due to the limited ground
coverage per photo. The issue is made worse by the usual endlap of 70% and
sidelap of 30% for creating ortho-mosaics. Nevertheless, photo overlaps do afford
abundant opportunities to identifying tie points or keypoints, common to adjoining2.1.2
photos, indispensable to the mosaicking of photos that must be geo-referenced to
the same ground coordinate system with precise GPS and navigation information.
The task has been eased by advanced drones accoutred with autopilot facilities
to gather the navigation information, including navigation-grade GPS and the
inertial measurement units (IMUs) that include three acceleration sensors, three
gyroscopes, a three-axis compass, and a pressure sensor, regulated by basic
proportional integral differential loops. In addition to the standard IMU navigation
sensor, some drones are equipped with the differential global navigation satellite
systems (DGNSS) receiver. For instance, small-size rotor-wing UAV (DJI Phantom
4) is furnished with GPS, Global Navigation Satellite System (GNSS) and Vision
Positioning System (forward and downward). The gathered information is vital to
spatially align, mosaic, and geo-reference drone images to create orthoimages and
produce high-resolution 3D models of the ground via feature matching and the
structure from motion (SfM) photogrammetric method (see Section 4.4.1 for more
details) if DGNSS and GCPs are available, together with camera position and
orientation information (Swayze et al., 2021). In each flight mission, the digital
camera continuously captures images at a fixed (e.g., 2 seconds) interval (Cheng et
al., 2020). The UAV operating in the autopilot mode can retain a vertical accuracy
of 0.1 m and a horizontal accuracy of 0.3 m when hovering over a study area.
Nevertheless, UAV missions are subject to certain weather conditions, entailing a
sophisticated setup, and the acquired large volume of data are difficult to manage
and process using complex computer vision-based techniques such as SfM multi￾view stereo techniques instead of the conventional photogrammetric
orthorectification workflow in constructing 3D models of the target. Moreover,
UAV flights are subject to a growing number of regulatory restrictions prohibiting
its operation in certain areas.
Aircraft
Similar to drones, light aircraft can be deployed very flexibly to take photographs at
a flying height of around a few kilometers above the ground (Figure 2.1), but the
exact flight configuration needs careful planning so as to minimize operation cost
(e.g., the number of flight paths, photographs taken per flight path, photo overlaps,
the total number of photographs to be taken, the camera to be used and its focal
length, and the operational range and duration of the aircraft). The major
differences between drone and aircraft lie in flight height and the ground area2.1.3
covered per photo, in addition to the duration of flight. Compared with drones,
aircraft can be airborne for a prolonged period, enabling a large ground area to be
covered by smaller-scale photographs because of the broader ground area covered
per photo. In comparison to drones, piloted aircraft uses more stabilization
measures to control its orientation and to minimize geometric distortion of the
acquired photographs, such as the hydraulically actuated Zeiss-Jena SM 2000
system. It enables platform pitch and roll to be corrected to within ±5°. The yaw
can be offset by ±20° with ±8° of stabilization (NASA Airborne Science Program –
HyMap, https://airbornescience.nasa.gov/). Aircraft is much more expensive to
operate than drones, but the acquired photographs have a higher degree of
geometric fidelity.
The heavy weight of a light aircraft allows it to carry bulky digital cameras and
LiDAR scanners. The former is used to take photographs in just four spectral bands
of blue, green, red, and NIR, reminiscent of drone photos. They have a much
coarser spatial resolution than drone photos but much finer resolution than satellite
images, even space-borne VHR images. They cover a larger ground area per scene,
and are best used for local-scale studies, such as city-wide or catchment-level
quantification. Airborne LiDAR scanning is advantaged in maintaining a roughly
uniform sensing distance to the target, resulting in the required data having a
uniform density across the whole area of study with minimal blind spots.
Satellites
Satellites are space-borne platforms whose altitude ranges widely, all beyond 100
km above the Earth (Figure 2.1). They are expensive to launch, but once put in
orbit, they are able to sense the target year-round over multiple years, acquiring
millions of images over their lifespan. Satellites can capture multispectral images
repeatedly, always at the same local time, a feature that is highly desirable and
valuable for long-term monitoring and longitudinal studies, drastically dissimilar to
aircraft or drones that just capture one-off data at a specific time. Owing to the high
satellite altitude, space-borne images cover a huge ground area per scene, enabling
them to quantify a parameter that is spectrally subtle and variable over large areas.
More importantly, space-borne multispectral scanners extend the sensible spectral
region to far infrared and even microwave wavelengths, with a sensing capability
that no drone or aircraft data can match. Remote sensing satellites are usually
launched by government agencies for scientific explorations and public goods.Some light-weight remote sensing satellites have been launched by private for￾profit companies to acquire superfine commercial satellite data for monetary gains.
For instance, a new trend of satellite remote sensing emerged about a decade ago,
with the launch of a large fleet of CubeSats by private companies. They are a class
of miniaturized satellites, weighting a fraction of the mass of traditional satellites
(usually <2 kg), and are relatively inexpensive to launch. These CubeSats comprise
a constellation of more than 130 small satellites, starting from Dove-1 launched in a
low sun-synchronous orbit in April 2013. Since then, the number of CubeSats has
continuously risen to reach 1,600 by August 2021. Jointly, they can cover 340
million km2 of the Earth’s surface daily (Planet Team, 2017). The operational
lifespan of a satellite is usually designed to be three to seven years, but some
satellites last exceptionally longer in orbit than this expectancy. Over the years,
CubeSats have evolved from lab experimental designs to practical applications in
remote sensing.
The details of the three types of sensing platforms are compared and contrasted
in Table 2.1. In general, the more expensive satellite images are more lasting,
recording millions of images at a finer temporal resolution than their less expensive
airborne counterparts. The higher the sensing platform, the more extensive the area
of ground cover. Of the three platforms, only aircraft offer the 3D capability
consistently but at a cost of massive photo overlap up to 60% endlap. It is hardly
possible to obtain 3D from drone images, but 3D views can be established from
certain satellite images. Given their coarse spatial resolution, however, it is
unrealistic to quantify any vertical changes from these satellite images, though.
Table 2.1
Comparison of three types of sensing platforms in acquiring remotely sensed data
Features Drone Aircraft Satellite
Sensing
altitude 10s m (<100 m) A few km 100s of km
Spatial
resolution 1 cm
Sub-meter to
meters
Submeters to
kms
Spatial
coverage Micro-scale Local scale
Regional and
global scales
Temporal
resolution
One-time off One-time off Repeatedly,
measured by2.2
Features Drone Aircraft Satellite
days and hours
Sensible
spectral
range
RGB, NIR only. No
SWIR bands
RGB, NIR,
LiDAR
RGB, NIR,
SWIR, TIR,
microwave
3D-viewing
ability Possible
Used to be the
norm
With certain
satellites
Deployment
Flexible on-demand
flight capacity at
critical times, prior
flight permission
needed. Minimal
control during flight
Careful
planning of
flight, guided
by GPS and
IMU
Highly subject
t o weather
conditions
All weather
conditions;
Fixed orbit
parameters,
Onboard GPS
and IMU for
quick geo￾referencing
Pro
Low operating cost;
Multi-sensor sensing,
Fine detailed images
Broad areal
coverage;
3D capability;
Reliable photo
geometry
Extensive and
repeated areal
coverage,
Broad spectral
range of
sensing;
Fine temporal
resolution
Cons
Short operating span;
Small area of coverage;
Less than ideal photo
geometry
Lengthy
process of
flight planning,
Expensive to
operate;
High degree of
photo overlap
Expensive to
launch
satellites;
Data processing
at ground
receiving
station;
Costly data
EARTH OBSERVATION SATELLITE DATA2.2.1
Earth observation (EO) satellites are those imaging satellites designed to study the
natural resources of the Earth and monitoring its environment. EO images are
acquired from various satellite missions, either commercial or public-funded. Most
space-borne images have a moderate resolution or coarser. Medium-resolution
optical images have the longest history of existence apart from meteorological
satellite images dating back to the 1960s, which makes them the ideal choice for
long-term quantification and monitoring of the quantified parameter. Satellite
images are unexceptionally multispectral to compensate for their coarse spatial
resolution (relative to aerial photographs). Multispectral images faithfully capture
the target of sensing under cloud-free conditions. They are rather good at revealing
the surface cover. Over the years the spatial resolution of satellite images has been
gradually refined with the exception of Landsat series images whose spatial
resolution remains stubbornly unchanged at 30 m. Although coarse resolution
optical images do not show much detail about the ground, they are the excellent
source of data for quantifying the concentration of trace elements in the atmosphere
and oceans on the regional or even continental scale. Some of the leading players of
OE satellites are detailed below.
Landsat 8/9 OLI
Landsat offers the longest time series data dating back to the late 1970s. The current
operational satellites, Landsat 8 and 9, both carry the identical Operational Land
Imager (OLI) payload. Its image format remains at 185 km×185 km. The temporal
resolution is still unchanged at 16 days, the same as the predecessors’. This
temporal resolution, however, has been shortened to eight days with the addition of
Landsat 9 OLI-2 launched on 27 September 2021 into a 705 km orbit. It replaces
Landsat 7 launched in 1999, and takes its orbit but with better radiometry and
geometry. Both satellites complete 14 revolutions per day at an orbital period of 98
minutes, acquiring more than 700 images. Apart from OLI-2, the payload of
Landsat 9 also comprises two TIR sensors. They acquire two TIR bands of a
coarser spatial resolution of 100 m than 30 m for all other bands except the
panchromatic band of 15 m resolution (Table 2.2).
Table 2.2
Spectral band designation, wavelength, and spatial resolution of Landsat 8/9 OLI images2.2.2
Band
Central
wavelength
Wavelength range
(nm)
Nadir spatial
resolution (m)
1.
Coastal/Aerosol 443 433–453 30
2. Blue 482 450–515 30
3. Green 562 525–600 30
4. Red 655 630–680 30
5. NIR 865 845–885 30
6. SWIR1 1610 1560–1660 30
7. SWIR2 2200 2100–2300 30
8. Panchromatic 590 500–680 15
9. Cirrus 1375 1360–1390 30
10. Thermal 1 10800 10300–11300 100
11. Thermal 2 12000 11500–12500 100
Sentinel-2 and -3
Of the two Sentinel-2 satellites, Sentinel-2A is an imaging satellite launched in
2015 as part of the European Space Agency’s (ESA) Copernicus Programme, and is
subsequently augmented by its sister satellite, Sentinel-2B on 7 March 2017. Both
follow an identical constellation of a sun-synchronous orbit of 786 km in height,
phased at 180° to each other on the same orbit, which halves the normal 10-day
revisit period to just five days at the equator (2–3 days at mid-latitudes). The
satellite completes 14.3 revolutions per day, with a 10:30 a.m. descending node,
sensing the Earth’s surface from 56°S to 84°N. Both satellites carry an identical
optical Multispectral Instrument (MSI) sensor of 13 spectral bands: four at 10 m,
six at 20 m, and three at 60 m spatial resolution (Table 2.3). This pushbroom sensor
acquires images at a swath width of 290 km created by the 12 VNIR and SWIR
detectors arranged in two offset rows.
Table 2.3
Comparison of the spectral properties of Sentinel-2A and -2B bandsBand
Sentinel-2A Sentinel-2B
Spatial
resolution
(m)
Central
wavelength
(nm)
Bandwidth
(nn)
Central
wavelength
(nm)
Bandwidth
(nn)
1 442.7 20 442.3 20 60
2 492.7 65 492.3 65 10
3 559.8 35 558.9 35 10
4 664.6 30 664.9 31 10
5 704.1 14 703.8 15 20
6 740.5 14 739.1 13 20
7 782.8 19 779.7 19 20
8 832.8 105 832.9 104 10
8a 864.7 21 864.0 21 20
9 945.1 19 943.2 20 60
10 1373.5 29 1376.9 29 60
11 1613.7 90 1610.4 94 20
12 2202.4 174 2185.7 184 20
Sentinel-3 is an EO satellite series launched by the ESA. At present, it consists of
two satellites: Sentinel-3A launched on 16 February 2016, and Sentinel-3B on 25
April 2018. Two recurrent satellites – Sentinel-3C and Sentinel-3D – will follow in
approximately 2024 and 2028, respectively, to ensure continuity of this mission.
Each Sentinel-3 satellite is expected to operate for seven years in a low sun￾synchronous orbit. Sentinel-3 builds directly on the heritage pioneered by ERS-2
and EnviSat satellites. Its payload comprises multiple sensors aiming to measure
sea surface roughness, land- and sea surface temperature and color, marine
ecosystems, water quality, pollution, and other features. Data are delivered in near￾real time for ocean forecasting, and water quality and pollution monitoring.
Of the five payloads of Sentinel-3, two optical imaging ones, Sea and Land
Surface Temperature Radiometer (SLSTR) and Ocean Land Color Instrument
(OLCI), are the most relevant to quantitative remote sensing of atmospheric and
oceanographic parameters. SLSTR has nine spectral bands, plus two additional
bands optimized for fire monitoring. The first six spectral bands cover the VNIR
and SWIR wavelengths at a spatial resolution of 500 m, while bands 7 to 9 as well
as the two additional bands have a spatial resolution of 1 km. These bands allow2.2.3
global sea surface temperatures to be quantified at an accuracy <0.3 K. OLCI is a
medium-resolution imaging spectrometer to be introduced in detail in Section 2.4.3.
RapidEye
As a member of the Earth-imaging system, the RapidEye constellation comprises
five identical satellites. Evenly spaced at an altitude of 630 km and about 19-minute
intervals, they all follow the same sun-synchronous orbital plane that is inclined by
97.9°, crossing the equator at a local time of 11:00 a.m. (±1 hour) on the descending
node. The deployment of the five satellites enables a huge number of images to be
acquired at an unprecedented frequency. All of the Earth’s surface between 84°N
and 84°S can be sensed daily with body pointing techniques. The revisit period
averages 5.5 days at nadir over mid-latitude regions (e.g., Europe and North
America).
All RapidEye satellite images share the same characteristics irrespective of the
satellites from which they are obtained. For instance, they all have a nadir
instantaneous-field-of-view (IFOV) of 6.5 m ground sampling distance that is
resampled to 5 m on orthorectified images. Such a fine spatial resolution enables
natural hazard features to be discernible and creates opportunities for quantifying
surface elevational changes (Figure 2.3). The satellite’s nadir FOV of ± 6.75º
corresponds to a ground swath width of 77 km. The view angle of RapidEye
imagery is always <20° and the large majority of RapidEye images has a view
angle <10°. RapidEye images entail five spectral bands of blue (440–510 nm),
green (520–590 nm), red (630–685 nm), red edge (690–730 nm), and NIR (760–
850 nm). The red-edge band is available in commercial satellites for the first time,
which boosts the possibility of quantifying chlorophyll content and its change.FIGURE 2.3 A post-event RapidEye image recorded on September 11, 2017 showing
landslides triggered by an earthquake in Jiuzaigou of Southwest China. (Yi and
Zhang, 2020, open access.)2.2.4 MODIS
Moderate Resolution Imaging Spectroradiometer (MODIS) is one of the five
sensors aboard the Terra/Aqua satellites launched by NASA. This multispectral
radiometer measures biological and physical processes in the terrestrial and hydro￾spheres using 36 spectral bands whose wavelengths span from visible light to TIR
(Table 2.4). These bands are designed to sense different targets optimally. For
instance, bands 3–7 are best for studying land/cloud/aerosol properties, bands 8–16
for estimating ocean color phytoplankton biogeochemistry, bands 17–19 for sensing
atmospheric water vapor on the regional or global scale. All MODIS bands have a
fine temporal resolution of up to six hours excellent for timely quantifying the
intensity of ephemeral sand and dust storms and estimating ocean current velocity.
MODIS images are available at three spatial resolutions of 250, 500, and 1000 m
related to band wavelength. The longer the wavelength, the coarser the spatial
resolution (Table 2.4). The shorter wavelength bands are the best for retrieving solid
atmospheric constituents while longer wavelengths are best used to quantify trace
gas concentrations in the atmosphere.
Table 2.4
Spectral bands, spatial resolution, and primary uses of MODIS imagery
Bands Resolution (m) Primary uses
1-2 250 Land/Cloud/Aerosols boundaries
3-7 500 Land/Cloud/Aerosols properties
8-16
17-19
20-23
24-25
26-28
29
30
31-32
33-36 1000
Ocean Color/Phytoplankton/Biogeochemistry
Atmospheric water vapor
Surface/Cloud temperature
Atmospheric temperature
Cirrus clouds / Water vapor
Cloud properties
Ozone
Surface/Cloud temperature
Cloud top altitude
The use of MODIS data for quantitative remote sensing has been eased by the
diverse data products derived from them using established algorithms operationally
for a long time. These products relieve researchers from deriving intermediate2.3
2.3.1
results themselves. Released at different spatial and temporal resolutions, these
MODIS data products pertain mostly to biophysical parameters (Table 2.5).
Compared to other data products of the same nature, MODIS data products have
extensive spatial coverage and a long history of data availability. In addition to
serving as the direct inputs to climate models, these data products can also function
as the ground truth, against which the reliability and accuracy of environmental
parameters retrieved from other remote sensing data or using different methods may
be verified and validated to indicate the accuracy of retrieval.
Table 2.5
Typical data products produced from combined MODIS images and their properties that are useful for
detecting dust storm intensity and estimating the concentration of solid particles in the atmosphere
Product name Description
Spatial resolution
(m)
Temporal
resolution
MCD12C1.006 Land cover 5600 Yearly
MCD12Q1.006 Land cover 500 Yearly
MCD12Q2.006 Land cover 500 Yearly
MCD12Q2.005 Land cover 500 Yearly
MCD15A2H.006 FPAR, LAI 500 Multi-day
MCD15A3H.006 FPAR, LAI 500 Multi-day
MCD18A1.006 Surface radiance 5600 Daily
MCD19A2.006
Aerosol optical
depth 1000 Daily
MOD09GQ
Band 1-2
reflectance 250 Daily
MOD09A1
Band 1-7
reflectance 500 8-Day
ATMOSPHERIC SATELLITE DATA
VIIRS
One of the key sensors aboard the Suomi National Polar-Orbiting Partnership
(Suomi NPP) spacecraft is the Visible Infrared Imaging Radiometer Suite (VIIRS)
successfully launched into orbit on October 28, 2011. This new generation ofsatellite with moderate resolution-imaging capabilities expands the legacies of
MODIS sensors by sensing the Earth’s terrestrial surface, cryosphere, ocean, and
atmosphere in 22 imaging bands over a revisit period of 16 days. They span the
wavelength range of 0.41 to 12.5 μm, including the spectral range of VIS (0.60-0.68
μm), SWIR (3.55–3.93 μm), and TIR (10.5–12.4 μm), of which VIS is the most
relevant to atmospheric sensing. These bands fall into three categories: 5 high￾resolution I-channels (spatial resolution = 375 m), 16 medium-resolution M￾channels (resolution = 750 m), and 1 panchromatic Day/Night channel (resolution =
750 m) (Table 2.6). VIIRS scans the Earth surface by rotating its telescope
assembly across track. Each scan comprises 32 along-track lines, and each image
line is composed of 6,400 pixels along scan. The actual pixel size of the VIIRS 375-
m data varies from 0.38 km×0.36 km at nadir to 0.79 km×0.78 km (along scan ×
along track) at the scan edge. In total, 43 data products are available from VIIRS
data at level-0 and level-1. They are useful to quantify cloud and aerosol properties,
ocean color, ocean, and land surface temperature (LST), and surface albedo.
Table 2.6
Properties and primary uses of 16 M-channels of NPP VIIRS imagery (resolution = 750 m)
Channels
Spectral
region
Wavelength range
(μm) Primary use
M1 Visible 0.402 – 0.422 Ocean color aerosol
M2 Visible 0.436 – 0.454 Ocean color aerosol
M3 Visible 0.478 – 0.498 Ocean color aerosol
M4 Visible 0.545 – 0.565 Ocean color aerosol
M5 NIR 0.662 – 0.682 Ocean color aerosol
M6 NIR 0.739 – 0.754 Atmospheric correction
M7 SWIR 0.846 – 0.885 Ocean color aerosol
M8 SWIR 1.23 – 1.25 Cloud particle size
M9 SWIR 1.371 – 1.386 Cirrus cloud cover
M10 SWIR 1.58 – 1.64 Snow fraction
M11 MWIR 2.23 – 2.28 Clouds
M12 MWIS 3.66 – 3.84 Sea surface temperature
M13 LWIR 3.97 – 4.13
Sea surface temperature
/fires2.3.2
2.3.3
Channels
Spectral
region
Wavelength range
(μm) Primary use
M14 LWIR 8.4 – 8.7 Cloud top properties
M15 LWIR 10.26 – 11.26 Sea surface temperature
M16 Day/night 11.54 – 12.49 Sea surface temperature
Himawari 8
Himawari-8 is an EO satellite launched on October 7, 2014 by the Japan
Meteorological Agency. Its payload encompasses the Advanced Himawari Imager
(AHI) sensor. AHI imagery has 16 channels, including three visible (0.47, 0.51, and
0.67 μm), three NIR, and 10 MIR and TIR channels (Table 2.7). All of them have a
spatial resolution of 2 km except channels 1, 2, and 4 (spatial resolution = 1 km),
and channel 3 (resolution = 500 m). AHI can scan the full disk of 120° × 120°
centered at (0°, 140°E) every 10 minutes, making it the ideal data source for
tracking dust storms and estimating their intensity, even though the temporal
resolution degrades over areas outside Southeast Asia, such as the Mid-East and
Africa. The application of AHI data in quantitative remote sensing is confined to
the estimation of sea surface temperature.
Table 2.7
Channel designations and wavelength of Advanced Himawari Imager (AHI)
Channel no. Central wavelength (μm) Channel no. Central wavelength (μm)
1 0.47 9 6.94
2 0.51 10 7.35
3 0.64 11 8.60
4 0.86 12 9.64
5 1.61 13 10.41
6 2.25 14 11.24
7 3.89 15 12.38
8 6.24 16 13.28
Source: www.jma-net.go.jp/msc/en/.
TMI2.3.4
The Tropical Rainfall Measuring Mission’s (TRMM) Microwave Imager (TMI) is a
passive sensor carried onboard the NASA’s Aqua and the Japanese ADEOS-II
satellites. It is designed to provide quantitative rainfall information over a wide
swath under the TRMM satellite, including the quantification of atmospheric water
vapor, cloud water, rainfall intensity, and the generation of the spatial distribution of
rainfall and 3D rain structure information. TMI data can provide precise
measurements of the minute amounts of microwave energy emitted by the Earth’s
atmosphere at five frequencies of 10.7, 19.4, 21.3, 37, and 85.5 GHz. These
frequencies are similar to those of the Special Sensor Microwave Imager (SSM/I), a
seven-channel, four-frequency, linearly polarized passive microwave radiometer
system. The additional 10.7 GHz channel of TMI is designed to provide a more
linear response for high rainfall rates commonly found in tropical areas. Another
main improvement of TMI is the refined image spatial resolution triggered by the
lowering of satellite altitude to 402 km of TRMM from 860 km of SSM/I. The
additional information supplied by the Precipitation Radar further helps to improve
algorithms.
MISR
The Multi-angle Imaging SpectroRadiometer (MISR) is one of the five instruments
aboard NASA’s Terra spacecraft that was launched into a polar, sun-synchronous
orbit in August 1999. It crosses the equator every 98 minutes, always at 10:30 a.m.
local time. This innovative instrument is subsumed with cameras. They view the
Earth at nine spread out angles, one of them is nadir, and the others are in the
forward and aft directions along the flight path to detect the solar radiation scattered
in multiple directions under natural conditions so as to fully understand Earth’s
climate. The MISR sensor detects Earth’s brightness in four spectral bands (blue,
green, red, and NIR) at nine look angles. As the satellite orbits above the ground,
the Earth’s surface is successively imaged by all nine cameras across a 400 km￾wide swath. MISR covers the entire globe about once every nine days at the
equator. Spatial samples are acquired every 275 m that are calibrated accurately
both absolutely and relatively using onboard hardware such as deployable solar
diffuser plates and several types of photodiodes.
In addition to improving our understanding of the fate of sunlight in the Earth’s
atmosphere, MISR data can also distinguish different types of clouds, aerosol
particles, and surfaces. Specifically, monthly, seasonal, and long-term MISR data2.4
2.4.1
2.4.2
prove invaluable in studying the trends in the amount and type of atmospheric
aerosol particles from natural processes and anthropogenic activities. MISR data
and data products are freely accessible from the Atmospheric Science Data Center
of the NASA Langley Research Center
(https://eosweb.larc.nasa.gov/project/misr/misr_table).
METEOROLOGICAL AND OCEANOGRAPHIC
SATELLITES
ASCAT
Advanced Scatterometer (ASCAT) is a real aperture radar sensor onboard the
MetOp satellite launched in 2006. It detects backscattered microwave radiation of
5.255 GHz (C-band) in VV polarization, and acquires daily measurements for
central Europe from an altitude of around 837 km at a spatial resolution of 25 km.
The instrument transmits vertically polarized pulses of microwave radiation
towards the sea surface that modifies the backscattering characteristics of the
transmitted radar pulses in windy conditions and causes cm-scale disturbance. Its
antennas scan a 500-km-wide ground swath on both sides of the satellite path as the
satellite revolves around the Earth. ASCAT is designed to detect the
electromagnetic radiation backscattered from ocean surface using two sets of
vertically polarized radar antennas. The three antennas on each side are oriented to
±45° of broadside, a configuration able to sequentially observe the backscattering
coefficient of the target to determine the speed and direction of winds over the
surface of the oceans from three directions. This capability is indispensable to
resolve the wind direction ambiguity. Surface wind images are created hourly from
ascending and descending images of 25 km×50 km in dimension.
In quantitative remote sensing, ASCAT data are used mostly for quantifying
surface wind speed and direction over the oceans from three directions. Besides,
ASCAT data are also useful for studying ice, snow, and soil moisture. ASCAT
backscatter measurements have been converted to the surface soil moisture (SSM)
data product using the time series-based change detection approach. The derived
SSM product corresponds to a depth of 2–3 cm and ranges between 0% (dry) and
100% (wet) representative of relative soil moisture saturation.
MERIS2.4.3
The MEdium Resolution Imaging Spectrometer (MERIS) is a programmable
spectrometer onboard the EnviSat satellite operational during 2002–2012. The
instrument is composed of five pushbroom spectrometers arranged side by side. It
captures the reflective solar energy received from the target in 15 bands. Their
width, gain, and position are all programmable. The spectrometer scans the Earth’s
surface every 300 m near nadir at a total FOV of 68.5°, corresponding to a ground
swath width of 1,150 km. The acquired imagery is known as the “Full Resolution”
product. It is aggregated to a nominal resolution of 1,200 m to produce the more
common “Reduced Resolution” product. This dual spatial resolution, in conjunction
with MERIS’ high spectral and radiometric resolutions, makes the data the ideal
candidate for quantifying the aquatic color of open oceans and coastal waters
around the world, with a particular leaning towards optically complex inland and
coastal waters. MERIS contains a suite of red and NIR bands that target natural
fluorescence and red-edge signatures characteristic of algal blooms. They offer an
alternative means to the standard blue to green band ratio in quantifying chlorophyll
(Chl) concentrations in optically complex waters and terrestrial plants.
Ocean and Land Color Instrument
The new generation of ocean color satellite Sentinel-3 offers a new data source for
remote sensing of ocean color. One of its payloads is OLCI that uses five cameras
to yield a wide FOV. OLCI makes use of along-track pushbroom scanning to avoid
scale distortion near the edge of an image. With both Sentinel-3 satellites in orbit,
the combined swath width of the two OLCI facilities allows the Earth to be sensed
almost daily around the globe to meet the requirements of high-frequency dynamic
monitoring of oceanic parameters. This apparatus inherits the band designation of
the MERIS sensor, but with six additional spectral bands in the blue, red, and NIR
spectrum at a maximum spatial resolution of 300 m. Its 21 spectral bands have a
sensible spectral range of 400–1020 nm, and most of them have a spectral width
<10 nm (Table 2.8), and a higher-end signal-to-noise ratio (SNR). They are
designed to assess the spectral characteristics of phytoplankton, and offer abundant
options for quantitatively sensing water color elements. OLCI data are best posed to
quantify water vapor and chlorophyll absorption, and aerosol content because one
of the bands covers the characteristic peak of phycocyanin absorption at 620 nm.
The high SNR of OLCI data means that they can accurately reflect the water
spectral characteristics, which is facilitated by its wide swath of 1270 km not2.4.4
centered at nadir to minimize sunglint. In this way, OLCI is perfectly positioned for
quantifying aquatic parameters of inland and coastal aquatic ecosystems. As a
matter of fact, it is the only functional sensor able to detect cyanobacteria in the
hydrosphere at present.
Table 2.8
Spectral characteristics of OLCI bands (spatial resolution: 300 m, temporal resolution: <2d)
Band
Central
wavelength (nm)
Bandwidth
(nn) Band
Central
wavelength (nm)
Bandwidth
(nn)
Oa1 400 15 Oa12 753.75 7.5
Oa2 412.5 10 Oa13 761.25 2.5
Oa3 442.5 10 Oa14 764.375 3.75
Oa4 490 10 Oa15 767.7 2.5
Oa5 510 10 Oa16 778.75 15
Oa6 560 10 Oa17 865 20
Oa7 620 10 Oa18 885 10
Oa8 665 10 Oa19 900 10
Oa9 673.75 7.5 Oa20 940 20
Oa10 681.25 7.5 Oa21 1020 40
Oa11 708.75 10
Geostationary Ocean Color Imager
Geostationary Ocean Color Imager (GOCI) is the world’s first geostationary
imaging sensor onboard the Communication, Ocean, and Meteorological Satellite
of South Korea launched on June 27, 2010. It is designed specifically to monitor
ocean color over the wavelength range of 412–865 nm in eight spectral bands (the
last two NIR bands are used mostly for atmospheric correction, vegetation, water
vapor reference over the ocean) (Table 2.9). GOCI images comprising 5,685×5,567
pixels apiece are acquired hourly at a spatial resolution of ⁓500 m. Within 24
hours, eight images are obtained during the daytime and two during the night time.
They encompass a ground area of about 2,500 km×2,500 km centered on the
Korean Peninsula (130°E, 36°N). GOCI has been superseded by GOCI-II launched
on February 18, 2020. It has 13 spectral bands of 250 m resolution over the range of2.5
380–900 nm. In particular, the images have pointable local area and full disk
coverage. Daily imaging frequency has improved from eight to ten.
GOCI data are excellent at retrieving chlorophyll concentration, the optical
diffuse attenuation coefficients, the concentration of dissolved organic material or
yellow substance, and the concentration of suspended particles of the sea. A variety
of data products have been derived from GOCI data, including reflectance,
chlorophyll concentration, total suspended solids concentration, absorption by
dissolved organic matter, diffuse attenuation coefficient for downward irradiance,
Secchi depth, marine primary productivity, red tide, surface layer current, aerosol
optical depth (AOD), aerosol type (yellow sand), terrestrial vegetation index, and
enhanced vegetation index (Yu et al., 2022).
Table 2.9
Wavelength designation, quality, and primary use of GOCI spectral bands (spatial resolution: 500 m,
temporal resolution: 1 h)
Band
Central
wavelength
(nm)
Bandwidth
(nn) SNR Primary application
1 412 20 1,000 Yellow substance and turbidity
2 443 20 1,090
Chlorophyll absorption
maximum
3 490 20 1,170 Chlorophyll and other pigments
4 555 20 1,070 Turbidity, suspended sediment
5 660 20 1,010
Atmospheric correction for turbid
water, baseline of fluorescence
signal, chlorophyll, suspended
sediment
6 680 10 870 Fluorescence signal
7 745 20 860
Atmospheric correction and
baseline of fluorescence signal
8 865 40 750
Atmospheric correction,
vegetation, water vapor reference
over the ocean,
HYPERSPECTRAL DATA2.5.1
Hyperspectral data offer hundreds of spectral bands with a bandwidth measured by
nm. These bands can reveal the subtle and minute variations in reflectance caused
by a low concentration of nutrients and pollutants. They are especially
indispensable in quantifying the biochemical contents of vegetation and in-water
constituents. Hyperspectral data are obtainable both airborne and space-borne using
several sensors.
Airborne Data
Airborne data are acquired by mounting the sensor (most likely a camera) in light
aircraft. It flies in calm and sunny weather conditions when the atmosphere is rather
stable with little turbulence. The aircraft flying at an altitude of approximate 2,000
m above the ground corresponds to a swath width of roughly 1,500 m (with a
typical FOV of 40°). Image resolution is dictated jointly by the flying height and
the camera’s focal length (or the IFOV of a scanner). A typical hyperspectral
imaging spectrometer has a spectral sampling interval of ~5 nm and a spectral
resolution of ~5–6 nm, depending on the wavelength (Chapman et al., 2019).2.5.1.1
2.5.1.2
FIGURE 2.4 Comparison of spectral resolutions and the number of spectral bands of
four types of sensing, of which ultraspectral data are rarely used in quantitative
remote sensing. (Ihab, 2017.)
AVIRIS-NG
The Airborne Visible InfraRed Imaging Spectrometer – Next Generation (AVIRIS￾NG) is a nadir-viewing sensor designed to measure solar radiation reflected off the
ground over the wavelength range of 380–2,450 nm (Thorpe et al., 2016). It has a
high SNR of up to 800 at 2,200 nm and a meter-scale spatial resolution whose exact
value depends on flight altitude and flight speed, with typical values of 3 × 3 to 5 ×
5 m2. The instrument contains 600 spatial pixels, each having a 1 mrad FOV. This
results in individual samples with 5 m spatial resolution and a 3 km swath from a
typical flying height of 5 km above the ground. AVIRIS-NG is best at quantifying
trace gas emissions near the Earth’s surface at a high accuracy.
CASI2.5.1.3
Compact Airborne Spectrographic Imager (CASI) is an airborne hyperspatial
resolution sensor that records the captured radiance in 188 contiguous narrow
spectral bands. They span the spectral range of 380–1050 nm, including the entire
reflective region of visible, NIR and SWIR wavelengths. CASI uses VNIR
pushbroom scanning to acquire images of a 20 × 20 μm pixel size at a spectral
resolution <3.5 nm. Pixel size or spatial resolution is ~0.5 mrd, subject to the lens
and system configuration (i.e., flight height). CASI-1500 has 48 spectral bands
whose wavelengths span from 382.5 nm (visible band) to 1055.5 nm (NIR band),
and a ground resolution of 1 m. Current versions of CASI are very configurable.
For instance, the maximal 288 spectral bands of CASI-1500 can be “binned” on￾chip flexibly, subject to the specification by the user (e.g., binning by 4 produces 72
bands). The sensor can operate in both spatial and spectral modes. The spatial mode
allows the sensor to be configured essentially as a multispectral imager. The
spectral mode enables the selection of the on-chip binning to produce a contiguous
hyperspectral cube.
HyMap
The HyMap sensor is a fully enclosed optomechanical system scanning the Earth
through a window, usually mounted in a three-axis, gyro-stabilized sensing
platform. This imaging system contains four spectrometers that continuously
acquire 126 spectral bands spanning the wavelength range of 0.45–2.5 μm minus
the two major water absorption atmospheric blockades. Image bandwidths vary
between 15 and 18 nm. The measured spectral radiance with a sun angle of 30° and
a 50% reflectance standard has a SNR of >500:1 except near the major water
absorption bands. The sensor has an FOV of 62° (512 pixels), and its IFOV
measures 2.5 mr (along track) × 2.0 mr (across track), resulting in a spatial
resolution of 3.5–10 m (Table 2.10). The exact resolution varies with the flight
height (2–5 km above the ground level). The main applications of HyMap data in
quantitative remote sensing are related to the estimation of biophysical parameters
of vegetation in the biosphere.
Table 2.10
Typical envelopes of images acquired with the HyMap sensor
Spectral sensitivity VIS, NIR, SWIR, TIR
Number of channels 100–2002.5.1.4
2.5.1.5
Spectral sensitivity VIS, NIR, SWIR, TIR
Spectral bandwidth 10–20 nm
Spatial resolution 2–10 m
Swath width (FOV) 60–70°
Signal to noise ratio (30° solar zenith angle, 50%
reflectance) > 500:1
Operational altitude
2–5 km above the ground
level
Source: Cocks et al. (1998).
Hyperspectral Infrared Imager
The NASA Hyperspectral InfraRed Imager (HyspIRI) mission carries two sensors,
a visible to short-wavelength infrared (VSWIR) imaging spectrometer and a
multispectral TIR imager, both having an intelligent payload module for onboard
processing and rapid downlink of selected data. The VSWIR sensor has contiguous
bands of 10 nm spectral resolution over the 380–2500 nm spectral range with a
spatial resolution of 30 m and a revisit period of 16 days. Of the eight TIR spectral
bands in the 4–13 μm range, one lies in the middle infrared (MIR) region at 4 μm
and seven in the longwave IR (LWIR) region between 7 and 13 μm with a spatial
resolution of 60 m and a revisit period of five days at the equator (Lee et al., 2015).
The MIR band has a high saturation limit of 1200 K, much higher than 400–500 K
of the longer wavelength bands. Both MIR and LWIR bands have a quantization
level of 14 bits with a radiometric accuracy of 0.5 K and 0.2 K, respectively. The
image SNR varies with wavelength, decreasing from 700:1 at 600 nm to 500:1 at
2,200 nm.
The HyspIRI scanner is titled 4° backward to reduce the effect of sun glint from
water surfaces, as such HyspIRI imagery is best suited to study freshwater
ecosystems. Nominal data collection scenario involves observing the land and
coastal zones to a depth ≤50 m at full spatial and spectral resolutions and
transmitting these data to ground stations. Over the open ocean, data are averaged
to a spatial resolution of ~1 km to minimize the spectral variation caused by waves
and ripples.
WiSHiRaPHIThe Wide Swath and High Resolution Airborne Pushbroom Hyperspectral Imager
(WiSHiRaPHI) is a new generation of imaging sensors aiming to acquire accurate
spectral information of the target at both a high spatial resolution and a high
spectral resolution. It can operate in one of three modes: high spectral resolution,
high spatial resolution, and high sensitivity to suit the needs of disparate
applications (Zhang et al., 2019). It partitions the 400–1000 nm sensible spectral
range into 3.5 nm in 256 channels at an interval of 3.5 nm. They are reduced to 64
channels in the high spatial resolution mode, in which the spectral resolution
decreases to 9.2 nm via modifying the velocity-height ratio (0.02–0.04). The
spectral resolution is resmapled to 3.5 nm in the full-width, half-maximum mode.
WiSHiRaPHI image spatial resolution ranges from 12.5 cm in the high spectral
mode to 25 cm in the high spatial resolution mode at a flying height of 1,000 m.
The instrument has a 40° FOV and 0.125 mrad IFOV in the high spatial mode. It
doubles to 0.25 mrad in the high spectral mode, with the SNR exceeding 500:1. Of
all the eight airborne hyperspectral sensors in Table 2.11, WiSHiRaPHI is the most
versatile and flexible, offering high-resolution data in multiple modes, albeit its
spectral sensing range being much narrower than that of most other sensors.
Table 2.11
Main features of eight airborne hyperspectral sensors
Sensor
Sensitivity
range
(μm)
Spectral
resolution
(nm)
No. of
channels FOV(°) IFOV(mrd)
AVIRIS 0.4–2.5 10 224 30 1
LEISA 1.0–2.5 4–10 432 19 2
AisaFENIX 0.38–2.5
3–5
(0.38–
0.97 μm)
12 (0.97–
2.5 μm) 448 32.3 1.4
OMIS 0.1–12.5 10 (0.46–
1.1 μm)
30 (1.1–
1.7 μm)
128 73 1.5/32.5.2
2.5.2.1
Sensor
Sensitivity
range
(μm)
Spectral
resolution
(nm)
No. of
channels FOV(°) IFOV(mrd)
15 (2.0–
2.5 μm)
2 μm (3–5
μm)
600 (8.0–
12.5 μm)
PHI 0.4–0.85 1.8 244 21 1.5
Hymap 0.4–2.5 10–20 128 61.3 2 × 2.5
CASI/SASI 0.4–2.5 2.4/7.5 96/200 40 0.49/0.698
WiSHiRaPHI 0.4–1.0
3.5/9.2,
adjustable
256/64,
adjustable 40
0.25/0.125,
adjustable
Source: Modified from Zhang et al. (2019), open access.
Note: OMIS = operational modular imaging spectrometer; PHI = Pushbroom Hyperspectral Imager; SASI =
Short-wave-infrared Airborne Spectrographic Imager (SASI).
Space-borne Sensors
IASI
Infrared Atmospheric Sounding Interferometer (IASI) is a hyperspectral sensor
onboard the polar-orbiting MetOp meteorological satellites. This series of satellites
totals three, of which MetOp-B was launched into a sun-synchronous mid-morning
orbit on September 17, 2012, followed by MetOp-C on 7 November 2018. It
acquires nadir data twice a day at 09:30 am (local time equator crossing,
descending node) and 09:30 pm globally. IASI detects the infrared portion of the
electromagnetic spectrum with a small to medium footprint from a 12-km-diameter
circle (horizontal resolution) at nadir to an ellipse of 20 km × 39 km at the maximal
swath width of around 2,200 km. This state-of-the-art sensor offers an uninterrupted
spectral sensing range from 645 to 2,760 cm−1 at a spectral resolution of 0.5 cm−1
with noises mostly <0.2°K below 2,000 cm−1. At a nadir FOV of 48.3°, IASI scans
the imaged area in 15 views (orbits). Although this mission strives primarily to
supply data about atmospheric conditions (e.g., temperature and water vapor) in
near real-time to facilitate weather prediction, IASI spectra also allow the retrieval2.5.2.2
2.5.2.3
of the concentrations of various trace gases. Various data products have been
derived from IASI data, including temperature, mixing ratio of water vapor, mixing
ratio of ozone, mixing ratio of carbon monoxide, mixing ratio of carbon dioxide,
mixing ratio of methane, and skin temperature. The IASI level-2 products from
MetOp-B satellite encompass temperature and humidity profiles with a vertical
accuracy of 1° K and 10% per 1-km layer, respectively.
Compact High Resolution Imaging Spectrometer
Compact High Resolution Imaging Spectrometer (CHRIS) is the major payload of
the Project for On-Board Autonomy (PROBA)-1 satellite that maintains a sun￾synchronous elliptical polar orbit of 615 km in height. It resolves the Earth at an
inclination of 97.9°, completing one revolution every 96.97 minutes. CHRIS
possesses 19 fully programmable spectral bands in the VNIR range (400–1050 nm).
Each image covers a nominal ground area of 13 km × 13 km (at perigee) at a
ground sampling distance of 17 m. This configuration can be altered to produce 63
spectral bands (up to 150 channels) at a spatial resolution of about 34 m. CHRIS
imagery is available in two spectral resolutions of 1.25 and 11 nm. Both the exact
number of spectral bands and bandwidth vary with the operational mode that totals
five. Mode 1 (full swath width) has 62 spectral bands over 773–1036 nm, and a
nadir sampling distance of 34 m and a swath width of 556 km on the ground. In
mode 2 (water bands, full swath width), there are 18 spectral bands, and the nadir
ground sampling distance is halved to 17 m, the same as in mode 3 (land channels)
and mode 4 (Chl band set). The swath width is also halved to 6.5 km in mode 5
(land channels) but the number of spectral bands rises to 37, while the nadir ground
sampling distance remains unchanged at 17 m. CHRIS data are suitable for
studying the atmospheric, terrestrial, oceanic, and coastal environments.
ZY-1 02D Hyperspectral Sensor
ZY-1 02D is a hyperspectral radiometer onboard the namesake satellite that was
launched into orbit on September 12, 2019. It contains an MSI and a hyperspectral
imager (HSI) in the same time-space. Both can function concurrently independent
of each other during data acquisition. The HSI sensor acquires images at a spatial
resolution of 30 m (swath width: 60 km) in 166 spectral bands, of which 76 are
VNIR at a spectral resolution of 10 nm, and 90 are SWIR (spectral resolution: 20
nm) over the spectral range of 400–2500 nm (Lu et al., 2021). MSI images have a2.5.2.4
spatial resolution of 10 m, comprising eight spectral bands over the spectral range
of 486–959 nm (swath width: 115 km).
GF-5 Hyperspectral Sensor
The Chinese GaoFen-5 (GF-5) satellite was launched into space on May 9, 2018,
and one of its six primary payloads is the advanced hyperspectral imager. This
sensor has 330 spectral bands over the wavelengths from 400 to 2,500 nm. Also
onboard the GF-5 satellite are two imaging spectrometers: a VNIR spectrometer
and a SWIR spectrometer. The former has a spectral sensitivity ranging from
approximately 400 to 1,000 nm. This range is split into 150 bands at an
approximate spectral resolution of 4 nm and a spatial resolution of 30 m. The SWIR
spectrometer detects radiation over the range of 1,000–2,500 nm in 180 bands at a
spectral resolution of 10 nm. The acquired hyperspectral images have a swath width
of 60 km. Its spatial resolution surpasses or equals that of most other on-orbit or
planned space-borne high spatial resolution missions (Table 2.12). GF-5
hyperspectral data have an SNR of >200:1 for VNIR bands and >100:1 for SWIR
bands.
Table 2.12
Comparison of major parameters of space-borne hyperspectral sensors onboard recently launched remote
sensing satellites
Sensor
GF-5
AHSI DESIS HYSIS
PRISMA
HSI
EnMAP
HSI
ALOS-3
HISUI
Country China Germany India Italy Germany Japan
Launch
time 2018 2018 2018 2019
2020
scheduled 2019
Spectral
range
(μm) 0.4–2.5 0.4–1.0
0.4–
2.5
0.4–
2.5 0.42–2.45 0.4–2.5
No. of
channels 330 235 55 239 >240 185
Spectral
resolution
(nm)
5
(VNIR)
10
(SWIR) 2.55 10 <12
6.5
(VNIR)
10
(SWIR)
10
(VNIR)
12.5
(SWIR)2.6
Sensor
GF-5
AHSI DESIS HYSIS
PRISMA
HSI
EnMAP
HSI
ALOS-3
HISUI
Spatial
resolution
(m) 30 30 30 30 30 30
Swath
width
(km) 60 30 30 30 30 30
Source: Ren et al. (2020), open access.
Note: GF-5 AHSI: Advanced Hyperspectral Imager; DESIS: DLR Earth Sensing Imaging Spectrometer;
HYSIS: ISRO’s Hyperspectral Imaging Satellite; PRISMA: PRecursore IperSpettrale della Missione
Applicativa; EnMAP: Environmental Mapping and Analysis Program; HISUI: Hyperspectral Imager Suite.
VNIR - visible near infrared; SWIR - shortwave infrared.
ACTIVE SENSING DATA
Optical images, be they multispectral or hyperspectral, suffer a vulnerability to
contamination by clouds. This drawback can be effectively overcome by radar
sensing. This type of sensing makes use of long wavelength microwave radiation
that is able to penetrate clouds to some degree. A common form of radar sensing is
known as synthetic aperture radar (SAR). This all-weather condition, active means
of sensing functions by first transmitting polarized electromagnetic pulses towards
the target and subsequently recording the radar signals backscattered by the target,
and by measuring the two-way travel time between the target and the sensor. The
distance between them is determined from the radiation’s velocity of propagation
which is a constant. Essential to all radar sensors is the antenna, a device that
operates in two alternative modes of transmission and reception. In the former
mode, it serves as a transmitter that transmits microwave pulses over a very short
duration on the order of micro-seconds towards the target of sensing. In the latter
mode, it serves as the receptor by recording the pulses scattered back from the
target of sensing. The echoes (signals) from the target are time-dependent, and their
amplitude is proportional to the intensity of radar return.
It is of particular notice that radar antennas capture the backscattered energy
from the target in the slant direction at an angle to the ground surface, hence is
extremely sensitive to micro-scale surface relief and topography (e.g., slope
orientation). Microwave sensors offer additional spectral information that2.6.1
supplements optical reflectance, and have found wide applications in quantitative
remote sensing. Two of them are featured in this section, TerraSAR and
interferometric SAR (InSAR).
TerraSAR
TerraSAR-X is a radar satellite successfully launched into a sun-synchronous orbit
(inclination: 97.44°) of 514 km above the Earth by the German Space Center on
June 15, 2007. It acquires multi-mode and high-resolution X-band radar images at a
spatial resolution up to 1 m. The exact resolution varies with the mode of scanning.
In the SpotLight mode, TerraSAR images have a resolution up to 1 m, covering a
ground area of 10 km (width) × 5 km (length) (Figure 2.5). The resolution drops to
3 m in the StripMap mode that enables a larger ground area of 30 km (width) × 50
km (length) to be sensed in one image. The resolution decreases further to 16 m in
the ScanSAR mode, even though each image encompasses a much larger ground
area of 100 km (width) × 150 km (length). The sensor is highly agile, able to switch
between imaging modes and between HH, VH, HV and VV polarizations quickly.
Owing to the use of an adjustable angle radar sensor, TerraSAR-X images can sense
any place on Earth within 1–3 days, even though the standard revisit period is 11
days for a specific point on the equator. The revisit time prolongs typically to 3–4
days for polar regions such as Northern Europe.2.6.2
FIGURE 2.5 Three modes of TerraSAR data acquisition along the flight path.
As the name implies, TerraSAR-X is best at producing fine-resolution and accurate
digital elevation models (DEMs) that may enable the detection of surface
movements such as excavation-triggered ground subsidence and landslides, and in
estimating biophysical parameters of vegetation. Vertical changes of the surface can
be reliably quantified owing to the high radiometric accuracy (absolute: 0.6 dB;
relative: 0.3 dB in the StripMap mode) and the high absolute geometric accuracy of
1 m under all normal imaging conditions combined with a high spatial resolution.
Sentinel-1 SAR
The Sentinel-1 constellation comprises two identical SAR satellites, A and B.
Sentinel-1A was launched on April 3, 2014, joined by Sentinel-1B on April 25,
2016. They half the normal revisit period of 12 days to just six days. Sentinel-1
significantly increases the availability of publicly accessible SAR data. Both
satellites carry a C-band sensor that operates at two incidence angles of 23° and
36°, alternating along the satellite orbital direction at an interval of 100 km.
Sentinel-1 image swath varies with scanning mode. In the interferometric wide￾swath mode, it is 250 km wide. Each single-look complex (SLC) image is2.6.3
2.6.1.1
composed of three sub-swaths, stemming from successive bursts by electronically
steering the SAR antenna between adjacent sub-swaths. In this way, the acquired
images of a large swath width do not suffer obvious scalloping and excessive noise.
Sentinel-1 data with pre-defined observational scenarios are archived and freely
accessible through the Copernicus Open Access Hub
(https://scihub.copernicus.eu/). These data produce potentially better coherence
than other SAR images owing to the enhanced spatial (2.7 m × 22 m) (range ×
azimuth) and radiometric resolutions. Active Sentinel-1 data not affected by clouds
are an excellent data source for quantifying wave heights in the hydrosphere and
tree height in the biosphere. The short temporal resolution of Sentinel-1 imagery
enables the acquisition of interferograms with a short temporal baseline ideal for
quantifying ground subsidence and the rapid quantification of debris volume.
LiDAR Data
Light detection and ranging (LiDAR) sensing is an active means of quickly
acquiring a huge volume of 3D dense point cloud data about the target. LiDAR
sensing systems fall into two types, discrete point return and continuous waveform
(Lefsky et al., 2002). The former collects estimates of a limited number of heights
by identifying major peaks representing discrete objects in the return signal of the
laser. Such systems have a high laser repetition rate and a footprint with a small
diameter. The collected data have a relatively high density of distribution and are
ideal for constructing a detailed model of surface relief such as forest canopy
(Lefsky et al., 2002). Waveform systems record the time varying intensity of the
energy returned by each laser pulse to obtain the height information of a surface.
They are able to collect dense points over larger areas than is possible with the use
of point return systems.
In quantitative remote sensing, LiDAR scanning may be accomplished via three
manners: airborne laser scanning (ALS), terrestrial laser scanning (TLS), and
mobile laser scanning (MLS). In MLS, the LiDAR scanner is deployed on a mobile
platform such as an all-terrain vehicle. It provides a much broader coverage of the
study area within the same period than TLS, but at a lower accuracy level that is
unsuited for precise quantification, so is not delved into further.
Terrestrial LiDAR Scanning2.6.1.2
In terrestrial LiDAR surveys, the target is scanned by a stationary, rotating scanner
mounted atop a tripod near the front of the area or object of study. The area of
interest is scanned through the horizontal rotation of the scanner, usually less than
180°. A single scan, multi-scans, and multi-single-scans may be carried out from
fixed positions on the ground. Multi-scans are undertaken to cover a large ground
area, but stitching multiple scans may be a time-consuming task that can introduce
a source of geometric unreliability to the stitched tile. TLS can be undertaken using
the novel build-in LiDAR sensors in iPad Pro 2020 and iPhone 12 Pro released by
Apple in 2020. In the scanning mode, the iPhone’s LiDAR sensor generates a mesh
of the close surroundings (< 5 m). It can measure the distance to surrounding
objects up to 5 m in both the indoor and outdoor settings. The LiDAR sensors are
able to create accurate high-resolution models of small objects with a side length
>10 cm at an absolute accuracy of ±1 cm (Luetzenburg et al., 2021). Such data are
useful for quantifying vertical movements such as erosion rate of a coastal cliff. A
3D model with a dimension of up to 130 m×15 m×10 m has an absolute accuracy of
±10 cm. A major source of model inaccuracy is hand shaking during data
acquisition if the personal laser scanner is hand-held. The short range of scanning
restricts such TLS to small areas only. Despite this, TLS is still an efficient way of
collecting ground truth data about tree diameters to verify the results gathered from
airborne LiDAR data.
TLS is good at studying surface elevation change on a steep slope or cliff. The
accuracy is higher than airborne LiDAR scanning because the scanner is immobile,
so its position and orientation can be easily maintained to a high accuracy level on
the ground. It is also rather inexpensive and versatile. The major drawback of TLS
is the prevalence of blind spots in the acquired data where no LiDAR pulses can
reach in areas of a high surface relief. Besides, the point cloud data do not have a
spatially uniform density that varies with the range of scanning. Namely, the
density is higher at a closer range of scanning than at a more distant range. For
instance, the iPhone 12 acquired point cloud data have a potential point density that
follows a linear trend on a logarithmic scale with 7,225 points×m−2 at a range of 25
cm that further decreases to only 150 points×m−2 at the range of 250 cm
(Luetzenburg et al., 2021). This disadvantage can be effectively avoided with ALS.
Airborne Laser Scanning
Non-imagery ALS shares the same principle of sensing as active radar sensing
except the energy used being laser light of 0.8–1.6 μm wavelengths. The laserscanner transmits laser pulses towards the target at a short duration up to 25,000
times per second and receives the signals bounced back from the target (Figure 2.6).
Through timing the duration (∆t) of laser signal propagation, the distance (D) of
pulse transmission from the sensor to the target and back to the sensor is half of the
product of the velocity of laser light and ∆t, orFIGURE 2.6 Sketch illustrating how airborne laser scanner senses the target feature.
The sensing platform (aircraft) is equipped with a GPS and an IMU to measure its
position (X, Y, Z) and orientation (φ, ω, κ) at any given moment, respectively, that
can be used to convert sensing range to ground coordinates. (Gao, 2023.)(2.1)
where C = velocity of light (299,792,458 m×s-1); ∆t = duration of the pulse
transmission from the laser scanner to the target and back. The target of interest
may be scanned at a point density up to 10 points×m-2.
Both the positional and orientational information of the laser scanner is precisely
known at a given moment during scanning via two onboard coupled instruments of
IMU and GPS. The former tracks the sensor’s orientational information (φ, ω, κ)
while the latter continuously logs its positional information (Xs, Ys, Zs) at the time
of transmission (Figure 2.6). During scanning, the LiDAR sensor captures and
records both the range of a given target from the sensor and its associated range
angle at the time of laser pulse transmission. Virtually, raw LiDAR data comprise
just a gigantic volume of slant ranges and range angles. The laser slant range
calculated using Eq. 2.1 is subsequently converted to 3D increments from the
scanner to the scanned spot in a global coordinate system with the assistance of the
recorded range angles through trigonometry. The 3D coordinates (Xt, Yt, Zt) of the
target are determined by adjusting the scanner position by these increments in the
respective direction. In the decades following LiDAR advent in the 1960s, it has
been found rare applications in quantitative remote sensing due to the lack of
positioning capability. This situation changed in the 1990s when mature GPS could
reach a reasonable positioning accuracy, and powerful LiDAR scanners became
widely available. At present, laser scanning is used mostly for quantitative sensing
of elevational information related to the terrestrial surface or vegetation height.
Whether LiDAR can be competently applied for certain quantification depends
on its positioning accuracy that is influenced by several factors, including the GPS
and IMU accuracy, the atmospheric condition, and the LiDAR signals bounced
back from the target. The accuracy of LiDAR data may be improved by deriving
the absolute aircraft positions (trajectory) from a differential, geodetic GPS network
that can eliminate consistent offsets in the GPS signals. The quantification of height
from LiDAR data is founded on the fact that not all LiDAR pulses are bounced
back from the target simultaneously. In fact, LiDAR returns are arriving at the
sensor a number of times from different parts of the same target. The most crucial
returns to height determination are the first return and the last return. The first
return is bounced back from the part of the target closest to the sensor (e.g., tree
top) that is first encountered by the propagating pulses (Figure 2.7). The last return
is bounced back from the bare ground further away from the LiDAR scanner afterthe LiDAR pulses have penetrated the canopy in their path of propagation. It can be
used to determine the height of the bare earth. The subtraction of the last return
from the first return virtually isolates the elevational difference between bare earth
points from non-ground points after spatial filtering. Certain height metrics of non￾ground points are indicative of change in surface elevation (e.g., tree height growth
or surface abrasion). In comparison, the intensity of LiDAR returns useful for
grouping the targets into bare earth surface, building footprints, vegetation, roads,
signs, and lamp posts, is not so useful in most quantitative remote sensing
applications.
FIGURE 2.7 Multi-returns of laser scanning from the treed ground cover and its
amplitude. (Gao, 2023.)
ALS is commonly implemented by mounting a scanner aboard a flying platform
such as a helicopter or a light plane that is in constant motion during the scanning
of the area under study (drone-based scanning is possible but drone geometry is
difficult to control, referring to Section 2.1.2 for details). The scanner transmits
dense LiDAR pulses towards the target perpendicular to the aircraft´s motion. The
entire study area is scanned as it flies forward, acquiring dense LiDAR point clouds
quickly and consistently. Airborne nadir scanning can maintain a roughly constant
range between the scanner and the target, resulting in the acquired LiDAR point
clouds having a mostly uniform density and accuracy across the whole area of2.6.1.3
scanning. Airborne LiDAR is suitable for surveying a large ground area quickly. If
the ground area to be covered is excessively extensive, however, it may be
necessary to scan it in multiple strips to cover it fully. These multiple scan strips
must be stitched together using GCPs or tie points as with TLS. ALS potentially
offers a few distinct advantages over slow and expensive ground-based leveling in
estimating surface deformation, especially in remote and inaccessible areas. It is
able to yield a spatial perspective into the vertical movement over an area. ALS is
expensive but flexible. It enables a much larger areas to be scanned much more
quickly at a more homogeneous point density than TLS. At present, LiDAR data￾based quantification is confined mostly to the terrestrial sphere and biosphere,
particularly in quantifying surface motion (e.g., rate of subsidence) and estimating
biophysical parameters of vegetation.
Space-borne LiDAR Data
So far, LiDAR remote sensing has been confined almost exclusively to terrestrial or
airborne with one exception, the Cloud-Aerosol LiDAR and Infrared Pathfinder
Satellite Observation (CALIPSO) mission. It was successfully launched into a sun￾synchronous orbit at an altitude of 685 km on April 28, 2006, with an expected life
expectancy of more than 17 years. CALIPSO maintains an orbital inclination of
98.2°, and a revisit period of 98.3 minutes. Its payload is composed of three
instruments: a Cloud-Aerosol LiDAR with Orthogonal Polarization (CALIOP), an
Imaging Infrared Radiometer (IIR), and a Wide Field Camera (WFC). CALIOP is a
three-channel LiDAR sensor that transmits pulses at a repetition frequency of 20.16
Hz to vertically profile clouds and aerosols. The backscattered radiation from
atmospheric molecules and particulates are detected at two wavelengths of 532 nm
(parallel and perpendicular) and 1064 nm at a horizontal and vertical resolution of
333 m and 30 m, respectively. IIR senses the atmosphere in three TIR regions of 8.7
µm, 10.5 µm, and 12.05 µm at a spatial resolution of 1 km×1 km and a swath width
of 64 km × 64 km. Its main uses are to contextualize nighttime CALIOP
observations and acquire the size of particles within semi-transparent clouds, while
WFC provides context for daytime CALIOP observations at a spatial resolution of
125 m and a swath width of 61 km. CALIPSO data are useful primarily for
estimating atmospheric constituents, including aerosols and cloud top temperature
(eoportal.org). CALIOP LiDAR data can yield high-resolution vertical profiles of
aerosols and clouds.REFERENCES
Chapman JW, DR Thompson, MC Helmlinger, BD Bue, RO Green, ML Eastwood,
S Geier, W Olson-Duvall, and SR Lundeen (2019) Spectral and radiometric
calibration of the next generation airborne visible infrared spectrometer
(AVIRIS-NG). Rem Sens 11: 2129. doi: 10.3390/rs11182129
Cheng KH, SN Chan, and JHW Lee (2020) Remote sensing of coastal algal blooms
using unmanned aerial vehicles (UAVs). Mar Pollut Bull 152: 110889.
Cocks T, R Jenssen, A Stewart, I Wilson, and T Shields (1998) The HyMaptm
airborne hyperspectral sensor: The system, calibration and performance. Paper
Presented at 1st EARSEL Workshop on Imaging Spectroscopy, Zurich, October
1998.
Gao J (2023) Remote Sensing of Natural Hazards. Boca Raton: CRC Press, 437 p.
Geipel J, J Link, and W Claupein (2014) Combined spectral and spatial modeling of
corn yield based on aerial images and crop surface models acquired with an
unmanned aircraft system. Rem Sens 6(11): 10335–10355. doi:
10.3390/rs61110335
Ihab J (2017) Hyperspectral Imaging for Landmine Detection. PhD thesis,
Lebanese University and Politecnico di Torino, 119 p.
Lechner AM, GM Foody, and DS Boyd (2020) Applications in remote sensing to
forest ecology and management. One Earth 2: 405–412. doi:
10.1016/j.oneear.2020.05.001
Lee CM, ML Cable, SJ Hook, RO Green, S. Ustin, DJ Mandl, and EM Middleton
(2015) An introduction to the NASA Hyperspectral InfraRed Imager (HyspIRI)
mission and preparatory activities. Rem Sens Environ 167: 6–19. doi:
10.1016/j.rse.2015.06.012
Lefsky MA, WB Cohen, GG Parker, and DJ Harding (2002) Lidar remote sensing
for ecosystem studies. Biosci 52: 19–30.
Lu H, D Qiao, Y Li, S Wu, and L Deng (2021) Fusion of China ZY-1 02D
hyperspectral data and multispectral data: Which methods should be used? Rem
Sens 13(12): 2354. doi: 0.3390/rs13122354
Luetzenburg G, A Kroon, and AA Bjørk (2021). Evaluation of the Apple iPhone 12
Pro LiDAR for an application in geosciences. Sci Rep 11: 22221. doi:
10.1038/s41598-021-01763-93 Radiometric Correction
DOI: 10.1201/9781003517504-4
The quantitative retrieval of the value or quality of the target parameter on the
Earth’s surface or in the atmosphere is founded on the interactions of the solar
energy with the target. These interactions fall into three broad types of absorption,
reflection, and transmission. As the electromagnetic energy from the sun
propagates through the atmosphere to reach the target, it is either absorbed,
scattered, or transmitted in the atmosphere. Similar interactions also take place
when the transmitted energy eventually reaches the Earth’s surface and is bounced
back towards the sensor. Of the three interactions, the reflected energy from the
target is widely exploited in both passive and active quantitative remote sensing.
Absorption takes place both on the Earth’s surface and in the atmosphere. It is
atmospheric absorption that is heavily relied on in the quantitative sensing of the
atmosphere. Although the absorbed radiation by the target cannot reach the sensor,
the magnitude of absorption and its spectral variation with the wavelength of the
solar energy supply the vital clues for the quantitative retrieval of trace gases in the
atmosphere, and are instrumental to the success of quantitative remote sensing of
the atmosphere. Compared to absorption, transmission is crucial to the success of
quantitative remote sensing in the hydrosphere where absorption is much less
important. Dependent upon the target of sensing, reflection is considered to
introduce noises to the quantification as it takes place at the water surface. The
reflected radiation bears no information on in-water constituents.
In qualitative remote sensing all the energy received at the sensor is converted to
a few discrete categories of pixel values representing different land covers via
image classification in the multispectral domain. The classified outcome is scarcely
affected by the atmospheric effects on the image, and they can be safely ignored inmost cases. However, this is no longer the case in quantitative remote sensing in
which the pixel value or the transformation of pixel values in multiple bands is
linked directly to the value of the target parameter to be quantified via Eq. 1.1.
Therefore, it is imperative to carry out atmospheric calibration that dictates the
accuracy and even success/failure of quantification. Without this calibration, it is
unimaginable that any reasonable quantification outcome can be derived from raw
remote sensing data as the atmosphere inevitably introduces an additional radiation
term to spectral signal of the target. This influence means either an artificial
exaggeration of the signal of the target (e.g., via aerosol scattering) or an extinct via
absorption. In either case, the total at-sensor radiance depends not only on the
atmosphere thickness and target but also on the reflectance of the stream bed, the
optical properties of the water column, and any light reflected from the water
surface in the hydrological domain. If not corrected, atmospheric effects can further
contaminate the radiance signal and may thwart quantification altogether.
Atmospheric correction is the process of estimating surface reflectance from the
satellite-captured top-of-atmosphere (TOA) reflectance with their associated
uncertainties by removing the atmospheric contribution from the remote sensing
images. Another objective of atmospheric correction is to account for the
absorption effects of water vapor and aerosols whose spatial and temporal
variations are the main causes of the inaccuracy of atmospheric correction. During
atmospheric correction the TOA radiance captured by the space-borne sensor is
decomposed into the signal from the atmosphere and the signal from the surface so
as to retrieve surface reflectance (ρs). Atmospheric correction is indispensable to
the accurate retrieval of environmental parameters and the generation of high￾quality remote sensing data (product) for quantitative applications. In order to
fulfill this objective, the satellite image data expressed as digital numbers (DN) are
first transformed to TOA radiance (Lt) using the calibration coefficients provided in
the image metadata. It is different from image calibration that is applied to all
images internally to standardize the response of all sensors, relative to the
benchmark.
This chapter first elucidates how the solar radiation interacts with the target of
sensing both in the atmosphere and on the Earth’s surface, including scattering,
absorption, and transmission in the atmosphere and reflection on the surface of
water. Then this chapter compares and contrasts different computing systems for
implementing the corrections. Finally, the performance of major correction3.1
3.1.1
algorithms is evaluated for their effectiveness and accuracy in atmospheric
correction.
RADIATION INTERACTIONS
The incoming solar energy is partitioned into three parts, transmitted, absorbed, and
reflected, when reaching the target of sensing. The proportion and relativity of each
partitioned component varies with the media of propagation. They are commonly
divided into three types: air, terrestrial surface, and water. This section first
discusses how the solar radiation interacts with particulars suspended in the
atmosphere, and how the energy interacts with terrestrial surface, and finally how it
interacts with water that is further divided into deep open water and shallow inland
water.
With Atmosphere
The atmosphere comprises predominantly gaseous substances, such as O2, N2,
CO2, helium, argon, CO, water droplets, dusts, pollen, and small amounts of other
trace gases. Their mass totals approximately 5.15×1015 tons, three quarters of
which lie within ⁓11 km of the Earth’s surface, with a higher concentration closer
to it. At the sea level, the density of air averages about 1.2 kg×m-3. In terms of
proportion, the predominant constituents are N2 (78%) and O2 (21%). Although the
remaining substances (e.g., water vapor, CO2 (0.04%), dust, and argon) make up
less than 1% of the atmosphere, they are the most critical to quantitative remote
sensing because of their relatively large diameter. Originating from the Earth’s
surface, they mainly scatter the incoming radiation while water vapor and droplets
selectively absorb it. If the scattering is non-selective, the amount of scattering
bears no relationship with radiation wavelength. All scattered energy by aerosols is
considered noise in atmospheric correction.
During the propagation of the solar radiation through the atmosphere to reach
the space-borne sensor, the photons inevitably interact with the atmospheric
constituents. Most visible lights have a short wavelength that can pass the
atmosphere with little hindrance. Nevertheless, IR radiation with a longer
wavelength does not pass the atmosphere unhindered even in cloud-free conditions.
For instance, the photons can be absorbed or scattered by molecules and aerosols,
and the atmospheric constituents emit photons into the beam reaching the satellite3.1.1.1
radiometer. The transmissivity of the clear-sky atmosphere varies noticeably with
IR wavelengths and the contents of atmospheric gases. The term “atmospheric
window” is used to describe wavelength ranges over which the incoming solar
radiation can penetrate the atmosphere to reach the target of sensing and ultimately
the sensor with little attenuation. It is the TOP radiance captured by satellite
radiometers in these intervals that permit the quantification of parameter values on
the ground or in the air.
Scattering
The scattering of the incoming solar radiation by the atmospheric molecules and
aerosols does not attenuate its intensity. Instead, it redirects its propagation (Figure
3.1). Depending on the relativity between particle size and the wavelength of solar
radiation, scattering falls into three types of Rayleigh, Mie, and non-selective.
Rayleigh scattering takes place if the diameter of particles (d) such as small specks
of dust or nitrogen and oxygen molecules is much smaller than the wavelength (λ)
of the radiation (e.g., d<0.1λ). In this case, the magnitude of scattering is inversely
related to the 4th power of λ (Eq. 3.1) and the 6th power of particle size. Thus,
shorter-wavelength radiation is scattered more than its longer wavelength
counterpart.FIGURE 3.1 Interactions of the solar radiation with the atmosphere and the Earth’s
surface.
(3.1)
where λ = wavelength of solar radiation; D = distance between the particle and the
sensor; θ = scattering angle. I0 = intensity of the incident radiation. refractive index
in terms of the molecular polarizability; α = proportional to the dipole moment
induced by the electric field of light.
At a lower elevation where the particle diameter becomes so large as to be
comparable to λ, such as dust, pollen, smoke, and water vapor droplets, the
scattering becomes Mie type, in which the magnitude of scattering is inversely
related to the diameter and its square. The intensity of Mie scattering is not fixed,
but varies between λ–2 and λ–4. Mie scattering affects longer wavelengths more
than Rayleigh scattering. It occurs mostly in the lower atmosphere where larger
particles have a higher concentration, and are dominant when the sky is overcast.
Non-selective scattering takes place when λ of the incoming radiation
approximately equals d. In this case (e.g., thick cloud coverage), the degree of
scattering remains constant irrespective of wavelength. Apart from magnitude,3.1.1.2
scattering also changes the direction of radiation propagation with the strongest
scattering predominating the direction of propagation (either 180° or 0°) while
scattering is much more subdued in the perpendicular direction (Figure 3.1). In
other words, the intensity of scattering is stronger in the 0° and 180°, but weaker in
the 90° and 270° directions. At a higher altitude in the troposphere, the suspended
particles have a much smaller diameter. Tiny aerosols suspend in the air for a long
time scatter the incoming radiation.
Absorption
The atmospheric gaseous absorption is caused principally by O2, O3, H2O (water
vapor), carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O). O2
absorbs radiation with a wavelength shorter than 0.2 μm in a narrow band centered
around 0.6 and 0.76 μm. O3 absorbs most of the ultraviolet radiation harmful to
most living organisms at a wavelength around 0.2–0.32 μm, plus 0.6 and 9.6 μm
from the sun. Water vapor in the atmosphere absorbs most of the longer wavelength
IR and microwave radiation between 22 µm and 1 m. The main absorption zones
peak at central wavelengths around 0.94, 1.13, 1.86, 2.5–3.0, 3.24, 5.0–7.0, and
>24 μm. CO2 is a much less effective absorber, having a peak absorptance around
2.8 and 4.3 μm (Figure 3.2). Water absorbs mostly infrared radiation while CO and
ozone selectively absorb radiations of different wavelengths. It is such absorption
behavior of these atmospheric constituents that is exploited to radiometrically
correct remote sensing images for the accurate quantification of environmental
parameters from them. Compared to scattering, the absorbed energy is much more
important to quantify some atmospheric trace gases, and widely taken advantage
of.FIGURE 3.2 Absorption spectra over the wavelengths of 0.2–70 μm by the main
absorbers in the atmosphere. (Created by Robert A. Rohde, open access.)
In spite of scattering and absorption, the atmosphere is still transmissive to certain
radiations. Transmittance is a measure of radiation’s ability to pass the atmosphere
to reach the sensor without being attenuated completely. The more transmissive a
radiation is, the more of it is captured by remotely sensed bands. The wavelengths
at which the solar radiation is rather transmissive (e.g., not absorbed or is
attenuated at a low level) are termed the “atmospheric windows.” Space-borne
quantification of surface features based on their reflected energy is best3.1.2
3.1.3
accomplished in roughly six atmospheric windows of the solar spectrum: 0.40–0.75
μm in the visible spectral range, and at around 0.85, 1.06, 1.22, 1.60, 2.20 μm in
the near and middle infrared regions.
Spectral Behavior of Typical Components
The quantification of atmospheric components relies either on backscattered
radiation by the particles themselves or via their absorption characteristics. The
former is used to quantify solid particles while the latter is heavily used to quantify
gaseous components. Since each gas component has its own unique absorption
spectra pattern, it can be isolated from other gases even if they co-exist in the same
atmosphere or troposphere. Shown in Figure 3.2 is the typical spectral absorptance
of water vapor, O2, and CO2. While the absorption regions in the figure can
indicate the general wavelengths of the atmospheric windows, they are unable to
indicate the exact behavior of trace gases such as CO and SO2.
With Terrestrial Surface
After the solar radiation has passed through the atmosphere, it finally reaches the
Earth’s surface, with which it further interacts (Figure 3.1). The exact manner of
interaction varies with the nature of the surface. It can be broadly categorized as
terrestrial and aquatic. This section discusses the former while the latter will be
covered in the next section. At the terrestrial surface, most of the incident energy is
either absorbed or reflected back to the atmosphere, with minimal transmission.
The degree of transmission is related to the surface feature, such as porosity of bare
ground. How the incident energy is reflected depends primarily on its surface
smoothness and secondarily on its color. There are three types of reflection:
specular reflection, diffuse reflection, and real surface reflection (Figure 3.3).
Specular reflection is similar to mirror reflection in that the angle of incidence
equals the angle of reflection (Figure 3.3a and b). Thus, all the incident energy is
reflected back in one direction or confined within a narrow cone. It is commonly
associated with flat surfaces, such as a calm lake surface. With land surface, the
reflection is more likely to be diffuse in which the incident energy is reflected in
multiple directions up to 180° caused by the micro-relief of the surface (Figure 3.3c
and d). Thus, the intensity in the main direction of reflection is considerably
reduced.FIGURE 3.3 Four types of reflection of the solar radiation at the terrestrial surface
related to surface smoothness or roughness.
There are two special cases of reflection that merit an in-depth discussion in image
radiometric correction. The first is Lambertian reflectance, an idealized surface that
reflects the incident energy equally in all directions (Figure 3.3d). The apparent
brightness of a Lambertian surface remains unchanged irrespective of the viewing
position and angle because of the isotropic luminance of the surface. The
luminance intensity can be depicted by Lambert’s cosine law. The second is the
bidirectional reflection distribution function (BRDF), a kind of reflection whose
intensity depends on the illumination and viewing geometry. It accounts for
directional effects of reflection on the surface of the target, or , which is defined as
the ratio of the reflected spectral radiance L from the surface in the viewing
direction to the directional spectral irradiance E on the surface in the incidence
direction , or
(3.2)
It has the unit of reciprocal steradians, but may or may not be considered in
atmospheric correction.
The ability of a surface to reflect the incident energy is termed reflectance,
defined as the ratio of the reflected energy to the incident energy at the same
wavelength (λ), or
(3.3)
The reflectance of vegetation with wavelength may be depicted graphically (Figure
3.4). The reflectance is generally low over the VIS portion of the spectrum due to
chlorophyll absorption, but there is a minor peak reflectance at the green3.1.4
wavelength. In contrast, the reflectance is much higher at the infrared wavelengths.
It must be acknowledged that the amount of reflected energy is inversely related to
the absorbed energy, as their sum is a constant. However, the amount of reflected
energy is reduced if the vegetation is rather healthy. Moist vegetation reflects less
energy than mature crops. Besides, the amount of chlorophyll in the leaves, as well
as the leaf area and shape also affect reflection. Healthy vegetation with plenty of
chlorophyll reflects more energy than diseased crops or plants of little biomass,
such as shrubs and grasses. This topic is so complex that it will be revisited in
Section 6.1.2.
FIGURE 3.4 Spectral reflectance curves of three typical types of terrestrial surface
covers.
With bare soil, the reflectance is subject to minerals in the soil. For instance, loam
soil with a high ferrous content appears as red that reflects more red radiation. The
presence of salt and trace minerals in the soil also alters its spectral behavior. It is
such changes that supply the vital clue for the successful quantification of soil salt
(and salinity) levels.
With WaterAs far as quantitative remote sensing is concerned, waters can be categorized into
two groups, open waters and inland waters. Open ocean waters tend to be deep,
transparent, clean with few impurities, and typically have low to moderate Chl
concentrations. They are known as Case I waters whose IOPs are dominated by
phytoplankton with little contribution from minerals and dissolved organic matter.
In contrast, nearshore and inland waters such as lakes, reservoirs, and rivers are
known as Case II waters. They are shallower, smaller, and spectrally more complex
and variable caused by algae, CDOM, detritus, inorganic mineral particles, and
even seaweeds, in addition to phytoplankton.
Of the three major types of surface features shown in Figure 3.4, water is the
most transparent. As such, it interacts with the incident energy quite differently
from that of land and vegetation. In brief, the incident radiation is partitioned into
five parts: surface reflection, transmission, absorption, scattering, and bottom
reflection (Figure 3.5). Once striking the water surface, the incident radiation is
reflected by it, forming the sun glint. The reflectance of water surface is related to
the viewing angle and the angle of the incident radiation. When the water surface is
calm, it behaves like a mirror, and the incident energy is reflected back to the
atmosphere at an angle equal to the incident energy (Figure 3.3a). If the weather is
rather rough, the white cap of open sea surface diffuses the incident energy in all
directions, like a Lambertian surface (Figure 3.3d). In either case, the reflection is
affected by water surface roughness.FIGURE 3.5 Interactions of incident solar radiation with a shallow water body (note:
surface scattering is relevant only when the surface is chopping under stormy
weather conditions). It produces glint on remote sensing imagery.
Of the three types of interactions, the most predominant is transmission ET,
followed by absorption EA while reflection ER is the most subordinate, usually
mounting to <5%. Water transmissivity is related to its clarity, and dictates the
depth at which the incident energy is able to penetrate the water column. It is a
function of the incidence angle, the wavelength of the radiation, and water
transparency. The strongest transmission takes place at a wavelength in the
proximity of 0.5–0.6 μm while most NIR radiation is absorbed by water. Naturally,
the incident energy is able to transmit deeper in Case I oceanic waters than in Case
II inland waters. Just as with the atmosphere, a large portion of the incident energy
is scattered (ES) and absorbed (EA) by the impurities suspended in turbid waters,
resulting in less penetration. Since the total incident energy EI remains unchanged
(Eq. 3.5), there is an inverse relationship between the two.
(3.5)3.2
In inland waters, if the water body is sufficiently transparent and shallow, then it is
possible for the solar radiation to penetrate the entire water column to reach the
floor, causing further bottom reflection (Figure 3.5).
Although the reflected energy (ER) accounts for only a small portion of the
redistributed energy, its impact on quantitative remote sensing cannot be ignored.
As a matter of fact, it must be differentiated from scattered energy (ES) in
quantitative remote sensing as surface reflection is mostly noise (unless the target
of study is surface roughness such as wave height). Scattering can be caused by a
choppy surface and in-water constituents and the quantification of their content
relies on the scattered radiation (Es), during which the reflected radiation must be
eliminated from consideration. In most cases, surface reflection and scattering
introduce noises to the captured energy unless the target of quantification is the
wave parameters in open water. The only exception is retrieval of wave height
when it serves as the signal.
The same reflected radiation can be regarded as either noise or useful
information, depending on the target of quantification. It is noise if the target of
quantification is the concentration of in-water constituents as it interferes with the
signal from in-water constituents and even the sea floor. As such, it must be
addressed in accurate quantification of in-water constituents and bathymetry.
Differently interacted energy is used to fulfill different quantification purposes. For
instance, in most hydrographic quantification it has to be eliminated. If the target of
quantification is bathymetry, then it is the only clue for retrieving wave height and
wind speed, though. The transmitted energy is useful for bathymetry retrieval as it
is increasingly attenuated by water with deeper downward penetration. The depth
to which it can transmit is inversely related to the amount of absorption that can be
used to quantify the impurities of water, such as algae and chlorophyll while
physical sediments scatter the radiation. The scattered energy is crucial for
retrieving the content of suspended impurities and solid in-water substances, such
as sediments and algae in a water body.
PHYSICAL MODELS OF CORRECTION
Radiometric correction methods fall into three categories of physical models, semi￾physical models, and image-based. As the name implies, image-based methods
correct image radiometry based on the information obtained from the image itself
without relying on external auxiliary data. These methods are generic and3.2.1
applicable to all sorts of satellite data. Common generic correction methods include
apparent reflectance and dark object subtraction (DOS). In contrast, physical
methods are grounded on radiative transfer models (RTMs), such as the Moderate￾resolution Atmospheric Transmission (MODTRAN) radiative transfer model.
Physical models simulate atmospheric conditions and correct image radiometry
degraded by atmospheric constituents from several ancillary parameters related to
the surface and the illumination conditions at the time of sensing. Physical models
contain numerous parameters, so they are complex and may be image-specific,
geared more specifically towards a particular type of data or for a particular type of
target. Several physical models have been developed for this purpose. They differ
from each other in how they deal with atmospheric absorption and simplify the
interactions of atmospheric constituents with the incoming solar radiation, and
whether they can correct surface reflection, atmospheric absorption, and
topographic effects. So they are much more complex than image-based methods.
The dominant ones are 6S and Fast Line-of-sight Atmospheric Analysis of Spectral
Hypercubes (FLAASH). So far many correction models/methods have been
developed for correcting different types of images. The generic ones will be
introduced and compared first, followed by niche correction algorithms in this
chapter.
6S (Vector)
The Second Simulation of a Satellite Signal in the Solar Spectrum (6S) is well￾established computer codes that have been rigorously tested, meticulously
documented, and extensively used for removing atmospheric contributions from
the observed at-sensor radiance. It can accurately estimate surface reflectance by
simulating the radiative transfer of solar electromagnetic radiation through the
atmosphere in the 400–2500 nm spectral range. The scattering and absorption of
the solar radiation by atmospheric constituents are resolved via constructing
comprehensive look-up tables (LUTs) with the assistance of RTMs. 6S addresses
aerosol and molecular scattering differently using separate atmospheric models for
molecules and aerosols. Molecular models sum to 13, of which seven are code￾embedded, and the remainders are user-defined. Aerosol models total 10, six of
which are embedded in the codes, and the remaining four are defined by the user
based on components and in situ observations. The user has the option of treating
the ground surface as either homogeneous or heterogeneous with/withoutdirectional effect (10 BRDF + 1 user-defined). Key parameters considered in these
models include surface air pressure, ozone concentration, and column water.
Compared with its predecessor, the improved 6S vector (6SV1) simulates the
surface reflection of solar radiation by coupling the atmosphere and surface as one
system under diverse atmospheric, spectral, and geometrical conditions. The vector
radiative transfer modeling of the coupled system is highly accurate (better than
1%) with the BRDF-atmosphere coupling correction algorithm (Collection 5).
Based on successive orders of scattering (SOS) approximations, this vector version
accounts for the polarization of radiation in the atmosphere by calculating the Q
and U components of the Stokes vector (Vermote et al., 2006). Owing to the
approximations and the computed vertical distribution of the reflected radiation in a
homogeneous atmosphere for a discrete number of atmospheric layers using the
SOS algorithm, Rayleigh and aerosol scattering effects are estimated much more
accurately than in the previous version while the step size (resolution) used for
spectral integration has been reduced to 2.5 nm.
The improved version enjoys several strengths by accommodating multiple
scattering angles, atmospheric layers, and viewing angles. It has provisions for
user-defined aerosol profiles, and the use of multiple types of polarization
(perpendicular, parallel, and elliptical) of radiation in the atmosphere. The
simulation of surface reflectance also considers target elevation in a realistic
molecular, aerosol, or mixed atmosphere, for both Lambertian and anisotropic
surfaces. For a Lambertian surface, the surface-atmosphere system is decoupled in
order to determine gaseous absorption by differentiating the at-sensor radiance into
intrinsic atmospheric signal, total transmittance, and spherical albedo.
Conceptually, 6SV relates the TOA reflectance captured by the sensor (ρTOA) to the
reflectance (ρs) of a Lambertian surface as:
(3.5)
where wavelength (λ) has been omitted from all the brackets and ; ρatm =
atmospheric intrinsic path reflectance; Ts = downward atmospheric transmittance
of the sun-surface path; Tv = upward atmospheric transmittance of the surface￾sensor path; Satm = spherical albedo of the atmosphere from the ground to count
multiple reflections between the surface and atmosphere; θs and θv = solar and
sensor zenith angle, respectively; φ = relative azimuth angle; Uwv and UO3, =
integrated water vapor and ozone content, respectively; τa, ωa, PA = aerosol opticaldepth, aerosol single scatter albedo, and aerosol phase function, respectively; and
Twv, TO3, TOG = gaseous transmission by water vapor, ozone, and other gases,
respectively. The exact contributors to TOA reflectance from all the sources are
illustrated in Figure 3.6.
FIGURE 3.6 The various processes contributing to the total TOA radiance from an
open sea. D = dust, pollen or aerosol molecules; N2 = nitrogen or any other
atmospheric gas molecules; red lines = glint terms.
The atmospheric intrinsic reflectance is approximated as
(3.6)
where ρR = atmospheric reflectance contributed by Rayleigh scattering; and ρA+R =
combined atmospheric reflectance caused by Rayleigh and aerosol scattering.The inversion of key atmospheric parameters (aerosol, water vapor) using 6SV
is via comprehensive LUTs established via RT simulations. The whole procedure of
correction follows the steps of AOT determination → 6SV simulation → LUT
construction → correction implementation (Vermote et al., 1997). This procedure is
complex and lengthy as numerous parameters must be specified or properly
configured, including vertical profiles of air pressure, water vapor concentration,
air temperature, ozone concentration, topography (e.g., DEM), aerosol optical
depth (AOD), and aerosol model, as well as the solar and sensor viewing geometry
to simulate atmospheric conditions and construct the LUT (Table 3.1). Some of
them are obtained from the metadata of the imagery (e.g. sensor type, sensor
altitude, and spectral band) and knowledge of ground objects/image coverage (e.g.
target altitude, atmospheric profile and ground reflectance type: homogeneous or
non-uniform surface). The accuracy of atmospheric correction has been reported as
the “standard accuracy” conditions which provide the user with a relative average
accuracy of approximately 0.4–0.6% (Vermote et al., 2006). The attributed
accuracy of within 1% is nevertheless subject to inherited errors from the ancillary
input parameters obtained from diverse sources. The 6SV algorithm has a limited
capacity to handle spherical atmosphere. Consequently, it cannot be used for limb
observations. In addition, it does not function in the presence of strong absorption
bands if absorption and scattering effects are decoupled.
Table 3.1
Typical values of exemplary 6SV atmospheric correction input parameters
Parameter Type Parameter Name Parameter Value
Geometrical
conditions
Date of data
acquisition 2021/07/27
Solar zenithal angle (°) 62.1
Solar azimuthal angle
(°) 31.3
Sensor zenithal angle
(°) 26.1
Sensor azimuthal angle
(°) 42.8
Atmospheric
Model
Atmospheric profile Mid-latitude winter
Aerosol model Maritime model3.2.2
Parameter Type Parameter Name Parameter Value
Atmospheric
conditions (visibility) 24 km
Target/sensor
altitude
Target altitude 0.2 km
Sensor altitude 780 km
Spectral
conditions Spectra range 400 – 435 nm
Ground
reflectance
Type
Homogeneous surface and no
directional effects
Surface reflectance
Mean spectral value of green
vegetation
Signal Correction model With BRDF (Lambertian)
SREM
The Simplified and Robust Surface Reflectance Estimation Method (SREM) is
essentially a simplified version of 6SV RTM by inverting surface reflectance for
VIS-SWIR bands. It is able to perform much simpler correction than other image￾based and physical methods because of the omission of ancillary parameters related
to AOD, aerosol type, water vapor, and ozone, so there is no need to consider
aerosol particles and atmospheric gases and pre-established LUT (Bilal et al.,
2019). Terms related to aerosol particle information such as τa, ωa, PA (i.e., ρA = 0)
and other atmospheric gas parameters such as Uwv, UO3, and TOG (i.e., TOG, TO3,
and Twv = 1) all disappear from Eq 3.6 that can be rewritten as
(3.7)
From which ρs is approximated as
(3.8)
after all the symbols inside the bracket are omitted for clarity and brevity. Of
particular notice is the difference between SREM-corrected ρs and the Rayleigh￾corrected that is calculated by subtracting Rayleigh reflectance (Eq 3.10) from the
TOA reflectance (Eq. 3.9).(3.9)
where LTOA = TOA radiance captured by the sensor. It can be derived from the
calibrated pixel DNs by applying the rescaling factors in the metadata; D = distance
between the Earth and the Sun in the astronomical unit; ESUN = mean solar
exoatmospheric spectral irradiance obtained from the Thuillier solar spectrum via
the sensor-specific relative spectral response functions (Bilal et al., 2019).
(3.10)
where A and B = coefficients accounting for the molecular asymmetry, with a value
of 0.9587256 and 1-A, respectively; PR = Rayleigh phase function of the scattering
angle () (Eq. 3.11); m = air mass calculated from cosine of solar angle (μs) and
cosine of sensor zenith angle (μv); τr = Rayleigh optical depth that is a function of
wavelength λ (Eq. 3.12). They are calculated as:
(3.11)
(3.12)
(3.13)
In Eq. 3.7, the atmospheric backscattering ratio and total atmospheric transmission,
without integrating aerosol information, are calculated as:
(3.14)
(3.15)
(3.16)
Illustrated in Figure 3.7 is the stepwise procedure of implementing this method in
detail. The SREM-produced correction results are similar to those achieved using
other well-known state-of-the-art methods. Tested using Landsat data, these
correction results are positively correlated with field-measured reflectance for each
spectral band significantly and strongly with a small mean bias error (MBE) and
root mean squared deviation (RMSD). SREM also maintains a consistent
performance in correcting Sentinel-2A (r = 0.994, MBE = -0.009, RMSD=0.014)3.2.3
and MODIS (r = 0.925, MBE = 0.007, RMSD = 0.014) images, demonstrating its
applicability to diverse multispectral EO images (Bilal et al., 2019).
FIGURE 3.7 Systematic methodology of the Simplified and Robust Surface
Reflectance Estimation model (SREM). (Bilal et al., 2019, open access.)
FLAASH-C/DHL
The FLAASH algorithm retrieves spatially aggregated ρs of the target using the
following standard at-sensor radiance equation after the omission of wavelength
dependence of relevant symbols for notational clarity and brevity:
(3.17)
where La = backscattered radiance of the atmosphere, and a and b = coefficients
dependent solely on atmospheric conditions and observation geometry. The term in
Eq. 3.17 corresponds to the at-sensor radiance diffused by the surface, known as
the “adjacency effect” (Perkins et al., 2012). The values of a, b, S, and La are
determined from MODTRAN (refer to Section 3.7.1 for more details) simulations
of total and ground-contributed spectral radiances at multiple ρs values (e.g., 0, 0.5,
and 1).Column water vapor and aerosol content are the two variable atmospheric
qualities that most profoundly affect VNIR-SWIR radiances. As illustrated in
Figure 3.4, strong water absorption occurs at λ around 0.82, 0.94, 1.13, and 1.4 μm.
Their contribution to the observed radiance is detected via iterations over a range of
column water vapor contents using MODTRAN. During the retrieval, the RT
equation is resolved to derive aerosol reflectance over a series of experimental
visibility values that are evenly spaced in optical depth. Scene visibility can be
retrieved from an image if it contains appropriate dark pixels. They are selected
based on a reflectance upper limit or cutoff imposed on the denominator bandpass
(default values are set at 0.08 and 0.03 for the land and water methods,
respectively), but FLAASH permits the use of values different from these default
bandpass selections, ratios, and cutoff. Different band passes and ratios are defined
for dark pixels of land and water. Pixels satisfying the cutoff criterion are selected,
and water and shadow pixels are eliminated using a radiance ratio test based on the
assumption that dark terrain has a characteristic, known reflectance ratio in a pair
of selective band passes. The scene visibility is calculated by interpolating the
experimental values to yield a difference of zero. Then the average difference
between the calculated and guessed reflectance in the numerator bandpass is
computed for the selected pixels. Finally, the visibility related to horizontal optical
depth at 550 nm is calculated as
(3.18)
Reciprocal of the visibility can be derived at an accuracy of approximately 0.01
km−1 using appropriate dark pixels and aerosol models.
The water vapor content is retrieved using the mean radiance of two channels:
an “absorption” channel centered at 1.13 μm (typical water band) and a “reference”
channel taken from the edges of the band. The retrieval can be accomplished
quickly if facilitated by a 2D LUT constructed from the MODTRAN outputs.
Water vapor is retrieved using the updated visibility to establish the final
atmospheric description at 1.9 μm. The averaging implied in ρs is a convolution
with a spatial point spread function (PSF). In a strict sense, different PSFs should
be used when ρs appears in the numerator and denominator of Eq. 3.17. However,
since ρsSatm is generally very small, the denominator PSF can be approximated as
the numerator PSF, which describes the upward diffuse transmittance. ρs is
estimated from an approximate form of Eq. 3.17 as:3.3
3.3.1
(3.19)
where Le = radiance image convolved with the PSF. Eq. 3.17 is then solved for ρs.
SEMI-ANALYTICAL METHODS
Simplified Method for Atmospheric Corrections
The Simplified Method for Atmospheric Corrections (SMAC) is a semi-empirical
method as the radiative transfer is approximated with some empirical models to
expedite RTM inversion. It is based on the 6S radiative transfer codes of the at￾sensor radiance. All the pertinent radiative quantities are parameterized as a
function of the input auxiliary data (Rahman and Dedieu, 1994). This correction
takes into account atmospheric gas content (mainly ozone and water vapor),
aerosol content and type, and molecular scattering mainly driven by the sea-level
surface pressure and surface elevation. The at-sensor radiance is considered to stem
from four components that are derived analytically:
i. Two-way gaseous transmission. The double path gaseous transmittance tgi for
a given gas is a function of absorption and air mass (m), or:
(3.20)
where a and n = constants for a given band but variable with spectral bands
and different gases, so need to be adjusted separately using the 6S output. In
case of multiple gases, tgi is equal to the product of all the individual
transmittance.
ii. Atmospheric spherical albedo Satm is expressed empirically as a function of
AOD at 550nm (), or:
(3.21)
where a0 and a1 = parameters to be determined for a given spectral band and
aerosol model. Their values are retrieved via a best-fit against a full RTM
using the 6S codes.
iii. Total (diffuse and direct) atmospheric transmission is empirically calculated
from and solar (or viewing) angle (θ) as:(3.22)
where b0 = constant; b1 and b2 = coefficients to be adjusted for a given spectral
band and a given aerosol type. This transmission takes into account both Rayleigh
(e.g., when τ550 = 0) and aerosol scattering, but the accuracy depends on the range
of AODs used to fit (9) coefficients. Aerosol absorption, if existent (single
scattering albedo, ωo ≠ I, ωo = single scattering albedo of the atmosphere), is
implicitly taken into account by b1, depending on the aerosol model. Diffuse
transmittance is computed by:
(3.23)
where τ = band-averaged optical depth of molecular scattering and aerosol
scattering and absorption.
Atmospheric reflectance is differentiated into Rayleigh and aerosol scattering.
For a molecular atmosphere, the Rayleigh reflectance is calculated from the first￾order scattering using 6S as:
(3.24)
where ρr(ξ) = molecular scattering phase function; τr = molecular optical depth. It
is a constant for a given spectral band and directly available from the 6S outputs,
but needs to be corrected for pressure variation as:
(3.25)
where τr(P0) = reference-level molecular optical depth, Po = 1013.25 hpa under
standard conditions, and P = observed air pressure.
The molecular scattering phase function in Eq. (3.24) is approximated as:
(3.26)
where σ = molecular depolarization factor (σ = 0·0139), and ξ = scattering angle
calculated as:
(3.27)3.3.2
The aerosol reflectance over a spectral band is calculated from the following
equation:
(3.28)
(3.29)
where g = the asymmetry factor. Both g and ωo are constant for a given aerosol
type and in a given spectral band; X, Y, Z = complex functions of ωo, β1, μs and μv.
The average AOD for a given spectral band () is calculated from as:
(3.30)
where c0 and c1 = coefficients whose value changes with the spectral band.
The aerosol phase function is obtained from a 2nd-order polynomial equation of
phase angle ξ (in degrees) as:
(3.31)
where d0, d1, d2 = coefficients varying with the spectral band used. The total
atmospheric reflectance is the sum of ρar and ρap.
Owing to these simplifications, the radiative transfer in the atmosphere is
computed at a significantly accelerated pace over the full model. However,
numerous coefficients have to be determined in advance from best-fits with a full
numerical model. The required inputs are diverse, such as the surface pressure, the
ozone content and the water vapor content, and most importantly, the aerosols of
the atmosphere, in addition to the measured TOA radiance. Also, empirical
coefficients have to be determined with the use of new spectral bands or spectral
bands from another sensor. If implemented in the Sentinel Application Platform
(SNAP) platform, the user has the option to select the MERIS meteorological data
for pressure, ozone and humidity. SMAC-retrieved reflectance generally falls
within 3% of the TOA reflectance derived from a 5S model.
VNIR Method
The aforementioned atmospheric correction models require highly accurate surface
reflectance in VIS and SWIR bands (400–2500 nm). The spectral correlations of
dark targets in the SWIR (around 2.2 µm), blue (480 nm) and red (660 nm)wavebands are used to calculate AOD. SWIR bands at 1.6 and 2.2 μm are essential
to the correction because water-leaving radiance ρs can be assumed to be 0 in these
bands (the dark water assumption). However, SWIR bands are not routinely
available with EO sensors. Some of them capture only the VNIR radiation over the
spectral range of 400–1000 nm in four spectral bands, typically blue, green, red,
and NIR. The absence of the SWIR bands means that they cannot be corrected
using the aforementioned methods. This prompts the use of broad multispectral
VNIR bands.
VNIR-based correction assumes that the total absorption is caused by pure
seawater and that a linear relationship exists between particulate backscattering in
red and NIR bands (Richter et al., 2006). Thus, the aerosol type from the nearest
non-turbid water can be transferred to turbid water pixels. The contribution of
atmospheric radiance to the observed radiance recorded in a band is determined
first. It works for average clear atmospheric conditions and involves three
approximations:
i. Column water vapor: If not available from existing products, then it is served
by either a seasonal climatological value (summer, winter, location￾dependent) or the measurement from meteorological stations in the area of
study.
ii. Ozone absorption: It is based on ozone data products. In their absence, a
seasonal geography-dependent value is used. The typical elevation￾dependence of ozone concentration as implemented in the MODTRAN code
(Berk et al., 2003) is taken into account.
iii. Aerosol type (single scattering albedo, phase function): Aerosol type is
estimated from blue and red bands (Kaufman et al., 1997). If these bands are
not available, it is selected from the climatology of the area under study using
built-in MODTRAN aerosols (e.g. rural, urban, and maritime).
This method of correction takes into account the absorption by CO2 and O2 in
addition to water vapor and ozone, even though they have a small global variation
and play a minor role in broad VNIR bands. The contribution of molecular
scattering depends only on atmospheric pressure that is related to elevation. The
remaining most important atmospheric parameter that varies spatiotemporally is
AOD or visibility. Visibility estimation forms the core of the VNIR algorithm and
is achieved via two iteration loops (visibility and ρred). Iterations start with three3.3.3
visibilities of 10, 23, 60 km that encompass a wide range of aerosol loadings. The
initial atmospheric conditions are assumed to be clear with an average AOD of 0.27
at 550 nm that corresponds to a visibility of 23 km. The surface reflectance in the
red and NIR bands is calculated using LUTs based on the MODTRAN radiative
transfer codes.
The next step is to identify dark dense vegetation (DDV) pixels as references
via multiple thresholds of ratio vegetation index (RVI) and NDVI in combination
with reflectance in red and NIR bands, such as RVI ≥ 3, 0.10 ≤ ρnir ≤ 0.25, and ρred
≤ 0.04. The condition ρnir ≥ 0.10 removes clear and turbid water and mixed
coniferous-soil pixels with a large percentage of dark soil or shadow, ρnir ≤ 0.25
excludes bright vegetation (e.g. meadows, deciduous forests), and the last criterion
is the initial threshold of the red band for iteration. The output of this mask
criterion is a percentage (p) of reference pixels with respect to the total number of
scene pixels which must exceed 5% for a successful search. If p falls below 5% of
the scene, the search for reference pixels is iterated until it reaches >5%.
Third, the surface reflectance for the masked DDV pixels in the red band is
estimated as a fraction α of the NIR reflectance as:
(3.32)
from which AOD (visibility) is determined.
This approach of correction differs from the VNIR approach of Kaufman and
Sendra (1988) in that it uses a variable reflectance subject to the vegetation index
and the NIR reflectance while the 1988 algorithm requires a constant a priori
reflectance in the red band (e.g. ρ = 0.02). Moreover, the mask of the reference
pixels is not obtained using two fractions (thresholded NDVI, and lowest radiance
in NIR) based on a priori knowledge of vegetation in the scene, but with multiple
threshold criteria (Richter et al., 2006). Apparently, the images to be successfully
corrected using this method must encompass water, preferably extensive nearshore
waters.
VHR Imagery Correction
The above methods are inapplicable to VHR optical images because they contain
only four broad bands of blue, green, red, and NIR over a narrow swath width <20
km, in which water has a non-negligible signal, especially turbid waters. These
images can be corrected using the dark spectrum fitting (DSF) algorithm proposedby Vanhellemont and Ruddick (2018). Within the small area covered by each frame
of VHR image, the atmosphere can generally be assumed to be homogeneous.
Under this assumption the atmospheric path reflectance (ρpath) can be estimated
from multiple targets in the scene, which are selected according to the minimum
observed TOA reflectance (ρTOA) instead of the pre-defined “dark” bands in the
NIR and SWIR wavelengths. The band yielding the lowest ρpath is considered the
best and selected. This selection avoids unrealistic negative (overcorrected)
reflectance after the radiometric correction. Nevertheless, inland water pixels in the
NIR bands are usually affected by scattering from adjacent land and vegetation
pixels, resulting in unrealistic ρpath. It can be determined using the dark object
method because VHR images with a submeter spatial resolution enables shadows
from trees and buildings to be spatially discernible. The use of these shadow pixels
produces better outcomes than water pixels for broad-band VHR images. ρTOA in
each band is calculated as:
(3.33)
where D = sun-earth distance, F0 = extraterrestrial solar irradiance, and θs = solar
zenith angle at the image center. must be corrected for the influence of water vapor
and ozone concentration in the atmosphere using gas transmittance (tg), taken as
the product of the band-averaged ozone and water vapor transmittance, or:
(3.34)
where twv is retrieved from a hyperspectral LUT generated using the 6SV RTM for
the scene center sun, viewing zenith angles (θs and θv) obtainable from the date
and location of the image metadata (together with D), and total precipitable water
(uwv) obtained from existing data product. tO3 is computed for the scene center air
mass as:
(3.35)
where = ozone optical thickness obtained by multiplying the ozone concentration
(μO3) by (). kO3 (in cm−1) is derived using SeaDAS software (see Section 3.6.1 for
more details). Daily observations are available from ozone data products. The
diffuse sky reflectance of water pixels reflected at the air-water interface is
computed analytically and removed from ρt:(3.36)
where τr = Rayleigh optical thickness, as retrieved from the LUT; and = sun and
viewing angles, respectively; and pr is calculated as:
(3.37)
where r(θ) = Fresnel reflectance for air-incident rays, calculated as:
(3.38)
where = angle of transmittance calculated as:
(3.39)
where nw = the refractive index of water with respect to air, taken as 1.34; = the
Rayleigh scattering phase function for scattering angle Θ:
(3.40)
It is denoted as θ+, signifying the photons reflected by the surface before or after
scattering, with Δϕ (relative azimuth between the sun and the sensor):
(3.41)
This method involves two assumptions: (i) The atmosphere is homogeneous within
a certain and limited spatial extent so that the atmospheric path reflectance (ρpath)
can be approximated as being constant within the imagery-covered area; and (ii)
The image contains pixels that have approximately zero surface reflectance (ρs = 0)
in at least one of its spectral bands (e.g., water and dark shadow pixels).
This method of atmospheric correction is implemented in multiple steps: (i) ρt is
corrected for atmospheric gas transmittance and air-water interface sky reflectance;
(ii) ρdark is determined via an ordinary least squares (OLS) regression in each band
(e.g., the intercept). ρdark(λ) of a band is estimated using either the percentile
approach, or by sorting the corrected ρt by brightness, and fitting an OLS
regression through the low reflectance end of the distribution (e.g. the darkest
1,000 pixels). The intercept of the OLS regression is considered to represent the
best estimate of the “darkest” target in this band; (iii) τa at 550 nm is estimated3.4
3.4.1
using ρdark in each band by interpolating the ρpath for the different τa increments in
the LUT. The selected ρdark is fitted to different aerosol models. For the scene￾specific sun and viewing geometry, ρpath(λ) is computed from the solar and viewing
geometry at the time of sensing for a few types of aerosol models at τa = 550 nm,
based on a pre-calculated LUT. The atmospheric path reflectance (ρpath), the two￾way diffuse atmospheric transmittance (tdu), and the spherical albedo (sa) of the
atmosphere are required to compute the directional surface reflectance as:
(3.42)
where = path-corrected reflectance, calculated as:
(3.43)
where tg = gas transmittance, and ρsky = an estimate of the air-water interface sky
reflectance, which is set to 0 for land pixels and estimated analytically for water
pixels using Eq. 3.36.(iv) For each aerosol model in the LUT, the band having the
lowest τa is retained. The band and model combination that produces the lowest
overall τa is finally used for the correction. This method of correction has a
satisfactory performance in retrieving AOT (τa) for an urban and a coastal site
(Vanhellemont and Ruddick, 2018). This method of correction does not take into
account sun glint and wave influence. Instead, the wave facets, and hence glint, are
assumed to be spatially resolved on the image and thus cannot be statistically
modeled based on wind speed. Aerosol effects on the image are not corrected,
either. Since the VNIR images have a broad bandwidth that is ill-suited to retrieve
in-water constituents, this omission does not compromise the applicability of the
atmospherically corrected images.
IMAGE-BASED METHODS
Dissimilar to the physical and semi-analytical methods, image-based methods of
correction modify pixel values based on image-contained information without the
need for any input external to the image, so they are much easier to realize. This
section introduces two of them, DOS and DDV.
Dark Object SubtractionDOS is a long-established and commonly used simple method of atmospheric
correction based solely on the image itself, also known as the histogram minimum
method, the darkest object subtraction method, and the dark target approach. This
method of correction relies on the absorption of the solar radiation by dark objects
such as shadow and water in an NIR band of the image to be radiometrically
calibrated. It must encompass a handful of pixels of dark objects whose DNs are
assumed to have a value of zero, along with a horizontally homogeneous
atmosphere (Chavez, 1988), because they either do not receive the incident energy
as in the shadow or the incident energy has been completely absorbed by the dark
object in the NIR band, a phenomenon known as the black water assumption. In
either case, no energy is reflected off the dark objects, causing their radiance on the
NIR band to be nil theoretically. The non-zero pixel values of dark targets must
stem from the upwelling path radiance of the atmosphere and the residual surface
radiance of the dark objects received by the sensor. If the black water assumption is
completely valid, the pixel value must represent the effects of scattering of the
atmosphere (the so-called haze effects) or the interference from the nearby bright
objects on the ground. They manifest numerically as the minimum pixel value in
the band’s histogram. The haze impacts can be effectively removed by simply
subtracting the non-zero pixel value of a dark object from the values of all pixels in
all the multispectral bands of the image.
Essentially, this method of radiometric correction just removes haze formed by
aerosol scattering from the image, but not the radiation absorbed by the
atmosphere. Conceptually, this method is easy to comprehend and implement.
Nevertheless, it is imprecise and crude in that the amount of the scattered radiation
recorded in different spectral bands is a function of the radiation wavelength
according to Rayleigh (Eq. 3.1) and Mie scattering laws. Thus, the scattering￾contributed radiance is unequal in multispectral bands, even though the
contribution is correlated with each other. Thus, the pixel value uplifted by
atmospheric scattering in an IR band differs from that of a shorter-wavelength
band. The linear relationship between satellite-observed radiance and surface
reflectance can be estimated empirically by using bright and dark (non-vegetated
and invariant) objects in the scene with known (i.e. measured) or modeled
reflectance. This method of correction addresses only the first-order scattering
component, not secondary scattering into shadowed areas, such as skylight. Thus,
the simple subtraction cannot eliminate the atmospheric effects in all bands
completely.DOS can be improved by predicting the haze values in all the available spectral
bands of an image from the haze value of a NIR band based on a relative
atmospheric scattering model that best describes the atmospheric conditions at the
time of sensing, such as the Rayleigh and Mie models (Chavez, 1988). They
stipulate that the amount of scattering is inversely proportional to the varying
power of radiation wavelength. The haze value in other bands is predicted from the
starting haze value determined from the histogram of the selected NIR band. The
predicted haze values may need normalization for the different gain and offset
parameters used by the imaging system before they are used to correct the raw DNs
of image pixels as:
(3.44)
where DNi = digital number of pixels in band i; ai and bi = gain and offset,
respectively; DNmi = raw DN contributed by multiplicative factors, including the
reflectance of the target and the atmospheric absorption in band i; DNhi = DN of
haze in band i calculated from a relative scattering model using different powers of
wavelength (e.g., 4—very clear, 2—clear, 1—moderate, 0.7—hazy, 0.5—very
hazy) (Chavez, 1988).
The above correction is unable to address the absorption effects of aerosols in
the atmosphere, so this method has been improved by considering atmospheric
transmittance and path radiation that are affected by four factors of atmospheric
molecule, ozone, aerosol, and water vapor (Wang et al. 2019). Of these factors,
atmospheric molecules and ozone content are relatively stable and hardly vary in
space, in contrast to aerosol and water vapor that vary more spatiotemporally. They
are the main factors degrading the accuracy of atmospheric correction. This
variability makes it extremely difficult to derive accurate water vapor optical
thickness using radiative transfer equations to model the contribution of aerosols to
the atmospheric path radiance. Thus, the ancillary information on AOD and total
water vapor is estimated from multispectral bands to correct the path radiance.
Under the assumption of surface uniformity, Lambertian reflection, isotropic sky
irradiance, surface-level radiance (L0) after ignoring atmospheric refraction,
polarization and proximity effects can be expressed as:
(3.45)where ρ = ground-level surface reflectance; E0 = extraterrestrial solar irradiance; =
the incidence angle between the solar vector and the normal vector of the terrain
(accounting for its slope and aspect); Edown = down-welling irradiance at the
surface due to the scattered solar flux in the atmosphere; Tz = atmospheric
transmittance through the path of sensor to the Earth. The apparent radiance
received by the satellite (at-sensor radiance Lsenor) is a function of surface radiance,
atmospheric transmission, atmospheric albedo, and upward atmospheric spectral
radiance Lp caused by atmospheric scattering, namely:
(3.46)
where Tv = atmospheric transmittance through the path of Earth to Sun; S =
spherical albedo of the atmosphere. Since S has a small value that can be ignored, ρ
is expressed as:
(3.47)
where D = atmospheric path length (e.g., the sun-Earth distance); If the reflectance
of a dark pixel is 0.01, then Lp is calculated as:
(3.48)
where Ld = radiance of dark pixels in the image. This equation requires three
variables in the input (Tz, Tv, Edown), triggering four versions of DOS
implementation, depending on how they are calculated (Table 3.2).
Table 3.2
Four versions of implementing the DOS correction method using different parameter settings
Method Tv Tz Edown
DOS1 1.0 0.0
DOS2 6SV
DOS3
DOS4 6SV 6SV 6SV
Source: Wang et al. (2019), open access.
The retrieval of AOD from multispectral bands faces a thorny issue of how to
isolate the atmospheric contribution to the observed reflectance from that of surface3.4.2
contribution. This issue can be resolved by retrieving AOD using the DDV method
(see Section 3.5.2 for more details). The retrieval relies on the linear relationship of
the low reflectance of dense vegetation in red and blue bands with its reflectance in
a SWIR band. If the surface is assumed to be a Lambertian reflector and the
atmospheric conditions are uniform, the at-sensor apparent reflectivity of the TOA
can be expressed as Eq. 3.5 (6SV), in which ρ0, S and T(θs)/T(θv) are parameters
related to atmospheric optical properties from which AOD is retrieved. During the
retrieval, the surface reflectance noise is removed by decoupling of the ground gas.
The correspondence between AOD and ρ0, S and T(θs)/T(θv) is calculated using a
RTM such as 6SV. AOD is determined via a LUT (Wang et al. 2019).
The ratio of multiple bands can eliminate the effects of atmospheric scattering
and surface reflection in the presence of atmospheric water vapor. Its content can
be retrieved using the band ratio method. The radiance received at different
wavelengths (λ) is expressed as:
(3.49)
where Lsen(λ) = radiance received by the sensor; Lsun(λ) = top-of-atmosphere solar
radiance, τ(λ) = atmospheric transmittance, ρ(λ) = surface reflectance, Lpath(λ) =
atmospheric path radiation.
The DOS model may be optimized by using the retrieved AOD and total water
vapor information. Sentinel-2 imagery optimized using this method exhibits
obviously improved visual effects, clarity, and contrast. The atmospherically
corrected reflectance curve resembles more closely the measured reflectance curve
of typical objects in both spectral shape and reflectance value than the raw one
(Wang et al., 2019). Compared with the traditional version, the improved DOS
method is more accurate and practical, but the image to be corrected must
encompass a few dark water pixels with zero reflectance for this method to
function properly.
Dense Dark Vegetation Algorithm
This method is founded on the assumption that vegetation is sufficiently dark to
guarantee a constant ratio between bottom-of-atmosphere (BOA) reflectance at
multiple wavelengths or in multispectral bands. The atmospheric effect is
eliminated based on the assumed linear and stable relationship between surface
reflectance at the SWIR (λ = 2.1–2.2 μm) wavelengths and that at blue (0.48 μm)and red (0.66 μm) wavelengths, namely, ρb = ρswir/4 and ρr = ρswir/2. Thus, surface
ρswir is approximated as , the TOA reflectance. This approximation is fairly
accurate, because of the small path radiance and high atmospheric transmittance (τ
= 0.9) in the SWIR spectral region (Richter et al., 2006). The differences between
the apparent TOA reflectance and the reflectance from the above relationships are
used to calculate Lpath(λ) and corresponding AOD.
For images that lack the 2.1 μm band, it is replaced by NIR and red bands. A
correction relationship between TOA and land surface reflectance at a short
wavelength near 2.13 μm is introduced to avoid the assumption that the TOA
reflectance is equal to the land surface reflectance at this wavelength, commonly
assumed in the classic DDV method (Li et al., 2009). Thus, the surface reflectance
in the red band is assumed to have a constant ratio with that of the blue band, or:
(3.50)
where , = angular spectral surface reflectance in red (0.66 μm) and blue (0.47 μm)
bands; κ = ratio commonly set to 2.
DDV is identified as a suitable reference to derive AOD over land surfaces in
four steps: (i) identification of all water and snow/ice pixels from ancillary
information and masked; (ii) identification of cloud pixels based on TOA, and
identification of dark target pixels using NDVI based on the fact that clouds have a
much higher reflectance and ratio than most dark features such as soil and
vegetation; (iii) LUT construction. A LUT is built from such parameters as band
response, aerosol type (continental vs urban), geometric conditions (solar zenith
angle, sensor zenith angle, and relative azimuth angle), AOTs (0.0, 0.25, 0.5, 1, 15.,
and 2) in 6S software; and (iv) dark dense target AOT retrieval using the following
equation (Li et al., 2009):
(3.51)
This method of inversion produces more accurate AOT than the ground-measured
AOT and the AOT derived from MODIS data product (Liu et al., 2019). The AOT
at 550 nm retrieved from simulated Hyperion hyperspectral data using the
improved DDV algorithm is tightly correlated with the actual values at correlation
coefficients higher than 0.99. However, this method cannot be used for images that
lack SWIR bands.3.5
3.5.1
CORRECTION OVER COMPLEX WATERS
Atmospheric correction over Case II waters is more challenging than over Case I
waters. How to accurately correct the atmospheric radiometry in inland and coastal
waters is one of the main challenges preventing the successful and quantitative
retrieval of biogeochemical variables from satellite data because the black water
assumption is no longer valid. The water-leaving radiance Lw is progressively
degraded by increasing suspended sediment loading and algal blooms in nearshore
waters. Scattering by suspended sediments causes it to be non-negligible. The
retrieval must be preceded by the isolation of aerosol and marine contributions to
the TOA radiance captured by space-borne sensors from each other under the
assumption that the signal remaining in SWIR bands after Rayleigh correction is
caused purely by aerosol scattering due to the extremely high pure water absorption
in these bands. Due to non-negligible water-leaving radiance (Lw) in NIR bands,
SWIR bands are required to derive surface reflectance (Rrs) reliably. Sentinel-2
SWIR bands at 1.6 and 2.2 μm allow for robust image-based atmospheric
correction, and water pixel detection. However, this method does not work for
images that lack the SWIR bands, so NIR bands have to be used. Atmospheric
correction of satellite imagery lacking the SWIR bands over turbid waters can still
be carried out using the modified iterative correction algorithm proposed by
Lavender et al. (2005), or using the modified Complex water Atmospheric
correction Algorithm Scheme (CAAS) method of correction using NIR bands in
ocean color sensing.
Iterative and Optimized Methods
The iterative algorithm is based on modified bright pixel atmospheric correction
that differentiates the NIR Rayleigh-corrected reflectance into two parts, aerosol￾contributed and turbid water contributed. The calculated NIR Lw is a function of
suspended solid matter concentration and sensor geometry. Normalized water￾leaving radiance (nLw) at 412 nm hardly varies with turbidity. The aerosol
reflectance over turbid waters is calculated for various aerosol types over clear
waters, derived using the standard Case I water correction algorithm if the water
has been stratified into clear and turbid waters. Reflectance of turbid waters at
wavelength λ is modeled as a function of the optical properties of pure water,
optically active water constituents, and the solar and viewing geometry. The
relationship between NIR reflectance and in-water suspended particulate matter3.5.2
concentration (phytoplankton ignored) is empirically established via field￾measured spectra. Alternatively, water reflectance in NIR bands can be estimated
via a bio-optical model. The estimated water reflectance is translated to TOA
reflectance with the assistance of the atmospheric diffuse transmittance value
calculated using SeaDAS (see Section 3.7.2 for more details), and ultimately yields
the NIR aerosol reflectance. The SeaDAS dark pixel atmospheric correction
method (with phytoplankton-dependent NIR reflectance correction) is reused to
extrapolate the aerosol radiance in the visible bands. This iterative method can be
further optimized for correcting images containing waters of a moderate to high
turbidity, in which the iterative method is used to process water pixels of a
moderate turbidity (solid particulate matter concentration < 200 g × m-3), and the
optimized method is used to process water pixels of a high turbidity.
CAAS
The CAAS developed by Shanmugam (2012) improves SeaDAS by not requiring
any ancillary information or involving any assumptions. Instead, atmospheric
properties (e.g., the aerosol and sun glint contributions) and oceanic signals are
derived solely from the total and Rayleigh-corrected radiance over the full
spectrum of ocean color. The total at-sensor signal (LTOA) in a spectral band
centered at wavelength λi is decomposed into three types of radiance after the
whitecap contribution is ignored for brevity (in ocean color data it is estimated by
using a previously established reflectance model with the input of sea surface wind
speed, but not accurately):
(3.52)
where = radiance of direct sun glint from the sea surface, = radiance of whitecaps
at the sea surface, = water-leaving radiance to be retrieved, and T(λi) and t(λi) =
respective direct and diffuse transmittance of the atmospheric column; = path
radiance resulting from scattering in the atmosphere and from specular reflection of
atmospherically scattered light (skylight) from the sea surface. It is further
decomposed into three parts (Eq. 3.53) under the assumption that radiances due to
aerosol or molecular scattering are separable:
(3.53)where = radiance caused by air molecule scattering, or Rayleigh scattering in the
absence of aerosol; = radiance formed by aerosol scattering in the absence of air
molecules; and = coupled radiance stemming from interactions between molecules
and aerosol. over the VNIR wavelength range depends on the atmospheric
molecular composition, the solar and viewing geometry, but only weakly on sea
surface roughness. Subtraction of from yields the aerosol-corrected radian as:
(3.54)
(3.55)
In order to determine the aerosol type, aerosol radiance must be calculated from
two NIR bands, La(λNIR1) and La(λNIR2) iteratively, as from the spectral
information of as below:
(3.56)
If the images are obtained with a tilted scanning mirror to avoid sun glint, this
influence also needs correction. Sun glint distribution depends on the solar and
viewing geometry and sea roughness (wind-dependent wave slopes), defined as
follows:
(3.57)
where Lsg = sun glint radiance; = normalized sun glint radiance; and is assumed
unity. Lsg at 412 nm is calculated using the following equation under the
assumption that the glint noise is present in , , , and :
(3.58)
If Lsg < 0, (i = 412-551). If Lsg > 0, (i = 412-551). The relative amount of sun glint 
can be obtained from as an exponential function:
(3.59)
The sun glint corrected radiance is computed as:
(3.60)3.5.3
The water-leaving radiance exiting the TOA can be retrieved if the coupled aerosol
term is known. This component critically degrades the accuracy of frequently and
has to be removed, even though it is complex to determine. One way of
determining is to use the following spectral model:
(3.61)
where and are obtained independently with two models that behave quite
differently according to:
(3.62)
(3.63)
The above spectral models (Eqs. 3.62 and 3.63) overcome a number of
shortcomings, thereby effectively removing most, if not all, of the contribution.
The CAAS algorithm yields physically realistic water-leaving radiance spectra
in optically complex waters. A preliminary comparison with in situ data from
several regional waters of moderate complexity to high clarity reveals that the
CAAS algorithm has absolute errors highly similar to those of the SeaDAS
algorithm (Shanmugam, 2012). It outperforms the SeaDAS algorithm in accurately
estimating pigment and recovering areas previously flagged out by SeaDAS. The
algorithm is likely to be sensor independent, and can be applied to retrieve an
unbiased estimate of Lw(λ) values from existing images.
Modified SWIR Method
Nevertheless, the NIR-based correction does not work in highly turbid waters
because of band saturation. Thus, the SWIR-based iterative correction method must
be optimized over turbid lake waters based on the relationship between remote
sensing reflectance (Rrs) in visible bands. The SWIR algorithm is applied to the
selected relatively clear pixels to derive absorptance at 531 nm. It is considered to
be representative of the aerosol property over the entire water body (Zhang et al.,
2014). Then, Rrs at 2130 nm is calculated for all water pixels using uniformity￾screened absorptance at 531 nm or (531). α(531) is used due to the unique value for
each aerosol model in the aerosol LUTs in SeaDAS (refer to Section 3.7.2 for more
details). The TOA radiance (Lt) at 1240 nm is indexed for all water pixels, from3.6
3.6.1
which 200 pixels with the lowest values are retained to derive AOT using the
SWIR algorithm. If ρrc(2130) is higher than the selected threshold (e.g., 0.037), the
pixel is masked out. α(λ) is calculated for those selected pixels without the failure
of atmospheric correction as:
(3.64)
where τa = AOT.
To minimize the effects of outliers, a uniformity screen is applied to α(531) to
produce (531). The unfiltered mean value (531) and the corresponding standard
deviation (STD) are calculated, from which the filtered mean value (531) is derived
using the data in the range of ((531) − 1.5 × STD) and ((531) + 1.5 × STD). Rrs is
retrieved for all water pixels using (531) and the single band reflectance at 2130
nm. Aerosol models are first selected from the aerosol LUTs that closely bracket
the relative humidity derived from the local meteorological data. LUTs are
constructed from eight models regarding relative humidity. The aerosol scattering
radiance at 2130 nm [La(2130)] is calculated using two models having the smallest
difference between α(531) and (531) since Lw is negligible and La in VNIR bands
can be extrapolated using the coefficients of the two selected models. Rrs is derived
by subtracting La and Rayleigh scattering radiance from Lt, the total at-sensor
radiance.
The retrieved Rrs spectra have a reasonably close agreement with in situ
measurements not only over relatively clear waters with Rrs(859) over 0.0014 sr-1
but also over turbid waters of Rrs(859) about 0.013 sr–1 (Zhang et al., 2014). This
SWIR algorithm has a good performance in turbid waters, grounded on the black
water assumption at SWIR wavelengths.
COMPUTING PLATFORMS
MODTRAN
MODTRAN is a Fortran code system for modeling atmospheric propagation of
electromagnetic radiation over the 0.2–100 µm spectral range (Berk et al., 2003).
The most recently released version (MODTRAN6) provides a spectral resolution of
0.2 cm−1 using its 0.1 cm−1 band model algorithm. The MODTRAN system is able
to determine absorption and correct the wavelength-dependent atmospheric effects,
and SWIR and TIR bands have to be corrected differently. This software computes3.6.2
LOS atmospheric spectral transmittances and radiances over the ultraviolet to the
long wavelength infrared spectral regime (0–50,000 cm-1; >0.2 μm). The RT
physics adopted in MODTRAN allows stratified, horizontally homogeneous
atmospheres to be modeled accurately and quickly. The core of the MODTRAN RT
is the atmospheric “narrow band model” algorithm. It models the constituent
vertical profiles in the atmosphere, both molecular and particulate, using either
built-in models or user-specified radiosonde or climatology data (Perkins et al.,
2012). MODTRAN solves the radiative transfer equation including the effects of
molecular and particulate absorption/emission and scattering, surface reflections
and emission, solar/lunar illumination, and spherical refraction. The code has been
embedded in many operational and research sensors and data processing systems to
remove the atmospheric effects from multi- and hyperspectral images, including
Atmospheric and Topographic Correction (ATCOR), FLAASH, and iCOR.
SeaDAS
Any satellite images lacking the SWIR bands cannot be corrected using most of the
introduced methods except the SeaWiFS Data Analysis Software (SeaDAS)
(seadas.gsfc.nasa.gov). This method requires the optical properties of aerosols to be
computed based on RTM in combination with a NIR correction scheme (the water￾leaving radiance in the NIR band used to be assumed to be zero) and several
ancillary parameters. As the NASA/OB.DAAC´s official Data Analysis Software,
SeaDAS is a comprehensive computing system designed primarily for processing,
displaying, analyzing, and quality controlling ocean color data. It can also be used
to analyze many satellite-based earth science data. Originally developed to process
SeaWiFS data, it now can analyze data from most U.S. and international ocean
color missions. The latest version (SeaDAS 8.3) is an extended ESA SNAP
computing platform to the NASA SeaDAS Toolbox (version 8.3.0) and the ESA
Sentinel-3 Toolbox (version 9.0.3) (Wang et al., 2022). The SeaDAS Toolbox
encompasses the core elements of NASA SeaDAS processing commands and user￾interface. The NASA satellite mission data file readers and the ESA processors for
the Sentinel-3 missions are contained within the Sentinel-3 Toolbox. SeaDAS 8.3 is
a significant modification over SeaDAS 7.5.3 regarding the core components and
inner framework of the GUI. It can be run on Windows 10, Linux and Intel Max
OS X 10.12. The major limitation of SeaDAS is its reliance on the use of NIR
bands that causes the black water assumption to be invalid, so its failure over3.6.3
3.6.3.1
complex waters. The main inadequacy of NIR correction and constraints lie in
deriving aerosol optical properties whose characteristics are the most difficult to
evaluate because of their rapid spatiotemporal variability.
ESA SNAP
Currently, several atmospheric correction algorithms have been developed to
process Sentinel-2 data, including atmospheric correction for OLI ‘lite’
(ACOLITE), Sentinel-2 data Correction (Sen2Cor), Image correction for
atmospheric effects (iCOR), and the Case 2 Regional CoastColour processor
(C2RCC). All of them are image-based, meaning that all input data required for the
atmospheric correction are derived from the image itself or provided through pre￾calculated LUTs. Sen2Cor, C2RCC, and iCOR are embedded into the ESA´s
Sentinel toolbox in the SNAP.
ACOLITE
ACOLITE is an automatic, public-domain package for atmospheric correction of
images over oceanic and inland waters (Vanhellemont and Ruddick 2016). This
system is robust and easy to use for atmospheric correction of high-resolution
satellite images such as Sentinel-2 and Landsat OLI. It supports the processing of
full L1C scenes and individual granules, either full-scene or a portion of it via the
defined region of interest. Multiple scenes can also be stitched together. Images
may be masked to remove non-water pixels after Rayleigh correction to exclude
land, clouds, sun glint, and objects based on a threshold on the 1.6 μm band
(Vanhellemont and Ruddick, 2016). ACOLITE offers two options, with (V-32) and
without (V) smoothing of the SWIR bands over an extent of 320 m. If the masked
image encompasses bright pixels in SWIR bands, then they should be spatially
filtered (averaged) to exclude these pixels as they will otherwise influence the
surrounding dark (i.e., water) pixels. Image smoothing allows the consideration of
the nature of aerosol types (e.g., fixed aerosol type and variable aerosol type), and
markedly suppresses noises in the output.
Atmospheric correction is generally performed in two steps in ACOLITE
(Vermote et al., 2006): (i) Rayleigh correction for molecular scattering using a LUT
generated using 6SV, and (ii) Aerosol correction based on the black water
assumption, and an exponential spectrum for multiple scattering aerosol reflectance
(Vanhellemont and Ruddick, 2016). ACOLITE outputs water-leaving reflectance in3.6.3.2
all visible and NIR bands, and can compute other multiple parameters. It can be
applied to output remote sensing reflectances over extremely turbid waters using
SWIR bands.
Sen2Cor
Sen2Cor is a semi-empirical correction method based on the ATCOR method (see
Section 3.9.2 for more details), available as a third-party plug-in of the Sentinel-2
Toolbox. This level-2a processor aims chiefly to remove the atmospheric effects
from single-date Sentinel-2 level-1c TOA products to derive the level-2a BOA
reflectance product. The basic framework of the Sen2Cor processor (Figure 3.8)
consists of five modules coordinating interactions within the workflow, reading and
processing the input parameters and data, and the creation of an internal temporary
database, which is then used by the scene classification and the atmospheric
correction modules to retrieve and store the data and intermediate products,
providing the configuration parameters, as well as converting the products into the
desired format. The Sentinel-2 level-1c input data are processed using two main
modules, one of them is the atmospheric correction (AC) module. The processing
can be undertaken in a loop, dependent on the number of product resolutions to be
generated. Data processing comprises cloud detection and image classification,
followed by the retrieval of AOT and the water vapor content from the input image.
The final step is to convert TOA to BOA reflectance. Other outputs include maps of
AOT, water vapor, and scene classification with quality (probability) indicators for
cloud and snow cover (Main-Knorn et al., 2017). All results are output at a spatial
resolution of either 60 m, 20 m or 10 m, depending on the input band used.
Sen2Cor requires the input image to contain DDV pixels for AOT retrieval (Richter
et al., 2012). The retrieved AOT has an uncertainty of 0.03 ± 0.02 if a sufficient
number of DDV pixels can be found in the image.3.6.3.3
FIGURE 3.8 Level-2A processing schema with Sen2Cor, including atmospheric
correction for AOT and water vapor retrieval. (Adapted from Gascon et al., 2017,
open access.)
iCOR
iCOR, previously known as Operational atmospheric correction for Land and Water
(OPERA) (Sterckx et al. 2015), is a computing package originally developed for
atmospherically correcting Landsat-8 OLI and Sentinel-2 images. They have been
expanded to include space-borne, airborne, and drone images (e.g., MERIS,
hyperspectral and Sentinel-3 images). It is functional for both land and water pixels
in coastal, transitional, and inland areas by first identifying whether a pixel is water
or land and then applying a dedicated correction algorithm accordingly. In
correcting land pixels, the terrestrial surface is usually treated as a Lambertian
reflector. This correction scheme accounts for surface elevation variation, but not
the contribution of specular reflection at the air-water interface. The adjacency
effect is corrected using the SIMilarity Environment Correction module to calculate
the contributing background or environment radiance, in which the water leaving
reflectance in the NIR band is assumed to have an invariant shape. Land pixels are
used to derive AOT. In the AOT retrieval, iCOR partitions a TOA reflectance image
into macro-pixels of about 15 km × 15 km in size, a resolution sufficiently broad to
encompass high spectral variation but is small enough to assume spatial
homogeneity of the atmosphere. During the retrieval the algorithm searches for the
minimal TOA radiance values in visible bands to set an upper AOT boundary value
for each macro-pixel. This AOT value is subsequently refined based on the spectral3.6.3.4
variation within the macro-pixel, using a multi-parameter endmember inversion
technique. Five pixels with high spectral contrast (selected on TOA NDVI values)
are represented by a linear combination of three pre-defined default vegetation
spectra and a soil spectrum. iCOR also corrects the adjacency effects, which
improves the image quality at the water-land boundary, land, inland, and coastal
waters with a treatment of water surface reflectance.
iCOR requires the consideration of a wide range of parameters for atmospheric
correction, including sun and view geometry, ozone, water vapor (requiring a water
vapor absorption band), and elevation. The parameters are derived from
MODTRAN5 for its radiative transfer calculations. Elevation allows for an
accurate Rayleigh correction over high altitude areas (Sterckx et al., 2015). LUTs
are used to perform atmospheric correction, together with information about solar
and viewing angles. AOT is retrieved over land and water differently. iCOR runs do
not require user intervention as the required input parameters are derived from the
input image automatically (Wolters et al., 2021). Basic versions are freely available
as plug-ins in the SNAP toolbox. The generated output contains BOA reflectance.
Validation using the data from Sentinel-3 OLCI, TOA observations over land,
globally retrieved AOT, Top-of-Canopy (TOC) reflectance, and VIs demonstrates
that iCOR is suitable for retrieving statistically and temporally consistent AOT and
TOC reflectance over land surfaces from Sentinel-3/OLCI observations.
C2RCC
C2RCC is a full version system for atmospheric correction and retrieval of in-water
constituents from optical satellite imagery. This processor is based on deep learning
neural networks (Schiller and Doerffer, 1999). A set of neural networks are trained
with representative TOA reflectance spectra simulated via radiative transfer
modeling to determine water-leaving radiance from the TOA radiance, and to
retrieve the IOPs of the water body, during which the input spectra are corrected for
gaseous absorption. Air pressure, and thus a proper altitude correction, forms an
inherent part of the neural network processing. During radiative transfer
simulations the ocean and the atmosphere are treated as a full system simulated via
a water-specific model. The RTMs are carefully parameterized for the water body
and the atmosphere from the characteristics of optically complex waters through its
IOPs and of coastal atmospheres. Covariance between the water constituents is
taken into account and a large database of reflectance at the water surface is
calculated. These reflectances are considered as lower boundary conditions in3.7
3.7.1
calculating the radiative transfer in the atmosphere. A total of five million cases is
generated and serves as the basis for network training. For example, the TOA full
spectrum is input to a neural net that produces the water-leaving reflectance in the
VNIR bands as the main output.
Apart from directional water leaving reflectances, the atmospheric correction
based on neural net also outputs out-of-range tests and out-of-scope tests of the
TOA reflectances, with the corresponding quality flags attached. The SNAP version
of C2RCC offers the option to write the output of the auto-associative neural net
used of the out-of-scope test to the output file. The output from the transmittance
NN is used to flag the risk of cloud cover. C2RCC was initially designed to process
MERIS data for inland waters only, but its new version supports several other
sensors, including Sentinel-2 MSI, Sentinel-3 OLCI, Landsat-8 OLI, MODIS,
VIIRS, and SeaWiFS (https://c2rcc.org/documentation/). C2RCC has been used to
generate data products on IOPs, absorption and scattering of different constituents,
and concentrations of three major optically relevant in-water constituents
(phytoplankton pigments, total suspended matter, and yellow substance) and their
uncertainties in Case II waters.
COMPARATIVE ASSESSMENT
The relative performance of various atmospheric correction methods has been
compared both systematically and ad hoc. The former is exemplified by the
Atmospheric Correction Inter-comparison eXercise (ACIX). Several authors have
compared the performance of various correction methods, all of which are
presented below.
ACIX
ACIX is an international initiative aiming to assess the Surface Reflectance (SR)
products derived from Landsat-8 and Sentinel-2 data using eight state-of-the-art
atmospheric correction processors, together with AOT and water vapor. This
initiative between the NASA and the ESA explores the different aspects of a given
atmospheric correction processor and the quality of the spectral reflectance
products so as to better understand the different uncertainty components and to
improve the processors’ performance. The inter-comparison is based on ground
truth data obtained at 19 AERONET stations around the world, of which five arecoastal and inland, and 14 are terrestrial, distributed widely in different climate
zones and land cover types (Doxani et al., 2018). The comparison is implemented
in two stages. The first-stage comparison is mandatory and includes the correction
of Rayleigh scattering, aerosol scattering, and atmospheric gases. The correction of
adjacency effects is only involved if it could not be omitted from the processing
chain. The second-stage comparison is optional and allows the participants to
implement the full processing chain of their processors. The images used span over
a period of seven months. Due to cloud cover, the selected images do not have a
regular time interval. At some sites only one or two images are available per
month.
The inter-comparison is based on image subsets of 9 km × 9 km centered on the
AERONET sunphotometer station at each site. This size ensures the inclusion of a
whole number of pixels at the pixel sizes of Landsat-8 and Sentinel-2 images (i.e.,
30 m, 60 m, 20 m, and 10 m, accordingly). However, the 9 km resolution did not
allow any significant difference related to the adjacency effect or terrain to be
detected. The quality masks submitted by the participants were blended either
jointly or in combination, and only the common pixels flagged as “good” are
considered in the analysis. AOT is calculated at = 550 nm. Only the AERONET
AOT measurements within ±15 minutes of the Landsat/Sentinel overpass are
included in the comparison. The same inter-comparison protocol is adopted for
both the retrieved water vapor and AOT. The comparison is based on the Sentinel-2
band B09 (central wavelength = 945 nm) in the water vapor absorption spectral
zone that is unavailable with Landat-8 OLI imagery. The spectral reflectance is
initially inter-compared by plotting the averaged values over the subset test area by
date, band, and correction method. The time series plots reveal different
performances among the evaluated methods and atmospheric conditions at the time
of sensing and test sites.
An N × N matrix of distance is created, where N is the number of AC
processors. The elements of the matrix are the normalized distances between the
resulting averaged spectral reflectance values of the 9 km × 9 km subsets,
considering only the pixels commonly classified as of “good quality” and averaged
over the available dates. The main diagonal cells all have a value of zero and the
off-diagonal values represent the disparity between the two compared correction
processors.
Table 3.3Performance comparison of eight atmospheric correction methods (processors) in terms of AOT and
water vapor retrieved from Sentinel-2 images over all sites
Processor
AOT Water vapor
n Range Mean
±RMS
(stdv) n Range Mean
±RMS
(stdv)
CorA 47
0.000-
0.757 0.133 0.155 36
0.008-
1.312 0.370 0.332
FORCE 48
0.003-
0.871 0.116 0.169 43
0.001-
1.504 0.215 0.305
iCOR 37
0.002-
0.599 0.150 0.151
LaSRC 48
0.002-
0.602 0.115 0.097 41
0.021-
1.906 0.297 0.303
MACCS 24
0.002-
0.778 0.176 0.200 20
0.002-
1.654 0.269 0.387
S2-
AC2020 36
0.002-
0.652 0.107 0.144 29
0.005-
2.180 0.344 0.437
GFZ￾AC 41
0.001-
0.920 0.159 0.223 39
0.027-
1.246 0.457 0.283
Sen2Cor 47
0.005-
0.805 0.158 0.147 41
0.012-
1.630 0.280 0.346
Source: Modified from Doxani et al. (2018), open access.
The relative performance of eight correction processors is presented in Table 3.3. In
general, they all produced good AOT results from Sentinel-2 images. However,
some processors generated estimates widely different from AERONET
measurements. Overall, LaSRC produced the closest agreement between the
retrieved AOT and the observed AOT from AERONET measurements, as indicated
by the low RMS value. S2-AC2020, Sen2Cor, iCOR, CorA, and FORCE all
achieve a similar, good performance over all land covers and aerosol types.
Overall, these seven processors are able to quantify water vapor accurately.
However, noticeable differences exist between mean and maximum values,
attesting the presence of outliers that degrade the performance of most of the
processors and increase the RMS values. Except for bands 9 and 10, the absorption
by water vapor generally falls below 5% that should therefore be translated into3.7.2
negligible noise to the surface reflectance. The best performer is LaSRC with a
RMS of 0.097, much lower than the maximum RMS error of 0.223 achieved by
GFZ-AC (Doxani et al., 2018).
Individual Processors
The effectiveness of 6S, FLAASH, and DOS has been compared in correcting
Sentinel-2 images, facilitated by ancillary information on AOT from MODIS data
for the physically based methods (Lantzanakis et al., 2016). In all cases, the three
models yield spectral surface reflectance of a similar shape, but the absolute values
appear larger. The physically based methods outperform the image-based ones for
the Sentinel-2 imagery. Nevertheless, their high computational cost and the need
for ancillary atmospheric information make them difficult to adopt.
The ability of Sen2Cor, ALOCITE, and C2RCC to eliminate the atmospheric
effects from single-date Sentinel-2 level-1c TOA products to produce level-2a
BOA reflectance products has been evaluated by Gafoor et al. (2022). Sen2Cor (R2
= 0.7, RMSE = 10.4) is the most effective corrector, outperforming the other two in
the initial bathymetry comparisons. C2RCC (R2 = 0.2, RMSE = 44) is the worst,
much worse than ALOCITE (R2 = 0.6, RMSE = 13). This comparison is expanded
to include iCOR for their ability to produce linear relationships between ratios of
log-transformed bands and bathymetry (Casal et al., 2019). The results are
inconclusive because ACOLITE and C2RCC output reflectances whereas Sen2Cor
and iCOR output BOA reflectances that include sun glint and whitecap
contributions. Of the four methods, Sen2Cor produced the lowest correlations with
the validation data because it was initially designed for terrestrial areas and could
not take into account the intrinsic characteristics of the water column. Designed for
turbid waters, unsurprisingly, ACOLITE achieved higher R2 even in optically
complex waters. iCOR had a mixed performance, achieving a high R2 with some of
the images but not consistently with the whole dataset. The low R² values with
some of the images were attributed likely to the presence of heterogeneously
distributed sun glint. The C2RCC processor, developed for Case II waters, relies on
a large database of simulated water leaving reflectances and related TOA radiances
(Brockmann et al. 2016). It produced the highest R2 values most consistently with
all the images, which demonstrates the importance of reducing sun glint from the
images in achieving sound correction.3.8
Validated against the reference surface reflectance SRref, the surface reflectances
from SREM (SRSREM) outperformed the Planet Scope (PS) SRprod, Landsat
operational Surface Reflectance Products (SRprod) generated through different RT
models using AOD, water vapor, and ozone data (Bilal et al., 2019). SRprod was
derived from PS at 3 m spatial resolution, Sentinel-2 AB (S2AB) MSI at 10 to 60 m
spatial resolution, and Landsat-8 OLI (30 m spatial resolution). Among the even
bands of S2AB MSI, SRSREM produced comparable results to SRprod for all of the
accuracy metrics except for band SB1 (correlation = 0.36). This relativity was
reversed in the same band of L8 OLI where SRSREM showed an R2 value (0.74)
higher than SRprod’s 0.35. SRSREM had a similar performance to SRprod. SRprod’s
performance with Landsat 8 OLI and S2AB was inferior (R of 0.35 and 0.57) in the
coastal blue (SB1) and blue (SB2) bands to SRSREM. Compared with SRprod, SREM
could produce robust SRSREM even without using AOD, water vapor, and ozone
data, for all RT models tested (Bilal et al., 2019). It under-corrected the
atmospheric effects for some dates, against SRref and SRprod that had higher values
than the TOA reflectance, even under clear-sky conditions but this was not the case
for SREM. An examination of the estimated surface reflectance in shadowed areas
revealed that negative SRref and SRprod values were limited mainly to the coastal
blue (SB1) and blue (SB2) bands of Landsat 8 OLI, but no bands had negative
SRSREM values. The inferior performance of these bands is attributed to the fact
that they are used within the atmospheric correction algorithm to perform aerosol
inversions, thus users are cautioned against their use. These results indicate that
SREM is comparable to, or better than the SR products derived from other sensors.
TOPOGRAPHIC CORRECTION
If the area under study is rugged or mountainous, then the image used for
quantification may be subject to the influence of topographic shadow, which is an
artificial radiance unrelated to the target of quantification. It must be eliminated to
ensure a reliable quantification result. The length of topographic shadow is related
to two factors, solar zenith angle θ and local relief Δh (Figure 3.9). The former is
further dictated by the imaging time which is recorded and available in the imagery
header information as metadata. The local topographic relief can be calculated from
a DEM. As illustrated in Figure 3.9, the topographic shadow length (ΔL) is
calculated as:3.8.1
FIGURE 3.9 Length of topographic shadow (ΔL) on satellite imagery related to the
solar zenith angle (θ) and local relief (Δh).
(3.65)
Once the shadow-affected area is determined, the intensity of effect is determined
via a simple comparison between the sun-lit slope pixel values and those in the
shadow, a calibration value can then be universally applied to all the shadow￾affected pixels. If the concerned image has multispectral bands, then different
calibration values need to be acquired from the respective band, and applied to the
relevant bands individually as the intensity of effect is wavelength dependent.
Topographic correction algorithms are not available in most systems introduced in
this chapter except FORCE and ATCOR, both requiring a DEM.
FORCEThe Framework for Operational Radiometric Correction for Environmental
monitoring (FORCE) (https://github.com/davidfrantz/force) is an open source
computing system for processing archived medium-resolution EO satellite images
(Frantz, 2019). It uses the data cube concept to bulk-process data for large area and
multi-temporal applications. It is composed of modules and executable programs
organized by processing level (e.g., lower, higher, and auxiliary). Level 2
processing includes detection of cloud, cloud shadows, and snow using a modified
mask algorithm, radiometric correction, and topographic correction to generate
harmonized, standardized, and radiometrically consistent products with quality tag
attached to each pixel (Frantz et al., 2016).
FORCE contains a module for correcting the atmospheric effect of water vapor
absorption via a pre-compiled (auxiliary) water vapor database for Landsat data (it
is estimated for each Sentinel-2 pixel). Each image is corrected using the retrieved
daily water vapor values. Radiometric correction includes radiative transfer-based
atmospheric correction by estimating AOT over dark and DDV objects identified
using a combination of existing database and image-based processing of multiple
scattering. AOT derivation takes into account the actual target reflectance and the
adjacency contamination effect. If bright objects are absent from the scene, a
modeled AOT climatology is used instead as the fallback alternative. The retrieved
AOT is highly correlated with AERONET sunphotometer measurements at R² of
0.72–0.79 (low intercepts and near-unity slopes) (Frantz et al., 2016). The AOT
retrieved from the Landsat TM SWIR1 band in the wet season based on the water
vapor fallback climatology has an uncertainty of approximately ±2.8%. The
corrected reflectance differs by 0.53% (4.8%) between the daily water vapor value
and the corresponding climatic average for the dry (wet) season image for
extremely bright features (nearly 100% reflectance). Thus, the uncertainty in the
wet season is slightly higher. The effect is less pronounced for grey objects (50%
reflectance), a difference of only 0.29% (2.6%).
A DEM (auxiliary data) is needed for enhanced cloud shadow detection and
topographic correction. It should cover the study area fully at an appropriate grid
size. The DEM will be warped to the extent and resolution of the processed image
using bilinear resampling. In FORCE, topographic correction is performed via a
physical model by applying an empirically derived extra parameter C (Frantz et al.,
2016). The C-factor is estimated for each pixel in every band of the image and then
propagated through the spectrum using the radiative transfer theory. Three kernels3.8.2
of increasing size are used to approximate the background reflectance for
environment correction. The topographic correction factor A is calculated as:
(3.66)
The illumination angle θi is computed as:
(3.67)
where and = solar elevation and azimuthal angle; and = terrain slope and aspect
computed from a DEM warped to the same extent and resolution of the satellite
image in use; h = fraction of the sky dome contributing to the diffuse illumination,
where h0 is the h-factor at cosθi = 0. The C-factor is estimated from the regressed
linear relationship between cosθi and the spectral radiance of a sloped surface L*.
The C value is empirically determined by dividing intercept (b) by slope (s) or C =
b/s. The core of computing the A-factor is to derive the C-factor. It is estimated for
specific land cover classes instead of the entire input image so as to improve the
retrieval accuracy. If the C-correction does not achieve an R² value above 1% in a
given class, it is replaced by the simple fallback Minnaert correction with a fixed
Minnaert coefficient of 0.8. It simplifies the A-factor to:
(3.68)
L* and cosθi are, in general, not closely correlated with each other, with the R²
value lying between 5% and 30% for different bands (Kobayashi and Sanga-Ngoie,
2008). Such high R² values are not always possible if the scene is not fully
mountainous.
ATCOR
Atmospheric/Topographic Correction for Satellite Imagery (ATCOR) is a public￾domain computing system initially developed for the terrestrial environment. It can
perform atmospheric and topographic correction for space- and airborne images
using two separate versions. The satellite version (ATCOR-2 for flat terrain, two
geometric degrees-of-freedom and ATCOR-3 for mountainous terrain with three
geometric degrees-of-freedom) supports all major commercially available images
acquired by small-to-medium FOV sensors with sensor-specific LUTs containing
pre-determined radiative transfer parameter values (Richter and Schläpfer, 2023).New sensors can be added on demand. The airborne version is called ATCOR-4 to
indicate the four geometric degrees-of-freedom (x, y, z, plus scan angle). Due to the
lack of standard set of airborne sensors and the temporal variation in the spectral
(radiometric) performance caused by sensor modifications, only a monochromatic
atmospheric database is compiled based on the MODTRAN®5 radiative transfer
codes. This database has to be resampled for each user-defined sensor.
Apart from the consideration of topographic effects, ATCOR is also capable of
processing TIR images. In the radiation component for atmospheric correction, the
user can select atmospheric parameters, such as aerosol type from four options of
rural, urban, maritime, and desert. The aerosol type includes the absorption and
scattering properties of particles, and the wavelength dependence of the optical
properties. The water vapor content can be automatically computed if the sensor
has spectral bands in the water absorption region (e.g., 920–960 nm). The approach
to measure the absorption depth is based on differential absorption determined
from spectral bands in absorption regions and the atmospheric windows. If a sensor
does not possess spectral bands in the water vapor regions (e.g. Landsat TM and
SPOT), the water vapor column is based on an estimate of the season
(summer/winter).
ATCOR offers two options for estimating visibility: (i) interactive estimation
using the SPECTRA module; (ii) automatic calculation if the scene contains dark
reference pixels. In the first method, the spectra of different targets in the scene are
extracted as a function of visibility that is subjectively determined by comparing
the observed spectra with the reference spectra in the libraries. In addition, dark
targets such as vegetation in the blue-to-red spectrum or water in the red-to-NIR
spectrum can be used, as well.
Dissimilar to the water surface, the terrestrial surface may be rough and rugged,
producing a strong BRDF effect with slopes facing the sun and others oriented
away from the sun. This effect has to be corrected for accurate quantitative
retrieval. In areas of steep slopes, the local solar zenith angle may vary from 0° to
90°, producing a range of solar irradiance with the maximum approaching zero
direct irradiance (i.e., shadow). The surface normal of a DEM pixel is at an angle
(β) to the solar zenith angle of the scene. There is no simple method to eliminate
the BRDF effects in a mountainous terrain. The usual assumption of an isotropic
(Lambertian) reflectance behavior often causes an over-correction of faintly
illuminated areas where local solar zenith angles range from 60°–90°. These areas
appear very bright. ATCOR uses empirical geometry-dependent functions for thecorrection. In the simplest case, the empirical BRDF correction employs only the
local solar zenith angle βi and an illumination angle threshold (βT) to reduce the
overcorrected surface reflectance ρL with a factor G, or:
(3.69)
where G = . The exponent b has a value within the range of 1/3 to 1, so the
reflectance is only decreased for local solar zenith angles βi > βT until the minimum
value G reaches g. βT and g may be scene-dependent and can be set by the analyst.
In many cases the default g = 0.25 works fine and only the threshold angle βT needs
to be decided (Richter and Schläpfer, 2023). In addition, the anisotropic reflectance
of natural surfaces causes brightness gradients if the image is obtained from
sensors of a large FOV and/or if the data are captured near the principal plane.
These effects can be removed empirically via data normalization to the nadir
reflectance. In addition, these effects are also present in rugged areas illuminated
by low local solar elevation angles, and can be addressed empirically by ATCOR.
REFERENCES
Berk A, GP Anderson, PK Acharya, ML Hoke, JH Chetwynd, LS, Bernstein, EP
Shettle, MW Matthew, and SM Adler-Golden (2003) MODTRAN4 Version 3,
Revision 1, User’s Manual. Bedford: Air Force Research Laboratory, Hanscom
AFB.
Bilal M, M Nazeer, JE Nichol, MP Bleiweiss, Z Qiu, E Jäkel, JR Campbell, L
Atique, X Huang, and S Lolli (2019) A simplified and robust surface reflectance
estimation method (SREM) for use over diverse land surfaces using multi-sensor
data. Rem Sens 11(11): 1344. doi: 10.3390/rs11111344
Brockmann C, R Doerffer, M Peters, S Kerstin, S Embacher, and A Ruescas (2016)
Evolution of the C2RCC neural network for Sentinel 2 and 3 for the retrieval of
ocean colour products in normal and extreme optically complex waters. In
Ouwehand L (ed.) Living Planet Symposium, Proceedings of the conference held
9-13 May 2016 in Prague, Czech Republic. ESA-SP 740, 54.
Casal G, X Monteys, J Hedley, P Harris, C Cahalane, and T McCarthy (2019)
Assessment of empirical algorithms for bathymetry extraction using Sentinel-2
data. Int J Rem Sens 40: 2855–2879.
Chavez PS (1988) An improved dark-object subtraction technique for atmospheric
scattering correction of multispectral data. Rem Sens Environ 24(3): 459–479.doi: 10.1016/0034-4257(88)90019-3
Doxani G, E Vermote, J-C Roger, F Gascon, S Adriaensen, D Frantz, O Hagolle, A
Hollstein, G Kirches, F Li, J Louis, A Mangin, N Pahlevan, B Pflug, and Q
Vanhellemont (2018) Atmospheric correction inter-comparison exercise. Rem
Sens 10(2): 352. doi: 10.3390/rs10020352
Frantz D (2019) FORCE—Landsat + Sentinel-2 analysis ready data and beyond.
Rem Sens 11: 1124. doi:10.3390/rs11091124
Frantz D, A Röder, M Stellmes, and J Hill (2016) An operational radiometric
Landsat preprocessing framework for large-area time series applications. IEEE
Trans Geosci Rem Sens PP(99): 1–16.
Gafoor FA, MR Al-Shehhi, C-S Cho, and H Ghedira (2022) Gradient boosting and
linear regression for estimating coastal bathymetry based on Sentinel-2 images.
Rem Sens 14(19): 5037. doi: 10.3390/rs14195037
Gascon F, C Bouzinac, O Thépaut, M Jung, B Francesconi, J Louis, V Lonjou, B
Lafrance, S Massera, A Gaudel-Vacaresse, F Languille, B Alhammoud, F
Viallefont, B Pflug, J Bieniarz, S Clerc, L Pessiot, T Trémas, E Cadau, R De
Bonis, C Isola, P Martimort, and V Fernandez (2017) Copernicus Sentinel-2A
calibration and products validation status. Rem Sens 9(6): 584. doi:
10.3390/rs9060584
Kaufman YJ, AE Wald, LA Remer, B-C Remer, R-R Li, and L Flynn (1997) The
MODIS 2.1 -/spl mu/m channel—correlation with visible reflectance for use in
remote sensing of aerosol. IEEE Trans Geosci Rem Sens 35: 1286–1298.
Kaufman YJ and C Sendra (1988) Algorithms for automatic atmospheric
corrections to visible and near-infrared satellite imagery. Int J Rem Sens 9:1357–
1381.
Kobayashi S and K Sanga-Ngoie (2008) The integrated radiometric correction of
optical remote sensing imageries. Int J Rem Sens 29(20): 5957–5985. doi:
10.1080/01431160701881889
Lantzanakis G, Z Mitraka, and N Chrysoulakis (2016) Comparison of physically
and image based atmospheric correction methods for Sentinel-2 satellite
imagery. Proc SPIE 9688. doi: 10.1117/12.2242889
Lavender SJ, MH Pinkerton, GF Moore, J Aiken, and D Blondear-Patissier (2005)
Modification to the atmospheric correction of SeaWiFS ocean color images over
turbid waters. Cont Shelf Res 25(4): 539–555.Li L, Z Chen, Q Wang, Q Li, and F Zheng (2009) Research on dark dense
vegetation algorithm based on environmental satellite CCD data. IEEE Int
Geosci Remote Sens Symposium, Cape Town, South Africa, II-515-II-518, doi:
10.1109/IGARSS.2009.5418132
Liu Y, Y Qian, N Wang, L Ma, C Gao, S Qiu, C Li, and L Tang (2019) An
improved dense dark vegetation based algorithm for aerosol optical thickness
retrieval from hyperspectral data. Proc. SPIE 11028, Optical Sensors, 1102812,
doi: 10.1117/12.2524488
Main-Knorn M, B Pflug, J Louis, V Debaecker, U Müller-Wilm, and F Gascon
(2017) Sen2Cor for Sentinel-2. Conference Paper October 2017. doi:
10.1117/12.2278218
Perkins T, SM Adler-Golden, MW Matthew, A Berk, LS Bernstein, J Lee, and M
Fox (2012) Speed and accuracy improvements in FLAASH atmospheric
correction of hyperspectral imagery. Opt Eng 51: 1707. doi:
10.1117/1.OE.51.11.111707
Rahman R and G Dedieu (1994) SMAC: A simplified method for the atmospheric
correction of satellite measurements in the solar spectrum. Int J Rem Sens 15(1):
123–143.
Richter R and D Schläpfer (2023) Atmospheric/Topographic Correction for
Satellite Imagery (ATCOR-2/3 User Guide, Version 9.4.1). ReSe Applications
Schläpfer, Switzerland.
Richter R, D Schläpfer, and A Muller (2006) An automatic atmospheric correction
algorithm for visible/NIR imagery. Int J Rem Sens 27(9–10): 2077–2085.
Richter R, J Louis, and U Müller-Wilm (2012) Sentinel-2 MSI—Level 2A Products
Algorithm Theoretical Basis Document; S2PAD-ATBD-0001, Issue 2.0.
Telespazio VEGA Deutschland GmbH, Darmstadt, Germany.
Schiller H and R Doerffer (1999) Neural network for emulation of an inverse
model operational derivation of Case II water properties from MERIS data. Int J
Rem Sens 20(9): 1735–1746. doi: 10.1080/014311699212443
Shanmugam P (2012) CAAS: An atmospheric correction algorithm for the remote
sensing of complex waters. Ann Geophys 30(1): 203–220. doi: 10.5194/angeo￾30-203-2012
Sterckx S, E Knaeps, S Adriaensen, I Reusen, L De Keukelaere, P Hunter, C
Giardino, and D Odermatt (2015) OPERA: An atmospheric correction for landand water. In Ouwehand L (ed.) Sentinel-3 for Science Workshop Proceedings,
2–5 June 2015. Venice, Italy.
Vanhellemont Q and K Ruddick (2016) ACOLITE for Sentinel-2: Aquatic
applications of MSI imagery. Proc. of the 2016 ESA Living Planet Symposium,
Prague, Czech Republic, 9-13 May 2016, ESA Special Publication SP-740.
Vanhellemont Q and K Ruddick (2018) Atmospheric correction of metre-scale
optical satellite data for inland and coastal water applications. Rem Sens Environ
216: 586–597. doi: 10.1016/j.rse.2018.07.015
Vermote EF, D Tanré, JL Deuze, M Herman, and J-J Morcette (1997) Second
simulation of the satellite signal in the solar spectrum, 6S: An overview. IEEE
Trans Geosci Rem Sens 35: 675–686, doi: 10.1109/36.581987
Vermote EF, D Tanré, JL Deuze, M Herman, and J-J Morcette (2006) Second
simulation of a satellite signal in the solar spectrum-vector (6SV). 6S User Guide
Version 3, University of Maryland, 56 p.
Wang J, Y Wang, Z Lee, D Wang, S Chen, and W Lai (2022) A revision of NASA
SeaDAS atmospheric correction algorithm over turbid waters with artificial
Neural Networks estimated remote-sensing reflectance in the near-infrared.
ISPRS J Photogram Rem Sens 194: 235–249. doi:
10.1016/j.isprsjprs.2022.10.014
Wang Y, X Wang, H He, and G Tian (2019) An improved dark object subtraction
method for atmospheric correction of remote sensing images. In Wang Y, Q
Huang, and Y Peng (eds.) Image and graphics technologies and applications.
IGTA 2019. Communications in computer and information science, vol 1043.
Springer, 425–435. doi: 10.1007/978-981-13-9917-6_41
Wolters E, C Toté, S Sterckx, S Adriaensen, C Henocq, J Bruniquel, S Scifoni, and
S Dransfeld (2021) iCOR atmospheric correction on Sentinel-3/OLCI over land:
Intercomparison with AERONET, RadCalNet, and SYN Level-2. Rem Sens
13(4): 654. doi: 10.3390/rs13040654
Zhang M, R Ma, J Li, B Zhang, and H Duan (2014) A validation study of an
improved SWIR iterative atmospheric correction algorithm for MODIS-Aqua
measurements in Lake Taihu, China. IEEE Trans Geosci Rem. Sens 52(8): 4686–
4695. doi: 10.1109/TGRS.2013.22835234
4.1
4.1.1
Analytical Methods
DOI: 10.1201/9781003517504-5
Core to the quantification of physical and chemical parameters in the four spheres is how to construct the
estimation model that translates remote sensing data or metrics derived from them into a spatial distribution map of
the attribute value of the target parameter, either solely or jointly with non-remote sensing environmental factors
that are indirectly obtained via remote sensing (e.g., temperature and elevation). Quantification of a given
parameter value of interest is virtually a process of transforming the remotely sensed data into the desired attribute
value via their analytically established models or equations, with or without the assistance of physical models.
They can be purely physical, semi-physical, and empirical. Physical models are based on RTMs over the target of
study. They are accurate but complex and require inputs of many parameters that are difficult or impossible to
parameterize using in situ collected data. Their accuracy suffers when the field data have a high degree of
uncertainty. Semi-physical models also face the same problem, but to a much lesser degree. Empirical models
depict the relationship between two sets of data: in situ observed parameter values and their corresponding values
in the concurrently acquired remote sensing data. These models or equations simply portray the mathematical
relationships between them without the power or ability to explain how one is affected by another. They are also
known as image-based methods as all the data needed for the quantification are derived from the images
themselves without the need for external data. As the name implies, image-based models involve independent
variables derivable from remote sensing images. These variables can be univariate or bivariate, corresponding to
one and two explanatory variables, respectively. The model can appear as linear or bi-linear, established through
simple regression analysis. Empirical models are constructed from the field-measured data and the image data at
the same spot statistically. Other methods of model construction include machine learning, and deep learning
methods.
This chapter concentrates primarily on how to construct the estimation model, using all sorts of statistical and
machine learning methods. Also included in the discussion are some miscellaneous methods of data analysis,
mostly concerning the construction of 3D models from time-series images and the identification of the most useful
predicator variables. The discussion starts from non-parametric methods of model construction, then progresses to
non-linear non-parametric methods. The third section of this chapter details kernel-based methods of analysis. The
last section introduces methods of 3D scene construction and detection of movements.
NON-PARAMETRIC METHODS
This suite of methods includes simple regression, stepwise multiple linear regression, principal components
regression, and Partial Least Squares Regression (PLSR). They are highly flexible and computationally efficient.
The number of explanatory variables can vary, depending on the actual application case.
Regression Models
Statistical regression is the classic method of quantification in which the in situ observed attribute values are linked
to the remotely sensed data or their transformed values statistically. In its simplest form, the regressed model is
linear, comprising only two terms, a coefficient (a) and an intercept (b) in the form of:(4.1)
Where a and b are determined via regression analysis automatically based on the ordinary least squares principle.
Such simple univariate linear models are quite accurate in certain quantification cases. For instance, the suspended
sediment concentration of a pixel is related closely to its pixel value in a single spectral band. This method is
simple, easy to understand, but is suitable only when Y can be reliably predicted from one remotely sensed
variable (e.g., univariate models) or two variables sometimes (bivariate models). Apart from linear models,
statistical models can be non-linear, constructed by fitting a set of in situ observations most closely, which means
that the overall discrepancy between the in situ observed and the remotely sensed data is minimized after squaring.
The prediction power of an empirical model is judged by its coefficient of determination commonly expressed
as R2, calculated using Eq. 1.26. A larger R2 value suggests a better and stronger model with a more powerful
prediction capability, and vice versa. As R2 is affected by the number of observations used (n), it is also adjusted
by sample size, termed adjusted R2. The degree of difficulty in fitting the observed data with a straight line (e.g.,
the linear prediction model) is related to n. It is much easier to fit a small sample neatly and tightly with a line than
with a large collection of samples. For this reason, n is also included in reporting the regression model.
In reality, the relationship between X and Y may be non-linear. The exact nature (simple linear, polynomial,
exponential, or logarithmic) of this empirical model is evaluated and judged visually by plotting the two sets of
data in a scatterplot (Figure 4.1). If the relationship is non-linear, then the raw data may be transformed
accordingly prior to the regression fitting. It is still possible to construct non-linear models, such as exponential,
power, or logarithmic after the proper transformation of the original input remotely sensed data or variables. Once
a linear relationship is constructed using the aforementioned method, it can be transformed back to the respective
model. Prior to any regression analysis, however, it is always advisable to plot the two sets of data graphically to
visually display the nature of the relationship, so that the appropriate model can be specified before running the
regression analysis.
FIGURE 4.1 Common relationships between the parameter value to be quantified and its remotely sensed quality. (a)
linear; (b) exponential. (Gao, 2022.)
In constructing the estimation model, the linear non-parametric regression model is usually the first choice because
of its simplicity and optimal performance. One particular form of linear regression is canonical linear regression
that typically relies on the estimation of co-variances. This can be problematic when the input data is of a limited
quantity with respect to the dimensionality of the dataset. To alleviate co-linearity, linear methods are frequently
applied after dimensionality reduction. Linear models, nevertheless, may not be the optimal choice when dealing
with complex datasets exhibiting non-linear attribute relationships, as is often the case with multi- or hyper-4.1.2
spectral imagery data, so non-linear models are frequently used instead. A number of classic linear approaches in
remote sensing applications are presented and compared with each other in Table 4.1.
Table 4.1
Comparison of the pros and cons of parametric regression in the quantitative retrieval of physical and chemical parameters of the environment
from remotely sensed data
Strengths Drawbacks
• Simple and flexible
• Easy to implement and comprehend
• Multiple models can be constructed and
compared with each other to identify the best
one
• Able to carry out sensitivity analysis
• Fast processing
• Computationally inexpensive
• Unable to utilize full spectral bands
• Vulnerable to data noise and the influence of outliers
• Potential correlation of explanatory variables in the same
model
• No theoretical ground on how to determine the nature (e.g.,
liner or logarithmic) of the model and its complexity (e.g., 1st
or 2nd order)
• Limited applicability as models are area- and even sensor
(scene)-specific
• Field data essential for model development are cumbersome
and costly to collect
• Unable to yield information on estimation uncertainty
Source: Adapted from Verrelst et al. (2015).
(Stepwise) Multiple Linear Regression
As remote sensing evolves, more and more complex ecosystem parameters are quantified. Since they are affected
by a plethora of factors, a single predictor variable can no longer achieve respectable accuracy based on a simple
linear model. Therefore, consideration of multiple variables has become the norm. Take forest aboveground carbon
as an example. It is related not only to image-derived variables, but also to climate (e.g., precipitation) and even
topographic (e.g., slope aspect) variables. They function as co-variables that also exert an influence on the
dependent variable or the parameter to be quantified. Reliable quantification of such parameters is possible only
with multiple linear regression analysis. For instance, the aboveground carbon stock of individual trees is related
jointly to tree diameter and height, so has to be predicted from a multi-linear regression model (Eq. 4.2). It is an
extension of the simple linear model in Eq. 4.1 in the form of
(4.2)
Where βi (i=1, 2, …, m) = model coefficients to be determined via regression; e = the error term; m = number of
predictor variables. This model is identical to Eq 4.1 except that it contains more terms or variables that total m. In
multiple linear regression analysis, the dependent variable is predicted from two or more variables, all of which are
input into the model in one analysis either simultaneously or stepwise. Multiple linear regression is easy to
implement and understand, and is still highly effective in certain quantification applications. However, this method
ignores the potential co-linearity between the multi-variables (Xi) included in a model.
In constructing the above model, a very critical consideration is the type and number (m) of potential
independent variables Xi that should be included in the model, an issue that has to be decided by the analyst.
Usually, a large pool of variables is considered as the candidates for inclusion in the model. The exact nature of
these variables requires expertise in the topic area to decide. They are selected based on specialized knowledge and
published literature on the topic, subject to data availability, spatial scale, and accuracy. Some variables, such as
temperature and soil moisture, must be spatial to be of any use. Initially, as many variables as possible should be
considered, even though not all of them affect the dependent variable equally. Naturally, not all of them are
(equally) important to the dependent variable Y, either.MLR faces the thorny issue of how to select the most useful independent variables from a large pool of
potential variables (e.g., more than a hundred hyperspectral bans), an issue known as feature selection. It aims to
develop a powerful predictive model (e.g., maximal model accuracy) using a minimal set of predictor variables
properly selected from the pool of inputs. Feature selection aims: (i) to make the model as simple as possible, and
(ii) to minimize the amount of computation needed. Feature selection can be based on unsupervised or supervised
learning (Brownlee, 2019). The former removes redundant variables based on their correlation with the dependent
variable via wrapper and filter. Wrapper searches for well-performing subsets of variables while filter selects the
subset of variables based on their relationship with the dependent variable, using Pearson’s correlation coefficient,
analysis of variance (ANOVA), and c2 test.
Feature selection may be implemented statistically based on feature importance, or automatically during
training using machine learning as part of learning the model, such as decision trees and recursive feature
elimination. Supervised learning, commonly associated with machine learning, learns the relationship between the
dependent and independent variables via the training or labeled samples. It will be covered in Section 4.3.
Statistical methods evaluate whether a candidate variable should be included in the model based on its contribution
to R2 of the regression model. Only those variables contributing a R2 value above the pre-defined threshold are
retained in the final model. The selection criterion can also be based on the correlation coefficient with Y, but this
selection strategy treats each candidate variable in isolation and ignores the potential correlation among multiple
independent variables, an issue known as the collinearity problem. For instance, tree height and its diameter may
be closely correlated with each other, so there is a possibility of double counting of the influence of the same
variable appearing in different formats. Chances are the more variables are included in the same estimation model,
the more likely they are correlated with each other to some degree.
Co-variance among all the considered factors is a major cause of a compromised model performance, and a
critical deficiency of multi-linear regression models. The consideration of more variables does not necessarily
guarantee proportional improvements in model accuracy. One method of checking whether multiple independent
variables in a model are correlated with each other is to use multicollinearity test. If they are too closely correlated
with each other, they will compromise the prediction capability of the constructed model (e.g., the p-value
becomes less trustworthy), and affect the proper interpretation of the modeled outcome because the coefficient of
the variables or their weight is highly sensitive to small changes. A high multi-collinearity also reduces the
precision of coefficient βi. Multi-collinearity is commonly tested via the Variance Inflation Factors (VIF). It identifies the strength of
correlation between independent variables and has a value ≥1. A VIF value of 1 indicates the absence of
correlation between the tested variables. A value between 1 and 5 suggests a moderate correlation that is not
sufficiently severe to warrant corrective efforts. However, a value above 5 signifies critical levels of multi￾collinearity and is indicative of unreliable coefficients and questionable p-values. In this case, one of the correlated
variables should be dropped from the model.
One method of minimizing multi-collinearity is to calculate the correlation between the dependent variable Y
and each of the considered independent variables Xi. An alternative is to make use of stepwise multiple linear
regression (SMLR) analysis, in which an additional variable is added to an existing model as shown in Eq. 4.2 in
turn, and the model´s changed R2 is noted. MLR is executed recursively a number of times. Initially, the dependent
variable is regressed against one of the selected independent variables. Each of them is used to establish a model
one at a time individually. Hence, the number of constructed models equals the number (n) of independent
variables. The R2 values of all the models are compared with each other. Only the model achieving the maximum
R2 is retained, to which one additional variable is added recursively. The same regression analysis is repeated by
inserting all the remaining variables subsequently (minus the one already included in the model) to the chosen
model, with their R2 values noted. The same analysis is thus performed n-1 times. The analyst needs to set up a
limit on the induced pace of improvement in model accuracy before deciding whether an independent variable
should be included in the quantification model. These values are compared to the R2 value of the previous
univariate model to see whether its inclusion in the model has improved model (prediction) accuracy above the
prescribed minimum threshold. If the increase in R2 falls below the specified threshold, then it is excluded from4.1.3
the new model. At the end of the iteration, only the variable that boosts the R2 value the most and above the
allowable threshold is retained in this round of analysis to form a new bivariate model. The relative improvements
in R2 of the two retained variables reveal their relative importance to the dependent variable to be quantified. Their
coefficients in the multiple linear regression model indicate their relative contribution to the dependent variable.
This process is iterated, during which the bivariate model is then updated using the same method as described
again by including one of the n-2 remaining variables in turn. At the end of this recursive process, a variable set is
finalized that optimally explains the dependent variable. In quantitative remote sensing, SMLR is commonly used
to establish the empirical relationship between Y and Xi that may include spectral bands and other co-variables. It
differs from ordinary multiple linear regression in that each considered independent variable is considered
incrementally in stepwise regression while all considered variables are used simultaneously in multiple linear
regression. SMLR requires a test (e.g., F test) of the variables most closely correlated partially with the dependent
variable to determine its suitability for inclusion in the model. Only those significant variables are included. Thus,
the performance of a given variable in the analysis is not clear, even though stepwise regression involving multiple
variables is superior to simple linear regression.
PLSR
The collinearity issue in multiple linear regression analysis can be resolved using PLSR. As a particular form of
multivariate analysis, PLSR is underpinned by an assumption that the dependent variable can be estimated via a
linear combination of several explanatory variables. PLSR has the combined features of regression analysis,
principal component analysis (PCA), and canonical correlation analysis. The PLSR method seeks to infer a linear
relationship between the explanatory and dependent variables, solving the problem of multi-collinearity while also
ensuring model stability. The general idea behind PLSR is to extract the orthogonal or latent predictor variables,
accounting for the maximal variation of the dependent variable. The PLSR method can be described as follows.
Let X be the input matrix comprising m explanatory variables with a total of n samples (Xn×m), and Y the
response variable involving n samples. Each sample has p independent variables. The dependent matrix Y has a
dimension of n×p. X is decomposed into a score matrix S and a loading matrix P via PCA. They have a dimension
of n×k and m×l (k, l = number of principal components for reconstructing X and Y), respectively. X and Y are
calculated as:
(4.3)
(4.4)
in which E1 (n × m) and E2 (n × p) = residual matrices, derived from the summary variance of k principal
components. Pm×l and Qn×l = loading matrices; S and U = component or factor matrix, representing the projections
of X and Y, respectively. Operationally, PLSR involves one inner equation (, B is an n×n regression coefficient
matrix determined via least squares minimization and two outer relations resulting from the decomposition of X
and Y, Figure 4.2). The inner equation is established via multiple linear regression between the score matrices S
and U, both having a dimension of n×l. It links the resultant eigenvalue matrices from these two eigen structure
decompositions X and Y. Of the two outer relations, the first is derived via PCA performed on X, resulting in the
eigenvalue matrix S (n×l) and the loading matrix P (m×l), plus an error matrix E1 (n×m) (Eq. 4.3). The second
outer relation is derived by decomposing Y into the score matrix U (n×l), the loading matrix Q (p×l), and the error
term E2 (n×p) (Eq. 4.4). The PLS model aims to maximize the covariance between X and Y via the inner relation
while minimizing the norm of F(n×p), the error term that is calculated by subtracting principal components from
Y. Y can then be estimated from B. The key to generating a model with a sound predictive power lies in the
specification of the optimal number of components. If a total of h components c1, c2, …, ch (h < n) are extracted
from X, and Y is expressed through regression over the original independent variables, the regression of y to the
components t1, t2, …, th is realized.4.2
FIGURE 4.2 Implementation of partial least square analysis that performs the regression according to both spectral
variables determined via genetic algorithms and in situ collected concentration values of the parameter to be
quantified. (Modified from Song et al., 2012, used with permission (5761090943395) from Springer.)
Owing to the use of the orthogonal score matrix S, model prediction capability is not affected by collinearity
between the explanatory variables. The decomposition of the explanatory variables enables the regression model to
be derived from a small sample (e.g., the components). PLSR elicits the directions of maximum input–output
cross-covariance. Therefore, it takes both input patterns and output variables into account. PLSR is particularly
suited to process hyperspectral data that offer tens, and possibly hundreds of spectral bands. The selection of the
most useful bands for quantification is a daunting task that cannot be realistically accomplished using multiple
regression analysis. The identification of the most spectrally sensitive bands and/or their transformed indices is
best accomplished using PLSR.
PLSR can be implemented as bagging PLSR, in which a number of models are trained in parallel, and each
model learns from a random subset of the data. Bagging PLSR is a bootstrap technique that leaves out about 37%
of the data in the course of resampling (Rossel, 2007). The bootstrap automatically calculates R2, adjusted R2,
RMSE, ME, RPD, and standard deviation of the error distribution (SDE). PLSR is suitable for application cases in
which the number of variables/bands far exceeds the number of samples.
NON-LINEAR NON-PARAMETRIC METHODS
In the presence of collinearity, the conventional statistical methods discussed previously can no longer meet the
demands of intensive computation imposed by multi-source data. The answer to this dilemma lies in machine
learning methods. Various machine learning algorithms can be used to construct the estimation models, all having
their own strengths, limitations, and best uses. The advantage of machine learning algorithms is their ability to
attach an importance value to every independent variable considered, thus avoiding the collinearity problem
among multiple variables. This is especially important when a multitude of variables is input to the computer in an
attempt to construct the most accurate model. The downside is the black box nature of the algorithm that fails to4.2.1
4.2.2
reveal how the decisions are made. The widely used algorithms in quantifying ecosystem parameters include
decision tree learning (e.g., random forest regression), support vector machine (SVM), random forest (RF), and
ridge (regulated) regression. Estimation of more complex attributes that are influenced by multiple factors, some of
which are impossible to known or describe mathematically, is best realized using more advanced algorithms, such
as Deep Neural Network (DNN), Long Short-Term Memory (LSTM), and Recurrent Neural Network (RNN).
Dissimilar to parametric models, non-parametric models do not involve any assumptions about data distribution or
relation between variables.
Decision Tree
Decision tree learning is based on decision tree predictive modeling. A decision tree is composed of a set of
hierarchically connected nodes. Each node represents a linear decision based on a specific input feature. In its
simplest form, a tree is binary with only two branches at each node established through a cyclic analysis of the
training dataset comprising the predictor (independent) and target variables. The algorithm searches for multiple
combinations of the independent variables, starting from the root node that encompasses the entire dataset. At the
next level of nodes, the dataset is partitioned into two groups (e.g., binary partitioning), corresponding to the two
branches. This partitioning process is terminated once the partitioned subsets are deemed sufficiently
homogeneous. In order to gauge the accuracy of the established tree, the available dataset is usually divided into
two parts, training and validation. The training dataset is used to construct the tree, and the validation dataset is
used to check its predictive power with unseen samples. A classical decision tree algorithm cannot cope with
strong non-linear input–output transfer functions. In this case, a combination of decision trees can improve results.
Decision trees can appear in various forms, the two most popular forms are bagging decision trees and Random
Forest (RF). The former is an early ensemble method based on building multiple decision trees by iteratively
replacing resampled training data and voting for the decision trees leading to a consensus prediction (Breiman,
1996). The RF approach applies a set of decision trees to improve prediction accuracy. It will be covered in depth
in Section 4.3.3.
Gradient-boosted Decision Trees
Gradient boosting is one of the most popular ensemble learning strategies for solving classification and regression
problems. A Gradient Boosting Decision Tree (GBDT) is an iterative algorithm comprising multiple additive
decision trees. The results of multiple decision trees are accumulated as the final prediction output. Based on the
boosting strategy, GBDT constructs a set of trees (learners) that are connected in series. All trees are trained
sequentially, during which each tree learns the mistakes (error residuals) of all the preceding trees in every
iteration and tries to minimize the error residuals. The weak learners are fit in such a way that each new learner fits
into the residuals of the previous iteration to improve the model performance. The final prediction model is formed
by aggregating many weak trees and is thus strong. It is worth noting that the insertion of a new tree or branch to a
model does not alter the trees it already has. The added decision tree fits the residuals from the current model. The
core of a GBDT is an additive model of the decision tree T(x,θ):
(4.5)
where x = input feature vector; θi = parameter of the ith decision tree, M = number of trees. The process of
constructing a GBDT model comprises five steps:
(i) Model initialization with a constant value:
(4.6)
for m = 1 to M; y = label of x.
(ii) Error residual (r) calculation:(4.7)
for i = 1, 2, …, N (N = number of samples).
(iii) Regression tree training with features x against r and creation of m terminal nodes Rjm (j = 1, 2, …, Jm);
(iv) Computation of gjm
(4.8)
for i = 1, 2, …, Jm
(v) Model updating
(4.9)
Of the five steps, the most important is step 2 (Liu et al., 2020). Error residuals are detected using various types of
loss functions, such as mean squared error (MSE) for regression tasks and logarithmic loss (log loss) for
classification tasks. Gradient boosting means optimizing the loss function to minimize loss and converge upon a
final output value based on gradient descent. Given a loss function L(y,p) = (y–p)2, where y is a label and p a
prediction, the pseudo response zi used to train the weak tree at step i is:
(4.10)
where Fi = prediction of the strong model. The gradient is expressed as:
(4.11)
For various loss functions L, GBDT uses the notion of the steepest descent, or the negative gradient of the loss
function to approximate the residuals, or:
(4.12)
Gradient boosting uses short, less complex decision trees instead of decision stumps. This technique enables the
optimization of an arbitrary differentiable loss function. Due to this sequential connectivity, boosting algorithms
learn at a very slow pace but are highly accurate. Learning rate refers to the speed at which the model learns. It
controls the pace of modification of the overall model induced by the addition of a new tree. The lower the rate,
the slower the model learns. A slow learning rate causes the model to be efficient and robust, but also prolongs
model training. Learning rate is inversely related to classification accuracy.
In statistical learning, models that learn slowly perform better. Thus, it is beneficial to optimize gradient
boosting using various methods. Of all gradient-based optimization algorithms, the stochastic gradient descent
family has become extremely popular in machine learning applications, mainly because computing the exact
gradient of a loss function is expensive, whereas computing a stochastic approximation of the gradient is much
cheaper (Mateo-García et al., 2018).
Gradient boosting differs slightly from Adaboost that produces the final output by weight-averaging all
individual outputs. Of various versions of gradient boosting, eXtreme Gradient Boosting is a highly efficient,
flexible, portable, optimized, and distributed algorithm, available in the Python XGBoost library (i.e., eXtreme
Gradient Boosting). This parallel boosting algorithm provides rapid and accurate parameter values quantified from
multispectral remote sensing data (Gafoor et al., 2022).
The performance of a GBDT depends on model parameterization. Some model parameters must be fine-tuned
to produce the ideal outcome. The number of parameters in a model requiring tuning can be as high as eight (e.g.,
number of leaves in one tree, depth of the tree, minimum number of data in one leaf, learning rate, number of
boosting iterations, fraction of data to be used for each iteration), and each parameter has its own dynamic range.
The number of trees in a model is known as the n_estimator. It is the most crucial parameter that must be
thoughtfully selected, as too many weak learners in the model may lead to overfitting of data. The hyperparameters4.2.3
or weights, learning rate, and n_estimator must be carefully tuned as they affect the performance and accuracy of
the model. The optimal performance of a GBDT is contingent upon the proper setting of these parameters
experimentally. The optimal combination of parameters needs hundreds of iterations to determine. In order to
expedite the training process, more trees are needed to train the model. However, too many trees can cause
overfitting that can be prevented via regularization (e.g., information is added to an objective function) penalties
on leaf weight values to slow down learning. There are two types of regularization, L1 and L2. The former avoids
overfitting by shrinking the parameters towards 0. The latter tackles overfitting by shrinking weights to a small
value, but not approaching 0. Regularization limits the minimum number of observations permissible in the
terminal nodes. Tree performance can also be enhanced by imposing constraints, such as the number of trees, tree
depth, minimum improvement in loss, and number of observations per split. The advantage of the GBDT method
is its robustness to outliers.
Random Forest
Among various machine learning algorithms, RF is considered one of the best in minimizing the inaccuracy of
modeling (Pourghasemi et al., 2020). Random forests are a bagging algorithm that integrates multiple decision
trees in parallel. Each tree is trained on a random subset of the same dataset and the results from all trees are
averaged to find the classification. A portion (e.g., 90%) of the input samples is used for training and the remaining
samples, called out-of-bag samples, are used for accuracy assessment. The core algorithm of RF is described as:
(4.13)
Where C1 and C2 = sample output mean of D1 and D2 datasets, respectively; A = division feature; s = division
point, yi = the ith sample.
As an assemble machine learning algorithm, RF classifies the input data using multiple decision trees. They
iteratively resample the input data based on the training dataset, from which n bootstrap samples are created. Each
of them is used to establish an original (e.g., unpruned) tree, and applied to the input data. A random set of
evidence features is selected to predict the output each time. The outputs from all the n trees are weighted by the
votes they receive, and the weighted average is output as the final outcome. There are different voting strategies,
one of which is majority voting based on the outputs of the estimated trees converging to a single tree. In
implementing RF classification, a portion of the input samples is reserved for validation. These out-of-bag samples
are used to determine prediction accuracy and feature importance, but not for tree growth. Apart from the number
of out-of-bag samples, three more training parameters must be specified in constructing a tree: the total number of
trees (ntree), the number of predictors at each split node (mtry), and the minimal size of the terminal nodes in the
tree (nodesize). It is crucial to specify an appropriate value for ntree and mtry to achieve acceptable classification
outcomes. A small ntree value causes the results to deteriorate considerably. Conversely, a value higher than the
optimal threshold makes the computation unnecessarily costly without the ability to improve the classification
accuracy notably. Similarly, the use of a small mtry value debilitates the predictive power of each tree due to the
insufficient predictor variables available. The determination of the optimal values for these two parameters may
require experimentation with a range of values.
The RF regression predictor is expressed mathematically as:
(4.14)
Where x = the input data vector comprising various evidential features; K = the number of decision trees. Each of
the K trees “plants” and “grows” in three main steps:
i. Partitioning of the training dataset into N equal samples via bootstrap sampling with reset;
ii. Selection of m variables from the M input variables (m<M) for each node in the tree. The selected variables
are used to determine the best split point, and grow m trees. Each decision tree is allowed to grow as deep as
possible without pruning; and4.2.4
iii. Use of the entire dataset for prediction by summing the results from all trees using majority voting in
classification and averaging in regression.
RF has a superior performance to that of a single weak model because it makes use of the integrated learning
method by training multiple weak trees to form a strong model. It can produce highly accurate predictions without
suffering the overfitting problem (e.g., able to produce an accurate prediction from the input based on samples
only, but having a poor ability to generalize the unseen data). The success of RF in classification lies in the
derivation of the high variance from different decision trees.
Before this section ends, it is important and valuable to compare the pros and cons of different non-parametric
methods (Table 4.2). The strengths include the ability to use the full set of spectral bands, and the models have a
high degree of adaptability. In general, non-parametric methods are efficient once properly trained, and can handle
data redundancy quite well. They can yield multiple outputs, with importance attached to the considered variables.
In addition to predictions, they also supply uncertainty estimates of the predictions. The drawbacks of these
methods include intensive computation that may lead to overfitting. To properly train a model may require expert
knowledge, and the trained model may be sensor-specific, so cannot be used for data obtained from another sensor.
Besides, they need a huge amount of field data to be trained properly. They do not reveal how the decisions are
made, so are almost impossible to improve.
Table 4.2
Strengths and weaknesses of non-parametric regression methods in quantitatively retrieving the physical and chemical parameters of the
environment from remote sensing data
Strengths Drawbacks
• Full-spectrum methods able to make use of the
complete optical spectral information
• Advanced, adaptive (non-linear) models
• Methodologically accurate with possible robust
performance
• Some MLRAs cope well with data redundancy and
high noise levels
• Once trained, imagery can be processed time
efficiently
• Yielding insights into model development (e.g.
GPR: relevant bands; decision trees: model structure)
• Possible multiple outputs (e.g. PLRS, ANN, SVR,
GPR and KRR)
• Some MLRAs (e.g. GPR) supply uncertainty
information
• Potential of computationally demanding training
essential
• Risk of generating over-complicated models with
compromised generalization ability
• Sensor-specific models hard to be directly transferable
to other sensors
• Difficult (or even impossible) to train with a large
number of samples
• Expert knowledge necessary for tuning, but eased by
toolboxes
• Some regression algorithms elicit instability with
datasets having statistics deviating from the training
datasets
• Field data vital (i.e. measurement campaigns are
necessary)
• A large sample (typically > 1,000,000) needed for
model training
• Mostly having the flavor of being in the “black boxes”
Source: Modified from Verrelst et al. (2015), with permission (5761090131500) from Elsevier.
Artificial Neural Networks
Artificial neural networks (ANNs) in their basic form are essentially fully connected artificial neurons (AN)
organized in various layers. An ANN is formally defined by three structural entities: (i) The interconnection pattern
between the different AN layers; (ii) The learning process that updates the weights of the interconnections, and (iii)
The activation function that converts the ANs weighted input with the output activation. This machine-learning
paradigm represents an effort to mimic the human brain in its decision-making. An AN is virtually a pointwise4.2.4.1
non-linear function applied to the output of a linear regression. ANN is able to approximate a set of input data to
the corresponding output. ANs basically perform a linear regression followed by a non-linear function f(•), such as
the Sigmoid or Gaussian function. The dependent variable is accurately predicted from the constructed model with
the assistance of training samples fed to it. If an ANN encompass only one AN, it produces results similar to or
only slightly better than those obtained with linear regression.
The many layers in an ANN are called Multi-Layer Perceptron (MLP). Unlike other statistical approaches,
MLP does not make assumptions about data distribution, and can model multivariate, complex, and nonlinear
relationships. An MLP consists of a set of nonlinear computational elements (i.e. ANs) that are interconnected in a
feed-forward way, so that each AN in a layer is only connected to the ANs in the immediate layer. Each connection
is defined by a set of weights. Structurally, a typical MLP comprises an input layer, one or more hidden layers, and
an output layer. The input layer contains all predictor variables, and the output layer generates the predicted
outcome. The input layer only distributes the input signals into the network but does not process them. This task is
performed by the ANs in the hidden layers and the output layer that transform the input signal according to the
activation function. Each node in the hidden layer performs two tasks of summation (summing weighted input
from the previous layer) and activation. All inputs to the output nodes are summed and passed to their counterparts
in the next layer via the non-linear activation function (e.g., Sigmoid or s). The nodes in the output layer denote the
outcome of prediction based on the output from the hidden layer(s). The strength of a connection (or weight) is
constantly adjusted during training so as to reach the best match between the observed value and the desired
outcome.
In a standard multi-layer ANN model, neuron j in layer l+1 yields the following output:
(4.15)
where = weight of the connection between neuron i in layer l and neuron j in layer l+1; = bias of neuron j in layer
l, and f = non-linear activation function. The output (prediction) of the model for sample xi is denoted as f(xi).
During network training, several parameters must be properly specified to prevent overfitting, including network
structure (number of hidden layers and nodes per layer), proper initialization of the weights, shape of the
nonlinearity, learning rate, and regularization parameters (Verrelst et al., 2012a). Depending on the manner of data
(or error) propagation in the network, ANNs can be classified into various forms, such as feed-forward,
backpropagation, recursive, and general regression (Figure 4.3), all of which are introduced below.
FIGURE 4.3 Comparison of the differences between feed-forward (a) and backpropagation – BP (b) artificial neural
networks. Empty circles: input nodes; grey circles: hidden nodes; dark circles: output nodes. Solid arrows: direction
of data flow; dashed arrows: direction of error propagation. (Gao, 2023.)
Feed-forward Neural NetworkFeed-forward ANN is the most common network configuration that allows data to pass from the input nodes to the
output nodes through the hidden nodes. It is a form of supervised learning that forwards data in the network
without loops. The training samples fed to the input nodes pass the network through the hidden nodes that generate
an output from the received set of weighted inputs via a user-specified activation function (f). In feed-forward
networks, data always flow from the input layer to the hidden layer, and ultimately to the output layer
unidirectionally (Figure 4.3a). The output (y) and the input (x) conform to the following relationship:
(4.16)
Lacking the feedback mechanism from the output to the input nodes, this kind of network is able to handle only
linear relationships through the linear learning discriminants. A set of weights (wj) and biases (b) associated with
each node must be known, usually determined during network training.
The simplest feed-forward neural network (FFNN) is composed of three layers and may be trained by a back￾propagation algorithm iteratively. Suppose there are n input xi (i = 1, ..., n), one output neuron y, and k neurons in
the hidden layer whose output is expressed as zj (j = 1, ..., k). The input X (x1, x2, ..., xn) is distributed to neurons in
the hidden layers. FFNNs are created by selecting distinct pairs of samples, , from a given training set of N input
vectors with the corresponding N output values . The desired output yi is modeled from the input xi via the
activation function of the neuron in the hidden layer denoted as fh, while the neuron activation function in the
output layer is fo (Warsito et al., 2018). The output (y) of the FFNN is expressed as:
(4.17)
where = activation function on the hidden layer j. If a bias is added to the input layer and the activation function of
each neuron in the hidden layer is fh, then Eq. 4.17 can be rewritten as:
(4.18)
Where = weight from bias to the output, and = weight from the bias to the hidden layer.
FFNN has been modified to form the cascade-forward neural network (CFNN) in which each neuron in the
input layer is connected to nodes in the output layer (Warsito et al., 2018). Similar to FFNNs, CFNNs contain
connections between each neuron in the input layer and each neuron in the hidden layer, but they also include
additional links between neurons in the input layer and all neurons in the output layer. Consequently, the network
weight to be estimated increases as much as the neurons in the input layer. The output y is expressed as:
(4.19)
where = activation function from the input layer, and = weight from the input layer to the output layer. If a bias is
added to the input layer and the activation function of each neuron in the hidden layer is fh, then Eq. (4.19)
becomes:
(4.20)
As with FFNN, the backpropagation (BP) algorithm on CFNN also consists of three stages: (i) feed-forward of the
input dataset; (ii) error calculation (the difference from the output to the target); and (iii) weight adjustment by
updating the weights and recalculating them. This step is terminated if no errors arise or when the number of
iterations reaches the pre-defined criterion (Warsito et al., 2018).
In BP networks, the differences (error) between the predicted and observed values at the output nodes are fed
back to the input nodes (Figure 4.3b). A BP neural network has a feed-forward topology and a supervised learning
algorithm. Data flow forward from the input nodes to the output nodes via the hidden nodes just as with the FFNN.
After the training samples are presented to the input nodes, they propagate forward in the network, based on which
an output is generated via randomly assigned connection strengths or weights initially. The produced prediction is
then compared with the desired output. Their discrepancy or the error signal is subsequently propagated backward4.2.4.2
from the output nodes to the input nodes (dashed arrows in Figure 4.3b) iteratively. In each iteration, the synaptic
strength of connection or weight between two nodes is continuously adjusted until the predicted and observed
values reach an agreement with each other that is deemed satisfactory (e.g., within the defined tolerance
threshold).
Owing to its self-adapting and self-organizing abilities, ANN is best at processing a huge volume of remote
sensing data and auxiliary features. It outputs the results that best match the given input (e.g., training samples)
through the self-learning process, independent of any particular functions, or involving any assumptions regarding
the data distribution. To a large degree, the performance of an ANN depends on how it is trained by presenting
inputs and corresponding outputs to it. The manner of network training suggests that it runs the risk of overfitting
the network in that it may be able to produce a good prediction based on the training samples, but its ability to
generalize the rules for the unseen data is compromised by producing a large error (i.e., unreliable prediction).
DNN and Long Short-term Memory
Deep learning or deep neural network (DNN) is a family of machine learning algorithms that use multiple hidden
layers to progressively extract high-level features from the input. A DNN is a supervised training method with a
feed-forward network structure that utilizes error BP to determine the weight and bias of each hidden node. The
number of hidden layers typically exceeds three, and a typical DNN is composed of n time-hidden input nodes (n =
number of hidden layers). The structure of the DNN influences the performance of the estimation model. DNNs
have several forms, such as recurrent neural network (RNN), convolutional NN (CNN), and long short-term
memory (LSTM). RNNs are a class of neural networks that use a recurrent structure. They operate on variable￾length input sequences and map to variable-length output sequences, for example, to map from an image to various
sentences that describe that image. This capability is achieved by sharing parameters and transformations over time
(Figure 4.4), in which ht is generated from ht-1 and other input variables (Eq. 4.21):
FIGURE 4.4 A sketch illustrating the two-time computation steps of a simple RNN with the bias (bh) omitted for
simplicity. Weight parameters Whh and Wxh are shared over time. The computation graph can be unrolled
indefinitely. *- matrix multiplication (with implicit ordering assumed, e.g., Whhh0, not h0Whh). (Modified from
DiPietro and Hager, 2020, with permission (5764680514360) from Elsevier.)
(4.21)
Where xi = input variables; the initial hidden state is omitted, and, unless otherwise specified, is often assumed to
be h0 = 0.
CNNs belong to the class of representation learning methods which automatically extract necessary features from
raw data without the need for any handcrafted features. They are effective at analyzing images graphically, and
have found few applications in quantitative remote sensing, so are not discussed further. Instead, LSTM is good athandling time-series data particularly suited for processing multi-temporal remote sensing data. Thus, it requires
true values such as in situ measurements for predictions.
LSTM is developed to tackle the vanishing gradient problem in recursive NN (RNN) and has become one of the
most popular RNN architectures so far. LSTM has a stronger memory capability and can better capture temporal
correlation in long-sequence data than traditional RNN models. It is particularly suited to process time-series data,
such as multi-temporal satellite data in crop yield estimation. Dissimilar to simple RNN that yields an exact path
between time T and time T-t, with each link inhibited by linear transformations and non-linearities, LSTM creates
exponentially more paths between the time-series nodes (Figure 4.5), with one path free from linear
transformations and non-linearities (DiPietro and Hager, 2020).
FIGURE 4.5 Comparison of a FFNN structure with that of a RNN in which function f and its parameters θh are shared
over time. (Modified from DiPietro and Hager, 2020, with permission (5764680514360) from Elsevier.)
Architecturally, a typical LSTM consists of one input layer, one or more LSTM layers, and an output layer that can
learn time-sequential information (e.g., crop growth cycle). The LSTM layers are composed of LSTM cells. Each
cell has three gates: the input, the forget, and the output. The input gate (it) controls the type of information that
can enter the current state through the input nodes, and then calculates the new state based on the current state and
the newly received input information. Specifically, the input of the LSTM consists of the input at the current
moment and the state at the preceding moment. The input gate also decides the type of input information to be
retained. The forget gate (ft) controls the type of information that can be forgotten and determines the portion of
the previous input to be retained to prevent it from interfering with the subsequent computation. The output gate
(ot) generates the final output by merging the output from the previous cells with the current input (Cheng et al.,
2022). These gates have a state lying between fully off (0) that shuts down information flow and fully on (1) that
permits completely free flow of information. This gating mechanism controls the type of information that can pass
through certain parts of the network. It alleviates the problems of gradient disappearance and gradient explosion
and the long-term dependency problem of traditional RNNs, thereby making the training of LSTM more stable.
Key to the LSTM networks is the activation function f for the three types of gates. It can be Sigmoid (σ), tanh,
and ReLU. The Sigmoid function restricts the output to between 0 and 1. Mathematically, the transition functions
at these gates are expressed as follows:
(4.22)
(4.23)
(4.24)
where xt = input at time t; b = bias; W = weight of a node or neuron, s = Sigmoid activation function; ht = state of
hidden nodes at time t, calculated as:4.2.4.3
(4.25)
where = element-wise multiplication (i.e., Hadamard product); = state of memory cell at time t, calculated as:
(4.26)
where = nonlinear transformation for better representing xt, calculated as:
(4.27)
Of special notice is that all three types of gates highly resemble simple RNNs, and are updated similarly with only
the activation function being unique. The formation of the new neuron (memory cell) is at the core of alleviating
the vanishing gradient problem. There is only one path between ct−1 and ct that is modulated by the forget gate. Of
the exponential number of paths between ct−t and ct, only one corresponds to elementwise multiplications by the
forget gate. The additive gradient component itself is the product of diagonal Jacobians with diagonal elements
corresponding to the forget gate. Thus, gradient contributions can still decay exponentially with τ, but if the forget
gate has close-to-1 elements, then the base of the exponential decay is also close to 1 (DiPietro and Hager, 2020).
In practice, the bias (bf) of the forget gate is initialized to be positive (e.g., 1 or 2) at the beginning of network
training.
The success and efficiency of LSTM depend on the proper configuration of key network parameters, including
the number of LSTM layers, the number of neurons in each layer, and how they are connected. Key running
parameters are dropout rate, regularization, learning rate, batch size, and epoch (Table 4.3). They must be carefully
parameterized to minimize the risk of overfitting and improve the model’s generalization efficiency.
Table 4.3
Range of hyperparameter values adopted to optimally configure a deep-learning model used for estimating ground-level PM2.5
Parameter Hyperparameter value
Number of hidden nodes 64 128 256 512 1024
Number of hidden layers 3-4 4-6 4-6 6-8 6-8
L1 regularization False, 0.01, 0.001, and 0.0001
L2 regularization False, 0.01, 0.001, and 0.001
Activation function ReLu, Leaky ReLu, and exponential linear unit
Optimization Adam and root mean square propagation
Learning rate 0.05, 0.001, and 0.005
Dropout rate 0.1, 0.2, and 0.3
Source: Lee et al. (2021), open access.
There are many variants of LSTM, one of which is ConvLSTM. It is a type of RNN for spatiotemporal predictions
that has a convolutional structure in both the input-to-state and state-to-state transitions. ConvLSTM usually makes
use of sliding window to process temporal data. The size of the sliding window is adjustable, and once determined,
is used to partition the temporal data into several sliding windows. Each sliding window contains a sub-sequence
of a fixed size, so that the whole sequential data can be decomposed into several short sequences, each of which is
processed by a ConvLSTM to obtain the corresponding output. With the sliding window mechanism, ConvLSTM
can process temporal data of any length, extract useful information from it, and generate the results via the output
gate from the current state and input information.
General Regression Neural Networks
General regression neural networks (GRNN) is a generalized radial basis function and probabilistic neural network
able to perform regression, prediction, and classification based on Gaussian kernels. This one-pass learning4.2.4.4
algorithm has a highly parallel structure, representing an improved neural network of radial basis neurons
grounded on non-parametric regression by approximating the map inherent in any sample dataset. GRNNs do not
require iterative training as the functional estimate is computed directly from the training dataset (Specht, 1991). A
general architecture of the GRNNs is made up of four layers of input, pattern, summation, and output. The network
must be trained using the training samples, during which the Euclidean distance between the input vector X and
the ith training input vector Xi is calculated as:
(4.28)
where = squared Euclidean distance between the input vector X and the ith training input vector Xi. It is
multiplied by Yi, the output vector corresponding to Xi, to predict Yi´ as:
(4.29)
where Y(X) = estimation corresponding to X, n = number of samples, and δ = smoothing parameter that controls
the size of the receptive region. The training of GRNNs virtually optimizes the smoothing parameter δ because the
structure and weights of the GRNNs are governed by the input. The GRNNs prediction power is significantly
influenced by δ that can be determined using the holdout method. In this method for a particular δ, one sample is
removed from the training data at a time and all the remaining training samples are then used to construct GRNNs
(Jia et al., 2015). The training process is terminated once the following cost function of the smoothing parameter
reaches minimization:
(4.30)
where (Xi) = estimation corresponding to Xi using the GRNNs trained over all of the training samples minus the
ith sample. The optimal smoothing parameter of the GRNNs may be obtained using the shuffled complex
evolution method. It is not susceptible to being trapped by small pits and bumps on the function’s surface. GRNN
enjoys several advantages. For example, it still achieves highly accurate estimates even though it uses single-pass
learning without the need for backpropagation owing to the use of the Gaussian kernel. It is robust even in light of
noisy, sparse input data. The main disadvantage of GRNN is its huge size that makes it computationally expensive.
It cannot be improved via optimization.
Extreme Learning Machine
FFNN has been improved using various schemes based on the fact that all the network parameters have some
degree of dependency between them. One of them is gradient descent-based learning algorithms such as
backpropagation. In general, gradient descent-based methods are very slow learners caused by improper learning
steps or easy convergence to local minima. Satisfactory learning requires many iterative steps. Another is extreme
learning machine (ELM) that resembles a FFNN in architecture. It contains an input layer, a single hidden layer,
and one output layer. The differences between FFNN and ELM lie in the use of random nodes and the random
assignment of input weights and hidden layer biases of single-layer FFNNs if the activation functions in the hidden
layer are infinitely differentiable (Huang et al., 2006). After the input weights and the hidden layer biases are
chosen randomly, single-layer FFNNs can be treated simply as a linear system, and the network-output weights
(linking the hidden layer to the output layer) can be analytically derived via simple generalized inversion of the
hidden layer output matrices. ELM is advantageous over other FFNN models in that the weights of the hidden
layer can be randomly produced and updated without complex iterative optimization, thereby improving
computation efficiency in model training and expediting network learning. Learning speed is exceptionally faster
(e.g., up to thousands of times) than traditional feed-forward network learning algorithms such as BP with a better
generalization capability. It avoids the weaknesses of the classic gradient-based learning algorithms, such as
minimum training error, but does not consider the magnitude of weights, local minimum, improper learning rate,
and overfitting. The ELM learning algorithm is much simpler than most learning algorithms for FFNNs. It tends to
converge not only at the minimum training error but also have the smallest norm of weights. Dissimilar to the4.3
classic gradient-based learning algorithms which are functional only with differentiable activation functions, the
ELM learning algorithm can be used to train single-layer feed-forward networks (SLFN) with non-differentiable
activation functions.
For N time-series samples (xi, ti), where and , a standard SLFN with M hidden neurons is mathematically
modeled as:
(4.31)
where g(x) = activation function, , the matrix of weight for the link between the input neuron and the ith hidden
neuron. denotes the weight matrix connecting the ith hidden node with the output neuron; bi = bias of the ith
hidden neuron; oj = output matrix. = inner product of and . The N samples can be approximated by the standard
SLFN with a zero error so that if , and meet the following criterion:
(4.32)
It can be rewritten compactly as:
(4.33)
where
The cost function used by the ELM model for a hidden node is expressed as:
(4.34)
where xi = input, y = output, and i = predicted output. = weight vector, and bj = bias of hidden node j, and h(•) =
non-linear activation function. The j output matrix (βj) represents the output weight for the link between hidden
node j and the output node. The formula for computing β is expressed compactly as:
(4.35)
Accordingly, the estimated output value for the ith input is calculated as:
(4.36)
where λ = user-defined constant, H = hidden-layer output matrix of the network, Y = desired matrix to be output,
and I = identity matrix (Peterson et al., 2018). ELM codes can be found at
www.ntu.edu.sg/home/egbhuang/elm_codes.html.
Theoretically, this algorithm has a good generalization capability and fast learning rate. Practical experiments in
very large complex applications, in which some functions are approximated, have confirmed that ELM has a sound
performance in most cases at a learning rate thousands of times faster than conventional feed-forward networks
with the smallest training error and norm of weights (Huang et al., 2006). ELM has reportedly surpassed other
widely used methods such as PLSR and support vector machine (SVM) (see Section 4.4.1 for details), possessing a
high predictive power while being significantly less computation-intensive in handling complex spectral
interactions.
KERNEL-BASED METHODS
Kernel methods in machine learning owe their name to the use of kernel functions to calculate similarities between
input samples. Similarity reproduces a linear dot product (scalar) computed in a feature space of a possibly higher
dimension regardless of data location in the feature space. A kernel function is defined as a real-valued function of4.3.1
two arguments, . Typically, it is symmetric and non-negative, so can be construed as a measure of similarity
(López et al., 2022). Several kernel-based algorithms have been developed to retrieve biogeophysical parameters
from remotely sensed data. Among them are support vector regression (SVR), kernel ridge regression (KRR),
Gaussian processes regression (GPR), and Bayesian networks. They are all non-parametric models that do not
make any assumptions about data distribution or relation between variables. They all learn how to make
predictions using kernels in the form of:
(4.37)
Where = bias that is often omitted for clarity and brevity; xi (i = 1, 2, …, N) = input data of N observations;=
weight assigned to observation i; = kernel function that evaluates the similarity between the test dataset x and all N
training samples, xi.
Support Vector Machine
The origin of SVM can be traced to the maximum margin classifier, a simple and elegant method for generating
binary outputs by assuming that the input observations are linearly separable. Owing to this assumption, it cannot
be used to classify many datasets. It has been extended to form the support vector classifier that allows some of the
training samples to be misclassified, thereby creating a reasonable narrow band along the separable linear
boundary that is more robust to overfitting. The space between the upper bound and the lower bound is known as
the margin (Figure 4.6a). It should be maximized to produce the best classification outcome. Generally, the wider
the functional margin, the lower the generalization error of the classifier. As a generalized support vector classifier,
SVM separates the input samples into groups enclosed by non-linear boundaries by expanding the feature space
with the assistance of kernels. This kernel-based algorithm offers a sparse solution, since the new inputs are
predicted by evaluating the kernel function in a subset of the training samples. The estimate of the model
parameters corresponds to an optimized convex, which means that the solution is always optimal globally.
FIGURE 4.6 Maximum margin hyperplane (dashed line) with two separable classes. The margin is defined as the
distance from the dashed line to any point on the solid line. The support vectors are the dots from each class that
coincide with the maximum margin hyperplane and each class must have at least one support vector. The two
classes are circles and triangles in (a), and green and blue dots in (b). (Modified from López et al., 2022, open
access.)SVMs are supervised learning models with associated learning algorithms. They analyze data and perform
pattern recognition. A SVM constructs a hyperplane or a set of hyperplanes in a high or even infinite dimensional
space. A liner hyperplane can be defined as:
(4.38)
Where p = dimension of the feature space formed by X points totaling p. They are grouped in two dichotomous
classes, , and ; (i=1, 2, … p) denotes the weights to be determined using the training samples, from which the
maximum margin classifier is constructed as:
(4.39)
Where = intercept. Once (, ) are estimated, they can be used to predict the output of a test observation containing 
There are two outcomes of prediction, class -1 if >0 or class 1 otherwise.
SVM faces the issue of how to set up the hyperplane in such a way that it can separate the training samples
perfectly but also classify unseen observations with a minimum error. The answer is to use the maximum margin
classifier that attempts to maximize the distance between the upper and lower bounds, M (Figure 4.4b), or:
(4.40)
subject to and ) ≥ M (i=1, 2, …, n); = output with a value between -1 and 1. This equation is solved via
optimization in the form of:
(4.41)
where [i = 1, 2, …, n, = (, ,…, )T] = auxiliary non-negative variables known as the Lagrange multipliers. The
derivatives of against and equal to 0 lead to the following conditions:
(4.42)
(4.43)
(4.44)
These affine constraints are imposed to ensure that the hyperplane is convex to guarantee a local (e.g., within the
kernel) optimum in the estimation. Eq. 4.42 indicates that the beta coefficients (except ) of the maximum margin
hyperplane are a linear combination of the training vectors x1, x2, …, xn. Vector xi belongs to that expansion if, and
only if, αi ≠ 0 and these vectors are called support vectors (Figure 4.6a). Plugging of Eqs. 4.42 and 4.43 into Eq.
4.41 results in the following dual optimization problem for the maximum margin classifier after simplification:
(4.45)
subject toand = 0 (i = 1, 2, …, n).
where xi . xj = dot product of vectors xi and xj. The optimal solution for the maximum margin classifier is obtained
via standard quadratic solvers as L(α) is a quadratic function of α. Once α is known, the weights (coefficients) can
be calculated and yi estimated (López et al., 2022).
The previously presented method has to be modified for non-linearly separable data to form the soft margin
classifier (Figure 4.7). It allows some observations to fall on the wrong side of the hyperplane by using a slack
variable (. It relaxes some constraints and measures the distance by which vector xi violates the established
inequality of . Thus, those observations lying within are correctly classified by the hyperplane as they are not
outliers. The outliers can fall not only on the wrong side of the hyperplane but also on the wrong side of the
margin. In order to ensure that most of the training samples can be correctly classified into the dichotomousclasses, the maximum margin hyperplane must be found via solving the following optimization problem, under the
constraints of:
FIGURE 4.7 Soft margin support vector machine in non-separable data training. Dots with 0 < ζi < 1 or not labeled
are correctly classified, while those with ζi > 1 are on the wrong side of the decision boundary and incorrectly
classified. (Modified from López et al., 2022, open access.)
where t = non-negative tuning parameter indicative of the total allowable errors. This hyper-parameter needs to be
tuned via cross-validation; M = margin width that should be maximized. Since this classifier is based solely on a
small fraction of the training observations (support vectors), it is quite robust to the classification of new
observations that are far away from the hyperplane (López et al., 2022).
If the input data are non-linearly separable in a low-dimension feature space, they can still be separated in a high
dimensional space via non-linear transformation of the data. The training samples may be transformed into a
feature space using a non-linear polynomial kernel. In enlarging the feature space using kernels, the support vector
classifier is extended to become an SVM, which is made possible by substituting the inner product essential in
solving the dual optimization problem for the support vector classifier with the kernel function . SVM offers the
solution to the following optimization problem:
(4.46)
under the constraints of and = 0 (i = 1, 2, …, n).4.3.2
Table 4.4
Four common types of SVM kernel function K(x, y)
Function Formula
Linear xT × y
Polynomial [xT × y + 1]d
Radial-based function
Sigmoid Tan h(xT × y + b)
The training samples in the training dataset are classified based on the sign of ; they are assigned to the -1 group if
f(x) <0 or to the group of 1 otherwise (James et al., 2023). The constructed non-linear SVM regression function
takes the following form:
(4.47)
where N = number of support vectors lying on the marginal hyperplane, () = Lagrange multipliers (i = 1, 2, …, n),
b = bias (a constant), K() = kernel function expressed as the dot product of mapped examples , via which the value
of the target parameter is estimated. It plays a decisive role since it implements the notion of similarity between
data points. Typical kernel functions fall into four types: linear, polynomial of various orders, (Gaussian) Radial￾Based Function (RBF), and Sigmoid (Table 4.4), all of which affect the candidate variables to be selected from a
large pool. The selection of each variable is based on the consideration of whether it can minimize the RMSE and
maximize the coefficient of determination (R²) of the estimation model to be established. The selection of a given
kernel function affects the generalizability of SVM.
SVMs are effective when the number of independent variables exceeds the number of observations, but they
can be computationally expensive at times. SVMs do not perform well with noisy datasets. The actual performance
is subject to two critical parameters: the penalty parameter (C) and the residual (ε), both of which are estimated
from a kernel. Therefore, three parameters must be fine-tuned: the regularization parameter C, the tolerance value
ε, and the kernel parameter s (for RBF only). The C value dictates SVM success and effectiveness. Whatever value
is selected for C, it always represents a compromise between the complexity of the adjustment function and the
tolerance of the empirical material. A larger C value requires more training and leads to an over-fitted model. The
other kernel parameters of d, σ, and b (Table 4.4) affect the efficiency of SVM, together with C for the error. In
particular, σ controls the shape of the clustering hyperplane, and hence the overall classification accuracy. Both C
and σ can be set via optimization. They may be optimized via cross-validation.
SVM can be implemented using the R package e1071 (R Core Team 2018) with linear, polynomial, Gaussian,
and Sigmoid kernels. Since the input data are mapped onto a high-dimensional feature space, SVM is highly suited
to handle a huge set of variables. It is particularly strong at addressing the non-linear relationship between the
dependent and independent variables. The other strengths of this algorithm include its global optimality, the
capability of generalization, simplicity, ease of calculation and implementation, and high learning efficiency. It is
particularly good at handling the smallest training set error common in the traditional classification method.
Support Vector Regression
Inspired by SVM for binary response variables, Support Vector Regression (SVR) is an analytical method for
exploring the relationship between one or more predictor variables and a real-valued (continuous) dependent
variable. The main concept behind SVR is to use residuals smaller in absolute value than some constant (called ε￾sensitivity), that is, fitting a tube of ε width to the data. It is a tolerant regression model that creates an “interval
band” with a spacing of e (the tolerance bias) either manually set or using an empirical value on both sides of the
hyperplane. Any samples that fall into the interval band are excluded from calculating the loss (Awad and Khanna,
2015). A model is constructed via minimizing the total loss while maximizing the interval. SVR is sensitive to the
chosen hyperparameters.4.3.3
As a successful alternative to ANNs, SVR uses a particularly robust cost function, and adopts additional linear
constraints. It defines a linear prediction model over mapped samples to a much higher dimensional feature space,
which is non-linearly related to the original domain of input features. Given a set of input–output pairs, , in the
training sample, the relationship between yi and xi in the SVR prediction model is mathematically expressed in a
form resembling the forward model as:
(4.48)
where ϕ is defined as xi → ϕ (xi). The standard SVR formulation uses the ε-insensitive cost function, in which
errors ei up to ε are not penalized, and all further deviations incur a linear penalty. Briefly, SVR estimates weights
w by minimizing the following regularization function, the core algorithm of SVR:
(4.49)
where w, b = model parameters, m = number of samples, ξ = relaxation variable with respect to w; and = three
constraints:
(4.50)
where = positive slack variables to handle training samples with a prediction error > e (ε > 0). C, the penalty
parameter applied to them, controls the trade-off between error minimization and the regularization term, thus
controlling the model generalization capability. SVRs are resolved through the linear restrictions in Eq. 4.50 that
are incorporated into Eq. 4.49 using Lagrange multipliers αi, computes the Karush-Kuhn-Tucker conditions, and
solves the dual problem using standard quadratic programming procedures, which yields the final solution (Eq.
4.47). The model is very fast to train and easy to use.
Kernel Ridge Regression
KRR is a simple yet powerful non-linear regression for predicting environmental parameter values. It solves a
regression problem by defining a least-squares cost function with regularization and by using the kernel similarity
function. Time-series non-linearly transformed remote sensing data are mapped out to high dimensional space
from a low dimension. Also known as least squares SVMs, KRR minimizes the squared residuals in a higher
dimensional feature space, and can be considered as the kernel version of the regularized least squares linear
regression. It belongs to the supervised learning family for data analysis, able to provide solutions to classification
and regression problems.
The kernel function of KRR is a feature map from d-dimensional Hilbert Space in such a way that . This
function becomes a kernel matrix with an input sample dataset of (xi, yi) (i = 1, 2, …, N) X• Y (yi = target
parameter value corresponding to xi to be estimated) (Ahmed et al., 2022):
(4.51)
The KPP problem is formulated as:
(4.52)
where Y = target matrix of n samples, w = unknown weight matrix to be determined as:
(4.53)
where In = identity matrix of N × n in dimension; λ = regularization item (λ ≥ 0 avoids a large range of w). KRR
offers the solution to the regression problem by minimizing the regularized squares loss function according to the
following formula:4.3.4
(4.54)
According to the Representer Theorem, the prediction function (f*) can be represented as an additive function of
k(xi, x) in the following form (Mateo-García et al., 2018):
(4.55)
where = weight matrix of N × 1 in dimension to be optimized; X = matrix of the N input sample values; = kernel
function matrix of 1 × N in dimension. The objective function f* in Eq. 4.54 can be rewritten in the matrix form as:
(4.56)
Where Y = target variable matrix comprising n elements in X; C = regularization parameter. It is set to C ≥ 0 to
avoid a large range of the unknown weight vector w. The weight that minimized J(f*) is calculated based on Eq.
4.53. Thus, weight calculation becomes computation of similarities between the training and testing samples via
kernel functions (Eq. 4.51).
The KRR prediction model is exactly the same as that of SVR, obtainable from the inversion of the kernel
matrix K with a dimension of n × n regularized by 1/λ. Therefore, in KRR only the regularization parameter λ and
the kernel (e.g., RBF kernel) parameters need tuning or optimization via cross-validation. The main advantage of
KRR is that the solution can be expressed in closed-form, without the need for the quadratic programming
procedure (Verrelst et al., 2012a). The major shortcoming is that the model is not sparse: all examples used for
training have a weight wi in the final solution. KRR is extremely efficient at handling high-dimension data since
the solution is independent of the input dimension. However, it requires the inversion of a matrix whose size grows
with the number of training samples, as illustrated in Eq. 4.53. In this case, a subset of the whole dataset may be
used, with the training procedure modified accordingly by basing parameter optimization on stochastic gradient
descent instead of the classical grid search (Mateo-García et al., 2018). This modification allows optimization of
complex models with a large number of parameters. The basic functions in the reproduced kernel Hilbert space are
defined by parameters to be optimized during the training process. This modification can extend the number of two
free parameters in the standard KRR with an RBF kernel to potentially millions of samples. The modified KRR
has a better performance than the classic version in predicting surface temperature from MetOp-IASI hyperspectral
infrared sounding data.
Gaussian Processes Regression
Recently, a new statistical method has emerged in the family of non-parametric statistical algorithms named
Gaussian Processes Regression (GPR). It generalizes Gaussian probability distributions in a function’s space. GP
builds a nonlinear regression as a linear combination of spectra mapped to a high-dimension space. Non￾parametric GP approaches provide flexible non-linear regression models that can jointly use all spectral bands as
the input. A GP is stochastic and describes the properties of functions as Gaussian distributions. This machine
learning approach learns the relationship between the input (e.g., spectra of multiple bands) and the output (e.g.,
the biophysical parameters of interest) by fitting a flexible model between them directly. The hyperparameters of
the model are typically optimized to minimize the error of estimation via validation against an independent dataset.
In this way, the model’s best generalization capabilities are preserved not just based on the training dataset while
avoiding the risk of generating an overfitted model.
A GP is described by its mean (a function) and covariance served by a kernel function. It represents an expected
covariance between function values at a given point. GPR is a probabilistic approach for solving generic regression
problems with kernels. The GPR model establishes the relationship of the input variable x with the output variable
(biophysical parameter) y in the same way as SVR and KRR (see Eq. 4.48). The relationship with noisy
observation x can be expressed as:
(4.57)4.3.5
where y = a stacked output of n values, or y = (y1, y2, …, yn)T. Under the assumption that the noise (e) is
additively independent and has an identical Gaussian distribution with a mean of 0 and a variance of , the output
values obey the following distribution:
(4.58)
where = covariance matrix expressed as {k(, x1), (, x2), …, (, xn)}T, and The GPR solution is found by computing
the conditional distribution:
It can be a Gaussian distribution with the predictive mean being and the predictive variance being . GPR is able to
make use of very sophisticated kernel functions in the GPR model because all hyperparameters can be learned
efficiently by maximizing the marginal likelihood in the training set. The Gaussian kernel function in Table 4.4 can
be modified through scaling as (Verrelst et al., 2012b):
(4.59)
Where v = scaling factor; B = number of spectral bands used; sb = a controller of the spread of the relationship for
each particular spectral band; sn = standard deviation of the noise. This kernel function takes into account both
signal through v and noise through sb while also reflecting the relative relevance of different input features through
sn. Signal parameters (v and sb) and sn are known as model hyperparameters. Denoted as θ, they can all be
automatically optimized by maximizing the (negative log) marginal likelihood p(y|x,θ) in the training set, together
with weights (αi):
(4.60)
where I = identity matrix; T = transpose of matrix y. θ is resolved via computing its derivatives as:
(4.61)
where . The actual implementation of optimization can be based on gradient.
This method has three important properties: (i) The obtained weights αi after optimization indicate the
relevance of each spectrum xi; GPR identifies the relevant bands and observations in establishing relationships
with a variable (Verrelst et al., 2013); (ii) The inverse of σb represents the relevance of band b. Intuitively, a high σb
value means that relations largely extend along that band, hence a lower information content; and (iii) A GPR
model not only provides pixel-wise predictions for each spectrum but also yields an accompanying uncertainty (or
confidence) estimate to them. In addition, GPR is able to indicate the most contributing bands for each parameter,
weight the most relevant spectra in the training dataset, and attach a confidence level to the retrieved estimate
(Verrelst et al., 2012b). GPR has a demonstrated superiority to SVR and ANN in accuracy. It can be trained far
more simply than ANN, requiring a relatively small training dataset. Dissimilar to SVM, GRP allows the adoption
of very flexible kernels, including several free parameters, since their optimization can be efficiently realized via
maximum likelihood estimation. In addition, a fully-adaptive GPR can also reveal data characteristics, including
both signal and noise properties. It automatically generates some physical insights in the problem at hand by
ranking the input features (bands) and samples (spectra) according to their relevance to the task at hand, hence
avoiding the “black box” flavor of ANN (Schulz et al., 2018). GPR is simpler and more robust than other machine
learning family members while maintaining a very good performance and stability.
Bayesian Networks
Bayesian Networks (BNs) are powerful probabilistic methods to make inferences based on evidences. They are
graphical models able to study a set of variables that depend on and interact with each other. In quantitative remote
sensing, the set of variables are parameters to be estimated from remotely sensed data. The BN representation issuited to model and handle a large number of variables that have complex interactions among them. BNs are a
marriage between probability theory and graphs. As a class of probabilistic models, they are characterized by
graphical structures representing information on domains of uncertainty (Cooper and Herskovits, 1992). The
probabilistic relationship among a set of variables is graphically structured as nodes and arcs, with directed acyclic
BNs (Figure 4.8). Each node in the graph represents a random variable, while node edges connect the probabilistic
dependencies between variables. Arcs or links are represented as a directed acyclic graph without the feedback
mechanism. The arcs (represented by arrows in the directed acyclic graph) encode the conditional dependencies
(i.e., parent/descendant relationships between variables).
FIGURE 4.8 Directed Acyclic Graph illustrating connections in a Bayesian Network graphical model that shows
parent-child relations.
A BN consists of (i) a network structure, represented by a directed acyclic graph with nodes and links that encode a
set of conditional (in)dependence assertions about the variables. They represent conditional dependency
relationship between them; and (ii) a set of probability functions associated with each variable. In contrast to
graphical models, which are based on uni-directional edges, this approach is known as the Markov Random Fields
methodology. BNs have four strengths: (i) the ability to handle a large number of variables and incomplete datasets
(i.e., missing data); (ii) the ability to handle both numeric and categorical data simultaneously; (iii) the ability to
incorporate experts’ knowledge via a participatory modeling procedure of causal relationships; and (iv) ease of
understanding and graphical visualization (Mello et al., 2013). The effectiveness of a BN lies in the possibility to
compute through Bayes’ theorem not only the probability distributions for descendant variables from the values of
their parents, but also the distributions of the parents given the values of their descendants. Namely, a BN enables
us to know the effects given the causes, and vice versa.
A BN can be made dynamic to form a Dynamic Bayesian Network (DBN) that relates variables to each other
over adjacent time steps. It is suitable for processing dynamic environmental parameters that have a temporal
component (e.g., biomass growth). It extends the Bayesian network to the multi-temporal domain and describes the
update processes of state variables during that time. As such, it is strong at handling multi-temporal data in
retrieving parameters that vary with time, such as LAI. Results from Bayesian approaches not only demonstrate
the predictive power of a BN, but also its explanatory power (or uncertainty). A DBN has two critical parameters: a
graph structure and parameters of each conditional probability distribution, with the model structure controlled by
the system logic or causality. For expression clarity, upper-case letters (e.g., V1, Vn) denote both variables and their
corresponding nodes, and lower-case letters (e.g., v1, vn) the state or value (defining a particular instantiation) of
the variables. The joint probability distribution for any particular instantiation of n variables in a BN is given by:
(4.62)
where vi = instantiation of variable Vi, and ϕi = instantiation of its parents Φi (i = 1 to n). Parent variables are
instantiations directly influencing other descendant variables. The joint probability of any instantiation of all thevariables in a BN is calculated as the product of n probabilities in the form of , where Vi refers to variables with
known values (vi, i.e., instantiated variables). A common type of BN containing continuous variables is the
Gaussian BN in which the random variables obey the Gaussian distribution. A few terms related to BN need
explanation. The first is inference, the ability to compute posterior probabilities given some evidence. The
parameter to be inferred is called the target variable and the variables that describe it the context variables (i.e.,
those variables that are somehow related to the phenomenon). They can be either numerical (e.g., pixel values) or
categorical (e.g., land cover). Context variables can have a dependent relationship with the target variable and/or
among themselves (Mustafa et al., 2012). They influence the target variable and are considered its parents.
The core of a BN is a joint probability distribution expressed as a product of conditional probabilities that
describe the dependency between variables of the network. The joint probability distribution of a given set of
variable X ={X1, X2, …, Xn}, each having its own parents, is given by:
(4.63)
where corresponds to the parent variables of Xi (i.e., {Mi, Pi, Xi-1}). The joint probability distribution associated
with variable X = {x1, x2, …, xn} (n = number of samples) is the multivariate Gaussian distribution , given by:
(4.64)
where = n-dimensional matrix of the mean; = positive definite covariance matrix of N × n with determinant ; T =
matrix transpose. The univariate Gaussian distribution with density equals the conditional probability distribution
of Xi, the variable of interest given its proportion in the area of study, or:
(4.65)
where = expectation of at time i; = regression coefficients of on its parents; = number of parents of ; vi =
conditional variance of given its parents.
(4.66)
where = unconditional variance of ; = covariance between and variables pai; = covariance matrix of pai.
At time step i (i> = 2), the conditional probability distribution is calculated as:
(4.67)
with the following expectation and covariance:
(4.68)
(4.69)
The marginal distribution of X1 is expressed as:
(4.70)
Since a node can have a number of parents (Mi, Pi, Qi, …), BN is very good at assimilating multi-source data. For
instance, both remotely sensed and auxiliary meteorological data may be proportionally weighted as the parents to
estimate LAI, namely:
(4.71)
where and .DBN-based estimation of the target variable is, in essence, to perform inference in a DBN to compute a time￾series posterior probability distribution of the target parameter using two kinds of inference algorithms: filtering
and smoothing. Filtering inference is an online analysis, where observations arrive in real time, and smoothing
inference is an offline analysis in which all observations should be collected before it can be triggered (Wikle and
Berliner, 2007). Filtering inference is implemented recursively to estimate the probability states of time-series
parameters. Since parameter dynamics, time slice, and reflectance can all be obtained from time-series satellite
data (or data products), the estimation of parameter values is virtually a process of performing an inference in a
DBN. The core of the filtering algorithm is how to define the probability functions, often the most complicated
part of BN modeling (Mello et al., 2013).
In practice, the probability functions of BNs are defined through discretization of the context variables based on
analyst knowledge, in which the observed values of X are represented by discrete quantities (analogous to data
processing for drawing a histogram). The range of observed values is divided into intervals defined by the user. Model parameters in a BN can be derived once the model structure is built. The state transition matrix and
likelihood probabilities are obtained through parameter learning of a BN. Likelihood probability is calculated from
the training dataset generated by physical models. The values of the training samples (e.g., spectral reflectance)
may be discretized at the appropriate intervals so that the conditional probability distribution function can be
represented in the tabular form (Zhang et al., 2012). This table lists the probability that the child node assumes
each of its different values for each combination of values of its parents. If there are latent variables in the DBN or
several missing data in the training dataset, a more complex algorithm must be considered, such as the expectation
maximization algorithm or gradient descent, to determine this parameter value.
Both the models and observations have a degree of uncertainty that should be considered when inferring the
state of the target parameter. A common way of dealing with uncertainty is to assume both the model and
observation errors to obey Gaussian distributions. In this way, for a certain time slice, the uncertainty associated
with the state transition matrix is calculated using a normal cumulative distribution function, and the likelihood
probability equals the product of the probabilities computed by the normal cumulative distribution function from
the observed reflectance (and its associated uncertainty) and the corresponding probabilities obtained from the
conditional probability table.
However, this technique has been rarely applied to processing remote sensing data, a situation that has changed
since the advent of the Bayesian Network for Raster Data – BayNeRD, available in the R language (R Core Team
2018). It accepts inputs as a raster layer in GeoTiff format corresponding to a variable (node) of the network.
BayNeRD allows the number of intervals to be computed following three criteria: (i) equidistant, in which
intervals are set at the same width, (ii) quantile, in which intervals tend to have the same number of elements
(pixels), and (iii) arbitrary, in which the user manually defines the limits of each interval. The discretization
impacts the probability functions, which are computed through pixel counting to both the dependence relations
defined in the graphical model and the intervals defined in the discretization process (Silva et al., 2014). The entire
procedure of using BayNeRD for quantitative retrieval of parameter values is illustrated in Figure 4.9.4.4
4.4.1
FIGURE 4.9 Procedures of using BayNeRD for the quantitative retrieval of biophysical variables from remote sensing
data. (Modified from Mello et al., 2013, open access.)
MISCELLANEOUS METHODS
Depending on the data used, they may be processed using special methods to prepare for subsequent
quantification. These processing tasks range from construction of 3D models from time-sequential images in
preparation for vertical change detection to enhancing the signal of the target parameters so that they can be
quantified more accurately. Three methods of data analysis are introduced in this section, Structure from Motion
(SfM), movement detection, and signal enhancement via continuum removal.
SfM Photogrammetry
SfM refers to the process of generating an accurate 3D representation (sparse 3D points) of a stationary scene of
interest that has been captured in time-sequential, overlapping multi-view images without any information on
camera position and orientation. This recent development in computer vision is based on softcopy photogrammetryin reconstructing the 3D model of the scene via triangulation from a pair of overlapping aerial photographs based
on the geometry of the scene, facilitated by the 3D location, camera position and orientation, and the 3D
coordinates of GCPs. In contrast, such information is not needed in the SfM method as the camera pose and scene
geometry are simultaneously, automatically, and iteratively resolved via redundant bundle adjustment, a non-linear
least squares approach, based on features automatically extracted from a set of overlapping images. This bundle
adjustment procedure identifies the 3D position of each matching feature on multiple overlapping, offset images
through the analysis of subtle differences in the relationships between all of them (angle, distance, etc.), and the
position and angle at which the photos are taken, dramatically reducing the number of unknowns during the
alignment of the photographs. In other words, the collinearity equations (Eqs. 4.72 and 4.73) that depict the
relationship between the position of ground point A (XA, YA, ZA) and its image coordinates (xa, ya) are solved
without prior knowledge of camera poses or ground control (Figure 4.10). Another disparity between the two is the
calculation of the fundamental matrix instead of the essential matrix:
(4.72)
FIGURE 4.10 The concept of collinearity stating that ground point A, its image point a, and the camera exposure
center (O) lie in a straight, uninterrupted ray. The coordinates of A in the ground (blue) and image (green)
coordinate systems are related to the focal length (f) of the camera lens (A’s ground coordinates are known if it is a
GCP). o – image principle point.
(4.73)where f = focal length of the camera lens used to photography; ai (i = 1, …, 9) = coefficients that can be calculated
from the camera orientation parameters of (w, j, κ) using the following matrix:
(4.74)
If the image is taken with a camera, the six unknown parameters are (X0,Y0, Z0, w, j, κ) that can be determined
with the use of at least three GCPs (Figure 4.10). The collinearity equations are inverted as:
(4.75)
(4.76)
The general workflow of SfM photogrammetry pipeline can be summarized as identification of matching points,
alignment of input photos, creation of a sparse point cloud, and transformation of the image from abstract
coordinates to absolute ones, and creation of dense RGB point clouds. These tasks can be grouped into three main
parts of pre-processing, core processing, and post-processing (Figure 4.11). Pre-processing has two major tasks of
image acquisition and GCP data collection. They are discussed below in separate sections.4.4.1.1
FIGURE 4.11 The general procedure of implementing SfM processing to generate 3D point clouds from overlapping
photos. (Modified from Westoby et al., 2012, used with permission (5761561405826) from Elsevier.)
Input Images
SfM photogrammetry can match points from images of widely differing scales, view angles, and orientations as
those obtained from drones. SfM is able to create high-resolution DEMs from extensive photosets obtained using a
consumer-grade digital camera. They can be acquired on the ground or from the air. Ground photos are taken at
stationary points from a camera mounted on a tripod. Multiple overlapping photos of the target are taken from
nearly a constant distance to it. These photographs are invariably oblique or horizontal, causing the captured
features to have a highly variable scale, especially those in the extreme background, which creates difficulties in
subsequent image matching and introduces inaccuracies to the extracted 3D point clouds. Nevertheless, oblique
photographs are able to generate SfM point clouds for very steep (vertical to over hanging) terrain surfaces that
may be obscured in nadir-viewing orthophotos. To generate the best 3D models, numerous photos are preferred, up
to 100. They must have a high degree of overlap up to 80%, usually taken from a moving platform, to enable
features common to them to be identified automatically.4.4.1.2
4.4.1.3
The number of photos that can be processed depends largely on the computer RAM. All the photos have the
same look angle and geometry or orientation if the tripod is leveled using a water bubble spirit. Not all features on
the ground are fully visible on the photos due to obstruction by foreground features. Ground photos are the most
suited to sense features on steeply sloping terrains or hanging on a cliff, as with artefact and archaeological sites.
In comparison, nadir-viewing aerial photographs taken from above are not subject to this restriction. The obtained
photos “view” the same scene from multiple perspectives, from which the 3D surface of the scene is constructed
using either discrete motion (wide baseline) photographs or continuous (infinitesimal) motion from video. Aerial
photographs have the added advantage of possessing precisely known orientation and 3D position of the camera.
They are essential in producing highly accurate DEMs for precise quantification. Low-altitude photographs are the
best for areas of more subdued topography over larger sites.
Compared with aerial photographs, drone images are harder to control geometrically and hence less reliable.
Geometric distortions can raise difficulty in matching features on the overlapping images. Regardless of the
platform of photo acquisition, the quality of the acquired photographs, such as the quantity, clarity, and resolution
of the photoset, determines the quality of the output point clouds. Photos of a large scale taken at a closer range
from the feature of interest tend to produce superior spatial density and resolution of the final point cloud, and vice
versa. A minimum of three photos is needed for successfully reconstructing the scene and for corresponding
features to show up in multiple photos, but the exact number is variable as it depends on the complexity, lighting,
and the nature of individual scenes. The general recommendation is to obtain as many images for SfM input as
possible, under the given logistical constraints. They can generate an optimal number of matched keypoints and
system redundancy.
Distinct marks widely distributed over the area of study should be laid out on the ground to serve as the GCPs.
Their precise coordinates can be logged with real-time correction accuracies <1 m. GCPs should be well-balanced
in their spatial distribution and easily identifiable in all the images. They must be stable from the oldest
photographs to the most recent ones, to obtain the highest possible precision in long-term studies of the terrain or
landslides. GCPs should be located in a minimum number of three consecutive images to allow the 3D model to be
geo-referenced to a specific coordinate system. It may be necessary to calibrate the camera lens used to take aerial
photographs. Lens calibration may be automatically undertaken during photos alignment.
Matching Point Selection
Matching points are distinct marks visible on overlapping photographs. Their major function is to link these
photos, so no ground coordinates are needed of them. They enable better correlation of the photographs and limit
the distortion between them. Also called the matching features, they are identified as distinct marks common to
adjacent individual images and help to establish correspondence between them. These matching features are
invariant to image scale and rotation, and partially invariant to changes in illumination conditions and 3D camera
viewpoint (Westoby et al., 2012). Also called features sometimes, keypoints are point features in an object of
interest automatically identified at all scales and locations in each image. They are likely to be visible in different
images of the same object. Ground features such as blimp or kite tethers or helicopter landing skids are not suitable
keypoint candidates as their position relative to other keypoints is constantly changing, and they can be
automatically filtered out using visibility and regularization constraints.
Scene Reconstruction
The core of the SfM workflow is to construct the 3D scene based on the selected keypoints in multiple images.
Scene reconstruction comprises three main steps (Schönberger and Frahm, 2016):
(i) Feature extraction. For each image Ii, SfM detects sets Fi = {(xj,fj) | j = 1 ... } of local features at location xj∈. Matching features can be detected from each image automatically using the popular Scale Invariant Feature
Transform (SIFT) object recognition system, in which the maxima of difference-of-Gaussians (DOG) pyramid is
considered as features. It enables characteristic image objects to be automatically detected, described, and matched
between photographs. To do so, the dominant gradient direction must be determined first, to which the feature
descriptor is rotated and fit to make it rotation-invariant. Features can also be detected using the Speeded-UpRobust Features (SURF) algorithm. It replaces the DOG with a Hessian matrix-based blob detector. SURF sums
the gradient components and their absolute values instead of relying on the gradient histograms. It uses integral
images to detect features at an extremely quick pace and high detection rate, but features are not so accurate in
their positions.
In identifying keypoints from individual images, their number that can be selected for an image is dictated by
image texture and resolution. Whatever keypoints are selected, they must be represented mathematically by an
appearance descriptor fj calculated via the transformation of local image gradients. Feature descriptors should be
unique and sufficiently informative to enable the same features in multiple overlapping photos to be matched in
large datasets (e.g., the same point in an object should have a similar characteristic in images of a slightly different
perspective).
(ii) Feature matching. Every feature that is part of a match in an image pair is searched in other adjacent,
overlapping images. A track is generated from features if they meet certain criteria (e.g., showing up in more than
three images). The descriptors from different images are matched and geometrically filtered, resulting in a
collection of matches between each image pair. Feature matching may be accomplished using different algorithms,
such as the näıve and greedy approaches. The former searches for feature correspondences by finding the most
similar feature in image Ia for every feature in image Ib, based on a similarity metric of feature appearance fj. The
latter seeks the best match in the second image for a given feature in the first image irrespective of other matches.
This method is suitable for small pace of motion with little rotation in a small search window. Other image
matching algorithms include approximate nearest neighbor (Arya et al., 1998) and random sample consensus
(RANSAC). They track and link identical keypoints in the photoset. Tracks work with a minimum of two
keypoints and three images that meet the specified criteria to reconstruct the point-cloud scene after transient
features (e.g., moving vehicles across the area of study) have been automatically eliminated, in conjunction with
all non-static objects of no interest from the input photoset. The matching of adjacent photos may be accomplished
by tracking the same features from one image to the next using the Lucas–Kanade optical flow tracker. Feature
extraction and matching are followed by geometric verification, and scene point triangulation.
(iii) Geometric verification. Since feature matching is based solely on appearance, not all matched features are
genuine. They need verification via a transformation that maps feature points between images using their
geometric relation. It is homographic if the camera just moves or rotates in capturing a planar scene, or epipolar if
the camera just moves. Epipolar transformation can be extended to three views using the tri-focal tensor through
the essential matrix E (calibrated) or the fundamental matrix F (uncalibrated). If a sufficient number of features
common to the images is mapped by a transformation, they are considered geometrically verified. Outlier
correspondences or incoherent matches can be eradicated via filtering using the RANSAC algorithm. It can also
solve the location determination problem, where the objective is to determine the points in space that project an
image into a set of landmarks with known locations. After feature correspondence has been established, the
essential matrix is computed, from which the relative 3D position and visual characteristic of the camera (sensor)
rotation and translation are retrieved to scale, followed by optional dense stereo matching using the recovered
epipolar geometry. Errors of image matching may be minimized via a non-linear least squares solution.
(iv) Image registration and triangulation. New images are registered to the current model based on feature
correspondences to triangulated points in the registered images. The pose of newly registered image and, for
uncalibrated cameras, its intrinsic parameters, are solved to extend the set P. Since the correspondences often
contain outliers, the pose for calibrated cameras is usually estimated using RANSAC and a minimal pose solver.
For uncalibrated cameras, various minimal solvers or sampling-based approaches may be used (Schönberger and
Frahm, 2016).
Newly registered images expand the covered scene by extending the set of points X through triangulation. A
new scene point Xk is triangulated and added to X once another overlapping multi-view stereopsis image is
registered. Triangulation stabilizes the existing model via redundancy, and enables new images to be registered by
creating more correspondences. All the matched image pairs are triangulated to calculate the 3D position of each
“track”, estimate the 3D point positions, and incrementally reconstruct scene geometry, referenced to a relative
coordinate system. This incremental approach solves camera poses and adds them to the collection frame by frame4.4.1.4
sequentially with an iterative reconstruction component. Other less popular strategies include hierarchical and
global approaches. Global SfM solves the poses of all cameras simultaneously. An intermediate approach
exemplified by the out-of-core SfM computes several partial reconstructions that are then integrated to generate a
global 3D that may be filtered to remove outliers; and
(v) Surface generation. This procedure applies to a pair of images captured with a calibrated camera (sensor).
The procedure is slightly changed with uncalibrated cameras in the last step in which the corresponding 3D scene
points are reconstructed to 3D projective transformation (or projective reconstruction rather than similarity
reconstruction) (https://cseweb.ucsd.edu/classes/fa21/cse252A-a/lec10.pdf). The 3D scene and positions are
reconstructed from temporal trajectories of features, together with the camera’s motion, using a similarity
transformation, during which constraints on camera pose orientation are placed based on keypoint
correspondences. Scene reconstruction ends with the creation of a sparse point cloud.
The entire process from keypoint extraction to accurate reconstruction of scene geometry can be fully
automated using bundle adjustment. A bundle block adjustment performed on the matched features can identify the
3D position and orientation of the camera, and the 3D location of each feature in the photographs, resulting in a
sparse 3D point cloud. During bundle adjustment, the reconstruction is refined and uncertainties of camera
parameters and point parameters are non-linearly and jointly minimized in a high-dimensional space. It also
minimizes the reprojection error caused by uncertainties in the camera pose during image registration and
triangulation. Without further refinement, SfM tends to drift quickly to a non-recoverable state. Bundle adjustment
may be fine-tuned using field-logged GPS coordinates of artificially installed GCPs manually identified in the
model and the corresponding photographs, and the camera GPS coordinates. Bundle adjustment is easily
accomplished using Bundler-sfm for producing sparse points (Snavely et al., 2008), and the Clustering for Multi￾View Stereopsis (CMVS) and Patch-based Multi-view Stereo 2 (PMVS) algorithms to produce dense point clouds
based on camera positions derived from Bundler. This generic bundle adjustment software non-linearly refines the
SfM scene by minimizing the structure reprojection in the images (residual error). This module allows a fine grain
control over the intrinsic (principal point, focal, distortion) and extrinsic (rotation, translation) parameters, and
structure landmarks as whether they should be held as constants or treated as a variable during the minimization.
Post-processing
Since no GCPs are used in SfM, the camera positions derived from SfM lack the scale and orientation information
relative to the ground. The generated 3D point clouds all have a relative “image-space” coordinate system, unable
to fulfil any quantification from them, which can be remedied during post-processing. It addresses how to
transform the 3D scene to the ground coordinate system and how to generate 3D point clouds. The first task is
resolved by transforming the relative coordinate system to the absolute one via a small number of manually
identified GCPs with known object-space coordinates in the point cloud to establish an appropriate 3D similarity
transformation (Figure 4.11). Such GCPs can be derived post-hoc in most cases. The number of GCPs needed for
the transformation depends on its complexity. A minimum of three GCPs are needed to perform a rigid body
transformation that can be decomposed into a rotation and a translation matrix, and a scale factor, achievable using
a modified Horn’s (1987) absolute orientation algorithm in MATLAB®. A minimum of seven GPSs is essential to
solve the seven unknowns (three each concerning camera position and orientation, plus one scaling factor), in the
total transformation of at least three matching images and object space coordinates involving warping. Post-image
alignment requires geolocation information of drone images, and may be optimized using focal length, principal
point coordinates (x0,y0,-f) (Figure 4.10), radial distortion coefficients, and tangential distortion coefficients with
adaptive camera fitting (Swayze et al., 2021). After manual georeferencing and image optimization, inferior GCPs
may be discarded, leaving the best ones for building dense point clouds. The corresponding 3D scene points are
constructed to scale. Prior to this, the transformed model may be manually processed to remove significant outliers
and artefacts (e.g. erroneous peaks and troughs stemming from keypoint descriptor mismatches).
High density points are produced by decomposing overlapping input images into subsets or clusters of a
manageable size, from which 3D points are independently reconstructed. Once the 3D point clouds are considered
acceptable, a more dense point cloud (“densification”) can be created from the raw point clouds into a typically4.4.1.5
coarser resolution model (e.g., meter-scale) using several algorithms. Point density can be extremely high,
reaching upwards of 103 points per m2, depending on the image set. In detecting canopy height, dense point clouds
are filtered to remove erroneous points below ground and above the canopy. The point clouds may be used to
generate a surface, either with colored vertices or with a texture. Dense clouds may be converted to a polygonal
mesh, to which texture may be added. Texture mapping adds photographic detail to the 3D surface based on the
input images. Depending on the system of implementation, it may be possible to export the mesh, edit it externally,
and re-import it.
Implementation
SfM photogrammetry may be undertaken using a range of free and open-source packages resulted from PhD thesis
research or post-doc work in U.S. research laboratories. Some of them deliver a software program (e.g., Bundler￾SfM and COLMAP), or provide a collection of libraries (e.g., TheiaSfM and OpenMVS) and modules (e.g., MVE,
OpenMVG, and OpenSfM), while others are based on third-party tools and libraries (e.g., Visual SfM and
Regard3D). TheiaSfM is a very efficient and accurate end-to-end SfM library with a modular structure for easy
reading and expansion. Written in Python, the OpenSfM library reconstructs camera poses and 3D scenes from
multiple images with basic modules for feature detection/matching using minimal solvers. Characterized by a
robust and scalable reconstruction pipeline, it can integrate external GPS and accelerometer measurements for
geographical alignment and robustness (Mapillary, 2021). These open-source systems have advanced the state of
the art tremendously, but none of them offer an end-to-end photogrammetry pipeline except MVE. Nevertheless,
the key problems with incremental SfM (e.g., robustness, accuracy, completeness, and scalability) prevent its wide
adoption as a general-purpose method. This situation changed with the release of COLMAP (Schonberger and
Frahm, 2016). It offers general purpose SfM and MVS pipeline, and encapsulates powerful tools for generating
impressive 3D models from ordered and unordered images. Some of the main packages are compared in Table 4.5.
Table 4.5
Major open-source software systems and libraries that can implement SfM
Software/source Main features Reference
OpenSfM
https://github.com/mapillary/OpenSfM/blob/main/doc/source/index.rst
• A library for
reconstructing
camera poses and
3D scenes from
multiple images;
• Basic modules for
feature
detection/matching,
minimal solvers
with a robust and
scalable pipeline;
• Integrates
external sensor for
geographical
alignment. https://forum.ma
Bundler-SfM
https://github.com/snavely/bundler_sfm
• Incremental 3D
reconstruction of
camera and
(sparse) scene
geometry for
unordered images;
Snavely et al. (2Software/source Main features Reference
• Produces sparse
point clouds that
are optimized using
Sparse Bundle
Adjustment;
Visual SfM
(http://ccwu.me/vsfm or
http://homes.cs.washington.edu/~ccwu/vsfm/
• Integrates
existing modules
and tools for 3D
reconstruction from
lots of
photographs;
• GUI interface,
automatic 3D
reconstruction,
quite simple to use;
• Fast multi-core
parallel processing; Wu (2011)
TheiaSfM (https://github.com/sweeneychris/TheiaSfM)
• Many common
algorithms for
pose, feature
detection and
description,
matching, and
reconstruction;
• Efficient, robust
and reliable
algorithms;
• State-of-the-art
performance on
large-scale
datasets; Sweeney (2016)
Regard3D
https://sourceforge.net/projects/regard3d/files/latest/download
• Construction of
3D models of
objects, based on
photographs from
different angles;
• Calculated 3D
model may be
cleaned up with
MeshLap;
• Based on
powerful third￾party tools and
libraries; Hiestand (2019)
COLMAP
https://github.com/colmap/colmap
• General purpose
SfM and Multi￾View Stereo
(MVS) pipeline;
Schonberger and
(2016)4.4.1.6
Software/source Main features Reference
• Powerful tools for
generating great
3D models from
ordered and
unordered images;
• GUI or
command-line
interface;
• Robust, accurate,
complete, and
scalable.
Open MVG (Multi-view geometry)
(https://github.com/openMVG/openMVG)
• A collection of
modules for
solving 2 to n-view
geometry
constraints;
• A generic
framework able to
embed modules for
robust estimation;
• “Keep it simple,
keep it
maintainable”
philosophy of
design. Moulon et al. (20
In spite of the prevalence of SfM freeware, most authors still prefer the SfM workflow using commercial software
packages such as Agisoft PhotoScan Professional, a tool for a photogrammetry pipeline (its freeware version is less
powerful). It has a ready-to-use workflow and additional modules essential for accurate photogrammetry such as
camera distortion calibration. It can automatically calibrate frame photos taken with spherical (including fisheye)
and even cylindrical cameras. The lens calibration module models the focal length, principal point coordinates, and
radial and tangential distortion coefficients using Brown´s distortion model and allows on-screen lens calibration.
This versatile system is able to process all sorts of imagery, be it aerial (nadir, oblique), close-range, or satellite
(RGB, NIR, thermal, and multispectral), and even scanned aerial photographs with fiducial marks support. It can
construct 3D models and produce orthophotographs. Agisoft also offers an elaborate module for ortho-photo
mosaicking and orthomosaic seamline editing, such as manual removal of outliers from the low-density 3D cloud
to achieve accurate clouds. It also contains inbuilt tools for stereoscopically measuring distances, areas, and
volumes from the constructed 3D model.
An Assessment
The advent of free software has considerably eased the technical burdens of using SfM as it has the apparent
logistical advantages of requiring limited hardware and portability in comparison with the direct methods of
collecting point clouds using laser scanning. However, the requirement of lengthy processing times still remains
the most critical bottleneck preventing its wide adoption because the main processing steps, such as keypoint
descriptor extraction, matching, and sparse and dense point cloud reconstruction algorithms, all requiring intensive
and highly demanding computation. Total processing times for a typical photoset of 400–600 images comprising
2272×1740 pixels each can run easily into tens of hours with desktop PCs (Westoby et al., 2012). This time length
has been reduced to 6.4 hours, of which 75 minutes are spent on feature detection and matching, 65 minutes on4.4.2
4.4.2.1
creation of dense 3D geometry, ⁓2 hours on manual/visual identification of GCPs, 65 minutes on re-creation of
dense 3D model; and 30 minutes on creation of DEM and orthophoto (Lucieer et al., 2014). The 3D models
contained 5 million vertices over an area of 1 ha, or a density of 500 points•m-2.
The exact run-time may be shortened by using coarser resolution images, but it will reduce the number of
returned keypoint descriptors, and ultimately decrease point density. Another method of shortening the
computation time is to significantly improve bundle adjustment (BA) by preconditioned conjugate gradient. This
novel BA strategy expedites incremental SfM and strikes a reasonable balance between speed and accuracy (Wu,
2013). Even with fewer steps of iteration, high accuracy is obtainable by regularly re-triangulating the feature
matches that initially fail to triangulate. The improved method offers a state-of-the-art performance for large-scale
3D reconstructions. The last approach for speeding up the process is to divide a large project into multiple
“Chunks”, each containing a logical group of photos (e.g., sub-area or certain object or environment). Each group
is processed identically but separately. At the end of processing, all Chunks are meshed, aligned, and fused into
one single 3D model.
The limited accuracy of SfM is another major obstacle for its wide adoption in quantitative remote sensing. A
comparison of ground photo-SfM surface with a precision TLS reveals a vertical accuracy of -0.5 to 0.5 m for sites
with complex topography and a range of land covers for a 300 m×300 m cliff section of a steep alpine hill slope
(Westoby et al., 2012), though the photosets are taken with a consumer grade camera (e.g., no camera distortion
correction). The scale of elevation differences between two repeat SfM-constructed models can capture only
relatively large (i.e., meter scale) topographic dynamics, limiting its capability to monitor landscape change via
differencing of SfM-constructed DEMs. However, a closer inspection reveals notable local deviations between the
SfM and TLS datasets largely in areas of relatively dense shrub and bush cover, but considerably lower (i.e. ≤±0.1
m mostly) in vegetation-free areas (with the exception of short grass). At the dm scale and below, 61% and 39% of
the vegetation-free and densely vegetated areas, respectively, possess zdiff values ≤±0.1 m (Westoby et al., 2012).
Similar accuracies have been achieved by Harwin and Lucieer (2012) using MVS SfM techniques combined with
images acquired from a multi-rotor micro-UAV at a flight height of about 50 m (<1–3 cm point spacing) in a
coastal area. The geo-referenced point cloud has a vertical uncertainty of 0.025–0.040 m, which means that SfM
can be used to detect sub-decimeter terrain change (e.g., coastal erosion) owing to the influence of other factors.
Better accuracies (e.g., a vertical accuracy of 0.016 m and 0.089 m) have been achieved in quantifying exposed
and submerged fluvial topography at the meso-habitat scale from drone images of 0.02 m spatial resolution using
SfM-photogrammetry and bathymetric LiDAR data (Woodget et al., 2015). The errors in submerged areas drop to
0.008 m without refraction correction to 0.053 m after correction. Thus, SfM photogrammetry, if combined with
other pertinent data, is able to achieve an accuracy about 2–5 times the image pixel size in both the coastal and
fluvial environments.
SfM can potentially offer boundless possibilities in processing remotely sensed imagery data. So far, it has been
used mostly for highly detailed 3D visualization of building facades and archaeological sites. Its application in
quantitative remote sensing is confined to estimate canopy height and mass movement changes. If applied to
process UAV images, SfM can acquire dense and accurate 3D data of the Earth surface cost-effectively and
efficiently. The high fidelity of TLS and SfM techniques, combined with the ability to generate SfM-based terrain
models of cliffs, offers unprecedented opportunities in estimating rockfall volumes and quantifying the rates of
cliff retreat in mountainous landscapes (Guerin et al., 2020).
Movement Detection Methods
The quantification of movement pace (velocity) requires comparison of two-time datasets of the same area, usually
based on their spatial (auto)correlation. It can be implemented in two ways, depending on the nature (image or
DEM) of the datasets. The manner of implementing the spatial comparison also affects the minimum detectable
pace and its accuracy. They are discussed separately below.
COSI-Corr4.4.2.2
Correlation analysis is the default choice for determining the pace of movement, usually from two-time lapsed
images or DEMs of the same area at the same resolution, such as debris displacement and ocean current circulation
speed. It can be easily achieved using the advanced image correlation method known as Co-registration of
Optically Sensed Images and Correlation or COSI-Corr (www.tectonics.caltech.edu) (Ayoub et al., 2009).
Correlation is calculated from two single-band spaceborne images, aerial photographs, or even orthorectified UAV
mosaics. In case of multi-band RGB images or multispectral images, they must be converted to a single-layer
image, or the calculation is repeated several times, each time for one of the selected bands. The correlation
between the two images is calculated based on a kernel using two methods: frequency and statistical. The
frequency correlation method computes the relative displacement between a pair of images retrieved from a
Fourier transform. The statistical method produces an absolute value of the correlation coefficient of a patch in one
image and the corresponding patch in the other image. Correlation is calculated by shifting the second patch in
rows and columns within a window in both the x and y directions. All the correlation coefficients in the kernel are
then compared among themselves and the maximum of the correlation matrix is then output. This method of
calculation requires three inputs (Figure 4.12): (i) patch size or the window of pixels whose values are correlated in
the x and y directions; (ii) step size or the interval of pixels in the x and y directions that should be shifted between
two sliding windows; and (3) search range or the distance in the x and y directions in pixels where the patch in the
second image is searched to detect the displacements (Lucieer et al., 2014). Typical correlation analysis utilizes a
window size of 128 by 128 pixels (down to 32 by 32 pixels) at a step size of four pixels between adjacent
correlations (Ayoub et al., 2009). The search radius varies with the magnitude of displacement, image spatial
resolution, and the temporal separation between the two images. It is commonly set to a few meters. The calculated
outcome is expressed in shifts by the number of pixels in the x (east-west) and y (north-south) directions
separately. They can be converted to an overall direction by squaring-root the sum of the squared displacements in
both directions. COSI-Corr also outputs a layer illustrating the spatial distribution of the SNR that can be used to
judge the quality of the computed displacement. COSI-Corr is limited by its vulnerability to noise as not all the
detected displacements are genuine. Besides, it does not supply any indication about the detection reliability. These
deficiencies are effectively overcome by other more powerful algorithms.
FIGURE 4.12 The concept of detecting displacement (d) from a pair of time-sequential images. (a) Window size (S)
defining the feature to be searched and search step size (ΔS) in image t1; (b) Search radius (R) for the defined
feature in image t2 and the location of the detected feature that has displaced by a distance of d. White arrows –
direction of search; dots – pixels.
M3C24.4.3
The multiscale model-to-model cloud comparison (M3C2) algorithm as implemented by Lague et al. (2013)
requires paired SfM-derived point clouds as the inputs rather than images, and output 3D distances. This algorithm
constructs cylinders that intersect the compared point clouds along the normal direction. Their dimension and
orientation are subject to the normal scale (D) and projection scale (d) specified by the analyst. D should be >20–
25 times larger than the roughness estimates at scale D to avoid normal incorrect orientation, whereas d should be
< 1–2 m for large roughness surfaces such as rockfall debris (Esposito et al., 2017). Alternatively, D and d may be
estimated by the algorithm itself automatically from the point clouds properties (roughness and number of points).
Each cylinder isolates two sub-clouds (n1 and n2), and project them onto the cylinder’s axis to estimate their
respective mean positions (i1 and i2). The length of the axis segment between i1 and i2 represents the distance
between the two sub-clouds (Lague et al., 2013). For each estimated distance, its uncertainty (level of detection at
95% or LoD95%) is calculated as:
(4.77)
where (d)2 and (d)2 = independent variances of the sub-cloud positions; n1 and n2 = number of points of the sub￾sampled clouds, and reg = co-registration error between the two corresponding multi-temporal clouds. A calculated
distance is considered statistically significant (there is significant change) if it is >LoD95%. The algorithm verifies
this condition automatically for each estimated distance. M3C2 outputs a new cloud, in which each point contains
the following information: (i) distance to the closest corresponding point of the compared cloud; (ii) distance
uncertainty; and (iii) significant change (Esposito et al., 2017). The co-registration error between the compared
point clouds is determined by the absolute distances between these points calculated in CloudCompare using the
Cloud-to-Cloud Distance module (C2C) (www.McDaniel.net/cc/). The mean distances between corresponding
stable areas not subject to movement is considered the co-registration error between two clouds.
Continuum Removal
The prevalent use of hyperspectral data in remote sensing has enabled the precise quantification of the content of
soil minerals at a minute concentration level. These data generally capture the reflectance behavior of the
substance at a wide range of wavelengths. The minor troughs in the spectral reflectance curve at certain
wavelengths are caused by their absorption (Figure 4.13, baby blue curve). Their absorption spectra can be
exploited to quantify certain features in soil and vegetation. The absorption of minerals in a spectrum comprises a
continuum and individual features. The former represents the overall or background reflectance of the spectrum
curve. The latter sheds critical information on the magnitude of absorption that is related to metal concentration
levels. These absorption troughs are the targets of interest, determined in two steps: (i) a convex hull of straight￾line segments or continuum is fitted over the reflectance curve (green line in Figure 4.13), in a fashion reminiscent
of fitting a rubber band over the kaolinite spectrum; (ii) the reflectance value of the hull is normalized to 100%
(van der Meer, 2004), causing the continuum to be constant over the entire wavelength range. The “normalized
spectrum” can be used to identify the lowest reflectance values or the deepest troughs. Since the grand sum of
reflectance and absorbance is a constant for all substances, the subtraction of the actual reflectance from the
continuum (e.g., continuum removal) effectively reveals the absorption behavior of substances. The absorption in a
spectrum has two components: a continuum and individual features. The reflectance (or absorption) by individual
features is subtracted from the reflectance that has been referenced to the normalized hull. Known as “continuum
removal” or “convex-hull” transform, this differencing enables spectra of different minerals or substances to be
compared with each other and to quantify their concentration via the continuum removed spectra.FIGURE 4.13 Diagrams illustrating the concept of continuum removed reflectance spectra. Green curve: fitting of a
rubber band over the measured kaolinite spectrum to derive the hull; blue curve: disparity between the original
spectrum and the hull that has been normalized to 100%. (Modified from van der Meer, 2004.)
A number of parameters can be derived from the continuum-removed spectra, such as absorption wavelength
position, the absorption feature depth, area, and asymmetry. The absorption-band position (λ) refers to the band
wavelength at which the reflectance value is the minimum over the spectrum of an absorbing feature. The relative
depth (D) of absorption is defined as the discrepancy between reflectance at the shoulders and the absorption-band
minimum, or the distance between the lowest absorption trough to the straight line linking the reflectance at the
two nearest wavelengths not affected by absorption. It is calculated relative to the continuum (Rc) as:
(4.78)
where Rb = reflectance at the band bottom; Rc = reflectance of the continuum at the same wavelength. Area
alludes to the space enclosed by the trough and the straight line. It can be further differentiated into left area and
right area (Figure 4.13).
After finding the minima, the shoulders to the left and right of it are searched to determine asymmetry, defined
as the ratio of the area of the absorption from the starting point to the maximum point (Aleft) to the area of the
absorption from the maximum absorption point to the end point (shoulder) of the absorption (Aright), or:
(4.79)The above two equations are underpinned by the assumption that the spectral data are nearly continuous
(contiguous), which is true with field-measured spectrometer data. They are not directly applicable to imaging
spectrometer data acquired at discrete spectral bands over certain wavelength ranges only. If the needed reflectance
at an unsampled wavelength is missing, it can be interpolated linearly from the reflectance observed at the adjacent
wavelengths within a local wavelength range.
The derived absorption-band parameters are significant to quantify the absorption feature parameters of certain
minerals from hyperspectral curves, and have been widely used in quantitative reflectance spectroscopy. The
absorption-band depth is indicative of the quantity of certain minerals in a sample. Furthermore, it can shed light
on the grain or particle-size that governs the amount of light scattered and absorbed by a grain. A combination of
absorption-band position and depth facilitates the derivation of surface mineral composition from hyperspectral
data.
REFERENCES
Ahmed AAM, E Sharma, SJJ Jui, RC Deo, T Nguyen-Huy, and M Ali (2022) Kernel ridge regression hybrid
method for wheat yield prediction with satellite-derived predictors. Rem Sens 14: 1136. doi:
10.3390/rs14051136
Arya S, MD Mount, NS Netanyahu, R Silverman, and AY Wu (1998) An optimal algorithm for approximate
nearest neighbour searching fixed dimensions. J Assoc Computing Machinery 45: 891–923.
Awad M and R Khanna (2015) Chapter 4 – Support vector regression. In M Awad and R Khanna (eds.) Efficient
learning machines—theories, concepts, and applications for engineers and system designers, pp 67–80. New
York: Springer Science+Business Media.
Ayoub F, LePrince S, and Keene L (2009) User’s guide to COSI-Corr: Co-registration of optically sensed images
and correlation. www.tectonics.caltech.edu/slip_history/spot_coseis/pdf_files/cosi-corr_guide.pdf
Breiman L (1996) Bagging predictors. Mach Learn 24: 123–140. doi: 10.1007/BF00058655
Brownlee J (2019) How to choose a feature selection method for machine learning. In Data preparation, retrieved
from https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/
Cheng E, B Zhang, D Peng, L Zhong, L Yu, Y Liu, C Xiao, C Li, X Li, Y Chen, H Ye, H Wang, R Yu, J Hu, and S
Yang (2022) Wheat yield estimation using remote sensing data based on machine learning approaches. Front
Plant Sci 13: 1–16. doi: 10.3389/fpls.2022.1090970
Cooper GF and E Herskovits (1992) A Bayesian method for the induction of probabilistic networks from data.
Mach Learn 9: 309–347.
DiPietro R and GD Hager (2020) Chapter 21 - Deep learning: RNNs and LSTM. In SK Zhou, D Rueckert, and G
Fichtinger (eds.) Handbook of medical image computing and computer assisted intervention. Academic Press, p.
503–519. doi: 10.1016/B978-0-12-816176-0.00026-0
Esposito G, R Salvini, F Matano, M Sacchi, M Danzi, R Somma, and C Troise (2017) Multitemporal monitoring
of a coastal landslide through SfM-derived point cloud comparison. Photogram Rec 32: 459–479. doi:
10.1111/phor.12218
Gafoor FA, MR Al-Shehhi, C-S Cho, and H Ghedira (2022) Gradient boosting and linear regression for estimating
coastal bathymetry based on Sentinel-2 images. Rem Sens 14(19): 5037. doi: 10.3390/rs14195037
Gao J (2022) Fundamentals of Spatial Analysis and Modelling. Boca Raton: CRC Press, 346 p.
Gao J (2023) Remote Sensing of Natural Hazards. Boca Raton: CRC Press, 437 p.
Guerin A, GM Stock, MJ Radue, M Jaboyedoff, BD Collins, B Matasci, N Avdievitch, and M-H Derron (2020)
Quantifying 40 years of rockfall activity in Yosemite Valley with historical structure-from-motion
photogrammetry and terrestrial laser scanning. Geomor 356. doi: 10.1016/j.geomorph.2020.107069
Harwin S, and A Lucieer (2012) Assessing the accuracy of georeferenced point clouds produced via multi-view
stereopsis from unmanned aerial vehicle (UAV) imagery. Rem Sens 4: 1573–1599.
Hiestand R (2019) Regard3D. www.regard3d.org/index.php.
Horn BKP (1987) Closed-form solution of absolute orientation using unit quaternions. J Opt Soc Am 4: 629–642.Huang GB, QY Zhu, and CK Siew (2006) Extreme learning machine: Theory and applications. Neurocomputing
70: 489–501.
James G, D Witten, T Hastie, and R Tibshirani (2023) An Introduction to Statistical Learning: With Applications in
R (2nd ed.). New York: Springer, 604 p.
Jia K, S Liang, S Liu, Y Li, Z Xiao, Y Yao, B Jiang, X Zhao, X Wang, S Xu, and J Cui (2015) Global land surface
fractional vegetation cover estimation using general regression neural networks from MODIS surface
reflectance. IEEE Trans Geosci Rem Sens 53: 4787–4796.
Lague D, N Brodu, and J Leroux (2013) Accurate 3D comparison of complex topography with terrestrial laser
scanner: Application to the Rangitikei canyon (N-Z). ISPRS J Photogram Rem Sens 82: 10–26. doi:
10.1016/j.isprsjprs.2013.04.009
Lee C, K Lee, S Kim, J Yu, S Jeong, and J Yeom (2021) Hourly ground-level PM2.5 estimation using
geostationary satellite and reanalysis data via deep learning. Rem Sens 13(11): 2121. doi: 10.3390/rs13112121
Liu Y, Y Yin, Z Chu, and S An (2020) CDL: A cloud detection algorithm over land for MWHS-2 based on the
gradient boosting decision tree. IEEE J Sel Topics Appl Earth Obs Rem Sens 13: 4542–4549. doi:
10.1109/JSTARS.2020.3014136
López OAM, A Montesinos López, and J Crossa (2022) Chapter 9 – Support vector machines and support vector
regression. In Multivariate statistical machine learning methods for genomic prediction, pp. 337–378.
Switzerland: Springer. doi: 10.1007/978-3-030-89010-0_9
Lucieer A, SM de Jong, and D Turner (2014) Mapping landslide displacements using Structure from Motion (SfM)
and image correlation of multi-temporal UAV photography. Prog Phys Geog: Earth and Environ 38(1): 97–116.
doi: 10.1177/0309133313515293
Mapillary K (2021) OpenSfM https://github.com/mapillary/OpenSfM, Accessed: 2024-06-18.
Mateo-García G, V Laparra, and L Gómez-Chova (2018) Optimizing kernel ridge regression for remote sensing
problems. IGARSS 2018 - IEEE Int Geosci Rem Sens Symp, Valencia, Spain, 4007–4010. doi:
10.1109/IGARSS.2018.8518016
Mello MP, J Risso, C Atzberger, P Aplin, E Pebesma, CAO Vieira, and BFT Rudorff (2013) Bayesian Networks for
Raster Data (BayNeRD): Plausible reasoning from observations. Rem Sens 5(11): 5999–6025. doi:
10.3390/rs5115999
Moulon P, P Monasse, R Perrot, and R Marlet (2017) OpenMVG: Open multiple view geometry. In Kerautret B, M
Colom, and P Monasse (eds.) Reproducible Research in Pattern Recognition. RRPR 2016. Lecture Notes in
Comp Sci, 10214: 60–74. Springer International. doi: 10.1007/978-3-319-56414-2_5
Mustafa YT, A Stein, V Tolpekin, and PE van Laake (2012) Improving forest growth estimates using a Bayesian
network approach. Photogram Eng Rem Sens 78 (1): 45–51.
Peterson KT, V Sagan, P Sidike, AL Cox, and M Martinez (2018) Suspended sediment concentration estimation
from Landsat imagery along the lower Missouri and middle Mississippi Rivers using an extreme learning
machine. Rem Sens 10(10): 1503. doi: 10.3390/rs10101503
Pourghasemi HR, N Kariminejad, M Amiri, M Edalat, M Zarafshar, T Blaschke, and A Cerda (2020) Assessing
and mapping multi-hazard risk susceptibility using a machine learning technique. Sci Rep 10: 3203. doi:
10.1038/s41598-020-60191-3
R Core Team (2018) R: A Language and Environment for Statistical Computing. Vienna: R Foundation for
Statistical Computing. www.R-project.or.g/
Rossel RAV (2007) Robust modelling of soil diffuse reflectance spectra by “bagging-partial least squares
regression”. J Near Infra Spectro 15: 39–47. doi: 10.1255/jnirs.694
Schönberger JL, and JM Frahm (2016) Structure-from-motion revisited. IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), Las Vegas, NV, USA, 4104–4113. doi: 10.1109/CVPR.2016.445
Schulz E, M Speekenbrink, and A Krause (2018) A tutorial on Gaussian process regression: Modelling, exploring,
and exploiting functions. J Math Psych 85: 1–16. doi: 10.1016/j.jmp.2018.03.001Silva A, M Mello, and LMG Fonseca (2014) Enhancements to the Bayesian Network for Raster Data (BayNeRD).
Proc Brazilian Symp GeoInfo: 73–82.
Snavely N, SM Seitz, and R Szeliski (2008) Modeling the world from internet photo collections. Int J Comput Vis
80: 189–210. doi: 10.1007/s11263-007-0107-3
Song KS, L Li, S Li, L Tedesco, B Hall, and LH Li (2012) Hyperspectral remote sensing of total phosphorus (TP)
in three central Indiana water supply reservoirs. Water Air Soil Poll 223: 1481–1502. doi: 10.1007/s11270-011-
0959-6
Specht DF (1991) A general regression neural network. IEEE Trans Neural Net 2(6): 568–576. doi:
10.1109/72.97934
Swayze NC, WT Tinkham, JC Vogeler, and AT Hudak (2021) Influence of flight parameters on UAS-based
monitoring of tree height, diameter, and density. Rem Sens Environ 263, 112540. doi: 10.1016/j.rse.2021.112540
Sweeney C (2016) Theia Multiview Geometry Library: Tutorial and Reference. http://theia-sfm.org/#
Verrelst J, G Camps-Valls, J Muñoz-Marí, JP Rivera, F Veroustraete, JGPW Clevers, and J Moreno (2015) Optical
remote sensing and the retrieval of terrestrial vegetation bio-geophysical properties – A review. ISPRS J
Photogram Rem Sens 108: 273–290. doi: 10.1016/j.isprsjprs.2015.05.005
Verrelst J, J Muñoz, L Alonso, J Delegido, J Rivera, G Camps-Valls, and J Moreno (2012a) Machine learning
regression algorithms for biophysical parameter retrieval: Opportunities for Sentinel-2 and -3. Rem Sens Environ
118: 127–139.
Verrelst J, L Alonso, G Camps-Valls, J Delegido, and J Moreno (2012b) Retrieval of vegetation biophysical
parameters using Gaussian process techniques. IEEE Trans Geosci Rem Sens 50 (5 PART 2): 1832–1843.
Verrelst J, L Alonso, J Rivera Caicedo, J Moreno, and G Camps-Valls (2013) Gaussian process retrieval of
chlorophyll content from imaging spectroscopy data. IEEE J Selected Topics Appl Earth Obs Rem Sens 6(2):
867–874.
Van der Meer F (2004) Analysis of spectral absorption features in hyperspectral imagery. Int J Appl Earth Obs
Geoinfo 5: 55–68.
Warsito B, R Santoso, Suparti, and H Yasin (2018) Cascade forward neural network for time series prediction. J
Phys: Conf Ser 1025: 012097. doi: 10.1088/1742-6596/1025/1/012097
Westoby MJ, J Brasington, NF Glasser, MJ Hambrey, and JM Reynolds (2012) ‘Structure-from-Motion’
photogrammetry: A low-cost, effective tool for geoscience applications. Geomor 179: 300–314. doi:
10.1016/j.geomorph.2012.08.021
Wikle CK, LM Berliner (2007) A Bayesian tutorial for data assimilation. Phys D: Nonlinear Phenom 230(1–2): 1–
16. doi: 10.1016/j.physd.2006.09.017
Woodget AS, PE Carbonneau, F Visser, and IP Maddock (2015) Quantifying submerged fluvial topography using
hyperspatial resolution UAS imagery and structure from motion photogrammetry. Earth Surf Process Landf
40(1): 47–64. doi: 10.1002/esp.3613
Wu C (2011) VisualSFM: A Visual Structure from Motion System. http://ccwu.me/vsfm/
Wu C (2013) Towards linear-time incremental structure from motion. 2013 Int Conf. on 3D Vision - 3DV 2013,
Seattle, WA, USA, 127–134. doi: 10.1109/3DV.2013.25.3D
Zhang Y, Y Qu, J Wang, S Liang, and Y Liu (2012) Estimating leaf area index from MODIS and surface
meteorological data using a dynamic Bayesian network. Rem Sens Environ 127: 30–43. doi:
10.1016/j.rse.2012.08.015Part II
Environmental Applications5
5.1
Quantification in the Terrestrial Sphere
DOI: 10.1201/9781003517504-7
Comprising about one-fifth of the Earth’s surface, the terrestrial sphere is the homeland of the mankind where
nearly all human activities are confined. This sphere exerts a decisive impact on human health and life quality. The
major components of the terrestrial sphere that humans directly interact with and influence are land and soil. The
land surface may be hilly or mountainous, and vulnerable to catastrophic disturbances, such as landslides and
volcanoes. Although the land surface is in a stable condition most of the time, occasionally, external forces can
cause it to shift either laterally or vertically in a number of natural and anthropogenic processes. Improper
excavation of the ground can cause landslides and surface creeping. Irrational extraction of land-based resources
(e.g., mining) can trigger environmental pollution and ground surface subsidence. Soil is the earth material that
sustains plant growth and supplies food and nutrients to meet human needs. Improper exploitation of soil and
mineral resources has led to soil salinization and contamination.
This chapter elucidates the quantification of surface parameters, both static and dynamic, in the terrestrial
sphere. It is important to quantify the surface properties and movement in order to understand the current state of
soil health and the pace of earth material flow. Such information can shed light on the potential hazard imposed by
the mobile earth materials to nearby properties and human lives. It helps identify vulnerable parts of the terrestrial
surface so that preventative measures can be implemented to revert the eventuation of natural disasters. Naturally,
the terrestrial sphere has been widely studied. These studies fall into three categories of soil quality (including soil
pollution) quantification, thickness and volume estimation, and derivation of surface movement (including both
horizontal and vertical) velocity. The nature of the quantification target dictates the best remote sensing data and
the appropriate method to use, and the achievable accuracy of quantification.
The topics covered in this chapter encompass land surface and soil pertaining to their physical and biochemical
conditions and properties. The two objects of study differ from each other in how best they should be quantified
remotely. In particular, soil is related to agricultural areas and is best studied when the ground is devoid of
vegetative cover or when crop is still at its infancy. Therefore, they must be elaborated in separate sections. The
first part of this chapter discusses how to quantify the physical state of land surface, such as temperature (including
lava temperature), heat flux, and surface albedo. The next section focuses on soil properties. The parameters to be
quantified are diverse biochemical traits, including soil salinity and salt content, soil organic carbon (SOC), and
soil contaminants. Then the discussion progresses to the estimation of debris volume, including lahar, lava, and
snow avalanches. Last, this chapter expounds how to quantify surface movements (both horizontal and vertical),
including the rate of surface deformation using both imagery and non-imagery data, especially active SAR data.
SURFACE PHYSICAL PARAMETERS
Surface physical parameters apply to both soil and land features. Soil physical properties depict the physical state
or condition of the soil, including texture, structure, density, porosity, moisture, and temperature or latent heat flux.
Although soil thickness or volume is also related to soil physical conditions, its quantification is deferred to
Section 5.3 as it is achieved using totally different data and methods. In order for these parameters to be quantified
from remotely sensed data, they must have their unique spectral signature that can be differentiated from that of5.1.1
5.1.2
other features existing in the same environs. The distinctiveness of their spectral reflectance pattern from that of
other features is instrumental to the accuracy of the retrieved parameter values. So far only two soil parameters
(temperature and moisture) have been successfully quantified from remote sensing data. In order to understand the
best spectral bands to use for their quantification, it is imperative to understand how the nature of surface (e.g., soil
or land) affects their spectral behavior.
Spectral Behavior of Soil
Bare soil has a moderate reflectance that is proportional to wavelength (Figure 3.4). It can appear bright at NIR
wavelengths. Dry soil has an especially high reflectance, but damp soil’s reflectance is much subdued due to strong
absorption by moisture. Soil can have low red reflectance (like vegetation), but the difference between NIR
reflectance and red reflectance for soil is much smaller than for live vegetation. Soil reflectance is affected mainly
by three components: color, surface roughness, and moisture content. The influence of color is difficult to
generalize as it varies with wavelength. In general, soil rich in iron such as loam tends to reflect more red light
than brown soil. Dark soil rich in humus absorbs more incident energy than light soil, leading to less reflection of
the incident energy back to the sensor. There is an inverse relationship between soil moisture content and
reflectance because more moisture absorbs more incident radiation, leaving less energy available for reflection.
Soil moisture absorbs mostly mid-infrared radiation at wavelengths ⁓1.42 μm and 1.86 μm (Figure 3.4). Besides,
surface conditions also exert an effect on reflectance. A rough surface causes the incident energy to be diffused
more than reflected (see Figure 3.3c), and vice versa. Variations in any one of the soil parameters give rise to a line
in a 2D scattergram. In the red-NIR scattergram, this is termed the “soil line,” and is used as a reference point in
most vegetation studies.
Land Surface Temperature
Surface temperature plays a critical role in air mass circulation and surface energy balance, both of which are
crucial to accurately modeling the atmosphere. Surface temperature can be further broken down into terrestrial and
aquatic types, the latter commonly being referred to as sea surface temperature (SST). While the quantification of
both follows the same principle of TIR sensing, the method and accuracy of quantification vary vastly between
them, so they are discussed separately in this book, with SST estimation deferred to Section 6.8.
Temperature is differentiated into kinetic and radiant, depending on how it is measured. The former refers to the
temperature measured at contact with the target. It is equivalent to the energy stored in it, produced by molecular
motion. The latter is the temperature detectable from remotely sensed data at a distance to the target. The radiant
temperature (TR) of a land surface bears the following relationship with its kinetic temperature (TK):
(5.1)
where ε = emissivity.
The quantification of land surface temperature (LST) from remotely sensed data requires the use of thermal
bands if the temperature is low (e.g., <100°C), or visible near-infrared (VNIR) bands when the temperature is high
because thermal bands have a lower saturation temperature than their VNIR counterparts. In thermal sensing, a
reference object is needed to gauge the quantity of thermal energy emitted by the target. This reference object is
served by the blackbody, a hypothetical object that completely absorbs all the incident energy (mostly short
wavelengths) to it and emits all the absorbed energy in long wavelengths without any loss during the conversion
from shortwave to longwave radiation. According to Planck’s blackbody radiation law, the spectral radiant
exitance L (W ∙ m–2 ∙ μm–1) of an object is related to its radiant temperature Trad in Kelvin in the form of:
(5.2)
where h = Planck’s constant (6.626068 × 10–34 J ∙ s
–1); c = speed of light (2.9979246 × 108 m ∙ s
–1); k =
Boltzmann’s constant (1.380662 × 10–23 J ∙ K–1); and λ = central wavelength of a spectral band (μm). Radianttemperature (Trad) is inverted from the spectral radiance (Lλ) recorded in a spectral band using the Planck’s
equation as:
(5.3)
where C1 and C2 = the first and second radiation (calibration) constants. They are expressed as C1 = 2hc²/λ5 and C2
= hc/(kλ). Their exact values vary with the spectral band of satellite imagery used. For Landsat TM band 6, C1 =
3.74151 × 108 m–2 μm4, C2 = 1.43879 × 104 μm K.
LST is commonly quantified from satellite data via inversion of the radiative transfer equation. Since the
sensor-captured radiance is affected by the atmosphere temperature, the quantification must take it into account by
measuring atmospheric sounding data synchronized with the satellite transit time to estimate the atmospheric
effects on land surface thermal radiation. The subtraction of such atmospheric effects from the total thermal
radiation observed by the spaceborne sensor yields the land surface radiation intensity. It is converted to the
corresponding LST in two steps: (i) conversion of the TIR radiance recorded in a spectral band L to the at-sensor
brightness temperature (Tb) using the following equation:
(5.4)
where K1 and K2 = calibration constants with values varying with the spectral bands and the sensor, for instance
K1 = 774.89/480.89 and K2 = 1321.08/1201.14 for Landsat 8 bands 10/11; (ii) calibration of Tb to LST via surface
emissivity. If calculated using Landsat 5 and 8 TIR bands, LST (K) can be derived using the following equation:
(5.5)
where (σ = Boltzmann constant, and h = Planck´s constant); λ = wavelength of the emitted radiance, and ε =
surface emissivity with a typical value of 0.95.
LST in urban areas has been retrieved from Landsat 8 OLI data (Figure 5.1). The main difficulty with this kind of
quantification is the large uncertainty involved as in situ temperature is not routinely obtainable at the time of
sensing, especially on the municipal scale.5.1.3
FIGURE 5.1 Land surface temperature of Shenzhen, China in Kelvin derived from Landsat 8 OLI data using the
radiative transfer equation. (Wang et al., 2019a, open access.)
Urban Heat Flux
As the global climate keeps warming, cities around the world are experiencing an enhanced urban heat island
phenomenon whose intensity is measured by urban heat flux. It is calculated from the LST determined using the
aforementioned method. Since urban areas are composed of complex surface covers, each having its own
temperature, especially vegetated and concrete covers, the precise quantification of heat flux must make use of
cover-specific models. A popular model is the dual-source heat flux model (Liu et al., 2017). It decomposes the
heat flux of mixed pixels into vegetated and non-vegetated components that cannot be accurately separated from
one-time satellite data observed at a single angle. One way of determining the sensible heat flux H is to use the
effective impedance method as:
(5.6)
where ρ = air density; Cp = specific heat of the air at a constant pressure; Ts = surface temperature; Ta =
atmospheric temperature; Ra_veg and Ra_non-veg = aerodynamic resistance values of vegetated and non-vegetated
pixels, respectively. It is calculated as:
(5.7)
where u = wind speed at a given height obtainable from meteorological data; Zm = height of wind measurements,
Zh = height of humidity measurements, d = zero-plane displacement height; Z0m = roughness length governing
momentum transfer; Z0h = roughness length governing the transfer of heat and vapor, k = von Karman’s constant.
Land surface impedance (Rs) in Eq. 5.6 in the boundary layer is calculated as:
(5.8)
where a = free convective velocity, b = coefficient representing the typical soil surface roughness, and us = wind
speed over the soil surface at a height of 0.05–0.20 m.5.1.4
Surface latent heat (LE) can be estimated from the vegetated fractional cover (VFC) (see Section 6.3.1 for more
details) using two-endmember spectral unmixing analysis as:
(5.9)
where LEveg and LEnon veg = latent heat fluxes corresponding to vegetated and non-vegetated areas, respectively,
calculated as:
(5.10)
(5.11)
where ea = atmospheric water vapor pressure, eo = saturation vapor pressure; γ = the psychometric constant. This
method requires the area under study to be classified into a binary map of urban (non-vegetation) and vegetation
covers, usually from Earth observation satellite data such as Landsat 8/9 OLI (Figure 5.2).
FIGURE 5.2 Latent heat flux (in W × m–2) distribution of Shenzhen, South China in September 2013 quantified from
Landsat 8 data. (Wang et al., 2019a, open access.).
Lava Temperature
Although the quantification of lava temperature follows the same principle as that of urban heat islands, both the
suitable images and the method of quantification differ vastly between them for two reasons: (i) Lava fields have a
much smaller spatial extent that requires images of a fine spatial revolution to resolve; (ii) Actively ejected lava
has a temperature usually an order of magnitude higher than that of urban heat islands. A higher temperature can
cause saturation on thermal bands recorded over the 8–14 μm wavelengths. For instance, the TIR band (band 6) of
Landsat 8 OLI imagery has a temperature saturation range of 49°C – ⁓110°C, much lower than the temperature of
lava fields. The higher temperature has to be quantitatively estimated using a spectral band of a shorter wavelength
that tends to have a lower saturation temperature, such as SWIR bands. They are able to detect pixel-scale
temperatures as high as 1,000°C without saturation. Some coarse-resolution SWIR bands have such a high
sensitivity to lava radiant temperature that enables small lava fields comprising only a small fraction of the imagepixel to be detected. This is because the lava-triggered thermal anomalies manifest themselves much larger than
the actual size of the lava fields on a TIR image if their temperature reaches 970–990°C.
In accordance with Wien´s displacement law, the peak radiation shifts to a shorter wavelength at a higher
temperature. The high temperature of a lava field is best quantified from NIR bands with a saturation temperature
of 973°C. Temperatures higher than this threshold have to be estimated from a spectral band of a shorter
wavelength, and vice versa. Even visible spectral bands may have to be used to retrieve searing lava temperature.
Since each spectral band has a limited range of sensible temperature, the broad temperature range of cooling lava
flows may be quantified fully from multiple bands individually, and the individual fields are then merged to form
the final temperature distribution map of the entire flow field.
The quantification of lava surface temperature from remotely sensed images using Eq. 5.5 is a complex process
and requires tremendous work. The thermal data must be pre-processed to isolate the volcanogenic signal from the
atmospheric effects. The ability to isolate the volcanogenic signal and the accuracy of isolation depend on the
spectral wavelength range of the thermal band in use, time of image acquisition, surface temperature, and the
physical size of the lava field. After the original digital number of pixels in spectral band i is converted to spectral
radiance (Li), it must be compensated for the absorption by the atmosphere (refer to Section 3.2.1 for more details).
Atmospheric gaseous matters of various types inevitably absorb some radiance leaving the volcanic field. The
degree of absorption is a function of wavelength, and varies with the gaseous components, such as Ch4, CO2, water
vapor, O3, CO, N2O, O2, and aerosols. It can be simulated via the MODTRAN atmospheric model (refer to Section
3.6.1 for details) that is able to correct the wavelength-dependent atmospheric effects, so SWIR and TIR bands are
corrected differently as:
(5.12)
(5.13)
where and = corrected spectral radiance at wavelengths and , respectively. and = at-sensor spectral radiance, RR =
spectral radiance contributed by the atmospheric gaseous matters, τ = atmospheric transmissivity; and ε = surface
emissivity (it can be set to 0.97).
The corrected spectral radiance may be further compensated for the contamination by reflected sunlight if the
images are captured during daytime (not necessary for nighttime images). Pixel-wise correction may be undertaken
using the reflectance of adjacent cold lava pixels (e.g., inactive lava). The temperatures of active and cooling lava
flows can be approximately estimated by inverting the Planck’s equation from spectral radiance captured on night￾time optical images with the assistance of image-specific calibration constants. Commonly used images are
Landsat 8 and ASTER bands. The temperatures retrieved from Landsat 8 blue, green, red, panchromatic, and NIR
bands have slightly different ranges (Figure 5.3), due to the differential behavior of the thermal radiance of active
lava flows in different bands. For instance, in Landsat panchromatic band 8 (0.52–0.90 μm), it concentrates
heavily in the upper reaches of the bandwidth but very little in the lower region (Wright et al., 2001), in violation
of the assumption that the measured radiance has an even distribution around the central wavelength of the
bandwidth. This is quite different from near-infrared band 4 (0.75–0.90 μm). Consequently, band 8 retrieves a
whole pixel temperature of 922°C (28.3 W ∙ m–2 ∙ sr–1 ∙ μm–1), >100°C higher than 819°C (35.7 W ∙ m–2 ∙ sr–1 ∙
μm–1) retrieved from band 4.5.1.5
FIGURE 5.3 Distribution of surface temperature of lava flows in Holuhraum, Iceland retrieved from blue, green, red,
panchromatic, and NIR bands of Landsat 8 OLI imagery. (Nádudvari et al., 2020, open access.)
The accuracy of the retrieved temperature is affected by a number of factors, the most important being saturation
of pixel values, pixel size, and the exact value of ε that is a function of surface roughness, composition, and
temperature. Whatever ε value is adopted, it inevitably affects the accuracy of radiometer-measured temperatures.
Finally, atmospheric conditions, such as haze, water vapor, and cloud cover also exert an influence. The accuracy
will be higher if their effects are eliminated completely from the used image via atmospheric correction.
Surface Albedo
Albedo is a biophysical parameter of land surface depicting the amount of radiative energy reflected off ground in
relation to the incident energy. Surface albedo is defined as the ratio of the reflected radiation off the surface to the
total incident radiation at the same wavelength. Unless specified otherwise, albedo refers to the portion of energy
reflected from a target over the entire spectrum, so it differs from reflectance that is always related to specific
wavelengths. Albedo ranges from 0 (for blackbody) to 1 (complete reflection), with most objects’ albedo varying
between these two extremes. Typical values are 0.05 for water, over 0.8 for vegetation, and >0.95 for fresh snow.
The albedo of an object is affected mainly by the wavelength of the incident radiation, called spectral albedo. It is
commonly captured by VNIR spectral bands. Albedo plays an important role in several atmospheric processes,
such as surface energy balance, precipitation, climate variability, and warming. While albedo can be readily
measured in the field using a hand-held spectrometer positioned towards the target of interest quickly and reliably
(see Section 1.2.2.1 for details), it cannot generate a field view of albedo to meet the needs of climate modeling.
Thus, it must be quantified from satellite images.
Remotely sensed albedo is defined as the directional reflectance incorporating all possible viewing and
illumination angles in a period. Image-based retrieval of albedo relies on the empirical relationships between insitu measured total shortwave albedo and satellite imagery data or radiative transfer simulations. Established from
a limited number of surface reflectance spectra, these conversion relationships are likely to be sensor-specific and
have a limited range of applicability. They can be further classified as total shortwave albedo and longwave
albedo. The former is calculated from shortwave spectral (e.g., visible) bands while the latter includes more bands
in the infrared spectrum. Virtually, it is a linear combination of pixel values in the bands used. As an example, the
total visible albedo is calculated from the linear combination of three MODIS bands as (Liang, 2001):
(5.14)
where α1, α3, and α4 = spectral reflectance value recorded in MODIS bands 1, 3, and 4, respectively, or bands of a
comparable wavelength range from other satellites. Such calculated albedo has nearly a perfect agreement (R2 =
0.9999, RMSE = 0.0017) with the results obtained from extensive radiative transfer simulations under various
surface and atmospheric conditions. But its accuracy drops slightly to R2 = 0.839 in comparison with ground
validation data (Figure 5.4). While this method of quantification is simple, it lacks scientific vigor as it does not
take into account the atmospheric effects. Since all the validation sites are located in the U.S., its global
applicability remains unknown. In addition to wavelength and the atmospheric properties, albedo is also affected
by surface properties (including color, roughness, orientation, porosity, and moisture content), angle of the incident
energy and viewing geometry, and in case of vegetation, the internal structure and chlorophyll content of plant
leaves and canopy geometry of forests.FIGURE 5.4 Comparison of in situ measured total visible albedo against model-predicted broadband total visible land
surface albedo from MODIS bands. (Adapted from Liang et al., 2003, used with permission (5761100545816) from
Elsevier.)
Physical-based retrieval of albedo is usually accomplished by inverting the BRDF served by a semi-empirical
kernel-based reflectance model following atmospheric correction, and/or optional sensor harmonization (Carrer et
al., 2021). The calculation of albedo involves angular integration of bi-directional reflectance factors and narrow￾to-broadband conversion. Apart from TOA radiance in several spectral bands and radiometric uncertainties for
each band, the derivation of albedo as a data product still requires the input of diverse auxiliary data that include (i)
the sensor and solar viewing and azimuth angles, (ii) image acquisition time, (iii) longitude and latitude, (iv) a
quality flag including land/water mask, (v) a binary cloud mask and a snow/ice mask, and (vi) atmospheric
components such as ozone, water vapor, pressure and aerosols, atmospheric correction coefficients, and DEM.
Some of them are available from the satellite data themselves. Others need to be retrieved from other satellite data
or from existing data products. Physical modeling is a pragmatic and cost-effective way of estimating albedo from
an extensive range of satellite data in near real-time, but it is rather complex and requires the input of a large
number of parameters whose accuracy affects the reliability of the retrieved albedo.5.1.6
Global albedo products have been produced by Copernicus Climate Change Service (C3S) of the EU from various
satellite data collected from 11 sensors for almost four decades. Up to three different albedo products are available
from one sensor. The SPOT.VGT albedo product is produced from archived Collection 3 of SPOT/VGT TOA
reflectance data in band B0 (blue, 0.43–0.47 µm), B2 (red, 0.61–0.68 µm), B3 (NIR, 0.78–0.89 µm) and SWIR
bands of a nominal resolution of 1 km × 1 km (1/1120) after radiometric correction and inter-sensor calibration
between SPOT4/VGT-1 and SPOT5/VGT-2. This product is generated every 10 days using a compositing window
of 20 days. The dates of production are the 10th, 20th, and 30th of each month with the C3S-V1 product (Figure
5.5). Product C3S-V0 indicates only the date of the mean age of the observations as product name instead of the
production date. The current product version is 2.
FIGURE 5.5 Total shortwave directional-hemispherical albedo (BB-DH) for 1 July 2015 based on PROBA-V (C3S￾V0). (Carrer et al., 2021, open access.)
Soil Moisture
Soil moisture is a main driver of water and heat flux between the land surface and the atmosphere, and a regulator
of air temperature and humidity. Surface soil moisture plays an important part in energy exchanges between the
Earth and the atmosphere. Accurate information on soil moisture is instrumental in modeling regional or global
weather and understanding the physics of land surface and the atmospheric processes. In agriculture, soil moisture
is a critical indicator of the growth environment and soil quality. As a key land surface parameter, moisture is
particularly important to the healthy growth of crops. The soil lacking moisture is stressful for plants and may stunt
their growth, and will likely reduce crop yield if the condition persists over a prolonged period. Soil moisture
content differs from soil water content that refers to the moisture below the surface, usually associated with the
percolation of rainwater. Such below-surface content cannot be estimated remotely due to the inability of the
radiative energy used to penetrate the ground. It is the surface moisture that can be quantified from remotely
sensed data. Surface moisture varies vertically, spatially, and temporally, and can be efficiently retrieved from
spaceborne multispectral images at multi-scales repeatedly. Global surface moisture can be quantified from various
satellites synchronously, frequently, and economically.
Table 5.1
Comparison of retrieving soil moisture from three types of remotely sensed data
Data Methods Advantages Disadvantages
Optical Visible data • Multispectral bands • Subject to weatherData Methods Advantages Disadvantages
• Fine spatial resolution • Coarse temporal resolution;
• Prone to vegetation interference
TIR based
• Fine spatial resolution data
from multi-satellites
• Same as above
• Prone to atmospheric effects
Passive
microwave
Physical and (semi-)
empirical modeling
• High temporal resolution
• High accuracy for bare soil
• Unaffected by clouds
• Coarse spatial resolution
• Subject to the impact of
vegetation cover and surface
roughness
Active
microwave
Physical and (semi-)
empirical modeling
• Fine spatial resolution
• Unaffected by clouds and/
or daytime conditions
• Subject to influence of surface
roughness & vegetation
• Coarse temporal resolution
• Complex
Synergistic
Optical + infrared
• High spatial resolution
• Simple, straightforward
workflow
• Restricted to cloud-free &
daytime conditions
• Coarse temporal resolution
• Shallow depth of penetration
Active + passive
microwave
• Improved temporal and
spatial resolutions
• Soil moisture scaling &
validation needs caution
• Different depths of moisture
measurement
Optical + microwave
• Minimized vegetation &
surface roughness effects • Same as above
Source: Modified from Rathore et al. (2021).
Different satellite data have different strengths and weaknesses in estimating soil moisture using different methods.
They are grouped into optical, infrared, and microwave (including scatterometer) data. Optical imagery can detect
the relative water content of the top few centimeters, but microwave imagery can detect the degree of wetness in
the topmost layer, expressed in percent saturation. The pros and cons of each type of remote sensing data in the
quantification are compared and contrasted in Table 5.1. Soil moisture can be sensed most effectively from NIR
bands as water strongly absorbs the incoming radiation, resulting in a reduced return of the incident energy to the
sensor, if not completely absorbed by the target. There is an inverse relationship between the intensity of the
returned radiation and moisture level (Figure 5.6). A higher moisture content corresponds to lower reflectance over
the wavelength range of 400–2,500 nm consistently (Mu et al., 2023), and vice versa. This inverse relationship
between the two is commonly exploited to quantify soil moisture. There are several troughs in the spectral
reflectance curve, with the most enhanced one occurring at around 2,000 nm. These wavelengths can serve as the
potential predictors of soil moisture. Nevertheless, the accuracy of the retrieved soil moisture is subject to the
influence of multiple factors, including the spectral bands used and the timing of sensing, as well as the vegetation
cover over the soil.FIGURE 5.6 Relationship between soil moisture content and reflectance over the spectral range of 400–2,500 nm.
(Mu et al., 2023, open access.)
In order to take advantage of the rich spectral information recorded in multispectral bands, soil moisture is
frequently quantified using vegetation indices. So far various indices have been proposed to quantify soil moisture,
most of which are derived from climatic data for indicating the level of drought, such as normalized difference
water index and soil water index. The latter quantifies precipitation-induced moisture conditions of soil at various
depths, related to the process of infiltration. They are beyond the scope of this chapter as they cannot be remotely
sensed. Remote sensing-based estimation of soil moisture is commonly achieved using the soil moisture index
(SMI) derived from TIR bands based on its relationship with emissivity and LST. The estimation of SMI from LST
follows the same philosophy as the derivation of NDVI and is calculated from multispectral bands as (Potić et al.,
2017):
(5.15)
where LST = land surface temperature of a pixel derived from its value or transform in multiple bands (see Section
5.1.2); LSTmax and LSTmin = maximum and minimum surface temperature in a given spectral band. They are
calculated using the following two equations, respectively:
(5.16)
(5.17)
where ai and bi (i = 1 and 2) = slope and intercept of the regression models, defining both dry and wet (warm and
cold) edges of the data (Mu et al., 2023). They are also called the coefficients and offsets that are empirically
determined via linear regression. The calculated SMI has a continuous value within the range of 0–1, with a larger
value close to 1 representing a higher moisture level. The application of Eq. 5.16 and 5.17 to the relevant bands
produces a map showing the spatial distribution of soil moisture (Figure 5.7). This map can be used to assess the
necessities for irrigation or devise proper crop management strategies in agriculture.FIGURE 5.7 Spatial distribution of soil moisture generated from data of different spatial resolutions. (Mu et al., 2023,
open access.)
In estimating soil moisture, optical imagery is limited by its incapability to sense the target deeper down the
surface and in the presence of clouds. Both deficiencies can be effectively overcome with the use of microwave
data. Radar-based estimation is grounded on the fact that subtle variations in soil moisture alter soil emissivity and
microwave backscattering. Water is the only naturally abundant medium with a high dielectric constant, so a
higher portion of liquid water in soil increases its dielectric properties, thereby significantly altering its scattering
and absorption behaviors. Through changing the complex permittivity of the soil, soil moisture can be retrieved
from radar data of various frequencies, such as P-, L-, C-, and X-band images. Both active and passive radar can
estimate soil moisture only in the top 5 cm layer globally using either physical, empirical, or semi-empirical
models of various levels of complexity. Physical models based on the RTM are complex and require several inputs
that are not readily available. In contrast, alternative empirical models developed for specific radar data are simpler
with few variables, such as the multi-temporal data-based change detection approach proposed by Wagner et al.
(1999). This method requires scatterometer data in the dry and wet seasons to determine relative soil moisture
(RSM) as:
(5.18)
where RSMt = relative soil moisture at time t, = backscatter coefficient at time t, = backscatter coefficient of soil in
the dry season, = backscatter coefficient of wet condition, θref = reference incidence angle.
Requiring a reference incidence angle, this method works only when there are wet and dry conditions, so it is
restrictive. In comparison, the normalized backscattering moisture index (NBMI)-based method developed by
Shoshany et al. (2000) is more generic as moisture is calculated from only backscattering coefficient (s°) and its
response at two times (t1 and t2), expressed as:5.2
5.2.1
(5.19)
The accuracy of the retrieved soil moisture is affected by various factors, subject to the data used. At a micro-scale,
slope and surface cover both interfere with soil moisture. The uncertainty of microwave-derived soil moisture is
related to the dielectric properties of the soil phases that vary with soil texture, soil salinity, and vegetation cover.
Soil moisture has been retrieved from remotely sensed data at widely ranging scales. In general, the broader the
scale, the more reliable the retrieved results. Global-scale soil moisture is usually quantified from images of a very
coarse spatial resolution acquired from purposely-designed satellites, such as the Soil Moisture Active Passive
(SMAP) mission and Soil Moisture and Ocean Salinity (SMOS) satellite missions. They have been used to derive
soil moisture data products that are accessible via the internet. The SMAP product is produced at three resolutions
of 3, 9, and 36 km and a temporal resolution of 2–3 days by merging high-resolution active radar data with coarse
resolution, but highly sensitive, passive radiometer data. However, only operational for nine months, the radar
sensor malfunctioned.
Another moisture product has been generated from the Global Passive Microwave (PMW) data. The version
03.2 moisture product is supplied at 0.25° spatial resolution by the ESA (www.esa-landcover-cci.org/). It is created
using a rigorous scheme that combines all level 2 soil moisture retrievals from available sensors into a (single)
weighted optimal estimate using their corresponding error variances. All PMW soil moisture products are
generated using the same land parameter retrieval model that interprets brightness temperature measurements of
several sensors. The third very coarse resolution level 2 moisture product is that derived from C-band (5.255-GHz)
ASCAT radar data acquired by the ESA MetOp satellite (http://hsaf.meteoam.it/soil-moisture.php). This product
has a spatial resolution of 25 km × 25 km that is resampled onto a regular 0.25° grid spacing of 12.5 km using
inverse distance weighting. Active microwave soil moisture retrievals from the 9:30 a.m./p.m. (local solar time)
overpasses are averaged to derive a daily product from January 2007 onward. The retrieved moisture has been
filtered for frozen soil conditions based on auxiliary flag information.
SOIL BIOCHEMICAL QUALITIES
The quality and health of farmland soil directly affects crop yield and quality. Healthy and fertile soil is conducive
to a higher yield, whereas soil suffering from contamination may have a reduced yield or even result in crop failure
in the worst case. The most commonly quantified soil biochemical qualities include soil salinity, salt content, SOC,
and soil contaminants.
Soil Salinity
Quantitative estimation of soil salinity is grounded on the fact that salt modifies soil texture, moisture content,
organic matter, and surface roughness. In turn, these alternations cause the soil surface to respond differentially to
the incident energy which is discernible from the captured spectral reflectance curve of the soil. Field-measured
spectral reflectance over the spectral range of 0.50–2.50 μm bears a positive correlation with bare soil salt content
(Figure 5.8). The higher the concentration, the higher the reflectance, and vice versa. The spectral reflectance
curves of soils containing widely ranging levels of soil salt share a general pattern that resembles each other except
at the extremely low level (Figure 5.8a). The existence of spectral disparity at different levels of salt concentration
on the ground, nevertheless, can never guarantee the feasibility of quantifying the exact level of salt content at a
satisfactory accuracy from space-borne satellite data because of four reasons: (i) The distance of sensing is much
further from space than from the ground, over which the sensor-captured salt signal is degraded by aerosol and
molecule scattering, and possibly absorbed by water vapor during its propagation from the target to the satellite;
(ii) The sensor’s radiometric resolution may not be sufficiently fine to enable the subtle spectral variation of low
salt contents to be discerned adequately; (iii) The recorded satellite imagery may have a coarse spectral or spatial
resolution to differentiate the spectral subtlety at a sufficiently high accuracy; and (iv) The spectral signal of soil
salt may be contaminated by the interference of the spectral reflectance of ground vegetation above it or in its
vicinity. Thus, bare soil salt tends to be estimated at higher accuracy than vegetated soil salt.FIGURE 5.8 Spectral reflectance curves of bare soil at four levels of salt content (left) measured in the field using a
spectrometer and the corresponding soil appearance on the ground (right). (Wang et al., 2019b, open access.)5.2.1.1
FIGURE 5.28 Spatial distribution of landslide deformation velocity detected from multi-temporal Sentinel-1 InSAR
images. GPS – Global Positioning System; PSI - Persistent Scatterer InSAR feature points. (Zhou et al., 2020, open
access.)
Utility of Spectral Bands
Soil salinity, as measured by electrical conductivity (EC) or the ability of soil water to carry electrical current, can
be quantified from space-borne multispectral or hyperspectral images even to a low concentration level at high
accuracy. A finer spectral resolution is conducive to the achievement of more accurate quantification, such as that
offered by the 12 multispectral bands of Sentinel-2 imagery. The spectral response of low EC levels does not vary
noticeably with Sentinel-2 spectral bands except for a slight decline in band 12. In general, the longer the
wavelength, the higher the reflectance except at the extremely high EC level at which it fluctuates mildly with
wavebands. Different wavebands have disparate utilities in estimating soil EC levels (Table 5.2). Of the six
Landsat OLI spectral bands, band 4 bears the closest correlation of 0.384 with in situ observed EC, closer than the
three VNIR bands. Together, their correlation coefficient with EC is higher than that of the two TIR bands that are
much less effective. NDVI derived from 20 m resolution SPOT multispectral imagery is also a poor predictor of
soil salinity.
Table 5.2
Correlation coefficient between EC values and Landsat TM spectral bands, indices, and environmental variablesVariables Elevation Slope Band1 Band2 Band3 Band4 Band5 Band7 NDVI NDSI SAV
r
–
0.181
–
0.138 0.250 0.290 0.256 0.384 0.128 0.058
–
0.145 0.145
–
0.13
Note: Italic: significant at p-value < 0.05; boldface: significant at p-value < 0.001.
Source: Elnaggar and Noller (2010), open access.
In fact, all individual spectral bands over the wavelength range of 0.40–2.30 μm have a compromised ability to
estimate soil salt content individually (Wang et al., 2018a), irrespective of the type of imagery (Table 5.3).
Although blue and green bands are the best, they can obtain a maximum R² value of only 0.24, with a RMSE
stubbornly >14.18 g ∙ kg–1. Thus, single bands are unable to individually predict soil salt adequately. The
realization of a higher estimation accuracy has to rely on the use of multi-bands. For example, the estimation
accuracy of EC is improved to R² > 0.5 (Eqs. 5.20 and 5.21) from the joint consideration of two bands of Landsat
OLI and Sentinel-2 MSI images. Sentinel MSI bands 2 and 4 enable EC to be estimated more accurately than their
OLI counterparts because their 10 m spatial resolution is thrice finer than 30 m of OLI bands (Davis et al., 2019).
Overall, the superior spatial and temporal resolutions of MSI to those of OLI enable EC to be estimated more
accurately judging by the higher adjusted R² (0.67) and RMSE (1.17) than OLI’s R² (0.51) and RMSE (1.15).
Thus, at the same spectral resolution, higher estimation accuracy is obtainable from finer spatial resolution bands
probably because of their better capability of resolving the spatial variability of soil salt than coarse resolution
images:
ECOLI = 2.0080 + 0.0698B2 – 0.0156B4 (adjusted R² = 0.51) (5.20)
ECMSI = 2.0065 + 0.0666B2 – 0.0156B4 (adjusted R² = 0.67) (5.21)
Table 5.3
Comparison of individual HJ-B and Landsat OLI bands in quantifying soil salinity measured by EC in the Ebinur Lake wetland of Northwest
China
Imagery Band Wavelength Prediction model R² RMSE RPD
HJ-B CCD
Blue 0.400-0.520 177.6x – 4.04 0.17 14.77 1.08
Green 0.520-0.600 266.9x – 12.78 0.22 14.41 1.11
Red 0.630-0.690 176.5x-7.3 0.13 14.94 1.07
NIR 0.760-0.900 119.2x-7.6 0.13 15.13 1.06
Landsat OLI
Blue 0.450-0.515 120.3x-8.19 0.24 14.18 1.13
Green 0.525-0.600 105.6x-9.14 0.23 14.27 1.12
Red 0.630-0.680 85.4x-5.9 0.18 14.76 1.08
NIR 0.845-0.885 73.7x-6.35 0.16 14.98 1.07
SWIR1 1.560-1.660 65.45x-4.51 0.15 15.05 1.06
SWIR2 2.100-2.300 74.94x-5.2 0.15 15.01 1.06
Source: Modified from Wang et al. (2018a), with permission (5761100978993) from Elsevier.
Of the 12 Sentinel-2 MSI bands, the VNIR bands have a limited capacity to differentiate soil salinity gradients as
the spectral signal of salt is heavily confused with that of soil optical properties (i.e., color and brightness)
(Bannari et al., 2018). In comparison, SWIR bands are better and produce a closer correlation of R² = 0.50 (SWIR￾1) and 0.64 (SWIR-2) with EC than VNIR bands (R²£9%), including the red-edge and NIR bands. Thus, these two
bands are the most promising in competently quantifying soil salt contents of a wide range. SWIR bands have a
higher sensitivity to low and moderate contents of soil salts than other bands (Bannari et al., 2018). It must be
emphasized that these findings are based on the results produced by singular bands individually. The performance
of the same band is improved by combining it with other bands, as is commonly practised in deriving salinity5.2.1.2
indices from multiple bands, including both VNIR and SWIR bands (see Section 5.2.1.2 below). As indicated by
the two models in Eqs. 5.20 and 5.21, they both achieve a higher R² value than all those models involving a sole
predictor variable in Table 5.3, suggesting that the use of two spectral bands enable much more accurate estimation
models to be constructed than those based on a sole band, irrespective of its wavelength.
Compared to multispectral bands, hyperspectral data allow salinity to be quantified more accurately using the
first derivative reflectance measured in the laboratory setting. They retain a validation R2 of 0.85 in predicting soil
salinity via bagging PLSR analysis (Mashimbye et al., 2012). Even raw reflectance (validation R2 = 0.70) and the
sole band at 2,257 nm (validation R2 = 0.60) both achieve an R2 value above 0.60. These results are obtained from
field spectra data collected using an ASD FieldSpec spectrometer. Naturally, the accuracy will be lower with
space-borne data.
Salinity Indices
In addition to individual spectral bands, a large number of indices have been developed and used to quantify soil
salinity (Table 5.4), such as the normalized difference salinity index (NDSI). It is calculated from two candidate
bands i and j for a sample (n) following the principle of the NDVI used in vegetation studies (Eq. 5.22):
(5.22)
where Ri,n and Rj,n = spectral reflectance of bands i and j for a sample n, respectively. The best settings are Ri =
[1600,1700] and Rj = [2145, 2185] (Al-Khaier, 2003).
Table 5.4
Soil salinity indices that have been used to estimate soil salinity from optical imagery data
Index Formula Sources
Normalized difference salinity index (R-NIR)/(R+NIR) Khan et al. (2001)
Brightness index (BI) Khan et al. (2001)
Salinity index (SI) Khan et al. (2001)
Salinity index (SI2) Yahiaoui et al. (2015)
Salinity index (SI-1) ALI9/ALI10 Bannari et al. (2008)
Salinity index (SI-2) (ALI6 – ALI9)/(ALI6 + ALI9) Bannari et al. (2008)
Salinity index (SI-3) (ALI9 – ALI10)/(ALI9 + ALI10) Bannari et al. (2008)
Salinity index (S1) B/R Abbas and Khan (2007)
Salinity index (S2) (B – R)/(B + R) Abbas and Khan (2007)
Salinity index (S3) G × R/B Abbas and Khan (2007)
Salinity index (S5) B × R/G Abbas and Khan (2007)
Source: Modified from Allbed and Kumar (2013), open access.
More salinity indices (SIs) have been derived from either two or three bands (Table 5.5). In general, 3-band indices
are marginally better than their 2-band counterparts, but some indices such as NDI produce a higher r than a few 3-
band indices. Although image-derived SI is closely correlated with field-measured salinity, it significantly
underestimates salinity in areas of high levels of salt concentration (Abdelkader et al., 2006).
Table 5.5
Correlation (r) of 2-band and 3-band indices with soil salinity
No. of bands Index abbreviation Optimal band combinations r
2-band indices RI B4/B12 0.41
DI B3 – B7 –0.16No. of bands Index abbreviation Optimal band combinations r
NDI (B12 – B7)/ (B12 + B7) 0.52
3-band indices
TBI1 B12/(B8a × B8a) –0.43
TBI2 B7/(B7 + B12) –0.51
TBI3 (B6 – B12)/(B12 + B11) 0.50
TBI4 (B12 – B3)/(B3 – B11) 0.54
TBI5 (B12 + B11)/B3 0.53
TBI6 (B11 – B3)/(B11 –2B3 + B12) 0.44
TBI7 B11 – 2B12 + B5 –0.49
Source: Wang et al. (2019a), open access.
With the assistance of the constructed model reminiscent of Eq. 5.20, the raw image pixel values in multispectral
bands and their ratio are transformed into maps illustrating the spatial distribution of soil salt concentration either
continuously or in a few discrete classes to show the spatial pattern of soil salt distribution (Figure 5.9).FIGURE 5.9 Detailed soil salinity distributions in three agricultural fields in the Yellow River delta of Shandong
Province of East China, derived from an EO-1 ALI Level 1B multispectral image. (a) in the north with slightly￾moderate saline soils; (b) along the Yellow River with non-saline and slightly saline soils, and (c) in the south with
moderately-to-highly saline soils. (Fan et al., 2015, open access.)
Apart from optical imagery, soil salt content can also be quantified from radar data in conjunction with optical data
using sophisticated machine learning methods. These data expand the pool of potential predictor variables to
encompass soil backscattering coefficient from Sentinel-1A SAR data, groundwater depth and SI derived from
optical imagery, and surface evapotranspiration from MODIS data product (MOD16). They are analyzed using
SVM regression that attains an R² value of 0.82 (RMSE = 2.01) using the training dataset or R² = 0.88 (RMSE =
1.36) using the testing dataset (Jiang et al., 2019). This accuracy is slightly better than R² = 0.79 and RMSE = 2.20
of ANN obtained using the training dataset, and R² = 0.68 and RMSE = 2.25 with the testing dataset (Table 5.6).
As illustrated in Figure 5.10, ANN overestimates EC with both the training and testing datasets in most cases. In
general, the integrated use of microwave and optical data, in combination with the covariates of soil salt, markedly
improves the estimation accuracy over using optical data alone or statistical analysis methods.5.2.2
FIGURE 5.10 Scatterplot of field measured EC versus the EC predicted from four variables using the ANN method by
training and test samples. (Jiang et al., 2019. © Taylor & Francis.)
Table 5.6
Comparison of SVM and ANN accuracy in mapping soil salt content measured by EC (dS×m-1)
Accuracy measure Training dataset (n = 60) Test dataset (n = 20)
Algorithm SVM ANN SVM ANN
R² 0.82 0.79 0.88 0.68
RMSE 2.01 2.20 1.36 2.25
Source: Jiang et al. (2019), © Taylor & Francis.
Soil Salt Content
Soil salt content differs from soil salinity in that it is expressed as a ratio of salt in the total volume of soil instead
of soil conductivity. If the quantification of soil salinity is challenging, the quantification of soil salt content is
even a more daunting task. As indicated previously, individual bands or their transformations do not always
accomplish satisfactory or adequately accurate quantification. So the achievement of an acceptable estimation
accuracy may require the use of many other variables (or bands), including a variety of spectral covariates, such as
salinity indices, red-edge indices, newly constructed 2D indices, and three-band indices. Such a large dataset is
ideally handled using machine learning methods, including Random Forest Regression (RFR), SVR, Gradient-5.2.3
Boosted Regression Tree (GBRT), Multilayer Perceptron regression, and Least Angle Regression (LAR). The
performance of these analytical methods in estimating soil salt has been comprehensively evaluated in a study area
having an extremely high soil salt content of 2.92–290.86 g ∙ kg–1 (Wang et al., 2019b).
A comprehensive evaluation of the five models reveals a relative performance on the decreasing order of RFR >
LAR > SVR > MLPR > GBRT (Wang et al., 2019b). As illustrated in Figure 5.11, RFR is the best performer for
high-dimensional data, attributed likely to its resistance to noise in the input data. It is almost identically accurate
to SVR that is satisfactorily stable, with the difference in accuracy between them falling probably within the range
of random variation. Both are highly comparable to LAR and Multilayer Perceptron regression in accuracy. The
worst performer is GBRT whose accuracy (R² = 0.77) is ⁓13% lower than that of the remaining four models.
Judged by all the accuracy indicators, overall, LAR is the most accurate in estimating soil content, followed by
RFR and SVR. These findings are based on field-measured spectra data. Whether they still hold with space-borne
spectral bands remain unknown.
FIGURE 5.11 Comparison of in situ measured and estimated soil salt contents from field-measured spectral data using
five analytical methods: (a) RFR, (b) SVR, (c) Gradient-Boosted Regression Tree, (d) Multilayer Perceptron
regression, and (e) Least angle regression. Accuracy indicators are obtained using leave-one-out cross-validation.
(Wang et al., 2019b, open access.)
Soil Organic Carbon
The terrestrial ecosystems are the largest reservoir of carbon on Earth, exceeding the combined carbon reserve in
both oceans and the atmosphere. SOC is a critical measure of soil fertility that affects crop growth and the carbon
exchange between soil and the atmosphere, and hence the regional and global carbon balance. It is important to
quantify SOC content as it is sensitive to global climate warming. Even minor climatic fluctuations significantly
impact atmospheric CO2 and induce a positive warming feedback. Accurate information on the broad-scale
distribution of SOC content is crucial to modeling global climate change and assessing food security. Field-scale
SOC is useful for informed carbon budgeting and guiding soil management.
Table 5.7Spectral indices from multispectral bands that have been used to estimate SOC
Index Formula
Brightness index (BI)
Second BI (BI2)
Normalized burn ratio (NBR2) (NIR-SWIR II)/(NIR+SWIR II)
SCMaP index (SWIR I – G)/(SWIR I + G)
Land surface water index (LSWI) (NIR-SWIR I)/(NIR+SWIR I)
Redness index R*R/(G*G*G)
Normalized difference soil index (NDSI) (SWIR I – NIR)/(SWIR I + NIR)
Bare soil index (BSI) [(SWIR I + R) – (NIR + B)]/[(SWIR I + R) + (NIR + B)]
Color index (CI) (R – G)/(R + G)
Transformed VI (TVI)
Green-red VI (GRVI) (G-R)/(G+R)
Vegetation index (V) NIR/R
Green NDVI (GNDVI) (NIR-R)/(NIR + R)
Soil adjusted total vegetation index (SATVI) (SWIR I – R)(1+ L)/(NIR + R + 1) – SWIR II/2
Green SAVI (GSAVI) (NIR – G) (1 + L)/(NIR + G + L)
Green OSAVI (GOSAVI) (NIR – G)/(NIR + G + Y)
EVI, NDVI, SAVI, MSAVI
Source: Zepp et al. (2021), open access.
Regional SOC is ideally quantified from medium-resolution EO satellite data in the SWIR (1.3–2.5 μm) and NIR
(0.7–1.3 μm) spectral regions as they are highly sensitive to SOC. Its content is inversely related to soil spectral
reflectance in both regions due to SOC absorption. Such a relationship facilitates SOC retrieval from individual
bands, such as red (Table 5.7). In addition, scores of spectral indices have been developed for the retrieval. Derived
from laboratory- and field-measured SWIR and NIR spectra, they are all related closely to the SOC content (Peón
et al., 2017). The effectiveness or relative competency of these indices has not been comprehensively assessed
except that the use of additional spectral indices can improve SOC modeling over the usage of reflectance data
alone (Zepp et al., 2021). Spectral indices, such as OSAVI, are even less important (importance value = 6–7%)
than singular bands, such as drone-acquired red band (importance value = ⁓8–9%) in estimating SOC (Ding et al.,
2023) while normalized difference red edge (NDRE) is even less important (importance = around 1%).
Nevertheless, SOC can still be reliably predicted from certain indices derived from multi-bands. For instance, the
ratio of Rrs(1608)/Rrs(833) is almost immaculately correlated with SOC at R2 = 0.98 for 32 soil samples in the
Palouse region of eastern Washington (Frazier and Cheng, 1989). In comparison, the SI1001–679 index [(ρ1001–
ρ679)/(ρ1001+ρ679)] from hyperspectral satellite data is less effective, attaining a maximum R2 value of only
0.56. However, both SWIR and NIR bands are sensitive to the confounding effects of soil moisture in estimating
SOC. Besides, they are not available in all sensors such as VHR satellite images. Their coarse spectral resolution is
ill-suited to quantify field-level SOC. So SOC has to be determined from visible wavebands of a fine spatial
resolution available virtually in all sensors. They allow the derivation of a new SOC index (SOCI) from three
visible bands of blue, green, and red calculated as (Thaler et al., 2019):
(5.23)
where rb, rg, rr = reflectance at wavelengths of 478, 546, and 659 nm, respectively. SOCI predicts the sampled SOC
concentrations at a RMSE of 1.5% that is remarkably comparable to RMSE = 1.3% achieved by the SWIR/NIR
ratio (Thaler et al., 2019). This index outperforms an index based on NIR and red wavelengths (RMSE = 2.8%).SOC bears a moderately close linear relationship with these indices, and it is more accurate than non-linear or
exponential relationships (with SI) as shown in the following three models:
(5.24)
(5.25)
(5.26)
The highest accuracy (R² = 0.50) is achieved for 54 level III ecoregions in the corn belt plains of the western U.S.
(n = 595), but the R² is lower in other regions owing probably to the interference of surface vegetation cover. A
similarly low accuracy is obtained from either individual bands or multiple bands and their ratio (Table 5.8). This
finding holds true even with Hyperion data. Broad band airborne data achieve slightly higher R² values, but also a
larger RMSE. The modest accuracy suggests that SOC can be reasonably predicted using simple VIs alone in
agricultural ecozones. It must be acknowledged that image-derived SOC applies to the surface only, dissimilar
from field-measured SOC carried out in a profile (e.g., 30 cm layer). So the predicted values have to be scaled to
the mean values for the top 30 cm layer of the soil. After SOC is calibrated region-wide by scaling the satellite
spectra to the same range as the Rapid Carbon Assessment laboratory spectra, the SOCI index becomes more
accurate with a RMSE of 0.54% (R2 = 0.67) in predicting field SOC values (Thaler et al., 2019).
Table 5.8
Comparison of cross-validation accuracy in retrieving SOC from air-borne and space-borne hyperspectral data based on different inputs
Data Analytical means R² RMSE (%) RPD
Airborne
(n = 39)
SLR-1 band 0.35 11.69 2.15
SLR-1 index 0.52 10.02 1.46
SMLR-3 bands 0.51 10.13 1.44
SMLR-8 indices 0.62 9.05 1.62
PLSR-3 factors 0.49 10.39 1.41
Hyperion
(n = 200)
SLR-1 band 0.34 8.55 1.24
SLR-1 index 0.23 9.26 1.14
SMLR-15 bands 0.44 7.91 1.33
SMLR-18 indices 0.49 7.58 1.39
PLSR-4 factors 0.32 8.68 1.22
Source: Peón et al. (2017), open access.
SOC has been estimated using various analytical methods, including MLR, PLSR, and machine learning, of which
Random Forest (RF) is the most capable of predicting SOC contents in Bavaria, southern Germany (R² = 0.67,
RMSE = 1.24%, RPD = 1.77, CCC = 0.78) using 30-year Soil Composite Mapping Processor (SCMaP) soil
reflectance composite of 30 m Landsat data alone (Zepp et al., 2021). This accuracy slightly surpasses the maximal
R² of 0.56 from PLSR, and 0.59 from MLR. Validation against an independent dataset reveals a mean discrepancy
of 0.11% between the measured and predicted SOC contents using the best RF model. Stepwise MLR of airborne
hyperspectral scanner data against a maximum of eight indices achieves a cross-validation R2 between 0.60 and
0.62, almost identical to R2 = 0.49–0.61, obtained using a maximum of 20 indices derived from space-borne
Hyperion data (Peón et al., 2017). Satellite data can retain the highest accuracy similar to the best of airborne data.
Stepwise MLR has a performance comparable or superior to PLSR, even using fewer bands. The application of the
best estimation model to the relevant satellite imagery data of a medium spatial resolution enables SOC to be
quantified at the field scale using vegetation indices (Figure 5.12).5.2.4
FIGURE 5.12 Field-scale spatial distribution of SOC contents in Bavaria of South Germany produced from 30 m
Landsat data using the RF model based on vegetation indices. (Zepp et al., 2021, open access.)
Drone images of four or five multispectral bands allow much fewer spectral indices to be derived, and this paucity
has been augmented by soil properties and SOC covariates that can be easily handled by SVM to estimate SOC. In
addition to red band, OSAVI, and NDRE index, they also encompass topographic variables (topographic position
index and roughness), soil texture (clay, silt, and sand), and other soil physical parameters (soil temperature, soil
volume water content, and pH), all treated as the covariates of SOC in predicting it in four representative micro￾morphologic units of an alpine meadow (Ding et al., 2023). The UAV covariates exert a moderate influence on
SOC mapping, which signals the feasibility of using UAV images in estimating local- and regional-scale SOC.
SVM is the optimal machine learning algorithm for predicting SOC content, which accounts for 53.06% (R2) of
the SOC content variation with the training data, or 51% with the validation dataset. This accuracy is almost
identical to R2 = 0.54 ± 0.12 (RPD = 1.68 ± 0.45 and RMSE = 2.09 ± 0.39 gC ∙ kg−1) obtained in the Belgian
Loam Belt using greening-up-NBR2 combination applied as a threshold for a two-year series of Sentinel-2 images
(Dvorakova et al., 2021). Soil texture parameters are the most important (76.17%), much more important than
multispectral drone data (>14.63%) to the SOC prediction accuracy.
Nevertheless, the estimation of SOC requires the extraction of bare soils from EO data, which is usually
hampered by the presence of temporal or permanent vegetation cover. Besides, it is also subject to the influence of
soil moisture and the condition of the soil surface, so it is imperative to remove vegetation effects through the use
of bare soil index and normalized burn ratio (refer to Section 6.8.3 for more details) or the temporal sequence of
NDVI binary values to detect the transition between vegetation and exposed soil (Dvorakova et al., 2021).
Soil Contaminants
With increasing industrialization and open-pit mineral exploitation, the soil and land on Earth have been subject to
heightened exposure to contaminants by industrial and mining wastes either onsite or offsite through rainwater.
Toxic chemicals are evaporated into the atmosphere and subsequently precipitated to the land, polluting the soil.
Soil contamination can also stem from the use of tainted water for irrigation, or chemical residuals of pesticides
and herbicides repeatedly applied over decades. Soils in close proximity to mining sites or in the lower stretches of
a river channel inside the mining catchment can also be polluted by heavy metals from surface runoff originating
from mining sites. The contaminants are absorbed by crops to enter the food chain, ending up eventually in human
blood vessels and adversely impairing human health. The most damaging heavy metal contaminants are cadmium
(Cd), lead (Pb), copper (Cu), and zinc (Zn), all being detrimental to the environment, as well.The quantification of soil heavy metals follows the same principle and methodology as the quantification of soil
salt content and SOC, because heavy metals have a spectral response behavior quite distinctive from that of soil.
More critically, heavy metal concentrations tend to be extremely low, causing their spectral response to be highly
subtle and minute, and hence the utmost difficulty in detecting metal-triggered reflectance anomaly. The spectral
response of soil cadmium and zinc concentration is the most sensitive at 1,850 nm and first derivatives at 950 and
2,154 nm corresponding to the smectite absorption features (Shin et al., 2019). Lead concentration is closely
related to the first derivatives at 1,453, 2,316, and 2,337 nm, which are absorption features of chlorite, tremolite,
and talc, respectively. The feasibility of quantifying heavy metal concentration in the soil has been repeatedly
confirmed using in situ collected hyperspectral data at a spectral resolution measured by microns or nanometers.
The selection of the most sensitive bands for deriving spectral indices for metal detection is founded on the most
distinctive behavior of the heavy metal content captured in hyperspectral data. For instance, soil copper abundance
is sensitive to the spectral index of 2209 nm/907 nm (Yin et al., 2021). Since metals have distinctive absorption
features at certain wavelengths, the quantification usually takes advantage of narrow bands, from which a variety
of vegetation indices (VI) have been derived (Table 5.9). In addition to VIs, two more parameters have been
calculated from hyperspectral spectra: absorption depth and area (see Figure 4.13 for details).
Table 5.9
Indices that have been proposed to estimate soil heavy metals using remote sensing and their calculation formulas from multiple spectral bands
Index Formula Reference
Normalized pigments chlorophyll index
(NPCI) (R600-R460)/(R600+R460) Blackbun (1998)
Structural independent pigment index (SIPI) (R810-R460)/(R810+R460) Penuelas et al. (1995)
Simple ratio, ratio VI (NIR/R)
R870/R950, R750/R550),
R725/R675 Hunt and Rock (1989)
Perpendicular VI (PVI)
Richardson and Wiegand
(1977)
Red edge vegetation stress index (RVSI) Morton and Huntington (1999)
Modified triangular VI2 (MRVI2) Haboudane et al. (2004)
VIs considering greenness and SWIR (VIGS) Hede et al. (2015)
NDVI, SAVI, OSAVI, MCARI
Source: Adapted from Wang et al. (2018b).
Soil zinc, cadmium, and lead concentrations near a hydrothermal ore deposit are statistically correlated with some
spectral responses (Shin et al., 2019), such as the ratio of 610 nm to 500 nm (r610/r500) in the visible and VNIR
spectral range, absorption area at 2,200 nm (Area2200), and asymmetry of the absorption feature at 2,200 nm
(asym2200) (Choe et al., 2008). The image-derived r1334/r778 and Area2200 parameters are weakly correlated with
the ground spectral parameters values of Zn and Pb (R2 > 0.5) while depth500, r1334/r778, and area2200 derived from
image pixels are comparable with ground-derived spectral parameters along a section of the polluted stream
channel. The ground-derived spectral parameters bear a reliable and quantitative linear regression relationship (R2
≥ 0.6) with in situ sampled heavy metal levels.
The estimation models of metal concentration have been constructed using MLR, PLSR, and adaptive neural
fuzzy inference system (ANFIS) modeling. Of these methods, MLR is able to establish close and statistically
significant relationships between the concentration of individual metals and their spectral manifestation (Table
5.10). Stepwise MLR models of zinc, cadmium, and lead have a statistically significant R2 > 0.7 value (Shin et al.,
2019). PLSR may be implemented as piecemeal or universal for the entire dataset. If all the samples are used to
construct the estimation model, it has a moderate accuracy of R2 = 0.58 (Eq. 5.27). The model using the image￾derived band ratio has a low RPD value of 1.18, revealing a feeble prediction power of the model over the entire
range of concentration (Yin et al., 2021):(5.27)
This model has a RMSEP of 41.20 mg∙kg–1. However, piecemeal PLSR models are more accurate if they are
constructed from two partitioned whole sample groups based on a selected threshold (e.g., 100 mg∙kg–1)
separately. The two models, one for predicting high (anomalous) (Eq. 5.28) and another for low (background) (Eq.
5.29) concentrations, are applied to the same image twice, producing two distribution maps. They are merged to
form a single prediction map of Cu distribution:
(5.28)
(5.29)
Table 5.10
Comparison of accuracy indicators in retrieving soil heavy metal content based on spectral parameters derived from field-measured spectra
Metal Predictor variable F P-value R² SEEa RPDb SDVc
Stepwise multiple linear regression
log(Pb) R610,500d 38.341 0.000 0.530 0.382 1.362 0.520
log(Zn) log(Area2200) 30.169 0.000 0.470 0.243
log(Area2200), R610,500 5.528 0.025 0.546 0.228 1.395 0.318
log(As) Asym2200 72.901 0.000 0.682 0.170
Asym2200, R610,550 21.607 0.000 0.808 0.134
log(Area2200), R610,550, Asym2200 7.082 0.012 0.843 0.123 2.386 0.293
log(total) R610,550 49.500 0.000 0.593 0.288 1.456 0.419
Enter multiple linear regression
log(Pb) Depth500, R610,550 7.727 0.000 0.615 0.374 1.390 0.520
log(Zn) R1344,778, Depth2200 7.132 0.000 0.596 0.230 1.387 0.318
log(As) log(Area2200) 34.258 0.000 0.876 0.114 2.562 0.293
log(Total) Asym2200 8.965 0.000 0.650 0.289 1.447 0.419
Source: Choe et al. (2008) with permission (5761110145231) from Elsevier.
Notes:
a Standard error of estimate b Rate of prediction deviation c Standard deviation of validation set d
Subscripts - wavelength in nm.
It must be cautioned that the established relationship based on field spectra may not be replicated with space-borne
hyperspectral data in predicting the spatial distribution of heavy metal concentration for the same four reasons as
those identified in estimating soil salt content (refer to Section 5.2.1). For instance, the model developed from
laboratory soil spectra has a very high R2 and RPD of 0.89 and 2.81, respectively (Yin et al., 2021), but they
decrease noticeably to R2 = 0.58 and RPD of 1.18 with the model based on hyperspectral bands. Most of the
relationships between metal concentration and the predictor variables either disappear or become significantly
weakened with space-borne HyMap hyperspectral images (Choe et al., 2008).
The spatial distribution of metal concentration is generated by applying the best established estimation model to
the multispectral or hyperspectral data from which the predictor variables are derived to construct it. The
quantification is successful only when the concentration level is high (28.4–302.81 mg∙kg–1) for Cu using
piecewise P-PLSR models (Yin et al., 2021), further restricted to areas not obstructed by vegetative cover. The
spatial distribution of Cu content is estimated at an acceptable accuracy of R2 = 0.83 and RPD = 1.56. Evaluated
against the ground truth, the P-PLSR model achieves a higher R2 (0.89) and a smaller RMSEP (21.40 mg∙kg–1)
than the holistic model, with the RPD value being 2.81. The background soil samples with Cu < 100 mg∙kg–1 are
all clustered well below 100 mg∙kg–1 with anomalous soil samples spread along the 1:1 line.However, accuracy drops markedly in estimating heavy metal concentration in channel sediments from HyMap
images (Choe et al., 2008). Hyperspectral imagery allows the concentration to be quantified at discrete levels in
different sections of the channel instead of a spatially continuous distribution (Figure 5.13). One cause for the
lower accuracy is the influence of water. Water interferes with the soil (and heavy metal) spectral reflectance and
thus degrades the reliability of the quantified results.
FIGURE 5.13 Spatial patterns in gradient maps for (a) total concentration, (b) Pb concentration, in streams down a
mining site in SE Spain retrieved from HyMap data using the best estimation models shown in Table 5.10. (Choe et
al., 2008, with permission (5761110145231) from Elsevier.)
Compared with hypersepctral (and very high resolution) data, multispectral Landsat data are much inferior in
retrieving Cu (0–400 mg∙kg–1, mean = 68.65), Pb (0–200 mg∙kg–1, mean = 163.52) and As (0–100 mg∙kg–1, mean
= 34.8) concentration levels in a mining site (Yang et al., 2021). The estimation models for Cu have the highest R2
= 0.59 after incorporating topographic factors. The highest R2 value is only 0.35 for Pb and 0.42 for As. Only
seven out of 14 spectral reflectance or indices (e.g., NDVI, greenness, MNDWI, differential VI, EVI, and wetness)
are significant for Pb, one fewer than for Cu. Surprisingly, none of them are statistically significant for As. But the5.3
5.3.1
highest correlation coefficient is only 0.518 for Cu and 0.428 for Pb. These values are well below their
counterparts in Table 5.10. Naturally, the low accuracy of quantification is also related to the metal concentration
level. In general, it is much easier to achieve a higher accuracy with higher concentrations, and vice versa. In
Choe’s study, the concentration levels are much higher for Cu (46.3–584.5, mean = 129.7 mg∙kg–1 mpp), Pb (96.0–
18,811.3, mean = 1412.7), Zn (61.3–728.1, mean = 200.9), and As (62.3–826.9, mean = 292.2). These levels are
several orders of magnitude higher than Yang’s case. Besides, multi-band ratios are used as the explanatory
variables. Moreover, the HyMap image has a spatial resolution of 4 m, much finer than 30 m of Landsat imagery.
However, it remains unknown which factor is the most important to the improved accuracy: a finer spatial
resolution, a finer spectral resolution, or a much higher concentration of heavy metals. More research is needed to
answer this question.
DEBRIS THICKNESS AND VOLUME
Debris thickness refers to the span between its base and top. It is synonymous to the change in surface elevation
induced by mass movement of earth materials. This change can be triggered by external processes such as rainfall￾induced landslides (negative change) and volcanism that deposits lava on the former surface (positive change).
Surface change is related directly to the level of damage and the difficulty of post-disaster recovery and restoration
of the destroyed ecosystem and landscape. For instance, the silts deposited on a farm will likely suffocate the
existing crops. The debris produced by a landslide can bury houses on its path. Therefore, it is frequently
imperative to quantify debris volume for the purpose of assessing the degree of destruction. Debris volume that is
3D in nature is very difficult, if not impossible, to estimate in the field. It is derivable from debris thickness that
cannot be easily determined using even ground penetration radar that has a limited range of penetration and does
not yield a field view of debris thickness. However, this task can be realized relatively easily from a pair of time￾lapsed images or two sets of LiDAR data of the same area, even to a very high accuracy. One of them must be
acquired prior to the event and another post event. Ideally, both of them should cover the same spatial extent at the
same spatial resolution (if not, they must be unified during data preparation).
Landslide Debris Thickness
Landslides refer to the downward displacement of a block of earth material under the force of gravity either
abruptly (e.g., slope failure) or slowly (e.g., surface creeping). The displacement can occur in the form of slide,
slump, flow, and creeping, triggered by earthquakes, rainfalls, and engineering works. The destruction induced by
a landslide to the environment and ecosystem is related to the volume of debris generated, which in turn, is
dictated by debris thickness. Remote sensing-based estimation of debris thickness relies primarily on elevational
change of the ground surface before and after the landslide event. Conceptually, it can be expressed as a difference
between two heights (Eq. 1.4).
The quantification of elevational change at the same spot but different times is ideally accomplished via two
DEMs. In order for Eq. 1.4 to work, the two DEMs must meet the following three requirements: (i) DEM height
must be referenced to the same geodetic system, such as WGS84 elliptical surface; (ii) DEM spatial resolution
must be identical. Ideally, the DEMs are constructed from the same data source acquired using the same method so
that data inaccuracy will partially cancel out during DEM subtraction. If not, the DEM grid cell size has to be
unified via spatial interpolation, and (iii) The two datasets must be accurately geo-referenced separately before
they can be superimposed with each other. The precise spatial co-registration of the two DEMs with each other
guarantees that the height change is detected at the same spot. Any spatial misalignment between the two DEMs
will cause artificial changes and degrade the reliability of the detected change in height. After these three
requirements are met, the subtraction is implemented pixel-wise and applied to all the grid cells in the DEM. Since
only the displaced earth experiences changes in its height, the stationary ground should have a difference of zero,
which does not affect the final results. In other words, the subtraction can effectively distinguish debris from intact
surface, and there is no need to make special efforts to delineate the spatial extent of the area affected by the
displaced debris.The disparity in height at the same grid cell (∆h) derived from differencing of DEMs (DoD) can be positive or
negative. If the difference between the current and historic DEMs is negative, it indicates that erosion has taken
place in the head zone where sediment is eroded or depleted. Otherwise, it signifies deposition or accumulation of
lava or the mobilized debris in the run-out zone, or lava flows onto the former surface. It is unlikely that the
number of Dh > 0 grid cells is equal to that of Dh < 0 grid cells because some of the debris could have been
washed down the landslide chutes to end up in lowlying channels. This elevational discrepancy indicates the mass
transfer from the erosional zone to the tracks and eventually to the depositional zone.
After the change in surface relief has been determined via DEM subtraction, it may be visualized graphically by
color-coding the magnitude of change to illustrate the spatial extent of the affected area. The initially detected Dh
showing continuous change in elevation may be categorized into more than ten classes to illustrate the spatial
pattern of elevation change (Figure 5.14), with erosion rendered in one color of various shades, and deposition in
another color. The changed part may be superimposed with aerial photographs or satellite images to illustrate the
relationship between the magnitude of change and surface land cover.
FIGURE 5.14 Change in surface elevation in three periods, detected from multi-temporal LiDAR data-derived DEMs.
Red – erosion; blue – deposition. (Baldo et al., 2009, with permission (5761110435452) from Elsevier.)
The DEMs essential to estimating landslide debris thickness may be constructed from images acquired via drones,
aircraft, and even satellites. In order to detect surface height at the time of sensing, drone images must be
stereoscopic to allow a 3D view of the surface to be established using the photogramemtric method or InSAR with
radar images. InSAR is an approach of gathering elevational information over a spatial extent from a pair of
interferometric radar images using complex data processing algorithms. Non-imagery data such as LiDAR are
preferred as they cover the area of study at a high point density and can yield accurate estimates without complex
data processing. LiDAR data may be acquired airborne or terrestrial. Airbone LiDAR data are perfectly capable of
detecting the thickness of multiple debris flows in the same area, but their accuracy is lower than terrestrial laser
scan. Due to the limited viewing range from a fixed vantage point, terrestrial LiDAR is suitable for quantifying the
debris thickness and volume of singular landslides that hang over a steep slope and that are fully visible from a
single vantage point. The challenge facing LiDAR estimation of landslide debris thickness is its limited accuracy
achievable. The rate of success, defined as the ratio of landslide areas to non-landslide areas encompassed in the
detected elevation change, is 79% for large-scale landslides and debris flows (e.g., over 10 m deep by 100 m wide)
in areas of a moderate slope, but drops to 65% in steep-sloped areas (Tsutsui et al., 2007). The accuracy of
elevation difference as measured by RMSE is affected by slope gradient. The RMSE lies between 4–5 m on slopes
<30° but rises to 5–6 m on slopes >30° in moderate topography and further to 5–9 m in steep terrain.
The quality of the detected change in elevation may be further refined via comparing the elevations in areas
where no known landsliding has taken place. Theoretically, the two elevations should be identical in the pre- and5.3.2
post-event DEMs. If not, their discrepancy represents the systematic offset between the two sets of height and can
be used to calibrate all other differences. However, this method of calibration does not work if the area under study
is covered by vegetation whose height grows with time. In this case provision needs to be enacted to accommodate
vegetation growth (e.g., ∆h < 0.2 m) or by eliminating vegetation from ∆h calculation through masking. The
LiDAR-generated DEM elevation inevitably involves a degree of uncertainty. In the presence of shadow,
shadowed areas and those suffering from spatial mismatch due to imprecise image co-registration can be excluded
from estimation to boost accuracy. The uncertainty of LiDAR-estimated debris thickness has not been budgeted in
detail. Without such information, it is difficult to broaden the application areas in which LiDAR data can be
competently used to estimate the debris volume of the minimum landslide features and to yield accurate
information on landslide movement.
Debris Volume
After debris thickness has been determined, it can be easily converted to debris volume. The volume of debris/lava
is calculated from the total number of erosion pixels multiplied by their area, derived from the product of ∆h and
the DEM grid cell area, or:
(5.30)
where R = spatial resolution or grid size of the DEM used, and n = total number of landslide grid cells whose
elevation has unchanged (e.g., Dh≠0). The above equation just shows the net change in the volume of the displaced
debris. This estimation can be further differentiated for depletion (∆h < 0) and depositional (∆h > 0) cells. The
volume of eroded debris is estimated using the same equation for those grid cells whose ∆h is >0 for the
depositional (accretion) zone, or <0 for the erosional zone (Figure 5.16). In theory, these two values should be
identical; any discrepancy between them must stem from processing or data inaccuracy unless some has been
removed out of the area of study.FIGURE 5.16 Changes in surface elevation caused by sediment shift within a channel, estimated from repeat-pass
Lidar-derived DEMs in the lower Tahoma Creek basin, Mount Rainier of Washington State over two periods.
(Anderson and Pitlick, 2014, free access.)
Core to the quantification of debris volume is the construction of two DEMs or DSMs from suitable data, such
as InSAR images, stereoscopic aerial photos taken from drones and airplanes, and LiDAR data. Drone images with
a superfine spatial resolution are suitable for detailed and accurate quantification. LiDAR data, after proper
processing, enable DEMs to be constructed directly from the 3D point clouds quickly. However, two-time LiDAR
data of the same area may not be available owing to the short history of LiDAR data existence. In this case,
LiDAR data can be combined with data gathered using other traditional 3D methods. Common candidates for
synergy are digital photogrammetric maps, ortho-rectified historical aerial photographs, radar interferograms, and
even existing DEMs. They are complementary with LiDAR data in their resolution, accuracy, coverage, and time
of acquisition. The inferior positioning accuracy of airborne LiDAR data may be refined using GPS data that are
useful in mosaicking multiple LiDAR scans to fully cover the area of study in one huge tile, even though it is less
accurate than a single scan tile. If combined with GPS, LiDAR can potentially determine the volumetric evolution
of a landslide complex, such as translational landslide evolving into mudflows, and numerically evaluate landslidemorphological evolution (Baldo et al., 2009). Once the data are properly prepared, the detection of vertical
displacement is easily implemented as an algebraic operation based on Eq. 1.4.
If debris volume is estimated from drone images or photographs, they are preferably processed using SfM-MVS
photogrammetry that can typically process hundreds of MVS photos from UAVs (refer to Section 4.4.1 for details).
Drone images of a cm-level resolution are crucial to the precise estimation of landslide volume. In SfM MVS, a
minimum of three GCPs is needed per project as opposed to per image, and they can be added directly onto
individual images or onto the orthorectified mosaic of all images. This drastically reduces the workload, and also
enables the use of historic aerial photographs. Drones are particularly suitable to estimate the volume of rock falls
via taking horizontal images in the air. From drone images of steep rock slopes prior to failure reconstructed using
SfM, past rockfall volumes can be accurately calculated via comparing against more recent SfM or TLS data
(Guerin et al., 2020). With SfM, debris volume is calculated from closed meshes, during which a surface is
generated from the points extracted from the SfM cloud automatically and transformed to a triangular mesh.
Rockfall volumes are calculated as the sum of the tetrahedron volumes contained inside a closed mesh generated
from the reference mesh and the point cloud to which it is compared (Figure 5.15).FIGURE 5.15 Volume of rockfalls estimated from helicopter-acquired oblique photographs and TLS data using SfM￾based smoothed comparison, separated into five temporal intervals. (A) Volume of rockfalls ranging from 0.02 to
20,193 m3 detected between Oct. 1976 and Oct. 2016 against the topographic surface of 2016 TLS mesh textured
with a gigapixel panorama. (B) Enlarged scar area of the 22 Feb. 2000 rockfall shown in Panel A. (C) Detail of the
progressive 2015–2016 rockfall scar area framed in Panel A. Volume and timing of the numbered rockfalls: 1 =
2844 m3; 2 = 991 m3; 3 = 316 m3; 4 = 248 m3; 5 = 329 m3. (Guerin et al., 2020, with permission (5761110722076)
from Elsevier.)
The uncertainty in the calculated volume is affected by the chosen LoD95% (which includes the registration
error and the residual noise) and the surface area of each rockfall, particularly its geometric features, and the
spatial resolution of the point clouds. It is inversely proportional to the volume itself. If the volume of a mass
movement material is tracked temporally using UAV SfM, the uncertainty of the volume change (EΔVi) can be
assessed from the standard error (SE) over stable terrain (SEstable) weighted by the hypsometry of the mass
movement, and expressed as:
(5.31)5.3.3
where n = number of independent pixels included in the DEM differencing, and its value depends on the original
number of pixels (Ntot), pixel size (PS), and spatial autocorrelation (r), or:
(5.32)
where r is estimated from the input images and expressed in pixels (Robson et al., 2022). The actual distance in
meters is obtained by multiplying r by pixel size. The SE for each r elevation band in meters equals the product of
it by the area of the mass movement materials in that band (Bolch et al., 2011). It is then summed up to derive
EΔVi as:
(5.33)
where Ai = area of the ith grid size. The uncertainty of the estimated rockfall volume is reported to vary between
13.0% and 39.8% (average value: 20.7%) with archival SfM, and between 4.2% and 36.0% (average value: 15.3%)
with recent SfM (Guerin et al., 2020). On average, volumes are overestimated by a factor of 1.4 (or a factor of
nearly 30 in one exceptional case). Overall, the quantified rockfall volumes become increasingly accurate with
more recent data, with an error range less than a factor of 1.5 since the late 1990s.
Sediment Budget
In order to assess river health on the catchment scale, it is vital to understand sediment budget and transfer.
Sediments in a watershed or catchment can be transferred around during rainfall events, usually from up-slope to
downstream areas either in the catchment or in the channel. The quantification of sediment volume follows the
same principle as that of landslide debris volume estimation except the skip of the delineation of the landslide
chute. Since the estimation is carried out at the catchment level, only catchment boundary needs to be delineated.
The same method of differencing DEMs created from LiDAR datasets recorded at different times can also be used
to estimate sediment yields from landslides. In addition to landslide-triggered debris, sediments are also secreted
by erosion in a much slower process but more extensive spatially than landslides. Thus, the two-time remote
sensing data used must be temporally separated by a sufficient interval before the accumulated sediments can be
detected to a creditable accuracy level, such as a year. Besides, the vertical change induced by erosion is much
smaller than debris thickness, and the displaced sediments are widely scattered in the catchment, which demands
that the used data have the highest vertical accuracy possible, or at least an order of magnitude higher than the
minimum elevational change to be detected.
Subtraction of the two DEMs constructed from multi-temporal LiDAR data quantifies the amount of sediments
originating from landslides and the total amount of post-event sediment discharge (Matsuoka et al., 2008). The net
change in all pixels’ height represents the amount of sediments lost to the catchment (Figure 5.16). However, it
remains unknown how much of it remains in the catchment, and the portion that has been discharged to the
downstream channel unless the channel bed is dry and exposed (flooded channels are less accurately mapped even
with bathymetric LiDAR). If some sediments end up in channels submerged underneath water, the only data useful
for their quantification is bathymetric LiDAR. It is able to penetrate the water to reach the channel floor (refer to
Section 7.2.2.2 for more information) or the data acquired at a low water level when most of the channel bed is
exposed. If the study area happens to be vegetated, the vertical change of vegetation caused by natural growth
should also be factored into the quantification. This can be done by setting a minimal change threshold, such as
0.02 m. All changes within this threshold are construed to be caused by random error or tree growth. In order to
minimize potential inaccuracy this threshold may introduce, it is best to map vegetation first and then strip it off
the image or assign a value of 0 to their height. After the change in elevation has been determined, it may be
overlaid with slope gradient maps produced from LiDAR surveys to quantify and reconstruct sediment movement
patterns over various sections of a landslide, and reveal the erosion rate for the source zone (Liu et al., 2012). In
addition, the trends and characteristics of landslides can be detected and their deformation accurately measured
(refer to Section 5.4.3 for more details).5.3.4 Volcanic Deposits
Volcanic deposits can be differentiated into lava, tephra, and lahar. Theoretically, their thickness can also be
quantified using the same method as that used to estimate debris thickness. The only difference between these
features and landslides is the temporal interval. They are triggered by special events that occur on a temporal scale
of hours or days instead of months or years. Thus, the two images should be recorded at a short interval. Since lava
and lahar are highly confined in their spatial distribution, commonly located at the top of a volcanic crater with a
steep slope, and some may be situated inside deeply incised gullies or crevices, images of a finer resolution are
essential for their accurate detection and thickness estimation. In order to minimize the effects of topographic
shadows, the images much be corrected for the topographic effects.
However, if the area of study also contains snow whose surface elevation may be lowered via melting and
sublimation, the lava fields must be first mapped from remotely sensed data so that the comparison in elevational
change is restricted to only these targets. Actively depositing lava has a searing temperature. The retrieval of sub￾pixel temperature of cool and hot components of a lava field is possible with the dual-channel method involving
two infrared bands to formulate two equations from the simultaneous solution of the Planck equation in each band
as shown in Eqs. 5.12 and 5.13. This dual-channel method works only if the SWIR and TIR bands are available.
This method is also able to determine crust thickness (∆h), convective flux, and radian flux, all from respective
models (Figure 5.17).
FIGURE 5.17 The thickness of lava flow of the Etna volcano detected from differencing DEMs created from multi￾temporal LiDAR data. Negative differences mean ablation (e.g., melting of snow and ice). Positive differences
represent lava deposits over the pre-deposit terrain. (Behncke et al., 2016, free access.)5.3.5
The detection of tephra and lahar thickness can follow the same principle and methodology as lava thickness
estimation. The only disparity is its spatial configuration as it is confined to deep gullies, so the DEM constructed
from LiDAR data or stereoscopic drone photos must have a finer spatial resolution. The quantification is much
easier if the area under study is not vegetated or covered by snow to guarantee that all changes in elevation are
attributed solely to the eruption events. If the area is vegetated, and the pre- and post-event data have a long
temporal separation (e.g., months), then the change in elevation (Dh) can also be caused by vegetation growth.
Such changes of no interest must be differentiated and eliminated from the target grid cells, such as by mapping
vegetation based on VI and then masking it out.
Accurate information on the total volume of ejected lava is significant to the realistic simulation of a lava flow.
Lava (debris) volume can be estimated using two methods, planimetric and topographic. The planimetric method
calculates the volume by multiplying lava thickness determined from ground surveys by the lava flow area mapped
from remote sensing images. This method is rather crude and the accuracy of the estimated volume is subject to the
uncertainties inherent in both the measured thickness and mapped area (Albino et al., 2015). Owing to the high
spatial heterogeneity of lava thickness over the underlying micro-topography, lava thickness measured at lava
edges can scarcely represent the genuine thickness and its spatial variability. In comparison, the topographic
method detects lava thickness based on the surface elevation change before and after the eruption. This method is
more accurate, but it is also more complex methodologically, and its accuracy depends on the DEM vertical
precision.
Snow Depth and Avalanche Volume
Snow depth in this section refers specially to fresh snow and snow avalanches. Snow depth may appear
insignificant as it just indicates the amount of precipitation in the solid form. However, the volume of a snow
avalanche, a highly destructive natural hazard in alpine areas is related exponentially to its damage on
infrastructure and risk to human lives. Avalanche volume is calculated from snow depth. The estimation of fresh
snow depth follows the same principle as that of landslide debris thickness using airborne sensing. The acquired
time-lapse data can reveal temporal changes in the sensed area, such as snow cover depth. Snow depth of an
avalanche is more accurately estimated using ground sensing than airborne sensing. Ground sensing is a viable and
effective method if the images are acquired from a strategically located vantage point that maximizes the coverage
of the target area with minimum hidden areas. The distance between the sensor and the target can range from a few
meters to 100s m, or should be sufficiently remote not to be hit by the avalanche that might eventuate in the
interim of ground sensing. A longer distance from the target enables more ground to be sensed. Even with a long
distance of sensing, ground monitoring has a limited FOV, and not all the sensed area is fully exposed and visible
from a single point.
This method is inexpensive and easy to implement, and the estimation can be implemented in real-time. If two
cameras are deployed to take stereoscopic photographs of the same target or with the use of SfM photogrammetry,
it is possible to construct a 3D model of the covered area. This model may be explored to identify the interactions
between snow distribution and terrain, and calculate the volume of an avalanche and the deposition area. The
disadvantages of terrestrial sensing are the frigid environment that demands short maintenance intervals and
unfavorable working conditions such as short daylight and frequent cloud cover. A ground surface may be
constructed from stereoscopic aerial photographs or LiDAR data. Airphotos are disadvantaged in that they may be
contaminated by clouds at the time of snowing. Actively sensed data such as LiDAR and InSAR are not affected
by clouds and hence better choices.
Dissimilar to ground photographs, non-graphic terrestrial LiDAR data are excellently posed to detect snow
depth by capturing surface height at the time of sensing. Two scans (one before and one after the snow avalanche
event) are essential to fulfil this task. It may be necessary and beneficial to set up the snow-free reference dataset
in a summer TLS survey. In case of multi-scans, they need to be tiled to form a large cover with the assistance of
distinct ground features. For ongoing quantification and monitoring, it is advantageous to establish a permanent
mount to minimize scan location uncertainty, together with at least one reflector tie point that remains stationary5.4
throughout the season. It can eliminate the need for manually identifying identical features common to adjacent
scans to serve as the tie points to stitch up multiple scans.
The scanned LiDAR point-cloud data need to be rasterized at a uniform cell size (e.g., 0.1 m or 0.25 m) for all
the scans. The disparity between the scanned surface and the reference surface at each grid cell represents snow
depth. Time-series maps of the snow surface can be produced from repeated scans from the same position and
orientation. The subtraction of the constructed snow surface from the reference surface produces time-series maps
of snow depth and its change at a high resolution (Deems et al., 2015). The accuracy of estimating snow depth
from TLS data is about 10 cm at a sensing distance of 500 m (Prokop et al., 2013).
As with debris mass balance, LiDAR enables the detection of snow mass balance in different zones and along
slide paths via subtraction of two DEMs constructed from multi-temporal LiDAR data. As illustrated in Figure
5.18, snow mass change can be differentiated into loss in the starting zone (zone of origin or the source zone) and
slide path (zone of transition) of the released avalanches, mass gain in the runout zone or the debris areas, and
calculation of avalanche debris volume based on the changed snow cover mass balance detected from TLS of the
same slope under both snow-free and snow-covered conditions (Deems et al., 2015). The main factor affecting the
accuracy of TLS detection is the presence of trees and shadow produced by protruding rocks. So this method
works much better in above tree-line areas.
FIGURE 5.18 Change in surface snow height (dHS in meters) between 23 January and 1 February of 2014 induced by
a single avalanche at point X, detected from two-time terrestrial laser scans, illustrating the mass gain (reddish) and
mass loss (greenish) areas. (Deems et al. 2015, with permission (5761111047967) from Elsevier.)
SURFACE MOVEMENT
Although the terrestrial surface is stagnant and immobile in a stable environment, some parts of it can move
horizontally at a small pace, such as debris creeping. Surface movement is broadly defined as changes in either
horizontal position or vertical elevation. Vertical shift pertains to the small movement of the Earth surface
associated with earthquakes and underground mineral extraction. It is different from debris thickness that refers to
the newly displaced slump of earth. Here vertical movement refers specifically to the change in surface elevation.
Traditionally, surface movement has been studied using digital photogrammetric maps, ortho-rectified historical5.4.1
5.4.1.1
5.4.1.2
aerial photographs, radar interferograms, and even DEMs. All of them are complementary with LiDAR data in
their resolution, accuracy, coverage, and time of acquisition. Their integration with LiDAR data is able to identify
the vertical and horizontal displacements associated with landslide movement.
Although multi-temporal LiDAR surveys can quantify the volume of debris transferred out of a slope (Baldo et
al., 2009), the subtraction of multi-temporal DEMs is unable to indicate the pace and direction of displacement
directly in longitudinal studies if the landslide debris under study is in a slow motion. The generation of such
information is still possible, however, if the debris object is recognized based on 2D transects in the main direction
of displacement. A comparison of such objects enables the displacement to be quantified at a precision of ±30 mm
in all directions. Inherently, surface movement is 3D, and horizontal and vertical movements are quantified using
totally different methods that are discussed separately below.
Horizontal Movement
Once a landslide is activated, it will move downhill under the force of gravity unexceptionally. Certain types of
landslides such as slump and debris creeping take place at a slow pace. Their velocity of downslope movement can
be quantified from time-lapsed remote sensing data using two methods, feature-tracking and elevation-tracking
with different philosophies of detection. The former tracks distinct features on the surface of the mobilized block
of earth while the latter detects the spatial pattern of elevational changes from images or LiDAR data. Images
allow a huge ground area to be covered. If acquired from EO satellites, they have a long history of existence and
are captured periodically. Space-borne images enable surface movement to be estimated from multi-temporal
images over different periods. Horizontal displacement is better detected from images than from non-imagery
LiDAR point clouds. LiDAR is good at detecting surface movement precisely at a local scale. The tracking can be
fulfilled using multi-temporal LiDAR data alone or in conjunction with other non-LiDAR data.
Feature Tracking
Feature tracking relies on recognizable features on the surface of the mobilized earth block, such as trees, houses,
or the landslide lobe front visible on time-series remote sensing images of a sufficiently large scale and fine spatial
resolution. They are treated as the surrogate targets of detection, and are assumed to represent the motion of the
entire block at the same pace. The same features are located visually on the respective images and their horizontal
position is identified as ending (xe, ye) and starting (xs, ys) points on the respective time-lapsed images to detect
displacement. These features may also be identified using automatic methods digitally, but the process is complex
and lengthy. The pair of identified horizontal coordinates is used to calculate the distance of down-slope travel
(∆d). If the topographic surface is steep and the distance of travel is long, it may be necessary to translate this
projected 2D horizontal distance to the actual 3D distance by dividing ∆d by conθ° (θ = slope gradient in the
downhill direction of movement, obtainable from a DEM). The actual 3D distance is converted to the velocity of
landslide creeping or downslope movement by dividing it by the temporal interval (∆t) of the two images using Eq.
5.34:
(5.34)
where ts = time of the start monitoring; te = time of the end monitoring.
This method works only when there are distinct features on the surface that are visible on both pre- and post￾event images of a fine spatial resolution to enable the tracked features to be discernible. Besides, this method
requires complex image processing as the pre- and post-event image pairs must be spatially co-registered with
each other to a high geometric standard to render geometric inaccuracy to be negligible or much smaller than the
pace of displacement itself. Thus, this method is restrictive with limited applicability. In comparison, the elevation￾tracking method is more flexible but more complex. It can be implemented with either LiDAR or InSAR data.
InSAR-based EstimationRadar images are dissimilar to optical images in that their signals encompass two components, amplitude and
phase. The former pertains to the radar signal intensity received at the radar antenna. The latter is related to the
range of sensing or the distance between the landslide and the radar antenna. Any movement on the ground triggers
a slightly varying part of the radar pulses to be backscattered to the sensor. Thus, a change in a landslide’s position
on the ground is always accompanied by a corresponding change in its image phase. This change is detectable
from a pair of repeat, time-lapsed SAR images to determine the distance of travel and thus velocity. The estimation
of ground movement from SAR images is ideally implemented as interferometric SAR (InSAR). It is a method of
generating interference of electromagnetic waves, usually from a pair of side-looking, superimposed SAR images
of the same area taken at the same time but from two slightly different positions in multiple paths or from the same
position in the same path at a short interval known as repeat pass (Figure 5.19). Their phase difference or
“interfere” is called interferogram that illustrates the phase change between the two SAR images. Subtraction of
the two phases effectively eliminates random scatters in the phase.
FIGURE 5.19 The geometry of InSAR scanning and the definition of line-of-sight (LOS).
One way of establishing single-pass interferometry is to capture SAR images using two antennas, serving as the
transmitter and receiver, respectively. The former transmits the radar pulses towards the target of sensing, whereas
the latter records the radar signal backscattered from it. Multi-pass images “view” the same landslide from a
slightly varying perspective at the same sensing position but different times (as with the use of one satellite) or
from two different spots simultaneously (as with the use of two satellites). Repeat-pass SAR images must be
acquired from a similar position in both paths. Thus, the same ground is sensed from the same antenna twice at atime interval from each other. The phase difference () between two co-registered SAR images is composed of five
parts:
(5.35)
where and = SAR phase values at the sensing time of t1 and t2; , , , , and = phase value contributed by surface
displacement, orbital change (e.g., satellite observation geometry), topography, the atmosphere, and noise (e.g.,
changed surface scattering properties), respectively. If the two SAR images are acquired simultaneously at
different positions, then the phase difference is attributed mostly to the ground surface. If they are captured from
the same position but at different times (e.g., after revisit), then the phase difference is caused mostly by the
change in surface elevation. If the ground is imaged from different positions at different times, then the phase
difference is reflective of both topography and its change induced by landslides. These maps reveal the line-of￾sight (LOS) ground surface displacement (range change) between the two times (Figure 5.18). In case of no
change in the scattering phase, the interferometric phase of the displaced earth (∆ϕdisp) is a function of the range
difference ∆r (r2 – r1), or:
(5.36)
where ϕ1 and ϕ2 = phase of the first and second SAR images, r2 and r1 = ranges to the target from the two sensing
positions; λ = wavelength of the microwave radiation.
On the interferogram, the similarity of ground scattering is gauged by coherence. As shown in the simplified
geometry (Figure 5.18), the look direction of the radar antenna is at the right angle to its motion. In this case, the
differential phase between the two repeat-pass SAR images is calculated as:
(5.37)
where θ = look (incidence) angle at the time of sensing; α = angle of the baseline vector measured from the orbital
horizontal; B = distance between the positions of the two antennas known as the baseline; ∆r = displacement of the
pixel from one pass to the next in the LOS direction.
From the calculated , the horizontal displacement vector Uhor is estimated as:
(5.38)
where DLOS = displacement in the LOS direction; = trigonometric coefficients to decompose the LOS
displacement to the planimetric displacement, calculated as (Av = mean azimuthal angle of the radar pulses)
(Schlögel et al., 2015).
The interferogram quality is influenced mostly by the temporal and spatial baseline, as well as surface cover
and relief, and atmospheric conditions. A scatterer’s contribution to the sum is altered by changes in its relative
motion or in the look direction of sensing, a phenomenon known as decorrelation. It can take place both spatially
and temporally. Temporal decorrelation increases rapidly with temporal baseline. A temporal baseline of more than
four months can lead to very low coherence, especially in vegetated areas. Hence, in order to minimize
decorrelation, interferograms should be produced from image pairs of a small temporal and spatial baseline. It
must be emphasized that InSAR-generated field view of movement velocity is confined to the LOS direction only
(Figure 5.20). It can be converted to the horizontal velocity of movement if the scanning angle at a given pixel is
known (it varies from pixel to pixel as the ground is scanned). For space-borne InSAR monitoring to be successful,
the displacements must take place over a relatively large spatial extent, and there must be houses located inside the
study area (Bayer et al., 2017).5.4.1.3
FIGURE 5.20 Earthflow velocity in the LOS direction derived from ALOS InSAR overlaid on top of a shaded relief
map derived from airborne LiDAR of 1 m grid size. The thin white line marks the active landslide extent. Black
lines indicate the starting and ending location of individual trees mapped on historic aerial photos and a recent
unfiltered DEM produced from LiDAR data. (Roering et al., 2009, free access.)
The quality of the phase values is assessed by their variability on stable terrains not affected by movements (e.g.,
outside landslides). The majority (e.g., >70%) of the pixels have a phase value within the range of [−0.5 rad, +0.5
rad] (Schlögel et al., 2015). The displacement velocity of landslides derived from ALOS/PALSAR interferograms
using differential InSAR is consistent with the ground-based GPS observations at 11 points with a maximum
discrepancy of 0.05 cm ∙ day–1, against the maximum absolute DLOS values of 8.9 ± 0.9 cm, 14.7 ± 0.9 cm, and
14.1 ± 0.9 cm in three periods. InSAR measurements have an uncertainty of ±0.9 cm, much lower than ±7 cm for
GNSS, and ±3 cm for terrestrial LiDAR.
LiDAR-based Estimation
The elevation-tracking method of detecting horizontal movement makes use of the surface manifestation of
landslide movement by analyzing elevations and their spatial pattern from time-lapsed LiDAR data that are
converted to DEMs. The detection of deep-seated landslide creeping from LiDAR data relies on height and its
change within a local spatial extent (e.g., a window). One possible way of implementing the detection is via
elevational discontinuity between adjacent DEM grid cells near the landslide scarp. The elevational trough
between two similar elevations in the downslope direction represents the distance of displacement. This method
functions only when the entire displaced block moves in unison translaterally. It does not work or is inaccurate if
one part moves faster than the remainder of the block. Another method of detection is based on the local maxima
of ∆h () for grid cells within a window in which after the two DEMs of the area under study have been constructed
and subtracted from each other. The grid cell that has the negative represents the origin of movement whereas the
cell having the positive denotes the displaced position. Their distance can be treated as the distance of creeping,and used to determine the velocity of movement. In order to avoid the influence of outliers, may be defined as 
(standard deviation of elevation within the search window).
Alternatively, the local maxima () may be replaced by examining all spatially contiguous grid cells of positive
and negative ∆h, and grouping them into patches of ∆h > 0 and ∆h < 0 cells after minor changes below the
threshold (e) (e.g., ∆h < e) have been filtered out. Each type of patches containing the largest number of cells is
vectorized to form two largest polygons, whose centroid can be treated as the origin and destination of the
movement, respectively. Their distance is equal to the distance of travel, calibrated by the surface gradient in the
steepest downslope direction. This method is functional in detecting the movement of singular landslides within a
small area. In the presence of multiple landslides, it is difficult to associate the patch of ∆h > 0 cells to its
corresponding ∆h < 0 counterpart, so it is dysfunctional. This method is best used for deep-seated, slow-moving
landslides only (Booth et al., 2013). The accuracy of this method depends on the pace of vertical change in
elevation and the influence of vegetation. Naturally, its accuracy decreases in forested areas. It is hence
advantageous to capture LiDAR data during the leaf-off season so as to make the optimal use of multi-temporal
LiDAR data in treed areas.
The detection of a singular landslide movement at a high speed is best accomplished using TLS to record
profiles of debris flows repeatedly at short intervals with a narrow swath width (~1 mm) (Rengers et al., 2021).
The scanner has a fixed azimuth of scanning to capture high-frequency longitudinal profiles of an experimental
debris flow at a high frequency (~60 Hz) as the flow shifts from the release point to the deposition point at the base
of a flume. The captured high-resolution profiles are used to track the debris flow surface geometry and quantify
flow front velocity based on the changes in the relative height of each LiDAR swath relative to the initial flume
elevation, or:
(5.39)
where H = flow depth; hbed = reference elevation (e.g., elevation of the flume bed surface devoid of flow); i = line
swath iteration that represents all of the points obtained during a 0.017-second LiDAR scan; and hi = vertical
elevation of the flow surface. hi and hbed are interpolated at evenly spaced points every 5 cm along the horizontal
flume axis. After rock detached from the main flows is filtered out, the position of the flow front Fx is numerically
identified as the maximum distance along the flume bed using the X’s downslope coordinate direction, where the
upslope flow depth minus the downslope flow depth is larger than 0.03 m, or:
(5.40)
where j = an index of the position along the sloped flume bed surface, not the horizontal distance. The threshold of
>0.03 eliminates any rolling particles prior to the flow. The flume bed-parallel flow-front velocity (V) is
determined from the changed position of the flow front along the slope distance over time between consecutive
LiDAR swaths:
(5.41)
where ti = maximum time recorded for each LiDAR swath i. Because the LiDAR unit takes approximately 0.017
seconds to acquire the full debris-flow profile geometry, there could be an uncertainty up to 0.017 seconds in the
estimated velocity. This method enables the documentation of the flow position and speed to identify changes in
flow characteristics but the detected movement is only one-directional from upslope to downslope. It is unable to
yield a 2D field view of flow velocity as with the use of InSAR data.
For a slow-creeping landslide, the LiDAR data probably capture its current position. In order to estimate its
moving velocity, its historic position has to be determined from non-LiDAR data, prompting the necessity of
coupling LiDAR data with other types of data to detect landslide movements (Roering et al., 2009). Common
candidates for integration are GPS data, laser theodolite (total station) data, field observations for currently active
landslides, and even historical movement data. This integration facilitates the detection of horizontal displacements
of the mobilized debris and estimation of its velocity of movement at different times. If combined with orthogonal5.4.1.4
historical aerial photographs, LiDAR data allow objective mapping of earthflow movement, from which the rate of
movement is calculated. The combination of displacement orientations with stacked radar data enables the
estimation of downslope velocities of the central transport zone of landslides (Roering et al., 2009).
UAV SfM Estimation
The horizontal movement or displacement of mass movement on a small scale is commonly quantified from high
resolution (1 cm) UAV images, supplemented with LiDAR data. They are best processed using SfM that does not
require the camera pose to be known. UAV images can run easily into 100s to cover a large ground area. The
automatic detection from these images is commonly implemented by identifying the maximum image correlation
coefficient between multi-temporal orthomosaics or DEMs. The pace of movement is defined as the distance
between the origin of the maxima and its destination, from which the direction of movement can be quantified, as
well (Figure 5.21). Inevitably, the quality of the quantified pace of displacement is subject to the horizontal
inaccuracy induced by imprecise co-registration of the two images or DEMs, or image coarse resolution, and
processing-introduced errors. This accuracy is judged by the residuals of the bundle adjustment, expressed as
RMSE. It varies between 0.036 and 0.078 m between two surveys, with the larger RMSE attributed to the windy
conditions during UAV flight (Lucieer et at., 2014). Evaluated against 39 GCP coordinates, the overall horizontal
RMSE changes to 0.074 m (vertical RMSE: 0.062 m). Paradoxically, the vertical accuracy is higher than
horizontal accuracy because the target of sensing is a landslide from a coastal cliff that must be imaged
horizontally. In this case, horizontal change is equivalent of vertical change in a nadir-viewing photograph. A
major source of inaccuracy stems from the imprecise identification of the displaced position using COSI-Corr (see
Section 4.4.2.1 for details). The derivation of optimal results requires specification of a proper window size, step
size, and search radius. They are finalized via heuristic fine tuning based on a priori knowledge of the
preliminarily detected pace of displacement. This accuracy level means that the detectable displacement must
exceed 0.20 m to be meaningful. This claim is supported by the mean (about 0.13 m) of the absolute accuracy
related to the bundle adjustments in studying a giant coastal landslide off a cliff (Esposito et al., 2017). The
horizontal RMSEs had a range of 0.11–0.14 m. Movements of chunks of ground material, patches of vegetation,
and the toes of the landslide can all be successfully quantified, but less so in mapping the retreat of the main scarp
probably because of its indistinct edge.FIGURE 5.21 The direction (white arrows) and magnitude (color) of lateral displacement of the Home Hill landslide
in Southeast Tasmania of Australia, detected from drone images captured between July 19, 2011 and November 10,
2011 using SfM. (Lucieer et al. 2014, authors´ copyright.)
The accuracy of estimating landslide movements over time and at different magnitudes with the assistance of the
UAV-SfM method can be improved by combining different methods, even with low-cost lightweight consumer￾grade UAV data (Yordanov et al., 2023). These methods include the use of Red Relief Image Map (RRIM, a
topographic visualization technique) and Lucas–Kanade (LK) optical flow algorithm for estimating the final
displacement. RRIM is a method of accentuating topographic features (e.g., concavities and convexities of the
topographic surface, in conjunction with slope gradient) in greater detail. It enhances the perception of detail and
depth free from the impact of the solar direction, shadow, and vegetation. The LK optical flow algorithm tracks the
motion of features in time-sequence images or videos by monitoring the movement of pixels between
images/frames and computing the apparent motion of features between two consecutive instances. This algorithm
assumes small or constant pixel intensity changes between frames and can detect 2D surface movements. Its
coarse-to-fine inverse pyramid strategy for the window search makes it very cost-efficient. However, its reliance
on image intensity causes it to be vulnerable to changed illumination between two images (e.g., different shadow5.4.2
5.4.2.1
length). Its performance is subject to the precision of image co-registration as local or global shifts are construed to
represent motion (decorrelation). Direct image-to-image co-registration is not deemed sufficiently accurate.
Instead, finer co-registration is imperative, achieved using local co-registration performed on stable parts outside
the active displacement zone, followed by more rigid shift correction that does not interfere with the actual debris
displacements, and finally, merging both co-registered segments into one (Yordanov et al., 2023). It may be filtered
to remove decorrelation. The comparison of the merged and co-registered secondary image with that of a previous
epoch directly yields the displacement vector in the u and v directions (see Figure 4.10), from which the magnitude
of the displacement is calculated as:
(5.42)
(5.43)
where θ = direction of the displacement vector. The use of these methods can potentially estimate landslide
displacements from a few centimeters to over tens of meters accurately and effectively (Yordanov et al., 2023).
However, in the absence of any concrete accuracy indication, this claim should be treated with caution.
Vertical Movement
Vertical movement can be earthquake-triggered uplifting of the surface or ground sinking related to land
subsidence. During subsidence, the elevation of the ground surface drops below its normal height, due to sinking
or settling. This process can be gradual or abrupt, depending on the causes. Gradual subsidence is commonly
associated with the extraction of groundwater within the underlying earth, compaction of loose soil, and erosion of
materials during rain. Ground subsidence takes place most commonly in urban areas and agricultural areas where
the groundwater is extracted excessively for irrigation. Sudden subsidence is caused by sinkholes, underground
tunneling, and underground mining. In coal-mining areas, the burning or collapse of coal pillars in coal fires can
trigger large-scale subsidence, in conjunction with coal extraction. The compaction of loose soil in the ground not
allowed to settle sufficiently in hastily executed engineering works can also trigger subsidence. In newly
developed high-rise residential areas, the added weight of building materials and infrastructure atop newly laid
foundations can also cause subsidence when the foundation is not made sufficiently sturdy. In karst areas, the
ground may sink after the dolomite cove has collapsed. It can be sporadic or haphazard spatially.
The quantification of ground subsidence is virtually detection of spatial surface elevation change over a period
of time. Thus, the principle and implementation of quantification remain identical to those of landslide debris and
lava deposits except that the surface always has a lower elevation on the ending image than on the starting image,
resulting in negative changes in elevation. Ground subsidence can be quantified using non-imagery LiDAR and
imagery data. The nature of remote sensing data affects both the method and complexity of quantification. Non￾imagery LiDAR data-based quantification is identical to the detection of sediment budget in a channel in principle
and method, so is not repeated here. Due to the small magnitude of vertical movement in ground subsidence, the
data used must have a high level of vertical accuracy. The most reliable and practical data for detection are SAR
images that are inherently side-looking, and thus sensitive to topographic relief and vertical movement.
LiDAR Method
The best way of quantifying the spatial variability of ground subsidence is to detect elevational changes from
multi-temporal grid DEMs produced from LiDAR data. LiDAR-based estimation of surface subsidence is identical
to the estimation of landslide debris thickness in principle. It is based on two-time LiDAR datasets of the same
area. The randomly distributed, dense 3D point cloud coordinates are converted to regular grid DEMs whose grid
cell size can range from 1 to 10 m (or submeters with close-range LiDAR data). Surface subsidence is detected on
grid cells by subtracting the recent DEM from the earlier one of the same area, spatial extent, and grid size. The net
difference represents the vertical change (e.g., ground subsidence) in elevation. The result is able to show the
spatial distribution of subsidence (Figure 5.22).FIGURE 5.22 Spatial distribution of land subsidence in a coalmine area near Wollongong, Eastern Australia, detected
from differencing of two DEMs interpolated from two airborne laser scan surveys. (Palamara et al., 2007. © Taylor
& Francis.)
One major factor affecting the accuracy of ALS-based estimation is ground cover. In deciduous forests, the
vertical accuracy of estimation lies around ±0.26 m, and it drops to ±0.17–0.19 m in low grass and evergreen
forests (Palamara et al., 2007). Nevertheless, accuracy is noticeably higher in bare ground areas devoid of
vegetation, with a typical nominal accuracy of around ±0.15 m. It must be noted that the accuracy of the estimated
vertical movement is further degraded by the inaccuracy of either data when the pre- and post-event DEMs are
compared with each other spatially. As mentioned previously, estimation inaccuracy is easily ascertained from the
estimated changes in elevation in areas of no subsidence (and no vegetation, as well). The mean elevational
difference of grid cells in these areas can be treated as the offset and used to calibrate the estimated subsidence.
After its subtraction from the detected outcome, the accuracy of airborne LiDAR data in estimating coalmine
subsidence averages ⁓0.23 m if the data are interpolated to a proper grid size (Palamara et al., 2007). It must be
noted that this accuracy is inflated by a systematic offset of 0.15 m between the two LiDAR surveys (the error is
much larger in areas of a steep terrain). Such an accuracy level may prove satisfactory in estimating mine5.4.2.2
subsidence, but may not be acceptable in quantifying urban subsidence <10 cm. It is not generally recommended to
estimate subsidence of steep or narrow features using LiDAR data due to their inaccurate horizontal positions.
Accuracy is expected to decrease in areas of a rugged terrain. Besides, small-footprint LiDAR data have a narrow
swath width not suitable for wider coverage, a disadvantage that can be overcome by integrating InSAR with EO
images.
InSAR Method
InSAR has been widely used to detect ground subsidence because of the ready availability of space-borne SAR
data and data processing software, mostly free of charge. InSAR estimation of ground subsidence is grounded on
the fact that any motion on the ground (e.g., subsidence) causes a slightly different portion of the wavelength to be
scattered back to the radar antenna. The differences in the phase information can be tracked via InSAR. The
interferometric phase is then converted to displacement along the LOS direction by multiplying it by the correction
factor of λ/4π (λ = radar wavelength), under the assumptions that the incidence angle of the radar beam is rather
small, and most of the surface deformation associated with land subsidence is confined to vertical shift only.
Subsidence-triggered surface deformation can be measured from the constructed interferograms (i.e., the negative
interferogram values). If proper SAR images are used, surface subsidence can be detected at the cm-level over
extensive areas at a spatial resolution up to 90 m.
InSAR-based quantification of subsidence is highly complex involving several steps. The exact procedure of
quantification varies with the SAR data (e.g., their format and level of processing), data pre-processing (e.g., how
the atmospheric effects are calibrated), and the computing software used. In general, this procedure comprises
seven essential steps of data preparation, creation of single look complex (SLC) images, image co-registration,
terrain geocoding, interferogram generation, phase unwrapping, and derivation of subsidence rate (Figure 5.23).
The generation of SLC images from raw SAR images requires auxiliary data, such as instrument characterization
data and external calibration data in the form of LUTs for initial processing. Each SLC image is made up of real
and imaginary complex radar echoes, and radar intensity, organized in three separate layers. During data
preparation, the raw SAR data are converted to SLC images at full resolution for each acquisition; each sub-swath
is de-bursted and all individual bursts are merged to form a seamless tile if the image has a shorter swath width
than the dimension of the area of interest (for Sentinel-1 SAR images only), from which the area is delineated
using clipping. If water is present inside it, it has to be masked out as it diffuses the incoming radar radiation.FIGURE 5.23 The general steps involved in the conversion of SLC radar images to interferograms and eventually the
subsidence map using the Branch Cut method. Green boxes – derived products. (Gao, 2023.)
As illustrated in Figure 5.24, the generation of the final rate of subsidence involves six essential steps of
connection graph creation, interferogram generation and filtering, orbital refinement and re-flattening, removal of
the atmospheric and topographic effects, phase unwrapping, and phase-to-displacement conversion. Additional
steps may be required for different types of implementation. Three typical and most crucial steps in processing
time-series InSAR data are identified as interferogram generation, DInSAR formation, and estimation of LOS
motion. They area elaborated below.FIGURE 5.24 Routine workflow of estimating earthflow velocity from time-series InSAR data. blue ovals:
interferogram domain; green ovals: time-series domain; dark green rectangles: input data; green rectangles: output
results; dashed border: optional steps/data. Note: The order of empirical tropospheric delay correction, topographic
residual correction, and phase deramping is inconsequential, and has a negligible influence on the noise-reduced
displacement time series. (Modified from Zhang et al., 2019, used with permission (5761120095827) from
Elsevier.)
i. Interferogram generation. An interferogram is generated from a pair of images with the shortest temporal
baseline after the baseline of every pair of SLC images is calculated. A set of n SAR images allows n-1 full￾resolution interferograms to be produced, all based on the same master image;
ii. DInSAR formation using an external DEM. Of all the n-1 interferograms, the one with the largest baseline is
set as the master image while all other images are treated as slaves and co-registered with the master image.
It should have a relatively short spatial baseline to other slave images, and be temporally close to the first and
the last images. If the image set is small (e.g., <15–20), then several images should be selected as the master
images. Multi-master-based InSAR is an appropriate supplement to sole master-based InSAR; and
iii. Estimation of preliminary LOS motion, topographic error, and atmospheric contribution in five sub-steps:
a. Correction of the synthetic topographic phase in the interferograms using the external DEM of the
study area;
b. Derivation of the displacement phase from all raw interferograms and corresponding coherence layers
for all paired images;
c. Processing of the differential interferogram to minimize phase noise and boost phase unwrapping
accuracy via adaptive filtering. Alternatively, phase unwrapping residuals may be reduced by screening
the interferometric phase to increase the SNR;
d. Differencing the topographic and deformation interferogram for all paired images, and unwrapping of
the phase of the produced differential interferogram using a few available methods. They includeminimum-cost flow and its extended version, weighted least mean squares, branch and cut, and
Statistical-Cost, Network-Flow Algorithm for Phase Unwrapping (SNAPHU). The last algorithm can
resolve ambiguity in unwrapping phase data. The unwrapped phase (∆ϕ) is transformed to the slant
range (LOS) displacement (DLOS) as:
(5.39)
a. Translation of DLOS to vertical subsidence (Dver) by dividing it by the cosine of the incident angle (θinc) after
the horizontal displacement is ignored (Ahmad et al., 2019):
(5.40)
Dver can be made more accurate by further refining step (c) for those pixels with a high SNR in the azimuth
direction using spatial smoothing over small areas (e.g., <5 × 5 km2) (Ferretti et al., 2000). The accuracy of the
estimated subsidence may be improved by adopting a short baseline from several interferograms (Ge et al., 2007).
Other factors that may affect InSAR signal quality include sensing position (angle and range), the timeline of the
interferogram, and the precision of controlling the look angle and position of the satellite. The detected magnitude
of subsidence (Dver) is converted to the rate of subsidence by dividing it by the temporal separation of the two
radar images used, and the results are expressed in cm ∙ yr–1 (Figure 5.25).FIGURE 5.25 Distribution of mean subsidence rate in the vertical and LOS (line-of-sight) directions in Tepic, central
Mexico, detected from time-series ALOS InSAR data. (Chaussard et al., 2014, with permission (5761120423047)
from Elsevier.)
As one of the best methods for estimating ground subsidence, InSAR can quantify the spatial extent, magnitude,
and temporal evolution of land subsidence rapidly and efficiently in urban areas. If generated from Sentinel-1 data,
InSAR is well-posed to estimate spatiotemporal ground subsidence over large areas and a long period at a fine (10
m) spatial resolution and high accuracy of ∼1 cm, which is impossible with other methods such as leveling. The
accuracy is the highest over a horizontal distance of meters to tens of km. InSAR can quantify mm-scale vertical
shifts, and potentially cm-scale ground motion cost-effectively if slant range, baseline, phase, and height are
properly configured (Liu et al., 2013). Space-borne RadarSat-2-estimated subsidence rate within a mining zone
averages 5.6 mm/quarter, slightly lower than 6.67 mm observed on the ground (Alam et al., 2018). Both results
have a close spatial agreement with each other. Their discrepancy stems chiefly from residual phase noise caused
by temporal decorrelation.
The success of InSAR in estimating ground subsidence is contingent on several factors, such as sensing the
same ground from the same position in space to minimize the perpendicular baseline. Robust InSAR results are
obtainable by making more efforts in addressing several InSAR factors, such as temporal decorrelation that
governs the accuracy of conventional differential InSAR, baseline decorrelation, external DEM inaccuracy,
vegetation (if present), and the atmospheric effects. A DEM is needed to eliminate the phase component
contributed by topography. Coherence is weakened by dense vegetation, which makes it increasingly difficult to5.4.2.3
determine vertical displacements to a high accuracy level. It should be mapped and masked. In addition, coherence
is also subdued by highly variable atmospheric conditions at the time of multi-temporal SAR imaging.
Atmospheric artifacts introduce an additional atmospheric phase to SAR images, and severely lowers the accuracy
of the estimated subsidence because the atmospheric phase screen’s spectral behavior is influenced by water vapor
in the troposphere and cannot be estimated from the coherence map produced from an interferogram (Ferretti et al.,
2000). Such noises can be detected easily because of their temporal and spatial variability, different from immobile
genuine subsidence (Dehghani et al., 2009). One way of minimizing the variation in the atmospheric conditions is
to select the SAR image pairs with the shortest temporal separation possible to produce an InSAR. Nevertheless,
they are still subject to atmospheric variations such as clouds and water vapor that are the most volatile and
unpredictable, and their impact on the phase can be eliminated using spatial filtering. Image smoothing improves
the quality of the estimated subsidence.
PS-InSAR Method
The radiometry of a SAR image stems from the total energy backscattered by all scatterers inside the sensed
ground area and in the atmosphere. The phase of pixels in a SAR image is altered by several factors, such as the
changed satellite-scatterer geometric relationship and the target. However, persistently dominant scatterers hardly
change in their phase over time, even if subordinate scatterers do move relative to them. If the ground is scanned
from multiple look angles, there is minimal variation in their phase. Thus, these scatterers are termed persistent
(permanent) scatters (PS). On InSAR imagery, they can be buildings and natural objects (e.g., rock outcrops), all
being stable over a long spatial perpendicular baseline. They can be easily identified on the interferograms based
on their brighter tone than the background scatterers. Commonly found in urban areas, these discrete scatterers are
temporally coherent at long temporal separations, and can be selected based on the amplitude dispersion index.
If the PS’s dimension falls below the pixel size of the SAR images used, the interferogram constructed from
them will have strong coherence if their baseline exceeds the decorrelation threshold. Hence, all the available SAR
images can be used to produce interferograms for operational estimation of ground subsidence. In the absence of
dominant scatterers, decorrelation-induced phase uncertainty is excessively large to overwhelm the signal.
However, this signal can still be detected from interferograms produced from a pair of SAR images of a short
temporal separation and a similar look angle, provided that decorrelation is sufficiently minimized (Hooper, 2008).
Decorrelation may be further reduced by spectral filtering in range and by removing non-overlapping azimuthal
Doppler frequencies.
As an advanced form of differential InSAR, PS-InSAR makes use of multiple stacked and co-registered SAR
images of the same area. They allow the identification of point-resembling PS dominating the return phase. They
have variable acquisition geometries, but preserve the temporal phase information. These phase-stable pointwise
targets can eradicate the spatially uniform atmospheric phase screen, and allow their relative motion to be
quantified to a precision on the order of 1 mm ∙ yr–1 for each scatterer (Zerbini et al., 2007). Functioning as a sort
of “natural geodetic network”, PS enable subsidence to be estimated at a very high spatial density up to several
hundred measurements per km², a density sufficient for analyzing the local pattern of surface subsidence in detail.
This approach is best suited to areas well distributed with bountiful, reliable, and coherent PS points, such as
urban, peri-urban, and built-up areas where PS density can reach up to several 100s per km2, but not rural or
wooded areas where it is very difficult to find coherent scatters because variations in vegetation geometry or
wetness cause phase decorrelation. The absence of PS candidates in rural areas thwarts successful phase
unwrapping for all pixels, and consequently the estimated pace of subsidence to be spatially discontinuous and
sporadic, available at selective dense points only (Figure 5.26a). This phenomenon is also common in forested
areas, a major drawback of PS-InSAR. Some pixels yield no results due to the failure of InSAR caused by several
factors, such as low coherence, large subsidence gradient, failed phase unwrapping, or the use of improper
deformation models. The density of measurements may be increased via additional processing such as spatial
interpolation from the successfully estimated pixels. It can fill the gaps in the original results (Figure 5.26b).FIGURE 5.26 Distribution of PS-InSAR-derived mean LOS velocity of ground deformation during 2007–2010. (a)
vertical deformation of all PS points; (b) raster map interpolated from (a) using kriging. (Hsu et al., 2015, open
access.)
Another way of increasing PS density is to install artificial corner reflectors in areas of no buildings in the field of
sensing. These corner reflectors can serve as candidate PS pixels. Another method of overcoming the low density
of PS candidates in rural areas is to make use of coherent scatterer InSAR (CSI) to combine PS with distributed
scatterers (DS). Through the use of the generalized likelihood ratio test, statistically homogeneous pixels are
identified and a phase-linking algorithm is used to estimate the optimal phase for each DS pixel. The joint
exploitation of both PS and DS targets drastically increases the density of measurements, which enhances the
reliability of phase unwrapping (Dong et al., 2018). The CSI results from time-series InSAR data retain an RMSE
of ∼10.5 mm ∙ yr-1 in monitoring landslide surface displacements, in comparison with in situ GPS measurements.
Although CSI produced a spatial distribution pattern of surface displacement rates similar to those obtained using
classic PS-InSAR feature points, it detected >10 times more measurement points than the other two methods in
vegetated mountainous areas, owing obviously to the use of DS targets.5.4.2.4
Apart from producing high-density measurements, PS-InSAR is also rather robust and accurate. It can retain an
adequate performance if a large number of images (e.g., 15–20) are used. Sentinel-1 PS-InSAR produced land
subsidence results resemble those measured at GPS stations with a MAE of 5.7 mm and R² of 0.94 (Ahmad et al.,
2019).
Image+SfM Method
Direct comparison of two-time DEMs is suitable for analyzing mass movements experiencing significant and
abrupt changes due to the high level of inaccuracy. Vertical change is commonly quantified via differencing of
DEMs constructed from drone, archive orthophotos or oblique images or aerial photographs using SfM for precise
quantification of local variation in elevation associated with natural movements, such as landslides, volcanoes,
glaciers, or river systems where the changes can make up a large portion of individual images (Robson et al.
2022). UAV+SfM quantification can be implemented using CloudCompare, in which elevation change is detected
via the direct difference between point clouds, in a manner resembling DoD. The two point clouds should be
spatially sampled to a relatively comparable density, and be co-registered as precisely as possible. The co￾registered point clouds are rasterized into a DSM at a super-fine spatial resolution (e.g. 10 cm). The interpolation
of the point cloud into a raster DSM may introduce noise and irregularity to the surface. They can be filtered using
a Gaussian filter to obtain a more genuine and smooth surface representation without losing details (Yordanov et
al., 2023). If the 3D models are constructed using UAV-SfM and TLS, vertical changes are detected via point-to￾mesh comparisons in chronological order. The raw point-to-mesh differences may be spatially filtered via image
smoothing to eliminate local spikes in elevation so as to reduce the influence of instrumental TLS noise on SfM
models. Change in surface elevation is detected by differencing filtered point-to-mesh of the SfM point cloud and
the TLS mesh (Figure 5.27). A DoD is produced via the subtraction of SfM elevation model from that obtained by
TLS, and 94% of overlapping model differences lie in the range of -1.0–1.0 m, with 86% between just -0.5 and 0.5
m. The elevation change uncertainty from historical aerial photographs processed using SfM, as determined from
unaffected areas, can be as high as 0.52 m (Robson et al., 2022), in comparison with a recent LiDAR-derived
DEM on stable terrain. This accuracy is expected to be lower in vegetated areas, and with historic data. For
instance, the point-to-surface standard deviations of the smoothed comparisons ranges from ±0.34 m for historic
oblique photographs taken from a helicopter to ±0.038 m for the recent photographs also taken from a helicopter
but with a long focal length camera (Guerin et al., 2020).5.4.3
FIGURE 5.27 Changes in surface elevation of the 10 March 1987 rockfall scar on Middle Brother. (A) Pre-event
topographic surface (red triangular mesh) and lateral boundaries (yellow polyline) of the rockfall on top of a
photograph. (B) Surface elevation change (m) detected by comparing spatially filtered point-to-mesh differences
between the 1976 SfM point cloud and the 2010 TLS mesh. Rectangles indicate the location of four missed
rockfalls. (Guerin et al., 2020, with permission (5761110722076) from Elsevier.)
Ground Deformation
Ground deformation is inherently 3D (vertical, north, and east), different from ground subsidence that is confined
to the vertical direction only. Ground deformation may be caused by earthquakes, landslides, and underground
mining. It is important to quantify deformation as it can adversely impact transport infrastructure and damage
utility facilities in urban areas. The quantification of the spatial distribution of 3D deformation rate is much more
challenging than detecting subsidence, usually requiring very high resolution optical and microwave images
(Metternicht et al., 2005) because of the small pace of movement. Of all the existing images, only those possessing
the 3D-viewing capacity can adequately meet the requirements as 3D deformation has temporal variations. If
optical images are used, they must be time-series and repeatable, and accurately geo-referenced. The processing of
time-series images increases the workload of estimation, but the task can be automated using sophisticated
software. However, most optical images are nadir-viewing that makes them insensitive to deformation-related
changes in surface height. Besides, optical images have a temporal resolution on the order of days and are subject
to cloud contamination. As such, they are not suited to quantify fast-changing deformations related to landslides. In
comparison, side-looking radar images are better choices as they are sensitive to surface relief and have been used
much more widely than optical images in quantifying deformation.
The estimation of surface deformation has been revolutionized by space-borne radar interferometry to the mm￾level accuracy. It can generate regional deformation maps associated with seismic and volcanic activities. InSAR
images allow 3D deformation to be effectively monitored. The most commonly used multi-temporal InSAR is
strong at quantifying the spatiotemporal rate of slow deformations and studying land displacement processesrelated to landslides (Figure 5.28). The displacement in multiple directions increases the difficulty of phase
unwrapping of interferograms, precipitating the use of more complex 2D and 3D phase unwrapping algorithms.
They enable the identification of landslide movements in two directions (Liu et al., 2013), but increase the
unwrapping complexity. The wrapped and unwrapped phase values can be used to infer the specific types of
movement (e.g., rotational, translational, and complex sliding) if integrated with information on the location of
specific morphological (scarps, grabens, and lobes) or topographic (steep slope, shape of slope) background
knowledge (Schlögel et al., 2015).
Ground deformation can be effectively quantified using differential InSAR data or DInSAR. It is an efficient and
cost-effective method of estimating ground deformation caused by natural events over a large area. DInSAR is a
promising technology capable of estimating surface deformation at dense points and the mm-accuracy level.
Single-track interferograms can yield only 1D deformation information between the sensor and the target in the
LOS direction, which is sufficient for detecting subsidence, but not 3D deformation. Vertical and horizontal
displacement maps of the coseismic ground deformations can be produced from two interferometric pairs to
retrieve the LOS displacements from the unwrapped interferograms obtained through the InSAR pairs (Valerio et
al., 2018). Another method of detecting 3D deformation components is to extend Eq. 5.39 that yields only 1D
results in the LOS direction by integrating single-track InSAR with a prior deformation model such as the linear
proportion model as:
(5.41)
(5.42)
where Ge-w(i, j, t) = first derivatives in the east-west direction at pixel (i, j) and time t; c(t) = dynamic proportional
coefficient between horizontal movement and the gradient of subsidence, calculated as b(t)*D(t)/tanβ(t), of which
H, b, and β = 3D deformation parameters (b = horizontal movement constant, β = radar pulse incidence angle, D =
depth of underground extraction activity). Model parameters are first considered as dynamic and further adaptively
estimated from multi-track InSAR observations using a robust iteratively reweighted least-squares estimator,
eliminating the need for in situ data collection (Yang et al., 2022). The estimation of the 3D deformation
components requires multi-track InSAR observations from at least two significantly different imaging geometries
(e.g., ascending and descending orbits) to ensure the accuracy of the decomposed 2D deformation components.
Instead of the direction methods (i.e., lower–upper decomposition), time-series displacements are resolved from
multi-track InSAR data using the conjugate gradient method. It works well even over a large area where the model
parameters are unknown. Vertical displacements are retrieved from DInSAR images through an appropriate phase
unwrapping operation performed on the generated differential interferograms.
Nevertheless, DInSAR-based monitoring is a complex process involving several steps (Figure 5.29) that have
been described in Section 5.4.1.2 already. The coupling of radar data with DInSAR data faces several challenges in
obtaining the true phase of a corner reflector, the difficulty of co-registering corner reflector pixels in light of
extremely low coherence of their surrounding area, and computing the interferometric phase of two co-registered
corner reflectors without the flat earth term and the corners’ height contribution (Ye et al., 2004).FIGURE 5.29 Procedure of detecting 3D ground deformation from SAR data using DInSAR, refined with a precise
external DEM. (Modified from Alam et al., 2018, © Indian Society of Remote Sensing.)
The derived deformation is 3D that is usually visualized by displaying the displacement in two perpendicular
directions (Figure 5.30). The resolved 3D mining displacements can be estimated with a mean error of about 1.8
cm and the accuracy of estimation is significantly improved by 69% if facilitated by a linear proportion model
using DInSAR (Yang et al., 2022). SAR-generated mining displacements are in close agreement with in situ 3D
displacements measured by a continuous GPS receiver at a RMSE of ⁓0.035, 0.015, and 0.005 m in the vertical,
east, and north directions, respectively, representing 3.7%, 8.8%, and 2.1% of the maximum 3D displacements
(i.e., 0.94, 0.17, and 0.24 m, respectively). A 12-day interferogram constructed from consecutive Sentinel-1 images
detects the landslide deformation pattern after 2D phase unwrapping and shows the accumulated deformation that
is clearly distinguishable from the surrounding areas (Crosetto et al., 2016). InSAR is more sensitive to vertical
displacement than horizontal creeping, but the opposite is true with images. Space-borne InSAR data-derived
deformations are in close agreement with in situ measurements obtained via inclinometer and GPS, and small
differences exist between time-series deformation determined using a ground-based InSAR sensor. RMS and mean
error values of InSAR-retrieved deformation rate deviate from the leveling mean deformation vertical velocities by0.5–0.9 mm ∙ yr
-1 along three leveling lines, and deformation in a heavily urbanized volcano island of Italy is
detected at a mean error of 0.2–0.4 mm ∙ yr-1 (Manzo et al., 2006).
FIGURE 5.30 Pace of 3D earthquake-induced deformation in two directions detected using DInSAR produced from
combined ascending and descending ALOS-2 SAR images. (c) Vertical displacement; (d) E-W displacement. White
and black lines represent the contour lines of the maximum deformation zones and the main faults. (Valerio et al.,
2018, open access.)
The major limitation of DInSAR is the absence of corner reflectors in vegetated areas, so they must be
artificially installed. In this way the method allows the estimation of deformation rates to several (potentially all
the) areas where no coherent DInSAR measurements are available due to temporal decorrelation to drastically
increase the spatial coverage of DInSAR measurements. Another major limitation of DInSAR is that most SAR
satellite images can measure deformation with a swath width ≤50–300 km. This limitation is imposed to reduce
slight inaccuracies in satellite orbital knowledge, which produce a linear or quadratic signal across most
interferograms.
REFERENCES
Abbas A and S Khan (2007) Using remote sensing techniques for appraisal of irrigated soil salinity. In L Oxley and
Kulasiri D (eds.) Int. Cong. Model. Simul. (MODSIM). Modelling and Simulation Society of Australia and New
Zealand, 2632–2638.
Abdelkader D, N Hervé, and W Christian (2006) Detecting salinity hazards within a semiarid context by means of
combining soil and remote-sensing data. Geoderma 134: 217–230. doi: 10.1016/j.geoderma.2005.10.009
Ahmad W, M Choi, S Kim, and D Kim (2019) Detection of land subsidence and its relationship with land cover
types using ESA Sentinel satellite data: A case study of Quetta Valley, Pakistan. Int J Rem Sens 40(24): 9572–
9603.Alam Md S, D Kumar, RS Chatterjee, and V Upreti (2018) Assessment of land surface subsidence due to
underground metal mining using integrated spaceborne repeat-pass differential interferometric synthetic aperture
radar (DInSAR) technique and ground based observations. J Indian Soc Rem Sens 46(10): 1569–1580. doi:
10.1007/s12524-018-0810-2
Albino F, B Smets, N d’Oreye, and F Kervyn (2015) High-resolution TanDEM-X DEM: An accurate method to
estimate lava flow volumes at Nyamulagira volcano (D. R. Congo). J Geophys Res Solid Earth 120: 4189–4207.
Al-Khaier F (2003) Soil salinity detection using satellite remotes sensing. ITC J P61.
Allbed A and L Kumar (2013) Soil salinity mapping and monitoring in arid and semi-arid regions using remote
sensing technology: A review. Adv Rem Sens 2: 373–385.
Anderson S and J Pitlick (2014) Using repeat lidar to estimate sediment transport in a steep stream. J Geophy Res:
Earth Surf 119: 621–643. doi:10.1002/2013JF002933
Baldo M, C Bicocchi, U Chiocchini, D Giordan, and G Lollino (2009) LiDAR monitoring of mass wasting
processes: The Radicofani landslide, Province of Siena, Central Italy. Geomor 105(3-4): 193–201.
Bannari A, A El-Battay, R Bannari, and H Rhinane (2018) Sentinel-MSI VNIR and SWIR bands sensitivity
analysis for soil salinity discrimination in an arid landscape. Rem Sens 10: 855. doi: 10.3390/rs10060855
Bayer B, A Simoni, DA Schmidt, and L Bertello (2017) Using advanced InSAR techniques to monitor landslide
deformations induced by tunneling in the Northern Apennines. Italy. Eng Geol 226: 20–32.
Behncke B, A Fornaciai, M Neri, M Favalli, G Ganci, and F Mazzarini (2016) Lidar surveys reveal eruptive
volumes and rates at Etna, 2007–2010. Geophys Res Lett 43: 4270–4278.
Blackbun GA (1998) Spectral indices for estimating photosynthetic pigment concentrations: A test using senescent
tree leaves. Int J Rem Sens 19: 657–675.
Bolch T, T Pieczonka, and DI Benn (2011) Multi-decadal mass loss of glaciers in the Everest area (Nepal
Himalaya) derived from stereo imagery. Cryosph 5: 349–358.
Booth AM, MP Lamb, J-P Avouac, and C Delacourt (2013) Landslide velocity, thickness, and rheology from
remote sensing: La Clapière landslide, France. Geophys Res Lett 40: 4299–4304. doi:10.1002/grl.50828
Carrer D, F Pinault, G Lellouch, IF Trigo, I Benhadj, F Camacho, X Ceamanos, S Moparthy, J Munoz-Sabater, L
Schüller, and J Sánchez-Zapero (2021) Surface albedo retrieval from 40-years of earth observations through the
EUMETSAT/LSA SAF and EU/C3S Programmes: The Versatile Algorithm of PYALUS. Rem Sens 13(3): 372.
doi: 10.3390/rs13030372
Chaussard E, S Wdowinski, E Cabral-Cano, and F Amelung (2014) Land subsidence in central Mexico detected by
ALOS InSAR time-series. Rem Sens Environ 140: 94–106. doi: 10.1016/j.rse.2013.08.038
Choe E, F van der Meer, F van Ruitenbeek, W van der Werff, B de Smeth, and K-W Kim (2008) Mapping of
heavy metal pollution in stream sediments using combined geochemistry, field spectroscopy, and hyperspectral
remote sensing: A case study of the Rodalquilar mining area, SE Spain. Rem Sens Environ 112(7): 3222–3233.
Crosetto M, O Monserrat, N Devanthéry, M Cuevas-González, A Barra, and B Crippaet (2016) Persistent scatterer
interferometry using Sentinel-1 data. The Int Arch. Photogram Rem Sens Spatial Info Sci XLI-B7, pp. 835–839.
doi: 10.5194/isprsarchives-XLI-B7-835-2016
Davis E, C Wang, and K Dow (2019) Comparing Sentinel-2 MSI and Landsat 8 OLI in soil salinity detection: A
case study of agricultural lands in coastal North Carolina. Int J Rem Sens 40(16): 6134–6153. doi:
10.1080/01431161.2019.1587205
Deems JS, PJ Gadomski, D Vellone, R Evanczyk, A LeWinter, KW Birkeland, and DC Finnegan (2015) Mapping
starting zone snow depth with a ground-based lidar to assist avalanche control and forecasting. Cold Reg Sci
Technol 120: 197–204. doi: 10.1016/j.coldregions.2015.09.002
Dehghani M, V Zoej, M Javad, AE Mansourian, and S Saatchi (2009) InSAR monitoring of progressive land
subsidence in Neyshabour, northeast Iran. Geophys J Int 178(1): 47–56. doi: 10.1111/j.1365-246X.2009.04135.x
Ding M, X Li, and Z Jin (2023) Digital mapping of soil organic carbon using UAV images and soil properties in a
thermo-erosion gully on the Tibetan Plateau. Rem Sens 15: 1628. doi: 10.3390/rs15061628Dong J, L Zhang, M Tang, M Liao, Q Xu, J Gong, and M Ao (2018) Mapping landslide surface displacements with
time series SAR interferometry by combining persistent and distributed scatterers: A case study of Jiaju
landslide in Danba, China. Rem Sens Environ 205: 180–198.
Dvorakova K, U Heiden, and B van Wesemael (2021) Sentinel-2 exposed soil composite for soil organic carbon
prediction. Rem Sens 13: 1791. doi: 10.3390/rs13091791
Elnaggar AA and S Noller (2010) Application of remote sensing data and decision-tree analysis to mapping salt
affected soils over large areas. Rem Sens 2(1): 151–165.
Esposito G, R Salvini, F Matano, M Sacchi, M Danzi, R Somma, and C Troise (2017) Multitemporal monitoring
of a coastal landslide through SfM-derived point cloud comparison. Photogram Rec 32: 459–479. doi:
10.1111/phor.12218
Fan X, Y Liu, J Tao, and Y Weng (2015) Soil salinity retrieval from advanced multi-spectral sensor with partial
least square regression. Rem Sens 7: 488–511. doi: 10.3390/rs70100488
Ferretti A, C Prati, and F Rocca (2000) Nonlinear subsidence rate estimation using permanent scatterers in
differential SAR interferometry. IEEE Trans Geosci Rem Sens 38: 2202–2212.
Frazier B and Y Cheng (1989) Remote sensing of soils in the eastern Palouse region with Landsat thematic
mapper. Rem Sens Environ 28: 317–325. doi: 10.1016/0034-4257(89)90123-5
Gao J (2023) Remote Sensing of Natural Hazards. Boca Raton: CRC Press, 437 p.
Ge L, H-C Chang, and C Rizos (2007) Mine subsidence monitoring using multi-source satellite SAR images.
Photogram Eng Rem Sens 73(3): 259–266.
Guerin A, GM Stock, MJ Radue, M Jaboyedoff, BD Collins, B Matasci, N Avdievitch, and M-H Derron (2020)
Quantifying 40 years of rockfall activity in Yosemite Valley with historical Structure-from-Motion
photogrammetry and terrestrial laser scanning. Geomor 356. doi: 10.1016/j.geomorph.2020.107069
Haboudane D, JR Miller, E Pattey, PJ Zarco-Tejada, and IB Strachan (2004) Hyperspectral vegetation indices and
novel algorithms for predicting green LAI of crop canopies: Modeling and validation in the context of precision
agriculture. Rem Sens Environ 90: 337–352. doi: 10.1016/j.rse.2003.12.013
Hede ANW, K Kashwaya, K Koike, and S Sakurai (2015) A new vegetation index for detecting vegetation
anomalies due to mineral deposits with application to a tropical forest area. Rem Sens Environ 171: 83–97. doi:
10.1016/j.rse.2015.10.006
Hooper A (2008) A multi-temporal InSAR method incorporating both persistent scatterer and small baseline
approaches. Geophys Res Lett 35(L16302): 96–106. doi: 10.1029/2008GL034654
Hsu WC, HC Chang, KT Chang, EK Lin, JK Liu, and YA Liou (2015) Observing land subsidence and revealing
the factors that influence it using a multi-sensor approach in Yulin county, Taiwan. Rem Sens 7: 8202–8223. doi:
10.3390/rs70608202
Hunt ER and BN Rock (1989) Detection of changes in leaf water content using Near- and Middle-Infrared
reflectances. Rem Sens Environ 30(1): 43–54.
Jiang H, Y Rusuli, T Amuti, and Q He (2019) Quantitative assessment of soil salinity using multi-source remote
sensing data based on the support vector machine and artificial neural network. Int J Rem Sens 40: 284–306.
Khan NM, VV Rastoskuev, EV Shalina, and Y Sato (2001) Mapping salt-affected soils using remote sensing
indicators - A simple approach with the use of GIS IDRISI. In Proc. 22nd Asian Conf. Rem. Sens., 5-9 November
2001, Center for Remote Imaging, National University.
Liang S (2001) Narrowband to broadband conversions of land surface albedo I Algorithms. Rem Sens Environ 76:
213–238.
Liang S, CJ Shuey, AL Russ, H Fang, M Chen, L Walthall, CST Daughtry, and R Hunt Jr (2003) Narrowband to
broadband conversions of land surface albedo II Validation. Rem Sens Environ 84: 25–41. doi:10.1016/S0034-
4257(02)00068-8
Liu K, H Su, and X Li (2017) Comparative assessment of two vegetation fractional cover estimating methods and
their impacts on modeling urban latent heat flux using Landsat imagery. Rem Sens 9: 455.Liu P, Z Li, T Hoey, C Kincal, J Zhang, and JP Muller (2013) Using advanced InSAR time series techniques to
monitor landslide movements in Badong of the three gorges region, China. Int J Appl Earth Obs Geoinfo 21:
253–264.
Liu SW, DH Guo, WT Chen, XW Zheng, SY Wang, and XJ Li (2012) The application of airborne LiDAR
technology in landslide investigation and monitoring of three gorges reservoir area. Geol China 39(2): 507–517.
Lucieer A, SM de Jong, and D Turner (2014) Mapping landslide displacements using Structure from Motion (SfM)
and image correlation of multi-temporal UAV photography. Prog Phys Geog: Earth Environ 38(1): 97–116. doi:
10.1177/0309133313515293
Manzo M, GP Ricciardi, F Casu, G Ventura, G Zeni, S Borgström, P Berardino, C Del Gaudio, and R Lanari
(2006) Surface deformation analysis in the Ischia Island (Italy) based on spaceborne radar interferometry. J
Volcanol Geoth Res 151: 399–416.
Mashimbye ZE, MA Cho, JP Nell, W. De Clercq, A Van Niekerk, and DP Turner (2012) Model-based integrated
methods for quantitative estimation of soil salinity from hyperspectral remote sensing data: A case study of
selected South African soils. Pedosph 22(5): 640–649. doi: 10.1016/S1002-0160(12)60049-6
Matsuoka A, T Yamakoshi, K Tamura, J Maruyama, and K Ogawa (2008) Sediment yield from seismically￾disturbed mountainous watersheds revealed by multi-temporal aerial LiDAR surveys. In Sediment Dynamics in
Changing Environments, Proc. of a symposium held in Christchurch, New Zealand, December 2008. I IAHS￾AISH Publication 325, 208–216.
Merton R and J Huntington (1999) Early simulation of the ARIES-1 satellite sensor for multi-temporal vegetation
research derived from AVIRIS. Summaries Eight JPL Airborne Earth Sci Workshop 99(17): 299–307.
Metternicht G, L Hurni, and R Gogu (2005) Remote sensing of landslides: An analysis of the potential contribution
to geo-spatial systems for hazard assessment in mountainous environments. Rem Sens of Environ 98(2-3): 284–
303. doi: 10.1016/j.rse.2005.08.004
Mu T, G Liu, X Yang, and Y Yu (2023) Soil moisture estimation based on multiple-source remote-sensing images.
Rem Sens 15: 139. doi:10.3390/rs15010139
Nádudvari Á, A Abramowicz, R Maniscalco, and M Viccaro (2020) The estimation of lava flow temperatures
using Landsat night-time images: Case studies from eruptions of Mt. Etna and Stromboli (Sicily, Italy), Kilauea
(Hawaii Island), and Eyjafjallajökull and Holuhraun (Iceland). Rem Sens 12: 2537. doi: 10.3390/rs12162537
Palamara DR, M Nicholson, P Flentje, E Baafi, and GM Brassington (2007) An evaluation of airborne laser scan
data for coalmine subsidence mapping. Int J Rem Sens 28(15): 3181–3203. doi: 10.1080/01431160600993439
Penuelas J, F Baret, and I Filella (1995) Semi-empirical indices to assess carotenoids/chlorophyll α ratio from leaf
spectral reflectance. Photosynthetica 31(2): 221–230.
Peón J, C Recondo, SF Fernández, J Calleja, E De Miguel, and L Carretero (2017) Prediction of topsoil organic
carbon using airborne and satellite hyperspectral imagery. Rem Sens 9:1211. doi:10.3390/rs9121211
Potić I, M Bugarski, and J Matić-Varenica (2017) Soil moisture determination using remote sensing data for the
property protection and increase of agriculture production. Paper presented at the 2017 World Bank Conference
On Land And Poverty, Washington DC, March 20-24, 2017.
Prokop A, P Schön, F Singer, P Gaëtan, M Naaim, and E Thibert (2013) Determining avalanche modelling input
parameters using terrestrial laser scanning technology. In Proc. of the Inter. Snow Science Workshop, 2013,
Chamonix Mont-Blanc, 770–774.
Rathore P, B Das, A Sharma, D Roy, D Chakraborty (2021) Remote Sensing of Soil Moisture. New Delhi: Division
of Agricultural Physics, ICAR-Indian Agricultural Research Institute, 110012 p. doi:
10.13140/RG.2.2.14074.62408
Rengers FK, TD Rapstine, M Olsen, KE Allstadt, RM Iverson, B Leshchinsky, M Obryk, and JB Smith (2021)
Using high sample rate lidar to measure debris-flow velocity and surface geometry. Environ Eng Geosci 27(1):
113–126. doi: 10.2113/EEG-D-20-00045
Robson BA, D Hölbling, PR Nielsen, and M Koller (2022) Estimating the volume of the 1978 Rissa quick clay
landslide in Central Norway using historical aerial imagery. Open Geosci 14: 252–263. doiL10.1515/geo-2020-0331.20
Roering JJ, LL Stimely, BH Mackey, and DA Schmidt (2009) Using DInSAR, airborne LiDAR, and archival air
photos to quantify landsliding and sediment transport. Geophy Res Lett 36: L19402.
doi:10.1029/2009GL040374
Schlögel R, C Doubre, J-P Malet, and F Masson (2015) Landslide deformation monitoring with ALOS/PALSAR
imagery: A D-InSAR geomorphological interpretation method. Geomor 231: 314–330.
Shin JH, J Yu, L Wang, L Kim, S-M Koh, and S-O Kim (2019) Spectral responses of heavy metal contaminated
soils in the vicinity of a hydrothermal ore deposit: A case study of boksu mine, South Korea. IEEE Trans Geosci
Rem Sens 57(6): 4092–4106.
Shoshany M, T Svoray, PJ Curran, GM Foody, and A Perevolotsky (2000) The relationship between ERS-2 SAR
backscatter and soil moisture: Generalization from a humid to semiarid transect. Int J Rem Sens 21: 2337–2343.
Thaler EA, IJ Larsen, and Q Yu (2019) A new index for remote sensing of soil organic carbon based solely on
visible wavelengths. Soil Sci Soc Am J 83: 1443–1450. doi: 10.2136/sssaj2018.09.0318
Tsutsui K, S Rokugawa, H Nakagawa, S Miyazaki, CT Cheng, T Shiraishi, and SD Yang (2007) Detection and
volume estimation of large-scale landslides based on elevation-change analysis using DEMs extracted from
high-resolution satellite stereo imagery. IEEE Trans Geosci Rem Sens 45: 1681–1696.
Valerio E, P Tizzani, E Carminati, C Doglioni, S Pepe, P Petricca, C Luca, C Bignami, G Solaro, R Castaldo, V de
Novellis, and R Lanari (2018) Ground deformation and source geometry of the 30 October 2016 Mw 6.5 Norcia
earthquake (central Italy) investigated through seismological data, DInSAR measurements, and numerical
modelling. Rem Sens, 10(12): 1901. doi: 10.1901.10.3390/rs10121901
Wagner W, G Lemoine, and H Rott (1999) A method for estimating soil moisture from ERS scatterometer and soil
data. Rem Sens Environ 70: 191–207.
Wang F, J Gao, and Y Zha (2018b) Hyperspectral sensing of heavy metals in soil and vegetation: Feasibility and
challenges. ISPRS J Photogram Rem Sens 136: 73–84. doi: 10.1016/j.isprsjprs.2017.12.003
Wang S, Y Chen, M Wang, and J Li (2019b) Performance comparison of machine learning algorithms for
estimating the soil salinity of salt-affected soil using field spectral data. Rem Sens 11(22): 2605. doi:
10.3390/rs11222605
Wang W, K Liu, R Tang, and S Wang (2019a) Remote sensing image-based analysis of the urban heat island effect
in Shenzhen, China. Phys Chem Earth, Parts A/B/C 110: 168–175. doi: 10.1016/j.pce.2019.01.002
Wang X, F Zhang, J Ding, HT Kung, A Latif, and VC Johnson (2018a) Estimation of soil salt content (SSC) in the
Ebinur Lake Wetland National Nature Reserve (ELWNNR), Northwest China, based on a Bootstrap-BP neural
network model and optimal spectral indices. Sci Total Environ 615: 918–930.
Wright R, LP Flynn, and AJL Harris (2001) Evolution of Lava flow-fields at Mount Etna, 27–28 October 1999,
observed by Landsat 7 ETM+. Bull Volcanol 63: 1–7. doi: 10.1007/s004450100124
Yahiaoui I, A Douaoui, Q Zhang, and A Ziane (2015) Soil salinity prediction in the lower Cheliff Plain (Algeria)
based on remote sensing and topographic feature analysis. J Arid Land 7(6): 794–805. doi: 10.1007/s40333-015-
0053-9
Yang Y, Q Cui, P Jia, J Liu, and H Bai (2021) Estimating the heavy metal concentrations in topsoil in the Daxigou
mining area, China, using multispectral satellite imagery. Sci Rep 11: 11718. doi: 10.1038/s41598-021-91103-8
Yang Z, J Zhu, J Xie, Z Li, L Wu, and Z Ma (2022) Resolving 3-D mining displacements from multi-track InSAR
by incorporating with a prior model: The dynamic changes and adaptive estimation of the model parameters.
IEEE Trans Geosci Rem Sens 60: 1-10. doi: 10.1109/TGRS.2021.3093058
Ye X, H Kaufmann, and XF Guo (2004) Landslide monitoring in the Three Gorges area using D-INSAR and
corner reflectors. Photogram Eng Rem Sens 70: 1167–1172.
Yin F, M Wu, L Liu, Y Zhu, J Feng, D Yin, C Yin, and C Yin (2021) Predicting the abundance of copper in soil
using reflectance spectroscopy and GF5 hyperspectral imagery. Inter J Appl Earth Obs Geoinfo 102: 102420.
doi: 10.1016/j.jag.2021.102420  6
6.1
6.1.1
Quantification in the Biosphere
DOI: 10.1201/9781003517504-8
The biosphere is a thin layer enclosed by the underlying terrestrial sphere and the atmosphere just
above it. In spite of this thinness, it plays a vital role in the survival and well-being of humans as
both the food they consume and the oxygen they breathe in originate from photosynthesis of plants
in the biosphere. The majority of the 21% of the Earth’s terrestrial surface is covered by vegetation
of one type or another. It is the vegetative cover that has been exhaustively studied using remote
sensing. Of all the ground features, vegetation is the most directly visible on remote sensing
imagery, especially in the infrared spectral range, and the easiest to quantify reliably, based mostly
on VIs. Imagery data are the best for quantifying horizontally spreading parameters such as
fractional vegetation cover (FVC) and density, but they are not so sensitive to the vertical relief of
vegetation because optical imagery data are acquired mostly from the air or space in nadir viewing
directly above the target of sensing. The target of quantification is diverse, both physical state (e.g.,
biomass and structure) and dynamic change (e.g., NPP). The quantification of these biosphere
parameters has been achieved from both imagery and LiDAR data. Although LiDAR data are also
acquired in a similar airborne and space-borne fashion to imagery data, they are more capable of
penetrating the canopy to reveal 3D information on the target. Consequently, LiDAR data are more
suited to quantify 3D biophysical features of vegetation. In addition, LiDAR data can also be
acquired on the ground, yielding more information on the vertical variation of vegetation. Thus, the
most reliable quantification results in the biosphere are obtainable via synergistic use of both
imagery and LiDAR data that are complementary to each other in their properties.
This chapter starts with a description of the spectral behavior of vegetation to identify the most
suitable wavelengths for the quantification. Also introduced in the same section are the diverse
generic spectral indices that have been widely used for the task. The discussion then progresses to
the quantification of biophysical features. Since LAI and AGB are both broad topics, they are
separately covered in two other sections to keep each section within a manageable length. The other
features quantified are biochemical features of chlorophyll content and LUE. The next section
elucidates the quantification of major crop yields. Finally, this chapter elaborates on how to quantify
fire-related parameters, including the rate of fire spread and intensity.
VEGETATION SPECTRAL BEHAVIOR AND INDICES
Spectral Signature of VegetationAlthough Figure 3.4 illustrates the general shape and pattern of the spectral reflectance curve of
vegetation, it is too crude for studying the precise properties of vegetation in detail. The proper
quantification of diverse vegetation parameters requires an in-depth scrutiny of the curve under
varying conditions. The spectral behavior of all types of plants, being they annuals, perennials, or
trees, follows the same general pattern characterized by a low reflectance in the visible spectrum,
and an abrupt and sharp rise at red/NIR wavelengths, and a rather high reflectance beyond NIR with
little variation with wavelength (Figure 6.1). Within the visible light wavelengths of 0.4–0.7 μm, the
unique pattern of reflectance is characterized by a minor peak reflectance at green wavelength and a
trough at red wavelength caused by the absorption of the radiation by moisture and chlorophyll.
Spectral bands capturing such variations are strong at detecting chlorophyll content. Of particular
notice is the abrupt and steep rise in reflectance at longer wavelengths between 0.681 and 0.753 μm
to a plateau. Commonly known as “red-edge”, it has been widely exploited to quantify the
chlorophyll content, leaf condition, and physiological status of terrestrial vegetation. It plays a
decisive role in the success or failure of quantifying vegetation parameters as it is highly sensitive to
the target of sensing, such as the content of heavy metal contaminants.
FIGURE 6.1 Spectral response pattern of typical types of vegetation over the spectral range of 0.4–2.4
μm. Leaf chlorophyll strongly absorbs radiation at 0.45 µm and 0.67 µm and has high reflectance in
the NIR spectral region (0.7–1.1 µm). In the shortwave-IR region, vegetation has three absorption
features that are related directly to the absorption of water contained within the leaf. (Modified from
Alleaume et al., 2013.)Admittedly, the general shape or pattern of the spectral response curve hardly varies for a given type
of vegetation or vary only slightly among different types of vegetation (Figure 6.1), but the
magnitude in certain portions of the spectrum may vary noticeably with vegetation status (e.g.,
health condition, stress level, and the growing environment) (Figure 6.2). The changes in spectral
curves are predominantly reflective of the amount of vegetation on ground, subject to the influence
of the environs. It is such variations that are exploited for quantifying various aspects of vegetation,
including its composition. The spectral pattern can also reveal special wavelength regions that
enable the maximum separation of one type of vegetation from another or differentiate its
conditions. The spectral signature of vegetation provides guidance as what are the best wavelengths
(bands) that should be selected for quantifying a given quality of vegetation. Since the spectral
reflectance of the target is likely to vary in various spectral regions, multispectral bands can capture
more spectral information on the target of sensing than a singular band. For instance, one band may
capture the peak reflection of the target at one wavelength while another band records its maximum
absorption at another wavelength. The use of multiple bands not only enriches the spectral
information on the target but also can suppress the influence of the background and the sensing
environment on the target’s spectral signature. It must be noted that the identified best bands may
not be universal as the spectral pattern is subject to the influence of multiple factors, including the
image properties and the vegetation growing stages. For instance, the best wavebands for estimating
crop yield at the mature stage may not be the best after all at the early juvenile stage. Besides, the
identified best wavelength range is still subject to the band designation and spectral resolution of
sensors. In a word, it is impossible to pinpoint the exact wavelength range of the best bands to use in
abstract. So it always pays to measure the spectral behavior of the target in the area of study before
the best bands for sensing can be determined.6.1.2
FIGURE 6.2 Spectral reflectance curves of bermudagrass and tree leaves of various conditions. Watered
grass is growing in a container. Dry grass is not watered in seven days. The same grass not watered in
14 days is dry, yellowing, and has a spectral curve almost identical to yellowing leaves. (Modified
from Caturegli et al., 2020, open access; and Yengoh et al., 2014, © Lund University Center for
Sustainability Studies.)
Affecting Factors
The spectral pattern shown in Figure 6.2 is affected by a number of factors, the most important
being the vegetation species, its biomass, and the environment (e.g., seasonality). Mature trees with
broad leaves are the most reflective, having the highest reflectance at both green and NIR
wavelengths. In comparison, coniferous trees with a small LAI have a lower reflectance at these
wavelengths. The reflectance is particularly subdued at NIR wavelengths. Compared with plants,
crops have a lower biomass that varies with crop type and seasonality. Their reflectance is the
highest at the mid-stage, but lower at the early stages of little biomass. Healthy and vigorously
growing plants have a high reflectance, but the reflectance of senescent or withered plants at the
same wavelength is significantly reduced due to the lowered chlorophyll content. The spectral
reflectance pattern shown in Figure 6.2 is also affected by the soil background, especially when the
vegetation has an incomplete or discontinuous cover. Soil reflectance is much higher than
vegetation’s when the ground is dry, and interferes with the spectral reflectance of vegetation.
Finally, the measured spectral reflectance is also affected by the viewing geometry. Under the same
growing conditions, a given vegetation will reflect the incident energy slightly differently due to the
viewing angle in relation to the incoming solar radiation. For broadleaf trees, leaf orientation also
exerts an effect. It is impossible to quantify the effect of each of the influencing factors exactly. In6.2
6.2.1
general, it is possible to study vegetation state and quantify its major biophysical and biochemical
parameters using remote sensing, but the accuracy of quantification is subject to the influence of the
identified factors.
PHYSICALAND SEMI-PHYSICAL MODELS
Over the years biologists and remote sensing scientists have attempted to describe the radiant energy
redistribution within a canopy using physical and semi-physical models. Physical models are
mathematical equations, either numerical or conceptual, that depict how the radiative energy from
the sun is transferred in different media or over different surfaces. Physical models are based on
established, well-founded knowledge of variable relationships. They articulate the physical
relationships between structural parameters of vegetation and the spectral behavior of plant leaves
and canopy. They can spell out the cause-effect relationships between the dependent variable and its
influencing factors. Model variables are inferred based on domain-specific knowledge, typically
gained with the assistance of radiative transfer functions. Physical models are grounded on physical
laws governing the transfer and interaction of radiative energy within the canopy, and are able to
explain how the dependent variable is affected by all the relevant independent variables. They have
varied complexity and may involve different parameters.
Commonly Used Models
The nature and complexity of physical models are dictated by the domain, scale, and purpose of
application. They may depict how crop growth is related to Photosynthetically Active Radiation
(PAR) in estimating crop yield, such as the simple algorithm for yield estimation (SAFY) for wheat
yield estimation (Ma et al., 2022). In simulating forest spectra, models can be at the leaf level and
the canopy level. Leaf-level models are exemplified by the Scattering by Arbitrarily Inclined Leaves
(SAIL) model that requires leaf reflectance and transmittance spectra as inputs in calculating the
canopy bidirectional reflectance considering leaf optical properties, canopy structure (LAI, mean
leaf inclination angle), illumination and viewing geometry (zenith and relative azimuth viewing
angles, zenith solar angle), and the wavelength-dependent reflectance of the underlying soil.
Atmospheric conditions are taken into account by the fraction of diffuse illumination (Verhoef,
1984). The other two widely used leaf-level models are Leaf Optical Properties Spectra
(PROSPECT) and LIBERTY RTM. Renowned canopy-level models are exemplified by the
PROSAIL RTM and the Invertible Forest Reflectance Model (INFORM). The former is an
innovative extension of the Forest Light Interaction Model (FLIM), with crown transparency,
infinite crown reflectance, and understory reflectance simulated using physically-based sub-models
of SAILH, LIBERTY, and PROSPECT. Thus, INFORM is essentially an innovative combination of
three physically-based models for simulating crown and understory reflectance. The INFORM RTM
simulates the bidirectional reflectance of a surface and top-of-canopy bidirectional spectral
reflectance of the target. It requires the specification of 15 input parameters, including their range,
standard deviation, and distribution (fixed, uniform, and truncated Gaussian). The bidirectional
reflectance (ρ) of forest stands between 400 and 2500 nm is simulated as a function of internal
canopy parameters, internal leaf parameters, and external parameters as:(6.1)
All the terms in the above equation will be explained below in the proper context. FLIM treats the
forest as a discontinuous canopy layer comprising tree crowns and gaps, so both the effects of
shadow and crown transmittance are taken into account in calculating stand reflectance R in a given
spectral band as:
(6.2)
where RC = crown reflectance at the infinite crown depth; RG = background reflectance. RC and RG
= endmember spectra derived from the image itself (Schlerf and Atzberger, 2006). C = crown factor
calculated as:
(6.3)
where Ts = mean crown transmittance in the solar direction; To = mean crown transmittance in the
viewing direction; both Ts and To are exponentially related to LAI; co, cs = ground coverage by
crowns in the viewing and solar direction, respectively, calculated from the viewing zenith angle
(θo) and solar zenith angle (θs) as:
(6.4)
where SD = tree or stem density (ha-1); and k = constant representing the average tree crown
horizontal area in hectares given by:
(6.5)
where CD = crown diameter in meters. The “ground factor” G is calculated as:
(6.6)
where Fx = ground surface fractions; Fa = tree crowns with shadowed background, Fb = tree crowns
with sunlit background, Fc = shadowed open space, and Fd = sunlit open space.
FLIM disregards the effects of leaf geometry and leaf optical properties on crown transmittance
terms (Ts and To). Instead, they are approximated as simple exponential functions of LAI, which can
be avoided by adopting the SAILH model. It calculates the bidirectional transmittance of a
vegetation canopy as one of four fluxes. To and Ts for a given leaf transmittance τ and leaf
reflectance ρ are determined from LAI, average leaf inclination angle (ALA), observation angle (θo),
sun angle (θs), the relative azimuth angle between the sun and the sensor (), the fraction of diffuse
radiation (skyl), and the hot spot parameter (hsp) as:
(6.7)
where hsp is defined as the ratio of the average leaf size to canopy height.
The crown reflectance at infinite crown depth RC and the background reflectance RG in Eq. 6.2
are computed using the SAILH model and an appropriate leaf optical property model for a givenillumination and observation geometry as:
(6.8)
where LAIU and ALAU = LAI and ALA of the understorey vegetation, respectively; and ρsoil = soil
reflectance either in situ measured or from the literature. The leaf optical properties of the
understorey vegetation ) are calculated using the PROSPECT model. For a given illumination and
observation geometry, RC is computed using Eq. 6.8 in which LAIU is replaced by LAIinf, the LAI at
infinite crown depth (i.e. LAI = 15).
Neither FLIM nor PROPECT in INFORM are applicable to coniferous forests where needles are
densely clumped within shoots. The multiple scattering of radiation within a shoot increases
absorption and reduces canopy reflectance in comparison with a canopy of a similar LAI. So it is
better to compute leaf spectral reflectance and transmittance of the over- and understorey plants
using LIBERTY. LIBERTY treats a needle as an aggregation of cells and calculates multiple
scattering between them. Needle transmittance (τ) and reflectance (ρ) are computed as a function of
three structural parameters (cell diameter-d, intercellular air space-i, and leaf thickness-t), and the
combined absorption coefficient of leaf biochemicals, given their respective concentrations
(chlorophyll-cAB, water-cW, lignin and cellulose-cL, and protein-cP):
(6.9)
The inversion of this modified INFORM requires 17 input parameters, of which five are external,
three related to canopy, and nine related to leaf properties (Schlerf and Atzberger, 2006). In
PROSPECT, leaf reflectance (ρleaf) and transmittance are treated as a function of the leaf mesophyll
structure parameter N, chlorophyll a + b concentration (Cab), leaf equivalent water thickness (Cw),
and leaf dry matter content (Cm) (Figure 6.3).FIGURE 6.3 A numerical inversion approach of retrieving biophysical parameters based on
PROSPECT and SAIL models (with Cw and Cm tied together in a 4:1 ratio) with RMSEr as the cost
function (N, ρs, θv, ψv, θs, and skyl are known or fixed, respectively). (Adapted from Vohland et al.,
2010.)
All physical models are extremely complex in some cases. The most distinguished strength of
physical models is their universality. Once properly parameterized, they can be applied to all
environments in all places (Table 6.1). They can also yield an indication of quantification accuracy.
But a model may contain many parameters that must be properly configured, usually from field￾collected data in a costly and slow process. The quality of the retrieved results is subject to the prior
knowledge and model regularization, as well as measurement uncertainty.
Table 6.1
Strengths and limitations of physical model inversion in operational retrieval of quantitative attribute values of vegetation
parameters from remotely sensed data
Strengths Limitations
• Full-spectrum methods
• Yielding additional information on
retrieval uncertainty
• Physically-based methods subject to
the impact of regularization factors
• General and global applicability
• Capability to provide multiple
outputs
• Complex involving many parameters
• Model parameterization and optimization essential using
field data
• Computationally demanding and inversion not always
successful
• Retrieval quality subject to the RTM used, prior
knowledge, and regularization
• Imposed upper/lower boundaries in the LUT have a
logical consequence and cannot be exceeded by estimated
variables
• LUT-based inversion strongly affected by noise and
measurement uncertainty (Liang, 2007)6.2.2
Source: Modified from Verrelst et al. (2015a).
The complexity of physical models can be simplified by approximating some variables with others
or by substituting some variables with field-collected data, effectively turning physical models to
semi-physical models. They have the strength of being able to explain the cause-effect relationship
between the dependent and independent variables while being not overly complex. In general, both
physical and semi-physical models require a huge amount of data to parameterize model variables.
Some of them may be a function of time, so have to be logged continuously. How to parameterize
physical models is the biggest hurdle in using them, even after some simplification that must also be
supported by in situ data. The alternative to field data is via simulation. For instance, how much
solar radiation is absorbed by aerosols in the atmosphere can be simulated under all possible
viewing angles and solar elevations. The simulated results are then input into physical models. In
spite of the complexity, physical models still play a vital role in quantitative remote sensing as they
supply the reference data, against which empirical model results are compared to check their
accuracy.
Hybrid Methods
The inversion of physical models yields the dependent variable, but always involves a trade-off
between model invertibility and realism. Realistic models are generally more difficult to invert than
simple ones. Traditionally, RTM model parameters are inverted via iterative optimization. Iteration
is a slow process that may not yield the globally optimal outcome. This deficiency can be overcome
by hybridizing models constructed using more than one method. For instance, physical models may
be hybridized with advanced non-parametric regression models to take advantage of their respective
strengths, namely, a high level of generalizing ability of the former, and the flexibility and
computational efficiency of the latter, achieved through the use of LUT and predictive equations.
The hybrid approach distinguishes itself from the LUT approach by using all available data to train a
(non-linear) non-parametric regression model, whereas the LUT approach seeks for a simulated
spectrum as close as possible to the measured one.
The hybridization of the two may be realized using various approaches. A common way is to
train a non-parametric model with simulated data produced from the inversion of an RTM. For
instance, forward modeling is relied on to generate synthetic canopy spectra that are fed to a
physical RTM (e.g., PROSAIL) to estimate biophysical variables of vegetation (Schlerf and
Atzberger, 2006). In the forward model, canopy reflectance models compute the spectral reflectance
for a certain set of leaf and canopy parameters (Figure 6.4). The modeled results are used to train a
Neural Network (NN) to the measured HyMap spectra for estimating forest canopy variables and
eventually modifying the network configuration. The properly trained NN is finally applied to the
remote sensing image for generating structural parameter maps based on predictive equations. They
are constructed from a large set of simulated data and corresponding parameters in a way
reminiscent of empirical regression equations. Although they can be applied to real images in
straightforward and fast manners without the need for field data to establish, as the training dataset
is generated by the RTM, they have to be established individually for each parameter to be retrieved,
and their precise form has to be specified explicitly by the analyst (i.e. linear or non-linear relation,
type of function, number of independent variables to be included).6.2.3
FIGURE 6.4 An example of hybridizing a canopy reflectance model with ANN in retrieving canopy
structural parameters. (Modified from Schlerf and Atzberger, 2006.)
All of the drawbacks of physical models can be avoided by using ANN that has been diversely
hybridized with them, such as the canopy RTM SAIL (Verhoef, 1984) and the PROSPECT-SAIL
models. In this hybridization, physical models produce simulated data that are used to train an ANN
(Verrelst et al., 2015a). This type of hybridization has been implemented as a core routine in various
processing chains, notably the PROSAIL inversion in the SNAP toolbox. Other machine learning
algorithms such as SVR models can also be similarly trained with PROSAIL data. The trained ANN
and SVR are then employed to predict biophysical parameters via reflectance model inversion
(Figure 6.4). This integration has been implemented for operational retrieval of several biophysical
data products (i.e., LAI, fraction of absorbed photosynthetically active radiation (FAPAR), and
FCOVER) from Sentinel-2 top-of-canopy reflectance data (Weiss and Baret, 2016).
Model Inversion StrategiesPhysical model inversion aims to infer model variables based on generally accepted knowledge
embedded in RTMs and a set of remote sensing image variables. The inversion of an RTM with
(full-spectrum) remote sensing data is a physically sound approach for retrieving bio-parameters in
the biosphere. Because of model complexity, it is almost impossible to come up with an analytical
solution for them. Instead, it has to rely on model inversion. The successful inversion of a physical
model depends on its soundness, an appropriate inversion procedure, and a set of calibrated samples.
Traditionally, model inversion is realized via optimization by minimizing a cost function. Many
optimization algorithms are in existence, such as particle swarm optimizer (PSO) and atom search
optimizer. The former is a population-based stochastic optimizer that has several favorable strengths,
including its basic structure, resilient mobility, and ease of implementation. Each particle is
considered a plausible solution in the search space of an optimization problem. The control
parameters determine the convergence of particle trajectories, keeping track of each particle’s unique
best fitness value, locating the global best particle, and updating each particle’s location and
velocity. In case of no convergence, the iterative process is repeated until either the optimization
problem converges to an optimal solution, or the maximum number of iterations has reached. The
atom search optimizer simulates the fundamental concepts of molecular dynamics and atom
movement principles, such as potential function characteristics, contact force, and geometric
constraint force. In PSO, each atom keeps track of two vectors: position and velocity. When it comes
to binary optimization, the atoms only have to deal with two numbers (“1” or “0”). As a result, a
means to leverage the atom’s velocity to alter the position from “0” to “1” or vice versa should be
discovered.
Apart from the least squares estimation distance, different cost functions dealing with different
distribution classes enable better treatment of outliers and non-linear distortions than the commonly
used ones. Most of these functions require one or two parameters to be tuned, which may hinder
their operational applications. It is better to use those cost functions with no additional parameters.
They are numbered as high as 10, all being potentially promising (Table 6.2). They all seek the best
estimate by solving the minimization problem using different statistical distances.
Table 6.2
Commonly used cost functions D[P,Q] for inversion optimization
Cost function Algorithm
Least absolute error
Shannon (1948) -
L-divergence Lin
Bhattacharyya divergence
Jeffreys-Kullback-Leibler
Neyman χ2
Pearson χ2
Normal distribution-Least square estimation
Geman and McClure
ExponentialSource: Verrelst et al. (2015b), with permission (5751611060847) from Elsevier.
Note: D[P,Q] represents the distance between two functions, where P = [p(λ1), p(λ2), …, p(λn)] is the reflectance signature derived
from satellite data and Q = [q(λ1), q(λ2), …, q(λn)] is the simulated reflectance with λ1, λ2, …, λn representing n spectral bands,
contained in a LUT.
The complexity of physical models demands that they be resolved via iteration to find the optimal
estimates of model parameters. This strategy faces three critical drawbacks: (i) difficulty in
obtaining globally optimal and stable results, (ii) difficulty in retrieving more than two parameters
simultaneously, and (iii) low computational efficiencies that prohibit an operational application on a
per-pixel basis for regional or global applications (Table 6.3). The required computer power
increases with the number of parameters to be retrieved and can be very high if the dataset has a low
(spectral and/or angular) dimension. Moreover, an initial guess has to be supplied to initialize the
search in the parameter space. Hence, multiple runs are necessary to prevent artefact related to the
starting position of the search algorithm.
Table 6.3
Comparison of the strengths and limitations of three model inversion strategies
Strategy Strengths Limitations
Numerical
optimization • Accurate
• Hard to obtain optimal and stable
results;
• Hard to retrieve >two parameters
simultaneously;
• Computationally inefficient;
• Time consuming, slow
LUT
• Accurate
• Easily applied to real imagery data
• Rather large table at a high density or
involving multiple parameters;
• Time-consuming to search table
ANN
• Computationally simple & fast;
• Able to represent (non-linear)
relationship between input and output
variables
• Time-consuming to train network;
• Unpredictable network behavior if no
quality training sample is presented to
it
These inversion strategies are time consuming and lengthy to realize. One way of expediting the
inversion process is to use LUTs and predictive equations. The identification of the modeled
dependent variable is finalized from the LUT that closely resembles the measured ones. A LUT
stores all possible data values (e.g., spectra) under different conditions, against which the measured
values are compared. The optimal estimate is determined if it corresponds to the parameter
combination yielding the maximum agreement between the measured and simulated values (i.e. the
smallest value of the merit function). If the inversion is based on LUT, a LUT representing a broad
set of realizations is built using an RTM directly. However, LUT becomes too large to handle if the
parameter space is to be sampled at a high density and/or if many parameters are inverted, even
though it is possible to impose constraints on model variables based on prior knowledge in the
development of a LUT to accelerate the inversion process. This may lead to a time-consuming6.3
search as each pixel has to be scanned individually (Schlerf and Atzberger, 2006), which can be
overcome using ANN.
Table 6.4
Comparison of accuracy of three inversion strategies in retrieving canopy biophysical parameters based on 12 validation plots
Method
Accuracy
indicator
LAI (m2 ×
m-2)
LAI × Cab (mg ×
cm-2)*
LAI × Cw (g ×
cm-2)*
LAI × Cm (g ×
cm-2)*
Numerical
optimization
Pearson’s r 0.92** 0.97** 0.88** 0.86**
RMSE 1.34 39.6 0.0252 0.0083
RPD 1.81 2.65 1.67 0.75
LUT
Pearson’s r 0.94** 0.97** 0.91** 0.85**
RMSE 1.48 84.2 0.0419 0.0120
RPD 1.64 1.25 1.00 0.52
ANN
Pearson’s r 0.78** 0.95** 0.83** 0.72**
RMSE 2.27 85.5 0.0981 0.0234
RPD 1.07 1.23 0.43 0.27
Source: Modified from Vohland et al. (2010).
Notes:
* Per ground area.
** Significant at the p=0.01 level.
Boldface: best results.
The performance of three inversion strategies has been comparatively assessed (Table 6.4). ANN
produces less accurate predictions than LUT that is worse than numerical optimization, indicating
that ANN may not always be the best choice for inversion applications. Numerical optimization
produces the most accurate results, followed by LUT and, with distinctly lower accuracy, the ANN
approach (Vohland et al., 2010). The results retrieved from ANN show a distinct shift towards higher
values than those from the other two methods. All three retrieval methods overestimate lower LAI
values but tend to underestimate the highest measured LAI values (LAI ≥ ⁓8).
BIOPHYSICAL VARIABLES
Biophysical variables of vegetation depict the physical status of trees and plants visible to the human
eyes. They may apply to individuals in a forest (e.g., tree height and diameter) or collectively for all
trees at the plot and watershed levels (e.g., FVC), all of which can be quantified from remotely
sensed data, even though both the ease and accuracy of quantification are subject to the type of
remotely sensed data used. The spectral variation of plant leaves and canopies with their chlorophyll
content and orientation provides crucial clues for how to best quantify the biophysical and
biochemical parameters of vegetation, such as primary productivity and canopy chlorophyll content
(CCC). The commonly quantifiable variables include LAI, biomass, FVC, FAPAR, NPP, and LUE.
Biochemical variables include dry matter content and chlorophyll content, all being essential input
parameters to crop yield and other climate models.6.3.1 Fractional Vegetation Cover and Tree Volume
FVC refers to the vertically projected ground area covered by tree crowns, so is best quantified from
nadir-view imagery data. Synonymous with relative vegetation abundance, this crucial biophysical
parameter is significant to studying the atmosphere, pedosphere, hydrosphere, biosphere, and their
interactions. FVC is a biophysical variable essential to simulating earth surface processes, modeling
climate, and studying global changes. Arithmetically, FVC is calculated from the area of tree (plant)
crowns projected onto the horizontal ground divided by the total ground area. It can be reliably
retrieved from remotely sensed VIs using relative vegetation abundance algorithms scaled by the
range of VI values, spectral mixing analysis, (semi-)empirical models, and machine learning
methods to generate fine-resolution FVC maps (Gao et al., 2020). Relative vegetation abundance
algorithms can take the form of linear or quadratic models. The simplest model takes the following
form:
(6.10)
where a = coefficient, b = offset.
The relative vegetation abundance algorithms are applicable for FVC quantification at the pixel
level. If the satellite imagery has a coarse spatial resolution, the quantified results may not be
detailed and accurate enough to meet the modeling needs. In this case, the quantification is carried
out at the sub-pixel level using spectral linear unmixing analysis. In this analysis, the value of a
mixed pixel r is decomposed as the proportional (weighted) sum of the signal of all its components
or members linearly, namely:
(6.11)
where fi = fractional cover of endmember i, ri = spectral signal of endmember i; n = total number of
endmembers, and = residual error of the model. In order to limit the complexity of the unmixing
model, the maximum number of endmembers in a pixel is usually restricted to four. In case of binary
endmembers (i.e., dichotomy of vegetation vs non-vegetation), Eq. 6.10 can be simplified as:
(6.12)
where
(6.13)
and
(6.14)
where f = proportion of vegetated area within the mixed pixel (fvc), NDVI = NDVI of the mixed
pixel; and NDVIv = NDVI of fully vegetated pixels, determined from terrestrial ecoregions of the
world and global land cover data for each biome (Jia et al., 2015).
Semi-physical methods rely on the portion of light transmitted through the canopy (Ptr) as it is
cast downward at the 0° zenith angle P0(0) in the form of:(6.15)
where Ptr in the solar zenith angle φ can be approximated as an exponential function of LAI (refer to
Section 6.3 for its quantification) as:
(6.16)
where a = leaf absorptance; = clumping index to account for the non-random spatial distribution of
phyto-elements within the canopy; = canopy attenuation coefficient analogous to the light extinction
coefficient (KVI) (Xiao et al., 2016). It is affected by canopy structure and properties, such as leaf
orientation, solar angle, and viewing geometry. For an ellipsoidal leaf angle distribution, it is
calculated as:
(6.17)
where x = ratio of mean areas of canopy elements projected onto horizontal and vertical surfaces. Its
value varies with vegetation types, such as 0.8 for grasses and crops, 1.0 for shrubs and Savannah,
and 1.2 for forests (Campbell and Norman, 1998). Apparently, the accuracy of the retrieved fvc
varies with the quality of LAI data products (see Section 6.3 for more details), which can be avoided
by substituting LAI by VI that is much easier to derive based on the following relationship:
(6.18)
where = asymptotic value of VI when LAI approaches infinity (e.g., full vegetation cover, fvc = 1.
Practically, this limit is always reached for LAI > 8.0); VIs = vegetation index value of exposed soil
or bare ground (i.e., fvc = 0); = coefficient equivalent to the light extinction coefficient of the canopy
(Gao et al., 2020). Thus:
(6.19)
where Kc = canopy attenuation factor analogous to KVI. Kc/KVI has a fixed value that can be found
from the literature. This equation translates the quantification of relative vegetation abundance to
determine the appropriate NDVI values of full vegetation cover (NDVI∞) and bare soil (NDVIs).
The accuracy of the quantified fvc is jointly affected by vegetation properties and image spatial
resolution. Spatially continuous vegetation cover can be accurately quantified from fine-resolution
images. Conversely, the accuracy drops noticeably if the vegetation is spatially sparse or
discontinuous, such as that found in semi-arid and arid deserts. In this case, fvc tends to be
overestimated as the isolated stands of vegetation likely make up the entire pixel-corresponding area
on the ground. One way of improving the accuracy is to replace NDVI by other indices, such as
MSAVI, VARI, NDVI, RVI, and MTVI2. Another way is to substitute NDVI with red-edge
vegetation indices such as MCARI(705,750) (Gao et al., 2020).
Table 6.5
Major properties of leading FVC data productsProduct
name Sensor
Available
time
Temporal
resolution
Spatial
resolution
Spatial
coverage Reference
GLASS MODIS
2001-
present 8 days 500 m Global
Jia et al.
(2015)
GEOV1
SPOT
VGT
2001-
present 10 days 1 km Global
Baret et al.
(2013)
TRAGL MODIS
2001-
2012 8 days 1 km Global
Xiao et al.
(2016)
Li MODIS
2001-
2012 8 days 0.011°
Northern
China
Jia et al.
(2015)
Source: Yang et al. (2018), open access.
Several FVC products have been produced from various data sources, including Global LAnd
Surface Satellite (GLASS), GEOV1, TRAGL, and Li (just northern China) (Table 6.5). The GLASS
product is produced from MODIS data at a temporal and spatial resolution of eight days and 0.5 km,
respectively, with a sinusoidal projection. The GEOV1 product
(http://land.copernicus.eu/global/products/FCover) is derived from SPOT VEGETATION data using
BP NNs, available from 1999 to present. The product is released in the Plate Carrée projection at
1/112° spatial resolution (about 1 km) at a 10-day interval. The TRAGL product is based on the
physical relations between FVC and LAI retrieved from GLASS product, which, in turn, is derived
from MODIS (version 5) surface reflectance data using GRNNs (Figure 6.5). The Li product is
derived from MODIS Version 5 surface reflectance data (MOD09A1) using unmixing analysis of a
dichotomous pixel composition of vegetation and non-vegetation. Of these products, GEOV1 is
considered the best global FVC product. The TRAGL product is spatially and temporally complete,
and provides slightly more accurate estimates against the mean values obtained from a high￾resolution fvc map (R² = 0.88, RMSE = 0.0865, and bias = 0.0171) than the GEOV1 product (R2 =
0.76, RMSE = 0.1541, and bias = 0.0754) (Xiao et al., 2016).6.3.2
6.3.2.1
FIGURE 6.5 Long-term (2001-2012) spatial patterns of maximum FVC in northern China obtained
from four data products. (a) GLASS; (b) TRAGL; (c) GEOV1; (d) Li. (Yang et al., 2018, open
access.)
Tree and Crown Height
Forest structure may be characterized by relief and horizontal spread at the canopy or stand level. It
can be described by several tree parameters, such as tree height, diameter, biomass, LAI, FVC, and
tree volume. It is crucial to quantify these parameters not only because they are directly related to
forest carbon stock, but also because they can characterize individual trees, groups of trees, and the
entire forest to inform ecological restoration. The quantification of these parameters is ideally
accomplished using both imagery and non-imagery terrestrial LiDAR data, some of which must be
large-scaled to be successful. The retrieval from imagery will be presented first, followed by LiDAR
retrieval in this section.
From Imagery
Tree height is one of the most important tree attributes in forest inventory and the key parameter for
estimating forest AGB. Height can be derived for individual trees (tree height) and the crown of a
forest (crown height) at the plot level from image-derived crown models using LiDAR data. By
default, nadir-viewing optical images are unable to reveal vertical relief on the ground unless they
are stereoscopic. Thus, a pair of consecutive images obtained from the same flight path must be used
to construct a 3D model of the canopy. Even if side-looking radar images are sensitive to surface
relief, a pair of co-registered time-lapsed radar images such as Radarsat and Sentinel-1 is needed to
produce a SAR interferogram, from which tree height is quantified via a CHM (Kumar and Krishna,
2019). The generation of this model from InSAR images involves several steps, including
computation of interferometric baselines from information contained in the data header,interferogram flattening, adaptive filtering, InSAR phase wrapping, coherence generation, baseline
fit, and phase to map, as described in Section 5.4.2.2.
At the plot level, the best data to use are UAV images of a super-fine spatial resolution (e.g., 1 cm).
They are good at delineating tree crown and deriving tree height from different band combinations,
such as multi-band and RGB bands, and even NDVI layers (Hao et al., 2021). The derivation
requires a DSM that can be created from RGB bands and their transformed NDVI using UAV image
mosaics. A DEM of the bare ground is also needed to detect tree height. It can be created from
interpolating the bare ground height of the forest gaps on the DSM. A CHM is produced by
subtracting the DEM from the DSM. It represents the 3D geo-referenced surface of the stand-level
canopy height. Regional tree and crown height estimation models are established using regression
analysis or machine learning methods, such as mask convolutional neural network (Mask R-CNN)
(Hao et al., 2021). They must be trained using the proper training datasets. The properly trained
Mask R-CNN model outputs each tree crown and corresponding height in vector form. Validated
against field measurements, the Mask R-CNN model with the NDVI-CHM combination is the best
performer (R2 = 0.87, rRMSE = 9.67%) in estimating tree height in a young plantation forest (Figure
6.6). In the estimation, drone images allow tree height to be estimated at a rather high accuracy over
R² = 0.82 with the assistance of the CHM. The accuracy is much lower (R² < 0.50) if DSM is used.
Thus, it is important to consider bare ground height to achieve accurate estimates of tree height from
drone photographs.FIGURE 6.6 Chinese fir crown and height in a young plantation forest estimated from a DJI Phantom4-
Multispectral drone image using a mask region-based convolutional neural network (Mask R-CNN)
and an NDVI-CHM merged image. (a), (c), (e), (g) show UAV images from near-infrared, red and
green bands; (b), (d), (f), (h) show the corresponding estimates of tree crown height. (Hao et al., 2021,
with permission (5761191379745) from Elsevier.)
In spite of the high accuracy achieved, this method cannot be applied to dense and mature forests
of no canopy gaps, thus the unavailability of the reference height. Besides, the use of UAV photos
restricts the application of the method to small areas. The high accuracy is achieved only when the
UAV photos have a high geometric accuracy. For instance, during the flight the precision of the6.3.2.2
horizontal position (x,y) and elevation (z) must be controlled within 2 cm and 3 cm, respectively,
using a precise imaging real-time kinematic positioning and navigation system. It must be linked to
a ground mobile GPS station to acquire high-precision UAV waypoints for accurately geo￾referencing the images to an acceptable level. Also, the images must have a fine resolution. The
accuracy of retrieval from coarse resolution (5 or 16 m) Radarsat-1 images is improved from an
MAE and RMSE of 1.48 and 1.53 m, respectively, to 1.3 and 1.34 m with the use of finer resolution
Sentinel-1 data (Kumar and Krishna, 2019).
From LiDAR Data
Since LiDAR data of dense 3D point clouds are randomly distributed over the canopy and on the
ground, they are excellently positioned to quantify the structure of a forest and measure the physical
properties of trees. If the LiDAR scanner is mounted in a platform at a close distance to the target as
in airborne UAV and TLS surveys, it is possible to manually group LiDAR point clouds into single
and clustered trees, from which height-related metrics (mean, standard deviation, and maximum
canopy height) can be produced based on the CHM and plant area index (Choi and Song, 2022). The
principle and general procedure of processing LiDAR data to derive tree height remain roughly
identical in all methods. The exact steps involved in a method may vary slightly with the flight
configuration and the computing system used. Fundamentally, all methods must aim to separate the
point clouds into ground and aboveground points via LiDAR point classification and height
normalization. In order to link in situ measured tree heights in a sampling plot, its boundary must be
overlaid with the normalized point clouds. Then a triangulated irregular network (TIN) is generated
iteratively at a user-specified step size to gradually include all the ground points. These TINs
delineate the ground surface, so the points excluded from the ground surface are regarded as the
aboveground ones, from which vegetation structural features are extracted. The aboveground
LiDAR data points within each plot are labeled and their height is analyzed statistically. This height
represents the normalized distance from the ground surface to the highest point of the canopy. The
aboveground points may also be used to generate a DSM for classifying the trees later for estimating
their AGB (see Section 6.6.3.2).
The core of LiDAR-based estimation of forest structural parameters is the creation of the CHM in
four steps: (i) creation of a bare ground DEM using only ground LiDAR points; (ii) thinning of the
normalized LiDAR dataset to a size about half of the resolution of the final CHM by extracting the
highest point in each grid cell; (iii) creation of a series of DEMs using a subset of the thinned
LiDAR points. A DEM is generated using only a portion of the total points that meet the specified
height threshold, such as 2, 5, and 10 for low (0.5 m < height ≤ 2.0 m), medium (2.0 m < height ≤5.0
m), and high (height > 5.0 m) vegetation, respectively. These thresholds vary with the vegetation
type in a study area, and have to be experimentally determined to preserve the original shape of tree
crowns through stepwise increments to a maximum of 50 at an interval of a few meters; and (iv)
merging of all the DEMs to form a single CHM. Initially, the LiDAR-derived CHM may suffer from
pits and holes that can be removed through window-based filtering. The pit-free CHM is then
segmented to delineate individual trees and tree crowns. The segmentation may start from a broad
scale to form coarse objects based on the nature of tree crowns in the sampling plots as observed in
the field. The formed crowns of trees are measured by their height (Figure 6.7).FIGURE 6.7 Procedure and main steps of retrieving tree height and AGB from ALS and TLS data.
(Modified from Ojoatre et al., 2019.)
The CHM used to measure canopy height must be constructed from the first LiDAR returns. At the
stand level, LiDAR data are preferably obtained at a low flight height close to the target, as with
drone-borne scanning. Airborne LiDAR is more suited to estimate tree parameters at the plot and
canopy levels. Drone-acquired LiDAR data allow more parameters to be extracted than drone-borne
images, such as LAI, FVC, AGB, timber volume, and stem diameter, in addition to tree height and
DBH (e.g., overstory and understory stand density) (Table 6.6). The retrieval of some of theseparameters to an acceptable accuracy level is possible only with small footprint LiDAR data at a
density of up to 20 points ∙ m-2.
Table 6.6
Main forest structural parameters that have been estimated from LiDAR data and their accuracy
Forest type/
country LiDAR data* Methods
Structural
parameter Accuracy Reference
Scots pine,
Sweden Waveform Regression
Stand
volume
Mean tree
H
R² = 0.78
2.1-3.7
UE**
Nilsson
(1996)
Spruce,
pine and
mixed,
Norway
0.3 points ×
m–2 Weighted
Mean
laser Ht
Weighted
4.1-5.5 m
UE
2.1-3.6 m
UE
Næsset
(1997)
Douglas fir
Canada
0.2 points ×
m-–2
Quantile-based
grid
Mean tree
height
R² = 0.62
(Mean laser
H), 0.65
(Max laser
H), 0.70
(bias ), 0.49
(Loreys H)
Magnussen
&
Boudewyn
(1998)
Norway
spruce &
Scots pine,
Finland
±20 points ×
m–2
Segmentation
(CHM)
Standwise
volume
Basal area
SE = 35.8
(m3 × ha–1)
SE = 3.4
(m2 × ha–1)
SE = 1.7 (m)
Hyyppä et
al. (2001)
Mixed
1 point × m–2
Multispectral
imagery
Segmentation,
regression
modeling
Crown
area
R2 = 0.46
(LiDAR)
R2 = 0.26
(imagery)
Coops et al.
(2004)
Scots pine
forest
0.57 pts × m–
2 (cross),
±9 pts × m–2
(along) LiDAR metrics
Height
Crown
bulk
density
Crown
volume
Foliage
biomass
R² = 0.93
R² = 0.80
R² = 0.92
R² = 0.84
Riaño et al.
(2004)
Ponderosa
pine
1.23 pt × m–2 Regression of
LiDAR metric
Height
AGB
Basal area
R² = 0.87,
SE = 0.69
Hall et al.
(2005)Forest type/
country LiDAR data* Methods
Structural
parameter Accuracy Reference
against field
data
Tree
density
R² = 0.74,
SE = 0.2
R² = 0.79,
SE = 0.19
R² = 0.67,
SE = 0.36
Loblolly
pine
plantation,
USA
Density not
given
Optimization +
regression
Forest
biomass
R = 0.59-
0.82
RMSE =
13.6-140 t ×
ha-–1
Bortolot &
Wynne
(2005)
Sugi
plantation,
Japan
4.76 pts × m–
2
Segmentation +
multiple
regression
Stem
volume
Adjusted R²
= 0.765
SE = 23.6%
Takahashi
et al. (2005)
Mixed
wood
boreal
4 pts × m–2;
0.035 pts ×
m–2 (large
footprint)
Regression
modeling
Mean
dominant
H
Basal area
Crown
closure
Biomass
R² = 0.90
R² = 0.91
R² = 0.60
R² = 0.92
Thomas et
al. (2006)
Source: Modified from Roberts et al. (2007), © Taylor & Francis.
Notes:
* Small footprint unless stated otherwise.
** UE = under-estimation.
The accuracy of LiDAR-retrieved tree height is assessed through validation against field
measurements obtained using hypsometers at the stand level initially and subsequently amalgamated
to the plot-level. The accuracy is examined by plotting the two sets of height in a scatterplot (Figure
6.8). Stand-level accuracy is always lower than that of the plot level (Table 6.7). Airborne LiDAR￾derived height tends to be higher (overestimated) than hypsometer-measured height at a RMSE of
3.11 m that decreases to 1.61 m with TLS height (Ojoatre et al., 2019). The discrepancy between the
two varies with the height metrics and vegetation type, such as the maximum height of point clouds
(ZMAX), maximum and mean canopy heights, and height at certain percentiles (e.g., 95%). The
biases from the ZMAX- and CHM-derived metrics do not bear strong linear relationships with tree
heights and canopy complexities with a Pearson correlation coefficient < |0.29| (Choi and Song,
2022). The 95th percentile height and mean height bear a close linear relationship with canopy
height and complexity. At the plot level, maximum canopy height is estimated at an R2 value of 0.86
for trees, but only 0.23 for shrub plots (Ojoate et al., 2019; Wang et al., 2020). The accuracy lies
between these two extremes at the stand level from airborne LiDAR data (R2 = 0.61). The major
sources of errors are identified as LiDAR data inaccuracy and the incompetence of LiDAR to
delineate individual tree crowns and thus to ascertain their height.FIGURE 6.8 Comparison of LiDAR-derived and in situ measured plot-level maximum canopy height
for trees (a) and shrubs (b). (Modified from Wang and Gao, 2019, with permission (240409-014020)
from Elsevier.)
Table 6.7
Overview of LiDAR-derived tree height definition, estimates and their accuracy at either the stand or plot level
Species/location
Lorey’s 
*
BAW**
Accuracy Reference
Norway spruce × ×
R² = 0.91/0.75 δ
= 1.49/3.15 m
Næsset &
Økland (2002)6.3.3
Species/location
Lorey’s 
*
BAW**
Accuracy Reference
W. Washington
State, US ×
R² = 0.98, ∆ =
1.5 m
Andersen et al.
(2005)
Douglas fir × ×
R² = 0.85/0.82
SE = 1.8/2.2 m
Coops et al.
(2007)
Norway spruce,
Scots pine, birch ×
R² = 0.90;
RMSE = 1.45-
1.56 m
Holmgren et
al. (2003)
Mountain pine,
stone pine ×
R² = 0.92;
RMSE = 0.6 m
Morsdorf et al.
(2008)
Queensland,
Australia × R² = 0.81
Lee & Lucas
(2007)
Dominating
Norway spruce × R² = 0.81-0.93 Næsset (2009)
Central W. Poland ×
R² = 0.8; δ = 1.8
m
Wezyk et al.
(2008)
Bayvarian Forest
NP, Germany × R² = 0.97-0.98
Heurich
(2008)
Source: Modified from van Leeuwen and Nieuwenhuis (2010), with permission (5761200596984) from Springer).
Notes:
* Subscript: p-plot level; s-(individual) tree stand.
** Basal area weighted.
DBH
Diameter at breast height (DBH) is a tree biophysical parameter measured off tree trunks at 1.1–1.4
m above the ground in the field. In case of branches, all those with a diameter above the pre-defined
threshold (e.g., > 8 cm) are measured using a tape, and then weight-summed to derive one single
value for one tree stand using the following equation:
(6.20)
where DBHi = DBH of the ith branch of the same tree; n = total number of branches. DBH value
varies with tree species and wood density, as well as tree age.
Such field measurements are laborious and slow, so DBH is commonly estimated from LiDAR
data obtained at a close range to the target so that individual stands are discernible as in UAV-borne
LiDAR data. Stand-level DBH cannot be retrieved directly from LiDAR data, though. Instead, its
extraction must rely on the point cloud slice at around 1.3 m. DBH is quantified based on the height￾diameter relationship for missing values. The general procedure comprises six steps (Swayze et al.,
2021): (i) tree location and height extraction from the CHM using the method described in Section
6.2.2; (ii) extraction of DBH values from the point cloud slice using least square circle fitting either
automatically or manually; (iii) matching of the extracted DBH with the extracted tree location; (iv)construction of the regional DBH model; (v) filtering of the matched DBHs with the regional model
at a high (e.g., 90th) percentile; and (vi) prediction of missing DBH values based on the created
DBH to height model (Figure 6.9).
FIGURE 6.9 Procedure of extracting DBH values from drone-acquired LiDAR data and assessing the
accuracy of the extracted tree- and stand-level characteristics. (Swayze et al., 2021, with permission
(5761200900212) from Elsevier.)
The reliability and detail of the retrieved DBH are affected by flight configuration. Both the quality
(mean error = 0.79 cm) and quantity (~10% of all trees) of the extracted DBH values are maximized
from drone data of a lower altitude, nadir crosshatch acquisitions (Swayze et al., 2021). Nadir (angle
= 0) crosshatch flight configuration maximizes the tree extraction accuracy and correctness (F-score
= 0.77). Off-nadir or crosshatch flight configuration at lower altitudes maximizes correlation (r >
0.70) and accuracy (basal area < ± 2 m2 ∙ ha−1) of stand density estimates, and UAV SfM individual
tree detection rates, and minimizes extracted tree height and DBH errors (Figure 6.10). The
combination of crosshatch with off-nadir camera angles improves UAV-based estimation of
individual tree DBH.FIGURE 6.10 Regressional relationship between observed and UAV-estimated DBH at two angles (red:
65 m crosshatch; blue: 65 m serpentine of LiDAR scanning). (Swayze et al., 2021, with permission
(5761200900212) from Elsevier.)
LiDAR-based estimation of tree DBH can also be fulfilled using terrestrial scanning accurately. It
is readily accomplished using the portable and affordable iPad LiDAR sensor. The method of TSL
retrieval of DBH remains unchanged from drone scanning except that tree trunks show up as
approximately semi-circular curves in TLS data. So DBH is estimated via manually fitting circular
features around them. The reported accuracy of DBH estimation varies widely, from an R2 of 0.52 in
comparison with field measurements (RMSE: 2.82–8.24 cm) (Wang et al., 2022) to 0.96. The6.3.4
estimation accuracy does not seem to vary with the distance from the scanning location to trees. TLS
achieves an R² of 0.96 in estimating DBH, noticeably higher than that from the optimal
configuration of UAV data (Ojoatre et al., 2019). The iPad LiDAR scanner can be potentially used
for DBH estimation and may replace tedious and time-consuming measurements of tree DBH in the
field for verifying the results from UAV and other remote sensing data. But TLS cannot perform the
estimation over a large area or even at the plot level to produce a field view of DBH.
Wood Volume
Wood volume is a physical parameter of trees that can be quantified from aerial photographs,
satellite images, and LiDAR data using aerial photogrammetry and satellite radargrammetry, all
being rather complex methodologically. In order to identify the smallest tree possible, stereoscopic
digital images may be taken with a Vexcel UltracamX camera with an endlap of 60% and a sidelap
of 20% to obtain a 3D view of the target. The airborne images may be PAN-sharpened to a ground
sampling distance of 20 cm and used to create a DSM using the photogrammetric method (Rahlf et
al., 2014). A CHM is produced from the subtraction of a fine-resolution DEM obtained from ALS
data. If radar imagery is used, it can be processed using two methods: radargrammetry and InSAR.
The former follows the same principle and method as photogrammetry, even though space-borne
radar images have a coarser resolution than aerial photos. The finest TerraSAR-X images have a
resolution of 1 m, five times that of air photo’s 0.2 m. A DSM may be produced from a pair of radar
images via phase unwrapping to convert the unwrapped phases to geocoded DSMs, with the phase
differences representing vegetation height. A CHM is produced from the differential interferograms￾created DSM. In order to produce quality estimates, random errors inherent in noisy radar images
must be removed via adaptive filtering, together with offset and ramp errors using GCPs.
Wood volume is a 3D parameter derivable by multiplying basal area by tree height. Although
wood volume can be calculated at the stand level, it is unpractical to do so for the entire forest.
Instead, its quantification in a forest has to rely on upscaling from the plot level. If derived from
satellite imagery, wood volume (m3 ∙ ha–1) is commonly estimated from spectral bands via linear
and non-linear regression analysis, non-parametric line-fitting, and K-nearest-neighbor classification
(Trotter et al., 1997). Due to significant errors of unknown magnitude in both the independent and
dependent variables, ordinary regression cannot generate any meaningful models. So, the sample
points have to be ordered by the ascending value in the independent variable and divided equally
into three groups. The two groups having the maximum () and the minimum () arithmetic mean
heights of the independent variable are used to estimate the slope of the fitted line, from which wood
volume is estimated as:
(6.21)
where Vj,p (j = 1, 2, ..., k) = value of variable V for the k nearest pixels to pixel p in the spectral
space; wi,p = weight calculated from distance (d) of the k nearest pixels to p as:
(6.22)The above equations are applicable to individual pixels in an image. Pixel-level wood volume is
subsequently upscaled to the forest-stand level by averaging the pixel-scale estimates over an area.
The accuracy of the estimated wood volume from coarse resolution images (e.g., 30 m Landsat data)
at the pixel scale is rather low due to the weak relationship (R2 = ⁓0.3) between Landsat data (and
their indices) and the measured wood volume of a coniferous plantation forest. The resultant
estimate has a large RMSE>100 m3 ∙ ha-1 because the coarse pixel size causes many different kinds
of plants to be lumped into one pixel (Trotter et al., 1997). The accuracy reaches an acceptable level
only after the pixel-scale estimates are averaged over a forest-stand area of about 40 ha (450 pixels)
(Figure 6.11). The N-dimensional k-nearest-neighbor classification at the pixel scale produces
similarly accurate estimates to those obtained via regression when k is large (15). Thus, fine￾resolution imagery is vital to obtaining acceptable estimates of wood volume.
FIGURE 6.11 Deviation of predicted wood volume from the observed values at the forest-stand scale.
Unless otherwise indicated in the key, the dataset from which the model is established is at the pixel
scale. (Adapted from Trotter et al., 1997.)
Even fine-resolution imagery is severely handicapped in estimating wood volume because
imagery data can only capture canopy spectral properties without the ability to reveal its vertical
structure. This deficiency can be effectively overcome using ALS data. The philosophy of LiDAR￾based estimation deviates drastically from aerial photos or images because LiDAR data do not allow
basal area to be estimated. So the estimation has to rely on tree stand metrics derived from large-scale LiDAR data (e.g., acquired a few hundreds of meters above ground) with a small footprint
diameter of just over 10 cm at a mean point density of ~10 pulses ∙ m-2. Such LiDAR data allow the
creation of 1 m DEMs. The derived canopy metrics include plant height and density from which
wood volume is estimated based on empirical linear mixed effect models (Rahlf et al., 2014). Other
commonly considered independent variables in the regression model are height (percentile),
maximum, mean, coefficient of variation, and canopy density metrics created by dividing the height
range by 10, the proportion of echoes above the vertical parts. These metrics can be computed a few
times from different LiDAR returns: all returns, first and single returns, and last returns. If
empirically modeled, wood volume is expressed as the linear summation of all the considered
variables as:
(6.23)
where (i = 1, 2, …, m; j = 1, 2, …, n) = observed timber volume of the ith stand (m = total number
of stands) in the jth sample plot (n = total number of sample plots); = the hth input variable derived
from the LiDAR metrics, = coefficient or weight of the hth input variable; bi = stand-level random
effects with a variance of , independent of the plot-level residuals εij. The heteroscedasticity in the
residual errors is modeled from the variance function (δ = the variance parameter), with the LiDAR
metric hmean as the variance covariate. The input variables to be included in Eq. 6.23 are finalized
using a stepwise forward variable selection algorithm that minimizes the Bayesian information
criterion. The importance of the included variables is determined via backward elimination based on
the cross-validated plot-level RMSE. If the removal of a variable from the model reduces its RMSE,
then it will be omitted from the final model (Rahlf et al., 2014).
Table 6.8
Comparison of leave-one-stand-out cross-validation RMSE in estimating timber volume from four types of remotely sensed
data at the plot and stand levels
Remote sensing dataset
Plot-level RMSE Stand-level RMSE
m3 × ha–1 % m3 × ha-–1 %
Airborne laser scanning 36.20 19.42 23.05 12.36
Aerial photogrammetry 58.59 31.43 33.79 18.12
Satellite InSAR 77.56 41.60 33.74 18.10
Satellite Radargrammetry 82.82 44.42 43.38 23.27
Source: Rahlf et al. (2014), with permission (5761210690354) from Elsevier.
The performance of four types of data (methods) in retrieving timber volume is compared in Table
6.8. It shows that the estimation accuracy is affected by the data used. At the plot level, ALS yields
the most accurate estimates with the lowest RMSE of 19%, followed by aerial photogrammetry
(31%) (Rahlf et al., 2014). InSAR (42%) and radargrammetry (44%) are noticeably less accurate
with doubling RMSE values. This relative order of accuracy remains unchanged at the stand level,
but RMSE values of all data/methods are markedly (12–23%) lower. Consideration of topography
(e.g., terrain slope and aspect) in the models improves the accuracy of all methods but only slightly
with airborne LiDAR. The estimates from airborne data are much superior to those from satellite6.4
images because of their coarse spatial resolution that is unable to capture the local variation in tree
physical size, and their results show little detail about the spatial variation of wood volume (Figure
6.12c).
FIGURE 6.12 Comparison of timber volume (m3 × ha–1) distribution produced from four types of
remote sensing datasets. (a) ALS: 1 m × 1 m resolution over which height values are averaged; (b)
Aerial photo of 0.2 m × 0.2 m resolution; (c) Space-borne InSAR of 10 m × 10 m resolution; (d)
TerraSAR-X of a resolution up to 1 m. (Rahlf et al., 2014, with permission (5761210690354) from
Elsevier.)
LEAF AREA INDEX6.4.1
LAI is a biophysical variable indispensable to modeling the exchange of fluxes of energy,
biogeochemistry, mass of water and CO2, and momentum between the biosphere and the
atmosphere. This parameter serves as a vital input to climate models for weather prediction. As an
overall indicator of canopy size, tree density, and leave orientation in relation to each other and to
the incoming radiation, LAI is a reliable measure of plant growth and a critical variable in several
processes, such as photosynthesis, respiration, and precipitation interception. This fundamental
biophysical parameter is commonly quantified from remotely sensed data very fast and the
quantified results are consistent and spatially uniform across the entire ecosystem around the world.
LAI has been estimated from remote sensing data, via VIs and other metrics, using four broad
categories of methods: physical model inversion, empirical relationships with ground observations,
non-parametric methods, and machine learning models, such as ANN and SVM. They are all
explained in this section below.
Model-based Methods
At the canopy-level, physical models such as SAILH, INFORM, and PROSAIL are radiometric
data-driven. SAILH is widely used to simulate the bidirectional reflectance of turbid medium plant
canopies, due to its ease of use, general robustness, and consistent performance. It requires inputs of
three geometric parameters (solar zenith angle, viewing zenith angle, and relative azimuth angle),
spectral bands of the image used, plus average leaf angle and hot spot parameter. In estimating LAI,
INFORM considers the 1D turbid medium radiative transfer within the crown and the 3D
characteristics such as leaf clumping and crown shadow. It represents a compromise between the
feasibility of model inversion and exhaustive characterization of the canopy structure (Schlerf and
Atzberger, 2006). INFORM has a demonstrated capacity and suitability in retrieving forest structural
and biophysical variables such as LAI in both broadleaf and conifer stands with varying levels of
success. The PROSAIL RTM estimates LAI based on physical laws governing the transfer and
interaction of radiative energy within the canopy, such as the PROSPECT and LIBERTY RTMs at
the leaf level.
In general, all physical models for LAI inversion are overly complex involving numerous
parameters that require sensitivity analysis to specify a reasonable range. The closer the range to
reality, the higher the inversion precision. Even the PROSAIL RTM that is relatively easy to use
requires 10 inputs. It can be implemented either as LUT inversion or PROSAIL inversion using
predictive polynomial equations established via regression of LAI against in situ collected data. The
inversion of the PROSAIL RTM based on 107 hyperspectral bands achieves an accuracy of nRMSE
= 0.18 (R2 = 0.91) in estimating Mediterranean grassland LAI in comparison with independent data
(Atzberger et al., 2015). This accuracy is comparable to that of statistical approaches based on VIs
(Table 6.9). If applied to a satellite image, such as HyMap, the model enables the spatial distribution
of LAI to be quantified on a micro-scale (Figure 6.13).FIGURE 6.13 Distribution of micro-scale LAI in part of the Majella National Park of Italy covered by
the HyMap data, quantified using LUT-based model inversion. (Atzberger et al., 2015, open access.)
Table 6.9
Comparison of accuracy in retrieving LAI (unit: m2 × m–2) from airborne HyMap spectra using four methods (n = 41)
Method Spectral setting Validation R2 RMSE nRMSE
PROSAIL
inversion
Simultaneous use of 107
bands for LUT search
(multiple LUT solutions)
Independent
data 0.91 0.53 0.186.4.2
Method Spectral setting Validation R2 RMSE nRMSE
PEphysical
Best 2-band VI within 126
× 126 cases from synthetic
data basea
Independent
data 0.79 1.10 0.38
PEre-adjust
Best 2-band combination
from PEphysical re-adjusted
to experimental dataa
Cross￾validation
0.75
(0.70-
0.86)
0.46
(0.53-
0.79)
0.16
(0.21-
0.39)
VI
Best 2-band VI within 126
× 126 cases from
experimental data baseb
Cross￾validationc
0.86
(0.75-
0.92)
0.59
(0.50-
0.76)
0.21
(0.17-
26)
Source: Atzberger et al. (2015), open access.
Notes:
a From the synthetic data, the best index form was found to be the D-index with λ1 = 846 nm and
λ2 = 1698 nm (minimum RMSE on simulated dataset: 1.08);
b From the experimental data base, it was found that the R-index performed best with λ1 = 543
nm and λ2 = 1953 nm;
c For the two experimental methods (PEre-adjust and VI) two validation results are provided: cross￾validated results and (in parentheses) the range (5% and 95% percentiles) from the repetitions of the
jackknife sampling with 20 samples. For the jackknife simulation, the same spectral bands
previously identified from the entire data set at 543 and 1953 nm were always used.
The inversion of physical-based canopy RTMs with actual (full-spectrum) remote sensing data may
be physically sound, but this approach faces the ill-posed problem of intrinsically undetermined
parameter values. This challenging task may be mitigated using several strategies, such as the
widely used LUT-based inversion. They require simulations of the spectral reflectance for a large yet
limited range of RTM variable values. The inversion problem is thereby transformed to the
identification of a modeled reflectance set closely resembling a measured one in the LUT. A LUT
query is typically performed by applying a cost function that generates a value for one or multiple
RTM input variable set by minimizing the summed differences between the simulated and measured
reflectance for all wavelengths. The robustness of the LUT-based inversion routines has been
optimized using various regularization strategies, such as the use of a priori information.
LUT-based RTM inversion proves challenging as an adequate RTM has to be chosen and a LUT
created by parameterizing the input variables and setting the boundary conditions. After the
generation of a LUT, a multitude of cost functions and regularization options have to be evaluated.
Despite the optimization of the inversion process, the best performances are not better than an R2 of
0.74 (RMSE = 0.80) based on the best performing cost function (Pearson’s R²) against the validation
dataset (Schlerf and Atzberger, 2006).
Parametric Estimation Models
Parametric methods assume an explicit relationship between LAI and spectral measurements. Thus,
explicitly parameterized expressions are defined by variables selected either statistically or based on
the physical knowledge about the spectral response they induce. Typically, a band arithmetic6.4.2.1
formulation is defined as a vegetation index and subsequently linked to LAI. The estimation models
are empirically established via regressing the field-observed LAI against image-derived VIs and
vegetation biophysical parameters from LiDAR data. The method of data processing and accuracy
of the retrieved LAI vary widely between images and LiDAR data, so they are discussed separately
below.
Image-derived Predictor Variables
Imagery data are suitable for deriving VI, from which LAI is estimated indirectly. In predicting LAI,
VIs derived from multiple bands are preferable to single bands as they make use of the rich spectral
information of the canopy. Various VIs (Table 6.10) have been proposed to estimate LAI, including
NDVI from NIR and red bands, and simple ratio (SR). Their utility in quantifying LAI varies with
vegetation properties. NIR bands are especially useful in densely vegetated areas where VI likely
faces the saturation problem, including NDVI, which can be circumvented using an alternative two￾narrow band red-edge difference VI called DVIn-v proposed by Delegido et al. (2013), calculated as:
(6.24)
where Rv and Rn = reflectance value in the visible and NIR spectral range, such as the two red edge
bands at 674 and 712 nm available in hyperspectral CHRIS imagery. These wavelengths correspond
to the maximal chlorophyll absorption and the red-edge position. This simple but robust red-edge
spectral index is related closely to the measured LAI values at a high accuracy (R² = 0.717), and
opens up the opportunities for estimating LAI from space-borne data.
Table 6.10
Vegetation indices used to predict green LAI over agroecosystems and their accuracy (p < 0.0001 for all indices)
Abbreviation Formula Reference R²
NDVI Rouse et al. (1974)
MCARI Daughtry et al. (2000) 0.614
TCARI Haboudane et al. (2002) 0.748
MTCI Dash and Curran (2004) 0.03
TCI Haboudane et al. (2008) 0.705
NIR-red index Gitelson et al. (2005) 0.403
TVI Broge and Leblanc (2000) 0.751
OSAVI Rondeaux et al. (1996) 0.688
PRI Gamon et al. (1992) 0.082
SR Jordan (1969) 0.412
SR705 Gitelson and Merzlyak (1994) 0.438
MCARI/SAVI Delegido et al. (2013) 0.582
TCARI/SAVI Delegido et al. (2013) 0.482
MTVI 1 Haboudane et al. (2004)
MTVI 2 Haboudane et al. (2004)Abbreviation Formula Reference R²
sLAIDI* Li and Guo (2011)
REIPlinear Darvishzadeh et al. (2019)
Source: Modified from Delegido et al. (2013), with permission (5761211121266) from Elsevier.
Other indices include SR calculated from 670 and 800 nm from quality of light on the forest floor
(Jordan, 1969), modified triangular vegetation index 1 (MTVI1) and 2 (MTVI2), transformed
triangular vegetation index (TTVI), standardized LAI-determining index (sLAIDI*), and the linear
interpolation of red-edge inflection point (REIPlinear). TTVI is calculated as:
(6.25)
Formulas for calculating other indices are provided in Table 6.11. It also lists more indices created
by dividing one index by another, such as MCARI divided by SAVI, but their utility in LAI
estimation has not been assessed comparatively.
Different indices have different utilities in estimating LAI individually. As illustrated in Table
6.11, TCARI (R2 = 0.748) and TVI (R2 = 0.751) are the two top performers, more accurate than
other indices or index ratios, such as MCARI/SAVI that achieves an R² of only 0.582. An evaluation
of more than ten thousand hyperspectral indices reveals that the best performing index is an
optimized three-band index in the form of (Verrelst et al., 2015b). It has a cross-validation R2 of
0.823 (RMSE = 0.62). The other two-band indices have an accuracy close to each other, but is
slightly less powerful. The worst performer is the single band centered at 865 nm (Table 6.11).
Table 6.11
Cross-validation accuracy of best performing indices (band) in estimating LAI of diverse crops from Sentinel-2 bands
Abbreviation Calculation Formula R² RMSE
3BSI Tian 0.823 0.615
mND 0.792 0.671
mSR 0.787 0.686
3BSI 0.776 0.691
SR 0.766 0.725
DVI 0.740 0.748
2BSI 0.739 0.777
3BSI Wang 0.730 0.770
R 0.618 0.923
Source: Modified from Verrelst et al. (2015b), with permission (5761211503019) from Elsevier.
If estimated from two HyMap wavebands at 837 nm and 1148 nm, the retrieved LAI map has an R2
of 0.73 and an RMSE of 0.58 (rRMSE = 18%) (Schlerf and Atzberger, 2006). Low LAI values are
slightly overestimated whereas relatively large LAI values are somewhat underestimated by all 2-
band indices. After HyMap data are resampled to Landsat TM spectral bands, the two “optimum”
bands at 840 nm and 1650 nm uplift the RMSE of estimation marginally to 0.66 and rRMSE to 21%.
Thus, broad NIR and MIR wavebands are almost equally well suited to estimate LAI as narrowband6.4.2.2
data. It remains unknown whether higher accuracy is possible with the use of more wavebands in the
inversion.
Table 6.12
Performance of six common VIs in estimating corn LAI from Sentinel-2 data
VI Model
Model accuracy
R²* RMSE nRMSE (%)
EVI 8.48X-1.64 0.63 0.84 25.80
EVI2 8.51X1.61 0.63 0.83 25.50
MSR 1.26e0.36X 0.68 0.78 24.00
NDVI 9.72X-4.68 0.62 0.84 25.80
OSAVI 0.21e4.68X 0.69 0.76 23.30
RVI 0.18X+0.89 0.69 0.76 23.30
Source: Ma et al. (2022), open access.
Note: *: significant at the 0.01 level.
Estimation Models
The estimation of LAI is grounded on its relationship with VI expressed as a modified Beer’s law
equation (Eq. 6.18). This relationship between and depends on irradiance, viewing geometry, and
leaf optical properties (e.g., leaf angle distribution). However, it is very difficult to estimate LAI via
VI measurements when VI approaches . The model in Eq. 6.18 can be embodied as linear,
polynomial, exponential, or logarithmic with canopy reflectance or VI as the sole explanatory
variable. The empirical LAI-VI relationship is mostly linear for broadleaf vegetation (Eq. 6.26), up
to a certain saturation point where the relationship approaches asymptote. Logarithmic models are
superior to linear models (Prathumchai et al, 2018), but not always. As illustrated in Table 6.12,
there is no apparent difference between linear and non-linear models in their accuracy. The simplest
linear LAI estimation model can be univariate in the form of:
(6.26)
where VI can be any indices in Table 6.11. Even DVIn-v of two bands can achieve a high prediction
accuracy in estimating crop LAI (Eq. 6.27) (Delegido et al., 2013):
(6.27)
This accuracy improves to R² = 0.824 after the removal of 12 samples representing senescent wheat
plants with dry and yellowing leaves. This accuracy is higher than R2 = 0.68 achieved by NDVI.
Validation of the red-edge DVI-derived LAI against field measurements also reveals a close linear
relationship between CHRIS bands-derived DVI and LAI with a slight change in the prediction
model:
(6.28)where the wavelengths have shifted from those in Eq. 6.27 because over time CHRIS bands have
suffered a small spectral displacement. This accuracy even surpasses that achieved using the
INFORM-based model inversion from Sentinel-2 red edge bands (R2 = 0.79, RMSE = 0.47,
NRMSE = 13%) in retrieving forest LAI (Brown et al., 2019). The application of the best model to a
CHRIS image produces a LAI distribution map (Figure 6.14).
FIGURE 6.14 LAI map illustrating the value of different types of crops, generated using the simple DVI
index from two wavelengths of a 19 June 2009 CHRIS image. (Delegido et al., 2013, with permission
(5761211121266) from Elsevier.)
In areas frequently obstructed by clouds, crop LAI is best estimated from L-band SAR data. This
single band does not allow VI calculation, so the estimation models are constructed from calibrated
HH-, VV-, and HV-polarized layers. The estimation accuracy from radar data varies with both the
radar band and vegetation type. C-band RadarSat-2 data achieve the lowest errors in estimating both6.4.3
corn and soybean LAI using linear cross polarization (HV) models (Hosseini et al., 2015). The
estimated and measured LAIs have a rather high R² value ranging from 0.80 (HH–HV and VV–HV)
for soybeans to 0.83 (HH–HV) for corns. The RMSE (MAE) of corn LAI ranges from 0.84 m2 ∙ m–2
(0.65 m2 ∙ m–2) to 0.75 m2 ∙ m–2 (0.62 m2 ∙ m–2) for HH–HV and VV–VH inputs, respectively. The
estimated soybean LAI from the like-cross polarization combinations has a RMSE of 0.64 m2 × m–2
from HH–HV data and 0.63 m2 ∙ m–2 from VV–HV data. The most accurate results for corn LAI are
obtained from HH–HV backscatter (RMSE = 0.91 m2 ∙ m–2; MAE = 0.64 m2 ∙ m–2). Although L￾band is able to estimate corn LAI with relatively low errors, its low frequency does not allow
soybean LAI to be adequately estimated because long L-band microwaves penetrate further into
canopies. This may be advantageous for larger canopies, but can also detect more direct interactions
between soil and the canopy when biomass is low, which introduces a source of noise. If the crop
has a low biomass, it is better to use C-band radar data to estimate its LAI. Both C-band (corn and
soybeans) and L-band (corn only) results have comparable errors to those achieved using optical
satellite data, but radar has the ability to function under all weather and solar illumination
conditions.
The retrieval of LAI from LiDAR data is based on the canopy gap fraction and involves the use
of DBH, calculated as:
(6.29)
where a and b = regression coefficients and offset derived from field-measured LAI and DBH for
various species, canopy height, and crop management. Since LiDAR-retrieved DBH faces a high
uncertainty (see Section 6.2.3), it is not recommended to retrieve LAI of wetland vegetation using
LiDAR data. This is because the relationship between wetland LAI and LiDAR-derived metrics
(e.g., H_p99, C_ppr, A_std (Standard deviation of signal strength within the search radius),
vegetation openness, ratio of the number of ground points to the total number of points within the
search radius) is sensitive to variations in ALS characteristics. So LiDAR data are ineffective at
estimating LAI of wetland vegetation, achieving a low R2 of only 0.08–0.30 (Koma et al., 2021).
This low estimation accuracy can be overcome using imagery data.
Non-parametric Regression Methods
Imagery data, either individual bands or their transformations, are useful to estimate LAI using non￾parametric regression methods without any assumption about data distribution or variable
interrelations. These methods are able to take into account the full spectral bands available. In
contrast with parametric regression methods, these non-parametric regression methods do not
require specification of their explicit relationships, transformation(s) or fitting functions. A notable
family of non-parametric regression methods is machine learning regression algorithms that have
been widely used to quantify LAI from VIs. They include regression trees, ANN, Gaussian process
regression (GPR), KRR, and ELM (Table 6.13).
Table 6.13
Comparison of cross-validation accuracy of representative machine learning algorithms in estimating crop LAI from
Sentineal-2 images6.4.4
Method Core algorithm R² RMSE
VH-Gaussian processes regression (VH-GPR)
Bayesian statistical
inference 0.902 0.436
Gaussian processes regression (GPR)
Bayesian statistical
inference 0.900 0.440
Kernel ridge regression (KRR) Matrix inversion 0.897 0.453
Bagging trees (BaT)
Bootstrap aggregation
(bagging) + RT 0.887 0.472
Relevance vector machine (RVM)
Bayesian statistical
inference 0.886 0.458
Extreme learning machine (ELM)
Pseudo matrix
inversion 0.879 0.506
Neural Network (NN)
Levenberg–Marquardt
algorithm 0.849 0.561
Partial least squares regression (PLSR) Matrix inversion 0.827 0.584
Boosting trees (BoT)
Least squares
boosting + RT 0.826 0.619
Regression tree (RT) Sorting & grouping 0.825 0.601
Principal component regression (PCR) Matrix inversion 0.686 0.803
Source: Modified from Verrelst et al. (2015b), with permission (5761220384382) from Elsevier.
The key to the success of non-parametric regression methods for accurate LAI estimation is to select
an adequate sample distribution to train the network using a wide range of samples representing
broad types of land cover. Besides, sufficient testing samples should be set aside for model
validation. The performance of 11 algorithms in the family of kernel machine learning regression
has been evaluated using Sentinel-2 derived VIs (Table 6.13) in comparison with physical model￾inverted results (Verrelst et al., 2015b). In general, all kernel-based machine learning regression
algorithms are highly accurate, with variational heteroscedastic (VH) GPR being the top performer
(R2 = 0.90, and RMSE = 0.44). It can yield both mean estimates and associated uncertainty
intervals. KRR has a very high cross-validation accuracy of R2 = 0.90 and RMSE = 0.45. In
contrast, the widely used PLSR and NN are much less optimal, and NN models require excessive
time to develop. The added strength of machine learning algorithms is their ability to evaluate the
predictive power of each single band for LAI estimation. As such, the optimal number of bands can
be assessed, for example, by using a backward elimination approach (Verrelst et al., 2012).
Of the three families of retrieval methods, non-parametric methods, especially kernel machine
learning regression algorithms, are the most accurate, and parametric methods are the fastest. The
family of kernel-based machine learning regression algorithms (e.g. GPR) are the most promising in
accurately characterizing vegetation LAI (Verrelst et al., 2015b).
A Comparison
The pros and cons of four types of analytical methods in estimating LAI are compared in Table 6.14.
Physical-based models are accurate for the full LAI range without calibration from one site toanother irrespective of the remote sensing data used and their acquisition geometry, but they are not
available for all types of plants and forests. Besides, field data are essential to properly parameterize
them. Semi-physical models face the same limitations as full physical models. Furthermore, they are
unable to make use of the full spectral bands available. The retrieved LAI quality is subject to
spectral noise in these bands. Semi-empirical models enjoy the strengths of physical models by
considering light transfer within the canopy and the flexibility of empirical models. They require
careful selection of a dataset representative of wide potential conditions for model calibration. But
they are relatively easy to use, and are highly insensitive to additive or multiplicative spectral noises.
VI-based statistical methods are the most flexible, but can still produce locally accurate estimates.
They do not need calibration and can be applied to large geographic areas. However, the models are
area- and possibly sensor-specific, and hence impossible to be transferred to other cases of study.
What predictor variables should be included in the model requires careful scrutiny of all the
considered potential candidates, which can take a long time and lots of effort to determine.
Table 6.14
Comparison of the strengths and limitations of four types of analytical methods in retrieving LAI from imagery data
Method Advantages Disadvantages
PROSAIL
RTM inversion
(LUT)
• No model calibration is required;
• Taking into account full set of spectral
bands, relatively insensitive to random
noise;
• Sound accuracy and precision;
• Generic, applicable for multiple
images, dates, acquisition geometries in
principle, even data from different
sensors can be combined;
• A well suited RTM needed for
the studied vegetation type;
• Suitable RTMs not equally
available for all types of
vegetation (e.g., forests);
• Computing skills needed to use
some resources and process data
for large areas;
• Information needed for model
parameterization (i.e. setting the
proper range of important model
parameters);
Semi-physical
(PEphysical)
• Extremely fast and simple model
inversion after calibration;
• (partially) use of physical knowledge
incorporated in the RTM.
• Same as for LUT;
• Unreliable wavelengths may be
combined due to problems with
simulated and/or measured data;
• Precision on LAI retrieval
subject to offset and noise in the
spectral data;
• Information available in
additional wavebands neglected;
Semi￾empirical
(PEre-adjusted)
• Simple to apply;
• Combining some advantages of
physical and statistical approaches;
• A well-chosen dataset needed for
model calibration (i.e., must be
representative of all potential
conditions, not only LAI);6.4.5
Method Advantages Disadvantages
• Relatively insensitive to additive or
multiplicative spectral errors
(depending on VI type);
• Information available in
additional wavebands neglected;
VI-based
statistical
(Empirical)
• Often yielding locally very accurate
results with minimum modeling efforts;
• Simple modeling/calibration technique
quickly applicable to larger images;
• Able to map all kinds of vegetation
parameters not necessarily being part of
existing RTM.
• Same as semi-empirical above;
• Sometimes lacking
generalization and reproducibility
and thus impeding transferability
to other images or conditions.
Source: Modified from Atzberger et al. (2015), open access.
Hybrid retrieval algorithms are created by combining physical models with empirical equations.
They are exemplified by the SAIL-based L2B retrieval algorithm available in the Sentinel
Application Platform (SNAP), and INFORM combined with ANN (Brown et al., 2019). In the
SNAP L2B retrieval algorithm, leaf reflectance spectra are simulated by PROSPECT tens of
thousands of times, during which the input parameters are assigned values randomly from their
range. In the hybrid methods of ANN with INFORM, the ANN is trained with measured spectra to
estimate LAI and to modify the network architecture if necessary. The application of the final ANN
to an image generates maps of LAI spatial distribution.
Table 6.15
Comparison of two hybrid analytical models in retrieving forest LAI from Sentinel-2 data
Method Day of year R² RMSE nRMSE (%) Bias Precision n
SNAP L2B
100 - 149 0.77 1.22 60.28 –1.03 0.68 17
150 - 249 0.07 1.34 32.06 –1.17 0.66 39
250 - 300 0.14 2.00 52.76 –1.94 0.49 25
INFORM
100 - 149 0.74 0.51 25.05 0.09 0.51 18
150 - 249 0.11 0.40  9.61 –0.19 0.36 43
250 - 300 0.02 0.55 14.47 –0.18 0.53 26
Source: Brown et al. (2019) open access.
A comparison of the two hybrid methods indicates that the SNAP L2B retrieval algorithm
underestimates forest LAI from Sentinel-2 data, leading to a moderate retrieval accuracy (R² = 0.54,
RMSE = 1.55, NRMSE = 43%) (Table 6.15). INFORM achieves better retrieval accuracy (R2 =
0.69, RMSE = 0.52 g ∙ m−2, NRMSE = 29%) because of its ability to better reproduce observed
multispectral imager spectra than SAIL (Brown et al., 2019). This accuracy is highly comparable to
0.73 obtained by Schlerf and Atzberger (2006).
Global LAI ProductGlobal distribution maps of LAI have been produced from integrated coarse resolution images, such
as FY-3D/MERSI-II (Figure 6.15). LAI data products are available at widely ranging scales. For
local heterogeneous scales, they are produced from hyperspectral data. Such products are unfit for
climate modeling. So global LAI products have to be generated from coarse resolution data using
routine algorithms. The individually retrieved results may be aggregated to a certain time period,
such as weekly or monthly. Monthly global LAI product has been available since 2017, produced
from MODIS reflectance data at 1 km resolution (up to 250 m) either daily or 8-day integrated
available at https://neo.gsfc.nasa.gov/view.php?datasetId = MOD15A2_M_LAI. Most of the recent
(since 2000) data products are derived from the new generation of satellite data at a finer spatial
resolution. The older data products dating back to the early 1980s are produced from AVHRR data at
a much coarser spatial resolution (Table 6.16).
FIGURE 6.15 Global distribution of LAI in February 2019 from FY-3D/MERSI-II. (Yang et al., 2019,
with permission (5761221039234) from Springer.)
Table 6.16
Major long-term global LAI products and their main characteristics
Product Sensor
Spatial
resolution
Temporal
resolution Algorithms References
EUMETSAT
polar system MetOp/AVHRR 1.1 km
10-day
(2015-)
Gaussian process
regression
García￾Haro et
al.
(2018)
GEOV2
SPOT/VEGETATION,
MODIS 1/1120
10-day
(1999-)
NN (red, NIR,
SWIR, and SZA)
Baret et
al.
(2013)6.5
Product Sensor
Spatial
resolution
Temporal
resolution Algorithms References
GLASS V3
SPOT/VEGETATION,
MODIS 1 km
8-day
(2000-)
NN (red and
NIR)
Xiao et
al.
(2014)
GLOBMAP
V2 MODIS 500 m
8-day
(2000-)
Empirical VI￾LAI relationship
Liu et
al.
(2012)
JRC-TIP MODIS 0.01°
16-day
(2000-)
Data
assimilation
retrieval from
albedo
Pinty et
al.
(2011)
MISR V2 MISR 1.1 km
Daily
(2000-)
LUT (red and
NIR)
Diner et
al.
(2008)
MODIS C6 MODIS 500 m
4-day
(2000-)
LUT (red and
NIR)
Huang
et al.
(2008)
PROBA-V PROBA-V 300 m
10-day
(2014-)
NN (blue, red,
NIR, and obs
geometry)
Baret et
al.
(2016)
VIIRS SNPP/VIIRS 500 m
8-day
(2012-)
LUT (red and
NIR)
Yan et
al.
(2018)
GEOV1 AVHRR 0.05°
Daily
(1981-)
Backpropagation
NN
Verger
et al.
(2011)
GLOBMAP AVHRR 8 km
Half￾monthly
(1981-)
VI-LAI
relationship
Liu et
al.
(2012)
Source: Modified from Fang et al. (2019), free access.
Generated and released by the Center for Global Change Data Processing and Analysis of Beijing
Normal University, the GLASS 8-day LAI product has one of the longest spans from 1981 to 2014
in the world, retrieved using GRNNs. Unlike existing neural network methods that use remote
sensing data acquired only at a specific time to retrieve LAI, the GRNNs were trained using fused
time-series LAI values from MODIS and CYCLOPES LAI products and reprocessed time-series
MODIS/AVHRR reflectance. The GLASS LAI product is also available from the Global Land
Cover Facility at the University of Maryland (https://geog.umd.edu/feature/global-land-cover￾facility-(glcf)).
CHLOROPHYLL CONTENT6.5.1
Chlorophyll content (CC) is a biochemical parameter of vegetation that can shed valuable
information on the physiology, condition, and process (function) of canopied plants. It is defined as
the sum of chlorophyll a and b pigments in contiguously growing plants per unit area, commonly
expressed as mg ∙ m-2. It is significant to quantify canopy CC (CCC) as it is indispensable to assess
how plant functions and adaptation have been influenced by climate change and anthropogenic
activities. Similar to LAI, CCC can be measured in the field using an optical chlorophyll meter,
which determines a relative value proportional to leaf CC based on the ratio of incident and
transmitted radiation at 650 nm and 940 nm. Relative values are converted to absolute units using
species-specific calibration functions. This method is time- and labor-demanding, unable to yield
spatial distribution of CCC. In comparison, CC estimation from satellite images is much faster, if
facilitated with powerful models and analytical algorithms. CC is stored mostly in tree leaves. Leaf
CC is difficult to estimate from remotely sensed data because it is affected by leaf orientation and
structure. Instead, it is canopy-level CC that has been intensively studied via both imagery and non￾imagery data using various methods.
Useful Spectral Indices
CC of both trees and crops has been quantified using VIs calculated from NIR and red/green bands.
Some of them are designed specifically for Chl-a retrieval while others are generic, devised to study
vegetation conditions in general. There are six chlorophyll-specific indices, including green
chlorophyll index (CIgreen) and red edge CI (CIred edge), red edge MERIS total chlorophyll index,
modified chlorophyll absorption reflectance index, transformed chlorophyll absorption reflectance
index, and triangular chlorophyll index, plus the novel Inverted Red-Edge Chlorophyll Index
(IRECI) (Table 6.17). They are all based on two bands, one NIR around 780 nm or the red edge
(around 705 nm), and another green (550 nm) or red band. The index is calculated from the ratio of
these bands or with some modification as:
(6.30)
CIred edge becomes CIgreen if the red edge waveband is substituted by the green waveband. Not all of
the indices in Table 6.18 are derivable from multispectral bands, such as Landsat 8 imagery. The
exact wavelengths have to be shifted slightly when multispectral bands containing the needed
wavelength do not exist. For instance, with Sentinel-2 bands, the wavelength for green band shifts
from 550 to 560 nm, and the NIR band shifts from 780 to 783 nm due to its unique band designation
(Ali et al., 2020). Since drone-acquired digital photographs have only four spectral bands, they
cannot be used to derive any of these Chl-a indices.
Table 6.17
Comparison of accuracy of four methods in estimating CCC in the Bavarian National Forest Park (Germany) from Sentinel-2
data
Method Variables R² RMSE RMSE(%)
Parametric statistical
method (VI-based)
Simple ratio VI (SRVI) 0.69 0.24 14.69
Modified Simple Ratio 2 (mSR2) 0.74 0.23 13.57Method Variables R² RMSE RMSE(%)
Modified Simple Ratio 3 (mSR3) 0.75 0.21 12.53
Modified Simple Ratio 4 (mSR4) 0.65 0.25 15.22
Datt Derivation (DD) 0.50 0.29 17.36
Normalized Difference Red Edge
index (NDRE) 0.63 0.25 14.56
Sentinel-2 red edge position index
(S2REP) 0.48 0.29 17.11
MCARI/OSAVI 0.57 0.26 15.44
Green Chlorophyll index 0.61 0.25 15.58
Red edge chlorophyll index 0.74 0.23 13.57
MERIS Terrestrial Chla index
MERIS Terrestrial CI (MTCI) 0.55 0.30 17.53
Modified MTCI (mMTCI) 0.68 0.25 14.09
Inverted red-edge chla index
(IRECI) 0.58 0.26 15.18
Non-parametric
statistical approach
PLSR using red edge bands 0.67 0.24 14.32
PLSR using 8 bands 0.78 0.22 13.10
PLSR using 10 bands 0.74 0.21 12.40
Physical model INFORM inversion using LUT 0.67 0.31 18.11
Hybrid method
PROSAIL inversion using ANN
(SNAP toolbox) 0.66 0.35 21.72
Source: Ali et al. (2020), open access.
Note: Bold: maximum value in a given method; italic: Chl-a specific indices.
The effectiveness of all indices in quantifying Chl-a is judged by their correlation with in situ
measurements. The results in Table 6.17 suggest that Chl-a specific indices are not always more
accurate than generic indices in that some can outperform the former. In general, all Chl-a specific
indices can achieve a rather high r close to 0.9 except CIred edge. It must be noted that these results
are based on irrigated maize. It remains unknown whether the findings will hold for other types of
crops that have different canopies or for trees and shrubs. In predicting forest CCC, Chl-a specific
indices are not as effective as simple ratios. Although the red edge CI is the best among all the CI
indices, its r of 0.74 is still lower than that of modified simple ratio 3 (r = 0.75). The modified
simple ratio 3 (mSR3) (665, 865) has the lowest cross-validation RMSE of 0.21 g ∙ m-2 (R2 = 0.75)(Ali et al., 2020). Whether the indices are derived from narrow hyperspectral bands or broad bands
from Landsat TM imagery do not make much difference to their accuracy (Table 6.18).
Table 6.18
Correlation coefficients (r) between various vegetation and chlorophyll indices from low-altitude AVIRIS data and irrigated
maize chlorophyll content obtained by a chlorophyll meter. The AVIRIS data were amalgamated to similar bands of TM and a
digital camera (n = 20)
Name Abbreviation AVIRIS
TM
bands
Camera
bands
Ratio vegetation index RVI 0.82 0.82 -
Normalized difference vegetation index NDVI 0.82 0.82 -
Soil adjusted vegetation index SAVI 0.74 0.64 -
Modified soil adjusted vegetation index MSAVI 0.76 0.66 -
Optimized soil adjusted vegetation
index OSAVI 0.79 0.73 -
Enhanced vegetation index EVI 0.73 0.61 -
Triangular vegetation index TVI 0.64 0.50a -
Second modified triangular vegetation
index MTVI2 0.72 0.64 -
Green normalized difference vegetation
index gNDVI 0.89 0.88 -
Normalized green red difference index NGRDI -0.92 -0.89 -0.87
Green leaf index GLI -0.91 -0.90 -0.89
Visible atmospherically resistant index VARI -0.91 -0.91 -0.84
Normalized difference red edge index NDREI 0.76 -b -
Chlorophyll index - green CI-G 0.90 0.89 -
Chlorophyll index - red edge CI-RE 0.76 - -
MERIS total chlorophyll index MTCI 0.89 - -
Modified chlorophyll absorption
reflectance index MCARI -0.89 - -
Transformed chlorophyll
absorption reflectance index TCARI -0.88 - -
Triangular chlorophyll index TCI -0.89 - -
Combined index with TCARI TCARI/OSAVI -0.89 - -
Combined index with MCARI MCARI/MTVI2 -0.89 - -
Triangular greenness index TGI -0.91 -0.91 -0.92
Source: Modified from Hunt et al. (2013).
Notes:
a Insignificant at the α = 0.01 level.
b Not applicable.6.5.2
Canopy Chl-a of maize and soybean crops bears a close relationship with the index of (RNIR/Rλ1) - 1
(λ1 in the green and red edge spectral wavelength region) (Gitelson et al., 2005). The exact value of
λ1 varies with the canopy structure of vegetation or crop type. A range of 720–730 nm accurately
estimates Chl-a in such very contrasting species as soybean and maize, and thus can be used to
estimate canopy Chl-a in a mixed vegetation. If soybean and maize are lumped together, their Chl-a
in the range of 0.03–4.33 g ∙ m-2 is estimated at a RMSE < 0.32 g ∙ m-2. For mixed pixels containing
both soybean and maize, the accuracy of estimation using the green index (Rλ1 = Rgreen) decreases to
RMSE < 0.69 g ∙ m-2 (R2 = 0.7) with coarse resolution MODIS bands, and the accuracy of the red
edge model (Rλ1 = Rred edge) decreases slightly to RMSE < 0.41 with MERIS bands.
In spite of the relative ease of generating VIs, most, if not all, spectral VIs, are affected by
confounding variables such as background and vegetation structure when upscaled to the canopy
level via space-borne optical images in assessing vegetation biophysical parameters (Verrelst et al.,
2012). Besides, the simple design of VIs fundamentally underexploits the full potential of the multi￾or hyperspectral data cube. Better alternatives are machine learning methods such as GPR.
Methods of Estimation
The methods for estimating CC at both the foliar and canopy levels fall into three types: statistical,
physical-based, and machine leaning algorithms. Physical-based methods are radiometric data￾driven, and they establish the cause-effect relationships based on physical laws governing the
transfer and interaction of radiative energy within the canopy, such as the RTMs, and leaf-level
models of PROSPECT and LIBERTY. The outputs from the PROSPECT model can be used as
inputs to the SAIL model. Physical models are the proper choices for heterogeneous canopies.
However, these models are complex involving a large number of input variables that must be
properly parameterized. A huge effort is needed to collect the data in the field or via simulation, such
as leaf structure, leaf chlorophyll content, understory LAI, stem density, stand height, crown
diameter, mean leaf angle, sun zenith angle, and observation zenith angle. Needless to say, their
accuracy directly governs the reliability of the quantified CCC. New approaches can be created by
combining some of the methods from each category, such as PROSAIL inversion using ANN as in
the SNAP toolbox.
A few physical models have been used to retrieve CCC, such as the SAIL-based L2B retrieval
algorithm available in the SNAP toolbox and INFORM. INFORM has a demonstrated capacity of
and suitability for retrieving forest structural and biochemical variables such as CC, FPAR, LAI, leaf
dry matter in both broadleaf and conifer stands at varying levels of success. These models are
complex, requiring the input of 15 parameters concerning LAI, dry matter, canopy height, crown
diameter, stem density, average leaf angle, solar and observer zenith angle, relative azimuth angle,
and soil brightness, and are suitable for retrieving CCC. Approaches for generating operational
products include the inversion of RTMs using LUTs, in addition to hybrid methods, which use RTM
outputs to train machine learning algorithms.
Table 6.19
Accuracy of retrieving CCC from Sentinel-2 MSI data using two physical models in comparison with interpolated field data,
by phenological subsetMethod Day of year R² RMSE (NRMSE) Bias Precision n
SNAP L2B
100-149 0.70 0.12 (36.78%) 0.06 0.11 17
150 to 249 0.10 0.72 (30.89%) -0.45 0.57 39
250 to 300 0.26 1.11 (59.92%) -0.94 0.59 25
INFORM
100-149 0.66 0.41 (126.32%) 0.39 0.15 18
150 to 249 0.04 0.49 (20.94%) -0.30 0.39 43
250 to 300 0.17 0.62 (33.56%) -0.04 0.63 26
Source: Brown et al. (2019), open access.
The SNAP L2B retrieval algorithm has a moderate performance (R² = 0.52, RMSE = 0.79 g ∙ m–2,
NRMSE = 45%). The INFORM-based retrieval algorithm is more accurate in estimating CCC (R² =
0.69, RMSE = 0.52 g ∙ m–2, NRMSE = 29%) (Brown et al., 2019). It is more capable of reproducing
the observed MSI spectra than SAIL. SAIL-based retrieval algorithms are suitable for estimating
crop CC due to their dense, leafy, and homogeneous composition. SAIL based on 1D RTM can
competently approximate these canopies, enabling CC to be retrieved at high accuracy levels. But
they are not suited to deciduous broadleaf forest canopies that are highly heterogeneous, containing
a larger number of woody elements and being subject to increased crown transmission, foliage
clumping, and shadowing (Brown et al., 2019). None of them can be considered by SAIL, causing
CCC to be underestimated when the SNAP L2B retrieval algorithm is applied to a deciduous
broadleaf forest (Table 6.19). INFORM retains a similar performance (R2 = 0.67 and RMSE = 0.31
g ∙ m–2) in estimating CCC of a mixed mountain forest using Sentinel-2 data (Ali et al., 2020).
However, INFORM has a compromised capacity in retrieving foliar chlorophyll, attaining a much
lower accuracy of R2 = 0.39 (RMSE = 6.2 and NRMSE = 0.31) from RapidEye data and only R2 =
0.45 (RMSE = 8.9 and NRMSE = 0.36) from Sentinel-2 data (Table 6.20). The Sentinel-2 accuracy
is even lower than that of SBAP L2B. However, neither is able to make a reputable estimation at a
lead time of more than 149 days. The higher accuracy from RapidEye is attributed likely to its finer
spatial resolution of 5 m, four times better than 20 m of Sentinel-2 images. They have been
employed to generate the spatial distribution of CCC in the Bavaia Forest National Park of Germany
using four methods (Figure 6.16).
Table 6.20
Accuracy indicators in retrieving foliar chlorophyll content from Sentinel-2 and RapidEye data using the INFORM algorithm
Solution
Sentinel-2 (n = 50) RapidEye (n = 30)
R² RMSE NRMSE R² RMSE NRMSE
Best fitting spectra 0.45 8.9 0.36 0.39 6.18 0.31
Mean of first 10 0.39 8.6 0.34 0.36 5.22 0.26
Mean of first 50 0.37 9.46 0.37 0.30 5.44 0.27
Mean of first 100 0.35 10.23 0.41 0.32 5.36 0.27
Source: Darvishzadeh et al. (2019), with permission (240318-001253) from Elsevier.FIGURE 6.16 Comparison of CCC (g × cm–2) distribution in the Bavaia Forest National Park of
Germany, generated from Sentinel-2 data using the SNAP toolbox, SRVI, INFORM inversion via
LUT, and PLSR approaches. (Ali et al., 2020, open access.)
Compared with physical models, statistical-based approaches are much simpler by establishing the
regressional relationships between CCC and VI such as the modified simple ratio (mSR) vegetation
index. They are variable-driven, either parametric or non-parametric. A renowned approach is
exemplified by PLSR. Parametric regression produces inductive and empirical models of CCC from
spectral reflectance or its transformation. It enjoys the strengths of being inherently simple, fast, and
requires minimum computation. Nevertheless, the constructed models may be vegetation type- and
site-specific without the ability to upscale the quantified results. Non-parametric models are either
liner or non-linear, constructed from training data via learning. This suite of methods includes
stepwise multiple linear regression, principal components regression, ridge (regulated) regression,
decision tree learning (e.g., random forest regression), ANN, kernel methods (e.g., SVM), GPR, and
Bayesian networks. These non-parametric models are computationally intensive, requiring field
data, and may suffer from overfitting if the regressed models are overly complex, and tend to be data
(or sensor)-specific (Verrelst et al., 2015a). Thus, new models must be constructed if remotely
sensed data from a different sensor are used.6.6
6.6.1
Of the aforementioned models, GPR is relatively simple and has been used for the retrieval of CC
of crop leaves from airborne hyperspectral CHRIS data. It allows easy parameterization of signal
and noise relations in the kernel function. GPP is therefore better able to establish complex
relationships at the canopy level than other machine learning algorithms. It is able to establish strong
relationships between reflectance at the red and red edge wavelengths with CC, and yields a
validation accuracy of R² = 0.96 and a RMSE = 3.82 μg ∙ cm–2 (12% rRMSE) (Verrelst et al., 2013).
The excellent performance of GPR becomes even more obvious (e.g., 3.4 orders higher) in
comparison with the conventional linear regression (rRMSE = 41%). The extremely high accuracy is
achieved probably because the ground area covered by the imagery is rather small, and tend to be
homogeneous (e.g., lack of variation in topography and crop types). Properly trained GPR models
can be smoothly adapted to spectrally resampled CASI images, and the produced CC estimate has
the same order of magnitude as that of the CHRIS images, but at a lowered confidence, especially in
recently irrigated areas, due probably to the spectral mixture between irrigated vegetation and wet
soil. This opens up the possibility for operational monitoring of crop health from super-fine￾resolution airborne hyperspecrtal data (a pixel size of 1.4 m), but at the field level only. Regional
level estimation requires coarser resolution data, so it remains to be seen whether the model can be
applied to medium-resolution images for landscape-scale estimation at a similar accuracy level.
BIO-QUALITY VARIABLES
Net Primary Productivity
NPP refers to the quantity of carbon ingested by plants to produce biomass in an ecosystem. “Net”
means the deduction of the quantity of carbon lost via respiration from the total amount of carbon
intake by plants through photosynthesis. It is expressed as ton carbon per unit area (usually, ha) per
annum. NPP differs from gross primary productivity that stands for all the carbon fixed by plants in
an ecosystem. Virtually, NPP represents the ecosystem’s energy flux. It is important to estimate NPP
to assess ecosystem health and value. Landscape- and regional-scale NPP quantification can be
competently achieved from remote sensing data based on various models such as the MODNPP
model. It makes use of LUE and PAR that is considered as the driving force of plant photosynthesis,
together with other external environmental factors, or:
(6.31)
where APAR = PAR absorbed by vegetation. Its determination requires shortwave radiation and
LUE, both of which can be calculated from coarse resolution weather satellite data, such as MODIS.
The suitable images must have a visible band (620–670 nm), an NIR band (841–876 nm), and an
“ocean” band (either 526–536 nm or 546–556 nm) to meet the MODNPP requirement in estimating
carbon dioxide (CO2) flux from terrestrial vegetation. Empirically, it is expressed as:
(6.32)
where PRIMOD = photochemical reflectance index calculated from MODIS bands as:
(6.33)(6.34)
where PAR = photosynthetically active radiation from sunrise to sunset, and is calculated as:
(6.35)
where SWR = shortwave radiation. Monthly SWR received by a given surface can be calculated
from solar altitude angle, solar azimuth angle, angle of latitude, and solar declination angle, and
hour angle, given a location’s slope and aspect that are derived from a DEM (Pachavo and Murwira,
2014).
Alternatively, NPP has been quantified using the dry matter productivity (DMP) model as:
(6.36)
where SWR = incoming shortwave (200–300 nm) solar radiation with an average value of 48% of
PAR (400–700 nm). fPAR = the PAR fraction absorbed by green vegetation (refer to Section 6.6.2
for its quantification); ε(T) = efficiency of converting the absorbed energy to biomass (radiation use
efficiency), the losses related to the transport of photosynthetates, and the maintenance of the
standing phytomass. There exists a close relationship between NPPMOD and DMP (ton C ∙ ha–1 ∙ yr–
1) at an R² value of 0.828.
Global NPP products have been produced using the improved multi-source data synergized
quantitative (MuSyQ) NPP algorithm. It requires long-term global land cover and terrestrial
satellite-derived LAI and fPAR products, LUE from the parameterization approach with the
clearness index, meteorological data, and other environmental factors (temperature, dew
temperature, surface net solar radiation, surface net thermal radiation, surface solar radiation
downwards, land cover and DEM), all of which can be sourced from other data products (Figure
6.17). This product is more accurate (R2 = 0.81, RMSE = 214.6 g C ∙ m–2 ∙ yr–1) than MOD17 NPP
(R2 = 0.55, RMSE = 214.7 g C ∙ m–2 ∙ yr–1) in comparison with the BigFoot NPP data.6.6.2
FIGURE 6.17 Spatial distribution of global mean annual NPP (1981–2018) produced using the multi￾source data synergized quantitative algorithm. (Wang et al., 2021, open access.)
fPAR
Fraction of PAR (fPAR), also known as FAPAR, refers to the portion of the incident visible light
radiation (wavelength: 0.4–0.7 μm) that is absorbed by living vegetation. It is expressed as a ratio
with a value ranging from 0 to 100%. This biophysical parameter measures directly the primary
productivity of photosynthesis and characterizes the growth status of vegetation and is indicative of
its capacity to absorb energy. Crop fPAR is an indirect indicator of crop yield and biomass
production. It is one of the key biophysical variables affecting mass and energy exchanges between
the surface and the planetary boundary layer. This variable plays an indispensable role in numerous
ecological, hydrological, and climate models. Thus, it is very essential to quantify its spatial
distribution using remotely sensed data.
Similar to LAI, fRAR can also be measured in the field, but its derivation from optical imagery
data is much more efficiently achieved based on its empirical relationship with VIs. fPAR is closely
correlated with normalized difference index (NDI), and scaled difference index (N*) (Table 6.21).
However, this relationship is affected by fPAR value in that NDI becomes saturated as fPAR values
rise to above 0.70, beyond which the relationship between fPAR and N* reverses to be inverse
instead of being positively proportional (Tan et al., 2018). Thus, a piecemeal model is needed to
estimate fPAR values lower than 0.70, and another for estimate fPAR values above 0.70 from N*.
Table 6.21
Vegetation indices derived from hyperspectral bands that have found use in estimating maize fPAR
Hyperspectral metrics Abbreviation Formula
Photochemical reflectance index PRI (R526 – R569)/(R526 + R569)Hyperspectral metrics Abbreviation Formula
Modified NDVI mNDVI (R755 – R712)/(R755 + R712)
Carter index Ctr2 R698/R755
Carotenoid reflectance index CRI (1/R512) – (1/R698)
Anthocyanin reflectance index ARI (1/R555) – (1/R698)
Vogelmann red edge index 1 VOG1 R741/R726
Vogelmann red edge index 2 VOG2 (R741 – R755)/(R712 – R726)
Simple ration 1 SR1 R411/R712
Simple ration 2 SR2 R411/R698
Simple ration 3 SR3 R783/R769
Simple ration 4 SR4 R755/R712
Simple ration 5 SR5 R898/R683
Simple ration 6 SR6 R798/R669
Simple ration 7 SR7 R669/(R555 × R712)
Optimized vegetation index 1 VIopt1 R755/R726
Optimized vegetation index 2 VIopt2 100*(lnR755 – lnR726)
Pigment specific simple ratio 1 PSSR 1 R798/R683
Pigment specific simple ratio 2 PSSR 2 R798/R641
Pigment specific simple ratio 3 PSSR 3 R798/R469
Sum green index SGI
Normalized mean reflectance of 500-600
nm
Structure intensive pigment index SIPI (R798 – R440)/(R798 – R683)
Normalized pigments chla ratio index NPCI (R683 – R426)/(R483 + R426)
Red-edge vegetation stress index RVSI 0.5(R712 + R755) – R726
Double difference index DDI (R755 – R726) – (R698 – R669)
Difference vegetation index DVI R798 – R683
Visible atmospheric resistant index VARI (R555 – R683)/(R555 + R683 – R483)
Water band index WBI R898/R969
Red green ratio RGratio Rred/Rgreen
Red edge position index REPI Max value from 690 to 740 nm
Plant senescence reflectance index PSRI (R683 – R497)/R755
Derivative chlorophyll index DCI δ715/δ726
Maximum first derivative for red edge δmax red-edge δmax [680-750]
TCARI, GNDVI, EVI, TVI, SAVI, MSAVI, OSAVI, MTVI, MSAVI, DDI/MSAVI,
MCARI/OSAVI, TCARI/OSAVI
Source: Qin et al. (2018), open access.
So far, more than 54 VIs have been explored to estimate corn fPAR from hyperspectral data (Table
6.21). Their effectiveness has not been systematically compared. Of all the VIs that have been tried,MSAVI is more accurate than others (Tan et al., 2018). NDVI is better for crops in the sense that it
can suppress the background influence when they are at their infancy with little biomass. A linear
relationship exists between fPAR and NDVI, from which fPAR can be quantified. The only problem
with this relationship is that NDVI suffers saturation with dense vegetation. So, it has been
substituted by other indices, and multiple models involving different VIs are applied to the
appropriate range of fPAR, such as NDI for fPAR ≤ 0.70, and scaled difference index (SDI) for
fPAR > 0.70 (Tan et al., 2018). In addition to SDI and NDI, other indices including red edge NDVI
and GNDVI (Table 6.22) have also been used. They all bear an exponential relationship with fPAR
at an R² value ≥ 0.74.
Table 6.22
Effectiveness of five VIs in estimating green fPAR of maize and soybean from field-collected reflectance spectra that were
resampled to MODIS and Sentinel-2 MSI band wavelengths
Spectral index Model R² RMSE
EVI2 –1.06x² + 2.28x – 0.26 0.88 0.096
NDVI 0.07e2.81x 0.92 0.075
WDRVIα = 0.5 0.85x + 0.16 0.92 0.069
Green NDVI 1.6891x – 0.5271 0.92 0.067
Red edge NDVI 1.2531x – 0.1035 0.95 0.057
Source: Gitelson et al. (2019), © Taylor & Francis.
The most common approach of estimating fPAR is to regress field-collected fPAR against VIs that
combine visible and NIR bands, such as green NDVI from drone images and WDRVI. The
relationship between fPAR and VIs established via statistical analysis can be either linear or non￾linear. In comparison with linear relationships, non-linear exponential relationships are more
common and accurate, achieving an R2 ≥ 0.736 with the smallest RMSE being only 0.114 (Table
6.23). The best piecemeal model for estimating wheat canopy fPAR from hyperspectral data takes
the form of:
(6.37)
Table 6.23
Accuracy of estimating wheat fPAR from various VIs derived from hyperspectral bands using empirical regression models, all
being statistically significant at the 0.01 level (p < 0.01)
Hyperspectral VI Model R² RMSE
SR[787, 765] 0.0175e2.4096X 0.764 0.163
PSND[800, 635]a 0.1797e1.8565X 0.757 0.167
PSND[800, 470] 0.0470e2.6013X 0.764 0.159
NDI 0.1950e1.5638X 0.865 0.114
GNDVI 0.1756e1.4955X 0.772 0.161
Scaled difference indexb 0.4396e0.9866X 0.839 0.143
MSAVI 0.0247e2.5335X 0.762 0.177Hyperspectral VI Model R² RMSE
NDVI[760, 708] 0.2746e1.0757X 0.822 0.176
NDVI[800, 600] 0.1637e1.7319X 0.779 0.161
NDVI[780, 550] 0.1396e1.7401X 0.735 0.152
NDVI[800, 700] 0.2003e0.9769X 0.751 0.159
NDVI[900, 680] 0.1946e0.8476X 0.736 0.172
Source: Tan et al. (2018), open access.
Notes:
a Pigment specific normalized difference = (ra – rb)/(ra + rb).
b (NDVI – NDVI0)/(NDVIS – NDVI0) (Barton and North, 2001).
The accuracy of estimating fPAR of maize and soybean using simple VIs can be very high, with a
small RMSE of 0.057, 0.067 and 0.069 from red edge NDVI, green NDVI, and WDRVIα=0.5,
respectively (Table 6.23). The corresponding R² value is exceptionally high at 0.92 or higher. Model
format, whether linear or non-linear, does not make much difference to model accuracy. Since the
results are obtained from field-collected reflectance spectra, caution needs to be exercised to
generalize the findings to space-borne images.
Apart from multispectral and hyperspectral images, crop fPAR can also be estimated from
airborne LiDAR data alone or using merged metrics from both LiDAR and imagery data. The
derivation of fPAR from LiDAR data relies on intensity-based fractional cover (fc-intensity) and
canopy relief in the form of:
(6.38)
where CNRH = canopy relief ratio of vegetation point cloud height, calculated as (Hmean−Hmin￾vegetation)/Hrange. fc-intensity is calculated as:
(6.39)
where Icpy = sum of vegetation canopy return intensity; Ig = sum of ground return intensity; k =
reflectance adjustment factor (default value = 2.0). Both Icpy and Ig need to be aggregated to the plot
level to derive the sum of raw LiDAR echo intensity I that must be corrected for the distance
between the sensor and the target Ds as:
(6.40)
where Ds = perpendicular distance from the sensor to the target; θ = incidence angle of the laser
pulse. fPAR can be estimated from LiDAR height and coverage metrics alone at a rather high
accuracy of R² = 0.81 using simple multiple linear regression models (Eq. 6.38). This accuracy
slightly surpasses 0.76 achieved by fc-intensity but is markedly higher than the maximum R2 of 0.50
achieved from the hyperspectral VI-based model involving two indices in the form of:
(6.41)where DCI = derivative Chl-a index (δ715/δ726); VOG2 = Vogelmann red edge index 2 [(R741 –
R755)/(R712 – R726)]. As illustrated in Table 6.24, the joint use of both LiDAR and hyperspectral
metrics improves the model accuracy only marginally to R2 = 0.88 (Qin et al., 2018). Thus, imagery
data are less effective than LiDAR data at predicting maize fPAR. The best accuracy of R² = 0.88 is
still lower than that in Table 6.23 obtained from in situ spectral data. Thus, the caution raised
previously is indeed warranted.
Table 6.24
Comparison of accuracy in estimating maize fPAR from LiDAR data with that from both LiDAR and hyperspectral imagery
Data Metric R² Adjusted R² RMSE
LiDAR fc-intensity 0.77 0.76 0.047
fc-intensity, CNRH 0.82 0.81 0.042
Imagery DCI 0.48 0.46 0.065
DCI, VOG2 0.53 0.50 0.061
LiDAR and imagery fc-intensity,CNRH, DCI 0.89 0.88 0.035
Source: Qin et al. (2018), open access.
Another method of estimating fPAR is via inversion of the PROSAIL model with the use of a LUT
by vegetation type (Hou et al., 2020). In total, 10 parameters related to canopy properties (e.g.,
crown spread, LAI) must be input to the model, each having its own range. Since the range is a
function of vegetation types, they have to be separated into cropland, grassland, shrubland, broadleaf
forest, and needleleaf forest. Given that the reflectance spectrum of sparse vegetation is mainly
affected by soil, different from that of dense vegetation that is mainly affected by the biochemical
composition of vegetation, the NDVI-based inversion of fPAR requires the area of study to be
grouped into two classes of NDVI < 0.4 for sparse vegetation and NDVI > 0.4 for dense vegetation
(NDVI < 0 non-vegetation). The inversion has to be repeated twice, once for sparse vegetation, and
another for lush vegetation, and the two are merged to form a final map.
As the secondary parameter of the PROSAIL model, fPAR is retrieved using the following
relationship between fPAR and LAI:
(6.42)
where K = extinction coefficient influenced by LAI and the solar zenith angle. Its value varies with
vegetation type, such as 0.3–0.5 for grasslands and mostly above 0.7 for herbs and shrubs. Satellite￾retrieved fPAR is fairly consistent both spatially and temporally, closely correlated with fPAR
products (Hou et al., 2020). However, this method is not accurate for all types of vegetation equally.
fPAR products have been generated from various data using different methods. The MODIS level-1
data products (MOD15A2H) are available from the Atmosphere Archive & Distribution System
Distributed Active Archive Center (LAADS DAAC) (https://ladsweb.modaps.eosdis.nasa.gov/)
(Figure 6.18). GEOV1 fPAR products have two spatial resolutions of 300 m and 1 km. The former is
produced from the PRoject for On-Board Autonomy–Vegetation (PROBA-V) data and is available
from 2014 to present. The latter is based on the SPOT Vegetation (SPOT-VGT) and PROBA-V
sensor-acquired data, with a temporal coverage since 1999. The GEOV1 fPAR data product isproduced by the Copernicus Global Land Service (https://land.copernicus.eu/global/). GLASS fPAR
products (GLASS09A01) are released by the Center for Global Change Data Processing and
Analysis at Beijing Normal University (accessible at www.geodata.cn/thematicView/GLASS.html).
FIGURE 6.18 Comparison of the fPAR inversion results from MOD09 (a) and FY-3C data (b),
respectively; (c) and (d) - the MODIS fPAR products in the Heihe research area of Northeast China.
(Hou et al., 2020, open access.)
Different data products are produced using different methods and have different accuracy levels.
Overall, the annual FY-FPAR product is closely correlated with other fPAR products for grassland,
cropland, shrub, broadleaf forest, and needleleaf forest. The correlation for grassland and cropland is
particularly strong with MODIS data product (R = 0.6102, RMSE = 0.1157), slightly less so with
GEOV1-FPAR for grassland (R = 0.5827, RMSE = 0.1205). The explanation is that sparse
vegetation contains more exposed soil, and its reflectivity affects the reflectance spectrum of
vegetation and the inversion accuracy. The inversion of shrub fPAR from satellite imagery needs to
consider the factors affecting bare soil (Hou et al., 2020). Of the MODIS MUltiscale Satellite6.6.3
remotE Sensing (MUSES) fPAR product, and the energy balance residual (EBR) method-based
fPAR product, the former exhibits the best spatial integrity, whereas the MODIS and EBR products
have numerous missing values in the equatorial rainforest regions and at high latitudes in the
Northern Hemisphere due probably to frequent cloud contamination (Zheng et al., 2022). EBR fPAR
products have a generally high consistency in their spatial patterns. However, a relatively large
discrepancy exists among these products in the equatorial rainforest regions and the middle- and
high-latitude regions dominated by forest. Direct validation reveals that the MUSES fPAR product is
more accurate (R2 = 0.69, RMSE = 0.15) than the MODIS fPAR product (R2 = 0.62, RMSE = 0.17)
and the EBR fPAR product (R2 = 0.57, RMSE = 0.19).
Light Use Efficiency
LUE is a key indicator of vegetation photosynthesis, which provides important insights into how
vegetation productivity responds to growing conditions. It is affected by such environmental factors
as moisture and temperature, and exhibits spatiotemporal variations in the photochemical reaction
process. LUE is commonly quantified from remotely sensed data using PRI calculated as:
(6.43)
where R531 and R570 = reflectance of the vegetation canopy at 531 and 570 nm, respectively. LUE is
then estimated from PRI using the following equation:
(6.44)
A stronger relationship exists between LUE and PRI if the reflectance data are corrected by the
BRDF. This correction increases estimation R2 from 0.46 to 0.60 (Ma et al., 2020). Generally, PRI is
able to capture diurnal and seasonal changes in LUE in subtropical, evergreen mixed forests.
However, the correlation has a large seasonal variation, with winter correlation significantly stronger
than summer correlation. Besides, this correlation is also strongly influenced by PAR, while vapor
pressure deficit (VPD) and air temperature (Ta) exert negative influences on the relationship.
Furthermore, the correlation is affected by air temperature. The correlation between PRI and LUE is
the strongest (R2 = 0.78) at a temperature between 10°C and 15°C. Both LUE and PRI decrease with
VPD, and the correlation between them decreases with increasing VPD. The closest correlation
between PRI and LUE (R2 = 0.22) occurs at VPD < 0.5. When VPD ranges from 1.5 and 2, the
correlation between them disappears. Thus, the use of PRI for accurate estimation of LUE needs to
take into account environmental factors and vegetation canopy structures.
A positive correlation exists between PRI and LUE variation at the canopy scale. However, PRI
varies widely with viewing angle. At large viewing angles (>30°) the index is also sensitive to leaf
angle distributions. To relate image-derived PRI to ground measured PRI requires correction for
Rayleigh scattering. Besides, it is strongly influenced by the soil background when the canopy LAI
falls below 3. Given the high sensitivity of PRI value to changes in LAI, the PRI-LUE relationship
to predict or improve estimates of canopy LUE based on either absorbed or incident light must be
modified by an independent estimate of LAI change between dates/locations of in situ
measurements (Barton and North, 2001).6.7
6.7.1
ABOVEGROUND BIOMASS (CARBON)
AGB is one of the most important metrics in measuring ecosystem functions and eco-service value.
The capability of a forest to sequester and store biomass (carbon) is critical to mitigating the local
climate warming caused by greenhouse gas emissions. Quantitative information on forest AGB is a
prerequisite of understanding the terrestrial carbon cycle and assessing the effectiveness of forest
management practices. It is very important to quantify AGB for three reasons: (i) Precise
information on AGC stock of vegetation is essential for carbon accounting and trading. Accurate
knowledge about the quantity of carbon stored in a forest sheds precise information on its health and
ecological value, and supplies the evidence for decision-making and transaction (e.g., carbon
economy); (ii) It serves as a benchmark in the long-term monitoring and change detection to identify
the trend of AGB change over time, and (iii) Accurate knowledge of forest carbon is indispensable
to precisely assessing how the forest ecosystem responds to climate change and to explore
environmental impacts on ecosystem functions and health. In case of crops, quantitative AGB allows
the exploration of how different treatments (e.g., dosage of fertilization) affect the state and the yield
at the plot level, and how external disturbance has affected ecosystem functionality and its recovery
in restorative ecology.
AGB is commonly estimated via field sampling and remote sensing. The former involves
destructive sampling and requires visual assessment of vegetation cover. It is time-consuming and
labor-intensive, and hence impractical for large-scale estimation. Such a deficiency can be overcome
with remote sensing. Both optical and radar images faithfully preserve the vegetative cover on the
ground graphically. With the assistance of in situ samples collected at a limited number of sites
concurrently with the acquisition of remote sensing images, images can be converted to maps
illustrating the quantitative distribution of AGB at high accuracy consistently and timely. Although
remote sensing is unable to quantify below-ground AGB directly, it is possible to estimate it
indirectly via its relationship with AGB. Remote quantification of AGB can be achieved using
different data, depending on the scale of quantification. It is critical to discuss AGB quantification
by scale as it affects not only the target of quantification but also the remotely sensed data useful and
the manner of data processing. In general, the broader the scale of quantification, the more remotely
sensed data are available at a high temporal frequency but a coarser spatial resolution, and more co￾variates of AGB can be considered in the quantification. The acquisition of some of them has been
eased by the availability of remote sensing data products. This section discusses micro-scale
quantification first, followed by meso-scale quantification that has been achieved using both
imagery and non-imagery LiDAR data.
Micro-scale Quantification
Micro-scale quantification is carried out for plot-level grassland (and even crop) AGB that is
extremely sensitive to climate change and external disturbances. The quantified AGB can inform us
of how the ecosystem has responded to climate warming, anthropogenic interventions, and rodent
disturbance. Quantitative grassland AGB information serves several purposes. It can yield insights
into the optimal carrying capacity of a pasture, and assess whether it has been overgrazed or
degraded (including degradation severity). Micro-scale grassland AGB is commonly estimated using
low-altitude UAV images. These fine spatial (about 1 cm) and temporal-resolution data areobtainable at a low cost, and their recording can be easily synchronized with in situ sampling (see
Section 1.2.3.4 for details). At a flying altitude of around 40 m, one frame image covers a ground
area <1 km². So drone images are suitable for only detailed micro-scale studies of ecological
impacts of external distances on ecosystem health rather than generating field views of AGB for
determining the proper stocking rate. The added advantage of using drone images is the redundancy
of radiometric correction for the atmospheric effects that can be safely ignored as drone images
acquired at a distance of 10s of meters from the target suffer minimal and negligible atmospheric
interference.
At the micro-scale, environmental variables (such as topography and climate) in the area of study
can be approximated as constant and securely disregarded. Thus, AGB is commonly estimated from
VIs calculated from NIR and red bands. VIs can effectively capture vegetation conditions and are
thus good at monitoring grassland biomass. So far, a large number of VIs has been developed from
drone photos, such as Green Leaf Index and soil-adjusted VI (Table 1.1). All the VIs useful for
quantifying LAI can be applied to AGB. Nevertheless, some of them cannot be derived from drone
images due to the lack of critical infrared bands. Visible band VIs from drone photos include
RGBVI (VIRGB), and green red vegetation index and green leaf index (Table 1.1). Their regression
against the in situ collected AGB establishes the estimation model that can be linear or non-linear
(e.g., polynomial) involving one or two variables.
Different indices have different powers in quantifying grassland AGB. As shown in Table 6.25,
RGBVI is the best predictor of grassland AGB, followed by GLI. RGBVI has the highest R² value
(0.797), and the lowest prediction nRMSE of 29.6 g ∙ m-2. The linear model of GLI also bears a
close relationship with the in situ sampled AGB, and its R² value is almost identical to that of the
RGBVI-based models but higher than others, reaching R² = 0.795 (Table 6.25). Model format (linear
or polynomial) does not make any apparent difference to model accuracy. The application of the best
model to the original photo produces a spatial distribution map of AGB (Figure 6.19).FIGURE 6.19 Micro-scale grassland AGB distribution quantified from drone-captured photos. Cyan
rectangles: sampling plots from which aboveground herbaceous biomass is harvested and dried to
serve as the in situ samples.
Table 6.25
AGB estimation models and validation statistics
Input VI Model type Model form R2 RMSE (g × m-2)
GRedVI
Linear y = 2952.6x – 2789.8 0.616 40.6
Polynomial y = –14498x2 + 31963x – 17297 0.622 40.3
GreenRVI
Linear y = 591.05x + 163.81 0.621 40.3
Polynomial y = -496.45x2 + 588.89x + 167.98 0.625 40.2
MGRVI
Linear y = 295.69x + 163.8 0.621 40.4
Polynomial y = -124.44x2 + 294.63x + 167.97 0.625 40.2
RGBVI
Linear y = 493.56x + 89.128 0.797 29.6
Polynomial y = 12.667x2 + 490.03x + 89.176 0.795 29.7
WI
Linear y = 10.285x + 157.99 0.143 60.7
Polynomial y = 1.0778x2 + 7.4418x + 152.42 0.171 59.7
IKAW Linear y = 937.77x + 64.698 0.536 44.7Input VI Model type Model form R2 RMSE (g × m-2)
Polynomial y = –3209.4x2 + 1723x + 25.218 0.563 43.3
GLI
Linear y = 802.61x + 89.527 0.795 29.7
Polynomial y = 86.01x2 + 788.08x + 89.626 0.794 29.8
VARI
Linear y = 389.81x + 165.95 0.622 40.3
Polynomial y = –166.04x2 + 383.84x + 169.24 0.622 40.3
Source: Shi (2022).
Note: Boldface: best model.
In spite of the close correlation of AGB with RGB-based VIs judged by the R² value, the quality of
estimation also needs to be evaluated in terms of residuals of prediction, as revealed by the
scatterplot of the estimated values versus the in situ observed ones (Figure 6.20). It shows
underestimation of high AGB values but overestimation of low AGB values because nadir-viewing
drone photos are insensitive to the vertical structure of grasses and their AGB. This deficiency can
be remedied by taking into account the biomass profile of grasses through adjusting the estimated
AGB based on the grazing intensity of grassland on the understanding that heavily grazed grassland
tends to contain shorter grasses that have a lower stature and thus less AGB. This calibration is
termed the F factor, calculated as:
FIGURE 6.20 Estimation accuracy of pike-disturbed grassland AGB after correction for grazing
(mowing) intensity and bare patch metric. MF = mowing factor; MFB = modification for bareground.
(Shi et al., 2021).
(6.45)
where B = mean AGB measured within the reference sample quadrats free of external disturbance;
bi = AGB measured in a given quarter of the ith sampling quadrat subject to grazing; and n =
number of sampled quadrats. The F factor is used to derive the profile-adjusted AGB as:
(6.46)
where m = total number of pixels in a sampling quarter within the quadrat (see Figure 1.3c); VIj =
vegetation index value of pixel j; k = (mowing-simulated) grazing intensity; l = grazing-intensity
adjuster determined via non-linear regression; (1–Fk)
l = grazing-intensity reversal factor, which can6.7.2
avoid division by zero as the reference quadrats of no grazing are assigned a mowing intensity of 0.
After the adjustment, the model R2 value rises markedly from 0.44 to 0.80 (Figure 6.20).
The overestimation of low AGB is explained by the spatial discontinuity of grass cover caused by
external disturbers such as plateau pika. They are responsible for the fragmentation of the grassland
and the advent of patchy grass coverage in heavily degraded grassland where the vegetation has
been denuded by pika burrowing. The indiscriminate application of the best estimation model in
Table 6.25 to all pixels in the input photo translates their VIs to AGB values regardless of whether
they actually represent bare ground of no biomass or not (Figure 6.20). This phenomenon is termed
“pseudo-quantification” caused by the fact that all gaps on the ground of non-vegetative covers still
have non-zero pixel values reflected off the bare surface. They cause the derived VIs to be non-zero,
as well. One way of remedying pseudo-quantification is to arbitrarily assign a value of 0 biomass to
all those pixels whose VI falls below a certain threshold as they likely represent bare patches of no
vegetation, instead of converting their VIs to AGB via the estimation model. This further
modification by the bare ground metrics increases the estimation accuracy only minimally from 0.88
to 0.90 (Figure 6.20) because bare grassland patches are highly limited in their spatial extent in the
study area. The pace of improvement will be much higher if there are more widespread bare
grassland patches on the ground. In spite of the rather high accuracy achieved after the two
adjustments, drone-based quantification is confined to small areas, not suitable for broader scale
quantification.
Catchment-scale Grassland Biomass
Catchment is a hydrological unit defined by the spatial movement and convergence of rainwater. Its
size varies with the order of river channels. Dissimilar to the micro-scale AGB covered in the
preceding section, catchment-scale grassland AGB is ideally positioned for studying the impact of
the heterogeneous environment on grassland productivity and its propensity to degradation in light
of external disturbances. At such a scale, there are more variations in topography, micro-climate, and
solar radiation caused by topographic heterogeneity. Consequently, the vegetation cover is likely to
be spatially more diverse than on the micro-scale, and can be further differentiated into specific
types to increase the reliability of quantification as different types of vegetation have their own
biomass density, and hence the total biomass in a region.
AGB quantification at the catchment scale is commonly achieved using much coarser medium￾resolution EO imagery data such as Landsat OLI and Sentinel-2 (Table 6.26). Landsat series of data
with the longest history of existence are an excellent source of data for studying long-term biomass
change associated with natural growth, deforestation, and climate change. In addition, Sentinel-2
data of a finer spatial resolution (10 and 20 m) expand the data pool, but they have a shorter history
of existence that cannot extend the period of study to decades. Their short revisit period of five days
at the equator may not prove much valuable in quantifying biomass but will prove valuable in
quantifying forest burn intensity (refer to Section 6.8.3). Finally, SPOT 6/7 and HJ-1 are also useful
data for this purpose. The former has a fine resolution, but is expensive and covers a limited ground
per scene. HJ-1 and Gaofen data are not widely available outside China.
Table 6.26Medium-resolution images ideal for quantifying regional above-ground biomass and carbon. Landsat 8 sensor: OLI
(Operational Land Imager); Landsat 5-7: TM (thematic mapper) and ETM+ (enhanced TM plus) sensors
Satellite Life span
Spatial resolution (m) Swath
width (km)
Revisit
period (day)
Spectral
Pan Multispectral bands
SPOT 5 2002-2015
2.5,
5 10 60  2-3
NIR, SWIR,
R, G, B, P
ASTER 1999-2008 15, 30, 90 60 16 VNIR, TIR
Landsat
8/9** 2013-present 15 30 185 16
NIR, TIR, R,
G, B, P
Sentinel
2A/B
2015/2016-
present 10 10 290  5 R,G,B, NIR,P
HJ-2A/B 2020-present 16 (48*) 200 Up to 2
R,G,B, NIR,
P, plus more
Formosat –
2 2002-2016 2 8 24  1
NIR, R,G, B,
P
Source: Gao (2023).
Notes:
* Hyperspectral bands
** Landsat 8/9 carries the OLI (Operational Land Imager) sensor while its predecessors carried the TM (thematic mapper) and ETM+
(enhanced TM plus) sensors.
R = red, G = green, B = blue, P = panchromatic.
At the catchment scale, more variables influence grassland AGB, so their consideration can
potentially improve the quantification accuracy. These variables fall into a few categories, such as
geospatial variables (GV), that include latitude, longitude, elevation, aspect, slope, and NDVI;
Vegetation type (VT): grass and shrub classified from medium-resolution satellite images using
various machine learning methods; in situ measurements (GM): vegetation height (H), shrub
coverage (SC), and soil moisture (Soil M); meteorological variables (MV): 30-days mean
temperature (T) and precipitation (P); and Observation time (OT). The consideration of so many
variables is ideally handled using machine learning algorithms that are able to establish non-linear
relationships between AGB and environmental variables. As an alternative to traditional models,
machine learning models, such as single-layer ANN, SVM, and RF, are flexible and do not require
the knowledge of internal factors but can still provide easy solutions for non-linear functions. All
machine learning algorithms need to be trained using in situ collected data. The better they are
trained, the more reliably they can predict AGB.
The entire process of quantification can be fully automated via scripting, with as many variables as
possible to be included in the input initially and the computer just determines their importance and
makes predictions from only a set of the most important variables. However, this is much easier said
than done as the importance of some considered variables varies with the temporal duration of AGB
aggregation (Figure 6.21). In predicting monthly AGB, temperature is the most important (0.384),
followed by elevation (0.298) in the frigid plateau environment of over 4000 m a.s.l. in West China
(Table 6.27). Slope and northness are less important with an importance value of 0.168 and 0.115,
respectively. The importance of the remaining variables lies below 0.1, and precipitation has the
lowest value of 0.045. However, the ranked order of variable importance changes in predictingannual AGB. Elevation has the highest importance score of 0.303, followed by northness of 0.230.
Slope retains the third highest score of 0.171. The other variables have a lower importance score less
than 0.11, including temperature and precipitation.
FIGURE 6.21 Comparison of the accuracy of four analytical methods in estimating catchment-scale
grassland AGB from medium resolution Sentinel-2 data based on various combinations of input
variables, rendered as heat maps. (Shi et al., 2023.)
Table 6.27
Importance of various variables in predicting monthly and annual AGB of an alpine grassland
Monthly AGB Annual AGB
Variable Importance Rank Variable Importance Rank
Temperature 0.384 1 Elevation 0.303 1
Elevation 0.298 2 Northness 0.230 2
Slope 0.168 3 Slope 0.171 3
Northness 0.115 4 Longitude 0.103 4
Longitude 0.099 5 Latitude 0.065 5
Latitude 0.095 6 Temperature 0.046 6
Precipitation 0.045 7 Precipitation 0.044 7
Source: Shi et al. (2023), open access.
In practice, accurate estimation of grassland AGB is achieved from multiple variables not just
individual ones. Not all the important variables can form the best set of variables for estimation. Of
all variable combinations, the GV-GM feature combination achieves the best accuracy across the
four model types (machine learning regression or MLR, SVM, ANN, and DNN) and all nine
combinations with an R2 over 0.815. Of the four model types, the MLR ones have the second￾highest R² of 0.583 with the combined GV-VT-OT features, followed by GV-VT-MV (R2 = 0.530)
and GV-VT (R2 = 0.513) features. For the SVM models, the GV-MV-OT combination is the second6.7.3
most accurate (R² = 0.744), followed by the GV-VT-MV-OT combination (R2 = 0.718). The ANN
models slightly outperform the SVM models regardless of feature combinations. The ANN model
with GV-VT-MV-OT variables has the second-best performance (R2 = 0.782) in AGB estimation,
followed by GV-VT-MV features (R2 = 0.775). The effectiveness of feature combinations does not
change for the DNN models, but the accuracy is marginally improved over the ANN models
irrespective of feature combinations. Among them, GV-VT-MV-OT yields the highest accuracy of
0.818. Therefore, DNN is the best for predicting AGB. The application of this model to the relevant
imagery and auxiliary data results in a map showing the spatial distribution of ABG in a catchment
(Figure 6.22). If the catchment size is large, multiple images may need to be mosaicked to cover it
fully. Initially, all areas contaminated by clouds are treated as having no AGB information or a
missing value. These gaps can be filled via spatial interpolation.
FIGURE 6.22 Catchment-level distribution of grassland AGB incorporating both grass and shrub
biomass, derived from mosaicked Sentinel-2, Gaofen-1 and PlanetScope images after the cloud￾blocked gaps are filled using spatial interpolation. (Shi et al., 2023, open access.)
Meso-scale Forest Biomass (Carbon)
Meso-scale refers to a sizeable administrative area, such as a local municipal territory or regional
district. Regional biomass (and carbon) estimation has become increasingly important in global
carbon trading schemes. Meso-scale biomass (carbon) can be quantified for forest using the same
method as described previously for grassland except that trees are more diverse in their species and
wood (carbon) density, so they need to be differentiated into broad types such as trees, shrubs, and
even grasses for precise quantification, although grasses have a fraction of tree biomass, and can be
ignored with impunity in most cases of quantification when mixed types of vegetation co-exist (e.g.,6.7.3.1
trees with shrubs and grasses). At the meso-scale, more factors affect AGB and its spatial
distribution, such as tree species and age than at the micro-scale, so is more complex. This
quantification is commonly fulfilled from medium-resolution imagery and LiDAR data, either solely
or jointly.
From Imagery Data
Forest AGC is still quantified via AGB with the use of a conversion ratio of 0.5 or 0.45, depending
on wood density. The principle and method of estimation from imagery data remain identical to
those of grassland AGB using drone images except three differences: (i) The images used are space￾borne with more spectral bands available. Several sources of freely available data can be used, and
they have unique spectral and spatial resolutions that may be complementary to each other. Space￾borne images with more spectral bands than drone images offer more choices in selecting the best
bands in calculating VIs from diverse sensors, allow more spectral indices to be derived, and thereby
increase the potential number of predictor variables in the estimation; (ii) At broad scales many
more environmental factors affect AGB, including topography, soil, and even micro-climate. They
can be considered as the co-variates of AGC. Accurate quantification is not possible unless they are
treated as auxiliary variables, together with image-derived variables; and (iii) The measurement of in
situ AGB must be based on non-destructive methods of sampling. Instead of chopping down trees
and weighting them to determine in situ AGB, it is estimated from the measured tree biophysical
parameters, such as height and DBH. The measured parameters are converted to biomass using
allometric equations.
Many allometric equations have been constructed for various forests and tree species growing in
diverse environments. Different allometric equations are needed to calculate the AGB of different
tree species of a unique wood density co-existing in the same forest at broad scales. If no suitable
equations are in existence, they have to be constructed from scratch from the collected field data. In
the temperature climate of New Zealand, five equations have been established for trees of different
stature growing in various environments (Table 6.28). Caution needs to be exercised in selecting the
appropriate allometric equations as they have their own suitable ranges of DBH (and CH) and
applicability.
Table 6.28
Main allometric models for calculating AGC of major types of vegetation in New Zealand
Authors Forest Site Vegetation
Suitable
range AGC Model
Coomes et al.
(2002)
Pylon Gully and
Maimai in the
South Island
Indigenous
trees and
shrubs
DBH:
> 2.5
cm
0.5(0.0000598ρ(DBH2 ×
CH)0.946 × (1–0.0019DBH) +
0.03DBH2.33+0.0406DBH1.53)
Beets et al.
(2012)
Whirinaki,
Kaimanawa, Mt.
Maungatautari,
Taranaki,
Hunua, Maimai
Indigenous
trees
DBH:
0.7 ~
142
cm
Cs_b≥10 + C b<10 + Cfoliage
Cs_b≥10 = 1.62× 10–2 × (CH ×
DBH2)0.943
C b<10 = 1.75 × 10–2 ×
(DBH)2.2Authors Forest Site Vegetation
Suitable
range AGC Model
and
Whakarewarewa
forests
CH:
2.4 ~
59 m
Cfoliage = 1.71×10–2 ×
(DBH)1.75
Mason et al.
(2014)
South Island
and D’Urville
Island
Native
shrubs and
some
small trees
DBH:
> 0.5
cm
CH: >
30 cm
0.5exp[1.0215ln(π × CH ×
DBH2/40000) + 6.1512]
Schwendenmann
and Mitchell
(2014)
An urban park
in Auckland
Native
trees
DBH:
11~20
cm
CH:
6~12
m
2.3 × 10–3 × DBH3.3885+1.21
× 10–2 × DBH2.5276
Beets et al.
(2012)
Mt.
Maungatautari,
Maimai and
Whakarewarewa
forests Tree ferns
DBH:
10.4 ~
32.7
cm
CH:
1.5 ~
10.2
m 2.7 × 10–3 (CH × DBH2)1.19
Note: CH = canopy height; Cs_b≥10 = carbon content in stems and branches ≥ 10 cm; Cb<10 = carbon content in branches ≤ 10 cm;
Cfoliage = carbon content in foliage; ρ = wood density with a value of 476 kg ∙ m–3, 333 kg ∙ m–3, and 197 kg ∙ m–3, respectively, for
trees, shrubs, and tree ferns/cabbage trees/palm trees (Marburg et al., 2013). For biomass-based allometric models, a factor of 0.5 is
used to convert dry biomass into AGC.
Source: Wang (2019).
Which equation to use is dictated primarily by tree size, such as DBH or CH thresholds, and
secondarily by species or wood density. If possible, the allometric equations developed for the same
climate zone should be adopted to minimize uncertainty of the calculated AGB. After the stand-level
AGB has been calculated using the appropriate equations, the calculated AGB is aggregated to the
plot level for all eligible trees (e.g., DBH above the minimum measurable threshold, usually 2.5
cm), and linked to the image-transformed VI layers or LiDAR-derived height metrics via their
location (e.g., plot centroid coordinates). Commonly used VIs in estimating forest AGB include
NDVI, SAVI, RVI, GNDVI, and DVI. And the two sets of data are analyzed to identify their
relationship using the methods presented in Section 4.2.
Image-based estimation models are constructed commonly using simple ordinary least square
regression, either univariate or multivariate. As with drone images, various regression models have
been established, including linear, quadratic, and exponential, either univariate or multivariate as:
(6.47)(6.48)
(6.49)
(6.50)
where xi = ith independent variable of trees, either spectral or structural metrics; = regression
equation coefficient of xi, and = error term.
The performance of a model depends on vegetation type and the number of explanatory variables
considered. As illustrated in Table 6.29, the two most reliable predictors of forest AGB are
GNDVI_P80 and SAVI_P80 (80th percentile), both achieving an identical R² value of 0.71. This
value highly resembles the adjusted R² of ⁓0.76 obtained by Wang et al. (2013) for broadleaf and
conifer forest based on quadratic models of a single variable (DVI or even Landsat TM band 4). For
shrubs, the two best predictors are Green_Max and Red_Max, attaining the same R² value of 0.53.
The use of two best predictors brings only slight improvement to the estimation accuracy over the
best univariate model, probably because the variation in forest AGB that is explained by the best
predictor is also explained by the second-best predictor. Compared with the best predictor variables,
model format exerts little influence on model accuracy, especially for shrubs. Since liner models
involve less computation, they should be used.
Table 6.29
Performance of regression models in quantifying plant AGC for two types of vegetation
Vegetation
type
Model
form Independent variables
Model performance
R2
RE
(%)
RMSE (Mg × ha–
1)
Forest
ULM GNDVI_P80 0.51 39.87 54.03
BLM GNDVI_P80, RVI_P80 0.51 41.77 56.60
UPM GNDVI_P80 0.46 41.22 55.86
BPM GNDVI_P80, SAVI_P80 0.48 42.65 57.80
UQLM GNVI_P80 0.51 44.88 60.81
BQLM
GNDVI_P80,
RVI_P80 0.71 42.64 57.79
BQRM GNDVI_P80, SAVI_P80 0.71 49.02 66.43
Shrub ULM Red_Max 0.46 35.77 11.58
BLM Red_Max, Green_Max 0.47 38.02 12.30
UPM Green_Max 0.50 39.84 12.89
BPM Green_Max, Red_Max 0.54 38.93 12.60
UQLM Red_Max 0.48 35.75 11.57
UQRM Green_Max 0.51 36.81 11.91
BQLM Red_Max, Green_Max 0.53 47.06 15.23Vegetation
type
Model
form Independent variables
Model performance
R2
RE
(%)
RMSE (Mg × ha–
1)
BQRM Green_Max, Red_Max 0.53 47.06 15.23
Source: Wang et al. (2020), © Taylor & Francis.
Note: Boldface: best model.
The best estimation models for individual vegetation types from a sole spectral variable are
expressed as (Wang and Gao, 2019):
(6.51)
(6.52)
where GNDVIp80 = 80 percentile of green band NDVI; Greenmax = maximum pixel value of the
green band. The application of these models to the respective type of vegetation produces two AGC
distribution maps, and their merging forms the final AGC distribution of the whole study area
(Figure 6.23). The accuracy of this method depends on the reliability of mapping the vegetation
types from the spectral data.FIGURE 6.23 Spatial distribution of vegetation AGC density in Northern-Western Auckland estimated
from spectral parameters derived from a 2014 Landsat 8 image at 30 m resolution (white patches:
urban residential buildings). (Wang et al., 2020, © Taylor & Francis.)
Apart from linear regression analysis, forest AGB at a broader regional scale can also be estimated
using machine learning methods, including stepwise regression (SR), SVR, and RF. The RF AGB
model has an acceptable modeling accuracy (R2 = 0.95, RMSE = 17.73 Mg ∙ ha–1) from Landsat TM
data, even though its validation accuracy is much lower (R2 = 0.71, RMSE = 39.60 Mg ∙ ha–1) (Liu
et al., 2017). RF is the best performer, attaining the maximal R2 and the minimal RMSE in modeling
accuracy (R2max = 0.96, RMSEmin = 17.73 Mg ∙ ha–1) and cross-validation (R2max = 0.76, RMSEmin6.7.3.2
= 39.60 Mg ∙ ha–1) (Table 6.30). SR is the worst performer among the three methods. The best
performing algorithms achieve an accuracy that is just merely comparable to that of linear regression
models reported by Wang et al. (2013) and Wang et al. (2020). This raises the questionable value of
complex machine learning methods in the estimation. More accurate retrieval may have to rely on
LiDAR data.
Table 6.30
Comparison of three types of analytical means in estimating regional forest AGB based on imagery and LiDAR data either
solely or jointly
Method Input
Model accuracy Cross-validation
R² RMSE (Mg × ha–1) R² RMSE (Mg × ha–1)
Stepwise regression
Imagery 0.38 61.17 0.36 65.42
LiDAR 0.78 36.31 0.49 59.18
Integrated 0.82 33.93 0.67 55.78
Support vector regression
Imagery 0.68 48.67 0.50 56.80
LiDAR 0.83 36.20 0.62 48.76
Integrated 0.85 33.98 0.70 44.48
Random forest
Imagery 0.96 17.73 0.71 39.60
LiDAR 0.96 17.87 0.72 39.83
Integrated 0.96 23.07 0.76 40.07
Source: Liu et al. (2017), open access.
From LiDAR Data
LiDAR data are virtually a large collection of dense points distributed over a canopy, some of which
may be bounced from the ground if the canopy contains gaps or is not dense enough to prevent the
penetration by LiDAR pulses. LiDAR data can be acquired airborne or space-borne. Similar to aerial
photographing, ALS is flexible. The acquired data have a density between 0.4 and ⁓12 points ∙ m-2,
depending on the flying height and the scanner capability. Space-borne LiDAR data are available
from the Geospatial Laser Altimeter System (GLAS) scanner onboard the NASA Ice, Cloud, and
Elevation Satellite (ICESat). This first space-borne full-waveform LiDAR sensor transmits pulses in
1064-nn channels, generating a ground footprint of approximately 65 m in diameter. The returned
pulses are recorded in multiple forms. GLA01 contains the transmitted and received waveforms,
while GLA14 produced from GLA01 also contains the geolocation information of the footprint. The
raw LiDAR data comprising a large collection of range and orientation information are transformed
to 3D coordinates of points based on the scanning angle.
Table 6.31
LiDAR-derived structural metrics that are treated as the candidate variables in predicting meso-scale AGC
Abbreviation Meaning Abbreviation Meaning
TRC Total return number HMAD Median of height absolute deviationAbbreviation Meaning Abbreviation Meaning
HMIN Minimum of height MHADME
Median of absolute deviations from the
overall median
HMEAN Mean of height MHADMO
Median of absolute deviations from the
overall mode
HMAX Maximum of height HIR Height interquartile range
HMODE Mode of height RAME
Number of returns above the height
median
HSTD
Standard deviation of
height RAMA
Number of returns above the height
mean
HCV
Coefficient variation of
height
HCV =
HSTD/HMEAN RAMO
Number of returns above the height
mode
HSK Skewness of height PAMA
Percentage of returns above the height
mean
HKur Kurtosis of height PAMO
Percentage of returns above the height
mode
HAAD
Average of height
absolute deviation Pn
Upper nth percentiles, including 1st,
5th~95th (with 5 intervals) and 99th
percentiles
Source: Wang (2019).
Because of the random nature of laser scanning, LiDAR beams rarely strike the tree top, causing
LiDAR points scarcely coincident with it or representative of the treetop height. Naturally, the tree
height derived from the LiDAR-generated CHM seldom corresponds to the height of individual
trees. So LiDAR-based AGB estimation must rely on structural parameters and their statistical
variants, such as percentile. Such metrics apply to the plot level at which the LiDAR points are
amalgamated over the sampling plot generated through statistical analysis. The produced height
metrics may include mean, median, mode, minimum, and maximum (Table 6.31). These height
parameters can be based on tree-top height (after correction), quartile height, and decimal height
calculated by subtracting the ground elevation of 25% or 10% of the returned waveforms, and
quadratic mean canopy height calculated from the canopy height profile. Apart from height metrics,
more metrics can be produced from LiDAR signal intensity, such as canopy cover defined as the
ratio of the canopy echo area to the total wave area (Liu et al., 2017).
Once LiDAR metrics are derived, the estimation of AGB from them follows the same analytical
procedure as imagery-based estimation in which the explanatory variables are analyzed either
statistically or using machine learning. These analyses can also reveal the importance of all the
considered variables. For instance, HSD (standard deviation of height) and Hmean are the two best
predictors of forest AGB while Hmode and HP35 (35% height percentile) are the best for predicting
shrubland AGB (Table 6.32). In other words, the bivariate linear model involving the standard
deviation of height (HSD) and the mode of median absolute deviation (MADMO) is the mostaccurate. These variables obtained from remnant forest parks in an urban environment of a
temperature climate are slightly different from the wild Moscow Mountain area in north central
Idaho of the U.S. where the forest is temperate mixed-conifer with the dominant species being
ponderosa pine, Douglas fir, grand fir, western red cedar, and western larch (Hudak et al. 2012). So
Hmean is much more important than other metrics.
Table 6.32
Accuracy of estimating forest and shrubland AGB from LiDAR-derived height metrics based on various regression models
Vegetation Type Model Form Independent Variables
Model Performance
RMSE (Mgha–1) RE (%) R2
Forest
ULM HSD 47.75 30.98 0.83
BLM HSD, HMADMO 34.93 22.67 0.91
UPM HSD 46.84 30.39 0.86
BPM HSD, HM 50.26 32.61 0.86
UQLM HSD 53.46 34.69 0.84
BQLM HSD, HMADMO 55.23 35.84 0.93
BQRM HSD, HM 66.00 42.83 0.89
Shrubland
ULM HMO 9.31 28.77 0.64
BLM HMO, HP35 9.63 29.75 0.67
UPM HMO 10.43 32.22 0.57
BPM HMO, HP30 10.03 31.00 0.61
UQLM HMO 12.34 38.14 0.64
BQLM HMO, HP35 10.80 33.36 0.69
BQRM HMO, HP30 11.34 35.04 0.69
Source: Wang (2019).
Note: Boldface: best model.
The best LiDAR-based AGC models have lower relative errors. Hence, the AGC of forest and
shrubland based on LiDAR-derived structural features (Eq. 6.53) is applied to forests. The univariate
linear model (Eq. 6.54) is applied to shrubland. Application of these models yield two AGB
distribution maps of the same area, and their merging into one map shows the spatial distribution of
AGB of the entire study area at a wide range of density (Figure 6.24).6.7.3.3
FIGURE 6.24 Land cover and AGC maps in northwestern Auckland. (a) Distribution of three types of
vegetative covers mapped from Landsat 8 images in combination with LiDAR-derived DSM; (b)
Distribution of regional AGC density from LiDAR data. (Wang et al., 2020, © Taylor & Francis.)
(6.53)
(6.54)
Imagery or LiDAR Data?Imagery- and LiDAR-based estimation of AGB has different philosophies. The former detects the
spectral reflectance off the forest canopy with little penetration of it to reach the ground level,
especially when the forest is dense and full of mature trees. Only spectral indices can be produced
from images for the quantification. LiDAR detects the position of the returned pulses that can reveal
the 3D structure of the canopy. As indicated in Table 6.33, the estimation models constructed from
LiDAR data are, on average, 40% more accurate than their counterparts from spectral features. This
difference in model accuracy can be traced to the effectiveness of spectral and structural parameters
of vegetation in the estimation.
Table 6.33
Comparison of spectral variables from imagery and structural variables from LiDAR data in estimating plot-level AGB for
two types of vegetation based on three types of models
Plot
type Model Spectral variable
Accuracy
Structural
variable
Accuracy
R² RMSE R²
RMSE
(%)
Trees
Linear GNDVI_P80 0.51
54.0
(39.9%) HSD 0.83
47.8
(31.0%)
Linear
GNDVI_P80,
SAVI_P80 0.53
56.7
(41.8%)
HSD,
Hmean 0.84
52.2
(33.9%)
Power GNDVI_P80 0.46
55.9
(41.2%) HSD 0.86
46.8
(30.4%)
Power
GNDVI_P80,
SAVI_P80 0.48
57.8
(42.7%)
HSD,
Hmean 0.86
50.3
(32.6%)
Quadratic GNDVI_P80 0.51
60.8
(44.9%) HSD 0.84
53.5
(34.7%)
Quadratic
GNDVI_P80,
SAVI_P80 0.71
66.4
(49.0%)
HSD,
HM 0.89
66.0
(42.8%)
Shrubs
Linear Green_Max 0.44
12.0
(37.2%) Hmode 0.64
9.3
(28.8%)
Linear
Green_Max,
Red_Max 0.47
12.3
(38.0%)
Hmode,
HP30 0.67
9.5
(29.4%)
Power Green_Max 0.50
12.9
(39.8%) Hmode 0.57
10.4
(32.2%)
Power
Green_Max,
Red_Max 0.54
12.6
(38.9%)
Hmode,
HP30 0.61
10.0
(31.0%)
Quadratic Green_Max 0.51
11.9
(36.8%) Hmode 0.64
12.3
(38.1%)
Quadratic
Green_Max,
Red_Max 0.53
15.2
(47.1%)
Hmode,
HP30 0.69
11.3
(35.0%)
Source: Modified from Wang and Gao (2019), with permission (240409-015129) from Elsevier.
*: HSD = standard deviation of tree height. Hmean = mean height, Hmode = mode of height; HP30 = height at 30 percentile.The importance of spectral and structural parameters in estimating plot-level AGC is compared in
Table 6.33 using the same types of model involving either the best or the two most useful variables.
Structural variables enable vegetation AGC to be estimated more reliably than spectral parameters,
especially for vegetation with a large height such as trees. On average, structural parameter-based
models are 23.8% more accurate for tree plots, and 21.7% more accurate for shrub plots than those
based on spectral parameters of vegetation (Table 6.33). Such a superiority of structural features
over spectral parameters is attributed to the ability of LiDAR point clouds to capture the vertical
variation of plant height and structure. The strong penetration of the canopy by laser pulses enables
the 3D distribution of vegetation biomass to be taken into consideration in the estimation. Of the two
types of vegetation, tree AGC (R2 = 0.86, RE = 30.4) is more accurately estimated than shrub AGC
(R2 = 0.64, RE = 28.8) because trees are much taller than shrubs. Hence, their height can be more
accurately estimated from the LiDAR data. In relative terms, the accuracy of height determination
for trees is higher than that of shrubs. Furthermore, HSD is the structural parameter that bears the
highest ranked correlation with plot-level AGC of trees. It is different from HMODE that bears the
highest ranked correlation with the plot-level AGC of shrubs. Such a difference is attributed to the
narrower height variation of shrubs than trees. Besides, shrub biomass depends largely upon the
number and volume of branches while tree biomass is concentrated heavily in its trunk. Therefore,
tree AGC is highly related to the height level at which the largest number of LiDAR points strike the
canopy.
In contrast, spectral parameters can capture only the vegetation canopy without detailed
information on its structure because the VNIR radiation used in sensing is unable to penetrate the
canopy. Viewed from the above, the canopy of trees and shrubs highly resembles each other, causing
the two spectral parameter-based models (Table 6.32, boldfaced rows) to have an identical R2 value
of 0.51 and a RE around 38. Overall, the substantially more accurate AGC estimate quantified from
structural parameters than its counterpart from spectral parameters demonstrates that these
parameters are preferable to image-derived spectral parameters in developing AGC estimation
models as they are robust statistics of LiDAR point heights of each plot, so potential errors in the
LiDAR aboveground points only exert a minute effect on model performance.
Although the forest AGC stocks in 2013 (979,118 Mg C) and 2014 (887,556 Mg C) (Table 6.34)
is calculated from the same vegetation map (Figure 6.23), they still differ from each other by 91,562
Mg C (Wang et al., 2019). Given the short interval between the 2013 LiDAR and the 2014 Landsat
imagery, this discrepancy cannot be accounted for solely by natural growth. The lower estimate from
Landsat 8 is attributed partly to the incapability of the 2D spectral features of imagery to capture the
vertical distribution of biomass (e.g., tree height). Thus, the limited penetration of the canopy by the
radiative energy used to reach the sub-canopy level means that understorey vegetation that also
contains AGC is not well-represented in the imagery signal, leading to a lower estimate of forest
AGC stock by 9.4%. The discrepancy is much lower at 2.3% for shrub AGC, though, because of the
shorter stature of shrubs.
Table 6.34
Comparison of AGC (Mg) by vegetation type (forest, shrubland, and grassland) estimated from LiDAR and Landsat 8 OLI
data
Vegetation type6.8
6.8.1
Vegetation type
Data Source Forest Shrubland Grassland Total
LiDAR 1134287.69 207606.09 NA 1341893.78
Landsat-8 (30 m) 844304.60 261433.90 173050.30 1278788.80
Source: Wang (2019).
It is worthwhile to note that LiDAR data do not yield precise metrics of trees, only the statistical
ones like percentiles, and structural parameters from LiDAR are more capably of revealing the 3D
nature of a forest canopy and thus its carbon stock. Besides, the canopy itself composed of leaves
and branches contains much less carbon than the tree trunk. It seems that its carbon stock is better
represented by the relief of the canopy, making structural parameters far more critical than spectral
parameters in the estimation. LiDAR data are better posed for the accurate estimation of plant AGC
because of their ability to distinguish vegetation into specific, homogeneous groups whose AGC
stock can be more reliably estimated from vegetation-specific estimation models than those for
mixed types of vegetation based on its height. While this separation of plants by height into trees,
shrubs, and even grasses in a natural forest can be reliably fulfilled from the LiDAR data singly, it is
problematic in urban built-up areas full of high-rise buildings. Their height can be similar to and
easily mixed with that of trees, so they cannot be mapped based on height alone. An ideal solution is
to integrate them with supplementary data.
Since LiDAR and imagery data are complementary to each other in their capture of the vegetated
target, naturally, they have been integrated. In this integration, the LiDAR-derived height can further
separate the vegetation cover mapped from imagery into broad groups based on their height. For
instance, integrated WorldView-2 imagery with a 2 m DSM produced from LiDAR data enables
urban vegetation to be mapped into trees, shrubs, and grasses at an extremely high (≥95.9%)
producer’s accuracy (overall accuracy = 91.7%), which is not possible from imagery data alone. As
illustrated in Table 6.34, the integrated data are important to stepwise regression analysis, but not so
critical with machine learning methods. The results and accuracy from LiDAR data solely are almost
comparable to those from the integrated data. The total AGC from LiDAR and imagery data differs
from each other by less than 5%.
CROP YIELD ESTIMATION
General
Crop yield refers to the quantity of grain produced per unit area after harvesting and drying. The
prediction of crop yield at an early lead time supplies vital information for preparing for and
organizing the upcoming harvest labor, machinery, and post-harvest grain transport and storage. At
the regional or country level, the information also plays an important role in ensuring food security
and helping minimize the risk of famine. Within-field crop yield information is essential for
precision farming. Plot-level, pre-harvest estimates of crop yield are vital for assessing the
effectiveness of crop management practices, such as irrigation and the use of pesticides and
fertilizers. Crop yield is traditionally estimated via government statistics without any lead time. In
contrast, remote sensing allows crop yield to be predicted at critical stages over the crop growth
cycle at multiple lead times repeatedly. Remote sensing estimation of crop yield is mainly achieved6.8.2
by identifying characteristic spectral and biophysical parameters of crops and by establishing their
quantitative relationships with relevant yield metrics (e.g. LAI and biomass) either statistically or
via crop growth modeling.
Crop yield estimation from remotely sensed data faces a unique paradox in that the ground truth
data needed to validate the estimated results are not routinely available at the time of sensing,
especially over large areas. Crop yields become known only after the harvest, usually through
regional yield statistics. It may be possible to sample the yield in small experimental plots in the
field before harvest, but field sampling is destructive and may not provide a long lead time. For the
purposes of planning for harvest and avoidance of famine, the earlier the yield is predicted, the more
useful the predicted outcome. However, a prediction made at an early stage in the crop growth cycle
is less reliable. In fact, the closer to the harvest time, the more accurately the yield can be predicted.
Thus, in reporting the predicting accuracy using different remote sensing data and analytical
methods, it must be linked to the lead time of prediction. This section expounds the estimation of
yield for three main types of crops (corn, wheat, and rice) using both optical and radar images.
These crops are selected because they are widely cultivated globally and supply the majority of
human calorie needs.
Corn
Also known as maize, corn is the third most important plant-based food source in the world. Precise
estimation of corn yield requires fine-resolution EO images at the regional level, or large-scale, low￾flying drone images at the field level, either solo or integrated with crop surface models (CSM). The
use of satellite images enjoys the advantage of repeated coverage by multi-temporal images
throughout the crop cycle. Drone images’super-fine spatial resolution up to 0.02 m enables crop and
soil to be separated easily. Airborne images can be one-time off, or multi-temporal, acquired at some
critical growth stages, such as reproductive and maturing. They enable corn fields to be mapped at a
high accuracy. Image-based estimation relies on the combined spectral and spatial properties of
corn. Commonly used spectral properties are vegetation indices, such as Excess Green Index (ExG),
Vegetation Index Green (VIg), and Plant Pigment Ratio (PPRb) (Geipel et al., 2014), derived from
RGB bands containing the greenness information of corns in relation to their redness and/or
blueness (Table 6.35). VIg is virtually Normalized Green-Red Difference Index (NGRDI). ExG
accounts for combined greenness and redness, and green and blue reflection differences. PPRb
makes use of the normalized green and blue difference. In estimating corn yield, they play differing
roles. Some of them are designed to detect the crop state while others are effective at separating crop
from the soil background in mapping crop fields.
Table 6.35
Three simple indices used to estimate corn grain yield from drone images
Index Full name Formula Reference
ExG Excess Green Index Meyer et al. (1998)
VIg Vegetation Index Green Gitelson et al. (2002)
PPRb Plant Pigment Ratio Metternicht (2003)Source: Geipel et al. (2014), open access.
Apart from indices, close-range drone images can also be used to construct a CSM reminiscent of
CHMs of trees using the aforementioned method in Section 6.3.2.2. This 3D scene of the corn field
may be constructed from overlapping, stereoscopic images using the SfM method (see Section 4.4.1
for details), facilitated by the positioning and orientation information of the drone camera. The
produced CSM represents the geo-referenced surface of the corn field. Absolute crop heights are
calculated from the differences between the CSM and the bare ground DEM or the pre-sowing earth
height identically referenced to the same ellipsoid as the CSM. The in situ collected crop height
within the sampling plots that have been delineated from the model is analyzed statistically to derive
the plot-level height metrics, including mean height and standard deviation, and relate them to the
image-derived VIs.
Corn yield has been predicted using regression analysis and machine learning methods. In the
statistical method, plot-level corn grain yield is predicted either from crop height (H) individually or
jointly with crop coverage (C) linearly from images at different growth stages in the form of:
(6.55)
where = mean height and coverage of corn in the ith plot, determined using the vth VI and the tth
Ridler threshold estimate at the rth ground resolution and sth growth stage, respectively.
This method of estimation is simple and flexible as it allows the incorporation of as many
variables as necessary into the model and is computationally simple with an adequate performance.
RGB-based VIs enable corn grain yield to be quantified at an R2 value of up to 0.74 and RMSE
ranging from 0.67 (8.8%) to 1.28 t∙ha-1 (16.9%) (Geipel et al., 2014). The importance of image
spatial resolution in the prediction varies with growth stage. Due to row-based cultivation of corn
and the absence of canopy closure, VHR images are indispensable for accurate CSM construction
and separation of the field into two information classes of crop and soil at the early growth stages.
This separation is critical to accurate yield predictions. However, high-resolution images and
crop/soil separation become far less important as the crop grows to form a closed canopy. And finer
resolution images do not lead to more accurate yield estimates than coarser resolution images. In
fact, the finest resolution image is associated with significantly less accurate predictions at the mid￾season growth stages. All other imagery resolutions have an almost equally good performance with
an approximate R² range of 0.60–0.70. The exact form of VIs does not make much difference to the
prediction accuracy at any crop growth stage. The sole reliance on unclassified mean crop heights
produces less accurate predictions, so crop height information detected from the CSM is vitally
significant to accurate predictions.
Optical imagery is limited by its inability to function in cloudy conditions. So it is more
advantageous to integrate it with fine-resolution radar images, such as TerraSAR-X and Radarsat-2.
TerraSAR-X band (f = 9.65 GHz, λ = 3.1 cm) imagery is acquired in the HH polarization mode at an
incidence angle between 27.3 and 53.3°. The two beam modes of StripMap and SpotLight
correspond to a pixel size of approximately 3 and 1.5 m, respectively. Radar images lack
multispectral bands, but have multiple polarization modes. For instance, C-band (f = 5.405 GHz, λ =
5.5 cm) Radarsat-2 images acquired in the full quad-polarization mode have four polarizations of
HH, VV, HV, and VH. The derivation of NDVI-like indices from radar imagery is possible only withmulti-temporal data that have to be angle-normalized to account for the varying incidence angles
between multiple images from different sensors in the form of (Fieuzal et al., 2017):
(6.56)
where = ratio of the difference in backscattering coefficients between two successive angle-adjusted
radar acquisitions (∆ySAR) and the difference of their incidence angle ∆θ; NDVI in the exponent =
mean NDVI derived from optical images; a, b, c = empirical parameters determined via regression.
The integration of optical images with radar images offers many variables for the estimation,
including reflectance of individual bands (green, red and NIR), and backscattering coefficients
associated with different polarizations (, ,, and ). The incorporation of so many variables is best
handled by machine learning methods, such as multi-layer FFNN and BP NN. In its simplest form,
an NN is composed of one input layer, one hidden layer, and one output layer. The remote sensing
data constitute the input variables while the output nodes represent corn yield.
ANN-based estimation can be implemented in two modes: diagnostic based on all the satellite
data acquired throughout the growth cycle and real-time. In the real-time mode, corn yield estimates
are updated as each microwave and optical image is acquired (Fieuzal et al., 2017). The diagnostic
approach with more images available yields better estimates than the real-time approach, achieving
an R² of 0.77 and RMSE of 6.6 q ∙ ha−1 from optical data alone (Figure 6.25). If combined with
radar C-HH backscattering coefficients, the accuracy of prediction changes to R2 = 0.69 and RMSE
= 7.0 q∙ha−1 at the developmental stage of central stems. This accuracy is highly comparable to R² =
0.74 achieved from optical images solely based on VIs. However, it must be borne in mind that this
accuracy represents a compromise between prediction accuracy and lead time (more than three
months before the harvest). Predictably, the accuracy will rise if the real-time images are acquired at
a time closer to crop maturity.FIGURE 6.25 Field-level distribution of corn grain yields estimated from the combined use of optical
and VHR radar images using the diagnostic approach. (Fieuzal et al., 2017, with permission (240318-
001597) from Elsevier.)
Apart from simple NN, more complex 3‑dimensional CNN (3D‑CNN) has also found applications in
corn (and soybean) yield estimation. Based on a deep learning approach, it combines multi-target
regression, CNNs, and remote sensing data for crop yield prediction. This non-linear model employs
3D kernels in its convolution operations to capture spatiotemporal features in the input data. The
input to the network is 3D histograms and the 3D convolution operation is performed over the
“time”, “bin”, and “band” dimensions (Khaki et al., 2021). The 3D-CNN captures the temporal
effects of remote sensing data as well as spatial and intra-band features extracted from individual
images. It utilizes a novel deep learning framework to simultaneously predict the yield of corn and
soybean that uses transfer learning between yield predictions by sharing the weights of the backbone
feature extractor. This weight-sharing feature substantially reduces the number of model parameters
and hence improves computational efficiency. Subsequently, the network can be successfully trained
even with a limited sample size. The trained network is able to predict large-scale corn (and
soybean) yield from MODIS satellite images captured during the growing season (March–October).
MODIS multispectral images recorded 30 times a year enable corn yield to be estimated at a lead
time ranging from one to four months with an MAE of 8.74% of the average yield. 3D-CNN
outperforms most of other machine learning methods in estimating large-scale corn (and soybean)6.8.3
yields from coarse resolution MODIS data at a lead time of several months (Table 6.36), second
only to YieldNet.
Table 6.36
Comparison of RMSE (bushels per acre) in estimating large-scale corn yield at four lead times from MODIS data in the U.S.
corn belt using seven machine learning methods
Year Lead time Ridge Lasso RF DFFNN RT*
3D￾CNN YieldNet
2017
(n = 882)
July 30.55 27.53 26.61 26.40 33.64 22.50 20.88
August 25.16 22.27 22.25 20.85 28.02 16.60 17.74
September 24.15 21.50 21.99 19.21 26.80 15.71 15.53
October 25.73 20.94 22.14 18.90 26.78 15.69 15.40
2018 (n =
784)
July 27.51 21.21 22.38 22.85 27.69 20.64 22.08
August 24.50 19.46 21.52 21.14 29.34 18.81 18.25
September 25.10 18.69 21.70 20.57 28.91 17.58 16.89
October 32.50 19.20 22.28 21.63 28.90 17.72 16.75
Mean 25.91 21.10 22.22 21.33 28.83 17.71 17.49
Source: Khaki et al. (2021), open access.
Note: *: regression tree; DFFNN-deep feed-forward neural network. boldfaced: best model in the same month.
Wheat
Wheat is one of the most important cereal crops widely cultivated around the world, with a summed
cultivation area in excess of 200 million ha. Wheat supplies the calorie needs for more than one third
of the world population. A steady and reliable wheat yield is essential for preventing food shortage
and starvation. Wheat yield has been estimated from remotely sensed data based on VIs and LAI.
Commonly used VIs include EVI, Normalized Difference Moisture Index (NDWI), SR, red edge
position (REP) index, and Red Edge Normalized Difference Vegetation Index (RENDVI). They can
be derived from both multispectral and hyperspectral data. Hyperpectral data offer more than 100
spectral bands for deriving more VIs, such as SSI, RSI, and NDSI. These indices from a pair of
randomly selected bands are correlated with field-collected data and those bearing the highest
correlation coefficient with wheat LAI are considered the best band combination and used to predict
wheat yield. These hyperspectral indices can fully reflect the changes in biophysical and chemical
parameters of wheat crops at various growth stages and improve the accuracy of yield estimation.
It must be noted that not all the considered VIs are equally important to the prediction of wheat
yield. The importance of VIs in the ANN-based prediction can be determined using the Permutation
Feature Importance method. It provides a global insight into the behavior of the ANN yield
prediction model, and automatically takes into account all interactions among the considered
features. The method is based on the relationship between the features and the true results that have
been destroyed and that the model prediction error increases after the replacement of the feature
values. The importance of four features (SR, NDWI, EVI and REP) used in the prediction is ranked
as SR > NDWI > EVI > REP (Cheng et al., 2022). The shuffled vegetation index SR produces thelargest loss of 0.4783, followed by a loss of 0.4492 for NDWI, 0.2385 for EVI, and 0.2371 for the
REP. A larger loss value indicates a greater contribution to the results (Figure 6.26), and thus is more
important.
FIGURE 6.26 Field-level winter wheat yields predicted from Sentinel-2 multispectral data and ZY-1
02D hyperspectral data using (A) LSTM, (B) RF, (C) GBDT, and (D) SVR models. (Cheng et al.,
2022, open access.)
Wheat yield has also been predicted from a large number of potential variables, including individual
bands and red edge indices, apart from VIs. These features are linearly regressed against in situcollected wheat yields inside sampling plots, or analyzed using machine learning methods, such as
LSTM, DNN, CNN, and RNN. Of these models, the LSTM model is the best, achieving the highest
R² value of 0.93, followed by DNN that has a slightly inferior performance (R2 = 0.886) (Figure
6.27). The GBDT and SVR models are much less accurate, achieving a markedly lower R2 value of
0.839 and 0.573, respectively (Cheng et al., 2022). The two tree-based models (RF and GBDT) can
explain at least 10% more variations in yield than SVR because of its incapacity to map highly non￾linear and complex relationships between wheat yield and the considered variables.
FIGURE 6.27 Comparison of accuracy in estimating winter wheat yield using the (A) LSTM, (B) RF,
(C) GBDT, and (D) SVR models. (Cheng et al., 2022, open access.)
The third method of wheat yield estimation is to assimilate remotely sensed data into a crop growth
model, such as Decision Support System for Agrotechnology Transfer (DSSAT), WOrld FOod
STudies (WOFOST) simulation model, and simple algorithm for yield estimation (SAFY) model.
SAFY is a crop growth model based on light-energy utilization theory. This model estimates crop
yield from the harvest index (HI) and dry aerial mass (DAM) calculated on the basis of the
biophysical crop processes (e.g. biomass accumulation, leaf distribution, leaf senescence, and so on),
or:(6.57)
where DAMmax = maximum AGB, and HI = harvest index.
The three critical parameters needing parameterization in the SAFY crop growth model are
emergence date (D0), effective LUE (ELUE), and senescence temperature threshold (STT) that is
empirically determined to simplify crop growth modeling and broaden model applicability (Ma et
al., 2022). Yield is simulated from the daily dynamic changes of crop LAI during emergence￾maturity from daily radiation and mean temperature. In the SAFY model, the growth cycle is
partitioned into two general stages of active growth and senescence (or biological aging), partitioned
according to the accumulated temperature (SMT) after emergence, calculated as:
(6.58)
where D0 = crop emergence period; Ta = daily average temperature, Tmin = minimum temperature
for crop growth. A too high or too low ambient temperature slows down the accumulation of crop
biomass. Growth stops if the daily average temperature drops below Tmin or rises above Tmax.
During the growth period, crop biomass increases with photosynthesis, and its DAM is calculated
from the temperature stress function, the absorbed PAR or APAR based on LAI and ELUE as:
(6.59)
(6.60)
where Rg = daily radiation, εc = climate efficiency factor, K = light extinction coefficient, and =
temperature stress function mathematically defined as:
(6.61)
where Topt and Tmax = the optimal (most suitable) and the maximum temperatures for crop growth,
respectively.
The proportion of leaf biomass Pl is calculated as:
(6.62)
where Pla and Plb = distribution coefficients. If Pl > 0, then:
(6.63)
where LAI+ = daily LAI increment, and SLA = specific leaf area (i.e., the ratio of the leaf unit area
to its dry weight).
If SMT > STT, then:
(6.64)
where LAI– = daily LAI reduction, and Rs = coefficient of leaf senescence.6.8.4
6.8.4.1
This method of estimation is complex as it requires the input of 11 parameter values (e.g., DAM0,
K, Tmin, Topt, Tmax, SLA, recession coefficient Rs, partition function coefficient Pla and Plb, D0,
ELUE, and STT). The accuracy of estimation from Sentinel-2 data at the regional level varies from
R2 = 0.49 (nRMSE = 21.9%) in one year to R2 = 0.61 (nRMSE = 23.3%) in another year (the
accuracy is much higher with the estimated LAI and biomass) (Ma et all., 2022). These accuracy
levels are not higher than those obtained using machine learning and simple regression. To a large
degree, the accuracy of modeling is affected by the authenticity and range of the 11 input
parameters. Since the values and ranges of these variables have a geographic component, growth
models are not as applicable as they seem and lots of effort are needed to determine the variable
values and their ranges before a decent estimation accuracy can be expected.
Rice
Rice is a staple food for millions of people around the world. It is a crop widely cultivated in
Southeast Asia where some cultivars are water-hungry, especially at the heading stage. Rice yield is
of extreme significance not only to national food security but also to social stability. Rice yield is
defined as grain weight at 14% moisture per unit area for paddy (rough) rice or with the husk
attached. Rice yield can be estimated using field sampling or gathered from local government
harvest statistics. Neither is satisfactory as the former is destructive while the latter cannot deliver
the results in any lead time. In comparison, remote sensing offers a practicable yield prediction with
a potential lead time of several months. Remote sensing images not only reveal the spatial extent (or
acreage) of rice cultivation, but also indicate the crops’ growth state and even nutrient content, all of
which are indispensable to the accurate prediction of rice yield. This section elucidates mapping of
paddy fields first, followed by the estimation of rice yield.
Paddy Mapping
Dissimilar to AGB that can be estimated from plants directly irrespective of its species, rice yield
quantification requires the separation of crops from soil. Thus, the prerequisite of any rice yield
prediction is the mapping of cultivated rice fields. Paddy field mapping is actually qualitative remote
sensing that falls outside the scope of this book, but it is still included in the discussion because the
accuracy of paddy mapping directly affects the reliability of the subsequently estimated rice yield.
This mapping is accomplished ideally from medium-resolution OE images, such as Sentinel-2 and
Landsat 8/9 OLI, using per-pixel spectral-based classification, or object-oriented image
classification of very high-resolution images. Since rice and other crops are just plants, they share a
spectral signature that highly resembles each other, making it difficult to differentiate them
accurately. Accurate separation may be possible by considering spectral metrics.
Nevertheless, rice cultivation has a strong phenology that can be captured by multi-temporal
images at critical growth stages. The strong and distinctive phenological pattern of rice crops is quite
different from other non-water-hungry plants. Their unique spectral behavior at different growth
stages (e.g., reproductive and ripening) is best characterized by spectral indices. The commonly used
ones are land surface water index (LSWI), Green Chlorophyll Vegetation Index (GCVI), and NDVI,
as well as their percentiles and statistical variants (mean, median, and standard deviation).
Phenology-based mapping of irrigated and rain-fed paddy fields in the wet and dry seasons requiresmulti-temporal images recorded over the entire growth cycle. They can be used to generate the
temporal profile of GCVI and LSWI that can be clustered using decision tree analysis (Dela Torre et
al., 2021). After matching with the reference profiles, they can be assigned to respective information
classes to differentiate paddy fields from other land covers.
Precise estimation of rice yield requires paddy fields to be grouped into rain-fed and irrigated
categories as they have different yield capacity and harvest index values. Irrigated paddies are
located in lowlying areas with a relatively gentle topography that can be easily connected to the
nearest water source via a canal or ditch. It tends to be spatially extensive and scarcely inter-mixed
with other crops. In comparison, rain-fed rice is cultivated in uplands in close proximity to other
crops, and may be spatially co-existing with them. Rain-fed paddy fields tend to be small and
spatially fragmented. In general, they are less productive than irrigated ones. Spectrally, it is difficult
to accurately distinguish paddy fields into specific types, especially for rain-fed ones on medium￾resolution images at the regional level. One possible method of successful differentiation is to take
into account environmental variables. The most obvious choice is topography in the form of DEM,
from which slope is derived. The simultaneous consideration of both spectral and environmental
variables is ideally implemented using machine learning methods such as RF, during which the
mapped paddy cluster is further refined into rain-fed and irrigated at an acceptable mapping
accuracy (Table 6.37). As expected, rain-fed paddies are mapped at a lower user’s accuracy of 60%
than 79% of irrigated paddies in the dry season. This relativity remains unchanged in the west
season, but the discrepancy between the two types of paddy fields diminishes as rain-fed rices are
not under drought stress.
Table 6.37
Confusion matrix illustrating the accuracy metrics of classifying regional paddy fields by type from Sentinel-2 data based on
phenology, VIs and environmental variables using machine learning
Reference
pixels
Classified pixels
Producer´s
accuracy
User´s
accuracy
Rain-fed
rice
Irrigated
rice
Non-rice
crops Sum
Dry Season
Rain-fed rice 151 23 77 251 86% 60%
Irrigated rice 9 54 5 68 70% 79%
Non-rice
crops 16 0 73 89 47% 82%
Total 176 77 155 408
Overall
accuracy 68%
Kappa
statistic 0.486
Wet Season
Rain-fed rice 149 24 40 213 80% 70%
Irrigated rice 9 53 4 66 67% 80%6.8.4.2
Reference
pixels
Classified pixels
Producer´s
accuracy
User´s
accuracy
Rain-fed
rice
Irrigated
rice
Non-rice
crops Sum
Non-rice
crops 29 2 129 160 75% 81%
Total 187 79 173 439
Overall
accuracy 75%
Kappa
statistic 0.605
Source: Dela Torre et al. (2021).
The mapping accuracy is affected by seasonality. Dry season paddies are mapped at an overall
accuracy of 68% (κ = 0.49), while the wet season crop is more accurately mapped at 75% (κ = 0.61).
The overall accuracy improves to 76% (dry season) and 83% (wet season) after the two rice classes
are merged into one class to eliminate inter-class misclassifications between them (Dela Torre et al.,
2021). The remotely sensed paddy fields have an r of 0.75 with municipal yield statistics. The major
source of classification inaccuracy stems from the consistent underestimation of small cultivated
areas due probably to the medium resolution (20 m × 20 m) of Sentinel-2 data that causes small
plots of paddy fields to be omitted.
The reported accuracy is lower than the accuracy of 95.3% for rice and 92.0% for non-rice
mapped from radar data in the Cauvery Delta Region of Tamil Nadu, India where rice is the
predominant crop growing extensively, all irrigated (Pazhanivelan et al., 2022). The overall mapping
accuracy is also high at 94.5%, with an average reliability of 92.0% from radar data. A similarly
high accuracy of 90.7% (spring season, κ = 0.81) and 94.7% (summer season, κ = 0.89) is achieved
in mapping paddy fields from Sentinel-1 radar data using a rule-based rice detection algorithm in the
Red River Delta of Vietnam (Setiyono et al., 2018). Thus, the delta homogeneous growing
environment is conducive to achieving a higher mapping accuracy of paddies even from radar
images than in mountainous tropical areas from spectrally-rich optical data.
Yield Estimation
Rice yield is sometimes treated as synonymous to LAI and NPP that are commonly predicted from
NDVI and other environmental variables, such as temperature and precipitation using three broad
categories of empirical, semi-physical, and physical models. Empirical models are the simplest,
taking the form of:
(6.65)
where βi (I = 1, 2, 3) = intercept and coefficients determined via regression analysis. Rice yield bears
a close relationship with NDVI if the cultivated areas are further partitioned into specific groups
(Table 6.38). Irrigated paddies have a consistent prediction accuracy while the accuracy of rain-fed
paddies varies widely with the cultivation region due probably to the spatial mixture of rice with
non-rice crops, and topographic effects.Table 6.38
Linear relationships between rice yield and NDVI by paddy type in different cultivation zones
Paddy type Estimation model R²
Irrigated I 3.625 × NDVI - 1.232 0.768
Irrigated II 5.864 × NDVI - 5.268 0.591
Rain-fed I 13.77 × NDVI - 18.22 0.698
Rain-fed II 4.450 × NDVI - 3.385 0.387
Rain-fed III 11.91 × NDVI - 15.096 0.878
Source: Dela Torre (2022).
In spite of the high R² values, the relationship is less accurate but still considered satisfactory if
judged by the adjusted R² value. The estimation accuracy can be further improved by taking into
account environmental variables, such as evapotranspiration (ET), precipitation, and night time
temperature using multivariate models. Multiple linear regression models of NDVI against these
environmental parameters for the five rice cultivation zones are generally more accurate than those
involving only NDVI (Table 6.39). They have a satisfactory accuracy even judged by the adjusted
R² value, especially for rain-fed paddies in zone III. In comparison, irrigated paddies benefit less
from the consideration of the extra growth environmental variables in their prediction accuracy.
Table 6.39
Multiple linear regression models for predicting NDVI from environmental variables in five rice cultivation zones in Iloilo of
the Philippines
Ecosystem Model R2
Adj￾R2
Irrigated I
0.1487 + 0.0724LSTnight + 0.0699ET - 0.0521PET -
0.0002PPT 0.774 0.592
Irrigated II 2.4567 - 0.0348LSTday + 0.0164PET - 0.0170ET 0.698 0.244
Rain-fed I
0.9565 + 0.0001Elevation + 0.0470ET - 0.0301PET +
0.0211Tnight 0.685 0.559
Rain-fed II 1.5659 + 0.0244ET - 0.0004PPT - 0.0192Tnight - 0.0002Slope 0.709 0.619
Rain-fed
III 0.7278 + 0.0001PPT + 0.0277ET - 0.0274Slope + 0.0069Tnight 0.969 0.928
Source: Dela Torre (2022).
Note: LSTday = daytime land surface temperature; LSTnight = night time land surface temperature; ET = evapotranspiration; PET =
potential evapotranspiration.
The four most important variables for predicting rice yield are identified as PPT (precipitation),
nighttime temperature (LSTnight), ET, and elevation (H) for irrigated rice, and PPT, LSTday, ET, and
slope for rain-fed rice. The yield is predicted as:
(6.66)
(6.67)where ig = irrigated; rf = rain-fed. R² values shown in the equations are based on training data. They
drop to 0.63 and 0.70 for irrigated and rain-fed paddies, respectively, using validation data.
Application of these models to cloud-free Sentinel-2 images produces a distribution map of irrigated
and rain-fed rice yields. They can be merged to form an overall distribution map (Figure 6.28).
Evaluated against yield statistics, the irrigated model has a RMSE of 0.197 t ∙ ha–1 and MAE of
0.134 t ∙ ha–1, about one third of RMSE (0.498 t ∙ ha–1) and MAE (0.367 t ∙ ha–1) of the rain-fed
model.
FIGURE 6.28 Spatial distribution of rice yield (t × ha–1) in Iloilo of the Philippines by paddy type
generated from multi-year Sentinel-2 data based on rice phenology. (Dela Torre, 2022.)
Semi-physical methods of quantification take into account the biochemical process of rice growth,
such as LUE, PAR, and fPAR in estimating rice yield. This estimation is based on NPP (g ∙ m–2 ∙
day–1) since the day of sowing to the harvest date periodically (e.g., eight days), as well as the stress
factors of water and temperature. Conceptually, NPP calculated using Eq. 6.31 has to be modified
for paddies as:
(6.68)where:
(6.69)
(6.70)
where Tmin, Tmax, and Topt = minimum, maximum, and optimal temperature (°C) for photosynthesis,
respectively. T = daily mean temperature (°C). Rice yield is derived from NPP multiplied by the HI
of rice. The overall method of estimation using the semi-physical method is illustrated in Figure
6.29 below.
FIGURE 6.29 Flow chart illustrating the procedure of estimating rice yield using the semi-physical
approach. (Modified from Pazhanivelan et al., 2022, open access.)
The last method of estimation is to assimilate image-derived products with rice growth simulation
models, such as DSSAT and ORYZA. DSSAT estimates daily biomass production from emergence
to harvest based on assimilated CO2 estimates. The rice biomass partitioned to different organs is afunction of the phenological age of the rice crop. Biomass accumulation is simulated from five
categories of variables in weather (daily maximum and minimum temperature, rainfall, and solar
radiation), soil (pH, organic carbon, texture, coarse fragments, cation exchange capacity, and bulk
density), crop parameters (rice cultivar or genotype, leaf development rate, relative leaf growth rate,
portioning factor, specific leaf area, and leaf mortality rate), and crop management (fertilizer use),
cropping calendar (e.g., sowing date, flowering date, and harvesting period), and plant population
(transplanting method and row spacing). These parameters can be extracted from different sources
or via field surveys. The CERES-Rice model available in DSSAT v4.8 simulates daily crop growth
and development. ORYZA is an ecophysiological model that can simulate the development, growth,
and yield of different rice cultivars in response to inherent soil physical and chemical properties,
micro-climate, and prevailing agronomic practices using logical algorithms and mathematical
equations.
In the assimilation, remotely sensed data are used to accurately map paddy fields and estimate the
date of crop cultivation, both of which are fed to the crop growth model. For instance, after multi￾temporal LAI estimates are derived from satellite reflectance data using a hybrid approach
exploiting machine learning algorithms, they are fed with the synthetic database generated from the
PROSAIL RTM inversion. These estimations are then fitted into a logistic function for LAI after
seasonality information (e.g., the seasonal drivers of crop growth of start and peak of season) has
been selected from satellite data using the rule-based PhenoRice algorithm. Rice yield is simulated
using the Rice Yield Estimation System (Rice-YES) to coordinate both remote sensing and other
relevant data related to meteorology, soil, and agronomic management, and mainstreaming them into
the ORYZA crop growth model (Setiyono et al., 2018). Biophysical parameters such as LAI derived
from remotely sensed data are assimilated into the DSSAT model using an optimization algorithm
(Son et al., 2016). During optimization, all the parameters (e.g., planting date, planting population,
and fertilizer use) in the crop simulation model are initiated and adjusted accordingly to calculate the
fitness value obtained from the cost function based on the differences between simulated LAI and
image-estimated LAI. It indicates whether the optimization has reached the optimal input parameters
according to the user-defined tolerance threshold. The process re-initiates the input parameters based
on the RMSE of the LAIs. The model produces crop yield and other parameters when a
minimization threshold is reached (Figure 6.30).FIGURE 6.30 Schematic diagram illustrating the procedure of integrating remote sensing data (optical
and SAR) with the Rice-YES and ORYZA crop growth models to predict spatially-explicit rice yield.
Ancillary data include climate data, soil, and agronomic management information. (Modified from
Setiyono et al., 2018, open access.)
The simulated growth parameters of rice, combined with satellite-derived LAI, can improve micro￾scale rice yield predictions and generate spatial distribution maps of rice yield. In order to be applied
to a specific geographic area, the DSSAT model needs to be calibrated using data collected in a prior
growing season. After calibration, the CERES-Rice model is capable of estimating growth stages
and rice yield accurately. Satellite-derived yields have an agreement of 88.97%–93.12% with the
observed yields at the district level, with a mean R2, RMSE, and NRMSE of 0.86, 0.33 t ∙ ha–1 and
9.43%, respectively (Pazhanivelan et al., 2022). This accuracy is highly comparable to the RMSE of
0.30 and 0.46 t ∙ ha–1 (NRMSE of 5% and 8%) for the spring and summer seasons, respectively,
achieved in the Red River Delta of Vietnam (Setiyono et al., 2018), even though the results are
obtained from much coarser resolution MODIS and SAR data assimilated into the ORYZA crop
growth model. This assimilation method can generate well-adjusted yield estimates and produce
adequately accurate maps of the spatial distribution of rice yield that reliably replicates the official
yield statistics. Quantitatively, the estimated yields have an RMSE of 11.7% and MAE of 9.7%,
respectively, in comparison with the government yield statistics (Son et al., 2016).
Of the three types of rice yield estimation methods, remote sensing data assimilation with a crop
growth model produces the most accurate estimates (mean R2 = 0.86), followed by spectral indices￾based regression analysis (R² = 0.81) (Pazhanivelan et al., 2022). The semi-physical approach is
slightly less accurate, achieving an R2 value of 0.78 (RMSE = 532.74 t ∙ ha–1, and NRMSE =
14.52%). Rice yield predicted from remote sensing data integrated with crop modeling has the
highest agreement of 90.57% with district-level mean rice yield statistics, followed closely by
spectral indices-based regression analysis (agreement = 90.52%). The semi-physical approach
achieves an agreement of only 85.47%. Both rice yield estimation techniques of integrating remote6.9
6.9.1
sensing products with the DSSAT crop growth model and spectral indices-based regression analysis
can be used for precise and operational estimation of rice yield spatially.
WILD FIRE PARAMETERS
Under the influence of accelerated climate warming, wildland fires or simply wild fires have
become more frequent occurrences in some vulnerable ecosystems around the world. Wild fires not
only destroy valuable habitats but also reduce biodiversity, as well as pollute the atmosphere and
threaten human safety. It is important to quantify wild fire parameters as the information is
invaluable for effective fire fighting and for assessing its ecosystem damage. The fire behavior
parameters that can be quantified from remotely sensed data include fire spread rate, fire intensity,
and burn intensity.
Rate of Spread
Once a wildfire is ignited, it spreads out in a direction dictated by the prevailing wind, subject to the
topographic influence and fuel availability and properties. The rate of fire spread refers to the speed
at which the flame is moving away from its origin to elsewhere. Ideally, fire rate of spread (ROS) is
quantified from multi-temporal infrared images that are geo-referenced to the same coordinate
system. ROS calculation requires the detection of the fire front, usually from infrared images based
on temperature thresholds on respective time-lapsed thermal images or the transformed brightness
temperature (BT) images. The flame front ROS of an actively burning fire is calculated as:
(6.71)
where L = length of the fire front spread vector determined from a pair of time-series images; t1 and
t2 = time of the satellite overpass. Ideally, the multi-temporal images needed for determining ROS
should have a short temporal separation measured by hours instead of days, such as 12 hours of
MODIS images or shorted, but the coarse resolution (≥500 m) of MODIS TIR bands may not be
sufficiently fine to enable small fires to be detected. They can be replaced by airborne middle
infrared (MIR) thermal images that allow the quantification of ROS of small- and large-scale fires.
These images can detect fires over an elevated BT range of 470–900 K, but still avoid the saturation
problem over intensely burning areas (Paugam et al., 2013). Areas not affected by burning appear as
blank as the sensor is unable to detect signals over the ambient (~300 K) background. The MIR
images are then converted into BT and fire radiative power (FRP) images, from which fires are
detected. MIR radiance can optimize the accuracy of FRP retrieval from areas of active flaming or
smoldering combustion (whose temperature is assumed to exceed 700 K) but tends to underestimate
FRP of cooler surfaces. In contrast, long-wave infrared (LWIR) thermal images are not preferred
because they have a much-reduced signal range. Besides, LWIR measurements are far less suited to
the derivation of FRP of small fires comprising a small fraction of large pixels.
The core in ROS estimation is to locate the position of the fire front line. It can be formed by
automatically vectorizing fire pixels identified through some kind of thresholding to form polygons
(Ruecker et al., 2021). Polygon boundaries are construed to represent the start line of the fire front.
The jagged boundaries from vectorization are usually smoothed to enable precise pinpointing of thefire front. The same detection is repeated for the second satellite image obtained at another time,
preferably by the same sensor, to detect the endpoints of the same fire. The detection may be
implemented automatically by clustering active fire pixels, followed by engulfing each cluster with a
concave hull envelope, as in continuum removal (refer to Section 4.4.3 for details). The utmost
endpoint of the hull is then connected to the start point to form the displacement vector. A large
number of start point–endpoint pairs may be connected, but not all of them are genuine. Their
authenticity is then checked using additional criteria to see if they traverse burned areas and barriers,
or are associated with different ignition events. Those plausible connections that cannot meet the
criteria are eliminated. A fire front´s local ROS (m ∙ s
–1) is derived from the displaced fire front over
a duration (Δt) by dividing the displaced distance in the direction perpendicular to the local fire front
by Δt (Figure 6.31). It is possible to further relate the calculated ROS results to fire driving
parameters, such as wind speed, fuel availability, and the coalescence of separate fire fronts to
explore its variability across the burning plot and its relationship with fire drivers (Paugam et al.,
2013).
FIGURE 6.31 Spatial variation in flame front ROS in a 45 m × 21 m experimental heather moorland
plot detected from off-nadir-viewing hand-held thermal image data using a camera onboard a
helicopter hovering at 130-170 m above the burning field overlaid on top of the fire radiative power
field at a pixel size of ~2 m². (Paugam et al., 2013, open access.)
Image-derived ROS is statistically and significantly influenced by the estimation method, and
sizeable inaccuracy can occur if the direction of ROS is not taken into account in its determination.
The TIR imaging method proposed by Paugam et al. (2013) is the most appropriate for estimating
the rate and direction of spread than other methods such as infrared grid array and infrared triangle
methods (Johnston et al., 2018). At 2.37 μm, the spectral response of fires exhibits the widest
variation, suggesting that it is the best at potentially assessing fire intensity. However, no results
have been produced to substantiate this potential.6.9.2 Fire Intensity
Wildfire intensity is an important parameter of fire behavior. Accurate information on fire intensity
is critical to assessing the ecological impacts of wildfires and devising the best fire response
strategies to suppress fires and to safeguard fire fighters in the field. Also known as fire line
intensity, fire intensity is commonly measured from the thermal energy emitted from a 1-m strip of
the actively combusting area from the leading edge of the fire front to the rear of the flaming zone
(Alexander, 1982). It can accurately indicate the quantity of heat released by the combusting
biomass, and manifest the burning intensity of an active fire. An intensive fire has a higher
temperature and spreads much faster than a less intensive one. Both temperature and spread velocity
can be used to define fire intensity. Since temperature is not easily measured in a fire, it has been
replaced with heat of combustion, fuel consumption, and ROS (m ∙ s
–1) in calculating fire intensity
as their product, namely:
(6.72)
where w = weight of the available (or burned) biomass per unit area (kg ∙ m–2) that is presumably
combusted; H = heat yield of unit combusted fuel (kJ ∙ kg–1), with its exact value varying with fuel
type. The product H ∙ w represents the available fuel energy. Of the three parameters, H makes the
least contribution to I. Since fuel weight has a relatively narrow range of variation (⁓10-fold),
much narrower than the ⁓1,000-fold range of ROS variations, it is the most influential factor to fire
intensity. Eq. 6.72 is best used to estimate I of a moderate intensity fire from TIR images for actively
spreading flame fronts at a resolution up to 0.13 m (Johnston et al., 2018). These images should
have a fine-temporal resolution such as MODIS images that allow the same fire event to be sensed
multiple times over its life span. They must have non-saturating mid- and long-wave infrared
(MWIR, LWIR) bands suitable for characterizing fires.
Another common fire intensity measure is Bryam’s fire intensity (IB,tot; kW ∙ m–1) that has been
widely used to quantify wildfire behavior. It is derived from the observed FRP that can be directly
measured from the radiant energy released from a fire in the field. FRP is a function of the 4th power
of temperature according to Stefan-Boltzman law, and indicates the radiant energy release rate from
a fire. It can be estimated from airborne and satellite data at the landscape scale. FRP (Watts ∙ pixel–
1) is quantified from MWIR radiance and converted to FRP density (FRPD) (kW ∙ m–2) as needed by
multiplying by pixel area. Muti-temporal FRP data (e.g., every 15 minutes) can be obtained from
Meteosat SEVIRI observations and estimated total fire radiative energy release over the observed
fire front, from which total fuel consumption is estimated by multiplying FRP by a conversion factor
(e.g., 0.368 kg ∙ MJ–1). Divided by the burned area, it leads to fuel consumption per unit area. Time￾series FRPD can be temporally integrated for each pixel to produce Fire Radiative Energy Density
(FRED) (kj ∙ m–2) maps. It can also be estimated from satellite images using several methods
without field samples but still yield a spatiotemporal view of fire intensity distribution.
Table 6.40
Main features of three methods of determining fire intensity output in the form of median by row used only for comparison
with ground samplingMethod
Image
needed
Radiant
energy Sampling Measurement FIradresolution*
FRP￾FFL
Single
frames
FRP
(kW ×
pixel–
1)
Separately
summed for
entire image &
flaming area
Flame front identified by
fixed threshold (773 K).
Length measured from
north to south on
platform
Single value
for each frame
FRED￾ROS
Time
series
FRPD
(kW ×
m–2)
Time-series
integrated
over each
pixel
ROS computed for
perimeter pixels using
Paugam et al. (2013) and
773 K arrival threshold
Each pixel
whose ROS is
computed
FRP￾FD
Single
frames
FRPD
(kW ×
m–2)
Integrated
from
perimeter to
flame depth
along the
normal
extension
Flame front identified by
threshold (773 K). Flame
depth is measured
following the normal, and
terminated where two
consecutive pixels fall
below the termination
threshold (773 & 700 K
used)
0.5-m
spacing along
flame front
Source: Johnston et al. (2017), with permission (license ID 1469277-1) from IAWF.
Note: * FIrad resolution describes the actual data available from each method.
As illustrated in Table 6.40, Byram’s fire intensity can be estimated using three methods (Johnston et
al., 2017): (i) FRED-ROS; (ii) FRP-Flame Depth (FD), and (iii) RP-Fire Front Length (FFL). The
first is a pixel-wise approach integrating FRP over the pixel´s burn time to calculate pixel-level fire
radiative energy to estimate I as:
(6.73)where IB,rad = the radiative portion of IB,tot; t = instantaneous time step of the image. This
method can produce the spatial distribution of IB,rad along the fire perimeter by calculating FRED
(kW ∙ m–2) for each pixel along it. FRED is observed over time to characterize temporal fluctuation
in FPRD by integrating it over the time domain of τ. FRPD is determined from an infrared image at
each time step and integrated at each pixel to produce FRED (kJ ∙ m–2). The infrared time series is
also employed to calculate ROS and determine the direction of fire spread (blue arrows in Figure
6.32) at each time step. The FRED and ROS values are then combined at each pixel along the flame
front to produce FIrad spatially wherever the ROS method produces measurements (colored pixels).
This method is advantageous in that it includes the most temporally unstable inputs to IB,tot (ROS)
directly, providing a complete description of fire behavior along the perimeter. However, this
advantage also demands very high temporal resolution images that are frequently unavailable.FIGURE 6.32 Depiction of IB,rad output from the fire radiative energy density - rate of spread
calculation. Grey legend: Brightness temperature (K); color legend: radiative fire intensity (kW m–1).
(Johnston et al., 2017, with permission (license ID 1469277-2) from IAWF.)
The second approach relies on ROS as the depth of the flaming zone multiplied by the flame
residence time, or:
(6.74)
where i = a pixel indicator along d; ∆d = distance along d subtended by one pixel (m); ∆p = pixel
resolution (m); (∆xd, ∆yd) = length (in pixels) of the horizontal and vertical components of the flame
depth vector. If the flame front is in a steady state, integrating the time series of FRPD at a pixel
over t (Eq. 6.74) is equivalent to integrating the FRPD along the depth of the flame front.
The last method is grounded on the FRP along the fire front length (Ruecker et al., 2021), in
which FRP is converted to an estimate of IB,rad averaged over the flame front length (l), or:
(6.75)6.9.3
Since the burned area is unknown with actively burning fires, this method is suitable for post-fire
intensity assessment only.
The estimated ROS results from proper time-series images are in close agreement with those
obtained using the traditional methods (R² = 0.34–0.73) without ground samples or ancillary data,
demonstrating that TIR sensing is a viable means of estimating fire intensity quantitatively
(Johnston et al., 2017).
Burn Intensity
Burn intensity differs from fire intensity as it depicts the impact of a fire, namely, how it has affected
the ecosystem functionality of the burned area. Burn intensity has been characterized using four
spectral indices of normalized burn ratio (NBR), difference normalized burn ratio (dNBR) that is
designed to measure fire intensity, relative difference normalized burn ratio (RdNBR), and mid￾infrared burn index (MIRBI) that is strong at mapping burned area but not burn intensity (so is not
used widely). They are calculated as:
(6.76)
(6.77)
(6.78)
(6.79)
where SWIR2 and SWIR1 are Landsat OLI image SWIR bands (Lu et al., 2015), or bands from
other OE images of an equivalent spectral range. Of the four indices, NBR and dNBR can be used to
derive relative burn ratio (RBR) as:
(6.80)
These indices are derivable from bi-temporal images that are either paired or reflectance
composited. In order to classify and validate image-derived burn severity based on these indices, the
field-measured burn intensity is usually enumerated as a composite burn index (CBI). Its score is
based on five vertical strata: three in the understory and two in the overstory vegetation. CBI score
ranges from 0 to 3, with a value ≤1.25 representing low burns, between 1.25 and 2.25 moderate
burns, or >2.25 high burns. The spatial distribution of burn severity is mapped by regressing the
indices against CBI non-linearly (Figure 6.33). CBI data bear a strong non-linear relationship (R2 >
0.85) with bi-temporal indices. The R² value of the three indices hardly varies among themselves,
regardless of whether they are derived from paired images or composited reflectance. In general, the
calibrated data produce slightly more accurate predictions than the non-calibrated data. Burn
severity is mapped at an overall accuracy of 76.9%–83.7% (κ = 0.61–0.72) and RdNBR produces
more accurate results than dNBR and RdNBR (Table 6.41). However, only three discrete classes of
burn severity are mapped, not a continuous variation. If derived from paired images, it is important
to calibrate bi-temporal dNBR indices as this can improve the overall classification accuracy of burn
severity over uncalibrated bi-temporal indices. MIRBI, NBR, and dNBR can all discriminate burnseverity in the semi-arid grassland of the North American prairie, but MIRBI achieves the highest
overall discrimination accuracy of 76%, noticeably higher than 62% of NBR (Lu et al., 2015). NBR
has an impaired capability of identifying burn severity in the dry season. Once produced from these
indices, the level of burn severity can be related to the quantity of pre-fire senescent biomass and the
local topography (e.g., more severe burns are located in lowlying valleys).
FIGURE 6.33 Non-linear regression models built using CBI field data (n = 251) and non-calibrated bi￾temporal indices derived from paired images (a–c) and reflectance composites (d–f). (Morresi et al.,
2022, open access.)
Table 6.41
Accuracy of three bi-temporal burn intensity indices derived using paired images and reflectance composites against field￾collected composite burn index obtained from five-fold cross-validation
Intensity index Measure
Paired images Reflectance composites
Uncalibrated Calibrated Uncalibrated Calibrated
dNBR
R²
0.865 0.869 0.871 0.873
RdNBR 0.874 0.878 0.880 0.882
RBR 0.872 0.877 0.878 0.879
dNBR RMSE 94.8 93.2 99.3 98.7
RdNBR 112.5 110.6 116.8 116.3Intensity index Measure
Paired images Reflectance composites
Uncalibrated Calibrated Uncalibrated Calibrated
RBR 55.3 54.4 57.6 57.3
Source: Morresi et al. (2022), open access.
Apart from indices, burn intensity can also be potentially studied from readily available FRP
products. The relationship between burn intensity as measured by mean dNBR and RdNBR indices
from Landsat images and FPR has been explored using 16 fires across a vegetation structure
continuum in the western U.S. at the pixel level (Heward et al., 2013). Among the fires, 69% of the
variation in RdNBR is explained by the 90th percentile of MODIS FRP (Table 6.42). In the two tree￾cover classes, the median and 90th percentile MODIS FRP per fire are sensible predictors of
RdNBR whereas across all cover classes the 90th percentile MODIS FRP is a reasonable predictor
(Heward et al., 2013). In each case, there is an asymptote in the RdNBR values, indicating a lack of
index sensitivity at higher fire intensities. Therefore, distributional MODIS FRP measures (median
and 90th percentile FRP) derived from multiple MODIS overpasses of the actively burning fire can
be used to infer the burn severity of less intense fires and predict potential long-term negative
ecological effects of individual fires. It is worth noting that fire intensity and burn intensity
measured by the indices are related to each other statistically only when the fire intensity is not
excessively high. Besides, the MODIS FRP (MW) is derived using a non-linear empirical
relationship between FRP and BT retrieved at MIR wavelengths. It may have missed small fires. No
significant relationships exist between burn severity metrics and maximum MODIS FRP.
Table 6.42
Accuracy indicators of the relationship between metrics of burn severity (dNBR and RdNBR) and median and 90th percentile
FRP overall and within two tree canopy closure percentage classes (α = 0.05)
Index
Median FRP 90th Percentile FRP
R² n F SEM R² n F SEM
dNBR
Overall - - - - - - - -
25-60%c 0.43a 10  6.1 81 0.49a 10 7.8 77
60-100% - - - - - -
RdNBR
Overall - - - - 0.42b 13 8.0 127
25-60% 0.63a 10 13.6 13.5 0.69a 10 18 122
60-100% - - - - - -
Source: Heward et al. (2013), with permission (license ID 1469280-1) from IAWF.
Notes: SEM = standard error of the mean.
a Linear relationship.
b Logarithmic relationship.
c Herbaceous grassland, herbaceous shrub steppe, open tree canopy (25–60%); and closed tree canopy (60–100%).
REFERENCESAlexander ME (1982) Calculating and interpreting forest fire intensities. Can J Botany 60: 349–357.
doi:10.1139/B82-048
Ali AM, R Darvishzadeh, A Skidmore, TW Gara, B O’Connor, C Roeoesli, M Heurich, and M
Paganini (2020) Comparing methods for mapping canopy chlorophyll content in a mixed
mountain forest using Sentinel-2 data. Int J Appl Earth Obs Geoinfo 87: 102037. doi:
10.1016/j.jag.2019.102037
Alleaume S, C Corbane, and M Deshayes (2013) Capacités et limites de la télédétection pour
cartographier les habitats naturels. Irstea- UMR TETIS Rapport réalisé dans le cadre du projet
CARHAB).
Andersen H-E, RJ McGaughe, and SE Reutebuch (2005) Estimating forest canopy fuel parameters
using LiDAR data. Rem Sens Environ 94:441–449.
Atzberger C, R Darvishzadeh, M Immitzer, M Schlerf, A Skidmore, and G le Maire (2015)
Comparative analysis of different retrieval methods for mapping grassland leaf area index using
airborne imaging spectroscopy. Int J Appl Earth Obs Geoinfo 43: 19–31. doi:
10.1016/j.jag.2015.01.009
Baret F, M Weiss, R Lacaze, F Camacho, H Makhmara, P Pacholcyzk, and B Smets (2013) GEOV1:
LAI and FAPAR essential climate variables and FCOVER global time series capitalizing over
existing products. Part1: Principles of development and production. Rem Sens Environ 137: 299–
309
Baret F, M Weiss, A Verger, and B Smets (2016) ATBD for LAI, FAPAR and FCOVER From
PROBA-V Products at 300M Resolution (GEOV3) (IMAGINES_RP2.1_ATBD-LAI300M).
Barton CVM and PRJ North (2001) Remote sensing of canopy light use efficiency using the
photochemical reflectance index: Model and sensitivity analysis. Rem Sens Environ 78: 264–273.
Beets PN, MO Kimberley, GR Oliver, SH Pearce, JD Graham, and A Brandon (2012) Allometric
equations for estimating carbon stocks in natural forest in New Zealand. Forests 3: 818–839. doi:
10.3390/f3030818
Bortolot ZJ and RH Wynne (2005) Estimating forest biomass using small footprint LiDAR data: An
individual tree-based approach that incorporates training data. ISPRS J Photogram Rem Sens 59:
342–360
Broge NH and E Leblanc (2000) Comparing prediction power and stability of broadband and
hyperspectral vegetation indices for estimation of green leaf area index and canopy chlorophyll
density. Rem Sens Environ 76: 156–172
Brown LA, BO Ogutu, and J Dash (2019) Estimating forest leaf area index and canopy chlorophyll
content with Sentinel-2: An evaluation of two hybrid retrieval algorithms. Rem Sens 11: 1752.
doi:10.3390/rs11151752
Campbell SG and JM Norman (1998) An Introduction to Environmental Biophysics (2nd ed.). New
York: Springer-Verlag.
Caturegli L, S Matteoli, M Gaetani, N Grossi, S Magni, A Minelli, G Corsini, D Remorini & M
Volterrani (2020) Effects of water stress on spectral reflectance of bermudagrass. Sci Rep 10:
15055. doi: 10.1038/s41598-020-72006-6
Cheng E, B Zhang, D Peng, L Zhong, L Yu, Y Liu, C Xiao, C Li, X Li, Y Chen, H Ye, H Wang, R
Yu, J Hu, and S Yang (2022) Wheat yield estimation using remote sensing data based on machinelearning approaches. Front Plant Sci 13: 1090970. doi: 10.3389/fpls.2022.1090970
Choi H and Y Song (2022) Comparing tree structures derived among airborne, terrestrial and mobile
LiDAR systems in urban parks. GISci Rem Sens 59(1): 843–860. doi:
10.1080/15481603.2022.2076381
Coomes DA, RB Allen, NA Scott, C Goulding, and P Beets (2002) Designing systems to monitor
carbon stocks in forests and shrublands. Forest Ecol Manage 164(1–3): 89–108. doi:
10.1016/S0378-1127(01)00592-8
Coops NC, MA Wulder, DS Culvenor, and B St-Onge (2004) Comparison of forest attributes
extracted from fine spatial resolution multispectral and LiDAR data. Can J Rem Sens 30(6): 855–
866.
Coops NC, T Hilker, M Wulder, B St-Onge, G Newnham, A Siggins, and JA Trofymow (2007)
Estimating canopy structure of Douglas-fir forest stands from discrete-return LiDAR. Trees 21:
295–310.
Darvishzadeh R, A Skidmore, H Abdullaha, E Cherenet, A Ali, T Wang, W Nieuwenhuis, M
Heurich, A Vrieling, B O’Connor, and M Paganini (2019) Mapping leaf chlorophyll content from
Sentinel-2 and RapidEye data in spruce stands using the invertible forest reflectance model. Int J
Appl Earth Obs Geoinfo 79: 58–70. doi: 10.1016/j.jag.2019.03.003
Dash J and PJ Curran (2004) The MERIS terrestrial chlorophyll index. Int J Rem Sens 25: 5403–
5413.
Daughtry CST, CK Walthall, MS Kim, E Brown de Costoun, and JE McMurtrey (2000) Estimating
corn leaf chlorophyll concentration from leaf and canopy reflectance. Rem Sens Environ 74: 229–
239.
Dela Torre DMG (2022) Rice yield mapping and modelling of climate change impacts in the
Philippines using remote sensing. PhD thesis, University of Auckland, 162 p.
Dela Torre DMG, J Gao, C Macinnis-Ng, and Y Shi (2021) Phenology-based delineation of irrigated
and rain-fed paddy fields with Sentinel-2 imagery in Google Earth Engine. Geo-spatial Info Sci
24: 4, 695–710. doi: 10.1080/10095020.2021.1984183
Delegido J, J Verrelst, CM Meza, JP Rivera, L Alonso, and J Moreno (2013) A red-edge spectral
index for remote sensing estimation of green LAI over agroecosystems. European J Agron 46:
42–52. doi: 10.1016/j.eja.2012.12.001
Diner DJ, JV Martonchik, C Borel, SAW Gerstl, HR Gordon, Y Knyazikhin, R Myneni, B Pinty, and
MM Verstraete (2008) Multi-angle Imaging Spectro-Radiometer (MISR) level 2 surface retrieval
algorithm theoretical basis (JPL D-11401, Rev. E).
http://eospso.gsfc.nasa.gov/sites/default/files/atbd/ATB_L2Surface43.pdf
Fang H, F Baret, S Plummer, and G Schaepman-Strub (2019) An overview of global leaf area index
(LAI): Methods, products, validation, and applications. Rev Geophy 57: 739–799. doi:
10.1029/2018RG000608
Fieuzal R, C Marais Sicre, and F Baup (2017) Estimation of corn yield using multi-temporal optical
and radar satellite data and artificial neural networks. Int J Appl Earth Obs Geoinfo 57: 14–23.
doi: 10.1016/j.jag.2016.12.011
Gamon JA, J Peñuelas, and CB Field (1992) A narrow-waveband spectral index that tracks diurnal
changes in photosynthetic efficiency. Rem Sens Environ 41: 35–44.Gao J (2023) Remote Sensing of Natural Hazards. Boca Raton: CRC Press, 437 p.
Gao L, X Wang, BA Johnson, Q Tian, Y Wang, J Verrelst, X Mu, and X Gu (2020) Remote sensing
algorithms for estimation of fractional vegetation cover using pure vegetation index values: A
review. ISPRS J Photogram Rem Sens 159: 364–377. doi: 10.1016/j.isprsjprs.2019.11.018
García-Haro FJ, M Campos-Taberner, J Muñoz-Marí, V Laparra, F Camacho, J Sánchez-Zapero,
and G Camps-Valls (2018) Derivation of global vegetation biophysical parameters from
EUMETSAT polar system. ISPRS J Photogram Rem Sens 139: 57–74. doi:
10.1016/j.isprsjprs.2018.03.005
Geipel J, J Link, and W Claupein (2014) Combined spectral and spatial modeling of corn yield
based on aerial images and crop surface models acquired with an unmanned aircraft system. Rem
Sens 6: 10335–10355. doi: 10.3390/rs61110335
Gitelson A, Y Kaufman, R Stark, and D Rundquist (2002) Novel algorithms for remote estimation of
vegetation fraction. Rem Sens Environ 80: 76–87.
Gitelson A and M Merzlyak (1994) Quantitative estimation of chlorophyll-a using reflectance
spectra: Experiments with autumn chestnut and maple leaves. J Photochem Photobio B: Biol 22:
247–252.
Gitelson AA, A Vina, V Ciganda, DC Rundquist, and TJ Arkebauer (2005) Remote estimation of
canopy chlorophyll content in crops. Geophys Res Lett 32: L08403. doi:10.1029/2005GL022688
Gitelsona AA (2019) Remote estimation of fraction of radiation absorbed by photosynthetically
active vegetation: Generic algorithm for maize and soybean. Rem Sens Lett 10(3): 283–291. doi:
10.1080/2150704X.2018.1547445
Haboudane D, JR Miller, N Tremblay, PJ Zarco-Tejada, and L Dextraze (2002) Integrated narrow￾band vegetation indices for prediction of crop chlorophyll content for application to precision
agriculture. Rem Sens Environ 81: 416–426.
Haboudane D, JR Miller, E Pattey, PJ Zarco-Tejada, and IS Strachan (2004) Hyperspectral
vegetation indices and novel algorithms for predicting green LAI of crop canopies: Modeling and
validation in the context of precision agriculture. Rem Sens Environ 90: 337–352.
Haboudane D, N Tremblay, JR Miller, and P Vigneault (2008) Remote estimation of crop
chlorophyll content using spectral indices derived from hyperspectral data. IEEE Trans Geosci
Rem Sens 46: 423–437.
Hall SA, IC Burke, DO Box, MR Kaufmann, and JM Stoker (2005) Estimating stand structure using
discrete-return LiDAR: An example from low density, fire prone ponderosa pine forests. For Ecol
Manage 208(1–3): 189–209.
Hao Z, L Lin, CJ Post, EA Mikhailova, M Li, Y Chen, K Yu, and J Liu (2021) Automated tree-crown
and height detection in a young forest plantation using mask region-based convolutional neural
network (Mask R-CNN). ISPRS J Photogram Rem Sens 178: 112–123. doi:
10.1016/j.isprsjprs.2021.06.003
Heurich M (2008) Automatic recognition and measurement of single trees based on data from
airborne laser scanning over the richly structured natural forests of the Bavarian Forest National
Park. For Ecol Manag 255: 2416–2433
Heward H, AMS Smith, DP Roy, WT Tinkham, CM Hoffman, P Morgan, and KO Lannom (2013) Is
burn severity related to fire intensity? Observations from landscape scale remote sensing. Int JWildland Fire. 22: 910–918. doi: 10.1071/WF12087
Holmgren J, M Nilsson, and H Olsson (2003) Estimation of tree height and stem volume on plots
using airborne laser scanning. For Sci 49: 419–428.
Hosseini M, H McNairn, A Merzouki, and A Pacheco (2015) Estimation of Leaf Area Index (LAI) in
corn and soybeans using multi-polarization C- and L-band radar data. Rem Sens Environ 170: 77–
89.
Hou W, J Su, W Xu, and X Li (2020) Inversion of the fraction of absorbed photosynthetically active
radiation (FPAR) from FY-3C MERSI data. Rem Sens 12, 67. doi: 10.3390/rs12010067
Huang D, Y Knyazikhin, W Wang, DW Deering, P Stenberg, N Shabanov, B Tan, and RB Myneni
(2008) Stochastic transport theory for investigating the three-dimensional canopy structure from
space measurements. Rem Sens Environ 112(1): 35–50. doi: 10.1016/j.rse.2006.05.026
Hudak AT, EK Strand, LA Vierling, JC Byrne, JUH Eitel, S Martinuzzi, and MJ Falkowski (2012)
Quantifying aboveground forest carbon pools and fluxes from repeat LiDAR surveys. Rem Sens
Environ 123: 25–40. doi: 10.1016/j.rse.2012.02.023
Hunt ER, PC Doraiswamy, JE McMurtrey, CST Daughtry, EM Perry, and B Akhmedova (2013) A
visible band index for remote sensing leaf chlorophyll content at the canopy scale. Int J Appl
Earth Obs Geoinfo 21: 103–112. doi: 10.1016/j.jag.2012.07.020
Hyyppä J, M Schardt, H Haggrén, B Koch, U Lohr, R Paananen, H Scherrer, H Luukkonen, M
Ziegler, H Hyyppä, U Pyysalo, H Friedländer, J Uuttera, S Wagner, M Inkinen, A Wimmer, A
Kukko, E Ahokas, and M Karjalainen (2001) HIGH-SCAN: The first European-wide attempt to
derive single-tree information from laserscanner data. Photogram J Finland 17: 58–68.
Jia K, S Liang, S Liu, Y Li, Z Xiao, Y Yao, B Jiang, X Zhao, X Wang, S Xu, and J Cui (2015)
Global land surface fractional vegetation cover estimation using general regression neural
networks from MODIS surface reflectance. IEEE Trans Geosci Rem Sens 53(9): 4787–4796. doi:
10.1109/TGRS.2015.2409563
Johnston JM, MJ Wheatley, MJ Wooster, R Paugam, GM Davies, and KA DeBoer (2018) Flame￾front rate of spread estimates for moderate scale experimental fires are strongly influenced by
measurement approach. Fire 1: 16. doi: 10.3390/fire1010016
Johnston JM, MJ Wooster, R Paugam, X Wang, TJ Lynham, and LM Johnston (2017) Direct
estimation of Byram’s fire intensity from infrared remote sensing imagery. Int J Wildland Fire 26:
668–684. doi: 10.1071/WF16178
Jordan CF (1969) Derivation of leaf-area index from quality of light on the forest floor. Ecology 50:
663–666.
Khaki S, H Pham, and L Wang (2021) Simultaneous corn and soybean yield prediction from remote
sensing data using deep transfer learning. Sci Rep 11: 11132. doi: 10.1038/s41598-021-89779-z
Koma Z, A Zlinszky, L Bekő, P Burai, AC Seijmonsbergen, and WD Kissling (2021) Quantifying
3D vegetation structure in wetlands using differently measured airborne laser scanning data. Ecol
Indicators 127: 107752. doi: 10.1016/j.ecolind.2021.107752
Kumar P and AP Krishna (2019) InSAR-Based tree height estimation of hilly forest using
multitemporal Radarsat-1 and Sentinel-1 SAR data. IEEE J Sel Topics Appl Earth Obs Rem Sens
12(12): 5147–5152. doi: 10.1109/JSTARS.2019.2963443Lee AC and RM Lucas (2007) A LiDAR-derived canopy density model for tree stem and crown
mapping in Australian forests. Rem Sens Environ 111:493–518
Li Z and X Guo (2011) A suitable vegetation index for quantifying temporal variation of leaf area
index (LAI) in semiarid mixed grassland. Can J Rem Sens 36(6): 709–721. doi: 10.5589/m11-002
Liang S (2007) Recent developments in estimating land surface bio-geophysical variables from
optical remote sensing. Prog Phys Geogr 31(5): 501–516.
Liu J, E Pattey, and G Jégo (2012) Assessment of vegetation indices for regional crop green LAI
estimation from Landsat images over multiple growing seasons. Rem Sens Environ 123: 347–358.
doi: 10.1016/j.rse.2012.04.002
Liu K, J Wang, W Zeng, and J Song (2017) Comparison and evaluation of three methods for
estimating forest above ground biomass using TM and GLAS Data. Rem Sens 9: 341. doi:
10.3390/rs9040341
Lu B, Y He, and A Tong (2015) Evaluation of spectral indices for estimating burn severity in
semiarid grasslands. Int J Wildland Fire 25(2): 147–157. doi: 10.1071/WF15098
Ma C, M Liu, F Ding, C Li, Y Cui, W Chen, and Y Wang (2022) Wheat growth monitoring and yield
estimation based on remote sensing data assimilation into the SAFY crop growth model. Sci Rep
12: 5473. doi: 10.1038/s41598-022-09535-9
Ma L, S Wang, J Chen, B Chen, L Zhang, L Ma, M Amir, L Sun, G Zhou, and Z Meng (2020)
Relationship between light use efficiency and photochemical reflectance index corrected using a
BRDF model at a subtropical mixed forest. Rem Sens 12: 550. doi: 10.3390/rs12030550
Magnussen S, and P Boudewyn (1998) Derivations of stand heights from airborne laser scanner data
with canopy-based quantile estimators. Can J For Res 28: 1016–1031.
Marburg AE, FE Carswell, MG St John, RJ Holdaway, AB Rose, and I Jacobs (2013) Implications
of Experimental Design on the Detection of Herbivore Impacts on Carbon Stocks in a
Broadleaved-Hardwood Forest. Wellington: NZ Department of Conservation, 22 p.
Mason N, P Beets, I Payton, L Burrows, R Holdaway, and F Carswell (2014) Individual-based
allometric equations accurately measure carbon storage and sequestration in shrublands. Forests
5(2): 309–324.
Metternicht G (2003) Vegetation indices derived from high-resolution airborne videography for
precision crop management. Int J Rem Sens 24: 2855–2877.
Meyer GE, T Mehta, M Kocher, D Mortensen, and A Samal (1998) Textural imaging and
discriminant analysis for distinguishing weeds for spot spraying. Trans ASAE 41: 1189–1197.
Morresi D, R Marzano, E Lingua, R Motta, and M Garbarino (2022) Mapping burn severity in the
western Italian Alps through phenologically coherent reflectance composites derived from
Sentinel-2 imagery. Rem Sens Environ 269: 112800.
Morsdorf F, O Frey, E Meier, KI Itten, and B Allgȍwer (2008) Assessment of the influence of flying
altitude and scan angle on biophysical vegetation products derived from airborne laser scanning.
Int J Rem Sens 29: 1387–1406.
Næsset E (1997) Determination of mean tree height of forest stands using airborne laser scanner
data. ISPRS J Photogram Rem Sens 52: 49–56.
Næsset E (2009) Influence of terrain model smoothing and flight and sensor configurations on
detection of small pioneer trees in the boreal-alpine transition zone utilizing height metricsderived from airborne scanning lasers. Rem Sens Environ 113: 2210–2223.
Næsset E, and T Økland (2002) Estimating tree height and tree crown properties using airborne
scanning laser in a boreal nature reserve. Rem Sens Environ 79: 105–115
Nilsson M (1996) Estimation of tree heights and stand volume using an airborne LiDAR system.
Rem Sens Environ 56: 1–7.
Ojoatre S, C Zhang, YA Hussin, HE Kloosterman, and MH Ismail (2019) Assessing the uncertainty
of tree height and aboveground biomass from terrestrial laser scanner and hypsometer using
airborne LiDAR data in tropical rainforests. IEEE J Sel Topics Appl Earth Obs Rem Sens 12(10):
4149–4159. doi: 10.1109/JSTARS.2019.2944779
Pachavo G, and A Murwira (2014) Remote sensing net primary productivity (NPP) estimation with
the aid of GIS modelled shortwave radiation (SWR) in a Southern African Savanna. Int J Appl
Earth Obs Geoinfo 30: 217–226. doi: 10.1016/j.jag.2014.02.007
Paugam R, MJ Wooster, and G Roberts (2013) Use of handheld thermal imager data for airborne
mapping of fire radiative power and energy and flame front rate of spread. IEEE Trans Geosci
Rem Sens 51(6): 3385–3399. doi: 0.1109/TGRS.2012.2220368
Pazhanivelan S, V Geethalakshmi, R Tamilmounika, NS Sudarmanian, R Kaliaperumal, K
Ramalingam, AP Sivamurugan, K Mrunalini, MK Yadav, and ED Quicho (2022) Spatial rice yield
estimation using multiple linear regression analysis, semi-physical approach and assimilating
SAR satellite derived products with DSSAT crop simulation model. Agron 12(9): 2008. doi:
10.3390/agronomy12092008
Pinty B, I Andredakis, M Clerici, T Kaminski, M Taberner, MM Verstraete, N Gobron, S Plummer,
and LL Widlowski (2011) Exploiting the MODIS albedos with the Two-stream Inversion Package
(JRC-TIP): 1. Effective leaf area index, vegetation, and soil properties. J Geophy Res 116:
D09105. doi: 10.1029/2010JD015372
Prathumchai K, M Nagai, NK Tripathi, and N Sasaki (2018) Forecasting transplanted rice yield at
the farm scale using moderate-resolution satellite imagery and the aquacrop model: A case study
of a rice seed production community in Thailand. ISPRS Int J Geo-Info 7. doi:
10.3390/ijgi7020073
Qin H, Wang C, Zhao K, and Xi X (2018) Estimation of the fraction of absorbed
01photosynthetically active radiation (fPAR) in maize canopies using LiDAR data and
hyperspectral imagery. PLoS ONE 13(5): e0197510. doi: 10.1371/journal.pone.0197510
Rahlf J, J Breidenbach, S Solberg, E Næsset, and R Astrup (2014) Comparison of four types of 3D
data for timber volume estimation. Rem Sens Environ 155: 325–333. doi:
10.1016/j.rse.2014.08.036
Riaño D, E Chuvieco, S Condes, J Gonzalez-Matesanz, and SL Ustin (2004) Generation of crown
bulk density for Pinus sylvestris L. from LiDAR. Rem Sens Environ 92: 345–352.
Roberts JW, S Tesfamichael, M Gebreslasie, J van Aardt, and FB Ahmed (2007) Forest structural
assessment using remote sensing technologies: An overview of the current state of the art. South
Hemisph For J 69:183–203.
Rondeaux G, M Steven, and F Baret (1996) Optimization of soil-adjusted vegetation indices. Rem
Sens Environ 55: 95–107.Rouse JW, RH Haas, JA Schell, and DW Deering (1974) Monitoring vegetation systems in the Great
Plains with ERTS. NASA Spec. Publ. 351: 309.
Ruecker G, D Leimbach, and J Tiemann (2021) Estimation of Byram’s fire intensity and rate of
spread from spaceborne remote sensing data in a savanna landscape. Fire 4: 65. doi:
10.3390/fire4040065
Schlerf M and C Atzberger (2006) Inversion of a forest reflectance model to estimate structural
canopy variables from hyperspectral remote sensing data. Rem Sens Environ 100(3): 281–294.
doi: 10.1016/j.rse.2005.10.006
Schwendenmann L and ND Mitchell (2014) Carbon accumulation by native trees and soils in an
urban park, Auckland. NZ J Ecol 38(2): 213–220.
Setiyono TD, ED Quicho, L Gatti, M Campos-Taberner, L Busetto, F Collivignarelli, FJ García￾Haro, M Boschetti, NI Khan, and F Holecz (2018) Spatial rice yield estimation based on MODIS
and Sentinel-1 SAR data and ORYZA crop growth model. Rem Sens 10: 293. doi:
10.3390/rs10020293
Shannon CE (1948) A mathematical theory of communication. Bell Syst Tech J 27: 379–423.
Shi, Y (2022) Aboveground biomass change of the alpine meadow on the Qinghai-Tibet Plateau
under climate warming. PhD thesis, University of Auckland, 157 p.
Shi Y, J Gao, G Brierley, X Li, GLW Perry, and T Xu (2023) Improving the accuracy of models to
map alpine grassland above-ground biomass using Google earth engine. Grass and Forage Sci
78(2): 237–253. doi: 10.1111/gfs.12607
Shi Y, J Gao, X Li, J Li, DM dela Torre, and GJ Brierley (2021) Improved estimation of
aboveground biomass of disturbed grassland through including bare ground and grazing intensity.
Rem Sens 13: 2105. doi: 10.3390/rs13112105
Son NT, CF Chen, CR Chen, LY Chang, and SH Chiang (2016) Rice yield estimation through
assimilating satellite data into a crop simulation model. Intern Archives of Photogra, Rem Sens
and Spatial Info Sci, vol XLI-B8, XXIII ISPRS Congress, 12–19 July 2016, Prague, Czech
Republic, pp. 993–996.
Swayze NC, WT Tinkham, JC Vogeler, and AT Hudak (2021) Influence of flight parameters on
UAS-based monitoring of tree height, diameter, and density. Rem Sens Environ 263, 112540. doi:
10.1016/j.rse.2021.112540
Takahashi T, K Yamamoto, Y Senda, and M Tsuzuku (2005) Predicting individual stem volumes of
sugi (Cryptomeria japonica D. Don) plantations in mountainous areas using small-footprint
airborne LiDAR. J For Res 10: 305–312.
Tan C, D Wang, J Zhou, Y Du, M Luo, Y Zhang, and W Guo (2018) Remotely assessing Fraction of
Photosynthetically Active Radiation (FPAR) for wheat canopies based on hyperspectral
vegetation indexes. Front Plant Sci 9: 776. doi: 10.3389/fpls.2018.00776
Thomas V, P Treitz, JH McCaughey, and I Morrison (2006) Mapping stand-level forest biophysical
variables for a mixedwood boreal forest using LiDAR: An examination of scanning density. Can J
For Res 36: 34–47.
Trotter CM, JR Dymond, and CJ Goulding (1997) Estimation of timber volume in a coniferous
plantation forest using Landsat TM. Int J Rem Sens 18: 10, 2209–2223. doi:
10.1080/014311697217846van Leeuwen M and M Nieuwenhuis (2010) Retrieval of forest structural parameters using LiDAR
remote sensing. Eur J For Res 129: 749–770. doi 10.1007/s10342-010-0381-4
Verger A, F Baret, and M Weiss (2011) A multisensor fusion approach to improve LAI time series.
Rem Sens Environ 115(10): 2423–2750.
Verhoef W (1984) Light scattering by leaf layers with application to canopy reflectance modeling:
The SAIL model. Rem Sens Environ 16: 125–141.
Verrelst J, G Camps-Valls, J Muñoz-Marí, JP Rivera, F Veroustraete, JGPW Clevers, and J Moreno
(2015a) Optical remote sensing and the retrieval of terrestrial vegetation bio-geophysical
properties – A review. ISPRS J Photogram Rem Sens 108: 273–290. doi:
10.1016/j.isprsjprs.2015.05.005
Verrelst J, JP Rivera, F Veroustraete, J Muñoz-Marí, JGPW Clevers, G Camps-Valls, and J Moreno
(2015b) Experimental Sentinel-2 LAI estimation using parametric, non-parametric and physical
retrieval methods – A comparison. ISPRS J Photogram Rem Sens 108: 260–272. doi:
10.1016/j.isprsjprs.2015.04.013
Verrelst J, L Alonso, G Camps-Valls, J Delegido, and J Moreno (2012) Retrieval of vegetation
biophysical parameters using Gaussian process techniques. IEEE Trans Geosci Rem Sens 50:
1832–1843.
Verrelst J, L Alonso, J Rivera Caicedo, J Moreno, and G Camps-Valls (2013) Gaussian process
retrieval of chlorophyll content from imaging spectroscopy data. IEEE J Sel Topics Appl Earth
Obs Rem Sens 6(2): 867–874.
Vohland M, S Mader, and W Dorigo (2010) Applying different inversion techniques to retrieve stand
variables of summer barley with PROSPECT+SAIL. Int J Appl Earth Obs Geoinfo 12(2): 71–80.
doi: 10.1016/j.jag.2009.10.005
Wang F, MK Heenkenda, and JT Freeburn (2022) Estimating tree Diameter at Breast Height (DBH)
using an iPad Pro LiDAR sensor. Rem Sens Lett 13(6): 568–578. doi
10.1080/2150704X.2022.2051635
Wang J (2019) Estimation of Vegetation Carbon Stocks in the Urban Environment Using Geo￾Informatics Methods. PhD thesis, University of Auckland, 180 p.
Wang J, R Sun, H Zhang, Z Xiao, A Zhu, M, Wang, T Yu, and K Xiang (2021) New global MuSyQ
GPP/NPP remote sensing products from 1981 to 2018. IEEE J Sel Topics Appl Earth Obs Rem
Sens 14: 5596–5612. doi: 10.1109/JSTARS.2021.3076075
Wang V, and J Gao (2019) Importance of structural and spectral parameters in modelling the
aboveground carbon stock of urban vegetation. Int J Appl Earth Obs Geoinfo 78: 93–101. doi:
10.1016/j.jag.2019.01.017
Wang V, J Gao, and L Schwendenmann (2020) Assessing changes of urban vegetation cover and
aboveground carbon stocks using LiDAR and Landsat imagery data in Auckland, New Zealand.
Int J Rem Sens 41(6): 2140–2158. doi: 10.1080/01431161.2019.1685716
Wang X, G Shao, H Chen, BJ Lewis, G Qi, D Yu, L Zhou, and L Dai (2013) An application of
remote sensing data in mapping landscape-level forest biomass for monitoring the effectiveness of
forest policies in Northeastern China. Environ Manage 52(3): 612–20. doi: 10.1007/s00267-013-
0089-67 Quantification in the Hydrosphere
DOI: 10.1201/9781003517504-9
Water makes up more than three quarters of the Earth’s surface. It plays a
commanding role in many atmospheric processes. It supplies critical and abundant
resources for human survival and well-being, but has been increasingly polluted by
human activities. It is important to quantify water quality to assess ocean health and
the suitability of freshwater for diverse uses. Quantification in the hydrosphere
covered in this chapter encompasses freshwater ecosystems and ocean waters, but is
exclusive of moisture in the atmosphere. Neither covered is the quantification of
columnar atmospheric water vapor, columnar cloud liquid water content, and cloud
(rain rate quantification to be covered in Section 8.4.2). In order to keep this chapter
within a reasonable length, tidal height and mean tidal energy flux, and wave energy
density are also excluded from discussion. All quantification related to spatial extent
is not included in the content (e.g., area of algal blooms), nor is the use of field
spectral data in sensing in-water constituents as it does not yield a spatial view of the
quantified target, such as the retrieval of phycocyanin concentration from field
reflectance spectra.
Compared with the terrestrial surface, water is much more challenging to quantify
because of its tendency to move horizontally and mix vertically. Retrieval of water
constituents or optical properties is realized via inversion of the water-leaving
reflectance spectrum, measured at TOA by ocean color sensors. This quantification is
not just confined to the skin depth as the solar radiation is able to penetrate
transparent waters, especially Case I, with the depth of penetration being a function
of water clarity and the radiation wavelength. In Case I waters, the concentration of
the targeted in-water constituents may be low, and the water surface may be choppy
in stormy weather. It introduces an additional component of energy reflected off the
water surface to the at-sensor radiance that may exceed the signal of the in-water7.1
7.1.1
target. For this reason, oceanic water parameters are quantified at a much coarser
spatial resolution than its inland counterpart, feasible only from purpose-designed
satellite sensors. Of the two types of water, fresh Case II waters’ properties are much
more challenging and demanding to quantify than their open water counterpart using
satellite data, especially at a fine spatial resolution and a high accuracy level because
they are small and spatially complex, requiring high fidelity spectroradiometry, and
are best described with biophysical variables derived from high spectral resolution
data.
This chapter starts from elucidating the spectral behavior of water, followed by a
review of suitable remote sensing data for quantification in the hydrosphere. The
discussion then shifts to the quantification of water clarity and bathymetry in both
inland and coastal areas using imagery and non-imagery LiDAR data. The third part
of this chapter explains how to quantify water surface features such as wave height,
temperature, and surface flow velocity. The fourth component of the chapter
elaborates the quantification of in-water inorganic substances, including suspended
sediments and salinity. Section five of this chapter expounds how diverse in-water
biochemical parameters and contents (including eutrophication state) are retrieved
from remote sensing data using various analytical methods. Featured prominently in
the discussion is the quantification of Chl-a concentrations, including its spectral
behavior and the best analytical method for spectral data manipulation.
FUNDAMENTALS
Spectral Signature of Water
Water interacts with the incoming solar radiation in more ways than with the
terrestrial surface because of its transparency and impurity content. In general, the
incident energy is partitioned into five parts: direct reflection off the water surface,
downward transmission through the water, scattering by in-water impurities,
absorption of a large portion of the incident energy by water, and possible reflection
by the seafloor and bed in shallow waters (Figure 7.1). The amount of absorbed
energy is not directly useful to quantify solid particulates but in-water biochemical
components in the hydrosphere. The amount of absorption is related to the
wavelength of the incoming radiation. Those beyond the infrared portion of the
spectrum are totally absorbed, resulting in nothing being reflected. Consequently, the
amount of energy reflected from water is rather low (<5%) over the visible portion of
the spectrum (Figure 3.4). Whether a targeted in-water parameter can be quantifiedremotely depends on its spectral behavior. Spectral reflection of water is dependent
strongly on wavelength, with the peak reflectance occurring around 0.5–0.6 μm,
subject to the color of the water (Figure 7.1). Pure, Case I waters have a short peak
reflectance around 0.45 μm while inland waters with abundant algae tend to be
greenish, causing the reflectance to peak at around 0.54 μm. Such a reflectance
pattern is quite different from that of vegetation and soil. Thus, it is rather easy to
separate water from other covers on the same image before the subtle variations in
water are further utilized to quantify various aspects of it, such as bathymetry and in￾water constituents. The spectral reflection and absorption characteristics shown in
Figure 7.1 are indicative of the best spectral wavelengths (bands) for quantifying
various water parameters and their content. The typical absorption wavelength range
can guide the selection of the most useful spectral bands for their retrieval from
remotely sensed data.
FIGURE 7.1 Spectral reflectance of clear water laden with various types of in-water
substances. (Modified from Lin et al., 2018, © Taylor & Francis.)
The spectral reflectance pattern of typical waters in Figure 7.1 is affected by a variety
of factors, the most critical being surface roughness, water depth, in-water
constituents, and bottom effects. Surface roughness affects not only the manner of
reflection but also its intensity. If the surface is smooth or even flat under a calm
weather condition, the reflection occurs as mirror (Figure 3.3a) or specular (Figure7.1.2
7.1.2.1
3.3b). When the surface is rough, the incident radiation is mostly diffused. Water
depth affects the reflectance pattern as the deeper the water, the more energy is
transmitted and absorbed, leading to less energy reflected back towards the sensor.
There exists an inverse relationship between water depth and the intensity of the
returned signal from water. However, the exact depth at which depth ceases to have
any signal is compounded by other factors, such as water impurities. In shallow
nearshore and inland waters of a sufficient transparency, the incident energy is able to
penetrate the entire water column to reach the seafloor (Figure 3.5). In this case, the
spectral signature of inland water is further confounded by bottom reflectance,
causing their spectral signature to be much weaker than inland water, and thus more
challenging to quantify, especially at a fine spatial resolution. In-water constituents
include inorganic and organic substances, such as sediments and algae. Sediments
mainly scatter the incoming radiation, but some algae absorb it. The spectral
variations of water-leaving radiance are related to the concentration of phytoplankton
pigments, colored or chromophoric dissolved organic matter (CDOM), and
suspended particulate matter, which lays the physical foundation for quantifying the
hydrosphere using remote sensing.
Suitable Remote Sensing Data
Space-borne Data
The quantification of oceanographic variables requires tailored sensors because of
their spectral peculiarity and the interference of the sensing environment (e.g.,
weather). This task is ideally accomplished using data from OceanSat, a series of
earth observation satellites launched and operated by Indian Space Research
Organization. At present, two are still operational, Oceansat-2 launched on 23
September 2009, and Oceansat-3 launched on 26 November 2022, both dedicated to
oceanographic and atmospheric studies. The Oceansat-3 mission inherits the legacy
of Oceansat-2 data by operating in a sun-synchronous orbit of 720 km in altitude,
with an inclination of 98.28°. It completes one orbital revolution in 99.31 minutes at
a local solar time of 12:00 pm (descending node). Its payload comprises the OceanSat
Scatterometer (OSCAT)-3, the Ocean Color Monitor (OCM)-3, and a Sea Surface
Temperature Monitor (SSTM). OSCAT-3 images have a spatial resolution of 25 km,
with a swath width of 1,440 km (6,000 pixels per scan line). This radar scatterometer
operates in the Ku band, with a frequency of 13.515 GHz, destined to measure sea￾level wind speeds and vectors. OCM-3 is a push broom scanner acquiring VNIR7.1.2.2
7.1.2.3
bands over the wavelength range of 0.4–1.3 µm at a spatial resolution of 360 m.
OCM-3 data are suitable for estimating phytoplankton concentration and assessing
primary productivity. SSTM is a multi-purpose VIS-IR imaging radiometer with two
bands, both having a spatial resolution of 1,080 m, and a swath width of 1,440 km.
Their radiometric resolution is sufficiently fine to enable SST to be detected down to
0.15 K (0.15°C) at a mean temperature of approximately 300 K (27°C) (Sarkar and
Patel, 2021).
UAV Data
Owing to the low-flying altitude, UAV data are especially advantageous in studying
small water bodies, particularly narrow channels. Drone images, either multispectral
or hyperspectral, do not need to be corrected for the atmospheric effects because the
atmospheric interference is insignificant owing to the low-flying altitude of <120 m.
The scattered irradiance from the atmosphere is negligible at a low altitude of 20–100
m. At such a low altitude, the sun glint becomes more enhanced and should be
avoided. With the avoidance of sun glint and typical surface reflection coefficients on
the order of 5–10%, the contribution of specular and diffuse reflection from the water
surface is assumed to be insignificant. Drone images captured with a consumer-grade
camera can be used to assess physical parameters of water bodies such as sediment
plumes and concentration, but are unable to retrieve in-water biochemical parameters
such as Chl-a because of their narrow range of spectral sensitivity and coarse spectral
resolution. Quantitative estimation of surface Chl-a in coastal waters based on surface
reflectance measurement from drone images is possible if they are taken with a five￾band multispectral camera (Chan et al., 2022).
Hyperspectral Data
Hyperspectral data are critical to the success of quantitative estimation of in-water
substances that have a narrow spectral range of characteristic absorption of the
incoming solar radiation. In the presence of multiple in-water constituents (e.g., algal
blooms with accompanying pigments), which is rather common with Case II waters,
their spectral patterns may confound each other in some spectral ranges. They can be
isolated from each other at a spectral resolution measured by a few microns. Such a
spectral range falls well below the bandwidth of most multispectral bands. The
characteristic absorption trait can only be detected from hyperspectral images that
offer several continuous narrow bands, and that can provide crucial clues to quantify
multiple color-producing agents accurately. The large-scale and continuous
hyperspectral resolution enables the recording of detailed and variable characteristics7.1.3
of inland waters, such as their eutrophic state and development based on water
eutrophication parameters, for example, Chl-a and phycocyanin (PC). The advent of
hyperspectral sensors or imaging spectroscopy (e.g. NASA Hyperion, NRL HICO,
ESA PROBA, and upcoming German DLR EnMAP, Japanese HIUSI, and NASA
HyspIRI) has expanded hyperspectral applications to the hydrosphere. Given that
space-borne hyperspectral data cover only a narrow strip of the sensed area, they are
suited to only local area applications (e.g., nearshore and inland waters of complex
optical properties). These sensors can also be mounted on aircraft and UAVs, and
therefore, can supply images of a finer spatial and spectral resolution for monitoring
small inland water bodies such as reservoirs and rivers (Song et al., 2012). The usual
across-track IFOV of 1 mrad corresponds to a pixel size of 1 m on the ground at a
flight height of 1,000 m, covering a swath width of only 1,000 m. Hyperspectral data
can be obtained from both in situ and Airborne Imaging Spectrometer for Application
(AISA) measurements. In situ data are acquired from non-imaging spectrometers.
They can demonstrate the optimal wavelength ranges and possible best combinations
of multiple bands, but the results may not be replicable with space-borne data.
Assisted with in situ data, airborne hyperspectral data can provide quantitative
information on the distribution and concentration of cyanobacteria, suspended matter,
and transparency at high accuracy (Song et al., 2012).
Of all hyperspectral sensors, AISA records images in 62 bands with a band width
of 7–8 nm. They offer abundant options in selecting the most sensitive spectral
variables from a pool of 100 spectral variables formed by band ratios (50), narrow￾band spectra (20), and derivative (30), and in calculating correlation with narrow￾band spectra reflectance, derivatives, and all possible band ratios.
Theoretical Grounding
The ocean color signal recorded in satellite imagery is quantitatively related to
remotely sensed reflectance just above the sea surface , defined as the ratio of
upwelling radiance to downwelling irradiance just above the sea surface or:
(7.1)
where r = mean specular reflectance of the sea surface (e.g., ∼3%); n = index of
refraction of seawater (1.341), = spectral upwelling radiance just below the sea
surface that can be measured by a profiling spectroradiometer; = incident spectral
irradiance just below the sea surface measured by a similar radiometer; and = IOPs
of absorption and backscattering coefficients, respectively; f/Q = factor accountingfor the bidirectional structure of the upward radiance field (≈0.0949); and k (≈0.54)
accounts for the transmission and reflection of the air-sea interface (Mobley, 1994). 
can be converted to the normalized water-leaving radiance nLw by multiplying it by
the mean solar irradiance at wavelength λ. It is more stable than Rrs (Shanmugam,
2011).
The commonly used analytical methods for in-water parameter retrieval are
mostly semi-analytical, including quasi-analytical algorithm (QAA) and the
generalized IOP algorithm (GIOP). GIOP is a semi-analytical ocean color algorithm
implemented in NASA’s l2gen ocean color processing code and distributed as part of
the SeaDAS image processing and analysis software suite
(http://seadas/gsfc.nasa.gov/). GIOP has a modular structure that allows end-users to
parameterize a semi-analytical algorithm (SAA) at runtime from a range of built-in
options. Thus, GIOP provides a platform for development and evaluation of various
SAA configurations. QAA is a semi-physical method as it simplifies full physical
models using regression equations or with approximations to derive the spectral
absorption α(λ) and backscattering coefficients bb(λ) in analytically inverting Rrs(λ).
QAA starts with the calculation of the total absorption coefficient (α) at a reference
wavelength (λ0), and then propagates the calculation to other wavelengths.
Component absorption coefficients (contributions by detritus/gelbstoff and
phytoplankton pigments) are further algebraically decomposed from the total
absorption spectrum.
Both the GIOP and QAA algorithms follow the theory that sea surface reflectance
at a given wavelength is proportional to the backscattering coefficient, and inversely
proportional to the absorption coefficient. Each has various assumptions, empirical
parameterizations, and inversion strategies to resolve IOPs and partition them into
their constituents. These include the total backscattering coefficient bbt(λ),
backscattering by particles bbp(λ), absorption by total particles, by phytoplankton, and
by the combination of non-algal particles and CDOM, adg(λ) = ad(λ) + ag(λ), where
ad(λ) is non-algal (or detrital) absorption. The strength of these algorithms is their
grounding on theoretical models about how the light field is affected by the inherent
properties of water, but can only retrieve the IOPs at those wavebands for which
Rrs(λ) is measured (i.e., they do not extend into the UV for the current and historical
suite of satellite sensors).
Table 7.1
Accuracy achievable in estimating SDD from in situ collected hyperspectral data based on various methods
(models)7.2
7.2.1
Models R² RMSE (m) Reference
0.6 0.31 Yang et al. (2005)
Quasi-analytical algorithm 0.82 0.58 Liu et al. (2020)
0.71 0.61 Yu et al. (2015)
- 0.81 0.09 Lyu et al. (2022)
Source: Lyu et al. (2022), open access.
As shown in Table 7.1, QAA is more accurate than other single band, two-band, and
three-band models. In comparing model accuracy, it should be borne in mind that
seasonality and geographic location both affect the range of performance because of
training and validation with different datasets. The number of samples, the range of
samples, and the composition of planktonic algae and suspended matter in the
datasets all change spatiotemporally, resulting in different optical properties of water
bodies. However, most variants of QAA suffer from moderate to high negative IOP
predictions when applied to tropical eutrophic waters.
WATER CLARITY AND BATHYMETRY
Clarity and Secchi Disc Depth
Water clarity is a reliable parameter for quantifying the eutrophic status of inland
waters owing to its strong relationship with Chl-a, total suspended matter, and
nutrients. Information on water clarity can support inland water management and
decision-makers for improving water quality. Water clarity is affected by suspended
sediment, CDOM, planktonic algae, and zooplankton. Closely related to water
transparency or visibility, it is commonly expressed as Secchi disc depth (SDD),
determined using the Secchi disc. It is a white, matte circle-shaped plate with a
standardized diameter and white color designed to measure the transparency of water.
SDD refers to the distance from the water surface to a level where the disc is still
visible after it has been immersed in the water on a graduated line or a rod with a
centimeter scale, usually deployed from a boat or ship. SDD is considered a reliable
proxy for water quality in remote sensing of the hydrosphere. It represents the depth
at which the signal of water cannot be detected from remotely sensed data.
The quantification of SDD from remotely sensed data relies on the association
between water-leaving spectral signal recorded on satellite images and concurrently
measured in situ Secchi depth. The captured water-leaving radiance primarily reflects
optically active constituents, such as phytoplankton, non-algal particles, and CDOM
interacting with pure water. So their effects must be removed via a bio-optical model.During inversion of this bio-optical model, SDD is linked to the light attenuation
coefficient (Kd) that is closely associated with optically active constituents. However,
variables in the bio-optical model are notoriously thorny to parameterize properly due
to the difficulty of collecting specific IOPs in the field. In contrast, (semi-)empirical
models depicting SDD as a function of remotely sensed signals are not subject to this
restriction, and have been widely used for quantification.
SDD is best estimated using optical bands that have a stronger penetration
capability than longwave bands. As indicated in Table 7.1, the three-band model is
more accurate than the two- and one-band models, achieving the highest R² of 0.81,
much higher than 0.71 and 0.6 from the other two models (just inferior to QAA).
However, these results may not be replicated with multispectral bands that have a
much broader bandwidth than hyperspectral data. SSD bears a strong association with
the spectral reflectance captured in Landsat OLI visible bands, yielding a low RMSE
of 63 cm between measured and estimated SDD for lakes (Song et al., 2020). The
closeness of the association varies with spectral ranges. Green (R2 = 0.83) and red
(R2 = 0.92) bands are better than blue (R2 = 0.62) and NIR (R2 = 0.37) bands due to
strong absorption by water. But the red/blue ratio exhibits a strong and stable
relationship with SDD. Thus, Landsat OLI imagery can be used to map SDD in
nearshore waters (Figure 7.2). The accuracy of retrieving SSD from visible bands is
complicated by whether the atmospheric effects are calibrated. The model based on
calibrated TOA reflectance from which the contributions of both Rayleigh and
aerosol scattering have been eliminated is the best performer, achieving the highest
R2 value of 0.75 (Song et al., 2020). The accuracy is lowered slightly to R2 = 0.69 if
the reflectance is not calibrated for the scattering effects.FIGURE 7.2 Distribution of Secchi disk depth values (SDDs) of nearshore waters in the
Chabahar Bay of South Iran, determined from a Landsat 8 image of 22 February, 2014.
(Kabiri, 2017, with permission (5761250456206) from Springer.)
Of the various Landsat TM bands, bands 1 and 3 are the best predictors of SDD in a
logarithmic model in the form of (Zhao et al., 2011):
(7.2)
where ai (i = 0, 1, 2,) = coefficients to be determined via regression analysis; Bi =
reflectance value of the ith Landsat TM band. Owing to its close correlation with the
corresponding field measurements, this model is considered appropriate for
predicting SDD in lake waters. If estimated from atmospherically corrected
reflectance values of Landsat-8 OLI bands, SDD is best estimated from the linear
combination of three bands (B1, B2 and B3) and the band ratio of B3/B1 and B2/B1
in the form of:
(7.3)7.2.2
7.2.2.1
This model is the best with the highest R2 = 0.866 and the lowest RMSE = 0.919 for
estimating SDD values in nearshore coastal waters of Chabahar Bay in southeastern
Iran (Kabiri, 2017). This R² value is higher than those in Table 7.1 but the RMSE is
also higher, due to the influence of the sensing environment, coarse spatial resolution
of 30 m, and the spatial variability of SDD.
Bathymetry
Bathymetry, or depth of water, refers to the distance from the water surface to the
floor of an inland water body or seabed of oceans. Bathymetric data are crucial to
marine shipping and navigation, seafloor profiling, and studying biological
oceanography, beach erosion, and sea-level rise. Accurate information on nearshore
bathymetry is important for monitoring underwater topography and movement of
deposited sediments, and for nautical charting in support of navigation. Such
information is also critical to port management, dredging operations, and to
predicting channel infill and sediment budgets. Bathymetry has been conventionally
mapped using vessel-based acoustic echo sounding. This method is able to generate
accurate point measurements or depth profiles along transects but is unsuited for
nearshore waters as shallow coastal waters are hazardous to navigate, especially at
low tides. Remote sensing of bathymetry is not subject to this restriction. It can be
accomplished using two broad categories of non-imaging and imaging approaches.
Imaging methods attempt to estimate water depth from pixel values of an image, such
as the ratio of the blue and green bands of optical satellite imagery. The non-imagery
method (active sensing) as exemplified by LiDAR detects the attenuated signal
bounced back from the water. Its intensity is inversely related to water depth. Each
method has its own detectable depth, accuracy, strengths, limitations, and best
settings of application. They are discussed separately below.
Imagery-based Retrieval
A. COASTAL BATHYMETRY
Optical remote sensing of bathymetry is underpinned by the principle that the total
amount of radiative energy reflected from a water column is a function of water
depth. As the incident solar radiation propagates down through water, it is
increasingly scattered and absorbed by it and any in-water constituents present,
leaving varied energy to be backscattered and captured by the remote sensor. The
energy received at the sensor is inversely proportional to water depth after calibrationfor the atmospheric and water column effects. Therefore, the intensity of the returned
signal is indicative of the depth at which the solar radiation has penetrated. Optical
estimation of bathymetry is grounded on a model between water-leaving radiance
recorded on satellite imagery and the depths sampled in the field. This model can be
analytical, semi-analytical, or empirical. A commonly used analytical model is the
flow RTM that requires the input of the spectral signatures of suspended and
dissolved materials, and bottom reflectance Rb(λ). Water depth (D) is calculated as:
(7.4)
where K = effective attenuation coefficient of water; f = geometric factor accounting
for path length; L = radiance recorded in a spectral band; = mean radiance over deep
water caused by water surface reflection, water column, and atmospheric scattering;
C = constant derived from irradiance at the water surface, the transmittance of the
water surface and the atmosphere, and the reduction of the radiance due to refraction
at the water surface.
The RTM is underpinned by an inherent assumption of a highly reflective bottom,
an appropriate level of water quality, and/or a shallow depth. Hence, it is inapplicable
to coastal waters that have a poor reflecting bottom caused by high turbidity. Thus, it
has to be modified for turbid waters in which bottom reflection is obscured or in deep
waters where bottom reflection does not exist. In such environments, the model is
adapted as Eq. 7.5 (Ji et al., 1992):
(7.5)
where A’ = parameter related to atmospheric transmission and the irradiance
reflectance of an optically deep-water column; B’ = atmospheric and sky irradiance
effects. Both are determined through regression analysis based on sea-truth
bathymetric data. With some modification, this model can be applied to very shallow
(e.g., D < 2 m) waters.
Physical models contain a few parameters whose precise parameterization requires
field data that are troublesome and laborious to collect, and the parameterizing
process is thus costly and lengthy. In comparison, empirical models are easy to
establish. The relationship between remotely sensed radiance of a water body and the
depth at sampled locations is established statistically without regard to how light is
transmitted in it. Empirical or semi-empirical models are commonly used to retrieve
bathymetry in inland waters. A close relationship exists between water depth and theradiance in a single band for waters of uniform optical properties and bottom
reflectance in the form of:
(7.6)
where Ai,j = the ith variable in the rotational matrix for band j; Rw = above-surface
radiance in band λj; and R∞ = average deep-water signal after atmospheric and sun
glint corrections. If the water has uniform optical properties, the first N-1 of these
linear combinations are independent of depth, while the last one (YN) is a function of
both water depth and bottom reflectance (Casal et al., 2019).
Both the detectable depth and accuracy of the quantified bathymetry are a function
of water clarity, with murky water much harder to sense, thus limiting the
quantifiable bathymetry to < 10 m. In the best case scenario it can be extended to
over 20 m. In the presence of other in-water constituents such as silt, multiple
shortwave bands may have to be used and transformed to derive depth due to the
stronger penetration. A common way of transformation is the ratio of the blue to
green bands. The most useful bands (band ratios) are determined via their correlation
with the in situ observed water depth. A good example is the log-transformed
algorithm of two-band ratio model developed by Stumpf et al. (2003) for oceanic
waters in the form of:
(7.7)
where Lobs-b = observed radiance in blue band; γ and e = offset and gain determined
empirically. This model is capable of retrieving depths > 25 m in clear coastal waters
and is applicable to turbid coastal waters. The ratio transformation algorithm
minimizes the bottom most radiance of one band more rapidly with respect to depth,
relative to another band. A near constant attenuation value is preserved between the
ratio of two bands which is nothing but the difference of the diffuse attenuation
coefficient at two wavelengths for uniformly mixed water columns. The algorithm
can reduce the error coupled with varying radiation in the atmosphere, water column,
and sea floor, as both bands are distributed equally. It achieves an R2 value of 0.878
in mapping coastal bathymetry up to 20 m from Landsat 8 bands (Jagalingam et al.,
2015). With coarse Sentinel-2A data, the linear band model has a better fitting than
the log-transformed band ratio model, yielding R2 values of 0.83–0.88 over the depth
range of 0–10 m. The closest fit is found in the depth range of 2–6 m (Casal et al.,
2019). Both linear and band ratio models have been used to map bathymetry of
nearshore bays (Figure 7.3). Linear, quadratic, exponential, power, and lowessOptimal Band Ratio Analysis (OBRA) models enable flexible curve-fitting in
calibrating spectral-based quantities to depth. The exponential model avoids artifacts
common to other models.
FIGURE 7.3 Bathymetric maps of Dublin Bay obtained using the linear model for the
image registered on 16 June 2017 (a) and 17 July 2017 (b), and the log-transformed
band ratio model for the June image (c) and the July image (d). The white areas close
to the shore are the intertidal zone (0 m depth). The black areas have negatives values.
(Casal et al., 2019. © Taylor & Francis.)
Band ratio has been extended to include four visible bands of Sentienl-2 imagery, for
example, B3/B1, B3/B2, B4/B1 and B4/B2 (Gafoor et al., 2022). This ratio of four
visible bands outperforms the green-to-blue ratio, with R2 > 0.8. When multiple
bands and variables are considered, the selection of the best predictor variables may
be optimized using the OBRA method developed by Legleiter et al. (2009). It
considers all possible band combinations and identifies the pair of wavelengths that
constructs the strongest relationship between D and the image-derived parameter.
OBRA-based depth estimates differ from the mean depth retrieved from airborne andUAV hyperspectral data by 14–15% and by 18% from WorldView-3 imagery
(Legleiter and Harrison, 2018). Bathymetry retrieved from WorldView-3 images (and
airborne hyperspectral sensors) has a mean error close to 0 but depth is
underestimated by up to 8%. Apart from linear band ratio models, the transformation
may take the form of logarithmic band ratio for coastal waters under different
morphological and environmental conditions (Casal et al., 2019), but the log￾transformed band ratio model is still outperformed by the simple linear band model,
achieving an R² value between 0.83 and 0.88.
The key factors affecting the retrieved bathymetry are identified as atmospheric
correction, bottom type and influence, and water column conditions. The accuracy of
optically sensed bathymetry is subject to image spatial, spectral, and radiometric
resolutions that may have a confounding effect (e.g., band designation and
bandwidth). Besides, the accuracy is also impacted by the sensing environments (e.g.,
solar elevation and azimuth, platform height), atmospheric absorption and scattering,
water surface conditions (e.g., roughness, waves, and currents), scattering by in-water
constituents, and substrate reflectance properties that might influence the
characteristics of the returned electromagnetic radiation. The total at-sensor radiance
depends on the optical properties of the water column, and the light reflected from
the water surface. If not corrected, atmospheric effects can further contaminate the
radiance signal. Finally, the accuracy of remotely sensed bathymetry is also affected
by a variety of water-related factors, such as water clarity, attenuation, depth, bottom
reflectance, and bottom materials if relevant. The retrieved bathymetry may be made
more accurate by tackling these relevant factors. For instance, errors are reduced
markedly after correction for the solar and view-angle effects, water clarity and
sediment loads, and bottom reflectance (Gafoor et al., 2022).
B. INLAND WATER BATHYMETRY
Bathymetric mapping for inland waters is much more challenging than coastal waters
because they tend to be shallower, smaller in size, and more turbid. Satellite remote
sensing has not been routinely used to estimate bathymetry for turbid inland lakes
due to the difficulty of atmospheric correction. The standard correction algorithm for
Case I waters fails when applied to Case II waters. Remote sensing of river channel
bathymetry can be successful only if the water is relatively clear and shallow, and the
channel is visible from above, unobstructed by riparian vegetation or shadows. Even
so, this estimation still faces hurdles as most existing data and analytical methods are
developed primarily for coastal waters. The alternative of estimating channel
bathymetry is to use GPS, total station, side-scan sonar, echo sounding, but theycannot generate a field view of bathymetry unless supplemented by spatial
interpolation.
Shallow channel bathymetry can be quantified from VHR data, such as multispectral
WorldView-3 satellite imagery, hyperspectral data collected from conventional and
unmanned aircraft, and bathymetric LiDAR, such as high spatial resolution
hyperspatial resolution CASI and even UAVs equipped with photogrammetrically
calibrated sensors using MVS SfM photogrammetry (Figure 7.4). It facilitates the
calibration of non-calibrated sensors (specifically SfM) at a spatial resolution of 1 m.
However, the turbidity of channel waters prevents the mapping in deep waters. The
image-produced results are better than the echo sounding-based models in waters
deeper than 0.2 m without showing small-scale bathymetric variation. Bathymetric
SfM requires, in addition to clear water, a clearly visible bed with enough structure,
as in classical photogrammetric mapping of stream bathymetry (Kasvi et al., 2019).
Color and depth affect optical model performance, but clearly less than bathymetric
SfM. As always, shadows cast by riparian vegetation restricts the spatial extent of
optical models (Chan et al., 2022). But SfM is much worse than airborne LiDAR data
and optical images that achieve a comparable accuracy between them, regardless of
the grid size (Table 7.2).FIGURE 7.4 Comparison of river bathymetry retrieved from multispectral WorldView-3
imagery, airborne CASI, drone-based hyperspectral images, and bathymetric LiDAR
data. (Legleiter and Harrison, 2018, free access.)
Table 7.2
Accuracy indicators of bathymetric models at two resolutions constructed from airborne LiDAR and
optical images
Model R²
ME
(m)
MAE
(m)
Range of error
(m)
SDE
(m) n
SfM_1m 0.01 –0.30 0.30 –2.41 – –0.05 ±0.50 54
ADCP_1m 0.96 –0.01 0.05 –0.23 – 0.24 ±0.07 52
Optical_1m 0.92 –0.05 0.09 –0.26 – 0.18 ±0.11 47
SfM_0.05m 0.00 –0.31 0.31 –3.00 – –0.03 ±0.56 54
ADCP_0.05m 0.96 –0.02 0.05 –0.24 – 0.24 ±0.07 52
Optical_0.05m 0.92 –0.06 0.09 –0.26 – 0.09 ±0.10 47
Source: Kasvi et al. (2019), open access.
Note: ME = mean error; MAE = mean absolute error; SDE = standard deviation of error.7.2.2.2 LiDAR Bathymetry
An effective way of mapping bathymetry is to use bathymetric LiDAR. The sensing
of floor bed using LiDAR is identical to scanning terrestrial surfaces. An airborne
(drone or light aircraft) LiDAR scanner simultaneously acquires dense 3D point
clouds about water surface and the floor of water bodies. The success of LiDAR
bathymetry depends on the selection of an appropriate wavelength for sensing.
Longer wavelength radiation is not favored due to increasing absorption by water.
Shorter wavelengths are not ideal either because of strong scattering and absorption
by in-water constituents, and hence a shallower depth of penetration. As opposed to
topographic LiDAR that uses 1,064 nm radiation, bathymetric LiDAR systems
employ an extra green wavelength (532 nm) that is the most capable of strongly
penetrating the water column. The use of two beams is beneficial for accurate
mapping as the infrared beam is used for ranging the distance between the sensor and
the water surface, and the green beam (532 nm) for water penetration. This
penetration extends the sensible depth of shallow water reservoirs, rivers, and coastal
sea waters to three Secchi depths.
Bathymetric LiDAR measures water depth from the two-way propagation time of
pulses between the water surface and the sea floor. Water depth is calculated from the
time difference between the surface-reflected and floor-reflected pulses. The bottom
return is the last signal to reach the scanner. Due to attenuation by in-water
substances and water absorption, this return is typically several orders of magnitude
weaker than the surface return. The number of bottom returns identified via
automated and manual classification methods can be limited by the depth and
turbidity of the water column and by the reflectance of the channel floor. If the
bathymetric LiDAR data are provided as bed elevations rather than depths, then
depth is calculated by subtracting the bed elevations derived from NIR LiDAR
acquired simultaneously as the bathymetric (green) LiDAR.
Table 7.3
Comparison of typical parameter properties of topographic and bathymetric LiDAR scanners
Parameter Topographic scanner Bathymetric scanner
Laser wavelength 1064 nm (IR) 532 nm (green)
Sent pulse beam
divergence Narrow (0.3 mrd) Narrow (0.3 mrd)
Return pulse beam
divergence
Narrow (0.3 m from a
height of 1000 m)
Wide (2 m from a height
of 300 m)Parameter Topographic scanner Bathymetric scanner
Frequency of pulse
generating High (up to 400 kHz) Low (1-10 kHz)
Pulse width Short (5-10 ns) Short (< 5 ns)
Energy emitted Weak (5-10 mJ) Strong (5-10 mJ)
Incidence angle Nadir (0°) Forward (15-20°)
Laser sensor Single laser Double (2 wavelengths)
Accuracy of distance
measurement 1-3 cm 3-5 cm
Scan trace Parallel lines, sinusoidal
Elliptical lines (Palmer
scanner)
Optical sensors MS digital camera HSI/MS digital camera
Georeference GNSS/INS GNSS/INS
Platform Helicopter, airplane
Airplane, helicopter,
drone
Flight height 500-1000 m (and higher) 300-500 m
Processing
Discrete reflections, full
wave shape Full wave shape
Source: Szafarczyk and Toś (2023), open access.
At present, airborne bathymetric LiDAR is the most common, but UAV-borne
bathymetric LiDAR scanners can effectively and cost-efficiently acquire large
volumes of detailed and high-quality 3D data via variable swath widths independent
of water depth (Table 7.3). Bathymetric LiDAR is a very specialized and unique
technology with only a handful of systems currently available, and they are used
almost exclusively for coastal waters. The most advanced bathymetric LiDAR
scanner is Leica HawkEye-5, the only stabilized sensor able to deliver a high￾performance solution for detailed bathymetric mapping of deep waters. It shields the
sensor from undesired aircraft movements, yielding consistent data density and
providing highly efficient area coverage. The system expands the capabilities of its
predecessors and enhances survey efficiency by up to 25% in the most diverse
environments from nearshore to deep waters.
The sensible depth of bathymetric LiDAR varies with LiDAR systems. Shallow
water systems have less laser power per pulse, a higher measurement frequency (high
resolution), a smaller laser footprint diameter, and a smaller FOV. In general, they
can estimate water depths within the visible water column only. Deep-water systemsuse more laser power per pulse, a lower measurement frequency (low resolution), a
larger laser footprint and receiver FOV. These systems can detect depths up to 2–3-
fold SDD from 1.5 to 60 m at an accuracy of up to 15 cm (Abbot et al., 1996). The
maximum depth of penetration typically ranges from 35 m to 50 m (Figure 7.5). In
clear Case II waters, the sensible depth can reach up to 70 m. This depth is limited by
water clarity, bottom material and composition, weather and sea state, background
light, and eye safety in Case I waters. Bathymetric LiDAR is highly accurate and
precise in shallow waters, generating the most accurate and precise depth estimates
(5% error) only for depths up to 2 m (Legleiter and Harrison, 2018). A lack of bottom
returns from areas deeper than 2 m may result in extensive voids in the generated
bathymetric map.
FIGURE 7.5 Topographic (NIR) and topo-bathymetric (green) laser scanners mounted
on the same airborne platform; (left) reflection of NIR signal at water surface; (right)
principle of airborne laser bathymetry (refraction of laser beam at water surface,
echoes from near water surface, water column, and the floor). (Szafarczyk and Toś,
2023, open access.)
Table 7.4
Bathymetric retrieval accuracy from space-borne WorldView-3 multispectral and airborne hyperspectral
CASI data using six estimation models in comparison with bathymetric LiDAR results7.3
7.3.1
Data Model R²
ME
(%)
Median
error (%)
Range of
error (%)
SD
(%)
WV-3
Multi￾spectral
(n = 20618; 
= 1.84 m)
Linear 0.67 1 –2
–66 –
108 20
Quad 0.73 1 0 –85 –93 18
Exp 0.73 3 1 –66 –96 18
Power 0.73 3 1 –64 –97 18
Low 0.74 1 0 –69 –92 18
KNN* 0.87 0 –1 –62 –90 13
CASI airborne
hyperspectral
(n = 21503, 
= 1.80 m)
Linear 0.75 1 0
–50 –
112 18
Quad 0.85 1 1 –60 –96 14
Exp 0.85 3 2 –63 –91 14
Power 0.74 3 4 –74 –99 21
Low 0.87 1 0 –78 –93 13
KNN 0.92 0 0 –81 –85 10
Bathymetric
LiDAR
(n = 13358)
(= 1.80
m) 0.95 –2 –2 –42 –77 5
Source: Modified from Legleiter and Harrison, (2018), free access.
Note: *: K nearest neighbor regression.
Compared to multispectral and hyperspectral imagery data, LiDAR is more accurate,
regardless of the model format, even though KNN produces an accuracy quite close
to LiDAR´s (Table 7.4). As expected, multispectral bands produce slightly less
accurate results than hyperspectral bands.
WATER SURFACE FEATURES
Sea surface lies at the interface of the ocean and the atmosphere. As such, it controls
the exchange of heat and gases between them. Sea surface parameters that can be
quantified remotely are exclusively physical, including temperature, flow velocity,
and wave height.
Sea Surface TemperatureSST is critical to the exchanges of heat, moisture, momentum, and gases between the
ocean and the atmosphere. This geophysical parameter plays an instrumental role in
weather forecasting and climate change modeling. It also offers insights into
atmospheric and oceanic circulation patterns and processes such as El Niño. On a
local scale, SST can be used operationally to assess eddies, fronts, and upwellings for
marine navigation and tracking bio-productivity. SST at different depths is in situ
measured via moored and drifting buoys, and ships of opportunity, but the
measurements are sporadic and spatially isolated. In comparison, the estimates from
remote sensing images are repeatable at a high temporal frequency (e.g., daily) and
spatially consistent. In fact, SST is the longest and most intensively estimated
oceanographic parameter from various remote sensing data using different methods.
Traditionally, SST is estimated from IR bands to a depth of about 10 μm, known as
the skin layer of the ocean or the “skin SST” (Figure 7.6), at which TIR (3.7–12 μm)
radiation is completely attenuated. The “sub-skin SST” at the base of the conductive
laminar sub-layer of the ocean surface (a depth of ~1 mm) corresponds to the
attenuation length of 6-11 GHz microwave radiation. Beyond this depth lies what is
commonly referred to as the “bulk SST” or “near-surface SST” (Figure 7.6). This
depth can be extended to a few millimeters using microwave radiometer data. The
satellite-sensed SSTs differ from in situ measurements in that they apply to the
surface skin temperature of the ocean rather than the subsurface “bulk” SST observed
from ships and buoys.7.3.1.1
FIGURE 7.6 Various kinds of sea surface (water) temperatures (SST) that may be
estimated from different types of remote sensing data. (Adapted from Minnett and
Kaiser-Weiss, 2012.)
IR Retrieval
SST retrieval follows the same principle as the retrieval of LST covered in Section
5.1.2 except the much coarser spatial resolution of the remote sensing data because of
the narrow range of SST values. SST is estimated from the radiation emitted by a sea
surface and recorded in spectral bands. The relationship between radiant SST and its
radiant exitance L in a spectral band is described by Eq. 5.3. The integration of the
Planck equation with the upper and lower limits of a spectral band (two wavelengths
of λ1 and λ2) yields brightness temperature or BT(λ). The exact relationship is
constructed using regression analysis with the assistance of independent in situmeasured SSTs. SST is commonly derived from radiometric observations at ~3.7 µm
and/or near 10 µm where peak radiation occurs and the atmosphere is transparent
(i.e., within the atmospheric windows). Though the 3.7 µm channel is more sensitive
to SST, it is primarily used only for nighttime measurements because of relatively
strong reflection of solar irradiation in this wavelength region, which contaminates
the retrieved radiation. Both the 3.7 and 10 μm bands are sensitive to the presence of
clouds and scattering by aerosols and atmospheric water vapor. For this reason, the
at-sensor radiance of TIR bands must be corrected for the atmospheric effects in the
retrieval and can only be made for cloud-free pixels. These two bands are available
from most EO sensors, such as Landsat 8/9 and ASTER (band 14) images, and other
satellite data, including MODIS, Sentinel, Atmospheric Infrared Sounder (AIRS),
IASI, and Himawari-8.
Sensor-recorded radiation in a given spectral band is converted to temperature via
the bidirectional conversion relationship established after the linearization of the
Planck’s equation and the radiative transfer equation in which the BT difference is
related to the decrease in temperature from SSTskin and one of the BTs, or:
(7.8)
where BTi and BTj = brightness temperatures measured in bands i and j, c = offset, γ
= differential absorption coefficient whose value depends on the atmospheric
conditions at the time of sensing, especially the profiles of temperature and water
vapor. a, γ and c are determined via regression analysis of in situ collected BT
measurements or using radiative transfer simulations. The former requires concurrent
measurements to represent the effects of water vapor. In order to account for the
effects of path lengths related to the satellite zenith angle θ, οne more term needs to
be inserted into Eq. 7.8 to form the following equation (Minnett et al., 2019):
(7.9)
where BTi = BT measured in the band of i μm wavelength; Tsfc = “first guess”
estimate of SST in the area of interest. This model is applicable to both daytime and
nighttime images. More accurate nighttime retrievals are feasible using the
measurements in the mid-IR transmission window. It needs to be adapted to data
from different sensors. For VIIRS bands, the adapted model becomes:
(7.10)7.3.1.2
with MODIS data that have two narrow bands in the mid-IR transmission window,
the accurate nighttime model becomes:
(7.11)
Subsequent to atmospheric corrections, coefficients are applied to the retrieved BT
signals to derive SST by factoring in the estimated surface emissivity. Simple linear
algorithms can produce reasonably accurate SST estimates under favorable
atmospheric and surface conditions, but more sophisticated higher-order models may
be needed for other conditions. Robust non-linear models have been established from
non-linearly combined TOA BT measured in the 10–12 μm wavelength interval over
which the atmosphere is relatively transmissive.
The accuracy of the retrieved SST is affected by a number of factors, the most
important being pixel size and the exact value of ε that is a function of surface
roughness and temperature. In addition, atmospheric conditions, such as haze, vapor,
and cloud cover also exert an influence. IR-retrieved SSTs have a higher resolution
(1–4 km) than 25 km of microwave data and superior sensitivity, but IR imagery does
not work in areas frequently covered by clouds, such as the tropics, and requires
atmospheric correction. This deficiency can be overcome by using microwave data.
Microwave Retrieval
Microwave retrieval is made possible on the ground that the vertically polarized
microwave BT of oceans is appreciably sensitive to SST at frequencies of 4-11 GHz.
Nevertheless, BT also depends on sea surface roughness, atmospheric temperature,
and moisture profile, or the net air–sea heat flux. Moreover, the spectral and
polarimetric signatures of these conditions differ distinctly from that of temperature.
Their effects can be simultaneously measured at multiple frequencies and
polarizations and eliminated. SST can be successfully retrieved from several radar
sensors, including TMI, Advanced Microwave Scanning Radiometer-Earth Observing
System (AMSR-E), AMSR-2, and WindSat. The launch of the TMI sensor heralds a
new era of microwave retrieval of SST. They all contain bands of multiple
frequencies for simultaneously measuring wind speed, columnar water vapor, and
rain rate, in addition to SST. Their intended use is to remove the surface roughness
and atmospheric effects. Sea surface roughness, which is tightly correlated with the
local wind, is usually parameterized in terms of near-surface wind speed and
direction. The additional 7 GHz channel of AMSR-E and AMSR-2 (but not TMI)provides improved estimates of sea surface roughness and improved accuracy for
SSTs below 12°C.
Microwave-based SST retrieval differs from that using visible light bands. At long
wavelengths, where hc λkT, Planck’s function is reduced to (Minnett et al., 2019):
(7.12)
As in the IR, ΒΤ(λ) measured by passive microwave radiometers is related to
thermodynamic temperature (T) and emissivity ε(λ) or:
(7.13)
Algorithms for deriving SST from passive microwave measurements are based on
RTM, as adopted by the ESA Climate Change Initiative. Sub-skin SSTs are
operationally estimated from AMSR-E data using the Optimal Estimation technique.
It calculates the simulated BTs utilizing a forward model based on surface emissivity
and an RTM. The RTM consists of an atmospheric absorption model for oxygen,
water vapor and cloud liquid water, and a sea surface emissivity model that treats it
as a function of SST, sea surface salinity (SSS), and sea surface wind speed, and
direction. TOA BTs are simulated from sensor information (azimuth and earth
incidence angles, frequency, polarization) and environmental data (SST, SSS, wind
speed, wind direction, atmospheric profiles of temperature, pressure, water vapor
density, and liquid cloud water density) (Nielsen-Englyst et al., 2018). The
relationship between the measured BT and these hydrophysical parameters is
expressed as:
(7.14)
where F(x) = non-linear forward model approximating the physics of the
measurement, including surface emissivity and the radiative transfer through the
atmosphere; x = state vector containing the relevant geophysical properties of the
ocean and atmosphere; and e = residual term of uncertainties contributed by the
measurement noise and in the forward model.
The forward model predicts the TOA microwave BTs measured by individual
channels of a radiometer given knowledge of the relevant geophysical parameters (x)
of the ocean and atmosphere. Differences between the simulated and measured TOA
BTs are normally attributed to inaccuracy in the RTM parameterizations or
measurement inaccuracy (e.g., imperfect model calibration and channel noise).7.3.1.3
7.3.2
Consideration of a priori information about the ocean and atmospheric state
improves the accuracy of IR SST retrievals over a non-linear algorithm. The retrieved
SSTs, on average, differ from drifting buoy measurements by 0.02 K (standard
deviation = 0.47 K) when considering the 64% match-ups, where the simulated and
observed BTs are the most consistent (Nielsen-Englyst et al., 2018). The
corresponding mean uncertainty is estimated to be 0.48 K, including in situ sampling
uncertainties. In an independent validation against Argo observations, the average
difference and standard deviation change to 0.01 K and 0.50 K, respectively, with a
mean uncertainty of 0.47 K for 62% of the best retrievals. The satellite-retrieved
SSTs deviate from in situ measurements the widest in the dynamic oceanic regions
due to the large satellite footprint size and the associated sampling effects.
SST Products
SST products have been derived from a variety of satellite datasets, including
Himawari-8 and Multifunctional Transport Satellite (MTSAT)-2 at different accuracy
levels. Daily SST product is available at 0.25° (~25 km) resolution. This coarse
resolution product is ideal for scientific applications that require complete, daily
global SST distribution information without missing data incurred from orbital gaps
or environmental conditions precluding SST retrieval because they have been filled
via spatial interpolation (www.remss.com/measurements/sea-surface-temperature/).
The Himawari-8 product has an average SST difference of 0.18 ±0.53 K, with an
average median of 0.16 K (Ditri et al., 2018). The MTSAT-2 product has a mean SST
difference of 0.26 ± 0.48 K, with an average median of 0.27 K. Overall, the
Himawari-8 AHI SSTs have smaller discrepancies (an average of 0.08 K) with in situ
observed temperatures. Nighttime measurements are subject less to the effects of
diurnal heating in the upper ocean, causing the SST differences to have a smaller
variation for both satellites at all locations. At times of higher wind speeds, there are
also smaller variations within the SST discrepancies. The large variations in the SST
discrepancies are related likely to diurnal thermal stratification arising from not
thoroughly mixed water column.
Significant Wave Height and Period
Significant wave height (SWH), the mean height of the third largest wave in the
descending order, is one of the most important parameters for characterizing ocean
waves and essential for coastal protection, shipping, and offshore industry operations.
It is reflective of the sea navigation condition and an important variable essential fora wide variety of calculations. SWH is best quantified from active, side-looking radar
images such as SAR data because they are operational under all weather conditions
and are sensitive to surface relief. SAR-based estimation of ocean wave height relies
on the intensity of radar signals backscattered by the rough sea surface using a
number of algorithms, including the Parameterized First-guess Spectrum Method
(PFSM) and the Max-Planck Institute (MPI) algorithm. Both need an initial “guess”
wave spectrum, such as an empirical wave model, a numerical wave model, or a
parametric wave spectrum model, to invert the wind-sea wave spectrum (Shao et al.,
2015). Initially developed for C-band SAR images, they are also functional with
TerraSAR-X images after a few modifications. Both SWH and mean wave period
(MWP) are retrieved via the inversion of the 2D wave number spectrum in several
steps, including the calculation of HH-polarized normalized radar cross-section
(NRCS) that is sensitive to high wind speeds, and the polarization ratio to convert
HH-polarization to VV-polarized NRCS based solely on incidence angle (Figure 7.7).
After the ambiguous wind direction is derived from the empirical wave model, the
European Centre for Medium-Range Weather Forecasts (ECMWF) wind direction
data may be used to determine the real wind direction from the SAR image. It may be
necessary to unify the 0.25° × 0.25° spatial resolution of ECMWF wind products
with the 5 m spatial resolution of SAR-X data via resampling. With the assistance of
XMOD2 for wind retrieval from TerraSAR-X, the VV-polarized NRCS is treated as a
function of the angle between the radar look direction, the wind direction, and the
incidence angle while the coefficients of the equation are functions of sea surface
wind speed and radar incidence angle. The PFSM separates the linear-mapping
spectrum portion from the SAR intensity spectrum via the calculated wave number
threshold. The portion of the linearly mapped SAR spectrum is where the wave
numbers are smaller than the separation wave number. SWH is output directly from
the inverted 2D wave number spectrum, together with MWP (Figure 7.7).FIGURE 7.7 The procedure of retrieving wind and wave parameters from HH-polarized
SAR imagery using the parameterized first-guess spectrum method. NRCS –
Normalized radar cross-section; X-PR – X-band polarization ratio (model); SWH –
significant wave height; MWP – mean wave period. FFT – Fast Fourier
Transformation. XMOD2 – a model for retrieving wind from TerraSAR-X imagery.
(Modified from Shao et al., 2015, open access.)
Apart from radar imagery, SWH can also be retrieved from multiple satellite
observations based on Global Navigation Satellite System Reflectometry (GNSS-R).
This active sensing system transmits L-band microwave signals, and synchronously
receives the signal from multiple GPS satellites and the corresponding GNSS signal
reflected off the Earth’s surface. Multiple scattered signals from surface waves and
the signals directly reaching the receiver have differentially scattered power, delays,
and Doppler shifts that are exploited to retrieve SWH (Qin and Li, 2021). SWH
exerts a strong effect on the scaled SNR of the delay-Doppler maps (DDM). The ratio
of power (the sum of relative powers in the peak area of the DDM) to the noise floor
is termed the accumulated DDM-to-noise ratio (ADNR), calculated as:
(7.15)where DDM(tm,fn) = power value at the delay of time tm and the Doppler frequency
of fn; Noise = mean power of noise floor in the corresponding DDM; m, n = indices
of delay and Doppler. An offset correction that combines the differences between the
elevation angles of multi-satellites is introduced to minimize the impact of the
fluctuation difference, calculated using a second satellite elevation angle (θ) as:
(7.16)
where ADNRi and ADNRj = ADNR values of the ith and jth satellites; θi and θj =
their corresponding elevation angels; f(θ) = function of satellite elevation angel with
ANDR, expressed empirically as:
(7.17)
From ADNR, SWH is calculated via a quadratic equation as:
(7.18)
where a, b and c = offset and coefficients determined via regression against sea-truth
data. The correlation between SNR and SWH is tightened by an offset correction that
combines the differences between the elevation angles of multi-satellites, which
enables the estimation of SWH at a RMSE of 0.1671 m, 17.0% lower than that of the
traditional SNR algorithm.
The GNSS data may also be acquired from the Cyclone GNSS (CYGNSS)
mission (Wang et al., 2023), from which SWH is retrieved from more variables than
the delay-Doppler map average (DDMA), such as the normalized bistatic radar cross￾section (NBRCS), leading edge slope, SNR, incidence angle, azimuth angle, and
pseudo-random noise, location (latitude and longitude), distance, wind speed, and
other auxiliary variables (e.g., instrument gain and scatter area) (Wang et al., 2022a).
Their relationship with SWH can be established using a wide range of analytical
means, including multivariate stepwise linear regression, Gaussian SVM, ANN,
sparrow search algorithm–ELM, and bagging tree. Of the five regression models
developed, the bagging tree model is the best performer, achieving a RMSE of 0.48
m and a correlation coefficient (CC) of 0.82 when tested with one million randomly
selected sets of data, but slightly less accurate (RMSE = 0.44 m, CC = 0.73) when
tested with the National Data Buoy Center (NDBC) buoy testing dataset (Wang et al.,
2023). The more parameters are considered, the lower the RMSE (m). Without the
auxiliary wind speed, the SWH retrieved using the trained neural network exhibits abias and an RMSE of -0.13 and 0.59 m, respectively, with respect to ECMWF data
(Wang et al., 2022a). It is beneficial to include wind speed in the model as it can
reduce the bias and RMSE of estimation to -0.09 and 0.49 m, respectively. When the
incidence angle ranges from 35° to 65° and the SNR is above 7 dB, the retrieval
performance is better than that obtained using other values.
Due to the non-linear imaging mechanisms of radar sensing, ocean waves are
geometrically distorted on SAR images. So, shorter waves such as those generated by
a hurricane are not ideally retrieved using image-to-wave spectra inversion. The
alternative is to empirically estimate SWH by exploiting the fetch- and duration￾limited wind-wave growth model (Zhang et al., 2018). Also known as the H model, it
treats SWH Hs and wave period Tp as a function of variance, frequency, and duration,
all being dimensionless. If fetch and duration are known, both wind parameters can
be derived from the wind field at 10 m high (U10) directly. In turn, the wind-wave
triplets (Hs, Tp, and U10) enable the derivation of equivalent fetch and equivalent
duration inside the hurricane. If the triplets are measured from a scanning radar
altimeter, then the fetch (unit: km) and duration (unit: hour) calculation formula can
be empirically established for the three sectors of a hurricane: left, right, and back
(Zhang et al., 2018), hence the H-3Sec model, all being functions of the radial
distance r (unit: km) from the hurricane center. Depending on how the fetch and
duration relationships are represented, the original H-model may take the form of H￾LUT and H-Harm. The former simulates surface waves in the azimuthal and radial
directions in stormy conditions while H-Harm considers the influence of the radius of
the maximum wind speed in storms in fetch- and duration-limited simulations. SWHs
inside the hurricane can be effectively calculated using all the three H-models based
on winds observed from Sentinel-1A SAR images, except areas close to the hurricane
eye (Figure 7.8). These models can also be used to potentially simulate Hs and Tp for
ocean waves inside a hurricane from RadarSat-2 images reliably.7.3.3
FIGURE 7.8 Wave height (top row) and period (bottom row) retrieved from SAR images
using different growth functions: (a) fetch-limited significant wave height (Hs); (b)
duration-limited significant wave height (Hs), (c) fetch-limited wave period (Tp); and
(d) duration-limited wave period (Tp). (Zhang et al., 2018, open access.)
Surface Flow Velocity
Surface flow includes ocean current circulation and river channel flow, of which the
former is important climatically as it affects the weather pattern and local climate.
The latter is critical to sediment transport and river bank erosion. Velocity of surface
flow is best estimated by tracking thermal features on time-series images, either
optical or microwave. The nature of images affects the method and capability of
velocity determination. Optical remote sensing allows tracing thermal features from
time-sequential images (after they are converted to SST) based on spatial
autocorrelation to estimate ocean current velocity. Channel flow velocity is
determined from time-sequential images via the recorded displacement of the tracerimmersed in a fluid (Martínez-Flores et al., 2019). These images share the same
spatial coverage and are captured at short time intervals up to hours from each other.
The first image is divided into continuous sub-areas called templates. For each
template, a search window is defined in the second image, containing the template at
its center (Figure 4.12). Sub-areas bearing the highest spectral similarity between the
two images are searched. The criterion for determining the degree of similarity
between the template and the search window is the cross-correlation coefficient (refer
to Section 4.4.2 for more details). The displaced position having the highest
coefficient is considered as the shifted position, and its distance from the source
origin is considered the distance of movement (d). Its division by the lapsed time
yields velocity. This method works when the entire ocean field moves translaterally
without deformations.
If estimated from sequential radar images, ocean current velocity can still be
estimated using the same cross-spectral correlation analysis as with optical images,
but the radar images must be pre-processed to reduce the dependency of gray levels
on incidence angel, radar range and azimuth via contrast-limited adaptive histogram
equalization, and angle unification. Radar images can be used to derive 2D cross￾spectral coherence and phase. The phases with large coherence are retained to
estimate the phase velocity and angular frequency of waves. Current velocity is
determined in two steps:
i. estimation of the cross-spectrum between neighboring images Ii and Ii+1 (I = 1,
2, … N-1) and the auto-spectrum, of each image Ii (I = 1, 2, ..., n). Both the
cross-spectra and auto-spectra are averaged to derive the mean cross-spectrum
to reduce the noise as:
(7.19)
(7.20)
where superscript (1, n) = mean of the first to the nth spectrum; (where k =
wave number, φ = direction]; and
ii. calculation of the cross-spectral coherence and cross-spectral phase of the radar
image sequence as:
(7.21)
(7.22)where mathematical operator |·| and arg(·) = module and argument of a complex
number, respectively. The peak wave number is obtained from the peak of the cross￾spectral coherence using Eq. 7.21. The 180° directional ambiguities are resolved by
choosing the wave direction with a positive phase in the last equation (Chen et al.,
2019). Current velocity is determined by fitting the wave phase velocity derived from
the theoretical dispersion relation with that estimated from radar images, calculated
as:
(7.23)
(7.24)
where ω = wave angular frequency; g = gravitational acceleration; , current velocity
vector in the north-south (ux) and east-west (uy) directions; ∆t = temporal separation
between two neighboring radar images, or the antenna rotation period of an X-band
marine radar.
The second method of deriving current velocity is to fit the 2D dispersion relation
into Eq. 7.25. With significant and large cross-spectral coherence, the wave
constituents are important to the Doppler shift in dispersion relation. The least-square
model is weighted by the cross-spectral coherence in Eq. 7.25, which is estimated
from radar images:
(7.25)
where kx = kcosφ, ky = ksinφ, and w = Φ/Δt; the wave number k and direction φ are
also chosen as those with the largest coherence, and γ(k,φ) ≥ γc (0.6).
The procedure of estimating current velocity is illustrated in Figure 7.9. Compared
with the current velocities measured by a current meter, the north-south velocity is
estimated from X-band marine radar images at a RMSE of 0.14 m ∙ s
-1, a correlation
coefficient of 0.86, a bias of 0.06 m ∙ s
-1, and a RE of 10.75% (Chen et al., 2019).
These values change to 0.15 m ∙ s
-1. 0.88, -0.05 m ∙ s
-1, and 7.79% for the east-west
direction.FIGURE 7.9 Methods of estimating ocean current velocity from sequential X-band
marine radar images and main steps involved. (Chen et al., 2019, open access.)
Channel flow velocity is much more challenging to quantify than ocean current
velocity because of the lack of temperature gradient and a narrow channel size that
falls well below the spatial resolution of most space-borne images, especially
oceanographic satellite images. The only viable source of data for estimating surface
flow velocities is satellite video for large, sediment-laden rivers. Consequently, the
suitable method of data processing also changes to particle image velocimetry (PIV).
It employs particle tracking instead of feature tracking. Nevertheless, the principle of
quantification remains unchanged from that of ocean currents with a few additional
steps to process the images, such as image co-registration and masking of non-water
images. PIV works only if the masked image contains distinct surface features, suchas those formed by naturally occurring sediment boil vortices. They are tracked from
frame to frame as they are advected by the channel flow, obviating the need to
implant artificial tracers. It is important to accurately align the time-series images so
that the detected motion of the surface features is attributable to advection by the
flow rather than the motion of the imaging platform. The core of PIV is still
correlation analysis of the tracked features on temporally successive images. The
correlation peak is considered to represent the displaced feature position, from which
the velocity vectors are detected (Legleiter and Kinzel, 2021). However, point-to￾point PIV-derived velocity estimates have a weak agreement (R2 = 0.39) with field
measurements. The correspondence between them becomes stronger (R2 = 0.76) only
after the PIV output is aggregated to the cross-sectional scale (Figure 7.10). This
method works only when the channel water is murky with a high sediment load that
is spatially diverse to supply trackers. The more diverse the surface sediment
concentration is, the more accurately the velocity can be quantified. Otherwise,
artificial trackers must be installed for this method to work.7.4
FIGURE 7.10 Spatial distribution of PIV-derived surface flow velocities from satellite
videos at three frame rates: (A) 1 Hz, (B) 0.5 Hz, and (C) 0.25 Hz. Red lines: transacts
at which the acoustic Doppler current profiler velocity is measured for accuracy
assessment. (Legleiter and Kinzel, 2021, open access.)
IN-WATER INORGANIC PARAMETERS
In-water constituents can be classified as inorganic and organic, suspended in the
water column because of their small particle size and light weight. Inorganic matter iscommonly associated with solid particles such as resuspended silts and sediments.
Waters in shallow inland estuaries may be laden with sediments originating from the
nearby watershed or resuspended by waves during a storm. These tiny particles can
be suspended in the waters for a long time. Two types of sediment have been
quantified from remotely sensed data, SSC, and total suspended solids (TSS) or total
suspended matter (TSM). TSS refers to any waterborne particles >2 μm in diameter,
mostly inorganic materials and possibly algae and bacteria. TSS includes anything
that floats on water surface or “suspends” in the water column, such as sand,
sediment, and plankton. Total column solid quantity is related to the vertical
distribution of solids which is non-uniform vertically. Unlike TSS, TSM also
encompasses organic in-water constituents, such as chlorophyll α (Chl-a), algae,
phytoplankton, and CDOM. They are commonly associated with water pollution
caused by excessive use of fertilizers in the watershed. They can trigger water
eutrophication that takes place much more commonly in inland waters than in Case II
open waters, even though occasional algal outbreaks do take place in nearshore
waters in recent years due to climate change-induced seawater warming.
It is important to quantify key water quality parameters, such as TSS, Chl-a, and
CDOM because their concentrations can serve as proxies for assessing phytoplankton
dynamics and particulate load. This quantification is based on the at-sensor radiation
(not) returned from the underwater scattering of the incident energy transmitting
down the water column (due to absorption). The deeper the penetration, the further
down the target can be quantified, subject to the wavelength of the radiation. Given
the inverse relationship between wavelength and the depth of penetration, shortwave
visible light bands of cloud-free optical images can penetrate water deeper than
longer wavebands, and are more useful to deeper quantification. Conceptually, the
relationship between remotely sensed reflectance Rrs(λ) and the IOPs of in-water
constituents, or the total absorption α(λ) and backscattering coefficients bb(λ), is
expressed as follows:
(7.26)
where λ = wavelength (nm), r = coefficient with a value dependent on the geometry
of the light field emerging from the water body; α(λ) = sum of the absorption
coefficients of phytoplankton particles αph(λ), non-phytoplankton particles αd(λ),
CDOM αCDOM(λ), and pure water αwater(λ) (Eq. 7.27). It is an effective characterizer
of SSC:
(7.27)7.4.1
This relationship also suggests that the closer-to surface concentration of the target is
estimated at a higher accuracy, especially when the water is highly murky. In light of
co-existing multiple constituents, it is still feasible to quantify each of them
individually from remotely sensed data based on their unique absorption and
scattering properties.
Suspended Sediments
The sediment load of a water body directly governs its underwater light field
distribution and thus affects its primary productivity. Estimation of TSM plays a vital
role in monitoring, evaluating, and protecting water quality. On a local scale, the
quantification of suspended sediment and its spatial variability in river waters is
critical to understanding the fluvial dynamics of streams and rivers. Suspended
sediment in inland waters can offer valuable insights on sediment discharge, erosion,
deposition, and potential effects on biological processes. Remote sensing
quantification of suspended sediments is theoretically grounded on the positive,
curvilinear relationship between sediment concentration and reflectance in visible
spectral bands, established via the water truth data concurrently sampled in situ using
a range of statistical modeling techniques and satellite data (Figure 7.11). Many
estimation models have been constructed to link SSC to spectral reflectance in a
single band or its transforms in multispectral bands. Most of the models are
empirically and semi-analytically established from various satellite images. These
models are linear between 0.6 μm and 0.9 μm when the concentration level is low,
such as <600 mg ∙ L-1, above which the relationship becomes logarithmic or
exponential. The estimation of SSC in open sea and coastal waters is ideally achieved
from coarse spatial resolution MODIS, MERIS, GOCI, and OLCI data with a spatial
resolution >250 m. In general, they are inapplicable to inland, optically complex
lakes and reservoirs of a limited spatial extent. They require finer spatial resolution
multispectral bands, such as Landsat series data of a medium resolution (30 m). This
suitable data source has been exploited to retrieve SSC at various scales and a high
level of accuracy for decades.FIGURE 7.11 Relationship between SSC and in situ measured Rrs over the spectral range
of 0.4–1.1 μm in a turbid bay of Eastern China. (Yu et al., 2022, open access.)
Table 7.5
Comparison of estimation models and accuracy of retrieving SSC from OLCI and GOCI data
Sensor
Predictor
variable (X)
Model format and accuracy
(n = 33) Validation accuracy (n = 17)
Format R²
RMSE (mg ×
L-–1)
MRE
(%)
OLCI
21.49 0.95 70.33 17.31
21.59 0.96 69.10 17.52
21.58 0.93 156.52 35.34
0.95 0.87 126.12 43.31
GOCI 20.69 0.91 102.23 25.04
Source: Yu et al. (2022), free access.Advanced ocean observation satellite data such as OLCI and GOCI images enable
SSC to be estimated at a high accuracy up to R² = 0.96 using exponential models
owing to the images’ numerous spectral bands of a finer spectral resolution (Table
7.5). These models are built from band ratios, such as the ratio of bands 12 to 5 for
the OLCI data, and bands 16 to 5 for the GOCI data (Yu et al., 2022). The ratio of
multi-bands is superior to individual bands as it can suppress the background
influences (e.g., atmospheric and solar effects) while taking advantage of more
spectral information. The ratio can be derived from logarithmically transformed
MODIS band 1 and band 2. The best model for OLCI data takes the following form:
(7.28)
where X = ratio of the equivalent band reflectance of band 16 to band 5 of the OLCI
imagery. This multi-band index model has a higher retrieval accuracy and model
stability (MRE = 17.52%, and RMSE = 69.10 mg ∙ L–1) than other models (Table
7.5). The best model for GOCI data is expressed as:
(7.29)
where X = ratio of the equivalent band reflectance of bands 8 to 6 of the GOCI
sensor. In relatively clear waters of SSC<100 mg ∙ L-1, the model can nearly perfectly
reflect the actual SSC. Estimation inaccuracy occurs in murky waters with an SSC
above ⁓300 mg ∙ L-1. Thus, accurate estimation of widely ranging SSCs is best
accomplished using piecemeal models for different levels of concentration in
different parts of the same water body separately after they have been stratified using
the initial model, followed by merging of all the distribution maps to form the final
map (Figure 7.12).FIGURE 7.12 Spatial distribution of SSC in the Hangzhou Bay of East China retrieval
from OLCI data (a) and GOCI data (b) captured on 13 April 2020. (Yu et al., 2022,
open access.)
Large-scale TSS may be estimated from MODIS bands 1 and 2 of 250 m resolution.
The accuracy of estimation is slightly lower if the atmospheric effects are not
radiometrically corrected except the ratio of the logarithmically transformed bands
(Eq. 7.28). A simple approach of handling atmospheric influence is to treat the
intercept of the linear regression model as the manifest of the effects. Their removal
from the data substantially improves the estimation accuracy by 18% to R² = 0.853
(p<0.001, n = 25) (Chen et al., 2009). The best regression model between the
corrected and the observed TSS concentrations is expressed as:
(7.30)
Model validation reveals an RMSE of 5.5 mg ∙ L–1 (n = 21), demonstrating the
applicability of the MODIS b1/b2 reflectance–TSS curvilinear regression model to
mapping large-scale TSS concentrations (Chen et al., 2009). The problem with
empirical models is that they may have a very high error, even in the same
geographic area for which they are initially developed due to the changed sensing
conditions (e.g., changed weather and sea conditions).
In addition to regression analysis, SSC estimation can also be implemented using
machine learning analytical methods, including FFNN, CFNN, and ELM (Peterson etal, 2018). In order to eliminate redundant spectral information among the input
bands, the implementation may be preceded by feature fusion based on canonical
correlation analysis to extract pertinent spectral information among multispectral
bands and their ratios. The fused spectral features are treated as input predictors or
explanatory variables and used to train the predictive reflectance–SSC model. The
ELM-generated models have an R2 > 0.9 from Landsat 7 and 8 data, and can
accurately predict both relatively high and low SSCs without suffering obvious
overfitting (Peterson et al., 2018). ELM outperforms FFNN, CFNN, and other widely
used machine learning methods such as RF and SVM by significant margins when
evaluated by R2 and RMSE (Table 7.6). ELM is robust in estimating a wide range of
SSC based on a single algorithm. ELM models for all Landsat datasets attain R2
values above 0.9 and exhibit noteworthy generalization abilities along with negligible
overfitting.
Table 7.6
Comparison of accuracy in retrieving SSC from Landsat spectral data using five machine learning
algorithms
Analytical
algorithm
Landsat 7 Landsat 8
Training dataset Testing dataset Training dataset Testing dataset
R² RMSE R² RMSE R² RMSE R² RMSE
RF 0.942 41.2 0.860 57.6 0.911 57.0 0.852 58.0
SVM 0.890 52.6 0.884 56.8 0.962 37.1 0.928 40.4
FFNN 0.868 82.1 0.814 93.1 0.909 71.6 0.783 71.7
CFNN 0.903 73.5 0.786 102.3 0.933 68.9 0.891 69.7
ELM 0.910 48.1 0.903 51.4 0.968 34.2 0.931 39.7
Source: Peterson et al. (2018), open access.
Note: Boldfaced: best results among the five methods.
The estimation of TSM that involves more types of particles than TSS is more
challenging. Successful quantification requires the use of physical or semi-analytical
models indirectly, from spectral reflectance (Rrs) in slightly turbid waters (TSM<25
mg ∙ L–1; TSM/Chla < 2.2 × 103). Conceptually, the Rrs behavior of the waters is
modeled as a function of absorptance of in-water substances and the water column
backscattering coefficient bb (Zhang et al., 2016). The inverse reflectance of Rrs(λi) in7.4.2
a band is expressed mathematically as the sum of three absorptance and
backscattering coefficients, divided by bb(λ):
(7.31)
where must be isolated from the total absorption α(λ). In extremely turbid inland
waters, α(λ) is dominated by αd(λ). It should be maximally sensitive to given the
significant positive relationship (R² > 0.90 at 542 nm) between SSC and ap through
the transition of ap(λ). This relationship takes into account the following facts: (i)
bb(λ) remains almost constant over 540–570 nm in waters dominated by aCDOM(λ)
and in which SSC hardly varies spatially; and (ii) in less turbid waters, the R2 value
between SSC and Rrs(540) is significantly higher than at any other wavelengths. In
other words, α(λ) is mainly conditioned by TSM. In the three-band model, TSM is
estimated from reflectance Rrs(λi) in three spectral bands λi, with the reciprocal
reflectance in the first two spectral bands as:
(7.32)
where a and b = constants whose value depends on the actual water constituents.
From the cruise-measured ap and awater spectra, the best wavelengths for are
determined as 542, 600, and 668 nm. Thus, ap is best estimated as:
(7.33)
Of the 11 available Landsat 8 OLI multispectral bands (Table 2.2), none can match ,
so they have to be defined as the average of two adjacent bands closely matching
them, or [Rrs(450-515)+Rrs(525-600)]/2,[Rrs(525-600)+Rrs(630-680)]/2, and Rrs(630-
680), respectively. The final semi-analytical model takes the following form:
(7.34)
When applied to 14 Landsat 8 OLI images, this semi-analytical model achieves an R2
of 0.85 and a normalized RMSE of 23%. Moreover, it is not imaging date-specific
and can be more flexibly applied to imagery data acquired at other times and over
different locations (Zhang et al., 2016). So they are more applicable geographically.
However, it remains unknown whether it is sensor-specific.
Sea Surface SalinitySSS is a key parameter for physical oceanography and the hydrologic cycle. Together
with SST, it dictates surface water density which impacts the vertical movement
through the thermohaline component of ocean circulation. SSS is also a tracer of the
water cycle because it is impacted by precipitations, evaporation, river outflow, and
ice melting. Spectrally, sea salinity has a negligible signature in optical bands. So it is
best estimated from low-frequency microwave bands, such as the ESA’s Microwave
Imaging Radiometer using Aperture Synthesis (MIRAS), onboard the Soil Moisture
and Ocean Salinity (SMOS) mission, and National Aeronautics and Space
Administration´s (NASA´s) Aquarius and Soil Moisture Active Passive (SMAP)
mission. SSS of the global ocean can be sensed using L-band radiometry.
SSS retrieval from satellite data requires removal of radiation contribution by
irrelevant factors to the signal. The retrieval process starts with calibrated antenna
temperature in four steps (Meissner et al., 2018): (i) removal of off-Earth
contributions by celestial sky, the sun and the moon; (ii) correction for the effects of
the antenna pattern, Faraday rotation, and the atmospheric attenuation. After
correction, the at-sensor BT is converted to sea surface BT; (iii) correction for surface
roughness to derive the flat ocean surface BT (TB0); and (iv) retrievals of SSS from
TB0 using an RTM or a maximum likelihood estimator. This estimator minimizes the
sum of squared difference between TB0 of two polarizations (vertical V and
horizontal H) and the RTM-derived TB0 based on the dielectric constant of sea water.
The general process of retrieval is illustrated in Figure 7.13. Apart from BT, the
salinity retrieval algorithm also requires a number of ancillary inputs, including SST,
atmospheric profiles, wind speed, wind direction, rain rate and rain flagging, and land
mask (if relevant). All of these ancillary data can be obtained from existing data
products.FIGURE 7.13 Flowchart illustrating the procedure of retrieving SSS from radar images
in the Aquarius Version 5 algorithm. (Meissner et al., 2018, open access.)
The core of SSS retrieval is an iterative algorithm that minimizes the Chi-square cost
function, generally expressed as:
(7.35)
where TBobs = observed TB, TBmod = modeled TB using the RTM, and the
normalization parameter , the estimated variance on TB. The sum of squared
differences in TB combines multi-incidence angles θ and multiple polarizations p.
Initially, the difference between observations and modeled (or initially guessed)
values is based on ancillary estimation for geophysical parameters, such as SST, SSS,
or wind, if needed. One or several geophysical parameters are adjusted to minimize
χ2 in subsequent iterations. Additional constraints may be imposed to avert the
iteration process from diverging from the initially guessed values excessively. The
second term in Eq. 7.35 introduces the difference between retrieved ancillaryparameters (Prtr) and their prior value derived from ancillary data (Panc). Such
constraints are inflicted so that SST and wind speed can be retrieved simultaneously
as SSS. Constraints can also be foisted on wind speed and radar cross-section.
Empirical wind speed derived in a prior step includes scatterometer observations. The
nature of TBs used in the minimization varies with algorithms, such as TOA TB in
SMOS, surface roughness-corrected TB in Aquarius ADPS and SMAP RSS (Remote
Sensing Systems), and surface TB that includes the wind effects in the Combined
Active-Passive retrieval algorithm. The flat sea surface TB is computed as:
(7.36)
If the seawater emissivity is assumed to be (1-R), the Fresnel reflectivity R of vertical
and horizontal polarization is given by:
(7.37)
(7.38)
where θ = radar incidence angle; and εr = sea water dielectric constant. The TB
dependence on SSS is via εr, and its dependence on SST is via both εr and the
transformation from emissivity to TB. Theoretical models predict a small dependency
of the roughness contribution to TB on SSS and SST, but it is of second order
compared to the flat surface component.
Several global salinity products have been produced from various satellite data
using different algorithms, including SMAP and SMOS. The SMAP product is
available from April 2015 onwards (www.remss.com/missions/smap/salinity/). The
SMAP SSS V5.0 data products (ftp://podaac-ftp.jpl.nasa.gov/allData/smap/L3/) are
validated and released at three levels (level 2C – swath data; level 3 – eight-day
running averages, and level 3 – monthly averages) by Remote Sensing Systems. The
near-polar orbit of SMAP allows for complete global coverage of the oceans in three
days with a repeat cycle of eight days. Aquarius Version 5 and the reduction of
spurious temporal and zonal biases over the open ocean remain consistent in the
SMAP Version 5.0 release. The 40-km and 70-km fields in V4.0 and V5.0 have been
consolidated as variables within a single data product. The 70-km data are based on
the smoothed 40-km product, which is retrieved directly from a geophysical model.
The 70-km fields are the best used for most open ocean applications, owing to their
significantly lower noise than the 40-km data.SMOS SSS products are available from Jan 2010 onward, produced from
combined ascending and descending passes at a few levels. Level 3 SMOS SSS
product is distributed by the Centre Aval de Traitement des Données SMOS (CATDS;
www.catds.fr/) based on the version 5 of reprocessing identified as CPDC (Centre de
Production des Données du CATDS) RE05 MIR_CSF3A (Figure 7.14). The original
maps are monthly averages oversampled on a ~25 km Equal Area Scalable Earth
Grid available for the period 01/2010–03/2017. Several other SMOS SSS products
are distributed either by the CATDS or the Barcelona Expert Center (BEC,
http://bec.icm.csic.es/data/available-products/).FIGURE 7.14 Global SSS distributions produced from satellite data over the period of
September 2011–May 2015, as obtained from the Argo (a) and SMOs (b) data
products. Grey color reports missing. (Dinnat et al., 2019, open access.)
The Aquarius product is available from August 25, 2011 until June 7, 2015 when the
spacecraft bus failed. The Aquarius end-of-mission (V5.0) salinity dataset was
released in December 2017 at two levels. Level-2 product is science data in swath
coordinates and matching ancillary data, and level-3 product contains gridded 1°
daily, weekly, and monthly salinity and wind speed maps. The standard Aquarius
level-3 data produced by the Aquarius Data Processing System (ADPS) use the
criterion for land fraction set as 0.01 (severe), which means values are excluded for a
grid point if its fraction of land area exceeds 1%. Consequently, the ADPS level-3
data have a high standard deviation of salinity biases induced by land contamination.
The RMSE of Aquarius level-2 and level-3 data products is estimated to be 0.17
practical salinity units (psu) and 0.13 psu, respectively (Kao et al., 2018). Validation
results suggest that the Aquarius data product has met the targeted accuracy of a
monthly average of 0.2 psu on 150 km scale. Caution needs to be exercised when7.5
7.5.1
7.5.1.1
using Aquarius salinity data in areas with high radio frequency interference and
heavy rainfall, close to the coastlines where leakage of land signals may significantly
affect the quality of the SSS data, and at high-latitude oceans where the L-band
radiometer has poor sensitivity to SSS.
IN-WATER BIOCHEMICAL CONSTITUENTS
Chlorophyll Concentration
Spectral Behavior
Chl-a is an integrative bioindicator of aquatic ecosystems common to almost all
photosynthetic organisms. Chl-a concentration is related to the live phytoplankton
biomass in the surface layer, commonly known as the amount of green pigment
chlorophyll. This concentration is frequently regarded as a proxy for estimating
phytoplankton biomass and treated as an indicator of water eutrophication level.
Accurate information on global Chl-a distribution is quite important and useful in
understanding the optical, biological, and biogeochemical properties of global ocean
waters. This parameter is important to bio-optical modeling of its absorption
coefficient and further differentiation of phytoplankton into specific groups. Together
with TSM, the concentrations of Chl-a can be used as proxies to assess phytoplankton
dynamics and particulate load. Chl-a is commonly retrieved from ocean color remote
sensing data. Satellite data allow Chl-a to be estimated at considerable frequency that
improves the effectiveness of monitoring programs (Harvey et al., 2015). In general,
there exists an inverse relationship between Chl-a content and sensor-captured
reflectance over the spectral range of 400–900 nm (Zhang et al., 2021). The inverse
relationship is particularly close at 443 nm, but much looser at 555 nm (Figure 7.15).
Such a relationship lays the foundation for satellite-based retrieval of Chl-a in coastal
waters at a high spatial and temporal resolution. The best spectral wavelengths of
sensing are blue and green bands.FIGURE 7.15 Comparison of Chl-a reflectance spectra at seven concentration levels over
the wavelength range of 400–900 nm. (Zhang et al., 2021, used with permission
(5761251240963) from Elsevier.)
Chl-a can be retrieved from a wide range of remote sensing data. Satellite images,
UAV images, hyperspectral data, and even radar images all allow it to be estimated.
These data have their pros and cons in the estimation. Satellite imagery such as
MERIS is able to capture the spatial dynamics and extent of phytoplankton blooms
owing to its synoptic view and high temporal resolution. MERIS satellite data enable
the operational retrieval of Chl-a concentrations at an accuracy and quantitative level
comparable to in situ measurements. However, UAV and hyperspectral data have
their own niche uses. In addition to the remotely sensed data used, Chl-a retrieval is
also affected by the type of water it is residing. Accurate quantification of Chl-a
concentrations in turbid Case II waters is challenging due to their optical complexity.
Typical Case II waters contain high concentrations of water constituents that have a
strong absorption in blue wavelengths, decoupling phytoplankton absorbance. The7.5.1.2
well-established band ratios for SSC retrieval no longer function well for the accurate
retrieval of Chl-a organic constituents that are more diverse in type and temporally
much less stable than TSS. This has prompted the adoption of various measures of
the NIR reflectance peak in Chl-a retrieval algorithms, such as the ratio of peak
reflectance to reflectance at 670 nm and the position of the peak in the form of Chl-a
indices.
Chlorophyll Indices
The quantity of Chl-a in a water body is commonly estimated based on VIs calculated
from diverse images. A total of four indices are derivable from drone images (Chan et
al., 2022). They are Normalized Green-Red Difference Index NGRDI = (G-R)/G+R),
Blue Normalized Difference Vegetation Index BNDVI = (NIR-B)/NIR+B,
Normalized Green-Blue Difference Index NGBDI = (G-B)/G+B), and Green
Normalized Difference Vegetation Index GNDVI = (NIR-G)/(NIR+G). Chl-a bears
an inverse correlation with NGRDI. With the training dataset, the use of all five
bands produces the best estimate. But validated against an independent dataset,
NGRDI is the best index, achieving the highest R² of 0.908 and the lowest RMSE of
5.34 μg ∙ L–1. They are much better than other individual drone bands and/or their
ratios. This index is the best predictor to use with drone images.
If quantified from space-borne images, surface Chl-a content can be derived from
more VIs owing to the availability of more spectral bands over a broader spectrum,
such as the normalized difference chlorophyll index (NDCI) and the MERIS
Maximum Chlorophyll Index (MCI). NDCI is a simple indexing method for
estimating Chl-a concentration in productive estuary and turbid coastal (Case II)
waters (Mishra and Mishra, 2012). It makes use of two bands, 665 and 709 nm
available from simulated MERIS bands, calculated as:
(7.39)
A quadratic function involving NDCI accurately explains the variance in the
simulated data over the Chl-a range of 1–60 mg ∙ m–3 (R2 = 0.95, p>0.0001). A
similar two-fold calibration and validation of Chl-a models using MERIS dataset
(Chl-a range: 0.9–28.1 mg ∙ m–3) yield an R2 of 0.94, and RMSE of ~2 mg ∙ m–3,
respectively (Table 7.7). These accuracy levels are better or significantly better than
those obtained via two- or three-band semi-analytical models.
NDCI calibration and validation results derived from simulated and MERIS
datasets demonstrate its potential applicability to widely varying water types andgeographic regions. Even if no ground truth data are available in remote coastal
waters, NDCI can still be used to qualitatively infer Chl-a concentration ranges in a
manner similar to NDVI in studying terrestrial vegetation.
Table 7.7
Validation accuracy of NDCI in retrieving Chl-a concentration for data points within the calibrated Chl-a
range (values inside the bracketed: obtained using the entire dataset) in comparison with four other semi￾analytical models and MERIS Case II data product
Indices RMSE (mg×m-3) R² Slope of regression model
Simulated dataset (n = 100)
NDCI 4.83 0.93 1.05
M09 5.26 0.92 1.07
T07 21.78 0.26 0.57
Field dataset (solar zenith angel, n = 20)
NDCI 1.89 0.80 1.005
M09 3.27 0.80 1.115
D05 1.97 0.81 0.795
T07 10.013 0.54 -0.544
G08 4.066 0.83 0.574
MERIS 5.856 0.74 0.403
Field dataset (Mobile Bay and Mississippi Delta, n = 14)
NDCI 1.43 (2.37) 0.94 (0.92) 0.88 (0.91)
M09 2.82 (5.17) 0.92 (0.94) 0.49 (0.45)
D05 2.69 (6.49) 0.91 (0.62) 0.51 (0.45)
T07 5.49 (10.08) 0.53 (0.04) -0.06 (-0.01)
G08 4.95 0.92 1.43
MERIS 7.3 0.94 2.29
Source: Mishra and Mishra (2012), with permission (5761251428122) from Elsevier.
Note: Boldfaced: best performing model. M09 – Two-band (red and NIR) semi-analytical model; T07 – Two
band (λ1 = 559, λ2 = 665 nm) model; D05 – Three-band (λ1 = 665 nm, λ2 = 708 nm, λ3 = 753 nm) model (see Eq.
7.42); G08 – Gons et al. (2008) model (see Eq. 7.42).
MCI measures the height of peak water-leaving reflectance at 709 nm relative to a
baseline extrapolated between bands on either side of 681 nm and 753 nm (Binding et
al., 2013). This index is founded on the fact that the ratio of red to NIR bands hasbeen shown to be superior to the standard blue to green ratio-based models in
detecting algal blooms under turbid, eutrophic conditions. It is designed to monitor
algal blooms in optically complex, turbid, eutrophic inland waters in low chlorophyll
and oligotrophic conditions. It is calculated from three MERIS bands as:
(7.40)
where 681, 709 = central band wavelength of MERIS bands 8 and 9, respectively. In
situ derived MCI bears a close linear relationship with Chl-a concentrations in Lake
Erie (MCIMERIS = 0.0004, R² = 0.70) where Chl-a concentrations range from 0.7 to
20.0 mg ∙ m–3 (Binding et al., 2013). Despite the strong correlation between in situ
MCI and surface Chl-a concentrations, MCI is significantly sensitive to mineral
scattering. At low Chl-a levels, mineral sediments substantially elevate reflectance at
700 nm and the resulting MCI, for instance, by 420% with mineral suspended
particulate matter of 20 g ∙ m–3 at a Chl-a concentration of 10 mg ∙ m–3. This increase
in reflectance uplifts MCI by 387%. However, this effect is significantly subdued at
higher Chl-a concentrations of 50 and 100 mg ∙ m–3, at which the reflectance at 700
nm increases by merely 80% and 38%, respectively, resulting in a rise of only 54%
and 21% in MCI. The MCI signal is clearly discernible even in typical turbid waters
with suspended particulate matter concentrations over 20 g ∙ m–3, suggesting that
MCI is capable of detecting blooms even in sediment-laden river plumes.
MERIS MCI product has been generated for intense algal blooms in turbid Lake
Erie waters (Figure 7.16). The MCI product has proved to be a versatile data source
in monitoring intense surficial algal blooms within the Chl-a concentration range of
10–300 mg ∙ m–3, but it has a limited applicability to low-biomass conditions. MCI￾chlorophyll relationships may vary regionally due to shifts in the wavelength position
of the MCI peak over varying Chl-a ranges, and variations in the IOPs of water
coloring constituents, which potentially hinders the derivation of a universally
applicable MCI product quantitatively.7.5.2
FIGURE 7.16 L1 MCI image of Lake Erie during a reported algal bloom on October 8,
2011 (bottom), in comparison to MERIS true-color composite (top). The L1 MCI
product is calculated from the TOA radiance and therefore has the unit of mW×m￾2×nm–1×sr-1. (Binding et al., 2013, with permission (5761260108980) from Elsevier.)
Retrieval Methods
Quantitative remote sensing of Chl-a is possible via ocean color that can be a proxy
for phytoplankton biomass. Many algorithms have been developed to estimate global
Chl-a concentrations from Ocean Color (OC) satellite data. They fall into three types:
bio-optical, empirical, and semi-empirical. Bio-optical algorithms exploit the
upwelling radiation in blue and green bands. Semi-analytical models estimate Chl-a
from ocean color sensors by relating water AOPs to its IOPs. In spite of having a
good physical interpretation, semi-analytical models involve a variety of water
optical parameters that are troublesome to parameterize, and may not suit inland7.5.2.1
waters due to their complex optical properties. Empirical models are established via
regression analysis while semi-empirical models are based on within-water light
transfer. Using band (ratios) as the predictor variables, empirical statistical models
are simple and easy to construct, and may have a wide range of applicability, albeit
not universal. The three types of models are explained in detail below.
Bio-optical Algorithms
Bio-optical algorithms are widely used for the operational retrieval of global Chl-a
concentrations from OC satellite data in turbid ocean waters where other algorithms
are less effective. They are commonly known as OC algorithms involving two to four
wavebands. These algorithms take advantage of decreased reflectance at 440–510 nm
(blue) and increased reflectance at 550–565 nm (green) by using their ratios. The
two-band OC2 algorithm assigns the wavelength closest to 490 nm as λb. The three￾band version, OC3M for MODIS, uses bands near 443 and 490 nm and relies on the
maximum band ratio determined as the greater of Rrs(443)/Rrs(551) or
Rrs(488)/Rrs(551) (O’Reilly and Werdell, 2019). It serves as the sole predictor
variable in the estimation model. The OC4v4 version is developed for SeaWIFS
images involving four bands of 443, 490 (488), 510, and 555 (551) nm. It estimates
Chl-a using a fourth-order polynomial (coefficients) logarithmic model (Eq. 7.41),
still based on the maximum band ratio determined as the greater of Rrs(443)/Rrs(555),
Rrs(490)/Rrs(555), or Rrs(510)/Rrs(555) values (O’Reilly and Werdell, 2019):
(7.41)
where ai (I = 0, 1, …, 4) = polynomial coefficients to be determined via regression
analysis; X = log[Rrs(λb)/Rrs(λg)]. λb,g = wavelength of blue and green bands.
The OC algorithm family is subsequently augmented to include two new members
of OC5 and OC6. They make use of the 412 nm band in deriving the maximum band
ratio because chlorophyll-specific absorption peaks at 443 nm available in most OC
spectral bands, the magnitude of chlorophyll-specific absorption at 412 nm can reach
upwards of ~70% of that at 443 nm (O’Reilly and Werdell, 2019). The OC5
algorithms, such as OC5_SEAWIFS, exploit up to four bands in the numerator of the
maximum band ratio and one band in the denominator. In addition to the blue (443
nm), cyan (490 nm), and cyan/green (510 nm) bands used in OC4, OC5 also exploits
the violet (412 nm) band. Rrs(412) is generally larger than Rrs(443) and, therefore,
dominates the maximum band ratio from the lowest Chl-a to approximately 0.3 mg ∙
m-3. Consequently, the role played by Rrs(443) in OC5 is subordinate to that of7.5.2.2
Rrs(412). The Chl-a results retrieved using the latest Version 7 of OC4, OC5, and
OC6 attain a closer agreement with in situ measurements than their Version-6
precursor for >97% of the satellite–in situ pairs for SeaWiFS data and for >99% of
MODIS Aqua data when considering only Chl-a > 0.2 mg ∙ m–3. The R2 for OC5
(0.838) is slightly lower than that for OC4 (0.851). OC6_SEAWIFS employs the
mean of Rrs(555) and Rrs(670) in the denominator of the multi-band ratio,
approximating radiance for a band at 613 nm. Consequently, the multi-band ratios
have a high value with this reposition to bands with longer wavelengths and lower
Rrs. The 412 nm band is the brightest band (namely, having the most dominant
magnitude) in remotely sensed reflectance retrieved by heritage passive OC
instruments when Chl-a falls below ~0.1 mg ∙ m–3. OC6_SEAWIFS achieves a
marginally higher R² (0.851) than OC5_SEAWIFS (0.836) and is equivalent to that of
OC4_SEAWIFS between model and in situ data. Application of these OC bio-optical
algorithms to atmospherically corrected bands converts normalized water-leaving
radiance data to Chl-a concentrations in turbid productive waters from a
cyanobacteria algorithm.
Regression Methods
Empirically established via statistical analysis, regression models can be linear or
non-linear (e.g., logarithmic) in the form of (cX + d), power (cXd), and exp(dX),
where c and d are the coefficients determined via least-square best-fitting; X denote
the ratio of DNc between two color bands. They have multiple terms involving
singular or multiple bands and their ratios. Even six-band ratios of drone imagery
have been attempted, including r/b, b/r, g/b, b/g, r/g, and g/r, where r = DNc(R); g =
DNc(G); b = DNc(B) (Cheng et al., 2020). These models based on data even without
atmospheric correction achieve an acceptable R2 value except the ones involving
green and blue band ratios. The best ratio is red/blue, followed by red/green. Their
effectiveness is not affected by the spatial resolution of the imagery used. Of the three
types of models, linear ones are the most accurate while the exponential ones are the
least reliable if r/b is their sole predictor variable irrespective of the image resolution
(Table 7.8). The power or exponential models are more accurate if b/r and r/b serve as
the predictor variables.
Table 7.8
Coefficient of determination (R²) of three types of regression models involving band ratios as the predictor
variables at five pixel window sizes generated using training dataPixel window
Estimation
model
Band ratio
R/B B/R G/B B/G R/G G/R
0.2 m ×0.2
m Linear 0.85 0.71 0.27 0.25 0.80 0.64
Power 0.76 0.76 0.28 0.28 0.68 0.68
Exponential 0.69 0.76 0.29 0.27 0.66 0.65
0.5 m ×0.5
m Linear 0.85 0.70 0.27 0.26 0.81 0.64
Power 0.76 0.76 0.29 0.28 0.68 0.68
Exponential 0.69 0.76 0.29 0.28 0.67 0.65
1.0 m ×1.0
m Linear 0.84 0.71 0.27 0.25 0.79 0.64
Power 0.76 0.76 0.28 0.28 0.69 0.69
Exponential 0.69 0.77 0.28 0.26 0.68 0.66
1.5 m ×1.5
m Linear 0.84 0.71 0.27 0.25 0.80 0.65
Power 0.76 0.76 0.29 0.29 0.69 0.69
Exponential 0.69 0.78 0.29 0.27 0.67 0.66
2.0 m ×2.0
m Linear 0.84 0.71 0.27 0.26 0.78 0.63
Power 0.77 0.77 0.32 0.32 0.67 0.67
Exponential 0.70 0.79 0.31 0.32 0.64 0.64
Source: Modified from Cheng et al. (2020), with permission (5761260260615) from Elsevier.
Boldface: best model.Validated against one-year field data, Chl-a concentration between 0 and 15 μg ∙ L–1
can be well estimated from the red/blue band ratio with an R² of 0.86. The model
performance deteriorates significantly for Chl-a >15 μg ∙ L–1, but is less satisfactory
for Chl-a >15–20 μg ∙ L–1 as the ratio as a function of algal biomass is much less
sensitive to Chl-a >15 μg ∙ L–1. Drone images are applicable to estimating algal
biomass concentrations up to and beyond the bloom level of around 10 μg ∙ L–1, but
the applicability of such visible image bands to extreme bloom levels may be
compromised by other complex factors not accounted for by the band ratios (e.g.
changing cell species and structure during the collapse of a bloom) (Table 7.9). The
highest accuracy is achieved by the spectral bands model, followed by the
exponential model with R²/NIR as the predictor variable. They are much more
accurate than the remaining models in estimating surface Chl-a concentrations.
Table 7.9
Accuracy of estimating surface Chl-a concentrations from drone image-derived variables based on
exponential and linear combination models
Predictor
variable X Estimation model
RMSE
(μg × L–
1)
RMSE
(μg × L–
1)
R²
Training (n =
46) R²
Validation (n
= 52)
R²/NIR Exp(-0.4694 + 55.083X) 0.709 7.17 <0 10.18
R²/RE Exp(-0.5203 + 64.676X) 0.692 7.33 <0 10.07
R²/B Exp(0.0183 + 67.39X) 0.622 7.64 0.730 7.18
R/B Exp(-0.9696 + 3.080X) 0.554 9.05 0.813 7.06
NGRDI 0.3314 0.506 8.30 0.908 5.34
Spectral
bands
Exp(0.341-39.5B + 5.43G
+113.1R+400.0RE￾488.7NIR) 0.741 6.68 0.163 7.13
Source: Chan et al. (2022), with permission (5761260411721) from Elsevier.
Note: RE = red edge.
The performance of 65 empirical Chl-a algorithms has been assessed using data
acquired by 25 OC sensors, with a median R2 of 0.859 among 903 pairwise
comparisons with OC-modeled chlorophyll (O’Reilly and Werdell, 2019). Similar to
OC4, Rrs(490) dominates over other bands at intermediate Chl-a levels from ~0.3 to
~1.6 mg ∙ m−3 and Rrs(510) generally dominates above ~1.6 mg × m−3. The7.5.2.3
performance of these empirical models varies with the data (spectral bands) used and
the water body under study, but the identified most useful spectral bands
(wavelengths) should remain unchanged with new satellite data.
Compared with multispectral data, hyperspectral data have not been used to
estimate Chl-a frequently because of the difficulty in processing them. One method
of analyzing these data is to use PLSR that decomposes hyperspectral data and water
quality variables into their latent structure to generate the eigen vectors of the
independent variables (e.g., hyperspectral bands and their derivatives). These eigen
vectors represent the relevant information present in the reflectance spectra useful for
predicting water quality parameters. The components or latent variables are derived
from the covariance between in situ measured water quality parameters and
reflectance variables. The corresponding scores not only explain the variance of the
independent variables (water quality parameters) but also have high correlation with
the response variables (spectral variables). They allow the identification of the
optimal bands to characterize most of the variance in the dependent variable. PLSR
accounts for the correlation between the signals in multiple spectral bands, and
enhances the potential discrimination and accuracy of the retrieved in-water
constituents.
The selection of the optimal number of PLSR components or latent variables
(refer to Section 4.2.3 for more details) is a key step in constructing a model with
strong predictive power. A PLS model to a full range (400–900 nm) of continuous
narrow spectral bands produces R2 = 0.84 (RMSE = 1.18 μg ∙ L-1) in estimating Chl-a
concentrations, illustrating its potential for isolating and extracting absorption
features characteristic of assorted color-producing agents in optically complex Case
II waters (Ali and Ortiz, 2016).
Semi-analytical Models
Semi-analytical models express the relationship between IOPs and multi-band
reflectance. The simplest semi-analytical model for estimating Chl-a in turbid waters
involves three bands in a form resembling Eq. 7.32 (Gitelson et al., 2008). In Case II
waters, the semi-analytical algorithms retrieve Chl-a from the reflectance peak near
700 nm. The three most useful wavebands are red (665 nm) and NIR (708 and 753
nm) (Dall’Olmo and Gitelson, 2005), embodied conceptually as:
(7.42)The validity of this model is guaranteed if three assumptions are met: (i) There is an
equal amount of absorption by suspended solids and CDOM beyond 700 nm and
between 665 and 675 nm or the disparity between them is too negligible to be
account for; (ii) The absorption by Chl-a, CDOM, and TSM ceases to exist beyond
730 nm; and (iii) Chl-a has a spectrally invariant backscattering coefficient.
If retrieved from MERIS data, the use of Rrs at three wavelengths solves for Chl-a
absorption at 665 nm, and estimates Chl-a content from the specific absorption
coefficient of Chl-a and the ratio of Rrs(709) to Rrs(665) as (Gons et al., 2008):
(7.43)
where bb = apparent backscattering coefficient (m-1) calculated as:
(7.44)
Chl-a can be retrieved from MERIS data using the two-band and three-band NIR-red
models as:
(7.45)
(7.46)
Both algorithms yield consistently and highly accurate estimates of Chl-a content
(range: 1.09–107.82 mg ∙ m-3), with a MAE of 4.32 and 4.71 mg ∙ m-3, respectively,
and a RMSE as low as 5.92 mg ∙ m-3. These models eradicate the need for case￾specific reparameterization of the algorithms, if the specific absorption coefficient of
phytoplankton in the water does not vary drastically. Therefore, the NIR-red
algorithms can potentially be used as the standard routine models for operational
near-real-time quantification of Chl-a concentration elsewhere (Gons et al., 2008).
Incorporating three MERIS spectral bands of 660–670, 703.75–713.75, and 750–
757.5 nm, the three-band algorithm-predicted Chl-a is strongly correlated with
observed Chl-a (R2 > 0.96), with a precision of 32% and an average bias of -4.9%–
11% across waters of widely varying Chl-a concentrations (1.2 to 236 mg ∙ m-3),
SDD (0.18 to 4.1m), and turbidity (1.3 to 78 nephelometric turbidity unit) (Gitelson
et al., 2008). Application of the two-band model to two MODIS spectral bands (λ1 =
662–672, λ3 = 743–753 nm) yields Chl-a estimates that are closely correlated with
the observed Chl-a content (R2 > 0.92). These algorithms can be used for quantitative
monitoring of Chl-a in turbid waters from the extensive database of MERIS and7.5.2.4
MODIS images at high accuracy if the red and NIR bands are calibrated for the
atmospheric effects (Figure 7.17).
FIGURE 7.17 Global chlorophyll-a distribution on 24 May 2016 retrieved from MODIS
data. (https://svs.gsfc.nasa.gov/30786/).
Machine Learning Methods
In both empirical and semi-analytical methods, the number of variables considered in
a model amounts to no more than five or six. With the use of hyperspectral data, the
number of spectral bands can easily exceed a hundred. The consideration of so many
variables in the quantification is ideally manipulated by machine learning methods,
including RF, K-nearest neighbor (KNN), SVR, and deep neural network (DNN). The
inputs needed for NN analysis can be served by suitable images paired with in situ
collected Chl-a measurements. Given that Chl-a in different ranges has a quite
distinctive spectral signature, it is conducive to achieving a higher prediction if it is
clustered into spectral groups, such as nearshore murky waters and offshore
transparent waters using fuzzy clustering. This clustering requires the specification of
two input parameters: the number of clusters c to be returned by the clustering and
the weighting exponent m, which can be any real number larger than 1. Consequently,
multiple NNs runs may be necessary: one encompassing the whole clusters, and
another using only pixels confined to a specific cluster. All NN-based algorithmshave an operational range defined by the optical characteristics of the waters for
which they are trained. The disparate results are later consolidated to obtain the final
chlorophyll concentration map.
The same estimation procedure can then be replicated for each of the spectral
clusters using the NN method to predict Chl-a from spectral data of individual bands
and geometry (e.g., sun zenith, view zenith, and difference between the view and the
sun azimuth). Although these values hardly vary in a given image, especially if the
area under study is very small, noticeable variations do appear among multi-temporal
images, even acquired by the same sensor. The estimation using an MLP NN is
accomplished in three phases: (i) network design, including network configuration,
such as the number of input and output nodes, the number of hidden layers, and the
activation function for each layer; (ii) network training, in which the previously
mentioned BP learning procedure is used according to the pre-established strategy;
and (iii) results validation, in which the performance of the trained MLP is evaluated
using a set of parameters, including reflectance and geometry.
The NN trained with the complete dataset attains an R2 of 0.77 (training dataset) and
0.63 (validation dataset) using the whole imagery data in estimating Chl-a (Vilas et
al., 2011). The estimation accuracy is rather satisfactory for low-to-medium Chl-a
concentrations, but not >4 mg ∙ m-3, with the mean absolute prediction errors >1 mg ∙
m-3 (Table 7.10). Predictions are significantly improved if the NN is trained using
high-quality data. The most abundant cluster (e.g., deep water) has the highest
prediction accuracy of R2 = 0.86, MPE = -0.14, RMSE = 0.75 mg ∙ m-3, and relative
RMSE = 66% in comparison with the validation dataset. The peaks of Chl-a in both
training and validation sets are quantified accurately. The separation of the waters
into spatial clusters improves the retrieval accuracy of Chl-a in Case II waters (Figure
7.18).FIGURE 7.18 Distribution of Chl-a in the coastal waters of Galician rias, NW Spain on
November 17, 2003, estimated from MERIS images using NN. (a) classified MERIS
image showing three clusters; (b) Chl-a map after the application of the NN for high￾quality data. Gray color represents pixels belonging to Cluster #2 and Cluster #3.
(Vilas et al., 2011, with permission (5761260670687) from Elsevier.)
Table 7.10
Summary of the validation parameters computed using the training and validation data sets for each of the
three NNs
NN Dataset N R² MPE VAR RMSE RMSE%
#1: Complete
dataset (n =
150)
Training 120 0.77 -0.03 0.66 0.81 67
Validation 30 0.63 0.08 1.04 1.00 74
#2: Cluster #1
dataset (n =
119)
Training 92 0.86 0.02 0.68 0.68 60
Validation 24 0.78 0.02 0.99 0.99 72
#3: Cluster #1,
quality = 9 (n
= 83)
Training 66 0.97 0.01 0.10 0.32 41
Validation 17 0.86 -0.14 0.57 0.75 66
Source: Vilas et al. (2011) with permission (5761260848134) from Elsevier.
All the models achieve an accuracy very close to each other except the ratio at 665
nm (Table 7.11). In terms of RMSE, the best is the exponential model involving
reflectance at 680 and 710 nm. As expected, in situ measured hyperspectral data7.5.3
enable Chl-a at a wide range of concentrations to be estimated at a higher accuracy
(R² up to 0.89) than multispectral bands.
Table 7.11
Comparison of common models used to estimate Chl-a concentration and their accuracy based on
hyperspectral data
Models R²
RMSE (μg ×
L–1) Reference
0.77 23.26 Randolph et al. (2008)
0.88 3.66 Sun et al. (2009)
Look up table
method 0.83 18 Tan et al. (2016)
0.89 10.2
Thiemann and Kaufmann
(2000)
0.89 1.53 Lyu et al. (2022)
Source: Modified from Lyu et al. (2022), open access.
Note: The results are based on in situ hyperspectral data, not airborne or space-borne hyperspectral imagery
data. This method only provides a methodological reference for the application of hyperspectral satellite images.
Eutrophication
Eutrophication is a process characterized by an increased concentration of nutrients
(mainly phosphorus and nitrogen) in an aquatic ecosystem. Water enrichment by
nutrients, and toxin-producing cyanobacteria (blue-green algae) blooms poses a
significant threat to the quality of surface water. Nuisance cyanobacterial blooms
degrade water quality and reduce its aesthetic appeal through the produced toxins,
scums, and foul stenches. The degree of water eutrophication is gauged by such
indicators as Chl-a, phycocyanin (PC), TSM, and SDD, as depicted by the Carlson’s
trophic state index (TSI) model. It is based on the empirical relationships among total
phosphorus (TP), Chl-a content, and SDD. Phosphorus (P) is an important substance
for the growth of phytoplankton. Chl-a is a pigment common to almost all
photosynthetic organisms, as a proxy for algal biomass.
The trophic state of inland waters is typically categorized into three levels:
oligotrophic, mesotrophic, and eutrophic, and is quantitatively measured by TSI that
has been widely used to assess water trophic level and classify water type. The
trophic degree of a water body is assessed from their Chl-a and phosphorusconcentrations usually determined in water samples and fed to an equation. They
have expanded to include remote sensing data. Copious remote sensing data sources
and a series of algorithms have accumulated for assessing the trophic state of inland
waters, either based on satellite-derived or in situ measured hyperspectral reflectance
data. If quantified from satellite images, TSI is calculated from the ratio of the NIR to
red bands to infer the Chl-a concentration in waters. For instance, the ratio of
TM4/TM3 can be used as a proxy of TSI mainly in eutrophic waters (Novo et al.,
2013). This TSI proxy is useful to map areas subject to recurrent phytoplankton
blooms to support mitigation efforts. However, the use of a single parameter can lead
to a sizable error, sometimes even up to two orders of difference. In general, the
trophic level of surface waters is affected by the synergistic interactions between
multiple parameters, and the dominant factor also varies with water type (Lyu et al.,
2022).
Thus, TSI has been empirically determined from three water quality parameters of
Chl-a, SDD, and TP according to the multi-parameter Carlson model based on their
empirical relationships. Two opposite causal relationships exist between them. A
higher TP concentration is conducive to algae thriving, resulting in an elevated Chl-a
concentration, and a corresponding reduction in SDD. Conversely, an increase in
suspended particulate matter in the water decreases SDD, which reduces
phytoplankton photosynthesis in the water column and causes a decline in standing
stocks of Chl-a. In this case, there will be little Chl-a lingering in the water in spite of
the high TP. TSI is empirically estimated as a linear combination of the three factors
in the form of (Lyu et al., 2022):
(7.47)
where TSI(Chla), TSI(SDD), and TSI(TP) = trophic state index in relation to Chl-a
(μg∙L-1), Secchi disk depth (m), and total phosphorus (mg∙L-1). They are calculated
as:
(7.48)
(7.49)
(7.50)
Correlation and first derivative analysis reveals that TP, SDD, and Chl-a are the most
sensitive spectrally at Rrs(440), Rrs(575), Rrs(680), and Rrs(710). In quantifying the7.5.4
eutrophication level, Carlson’s TSI achieves a high estimation accuracy of R2 (0.88),
a low RMSE (3.87), and a low MRE (6.83%) (Lyu et al., 2022). However, TSI-based
estimation methods are only applicable to turbid waters of a low Chl-a concentration.
It remains unknown whether they are applicable to other waters in different
geographic areas due to the differences in climate (especially temperature regime),
the surrounding environment, and IOPs of the water body. The dominant factors
triggering eutrophication require Ternary contribution analysis to determine. It can
reveal the contribution of TP and transparency to the observed eutrophication pattern.
Algal Blooms and Biomass
Elevated levels of water nutrients, such as TN and TP through agricultural waste,
industrial or sewage discharge, coupled with warmer temperature, commonly spur
algal growth, and sometimes even result in nuisance algal blooms in inland waters.
Blue-green algal blooms in lakes and reservoirs are detrimental to the aquatic
ecosystems and often pose a potentially grave health hazard to people in contact with
the polluted waters. They threaten the aquatic ecosystem biodiversity and are
unpalatable for commercially valuable shellfish in many coastal waters. Persistent
and epidemic algal blooms have reportedly been blamed for fish mortality, shellfish
poisoning, physiological impairment, and numerous ecological and health problems.
Ideally, algal blooms in turbid, eutrophic waters are quantified from satellite images
over extensive waters at high accuracy owing to their birds-eye view. Algal blooms
can be much more reliably and readily detected using just red and NR bands than the
standard blue to green band ratio. They are easily quantifiable using the algal bloom
index (ABI), calculated from the normalized water-leaving radiance (nLw) at three
visible wavelengths of 443, 490, 555 nm as follows (Ahn and Shanmugam, 2006):
(7.51)
The use of nLw instead of Lw is able to better capture the spectral changes of algal
blooms in VIS bands. The threshold value of a is assumed unity, which allows the
calculation to work well as phytoplankton absorption is the important driver of
reflectance in the blue (443 nm) band, and other factors regulate the 555 nm
reflectance. ABI value ranges from 0 (zero possibility) to 10 (most likely, maximum
possibility). ABI should be maximally sensitive to bloom variability but minimally to
in-water substances other than phytoplankton such as suspended sediments and
CDOM in typical Case II waters. However, the restrictive range of ABI values (0–10)
cannot account for widely ranging Chl-a variability in diverse waters. Thus, theamplitude of the ratio is modified by X′ (Eq. 7.52), a variable derived from two
conventional nLw ratios commonly used in the standard bio-optical algorithms of
OC3 and OC4v4 to reduce uncertainties in the retrieved Chl-a content:
(7.52)
A larger number of in situ measurements are needed to link with Chl-a
concentrations. These two parameters are tightly and inversely correlated with each
other (R2 = 0.90, n = 648), even though Chl-a data are collected from a wide variety
of waters (Shanmugam, 2011). The Chl-a data and can be fitted as a power function:
(7.53)
where εchl−a = initial estimate of Chl-a concentration constrained to similar optical
regimes from where the large in situ data are collected and used to derive the above
model. To make this relationship applicable to a wide range of water types and to be
well-suited for use with other data, it is further tuned by correlating εchl−a with ABI_
Chla products as:
(7.54)
The above model is globally applicable. To achieve the best results for a local area,
the coefficients in the model need to be locally optimized using the same datasets as
in Eq. 7.54 so as to minimize uncertainties in the retrieved Chl-a. Although this new
bio-optical ABI algorithm is developed based on a large in situ dataset for accurately
assessing Chl-a concentrations and mapping algal blooms in complex marine
environments, it can be extended to inland lake waters and reservoirs after fine tuning
instead of wholesale reparameterization of the algorithm. However, doing so requires
an understanding of the specific optical properties of the new environments. In
complex waters, the ABI algorithm can readily differentiate algal blooms from
shallow water features, and accurately retrieve Chl-a of high values in truly bloomed
waters and relatively low values in highly scattering waters (Shanmugam, 2011). It is
more accurate than the OC3 and OC4 algorithms (Table 7.12).
Table 7.12
Performance comparison of three algorithms in retrieving algal blooms from OCLI data
Algorithm MREa(%) RMSE Intercept Slope R² N
Carder in situ datasetAlgorithm MREa(%) RMSE Intercept Slope R² N
ABI -17.44 0.2366 -0.0859 0.9461 0.8668 643
OC3 -52.81 0.3105 -0.2127 0.9706 0.8560 643
OC4 -38.53 0.3068 -0.1830 0.8683 0.8466 643
NOMAD in situ datasetb
ABI 40.06 0.2723 0.0735 1.0601 0.8190 2074
OC3 -6.75 0.2677 0.0060 1.1149 0.8148 2074
OC4 25.14 0.2658 0.0404 1.0018 0.8271 2074
NOMAD SeaWiFS-matchup dataset
ABI -17.97 0.2110 -0.0461 0.9946 0.8714 221
OC3 -57.68 0.2523 -0.1442 1.0033 0.8689 221
OC4 -37.67 0.2348 -0.1046 0.9350 0.8709 221
Source: Shanmugam (2011), free access.
Notes:
a Mean relative error.
b NOMAD - NASA bio-Optical Marine Algorithm Data.
The detection and identification of algal blooms may be accomplished using near￾real-time global daily Chl-a anomaly products generated from satellite OC
measurements such as those from the VIIRS onboard the SNPP and NOAA-20
satellites (Wang et al., 2021). The detection is based on the Chl-a anomaly in
difference and the Chl-a anomaly ratio (or its relative difference) in comparison with
the 61-day Chl-a median value from the previous period (as a Chl-a reference). These
two Chl-a anomaly products represent the global ocean and nearshore (inland) Chl-a
abnormality and can comprehensively characterize the daily phytoplankton (or algae)
biomass status in relation to the normal condition. The two products are routinely
produced daily and distributed at www.star.nesdis.noaa.gov/socd/mecb/color/. The
VIIRS global daily Chl-a anomaly products are useful to detect harmful algal blooms
in oceanic, coastal, and inland waters.
Algal biomass in non-bloom conditions is ideally quantified from OLCI data. The
estimated result is column-integrated biomass (CIB) obtained using either process￾oriented or result-oriented methods. Process-oriented methods aim to retrieve the
profile under the assumption that the Chl-a profile satisfies distribution functions.
Result-oriented methods directly establish the quantitative relationship between the
concentration of surface Chl-a (Chlasurf) and CIB with little regard to the process of
profile modeling. All result-oriented methods involve three assumptions: (i) Thevertical distribution of Chl-a concentration satisfies one of the simulation functions;
(ii) Chl-a below a certain depth is considered to be uniform (i.e., the certain depth is
the mixed layer); (iii) CIB is linearly related to Chlasurf (Eq. 7.55), and the linear
parameters are functions of depth (Eqs. 7.56 and 7.57) (Bi et al., 2019), or:
(7.55)
(7.56)
(7.57)
where CIBest = estimated column concentration (mg∙m-2) of Chl-a; fslope and fint =
slope and intercept of the function, respectively; h = height (depth) of the water
column in meters; and ai and bi = parameters of functions fslope and fint; Chlasurf(Rrs)
= estimation model of Chlasurf based on Rrs. Chlasurf is commonly estimated from
ocean satellite data using several methods, including the band ratio algorithm, three￾band algorithm, fluorescence line height, and the Universal Model of Chl-a (UMOC)
estimation for Case II waters, one of the advanced three-band algorithms. The result￾oriented methods are applicable to estimating CIB in non-bloom conditions,
achieving a MAPE of 10.55% and a mean ratio (MR) of 0.996 (R² = 0.691). Chlasurf
is best estimated using UMOC (MAPE = 15.34%, MR = 1.009). It slightly
outperforms the three-band algorithm (MAPE = 15.80%, MR = 1.030), and is
noticeably more accurate than the band ratio algorithm (MAPE = 21.07%, MR =
1.053). Therefore, CIB can be estimated using the UMOC for Case II waters (R² =
0.691). CIB is estimated from Chlasurf according to its linear relationship with
Chlasurf after it has been estimated using the same analytical methods at r of 0.648
(UMOC) (Bi et al., 2019). CIB bears a linear relationship with Chlsurf at an R² = 0.96
at a depth of 3 m, and even a higher R² value at a shallower depth.
Algal blooms may be quantitatively scrutinized using synoptic monitoring, algal
pigment analysis, and a deep learning model, based on water surface reflectance.
Major pigments include Chl-a and phycocyanin while accessory pigments include
lutein, fucoxanthin, and zeaxanthin, and absorption coefficients. Their concentration
can be quantified based on the reflectance and absorption coefficients using one￾dimensional convolutional neural network (1D-CNN) (Pyo et al., 2022). It uses 1D
filters with a size of 1 χ s (s: filter size). The extracted feature is output as:
(7.58)where = output layer t; = input layer t-1; = weight with a 1D filter size s; = bias of
layer t; Conv1D = element-wise multiplication, and f = activation function. The
network is configured as multiple fully-connected layers so as to simultaneously
estimate biomass and accessory pigment concentrations. Batch normalization may be
adopted to enhance stable model learning by regularizing the input features,
determine the max-pooling layer to minimize computation, extract the highlight
features, and obtain a dropout layer to prevent model overfitting (Figure 7.19). A
convolutional block attention module applied to the first convolutional layer in the
1D-CNN model can identify the importance of pigments, highlight important spectra,
and suppress insignificant bands (Pyo et al., 2022). The attention module is expressed
as:
FIGURE 7.19 Flowchart for estimating biomass and accessory pigments of water from
drone images using the CNN deep learning model. (a) acquisition of the reflectance
and absorption coefficient data from field monitoring and experimental analysis; (b)
and (c) application of the deep learning model to estimate the pigments; (d) and (e)
another deep learning model designed to estimate the absorption coefficient; (f) and
(g) application of drone-acquired reflectance to the trained deep learning model to
generate the estimated absorption coefficient map; (h) and (i) preparation of the input
data to be fed to the deep learning model. (Pyo et al., 2022, open access.)(7.59)
where Aw = weights of the refined input features; Conv1D = one-dimensional
convolution layer; Pave = average pooling of input; Pmax = maximum pooling; =
results of Pave and Pmax concatenation that allows the consideration of comprehensive
spectral features in the input; = filter with a size of 1 × 7. The 1D-CNN model must
be trained with in situ sample data. The trained 1D-CNN model is applied twice, first
to the reflectance data to generate an absorption coefficient distribution layer.
Second, the previously trained model is applied to the distribution layer together with
reflectance and absorption spectra in each pixel to produce spatial distribution maps
of Chl-a and pigments (Figure 7.20).
FIGURE 7.20 Spatial distribution maps and concentration levels of major pigments
generated from a Cary-5000 UV-Vis-NIR spectrophotometer by the 1D-CNN model.
(a) ground truth RGB image, (b) chlorophyll-a map, (c) phycocyanin map, and (d)
histogram of pigment concentration. (Pyo et al., 2022, open access.)
Periodic trends of Chl-a, phycocyanin, lutein, fucoxanthin, and zeaxanthin can be
modeled using 1D-CNN with R2 values ranging from 0.71 to 0.87. Moreover, the
trained 1D-CNN model can successfully generate spatial information of primary and
secondary pigments regarding the spots of interest from hyperspectral images. Deep
learning models can be extended to drone-acquired hyperspectral data for analyzing
various algal pigments to gain a deeper understanding of algal blooms.
Of the two types of algae (macroalgae and microalgae), macroalgae biomass is
relatively easy to estimate from remote sensing data even at a high accuracy level, as
exemplified by Pelagic Sargassum. This unique type of brown algae is found mainly
in the Atlantic Ocean that serves as a critical habitat and refuge for various marine
organisms. Sargassum biomass can be estimated from image-captured surface
reflectance using the alternative floating algae index (AFAI) as:
(7.60)where subscript = central wavelength in nm. This index is then linked to Sargassum
biomass density empirically. The nature of the established relationship between them
varies with biomass density. At low densities (<0.93 kg∙m-2, AFAI <0.04), AFAI
increases linearly with density (R2 = 0.98) (Wang et al., 2018). At higher densities,
the relationship becomes non-linear and can be precisely depicted by a second-order
polynomial equation (Eq. 7.61). Thus, the AFAI biomass density model is piecemeal
as:
(7.61)
Assessed against measurements independently collected both in the bucket and in the
ocean, the estimated density has a mean uncertainty of ~11%, and a relatively high
consistency for both low and higher densities (Wang et al., 2018). Additional
uncertainties stem from the variable atmospheric conditions. Under different
conditions, MODIS-derived AFAI has a mean relative uncertainty of 1.2%
(maximum = 2.0%). Considering all uncertainty sources, the overall uncertainty in
the modeled Sargassum biomass density is conjectured to be lower than 12% for a
local patch (Figure 7.21).
FIGURE 7.21 Mean Sargassum density (%) (a) and biomass density (g•m-2) (b) in July
2005 in the Caribbean Sea and central West Atlantic retrieved from MODIS data using
the AFAI (resolution: 0.5°). (Wang et al., 2018, free access.)
Compared with macroalgae, the biomass of microalgae such as microphytobenthos is
much more challenging to quantify because they are found mainly in estuarine tidal
flat ecosystems that are subject to tidal fluctuations and that have complex
components of sediment, highly variable irradiance, light, air/sediment temperatures,
salinity, tidal level, sediment composition, wind velocity, and nutrient concentrations.
This difficulty may be overcome by integrating radar with optical data. In the
integration, radar data not affected by clouds can detect the shore heights of the
exposed tidal flat, from which the area of interest can be segmented into foreground
(water areas) and background (land areas) using the radar data (Zhang et al., 2021).7.5.5
All pooled water areas have a null value in the maximum value composite map
generated from the radar data. They are masked using the modified normalized
difference water index (MNDWI) calculated from the optical data and a series of
MNDWI thresholds based on the pooled water depth, together with clouds and other
non-water pixels such as salt marsh using the first red-edge band of optical data.
Level-2 algal pigment concentration products have been produced from OLCI
data by ESA using two algorithms: OC4ME and Neural Network Case2R. Each of
the derived concentrations is included in a specific file: chl_oc4me.nc and chl_nn.nc,
respectively. The input parameters considered are Oa3 (443 nm) to Oa6 (560 nm) in
OC4Me, and Oa1 (400 nm) – Oa12 (753.5 nm), Oa16 (778.75 nm), Oa17 (865 nm),
and Oa21 (1020 nm) in NN. OC4Me uses the ancillary and auxiliary data stored in
pre-established tables and Chl polynomial coefficients contained in the auxiliary data
files. NN contains only neural net coefficients in the LUT form. These variables are
used to derive the global data product at two spatial resolutions of 300 m×300 m (full
resolution) and 1.2 km×1.2 km. The detectable level of concentration ranges from
0.01–100 mg∙m-3 in Case II waters and 0.01–30 mg∙m-3 in Case I waters (ESA,
2024).
Colored Dissolved Organic Matter/Carbon
CDOM is a waterborne substance commonly found in highly polluted or eutrophic
shallow inland waters. Existing in all natural waters and having the highest
concentration in nearshore waters, CDOM stems from the breakdown products of
plants and other organic matter into humic materials. It plays a significant role in
aquatic photochemistry and photobiology. Its presence is indicative of severe water
quality degradation and water pollution. The quantification of CDOM from remotely
sensed data takes advantage of its absorption of blue and UV radiation in the
electromagnetic spectrum by various chromophores. CDOM absorption bears a
roughly exponentially or hyperbolically inverse relationship with wavelength in the
VIS and UV spectral ranges. CDOM tends to dominate the blue and UV spectrum in
many coastal and estuarine waters, and is the most important in-water constituent
affecting UV and blue light penetration even in the open ocean despite its generally
lower concentration. The absorption behavior of CDOM offers an effective clue for
estimating it remotely from OC reflectance.
Remote quantification of CDOM exploits its absorption coefficient ag(λ) that is
modeled as an exponentially decaying function of wavelength (λ), or:(7.62)
where λ0 = reference wavelength; Sg = spectral slope parameter. Sg at certain UV and
VIS wavelengths is very revealing about CDOM’s photoreactive state, chemical
composition, and molecular weight distribution, but its exact value varies with the
wavelength range used, spectral resolution, and reference wavelength.
There are no standard algorithms for global OC retrievals of CDOM. Many ad hoc
approaches have proven robust in retrieving CDOM absorption and its spectral slope
over the years, though most are regionally optimized with almost no provision for
benchmarking them (e.g., lack of proxies for optical water types). The development
of global algorithms has been hampered by relatively small datasets of coincident
radiometry with CDOM and CDOM slope extending into the UV. CDOM has been
retrieved from satellite data using only a few OC algorithms taking advantage of its
absorption peak near 443 nm. These algorithms assume covariance among Chl-a,
CDOM, and other water column constituents (i.e., the “Case I waters” assumption).
Others are semi-analytical and empirical approaches (Aurin et al., 2018). Semi￾analytical algorithms invert the OC signal to retrieve individual component
absorption spectra (e.g., particles, CDOM, water). They are stymied by the presence
of non-algal particulates that share a spectral shape similar to CDOM. So it is better
to use empirical models. A single exponential decay model based on the blue-green
band ratio is quite suited for the continental shelf waters for which it is originally
derived rather than for the deep ocean. Simple relationships of CDOM with remote
sensing reflectance over 440–555 nm may be empirically established via least-square
difference minimization regression. The most versatile and best performing algorithm
is the following four-waveband model:
(7.63)
where Y = biovariable (e.g., CDOM, dissolved organic carbon - DOC) to be
quantified; β0–4 = regression coefficients; λ1-4 = sensor-specific wavelengths, e.g.,
443, 488, 531, and 547 nm for MODIS bands, and 443, 490, 510, and 555 nm for
SeaWiFS bands. This model achieves the highest R2 of 0.9 at λ = 355 nm in
retrieving ag, and the lowest R² of 0.56 at λ = 275 nm (Aurin et al., 2018). CDOM
can be retrieved from MODIS Aqua data at an R2 generally over 0.80 in comparison
with field data, and within 16%–34% of field radiometry, depending on the
wavelength.
DOC refers to the portion of organic carbon that can pass through a filter with a
pore size typically between 0.22 and 0.7 microns. DOC alludes specifically to themass of carbon in the dissolved organic material. It differs from dissolved organic
matter (DOM) that refers to the total mass of the dissolved organic matter. So DOM
also includes the mass of other elements present in the organic material, such as
nitrogen, oxygen, and hydrogen. DOC retrievals have been especially challenging
due to the highly variable and often unpredictable fraction of chromophoric content,
and is feasible only globally at a very coarse spatial resolution measured by several
kilometers. Global DOC is best estimated from satellite data using two kinds of
empirical models (Figure 7.22). The first type is identical to Eq. 7.63. The second
type is to substitute Rrs(λ1) in Eq.7.63 with ag(355) and replace Rrs(λ2) with salinity,
and it has proved very robust (e.g., R² = 0.91, %Bias = 0, n = 464). If predicted from
CDOM and salinity, the R2 of DOC retrieval rises from 0.76 to 0.91, and MAPD
drops by about 3% (Aurin et al., 2018). In comparison, the MLR model achieves only
R² = 0.76 from MODIS data (n = 183) or R² = 0.68 from SeaWIFS data. Caution
needs to be exercised when applying this DOC algorithm to regions where DOC is
known to change without commensurate changes in CDOM and/or salinity.
Validation statistics are reasonably good for the ag(355) salinity model, but a larger
number and wider geographic distribution of validation stations than are currently
available is required to thoroughly evaluate the geographic and water-type limitations
of this model, particularly in the mid-ocean gyres. Overestimates of DOC (~41%) are
confined to the southern oceans (S of 40° S), but only for MODIS Aqua (i.e., not
Terra, and no SeaWiFS stations are identified). Elsewhere (i.e., north of 40° S), DOC
tends to be slightly underestimated by <10%.7.5.6
FIGURE 7.22 Global distribution of three-year averaged DOC retrieved from Aquarius
and MODIS Aqua data at 9 km nominal resolution from ag(555) and salinity. (Aurin et
al., 2018, open access.)
Phytoplankton Layer Depth
Phytoplankton are a crucial component of marine and freshwater ecosystems, and
their photosynthesis plays an important role in the marine ecosystem carbon sink.
The vertical distribution of subsurface phytoplankton in the ocean contains key
information about not only ocean ecology but also water column optical properties
relevant to remote sensing. Waters with an extremely high phytoplankton content
exhibit a higher reflectance at the red/NIR wavelengths than typical terrestrial
vegetation. Quantitative estimations of the vertical distribution of the subsurface
phytoplankton layer have to rely on airborne LiDAR as passive remote sensing
imagery offers no information on subsurface vertical structure. This is because
marine phytoplankton tend to form a Chl-a maximum layer in the subsurface due to
their strong vertical concentration maxima caused by nutrient gradient and light
(Ryan et al., 2010).
Airborne LiDAR quantification of subsurface phytoplankton takes advantage of
an inversion model based on the quasi-single-scattering LiDAR equation. The depth-dependent LiDAR signal due to seawater scattering P(λ,z) is the received signal
power at laser wavelength λ with water depth (z) and can be simplified as:
(7.64)
where α = LiDAR system constant. It is an integrative function value by multiple
parameters, such as transmitted efficiency, detector efficiency, transmittance
efficiency through the atmosphere and ocean surface, among others; H = distance
between the LiDAR sensor and the sea surface; n = refractive index of seawater; βπ =
volume scattering coefficient at a scattering angle of π radians; klidar = effective
LiDAR attenuation coefficient. The inversion of Eq. 7.64 infers two quantities, βπ
and klidar, which may be fulfilled using a hybrid retrieval method (e.g., the Klett
method for estimating klidar combined with the perturbation method for estimating βk)
(Chen et al., 2021), or:
(7.65)
where S(z) = range-corrected LiDAR return in logarithmic form; r = an approximate
value related to βπ and klidar and is often set to 1; and m = the chosen reference
boundary depth, which equals the theoretical maximum detectable depth, defined as
the depth where the signal strength merely exceeds the background signal in
magnitude. In reality, the reference boundary depth is often limited to four-fifths of
the theoretical maximum detectable depth where the SNR is moderate so as to retain
a viable accuracy of inversion; Sm = range-corrected LiDAR return at depth m.
In the perturbation retrieval method, βπ is treated as comprising two parts: a
homogeneous part invariable with depth (S0(z)), and a heterogeneous part varying
with water stratification, or:
(7.66)
where S0(z) = the homogeneous part of the range-corrected LiDAR return that can be
determined via linear regression as:
(7.67)
where Klidar(0) and βπ(0) = linear fitting parameters, or the non-varying parts of klidar
and βπ, respectively. Hence, Klidar closely approximates Kd with dual-polarized
LiDAR data of a large FOV, or dual-wavelength LiDAR of a high flight altitude and
a relatively large FOV. Klidar closely resembles the beam attenuation coefficient (c)when the dual-polarized LiDAR is set to the small FOV mode. Klidar bears the
following empirical relationship with Chl-a concentration (C) based on the bio￾optical models, from which C is estimated (Chen et al., 2021):
(7.68)
(7.69
where a = absorption coefficient (first term in Eq. 7.69), and b = scattering coefficient
(second term in Eq. 7.69). Kw(λ) = pure water diffuse attenuation coefficient
available from the literature; Kbio(λ) = particle diffuse attenuation coefficient
calculated as a function of Chl-a concentration; χ(λ) and e(λ) = scaling factors whose
values are available in the literature. The LiDAR-retrieved Chl-a concentrations are
in close agreement (R2 = 0.68, 0.87, and 0.88) with in situ observations at three
stations. The corresponding RMSEs are 0.16, 0.038, and 0.017 μg ∙ L-1, with the
corresponding MAPEs being 31.3%, 4.2%, and 5.1%, respectively (Chen et al.,
2021). The LiDAR-retrieved Chl-a concentration has a MAPE < 15%. Clearly, the
LiDAR-generated estimates are accurate in waters of a low Chl-a concentration and a
homogeneous composition, but the accuracy drops in areas of a high Chl-a
concentration above 0.6 μg ∙ L-1 with stratified water (Figure 7.23).FIGURE 7.23 Vertical distribution of Chl-a concentration in subsurface phytoplankton
level in the coastal water of Sanya Bay, South China detected using dual-polarized
LiDAR. horizontal axis = flight distance, vertical axis – water depth; white patches:7.5.7
areas where inversion failed, due possibly to strong LiDAR signal reflection
interference from the water surface. (a) Mar. 11, 2018; (c) Sept. 30, 2017. (Chen et al.,
2021, with permission (5761261074698) from Elsevier.)
Phycocyanin (Cyanobacterial) Blooms
Nuisance cyanobacterial blooms degrade water quality through accelerated
eutrophication, odor generation, and production of toxins, all having adverse effects
on human health. The frequency, extent, and magnitude of cyanobacterial harmful
algal blooms have risen globally over the past few decades. In eutrophied waters,
abundant nutrient supplies fuel phytoplankton growth (e.g, blue-green algae) and
cause the algae to rapidly proliferate and congregate on the water surface, forming
cyanobacterial blooms. They present a range of amenity, water quality, ant treatment
issues, and are hazardous to humans. Phycocyanin (PC in cells∙ml-1) is a proxy
pigment indicator of the presence of cyanobacteria. PC is highly specific to
cyanobacteria and some cryptophytes as a minor pigment. Quantitative estimation of
the dynamic variation of PC concentration in lakes is tremendously significant for the
effective prevention and control of cyanobacterial blooms, and is ideally fulfilled
using remote sensing. It is a quick and effective method of detecting cyanobacterial
abundance in drinking water supplies that complements costly and time-consuming
laboratory analyses. As a diagnostic pigment for cyanobacteria detection, PC has a
diagnostic and strong absorption around 620–630 nm in freshwater. This spectral
absorption is detectable from optical remote sensing data. This narrow absorption
feature of PC can be discriminated from Chl-a, which has an absorption feature
independent of PC in the 676 nm waveband available in OC satellite images such as
Sentinel-3 OLCI. Of all OLCI bands, band Oa7 (620 nm) is the absorption peak band
of PC. Its algebra combinations with other bands (Eq. 7.70) are correlated highly with
PC concentrations (R > 0.6) and are regarded as useful independent variables in the
PC estimation model (Qi et al, 2014). In particular, the ratio of ρmax (640–650)/ρmin
(610–630) has been used as the predictor of PC concentration as (Kutser et al., 2005):
(7.70)
where Cpc = PC concentration in μg∙L-1; b6, b7, and b8 = reflectance of bands Oa6,
Oa7, and Oa8 of OLCI images, respectively; and k and b = parameters determined
via model fitting. PC index is not markedly better than simple band combinations
such as differencing and ratioing (Table 7.13).Table 7.13
Comparison of accuracy of different models and analytical algorithms in retrieving PC concentration from
OLCI images
Models Input variables R²
RMSE (μg
∙ L–1)
MAPE
(%)
Band
combination
Oa7 – Oa12 0.50 88.24 45.34
Oa7/(Oa6 + Oa12) 0.55 68.01 33.76
(1/Oa8-1/Oa10)/Oa7 0.45 70.29 35.05
PC index Oa6 + ( 0.52 79.70 41.76
RF Six indices, including Oa6 ~
Oa12, Oa7 – Oa12,
Oa7/(Oa6 + Oa12),
(1/Oa7 + 1/Oa10) × Oa11,
(Oa7 – Oa10)/(Oa7 + Oa10),
and
(1/Oa8 + 1/Oa10) × Oa7
0.76 57.15 28.34
GBT 0.86 45.44 26.27
SVR 0.54 76.93 40.27
KNN 0.53 76.80 38.27
DNN 0.60 67.15 32.16
Source: Wang et al. (2022b), open access.
Note: Boldface: best model.
Apart from empirical statistical models based on the PC index and the semi-analytical
models, PC concentrations can also be estimated using machine learning (ML)
models, including RF, SVM, GBT, KNN, and DNN (Table 7.13). ML models depict
the non-linear relationship between an input layer and an output layer through
repeated training of large samples to better capture the characteristics of the sampled
PC concentration data and mine its relationship with all considered predictor
variables. Of the five machine learning methods, the GBT algorithm is the best at
estimating PC from Sentinel-3 OLCI images, achieving the highest R2 of 0.86, and
the lowest RMSE of 45.44 μg∙L-1 and MAPE of 26.27% (Table 7.13). It is much
more accurate than KNN, DNN, and SVM. They manage to attain an accuracy
comparable to that of simple indices from two or three bands, even though the
predictions are made from many more (six in total) input variables. Since the GBT
model is the most accurate, it is applied to the satellite image dataset to show the
spatial variation of PC in an inland, murky lake in four seasons (Figure 7.24).
Validated against the ground truth, GBT slightly underestimates high PC
concentrations above 300 μg∙L-1 and slightly overestimates low PC concentrations
below 50 μg∙L-1. However, the measured and estimated PC concentrations show goodspatial consistency with the variation at sample sites, with a relatively small error
between them (MAPE = 26.27%). These results suggest that PC concentrations in
lakes and reservoirs can be dynamically monitored at a high frequency from Sentinel￾3 OLCI images.
FIGURE 7.24 Spatial distribution of PC concentration in Lake Chaohu, East China in
four seasons quantified from Sentinel-3 OLCI images using the GBR model shown in
Table 7.13. (Wang et al., 2022b, open access.)
REFERENCES
Abbot RH, DW Lane, MJ Sinclair, and TA Spruing (1996) Lasers chart the waters of
Australia’s Great Barrier Reef. Proc SPIE 2964: 72–90.
Ahn YH and P Shanmugam (2006) Detecting red tides from satellite ocean color
observations in optically complex Northeast-Asia coastal waters. Rem Sens
Environ 103: 419–437. doi: 10.1016/j.rse.2006.04.007
Ali KA and JD Ortiz (2016) Multivariate approach for chlorophyll-A and suspended
matter retrievals in Case II type waters using hyperspectral data. Hydrol Sci J
61(1): 200–213. doi: 10.1080/02626667.2014.964242Aurin D, A Mannino, and DJ Lary (2018) Remote sensing of CDOM, CDOM
spectral slope, and dissolved organic carbon in the global ocean. Appl Sci 8(12):
2687. doi: 10.3390/app8122687
Bi S, Y Li, H Lyu, M Mu, J Xu, S Lei, S Miao, T Hong, and L Zhou (2019)
Quantifying spatiotemporal dynamics of the column-integrated algal biomass in
nonbloom conditions based on OLCI Data: A case study of Lake Dianchi, China.
IEEE Trans Geosci Rem Sens 57(10): 7447–7459. doi:
10.1109/TGRS.2019.2913401
Binding CE, TA Greenberg, and RP Bukata (2013) The MERIS Maximum
Chlorophyll Index; its merits and limitations for inland water algal bloom
monitoring. J Great Lakes Res 39(S1): 100–107. doi: 10.1016/j.jglr.2013.04.005
Casal G, X Monteys, J Hedley, P Harris, C Cahalane, and T McCarthy (2019)
Assessment of empirical algorithms for bathymetry extraction using Sentinel-2
data. Int J Rem Sens 40: 2855–2879.
Chan SN, YW Fan, and XH Yao (2022) Mapping of coastal surface chlorophyll-A
concentration by multispectral reflectance measurement from unmanned aerial
vehicles. J Hydro-Environ Res 44: 88–101. doi: 10.1016/j.jher.2022.08.003
Chen P, C Jamet, Z Zhang, Y He, Z Mao, D Pan, T Wang, D Liu, and D Yuan (2021)
Vertical distribution of subsurface phytoplankton layer in South China Sea using
airborne LiDAR. Rem Sens Environ 263: 112567. doi: 10.1016/j.rse.2021.112567
Chen S, W Huang, H Wang, and D Li (2009) Remote sensing assessment of sediment
re-suspension during Hurricane Frances in Apalachicola Bay, USA. Rem Sens
Environ 113(12): 2670–2681. doi: 10.1016/j.rse.2009.08.005
Chen Z, B Zhang, V Kudryavtsev, Y He, and X Chu (2019) Estimation of sea surface
current from X-band marine radar images by cross-spectrum analysis. Rem Sens
11(9): 1031. doi: 10.3390/rs11091031
Cheng KH, SN Chan, and JHW Lee (2020) Remote sensing of coastal algal blooms
using unmanned aerial vehicles (UAVs). Mar Pollut Bull 152: 110889.
Dall’Olmo G and AA Gitelson (2005) Effect of bio-optical parameter variability on
the remote estimation of chlorophyll-A concentration in turbid productive waters:
Experimental results. Appl Opt 44: 412–422.
Dinnat EP, DM Le Vine, J Boutin, T Meissner, and G Lagerloef (2019) Remote
sensing of sea surface salinity: Comparison of satellite and in situ observations and
impact of retrieval parameters. Rem Sens 11(7): 750. doi: 10.3390/rs11070750
Ditri AL, PJ Minnett, Y Liu, K Kilpatrick, and A Kumar (2018) The accuracies of
Himawari-8 and MTSAT-2 sea-surface temperatures in the tropical western PacificOcean. Rem Sens 10(2): 212. doi: 10.3390/rs10020212
ESA (Europe Space Agency) (2024) Sentinel Online - Algal Pigment Concentration.
https://sentinels.copernicus.eu/web/sentinel/technical-guides/sentinel-3-olci/level￾2/algal-pigment-concentration)
Gafoor AF, MR Al-Shehhi, C-S Cho, and H Ghedira (2022) Gradient boosting and
linear regression for estimating coastal bathymetry based on Sentinel-2 images.
Rem Sens 14(19): 5037. doi: 10.3390/rs14195037
Gitelson AA, G Dall’Olmo, W Moses, DC Rundquist, T Barrow, TR Fisher, D
Gurlin, and J Holz (2008) A simple semi-analytical model for remote estimation of
chlorophyll-A in turbid waters: Validation. Rem Sens Environ 112(9): 3582–3593.
Gons HJ, MT Auer, and SW Effler (2008) MERIS satellite chlorophyll mapping of
oligotrophic and eutrophic waters in the Laurentian Great Lakes. Rem Sens
Environ 112(11): 4098–4106. doi: 10.1016/j.rse.2007.06.029
Harvey ET, S Kratzer, and P Philipson (2015) Satellite-based water quality
monitoring for improved spatial and temporal retrieval of chlorophyll-A in coastal
waters. Rem Sens Environ 158: 417–430. doi: 10.1016/j.rse.2014.11.017
Jagalingam P, BJ Akshaya, and AV Hegde (2015) Bathymetry mapping using Landsat
8 satellite imagery. 8th International Conference on Asian and Pacific Coasts
(APAC 2015). Proc Eng 116: 560–566. doi: 10.1016/j.proeng.2015.08.326
Ji W, D Civco, and WC Kennard (1992) Satellite remote bathymetry: A new
mechanism for modeling. Photogram Eng Rem Sens 58(5): 545–549.
Kabiri K (2017) Accuracy assessment of near-shore bathymetry information retrieved
from Landsat-8 imagery. Earth Sci Info 10: 235–245. doi: 10.10.1007/s12145-017-
0293-7
Kao HY, GSE Lagerloef, T Lee, O Melnichenko, T Meissner, and P Hacker (2018)
Assessment of Aquarius sea surface salinity. Rem Sens 10(9): 1341. doi:
10.3390/rs10091341
Kasvi E, J Salmela, E Lotsari, T Kumpula, and SN Lane (2019) Comparison of
remote sensing based approaches for mapping bathymetry of shallow, clear water
rivers. Geomor 333: 180–197.
Kutser T, DC Pierson, L Tranvik, A Reinart, S Sobek, and K Kallio (2005) Using
satellite remote sensing to estimate the colored dissolved organic matter absorption
coefficient in lakes. Ecosystems 8: 709–720. doi: 10.1007/s10021-003-0148-6
Legleiter CJ and LR Harrison (2018) Remote sensing of river bathymetry: Evaluating
a range of sensors, platforms, and algorithms on the upper Sacramento River,
California, USA. Water Res Res 55(3): 2142–2169. doi: 10.1029/2018WR023586Legleiter CJ and PJ Kinzel III (2021) Surface flow velocities from space: Particle
image velocimetry of satellite video of a large, sediment-laden river. Front Water
3. doi: 10.3389/frwa.2021.652213
Legleiter CJ, DA Roberts, and RL Lawrence (2009) Spectrally based remote sensing
of river bathymetry. Earth Surf Process Landf 34(8): 1039–1059.
Lin S, J Qi, J Jones, and R Stevenson (2018) Effects of sediments and coloured
dissolved organic matter on remote sensing of chlorophyll- A using Landsat
TM/ETM+ over turbid waters. Int J Rem Sens 39: 1421–1440. doi:
10.1080/01431161.2017.1404164
Liu Y, C Xiao, J Li, F Zhang, and S Wang (2020) Secchi disk depth estimation from
China’s new generation of GF-5 hyperspectral observations using a semi-analytical
scheme. Rem Sens 12(11): 1849.
Lyu L, K Song, Z Wen, G Liu, Y Shang, S Li, H Tao, X Wang, and J Hou (2022)
Estimation of the lake trophic state index (TSI) using hyperspectral remote sensing
in Northeast China. Opt Express 30(7): 10329. doi: 10.1364/OE.453404
Martínez-Flores G, O Zaitzev, and EH Nava-Sánchez (2019) Sea surface current
velocity vectors from passive remote sensing imagery. IGARSS 2019 - 2019 IEEE
Int Geosci Rem Sens Sympo, Yokohama, Japan, 8260–8263. doi:
10.1109/IGARSS.2019.8897901
Meissner T, FJ Wentz, and DM Le Vine (2018) The salinity retrieval algorithms for
the NASA aquarius version 5 and SMAP version 3 releases. Rem Sens 10(7): 1121.
doi: 10.3390/rs10071121
Minnett PJ, A Alvera-Azcárate, TM Chin, GK Corlett, CL Gentemann, I Karagali, X
Li, A Marsouin, S Marullo, E Maturi, R Santoleri, S Saux Picart, M Steele, and J
Vazquez-Cuervo (2019) Half a century of satellite remote sensing of sea-surface
temperature. Rem Sens Environ 233: 111366. doi: 10.1016/j.rse.2019.111366
Minnett PJ and Kaiser-Weiss AK (2012) Group for High Resolution Sea-Surface
Temperature Discussion Document: Near-Surface Oceanic Temperature Gradients
(7 pp.) www.ghrsst.org/wp￾content/uploads/2016/10/SSTDefinitionsDiscussion.pdf
Mishra S and DR Mishra (2012) Normalized difference chlorophyll index: A novel
model for remote estimation of chlorophyll-A concentration in turbid productive
waters. Rem Sens Environ 117: 394–406. doi: 10.1016/j.rse.2011.10.016
Mobley CD (1994) Light and Water: Radiative Transfer in Natural Waters. New
York: Elsevier, 592 p.Nielsen-Englyst P, JL Høyer, LT Pedersen, CL Gentemann, E Alerskans, T Block, and
C Donlon (2018) Optimal estimation of sea surface temperature from AMSR-E.
Rem Sens 10(2): 229. doi: 10.3390/rs10020229
Novo E, L Londe, C Barbosa, C Araújo, and C Rennó (2013) Proposal for a remote
sensing trophic state index based upon thematic mapper/Landsat images. Ambiente
& Água - An Interdis J Appl Sci 8(3): 65–82. doi: 10.4136/ambi-agua.1229
O’Reilly JE and WP Jeremy (2019) Chlorophyll algorithms for ocean color sensors -
OC4, OC5 & OC6. Rem Sens Environ 229: 32–47. doi: 10.1016/j.rse.2019.04.021
Peterson KT, V Sagan, P Sidike, AL Cox, and M Martinez (2018) Suspended
sediment concentration estimation from Landsat imagery along the lower Missouri
and middle Mississippi Rivers using an extreme learning machine. Rem Sens
10(10): 1503. doi: 10.3390/rs10101503
Pyo JC, SM Hong, JP Jang, P Sanghun, N Jongkwan, Noh, H Jae, and KH Cho
(2022) Drone-borne sensing of major and accessory pigments in algae using deep
learning modeling. GISci Rem Sens 59(1): 310–332. doi:
10.1080/15481603.2022.2027120
Qi L, C Hu, H Duan, J Cannizzaro, and R Ma (2014) A novel MERIS algorithm to
derive cyanobacterial phycocyanin pigment concentrations in a eutrophic lake:
Theoretical basis and practical considerations. Rem Sens Environ 154: 298–317.
doi: 10.1016/j.rse.2014.08.026
Qin L and Y Li (2021) Significant wave height estimation using multi-satellite
observations from GNSS-R. Rem Sens 13(23): 4806. doi: 10.3390/rs13234806
Randolph K, J Wilson, L Tedesco, L Li, DL Pascual, and E Soyeux (2008)
Hyperspectral remote sensing of cyanobacteria in turbid productive water using
optically active pigments, chlorophyll A and phycocyanin. Rem Sens Environ
112(11): 4009–4019.
Ryan JP, MA McManus, and JM Sullivan (2010) Interacting physical, chemical and
biological forcing of phytoplankton thin-layer variability in Monterey Bay,
California. Cont Shelf Res 30: 7–16.
Sarkar SS and VD Patel (2021) OCM-3 and SSTM-1 payloads on Oceansat-3 (EOS￾06) mission. IEEE Int. India Geosci Rem Sens Sympos (InGARSS), Ahmedabad,
India, 23–26. doi: 10.1109/InGARSS51564.2021.9792032
Shanmugam P (2011) A new bio-optical algorithm for the remote sensing of algal
blooms in complex ocean waters. J Geophys Res C Oceans 116(4). doi:
10.1029/2010JC006796Shao J, X Li, and J Sun (2015) Ocean wave parameters retrieval from TerraSAR-X
images validated against buoy measurements and model results. Rem Sens 7:
12815–12828. doi: 10.3390/rs71012815
Song K, G Liu, Q Wang, Z Wen, L Lyu, Y Du, L Sha, and C Fang (2020)
Quantification of lake clarity in China using Landsat OLI imagery data. Rem Sens
Environ 243, 111800. doi: 10.1016/j.rse.2020.111800
Song K, L Li, LP Tedesco, S Li, NA Clercin, BW Hall, Z Li, and K Shi (2012)
Hyperspectral determination of eutrophication for a water supply source via
genetic algorithm-partial least squares (GA-PLS) modeling. Sci Total Environ 426:
220–232. doi: 10.1016/j.scitotenv.2012.03.058
Stumpf RP, K Holderied, and M Sinclair (2003) Determination of water depth with
high-resolution satellite imagery over variable bottom types. Limno Oceano 48:
547–556. doi: 10.4319/lo.2003.48.1_part_2.0547
Sun D, Y Li, and Q Wang (2009) A unified model for remotely estimating chlorophyll
a in Lake Taihu, China, based on SVM and in situ hyperspectral data. IEEE Trans
Geosci Rem Sens 47(8): 2957–2965.
Szafarczyk A and C Toś (2023) The Use of green laser in LiDAR bathymetry: State
of the art and recent advancements. Sensors 23(1): 292. doi: 10.3390/s23010292
Tan J, KA Cherkauer, and I Chaubey (2016) Developing a comprehensive spectral￾biogeochemical database of midwestern rivers for water quality retrieval using
remote sensing data: A case study of the Wabash River and its tributary, Indiana.
Rem Sens 8: 517. doi: 10.3390/rs8060517
Thiemann S and H Kaufmann (2000) Determination of chlorophyll content and
trophic state of lakes using field spectrometer and IRS-1C satellite data in the
Mecklenburg lake district, Germany. Rem Sens Environ 73(2): 227–235.
Vilas LG, E Spyrakos, and JMT Palenzuela (2011) Neural network estimation of
chlorophyll a from MERIS full resolution data for the coastal waters of Galician
rias (NW Spain). Rem Sens Environ 115(2): 524–535. doi:
10.1016/j.rse.2010.09.021
Wang C, K Yu, K Zhang, J Bu, and F Qu (2023) Significant wave height retrieval
based on multivariable regression models developed with CYGNSS data. IEEE
Trans Geosci Rem Sens 61: 1–15, Art no. 4200415. doi:
10.1109/TGRS.2022.3233102
Wang F, D Yang, and L Yang (2022a) Retrieval and assessment of significant wave
height from CYGNSS mission using neural network. Rem Sens 14(15): 3666. doi:
10.3390/rs141536668 Atmospheric Quantification
DOI: 10.1201/9781003517504-10
As the world’s climate is experiencing unabated warming over the recent decades, increasing efforts
have been made to study the atmosphere and its constituents. The atmosphere is unique not only in
terms of its sheer size and ubiquitous existence, but also having characteristically low concentrations
of atmospheric constituents and their tendency to constant change under the varying effect of the
diurnal solar radiation. Of the four spheres of sensing covered in this book, the atmosphere is the
largest and tends to change most likely, encircling the entire surface of the Earth, be it terrestrial,
aquatic, or biological. It forms an integral coupled system with the other three, in which physical,
chemical, and biological processes take place. They, in turn, affect the other three spheres. Vertically,
the atmosphere is also the thickest, extending from the terrestrial/biosphere/hydro-sphere all the way
to the stratosphere. Although the bottom layer affects human lives the most, the middle layer of the
atmosphere interacts closely with the underlying and overlying atmospheric layers through the
constant exchange of momentum, energy, and physical and chemical constituents. It is also the zone
where radiative, photochemical, and dynamical processes are intricately associated with each other.
Lying above the atmosphere is the troposphere that is rather distant from the biosphere where
humans dwell. The troposphere (and to a much lesser extent, the stratosphere) is occasionally
covered in this chapter on the ground that the most important biochemical cycles essential to life
take place in these layers. The processes and gases in the troposphere such as ozone depletion can
form acid rain and affect humans just like those happening in the atmosphere.
Of the four spherical zones, the atmosphere is the most challenging to quantify because of the
low concentration of the targets of quantification, be they gaseous substances or solid particulates.
Furthermore, all of the air quality targets of quantification are suspended in the atmosphere in a state
of constant motion and mixing during atmospheric circulation and due to climatic forcing, causing
the quantified results almost impossible to validate. Neither is it possible to carry out the
quantification at a fine spatial resolution for two reasons: (i) The concentration level of the target
constituent is so low that its spectral signature hardly registers on fine-resolution satellite images;
and (ii) Fine-resolution images tend to have a long revisit period. Such a coarse temporal resolution
can hardly capture certain ephemeral phenomena such as volcanic-ejected pollutants, let alone
quantify them repeatedly. Thus, the quantification is invariably implemented from coarse-resolution
images on a broad scale. The global-level results may be downscaled to the regional or local scale
with the assistance of additional data and data processing, such as in situ observations via machine
learning.8.1
Dissimilar to the sensing of solid targets that is based on the radiative energy reflected and
emitted by the target, the estimation of gaseous components has to rely on the directly measured
absorption spectra of solar radiation in the UV and IR spectral regions. UV sensing is not subject to
the influence of water vapor in the atmosphere. Although IR sensors have been used, they are much
less common than UV sensors. Owing to the difficulty of collecting in situ samples in the
atmosphere, empirical models whose authenticity needs to be verified by field data are used far less
frequently than physical and semi-physical models. Atmospheric constituents and surface reflectance
are retrieved via modeling the radiative transfer in the atmosphere. In most cases, no information on
the bidirectional reflectance behavior of surfaces is available, so a simple isotropic (Lambert)
reflectance surface is assumed. These assumptions are needed to simplify the models. Otherwise, the
complexity of model inversion will thwart the retrieval process altogether.
The unique nature of the atmosphere requires special satellite data to detect and sophisticated
analytical models and methods to retrieve atmospheric constituents, especially those co-existing
trace gas concentrations. Those satellite data that have been tailored specifically to study the
atmosphere and publicly accessible data products derived from them are first introduced and
described in this chapter. The quantification of atmospheric clarity and its measurement by AOT
forms the content of the next section, followed by the quantification of solid particulate matter in the
atmosphere. The last section of this chapter elucidates how to retrieve gaseous components from
widely ranging satellite data. All these sections are preceded by the principle of quantification.
PRINCIPLE OF QUANTIFICATION
The fundamental principle of retrieving atmospheric gaseous components is Lamber-Beer law that
spells out how light absorption is related to the concentration of chemical species. Closely related to
this law is empirical Bouguer-Lambert law that relates the extinction or attenuation of light to the
medium through which light is propagating, namely, the relationship between absorption medium
and intensity of monochromatic absorption. In a homogeneous medium, the attenuation of light
intensity at wavelength λ [I(λ)] is a function of the gaseous concentration and the path of
propagation or optical length (L), or:
(8.1)
where I0(λ) = ambient at-sensor radiance recorded in a spectral band (known); (λ) = absorption
cross-section (known); m = mean concentration (or mass) of the absorbing substance to be retrieved;
L = slant distance from the sensor to the target (known). The typical length of the light path in the
atmosphere ranges from 100s to several hundreds of kilometers, depending on the altitude of the
sensor. In case of a sole component, its concentration m is calculated from Eq. 8.1 as:
(8.2)
where = the difference between the absorption cross-section calculated from the absorptance
measured at the surface and its at-sensor counterpart; = transmissivity of the atmosphere (t), known
through in situ measured (or simulated) spectra. When multiple non-interfering absorbers co-exist in
the same medium, the above equation is modified to:(8.3)
where mi = concentration of the ith trace gas species; = absorption cross-section spectra of the ith
trace gases in the atmosphere captured by specially designed sensors. In addition to absorption by
trace gases in the atmosphere, the intensity of the sun light I0(λ) is also extincted by tiny air
molecules [sRay(λ)] and aerosols [sMie(λ)], and turbulence [T(λ)]. The derivation of mi requires the
isolation of these scattering/absorption effects at different wavelengths. Thus, the consideration of
their effects on I(λ) changes Eq 8.1 to:
(8.4)
The above equation is resolved using the differential optical absorption spectroscopy (DOAS) of
UV-VIS lights (ESA, 2024). The absorption spectra are measured in specific narrow bands of the
UV and visible wavelengths instead of the intensity of monochromatic light, based on which the
absorption spectral signature of the target trace gas species is isolated from that of other components,
taking into account the extinction caused by molecule and aerosol scattering.
The core of DOAS is to non-linearly fit the measured spectra by the absorption spectra of the
trace gases in the respective spectral range iteratively. After mi is represented as the slant column
density (SCDi), Eq. 8.2 is rewritten as:
(8.5)
where I(λ) = measured earthshine backscattered radiance spectrum; = extraterrestrial solar
irradiance spectrum; θ0 = solar zenith angle; = differential absorption cross-section of species i (e.g.,
the high-pass filtered parts of the absorption cross-sections); cp = coefficients of the polynomial
function (ESA, 2024). SCDj and cp are the fit parameters to be determined using a non-linear, least￾squares fitting algorithm. This algorithm iteratively combines the linear part (first term) for
retrieving the trace gas absorption with a non-linear part (second term) represented by a low-order
polynomial function. This function accounts for possible spectral shifts between the measured
spectrum and the reference spectrum, and molecule and aerosol scattering. They are also known as
the intensity fit (non-linear) and the optical density fit (linear).
The fitting procedure is usually optimized by considering additional structured spectral effects,
such as the Ring effect induced by inelastic Raman scattering by N2 and O2 molecules. Compared to
elastic scattering, inelastic scattering alters the wavelengths of scattered photons. It causes the
filling-in effect of solar Fraunhofer lines, the most dominant features in the recorded spectra. It is a
function of wavelength, and is corrected by adding one further spectrum to Eq. 8.5 in the fitting
process to form Eq. 8.6 to account for the solar Fraunhofer lines. The fit parameters are expanded to
include SCD and :
(8.6)
DOAS has been implemented in several ways in which some parameters are approximated
differently. Two popular versions are weighting function modified (WFM)-DOAS that is originally
developed for Scanning Imaging Absorption Spectrometer for Atmospheric Chartography
(SCIAMACHY) data, conceptually lying between the statistical matched filter (MF) and the8.2
optimal-estimation-based iterative maximum a posteriori DOAS (IMAP-DOAS) retrieval algorithm
that is computationally very expensive. They will be introduced in relevant sections later when they
are applied for retrieving a specific trace gas content. Irrespective of the specific versions, the
general procedure of retrieval remains unchanged in four steps: (i) determination of SCD via DOAS
fitting to derive the total slant column of the targeted trace gas in a pre-defined spectral window, for
example, ozone and NO2; (ii) determination of the cloud cover fraction to account for the trace gas
column below the cloud; (iii) calculation of air mass factors (AMFs) to convert SCD to vertical
column density (VCD) by dividing it by the air mass factor [AMF = ratio of slant optical density
τi,s(λ) to vertical optical density τi,z(λ)] in clear sky conditions or SCDi/AMFi; and (iv) calculation of
the vertical column density using the results from the preceding steps (Burrows et al., 1999). The
only differences for different gas species are the slight variations in the averaging window size
caused by the differential absorption behavior of a unique gas species.
This principle of detection has been implemented in DOAS instruments. A typical DOAS
instrument contains an optical device to receive the continuous or scattered sunlight through the
atmosphere. An excellent system for trace gases retrieval is the multi-axis differential optical
absorption spectroscopy (MAX-DOAS). This device consists of various components, including a
spectrometer of two sensitive ranges (UV: 300–405 nm, and VIS: 407–540 nm) at a fine spectral
resolution of 0.6 nm, a 2D charge-coupled device detector with 2,048 individual pixels, and a
telescope unit with stepping motor and multi-mode quartz glass fiber. If deployed on the ground,
MAX-DOAS can measure scattered sunlight at various altitudes in the atmosphere, ideal for
accurately measuring column concentration of trace gas species. Differential slant column densities
(DSCDs) are obtained by setting 90° as the Fraunhofer reference spectrum, which is used to fit the
recorded spectra at various elevation angles for each scanning series. The measured spectra represent
the sum of sunlight intensity and electronic offset by the spectrometer that must be corrected by
subtracting it. Since atmospheric scattering impacts the quality of the spectra recorded, the measured
spectra may be screened with those datasets with a RMSE > 0.002 and a solar zenith angle (SZA) >
75 eliminated because of their strong alteration by scattering (Javed et al., 2021). DOAS has proven
useful in quantifying many trace gases in the atmosphere, including hydroxide, NO3, HONO, and
BrO.
USEFUL SATELLITE DATA
Quantification of atmospheric quality requires purposely tailored sensors that tend to have a low
spatial resolution but an extremely fine temporal resolution measured by hours instead of days. They
are designed specifically for studying atmospheric composition by different agencies around the
world. Atmospheric features require sophisticated algorithms to quantify. The aforementioned
uniqueness of the atmospheric properties dictates that they cannot be quantified using the satellite
data introduced in Chapter 2 due to their coarse temporal resolution. In fact, the quantification can
be successful only with atmospheric satellite data that must have a very coarse spatial resolution and
unique spectral band designation in wavelength at which the target of sensing has characteristic
absorption or reflection troughs. Several specific sensors have been developed primarily for this
purpose. Six of them merit detailed elaboration in this section: TOMS, OMI, Global Ozone
Monitoring Experiment (GOME), SCIAMACHY, GOSAT, and TROPOMI, plus Earth8.2.1
8.2.2
Polychromatic Imaging Camera (EPIC). They differ from other ocean and land satellites in their
spatial, temporal, and radiometric resolutions, as well as the designation of spectral bands that even
fall outside the atmospheric windows.
Total Ozone Mapping Spectrometer
As its name suggests, the Total Ozone Mapping Spectrometer (TOMS) is a sensor designed to study
the ozone layer. It maintains the longest operation record dating back to October 1978 when the first
TOMS was launched into orbit by the NASA’s Nimbus-7 satellite. This successful mission was
operational for 14 years until May 1993. Since then it has been superseded by successive missions.
The second mission lasted from October 1991 until December 1994. The third mission was launched
18 months later. The latest Earth Probe (EP) mission started collecting data from July 1996. EP has a
sun-synchronous orbit with an altitude of 500 km and an equatorial crossing time of ⁓11:16 a.m.
local time. It provides supplemental measurements at a boosted higher orbit of 740 km. The TOMS
instrument measures incident solar radiation and backscattered UV radiation in six discrete 1-nm
wavebands centered around 313, 318, 331, 340, 360, and 380 nm for the first three TOMS
instruments (Stolarski and McPeters, 2003). In the near UV region of the spectrum, the solar
radiation is partially absorbed only by ozone. This second-generation backscattering sounder
provides daily daytime measurements of “total column ozone” from the Earth’s surface to the TOA.
Measurements are made every eight seconds, each covering a width of 50–200 km on the ground,
strung in a direction perpendicular to the satellite motion.
The TOMS mission ceased operating on December 2, 2006 when the backup transmitter
malfunctioned. In spite of its decommission, the continuity of NASA ozone observations is
maintained by the Ozone Monitoring Instrument (OMI) mission for studying total ozone and other
atmospheric parameters related to ozone chemistry and climate.
Ozone Monitoring Instruments
OMI is a payload of the Aura (EOS/Chem-1) spacecraft launched by the NASA in 2004. This
satellite revolves the Earth at an altitude of 705 km in a sun-synchronous polar orbit, completing 16
revolutions per day. OMI has a daily global coverage in 14 orbits. It crosses the equator at a local
time of 1:45 p.m. on the ascending node. The orbital inclination of 98.1° enables the latitudinal
range of 82°N–82°S to be sensed. This UV-VIS nadir-viewing imaging spectrometer has an across￾track FOV of 114°. It makes use of a hyperspectral push broom sensor to measure solar irradiance
and the atmosphere-backscattered radiance at the TOA over the UV-VIS spectral range of 264–504
nm in 740 bands. There are two useful UV ranges of sensing, UV-1 (270–314 nm) and UV-2 (306–
380 nm). The spectral resolution of OMI ranges from 0.42 to 0.63 nm (Table 8.1). The satellite is
equipped with charge-coupled device that acquires images comprising 780 × 576 pixels at an IFOV
of 3 km. It decreases to 26 km × 135 km at the swath edges, allowing for almost daily global
coverage. OMI can operate in three modes: global observation, spectral and spatial zoom-in. In the
spatial zoom-in mode, nadir spatial resolution is reduced to 13 km × 12 km while swath width is
quadruply halved from 2600 to 725 km to detect and track pollution sources in urban areas (Levelt et
al., 2006). OMI has a supreme in-flight performance, with only ~0.5% radiometric degradation in
the visible channel during its 15 years of operation.8.2.3
Table 8.1
The spectral properties of OMI data and data products
Channel
Sensitivity
range (nm)
Spectral
resolution
Mean
spectral
sampling
interval Data products
UV-1 270-310 0.42 nm
0.32 nm ×
pixel-1
O3 profile, O3 column (TOMS), surface
UV-B
UV-2 310-365 0.45 nm
0.15 nm ×
pixel-1
O3 profile, O3 column (TOMS & DOAS),
Bro, OCIO, SO2, HCHO, aerosol, surface
UV-B, surface reflectance, cloud top
pressure, cloud cover
VIS 365-500 0.63 nm
0.21 nm ×
pixel-1
Aerosol, NO2, OCIO, surface UV-B,
surface reflectance, cloud properties (top
pressure and cover)
Source: Modified from Levelt et al. (2006).
This mission aims to monitor the recovery of the ozone layer in response to the phase out of
chemicals. OMI data increase the accuracy of retrieving the total ozone profile from the Earth’s
surface to the TOA, supplying near real-time (about 100 to 165 minutes after satellite recording)
ozone products. It can also sense other key air quality components, including NO2, SO2, BrO, OCIO
and aerosols, and provide the total column quantity of atmospheric ozone, NO2, and lower
atmospheric dust, smoke, and other aerosols that can be further distinguished as smoke, dust, and
sulfates types. The hyperspectral capabilities of OMI allow low-level SO2 to be retrieved. Besides,
OMI is also a valuable data source for monitoring volcanoes to complement airborne sensing, able to
capture explosive SO2 emissions at a high temporal resolution. It can detect volcanic ash and SO2
originating from volcanic eruptions at a sensitivity up to at least 100 times higher than TOMS. The
improved accuracy and precision of the total ozone amounts enable accurate radiometric and
wavelength self-calibration over the long term.
Global Ozone Monitoring Experiment-1 and -2
GOME was launched in April 1995 onboard the ERS-2 satellite into a near-polar sun-synchronous
orbit at a mean altitude of ⁓785 km, with a mean equatorial crossing time of 10:30 a.m. local time.
This mission aims to determine the global distribution of ozone and several other trace gases.
GOME is a 4–channel grating spectrometer measuring the radiation backscattered from the
atmosphere and/or reflected by the Earth’s surface in the UV and visible spectral region of 237–794
nm at a moderate spectral resolution between 0.2 and 0.4 nm. GOME covers the globe only every
three days after 43 orbits, scanning the surface in a viewing zenith angle of ±30° in 4.5 s (Burrows et
al., 1999). One across-track scan is divided into three 1.5 s ground pixels. At the maximum 960-km
across-track swath, GOME pixels have a spatial resolution of 40 km × 320 km for most of the orbits.
For about 10% of the time, the swath is reduced to 240 km, causing all pixel sizes to be four times
smaller. GOME is able to measure total columns of NO2, BrO, H2O, O4, O2, and NO3; OClO and8.2.4
ClO under ozone hole conditions, NO (above 40 km), SO2 under polluted conditions and following
volcanic eruptions, H2CO under polluted conditions, and ozone profiles globally.
After GOME-1 malfunctioned in 1999/2000, it was succeeded by GOME-2, the second
generation of EO sensor flying on the ESA´s MetOp satellite launched on October 19, 2006.
GOME-2 is a UV-visible spectrometer operating in the spectrum of 240–790 nm with a spectral
resolution of 0.2–0.5 μm. It inherits the design of GOME in its spectral range of sensing and band
designation. The sensor is equipped with an optical spectrometer, fed by a scan mirror, performs
across-track nadir scanning at a swath width of 1,920 km, and sideway viewing for polar regions.
The entire globe is scanned daily, forming an FOV of 80 km × 40 km on the ground, four times finer
than GOME images with better polarization and calibration capabilities.
GOME-2 senses the solar radiation backscattered by the atmosphere and reflected from the
Earth’s surface. This instrument is best at sensing atmospheric ozone, trace gases, and UV radiation.
It also provides accurate measurements of the total column concentration of NO2, SO2, water vapor,
oxygen/oxygen dimmer, bromine oxide and other trace gases, and aerosols. GOME-2 data products,
produced using the DOAS approach, include absolute radiometrically calibrated earthshine radiance
and solar irradiance spectra (level-1 products), and global column concentrations of ozone and NO2
(level-2 products). In addition to these standard operational data products, GOME also delivers
important information about other minor trace gases such as O3, CHO, ClO, volcanic SO2, H2CO
from biomass burning, tropospheric BrO, and aerosol properties.
Earth Polychromatic Imaging Camera and AIRS
The EPIC is a payload of the Deep Space Climate Observatory (DSCOVR) satellite, an EO satellite
launched by the NASA on February 11, 2015. DSCOVR flies on the Lissajous orbit about 1.5 × 106
km away at the Lagrange-1 point in the Earth-Sun system. EPIC can track the change of aerosol
layer height multiple times a day, yielding a unique angular perspective at an almost constant
scattering angle between 165° and 178°. EPIC continuously measures the solar backscattered and
reflected radiance on the entire sunlit half of the Earth every 1–2 hours. The scanned images have a
nadir resolution of 2048 × 2048 pixels, rendering a pixel size of 12 km × 12 km at the image center.
EPIC comprises 10 narrow channels whose wavelength spans from UV to NIR spectral zones. Two
absorption bands have a central wavelength of 688 and 764 nm with a bandwidth of 0.8 nm and 1.0
nm, respectively. Two continuum bands at 680 and 780 nm are useful for monitoring cloud height.
The four UV bands are designed primarily for detecting total O3 and aerosols based on heritage
algorithms developed for the series of TOMS. The UV measurements are also functional in
estimating sensitive levels of SO2 and volcanic ashes (Huang and Yang, 2022).
Operational since May 2002, the Atmospheric Infrared Radiation Sounder (AIRS) is a
hyperspectral sensor onboard the Aqua satellite. AIRS offers 2,378 infrared channels covering the
spectral range of 650-2700 cm-1 (15.4–3.7 mm), all having a swath width of 1,650 km and a nadir
spatial resolution of 14 km. AIRS images cover 95% of the globe daily, and has the potential to
retrieve a number of volcanic ashes at several wavelengths, including 1362 cm-1 (7.34 μm) in the
strong SO2 asymmetric stretch mode (v3), 2500 cm-1 (4 μm) in the weaker symmetric and
asymmetric stretch mode (v1 + v3), and between 830 cm-1 (12.048 μm) and 1250 cm-1 (8 μm) for
measuring aerosol and ash absorption features (Thomas et al., 2011).8.2.5
Table 8.2
Spatial coverage and resolution of SCIAMACHY measurements in three operation modes (unit: km)
Mode Direction FOV Coverage
Type of spatial resolution
Detector arrays PMD
Nadir
Along track 25 Continuous 30 ⁓30
Across track 0.6 ±480 240 (60*) ⁓15
Limb
Azimuth 110 ±480 240 ⁓30
Elevation 2.5 0-150 3 3
Solar occultation
Azimuth** 40 n.a. 30 ⁓30
Elevation 2.5 0-150 2.5 2.5
Source: Modified from Bovensmann et al. (1999). © American Meteorological Society. Used with permission)
Notes: PMD – polarization measurement device.
* selected spatial windows.
** Spatial resolution is defined by the diameter of the sun.
Italics: at tangent point.
SCIAMACHY
SCIAMACHY is a spectrometer onboard ESA’s EnviSat-2 that was operational during 2002–2012.
It is designed to measure sunlight transmitted, reflected, and scattered by the Earth’s atmosphere and
surface in the UV, VIS, and NIR wavelength range of 240–2380 nm (Bovensmann et al., 1999). This
passive imaging sensor has a moderate spectral resolution of 0.2–1.5 nm. It comprises a mirror
system, a telescope, a spectrometer, and thermal and electronic subsystems. The nadir mirror scans
across the satellite track and each full scan corresponds to a ground area of approximately 30 km
(along track) by 960 km (across track). SCIAMACHY is configured with three viewing geometries
of nadir, limb, and sun/moon occultation (Table 8.2). The limb and nadir-viewing geometries allow
the detection of the earthshine radiance whereas occultation enables measurements of solar or lunar
radiance transmitted through the atmosphere continuously from 240 to 1700 nm and selectively over
the wavelengths of 1900–2400 nm (Table 8.3). The ability to combine limb–nadir measurement
modes is a special feature unique to SCIAMACHY. The measured spectra of sunlight are
backscattered by the Earth’s surface and atmosphere in the shortwave infrared (SWIR) spectral
range. The absorption, reflection, and scattering behavior of the atmosphere and the Earth’s surface
is determined from comparison of earthshine radiance with solar irradiance. Inversion of the ratio of
earthshine radiance to solar irradiance yields information about the quantity and distribution of
important atmospheric constituents and the spectral reflectance (or albedo) of the Earth’s surface.
SCIAMACHY’s spectral stability is designed as 1/50 of a detector pixel, which requires a
temperature stability of the spectrometer to be better than 250 mK over one orbit in combination
with dedicated calibration measurements. SCIAMACHY has a high absolute radiometric accuracy
of 2–4% (relative accuracy <1%).
Table 8.3
Major spectral properties of the SCIAMACHY spectrometer in two modes of operationMode of sensing Channel
Spectral range
(nm)
Spectral resolution
(nm)
Stability
(nm)
High resolution channels
1 240–314 0.24 0.003
2 309–405 0.26 0.003
3 394–620 0.44 0.004
4 604–805 0.48 0.005
5 785–1050 0.54 0.005
6 1000–1750 1.48 0.015
7 1940–2040 0.22 0.003
8 2265–2380 0.26 0.003
Polarization measurement
device
PMD1 310–377 broadband
PMD2 450–525 broadband
PMD3 617–705 broadband
PMD4 805–900 broadband
PMD5 1508–1645 broadband
PMD6 2265–2380 broadband
PMD7 802–905 broadband
Source: Modified from Bovensmann et al. (1999), © American Meteorological Society. Used with permission.
SCIAMACHY data can resolve a variety of important issues in sensing the physic-chemical
properties of the atmosphere (including the troposphere, stratosphere, and even mesosphere) and
potential changes resulting from increasing anthropogenic activity and the variability of natural
phenomena. The inversion of SCIAMACHY data enables the quantification of tropospheric column
content and distribution of various trace gas species, including O3, BrO, OClO, ClO, SO2, H2CO,
NO, NO2, NO3, CO, CO2, CH4, H2O, N2O, and aerosol, generate knowledge on other atmospheric
parameters such as pressure, temperature, radiation field, cloud cover, cloud-top height, and surface
spectral reflectance (Bovensmann et al., 1999). The level-2 products from SCIAMACHY data are
available in two modes, fast delivery and online (Table 8.4). Fast delivery data products are available
within a few hours of spectrum acquisition, produced using LUTs for radiative transfer instead of
operational model inversion. Offline products are produced using improved ancillary data (e.g.,
analyzed temperature and pressure fields).
Table 8.4
SCIAMACHY level-2 operational data products and their temporal availability
Mode
Column concentration (nadir) Stratospheric profile (limb)
UV/Vis IR UV-IR UV/Vis IR UV-IR
Fast delivery O3 H2O Cloud
NO2 N2O Aerosol
OCIO* CO
SO2* CH4
H2CO*8.2.6
Mode
Column concentration (nadir) Stratospheric profile (limb)
UV/Vis IR UV-IR UV/Vis IR UV-IR
BrO*
Offline
O3 H20 Cloud O3 H2O Aerosol
NO2 N2O Aerosol NO2 CO2
BrO CO BrO CH4
OCIO* CO2 p, T
SO2* CH4 N2O
H2CO* p, T CO
UV index*
Source: Modified from Bovensmann et al. (1999). © American Meteorological Society. Used with permission.
Note: *: observed under special conditions (e.g., volcanic eruption, ozone hole conditions, or tropospheric pollution) or after
averaging.
GOSAT
The Greenhous gases Observing SATellite (GOSAT) launched on January 23, 2009 by Japan is the
world’s first satellite designed for exclusively monitoring column greenhouse gases of CO2 and CH4
concentration, expressed as the number of gas molecules in a column above a unit surface area.
GOSAT aims to estimate emission and absorption of these greenhouse gases on a subcontinental
scale accurately and to facilitate the evaluation of the carbon balance of the terrestrial ecosystem and
assessments of regional emission and absorption. Launched to an orbital altitude of approximately
666 km, GOSAT completes one revolution in ⁓100 minutes, with a revisit period of three days.
This world’s first CO₂-specific sensing satellite carries the payload of the Thermal And Near￾infrared Sensing for carbon Observation (TANSO) sensor that has a high optical pass for CO₂
observation at a high spectral resolution. This sensor is composed of two sub-units: a Fourier
Transform Spectrometer (FTS) and a Cloud and Aerosol Imager (CAI), both having four spectral
bands. All CAI images have a nadir spatial resolution of 500 m and a swath width of 1,000 km
except band 4 whose spatial resolution is 1.5 km. They are dedicated to estimate clouds and
aerosols. The four TANSO spectral bands have a spectral resolution of 0.2 cm-1, and a spatial
resolution of 15.8 mrad, translated to 10.5 km on the Earth’s surface. TANSO has a reduced range of
absorption spectrum but contains an extra 5.56–14.3 μm far infrared absorption band (band 4) to
improve the observation accuracy. All four TANSO bands can detect CO2 except band 1 which
targets O2. GOSAT data can ascertain the global distribution of CO2 and CH4 that jointly account for
nearly 90% of the total warming effect caused by these gases. GOSAT measurements have been
turned into various data products at a few levels (Table 8.5). They can deepen our understanding of
the global distribution and temporal variation of the greenhouse gases, as well as the global carbon
cycle and its influence on climate.
Table 8.5
Major GOSAT data products and their properties (L1 radiance data details omitted)
Processing
level Sensor/band Product name Description Unit8.2.7
Processing
level Sensor/band Product name Description Unit
L2
FTS
SWIR
CO2 column
Abundance retrieved from
SWIR radiance spectral data
1 - multiple
scans
CH4 column
H2O column
FTS TIR*
CO2 profile Retrieved from TIR radiance
CH4 profile spectral data
CAI Cloud flag Cloud coverage data CAI frame
L3
FTS
SWIR
Global CO2
Column-averaged mixing ratio
data projected on a global map
Global
monthly
Global CH4 average
CAI
Global
radiance
Three days’ worth of data,
including data for cloudy
segments
global
Global
reflectance
Radiance data (clear-sky
segments from a month worth of
data)
NDVI data
Global distribution with cloudy
segments removed
Lat. 30°´Lon.
60°
L4A -
Global CO2**
Monthly average flux per each
of 64 global regions
1° mesh
(annual)
Global CH4**
Monthly average Flux per each
of 43 global regions
1° mesh
(annual)
L4B -
Global CO2 3D global distribution of
concentration
2.5° monthly
Global CH4 mesh
Source: GOSAT project.
Notes:
* L2 temperature and H2O profiles added later.
** flux. All others are global distribution.
GOSAT is replaced by GOSAT-2 that was launched into a sun-synchronous orbit of 613 km on
October 29, 2018. Its payload comprises two sensors, TANSO-FTS-2 and CASI-2. TANSO-FTS-2
follows its predecessor in band designation with one extra band 5 (8.4–14.3 mm), and a better SNR
for band 1. TANSO-CAI-2 is a push broom imaging radiometer using linear array sensor and band
pass filter, identical to its predecessor carried aboard the first GOSAT satellite. The LOS of CAI-2 is
tilted ±20° in the along-track direction, not the nadir direction as with the GOSAT. CAI-2 consists of
five lens systems, each having two FOVs of forward and backward viewing by ±20°. All bands have
a spatial resolution of 460 m (15.8 mrad) except band 5 (700 mrad or 1,400 m), with a swath width
of 903 km and an FOV of 67.6°. GOSAT is a valuable data source for measuring the global levels of
CO, CO2, NO2, CH4, O3, and water vapor in the atmosphere at a temporal resolution of three days.
Tropospheric Monitoring InstrumentOnboard the low Earth orbiting Sentinel-5 Precursor satellite launched on October 13, 2017 is the
sole payload of TROPOspheric Monitoring Instrument (TROPOMI) (www.tropomi.eu), dedicated to
monitor the Earth´s atmosphere exclusively. This mission detects Earth radiance spectra in the SWIR
spectral range around 2.3 μm with a dedicated instrument module. The satellite operates in an 824
km sun-synchronous polar orbit (inclination = 98.6446°) with an ascending node of 13:30 hours
(local time), and a revisit period of 16 days in 226 orbits. This state-of-the-art passive push broom
nadir-viewing spectrometer inherits the design of OMI and SCIAMACHY for remotely sensing
atmospheric composition, including the estimation of O3, CH4, CH2O (formaldehyde), CO, NO2,
SO2, and aerosol using four spectral spectrometers in UV (270–320 nm), VIS (405–500 nm), NIR
(675–775 nm), and SWIR (2305–2385 nm) wavelength ranges. Each range is split into two bands
electronically, all having a spatial resolution of 7 km × 7 km. The visible band has a spectral
resolution of 0.55 nm and a spectral sampling interval of 0.2 nm. Similar to OMI, TROPOMI
maintains a swath width of 2,600 km that is comprised of 450 across-track pixels.
TROPOMI provides the best spatial resolution to date, offering an unprecedented true nadir
ground pixel size as small as 3.5 km × 5.5 km that drops to 1.8 km × 1.8 km in the occasionally used
zoom mode, one of its major advantages, along with an increased SNR. This passive grating imaging
spectrometer makes use of push broom staring instead of scanning with a nadir swath width of 2,600
km made up of 450 pixels. There are 3,245 or 3,246 along-track scan lines (4,172 or 4,173 after the
along-track pixel size reduction) in regular radiance orbits, leading to about 1.46 (1.88) million
ground pixels per orbit. TROPOMI extends the capabilities of the OMI from the Aura satellite and
the SCIAMACHY sensor from EnviSat. TROPOMI records measurements every second, covering
an area of approximately 2600 km wide × 7 km long.
The atmospheric components captured by TROPOMI at a high spatiotemporal resolution are
excellent for monitoring and forecasting air quality, ozone, and UV radiation. TROPOMI
measurements have been converted to a wide range of data products released to the public at a few
levels (Table 8.6). Level-1B products (i.e., all spectral bands) are geo-located and radiometrically
corrected TOA earth radiance and solar irradiance. Level-2 products are operational and more
diverse, including total column O3, SO2, NO2, CO, CH2O, and CH4, tropospheric columns of O3, O3
vertical profiles, cloud, and aerosol information (e.g. absorbing aerosol index and aerosol layer
height).
Table 8.6
The name, identifier and producer of standard Sentinel-5P level 2 data products
Product ATBD PUM Identifier Producer
Cloud RD3 RD4 L2__CLOUD__ DLR
NPP-VIIRS clouds RD5 L2__NP_BDx__ RAL
HCHO RD6 RD7 L2__HCHO___ BIRA/DLR
SO2 RD8 RD9 L2__SO2_____ BIRA/DLR
O3 total column RD10 RD11 L2__O3______ BIRA/DLR
O3 tropospheric column RD12 RD13 L2__O3___TCL IUP/DLR
Aerosol layer height RD14 RD15 L2__AER__LH KNMI
Ultraviolet aerosol index RD16 RD17 L2__AER___AI KNMI8.3
8.3.1
Product ATBD PUM Identifier Producer
O3 full profile RD18 RD19 L2__O3__PR KNMI
O3 troposheric profile RD18 RD19 L2__O3___TPR KNMI
Tropospheric NO2 RD20 RD21 L2__NO2_____ KNMI
CO RD22 RD23 L2__CO_______ SRON/KNMI
CH4 RD24 RD25 L2__CH4______ SRON/KNMI
Source: Apituley et al. (2022). © Royal Netherlands Meteorological Institute.
METEOROLOGICAL PARAMETERS
Meteorological parameters are diverse in nature, and are quantified using various remote sensing
data that are processed differently, such as humidity and air pressure. This section focuses on the
quantification of two parameters that are critical to modeling the climate, wind speed and rain rate.
Wind Speed
Wind is formed by moving air masses under the effects of differential atmospheric pressure between
high- and low-pressure systems. Knowledge of wind characteristics over vast oceans is important to
weather forecasting, ocean navigation, and climate modeling. Land surface wind can be measured
using anemometers, which is not possible over oceans unless they are deployed on small islands,
ships, and buoys. They generate only sporadic and point-based measurements. In comparison,
satellite data can produce spatial distribution of winds around the globe systematically and
repeatably. Wind speed can be estimated by tracking cloud motion on VIS images, from which wind
speed and direction are indirectly inferred. This method does not work in cloudless skies. Thus,
ocean wind vectors are commonly determined using microwave data. In total, four types of radar
data are suitable for detecting wind vector, including radar altimeter, scatterometer as exemplified by
ASCAT, polarimetric radiometer as exemplified by WindSat, and dual-frequency precipitation radar
(DPR). Radar altimeter can generate accurate wind speed but not spatially, even though the retrieved
speed is vital to verify the accuracy of wind speed estimated from other data or using other methods.
Surface wind images are created hourly from ascending and descending imagery data at a grid size
of 25 km × 50 km. The sequentially observed backscattering coefficient of the target enables
resolving the wind direction ambiguity.
Polarimetric radiometers are passive radar sensors with multiple polarizations. In addition to the
common vertical (V) and horizontal (H) polarizations, they also include the ±45° polarized and
left/right circularly polarized channels. The signal of a rough ocean surface in these channels
contains a small component whose intensity varies with wind direction in relation to the incident
angle (Yueh et al., 1999). This allows the estimation of both scalar wind speed and wind direction at
an accuracy similar to that from scatterometers. Polarimetric radar such as the WindSat sensor is a
conical scanner measuring all four Stokes parameters at three frequencies of 10.7 (X-band), 18.7
(Ku-band), and 37.0 (Ka-band) GHz, plus dual-polarized V and H channels at 6.8 (C-band) and 23.8
GHz (K-band). Dissimilar to previous radiometers, the WindSat sensor scans the target using both
forward and aft viewing. The 1.8 m conically scanning reflector produces a swath width of ~1000
km with a native resolution of 39 km × 71 km in the low-frequency channel.Wind speed is retrieved from radar data based on an RTM to calculate the microwave emission
from smooth and rough ocean surfaces that have differential backscattering properties. Empirically,
the estimation relies on the relationship of wind speed U10 (speed at 10 m above the mean sea
surface) with radar backscatter coefficient (). If estimated from Ku band radar altimeter data, the
equation takes the following form:
(8.7)
where Um = first guess estimate of U10, calculated as:
(8.8)
where a, β, γ, δ, and σb = model parameters to be determined via regression fitting. Their values are
determined to be 46.5 (a), 3.6 (β), 1690 (γ), 0.5 (δ), and 10.917 dB (σb) (Abdalla, 2012). This model
is flexible and can be used with any altimeter data so long as a fixed offset is applied to them. This
algorithm increases the measured wind speeds by about 0.40 m·s−1 over those retrieved using the
modified Chelton-Wentz algorithm.
If estimated from the Ku band, U10 estimation models consist of two parts involving nine
coefficients that can be simplified to four as:
(8.9)
where a, b, c, and d = coefficients for Ku-band dependence whose values are a = 1.92, b = -28.02, c
= 1.69, and d = 2.02 (Panfilova and Karaev, 2021). This model has a valid NRCS within 10 and 20
dB. Assessed against ASCAT data, it achieves an R² of 0.84, an overall bias of 0.26, and a standard
deviation of difference of 1.88 m·s−1 (Panfilova and Karaev, 2021).
Scatterometer data are seriously compromised in the presence of rain, in addition to a lack of data
near land, and a long revisit time (Bourassa et al., 2019). Even light rain (rain rates > 1 mm ∙ h-1) can
degrade the retrieved speeds with significant errors due to the attenuation of the radar signal by rain
drops that cannot be simulated using RTMs, particularly at high frequencies when scattering by rain
drops occurs. Longer wavelength C band outperforms Ku band in rainy conditions. Since rain rates
can be accurately estimated (refer to Section 8.4.2), the affected pixels can be flagged. The retrieval
of wind speeds in rainy conditions is still possible by exploiting the spectral differences of the wind
and rain signal via selectively combining multiple channels of different frequencies to reduce the
impact of rain on the BTs to a level that is still sufficiently sensitive to wind speed. A decrease in the
wind speed signal has no effect on the BTs at wind speeds up to 35 m ∙ s
-1. However, BTs in rainy
atmospheres cannot be resolved using a physical model, so have to be modeled statistically
(Meissner and Wentz, 2009).
Wind products have been produced from WindSat and ASCAT data. ASCAT data products are
released at three levels. Level-1B full data correspond to the σ0 values generated from individual
echo samples within an echo line in a source packet, together with associated data. Level-1B 12.5
and 25 km data correspond to re-sampled (spatially averaged) σ0 values, on a 12.5 and 25 km grid,
respectively. The product is organized as successive lines of nodes along track, referenced by the
orbit time that corresponds to that line of nodes. The RSS WindSat products are the only dataset
available that uses both the fore and aft look directions to obtain a wider swath and more8.3.2
complicated swath geometry, supplied in daily and time-averaged (three-day, weekly, and monthly)
binary format (Figure 8.1). Daily products are orbital data mapped to 0.25° grids and divided into
two sets of maps based on ascending and descending passes when adjacent passes overlap at high
latitudes and daily “seam”, earlier data are chosen. Three-day, weekly, and monthly products are
averages of three days ending on the file date (inclusive), average of seven days ending on the
Saturday file date (inclusive), average of all data within a calendar month.
FIGURE 8.1 Weekly wind speed of the world´s oceans produced by Remote Sensing Systems from
WinSat data using both the fore and aft look directions to obtain a wider swath and more complicated
swath geometry. (www.remss.com/measurements/wind, Remote Sensing Systems, Santa Rosa, CA,
USA.)
Precipitation Rate
Precipitation is one of the most important climate variables to the normal functioning of sustainable
agriculture. It directly affects the water balance on Earth and quality of life. Although detailed (e.g.,
time-sequential) records of precipitation are maintained by numerous weather stations around the
world, they are limited by their spatial extent and time of delivery that cannot provide sufficient lead
time for weather forecasting and spatial perspective needed for climate modeling. Thus, it is
necessary and advantageous to produce such information from remotely sensed data. Precipitation
rate is estimated exclusively from active radar data that are able to penetrate clouds to detect water
drops or particle diameter related to rain rate. Radar radiation can produce useful spectral cues
essential for accurate prediction of cloud properties and rainfall information. The radiative energy
received from precipitating particles in a radar band is expressed as:
(8.10)where C = radar constant; K = imagery dielectric constant; r = range of the target; ZH (mm6·m3) =
horizontal polarization reflectivity in dBZ. It is the standard radar measurement used for rainfall
estimation, and typically derived from low frequency (high-power) radar bands. The estimation of
rain rate is based on the ZH–rain rate relationship that can be embodied mathematically as a power
function:
(8.11)
where a and b = coefficients related to the sensing environments, such as storm type, temperature,
horizontal wind, and aerosol effects. They are determined via fitting radar observations and gauge
measurements and simulated radar/rainfall parameters from disdrometer readings. However, they do
not convery sufficient information to enable the adequate characterization of the microphysical
variability of raindrops in a range of meteorological systems, such as cold frontal, summer
thunderstorms, and tropical storms. So they have to be determined for different types of rainfall, or
using differential reflectivity ZDR (in dB) from dual-polarized bands (Michaelides et al., 2009).
Dual polarization radar measures ZH, ZDR, and the specific propagation differential phase shift
KDP in deg/km. Under the assumption of an equilibrium raindrop shape-size relation, rainfall is
estimated from these parameters using the following models:
(8.12)
(8.13)
(8.14)
where c and d = coefficients. Although consideration of polarimetric observations (e.g., ZDR)
improves rainfall estimation over the standard ZH-R estimator, they cannot capture the complete
variability of raindrop size distribution. An alternative method is to retrieve the prevailing diameter
of raindrops (Dm) directly from the polarimetric radar measurements and then derive rain rates from
those parameters by skipping the raindrop shape-size assumption. And ZDR in the above three
models is replaced by Dm.
The use of dual-frequency radar data improves the estimation accuracy and outputs more rain
parameters. The philosophy of estimation is more precise but also more complex involving five
equations, starting with liquid water content (LWC) (Adirosi et al., 2021):
(8.15)
(8.16)
(8.17)
(8.18)
(8.19)8.4
8.4.1
where λ = wavelength of the radar band in m; Kw = complex dielectric constant of water; rw = water
density (1 g·cm−3); = backscattering radar cross-section (in m2) for Ku- and Ka-band of a drop of
equivalent diameter; Nw = normalized intercept parameter (mm−1·m−3); Dm = volume-weighted
diameter; and v(D) = raindrop fall velocity that can be estimated from an empirical power
relationship with Dm.
Global rainfall information can be quantified using three types of coarse-resolution, purpose-built
microwave imagery data: TRMM Microwave Imager (TMI), Global Precipitation Measurement
(GPM) Microwave Imager (GMI), and GPM Core Observatory radar. TRMM is the first satellite to
use both active and passive sensors to observe moderate and heavy rains in the tropical and
subtropical regions. This mission has been replaced by the more advanced new generation GMI. It
has a 1.2 m diameter antenna that provides significantly improved spatial resolution over its
predecessor. This 13 microwave-channel, conical-scanning radiometer has a sensing frequency of
10–183 GHz. In addition to channels similar to those on the TMI, the GMI also carries four
millimeter-wave channels near 166 GHz and 183 GHz to provide near-global coverage between
65°N and 65°S at high frequency.
One of the primary payloads of the GPM Core Observatory is the DPR sensor that acquires data
in two bands of Ku (13.6 GHz) and Ka (35,5 GHz). Both bands have the same ground swath width
of 245 km and a nadir spatial resolution of 5 km (beam width: 0.71°; pulse width: 1.667 µs).
However, the Ku band has a range resolution of 250 m and operates in the normal scan mode,
whereas the Ka band has a range resolution of 250/500 m operating in the high sensitivity scan
mode. This radar sensor provides more accurate and sensitive rainfall measurements than its TRMM
predecessor, especially in quantitatively sensing light rainfall and snowfall in mid-latitude regions
(https://gpm.nasa.gov/missions/GPM/DPR), owing to the use of differential attenuation between the
Ku and Ka frequencies. Jointly, the two bands provide global 3D measurements of rain (and snow)
rates every three hours day and night.
Precipitation data from the GPM and TRMM missions are freely available to the public in a
variety of formats. TRMM levels-2 and -3 products are in GPM HDF5 format labeled with product
version V05A/V06A and V07 (http://disc.sci.gsfc.nasa.gov/). The improved rainfall products have a
wide swath of 878 km. In addition to precipitation rate (R), level-2 DPR products also encompass
vertical profiles of the attenuation-corrected radar reflectivity (Z), mass-weighted mean drop
diameter (Dm), and the normalized intercept parameter (Nw, i.e., the intercept parameter of the
gamma drop size distribution normalized with respect to the liquid water content Nw) (Adirosi et al.,
2021). Level-3 products provide daily and monthly global statistics of the level-2 products on a
latitude-longitude grid. In V07, the products are organized into high G2 (0.250×0.250) and low G1
(50×50) spatial resolution grids. The statistics of G1 data include mean, standard deviation, counts,
and histogram. For variables defined on the high-resolution grid, the same statistics are computed
except for a histogram, which is omitted (https://gpm.nasa.gov/data/sources).
ATMOSPHERIC IMPURITIES
Aerosols
Aerosols are atmospheric particles originating from natural emissions and anthropogenic activities in
both liquid and solid forms. They include sand and dust from deserts, sea salt, urban haze, soot fromsmoke particles and forest exudates and wildfires, fog, and even geyser steams. Aerosols fall into a
few types, such as urban haze, smoke particles, smoke aerosols, cloud aerosols, desert dust, sea salt,
and water. The diameter of aerosol particles varies between 0.1 and a few microns, depending on
their type or the atmospheric moisture content. Typically, aerosol type varies spatially and
temporally. In urban areas, aerosols are produced by industrial particulate air pollutants and smoke,
transportation, other human activities, and sulfates with a higher concentration. Aerosols, especially
those in the planetary boundary layer, are harmful to human health and commonly associated with
respiratory ailments. It is of great significance to study aerosols because they play a major role in
weather and climate systems by interfering with many atmospheric processes, such as altering the
atmospheric radiation and circulation patterns, and influencing air temperature.
Aerosols are detected from remotely sensed data via the level of attenuation they cause to the
solar energy reflected off the Earth’s surface on its transmission path to the TOA, or the atmospheric
extinction coefficient. It is commonly expressed as aerosol optical depth (AOD) or AOT that is
indicative of the quantity of aerosols distributed in an air column. Aerosols are commonly quantified
using the UV aerosol index (AI). It measures the deviation of the spectral variation of TOA radiance
from that of a pure molecular atmosphere, calculated as the N-value or –100log10I (I = solar
irradiance) difference between the modeled (ITOA) and measured (IM) radiance at wavelength λ
(Torres et al., 1998), or:
(8.20)
where R(λ) = at-sensor atmospheric reflectance at wavelength λ; Re = estimated reflectivity
parameter; and cl = spectral slope (coefficients) of the rgfc extrapolation (rg = natural surface
reflectance; fc = particle scattering and absorption). If derived from TOMS and OMI data, Eq. 8.20
is modified as:
(8.21)
The AI value varies with AOT, single scattering albedo, and aerosol height. Pixel-level AI has a
detectable limit of 0.3–0.6 due to TOMS instrument noise, footprint size, and wavelengths,
depending on aerosol type. Under most conditions, the AI value is positive for absorbing aerosols
and negative for non-absorbing aerosols (e.g., pure scattering). Although AI values ≤ 0.3 are
frequently valid, dust and smoke have an AI value above 0.6 that can be used for their detection.
Aerosols are differentiated into absorbing and scattering types in terms of their effects on the at￾sensor radiance. The former reduces the reflected radiation instead of boosting it as scattering
aerosols do. An absorbing aerosol layer exists between 3 and 4 km for all wavelengths. Elevated
absorbing aerosols in the troposphere can be estimated using the absorbing aerosol index (AAI), also
known as the UV Aerosol Index (UVAI). It distinguishes the spectral disparity between absorbing
aerosols and the spectral variations induced by molecule scattering, gaseous absorption, aerosol and
cloud scattering, and surface reflection. AAI isolates absorbing from scattering effects in the UV, and
hence is capable of monitoring UV-absorbing aerosols such as desert dust and aerosols originating
from biomass combustion from space. AAI quantifies the spectral contrast of UV reflectance at a
pair of wavelengths of 340 and 380 nm, or 309 and 322 nm (Eq. 8.22) and compares it to that of a
pure Rayleigh atmosphere. If derived from EP/TOMS data, the two wavelengths become the 331and 360 nm pair. AAI absolute value depends on many parameters, most notably AOT, the height of
the absorbing aerosol layer, and microphysical properties. Thus, it is difficult to retrieve unique
aerosol quantities from AAI (de Graaf et al., 2005). Nevertheless, AAI is useful for studying aerosol
impact on climate and heavy dust, biomass combustion, and volcanism:
(8.22)
where is calculated by assuming a Lambert Equivalent Reflectivity of (Torres et al., 2007). The
UVAI represents error (expressed in percentage divided by 2.3) in estimating satellite radiance at
354 nm from radiance at 388 nm, assuming a purely molecular atmosphere bounded by a spectrally
varying Lambertian surface.
Aerosol optical property and profile data are easily generated from the proper satellite data using
the Aerosol Toolkit in MORTRAN’s SAP (Spectral Aerosol Profile) package from analyst-specified
aerosol traits. This toolkit has the provision for multi-customizing the output of aerosol properties,
such as particle types, spectral refractive index data, particle size distributions, and density profiles.
Single particle optical properties for each aerosol component are computed directly using the Mie
codes for spherical particulates, or the T-Matrix codes for more general axisymmetric particulates.
Daily global AAI data product has been generated from the TOMS ozone data. This longest-existing
data product records monthly AAI at a grid size of 1° (latitude) ´ 1.25° (longitude), accessible from
the Internet. TOMS AAIs are calculated by averaging daily positive residues over one month, with
all averages lower than 0.7 set to zero. Erroneous measurements are flagged, which is true at least
for all measurements at latitudes higher than 60°N and 60°S, where satellite measurements are
inaccurate due to large solar zenith angles (de Graaf et al., 2005). AAI has also been derived from
GOME data. GOME AAI results bear a close correspondence with known UV-absorbing aerosol
events and the TOMS AAI. As GOME-2 has a global coverage only every three days, the GOME
AAI is ill-suited to monitor daily aerosol events but is well positioned to provide monthly global
maps of absorbing aerosols. Daily AAI results are available from TROPOMI (Figure 8.2).FIGURE 8.2 Near real-time absorbing aerosol index around the globe on July 29, 2023 derived from
TROPOMI by the Royal Netherlands Meteorological Institute.
(www.temis.nl/airpollution/absaai/#TROPOMI_AAI).
Apart from tailored satellite imagery data, aerosol properties such as height, optical depth, and
particulate extinction profile, can also be retrieved from the Cloud-Aerosol Lidar with Orthogonal
Polarization (CALIOP) and Aerosol LiDAR data. They can yield insights into the role of clouds and
atmospheric aerosols in regulating Earth’s weather, climate, and air quality. But the data are unable
to indicate the spatial distribution of aerosols as they are just laser altimeter data, so are not delved
further here. Instead, the attention focuses on Aerosol LiDAR, a network of multi-wavelength
LiDAR systems operated by the ESA across Europe as it is able to produce the spatial distribution of
aerosol profiling information. Aerosol LiDAR aims to create a comprehensive database about the 3D
spatiotemporal distribution of aerosols in the atmosphere. This airborne scanner detects the relative
concentration and spatial distribution of aerosol particles from the backscatter profiles at 532 and
1064 nm and aerosol (cloud) depolarization at 532 nm. The 532- and 1064-nm signals are isolated
by a dichroic beam splitter and the 532-nm signal is further split into orthogonal polarization
components. Comparison of aerosol backscattering signals at the two wavelengths indicates particle
size. This instrument is operational both daytime and nighttime, with a slightly degraded data quality
during the daytime.
Aerosol products have been produced from MISR data at a spatial resolution of 4.4 km. Data
product and its interpretation vary considerably with the version of the algorithm (V22). The V23
dataset is created from two separate retrieval algorithms that are applied over dark water and land8.4.2
surfaces. Besides having an improved horizontal resolution of 4.4 km from the coarser 17.6 m
resolution in V22 and streamlined format and content, the V23 product also contains additional
geolocation information, pixel-level uncertainty estimates, and improved cloud screening. The level￾3 (gridded) aerosol product is still released at the same spatial resolution with outcome aggregated
from the higher-resolution level-2 data. The format and content at level 3 have also been updated to
reflect the changes made at level 2. Both the level-2 and level-3 products are now released in
NetCDF format (Garay et al., 2020).
Aerosol Optical Thickness
AOT (AOD) refers to the degree at which light is absorbed or scattered by aerosols in the
atmosphere. AOT measures the visual transparency of the atmosphere, with a typical range from 0 to
0.6 (or 0 to 600 in terms of Digital Numbers or DN) in value. An AOT value <0.1 indicates a crystal￾clear sky with maximum visibility, whereas a value of 4 indicates the presence of dense aerosols that
make the sun not so visible even at midday. AOT quantifies aerosol load in the atmosphere and is
critical to understanding atmospheric physics and modeling regional air quality. As indicated in
Chapter 4, it is a very important parameter in the atmospheric correction of remotely sensed data for
retrieving parameters in other three spheres. Atmospheric AOT is measured via two ways, either
using a sun photometer at a fixed location on the ground or via satellite imagery. The former is
exemplified by the global AERONET of over 500 sites that has been in operation for decades. The
voltage (V) measured by a sun photometer is proportional to the received spectral irradiance (I). The
estimated TOA spectral irradiance (Io) in terms of voltage (Vo) is obtained by sun photometer
measurements. The total optical thickness (τTOT) is calculated using the following equation
according to Beer-Lambert-Bouguer law:
(8.23)
where V = digital voltage measured at wavelength λ; V0 = extraterrestrial voltage; r = ratio of the
average to the actual Earth-Sun distance; and m = optical air mass (Holben et al., 1998). Other
atmospheric constituents can scatter light and must be considered when calculating AOT. The optical
thickness due to water vapor, Rayleigh scattering, and other wavelength-dependent trace gases must
be subtracted from the total optical thickness to obtain the aerosol component
(https://aeronet.gsfc.nasa.gov/new_web/Documents/Aerosol_Optical_Depth.pdf), or:
(8.24)
All terms in the equation are a function of λ. Sun photometer-yielded AOT data are available at the
monitoring stations only without the ability to yield a spatial distribution of AOT unless the
observations are sufficiently dense to be spatially interpolated. This deficiency can be circumvented
by using satellite sensing. Satellite data are particularly strong at monitoring large-scale aerosol load.
They are acquired at visible and IR wavelengths, where multiple scattering in the atmosphere is less
important than in the UV, and RTMs are relatively simple to invert. In comparison, visible and NIR
radiation is not used so frequently as these spectral regions make aerosols difficult to retrieve over
large terrestrial surfaces.A series of retrieval algorithms have been developed for various underlying surfaces and data
acquired by several sensors. They differ from each other in how to resolve the two fundamental
issues in AOT inversion: the removal of the reflection contributed by the underlying surface, and the
specification of a proper aerosol model. The ease and complexity of AOT retrieval algorithms
fluctuate with the satellite data used. All algorithms fall into five general types: single-channel
algorithms, multi-angle algorithms based on MISR and other sensors, multi-channel algorithms,
structural function methods based on high-resolution data, and dark pixel methods (Li et al., 2021).
AOT may be simply derived from AI if the microphysical properties and height of aerosols are
known (Hsu et al., 1999). The computed AI values from TOMS are linearly proportional to the AOT
recorded by ground-level sun photometers deployed in areas affected by biomass burning and dust.
This linear relationship can be employed to convert AI to AOT of smoke and dust aerosols for
regions in close proximity to the sun-photometer sites. The relationship between AI and AOT is
predominantly affected by aerosol height and single scattering albedo. Extremely high aerosol loads
(i.e., τa(380 nm)>3 for dust or τa(380 nm)>4 for smoke) can destroy the linear relationship between
AI and sun-photometer AOT, as revealed by the results of theoretical model simulation (Hsu et al.,
1999). This relationship can be used to infer aerosol height but does not allow reliable retrieval of
AOT, so other methods are needed, such as the one proposed by Nguyen and Tran (2014):
(8.25)
(8.26)
(8.27)
where = aerosol scattering phase function; θSZ = local solar zenith angle; θv = viewing (sensor)
zenith angle; f = relative azimuth angle; μ = cosine of the view direction; μo = cosine of the
illumination direction; ωo = single scattering albedo. If quantified from optical images such as
Landsat 8 OLI, the calibrated pixel DN values of a spectral band are converted to TOA spectral
reflectance using the coefficients of the reflectance rescaling factors in the following equations (Sun
et al., 2016):
(8.28)
(8.29)
where ρλ = TOA reflectance without correction for the solar angle, DNcal = calibrated standard DN
value; M = band-specific multiplicative rescaling factor; A = band-specific additive rescaling factor;
θSZ = local solar zenith angle; θSE = local viewing zenith angle; ρΤOΑ = TOA reflectance.
The second simple method of estimating AOT is to use a SWIR band to calculate visibility that is
converted to AOT via a LUT (Main-Knorn et al., 2017). Visibility (VIS) is converted to AOT at 550
nm empirically using the following equation:
(8.30)where a(z) and b(z) are obtained using linear regression for a set of elevations z between 0 and 2.5
km above sea level. The regression is performed with height data. Visibility (horizontal
meteorological range) is approximated as the maximum horizontal distance calculated as:
(8.31)
where β = extinction coefficient (unit km−1) at 550 nm.
The third simple way of quantifying AOT is to make use of reflectance on two occasions by
calculating the difference in their optical thickness Dt, or:
(8.32)
where Δτ (τ2 − τ1) = difference in unitless optical thickness of clear days and polluting days; σ1(ρ)
and σ2(ρ) = standard deviation of reflectance on clear days and polluting days, respectively (Thi Van
et al., 2018). If the clear day τ1 is assumed to be 0, then:
(8.33)
where τ2 = AOT on the polluting day image.
The general methodology of AOT retrieval may be implemented using two algorithms:
Simplified Aerosol Retrieval Algorithm (SARA) and Simplified and Robust Surface Reflectance
Estimation (SREM). SARA is developed using MODIS data with a spatial resolution of 500 m to
retrieve regional AOT. It requires TOA reflectance/radiance, solar zenith and sensor azimuth angles,
and aerosol information to iteratively invert aerosol over a wide range of aerosol types. This
algorithm uses the surface reflectance estimated by SREM and AERONET-measured AOT or other
high-quality AOT as the inputs to estimate AOT without prior information on the aerosol optical and
microphysical properties usually obtained from a LUT constructed from long-term AERONET data.
SREM and SARA can be integrated to form SEMARA that enables AOT at 550 nm to be retrieved at
a local scale from 30 m Landsat 8 OLI, 500 m MODIS, and 750 m VIIRS data over bright urban
surfaces (Bilal et al., 2022). In this integration, SREM is used to estimate surface reflectance ρs as:
(8.34)
(8.35)
where ρTOA = TOA reflectance, which is a function of measured spectral radiance (LTOA), solar
zenith angle, earth-sun distance (d), and mean solar exoatmospheric radiation (ESUN); ρR =
Rayleigh reflectance in the absence of aerosols; λ = wavelength; Ts = atmospheric transmittance on
the sun-surface path (downward); Ts0 = same as Ts but in an aerosol-free atmosphere; Tv =
atmospheric transmittance on the surface-sensor path (upward); Tv0 = same as Tv but in an aerosol￾free atmosphere; Satm = atmospheric backscattering ratio to account for multiple reflections between
the surface and atmosphere; Satm0 = same as Satm but in an aerosol-free atmosphere; μs = cosine of
the solar zenith angle; μv = cosine of the sensor zenith angle; Pa = aerosol phase function; ωo= single
scattering albedo; τa = SARAAOT.AOT products have been generated from satellite data acquired by several sensors, including
MODIS, MISR, and VIIRS at a spatial resolution ranging from 1 km to 10 km (Table 8.7). The
quality of AOT products is influenced by sensor properties, retrieval algorithms, orbiting mechanics,
and data processing schemes. The accuracy of the retrieved AOT is sensitive chiefly to the
appropriate specification of an accurate cloud mask, the aerosol model, the accuracy of the estimated
surface reflectance, or effective decoupling of the surface- and atmosphere-contributed TOA
reflectance, and the surface-level aerosol extinction coefficient.
Table 8.7
Major properties of active geostationary satellite data that have found applications in deriving AOT data products
Name Country
Spatial
resolution
(km)
Temporal
resolution
Spectral
resolution
(μm) Longitude
Launch
time
GOES￾W(17) USA 0.5–2 15 min 0.45–13.6 135W 2018
GOES-E
(16) USA 2 ´ 2 15 min 0.45–13.6 75W 2016
Meteosat
10/11 Europe 3 ´ 3 Daily
0.81 or
0.55 41.5E/-0.02/0/0 2012/15
Himawar￾8/9 Japan 0.5–2 10 min 0.43–13.4 140E 2014/16
MTSAT-2 Japan 1–4 30 min 0.55–4.0 145.06E 2010
FY-2H China 1.25–5.0 30 min 0.5–12.5 79E 2018
FY-4A China 0.5–4 15 min 0.55–14.3 105E 2016
GF-4 China 0.05–0.4 20 sec 0.45–4.1 105.5E 2015
INSAT
3D,3DR India 4 30 min 0.55–4.8 82/74E 2013/16
Electro￾L2 Russia 1–4 30 min 0.5–12.5 77.8E 2015
Source: Wei et al. (2020). © Taylor & Francis.
The Sentinel AOT data product is derived at 550 nm using the DDV algorithm (see Section 3.5.2 for
more details), based on the correlation between SWIR (band 12) and VIS (red – band 4, and blue –
band 2) reflectance. The algorithm requires the scene to contain reference areas of a known
reflectance, preferably DDV and/or dark soil and water bodies (Main-Knorn et al., 2017). In the
absence of DDV pixels from the scene, atmospheric correction is undertaken using a constant AOT,
the start visibility designated by the analyst in the configuration file as the fallback solution. Default
visibility value starts from 40 km which corresponds to a sea-level AOT of 0.2 at 550 nm. The
quantitative value to convert DNs to AOT is equal to 1000. The algorithm outputs an AOT spatial
distribution map.8.5
8.5.1
8.5.1.1
Besides, AOT profile products are available from CALIOP data, mainly 532 and 1064 nm
column AOT, the vertical distribution of the extinction coefficient, the backscatter coefficient, and
the depolarization ratio. They have a high horizontal and vertical resolution of 333 m and 30 m,
respectively, over altitudes of 0–8.2 km, 1.0 km and 60 m for altitudes of 8.2–20.2 km, and 1.67 km
and 180 m over altitudes of 20.2–30.1 km (www-calipso.larc.nasa.gov/ products). More details on
the CALIPSO satellite and its parameters are available on the NASA website (www￾calipso.larc.nasa.gov/documents).
SOLID PARTICULATES
PM2.5 and PM10
Particulate matters (PMs) are tiny particulates with an aerodynamic diameter of a few microns
suspended in the atmosphere. They originate naturally from forest fires and volcanic eruptions, and
anthropogenically from the combustion of fossil fuels and industrial activities. If inhaled, surface￾level PMs can puncture the lung to cause chronic and respiratory diseases. PMs are classified as 2.5,
5, and 10 μm according to their diameter. The smaller PMs are more harmful to human health than
the larger ones, especially PM2.5. Thus, they have been extensively studied. Although PMs can be
monitored on ground, it is remote sensing that is capable of generating a spatial perspective on their
distribution. Remote sensing-based estimation of PM concentrations is most commonly achieved
from AOT, a popular data product of satellite and ground observations. AOT measures spectral
extinction of solar radiation by aerosol scattering and absorption in the atmospheric column. As
stated by Beer-Lambert law, the more particulates are suspended in the atmosphere, the more the
incoming solar radiation is attenuated by them (Eq. 8.1). Theoretical analysis based on the
atmospheric RTM has identified a positive correlation between AOT and PM concentration, and this
relationship is exploited to retrieve PMs. Nevertheless, the two are not enumerated at the same
height. Satellite-derived AOT reflects the aerosol optical properties of the atmospheric column while
the PM concentration is usually measured near the Earth’s surface. The column concentration of
PMs refers to the total PM mass integrated over the vertical column from the sensor all the way
down to the Earth’s surface, even though the mass does not have a vertically uniform distribution. It
is the concentration derived from spaceborne remote sensing data after correction for the
atmospheric effects. Thus, the correlation between them is severely influenced by the vertical
distribution of aerosols and relative humidity (RH). Their exact impacts are subject to atmospheric
profiles, ambient conditions, and the size distribution and chemical compositions of aerosols.
Estimation Models
Remote sensing-based estimation of PMs falls into two types, surface concentration estimation from
imagery data, and near-ground concentration from imagery-derived AOT in combination with broad￾scale environmental co-variables. In the former category, PM estimation models are empirically
developed via regression analysis (single or combined statistical models), chemical transport model￾based semi-empirical/physical models, and vertical correction. The latter retrieval is based on big
data-driven machine learning methods or linear regression that can increase the spatiotemporal
resolution of PM2.5 products to daily variations at 1 km and hourly intervals. Physical models are
complex and require specification of parameters on the atmospheric state, from which PM propertiesare inverted, such as particulate size, vertical distribution, RH, extinction, and mass conversion.
Some of them are specific to sensors. For instance, the global atmospheric chemistry model (GEOS￾CHEM) is developed for MISR data for inverting the vertical distribution and propagation
characteristics of AOT (Liu et al., 2004). This simple approach estimates ground-level PM2.5
concentrations by using local-scale variables from the GEOS-CHEM with dust and sea salt data to
retrieve AOT, namely:
(8.36)
In comparison, empirical models are much simpler involving only AOT, more flexible and easier to
develop and use. Empirical or statistical models are constructed by regressing the AERONET￾measured PM2.5 at certain stations against satellite-derived AOT. So far, tens of estimation models
have been empirically established for different seasons and environments (e.g., aerosol types). They
can be linear or non-linear, applicable to AOT data obtained from diverse sensors. The simplest
model takes the following form (Liu et al., 2005):
(8.37)
This model is inaccurate and has a limited applicability as it ignores aerosol size distribution and
altitude. Thus, it has been modified by considering RH and particle dry mass density, leading to the
formation of semi-empirical models (Zhang and Li, 2015). They are also known as semi-physical
models because they are based on the physical properties of PMs and their vertical distribution,
taking into account the ambient environmental settings, some of which are empirically determined.
PM2.5 is estimated as:
(8.38)
where = dry mass density of near-ground particulates; PBLH = planetary boundary layer height in
km; RH = relative humidity (from 0 to 100); = optical hydroscopic growth function, served by the
particulate hydroscopic growth function calculated as:
(8.39)
where a, b = parameters of hydroscopic growth function. VEf (unit: mm3•mm-2) = columnar volume￾to-extinction ratio of fine particulates, empirically determined as:
(8.40)
where FMF = fine mode fraction, or the ratio of AOTf (fine particle AOT) to AOT. However, this
method yields only r = 0.5 (n = 421) in predicting PM2.5 in comparison with hourly in situ
measurements during three months. One reason for the low reliability is the mismatch in temporal
scale between the in situ and satellite measurements.
The accuracy of retrieval can be improved via vertical-and-RH correction. It reduces the
uncertainty or the discrepancy between surface PM and its vertical distribution, via PBLH and
ambient RH (Wang et al., 2010). Theoretically, it is grounded on the direct solar irradiance E
reaching the ground after attenuation by the atmosphere that can be expressed as:(8.41)
where Es = TOA solar irradiance; and m = air mass defined as m = 1/cosθs (θs = solar zenith angle);
τ = total AOT. If the gaseous absorption is negligible in a clear sky, it is equal to the sum of
molecular optical thickness (τm) and AOT. τm depends only on wavelength and air pressure, and is
easily computed according to Rayleigh scattering law. AOT is then derived by subtracting τm from τ.
Under the assumption of a plane parallel atmosphere, AOT represents the integral of aerosol
extinction coefficient ka at all vertical altitudes, namely:
(8.42)
where ka(λ,z) = ka at altitude z and wavelength λ. The vertical distribution of ka(λ,z) is approximated
as the negative exponent form of aerosol height (Ha), or:
(8.43)
where Ha = scaled height of aerosols. It can be approximated as the PBLH.
(8.44)
Thus, ka,0 can be calculated from AOT and Ha approximately, from which PMs are estimated
empirically as:
(8.45)
(8.46)
(8.47)
where f(HR) = (1 – RH)-g (g = an empirical fit coefficient).
Vertical correction boosts the R2 value of estimation from 0.35 to 0.56. The correlation between
ka,0 and PM concentrations is significantly improved by the RH correction with the R2 rising from
0.43 to 0.77 for PM10, and from 0.35 to 0.66 for PM2.5 (Wang et al., 2010). Both MODIS-estimated
PM10 and PM2.5 have R2 = 0.47 with their in situ counterparts, with the corresponding bias being
26.33% and 6.49%, respectively. When averaged in the urban area of Beijing (Figure 8.3), the R2
between the estimated PM10 and the field measurements rises 0.66.8.5.1.2
FIGURE 8.3 Distribution of PM2.5 in Beijing urban areas on August 15, 2007, derived from the 1-km
MODIS AOT products. (Wang et al., 2010, with permission (5761270446727) from Elsevier.)
Surface-level Concentration
Column concentration of PM2.5 bears little significance to surface-level concentration that is the
most harmful to human health, so it needs to be converted to the surface level. The translation of
column concentration to surface concentration is usually implemented in two stages (Li et al., 2021).
The first stage is to retrieve AOT using the methods described above. The second stage is to estimate
PM2.5 from AOT in conjunction with other supplemental data that govern the spatiotemporal
distribution of PM2.5 or data correlated with PM2.5 variation. Different AOT-PM2.5 models have
been constructed using statistical and machine learning methods. Statistical models include multiple
linear regression (MLR) model, mixed-effects model, generalized additive model (GAM), and
geographically weighted regression (GWR) model. GWR models are more accurate than linear
regression models. Since PM concentrations vary temporally, the commonly used GWR model has
to be adapted to form the geographically and temporally weighted regression (GTWR) model (Sun
et al., 2021). This model considers PBLH, RH, AOT, time, space, and vegetation factors. The first
three are treated as the independent variables in predicting PM, with the regression coefficients
being functions of the remaining three, namely:(8.48)
where βj (j = 0, 1, 2, 3) are all functions of location (x,y), Julian day (d), time (hour or h) and land
use information represented by NDVI. i = 1, 2, …, n (total number of observation points). They are
weighted by the distance between the point in question (u0, v0, d0, h0, NDVI0) and other
observations. Essentially, this method is grounded on spatial interpolation of PM concentration
combined with linear regression analysis, taking into account time. This method is applicable to both
PM2.5 and PM10 over land surface of an hourly temporal resolution between 00:00 and 09:00 (UTC)
each day. The PM2.5 model has an R2 of 0.909 and RMSE of 5.802 g ∙ m-3. These accuracy
indicators change to 0.915 and 12.939 g ∙ m-3 for the estimated PM10 after land use classification
data are replaced by NDVI in the empirical model.
The aforementioned methods achieve the highest accuracy of only 65% over urban areas because
they are not corrected for vertical distribution, induced by topography that is very critical to PM
distribution. A better alternative is to make use of machine learning methods such as DNN and RF.
They can accommodate as many variables affecting PM2.5 as necessary. DNN utilizes error
backpropagation to determine the weight and bias of each hidden node, but must be trained using
ground truth data, such as in situ measured PM concentrations. The DNN consists of n time-hidden
input nodes, and the optimal hyperparameters for the deep-learning model include the number of
hidden layers, L1 regularization value, batch normalization, and activation function. Overall, the
DNN achieves the highest accuracy (R2 = 0.698, RMSE = 7.042 μg ∙ m-3, and MBE = -0.340 μg ∙ m￾3) in comparison with the ground-measured PM2.5. RF is less accurate than DNN, achieving R2 =
0.619, RMSE = 7.904 μg ∙ m-3, and MBE = 0.225 μg ∙ m-3 (Lee et al., 2021). Both RF and DNN
tend to underestimate high concentrations above a certain threshold (e.g., >25 μg ∙ m-3), especially
RF. Compared with machine learning methods, MLR is much less accurate in estimating PM
concentrations that are clustered in a narrower range compared to that of the actual values, and the
maximum output value is not reflected in high concentration areas within 40 μg ∙ m-3 (Table 8.8).
Table 8.8
Comparison of accuracy of three analytical methods in predicting ground-level PM2.5 from geostationary satellite and
reanalysis data
Method R² RMSE (μg × m–3) MBE (μg × m–3)
DNN 0.49 9.166 0.293
RF 0.47 9.342 0.337
MLR 0.25 11.133 -0.0428
Source: Lee et al. (2021), open access.
The low cross-validation accuracy prompts the use of a new spatiotemporally weighted random
forest (SWRF) model to improve the estimation accuracy and to expand the spatial coverage of
PM2.5 concentrations by taking advantage of the latest release of the VIIRS Deep Blue (DB) aerosol
product (Xue et al., 2021). It also considers meteorological variables and socioeconomic data in two
steps. Initially, preliminary near-surface PM2.5 is estimated from ground observations, nighttime
light NDVI, ERA-Interim meteorology, and VIIR APO product (6 km) via RF regression (Figure
8.4). Subsequently, the preliminary results are further refined using VIIRS AOT, PM2.5 biases, andDEM via GWR to generate residuals for correction by considering 12 variables (DEM, land cover,
wind speed, wind direction, temperature, RH, nighttime light, precipitation, ET, BLH, surface
pressure, and NDVI). As indicated in Eq. 8.49, only three of them (AOT, DEM, and land cover) are
the most critical to predicting PM2.5 residuals. Although the accuracy is much higher than in
previous studies (R² ≥ 0.83) (Table 8.9), the results are expressed at a much coarser spatial resolution
of 6 km:
FIGURE 8.4 Flowchart of data processing in constructing the SWRF model to estimate near-surface
PM2.5 concentrations. (Modified from Xue et al., 2021, open access.)
(8.49)where PM2.5−resigt = PM2.5 residual simulated by the RF model in grid g in year t; a0g = intercept in
grid g; a1, a2, and a3 = location-dependent (xgt,ygt) coefficients of AOD, DEM, and LUC; and ε0gt =
error term.
Table 8.9
Typical accuracy achievable in predicting surface-level PM2.5 from various aerosol data products at 6 km resolution using
various modeling approaches
Method
Aerosol
product Study area R² RMSE Reference
TFER+GWR VAOOOO
Beijing-Tianjin￾Hebei 0.72 19.29 Wu et al. (2016)
TFER VAOOOO
Beijing-Tianjin￾Hebei 0.72 22.07 Yao et al. (2018)
LME* VAOOOO Central China 0.64 18.02
Zhang et al.
(2019)
LME+GAM VAOOOO Central China 0.69 15.82
LME+GWR VAOOOO Central China 0.70 15.73
TFER+GWR VAOOOO China 0.60 21.76 Yao et al. (2019)
SWRF AERDB China 0.87 11.53
Xue et al. (2021)
AERDB BTH 0.89 12.65
AERDB Yellow River Delta 0.87 9.74
AERDB Pearl River Delta 0.83 8.35
Source: Xue et al. (2021), open access.
Note: GAM = generalized additive model; GWR = geographically weighted regression; LME = linear mixed effect; SWR =
spatiotemporally weighted random forest; TFER = time fixed effects regression. AERDB = VIIRS aerosol product (VAOOO VIIRS
Version 1 DB aerosol) generated using the DB algorithm over land and a satellite ocean aerosol retrieval algorithm over the ocean.
The use of two-stage RF allows even more variables to be incorporated into the estimation of
surface-level PM2.5 (Jung et al., 2021). At the first stage, a RF model is used to impute the missing
AOT values according to the following model:
(8.50)
where AOTj = AOT value on day j; Lon and Lat = longitude and latitude; Tj = temperature on day j;
RHj = relative humidity on day j; CFj = cloud fraction on day j; Ht = boundary layer height. This
model applies to the pixel level, so all terms have the subscript i (ith grid) that has been omitted for
brevity.
The second-stage RF model estimates ground-level PM2.5 concentrations from surface pressure,
wind speed and direction, and precipitation, all of which significantly influence the formation and
distribution of PM2.5. Industrial areas, road networks, and human population are regarded as the
emitters of PM2.5 while accessibility (e.g., distance to major roads) is considered as a factor
governing PM2.5 decay. Because the estimation model is obscure to the viewer, the PM at a given
location on day j can be expressed only conceptually as:(8.51)
where JD = Julian day of the year; T = temperature; SP = surface pressure; ZW = zonal wind speed
at 10 m height; MW = meridional wind at a height of 10 m; RF = rainfall; Aind = area of industrial
facilities; Aub = urban and built-up area; Ht = height; Lrd = road length; D2pr = distance between the
grid centroid and the nearest primary road; D2hw = distance from the grid centroid to the nearest
highway; and P = average population count. The model is applicable to individual cells or grids, so
all the parameters in the model are a function of grid location (lon., lat.).
The consideration of so many variables produced consistent multi-angle atmosphere-corrected
AOT with ground truth in Japan (correlation coefficient = 0.82 and 74.62% of data falling within the
expected error). With the training dataset, the model has a R2 of 0.98 and a RMSE of 1.22 μg ∙ m-3
(Jung et al., 2021). A 10-fold cross-validation confirms the excellent performance of the model with
an R2 of 0.86 and an RMSE of 3.02 μg ∙ m-3, which demonstrates that PM2.5 estimates derived from
the satellite-based model are reliable and accurate (Figure 8.5). It must be noted that the PM2.5 data
used to train the RF are collected from 1,068 observations stations densely distributed in urban
areas. It is speculated that a similarly high accuracy can be achieved using simple spatial
interpolation instead of the complex RF method. Besides, the high accuracy is attained by
aggregating the data over five years (2011–2016). Undoubtedly, the accuracy will drop as the
temporal span is shortened to the monthly or daily duration.FIGURE 8.5 The overall average estimates of surface-level PM2.5 concentrations at 1 km spatial
resolution in Japan during 2011–2016. (Jung et al., 2021, open access.)
Table 8.10 compares the strengths and limitations of three types of methods in retrieving PM2.5
from AOT. The empirical-statistical methods are simple and easily transferable to other areas than
chemical transport models (CTM), allowing multiple variables of diverse natures to be considered. If
machine learning is used, it can solve non-linear relationships, but cannot explain how PM2.5 is
affected by the variables. CTMs can fill the gaps of AOT data and provide continuous data, the
vertical profile and aerosol characteristics which are generally limited in the observations and
provide the forecasting data, but they are complex and cannot be easily implemented. They also face
a high level of uncertainty stemming from model scheme, parameterization, and emissions (Li et al.,
2021). More critically, they cannot be used for local-scale estimations. Built on physical mechanism
and being independent of geographical area, the semi-empirical method is able to account for the
impact of integrated aerosol characteristics directly via observational data regression, and is suitable
for operational retrieval, but it cannot gather integrated aerosol characteristics at a fine resolution
over extensive areas. It is also difficult to gain information on the temporal variation of aerosol
properties.
Table 8.10
Comparison of the pros and cons of three types of methods in retrieving PM2.5 from AOD8.5.2
Method Strengths Weaknesses
Chemical
transport model
• Able to fill the gaps of AOT data
and provide continuous data;
• Able to yield vertical profile and
aerosol characteristics information
which are generally limited in the
observations;
• Able to provide the forecasting
data.
• Relatively high cost and not as
easy as other methods to
implement;
• High uncertainties due to the
model schemes, parameterization,
emission and others;
• Difficult to apply to local scales.
Semi-empirical
• Founded on physical mechanism,
not dependent on geographical data;
• Able to include integrated aerosol
characteristics directly;
• Low computational cost and robust
with equivalent accuracy to other
methods.
• Not easy to obtain the integrated
aerosol characteristics at a high
resolution and a large scale;
• Hard to derive temporal variation
of aerosol properties.
Empirical￾statistical
(machine
learning)
• Purely empirical without
knowledge of complex chemical and
physical interactions;
• Simply and easily transferrable to
other areas than CTM-based
methods;
• Able to handle all kinds of
meteorological data and geographic
information;
• Able to solve nonlinear
relationship.
• Not clearly reflective of the
physical mechanism between input
variables and PM2.5;
• Possible over fitting samples used
for model training;
• Able to obtain only the spatial
distribution by interpolation based
on the spared ground measurement.
Source: Modified from Li et al. (2021), with permission (5761270655106) from Springer.
Dust Intensity
Dusts in the atmosphere are sourced from deserts of an arid climate. Each spring dust storms are a
frequent occurrence in northwest China where the Talakma desert is located. The physical parameter
of dust that can be remotely estimated is dust intensity or thickness, which is indirectly related to
dust volume. So far dust intensity has been quantified using two indices, the enhanced dust index
(EDI) and the enhanced dust intensity index (EDII). Based on the apparent reflectance in the VIS
(0.65 μm) and SWIR (1.63 μm) bands, EDI is empirically derived from the apparent TOP reflectance
(ρ) of these two bands, in conjunction with BT derived from the MIR and TIR bands, and the
retrieved AOT as:
(8.52)
where a, b, and c = coefficients for scaling the three components to an equivalent range of 0−1. They
are commonly set as a = c = 0.1, and b = 10. EDI is simply calculated as the weighted sum of the8.6
three components, and its meaning is not precise. A critical EDI value is 1. Any value above it
indicates the presence of dust, and below it means non-dust areas. EDI is estimated in three steps: (i)
detection of cloud and snow pixels if their ρVIS − ρSWIR is <0; (ii) detection of dust pixels using Eq.
8.52; and (iii) testing of homogeneity within a window for EDI to eliminate incorrect pixels (e.g.,
clouds). The calculated EDI is positively correlated with dust intensity. It bears a close linear
correlation (R2 = 0.78, p < 0.01) with in situ measured visibility, which demonstrates its potential in
detecting dust events and delineating their extent. EDI is a rather competent and accurate method in
estimating dust intensity if the dust can be assumed to be spatially homogeneous (Di et al., 2016).
However, this index is developed and tested using INSAT images only. Whether the conclusion is
still valid with other satellite images remains unknown due to variability in the wavelengths of
spectral band, with the exception of the Advanced Himawari Imager (AHI) data. Adapted for these
data, EDI is turned to the EDII by She et al. (2018). It is based on a dynamic threshold of BT
difference (BTD) determined from three BTDs (BTD11–12, BTD3–11, and BTD11–8) calculated from
four AHI bands (Eq. 8.53). This equation is grounded on three facts: (i) Dust has the minimum and
maximum reflectance at 0.47 and 1.61 μm, respectively. Thus, the normalized difference of these
two AHI bands, , can indicate dust intensity; (ii) Dust has an elevated BT at 3.9 μm but a subdued
BT at 11 μm. The difference in BT between these two bands is positively related to dust intensity;
and (iii) AOD is also positively related to dust intensity, so is also essential to EDII calculation:
(8.53)
where a, b, and c have a value of 1, 10, and 0.1, respectively. Judged by the close correlation
(coefficient of 0.81) of EDII with visibility, it can be considered as a semi-quantitative measure of
dust intensity. EDII-detected results have an agreement of 84% with ground measurements (Figure
8.6). This index can be calculated quickly, and is suitable for monitoring dust intensity from
geostationary satellite data in real-time (She et al., 2018).
FIGURE 8.6 Comparison of EDII value of a dust storm observed on May 3, 2017 in North China (h)
with a color composite image of the same area (g). (She et al., 2018, open access.)
GASEOUS COMPONENTS8.6.1
Although the absolute majority (⁓98%) of the atmosphere is made up of N2 and O2, it is the minor
trace gases that do havoc to the environment and the climate. They can cause climate warming and
acid rain, and deplete the atmospheric absorption of harmful radiation to humans. It is important to
retrieve their concentration and predict their movement in the atmosphere in order to devise proper
response measures. Atmospheric trace gases are widely studied from remotely sensed UV-VIS
absorption spectroscopy based on extraterrestrial light sources, particularly remote sensing of
stratospheric species. Apart from requiring special satellite data, this retrieval is also complex and
needs models, and the vertical profiles of trace gases. Admittedly, some trace gases are confined to
the troposphere and even the stratosphere. In spite of this, they are also included in the discussion as
they impact the biosphere and human life just as if they were residing in the atmosphere. The
quantified gaseous components to be covered in this section include ozone, carbon monoxide,
carbon dioxide, nitrogen dioxide, sulphur dioxide, and methane.
Ozone
Ozone (O3) is a pale blue inorganic gas. It is unstable, and easily decomposed to O2 in the lower
atmosphere by UV radiation and electrical discharges. Thus, it is constantly formed and destroyed in
the atmosphere. At any point its concentration reflects the dynamic equilibrium between these two
processes. O3 concentration is low in the atmosphere but much higher in the ozone layer above it.
The majority of the earthly load of ozone resides in the stratosphere where its abundance is
controlled by a variety of catalytic chemical cycles. Although the ozone layer is located distantly
from the biosphere where humans inhabit, it is still important to monitor and quantify this powerful
oxidant because ozone in the stratosphere absorbs most of the solar UV radiation. Owing to its
powerful oxidizing potential, ozone is considered a greenhouse gas and a pollutant detrimental to
public health, damaging mucous and respiratory tissues in humans. A high concentration of ozone
near the ground level is a potent respiratory hazard. However, a higher concentration in the ozone
layer is beneficial, as it absorbs more baleful solar UV radiation and prevents it from reaching the
Earth’s surface to harm humans.
Ozone vertical profiles can be retrieved from the inversion of the UV-VIS spectra using an
iterative optimal estimation scheme or the DOAS algorithm described in Section 8.2.2 with some
approximations. On its way to the Earth’s surface through the atmosphere, solar irradiance I0(λ) is
modulated by the absorption and scattering of molecules and aerosols. The measured SCD τs(λ) can
be derived from the measured earthshine radiance I(λ) and the solar irradiance I0(λ) captured by a
spaceborne sensor, and the absorption cross-sections σi(λ) of all relevant species approximately as:
(8.54)
where σi(λ) = differential absorption cross-section of the ith gas species at wavelength λ; and SCD
(molecules×cm-2) = integrated ozone density along the slant optical path, which is mainly defined by
the solar zenith angle and viewing geometry (LOS) of the sensor. The last term in Eq. 8.54 depicts
the broadband modulation of the solar irradiance by Rayleigh and Mie scattering by molecules and
aerosol particles, and the slowly varying component of the molecular absorption. They are
commonly fitted with a low-order polynomial with cj being the coefficients, and are subtracted from
the measured optical depth. If properly selected, a spectral fitting window enables SCDi to bepresumably independent of wavelength, such as 325–335 nm (e.g., GOME channel 2), so the
retrieval of SCD evolves into the minimization of:
(8.55)
This equation is solved via the least square fitting with the fit parameters being SCDi (and SCDRing)
and the regression coefficients cj. After SCDi has been retrieved, it can be converted to VCD by
dividing it by con (–viewing angle). This retrieval algorithm is underpinned by the assumption that
the differential absorption cross-section in Eq. 8.54 is independent of altitude (temperature). Even
though the cross-sections of ozone and nitrogen are temperature-dependent, the effective cross￾section can be sufficiently determined using the temperature at the number density maximum of the
climatological profile. Within the properly selected spectral fitting window, the wavelength
dependence of the AMF is so small that it can be safely ignored for weak absorbers. However, ozone
in the UV spectral range of 325–335 nm cannot be considered a weak absorber. Thus, more accurate
ozone retrieval requires special treatment of the AMF whose spectrum has a rather significant
wavelength dependence, so it has to be calculated from an RTM that requires trace gas profiles in the
input. They can be taken from trace gas climatologies.
Another modification is to replace with , the variation of with elevation z. The vertical optical
density is the vertically integrated climatological number density profile ρi(z) defined as:
(8.56)
where σi(z, λ) = absorption cross-section as a function of altitude z. The AMFs are determined for
the central wavelength of the spectral fitting window and are averaged for three LOS angles
(minimum, center, and maximum) for each ground pixel in the satellite image. Missing from Eq.
8.56 is the Ring effect that must be accounted for in the slant column retrieval from UV-VIS spectra.
The Ring reference spectrum can be measured using the cross-polarizer method, or calculated by
radiative transfer during pre-flight calibration. However, in order to improve the relative spectral
alignment of the radiance and irradiance as well as the molecular and Ring reference spectra, small
shifts and squeezes are performed for each spectrum in addition to the slant column fits (Burrow et
al., 1999).
Global ozone can be estimated from remote sensing data from several tailored sensors, including
EPIC, TOMS, GOME-2, and OMI (Figure 8.7). Total ozone products from GOME are based on the
Solar Backscattered UV (SBUV)/2 version 8 algorithm at accuracy on the order of 1%. Additional
uncertainty for the vertical column density stems from the AMF. When it is calculated from an a
priori profile, it is difficult to assess the accuracy that is conservatively estimated to be 5% for SZA
<70° (Burrows et al., 1999). The GOME Data Processor air mass calculation can be improved by
selecting a more representative wavelength (325 nm) in the ozone spectral window (325–335 nm)
(Table 8.11) and by extending the multiple scattering correction up to an SZA of 90°. GOME￾retrieved ozone has an agreement >64% with the ground measurements at mid-European stations.FIGURE 8.7 Global O3 distribution on December 3, 2015, retrieved from EPIC bands B1, B3, and B4
measurements. (a) Optimized (i.e., α = β = 0.5) O3 map based on the spatial optimal estimation
method; (b) Comparison of optimized (orange) and independent-pixel (blue, α = 0.0; β = 1.0) O3
along the horizontal line (left to right) across the middle of the O3 map in (a). (Huang and Yang, 2022,
open access.)
Table 8.11
Fitting windows used by DOAS to retrieve six trace gases from GOME data
Trace
gas
Fitting window
(nm) Other fit parameters Sphere/Observations8.6.2
Trace
gas
Fitting window
(nm) Other fit parameters Sphere/Observations
Level 2 products
O3 325–335 Ring, cloud correction T: smog; S: global
NO2 425–450
O3, Ring, cloud correction,
smoothing
T: combustion, biomass burning;
S: global
Research products
OCIO 357–381 NO2, O4, Ring S: twilight, polar vortex
BrO 345–359 O3, NO2, O4, Ring T: local, S: polar vortex
SO2 314–327 O3, Ring T: volcano
H2CO 337–356 O3, NO2, Ring T: biomass burning
Source: Burrow et al. (1999). © American Meteorological Society. Used with permission.
Note: *: Ring = Ring reference spectra; T = troposphere; S = stratosphere.
If retrieved from EPIC data, the radiance of their shorter UV bands 1 (317.5 nm) and 2 (325.0 nm) is
sensitive to both O3 and SO2 absorption and hence allows the retrieval of their total vertical column
concentrations. In the retrieval, the reflectance spectra of the underlying surface is obtained from the
radiance measured in bands 3 (340.0 nm) and 4 (388.0 nm). These bands are not overly sensitive to
O3 and SO2 absorption, and the presence of O3 and SO2 in the atmosphere hardly affects the
radiance measurements of these two bands irrespective of their concentration levels. The retrieval
still relies on the same forward model, expressed as:
(8.57)
where = column vector, calculated for each band as the difference between the radiance
measurement IM and the forward modeled ITOA [; = state adjustment vector ( and K = Jacobian
matrix of m×n ( (m = number of spectral band; n = number of measurements). The state adjustment
vector is solved iteratively as . The forward model is inverted to search for the state vector x that
satisfies a set of m simultaneous equations, ∆yi = 0 (i = 1, 2, …, m); λi = wavelength that
characterizes the ith (1≤ i ≤ m) spectral band, and ∆yi = residual of this band (Huang and Yang,
2022).
Carbon Dioxide
Carbon dioxide (CO2) is the most abundant greenhouse gas originating from human activities in the
atmosphere. Industrialization and urbanization since last century have considerably uplifted the
atmospheric CO2 content. Natural sources of CO2 include volcanism, leakage from hydrocarbon
reservoirs, and spontaneous combustion of coal. Elevated CO2 content in the atmosphere is
reportedly responsible for causing global climate warming, and the ever-frequent occurrence of
extreme and destructive weather events. Accurate information on the total amount of atmospheric
CO₂ and its global distribution plays a decisive role in predicting climate change and devising
informed mitigation measures. CO2 concentration is tricky to retrieve accurately because it has a
large and well-mixed atmospheric background. Remote sensing retrieval of volcanic CO2 verticalcolumn densities enhancements (∆CO2) requires compensating for the changing altitude of the
sensor and background concentration variability. Thus, it is necessary to simultaneously measure the
overhead oxygen columns and assume co-variation of O2 and CO2 with altitude. The atmospheric
CO2 background can be determined by identifying background soundings via other co-emitted
volcanic gases.
Atmospheric CO2 in the path of sunlight reflected by the surface is retrieved from passive
spaceborne remote sensing data and active airborne LiDAR data using drastically contrasting
methods. The former detects NIR radiation of CO2. In contrast to the TIR observations, NIR
measurements have a nearly uniform sensitivity to CO2 from the surface all the way up to the middle
troposphere. CO2 retrieval can be accomplished using three types of sensors: passive IR
hyperspectral sounders for profiling (e.g., AIRS, Infrared Atmospheric Sounding Interferometer or
IASI), spectrometer-based retrieval of concentration, and imaging sensors for measuring
composition. IASI onboard the polar sun-synchronous MetOP satellite is a nadir-viewing instrument
sensing IR emission spectra over the spectral range of 645–2,760 cm−1 (15.5–3.62 μm). It has 8,461
spectral samples that are aligned in three bands within the spectral range at a spectral resolution of
0.5 cm−1. Their spatial resolution varies from 12 km at nadir to up to 40 km at the edge of the 2,200
km swath. Although the IASI sensor is designed primarily to acquire near real-time data on
atmospheric temperature and water vapor to assist weather forecasting, the captured spectra also
allow the retrieval of the concentrations of various trace gases, one of which is CO2.
Mean atmospheric CO2 concentrations can also be retrieved using spectrometers carried onboard
Orbiting Carbon Observatory (OCO)-2/3, GOSAT-2, TanSat, and GOSAT. OCO-2/3 and GOSAT
detect CO2 optical depth with two bands centered around 2.0 μm and 1.6 μm, respectively. The
spectrometer carried by the OCO-2 satellite launched by NASA in 2014 measures CO₂ concentration
in the lower atmosphere and the atmospheric composition on the regional scale. The referenced
constellation consists of six satellites orbiting the Earth at an altitude of 705 km. One of its payloads
is a hyperspectral imaging sounder. It measures the NIR spectra of sunlight reflected off the Earth’s
surface in three spectral regions centered at 0.765 μm (band 1), 1.61 μm (band 2), and 2.06 μm
(band 3). The 2.06 μm strong absorption band is intended to eliminate the impact of clouds and
aerosols. The 1.61 μm band is a CO2 absorption band, and the 0.765 μm oxygen A band is designed
to calibrate the results (Gao, 2022). They have been routinely supplying around 1 million soundings
daily at a nadir spatial resolution of roughly 1.3 km × 2.25 km. Due to the improvement in spatial
resolution, the revisit period has been expanded from 72 tracks/3d to 233 tracks/16d. OCO-3 is an
improved instrument onboard the International Space Station for enhanced coverage of CO2
measurements with improved accuracy and resolution.
OCO-2 datasets are converted to consistent and reliable level-2 CO2 data products using the
physical-based Atmospheric CO2 Observations from Space (ACOS) retrieval algorithm. It is based
on a forward model (Eq. 8.58) with the input parameters optimized to yield simulated spectra that
best match the observed spectra under simultaneous constraints by prior knowledge (O’Dell et al.,
2011). The spectra to match are the three NIR bands:
(8.58)
Where y = CO2 concentration to be retrieved; F = forward model for retrieving y; x = state vector
parameters to be retrieved; b = input parameters affecting the measurement that may include gaseousabsorption coefficients, solar and viewing geometry, and atmospheric state; and ε = term related to
instrument noise and inaccuracy of the forward model. F is a non-linear model solved iteratively.
The core of the ACOS algorithm is the retrieval of the column-averaged CO2 dry-air mole fraction
(XCO2), during which an a priori state xa is constructed based on meteorological inputs and the
observed spectra. The first guess state vector is taken to be the prior for simplicity. An inverse
model, coupled with the forward model F(x), solves the state vectorˆthat minimizes the cost
function:
(8.59)
where Sa = priori covariance matrix, T = matrix transpose; and Se = observation (or instrument)
inaccuracy covariance matrix, taken to be diagonal elements in the ACOS retrieval algorithm for
simplicity.
The cost function is minimized using the modified Levenberg-Marquardt method and solved via
model inversion iteratively until a convergence criterion has been reached, during which the state
variable is updated:
(8.60)
where = forward model at xi; , corresponding Jacobian matrix; g = Levenberg-Marquardt parameter.
The CO2 concentration is retrieved at discrete levels, not column integrated, so XCO2 has to be
calculated by considering the CO2 profile as:
(8.61)
where = retrieved profile of CO2 concentration at fixed pressure levels, and h = pressure weighting
function. As with the retrieval of other trace gases, the inversion of Eq. 8.61 requires the
consideration of surface albedo using the surface model, atmospheric scattering using the
atmospheric model, and the instrument model. The instrument model comprises two models: one to
compute the measured radiance for each spectral band, and another to handle the noise. The entire
procedure of retrieval is illustrated in Figure 8.8.FIGURE 8.8 Flowchart illustrating the primary steps in the end-to-end ACOS level-2 algorithm.
(Modified from O’Dell et al., 2012, open access.)
The results retrieved from the three bands is merged to derive XCO2. Its quality is vulnerable to the
impacts of aerosols and clouds, and many of the results are unreliable in extreme cases. The poorest
retrievals, which arise primarily from imperfect cloud screening, must be eliminated via post￾retrieval filtering (O’Dell et al., 2011). The accuracy of the retrieved CO2 varies with the satellite
data and the retrieval algorithm used (Table 8.12). The inaccuracy of the XCO2 products retrieved
from OCO-2 data decreases from 4 ppm using the initial algorithm to 1–2 ppm with the improved
algorithm. Inaccuracy stems mostly from the forward model error (ε) that contributes about three
quarters of the error variance in the ACOS algorithm. After continuous refinement, the GOSAT
algorithm reaches the accuracy of 1.47 following the correction of the deviation from the observed
values at the Total Carbon Column Observing Network global stations (Table 8.12).
Table 8.12
Properties of recently launched satellites and sensors for (mostly) CO2 estimation and their product accuracy
Satellite/
sensor
Launch
time
Substellar
point
resolution
Orbital
altitude
(km)
Type of
spectrometer Target gases
Product
accuracy
/ppm
AQUA/AIRS 2002.05 15 km (d) 705 Grating
CO2, CH4, O3,
CO, H2O, SO2 1.5Satellite/
sensor
Launch
time
Substellar
point
resolution
Orbital
altitude
(km)
Type of
spectrometer Target gases
Product
accuracy
/ppm
METOP￾A/IASI 2006.10 12 km (d) 820
Fourier
transform
(FTS)
CO2, CH4, O3,
CO, H2O,
SO2, N2O 2
GOSAT 2009.1
10.5
km(d) 666
TANSO￾FTS and -
CAI
CO2, CH4,
O3,H2O 4
OCO-2 2014.7
1.29x225
km² 705
Three￾channel
grating CO2 1-2
TanSat 2016.12 2 x 2 km² 708
ACGS,
CAPI CO2 4
FY-3D 2017.11 10 km (d) 836 GAS, FTS
CO2, CH4,
SO2, N2O,
atmospheric
aerosol --
GOSAT-2 2018.1 9.7 km(d) 613
TANSO￾FTS 2 and
-CAI 2 CO2 0.5
GF-5 2018.5
10.3
km(d) 705 GMI
CO2, CH4, O3,
O2, H2O, CO,
black carbon,
PM2.5 2.72
OCO-3 2019.5 4 km² 400
3-channel
grating CO2 1-2
Source: Gao (2022), open access.
Alternatively, atmospheric CO2 may be quantified using active LiDAR such as the differential
absorption LiDAR (DIAL) that can potentially overcome the limitations of passive sensing.
Airborne range-resolved DIAL can detect CO2 vertical profile independent of geographical location
and time with straightforward retrieval algorithms. As a special type of DIAL, the integrated path
differential absorption (IPDA) LiDAR measures weight-averaged CO2 content over a sampling
volume rather than range-resolved profile (Refaat et al., 2021). LiDAR returns from firm targets
have a high SNR, resulting in high precision and highly accurate estimates. The IPDA LiDAR
targets the CO2 R30 absorption line using a high-energy laser sensor that transmits pulsed or
continuous wave laser emissions of 1.6 or 2.0 μm in wavelength. Both wavelengths coincide with
the optimum sensing absorption features for CO2, with additional ranging capability for pulsed
techniques.
CO2 retrieval from IPDA operating in the double-pulse mode takes advantage of the double-path
optical depth (). It is derived from the total measured double-path differential optical depth () minusthe effects of other interfering atmospheric molecules. In the vicinity of R30, the dominant
interfering molecule is water vapor. Thus:
(8.62)
where = water vapor double-path differential optical depth; RA= aircraft altitude; RG = ground
elevation; λon and λoff = nadir-pointing airborne IPDA operating at the on- and offline wavelength,
respectively:
(8.63)
where t = width of returned laser pulses; E = transmitted energy for on- and offline wavelengths
seeding pulses 1 and 2, respectively. P = measured return power. CO2 density (Ncd) is retrieved
according to the following equation:
(8.64)
where CL = range correction factor arising from IPDA airborne attitude, by which IPDA LOS range
(RL) deviates from a perfect nadir-pointing system along the aircraft nadir line on ground (CL =
1/cosθ, = angle between LOS and its ground projection); ∆σcd = CO2 differential absorption
coefficient, obtained at the sensing wavelengths and the corresponding temperature and pressure at
range r (Eq. 8.65). The integration is performed with respect to the range along the aircraft ground
projection line twice, one for CO2, and another for water vapor. To derive the IPDA model to
retrieve XCO2, the necessary time-series meteorological and geographic data must be converted to
vertical profiles by averaging the data within a range (e.g., 200 m bin), followed by linear
interpolation to bring down the resolution to 1 m (Refaat et al., 2021). ∆τcd and ∆τwv are calculated
in relation to LiDAR range with the assistance of meteorological and geographical data and an
atmospheric model (e.g., the mid-latitude summer model):
(8.65)
where = CO2 dry-air volume mixing ratio. CO2 is a well-mixed atmospheric gas with an almost
constant vertical profile, so XCO2 (Xcd) can be expressed as:
(8.66)
where
(8.67)
where Ndry = dry-air number density, obtained by subtracting H2O number density (Nwv) from total
air number density calculated using Ideal Gas law as:
(8.68)
IPDA-retrieved XCO2 concentration of 404.43 ± 1.23 ppm closely resembles model-predicted
405.49 ± 0.01 ppm, obtained using consistent reflectivity and steady elevation oceanic surface8.6.3
targets (Refaat et al., 2021). This translates to a relative accuracy and precision of 0.26% and 0.30%,
respectively. The retrieved XCO2 is affected by both the ranging inaccuracy and measurement
inaccuracy. Error budgeting for XCO2 retrieval reveals a lower random error for thick sensing
columns. The accuracy and precision change to 0.44 ppm and 2.24 ppm, respectively, for CO2
weight-averaged column dry-air volume mixing (XCO2) measurement. Systematic errors are
dominated by water vapor through dry-air number density derivation, followed by H2O interference
and ranging-related uncertainties (IPDA ranging precision = 0.93).
Carbon Monoxide
Carbon monoxide (CO) is a colorless, odorless, and tasteless atmospheric trace gas. At a high
dosage, it can disrupt the transport of oxygen by hemoglobin in the red blood cells, and potentially
cause severe health havoc. During its life span of about one to two months in the atmosphere, CO
serves as a tracer for the long-distance dispersal of other polluting gases. It plays an instrumental
role in tropospheric chemistry by acting as a precursor to tropospheric ozone. CO concentration is
usually retrieved from spaceborne data. Vertically integrated column densities of CO are estimated
from satellite measurements of Earth-reflected solar radiation in SWIR bands, particularly the 2.3
μm wavelength. SWIR radiance is sensitive to the vertically integrated quantity of CO and
particularly suitable for detecting surface sources of CO from space. Under clear sky conditions, this
spectral range is scarcely affected by atmospheric scattering, and most of the detected sunlight is
thus reflected off the Earth’s surface. The 2.3 μm band is available in several sensors, including
Copernicus Sentinel 5 Precursor TROPOMI, SCIAMACHY, and MOPITT (Measurements of
Pollution in the Troposphere) onboard the Terra satellite launched in 1999 by NASA. Sentinel 5P
ensures the continuity of measuring atmospheric CO quantity from the SWIR band in addition to
TIR observations in the 4.7 μm CO fundamental band (Landgraf et al., 2016).
The complexity of CO retrieval algorithms varies with the spectral band designation, and hence
the satellite data. The TROPOMI CO retrieval algorithm makes use of the wavelength range of
2311–2315.5 nm due to the absence of the spectral band around 1600 nm, and is known as
Weighting Function Modified DOAS or WFM-DOAS (Schneising et al., 2019). It infers the total
column concentration of CO from SWIR measurements, focusing on clear-sky observations over
land and cloudy observations over land and ocean in the presence of low-altitude watery clouds.
This algorithm minimizes the disparity between the observed and the modeled spectrum by scaling
weighting functions for different trace gas profiles such as CH4 and CO2, shifting the temperature
profile and fitting a low-order polynomial for broadband absorption (e.g., at the surface) or
scattering by atmospheric molecules and aerosols. The weighting functions portray a linear
relationship between the change in observed radiance and the change in the atmospheric parameters.
It is a linear least-squares method by scaling (or shifting) pre-selected vertical profiles of the
atmosphere.
The vertical column concentrations of CO are determined from sensor-measured sun-normalized
radiance by fitting a linearized RTM to it. The inversion can be expedited using a LUT for the
radiance and the derivatives. This table encompasses thousands of reference spectra under all
varying solar zenith angle, altitude, albedo, water vapor, and temperature conditions. These
reference spectra are computed at a high spectral resolution and convoluted to the spectral resolution
of TROPOMI SWIR bands. The retrieval algorithm requires several auxiliary inputs, such asatmospheric profiles, aerosol model, and spectroscopic parameters. The linearized RTM
appropriately chosen from the LUT according to the relevant parameters and a low-order polynomial
are linearly least-squares fitted to the logarithm of the measured sun-normalized radiance. The trace
gas vertical profiles of CO (plus CH4 and H2O) are scaled for the fit (i.e. the profile shape is not
varied). Additional fitting parameters include the shift in the pre-selected temperature profile, the
scaling factor for the pressure profile, and parameters for a second-order polynomial. The radiance at
wavelength λ is modeled as:
(8.69)
where m = number of spectral points in the fitting window; n = number of state vector (v) elements
(fitting parameters) with m >> n; --> = linearization point; α = coefficients of second-order
polynomial P. A derivative with respect to a vertical column refers to the change in the TOA radiance
caused by scaling the vertical profile of the pre-selected absorber concentration. Eq. 8.69 may be
repeated m times, each time for a pixel in the fitting window. All of them are resolved via least
square fitting, namely, the linear model best fitting the observed radiance.
The operational retrieval of CO from Sentinel 5P TROPOMI SWIR measurements is ideally
achieved using a baseline algorithm, such as the physical-based approach to infer CO columns from
SWIR measurements, together with atmospheric H2O abundance, surface albedo, and a spectral
calibration of the measured spectrum (Landgraf et al., 2016). In this approach, a forward model F
(see Eq. 8.58) depicts the measurements as a function of the atmospheric state (including scattering).
SWIR spectral measurements between 2324 and 2338 nm (y) are determined iteratively. At each
iteration step, the least square solution is given by:
(8.70)
where describes the Euclidean norm. The state vector x contains the CO profile xCO, expressed in
relative terms to a reference profile ρref, or:
(8.71)
The reference profile can be extracted from spectral band model fields. Apart from CO, the state
vector may also encompass water vapor column density, surface albedo, effective cloud center
height, and cloud optical depth. They must be taken into account by modifying Eq. 8.70 as:
(8.72)
where g = regularization parameter; W = diagonal weighting matrix, of which diagonal elements are
1 for all elements of the state vector related to the scattering layer and surface albedo, and zero
otherwise (Landgraf et al., 2016). This equation is solved via iteration based on the degree of
convergence, defined as the disparity in the reduced c2 between two consecutive iterations, and
convergence is achieved when (user defined, e.g., 0.5).
The actual retrieval is implemented in two steps: (i) retrieval of non-scattering of the total amount
of CH4 in the spectral range of 2315–2324 nm for cloud filtering. The spectral absorption by CH4 is
used to infer atmospheric scattering by clouds and aerosols; (ii) retrieval of CO total column, thecorresponding column averaging kernel, together with the estimated random error. For clear sky
atmospheres and within the bounds of the measurement error, only column-integrated CO can be
retrieved from image-captured radiance, but not its vertical distribution. To count for the effects of
clouds, regularized CO profile retrieval is required to take into account the differential sensitivity of
the measurements to CO at different altitudes.
CO data products have been generated from MOPITT data using simultaneous TIR and NIR
products with a mixing ratio of 1000 hPa. Gridded level-3 CO data products are aggregated monthly
(version 8). This product offers the finest vertical resolution and considerable sensitivity to CO in
the lower troposphere. Other products are generated from IASI, OMI, TROPOMI, and GOME-2
satellite data. The IASI CO algorithm uses the nadir spectral radiance in the spectral range of 2143–
2181.25 cm−1 to retrieve the CO profile formed in 19 fixed layers up to 60 km, corresponding to 18
layers from 0 to 18 km of 1 km width, and a top layer between 18 and 60 km. The IASI CO product
has an error <10% and <21% in the upper troposphere and the lower troposphere, respectively
(Rawat and Naja, 2022). The retrieved XCO is characterized by a random error of 5.1 ppb (5.8%)
and a systematic error of 1.9 ppb (2.1%) (Schneising et al., 2019). The results retrieved from the
satellite data well-capture natural XCO variations, as backed up by the close correlation with the
validation data (r = 0.97) based on daily averages (Figure 8.9).
FIGURE 8.9 Spatial distribution of global annual atmospheric column-averaged dry-air mole fractions
CO in 2018, retrieved from TROPOMI data using the Weighting Function Modified DOAS
algorithm. (Schneising et al., 2019, open access.)8.6.4 Nitrogen Dioxide
Nitrogen dioxide (NO2) is an atmospheric trace gas that critically influences air quality, climate
change, ecological conditions, and human health both regionally and globally. This atmospheric
pollutant is associated with adverse health issues. It can lead to severe environmental pollution, such
as photochemical smog. Moreover, NO2 plays an influential role in the formation of acid rain and
disorders greenhouse gas and ozone levels that can affect the global climate. NO2 can be retrieved
from the satellite data presented in Section 8.3 using the well-established DOAS algorithm in a
manner resembling O3 retrieval as explained in Section 8.6.1. The best wavelengths for NO2
retrieval from TROPOMI visible band lie within 400–496 nm, with spectral resolution and sampling
being 0.54 and 0.20 nm, respectively, and an SNR around 1500. The visible channel (349–504 nm)
of OMI can also be used. It has an SNR of around 500, and a spectral resolution and sampling
interval of 0.63 and 0.21 nm, respectively.
As a weak absorber, NO2 has been typically retrieved from observed earthshine spectra from
GOME, SCIAMACHY (Bovensmann et al., 1999), OMI (Levelt et al., 2006) aboard Aura, GOME-2
aboard MetOp-A, -B, and -C, and more recently TROPOMI data using DOAS. Depending on the
data used, DOAS has been modified to discrete-wavelength DOAS (DW-DOAS) and Dutch OMI
DOAS (DOMINO). Discretizing the spectra means that wavelength calibration can no longer be
performed using the Fraunhofer lines of the solar reference spectrum (Villena et al., 2020). Instead,
the equation is resolved at discrete wavelengths usually numbered < 10.
In DW-DOAS, SCD of trace gases is linearly estimated as:
(8.73)
where = absorption cross-section of the ith species fitted, including the Ring spectrum, as a pseudo
absorber; P() = low-order polynomial; I = Earth radiance; and = solar irradiance. The order of the
polynomial must not exceed 2, as fitting higher-order polynomials results in erroneously low SCDs
(Villena et al., 2020). This equation needs to be resolved for each wavelength used, and can be
expressed linearly as:
(8.74)
where A = an M × N matrix containing the absorption cross-sections and the polynomial basis for
each wavelength; x = a column vector of N elements containing the SCDs of the absorbers (SCDi)
and the polynomial coefficients (a, b, c); and B = column vector of M elements containing the
optical density for each wavelength. The limit of not having a higher polynomial order can be
overcome by having a fitting window narrow enough so that the broadband component can be
approximated by a second-order polynomial as:
(8.75)
The solution to x needs to calculate the pseudo inverse of matrix A, or A-1, using the singular value
decomposition numerical method to factorize A:
(8.76)(8.77)
where U = column orthogonal matrix of M × N in size; W = diagonal matrix of the same size with
non-negative real numbers in the diagonal (singular values); V = an N × N orthogonal matrix.
The general procedure of retrieving NO2 SCD using the DW-DOAS method is illustrated in Figure
8.10. The three critical steps in the OMINO retrieval algorithm are identified as: (i) estimation of
NO2 SCDs from the OMI reflectance spectra using the DOAS algorithm; (ii) separation of the
stratospheric and tropospheric contribution to the SCDs, and (iii) conversion of the tropospheric
SCD to a vertical column with the assistance of the tropospheric AMF. The uncertainty in individual
retrievals due to spectral fitting can reach up to 0.7 × 1015 molecules × cm−2 and dominates the
overall retrieval error over the oceans and remote areas. In step 2, the stratospheric slant column
NO2 is estimated by assimilating OMI total slant columns in the CTM. The DOMINO-retrieved
mean (annual) stratospheric NO2 columns differ from independent, ground-based observations by <
0.3 × 1015 molecules × cm−2. In step 3, AMF is calculated as the linear sum of atmospheric layer
contributions to the slant column divided by the vertical column (Eq. 8.78) because the slant optical
thickness of NO2 is generally <0.005:
FIGURE 8.10 Flow chart of retrieving SCD of NO2 using the DW-DOAS method. (Modified from
Villena et al., 2020, open access.)
(8.78)
where l = NO2 layer; = layer specific sub-columns from the a priori profile xa for atmospheric layer
l; ml = altitude-dependent AMFs that describe vertically resolved sensitivity to NO2. Altitude￾dependent AMFs are calculated by adding a finite amount of NO2 to layer l and subsequently
dividing the RTM-simulated NO2 slant column by the vertical column added to that layer (). In thisway, AMF is treated as altitude-dependent and stored in a LUT as a function of forward model
parameters , together with the satellite viewing geometry, surface pressure, and albedo (Boersma et
al., 2011). DOMINO is able to handle cloud-contaminated pixels by expressing the AMF as a linear
function of a clear-sky AMF and a cloudy AMF using the independent-pixel approximation, or:
(8.79)
where w = fraction of cloud radiance that depends on the effective cloud fraction ( with Icld being the
radiance from the cloud part of the pixel, and R the total scene radiance). The effective cloud
fraction and cloud pressure are retrieved using other methods; Mcld and Mclr = cloudy-sky and clear￾sky AMFs, respectively. Mclr depends on the assumed albedo of 0.8 for the Lambertian reflector, but
is independent of the assumed surface albedo. The estimation can be improved by better depicting
the radiative transfer for the lowest atmospheric layers, surface albedo, terrain height, clouds, and a
priori NO2 profiles. Interpolation errors are reduced if the altitude-dependent AMF in a LUT is
based on a realistic atmospheric profile, and with the adoption of a larger number of reference
vertical layers and surface albedo. The interpolation error may be further mended using a new LUT
in the computation of the altitude-dependent AMFs for the lowest atmospheric layer. This LUT
singly increases AMFs by 20–30%, thereby reducing the retrieved column NO2 by up to 20%.
Table 8.13
Comparison of major settings in retrieving NO2 from OMI and TROPOMI reference products using DW-DOAS
Settings
OMI QA4ECV (v 1.1)
(Boersma et al., 2018)
TROPOMI
(van Geffen et al.,
2020)
DW-DOAS
(Villena et al., 2020)
Fitting window
(nm) 405–465 405–465 425–450
Fitting method Optical depth (linear)
Intensity (non￾linear) Optical depth (linear)
c2 minimization
method Levenberg-Marquardt
Optimal
estimation Not applicable
Level 1B
uncertainty in c2 No Yes Not applicable
Reference
spectrum
Annual mean (2005)
solar reference
Daily solar
reference
Annual mean/daily solar
reference product
Polynomial order 4 5 2
Intensity offset
correction Constant No No
Fitting parameters
O3, NO2, O2-O2,
H2Ovap, Ring, H2Oliq,
Ioff, shift, and stretch
O3, NO2, O2-O2,
H2Ovap, H2Oliq,
shift
O3, NO2, O2-O2,
H2Ovap, Ring, H2Oliq
(only TROPOMI)
Treatment of Ring
effect Pseudo-absorber Non-linear fit Pseudo-absorberSettings
OMI QA4ECV (v 1.1)
(Boersma et al., 2018)
TROPOMI
(van Geffen et al.,
2020)
DW-DOAS
(Villena et al., 2020)
Wavelength
(radiance)
calibration
Alng with fit, 405-465
nm
Before fit, 405-
465 nm No
Temperature (K) 220 221
Same as reference
product (220 or 221)
Source: Villena et al. (2020). Creative Commons.
Tropospheric NO2 products have been generated from OMI, GOME, and TROPOMI data routinely
(available at https://disc.gsfc.nasa.gov/datasets). The major settings in retrieving NO2 from OMI and
TROPOMI reference products using DW-DOAS are compared in Table 8.13. The latest OMI NO2
Standard Product (Version 4.0) is more reliable and consistent with ground and modeled
observations than its predecessors. Validations reveal a generally close agreement of NO2 column
concentration with lower and higher values to within ± 20% of in situ measurements without
consistent seasonal biases (Rawat and Naja, 2022). However, column concentrations are slightly
underestimated in urban regions of a high concentration and somewhat overestimated in remote
areas (Figure 8.11). TROPOMI supplies near real-time products (www.tropomi.eu/featured￾results/archive/202110) for vertical columns of NO2 and other trace gases at a spatial resolution
around 5.5–7.5 km before August 2019 and 3.5–5.5 km afterward.FIGURE 8.11 Global distribution of NO2 column density retrieved from TROPOMI (a) and OMI–
QA4ECV (b) averaged over July 20–26, 2019 on a common grid of 0.8° (long.)´0.4° (lat.). Only
clear-sky ground pixels (i.e. with cloud radiance fraction<0.5) are processed (white: missing data).
(van Geffen et al., 2020, open access.)
Daily (seasonal or annual) ground NO2 concentrations can be effectively obtained via a statistical
model from the regional to global scales using GTWR and machine learning methods, such as RF,
NN, Bayesian maximum entropy, and extreme gradient boosting. Ground NO2 concentrations are
commonly estimated from OMI data by considering both primary factors and co-variates (Figure
8.11). The former includes NO2 related variables (i.e., nitric acid surface mass concentration). The8.6.5
latter embraces meteorological conditions (i.e., surface pressure, 10-m specific humidity, 2-m air
temperature, 10-m eastward wind, 10-m northward wind, PBLH, and evaporation from turbulence).
Another co-variate considered in the modeling is land cover represented by NDVI. GTWR and
machine learning methods produce generally better estimates of NO2 than traditional statistical
models. However, these models assume that the relationship between NO2 and its influencing factors
remains constant within the entire area of study, which degrades the quality of the retrieved
concentration, especially when the area is spatially extensive and heterogeneous (Wang et al., 2021).
Sulphur Dioxide
Sulphur dioxide (SO2) is an important trace gas species and air pollutant abundantly found in most
sulfur-containing compounds. About three quarters of atmospheric SO2 stem from the combustion of
fossil fuels of a high sulfur content such as coals. Volcanic degassing is a major natural contributor
to atmospheric SO2, a crucial constituent of the atmosphere and an influential degrading compound
of air quality and regional climate. Atmospheric SO2 causes environmental problems similar to those
of NO2, including reduced visibility, formation of acid rain and secondary aerosols, acidification of
aquatic ecosystems, and adverse impacts on human health. Background atmospheric SO2
concentration is retrievable from coarse-resolution, generic satellite images on the global scale, and
from a suite of satellite data tailored specifically for sensing SO2 associated with special events such
as volcanic eruptions. Volcanic emissions have a high spatiotemporal variability, and thus require
images of a finer resolution to detect. Satellite sensing is an efficient and cost-effective means of
estimating SO2 emissions from both anthropogenic and natural sources. SO2 is usually quantified
from such images as MODIS, GOME, AIRS, and OMI via absorption spectroscopy because of
SO2’s strong and distinctive absorption signature at both UV and TIR wavelengths. In the UV
spectral region, SO2 exhibits absorption signatures. The wavelength window between 312.5 and
327.6 nm covered by GOME channel 2b is particularly suitable for the retrieval. In this spectral
region, the weak SO2 absorption overlaps with the strong ozone Huggins band (Khokhar et al.,
2005). Thus, it is indispensable to have precise knowledge about the instrumental function depicting
the convolution of the highly resolved incoming signal to the instrument resolution, because small
uncertainties in the spectral structure of strong absorbers can lead to residuals larger than the weak
SO2 absorption itself.
The retrieval of SO2 concentrations also requires spectral observations outside the SO2 absorption
signature in the TIR spectrum, as SO2-free radiance varies approximately linearly with wavelength.
In the presence of SO2, the BT measured at 7.3 μm is lower than the linearly interpolated value
(Thomas et al., 2011). As a result of strong absorption by water vapor at this wavelength, this
retrieval applies only to high altitude (43–45 km) volcanic plumes above the lower troposphere
(4,850 hPa) where the majority of the total atmospheric water vapor is concentrated (Prata et al.
2003). The retrieval is grounded on the relationship between atmospheric transmittance and SO2
absorbance (Eq. 8.80) due to the asymmetric stretch of the SO2 molecules around 7.34 μm:
(8.80)
where I = radiance measured at 7.3 μm; Iα = radiance of the SO2-free atmosphere at 7.3 μm, τs =
SO2 transmittance; and Bs = Planck radiance of cloud at temperature Ts. The temperature difference(∆T) between the measured radiance and the calculated SO2-free radiance (derived from the inverse
Planck function) is related to transmittance as:
(8.81)
where α, β = parameters derived from the cloud altitude. Estimates of the atmospheric state and
cloud temperatures are determined from a prior established LUT (Prata et al. 2003).
If retrieved from MODIS data, the absorption of ground-leaving radiance by SO2 at 8.6 mm
concurs with the wavelength of channel 29, which can be fulfilled via the MODTRAN codes for
MODIS images. The ground-leaving radiance Ls is generally expressed as:
(8.82)
where Ls = at-sensor radiance; Ld = down-welling radiance; Lu = upwelling radiance; εg = ground
emissivity; ta = atmospheric spectral transmittance; and Tg = ground temperature. This model
assumes that isothermal atmospheric layers both emit and absorb the ground-leaving radiance; Lu,
Ld, ta = total emission and absorption of all layers that are treated as a function of wavelength using
a pre-defined atmospheric profile and an estimate of plume altitude in the MODTRAN codes, with
the ground temperature determined by solving Eq. 8.81 for wavelengths outside the SO2 absorption
window (Realmuto et al. 1994). This retrieval algorithm may be functional to detecting both
passively degassed and explosive plumes but its performance will be compromised if the cloud
(plume) has a minimal thermal contrast with the background.
If retrieved from GOME channel 2b data, SO2 SCDs can be estimated from the ratio of
earthshine radiance to solar irradiance. Differential absorption cross-sections are transformed to the
measured sun-normalized Earth radiance spectrum. The optimum slit function is determined via
non-linear fitting of a highly resolved solar spectrum to the solar irradiance measurements of
spectral bands with the least squares. The slit function has an asymmetric Voigt line shape, with the
width and asymmetry being strongly wavelength-dependent. This asymmetric and wavelength￾dependent slit function is used to convolute the highly resolved laboratory spectra of SO2, two ozone
spectra at different temperatures, and a Ring spectrum to generate an optimal set of reference spectra
at instrument resolution (Khokhar et al., 2005). If retrieved from OMI, the differential absorption of
reflected solar radiance between O3 and SO2 at UV wavelengths (300–350 nm) allows SO2
concentrations to be estimated using the currently operational Linear Fit algorithm, even though it
was originally developed for use with the TOMS data (Yang et al., 2007). This algorithm employs up
to 10 wavebands positioned at the minima and maxima of the SO2 absorption cross-section. The
large difference in TOA radiance between adjacent band pairs permits the simultaneous retrieval of
SO2, O3, and surface reflectivity using the forward model.
If retrieved from EPIC data, the best channels to use are the first two UV bands whose radiance is
highly responsive to both O3 and SO2 absorption. They enable the retrieval of total O3 and SO2
vertical columns, provided that the reflectivity of the underlying surface is known (Huang and Yang,
2022). It can be determined from the radiance measurements of the remaining two UV bands. They
contain information on surface reflectance and particle backscattering, but are not so sensitive to O3
and SO2 absorption such that changes in their quantity hardly alter the radiance captured in these
two bands. EPIC bands 1 and 2 have very contrasting sensitivities: in band 1, SO2 is more than twiceas sensitive as O3 in absorbing UV, but significantly less so in band 2 (about 70% as absorbent as
O3). Such huge dissimilarities in absorption sensitivity facilitate the quantification of SO2 in the
atmosphere (Huang and Yang, 2022). Given a radiance SNR of 290:1, in theory, the minimum
detectable level of SO2 enhancement is ~0.5DU in the upper troposphere and above. Low levels of
SO2 cannot be reliably estimated from EPIC data.
At present, there is a paucity of common analytical approaches for SO2 retrieval except the
DOAS method. The trace gases that can be detected using it and the wavelength interval of their
identification are listed in Table 8.14 (Plane and Saiz-Lopez, 2006). Crucial to DOAS fitting analysis
is how to select an appropriate instrumental slit function. Three common fitting windows have been
attempted: 305–317, 307–328, and 312–326 nm. The second window is the best performer under all
weather conditions (e.g., clear, foggy and hazy), achieving the lowest mean RMSE of 0.0008
molecules×cm-2 and a mean fit error of 1.77 × 1015 molecules×cm-2 in clear sky conditions (Zeeshan
et al., 2021).
Table 8.14
Main gases that can be estimated using the DOAS method, wavelength interval of their identification and detection limits at
various path lengths
Species Wavelength range (nm) Detection limits (parts per trillion) Path length (km)
SO2 290–310 17 0.2
CS2 320–340 500 5.0
NO 200–230 240 0.2
NO2 330–500 80 5.0
NO3 600–670 2 5.0
NH3 200–230 800 0.2
HNO2 330–380 40 5.0
O3 300–330 4000 5.0
CH20 300–360 400 0.2
Benzene 250–290 200 2.0
Trimethylbenzene 250–290 600 2.0
Source: Plane and Saiz-Lopez (2006), used with permission (5761271074045) from Wiley.
Satellite images acquired from the aforementioned tailored sensors are limited by their coarse spatial
resolution, even though they are suitable to study extensive SO2-laden volcano plumes. SO2 is
retrieved using the UV-based DOAS spectrometer over the spectral window of 0.312–0.327 μm.
Since spectroscopy requires sunlight to function, this method is operational only during daytime
when the sky is cloud-free. It cannot yield a spatial perspective on the concentration distribution as
the sensors are non-imaging.
The differential range of satellite orbits and image resolutions causes SO2 concentration to be
estimated at variable levels of accuracy. Due to the lack of ground true data, the accuracy of
retrieved SO2 has not been widely assessed or reported. It is generally understood that inversion
inaccuracy arises from six sources (Huang and Yang, 2022): (i) errors of spectral measurements. For
radiance biases within ±1%, the systematic SO2 column errors are within ~±8 Dobson Unit (DU, 1DU = 2.69•1016 molecules.cm-2). In addition, the 290:1 SNR would translate to a random error of
0.345%. The exact value varies with the viewing zenith angle (VZA). At 75°, it is minimum at about
2.2 DU; (ii) model parameter inaccuracy. A bias in SO2 cross-sections causes SO2 column to be
biased between 1 and 2%; (iii) forward modeling errors that are a function of the VZA. At 75°, SO2
vertical column has an error of ~±1 DU; (iv) profile errors. Overestimation (up to 150%) increases
quickly with larger VZAs when the plume is at a higher altitude. Underestimation is more severe
with lower-altitude plumes: 10% to 20% per 1 km lower than the assumed altitude; (v) errors from
Lambertian treatment of natural surfaces. Reflections from surfaces are anisotropic but
approximated as isotropic. This approximation introduces an error of within ±5% for SO2 layers
above 5 km, and it decreases (increases) with higher (lower) altitude. Given that the vast majority of
volcanic SO2 clouds are distributed below 20 km in altitude, SO2 estimation errors are proportional
to the total SO2 columns. Higher SO2 clouds are not affected by this treatment; and (vi) errors from
the mixed Lambertian equivalent reflectivity treatment of clouds and aerosols that predominate in
the lower troposphere, but the model focuses on light transfer in the particle-laden atmosphere. With
clouds, this discrepancy introduces an uncertainty of within ±2% when the SO2 layer in the
troposphere lies well above the underlying cloud. This retrieval error increases with a smaller
separation between the SO2 layer and the cloud, reaching ±15% when the SO2 layer lies just above
the cloud. It is impossible to derive an overall error from the six sources as some of them depend on
instruments while others are related to column thickness.
Both the minimum error and retrieval precision vary with the satellite data used, and the
underlying surface cover. In general, the most reliable retrieval occurs over oceans, and the least
precise retrieval is associated with clouds. Of the three types of data listed in Table 8.15, the OMI
sensor is the most sensitive to SO2 and the least affected by background noise. Detection inaccuracy
also varies with the SO2 concentration level, and higher concentrations are more accurately
estimated. AIRS and MODIS 7.3 μm retrievals have a detection limit of 5 and 25 DU respectively,
or around 10 DU for the MODIS 8.6 μm retrieval (Table 8.15). SO2 concentrations retrieved from
infrared data are consistently lower than those derived from OMI, with the exception of the MODIS
8.6 μm results, which, under optimal conditions, agree with those of OMI data within errors. Biases
encountered due to the influence of water vapor in the atmosphere and clouds should be considered
when only performing IR retrievals, as otherwise estimates of total emission may underestimate the
actual concentrations (Thomas et al., 2011).
Table 8.15
Comparison of accuracy in retrieving SO2 from four types of satellite data reported in the literature
Data Minimum error
Retrieval precision
(DU)
Equivalent OMI
detection limit Reference
AIRS 6 DU 4.3 5 DU
Prata and
Bernardo
(2007)
MODIS
7.3 μm
±5-20% for the same
retrieval on the TOVs
instrument 5.2 25 DU
Prata et al.
(2003)8.6.6
Data Minimum error
Retrieval precision
(DU)
Equivalent OMI
detection limit Reference
MODIS
8.6 μm ±17%
0.9 over ocean;
21.5 over desert
sand
97.7
meteorological
cloud 10 DU
Realmuto et
al. (1994)
OMI
20% for 100 DU, up to
70% for 400 DU 0.9 n.a.
Yang et al.
(2007)
Source: Modified from Thomas et al. (2011). © T & F.
Global SO2 distribution from satellite instruments can be browsed online on the Global Sulfur
Dioxide Monitoring Home Page (https://so2.gsfc.nasa.gov/). Archived daily images can be viewed
onscreen. SO2 is color coded as daily volcanic regions, daily pollution regions, and long-term
pollution images, but no concentration levels are provided on the global SO2 distribution map
(Figure 8.12), only SCD.
FIGURE 8.12 Global distribution of mean SO2 slant column densities from 1996 to 2002, reflecting
volcanic activities around the world. (Khokhar et al., 2005, ESA report.)
Methane Emissions
Methane (CH4) is the most abundant and damaging greenhouse gas second only to CO2 in the
atmosphere. Although atmospheric methane concentration is low, it is on the rise, and contributes
disproportionately to the current temperature rise in global climate warming. Most of the
atmospheric methane (60%) is sourced naturally via geologic seeps while 40% stems from human
activities that include animal husbandry, decomposition in landfills, and oil and gas extraction andproduction. However, the predominant anthropogenic source is agriculture that contributes around
one quarter of emissions, closely followed by emissions from coal, oil, natural gas, and biofuels.
CH4 abundances in the atmosphere can be estimated from several current and planned satellite
missions. Spaceborne observations are able to supply CH4 concentrations near the Earth’s surface
with high sensitivity and good spatiotemporal coverage at sufficient accuracy. The retrieval of
atmospheric CH4 concentration from satellite data is based on its absorption features with high
sensitivity to the ground and the lower atmosphere where most CH4 is confined. The advantage of
satellite observations for estimating methane source/sink, nevertheless, lies strongly in the
achievable retrieval precision and accuracy.
The atmospheric CH4 concentration can be retrieved from an NIR band around 760 nm and an
absorption band in the SWIR spectral range. However, the retrieval of methane content from
satellite-observed backscattered radiance in the SWIR spectral range is impeded by aerosol and
cirrus cloud scattering. The net light path effect is strongly affected by the quantity, microphysical
properties, height of the scatterers, and the reflectance of the underlying surface. Therefore, CH4
concentration is retrieved either simultaneously with scattering properties of the atmosphere or with
a light path proxy (Borchardt et al., 2021). The “proxy” approach is underpinned by the assumptions
that the scattering effects can be canceled by the ratio of the CH4 column to the CO2 column and that
the a prior estimate of the CO2 column is sufficiently accurate to warrant the reliable re-calculation
of the methane column from the ratio. Thus, the accuracy of this approach is contingent on the
uncertainty of CO2 column used for rescaling and on the cancelation of errors by the CH4/CO2 ratio.
This approach has been successfully applied to GOSAT and SCIAMACHY data around 1600 nm, by
using the CO2 column after it has been retrieved from SCIAMACHY data in the same spectral
range, as a light path proxy.
Alternatively, scattering-induced light path modification can be taken into account by
simultaneously inferring the atmospheric CH4 concentration and physical scattering properties of the
atmosphere based on a physical model such as the forward model. Several physical methods have
been developed for space-based CH4 retrieval from SCIAMACHY and GOSAT data. They all allow
iterative retrieval of XCH4 concentrations along with atmospheric scattering properties and other
auxiliary parameters. Compared to the proxy method, physical models are advantageous in that they
do not require accurate prior information on the CO2 column, even though they are complex and
may be compromised by the information content of inaccurate measurements with respect to aerosol
properties and/or forward model inaccuracy in depicting aerosols.
The retrieval of the atmospheric methane content from the TROPOMI data may be operationally
implemented using the WFM-DOAS algorithm, in a manner reminiscent of that of CO retrieval from
TROPOMI data. It is founded on a first-order Taylor series approximation of Lambert–Beer law
using only one pre-defined radiative transfer calculation per scene to quickly process large datasets
based on the forward model (Eq. 8.58), in which y stands for the (log-)radiance difference (y) of the
measurement and linearized model due to a deviation x of the state vector from the multi￾dimensional linearization point, weighting function (Jacobian) matrix A (with derivatives at the
linearization point and polynomial basis functions as columns), the sum of forward model errors,
and (normally distributed log-transformed) instrument noise .
The usual least-squares fitting approach is adapted to the weighted least squares method by
assigning a larger weight to spectral points with smaller error variances to obtain error estimates ofthe retrieval parameters via error propagation from the uncorrelated measurement errors sI
(Schneising et al., 2019). The weights are defined as W = (C = covariance matrix associated with
measurement noise, Cy = diag(, ,…, ). With the posterior probability p(x|y) of x given y, the most
probable inference of the inversion is obtained by minimizing:
(8.83)
with respect to x, where T = matrix transpose. Hence:
(8.84)
provides the solution of the inversion problem, where is the covariance matrix of solution ().
The satellite-retrieved XCH4 results exhibit a random error of 14.0 parts per billion or ppb
(0.8%) and a systematic error of 4.3 ppb (0.2%) (Schneising et al., 2019). They capture natural
XCH4 variations well and have a high correlation with the validation data (r = 0.91) for daily
averaged XCH4 (Figure 8.13). Major retrieval inaccuracies emanate from scattering by aerosols and
cirrus clouds. The contamination by optically thick clouds can be easily and reliably eliminated via
filtering. In contrast, optically thin scatterers are much thornier to handle. They diminish the
reliability of the estimated methane column by modifying the light path of the image-captured
backscattered signal, a leading source of underestimation or overestimation of the true methane
column if not appropriately accounted for..FIGURE 8.13 Global distribution of mean XCH4 (the total column-averaged dry-air mole fraction of
methane) in parts per billion over the period of November 12–December 30, 2017, estimated from
TROPOMI data. (Apituley et al., 2022. ESA.)
Methane data products have been produced from Sentinel-5P data and released to the public since
mid-July 2018. They fall into three streams: near real-time (NRT), non-time critical or offline, and
reprocessed. NRT data products are delivered within three hours of data acquisition for quick access
and rapid operational processing. The products may suffer from a slightly inferior data quality and
incompleteness. These defects are remedied in the offline data products available within a few days
of data acquisition, or the latest version of reprocessed data for most users. It is recommended that
longer-term trend analyses should all make use of the reprocessed data to avoid changed version
updates.
The aforementioned methods of retrieval from spaceborne data are ill-suited to the estimation of
local scale concentrations of point-emitted CH4 associated with landfills or mining activities from a
single source. CH4 contents in emission hot spot regions are better estimated from airborne remote
sensing data due to their finer spatial resolution. Such data allow a defined area to be sampled with
stronger sensitivity to locally sourced CH4. However, the well-documented retrieval methods for
spaceborne data are no longer applicable. Instead, local-scale methane emissions have to be
estimated using totally different methods, including ground-based in situ tracer gas method, inverse
modeling of direct CH4 measurements, ground-based remote sensing (laser plume mapping,
differential absorption light detection and ranging), and airborne IR laser spectroscopy. The best
option for CH4 retrieval is to make use of two absorption bands around 1,580 and 1,660 nmavailable in the Methane Airborne MAPper (MAMAP) instrument, together with CO2. This sensor
detects the solar radiation reflected from and scattered by the surface in the spectral region of 1590–
1690 nm at a medium spectral resolution of around 0.9 nm to retrieve total column concentration of
CH4 (Krautwurst et al., 2016). Since it is a non-imaging sensor, only data along the flight path are
available for retrieval. This method can quantify the methane concentrations along the flight path, or
identify the source of methane emissions, but cannot yield a field view of methane distribution, a
task achievable using imaging spectrometers in passive sensing.
Airborne imaging spectrometers have a medium spectral resolution (0.9 nm) in the SWIR region
around 1.65 μm, mostly hyperspectral. All the hyperspectral sensors introduced in Section 2.5 can be
used for this purpose. In particular, AVIRIS-NG has a high SNR in spite of its much coarser spectral
resolution than SWIR sensors designed specifically to target CH4. It detects reflected radiance
between 380 and 2510 nm in 432 bands. AVIRIS-NG images have a pixels size of ~3 m at a flight
altitude of 3 km. Column CH4 content is inferred from the measured backscattered solar radiance
from the surface. A key challenge facing airborne image-based retrieval is how to distinguish CH4
source emissions from background noise. One method is to estimate column-integrated methane
concentrations using a linearized and albedo-corrected matched filter (Foote et al., 2020). It is able
to retrieve CH4 above the background concentration, also known as an enhancement. The resulting
CH4 enhancement image is expressed in ppm-m, where ppm represents concentration and m denotes
the path length over which absorption occurs (Figure 8.14).
FIGURE 8.14 Methane plumes around a natural gas processing plant, and the disparity between
AVIRIS-NG retrieved radiance (Meas.) and modeled radiance in relation to wavelength along a line.
(https://avirisng.jpl.nasa.gov/greenhouse_gas_mapping.html.)
CH4 can be retrieved from airborne hyperspectral data using the well-established WFM-DOAS
algorithm originally developed for a finer spectral resolution band of ~0.9 nm. So it has to be
adapted in processing hyperspectral AVIRIS-NG data with a much coarser spectral resolution of 5
nm. The WFM-DOAS algorithm assumes the background state of the atmosphere at the time and
location of sensing, including scattering. Atmospheric parameters deviating from this background
state are linearly fit, making it a faster quantitative method than iterative retrievals. The general
procedure of retrieval remains identical to those for estimating atmospheric CH4 from spacebornedata. The only additional step required is to isolate CH4 enhancement by an emission source from
the background CH4 after it has been successfully retrieved. This task is commonly completed by
using a proxy method to correct the retrieved column enhancement caused by pressure profile or
scattering using another well-mixed gas that can be assumed to be invariable over the region of
study and at the time of overflight. An ideal candidate gas is CO2 that shares a close spectral
proximity to the CH4 absorption band, so the proxy profile-scaling factor (PSF) is calculated as:
 (8.85)
The enhancement in a detected plume has to be corrected for large-scale effects by normalization
over the local background around the plume (). The local column enhancement of CH4 below the
aircraft in a plume is calculated as:
(8.86)
where = background total columns of methane, calculated from averaged climatology over several
years that is enhanced by the total rise in methane based on globally averaged marine satellite data;
kAK = air mass underneath the aircraft. The mass flux (kg∙hr-1) through a transect orthogonal to the
wind direction is calculated from the total column enhancements (molecules∙cm-2) at position i () as:
(8.87)
where u = wind speed (m∙s-2); dxi = length of segments (m).
The accuracy of AVIRIS-NG retrieved CH4 is affected by surface cover. Gas enhancement is
more difficult to detect in pixels of low albedo surfaces because the absorption signal in terms of
absolute radiance is reduced as surface albedo decreases. The more closely the surface resembles
CH4 dark surfaces or surfaces having a spectral reflectance pattern akin to CH4 absorption features
at the spectral resolution of the sensor, the larger the noise. Surfaces of low spectral reflectance have
a weak at-sensor signal. The retrieved CH4 map over bright surfaces (>1 μW ∙ cm-2 ∙ nm-1sr-1 at
2140 nm) has a typical noise of ±2.3% of the background total column CH4 when fitting strong
absorption lines around 2300 nm, but it can exceed ±5% for darker surfaces (<0.3 μW ∙ cm-2 ∙ nm-1
sr-1 at 2140 nm) (Borchardt et al., 2021). Instead of using a single-target spectrum for many flight
lines and for all the pixels within the covered area, per-pixel estimation of the target spectrum can
account for variations in local albedo and should yield more accurate estimates. Additionally, a
worst-case large-scale bias due to the assumptions made in the WFM-DOAS retrieval algorithm is
estimated to be ±5.4%. The retrieval accuracy may be improved using several strategies, such as
sparsity and albedo correction that jointly reduce the RMSE of the retrieved methane concentration￾path length enhancement by 60.7% over a previous robust matched filter method (Foote et al.,
2020). It reduces background noise by a factor of 2.64.
REFERENCES
Abdalla S (2012) Ku-band radar altimeter surface wind speed algorithm. Marine Geodesy 35(sup1):
276–298. doi: 10.1080/01490419.2012.718676
Adirosi E, M Montopoli, A Bracci, F Porcù, V Capozzi, C Annella, G Budillon, E Bucchignani, AL
Zollo, O Cazzuli, G Camisani, R Bechini, R Cremonini, A Antonini, A Ortolani, and L Baldini(2021) Validation of GPM rainfall and drop size distribution products through disdrometers in
Italy. Rem Sens 13(11): 2081. doi: 10.3390/rs13112081
Apituley A, M Pedergnana, M Sneep, JP Veefkind, D Loyola, O Hasekamp, AL Delgado, and T
Borsdorff (2022) Sentinel-5 precursor/TROPOMI Level 2 Product User Manual – Methane. Royal
Netherlands Meteorological Institute, Amsterdam.
https://sentinels.copernicus.eu/documents/247904/2474726/Sentinel-5P-Level-2-Product-User￾Manual-Methane.pdf
Bilal M, A Mhawish, Md A Ali, JE Nichol, G de Leeuw, KM Khedher, U Mazhar, Z Qiu, MP
Bleiweiss, and M Nazeer (2022) Integration of surface reflectance and aerosol retrieval algorithms
for multi-resolution aerosol optical depth retrievals over urban areas. Rem Sens 14(2): 373. doi:
10.3390/rs14020373
Boersma KF, HJ Eskes, A Richter, I De Smedt, A Lorente, S Beirle, JHGM van Geffen, M Zara, E
Peters, M Van Roozendael, T Wagner, JD Maasakkers, RJ van der A Nightingale, A De Rudder, H
Irie, G Pinardi, JC Lambert, and SC Compernolle (2018) Improving algorithms and uncertainty
estimates for satellite NO2 retrievals: Results from the quality assurance for the essential climate
variables (QA4ECV) project. Atmos Meas Tech 11: 6651–6678. doi: 10.5194/amt-11-6651-2018
Boersma KF, HJ Eskes, RJ Dirksen, RJ van der AJP Veefkind, P Stammes, V Huijnen, QL Kleipool,
M Sneep, J Claas, J Leitão, A Richter, Y Zhou, and D Brunner (2011) An improved tropospheric
NO2 column retrieval algorithm for the Ozone Monitoring Instrument. Atmos Meas Tech 4: 1905–
1928. doi: 10.5194/amt-4-1905-2011
Borchardt J, K Gerilowski, S Krautwurst, H Bovensmann, AK Thorpe, DR Thompson, C
Frankenberg, CE Miller, RM Duren, and JP Burrows (2021) Detection and quantification of CH4
plumes using the WFM-DOAS retrieval on AVIRIS-NG hyperspectral data. Atmos Meas Tech 14:
1267–1291. doi: 10.5194/amt-14-1267-2021
Bourassa MA, T Meissner, I Cerovecki, PS Chang, et al. (2019) Remotely sensed winds and wind
stresses for marine forecasting and ocean modeling. Front Mar Sci 6: 1–28. doi:
10.3389/fmars.2019.00443
Bovensmann H, JP Burrows, M Buchwitz, J Frerick, S Noël, VV Rozanov, KV Chance, and APH
Goede (1999) SCIAMACHY: Mission objectives and measurement modes. J Atmos Sci 56(2):
127–150. doi: 10.1175/1520-0469(1999)056<0127:SMOAMM>2.0.CO;2
Burrows JP, M Weber, M Buchwitz, V Rozanov, A Ladstätter-Weißenmayer, A Richter, R DeBeek, R
Hoogen, K Bramstedt, K Eichmann, M Eisinger, and D Perner (1999) The Global Ozone
Monitoring Experiment (GOME): Mission concept and first scientific results. J Atmos Sci 56(2):
151–175. doi: 10.1175/1520-0469(1999)056<0151:TGOMEG>2.0.CO;2CO2
de Graaf M, P Stammes, O Torres, and RBA Koelemeijer (2005) Absorbing aerosol index:
Sensitivity analysis, application to GOME and comparison with TOMS. J Geophys Res 110: 1–19.
doi:10.1029/2004JD005178
Di A, Y Xue, X Yang, J Leys, J Guang, L Mei, and Y Che (2016) Dust aerosol optical depth retrieval
and dust storm detection for Xinjiang region using Indian National Satellite Observations. Rem
Sens 8: 702. doi: 10.3390/rs8090702
ESA (2024) DOAS Method - Level-2 Processing - Sentinel-5P Technical Guide - Sentinel Online -
Sentinel Online (copernicus.eu). https://sentinels.copernicus.eu/web/sentinel/technical-guides/sentinel-5p/level-2/doas-method accessed on 9 February 2024
Foote MD, PE Dennison, AK Thorpe, DR Thompson, S Jongaramrungruang, C Frankenberg, and
SC Joshi (2020) Fast and accurate retrieval of point-source methane emissions from imaging
spectrometer data using sparsity prior. IEEE Trans Geosci Rem Sens 58(9): 6480–6492. doi:
10.1109/TGRS.2020.2976888
Gao R (2022) Research progress of atmospheric CO2 monitoring by satellite remote sensing. J
Phys: Conf Ser 2386: 012028.
Garay MJ, ML Witek, RA Kahn, FC Seidel, JA Limbacher, MA Bull, DJ Diner, EG Hansen, OL
Kalashnikova, H Lee, AM Nastan, and Y Yu (2020) Introducing the 4.4 km spatial resolution
Multi-Angle Imaging SpectroRadiometer (MISR) aerosol product. Atmos Meas Tech 13: 593–628.
doi: 10.5194/amt-13-593-2020
Holben BN, TF Eck, I Slutsker, D Tanre, JP Buis, A Setzer, E Vermote, JA Reagan, Y Kaufman, T
Nakajima, F Lavenu, I Jankowiak, and A Smirnov (1998) AERONET - A federated instrument
network and data archive for aerosol characterization. Rem Sens Environ 66: 1–16.
Hsu NC, JR Herman, O Torres, BN Holben, D Tanre, TF Eck, A Smirnov, B Chatenet, and F Lavenu
(1999) Comparisons of the TOMS aerosol index with Sun-photometer aerosol optical thickness:
Results and applications. J Geophysical Res - Atmos 104(D6): 6269–6279. doi:
10.1029/1998JD200086
Huang X and K Yang (2022) Algorithm theoretical basis for ozone and sulfur dioxide retrievals from
DSCOVR EPIC. Atmos Meas Tech 15: 5877–5915. doi: 10.5194/amt-15-5877-2022
Javed Z, A Tanvir, Y Wang, A Waqas, M Xie, A Abbas, O Sandhu, and C Liu (2021) Quantifying the
impacts of COVID-19 lockdown and spring festival on air quality over Yangtze river delta region.
Atmosphere 12(6): 735. doi: 10.3390/atmos12060735
Jung C-R, W-T Chen, and SF Nakayama (2021) A national scale 1-km resolution PM2.5 estimation
model over Japan using MAIAC AOD and a two-stage random forest model. Rem Sens 13(18):
3657. doi: 10.3390/rs13183657
Khokhar MF, C Frankenberg, J Hollwedel, S Beirle, S Kühl, M Grzegorski, W Wilms-Grabe, U
Platt, and T Wagner (2005) Satellite Remote Sensing of Atmospheric SO2: Volcanic Eruptions and
Anthropogenic Emissions. Proc. 2004 Envisat & ERS Sympo (ESA SP-572). 6–10 September
2004, Salzburg, Austria. ESA Special Publication.
Krautwurst S, K Gerilowski, H Jonsson, D Thompson, R Kolyer, A Thorpe, M Horstjann, M
Eastwood, I Leifer, S Vigil, T Krings, J Borchardt, M Buchwitz, M Fladeland, J Burrows, and H
Bovensmann (2016) Methane emissions from a Californian landfill, determined from airborne
remote sensing and in-situ measurements. Atmos Meas Tech 2016: 1–33. doi: 10.5194/amt-2016-
391
Landgraf J, J aan de Brugh, R Scheepmaker, T Borsdorff, H Hu, S Houweling, A Butz, I Aben, and
O Hasekamp (2016) Carbon monoxide total column retrievals from TROPOMI shortwave
infrared measurements. Atmos Meas Tech 9: 4955–4975. doi: 10.5194/amt-9-4955-2016
Lee C, K Lee, S Kim, J Yu, S Jeong, and J Yeom (2021) Hourly ground-level PM2.5 estimation
using geostationary satellite and reanalysis data via deep learning. Rem Sens 13(11): 2121. doi:
10.3390/rs13112121Levelt PF, GHJ van den Oord, MR Dobber, and A Mälkki (2006) The ozone monitoring instrument.
IEEE Trans Geosci Rem Sens 44(5): 1093.
Li Y, S Yuan, S Fan, Y Song, Z Wang, Z Yu, Q Yu, and Y Liu (2021) Satellite remote sensing for
estimating PM2.5 and its components. Curr Pollution Rep 7: 72–87. doi: 10.1007/s40726-020-
00170-4
Liu Y, JA Sarnat, V Kilaru, DJ Jacob, and P Koutrakis (2005) Estimating ground-level PM2.5 in the
eastern United States using satellite remote sensing. Environ Sci Tech 39(9): 3269–3278. doi:
10.1021/es049352m
Liu Y, RJ Park, DJ Jacob, QB Li, V Kilaru, and JA Sarnat (2004) Mapping annual mean ground￾level PM2.5 concentrations using Multiangle Imaging Spectroradiometer aerosol optical thickness
over the contiguous United States. J Geophys Res- Atmos 109: 1–-10. doi:
10.1029/2004JD005025
Main-Knorn M, B Pflug, J Louis, V Debaecker, U Müller-Wilm, and F Gascon (2017) Sen2Cor for
Sentinel-2. Proc SPIE 3: 1–12. doi: 10.1117/12.2278218
Meissner T and FJ Wentz (2009) Wind-vector retrievals under rain with passive satellite microwave
radiometers. IEEE Trans Geosci Rem Sens 47(9): 3065–3083. doi: 10.1109/TGRS.2009.2027012
Michaelides S, V Levizzani, E Anagnostou, P Bauer, T Kasparis, and JE Lane (2009) Precipitation:
Measurement, remote sensing, climatology and modeling. Atmos Res 94(4): 512–533. doi:
10.1016/j.atmosres.2009.08.017
Nguyen NH and VA Tran (2014) Estimation of PM10 from AOT of satellite Landsat image over
Hanoi city. Int Symp Geoinfo for Spatial Infrastr Dev in Earth and Allied Sci (GIS IDEAS) 2014.
doi: gisws.media.osaka-cu.ac.jp/gisideas14/viewpaper.php?id=518
O’Dell C, B Connor, H Boesch, D O’Brien, C Frankenberg, R Castaño, M Christi, D Crisp, A
Eldering, B Fisher, M Gunson, J McDuffie, C Miller, V Natraj, F Oyafuso, I Polonsky, M Smyth,
T Taylor, G Toon, and D Wunch (2012) The ACOS CO2 retrieval algorithm - Part 1: Description
and validation against synthetic observations. Atmos Measur Tech 4: 6097–6158. doi:
10.5194/amtd-4-6097-2011
Panfilova M and V Karaev (2021) Wind speed retrieval algorithm using Ku-band radar onboard
GPM satellite. Rem Sens 13(22): 4565. doi: 10.3390/rs13224565
Plane J and A Saiz-Lopez (2006) UV-Visible Differential Optical Absorption Spectroscopy (DOAS).
(ISAC-Bologna PPT). www.researchgate.net/publication/227555901
Prata AJ and C Bernardo (2007) Retrieval of volcanic SO2 column abundance from atmospheric
infrared sounder data. J Geophy Res – Atmos 112, D20204.
Prata AJ, WI Rose, S Self, and DM O’Brien (2003) Global, long–term sulphur dioxide
measurements from the TOVS data: A new tool for studying explosive volcanism and climate.
Geophys Monograph 139: 75–92.
Rawat P and M Naja (2022) Remote sensing study of ozone, NO2, and CO: Some contrary effects of
SARS-CoV-2 lockdown over India. Environ Sci Pollut Res Int 29(15): 22515–22530. doi:
10.1007/s11356-021-17441-2
Realmuto VJ, MJ Abrams, MF Buongiorno, and DC Pieri (1994) The use of multispectral thermal
infrared image data to estimate the sulfur dioxide flux from volcanoes: A case study from Mount
Etna, Sicily, July 29, 1986. J Geophy Res 99: 481–488.Refaat TF, M Petros, CW Antill, UN Singh, Y Choi, JV Plant, JP Digangi, and A Noe (2021)
Airborne testing of 2-μm pulsed IPDA Lidar for active remote sensing of atmospheric carbon
dioxide. Atmosphere 12(3): 412. doi:10.3390/atmos12030412
Schneising O, M Buchwitz, M Reuter, H Bovensmann, JP Burrows, T Borsdorff, NM Deutscher, DG
Feist, DWT Griffith, F Hase, C Hermans, LT Iraci, R Kivi, J Landgraf, I Morino, J Notholt, C
Petri, DF Pollard, S Roche, K Shiomi, K Strong, R Sussmann, VA Velazco, T Warneke, and D
Wunch (2019) A scientific algorithm to simultaneously retrieve carbon monoxide and methane
from TROPOMI onboard Sentinel-5 Precursor. Atmos Meas Tech 12: 6771–6802. doi:
10.5194/amt-12-6771-2019
She L, Y Xue, X Yang, J; Guang, Y Li, Y Che, C Fan, and Y Xie (2018) Dust detection and intensity
estimation using Himawari-8/AHI observation. Rem Sens 10(4): 490. doi: 10.3390/rs10040490
Stolarski RS and RD McPeters (2003) Satellite remote sensing | TOMS ozone. In JR Holton (ed.)
Encyclopedia of atmospheric sciences. Academic Press, 1999-2005. doi: 10.1016/B0-12-227090-
8/00351-1
Sun L, J Wei, M Bilal, X Tian, C Jia, Y Guo, and X Mi (2016) Aerosol optical depth retrieval over
bright areas using Landsat 8 OLI images. Rem Sens 8: 23. doi: 10.3390/rs8010023
Sun Y, Y Xue, X Jiang, C Jin, S Wu, and X Zhou (2021) Estimation of the PM2.5 and PM10 mass
concentration over land from FY-4A aerosol optical depth data. Rem Sens 13(21): 4276. doi:
10.3390/rs13214276
Thi Van T, NH Hai, VQ Bao, and HDX Bao (2018) Remote sensing-based aerosol optical thickness
for monitoring particular matter over the city. In Proceedings of the 2nd Intern Electronic Conf
Rem Sens no. 7: 362. doi: 10.3390/ecrs-2-05175
Thomas HE, IM Watson, SA Carn, AJ Prata, and VJ Realmuto (2011) A comparison of AIRS,
MODIS and OMI sulphur dioxide retrievals in volcanic clouds. Geomat Nat Haz Risk 2(3): 217–
232. doi: 10.1080/19475705.2011.564212
Torres O, A Tanskanen, B Veihelmann, C Ahn, R Braak, PK Bhartia, P Veefkind, and P Levelt
(2007) Aerosols and surface UV products from Ozone Monitoring Instrument observations: An
overview. J Geophys Res-Atmos 112: D24S47. doi: 10.1029/2007JD008809
Torres O, PK Bhartia, JR Herman, Z Ahmad, and J Gleason (1998) Derivation of aerosol properties
from satellite measurements of backscattered ultraviolet radiation: Theoretical basis. J Geophys
Res - Atmos 103: 17099–17110. doi: 10.1029/98JD00900
van Geffen J, KF Boersma, H Eskes, M Sneep, M ter Linden, M Zara, and JP Veefkind (2020) S5P
TROPOMI NO2 slant column retrieval: Method, stability, uncertainties and comparisons with
OMI. Atmos Meas Tech 13: 1315–1335. doi: 10.5194/amt-13-1315-2020
Villena CR, JS Anand, RJ Leigh, PS Monks, CE Parfitt, and JD Vande Hey (2020) Discrete￾wavelength DOAS NO2 slant column retrievals from OMI and TROPOMI. Atmos Meas Tech 13:
1735–1756. doi: 10.5194/amt-13-1735-2020
Wang Y, Md A Ali, M Bilal, Z Qiu, A Mhawish, M Almazroui, S Shahid, M N Islam, Y Zhang, and
Md N Haque (2021) Identification of NO2 and SO2 pollution hotspots and sources in Jiangsu
Province of China. Rem Sens 13(18): 3742. doi: 10.3390/rs13183742
Wang Z, L Chen, J Tao, Y Zhang, and L Su (2010) Satellite-based estimation of regional particulate
matter (PM) in Beijing using vertical-and-RH correcting method. Rem Sens Environ 114(1): 50–Index
A
Aboveground carbon, 110
Absorbing aerosol index, 387–388
Absorption
absorption coefficient, 29, 223, 316
relative depth of absorption, 151
Accumulated DDM-to-noise ratio, 330
AccuPAR, 22
ACIX, 100–101
ACOLITE, 98, 102
Activation function, 121–123
Adaptive neural fuzzy inference system, 181
Advanced Himawari Imager, see Himawari
Advanced Earth Observing Satellite, 51–55
Advanced Microwave Scanning Radiometer, 327
Advanced Scatterometer, 58
AERONET, 30, 101
Aerosol, 389
absorbing aerosol index, 381, 387
aerosol LiDAR, 69, 387–388
aerosol optical depth, 85–87, 389–392
aerosol optical thickness, 93, 96, 125, 127, 387–389
UV aerosol index, 386–387
Agisoft PhotoScan, 146Airborne laser scanning, see LiDAR
Airborne Visible InfraRed Imaging Spectrometer-Next Generation
(AVIRIS–NG), 60, 256, 423–424
Air mass factor, 374
Albedo, see Surface albedo
Algal biomass, 348, 356
Algal bloom index, 355
Allometric equations, 274–275
Alternative floating algae index, 358
Analytical methods, 83–88, 108–152
Ancillary data, 30–31, 341
Aquarius, 342–343
Artificial neural networks, 119–128
back-propagation neural network, 120
cascade-forward neural network, 121
ConvLSTM, 124
deep feed-forward neural network, 288
deep neural network, 121–122
extreme learning machine, 125–127
feed-forward ANN, 119–121
general regression neural networks, 124–125
long short-term memory, 121–124
mask convolutional neural network, 232, 287, 357
recurrent neural network, 121
(3D) convolutional NN, 121–122
ATCOR, 105–106
Atmosphere, 371–420
Atmospheric correction, 71–106
Atmospheric Infrared Radiation Sounder, 377
Atmospheric satellites, 374–380AVHRR, 253, 254
B
Backscattering, 29, 169
backscattering coefficient, 286, 316
Band ratio, 315
band ratio model, 320
Bathymetry, 318–322
coastal bathymetry, 319–321
LiDAR bathymetry, 322–325
river channel bathymetry, 321–322
Bayesian networks, 136–139
Beer law (Lamber-Beer), 372, 421
Biomass
aboveground biomass, 267–283
catchment-scale biomass, 270–273
belowground biomass, 24, 267
grassland biomass, 268–272
meso-scale biomass, 273–284
micro-scale biomass, 269–270
Bio-optical algorithms, 347–348
Biophysical variables, 228–242
Biosphere, 218–302
Blackbody, 159
Bouguer-Lambert law, 372, 389
Brightness temperature, 298, 326, 327
Byram’s fire intensity, 300
C
CAAS, 94–96
CALIPSO, 69Canopy diameter, 20
Canopy height model, 232–236
Carbon dioxide, 405–410
Carbon monoxide, 410–412
Carlson’s trophic state index, 353–354
Case I waters, 93, 313, 362
Case II waters, 93, 312, 315
CERES-Rice model, 297–298
Chemical transport model, 399, 414
Chlorophyll content, 23, 223, 253–259
canopy chlorophyll content, 255–259
chlorophyll content, 23–24
chlorophyll content meter, 23–24
Chlorophyll indices, 345–346
inverted red-edge chlorophyll index, 254
maximum chlorophyll index, 345
normalized difference chlorophyll index, 345–346
Cloud-Aerosol Lidar with Orthogonal Polarization (CALIOP), see LiDAR
Coefficient of determination, 38, 109, 349
Collinearity equation, 166–167
Colored dissolved organic matter (CDOM), 29, 317–336
Column-integrated biomass, 356–357
Compact Airborne Spectrographic Imager, 60–61
Compact High Resolution Imaging Spectrometer, 63–64
Continuum removal, 150–152, 299
Correlation coefficient, 149, 171, 256
COSI-Corr, 149–150
Cost functions, 226–227
C2RCC, 100
Crop surface models, 285Crop yield estimation, 31, 284–298
corn yield, 285–288
rice yield, 292–298
wheat yield, 288–292
Cross-correlation coefficient, 148–150
Cross-validation, 36–38
K-fold cross validation, 37
leave-one-out cross validation, 37–38
CubeSats, 50–51
D
Dark object subtraction, 89–92
Decision tree, 114
gradient boosting decision tree, 114–116
Decision Support System for Agrotechnology Transfer (DSSAT) 298
Decorrelation, 196, 206–207
spatial decorrelation, 196, 200, 207
temporal decorrelation, 196, 206, 213
Deep learning models, 359
Delay-Doppler maps, 330
Dense dark vegetation, 92–93
Depth of penetration, 312, 323
Diameter at breast height, 236–240
Difference normalized burn ratio, 302–305
Differencing of DEMs, 185, 209
Differential absorption LiDAR (DIAL), 408
Differential global navigation satellite systems, 49
Differential slant column density, 373–374
Diffuse reflection, 75, 314
Digital elevation model, 104
Digital number, 90Digital surface model, 209, 232
Dissolved organic carbon, 362–363
Dissolved organic matter, 59, 76, 360–362
Distributed scatterers, 208–209
DOAS, 373–374
discrete-wavelength DOAS, 413–415
Dutch OMI DOAS, 413–414
Drone images, 31, 48–50, 179, 268, 285
Dual-frequency precipitation radar, 382, 385–386
Dynamic quantification, 6
E
Earth observation satellites, 51–57
Earth Polychromatic Imaging Camera, 376–377
Earthshine radiance, 378, 403, 418
Electrical conductivity, 20, 171–172
Emissivity, 163, 168, 328
Empirical models, 108–109
Enhanced dust index, 400–401
Enhanced dust intensity index, 401
Estimation models, 247–250
Eutrophication, 353–354
Evapotranspiration, 294–295
potential evapotranspiration, 294
Excess green index, 285
F
Fast Fourier Transformation, 330, 379, 408
Feature matching, 144
Feature selection, 111
Feature tracking, 194Field measurements, 28–29, 37, 355
Field of view, 18, 58, 62–63, 364
Fire parameters, 298–305
burn intensity, 302–305
normalized burn ratio, 302–303
relative burn ratio, 303–304
FLAASH, 82–83, 101
Flow velocity, 205, 332–335
Fluorescence spectrometry, 20
FORCE, 103–105
Forest Light Interaction Model, 221–223
Forward modeling, 224, 328, 405
Fourier Transform Spectrometer, 379–380, 408
Fractional vegetation cover, 228–231
fractional vegetation cover product, 230–231
G
Gaussian process regression, 118, 134–136
Generalized additive model, 134–136
Geographically and temporally weighted regression, 396
Geographically weighted regression, 396, 398
Geo-referencing, 51, 232
Geostationary Ocean Color Imager, 59–60, 337–338
GF-5, 64, 408
GLASS, 230–231
Global Navigation Satellite System, 49, 331, 324
Global Ozone Monitoring Experiment, 376, 387, 404–405
Global positioning system, 12, 16, 23, 49, 67–68
Global Precipitation Measurement (GPM), 385–386
Global Navigation Satellite System Reflectometry (GNSS-R), 329
Gradient boosted decision tree, 114–117Greenhouse gases Observing SATellite (GOSAT), 378–380, 406
Ground control point, 141, 143, 145
Ground deformation, 209–213
Ground sampling, see Spatial sampling
Ground subsidence, 201–203
H
Harvest index, 289–290
Himawari-8, 56–57, 329
HJ-1, 271
Horizontal movement, 193–201, 212
Hybrid method, 224–225, 255
Hydrosphere, 312–367
HyMap, 61–62
Hyperparameter, 124, 135
Hyperplane, 128–131
Hyperion, 178
Hyperspectral data, 60–64
Hyperspectral InfraRed Imager (HyspIRI) 61–62
I
iCOR, 99–100
Image registration, 144
Inertial measurement unit, 49, 50
Infrared Atmospheric Sounding Interferometer, 62–63, 406
InSAR, 8, 194–197
coherent scatterer InSAR, 208–209
differential InSAR, 203, 211–212
persistent scatter InSAR, 211
In situ sampling, see spatial sampling
Instantaneous field of view, 53, 60, 63Interferogram, 195–196, 203–207
Invertible Forest Reflectance Model, 221–223, 257
K
Kernel function, 127, 131, 133–134
Kernel ridge regression, 133–134, 251
Keypoints, 143–144
K nearest neighbour regression, 325, 366
L
Lahar, 189–190
Landsat ETM+, 271
Landsat OLI, 172, 271, 317
Landslide debris, 184–188
thickness, 184–186
volume, 186–188
Land surface water index, 177, 295
Leaf area index (LAI) 242–253
global LAI product, 252–253
Least angle regression, 176
LIBERTY, 221, 223
LiDAR, 66–69
airborne laser scanning, 67–69
bathymetric LiDAR, 322–325
CALIOP LiDAR, 69
differential absorption LiDAR, 408
integrated path differential, 408–410
terrestrial laser scanning, 66–67, 148
terrestrial LiDAR, 185, 192, 231
Light use efficiency, 265–267
Line of sight, 41, 196–197Look-up table, 81, 93, 224, 226–228
M
M3C2, 150
Machine learning models, 242, 271
Matching points, 141, 143
Mean absolute error, 323
Mean absolute percentage error, 40
Mean prediction error, 40, 353
Mean wave period, 329–330
Medium Resolution Imaging spectrometer, 58–59
Meteorological satellites, 58, 62
Methane Airborne MAPper, 423
Methane emissions, 420–425
MetOp satellite, 62, 170, 406
Mid-infrared burn index, 302–303
Model inversion strategies, 225–228
Moderate-resolution Atmospheric Transmission (MODTRAN), 86, 97
Moderate Resolution Imaging Spectroradiometer, 54–55, 164, 230–231,
253
Modified normalized difference water index, 360
MOPITT, 410
Movement detection, 148–150
Multi-angle Imaging SpectroRadiometer, 57–58, 254, 393
Multi-collinearity, 111–112
Multifunctional transport satellite, 329, 392
Multi-layer perceptron, 119, 352
Multiple linear regression, 110–112
Multispectral data, 51–54, 289
Multispectral imager (instrument), 252N
National Polar-Orbiting Partnership, 55–56
Net primary productivity, 259–261
Nitrogen dioxide, 412–416
Non-parametric method, 108–114, 118
Non-parametric regression, 250
Non-parametric statistical method, 134, 255
O
OceanSat, 314
Oceansat Scatterometer, 314
Ocean and Land Color Instrument, 58–59
Ocean color algorithms, 347–348, 362
Ocean Colour Monitor, 314
Oceanographic satellites, 58–60
OC4ME, 360
Optical measurements, 29–30
in-door measurements, 29
in-situ measurements, 25–28
underwater method, 27
Orbiting Carbon Observatory, 406
ORYZA crop growth model, 296, 298
Ozone, 79, 86, 88, 374–376, 401–405
Ozone Monitoring Instruments, 375–376
P
Paddy mapping, 292–293
Parameterized first-guess spectrum method, 329–330
Parametric statistical method, 255
Partial least square regression, 112–114
Particle image velocimetry, 334–335Particulate matters, 392–400
Photogrammetry, see SfM photogrammetry
Photosynthetically active radiation, 22–23, 260
APAR, 225, 228, 260
fPAR, 260
FAPAR, 262
Phycocyanin (cyanobacterial) blooms, 365–367
Physical models, 11, 78–82
Phytoplankton, 29, 94, 314, 316, 336, 344, 355
phytoplankton layer depth, 363–365
Planck’s constant, 159, 160
Planck’s equation, 326, 417
Planetary boundary layer height, 393–395
Planimetric displacement, 196
Plant canopy analyzer, 21
Plant pigment ratio, 285
Polarimetric radiometer, 382
Precipitation rate, 384–386
Predictor variables, 31–36
Project for On-Board Autonomy (PROBA), 63, 265
PROSAIL, 225, 243–244, 252
PROSPECT, 223, 257
Proxy profile-scaling factor, 423
Q
Qualitative remote sensing, 4–13
Quantification, 4–13
accuracy, 38–39
components, 9–11
nature, 7–11
principles, 7–11requirements, 11–13
Quasi-analytical algorithm, 316
R
RadarSat, 249, 286
Radar sensing, 47, 64–66
normalized radar cross section, 329–330, 382
radar altimeter, 382
radargrammetry, 240, 242
radar polarization, 65, 79, 249, 286
Radial-based function, 131, 134
Radiance, 18, 25–28
backscattered radiance, 83, 373, 421
bottom-of-atmosphere radiance, 99, 102
top-of-atmosphere radiance, 72, 80, 347
water downwelling radiance, 27–28
water-leaving radiance, 25–28
water upwelling radiance, 26
Radiometric calibration, see Atmospheric correction
Random forest, 117–118
spatiotemporally weighted random forest, 396–398
RapidEye, 5–54, 257–258
Radiative transfer model, 78, 87, 221, 224
Real-time kinematic, 232
Red edge, 35
normalized difference red edge, 177, 256
red edge normalized difference, 288
Reflection, 25, 75–77
bidirectional reflection distribution function BRDF, 75, 79, 105
diffuse attenuation, 25
Lambertian reflector, 75, 91, 415Specular, 75, 314
Regression models, 109–112
Relative humidity, 97, 394
Retrospective quantification, 42
Root-mean-square error, 38–40, 175, 178
normalized root-mean-square error, 39–40
relative root-mean-square error, 39
RPD-ratio of the performance to the derivation, 40, 172, 178, 182, 228
S
Scattering by Arbitrarily Inclined Leaves (SAIL) 221–225
SAILH, 221–222, 243
Sampling strategy, see Spatial sampling
Sampling unit, 15–16, 21
point sampling, 15–16
plot sampling, 15–16, 24
transect sampling, 15
Scale invariant feature transform, 143
Scale mismatch, 37, 43
Scattering, 28, 73, 77, 84
scattering coefficient, 29
successive order of scattering, 79
Scene reconstruction, 143–145
SCIAMACHY, 377–378
SeaDAS, 94, 96–99
Sea surface salinity, 340–343
Sea surface temperature, 314, 325–329
Sea surface wind speed, 329
SeaWIFS, 348, 356
Secchi disc depth, 317, 318
Sediment budget, 188–189Semi-analytical algorithm, 316
Semi-analytical model, 339–340, 346
Semi-empirical method, 251–252, 393
Semi-physical models, 108, 221–224
Sen2Cor, 98–99, 102
Senescence temperature threshold, 290
Sensing platforms, 48–51
aircraft, 50
drone, 48–50
satellite, 50–51
Sentinel-1 SAR, 66, 203
Sentinel 2 and 3, 53, 58, 97–98, 100
Sentinel-5P, 381, 422
SfM photogrammetry, 140–148
COLMAP, 146, 147
OpenSfM, 146, 147
SfM Multi-View Stereo photogrammetry, 145, 147, 187
TheiaSfM, 146–147
Visual SfM, 146–147
Signal-to-noise ratio, 59–62, 205, 330–331, 419
Significant wave height, 329–332
Simple algorithm for yield estimation (SAFY), 221, 289–290
Simple look complex images, 203, 204
Simplified Aerosol Retrieval Algorithm, 391
Simplified and Robust Surface Reflectance Estimation, 81–82, 102–103,
391
Simplified Method for Atmospheric Corrections, 83–86
6S models, 78–80
6SV, 78
SNAP toolbox, 100, 257Snow avalanche, 191–192
Snow depth, 191–192
Softcopy photogrammetry, see SfM photogrammetry
Soil adjusted vegetation index, 33–34, 177, 246, 283
optimized SAVI, 33–34, 177, 246
Soil contaminants, 180–184
Soil moisture, 58, 165–170
normalized backscattering moisture index, 169
relative soil moisture, 168–169
soil moisture index, 168
Soil Moisture and Ocean Salinity mission, 342
Soil organic carbon, 20, 176–180
total organic carbon, 100
Soil salinity, 170–175
normalized difference salinity index, 173, 177
soil salinity index, 173–174
soil salt content, 175–176
Spatial autocorrelation, 187
Spatial resolution, 12, 17, 47–48, 52, 55, 59, 60, 62, 64, 102, 167, 230
Spatial sampling, 13–17
random sampling, 14
sample size, 13–14
sampling considerations, 13
stratified sampling, 14
systematic sampling, 14
Spectral measurement, 17–19
Spectral parameters, 180, 280
Spectral reflectance, 18, 101, 170, 219–220, 313, 349, 378, 390
bidirectional reflectance, 221–222
spectral reflectance curve, 76, 170–171, 219–220Spectral signature of vegetation, 218–220
Spectral signature of water, 313–314
Spectroradiometer, 18, 22, 25
Static quantification, 6
Stefan-Boltzman constant, 159
Stefan-Boltzman law, 300
Structural parameter, 24, 221, 233, 235, 279–280
Sulphur dioxide, 376–377, 405, 416–420
Sun photometer, 389
Support vector machine, 128–132
Support vector regression, 132–133
Surface albedo, 164–166
Surface latent heat, 161–162
Surface movement, 192–213
Suspended sediment, 15, 336–340
suspended sediment concentration, 39, 336–340
total suspended solids, 336, 339, 344
T
Temperature
kinetic temperature, 158
land surface temperature, 158–160
lava temperature, 161–164
radiant temperature, 158–159, 162
sea surface temperature, 158, 314, 325–329
Tephra, 189, 190
TerraSAR, 65–66, 243, 286
Terrestrial sphere, 157–213
3-band indices, 173–174
3D point cloud, 142, 145–146, 201, 232
Timber volume, see Wood volumeTopographic correction, 103–106
Total Ozone Mapping Spectrometer, 374–376
Tree height, 20, 231–236
Tree volume, see Wood volume
Triangulated irregular network, 232
Triangulation, 144–145
TRMM Microwave Imager, 57, 327, 386
Tropical Rainfall Measuring Mission, 57, 385–386
TROPOspheric Monitoring Instrument, 380–381, 388, 410–413
U
UAV, see Drones
Uncertainty, 42, 104, 139, 150, 187, 250, 328
Urban heat flux, 160–161
V
Validation, 36–37
Vegetation indices, 32–36, 246
atmospherically resistant vegetation index, 36
enhanced vegetation index, 35
global environmental monitoring index, 36
green chlorophyll vegetation index, 292
normalized difference vegetation index, 33–35, 229–230, 256, 261, 263,
276, 286
normalized green-red difference index, 285, 345
perpendicular vegetation index, 35, 181
ratio vegetation index, 35, 86, 270
RGB vegetation index, 31–32, 268, 270
transformed triangular vegetation index, 247
weighted difference vegetation index, 32–34
wide dynamic range vegetation index, 35–36, 262, 263Velocity
earthflow velocity, 197, 205
ocean current velocity, 332–334
wind speed, 381–383
Vertical column density, 374, 403
Very high resolution imagery, 87–88, 286
Vexcel UltracamX camera, 240
Viewing zenith angle, 222, 376, 419
VIIRS, 55–56, 254
Visibility, 12, 83, 86, 105, 390
Volcanic deposits, 189–191
W
Water content measurement, 30
Water optical properties, 25
apparent optical properties, 25, 29
inherent optical properties, 29, 100, 316, 350
Water turbidity, see Secchi disc depth
Weighting Function Modified DOAS, 373, 410, 421, 423
Wide Swath and High Resolution Airborne Pushbroom Hyperspectral
Imager (WiSHiRaPHI) 62, 63
Wild fires, 298–305
fire front length, 300–302
fire intensity, 299–301
fire radiative energy density, 300, 302
fire radiative power, 299–305
flame depth, 300–301
FRP density, 301
rate of spread, 298–300
WindSat, 382–383
Wood volume, 240–243WorldView-3, 320, 322, 325
Y
Yield estimation, 284–298
YieldNet, 287–288
Z
ZY-1, 64, 289
