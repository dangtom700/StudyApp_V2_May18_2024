Numerical Methods
Classical and Advanced Topics
This book presents a pedagogical treatment of a wide range of numerical methods to suit the 
needs of undergraduate and postgraduate students, and teachers and researchers in physics, 
mathematics and engineering. For each method, the derivation of the formula/algorithm, error 
analysis, case studies, applications in science and engineering and the special features are cov￾ered. A detailed presentation of solving time-dependent Schrödinger equation and nonlinear 
wave equations, along with the Monte Carlo techniques (to mention a few), will aid in students’ 
understanding of several physical phenomena including tunnelling, elastic collision of nonlinear 
waves, electronic distribution in atoms and diffusion of neutrons through a simulation study. 
The book covers advanced topics such as symplectic integrators and random number generators 
for desired distributions and Monte Carlo techniques, which are usually overlooked in other 
numerical methods textbooks. Interesting updates on classical topics include curve fitting to 
a sigmoid and Gaussian functions and product of certain two functions, solving of differential 
equations in the presence of noise and solving the time-independent Schrödinger equation. 
Solutions are presented in the form of tables and graphs to provide visual aid and encourage a 
deeper comprehension of the topic. The step-by-step computations presented for most of the 
problems can be verifiable using a scientific calculator and are therefore appropriate for class￾room teaching. The readers of the book will benefit from acquiring an acquittance, knowledge, 
experience and realization of the significance of the numerical methods covered, their applica￾bility to physical and engineering problems and the advantages of applying numerical methods 
over theoretical methods for specific problems.
Shanmuganathan Rajasekar was born in Thoothukudi, Tamil Nadu, India, in 1962. He was 
awarded Ph.D. from Bharathidasan University in 1992 under the supervision of Prof. M. Laksh￾manan. In 1993, he joined as a Lecturer at the Department of Physics, Manonmaniam Sundaranar 
University, Tirunelveli. In 2005, he joined as a Professor at the School of Physics, Bharathidasan 
University, Tiruchirapalli. His recent research focuses on nonlinear dynamics with a special em￾phasis on nonlinear resonances. He has authored or co-authored more than 120 research papers 
in nonlinear dynamics.Numerical Methods
Classical and Advanced Topics
Shanmuganathan RajasekarDesigned cover image: S. Rajasekar
First edition published 2024
by CRC Press
2385 NW Executive Center Drive, Suite 320, Boca Raton FL 33431
and by CRC Press
4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN
CRC Press is an imprint of Taylor & Francis Group, LLC
© 2024 S. Rajasekar 
Reasonable efforts have been made to publish reliable data and information, but the author and publisher cannot 
assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers 
have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright 
holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowl￾edged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, transmitted, or 
utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including pho￾tocopying, microfilming, and recording, or in any information storage or retrieval system, without written permission 
from the publishers.
For permission to photocopy or use material electronically from this work, access www.copyright.com or contact the 
Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 01923, 978-750-8400. For works that are 
not available on CCC please contact mpkbookspermissions@tandf.co.uk
Trademark notice: Product or corporate names may be trademarks or registered trademarks and are used only for 
identification and explanation without intent to infringe.
ISBN: 978-1-032-63894-2 (hbk)
ISBN: 978-1-032-64991-7 (pbk)
ISBN: 978-1-032-64993-1 (ebk)
DOI: 10.1201/9781032649931
Typeset in CMR10
by KnowledgeWorks Global Ltd.
Publisher’s note: This book has been prepared from camera-ready copy provided by the authors.To
Professor M. Lakshmanan
Professor K.P.N. MurthyContents
Preface xiii
About the Author xv
1 Preliminaries 1
1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2 Binary Number System . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.3 Floating Point Arithmetic and Significant Digits . . . . . . . . . . . . . . 2
1.4 Type of Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.5 Periodic and Differentiable Functions and Series . . . . . . . . . . . . . . . 6
1.6 Rolle’s, Intermediate-Value and Extreme-Value Theorems . . . . . . . . . 7
1.7 Iterations and a Root of an Equation . . . . . . . . . . . . . . . . . . . . . 8
1.8 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
1.9 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
1.10 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
2 Solutions of Polynomial and Reciprocal Equations 12
2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
2.2 Determination of the Region Enclosing all the Roots . . . . . . . . . . . . 13
2.3 Descartes’ Rule for Sign of Real Roots . . . . . . . . . . . . . . . . . . . . 13
2.4 Determination of Exact Number of Real Roots − Sturm’s Theorem . . . . 14
2.5 Roots of the Quadratic Equation . . . . . . . . . . . . . . . . . . . . . . . 15
2.6 Solutions of the General Cubic Equation . . . . . . . . . . . . . . . . . . . 16
2.7 Roots of Some Special Cubic Equations . . . . . . . . . . . . . . . . . . . 19
2.8 Gr¨affe’s Root Square Method . . . . . . . . . . . . . . . . . . . . . . . . . 20
2.9 Laguere’s Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.10 Reciprocal Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
2.11 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
2.12 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
2.13 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
3 Solution of General Nonlinear Equations 30
3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
3.2 Bisection Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
3.3 Method of False Position . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
3.4 Secant Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
3.5 Newton–Raphson Method . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
3.6 M¨uller Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
3.7 Chebyshev Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
3.8 Comparison of Iterative Methods . . . . . . . . . . . . . . . . . . . . . . . 49
3.9 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
3.10 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
viiviii Contents
3.11 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
4 Solution of Linear Systems AX = B 56
4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
4.2 Cramer’s Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
4.3 Upper- and Lower-Triangular Systems . . . . . . . . . . . . . . . . . . . . 58
4.4 Gauss Elimination Method . . . . . . . . . . . . . . . . . . . . . . . . . . 60
4.5 Gauss–Jordan Elimination Method . . . . . . . . . . . . . . . . . . . . . . 65
4.6 Inverse of a Matrix by the Gauss–Jordan Method . . . . . . . . . . . . . . 67
4.7 Triangular Factorization or Decomposition Method . . . . . . . . . . . . . 68
4.8 Tridiagonal Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
4.9 Counting Arithmetic Operations . . . . . . . . . . . . . . . . . . . . . . . 70
4.10 Iterative Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
4.11 System AX = B with A being Vandermonde Matrix . . . . . . . . . . . . 78
4.12 Ill-Conditioned Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
4.13 Homogeneous Systems with Equal Number of Equations and Unknowns . 82
4.14 Least-Squares Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
4.15 Singular-Value Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
4.16 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
4.17 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
4.18 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
5 Curve Fitting 93
5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
5.2 Method of Least-Squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
5.3 Least-Squares Straight-Line Fit . . . . . . . . . . . . . . . . . . . . . . . . 96
5.4 Curve Fitting to Exponential and Power-Law Functions . . . . . . . . . . 99
5.5 Curve Fitting to a Sigmoid Function . . . . . . . . . . . . . . . . . . . . . 103
5.6 Polynomial Fit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
5.7 Curve Fitting to Gaussian Functions . . . . . . . . . . . . . . . . . . . . . 108
5.8 Trigonometric Polynomial Fit . . . . . . . . . . . . . . . . . . . . . . . . . 111
5.9 Least-Squares Curve Fitting to y = af(x) + bg(x) . . . . . . . . . . . . . . 113
5.10 Least-Squares Curve Fit to y = f(x)g(c, x) . . . . . . . . . . . . . . . . . 115
5.11 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
5.12 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
5.13 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
6 Interpolation and Extrapolation 123
6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
6.2 Newton Interpolation Polynomial . . . . . . . . . . . . . . . . . . . . . . . 124
6.3 Gregory–Newton Interpolation Polynomials . . . . . . . . . . . . . . . . . 131
6.4 Lagrange Interpolation Polynomials . . . . . . . . . . . . . . . . . . . . . . 134
6.5 Calculation of Polynomial Coefficients . . . . . . . . . . . . . . . . . . . . 139
6.6 Cubic Spline Interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
6.7 Rational Function Approximation . . . . . . . . . . . . . . . . . . . . . . . 140
6.8 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
6.9 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
6.10 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144Contents ix
7 Eigenvalues and Eigenvectors 148
7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
7.2 Some Basic Properties of Eigenvalues and Eigenvectors . . . . . . . . . . . 149
7.3 Applications of Eigenvalues and Eigenvectors . . . . . . . . . . . . . . . . 150
7.4 The Power Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
7.5 Eigenpairs of Symmetric Matrices – Jacobi Method . . . . . . . . . . . . . 153
7.6 Eigenvalues of Symmetric Tridiagonal Matrices – QL Method . . . . . . . 159
7.7 Eigenpairs of General Real Matrices . . . . . . . . . . . . . . . . . . . . . 162
7.8 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
7.9 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
7.10 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
8 Numerical Differentiation 168
8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
8.2 Formulas for First-Order Derivative . . . . . . . . . . . . . . . . . . . . . . 169
8.3 Formulas for Second-Order Derivative . . . . . . . . . . . . . . . . . . . . 171
8.4 Formulas for Third-Order Derivative . . . . . . . . . . . . . . . . . . . . . 171
8.5 Fractional Order Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . 174
8.6 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
8.7 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
8.8 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
9 Numerical Minimization of Functions 182
9.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
9.2 Minimization of One-Dimensional Functions . . . . . . . . . . . . . . . . . 182
9.3 Minimization of Two-Dimensional Functions . . . . . . . . . . . . . . . . . 186
9.4 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
9.5 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
9.6 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
10 Numerical Integration 192
10.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192
10.2 Newton–Cotes Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
10.3 Composite Quadrature Formulas . . . . . . . . . . . . . . . . . . . . . . . 199
10.4 Gauss–Legendre Integration . . . . . . . . . . . . . . . . . . . . . . . . . . 205
10.5 Double Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209
10.6 Fractional Order Integration . . . . . . . . . . . . . . . . . . . . . . . . . . 210
10.7 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
10.8 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
10.9 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
11 Ordinary Differential Equations – Initial-Value Problems 220
11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220
11.2 Euler Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
11.3 Runge–Kutta Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
11.4 Fourth-Order Runge–Kutta Method . . . . . . . . . . . . . . . . . . . . . 236
11.5 Convergence of Runge–Kutta Methods . . . . . . . . . . . . . . . . . . . . 241
11.6 Adaptive Step Size and Runge–Kutta–Fehlberg Method . . . . . . . . . . 244
11.7 Multi-Step Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
11.8 Second-Order Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
11.9 Stiff Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254x Contents
11.10 Solving a Differential Equation with a Noise Term . . . . . . . . . . . . . 260
11.11 Dynamical Systems with Coloured Noise . . . . . . . . . . . . . . . . . . . 261
11.12 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
11.13 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
11.14 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
12 Symplectic Integrators for Hamiltonian Systems 269
12.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
12.2 Hamiltonian Systems and Hamilton’s Equation of Motion . . . . . . . . . 270
12.3 Application of the Explicit Euler and the Runge–Kutta Methods . . . . . 272
12.4 Basic Idea of Symplectic Methods . . . . . . . . . . . . . . . . . . . . . . . 273
12.5 Explicit Euler Method is Not a Symplectic Method . . . . . . . . . . . . . 275
12.6 First-Order Symplectic Algorithms . . . . . . . . . . . . . . . . . . . . . . 276
12.7 A Second-Order Symplectic Algorithm . . . . . . . . . . . . . . . . . . . . 280
12.8 Runge–Kutta Type Algorithms . . . . . . . . . . . . . . . . . . . . . . . . 281
12.9 Other Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287
12.10 A Symplectic Integrator for Spin Systems . . . . . . . . . . . . . . . . . . 289
12.11 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
12.12 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290
12.13 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
13 Ordinary Differential Equations – Boundary-Value Problems 293
13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293
13.2 Shooting Method – Reduction to Two Initial-Value Problems . . . . . . . 294
13.3 Finite-Difference Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295
13.4 Solving Time-Independent Schr¨odinger Equation . . . . . . . . . . . . . . 299
13.5 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302
13.6 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304
13.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304
14 Linear Partial Differential Equations 307
14.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
14.2 Classification of Partial Differential Equations . . . . . . . . . . . . . . . . 307
14.3 Initial and Boundary Conditions . . . . . . . . . . . . . . . . . . . . . . . 309
14.4 Finite-Difference Approximations of Partial Derivatives . . . . . . . . . . 310
14.5 Hyperbolic Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312
14.6 Parabolic Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
14.7 Elliptic Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325
14.8 First-Order Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335
14.9 Time-Dependent Schr¨odinger Equation . . . . . . . . . . . . . . . . . . . . 336
14.10 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 346
14.11 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 346
14.12 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 346
15 Nonlinear Partial Differential Equations 350
15.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350
15.2 Hamilton’s Equation of Motion . . . . . . . . . . . . . . . . . . . . . . . . 351
15.3 Conservation of Symplecticity, Energy and Momentum . . . . . . . . . . . 353
15.4 Multi-Symplectic Integrator . . . . . . . . . . . . . . . . . . . . . . . . . . 355
15.5 Korteweg–de Vries Equation . . . . . . . . . . . . . . . . . . . . . . . . . . 357
15.6 Sine-Gordon Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 364Contents xi
15.7 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372
15.8 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372
15.9 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
16 Fractional Order Ordinary Differential Equations 376
16.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376
16.2 Fractional Order Differential Operators . . . . . . . . . . . . . . . . . . . . 377
16.3 Backward-Difference Methods . . . . . . . . . . . . . . . . . . . . . . . . . 379
16.4 Fractional Euler Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384
16.5 Adams–Bashforth–Moulton Method . . . . . . . . . . . . . . . . . . . . . 390
16.6 A Two-Step Adams–Bashforth Method . . . . . . . . . . . . . . . . . . . . 393
16.7 Nonlinear Fractional Differential Equations . . . . . . . . . . . . . . . . . 397
16.8 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 405
16.9 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 406
16.10 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 410
17 Fractional Order Partial Differential Equations 412
17.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 412
17.2 Time-Fractional Diffusion Equation . . . . . . . . . . . . . . . . . . . . . . 413
17.3 Time-Fractional Advection-Diffusion Equation . . . . . . . . . . . . . . . . 418
17.4 Time-Fractional Wave Equation . . . . . . . . . . . . . . . . . . . . . . . . 420
17.5 Time-Fractional Damped Wave Equation . . . . . . . . . . . . . . . . . . 423
17.6 Time-Fractional Fisher Equation . . . . . . . . . . . . . . . . . . . . . . . 426
17.7 Diffusion Equation with Fractional Order Space Derivative . . . . . . . . . 427
17.8 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 436
17.9 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 437
17.10 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 439
18 Fourier Analysis and Power Spectrum 442
18.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 442
18.2 Fourier Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 442
18.3 Numerical Calculation of the Fourier Coefficients . . . . . . . . . . . . . . 445
18.4 Fourier Transform and Power Spectrum . . . . . . . . . . . . . . . . . . . 445
18.5 Discrete Fourier Transform . . . . . . . . . . . . . . . . . . . . . . . . . . 448
18.6 Fast Fourier Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 449
18.7 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458
18.8 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459
18.9 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459
19 Random Numbers 463
19.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 463
19.2 Importance of Study of Noise and Random Numbers . . . . . . . . . . . . 464
19.3 Uniform Random Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . 465
19.4 Tests for Randomness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 470
19.5 Gaussian (Normal) Distribution . . . . . . . . . . . . . . . . . . . . . . . . 471
19.6 Generation of Random Numbers with a Desired Distribution . . . . . . . 474
19.7 Some Other Types of Random Numbers . . . . . . . . . . . . . . . . . . . 480
19.8 Skewness, Kurtosis and Students t Tests . . . . . . . . . . . . . . . . . . . 487
19.9 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 488
19.10 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 488
19.11 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 490xii Contents
20 Monte Carlo Technique 495
20.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 495
20.2 Evaluation of Definite Integrals . . . . . . . . . . . . . . . . . . . . . . . . 497
20.3 nth Root of a Real Positive Number . . . . . . . . . . . . . . . . . . . . . 501
20.4 Estimation of π . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 501
20.5 Estimation of Value of e . . . . . . . . . . . . . . . . . . . . . . . . . . . . 505
20.6 Electronic Distribution of Hydrogen Atom . . . . . . . . . . . . . . . . . . 507
20.7 Radioactive Decay . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 510
20.8 Diffusion of Neutrons by a Moderating Material Slab . . . . . . . . . . . . 512
20.9 Percolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 513
20.10 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 516
20.11 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 516
20.12 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 518
Answers to Some Selected Problems 521
Index 533Preface
The widespread use of computers, rapid increase of speed of computation in computers
and the growth of sophisticated utility tools for computation led to a great deal of focus
on numerical methods by the scientists and engineers. For a certain class of mathemati￾cal problems even if the construction of exact analytical solutions are feasible, numerical
methods are often preferred to extract first-hand information about the solutions. Sim￾plified user-friendly computer graphics made it easy to visualize and understand complex
numerical solutions. Nowadays, a course on numerical methods is a part of undergradu￾ate and postgraduate programmes in physics, mathematics, computer science, biophysics,
bioinformatics and in various engineering subjects.
This book is developed as a textbook on numerical methods. In addition to covering stan￾dard numerical methods, the book deals with basic aspects of some novel advanced topics
that have applications in various branches of applied science. For each numerical method
the book thoroughly presents basic idea, derivation of formulas, systematic implementation
of procedures and illustrative examples. In addition, wherever possible, the error analysis,
convergence of iterated solutions, efficiency and stability of the methods are addressed.
The pedagogic features of the present book, which are not usually found in the text￾books on numerical methods, are the presentation of symplectic integrators for Hamiltonian
systems, methods for solving time-independent and time-dependent Schr¨odinger equations,
multi-symplectic integrators for nonlinear partial differential equations, fractional order or￾dinary and partial differential equations, fast Fourier transform, generators for random
numbers with desired distributions and Monte Carlo technique. There are numerous col￾lection of problems at the end of each chapter. Most of the problems belong to physical
systems. Solutions to a large number of problems are listed at the end of the book.
The book has 20 chapters. In Chapter 1, certain preliminaries of numerical methods are
provided. Computation of solutions of polynomial and reciprocal equations are treated in
Chapter 2. The next chapter presents various methods for determining a root of general
nonlinear equations. Comparison of various methods is made. Chapter 4 is concerned with
the solution of system of linear equations. Direct and iterative methods such as Gauss
elimination, Gauss–Jordan, triangular factorization, Jacobi and Gauss–Seidel are described.
Solving tridiagonal systems, Vandermonde matrix systems, least-squares and singular-value
problems are also addressed. The least-squares method to fit straight-line, power-law and
exponential functions, polynomials, Gaussian functions and trigonometric polynomials are
illustrated in Chapter 5. Interpolation is discussed in Chapter 6. Newton, Gregory–Newton
and Lagrange interpolating polynomials are derived. Cubic spline and rational function
approximation are also covered. Chapter 7 is devoted to the computation of eigenvalues
and eigenvectors of arbitrary, symmetric and symmetric tridiagonal matrices. The power,
Jacobi and QL methods are presented.
Numerical differentiation of a function is considered in Chapter 8. Various formulas for
first-, second- and third-order derivatives are derived. The case of fractional order derivative
is also dealt. The topic of minimization of one-dimensional and two-dimensional functions is
treated in Chapter 9. In Chapter 10 certain methods for numerical integration of a function
xiiixiv Preface
including fractional order integration are covered. A section on double integration is also
included.
Chapters 11 through 15 are concerned with the numerical integration of differential equa￾tions with integer order derivatives. Chapter 11 is exclusively for the initial-value problem of
ordinary differential equations. The Euler, Runge–Kutta, Runge–Kutta Fehlberg, Adams–
Bashforth–Moulton methods are discussed in depth. A section on stiff equations is included.
Solving differential equations with Gaussian white noise and coloured noise are described.
Chapter 12 deals with classical ordinary differential equations of Hamiltonian systems, that
is, systems in which energy remains a constant. The basic idea of symplectic methods is elu￾cidated. Symplectic Euler, St¨ormer–Verlet and Runge–Kutta type methods are considered
in detail. Chapter 13 is reserved for solving ordinary differential equations with appropri￾ate boundary conditions. The shooting method is illustrated. Solving of time-independent
Schr¨odinger equation is presented. The next chapter is devoted for linear partial differential
equations. Particularly, finite-difference schemes for wave, heat, Laplace and Poisson equa￾tions are derived and discussed. Solving of time-dependent Schr¨odinger equation for certain
interesting potentials is also considered. In Chapter 15, first the multi-symplectic method
of constructing numerical solution for nonlinear partial differential equations is discussed.
Then, the schemes for the ubiquitous Korteweg–de Vries and sine-Gordon equations are
derived and generation of different types of localized solutions is presented.
Numerical integration methods for fractional order differential equations are considered
in Chapters 16 and 17. In Chapter 16 for the fractional order ordinary differential equa￾tions the backward-difference, fractional Euler, Adams–Bashforth–Moulton and a two-step
Adams–Bashforth methods are developed and analyzed for linear and nonlinear systems.
The next chapter is devoted for fractional order partial differential equations. Particu￾larly, schemes for the time-fractional diffusion and advection-diffusion equations, wave and
damped wave equations, Fisher equation are set up and the validity of them are discussed.
A method for the space-fractional diffusion equation is also presented. The accuracies of the
methods are tested by considering the cases of the systems with exact analytical solutions.
A way of constructing stable equilibrium solutions is also described.
Chapter 18 is on Fourier analysis and power spectrum. Computation of Fourier coeffi￾cients is illustrated. A simplified procedure for obtaining fast Fourier transform is described
and power spectra for a certain types of functions and solutions of differential equations are
computed and discussed. In Chapter 19 generation of random numbers with specific empha￾size on random numbers obeying uniform, Gaussian, exponential, Cauchy, dichotomous and
L´evy distributions are described in detail. Tests for randomness are also presented. The fi￾nal chapter is devoted for Monte Carlo technique. The basic idea of Monte Carlo technique
is illustrated through the evaluation of definite integrals and nth root of a real positive
number and estimation of values of π and e. Applications of Monte Carlo technique to the
simulation of electronic distribution in an atom, radioactive decay, diffusion of neutrons in
a material slab and percolation are described in detail.
The book is prepared with great supports from many colleagues, students and friends. In
particular, the author is grateful to K.P.N. Murthy, M. Marudai, A. Tamilselvan, P. Philom￾inathan, A. Venkatesan and V. Chinnathambi for critical reading of some of the chapters,
their suggestions and encouragement. The author thanks his family members for their un￾flinching support, cooperation and encouragement during the course of preparation of this
work.
Tiruchirapalli, Tamilnadu, India S. Rajasekar
October, 2023About the Author
Shanmuganathan Rajasekar was born in Thoothukudi, Tamil￾nadu, India in 1962. He received his B.Sc. and M.Sc. in Physics
both from the St. Joseph’s College, Tiruchirapalli. He was
awarded Ph.D. degree from Bharathidasan University in 1992
under the supervision of Prof. M. Lakshmanan. In 1993, he
joined as a Lecturer at the Department of Physics, Manonma￾niam Sundaranar University, Tirunelveli. In 2003, the book on
Nonlinear Dynamics: Integrability, Chaos and Patterns writ￾ten by Prof. M. Lakshmanan and the author was published
by Springer. In 2005, he joined as a Professor at the School
of Physics, Bharathidasan University. In 2016, Springer has
published the book on Nonlinear Resonances written by the author and Prof. Miguel
A.F. Sanj´uan. Professors U.E. Vincent, P.V.E. McClintock, I.A. Khovanov and the au￾thor compiled and edited two issues of Philosophical Transactions of the Royal Society A
on the theme Vibrational and Stochastic Resonances in Driven Nonlinear Systems in 2021.
He has also edited a book on Recent Trends in Chaotic, Nonlinear and Complex Dynamics
with Professors Jan Awrejecewicz and Minvydas Ragulskis published by World Scientific
in 2022. In 2023, the second revised edition of the books on Quantum Mechanics I: The
Fundamentals and Quantum Mechanics II: Advanced Topics written by the author and
Prof. R. Velusamy were published by CRC Press. In the same year, World Scientific has
published the book on Understanding the Physics of Toys: Principles, Theory and Exer￾cises written by the author, Prof. Miguel A.F. Sanj´uan and Prof. R. Velusamy. His recent
research focuses on nonlinear dynamics with a special emphasize on nonlinear resonances.
He has authored or coauthored more than 120 research papers in nonlinear dynamics.
xv1
Preliminaries
1.1 Introduction
Even long before the invention of computer, numerical methods have been developed and
used in many scientific and engineering problems. However, the invention of computer and
rapid increase in the speed of computation gave a new life to numerical methods. As a
result, a great deal of interest has been focused on the formulation of very effective and
accurate new numerical procedures for mathematical problems and modification of exist￾ing techniques for higher accuracy. Numerical simulations of dynamics of complex systems
have also become possible. It is noteworthy to mention that many phenomena exhibited
by dynamical systems have been first realized in the numerical study of the dynamics of
the systems, and then necessary theories and characteristic measures have been developed.
Nowadays, time-consuming analytical procedures are often replaced by appropriate numer￾ical methods. This book is concerned with the various numerical methods useful for mathe￾maticians, physicists and engineers. Before starting the presentation of numerical methods,
this chapter defines certain basic terms in numerical computation and mathematics which
are essential to follow the various methods presented in this book.
1.2 Binary Number System
In everyday life the decimal number system is used. In the decimal system, the base is 10.
That is, the number 382 can be expressed as
(382)10 = 3 × 102 + 8 × 101 + 2 × 100 . (1.1)
The number 0.382 can be written as
(0.382)10 = 3 × 10−1 + 8 × 10−2 + 2 × 10−3 . (1.2)
Computers, however, perform arithmetic calculations, using the binary number system. The
decimal numbers given as input to a computer are converted to machine numbers with digits
0 and 1 only. The base of the binary number system is 2, and in this system, a number is
represented in terms of only two digits 0 and 1 called bits. The numbers 29 and 0.90625 are
represented in the binary system as
29 = 16 + 8 + 4 + 1
= 1 × 24 + 1 × 23 + 1 × 22 + 0 × 21 + 1 × 20
= (11101)2 (1.3)
DOI: 10.1201/9781032649931-1 12 Preliminaries
and
0.90625 = 1 × 2−1 + 1 × 2−2 + 1 × 2−3 + 0 × 2−4 + 1 × 2−5
= (0.11101)2 . (1.4)
In general, a number M can be written in the binary form as
M = bn−1 × 2n−1 + bn−2 × 2n−2 + ··· + b0 × 20
+b−12−1 + ··· + b−m × 2−m
= bn−1bn−2 ...b0.b−1 ...b−m , (1.5)
where bi’s take the values 0 or 1. Note the binary point ‘·’ between b0 and b−1.
The octal system of representation of numbers uses the eight digits 0, 1, 2, 3, 4, 5, 6 and
7 while the hexadecimal system uses the digits 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 and the alphabets
A, B, C, D, E, F for the numbers 10, 11, 12, 13, 14, 15, respectively.
1.3 Floating Point Arithmetic and Significant Digits
Computers have both an integer mode and a floating-point mode for representing numbers.
The integer mode is used for performing calculations that are integer values. Computers
usually perform scientific calculations in floating point arithmetic, and a real number is
represented in a normalized floating point binary form. That is, computers store a real
number x in the binary form as
x = ± (·d1d2 ...dn)β βe , (1.6)
where (·d1d2 ...dn)β is called mantissa and e is an integer called the exponent. In most
of the computers β = 2. Other values of β used in some computers are 10 and 16. The
term ‘normalized’ means that the leading digit d1 is always nonzero unless the number
represented is 0. That is, d1 = 0 or d1 = d2 = ... = dn = 0. The representation 0.00123
is not in normalized form. In the normalized form it reads as 0.123 × 10−2. In the form
x = p.d1d2 ...dn, p is called characteristic. The terms characteristic and mantissa were
suggested by Henry Briggs in 1624. Mantissa is a Latin word of Etruscan origin, meaning a
makeweight, a small weight added to a scale to bring the weight to a desired value [1].
Computers using 32 bits to represent real numbers in single-precision use 8 bits for the
exponent and 24 bits for the mantissa. The magnitude of the numbers represented in this
way has the range
2.938736E − 39 (2−128) to 1.701412E + 38 (2127).
Note that in the above the numbers are represented with six decimal digits of numerical
precision. A 48-bit computer uses 8 bits for the exponent and 40 bits for the mantissa. In
this case, the range of real numbers is
2.9387358771E − 39 (2−128) to 1.7014118346E + 38 (2127)
with 11 decimal digits precision. If a computer has 64 bits double-precision real numbers,
it might use 11 bits for the exponent and 53 bits for the mantissa. They can represent real
numbers in the range
5.562684646268003 × 10−309 
21024
to 8.988465674311580 × 10307 
21023Type of Errors 3
with about 16 decimal digits of precision. Calculation in double-precision usually requires
more memory space and double the running time (that is, calculation slows down) as com￾pared to single-precision.
All the digits beginning with a nonzero digit are called significant digits. For example,
the underlined digits in
n1 = 0.072451 and n2 = 0.006500
are the significant digits of n1 and n2, respectively. A significant digit is said to be correct
if the error n − n
, where n is an approximation of n, does not exceed unity of the order
corresponding to that digit. When
n1 = 0.072451 and n1 − n
1 = 0.000004
then the underlined digits are correct. If n1 − n
1 = 0.0003000, then n
1 is correct only up to
the digits 2, that is n1 = 0.072451. A number is said to be correct to m decimal places if
there are m decimal digits up to the last correct digit. For
n1 = 0.072451 and n1 − n
1 = 0.000300
the number of correct digits is 3.
A loss in significant digits occurs often when a number is divided by a small number.
For example, the actual value of f(x)=1/(1 − x3) for x = 0.80005 is 2.04958. If x is
approximated as 0.8, then f = 2.04918. That is, rounding-off a number in 5th decimal
causes a change in the 4th decimal. The observed loss of significant digits propagates when
the value of f is used in further calculations.
1.4 Type of Errors
All experimental measurements and numerical computations, however, carefully performed
are subject to some kind of errors. Here, the term error does not mean a mistake or a
blunder. An error in a computation means the presence of an unavoidable deviation from
the exact value. One cannot avoid errors; however, one can make the errors as small as
possible.
The outcomes of measurements and numerical computations are often not absolutely pre￾cise due to various limitations of the apparatus/experimenter/adopted method. A knowledge
of possible errors associated with a quantity is of great importance in analysing approxima￾tion methods, and it can often tell us whether a proposed theory or numerical algorithm
or experiment needs refinements [2]. In the following let us discuss the errors occurring in
numerical computations.
1.4.1 Round-Off Error
A given real number x can be converted into a floating point number denoted as fl(x) by
1. rounding or
2. chopping.
Suppose, a computer uses three-decimal digit floating point arithmetic. If the value of a real
number extends beyond three-decimal places, then in rounding only the decimal values up
to the third are retained. Further, if the fourth decimal value is > 5, then 0.001 is added4 Preliminaries
to the number. On the other hand, in chopping, digits appearing after three-decimal places
are dropped and 0.001 is not added even if the fourth decimal value is > 5.
Example:
Values of some fl(x) in rounding and chopping in three-decimal digit floating point arith￾metic are given below:
fl(x) x
rounding chopping
2473 0.247 × 104 0.247 × 104
2478 0.248 × 104 0.247 × 104
1/3=0.3333 ... 0.333 × 100 0.333 × 100
2/3=0.6666 ... 0.667 × 100 0.666 × 100
The difference between x and fl(x) is called round-off error. It depends on the size of x and
is measured relative to x. Cases with |x| ≥ βm and 0 < |x| ≤ βm−n are referred as overflow
and underflow, respectively. Here, m and n are bounds on the exponents. In these cases
either fl(x) is not defined and computation is stopped or fl(x) is represented by a specified
number that is not subject to the rules of arithmetic. Round-off error plays an important
role in numerical computations.
Consider
a = −1.0 e + 20, b = 1.0 e + 20, c = a + b + 1.0, d = a + (b + 1.0) and e = 1.0 + b + a.
What are the values of c, d and e if they are calculated using a computer in double precision
mode? The result is
c = 1.0, d = 0.0 and e = 1.0.
What is happening? Note that the computation of the values of c, d and e involve only
three numbers. There is no mysterious round-off error occurrence in the calculations. In
computing the value of c as a + b = 0 the value of c = a + b + 1.0 becomes 1.0 and is
correct. In the case of computing the value of d, the value of b and the number 1.0 are in
highly different magnitudes. Compared to 1.0 the value of b is 20 orders high. To correctly
represent the value of b + 1.0 the number of significant figures needed is 21 which is beyond
the scope of double precision. The point is that in this example, b + 1.0 is simply b, in the
calculation done by the computer. Consequently,
d = a + (b + 1.0) = a + b = 0.0.
Similarly, the value of e becomes 1.0. As pointed out in [3] the rounding errors are not
random. They are correlated. Sometimes the way computation is performed can lead to an
incorrect value of a quantity.
1.4.2 Truncation Error
Truncation error occurs in the computation of values of certain mathematical functions
such as cos x, ex, (1 + x)−1 from their series expansion. For example, consider the series
expansion of ex:
ex =1+ x +
1
2!x2 + ··· . (1.7)Type of Errors 5
In the computation of ex with x  1 one may use only the first few terms in Eq. (1.7) and
neglect the remaining higher powers of x. That is, the series is truncated. This truncation of
an infinite series into a finite number of terms gives rise to an error which is called truncation
error.
Consider the evaluation of  0.2
0
exdx. Its true value is 0.2214027... . Suppose, approxi￾mate ex as 1 + x + x2/2!. Use of this in the integral gives the value of it as 0.2213333... .
The error
0.2214027 ... − 0.2213333 ... = 6.94 ... × 10−5
is called the truncation error . Next, the number 2/3=0.6666 ... cannot be represented by
finite number of digits. Consequently, an error occurs in representing this number. Numbers
like π,
√3, √2 and 1/3 require strictly infinite digits. However, in a computer they are
approximated by finite digit numbers leading to an unavoidable error.
1.4.3 Discretization Error
In the numerical integration of a function ( b
a f(x)dx) the interval [a, b] is divided into
number of subintervals with width h. Similarly, in the numerical integration of an ordinary
differential equation the interval between the starting and the end values, a and b, respec￾tively, of the independent variable, say t, is discretized with step size h. This discretization
leads to an error and is called discretization error [4-6]. Consider the case of an ordinary
differential equation. Denote Xode and Xdis as the exact solution of the given differential
equation and the solution of the discretized equation, respectively. Then, the discretized
error, Edis, is given by
Edis = Xode − Xdis. (1.8)
The discretization error can be greatly reduced by the Richardson extrapolation. For details
of the Richardson extrapolation see Subsection 13.4.2.
1.4.4 Noise or Random Error
Slight variations in a measurable quantity can be observed when an experiment is repeated
in apparently identical conditions. These errors are called random errors. The errors that
cannot be revealed by repeating the measurements are called systematic errors. Random
errors may be caused by small fluctuations in certain quantities which are assumed to
remain constant. Examples include small changes in room temperature, pressure, resistivity
of a resistor and applied electric and magnetic fields. The distribution of random errors is
approximately Gaussian. By statistically analyzing the spread, that is, standard deviation
in results, a very reliable estimate of random error can be obtained.
1.4.5 Measurement of Error
To describe the accuracy of a result, one may estimate either absolute error or relative
error. Suppose, x is an approximation to the true value x∗. The quantity |x − x∗| is called
an absolute error while |x − x∗|/|x∗| is referred as a relative error. The absolute error is
simply the absolute difference between the true and approximate values whereas the relative
error is a portion of the true value. In any error analysis measuring relative error is often
desirable, particularly, when |x∗|  1 or |x∗|  1. This is clear from the following examples.6 Preliminaries
Example 1:
Let us calculate the absolute and relative errors for x∗ = 105 with x = 100005. By obser￾vation, one can conclude that x is a good approximation to x∗. Let us see what one may
conclude by calculating the errors:
Absolute error = 5.
Relative error = 5 × 10−5.
Based on absolute error one can conclude that x is far away from x∗ whereas the smallness
of relative error implies x is close to x∗.
Example 2:
Consider the numbers x∗ = 1 × 10−5 and x = 6 × 10−5. Now,
Absolute error = 5 × 10−5.
Relative error = 5.
In this example, x is 6 times higher than x∗ and hence it is not at all a good approximation to
x∗. But the absolute error says x is a good approximation to x∗. The relative error correctly
predicts our conclusion. In the above two examples, disagreement is with the conclusions
drawn based on absolute error while agreement is with that of relative error. Therefore,
in any numerical error analysis, relative error is often desirable over absolute error. The
relative error is frequently given in percentage.
Let us list some of the properties associated with relative error.
1. If a given number is correct up to n significant figures and k is the first significant
digit of the number then the relative error is less than k × 10(1−n)
.
2. The total relative error in the product of n numbers is equal to the sum of the
individual relative errors.
3. The total relative error in x/y is equal to the sum of the relative errors of x and
y.
4. The relative error of xm is m times the relative error of x.
5. The relative error of x1/m is 1/m times the relative error of x.
1.5 Periodic and Differentiable Functions and Series
A function f(x) is said to be periodic with period T if
f(x + T) = f(x) (1.9)
for all x. The trigonometric functions sin x and cos x are periodic functions with period
T = 2π. A function f(x) with period T not equal to 2π can always be transformed into aRolle’s, Intermediate-Value and Extreme-Value Theorems 7
function g(x) of period 2π by the change of variable T x/(2π). This can be easily verified:
g(x + 2π) = f
T x
2π + T

= f
T x
2π

= g(x). (1.10)
A function f(x) is said to be differentiable at a point x0 if
f
(x0) = limx→x0
f(x) − f(x0)
x − x0
(1.11)
exists. Here, f is called first derivative of f at x = x0.
A most useful series expansion of a function f(x) is the Taylor series. If the first n + 1
derivatives of f(x) are continuous in the interval [a, b] then f(x) can be expressed near
x0 ∈ [a, b] in series as
f (x + x0) = f (x0)+(x − x0) f (x0) + 1
2! (x − x0)
2 f(x0) + ··· . (1.12)
In Eq. (1.12) f
(x0), f(x0), ... are the successive derivatives of f(x) evaluated at x0. When
x0 = 0, the series is called Maclaurin series.
1.6 Rolle’s, Intermediate-Value and Extreme-Value Theorems
The Rolle’s theorem states that if f(x) is continuous and differentiable in the interval
x ∈ [a, b] with f(a) = f(b) = 0 then f
(c) = 0 for some c with a<c<b. The function
f(x) = x2−5x+6 has zeros at x = 2 and 3, that is f(2) = f(3) = 0. According to the Rolle’s
theorem, there exists a point c between 2 and 3 with f
(c) = 0. For the above function, this
point is computed as 2.5.
Suppose, f(x) is continuous in the interval [a, b] and m is any number between f(a) and
f(b). Then, there exists a value of x, say c, with a<x = c<b such that f(x = c) = m.
This is the intermediate-value theorem for a function. As an example, let f(x) = sin x,
x ∈ [−π/2, π/2]. Here, f(−π/2) = −1 and f(π/2) = 1. Choose m as 0.5 then c = 0.52359 ...
which falls between −π/2 and π/2.
If g(x) is continuous and does not change its sign on an interval [a, b] then, there exists
a number c with a<c<b such that
 b
a
f(x)g(x) dx = f(c)
 b
a
g(x) dx . (1.13)
This is known as the intermediate-value theorem or mean-value theorem for an integral. To
verify this theorem, consider the integral  2
1
x(x2 − 2)dx with g(x) = x. The actual value of
the integral is 3/4. By the above theorem, the integral is equivalent to I = 
c2 − 2
  2
1
xdx
and I = 3(c2 − 2)/2. The value of I is 3/4 if c = √2.5. This value of c lies in the interval
[1, 2].8 Preliminaries
The extreme value theorem is the following. For a continuous function f(x) defined in
the interval [a, b] there exists a lower bound and an upper bound (denoted as mmin and
mmax, respectively) at two numbers c1 and c2 with c1, c2 ∈ [a, b] such that
mmin = f(c1) ≤ f(x) ≤ f(c2) = mmax . (1.14)
mmin and mmax can also be expressed as
mmin = f(c1) = min
a≤x≤b
[f(x)], (1.15a)
mmax = f(c2) = max
a≤x≤b
[f(x)] . (1.15b)
For f(x) = sin x, x ∈ [0, 2π] mmin = −1 at x = c1 = −π/2 and mmax = +1 at
x = c2 = π/2. In this example
mmin = f(−π/2) ≤ f(x) ≤ f(π/2) = −1 . (1.16)
When f(x) = cos x, x ∈ [0, 2π] then
mmin = f(π) ≤ f(x) ≤ f(0) = −1 . (1.17)
1.7 Iterations and a Root of an Equation
Numerical methods of finding the roots of nonlinear equations are iterative methods. There￾fore, first define an iteration.
1.7.1 Iterations
Consider the difference equation xn+1 = f(xn), where n = 0, 1, 2,..., f(xn) is a known
function of xn and xn can take any value in the interval [−∞, ∞]. Assume that x0 is given.
Starting with the given x0 calculate x1 as f(x0). Using the obtained x1 the value of x2 can
be computed as f(x1) and so on. This process of obtaining the sequence x1, x2,... is known
as an iteration. When
limn→∞ xn = x∗ (1.18)
then x∗ is called the limit of the sequence. When x∗ is finite, the sequence is a convergent
series. If xn is assumed as an approximation of the solution of a problem, for example, a
root of the equation f(x) = 0, then the sequence x0, x1,... is assumed to be converging
to an actual root. When a method of finding the solution of a problem involves iteration
as described above, then it is called an iterative method. There are iterative methods for
finding a root of a function, solution of a system of equations, eigenvalues of a square matrix
and so on.
1.7.2 Root of an Equation
Consider the function f(x) with one variable x. Assume that f(x) is a continuous function
of x. A value of x, say x∗, for which f(x∗) = 0 is called a root of the equation f(x)=0
or zero of the function f(x). For example, the roots of x2 − 3x + 2 = 0 are x∗ = 1, 2. If
f(x) can be rewritten as (x − x∗)mg(x) then x∗ is called a multiple root with multiplicity
m. When m = 2, x∗ is said to be a double root.Iterations and a Root of an Equation 9
1.7.3 Convergence Criteria
In both iterative and noniterative methods of solving a problem when a specified algorithm
is applied successively a sequence {xn} is obtained. This sequence is assumed to converge
to an actual root (or a solution) x∗. A sequence {xn} is said to converge to the root x∗ if
limn→∞ |x∗ − xn| = 0 . (1.19)
To check the convergence of the sequence and to accept the solution one or more appropriate
criteria must be used. In the case of finding a root of the equation f(x) = 0, the final xn
should have the property
f(xn) ≈ 0 or |f(xn)| ≤ δf, (1.20)
where δf is a positive small quantity, δf  1 and is called error tolerance in f. This alone
does not guarantee the convergence of {xn}. For some functions which have slow variations
near the roots, |f(x)| may be less than δf for a certain range of values of x which are far
away from x∗. Thus, to ensure the convergence another criterion is required. One may check
whether the consequent iterates xn−1 and xn are sufficiently close. The closeness is checked
with either the absolute error |xn − xn−1| is ≤ δx or the relative error |xn − xn−1|/|xn| is
≤ δx, δx  1.
To summarize, the two useful termination criteria for an iteration process are
1. |f(xn)| ≤ δf and
2. |xn − xn−1| ≤ δx or |xn − xn−1|/|xn| ≤ δx.
The process can be stopped when both the above conditions are met. The values of δf and
δx are crucial. If these values are too small then the iteration may continue forever.
1.7.4 Rate of Convergence
How rapidly the sequence {xn} converge to an actual root in an iterative method? How
does one measure it? A quantity describing the speed or rate of convergence of {xn} is
the order of convergence, R. It is expressed in terms of the errors en = x∗ − xn and en+1 in
successive approximations. It is defined through
limn→∞
|en+1|
|en|
R = A , (1.21)
where R and A are two positive constants. The number A is called an asymptotic error
constant. The values of R equal to 1 and 2 are referred as linear and quadratic order of
convergence, respectively. When R is large the sequence {xn} converge rapidly to x∗. If R
is small ( 1) then {xn} converges slowly. R can be used to characterize the efficiency of
an iteration scheme.
1.7.5 Efficiency Index
Another quantity that can be used to characterize the efficiency of an iterative method is
the efficiency index α. It is defined as
α = R1/m , (1.22)
where R is the order of convergence of the method and m is the number of function and
derivative evaluations in each step of the iteration. Like the order of convergence if the value
of α is larger then the method is more efficient.10 Preliminaries
1.8 Concluding Remarks
In this chapter, some preliminaries related to the numerical methods and the error analysis
are discussed. As the calculators and the computers perform computations with a finite pre￾cision arithmetic, an error occurs in the results. Errors also occur due to the approximations
used in obtaining appropriate schemes or formulas to solve the problems numerically. To
minimize the errors one can modify the schemes or introduce additional calculations in the
numerical methods. When the calculations are done using a computer it is better to declare
the variables and constants involved in double precision. For the numerical methods, it is
important to study the accuracy of the results, stability of the schemes, advantages and
disadvantages and the applicability of the methods concerned.
1.9 Bibliography
[1] E. Maos, e: The Story of a Number. University Press, Hyderabad, 1994. pp.24.
[2] J.R. Taylor, An Introduction to Error Analysis. University Science Books, Cali￾fornia, 1982.
[3] A. Gezerlis and M. Williams, Am. J. Phys. 89:51, 2021.
[4] G. Strang, Computational Science and Engineering. Wellesley-Cambridge Press,
Wellesley, 2007.
[5] C. Roy, Review of discretization error estimators in scientific computing. In
48th AIAA Aerospace Sciences Meeting Including the New Horizons Forum and
Aerospace Exposition, 2010. pp.126.
[6] A. Greenbaum and T.P. Chartier, Numerical Methods: Design, Analysis and
Computer Implementation of Algorithms. Princeton, University Press, 2012.
1.10 Problems
1.1 Write 382.382 in the form of dn−110n−1.
1.2 What is the decimal equivalent of 11101.11101?
1.3 Try to express the number 1.9 in the binary representation. What do you observe?
1.4 Represent the binary number 11010.11 in decimal, octal and hexadecimal systems.
1.5 Compute e0.1 and e0.9 using first two terms in the Taylor series of ex. Also,
compute the percentage of the relative error in the approximation.
1.6 Calculate f(x)=1/(1 − x3) for x1 = 0.8 and x
1 = 0.80005 and then compute
g(f(x)) = f + f 2. Observe the propagation of error in x.
1.7 Calculate the values of f(x)=1/x2 for x = 0.1, 0.10005, 10 and 10.00005. What
do you notice?Problems 11
1.8 Write a program to calculate x1, x2,...,x100 by iterating the equation xn+1 =
axn(1 − xn) with x0 = 0.3 and a given value of a. Calculate x1, x2,...,x100 for
a = 3, 3.3, 3.5 and 3.9. What do you observe?
1.9 Verify the Rolle’s and intermediate-value theorems for the function x3−2x2−x+2,
x ∈ [a, b], with an appropriate choice of a and b.
1.10 Verify the intermediate-value theorem for  2
1
sech(x)exdx.2
Solutions of Polynomial and Reciprocal Equations
2.1 Introduction
Finding the solutions of nonlinear equations is an important mathematical problem. The
present chapter is restricted to the discussion on solutions of polynomial equations. The next
chapter describes several iterative methods to find roots of general nonlinear equations. In
everyday life quadratic equations occur in the calculations of kinetic energy of an object,
surface area of a bounded space or objects, time to reach the maximum height of a moving
object and the distance travelled by an object and so on. Cubic and higher-order polynomial
equations are found in many places in mathematics, physics and engineering. Examples
include potentials of anharmonic oscillators, van der Waals equation connecting pressure,
volume and temperature of a substance, the compressibility of a gas, the chemical equilibria
of water and carbon dioxide and the characteristic equation of a square matrix. The Hermite
and the Legendre polynomials have notable applications in electromagnetic theory and
quantum mechanics. For some details of applications of quadratic and cubic equations, one
may refer to the refs. [1-3].
For polynomial equations, more information about the roots can be extracted without
solving the equations. Exact formulas to determine all the roots of quadratic and cubic
equations are available. For a quartic equation also formula exists for exact roots which will
not be considered in this book. For polynomials of degree greater than four exact formulas
for the roots are not available in the mathematical literature. However, there is a numerical
procedure namely, Gr¨affe’s root squaring method, which can be used to find all the roots of
a given polynomial equation. Further, certain classes of polynomial equations can be solved
exactly by rewriting them in terms of a new variable y → q/x. The resulting equations are
called reciprocal equations.
The present chapter first describes determining analytically
1. a region or an interval enclosing all the roots and
2. the number of real and complex roots.
Next, the determination of roots of quadratic and cubic equations is considered. Then, the
Gr¨affe’s root squaring method of finding real and complex roots of polynomial equations is
presented. Finally, solving certain reciprocal equations is discussed. Though these methods
are analytical methods, they are presented in this book since roots of cubic and lower-order
polynomial equations are required in the numerical computation of certain quantities.
DOI: 10.1201/9781032649931-2 12Determination of the Region Enclosing all the Roots 13
2.2 Determination of the Region Enclosing all the Roots
Let us consider a polynomial equation of degree n of the form
P(x) = xn + a1xn−1 + a2xn−2 + ··· + an−1x + an = 0, (2.1)
where ai’s are all real and an = 0. If an = 0, an−1 = 0, ··· , an−p = 0 then Eq. (2.1) can be
rewritten as
xp+1 
xn−(p+1) + a1xn−(p+2) + ··· + an−(p+1)
= 0 (2.2)
so that x = 0 is a root of order p + 1. Then, defining n − (p + 1) = m, from the above
equation one can write
xm + a1xm−1 + ··· + am = 0 (2.3)
which is of the form of Eq. (2.1) and am = 0.
If r1, r2, ··· , rn are the roots of Eq. (2.1) then it can be rewritten as
(x − r1) (x − r2)...(x − rn)=0. (2.4)
With the convenient notations
[ri] = n
i=1
ri, [rirj ] = n
i=1
n
j>i
rirj , [r1r2 ··· rn] = n
i=1
ri (2.5)
Eq. (2.4) takes the form
xn − [ri] xn−1 + [rirj ] xn−2 −··· + (−1)n [r1r2 ...rn]=0. (2.6)
Now, comparison of Eqs. (2.1) and (2.6) gives a1 = − [ri], a2 = [rirj ] and so on. If |r1| 
|r2|···|rn| then one may write
[ri] = ri ≈ r1 and [rirj ] ≈ r1r2. (2.7)
From Eq. (2.5), for any root rk
r2
k ≤ r2
1 + r2
2 + ··· + r2
n =
ri
2
− 2
rirj = a2
1 − 2a2
or
|rk| ≤ 
a2
1 − 2a2 . (2.8)
Any root of Eq. (2.1) must lie in the interval 
−
a2
1 − 2a2 ,
a2
1 − 2a2

. Therefore, one
need not search for a root outside this interval. The complex roots occur in conjugate pairs
since the coefficients ai’s in Eq. (2.1) are all assumed as real. If n in Eq. (2.1) is an odd
integer then it always has one real root.
2.3 Descartes’ Rule for Sign of Real Roots
The number of positive real roots of P(x) = 0 cannot exceed the number of sign changes in
Eq. (2.1). The number of negative real roots cannot exceed the number of sign changes in
P(−x) = 0.14 Solutions of Polynomial and Reciprocal Equations
Example:
The exact roots of
P(x) = x3 + 2x2 − x − 2=0 (2.9)
are x∗ = −2, −1, 1. According to the Descartes’ rule, the number of sign changes in P(x)
is one and the number of sign changes in P(−x) = −x3 + 2x2 + x − 2 = 0 is 2. Therefore,
the numbers of positive and negative real roots cannot exceed 1 and 2, respectively. In the
above example, the prediction based on the Descartes’ rule is in agreement with the exact
result. However, the rule does not give the exact number of real roots but gives only their
upper bounds. The exact number of real roots of Eq. (2.1) can be determined by the Sturm’s
theorem presented in the next section.
2.4 Determination of Exact Number of Real Roots − Sturm’s
Theorem
Let P(x) is a polynomial of degree n and P1(x)=dP(x)/dx is its first-order derivative.
Denote the remainder of (−P)/P1 as P2(x), the remainder of (−P1)/P2 as P3(x) and so on
until a constant is reached. The obtained sequence of functions
P(x), P1(x), ..., Pn(x) (2.10)
are called Sturm functions or Sturm sequences.
Theorem:
The number of real roots of the equation P(x)=0 in the interval [a, b] is equal to the
difference between the number of changes of sign in the Sturm sequence at x = a and b
provided P(a) = 0 and P(b) = 0.
Example:
Consider the equation
P(x) = x3 − 2x2 − x +2=0. (2.11)
Its exact roots are x = −1, 1, 2. The number of real roots is 3. The Sturm functions for
Eq. (2.11) are
P(x) = x3 − 2x2 − x + 2,
P1(x)=3x2 − 4x − 1,
P2(x) = 2
9
(7x − 8),
P3(x) = 81
49.
For the interval [−2, 3] the Sturm sequence at x = −2 and 3 are obtained as
At x = −2 : −12, 19, −44/9, 81/49.
At x =3 : 8, 14, 26/9, 81/49.Roots of the Quadratic Equation 15
The number of sign changes at x = −2 is 3 while it is 0 at x = 3. The difference in the
numbers 3 and 0 is 3. Therefore, the number of real roots lying in the interval [−2, 3] is 3
which is in fact true. Next, consider the interval [0, 3]. The number of exact roots in this
interval is two. The Sturm sequence at x = 0 and 3 are
At x =0 : 2, −1, −16/9, 81/49.
At x =3 : 8, 14, 26/9, 81/49.
The number of sign changes in the above two sequences are 2 and 0, respectively. Therefore,
the number of real roots of Eq. (2.11) in the interval [0, 3] must be 2 which is again true.
If P(x) = 0 has a multiple root then Pn(x) in the Sturm sequence will be zero. In
this case obtain a new sequence by dividing all the Sturm functions by Pn−1(x). The new
sequence can be used to obtain the number of real roots in an interval [a, b] without taking
multiplicity into account as shown in the following example.
Example:
Consider the equation
P(x) = x3 − 4x2 + 5x − 2=0. (2.12)
Its exact roots are 1, 1, 2 and 1 is a double root. The Sturm functions are
P(x) = x3 − 4x2 + 5x − 2,
P1(x)=3x2 − 8x + 5,
P2(x) = 2
9
(x − 1),
P3(x)=0.
The occurrence of P3(x) = 0 implies that Eq. (2.12) has a multiple root. For the interval
[0, 3] the Sturm sequences at x = 0 and x = 3 are
at x =0 : −2, 5, −2/9, 0,
at x =3 : 4, 8, 4/9, 0,
respectively. The difference in the number of sign changes in the two sequences is 3. Thus,
Eq. (2.12) has 3 real roots in the interval [0, 3]. Now, to check the multiplicity, divide P,
P1, P2 by P2 and obtain the new sequence
9
2

x2 + 3x + 8
, 9
2 (3x − 1), 1.
The sequence at x = 0 and 3 are:
At x = 0 : 36, −9/2, 1.
At x = 3 : 117, 36, 1.
The number of sign changes is 2 and hence the number of real roots lying in the interval
[0, 3] is 2. Further, one root must be a double root.
2.5 Roots of the Quadratic Equation
The formula for the roots of the quadratic equation
ax2 + bx + c = 0 (2.13)16 Solutions of Polynomial and Reciprocal Equations
is
x± = −b ± √b2 − 4ac
2a . (2.14)
The roots are real if b2 − 4ac ≥ 0 and are complex conjugates if b2 − 4ac < 0. Note that
the discriminant b2 − 4ac discriminates the roots of the quadratic equation. The equivalent
formulas for x± are
x± = −2c
b ± √b2 − 4ac . (2.15)
When |b| ≈ √
b2 − 4ac care must be taken to avoid loss of precision. If b > 0 then the
formulas for x± are
x+ = −2c
b + √b2 − 4ac
, x− = −b − √b2 − 4ac
2a . (2.16)
For b < 0 one should use the formulas
x+ = −b + √b2 − 4ac
2a
, x− = −2c
b − √b2 − 4ac . (2.17)
To illustrate this, consider Eq. (2.13) with a = 1, b = 100.12 and c = 2.2345. The exact
roots with five decimal digit accuracy are x+ = −0.02232 and x− = −100.09. Now, compute
the roots using Eq. (2.14) with five decimal digit floating point arithmetic. Here,
b2 = 10024, b2 − 4ac = 10015, b2 − 4ac = 100.07 (2.18)
and
x+ = −0.025. (2.19)
The percentage of relative error is
| − 0.02232 − (−0.025)|
|0.02232| × 100 = 12%. (2.20)
The obtained root differs from the exact root in the third decimal place. This loss of sig￾nificant digits can be avoided by using the formula given by Eq. (2.16). This formula gives
x+ = −0.02232 which is accurate to five digits.
2.6 Solutions of the General Cubic Equation
All the roots of a cubic equation (a polynomial of degree 3) of the form
y3 + ay2 + by + c = 0 (2.21)
can be determined directly. Elimination of y2 in Eq. (2.21) by the substitution
y = x − a
3 (2.22)
gives the equation
x3 − qx − r = 0, (2.23a)Solutions of the General Cubic Equation 17
where
q = 1
3
a2 − b, r = − 2
27
a3 +
1
3
ab − c. (2.23b)
The roots of Eq. (2.23) are given separately for the following five cases [4].
1. 27r2 ≤ 4q3, q = 0
2. 27r2 > 4q3, q = 0
3. q = 0, r = 0
4. q = 0, r = 0
5. q = 0, r = 0
Case 1: 27r2 ≤ 4q3, q = 0
In this case, Eq. (2.23) has three real roots:
x1 = 2q/3 cos(φ/3), (2.24a)
x2 = −2
q/3 cos [(π − φ)/3], (2.24b)
x3 = −2
q/3 cos [(π + φ)/3], (2.24c)
where
φ = cos−1

(3/q)
3/2r/2

. (2.24d)
Case 2: 27r2 > 4q3, q = 0
Equation (2.23) has one real root and two complex roots. Now, one has to consider four
subclasses.
(a) q, r > 0
The real root is given by
x1 = 2q/3 cosh(φ/3), φ = cosh−1

(3/q)
3/2r/2

. (2.25)
To determine the pair of complex roots divide Eq. (2.23) by (x−x1) and obtain the quadratic
equation
x2 + x1x + 
x2
1 − q

= 0. (2.26)
Then, the two complex roots are written as
x2,3 = 1
2

−x1 ±

x2
1 − 4 (x2
1 − q)

. (2.27)
(b) q < 0, r > 0
The real root is given by
x1 = 2(−q/3)1/2 sinh(φ/3), φ = sinh−1

(−3/q)
3/2 r/2

. (2.28)
Then, the two complex roots are obtained from Eq. (2.27).18 Solutions of Polynomial and Reciprocal Equations
(c) q < 0, r < 0
The change of variable x → −x in Eq. (2.23) gives the equation x3 − qx + r = 0 which can
be rewritten as x3 − qx − (−r) = 0. This is simply the case (2b). After obtaining the roots
change their signs.
(d) q > 0, r < 0
The substitution x → −x in Eq. (2.23) gives an equation with q > 0, r > 0 which is the
case (2a).
Case 3: q = 0, r = 0
Equation (2.23) has one real root and a pair of complex roots. The real root is given by
x1 = sgn(r)(|r|)
1/3, (2.29)
where sgn(r) is the sign of r. The complex roots are given by Eq. (2.27) with q = 0.
Case 4: q = 0, r = 0
In this case, Eq. (2.23) becomes x3 = 0 and it has a real root x = 0 with multiplicity 3.
Case 5: q = 0, r = 0
When r = 0 and q = 0 Eq. (2.23) becomes x(x2 − q) = 0. For q > 0, the equation has three
real roots and are x1 = 0, x2,3 = ±√q. When q < 0 it has a real root x1 = 0 and two pure
imaginary roots x2,3 = ±i
√−q.
Finally, for all the cases the roots of Eq. (2.21) are obtained from Eq. (2.22).
Example:
The equilibrium points of an anharmonic oscillator are the roots of the equation y3 − y2 −
4y + 4 = 0. Find all the equilibrium points.
First, eliminate y2 in the given equation and write it in the standard form, Eq. (2.23). For
the given equation a = −1, b = −4, c = 4. Therefore, the change of variable y = x + 1/3
converts the given equation into
x3 − qx − r = 0, q = 13/3, r = −70/27. (2.30)
Since 27r2(= 181.48 ...) < 4q3(= 325.48 ...) Eq. (2.30) has three real roots. From
Eq. (2.24d) φ is found as 2.414 radians. Then, the roots computed from Eqs. (2.24) are
x1 = 1.66666 ..., x2 = −2.33333 ..., x3 = 0.66666....
Finally, the roots of the given equation are
y1 = 2, y2 = −2, y3 = 1.
One can find all the roots of a cubic equation from a plot of y versus f(y). The procedure
is the following [5]. Draw a graph of y versus f(y). The curve must pass through at least
one point on the y-axis since at least one root should be real. Consider Fig. 2.1, where
there is only one y intercept and hence only one real root. If a given equation has three
real roots then f(y) curve must intersect the y-axis at three points which are the exact
roots of the equation. Complex roots can also be determined. Denote the location of theRoots of Some Special Cubic Equations 19




 
FIGURE 2.1
Determination of complex roots of a cubic equation.
real root as R and the origin as O. The distance OR is the real root. Draw a straight-line
passing through the point R and also touching a point on the curve. This is the line passing
through R and the local minimum of the curve. Mark the touching point on the curve as
T and the corresponding y-coordinate as M as shown in Fig. 2.1. Define OM = b. Measure
TM(= f(OM)), RM = OM − OR and then compute
a =
 f(OM)
OM − OR. (2.31)
Now, the two complex roots are a ± ib.
2.7 Roots of Some Special Cubic Equations
The determination of roots of cubic equations by the formulas given in the previous section
requires trigonometric, hyperbolic and their inverse functions evaluations of certain quan￾tities. If the roots of a given equation has some special characteristic properties then they
can be obtained in a simpler way. This is discussed in this section. As seen in Section 2.2 if
r1, r2, ..., rn are the n roots of an nth-order polynomial equation then it can be rewritten
as Eq. (2.6). For a cubic equation, (2.6) becomes
x3 − (r1 + r2 + r3) x2 + (r1r2 + r2r3 + r3r1) x − r1r2r3 = 0 . (2.32)
Comparison of Eq. (2.32) with Eq. (2.21) with y = x gives
a1 = − (r1 + r2 + r3), a2 = r1r2 + r2r3 + r3r1, a3 = −r1r2r3. (2.33)
These relations can be used to determine the roots of the cubic equation if some details of
the roots are known.
For an early history of the solutions of certain special types of cubic equation one may
refer to the refs. [5,6].20 Solutions of Polynomial and Reciprocal Equations
Example 1:
If one root of the equation x3 + 6x + 20 = 0 is 1 − 3i find the other roots.
Since complex roots occur in pairs the second root is 1 + 3i. The third root is then obtained
from the first relation in the Eq. (2.33). It is calculated as −2.
Example 2:
Solve the equation x3 − 9x2 + 23x − 15 = 0 whose roots are in arithmetic progression.
Let the roots are r − α, r, r + α. The sum of the roots are
a1 = −r − α + r + r + α = −9
which gives r = 3. Therefore, x = 3 is one root. To determine α consider the third relation
in Eq. (2.33). One obtains α = ±2. For α = −2 the roots r − α, r, r + α are 5, 3, 1. For
α = 2 the roots are 1, 3, 5. Thus, the roots are 1, 3, 5.
2.8 Gr¨affe’s Root Square Method
The Gr¨affe’s root square method was introduced by Germinal Pierre Dandelin (1794-1847)
in 1826 and independently by Karl Heinrich Gr¨affe (1799-1873) in 1837. It can be used to
determine all real and complex roots of a given polynomial equation [4]. In this method,
a new equation is formed whose roots are some higher power of the roots of the given
equation. That is, if r1, r2, ..., rn are the roots of the original equation then the roots of
the new equation are rs
1, rs
2, ..., rs
n. For large s the high powers of the roots are widely
separated. In this case, the roots are obtained by a simple process.
First, separate the even and odd powers of x in Eq. (2.1) and square the both. This
gives
x2n − 
a2
1 − 2a2

x2n−2 + ··· + (−1)n a2
n = 0. (2.34)
Introducing the change of variable z = −x2 Eq. (2.34) becomes
zn + b1zn−1 + ··· + bn−1z + bn = 0, (2.35)
where
b1 = a2
1 − 2a2,
b2 = a2
2 − 2a1a3 + 2a4,
b3 = a2
3 − 2a2a4 + 2a1a5 − 2a6,
.
.
. (2.36)
bn−1 = a2
n−1 − 2an−2an,
bn = a2
n.
Looking at Eqs. (2.36) one can easily identify a simple rule to compute the coefficients bi’s.
This is left as an exercise to the readers.Gr¨affe’s Root Square Method 21
2.8.1 Determination of Real Roots
Assume that −r2
1, −r2
2, ..., −r2
n are the roots of Eq. (2.35). Then, the roots of the Eq. (2.34)
are r1, r2, ..., rn. Repeating the above squaring process m times one has
zn − B1zn−1 + B2zn−2 − ··· + (−1)nBn = 0. (2.37)
Its roots Ri are related to the roots of Eq. (2.34) as
Ri = −r2m
i , i = 1, 2, . . . , n. (2.38)
If one assumes
|r1| > |r2| > ··· > |rn| (2.39)
then
|R1||R2|···|Rn|. (2.40)
Referring to Eqs.(2.37) one can write
−B1 = Ri ≈ R1,
B2 = RiRj ≈ R1R2,
−B3 = RiRjRk ≈ R1R2R3,
.
.
. (2.41)
(−1)nBn = R1R2 ··· Rn.
From Eq. (2.41) the following simple formula for Ri’s is obtained:
Ri = − Bi
Bi−1
, i = 1, 2,...,n (2.42)
where B0 = 1. Use of Eq. (2.38) in Eq. (2.42) gives



r2m
i


 = |Bi|
|Bi−1|
. (2.43)
Taking logarithm on both sides of Eq. (2.43) leads to the equation
ln |ri| = 2−m (ln |Bi| − ln |Bi−1|) = α. (2.44)
Then, |ri| is given by
|ri| = eα. (2.45)
In this way, the absolute value of each of the roots can be determined. The sign of a root
is then determined by substituting it in the given equation. ri is positive if P(ri) ≈ 0. ri is
negative if P(−ri) ≈ 0.
2.8.2 Determination of Complex Roots
Suppose, the given equation has a pair of complex roots. Let these roots are ri and ri+1
and are given by ri,i+1 = αiexp(±iφi). In this case the coefficients of xn−i in the successive
squaring fluctuate both in magnitude and sign. For sufficiently large m one has
α2(2m)
i =




bi+1
bi−1




(2.46)22 Solutions of Polynomial and Reciprocal Equations
or
ln α2
i = 1
2m ln




bi+1
bi−1



 = β. (2.47)
The above equation gives
α2
i = eβ. (2.48)
First, determine all the real roots. Next, write the complex pair as ri,i+1 = p ± iq. The sum
of all the roots is then given by
r1 + r2 + ··· + ri−1 + 2p + ri+2 + ··· + rn = −a1. (2.49)
That is,
p = 1
2

−a1 −
R
ri

, (2.50)
where 
R represents sum of all real roots. Since p2 + q2 = α2
i the imaginary part q is
determined from
q =

α2
i − p2. (2.51)
2.8.3 Illustrative Examples
This subsection illustrates finding the solutions of three polynomial equations.
Example 1:
Find all the roots of the equation
x3 + 2x2 − x − 2=0 (2.52)
by the Gr¨affe’s root square method.
For the cubic equation of the form x3 + a1x2 + a2x + a3 = 0 to determine the coefficients
bi’s in Eq. (2.35) rewrite the given equation as
x3 + a2x = − 
a1x2 + a3

.
The left-side contains odd powers of x while the right-side has even powers of x. Squaring
on both sides gives
x6 + a2
2x2 + 2a2x4 = a2
1x4 + 2a1a3x2 + a2
3.
Then, introducing the change of variable z = −x2 the above equation becomes
z3 + z2(a2
1 − 2a2) + z(a2
2 − 2a1a3) + a2
3 = 0.
Rewrite the above equation as
z3 + b1z2 + b2z + b3 = 0.
The computation of bi’s is summarized in Table 2.1. This is with m = 0. Next, starting with
the above equation and repeating the squaring process results in a cubic equation which
corresponds to m = 1 and so on. Since the above obtained equation is similar to the given
equation the bi’s for m ≥ 1 can be obtained in a simple way (how?).Gr¨affe’s Root Square Method 23
TABLE 2.1
Computation of the coefficients bi’s in Eq. (2.35) corresponding to Eq. (2.52). The first row
gives the coefficients of x3, x2, x and x0 in the given equation. The second and third rows
give the coefficients of z3, z2, z and z0. The addition of second and third rows gives the
coefficients bi’s and is indicated.
1 a1 a2 a3
1 a2
1 a2
2 a2
3
−2a2 −2a1a3
1 b1 b2 b3
The coefficients ai’s and bi’s at first few squaring processes are given in Table 2.2. The
roots can be calculated from this table. Let us calculate them for m = 6. |r1| is calculated
as
ln |r1| = 1
26 ln |b1| = 0.6931
or
|r1| = e0.6931.
Next, |r2| is obtained from
ln |r2| = 1
26 (ln |b2| − ln |b1|)=0.0108
as 1.011. Similarly, one obtains |r3| = 0.989. Table 2.3 gives the computed absolute values
of the roots as a function of m.
To determine the signs of the roots use the Descarte’s rule and evaluate P(x) for x = ±ri.
Because P(r1) = 12 and P(−r1) = 0 the sign of the root is negative. P(r2)=0.067 and
P(−r2)=0.0218. P(r2) and P(−r2) both are close to zero which implies that r2 as well
as −r2 may be roots. Similar conclusion is drawn for the third root. The number of sign
changes in P(x) is one and that of P(−x) is two. Therefore, Eq. (2.52) cannot have more
than one positive real root and two negative real roots. Thus, r1 = −2, r2 ≈ −1.011,
r3 ≈ 0.989. The exact roots are −2, −1 and 1.
Example 2:
Find all the roots of the equation
x3 − x2 + 3x +5=0. (2.53)
The coefficients ai’s and bi’s obtained at first few squaring processes are given in table 2.4.
The sign of the coefficient b1 changes with m. That is, the coefficient b1 oscillates which
indicates that Eq. (2.53) has a pair of complex roots. At m = 5, the real root is obtained as
|r3| = 1. Since P(r3) = 8 and P(−r3) = 0 the real root is r3 = −1. Next, let the complex
roots are r1,2 = p ± iq with α2 = p2 + q2. Now,
ln α2 = 1
25 (ln |b2| − ln |b0|)=1.6094424 Solutions of Polynomial and Reciprocal Equations
TABLE 2.2
Calculated values of bi’s at first few squaring processes for Eq. (2.52). For each value of m
the table is formed.
m 2m 1 a1 a2 a3
0 1 1 2 −1 −2
1 4 1 4
+2 +8
1 2 bi’s → 1 6 9 4
1 36 81 16
−18 −48
2 4 bi’s → 1 18 33 16
1 324 1089 256
−66 −576
3 8 bi’s → 1 258 513 256
1 66564 263169 65536
−1026 −132096
4 16 bi’s → 1 65538 131073 65536
1 4.29522 × 109 1.71801 × 1010 4.29496 × 109
−2.62146 × 105 −8.59019 × 109
5 32 bi’s → 1 4.29495 × 109 8.58991 × 109 4.29496 × 109
1 1.84465 × 1019 7.37865 × 1019 1.84466 × 1019
−1.71798 × 1010 −3.68932 × 1019
6 64 bi’s → 1 1.84465 × 1019 3.68933 × 1019 1.84466 × 1019
b0 b1 b2 b3
TABLE 2.3
Computed absolute values of the roots as a function of m for Eq. (2.52).
m |r1| |r2| |r3| m |r1| |r2| |r3|
1 2.450 1.220 0.667 4 2.000 1.044 0.958
2 2.060 1.164 0.834 5 2.000 1.022 0.979
3 2.002 1.090 0.917 6 2.000 1.011 0.989
and so α2 = 5. From Eq. (2.50) p is calculated as 1. Then, from Eq. (2.51) q = α2 − p2 = 2.
Thus, the complex roots are r2,3 ≈ 1 ± i2. The exact roots are x = 1 and 1 ± i2.Gr¨affe’s Root Square Method 25
TABLE 2.4
Calculated values of bi’s at first few squaring processes for Eq. (2.53).
m 2m 1 a1 a2 a3
0 1 1 −1 3 5
1 1 9 25
−6 +10
1 2 bi’s → 1 −5 19 25
1 25 361 625
−38 +250
2 4 bi’s → 1 −13 611 625
1 169 373321 390625
−1222 +16250
3 8 bi’s → 1 −1053 389571 390625
1 1108809 1.51765 × 1011 1.52587 × 1011
−779142 +8.22656 × 108
4 16 bi’s → 1 329667 1.52587 × 1011 1.52587 × 1011
1 1.0868 × 1011 2.32827 × 1022 2.32827 × 1022
−3.05174 × 1011 −1.00605 × 1017
5 32 bi’s → 1 −1.9649 × 1011 2.3285 × 1022 2.32827 × 1022
b0 b1 b2 b3
Example 3:
If all the powers of x in the given polynomial equation are multiples of some integer then
re-scale the variable x and minimize the maximum power of x, otherwise the method may
lead to incorrect result. As an example, consider the equation
x4 + 5x2 +4=0 . (2.54)
Its exact roots are ±i and ±2i. Consequently, one expects oscillation of the coefficients bi’s
as m increases. Instead of it, no change in the signs of bi’s was found to occur for m > 2
(verify). At m = 5
b1 = 8.589935 × 109, b2 = 1.844674 × 1019,
b3 = 3.689349 × 1019, b4 = 1.844674 × 109 .
From these bi’s the |ri|’s are calculated to be
|r1| = 2.044, |r2| = 1.957, |r3| = 1.022, |r4| = 0.979.
In all these cases P(±|ri|) are far from 0.26 Solutions of Polynomial and Reciprocal Equations
Let us proceed by minimizing the maximum power of x. Substitution of x2 = y in
Eq. (2.54) gives
y2 + 5y +4=0.
Now, applying the method, the coefficients b1 and b2 at m = 5 are
b1 = b2 = 1.844674 × 1019.
Thus, |r1| = 4 and |r2| = 1. Since P(−4) = 0 and P(−1) = 0 the roots of y2 + 5y +4=0
are −4 and −1. Then, their square roots ±i2 and ±i are the roots of Eq. (2.54).
The procedure described in this section can be applied to polynomial equations of ar￾bitrary order whose roots are all real or at the most one pair of roots are complex. The
procedure can be extended to equations possessing more than one pair of complex roots.
This is left as an exercise to the reader.
2.9 Laguere’s Method
Assume that the given polynomial of order n is of the form
Pn(x) = n
i=1
(x − xi). (2.55)
By calculating the derivative of ln |Pn| one can show that
A = P
n
Pn
= n
i=1
1
(x − xi)
. (2.56)
The second derivative of ln |Pn| gives
B = −a2 ln |Pn|
ax2 =
P
n
Pn
2
− P
n
Pn
= n
i=1
1
(x − xi)
2 . (2.57)
Suppose x1 is the root to be determined and assume that the other roots x2, ··· , xn are
distant from x1 and are close to a point say X. Define a = x − x1 and b = x − X. Then,
A = 1
a
+ (n − 1)
b , (2.58a)
B = 1
a2 + (n − 1)
b2 . (2.58b)
Solving these two equations for a gives
a = n
A ± (n − 1)(nB − A2)
. (2.59)
In Eq. (2.59) ‘+’ sign is used if A is positive otherwise use ‘−’ sign. Start with a value x0
and calculate A, B and a. Then, the next iteration is x1 = x0 − a and repeats this until a
is sufficiently small.Reciprocal Equations 27
2.10 Reciprocal Equations
Certain types of polynomial equations of degree higher than three can also solved exactly.
Introducing the change of variable x → 1/x and then dropping the prime Eq. (2.1) becomes
anxn + an−1xn−1 + ··· + a1x +1=0. (2.60)
Equation (2.60) is a polynomial of degree n and is called reciprocal equation of Eq. (2.1).
If an = 1 then Eq. (2.60) becomes
xn + an−1xn−1 + ··· + a1x +1=0. (2.61)
Comparison of Eqs. (2.1) and (2.61) gives an−1 = a1, an−2 = a2, ..., or simply
an−r = ar, r = 1, 2,...,(n − 1)/2 if n is odd
r = 1, 2,...,(n − 2)/2 if n is even. (2.62)
That is, when an = 1 the coefficients of the terms equidistant from the beginning and end
are equal in both sign and magnitude. If an = −1 then an−r = −ar. Now, the coefficients
of the terms equidistant from the beginning and end are equal in magnitude but opposite
in sign. Reciprocal polynomials are noticed or used for example, in the deflection surface of
the rectangular plates and cyclic error correcting codes, etc.
2.10.1 Some Properties of Reciprocal Equations
Some of the properties of the roots of reciprocal equations are summarized as follows:
(1) The roots of a reciprocal equation always occur in pairs like r1, 1/r1, r2, 1/r2,
... . That is, if r is a root then 1/r is also a root.
(2) Type-1: Reciprocal equation of degree four
Equation (2.60) with n = 4 can be reduced to a quadratic equation in u by
dividing it by x2 and then by the change of variable u = x+ 1/x. Then, the roots
of the original equation are obtained by solving the equation x2 − ux + 1 = 0 for
each of the two values of u calculated from its quadratic equation.
(3) Type-2: Reciprocal equation of odd degree n with an−r = ar, r =
1, 2,...,(n + 1)/2
For such equations, x = −1 is a root.
(4) Type-3: Reciprocal equation of odd degree n with an−r = −ar, r =
1, 2,...,(n + 1)/2
In this case x = 1 is always a root.
(5) Type-4: Reciprocal equation of even degree n with an−r = −ar, an/2 = 0
For such equations x = −1 and 1 are always two roots.
Example:
Find the roots of the equation
x4 + 2x3 − x2 + 2x +1=0. (2.63)28 Solutions of Polynomial and Reciprocal Equations
For Eq. (2.63), n = 4, an = a4 = 1 and a3 = a1. That is, an−r = ar, r = 1, 2,...,(n − 2)/2.
The given equation is thus a type-1 reciprocal equation. One root of this equation is x+1/x.
Dividing Eq. (2.63) by x2 one has

x2 +
1
x2

+ 2 
x +
1
x

− 1=0.
Substitution of u = x + 1/x in the above equation gives u2 + 2u − 3 = 0. Its roots are
u = 1, −3. When u = 1 the equation x + 1/x = u becomes x2 − x + 1 = 0 whose roots are
x = 1 ± i
√3
2 .
When u = −3 the equation x + 1/x = u becomes x2 + 3x + 1 = 0. Its roots are
x = −3 ± √5
2 .
Thus, the roots of Eq. (2.63) are given by x = (1±i
√3)/2. One can easily verify the property
(1) of the reciprocal equations for Eq. (2.63).
2.11 Concluding Remarks
As shown in Section 2.5, the well-known standard formula for the roots of a quadratic
equation gives large error in the solution in certain cases. Therefore, it is necessary to know
the applicability of even the theoretical formula involved in a method of solving a problem.
The readymade analytical expressions available for all the roots of a cubic equation are
presented in this chapter because computation of roots of cubic equations occurs in many
scientific and engineering problems.
2.12 Bibliography
[1] Pavithra Rajendran, Quadratic Equations in Real Life. https://study.com/learn/
lesson/quadratic-equation-in-real-life-overview-examples.html (accessed on July
27, 2023).
[2] Laurence Lavelle, Linear, Quadratic, and Cubic Equations With Applications in
Chemistry. https://lavelle.chem.ucla.edu/wp-content/supporting-files/Chem14B/
Linear−Quadratic−Cubic−Equations.pdf (accessed on July 27, 2023).
[3] Zahedi, A.A. Kamil, Irvan, Jelita, H. Amin, A. Marwan, S. Suparni, Math. Model.
Eng. Prob. 9:129, 2022.
[4] L.A. Pipes and L.R. Harvill, Applied Mathematics for Engineers and Physicists.
McGraw-Hill, Singapore, 1984. 3rd edition.
[5] P.J. Nahin, An Imaginary Tale: The Story of √−1. University Press, Hyderabad,
2002.
[6] V.M. Tikhomirov, Stories About Maxima and Minima. University Press, Hyder￾abad, 1998. English translation by A. Shenitzer.Problems 29
2.13 Problems
2.1 Verify the Sturm’s theorem for the following equations in the specified intervals.
(a) x2 − 1 = 0, [0, 2], [−2, 2].
(b) x3 − 1 = 0, [0, 2], [2, 3].
(c) x3 + 1 = 0, [0, 2], [−2, 0].
(d) (x − 1)(x − 2)(x − 3) = 0, [0, 4], [0, 2].
2.2 Find the roots of the following equations using the formulas given in Section 2.6.
(a) x3 − 5x2 + 6x = 0.
(b) x3 − 2x2 + x − 2 = 0.
(c) x3 + x2 + x − 5 = 0.
(d) x3 − x2 − 3x + 5 = 0.
2.3 Develop a Python program to calculate the three roots of a given cubic equation.
Then, find the roots of the equations given in the previous problem.
2.4 Find the value of α for which the roots of the equation 2x3 + 6x2 + 5x + α = 0
are in arithmetic progression.
2.5 If the roots of the equation 27x3 + 42x2 − 28x − 8 = 0 are r/α, r and rα then
find them.
2.6 Find all the roots of the following equations by Gr¨affe’s root square method.
(a) x2 + 5x + 4 = 0.
(b) x2 + 2x + 2 = 0.
(c) x3 + 3x2 + 3x + 1 = 0.
(d) x3 + x − 2 = 0.
2.7 Develop a C++ or a Python program for determining all the roots of a cubic
equation by Gr¨affe’s root square method.
2.8 Obtain all the roots of the following equations by solving their reciprocal equa￾tions.
(a) x3 + 2x2 − 2x − 1 = 0.
(b) x4 + 9
2x3 + 7x2 + 9
2 + 1 = 0.
(c) x5 + 3x4 + x3 + x2 + 3x + 1 = 0.
(d) x5 + 2x4 + x3 + x2 + 2x + 1 = 0.
2.9 The equilibrium points of a dynamical system are the roots of the equation x5 +
3x4 − 5x3 − 15x2 + 4x + 12 = 0. Find all its equilibrium points.
2.10 Write the characteristic equations of the following matrices and then find the
eigenvalues.
a)


321
103
521

. b)


3210
1835
0214
1531


. c)


2314
5023
4811
2201

.3
Solution of General Nonlinear Equations
3.1 Introduction
An equation f(x) = 0 with f(x) directly proportional to x is called a linear equation,
otherwise it is nonlinear. An example of linear equation is ax+b = 0. Some of the nonlinear
equations are sin x = 0, x2 + x + 2 = 0, ex − 1 = 0. In many physical, mathematical and
engineering problems it is often required to determine the roots (that is, zeros) of equations
of the form f(x) = 0. For some simple equations, roots can be determined analytically. For
example, ready-made formulas are available for linear equations and polynomial equations
of degree two and three. However, general analytical methods are not available for nonlinear
equations. There are numerous nonlinear equations occurring in physical problems. As an
example, consider an elastic beam of length l whose one end is fixed and the other end is
pinned. The natural frequency ω of the beam satisfies the equation
tan 
l
ω/c
− tanh 
l
ω/c
= 0 , (3.1)
where c is a constant. Analytical determination of ω from the above equation is difficult.
In such a case one may look for a suitable numerical scheme. Nonlinear relations between
two or more variables are very common in science. For some examples see Problems 13-19
at the end of the present chapter.
Most of the numerical methods are iterative and are based on the idea of successive
approximation. Essentially, starting with one or more initial approximations to the root,
using an iterative formula a sequence of approximations or iterates xn is obtained. This
sequence converges to an exact root of the given equation. Some of the basic questions on
any iterative method are the following:
1. How does one proceed from the starting approximations?
2. When does one stop the iteration procedure?
3. What is the rate of convergence of the procedure?
4. What is the error involved in the approximation?
5. What are the advantages and disadvantages of the method?
Answers to these questions must be investigated in the iterative schemes.
The present chapter is devoted to general nonlinear equations. Several interesting nu￾merical methods are available to find roots of general nonlinear equations. Some of them
are:
1. Bisection method
2. False position method
3. Secant method
4. Newton–Raphson method
DOI: 10.1201/9781032649931-3 30Bisection Method 31


 

(a)
   


 

(b)
  
(c)     



FIGURE 3.1
(a) Illustration of the choice of x0 and x1 enclosing a root x∗ of a function f(x). (b) f(x0)
and f(x2) have opposite signs. (c) f(x1) and f(x2) have opposite signs.
5. M¨uller method
6. Chebyshev method
The above methods started with a good enough initial guess(es) to improve the solution
of the given equation until some predetermined convergence criteria are satisfied. These
methods have their own advantages and disadvantages. The salient features of the various
methods are discussed in this chapter. Particularly, (i) basic idea, (ii) convergence rate and
(iii) merits and demerits of them are presented. Finally, a comparative study of the methods
considered is also made. The prime emphasis is on one-dimensional equations of the form
f(x)=0, (3.2)
where f(x) is assumed to be continuous. The methods discussed in this chapter can in
principle be extended to higher-dimensional equations also. A case study is considered for
the Newton–Raphson method.
3.2 Bisection Method
The bisection method for Eq. (3.2) is based on the intermediate-value theorem (see Sec￾tion 1.6). The theorem states that if f(x) is a continuous function on some interval [a, b]
and f(a)f(b) < 0 then Eq. (3.2) has at least one real root or an odd number of real roots
in the interval [a, b].
3.2.1 Procedure
In the bisection method, two trial points, say, x0 and x1 are chosen such a way that signs
of f(x0) and f(x1) are opposite with f(x0) < 0 and f(x1) > 0. An exact root x∗ of f(x)
lies between x0 and x1. This is shown in Fig. 3.1a. The mid point x2 of the interval [x0, x1]
is given by
x2 = (x0 + x1) /2 . (3.3)
Then, the following three possibilities are analyzed:32 Solution of General Nonlinear Equations
(1) |f(x2)| < δf , where δf is the tolerance in f(x): x2 is a root of the given Eq. (3.2).
Stop the calculation.
(2) f(x2) > 0: x2 lies right to x∗ (Fig. 3.1b). The signs of f(x0) and f(x2) are opposite
and hence the root lies in the interval [x0, x2].
(3) f(x2) < 0: x2 lies left to x∗ (Fig. 3.1c). The signs of f(x1) and f(x2) are opposite
and hence the root lies in the interval [x2, x1].
For the cases (2) and (3), a new interval is obtained. Its width is half of the previous interval.
The above process is repeated for the new interval by relabelling it as [x0, x1]. The iteration
is stopped when the width of the redefined interval is smaller than a preassumed small value
δx.
The general formula of the method is
xn+1 = (xn + xn−1) /2 , (3.4)
where xn and xn−1 enclose the root x∗.
3.2.2 Termination Condition
How many iterations, N, are necessary to reach the Nth interval with width less than a
tolerance δx? To compute this number, let us denote [x0, x1] and [xN−1, xN ] as the initial
and Nth intervals, respectively. The width of the Nth interval denoted as WN is |xN −xN−1|
which is half of the width of the interval at (N − 1)th iteration; one-fourth of the width of
the interval at (N − 2)th iteration and so on. That is,
WN = |xN − xN−1| = |x1 − x0|/2N . (3.5)
Suppose, at Nth iteration WN is less than or equal to δx, that is,
|x1 − x0|/2N ≤ δx . (3.6)
Taking logarithm on both sides of Eq. (3.6) with equality sign gives
N = Integer part of 
ln |x1 − x0| − ln δx
ln 2 
. (3.7)
In the above equation, integer part of right-side is taken since N is an integer. An important
result is that the value of N depends on x0, x1 and δx and not on the functional form of
f(x). Therefore, if x0 and x1 enclose a root of two different functions f1(x) and f2(x) then
these functions will have the same value for N. Equation (3.7) implies that convergence to
an interval of an arbitrary tolerance is always guaranteed. The process can be stopped after
N + 1 iterations.
If a given function varies very slowly near the actual root then even if the possibility
|f(x2)| ≤ δf is realized the approximate root may be far away from the actual root. On
the other hand, if the given function varies very rapidly near the actual root then even if
|x1 − x0| ≤ δx, one may have |f(x2)|  δf . In such cases or for a higher accuracy, the
iteration may be stopped when both the conditions |f(x2)| ≤ δf and |x1 − x0| ≤ δx are
satisfied. Write a program that will give a root when the above two conditions are realized.
3.2.3 Examples
Now, compute the roots of two equations by the bisection method.Bisection Method 33
TABLE 3.1
The bisection method: This table gives values of end points of the interval [x0, x1], its
mid point x2 and the function values at x0, x1 and x2 at each iteration for the equation
ex − 1 = 0. The trial points are (x0, x1)=(−0.5, 0.4).
Iteration x0 x1 x2 f(x0) f(x1) f(x2)
0 −0.50000 0.40000 − − −− −0.39347 0.49182 − − −−
1 −0.50000 0.40000 −0.05000 −0.39347 0.49182 −0.04877
2 −0.05000 0.40000 0.17500 −0.04877 0.49182 0.19125
3 −0.05000 0.17500 0.06250 −0.04877 0.19125 0.06449
4 −0.05000 0.06250 0.00625 −0.04877 0.06449 0.00627
5 −0.05000 0.00625 −0.02187 −0.04877 0.00627 −0.02163
6 −0.02187 0.00625 −0.00781 −0.02163 0.00627 −0.00778
7 −0.00781 0.00625 −0.00078 −0.00778 0.00627 −0.00078
Example 1:
Consider the equation ex −1 = 0. Its exact root is x∗ = 0. The signs of f(−0.5) = −0.39347
and f(0.4) = 0.49182 are opposite and hence a root of the given function lies in the interval
[-0.5,0.4]. Choose x0 = −0.5, x1 = 0.4, δx = 10−2 and δf = 10−5. It is desired to obtain a
root which is a mid point of an interval whose width is less than δx. Then, (N + 1) iterations
have to be performed, where N is given by Eq. (3.7), with the given starting points. For
example, if δx = 10−2 and the starting points are (−0.5, 0.4) then N value from Eq. (3.7)
is computed as 6. Therefore, the process can be stopped after 7 iterations. The mid point
at the 7th iteration is the desired root.
With x0 = −0.5 and x1 = 0.4 the mid point of this interval is x2 = −0.05 and
f(x2) = −0.04877. Since f(x2) < 0, x0 is replaced by x2 and the new interval is
[x0, x1] → [−0.05, 0.4]. Table 3.1 gives the result of successive iterations. At the 7th it￾eration the mid point (x2) is −0.00078. Then, the new interval is [−0.00078, 0.00625]. Its
width is less than the tolerance δx = 10−2. Therefore, x = −0.00078 is the approximation to
the exact root with δx = 10−2. A small value of δx requires a greater number of iterations.
Example 2:
Consider the equation x3 − 1 = 0. Its exact real root is 1. (x0, x1) are chosen as (0.9, 1.1).
Then, the number of iterations N + 1 is calculated as 5. At the first iteration x2 = (0.9 +
1.1)/2 = 1. Now, checking the three possibilities (1)–(3) one realizes the possibility (1).
Therefore, x2 = 1 is the root. In this case, one need not do the remaining iterations. This
kind of possibility arises whenever the points x0 and x1 are at equal distance from an exact
root.
3.2.4 Limitations, Order of Convergence and Efficiency Index
The bisection method cannot be applied to equations with f
(x∗) = 0. In this case f(x)
does not change sign on either side of x∗. An example is x4 +x2 = 0 which has one real root
x∗ = 0 but f(x) is positive on the entire real x-axis. When a given equation has multiple34 Solution of General Nonlinear Equations
roots, bisection method may not be applicable because the function may not change sign
at points on either side of the roots.
The error in the bisection method at nth iteration is the difference between the mid
point xn of the nth interval and the exact root x∗. That is,
en = x∗ − xn . (3.8)
Since x∗ and xn are always contained in the initial interval one can write
|en| = |x∗ − xn| ≤ |x1 − x0|
2n+1 . (3.9)
Replacing n by n + 1 in Eq. (3.9) gives
|en+1| ≤ |x1 − x0|
2n+2 or en+1 ∝ en . (3.10)
The order of convergence R of a sequence {xn} is defined through the equation
|en+1| = A|en|
R , (3.11)
where A is a constant. From Eqs. (3.9)-(3.11) R is found to be 1. Thus, the rate of conver￾gence of the sequence {xn} is linear in the bisection method. The efficiency index α in the
bisection method is 1 since R = 1 and m = 1 (refer Eq. (1.22)).
3.3 Method of False Position
Like the bisection method, the false position method also begins with two appropriate
starting points. However, instead of locating the mid point of the interval [x0, x1] the points
X0 = (x0, f(x0)) and X1 = (x1, f(x1)) in x − f(x) plane are connected by a straight-line.
The next approximate root is the point at which the straight-line intersects the x-axis.
3.3.1 Procedure
Consider two starting points x0 and x1 with f(x0) < 0 and f(x1) > 0. These two points
X0 = (x0, f(x0)) and X1 = (x1, f(x1)) in x − f(x) are joined by a straight-line. Then,
the point x2 at which the straight-line cuts the x-axis is determined. This is schematically
represented in Fig. 3.2. The value of x2 is obtained by constructing the equation for straight￾line passing through X0 and X1. The straight-line equations at the points X0, X1 and X2
are
f (x0) = ax0 + b, f (x1) = ax1 + b, 0 = ax2 + b , (3.12)
respectively. From Eqs. (3.12) x2 is obtained as
x2 = x0f (x1) − x1f (x0)
f (x1) − f (x0) . (3.13)
Next, the following three possibilities (the same possibilities considered in the bisection
method) are analyzed:
(1) |f(x2)| ≤ δf , where δf is the tolerance in f(x): x2 is a root of the given Eq. (3.2).
Stop the calculation.Method of False Position 35




 




FIGURE 3.2
Illustration of the false position method. For description see the text.
(2) f(x2) > 0: x2 lies right to x∗. The signs of f(x0) and f(x2) are opposite and
hence the root lies in the interval [x0, x2].
(3) f(x2) < 0: x2 lies left to x∗. The signs of f(x1) and f(x2) are opposite and hence
the root lies in the interval [x2, x1].
When possibility (2) occurs the value of x1 is replaced by x2; when possibility (3) occurs
then the value of x0 is replaced by x2. The formula given by Eq. (3.13) is repeated to get
successive approximations to the root.
The general formula for (n + 1)th approximation is
xn+1 = xn−1f (xn) − xnf (xn−1)
f (xn) − f (xn−1) , (3.14)
where xn and xn−1 enclose the root x∗.
In the bisection method, the number of iterations required to reach the approximate root
with a specified accuracy δx in x is predetermined. This number N is given by Eq. (3.7).
But the number N is unknown in the false position method. Therefore, the following criteria
may be checked to stop the iteration:
1. |x0 − x2| ≤ δx if f(x2) > 0 or |x2 − x1| ≤ δx if f(x2) < 0.
2. |f(x2)| ≤ δf .
The process can be stopped when any one of these two conditions is met.
Example:
Let us find the root of the equation ex − 1 = 0, the same equation considered for the
bisection method, with x0 = −0.5, x1 = 0.4, δx = 0.01 and δf = 10−5. Table 3.2 displays
the summary of the calculation. At the end of 4th iteration the value of |x0 − x2| is 0.00289
and is < δx. Hence, the root is x∗ = −0.00066. (In the bisection method, the root after 436 Solution of General Nonlinear Equations
TABLE 3.2
The method of false position: Summary of iterations for the equation ex − 1 = 0. The
starting values are x0 = −0.5 and x1 = 0.4. The tolerances in x and f(x) are chosen as
δx = 10−2 and δf = 10−5. At the end of 4th iteration |x0 − x2| = 0.00289 < δx. Hence, the
root is x∗ = −0.00066. If both δx and δf are chosen as 10−5 then at the end of 7th iteration
|f(x2)| < δf and hence the root is x∗ = 0.0.
Iteration x0 x1 x2 f(x0) f(x1) f(x2)
0 −0.50000 0.40000 − − −− −0.39347 0.49182 − − −−
1 −0.50000 0.40000 −0.09999 −0.39347 0.49182 −0.09516
2 −0.09999 0.40000 −0.01894 −0.09516 0.49182 −0.01878
3 −0.01894 0.40000 −0.00355 −0.01876 0.49182 −0.00354
4 −0.00355 0.40000 −0.00066 −0.00354 0.49182 −0.00066
5 −0.00066 0.40000 −0.00012 −0.00066 0.49182 −0.00012
6 −0.00012 0.40000 −0.00002 −0.00012 0.49182 −0.00002
7 −0.00002 0.40000 0.00000 −0.00002 0.49182 0.00000
iterations is 0.00625.) For δx = δf = 10−5 then at the end of 7th iteration |f(x2)| < δf and
the root is x∗ = 0.0. In the bisection method, the root after 7 iterations is x∗ = −0.00078.
This shows that the convergence to an exact root in the false position method is much faster
than the bisection method.
3.3.2 Limitation, Order of Convergence and Efficiency Index
Like the bisection method, the present method is also not applicable to the functions which
do not cross the x-axis but touch it x = x∗.
The error en+1 is given by
|en+1| = |x∗ − xn+1|≤|xn − xn−1| . (3.15)
Replacing n by n − 1 in Eq. (3.15) one obtains
|en| = |x∗ − xn|≤|xn−1 − xn−2| . (3.16)
Then,
|en+1|
|en| ≈ |xn − xn−1|
|xn−1 − xn−2|
= constant. (3.17)
Thus, the order of convergence R is 1 for the false position method. Like the bisection
method, the value of α for the false position method is 1.
3.4 Secant Method
Similar to the bisection and the false position methods the secant method also starts with
two initial points x0 and x1. However, the interval [x0, x1] need not enclose a root.Secant Method 37


 


 
FIGURE 3.3
Illustration of the secant method. The interval [x0, x1] not enclose a root.
3.4.1 Procedure
Consider two arbitrary points x0 and x1. In the first iteration, a straight-line passing through
the two points X0 = (x0, f(x0)) and X1 = (x1, f(x1)) in the x − f(x) plane is constructed.
The point that cuts the x-axis is x2. This x2 is computed from Eq. (3.13). In the second
iteration, x0 and x1 are replaced by x1 and x2, respectively. Then, x2 is computed. The
general formula is
xn+1 = xn−1f (xn) − xnf (xn−1)
f (xn) − f (xn−1) . (3.18)
In Eq. (3.18) the interval [xn, xn−1] need not enclose the root x∗. The successive approxima￾tion to the root by the secant method is schematically illustrated in Fig. 3.3. The iteration
is stopped when one of the following criteria is met:
(1) |f (xn) − f (xn−1)| ≤ δs, where δs is a preassumed small value.
(2) |f(xn+1)| ≤ δf .
(3) Case (1) or (2) is not realized within N iterations.
When the first criterion is satisfied the process is terminated because division by a small
quantity in Eq. (3.18) is encountered. In this case if both |f(xn)| and |f(xn−1)| are less
than δf then xn and xn−1 are roots. xn and xn−1 may or may not be equal. For example,
if x0 and x1 are chosen as an exact root of the given equation then the case (1) occurs.
Alternatively, if xn is close to a root and xn−1 is close to another exact root then again
case (1) occurs. If both |f(xn)| and |f(xn−1)| are greater than δf then start with another
set of starting values. When the case (2) occurs the convergent root is xn+1. For the case
(3) convergence is not obtained with the tolerance δf . This may occur if the given equation
has no real root or δf is too small that a greater number of iterations is necessary or the
starting values x0 and x1 are far away from the exact root.38 Solution of General Nonlinear Equations
TABLE 3.3
The secant method: Successive approximation to the root of the equation f(x)=ex −1=0
with x0 = 0.1 and x1 = 0.3. δf and δs are chosen as 10−5 and 10−9, respectively. After 4
iterations the root is x∗ = 0.0.
Iteration x0 x1 x2 f(x2)
0 0.100000 0.300000 − − −− − − − − −−
1 0.100000 0.300000 0.014037 0.14136e−01
2 0.300000 0.014037 0.001996 0.19982e−02
3 0.014037 0.001996 0.000014 0.13947e−04
4 0.001996 0.000014 0.000000 0.00000e+00
Example:
For the equation f(x)=ex − 1 = 0, let us choose x0 = 0.1 and x1 = 0.3. For these starting
values f(x0) > 0 and f(x1) > 0. The interval [x0, x1] not enclose the root of the equation.
Table 3.3 presents the numerical result, where δx = 10−2, δf = 10−5 and δs = 10−9. After
4 iterations |f(x2)| is < δf and hence the root is 0.0. For x0 = −0.5 and x1 = 0.4, the
root is obtained after 4 iterations and is −0.9 × 10−5 (verify). Compare this value with the
approximate root obtained by the bisection and the false position methods.
3.4.2 Limitation, Order of Convergence and Efficiency Index
Since the approximations xn and xn+1 need not enclose the exact root, convergence of {xn}
to x∗ is not always guaranteed.
Now, determine the order of convergence of the secant method. Substitution of xn+1 =
x∗ − en+1, xn = x∗ − en and xn−1 = x∗ − en−1 in Eq. (3.18) gives
x∗ − en+1 = (x∗ − en−1) f (x∗ − en) − (x∗ − en) f (x∗ − en−1)
f (x∗ − en) − f (x∗ − en−1) . (3.19)
Replacement of e by −e and then dropping the prime in the above equation gives
x∗ + en+1 = (x∗ + en−1) f (x∗ + en) − (x∗ + en) f (x∗ + en−1)
f (x∗ + en) − f (x∗ + en−1) . (3.20)
Writing the Taylor series expansion of f about x∗, neglecting the higher powers of e and
substituting f(x∗) = 0 one obtains
x∗ + en+1 =
(x∗ + en−1)

enf + e2
n
2 f
(en − en−1) f
−
(x∗ + en)

en−1f + e2
n−1
2 f
(en − en−1) f
= x∗ + f 
e2
nx∗ + e2
nen−1 − e2
n−1x∗ − e2
n−1en

2f (en − en−1) . (3.21)Newton–Raphson Method 39
In the limit n → ∞, e2
nx∗ − e2
n−1x∗ = 0. Then,
x∗ + en+1 = x∗ + f
2f enen−1
or
en+1 = f
2f enen−1 . (3.22)
To estimate the order of convergence R, let us express the above equation in the standard
form of Eq. (3.11). From Eq. (3.11) one can write
en = AeR
n−1 . (3.23)
Inverting this equation, en−1 is obtained as
en−1 =
en
A
1/R
. (3.24)
Replacing en−1 in Eq. (3.22) with Eq. (3.24) gives
en+1 = fe
(1+R)/R n
2f
A1/R . (3.25)
Comparison of Eqs. (3.11) and (3.25) leads to
A =
 f
2f
R/(R+1)
, R = (1 + R)
R or R2 − R − 1=0 . (3.26)
The roots of R are
R = 1 ± √5
2 . (3.27)
Since R must be greater than zero, one has R = (1 + √5)/2 ≈ 1.62. Thus, the order of
convergence of the secant method is ≈ 1.62. Because of m = 1 and R ≈ 1.62, the value of
the efficiency index α is 1.62.
3.5 Newton–Raphson Method
If f(x) and its first two derivatives f
(x) and f(x) are continuous near a root x∗ then
this additional information about the nature of the function f(x) can be used to develop
algorithms that will produce sequences converging faster to x∗ than the methods discussed
earlier in this chapter. One such method is the Newton–Raphson method. This is one of the
most widely used methods of solving nonlinear equations. In this section, first the features
of the method are presented for one-dimensional equations. Then, the iterative rule for n￾coupled equations is given. The case of n = 2 is illustrated. For a historical development of
the Newton–Raphson method one may refer to the ref. [1].
3.5.1 One-Dimensional Equations
Unlike the earlier methods, the Newton–Raphson method starts with a single guess x0.
Figure 3.4 gives the graphical description of the method. A tangent line to f(x) at x0 is40 Solution of General Nonlinear Equations

B
A


 

C  
FIGURE 3.4
Graphical illustration of x1 and x2 for the Newton–Raphson method.
drawn. The point, say x1, on the x-axis at which the tangent line intersects is the next
approximation for the root. This process is repeated until a root with a desired accuracy is
reached. From Fig. 3.4, tan θ is written as
tan θ = AB
AC = f (x0)
x0 − x1
. (3.28)
tan θ represents f
(x) at x = x0. Replacing tanθ by f
(x0) in Eq. (3.28) gives
x1 = x0 − f (x0)
f (x0)
. (3.29)
Then, the general formula for nth to (n + 1)th approximation is
xn+1 = xn − f (xn)
f (xn)
. (3.30)
This is the Newton–Raphson iterative rule for a root of a one-dimensional nonlinear equa￾tion.
3.5.2 Termination Criteria
In order to stop the iteration process certain possibilities are to be analyzed at each iteration.
For this purpose, let us denote the tolerance in |xn+1−xn| as δx; in |f(xn)| as δf and |f
(xn)|
as δs with δs  δf . Then, the following possibilities are analyzed:
(1) |f
(xn)| ≤ δs and |f(xn)| ≤ δf . The root is xn.
(2) |f
(xn)| ≤ δs and |f(xn)| > δf . Division by a small number is encountered and
the process is terminated.
(3) |xn+1 − xn| ≤ δx and |f(xn+1)| ≤ δf . The root is xn+1.
(4) No convergence in N iterations.Newton–Raphson Method 41
TABLE 3.4
The Newton–Raphson method: Successive approximation of the real root of f(x)=ex−1 =
0 with x0 = 0.5, δx = 10−5, δf = 10−5 and δs = 10−9.
Iteration x value f(x) value |xn+1 − xn|
0 0.500000 0.648721e+00 − − −−
1 0.106531 0.112412e+00 0.393469
2 0.005478 0.549316e−02 0.101053
3 0.000015 0.149012e−04 0.005463
4 0.000000 0.000000e−07 0.000015
5 0.000000 0.000000e+00 0.000000
Instead of checking the absolute error in case (3), one may check the relative error given by
|(xn+1 − xn)/xn+1|. If the possibility (2) or (4) occurs for a large number of real values of
x0 then the real root may be ±∞ or the equation has no real root.
3.5.3 Systematic Procedure
The following is a systematic procedure of the Newton–Raphson method.
(1) Choose the tolerance values δx, δf and δs and the maximum number of iterations
N.
(2) Choose a starting value of x, x0.
(3) Calculate f(x0) and f
(x0). If |f
(x0)| ≤ δs and |f(x0)| ≤ δf the root is x0 and
stops the process. If |f
(x0)| ≤ δs but |f(x0)| > δf stop the process because
division by a small quantity is encountered.
(4) Compute x1 using Eq. (3.30) and f(x1). If |x1 − x0| ≤ δx and |f(x1)| ≤ δf the
root is x1 and stop the process. Otherwise, repeat the steps (3) and (4) with x1
and so on.
(5) Terminate the process if convergence is not realized in N iterations.
In writing a computer language program to implement the Newton–Raphson method
care must be taken to provide all the possible decision makings.
3.5.4 Examples
The following two examples illustrate the Newton–Raphson method.
Example 1:
Consider the equation ex − 1 = 0. Let us choose δx = 10−5, δs = 10−9 and δf = 10−5. The
derivative of the function is f
(x)=ex. Table 3.4 gives the summary of the computation
with x0 = 0.5. After 5 iterations |x5 − x4| = 0 < δx and |f(x5)| = 0 < δf . Thus, the root is
x∗ = 0. For the starting value x0 = 2 the same result is obtained after 6 iterations (verify).
Example 2:
Find a root of the equation x3 − 8 = 0 with five decimal point accuracy with x0 = 2.5.42 Solution of General Nonlinear Equations
The iteration can be stopped when the absolute difference between xn+1 and xn, |xn+1−xn|,
is < 10−5. By the Newton–Raphson formula one has
x1 = x0 − f (x0)
f (x0) = x0 − x3
0 − 8
3x2
0
= 2.0933333 , x2 = 2.0040996 .
The variation is in second decimal point and hence one has to continue the iteration. x3
is 2.0000084. The difference between x2 and x3 is in third decimal. Next, x4 is calculated
as 2.0000000. Now, the difference between x3 and x4 is in sixth decimal place only. So, the
iteration can be stopped. x4 = 2.0 is the root with five decimal accuracy. The exact root is
also 2.0.
3.5.5 Newton–Raphson Method for a Multiple Root
The Newton–Raphson method can be extended to compute a multiple root of the equation
f(x) = 0. Assume that x∗ is a root with multiplicity p. Then, x∗ is also a root of f
(x)=0
with multiplicity (p − 1), a root of f(x) = 0 with multiplicity (p − 2), ..., a root of
f p−1(x) = 0 with multiplicity 1. In this case the formula
xn+1 = xn − p
f (xn)
f (xn) (3.31)
is useful. In fact, for xn ≈ x∗, the expressions
xn − p
f (xn)
f (xn)
, xn − (p − 1) f (xn)
f (xn)
, etc, (3.32)
all will have the same value.
3.5.6 Limitations
Let us point out the limitations of the Newton–Raphson method.
1. Division by Zero
An obvious limitation of the method is the occurrence of division by zero in the second term
of right-side of Eq. (3.30). If |f
(xn)| is zero or too small then this term becomes infinity
or diverge. In this case if |f(xn)| ≤ δf then xn is an approximation to the actual root x∗.
Otherwise, use another starting value x0.
Examples:
Consider the function f(x) = x − sin x = 0. Its derivative is f
(x)=1 − cos x. The exact
root of this equation is x∗ = 0. If x0 = 0 or close to zero then f
(x0) ≈ 0. Then, the
second term in the right-side of Eq. (3.30) becomes infinity or very large. However, x0 = 0
is the exact root. Next, consider the equation f(x) = sin x = 0. Its exact roots are nπ,
n = 0, ±1, ±2,... . f
(x) is cos x. When x0 ≈ (2n + 1)π/2, f
(x0) ≈ 0. The iteration cannot
be continued. Further, x0 ≈ (2n + 1)π/2 is not a root.
2. Sensitive Dependence on x0
The method is sensitive to the starting values x0. If the equation has more than one real
root then the iterated values x converge to one of the roots. On the other hand, if x0 is far
from all real roots then xn may diverge.Newton–Raphson Method 43
Example:
The roots of the equation x2 −4x+ 3 = 0 are x∗ = 1, 3. If x0 is close to 1 then xn’s converge
to x∗ = 1. If x0 is close to 3 then xn converges to x∗ = 3.
3. Cycling
In the Newton–Raphson method it is not always the case that after N iterations a root is
reached. For some equations and for certain values of x0 the iterations tend to repeat or
almost repeat with a definite periodicity without approaching a root of the equation.
Example:
For the equation x3−x−3 = 0 the initial guess x0 = 0 generates x1 = −3, x2 = −1.9615384,
x3 = −1.1471759, x4 = −0.0065792, x5 = −3.0003891, x6 = −1.9618182, x7 = −1.1474303,
x8 = −0.0072564, x9 = −3.0004733, ... . The iterated values exhibit almost a period-4
cycle.
3.5.7 Order of Convergence and Efficiency Index
Defining en = x∗ − xn and en+1 = x∗ − xn+1 the rule (3.30) becomes
en+1 = en + f (x∗ − en)
f (x∗ − en) . (3.33)
Writing the Taylor series expansions for f(x∗ − en) and f
(x∗ − en) and neglecting higher
powers of en one has
en+1 = en − en

1 − enf
2f
 1 − en
f
f
−1
= en − en

1 − enf
2f
 1 + en
f
f

= − e2
nf
2f , (3.34)
where, in obtaining the last expression, the term e3
n is neglected. Then,
|en+1| = |en|
2




f
2f



 . (3.35)
The order of convergence R is thus 2 (quadratic) for the Newton–Raphson method. That
is, near a root, the number of significant digits roughly doubles in each iteration. Because
of this the Newton–Raphson method is called a powerful method.
Next, find the value of the efficiency index α. At each iteration, the values of both
f(x) and f
(x) are used so m = 2. Substitution of R = 2 and m = 2 in Eq. (1.22) gives
α = √2=1.41.
3.5.8 Complex Roots of One-Dimensional Equations
The Newton–Raphson method can be used to compute complex roots of nonlinear equations.
They can be obtained by choosing the starting value x0 as a complex number and assuming44 Solution of General Nonlinear Equations
TABLE 3.5
The Newton–Raphson method: Successive approximation of the pair of complex roots of
the equation f(x) = x2 + x + 1 = 0. The initial guess is x0 = −0.4 + i0.6. Here, δx = 10−5,
δf = 10−5 and δs = 10−9. xr and xi denote real and imaginary parts of x, respectively.
Similarly, fr and fi are the real and imaginary parts of f(x), respectively. The exact roots
are x∗ = −0.5 ± i0.86603.
Iteration xr xi fr fi
0 −0.400000 0.600000 − − −− − − −−
1 −0.551351 0.908108 −0.072023 −0.093265
2 −0.502399 0.865684 0.000597 −0.004153
3 −0.499999 0.866022 0.000006 0.000002
4 −0.500000 0.866025 0.000000 0.000000
f(x) and f
(x) as complex functions. Imaginary part of x0 should be sufficiently large. If it
is zero or too small then the successive approximations {xn} are always real, if there is a
real root, and convergence to a complex root never occurs. Since the complex roots always
occur in pairs, if x∗ = a + ib is a root obtained by the Newton–Raphson method then the
other root is its conjugate a − ib.
Example:
Let us compute the complex roots of the quadratic equation f(x) = x2 + x + 1 = 0. Its
exact roots are x∗
1,2 = (−1 ± i
√3 )/2. Table 3.5 summarizes the result for x0 = 0.2 + i0.2
with δx = 10−5, δf = 10−5 and δs = 10−9. At the end of four iterations |x4 − x3| < δx and
|f(x4)| < δf . The roots are −0.5 ± i0.86603.
3.5.9 Roots of n-Dimensional Equations
Having studied the Newton–Raphson method for one-dimensional equations now deduce
the formula for n-dimensional equations, that is, n-coupled first-order nonlinear equations
with n variables.
Let
F(X)=0 (3.36)
be a set of n equations, where X = (x(1), x(2), ..., x(n)
) and F = (f1, f2, ..., fn). The Taylor
series expansion of F(X∗) is
F (X∗) = 0= F (Xi)+(X∗ − Xi) ∂F
∂Xi
+ ... , (3.37a)
where
∂F
∂Xi
= F = n
j=1
∂F
∂x(j)
i
. (3.37b)
Here, Xi represents the root at ith approximation or iteration. The elements of F form a
matrix J called Jacobian matrix of F at Xi. Keeping the first two terms in the right-side
of Eq. (3.37a) one has
0 = F (Xi) + δXi · ∂F
∂Xi
, (3.38)Newton–Raphson Method 45
where δXi = X∗ − Xi. From Eq. (3.38) one can write
δXi = −
 ∂F
∂Xi
−1
F (Xi). (3.39)
Then, the value of X at the (i + 1)th iteration is
Xi+1 = Xi + δXi. (3.40)
More precisely,
x(j)
i+1 = x(j)
i + δx(j)
i , j = 1, 2, ..., n. (3.41)
3.5.10 Roots of Two-Dimensional Equations
For the two-dimensional equations of the form
f (x, y)=0, g (x, y)=0 (3.42)
the Jacobian matrix J is given by
J =
 fx fy
gx gy

. (3.43)
Then, Eq. (3.39) becomes
 δxi
δyi

= − 1
detJ
 gy −fy
−gx fx
  f
g

, (3.44a)
where
detJ = fxgy − gxfy. (3.44b)
Referring the matrix multiplication in Eq. (3.44a) gives
δxi = 1
detJ (gfy − fgy) , δyi = 1
detJ (fgx − gfx). (3.45)
Then, the Newton–Raphson iteration rule is
xi+1 = xi + δxi , (3.46a)
yi+1 = yi + δyi . (3.46b)
Example:
Consider the set of equations
x2 + x − y = 0, (3.47a)
xy − 2=0. (3.47b)
An exact root of this system is (x∗, y∗) = (1, 2). For the given system fx = 2x+ 1, fy = −1,
gx = y, gy = x. Table 3.6 summarizes the result, where x0 = 0.5 and y0 = 1.5. A root with
the specified accuracy is arrived at 5th iteration. The obtained root is (x, y) = (1, 2). For
the initial guess (x0, y0) = (1, 1) the same root is obtained after two iterations.
Newton–Raphson method is found applications in the studies of hydraulic problems
[2,3], water distribution network [4], power flow problems [5,6], nonlinear regression models
[7], thermal EHD lubrication model of line contacts [8], optimization of transmission control
protocol [9] and data sciences and education [10], a few to mention. Methods to improve the
order of convergence to a root have been reported [11-14]. Improvement and modifications
of the Newton–Raphson method are proposed [15,16].46 Solution of General Nonlinear Equations
TABLE 3.6
The Newton–Raphson method: Successive approximation of a real root of the two￾dimensional Eqs. (3.47). The initial values of x and y are x0 = 0.5 and y0 = 1.5. Here,
δx = δy = δf = δg = 10−5 and δs = 10−9. The root at the end of 5th iteration is
(x, y) = (1, 2).
Iteration x value y value f value g value
0 0.500000 1.500000 − − −− − − −−
1 1.150000 2.446500 0.026000 0.813475
2 1.014876 2.054657 −0.009807 0.085222
3 1.000202 2.000793 −0.000186 0.001198
4 1.000000 2.000000 0.000000 0.000000
5 1.000000 2.000000 0.000000 0.000000
3.6 M¨uller Method
The methods considered so far approximated the function f(x) in the neighbourhood of a
root by a straight-line. Contrast to this the M¨uller method is based on approximating f(x)
by a quadratic polynomial.
The method of M¨uller uses three starting points x0, x1 and x2 and assumes that x2
is the best approximation to a root x∗. A parabola passing through these three points is
constructed as shown in Fig. 3.5. Then, introduce a change of variable
t = x1 − x2 (3.48)
and define t0 and t1 as
t0 = x0 − x2 and t1 = x1 − x2 . (3.49)
In terms of the variable t, the quadratic polynomial equation for parabola is
f = at2 + bt + c . (3.50)
To determine the unknowns a, b and c three equations are formulated from Eq. (3.50).
These are Eq. (3.50) at t = t0, t1 and 0:
at2
0 + bt0 + c = f0 , at2
1 + bt1 + c = f1 , c = f2 , (3.51)
where f0 = f(x0), f1 = f(x1) and f2 = f(x2). Solving these equations one has
c = f2, a = e1t0 − e0t1
t0t1 (t1 − t0)
, b = e0t
2
1 − e1t
2
0
t0t1 (t1 − t0)
, (3.52a)
where
e0 = f0 − f2, e1 = f1 − f2 . (3.52b)
Now, the roots of Eq. (3.50) can be easily determined. As mentioned in Section 2.5 if the
roots z1 and z2 of a quadratic equation are small then roots calculated from the formula
(2.14) are accurate to fewer decimal points only. Since t = x−x2, at successive approximationM¨uller Method 47



 


 
FIGURE 3.5
The M¨uller method: Construction of a parabola (dashed curve) passing through a set of
three starting points x0, x1 and x2.
the root t of Eq. (3.50) is required to decay to zero. Therefore, to avoid the loss of significant
digits in the roots of Eq. (3.50) one can choose the formula
t = −2c
b ± √b2 − 4ac . (3.53)
Of these two roots the one which has the smallest absolute value is chosen. The smallest one
can be identified by inspecting the sign of b. For b > 0, the positive sign gives the smallest
root. For b < 0, use the negative sign. Then, x3, the next approximation to x∗, is
x3 = x2 + t . (3.54)
Next, out of x0, x1 and x2 find the two points which are closer to x3. Relabel them as x0
and x1. Relabel x3 as x2. Repeat the above process until a root (x2) with a desired accuracy
is reached. The iteration can be stopped when |x3 − x2| ≤ δx and |f(x3)| ≤ δf .
The iteration rule is given by
xn+1 = xn + (xn − xn−1) 2c
b ± √b2 − 4ac . (3.55)
In Eq. (3.53), if b2−4ac < 0 then t’s are complex conjugates. That is, complex roots can also
be determined by employing the M¨uller method. This is a great advantage of this method.
Assume that the given equation has both real and complex roots. For certain real values
of the starting points x0, x1 and x2 one may encounter complex values of t. Consequently,
successive approximation may lead to a complex root. If one’s aim is to find a real root then
the imaginary part can be made zero and proceed with real numbers.
Example:
Find the root of the equation f(x)=ex −1 = 0. Choose δx = δf = 10−5, x0 = 0.9, x1 = 0.7
and x2 = 0.5.48 Solution of General Nonlinear Equations
TABLE 3.7
The M¨uller method: Successive approximation of the real root of equation f(x)=ex−1 = 0.
The starting points are x0 = 0.9, x1 = 0.7, x2 = 0.5 and δx = δf = 10−5. After 5 iterations
|x3 − x2| < δx and |f(x3)| < δf so root is x∗ = 0.
Iteration x0 x1 x2 x3 f(x3)
0 0.900000 0.700000 0.500000 − − −− − − −−
1 0.900000 0.700000 0.500000 −0.246809 −0.218710
2 0.700000 0.500000 −0.246809 0.018589 0.018762
3 0.500000 −0.246809 0.018589 0.000404 0.000404
4 −0.246809 0.018589 0.000404 0.000000 0.000000
5 0.018589 0.000404 0.000000 0.000000 0.000000
Table 3.7 displays the numerical computation. The conditions |x3−x2| ≤ δx and |f(x3)| ≤ δf
are met at the end of 5th iteration. Therefore, the root is x∗ = 0.
Compared with other methods described earlier, the M¨uller method involves a lot of
calculations at each stage of approximation. The order of convergence of the M¨uller method
is 1.84. The efficiency index α is also 1.84.
3.7 Chebyshev Method
In the M¨uller method a parabola passing through a set of three starting points is constructed.
In contrast to this in Chebyshev method a parabola passing through a single starting value
is constructed. Assume a polynomial of degree 2 of the form
f (x) = ax2 + bx + c = 0 , (3.56)
where a, b and c are to be determined. For the initial guess x0, Eq. (3.56) is
f (x0) = ax2
0 + bx0 + c . (3.57)
Then, f
(x0) and f(x0) are obtained as
f (x0)=2ax0 + b, f (x0)=2a . (3.58)
Solving Eqs. (3.57) and (3.58) for a, b and c gives
a = 1
2
f (x0) , b = f (x0) − x0f (x0) , (3.59a)
c = f (x0) − x0f (x0) + 1
2
x2
0f (x0) . (3.59b)
Then, substitution for a, b and c in Eq. (3.56) gives
f (x0)+(x − x0) f (x0) + 1
2 (x − x0)
2 f (x0)=0 . (3.60)Comparison of Iterative Methods 49
TABLE 3.8
The Chebyshev method: Successive approximation of the real root of the equation f(x) =
ex − 1 = 0. The initial guess is x0 = 0.5. Here, δx = δf = 10−5 and δs = 10−9. The root is
x∗ = 0.
Iteration x value f(x) value |xn+1 − xn|
0 0.50000 − − − − −− − − −−
1 0.02912 0.29550E−01 0.47088
2 0.00001 0.80550E−05 0.02911
3 0.00000 0.17423E−15 0.00001
From Eq. (3.60), the nth approximation is written as
f (xn)+(x − xn) f (xn) + 1
2 (x − xn)
2 f (xn)=0 . (3.61)
Replacement of x by xn+1 gives
xn+1 = xn − f (xn)
f (xn) − 1
2 (xn+1 − xn)
2 f (xn)
f (xn) . (3.62)
Using the Newton–Raphson formula, Eq. (3.30), for (xn+1−xn) in the right-side of Eq. (3.62)
one obtains
xn+1 = xn − f (xn)
f (xn) − 1
2
f 2 (xn) f (xn)
(f (xn))3 , f (xn) = 0 . (3.63)
When f is neglected assuming that |f|  1 then Eq. (3.63) becomes the Newton–Raphson
rule (3.30). Thus, the iteration rule (3.63) can be considered as an improved version of the
Newton–Raphson method.
The order of convergence of the Chebyshev method is 3. (The proof of this is left as
an exercise to the reader.) At each iteration, the values of f, f and f are required in the
Chebyshev method. Thus, m in Eq. (1.22) is 3 and the efficiency index α = 31/3 ≈ 1.44.
Example:
Compute the approximate real root of the equation ex−1 = 0 with |xn+1−xn| ≤ δx = 10−5,
|f(xn+1)| ≤ δf = 10−5 and δs = 10−9.
Table 3.8 presents the result, where the starting value x0 is 0.5. The significance of the
third term in the right-side of Eq. (3.63) can be seen by comparing the tables 3.4 and 3.8.
In the Newton–Raphson method for x0 = 0.5 the root with δx = δf = 10−5 is obtained at
5th iteration. In the Chebyshev method convergence is reached at 3rd iteration. That is,
convergence is faster in the Chebyshev method.
3.8 Comparison of Iterative Methods
The applicability and efficacy of various iterative methods described in this chapter can be
compared on the basis of50 Solution of General Nonlinear Equations TABLE 3.9 A comparative study of various iterative methods of finding roots of nonlinear equations. Method Formula No. of starting values Order of conv. Effi- ciency index Reliability of convergence Advantages and disadvantages Bisection xn+1 = (xn + xn−1)/2 2 1 1 Convergence is guaranteed. Applicable to equations in which f(x) crosses the exact roots. Start- ing values must enclose a root. False position xn+1 = xn−1f(xn)−xnf(xn−1) f(xn)−f(xn−1) 2 1 1 Convergence is guaranteed. Applicable to equations in which f(x) crosses the exact roots. Start- ing values must enclose a root. Secant xn+1 = xn−1f(xn)−xnf(xn−1) f(xn)−f(xn−1) 2 1.62 1.62 Convergence is not guaranteed. The starting values need not en- close a root. It can be applied to compute complex roots. Newton– Raphson xn+1 = xn − f(xn) f(xn) 1 2 1.41 Fast converge- nce if x0 is close to a root. Sensitive to x0. Both real and complex roots can be calculated. Easily extendable to coupled equations. Chebyshev xn+1 = xn − f(xn) f(xn) −12 f2(xn)f(xn) [f(xn)]3 1 3 1.44 Faster than the Newton– Raphson rule. Complex roots can also be calcu- lated. Easily extendable to coupled equations. M¨uller xn+1 = xn + (xn − xn−1) × 2c b±√b2−4ac 3 1.84 1.84 Sensitive to the starting values. Complex roots can be computed. Three starting points are required.Concluding Remarks 51
1. order of convergence,
2. efficiency index and
3. sensitive dependence on starting value(s).
The bisection and the false position methods are slow but convergence to an actual
root is guaranteed. These methods are applicable only to equations f(x) = 0, where f(x)
crosses the exact roots. The secant, Newton–Raphson, Chebyshev and M¨uller methods can
be applied to determine both real and complex roots. But in the first three methods, to
obtain a complex root the starting value(s) must be complex. In the M¨uller method a
complex root can be obtained for real starting values. Of the various methods studied in
this chapter the Newton–Raphson method is very simple and is widely used. This method
and its improvement, Chebyshev method, alone require one initial guess. Table 3.9 gives a
brief comparison of various iterative methods.
3.9 Concluding Remarks
Among the various methods discussed in this chapter for numerically computing the roots
of a nonlinear equation the Newton–Raphson method is often preferred because of the
simplicity of the formula, easy to develop a computer program and the order of convergence
is quadratic. The iterative methods, particularly the Newton–Raphson method is of great
use in the field of fractals. For example, consider the equation P(z) = z2 + 1. Its roots are
not real but complex z∗ = ±i1. The set of z0 (initial guess) converging to a particular root
(for example, through the Newton–Raphson method) is called its basin of attraction. The
boundary of the basin of attraction is not a simple curve but a complicated curve exhibiting
a self-similar structure characterized by noninteger dimension. Such a pattern is called a
fractal. Many complex functions have fractal basins of attraction. Iterative methods have
applications in estimating regression models, for example, in finding the values of parameters
in the Poisson, logistic and binomial regression models. Newton–Raphson type methods are
used in stochastic programming and portfolio management.
3.10 Bibliography
[1] T.J. Ypma, SIAM Rev. 37:531, 1995.
[2] P.W. France, Solution of various hydraulic problems using the Newton-Raphson
method. In the Proc. of 2nd International Conference on Computer Methods in
Water Resources, Published by Computational Mechanics, Southampton, Eng￾land, 1991. pp.3−70.
[3] D.M. Bonilla-Correa, O.E. Coronado-Hernandez, V.S. Fuertes-Miquel, M. Be￾sharat and H.M. Ramos, Water 15:1304, 2023.
[4] E.W. Martin and G. Peters, J. Inst. Water Engrs. 17:115, 1963.
[5] A.K. Sameni, Application of Newton-Raphson method in three-phase unbalanced
power flow. Toronto Metropolitan University, Thesis, 2010. https://doi.org/10.329
20/ryerson.14644656.v1 (accessed on June 10, 2023).52 Solution of General Nonlinear Equations
[6] S. Nirupama, Newton-Raphson Method to Solve Power Flow Problem: Electrical
Engineering. Engineering Notes, 22 August 2017. https://www.engineeringenotes.
com/electrical-engineering/power-flow/newton-raphson-method-to-solve-power￾flow-problem-electrical-engineering/25354 (accessed on June 10, 2023).
[7] H.R. Bakari, T.M. Adegoke and A.M. Yahya, Int. J. Math. Stat. Stu. 4(4):21,
2016.
[8] R. Wolff and A. Kubo, J. Tribol. 116:733, 1994.
[9] J. Viji Priya and S. Suppiah, J. Comp. Sci. 9:566, 2013.
[10] B.C. Truong, N.V. Thuan, N.H. Hau and M. McAleer, Adv. Dec. Sci. 23(4), 2019.
[11] J. Kou, Appl. Math. Comput. 189:602, 2007.
[12] C. Chun, Appl. Math. Comput. 189:1384, 2007.
[13] M.A. Noor, Appl. Math. Comput. 191:128, 2007.
[14] C. Chun, Appl. Math. Comput. 191:193, 2007.
[15] S. Abbasbandy, Appl. Math. Comput. 145:887, 2003.
[16] W. Nazeer, A. Naseem, S.M. Kang and Y.C. Kwun, J. Nonl. Sci. Appl. 9:2923,
2016.
3.11 Problems
A. Bisection Method
3.1 For the following nonlinear equations calculate the number of iterations N nec￾essary to obtain an interval with width less than the tolerance δx = 10−2 for the
specified starting points. Find the root after N + 1 iterations.
(a) ex − 1 = 0, (x0, x1)=(−0.2, 0.1).
(b) x ex − 1 = 0, (x0, x1) = (0.5, 0.6).
(c) 0.5 sin x + 0.75 cos x = 0, (x0, x1)=(−1, −0.9)
(d) x3 − 1 = 0. (x0, x1) = (0.9, 1.05).
(e) x3 − 8 = 0, (x0, x1) = (1.9, 2.2).
3.2 Write a few equations which cannot be solved by the bisection method.
3.3 Can you apply the bisection method to the equation 1 − sin x = 0? Why?
B. False Position Method
3.4 Compute a real root of the following nonlinear equations with the specified start￾ing values. Stop the iteration when the width of the interval is less than the
tolerance δx = 10−2 or |f(x)| ≤ δf = 10−5.
(a) ex − 1 = 0, (x0, x1)=(−0.2, 0.1).
(b) x ex − 1 = 0, (x0, x1) = (0.5, 0.6).
(c) 0.5 sin x + 0.75 cos x = 0, (x0, x1)=(−1, −0.9).
(d) x3 − 1 = 0, (x0, x1) = (0.9, 1.05).
(e) x3 − 8 = 0, (x0, x1) = (1.9, 2.2).
3.5 List out the differences between the bisection and the false position methods.Problems 53
C. Secant Method
3.6 Compute a real root of the following equations with the specified starting values.
Stop the iteration when |xn+1 − xn| ≤ δx = 10−2 or |f(x)| ≤ δf = 10−5.
(a) ex − 1 = 0, (x0, x1)=(−0.2, 0.1).
(b) x ex − 1 = 0, (x0, x1) = (0.5, 0.6).
(c) 0.5 sin x + 0.75 cos x = 0, (x0, x1)=(−1, −0.9).
(d) x3 − 1 = 0, (x0, x1) = (0.9, 1.1).
(e) x3 − 8 = 0, (x0, x1) = (1.0, 1.8).
D. Newton–Raphson Method
(i) Real roots of one-dimensional equations
Some of the following problems can also be solved by other methods. In all the following
root finding problems choose δx = δf = 10−5 and δs = 10−9.
3.7 Find a real root of the following equations with the given starting point.
(a) ex − 1 = 0, x0 = 0.09.
(b) x ex − 1 = 0, x0 = 0.4.
(c) 0.5 sin x + 0.75 cos x = 0, x0 = −1.
(d) x3 − 1 = 0, x0 = 1.1.
(e) x3 − 8 = 0, x0 = 2.5.
3.8 Find the square root of 11. [Hint: Assume that the square root of 11 as x. Then,
one has the equation x2 − 11 = 0. A root of this equation is the square root of
11. Find the root with x0 = 3].
3.9 Determine the value of (50)1/3. Choose the initial guess as 4.
3.10 Given the value of π as 3.14159 compute 1/π.
3.11 Consider a unit line segment whose ends are marked as A and B and C is a point
on the line segment such that AB/AC = AC/CB. Find the length of AC.
3.12 The motion of a particle along a one-dimensional line is described by the equation
x(t) = c v 
1 − e−t/c
,
where x is the distance of the particle at time t from the origin, v is the initial
velocity of the particle and c is a constant whose value is 5. If the initial velocity
of the particle is 2 m/sec find the time required to travel through a distance 5 m.
Choose the initial guess of t as 2 sec. Compare the numerical result with the exact
result.
3.13 In celestial mechanics, Kepler’s equation is y = x−α sin x, where y is the planet’s
mean anomaly, x is its eccentric anomaly and α is the eccentricity of its orbit. If
y = 1 and α = 0.75 for a planet then find its eccentric anomaly x with the initial
guess x0 = 1.5.
3.14 The wave function of a quantum mechanical particle is given by
ψ = A sin 2πx
A + α

.
If α = 0.5, A = 3 and π = 3.14159 find the value of x at which a) ψ = 0 and b)
ψ = 0.5. Choose the initial guess of x as 0.54 Solution of General Nonlinear Equations
3.15 The energy of the lowest molecular orbital occupied two electrons in the ground
state of lithium hydride molecule is the negative root of the equation
0.75E2 + 0.35E − 0.023 = 0 .
Determine the lowest energy after three iterations with E0 = −0.5 units.
3.16 The ripple factor of a full wave rectifier is given by
r =

(Irms/Idc)
2 − 1.
If one wishes to have a ripple factor 0.482 determine the value of (Irms/Idc) with
the starting value of it as 1.
3.17 The relation between p-n junction diode current, voltage and temperature is
I = I0

eV /VT − 1

.
If I/I0 = 1 at room temperature and V = 1 V compute VT with VT,0 = 1.5 V and
δx = δf = 10−2.
3.18 The frequency of oscillation of a LCR circuit is given by
f = 1
2π
 1
LC − R2
4L2
1/2
.
What value of R has to be chosen in order to have f = √0.75/(2π), L = 1 H,
C = 1 F? Choose the initial guess of R as 0.8 Ω.
3.19 The Planck’s radiation formula is given by
u = 8πkBT
4c4
x5
ex − 1 ,
where x = c/(kBT λ) and , c kB are constants. T and λ are the temperature
and the wavelength of the radiation, respectively. Find the value of λ or x at
which u is a maximum.
3.20 For the equation cos x = 0, where x is in radian, can one use x0 = 0 to find its
root? Why?
3.21 Show that for the problem of finding the square-root of A > 0, the Newton–
Raphson formula reduces to xn+1 = 
x2
n + A

/ (2xn).
3.22 For the equation x2e−x + x = 0 simplify the Newton–Raphson formula for xn+1.
3.23 A man is running at a speed of 3 m/sec to catch a bus that is stopped at a traffic
light. When he is still a distance of 1 m from the bus the light changes and the
bus starts to move away from the running man with a constant acceleration of
2 m/sec. If T is the time at which the man catches the bus then it is the root of
the equation T2 − 3T + 1 = 0. By the Newton–Raphson method with T0 = 2 sec
find the value of T after four iterations. Also, compute the exact time.
(ii) Complex roots of one-dimensional equations
3.24 Find the complex roots of the following equations with the given starting value.
Use δx = δf = 10−5 and δs = 10−9.Problems 55
(a) ex + e−x = 0, (xR0 + i xI0) = (0.5 + i 1).
(b) x3 + 1 = 0, (xR0 + i xI0) = (0.5 + i 1).
(c) x3 − 1 = 0, (xR0 + i xI0)=(−0.5 − i 1).
(d) ex + 1 = 0, (xR0 + i xI0) = (0 + i 3).
(e) x3 − 3x2 + 7x − 5 = 0, (xR0 + i xI0) = (1.0 + i 1).
3.25 Can the Newton–Raphson method be used to find √−11 ? How?
(iii) Real roots of two-coupled equations
3.26 Compute a real root of the following equations. Choose δx = δy = δf = δg = 10−5
and δs = 10−9.
(a) x3 + 2y + 3 = 0, y2 + 3xy + 2 = 0, (x0, y0) = (1, −1.5).
(b) x2 + y − 4 = 0, xy − 3 = 0, (x0, y0) = (1, 3).
(c) x4 + y3 − 8 = 0, y2 + 2xy + x − 4 = 0, (x0, y0) = (0, 1.9).
(d) x2 + x − y = 0, xy − 2 = 0, (x0, y0) = (1, 3).
D. M¨uller Method
(i) Real roots of one-dimensional equations
3.27 Compute a real root of the following equations with the given starting values. Stop
the iteration when |xn+1 − xn| ≤ 10−5 and |f(xn+1)| ≤ 10−5 or |t0t1(t1 − t0)| ≤
δs = 10−9.
(a) ex − 1 = 0, (x0, x1, x2) = (0.3, 0.2, 0.1).
(b) x ex − 1 = 0, (x0, x1, x2) = (0.4, 0.5, 0.6).
(c) 0.5 sin x + 0.75 cos x = 0, (x0, x1, x2)=(−0.7, −0.8, −0.9).
(d) x3 − 1 = 0, (x0, x1, x2) = (0.8, 1.0, 1.2).
(ii) Complex roots of one-dimensional equations
3.28 Obtain a pair of complex roots of the following equations. Use the specified start￾ing values. Stop the iteration when |xn+1 − xn| ≤ 10−5 and |f(xn+1)| ≤ 10−5.
(a) ex + e−x = 0, (x0, x1, x2) = (0 + i 1.4, 0+i1.5, 0+i1.6).
(b) x3 + 1 = 0, (x0, x1, x2) = (0.4+i0.8, 0.5+i0.8, 0.6+i0.8).
(c) x3 − 1 = 0, (x0, x1, x2)=(−0.4, −0.5, −0.6).
(d) ex + 1 = 0, (x0, x1, x2) = (0.2+i2.8, 0+i3, 0+i3.2).
E. Chebyshev Method
3.29 Obtain the roots of the equations given in Problem 3.7.
3.30 Show that the order of convergence of the Chebyshev method is 3.4
Solution of Linear Systems AX = B
4.1 Introduction
In many branches of physics and engineering, it is often required to solve a set of simulta￾neous linear equations. A general form of a system of n linear equations with n variables is
given by
a11x1 + a12x2 + ··· + a1nxn = b1 ,
a21x1 + a22x2 + ··· + a2nxn = b2 ,
.
.
. (4.1)
an1x1 + an2x2 + ··· + annxn = bn ,
where aij ’s and bi’s are known constants and xi’s are to be determined. The above system
of linear equations can conveniently be expressed in matrix form as
AX = B . (4.2)
Denote the elements of A as aij and B as bi. When all bi’s are zero then the system (4.2)
is said to be homogeneous. If at least one of the bi’s is nonzero then the system (4.2) is
called inhomogeneous. An inhomogeneous system has a nontrivial solution if and only if
the determinant of A is nonzero.
System (4.2) actually represents a wide class of problems. Some of them are the follow￾ing:
(1) Straight-forward problem
The number of variables is equal to the number of equations, that is, A is a square
matrix, B = 0 and det A = 0. If det A = 0 then the problem is a singular value
and it belongs to class (3).
(2) The least-squares
The number of variables is less than the number of equations.
(3) The singular-value problem
The number of variables is more than the number of equations. Such systems have
infinitely many solutions. The solution closest to the origin is called a singular￾value solution.
(4) The eigenvalue problem
The number of variables is equal to the number of equations and B = 0. The
trivial solution is X = 0. The condition for a nontrivial solution is det A = 0. For
det A = 0 the only solution is X = 0.
DOI: 10.1201/9781032649931-4 56Introduction 57
4V
8V
A
B D
C
I
1 I
1
I
2 I
2
I
3 I
3
R1 R2
R3
R4
FIGURE 4.1
A simple dc circuit.
Systems of linear equations occur in optimal planning in industrial engineering, geome￾try, structural analysis in civil engineering and the representation of chemical reactions in
equation forms. The equations of an equilibrium point of a system of first-order coupled
autonomous linear differential equations essentially form a system of linear equations. In
business management linear equations are used to compute cost, revenue, profit and loss in
a business. In electrical engineering systems, coupled linear equations arise in writing the
equations for the currents through various branches in circuits. Electrical networks contain
number of resistances and circuits. Determination of currents flowing through various paths
or branches of an electrical circuit and electrical networks involves solving a system of linear
equations.
Consider the circuit shown in Fig. 4.1 [1]. Applying the Ohm’s and Kirchhoff’s law at
the junctions B and D and the closed paths ABDA and CBDC gives
I1 + I2 − I3 = 0 , (4.3a)
(R1 + R2) I1 + R3I3 = 4 , (4.3b)
R4I2 + R3I3 = 8 . (4.3c)
Solving these system of linear equations gives the values of the currents I1, I2 and I3. An
unbalanced chemical reaction can be written in a system of linear equations and solving it
the reaction equation can be balanced (see Section 4.13).
In this chapter, the prime emphasis is on solving the class (1) type of problem. Solutions
of such systems can be determined either by direct methods or iterative methods. Direct
methods, in the absence of round-off and other errors, produce an exact solution after a
finite number of arithmetic operations. On the other hand, the iterative methods start with
an initial approximation to the solution, give a sequence of approximate solutions, which
converge to the exact solution. Some advantages of iterative methods are:
1. They can be easily implemented on computers.
2. They are relatively insensitive to the growth of round-off errors.
Systems of classes (3) and (4) can also be solved for a nontrivial solution which will be58 Solution of Linear Systems AX = B
presented at the end of this chapter. To start with, in the next section, the famous Cramer’s
rule for solving a system of simultaneous equations is discussed. This rule is more suitable
for two- and three-coupled equations.
4.2 Cramer’s Rule
Let us first obtain the Cramer’s rule for determining the unknowns in the two-coupled linear
equations of the form
a11x1 + a12x2 = b1 , (4.4a)
a21x1 + a22x2 = b2 . (4.4b)
For a nontrivial solution, the determinant of the coefficient matrix is nonzero. That is,
|A| = a11a22 − a21a12 = 0 . (4.5)
Solving Eqs. (4.4) for x1 and x2 gives
x1 = a22b1 − a12b2
a11a22 − a21a12
, x2 = a11b2 − a21b1
a11a22 − a21a12
. (4.6)
Defining
B1 =
 b1 a12
b2 a22 
, B2 =
 a11 b1
a21 b2

(4.7)
the solution (4.6) can be expressed as
x1 = |B1|
|A| , x2 = |B2|
|A| . (4.8)
For a system of n linear equations
xi = |Bi|
|A| , (4.9)
The above process is known as Cramer’s rule. Since the solution given by Eq. (4.9) involves
the calculation of (n + 1) determinants the method is inconvenient when n > 3.
4.3 Upper- and Lower-Triangular Systems
A system of linear Eqs. (4.2) with A being an upper-triangular coefficient matrix with all
diagonal elements nonzero can be solved by the back-substitution algorithm. An upper￾triangular system is of the form
a11x1 + a12x2 + ··· + a1,n−1xn−1 + a1nxn = b1 ,
a22x2 + ··· + a2,n−1xn−1 + a2nxn = b2 ,
.
.
. (4.10)
an−1,n−1xn−1 + an−1,nxn = bn−1 ,
annxn = bn ,Upper- and Lower-Triangular Systems 59
where aii = 0, i = 1, 2,...,n. The last equation of the system (4.10) is solved first which
gives
xn = bn
ann
. (4.11)
Then, solving the second last equation of the system (4.10) for xn−1 gives
xn−1 = bn−1 − an−1,nxn
an−1,n−1
, (4.12)
where xn is given by Eq. (4.11). Knowing xn and xn−1 the third last equation of the system
(4.10) can be used to solve xn−2 and so on. The general formula for xk is
xk = 1
akk 
bk − n
i=k+1
akixi

, k = n, n − 1,..., 2, 1. (4.13)
In the above formula when k = n, the summation becomes n
i=n+1 which is interpreted
as the sum over no terms and gives the value 0. Since akk occurs in the denominator in
Eq. (4.13) the requirement is that all akk = 0. The above process of determining the solution
of the system (4.10) is called back-substitution.
System (4.2) with A being a lower-triangular matrix can be solved by forward￾substitution algorithm, where the formula for xk is given by
xk = 1
akk 
bk −
k
i=1
akixi

, k = 1, 2, . . . , n. (4.14)
Example:
Consider the system
x1 + 2x2 + x3 = 4 , (4.15a)
x2 − x3 = 3 , (4.15b)
x3 = −1 . (4.15c)
Here, a11 = 1, a12 = 2, a13 = 1, a22 = 1, a23 = −1, a33 = 1, b1 = 4, b2 = 3 and b3 = −1.
The last subequation straight-forwardly gives x3 = −1. Then, for k = 2 Eq. (4.13) gives
x2 = 1
a22
(b2 − a23x3)=1 × (3 − 1) = 2 . (4.16)
When k = 1
x1 = 1
a11
(b1 − a12x2 − a13x3)=1 × (4 − 2 × 2 − 1 × −1) = 1 . (4.17)
The solution of the system (4.15) is (x1, x2, x3) = (1, 2, −1).
Triangular linear systems occur in the least-squares problems [2] and splitting-based
iterative schemes and in the Cholesky factorizations [3]. It is convenient to reduce a general
system of linear equations into an upper- or lower-triangular system and obtain the solution
by the back-substitution.60 Solution of Linear Systems AX = B
4.4 Gauss Elimination Method
The back-substitution algorithm described in the previous section can be applied without
difficulty to solve triangular system of equations. How does one proceed if A is not an
upper- or lower-triangular form? In this case, the system (4.2) can be transformed into an
equivalent system with an upper-triangular form and then it can be solved by the back￾substitution. This process of solving the system (4.2) is called Gauss elimination. The name
is due to the fact that equations are combined to eliminate the unknowns. The method is
straight forward, easily understandable and pedagogical.
Before going to describe the Gauss elimination method let us first recall the basic el￾ementary transformations and the notion of augmented matrix, associated with the linear
system of equations, which are essential to follow the underlying method.
4.4.1 Elementary Operations and Augmented Matrix
The solution of a given set of equations is unaffected by the following operations:
1. multiplication of an equation by a nonzero constant,
2. interchange of two equations and
3. replacement of an equation by the sum of that equation and a multiple of any
other equation.
Practically, it is convenient to work with the so-called augmented matrix of the system
(4.2). The augmented matrix denoted by [A, B] is given by
[A, B] =


a11 a12 ··· a1n b1
a21 a22 ··· a2n b2
.
.
.
an1 an2 ··· ann bn

 =


a11 a12 ··· a1n a1,n+1
a21 a22 ··· a2n a2,n+1
.
.
.
an1 an2 ··· ann an,n+1

 . (4.18)
It is a n × (n + 1) matrix with the elements of B forming the (n + 1)th column and the
elements of A forming the rest. Since Eqs. (4.2) and (4.18) are equivalent, the elementary
transformations listed above are also applicable to Eq. (4.18).
4.4.2 Systematic Procedure
Let us proceed to describe the various steps involved in the Gauss elimination method. For
simplicity and to avoid writing many ellipses (...) the method is described for a system of
three equations. Extension to a large number of equations is straight forward.
(i) Pivot Row and Pivot Element
Consider the system
a11x1 + a12x2 + a13x3 = b1 , (4.19a)
a21x1 + a22x2 + a23x3 = b2 , (4.19b)
a31x1 + a32x2 + a33x3 = b3 . (4.19c)
Eliminating x1 in Eqs. (4.19b) and (4.19c) and then eliminating x2 in the modified
Eq. (4.19c) the system can be brought into an upper-triangular form. For this purposeGauss Elimination Method 61
redefine the elements aij in Eqs. (4.19) as a(1)
ij . The number 1 in the superscript of a(1)
ij
refers to the value of aij at the first stage of elimination. To eliminate x1 in Eq. (4.19b)
multiply Eq. (4.19a) by −a(1)
21 /a(1)
11 and add it to Eq. (4.19b). Similarly, to eliminate x1 in
Eq. (4.19c) multiply Eq. (4.19a) by −a(1)
31 /a(1)
11 and add it to Eq. (4.19c). These give
a(2)
22 x2 + a(2)
23 x3 = b
(2)
2 , (4.20a)
a(2)
32 x2 + a(2)
33 x3 = b
(2)
3 , (4.20b)
where
a(2)
22 = a(1)
22 − a(1)
21
a(1)
11
a(1)
12 , a(2)
23 = a(1)
23 − a(1)
21
a(1)
11
a(1)
13 , (4.21a)
a(2)
32 = a(1)
32 − a(1)
31
a(1)
11
a(1)
13 , a(2)
33 = a(1)
33 − a(1)
31
a(1)
11
a(1)
13 , (4.21b)
b
(2)
2 = b
(1)
2 − a(1)
21
a(1)
11
b
(1)
1 , b(2)
3 = b
(1)
3 − a(1)
31
a(1)
11
b
(1)
1 . (4.21c)
Now, the given system of equations becomes
a(1)
11 x1 + a(1)
12 x2 + a(1)
13 x3 = b
(1)
1 , (4.22a)
a(2)
22 x2 + a(2)
23 x3 = b
(2)
2 , (4.22b)
a(2)
32 x2 + a(2)
33 x3 = b
(2)
3 , (4.22c)
Next, to eliminate x2 in Eq. (4.22c) multiply Eq. (4.22b) by −a(2)
32 /a(2)
22 and add it to
Eq. (4.22c). This gives
a(3)
33 x3 = b
(3)
3 , (4.23)
where
a(3)
33 = a(2)
33 − a(2)
32
a(2)
22
a(2)
23 , b(3)
3 = b
(2)
3 − a(2)
32
a(2)
22
b
(2)
2 . (4.24)
Then, the equivalent upper-triangular form of the system Eq. (4.19) is given by
a(1)
11 x1 + a(1)
12 x2 + a(1)
13 x3 = b
(1)
1 , (4.25a)
a(2)
22 x2 + a(2)
23 x3 = b
(2)
2 , (4.25b)
a(3)
33 x3 = b
(3)
3 , (4.25c)
The system (4.25) can be solved by back-substitution.
Let us generalize the above procedure for n number of equations. For simplicity focus
the analysis on the augmented matrix. The formula for the elements aij after (k+1)th stage
of elimination is given by
a(k+1)
ij = a(k)
ij − a(k)
ik
a(k)
kk
a(k)
kj , (4.26a)
where
k = 1, 2,...,n − 1; i = k + 1, k + 2,...,n; (4.26b)
j = k + 1, k + 2,...,n + 1. (4.26c)62 Solution of Linear Systems AX = B
At kth stage of elimination, the unknown xk is eliminated in the equations appearing below
the kth equation using the kth equation. Therefore, at kth stage, the kth equation in the
system is crucial or essential. Because of this at kth stage of elimination, the kth equation
is called pivot equation (or pivot row if one works with augmented matrix). The coefficient
a(k)
kk is called pivot element.
In order to use the method in most circumstances two modifications are necessary in
the Gauss elimination method. These are described below.
(ii) Pivoting
If a(1)
11 = 0 then the first row in the augmented matrix does not help us to eliminate the
elements in the first column appearing below a(1)
11 . Similarly, if a(2)
22 = 0 then elimination
of the elements appearing below the second row in the second column using the second
row is not possible. In general, if a(k)
kk = 0 then the elements in the kth column appearing
below the kth row cannot be eliminated using the kth row. How to overcome this difficulty?
Whenever a(k)
kk = 0 identify the row r with a(k)
rk = 0, r>k and interchange the rows r and
k. After this interchange the new pivot element a(k)
kk is nonzero. This process is known as
pivoting.
(iii) Partial Pivoting
To reduce the propagation of errors due to fixed-finite precision arithmetic, at kth stage
of elimination process, irrespective of whether a(k)
kk is zero or not, the row r with r ≥ k for
which the element a(k)
rk has the largest absolute value, that is,



a(k)
rk



= max.



a(k)
kk


,



a(k)
k+1,k


,...,



a(k)
nk




(4.27)
and the row k is interchanged. This process is known as partial pivoting. If all a(k)
rk are zero
then the matrix A is singular.
4.4.3 Summary of Gauss Elimination Method
The various steps involved in the method are summarized below.
(1) Write the augmented matrix for the given system of n equations.
(2) Pivoting: At kth stage of elimination, find the row r with max.|a(k)
rk | = 0, r ≥ k. If
r = k interchange the rows r and k. If all a(k)
rk are zero then the matrix is singular
and no solution exists.
(3) Eliminate the elements a(k)
rk in all the rows r>k.
(4) Repeat the steps (2) and (3) until the elimination is completed.
Example:
Find the point of intersection of the three straight-lines governed by the equations
x1 + x2 + x3 = 2, (4.28a)
2x1 + x2 + 2x3 = 2, (4.28b)
3x1 − x2 + 2x3 = −1. (4.28c)
in the x1 − x2 − x3 space.Gauss Elimination Method 63
At the point of intersection, all the given equations must be simultaneously satisfied. There￾fore, treat the above three equations as a system of coupled equations. Its solution is the
point of intersection.
Let us solve the given set of equations applying the Gauss elimination method. The
augmented matrix of the system (4.28) is


a(1)
11 a(1)
12 a(1)
13 b
(1)
1
a(1)
21 a(1)
22 a(1)
23 b
(1)
2
a(1)
31 a(1)
32 a(1)
33 b
(1)
3

 =


1 11 2
2 12 2
3 −1 2 −1

 (4.29)
Because 
a(1)
31

 = 3 is the maximum of {

a(1)
11

 = 1, 
a(1)
21

 = 2, 
a(1)
31

 = 3} interchange
the rows 1 and 3. After this interchange the augmented matrix is (the pivot element is
underlined)


3 −1 2 −1
2 12 2
1 11 2

 . (4.30)
To eliminate the element a(1)
21 = 2 in the above matrix multiply the row 1 by −a(1)
21 /a(1)
11 =
−2/3 and add it with the row 2. Similarly, multiplication of the row 1 by −a(1)
31 /a(1)
11 = −1/3
and adding it with the row 3 eliminates a(1)
31 . Then, the augmented matrix is


3 −1 2 −1
2+3 × −
2
3
1 − 1 × −
2
3 2+2 × −
2
3
2 − 1 × −
2
3
1+3 × −
1
3
1 − 1 × −
1
3
2 × 1 − 1
3
2 − 1 × −
1
3


=


3 −1 2 −1
0 5/3 2/3 8/3
0 4/3 1/3 7/3

 . (4.31)
The next step is to eliminate a(2)
32 (that is, x2) in the last row of the above matrix. Since

a(2)
22

 = 5/3 > 
a(2)
32

 = 4/3 no need for interchanging the rows. Elimination of a(2)
32 leads to
the matrix


3 −1 2 −1
0 5/3 2/3 8/3
0
4
3 − 4
5
×
5
3
1
3 − 4
5
×
2
3
7
3 − 4
5
×
8
3


=


3 −1 2 −1
0 5/3 2/3 8/3
0 0 −1/5 1/5

 . (4.32)
In equational form, the above matrix is the upper-triangular system
3x1 − x2 + 2x3 = −1 , (4.33a)
5
3
x2 +
2
3
x3 = 8
3 , (4.33b)
−1
5
x3 = 1
5 . (4.33c)
The last equation gives x3 = −1. Back-substitution of the value of x3 in the second equation64 Solution of Linear Systems AX = B
of (4.33) gives x2 = 2. Then, substitution of x2 = 2 and x3 = −1 in the first equation of
Eqs. (4.33) gives x1 = 1. The solution is (x1, x2, x3) = (1, 2, −1).
Back-substitution is possible in the augmented matrix given by Eq. (4.32). The first
back-substitution gives


a(1)
11 a(1)
12 a(1)
13 b
(1)
1
a(2)
22 0 b
(2)
2 = b
(2)
2 − a(2)
23 b
(3)
3 /a(3)
33
a(3)
33 b
(3)
3


=⇒


3 −1 1 −1
5/3 0 10/3
−1/5 1/5

 . (4.34)
The second back-substitution gives


a(1)
11 0 0 b
(1)
1 − a(1)
12 b
(2)
2 /a(2)
22 − a(1)
13 b
(3)
3 /a(3)
33
a(2)
22 0 b
(2)
2
a(3)
33 b
(3)
3


=


30 0 3
5/3 0 10/3
−1/5 1/5

 . (4.35)
Dividing each row by its diagonal element one obtains


100 1
10 2
1 −1

 . (4.36)
The last column gives the solution as (x1, x2, x3) = (1, 2, −1) and is the point of intersection.
In the case of hand calculation, to avoid confusion, after obtaining the upper-triangular
matrix the back-substitution can be done on the equational form of the matrix.
4.4.4 Significance of Pivoting
What is the significance of pivoting strategy? Is this process necessary? Pivoting is not nec￾essary if the arithmetic operations in the Gauss elimination method are performed with
infinite-precision accuracy. If the calculations are done without pivoting but with a finite￾precision then the method will lead to an incorrect result in many circumstances. For ex￾ample, the exact solution of the system
0.0002x1 + 1.572x2 = 1.575 , (4.37a)
0.3210x1 + 1.231x2 = 6.046 (4.37b)
is (x1, x2) = (15, 1). Now, apply the Gauss elimination method without and with pivoting
using four-decimal floating arithmetic and observe the difference.
Solution without pivoting
Elimination of x2 in Eq. (4.37a) by multiplying Eq. (4.37b) by −0.321/0.0002 and adding it
with Eq. (4.37b) gives x2 = 0.9996. Then, Eq. (4.37a) gives x1 = 20 while the exact solution
is x1 = 15, x2 = 1. The method incorrectly predicts the value of x1. What will happen with
pivoting?Gauss–Jordan Elimination Method 65
Solution with pivoting
With pivoting the system (4.37) is rewritten as
0.3210x1 + 1.231x2 = 6.046 , (4.38a)
0.0002x1 + 1.572x2 = 1.575 . (4.38b)
Equation (4.38b) after elimination of x1 becomes 1.571x2 = 1.571 giving x2 = 1. Then,
from Eq. (4.38a) x1 = 15. The obtained solution is identical to the exact solution. This
example clearly illustrates the necessity of pivoting for a general circumstance.
4.5 Gauss–Jordan Elimination Method
Wilhelm Jordan proposed a modification in the Gauss elimination method. He introduced
additional calculations that avoid back-substitution by reducing the augmented matrix into
a diagonal rather than an upper-triangular form. The additional steps are:
1. After interchanging the rows as per the relation (4.27) the pivot equation (that is,
the pivot row in the augmented matrix) is divided by the pivot element thereby
making the pivot element 1.
2. In the Gauss elimination scheme at the kth stage of elimination, the variable xk
(the element ark, r>k in the augmented matrix) is eliminated in the equations
appearing below the kth equation. The other equations, that is r = 1, 2,...,k −1
are undisturbed. In the Gauss–Jordan method the variable xk is eliminated in
the equations 1, 2,...,k − 1 also.
The final augmented matrix will be of the form


1 0 ··· 0 α1
0 1 ··· 0 α2
.
.
.
0 0 ··· 1 αn

 . (4.39)
The last column elements of the final augmented matrix are the unknowns xi’s.
In the Gauss–Jordan method the formula (4.26) takes the form
a(k+1)
ij = a(k)
ij − a(k)
ik a(k)
kj , (4.40a)
where
k = 1, 2,...,n − 1; i = 1, 2,...,n; (4.40b)
j = k + 1, k + 2,...,n + 1. (4.40c)
Remember that a(k)
kk is set into 1 by dividing the kth row by a(k)
kk . Note that when the un￾known xk is eliminated in the Gauss–Jordan method, it is eliminated in all equations except
in the kth equation. In Gauss elimination, xk is eliminated only in the equations appearing
below the kth equation. Further, in the Gauss–Jordan method all rows are normalized by
dividing them by their pivot element resulting in identity matrix contrast to a triangular
matrix in Gauss elimination.66 Solution of Linear Systems AX = B
Example:
Solve the system (4.28) by the Gauss–Jordan method.
The additional calculations involved in the Gauss–Jordan method over Gauss elimination
are indicated by italics. The augmented matrix given by Eq. (4.29) after interchanging the
first and third rows (since |a31| > |a11| and |a21|) is


3 −1 2 −1
2 12 2
1 11 2

 . (4.41)
Dividing the pivot row (row 1) by its pivot element 3, the above matrix becomes


1 −1/3 2/3 −1/3
21 2 2
11 1 2

 . (4.42)
Eliminating a21 and a31 as done earlier in Gauss elimination method, the augmented matrix
becomes


1 −1/3 2/3 −1/3
5/3 2/3 8/3
4/3 1/3 7/3

 . (4.43)
Since |a22| is greater than |a32| interchange of rows is not necessary. Dividing the second
row by its pivot element 5/3, the above matrix becomes


1 −1/3 2/3 −1/3
1 2/5 8/5
4/3 1/3 7/3

 . (4.44)
The next step is elimination of a12 and a32 (that is, x2 in the first and third equations of
the system) in the first and third rows, respectively. After this step, the augmented matrix
is obtained as


10 4/5 1/5
1 2/5 8/5
−1/5 1/5

 . (4.45)
Next, divide the third row by its pivot element −1/5. Then, eliminate a13 and a23. The
augmented matrix now becomes


100 1
10 2
1 −1

 . (4.46)
Thus, the solution is


x1
x2
x3

 =


1
2
−1

 . (4.47)
The solution is obtained without back-substitution. This is the advantage of Gauss–Jordan
method over Gauss elimination.Inverse of a Matrix by the Gauss–Jordan Method 67
4.6 Inverse of a Matrix by the Gauss–Jordan Method
A slight modification of the Gauss–Jordan method will provide an algorithm for finding the
inverse of a matrix. To find the inverse of a matrix A the augmented matrix is written as
[A, I], where I is the unit matrix of order same as A. Performing Gauss–Jordan elimina￾tion on this augmented matrix finally gives the matrix [I,A−1]. In this way, A−1 can be
determined.
Example:
Find the inverse of the matrix
A =


1 02
−1 12
1 −2 0

 (4.48)
applying the Gauss–Jordan method.
The augmented matrix is
[A, I] =


1 02100
−1 12010
1 −20001

 . (4.49)
The augmented matrix after eliminating a21 and a31 is


1 0 2 100
0 1 4 110
0 −2 −2 −101

 . (4.50)
Since |a32| > |a22| interchange the rows 2 and 3 and divide the second row by its pivot
element a22 = −2. Then, eliminating a12 (which is already zero) and a32 leads to the
augmented matrix


102 10 0
0111/2 0 −1/2
0031/21 1/2

 . (4.51)
Finally, eliminating a13 and a23 one has


1002/3 −2/3 −1/3
0101/3 −1/3 −2/3
0011/6 1/3 1/6

 . (4.52)
Thus, the inverse of A is


2/3 −2/3 −1/3
1/3 −1/3 −2/3
1/6 1/3 1/6

 . (4.53)68 Solution of Linear Systems AX = B
4.7 Triangular Factorization or Decomposition Method
Another method of solving coupled linear equations is the triangular factorization or de￾composition method. In this method, the matrix A of the system AX = B, with det A = 0
is expressed as a product of an upper-triangular matrix U with nonzero diagonal elements
and the lower-triangular matrix L with all its diagonal elements being 1. That is,
A = LU. (4.54)
Explicitly,


a11 a12 ··· a1n
a21 a22 ··· a2n
.
.
.
an1 an2 ··· ann

 =


1 0 ··· 0
l21 1 ··· 0
.
.
.
ln1 ln2 ··· 1


×


u11 u12 ··· u1n
0 u22 ··· u2n
.
.
.
0 0 ··· unn

 . (4.55)
How does A decompose into LU? Let us illustrate this for n = 3. Now, Eq. (4.55) becomes


a11 a12 a13
a21 a22 a23
a31 a32 a33

 =


100
l21 1 0
l31 l32 1




u11 u12 u13
0 u22 u23
0 0 u33

 . (4.56)
Carrying out the matrix multiplication on the right of Eq. (4.56) and then equating the cor￾responding elements on both sides of this equation the unknown u’s and l’s are determined
as
u11 = a11, u12 = a12, u13 = a13, l21 = a21/u11 , (4.57a)
u22 = a22 − u12l21, u23 = a23 − l21u13 , (4.57b)
l31 = a31/u11, l32 = (a32 − l31u12) /u22 , (4.57c)
u33 = a33 − l31u13 − l32u23 . (4.57d)
The matrix B is not used in the decomposition of A into LU. Therefore, the obtained LU
can be used to solve any number of systems AX = B having same A but different B. This
is an advantage of LU over the Gauss elimination and the Gauss–Jordan methods.
In terms of L and U the system AX = B is LUX = B. Defining UX = Y the equation
LUX = B is written as LY = B. Y can be determined by solving the system LY =
B and then X by solving the system UX = Y. The system LY = B can be solved by
forward-substitution while the system UX = Y by back-substitution. It is easy to obtain
the determinant of a matrix whose LU form is known. The determinant of A is simply the
product of the diagonal elements of U.
Example:
Consider the system (4.28). From Eqs. (4.57) and (4.28) the u’s and l’s are obtained as
u11 = u12 = u13 = 1, u22 = −1, u23 = 0, u33 = −1, (4.58a)
l21 = 2, l31 = 3, l32 = 4 . (4.58b)Tridiagonal Systems 69
Thus, the matrices L and U are given by
L =


100
210
341

 , U =


111
0 −1 0
0 0 −1

 . (4.59)
The system LY = B is


100
210
341




y1
y2
y3

 =


2
2
−1

 . (4.60)
Solving the above system by forward-substitution gives


y1
y2
y3

 =


2
−2
1

 . (4.61)
Then, the system UX = Y becomes


111
0 −1 0
0 0 −1




x1
x2
x3

 =


2
−2
1

 . (4.62)
The back-substitution in the above system of equations gives the solution as (x1, x2, x3) =
(1, 2, −1).
4.8 Tridiagonal Systems
In certain problems, many coefficients in a given system of equations become zero. One such
system of practical importance is the system of tridiagonal form
d1x1 + c1x2 = b1,
a1x1 + d2x2 + c2x3 = b2,
a2x2 + d3x3 + c3x4 = b3,
······ .
.
. (4.63)
an−2xn−2 + dn−1xn−1 + cn−1xn = bn−1,
an−1xn−1 + dnxn = bn.
From the appearance of Eq. (4.63) notice that interchanging of equations is not needed.
Then, elimination of xk, k = 1, 2,...,n − 1 in (k + 1)th equation using kth equation gives
the following system of equation
d
1x1 +c
1x2 = b
1,
d
2x2 +c
2x3 = b
2,
d
3x3 +c
3x4 = b
3,
· · .
.
.
d
n−1xn−1 +c
n−1xn = b
n−1,
d
nxn = b
n,
(4.64)70 Solution of Linear Systems AX = B
where
d
1 = d1, b
1 = b1, c
k = ck, k = 1, 2,...,n − 1 (4.65a)
d
k = dk − ak−1c
k−1
d
k−1
, k = 2, 3,...,n (4.65b)
b
k = bk − ak−1b
k−1
d
k−1
, k = 2, 3, . . . , n. (4.65c)
System (4.64) is now in a very simple upper-triangular form and can be solved by back￾substitution. The result is
xn = b
n
d
n
, xk = b
k − c
kxk+1
d
k
, k = n − 1, n − 2,..., 1. (4.66)
Example:
Consider the following system of equations
x1 +2x2 = −1,
x1 − x2 + x3 = 4,
2x2 +2x3 +4x4 = 10,
x3 − x4 +2x5 = 0,
4x4 − x5 +x6 = 5,
x5 −x6 = 3.
(4.67)
Elimination of xk in (k + 1)th equation for k = 1, 2,..., 5 gives
x1 +2x2 = −1,
−3x2 + x3 = 5,
8
3
x3 + 4x4 = 40
3 ,
− 5
2
x4 + 2x5 = −5,
11
5
x5 + x6 = −3,
−16
11
x6 = 48
11.
(4.68)
The last equation gives x6 = −3. Substitution of x6 = −3 in the 5th equation gives x5 = 0.
In this way, successive back-substitution leads to x4 = 2, x3 = 2, x2 = −1 and x1 = 1.
Therefore, the solution is
(x1, x2, x3, x4, x5, x6) = (1, −1, 2, 2, 0, −3)
which is the exact solution.
Tridiagonal systems are realized in solving boundary-value problems of linear partial dif￾ferential equations (see Chapter 13), interpolation problems, fluid simulation and scattering
problems.
4.9 Counting Arithmetic Operations
A quantity useful to compare various direct methods of solving a system of linear equa￾tions is the total number of arithmetic operations involved. The method which requiresCounting Arithmetic Operations 71
relatively lesser number of arithmetic operations is preferable. Here, arithmetic operations
refer only the operations such as addition, subtraction, multiplication and division. It is easy
to compute the number of arithmetic operations involved in the direct methods considered
so far. In this section, the number of arithmetic operations required in the Gauss and the
Gauss–Jordan methods are computed. The same for the triangular factorization is left as
an exercise (see Problem 4.15 at the end of this chapter).
4.9.1 Gauss Elimination Method
Let us calculate the number of arithmetic operations required for
1. eliminating the coefficients ak1, k = 2, 3,...,n,
2. complete elimination of the coefficients appearing below the diagonal elements
and
3. back-substitution.
(i) Number of arithmetic operations for eliminating ak1
The first equation of the system is used to eliminate x1 in the remaining (n − 1) equations.
When working with augmented matrix this is equivalent to eliminating ak1 in the rows
k = 2, 3,...,n using the first row. To eliminate x1 in the kth equation, the first equation is
multiplied by −ak1/a11. That is, to eliminate ak1 in the kth row of the augmented matrix
the first row is multiplied by −ak1/a11. Call this quantity as a multiplier, the calculation
of this quantity requires one division operation. Therefore, for (n − 1) rows totally (n − 1)
division operations are required for the calculation of (n − 1) multipliers. That is,
number of division operations = n − 1 . (4.69)
After determining the multiplier −ak1/a11 the first row of the system (4.2) is multiplied
by it. Specifically, the elements a1k, k = 2, 3,...,n and b1 are multiplied by the multiplier.
Then, the resultant row is added to the second row of the augmented matrix. So, for the
second row, the number of multiplications is n and addition is also n. This procedure has
to be repeated for (n − 1) rows appearing below the first row. Therefore,
number of multiplication = n(n − 1), (4.70a)
number of division = n(n − 1). (4.70b)
(ii) Number of operations for complete elimination
The numbers specified in Eqs. (4.70) are only for the elimination of ak1, k = 2, 3,...,n.
The total number of operations for complete elimination of elements appearing below the
diagonal elements are
total division = (n − 1) = 1
2
(n − 1)n , (4.71a)
total multiplication = n(n − 1) = 1
3
(n − 1)n(n + 1), (4.71b)
total addition = n(n − 1) = 1
3
(n − 1)n(n + 1). (4.71c)72 Solution of Linear Systems AX = B
(iii) Number of operations in back-substitution
In the back-substitution algorithm
multiplication operation = 1 + 2 + ··· + (n − 1)
= 1
2
(n − 1)n , (4.72a)
subtraction (from bi

s) = 1 + 2 + ··· + (n − 1)
= 1
2
(n − 1)n , (4.72b)
division (by the element akk) = n . (4.72c)
The total number of arithmetic operations, N, is then obtained, by combining Eqs. (4.71)
and (4.72), as
N = 1
2
(n − 1)n +
1
3
(n − 1)n(n + 1) + 1
3
(n − 1)n(n + 1)
+
1
2
(n − 1)n +
1
2
(n − 1)n + n
= 1
6

4n3 + 9n2 − 7n

. (4.73)
For n = 2, 3 and 4 the total number of arithmetic operations are 9, 28 and 62, respectively.
4.9.2 Gauss–Jordan Elimination Method
Next, count the number of arithmetic operations involved in the Gauss–Jordan method and
compare it with that of the Gauss elimination method.
(i) Number of arithmetic operations to eliminate ak1
Dividing the first row of the augmented matrix by its pivot element involves n division
operation. Note that the element a11 need not be divided by a11 instead it can be set into
1. The multiplier is simply −ak1 and so no operation is involved in calculating it. Then,
division operation = n , (4.74a)
multiplication operation = (n − 1)n , (4.74b)
addition operation = (n − 1)n . (4.74c)
(ii) Number of operations for complete elimination
The total number of various operations involved in the elimination of all the elements below
the diagonal element in the augmented matrix are
division operation = n = 1
2
n(n + 1), (4.75a)
multiplication operation = (n − 1)n = 1
3
(n − 1)n(n + 1) , (4.75b)
addition operation = (n − 1)n = 1
3
(n − 1)n(n + 1). (4.75c)
In the Gauss–Jordan method back-substitution is avoided by additional calculations. There￾fore, next count the number of operations in the additional calculations.Iterative Methods 73
(iii) Number of operations in the additional calculations
Elimination of a12 in the first row of the augmented matrix involves
multiplication operation = (n − 1) · 1 , (4.76a)
addition operation = (n − 1) · 1 . (4.76b)
Eliminations of a13 and a23 in the first and second rows, respectively, of the augmented
matrix involve
multiplication operation = (n − 2) · 2 , (4.77a)
addition operation = (n − 2) · 2 . (4.77b)
Therefore, the total number of additional calculations are
Na = 2n
k=1
(n + 1 − k)(k − 1)
= 2 
(n + 1)k −
k2 − k

− (n + 1)1

= n(n + 1)2 − 1
6
n(n + 1)(2n + 1) − n(n + 1)
= 1
3
n

n2 − 1

. (4.78)
Combining the numbers in Eqs. (4.75) and (4.78) gives the total arithmetic operations as
N = 1
2
n(n + 1)(2n − 1). (4.79)
For n = 2, 3 and 4 the total number of arithmetic operations involved in the Gauss–Jordan
method are 9, 30 and 70, respectively. For large n,
N(Gauss) ≈ 2
3
n3 , (4.80a)
N(Gauss−Jordan) ≈ n3 . (4.80b)
The Gauss–Jordan method, for large n, has additionally n3/3 operations over the Gauss
elimination method.
4.10 Iterative Methods
Having studied some direct methods of solving linear system of equations, in this section,
the focus is on two indirect or iterative methods, namely, the Jacobi and Gauss–Seidel
methods. For hand computation, the iterative methods have the advantage that they are
self-correcting if an error is made and they can be used to reduce the round-off error in the
solutions obtained by the direct methods.
4.10.1 Jacobi Method of Iteration
For the linear system (4.2) the Jacobi iteration scheme is
X(m+1) = X(m) + D−1

B − AX(m)

, m = 0, 1, 2,... (4.81)74 Solution of Linear Systems AX = B
where X(m) is the mth approximation of X and D be the diagonal of A. When X(m+1) ≈
X(m) Eq. (4.81) gives
B = AX(m) (4.82)
and therefore X(m+1) is the solution of the given system. Since D is the diagonal of A,
Eq. (4.81) is written as
x(m+1)
i = x(m)
i +
1
aii

bi −n
j=1
aijx(m)
j

 ,
= 1
aii


bi −n
j=1
j i
aijx(m)
j

 , i = 1, 2,...,n (4.83)
where x(0)
i are the starting values of xi and they may be chosen as zero. Successive approx￾imations X(m) are assumed to converge towards the exact solution. Note that ith equation
is used to solve ith unknown. Since each of the equation in the iterative formula is simulta￾neously changed by using the most recent set of x values the Jacobi method is also called
the method of simultaneous displacements.
4.10.2 Gauss–Seidel Method of Iteration
In the Jacobi iteration formula (4.83) X(m+1) is evaluated from the known values of X(m)
,
that is, the old coordinates are used to obtain the new coordinates. In the Gauss–Siedel
method the new coordinates are used immediately as soon as they are available. The iterative
formula here is
x(m+1)
i = 1
aii

bi −
j<i
aijx(m+1)
j −
j>i
aijx(m)
j

 , i = 1, 2, . . . , n. (4.84)
4.10.3 Convergence Criteria
The convergence of the Gauss–Seidel method is more rapid than the Jacobi method be￾cause the unknown x(m+1)
i is used in the calculation of all succeeding unknowns xi+r,
r = 1, 2,...,n − i. The rate of the Gauss–Seidel method is roughly twice that of the Jacobi
method.
Now, find out the condition for convergence of the Jacobi and the Gauss–Seidel methods.
Consider the Jacobi iteration rule (4.83). Denote X¯ as the exact solution. Then, the iteration
rule with X¯ is
x¯i = 1
aii

bi −aijx¯j

. (4.85)
Subtraction of (4.83) from (4.85) gives
ξ
(m+1)
i = ¯xi − x(m+1)
i
= − 1
aii
aij ξ
(m)
j , ξ(m)
j = ¯xj − x(m)
j . (4.86)Iterative Methods 75
For convergence the requirement is





ξ
(m+1)
i





 ≤
1
|aii|
|aij |





ξ
(m)
j






≤
1
|aii|
|aij |
 




ξ
(m)
j






≤ A





ξ
(m)
j





 , (4.87)
where
A = maxi
1
|aii|
|aij |. (4.88)
If A < 1 the successive ||ξi||’s decrease. The condition A < 1 gives

j=i
|aij | < |aii|. (4.89)
A matrix with the above property is said to be diagonally dominant. The above condition
is also the condition for the convergence of the Gauss–Seidel method. Both the methods
give convergent solution if the coefficient matrix A is strictly diagonally dominant. That is,
|aii| > 
j=i
|aij |, i = 1, 2, . . . , n. (4.90)
In other words, in each row of A, the absolute value of the diagonal element must be greater
than the sum of the other elements. The above criterion can also be expressed as
2|aii| > n
j=1
|aij |, i = 1, 2, . . . , n. (4.91)
If this condition is violated then successive iterations diverge from the actual solution. Note
that if the above condition is not satisfied in the given order of appearance of the equations,
one must check whether the above condition is satisfied if the order of occurrence of the
equations is altered. An example for this is given below. The iteration can be stopped if the
Euclidean distance between X(m) and X(m+1) given by
d =

n
i=1

x(m+1)
i − x(m)
i
2
1/2
(4.92)
is less than a preassumed tolerance δ.
There are some examples of system of the form AX = B, where the matrix A does
not have diagonal dominance but still the Jacobi and Gauss–Seidel methods do converge.
For example, the Gauss–Seidel method will converge from any starting initial guess if A is
symmetric and positive definite, that is, A = AT and XT AX > 0 for all nonzero X. On the
other hand, when A has diagonal elements that are all positive and off-diagonal elements
that are all negative then both the methods will either converge or diverge.
Example 1:
Find the solution of the system
6x1 − x2 + 2x3 = 2 , (4.93a)
x1 + 5x2 + x3 = 10 , (4.93b)
2x1 + x2 + 7x3 = −3 (4.93c)76 Solution of Linear Systems AX = B
by (i) the Jacobi and (ii) the Gauss–Seidel methods.
In order to apply these two methods first verify whether the system is diagonally dominant.
A system of three equations is said to be diagonally stable if the following conditions are
satisfied:
2|a11| > |a11| + |a12| + |a13|, (4.94a)
2|a22| > |a21| + |a22| + |a23|, (4.94b)
2|a33| > |a31| + |a32| + |a33| . (4.94c)
For the given system Eqs. (4.94) are 12 > 9, 10 > 7, 14 > 10. Thus, the system is diagonally
stable. Therefore, one can solve the system by the two methods.
Solution by the Jacobi method
The Jacobi iteration rule for a system of three equations is
x(m+1)
1 = 1
a11

b1 − a12x(m)
2 − a13x(m)
3

, (4.95a)
x(m+1)
2 = 1
a22

b2 − a21x(m)
1 − a23x(m)
3

, (4.95b)
x(m+1)
3 = 1
a33

b3 − a31x(m)
1 − a32x(m)
2

. (4.95c)
For the given system the above rule becomes
x(m+1)
1 = 1
6

2 + x(m)
2 − 2x(m)
3

, (4.96a)
x(m+1)
2 = 1
5

10 − x(m)
1 − x(m)
3

, (4.96b)
x(m+1)
3 = 1
7

−3 − 2x(m)
1 − x(m)
2

. (4.96c)
Table 4.1 displays the iterated values of x1, x2 and x3. The initial values of x1, x2 and x3
are all chosen as 0. After 7th iteration the solution with three decimal points accuracy is
(x1, x2, x3) = (1, 2, −1) which is the exact solution.
Solution by the Gauss–Seidel method
The Gauss–Seidel iteration formula for a system of three equations is
x(m+1)
1 = 1
a11

b1 − a12x(m)
2 − a13x(m)
3

, (4.97a)
x(m+1)
2 = 1
a22

b2 − a21x(m+1)
1 − a23x(m)
3

, (4.97b)
x(m+1)
3 = 1
a33

b3 − a31x(m+1)
1 − a32x(m+1)
2

. (4.97c)
For the given system the algorithm is
x(m+1)
1 = 1
6

2 + x(m)
2 − 2x(m)
3

, (4.98a)
x(m+1)
2 = 1
5

10 − x(m+1)
1 − x(m)
3

, (4.98b)
x(m+1)
3 = 1
7

−3 − 2x(m+1)
1 − x(m+1)
2

. (4.98c)Iterative Methods 77
TABLE 4.1
Successive iterated values of x1, x2 and x3 of the system (4.93) by the Jacobi method. Exact
solution is (x1, x2, x3) = (1, 2, −1).
Iteration
number m x1 x2 x3
0 0.0000000 0.0000000 0.0000000
1 0.3333333 2.0000000 −0.4285714
2 0.8095238 2.0190476 −0.8095238
3 0.9396825 2.0000000 −0.9482993
4 0.9827664 2.0017234 −0.9827664
5 0.9945427 2.0000000 −0.9953223
6 0.9984408 2.0001559 −0.9984408
7 0.9995062 2.0000000 −0.9995768
8 0.9998589 2.0000141 −0.9998589
9 0.9999553 2.0000000 −0.9999617
10 0.9999872 2.0000013 −0.9999872
11 0.9999960 2.0000000 −0.9999965
TABLE 4.2
Successive iterated values of x1, x2 and x3 of the system (4.93) by the Gauss–Seidel method.
Exact solution is (x1, x2, x3) = (1, 2, −1).
Iteration
number m x1 x2 x3
0 0.0000000 0.0000000 0.0000000
1 0.3333333 1.9333334 −0.8000001
2 0.9222223 1.9755557 −0.9742858
3 0.9873545 1.9973863 −0.9960136
4 0.9982356 1.9995556 −0.9994324
5 0.9997368 1.9999392 −0.9999161
6 0.9999619 1.9999908 −0.9999878
7 0.9999945 1.9999987 −0.9999982
8 0.9999992 1.9999998 −0.9999997
Table 4.2 gives the iterated values of x1, x2 and x3 with the initial guess x(0)
1 , x(0)
2
and x(0)
3 all are 0. After 5 iterations the solution with three decimal point accuracy is
(x1, x2, x3) = (1, 2, −1) which is the exact solution. In the Jacobi method after fifth iteration
the solution is (0.995, 2.000, −0.995). This clearly shows that convergence in the Gauss–
Seidel algorithm is much faster than in the Jacobi method.78 Solution of Linear Systems AX = B
TABLE 4.3
First few iterated values of x1, x2 and x3 of the system (4.99) by the Jacobi method.
Iteration
number m x1 x2 x3
0 0.00000 0.00000 0.00000
1 −1.50000 −2.00000 10.00000
2 −35.50000 9.00000 21.50000
3 −81.25000 −172.00000 0.50000
4 82.75000 −488.50000 951.25000
Example 2:
Verify that the iterated values of the system
2x1 + x2 + 7x3 = −3 , (4.99a)
6x1 − x2 + 2x3 = 2 , (4.99b)
x1 + 5x2 + x3 = 10 (4.99c)
obtained by the Jacobi and the Gauss–Seidel methods diverge.
The above system is same as the system (4.93) but the order of occurrence of the equations
is changed. The first few iterated values of x1, x2 and x3 by the Jacobi method are given
in table 4.3.
The divergence is due to the fact that the system is not diagonally dominant (verify).
(The Gauss–Seidel algorithm also gives diverging values for x1, x2 and x3. This is left as
an exercise to the reader.) However, if one reorders the system (4.99) as in (4.93) then it
is diagonally dominant. Then, both the methods give convergent solution as seen in the
Example 1.
4.11 System AX = B with A being Vandermonde Matrix
A system of equations AX = B with upper- and lower-triangular forms of A discussed earlier
in Section 4.3 are two special cases of a linear system of equations. Another special system is
the one in which A is the so-called Vandermonde matrix , named after Alexandre-Th´eophile
Vandermonde.
A Vandermonde matrix is an N × N matrix whose elements are expressed in a simple
manner in terms of N numbers say a1, a2,...,aN . The N2 components of a Vandermonde
matrix A are simply aj−1
i , i, j = 1, 2,...,N. The elements of a Vandermonde matrix follow
a geometric progression. The system takes the form


1 a1 a2
1 ··· aN−1
1
1 a2 a2
2 ··· aN−1
2
.
.
. .
.
. .
.
. .
.
. .
.
.
1 aN a2
N ··· aN−1
N




x1
x2
.
.
.
xN

 =


b1
b2
.
.
.
bN

 . (4.100)System AX = B with A being Vandermonde Matrix 79
Such types of equations arise in the problem of constructing a polynomial interpolation
function. Vandermonde matrices are realized in the discrete Fourier transform analysis of
digital signal processing, matrix models of quantum field theories, representation theory of
the symmetric groups, theory of BCH code and time series analysis.
The system (4.100) can be solved as follows [4]. Let Pj be the Lagrange polynomial of
order N − 1 given by
Pj (a) = 
N
n=1
n j
a − an
aj − an
= 
N
k=1
Ajkak−1 . (4.101)
From Eq. (4.101), it is easy to find that P = 0 at all a = ai with i = j and is 1 at a = aj .
That is,
Pj (ai) = 
N
k=1
Ajkak−1
i . (4.102)
Then, xj ’s are given by
xj = 
N
k=1
Akj bk . (4.103)
To get Ajk define a master polynomial
Pa = 
N
n=1
(a − an) . (4.104)
Then, proceed to calculate its coefficients and obtain the numerators and denominators
of the specific Pj ’s via synthetic division by the one supernumerary term. Note that the
total procedure is of the order N2. The system (4.100) can be solved by the Gauss–Jordan
method also which requires operations of the order of N3. The method described above is
specially developed for the system (4.100).
Example:
Solve the system
x1 + x2 + x3 = 2 , (4.105a)
x1 − x2 + x3 = 4 , (4.105b)
x1 + 3x2 + 9x3 = 16 . (4.105c)
In the form of Eq. (4.100) the above system is written as


1 11
1 −1 1
1 39




x1
x2
x3

 =


2
4
16

 . (4.106)
The coefficients a’s are a1 = 1, a2 = −1, a3 = 3. Develop a Python program that reads
the number of equations N, the coefficients a1, a2,...,aN and b1, b2,...,bN and solve the
system AX = B with A being Vandermonde matrix employing the method described in the
present section. The solution obtained is (x1, x2, x3) = (1, −1, 2).80 Solution of Linear Systems AX = B
4.12 Ill-Conditioned Systems
Consider the system
0.1x + y = 2.1, (4.107a)
x + 9y = 19. (4.107b)
Equation (4.107) can be expressed in matrix form as AX = B with
A =
 0.1 1
1 9 
, B =
 2.1
19 
, X =
 x
y

. (4.108)
The solution of Eqs. (4.107) is (x, y) = (1, 2). Now, slightly change the coefficient of x
in Eq. (4.107a) as 0.11. The solution becomes (x, y) = (10, 1). When the coefficient 0.1
is replaced by 0.111 the result is (x, y) = (100, −9). Replacement of the coefficient of y
in Eq. (4.107a) by 1.1 gives (x, y) = (10, 1). A similar result can be noticed when the
coefficient of x or y is altered in Eq. (4.107b). A small change in the coefficient of x or y
in Eqs. (4.107) produces a large change in the solution. Such systems of linear equations
are called ill-conditioned systems [6-8]. In general, in an ill-conditioned system of the form
AX = B a small change in the elements of A (as well as in B) leads to a large change in the
solution. The corresponding matrix A is termed as an ill-conditioned matrix. Note that in
certain problems of the form AX = B one may wish to find the solution for fixed A (B) for
slightly different set of B (A). In such a case ill-conditioning has to be taken into account.
Ill-conditioned system of equations or ill-conditioned matrices occur in the analysis of
multiple scattering problems, inversion problem of the heat capacity of crystals and the
determination of the momentum distribution of cosmic-ray muons [9], a few to mention.
4.12.1 Condition of a System of Linear Equations
For ill-conditioning, one has to find what happens to X+∆X when A is changed into A+∆A
(or B into B + ∆B) with infinitesimally small ∆A [7]. Replacement of A by A + ∆A and
X by X + ∆X in AX = B and neglecting ∆A∆X gives
A∆X + (∆A)X = 0. (4.109)
Thus,
||∆X|| ≤ ||A−1|| ||∆A|| ||X|| (4.110)
with the norms of ∆X and A are given by
||∆X|| =

n
i=1
|∆xi|
2
1/2
, ||A|| =


n
i=1
n
j=1
|aij |
2


1/2
, (4.111)
where xi’s are the components of X and aij ’s are the elements of the n × n matrix A.
Equation (4.111) can be rewritten as
||∆X||
||X|| ≤ ||A|| ||A−1||||∆A||
||A|| (4.112)
or
||∆X||
||X|| ≤ κ(A)
||∆A||
||A|| , κ(A) = ||A|| ||A−1||. (4.113)Ill-Conditioned Systems 81
Amplification of small perturbation of A is determined by κ(A) called the condition number
[7]. A perturbation is not amplified if κ(A) is small and the system is a well-conditioned
one. For an ill-conditioned system κ(A) will be large. The value of κ(A) lies in the interval
[0, ∞]. If the condition number is < 100 then the system can be treated as a well-conditioned
system.
Another parameter one can consider is γ(A) given by
γ(A) = |A|
r1r2 ··· rn
, (4.114a)
where |A| is the determinant of A and
ri =

n
i=1
|aij |
2
1/2
, i = 1, 2, . . . , n. (4.114b)
γ(A) lies between −1 and 1. If γ(A) is close to 0 then A is an ill-conditioned, otherwise,
well-conditioned. γ(A) describes how small the determinant of A.
Can one use |A| ≈ 0 as a requirement for ill-conditioned behaviour of the system AX =
B? Consider a system with |A| ≈ 0. For any system of linear equations, multiplication
of any equation by a number will not affect the solution but affect the value of |A|. For
example, for Eqs. (4.107) the |A| is −0.1, close to zero. Suppose, multiply Eqs. (4.107) by
105. Then, |A| = −0.1 × 1010 which is not close to zero. The point is that |A| can be freely
changed to any order without altering the solution. Thus, |A| ≈ 0 is unsuitable to identify
whether a system is ill-conditioned or well-conditioned [8]. What is the condition to be used
for identifying ill-conditioned system if A is fixed and B is varied?
In the following subsection, let us consider a few systems, compute κ(A) and γ(A),
identify the ill- or well-conditioned behaviour and verify the prediction.
4.12.2 Examples of Ill- and Well-Conditioned Systems
As a first example, consider the system (4.107) with A and B given by Eq. (4.108) and
(x, y) = (1, 2). For this A
|A| = −0.1, A−1 = 10  −9 1
1 −0.1

, ||A|| = √
83.01 , (4.115a)
||A−1|| = 10√
83.01 , r1 = √
1.01 , r2 = √
82 (4.115b)
and
κ(A) = 830.01, γ(A) = −0.01099. (4.116)
As κ(A) is very large the system is ill-conditioned. Further, γ(A) is close to zero. Change
the coefficient of x in Eq. (4.107a) to 0.11. The solution becomes (x, y) = (10, 1) which is a
large change. The system is thus verified as an ill-conditioned system.
The second system is
0.3x + y = 2.3, (4.117a)
x + 3y = 7 (4.117b)
with (x, y) = (1, 2). With  0.3 1
1 3 
one has
|A| = −0.1, A−1 = 10  −3 1
1 −0.3

, ||A|| = √
11.09 , (4.118a)
||A−1|| = 10√
11.09 , r1 = √
1.09 , r2 = √
10 (4.118b)82 Solution of Linear Systems AX = B
and
κ(A) = 110.9, γ(A) = −0.03029. (4.119)
As κ(A) is large and γ(A) is ≈ 0 the system (4.117) is an ill-conditioned system. Changing
0.3 in Eq. (4.117a) by 0.31 and 0.33 give the solutions as (x, y) = (1.42857, 1.85714) and
(x, y) = (10, −1), respectively. The changes in the solutions are large.
For the third system
x + 2y = 5, (4.120a)
2.2x + 4.5y = 11.2 (4.120b)
with (x, y) = (1, 2) and A =
 1 2
2.2 4.5

the values of κ(A) and γ(A) are obtained as
300.9 and 0.00893, respectively, implying that the given system is an ill-conditioned one.
Change of 2.2 in Eq. (4.120b) to 2.3 leads to the solution as (x, y)=(−1, 3) confirming the
ill-conditioned behaviour of the system.
Next, consider two systems that are well-conditioned. The first example system is
3x + 5y = 13, (4.121a)
−x + y = 1, (4.121b)
where (x, y) = (1, 2). For this system
|A| = 8, A−1 = 1
8
 1 −5
1 3 
, κ(A)=4.25, γ(A)=0.97014. (4.122)
As κ(A) is small and γ(A) is not close to 0, the above system is a well-conditioned system.
Changing the coefficient of x in Eq. (4.121a) to (a) 3.03 and (b) 3.3 leads to the solutions
(a) (x, y) = (0.99626, 1.99626) and (b) (x, y) = (0.96386, 1.96386), respectively. The changes
are very small.
The second well-conditioned system with the solution (x, y) = (1, 2) is
0.1x + 0.2y = 0.5, (4.123a)
0.3x + 0.4y = 1.1. (4.123b)
The determinant of the coefficient matrix A of this system is −0.02 and is ≈ 0. A−1, κ(A)
and γ(A) are obtained as
A−1 =
 −20 10
15 −5

, κ(A) = 15, γ(A) = −0.17889. (4.124)
Since κ(A) is small and γ(A) is not close to 0 the system (4.123) is a well-conditioned system
even though |A| ≈ 0. The change of the coefficient of x in Eq. (4.123a) to 0.101 and 0.11
change the solution to (x, y) = (1.02041, 1.98469) and (x, y) = (1.25, 1.8125), respectively.
The changes in the solution are not large.
4.13 Homogeneous Systems with Equal Number of Equations and
Unknowns
For systems of the form (4.2) with B = 0, detA = 0 the unique solution is the trivial
solution X = 0. Nontrivial solution exists when det A = 0. To balance a chemical reactionHomogeneous Systems with Equal Number of Equations and Unknowns 83
equation one can set up homogeneous system of equations for the number of atoms of the
elements in the chemical reactions. Finding the equilibrium points of autonomous coupled
linear differential equations essentially involves solving a system of homogeneous equations.
Example 1:
Consider the unbalanced chemical reaction Na2O+H2O → NaOH. To balance this reac￾tion introduce the coefficients and determine them. For this purpose introduce the vector
notation in the matrix form


Na
O
H

. Now, write the given reaction in equation form as
x1


2
1
0




Na
O
H

 + x2


0
1
2




Na
O
H

 = x3


1
1
1




Na
O
H

 . (4.125)
This gives the equations for the coefficients x1, x2 and x3 as
2x1 − x3 = 0, x1 + x2 − x3 = 0, 2x2 − x3 = 0. (4.126)
The determinant of the coefficient matrix is 0. Elimination of x1 in the last two subequations
gives
2x1 − x3 = 0, 2x2 − x3 = 0, 2x2 − x3 = 0. (4.127)
The last two subequations are identical and hence there are only two equations for the three
variables which are
2x1 − x3 = 0, 2x2 − x3 = 0. (4.128)
Thus, x2 = x1 and x3 = 2x2. The choice x1 = 1 gives x2 = 1 and x3 = 2. Then, from
Eq. (4.125) the balanced reaction equation is Na2O+H2O → 2NaOH.
Example 2:
Consider the system
x1 + x2 + x3 = 0, (4.129a)
5x1 − x2 + x3 = 0, (4.129b)
7x1 − 2x2 + x3 = 0. (4.129c)
The determinant of its coefficient matrix A is zero. It can have a nontrivial solution. The
system after eliminating x1 in the last two equations by Gauss elimination is
x1 + x2 + x3 = 0, (4.130a)
−6x2 − 4x3 = 0, (4.130b)
−9x2 − 6x3 = 0. (4.130c)
The third equation in (4.130) is a multiple (3/2 times) of the second equation. Therefore,
essentially the system (4.130) has only two equations, say, (4.130a) and (4.130c). Elimination
of x3 in the first equation using the last equation gives the system
2x1 − x2 = 0, (4.131a)
3x2 + 2x3 = 0. (4.131b)84 Solution of Linear Systems AX = B
Now, there are two equations for three unknowns. Therefore, one variable is arbitrary. Let
it be x3 and x3 = α. Then, Eq. (4.131) gives x1 = −α/3, x2 = −2α/3. Thus, the solution is
X =


x1
x2
x3

 = α


−1/3
−2/3
1

 . (4.132)
4.14 Least-Squares Problem
Let the number of variables say m is less than the number of equations n. The solution of
such systems can be obtained as follows.
From Eq. (4.2) one can write X = A−1B. For the present case, A is not a square
matrix. However, one can define a generalized inverse, which can be used to solve the set
of equations. One can note that AT A is a square matrix with m rows and columns. Now,
multiplication of Eq. (4.2) by AT gives
AT AX = AT B . (4.133)
Since, AT A is a square matrix, from Eq. (4.133) one can obtain
X = 
AT A
−1
AT B . (4.134)
Let us solve the system
x1 + x2 = 3, 2x1 + x2 = 4, x1 − x2 = −1 (4.135)
by the above procedure. The matrix A for the given system is
A =


1 1
2 1
1 −1

 . (4.136)
Then,
X =


 12 1
1 1 −1
 

1 1
2 1
1 −1




−1
 12 1
1 1 −1
 

3
4
−1


=
 6 2
2 3 −1  10
8

= 1
14  3 −2
−2 6   10
8

=
 1
2

. (4.137)
4.15 Singular-Value Problems
Is it possible to find a nontrivial solution for a homogeneous/inhomogeneous linear systemSingular-Value Problems 85
with fewer equations than unknowns? The answer is yes. In this section, a method of solving
this type of systems is outlined. Note that the method described in the previous section
for more equations than variables does not work for more variables with a lesser number of
equations. This is because for the latter case the size of the square matrix AT A is larger
than the number of equations and AT A is always singular.
4.15.1 Homogeneous Systems
First, consider the case of two unknowns but one equation. Let it be
a11x1 + a12x2 = 0 . (4.138)
The nontrivial solutions are
1. x1 = 0, x2 = arbitrary if a12 = 0.
2. x1 = a12, x2 = −a11, if a11, a12 = 0.
Next, consider the case of n equations and m unknowns, where n<m. Then, the matrix
A in AX = 0 is n × m matrix. Choose a nonzero element in the mth column of A and let
it be say aim. Now, construct a n × (m − 1) matrix B with its jth column given by
bj = aj − aij
aim
am , j = 1, 2,...,m − 1. (4.139)
For each j, the ith element of bj is
aij − aij
aim
aim = aij − aij = 0 . (4.140)
That is, ith equation of BX = 0 is
0.x1 + 0.x2 + ··· + 0.xn−1 = 0 . (4.141)
Ignoring the ith equation one can obtain the system
BX = 0 (4.142)
which is a linear homogeneous system with n − 1 equations in m − 1 unknowns. Since
Eq. (4.142) has fewer equations than unknowns it has nontrivial solution. Consequently,
BX = 0 also has nontrivial solution and so the equivalent system AX = 0 has a nontrivial
solution.
Example 1:
Consider the system
x1 + x2 + x3 = 0 , (4.143a)
x1 − x2 + 2x3 = 0 . (4.143b)
For the system (4.143) n = 2 and m = 3. Since a13 = 1 = 0 one may choose i = 1 and
obtain
b1 = a1 − a11
a13
a3 =
 1
1

− 1
1
 1
2

=
 0
−1

, (4.144a)
b2 = a2 − a12
a13
a3 =
 0
−3

. (4.144b)86 Solution of Linear Systems AX = B
Then, the system BX = 0 is
0.x1 + 0.x2 = 0, (4.145a)
−x1 − 3x2 = 0 . (4.145b)
Ignoring the first equation of the system (4.145), one has BX = 0 as
x1 + 3x2 = 0 . (4.146)
For simplicity choose x2 = 1 which gives x1 = −3. Then, from Eq. (4.143) x3 is found to
be 2.
Example 2:
Balance the chemical reaction equation C5H8 + O2 → CO2 + H2O.
To set up a system of equations for the given chemical reaction write it as
x1


5
8
0




C
H
O

 + x2


0
0
2




C
H
O


= x3


1
0
2




C
H
O

 + x4


0
2
1




C
H
O

 . (4.147)
From this equation write
5x1 − x3 = 0, 4x1 − x4 = 0, 2x2 − 2x3 − x4 = 0 . (4.148)
Suppose solve the above system by the Gauss–Jordan method. This gives
5x1 − x3 = 0, 2x2 − 2x3 − x4 = 0, 4x3 − 5x4 = 0 . (4.149)
There are three equations but four unknows. Therefore, choose x1 = 1 which gives x2 = 7,
x3 = 5 and x4 = 4. Then, the balanced chemical reaction is C5H8 + 7O2 → 5CO2 + 4H2O.
4.15.2 Inhomogeneous Systems
Next, consider the system AX = B with B = 0 [5]. For a system with n equations involving
m variables and when m>n, one can eliminate (n−1) variables to obtain a single equation
in (m − n + 1) variables. Such an equation represents a hyper-surface in an (m − n + 1)-
dimensional space, for example, a line on a plane or a plane in a three-dimensional space.
The singular-value solution is defined as that point on the hyper-surface which lies closest
to the origin. Once the solution to the single equation is obtained then the values of the
remaining variables can be found by back-substitution.
Let the single equation is with two variables. This equation can be of the form, say,
x1
a
+
x2
b = 1 . (4.150a)
The point which is closest to the origin will lie on the line
ax1 − bx2 = 0 . (4.150b)Singular-Value Problems 87
Thus, the singular-value problem is now reduced to the solution of the system (4.150). In
matrix form, the system (4.150) is written as
 1/a 1/b
a −b
  x1
x2

=
 1
0

. (4.151)
The determinant D of the square matrix in Eq. (4.151) is
D = −

a2 + b2
ab = 0 . (4.152)
Thus, the system (4.151) will have a nontrivial solution.
When the reduced single equation with three variables is, say, of the form
x1
a
+
x2
b +
x3
c
= 1 (4.153)
which is the equation of a plane, then the point on this plane which is closest to the origin
lies on the planes
ax1 − cx3 = 0 , bx2 − cx3 = 0 . (4.154)
That is, one has the system


1/a 1/b 1/c
a 0 −c
0 b −c




x1
x2
x3

 =


1
0
0

 . (4.155)
The determinant of the square matrix in the system (4.155) is
D =

a2b2 + b2c2 + c2a2
abc = 0. (4.156)
Thus, the system (4.155) can have a nontrivial solution.
Example:
Let us find the solution of the system
x1 + x2 + x3 = 4, (4.157a)
x1 + 2x2 − x3 = 1. (4.157b)
The variable x3 in Eq. (4.157a) can be eliminated using (4.157b) giving the single equation
2x1 + 3x2 = 5 (4.158)
which can be rewritten in the form of Eq. (4.150a) as
x1
5/2 +
x2
5/3
= 1. (4.159)
The equation corresponding to (4.150b) is (5/2)x1 − (5/3)x2 = 0. The result is the system
 2/5 3/5
5/2 −5/3
  x1
x2

=
 1
0

. (4.160)88 Solution of Linear Systems AX = B
The solution of the above system is
 x1
x2

=
 2/5 3/5
5/2 −5/3
−1  1
0

=
 10/13
15/13 
. (4.161)
Use of these values in Eq. (4.157a) gives x3 = 27/13. The singular-value solution of the
system (4.157) is
(x1, x2, x3) = (10/13, 15/13, 27/13). (4.162)
4.16 Concluding Remarks
In this chapter, methods of solving certain kinds of system of linear equations are discussed.
Depending upon the nature of the problem one can choose an appropriate method. In
scientific problems, while solving a system of linear equations it is important to check
whether the concerned system is an ill-conditioned or well-conditioned system.
Sometimes it is required to find the solutions of several systems of equations AX =
Bq, q = 1, 2,...,p with the same matrix A. In this case introduce the notation Bq =
(a1,m+q,...,am,m+q)T . That is, now Bq is a m × q matrix with its qth column elements are
Bq. The augmented matrix is of the order m×(m+1+q). Application of the Gauss–Jordan
method to this augmented matrix finally gives a matrix with aii = 1 for i = 1, 2,...,m and
aij = 0 for i = 1, 2,...,m, j = i, ≤ m. The (m + 1)th column elements are the solution of
the system with B1, (m + 2)th column elements are the solution of the system with B2 and
so on. Develop a program to solve two systems of equations with same A by Gauss–Jordan
method. Verify the program by solving the systems
2x + 3y − z = 4 , −x + y − z = −1 , x + y + z = 3 , (4.163)
2x + 3y − z = 3 , −x + y − z = 0 , x + y + z = 0 . (4.164)
The solution of the first three equations is (x, y, z) = (1, 1, 1) while the solution of the
second set of three equations is (x, y, z) = (1, 0, −1).
4.17 Bibliography
[1] R. Odisho, Applications of systems of Linear equations to electrical networks.
[2] C.F. van Loan and G.H. Golub, Matrix Computations. The John Hopkins, Uni￾versity Press, 1996.
[3] Y. Saad, Iterative Methods for Sparse Linear Systems. SIAM, Philadelphia, 2003.
[4] W.H. Press, S.A. Teukolsky, W.T. Vetterling, B.P. Flannery, Numerical Recipes
in Fortran. Foundation Books, New Delhi, 1993. Indian edition.
[5] P.H. Borcherds, Eur. J. Phys. 16:201, 1995.
[6] J.H. Mathews, Numerical Methods for Mathematics, Science and Engineering.
Prentice Hall of India, New Delhi, 1998.
https://home.csulb.edu/∼jchang9/m247/m247−poster−R−Odisho−sp09.pdf.Problems 89
[7] L.N. Trefethen and D. Bau III, Numerical Linear Algebra. Society for Industrial
and Applied Mathematics, Pennsylvania, 2002.
[8] A. Gezerlis and M. Williams, Am. J. Phys. 89:51, 2021.
[9] J.J. Torsti and A.M.Auerela, Comput. Phys. Commun. 4(1):27, 1972.
4.18 Problems
A. Cramer’s Rule
4.1 Solve the following systems of equations by the Cramer’s rule.
a) x1 + x2 = −1,
2x1 + 5x2 = −8.
b) 3x1 + 5x2 = 21,
2x1 + 7x2 = 25.
c) 2x1 + 8x2 = −1,
3x1 − 2x2 = 2.
B. Upper- and Lower-Triangular Systems
4.2 Show that the equation
x1 + 2x2 + x3 = 5,
0.x2 + 3x3 = 6,
2x3 = 4
has infinitely many solutions.
4.3 Find the solution of the upper-triangular system
x1 + x2 − x3 = 6,
3x2 + 2x3 = 7,
x3 = −1.
4.4 Find the solution of the lower-triangular system
3x1 = 6,
2x1 + 3x2 = 7,
x1 + x2 + 3x3 = −6.
C. Gauss Elimination Method
4.5 For the equation
x + By = C, Dx + Ey = F,
where  is a sufficiently small number show that without pivoting x ≈ 0 is a
solution for any values of C and F. Obtain the solution by the Gauss elimination
for F = D + E and C = B + .90 Solution of Linear Systems AX = B
4.6 Obtain the solutions of the following systems.
a) x1 + 2x2 + x3 = 2,
2x1 − x2 − x3 = 1,
x1 − x2 − x3 = 0.
b) 2x1 + x2 − x3 = 3,
4x1 − x2 + 2x3 = −1,
x1 + x2 − x3 = 2.
c) 3x1 − x2 + 2x3 = 11,
2x1 − 2x2 + 3x3 = 10,
x1 + x2 − x3 = 1.
d) 3x1 + x3 = 3,
2x1 + 3x2 − x3 = 2,
−x1 + 2x2 + x3 = −1.
4.7 Observe what happens when you try to solve the following systems by the Gauss
elimination method. State the reason.
a) x1 + 2x2 + 4x3 = 10,
2x1 − 3x2 − x3 = 5,
1.5x1 + 3x2 + 6x3 = 15.
b) 2x1 + x2 + x3 = 5,
3x1 + 2x2 − 3x3 = 2,
2x1 + x2 + x3 = 7.
D. Gauss–Jordan Method
4.8 Solve the equations given in Problem 4.6 by the Gauss–Jordan method.
4.9 Find the inverse of the following matrices by the Gauss–Jordan method.
a)


12 1
03 1
2 5 −3

. b)


0 11
1 −2 3
5 21

.
4.10 Observe what happens when you apply the Gauss–Jordan method to find the
inverse of the following matrices.
a)


111
213
111

. b)


213
426
121

.
4.11 The inverse of A of a system of the form AX = B is given by
A−1 =


1 02
−1 13
2 −2 5

.
Find the solution X if
B =


2
−1
1

.Problems 91
E. Triangular Factorization Method
4.12 Solve the systems in Problem 4.6 by the triangular factorization.
4.13 Calculate the determinant of the following matrices by writing them in LU form.
a)


3 −1 2
2 12
1 11

. b)


1 02
−1 12
1 −2 0

. c)


12 1
03 1
2 5 −3

.
F. Counting Arithmetic Operations in Direct Methods
4.14 Show that the number of arithmetic operations involved to find the inverse of a
given matrix of order n×n by the Gauss–Jordan method is 
16n3 − 9n2 − n

/6.
4.15 Show that the number of arithmetic operations involved in the triangular factor￾ization is same as the number involved in the Gauss elimination method.
G. Iterative Methods
4.16 Obtain the solutions of the following systems by applying (i) the Jacobi method
and (ii) the Gauss–Seidel method. When the calculation is done using computer
use the initial guess as zero for all unknowns in these systems and stop the itera￾tion if the distance given by (4.92) is 10−5. For hand calculation find the solution
with two-decimal point accuracy after 2 iterations starting with the specified
guess.
a) 5x1 + x2 = 3,
x1 − 7x2 = 15.

x(0)
1 , x(0)
2

= (0, 0).
b) 5x1 + x2 − x3 = 10,
x1 − 4x2 + x3 = −10,
x1 + x2 − 4x3 = 15.

x(0)
1 , x(0)
2 , x(0)
3

= (2, 2.5, −3.75).
c) 6x1 − x2 − x3 = −4,
x1 + 6x2 − x3 = 2,
x1 + x2 + 4x3 = 2.

x(0)
1 , x(0)
2 , x(0)
3

= (−0.6, 0.6, 0.6).
d) 3x1 + x2 + x3 = 0.4,
x1 − 4x2 − x3 = 1.0,
x1 + x2 − 5x3 = 0.

x(0)
1 , x(0)
2 , x(0)
3

= (0, 0, 0).
e) 6x1 − x2 − x3 − x4 = 3,
2x1 + 5x2 + x3 − x4 = 7,
x1 − 2x2 + 4x3 + x4 = 4,

x1 + x2 + x3 + 3x4 = 6.
x(0)
1 , x(0)
2 , x(0)
3 , x(0)
4

= (0.9, 0.9, 0.9, 0.9).
H. Systems AX = B with A being Vandermonde Matrix
4.17 Solve the following systems of equations, where the matrix A is Vandermonde.
a) x1 + x2 + x3 = 3,
x1 + 4x2 + 16x3 = 36,
x1 − 2x2 + 4x3 = 6.92 Solution of Linear Systems AX = B
b) x1 + 2x2 + 4x3 = 0,
x1 + x2 + x3 = 2,
x1 − x2 + 4x3 = −3.
c) x1 + 3x2 + 9x3 = 12,
x1 + 5x2 + 25x3 = 16,
x1 − 2x2 + 4x3 = −5.
I. Homogeneous Systems with Equal Number of Equations and
Unknowns
4.18 Obtain the nontrivial solution of the following systems.
a) x1 + x2 + 2x3 = 0,
2x1 + 3x2 + 5x3 = 0,
−2x1 + x2 − x3 = 0.
b) x1 + x2 + 2x3 = 0,
3x1 + x2 + 5x3 = 0,
−x1 + x2 − x3 = 0.
c) 2x1 + 3x2 + 7x3 = 0,
3x1 + x2 + 7x3 = 0,
5x1 − 2x2 + 8x3 = 0.
J. Inhomogeneous Systems with More Equations than unknowns
4.19 Find the solution of the following systems.
a) 2x1 + x2 = 0,
x1 + x2 = 1,
3x1 + 2x2 = 1.
b) x1 − x2 = −4,
2x1 + x2 = −2,
3x1 − 2x2 = −10.
c) 3x1 + 2x2 = 9,
4x1 − x2 = 1,
5x1 + 3x2 = 14.
K. Singular-Value Problems
4.20 Determine the nontrivial solution of the following equations.
a) x1 + 2x2 − x3 = 0,
−2x1 − x2 + x3 = 0.
b) x1 + x2 + x3 = 0,
3x1 + x2 + 2x3 = 0.
c) 2x1 − x2 + x3 = 0,
−5x1 − 11x2 + 2x3 = 0.5
Curve Fitting
5.1 Introduction
In many scientific and engineering experiments number of values of two variables are often
measured. For example, in an experiment, by varying the volume (V ) of a cylinder one may
measure the pressure (P) of steam in the cylinder. Assume that the law or rule connecting
the two variables P and V is not known. To get a rough idea about how P varies with V a
graph between the measured P and V may be drawn and then a conclusion on the depen￾dence of P on V can be made. Sometimes, one may also wish to know the values of P for
the values of V that are not considered in the experiment. How does one determine the cor￾responding values of P without performing the experiment? This is possible by constructing
an appropriate mathematical relation between P and V from the given experimental data.
Alternatively, from the given set of values of P and V an approximate value of the unknown
P for a given value of V not in the set can be estimated by an extrapolation or interpolation
which will be discussed later in Chapter 6.
The general problem of construction of an appropriate mathematical equation which
fit a set of given data is called curve fitting. Let us assume that a set of data (xk, yk),
k = 1, 2, ..., n representing the values of two variables is given. The goal of the curve fitting
is to determine a formula
y = f(x). (5.1)
How does one choose an appropriate form of f(x)? Usually, by inspecting the given data
and the physical situation a particular form of f(x) is chosen. For example, in Fig. 5.1a
y appears to be varying linearly with x. Therefore, the linear relation y = ax + b can be
tried. For the data in Fig. 5.1b an exponential function y = beax is suitable. Methods are
available to determine the constants a and b. The method of curve fitting can be extended
to functions of several variables also. In this chapter, the least-squares curve fitting to linear
and certain nonlinear functions of one variable which are often encountered in physical and
engineering problems is studied. Many examples of physical quantities exhibiting linear and
nonlinear relations are given in the problems section at the end of this chapter.
5.2 Method of Least-Squares
Let (xk, yk), k = 1, 2, ..., n be n sets of observations and it is required to make a fit to the
function
y = F (x, c1, c2, ..., cm), (5.2)
where F is some function of x which depends linearly on the parameters {ci}. That is,
F = c1φ1(x) + c2φ2(x) + ··· + cmφm(x), (5.3)
DOI: 10.1201/9781032649931-5 9394 Curve Fitting




(a) (b)
FIGURE 5.1
Data points y varying roughly (a) linearly and (b) nonlinearly with x.
where {φi(x)} are a priory selected set of functions and {ci} are to be determined. Normally,
m is small compared with the number, n, of the data points. The functions {φi(x)} may be
simply x or xk or cos(kπx) and so on.
5.2.1 Basic Idea
The basic idea of curve fitting is to choose the parameters c = {ci} in such a way that the
deviation errors or the residuals ek given by
ek = yk − F (xk, c), k = 1, 2, ..., n (5.4)
are minimum. In other words, the difference between the observed or given y and the value
of y calculated from the relation (5.2) to be determined is made as small as possible. Here
ek’s can take both positive and negative values. In order to give equal weightage to positive
and negative ek’s one can consider the following two kinds of errors:
1. Average Error:
Eav = 1
n
n
k=1
|ek|. (5.5)
2. Root-Mean-Square (rms) Error:
Erms =

1
n
n
k=1
e2
k
1/2
. (5.6)
The best fit is then obtained by minimizing one of these two quantities. The choice Eav
will lead to a system of linear equations for the unknowns {ci} but with constraints and an
analytical method of solving them not exists. For more details see Section 5.2.3.
Consideration of Erms will lead to a system of linear equations for {ci} without any
constraints as shown in Sections 5.2.2 and 5.3. Because of solving a set of coupled linear
equations is simpler than the equations obtained from the minimization of Eav, the case of
Erms is generally preferred. Further, the system of linear equations obtained using Erms can
be solved either directly or iteratively as discussed in Chapter 4.Method of Least-Squares 95
From Eq. (5.6) it is clear that the Erms will be minimum if and only if the quantity
E(c) = n
k=1
e2
k (5.7)
is a minimum. Hence, the best curve fit to a set of data is that for which the sum of the
squares of the residuals is a minimum. This is known as least-squares criterion and the
resulting approximation F(x, c) is called a least-squares approximation to the given data.
5.2.2 Derivation of Equations for the Unknowns {ci}
It is desired to find {ci} for which E(c) given by Eq. (5.7) is a minimum. Recall that the
slope df(x)/dx = f
(x) of a single variable function f(x) is zero at the minimum value of f.
A function f(X) of several variables is minimum when ∇f(X) = 0, where ∇ is the gradient
operator. Thus, the necessary condition for the function E to be minimum is
∇E(c)=0, (5.8)
where the gradient operator ∇ here takes the form
∇ = i1
∂
∂c1
+ i2
∂
∂c2
+ ··· + im
∂
∂cm
(5.9)
with ij ’s being unit vectors. Equation (5.8) is satisfied if and only if all the components of
∇E are identically zero: ∂E/∂ci = 0. That is,
∂E
∂ci
= ∂
∂ci
n
k=1
[yk − F (xk, c)]2 = 0, i = 1, 2, ..., m. (5.10)
From Eq. (5.3) one has ∂F/∂ci = φi. Therefore, Eqs. (5.10) become
−2
n
k=1
[yk − F (xk, c)] φi (xk)=0. (5.11)
Then, using Eq. (5.4) in (5.11) one gets
n
k=1
ekφi (xk)=0, i = 1, 2, ..., m. (5.12)
That is,
e · Φi = 0, i = 1, 2, ..., m (5.13)
where
e = [e1, e2, ..., en]
T , (5.14a)
Φi = [φi(x1), φi(x2), ..., φi(xm)] . (5.14b)
Equation (5.13) implies that the error vector e, for all n, should be normal or orthogonal
to each of the n vectors Φi. Because of this, the m equations, namely, Eqs. (5.11) are called
normal equations.96 Curve Fitting
5.2.3 Least Absolute Residuals
In the case of Eav the quantity E(c) = n
k=1 |ek|. For the best fit, it is desired to minimize
E(c) = n
k=1
|yk − c1φ1(xk) − c2φ2(xk) −···− cmφm(xk)|. (5.15)
For simplicity consider c3 = c4 = ··· = cm = 0, φ1 = x and φ2 = 1. Then,
E(c) = n
k=1
|yk − c1xk − c2| = n
k=1
zk. (5.16)
Minimization of Eav is minimizing zk with respect to c1, c2 and z1, z2, ··· , zn subjected to
zk ≥ yk − c1xk − c2 for k = 1, 2, ··· , n (5.17a)
zk ≥ −(yk − c1xk − c2) for k = 1, 2, ··· , n. (5.17b)
The result will be constraint equations. After minimization, each zk must be equal to |yk −
c1xk − c2|. This kind of problem can be solved by a linear programming package. For
the least absolute deviations approach analytically solving the problem is not available
in the mathematics literature. Further, another advantage of using Erms over Eav is that
in the case of former ek is squared, that is, the deviation errors are enlarged. For more
details, one may refer to the refs. [1,2].
5.3 Least-Squares Straight-Line Fit
In this section how to make a linear or straight-line fit to a given set of n data is described.
Essentially, the problem is to determine the values of the constants a and b in the function
y = f(x, a, b) = ax + b. (5.18)
Simple formulas for a and b can be obtained by solving the normal equations of a and b.
From Eqs. (5.10) and (5.18) the normal equations are written as
∂E
∂a = ∂
∂a
n
k=1
(yk − axk − b)
2 = 0, (5.19a)
∂E
∂b = ∂
∂b
n
k=1
(yk − axk − b)
2 = 0. (5.19b)
Performing the partial derivatives in Eqs. (5.19) one has
−2
(yk − axk − b) xk = 0, (5.20a)
−2
(yk − axk − b)=0, (5.20b)
where the suffices in the summations are dropped for simplicity. Equations (5.20) can be
rewritten as
a
x2
k + b
xk = xkyk, (5.21a)
a
xk + nb = yk. (5.21b)Least-Squares Straight-Line Fit 97
The above set of equations are linear in the unknowns a and b and can be easily solved.
The equation for a is obtained by multiplying Eq. (5.21a) by n, Eq. (5.21b) by xk and
then subtracting one from another. Similarly, equation for b is obtained by multiplying
Eq. (5.21a) by xk, Eq. (5.21b) by x2
k and then subtracting one from another. The
equations for a and b are obtained as
a = nxkyk − xk
yk
nx2
k − (
xk)
2 , (5.22a)
b =
x2
k
yk − xk
xkyk
nx2
k − (
xk)
2 . (5.22b)
If y is expected to fall on a straight-line through the origin, y = ax, and if the measurement
of

y all have the same uncertainties, then show that the best estimate for the constant a is
xkyk/
x2
k.
To assess the fitness of the obtained relation one may inspect a plot of the given data
with the obtained curve and from the value of the coefficient of determination r2 which is
defined as
r2 = St − Sr
St
, St = n
k=1
(yk − y¯)
2 , Sr = n
k=1
[yk − (axk + b)]2 (5.23)
with ¯y being the average value of yk. For a perfect fit Sr = 0 and hence r2 = 1. For a poor
fit r2 will be far from the value 1.
One can also determine the uncertainty or standard deviation σy in the numbers y1, y2,
..., yn. The given set of values yk are normally distributed about the true value axk +b with
width parameter σy. Thus, the deviations yk − (axk + b) are normally distributed. Then,
σy is given by
σy =

1
n
(yk − (axk + b))2
1/2
. (5.24)
In Eq. (5.24) a and b are unknown. They must be replaced by the best estimates, namely,
(5.22) and this replacement would slightly reduce the value of σy given by Eq. (5.24). It
can be shown that this reduction is compensated if n in the denominator of Eq. (5.24) is
replaced by (n − 2). Now, σy is
σy =
 1
(n − 2)
(yk − (axk + b))2
1/2
. (5.25)
As long as n is sufficiently large the difference between n and (n−2) is unimportant. On the
other hand, (n − 2) is reasonable if only two data points, say, (x1, y1) and (x2, y2) are used.
In this case, the fitted line passes exactly through both points. Because it is always possible
to find a straight-line passing through any two given points, one cannot deduce anything
about the reliability of the estimate. Since both points fall exactly on the best line, the
summation terms in Eqs. (5.24) and (5.25) are zero. The formula (5.24) gives σy = 0 which
is absurd whereas with (n − 2) Eq.(5.25) gives σy = 0/0, undetermined, and is correct.
Essentially, there is only (n − 2) degrees of freedom. The uncertainties in a and b are given
by
σa = σy
n/∆, σb = σy
x2
k
 ∆, ∆ = n
x2
k −
xk
2
. (5.26)
What is the expression for σy in the case of a straight-line passing through the origin?
In this case
a =


xkyk
x2
k
. (5.27)98 Curve Fitting
To draw a straight-line passing through the origin only one additional point is required.
Hence,
σy =
 1
n − 1
(yk − axk)
2
1/2
. (5.28)
Example:
The following table gives the measured output voltage (y) of an electronic circuit as a
function of the applied input voltage (x). Find the least-squares straight-line fit for the
data given.
x in V 0.1 0.2 0.3 0.4 0.5 0.6 0.7
y in V 0.16 0.21 0.23 0.30 0.36 0.39 0.46
For the above data, the values of x2
k, xkyk,
xk,
yk,
x2
k and xkyk are given in the
following table:
xk yk x2
k xkyk
0.1 0.16 0.01 0.016
0.2 0.21 0.04 0.042
0.3 0.23 0.09 0.069
0.4 0.30 0.16 0.120
0.5 0.36 0.25 0.180
0.6 0.39 0.36 0.234
0.7 0.46 0.49 0.322
 2.8 2.11 1.40 0.983
Now, from Eqs. (5.22) the coefficients a and b are calculated as
a = 7 × 0.983 − 2.8 × 2.11
7 × 1.4 − 2.8 × 2.8
= 0.49643,
b = 1.4 × 2.11 − 2.8 × 0.983
7 × 1.4 − 2.8 × 2.8
= 0.10286 V.
Thus, the least-squares straight-line fit is y = 0.49643x + 0.10286. Figure 5.2 shows the
graph of x versus y. Solid circles are the given data and the straight-line is the best fit
obtained. The straight-line is drawn by generating 100 points of (xk, yk) from the obtained
fit. (This is followed in other examples in the present chapter.) The value of the coefficient
of determination r2 is estimated as 0.987 which is close to the perfect fit value 1.Curve Fitting to Exponential and Power-Law Functions 99


0 0.5 1
0.5
0.25
0
FIGURE 5.2
Graphical verification of fitness of the least-squares straight-line fit of the data given in
the example. Solid circles are the given data and straight-line is the best fit given by y =
0.49643x + 0.10286. The units of x and y are in V.
5.4 Curve Fitting to Exponential and Power-Law Functions
In many physical problems nonlinear relations between two or more variables are encoun￾tered. For example, the number of particles travelled a distance x from a source obeys the
relation N = N0e−λx, where N0 and λ are constants. The absorption spectrum of crystals
is of exponential form. Suppose, it is desired to fit a nonlinear curve for a set of given data.
In the case of linear function y = ax + b the normal Eqs. (5.21) are also linear in a and b.
Consequently, the normal equations are easily solved and simplified formulas for a and b
are obtained. What is the situation for nonlinear functions? One may consider two types
of nonlinear functions in Eq. (5.2):
(1) nonlinear in x but linear in {ci} and
(2) nonlinear in both x and {ci}.
The functions y = c1x + c2x2 + c3x3, c1x3/2 and c1 sin x + c2 are nonlinear in x but linear
in the unknowns to be determined. In contrast, the functions y = axb, beax and a/(x + b)
are nonlinear in x as well as in the unknowns.
For case (1), the unknowns {ci} in the normal Eqs. (5.11) are still linear and hence they
can be easily determined. This will be done for polynomial and trigonometric polynomials
in Sections 5.6 and 5.8, respectively. For case (2), the normal Eqs. (5.11) are obviously
nonlinear. Consequently, solving them is difficult and often one has to employ a suitable
numerical scheme which requires time-consuming computations and also one has to make a
good starting values of the unknowns. However, for certain nonlinear functions of the case
(2), by suitable change of variables and parameters a linear relation can be obtained. For
the redefined constants, the normal equations are linear.
In this section, curve fitting to the often realized exponential and power-law nonlinear
functions by transforming them into linear function is demonstrated. These two functions100 Curve Fitting
occur in many physical problems, for example, see Problems 5.6–5.10, 5.13 and 5.14 at the
end of this chapter.
5.4.1 Exponential Function y = b eax
Let the given set of data be fitted into the exponential relation
y = f(x) = b eax. (5.29)
Taking logarithm on both sides of Eq. (5.29) results in
ln y = ln b + ax. (5.30)
Introducing the change of variables and constants
Y = ln y, X = x, A = a, B = ln b (5.31)
Eq. (5.30) is rewritten as
Y = AX + B. (5.32)
(Note: In Eq. (5.29) y must be greater than zero because logarithmic function is undefined
for y < 0. If the values of y are not greater than 0 then y can be redefined. If all yk’s are
negative then assume the relation y = −b eax. In this case Eqs. (5.30)-(5.31) read as
ln(−y) = ln b + ax (5.33)
and
Y = ln(−y), X = x, A = a, B = ln b , (5.34)
respectively.) That is, under the change of variables given by Eq. (5.31) the nonlinear
Eq. (5.29) is transformed into the linear Eq. (5.32). For the data set (Xk, Yk) the straight￾line fit, Eq. (5.32), can be obtained by the least-squares approximation. This gives the
unknowns A and B in Eq. (5.32). Then, the unknown a is simply A and b is eB. The
following example illustrates the curve fitting to the exponential function (5.29).
Example:
A particle is ejected from a source and its measured velocities at a few values of time t are
given below. Fit the given data to v = b eat.
t in sec. 0 1 2 3 4 5
v in m/sec. 12.6 4.5 2.4 1.1 0.6 0.1
Fitting the above data to the given nonlinear function is equivalent to fitting the transformed
data X, Y (Eq. (5.31)) to the linear relation (5.32) and determining the constants A and
B. Then, the values of a and b in the given nonlinear function can be obtained from the
transformation given by Eq. (5.31). The least-squares calculation is summarized below.
Xk = tk Yk = ln vk X2
k XkYk
0 2.53370 0 0.00000
1 1.50408 1 1.50408
2 0.87547 4 1.75094
3 0.09531 9 0.28593
4 −0.51083 16 −2.04330
5 −2.30259 25 −11.51293
 15 2.19514 55 −10.01528Curve Fitting to Exponential and Power-Law Functions 101
t sec.
v m/sec.
0 1 32 4 5 6
12
8
4
0
FIGURE 5.3
Graphical verification of fitness of exponential fit of the given data. Solid circles are the
given data and the continuous curve is the obtained best fit.
From Eqs. (5.22) the constants A and B are obtained as (here n = 6)
A = 6 × −10.01528 − 15 × 2.19514
6 × 55 − 15 × 15 = −0.88589,
B = 55 × 2.19514 − 15 × −10.01528
6 × 55 − 15 × 15
= 2.58059.
The constants a and b are then calculated as
a = A = −0.88589 sec−1,
b = eB = 13.20494 m/sec .
That is, the exponential fit to the given data is
v = 13.20494 e−0.88589t m/sec .
Figure 5.3 illustrates the correctness of the obtained best fit. Here the given data are repre￾sented by solid circles while the continuous curve is the computed best exponential fit. The
given data falls closely to the continuous curve implying that the obtained fit is the best
one.
5.4.2 Power-Law Function y = b xa
The number of earthquakes of size x or greater, where x is the energy released in different
geographic regions or even across the world as a whole, is found to be proportional to x−0.8
to x−1. Power-law distributions are increasingly being used by reinsurance companies and
governments to assess the risks posed by natural hazards. The number of wildfires per unit
area per year per eco-region versus the area of the wildfire and the number of rockfalls per
unit volume versus their volume have exhibited power-law relation [3–5].
Like the exponential function (5.29) the power-law function y = bxa can be transformed
into a linear relation by suitable change of variables and constants. The following example
illustrates curve fitting to the power-law relation.102 Curve Fitting


0 5 10
2.2
2
1.8
FIGURE 5.4
Graphical verification of fitness of power-law fit of the given set of data. Solid circles are
the given data and the continuous curve is the obtained best fit.
Example:
A nonlinear oscillator is subjected to an external periodic force of fixed amplitude and
frequency. The following table gives the ratio (G) of the amplitudes of the oscillation of the
oscillator and the applied force calculated as a function of a control parameter α by solving
the underlying equation of motion. Obtain the power-law fit G = bαa to the given data
after linearizing them.
α 13579
G 2.00 2.09 2.16 2.21 2.24
For illustrative purpose, define x = α and y = G. The nonlinear function y = bxa can be
converted into the linear form Y = AX + B under the change of variables and constants
Y = ln y, X = ln x, A = a and B = ln b. From the given data one has
Xk = 6.851185,
Yk = 3.799888,
X2
k = 12.411600,
XkYk = 5.364400
and
a = A = 0.05214, B = 0.68854, b = eB = 1.9908.
Thus, the power-law fit is y = 1.9908 x0.05214. Figure 5.4 depicts the plot of x versus given y
(marked by solid circles) and the obtained best power-law relation (continuous curve). The
deviation between the computed fit and the given data is negligible.Curve Fitting to a Sigmoid Function 103
TABLE 5.1
Linearization of certain nonlinear functions. For all the cases the linearized relation is Y =
AX + B.
Function y = f(x) Example New variables and constants
problems X = Y = A = B =
y = b eax, y > 0 6–10 x ln y a ln b
y = −b eax, y < 0 11 x ln(−y) a ln b
y = b e±ax2
12 ±x2 ln y a ln b
y = b xa, y > 0 13–15 ln x ln y a ln b
y = −b xa, y < 0 16 ln x ln(−y) a ln b
y = a ln x + b 17, 18 ln x y a b
y = b x eax 19 x ln(y/x) a ln b
y = 1/(ax + b) 20 x 1/y a b
y = x/(bx + a) 21 1/x 1/y a b
y = 1/(c + beax) 22 x ln(1/y − c) a ln b
c must be given
y = (a/x) + b 23 x xy a b
y = b ax, y > 0 24 x ln y ln a ln b
y = −b ax, y < 0 25 x ln(−y) ln a ln b
y = a x2 + b 26 x2 y a b
y = ax2 + bx 27 x y/x a b
y = ag(x) + b 28 g(x) y a b
In addition to the exponential and power-law functions there exist many nonlinear func￾tions which are convertible into a linear form. Some of the nonlinear functions and their
corresponding linear relations and the change of variables and constants are summarized in
Table 5.1. Draw graphs of x versus y for the nonlinear functions listed in Table 5.1 for fixed
values of the constants a, b and c.
5.5 Curve Fitting to a Sigmoid Function
A sigmoid curve is an ‘S’ shape curve and is represented by mathematical functions called
sigmoid functions. Some of the sigmoid functions are the logistic function 1/(1 + e−x),
hyperbolic tangent function (ex − e−x)/(ex + e−x) and the error function erf(x) =
(2/
√π)
 x
0 e−t2
dt. Figure 5.5 shows the plot of the logistic function. The sigmoid logis￾tic function maps any real value to the range [0, 1] as seen in Fig. 5.5.
Sigmoid functions are used as an activation function of artificial neurons and cumulative
distribution functions. It is also used to find the probability of a binary variable in binary
classification. Certain experimentally measurable variables in dynamical systems are found
to display sigmoid-type variation with a control parameter, an example is the dependence
of normalized transmittance of crystals on input intensity. Another example is the reflection104 Curve Fitting
x
y
-8 -4 0 4 8
1
0.8
0.6
0.4
0.2
0
FIGURE 5.5
x versus y(x)=1/(1 + e−x).
probability of neutrons in the diffusion of neutrons through a moderating material slab (see
Section 20.8 and Fig. 20.7).
For simplicity consider the sigmoid curve of the form appearing for x > 0 in Fig. 5.5.
Let us describe the fitting a given data set to a sigmoid function. A choice of the sigmoid
function is
y(x) ∝
1
1 + beax , x> 0. (5.35)
If x is not > 0 then introduce the change of variable x = x−xmin where xmin is the minimum
value of x. Since the right-side of Eq. (5.35) has the exponential term, by appropriately
redefining the variables x and y and the constants, an exponential fit can be constructed
and then it can be converted to the sigmoid function.
Assume that x > 0 and convert the range of values of y into, for example, the range
[1, 2] (why?) by introducing the change of variable
y =1+ y − ymin
ymax − ymin
, (5.36)
where ymin and ymax are the minimum and the maximum values of y, respectively. In this
case, an appropriate sigmoid function is
y = 2
1 + b eax . (5.37)
This equation can be rewritten as
y = 2
y − 1 = b eax. (5.38)
Taking logarithm on Eq. (5.38) gives
ln y = ln b + ax. (5.39)
Defining
X = x, Y = ln y, A = a, B = ln b (5.40)Curve Fitting to a Sigmoid Function 105
1 2 3 4 5 m 1 m
FIGURE 5.6
A system consisting of m oscillators coupled in one way. Larger solid circles represent
oscillators. Arrows indicate the direction of coupling. jth oscillator is coupled to (j + 1)th
oscillator only.
Eq. (5.39) takes the form
Y = AX + B. (5.41)
For the given data set (xk, yk), k = 1, 2,...,n with xk’s are arranged in an ascending
order ymin = y1 and ymax = yn (generally). When y = yn(= ymax) then in Eq. (5.36) y
n = 2
and y
n (refer Eq. (5.38)) becomes 0 and Yn = ln y
n. = ln 0 which is undefined. So, discard
(Xn, Yn) (as well as any other pair of data giving Yk = ln 0) and consider the remaining set
of data. For this data set make a straight-line fit, Eq. (5.41), and find the values of A and
B and then a = A and b = eB. From Eqs. (5.36) and (5.38) the relevant fit is
y = ymin + (ymax − ymin)

−1 +
2
1 + b eax 
. (5.42)
Example:
In a system of one-way coupled m oscillators (refer Fig. 5.6) the output gain measured in
the first 10 oscillators is given below. A rough graph of the oscillator number (n) versus the
gain (G) shows a sigmoid curve. Fit the given data to an appropriate sigmoid function.
n 1 2 3 4 5 6 7 8 9 10
G 0.7 1.58 1.9 2.45 2.74 2.8 2.9 2.93 2.96 3
Here, n is > 0 while G ∈ [0.7, 3]. First, convert G into the range [1, 2] using Eq. (5.36)
by taking G as y and G as y
. The result is (n, G
) = (1, 1), (2, 1.38261), (3, 1.52174),
(4, 1.76087), (5, 1.88696), (6, 1.91304), (7, 1.95652), (8, 1.96957), (9, 1.98261) and (10, 2). For
these data assume the sigmoid function as
G = 2
1 + b ean . (5.43)
With G = (2/G
) − 1, X = n, Y = ln G, A = a and B = ln b the relation between X and
Y is Y = AX +B. For (n, G
) = (10, 2) the value of Y = ln G = ln 0 so discard this data for
curve fit. A and B are computed as A = −0.59048 and B = 0.44363 and so a = −0.59048
and b = eB = 1.55835. Then, referring to Eq. (5.42) one can write
G = Gmin + (Gmax − Gmin)

−1 +
2
1 + b ean 
= −1.6 +
4.6
1+1.55835 e−0.59048n . (5.44)
Figure 5.7 compares the obtained fit with the given data. The obtained fit is satisfactory.106 Curve Fitting
n
G
0 1 32 4 5 6 7 1098
3
2.5
2
1.5
1
0.5
FIGURE 5.7
Plot of n versus G with solid circles and continuous line representing the given data and
the obtained fit (Eq. (5.44)), respectively.
5.6 Polynomial Fit
There are certain physically measurable quantities obeying polynomial relations. For exam￾ple, the square of magnetic rigidity of a particle as a function of temperature T is given
by
H2
r = A 
BT + T2
, (5.45)
where A and B are constants. Thus, it is of interest to study the polynomial fit to the given
data. In this sections the problem of polynomial fit using polynomial regression is described.
It is desired to fit the given data (xk, yk), k = 1, 2, ..., n to the polynomial P(x) of degree
m that is given by
y = P(x) = c1 + c2x + c3x2 + ··· + cm+1xm. (5.46)
Referring to Section 5.2.2 the normal equations for the function of the form
y = c1φ1(x) + c2φ2(x) + ··· + cm+1φm+1(x) (5.47)
are obtained as
n
k=1
[φi (xk) yk − φi (xk) P (xk)] = 0, i = 1, 2,...,m + 1. (5.48)
For the polynomial function, Eq. (5.46), the φ’s are given by
φi = xi−1, i = 1, 2, ..., m + 1. (5.49)
Now, the normal Eqs. (5.48) become
m

+1
j=1
cj
n
k=1
x
i+j−2
k = n
k=1
ykxi−1
k , i = 1, 2, ..., m + 1. (5.50)
Defining
n
k=1
ykxi−1
k = bi (5.51)Polynomial Fit 107
Eqs. (5.50) are rewritten as
m

+1
j=1
cj
n
k=1
x
i+j−2
k = bi, i = 1, 2, ..., m + 1. (5.52)
This generates m+ 1 equations for m+ 1 unknown c’s. The Gauss–Jordan method discussed
in Section 4.5 can be used to solve the above simultaneous equations. For this purpose
Eqs. (5.52) can be re-expressed in the form Ac = B (refer Eq. (4.18), Section 4.4), where c
is the unknown to be determined, that is,
m

+1
j=1
aij cj = bi, i = 1, 2, ..., m + 1, (5.53a)
with
aij = n
k=1
x
i+j−2
k . (5.53b)
In vector form Eq. (5.53a) is given by Ac = B, where A is a matrix of order (m+1)×(m+1)
and B is a column matrix with m + 1 rows. Note that when m = 1 the normal equations
for the straight-line fit are recovered (verify).
Example:
(a) Derive the normal equations for the quadratic polynomial
y(x) = P(x) = c1 + c2x + c3x2.
(b) The potential energy V (x) of a system measured at five values of x are given below. The
plot of V (x) suggests a polynomial form of V (x). Fit the data to the quadratic polynomial.
x in cm 0 1 2 3 4
V in g cm2/sec2 1 2 7 16 29
(a) The normal equations for the given quadratic polynomial function are obtained from
Eq. (5.53a) by substituting m = 2. They are
a11c1 + a12c2 + a13c3 = b1,
a21c1 + a22c2 + a23c3 = b2,
a31c1 + a32c2 + a33c3 = b3,
where the coefficients aij ’s and bi’s are (from Eqs. (5.51) and (5.53b))
a11 = n, a12 = xk, a13 = x2
k, b1 = yk ,
a21 = xk, a22 = x2
k, a23 = x3
k, b2 = xkyk ,
a31 = x2
k, a32 = x3
k, a33 = x4
k, b3 = x2
kyk.
(b) For convenience define y = V . For the given data the coefficients aij ’s and bi’s are
calculated as
a11 = 5, a12 = 10, a13 = 30, b1 = 55 ,
a21 = 10, a22 = 30, a23 = 100, b2 = 180 ,
a31 = 30, a32 = 100, a33 = 354, b3 = 638.108 Curve Fitting


20-2 4 6
40
30
20
10
0
FIGURE 5.8
Graphical verification of fitness of the polynomial fit y(x) = c1 + c2x + c3x2 of the given
data. Solid circles are the given data and the continuous curve is the obtained best fit.
Solving the normal equations gives
c1 = 1, c2 = −1, c3 = 2.
Thus, the polynomial fit is y(x)=1 − x + 2x2. Figure 5.8 shows the graph of x versus y
for the given data and the best fit obtained. What are the units of c1, c2 and c3? Suppose,
the problem is to find c2 and c3 with the value of c1 given as 1. In this case the given
polynomial can be transformed into the linear equation Y = c2 + c3x by the change of
variable Y = (y − c1)/x. Then, the least-squares straight-line fit can be performed to the
given data.
5.7 Curve Fitting to Gaussian Functions
Gaussian functions are realized in many branches of physics, mathematics and engineering,
for example, in quantum mechanics and optics. Fluctuations in the values of observables in
most of the real experiments obey Gaussian statistics. The height distribution of a popula￾tion, the distribution of the sum of a throw of a set of six-sided dice and voltage fluctuations
in a power supply, the photoluminescence spectrum of certain crystals are of the Gaussian
form [6-7].
The functional form of the Gaussian distribution is
G(x) = 1
σ
√2π
e−1/(2σ2)(x−x0)2
, (5.54)
where x0 is the average of the measurement and σ is the standard deviation of the measure￾ment. Often G(x) is related to the probabilities of measurements of the values of certain
variables. Suppose, the probability of outcome of a measurement follows the Gaussian dis￾tribution. Denote the probability of getting a value of a variable, say, x in a measurementCurve Fitting to Gaussian Functions 109
in the range of x → x + ∆x is P(x → x + ∆x)=∆xG(x), that is,
P(x → x + ∆x) = ∆x
σ
√2π
e−1/(2σ2)(x−x0)2
. (5.55)
Out of M measurements the expected number of measured values within a range [x, x+∆x]
is MP(x → x + ∆x). This is written as
m(x) = MP(x → x + ∆x) = M∆x
σ
√2π
e−1/(2σ2)(x−x0)2
. (5.56)
When x versus m follows Eq. (5.56), it is desired to find the values of x0 and σ through
the curve fit. Taking logarithm on both sides of Eq. (5.56) gives
ln m = ln(M∆x) − ln 
σ
√
2π

− x2
0
2σ2 +
x0
σ2 x − 1
2σ2 x2. (5.57)
With
y = ln(m), c1 = ln(M∆x) − ln 
σ
√
2π

− x2
0
2σ2 , (5.58a)
c2 = x0
σ2 , c3 = − 1
2σ2 (5.58b)
Eq. (5.57) is rewritten as
y = c1 + c2x + c3x2 . (5.59)
In Eq. (5.58) M and ∆x are known values while x0 and σ are the unknown to be determined
through the curve fit. The values of c1, c2 and c3 can be calculated from the polynomial fit
discussed in Section 5.6. After this the values of x0 and σ can be obtained from the relations
x0 = − c2
2c3
, (5.60a)
σ = 1
√2π exp 
ln(M∆x) + c2
2
4c3
− c1

. (5.60b)
Example 1:
A power supply was set to 5 V and 500 measurements of voltage at a sampling rate of, say,
1 MHz were made. The values of V varied in the interval [4.9588 V, 4.9656 V]. This interval
was divided into 18 bins with equal size ∆V = 0.0004 V and the number of values of V
denoted as m in each bin noted are given in the table below.
V 4.9588 4.9592 4.9596 4.96 4.9604 4.9608 4.9612 4.9616 4.962
m 1 1 2 5 12 24 40 53 63
V 4.9624 4.9628 4.9632 4.9636 4.964 4.9644 4.9648 4.9652 4.9656
m 65 61 55 45 37 21 12 2 1
Fit the data to the Gaussian function
m = M∆V
σ
√2π e−1/(2σ2)(V −V0)2
, M = 500 .110 Curve Fitting
V
ln(
m)
(a)
4.958 4.96 4.962 4.964 4.966
4
3
2
1
0
V
m
(b)
4.958 4.96 4.962 4.964 4.966
70
60
50
40
30
20
10
0
FIGURE 5.9
(a) Quadratic polynomial fit of V versus ln(m). (b) Plot of given V versus m (solid circles)
and the obtained corresponding Gaussian fit (continuous line).
Fitting the given data to the quadratic Eq. (5.59), as in the example in Section 5.6, with
x = V and y = ln(m) gives
c1 = −9075343.58068,
c2 = 3657621.57493 V−1 ,
c3 = −368531.21193 V−2 .
Then, substitution of these values of c1, c2 and c3, M = 500 and ∆x = ∆V = 0.0004 in
Eq. (5.60) gives V0 = x0 = 4.96243 V and σ = 0.00126 V. Figures 5.9a and b show the plots
of V versus ln(m) and V versus m, respectively. The closeness of the given data with the
fitted Gaussian function indicates that the fluctuations in the voltage follow the Gaussian
statistics.
Example 2:
The height, H, of 1000 girls of a particular age in a city was measured. The values of H
varied in the interval [110.3 cm, 111.8 cm]. This interval was divided into 16 bins with equal
size ∆H = 0.1 cm and the number of values of H denoted as m in each bin noted are given
in the table below.Trigonometric Polynomial Fit 111
H in cm 110.3 110.4 110.5 110.6 110.7 110.8 110.9 111.0
m 1 5 10 47 64 123 157 165
H in cm 111.1 111.2 111.3 111.4 111.5 111.6 111.7 111.8
m 159 129 75 33 20 8 3 1
Fit the data to the Gaussian function
m = M∆H
σ
√2π e−1/(2σ2)(H−H0)2
, M = 1000.
Fitting the given data to the quadratic Eq. (5.59) with x = H and y = ln(m) gives
c1 = −111889.95601,
c2 = 2015.58007 cm−1,
c3 = −9.07673 cm−2.
Then, substitution of these values of c1, c2 and c3, M = 1000 and ∆x = ∆V = 0.1 in
Eq. (5.60) gives H0 = x0 = 111.03014 cm and σ = 0.24126 cm.
5.8 Trigonometric Polynomial Fit
In many phenomena, particularly, in oscillatory processes periodic variation of certain phys￾ically measurable quantities is encountered. For example, solutions of certain differential
equations, intensity of light in an experiment and planetary motion appear periodic or
almost periodic. Consider the so-called van der Pol oscillator equation
x¨ − b

1 − x2
x˙ + x = 0. (5.61)
Exact analytical solution of this equation is not known. A numerical solution of it is shown
in Fig. 5.10 for b = 0.4. Because x(t) appears as a periodic function of t, one can try
to fit the data to an approximate trigonometric polynomial. The least-squares procedure
developed earlier for linear function can be readily extended to trigonometric polynomials.
This section defines trigonometric polynomials and discusses curve fitting of a given set of
data to it.
5.8.1 Trigonometric Polynomials
Consider the Fourier series
f(x) = a0
2 +∞
i=1
(ai cosix + bi sin ix), (5.62)
where a0, ai’s and bi’s are constants and are computed from the Euler’s formula
ai = 1
2π
 2π
0
f(x) cosixdx, (5.63a)
bi = 1
2π
 2π
0
f(x) sin ixdx, i = 1, 2, .... (5.63b)112 Curve Fitting


0 10 20 30
3
0
-3
FIGURE 5.10
The numerical solution of the van der Pol oscillator Eq. (5.61).
The Fourier series f(x) given by Eq. (5.62) is a convergent series in the interval [0, 2π]
if f
(x) is piece-wise continuous on [0, 2π]. Therefore, in practical problems it is sufficient
to consider first few terms in the summation of Eq. (5.62). When the running index i in
Eq. (5.62) is chosen as 1, 2, ... up to M then the Fourier series becomes
f(x) = a0
2 +
M
i=1
(ai cosix + bi sin ix) (5.64)
then it is called a trigonometric polynomial of order M. When f(x) is an even function,
that is, f(−x) = f(x) for all x then the Fourier series Eq. (5.62) becomes a cosine series
fc(x) = a0
2 +∞
i=1
(ai cosix). (5.65)
On the other hand, the series reduces to a sine series
fs(x) = a0
2 +∞
i=1
(bi sin ix) (5.66)
if f(x) is an odd function, that is, f(−x) = −f(x) for all x.
5.8.2 Trigonometric Polynomial Approximation
Let (xk, yk), k = 1, 2, ..., n be a given set of n data, where yk = f(xk) and xk are equally
spaced. The values of xk are given by
xk = k
n
2π (5.67)
so that k = 1 gives x1 = 2π/n and k = n gives xn = 2π. In other words, y is assumed as a
periodic function of x with period 2π. It is desired to fit the data to the trigonometric func￾tion given by Eq. (5.64). The coefficients a0, ai’s and bi’s can be determined by minimizing
the quantity
n
k=1
[yk − f (xk)]2 . (5.68)Least-Squares Curve Fitting to y = af(x) + bg(x) 113
The coefficients are given by
ai = 2
n
n
k=1
yk cosixk, i = 0, 1, ..., M (5.69a)
bi = 2
n
n
k=1
yk sin ixk, i = 1, ..., M. (5.69b)
The fitting is possible provided 2M + 1 ≤ n.
Example:
The numerically computed solution of y(x) + y(x) = 0, y(0) = 1 and y
(0) = 0 at four
values of x are given below. Fit the data to an appropriate trigonometric polynomial.
x π/2 π 3π/2 2π
y 0 −10 1
Since the number of (x, y) pairs is (n =)4 the maximum value of M according to 2M +1 ≤ n
is 1. That is, the data can be fitted to the function of the form
f(x) = a0
2 + a1 cos x + b1 sin x.
The values of the coefficients a0, a1 and b1 are estimated from Eqs. (5.69) and are
a0 = 2
4

4
k=1
yk = 0,
a1 = 2
4

4
k=1
yk cos xk = 1,
b1 = 2
4

4
k=1
yk sin xk = 0.
Thus, the obtained trigonometric polynomial fit for the given data is simply f(x) = cos x
which is the exact solution of the given equation.
5.9 Least-Squares Curve Fitting to y = af(x) + bg(x)
Assume that the functional relation is not a linear or linearizable or polynomial or trigono￾metric polynomial. In such a case if the constants a and b are still linear then linear normal
equations can be easily derived. This is illustrated in this section.
Let the function to which the given data is to be fitted is of the form
y = af(x) + bg(x). (5.70)
The error quantity E(a, b) (Eq. (5.7)) for the function given by Eq. (5.70) is
E(a, b) = n
k=1
(yk − af − bg)
2 . (5.71)114 Curve Fitting
The necessary conditions for E to be minimum are
∂E
∂a = −2
(yk − af − bg) f = 0, (5.72a)
∂E
∂b = −2
(yk − af − bg) g = 0. (5.72b)
The above equations are rewritten as
a
f 2 + b
fg = ykf, (5.73a)
a
fg + b
g2 = ykg (5.73b)
which are linear in a and b. Defining
c11 = f 2, c12 = fg, c22 = g2,
b1 = ykf, b2 = ykg
(5.74)
Eqs. (5.73) become
ac11 + bc12 = b1, ac12 + bc22 = b2. (5.75)
Solving the above equations gives
a = b1c22 − b2c12
c11c22 − c2
12
, (5.76a)
b = b2c11 − b1c12
c11c22 − c2
12
. (5.76b)
Example:
Obtain the least-squares fit to the function y = a e−2x + b sin x for the following data.
x 01 2 3 4
y 4.8 −1 −1.5 −0.3 1.4
For the given function f(x)=e−2x and g(x) = sin x. For the given data Eqs. (5.74) become
c11 = e−4xk = 1.018657,
c12 = e−2xk sin xk = 0.130631,
c22 = sin2 xk = 2.12756,
b1 = yke−2xk = 4.636918,
b2 = yk sin xk = −3.307277.
The constants a and b are then calculated as a = 4.78904 and b = −1.84854. Figure 5.11
depicts x versus y for the given data and the obtained fit.Least-Squares Curve Fit to y = f(x)g(c, x) 115


-1 0 1 32 4 5
6
4
2
0
-2
FIGURE 5.11
Graphical verification of fitness for the function y(x) = ae−2x + b sin x of the given data.
Solid circles are the given data and the continuous curve is the obtained best fit.
5.10 Least-Squares Curve Fit to y = f(x)g(c, x)
Let us consider the functional relation between x and y(x) as
y = f(x)g(c, x), (5.77)
where f(x) is independent of the unknown parameters and g(c, x) can be linearizable or
polynomial or trigonometric polynomial or for which the normal equations are linear in c.
In this case rewrite this equation with y = y/f(x) as y = g(c, x). The curve fitting of the
given data set to this new equation can be done.
Relations of the form of Eq. (5.77) are realized in many fields of science. For example,
the famous Planck’s black body radiation formula is
E(ν, t) = 8πν2
c3
hν
ehν/(KBT) − 1
. (5.78)
For visible and ultraviolet regions (higher values of ν) E(ν, T) is approximated as
E(ν, t) ∝ aν3 e−bν/T , (5.79)
where a and b are constants. E(ν, T) given by Eq. (5.79) is the Wien’s formula. In certain
nonlinear systems driven by a weak external periodic driving signal and noise the so-called
signal-to-noise ratio (SNR) scales with the noise intensity D as
SNR = b
D2 ea/D. (5.80)
Example:
The numerically computed SNR values of a driven nonlinear oscillator for 8 values of noise
intensity D are given below (units of D and SNR are dropped for simplicity). Assuming
the relation (5.80) compute the values of a and b.116 Curve Fitting
D
SNR
0 0.5 1 1.5 2 2.5 3
5
4
3
2
1
0
FIGURE 5.12
Plot of given SNR versus D (solid circles) and the obtained corresponding fit (Eq. (5.83))
(continuous line).
D 0.1 0.2 0.4 0.6 1 1.7 2.4 3
SNR 0.3 2.5 4.8 3.6 2.6 1.2 0.7 0.3
Rewrite the given relation as
(SNR)D2 = b ea/D. (5.81)
Define
X = 1/D, Y = ln 
(SNR)D2
, A = a, B = ln b. (5.82)
Then, Eq. (5.81) takes the form ln Y = AX + B. For the given data set the straight-line fit
gives a = A = −0.73978, B = 1.57735 and b = eB = 4.84213. So, the obtained fit is
SNR = 4.84213
D2 e−0.73978/D. (5.83)
Figure 5.12 shows the plot of given SNR (solid circles) and the SNR obtained (continuous
line) from the constructed mathematical relation (5.83). The closeness of the given SNR
with the obtained SNR indicates that the Eq. (5.83) is an appropriate relation between D
and SNR.
5.11 Concluding Remarks
Curve fitting to a variety of relations between two variables is presented. For certain nonlin￾ear relations under suitable change of variables and constants, the nonlinear relations can
be transformed to the linear relation. For a data set following, say, a straight-line relation,
it is possible to make a fit to any other functional relation also. But the deviation of the
data obtained from the constructed fit from the given data will be large. Therefore, first
draw a graph of the given data and identify an appropriate functional relation for the given
data. After obtaining the mathematical relation by the least-squares approximation drawBibliography 117
the graph of the computed data and the given data and check the closeness of these two.
It is important to compute the coefficient of determination r2 to assess the fitness of the
arrived fit. Further, it is appropriate to specify the uncertainties in the constants involved
in the fit.
5.12 Bibliography
[1] Least absolute deviations. https://en.wikipedia.org/wiki/Least−absolute−devia￾tions (accessed on January 10, 2023).
[2] Why we use ‘least squares’ regression instead of ‘least absolute deviations’ regres￾sion. https://www.bradthiessen.com/html5/docs/ols.pdf (accessed on January
10, 2023).
[3] B.D. Malamud, Phys. World August 2004. pp.31–35.
[4] S. Hergarten, Natural Hazards and Earth System Sciences 4:309, 2004.
[5] M. Mitzenmacher, Internet Maths. 1:226, 2004.
[6] J. Taylor, An Introduction to Error Analysis. University Science Books, Sausalito,
1982. 2nd edition.
[7] M.W. Ray, The Phys. Teach. 61:224, 2023.
5.13 Problems
A. Linear Fit
5.1 A particle of charge q is accelerated to 5 different potentials of V volts and
the corresponding masses m are measured. The ratio m/m0, where m0 is the
rest mass, as a function of V is given below. Fit the data to the relation m =
m0(aV + b) and find a and b.
V (in volts) 1 2 3 4 5
m/m0 1.5 2.0 2.5 3 3.5
5.2 The volume V of a gas measured at various temperatures T is listed below.
Assuming the linear relation V = V0(1 + aT), where V0 is the volume of the gas
at 0◦C and a is the expansion coefficient, determine V0 and a.
T ◦ C 70 90 110 130 150
V m3 2.084 2.108 2.132 2.156 2.180
5.3 The latent heat lH of vaporization of steam is given below as a function of tem￾perature T. Fit the data to the relation lH = aT + b.
T ◦ C 40 50 60 70 80
lH J/kg 920 880 870 835 780118 Curve Fitting
5.4 The measured values of current (I) flowing through a resistor for various values
of applied voltage (V ) are given below. Assuming the linear relation V = IR find
the value of resistance R.
V (in volts) 0.5 1.0 1.5 2.0 2.5
I (in ampere) 0.005 0.01 0.015 0.02 0.025
5.5 The zero-pressure molar heat capacities C in KJ/mol.K as a function of temper￾ature T in Kelvin for water are given below. Perform the straight-line fit to the
data.
T 10 20 30 40 50
C 32.258 32.281 32.305 32.332 32.360
B. Curve Fit to Nonlinear Functions That Can Be Converted into a Linear
Function
5.6 In an experiment, molecules are ejected from a source of particles and a beam is
formed by means of a diaphragm. The number of particles N travelled a distance
x without collisions is given below as a function of x. Assume that N follows the
relation N = N0e−λx. Determine N0 and λ.
x (in m) 0.5 1.0 1.5 2.0 2.5
N 687 472 325 223 153
5.7 The probability P of finding a particle as a function of energy E is given in the
table below. If P is assumed to obey the relation P = be−aE determine a and b.
E in eV 0.5 0.6 0.7 0.8 0.9
P 0.1839 0.1506 0.1233 0.1009 0.0827
5.8 The amount of charge q discharged through a resistor R in an RC circuit at
various time are given below. Assuming the relation q = q0e−τ t determine the
relaxation time τ and the initial amount of charge q0 stored in the capacitor.
t sec 0.05 0.10 0.15 0.20 0.25
q ×10−5 coulomb 9.87 9.75 9.63 9.5 9.4
5.9 In an experiment, a metal foil of thickness t is placed between a β-particles
source and a β-detector. The number of particles, N, reaching the detector is
counted. The experiment is repeated with metal foils of different thickness. The
data obtained are listed below. Assuming that N varies according to the relation
N = N0e−µt determine the unknowns N0 and µ.
t in minutes 0.001 0.010 0.020 0.030 0.040
N 780 623 485 377 294Problems 119
5.10 The measured number of particles N emitted from a sample at 5 instants of time
are given below. The decay law is N = N0e−λt, where N0 and λ are the number
of particles present in the sample at time t = 0 and decay rate, respectively, and
the unit of time is hour. Curve fitting the data to the above relation determine
the values of N0 and λ.
t (hours) 20 40 60 80 100
N(t) 8200 6700 5500 4500 3700
5.11 Obtain the exponential fit y = beax to the following data.
x 0.2 0.4 0.6 0.8 1.0
y −1.5 −2.8 −4.5 −7.8 −11.6
5.12 Find the fit y = beax2
to the given data.
x 0.3 0.5 0.7 0.9 1.1
y 2 2.25 2.5 2.9 3.7
5.13 The resistivity R of a conductor as a function of temperature T is given below.
Show that R varies as bT a and find the constants a and b.
T ◦ C 20 40 60 80 100
R ohms 2.2 1.6 1.3 1.1 1.0
5.14 The pressure (P) of helium measured at 5 different volumes (V ) is listed below.
Fitting the data to the power-law relation P = bV a determine the constants a
and b.
V m3 0.02 0.06 0.1 0.14 0.3
P bar 23.0 4.4 2.0 1.2 0.4
5.15 Find the fit y = bxa to the following data by the least-squares approximation
after linearizing the data.
x 0.4 0.8 1.2 1.6 2.0
y 1.9 0.7 0.4 0.2 0.1
5.16 Obtain the fit y = −bxa to the following data.
x 12345
y −0.700 −0.990 −1.212 −1.400 −1.565
5.17 The electric potential V as a function of perpendicular distance r from a long
straight-wire of cross-sectional radius a is given in the following table. Fitting the
data to the relation V = −K ln(r/a) determines the unknowns K and a.120 Curve Fitting
r m 123 4 5
V volts −0.55 −1.1 −1.40 −1.65 −1.85
5.18 Obtain the fit y = a ln x + b to the following data after linearizing the data.
x 0.25 0.40 0.55 0.70 0.85
y −3.65 −2.25 −1.30 −0.55 0.01
5.19 Assuming that the data given below vary according to the relation y = b x eax
find the approximate values of a and b.
x 12345
y 0.45 0.20 0.07 0.02 0.01
5.20 For the following data obtain a least-squares fit of the form y = 1/(ax + b).
x 0.0 0.25 0.5 0.75 1.0
y 2.0 1.3 1.0 0.8 0.6
5.21 Apply the least-squares method fit to the following data to the nonlinear relation
y = x/(b + ax).
x 0.25 0.5 0.75 1.00 1.25
y 0.2 0.3 0.4 0.5 0.6
5.22 Find the fit y = 1/(c + beax), where c = 2 to the given data.
x 12345
y 0.34 0.35 0.36 0.37 0.38
5.23 For the following data fit the nonlinear function y = (a/x) + b.
x −4 −6 −8 −10 −12
y −2.75 −2.50 −2.37 −2.30 −2.25
5.24 Obtain the fit y = bax to the following data after linearizing them.
x 01234
y 0.5 1.0 2.0 4.0 8.0
5.25 Find the fit y = −bax to the given data.
x 0123 4
y −0.25 −0.75 −2.25 −6.75 −20.25
5.26 Obtain the fit y = ax2 + b to the given data after linearizing them.Problems 121
x 0 −1 −2 −3 −4
y 3 2 −1 −6 −13
5.27 Find the fit y = ax2 + bx to the following data.
x 1234 5
y 1.5 2.0 1.5 0.0 −2.5
5.28 The experimentally measured kinetic energy E of beta particles as a function of
v/c, where v is the velocity of the particles and c is the velocity of light is given
below. Assume that E varies according to E = a

√
1
1−v2/c2 − b

. By suitable
change of variables transform the above nonlinear equation into a linear equation.
Then, fit the data to the linearized equation and determine a and b.
v/c 0.0 0.2 0.4 0.6 0.8
E MeV 0.0 0.01054 0.04655 0.1278 0.3407
5.29 The ratio l/l0 of a rod, where l0 and l are the lengths at rest in a reference frame
moving with velocity v and in another frame, respectively, as a function of v/c,
where c is the velocity of light, is given below. Assuming that l varies as l =
l0
b + a(v/c)2 determine the unknowns a and b.
v/c 0.0 0.2 0.4 0.6 0.9
l/l0 1.0 0.9798 0.9165 0.80 0.4359
C. Polynomial Fit
5.30 During the calibration of a thermocouple, the readings of emf and temperature
are as follows:
T ◦ C 50 100 150 200 250 300
emf mV 122.5 170.0 232.5 310.0 402.5 510.0
(a) Assuming the polynomial relation emf = A + BT + CT2 determine the
calibration constants A, B and C. (b) What are the units of A, B and C?
5.31 The lattice constants lc of solid argon as a function of temperature T are given
below. (a) Assuming that lc varies according to the polynomial relation lc =
C1 + C2T + C3T2 find the approximate values of C1, C2 and C3. (b) Identify the
units of C1, C2 and C3.
T ◦ K 0 20 40 60 80
lc ˚A 5.30 5.31 5.33 5.38 5.44
5.32 The isothermal compressibility (K) of water as a function of temperature (T) is
given below. Fit the data to the function K = C1 + C2T + C3T2 and state the
units of C’s.122 Curve Fitting
T ◦ C 10 30 50 70 90
K (GPa)−1 0.480 0.450 0.440 0.450 0.467
5.33 The magnetic rigidity of a particle is found to obey the relation Hr = A(T2 +
BT)1/2, where A and B are constants and T is the kinetic energy of the particle.
For the given data using a polynomial fit determine the values of A and B.
T units 20 30 40 50
Hr units 68399.357 101764.320 135125.84 168485.97
5.34 The number of arithmetic operations (N) involved in solving a system of simul￾taneous linear equations of order n is given below as a function of n. Assuming
the relation N = 
C1 + C2n + C3n2 + C4n3
/6 determine the values of C1, C2,
C3 and C4.
n 23 4 5 6 7
N 9 28 62 115 191 294
D. Trigonometric Polynomials
5.35 In the following some sets of values of x and y are given. Develop a computer
program to construct the best trigonometric polynomials for the given set of (x, y)
values.
(a) x 0 π/2 π 3π/2
y 0.0 0.5 0.0 −0.5 (b) x 0 π/2 π 3π/2
y 0.7 1.5 1.3 0.56
Interpolation and Extrapolation
6.1 Introduction
Suppose, the values of a function y = f(x) at a set of points x1, x2,...,xn+1 with x1 < x2 <
··· < xn+1 are given but the analytic expression for f(x) is not known. The f(xi)’s might
be obtained in an experiment. For example, one has measured resistivity R of a resistor
at the temperatures (T), say, 20◦C, 30◦C, 40◦C, 50◦C and 60◦C and wishes to know the
functional form of the dependence of R on T or to compute R for an arbitrary temperature.
To estimate f(x) for an arbitrary x using the given data set (xi, yi = f(xi)), i = 1, 2,...,n
one has to construct a suitable functional form of f(x). A most common functional form
is a polynomial. Rational function approximation is also useful in certain cases. When the
desired (target) x at which f(x) is to be determined lies inside the interval [x1, xn+1] then
the problem is called an interpolation. When the target x falls outside the interval [x1, xn+1]
it is an extrapolation. The order of the interpolation/extrapolation is n if the number of
points used is n + 1.
The interpolation and the extrapolation are utilized in the stock analysis to visualize
the near future trend in the stock market. Nowadays, interpolation is utilized in computer
graphics and data engineering to minimize the number of data sufficient to represent pat￾terns in photos and videos.
How are the interpolation or the extrapolation different from curve fitting? Curve fitting
is a smooth process in which the fitted curve need not pass through all the given n data
points (xi, yi). In contrast to curve fitting the interpolation function perfectly passes through
all the given data points. Further, in curve fitting the number of coefficients in the analytical
expression of y(x) is much less than the data points. For example, for a straight-line fit
y = ax + b or exponential fit y = be−ax, the number of coefficients is always two and is
independent of number of data points. On the other hand, in interpolation, the number of
coefficients in the analytical expression of y(x) is equal to the number of given data points.
An interpolation essentially consists of two stages. They are:
1. Construction of an interpolating function from the given set of data.
2. Calculation of value of the function at the desired point x from the constructed
function.
In this chapter, the following interpolation approximations are presented.
1. Newton polynomial interpolation
2. Lagrange polynomial interpolation
3. Vandermonde approximation
4. Rational function interpolation
For other types of interpolation such as Chebyshev, Hermite and Legendre polynomials
see the refs. [1–2]. Restrict ourselves to interpolation in one-dimension, that is, y = f(x).
DOI: 10.1201/9781032649931-6 123124 Interpolation and Extrapolation
For interpolation in two or more dimensions refer the ref. [3]. The interpolation methods
described in this chapter are also the methods for extrapolation.
6.2 Newton Interpolation Polynomial
Consider the following set of polynomials:
P1(x) = a1 + a2 (x − x1) ,
P2(x) = a1 + a2 (x − x1) + a3 (x − x1) (x − x2) ,
.
.
. (6.1)
Pn(x) = a1 + a2 (x − x1) + ··· + an+1 (x − x1) (x − x2)···(x − xn) .
The Pn(x) can be rewritten as
Pn(x) =
n

+1
i=1
ai
i
−1
j=1
(x − xj ) . (6.2)
The polynomial given by Eq. (6.2) is called a Newton polynomial of order n with n centres
or nodes x1, x2,...,xn. The xi’s need not be equally spaced. For a given n+ 1 data (xi, yi =
f(xi)) the polynomial Pn(x) can be constructed. The coefficients ai’s can be computed as
follows.
6.2.1 Linear Interpolation
Linear interpolation means one wish to construct the polynomial
P1(x) = a1 + a2 (x − x1) , (6.3)
where a1 and a2 are to be determined. To calculate a1 and a2 two sets of data (x1, f(x1))
and (x2, f(x2)) are sufficient. When x = x1 Eq. (6.3) gives
a1 = f (x1) . (6.4a)
When x = x2 one has
f (x2) = a1 + a2 (x2 − x1) = f (x1) + a2 (x2 − x1)
or
a2 = f (x2) − f (x1)
(x2 − x1) . (6.4b)
The truncation error E1(x) in the approximation is
E1(x) = f(x) − P1(x), (6.5)
where f(x) is the exact or true function and P1(x) is its polynomial approximation. To
obtain an expression for the truncation error consider
g(t) = f(t) − P1(t) − [f(x) − P1(x)] (t − a)(t − b)
(x − a)(x − b)
. (6.6)Newton Interpolation Polynomial 125
Assume that g(t) is zero at the two points t = a and t = b. g(t) is given by
g(t) = f(t) − 2 [f(x) − P1(x)]
(x − a)(x − b) . (6.7)
According to Rolle’s theorem (refer Section 1.6) if g(t) is a continuous function on some
interval [a, b], differentiable on [a, b] and g(a) = g(b) = 0 then there is at least one point c
inside [a, b] with g(c) = 0. Since g(t) given by Eq. (6.6) is 0 at t = a = x1 and t = b = x2
one can set g(c) = 0. Then, Eq. (6.7) with the replacement f(x) − P1(x) = E1(x) gives
E1(x) = 1
2 (x − x1) (x − x2) f(c), (6.8)
where c ∈ [x1, x2]. E1 is zero if f(x) is a polynomial of order less than 2. E1 can be calculated
only if the analytical expression for f(x) is known.
To obtain a bound for the truncation error denote the absolute maximum value of f(x)
on x ∈ [x1, x2] as f
m(x). Then,
|E1| ≤ 1
2
f
m(x) max|(x − x1) (x − x2)| , x ∈ [x1, x2] . (6.9)
The maximum value of u(x) = |(x − x1)(x − x2)| corresponds to the value of x(= xm) at
which u
(x) = 0. This gives xm = (x1 + x2)/2. Now,
|E1| ≤ 1
2



x1
2 +
x2
2 − x1
 x1
2 +
x2
2 − x2


 f
m(x)
≤
1
8 (x2 − x1)
2 f
m(x). (6.10)
6.2.2 Examples
Now, present two examples to illustrate the linear interpolation.
Example 1:
The values of ex at x = 0, 1 are 1 and 2.71828, respectively. Compute e0.61. Obtain a bound
on the truncation error.
Choosing x1 = 0 and x2 = 1 the values of a1 and a2 are calculated as
a1 = f (x1)=e0 = 1 ,
a2 = f (x2) − f (x1)
(x2 − x1) = 2.71828 − 1
1
= 1.71828 .
The linear polynomial approximation of ex is
P1(x) = a1 + a2x =1+1.71828x .
e0.61 is then evaluated as 2.04815 while the exact value is 1.84043.
For the given function f(x) is ex. Its maximum value in x ∈ [0, 1] is e1. Therefore,
|E| ≤ 1
8 (x2 − x1)
2 f
m(x) = 1
8
e1 = 0.3397852 .
Given the data set (x, f(x)) = (0, 1) and (1, 2.71828) how do you compute the value of x at
which f(x) is, say, 2?126 Interpolation and Extrapolation
Example 2:
The value of f(x)=ex is given below at four values of x. Calculate e0.61 using the linear
interpolation.
x 0.0 0.5 0.7 1.0
ex 1.0 1.64872 2.01375 2.71828
For the linear interpolation, only two data points are needed. One can use the nodes (0, 1)
or (0, 0.7) or (0, 0.5) or (0.5, 0.7). The target point x = 0.61 is close to the nodes (0.5, 0.7).
Therefore, use of these nodes would produce relatively a higher accurate result than the
other set of nodes. Using the nodes (0.5, 0.7) the values of a1 and a2 are computed as
a1 = 1.64872, a2 = 1.82520 .
Then,
ex = P1(x)=1.64872 + 1.82520(x − 0.5).
For x = 0.61
e0.61 = 1.849492
which is more accurate than the value obtained in the previous example using the nodes
(0, 1).
6.2.3 Calculation of Coefficients for Higher-Order Polynomials
Let us calculate the coefficients a1, a2 and a3 for three-points set (xi, f(xi)), i = 1, 2, 3.
Then, inspecting the sequence a1, a2, a3 one can write the other coefficients. For n + 1
points the order of the polynomial is n. Therefore, for 3 points set the Newton polynomial
is
P2(x) = a1 + a2 (x − x1) + a3 (x − x1) (x − x2) . (6.11)
Substitution of x = x1 in Eq. (6.11) gives
a1 = f (x1) . (6.12a)
Next, substituting x = x2 one obtains
a2 = f (x2) − f (x1)
(x2 − x1) . (6.12b)
Then, x = x3 in Eq. (6.11) gives
a3 = 1
(x3 − x1)

f (x3) − f (x2)
(x3 − x2) − f (x2) − f (x1)
(x2 − x1)

. (6.12c)
Derivation of Eq. (6.12c) is left as an exercise to the reader.
Example:
Estimate e0.61 using second-order Newton polynomial interpolation formula to the following
table of data.Newton Interpolation Polynomial 127
TABLE 6.1
Divided-difference table for the function f(x).
i xi f[xi] f[ , ] f[ , , ] ···
1 x1 f[x1]
2 x2 f[x2] f[x1, x2]
3 x3 f[x3] f[x2, x3] f[x1, x2, x3]
.
.
. .
.
. .
.
. .
.
. .
.
. .
.
.
x 0.0 0.5 0.7
ex 1.0 1.64872 2.01375
The coefficients a1, a2 and a3 in Eq. (6.11) are obtained as
a1 = f(x1) = f(0) = 1 ,
a2 = f(x2) − f(x1)
(x2 − x1) = f(0.5) − f(0)
0.5 − 0
= 1.29744 ,
a3 = 1
0.7 − 0

f(0.7) − f(0.5)
(0.7 − 0.5) − f(0.5) − f(0)
(0.5 − 0) 
= [1.82515 − 1.29744]/0.7
= 0.7538714.
Then,
P2(x)=1+1.29744x + 0.7538714x(x − 0.5).
For x = 0.61 the result is P2(0.61) = 1.84202.
In Eqs. (6.12) observe that a1 is simply f(x1); a2 and a3 are functions of differences of
f(xi)’s. In view of this, the coefficients ai’s can be re-expressed in terms of the so-called
first-order divided-differences as described below.
6.2.4 First-Order Divided-Differences
The divided-differences for a function f(x) are defined as follows:
f [xi] = f (xi) , (6.13a)
f [xi−1, xi] = f [xi] − f [xi−1]
(xi − xi−1) , (6.13b)
f [xi−2, xi−1, xi] = f [xi−1, xi] − f [xi−2, xi−1]
(xi − xi−2) (6.13c)
and so on. Let us call f [xi−1, xi] as first divided-differences, f [xi−2, xi−1, xi] as second
divided-differences and so on. Using f[ ]’s a divided-difference Table 6.1 can be constructed.
In terms of the above-defined divided-differences the coefficients a1, a2 and a3 given by
Eqs. (6.12) are rewritten as
a1 = f [x1] , a2 = f [x1, x2] , a3 = f [x1, x2, x3] . (6.14)128 Interpolation and Extrapolation
TABLE 6.2
Divided-difference table for four nodes.
i xi f[xi] f[ , ] f[ , , ] f[ ,,, ]
1 x1 f [x1]
2 x2 f [x2] f [x1, x2]
3 x3 f [x3] f [x2, x3] f [x1, x2, x3]
4 x4 f [x4] f [x3, x4] f [x2, x3, x4] f [x1, x2, x3, x4]
In general
ai = f [x1, x2,...xi] . (6.15)
The ai’s are simply the diagonal elements in Table 6.1. The nodes xi’s need not be equally
spaced. When the ai’s are equally spaced the polynomial Pn(x) is called Gregory–Newton
interpolation polynomial.
6.2.5 Error in the Approximation
If f(x) is the true function and Pn(x) is the Newton polynomial approximation then the
error term En(x) in f(x) = Pn(x) + En(x) is given by
En(x) = n

+1
i=1
(x − xi)

f(n+1)(c)
(n + 1)! , (6.16)
where c ∈ [x1, xn+1]. Calculation of En(x) is possible if an analytical expression for f(x) is
known.
6.2.6 Examples
The following two examples illustrate the use of the divided-difference table.
Example 1:
The values of f(x)=ex at four nodes (which are not equally spaced) with five decimal
accuracy are given below. Compute e0.61 by constructing a third-order Newton interpolating
polynomial by forming a divided-difference table.
x 0.0 0.5 0.7 1.0
ex 1.0 1.64872 2.01375 2.71828
The third-order Newton interpolating polynomial is given by
P3(x) = a1 + a2 (x − x1) + a3 (x − x1) (x − x2)
+ a4 (x − x1) (x − x2) (x − x3) . (6.17)
To avoid confusion or mistakes in calculating the entries in the difference table first write
the table in terms of f[ ]’s. For four nodes Table 6.2 is the corresponding divided-difference
table.Newton Interpolation Polynomial 129
TABLE 6.3
Divided-differences for function ex calculated based on 4 nodes.
i xi f [xi] f [ , ] f [ , , ] f [ ,,, ]
1 x1 1.00000
2 x2 1.64872 1.29744
3 x3 2.01375 1.82515 0.75387
4 x4 2.71828 2.34843 1.04657 0.29270
Now, replace each f[ ]’s by their numerical number. When computing the value of
f[xj ,...,xk] remember that the denominator is xk − xj . When calculating the values of
f[ ]’s in 5th and 6th columns one may make a mistake by writing
f [x1, x2, x3] = f [x2, x3] − f [x1, x2]
(x3 − x2) .
This is wrong. The denominator in f[ ]’s is always the difference between the values of last
x and first x in the arguments of f[ ]’s. The correct f[x1, x2, x3] is
f [x1, x2, x3] = f [x2, x3] − f [x1, x2]
(x3 − x1) .
Table 6.3 shows the divided-differences for the given function. From this table the coefficients
a1 to a4 are obtained as
a1 = 1, a2 = 1.29744, a3 = 0.75387, a4 = 0.29270.
The value of the polynomial at x = 0.61 is calculated as 1.84026 which is very close to the
exact value 1.84043. P(0.61) is calculated as a function of order n and the result is presented
in Table 6.4. The accuracy increases with increase in the order n. The percentage of relative
error is computed using
Percentage of relative error = 100|(ex − Pn(x)) /ex|.
TABLE 6.4
Calculated values of Pn(x) at x = 0.61 and percentage of relative error for various order n
of the polynomial (6.2) for ex. The nodes are equally spaced in the interval [0, 1]. For n = 2
the nodes are x1 = 0, x2 = 0.5 and x3 = 1; for n = 3 the nodes are x1 = 0, x2 = 1/3,
x3 = 2/3, x4 = 1 and so on.
Order n Pn(x) % of error Order n Pn(x) % of error
2 1.84792 0.417 5 1.84043 0.000
3 1.84017 0.014 6 1.84043 0.000
4 1.84041 0.001 7 1.84043 0.000130 Interpolation and Extrapolation
TABLE 6.5
Same as Table 6.4 but at the target value x = 1.1. The exact value of e1.1 is 3.00417.
Order n Pn(x) % of error Order n Pn(x) % of error
2 2.98269 0.715 5 3.00414 0.001
3 3.00130 0.096 6 3.00416 0.000
4 3.00386 0.010 7 3.00417 0.000
The constructed polynomial can be used to evaluate its value at a point falling outside
the interval [0, 1]. The value of P3(1.1) (extrapolated value) computed from Eq. (6.17) is
3.00201 while the exact value is 3.00417. Table 6.5 gives the extrapolated value of ex at
x = 1.1 as a function of order n.
Example 2:
From the table given below, form the difference table and determine the maximum degree
of the polynomial to which the data can be interpolated.
x 01 2 3 4 5 6
f(x) 2 3 10 29 66 127 218
Table 6.6 shows the divided-differences for the given data set. The divided-differences
f [ ,,, ] are all identical, that is, the 4th and the other higher divided-differences become
zero. Therefore, the coefficients a5, a6 and a7 are zero. For a 7-node data set the maximum
degree (or order) of the Newton polynomial is 6. However, for the given data set the maxi￾mum degree of the polynomial is 3. In general, if the nth divided-differences are all identical
then the degree of the polynomial to which a data set can be interpolated is n.
6.2.7 Other Uses of Newton Polynomials
Derivatives and integral values of an unknown function at a desired value of x lying in the
interval [x1, xn+1] can also be obtained from Eq. (6.2). For example, consider P2(x). Its
TABLE 6.6
Divided-differences for the given data set in Example 2.
i xi f [xi] f [ , ] f [ , , ] f [ ,,, ]
10 2
21 3 1
3 2 10 7 3
4 3 29 19 6 1
5 4 66 37 9 1
6 5 127 61 12 1
7 6 218 91 15 1Gregory–Newton Interpolation Polynomials 131
TABLE 6.7
Table 6.1 in terms of a two-dimensional array ai,j .
i xi f [xi] f [ , ] f [ , , ] ···
1 x1 a1,1
2 x2 a2,1 a2,2
3 x3 a3,1 a3,2 a3,3
.
.
. .
.
. .
.
. .
.
. .
.
. .
.
.
derivative is
P
2(x) = a2 + a3 [(x − x1)+(x − x2)] . (6.18)
In this way, derivative of Pn(x) can be constructed (see Problem 6.4 at the end of this
chapter).
6.2.8 Program Aspect to Construct Newton Polynomials
For developing a program to construct Newton polynomials the Table 6.1 can be rewritten
in terms of a two-dimensional array ai,j leading to Table 6.7.
Comparison of the Tables 6.1 and 6.7 gives
ai,1 = f [xi] , ai,2 = f [xi−1, xi] , (6.19a)
ai,3 = f [xi−2, xi−1, xi] (6.19b)
and so on. The above set of equations can be arranged in a simpler form as
ai,1 = yi = f [xi], i = 1, 2,...,n + 1 (6.20a)
ai.j = f [xi−j+1, xi−j+2,...,xi] , i = 2, 3,...,n + 1, j ≤ i. (6.20b)
Using Eqs. (6.13) aij ’s are given by
ai,1 = yi, i = 1, 2,...,n + 1 (6.21a)
ai,j = (ai,j−1 − ai−1,j−1)
xi − xi−j+1
, j = 2, 3,...,n + 1,
i = j, j + 1,...,n + 1. (6.21b)
Then, the Newton polynomial of order n is given by
Pn(x) = a1,1 + a2,2 (x − x1) + a3,3 (x − x1) (x − x2)
··· + an+1,n+1(x − x1)(x − x2)···(x − xn). (6.22)
6.3 Gregory–Newton Interpolation Polynomials
Suppose, the nodes are equally spaced: xi+1 = x1 + ih, i = 1, 2,...,n. In this case, it is
convenient to introduce forward- and backward-difference operators which will avoid division
in (6.13).132 Interpolation and Extrapolation
TABLE 6.8
Forward-difference table for a function f(x).
i xi f (xi) ∆f ∆2f ···
1 x1 f1
2 x2 f2 ∆f1
3 x3 f3 ∆f2 ∆2f1
.
.
. .
.
. .
.
. .
.
. .
.
. .
.
.
6.3.1 Gregory–Newton Forward-Difference Interpolation
Define the forward-difference operator ∆f(xi) as
∆f (xi)=∆fi = f (xi + h) − f (xi) = fi+1 − fi . (6.23)
In general, an nth order forward-difference operator ∆nfi is given by
∆nfi = n
k=0
(−1)k n!
k!(n − k)! fi+n−k . (6.24)
Then, the forward-difference Table 6.8 similar to Table 6.1 can be formulated. The ai’s
given by Eqs. (6.14) and (6.15) are written in terms of the forward-difference operators as
a1 = f1 ,
a2 = (f2 − f1)
(x2 − x1) = 1
h∆f1 ,
a3 = f3 − f2
2h2 − f2 − f1
2h2 = 1
2h2 (∆f2 − ∆f1) = 1
2h2 ∆2f1 ,
.
.
. (6.25)
an+1 = 1
n!hn ∆nf1 .
Now, the Newton polynomial Pn(x) given by Eq. (6.2) takes the form
Pn(x) = a1 + a2 (x − x1) + a3 (x − x1) (x − x2) + ···
··· + an (x − x1) (x − x2)···(x − xn−1)
= f1 + (x − x1)
h ∆f1 + (x − x1) (x − x2)
2!h2 ∆2f1 + ···
+(x − x1)···(x − xn)
n!hn ∆nf1. (6.26)
This polynomial is called Gregory–Newton forward-difference interpolation polynomial. The
error term in the approximation is given by Eq. (6.16).
6.3.2 Gregory–Newton Backward-Difference Interpolation
Like the forward-difference operator define the backward-difference operator ∇f(xi) as
∇f (xi) = ∇fi = f (xi) − f (xi − h). (6.27)Gregory–Newton Interpolation Polynomials 133
TABLE 6.9
Backward-difference table for the function f(x) whose values are specified at 4 nodes. Sim￾ilarly, a table for n nodes can be constructed.
i xi f (xi) ∇f ∇2f ∇3f
1 x1 f (x1) ∇f2 ∇2f3 ∇3f4
2 x2 f (x2) ∇f3 ∇2f4
3 x3 f (x3) ∇f4
4 x4 f (x4)
The nth order backward-difference operator ∇nfi is given by
∇nfi = n
k=0
(−1)n n!
k!(n − k)!fi−k . (6.28)
In this case, the backward-difference quantities form the Table 6.9. The Gregory–Newton
backward-difference interpolation polynomial with n + 1 data is then obtained as
Pn(x) = fn+1 +
1
h (x − xn+1) ∇fn+1
+
1
2!h2 (x − xn+1) (x − xn) ∇2fn+1
+ ···
+
1
n!hn (x − xn+1)···(x − x2) ∇nfn+1 . (6.29)
Example:
The value of f(x)=ex is given below at 4 values of x. Calculate e0.61 using the Gregory–
Newton forward- and backward-difference polynomials.
x 0.0 0.25 0.50 0.75
f(x) 1.0 1.28403 1.64872 2.117
The forward-difference Table 6.10 is constructed. This table gives
h = 0.25, n +1=4, f1 = 1, ∆f1 = 0.28403,
∆2f1 = 0.08066, ∆3f1 = 0.02293 .
Substitution of these values in the polynomial
P(x) = f1 +
1
h (x − x1) ∆f1 +
1
2!h2 (x − x1) (x − x2) ∆2f1
+
1
3!h3 (x − x1) (x − x2) (x − x3) ∆3f1
gives
P(x) = 1+1.13612x + 0.64528x(x − 0.25)
+0.24459x(x − 0.25)(x − 0.5),
P(0.61) = 1 + 0.69303 + 0.14170 + 0.00591 = 1.84064 .134 Interpolation and Extrapolation
TABLE 6.10
The forward-difference table for the given four-points data set for f(x)=ex.
i xi f ∆f ∆2f ∆3f
1 0.0 1.0
2 0.25 1.28403 0.28403
3 0.50 1.64872 0.36469 0.08066
4 0.75 2.11700 0.46828 0.10359 0.02293
Next, apply the Gregory–Newton backward-difference formula. In this case, Table 6.11
is obtained. For n + 1 = 4, that is, n = 3 the polynomial is
P(x) = f4 +
1
h (x − x4) ∇f4 +
1
2!h2 (x − x4) (x − x3) ∇2f4
+
1
3!h3 (x − x4) (x − x3) (x − x2) ∇3f4 . (6.30)
Table 6.11 gives
h = 0.25, f4 = 2.11700, ∇f4 = 0.46828,
∇2f4 = 0.10359, ∇3f4 = 0.2293 .
Substitution of these values in Eq. (6.30) gives
P(x)=2.117 + 1.87312(x − 0.75) + 0.82872(x − 0.75)(x − 0.5)
+ 0.24459(x − 0.75)(x − 0.5)(x − 0.25).
For x = 0.61
P(0.61) = 2.117 − 0.26224 − 0.01276 − 0.00135 = 1.84065.
6.4 Lagrange Interpolation Polynomials
Joseph Louis Lagrange introduced a method slightly different from the Newton polynomial
method to construct a polynomial passing through a given set of points. Instead of the form
TABLE 6.11
The backward-difference table for the given four-points data set for the function ex.
i xi f ∇f ∇2f ∇3f
1 0.0 1.0 0.28403 0.08066 0.02293
2 0.25 1.28403 0.36469 0.10359
3 0.50 1.64872 0.46828
4 0.75 2.11700Lagrange Interpolation Polynomials 135
given by Eq. (6.2) Lagrange assumed the polynomial of the form
P(x) = a1 + a2x + ··· + an+1xn =
n

+1
i=1
aixi−1. (6.31)
First, consider the method for the simplest case of two points.
6.4.1 Lagrange Linear Polynomial
For a set of two points, the polynomial to be constructed is a linear function
P1(x) = a1 + a2x. (6.32)
Note the difference between this P1(x) and the P1(x) given by Eq. (6.3). For x = x1 and
x = x2 Eq. (6.32) becomes
f (x1) = P1 (x1) = a1 + a2x1, (6.33a)
f (x2) = P1 (x2) = a1 + a2x2. (6.33b)
Solving Eqs. (6.33) for a1 and a2 gives
a1 = x2f (x1) − x1f (x2)
x2 − x1
, (6.34a)
a2 = f (x2) − f (x1)
x2 − x1
. (6.34b)
Substitution of the above expressions for a1 and a2 in Eq. (6.32) leads to the following
functional form for P1(x):
P1(x) = x2f (x1) − x1f (x2) + x [f (x2) − f (x1)]
(x2 − x1)
= f (x1) (x − x2)
(x1 − x2) + f (x2) (x − x1)
(x2 − x1)
. (6.35)
Defining
L1 = (x − x2)
(x1 − x2)
, L2 = (x − x1)
(x2 − x1) (6.36)
Eq. (6.35) is rewritten as
P1(x) = f (x1)L1(x) + f (x2)L2(x). (6.37)
The error term E1(x) is same as the one given for the Newton polynomial interpolation.
Example:
Compute the value of f(x)=ex at x = 0.61 using the data (x, f(x)) = (0, 1), (1, 2.71828).
The functions L1 and L2 are calculated as
L1(x = 0.61) = (x − x2)
(x1 − x2) = 0.61 − 1.0
0.0 − 1.0 = 0.39 .
L2(x = 0.61) = (x − x1)
(x2 − x1) = 0.61 − 0.0
1.0 − 0.0 = 0.61 .136 Interpolation and Extrapolation
Then,
P1(x = 0.61) = f (x1)L1(x) + f (x2)L2(x)
= 0.39 × 1+0.61 × 2.71828
= 2.04815.
6.4.2 Higher-Order Lagrange Polynomials
The Lagrange interpolation polynomials based on n + 1 points can be obtained in a way
described for two points. They are given by
f(x) = Pn(x) =
n

+1
i=1
f (xi)Li(x), (6.38a)
where
Li(x) =
n + 1

j = 1
j = i
 x − xj
xi − xj

, i = 1, 2,...,n + 1 . (6.38b)
In Eq. (6.38a) the term Li is made of product of n terms with each being of the form
(x − xj )/(xi − xj ). The factor (x − xj )/(xi − xi) is excluded in the product in Eq. (6.38b)
for obvious reason. The functions Li are called the Lagrange fundamental polynomials or
Lagrange coefficients polynomials. Each Li is a polynomial of order n. Pn(x) based on n+ 1
points is called nth order Lagrange polynomial. The error in the approximation is same as
the one given by Eq. (6.16) for the Newton polynomial of order n.
Example:
The values of f(x)=ex at three nodes with five decimal accuracy are given below. Compute
e0.61 and e1.1 by constructing the Lagrange polynomial P2(x).
x 0.0 0.50 1
f(x) 1.0 1.64872 2.71828
For a three-points data set, the Lagrange polynomial is
P2(x) = f (x1)L1(x) + f (x2)L2(x) + f (x3)L3(x).
Li’s in the above equation are computed as
L1(x = 0.61) = (x − x2) (x − x3)
(x1 − x2) (x1 − x3)
= 0.11 × −0.39
−0.5 × −1.0
= −0.08580.
L2(x = 0.61) = (x − x1) (x − x3)
(x2 − x1) (x2 − x3) = 0.95160.
L3(x = 0.61) = (x − x1) (x − x2)
(x3 − x1) (x3 − x2) = 0.1342.Lagrange Interpolation Polynomials 137
TABLE 6.12
The numerically computed value of e0.61 by the Lagrange interpolating polynomial approx￾imation.
Order Pn(x) % of Order Pn(x) % of
n value error n value error
2 1.84792 0.407 5 1.84043 0.000
3 1.84017 0.014 6 1.84043 0.000
4 1.84041 0.001 7 1.84043 0.000
For x = 0.61
P2(0.61) = 1 × −0.08580 + 1.64872 × 0.95160 + 2.71828 × 0.1342
= 1.84792.
The exact value is 1.84043. Accuracy can be improved by considering more points in the
interval [0, 1]. Table 6.12 displays the computed Pn(0.61) for ex as a function of order n.
The value of e1.1 computed with the given 3 data points is 2.98269 whereas the exact value
is 3.00417.
6.4.3 Interpolation with Large Number of Nodes
In the Newton and Lagrange polynomial interpolations the spacing between the successive
nodes in the interval [a, b] need not be the same. Suppose, the number of nodes is large, say,
greater than 50 and the nodes are equally spaced. The fitted polynomial perfectly passes
through all the nodes. However, for the x values between the nodes near the end points a
and b the interpolated values of the constructed polynomial may highly deviate from the
true values and the plot of x versus Pn(x) will show wiggles. In such a case if the choice of
the nodes is free then it is better to choose more points near the end points a and b. Now,
the spacings between the successive nodes are not the same. The Pn(x) obtained with a
large value of n does not produce wiggles [4]. One choice for unequally spaced nodes with
clustering near the end points a and b is the Chebyshev points. For the interval [−1, 1] these
points are given by
xi = − cos (i − 1)π
n

, i = 1, 2,...,n + 1. (6.39)
Here, x1 = −1 and xn+1 = 1. For the interval [a, b] the Chebyshev points xi given by
Eq. (6.39) can be rescaled as
xi = 1
2 (xi + 1) (b − a) + a . (6.40)
Let us illustrate the above with an example in the case of Lagrange interpolation.
Consider the function
f(x) = 1
1 + √1 − x2 , x ∈ [0, 1]. (6.41)
In Fig. 6.1a continuous line represents the exact f(x) given by Eq. (6.41). The filled circles
are the nodes (n+1 = 11) chosen for the Lagrange interpolation. The order of the polynomial138 Interpolation and Extrapolation
P10(x)
f(x)
(a)
x
f(x), P10(x)
0 0.25 0.5 0.75 1
1
0.8
0.6
(b)
x
P65(x) 0 0.25 0.5 0.75 1
8
4
0
(c)
x
f(x)
0 0.25 0.5 0.75 1
1
0.8
0.6 E
P65(x)
(d)
x
P65(x), E
0 0.25 0.5 0.75 1
1
0.8
0.6
0.4
FIGURE 6.1
(a) x versus f(x) (continuous line) and x versus P10(x). The 11 nodes used to construct
the Lagrange polynomial P10(x) are the filled circles. (b) x versus P65(x) obtained from
uniformly spaced 66 nodes. (c) x versus f(x) (continuous line) and the nonuniformly spaced
Chebyshev points. (d) Variation of P65(x) constructed with the Chebyshev nodes. E is the
percentage of relative error in P65(x). For convenience E in this subplot is shifted by 0.4.
obtained from these 11 nodes is n = 10. The Lagrange polynomial P10(x) is constructed. The
numerical values of P10(x) are computed for m = 501 target values of x with xi = (i−1)∆x,
i = 1, 2,...,m and ∆x = 1/(m−1). P10(x) is shown in Fig. 6.1a as the dotted line. A smooth
variation of P10(x) is clearly seen near the end points.
Next, P65(x) is constructed with equally spaced 66 nodes. Then, P65(x) is calculated for
m = 501 target values of x. The variation of the obtained P65(x) is plotted in Fig. 6.1b,
where the wiggles are clearly seen near the end points x = 0 and x = 1. To eliminate the
occurrence of wiggles in the constructed Lagrange polynomial, now, choose the 66 nodes as
the Chebyshev points using Eqs. (6.39) and (6.40). For the function f(x) given by Eq. (6.41)
the generated nodes are shown in Fig. 6.1c as filled circles and f(x) by the continuous line.
The spacings between the successive nodes are not the same and moreover the nodes are
clustered near the end points of the interval [0, 1]. Using these 66 nodes P65(x) is constructed.
Figure 6.1d presents x versus P65(x) with 501 target values of x. The obtained polynomial
is now a smooth curve between all the nodes without wiggles. The percentage of relative
errors E(x) shown in Fig. 6.1d is less than 0.2.Calculation of Polynomial Coefficients 139
6.5 Calculation of Polynomial Coefficients
It is possible to construct an interpolating polynomial of the form
y(x) = c1 + c2x + c3x2 + ··· + cnxn−1 (6.42)
from the given n data points (xi, yi). Substituting x = x1, x2,...,xn and y = y1, y2,...,yn
in Eq. (6.42) generate a system of n linear equations for the coefficients ci’s which in matrix
form is given by


1 x1 x2
1 ··· xn−1
1
1 x2 x2
2 ··· xn−1
2
.
.
. .
.
. .
.
. .
.
. .
.
.
1 xn x2
n ··· xn−1 n




c1
c2
.
.
.
cn

 =


y1
y2
.
.
.
yn

 . (6.43)
The first (n × n) matrix in the left-side of Eq. (6.43) is known as Vandermonde matrix (see
Section 4.11). The system (6.43) can be solved by the Gauss–Jordan method. However, for
Vandermonde matrix system a special method is available. For details see Section 4.11 and
also the ref. [3].
The method is tested for the function y = ex using the data (x, y) = (0, 1),
(0.15, 1.16183), (0.5, 1.64872), (0.7, 2.013750), (1, 2.71828). The polynomial constructed is
P(x)=1+0.99929x + 0.50743x2 + 0.14396x3 + 0.06761x4 . (6.44)
Don’t compare the polynomial (6.44) with the Taylor series of ex. The value of P(0.61) is
calculated as 1.84041 while the exact value is e0.61 = 1.84043.
6.6 Cubic Spline Interpolation
Another interpolation method of interest is the cubic spline interpolation. It uses poly￾nomial of degree 3 to connect points. This method assumes that for a given data set
(x0, f(x0)),...,(xn, f(xn)) the second derivatives f(x0) and f(xn) are 0. The remain￾ing second derivatives are obtained by solving the set of equations
(xi − xi−1) f (xi−1)+2(xi+1 − xi−1) f (xi)+(xi+1 − xi) f (xi+1)
= 6
(xi+1 − xi)
[f (xi+1) − f (xi)]
+
6
(xi − xi−1)
[f (xi−1) − f (xi)] , i = 1, 2,...,n − 1. (6.45)
Then, the cubic function for each interval is given by
fi(x) = f (xi−1)
6 (xi − xi−1)
(xi − x)
3 + f (xi)
6 (xi − xi−1)
(x − xi−1)
3
+




f (xi−1)
(xi − xi−1) − 1
6
f (xi−1) (xi − xi−1)




(xi − x)
+




f (xi)
(xi − xi−1) − 1
6
f (xi) (xi − xi−1)




(x − xi−1) . (6.46)140 Interpolation and Extrapolation
6.7 Rational Function Approximation
In the early sections, polynomial approximations of continuous functions are described. A
polynomial approximation is not always the best approximation. For functions behaving
as rational functions (that is, quotient of two polynomials), assuming finite values for very
large values of x and changing rapidly in certain small regions like poles the polynomial
approximation gives poor results. In these cases, rational function approximation is of useful.
In this section, a rational function approximation, namely, Pad´e approximation is described.
6.7.1 Rational Function
A rational function is a quotient of two polynomials and is written as
Rn,m(x) = Pn(x)
Qm(x) =
n
i=0 aixi
m
i=0 bixi . (6.47)
The numerator and denominator are polynomials of order n and m, respectively. Functions
such as ex, log(1 + x), sin x, etc. can be approximated as rational functions. For example,
ex can be approximated as
ex ≈ 1 + x
2
1 − x
2
, 1 + x
2 + x2
12
1 − x
2 + x2
12
, 1 + x
2 + x2
12 + x3
120
1 − x
2 + x2
12 − x3
120
, ... . (6.48)
Consider a kind of rational function called Pad´e approximant. It is a rational function
which can also be expressed as the power series expansion
f(x) = ∞
i=0
cixi
, R(0) = f(0) (6.49)
and
di
dxi R(x)|x=0 = di
dxi f(x)|x=0 , i = 0, 1, 2,...,n + m. (6.50)
6.7.2 Analytical Construction of Rational Functions
The problem is to construct an analytical expression for Rn,m(x) using the given analytic
expression f(x). For example, it is desired to find Rn,m(x) for, say, ex or sin x. For this
purpose, consider the equation
f(x) − Rn,m(x)=0 . (6.51)
Use of Eqs. (6.47) and (6.49) in Eq. (6.51) gives
m
i=0
bixi
 ∞
i=0
cixi

−n
i=0
aixi = 0. (6.52)
ci’s are obtained from the power series approximation of f(x) given by Eq. (6.49). Now, ai’s
and bi’s are to be determined. Equating the coefficients of various powers of xi in Eq. (6.52)
one gets m+n+1 linear systems of equations for the unknowns ai’s and bi’s. These equationsRational Function Approximation 141
are given below, where b0 = 1:
c0 − a0 = 0,
b1c0 + c1 − a1 = 0,
b2c0 + b1c1 + c2 − a2 = 0,
.
.
. (6.53)
bmcn−m + bm−1cn−m+1 + ··· + cn − an = 0
and
bmcn−m+1 + bm−1cn−m+2 + ··· + b1cn + cn+1 = 0,
.
.
. (6.54)
bmcn + bm−1cn+1 + ··· + b1cn+m−1 + cn+m = 0.
Equations (6.54) involve bi’s and ci’s only and hence they can be solved first. Then,
Eqs. (6.53) can be used successively to compute ai’s. For the special case of m = n
Eqs. (6.53) and (6.54) take the form [3]
a0 = c0, b0 = 1, n
i=1
bicn−i+j = −cn+j , j = 1, 2,...,n (6.55a)

j
i=0
bicj−i = aj , j = 1, 2, . . . , n. (6.55b)
Example:
Construct the Pad´e approximations R1,1, R2,2, R3,3 and R4,4 for ex.
The series expansion of ex is
ex = ∞
i=0
1
i!
xi = ∞
i=0
cixi . (6.56)
Here ci = 1/i!. First few ci’s are c0 = 1, c1 = 1, c2 = 1/2, c3 = 1/6 and c4 = 1/24. For R1,1
the values of m and n are 1. Now, Eqs. (6.55) become
a0 = c0, b0 = 1, b1c1 = −c2, b0c1 + b1c0 = a1 . (6.57)
Substituting the values of c’s, b0 = 1 and a0 = c0 = 1 the above equations become b1 =
−1/2, 1 + b1 = a1. That is,
a0 = 1, b0 = 1, b1 = −1/2, a1 = 1/2 . (6.58)
The approximation R1,1 is then
R1,1 = a0 + a1x
b0 + b1x = 1 + 1
2x
1 − 1
2x . (6.59)
For R2,2, m = n = 2 and Eqs. (6.55) are
a0 = c0, b0 = 1, b1c2 + b2c1 = −c3, b1c3 + b2c2 = −c4, (6.60a)
b0c1 + b1c0 = a1, b0c2 + b1c1 + b2c0 = a2. (6.60b)142 Interpolation and Extrapolation
TABLE 6.13
Comparison of the exact value of ex with the rational function approximations R1,1, R2,2,
R3,3, R4,4 at several values of x.
x ex R1,1 R2,2 R3,3 R4,4
−0.5 0.60653 0.60000 0.60656 0.60653 0.60653
−0.4 0.67032 0.66667 0.67033 0.67032 0.67032
−0.3 0.74082 0.73193 0.74082 0.74082 0.74082
−0.2 0.81873 0.81818 0.81873 0.81873 0.81873
−0.1 0.90484 0.90476 0.90484 0.90484 0.90484
0.0 1.00000 1.00000 1.00000 1.00000 1.00000
0.1 1.10517 1.10526 1.10517 1.10517 1.10517
0.2 1.22140 1.22222 1.22140 1.22140 1.22140
0.3 1.34986 1.35294 1.34985 1.34986 1.34986
0.4 1.49182 1.50000 1.49180 1.49182 1.49182
0.5 1.64872 1.66667 1.64865 1.64872 1.64872
Substituting the values of c’s the above set of equations become
a0 = 1, b0 = 1 , b1
2 + b2 = −1
6 , b1
6 +
b2
2 = − 1
24 , (6.61a)
1 + b1 = a1 , 1
2 + b1 + b2 = a2. (6.61b)
The solution of the above system of equations is
a0 = 1, b0 = 1, a1 = 1
2
, a2 = 1
12, b1 = −1
2
, b2 = 1
12 . (6.62)
Then,
R2,2 = a0 + a1x + a2x2
b0 + b1x + b2x2 = 1 + 1
2x + 1
12x2
1 − 1
2x + 1
12x2 . (6.63)
Proceeding further with m = n = 3, 4 one obtains
R3,3 = 1 + 1
2x + 1
10x2 + 1
120x3
1 − 1
2x + 1
10x2 − 1
120x3 , (6.64)
R4,4 = 1 + 1
2x + 3
28x2 + 1
84x3 + 1
1680x4
1 − 1
2x + 3
28x2 − 1
84x3 + 1
1680x4 . (6.65)
Table 6.13 displays the values of ex, R1,1, R2,2, R3,3 and R4,4 for several values of x.
Successive approximations converge to the exact value of ex.
6.7.3 Rational Function Interpolation
Stoer and Bulirsch [5] described an algorithm to compute the value of a function using the
given data set (xi, yi) by rational function approximation. The algorithm is the following.Rational Function Approximation 143
TABLE 6.14
Illustration of construction of various rational functions R’s.
xi yi R’s
x1 y1 = R1
x2 y2 = R2 R1,2
x3 y3 = R3 R2,3 R1,2,3
x4 y4 = R4 R3,4 R2,3,4 R1,2,3,4
x5 y5 = R5 R4,5 R3,4,5 R2,3,4,5
.
.
. .
.
. .
.
. .
.
. .
.
. .
.
.
Suppose (xi, yi = f(xi)) are n given data points. Let us denote Ri be the value of the
function f(x) at xi, that is, Ri = f(xi). Then, define R1,2 as the value of f at x of a rational
function approximation, passing through both (x1, y1) and (x2, y2). Similarly, R2,3, R3,4,
..., R1,2,3, R2,3,4, ... can be defined. In this way, one can construct Ri(i+1)...(i+m). The R’s
form the Table 6.14.
The recurrence relation for R’s is given by [3]
Ri,(i+1),··· ,(i+m) = R(i+1),··· ,(i+m)
+
R(i+1),··· ,(i+m) − Ri,··· ,(i+m−1)
 x−xi
x−xi+m
 1 − R(i+1),··· ,(i+m)−Ri,··· ,(i+m−1)
R(i+1),··· ,(i+m)−R(i+1),··· ,(i+m−1) 
− 1
. (6.66)
The recurrence relation starts with Ri = yi. Now, define
Cm,i = Ri,··· ,(i+m) − Ri,··· ,(i+m−1), (6.67a)
Dm,i = Ri,··· ,(i+m) − R(i+1),··· ,(i+m) (6.67b)
which satisfies the relation
Cm+1,i − Dm+1,i = Cm,i+1 − Dm,i. (6.68)
Using Eqs. (6.67) and (6.68) one can obtain the recurrence relations
Dm+1,i = Cm,i+1 (Cm,i+1 − Dm,i)
 x−xi
x−xi+m+1 
Dm,i − Cm,i+1
, (6.69a)
Cm+1,i =
 x−xi
x−xi+m+1 
Dm,i (Cm,i+1 − Dm,i)
 x−xi
x−xi+m+1 
Dm,i − Cm,i+1
. (6.69b)
At each level m, the C’s and D’s are the corrections which make the interpolation one
order higher. For n data points R1,··· ,n is the final approximation. It is the sum of any yi and
a set of C’s and/or D’s. The value of f(0.61) = e0.61 is computed employing the rational
function approximation. The five data points (x, y) = (0, 1), (0.15, 1.16183), (0.5, 1.64872),
(0.7, 2.01375), (1.0, 2.71828) are used. The result obtained is f(0.61) = 1.84043. For x =
1.1 the numerically computed value of f(1.1) is 3.00407 while the exact value is 3.00417.
Tabulate the numerical values of e0.61 obtained by rational function approximation as a
function of order n and also the percentage of relative error.144 Interpolation and Extrapolation
6.8 Concluding Remarks
If a given set of data displays a significant trend in the variation then the value of the data
at points that are missing in the set can be computed either by an appropriate curve fitting
or by an interpolation technique. Curve fitting needs a good choice of the mathematical
function relating the variables involved. In contrast to this, in the case of interpolation, one
may able to construct, for example, a polynomial passing through all the given data points.
In the case of constructing an interpolation polynomial if the nodes are equally spaced
then one can construct Gregory–Newton forward or backward interpolating polynomials
and then evaluate the value of the function at a specified node value. For unequally spaced
nodes the Newton interpolation polynomial or the Lagrange interpolation polynomial can
be used to compute the value of the function at a desired node value. The Newton and the
Lagrange interpolation can also be applied to equally spaced nodes. The problem one may
encounter when the target points are near the end nodes is pointed out in Section 6.4.3.
How to overcome this problem is also illustrated with an example.
6.9 Bibliography
[1] J.H. Mathews, Numerical Methods for Mathematics, Science and Engineering.
Prentice–Hall, New Delhi, 1998.
[2] M.K. Jain, R.K. Iyengar and R.K. Jain, Numerical Methods for Scientific and
Engineering Computation. New Age International Pvt Ltd, New Delhi, 1993.
[3] W.H. Press, S.A. Teukolsky, W.T. Vetterling, B.P. Flannery, Numerical Recipes
in Fortran. Foundation Books, New Delhi, 1993. Indian edition.
[4] A. Gezerlis and M. Williams, Am. J. Phys. 89:51, 2021.
[5] J. Stoer and R. Bulirsch, Introduction to Numerical Analysis, Springer, New York,
1980.
6.10 Problems
A. Newton and Gregory–Newton Polynomial Interpolations
6.1 The values of some functions f(x) at two values of x are given below. For these
functions
i) calculate f(x) at the specified target points using Newton linear interpolation
polynomial,
ii) compare the interpolated value with the exact value by calculating percentage
of relative error and
iii) calculate bound on truncation error.
a) f(x)=ex : (x, f(x)) = (−1, 0.36788), (−2, 0.13535), target x = −1.5.
b) f(x)=1/(1 + x): (x, f(x)) = (2.0.33333), (3, 0.25), target x = 2.5.Problems 145
c) f(x) = sin x : (x, f(x)) = (1.57080, 1), (2.35619, 0.70711) target x = 2.
6.2 The values of some functions f(x) at four values of x are given below. For these
functions
i) construct the divided-difference table,
ii) construct the third-order Newton polynomial, compute the values of the func￾tion at the target points and
iii) compare the numerical approximation with the exact value by calculating
percentage of relative error.
a) f(x)=ex, target points x = −1.5 and −3.1.
x 0 −1 −1.75 −3.0
f(x) 1 0.36788 0.17377 0.04979
b) f(x) = ln(1 + x), target points x = 0.3 and −0.1.
x 0 0.25 0.5 0.75
f(x) 0 0.22314 0.40547 0.55962
c) f(x) = sin x, target points x = 2 and −0.2.
x 0 0.78540 1.57080 2.35619
f(x) 0 0.70711 1.00000 0.70711
6.3 In Table 6.7 note that if all the elements in (i + m)th column are zero then the
elements in the next successive columns are also zero. Therefore, if |ai,m| < δ,
i = m, m + 1, ··· , n + 1 where δ is a preassumed small positive value then one
need not calculate the remaining elements and jump to calculate the polynomial
value at the target x. In this case, the Newton polynomial is of order m − 1.
Develop a Python program implementing this aspect in the Newton polynomial
interpolation.
6.4 Write the derivative of P3(x). For the functions given in Problem 6.2 compute
P
3(x) at the given target values. Compare the numerical value with the exact
value.
6.5 Calculate the step size h to be used in the tabulation of f(x)=ex in the interval
[0, 1] so that the linear interpolation will be correct to 5 decimal places.
6.6 Show that the error in the linear interpolation for the case of equispaced tabular
data does not exceed 1/8 of the second difference.
For Problems 6.7–6.11
i) construct the divided-difference table,
ii) construct the third-order Newton polynomial, compute the value of the variable
at the target point and
iii) construct the Gregory–Newton forward- and backward-difference tables and then
compute the value of the variable at the target points.
6.7 The measured value of current I in an electronic circuit as a function of applied
voltage V is given below. It is required to know the current for the applied voltage
1.65 V.146 Interpolation and Extrapolation
V in volts 1.0 1.5 2.0 2.5
I in amperes 0.1 0.15 0.2 0.25
6.8 The distance of a particle measured at various time is given below. Calculate the
distance of the particle at t = 0.9 minute.
Time in minutes 0 0.5 1.0 1.5
Distance in meter 0 2.2 4.0 5.9
6.9 The volumes V of a gas measured at various temperatures T are listed below.
Calculate the volume at T = 100oC.
To C 90 110 130 150
V in m3 2.108 2.13 2.157 2.180
6.10 The probabilities P of finding a particle with energy E = 0.5, 0.7, 0.9 and 1.1
units are 0.184, 0.123, 0.083, 0.052, respectively. Compute P for E = 0.6.
6.11 The mass of radioactive decaying sample 222Rn measured at few times (in units
of days) is given below. Compute the mass at the end of 12 days.
Time in days 0 5 10 15
Mass in mg 1 0.42 0.17 0.08
6.12 The values of some functions f(x) at four node values are given below.
i) Construct the Gregory–Newton forward-difference interpolation polynomial.
Then, compute the values of the function at the specified node values.
ii) Construct the Gregory–Newton backward-difference interpolation polyno￾mial. Then, compute the values of the function at the target points.
a) f(x)=e−x, target points x = 1.5 and 3.1.
x 01 2 3
f(x) 1 0.36788 0.13535 0.04979
b) f(x)=1/(1 + x), target points x = 2.5 and −0.1.
x 01 2 3
f(x) 1 0.5 0.33333 0.25
c) f(x) = (sin x)/x, target points x = 0, and 0.8.
x −0.1 0.1 0.3 0.5
f(x) 0.99833 0.99833 0.98507 0.95885
B. Lagrange Interpolation Polynomials
6.13 Apply the Lagrange interpolation for Problems 6.7–6.11.
C. Analytical Construction of Rational Function Rn,n(x)
6.14 Verify that for the function 1/(1 − x) with 1
1−x =1+ x + x2 + ··· the rational
approximation R1,1 is also 1/(1 − x).Problems 147
6.15 The series expansion of sin x is given by sin x = x − x3
3! + x5
5! −··· . Show that
R1,1 = x, R2,2 = x
1 − x2
6

and R3,3 = 
x − 13
60x3 1 − x2
20
. Tabulate the
values of sin x, R1,1, R2,2 and R3,3 for x = 0.1, 0.2, 0.3, 0.4, 0.5.
6.16 The Maclaurin series for (tan−1 √x)/
√x is given by
tan−1 √x
√x
= 1 − x
3 +
x2
5 − x3
7 + ··· .
Show that
R1,1 =

1 +
4
15
x
1 + 3
5
x

,
R1,1 =

1 + 7
9
x +
64
945x2
1 + 10
9
x +
5
21
x2

.
6.17 Show that for cos x
R2,2 =

1 − 5
12
x2
1 +
1
12
x2

.
D. Rational Function Interpolation/Extrapolation
6.18 For the data set given below compute the values of the function at x = −1.5 and
−3.1 by rational function approximation.
x 0 −1 −1.75 −3.0
f(x) 1 0.36788 0.17377 0.04979
6.19 For the data set given below compute the values of the function at x = 0.3 and
−0.1 by rational function approximation.
x 0 0.25 0.5 0.75
f(x) 0 0.22314 0.40547 0.55962
6.20 For the data set given below compute the values of the function at x = 2 and
−0.2 by rational function approximation.
x 0 0.78540 1.57080 2.35619
f(x) 0 0.70711 1.00000 0.707117
Eigenvalues and Eigenvectors
7.1 Introduction
Consider a system of the form
AX = λX , (7.1)
where A is a n × n square matrix, λ is a scalar and X = (x1, x2,...,xn)T. The solution
X of the system (7.1) is called an eigenvector of the matrix A and λ is the corresponding
eigenvalue. Any multiple of an eigenvector X will also be an eigenvector. λ and X are called
eigenpair of A. Equation (7.1) can also be written as
(A − λI)X = 0 (7.2)
which implies that the product of the matrix (A−λI) and the nonzero vector X is the zero
vector. System (7.2) will have a nontrivial solution (that is, at least one xi is nonzero) if
and only if
det(A − λI) =









a11 − λ a12 ··· a1n
a21 a22 − λ ··· a2n
···
an1 an2 ··· ann − λ









= 0 . (7.3)
When the above determinant is expanded one gets the following polynomial of degree n:
P(λ)=(−1)n 
λn + b1λn−1 + ··· + bn−1λ + bn

. (7.4)
This equation is called characteristic equation of the matrix A and it has n roots. These
roots when substituted in Eq. (7.2) give the corresponding nontrivial solution X, that is,
the eigenvector X. The system (7.1) has nontrivial solutions X for certain values of λ only
and are the roots of Eq. (7.4). These certain values of λ are called eigenvalues. Eigen is a
German word which means certain.
Equation (7.4) suggests that roots of a polynomial of degree n can be viewed as eigen￾values of a matrix. For example, the companion matrix C of the equation
f(λ) = λ3 + a1λ2 + a2λ + a3 = 0 (7.5)
is
C =


010
001
−a3 −a2 −a1

 . (7.6)
The eigenvalues of C are the roots of Eq. (7.5). Therefore, the roots of the equation
λn + a1λn−1 + ··· + an−1λ + an = 0 (7.7)
DOI: 10.1201/9781032649931-7 148Some Basic Properties of Eigenvalues and Eigenvectors 149
are the eigenvalues of the matrix
C =


01 0 ··· 0
00 1 ··· 0
00 0 ··· 0
···
00 0 ··· 1
−an −an−1 −an−2 ··· −a1


. (7.8)
Simple algorithms are not available to find all eigenpairs of arbitrary n × n matrices.
However, efficient and easily understandable methods are developed for some special cases.
Depending upon the nature of the given matrix and the requirement one can choose a suit￾able method. For example, power method is useful to compute the absolute largest eigen￾value and the corresponding eigenvector of an arbitrary matrix. For a symmetric matrix,
Jacobi method enables us to calculate all the eigenpairs. For a symmetric tridiagonal ma￾trix QL algorithm is efficient to determine all the eigenvalues. The method of Rutishauser
is of useful for arbitrary matrices of small order. In this chapter, the power, Jacobi, QL
and Rutishauser methods are described. To start with, in the next section some of the ba￾sic properties of eigenvalues and eigenvectors are summarized which are essential for our
discussion.
7.2 Some Basic Properties of Eigenvalues and Eigenvectors
In this section, some basic properties of eigenvalues and eigenvectors of square matrices are
given without proof. For details see the refs. [1-2].
1. For a real square matrix A the eigenvalues are the roots of the characteristic
polynomial P(λ) = det(A − λI). The eigenvalues can be real or complex.
2. If λ is an eigenvalue of a matrix A then the associated eigenvector X is defined
through the relation AX = λX.
3. For each distinct eigenvalue λ there exists at least one eigenvector.
4. The largest eigenvalue in modulus of a matrix cannot exceed the largest sum of
the moduli of the elements along any row or column.
5. An eigenvalue whose absolute value is greater than any other eigenvalue is called
the dominant eigenvalue. The associated eigenvector is called the dominant eigen￾vector .
6. If the largest magnitude of components of an eigenvector is unity then the eigen￾vector is said to be normalized. An eigenvector X = (x1, x2,...,xn)T can be
normalized by constructing a new vector V = (x1, x2, ..., xn)/β, where β = xj
with xj = maximum of { |x1|, |x2|, ..., |xn| }.
7. If (λ, X) is an eigenpair of a matrix A then (λ − α, X) is an eigenpair of the
matrix A − αI, where α is a constant.
8. If (λ = 0, X) is an eigenpair of a matrix A then an eigenpair of A−1 is (1/λ, X).
9. If (λ = 0, X) is an eigenpair of a matrix A then (1/(λ − α), X) with α = λ is an
eigenpair of the matrix (A−αI)−1. The eigenvalues of diagonal, upper-triangular
and lower-triangular matrices are simply their diagonal elements.150 Eigenvalues and Eigenvectors
10. A matrix A is said to be symmetric if AT = A. The eigenvalues of a real symmetric
matrix are all real numbers. Eigenvectors corresponding to distinct eigenvectors
of a symmetric matrix are orthogonal, that is, xi · xj = 0, for i = j.
7.3 Applications of Eigenvalues and Eigenvectors
Eigenvalues and eigenvectors have applications in the field of vibration analysis, electric
circuits, control theory, linear stability analysis and quantum mechanics. Let us enumerate
some of the notable applications of them [3-8].
1. Eigenvalues arise in the construction of the solution of linear ordinary differential
equations with constant coefficients.
2. The stability of an equilibrium point of a dynamical system depends on the nature
of the eigenvalues of a square matrix involved in the linear stability analysis.
3. In the vibrational analysis eigenvalues (that is, the eigenfrequencies) are the al￾lowed frequencies of vibrations and the corresponding eigenmodes of vibrations
are the allowed possible meaningful bounded solutions.
4. In mechanics, the study of rotation of a rigid body about its centre of mass in￾volves the tensor of moment of inertia. The principal axes are defined by the
eigenvalues of the moment of inertia tensor. The stress tensor can be decom￾posed into a diagonal tensor by considering the eigenvectors as a basis and the
eigenvalues on the diagonal.
5. In quantum mechanics, eigenvalues are the allowed values of certain experimen￾tally measurable quantities such as energy, orbital angular momentum and spin
and the eigenvectors (eigenfunctions) represent the allowed states.
6. Eigenvalues and the eigenvalue analysis are employed in oil firm to identify the
location of an oil reserve.
7. A theoretical upper limit of information that can be transmitted in a communi￾cation channel can be obtained using the eigenvalues involved in the concerned
problem.
8. In linear transformations, the building blocks are the eigenvalues and the eigen￾vectors. They denote the scaling factor and the direction of the transformation
applied.
9. Eigenvalues and the eigenvectors have applications in image compression, dimen￾sionality reduction in machine learning and in the Google page rank algorithm.
10. The facial recognition schemes like the eigenfaces make use of the eigenvalues and
the eigenvectors.
7.4 The Power Method
To determine the dominant eigenvalue and the associated normalised eigenvector of a square
matrix of order n there is a simple method called power method. It is an iterative method.
The iteration rule is obtained as follows.The Power Method 151
Let X0 is the initial guess of the exact dominant eigenvector V1 corresponding to the
dominant eigenvalue λ1, where λ’s are ordered as |λ1| > |λ2| >...> |λn|. X0 is written as
X0 = n
i=1
biVi. (7.9)
Assume that X1 is the next approximation of V1 and α1 is the corresponding approximation
to the associated eigenvalue. From the eigenvalue equation one may write
AX0 = α1X1 + δX1. (7.10)
Neglecting δX1, from Eq. (7.10) X1 is written as
X1 = 1
α1
AX0 = 1
α1
(b1AV1 + b2AV2 + ··· + bnAVn). (7.11)
Because AVi = λiVi, i = 1, 2,...,n
X1 = λ1
α1

b1V1 + b2
λ2
λ1
V2 + ··· + bn
λn
λ1
Vn

. (7.12)
X1 is normalised because of the term α1 in the denominator in Eq. (7.12). Repeating the
above process the kth approximation is obtained as
Xk = λk
1
α1α2 ··· αk

b1V1 + b2
λ2
λ1
k
V2 + ··· + bn
λn
λ1
k
Vn

. (7.13)
Xk is again normalized. Because λ1 is the dominant eigenvalue all the terms except the first
term in the right-side of Eq. (7.13) vanish so that
lim
k→∞ Xk = lim
k→∞
b1λk
1
α1 ··· αk
V1 . (7.14)
For normalized Xk and V1 their largest component will have the value 1. Therefore,
lim
k→∞
b1λk
1
α1 ··· αk
= 1 . (7.15)
Then, Eq. (7.14) becomes
lim
k→∞ Xk = V1 . (7.16)
Next, from Eq. (7.15)
lim
k→∞
b1λk−1
1
α1 ··· αk−1
= 1 . (7.17)
Dividing Eq. (7.15) by Eq. (7.17) gives
lim
k→∞ αk = λ1 . (7.18)
From the above analysis, the iterative formula for the dominant eigenvector is written
as
Xk+1 = 1
αk+1
AXk , (7.19)
where αk+1 is the absolute largest coordinate of AXk. The iteration can be started with152 Eigenvalues and Eigenvectors
X0 = (1, 1,..., 1)T. The sequences {αk} and {Xk} converge to the exact dominant eigen￾value and the dominant eigenvector. The iteration is stopped if
|αk+1|−|αk| < δ and  Xk+1 − Xk  < δ, (7.20)
where δ is a preassumed small positive number and
 Xk+1 − Xk =

n
i=1
(xi,k+1 − xi,k)
2
1/2
. (7.21)
The rate of convergence of {Xk} to V1 is linear and is governed by the term (λ2/λ1)k.
Similarly, the rate of convergence of the sequence {αk} to λ1 is also linear.
Example:
Consider the matrix


100
213
522

 . (7.22)
With the choice X0 = (1, 1, 1)T Eq. (7.19) becomes
X1 = 1
α1
AX0 = 1
α1


100
213
522




1
1
1

 = 1
α1


1
6
9

 .
The absolute largest component of the above vector (1, 6, 9)T is 9. So, α1 = 9. Then,
X1 =


1/9
2/3
1

 .
The second iteration gives
X2 = 1
α2
AX1 = 1
α2


1/9
35/9
35/9

 ,
where α2 = 35/9=3.88889. That is,
X2 =


1/35
1
1

 .
The third iteration gives
X3 = 1
α3
AX2 = 1
α3


1/35
142/35
145/35

Eigenpairs of Symmetric Matrices – Jacobi Method 153
TABLE 7.1
The numerically computed dominant eigenpair of the matrix A given by Eq. (7.22) at each
iteration (k) of the power method. δ in Eq. (7.20), stopping criterion for the iterations, is
chosen as 10−5. The starting eigenvector is X0 = (1, 1, 1)T.
k Eigen- Components of eigenvector |αk+1  Xk+1
value x1 x2 x3 −αk| −Xk 
0 0.00000 1.00000 1.00000 1 − − −− − − −−
1 9.00000 0.11111 0.66667 1 9.00000 0.94933
2 3.88889 0.02857 1.00000 1 5.11111 0.34340
3 4.14286 0.00690 0.97931 1 0.25397 0.02996
4 3.99310 0.00173 1.00000 1 0.14975 0.02133
5 4.00864 0.00043 0.99871 1 0.01553 0.00183
6 3.99957 0.00011 1.00000 1 0.00907 0.00133
7 4.00054 0.00003 0.99992 1 0.00097 0.00011
8 3.99997 0.00001 1.00000 1 0.00057 0.00008
9 4.00003 0.00000 0.99999 1 0.00006 0.00001
10 4.00000 0.00000 1.00000 1 0.00004 0.00001
11 4.00000 0.00000 1.00000 1 0.00000 0.00000
with α3 = 145/35 = 4.14286. The result is
X3 =


1/145
142/145
1

 .
Table 7.1 displays the successive approximation of the dominant eigenvalue and the
corresponding eigenvector. If δ is chosen as 10−1 then the process is to be stopped after
five iterations. At the end of five iterations λ1 = α5 = 4.00864, where the exact dominant
eigenvalue is 4. The numerically computed eigenvector is X5 = (0.00043, 0.99871, 1.0)T
whereas the exact eigenvector is V1 = (0, 1, 1)T. 11 iterations are required to obtain λ1 and
X with δ = 10−5. At the end of 11 iterations λ1 = 4 and X = (0, 1, 1)T.
7.5 Eigenpairs of Symmetric Matrices – Jacobi Method
For symmetric matrices, a few methods are available to find all the eigenpairs. In this
section, the Jacobi method is described. This method is straight forward and simpler than
the other more efficient methods. The method is reliable to produce accurate eigenpairs
for all real symmetric matrices. The method consists of a sequence of orthogonal similarity
transformations. Let us first define plane rotations, similarity transformation and orthogonal
transformation which are essential to follow the Jacobi method.154 Eigenvalues and Eigenvectors
7.5.1 Similarity and Orthogonal Transformations
Let A and B are two matrices and S is another non-singular matrix, that is, detA = 0 such
that
B = S−1AS . (7.23)
The matrices A and B satisfying Eq. (7.23) are said to be similar and this relation is called
a similarity transformation. The matrix S is called a similarity matrix. A matrix C is said
to be orthogonal if C−1 = CT. If S in Eq. (7.23) is orthogonal then the transformation is
an orthogonal similarity transformation. Further, when S−1 = ST then STS = S−1S = I,
unit matrix.
For a real symmetric matrix A, it is possible to find a real orthogonal matrix S such
that S−1AS is a diagonal matrix:
D = S−1AS = STAS. (7.24)
What can one do with the transformation (7.24)? Interestingly, as the eigenvalues of a
diagonal matrix D are its diagonal elements, the eigenvalues of A are the eigenvalues of D
and there exists a relation between the eigenvectors of A and D. This is shown below.
Multiplication of Eq. (7.24) by STX gives
DSTX = STASSTX
= STASS−1X
= STAX
= STλX
= λSTX , (7.25)
where λ is the eigenvalue of A corresponding to the eigenvector X. Defining
Y = STX 
= S−1X (7.26)
Eq. (7.25) is rewritten as
DY = λY (7.27)
which implies that eigenvalues of D are simply the eigenvalues of A. The eigenvectors Y of
D can be easily determined since D is a diagonal. Once Y is known then the eigenvectors
X of A are determined from the relation (obtained from Eq. (7.26))
X = SY (7.28)
Thus, if a given real symmetric matrix A is diagonalized into D by an orthogonal similarity
matrix S through the orthogonal similarity transformations (7.24) then its eigenvalues are
the diagonal elements of D and the eigenvectors are given by Eq. (7.28). The next subsection
describes how to diagonalize A.
7.5.2 Jacobi Rotation
Matrix A can be diagonalized by applying a series of orthogonal similarity transformation
as described below. The idea is that by applying a similarity transformation STAS to A
one of its off-diagonal elements is made zero. Successive transformations make the values
of the off-diagonal elements smaller and smaller until the matrix is diagonal. How does one
determine the matrix S which does the above?Eigenpairs of Symmetric Matrices – Jacobi Method 155
Let us choose the form of S as
S =


1
···
cos θ ··· sin θ
.
.
. 1 .
.
.
− sin θ ··· cos θ
···
1


. (7.29)
S is called Jacobi rotation. In S
1. all the diagonal elements are unity except for two rows say i and k,
2. all off-diagonal elements are zero except for two elements and
3. θ is the angle of rotation to be determined.
Using S the matrix A is transformed into A by
A = STAS. (7.30)
STA alters only the elements in the rows i and k and AS changes only the elements in the
columns i and k.
7.5.3 Setting of Two Off-Diagonal Elements to Zero
θ is chosen to make two off-diagonal elements in A to zero. Out of the various off-diagonal
elements, first the absolute largest off-diagonal elements are made zero. Suppose, |aik| (and
|aki| since A is symmetric) be the absolute largest among the off-diagonal elements. The
2 × 2 submatrix of A with the elements aii, aik, aki and akk is transformed into a diagonal
form by the Jacobi rotation. The elements aik and aki are made to zero. Now, write
S1 =

cos θ sin θ
− sin θ cos θ

. (7.31)
To find the value of θ which can make aik and aki to zero consider the transformation
A1 = ST
1 AS1. This is worked out as
A1 = ST
1 AS1
=

cos θ − sin θ
sin θ cos θ
  aii aik
aki akk   cos θ sin θ
− sin θ cos θ

=

A11 A12
A21 A22 
, (7.32a)
where
A11 = aii cos2 θ + akk sin2 θ − aik sin 2θ , (7.32b)
A12 = 1
2 (aii − akk) sin 2θ + aik cos 2θ , (7.32c)
A21 = 1
2 (aii − akk) sin 2θ + aik cos 2θ , (7.32d)
A22 = aii sin2 θ + akk cos2 θ + aik sin 2θ . (7.32e)156 Eigenvalues and Eigenvectors
In obtaining Eqs. (7.32) the relations aik = aki, cos 2θ = cos2 θ − sin2 θ and sin 2θ =
2 sin θ cos θ are used. Setting A12 = A21 = 0 in Eqs. (7.32) gives
tan 2θ = 2aik/(akk − aii). (7.33)
The most suitable stable reduction is given by the smallest rotation. Therefore, choose the
root in the interval −π/4 ≤ θ ≤ π/4. The desired root is given by
θ =



1
2 tan−1 [2aik/(akk − aii)] , if aii = akk
π/4, if aii = akk, aik > 0
−π/4, if aii = akk, aik < 0 .
(7.34)
The round-off error in the calculation of θ using the formula (7.34) for aii = akk seriously
affect the further computation which then leads to an inaccurate result. To minimize the
round-off error let us obtain an alternate formula for cos θ and sin θ using trigonometric
identities. This is described in the next subsection.
7.5.4 More Accurate Calculation of cos θ and sin θ
Define
φ = (akk − aii) /(2aik) (7.35)
and consider the identity
φ = 1
tan 2θ = cos 2θ
sin 2θ = cos2 θ − sin2 θ
2 sin θ cos θ . (7.36)
From Eq. (7.36) one has
φ = 1 − tan2 θ
2 tan θ
or
tan2 θ + 2φ tan θ − 1=0 . (7.37)
The smaller root of this equation using the formula for quadratic equation given in Sec￾tion 2.5 is
tan θ = −φ ± 
φ2 + 11/2
= sign(φ)
|φ| + (φ2 + 1)1/2 , (7.38a)
where sign(φ) = 1 for φ ≥ 0 and −1 for φ < 0. Then,
cos θ = 1/

1 + tan2 θ
1/2
, (7.38b)
sin θ = cos θ tan θ . (7.38c)
Instead of using the θ given by Eq. (7.34), to compute cos θ and sin θ Eqs. (7.38) can be
used to minimize the round-off error. When aii = akk from Eq. (7.35) and (7.38) φ = 0,
tan θ = 1, cos θ = sin θ = 1/
√2 and θ = π/4.Eigenpairs of Symmetric Matrices – Jacobi Method 157
7.5.5 Effect of Successive Jacobi Rotation
The transformation ST
1 AS1 gives a new matrix A1 in which two off-diagonal elements are set
to zero. The next transformation is applied to the matrix A1. Another matrix A2 = ST
2 A1S2
is obtained in which two off-diagonal elements of A1 are made zero. At rth rotation
Ar = ST
r ST
r−1 ··· ST
1 A S1S2 ··· Sr−1Sr
= (S1S2 ··· Sr)
T A (S1S2 ··· Sr)
= S−1AS. (7.39)
As r → ∞, Ar → D, a diagonal matrix. Its diagonal elements are the eigenvalues of A.
The matrix of eigenvalues of A is given by
X = S1S2 ··· Sr . (7.40)
In the Jacobi method, each rotation is designed to make the absolute largest off-diagonal
elements to zero. If the calculations are done using computer then one can proceed to set
the elements (a12, a21) to zero then set (a13, a31) to zero and so on. In this way searching
of largest off-diagonal element can be avoided.
7.5.6 Examples
Let us determine the eigenpairs of two matrices employing the Jacobi method.
Example 1:
Compute all the eigenpairs of the symmetric matrix
A =


121
211
113

 . (7.41)
The off-diagonal elements are a12 = a21 = 2, a13 = a31 = 1, a23 = a32 = 1. Among them,
the absolute largest element is a12 = a21 = 2. Therefore, first the off-diagonal elements a12
and a21 in A can be made zero. The Jacobi rotation matrix which do this is given by
S1 =


cos θ sin θ 0
− sin θ cos θ 0
0 01

 .
sin θ and cos θ are determined using Eqs. (7.35) and (7.38). For the given matrix A these
equations become
φ = (a22 − a11) / (2a12)=0 ,
tan θ = 1, cos θ = 1/

1 + tan2 θ
1/2 = 1/
√
2, sin θ = 1/
√
2 .
Then,
S1 =


1/
√2 1/
√2 0
−1/
√2 1/
√2 0
0 01

158 Eigenvalues and Eigenvectors
and
A1 = ST
1 AS1 =


−100
0 3 √2
0 √2 3

 .
In A1 the elements a12 = a21 = 0. The rotation has made a13 and a31 to zero which is
unexpected, however, a nice effect of S1.
Next, perform the above analysis for the matrix A1. Among its off-diagonal elements, the
absolute largest is a23(= a32) = √2. Therefore, the off-diagonal elements of the submatrix

a22 a23
a32 a33 
in A1 are chosen to set zero by a Jacobi rotation. The matrix S2 is
S2 =


1 00
0 cos θ sin θ
0 − sin θ cos θ

 .
cos θ and sin θ are calculated as 1/
√2. Thus,
S2 =


1 00
0 1/
√2 1/
√2
0 −1/
√2 1/
√2

 .
The second similarity transformation A2 = ST
2 A1S2 gives
A2 =


−100
0 3 − √2 0
0 0 3+ √2

 .
The matrix A2 is diagonal and its diagonal elements are the eigenvalues. In order to verify
the above result consider the characteristic equation of the given matrix A. It is
λ3 − 5λ2 + λ +7=0 .
The three eigenvalues
λ1 = −1, λ2,3 = 3 ± √
2
satisfy the above characteristic equation. The eigenvectors are the column vectors of the
product matrix S = S1S2. S is obtained as
S =


1/
√2 1/2 1/2
−1/
√2 1/2 1/2
0 −1/
√2 1/
√2

 .
The eigenpairs are thus

−1,

1/
√
2, −1/
√
2, 0
T
,

3 + √
2,

1/2, 1/2, −1/
√
2
T
,

3 − √
2,

1/2, 1/2, 1/
√
2
T
.Eigenvalues of Symmetric Tridiagonal Matrices – QL Method 159
Example 2:
In the Jacobi method, a similarity transformation is used to set off-diagonal elements to
zero in the given or its equivalent matrix. But the transformation may produce nonzero
value for off-diagonal elements which are set to zero earlier. Consequently, more iterations
than the number of pairs of nonzero off-diagonal elements may be required to set all the
off-diagonal elements to zero. This happens in the matrix
A =


101
012
121

 . (7.42)
When the elements a23 and a32 are tried to set zero the resultant matrix is
A1 =


1 −1/
√2 1/
√2
−1/
√2 −1 0
1/
√2 03

 .
Though the elements a23 and a32 are zero in A1, the elements a12 and a21 which are zero in
A now become nonzero. This problem is overcome in the Householder’s and Given’s method,
where the zero off-diagonal elements remain zero in subsequent iterations. For details of this
7 rotations the diagonal matrix obtained is
A7 =


1 0 0
0 −1.23607 0
0 0 3.23607

 .
Thus, the eigenvalues are 1, −1.23607, 3.23607. The eigenvector matrix is computed as


0.89443 0.31623 0.31623
−0.44721 0.63246 0.63246
0.00000 −0.70711 0.70711

 .
7.6 Eigenvalues of Symmetric Tridiagonal Matrices – QL Method
A symmetric tridiagonal matrix is of the form
A =


a1 d1 0 ··· 0
d1 a2 d2 ··· 0
0 d2 a3 ··· 0
··· ·
an−2 dn−2 0
dn−2 an−1 dn−1
0 dn−1 an


. (7.43)
method see for example refs. [9–11]. Proceeding the Jacobi method with the matrix A1 after160 Eigenvalues and Eigenvectors
All the eigenvalues of A can be computed by the QL method. In quantum mechanics, the
problem of finding numerical solution of time-independent Schr¨odinger equation can be
converted into the eigenvalue problem (A−λI)X = 0, with A being symmetric tridiagonal.
The QL method is of great use in solving such problems.
In the QL method, the given matrix A1 = A is factorized into the form
A1 = Q1L1 , (7.44)
where Q1 = Q and L1 = L are orthogonal and lower-triangular matrices, respectively. To
write A1 into the form of Eq. (7.44), a matrix Pn−1 is constructed in such a way that the
element in the location (n − 1, n) in the matrix Pn−1A is zero. That is,
Pn−1A =


a1 d1 0
d1 a2 d2
0 d2 a3
···
an−2 dn−2 0
qn−2 pn−1 0
rn−2 qn−1 pn


. (7.45)
Similarly, by a suitable Pn−2 the element in the position (n − 2, n − 1) of Pn−1A can be
made zero. Repetition of the above process (n − 1) times gives
P1P2 ··· Pn−1A =


p1 0 0
q1 p2 0
r1 q2 p3
···
qn−3 pn−2 0 0
rn−3 qn−3 pn−1 0
0 rn−2 qn−1 pn


. (7.46)
Now, define
Q1 = P T
n−1P T
n−2 ··· P T
1 (7.47)
and form the matrix
A2 = L1Q1 . (7.48)
The matrix Q1 is orthogonal, therefore, QT
1 A1 = QT
1 Q1L1 = L1. Thus, A2 = QT
1 A1Q1 =
Q−1
1 A1Q1 which implies that A2 is similar to A1 and has the same eigenvalues. In general
Ak+1 = QT
k AkQk so that Ak+1 is similar to Ak.
Let us consider the case of making the element Apq and Aqp to zero. In this case, P1 has
the form
P1 =


1 ···
0 ...
c ··· s
···
−s ··· c
···
1


. (7.49)Eigenvalues of Symmetric Tridiagonal Matrices – QL Method 161
The tridiagonal form of A2 implies that it also has zeros below the lower diagonal. The
detailed calculation shows that the terms rj are used only to compute these zero elements.
In writing a computer program the rj ’s need not be stored. For each Pj it is enough to store
the coefficients sj and cj . Further, it is not necessary to compute and store Q explicitly.
Instead sj ’s and cj ’s can be used to find the product
A2 = LQ = LP T
n−1P T
n−2 ··· P T
1 . (7.50)
To speed up the process a shifting technique can be employed. The idea is that if λj is
an eigenvalue of A then λj − si is an eigenvalue of A − siI. This is implemented in the step
Ai − siI = QiLi (7.51)
then form
Ai+1 = LiQi, for i = 1, 2,...kj (7.52a)
where
λj = s1 + s2 + ··· + skj . (7.52b)
The correct shift required at each step can be found by using the four elements in the
upper-left corner of the matrix. For example, for the matrix 
a1 d1
d1 a2

the eigenvalues
x1 and x2 are the roots of equation
x2 − (a1 + a2) x + a1a2 − d2
1 = 0 . (7.53)
The root which is closer to a1 is the value of si in Eq. (7.51).
The QL iteration with shifting is repeated until d1 ≈ 0 which produces λ1 = s1 +
s2 + ··· + sk1 . Repeating the process with the lower (n − 1) rows gives d2 ≈ 0 and the
next eigenvalue λ2. Next iteration gives dn−2 ≈ 0 and the eigenvalue λn−2. The quadratic
formula can be used to find the last two eigenvalues.
Example:
Let us apply the QL method to the matrix
A =


1 −1 0
−1 2 −1
0 −1 1

 . (7.54)
Its exact eigenvalues are λ = 0, 1, 3.
For the given matrix a1 = 1, a2 = 2, d1 = −1 and the quadratic Eq. (7.53) is x2−3x+1 =
0. Its roots are x1 = 2.61803 and x2 = 0.38197. The root close to a1 = 1 is x2. Therefore,
the shift is s1 = 0.38197. The first shifted matrix is
A1 − s1I =


0.61803 −1 0
−1 1.61803 −1
0 −1 0.61803

 .
The LQ matrix is computed as
A2 = L1Q1 =


0 0.52573 0
0.52573 0.61803 −0.85065
0 −0.85065 2.23607

 .162 Eigenvalues and Eigenvectors
|d1| = 0.52573 in A is not less than the preassumed small value δ = 10−5. Therefore,
perform second iteration with A2. Now, a1 = 0, a2 = 0.61803, d1 = 0.52573 and the roots
of Eq. (7.53) are x1 = 0.91884, x2 = −0.30081 and the second shift is s2 = −0.30081. The
second shifted matrix is
A2 − s2I =


0.30081 0.52573 0
0.52573 0.91884 −0.85065
0 −0.85065 2.53687

 .
The LQ matrix is obtained as
A3 = L2Q2 =


−0.07668 −0.06711 0
−0.06711 0.94706 −0.25379
0 −0.25379 2.88615

 .
In the third iteration the shift is s3 = −0.08107. Next,
A4 = L3Q3 =


−0.00009 0.00001 0
0.00001 1.00358 −0.08565
0 −0.08565 2.99623

 .
The shift s4 is −0.00009. The LQ matrix is obtained as
A5 = L4Q4 =


0 0 0
0 1.00041 −0.02860
0 −0.02860 2.99959

 .
In the next iteration s5 = 0 hence an eigenvalue is obtained as
λ1 = s1 + s2 + s3 + s4 + s5 = 0 .
Replace the first diagonal element in A5 by λ1 and write A5 as
A5 =


0 0 0
0 1.00041 −0.02860
0 −0.02860 2.99959

 .
The LQ matrix is
A6 = L5Q5 =


0 0 0
0 1.00005 −0.00953
0 −0.00953 2.99995

 .
The last two eigenvalues are determined using the 2×2 right-side corner matrix. The roots of
the quadratic equation are x1 = 3 and x2 = 1. Then, λ2 = λ1+x1 = 3 and λ3 = λ1+x2 = 1.
Therefore, the eigenvalues are 0, 1 and 3.
7.7 Eigenpairs of General Real Matrices
For general real matrices, the method proposed by Rutishauser is of use to find all eigenpairs.
In his method a given matrix A1 is decomposed into lower and upper triangular matricesEigenpairs of General Real Matrices 163
as described in Section 4.7. Let us call these matrices as L1 and U1, respectively. Then,
A1 = L1U1 , (7.55)
where lii = 1. A matrix A2 is next constructed through equation
A2 = U1L1 . (7.56)
Multiplication of right-side of Eq. (7.56) by I = U1U −1
1 gives
A2 = U1L1U1U −1
1 = U1A1U −1
1 . (7.57)
From Eq. (7.57) A1 is expressed as
A1 = U −1
1 A2U1 (7.58)
which is the similarity transformation. Equation (7.58) implies that A1 and A2 are equivalent
and hence have same eigenvalues. Decomposing A2 into L2U2 a new matrix A3 = U2L2 is
constructed. This process is continued until an upper-triangular matrix is arrived. The
diagonal elements of the final matrix are the eigenvalues of the given matric A1. Then the
associated eigenvectors are obtained by solving the linear system A1X = λX.
Example:
Consider the matrix
A1 =

4 1
−1 1 
. (7.59)
The equation A1 = L1U1 is

4 1
−1 1 
=

1 0
l21 1
  u11 u12
0 u22 
=

u11 u12
l21u11 l22u12 + u22 
.
Comparison of the elements on both sides of the matrices in the above equation gives
L1 =

1 0
−1/4 1 
, U1 =

4 1
0 5/4

.
Repeating the process 4 more times gives
A2 =

15/4 1
−5/16 5/4

, A3 =

11/3 1
−1/9 4/3

,
A4 =

40/11 1
−13/297 13/9

, A5 =

3.62432 1
−0.01753 1.45648 
.
The magnitude of the element a21 decreases and in the limit r → ∞ one expects a21 in
Ar to be ≈ 0. After 5 iterations the eigenvalues are 3.62432 and 1.45648, where the exact
values are 3.61803 and 1.38196.164 Eigenvalues and Eigenvectors
7.8 Concluding Remarks
In Chapter 4, the linear system of equations of the form AX = B is considered. When
B = 0 the system AX = 0 is a system of homogeneous equations. The eigenvalue equation
can be rewritten in the homogeneous form (A−λI)X = 0. In the present chapter, numerical
computation of eigenvalues and eigenvectors of certain types of matrices are presented. Using
the power method dominant eigenpair can be obtained. The Jacobi method is for symmetric
matrices. For symmetric tridiagonal matrices, the QL method is useful. A method for general
real matrices is also considered.
The eigenvalues of a matrix in a problem can be zero, real positive, real negative, complex
conjugate with negative real part, complex conjugate with positive real part, pure imaginary,
magnitude being small and magnitude being large. Some of the eigenvalues may be identical
and in some cases all can be different and so on. What are the significances of these cases?
The significances of these cases depend on the nature of the problem concerned. In certain
problems real eigenvalues are meaningful.
Consider the anharmonic oscillator equation ¨x + dx˙ + ax + bx3 = 0. It can be rewritten
as a system of first-order equations as
x˙ = y = P(x, y), y˙ = −dy − ax − bx3 = Q(x, y). (7.60)
Its equilibrium points (x∗, y∗) are the roots of the equation P(x, y) = 0 and Q(x, y) = 0.
Depending upon the values of the parameters d, a and b the given system can admit one
or three real equilibrium points. The stability of an equilibrium point is determined by the
nature of the eigenvalues of the Jacobian matrix
J =

∂P/∂x ∂P/∂y
∂Q/∂x ∂Q/∂y 
(7.61)
evaluated at (x∗, y∗). For lower-dimensional systems, the eigenvalues can be determined
analytically. For higher-dimensional systems often they need to be computed numerically.
An equilibrium point is stable, that is nearby trajectories approach it in the long-time
limit, only if all the eigenvalues have negative real part. It at least one eigenvalue has a
positive real part then the equilibrium point becomes unstable and the nearby trajectories
diverge from its neighbourhood. An equilibrium point can be further classified into a stable
star, an unstable star, a stable node, an unstable node, a saddle, a stable focus, an unstable
focus and a centre in terms of the nature of the eigenvalues. For details refer the ref. [8].
7.9 Bibliography
[1] E. Kreyszig, Advanced Engineering Mathematics. John Wiley, New York, 1999.
8th edition.
[2] A. Jeffrey, Advanced Engineering Mathematics. Academic Press, San Diego, 2003.
Indian reprint.
[3] Yandasaketh, Applications of Eigenvalues and Eigenvector . https://www.geeks￾forgeeks.org/applications-of-eigenvalues-and-eigenvectors/ (accessed on June 4,
2023).Problems 165
[4] Some Applications of the Eigenvalues and Eigenvectors of a square matrix .
https://sthcphy.files.wordpress.com/2020/05/eigenvalue-applications.pdf (acces￾sed on June 4, 2023).
[5] Applications of Eigenvalues and Eigenvectors. https://www.sheffield.ac.uk/media
/32039/download?attachment (accessed on June 4, 2023).
[6] Eigenvalues and Eigenvectors. https://en.wikipedia.org/wiki/Eigenvalues−and−
eigenvectors.
[7] Ajitesh Kumar, Why & When to use Eigenvalues & Eigenvectors? https://vitalfl￾lx.com/why-when-use-eigenvalue-eigenvector/ (accessed on June 4, 2023).
[8] M. Lakshmanan and S. Rajasekar, Nonlinear Dynamics: Integrability, Chaos and
Patterns. Springer, Berlin, 2002.
[9] J.H. Mathews, Numerical Methods for Mathematics Science and Engineering.
Prentice-Hall of India, New Delhi, 2005.
[10] M.K. Jain, S.R.K. Iyengar, R.K. Jain, Numerical Methods for Scientific and
Engineering Computation. New Age International, New Delhi, 1993.
[11] W.H. Press, S.A. Teukolsky, W.T. Vetterling, B.P. Flannery, Numerical Recipes
in Fortran. Foundation Books, New Delhi, 1993. Indian edition.
7.10 Problems
A. Power Method
In Problems 7.1–7.3 for hand calculation obtain the eigenpair after 3 iterations. If the
calculation is done using a computer choose δ = 10−5.
7.1 For the following cubic equations
i) obtain their dominant root by finding the dominant eigenvalue of their com￾panion matrix and
ii) obtain their smallest real root if they exist (hint: substitute z = 1/y in the
cubic equations).
a) z3 − 16z2 + 68z − 80 = 0. b) z3 − z2 + 10z − 10 = 0.
c) z3 − 5z2 + 3z − 9 = 0.
7.2 Compute the dominant eigenpair of the following matrices.
a)


2 11
1 10
1 −1 0

 b)


111
203
211

 c)


2002
1101
1110
2341


7.3 Obtain the dominant eigenpair of the matrix
A =


2 0 −1
02 0
00 1

 .
By inspecting X0, X1, X2 and X3 write the eigenvector after large number of
iterations.166 Eigenvalues and Eigenvectors
B. Jacobi Method
7.4 Obtain all the eigenpairs of the following symmetric matrices.
a) 
1 2
2 1 
b)  √2 −1
−1 √2

c) 
3 1
1 3 
7.5 Compute all the eigenpairs of the following symmetric matrices.
a)


1 √2 2
√2 3 √2
2 √2 1

 b)


102
010
201


c)


−2 30
3 −2 1
0 10


7.6 Develop a Python program to compute the eigenvalues and the corresponding
eigenvectors by the Jacobi method. Then, modify the program so that the eigen￾values are arranged in ascending order in the final diagonalized matrix and the
corresponding eigenvectors are normalized.
7.7 Find the eigenpairs of the matrix
A =


−2 30
3 −2 1
0 10


(a) by searching for the largest off-diagonal element and
(b) without searching for the largest off-diagonal element.
Compare the two approaches.
C. QL Method
7.8 Compute the eigenvalues of the following symmetric tridiagonal matrices. Also,
compute the associated eigenvectors.
a) 
2 1
1 2 
b)


21 0
13 2
0 2 −2

 c)


1 −2 0
−2 −2 1
0 11


7.9 Find the eigenpairs of the following symmetric tridiagonal matrices.
a)


310
131
013

 b)


3 −1 0
−1 3 −1
0 −1 3


c)


1 −100
−1 1 −1 0
0 −1 1 −1
0 0 −1 1

Problems 167
7.10 Compute the eigenpairs of the following symmetric tridiagonal matrices.
a)


2 −100
−1 3 −2 0
0 −2 4 −3
0 0 −3 5


b)


2 −200
−2 2 −2 0
0 −2 2 −2
0 0 −2 2

8
Numerical Differentiation
8.1 Introduction
What is meant by numerical differentiation? It is defined as a mathematical process of
determining the numerical value of an nth-order derivative f(n)
(x) at a target point x0
using the values of f(x) given at several nodal points of x near x0. The goal of the numerical
differentiation is to find approximate formulas for the computation of derivatives of f(x),
using only the given data set, say, (xi, f(xi)), i = 0, 1, 2,...,N.
In many phenomena in science and engineering derivatives of a function describing vari￾ation of a physical variable with respect to one or more control parameters are helpful to
understand them [1-2]. It is well known that differentiation of position x(t) gives velocity
v(t), velocity to acceleration a(t), momentum ma (mass × acceleration) to force F, charge q
to current I and energy E to power P. In mathematics, differentiation makes a cumulative
distribution into a probability density, volumes into areas and areas into lengths.
Consider the motion of a particle in a potential well. Let its displacement from the
equilibrium point is given by x(t) = sin t. Then, the velocity v(t) of the particle is v(t) =
dx/dt = cost and it changes continuously with time t. From the above expressions of x(t)
and v(t) the position and velocity of the particle at time t = 1 and 1.01 are
x(1) = 0.8414709, v(1) = 0.5403023, x(1.01) = 0.8468318, v(1.01) = 0.5318607. (8.1)
The explicit forms of x(t) and v(t) are not known but only the numerical values of x at
discrete values of t are given. For example, the values of x(0), x(0.01),...,x(2.0) are given.
How does one calculate v(t) from the time series of x(t)? Since the velocity is the rate
of change of position, one may approximate
v(t) = dx
dt = x(t + h) − x(t)
h + O(h). (8.2)
Equation (8.2) is obtained from the Taylor series expansion of x(t + h) about x(t) by as￾suming that |h|  1. Using the values of x(1) and x(1.01) in Eq. (8.2), v(1) is computed as
0.53609 while the exact value is 0.5403023. The approximation of dx/dt given by Eq. (8.2)
is inaccurate. Thus, more accurate formulas are required to determine v(t)=dx/dt. There￾fore, it is important to develop and study good approximate formulas to evaluate first,
second and higher-order derivatives of a function f(x) whose exact form is not known but
only its numerical values are given at discrete values of x. This is the goal of numerical dif￾ferentiation. The present chapter first develops some simple numerical formulas to compute
integer order derivatives of a function f(x) and illustrate their efficiency. Next, computation
of fractional order derivatives is considered.
DOI: 10.1201/9781032649931-8 168Formulas for First-Order Derivative 169
8.2 Formulas for First-Order Derivative
Methods involving difference quotient approximations, called finite-difference approxima￾tions, were used for derivatives first by Euler in 1768. Such simple formulas for evaluating
numerically the first-, second- and higher-order derivatives can be derived using Taylor se￾ries expansion of a given function f(x). Runge, Richardson and Liebmann were the first to
explore the finite-differences for derivatives. To start with, first presents the formulas for
first-order derivative.
8.2.1 Forward- and Backward-Difference Formulas
It is desired to evaluate the derivative f of the function f(x) at x0, where x, x0 ∈ [a, b].
For this purpose, consider the Taylor series expansion of f(x0 + h) about the point x0:
f (x0 + h) = f (x0) + hf (x0) + h2
2! f (x0) + h3
3! f(3) (x0)
+
h4
4! f(4) (x0) + h5
5! f(5) (x0) + h6
6! f(6) (x0) + ··· . (8.3)
Equation (8.3) after neglecting the terms containing h3 and higher powers of h becomes
f (x0 + h) = f (x0) + hf (x0) + h2
2! f (x0). (8.4)
Solving Eq. (8.4) for f
(x0) gives
f (x0) = f (x0 + h) − f (x0)
h − h
2!f (x
), (8.5)
where x ∈ [a, b] with x0, x0+h ∈ [a, b]. The first term in the right-side of the above equation
is called forward-difference formula and the second term is the truncation error. In Eq. (8.5)
the error is of the order of h, that is, the approximation is only first-order accurate.
On the other hand, from the Taylor series expansion
f (x0 − h) = f (x0) − hf (x0) + h2
2! f (x0) − h3
3! f(3) (x0)
+
h4
4! f(4) (x0) − h5
5! f(5) (x0) + h6
6! f(6) (x0) −··· . (8.6)
f
(x0) is obtained as
f (x0) = f (x0) − f (x0 − h)
h +
h
2!f (x
) . (8.7)
This is known as backward-difference formula and is also first-order accurate in h.
Another formula in terms of forward-difference quantities can be obtained. Replacing h
by 2h in Eq. (8.3) gives
f (x0 + 2h) = f (x0)+2hf (x0)+2h2f (x0) + 4h3
3 f(3) (x0) + ··· . (8.8)
Equation (8.8) − 4×Eq. (8.3) results in
f (x0) = −3f (x0)+4f (x0 + h) − f(x0 + 2h)
2h , (8.9)
where the terms containing higher powers of h are neglected. The error term is O(h2).170 Numerical Differentiation
8.2.2 Central-Difference Formulas
Subtraction of Eq. (8.6) from Eq. (8.3) gives
f (x0 + h) − f (x0 − h)=2hf (x0) + 1
3
h3f(3) (x0) + 2
5!h5f(5) (x0) + ··· . (8.10)
From this equation, f
(x0) is obtained as
f (x0) = f (x0 + h) − f (x0 − h)
2h − h2
6 f(3) (x0) − 1
5!h4f(5)(x0) −··· . (8.11)
The second and the other successive terms in the right-side of the above equation contain
even powers of h. Neglecting the terms containing h2 and other higher-order terms result
in
f (x0) = f (x0 + h) − f (x0 − h)
2h . (8.12)
Because the two nodes used to compute f
(x0) are symmetrically situated from x0, the
formula, Eq. (8.12), is called a two-point central-difference formula. The value of the first￾order derivative of a function f(x) at a point x0 is thus given by the difference between
the values of the function evaluated at the points x0 − h and x0 + h lying to the left
and right of x0, respectively, divided by 2h. In this formula the truncation error term is
Etrun = −(h2/6)f(3)(x
), where x ∈ [a, b] with x0 + h, x0 − h ∈ [a, b]. The error is of the
order of h2 and is exact for polynomials of degree ≤ 2. Like the forward and backward for￾mulas, the central-difference formula also needs only two values of f. The central-difference
approximation has a better accuracy than the forward- and backward-difference formulas
and hence it is most commonly used.
Equation (8.10) with terms up to h5 is
f (x0 + h) − f (x0 − h)=2hf (x0) + 1
3
h3f(3) (x0) + 1
60
h5f(5) (x0). (8.13)
Replace h by 2h in Eq. (8.13) and obtain
f (x0 + 2h) − f (x0 − 2h)=4hf (x0) + 8
3
h3f(3) (x0) + 8
15
h5f(5) (x0). (8.14)
Multiplication of Eq. (8.13) by 8 and then subtracting it from (8.14) lead to
f (x0) = 1
12h {8 [f (x0 + h) − f (x0 − h)] − [f (x0 + 2h) − f (x0 − 2h)]}
+
1
30
h4f(5) (x
) . (8.15)
The first term in the right-side of Eq. (8.15) is a four-point central-difference formula while
the second term is the error. Note that the value of the derivative of f(x) at x0 is determined
using the four values of f at equally spaced nodes with two on left-side of x0 and another
two on right-side of x0. The error term is proportional to fifth derivative of f. That is, the
formula (8.15) is exact for polynomial of degree ≤ 4.
Assume that the values of f(xi), i = 0, 1, 2,...,N are given. Forward-difference formula
is not useful to find f
(xN ) while f
(x0) cannot be computed using backward-difference
formula. On the other hand, central-difference formula cannot be applicable at both x0 and
xN . For f
(x0) the formula (8.9) can be used. To obtain a formula for f
(xN ), replace x0
by xN and h by −h in Eq. (8.9) and obtain
f (xN ) = 1
2h [−4f (xN − h) + f (xN − 2h)+3f (xN )] . (8.16)Formulas for Second-Order Derivative 171
8.3 Formulas for Second-Order Derivative
Formulas for second-order and higher-order derivatives of f(x) can also be derived from
Taylor series. Let us derive the formulas for second-order derivative.
Adding of Eqs. (8.3) and (8.6) results in
f (x0 + h) + f (x0 − h)=2f (x0) + h2f (x0) + 2
4!h4f(4) (x0)
+
2
6!h6f(6) (x0) + ··· . (8.17)
From the above equation f(x0) is obtained as
f (x0) = 1
h2 [f (x0 + h) − 2f (x0) + f (x0 − h)] − h2
12 f(4) (x
), (8.18)
where the series is truncated at fourth derivative. The first term in the right-side of Eq. (8.18)
is an approximate formula for second derivative and the second term is the truncation error
and is of the order of h2. Equation (8.8)−2×Eq. (8.3) gives the formula
f (x0) = 1
h2 [f (x0) − 2f (x0 + h) + f (x0 + 2h)] . (8.19)
A formula with the truncation error of the order of h4 is obtained as follows. Equation
(8.17) with terms up to h6 is written as
f (x0 + h) + f (x0 − h)=2f (x0) + h2f (x0)
+
2
4!h4f(4) (x0) + 2
6!h6f(6) (x0). (8.20)
Replacing h by 2h in Eq. (8.20) gives
f (x0 + 2h) + f (x0 − 2h)=2f (x0)+4h2f (x0)
+
32
4! h4f(4) (x0) + 128
6! h6f(6) (x0). (8.21)
Multiplication of Eq. (8.20) by 16 and then subtracting it from Eq. (8.21) give
f (x0) = 1
12h2 [−f (x0 + 2h) + 16f (x0 + h) − 30f (x0)
+16f (x0 − h) − f (x0 − 2h)] + 23
2160
h4f(6) (x
). (8.22)
8.4 Formulas for Third-Order Derivative
Formulas for third-order derivative with the error term of the order of h and h2 can be
obtained as follows. Replacing h by 3h in Eq. (8.3) gives
f (x0 + 3h) = f (x0)+3hf (x0) + 9
2
h2f (x0) + 9
2
h3f(3) (x0)
+
27
8
h4f(4) (x0) + ··· . (8.23)172 Numerical Differentiation
Equation (8.23)−3×Eq. (8.8) gives
f (x0 + 3h) − 3f (x0 + 2h) = −2f (x0) − 3hf (x0) − 3
2
h2f (x0)
+
1
2
h3f(3) (x0) + ··· . (8.24)
Then, Eq. (8.24)+3×Eq. (8.3) leads to the formula
f(3) (x0) = 1
h3

− f (x0)+3f (x0 + h) − 3f (x0 + 2h) + f (x0 + 3h)

+ O (h). (8.25)
Next, consideration of Eq. (8.14)−2×Eq. (8.13) gives the central-difference formula
f(3) (x0) = 1
2h3

f (x0 + 2h) − f (x0 − 2h)
−2f (x0 + h)+2f (x0 − h)

+ O 
h2
. (8.26)
8.4.1 Computation of a Highly Accurate Result
Generally, the numerical value of f
(x) from an approximation is expected to approach the
true value of f
(x) in the limit h → 0. Consequently, accuracy in the numerical compu￾tation of derivatives can be highly improved by combining an approximation, for example
the central-difference formula, and an extrapolation method like Richardson extrapolation
technique. Denote y(h) and y(αh) are the two approximate values of y obtained by a method
of order p with step size h and αh, respectively. Then,
y(h) = y + chp + O 
hp+q
, (8.27a)
y(αh) = y + cαphp + O 
hp+q
. (8.27b)
Multiplication of Eq. (8.27a) by αp and then subtracting from Eq. (8.27b) give
y = y(h)αp − y(αh)
αp − 1 + O 
hp+q
. (8.28)
The above formula is known as Richardson extrapolation. The first term in the right-side of
Eq. (8.28) is a more accurate value of y with error term of the order of hp+q. For a two-point
formula p = q = 2 in Eq. (8.28). If the above extrapolation scheme is applied to a two-point
formula for the computation of f then the local truncation error is O(h4).
For convenience define h = h1, αh = h2, α = h2/h1. Then,
y(accurate) = (h2/h1)
p y (h1) − y (h2)
(h2/h1)
p − 1 . (8.29)
Suppose, f
1 and f
2 are the values of f obtained at a point x with step size h1 and h2 using
a two-point formula. Then, a more accurate value of f is
f
(accurate) = (h2/h1)
2 f
1 − f
2
(h2/h1)
2 − 1 . (8.30)
Example:
Compute the first and second derivatives of f(x) = sin x at x = 1 using the two-point
central-difference formula for two-step sizes h = 0.05 and 0.1. Then, obtain the accurateFormulas for Third-Order Derivative 173
value of f
(1) by employing Richardson extrapolation to the two computed values of f
.
Also, compute the bounds of truncation error in the two-point central-difference formula.
1. Calculation of first derivative
The two-point formula for f
(x0) is
f (x0) = f (x0 + h) − f (x0 − h)
2h .
For f(x) = sin x with h = 0.05, the above formula at x0 = 1 becomes
f
(1) = sin(1.0+0.05) − sin(1.0 − 0.05)
2 × 0.05 = 0.5400772 .
The exact value of f
(1) is cos 1 = 0.5403023. The percentage of relative error is 0.042. With
h = 0.1
f
(1) = 1
2 × 0.1 (sin(1.0+0.1) − sin(1.0 − 0.1)) = 0.5394022 .
The percentage of relative error is 0.167.
A more accurate result is then obtained using Richardson’s extrapolation (Eq. (8.30)).
With h1 = 0.1, h2 = 0.05, f
1 = 0.5394022 and f
2 = 0.5400772
f
(accurate) = 0.5394022 × (0.05/0.1)2 − 0.5400772
(0.05/0.1)2 − 1 = 0.5403022
which is very close to the exact value and the percentage of relative error is 1.47 × 10−5.
Next, proceed to compute the bounds of truncation error. The truncation error is
Etrun = −1
6
h2f(3) (x
) ,
where x ∈ [a, b] with x0+h, x0−h ∈ [a, b]. For h = 0.1, [a, b] → [0.9, 1.1]. Since, |sin(1.1)| >
|sin(0.9)| one can choose x = 1.1. Now, |f(3)(x
)|≤|− sin(1.1)| = 0.8912073. Then,
|Etrun| ≤




−1
6
h2f(3)(1.1)




= 0.00148534 .
For h = 0.05, [a, b] → [0.95, 1.05] and
|Etrun| ≤ 1
6
h2


f(3)(1.05)



= 0.000361426 .
2. Calculation of second derivative f(1)
The two-point formula for f(x0) is
f (x0) = 1
h2 [f (x0 + h) − 2f (x0) + f (x0 − h)] .
With h = 0.05
f(1) = 1
0.052 [sin(1.05) − 2 sin(1) + sin(0.95)] = −0.841296.
The exact value of f(1) = − sin(1) is −0.841471. The percentage of relative error is 0.021.174 Numerical Differentiation
Next, with h = 0.1
f(1) = 1
0.12 [sin(1.1) − 2 sin(1) + sin(0.9)] = −0.84077.
The percentage of relative error is 0.083. Employing the Richardson extrapolation results
in
f(accurate) = (0.05/0.1)2 × −0.840771 − (−0.841296)
(0.05/0.1)2 − 1 = −0.841471.
8.5 Fractional Order Derivatives
The focus of this section is on the numerical evaluation of fractional order derivatives of
a function. There are certain definitions of the fractional order derivative of order α > 0
[3-6]. The widely used definitions are the Riemann–Liouville and Caputo. The Caputo’s
definition is often preferred in physical applications as it has the advantage that the initial
condition is specified in terms of field variables and their integer order derivatives. Why
is the study of fractional derivatives important? A fractional derivative of a function with
respect to space or time variable contains memory effect, that is, it possesses details of the
function at earlier space or time. The fractional derivatives are thus nonlocal operators.
This feature has applications in viscoelastic materials, polymers and anomalous diffusion,
a few to mention. For more details, see Chapter 16.
The most common version of the Caputo’s definition of the fractional order derivative
Dα
a of f(t) is
Dα
a f(t) = 1
Γ(m − α)
 t
a=0
(t − τ )
m−1−αf(m)
(τ )dτ , (8.31)
where m = [α] + 1 with [α] being the integer part of α and Γ(γ) is the gamma function
with argument γ and Γ(γ)=(γ − 1)!. The subscript a in Dα
a denotes the lower limit of the
integration. For convenience and simplicity, the subscript a may be dropped. The gamma
function is defined as
Γ(γ) =  ∞
0
zγ−1e−zdz . (8.32)
For 0 <α< 1, the value of m is 1 and for 1 <α< 2, its value is 2. Methods for evaluating
numerically Dαf(t) are proposed and analyzed [7-11].
8.5.1 Fractional Derivative of t
β
Let us find Dαt
β for 0 <α< 1 and for positive integer values of β. For 0 <α< 1, Eq. (8.31)
becomes
Dαf(t) = 1
Γ(1 − α)
 t
0
(t − τ )
−αf
(τ )dτ, t ≥ 0. (8.33)
Start with Dαt and obtain
Dαt = 1
Γ(1 − α)
 t
0
(t − τ )
−αdτ = Γ(2)
Γ(2 − α)
t
1−α. (8.34a)Fractional Order Derivatives 175
Next, Dαt
2 and Dαt
3 are determined as
Dαt
2 = 2
Γ(1 − α)
 t
0
(t − τ )
−ατdτ
= 2
(1 − α)Γ(1 − α)
 t
0
(t − τ )
1−αdτ
= Γ(3)
Γ(3 − α)
t
2−α (8.34b)
and
Dαt
3 = 3
Γ(1 − α)
 t
0
(t − τ )
−ατ 2dτ
= 3 · 2
(1 − α)Γ(1 − α)
 t
0
(t − τ )
1−ατdτ
= 3 · 2 · 1
(2 − α)(1 − α)Γ(1 − α)
 t
0
(t − τ )
2−αdτ
= Γ(4)
Γ(4 − α)
t
3−α . (8.34c)
From Eqs. (8.34) Dαt
β is written as
Dαt
β = Γ(β + 1)
Γ(β + 1 − α)
t
β−α. (8.35)
8.5.2 Composite Trapezoidal Rule for Dαf(t), 0 <α< 1
For 0 <α< 1, Dαf(t) is given by Eq. (8.33). It is desired to evaluate the derivative at,
say, t = T. Can this integral be evaluated by applying the composite quadrature formulas?
There is a problem in applying these formulas directly to this integral. The integrand has
a singularity at τ = T and for values of τ near T the term (t − τ )−α diverges rapidly.
Interestingly, the singularity can be avoided by integrating the integral by parts once. This
gives
Dαf(t) = t
1−αf
(0)
(1 − α)Γ(1 − α) +
1
(1 − α)Γ(1 − α)
 t
0
(t − τ )
1−αf(τ )dτ
= 1
Γ(2 − α)

t
1−αf
(0) +  t
0
(t − τ )
1−αf(τ )dτ

. (8.36)
The above formula is proposed and analyzed in [10]. Approximate the integral in this equa￾tion by the composite trapezoidal rule (10.35). Divide the interval [0, t(= T)] into n subin￾tervals with nodes τk = kh, k = 0, 1,...,n, h = t(= T)/n. Application of the composite
trapezoidal rule leads to the equation
Dαf(t) = 1
Γ(2 − α)

t
1−αf
(0)
+
h
2

t
1−αf(0) + 2
n
−1
k=1
(t − τk)
1−αf(τk)
 . (8.37)
At the node τn = t, the term (t − τn)1−αf(τn) is 0 and hence this term does not appear
in the above equation.176 Numerical Differentiation
If the analytical expressions for f and f are known and determining their values is
easy then the formula (8.37) can be used to compute Dαf(t) at t = T. When finding f and
f are difficult or only numerical values of them are known at the nodes then it is necessary
to replace them by appropriate finite-difference approximations. Since t ≥ 0 the derivatives
f
(0) and f(0) are approximated by the formulas (refer Eqs. (8.9) and (8.19))
f
(0) = 1
2h [−3f(0) + 4f(h) − f(2h)] , (8.38a)
f(0) = 1
h2 [f(0) − 2f(h) + f(2h)] . (8.38b)
For the nodes τk, k = 1, 2,...,n − 1 the three-point central-difference formula is useful and
is
f(τk) = 1
h2 [f(τk − h) − 2f(τk) + f(τk + h)]. (8.38c)
The error term in the composite trapezoidal rule is
E = − t
12
h2 d2
dτ 2

(t − τ )
1−αf(τ )
 

τ=µ, (8.39)
where 0 <µ<t. The error E → 0 as h → 0. Further, the error terms in the approximations
of f and f also approach zero in the limit of h → 0.
Example:
For the function f(t) = t
3+α the exact value of Dαf(t) is Γ(4 + α)t
3/6 (for proof see
Problem 16.8). Compute Dαf(t) at t = 0.5 numerically applying the formula (8.37), with
the derivatives f and f given by Eqs. (8.38), and compare the result with the exact value
of it.
The formula (8.37) contains Γ(2−α) and the exact solution contains Γ(4+α). These values
to be computed by numerically evaluating the integral in (8.31) for 0 <α< 1. Using the
composite trapezoidal rule Γ(γ) is approximated as
Γ(γ) ≈ h
∞
k=1
zγ−1
k e−zk . (8.40)
In practice, the summation can be stopped, for example, when z > 10 and zγ−1
k e−zk < 10−6.
For α = 0.5 with h = 0.001, 0.0001 and 0.00001 the values of Γ(2 − α) are computed as
0.886220, 0.886226 and 0.886226, respectively. The exact value of Γ(0.5) = √π and hence
Γ(2 − α) = Γ(1.5) = 0.5Γ(0.5) = √π/2=0.886226. Therefore, the choice h = 0.0001 is
desirable for the calculation of Γ(2 − α). The exact value of Γ(4 + α) = Γ(4.5) = 3.5 × 2.5 ×
1.5 × 0.5 × √π = 11.631728. The numerically calculated value of Γ(4.5) with h = 0.0001 is
11.631728. In the numerical calculation of Γ(γ) for noninteger values of γ > 0 an appropriate
value of h has to be chosen for realizing desired accuracy.
Table 8.1 presents Dαf(t) computed at t = 0.5 for α = 0.25, 0.5, 0.75 and 1. The
calculations are done for h = 10−2, 10−3, 10−4 and 10−5. Accuracy improves with decrease
in the value of h. For h = 10−4 the numerically computed Dαf(t) is in very close agreement
with the exact value of it. Use of composite Simpson’s rules can give more accurate results
for h = 10−2 and 10−3.
In the above example, the values of Γ(γ) for noninteger values of γ are computed by
applying the composite trapezoidal rule. Certain other formulas are available to calculateFractional Order Derivatives 177
TABLE 8.1
The numerically computed Dαf(t), f(t) = t
3+α, at t = 0.5 for four values of step size h.
The exact values of Dαf(t) are also given. Dα
n and Dα
e denote the numerically computed
and the exact values, respectively, of the derivative.
α Dα
e Dα
n for
h = 10−2 h = 10−3 h = 10−4 h = 10−5
0.25 0.17261 0.17250 0.17260 0.17261 0.17261
0.50 0.24233 0.24165 0.24231 0.24233 0.24233
0.75 0.34555 0.34221 0.34536 0.34554 0.34555
1.00 0.50000 0.48522 0.49853 0.49988 0.50001
Γ(γ) for noninteger values of γ [12]. The value of Γ(n + p) for n ≥ 2 and 0 ≤ p ≤ 1 can be
calculated using the formula [12]
Γ(n + p) = n!

n +
1
2
p
2
+ p(2 − p)
12 (p−1)/2
. (8.41)
The error in Γ(n + p) is never more than 1 part in 13, 000 [12]. The accuracy will increase
with n. If Γ(1 + p) value is needed then calculate Γ(2 + p) from (8.41) and then use the
relation Γ(2+p) = (1+p)Γ(1+p). Similarly, for Γ(p) use the relation Γ(2+p) = (1+p)pΓ(p).
Calculate the values of Γ(0.5) and Γ(1.5) using (8.41) and also using (8.40) and compare
them with the exact values.
8.5.3 Formula for Dαf(t), 1 <α< 2
In the case of 1 <α< 2, Eq. (8.31) gives
Dαf(t) = 1
Γ(2 − α)
 t
0
(t − τ )
1−αf(τ )dτ, t ≥ 0. (8.42)
Due to the presence of a singularity in the integrand at τ = t, perform the integration by
parts once. The result is
Dαf(t) = 1
Γ(3 − α)

t
2−αf(0) +  t
0
(t − τ )
2−αf(τ )dτ

, t ≥ 0. (8.43)
This formula is analyzed in [11]. Approximating the integral in the above equation by the
composite trapezoidal rule gives
Dαf(t) = 1
Γ(3 − α)

t
2−αf(0)
+
h
2

t
2−αf(0) + 2
n
−1
k=1
(t − τk)
2−αf(τk)
 . (8.44)178 Numerical Differentiation
TABLE 8.2
The numerically computed Dαf(t), f(t) = t
4, at t = 0.5 and 1 for three values of step size
h. The exact values of Dαf(t) are also given. Dα
n and Dα
e denote the numerically computed
and the exact values, respectively, of the derivative.
tα Dα
e Dα
n for
h = 10−2 h = 10−3 h = 10−4
0.5 1.25 0.80661 0.80800 0.80662 0.80661
0.5 1.50 1.27662 1.27620 1.27655 1.27661
0.5 1.75 1.97916 1.96857 1.97843 1.97912
0.5 2.00 3.00000 2.94322 2.99405 2.99942
1.0 1.25 5.42620 5.42836 5.42621 5.42620
1.0 1.50 7.22163 7.21939 7.22148 7.22162
1.0 1.75 9.41451 9.39103 9.41304 9.41443
1.0 2.00 12.00000 11.88326 11.98809 11.99886
The nodes are τk = kh, k = 0, 1,...,n, h = t(= T)/n. The useful approximations for the
integer derivatives of f in the above equation are
f(0) = 1
h2 [f(0) − 2f(h) + f(2h)], (8.45a)
f(0) = 1
h3 [−f(0) + 3f(h) − 3f(2h) + f(3h)], (8.45b)
f(τk) = 1
2h3 [−f(τk − 2h)
+2f(τk − h) − 2f(τk + h) + f(τk + 2h)]. (8.45c)
Example:
Test the applicability of the formula (8.44) for the function f(t) = t
4 for 1 ≤ α ≤ 2. The
exact analytical expression of Dαf(t) is 24
Γ(5 − α)
t
4−α.
Choose the target values of t as 0.5 and 1 and h = 0.01, 0.001 and 0.00001. The computed
values of Dαf(t) for α = 1.25, 1.5, 1.75 and 2 are presented in Table 8.2 along with the
exact values of Dαf(t). The convergence of the numerical result to the exact result with
decrease in the value of h is clearly seen.
Formulas for Dαf(t) for α > 2 can be straight-forwardly obtained following the steps
used for 0 <α< 1 and 1 <α< 2.
8.6 Concluding Remarks
In the present chapter, certain formulas for computing both integer order and fractional
order derivatives are derived and tested. For more accurate result the Richardson interpo￾lation can be combined. Numerical differentiation is useful to compute the discontinuousBibliography 179
points and the edge detection in image processing and certain inverse problems occurring
in model equations. In signal processing to find the rates of change in the signals and to
remove the noise present in the signal numerical differentiation is utilized. In design engi￾neering problems numerical differentiation is applied to calculate gradients and objective
functions. To identify the local maxima and minima of an analytical function or a numerical
data set computation of derivatives is required.
8.7 Bibliography
[1] What is the real-life application of numerical differentiation? https://www.quora.
com/What-is-the-real-life-application-of-numerical-differentiation (accessed on
June 12, 2023).
[2] Applications of Numerical Differentiation and Integration in Engineering.
https://www.studocu.com/in/document/amity-university/ fundamentals-of-ma￾thematics/math-nice/59089228 (accessed on June 12, 2023).
[3] L. Debnath, Int. J. Math. Educ. Sci. Technol. 35:487, 2004.
[4] K.B. Oldham and J. Spanier, The Fractional Calculus: Theory and Applications
of Differentiation and Integration to Arbitrary Order. Dover, New York, 2006.
[5] R. Hilfer, Threshold Introduction to fractional derivatives. In Anomalous Trans￾port: Foundations and Applications. R. Kloges et al. (Eds.). Wiley-VCH, Wein￾heim, 2008.
[6] R. Herrmann, Fractional Calculus: An Introduction for Physicists. World Scien￾tific, Singapore, 2014.
[7] Z. Odibat, Appl. Math. Comput. 178:527, 2006.
[8] E. Sousa, How to approximate the fractional derivative of order 1 <α< 2?
In the Proceedings of the 4th IFAC Workshop Fractional Differentiation and its
Applications. I. Podlubny, B.M. Vinagre Jara, Y.Q. Chen, V. Feliu Batile and
I. Tejado Balsera (Eds.). 2010, Article number FDA10-019.
[9] P. Novati, Numerische Mathematik 127:539, 2014.
[10] R.B. Albadarneh, M. Zerqat and I.M. Batiha, Int. J. Pure Appl. Math. 106:859,
2016.
[11] R.B. Albadarneh, I.M. Batiha and M. Zurigat, J. Math. Comput. Sci. 16:103,
2016.
[12] M.S. Raff, The Am. Stat. 24:22, 1970.
8.8 Problems
8.1 Derive the following central-difference formulas of third derivative of f(x):
f(3) (x0) = 1
2h3 [f (x0 + 2h) − 2f (x0 + h)+2f (x0 − h)
−f (x0 − 2h)] + O 
h2180 Numerical Differentiation
and
f(3) (x0) = 1
8h3 [−f (x0 + 3h)+8f (x0 + 2h) − 13f (x0 + h)
+13f (x0 − h) − 8f (x0 − 2h) + f (x0 − 3h)] + O 
h4
.
8.2 Compute the values of f
(x0) by the two-point and four-point formulas and f(x0)
by the three-point and five-point formulas of the following functions at the spec￾ified value of x for h = 0.05 and 0.1. Also, obtain the accurate values of f and
f by Richardson extrapolation. Compute the bounds of the truncation error for
h = 0.05 and 0.1.
a) cos x, x0 = 0, x is in radian.
b) ln x, x0 = 1.
c) ex, x0 = 0.5.
d) xe−x, x0 = 0.
e) 1/(1 + x), x0 = 1.
8.3 Obtain an expression for the optimum value of h for the computation of f
(x0)
by the two-point central-difference formula.
8.4 The two-point central-difference formula for the partial derivatives fx(x0, y0) and
fy(x0, y0) of f(x, y) are given by
fx (x0, y0) = 1
2h [f (x0 + h, y0) − f (x0 − h, y0)] + O 
h2
,
fy (x0, y0) = 1
2h [f (x0, y0 + h) − f (x0, y0 − h)] + O 
h2
.
Compute fx(0.2, 0.2) and fy(0.2, 0.2) of f(x, y)=e−(x2+2y) with h = 0.05 and
0.1. Compare them with the exact values.
8.5 Compute fx(0, 0) and fy(0, 0) of f(x, y) = (1 + x) cos y with h = 0.05 and 0.1.
Compare them with the exact values.
8.6 The motion of a particle along a one-dimensional line is governed by the equation
dx/dt = v0 + gt, where v0 is the initial velocity of the particle and g is the
acceleration due to gravity at the surface of the earth. The position of the particle
measured at three instants of time (in sec) are x(1) = 5.4 m, x(1.1) = 6.479 m and
x(1.2) = 7.656 m. Use the two-point central-difference formula for the calculation
of dx/dt at t = 1.1. Compute v0. Assume that g = 9.8 m/sec2.
8.7 The equation of motion of a linear harmonic oscillator is given by (in dimensionless
form) x+ω2
0x = 0. Following table gives the values of x (in metre) for five values
of t (in sec). Using the five-point formula compute the second derivative at t = 0.2.
Then, determine the value of the parameter ω2
0.
t 0 0.1 0.2 0.3 0.4
x 1 0.98 0.921 0.825 0.697
8.8 The position x(t) values (in metre) of a particle executing a simple harmonic
motion at five values of t (in sec) are given below. Using the four-point central￾difference formula compute the first derivative x (velocity) at t = 0.2. Then,
determine the constant energy E of the particle, where E = 1
2 (x
)2 + x2.Problems 181
t 0.0 0.1 0.2 0.3 0.4
x 1.0 0.995 0.98 0.955 0.921
8.9 The state equation of an LC circuit in dimensionless form is given by d2v/dt
2 +
(1/LC)v = 0. The following table gives the values of v(t) measured at five values
of t with C = 1. Compute the value of L.
t 1.0 1.1 1.2 1.3 1.4
v 0.5403 0.4536 0.3624 0.2675 0.17
8.10 Obtain a six-point central-difference formula for the first-order derivative f
(x0)
with sixth-order accuracy in h.
8.11 For f(t) = t
2−t the exact value of D1/2f(t) = 8
3
t3
π − 2
 t
π . Compute D1/2f(t)
at t = 0.5 and compare it with the exact value of it for h = 0.01, 0.001 and 0.0001.
8.12 Compute D1/2f(t) for f(t) = t
2 for t ∈ [0.1, 0.5] with ∆t = 0.1 and h = 0.0001.
The exact value of D1/2f(t) is 2t
3/2/Γ(5/2).
8.13 The exact value of D1/2(t) is 2t/π. Find the value of D1/2(t) for t ∈ [0.1, 0.5]
with h = 0.0001.
8.14 The exact value of D3/2t
2 is 4t/π. Compute D3/2t
2 for t ∈ [0.1, 0.5] with
∆t = 0.1 and for h = 0.0001. Compare the numerical result with the exact result.
8.15 The approximate solution of D1.9x(t)+x(t) = (2/Γ(3−α))t
2−α +t
3 for x(0) = 0,
x
(0) = 0 is x(t) ≈ t
2. Verify this numerically computing the fractional order
derivative.9
Numerical Minimization of Functions
9.1 Introduction
For a function f(x) a local minimum is a value of x at which the function attains a lowest
value in a certain region of it. The local maximum is a value of x where f(x) reaches the
highest value in a certain region of it. A function can have more than one minimum and
maximum. The concept of minimum and maximum of a function was introduced by Sir
Issac Newton. Consider the function f(x) = sin x, 0 ≤ x ≤ 2π. The minimum value of sin x
is −1 and this happens at x = 3π/2 while at x = π/2 it takes the maximum value 1. For
every value of x in the interval [0, 2π] the function sin x assumes a value between [−1, 1].
The concept of minimum and maximum of a function has real-life applications in physics,
engineering, economics and business management [1-3]. In the case of a classical particle
in a potential V (x) the minima of V are the stable equilibrium points and the maxima
are the unstable equilibrium points. Certain coupled oscillators and networks of oscillators
show multiple resonance and anti-resonance (where the amplitude A of oscillation is min￾imum). An anti-resonance with A = 0 means suppression of oscillation. It is of interest to
determine the values of the parameters at which resonance and an anti-resonance occur and
the conditions for their occurrence. Bubbles tend to acquire a spherical form to minimize
the shape. Atoms occupy position that correspond to minimum elastic potential energy.
To reduce the cost and the pump sizes the designing of piping systems requires details of
minimizing pressure drops. On the other hand, maximizing the strength is the basic one
in the choice of shapes of steel and concrete beams. In business management, maxima and
minima are utilized to realize a maximum profit and efficiency and minimize the effort and
expenses, respectively. With the knowledge of minimum and maximum values of the profit
function an economist is able to derive the limits of salaries of the employees to avoid loss.
One way of finding the minima and maxima of a function is to draw a graph of the
function and identify the points of minima and maxima. Mathematically, at the points of
maxima and minima f
(x) = 0. Thus, the roots of f
(x) = 0 are the points of maxima or
minima. At a maximum f(x) < 0 while f(x) > 0 at a minimum. This can be extended
to multivariable functions. For some simple functions, the maxima and minima can be
determined using the above conditions. When it is difficult to calculate the minima or
maxima of a given function, it is desirable to use a numerical approach. This chapter is
concerned with finding a minimum of single variable and multivariable functions.
9.2 Minimization of One-Dimensional Functions
This section deals with the problem of finding a minimum of one-dimensional functions.
The case of two-dimensional functions will be considered in the next section. For an
DOI: 10.1201/9781032649931-9 182Minimization of One-Dimensional Functions 183
TABLE 9.1
Result of the secant method in finding a root of f = ex−1 − e−x+1 = 0.
Iteration x0 x1 x2 f
(x2)
number
0 0.200000 0.300000
1 0.200000 0.300000 0.885678 −0.229142e+00
2 0.300000 0.885678 0.989871 −0.202576e−01
3 0.885678 0.989871 0.999976 −0.479518e−04
4 0.989871 0.999976 1.000000 −0.821820e−09
n−dimensional function the standard approach is the numerical implementation of con￾ditions for a function to be minimum at a point. A straight forward (noniterative) and an
iterative method for both one- and two-dimensional functions are discussed in the following.
These methods can be extended to higher-dimensional functions also.
Consider a function f(x) defined in the interval x ∈ [a, b] with its value (i) decreasing
(increasing) on [a, x∗] and (ii) increasing (decreasing) on [x∗, b], where a<x∗ < c. The
point x∗ is then a local minimum (maximum) of f(x). At a local minimum or maximum
f
(x)|x=x∗ = 0. When f(x∗) > 0 then f(x) is minimum at x = x∗. The function f(x) is
maximum at x∗ if f(x∗) < 0. The test is inconclusive if f(x∗) = 0.
9.2.1 Method by Solving f
(x)=0
The point of minimum or maximum can be calculated by solving f
(x) = 0. That is, the
roots of f
(x) = 0 are the minima or maxima of f(x). A root of f
(x) = 0 can be computed
numerically by employing one of the methods discussed in Chapter 3. Then, the sign of f
can be used to identify whether x∗ corresponds to a minimum or maximum of f(x).
Example:
Determine numerically a local minimum of f(x)=ex−1 + e−x+1.
The first derivative of the given function is f
(x)=ex−1 − e−x+1 and f = ex−1 + e−x+1
(which is always positive). To find a root of the equation f
(x) = 0 by the secant method
(see Section 3.4) two starting values x0 and x1 are required and they need not enclose a
root. Choose x0 = 0.2 and x1 = 0.3. To stop the iteration in the secant method choose
the tolerances as δx = 10−2, δf = 10−5, δs = 10−5. Table 9.1 presents the successive
approximate root (x2). At the end of the 4th iteration, |x2 −x1| < δx. A root of f
(x) = 0 is
obtained as 1 which is an exact root. Since, f(x∗ = 1) = 2 > 0 the function f is minimum
at x = x∗ = 1.
9.2.2 Method by Enclosing the Minimum
Consider a starting value x0, a<x0 < b. The point x0 is left to x∗ if f
(x0) < 0 otherwise
right to x∗. Identify two more points x1 = x0 + ∆x and x2 = x0 + 2∆x such that
f(x0) > f(x1) and f(x1) < f(x2). (9.1)184 Numerical Minimization of Functions
x2
x1
x0
(b)
x
f
x2
x1
x0
(a)
x
f
x2
x1
x0
(c)
x
f
x2
x1
x0
(d)
x
f
FIGURE 9.1
(a) Points x0, x1 and x2 satisfying the condition (9.1). (b) x0, x1 and x2 are all left to
the minimum of f(x) with f(x0) > f(x1) and f(x1) > f(x2). (c) x0 and x1 are such that
f(x0) ≤ f(x1), x1 is right to x∗. (d) x0 is right to the minimum of the function in which
case the initial ∆x has to be set −1.
To realize the condition (9.1) a suitable ∆x has to be found. Initially choose ∆x = 1 if x0
is left to x∗ and analyze the following three cases.
Case (i)
The condition (9.1) is satisfied. Then, x0, x1 and x2 are the desired points (Fig. 9.1a).
Case (ii)
f(x0) > f(x1) > f(x2). In this case x2 < x∗ (Fig. 9.1b). Double the value of ∆x, redefine
the values of x1 and x2 and start again from the case (i).
Case (iii)
f(x0) ≤ f(x1). Now, x1 is right to x∗ (Fig. 9.1c) and ∆x is too large. Reduce ∆x by a
factor of 2 and repeat the above process. When case (i) is realized, the three points x0, x1
and x3 are enclosing a minimum of f(x), that is, a root of f
(x) = 0. Then, construct a
second-order Newton interpolation polynomial P2(x) for the data set (x0, f0), (x1, f1) and
(x2, f2). The root of P
2(x) = 0, denoted as x∗
1, is an approximation of x∗. Next, with x0 = x∗
1
and updated ∆x repeat the above process and obtain successive approximation of x∗. Stop
the iterative process when, say, |∆x| < 0.001. If the initial x0 lies right to the minimum of
the function (Fig. 9.1d) then ∆x = −1 and the above three cases are to be analyzed. ForMinimization of One-Dimensional Functions 185
the data set (x0, f0), (x1, f1) and (x2, f2) the Newton polynomial is (see Section 6.2.3)
P2 = a1 + a2 (x − x0) + a3 (x − x0) (x − x1)
= f0 + (f1 − f0)
∆x (x − x0) + f2 − 2f1 + f0
2(∆x)2 (x − x0) (x − x0 − ∆x). (9.2)
The root of the equation P
2(x) = 0 (which is first-order in x) gives
x∗
1 = x0 +
∆x
2 − (f1 − f0)∆x
f2 − 2f1 + f0
= x0 +
∆x
2
f2 − 4f1 + 3f0
f2 − 2f1 + f0

. (9.3)
Example:
Determine numerically a local minimum of f(x)=ex−1 + e−x+1 by the method of enclosing
the minimum.
A Python program is developed to find a minimum of the given function. This program is
used to find successive approximation of x∗. The following is the result of the execution of
the program with the choice x0 = −0.6.
x0 = −0.6, f
(x0) = −4.75114. x0 is left to the point x∗.
First Iteration
∆x = 1, x0 = −0.60000, x1 = 0.40000, x2 = 1.40000,
f0 = 5.15493, f1 = 2.37093, f2 = 2.16214.
f0 > f1 and f1 > f2. Double the value of ∆x.
∆x = 2, x0 = −0.60000, x1 = 1.40000, x2 = 3.40000,
f0 = 5.15493, f1 = 2.16214, f2 = 11.11389.
f0 > f1 and f1 < f2. x0, x1 and x2 are correct choice.
x∗
1 = 0.90111, f ∗ = 2.00979, f ∗ = −0.19810, f ∗ = 2.00979.
Second Iteration
∆x = 2, x0 = 0.90111, x1 = 2.90111, x2 = 4.90111,
f0 = 2.00979, f1 = 6.84275, f2 = 49.47771. f0 ≤ f1. Reduce ∆x.
∆x = 1, x0 = 0.90111, x1 = 1.90111, x2 = 2.90111,
f0 = 2.00979, f1 = 2.86846, f2 = 6.84275. f0 ≤ f1. Reduce ∆x.
∆x = 0.5, x0 = 0.90111, x1 = 1.40111, x2 = 1.90111,
f0 = 2.00979, f1 = 2.16306, f2 = 2.86846. f0 ≤ f1. Reduce ∆x.
∆x = 0.25, x0 = 0.90111, x1 = 1.15111, x2 = 1.40111,
f0 = 2.00979, f1 = 2.02288, f2 = 2.16306. f0 ≤ f1. Reduce ∆x.
∆x = 0.125, x0 = 0.90111, x1 = 1.02611, x2 = 1.15111,
f0 = 2.00979, f1 = 2.00068, f2 = 2.02288.
f0 > f1 and f1 < f2. x0, x1 and x2 are correct choice.
x∗
2 = 0.99997, f ∗ = 2.00000, f ∗ = −0.00006, f ∗ = 2.00000.
Third Iteration
∆x = 0.125, x0 = 0.99997, x1 = 1.12497, x2 = 1.24997,
f0 = 2.000000, f1 = 2.01564, f2 = 2.06281. f0 ≤ f1. Reduce ∆x.186 Numerical Minimization of Functions
······
∆x = 0.00195..., x0 = 0.99997, x1 = 1.00193, x2 = 1.00388,
f0 = 2.000000, f1 = 2.00000, f2 = 2.00002. f0 ≤ f1. Reduce ∆x.
∆x = 0.000976... . |∆x| < 0.001. Iteration is stopped.
The point of minimum is x∗ = 0.99997.
The choice x0 = 1.6 gives f = 1.27331 and hence it is right to the point x∗ and the
initial value of ∆x = −1. The first iteration gives ∆x = −1, x∗
1 = 1.01110, f ∗ = 2.00012,
f ∗ = 0.02219 and f ∗ = 2.00012. The second iteration gives ∆x = −0.015625, x∗
2 = 1.0,
f ∗ = 2.0, f ∗ = 0.0 and f ∗ = 2.0. In the third iteration before getting a desired set of
points x0, x1 and x2 the value of ∆x becomes less than the tolerance value 0.001. Hence,
the given function is minimum at x∗
2 = 1.0 which is identical to the exact x∗.
9.3 Minimization of Two-Dimensional Functions
This section discusses two methods of minimization of two-dimensional functions. They can
be extended to multivariable functions. Minimization of a function forms a major role in
the numerical algorithm of computing transmission probability of scattering.
9.3.1 Using Derivatives
Let us consider a two-dimensional function f(x, y) and define
fx = ∂f
∂x, fy = ∂f
∂y , fxx = ∂2f
∂x2 , fyy = ∂2f
∂y2 , fxy = ∂2f
∂x∂y , (9.4)
∆2 = 
fxxfyy − f 2
xy
|(x∗,y∗) , (9.5)
where (x∗, y∗) is a root of the system of equations
fx(x, y)=0, fy(x, y)=0 . (9.6)
Then, (x∗, y∗) is a local minimum if
∆2 > 0 and fxx(x∗, y∗) > 0 . (9.7)
(x∗, y∗) is a local maximum if
∆2 > 0 and fxx(x∗, y∗) < 0 . (9.8)
When ∆2 < 0 then f(x, y) does not possess a local maximum at (x∗, y∗). The test is
inconclusive when ∆2 = 0. The above mathematical process of determining a minimum or
a maximum of f(x, y) can be used for numerical computation of the same.
Starting from an initial guess (x0, y0) of a minimum point of f(x, y) one can find the
root (x∗, y∗) of the two-coupled Eqs. (9.6), which is generally nonlinear, by the Newton–
Raphson method discussed in Section 3.5.10. Then, computing ∆2 and fxx whether (x∗, y∗)
is a minimum can be identified.
Example:
Compute the point (x∗, y∗) at which f(x, y) = sin x cos y is minimum.Minimization of Two-Dimensional Functions 187
TABLE 9.2
Result of the Newton–Raphson method in finding a root of fx = cos x cos y = 0 and fy =
− sin x sin y = 0 with (x0, y0) = (1.2, 2.5).
Iteration x y fx fy
number
0 1.200000 2.500000 —– —–
1 1.861534 3.439150 0.274062 0.280881
2 1.531399 3.102196 −0.039356 −0.039356
3 1.570878 3.141674 0.000082 0.000082
4 1.570796 3.141593 0.000000 0.000000
5 1.570796 3.141593 0.000000 0.000000
For the given function
fx = cos x cos y, fy = − sin x sin y, fxx = − sin x cos y,
fyy = − sin x cos y, fxy = − cos x sin y,
∆2 = 
sin2 x cos2 y − cos2 x sin2 y

|(x∗,y∗) ,
Table 9.2 shows the successive approximation of (x∗, y∗) after each iteration in the Newton–
Raphson method, where (x0, y0) = (1.2, 2.5). After 5 iterations the desired root of fx = 0
and fy = 0 is (x∗, y∗) = (1.57080, 3.14159). At this point ∆2 = 1 and fxx = 1. Hence,
(x∗, y∗) = (1.57080, 3.14159) is a point of minimum of the given function. The exact value
of (x∗, y∗) is (π/2, π).
9.3.2 Nelder–Mead Simplex Method
Now, outline the Nelder–Mead simplex method [4-7] for finding a minimum of a function of
more than one independent variable. Let us consider a continuous function F of the form
F = F (x1, x2,...,xN ). (9.9)
The Nelder–Mead method requires only function evaluations to find a minimum of F.
For simplicity, let us describe the method for a function of two variables x, y, that is,
F(x, y). The method starts with a triangular simplex. A simplex is a geometrical figure
consisting, in N-dimensions, of N + 1 points (or vertices) and all their interconnecting line
segments. For a function of two variables, a simplex is a triangle. For a given initial three
vertices of a triangle, the function values at these points are evaluated. The vertex at which
F(x, y) is the largest among the three vertices is called the worst vertex . Replace the worst
vertex by a new vertex and form a new triangle. Continue this process until a minimum
point is reached. The method essentially consists of six steps.
Step-1: The initial best-good-worst points
Evaluate the value of the function F(x, y) at each of the three given points (initial guess),
Xk = (xk, yk), k = 1, 2, 3. (For a function of N variables, the number of initial points is
N + 1.) Rearrange the points (xk, yk) so that F(x1, y1) ≤ F(x2, y2) ≤ F(x3, y3). Use the
notations B(best)= (x1, y1), G(good)= (x2, y2) and W(worst)= (x3, y3).188 Numerical Minimization of Functions
Step-2: Mid-point of the line segment BG
Compute the mid-point M of the line segment joining the best and the good vertices:
M = (B + G)/2 = [(x1 + x2) /2,(y1 + y2) /2]. (9.10)
Step-3: Computation of a test point R
Find a test point R as follows: From the mid-point M draw the line segment from W to
M. The distance WM is d. Extend the last segment by a distance d through M to locate
the point R as shown in Fig. 9.2a. The point R is given by R = 2M − W.
Step-4: Expansion using the point E
If F(R) < F(W) then the movement is in the right direction towards the minimum. The
minimum is faster than the point R. Therefore, a point E is formed as shown in Fig. 9.2b:
E = 2R − M. If F(E) < F(R) then E is a better vertex than R.
Step-5: Contraction using the point C
If F(R) = F(W) then test another point. For this purpose, consider two points C1 and C2
of the line segments WM and MR as shown in Fig. 9.2c: Call the point for which F is
smaller as C. The new triangle is then BGC. The contraction is along one direction only.
Step-6: Shrink towards B
A contraction along all dimensions towards the best vertex B can be achieved as follows.
If F(C) < F(W), the points G and W must be shrunk toward B (see Fig. 9.2d). Define a
point S as the mid-point of BW. Replace the point G with M and W with S.
The logical steps to stop the process are as follows. If F(R) < F(G) then analyze the
following case (i) otherwise the case (ii).
Case (i)
It performs refection (step 3) or expansion (step-4). If F(B) < F(R) then W is replaced with
R. Otherwise, E is computed. Then W is replaced with E if F(E) < F(B). If F(E) ≥ F(B),
W is replaced with R.
Case (ii)
It performs contraction (step 5) or shrunk (step 6). Replaced W with R if F(R) < F(W).
Otherwise, compute C. When F(C) < F(W), replace W by C. If this condition is not
satisfied then compute S. Next, replace W by S and G by M.
Example:
Compute the minimum point of the function F(x, y) = x2 + y2.
Table 9.3 gives the values of the three vertices and the function values obtained successively
by the Nelder–Mead method. M = 0 corresponds to the initial vertices. At the end of 8th
iteration the termination criterion is satisfied and the process is stopped. The minimum
coordinate obtained is (x, y) = (0, 0) while the exact result is (0, 0).
9.4 Concluding Remarks
In this chapter, numerical methods for computing minima of mathematical functions are
described. For certain simple functions minima and maxima can be determined analytically.Concluding Remarks 189
W
B
R
G
W M E
B
R
G
W C1 M R
B
C2
G
M
W
B
S
G
d
M (a)
(b)
(c)
(d)
FIGURE 9.2
Possible outcomes of the Nelder–Mead simplex method. (a) Reflection away from the worst
point W. (b) The triangle BGW, point R and the extended point E. This is a reflection
and expansion away from the worst point. (c) A contraction along one-dimension from the
worst point. (d) A contraction along all dimensions towards the best point.
When the function is complicated then it is desirable to use numerical methods to compute
minima of it. In some physical phenomena minima of a quantity are beneficial. In certain
events maxima of a quantity are useful. In the tunnel effect in quantum mechanics, the
transmission amplitude T as a function of energy E of the incident particle shows a series
of maxima and minima with Tmax = 1 corresponds to the maximum value of T. In some190 Numerical Minimization of Functions
TABLE 9.3
The values of the function F = x2 + y2 at various triangles.
M Point x y F
0 Best −0.010000 −0.010000 0.200000e−03
Good 0.010000 0.010000 0.200000e−03
Worst 0.020000 0.020000 0.800000e−03
1 Best −0.010000 −0.010000 0.200000e−03
Good 0.010000 0.010000 0.200000e−03
Worst 0.010000 0.010000 0.200000e−03
2 Best 0.000000 0.000000 0.000000e+00
Good 0.010000 0.010000 0.200000e−03
Worst 0.010000 0.010000 0.200000e−03
3 Best 0.000000 0.000000 0.000000e+00
Good −0.005000 −0.005000 0.500000e−04
Worst 0.010000 0.010000 0.200000e−03
4 Best 0.000000 0.000000 0.000000e+00
Good −0.005000 −0.005000 0.500000e−04
Worst 0.003750 0.003750 0.281250e−04
5 Best 0.000000 0.000000 0.000000e+00
Good −0.001563 −0.001563 0.488281e−05
Worst 0.003750 0.003750 0.281250e−04
6 Best 0.000000 0.000000 0.000000e+00
Good −0.001563 −0.001563 0.488281e−05
Worst 0.001484 0.001484 0.440674e−05
7 Best 0.000000 0.000000 0.000000e+00
Good −0.000410 −0.000410 0.336456e−06
Worst 0.001484 0.001484 0.440674e−05
8 Best 0.000000 0.000000 0.000000e+00
Good −0.000410 −0.000410 0.336456e−06
Worst 0.000640 0.000640 0.818300e−06
systems, occurrence of resonance is to be avoided. At resonance, a system can absorb more
energy and may develop violent vibrations leading to disaster of bridges, building and air￾planes. In designing such constructions the engineers wish to know the details of conditions
of such resonance where the amplitude of vibrations become maximum and it is desirable
to have a minimum amplitude of vibrations.
9.5 Bibliography
[1] M. Lakshmanan and S. Rajasekar, Nonlinear Dynamics: Integrability, Chaos and
Patterns. Springer, Berlin, 2002.Problems 191
[2] S. Rajasekar and M.A.F. Sanjuan, Nonlinear Resonances. Springer, Berlin, 2016.
[3] J.R. Maddocks, Maxima and Minima: Applications https://science.jrank.org/pag
es/4186/Maxima-Minima.html (accessed on June 18, 2023).
[4] J.H. Mathews, Numerical Methods for Mathematics Science and Engineering.
Prentice-Hall of India, New Delhi, 2005.
[5] J.A. Nelder and R. Mead, Comput. J. 7:308, 1965.
[6] S. Singer and J. Nelder, Nelder-Mead Algorithm, Scholarpedia.
http://var.scholarpedia.org/article/Nelder-Mead−algorithm.
[7] H.P. Gavin, The Nelder-Mead algorithm in two-dimensions. https: //people.duke.
edu/∼hpgavin/SystemID/CourseNotes/Nelder-Mead-2D.pdf.
9.6 Problems
9.1 Find a minimum or a maximum of f(x) = x sin x cos x by computing the root of
f = 0 with the starting values a) (x0, x1) = (1.5, 2.5), b) (x0, x1) = (1.1, 1.2) and
c) (x0, x1) = (4, 5).
9.2 The function f(x) = xe−x2
has a local minimum and a maximum. Determine
them by solving f = 0.
9.3 Compute all the points of minima and maxima of f(x) = x6+x4−10x2 by finding
the roots of f = 0.
9.4 Find the minimum of the function f(x) = 10 +x2 + e−x2
with the starting values
x0 = 1 and x1 = 0.8 by solving the equation f = 0.
9.5 In the method of enclosing the minimum verify that instead of Newton interpolat￾ing polynomial, if a second-order Lagrange interpolating polynomial is considered
then the expression for x∗
1 is identical to Eq. (9.3).
9.6 Determine a minimum of the function f = −e−x2
by the method of enclosing a
minimum with the starting point x0 = 0.1.
9.7 Determine the minimum of the function f = xe−x2
by the method of enclosing a
minimum with the starting point x0 = 0.1.
9.8 By computing a root of the first partial derivatives find the minimum of the
following functions. Use the given initial guess in the Newton–Raphson method.
a) f(x, y)=e−x2
cos y, (x0, y0) = (0.5, 2.5).
b) f(x, y) = −e−x2−y2
, (x0, y0) = (0.3, 0.3).
9.9 Determine the minimum or maximum of the function f(x, y) = x4 − x2 + y4 −
y2 − x2y2 with the initial guesses (x0, y0)=(±0.8, ±0.8), (0.2, 0.2) and (0, 0.7)
by finding a root of its first derivatives.
9.10 a) Compute the local minimum of the function f(x, y)=e−x2
cos y by the Nelder–
Mead simplex method with the vertices (0, 2.9), (−0.1, 3.0) and (0.2, 3.1).
b) Compute the local minimum of the function f(x, y) = −e−x2−y2
by the Nelder–
Mead simplex method with the vertices (0, 0.1), (−0.1, 0.1) and (0.2, 0.0).10
Numerical Integration
10.1 Introduction
Integration of a function f(x) with one variable is defined as
g(x) = 
f(x) dx . (10.1)
Similarly, integration of a multi-variable function with more than one variable can be de￾fined. Because of dg/dx = f(x) an integration of a function f(x) is equivalent to finding the
function, the derivative of which is f(x). Integrals are broadly classified into two classes:
1. definite integrals and
2. indefinite integrals.
An integral of the form given by Eq. (10.1) is called an indefinite integral. Here, when
the function f(x), called the integrand, is integrated a new function g(x) is obtained. The
general form of definite integrals is
I =
 b
a
f(x) dx = g(x)


x=b
x=a = g(b) − g(a). (10.2)
In this case, the final result is not a function of x but is a number. What does the integral
 b
a
f(x)dx represent? The integration of a function f(x) in an interval [a, b] represents the
area under the curve in this interval.
Integrating a function or a set of data one can get an estimate of rates of change, area
under the curves, volume, etc. In finite-element analysis, integrals arise in the discretization
of concerned equations and are to be evaluated numerically. In engineering optimization
techniques numerical integrations of objective functions are helpful to obtain optimal so￾lutions for appropriate designs. In electrical circuits studies integration methods are used
to find power dissipation in the circuits. Oscillation period of certain mechanical systems
and electrical circuits, electric and magnetic potentials at a point, the centre of mass of an
object and kinetic energy are expressed as integrals of certain quantities. For details see
Problems 14–25 at the end of the present chapter. Calculations of average of quantities and
functions over an interval, Fourier coefficients and probability of an event in an interval
and normalization of quantum mechanical wave function of a system involve evaluation
of certain definite integrals. In data analysis, numerical integration is employed to get an
appropriate distribution of sample data.
Certain types of integrals can be evaluated employing direct methods or employing tech￾niques such as Laplace and Fourier transforms and complex variable analysis (for example,
see the refs. [1–3]). One may wish to evaluate a given integral  b
a
f(x)dx numerically when
DOI: 10.1201/9781032649931-10 192Newton–Cotes Methods 193
solving it exactly by analytical methods is not possible or difficult or f(x) is known only
at a finite number of values of the variable x. For example, the x-component of a magnetic
field Bx at a point due to a hollow circular wire is given by
Bx =
 b
a
f(θ) dθ = µ0I
2π
 2π
0
2 − sin θ
5 − 4 sin θ dθ. (10.3)
Because of the complexity of f(θ), one may wish to evaluate it numerically.
What is numerical integration? The general formula of all the numerical integration
algorithms assumes the form
I =
 b
a
f(x) dx = n
i=0
Wif (xi). (10.4)
The formula given by Eq. (10.4) is called numerical integration or quadrature. In Eq. (10.4)
the quantities xi and Wi, i = 0, 1,...,n are called quadrature nodes or simply nodes and
weights, respectively. I is an approximate value of the integral. The error in the approxi￾mation is given by
E =
 b
a
f(x) dx − I . (10.5)
This chapter presents some of the methods of numerical integration of definite integrals
including fractional order integrations.
10.2 Newton–Cotes Methods
The quadrature formula, Eq. (10.4), is derived from Lagrange polynomial interpolation. As
shown in Section 5.6 a unique polynomial P(x) of degree ≤ n passing through the n+1 nodes
or sample points xi, i = 0, 1,...,n can be constructed. Thus, the function f(x) over [a, b]
can be approximated by the polynomial P(x). Then, the integral of f(x) is approximated
by the integral of P(x). The resulting formula is called a Newton–Cotes quadrature formula.
10.2.1 Newton–Cotes Quadrature Formula
Let us assume that the integrand f(x) is sufficiently smooth in the interval [a, b] and
x0, x1,...,xn be n + 1 distinct points in the interval, say, [a, b]. Then, referring to Sec￾tion 6.4 the Lagrange polynomial interpolation formula for the function f(x) is written as
f(x) = Pn(x) +n
i=0
Li(x)f (xi) + Rn , (10.6a)
where
Li(x) = n
j=0
j i
(x − xj )
(xi − xj )
, i = 0, 1, . . . , n. (10.6b)
Here, Rn is the error in the polynomial approximation and is given by (refer Section 6.4)
Rn = 1
(n + 1)!ψn(x)f(n+1)(x), (10.6c)194 Numerical Integration
where
ψn(x) = n
i=0
(x − xi). (10.6d)
Substitution of Eq. (10.6a) for f(x) in Eq. (10.4) with I = In gives
In = n
i=0
f(xi)
 b
a
Li(x) dx +
 b
a
Rn(x) dx . (10.7)
Defining
Wi =
 b
a
Li(x) dx, En =
 b
a
Rn dx (10.8)
Eq. (10.7) is rewritten as
In = n
i=0
Wif (xi) + En. (10.9)
Neglecting the error term En Eq. (10.9) becomes
In = n
i=0
Wifi, fi = f(xi). (10.10)
When the nodes xi are equally spaced in the interval [a, b] the formula given by Eq. (10.10)
is called Newton–Cotes quadrature and the numbers Wi are called Cotes numbers.
10.2.2 Error in the Newton–Cotes Formula
From Eqs. (10.6c) and (10.8) the error in the approximation given by Eq. (10.10) is obtained
as
En = 1
(n + 1)!  b
a
f(n+1)(x)ψn(x) dx . (10.11)
As shown below the above integral can be simplified using the intermediate-value theorem
for integrals (see Section 1.6). According to this theorem if ψn(x) in Eq. (10.11) does not
change in sign on [a, b] then
En = 1
(n + 1)!f(n+1)(x
)
 b
a
ψn(x) dx, x ∈ [a, b] . (10.12)
If the function ψn(x) changes sign in [a, b] then En is given by the next higher-order term
of f(x
),
En = 1
(n + 2)!f(n+2)(x
)
 b
a
ψn+1(x) dx (10.13)
provided the sign of ψn+1 remains same in [a, b], and so on. For polynomials of degree less
than n + 2 the function f(n+2)(x) is zero and hence En = 0. Therefore, the formula given
by Eq. (10.10) is exact for polynomials of degree ≤ n + 1.
10.2.3 Some Newton–Cotes Formulas
Let us proceed to obtain the Newton–Cotes formulas for first few values of n. Set x0 = a,
xn = b and choose the spacing between the nodes as h = (b − a)/n. Then, xi’s are given by
xi = x0 + ih , i = 0, 1, 2, . . . , n. (10.14)
Figure 10.1 shows the representation of the nodes xi, i = 0, 1, 2,...,n on the interval [a, b]
for n = 1, 2, 3.Newton–Cotes Methods 195
a b h = (b a)/3
n = 3:
x0 x1 x2 x3
a h = (b a)/2 b
n = 2:
x0 x1 x2
a h = (b a) b
n = 1:
x0 x1
FIGURE 10.1
Representation of the nodes in the interval [a, b] for the integral  b
a f(x)dx for n = 1, 2, 3. n
is the number of subintervals in [a, b].
(i) n = 0: (a) Rectangle Rule
For n = 0, Eq. (10.10) becomes
I0 = W0f0 . (10.15)
From Eqs. (10.8) and (10.6b) W0 is evaluated as
W0 =
 b
a
L0(x) dx =
 b
a
dx = b − a . (10.16)
Thus, for x0 = a
I0 = (b − a)f0. (10.17)
The quantity (b − a)f0 in the above formula represents the area of rectangle of length
(b − a) and breadth f0 and hence the name rectangle rule. To determine the error in the
approximation consider the function ψ0(x). Since ψ0(x)=(x − x0)=(x − a) > 0 the error
is given by Eq. (10.12). E0 is calculated as
E0 = f
(x
)
 b
a
ψ0(x) dx = f
(x
)
 b
a
(x − x0) dx = 1
2
f
(x
)(b − a)
2, (10.18)
where x ∈ [a, b].
(b) Mid-Point Rule
For the choice x0 = (a+b)/2 the midpoint of the interval [a, b], W0, is again (b−a). However,
I0 = (b − a)f ((a + b)/2) (10.19)
and is called the mid-point rule. The function ψ0 = (x − x0)=(x − (a + b)/2) is ≤ 0 for
a ≤ x ≤ (a+b)/2 and > 0 for (a+b)/2 <x<b. That is, ψ changes sign in [a, b]. Therefore,
the error term is not given by Eq. (10.12). For x1 = x0, ψ(x)=(x−x0)(x−x1)=(x−x0)2
does not change its sign in [a, b] and hence the error is given by Eq. (10.13). E0 is then
evaluated as
E0 = 1
2
f(x
)
 b
a
(x − x0)
2 dx = 1
6
(b − a)
3f(x
). (10.20)196 Numerical Integration
(ii) n = 1: Trapezoidal Rule
For n = 1 the nodes are x0 = a, x1 = b and h = b − a. Now, Eq. (10.10) becomes
I1 = W0f0 + W1f1 . (10.21)
W0 and W1 are evaluated as
W0 =
 b
a
L0(x) dx = (b − a)/2 = h/2 , (10.22a)
W1 =
 b
a
L1(x) dx = (b − a)/2 = h/2. (10.22b)
Then,
I1 = 1
2
h (f0 + f1) (10.23)
and is called trapezoidal rule. The function ψ1(x)=(x − x0)(x − x1)=(x − a)(x − b) is
always negative in the interval [a, b] and therefore the error term E1 is given by Eq. (10.12).
E1 is estimated as
E1 = 1
2!f(2)(x
)
 b
a
ψ1(x) dx
= 1
2
f(2)(x
)
 b
a
(x − x0) (x − x1) dx
= − 1
12
h3f(2)(x
). (10.24)
The rule is exact for linear function of x.
(iii) n = 2: Simpson’s 1/3-Rule
Next, consider the case n = 2. The nodes are x0 = a, x1 = (a+b)/2, x2 = b and h = (b−a)/2.
From (10.10)
I2 = W0f0 + W1f1 + W2f2 . (10.25)
The weight factors are worked out as
W0 =
 b
a
L0(x) dx
=
 b
a
(x − x1) (x − x2)
(x0 − x1) (x0 − x2)
dx
= 2
(a − b)2
 b
a
(x − (a − b)/2) (x − b) dx
= 1
3
h (10.26a)
and
W1 =
 b
a
L1(x) dx =
 b
a
(x − x0) (x − x2)
(x1 − x0) (x1 − x2)
dx = 4
3
h , (10.26b)
W2 =
 b
a
L2(x)dx =
 b
a
(x − x0) (x − x1)
(x2 − x0) (x2 − x1)
dx = 1
3
h . (10.26c)Newton–Cotes Methods 197
Then,
I2 = 1
3
h (f0 + 4f1 + f2) . (10.27)
The above formula is known as Simpson’s 1/3-rule. To estimate the error term first check
whether the sign of ψ2(x) is changing in [a, b] or not. ψ2 is given by
ψ2(x)=(x − x0) (x − x1) (x − x2)=(x − a) (x − (a + b)/2) (x − b).
ψ2 > 0 for a<x< (a + b)/2 while it is < 0 for (a + b)/2 <x<b. Therefore, consider the
next higher-order term of ψ:
ψ3(x)=(x − x0)
2 (x − x1) (x − x2) . (10.28)
In this case
E2 = 1
4!f(4)(x
)
 b
a
ψ3(x) dx
= 1
4!f(4)(x
)
 b
a
(x − x0)
2 (x − x1) (x − x2) dx
= − 1
90
h5f(4)(x
). (10.29)
The Simpson’s 1/3-rule is exact for polynomials of degree ≤ 3.
(iv) n = 3: Simpson’s 3/8-Rule
The values of the nodes for n = 3 are x0 = a, x1 = (2a + b)/3, x2 = (a + 2b)/3, x3 = b and
h = (b − a)/3. The formula for the integral  b
a
f(x)dx is
I3 = W0f0 + W1f1 + W2f2 + W3f3 . (10.30)
The W’s are computed as (verify)
W0 = 3
8
h, W1 = 9
8
h, W2 = 9
8
h, W3 = 3
8
h . (10.31)
Then,
I3 = 3
8
h (f0 + 3f1 + 3f2 + f3) (10.32)
and is called Simpson’s 3/8-rule. To compute the error in the above formula consider the
function ψ3(x):
ψ3(x)=(x − x0) (x − x1) (x − x2) (x − x3)
= [x − a] [x − (2a + b)/3] [x − (a + 2b)/3] [x − b] . (10.33)
ψ3 is < 0 for a<x< (2a + b)/3 and (a + 2b)/3 <x<b. It is > 0 for (2a + b)/3 <x<
(a + 2b)/3. That is, ψ3 changes its sign in [a, b]. Hence,
E3 = 1
5!f(5)(x
)
 b
a
ψ4(x) dx
= 11
480h6f(5)(x
). (10.34)198 Numerical Integration
Example 1:
Evaluate the integral  1
0

1 + x2
dx using the various formulas obtained in this section.
(i)(a) Rectangle Rule
With x0 = a = 0 from Eq. (10.17) I0 is computed as 1 while the exact value of the integral
is 1.33333.
(i)(b) Mid-Point Rule
With x0 = (a + b)/2 = (0 + 1)/2=0.5, I0 is estimated as 1.25.
(ii) Trapezoidal Rule
The nodes are x0 = 0 and x1 = 1. With h = 1 I = 1
2 (f0 + f1) = 1
2 (1 + 2) = 1.5.
(iii) Simpson’s 1/3-Rule
For the Simpson’s 1/3-rule x0 = 0, x1 = 0.5, x2 = 1 and h = 0.5. The function values are
f0 = 1, f1 = 1.25 and f2 = 2. Then, from Eq. (10.27)
I = 1
3
h (f0 + 4f1 + f2)=1.33333... .
(iv) Simpson’s 3/8-Rule
The nodes are x0 = 0, x1 = 1/3, x2 = 2/3 and x3 = 1. The step size is h = 1/3. The
function values are f1 = 1, f2 = 10/9, f3 = 13/9 and f4 = 2. The value of the integral is
I = 3
8
h (f0 + 3f1 + 3f2 + f3)=1.33333... .
For the given integral the Simpson’s rules are exact (why?).
Example 2:
Evaluate the integral  3
0
(1 + ex)dx by the trapezoidal and the Simpson’s rules and also
compute the percentage of relative error, Erel, using the expression
Erel = 100 × (Iexact − Inum) /Iexact ,
where Iexact is the exact value of the integral obtained by direct integration and Inum is
the value of the integral computed by an approximation method. The exact value of the
integral is 22.085537.
(i) Trapezoidal Rule
For the trapezoidal rule h = 3, x0 = 0, x1 = 3, f0 = 2 and f1 = 21.085537. The value of
the integral is
I = 1
2
h (f0 + f1) = 34.628305 .
The percentage of relative error in the calculation is 56.79.Composite Quadrature Formulas 199
(ii) Simpson’s 1/3-Rule
Now, h = 1.5, x0 = 0, x1 = 1.5, x2 = 3, f0 = 2, f1 = 5.4816891 and f2 = 21.085537. Then
I = 1
3
h (f0 + 4f1 + f2) = 22.506147 .
The percentage of relative error in the result is 1.91.
(iii) Simpson’s 3/8-Rule
For the Simpson’s 3/8-rule h = 1, x0 = 0, x1 = 1, x2 = 2, x3 = 3, f0 = 2, f1 = 3.7182818,
f2 = 8.3890561, f3 = 21.085537 and
I = 3
8
h (f0 + 3f1 + 3f2 + f3) = 22.277832 .
The percentage of relative error is 0.87.
In the above example 2 the error in the numerical integration of the given integral by the
various methods is found to be large. How does one improve these approximations? This is
considered in the next section.
Remark:
Can you apply the quadrature formulas to evaluate the integral  1
0
−2x
√1 − x2
dx where its
exact value is −1? The value of the integrand becomes −∞ at the node value x = 1.
Therefore, the quadrature formulas cannot be applied to the given integral. In general,
for an integral of the form  b
a f(x)/g(x)dx if g(x) = 0 or g(x) ≈ 0 and |f(x)|  0 or
limx→xi f(xi)/g(xi) is not finite at a node value then the quadrature formulas are not useful
to evaluate the integral even if the exact value of the integral is finite. If g(x) = 0 at the
end points and the value of the integral is finite then one may use the Gauss–Legendre (see
Section 10.4.3) or any other suitable integration method. The integral formulas discussed in
this section can be applied if the function f(x) is not known or not given explicitly but its
values are given for a set of points (see Problem 10.4 at the end of this chapter).
10.3 Composite Quadrature Formulas
In the quadrature formulas such as the trapezoidal and the Simpson’s 1/3 and 3/8 formulas
considered in the previous section the interval [a, b] is divided into 1, 2 and 3 subintervals,
respectively. Then, using the values of the function at the nodes the given integral can
be evaluated. When the interval [a, b] is large these quadrature formulas generally will not
produce sufficiently accurate estimates as is the case in example 2 in the previous section.
Further, some functions may have a large variation in a small interval and a small variation
in a wide interval. Considering the above, it is necessary to develop more accurate formulas
to evaluate the given integral. Such formulas can be obtained by dividing the interval [a, b]
into a large number of subintervals (conveniently equally spaced) and then applying a
quadrature formula to each subinterval. An integral formula obtained in this manner is
called a composite quadrature formula.200 Numerical Integration
a b
h
x0 x1 x2 xn-1 xn
1 2 n-1 n
FIGURE 10.2
Dividing of the interval [a, b] into n equally spaced subintervals for obtaining the composite
trapezoidal rule.
10.3.1 Composite Trapezoidal Rule
Let the interval [a, b] is divided into n-equally spaced subintervals as shown in Fig. 10.2.
The nodes xi are given by xi = a + ih, i = 0, 1,...,n and h = (b − a)/n. Then, applying
the trapezoidal rule to each interval gives
 b
a
f(x) dx =
 b
a
f(x)dx
=
 x1
x0
fdx +
 x2
x1
fdx + ··· +
 xn−1
xn−2
fdx +
 xn
xn−1
fdx
= 1
2
h [f0 + f1 + f1 + f2 ··· + fn−1 + fn−1 + fn]
= 1
2
h

f0 + 2
n
−1
i=1
fi + fn

. (10.35)
Formula given by Eq. (10.35) is called the composite trapezoidal rule.
Referring to Eq. (10.24) the error term for ith interval is −(h3/12)f(2)(x
i). The error
term for the composite trapezoidal rule is obtained by summing this above error term over
all the subintervals:
ECT = − 1
12
h3n
i=1
f(2)(x
i) = − 1
12n
h2(b − a)
n
i=1
f(2) (x
i) , (10.36)
where x
i is a value of x in the ith interval. Defining
f(2) (x
) = 1
n
n
i=1
f(2) (x
i) (10.37)
one has
ECT = − 1
12
h2(b − a)f(2) (x
) . (10.38)
10.3.2 Composite Simpson’s Rules
The composite Simpson’s 1/3-rule is obtained by dividing the interval [a, b] into 2n equally
spaced subintervals as shown in Fig. 10.3. The nodes xi are given by xi = a + ih, i =
0, 1,..., 2n and h = (b−a)/(2n). Then, applying the rule to each pair of subintervals givesComposite Quadrature Formulas 201
a b
h
x0 x1 x2 x2n-1 x2n
1 2 2n-1 2n
x3 x4
3 4
} 1S1/3 } 2S1/3 } nS1/3
x2n-2
FIGURE 10.3
Dividing of the interval [a, b] into 2n equally spaced subintervals for obtaining the composite
Simpson’s 1/3-rule.
 b
a
f(x) dx =
 x2n
x0
f(x) dx
=
 x2
x0
fdx +
 x4
x2
fdx + ··· +
 x2n
x2n−2
fdx
= h
3 [f0 + 4f1 + f2 + ··· + f2n−2 + 4f2n−1 + f2n]
= h
3

f0 + 4n
i=1
f2i−1 + 2
n
−1
i=1
f2i + f2n

. (10.39)
The error term for the composite Simpson’s 1/3-rule is
ECS = − 1
180(b − a)h4f(4)(x
), f(4)(x
) = 1
n
n
i=1
f(4) (x
i), (10.40)
where x
i ∈ [x2i−2, x2i].
To obtain the composite Simpson’s 3/8-rule the interval [a, b] is divided into 3n equally
spaced subintervals (refer Fig. 10.4). The nodes are xi = a + ih, i = 0, 1,..., 3n, where
a b
h
x0 x1 x2 x3n-1 x3n
1 2 3n-1 3n
x3
3
} 1S3/8 nS3/8
3n-2
} x3n-3 x3n-2
FIGURE 10.4
Dividing of the interval [a, b] into 3n equally spaced subintervals for obtaining the composite
Simpson’s 3/8-rule.202 Numerical Integration
h = (b − a)/(3n). Now,
 b
a
f(x) dx =
 x3n
x0
f(x) dx
=
 x3
x0
fdx +
 x6
x3
fdx + ··· +
 x3n
x3n−3
fdx
= 3
8
h

f0 + 3n
i=1
(f3i−2 + f3i−1) + ··· + 2
n
−1
i=1
f3i + f3n

. (10.41)
The error term in the above formula is
ECS = 11
1440h5(b − a)f(5) (x
) , f(5) (x
) = 1
n
n
i=1
f(5) (x
3i), (10.42)
where x
3i ∈ [x3i−3, x3i].
Example:
Evaluate the integral I =
 3
0
(1 + ex) dx employing the composite trapezoidal and the Simp￾son’s rules. In all the cases choose the width of each subinterval as h = 0.5.
(i) Trapezoidal Rule
For the trapezoidal rule the value of n for h = 0.5 is 6. The nodes are xi = 0+ ih,
i = 0, 1,..., 6. That is, x0 = 0, x1 = 0.5, x2 = 1, x3 = 1.5, x4 = 2, x5 = 2.5 and x6 = 3.
From Eq. (10.35)
I =
 3
0
(1 + ex) dx
= 0.5
2 [f0 +2(f1 + f2 + f3 + f4 + f5) + f6]
= 0.25 [2 + 2 {2.6487213 + 3.7182818 + 5.4816891
+ 8.3890561 + 13.182494} + 21.085537 ]
= 22.481505.
The percentage of relative error in the computation is 1.79.
(ii) Simpson’s Rules
For Simpson’s 1/3-rule, h = 0.5, 2n = 6, xi =0+ ih, i = 0, 1,..., 6. Then,
I = 0.5
3 [f0 +4(f1 + f3 + f5)+2(f2 + f4) + f6]
= 0.5
3 [2 + 4 (2.6487213 + 5.4816891 + 13.182494)
+2 (3.7182818 + 8.3890561) + 21.085537]
= 22.091972.
The percentage of error in the result is 0.03. Verify that the value of the integral by the
composite Simpson’s 3/8-rule with 3n = 6 is 22.09961.
Tables 10.1 and 10.2 present the numerically computed value of the integral as a function
of order of the composite quadrature rules. The percentage of relative error Erel is also given.Composite Quadrature Formulas 203
TABLE 10.1
Computed value of the integral  3
0
(1 + ex) dx by the composite trapezoidal rule as a func￾tion of number of equally spaced subintervals n. The exact value of the integral is 22.08554.
Integral Integral
n h value Erel n h value Erel
5 0.60000 22.65470 2.58 30 0.10000 22.10144 0.07
10 0.30000 22.22846 0.65 35 0.08571 22.09722 0.05
15 0.20000 22.14911 0.29 40 0.07500 22.09448 0.04
20 0.15000 22.12131 0.16 45 0.06667 22.09261 0.03
25 0.12000 22.10843 0.10 50 0.06000 22.09126 0.03
10.3.3 Estimation of Optimum Values of n and h in the Composite
Rules
What are the optimum values of n, nopt and the step size h, hopt so that the error in the
value of an integral  b
a
f(x)dx computed by a composite quadrature rule is less than δ?
For a chosen value of δ and a quadrature formula, the nopt and hopt can be calculated
from the error term of the formula. In the following, the expressions for these quantities for
the composite trapezoidal and the composite Simpson’s rules are obtained. Then, they are
computed for the integral  3
0
(1 + ex) dx for two values of δ.
Consider the error term given by Eq. (10.36) for the composite trapezoidal rule. Substi￾tution of h = (b − a)/n in this equation gives
ECT = − 1
12n2 (b − a)
3f(2)(x
). (10.43)
TABLE 10.2
Computed value of the integral  3
0
(1 + ex) dx by the composite Simpson’s 1/3-rule as a
function of number of equally spaced subintervals 2n. The exact value of the integral is
22.08554.
Integral Integral
2n h value Erel 2n h value Erel
2 1.50000 22.50615 1.90 12 0.25000 22.08595 0.00
4 0.75000 22.11696 0.14 14 0.21429 22.08576 0.00
6 0.50000 22.09197 0.03 16 0.18750 22.08567 0.00
8 0.37500 22.08760 0.01 18 0.16667 22.08562 0.00
10 0.30000 22.08639 0.00 20 0.15000 22.08559 0.00204 Numerical Integration
It is required to have |ECT| < δ. That is,




1
12n2 (b − a)
3f(2)(x
)




<δ. (10.44)
This equation gives
n>nopt = int.
 



1
12δ
(b − a)
3f(2)
m (x
)




1/2
, (10.45)
where int.(y) is the integer part of y and f(2)
m (x
) is the maximum value of f(2)(x
) in the
interval x ∈ [a, b]. Then, hopt = (b − a)/nopt.
For the Simpson’s 1/3-rule, Eq. (10.40) with the substitution h = (b − a)/2n becomes
ECS = − 1
2880n4δ
(b − a)
5f(4) (x
) . (10.46)
Thus,
n>nopt = int.
 



1
2880δ
(b − a)
5f(4)
m (x
)




1/4
(10.47)
and hopt = (b − a)/2nopt.
For the Simpson’s 3/8-rule nopt is given by
nopt = int.
 



11
349920δ
(b − a)
6f(5)
m (x
)




1/5
(10.48)
and hopt is (b − a)/3nopt.
Next, compute nopt and hopt for the integral  3
0
(1 + ex)dx. For this integral f(n)
(x
) =
ex
. Its maximum value in x ∈ [0, 3] is at x = 3. For the composite trapezoidal rule
nopt = int.
33e3
12δ
1/2
. (10.49)
For δ = 10−3 Eq. (10.45) gives nopt = 212 and then hopt = (b − a)/nopt = 3/212. Thus,
to have δ = 10−3 the value of n must be greater than 212. For δ = 10−5 the results are
nopt = 2125 and hopt = 3/2125.
For the composite Simpson’s 1/3-rule
nopt = int.
 35e3
2880δ
1/4
. (10.50)
The choice δ = 10−3 gives nopt = 6 while for δ = 10−5 it is 20. For the composite Simpson’s
3/8-rule
nopt = int.
 11
349920
36e3
δ
1/5
. (10.51)
δ = 10−3 leads to nopt = 3 (while for δ = 10−5 it is 8).Gauss–Legendre Integration 205
10.4 Gauss–Legendre Integration
Suppose the aim is to obtain a reasonable approximate value of an integral by evaluating
the value of the function at very few nodes, say, 2 or 3 or 4. In this case, Gauss–Legendre
(GL) formulas are extremely useful.
10.4.1 Gaussian Formula
Let us rewrite the integral  b
a
f(x)dx as
I =
 b
a
g(x)W(x) dx , (10.52a)
where g(x) is chosen as
g(x) = Pn(x) + g [x0, x1,...,xn, x] ψn(x) (10.52b)
with Pn(x) being a polynomial of degree ≤ n and
ψn(x) = n
i=0
(x − xi) . (10.53)
Now, write
I(g) = I (Pn) +  b
a
g [x0,...,xn, x] ψn(x)W(x) dx . (10.54)
Writing Pn(x) in the Lagrange form
Pn = n
i=0
g (xi)Li(x), Li(x) = n
j=0
j i
(x − xi)
(xi − xj )
, i = 0, 1,...,n (10.55)
gives
I (Pn) =  b
a
Pn(x)W(x) dx = n
i=0
Aig (xi), (10.56a)
where
Ai =
 b
a
Li(x)W(x) dx, i = 0, 1, . . . , n. (10.56b)
I given by Eq. (10.56) is called the Gaussian formula.
10.4.2 Error in the Gaussian Formula
In order to compute the error in the Gaussian formula, consider the equation
I(g) − I (Pn) =  b
a
g [x0, x1,...,xn; x] ψn(x)W(x) dx . (10.57)206 Numerical Integration
If
 b
a
ψ(x) (x − xn−1)···(x − xn+1+i) W(x) dx = 0 , i = 0, 1,...,m − 1 (10.58)
for certain x0, x1,...,xn+m then for any xn+m+1
I(g) − I (Pn) =  b
a
g [x0, x1,...,xn+m+1; x] ψn+m+1(x)W(x) dx . (10.59)
One can find an orthogonal polynomial
Pn+1(x) = αn+1 (x − ξ0) (x − ξ1)···(x − ξn) , (10.60)
where ξ0, ξ1,...,ξn are the (n + 1) distinct points in [a, b] at which Pn+1 = 0 such that
 b
a
Pn+1(x)q(x)W(x) dx = 0 (10.61)
for all polynomials q(x) of degree ≤ n. When m = n Eq. (10.59) becomes
I(g) − I (Pn) =  b
a
g [x0, x1,...,x2n+1; x] ψ2n+1(x)W(x) dx . (10.62)
The choice xn+j = ξj−1, j = 1, 2,...n + 1 gives
ψ2n+1(x)=(x − x0)...(x − x2n+1) = Pn+1(x)
αn+1 2
. (10.63)
ψ2n+1W(x) does not change the sign in [a, b]. Hence,
I(g) − I (Pn) = 1
(2n + 2)! g(2n+2)(ξ)
Sn+1
α2
n+1
, (10.64a)
where
Sn+1 =
 b
a
P2
n+1(x)W(x) dx . (10.64b)
From Eq. (10.64) one finds that the Gaussian formula (10.56) is exact for all polynomials of
degree ≤ 2n + 1 provided the points x0, x1,...,xn in Eq. (10.56) are the zeros of Pn+1(x)
of degree n + 1 in [a, b] and the coefficients Ai(i = 0, 1,...,n) in Eq. (10.56a) are chosen
according to Eq. (10.56b).
10.4.3 Special Case n = 1: Two-Point Gauss–Legendre Integral
Formula
Let us choose W(x) = 1 and the orthogonal polynomials Pn+1(x) as the Legendre polyno￾mials. Since Pn(x) are defined in the interval [−1, 1] the arbitrary interval [a, b] has to be
changed to [−1, 1]. This can be made by the change of variable
x = 1
2
(b − a)t +
1
2
(b + a). (10.65)Gauss–Legendre Integration 207
TABLE 10.3
First few Legendre polynomials and their zeros.
Value of n Pn+1(x) Zeros of Pn+1(x)
0 P1(x) = x x0 = 0
1 P2(x) = 
3x2 − 1

/2 x0 = −1/
√3, x1 = 1/
√3
2 P3(x)=8x3 − 12x x0 = −
3/2, x1 = 0, x2 = 3/2
Then, the integral in Eq. (10.52a) becomes
I =
 b
a
g(x) dx = 1
2
(b − a)
 1
−1
g
1
2
t + (b + a)
2

dt. (10.66)
Since a given integral with finite limits can be re-expressed as in Eq. (10.66) consider the
integral
I =
 1
−1
g(x) dx . (10.67)
First few Legendre polynomials and their zeros are given in Table 10.3.
For n = 1 Eq. (10.56) becomes
I = A0g (x0) + A1g (x1), (10.68)
where A0 and A1 are obtained from Eq. (10.56b) as
A0 =
 1
−1
L0(x) dx
=
 1
−1
(x − x1) / (x0 − x1) dx
=
 1
−1

x − 1/
√
3
−1/
√
3 − 1/
√
3

dx
= 1 (10.69)
and
A1 =
 1
−1
L1(x) dx = 1. (10.70)
Hence,
I = g (x0) + g (x1), x0 = −1/
√
3, x1 = 1/
√
3. (10.71)
The above formula is called a two-point Gauss–Legendre rule.
The error term in the formula (10.68) is obtained from Eq. (10.64) by substituting n = 1:
E = 1
4!
S2
α2
2
g(4) (x
) . (10.72)
S2 and α2 are found to be S2 =
 1
−1
(P2(x))2 dx = 2/5 and α2 = 3/2. Then,
E = 1
135g(4) (x
). (10.73)208 Numerical Integration
For n ≥ 1 the nodes are irrational.
For some other integration methods see Problems 10.1, 10.26–10.29.
Example 1:
Evaluate the integral  3
0
(1 + ex)dx employing the two-point Gauss–Legendre formula.
First, transform the given integral into the standard form  1
−1
g(x)dx. Substitution of a = 0
and b = 3 in Eq. (10.65) gives
x = 3(t + 1)/2 and dx = 3/2 dt.
Introduce the above change of variables in the given integral and use the Gauss–Legendre
formula. Then,
I = 3
2
 1
−1

1+e3(t+1)/2

dt
= 3
2 [g (t0) + g (t1)] , t0 = −1/
√
3, t1 = 1/
√
3
= 3
2

1+e3(−1/
√3 +1)/2 +1+e3(1/
√3 +1)/2

= 21.810071.
The exact value of the integral is 22.085537. The maximum possible error computed from
the formula, Eq. (10.72), is |E| = 9e3/320 = 0.5649057. The percentage of relative error in
the numerical result is 1.25. This error is lower than the error in the trapezoidal and the
Simpson’s 1/3-rules but slightly higher than the Simpson’s 3/8-rule.
Example 2:
Evaluate the integral  1
0
1/(1 + x2)dx.
Since the limits of the integral are not [−1, 1] introduce the change of variables given by
Eq.(10.65) with a = 0 and b = 1. Then,
I =
 1
0
1
1 + x2 dx
= 2  1
−1
1
4 + (1 + t)2 dt
= 2 
g

−1/
√
3

+ g

1/
√
3

= 2 [0.2393127 + 0.1541299]
= 0.7868852,
where the exact value is I = tan−1 x|
1
0 = π/4=0.7853981. The percentage of relative error
in the result is 0.19.Double Integration 209
10.5 Double Integration
The methods discussed so far to evaluate single integrals can be extended to solve a double
integral of the form
I =
 d
c
 b
a
f(x, y) dx dy. (10.74)
This section obtains the trapezoidal rule and the composite trapezoidal rule for double
integrals. Derivation of the Simpson’s rules for the above integral is left as an exercise to
the reader.
10.5.1 Trapezoidal Rule
Application of the trapezoidal rule, Eq. (10.23), to the inner integral gives
I = 1
2
(b − a)
 d
c
f(a, y) dy +
 d
c
f(b, y) dy

. (10.75)
Next, the application of the rule to each integral in Eq. (10.75) gives
I = 1
4
(b − a)(d − c) [f(a, c) + f(a, b) + f(b, c) + f(b, d)]
= 1
4
(b − a)(d − c) [f00 + f01 + f10 + f11] . (10.76)
10.5.2 Composite Trapezoidal Rule
To obtain the composite trapezoidal rule for the double integral, in Eq. (10.74) divide the
interval [a, b] into N subintervals and [c, d] into M subintervals. The nodes are given by
xi = x0 + ih, i = 1, 2,...,N (10.77a)
yi = y0 + jk, j = 1, 2,...,M (10.77b)
where x0 = a, y0 = b, h = (b − a)/N and k = (d − c)/M. Referring to the discussion in
Section 10.3.1 the composite trapezoidal rule for the double integral (10.74) is obtained as
I = hk
4

f00 + fN0 + f0M + fNM + 2
N
−1
i=1
(fi0 + fiM)
+2
M
−1
j=1
(f0j + fNj )+4
N
−1
i=1
M
−1
j=1
fij
, (10.78)
where fij = f(xi, yj ).
Example:
Evaluate the integral  1
0
 1
0
x2ydxdy by the composite trapezoidal rule with step size
h = 0.25 and k = 0.25.
h = k = 0.25 gives N = M = 4. The nodes are
xi = 0+ i × 0.25, i = 0, 1, 2, 3, 4
xj = 0+ j × 0.25, j = 0, 1, 2, 3, 4.210 Numerical Integration
The value of the integral is obtained as
I = 0.252
4

f00 + f40 + f04 + f44 + 2
3
i=1
(fi0 + fi4)
+2
3
j=1
f0j + 2
3
i=1
f4j + 4
3
i=1

3
j=1
fij
= 0.252
4
× 11
= 0.171875 .
The exact value of the integral is x3y2/6


1
0 = 1/6=0.16666... .
10.6 Fractional Order Integration
The generalization of ordinary integration and differentiation to noninteger is known as
fractional calculus. In Section 8.5, fractional order derivatives are considered. The present
section is concerned with the numerical evaluation of fractional order integration. For the
numerical computation of fractional order differential equation refer Chapter 16.
The Riemann–Liouville fractional integral operator of order α ∈R+ is defined by [4-6]
Iαf(t) = 1
Γ(α)
 t
0
(t − τ )
α−1f(τ )dτ, t ≥ 0, α> 0, (10.79)
where Γ denotes the gamma function
Γ(γ) =  ∞
0
zγ−1e−zdz , Γ(γ)=(γ − 1)! . (10.80)
10.6.1 Fractional Order Exact Integration of t
β
For certain simple functions, the integral in Eq. (10.79) can be easily evaluated analytically.
For example, for f(t) = t
Iαt = 1
Γ(α)
 t
0
(t − τ )
α−1τdτ . (10.81)
Integrating by parts gives
Iαt = 1
Γ(α)

−(t − τ )ατ
α



t
0
+
1
α
 t
0
(t − τ )
αdτ

= 1
αΓ(α)
 t
0
(t − τ )
αdτ
= Γ(2)
Γ(α + 2)t
α+1 . (10.82a)
For f(t) = t
2 and t
3
Iαt
2 = 1
Γ(α)
 t
0
(t − τ )
α−1τ 2dτ = Γ(3)
Γ(α + 3)t
α+2 , (10.82b)Fractional Order Integration 211
τ
(t −
τ )
−0.5
0 0.2 0.4 0.6 0.8 1
20
10
0
FIGURE 10.5
Plot of (t − τ )α−1 as a function of τ for α = 0.5 and t = T = 1. The vertical dashed line
represents the singularity at τ = T = 1.
and
Iαt
3 = 1
Γ(α)
 t
0
(t − τ )
α−1τ 3dτ = Γ(4)
Γ(α + 4)t
α+3 , (10.82c)
respectively. From Eqs. (10.82)
Iαt
β = Γ(β + 1)
Γ(α + β + 1)t
α+β . (10.83)
10.6.2 A Modified Trapezoidal Formula
Suppose, it is desired to evaluate Iαf(t) over the interval [0, T]. The integral in Eq. (10.79)
cannot be evaluated numerically for 0 <α< 1 applying the composite quadrature rules.
This is because for 0 <α< 1 and for the values of τ near t = T the quantity (t − τ )α−1
diverges rapidly and it has a singularity at τ = T (refer Fig. 10.5). Numerical methods have
been proposed for such integrals [7-11].
To obtain a formula for the evaluation of the integral in Eq. (10.79) divide the interval
[0, T] into n subintervals of equal width with the nodes τk = kh, k = 0, 1,...,n and h = T /n.
Then, Eq. (10.79) becomes
Iαf(t)


t=T = 1
Γ(α)
n
−1
k=0
 tk+1
tk
(T − τ )
α−1f(τ )dτ . (10.84)
Replace f by the piecewise linear interpolant in each interval with the nodes τk. This results
in the formula [7]
Iαf(t)


t=T = hα
Γ(2 + α)

(n − 1)α+1 − (n − α − 1)nα

f(0)
+
n
−1
k=1

(n − k + 1)α+1 − 2(n − k)
α+1
+(n − k − 1)α+1
f(τk) + f(T)

. (10.85)212 Numerical Integration
TABLE 10.4
IMTF
error in the numerically computed value of I0.5(t
3)|0.5 by the modified trapezoidal formula
as a function of the step size h.
h IMTF
error h IMTF
error
0.10000 0.11378 e-2 0.0031250 0.12715 e-5
0.05000 0.29936 e-3 0.0015625 0.32342 e-6
0.02500 0.77347 e-4 0.0010000 0.13548 e-6
0.01250 0.19770 e-4 0.0005000 0.37243 e-7
0.00625 0.50206 e-5 0.0001000 0.56431 e-8
This is known as the modified trapezoidal formula. The error term is found to be O(h2) [7].
Examples:
Let us apply the modified trapezoidal formula to two integrals. The first example is Iαt
β.
Its exact value is given by Eq. (10.83). For α = 0.5 and β = 3 the exact value of the integral
I0.5 
t
3 

t=0.5 is
Iexact = 2
√2
35√π ≈ 0.04559. (10.86)
Obviously, the value of a fractional order integral cannot be calculated employing the for￾mula (10.85) using a calculator. A Python or C++ or Fortran90 program implementing the
modified trapezoidal formula can be developed.
The numerical value of the integral denoted as Inum is computed (using a developed
program) for 10 values of h and compared with Iexact by calculating
IMTF
error = 
Iexact − Inum

. (10.87)
The value of Γ(2 + α) is computed using Eq. (8.40). The exact value of Γ(2.5) is 1.5 × 0.5 ×
Γ(0.5) = 0.75√π ≈ 1.32934. For the step sizes 0.1, 0.01, 0.001 and 0.0001 the values of
Γ(0.5) computed numerically are 1.32925, 1.32934, 1.32934 and 1.32934, respectively. The
step size 0.0001 is chosen. This step size can also be used for other values of α. Table 10.4
shows the variation of IMTF
error with the step size h.
The second example is with
f(t) = 8
3
t3
π − 2
 t
π . (10.88)
For α = 0.5, I0.5 exact = t
2 − t. Variation of IMTF
error with h is given in Table 10.5 for T = 0.5.
Tables 10.4 and 10.5 clearly illustrate the effect of h on the values of Inum and also the
applicability of the method. The method can be applied to the case of α > 1, for example
see Problem 34 at the end of the present chapter.
10.6.3 Implementation of Composite Trapezoidal Formula
Due to the presence of singularity at τ = t in Eq. (10.79) the composite trapezoidal formula
is not applicable directly to evaluate it numerically. However, the singularity can be avoidedFractional Order Integration 213
TABLE 10.5
IMTF
error versus h for I0.5

(8/3)t3/π − 2
t/π  


0.5
.
h IMTF
error h IMTF
error
0.10000 0.74393 e-2 0.0031250 0.34285 e-4
0.05000 0.24813 e-2 0.0015625 0.11952 e-4
0.02500 0.83887 e-3 0.0010000 0.60743 e-5
0.01250 0.28668 e-3 0.0005000 0.21246 e-5
0.00625 0.98816 e-4 0.0001000 0.17955 e-6
by integrating this equation once by parts. Taking u = f(τ ) and dv = (t − τ )α−1dτ the
relation  t
0
udv = uv|
t
0 −
 t
0
vdu gives
Iαf(t) = 1
Γ(1 + α)

t
αf(0) +  t
0
(t − τ )
αf
(τ )dτ

. (10.89)
Then, the application of the composite trapezoidal formula to the integral in the above
equation results in
Iαf(t)|t=T = 1
Γ(1 + α)

T αf(0) + h
2

T αf
(0) + 2
n
−1
k=1
(T − τk)
α f
(τk)
, (10.90)
where τk = kh, k = 0, 1,...,n and h = T /n. If the analytical expression for f
(τ ) is easy to
determine then it can be used in the above formula. Otherwise the following finite-difference
approximation can be used for f
(0) and f
(τk):
f
(0) = 1
2h [−3f(0) + 4f(h) − f(2h)] , (10.91a)
f(τk) = 1
2h [f(τk + h) − f(τk − h)] . (10.91b)
Iαf(t)|t=T is computed for f(t) = t
3 for α = 0.5, T = 0.5 and for a range of values of h.
Table 10.6 displays the error IMTF
error and ICTF
error , where CTF stands for composite trapezoidal
formula. Desired accuracy can be obtained by choosing appropriate value of h. For a given
value of h, ICTF
error is higher than IMTF
error .
TABLE 10.6
Comparison of the errors in the modified trapezoidal formula and the composite trapezoidal
formula in the case of I0.5(t
3)|0.5.
h IMTF
error ICTF
error h IMTF
error ICTF
error
0.100 0.11378 e-2 0.38882 e-2 0.0100 0.12719 e-4 0.14967 e-4
0.050 0.29936 e-3 0.14089 e-2 0.0010 0.13548 e-6 0.52793 e-5
0.025 0.77347 e-4 0.53985 e-3 0.0001 0.56431 e-8 0.15665 e-6214 Numerical Integration
10.7 Concluding Remarks
It is possible to perform hybrid multi-dimensional numerical integrations with a moder￾ate precision. Methods have been introduced for numerical integration to get results with
several hundreds of digits accuracy. Such techniques have applications in high-performance
computations. Combination of such high-precision methods with integer relation detection
algorithms is employed to obtain analytic expressions for integrals. Gaussian quadrature,
tanh-sinh quadrature and highly parallel quadrature are useful for high-precision results.
10.8 Bibliography
[1] L.A. Pipes and L.R. Harvill, Applied Mathematics for Engineers and Physicists.
McGraw-Hill, Singapore, 1971.
[2] A. Jeffrey, Advanced Engineering Mathematics. Academic Press, New Delhi, 2003.
[3] E. Kreyszig, Advanced Engineering Mathematics. John Wiley, New York, 1999.
[4] K.S. Miller and B. Ross, An Introduction to the Fractional Calculus and Fractional
Differential Equations. John Wiley and Sons, Inc., New York, 1993.
[5] I. Podlubny, Fractional Differential Equations. Academic Press, Cambridge, 1999.
[6] A.A. Kilbas, H.H. Srivastava and J.J. Trujillo, Theory and Applications of Frac￾tional Differential Equations. Elsevier, Amsterdam, 2006.
[7] Z.M. Odibat, Appl. Math. Comput. 178:527, 2006.
[8] Z.M. Odibat and S. Momani, J. Appl. Math. & Inform. 26:15, 2008.
[9] T. Blaszczyk and J. Siedlecki, J. Appl. Math. Comput. Mech. 13:137, 2014.
[10] T. Blaszczyk, M.Ciesielski, Fract. Calc. Appl. Anal. 17:307, 2014.
[11] N. Pandiangan, D. Johar and S. Purwani, World Scientific News 153:169, 2021.
10.9 Problems
10.1 Show that (a) for n = 5 (Boole’s rule) the integration formula given by Eq. (10.10)
becomes [G. Boole and J.F. Moulton, A Treatise on the Calculus of Finite Dif￾ferences. Dover, New York, 1960.]
I5 = 2h
45 [7f0 + 32f1 + 12f2 + 32f3 + 7f4] − 8
945h7f(6)(x
)
and (b) for n = 6 (Hardy’s rule) [E.W. Weisstein, Hardy’s Rule–A Wolfram
Web Resource. http://mathworld.wolfram.com/HardysRule.html (accessed on
May 10, 2020).]
I6 = h
100 [28f0 + 162f1 + 220f3 + 162f5 + 28f6] .Problems 215
10.2 Evaluate the following integrals by the trapezoidal, the Simpson’s, the two-point
Gauss–Legendre’s, the composite trapezoidal and the composite Simpson’s rules.
Compare the numerical value with the exact result by computing the percentage
of relative error wherever possible. For the composite rules divide the range of
integration into 3 equally spaced subintervals and then apply the rules to each
subinterval. That is, n = 3 in the formulas (10.35), (10.39) and (10.41).
(a)  1
0
cos xdx. (b)  1
0
e−x2
dx. (c)  2
1
1/xdx.
10.3 For the following problems employ the trapezoidal, the Simpson’s, the two-point
Gauss–Legendre, the composite trapezoidal and the Simpson’s rules. For compos￾ite rules divide the range of integration into 3 equally spaced subintervals, that
is n = 3.
(a) The Si(x) function is defined as
Si(x) =  x
0
sin u
u
du .
Compute Si(1) assuming (sin u)/u for u = 0 as 1.
(b) The beta function is given by
β(m, n)=2  π/2
0
sin2m−1 θ cos2n−1 θ dθ.
Find the value of β(3, 2).
(c) The error function erf(x) is defined as
erf(x) = 2
√π
 x
0
e−t2
dt.
Compute erf(0.1).
10.4 Evaluate the integral  1
0
f(x)dx, with the values of f(x) at certain points given
below, employing the trapezoidal and the Simpson’s 1/3 rules.
x 0 0.25 0.5 0.75 1
f(x)00.015625 0.125 0.421875 1
10.5 Compare the relative errors in the evaluation of the integral I =
 2
0
x3dx by
the composite trapezoidal and Simpson’s rules as a function of n, with n =
3, 6, 12, 18, 24, 30.
10.6 List some integrals which are not solvable by the composite trapezoidal and the
Simpson’s rules but solvable by the Gauss–Legendre two-point method.
10.7 For what value(s) of n the composite trapezoidal and the Simpson’s rules are
applicable to evaluate the integral
I =
 π
0
sin3 x
cos2 x
dx .216 Numerical Integration
10.8 Compute the absolute maximum errors possible in the trapezoidal, the Simpson’s
and the two-point Gauss–Legendre rules for the following integrals.
a)  2
0
1/(2 + x)dx. b)  1
0
x2dx. c)  1
0
cos xdx.
10.9 Calculate the optimum number of subintervals nopt and their width hopt, so that
the integrals in Problem 10.8 can be evaluated using (i) the composite trapezoidal
rule, (ii) the composite Simpson’s 1/3-rule and (iii) the composite Simpson’s 3/8-
rule with accuracy 10−5.
10.10 Compute the values nopt and hopt for the integral  1
0
(ex + sin x)dx (x is in ra￾dian) for a) the composite trapezoidal rule, b) the composite Simpson’s 1/3-rule
and c) the composite Simpson’s 3/8-rule so that the error in the approximation is
10−3. Then, using the calculated nopt and hopt evaluate the integral by the above
three rules.
10.11 Write a program in Python which first calculate the optimum value of n, nopt,
in the composite trapezoidal rule for a given integral of the form I =
 b
a
f(x)dx
and then evaluate the value of the integral with n = nopt + 1. Using the program
evaluate the values of the integrals of Problem 10. 8.
10.12 Do Problem 10.11 for the composite Simpson’s rules.
10.13 Can one use the trapezoidal and the Simpson’s rules to compute the value of the
following integrals? Why?
a)  1
0
x2/
√
1 − x dx. b)  π/2
0
(tan x/ cos x)dx.
c)  1
0
1/(1 − x2)
1/4dx.
For the following Problems 10.14–10.30 employ the trapezoidal, the Simpson’s, the two-point
Gauss–Legendre, the composite trapezoidal and the Simpson’s rules. For the composite rules
divide the range of integration into 3 equally spaced subintervals, that is n = 3. Compare
the numerical value with the exact result (if known) by computing percentage of relative
error.
10.14 For an electron in a box of size unity the transition dipole moment integral is
given by
I =
 1
0
x sin πx sin 2πx dx .
Compute the value of I.
10.15 The oscillation period T of a Josephson junction is given by
T = 
2er  2π
0
1
Ib − I sin θ dθ .
For Ib = 2 mA and I = 1 mA evaluate the integral.Problems 217
10.16 The time period of oscillation of a pendulum is given by
T(h) = 4
√
2h
 π/2
0
1

1 − 2
h sin2 θ
dθ,
where h is the energy of the system. Compute T(4).
10.17 The calculation of total dipole moment of a dielectric material involves the integral
 1
−1
xexdx. Compute the value of this integral.
10.18 The vector potential at a point r, due to a current (I) carrying wire is given by
A = Kµ0I
4π
 L
−L
1
(r2
1 + z2)1/2 dz ,
where 2L is the length of the wire. Compute the integral for L = 1 metre and
r1 = 1 metre.
10.19 The quantum mechanical probability PQM for a particle to be found in a classically
allowed region of a linear harmonic oscillator potential is given by
PQM = 2
√π
 0.5
0
e−x2
dx .
Compute PQM.
10.20 Consider a spherical shell of radius R, carrying a uniform surface charge density
σ which is set to spin at an angular velocity ω. The expression for the vector
potential at a point P produced by the spherical shell involves the integral
 1
−1
u
√R2 + s2 − 2Rsu
du ,
where s is the distance of the point P from the centre of the sphere. Compute
the value of the above integral for R = 1 and s = 2.
10.21 The magnetic field at the origin due to a current (I) carrying elliptic wire centered
at origin with the lengths of major and minor axes 2a and 2b, respectively, is given
by
B = µ0I
πa  π/2
0

1 − k2 sin2 θ dθ ,
where k = 1 − (a2/b2) . Compute the value of the integral in B for a = 2b (that
is, k2 = −3).
10.22 The magnetic flux φ flowing through a surface due to a linear conductor of infinite
length carrying a current I0 is given by
φ = 3
√2
µ0I0
π
 1
−1
(x + 1)
(x2 + 2x + 2) dx .
Evaluate the above integral.218 Numerical Integration
10.23 The centre of mass of a uniform solid hemisphere of radius R and mass M is
given by
Z = 3
2R3
 R
0
x(R2 − x2) dx .
Compute Z for R = 0.2.
10.24 The x-component of a magnetic field Bx at a point due to a hollow circular wire
is given by
Bx = µ0I
2π
 2π
0
2 − sin θ
5 − 4 sin θ dθ .
Compute the value of the above integral.
10.25 A rod of 0.3 m length (L) has a nonuniform density λ. The mass per unit length
of the rod varies as λ = λ0(s/L), where λ0 is a constant and s is the distance
from the end. The centre of the mass is given by the integral
R = 2
λ0L
 L
0
xλ0
x
L dx = 2
L2
 L
0
x2 dx .
Calculate R.
10.26 Show that the three-point Gauss–Legendre formula for the integral I =
 1
−1
g(x)dx
is
I = 5
9
g(−
3/5)+ 8
9
g(0) + 5
9
g(
3/5)+ 1
15750g(6)(x
).
Then evaluate the integrals
a)  1
0
e−x2
dx b)  2
1
1/xdx c)  1
0
(sin x)/xdx.
10.27 Derive a four-point Gauss–Legendre rule for the integrals of the form  1
−1
f(x)dx.
Then find the values of the following integrals given below.
a)  1
0
1/(2 − x)dx. b)  1
0
1/(2 + x2)dx. c)  3
0
(1 + ex)dx.
10.28 A three-point Lobatto integration rule is given by
 1
−1
g(x) dx = 1
3 [g(−1) + 4g(0) + g(1)] − 1
90g(4)(x
).
Compute the values of the integrals in Problem 10.26 by this rule.
10.29 A three-point Radan integration rule is
 1
−1
g(x) dx = 2
9
g(−1) + 16 + √6
18 g

1 − √6
5

+
16 − √6
18 g

1 + √6
5

.
Compute the values of the integrals in Problem 10.26 by this rule.Problems 219
10.30 Evaluate the integral
I =
 1
0
 1
0
1
1 + x + y
dx dy
using the composite trapezoidal rule with step size h = k = 0.25.
10.31 Develop composite Simpson’s 1/3-rule to evaluate the double integral
 d
c
 b
a
f(x, y) dx dy .
10.32 The exact value of I1.5

4
t/π 


2
is t
2. Compute the value of this integral for
h = 0.1, 0.01 and 0.001 by the modified trapezoidal formula.
10.33 Develop a program for evaluating I0.75 
t
3 


0.5
. Compute the value of the integral
for h = 0.1, 0.01 and 0.001.
10.34 Determine the value of I1.5

4
t/π 


t=1
by the composite trapezoidal formula
given by Eq. (10.91) for h = 0.01, 0.001 and 0.0001.
10.35 Develop a program to evaluate the integral Iαt
β by the fractional Simpson’s rule
given in [9]. Compare the value of the integral for α = 0.5 with the exact value
of the integral.11
Ordinary Differential Equations – Initial-Value
Problems
11.1 Introduction
Differential equations arise in the mathematical modelling of many physical, chemical and
biological systems. A differential equation is said to be a linear if the degree or the power
of dependent variables and their derivatives is zero or one and no product of them occurs.
A differential equation which is not linear is called a nonlinear. A differential equation with
only one independent variable is called an ordinary differential equation, otherwise a partial
differential equation. Given a differential equation our prime aim is to find its solution.
Particular interest is to find an exact analytical solution expressed in terms of well-known
functions. This is because, from the solution, the state of the system can be determined
immediately for any time for a given initial condition. General analytical methods are
available to find exact solution for linear differential equations with constant coefficients
[1–4]. For other types of equations, general methods are not available. When the given
equation is not solvable exactly by the existing analytical methods then it can be solved
by an appropriate numerical integration scheme. Then, proceed to analyze the behaviour
of the solution.
The problem of solving differential equations of the form
dxi
dt = fi (x1, x2,...,xn, t) , i = 1, 2,...,n (11.1)
is generally classified into the following three main classes:
1. Initial-Value Problem
Solving a differential equation of the form (11.1) with the values of xi given at a single
starting value of t is known as an initial-value problem.
2. Boundary-Value Problem
Solving a differential equation with the value of xi given at the starting and ending values
of t is known as a boundary-value problem.
3. Eigenvalue Problem
For certain differential equations of the form Ly = λy, where L is a differential operator,
bounded solution exists only for a specific set of values of the parameter λ. These specific
values of λ are called eigenvalues and the corresponding solutions are called eigenfunctions.
The eigenvalue problem of equations of the form Ly = λy involves determination of those
specific values of the parameter λ and then the corresponding solution.
DOI: 10.1201/9781032649931-11 220Euler Method 221
Any nth order differential equation of the form
dnx
dtn + a1(t)
dn−1x
dtn−1 + ··· + an−1(t)
dx
dt + an(t)x = 0 (11.2)
can always be rewritten in the form (11.1) as x1 = x,
x˙ 1 = x2,
x˙ 2 = x3,
.
.
. (11.3)
x˙ n = −a1(t)xn−1 + ··· + an−1(t)x2 + an(t)x1 ,
where overdot refers to differentiation with respect to the independent variable t.
The basic idea of any method of solving the initial-value problem is the following. For
simplicity consider a first-order equation of the form
x˙ = f(x, t). (11.4)
The goal is to find the solution of Eq. (11.4) for t ∈ [a, b] with x(a) = xa or x0. The interval
[a, b] is divided into m intervals by the points t0 = a<t1 = a + h<t2 = a + 2h <
··· < tm = a + mh = b, where h = (b − a)/m. Write tn = a + nh, n = 0, 1,...,m. The
points tn are called grid points and h is called step size. The exact solution x(t) at t = tn
is approximated by a number and is denoted as xn. The sequence x0, x1,...,xm is called
a numerical solution. In general, the numerical value xn+1 at tn+1 is obtained from the
formula
xn+1 = xn + hφ (xn, xn−1,...,xn−k, tn, tn−1,...,tn−k) . (11.5)
When k = 0, the value of xn+1 is found by using (xn, tn) only. In this case, the method is
called a single-step method. For k > 0, the method is a multi-step method. If the error in a
method is O 
hN+1 then it is called Nth order method. Some of the single-step methods
are the Euler method and the Runge–Kutta methods. An example of a multi-step method
is the Adams–Bashforth–Moulton method. These and other methods essentially discretize
the differential equation and produce a difference equation generally of the form (11.5). Dif￾ferent methods give rise to different difference equations for the same differential equation.
However, their aim is to produce a solution which should correspond closely to the exact
solution of the given equation.
The present and the next two chapters concentrate on numerical integration of ordinary
differential equations. Some methods for solving partial differential equations will be de￾scribed in Chapters 14 and 15. In the present chapter, for the initial-value problem certain
single-step methods such as the Euler and the Runge–Kutta methods and the multi-step
methods such as the Adams–Bashforth–Moulton, the Milne–Simpson and the Hamming
methods are considered. The features of these methods are discussed. Also, the methods
to solve stiff equations are presented. Numerical methods for Hamiltonian equations will
be addressed in Chapter 12. Methods for boundary-value problems will be considered in
Chapter 13.
11.2 Euler Method
In this section let us obtain the Euler formula for the numerical integration of an ordinary
differential equation, discuss the error in the formula and analyze the stability of the method.222 Ordinary Differential Equations – Initial-Value Problems
11.2.1 Euler Formula
For simplicity and illustrative purpose consider the first-order equations of the form (11.4).
Assume that x(t0 = a) is given and denote it as x0. If ˙x = dx/dt (rate of change of x)
remains as f(x0, t0) for all time then integration of Eq. (11.4) gives
x(t) = f (x0, t0)t + C, (11.6)
where C is an integration constant. With x(t) = x0 at t = t0 the above equation is rewritten
as
C = x0 − f (x0, t0)t0 . (11.7)
Then,
x(t) = x0 + f (x0, t0) (t − t0) . (11.8)
When ˙x changes with time, it is reasonable to expect that it remains close to f(x0, t0) for
t close t0. In this case
x(t) ≈ x0 + f (x0, t0) (t − t0) + 1
2
x (c1) (t − t0)
2 , (11.9)
where t0 < c1 < t.
Define t1 = t0 + h, where h is a small number then at t = t1
x (t1) = x1 = x0 + hf (x0, t0) + 1
2
h2x (c1). (11.10)
Dropping the h2 term gives
x1 = x0 + hf (x0, t0) . (11.11)
Like-wise, define
t2 = t1 + h = t0 + 2h (11.12a)
and write at t = t2
x (t2) = x2 = x1 + hf (x1, t1) . (11.12b)
In general, with
tn+1 = t0 + (n + 1)h, n = 0, 1,... (11.13a)
xn+1 is given by
xn+1 = xn + hf (xn, tn) . (11.13b)
The above algorithm of determining x(t) from x(t0) is called Euler method. For an mth￾order equation the Euler formula is
tn+1 = t0 + (n + 1)h, n = 0, 1,... (11.14a)
xn+1 = xn + hF (xn, tn) . (11.14b)
In order to understand the geometric picture of the Euler method consider the direct
integration of Eq. (11.4) from tn to tn+1:
 tn+1
tn
dx
dt
dt = x (tn+1) − x (tn) =  tn+1
tn
f(x(t), t) dt . (11.15)Euler Method 223
  
(a)
  
 
  
(b)
  
 
  
(c)
  
 
FIGURE 11.1
Geometrical description of the Euler method. For details see the text.
The above is exact therefore replace x by X, the exact value of x, and write
X (tn+1) = X (tn) +  tn+1
tn
f(X(t), t) dt . (11.16)
Because the exact value X is not known a numerical approximation can be used for the
value of the integral in Eq. (11.16). Denoting xn as the approximation to X(tn) Eq. (11.16)
becomes
xn+1 = xn + an approximate value for  tn+1
tn
f(X(t), t) dt . (11.17)
Comparison of Eqs. (11.17) and (11.13b) indicates that in the Euler method the second term
in the right-side of (11.17) is hf(xn, tn). Geometrically, what does hf(xn, tn) represent?
Assume that the form of f(x, t) over the interval [tn, tn+1] is as shown in Fig. 11.1a. The
value of the integral in Eq. (11.16) is the area of the stripped portion in Fig. 11.1b. This
area is, say, approximated by the area of the stripped rectangle shown in Fig. 11.1c and is
(tn+1 − tn)f(xn, tn), that is hf(xn, tn).
Example:
The equation of motion of the overdamped linear harmonic oscillator driven by the periodic
external driving force sin t is the first-order equation
x˙ = −x + sin t, x(0) = 1. (11.18)
The exact solution of this equation is
x(t)=(x(0) + 0.5) e−t + 0.5(sin t − cost). (11.19)
(a) Calculate x(0.1) by the Euler method with step size h = 0.1.
(b) Solve the equation in the interval t ∈ [0, 3] with h = 0.01, 0.1, 0.25, 0.5.
(a) The Euler formula for x(0.1) is
x(0.1) = x(0) + hf(x(0), t = 0).
Then,
x(0.1) = x(0) + h(−x(0) + sin 0) = 1.0+0.1(−1.0 + 0) = 0.9224 Ordinary Differential Equations – Initial-Value Problems


0 1 2 3
1
0.8
0.6
0.4
FIGURE 11.2
Comparison of the numerical solution of the equation ˙x = −x+ sin t obtained with the step
sizes h = 0.1 (marked by +), 0.25 (marked by the painted circles) and 0.5 (marked by the
open circles) by the Euler method. The continuous curve represents the exact solution. The
initial condition used is x(0) = 1.
while the exact solution is 0.9096707. The difference is 0.0096707 and the percentage of
relative error (= 100 × |(xexact − xnumer)/xexact|) is 1.063.
(b) Figure 11.2 shows the plot of x(t) with h = 0.10, 0.25 and 0.50 along with the exact
solution. Table 11.1 gives the numerical values of x at a few values of t for different values
of h.
11.2.2 Error Analysis
Generally, there are three types of numerical errors in a numerical integration algorithm.
They are round-off error, local truncation error and final global error or simply global error.
The round-off error is due to the finite-precision arithmetic operations. It depends on the
TABLE 11.1
Comparison of the numerical solution obtained by the Euler method with different step sizes
for the equation ˙x = −x + sin t, x(0) = 1. The exact solution is x(t)=(x(0) + 0.5) e−t +
0.5(sin t − cost).
Numerically computed x with
t h = 0.01 h = 0.10 h = 0.25 h = 0.50 Exact x
0.0 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
0.5 0.7077484 0.6793987 0.6243510 0.5000000 0.7107175
1.0 0.6991978 0.6688721 0.6114994 0.4897128 0.7024035
1.5 0.7959364 0.7759006 0.7389904 0.6655919 0.7980741
2.0 0.8650723 0.8590772 0.8487089 0.8315434 0.8657251
2.5 0.8236080 0.8300115 0.8424103 0.8704204 0.8229354
3.0 0.6417220 0.6556509 0.6814846 0.7344463 0.6402369Euler Method 225
type of arithmetical operations and the number of operations used in a step. A consequence
of this is that a very small step size cannot be chosen since this involves a greater number
of arithmetical operations leading to an increase in the round-off error. Since the round￾off error depends on the computer on which the method is implemented, it is generally
not considered. In a numerical integration of ordinary differential equations, the other two
errors are important.
The local truncation error is the error in a single-step due to the approximation used.
It is present even with an infinite-precision arithmetic. It depends on the step size as well
as the order of the method. The global error is the sum of the local truncation error over
the interval say t0 = a to tN = b and is due to the repeated application of the algorithm.
Denoting X(t) as the exact solution, the local truncation error is given by
En+1 = Xn+1 − xn+1
= Xn+1 − xn − hφ (xn, tn; h). (11.20)
For the Euler method the local truncation error En+1 is given by
En+1 = Xn+1 − xn+1 , n = 0, 1,...,m − 1. (11.21)
Use of Eq. (11.13b) in the above equation gives
En+1 = Xn+1 − xn − hf (xn, tn) . (11.22)
Since, the exact solution is not known replace Xn+1 by = xn+1. Then,
En+1 = 1
2
h2x (c1), (11.23)
where tn < c1 < tn+1. The local truncation error is O(h2). If
max
|t0, tm|
|En+1| = T (11.24a)
and
max
|t0, tm|
|x(c)| = M1, t0 <c<tN (11.24b)
then T ≤ h2M1/2. The error En+1 given by Eq. (11.23) is the error made in the single-step
from tn to tn+1. Therefore, the global error committed in repeating the above formula from
t0 = a to tm = b with step size h is
E(x(b), h) = 1
2
h2 m
n=1
x (cn)
= 1
2
mx(c)h2
= 1
2
(b − a)x(c)h
= O(h). (11.25)
The global error in the Euler method is O(h).226 Ordinary Differential Equations – Initial-Value Problems
TABLE 11.2
The final global error (|xexact − xnumer|) and the percentage of relative error at t = 1 for
Eq. (11.18) with the Euler method. The exact solution at t = 1 is 0.7024035.
x by the Final global % of relative
h Euler method error at t = 1 error
0.001 0.7020843 0.0003192 00.045
0.010 0.6991978 0.0032057 00.456
0.025 0.6943304 0.0080731 01.149
0.050 0.6860566 0.0163469 02.327
0.100 0.6688721 0.0335314 04.774
0.250 0.6114994 0.0909041 12.942
0.500 0.4897128 0.2126907 30.280
Example:
Compare the final global error in the solution of Eq. (11.18) obtained at t = 1 by the Euler
method with h = 0.001, 0.01, 0.025, 0.05, 0.1, 0.25 and 0.5 by calculating the percentage of
relative error.
Table 11.2 presents the numerically computed final global error at t = 1 for different values
of h. The final global and the relative errors decrease with decrease in the value of h.
11.2.3 Prefactor in Global Error
The global error E(x(b), h) = 1
2 (b − a)x(c)h in the previous subsection is simply obtained
by summing all local truncation errors from t = a to t = b. In [5] it has been shown that
the prefactor 1
2 (b − a)x(c) violates the actual error.
Denote the global error from t = a to t = b as Em = E. It is to be noted that Em = E
and Em = E = 1
2 (b − a)x(c)h (obtained in the previous subsection) are not the same.
From Eq. (11.21) En+1 is written as
En+1 = Xn+1 − xn+1
= Xn + hf(Xn, tn) − xn − hf(xn, tn) − 1
2
h2x(cn)
= En + h [f(Xn, tn) − f(xn, tn)] − 1
2
h2x(cn). (11.26)
To determine E this formula has to be used with a care. One cannot simply sum all the
single-step local truncation errors to get the global error (why?). It has been pointed out
in [5] that
|E| ≤ hM
2L

eL(b−a) − 1

, (11.27)
where L is a constant and x(cn) are bounded by M. The errors given by both (11.25) and
(11.27) are O(h). However, the prefactors in E and E are different.
For further understanding, let us take the problem of solving the equation [5]
x = 1 − t
2 + x, x(0) = 0.5, t ∈ [0, 2] (11.28)Euler Method 227
TABLE 11.3
The variation of |xexact−xnumer| and x(t) with t for the Eq. (11.28) with time step h = 0.1.
t |xexact x(t) t |xexact x(t)
−xnumer| −xnumer|
0.1 0.00741 1.44741 1.1 0.10979 0.49792
0.2 0.01530 1.38930 1.2 0.12300 0.33994
0.3 0.02367 1.32507 1.3 0.13671 0.16535
0.4 0.03255 1.25409 1.4 0.15090 −0.02760
0.5 0.04195 1.17564 1.5 0.16550 −0.24084
0.6 0.05188 1.08894 1.6 0.18047 −0.47652
0.7 0.06235 0.99312 1.7 0.19571 −0.73697
0.8 0.07338 0.88723 1.8 0.21113 −1.02482
0.9 0.08497 0.77020 1.9 0.22660 −1.34295
1.0 0.09710 0.64086 2.0 0.24197 −1.69453
by the Euler method. The exact solution of this equation is (derivation of the solution is
left as an exercise to the readers)
x(t)=1+2t + t
2 − 0.5et
. (11.29)
Its second derivative is x(t)=2 − 0.5et
. For the chosen equation L = 1 and x(cn) is
bounded by M = 0.5e2 − 2 = 1
2

e2 − 4

. Then,
|E| ≤ h
4

e2 − 4
 e2 − 1

. (11.30)
For the choice h = 0.1 the error bound is |E| ≤ 0.54132. The error cannot be larger than
|E|.
What is the (global) error predicted by the Euler method? What is the actual difference
between the exact value of x(t) and the solution computed by the Euler method at t = 2?
Since the exact solution of Eq. (11.28) is known, these two quantities can be easily computed.
Table 11.3 presents |xexact − xnumer| and x(t) as a function of t with the time step h = 0.1
for Eq. (11.28). Here, xexact is the exact solution given by Eq. (11.29), xnumer is the Euler
method predicted solution of Eq. (11.28) and x(t) is the second derivative of the exact
solution given by Eq. (11.29).
From Table 11.3 the global error at t = 2 is d = |xexact − xnumer| = 0.24197. Compare
this error with the global error |E| given by Eq. (11.30) and |E|≤| 1
2 (b−a)x(c)h| (obtained
by adding the local errors). d is 2.24 times less than the error bound |E|. For E choose the
maximum value of |x(c)| where c ∈ [0, 2]. From Table 11.3 the maximum value of |x| is
1.69453 and hence |E| ≤ 0.16945. The actual global error d = 0.24197 is higher than |E|.
That is, d is not ≤ |E| but roughly 1.4 times higher than |E|. Note that d ≤ |E| = 0.54312.
The point is that simply summing the local truncation errors does not give an appropriate
global error bound. This is because the single-step errors are not independent from each
other [5].
11.2.4 Stability Analysis
In a numerical integration method if the step size is too small then an enormous amount of
computation time is required and round-off error result. Computation time can be reduced228 Ordinary Differential Equations – Initial-Value Problems
by taking a higher step size. When the step size is above a critical value a method numerically
produces a solution which no longer falls close to the exact solution. Then, the method is
said to be numerically unstable. In general, a method is said to be stable if
lim
h→0
xn = Xn , n = 0, 1, . . . , m. (11.31)
Here, mh = b − a is kept constant so that tn is always the same point. It is possible to find
the upper bound on the step size of a method for its stability.
To study the stability of a method it is necessary to analyze the solution of Eq. (11.4)
in the neighbourhood of a point say, t
 (denote the corresponding value of x as x
(t

)). The
behaviour of the solution of Eq. (11.4) near t
 can be determined from the linearization of
it. Replacing f by its Taylor series expansion about x
(t

) and keeping only the first-order
terms leads to
x˙ = f (x
, t
)+(x − x
) ∂f
∂x



t
 + (t − t

)
∂f
∂t



t

= λx + c , (11.32a)
where
λ = ∂f
∂x



t
 , c = f (x
, t
)+(t − t

)
∂f
∂t



t
 − x ∂f
∂x



t
 . (11.32b)
The change of variable x = x − c/λ and dropping the prime bring the above equation into
the linear equation
x˙ = λx . (11.33)
The analytical solution (X) of Eq. (11.33) is
X(t) = αeλt, (11.34)
where α = X(0). From Eq. (11.34)
x1 = X(h) = αeλh. (11.35)
Writing the Taylor series approximation of eλh gives
x1 = X(h)
= αeλh
= α

1 + λh +
1
2
(λh)
2 + ··· 
(11.36a)
and
xn+1 = x((n + 1)h) = xnE(λh), n = 0, 1,... (11.36b)
where E(λh) is some approximation of eλh and is called stability function. Defining
en = Xn − xn (11.37)
Eq. (11.36b) can be written as
Xn+1 − en+1 = (Xn − en) E(λh). (11.38)
From Eq. (11.35) write
xn+1 = x1eλnh = x2eλ(n−1)h = ··· = xneλh = Xneλh. (11.39)Euler Method 229
The above equation becomes
en+1 = 
eλh − E(λh)

Xn + E(λh)en. (11.40)
The first part in Eq. (11.40) is the local truncation error. By choosing higher-order methods
the error can be made as small as possible. The second term is the propagation error from
time tn to tn+1. This term will not diverge when |E(λh)| ≤ 1. Now, the following two cases
to be considered.
(1) If |E(λh)| ≤ 1 the method is said to be absolutely stable.
(2) If |E(λh)| ≤ eλh the method is said to be relatively stable.
When the real part of λ is negative (λR < 0) then the exact solution decays to zero as
t → ∞ and the important condition is (1). The region of absolute stability for a method
is the set of values of h (which is real and positive) and λ (complex) for which xn → 0 as
n → ∞. Thus, for a method to be stable the condition is |E(λh)| ≤ 1. That is, the values
of h and λ are to be set in such a way that |E(λh)| ≤ 1.
The above treatment is for a first-order Eq. (11.4). For a system of nonlinear equations
x˙ = F(x, t), where F is nonlinear, its linearized system of equations x˙ = Ax, where A
is m × m constant matrix and x has m components, the condition for a solution of the
nonlinear system to be stable is the real part of all the eigenvalues of the matrix A should
be negative.
For the Euler method
xn+1 = xn + hfn
= (1 + λh)xn
= E(λh)xn (11.41)
giving E(λh)=1+ λh. Write λh = h¯ = h¯R + ih¯I. Then,
E(λh) = 1+ h¯R + ih¯I , (11.42a)
|E| =

1 + h¯R
2
+ h¯2
I . (11.42b)
The requirement is |E| ≤ 1. When λ is real then |E| = |1 + h¯R| ≤ 1. This condition is
satisfied for −2 < λh < 0. For complex λ the condition |E| ≤ 1 is possible if λh lies inside
a circle of radius unity centered at λh = (−1, 0).
11.2.5 Improved Euler Method
In the Euler method, the value of the integral in Eq. (11.17) is approximated into the area of
the stripped rectangle of Fig. 11.1c. In the improved Euler method, a better approximation
for this integral is realized by means of the trapezoid (stripped region in Fig. 11.3) rather
than the rectangle in Fig. 11.1c. The exact area of this trapezoid is the length h of the base
multiplied by the average of the heights of the two sides:
h[f (Xn, tn) + f (Xn+1, tn+1)]
2 . (11.43)
The integral in (11.17) is approximated as
 tn+1
tn
f(X, t) dt = 1
2
h [f (Xn, tn) + f (Xn+1, tn+1)]. (11.44)230 Ordinary Differential Equations – Initial-Value Problems
  
  
 
FIGURE 11.3
Approximation of the stripped area of Fig. 11.1b.
Since X is not known replace Xn by xn and write
 tn+1
tn
f(X, t) dt = 1
2
h [f (xn, tn) + f (xn + hf (xn, tn), tn+1)] . (11.45)
Then,
xn+1 = xn +
1
2
h [f (xn, tn) + f (xn + hfn, tn+1)] . (11.46)
The result is the improved Euler algorithm
pn+1 = xn + hf (xn, tn) , (11.47a)
xn+1 = xn +
1
2
h [f (xn, tn) + f (pn+1, tn+1)] . (11.47b)
Defining
k1 = hf (xn, tn), (11.48a)
k2 = hf (xn + k1, tn + h) (11.48b)
the improved Euler algorithm takes the form
xn+1 = xn +
1
2 (k1 + k2) . (11.48c)
In Subsection 11.3.2, the improved Euler algorithm is shown to be exactly the second-order
Runge–Kutta method. Solving Eq. (11.18) by this method is taken up in the next section.
Equation (11.47a) is simply the Euler formula and the last term in Eq. (11.47b) is the
trapezoidal rule. At a point tn+1 the Euler method is used to predict the value of xn+1 (and
hence pn+1 is called predictor ) while the trapezoidal rule is employed to get a correction
to the final value. The corrected value of xn+1 is re-substituted in Eq. (11.47b) to get a
re-correction. In this way, successive re-correction can be obtained. The above method is
also called the Euler predictor-corrector method.
The approximation of the integral in Eq. (11.17) involves trapezoidal rule. Hence, the
error term for this approximation is simply the error term for the trapezoidal rule and is
En+1 = − 1
12
h3x (cn) . (11.49)Runge–Kutta Methods 231
The final global error after m steps is
E = −m
n=1
1
12
h3x (cn)
≈ − 1
12(b − a)h2x(c)
≈ O 
h2
. (11.50)
11.3 Runge–Kutta Methods
The Euler method is usually not recommended for practical purposes because it is not very
accurate when compared to other methods and further it is not very stable. The German
mathematicians Runge and Kutta derived integration formulas for which the truncation
error is O(hN+1) while the final global error is O(hN ). The Runge–Kutta methods can be
constructed for any order p. These methods are closely related to the Taylor series expansion
of f(x, t) but differentiation of f is not necessary in the use of the methods. Essentially,
they construct a solution over an interval by combining the information from a few Euler￾style steps, each of which needs only one calculation of the function f and then equate the
obtained result to a Taylor series expansion up to some higher order. The present section
discusses the features of the Runge–Kutta methods. First, the algorithm for the Nth order
Runge–Kutta method is derived. Then, the second-order and fourth-order methods are
treated.
11.3.1 Nth Order Runge–Kutta Method
Let us first obtain the Taylor series approximation of the solution of Eq. (11.4). Denote
X(t) = X(tn), tn = a + nh, n = 0, 1,... as the exact solution of Eq. (11.4) with the initial
condition X(a) = x(a) = x0. The Taylor series expansion of the approximate solution x(t)
about t = tn is written as
x (tn + h) = x (tn) + hx(1) (tn) + 1
2
h2x(2) (tn) + ···
+
1
N!
hN x(N) (tn) + O 
hN 
, (11.51a)
where
x(N) (tn) = dN
dtN x (tn). (11.51b)
Defining xn = x(tn) the above equation is rewritten as
xn+1 = xn + hx(1)
n +
1
2
h2x(2)
n + ··· +
1
N!
hN x(N) n + O 
hN+1
= xn +
N
i=1
1
i!
hi
x(i)
n + O 
hN+1
. (11.52)232 Ordinary Differential Equations – Initial-Value Problems
The derivatives of x can be computed recursively as
x(1)(t) = f ,
x(2)(t) = ft + fxx = ft + fxf ,
x(3)(t) = ftt + 2ftxf + f 2
fxx + ffx (ft + fx) ,
x(4)(t) = 
fttt + 3fttx + 3f 2
ftxx + f 3
fxxx
+ ft

ftt + 2fftx + f 2
fxx
+3 (ft + ffx) (ffxx + ftx) + f 2
x (ft + ffx) (11.53)
and in general
x(N)
(t) =  ∂
∂t + f ∂
∂xN−1
f(x, t). (11.54)
Equation (11.52) can be considered as an approximate solution to Eq. (11.4). The global
error in the approximation is O(hN+1) if N terms in Eq. (11.52) are taken into account. A
drawback of the Taylor method is the computation of higher derivatives. However, from the
Taylor method, it is possible to obtain methods with global error of the order of hN but
avoiding the computation of derivatives. Such methods are called the Runge–Kutta methods
of order N.
To obtain a N-stage Runge–Kutta method write
xn+1 = xn + hφ (xn, tn; h) , φ (xn, tn; h) = 
N
i=1
ωiki . (11.55)
It can be written as
xn+1 = xn + h

N
i=1
ωiki , (11.56a)
where
ki = f

xn + h

i−1
j=1
αijkj , tn + hβi

 (11.56b)
and β1 = 0. For Nth order method determine the values of αij , βi and ωi for 1 ≤ (i, j) ≤ N
so that N + 1 terms in Eq. (11.56a) and (11.52) are identical. For this purpose write the
Taylor expansion of Eq. (11.56a) about (xn, tn) assuming x(tn) = X(tn) and compare this
with Eq. (11.52).
When N = 1
xn+1 = xn + hω1k1 = xn + hf (xn, tn) (11.57)
which is the Euler formula. The following subsections discuss the cases with N = 2 and 4.
11.3.2 Second-Order Runge–Kutta Method
The choice N = 2 in Eq. (11.51) gives
xn+1 = xn + hf +
1
2
h2 df
dt



(xn,tn)
+ O 
h3
, (11.58)
where f = f(xn, tn). Inserting the Taylor expansion of f gives
xn+1 = xn + hf +
1
2
h2 [ft + fxf] + O 
h3
, (11.59)Runge–Kutta Methods 233
where f and its partial derivatives are evaluated at (xn, tn). From Eqs. (11.56) write (with
β1 = 0)
xn+1 = xn + hω1k1 + hω2k2 , (11.60a)
where
k1 = f (xn, tn) , (11.60b)
k2 = f (xn + hα21k1, tn + hβ2)
= f (xn + hα21k1, tn) + hβ2
∂
∂tf (xn + hα21k1, tn) + O 
h2
= f (xn, tn) + hα21k1fx + hβ2ft + O 
h2
= f + hα21ffx + hβ2ft + O 
h2 (11.60c)
with ft and fx being evaluated at (x, t)=(xn, tn). Now, Eq. (11.60a) becomes
xn+1 = xn + hω1f + hω2 (f + hβ2ft + hα21ffx) + O 
h3
= xn + h (ω1 + ω2) f + h2ω2β2ft + h2ω2α21ffx . (11.61)
Comparison of Eqs. (11.59) and (11.61) gives
ω1 + ω2 = 1, ω2β2 = 1/2, ω2α21 = 1/2 . (11.62)
For the four unknowns ω1, ω2, α21 and β there are only three equations. Therefore, solve
Eqs. (11.62) in terms of ω2 and find ω1 = 1 − ω2, β2 = α21 = 1/(2ω2). Then, the choice
ω2 = 1/2 gives
k1 = f (xn, tn) , (11.63a)
k2 = f (xn + hk1, tn + h) , (11.63b)
xn+1 = xn +
1
2
h (k1 + k2) . (11.63c)
Equations (11.63) can be rewritten as
k1 = hf (xn, tn) , (11.64a)
k2 = hf (xn + k1, tn + h) , (11.64b)
xn+1 = xn +
1
2 (k1 + k2) . (11.64c)
This is called the second-order Runge–Kutta method. The algorithm given by Eqs. (11.64)
is identical to the improved Euler formula given by Eqs. (11.48).
The local truncation error of the method is O(h3). It can be shown that the second￾order Runge–Kutta method reduces to the trapezoidal rule when f(x, t) is independent of
x. In fact, each Runge–Kutta method reduces to a quadrature formula. Application of the
trapezoidal rule to evaluate the integral in x =  fdt from tn to tn+1 gives
x (tn+1) − x (tn) =  tn+1
tn
f(x, t) dt
= 1
2
h [f (x (tn), tn) + f (x (tn + h), tn + h)] . (11.65)
Comparison of Eqs. (11.65) and (11.64) gives
f (x (tn), tn) + f (x (tn + h), tn + h) = k1 + k2234 Ordinary Differential Equations – Initial-Value Problems


-3 0 3
3
0
-3
FIGURE 11.4
Stability boundary of the second-order Runge–Kutta method in the complex λh plane. The
method is stable in the stripped region.
or
f (x (tn + h), tn + h) = k2 . (11.66)
When f(x, tn + h) is independent of x then f(x, tn + h) = f(tn + h) = k2 and Eq. (11.65)
becomes
x (tn+1) − x (tn) = 1
2
h [f (tn) + f (tn+1)]. (11.67)
The right-side of Eq. (11.67) is identified as the trapezoidal rule. The formula (11.64) can
be viewed as the trapezoidal rule when the value of k2 is used instead of f(x(tn +h), tn +h).
Therefore, the local truncation error in the second-order Runge–Kutta method is approxi￾mately the error involved in the trapezoidal rule and is −h3x(c1)/12 where tn < c1 < tn+1
and the global error is O(h2).
Next, proceed to study the stability of the second-order Runge–Kutta method. Write
xn+1 = xn +
1
2
h (k1 + k2)
= xn +
1
2
h [f (xn, tn) + f (xn + hk1, tn + h)]
= xn +
1
2
hf (xn, tn) + 1
2
f (xn + hk1, tn + h)
= xn +
1
2
hf +
1
2
h [f + hk1fx + hft]
= xn + hf +
1
2
h2
k1fx +
1
2
h2
ft
= xn + hλxn +
1
2
h2
ffx +
1
2
h2
ft
= xn + hλxn +
1
2
h2
λxnfx +
1
2
h2
ft
= xn + hλxn +
1
2
h2
λ2
xn
= xn

1 + λh +
1
2
(λh)
2

= xnE(λh), (11.68)
where
E(λh)=1+ λh +
1
2
(λh)
2 . (11.69)
The stability condition is |E| ≤ 1 with λh being generally complex. Figure 11.4 depicts
the region of absolute stability for the second-order Runge–Kutta method. For real λ the
method is absolutely stable for −2 < λh < 0.Runge–Kutta Methods 235


0 1 2 3
1
0.8
0.6
0.4
FIGURE 11.5
Comparison of the numerical solution of ˙x = −x+ sin t obtained with the step sizes h = 0.1
(marked by +), 0.25 (marked by solid circles) and 0.5 (marked by open circles) by the
second-order Runge–Kutta method. The continuous curve represents the exact solution.
The initial condition used is x(0) = 1.
Example:
Consider the overdamped linear harmonic oscillator Eq. (11.18).
(a) Calculate x(0.1) by the second-order Runge–Kutta method with step size h = 0.1.
(b) Solve the equation in the interval t ∈ [0, 3] with h = 0.01, 0.10, 0.25 and 0.5.
(c) Compare the final global error at t = 1 for h = 0.001, 0.01, 0.025, 0.05, 0.1, 0.25
and 0.5 by calculating percentage of relative error.
(a) For h = 0.1
k1 = hf(x(0), t = 0) = −0.10,
k2 = hf(x(0) + k1, 0 + h) = f(0.9, 0.1) = −0.0800167,
x(0.1) = x(0) + 1
2 (k1 + k2)
= 1.0 +
1
2
(−0.10 − 0.0800167)
= 0.9099917
while the exact solution is 0.9096707. The difference is 0.0003210 and the percentage of
relative error is 0.035 which is much lower than the value obtained for the Euler method.
(b) Figure 11.5 shows the plot of x(t) with h = 0.1, 0.25 and 0.5 along with the exact
solution. Table 11.4 gives the numerical values of x at certain values of t for different values
of h.
(c) Table 11.5 presents the numerically computed final global error at t = 1 for different
values of h. The final global error and the relative error decrease with decrease in the value
of h.236 Ordinary Differential Equations – Initial-Value Problems
TABLE 11.4
Comparison of the numerical solution obtained by the second-order Runge–Kutta method
with different step sizes for the equation ˙x = −x + sin t, x(0) = 1. The exact solution is
x(t)=(x(0) + 0.5) e−t + 0.5(sin t − cost).
Numerically computed x with
t h = 0.01 h = 0.10 h = 0.25 h = 0.50 Exact x
0.0 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
0.5 0.7107265 0.7116925 0.7176344 0.7448564 0.7107175
1.0 0.7024114 0.7032733 0.7087773 0.7358312 0.7024035
1.5 0.7980765 0.7983631 0.8005629 0.8144521 0.7980741
2.0 0.8657212 0.8653533 0.8636868 0.8610438 0.8657251
2.5 0.8229270 0.8220796 0.8174886 0.8014326 0.8229354
3.0 0.6402270 0.6392140 0.6334812 0.6109844 0.6402369
11.4 Fourth-Order Runge–Kutta Method
In the second-order Runge–Kutta method the local truncation error is O(h3). The present
section considers a Runge–Kutta algorithm with the local truncation error O(h5).
11.4.1 Integration Formula
For N = 4, Eq. (11.52) takes the form
xn+1 = xn + hf +
1
2
h2f +
1
6
h3f +
1
24h4f + O 
h5
, (11.70)
TABLE 11.5
Final global error (|xexact − xnumer|) and the percentage of relative error at t = 1 for
Eq. (11.18) with the second-order Runge–Kutta (RK) method. The exact solution at t = 1
is 0.7024035.
x by the Final global % of
h second-order error relative
RK method at t = 1 error
0.001 0.7024036 0.0000001 0.000
0.010 0.7024114 0.0000079 0.001
0.025 0.7024538 0.0000503 0.007
0.050 0.7026100 0.0002065 0.029
0.100 0.7032733 0.0008698 0.124
0.250 0.7087773 0.0063738 0.907
0.500 0.7358312 0.0334277 4.759Fourth-Order Runge–Kutta Method 237
whereas Eq. (11.56) becomes
xn+1 = xn + hω1k1 + hω2k2 + hω3k3 + hω4k4, (11.71a)
where
k1 = f (xn, tn), (11.71b)
k2 = f (xn + hα21k1, tn + hβ2), (11.71c)
k3 = f (xn + hα31k1 + hα32k2, tn + hβ3), (11.71d)
k4 = f (xn + hα41k1 + hα42k2 + hα43k3, tn + hβ4). (11.71e)
Expanding k2, k3 and k4, substituting them in Eq. (11.71a) and then comparing the resultant
equation with Eq. (11.70) one obtains the following system of nonlinear algebraic equations:
α21 = β2 , (11.72a)
α31 + α32 = β3 , (11.72b)
α41 + α42 + α43 = β4 , (11.72c)
ω1 + ω2 + ω3 + ω4 = 1 , (11.72d)
ω2β2 + ω3β3 + ω4β4 = 1
2 , (11.72e)
ω2β2
2 + ω3β2
3 + ω4β2
4 = 1
3 , (11.72f)
ω3β3
2 + ω3β3
3 + ω4β3
4 = 1
4 , (11.72g)
ω3β2α32 + ω4 (β2α42 + β3α43) = 1
6 , (11.72h)
ω3β2β3α32 + ω4β4 (β2α42 + β3α43) = 1
18 , (11.72i)
ω3β2
2α32 + ω4

β2
2α42 + β2
3α43 = 1
12 , (11.72j)
ω4β2α32α43 = 1
24 . (11.72k)
For 13 unknowns there are only 11 equations. Therefore, two unknowns can be fixed. The
most useful choice is β2 = 1/2 and α31 = 0. Then, the solution of Eqs. (11.72) is
α21 = 1/2, α32 = 1/2, α41 = 0, α42 = 0, α43 = 1, (11.73a)
β3 = 1/2, β4 = 1, ω1 = 1/6, ω2 = 1/3, (11.73b)
ω3 = 1/3, ω4 = 1/6. (11.73c)
With these values of the parameters, the fourth-order Runge–Kutta formula for the first￾order differential equation is given by
k1 = f (xn, tn) , (11.74a)
k2 = f

xn +
1
2
hk1, tn +
1
2
h

, (11.74b)
k3 = f

xn +
1
2
hk2, tn +
1
2
h

, (11.74c)
k4 = f (xn + hk3, tn + h) , (11.74d)
xn+1 = xn +
1
6
h (k1 + 2k2 + 2k3 + k4) . (11.74e)238 Ordinary Differential Equations – Initial-Value Problems
The above formula can also be written as
k1 = hf (xn, tn) , (11.75a)
k2 = hf 
xn +
1
2
k1, tn +
1
2
h

, (11.75b)
k3 = hf 
xn +
1
2
k2, tn +
1
2
h

, (11.75c)
k4 = hf (xn + k3, tn + h) , (11.75d)
xn+1 = xn +
1
6 (k1 + 2k2 + 2k3 + k4) . (11.75e)
For simplicity use the form given by Eqs. (11.75). The fourth-order method is the most
popular and widely recommended method. It is quite accurate, stable and easy to program.
It is not necessary to go for higher-order methods because accuracy can be improved by
choosing either a small step size or using an adaptive size (see next section).
Another fourth-order Runge–Kutta method which uses the slope f at 5 different points
is the Runge–Kutta–Merson method. Its algorithm is the following:
k1 = hf (xn, tn) , (11.76a)
k2 = hf 
xn +
1
3
k1, tn +
1
3
h

, (11.76b)
k3 = hf 
xn +
1
6
k1 +
1
6
k2, tn +
1
3
h

, (11.76c)
k4 = hf 
xn +
1
8
k1 +
3
8
k3, tn +
1
2
h

, (11.76d)
k5 = hf 
xn +
1
2
k1 − 3
2
k3 + 2k4, tn + h

, (11.76e)
xn+1 = xn +
1
6 (k1 + 4k4 + k5) + O 
h5
. (11.76f)
11.4.2 Geometric Description
Let us briefly discuss the geometric meaning of the components in the formula (11.75) [6-8].
Essentially, the Runge–Kutta method iterates the t values by adding a step size h at each
iteration. What about x-iteration formula? Equation (11.75) implies that x-iteration is a
weighted average of the four values k1, k2, k3 and k4. k1 and k4 are given a weight of 1/6
in the weighted average whereas k2 and k3 are weighted by the factor 1/3 which is twice as
heavily as k1 and k4.
What are k1, k2, k3 and k4? The quantity k1 is h times x
(t) = f(x, t) evaluated at
tn. It is the vertical jump from the current time tn to the next Euler computed point along
the numerical solution. Next, consider k2. For k2, the value of t at which f is calculated
is tn + h/2 and is the midpoint of the interval h between the times tn and tn+1 = tn + h.
The x-value corresponding to k2 is xn + k1/2 and is the current x-value and half of the
Euler calculated value of x. xn + k1/2 is vertically half-way up from the current point to
the Euler calculated next point. f(xn + k1/2, tn + h/2) is an estimation of the slope x of
the solution at h/2. Therefore, hf(xn + k1/2, tn + h/2) gives an estimation of the jump in
x in the actual solution over the full length of the interval.
The meaning of k3 is similar to k2, however, instead of k1 used in k2 now k2 is used in
k3. The f value is another estimation of the interval. The x-value of the midpoint is basedFourth-Order Runge–Kutta Method 239
on the jump in x predicted with k2. This slope multiplied by h gives another estimation of
the jump in x made by the actual solution across the full length of the interval.
For k4 the value of f is at t = tn +h. This time value is the extreme right of the interval
considered. Here, f is evaluated with x = xn + k3 and is an estimate of the value of x at
the right end of the interval, considering the jump in x found by k3. The calculated f value
is then multiplied by h.
Note that k1 uses the Euler method; k2 and k3 use estimates of the slope x of the
solution x at the middle point; k4 uses an estimate of the slope at the right extreme point.
Each ki uses the ki−1 as a basis for its calculation of the jump in x.
Use of the Simpson’s 1/3-rule with step size h/2 to evaluate the integral in the equation
x =
 tn+1
tn
fdt gives
 tn+1
tn
f(x, t) dt ≈ 1
3
h
2 [f (xn, tn)+4f (x (tn + h/2), tn + h/2)
+f (x (tn+1), tn+1)] . (11.77)
Comparison of Eqs. (11.77) and (11.75) gives
f (x (tn + h/2), tn + h/2) = (k2 + k3) /2 . (11.78)
Therefore, Eq. (11.75) can be treated as the Simpson’s rule, where instead of f(x(tn +
h/2), tn + h/2) the average of k2 and k3 is considered.
11.4.3 Error Versus Step Size and Stability
For the fourth-order Runge–Kutta method, referring to Eq. (11.70), the local truncation
error is Tn+1 ≈ O(h5). Equation (11.68) can be viewed as the Simpson’s 1/3-rule with step
size h/2. So, take Tn+1 as the error in the Simpson’s rule. It is simply −x(4)(c1)(h/2)5/90
(refer Eq. (10.30)). If t ∈ [a, b] then the error (global) at the end point t = b with h =
(b − a)/m is O(h4). For two different time steps, say h and h/2,
E(x(b), h) ≈ αh4 , (11.79a)
E(x(b), h/2) ≈ 1
16
αh4 ≈ 1
16
E(x(b), h). (11.79b)
Thus, if the step size is reduced by a factor of h/2 then the overall global error will be
reduced by a factor of 1/16.
For the test Eq. (11.33) the stability condition is
|E(λh)| =




1 + λh +
1
2
(λh)
2 +
1
3!(λh)
3 +
1
4!(λh)
4




≤ 1 . (11.80)
The stability region is shown in Fig. 11.6. For real λ the stability condition is −2.795 <
λh < 0. For pure imaginary λ the condition is 0 < |λh| ≤ 2
√2.
Example:
Consider the first-order Eq. (11.18).
(a) Calculate x(0.1) by the fourth-order Runge–Kutta method with step size h = 0.1.
(b) Solve it in the interval t ∈ [0, 3] with h = 0.01, 0.10, 0.25 and 0.5.240 Ordinary Differential Equations – Initial-Value Problems


-3 0 3
3
0
-3
FIGURE 11.6
Stability boundary of the fourth-order Runge–Kutta method in the complex λh plane. The
method is stable in the stripped region.
(c) Compare the final global error at t = 1 for h = 0.001, 0.01, 0.025, 0.05, 0.1, 0.25
and 0.5 by calculating percentage of relative error.
(a) The fourth-order Runge–Kutta algorithm for x(0.1) is
k1 = hf(x(0), t = 0),
k2 = hf(x(0) + k1/2, 0.05),
k3 = hf(x(0) + k2/2, 0.05),
k4 = hf(x(0) + k3, 0.1),
x(0.1) = x(0) + (k1 + 2k2 + 2k3 + k4) /6 .
Then,
k1 = 0.1 × f(1, 0) = −0.1 ,
k2 = 0.1 × f(0.95, 0.05) = 0.1 × −0.9000208 = −0.090002 ,
k3 = 0.1 × f(0.954999, 0.05) = 0.1 × −0.9050198 = −0.0905019 ,
k4 = 0.1 × f(0.909498, 0.1) = 0.1 × −0.8096646 = −0.0809664 ,
x(0.1) = 0.9096709
while the exact solution is 0.9096707. The difference is −0.0000002 and the percentage of
relative error is 2 × 10−5 which is negligible.
(b) Figure 11.7 shows the plot of x(t) with h = 0.1, 0.25 and 0.5 along with the exact
solution. Table 11.6 presents the numerical values of x at a few values of t for different
values of h.
(c) Table 11.7 gives the numerically computed final global error at t = 1 for different values
of h. The final global error and the relative error can be compared with the values obtained
for the Euler and the second-order Runge–Kutta methods (Tables 11.2 and 11.5).Convergence of Runge–Kutta Methods 241


0 1 2 3
1
0.8
0.6
FIGURE 11.7
Comparison of the numerical solution of ˙x = −x + sin t with x(0) = 1 obtained for h = 0.1
(marked by +), 0.25 (solid circles) and 0.5 (open circles) by the fourth-order Runge–Kutta
method. The continuous curve is the exact solution.
11.5 Convergence of Runge–Kutta Methods
An integration scheme is said to be convergent if the numerical solution converges to the
exact solution as the step size h is decreased. That is,
limh→0
nh=t−a
xn = Xn , (11.81)
where Xn is the exact solution at t = tn. Here, nh is kept constant so that tn is always the
same point. A method is said to be consistent if
φ (xn, tn; h = 0) = f (xn, tn) . (11.82)
TABLE 11.6
Comparison of the numerical solution obtained by the fourth-order Runge–Kutta method
with different step sizes for the equation ˙x = −x + sin t, x(0) = 1. The exact solution is
x(t)=(x(0) + 0.5) e−t + 0.5(sin t − cost).
Numerically computed x with
t h = 0.01 h = 0.10 h = 0.25 h = 0.50 Exact x
0.0 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
0.5 0.7107175 0.7107179 0.7107387 0.7111511 0.7107175
1.0 0.7024035 0.7024040 0.7024248 0.7028600 0.7024035
1.5 0.7980741 0.7980744 0.7980863 0.7983662 0.7980741
2.0 0.8657251 0.8657250 0.8657260 0.8658017 0.8657251
2.5 0.8229354 0.8229352 0.8229274 0.8228304 0.8229354
3.0 0.6402369 0.6402366 0.6402247 0.6400323 0.6402369242 Ordinary Differential Equations – Initial-Value Problems
TABLE 11.7
Final global error (|xexact − xnumer|) and the percentage of relative error at t = 1 for
Eq. (11.18) with the fourth-order Runge–Kutta (RK) method. The exact solution at t = 1
is 0.7024035.
x by the Final global % of
h fourth-order error relative
RK method at t = 1 error
0.001 0.7024035 0.0000000 0.000
0.010 0.7024035 0.0000000 0.000
0.025 0.7024035 0.0000000 0.000
0.050 0.7024035 0.0000000 0.000
0.100 0.7024039 0.0000005 0.000
0.250 0.7024248 0.0000213 0.003
0.500 0.7028600 0.0004565 0.065
This section shows that the Runge–Kutta methods are convergent by mainly following the
ref. [9].
Let Eq. (11.4) has a unique solution x(t) such that f(x, t) = φ(x, t; 0) and write x(tn) ≈
(xn+1 − xn)/h. When x(t) is continuous on the closed interval [a, b] then at some point c
with a<c<b
x˙ = x
(c) = (x(b) − x(a))
(b − a) . (11.83)
This is known as the mean-value theorem for the derivative x
. Choosing a = tn and
b = tn+1 = tn + h, the above equation is written as
x
(c)=(x (tn + h) − x (tn)) /h
or
x (tn + h) − x (tn) = hx
(c), (11.84)
where tn <c<tn+1. Define c = tn + δh with 0 <δ< 1. Then,
x (tn + h) − x (tn) = hx (tn + δh) . (11.85)
Therefore,
xn+1 − xn = x (tn + h) − x (tn)
= hx (tn + δh)
= hf (x (tn + δh), tn + δh) . (11.86)
For the exact solution X(t) with X(a) = X0
Xn+1 = Xn + hφ (Xn, tn; h) (11.87)
and the global truncation error is
en = |xn − Xn| . (11.88)Convergence of Runge–Kutta Methods 243
Then,
en+1 = |xn+1 − Xn+1|
= |xn + hf (x (tn + θh), tn + θh) − Xn − hφ (Xn, tn; h)|
≤ en + h [ | f (x (tn + θh), tn + θh) − φ (Xn, tn; h)| ]
≤ en + h |f (x (tn + θh), tn + θh) − f (xn, tn) + φ (xn, tn; 0)
−φ (xn, tn; h) + φ (xn, tn; h) − φ (Xn, tn; h)| . (11.89)
Define
χ(h) = max |f (x(t + θh), t + θh) − f(x, t)| (11.90)
and
ξ(h) = max |φ(x, t; 0) − φ(x, t; h)| , (11.91)
where t ∈ [a, b] and θ ∈ [0, 1]. In the limit h → 0, χ(h) = 0 and ξ(h) = 0.
If φ(x, t; h) is a continuous function and satisfies the Lipschitz condition then
|φ (xn, tn; h) − φ (Xn, tn; h)| ≤ L|xn − Xn|. (11.92)
Therefore,
en+1 ≤ en + h (χ(h) + ξ(h) + Len)
≤ (1 + hL)en + h(χ(h) + ξ(h))
or
en ≤ (1 + hL)en−1 + h(χ(h) + ξ(h)). (11.93)
Replacing en−1 in terms of en−2; en−2 in terms of en−3 and so on lead to
en ≤ (1 + hL)
ne0 +
1
L [(1 + hL)
n − 1] [χ(h) + ξ(h)] . (11.94)
Substitutions of x0 = X0 and e0 = 0 give
en ≤
1
L [(1 + hL)
n − 1] [χ(h) + ξ(h)] . (11.95)
In the limit h → 0
limh→0
nh=t−a
en ≤ limh→0
nh=t−a
1
L [(1 + hL)
n − 1] [χ(h) + ξ(h)] . (11.96)
Because of χ(h) and ξ(h) → 0 as h → 0
limit h→0
nh=t−a
en = 0 . (11.97)
In the limit h → 0, for the Runge–Kutta methods, Eq. (11.56b) becomes
ki = f (xn, tn) (11.98)
and hence
φ (xn, tn; 0) = 
N
i=1
ωiki = n
i=1
ωif (xn, tn) . (11.99)244 Ordinary Differential Equations – Initial-Value Problems
Because of φ(x, t; 0) = f(x, t)

N
i=1
ωi = 1 . (11.100)
This equation is satisfied in deriving the Runge–Kutta methods. That is, consistency con￾dition is satisfied for all the Runge–Kutta methods. Equation (11.100) is the necessary and
sufficient condition for the Runge–Kutta methods to be consistent. Since, the consistency
is the necessary and sufficient condition for convergence of the Runge–Kutta methods the
point is that all Runge–Kutta methods are convergent.
11.6 Adaptive Step Size and Runge–Kutta–Fehlberg Method
How does one make sure that the solution obtained by a Runge–Kutta method is sufficiently
accurate? One approach is comparison of solutions at the end of a time interval computed
with two different time steps, for example, with h and h/2. If the difference between the
two solutions is slightly changed or less than a preassumed tolerance then the result is
accepted. If the difference is higher than the tolerance then the step size must be halved
again until a satisfactory result is obtained. However, to apply this procedure with the
fourth-order Runge–Kutta method an additional seven function evaluations are required.
That is, totally 11 function evaluations to go from xn to xn+1.
Another approach to get some preassumed accuracy in the numerical solution is to
introduce a suitable adaptive control on step size. Suppose, for the fourth-order Runge–
Kutta method find the solution at t = t0 + 2h from t0 using two different step sizes h and
2h. Denoting x1 and x2 are the two solutions obtained at t = t0 + 2h with step sizes 2h and
h, respectively, then
X (t0 + 2h) = x1 + (2h)
5φ + O 
h6
, (11.101a)
X (t0 + 2h) = x2 + 2h5φ + O 
h6
. (11.101b)
Since, the error is h5φ when the step size is h Eq. (11.101b) has the error term 2h5φ. Now,
the difference between the two numerical estimations is
∆ = x2 − x1. (11.102)
It is desirable to keep this quantity lower than the preassumed accuracy by adjusting the
value of h. Ignoring the terms O(h6) in Eqs. (11.101) they can be solved for h5. The result
is
h5 = (x2 − x1) /30 (11.103)
and
X (t0 + 2h) = x2 +
∆
15 + O 
h6
. (11.104)
This estimate has fifth-order accuracy, however, there is no way of monitoring its truncation
error [7]. Another disadvantage of this method is the calculation of the solution with two
different step sizes h and 2h.
An alternative approach developed by Fehlberg uses two Runge–Kutta methods of dif￾ferent orders. For example, the fourth-order and fifth-order methods can be used to go from
xn to xn+1, compare the solution at xn+1 and obtain the error. If the error is larger than
the preassumed tolerance then compute a new step size so that the error is less than theAdaptive Step Size and Runge–Kutta–Fehlberg Method 245
tolerance. The method which implements the above is the Runge–Kutta–Fehlberg method.
It requires six function evaluation and has an estimate of the error. The following is the
algorithm:
k1 = hf (xn, tn), (11.105a)
k2 = hf 
xn +
1
4
k1, tn +
1
4
h

, (11.105b)
k3 = hf 
xn +
3
32
k1 +
9
32
k2, tn +
3
8
h

, (11.105c)
k4 = hf 
xn +
1
2197(1932k1 − 7200k2 + 7296k3), tn +
12
13
h

, (11.105d)
k5 = hf 
xn +
439
216
k1 − 8k2 +
3680
513
k3 − 845
4104k4, tn + h

, (11.105e)
k6 = hf 
xn − 8
27
k1 + 2k2 − 3544
2565
k3 − 1859
4104k4 − 11
40k5, tn +
1
2
h

. (11.105f)
The fourth-order and fifth-order solutions are given by
xn+1 = xn +
 25
216
k1 +
1408
2565
k3 +
2197
4104k4 − 1
5
k5

(11.106)
and
xn+1 = xn +
 16
135
k1 +
6656
12825
k3 +
28561
56430k4 − 9
50
k5 +
2
55
k6

, (11.107)
respectively.
The error in the approximation (xn+1 − xn+1) is
E = 1
360
k1 − 128
4275k3 − 2197
75240k4 +
1
50
k5 +
2
55
k6. (11.108)
Note that xn+1 and xn+1 are calculated using the same k
s. The global error in xn+1 and
xn+1 are O(h4) and O(h5), respectively. If the error E is less than the tolerance chosen then
xn+1 is accepted. If the error E is higher than the preassumed tolerance then the time step
h can be reduced to meet the requirement. The time step can be increased depending upon
the error.
Let us denote h as the step size giving the error E and E0 is the desired accuracy. The
time step, say hnew, which would produce the error E0 is given by
hnew = h |E0/E|
1/5 . (11.109)
This equation points out how much h to be decreased (increased) if E1 > E0 (E1 < E0) to
have the error ≈ E0. In this way, the adaptive strategy continuously controls the value of h
and helps to maintain the accuracy E0.
Example:
Compare the fourth-order Runge–Kutta (RK4) method and the Runge–Kutta–Fehlberg
(RKF) method with reference to the equation
x˙ = 3x − 3t + 1 , x(0) = 1 .
The exact solution of this equation is x(t) = x(0) e3t + t.246 Ordinary Differential Equations – Initial-Value Problems
TABLE 11.8
The numerical solution of x = 3x − 3t + 1, x(0) = 1 obtained using the RK4 method with
h = 0.1. PER denotes the percentage of relative error.
t xnumer xexact |xexact PER
−xnumer|
0.0 1.0000000 1.0000000 0.0000000 0.000
0.1 1.4498375 1.4498588 0.0000213 0.001
0.2 2.0220613 2.0221188 0.0000575 0.003
0.3 2.7594866 2.7596031 0.0001165 0.004
0.4 3.7199073 3.7201169 0.0002096 0.006
0.5 4.9813354 4.9816891 0.0003537 0.007
0.6 6.6490745 6.6496475 0.0005730 0.009
0.7 8.8652676 8.8661699 0.0009023 0.010
0.8 11.8217844 11.8231763 0.0013919 0.012
Table 11.8 gives the numerical solution obtained by the RK4 method with the fixed step
size h = 0.1. Table 11.9 presents the numerical solution and the relative error with the
RKF method. At time t = 0 the step size is chosen as 0.1. The tolerance E0 is chosen as
10−5. At t = 0.6 the error |E| (Eq. (11.108)) is 1.124e−4 which is higher than E0. The new
step size h0 (Eq. (11.109)) is computed as 0.09580074 and rounded it to 0.09. Then, using
the value of x(0.5) the values of x(0.59) and x(0.68) are obtained. At t = 0.77 the error in
the approximation is |E| = 1.273e−4. As this error is higher than E0 the new step size is
calculated and is 0.08.
Table 11.10 gives a comparison of the various numerical integration methods. The meth￾ods discussed so far are called single-step methods because they use only (xn, tn) to obtain
(xn+1, tn+1). The next section is devoted to the multi-step methods.
TABLE 11.9
The numerical approximation of the solution of x = 3x − 3t + 1, x(0) = 1 by the RKF
method. PER denotes the percentage of relative error.
th xnumer |E| |xexact PER
−xnumer|
0.00 −− 1.0000000 −− 0.0000000 0.000000
0.10 0.10 1.4498581 0.277e−5 0.0000007 0.000048
0.20 0.10 2.0221169 0.373e−5 0.0000019 0.000094
0.30 0.10 2.7595993 0.503e−5 0.0000038 0.000138
0.40 0.10 3.7201100 0.680e−5 0.0000069 0.000185
0.50 0.10 4.9816774 0.919e−5 0.0000117 0.000235
0.59 0.09 6.4608363 0.742e−5 0.0000171 0.000265
0.68 0.09 8.3705846 0.971e−5 0.0000246 0.000294
0.76 0.08 10.5366478 0.717e−5 0.0000326 0.000309Multi-Step Methods 247
TABLE 11.10
Comparison of various single-step numerical integration schemes.
Method Estimation Number of Local Global
of f calculation truncation error
of f per step error
Euler Initial value 1 O 
h2 O(h)
Improved Average of initial 2 O 
h3 O 
h2
Euler and final predicted
values of f
Second-order Average of two 2 O 
h3 O 
h2
Runge–Kutta values of f
Fourth-order Weighted average 4 O 
h5 O 
h4
Runge–Kutta of four values of f
Runge–Kutta– Weighted average 5 O 
h5 O 
h4
Merson of three values of f
Runge–Kutta– Weighted average 6 O 
h6 O 
h5
Fehlberg of six values of f
11.7 Multi-Step Methods
In the single-step methods (x0, t0) is alone required to compute (x1, t1) and (x1, t1) alone is
needed to compute (x2, t2) and so on. In general, knowing (xn, tn) the single-step methods
determine (xn+1, tn+1). Once (xn, tn) is known for a first few values of n then they can
be used for further calculation. The methods which use the information at two or more
prior points are called multi-step methods. An advantage of a multi-step method is that
it is possible to determine the local truncation error. Further, a correction term can be
included to improve the accuracy of the result at each step. This section mainly focuses on
the Adams–Bashforth–Moulton four-step method. It requires xn−3, xn−2, xn−1 and xn to
compute xn+1. The formulas of a few other methods are given at the end of this section.
11.7.1 Derivation of Adams–Bashforth–Moulton Four-Step Method
Consider that for Eq. (11.4) the values of (xn−3, tn−3), (xn−2, tn−2), (xn−1, tn−1) and
(xn, tn) are given. If (xn−3, tn−3) alone given then the other three sets can be calculated us￾ing a suitable single-step method, for example, a fourth-order Runge–Kutta method. From
the above four sets, the values of fn−3, fn−2, fn−1 and fn can be computed. Now, consider
the equation
xn+1 = xn +
 tn+1
tn
dx
dt
dt
= xn +
 tn+1
tn
f(x(t), t) dt . (11.110)248 Ordinary Differential Equations – Initial-Value Problems
Adams’ methods are based on the use of the Newton polynomial approximation for f
in the interval (tn, tn+1). Since, f is known at four points it is desirable to choose the
third-order Newton polynomial p3(x). Using the divided-differences write p3(t) as (refer the
Section 6.2.4)
p3(t) = fn−3 + f [tn−3, tn−2] (t − tn−3)
+f [tn−3, tn−2, tn−1] (t − tn−3) (t − tn−2)
+f [tn−3, tn−2, tn−1, tn] (t − tn−3) (t − tn−2) (t − tn−1) , (11.111a)
where
f [tn−3, tn−2] = 1
h (fn−2 − fn−3) , (11.111b)
f [tn−3, tn−2, tn−1] = 1
2h2 (fn−1 − 2fn−2 + fn−3) , (11.111c)
f [tn−3, tn−2, tn−1, tn] = 1
6h3 (fn − 3fn−1 + 3fn−2 − fn−3). (11.111d)
Then,
xn+1 = xn +
 tn+1
tn
p3(t) dt +
 tn+1
tn
E(t) dt . (11.112)
The integrals
I0 =
 tn+1
tn
dt , (11.113a)
I1 =
 tn+1
tn
(t − tn−3) dt , (11.113b)
I2 =
 tn+1
tn
(t − tn−3) (t − tn−2) dt , (11.113c)
I3 =
 tn+1
tn
(t − tn−3) (t − tn−2) (t − tn−3) dt (11.113d)
occurring in the second term are to be evaluated. I0 is found to be h. The other integrals
can be evaluated by introducing the change of variable t = t0 + hτ with τ varying from 0
to 1. Consider the integral I1. In this integral t − tn−3 = h(τ + 3), dt = hdτ and hence
I1 = h2
 1
0
(τ + 3)dτ = 7
2
h2. (11.114)
Similarly, the other two integrals are worked out as
I2 = 53
6
h3 and I3 = 55
4 h4. (11.115)
Then,
 tn+1
tn
dt = hfn−3 +
7
2
h (fn−2 − fn−3) + 53
12
h (fn−1 − 2fn−2 + fn−3)
+
55
24h (fn − 3fn−1 + 3fn−2 − fn−3)
= 1
24h [55fn − 59fn−1 + 37fn−2 − 9fn−3] . (11.116)Multi-Step Methods 249
The numerical estimation
pn+1 = xn +
 tn+1
tn
p3(t) dt
= xn +
1
24h [55fn − 59fn−1 + 37fn−2 − 9fn−3] (11.117a)
is called the Adams–Bashforth predictor .
Next, the integral  p3(t)dt is evaluated by constructing the Newton polynomial of f at
tn−2, tn−1, tn and tn+1. Here, pn+1 is used for tn+1, that is, fn+1 = f(pn+1, tn+1) while x’s
are used for other t’s. The result is
xn+1 = xn +
1
24h [fn−2 − 5fn−1 + 19fn + 9fn+1] (11.117b)
and is called the Adams–Moulton corrector. Redefining the obtained xn+1 as pn+1 and
using it in Eq. (11.117b) obtain a re-correction. In this way, successive re-correction can be
achieved.
The error term in the predictor is worked out as
 tn+1
tn
E(t) dt = 1
24f(4)  tn+1
tn
(t − tn−3) (t − tn−2) (t − tn−1) (t − tn) dt
= 1
24x(5)  tn+1
tn
(t − tn−3) (t − tn−2) (t − tn−1) (t − tn) dt
= 1
24x(5)  tn+1
tn
(u + 3h)(u + 2h)(u + h) du
= 251
720
x(5)(cn+1), (11.118)
where f(4) = f(4)(cn+1), u = t − tn, t − tn−3 = t − (tn − 3h) = u + 3h and so on.
The error given above is the local truncation error. The error term in the corrector is
−(19/720)h5x(5) (dn+1). Thus, the error terms in the predictor and corrector are O(h5).
Example:
Compute the numerical solution of the first-order Eq. (11.18) by the Adams–Bashforth–
Moulton four-step method over the interval [0, 3] with different step sizes. Compare the
numerical solution with the exact solution x(t)=(x(0) + 0.5)e−t + 0.5(sin t − cost).
In the Adams–Bashforth–Moulton method xn−3, xn−2, xn−1 and xn are used to calculate
xn+1. For the given equation only x(0) is given. The method requires x(0), x(h), x(2h) and
x(3h) to calculate x(4h). The unknown quantities x(h), x(2h) and x(3h) can be calculated
using a method which requires only one previous value of x. Using the fourth-order Runge–
Kutta method the values of x(h), x(2h) and x(3h) are found. Then, the values of x(0) to
x(3h) are used to calculate x(4h) and so on. Table 11.11 gives the numerical solution for
different values of h. The final global error and the percentage of relative error at t = 1 for
different values of h are given in Table 11.12.
11.7.2 Other Methods
Some other multi-step methods for Eq. (11.4) are given below:250 Ordinary Differential Equations – Initial-Value Problems
TABLE 11.11
Comparison of the numerical solution obtained for the equation ˙x = −x + sin t, x(0) = 1
by the Adams–Bashforth–Moulton four-step method.
Numerically computed x with
t h = 0.01 h = 0.10 h = 0.25 h = 0.50 Exact x
0.0 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
0.5 0.7107175 0.7107172 0.7107387 0.7111511 0.7107175
1.0 0.7024035 0.7024030 0.7024112 0.7028600 0.7024035
1.5 0.7980741 0.7980743 0.7980822 0.7983662 0.7980741
2.0 0.8657251 0.8657256 0.8657681 0.8662206 0.8657251
2.5 0.8229354 0.8229356 0.8229858 0.8238864 0.8229354
3.0 0.6402369 0.6402363 0.6402523 0.6411582 0.6402369
(a) Adams–Bashforth–Moulton Three-Step Method
pn+1 = xn +
1
12
h (5fn−2 − 16fn−1 + 23fn) , (11.119a)
xn+1 = xn +
1
12
h (−fn−1 + 8fn + 5fn+1) . (11.119b)
(b) Milne–Simpson Method
pn+1 = xn−3 +
4
3
h (2fn−2 − fn−1 + 2fn) , (11.120a)
xn+1 = xn−1 +
1
3
h (fn−1 + 4fn + fn+1) . (11.120b)
TABLE 11.12
Comparison of the numerical solutions obtained at t = 1 for the equation x = −x + sin t,
x(0) = 1 by the Adams–Bashforth–Moulton four-step method with different values of h.
h xnumer Final global error % of relative error
0.001 0.7024035 0.0000000 0.000
0.010 0.7024035 0.0000000 0.000
0.025 0.7024035 0.0000000 0.000
0.050 0.7024035 0.0000000 0.000
0.100 0.7024030 0.0000005 0.000
0.250 0.7024112 −0.0000077 0.001
0.500 0.7028600 −0.0004565 0.065Multi-Step Methods 251
TABLE 11.13
The numerical solution of the equation ˙x = −x + sin t, x(0) = 1 at t = 1 obtained by the
Adams–Bashforth–Moulton (ABM) three- and four-step methods, the Milne–Simpson (MS)
method and the Hamming method. The exact solution is x(1) = 0.7024035.
Numerically computed x by the methods
h ABM three-step ABM four-step MS Hamming
0.001 0.7024035 0.7024035 0.7024035 0.7024035
0.010 0.7024035 0.7024035 0.7024035 0.7024035
0.025 0.7024039 0.7024035 0.7024035 0.7024035
0.050 0.7024069 0.7024035 0.7024035 0.7024035
0.100 0.7024362 0.7024030 0.7024032 0.7024030
0.250 0.7029729 0.7024112 0.7024105 0.7024185
0.500 0.7029729 0.7028600 0.7628600 0.7028600
(c) Hamming Method
pn+1 = xn−3 +
4
3
h (2fn−2 − fn−1 + 2fn) , (11.121a)
xn+1 = 1
8 (−xn−2 + 9xn) + 3
8
h (−fn−1 + 2fn + fn+1). (11.121b)
Example:
Compare the numerical solutions obtained at t = 1 by the various multi-step methods with
reference to the first-order Eq. (11.18).
The solution at t = 1 is calculated by the methods, namely, Adams–Bashforth–Moulton
three- and four-step, Milne–Simpson and Hamming and compared with the exact solution
by calculating the percentage of the relative error. This is done for the various values of h.
The results are presented in Tables 11.13 and 11.14.
TABLE 11.14
Computed percentage of relative errors for the Adams–Bashforth–Moulton (ABM) three￾and four-step methods, the Milne–Simpson (MS) method and the Hamming method for the
numerical solution of the equation ˙x = −x + sin t, x(0) = 1 at t = 1.
h ABM three-step ABM four-step MS Hamming
0.001 0.000 0.000 0.000 0.000
0.010 0.000 0.000 0.000 0.000
0.025 0.000 0.000 0.000 0.000
0.050 0.000 0.000 0.000 0.000
0.100 0.005 0.000 0.000 0.000
0.250 0.065 0.001 0.001 0.002
0.500 0.081 0.065 0.065 0.065252 Ordinary Differential Equations – Initial-Value Problems
11.7.3 Convergence Criteria − Right Step
A predictor-corrector method becomes unstable if
1. fx(x, t) < 0 and
2. the step size h is too large.
Now, obtain a condition on h for the stability and the convergence of the three predictor￾corrector methods.
Let the value of xn+1 from the predictor and the corrector formulas as xp and xc,
respectively, and define D = xc − xp. The successive re-corrections are denoted as xcc, xccc
and so on. x∗ is the value to which the successive re-corrections converge. First, obtain the
convergence criteria for the Adams–Bashforth–Moulton method. The correction xc and the
re-correction xcc are
xc = xn +
1
24h (fn−2 − 5fn−1 + 19fn + 9fn+1)
= xn +
1
24h

x
n−2 − 5x
n−1 + 19x
n + 9x
p
 (11.122a)
and
xcc = xn +
1
24h

x
n−2 − 5x
n−1 + 19x
n + 9x
c

. (11.122b)
Then,
xcc − xc = 3
8
h

x
c − x
p

. (11.123)
The difference quantity x
c − x
p is
x
c − x
p = f (xc, tn+1) − f (xp, tn+1)
= 1
(xc − xp)
[f (xc, tn+1) − f (xp, tn+1)] (xc − xp)
= fx (ξ1) D, xc <ξ<xp and D = xc − xp . (11.124)
Substitution of Eq. (11.124) in Eq. (11.123) gives
xcc − xc = 3
8
hDfx (ξ1) . (11.125)
Further,
xccc − xcc = 3
8
h (x
cc − x
c)
= 3
8
hfx (ξ2) (xcc − xc)
= 3
8
hfx (ξ2) ·
3
8
hDfx (ξ1)
=

3
8
hfx(ξ)
2
D , (11.126)
where xc <ξ<xcc. The sum of such successive corrections gives
x∗ = xp + (xc − xp)+(xcc − xc)+(xccc − xcc) + ···
= xp + D + rd + r2D + r3D + ··· , (11.127)Second-Order Equations 253
where r = 3hfx(ξ)/8. Rewrite Eq. (11.127) as
x∗ = xp + D(1 − r)
−1 . (11.128)
The right-side of Eq. (11.128) is a convergent series provided |r| < 1, that is
h < 8
3
1
|fx (xn, tn)|
. (11.129)
The above is the first criterion for the convergence. It is desired to have xc and x∗ the same
within one in the mth decimal place. Then,
x∗ − xc = xp + D(1 − r)
−1 − (d + xp) = r
1 − r
D < 10−m . (11.130)
For r  1, r/(1 − r) is ≈ r and the second criterion for the convergence is rD < 10−m,
that is,
10mD < |1/r| = 8
3
1
|fx (xn, tn)|
. (11.131)
Note that the above criteria are for a first-order equation. Similar analysis can be performed
for a system of first-order equations but the analysis is tedious. For the other two multi-step
methods the convergence criteria are given in Problems 11.3 and 11.4.
11.8 Second-Order Equations
The methods discussed so far for first-order ordinary differential equations can be extended
to second-order and higher-order equations. For example, consider a system of two first￾order equations of the form
x˙ = f(x, y, t), y˙ = g(x, y, t). (11.132)
Any second-order equation can always be rewritten in the above form. The fourth-order
Runge–Kutta algorithm for Eqs. (11.132) is
X1 = hf (xn, yn, tn) , (11.133a)
Y 1 = hg (xn, yn, tn) , (11.133b)
X2 = hf 
xn +
1
2
X1, yn +
1
2
Y 1, tn +
1
2
h

, (11.133c)
Y 2 = hg 
xn +
1
2
X1, yn +
1
2
Y 1, tn +
1
2
h

, (11.133d)
X3 = hf 
xn +
1
2
X2, yn +
1
2
Y 2, tn +
1
2
h

, (11.133e)
Y 3 = hg 
xn +
1
2
X2, yn +
1
2
Y 2, tn +
1
2
h

, (11.133f)
X4 = hf (xn + X3, yn + Y 3, tn + h) , (11.133g)
Y 4 = hg (xn + X3, yn + Y 3, tn + h) , (11.133h)
xn+1 = xn +
1
6 (X1+2X2+2X3 + X4) , (11.133i)
yn+1 = yn +
1
6 (Y 1+2Y 2+2Y 3 + Y 4) . (11.133j)254 Ordinary Differential Equations – Initial-Value Problems
TABLE 11.15
The numerical solution of Eq. (11.134) by the fourth-order Runge–Kutta method with step
size h = 0.1. The exact solution is given by Eq. (11.135).
t xnumer xexact ynumer yexact
0.0 0.0000000 0.0000000 0.0000000 0.0000000
0.1 0.0048333 0.0048333 0.0950000 0.0950001
0.2 0.0186666 0.0186668 0.1800024 0.1800026
0.3 0.0405007 0.0405010 0.2550190 0.2550192
0.4 0.0693384 0.0693387 0.3200794 0.3200797
0.5 0.1041864 0.1041868 0.3752384 0.3752387
0.6 0.1440587 0.1440593 0.4205830 0.4205832
0.7 0.1879798 0.1879804 0.4562370 0.4562373
0.8 0.2349885 0.2349892 0.4823667 0.4823669
0.9 0.2841425 0.2841433 0.4991834 0.4991836
1.0 0.3345232 0.3345241 0.5069468 0.5069469
Example:
Using the fourth-order Runge–Kutta method compute the solution of
x¨ + x = e−t
, x(0) = 0, x˙(0) = 0 (11.134)
for t ∈ [0, 1] with time step h = 0.1.
The exact solution of the given equation is
x(t)= (˙x(0) + 0.5) sin t + (x(0) − 0.5) cost + 0.5e−t . (11.135)
In order to use the Runge–Kutta method rewrite the given equation as
x˙ = y, y˙ = −x + e−t
, x(0) = 0, y(0) = 0 . (11.136)
Table 11.15 gives the solution x(t) and y(t), where h = 0.1.
Programs can be developed to solve Eq. (11.134) by the Euler, second-order Runge–
Kutta, fourth-order Runge–Kutta and Adams–Bashforth–Moulton four-step methods.
11.9 Stiff Equations
There is a class of ordinary differential equations which are numerically unstable when
solved by certain methods unless the time step is extremely small.
11.9.1 What are Stiff Equations?
Consider a system of N first-order differential equations of the form
x˙ = f(x, t) , (11.137)Stiff Equations 255
where x = (x1, x2,...,xN ), f = (f1, f2,...,fN ). The system (11.137) is said to be stiff if
the real part of all the eigenvalues of the Jacobian matrix
J =


∂f1/∂x1 ∂f1/∂x2 ··· ∂f1/∂xN
.
.
.
∂fN /∂x1 ∂fN /∂x2 ··· ∂fN /∂xN

 (11.138)
for every value of t are all negative and they differ greatly in magnitude. That is, for a stiff
equation
1. Reλi < 0 and
2. max.|Reλi| min.|Reλi|, i = 1, 2,...,N.
In stiff equations, the dependent variables x depend on independent variables with two or
more greatly differing scales. That is, a stiff differential equation is one in which there are
both slow and fast-changing components.
11.9.2 Examples
The analytical solution of the equation
x˙ = −100x − 99e−t , x(0) = 0 (11.139)
is
x(t)=e−100t − e−t . (11.140)
Let us calculate the value of x(0.02) with h = 0.02 by the Euler method. The result is
x(0.02) = −1.98 while the exact value is −0.8448634. The error is very large. For h =
0.001( 1/100) the numerical solution is x(0.001) = −0.099 while the exact solution is
−0.0941631. That is, the time step h should be  1/100.
Next, consider the equation
x˙ = −1998x + 1996y = f1 , (11.141a)
y˙ = −998x + 996y = f2 . (11.141b)
Its Jacobian matrix is
J =
 ∂f1/∂x ∂f1/∂y
∂f2/∂x ∂f2/∂y 
=
 −1998 1996
−998 996 
. (11.142)
Its eigenvalues are the roots of the equation
|J − λI| =




−1998 − λ 1996
−998 996 − λ




= λ2 + 1002λ + 2000 = 0. (11.143)
Solving of Eq. (11.143) gives λ1 = −2, λ2 = −1000. Here, |λ1||λ2| and Reλ1, Reλ2 < 0.
The exact solution of Eq. (11.141) is
x(t) = Ae−2t + Be−1000t , (11.144a)
y(t) = Ae−2t +
B
2
e−1000t , (11.144b)256 Ordinary Differential Equations – Initial-Value Problems
(a)


0 1 2 3
2
1
0
(b)



0 0.005 0.01
2
1
0
FIGURE 11.8
Variation of the two particular solutions of Eq. (11.141).
where
A = −x(0) + 2y(0), B = 2x(0) − 2y(0). (11.144c)
The two particular solutions x1 = e−2t and x2 = e−1000t have time dependence with greatly
differing scales. Figure 11.8 depicts the variation of these two solutions. The solution x2
decays to zero much faster than the solution x1.
In order to solve Eq. (11.141) by the Euler or the Runge–Kutta method the step size h
must be  2/1000 so that these methods will be stable. This can be easily verified. With
x(0) = 3 and y(0) = 2 solve Eq. (11.141) by the fourth-order Runge–Kutta method with
different step sizes, namely, h = 0.0001, 0.001 and 0.002. Figure 11.9 depicts the numerical
solution x(t) obtained with these three step sizes. For h = 0.0001 the numerical solution
(represented by solid circles in Fig. 11.9) is in good agreement with the exact solution
(continuous curve). But the solutions obtained with h = 0.001 and 0.002 deviate largely


0 0.01 0.02
4
0
-4
FIGURE 11.9
Comparison of the numerical solutions of Eq. (11.141) by the fourth-order Runge–Kutta
method with the step sizes h = 0.0001 (solid circles), h = 0.001 (dashed curve) and h = 0.002
(open circles). Continuous curve is the exact solution.Stiff Equations 257
from the exact solution. Similar result is observed when the Euler method is used. For
example, the Euler method gives
x(0.005) = x(0) + hf1 = −7.05, xexact = 1.00353, (11.145)
y(0.005) = y(0) + hf2 = −3.01, yexact = 0.9967877, (11.146)
and
x(0.01) = 33.3397, xexact = 0.9802894, (11.147)
y(0.01) = 17.1797, yexact = 0.9802440. (11.148)
The numerical solution diverges as t increases whereas the exact solution decays to zero.
To understand the problem with Eq. (11.141) consider the equation
x˙ = −cx , c > 0. (11.149)
Its exact solution is x(t) = x(0) e−ct. The explicit Euler method gives
xn+1 = xn + hf
= xn − hcxn
= (1 − ch)xn . (11.150)
If ch > 2 (that is h > 2/c) then |xn| diverges to infinity as n → ∞. The method is thus
unstable for h > 2/c.
11.9.3 Implicit Euler Method
To overcome the problem with the explicit Euler method consider an implicit differencing:
xn+1 = xn + hf (xn+1)
= xn − hcxn+1
or
xn+1 = xn
1 + hc . (11.151)
This is called an implicit backward Euler scheme. It is to be noted that xn+1 → 0 (as is
the case of the exact solution) even for h → ∞. That is, in the long time limit, t → ∞, the
implicit method converges to the exact fixed point solution for large step sizes [7].
Let us derive an implicit Euler method for a system of differential equations. For sim￾plicity consider a system of linear equations of the form
x˙ = − Cx , (11.152)
where x = (x1, x2,...,xN ) and C is a N × N definite matrix. It is easy to show that for
an explicit Euler method, the numerical solution xn is bounded only if h < 1/λmax, where
λmax is the largest eigenvalue of the matrix C [7].
The implicit differencing gives
xn+1 = xn + hf (xn+1) , (11.153)
that is,
xn+1 = (I + Ch)
−1xn . (11.154)258 Ordinary Differential Equations – Initial-Value Problems
If f is nonlinear then Eqs. (11.153) for known xn are nonlinear equations. These can be
solved iteratively for each n, for example, using the Newton–Raphson method. In this case
xn+1 = xn + h

f (xn) + ∂f
∂x




xn
(xn+1 − x)

or
xn+1 = xn + h

I − h ∂f
∂x
−1
· f (xn) . (11.155)
This is called the semi-implicit Euler method.
Example:
Develop an implicit Euler algorithm to solve a second-order linear stiff equation of the form
x˙ = −a11x − a12y , (11.156a)
y˙ = −a21x − a22y, x(0) = x0 = 3, y(0) = y0 = 2. (11.156b)
Then, apply the algorithm to Eq. (11.141) with h = 0.0001, 0.001 and 0.01.
Since the given system (11.156) is linear the implicit Euler algorithm is given by Eq. (11.154).
Explicitly it is
 xn+1
yn+1 
=
 1 + a11h a12h
a21h 1 + a22h
−1  xn
yn

. (11.157)
Equation (11.157) can be rewritten as
 xn+1
yn+1 
=
 b11 b12
b21 b22   xn
yn

or
xn+1 = b11xn + b12yn , (11.158a)
yn+1 = b21xn + b22yn , (11.158b)
where
b11 = (1 + a22h) /det, b12 = −a12h/det, (11.159a)
b21 = −a21h/det, b22 = (1 + a11h) /det, (11.159b)
det = (1 + a11h) (1 + a22h) − a12a21h2 . (11.159c)
The quantities b11, b12, b21 and b22 are constants and do not change at every iteration.
Equations (11.158) appear as a two-dimensional linear map.
Now, apply the above algorithm to Eq. (11.141). For this equation
a11 = 1998, a12 = −1996, a21 = 998, a22 = −996.
For h = 0.001
b11 = 0.0019960, b12 = 0.9960079,
b21 = −0.4980039, b22 = 1.4960080, det = 2.004
and
x(0.001) = 1.9980038 , y(0.001) = 1.4980043 .
Using Eqs. (11.158) the values of x and y for t = ih, i = 2, 3,... are easily computed. Table
11.16 presents the numerical solution over the time interval [0, 5] for different values of h.
For large t the numerical solution converges to the exact solution even for a large time step
(h > 1/1000).Stiff Equations 259
TABLE 11.16
The numerical solution of Eq. (11.141) is obtained by the implicit Euler method with
different step sizes.
Numerically computed x with
t h = 0.0001 h = 0.001 h = 0.01 Exact x
0.000 3.0000000 3.0000000 3.0000000 3.0000000
0.002 1.2932956 1.4960120 − − −− 1.2666786
0.004 1.0362226 1.1170398 − − −− 1.0286632
0.006 0.9946414 1.0193336 − − −− 0.9930292
0.008 0.9851053 0.9919555 − − −− 0.9847982
0.010 0.9803458 0.9821714 1.1622103 0.9802895
0.100 0.8187471 0.8188943 0.8203483 0.8187308
1.000 0.1353623 0.1356059 0.1380330 0.1353353
5.000 0.0000454 0.0000459 0.0000501 0.0000454
11.9.4 Rosenbrock Method
Another method of solving the stiff equations of order N ≤ 10 is Rosenbrock method. Let
us review the description of this method given in ref. [7]. For the equation of the form
x˙ = f(x) (11.160)
the method looks for a solution of the form
x (t0 + h) = x0 +s
i=1
ciki , (11.161)
where ki is to be determined by solving the following equations which are the general form
of Eq. (11.155):
(I − γhf
) · ki = hf

x0 +
i−1
j=1
aijkj

 + hf ·

i−1
j=1
cijkj , i = 1, 2, . . . , s. (11.162)
Here, f is the Jacobian matrix of f and γ, ci, aij and cij are fixed constants and s is chosen
as 4. To minimize the matrix vector multiplications define
gi = 
i−1
j=1
cijkj + γki . (11.163)
Then, Eqs. (11.162) are rewritten as
 I
hγ − f

· gi = f

x0 +
i−1
j=1
aijgj

 +
1
h

i−1
j=1
cijgj , i = 1, 2,...,s (11.164)
The above set of equations is solved by first calculating the matrix (I/(hγ) − f
), next LU
decomposition of it and then gi are determined by back-substitution. The result is
x (t0 + h) = x0 +s
i=1
cigi . (11.165)260 Ordinary Differential Equations – Initial-Value Problems
TABLE 11.17
The numerical solution of Eq. (11.141) is obtained by the Rosenbrock method with different
step sizes.
Numerically computed x with
t h = 0.0001 h = 0.001 h = 0.002 Exact x
0.000 3.0000000 3.0000000 3.0000000 3.0000000
0.002 1.2684000 1.2490727 1.4323740 1.2666786
0.004 1.0280742 1.0214135 0.9249354 1.0286632
0.006 0.9928845 0.9918221 1.0367018 0.9930292
0.008 0.9848135 0.9850506 0.9637871 0.9847982
0.010 0.9803393 0.9808123 0.9919037 0.9802895
0.100 0.8187754 0.8191774 0.8196245 0.8187308
1.000 0.1353369 0.1353513 0.1353674 0.1353353
5.000 0.0000454 0.0000453 0.0000452 0.0000454
A Fortran code algorithm for the Rosenbrock method is given in ref. [7] with step-size control
for the equations which have explicit time dependence also. The values of the parameters
in the method are given by
c1 = 19/9, c2 = 1/2, c3 = 25/108, c4 = 125/108, (11.166a)
γ = 1/2, A21 = 2, A31 = 48/25, A32 = 6/25, (11.166b)
c21 = −8, c31 = 372/25, c32 = 12/5, (11.166c)
c41 = −112/125, c42 = −54/125, c43 − 2/5 . (11.166d)
Equation (11.141) is solved by the Rosenbrock method with h = 0.0001, 0.001 and 0.002.
The result is presented in Table 11.17.
An adaptive Runge–Kutta integration algorithm for stiff systems like a linear harmonic
oscillator subjected to nonlinear thermal constraints (Nos´e–Hoover equations) has been
described in the ref. [10].
11.10 Solving a Differential Equation with a Noise Term
This section points out how to solve numerically a differential equation in the presence of a
noise term. As an example, consider the first-order system
dx
dt = F(x) + η(t), (11.167)
where η(t) is, say, a continuous time white noise. η(t) is specified by its mean and correla￾tions. Assume that
η(t) = 0 , η(t)η(t + τ ) = Dδ(τ ), (11.168)
where · denotes an averaging over an ensemble of realizations of the random variable η at
time t and D is the strength of the noise at any time t.Dynamical Systems with Coloured Noise 261
Because the numerical methods such as the Euler and the Runge–Kutta solve a differ￾ential equation at discrete times (independent variable) η(t) has to be treated as a discrete
process. Let η(ti) = ηi denote the noise at time t = ti. Choose to represent the continu￾ous time white noise at discrete times: {ti : i = 1, 2,...}. Thus, {ηi : i = 1, 2,...} denotes
the discrete time noise process (random numbers) equivalent to the continuous time noise
process η(t).
What are the statistical properties of the discrete time process {ηi : i = 1, 2,...}? The
answer is
ηi = 0 for all i = 1, 2,..., ηiηj  = D
δij , (11.169)
where the scaled noise strength D (of the discrete process) and noise strength D (of the
continuous process) are related to each other as follows:
D = D
∆t
, (11.170)
where ∆t = ti+1 − ti. To prove this consider
 t
0
η(t

) dt
 = lim
∆t→0

i
η(ti)∆t . (11.171)
As η(ti)η(t

) = Dδ(ti, t
)
 t
0
η(ti)η(t

) dt
 =
 t
0
Dδ(ti, t
) dt
 = D . (11.172)
Evaluating the above integral in discrete time results in
 t
0
η(ti)η(t

) dt
 = 
j
η(ti)η(tj )∆t = 
j
ηiηj ∆t = D
∆t. (11.173)
Integrals (11.172) and (11.173) are the same and hence D = D/∆t.
Let η(t) is a Gaussian noise with zero mean and unit variance. How does one sample ηi?
First, generate a set of Gaussian random numbers of zero mean and unit variance employing
a standard sampling technique, for example, the Box–Muller algorithm and call it ξ. Then,
ηi =
√
D
√
∆t
ξi . (11.174)
Finite differencing of Eq. (11.167) with xi = x(ti) and η(ti) = ηi yields
xi+1 = xi + ∆tf(xi)+∆t ηi = xi + ∆tf(xi) + √
D∆t ξi . (11.175)
In other words, given x(ti) integrate Eq. (11.167) numerically without η(t) and obtain
x(ti+1) = x(ti + ∆t). Then, the solution of Eq. (11.167) with η(t) at t = ti + ∆t is given by
x(ti+1) → x(ti+1) + √
D∆t ξi.
11.11 Dynamical Systems with Coloured Noise
A white noise is a uniform mixture of random energy at every frequency. Its Fourier spectrum
contains all frequencies. For a white noise (t), (t) = 0 and (t)(t

) = δ(t − t

). The262 Ordinary Differential Equations – Initial-Value Problems
term white is by analogy with white light which is a mixture of all different possible colours.
Using a filter a white colour can be transformed into a desired colour. Like-wise one can
use filters on signals to change the balance of frequency components. The resulting noise is
no longer has the quality of white but has some other quality with certain frequencies are
more prominent.
In contrast to δ-correlation of white noise, in certain events fluctuation is time-correlated.
For example, fluctuations associated with the opening and closing of calcium channels are
slow in comparison with those associated with the opening and closing of sodium and
potassium channels. To model such fluctuations δ-correlated noise terms are inappropriate.
In such a case coloured noise, for example, time-correlated Gaussian noise with zero mean
and an exponential correlation function, that is
(t) = 0, (t)

(t) = Dλe−λ|t−t

|
, (11.176)
where D is the strength of the noise and λ is the inverse of the correlation time τ , is
desirable. The variance of (t) with the properties given by Eq. (11.176) is σ2 = 2 = Dλ.
In the limit λ → ∞, the correlation function approaches the δ-function and thus (t) is a
white noise if D is kept constant.
The noise with the property (11.176) can be generated from the linear Ornstein–
Uhlenbeck process given by
˙ = h() + λψω , h() = −λ , (11.177)
where the Gaussian white noise ψω has the properties
ψω(t) = 0, ψω(t)ψω(t

) = 2Dδ(t − t

). (11.178)
ψ(t) can be generated by the Box–Muller method.
As Eq. (11.177) for the coloured noise is an ordinary differential equation the numerical
integration algorithms such as the Euler and the Runge–Kutta methods can be used. Let
us now describe these algorithms.
1. The Euler Integration Scheme
The Euler algorithm is
(t0 + ∆t) = (t0)+∆t h((t0)) + (2Dλ2∆t)
1/2ψ , (11.179a)
where
ψ = (−2 ln a)
1/2 cos(2πb) (11.179b)
wth a and b are two uniform random numbers in the interval [0, 1] and ψ is a Gaussian noise
with mean zero and unit variance. To start the simulation, an initial value for  is needed
and is given by
0 = (t0)=(−2Dλ ln c)
1/2 cos(2πd), (11.180)
where c and d are uniform random numbers in the interval [0, 1]. Using the (t0) given
by Eq. (11.180) compute (t0 + ∆t) employing Eq. (11.179a). Next, with the calculated
(t0 + ∆t) find (t0 + 2∆t) by replacing t0 by t0 + ∆t. In this way by iterating Eq. (11.179a)
it is easy to generate (t0 + i∆t), i = 0, 1, 2,....
The problem is to solve numerically the one-variable and additive coloured noise equation
x˙ = f(x) +  = f(x, ) (11.181)Dynamical Systems with Coloured Noise 263
with given initial condition x(t0) = x0. Since (t) is described by the ordinary differential
Eq. (11.2) it is evident that
x˙ = f(x, ) = f(x) +  , (11.182a)
˙ = h() + λψ = −λ + λψ . (11.182b)
2. The Second-Order Runge–Kutta Scheme
Honeycutt [11] developed a second- and fourth-orders Runge–Kutta algorithms for coloured
noise equations. The second-order Runge–Kutta scheme for (t0 + ∆t) from t0 to t0 + ∆t is
given by
α = λ(2D∆t)
1/2ψ , (11.183a)
H1 = ∆t h((t0)), (11.183b)
H2 = ∆t h ((t0) + H1 + α) , (11.183c)
(t0 + ∆t) = (t0) + 1
2 (H1 + H2) + α , (11.183d)
where ψ is a Gaussian random variable with zero mean and unit variance. Initial value of
, (t0), is given by Eq. (11.180).
For the system (11.181) driven by the coloured noise  the stochastic Runge–Kutta
second-order scheme is
α = λ(2D∆t)
1/2ψ , (11.184a)
H1 = ∆t h((t0)), (11.184b)
F1 = ∆t f(x(t0), (t0)), (11.184c)
H2 = ∆t h ((t0) + H1 + α) , (11.184d)
F2 = ∆t f (x(t0) + F1, (t0) + H1 + α) , (11.184e)
x(t0 + ∆t) = x(t0) + 1
2 (F1 + F2) , (11.184f)
(t0 + ∆t) = (t0) + 1
2 (H1 + H2) + α . (11.184g)
3. The Fourth-Order Runge–Kutta Scheme
Define
a0 = 1 , b0 = 1 , (11.185a)
a1 = 1
4 +
√3
6 , b1 = 1
4 −
√3
6 +
√6
12 , (11.185b)
a2 = 1
4 +
√3
6 , b2 = 1
4 −
√3
6 −
√6
12 , (11.185c)
a3 = 1
2 +
√3
6 , b2 = 1
2 −
√3
6 , (11.185d)
a4 = 5
4 +
√3
6 , b4 = 5
4 −
√3
6 +
√6
12 (11.185e)264 Ordinary Differential Equations – Initial-Value Problems
and ψ1 and ψ2 as Gaussian random numbers with ψi = 0, ψiψj  = δij , i = 1, 2. The
stochastic fourth-order Runge–Kutta scheme is given by
αi = λ(D∆t)
1/2(aiψ1 + biψ2), i = 0, 1, 2, 3, 4 (11.186a)
H1 = ∆t h((t0) + α1), (11.186b)
H2 = ∆t h 
(t0) + 1
2
H1 + α2

, (11.186c)
H3 = ∆t h 
(t0) + 1
2
H2 + α3

, (11.186d)
H4 = ∆t h((t0) + H3 + α4), (11.186e)
(t0 + ∆t) = (t0) + 1
6
(H1 + 2H2 + 2H3 + H4) + α0 . (11.186f)
The algorithm for the system (11.181) is
H1 = ∆t h ((t0) + α1) , (11.187a)
F1 = ∆t f (x(t0), (t0) + α1) , (11.187b)
H2 = ∆t h 
(t0) + 1
2
H1 + α2

, (11.187c)
F2 = ∆t f 
x(t0) + 1
2
F1, (t0) + 1
2
H1 + α2

, (11.187d)
H3 = ∆t h 
(t0) + 1
2
H2 + α3

, (11.187e)
F3 = ∆t f 
x(t0) + 1
2
F2, (t0) + 1
2
H2 + α3

, (11.187f)
H4 = ∆t h ((t0) + H3 + α4) , (11.187g)
F4 = ∆t f (x(t0) + F3, (t0) + H3 + α4) , (11.187h)
(t0 + ∆t) = (t0) + 1
6 (H1 + 2H2 + 2H3 + H4) + α0 , (11.187i)
x(t0 + ∆t) = x(t0) + 1
6 (F1 + 2F2 + 2F3 + F4) . (11.187j)
11.12 Concluding Remarks
Various phenomena, processes and events in science, engineering and social science are
modelled by appropriated mathematical model equations. These equations are generally
nonlinear differential equations. In the present chapter, certain methods for numerical com￾putation of solutions of ordinary differential equations are presented and applied to equa￾tions for which exact solutions are known. Developing methods for computing numerical
solutions with higher accuracy is important since exactly solvable systems are very limited.
Numerical methods of solving differential equations are playing a key role in the study of
dynamical systems. Features of novel phenomena such as complicated regular solutions,
chaotic solutions, different routes to chaotic behaviour, synchronization of chaotic systems,
various resonances, etc. are investigated through numerical solutions. In this chapter ordi￾nary differential equations with integer order derivatives are considered. The case of ordinary
differential equations with fractional order derivatives is dealt in Chapter 16.Bibliography 265
11.13 Bibliography
[1] E. Kreyszig, Advanced Engineering Mathematics. John Wiley, New York, 1999.
8th edition.
[2] G.F. Simmons, Differential Equations. Tata McGraw–Hill, New Delhi, 2001.
[3] E.A. Coddington, An Introduction to Ordinary Differential Equations. Printice￾Hall of India, New Delhi, 2002.
[4] A. Jeffrey, Advanced Engineering Mathematics. Academic Press, San Diego, 2003.
Indian reprint.
[5] A. Gezerlis and M. Williams, Am. J. Phys. 89:51, 2021.
[6] E. Suli and D. Mayers, An Introduction to Numerical Analysis. Cambridge Uni￾versity Press, Cambridge, 2003.
[7] W.H. Press, S.A. Teukolsky, W.T. Vetterling, B.P. Flannery, Numerical Recipes
in Fortran. Foundation Books, New Delhi, 1993. Indian edition.
[8] J.H. Mathews, Numerical Methods for Mathematics Science and Engineering.
Prentice-Hall of India, New Delhi, 2005.
[9] J. Cartwright, Convergence of Runge-Kutta Methods (1995); https://www.iact.ugr￾csic.es/personal/julyan−cartwright/papers/rkpaper/node13.html (accessed on
June 20, 2020).
[10] W.G. Hoover, J.C. Sprott and C.G. Hoover, Am. J. Phys. 84:786, 2016.
[11] R.L. Honeycutt, Phys. Rev. A 45:600, 604, 1992.
11.14 Problems
11.1 Obtain the second-order Runge–Kutta methods with ω2 = 3/4 and 0. Compare
them with ω2 = 1/2.
11.2 Show that for the linear differential equation x = x, x(0) = A the truncation
error in the fourth-order Runge–Kutta method is ≈ (A/120)h5 + O(h6).
11.3 Show that for a first-order equation the convergence criteria for the Milne–
Simpson method are
h < 3
|fx (xn, tn)| and 10mD < 3
h |fx (xn, tn)|
.
11.4 Show that for a first-order equation the convergence criteria for the Hamming
method are
h < 8
3 |fx (xn, tn)| and 10mD < 8
3h |fx (xn, tn)|
.
11.5 The error function erf(x) is defined as
erf(x) = 2
√π
 x
0
e−u2
du .266 Ordinary Differential Equations – Initial-Value Problems
Solve the differential equation for erf(x) with the initial condition erf(0) = 2/
√π
by the Euler, the second- and the fourth-order Runge–Kutta methods with step
size h = 0.1 and obtain the table of erf(x) in the interval x ∈ [0, 1].
11.6 The Fresnel integral is given by
C(x) =  x
0
cos 
πu2/2

du
Solve the differential equation for C(x) with the initial condition C(0) = 1 by the
Euler, second and fourth-order Runge–Kutta methods with step size h = 0.1 and
obtain the table of C(x) in the interval x ∈ [0, 1].
For the following problems find the numerical solution using Euler, second-order and fourth￾order Runge–Kutta methods at time (independent variable) t = 0.1 with h = 0.1. Also, find
the numerical solution in the interval t ∈ [0, 1] with h = 0.01, 0.05, 0.1 and 0.2 by the above
methods and compare the numerical solution with the exact solution.
11.7 Find the numerical solution of the equation ˙x = 1 − x2 with x(0) = 0. The exact
solution of the above equation is x(t) = tan 
t + tan−1 x(0)
.
11.8 Find the numerical solution of the equation ˙x = 2x−x2 with x(0) = 1. The exact
solution of the above equation is x(t) = 1+tanht.
11.9 The exact solution of the equation ˙x = −x3 is x(t) = x(0)/
1+2x(0)2t. Obtain
the numerical solution for x(0) = 1.
11.10 Find the numerical solution of the equation ˙x = µx−x3 for µ = 1 and x(0) = 0.5
whose exact solution is
x(t) = 
1
µ
+
 1
x(0)2 − 1
µ

e−2µt−1/2
.
11.11 Find the numerical solution of the equation ˙x = −µx+x2 for µ = 2 and x(0) = 1
whose exact solution is
x(t) = µ
Ae−µt
Ae−µt − 1
, A = x(0)
x(0) − µ
, µ = 0.
11.12 Find the numerical solution of the Bernoulli equation ˙x = −x + x2t for the
initial condition x(0) = 0.5. Its exact solution is x(t)=1/(Aet +1+ t), where
A = (1 − x(0))/x(0).
11.13 A radioactive substance disintegrate at a rate proportional to the amount present.
The equation governing the process is ˙x = −λx, where x(t) is the amount of the
substance present at time t and λ is a constant. Find the numerical solution for
λ = 0.5 and x(0) = 5 gm.
11.14 The equation of motion of an overdamped linear harmonic oscillator driven by a
periodic force is given by the equation ˙x = −x+cost. Find the numerical solution
for x(0) = 0.1.
11.15 Hormone secretion is modelled by the equation ˙x = a − b cos(2πt/24) − cx, where
x(t) is the amount of a certain hormone in the blood, a is the average secretion
rate, cx models the removal rate of the hormone from the blood and b cos(2πt/24)Problems 267
represents the daily secretion cycles. Find the solution for a = b = c = 1 and
x(0) = 3.0. The exact solution is given by
x(t) = a
c
+ Ae−ct − bω
ω2 + c2 sin ωt − bc
ω2 + c2 cos ωt,
where
A =

x(0) + bc
ω2 + c2 − a
c

, ω = 2π
24 .
11.16 The equation for the velocity of a parachutist is given by ˙v = g−(a/m)v2, where v
is the velocity and m is the mass of the person plus the equipment. For simplicity
choose m = 1 and a = 1. g is 9.8 m/sec2. If the initial velocity is v(0) = 0.5 m/sec
calculate v(t).
11.17 The evolution of population density x of species in a place is governed by the
logistic equation ˙x = kx(1−x), where k is a constant parameter. Its exact solution
is
x(t) = x(0)
x(0) + (1 − x(0))e−kt .
Find the numerical solution for k = 0.5 and x(0) = 0.5.
11.18 Find the numerical solution of the RC circuit equation
q˙ + q/(RC) − V /R = 0, R = 10 Ω, C = 0.1 farad, V = 4V
with the initial condition q(0) = 0. The exact solution of the equation is q(t) =
CV + (q(0) − CV )e−t/RC .
11.19 The RL circuit equation is given by
i
 + iR/L − V /L = 0, R = 5Ω, L = 10 henry, V = 2 volts.
Solve the equation with i(0) = 0. Its exact solution is
i(t) = V
R +

i(0) − V
R

e−t/(L/R) .
11.20 In a chemical reaction cane sugar and water in the presence of the catalyst H+
react to form glucose. The rate equation describing the time variation of concen￾tration x(t) of glucose is given by ˙x = k(α − x), where k is a positive constant
for reaction and α is the initial concentration of cane sugar. Find the numeri￾cal solution for k = 1, α = 0.5 and x(0) = 5. The exact solution is given by
x(t) = α + (x(0) − α)e−kt.
11.21 The intensity I(x) of sound waves travelling through a medium is given by the
equation dI/dx = −I, where x is the distance travelled. If I(0) = 3 units find the
numerical solution. The exact solution is I(x) = I(0)e−x.
11.22 A hot object of temperature T0 = 60◦C is put in an environment of temperature
T1 = 30◦C. The variation of temperature of the object is governed by the New￾ton’s cooling law given by T˙ = −(T −T1). Find the numerical solution. The exact
solution is T(t) = T1 + (T0 − T1)e−t
.268 Ordinary Differential Equations – Initial-Value Problems
11.23 The Hermite differential equation is given by ¨x−2tx˙ +2x = 0. Find the numerical
solution for x(0) = 0, ˙x(0) = 2. The exact solution is x = 2t.
11.24 A particle of unit mass moves in a vertical direction under the force of gravity.
If the downward direction is positive the equation of motion is ¨x − g = 0, g =
9.8 m/sec
2
. For the initial conditions x(0) = 0, ˙x(0) = v0 = 1 solve the equation.
The exact solution of the equation is
x(t) = x(0) + v0t +
1
2
gt2 .
11.25 Find the numerical solution of the equation N˙
1 = −λ1N1, N˙
2 = λ1N1 −λ2N2 for
λ1 = 0.7 hour−1, λ2 = 0.14 hour−1 with the initial conditions N1(0) = N0 = 100
and N2(0) = 0. Its exact solution is
N1 = N0e−λ1t
, N2 = λ1
λ2 − λ1
N0

e−λ1t − e−λ2t

.12
Symplectic Integrators for Hamiltonian Systems
12.1 Introduction
Hamiltonian systems of ordinary differential equations describing non-dissipative phenom￾ena occur in many fields of physics, chemistry and other branches of science. Numerical
methods such as the Euler and the Runge–Kutta discussed in the previous chapter are
not appropriate for numerically solving Hamiltonian systems. These methods do not con￾serve the total energy (H). Further, such systems are not structurally stable against non￾Hamiltonian perturbations. The numerical approximation to the differential equations of
Hamiltonian systems essentially introduce non-Hamiltonian perturbations. Consequently,
the long-time behaviour obtained from an ordinary numerical method will be completely
different from the actual behaviour of the original Hamiltonian system and the Hamiltonian
is not conserved, that is, the Hamiltonian associated with the numerical solution is not a
constant of motion. To overcome this problem special types of methods called symplec￾tic integrators are developed for Hamiltonian systems. Symplectic integrators are a special
type of geometric integrators. Geometric integration methods are the numerical methods
developed for Hamiltonian systems that preserve at least any one of the geometric prop￾erties [1–7] of the flow of the Hamiltonian systems. Why are the symplectic methods called
so? Because when these methods are employed to Hamiltonian systems they preserve the
so-called linear symplectic structure that are inherent in the phase space.
The first symplectic method was proposed by de Vogelaere [8] and later by Ruth [9].
Symplectic integration algorithms are important in many branches of science. For instance,
in the simulation of particle accelerators and in the numerical fluid analysis the conservation
of symplectic structure is very important. Symplectic integration methods are very useful
for accurate computation of singularities, long-term integration of solar systems, analysis
of highly oscillatory systems, study of molecular dynamics and astronomy [7,10]. Highly
accurate determination of track of dynamical variables over a long time is very important
in accelerators like the Large Hadron Collider, interplanetary spacecraft and near-earth
satellites for which symplectic integrators are helpful.
Symplectic methods of Hamiltonian systems have many features concerning long-time
integration. Particularly, they have
1. no divergence of error in the energy (H),
2. linear error growth in the canonically conjugate dependent variables instead of
quadratic growth and
3. correct qualitative behaviour.
The present chapter, first briefly introduces some basic ideas of Hamiltonian systems
and shows the failure of Euler method in solving them. Then, presents a few symplec￾tic integration methods. For more details on symplectic integrators the refs. [1–19] can be
referred.
DOI: 10.1201/9781032649931-12 269270 Symplectic Integrators for Hamiltonian Systems
12.2 Hamiltonian Systems and Hamilton’s Equation of Motion
The Lagrangian of a classical mechanical system is defined as
L = T − V, (12.1)
where T and V are kinetic and potential energies, respectively. Assume that V is a function
of generalized coordinates q only while T is a function of q and q˙ . The Euler–Lagrange
equations describing the motion of the system are
d
dt
∂L
∂q˙

− ∂L
∂q
= 0 . (12.2)
Hamilton introduced the coordinates
p = ∂L
∂q˙ (12.3)
and defined the Hamiltonian
H(q, p) = T + V. (12.4)
H represents the total energy of the system and p is called the conjugate generalized mo￾mentum. Hamilton showed that Eqs. (12.2) are equivalent to the Hamilton’s equations
q˙ = ∂H
∂p , (12.5a)
p˙ = −∂H
∂q . (12.5b)
Examples:
1. Harmonic Oscillator
The Hamiltonian of a one-dimensional particle in the potential V = q2/2 is
H = 1
2

p2 + q2
= c , a constant. (12.6)
The associated equations of motion are
q˙ = p , p˙ = −q . (12.7)
2. Pendulum System
The Hamiltonian of a pendulum system is
H = 1
2
p2 − cos q . (12.8)
The Hamilton’s equations of motion are
q˙ = p , p˙ = − sin q . (12.9)
The phase space of the system (12.5) is an abstract space formed by the coordinates qi and
pi. For a one-dimensional particle, the coordinates axes of the phase space are qx and pxHamiltonian Systems and Hamilton’s Equation of Motion 271
only and hence its dimension is 2. For a particle moving in a three-dimensional real space
q and p have components qx, qy, qz and px, py, pz, and hence its phase space dimension is
6. Each point in a phase space represents a state of the system at a given time. As time
changes, the state of the system evolves and the particle traces out a trajectory in the phase
space.
What is the rate of change of phase space volume under time evolution? To determine
this let us consider the equation of motion of the form
dX
dt = F(X), (12.10)
where X = (x1, x2,...,xn) and F = (F1, F2,...,Fn). The Hamilton’s Eqs. (12.5) can be
rewritten in the above form. The rate of change of a volume is given by
Λ =  ∂Fi
∂xi
= ∇ · F . (12.11)
The conditions ∇ · F = 0, < 0 and > 0 represent conservative, dissipative and volume
expanding systems, respectively. For details about the various types of solutions and the
behaviours exhibited by these systems, a reader may refer to the refs. [20–23].
For a Hamiltonian system of the form
q˙i = ∂H
∂pi
, p˙i = −∂H
∂qi
, i = 1, 2,...,n (12.12)
define X = (x1, x2,...,x2n)=(q1, q2,...,qn, p1, p2,...,pn). Then,
∇ · F = 
2n
k=1
∂Fk
∂xk
= n
i=1
 ∂
∂qi
∂H
∂pi

+
∂
∂pi

−∂H
∂qi

= 0 . (12.13)
At t = 0 choose a closed (N − 1) dimensional surface S0 in the N-dimensional phase space
and then evolve each point on the surface S0 forward in time by using them as a set of
initial conditions. S0 evolves to a closed surface St at some later time t. The N-dimensional
volume V0 of the region enclosed by S0 and V (t) of the region enclosed by St are the same.
That is, V (t) = V (0). Therefore, the system is a volume preserving in phase space and is
conservative.
From Eq. (12.5) notice that
dH
dt = ∂H
∂q
dq
dt +
∂H
∂p
dp
dt = −p˙q˙ + ˙qp˙ = 0 . (12.14)
Further, for ω = dp ∧ dq (called symplectic manifold), where ∧ represents wedge product,
dω
dt
= d ˙p ∧ dq + dp ∧ d ˙q
= − (∇qqH) dq ∧ dq + dp ∧ (∇ppH) dp
= 0 . (12.15)
Equation (12.5) conserves the energy and the symplectic form ω = dp ∧ dq.272 Symplectic Integrators for Hamiltonian Systems
q
p
➤
➤
(a)
-1 -0.5 0 0.5 1
1
0.5
0
-0.5
-1
q
p
➤
➤
(b)
-2 -1 0 1 2
2
1
0
-1
-2
FIGURE 12.1
The numerical solution of (a) the linear harmonic oscillator Eq. (12.7) and (b) the pendulum
Eq. (12.9) obtained by the explicit Euler method with step size h = 0.01. For Eq. (12.7)
the energy is H = 0.5 and the initial conditions are q(0) = 1 and p(0) = 0. For Eq. (12.9)
q(0) = 1.6 and p(0) = 0.
12.3 Application of the Explicit Euler and the Runge–Kutta
Methods
First, consider the harmonic oscillator Eq. (12.7). Its exact solution is
q(t) = q(0) cost + p(0) sin t , (12.16a)
p(t) = −q(0) sin t + p(0) cost . (12.16b)
The Hamiltonian is a conserved quantity (dH/dt = 0), that is, H is a constant of time. The
solutions for different initial conditions are bounded and periodic.
A forward Euler discretization of Eq. (12.7) gives
qn+1 − qn
h = pn, −→ qn+1 = qn + hpn, (12.17a)
pn+1 − pn
h = −qn , −→ pn+1 = pn − hqn. (12.17b)
Now, obtain
1
2

p2
n+1 + q2
n+1
= 1
2

1 + h2 p2
n + q2
n

. (12.18)
That is, Hn+1 = (1 + h2)Hn. H increases with n and H is not a conserved quantity.
Consequently, the solution obtained by the Euler method is unbounded and nonperiodic.
Figure 12.1a shows the plot of the numerical solution obtained by the Euler method with h =
0.01. The initial conditions used are q(0) = 1, p(0) = 0 and H(0) = 0.5. The solution spirals
outward and diverges to infinity. Similar result is obtained for the pendulum Eq. (12.9)
(Fig. 12.1b).
The fourth-order Runge–Kutta method is also found to be unsuitable for Eqs. (12.7)
and (12.9). However, H is conserved for much longer time compared to Euler method.
Figure 12.2a depicts the variation of the error in the Hamiltonian (He) computed from the
numerical solution (q, p) for different values of h in the case of the Euler method. Here,
He = Hexact − Hnumerical with Hexact = 0.5. He diverges rapidly with time. Figure 12.2bBasic Idea of Symplectic Methods 273
(a)
     
  

e
0 100 200 300 400 500
25
0
-25
-50
-75
(b)
  
  
  


  

e
0 5 10 15 20 25
0.5
0.4
0.3
0.2
0.1
0
FIGURE 12.2
Variation of the error associated with the Hamiltonian computed from the numerical solu￾tion of the linear harmonic oscillator. (a) Solution is obtained by the explicit Euler method
with three different step sizes. (b) Solution is obtained by the fourth-order Runge–Kutta
method. In both the methods q(0) = 1, p(0) = 0 and H(0) = 0.5.
shows the result for the fourth-order Runge–Kutta method. The Euler method produced
solutions which are diverging to ±∞. In Fig. 12.2b as t → ∞, He → 0.5, that is Hnumer = 0,
which implies q(t), p(t) → 0 as t → ∞. The solution approached the equilibrium point
(q, p) = (0, 0).
From the above examples, it is clear that ordinary numerical methods are not suitable
for numerically integrating Hamiltonian systems. A reason is that the numerical approx￾imation to the ordinary differential equation of the equation of motion of a Hamiltonian
system introduces a non-Hamiltonian perturbation. Consequently, the Hamiltonian H is
not conserved. Numerical methods are developed for Hamiltonian systems which preserve
their features and are called symplectic methods or symplectic integrators. The basic idea
of symplectic methods is presented in the next section.
12.4 Basic Idea of Symplectic Methods
In symplectic methods the Hamiltonian is not strictly conserved, however, it undergoes
bounded oscillations. In contrast, as shown above, in nonsymplectic methods like the Euler274 Symplectic Integrators for Hamiltonian Systems
method the energy would increase without limit. The symplectic methods produce be￾haviour which looks like that of a Hamiltonian system. This is an indication that the non￾Hamiltonian perturbations introduced by the symplectic methods are much smaller than
those introduced by the nonsymplectic methods. A symplectic integrator preserves phase
space volume to a desired accuracy and also preserves qualitative properties of phase space
trajectories. Even though energy is not exactly conserved the orbits do not cross. However,
symplectic methods are generally implicit and require more function evaluations and smaller
time steps compared to nonsymplectic methods.
Consider a two degrees of freedom Hamiltonian system with coordinate denoted by q
and the conjugate momentum by p. The equation of motion is
q˙ = ∂H
∂p , p˙ = −∂H
∂q , (12.19)
where H = H(q, p) is the Hamiltonian. For n-degrees of freedom, Eq. (12.19) becomes
Eq. (12.12). Equation (12.19) can be rewritten as
 q˙
p˙

=
 0 1
−1 0   ∂H/∂q
∂H/∂p 
. (12.20)
or
Z˙ = J ∂H
∂Z , Z =
 q
p

, J =
 0 1
−1 0 
, (12.21a)
and
∂H
∂Z =
 ∂H/∂q
∂H/∂p 
. (12.21b)
The matrix J is called symplectic matrix (which means interwined matrix) [24].
The transformation
Z (t0) = Z(t) (12.22)
conserves both the energy (H) and
S (Z1, Z2) = ZT
1 JZ2 , (12.23)
where S is the area of the parallelogram defined by the two initial vectors Z1 and Z2.
Let us write
Z(t) = AZ(0). (12.24)
Then, Eq. (12.23) becomes
S (Z1, Z2) = ZT
1 (0)AT JAZ2(0) = ZT
1 (0)JZ2(0), (12.25)
where AT HA = J. The solution of Eq. (12.19) induces a transformation A on the phase
space and the associated Jacobian is A. The associated map is called symplectic.
Example:
Consider the linear harmonic oscillator Hamiltonian (Eq. (12.6)) and its equation of motion
(Eq. (12.7)). Its solution (12.16) can be rewritten in the form of Eq. (12.24) as
Z(t) =  cost sin t
− sin t cost
  q(0)
p(0) 
= AZ(0). (12.26)Explicit Euler Method is Not a Symplectic Method 275
For the linear harmonic oscillator Hamiltonian
AT JA =
 cost − sin t
sin t cost
  0 1
−1 0   cost sin t
− sin t cost

=
 0 1
−1 0 
= J . (12.27)
Thus, the transformation (12.22) not only conserves H but also preserves the symplectic
form given by Eq. (12.23).
Suppose, a numerical method approximates the solution of a differential equation, for
example Eq. (12.19), and the time step h is a constant. Let the exact solution of the equation
is ψ(h) and the approximation is ψh. The map ψh to be symplectic if it satisfies the equation
ψT
h Jψh = J . (12.28)
ψh has to be symplectic if ψ(h) is symplectic. For a symplectic ψ(h) certain numerical meth￾ods produce nonsymplectic ψh. An example is the Euler method. The numerical methods
which give rise to symplectic map are called symplectic numerical methods.
12.5 Explicit Euler Method is Not a Symplectic Method
In this section, let us apply the Euler method to the linear harmonic oscillator and show
that it is nonsymplectic.
The Euler algorithm for Eq. (12.7) is given by Eq. (12.17). A discrete map of the map
Zn+1 = F (Zn) (12.29)
is said to be conservative if the determinant of the Jacobian is 1:
|J| = |∂F/∂Z|
= 1 . (12.30)
For the map (12.17)
|J| =




1 h
−h 1




= 1+ h2 . (12.31)
Since |J| = 1, the map (12.17) is not conservative or area-preserving. Further, as h2 > 0
and 1 + h2 > 1 the map is area expanding.
Consider
q(h) = q(0) + hp(0), (12.32a)
p(h) = p(0) − hq(0). (12.32b)
Or
Z(h) =  1 h
−h 1
  q(0)
p(0) 
= AZ(0). (12.33)276 Symplectic Integrators for Hamiltonian Systems
The symplectic form (12.23) or (12.25) is
S = ZT
1 (0)ATJAZ2(0)
= ZT
1 (0)  1 −h
h 1
  0 1
−1 0   1 h
−h 1

Z2(0)
= ZT
1 (0)
 h 1
−1 h
  1 h
−h 1

Z2(0)
= ZT
1 (0)
 0 1+ h2
−1 − h2 0

Z2(0)
= ZT
1 (0) 
1 + h2  0 1
−1 0 
Z2(0)
= 
1 + h2
ZT
1 (0)JZ2(0). (12.34)
That is, the symplectic form is increased by a factor (1 + h2) at each time step. When
Eq. (12.17) is viewed as a discrete version of Eq. (12.7) the following can be noticed:
1. Equation (12.7) is area-preserving (conservative) while Eq. (12.17) is area ex￾panding.
2. The (exact)solution of Eq. (12.7) preserves the symplectic form. But the solution
of Eq. (12.17) not preserve the symplectic form.
The point is that the explicit Euler algorithm is not a symplectic algorithm.
12.6 First-Order Symplectic Algorithms
For the equation ˙x = f(x), x(t0) = x0, an integration scheme is said to be nth-order if
x1 − x(t0 + h) = O(hn+1) as h → 0, where x1 is the result obtained by applying the
numerical scheme to the initial state x0 for one time step. Let us begin with simple first￾order methods.
The simplest symplectic method is the leapfrog method. Restrict to the equations of the
form
q˙ = p , p˙ = F(q), (12.35)
where q = (q1, q2,...,qN ), p = (p1, p2,...,pN ) and F = (f1, f2,...,fN ). The leapfrog
method looks like one of the following two ways.
1. One approach is the discretization of Eq. (12.35) in the form
qn+1 = qn + hpn , (12.36a)
pn+1 = pn + hF (qn+1) . (12.36b)
In the above algorithm, first q is updated using pn and qn and then p is updated
using the current pn and the updated latest q, qn+1.
2. The second approach first determines pn+1 using qn and pn and then determines
qn+1 using the current qn and the updated p, namely, pn+1. This gives
pn+1 = pn + hF (qn) , (12.37a)
qn+1 = qn + hpn+1 . (12.37b)First-Order Symplectic Algorithms 277
Because of the alternating process in the algorithms (12.36) and (12.37) these methods are
called leapfrog methods. The above two algorithms are accurate to first-order in h.
For the linear harmonic oscillator Eq. (12.7) the algorithm (12.36) becomes
qn+1 = qn + hpn , (12.38a)
pn+1 = 
1 − h2
pn − hqn . (12.38b)
Then,
|J| =




1 h
−h 1 − h2




= 1 . (12.39)
The map (12.38) is area-preserving. The discrete evolutionary operator ψh for (q, p) from
Eq. (12.38) is written as [11]
ψh
 q
p

=
 1 h
−h 1 − h2
  q
p

. (12.40)
Note that det(ψh) = 1 and ψh is a symplectic.
Next, the symplectic form (12.25) is
S = ZT
1 (0)  1 h
−h 1 − h2
  0 1
−1 0   1 −h
h 1 − h2

Z2(0)
= ZT
1 (0)  −h 1
−1 + h2 −h
  1 −h
h 1 − h2

Z2(0)
= ZT
1 (0)  0 1
−1 0 
Z2(0)
= ZT
1 (0)JZ2(0). (12.41)
Thus, the formula (12.38) preserves the symplectic form and hence it is a symplectic inte￾grator. Similarly, it can be shown that the formula (12.37) for the linear harmonic oscillator
is also a symplectic integrator. The algorithms (12.36) and (12.37) are called implicit Euler
algorithms or symplectic Euler algorithms. Why is the symplectic Euler method called a
first-order method?[5].
What is p2 + q2?
For the continuous time dynamical system (12.7)
p2 + q2 = 2H = C2 (12.42)
which is an equation for a circle Γ in p − q phase space. The solutions of the ordinary
differential equation of the linear harmonic oscillator with different values of H form circles
in p − q plane. Let us compute p2 + q2 for the map (12.38). Solving the map (12.38) for qn
and pn gives
qn = 
1 − h2
qn+1 − hpn+1 , (12.43a)
pn = pn+1 + hqn+1 . (12.43b)
Then,
C2 = 2H
= p2
n + q2
n
= p2
n+1 + h2q2
n+1 + 2hpn+1qn+1 + 
1 − h22
q2
n+1
+h2p2
n+1 − 2h

1 − h2
pn+1qn+1
= 
1 + h2
p2
n+1 + 
1 + h4 − h2
q2
n+1 + 2h3pn+1qn+1 (12.44)278 Symplectic Integrators for Hamiltonian Systems
(a)


-1 -0.5 0 0.5 1
1
0.5
0
-0.5
-1
(b)


-1 -0.5 0 0.5 1
1
0.5
0
-0.5
-1
FIGURE 12.3
Comparison of the numerical solution with the exact solution (at some selected values of t)
for the linear harmonic oscillator Eq. (12.7) with h = 0.01. The solid circles represent the
exact solution. Continuous curve is the numerical solution. Here, q(0) = 1, p(0) = 1 and
the exact H = 0.5. (a) The numerical solution is obtained by the implicit first-order Euler
algorithm given by Eq. (12.45). (b) The numerical solution is obtained by the algorithm
given by Eq. (12.46).
which is an equation for an ellipse γ. Under one iteration the circle Γ is given by p2
n +
q2
n = 2H = C2 which is mapped into the ellipse γ given by Eq. (12.44). However, both Γ
and γ have same area. Thus, the qualitative features of closed orbits and bound solutions
(nonperiodic) are preserved by the symplectic Euler methods while they are not preserved
in the ordinary Euler method.
Example:
For the linear harmonic oscillator Eq. (12.7) an implicit Euler algorithm (refer Eq. (12.36))
is
qn+1 = qn + hpn , (12.45a)
pn+1 = pn − hqn+1 . (12.45b)
In Eq. (12.45) q is first updated and this updated q is used to update p. An alternative
algorithm is
pn+1 = pn − hqn , (12.46a)
qn+1 = qn + hpn+1 . (12.46b)
In Eq. (12.46) p is first updated and then this p is used to update q. Equation (12.7) is solved
by the above two methods with time step h = 0.01 and with q(0) = 1 and p(0) = 0 and
H = 0.5. Figure 12.3 presents the numerical approximation along with the exact solution.
The obtained numerical solution not diverge in both methods. Figure 12.4 depicts the
variation of H associated with the numerical solution. H is not constant but it oscillates
and is bounded. It remains near its true value. Table 12.1 gives the numerical solution for
a few values of t.First-Order Symplectic Algorithms 279


0 50 100
0.51
0.5
0.49
FIGURE 12.4
Variation of Hamiltonian (energy) H of linear harmonic oscillator computed using the
numerical solution obtained by the symplectic Euler algorithm (12.45). Here, h = 0.01,
q(0) = 1, p(0) = 1 and the exact H = 0.5.
To compare the numerical solutions of different methods use the distance D between
the exact and the numerical solutions. It is defined as
D =

(qexact − qnumer )
2 + (pexact − pnumer )
2 . (12.47)
Table 12.2 presents D versus t with the nonsymplectic and the symplectic Euler methods.
TABLE 12.1
Comparison of the numerical solution with the exact solution of Eq. (12.7). The symplectic
Euler method, Eq. (12.45), with time step h = 0.01 is used. Here, q(0) = 1, p(0) = 0 and
the exact H = 0.5.
t qnumer qexact pnumer pexact Hnumer
10 −0.8417693 −0.8390715 0.5440629 0.5440211 0.5022899
20 0.4125709 0.4080821 −0.9129907 −0.9129453 0.5018834
30 0.1494348 0.1542514 0.9880247 0.9880316 0.4992618
40 −0.6633372 −0.6669381 −0.7450113 −0.7451132 0.4975290
50 0.9637098 0.9649660 0.2621771 0.2623749 0.4987367
60 −0.9538620 −0.9524130 0.3050525 0.3048106 0.5014549
70 0.6369639 0.6333192 −0.7740850 −0.7738907 0.5024653
80 −0.1150256 −0.1103872 0.9939378 0.9938887 0.5005716
90 −0.4439396 −0.4480736 −0.8938397 −0.8939967 0.4980159
100 0.8599997 0.8623189 0.5060126 0.5063656 0.4978241
200 0.4835508 0.4871877 0.8729019 0.8732973 0.4978895
500 −0.8852209 −0.8838493 0.4696180 0.4677718 0.5020786280 Symplectic Integrators for Hamiltonian Systems
TABLE 12.2
Distance D between the exact and the numerical solutions of the harmonic oscillator
Eq. (12.7) with the nonsymplectic and symplectic Euler methods. Here, H = 0.5, q(0) = 1,
p(0) = 0 and h = 0.01. DNSE, DSEQ and DSEP correspond to the distances obtained by
the ordinary Euler method (Eq. (12.17)), the symplectic Euler algorithms (Eq. (12.45)) and
Eq. (12.46), respectively.
t DNSE DSEQ DSEP
0 0.0000000 0.0000000 0.0000000
10 0.0512696 0.0026980 0.0027433
20 0.1051677 0.0044891 0.0046413
30 0.1618291 0.0048166 0.0050636
40 0.2213954 0.0036023 0.0038506
50 0.2840156 0.0012717 0.0013798
100 0.6486942 0.0023459 0.0027636
200 1.7181811 0.0036584 0.0051074
300 3.4814173 0.0037495 0.0062488
400 6.3884201 0.0029748 0.0057465
500 11.1811226 0.0023000 0.0038028
1000 147.3766221 0.0024475 0.0079505
12.7 A Second-Order Symplectic Algorithm
A second-order algorithm can be obtained by taking the time step as h/2 instead of h and
making use of the formulas (12.36) and (12.37). Use the algorithm Eq. (12.37) from t to
t + h/2 and the algorithm (12.36) from t + h/2 to t + h. These give the following set of four
equations:
p(t + h/2) = p(t) + 1
2
hF(q(t)), (12.48a)
q(t + h/2) = q(t) + 1
2
hp(t + h/2), (12.48b)
q(t + h) = q(t + h/2) + 1
2
hp(t + h/2), (12.48c)
p(t + h) = p(t + h/2) + 1
2
hF(q(t + h)). (12.48d)
Replacing p(t + h/2) and q(t + h/2) in Eqs. (12.48c–d) by Eqs. (12.48a–b), respectively,
leads to
q(t + h) = q(t) + hp(t) + 1
2
h2F(q(t)), (12.49a)
p(t + h) = p(t) + 1
2
h [F(q(t)) + F(q(t + h))] . (12.49b)Runge–Kutta Type Algorithms 281
Rewrite Eqs. (12.49) as
qn+1 = qn + hpn +
1
2
h2F (qn), (12.50a)
pn+1 = pn +
1
2
h [F (qn) + F (qn+1)]. (12.50b)
The above method is called St¨ormer–Verlet method. Comparison of this method with the
other symplectic algorithms is discussed at the end of the next section.
12.8 Runge–Kutta Type Algorithms
For separable Hamiltonians of the form
H(q, p) = T(p) + V (q), (12.51)
where q = (q1, q2,...,qN ), p = (p1, p2,...pN ) explicit Runge–Kutta type algorithms exist
which preserve the symplectic structure [1,14,15,25]. This section describes the symplectic
method of Candy and Rozmus [25] for separable Hamiltonians.
Consider the Hamiltonian of the form (12.51). The aim is to find a series of difference
equations which preserve ω2 = dq∧dp and give the approximation of (q(t = h), p(t = h)) for
a given (q(0), p(0)) = (q0, p0). (If the error in the difference approximation is O(hn), where
h is the step size then the approximation is said to be an nth-order symplectic method.)
That is, the goal is to find a map of the form
(q, p) −→ (q0, p0) + O 
hn+1
, (12.52)
where q0 and p0 represent approximations to q0 and p0, respectively. If it is possible to
find a set of transformations which leave the Hamiltonian with the final form
H (q0, p0) = ∞
i=n
Hi (q0, p0) hi (12.53)
such that (q0, p0) −→ (q0, p0) as h → 0 then it can be proved that Eq. (12.52) is satisfied
[25].
12.8.1 Generating Function
In order to find an nth-order integration algorithm, introduce the series of l (canoni￾cal)transformations
(ql, pl)
Kl
−−→ (ql−1, pl−l)
Kl−1
−−−−→ · · ·
Kl
−−→ (q0, p0) . (12.54)
In Eq. (12.54) (q0, p0) is the initial condition at t = 0, (ql, pl) is the numerical approxima￾tion at t = t0 + lh and the other set (q1, p1),...,(ql−1, pl−1) are intermediate points. The
choice
Ki = Ki (qi−1, pi, t)
= −qi−1 · pi − h [ai T (pi) + biV (qi−1)] , i = 1, 2,...,l (12.55)282 Symplectic Integrators for Hamiltonian Systems
gives
pi−1 = −∇qi−1Ki = pi + hbi∇qi−1 V (qi−1) , (12.56a)
qi = −∇piKi = qi−1 + hai∇piT (pi) (12.56b)
and
Hi−1 = Hi + ∂tKi = Hi − [aiT (pi) + biV (qi−1)] . (12.57)
Define F(q) = −∇qV (q) and P(p) = −∇pT(p). Then, from Eqs. (12.56)
qi = q0 + h

i
m=1
amP (pm) , (12.58a)
pi = p0 + h

i
m=1
bmF (qm−1) , i = 1, 2, . . . ,l. (12.58b)
To determine the unknown coefficients ai and bi expand qi and pi so that
H0 (q0, p0) =
n
−1
m=1
Hm (ai, bi, q0, p0) hm + O (hn) . (12.59)
ai and bi are found by setting Hm = 0 for m = 0, 1,...,n − 1. Generally, for an equation of
the form ˙x = f(x(t)) a K-stage Runge–Kutta method is
yi = x0 + h

K
m=1
aimf (ym) , i = 1, 2,...,K (12.60a)
x1 = x0 + h

K
m=1
bmf (ym), (12.60b)
where x0, x1 are the values of x at time t = t0 and t = t0 + h. Since Eqs. (12.58) are of
the form of Eqs. (12.60) the method is called Runge–Kutta type symplectic method since
Eqs. (12.58) are in the form of the Runge–Kutta algorithms.
12.8.2 Hm for n ≤ 4
For n ≤ 4
qi = q0 +
3
j=1
αjihj + O 
h4
, (12.61a)
pi = p0 +
3
j=1
βjihj + O 
h4
, (12.61b)
where
α1i = P 
i
m=1
am , α21 = (F · ∇p) P 
i
m=1
am
m
r=1
br , (12.61c)
α3i = 1
2 (F · ∇p)
2 P 
i
m=1
am

m
r=1
br
2
+ [((P · ∇q) F) · ∇p ] P 
i
m=2
am
m
r=2
br
r−1
s=1
as , (12.61d)Runge–Kutta Type Algorithms 283
and
β1i = F 
i
m=1
bm , β2i = (P · ∇p) F 
i
m=2
bm
m
−1
r=1
ai , (12.61e)
β3i = 1
2 (P · ∇q)
2 F 
i
m=2
bm
m
−1
r=1
ar
2
+ [((F · ∇p) P) · ∇q ] F 
i
m=2
bm
m
−1
r=1
br
r
s=1
bs . (12.61f)
Next, expand V (qi) and T(pi) as
V (qi) = V (q0) + h [α1i · ∇q] V (q)
+h2

α2i · ∇q +
1
2 (α1i · ∇q)
2

V (q) + h3

α3i · ∇q +
1
6 (α1i · ∇q)
3
+ (α1i · ∇q) (α2i · ∇q)

V (q) + O 
h4 (12.62a)
and
T (pi) = T (p0) + h [β1i · ∇p] T(p)
+h2

β2i · ∇p +
1
2 (β1i · ∇p)
2

T(p) + h3

β3i · ∇p +
1
6 (β1i · ∇p)
3
+ (β1i · ∇p) (β2i · ∇p)

T(p) + O 
h4
. (12.62b)
In Eqs. (12.61) and (12.62) the derivatives are evaluated at (q0, p0). Substitution of
Eqs. (12.61) and (12.62) in Eq. (12.59) results in
H0 (q0, p0) = 
3
i=0
hi
Hi + O 
h4
, (12.63)
where H0 and H1 are
H0 = T (p0)

1 −n
i=1
ai

+ V (q0)

1 −n
i=1
bi

, (12.64a)
H1 = (F0 · P0)

n
m=1
bm −n
i=1
ai

i
m=1
bm − n
m=1
am +n
i=2
bi

i−1
m=1
am

. (12.64b)
H2 is
H2 = (P0 · ∇q) (F · P0)

n
m=2
bm
m
−1
r=1
ar −n
i=2
ai

i
m=2
bm
m
−1
r=1
ar
−1
2
n
m=1
am
2
+
1
2
n
i=2
bi

i−1
m=1
am
2

− (F0 · ∇p) (P · F0)

n
m=1
am
m
r=1
br −n
i=2
bi

i−1
m=1
am
m
r=1
br
−1
2
n
m=1
bm
2
+
1
2
n
i=1
ai

i
m=1
bm
2
 . (12.64c)284 Symplectic Integrators for Hamiltonian Systems
H3 is worked out as
H3 = (P0 · ∇q)
2 (F · P0)


1
2
n
m=2
bm
m
−1
r=1
ar
2
− 1
6
n
m=1
am
3
−1
2
n
i=2
ai

i
m=2
bm
m
−1
r=1
ar
2
+
1
6
n
i=2
bi

i−1
m=1
am
3

− (F0 · ∇p)
2 (P · F0)


1
2
n
m=1
am
m
r=1
br
2
− 1
6
n
m=1
bm
3
−1
2
n
i=2
bi

i−1
m=1
am
m
r=1
br
2
+
1
6
n
i=1
ai

i
m=1
bm
3

+ (P0 · ∇q) (F0 · ∇p) (P · F)

n
m=2
bm
m
−1
r=1
ar
r
s=1
bs
− n
m=2
am
m
r=2
br
r−1
s=1
as −n
i=2
ai

i
m=2
bm
m
−1
r=1
ar
r
s=1
bs
+
n
i=3
bi

i−1
m=2
am
m
−1
r=2
br
r−1
s=1
as + n
m=1
bm
n
m=2
bm
m
−1
r=1
ar
−
n
i=2
ai

i
m=1
bm

i
m=2
bm
m
−1
r=1
ar − n
m=1
am
n
m=1
am
n
r=1
br
+
n
i=2
bi

i−1
m=1
am
m
r=1
br

. (12.64d)
12.8.3 Second-Order Algorithm
To obtain an algorithm which is second-order in h (n = 2) equate H0 and H1 to zero and
obtain
H0 = T (p0) (1 − a1 − a2) + V (q0) (1 − b1 − b2)=0 , (12.65a)
H1 = (F0 · P0) (b1 + b2 − a1b1 − a2b1 − a2b2 − a1 − a2 + a1b2)
= 0 . (12.65b)
Since T(p0), V (q0) and (F0 · P0) need not be zero, choose
1 − a1 − a2 = 0, 1 − b1 − b2 = 0, (12.66a)
b1 + b2 − a1b1 − a2b1 − a2b2 − a1 − a2 + a1b2 = 0 (12.66b)
and obtain
a1 + a2 = 1, b1 + b2 = 1, a1b1 + a2 − a1b2 = 0 . (12.67)
There are three equations for the four unknowns a1, a2, b1 and b2. The choice b2 = 1 gives
b1 = 0. Then, the last equation in (12.67) gives a1 = a2. Since a1 + a2 = 1 the result is
a1 = a2 = 1/2. Thus, a solution of Eq. (12.67) is
a1 = a2 = 1/2, b1 = 0, b2 = 1 . (12.68)Runge–Kutta Type Algorithms 285
12.8.4 Third-Order Algorithm
For a third-order algorithm
a1 = 2/3, a2 = −2/3, a3 = 1, b1 = 7/4 b2 = 3/4, b3 = −1/24 . (12.69)
12.8.5 Fourth-Order Algorithm
For n = 4, equating H0, H1, H2 and H3 separately to zero leads to
a1 + a2 + a3 + a4 − 1=0 , (12.70a)
b1 + b2 + b3 + b4 − 1=0 , (12.70b)
a1b2 + (a1 + a2)b3 + (1 − a4)b4 − 1
2 = 0 , (12.70c)
a1b2
1 + a2(b1 + b2)
2 + a3(1 − b4)
2 + a4 − 1
3 = 0 , (12.70d)
a2
1b2 + (a1 + a2)
2b3 + (1 − a4)
2b4 − 1
3 = 0 , (12.70e)
a1b3
1 + a2(b1 + b2)
3 + a3(1 − b4)
3 + a4 − 1
4 = 0 , (12.70f)
b2a3
1 + (a1 + a2)
3b3 + (1 − a4)
3b4 − 1
4 = 0 , (12.70g)
a1b2 + (a1 + a2)b3 [a1b1 + a2(b1 + b2)]
+(a1 − a4)(1/2 − a4)b4 − (b1 + b2)a1a2b2
−a3(1 − b4)

1
2 − b4(1 − a4)

− 1
2
a4 = 0 . (12.70h)
An analytic solution of the above set of equations is
a1 = 1
6

2+21/3 + 2−1/3

, (12.71a)
a2 = 1
6

1 − 21/3 − 2−1/3

, (12.71b)
a3 = a2, a4 = a1, (12.71c)
b1 = 0, b2 = 1/

2 − 21/3

, (12.71d)
b3 = 1/

1 − 22/3

, b4 = b2 . (12.71e)
12.8.6 General Algorithm for nth Order Method
From Eqs. (12.58) the general scheme for the calculation of (q(h), p(h)) = (qn, pn) at t = h
from the given (q(0), p(0)) = (q0, p0) is
pi = pi−1 + hbiF (qi−1) , (12.72a)
qi = qi−1 + haiP (pi) , i = 1, 2, . . . , n. (12.72b)
The above algorithm is called nth-order Candy–Rozmus method. Since b1 = 0 in the
second-order solution set (12.68) and the fourth-order set (12.71) only one evaluation of F
and three evaluations of P are necessary in the second-order and fourth-order algorithms,
respectively. For a detailed analysis of the Candy–Rozmus method one may refer to ref. [25].286 Symplectic Integrators for Hamiltonian Systems
(a)
 
 
0 2 4 6
0
-5
-10
-15
(b)
 


0 2 4 6
0
-5
-10
FIGURE 12.5
(a) Variation of HH, the absolute difference between the exact value of the Hamiltonian
and the numerically calculated value of the Hamiltonian of the harmonic oscillator obtained
by the three symplectic methods with h = 0.01. (b) Plot of the error D, Eq. (12.47), versus
time in log10 scale for the three symplectic methods. In both the subplots, solid circles,
dashed curve and continuous curve represent the result obtained by the implicit Euler,
St¨ormer–Verlet and the fourth-order methods, respectively.
For a time-dependent potential V = V (q, t) the algorithm is
pi = pi−1 + hbiF (qi−1, ti−1) , (12.73a)
qi = qi−1 + haiP (pi) , (12.73b)
ti = ti−1 − hai , i = 1, 2, . . . , n. (12.73c)
The linear harmonic oscillator Eq. (12.7) is solved by the St¨ormer–Verlet and the fourth￾order algorithms with
H = 0.5, q(0) = 1, p(0) = 0 and h = 0.01. (12.74)
Figure 12.5a shows the variation of the Hamiltonian H against time t in log10 − log10 scale
for the three symplectic methods considered so far. As expected the variation of H is
relatively very small and of the order of 10−10 for the fourth-order method. In all the
methods H is not diverging. Figure 12.5b presents the error D, Eq. (12.47), versus t for the
three methods. For large t, the errorD grows linearly for all the three methods.
Example:
Find the numerical solution of the harmonic oscillator Eq. (12.17) at t = 1 subjected to
the initial conditions q(0) = 1 and p(0) = 0 by the explicit Euler method (Eq. (12.17)), the
symplectic Euler method (Eq. (12.45)), the St¨ormer–Verlet method (Eq. (12.50)) and the
fourth-order Candy–Rozmus symplectic method (Eq. (12.72)) with h = 0.01. Also, compute
the error D in each method.
The exact solution at t = 1 is
q(1) = 0.5403023, p(1) = −0.8414710.Other Methods 287
By the explicit Euler method:
q(1) = 0.5430387, p(1) = −0.8456706, D = 0.50124e−2.
By the symplectic Euler method:
q(1) = 0.5445062, p(1) = −0.8414838, D = 0.42039e−2.
By the St¨ormer–Verlet method:
q(1) = 0.5402988, p(1) = −0.8414627, D = 0.89800e−5.
By the Candy–Rozmus method:
q(1) = 0.5403023, p(1) = −0.8414710, D = 0.87679e−9.
12.9 Other Methods
The symplectic algorithms considered so far are of order ≤ 4 and are for separable Hamil￾tonian systems. Much interest has been focused on developing higher-order symplectic in￾tegrators and integrators for certain special problems. Higher-order Runge–Kutta–Nystr¨om
methods are constructed [26–28]. Methods preserving first integrals (constants of motion)
have also been proposed [29]. For general Hamiltonians the symplectic Runge–Kutta meth￾ods are implicit [30,31], where one has to solve the implicit algebraic equations for the inter￾mediate stage values using an iterative approximation method like the Newton–Raphson.
If the potential V (q) of a Hamiltonian system has a singularity then the numerical inte￾gration of equations of motion would produce large errors when the trajectory approaches
the singularity. Symplectic integration methods have been proposed and analyzed for this
problem [10,32–35]. Symplectic variational integrators with energy and momentum conserv￾ing are developed [12].
Yoshida and McLachlan introduced splitting methods [36,37] for the equations of the
form
dY
dt = f1 + f2 . (12.75)
In their approaches the equation is decomposed into
dy1
dt = f1, dy2
dt = f2 (12.76)
so that both the equations in (12.76) can be integrated exactly or approximately using a
discrete method. Such methods have been used very successfully in the study of solar system
and molecular dynamics [10,38].
Numerical experiments [39,40] have shown that variable time step symplectic algorithms
lose their favourable properties for a long-time integration and are not superior to standard
nonsymplectic methods. Several methods have been suggested to overcome this difficulty
[13,41–45]. In an approach of Hairer [13], small perturbations are added to the discretiza￾tion such that the qualitative behaviour for long-time integration is re-stabilized. These
algorithms are theoretically justified and the practical considerations are discussed. In the
following, these algorithms [13] are briefly outlined.
The step size is chosen as h = s(q, p, ), where s > 0 is a state-dependent given function
and  > 0 is a small parameter. For a fixed initial value (q0, p0) with H0 = H(q0, p0) the
new Hamiltonian is
Hnew = s(q, p, ) (H(q, p) − H0) . (12.77)288 Symplectic Integrators for Hamiltonian Systems
The Hamilton’s equations
q = Hp , p = −Hq . (12.78)
become
q = sHp + sp (H − H0) , p = −sHq − sq (H − H0) . (12.79)
The introduced perturbation vanishes on the solution of Eqs. (12.78) passing through
(p0, q0) but makes the system Hamiltonian. Transformations such as in Eq. (12.77) are
already used in classical mechanics for an analytic treatment of Hamiltonian systems.
12.9.1 Symplectic Euler Method with q-Dependent Step Size Function
Consider the Hamiltonian systems of the form
H = 1
2
pTM−1p + V (q), (12.80)
where M is a constant symmetric matrix. Application of the Euler method with step size
function h = s to Eq. (12.79) with the Hamiltonian (12.80) gives the approximation at
t1 = t0 + s(q0, ) as
p1 = p0 − s (q0, ) Vq (q0) − sq (q0, )
1
2
pT
1 M−1p1 + V (q0) − H0

, (12.81a)
q1 = q0 + s (q0, ) M−1p1 . (12.81b)
A quadratic equation in p1 is obtained. To solve this let us consider the norm  p 2
M:=
pTM−1p and calculate the scalar β = pT
1 M−1p1 = p1 2
M from
β = p0 − sVq − sq (β/2 + V − H0) 2
M . (12.82)
After solving the above equation for β the numerical solutions p1 and q1 can be obtained
from Eqs. (12.81).
12.9.2 Verlet Scheme with q-Dependent Step Size Function
The St¨ormer–Verlet algorithm for the system (12.79) with H given by Eq. (12.80) and with
p-dependent function s is
pn+1/2 = pn − 1
2
s (qn, ) Vq (qn) − 1
2
sq (qn, )

H 
pn+1/2, qn

− H0

, (12.83a)
qn+1 = qn +
1
2
 (s (qn, ) + s (qn+1, )) M−1pn+1/2 , (12.83b)
pn+1 = pn+1/2 − 1
2
s (qn+1, ) Vq (qn+1)
−1
2
sq (qn+1, )

H 
pn+1/2, qn+1
− H0

, (12.83c)
tn+1 = tn +
1
2
 (s (qn, ) + s (qn+1, )) . (12.83d)
Equation (12.83b) is implicit in qn+1. Solve the scalar equation
γ = s

qn +
1
2
 (s (qn, ) + γ) M−1pn+1/2, 
(12.84)
for γ := s(sn+1, ). The Newton–Raphson iterations can be used because the derivative sq
is available.A Symplectic Integrator for Spin Systems 289
12.9.3 An Adaptive St¨ormer–Verlet Method
A variable step size method proposed by Huang and Leimkuhler [42] is
pn+1/2 = pn − 1
2
σn+1/2Vq (qn) , (12.85a)
qn+1 = qn + σn+1/2M−1pn+1/2 , (12.85b)
pn+1 = pn+1/2 − 1
2
σn+1/2Vq (qn+1) , (12.85c)
where σn+1/2 is defined by the recursion
σ1/2 = s (q0, ) , 1
σn+1/2
+
1
σn−1/2
= 2
s (qn, )
. (12.85d)
12.10 A Symplectic Integrator for Spin Systems
Classical spin systems are essentially noncanonical Hamiltonian systems. Examples of spin
systems include point vortices on a sphere, the Landau–Lifshitz equation of micromagnetics,
reduced motion of a spinning top and the classical limit of Heisenberg spin chains [46–48].
Such systems have phase space (S2)N . Moreover, symplectic is said to form the sum of the
standard area element on each sphere. If the spheres are realized as ||si||2 = 1, si ∈ R3 then
the Hamiltonian H is on (S2)N . The equations of motion are given by
s˙ i = si × ∇siH (s1,..., sN ) = fi (s1,..., sN ) . (12.86)
McLachlan, Modin and Verdier [49] proposed the integrating scheme
si,n+1 = si,n + hfi (u1,..., uN ) , ui = si,n + si,n+1
||si,n + si,n+1|| . (12.87)
This scheme is found to preserve certain structural properties associated with the exact flow
of the system.
12.11 Concluding Remarks
The symplectic integrators are shown to be successful in determining qualitative features of
conservative systems over a very long-time integration including 1-million-year, 5-million￾year and 210-billion-year integration. The simulation study of motion of Pluto has predicted
chaotic behaviour of it. Numerical simulation of dynamics of a large system called molecular
dynamics requires solving a system of nonlinear differential equations. Such systems are con￾servative and highly sensitive to discretization and round-off errors. Symplectiic integrators
are appropriate for molecular dynamics applications.
Symplectic methods are proposed for stochastic Hamiltonian systems governed by
stochastic ordinary differential equations and Hamiltonian systems subjected to non￾dissipative forces. Symplectic integrators in curved spacetimes and for inseparable Hamil￾tonian systems are developed. Symplectic methods are applicable to eigenvalue problems,
harmonic analysis, optimization and control problems, robotic dynamics and control and
quantum cosmology have been reported. [50-54].290 Symplectic Integrators for Hamiltonian Systems
12.12 Bibliography
[1] E. Hairer, C. Lubich and G. Wanner, Geometric Numerical Integration: Struc￾ture Preserving Algorithms for Ordinary Differential Equations. Springer, Berlin,
2002.
[2] J.M. Sanz-Serna, Acta Numerica 1:243, 1992.
[3] R.D. Skeel, G. Zhang and T. Schlick, SIAM J. Sci. Comput. 18:203, 1997.
[4] S. Blanes, F. Casas and J. Ros, SIAM J. Sci. Comput. 21:711, 1999.
[5] D. Donnelly and E. Rogers, Am. J. Phys. 73:938, 2005.
[6] R. McLachlan and G.R.W. Quispel, Six Lectures on the Geometric Integration of
Ordinary Differential Equations in Foundations of Computational Mathematics,
R. DeVore, A. Iserles and E. Suli (Eds). Cambridge University Press, Cambridge,
2001. pp.155-210.
[7] C.J. Budd and D. Piggott, Geometric Integration and Its Applications in Hand￾book of Numerical Analysis 11:35, 2003.
[8] R. de Vogelaere, Methods of integration which preserve the contact transformation
property of the Hamiltonian equations, Report No. 4, Dept. Math., Univ. of Notre
Dame, Notre Dame, Ind., 1956.
[9] R.D. Ruth, IEEE Trans. Nuclear Science 30:2669, 1983.
[10] B. Leimkuhler, Phil. Trans. Roy. Soc. London A 357:1101, 1999.
[11] J.M. Sanz-Serna and M.P. Calvo, Numerical Hamiltonian Problems. Chapman
and Hall, London, 1994.
[12] C. Kane, J.E. Marsden and M. Ortiz, J. Math. Phys. 40:3353, 1999.
[13] E. Hairer, Appl. Numer. Math. 25:219, 1997.
[14] R.I. McLachlan and P. Atela, Nonlinearity 5:541, 1992.
[15] E. Forest and R.D. Ruth, Physica D 43:105, 1990.
[16] P.J. Channel and C. Scovel, Nonlinearity 3:231, 1990.
[17] P.J. Channell and C. Scovel, Los Alamos National Laboratory LA–VR–88–1828,
1988.
[18] M.H. Holmes, Am. J. Phys. 88:60, 2019.
[19] T. Itoh and K. Abe, J. Comput. Phys. 77:85, 1988.
[20] A.M. Ozorio de Almeida, Hamiltonian Systems. Cambridge Univ. Press, Cam￾bridge, 1988.
[21] R.S. MacKay and J.D. Meiss, Hamiltonian Dynamical Systems. Adams Hilger,
Bristol, 1987.
[22] A.J. Lichtenberg and M.A. Lieberman, Regular and Stochastic Motion. Springer,
New York, 1983.
[23] M. Lakshmanan and S. Rajasekar, Nonlinear Dynamics: Integrability, Chaos and
Patterns. Springer, Berlin, 2003.
[24] F.J. Vesely, Computational Physics: An Introduction. Springer, New York, 1994.
[25] J. Candy and W. Rozmus, J. Comput. Phys. 92:230, 1991.Bibliography 291
[26] M. Qin and W.J. Zhu, Computing 47:309, 1992.
[27] S. Blanes, F. Casas and J. Ros, New families of symplectic Runge–Kutta–Nystr¨om
integration methods. In the Proceedings of the Second International Confer￾ence on Numerical Analysis and Its Applications. L. Vulkov, J. Wasniewski and
P. Yalamov (Eds.). Springer, London, 2001. pp.102-109.
[28] S. Blanes, F. Casas and J. Ros, App. Num. Math. 39:245, 2001.
[29] G.R.W. Quispel and G.S. Turner, J. Phys. A 29:L341, 1996.
[30] J.M. Sanz-Serna, BIT 28:877, 1988.
[31] X. Tan, J. Comput. Phys. 203:250, 2005.
[32] C.J. Budd, W. Huang and R.D. Russel, SIAM J. Sci. Comp. 17:305, 1996.
[33] C.J. Budd, B. Leimkuhler and M.D. Piggott, Appl. Numer. Math. 39:261, 2001.
[34] H. Arakida and T.Fukushima, Astron. J. 120:3333, 2000.
[35] C.J. Budd and M.D. Piggott, J. Comp. Appl. Math. 128:399, 2001.
[36] H. Yoshida, Phys. Lett. A 150:262, 1990.
[37] R.I. McLachlan, SIAM J. Sci. Comput. 16:15, 1995.
[38] Ch. Schlier and A. Seiter, J. Chem. Phys. A 102:9399, 1998.
[39] M.P. Calvo and J.M.Sanz-Serna, SIAM J. Sci. Comput. 14:936, 1993.
[40] B. Gladman, M. Duncan and J. Candy, Celestial Mech. Dyn. Astro. 52:221, 1991.
[41] E. Hairer and D. Stoffer, SIAM J. Sci. Comput. 18:257, 1997.
[42] W. Huang and B. Leimkuhler, SIAM J. Sci. Comput. 18:239, 1997.
[43] P. Hut, J. Makino and S. McMillan, Astrophys. J. 443:L193, 1995.
[44] R.D. Skeel and J.J. Biesiadecki, Ann. Numer. Math. 1:191, 1994.
[45] D. Stoffer, Computing 55:1, 1995.
[46] M. Lakshmanan, Philos. Trans. R. Soc. A: Math. Phys. Eng. Sci. 369:1280, 2011.
[47] S. Pekarsky and J.E. Marsden, J. Math. Phys. 39:5894, 1998.
[48] J.E. Marsden and T.S. Ratiu, Introduction to Mechanics and Symmetry: A Basic
Exposition of Classical Mechanical Systems. Springer, Berlin, 1999.
[49] R.I. McLachlan, K. Modin and O. Verdier, Phys. Rev. E 89:061301, 2014.
[50] H. Fassbender, Symplectic Methods for the Symplectic Eigenproblems. Kluwer
Academic Publishers, New York, 2002.
[51] M.A. de Gosson, Symplectic Methods in Harmonic Analysis in Mathematical
Physics. Birkhauser, Berlin, 2011.
[52] A. Agrachev and G. Revaz, Pure Appl. Math. 207:19, 1998.
[53] Z. Xu, L. Du, H. Wang and Z. Deng, Appl. Math. Mech. 40:111, 2019.
[54] E.V. Correa Silva, G.A. Monerat, G. Olivera-Neto, C. Neves and L.G. Ferreira
Filtio, Phys. Rev. D 80:047302, 2009.292 Symplectic Integrators for Hamiltonian Systems
12.13 Problems
12.1 Obtain a second-order symplectic Euler algorithm using (12.37) from t to t+h/2
and Eq. (12.36) from t + h/2 to t + h.
12.2 From Eq. (12.64) obtain the conditions on ai and bi for a third-order method.
Solve the obtained equations for b3 = 1 analytically.
12.3 Obtain a solution of Eqs. (12.70) with a1 = 0.
12.4 Find the numerical solution of the harmonic oscillator Eq. (12.7) at t = 0.1
subjected to q(0) = 0 and p(0) = 1 by the explicit Euler, the symplectic Euler,
the St¨ormer–Verlet and the Candy–Rozmus methods with h = 0.1. Compute the
error D in each method.
12.5 Find the numerical solution of the anharmonic oscillator equation ˙q = p, ˙p = q−q3
at t = 0.1 subjected to the conditions q(0) = 0.9 and p(0) = 0 by the explicit
Euler, symplectic Euler, St¨ormer–Verlet and Candy–Rozmus methods with h =
0.1. Compute the Hamiltonian at t = 0.1 (the exact value of it is −0.25) in each
method.
12.6 Find the numerical solution of the anharmonic oscillator equation ˙q = p, ˙p =
−q − q3 at t = 0.1 subjected to q(0) = 0 and p(0) = 1 by the explicit Euler,
symplectic Euler, St¨ormer–Verlet and Candy–Rozmus methods with h = 0.1.
Compute the Hamiltonian at t = 0.1 (the exact value of it is 0.5) in each method.
12.7 Find the numerical solution of the anharmonic oscillator equation ˙q = p, ˙p =
−q +q3 at t = 0.1 subjected to the initial conditions q(0) = 0.0 and p(0) = 0.5 by
the explicit Euler, symplectic Euler, St¨ormer–Verlet and Candy–Rozmus methods
with h = 0.1. Compute the Hamiltonian at t = 0.1 (the exact value of the
Hamiltonian is 0.125) in each method.
12.8 Find the numerical solution of the pendulum equation ˙q = p, ˙p = − sin q at
t = 0.1 subjected to the conditions q(0) = 2 and p(0) = 0 by the explicit Euler,
symplectic Euler, St¨ormer–Verlet and Candy–Rozmus methods with h = 0.1.
Compute the Hamiltonian at t = 0.1 (the exact value of it is 0.4161468) in each
method.
12.9 The Hamiltonian of a linear harmonic oscillator in the presence of an applied
external periodic force is
H = 1
2
p2 +
1
2
q2 − qf sin ωt.
Fix ω = 2 and f = 0.2. If the initial conditions are q(0) = 1 and p(0) = 1
compute q(0.01) and p(0.01) with time step 0.01 by the fourth-order method.
Also, develop a program to solve the above equation, where the potential has
explicit time-dependence.
12.10 The Hamiltonian of a particle in the field of a standing wave is
H = 1
2
p2 − f cos q cost .
Write its equation of motion. Calculate q(0.1) and p(0.1) if q(0) = 0.3, p(0) = 0.5,
f = 1 and h = 0.1 by the fourth-order method.13
Ordinary Differential Equations – Boundary-Value
Problems
13.1 Introduction
In Chapter 11 the problem of numerically solving ordinary differential equations with spec￾ified initial conditions is discussed. To find the numerical solution of an nth-order equation
of the form X˙ = F(X, t) by a single-step method over the interval t ∈ [a, b] the values of the
components of X at time t = a are need to be specified. However, there are instances, where
the conditions on the solution are specified at more than one value of time (independent
variable), often, at two different values of t. Usually, these time values are at the end points
a and b. Such problems are called boundary-value problems.
Let the problem is to determine the electrostatic potential V (r) between two concentric
spheres of radii r1 = 2 cm and r2 = 10 cm kept at potentials V1 = 2 V and V2 = 0 V,
respectively. The equation for V is
V  +
2
r
V  = 0 . (13.1)
The solution of this equation is subjected to the conditions V (r1) = V (2) = 2 V and
V (r2) = V (10) = 0 V. The exact solution of (13.1) is
V (r) = c1 + c2
1
r . (13.2)
Determination of the integration constants c1 and c2 by the values of V and V  at a particular
value of r is known as the initial-value problem. In the present case, c1 and c2 are to be
determined by the values of V given at r = 2 and r = 10. This is an example of a boundary￾value problem. The solution V (r) should satisfy the values of V given at the two values of
r, namely, at r = 2 and 10. Boundary-value problems have applications in boundary layer
flow, population balance, transport phenomena, vibration analysis and so on.
Consider the vibration of a string whose ends are fixed to rigid objects so that dis￾placements of the end points are zero. If the string is displaced from the rest state then its
vibration must be such that at the end points the displacement is always zero. The math￾ematical model equation for the vibration of the string is now subjected to the boundary
conditions. That is, this is another boundary-value problem.
The numerical methods for solving the boundary-value problems are of two types: dif￾ference methods and shooting methods. In a difference method derivatives are replaced by
finite-difference approximations. In this case, the solution is found by solving a set of simul￾taneous equations. In the shooting method sufficient conditions at the starting point, say, at
t = a are introduced and adjusted them suitably so that the conditions at the end points are
satisfied. First-order equations are essentially initial-value problem. For simplicity restrict
the discussion and examples on second-order linear equations only.
DOI: 10.1201/9781032649931-13 293294 Boundary-Value Problems
13.2 Shooting Method – Reduction to Two Initial-Value Problems
A general form of a second-order linear ordinary differential equation is given by
y − p(x)y − q(x)y = r(x). (13.3)
Assume that the boundary conditions are
y(a) = ya and y(b) = yb . (13.4)
Since the solutions of linear equations satisfy linear superposition principle one can show
that the solution of the boundary-value problem is a suitable linear combination of solutions
of two initial-value problems.
Suppose that u and v are two solutions of the boundary-value problem. It is easy to
show that
y(x) = c1u + c2v
c1 + c2
(13.5)
is also a solution. Substitution of this solution (13.5) in the left-hand side (LHS) of Eq. (13.3)
yields
LHS = c1u + c2v
c1 + c2
− p(x)
c1u + c2v
c1 + c2

− q(x)
c1u + c2v
c1 + c2

= 1
c1 + c2
[c1 (u − pu − qu) + c2 (v − pv − qv)]
= 1
c1 + c2
(c1r + c2r)
= r . (13.6)
Thus, y given by Eq. (13.5) is also a solution of Eq. (13.3).
From Eq. (13.5)
y(a) = c1u(a) + c2v(a)
c1 + c2
, (13.7a)
y(b) = c1u(b) + c2v(b)
c1 + c2
. (13.7b)
Choose u(a) = v(a) = y(a) and u
(a) and v
(a) as our own choice. Then, by suitably
adjusting c1 and c2 it is possible to make y(b) = yb. The suitably chosen values of c1
and c2 can be used in Eq. (13.5). Because the solutions u and v are solutions of Eq. (13.3)
corresponding to the initial conditions (u(a), u
(a)) and (v(a), v
(a)) they can be obtained by
solving Eq. (13.3) by using, say, the fourth-order Runge–Kutta method for a<x ≤ b. Then,
Eq. (13.5) can be used to compute y(x). However, it cannot be determined immediately after
knowing u(x) and v(x). This is because Eq. (13.5) contains both u(b) and v(b).
A simpler and a variant of the above method is the following: Consider the two initial￾value problems given by
u − p(x)u − q(x)u − r = 0, u(a) = ya, u
(a)=0, (13.8)
v − p(x)v − q(x)v = 0, v(a)=0, v
(a)=0. (13.9)
If u(x) and v(x) are the solutions of Eqs. (13.8) and (13.9), respectively, then the solution
of Eq. (13.3) is given by
y(x) = u(x) + cv(x). (13.10)Finite-Difference Method 295
Differentiation of Eq. (13.10) with respect to x twice gives
y = u + cv
= p(x) [u + cv
] + q(x) [u + cv] + r(x)
= p(x)y + q(x)y + r(x) (13.11)
which is Eq. (13.3). The unknown constant c in Eq. (13.10) can be determined by imposing
the conditions (13.4) to the solution (13.10). At x = a the solution y(x) is ya and the
left-side of Eq. (13.10) is u(a) + cv(a) = ya. Equation (13.10) is satisfied at x = a. On the
other hand, when x = b
y(b) = yb = u(b) + cv(b) (13.12)
which gives
c = yb − u(b)
v(b) . (13.13)
Now,
y(x) = u(x) + yb − u(b)
v(b) v(x). (13.14)
Example:
Solve the Hermite differential equation
y − 2xy + 6y +6=0 . (13.15)
subjected to the boundary conditions y(0) = −1, y(1) = −5 by the shooting method with
step size h = 0.1 and h = 0.01.
Equations (13.8) and (13.9) for the above Hermite equation are
u − 2xu + 6u +6 = 0, u(0) = −1, u
(0) = 0, (13.16)
v − 2xv + 6v = 0, v(0) = 0, v
(0) = 1. (13.17)
The above two initial-value problems are solved using the fourth-order Runge–Kutta method
with h = 0.1 and h = 0.01. Then, y is obtained from
y(x) = u(x) + y(1) − u(1)
v(1) v(x). (13.18)
The obtained solutions for h = 0.1 and h = 0.01 are given in Table 13.1 along with the
exact solution
y(x)=8x3 − 12x − 1 . (13.19)
Does it possible to solve the equation y − 2xyy = 0, y(0) = 0, y(1) = 1 by the linear
shooting method?
13.3 Finite-Difference Method
Another method of solving boundary-value problems of linear second-order differential equa￾tions is approximating the derivatives by finite-difference quotients. The differential equation296 Boundary-Value Problems
TABLE 13.1
Comparison of the numerical solution of Eq. (13.15) obtained by the shooting method with
the exact solution given by Eq. (13.19).
ynumer ynumer
x with with yexact
h = 0.1 h = 0.01
0.0 −1.0000000 −1.0000000 −1.0000000
0.1 −2.1943810 −2.1920024 −2.1920000
0.2 −3.3404645 −3.3360044 −3.3360000
0.3 −4.3901570 −4.3840061 −4.3840000
0.4 −5.2953662 −5.2880073 −5.2880000
0.5 −6.0080070 −6.0000079 −6.0000000
0.6 −6.4799962 −6.4720079 −6.4720000
0.7 −6.6632590 −6.6560071 −6.6560000
0.8 −6.5097209 −6.5040056 −6.5040000
0.9 −5.9713210 −5.9680033 −5.9680000
1.0 −5.0000000 −5.0000000 −5.0000000
of the form (13.3) can be converted into a difference equation by replacing the derivative
by finite-difference quotients. This results in a system of algebraic equations whose solution
is an approximation to the solution of the differential equation. For a nonlinear differential
equation this method gives rise to a set of nonlinear equations which are generally difficult
to solve. The set of obtained nonlinear equations can be solved by iterative methods.
Consider Eq. (13.3) over the interval [a, b] with y(a) = ya and y(b) = yb. Divide the
interval [a, b] into N equal intervals with grid points x0(= a) < x1 < x2 < ··· < xN (= b),
where xi = a + ih, i = 0, 1,...,N and h = (b − a)/N and denote yi as the solution at xi.
The central-difference approximations for the first and second derivatives are given by
y (xi) = 1
2h [y (xi+1) − y (xi−1)] + O 
h2
, (13.20a)
y (xi) = 1
h2 [y (xi+1) − 2y (xi) + y (xi−1)] + O 
h2
. (13.20b)
Substitution of y and y given by Eqs. (13.20) in Eq. (13.3) gives
1
h2 (yi+1 − 2yi + yi−1) = 1
2h (yi+1 − yi−1) pi + qiyi + ri . (13.21)
Rearrange Eq. (13.21) as

−h
2
pi − 1

yi−1 + 
2 + h2qi

yi +
h
2
pi − 1

yi+1
= −h2ri, i = 1, 2,...,N − 1 (13.22)Finite-Difference Method 297
with y0 = ya and yN = yb. When N = 4, Eq. (13.22) takes the form

−h
2
p1 − 1

y0 + 
2 + h2q1

y1 +
h
2
p1 − 1

y2 = −h2r1, (13.23a)

−h
2
p2 − 1

y1 + 
2 + h2q2

y2 +
h
2
p2 − 1

y3 = −h2r2, (13.23b)

−h
2
p3 − 1

y2 + 
2 + h2q3

y3 +
h
2
p3 − 1

y4 = −h2r3, (13.23c)
where y0 = ya, y4 = yb, x0 = a, x1 = a + h, x2 = a + 2h, x3 = a + 3h and x4 = a + 4h = b.
In matrix form Eq. (13.23) is written as


2 + h2q1
h
2 p1 − 1 0
−h
2 p2 − 1 2+ h2q2
h
2 p2 − 1
0 −h
2 p3 − 1 2+ h2q3




y1
y2
y3

 =


−h2r1 + B0
−h2r2
−h2r3 + B4

 ,
(13.24a)
where
B0 =
h
2
p1 + 1
ya, B4 =

1 − h
2
p3

yb . (13.24b)
An interesting observation is that the system (13.24) is a tridiagonal. This is true for arbi￾trary values of N. This is because the approximation of derivative of y involves only points
to the left, to the right and the central point.
For an arbitrary N Eq. (13.23) can be written in matrix form as


Q1 P1 − 1
−P2 − 1 Q2 P2 − 1
.
.
.
−PN−2 − 1 QN−2 PN−1 − 1
−PN−1 − 1 QN−1


×


y1
y2
y3
.
.
.
yN−2
yN−1


=


−h2r1 + B0
−h2r2
.
.
.
−h2rN−2
−h2rN−1 + BN


, (13.25a)
where
Qi = 2+ h2qi, (13.25b)
Pi = h
2
pi, i = 1, 2,...,N − 1 (13.25c)
and
B0 =

1 + h
2
p1

ya, BN =

1 − h
2
pN−1

yb . (13.25d)298 Boundary-Value Problems
TABLE 13.2
The numerical solution of the Hermite differential Eq. (13.15) over the interval x ∈ [0, 1] by
the finite-difference method.
Numerical solution
x h = 0.025 h = 0.05 by Richardson Exact
extrapolation solution
0.0 −1.0000000 −1.0000000 −−−−− −1.0000000
0.1 −2.1924953 −2.1939850 −2.1919987 −2.1920000
0.2 −3.3369606 −3.3398496 −3.3359976 −3.3360000
0.3 −4.3853659 −4.3894737 −4.3839999 −4.3840000
0.4 −5.2896811 −5.2947368 −5.2879958 −5.2880000
0.5 −6.0018762 −6.0075188 −5.9999953 −6.0000000
0.6 −6.4739212 −6.4796992 −6.4719952 −6.4720000
0.7 −6.6577861 −6.6631579 −6.6559955 −6.6560000
0.8 −6.5054409 −6.5097744 −6.5039964 −6.5040000
0.9 −5.9688555 −5.9714286 −5.9679978 −5.9680000
1.0 −5.0000000 −5.0000000 −5.0000000 −5.0000000
The error in the numerical approximation is O(h2). Accuracy can be improved by using the
Richardson extrapolation.
Let y(x, h) and y(x, 2h) are two numerical approximations of y(x) at a point x computed
with two different step sizes h and 2h. Denoting y(x) as the exact value of y then
y(x) = y(x, h) + ch2 , (13.26a)
y(x) = y(x, 2h) + c(2h)
2 ,
= y(x, 2h)+4ch2 . (13.26b)
Multiplying Eq. (13.26a) by 4 and subtracting the resulting equation from (13.26b) we have
y(x) = [4y(x, h) − y(x, 2h)] /3 . (13.27)
Example:
Solve the Hermite differential Eq. (13.15) subjected to the boundary conditions y(0) = −1,
y(1) = −5 over the interval x ∈ [0, 1] by the finite-difference method with the step sizes
h = 0.025 and 0.05. Applying the Richardson extrapolation for the numerical solutions
obtained with h = 0.025 and 0.05 determines the improved solution. The exact solution of
the given equation is given by Eq. (13.19).
Table 13.2 presents the numerical solutions with h = 0.025, 0.05 and by the Richardson
extrapolation.Solving Time-Independent Schr¨odinger Equation 299
13.4 Solving Time-Independent Schr¨odinger Equation
This section deals with the problem of finding the eigenvalues and the eigenfunctions of the
time-independent Schr¨odinger equation
ψ(x) + 2m
2 [E − V (x)]ψ(x)=0 , x ∈ [a, b] (13.28)
of a quantum mechanical particle of mass m and subjected to the boundary conditions
ψ(a) = 0 and ψ(b) = 0. In Eq. (13.28)  is h/(2π) and h is the Planck constant. For
simplicity set m = 1 and  = 1. Solving the above equation is the starting point of study
of quantum mechanical systems [1-3] with the potential V (x). For most of the systems,
the boundary conditions are ψ → 0 as x → ±∞. In numerical computation choose these
boundary conditions as ψ = 0 for x = a and b, where |a| and |b| are sufficiently large. ψ(x)
is zero outside the interval [a, b]. There is a simple procedure developed by van der Maelen
Uria et al [4] to solve Eq. (13.28) for arbitrary numerical or analytical potentials V (x).
Their method is illustrated in the following.
13.4.1 Discrete Equivalent of Schr¨odinger Equation
Discretize the space variable x in the interval [a, b] by choosing a grid of points x0, x1, x2,
..., xN with xi+1 − xi = h, where h is a constant. The solutions of Eq. (13.28) are called
eigenfunctions. The eigenfunctions will take values only in the grid of points. That is, ψ
will be a set of numbers ψ0, ψ1, ..., ψN , where ψi = ψ(xi). Usually ψ0 = ψ(a) = 0 and
ψN = ψ(b) = 0. Now, Eq. (13.28) becomes the following discrete equation
ψ
i +2[E − V (xi)] ψi = 0, i = 1, 2,...N − 1. (13.29)
Equation (13.29) is converted into a difference equation by the substitution
ψ |x=xi = 1
h2 [ψi+1 − 2ψi + ψi−1] . (13.30)
The result is the equation
ψi−1 + 2 
h2E − h2V (xi) − 1

ψi + ψi+1 = 0, i = 1, 2,...,N − 1. (13.31)
Thus, the Schr¨odinger Eq. (13.28) is transformed into a linear finite-difference equation.
The above N − 1 equations can be written in a matrix form as
Sψ = 0 (13.32a)
with
Sii = 2 
h2E − h2V (xi) − 1

, i = 1, 2,...,N − 1
Sii+1 = Si+1i = 1, i = 1, 2,...,N − 2
Sij = 0, j = i − 1, i, i + 1, i = 1, 2,...,N − 1. (13.32b)
Equations (13.31) can be written as
(A − λI)ψ = 0 , (13.33a)300 Boundary-Value Problems
where
aii = 2 
1 + h2V (xi)

, i = 1, 2,...,N − 1
aii+1 = ai+1i = −1, i = 1, 2,...,N − 2
aij = 0, j = i − 1, i, i + 1, i = 1, 2,...,N − 1
λ = 2h2E (13.33b)
and I is the unit matrix. The matrix A is explicitly written as
A =


a11 −100
−1 a22 −1 0
0 −1 a33 −1
.
.
. .
.
. .
.
.
−1 aN−2 −1
−1 aN−1


. (13.33c)
Thus, the original differential eigenvalue problem is transformed into an algebraic eigenvalue
problem. The values of λ for which nontrivial solutions to the system of linear Eqs. (13.33)
exist are called (energy) eigenvalues and the corresponding solutions are eigenfunctions. So,
the calculation of E and ψ of the Schr¨odinger Eq. (13.28) for a given potential is reduced
now to the calculation of λ and ψ of the matrix A.
13.4.2 Eigenvalues of the Matrix A
Now, choose a suitable technique to compute the eigenvalues and the eigenfunctions of the
matrix A. Interestingly, the matrix A is a tridiagonal symmetric matrix. For such a matrix,
efficient algorithms are available to calculate λ. A simple and efficient technique is the QL
algorithm discussed in Chapter 7.
The truncation error in the finite-difference approximation of the second derivative term,
Eq. (13.30), is proportional to h2. The error can be made proportional to h4 using the
Richardson’s extrapolation. Suppose E(1)
0 , E(1)
1 , ..., E(1)
m and E(2)
0 , E(2)
1 , ..., E(2)
m are two
sets of eigenvalues computed with two values of h, say h1 and h2. Then, a more accurate
estimation is given by
Ei(more accurate) = 1
h2
2 − h2
1

h2
2E(1)
i − h2
1E(2)
i

. (13.34)
The eigenvalues of Eq. (13.28) may be infinite in number. Since, A is (N − 1) × (N − 1)
matrix the numerical computation gives only N − 1 eigenvalues.
13.4.3 Eigenfunction of the Matrix A
The eigenfunction of the matrix A is obtained by solving the algebraic eigenvalue problem,
Eq. (13.33), where λ is a given eigenvalue, λ = 2h2E. Equation (13.33a) is written as (after
redefining N − 1 as N)


A1 −100 ... 0 00
−1 A2 −1 0 ... 0 00
0 −1 A3 −1 ... 0 00
.
.
.
0000 ... −1 AN−1 −1
0000 ... 0 −1 AN




ψ1
ψ2
ψ3
.
.
.
ψN−1
ψN


= 0. (13.35)Solving Time-Independent Schr¨odinger Equation 301
It is necessary to normalize the eigenfunction. The normalization condition is
 b
a ψ∗ψdx = 1. Let Cψ is the normalized eigenfunction, where C is the normalization
constant. This constant can be determined from the condition
C2
 b
a
ψ∗ψdx = 1 . (13.36)
Keeping this in mind, choose ψ1 = 1 and calculate other ψi’s from Eq. (13.35). The first
row gives
ψ2 = A1ψ1 . (13.37)
That is, row 1 can be used to find ψ2, row 2 can be used to find ψ3 and so on. (N − 2)th
row can be used to calculate ψN−1. Now, (N −1)th and Nth rows are not yet used whereas
only ψN to be determined. The last two rows can be added to reduce the two equations
into one equation. Using this new equation find ψN as
ψN = 1
1 − AN
[−ψN−2 + (AN−1 − 1) ψN−1] . (13.38)
Inspection of the expressions for ψ3, ψ4, ..., ψN−1 leads to the following general expression
ψi = −ψi−2 + Ai−1ψi−1 , i = 3, 4,...,N − 1. (13.39)
Here, ψ1 = 1, ψ2 = A1ψ1 and ψN is given by Eq. (13.38). After computing an unnormalized
eigenfunction, the normalization constant C is determined from Eq. (13.36). The integral
in Eq. (13.36) can be evaluated, for example, using the composite trapezoidal rule. That is,
1
C2 =
 b
a
|ψ|
2 dx
= h

1
2
ψ2
1 + ψ2
2 + ψ2
3 + ··· + ψ2
N−1 +
1
2
ψ2
N

. (13.40)
Example:
For the linear harmonic oscillator potential V (x) = x2/2 the exact eigenvalues are [1, 2]
Eexact =

n +
1
2

, n = 0, 1, 2,... . (13.41)
Compute numerically the energy eigenvalues and the eigenfunctions.
A Python program implementing the QL algorithm is developed to compute the energy
eigenvalues for a given value of h. The program is instructed to store only the eigenval￾ues less than, for example, 10 in an external output file. In order to get a more accurate
result, eigenvalues are calculated for two values of h, namely, h1 and h2. Then, using the
Richardson extrapolation more accurate values of E are obtained. Table 13.3 presents first
few eigenvalues calculated with h1 = 0.1 and h2 = 0.05, by extrapolation and the exact
values. Here, a = −10 and b = 10. Accuracy can be improved by reducing the values of h1
and h2 and choosing large values of |a| and |b|.
The exact eigenfunctions ψn(x) are given by
ψn(x) = NnHn(x) e−x2/2 , (13.42)
where
Nn =
 1
√π 2nn!
1/2
, Hn(x)=(−1)n ex2 ∂n
∂xn e−x2
. (13.43)302 Boundary-Value Problems
TABLE 13.3
The numerically computed first few eigenvalues of the one-dimensional linear harmonic
oscillator with the potential V (x) = x2/2. Here, a = −10 and b = 10.
Computed E with Extra- Exact
h = 0.1 h = 0.05 polated E E
9.44309 9.48584 9.50009 9.5
8.45444 8.48868 8.50007 8.5
7.46452 7.49116 7.50004 7.5
6.47333 6.49335 6.50002 6.5
5.48087 5.49523 5.50002 5.5
4.48715 4.49679 4.50000 4.5
3.49217 3.49805 3.50001 3.5
2.49593 2.49898 2.50000 2.5
1.49844 1.49961 1.50000 1.5
0.49969 0.49992 0.50000 0.5
It is easy to develop a Python program to compute the normalized eigenfunction for a
given value of energy E. For the lowest energy E = 0.5 the exact eigenfunction is given by
ψ(x) =  1
π
1/4
e−x2/2 . (13.44)
Table 13.4 gives the numerically computed lowest energy (ground state) eigenfunction values
at some selected grid points for three values of h, where a = −5 and b = 5. The numerically
computed values of ψ(x) approach the exact result as h decreases. Very good agreement
with exact eigenfunction is observed for other values of E also. The numerical solution is
symmetric with respect to the origin. This solution is called an even-parity solution. An odd￾parity solution can be obtained by choosing ψ(a) = −1. Figure 13.1 shows the numerically
computed eigenfunctions for E = 0.5, 1.5, 2.5 and 3.5. For E = 0.5 and 2.5 the value of
ψ(a) is set as 1 whereas for E = 1.5 and 3.5 the value of ψ(a) is set as −1.
The problem of solving the time-dependent Schr¨odinger equation is discussed in Sec￾tion 14.9.
13.5 Concluding Remarks
In the present chapter, only two methods for the boundary-value problems of ordinary
differential equations are described. There are many methods that exist, for example, mul￾tiple shooting methods [5] and wavelet approaches are developed [6]. Numerical methods
for ordinary differential equations with singular coefficients [7], deviated arguments [8] and
on an infinite domain with solutions having a slowly decaying tail [8] have been reported.
Higher-order boundary-value problems occur in astrophysics, astronomy, fluid dynamics,
viscoelastic flows, coating flows, etc. For the numerical methods for higher-order equations,
one may refer to the refs. [9-15].Concluding Remarks 303
TABLE 13.4
The numerically computed lowest energy eigenfunction values at some grid points for three
values of h. The last column is the exact result.
Numerical ψ with
x h = 0.02 h = 0.01 h = 0.001 Exact ψ
±5.0 0.00000 0.00000 0.00000 0.00000
±4.0 0.00024 0.00025 0.00025 0.00025
±3.0 0.00803 0.00832 0.00834 0.00833
±2.5 0.03175 0.03292 0.03300 0.03300
±2.0 0.09779 0.10138 0.10165 0.10165
±1.5 0.23459 0.24321 0.24385 0.24385
±1.0 0.43828 0.45438 0.45558 0.45558
±0.5 0.63770 0.66112 0.66287 0.66287
0.0 0.72261 0.74915 0.75113 0.75113
E = 0.5 (a)
x
ψ
-5 0 5
1
0
-1
E = 1.5 (b)
x
ψ
-5 0 5
1
0
-1
E = 2.5 (c)
x
ψ
-5 0 5
1
0
-1
E = 3.5 (d)
x
ψ
-5 0 5
1
0
-1
FIGURE 13.1
The numerically computed eigenfunctions of the linear harmonic oscillator (continuous
curve) for first few values of E. The exact values of the eigenfunctions for selected val￾ues of x are represented by solid circles.304 Boundary-Value Problems
13.6 Bibliography
[1] L.I. Schiff, Quantum Mechanics. McGraw Hill, Singapore, 1968.
[2] P.M. Mathews and K. Venkatesan, A Text Book of Quantum Mechanics. Tata
McGraw Hill, New Delhi, 2006.
[3] S. Rajasekar and R. Velusamy, Quantum Mechanics I: The Fundamentals. CRC
Press, Boca Raton, 2022.
[4] J.F. van der Maelen Uria, S. Garcia-Granda and A. Menendez-Velazzquez, Am.
J. Phys. 64:327, 1996.
[5] U.M. Aischer and L.R. Petzold, Computational Methods for Ordinary Differential
Equations and Differential Algebraic Equations, SIAM, Philadelphia, 1998.
[6] S. Ul Arifeen, S. Haq, A. Ghafoor, A. Ullah, P. Kumam and P. Chatpanya, Adv.
Diff. Eqs. 2021:347, 2021.
[7] R.D. Russell and L.F. Shampine, SIAM J. Numer. Anal. 12:13, 1975.
[8] T. Jankowski, Numerical solution of boundary-value problems with deviated ar￾guments. In Numerical Methods for Advanced Applications. F. Brezzi, A. Buffa,
S. Corasaro and A. Murli (Eds.). Springer, Milano, 2003.
[9] M.M. Chawla and C.P. Katti, BIT Numer. Math. 19:27, 1979.
[10] A.M. Wazwaz, Appl. Math. Comput. 118:311, 2001.
[11] I. Ullah, H. Khan and M.T. Rahim, J. Comput. Eng., 2014, article ID 286039.
[12] M.J. Iqbal, S. Rehman, A. Pervaiz and A. Hakeem, Proc. Pak. Acad. Sci. 182:389,
2015.
[13] J. Ahmed, Kyungpook Math. J. 57:651, 2017.
[14] S. Owyed, M.A. Abdou, A. Abdel-Aty, A.A. Ibraheem, R. Nekhile and
D. Baleanu, J. Intell. Fuzzy Syst. 38:2859, 2020.
[15] R. Amin, K. Shah, I. Khan, M. Arif, K.M. Abualnaja, E.E. Mahmoud and
A.H. Abdel-Aty, Open Phys. 18:1048, 2020.
13.7 Problems
13.1 Solve the Legendre differential equation
y − 2x
1 − x2 y +
2
1 − x2 y +
4
1 − x2 = 0
subjected to the boundary conditions y(0) = −2, y(1) = −1 over the interval
x ∈ [0, 1] by the step sizes h = 0.025 and 0.05. Applying the Richardson extrapo￾lation for the numerical solution obtained with h = 0.025 and 0.05 determine the
improved solution. The exact solution of the given equation is y = x − 2.
13.2 Solve the differential equation
y + 4y + 4y − 4e−2x = 0Problems 305
subjected to the boundary conditions y(0) = −1, y(2) = 11e−2 over the interval
x ∈ [0, 2] by the step sizes h = 0.025 and 0.05. Applying the Richardson extrap￾olation for the numerical solution obtained with h = 0.025 and 0.05 determines
the improved solution. The exact solution of the given equation is
y = e−2x 
2x2 + 2x − 1

.
13.3 Consider the differential equation

x2 − 1
2
y + 2(x − 1)(x − 2)y + 4y +4=0
subjected to the boundary conditions y(0) = −1, y(1) = 1. Find the solution
in the interval x ∈ [0, 1]. The exact solution of the given equation is y = (3x −
1)/(x + 1).
13.4 Consider a metal bar of length 1 m and of uniform cross-section. At a certain
time t, the temperature y of the bar at the ends is found to be y(0) = 50◦C and
y(1m) = 50◦C. The temperature variation is governed by the equation
y − y = 0 .
Find y(x) in the interval 0 <x< 1m. The exact solution is y(x) = Aex + Be−x,
where A = 4.9378898 and B = 45.06211.
13.5 A violin string is stretched a little and then its ends are fixed at x = 0, π units.
It is allowed to vibrate. Assume that the deflection of the string is given by
u(x, t) = y(x)W(t). If y is the solution of the equation
y + 4y = 0, y(0) = 0, y(π)=0
find y(x) in the interval 0 <x<π. The exact solution is y(x) = sin 2x.
13.6 The variation of the electrostatic potential V (r) between two concentric spheres
of radii r1 = 2 cm and r2 = 10 cm kept at the potentials V1 = 2 volts and V2 = 0
volt, respectively, is governed by the equation
V  + 2V 
/r = 0 .
The boundary conditions are V (r1) = V (2) = 2 volts and V (r2) = V (10) = 0
volt. Find V (r) in the interval 2 cm <r< 10 cm. The exact solution is V (r) =
−0.5+5/r volts.
13.7 The temperature distribution y(x, t) of a solid remains the same for all values
of t. That is, y(x, t) is in equilibrium with respect to time. The variation of
temperature with respect to space variable is governed by the equation
y + 2y + 2y = 0 .
If the temperature at x = 0 and 5π/2 units are 30◦C and 1◦C calculate the
temperature distribution y(x) for 0 <x< 5π/2 units. The exact solution is
y(x) = Ae−x sin(x + B), where A = 2576.1605 and B = 0.0116455.
13.8 The wave function of a quantum mechanical particle in the potential V (x) = x2p,
for large values of p is the solution of the Schr¨odinger equation ψxx + λ2ψ = 0,
λ = π/2. ψ is subjected to the boundary conditions ψ(0) = 1, ψ(1) = 0. Find
ψ(x) for 0 <x< 1. Exact solution is ψ(x) = cos(πx/2).306 Boundary-Value Problems
13.9 For the shifted harmonic oscillator potential V (x) = 1
2x2 + m, m = 0, 1, 2,... the
exact energy eigenvalues are E(m) n = (m + n + 1
2 ).
(a) Compute numerically the energy eigenvalues for several values of m and com￾pare them with the exact values.
(b) When n = 0, E(m)
0 = (m + 1
2 ), m = 0, 1,... . Verify that the eigenfunctions
ψm(x), m = 0, 1,... all are identical even though the eigenvalues are distinct.
13.10 The eigenvalues and the ground state eigenfunction of some interesting poten￾tials are given below. Obtain the first few lowest energy eigenvalues and the
corresponding eigenfunctions.
(a) Box Potential:
V (x) =  0 for |x| < a
∞ for |x| > a
En = (n + 1)2π2
8a2 , ψ0 =
2
a sin(πx/a).
(b) Coulomb Potential:
V (r) = −b
r
, En = − b2
2(n + 1)2 , n = 0, 1,... , ψ0 ∼ re−br.
(c) Perturbed Coulomb Potential:
V (r) = −b
r
+
1
r2 ,
En = − b2
2(n + 2)2 , n = 0, 1,..., ψ0 ∼ r2e−br/2.14
Linear Partial Differential Equations
14.1 Introduction
When the number of independent variables of a differential equation is more than one
then it is said to be a partial differential equation. In a partial differential equation all
the independent variables may be space variables or one of them may be a time variable.
When a physical property of a system or a process or an event depends on more than one
independent variable then it is generally described by a partial differential equation. Partial
differential equations occur in many branches of engineering, physics, chemistry, biology
and so on. Some of the ubiquitous partial differential equations are the heat equation, the
wave equation, the Laplace equation, the Poisson equation and the Maxwell equations. For
certain linear partial differential equations exact analytical solutions can be obtained by the
well-known variable separable method.
Simple and efficient numerical methods for partial differential equations can be ob￾tained by the finite-difference methods. In 1908, Runge studied the numerical solution of
the Poisson equation utt +uxx = constant (where utt = ∂2u/∂t2 and uxx = ∂2u/∂x2) using
finite-difference methods. Bindes and Schmidt proposed a finite-difference formula for the
heat equation. The work of Courant and Friedrichs and L´evy in 1928 was considered as the
beginning of numerical methods for partial differential equations. In the case of ordinary
differential equations, a particular method, for example, Runge–Kutta method, can be ap￾plied to a variety of equations. In contrast, one has to develop a suitable formula separately
for each of the partial differential equations.
Partial differential equations are generally classified according to their linearity, order
and boundary conditions. In this chapter, restrict ourselves to linear equations and take
up the problem of nonlinear equations in the next chapter. The present chapter first gives
a brief account of classification of partial differential equations into linear, quasilinear and
nonlinear. Many partial differential equations of practical applications are of second-order
in form. They are classified into hyperbolic, parabolic and elliptic. Next, certain useful
boundary conditions are defined. Some of the partial derivatives are expressed in terms
of finite-differences. Then, finite-difference formulas are developed for certain partial dif￾ferential equations. Particular emphasize is on the numerical methods to solve hyperbolic,
parabolic and elliptic equations.
14.2 Classification of Partial Differential Equations
The order of a partial differential equation is simply determined by the highest-order partial
derivative occurring in the given equation. Let us consider a physical variable u a function of
two variables x and t. The partial derivatives ∂u/∂t and ∂u/∂x are called first-order partial
DOI: 10.1201/9781032649931-14 307308 Linear Partial Differential Equations
derivatives. The derivatives ∂2u/∂t2, ∂2u/∂x2, ∂2u/(∂x∂t) and ∂2u/(∂t∂x) are second-order
partial derivatives. In (∂u/∂x)n or (∂2u/∂t2)n n is the degree or power of the respective
derivatives.
If each of the terms of a given partial differential equation, after rationalization, has a
total degree either 0 or 1 in the dependent variables and their partial derivatives then it is
said to be a linear partial differential equation. Otherwise, it can be either quasilinear or
nonlinear. For example, consider the following second-order partial differential equation
a
∂2u
∂t2 + b ∂2u
∂t∂x + c
∂2u
∂x2 + d = 0 . (14.1)
If all the coefficients a, b, c and d are constants then Eq. (14.1) is a linear second-order
partial differential equation with constant coefficients. If at least one of the coefficients is
a function of the independent variables x and/or t then it is a variable coefficient linear
equation. When any one of the coefficients is a function of the dependent variable u and/or
any of its derivatives of lower order than that of the differential equation then the equation
is said to be a quasilinear (often called nonlinear ). For example, if b has a term u or ∂u/∂x
or ∂u/∂t then Eq. (14.1) is a quasilinear. If any of the coefficients a, b, c and d is a function
of the dependent variable u and/or any of its derivatives of the same order as that of the
equation then the equation is nonlinear. For example, if c has a term ∂2u/∂t2, ∂2u/∂x2 or
∂2u/∂t∂x then it is a nonlinear. Examples of partial differential equations with different
order and linearity are given below:
1. First-order linear equation:
∂u
∂t = α
∂u
∂x . (14.2)
2. First-order quasilinear equation:
∂u
∂t + u
∂u
∂x = 0 . (14.3)
3. First-order nonlinear equation:
∂u
∂t +
∂u
∂t
∂u
∂x = 0 . (14.4)
4. Second-order linear equation:
∂2u
∂x2 +
∂2u
∂y2 + u = 0 . (14.5)
5. Second-order quasilinear equation:
∂2u
∂t2 +
∂u
∂x + u2 = 0 . (14.6)
6. Second-order nonlinear equation:
∂2u
∂t2 +
∂2u
∂t∂x
∂2u
∂x2 = 0 . (14.7)Initial and Boundary Conditions 309
Linear second-order partial differential equations with two independent variables are
found to describe many real physical systems and are classified into hyperbolic, parabolic
and elliptic. Consider the following second-order linear partial differential equation
a
∂2u
∂t2 + 2b ∂2u
∂t∂x + c
∂2u
∂x2 + d
∂u
∂t + e
∂u
∂x + fu + g = 0 , (14.8)
where u = u(x, t) and the coefficients are either constants or functions of the independent
variables only. Equation (14.8) is classified into the following types:
Hyperbolic : b2 − ac > 0 . (14.9a)
Parabolic : b2 − ac = 0 . (14.9b)
Elliptic : b2 − ac < 0 . (14.9c)
Examples:
1. Wave equation (hyperbolic):
∂2u
∂t2 = c2 ∂2u
∂x2 . (14.10)
2. Heat equation (parabolic):
∂u
∂t = c2 ∂2u
∂x2 . (14.11)
3. Laplace equation (elliptic):
∂2u
∂x2 +
∂2u
∂y2 = 0 . (14.12)
Partial differential equations of physical systems are further classified into 1) steady￾state problem, 2) propagation problem and 3) eigenvalue problem [1]. In a closed domain,
the solution, say, u(x, y) of a steady state problem is described by an elliptic type differential
equation along with appropriate boundary conditions. Propagation problems are essentially
initial-value problems, that is, the time evolutions of the systems are subjected to the
initial conditions in an open interval. The eigenvalue problems correspond to the cases
where bounded meaningful solutions exist only for certain specific values of the parameters
called, eigenvalues.
14.3 Initial and Boundary Conditions
Partial differential equations are solved with the given initial and boundary conditions.
Space variables generally extends from −∞ to ∞. However, real systems are finite in ex￾tension or periodic in the independent variables. Therefore, the solution must be subjected
to appropriate boundary conditions. A given equation with different boundary conditions
give rise to qualitatively different solutions.
The following are the initial/boundary conditions often employed in solving the second￾order partial differential equations.310 Linear Partial Differential Equations
1. Dirichlet Conditions
Consider a second-order partial differential equation with dependent variable u(x, t) and
the independent variables t and x with a1 ≤ x ≤ a2 and b1 ≤ t ≤ b2. In the Dirichlet
conditions, the values of u are given at x = a1 and a2 and t = b1 and b2.
Example:
Initial Condition:
An initial condition specifies the value of u and ut at the initial value of time t, say, at b1
for the range of x considered:
u (x, t = b1) = f1(x), a1 ≤ x ≤ a2 (14.13a)
ut (x, t = b1) = f2(x), a1 ≤ x ≤ a2. (14.13b)
Boundary Condition:
A boundary condition usually specifies the solution u(x, t) at the left- and the right￾boundaries of the space variable x, namely at x = a1 and a2, respectively. An example
is
u (x = a1, t) = g1(t), b1 ≤ t ≤ b2 (14.14a)
u (x = a2, t) = g2(t), b1 ≤ t ≤ b2. (14.14b)
2. Neumann Condition
In certain problems, it is required to fix the value of, say, ux at x = a1 or a2 for all t. An
example is
ux (x = a2, t)=0 , b1 ≤ t ≤ b2. (14.15)
This Neumann boundary condition specifies that the partial derivatives ux at right￾boundary is zero. In the heat conduction problem, this boundary condition is realized if
the right-boundary is attached to a perfect insulator.
3. Cauchy Condition
Cauchy condition is a combination of both the Dirichlet condition and the Neumann con￾dition.
14.4 Finite-Difference Approximations of Partial Derivatives
In Chapter 8, ordinary derivatives are approximated in terms of finite-differences and
demonstrated the evaluation of first and second derivatives. The method of finite-differences
can be applied for expressing partial derivatives. In a given partial differential equation
each partial derivative can be replaced by the corresponding finite-difference expression.
Rearranging the terms in the resulting expression gives an approximate formula for the
computation of numerical solution of the equation. Therefore, this section expresses a few
partial derivatives in terms of finite-differences.Finite-Difference Approximations of Partial Derivatives 311
k
h
➤
➤
y
x
j
i
FIGURE 14.1
A two-dimensional finite-difference 6 × 6 grid.
For an ordinary derivative, the number of independent variables is one and hence the
one-dimensional axis of that variable is divided into number of grid points. The derivatives
are then approximated by the value of the dependent variable on either side of the grid
points. Since a partial differential equation involves more than one independent variable
it is desirable to consider grid points on a two-dimensional plane or a three-dimensional
volume for systems with two or three independent variables, respectively. The equations
that are going to consider in this chapter and in the next chapter have only two indepen￾dent variables. Therefore, obtain the finite-differences of partial derivatives of a dependent
variable with two independent variables.
Let the variable u(x, y) is a function of two variables x and y. The two-dimensional x−y
plane is divided into number of rectangles of sides say ∆x = h and ∆y = k by drawing a
set of equally spaced lines parallel to the x and y axes as shown in Fig. 14.1. The points
of intersection of these lines are the grid points. They are also called lattice points or mesh
points. Use the notation (i, j) for the grid points (xi, yi) where i and j are the counters in
the x and y directions, respectively.
Express first, second and mixed partial derivatives in terms of finite-differences at a grid
point (xi, yi), that is at (i, j). Because the partial derivatives of u with respect to x implies
that y is kept constant one can write
∂u
∂x



i,j
= du
dx



i,j
. (14.16)
Then, use of Eq. (8.12), which is the two-point central-difference formula of the first deriva￾tive, gives
∂u
∂x



i,j
= ∂
∂xu (xi, yj )
= 1
2h [u (xi + h, yj ) − u (xi − h, yj )] + O 
h2
= 1
2h [u (xi+1, yj ) − u (xi−1, yj )] + O 
h2
. (14.17)312 Linear Partial Differential Equations
Writting u(xi, yj ) as ui,j the above equation can be rewritten as
∂
∂xui,j = 1
2h (ui+1,j − ui−1,j ) + O 
h2
. (14.18a)
For the partial derivative of u with respect to y the variable x is held constant and at (xi, yj )
∂
∂y ui,j = 1
2k (ui,j+1 − ui,j−1) + O 
k2
. (14.18b)
The truncation errors in the first-order partial derivatives ∂u/∂x and ∂u/∂y approximations
in Eqs. (14.18) are of the order of h2 and k2, respectively.
In a similar manner, using Eq. (8.18) the three-point central-difference formulas of
second-order partial derivatives are obtained as
∂2
∂x2 u (xi, yj ) = 1
h2 (ui+1,j − 2ui,j + ui−1,j ) + O 
h2
, (14.19a)
∂2
∂y2 u (xi, yj ) = 1
k2 (ui,j+1 − 2ui,j + ui,j−1) + O 
k2
. (14.19b)
For the mixed derivative
∂2u
∂x∂y = ∂2u
∂y∂x (14.20)
the finite-difference approximation is
∂2
∂x∂y u (xi, yj ) = 1
4hk (ui+1,j+1 − ui−1,j+1 − ui+1,j−1 + ui−1,j−1)
+O 
h2 + k2
. (14.21)
In the above finite-difference approximations of derivatives the truncation error is second￾order in the grid sizes. In Chapter 8, the finite-difference approximations with higher-order
accuracies have also been developed. However, such more accurate formulas are not com￾monly employed to solve partial differential equations because they involve a larger number
of terms in the resulting formula and require more extensive computation times.
Here onwards for simplicity denote ∂u/∂t as ut, ∂2u/∂t2 as utt, ∂u/∂x as ux, ∂2u/∂x2
as uxx, ∂2u/∂t∂x as utx and so on.
14.5 Hyperbolic Equations
Hyperbolic equations occur in the fields of light waves, acoustic waves, gravitational waves
and vibrations of certain musical instruments. Specifically, torsional oscillations of a rod,
vibrations of a stretched string, sound waves in a pipe, water waves in a narrow canal,
predator-prey mute system on oranges, oil pipe lines system, gravitational instability of
nebulae, aerodynamic and atmospheric flows in normal conditions, flows of contaminants
through a porus medium are governed by or well approximated by (linear) hyperbolic partial
differential equations.
In this section, explicit and implicit finite-difference formulas are set up for the ubiqui￾tous wave equation, their stabilities are analyzed and the numerical solutions are compared
with the exact solutions.Hyperbolic Equations 313
t2
tm
tm−1
tj
t1
t0
x0 x1 x2 xi xn−1 xn
FIGURE 14.2
The (n + 1) × (m + 1) grid points for solving the wave equation. The values of u at the grid
points marked by open circles are given by the conditions (14.23). The values of u at the
grid points marked by solid circles are to be determined.
14.5.1 The Wave Equation
The wave equation is
utt = c2uxx , 0 ≤ x ≤ a, 0 ≤ t ≤ b. (14.22)
This partial differential equation models the displacement u(x, t) of a stretched elastic string
with the ends fixed at x = 0 and a. Let Eq. (14.22) is subjected to the following initial and
boundary conditions:
u(x, 0) = f(x), 0 ≤ x ≤ a (14.23a)
ut(x, 0) = g(x), 0 ≤ x ≤ a (14.23b)
u(0, t) = v1(t), 0 ≤ t ≤ b (14.23c)
u(a, t) = v2(t), 0 ≤ t ≤ b. (14.23d)
Divide the region R = {(x, t): 0 ≤ x ≤ a, 0 ≤ t ≤ b} into (n + 1) × (m + 1) rectangles
with sides ∆x = h and ∆t = k as shown in Fig. 14.2. The values of xi and ti at the grid
points are given by
xi = ih , i = 0, 1,...,n; tj = jk , j = 0, 1, . . . , m. (14.24)
The values of u at the bottom-row and the left-edge and right-edge are given by the boundary
conditions. The solution at the grid points (i, j = 0) in the bottom-row namely u(i, 0),
i = 0, 1,...,n is given by the initial condition (14.23a). The values of the partial derivative
ut at these grid points are given by the initial condition (14.23b). The values of u at the grid
points on the left-edge, that is the values of u(0, j), j = 0, 1,...,m are given by Eq. (14.23c).
The solution at the right-side grid points is given by Eq. (14.23d). The solution at the grid
points marked by solid circles has to be determined while that at the points marked by
open circles is known from the given boundary conditions.314 Linear Partial Differential Equations
ui,j+1 tj+1
tj
tj−1
xi−1 xi xi+1
FIGURE 14.3
The known grids (marked by open circles) are used to compute the unknown solution at
the grid (i, j + 1) (marked by a solid circle) for the wave equation.
14.5.2 Derivation of Finite-Difference Formula
The central-difference approximations for utt and uxx from Eq. (14.19) are
utt (xi, tj ) = 1
k2 (ui,j+1 − 2ui,j + ui,j−1) + O 
k2
, (14.25a)
uxx (xi, tj ) = 1
h2 (ui+1,j − 2ui,j + ui−1,j ) + O 
h2
. (14.25b)
Substitution of (14.25) in (14.22) after dropping O(k2) and O(h2) gives
1
k2 (ui,j+1 − 2ui,j + ui,j−1) = c2
h2 (ui+1,j − 2ui,j + ui−1,j ) . (14.26)
Define s = ck/h and rewrite Eq. (14.26) as
ui,j+1 − 2ui,j + ui,j−1 = s2 (ui+1,j − 2ui,j + ui−1,j ) . (14.27)
In the above equation assume that the value of u in the rows j and j − 1 are known. This
results in
ui,j+1 = 
2 − 2s2
ui,j + s2 (ui+1,j + ui−1,j ) − ui,j−1 ,
i = 1, 2,...,n − 1, j = 1, 2, . . . , m. (14.28)
The values of u at the grid points (i, j), (i+1, j), (i−1, j) and (i, j−1) are used to determine
the value of u at the grid point (i, j + 1). This is depicted in Fig.14.3.
14.5.3 Calculation of u at t = k
In the formula (14.28) to calculate the value of ui,j+1 the values of u at the grid points (i, j),
(i + 1, j), (i − 1, j) and (i, j − 1) must be known. Set the (j − 1)th row as the bottom-rowHyperbolic Equations 315
(zeroth row) then the calculation of u begins from the grid points in the (j + 1)th row, that
is from the second row. The values of u at the grid points in the zeroth row are given by the
initial condition (14.23a). In the second row, the values of u at the grid points (0, 1) and
(n, 1) alone are known from the boundary conditions (14.23c) and (14.23d), respectively.
The u values at other grid points in the first row are also required to compute u at the
grid points in the second row. Therefore, before using the formula (14.28) it is necessary
to determine the solution corresponding to the first row in Fig. 14.2. This can be done as
follows.
To know u(xi, t1) consider the Taylor series of u(xi, t1) about u(xi, t0):
u (xi, t1) = u (xi, t0) + kut (xi, t0) + O 
k2
. (14.29)
In other words
ui,1 = ui,0 + k(ut)i,0 + O 
k2
. (14.30)
(ut)i,0 is known from the condition (14.23b). Neglecting the truncation error term and
substituting (ut)i,0 = gi = g(xi) and ui,0 = fi in Eq. (14.30) yield
ui,1 = fi + kgi . (14.31)
This numerically computed ui,1 is not identical to the exact ui,1 but is only an approxi￾mation. The point is that the error in (14.31) will propagate throughout the grid points
and will not be dampened out. Therefore, to reduce the error in (14.31) include the next
higher-order term in (14.29).
Equation (14.29) with the inclusion of one more term in the series is
ui,1 = ui,0 + k(ut)i,0 +
1
2
k2 (utt)i,0 + O 
k2
. (14.32)
For finding (utt)i,0 consider the wave equation at the zeroth row:
(utt)i,0 = c2 (uxx)i,0
= c2 (fxx)i
= c2
h2 (fi+1 − 2fi + fi−1) + O 
h2
. (14.33)
Use (14.33) in (14.32) and obtain
ui,1 = fi + kgi +
c2k2
2h2 (fi+1 − 2fi + fi−1) + O 
h2
O 
k2
+ O 
k2
= 
1 − s2
fi + kgi +
1
2
s2 (fi+1 + fi−1), i = 1, 2,...,n − 1 (14.34)
where in the last step the error term is neglected.
The truncation error of the formula (14.28) is O(h2 + k2). It is the error in the finite￾difference equation and not of the solution. The independent variables x and t actually
assume continuous values. In the finite-difference methods, they take only discrete values.
That is, they are discretized. The discretization of a continuous problem gives rise to an
error in the solution and is termed as discretization error . An additional error called round￾off error will occur if the finite-difference equation is not solved exactly. Further, the mesh
sizes h and k affect the discretization and round-off errors. Decrease in h or k decreases
the discretization error but increases the round-off error since more number of iterations
are used. This means that one cannot conclude that accuracy can always be improved by
decreasing h and k.316 Linear Partial Differential Equations
14.5.4 Stability Condition
The stability of the method, that is the condition for decaying of an error made at one
stage of computation, can be investigated using the so-called Fourier method (also known
as von Neumann analysis) [2]. To obtain the stability condition let us assume a solution of
the given equation as
u(x, t)=eiαxeβt . (14.35)
Replacing x by ih and t by jk in the solution gives
ui,j = eiαiheβjk . (14.36)
Define ξ = eβk. Then,
ui,j = eiαihξj . (14.37)
The finite-difference formula will be stable if |ui,j | remain bounded as h, k → 0 for jk ≤ b,
0 ≤ t ≤ b, where b is finite. Suppose, the exact solution is bounded. Then, the necessary
and sufficient condition for stability is |ξ| ≤ 1, −1 ≤ ξ ≤ 1.
For the wave equation the substitution of
ui,j = eiαihξj (14.38)
in (14.28) gives
ξ = 
2 − 2s2
+ s2 
eiαh + e−iαh
− ξ−1 . (14.39)
Substitute
eiαh + e−iαh = 2 cos αh = 2 − 4 sin2(αh/2) (14.40)
and obtain
ξ2 − 2ξ

1 − 2s2 sin2(αh/2)
+1=0 . (14.41)
The above equation can be rewritten as
ξ2 − 2δξ +1=0 , δ = 1 − 2s2 sin2(αh/2). (14.42)
The two roots of the above equation are
ξ+ = δ + δ2 − 1 , ξ− = δ − δ2 − 1 . (14.43)
|ξ+| and |ξ−| are ≤ 1 only if |δ| ≤ 1. That is,
−1 ≤ 1 − 2s2 sin2(αh/2) ≤ 1 . (14.44)
The right-side inequality gives −s2 sin2(αh/2) ≤ 0. This is satisfied for all values of s (since
s is real). The left-side inequality gives s2 sin2(αh/2) ≤ 1. Since sin2(αh/2) is ≤ 1 the
requirement is
s2 = c2k2
h2 ≤ 1 . (14.45)
Thus, the stability condition is
s = ck
h ≤ 1 . (14.46)
Example:
A string of unit length is subjected to the initial displacement u(x, 0) = sin πx and zero
initial velocity. The time evolution of u is governed by the wave equation
utt(x, t) = uxx(x, t), 0 ≤ x ≤ 1, 0 ≤ t ≤ 0.5 (14.47)Hyperbolic Equations 317
TABLE 14.1
The numerical solution of the wave Eq. (14.48) is obtained from the difference Eq. (14.28)
and the exact solution. For each value of t, the first and second rows give the numerical and
exact solutions, respectively.
t x0 = 0.0 x4 = 0.2 x8 = 0.4 x12 = 0.6 x16 = 0.8 x20 = 1.0
0.00 − − −− − − −− − − −− − − −− − − −− − − −−
0.000000 0.587785 0.951057 0.951057 0.587785 0.000000
0.10 − − −− 0.559017 0.904508 0.904508 0.559017 − − −−
0.000000 0.559017 0.904508 0.904508 0.559017 0.000000
0.20 − − −− 0.475528 0.769421 0.769421 0.475528 − − −−
0.000000 0.475528 0.769421 0.769421 0.475528 0.000000
0.30 − − −− 0.345492 0.559017 0.559017 0.345492 − − −−
0.000000 0.345492 0.559017 0.559017 0.345492 0.000000
0.40 − − −− 0.181636 0.293893 0.293893 0.181636 − − −−
0.000000 0.181636 0.293893 0.293893 0.181636 0.000000
0.50 − − −− 0.005077 0.008215 0.008215 0.005077 − − −−
0.000000 0.000000 0.000000 0.000000 0.000000 0.000000
subjected to the initial and boundary conditions
u(x, 0) = sin πx, 0 ≤ x ≤ 1 (14.48a)
ut(x, 0) = g(x)=0, 0 ≤ x ≤ 1 (14.48b)
u(0, t) = v1 = 0, 0 ≤ t ≤ 0.5 (14.48c)
u(1, t) = v2 = 0, 0 ≤ t ≤ 0.5. (14.48d)
Determine the solution u(x, t).
In Eq. (14.47) c2 = 1, that is c = 1. Choose h = 0.05, k = 0.05 which give s = ck/h = 1.
The exact solution for the given conditions is
ue(x, t) = sin πx cos πt. (14.49)
The numerical and the exact solutions for a few values of x for t = 0, 0.05, 0.1, ... , 0.5 are
given in Table 14.1. The difference between ue and numerically computed u for x = 0.5 is
plotted in Fig. 14.4.
14.5.5 An Implicit Method
The finite-difference formula (14.28) for Eq. (14.22) is an explicit method since the unknown
u at a grid point is determined using the known u at some other grid points. Though the
method is simple, a disadvantage of this method is that it is stable only if 0 < s ≤ 1. This
means, ck/h ≤ 1, that is, k ≤ h/c. The time step k must be sufficiently small. This difficulty
is avoided in implicit methods. An implicit finite-difference formula is one in which more
than one unknown values in the (j + 1)th row are specified in terms of the known values in
the rows j, j − 1,... . In an implicit method the second-order spatial partial derivative at318 Linear Partial Differential Equations
t
Error
0 2 4 6 8 10
3e-06
0
-3e-06
FIGURE 14.4
Error in the numerical solution of Eq. (14.47) for x = 0.5 as a function of time.
the grid point (i, j) is taken as the average of the spatial derivative at (i, j + 1) and (i, j −1):
uxx (xi, tj ) = 1
2 [uxx (xi, tj+1) + uxx (xi, tj−1)] . (14.50)
Write
uxx (xi, tj+1) = 1
h2 (ui+1,j+1 − 2ui,j+1 + ui−1,j+1) , (14.51a)
uxx (xi, tj+1) = 1
h2 (ui+1,j+1 − 2ui,j+1 + ui−1,j+1) , (14.51b)
utt (xi, tj ) = 1
k2 (ui,j+1 − 2ui,j + ui,j−1) . (14.51c)
At (i, j) the wave equation is
utt (xi, tj ) = 1
2
c2 [uxx (xi, tj+1) + uxx (xi, tj−1)] . (14.52)
Substitute (14.51) in (14.52) and obtain
ui−1,j+1 + dui,j+1 + ui+1,j+1 = −2eui,j − ui+1,j−1 − dui,j−1 − ui−1,j−1 ,
i = 1, 2,...,n − 1 (14.53)
where d = −2(1 + s2)/s2 and e = 2/s2. If u for j and j − 1 are assumed to be known then
the right-side of the above equation contains only known terms. Then, Eq. (14.53) simply
generates a system of linear equations for the n − 1 unknowns ui,j+1, i = 1, 2,...,n − 1
for (j + 1)th time step. Interestingly Eq. (14.53) gives a tridiagonal system. Let us write
Eq. (14.53) explicitly for i = 1, 2,...,n − 1:
u0,j+1 + du1,j+1 + u2,j+1 = b1,j ,
u1,j+1 + du2,j+1 + u3,j+1 = b2,j ,
.
.
. (14.54)
un−3,j+1 + dun−2,j+1 + un−1,j+1 = bn−2,j ,
un−2,j+1 + dun−1,j+1 + un,j+1 = bn−1,j ,Hyperbolic Equations 319
where
bi,j = −2eui,j − ui+1,j−1 − dui,j−1 − ui−1,j−1 ,
i = 1, 2,...,n − 1, j = 1, 2,...,n − 1. (14.55)
In each subequation of Eqs. (14.54) more than one unknown appears and hence the method
is an implicit one. In matrix form Eqs. (14.54) can be written as











d 1
1 d 1
1 d 1
.
.
.
1 d 1
1 d 1
1 d






















u1,j+1
u2,j+1
u3,j+1
.
.
.
un−3,j+1
un−2,j+1
un−1,j+1











=







b1,j − u0,j+1
b2,j
.
.
.
bn−2,j
bn−1,j + un,j+1







. (14.56)
Since u0,j+1 and un,j+1 in Eq. (14.55) are known from the boundary conditions they are
brought to right-side of Eq. (14.56). The above tridiagonal system can be solved easily using
the procedure discussed in Section 4.8. For each value of j, namely, j = 1, 2,...,n − 1 the
system (14.56) is solved. That is, using ui,0 and ui,1 the system (14.56) gives ui,2. Then,
ui,1 and ui,2 are used to calculate ui,3 and so on.
14.5.6 Stability Analysis of the Implicit Method
Substituting ui,j = eiαihξj and ξ = eβk in the implicit formula (14.53) gives
ξ2

cos αh +
d
2

+ eξ +
d
2 + cos αh
= 0 (14.57)
which can be rewritten as
ξ2 − 2
1+2s2 sin2(αh/2) ξ +1=0 . (14.58)
Its roots are
ξ± = a ± a2 − 1 , a = 1
1+2s2 sin2(αh/2) . (14.59)
|ξ±| < 1 if |a| < 1. This is satisfied without any restriction on h and k values. That is, the
implicit method is unconditionally stable.
Example:
Solve the wave Eq. (14.47) subjected to the conditions (14.48) by the implicit method.
Choose h = 0.05 and k = 0.05. The numerical solution is displayed in Table 14.2.320 Linear Partial Differential Equations
TABLE 14.2
The numerical solution of the wave Eq. (14.47) is calculated by the implicit method and
the exact solution. Here, h = k = 0.05. For each value of t, the first and second rows give
the numerical and exact solutions, respectively.
t x0 = 0.0 x4 = 0.2 x8 = 0.4 x12 = 0.6 x16 = 0.8 x20 = 1.0
0.00 − − −− − − −− − − −− − − −− − − −− − − −−
0.000000 0.587785 0.951057 0.951056 0.587785 0.000000
0.10 − − −− 0.559191 0.904790 0.904790 0.559191 − − −−
0.000000 0.559017 0.904508 0.904508 0.559017 0.000000
0.20 − − −− 0.476521 0.771027 0.771026 0.476521 − − −−
0.000000 0.475528 0.769421 0.769421 0.475528 0.000000
0.30 − − −− 0.347769 0.562702 0.562702 0.347769 − − −−
0.000000 0.345492 0.559017 0.559017 0.345492 0.000000
0.40 − − −− 0.185387 0.299962 0.299962 0.185387 − − −−
0.000000 0.181636 0.293893 0.293893 0.181636 0.000000
0.50 − − −− 0.005077 0.008215 0.008215 0.005077 − − −−
0.000000 0.000000 0.000000 0.000000 0.000000 0.000000
14.5.7 Damped Wave Equation
Consider the damped wave equation
utt = uxx − 2cut , 0 ≤ x ≤ ∞, t ≥ 0 (14.60)
subjected to the initial and boundary conditions
u(x, 0) = e−x , (14.61a)
ut(x, 0) = g(x) = 
−1 − √
2

e−x , (14.61b)
u(0, t)=e(−1−√2 )t . (14.61c)
The exact solution corresponding to the given conditions is
u(x, t)=e−x e(−1−√2 )t . (14.62)
Replacing the derivatives in (14.60) by the central-difference approximations yields
ui,j+1 = 1
1 + ck

−ui,j−1 + 2 
1 − s2
ui,j
+s2 (ui+1,j + ui−1,j ) + ckui,j−1

, (14.63)
where s = k/h. ui,0 is given by the initial condition u(x, 0). To calculate ui,1 use the initial
condition (14.61b). The relation
ut(x, 0) = 1
2k (ui,1 − ui,−1) = gi (14.64)
gives
ui,−1 = ui,1 − 2kgi . (14.65)Parabolic Equations 321
Use the above in Eq. (14.63) for j = 0 and obtain
ui,1 = (1 − ck)kgi +
1
2

2

1 − s2
ui,0 + s2 (ui+1,0 + ui−1,0)

. (14.66)
For stability investigation substitute
ui,j = eiαih eiβjk = eiαihξj (14.67)
in Eq. (14.63) and obtain
(1 + ck)ξ2 − 2ξ

1 − 2s2 sin2(αh/2)
+ 1 − ck = 0 . (14.68)
Its roots are
ξ± = a ± √a2 − 1 + c2k2
1 + ck , (14.69a)
where
a = 1 − 2s2 sin2(αh/2). (14.69b)
For stability |ξ±| < 1. If a < −1 then a2 > 1, c2k2 > 0 and a2 − 1 + c2k2 > 0. So ξ− is
< −1. Similarly, if a > 1 then ξ+ is > 1. The method is thus unstable for |a| > 1. Next,
consider −1 <a< 1. Now, a2 < 1 and, say, a2 − 1 + c2k2 > 0. In this case
a2 − 1 + c2k2 < ck. (14.70)
Write
ξ± = [(−1 <a< 1) ± (< ck)]
1 + ck , (14.71)
Since the absolute value of the numerator in (14.71) is < 1 + ck, |ξ±| < 1. When a2 − 1 +
c2k2 < 0 then
ξ± = [(−1 <a< 1) + i(< ck)]
1 + ck (14.72)
and
|ξ±| =

0 < a2 < 1

+ 
< c2k2
1 + c2k2 + 2ck 1/2
. (14.73)
That is, |ξ±| < 1. Thus, the stability condition is |a| ≤ 1, that is s ≤ 1 or k ≤ h.
14.6 Parabolic Equations
Some examples of phenomena described by parabolic type differential equations are heat
flow, diffusion of particles, transport-reaction in industrial chemicals and materials engi￾neering processes. Specifically, heat flow in a material slab, diffusion of neutrons through
atomic piles and time evolution of probability distribution in certain stochastic dynamics
are governed by parabolic equations.
In this section, consider the heat equation as an example of parabolic type equations
and describe an explicit method and an implicit method for solving it.322 Linear Partial Differential Equations
ui,j ui+1,j
ui,j+1 tj+1
tj ui−1,j
xi−1 xi xi+1
FIGURE 14.5
The grid points (marked by open circles) are involved in the numerical computation of u at
the grid point (i, j + 1) (marked by a solid circle) for the heat equation.
14.6.1 An Explicit Method for Heat Equation
Consider the one-dimensional heat conduction equation
ut = c2uxx , 0 ≤ x ≤ a, 0 ≤ t ≤ b. (14.74)
Equation (14.74) models the temperature distribution in an insulated rod with both the
ends kept at constant temperatures say at g1 and g2 and the initial temperature (t = 0)
distribution along the rod being f(x). Divide the region R = {(x, t):0 ≤ x ≤ a, 0 ≤ t ≤ b}
into n × m rectangles with sides ∆x = h and ∆t = k as done earlier for the wave equation
(see Fig. 14.2). The xi and tj are given by Eq. (14.24).
To obtain a finite-difference formula express the derivatives ut and uxx in terms of
central-differences and forward-differences, respectively, as
ut = 1
k (ui,j+1 − ui,j ) + O(k), (14.75a)
uxx = 1
h2 (ui−1,j − 2ui,j + ui+1,j ) + O 
h2
. (14.75b)
Substituting (14.75), after dropping O(k) and O(h2), in (14.74) and rearranging the terms
result in
ui,j+1 = sui−1,j + (1 − 2s)ui,j + sui+1,j , (14.76)
where s = c2k/h2. The above formula proposed by Schmidt gives the value of ui,j+1 in
terms of ui−1,j , ui,j and ui+1,j and is shown in Fig. 14.5. Note that for the wave equation,
to calculate u at tj+1 the values of u at t = tj−1 and tj are required. For the heat equation
u values at t = tj are alone required to calculate it at tj+1.
Next, to determine a stability condition substitute Eq. (14.37) in the difference
Eq. (14.76) and obtain
eiαihξj+1 = (1 − 2s)eiαihξj + s eiαihξj 
e−iαh + eiαh
. (14.77)
Simplification of this equation results in
ξ = (1 − 2s)+2s cos αh . (14.78)Parabolic Equations 323
TABLE 14.3
The numerical solution of the heat Eq. (14.80) is computed by the explicit formula (14.76)
and the exact solution. Here, h = 0.1, k = 0.005 and s = 0.5. For each value of t, the first
and second rows give the numerical and exact solutions, respectively.
t x0 = 0.0 x2 = 0.2 x4 = 0.4 x6 = 0.6 x8 = 0.8 x10 = 1.0
0.00 − − −− − − −− − − −− − − −− − − −− − − −−
0.000000 0.587785 0.951057 0.951057 0.587785 0.000000
0.10 − − −− 0.215449 0.348604 0.348604 0.215449 − − −−
0.000000 0.219072 0.354466 0.354466 0.219072 0.000000
0.20 − − −− 0.078972 0.127779 0.127779 0.078972 − − −−
0.000000 0.081650 0.132112 0.132112 0.081650 0.000000
0.30 − − −− 0.028947 0.046837 0.046837 0.028947 − − −−
0.000000 0.030432 0.049239 0.049239 0.030432 0.000000
0.40 − − −− 0.010610 0.017168 0.017168 0.010610 − − −−
0.000000 0.011342 0.018352 0.018352 0.011342 0.000000
0.50 − − −− 0.003889 0.006293 0.006293 0.003889 − − −−
0.000000 0.004227 0.006840 0.006840 0.004227 0.000000
Since −1 ≤ cos αh ≤ 1 the condition |ξ| ≤ 1 gives
|1 − 2s + 2s| ≤ 1 or |1 − 2s − 2s| ≤ 1 . (14.79)
The second inequality is satisfied for s ≤ 1/2. Thus, the stability condition for the heat
equation is s ≤ 1/2, that is, c2k/h2 ≤ 1/2 or k ≤ h2/(2c2).
Example:
Solve the heat equation
ut = uxx (14.80a)
with the initial condition
u(x, 0) = f(x) = sin πx (14.80b)
and the boundary conditions
u(0, t) = v1 = 0, u(1, t) = v2 = 0 . (14.80c)
Here, c = 1 and fix h = 0.1. The stability condition k ≤ h2/(2c2) gives k ≤ 0.005. Hence,
choose k = 0.005. The exact solution for the given conditions is
ue(x, t) = sin πx e−π2t . (14.81)
The numerical and the exact solutions for a few values of x for t = 0, 0.05, 0.1, ... , 0.5
are given in Table 14.3. Figure 14.6 shows error versus t for some values of h and k with
s = 0.5.324 Linear Partial Differential Equations
d
c
b
a
t
Error
0 0.2 0.4 0.6 0.8 1
0.01
0.005
0
FIGURE 14.6
Error (ue − u) versus t at x = 0.5 for the heat equation. The values of (h, k) are: (a)
(h, k) = (0.125, 0.0078125), (b) (h, k) = (0.1, 0.005), (c) (h, k) = (0.0625, 0.0019531) and
(d) (h, k) = (0.05, 0.00125).
14.6.2 Crank–Nicholson Implicit Method for Heat Equation
Even though the difference Eq. (14.76) is simple, the method is stable only if 0 < s ≤
0.5. An alternative method was proposed by Crank and Nicholson. In their method, the
approximation of the derivatives ut and uxx at (x, t + k/2) are considered. ut(x, t + k/2) is
given by
ut(x, t + k/2) = 1
k [u(x, t + k) − u(x, t)]
or
ut|i,j+1/2 = 1
k (ui,j+1 − ui,j ) . (14.82)
In Eq. (14.82) the partial derivative ut is expressed in terms of central-difference around
the half-point. The derivative uxx(x, t + k/2) is approximated as the average of the approx￾imations uxx(x, t) and uxx(x, t + k):
uxx|i,j+1/2 = 1
2h2 (ui−1,j+1 − 2ui,j+1 + ui+1,j+1
+ui−1,j − 2ui,j + ui+1,j ) + O 
h2
. (14.83)
Substitution of (14.82) and (14.83) in the heat Eq. (14.74) gives
−ui−1,j+1 +

2 +
2
s

ui,j+1 − ui+1,j+1
=

2 − 2
s

ui,j + ui−1,j + ui+1,j ,
i = 1, 2,...,m − 1
j = 0, 1,...,m − 1. (14.84)Elliptic Equations 325
Define
b1,j =

2 − 2
s

u1,j + u0,j + u2,j + u0,j+1 , (14.85a)
bi,j =

2 − 2
s

ui,j + ui−1,j + ui+1,j+1 , (14.85b)
bn−1,j =

2 − 2
s

un−1,j + un−2,j + un,j + un,j+1 ,
j = 2, 3,...,n − 1 (14.85c)
d = 2+
2
s . (14.85d)
Equation (14.84) can be explicitly written as
du1,j+1 − u2,j+1 = b1,j ,
−u1,j+1 + du2,j+1 − u3,j+1 = b2,j ,
−u2,j+1 + du3,j+1 − u4,j+1 = b3,j ,
.
.
. (14.86)
−un−3,j+1 + dun−2,j+1 − un−1,j+1 = bn−2,j ,
−un−2,j+1 + dun−1,j+1 = bn−1,j .
Assume that the solution is known at t = tj and u0,j+1 and un,j+1 are also known by
the boundary conditions. Then, Eq. (14.86) constitutes a system of linear simultaneous
equations for the unknowns ui,j+1 which can be solved. In matrix form, Eq. (14.86) is
written as









d −1
−1 d −1
−1 d −1
.
.
.
−1 d −1
−1 d


















u1,j+1
u2,j+1
u3,j+1
.
.
.
un−2,j+1
un−1,j+1









=









b1,j
b2,j
b3,j
.
.
.
bn−2,j
bn−1,j









. (14.87)
Since the system (14.87) is in symmetric tridiagonal form it can be solved easily (refer
Section 4.8).
Example:
Solve the heat Eq. (14.80) by the Crank–Nicholson implicit method.
Choose h = 0.1 and k = 0.01. The numerical solution is given in Table 14.4. Figure 14.7
shows error versus t for four set of values of h and k.
14.7 Elliptic Equations
Elliptic partial differential equations are describing the cases where the dynamical variables
do not evolve with time or attain a stationary state. They are often found in the study of
steady-state heat conduction and diffusion problems. For example, the heat conduction in326 Linear Partial Differential Equations
TABLE 14.4
The numerically computed solution of the heat Eq. (14.80) with h = 0.1 and k = 0.01 using
the Crank–Nicholson implicit method and the exact solution. For each value of t, the first
and second rows give the numerical and exact solutions, respectively.
t x0 = 0.0 x2 = 0.2 x4 = 0.4 x6 = 0.6 x8 = 0.8 x10 = 1.0
0.00 − − −− − − −− − − −− − − −− − − −− − − −−
0.000000 0.587785 0.951057 0.951057 0.587785 0.000000
0.10 − − −− 0.220679 0.357066 0.357066 0.220679 − − −−
0.000000 0.219072 0.354466 0.354466 0.219072 0.000000
0.20 − − −− 0.082852 0.134057 0.134057 0.082852 − − −−
0.000000 0.081650 0.132112 0.132112 0.081650 0.000000
0.30 − − −− 0.031106 0.050331 0.050331 0.031106 − − −−
0.000000 0.030432 0.049239 0.049239 0.030432 0.000000
0.40 − − −− 0.011679 0.018896 0.018896 0.011679 − − −−
0.000000 0.011342 0.018352 0.018352 0.011342 0.000000
0.50 − − −− 0.004385 0.007094 0.007094 0.004385 − − −−
0.000000 0.004227 0.006840 0.006840 0.004227 0.000000
d
c
b
a
t
Error
0 0.2 0.4 0.6 0.8 1
0
-0.002
-0.004
FIGURE 14.7
Error (ue − u) versus t at x = 0.5 for the heat equation solved by the Crank–Nicholson
implicit method. The values of (h, k) are: (a) (h, k) = (0.025, 0.000625), (b) (h, k) =
(0.05, 0.0025), (c) (h, k) = (0.1, 0.01) and (d) (h, k) = (0.125, 0.015625).
a solid is modelled by Laplace equation. It also arises in the characteristic of electrostatic
potentials at points of free space and the velocity potential of an irrotational incompressible
fluid flow.
14.7.1 Laplace Equation with Dirichlet Conditions
The heat equation in two-dimension is
ut = µ (uxx + uyy) , (14.88)Elliptic Equations 327
= f1 = f1 = f1
= f2
u3,4
= f2
u2,4
= f2
u1,4
u4,4
u4,3 = g2
u4,2 = g2
u4,1 = g2
u0,4
u0,3 = g1
u0,2 = g1
u0,1 = g1
u0,0 u1,0 u2,0 u3,0 u4,0
.
FIGURE 14.8
A 5×5 grid region for Laplace equation. The values of u at the grid points marked by open
circles are known from the boundary conditions and those at the grid points marked by
solid circles are to be determined.
where µ is the thermal diffusivity and u(x, y, t) is the temperature at (x, y, t). In the case
of steady-state transfer of heat ut = 0 giving uxx + uyy = 0. This equation is called the
Laplace equation. The equation uxx + uyy = f(x, y) is termed as the Poisson equation.
Consider the two-dimensional Laplace equation
uxx + uyy = 0 , 0 ≤ x ≤ a, 0 ≤ y ≤ b . (14.89a)
Assume that the x − y plane with 0 ≤ x ≤ a and 0 ≤ y ≤ b is divided into (n + 1) × (m + 1)
grid points and xi = ih, i = 0, 1,...,n and yj = jk, j = 0, 1,...,m. h = k gives b/a = m/n.
The boundary conditions are
u(x, 0) = f1(x), u(x, b) = f2(x), (14.89b)
u(0, y) = g1(y), u(a, y) = g2(y). (14.89c)
Figure 14.8 shows the 5 × 5 grid points of a square region. The values of u on the four sides
of the square are given by the boundary conditions (14.89b) and (14.89c). The values of u
at the grid points inside the square are unknown and are to be determined.
Replace the second-order partial derivatives by their central-difference approximations.
The result is
1
h2 (ui+1,j − 2ui,j + ui−1,j )
+
1
k2 (ui,j+1 − 2ui,j + ui,j−1) + O 
h2 + k2
= 0 . (14.90)
For the equidistant grid (h = k) Eq. (14.90) becomes (m = n)
ui+1,j + ui−1,j + ui,j+1 + ui,j−1 − 4ui,j = 0, (14.91)
where i = 1, 2,...,n − 1 and j = 1, 2,...,m − 1. Equation (14.91) gives
ui,j = 1
4 [ui+1,j + ui−1,j + ui,j+1 + ui,j−1] . (14.92)328 Linear Partial Differential Equations
ui,j
tj+1
tj
tj−1
xi−1 xi xi+1
FIGURE 14.9
Representation of the formula (14.92) of the Laplace equation. The solution at the solid
circle is given in terms of the solution at the points marked by open circles.
The beauty of Eq. (14.92) is that the solution at the grid point (i, j) is the average of the
solution at the grid points adjacent to it. This is depicted in Fig. 14.9.
For a 5 × 5 grid points (n = m = 4) Eq. (14.91) generates a system of 9 linear equations
for the solutions at the 9 interior points of the square of Fig. 14.8. These equations are
explicitly written as
i = 1, j =1: u2,1 + u0,1 + u1,2 + u1,0 − 4u1,1 = 0 , (14.93a)
i = 2, j =1: u3,1 + u1,1 + u2,2 + u2,0 − 4u2,1 = 0 , (14.93b)
i = 3, j =1: u4,1 + u2,1 + u3,2 + u3,0 − 4u3,1 = 0 , (14.93c)
i = 1, j =2: u2,2 + u0,2 + u1,3 + u1,1 − 4u1,2 = 0 , (14.93d)
i = 2, j =2: u3,2 + u1,2 + u2,3 + u2,1 − 4u2,2 = 0 , (14.93e)
i = 3, j =2: u4,2 + u2,2 + u3,3 + u3,1 − 4u3,2 = 0 , (14.93f)
i = 1, j =3: u2,3 + u0,3 + u1,4 + u1,2 − 4u1,3 = 0 , (14.93g)
i = 2, j =3: u3,3 + u1,3 + u2,4 + u2,2 − 4u2,3 = 0 , (14.93h)
i = 3, j =3: u4,3 + u2,3 + u3,4 + u3,2 − 4u3,3 = 0 . (14.93i)
In the above set of equations, the values of u at the four corners of the square grid in
Fig. 14.8 namely, u0,0, u4,0, u0,4 and u4,4 are not involved.
Bringing the u’s given by the boundary conditions to the right-side and writing
Eq. (14.93) in the matrix form AX = B give














−410100000
1 −41010000
0 1 −4001000
100 −410100
0101 −41010
00101 −4001
000100 −410
0000101 −4 1
00000101 −4




























u1,1
u2,1
u3,1
u1,2
u2,2
u3,2
u1,3
u2,3
u3,3













Elliptic Equations 329
=














−u0,1 − u1,0
−u2,0
−u3,0 − u4,1
−u0,2
0
−u4,2
−u0,3 − u1,4
−u2,4
−u3,4 − u4,3














. (14.94)
For the wave Eq. (14.22) and the heat Eq. (14.74), a system of equations is obtained for
the unknown solution by the implicit method. The obtained system is simply a tridiagonal
in form. But for the Laplace equation, the coefficient matrix in (14.94) is not in tridiagonal
form. However, Eq. (14.94) can be solved by Gauss or Gauss–Jordan method.
Example:
Compute the numerical solution of the Laplace equation
uxx + uyy = 0 , 0 ≤ x ≤ 1, 0 ≤ y ≤ 1 (14.95a)
subjected to the boundary conditions
u(x, 0) = f1(x) = sin πx, (14.95b)
u(x, 1) = f2(x) = sin πx e−π , (14.95c)
u(0, y) = g1(y)=0 , (14.95d)
u(1, y) = g2(y)=0 . (14.95e)
Choose h = k = 0.25, n = m = 4. That is, the x − y plane with x ∈ [0, 1] and y ∈ [0, 1] is
divided into 5 × 5 grids. Gauss–Jordan method is used to solve the system of Eq. (14.94).
The numerical solution is given along with the exact solution in Table 14.5. The exact
solution of Eq. (14.95) is given by
u(x, y) = sin πx e−πy . (14.96)
14.7.2 Laplace Equation with Derivative Boundary Conditions
In certain applications of heat flow the edge is insulated so that there is no heat flux through
the edge. Suppose, y = ym = b is insulated. That is, top edge in Fig. 14.8 is fixed. On this
edge the boundary condition
∂
∂y u (x, ym) = uy(x, b)=0 (14.97)
must be satisfied. Then, for the grid point (xi, ym) Eq. (14.91) becomes
ui+1,m + ui−1,m + ui,m+1 + ui,m−1 − 4ui,m = 0 . (14.98)
The grid point (i, m + 1) lies outside the region R and so ui,m+1 is unknown. However, the
approximation
uy (xi, ym) ≈ 1
2k (ui,m+1 − ui,m−1)=0 (14.99)330 Linear Partial Differential Equations
TABLE 14.5
The numerical and the exact solutions of the Laplace Eq. (14.95). The numerical solution
is obtained by solving (14.94). For each y the first row gives the numerical solution while
the second row gives the exact solution.
y x = 0.0 x = 0.25 x = 0.50 x = 0.75 x = 1.0
0.00 − − −− − − −− − − −− − − −− − − −−
0.000000 0.707107 1.000000 0.707107 0.000000
0.25 − − −− 0.334334 0.472819 0.334334 − − −−
0.000000 0.322397 0.455938 0.322397 0.000000
0.50 − − −− 0.157409 0.222610 0.157409 − − −−
0.000000 0.146993 0.207880 0.146993 0.000000
0.75 − − −− 0.072692 0.102802 0.072692 − − −−
0.000000 0.067020 0.094780 0.067020 0.000000
1.00 − − −− − − −− − − −− − − −− − − −−
0.000000 0.030557 0.043214 0.030557 0.000000
gives
ui,m+1 = ui,m−1 + O 
k2
. (14.100)
Replace ui,m+1 by ui,m−1 in (14.98) and obtain
ui+1,m + ui−1,m + 2ui,m−1 − 4ui,m = 0 . (14.101)
For convenience rewrite the above equation as
2ui,m−1 + ui−1,m + ui+1,m − 4ui,m = 0 , (top − edge). (14.102)
For the bottom-edge, the boundary condition is uy(xi, 0) = 0 and the Laplace difference
equation for the grid point (xi, y0) is
ui+1,0 + ui−1,0 + ui,1 + ui,−1 − 4ui,0 = 0 . (14.103)
Approximating
uy (xi, y0) ≈ 1
2k (ui,1 − ui,−1)=0 (14.104)
gives ui,−1 = ui,1. Then, Eq. (14.103) takes the form
2ui,1 + ui−1,0 + ui+1,0 − 4ui,0 = 0 , (bottom − edge). (14.105)
In a similar way for the left- and right-edges for the boundary conditions ux(0, yj ) = 0 and
ux(a, yj ) = 0 the relevant equations are, respectively,
2u1,j + u0,j−1 + u0,j+1 − 4u0,j = 0 , (left − edge) (14.106)
2un−1,j + un,j−1 + un,j+1 − 4un,j = 0 , (right − edge). (14.107)
When the Neumann boundary condition ∂u(x, y)/∂N is used on certain edges then the value
of u on these edges are unknown and are to be determined by the appropriate difference
Eqs. (14.103), (14.104), (14.106), (14.107) while the values of u on the other edges are given
by the known boundary conditions and the values of u at the interior points are governed
by Eq. (14.91).Elliptic Equations 331
u1,4 u2,4 u3,4 u4,4
u4,3
u4,2
u4,1
u0,4
u0,3
u0,2
u0,1
u0,0 u1,0 u2,0 u3,0 u4,0
FIGURE 14.10
A five by five grid for the Laplace equation uxx +uyy = 0 in the region 0 ≤ x ≤ 1, 0 ≤ y ≤ 1
with a derivative boundary condition at the left-edge. Solution is known at the grid points
marked by open circles. The solution is to be found at the grid points marked by the solid
circles. There are twelve grid points at which the solution needs to be computed.
Example:
Solve the Laplace equation uxx + uyy = 0 in the region 0 ≤ x ≤ 1, 0 ≤ y ≤ 1 subjected to
the boundary conditions
u(x, 0) = cos πx, (14.108a)
u(x, 1) = cos πx e−π , (14.108b)
ux(0, y)=0, (14.108c)
u(1, y) = −e−πy . (14.108d)
The grid points at which solution is known and, where the solution is to be determined are
depicted in Fig. 14.10. Choose
h = k = 0.25 so that n = m = 4. (14.109)
The derivative boundary condition ux(0, y) means the solution at the left-edge of the grid
plane is unknown. The solution at the right-edge is given by the condition u(1, y) = −e−πy.
The solution at the bottom- and top-edges are given by u(x, 0) = cos πx and u(x, 1) =
cos πx e−π, respectively. At the grid points on the left-edge Eq. (14.106) yields
−4u0,1 + 2u1,1 + u0,2 = −u0,0 , (14.110a)
u0,1 − 4u0,2 + 2u1,2 + u0,3 = 0 , (14.110b)
u0,2 − 4u0,3 + 2u1,3 = −u0,4 . (14.110c)
For the 9 interior grid points the Laplace difference Eq. (14.91) gives 9 equations. With the
above three equations there are, totally, 12 equations. These equations are linear system of
equations. This system can be written in matrix form.332 Linear Partial Differential Equations
TABLE 14.6
The numerical solution of the Laplace equation uxx + uyy = 0 subjected to the boundary
conditions (14.108) obtained by solving (14.111) and the exact solution cos(πx) e−πy. For
each y, the first row gives the numerical solution and the second row gives the exact solution.
y x = 0.0 x = 0.25 x = 0.50 x = 0.75 x = 1.0
0.00 − − −− − − −− − − −− − − −− − − −−
1.000000 0.707107 0.000000 −0.707107 1.000000
0.25 0.470232 0.333129 0.001460 −0.328172 − − −−
0.455938 0.322397 0.000000 −0.322397 −0.455938
0.50 0.214671 0.153717 0.000882 −0.151104 − − −−
0.207880 0.146993 0.000000 −0.146993 −0.207880
0.75 0.081018 0.066187 −0.000544 −0.069246 − − −−
0.094780 0.067020 0.000000 −0.067020 −0.094780
1.00 − − −− − − −− − − −− − − −− − − −−
0.043214 0.030557 0.000000 −0.030557 −0.043214
In matrix form the 12 system of equations are written as




















−420010000000
1 −41001000000
0 1 −4100100000
001 −400010000
1000 −42001000
01001 −4100100
001001 −410010
0001001 −40001
00001000 −4100
000001001 −410
0000001001 −4 1
00000001001 −4




















×




















u0,1
u1,1
u2,1
u3,1
u0,2
u1,2
u2,2
u3,2
u0,3
u1,3
u2,3
u3,3




















=




















−u0,0
−u1,0
−u2,0
−u3,0 − u4,1
0
0
0
−u4,2
−u0,4
−u1,4
−u2,4
−u3,4 − u4,3




















. (14.111)
The system (14.111) is solved by the Gauss–Jordan method. The numerical and the exact
solutions (u(x, y) = cos πx e−πy) are given in Table 14.6.Elliptic Equations 333
14.7.3 Iterative Methods for Laplace Equation
The method used for computing the solution of the Laplace equation essentially involved
solving a system of linear equations. For a (n + 1) × (m + 1) grid points (with the solution
specified by the boundary conditions at four edges) a system of (n−1)×(m−1) equations is
to be solved for the solution at the interior grid points. For a 5×5 grid points 9 equations to
be solved. If 10×10 grid points are used then 64 equations are involved. Thus, it is important
to develop methods that will reduce the amount of storage. An example is iterative methods.
Let us consider the Laplace difference Eq. (14.91) with the boundary conditions (14.89b)
and (14.89c). Rewrite Eq. (14.91) in the form
ui,j = ui,j + si,j , (14.112a)
where the residual term si,j is given by
si,j = 1
4 (ui+1,j + ui−1,j + ui,j+1 + ui,j−1 − 4ui,j ) (14.112b)
and i = 1, 2,...,n − 1, j = 1, 2,...,m − 1.
The initial guess may be chosen as the average of the 2(n − 1) + 2(m − 1) boundary
values on the edges of the region R leaving the corner grid points. Iteration of (14.112) is
continued until |si,j | < δ, where δ is a small preassumed positive constant. Denote ue and
un as the exact and numerical solutions, respectively. One cannot realize |ue − un| → 0 as
δ → 0. This is because the formula (14.91) is not exact and has an error O(h2).
si,j in Eqs. (14.112) is to change the value of ui,j for one iteration. The speed of conver￾gence can be increased by adding a larger change than this. That is, write
ui,j = ui,j + ωsi,j , 1 ≤ ω < 2 . (14.113)
This is called successive over-relaxation (SOR). In the SOR method, the optimal value of ω
obtained from the study of eigenvalues of iteration matrices for linear systems is often used
and is given by [3]
ω = 4
2 + 
4 − [cos(π/(n − 1)) + cos(π/(m − 1))]2
1/2 . (14.114)
If the derivative boundary condition is applied on some edges then in addition to Eq. (14.113)
(which are for interior grid points) appropriate following equations have to be used.
Top-edge:
ui,m = ui,m +
ω
4 [2ui,m−1 + ui−1,m + ui+1,m − 4ui,m] . (14.115a)
Bottom-edge:
ui,0 = ui,0 +
ω
4 [2ui,1 + ui−1,0 + ui+1,0 − 4ui,0] . (14.115b)
Left-edge:
u0,j = u0,j +
ω
4 [2u1,j + u0,j−1 + u0,j+1 − 4u0,j ] . (14.115c)334 Linear Partial Differential Equations
TABLE 14.7
The numerical solution of the Laplace Eq. (14.95) obtained by Gauss–Seidel iteration of
Eq. (14.113) with h = k = 0.25. The solution with |si,j | < δ = 0.001 is realized at the end
of 8th iteration. For each y the first row gives the numerical solution and the second row
gives the exact solution.
y x = 0.0 x = 0.25 x = 0.50 x = 0.75 x = 1.0
0.00 − − −− − − −− − − −− − − −− − − −−
0.000000 0.707107 1.000000 0.707107 0.000000
0.25 − − −− 0.334783 0.473230 0.334522 − − −−
0.000000 0.322397 0.455938 0.322397 0.000000
0.50 − − −− 0.157820 0.222986 0.157581 − − −−
0.000000 0.146993 0.207880 0.146993 0.000000
0.75 − − −− 0.072880 0.102974 0.072771 − − −−
0.000000 0.067020 0.094780 0.067020 0.000000
1.00 − − −− − − −− − − −− − − −− − − −−
0.000000 0.030557 0.043214 0.030557 0.000000
Right-edge:
un,j = un,j +
ω
4 [2un−1,j + un,j−1 + un,j+1 − 4un,j ] . (14.115d)
Example:
Solve the Laplace Eq. (14.95) by the iteration method.
Iterate Eq. (14.113) by Gauss–Seidel method, where the updated values of u are used as
they become available. Choose h = k = 0.25, the tolerance δ = 0.001 and ω given by
Eq. (14.114). At the end of 8th iteration solution with |si,j | < δ is realized. Table 14.7 gives
the numerical solution.
14.7.4 Poisson Equation
The methods discussed for Laplace equation can be extended to the Poisson equation
uxx + uyy = g(x, y) (14.116)
and the Helmholtz’s equation
uxx + uyy + f(x, y)u = g(x, y). (14.117)First-Order Equation 335
TABLE 14.8
The numerical solution of the Poisson equation uxx + uyy = x − (y/2) subjected to the
boundary conditions (14.118). For each y the numerical solution is given in the first row
while the exact solution is given in the second row.
y x = 0.0 x = 0.25 x = 0.50 x = 0.75 x = 1.0
0.00 − − −− − − −− − − −− − − −− − − −−
0.000000 0.709711 1.020833 0.777419 0.166667
0.25 − − −− 0.335636 0.492351 0.403344 − − −−
−0.001302 0.323699 0.475469 0.391407 0.165365
0.50 − − −− 0.149596 0.233026 0.217305 − − −−
−0.010417 0.139181 0.218296 0.206889 0.156250
0.75 − − −− 0.040140 0.088479 0.107848 − − −−
−0.035156 0.034468 0.080457 0.102176 0.131510
1.00 − − −− − − −− − − −− − − −− − − −−
−0.083333 −0.050172 −0.019286 0.017536 0.083333
Example:
Solve the Poisson Eq. (14.116) with g(x, y) = x − y/2 and 0 ≤ x ≤ 1, 0 ≤ y ≤ 1 subjected
to the boundary conditions
u(x, 0) = f1 = sin πx +
x3
6 , (14.118a)
u(x, 1) = f2 = sin πx e−π +
x3
6 − 1
12 , (14.118b)
u(0, y) = g1 = −y3
12 , (14.118c)
u(1, y) = g2 = 1
6 − y3
12 . (14.118d)
Its exact solution is
u(x, y) = sin πx e−πy +
x3
6 − y3
12 . (14.119)
The Poisson difference equation (refer Eq. (14.91)) for the solution is
ui+1,j + ui−1,j + ui,j+1 + ui,j−1 − 4ui,j = h2gi,j , (14.120)
where i = 1, 2,...,n − 1, j = 1, 2,...,m − 1 and gi,j = xi − yj/2. The obtained numerical
solution is given in Table 14.8.
14.8 First-Order Equation
In this chapter, so far certain second-order linear partial differential equations are consid￾ered. In all these equations the starting and end values of x are fixed as, say, a and b and336 Linear Partial Differential Equations
the solution u(x, t) is explicitly specified at x = a and x = b, that is u(a, t) and u(b, t) are
given. Now, consider a linear first-order equation with x ∈ [−∞, ∞]. However, assume that
u(x, t) → α as x → −∞ and u(x, t) → β as x → +∞. α and β may be zero.
Consider the Fisher equation
ut + cux = 0 , (14.121)
where c is a constant. The exact solution of (14.121) is
u(x, t) = sech(x − ct). (14.122)
The solution u has the property u → 0 as x → ±∞. To find a numerical solution approxi￾mate ut and ux as
ut = 1
2k (ui,j+1 − ui,j−1) , (14.123a)
ux = 1
2h (ui+1,j − ui−1,j ) . (14.123b)
Then, Eq. (14.121) gives
ui,j+1 = ui,j−1 − ck
h (ui+1,j − ui−1,j ) . (14.124)
To determine ui,2 the solution at j = 0 and 1 are needed. For ui,1 approximate
ut = 1
k (ui,j+1 − ui,j ) (14.125)
and substitute (14.123b) and (14.125) in (14.121). This gives for j = 0
ui,1 = ui,0 − ck
2h (ui+1,0 − ui−1,0) . (14.126)
ui,0 is given by the initial condition so choose ux,0 = sechx. ui,1 can be computed from
(14.126). Then, Eq. (14.124) determines ui,2, ui,3 and so on. For stability of Eq. (14.124)
the condition is k ≤ h/c.
Because the initial solution is localized about the origin and the exact solution is a
traveling wave moving in the forward direction choose x ∈ [−20, 40]. Fix h = 0.1, k = 0.01
and c = 1. Figure 14.11 shows the numerical solution at t = 5 and 15. The exact solution is
also shown for comparison.
14.9 Time-Dependent Schr¨odinger Equation
In Section 13.4, the problem of numerically solving the time-independent Schr¨odinger equa￾tion by the finite-difference method is discussed. This is a boundary-value problem. The
present section deals with a method of solving the time-dependent Schr¨odinger equation.
Here again finite-difference scheme is employed. Specifically, the focus is on capturing of
time development of certain initial wave profile in a few physically important systems. This
is an initial-value problem.
The time-dependent Schr¨odinger equation of a quantum mechanical particle of mass m
in a potential V (x) is given by
i
∂ψ
∂t = − 2
2m
∂2ψ
∂x2 + V (x)ψ , (14.127)Time-Dependent Schr¨odinger Equation 337
t = 5 t = 15
x
u
-10 0 10 20
1
0.5
0
FIGURE 14.11
The numerical solutions (continuous curves) of Eq. (14.121) at t = 5 and 15. The exact
solutions at selective values of x are represented by solid circles and open circles for t = 5
and 15, respectively.
where  is the Planck’s constant and ψ is function of x and t. In quantum mechanics, the
operator i∂/∂t is the energy operator. For time-independent solutions i∂/∂t is replaced
by E. In this case, Eq. (14.127) becomes Eq. (13.28). In the following, the finite-difference
method described by Chen [4] is used to solve Eq. (14.127). The final finite-difference formula
derived by Chen is equivalent to the one obtained by Goldberg et al [5]. As Eq. (14.127) is
complex its solution is also complex.
For convenience set the values of  and m as unity and rewrite Eq. (14.127) as
i
∂ψ
∂t = −1
2
∂2ψ
∂x2 + V (x)ψ . (14.128)
Discretize x in the interval [a, b] with grid points x0,x1,...,xN with xi+1 − xi = h and the
time interval [0, T] with grid points 0, t1, t2,...,tm with tj+1 − tj = k. Equation (14.127) is
subjected to the following initial and boundary conditions
ψ(x, 0) = φ(x), ψ(±∞, t)=0 . (14.129)
Denote ψi,j as the solution ψ(x, t) at the grid point (i, j). ψ and their partial derivatives
∂ψ/∂t and ∂2ψ/∂x2 are approximated as
ψ(x, t) = ψ(xi, tj ) = 1
2 (ψi,j + ψi,j−1) , (14.130a)
∂
∂tψ(x, t) = 1
k (ψi,j − ψi,j−1) , (14.130b)
∂2
∂x2 ψ(x, t) = 1
2h2 [(ψi+1,j − 2ψi,j + ψi−1,j )
+ (ψi+1,j−1 − 2ψi,j−1 + ψi−1,j−1)] . (14.130c)
Replacing ψ, ∂ψ/∂t and ∂2ψ/∂x2 by Eqs. (14.130) in Eq. (14.128) gives
1
2h2 ψi+1,j +
2i
k − 1
h2 − Vi

ψi,j +
1
2h2 ψi−1,j
−
2i
k +
1
h2 + Vi

ψi,j−1 +
1
2h2 (ψi+1,j−1 + ψi−1,j−1)=0. (14.131)338 Linear Partial Differential Equations
Defining
a = c = 1
2h2 , d =
2i
k − 1
h2 − Vi

, (14.132a)
bi =
2i
k +
1
h2 + Vi

ψi,j−1 − 1
2h2 (ψi+1,j−1 + ψi−1,j−1) (14.132b)
Eq. (14.131) is rewritten as
aψi−1,j + dψi,j + cψi+1,j = bi . (14.133)
With ψ0,j = 0 and ψN,j = 0 Eq. (14.133) can be written in the following matrix form


100
adc
0 adc
.
.
.
adc 0
adc
001




ψ0,j
ψ1,j
ψ2,j
.
.
.
ψN−2,j
ψN−1,j
ψN,j


=


0
b1
b2
.
.
.
bN−2
bN−1
0


. (14.134)
Since ψ0,j and ψN,j are set to zero according to the boundary condition given in Eq. (14.129)
the system (14.134) becomes


d c;
adc
adc
.
.
.
adc
adc
a d




ψ1,j
ψ2,j
ψ3,j
.
.
.
ψN−3,j
ψN−2,j
ψN−1,j


=


b1
b2
b3
.
.
.
bN−3
bN−2
bN−1


(14.135)
which is of the form Aψ = B. An interesting feature of the matrix A is that it is a tridiagonal
form and can be solved in a simple manner using the procedure discussed in Section 4.8.
Equation (14.133) is the finite-difference formula for the solution of the time-dependent
Schr¨odinger Eq. (14.128).
Now, consider a few quantum mechanical systems and find the time-evolution of initial
wave function.
Example 1: Free Particle
For a free particle V (x) = 0 and a Gaussian wave packet of it at t = 0 is
ψ(x, 0) =  1
√2π σ 1/2
e−(x−x0)2/(4σ2) eik0x . (14.136)
It is a plane wave with wave number k0 modulated by a Gaussian profile. Exact ψ(x, t) is
given by
ψe(x, t) =  σ √2π α2
1/2
e−[σ2k2
0−(β2/(4α2))] , (14.137a)
where
α2 = σ2 +
i
2m
t, β = −i

x − x0 − 2iσ2k0

. (14.137b)Time-Dependent Schr¨odinger Equation 339
0.4
0.2
0
t
10
5
0 x
15
0
-15
|ψ|
2
FIGURE 14.12
The numerically computed |ψ(x, t)|
2 of a free particle. The initial Gaussian wave packet
decays with time.
As t → ∞, ψ(x, t) → 0. To compute numerically ψ(x, t) fix x0 = 0, k0 = 0, σ = 1,
 = m = 1, h = 0.1, k = 0.1 and x ∈ [−50, 50]. ψ(x, t) is complex even if ψ(x, 0) is a pure
real. Therefore, in the program ψ(x, t) is declared as a complex variable. Real and imaginary
parts of ψ can be obtained as
ψR = [ψ + conjg(ψ)]/2 , (14.138a)
ψI = −i[ψ − conjg(ψ)]/2 , (14.138b)
where conjg(ψ) denotes complex conjugate of ψ, that is ψ∗. |ψ|
2 = ψ∗ψ is obtained through
|ψ|
2 = ψ × conjg(ψ). (14.139)
Figure 14.12 shows |ψ(x, t)|
2. The initial wave packet spreads in space as time increases and
finally flatten.
Example 2: Harmonic Oscillator
The potential of the linear harmonic oscillator is V (x) = x2/2. The eigenfunctions of the
time-independent Schr¨odinger equation are given by Eq. (13.42), where n is the quantum
number which takes values 0, 1,... and the energy eigenvalues are En = (n+ 1
2 ). The exact
time evolution of the stationary state eigenfunction ψn(x) is given by
ψn(x, t) = ψn(x) e−iEnt/ . (14.140)
ψ1(x, t) is
ψ1(x, t)=2  1
2
√π
1/2
xe−x2/2e−i3t/2 , (14.141)
where  is set as 1. ψ1(x, t) → 0 as x → ±∞ and is periodic in time with period T = 4π/3.
The Schr¨odinger Eq. (14.128) is solved for V (x) = x2/2 with ψ1(x, 0) as the initial wave
function. Fix h = 0.1, k = 0.1 and x ∈ [−5, 5]. Figure 14.13 depicts the numerically
computed ψR (real part of ψ1), ψI (imaginary part of ψ1) and |ψ1|
2 for t < 10. Note that at
t = 0, ψI = 0. However, as time increases ψI evolves. ψR and ψI evolve periodically in time
while |ψ(x, t)|
2 is independent of time.340 Linear Partial Differential Equations
0.50 -0.5
t
10
5
0
x
5
0
-5
ψ
R
(a)
0.50 -0.5
t
10
5
0
x
5
0
-5
ψI
(b)
0.5
0
t
8
4
0
x
5 0 -5
|ψ|
2 (c)
FIGURE 14.13
The numerically computed time evolution of real and imaginary parts of wave function of
the harmonic oscillator with n = 1. |ψ(x, t)|
2 is also plotted. ψR and ψI evolve periodically in
time while |ψ(x, t)|
2 is independent of time. The period of the wave function is 4π/3 ≈ 8.38.Time-Dependent Schr¨odinger Equation 341
1
0
-1
t
10
5
0
x
1
0
-1
ψ
R
(a)
1
0
-1
t
10
5
0
x
1
0
-1
ψ
R
(a)
1
0
-1
t
10
5
0
x
1
0
-1
ψ
I
(b)
1
0
-1
t
10
5
0
x
1
0
-1
ψ
I
(b)
1
0
t
10
5
0 x
1
0
-1
|ψ|
2
(c)
1
0
t
10
5
0 x
1
0
-1
|ψ|
2
(c)
FIGURE 14.14
The numerically computed time evolution of real and imaginary parts of the wave function
of a particle in a box with n = 1. |ψ(x, t)|
2 is also plotted. ψR and ψI evolve periodically in
time while |ψ(x, t)|
2 is independent of time. The period of the wave function is 16/π ≈ 5.093.
Example 3: Particle in a Box
Consider a particle confined to a box potential V (x) = 0 for |x| < a and ∞ for |x| > a. Its
stationary state energy eigenvalues and eigenfunctions are given by
En = 2π2n2
8ma2 , n = 1, 2,... (14.142)
and
ψn(x) =



√
1
a sin knx, kn = nπ
2a , n = 2, 4,...
√
1
a cos knx, kn = nπ
2a , n = 1, 3,... .
(14.143)
The time evolution of ψ(x, 0) is ψ(x, t) = ψ(x, 0)e−iEnt/. In the numerical study a = 1,
h = 0.01, k = 0.025, n = 1. Figure 14.14 shows the numerical result. The initial wave
function is a half-cosine wave. The period of the wave function is 16/π ≈ 5.093 which can
be clearly noticed in this figure. For n = 4, the initial wave is a sine wave. Figure 14.15342 Linear Partial Differential Equations
1
-1
t
0.8
0.4
0
x
1
0
-1
ψ
R
(a)
1
-1
t
0.8
0.4
0
x
1
0
-1
ψ
R
(a)
1
-1
t
0.8
0.4
0
x
1
0
-1
ψ
I
(b)
1
-1
t
0.8
0.4
0
x
1
0
-1
ψ
I
(b)
1
0
t
0.8
0.4
0 x
1
0
-1
|ψ|
2
(c)
1
0
t
0.8
0.4
0 x
1
0
-1
|ψ|
2
(c)
FIGURE 14.15
The numerically computed time evolution of real and imaginary parts of the wave function
of a particle in a box with n = 4. |ψ(x, t)|
2 is also plotted. ψR and ψI evolve periodically in
time while |ψ(x, t)|
2 is independent of time. The period of the wave function is 1/π ≈ 0.318.
presents the numerically computed ψR, ψI and |ψ|
2. The period of ψ(x, t) is 1/π ≈ 0.318.
Example 4: Particle in the Presence of a Step Potential
The harmonic oscillator and the particle in a box potential possess stationary state eigen￾functions. These two systems are simple physical systems with bound states with ψ(x, t) → 0
as x → ±∞. Next, consider an example of a system, where the wave function incident on
a step potential and undergoes reflection and refraction. Let the potential of the system isTime-Dependent Schr¨odinger Equation 343
1
-1
t
2
1
0 x
10
5
0
ψ
R
(a) V0 = 5
1
-1
t
2
1
0 x
10
5
0
ψ
R
(b) V0 = 10
1
-1
t
2
1
0 x
10
5
0
ψ
R
(c) V0 = 50
FIGURE 14.16
The numerically computed time evolution of real part of the wave function ψ(x, t) of a
particle in the presence of a step potential with ψ(x, 0) given by Eq. (14.144).
V (x) = 0 for x<x0 and V0 for x>x0. Choose the initial wave function as
ψ(x, 0) =  1
√2π
1/2
e−x2
eik0x . (14.144)
In the numerical simulation x0 = 5, k0 = 5, h = 0.05, k = 0.05, x ∈ [−10, 20]. Figure 14.16
shows the ψR and |ψ|
2 for three values of V0.
For V0 = 5 and V0 = 10 the incident wave is partly reflected and partly refracted. The
amplitude of the reflected wave is relatively large for V0 = 10 when compared to the case of344 Linear Partial Differential Equations
1
0
t
2
1
0 x
10
5
0
|ψ|
2 (a) V0 = 5
1
0
t
2
1
0 x
10
5
0
|ψ|
2 (b) V0 = 10
2
0
t
2
1
0 x
10
5
0
|ψ|
2 (c) V0 = 50
FIGURE 14.17
The numerically computed |ψ(x, t)|
2 of a particle in the presence of a step potential with
ψ(x, 0) given by Eq. (14.144).
V0 = 5. As V0 increases the amplitude of the reflected wave increases while the amplitude
of the refracted wave decreases. For sufficiently large V0 the wave is completely reflected.
This can be clearly seen in the Figs. 14.16 and 14.17 for V0 = 50.
Example 5: Tunnelling
An interesting phenomenon which can occur in certain quantum systems and not observable
in classical systems is the tunnelling. If a quantum mechanical particle is impinging on a
barrier with an energy relatively lower than the height of the barrier potential, it will not
necessarily be reflected by the barrier, but there is always a finite nonzero probability thatTime-Dependent Schr¨odinger Equation 345
1
-1
t
3
2
1
0 x
10
5
0
-5
ψ
R
(a) V0 = 5
1
-1
t
3
2
1
0 x
10
5
0
-5
ψ
R
(b) V0 = 20
1
-1
t
3
2
1
0 x
10
5
0
-5
ψ
R
(c) V0 = 500
FIGURE 14.18
The numerically computed time evolution of real part of the wave function ψ = e−x2+i5x in
the presence of a rectangular barrier of height V0 and width 0.1 at x = 5. The effect of V0
can be clearly seen.
it may cross the barrier and continue its forward motion in the classically forbidden region.
This is called tunnel effect and can be simulated numerically by solving the time-dependent
Schr¨odinger equation by the finite-difference method.
As an example consider a particle in the presence of a rectangular barrier of height V0
and width 0.1. The potential is V (x) = V0 for x ∈ [5, 5.1] and 0 elsewhere. Choose the initial
wave function as e−x2+i5x and h = 0.05, k = 0.05, x ∈ [−10, 20]. Figure 14.18 shows the
time evolution of the real part of ψ for V0 = 5, 20 and 500. For small values of V0, ψR is346 Linear Partial Differential Equations
nonzero in the forbidden region x > 5.1. The amplitude of the wave in this region decreases
with increase in V0. For V0 = 500 in Fig. 14.18 ψR is almost negligible in magnitude.
14.10 Concluding Remarks
Numerical computations of solutions of variety of linear partial differential equations are
well established and several techniques have been developed. In this chapter, presentation
is restricted to applications of finite-difference schemes to a few standard partial differen￾tial equations modelling physical systems. For solving linear equations apart from finite￾difference schemes other methods such as gradient discretization, finite-volume element,
multi-grid, meshfree, spectral and domain composition, etc. are well known. For a brief in￾troduction to these methods, a reader may refer to the ref. [1]. Algorithms for linear partial
differential equations with variable coefficients, singular coefficients and stochastic terms
have been proposed and tested.
It is also possible to solve numerically the heat and the wave equations subjected to a
pure initial-value problem. For example, for the initial condition u(x, 0) = f(x), −∞ <x<
∞ the explicit method can be used. The use of implicit method is practically not possible
since it generates an infinite set of equations to be solved.
14.11 Bibliography
[1] M. Kumar and G. Mishra, Appl. Math. 2:1327, 2011.
[2] M. Pal, Numerical Analysis for Scientists and Engineers: Theory and C Programs.
Narosa, New Delhi, 2007.
[3] J.H. Mathews, Numerical Methods for Mathematics Science and Engineering.
Prentice-Hall of India, New Delhi, 2005.
[4] R.L.W. Chen, Am. J. Phys. 50:902, 1982; 51:570, 1983.
[5] A. Goldberg, H.M. Schey and J.L. Schwartz, Am. J. Phys. 35:177, 1967.
14.12 Problems
14.1 Solve the wave equation
utt = 4uxx , 0 ≤ x ≤ 1, 0 ≤ t ≤ 1
subjected to the conditions
u(x, 0) = sin πx, 0 ≤ x ≤ 1
ut(x, 0) = 0 , 0 ≤ x ≤ 1
u(0, t)=0 , u(1, t)=0 0 ≤ t ≤ 1
with h = 0.1 and k = 0.05. Compare the numerical solution with the exact
solution ue(x, t) = sin πx cos 2πt for x = 0.5.Problems 347
14.2 The exact solution of the wave equation
utt = 4uxx , 0 ≤ x ≤ 1, 0 ≤ t ≤ 1
is ue(x, 0) = sin πx cos 2πt+sin 2πx cos 4πt. Solve the equation with the conditions
u(x, 0) = sin πx + sin 2πx, 0 ≤ x ≤ 1
ut(x, 0) = 0 , 0 ≤ x ≤ 1
u(0, t)=0 , u(1, t)=0 , 0 ≤ t ≤ 1
with (a) h = 0.1, k = 0.05 and (b) h = 0.2, k = 0.1. Compare the numerical
solution with the exact solution for x = 0.2.
14.3 Solve the wave equation utt = uxx, with the conditions
u(x, 0) = (3/4) sin πx − (1/4) sin 3πx, 0 ≤ x ≤ 1
ut(x, 0) = 0 , 0 ≤ x ≤ 1
u(0, t)=0 , u(1, t)=0 , 0 ≤ t ≤ 1.
Choose h = 0.1 and k = 0.05. Construct its exact solution and compare the
numerical solution with the exact solution for x = 0.5.
14.4 Solve the damped wave equation
utt = uxx − 2cut , 0 ≤ x ≤ ∞, t ≥ 0
subjected to the initial and boundary conditions
u(x, 0) = e−x , ut(x, 0) = g(x) = 
−1 − √
2

e−x ,
u(0, t)=e(−1−√2)t .
Compare the numerical solution with the exact solution
u(x, t)=e−x e(−1−√2 )t .
for t = 5 and 10 with h = 0.1, k = 0.05 and x ∈ [0, 30].
14.5 Obtain the formula for solving the heat equation with the approximation
ut = 1
2k (ui,j+1 − ui,j−1) , uxx = 1
h2 (ui+1,j − 2ui,j + ui−1,j ) .
Applying the stability analysis shows that the formula is unconditionally unstable.
14.6 Obtain the numerical solution of the heat equation ut = uxx for 0 ≤ x ≤ 1,
0 ≤ t ≤ 1 with the boundary conditions
u(x, 0) = sin 2πx, u(0, t) = sin 2πt, u(1, t) = sin 2πte−2π .
Fix h = 0.1 and k = 0.005. Compare the numerical solution with the exact
solution
ue(x, t) = sin 2πx e−2πy + sin 2πt e−2πx .348 Linear Partial Differential Equations
14.7 The exact solution of the heat equation ut = uxx subjected to the boundary
conditions u(x, 0) = 0, u(0, t) = 0, u(1, t) = sinhπ sin πt is
u(x, t) = sin πx sinhπt + sinhπx sin πt.
Solve the equation by the Crank–Nicholson method with h = 0.1 and k = 0.01.
Compare the numerical solution with the exact analytical solution.
14.8 Derive an explicit and implicit difference formulas for the parabolic equation
ut − uxx = g(x).
14.9 Construct an iterative formula similar to Eq. (14.112) for the heat equation ut =
c2uxx. Then for h = 0.1, k = 0.01, u(x, 0) = sin πx, u(0, t) = sin πt, u(1, t) =
sin πte−π find the numerical solution using Gauss–Seidel method.
14.10 Find the numerical solution of the equation uxx+uyy = −u, 0 ≤ x ≤ π, 0 ≤ y ≤ π
with the boundary values u(x, 0) = cos x, u(x, π) = cos x, u(0, y) = 1 + sin y,
u(π, y) = −1 + sin y. Compare the numerical solution with the exact solution
u(x, y) = cos x + sin y.
14.11 Consider the Laplace equation uxx + uyy = 0, 0 ≤ x ≤ 1, 0 ≤ y ≤ 1, u(x, 0) =
cos(πx/2), u(x, 1) = cos(πx/2)e−π/2, u(0, y)=e−πy/2, u(1, y) = 0. With h = k =
0.25 obtain the numerical solution by the explicit method and the Gauss–Seidel
iteration method. What is the exact solution?
14.12 Consider the Laplace equation uxx + uyy = 0, 0 ≤ x ≤ 1, 0 ≤ y ≤ 1 with
the boundary condition u(x, 0) = cos πx, u(x, 1) = cos πxe−π, ux(1, y) = 0,
u(1, y) = −e−πy. Show that its exact solution is u(x, y) = cos πxe−πy. Divide the
region 0 ≤ x ≤ 1, 0 ≤ y ≤ 1 into 5 × 5 grid points and construct the system
of equations of the form Au = B for the unknown solution at the grid points.
Develop a computer program in C++ or Python for solving the Laplace equation
by Gauss–Jordan method and then using the program find the numerical solution.
14.13 Construct the exact solution of the Laplace equation uxx + uyy = 0, 0 ≤ x ≤ 1,
0 ≤ y ≤ 1 with the boundary conditions u(0, y) = cos πy, u(1, y) = cos πye−π,
u(x, 0) = e−πx, uy(x, 1) = 0. For a 5 × 5 grid points explicitly write down the
system of equations for the unknown solutions. Develop a computer program in
C++ or Python for solving them by Gauss–Jordan method. Using the program
obtain the numerical solution.
14.14 Obtain the exact solution of the Laplace equation uxx + uyy = 0, 0 ≤ x ≤ 1,
0 ≤ y ≤ 1 with the boundary conditions u(0, y) = cos πy, u(1, y) = cos πye−π,
u(x, 1) = −e−πx, uy(x, 0) = 0. For a 5 × 5 grid points explicitly write down the
system of equations for the unknown solution. Develop a computer program in
C++ or Python for solving them by Gauss–Jordan method. Using the program
obtain the numerical solution.
14.15 Develop a nine-point difference formula for the Laplace equation.
14.16 Show that the stability condition for the formula (14.124) is k ≤ h/c.
14.17 Instead of using Eq. (14.126) for ui,1 let ui,1 = ui,0 and then solve Eq. (14.121).
Compare the obtained solution at t = 10 with the one obtained using Eq. (14.126)
and also with the exact solution.
14.18 The normalized stationary state eigenfunction of a particle in a box potential is
ψ(x, 0) = 15/16 (x2 −a2) for |x| < a and 0 elsewhere. Study the time evolution
of ψ.Problems 349
14.19 Consider the wave function ψ(x, 0) = 1
2 (cos π
2 x + sin πx) of a particle in a box
potential. It is a sum of the stationary state eigenfunctions ψ1 and ψ2. Compute
numerically ψ(x, t) and determine its periodicity.
14.20 Tunnelling of a wave function can also be observed with the truncated Gaussian
potential V (x) = V0e−β2x2
for |x| < a and 0 elsewhere. Study the time evolution
of the initial Gaussian wave packet. Write a note on the effect of V0 and β on the
amplitudes of the reflected and transmitted waves.15
Nonlinear Partial Differential Equations
15.1 Introduction
The previous chapter dealt with linear partial differential equations (LPDEs). The present
chapter is concerned with nonlinear partial differential equations (NLPDEs). Certain class of
NLPDEs like Korteweg–de Vries (KdV) equation, sine–Gordon (sG) equation and nonlinear
Schr¨odinger (NLS) equation possess a variety of fascinating localized solutions called soli￾tary waves [1–3]. In certain nonlinear wave systems, these waves undergo elastic collisions.
Their shapes and speeds are preserved after interactions with other solitary waves. Such
wave solutions are named as solitons by Zabusky and Kruskal [4]. Zabusky and Kruskal
numerically solved the KdV equation with a periodic boundary condition and applied a
finite-difference (FD) scheme. The initial cosine wave evolved into a number of solitary
waves with different heights and speeds. Zabusky and Kruskal observed elastic collisions
of these waves. Their scheme is able to predict accurately the solution during short time
interval only and in the long-time limit unphysical oscillations are developed in the numer￾ical solution leading to blowup of the solution. Since then, a great deal of interest has been
focused on developing stable numerical methods for NLPDEs. Methods have been devel￾oped to simulate and analysis the novel phenomena like elastic collision of localized waves,
vortex formation, rogue waves, spiral waves, chimeras, spatio-temporal patterns, quenching
dynamics, hierarchical decomposition of an MRI image, etc.
This chapter restricts to NLPDEs possessing solitary waves and solitary wave-like solu￾tions. Various numerical approaches such as FD schemes, spectral method, symplectic and
multi-symplectic theories have been developed. In the FD schemes, the partial derivatives
are approximated by FD formulas. In the spectral method spatial derivatives are computed
in Fourier space and time derivatives are approximated by FD formulas. Since the soli￾tonic equations are conservative equations a scheme developed for them should preserve
the energy associated with them. Moreover, the integrable solitonic equations admit infinite
number of conservation laws or integrals of motion. They need to be preserved to a desired
accuracy in order to generate numerical solutions with desired accuracy. For this purpose,
for Hamiltonian PDEs symplectic and multi-symplectic methods were developed [5–8]. For
a review on the numerical methods of NLPDEs one may refer to the refs. [9-13].
The symplectic and multi-symplectic methods start with the Hamiltonian of the given
equation. In the symplectic approach the spatial derivatives are approximated by FD for￾mulas and then the equation of motion is written in the Hamiltonian form by introducing
additional variables. The result is a system of first-order equations. Applying a discretiza￾tion to this system and then eliminating additional variables introduced lead to a symplectic
scheme. In the multi-symplectic method without discretizing the spatial derivatives the given
equation is expressed in Hamiltonian form. In this case, for example, for the equation with
one time variable and one space variable the conservation law has a component along the
time direction and another component along the spatial direction. That is, the conservation
law is in multi-symplectic form.
DOI: 10.1201/9781032649931-15 350Hamilton’s Equation of Motion 351
In this chapter, first the Hamiltonian formulation of PDEs and the notion of conservation
of symplecticity are introduced. Next, the multi-symplectic integration with the Euler box
scheme is presented. Then, the multi-symplectic schemes for the KdV and sG equations are
derived and analyzed for their applicability and efficacy by comparing the numerical results
with the exact results.
15.2 Hamilton’s Equation of Motion
In classical mechanics, there is a class of systems whose evolution equations of dynamical
variables can be expressed as
˙p = −∇qH(q, p), (15.1a)
˙q = ∇pH(q, p), (15.1b)
where H is called Hamiltonian (total energy). In Eq. (15.1) there is a skew(anti)-symmetry
between q and p. Assume that H = P(p) + V (q). H is a constant of motion (does not
change with time) because
dH
dt = (∇pH) ˙p + (∇qH) ˙q = ˙q ˙p − ˙p ˙q = 0 . (15.2)
Further, for ω = dp ∧ dq (called symplectic manifold), where ∧ represents wedge product
and dp denotes the vector of differentials,
dω
dt
= dpt ∧ dq + dp ∧ dqt
= d ˙p ∧ dq + dp ∧ d ˙qt
= − (∇qqH) dq ∧ dq + dp ∧ (∇ppH) dp
= 0 . (15.3)
In the (q, p) phase space of (15.1) q and p evolve in such a way that dω/dt = 0. The
time evolution of (15.1) is then said to be symplectomorphism. Note that dH/dt = 0 is
conservation of energy while dω/dt = 0 is conservation of symplecticity.
Let
H = H(z), z =
 p
q

(15.4)
then
p˙ = −∂H
∂q , q˙ = ∂H
∂p (15.5)
can be rewritten as
Jz˙ = ∇zH(z), J =
 0 −1
1 0 
, z =
 p
q

, (15.6)
where J is skew-symmetric. Equation (15.6) can be generalized to z with n components.
This Hamiltonian system possesses two conservation laws (refer Chapter 14)
dH
dt
= 0, dω
dt
= 0 . (15.7)352 Nonlinear Partial Differential Equations
Let us consider (1+1)-dimensional (one space variable and one-time variable) continuous
system described by H =
 ∞
−∞
H dx, where H is the Hamiltonian density. Suppose that the
system is
utt − uxx + V 
(u)=0 , (15.8)
where V (u) is a smooth function. Define
δH
δφ = ∂H
∂φ − ∂
∂x  ∂
∂φx

+
∂2
∂x2
 ∂
∂φxx 
+ ··· . (15.9)
Equation (15.8) can be rewritten as a system of first-order equations as
ut = v, vt = uxx − V 
(u). (15.10)
One can rewrite the above equation as
ut = δH
δv = v, vt = −δH
δu = uxx − V 
(u), (15.11a)
where
H = 1
2
v2 +
1
2
u2
x + V (u). (15.11b)
In matrix form (15.11) takes the form
 0 −1
1 0  ∂
∂t  u
v

=
 δH/δu
δH/δv 
. (15.12)
Equation (15.11a) has a second-order spatial derivative. Write Eq. (15.8) with variables
containing at most first derivative in x and t. A choice is
−vt − px = V 
(u), ut = v , 0 = p + w, ux = w . (15.13)
That is,


0 −100
1 000
0 000
0 000




ut
vt
wt
pt


+


000 −1
000 0
000 0
100 0




ux
vx
wx
px

 =


V 
(u)
v
p + w
w

 . (15.14)
Defining
M =


0 −100
1 000
0 000
0 000


, K =


000 −1
000 0
000 0
100 0

 , (15.15a)
z =


u
v
w
p


, S(z) = 1
2
v2 +
1
2
w2 + pw + V (u) (15.15b)
Eq. (15.14) becomes
Mzt + Kzx = ∇zS(z). (15.16)Conservation of Symplecticity, Energy and Momentum 353
In the above form each facet of the equation is arranged neatly. The time derivatives occur
in Mzt and the space derivatives occur in Kzx. Equation (15.16) is the generalization of
Eq. (15.6).
Compare Eqs. (15.12) and (15.16). Equation (15.12) is a Hamiltonian formulation of the
system (15.8) on a single symplectic form since in Eq. (15.12) there is only one pre-symplectic
operator  0 −1
1 0 
. The same system is in the multi-symplectic form in Eq. (15.16) as
there are two pre-symplectic operators M and K. Not only the system (15.8), a large class
of PDEs can be written in the form of Eq. (15.16) with M and K being skew-symmetric
n×n matrices with n ≥ 3 [14]. Examples include KdV, sG, Boussinesq and NLS equations.
15.3 Conservation of Symplecticity, Energy and Momentum
The symplectic form for the system (15.13) is ω = dv ∧du. Differentiation of ω with respect
to t gives ∂ω/∂t = dvt ∧ du + dv ∧ dut. Expanding this one obtains
∂ω
∂t = (duxx − V 
(u)du) ∧ du + dv ∧ dv
= duxx ∧ du
= ∂
∂x (dux ∧ du) . (15.17)
Defining w = ux and κ = du∧dw (called flux of symplecticity) the above equation becomes
∂ω
∂t +
∂κ
∂x = 0 . (15.18)
Equation (15.18) is the conservation law of symplecticity and is a generalization of dω/dt =
0 of Eq. (15.6).
15.3.1 Conservation of Multi-Symplectic Structure
For the multi-symplectic form (15.16) of the system (15.8) the multi-symplectic conservation
law is given by Eq. (15.18) with
ω = dz ∧ Mdz
= d


u
v
w
p


∧


0 −100
1 000
0 000
0 000


d


u
v
w
p


= 2dv ∧ du , (15.19)
κ = dz ∧ Kdz
= d


u
v
w
p


∧


000 −1
000 0
000 0
100 0


d


u
v
w
p


= 2dp ∧ du . (15.20)354 Nonlinear Partial Differential Equations
Let us verify Eq. (15.18). Substitute ω = dz ∧ Mdz and κ = dz ∧ Kdz in Eq. (15.18) and
obtain
ωt + κx = dzt ∧ Mdz + dz ∧ Mdzt + dzx ∧ Kdz + dz ∧ Kdzx
= −Mdzt ∧ dz + dz ∧ Mdzt − Kdzx ∧ dz + dz ∧ Kdzx
= −(Mdzt + Kdzx) ∧ dz + dz ∧ (Mdzt + Kdzx)
= −Szzdz ∧ dz + dz ∧ Szzdz
= 0 , (15.21)
where the Leibniz’s rule, skew-symmetry of M and K and the fact that Szz is symmetric are
used. Equation (15.18) implies that at each point (x, t), the multi-symplectic structure is
conserved. ω and κ define the symplectic structures associated with the time and space di￾rections, respectively. One may wish to know whether energy and momentum are conserved
in the multi-symplectic form.
15.3.2 Conservation Law of Energy
To obtain a conservation law of energy take the inner product of (15.16) with zt:
zt,Mzt + Kzx = ∂S
∂t . (15.22)
Because zt,Mzt = 0 the above equation becomes
∂S
∂t − zt,Kzx = 0 (15.23)
or
0 = ∂S
∂t − 1
2
zt,Kzx − 1
2
zt,Kzx
= ∂S
∂t − 1
2
Kzx, zt − 1
2
Kzx, zt
= ∂S
∂t − 1
2
Kzx, zt − 1
2
Kzxt, z − 1
2
Kzx, zt +
1
2
Kzxt, z
= ∂S
∂t − 1
2
∂
∂tKzx, z +
1
2
Kzt, zx +
1
2
Kzxt, z
= ∂S
∂t − 1
2
∂
∂tKzx, z +
1
2
∂
∂xKzt, z. (15.24)
This equation can be rewritten as
Et + Fx = 0 , (15.25a)
where
E(z) = S(z) − 1
2
Kzx, z, (15.25b)
F(z) = 1
2
Kzt, z. (15.25c)
Equation (15.25) is the local conservation law of energy. E is called energy density while F
is the energy flux .Multi-Symplectic Integrator 355
15.3.3 Conservation Law of Momentum
To derive the momentum conservation law let us take the inner product of (15.16) with zx:
zx,Mzt + Kzx = zx, ∇zS. (15.26)
Because zx,Kzx = 0 the above equation becomes
zx,Mzt − ∂S
∂x = 0 . (15.27)
That is,
0 = ∂S
∂x − 1
2
zx,Mzt − 1
2
zx,Mzt
= ∂S
∂x − 1
2
Mzt, zx − 1
2
Mzt, zx − 1
2
Mzxt, z +
1
2
Mzxt, z
= ∂
∂x 
S − 1
2
Mzt, z

+
1
2
∂
∂tMzx, z
= ∂G
∂x +
∂I
∂t , (15.28a)
where
G = S − 1
2
Mzt, z, I = 1
2
Mzx, z. (15.28b)
Equation (15.28) is the local conservation law of momentum.
15.3.4 Splitting of the Matrices M and K
Let us split the matrices M and K such that [15]
M = M+ + M−, K = K+ + K− (15.29a)
with
MT
+ = −M−, KT
+ = −K− . (15.29b)
In this case, one can easily verify the following:
dz ∧ M+dz = dz ∧ M−dz, dz ∧ K+dz = dz ∧ K−dz, (15.30a)
ω = dz ∧ M+dz, κ = dz ∧ K+dz, (15.30b)
E(z) = S(z) − K+zx, z, F(z) = K+zt, z, (15.30c)
G(z) = S(z) − M+zt, z, I(z) = M+zx, z. (15.30d)
The multi-symplectic PDE (15.16) takes the form
M+δtz + M−δtz + K+δxz + K−δxz = ∇zS(z). (15.31)
15.4 Multi-Symplectic Integrator
A multi-symplectic integrator is a numerical scheme that preserves a discrete version of
conservation of symplecticity. Bridges and Reich [7] applied a symplectic integrator to the
independent variables x and t and obtained a multi-symplectic integrator.356 Nonlinear Partial Differential Equations
For the purpose of numerically solving a given PDE with two independent variables x
and t consider xmin ≤ x ≤ xmax, t0 ≤ t ≤ tend. Discretize the space and time variables
with number of grid points as
xj = xmin + jh, j = 0, 1,...,Nx (15.32a)
tn = t0 + nk, n = 0, 1,...,Nt (15.32b)
where
h = (xmax − xmin)/Nx, k = (tend − t0)/Nt . (15.32c)
h and k are step sizes along space and time directions, respectively. We denote the numerical
approximation of z at (xj , tn) as zn
j . The forward- and backward-difference formulas for zt
are given by
δ+
t zn
j = 1
k

zn+1
j − zn
j

, δ−
t zn
j = 1
k

zn
j − zn−1
j

. (15.33)
Similarly, for zx
δ+
x zn
j = 1
h

zn
j+1 − zn
j

, δ−
x zn
j = 1
h

zn
j − zn
j−1

. (15.34)
Let us discretize Eq. (15.31). Apply the two-point forward-difference formula for δtz in
the first term and the two-point backward-difference formula for δtz in the second term.
Use similar formulas for the spatial derivative in the third and fourth terms. The result is
M+δ+
t zn
j + M−δ−
t zn
j + K+δ+
x zn
j + K−δ−
x zn
j = ∇zS(zn
j ). (15.35)
This method is referred as the Euler box scheme.
Example:
Show that the scheme (15.35) satisfies a discrete multi-symplectic conservation law.
Start with the variational equation of (15.35) given by
M+δ+
t dzn
j + M−δ−
t dzn
j + K+δ+
x dzn
j + K−δ−
x dzn
j = ∇zzS(zn
i )dzn
j . (15.36)
The wedge product of dzn
j with (15.36) gives, using the result dzn
j ∧ Szzdzn
j = 0,
0=dzn
j ∧ M+δ+
t dzn
j + dzn
j ∧ M−δ−
t dzn
j
+dzn
j ∧ K+δ+
x dzn
j + dzn
j ∧ K−δ−
x dzn
j
= dzn
j ∧ M+δ+
t dzn
j + δ−
t dzn
j ∧ M+dzn
j
+dzn
j ∧ K+δ+
x dzn
j + δ−
x dzn
j ∧ K+dzn
j
= δ+
t

dzn−1
j ∧ M+dzn
j

+ δ−
x

dzn
j−1 ∧ K+dzn
j

= δ+
t ωn
j + δ−
x κn
j , (15.37a)
where
ωn
j = dzn−1
j ∧ M+dzn
j , κn
j = dzn
j−1 ∧ K+dzn
j . (15.37b)
Through the backward-error analysis, it has been shown that [15] if a multi-symplectic
integration scheme is applied to a Hamiltonian PDE, the resulting modified equation is again
Hamiltonian. The next two sections apply the multi-symplectic scheme to two ubiquitous
NLPDEs.Korteweg–de Vries Equation 357
15.5 Korteweg–de Vries Equation
The KdV equation is
ut + µuux + δuxxx = 0 . (15.38)
It is a simple nonlinear dispersive wave equation illustrating the combined effect of dispersion
and nonlinearity. It describes one-dimensional water wave propagation in a shallow channel.
The KdV equation also plays an important role in the plasma physics (ion acoustic waves)
and in solid state physics. The KdV and family of KdV equations model internal surface
wave in the Andaman Sea between Thailand and Sumatra, acoustic waves in liquids with
gas bubbles (bubbly liquids), Great Red Spot (GRS) and certain features in the Jovian
atmosphere, generation and interaction of ion-acoustic waves in a cold plasma, acoustic
waves on a crystal lattice and so on.
It represents the starting point to investigate soliton behaviour, that is, unscattered
(or elastic) interaction of solitary waves (localized, single-hump and bell-shaped waves)
solutions [1–3]. The term uux in Eq. (15.38) is the nonlinear term.
15.5.1 A Multi-Symplectic Scheme
An equivalent system of first-order equations of the KdV equation is
1
2
ut + wx = 0 , (15.39a)
−1
2
φt − δvx = −w +
1
2
µu2 , (15.39b)
δux = v , (15.39c)
−φx = −u . (15.39d)
The KdV Eq. (15.39) written in the form of (15.16) has
M =


0 1/200
−1/2 000
0 000
0 000


, K =


00 01
0 0 −δ 0
0 δ 0 0
−10 00

 , (15.40a)
and
z =


φ
u
v
w

 , (15.40b)
S(z) = 1
2
v2 − uw +
1
6
µu3 . (15.40c)
The multi-symplectic conservation law
∂ω
∂t +
∂κ
∂x = ∂t [dz ∧ Mdz] + ∂x [dz ∧ Kdz]=0 (15.41)
becomes
∂t (dφ ∧ du)+2∂x (dφ ∧ dw + δdv ∧ du)=0 . (15.42)358 Nonlinear Partial Differential Equations
Splitting M and K as
M+ =


0 1/200
0 000
0 000
0 000


, M− =


0000
−1/2000
0000
0000

 , (15.43a)
K+ =


00 01
0 0 −δ 0
00 00
00 00


, K− =


0000
0000
0 δ 0 0
−1000


(15.43b)
and applying the Euler box scheme (15.35) give
1
2k

un+1
j − un
j

+
1
n

wn
j+1 − wn
j
 = 0 , (15.44a)
− 1
2k

φn
j − φn−1
j

− δ
h

vn
j+1 − vn
j
 = −wn
j + µ
2

un
j
2 , (15.44b)
δ
h

un
j − un
j−1
 = vn
j , (15.44c)
− 1
h

φn
j − φn
j−1
 = −hun
j . (15.44d)
Using Eq. (15.44b) for wn
j and wn
j+1 in Eq. (15.44a) and then eliminating v’s and φ’s using
Eqs. (15.44c) and (15.44d), respectively, lead to the multi-symplectic FD scheme
1
2
un+1
j − un
j
k +
un
j+1 − un−1
j+1
k

+ µ
un
j+1 + un
j
2
 un
j+1 − un
j
h

+δ
un
j+2 − 3un
j+1 + 3un
j − un
j−1
h3

= 0 . (15.45)
Comparing this equation with the KdV Eq. (15.38) observe that at the grid point (xj , tn),
the last term in (15.45) is an FD formula for uxxx, (un
j+1 − un
j )/h is the forward-difference
formula for ux and (un
j+1 + un
j )/2 is the approximation of u. The first term in (15.45) is the
discrete version of ut. It is the average of ut at (j, n) and (j + 1, n). ut at (j, n)th grid point
is approximated by the forward-difference formula while that at (j + 1, n)th grid point is
approximated by the backward-difference formula.
Equation (15.45) can be rewritten as
un+1
j = un
j + un−1
j+1 − un
j+1 − µk
h

un
j+1 + un
j
 un
j+1 − un
j

−2δk
h3

un
j+2 − 3un
j+1 + 3un
j − un
j−1

. (15.46)
To compute un+1
j , the values of un−1
j+1 , un
j−1, un
j , un
j+1 and un
j+2 are to be used. This is
depicted in Fig. 15.1. Call (15.46) as a five-point formula. In Eq. (15.46) n = 1, 2,...,Nt
and j = 1, 2,...,Nx − 2 (refer Eq. (15.32)).
15.5.2 Numerical Implementation of the Scheme
When n = 1 the formula (15.46) takes the form
u2
j = u1
j + u0
j+1 − u1
j+1 − µk
h

u1
j+1 + u1
j
 u1
j+1 − u1
j

−2δk
h3

u1
j+2 − 3u1
j+1 + 3u1
j − u1
j−1

. (15.47)Korteweg–de Vries Equation 359
j-1 j j+1
n-1
n+1
 n
uj
n+1
x
t
j+2
FIGURE 15.1
The grid points involved (marked by solid circles) to compute the numerical solution of
KdV equation at the grid point (j, n + 1) (marked by an open circle).
To find u2
j the values of u at t = t0 and t = t0 + k are needed. u(x, t0) is given by the
initial condition. u(x, t0 + k), that is, u1
j is need to be known. To compute u1
j use the
forward-difference formula for ut:
ut = 1
k

un+1
j − un
j

. (15.48)
Then, the KdV Eq. (15.38) gives
un+1
j = un
j − µk
2h

un
j+1 + un
j
 un
j+1 − un
j

− δk
h3

un
j+2 − 3un
j+1 + 3un
j − un
j−1

. (15.49)
When n = 0 the above equation becomes
u1
j = u0
j − µk
2h

u0
j+1 + u0
j
 u0
j+1 − u0
j

− δk
h3

u0
j+2 − 3u0
j+1 + 3u0
j − u0
j−1

. (15.50)
To determine u2
j (Eq. (15.47)) first generate u0
j from the given initial condition and using
it compute u1
j from (15.50) with
u1
0 = u0
0, u1
Nx−1 = u0
Nx−1, u1
Nx = u0
Nx . (15.51)
For un+1
j with n > 1 assume that
un+1
0 = un
0 , un+1
Nx−1 = un
Nx−1, un+1
Nx = un
Nx . (15.52)
15.5.3 Stability Analysis
The stability condition for the five-point scheme can be obtained. Substitution of u = eijαhξn
(as done for LPDEs in the previous chapter) in Eq. (15.46) with (un
j+1 + un
j )/2 = u gives
ξ2 − aξ − e−iαh = 0 , (15.53a)
where
a = 1 − eiαh + µku
h

1 − eiαh
− 2δk
h3

3+ei2αh − 3eiαh − e−iαh
. (15.53b)360 Nonlinear Partial Differential Equations
unstable
stable
h
k
c
0 0.1 0.2
0.002
0.001
0
FIGURE 15.2
Plot of kc versus h for the KdV equation corresponding to the two-solitary wave solution.
The roots of (15.53a) are
ξ± = 1
2

a ± 
a2 + 4e−iαh
. (15.54)
Then, the stability condition, |Reξ±| ≤ 1, gives [16]
k
h

µ|umax| + 4 δ
h2

≤ 1 , (15.55)
where |umax| is the maximum of u. That is,
k ≤ kc = h
µ|umax| + 4(δ/h2)
. (15.56)
15.5.4 Interaction of Solitary Waves
The KdV equation admits N-solitary wave solutions and the analytical expressions for them
are well-known in the literature [1–3]. Solve the KdV equation to simulate a two-solitary
wave solution. For this consider the initial-value problem, that is, there is no boundary
condition.
For µ = 6 and δ = 1 a two-solitary wave solution is given by
u(x, t) = 1
2

k2
2 − k2
1
 k2
2cosh2
η1 + k2
1sinh2
η2
k2coshη1coshη2 − k1sinhη1sinhη2
, (15.57a)
where
η1 = k1x − k3
1t + η10 , η2 = k2x − k3
2t + η20 . (15.57b)
In (15.57) k1, k2, η10 and η20 are constants and k1, k2 = 0. Choose k1 = 0.2, k2 = √3 k1,
η10 = η20 = 0, x ∈ [−150, 150] with h = 0.2 and the initial value of time t0 as −900. The
maximum value of u, umax = 0.06. Figure 15.2 shows the plot of kc versus h. The scheme
will be stable for the values of h and k in the region marked as stable. Next, verify the
numerical solution and the stability condition.
For the choice k = 0.001 the stability determining quantity ∆ = |Reξ±| = 0.5018 < 1.
Figure 15.3a shows the initial wave solution (t = −900). There are two well-separated soli￾tary waves of different amplitudes. Figures 15.3b-d depict the numerical and exact solutionsKorteweg–de Vries Equation 361
t = −900
(a)
u
0.06
0.03
0
t = −150
(b)
x
u
-150 0 150
0.06
0.03
0
t = 150
(c)
u
0.06
0.03
0
t = 600
(d)
x
u
-150 0 150
0.06
0.03
0
FIGURE 15.3
The numerically computed two-solitary wave solution (continuous line) at a few values of
t illustrating elastic collision. The exact analytical solution is marked by solid circles at
selected values of x. Here, h = 0.2 and k = 0.001.
for three values of t showing the interaction of the two waves. Numerical solution coincides
with the exact solution for large t also. Let us explain Fig. 15.3. As the initial wave profile
evolves as per the KdV equation, after a sufficient time the waves overlap and interact. The
taller wave catches up the shorter wave. This is shown in Fig. 15.3b. At t = 0 both the
waves merge together. However, the two waves reappear for t > 0 (Figs. 15.3c and 15.3d).
The taller wave separates from the shorter one, after overtaking it, and asymptotically (as
t → ∞) the two waves regain the initial shape and hence their speeds. The interaction
between the two solitary waves is elastic. Such solitary waves are called solitons [1–3].
What is the effect of k? For h = 0.2 and k = 0.0025 > kc the stability determining
quantity ∆ = 1.2545. The method is thus unstable. Figure 15.4 shows the numerical solution.
The two-solitary wave pattern of the solution is destroyed. The amplitude of the numerical
solution grows with time.
15.5.5 Efficiency of the Five-Point Scheme
An important criterion of judging a numerical scheme is to check its ability to preserving
constants of motion or conservation laws. The integrable solitonic equations such as KdV
and sG equations possess infinite number of conservation laws. The numerical compuation
of first few conservation laws can be used to analyze the efficiency of a numerical scheme.
Example:
The first three conservation laws of the KdV equation are [1]
I1(t) =  ∞
−∞
u dx, I2(t) = 1
2
 ∞
−∞
u2 dx (15.58a)362 Nonlinear Partial Differential Equations
t = −900 (a)
u
0.06
0.03
0
t = −899.9 (b)
u
0.1
0.05
0
t = −899.89 (c)
u
1.5
0
-1.5
t = −899.88 (d)
x
u
-200 -100 0 100
80
0
-80
FIGURE 15.4
The numerically computed two-solitary wave solution.
and
I3(t) =  ∞
−∞ 1
2
δu2
x − 1
6
µu3

dx . (15.58b)
I1, I2 and I3 are momentum, energy and Hamiltonian, respectively, of the wave solution of
the KdV equation. The discrete versions of them are
I1 = h

Nx
j=1
un
j , I2 = h
2

Nx
j=1

un
j
2 , (15.59a)
I3 = h

Nx
j=1
 δ
8h2

un
j+1 − un
j−1
2 − µ
6

un
j
3

. (15.59b)
Apart from I1, I2 and I3 consider the maximum error in u, ume(t), given by
ume(t) = max.|unum(x, t) − uexact(x, t)| . (15.60)
Compute the percentage of relative errors in the above-mentioned four quantities over a
long time. The result is displayed in Fig. 15.5. The errors in all these quantities are < 0.2%.
The errors in I2, I3 and ume are very small far before and far after the interaction of the
solitary waves (t = 0 is the time at which two waves merge together and form a single
solitary wave).
Can one use the five-point formula to study the evolution of an arbitrary wave profile?
To answer this in the next subsection solve the KdV equation with periodic boundary
condition.Korteweg–de Vries Equation 363
(a)
t
% of R.E. in
I1
-1000 0 1000
0.006
0.003
0
(b)
t
% of R.E. in
I2
-1000 0 1000
0.006
0.003
0
(c)
t
% of R.E. in
I3
-1000 0 1000
0.009
0.006
0.003
0
(d)
t
% of R.E. in
ume
-1000 0 1000
0.15
0.1
0.05
0
FIGURE 15.5
Variations of the percentage of relative errors in the constants of motion I1, I2 and I3 and
the percentage of maximum error in u.
15.5.6 Korteweg–de Vries Equation with Periodic Boundary Condition
The KdV Eq. (15.38) was first numerically solved by Zabusky and Kruskal (ZK) [4] using
the FD formula
un+1
j = un−1
j − µk
3

un
j+1 + un
j + un
j−1
 un
j+1 − un
j−1

−δk
h3

un
j+2 − 2un
j+1 + 2un
j−1 − un
j−2

, (15.61)
where ut and ux are approximated by two-point central-difference formulas while uxxx is
approximated by a four-point central-difference formula. The following example shows that
the above scheme is numerically unstable while the multi-symplectic scheme (15.46) is stable
over a long time.
Example:
Fix the parameters in the KdV equation as µ = 1 and δ = 0.0036 and choose
u(x, 0) = cos πx, 0 ≤ x ≤ 2, h = 0.01, k = 0.00001, u(x + 2, t) = u(x, t). (15.62)
Figure 15.6 shows the outcomes of the ZK scheme. The initial wave profile evolves into four
solitary waves. This is clearly evident in Fig. 15.6b for t = 4. As t increases the scheme
becomes unstable and u(x, t) grows with time and wiggles are generated. These are clearly
seen in Figs. 15.6c and d for t = 12.65 and 12.6574, respectively.
The result of the multi-symplectic five-point scheme (15.46) is presented in Fig. 15.7.
At t = 4 observe four solitary waves with distinct amplitudes. In Figs. 15.7c and d for
large values of t the solution is bounded and there are four solitary waves. Compute the364 Nonlinear Partial Differential Equations
(a) t = 0
u
3
2
1
0
-1
(b) t = 4
u
3
2
1
0
-1
(c) t = 16.65
x
u
0 1 2
25
0
-25
(d) t = 16.6574
x
u
0 1 2
500
250
0
FIGURE 15.6
The numerically computed solutions of the KdV equation using the ZK scheme with the
initial wave profile as u(x, 0) = cos πx, 0 ≤ x ≤ 2 with the periodic boundary condition
u(x + 2, t) = u(x, t).
conserved quantities I1 − I3 over a long time interval and calculate their mean values. The
percentage of relative errors in these quantities compared to their mean values are plotted
in Fig. 15.8. The error in I1 is ≈ 0. The errors in I2 and I3 are < 0.5% and 5%, respectively.
Figure 15.9 shows the numerical solution for u(x, 0) = 2x sin πx. The initial wave evolves
into a five-solitary wave solution. Different number of solitary waves can be generated with
different initial conditions.
15.6 Sine-Gordon Equation
Another interesting NLPDE exhibiting soliton solutions is the sG equation
utt − uxx + sin u = 0 . (15.63)
The sG equation arises in classical mechanics and electronics. It models the dynamics of
a one-dimensional chain of identical pendula connected by a torsion bar and propagation
(a) t = 0
u
3
2
1
0
-1
(b) t = 4
u
3
2
1
0
-1
(c) t = 101.65
x
u
0 1 2
3
2
1
0
-1
(d) t = 200.95
x
u
0 1 2
3
2
1
0
-1
FIGURE 15.7
The numerically computed solutions of the KdV equation using the multi-symplectic scheme
(15.46) with the initial wave profile as u(x, 0) = cos πx, 0 ≤ x ≤ 2 with the periodic
boundary condition u(x + 2, t) = u(x, t).Sine-Gordon Equation 365
(a)
t
% of R.E. in I1
100 110 120
0.016
0.015
0.014
(b)
t
% of R.E. in I2
100 110 120
0.4
0.3
0.2
0.1
(c)
t
% of R.E. in I3 100 110 120
4
2
0
FIGURE 15.8
Variations of the percentage of relative errors in the constants of motion I1, I2 and I3
computed using the numerical solution generated by the multi-symplectic five-point scheme
for the KdV equation subjected to the periodic boundary condition u(x + 2, t) = u(x, t).
of transverse electromagnetic waves on a superconducting strip-line transmission system.
The sG equation is used to model propagation of dislocations in crystals, waves along
lipid membranes, wave propagation along the arrays of Josephson junctions, strain waves,
periodicity of episodic tremor and slow slip events and redistribution and migration of
stresses in the lithosphere.
15.6.1 Multi-Symplectic Four-Point Scheme
In order to express the sG equation in the multi-symplectic form (15.16) write it as a system
of first-order equation by introducing additional variables. An example is
−vt − px = sin u , (15.64a)
ut = v , (15.64b)
p + w = 0 , (15.64c)
ux = w . (15.64d)
(a) t = 11
x
u
0 1 2
3
0
-3
(b) t = 20
x
u
0 1 2
3
0
-3
FIGURE 15.9
The numerically computed solution of the KdV equation using the multi-symplectic scheme
(15.46) with the initial wave profile as u(x, 0) = 2x sin πx, 0 ≤ x ≤ 2 with the periodic
boundary condition u(x + 2, t) = u(x, t).366 Nonlinear Partial Differential Equations
The above equation is in the multi-symplectic form (15.16) with
M =


0 −100
1 000
0 000
0 000


, K =


000 −1
000 0
000 0
100 0

 , (15.65a)
and
z =


u
v
w
p

 , (15.65b)
S(z) = 1
2
v2 +
1
2
w2 + pw + 1 − cos u . (15.65c)
M and K can be splitted into M+, M−, K+ and K− with MT
+ = −M− and KT
+ = −K− as
M+ =


0 −100
0 000
0 000
0 000


, M− =


0000
1000
0000
0000

 , (15.66a)
K+ =


000 −1
000 0
000 0
000 0


, K− =


0000
0000
0000
1000

 . (15.66b)
The Euler box scheme (15.35) gives
−1
k

vn+1
j − vn
j

− 1
h

wn
j+1 − wn
j
 = sin un
j , (15.67a)
1
k

un
j − un−1
j
 = vn
j , (15.67b)
0 = pn
j + wn
j , (15.67c)
1
h

un
j − un
j−1
 = wn
j . (15.67d)
Elimination of vn
j , vn+1
j , wn
j and wn+1
j in Eq. (15.67a) using the subequations leads to the
explicit five-point scheme
1
k2

un+1
j − 2un
j + un−1
j

− 1
h2

un
j+1 − 2un
j + un
j−1

+ sin un
j = 0 . (15.68)
Note that the first and the second terms in the above equation are the three-point central￾difference formulas for utt and uxx, respectively. Rewrite Eq. (15.68) as
un+1
j = 2un
j − un−1
j +
k2
h2

un
j+1 − 2un
j + un
j−1

− k2 sin un
j . (15.69)
The solution at the grid points (j, n−1), (j −1, n), (j, n) and (j + 1, n) are used to compute
the solution at the grid point (j, n + 1) and is depicted in Fig. 15.10.Sine-Gordon Equation 367
j-1 j j+1
n-1
n+1
 n
uj
n+1
x
t
FIGURE 15.10
The grid points involved (marked by solid circles) to compute the numerical solution of the
sG equation at the grid point (j, n + 1) (marked by a open circle).
15.6.2 Numerical Implementation of the Scheme
In order to use the formula (15.69) the solution at n = 0 and n = 1 are needed. The
solution at n = 0, that is, at t = 0, is given by the initial condition u(x, 0). The sG equation
is second-order in time derivative. Therefore, to solve it the initial condition u(x, t0) = f(x)
and ut(x, t0) = g(x) must be specified, where t0 is the initial time. To calculate ui,1, one
can make use of the initial condition on ut.
Using the central-difference formula for the first-derivative the initial condition
ut(x, t0) = g(x) is written as
ut(x, t0) = 1
2k

un+1
j − un−1
j

|n=0 = g(xj ) = gj . (15.70)
That is,
u−1
j = u1
j − 2kgj . (15.71)
Equation (15.69) for n = 0 is
u1
j = 2u0
j − u−1
j − k2
h2

u0
j+1 − 2u0
j − u0
j−1

− k2 sin u0
j . (15.72)
Replacing u−1
j by Eq. (15.71) gives
u1
j = u0
j + kgj − k2
2h2

u0
j+1 − 2u0
j − u0
j−1

− k2
2
sin u0
j . (15.73)
Equations (15.69) and (15.73) constitute the FD method for solving the sG equation sub￾jected to the initial conditions u(x, t0) = f(x) and ut(x, t0) = g(x).
15.6.3 Stability Condition
To obtain the stability condition on k and h substitute un
j = eiαjhξn in (15.69). When
the initial disturbance is small then the term k2 sin un
j is very small and hence it can be
neglected. Then, obtain
ξ2 + 2ξ

−1 + k2
h2 − k2
h2 cos αh
+1=0 . (15.74)
The condition for |Reξ±| ≤ 1 is 2−(k2/h2)(1−cos αh) ≥ 0. That is, 1−(k2/h2) sin2(αh/2) ≥
0. Thus, the required condition for stability is k ≤ h.368 Nonlinear Partial Differential Equations
(a) t = 100
t = 0
x
u
0 50 100
6
3
0 (b)
t
LOG10(
%R.E. in
E )
0 100 200
-6
-7
-8
FIGURE 15.11
The numerically predicted kink solution of the sG equation at t = 100 (continuous curve).
The solid circles represent the analytical solution at some selected values of x.
15.6.4 Numerical Simulation of Different Soliton Solutions
The sG equation admits a variety of exact analytical solution. For example, seeking the
travelling wave solution u(x, t) = u(x − ct) = u(ξ) the sG equation becomes

1 − c2
uξξ − sin u = 0 . (15.75)
Integrating (15.75) in the standard way one can obtain an elliptic function solution. Choos￾ing the integration constant suitably so that u → 0 (mod 2π) and uξ → 0 as ξ → ±∞, the
travelling wave solution can be obtained. Its form is [1]
u(x, t) = 4 tan−1

exp ± (x − ct − x0)
√1 − c2
 . (15.76)
Fix xmin = −10, xmax = 100, h = 0.01, k = h/2, c = 0.75 and x0 = 0. The initial wave
solution is chosen as the solution (15.76) with ‘+’ sign and t = 0. Figure 15.11a shows the
initial solution and the numerical solution at t = 100. The solution represents a monotonic
change in the value of u from 0 to 2π as x increases from −∞ to ∞. Even though it is not a
true pulse, the changes in its amplitude occur in a localized region and hence the solution is
called a kink solution. (The solution (15.76) with the ‘−’ sign corresponds to a monotonic
decrease of u from the value 2π at x = −∞ to 0 at x = ∞ and is known as an antikink).
The kink moves in the forward direction with the velocity c.
Example:
Check the accuracy of the multi-symplectic scheme for the sG equation.
Numerically compute the energy of the system given by
E = 1
2
 ∞
−∞

u2
t + u2
x + 2(1 − cos u)

dx (15.77a)
with
ut = 1
2k

un+1
j − un−1
j

, ux = 1
2h

un
j+1 − un
j−1

. (15.77b)
The exact energy of the kink solution is 12.09482. In Fig. 15.11b the percentage of relative
error in E exhibits a damped oscillation. The error is < 10−6.Sine-Gordon Equation 369
(a)t = −30
u
7
0
-7
(b)t = −15
x
u
-60 60300-30
7
0
-7
(c)t = 0
u
7
0
-7
(d)t = 50
x
u
-60 60300-30
7
0
-7
FIGURE 15.12
(a) The two-kink solution chosen as the initial wave profile of the sG equation. (b-d) The
numerically computed two-kink solution of the sG equation. The continuous curve is the
numerical solution. The solid circles represent the analytical solution at some selected values
of x.
Next, consider the two-kink interaction. A kink of velocity +c colliding at the origin
with a kink of velocity −c is given by [1]
u(x, t) = 4 tan−1

c sinh(x/√1 − c2 )
cosh(ct/√1 − c2 )

. (15.78)
Fix c = 0.75, xmin = −60, xmax = 60 and the initial time as −30. For t  0 the solution
represents a well separated two kinks (see Fig. 15.12a). In Figs. 15.12b-d both numerical
and exact solutions are plotted for a few values of t. As t increases the left(right)-side kink
moves towards right(left)-side. The two kinks interact with each other. At t = 0, the two
kinks appear as a single kink since the centre of them coincide. As t increases further the
two kinks re-emerge. The kink which is confined to the range [−2π, 0] for t < 0 becomes
the kink with u ∈ [0, 2π].
Figure 15.13 presents the numerical simulation of kink-antikink interaction. The exact
solution of the kink-antikink is [1]
u(x, t) = 4 tan−1

sinh(ct/√
1 − c2 )
ccosh(x/√1 − c2 )

. (15.79)
Figure 15.14 shows the change in the energy with time.
Another class of a localized solution of the sG equation is the so-called breather solution
[1]. This kind of solution can be obtained by the method of separation of variables. The
breather solution is given by
u(x, t) = 4 tan−1
 β sin ωt
ω coshβ(x − x0)

, (15.80)
where β = √
1 − ω2 and ω and x0 are constants. The initial conditions associated with the
breather soliton (15.80) with x0 = 0 are
u(x, 0) = 0 , ut(x, 0) = 4βsechx . (15.81)370 Nonlinear Partial Differential Equations
AK K
0 −1
−10
−30
(a)
u
0
-3
-6
K AK
0 1
10
30 (b)
x
u
-50 -25 0 25 50
6
3
0
FIGURE 15.13
The numerically computed kink-antikink interaction. The continuous curve is the numerical
solution. The solid circles represent the analytical solution at some selected values of x. The
wave profile at t = −30 is the initial condition with c = 0.75.
Fix β = 0.75 and ω = 1 − β2 . The numerical solution is depicted in Fig. 15.15 for various
values of t. In Fig. 15.15a as t increases the flat initial solution evolves into a bell shape
wave pattern. The amplitude of the wave increases with time and reaches a maximum at
t = π/(2ω). When t increases further the amplitude begins to decrease and at t = π/ω the
wave flattens. For t ∈ [2π/ω, 3π/(2ω)] the amplitude increases but u(x, t) < 0 (Fig. 15.15c).
From t = 3π/(2ω) the amplitude decreases and u(x, t) = 0 at t = 2π/ω. The above type
of evolution repeats in the interval 2π/ω to 4π/ω and so on. This wave solution is different
from the other types of waves such as solitary waves and kinks. The centre of the solitary
waves of the KdV equation and the kinks of the sG equation propagates in space, that is
the centre moves along the direction of propagation, and the amplitude remains the same.
t
LOG10(
% of R.E. in
E)
-30 0 30
0
-3
-6
-9
FIGURE 15.14
Variation of the error in the energy of the numerically computed kink-antikink solution of
the sG equation.Sine-Gordon Equation 371
t = 0.15, 0.45, 0.9, 2.3
Bottom to top
(a)
x
u
-10 0 10
4
2
0
t = 5.0, 5.3, 5.8, 7.2
Top to bottom
(c)
x
u
-10 0 10
0
-2
-4
t = 7.2, 8.6, 9.0, 9.4
Bottom to top
(d)
x
u
-10 0 10
0
-2
-4
t = 2.3, 3.7, 4.2, 4.5
Top to bottom
(b)
x
u
-10 0 10
4
2
0
FIGURE 15.15
The numerically computed breather solutions of the sG equation at various values of t.
In contrast, the centre of the wave in Fig. 15.16 remains unchanged while the amplitude
oscillates with time. Because the wave pattern resembles the breathing of our heart it is
called a breather. In Fig. 15.16, the percentage of relative error in E oscillates with t and it
is less than 0.1%.
t
% of R.E. in
E
150 160 170 180 190 200
0.1
0.05
0
FIGURE 15.16
Variation of the error in the energy of the numerically computed breather solution of the
sG equation.372 Nonlinear Partial Differential Equations
15.7 Concluding Remarks
One can extend the concept of multi-symplectic scheme to a higher space dimension. For
L-space dimensions Eq. (15.16) takes the form
Mzt +
L
l=1
Klzxl = ∇zS(z). (15.82)
This chapter is restricted to the Euler box scheme. An application of the implicit midpoint
scheme to (15.16) gives the Preissman scheme
Mδ+
t zn
j+ 1
2
+ Kδ+
x z
n+ 1
2
j = ∇zS

x
n+ 1
2
j+ 1
2

. (15.83)
For a detailed discussion on Preissman scheme one can refer to [7,17].
Various conservation properties of the methods preserving a multi-symplectic conser￾vation law have been investigated. It has been shown that Gauss–Legendre Runge–Kutta
methods preserve energy and momentum laws for linear equations [18]. Multi-symplectic
formulation for fluid dynamics using inverse map [19], conservation of wave action [20], non￾linear Schr¨odinger equation with Runge–Kutta Nystrom methods [21], phase space structure
preservation [22], Maxwell’s equation [23] and wave equations [24] have been reported. Back￾ward error analysis for multi-symplectic methods [15] has been introduced. Multi-symplectic
theory is extended for damped Hamiltonian PDEs [25] also.
Detailed numerical investigations of the nonlinear equations considered in this chapter
in the presence of linear and nonlinear dampings, external periodic driving force and weak
noise can be treated as project works and may give rise to interesting results.
15.8 Bibliography
[1] M. Lakshmanan and S. Rajasekar, Nonlinear Dynamics: Integrability, Chaos and
Patterns. Springer, Berlin, 2003.
[2] M.J. Ablowitz, P.A. Clarkson, Solitons, Nonlinear Evolution Equations and In￾verse Scattering. Cambridge University Press, Cambridge, 1992.
[3] J. Hoppe, Lectures on Integrable Systems. Springer, Berlin, 1992.
[4] N.J. Zabusky and M.D. Kruskal, Phys. Rev. Lett. 15:240, 1965.
[5] R.I. McLachlan, Numer. Meth. 6:465, 1994.
[6] J.E. Marsden, G.P. Patrick and S. Shkoller, Commun. Math. Phys. 199:351, 1998.
[7] T.J. Bridges and S. Reich, Phys. Lett. A 284:184, 2001.
[8] T.J. Bridges and S. Reich, J. Phys. A: Math. Gen. 39:5287, 2006.
[9] E. Tadmor, Bull. Am. Math. Soc. 47:507, 2012.
[10] X. Feng, R. Glowinski and M. Neilan, SIAM Review 55:205, 2013.
[11] S. Mashayekhi, Numerical Methods for Nonlinear PDEs in Finance, Ph.D. Thesis,
Univesity of Copenhagen, 2015.
[12] M. Neilan, A. Salgado and W. Zhang, Acta Numerica 26:137, 2017.Problems 373
[13] M. Ratas, A. Salupere and J. Majak, Math. Model. Anal. 26:147, 2021.
[14] T.J. Bridges, Math. Proc. Camb. Phil. Soc. 121:147, 1997.
[15] B. Moore and S. Reich, Numer. Math. 95:625, 2003.
[16] W.H. Ping, W.Y. Shun and H.Y. Ying, Chin. Phys. Lett. 25:2335, 2008.
[17] B.E. Moore and S. Reich, Future Generation Computer Systems 19:395, 2003.
[18] S. Reich, J. Comput. Phys. 157:473, 2000.
[19] C.J. Cotter, D.D. Holm and P.E. Hydon, Proc. Roy. Soc. A 463:2671, 2007.
[20] J. Frank, J. Phys. A: Math. Gen. 39:5479, 2006.
[21] J. Hong, X. Liu and C. Li, J. Comput. Phys. 226:1968, 2007.
[22] A.L. Islas and C.M. Schober, J. Comput. Phys. 197:585, 2004.
[23] L. Kong, J. Hong and J. Zhang, J. Comput. Phys. 229:4259, 2010.
[24] H. Liu and Z. Zhang, IMA J. Numer. Anal. 26:252, 2006.
[25] B.E. Moore, L. Norena and C.M. Schober, J. Comput. Phys. 231:214, 2013.
15.9 Problems
15.1 For the system (15.13) with
M+ =


0 −100
0 000
0 000
0 000


and K+ =


000 −1
000 0
000 0
000 0


show that ω = du ∧ dv and κ = du ∧ dp. Also, verify the energy conservation law
(15.18).
15.2 Obtain a semi-discrete equation of Mzt+Kzx = ∇zS(z) by applying a symplectic
Euler discretization in space. Then, taking inner product of it with zn
t show
that the discretization in space yields the conservation law of energy of the form
∂tEj + δ+
x Fj = 0 and find Ej and Fj .
15.3 Write the perturbed sG equation
utt − uxx + sin u = −(aut + b cos ωt + c cos kx)
in the symplectic form Mzt + Kzx = ∇zS(z) + F(z, x, t).
15.4 An exact travelling wave solution of the fifth-order KdV equation
ut + uux + uxxx − uxxxxx = 0
is [D. Kaya and K. Al-Khaled, Phys. Lett. A 363:433, 2007]
u = − 72
169 +
105
169
sech4
 1
2
√13 
x +
36
169
t
 .
Express the fifth-order KdV equation as a system of first-order equation in the
form of Mzt + Kzx = ∇zS(z) and then obtain a multi-symplectic scheme.374 Nonlinear Partial Differential Equations
15.5 The nonlinear Schr¨odinger equation iψt + ψxx + 2|ψ|
2ψ = 0 admits the solitary
wave solution
ψ = 2βexp 
−2i 
αx + 2(α2 − β2)t + αδ1

×sech [2β(x + 4αt + δ2)] ,
where α, β, δ1 and δ2 are constants. Letting ψ = p + iq and then introducing
two new variables v = qx and w = px express the nonlinear Schr¨odinger equation
in the multi-symplectic form Mzt + Kzx = ∇zS(z). Determine E, F, G and I
in Eqs. (15.25) and (15.28). Develop a multi-symplectic scheme and study its
applicability by simulating the above given solitary wave.
15.6 Show that the Boussinesq equation utt −uxx −(u2)xx −uxxxx = 0 can be written
in the multi-symplectic form Mzt + Kzx = ∇zS(z) [H. Wei-Peng and D. Zi￾Chen, Appl. Math. Mech. 29:927, 2008]. Also, prove that the conservation law of
symplecticity, local energy conservation law and local momentum conservation
law are
∂t(du ∧ dw) + ∂x(dv ∧ du + dw ∧ dp)=0 , ∂te + ∂xf = 0 ,
1
2
∂t(u∂xw − w∂xu) + ∂x

S(z) − 1
2
u∂tw − w∂tu

= 0,
respectively, where v = ux, p = wx, e = S(z) − 1
2 zx,Kz and f = 1
2 z,Kzt.
15.7 For the Ito type coupled KdV equation [Y. Chen, S. Song and H. Zhu, Appl.
Math. Comp. 218, 5552 (2012)]
ut − 6uux − 2vvx − γuxxx = 0 , vt − 2uvx = 0
with z = [φ, ψ, u, v, w, p, q]
T establish that the conservation laws are ωt + κx = 0,
Et + Fx = 0 and It + Gx = 0, where
ω = 1
2
(dφ ∧ du + dψ ∧ dv), κ = dp ∧ dφ + dq ∧ dψ − dw ∧ du ,
E = −u3 + uv2 − 1
2
u2
x , F = −pφt − qψt + wut ,
G = S(z) − 1
2
(uφt + vψt), I = 1
2

u2 + v2
.
15.8 Develop a multi-symplectic FD scheme for the modified KdV equation
ut + αu2ux + δ2uxxx = 0 .
Study its applicability by numerically simulating the two-soliton solution
u(x, t) = 2 [k1cosh (η2 + A12/2) + k2cosh (η1 + A21/2)]
α1 [coshs1 + α2coshs2 + α3] ,
where
A12 = log α2
1
4k2
2
, A21 = log α2
1
4k2
1
,
α1 = k1 − k2
k1 + k2
, α2 =
k1 + k2
k1 − k2
2
, α3 = 4k1k2
(k1 + k2)2 ,Problems 375
and
s1 =

η1 + η2 +
R4
2

, s2 =

η1 − η2 +
R1
2 − R2
2

,
R1 = log 1
4k2
1
, R2 = log 1
4k2
2
, R4 = log α4
1
16k2
1k2
2
,
η1 = k1x − k3
1t + η10 , η2 = k2x − k3
2t + η20 .
15.9 The Burgers equation modelling fluid turbulence in a channel, unidirectional
sound waves in a gas and shock waves in real fluids is given by ut+uux−uxx = 0.
A shock wave solution of it is [P. Sachdev, Nonlinear Diffusive Waves. Cambridge
University Press, Cambridge, 1987; W. Malfliet, Am. J. Phys. 60:650, 1992]
u(x, t) = c

1 − tanh  c
2
(x − ct)
 ,
where c is the velocity of the wave. Develop a multi-symplectic scheme and using
it study the accuracy of the method for the initial wave profile corresponding to
the above given shock wave solution.
15.10 The Fisher equation ut − u(1 − u) − uxx = 0 is used to model a one-dimensional
habitat, neutron population in a nuclear reaction, chemical kinetics and so on
[E. Infeld and G. Rowlands, Nonlinear Waves, Solitons and Chaos. Cambridge
University Press, Cambridge, 1990; P. Gray and S. Scott, Chemical Oscillations
and Instabilities. Clarendon, Oxford, 1990]. The shock wave solution of it is given
by
u(x, t) = 1
4

1 − tanh  1
2
√6

x − 5
√6
t
2
.
Construct a multi-symplectic scheme and analyze its accuracy for the initial wave
profile corresponding to the above given shock wave solution.16
Fractional Order Ordinary Differential Equations
16.1 Introduction
In 1965 L’Hopital asked a question about differentiation of order 1/2 to Leibniz. The reply
was ‘This is an apparent paradox from which, one day, useful consequences will be drawn’
[1]. In the following centuries, considerable progress has been made on fractional calculus
of derivatives and integrals. Fractional derivatives are nonlocal operators and appropriate
for the description of the presence of hereditary and memory properties in physical pro￾cesses and in certain materials [2]. In Chapters 8 and 10, the fractional order derivatives
and integrals, respectively, are introduced and methods to evaluate them numerically are
presented.
In the past few decades, fractional calculus has found notable applications in physics
and engineering, particularly, in signal processing and control engineering [3,4], electromag￾netism [5,6], viscoelasticity [7,8], biology [9], electrochemistry [10], statistical mechanics [11],
diffusion process [12-19], relaxation oscillation [20], fluid flow [21,22], heat conduction [23]
and nanotechnology [24]. One may realize fractional order multiploes in electromagnetism
[5]. Fractional order models have been proposed for the dynamics of premotor neurons in
the vestibulo-ocular reflex system [25]. Electrical circuits may have fractance which cor￾responds to a circuit element with fractional order impedance. Fractional order capacitor
is also realized. Certain fractional-order circuits are analyzed [26-34]. For a review on the
applications of fractional calculus to science and engineering refer to the ref. [35].
Consider the fractional differential equations of the form
Dαx(t) = f(x(t), t), α> 0. (16.1)
It is desired to find the solution of (16.1) for, say, t ∈ [0, T]. How many initial conditions
are to be specified for the initial-value problem of (16.1)? The number of initial conditions
needed is m = [α] + 1 where [α] is the integer part of α. For 0 <α< 1 the value of m is 1,
that is, one initial condition is sufficient. For α > 0, the initial conditions are
x(k)
(0) = x(k)
0 , k = 0, 1,...,m − 1 (16.2)
and are assumed to be given. Exact solution is not known for most of the fractional order
differential equations. Some of the methods for finding approximate solutions are Adomian
decomposition, homotopy, variational and perturbation analysis and generalized differen￾tial transform. Approximate analytical solutions are found for certain linear and nonlinear
fractional differential equations, for example see the refs. [36-42]. Several numerical meth￾ods are developed and analyzed [43-71] for fractional order ordinary differential equations.
Note that as the fractional integral is nonlocal, the numerical integration of factional or￾der differential equations is expensive taking into the account of the memory needed and
computational effect.
DOI: 10.1201/9781032649931-16 376Fractional Order Differential Operators 377
The present chapter is devoted to the numerically solving the fractional order ordinary
differential equations. Particularly, the following four methods are considered:
1. Backward-Difference methods.
2. Fractional Euler method.
3. Adams–Bashforth–Moulton method.
4. A Two-Step Adams–Bashforth method.
First, fractional order differential operators are introduced. Next, the above four methods
are presented. For each method, the formula for the numerical integration of differential
equation of the form (16.1) is developed. The methods are applied to one or two linear
differential equations for which exact solutions are known. The accuracies of the methods are
discussed by comparing the numerical solution with the exact solution. Then, applicability
of the fractional Euler method to certain nonlinear differential equations is demonstrated
by verifying the qualitative behaviour of numerical solution with the results of equilibrium
point analysis. The occurrence of period-doubling route to a complicated dynamics called
chaotic behaviour in the ubiquitous Duffing oscillator is illustrated by solving the system
by the fractional Euler method.
16.2 Fractional Order Differential Operators
In this section, certain definitions of fractional derivatives are reviewed.
1. Gr¨unwald–Letnikov Operator
The Gr¨unwald and Letnikov (GL) fractional derivative of order α (α > 0) is defined as
[51,72-75]
Dα
a f(t) = limh→0
∆α
hf(t)
hα = lim
h→0
1
hα
n
k=0
(−1)k
 α
k

f(t − kh), (16.3)
where a ≤ t ≤ b, h = (t − a)/n and ∆α
hf(t) is a fractional formulation of a backward￾difference for an arbitrary function f(t). In the above equation
 α
k

= α!
k!(α − k)! = Γ(α + 1)
Γ(k + 1)Γ(α − k + 1) and  α
0

= 1, (16.4)
where Γ(γ) is the gamma function with argument γ and is given by
Γ(γ) =  ∞
0
zγ−1e−zdz . (16.5)
Dα
a is the left-GL derivative. The right-GL derivative of f(t) is
Dα
b f(t) = limh→0
1
hα
n
k=0
(−1)k
 α
k

f(t + kh), (16.6)
where h = (b − t)/n.378 Fractional Order Ordinary Differential Equations
An important result of the GL derivative is
Dα
a f(t) =
m
−1
k=0
f(k)
(a)
Γ(k + 1 − α)
(t − a)
k−a
+
1
Γ(m − α)
 t
a
(t − τ )
m−1−αf(m)
(τ )dτ , (16.7)
where m is the integer with m − 1 < α ≤ m. m = [α] + 1 with [α] being the integer part of
α. That is, m is greater than α but the smallest integer. For 0 <α< 1, the value of m is
1. For simplicity, the left-fractional derivative is called fractional derivative.
For Dαf(tj ) if the limit h → 0 is not performed then the result is the finite GL operator
Dαf(tj ) = 1
hα

j
k=0
(−1)k
 α
k

f(tj − kh), j = 0, 1, . . . , n. (16.8)
2. Riemann–Liouville Operator
The Riemann–Liouville (RL) integral operator of order α > 0 is given by
Iαf(t) = D−αf(t) = 1
Γ(α)
 t
0
(t − τ )
α−1f(τ )dτ , (16.9)
where the integral is pointwise defined on [0,∞]. The RL fractional derivative operator Dα,
α > 0 is defined as [76-78]
Dαf(t) = dm
dtm Im−αf(t)
= 1
Γ(m − α)
dm
dtm
 t
0
(t − τ )
m−α−1f(τ )dτ , (16.10)
where m is the integer with m − 1 < α ≤ m. One can write
DαIαf(t) = f(t) (16.11)
and
IαDαf(t) = f(t) −
m
−1
k=0
f(k)
(0+)
t
k
k!
, t> 0 . (16.12)
3. Caputo Fractional Derivatives
For a function f(t), the Caputo type fractional derivative of order α > 0 is defined as [76-82]
Dαf(t) = 1
Γ(m − α)
 t
0
(t − τ )
m−α−1f(m)
(τ )dτ , (16.13)
where again m is the integer with m−1 < α ≤ m. This operator was introduced by Michele
Caputo in his study on the theory of viscoelasticity [79]. Dαf(t) can also be defined by
Dαf(t) = Im−αf(m)
(t), (16.14)Backward-Difference Methods 379
where f(m) is the ordinary mth derivative of f and I is the RL integral operator of order
β = m − α > 0.
The Caputo–Fabrizio fractional derivative of f is
Dαf(t) = M(α)
1 − α
 t
0
f
(τ ) exp 
−α(t − τ )
1 − α

dτ , (16.15)
where M(α) is the normalization function with M(0) = M(1) = 1 [83]. Certain other types
of fractional derivatives are Hadamard [84], Riesz [85], Sonin–Letnikov [85] and Miller–Ross
[84].
For a suitable smooth f(t), t ∈ [a, b], m − 1 <α<m with m being a positive integer
RLDαf(t) = GLDαf(t) and
RLDαf(t) = CDαf(t) +
m
−1
k=0
f(k)
(0)t
k−α
Γ(k + 1 − α)
, (16.16)
where CDα is the Caputo fractional derivative operator.
16.3 Backward-Difference Methods
This section set up the numerical algorithms [51] for the equations of the form given in (16.1)
treating Dα as the GL derivative. The formulas are obtained for x(0) = 0 and x(0) = 0.
16.3.1 The Algorithms
Consider the fractional differential equations of the form
Dαx(t) = f(x(t), t), x(0) = 0, 0 <α< 1 , (16.17)
where Dα is the GL differential operator. The case x(0) = 0 will be considered later. It is
desired to find the solution x(t) for t ∈ [0, T]. The interval [0, T] is discretized with step
size h with n + 1 mesh points given by tm = mh, m = 0, 1,...,n, where h = T /n. The
solution x(t) at t = tm is denoted as x(tm) or simply xm and f(xm, tm) is denoted as fm.
In the limit of h → 0, Dαx(t) is given by Eq. (16.3) with f replaced by x. Relaxing the
limit h → 0 and for finite value of h, Dαx is written as
Dαx(tm) = 1
hα
m
k=0
ωkx(tm − kh), m = 0, 1, . . . , n, (16.18a)
where
ωk = (−1)k
 α
k

=

1 − 1 + α
k

ωk−1, ω0 = 1. (16.18b)
For α = 0.5, ωk values are ω0 = 1, ω1 = −0.5, ω2 = −0.125, ω3 = −0.0625, ..., ω100 =
−0.00028, ..., ω1000 = −0.0000089, ... . Substitution of Eq. (16.18a) in Eq. (16.17) gives
1
hα
m
k=0
ωkxm−k = fm . (16.19)380 Fractional Order Ordinary Differential Equations
Separating the case k = 0 out of the summation, the above equation can be rewritten as
1
hα xm +
1
hα
m
k=1
ωkxm−k = fm . (16.20)
That is,
xm = hαfm −m
k=1
ωkxm−k, m = 1, 2,...,n (16.21)
with x0 = 0. This is called backward-difference algorithm [51]. If the dependent variable x
occurs explicitly in f(x(t), t) then the first term in the right-side of Eq. (16.21) also contains
xm. For f containing the linear term −cx
xm = hα(−cxm) + hαf(tm) −m
k=1
ωkxm−k, m = 1, 2, . . . , n. (16.22)
xm is then written as
xm = 1
1 + chα

hαf(tm) −m
k=1
ωkxm−k

, m = 1, 2, . . . , n. (16.23)
For the case of f being nonlinear in x, each subequation in Eq. (16.21) becomes nonlinear
in xm. They can be solved by applying the fixed point method. With x0 = 0, the solution
x1, x2, ..., xn can be computed successively from Eq. (16.21). This formula is a first-order
numerical method.
Next, in each step introduce an additional starting weight ω
m given by [51]
ω
m = 1
mαΓ(m − α) −m
k=0
ωk . (16.24)
This leads to the modification of the formula (16.21) as
xm = hαfm −m
k=1
ωkxm−k − ω
mx0, m = 1, 2, . . . , n, (16.25)
where x0 = x(0). The error in this formula is O(h). For x(0) = 0, the algorithm (16.25) can
be used. For x(0) = 0, this algorithm becomes the one given by (16.21).
16.3.2 Numerical Examples
Let us apply the backward-difference method to a few systems. The first system is
Dαx(t) = f(x, t) = t . (16.26)
It is easy to obtain the exact analytical solution of the above equation. Fractional integration
of Eq. (16.26) is written as IαDαx(t) = Iαt. This gives
x(t) −
m

−1
k=0
x(k)
(0+)
t
k
k! = 1
Γ(α)
 t
0
(t − τ )
α−1τdτ , (16.27)Backward-Difference Methods 381
TABLE 16.1
xn, xe, |xe −xn| of Eq.(16.26) at certain values of t computed using the backward-difference
algorithm (Eq. (16.21)) with h = 0.001 and x(0) = 0.
t xn xe |xe − xn| t xn xe |xe − xn|
0.1 0.02388 0.02379 0.00009 0.6 0.34983 0.34962 0.00021
0.2 0.06741 0.06728 0.00013 0.7 0.44080 0.44057 0.00023
0.3 0.12376 0.12361 0.00015 0.8 0.53852 0.53827 0.00025
0.4 0.19048 0.19031 0.00017 0.9 0.64255 0.64228 0.00027
0.5 0.26616 0.26596 0.00020 1.0 0.75253 0.75225 0.00028
where m = [α] + 1. For 0 <α< 1, the value of m is 1 and integrating the integral in the
above equation by parts once results in
x(t) − x(0) = 1
Γ(α + 2)t
α+1 . (16.28)
The exact solution of a linear fractional order differential equation with constant coefficients
can be determined by applying the Laplace transform (see Problem 4 at the end of the
present chapter). For α = 0.5, the exact solution after substituting Γ(2.5) = 1.5×0.5×Γ(0.5)
and Γ(0.5) = √π is
x(t) = x(0) + 1
Γ(2.5)t
3/2 = x(0) + 4
3
√π t
3/2 . (16.29)
First choose x(0) = 0 for which the formula (16.21) is desired. Assume that the solution is
to be determined for t ∈ [0, 1].
For the given equation f is independent of x. Hence, in (16.21) fm is simply f(tm) and
the right-side of it not contain xm. For m = 1, 2, 3 the formula (16.21) gives
x1 = hαf(t1) − ω1x0 , (16.30a)
x2 = hαf(t2) − ω1x1 − ω2x0 , (16.30b)
x3 = hαf(t3) − ω1x2 − ω2x1 − ω3x0 . (16.30c)
The solution is computed with h = 0.001. Table 16.1 presents the numerically computed
solution xn, the exact solution xe and the absolute difference between them, |xe − xn|, at
t = 0.1, 0.2, ..., 1.0.
Next, consider Eq. (16.26) with x(0) = x0 = 1. The relevant backward-difference formula
is (16.25). For each mesh point or node tm the quantity ω
m to be calculated that involves
Γ(m − α) . To compute it for 0 <α< 1 define g(1) = Γ(1 − α) = Γ(¯α) with
Γ(¯α) =  ∞
0
zα¯−1e−zdz . (16.31)
Then, to find Γ(m − α), consider the relations
g(2) = Γ(2 − α) = (1 − α)Γ(1 − α) = (1 − α)g(1), (16.32a)
g(3) = Γ(3 − α) = (2 − α)(1 − α)Γ(1 − α) = (2 − α)g(2). (16.32b)382 Fractional Order Ordinary Differential Equations
α¯ = 0.25
α¯ = 0.35
α¯ = 0.50
α¯ = 0.75
log10h
Γ(¯α)
0-1-2-3-4-5-6
4
3
2
1
FIGURE 16.1
The numerically computed Γ( ¯α) versus log10 h for four values of ¯α.
In general, with g(1) = Γ(1 − α),
g(m) = Γ(m − α)=(m − α)g(m − 1), m = 2, 3,... . (16.33)
In this way, Γ(m − α) can be computed. For α = 0.5, the exact value of Γ(¯α) = Γ(0.5) =
√π ≈ 1.77245. For other values of α in the interval [0, 1] the values of the gamma function
need to be evaluated numerically. Can one apply the composite quadrature formulas to
evaluate the gamma function given by Eq. (16.31)? Here 0 <α< 1 for which 0 < α <¯ 1
and ¯α − 1 is negative. The integrand in Eq. (16.31) has a singularity at z = 0. It is easy to
overcome this difficulty. Let us perform integration by parts once and obtain
Γ( ¯α) = 1
α¯
 ∞
0
zα¯e−zdz (16.34)
which has no singularity. The integrand zα¯e−z rapidly decays to zero. So, the integral in
Eq. (16.34) can be evaluated numerically, for example, applying the composite trapezoidal
rule. With the application of this rule, Eq. (16.34) takes the form
Γ(¯α) ≈ h
α¯
∞
k=1
zα¯
k e−zk , (16.35)
where h is the step size in z and zk = kh, k = 1, 2,... . In practice, the summation can be
stopped, for example, when zk > 10 and zα¯
k e−zk < 10−6.
Using Eq. (16.35), the values of the gamma function are computed for a few values of
α¯ for a set of values of h in the interval 
10−1, 10−7
. Figure 16.1 depicts the variation of
Γ(¯α) with log10 h. For each value of ¯α, Γ(¯α) approaches a constant value with h. The values
of Γ( ¯α) obtained are
Γ(0.25) = 3.62560, Γ(0.35) = 2.54614, Γ(0.5) = 1.77245, Γ(0.75) = 1.22541.(16.36)
For further clarity, in Fig. 16.2 log10 E is plotted as a function of log10 h for ¯α = 0.5 where
E is the absolute difference between the exact value of Γ(0.5) = √π and the numerically
computed value of Γ(0.5). For h = 10−4 the error is < 10−5. So, this value of h can be
chosen for the numerical computation of the gamma function. However, before using the
numerically computed value of the gamma function it is necessary to ensure the desired
accuracy in it by suitably choosing the value of h and the tolerance in zα¯
k e−zk .Backward-Difference Methods 383
log10h
log10
E
0-1-2-3-4-5-6
-1
-2
-3
-4
-5
-6
FIGURE 16.2
Variation of the error E in the numerical computation of Γ(¯α) with log10 h for ¯α = 0.5.
In Table 16.2, |xe − xn| for three values of h are presented for selective values of t along
with the exact solution. The error is found to decrease with decrease in the value of h.
In Eq. (16.26) the function f(x(t), t) is independent of x(t). Now, deal with the case of
f being a linear function of x(t). A simple system of this form is
Dαx(t) = f(x(t), t) = −x(t). x(0) = x0, 0 <α< 1 . (16.37)
Its exact solution is [56] (also see Problem 4 at the end of this chapter)
x(t) = Eα(−t
α) = x0
∞
k=0
(−t
α)k
Γ(αk + 1) , (16.38)
where Eα(z) is the Mittag–Leffler function of order α. Rewrite the algorithm (16.25) as
xm = −hαxm + hαf(tm) −m
k=1
ωkxm−k − ω
mx0 . (16.39)
TABLE 16.2
Comparison of |xe −xn| for three values of h at ten values of t for Eq. (16.26) with x(0) = 1.
|xe − xn| for
t xe h = 10−3 h = 10−4 h = 10−5
0.1 1.02379 0.05350 0.01684 0.00532
0.2 1.06728 0.03764 0.01190 0.00376
0.3 1.12361 0.03065 0.00971 0.00307
0.4 1.19031 0.02648 0.00840 0.00266
0.5 1.26596 0.02364 0.00751 0.00238
0.6 1.34962 0.02154 0.00685 0.00217
0.7 1.44057 0.01990 0.00634 0.00201
0.8 1.53827 0.01859 0.00593 0.00188
0.9 1.64228 0.01749 0.00559 0.00177
1.0 1.75225 0.01656 0.00530 0.00168384 Fractional Order Ordinary Differential Equations
TABLE 16.3
The numerically computed solution of Eq. (16.37) with α = 0.5 for three values of h. The
algorithm given by Eq. (16.40) is used to compute xn. The exact solutions xe are also given
for comparison.
xn for
t xe h = 10−3 h = 10−4 h = 10−5
0.1 0.72358 0.69200 0.71360 0.72041
0.2 0.64379 0.62550 0.63798 0.64194
0.3 0.59202 0.57908 0.58790 0.59071
0.4 0.55361 0.54362 0.55042 0.55259
0.5 0.52316 0.51506 0.52057 0.52233
0.6 0.49803 0.49124 0.49586 0.49733
0.7 0.47670 0.47089 0.47484 0.47611
0.8 0.45825 0.45317 0.45662 0.45773
0.9 0.44202 0.43753 0.44058 0.44156
1.0 0.42758 0.42357 0.42630 0.42717
That is,
xm = 1
1 + hα

hαfm −m
k=1
ωkxm−k − ω
mx0

, m = 1, 2, . . . , n, (16.40)
where fm = f(tm). For Eq. (16.37) the term hαfm = 0. For h = 10−3, 10−4 and 10−5
and for α = 0.5 with x0 = 1 the numerically computed solutions xn are given in Table
16.3. The exact solution is also given for comparison. xn approaches the exact solution with
decreasing the value of h.
16.4 Fractional Euler Method
This section develops a generalization of the classical Euler method applicable for fractional
order differential equations. This method has been proposed by Odibat and Momani [56].
16.4.1 Basic Definitions
The RL fractional integral operator Iα of order α > 0 for a function f(t) with t > 0 is given
by Eq. (16.9). For the properties of Iα one may refer to the refs. [76,86,87]. Two notable
properties of Iα and Dα are given by Eqs. (16.11) and (16.12). The RL fractional order
derivative appears as the left-inverse of the RL fractional integral. The Caputo fractional
derivative given by Eq. (16.13) is a modified version of the RL fractional derivative given
by Eq. (16.10). From Eq. (16.12), for 0 <α< 1,
IαDαf(t) = f(t) − f(0+). (16.41)Fractional Euler Method 385
By definition
IαDαf(t) = 1
Γ(α)
 t
0
(t − τ )
α−1Dαf(τ )dτ . (16.42)
According to the intermediate-value theorem for integrals (refer Section 1.6),
 t
0
(t − τ )
α−1Dαf(τ )dτ = Dαf(ξ)
 t
0
(t − τ )
α−1dτ = Dαf(ξ)
t
α
α . (16.43)
Then,
IαDαf(t) = 1
αΓ(α)
Dαf(ξ)t
α = 1
Γ(α + 1)Dαf . (16.44)
Comparison of Eqs. (16.41) and (16.44) gives
f(t) = f(0+) + 1
Γ(α)
Dαf(ξ) · t
α . (16.45)
Denoting Dnα = DαDα ··· Dα (n times) the following two relations can be easily proved
[56]:
InαDnαf(t) − I (n+1)αD(n+1)αf(t) = t
nα
Γ(nα + 1)Dnαf(0+), (16.46)
and
f(t) = n
k=0
t
kα
Γ(kα + 1)Dkαf(0+) + D(n+1)αf(ξ)
Γ[(n + 1)α + 1]t
(n+1)α, 0 ≤ ξ ≤ t. (16.47)
The f(t) given by Eq. (16.47) is the generalized Taylor’s formula.
The modified trapezoidal rule for the numerical computation of Iαf(t) in the interval
t ∈ [0, T] with step size h = T /n and the nodes tm = mh, m = 0, 1,...,n is (refer
Eq. (10.85))
(Iαf(t)) (T) = hα
Γ(2 + α)

(n − 1)α+1 − (n − α − 1)nα

f(0)
+
n
−1
m=1

(n − m + 1)α+1 − 2(n − m)
α+1
+(n − m − 1)α+1
f(tm) + f(T)

. (16.48)
16.4.2 Fractional Euler Formula
The initial-value problem is
Dαx(t) = f(x(t), t), x(0) = x0, 0 < α ≤ 1, t> 0. (16.49)
The solution is to be determined for t ∈ [0, T] at the set of nodes tm = mh, m = 0, 1,...,n,
h = T /n. Assume that x(t), Dαx(t) and D2αx(t) are continuous in the interval t ∈ [0, T].386 Fractional Order Ordinary Differential Equations
Expansion of x(t) as the generalized Taylor’s formula (16.47) about t = t0 = 0 (with n = 1)
is
x(t) = x(t0) + t
α
Γ(α + 1) (Dαx(t)) (t0) + t
2α
Γ(2α + 1)

D2αx(t)

(ξ1), (16.50)
where ξ1 is a value that exists for each value of t. Suppose (Dαx(t))t0 = f(x(t0), t0). Then,
at t = t1 = h, Eq. (16.50) gives
x(t1) = x(t0) + hα
Γ(α + 1)f(x(t0), t0) + h2α
Γ(2α + 1)

D2αx(t1)

(ξ1). (16.51)
For small h, the last term in Eq. (16.51) can be neglected so that
x(t1) = x(t0) + hα
Γ(α + 1)f(x(t0), t0). (16.52)
In general, from tm to tm+1,
xm+1 = xm +
hα
Γ(α + 1)f(xm, tm), (16.53a)
tm+1 = tm + h, m = 0, 1,...,n − 1 . (16.53b)
When α = 1, the above formula reduces to the well-known classical Euler formula.
Let us proceed to develop an algorithm making use of the modified trapezoidal rule and
the formula (16.53) [56]. Write x(t) as
x(t) = Iαf(x(t), t) + x(0). (16.54)
At t = t1,
x(t1)=(Iαf(x(t), t)) (t1) + x(0). (16.55)
The modified trapezoidal rule (16.48) for the interval [t0 = 0, t1 = h] becomes (n = 1)
(Iαf(x(t), t)) (t1) = αhα
Γ(α + 2)f(x(t0), t0) + hα
Γ(α + 2)f(x(t1), t1). (16.56)
Then,
x(t1) = x(0) + αhα
Γ(α + 2)f(x(t0), t0) + hα
Γ(α + 2)f(x(t1), t1). (16.57)
Note that the right-side of Eq. (16.57) also contains x(t1). Therefore, approximate x(t1) in
the right-side of the above equation by Eq. (16.52). This gives
x(t1) = x(0) + αhα
Γ(α + 2)f(x(t0), t0)
+
hα
Γ(α + 2)f

x(t0) + hα
Γ(α + 1)f(x(t0), t0), t1

. (16.58)
Next, generalize the Eq. (16.58). For the interval [t0, tm+1], m = 0, 1, 2,...,n − 1
x(tm+1) = x(0) + hα
Γ(α + 2)

mα+1 − (m − α)(m + 1)α
f(x(t0), t0)
+
hα
Γ(α + 2)
m
k=1

(m − k + 2)α+1 − 2(m − k + 1)α+1
+(m − k)
α+1
f(x(tk), tk)
+
hα
Γ(α + 2)f(x(tm+1), tm+1). (16.59)Fractional Euler Method 387
Define
ak,m+1 =



mα+1 − (m − α)(m + 1)α, if k = 0
(m − k + 2)α+1 − 2(m − k + 1)α+1
+(m − k)α+1, if 1 ≤ k ≤ m
1, if k = m + 1.
(16.60)
Then, the second term in the right-side of (16.60) can be brought to the third term with
the summation running index starting from 0. Thus,
x(tm+1) = x(0) + hα
Γ(α + 2)
m
k=0
ak,m+1f(x(tk), tk)
+
hα
Γ(α + 2)f(x(tm+1), tm+1). (16.61)
In Eq. (16.61) the unknown quantity x(tm+1) occurs on both sides. If f is linear in x then
from Eq. (16.61) an expression for x(tm+1) can be written. In general, for nonlinear function
one cannot solve Eq. (16.61) directly for x(tm+1). In this case, a preliminary approximation
can be sought. Suppose, approximate x(tm+1) by the Euler formula (16.53a). Then,
x(tm+1) = x(0) + hα
Γ(α + 2)
m
k=0
ak,m+1f(x(tk), tk)
+
hα
Γ(α + 2)f

x(tm) + hα
Γ(α + 1)f(x(tm), tm), tm+1
. (16.62)
This formula is known as the fractional Euler method or the fractional Euler algorithm.
Instead of approximating x(tm+1) in the right-side of Eq. (16.61) by the Euler formula
(16.53a) an iterative process can be used. This is considered in the next section.
16.4.3 Examples
Let us apply the fractional Euler method to two equations, one with order α, 0 <α< 1
and the other with order greater than α.
1. Example 1
First, apply the algorithm to the system (16.37) where f is a linear function of x.
Γ(α + 2) is calculated by numerically evaluating the integral in Eq. (16.5) by the composite
trapezoidal rule with step size 0.0001. Table 16.4 shows the numerically computed solution
xn and the exact solution (16.38) for α = 0.5 and 0.75. The step size used is h = 0.001. xn
is close to the exact solution xe. Accuracy can be improved by decreasing the value of h.
2. Example 2
Consider the linear and inhomogeneous equation
D2x + D1.5x + x =1+ t, x(0) = x
(0) = 1 . (16.63)
A. Exact Solution
The exact solution of Eq. (16.63) can be obtained by applying the Laplace transform388 Fractional Order Ordinary Differential Equations
TABLE 16.4
The numerical solution of Eq. (16.37) computed for α = 0.5 and 0.75 with step size h = 0.001
and x(0) = 1 by employing the fractional Euler algorithm.
α = 0.5 α = 0.75
t xn xe xn xe
0.1 0.72399 0.72358 0.82826 0.82825
0.2 0.64412 0.64379 0.73259 0.73259
0.3 0.59229 0.59202 0.66034 0.66034
0.4 0.55384 0.55361 0.60212 0.60212
0.5 0.52336 0.52316 0.55360 0.55360
0.6 0.49821 0.49803 0.51228 0.51229
0.7 0.47687 0.47670 0.47655 0.47656
0.8 0.45840 0.45825 0.44529 0.44529
0.9 0.44216 0.44202 0.41768 0.41768
1.0 0.42771 0.42758 0.39310 0.39311
[88]. The Laplace transform of Caputo fractional derivatives of order α > 0 with m = [α]+1
is
L[Dαf(t)] = L

Im−αf(m)
(t)

= L
f(m)
(t)

sm−α
= smF(s) − sm−1f(0) − ... − f(m−1)(0)
sm−α , (16.64)
where L[f(t)] = F(s). Now, taking the Laplace transform on both sides of Eq. (16.63) gives
s2F(s) − sx(0) − x
(0) + 1
s1/2

s2F(s) − sx(0) − x
(0)
+ F(s) = 1
s
+
1
s2 . (16.65)
Simplifying this equation results in

s2 + s3/2 + 1
F(s) = 
s2 + s3/2 + 1 1
s
+
1
s2

. (16.66)
That is,
F(s) = 1
s
+
1
s2 . (16.67)
Taking the inverse Laplace transform gives the solution as x(t)=1+ t.
B. Equivalent System of Equations
To numerically solve Eq. (16.63) write it in the form of system of equations with each
equation of the form of Dβx = f(x, t), 0 <β< 1. Equation (16.63) is rewritten as
D1/2x1 = x2 , (16.68a)
D1/2x2 = x3 , (16.68b)
D1/2x3 = x4 , (16.68c)
D1/2x4 = −x4 − x1 +1+ t . (16.68d)Fractional Euler Method 389
TABLE 16.5
The variation of |xe − xn| with the step size h for Eq. (16.63). The numerical solution is
computed by the fractional Euler method.
|xe − xn| for
t h = 10−2 h = 10−3 h = 10−4 h = 10−5
0.1 0.00666 0.00079 0.00008 0.00001
0.2 0.00705 0.00079 0.00008 0.00001
0.3 0.00704 0.00077 0.00008 0.00001
0.4 0.00684 0.00073 0.00008 0.00001
0.5 0.00652 0.00069 0.00007 0.00001
0.6 0.00610 0.00064 0.00007 0.00001
0.7 0.00560 0.00058 0.00006 0.00001
0.8 0.00502 0.00051 0.00005 0.00001
0.9 0.00438 0.00044 0.00005 0.00001
1.0 0.00369 0.00036 0.00004 0.00001
It is easy to find the initial conditions xi(0), i = 1, 2, 3, 4 corresponding to the exact ana￾lytical solution x1(t)=1+ t. From this solution,
x1(0) = 1, (16.69a)
x3(0) = Dx1(t)|t=0
= 1 . (16.69b)
Using the definition of Dαf(t), 0 <α< 1
x2 = Dα(1 + t)
= 1
Γ(1 − α)
 t
0
(t − τ )
−αdτ
= 1
Γ(2 − α)
t
1−α, (16.69c)
x4 = D1+α(1 + t)
= Dα1
= 0 . (16.69d)
Thus, x2(0) = 0 and x4(0) = 0.
C. Numerical Solution
With the initial conditions
x1(0) = 1, x2(0) = 0, x3(0) = 1, x4(0) = 0 (16.70)
apply the fractional Euler method to each of the subequations of Eqs. (16.68) and compute
the solutions xi(t), i = 1, 2, 3, 4 for t ∈ [0, 1] with a fixed h. Table 16.5 presents the values
of |xe − xn| for four values of h.390 Fractional Order Ordinary Differential Equations
A fractional order differential equation with the higher order of the derivative greater
than 1 can be numerically solved by equivalently rewriting it as a system of equations with
each subequation with appropriate order in the range 0 <β< 1. Each subequation need
not be of the same fractional order.
16.5 Adams–Bashforth–Moulton Method
Diethelm, Ford and Freed [48] developed an algorithm to solve fractional differential equa￾tions. Their method is a generalization of the Adams–Bashforth–Moulton method of or￾dinary differential equations with integer derivatives and is applicable to a wide class of
fractional differential equations. This method is applied to certain linear and nonlinear
systems [61]. Error analysis on this method is performed and reported in [49,69].
16.5.1 Review of the Classical Method
For the equation Dx(t) = f(x(t), t) with x(0) = x0 divide the time interval [0, T] into n
nodes with step size h = T /n. The nodes are tm = mh, m = 0, 1,...,n. A unique solution is
assumed to exist in the interval [0, T]. From the known x(tm) an approximation of x(tm+1)
is written as
x(tm+1) = x(tm) +  tm+1
tm
f(x(τ ), τ )dτ. (16.71)
Use the trapezoidal rule for the integral in the above equation. Then,
x(tm+1) = x(tm) + h
2 [f(x(tm), tm) + f(x(tm+1), tm+1)] . (16.72)
The unknown quantity x(tm+1) occurs on both sides of this equation. If f is linear in x then
from (16.72) an expression for x(tm+1) can be written. In general, for a nonlinear function
Eq. (16.72) cannot be solved directly for x(tm+1). In this case, an iterative scheme is useful
with an approximation. This approximation is usually termed as predictor and denoted as
xp
m+1. For determining the predictor use the rectangle rule (refer the subsection 10.2.3). The
rectangle rule is
 tm+1
tm
f(x(τ ), τ )dτ = (tm+1 − tm)f(x(tm), tm) = hf(x(tm), tm). (16.73)
Then, Eq. (16.71) gives (with xp(tm+1) for x(tm+1))
xp(tm+1) = x(tm) + hf(x(tm), tm). (16.74)
This is known as the Euler formula or one-step Adams–Bashforth method [90].
With the predictor xp(tm+1), Eq. (16.72) becomes
x(tm+1) = x(tm) + h
2 [f(x(tm), tm) + f(xp(tm+1), tm+1)] . (16.75)
This is the one-step Adams–Bashforth–Moulton method. The order of convergence of this
method is 2.Adams–Bashforth–Moulton Method 391
16.5.2 Construction of the Algorithm
The equation of our interest is
Dαx(t) = f(x(t), t), α> 0 (16.76)
subjected to the initial conditions x(k)
(0) = x(k)
0 , k = 0, 1,..., [α] where [α] is the integer
part of α and x(k) is the ordinary kth derivative of x. Dα is chosen as the Caputo type.
Consideration of the integration of Eq. (16.76) according to Eq. (10.79) gives
x(t) = 
[α]
k=0
x(k)
0
t
k
k!
+
1
Γ(α)
 t
0
(t − τ )
α−1f(x(τ ), τ )dτ . (16.77)
This equation is a second-order Volterra equation.
A. Corrector Formula
For discretized t, Eq. (16.77) takes the form
x(tm+1) = 
[α]
k=0
t
k
m+1
k! x(k)
0 +
 tm+1
0
(tm+1 − τ )
α−1f(x(τ ), τ )dτ . (16.78)
To evaluate the integral in the above equation, the product trapezoidal rule can be applied.
This gives
 tm+1
0
(tm+1 − τ )
α−1f(x(τ ), τ )dτ
≈
 tm+1
0
(tm+1 − τ )
α−1 ˜fm+1(x(τ ), τ )dτ , (16.79)
where ˜fm+1 is the piecewise linear interpolant for f. Then,
 tm+1
0
(tm+1 − τ )
α−1 ˜fm+1dτ
= hα
α(α + 1)
m

+1
k=0
ak,m+1f(x(tk), tk), (16.80a)
where
ak,m+1 =



mα+1 − (m − α)(m + 1)α, if k = 0
(m − k + 2)α+1 − 2(m − k + 1)α+1 + (m − k)α+1, if 1 ≤ k ≤ m
1, if k = m + 1 .
(16.80b)
Thus, the corrector formula is
x(tm+1) =
[α

]−1
k=0
t
k
m+1
k! x(k)
0 +
hα
Γ(α + 2)
m
k=0
ak,m+1f(x(tk), tk)
+
hα
Γ(α + 2)f (xp(tm+1), tm+1) , (16.81)
where α > 0. Compare this formula with the fractional Euler formula (16.62) in the previous
section for the case 0 <α< 1.392 Fractional Order Ordinary Differential Equations
B. Predictor Formula
Next, procced to find an approximation for xp(tm+1). Now, approximate the integral in
Eq. (16.77) by the product rectangle rule
 tm+1
0
(tm+1 − τ )
α−1f(x(τ ), τ )dτ ≈ hα
α
m
k=0
bk,m+1f(x(tk), tk), (16.82a)
where
bk,m+1 = (m + 1 − k)
α − (m − k)
α . (16.82b)
Then, the predicted value xp(tm+1) is given by the fractional Adams–Bashforth method as
xp(tm+1) = 
[α]
k=0
t
k
m+1
k! x(k)
0 +
hα
Γ(α + 1)
m
k=0
bk,m+1f(x(tk), tk). (16.83)
The combined algorithm given by Eqs. (16.81) and (16.83) is called the fractional Adams–
Bashforth–Moulton (FABM) method.
A detailed error analysis has shown that the error behaves as [49]
max|X(tk) − x(tk)| = O(hp), (16.84)
where k = 0, 1, 2,...,n, p = min(2, 1 + α) and X and x denote the exact solution and
numerically predicted solution, respectively.
16.5.3 Example
Let us apply the FABM method to Eq. (16.37). In developing a program to implement the
ABM method, Γ(α+1) can be computed by evaluating the integral in Eq. (16.5) numerically
by the composite trapezoidal rule with step size h = 0.0001. Then, Γ(α+2) can be calculated
using Γ(α+ 2) = (α+ 1)Γ(α+ 1). For 0 <α< 1, [α] = 0 and the first term in the right-side
of Eqs. (16.81) and (16.83) become simply x(0)
0 = x(0) = 1.
The numerical solution is computed for t ∈ [0, 1] for h = 10−2, 10−3 and 10−4 for
α = 0.5. The result is summarized in Table 16.6. The numerically computed solution is
in very good agreement with the exact solution for even h = 10−2. For a value of t, the
numerical solutions computed for the above-mentioned three values of h are all almost the
same. |xe − xn| is computed for α = 0.5, 0.75 and 1 with h = 0.01. Table 16.7 shows the
result. The quantity |xe − xn| is < 0.00003 for α = 0.5 while it is < 0.00002 for α = 0.75
and 1.
Compare the fractional Euler formula (16.62) and the FABM formula given by
Eqs. (16.81) and (16.83). In the fractional Euler method x(tm+1) in the right-side of
Eq. (16.61) is approximated simply by
x(tm)+(hα/Γ(α + 1))f(x(tm), tm), (16.85)
that is, approximated using the value of f at only one node. But in the FABM method,
from Eq. (16.83) note that for the solution at tm+1 the values of f at the nodes t0 to tm are
used. FABM involves more calculations than the fractional Euler method. Comparison of
the errors involved in these two methods for Eq. (16.37) is left as an exercise to the readers.A Two-Step Adams–Bashforth Method 393
TABLE 16.6
The numerical solutions xn of the equation Dαx = −x, x(0) = 1 for α = 0.5 computed by
the Adams–Bashforth–Moulton method with the step sizes h = 10−2, 10−3 and 10−4.
xn for
t xe h = 0.01 h = 0.001 h = 0.0001
0.1 0.72358 0.72354 0.72358 0.72358
0.2 0.64379 0.64380 0.64379 0.64379
0.3 0.59202 0.59205 0.59202 0.59202
0.4 0.55361 0.55364 0.55361 0.55361
0.5 0.52316 0.52319 0.52316 0.52316
0.6 0.49803 0.49806 0.49803 0.49802
0.7 0.47670 0.47674 0.47670 0.47670
0.8 0.45825 0.45828 0.45825 0.45825
0.9 0.44202 0.44205 0.44202 0.44202
1.0 0.42758 0.42761 0.42758 0.42758
16.6 A Two-Step Adams–Bashforth Method
In [63] Gnitchogna and Atangana proposed an algorithm for fractional order partial and
ordinary differential equations and is presented in this section.
16.6.1 Derivation of the Algorithm
Consider the general fractional partial differential equation of the form
Dα
t u(x, t) = ALu(x, t) + ANu(x, t), (16.86)
where AL and AN are a linear operator and a nonlinear operator, respectively, and Dα
t is
the Caputo type fractional partial derivative. Taking the Laplace transform on both sides
TABLE 16.7
The variations of |xe − xn| for the equation Dαx = −x, x(0) = 1 for three values of α. xn
is computed by the Adams–Bashforth–Moulton method with the step size h = 10−2.
|xe − xn| for |xe − xn| for
t α = 0.5 α = 0.75 α = 1.0 t α = 0.5 α = 0.75 α = 1.0
0.1 0.00004 0.00002 0.00000 0.6 0.00003 0.00001 0.00001
0.2 0.00001 0.00001 0.00000 0.7 0.00004 0.00001 0.00000
0.3 0.00003 0.00000 0.00000 0.8 0.00003 0.00002 0.00000
0.4 0.00003 0.00001 0.00000 0.9 0.00003 0.00002 0.00001
0.5 0.00003 0.00001 0.00001 1.0 0.00003 0.00001 0.00001394 Fractional Order Ordinary Differential Equations
of the above equation with respect to the space variable x gives
Dα
t u(t) = L[ALu(x, t) + ANu(x, t)] = F(u, t), (16.87)
where u(t) = u(s, t). Application of the Caputo fractional integral operator on (16.87) gives
u(t) = u(t0) + 1
Γ(α)
 t
0
(t − τ )
α−1F(u, τ )dτ . (16.88)
Discretization of time as t0, t1, ... with step size h leads to the equation
um = u(tm) = u0 +
1
Γ(α)
 tm
0
(tm − τ )
α−1F(u, τ )dτ . (16.89)
Then,
um+1 − um = 1
Γ(α)
 tm+1
0
(tm+1 − τ )
α−1F(u, τ )dτ
− 1
Γ(α)
 tm
0
(tm − τ )
α−1F(u, τ )dτ . (16.90)
Since t is discretized
um+1 − um = 1
Γ(α)
m
k=0
 tk+1
tk
(tm+1 − τ )
α−1Fdτ
− 1
Γ(α)
m
−1
k=0
 tk+1
tk
(tm − τ )
α−1Fdτ
= 1
Γ(α)
(I1 − I2) . (16.91)
To evaluate the integrals I1 and I2, replace F by the Lagrange polynomial approximation
F(u, t) ≈ P(t) = t − tm−1
h Fm +
t − tm
h Fm−1 , (16.92)
where Fm = F(u, tm). The first integral I1 in Eq. (16.91) then becomes
I1 = n
k=0

Fm
h
 tk+1
tk
(tm+1 − t)
α−1(t − tm−1)dt
−Fm−1
h
 tk+1
tk
(tm+1 − t)
α−1(t − tm)dt

. (16.93)
Carrying out the integration, gives
 tk+1
tk
(tm+1 − t)
α−1(t − tm−1)dt
= 1
α + 1

(tm+1 − tk+1)
α+1 − (tm+1 − tk)
α+1
−2h
α [(tm+1 − tk+1)
α − (tm+1 − tk)
α] (16.94)A Two-Step Adams–Bashforth Method 395
and
 tk+1
tk
(tm+1 − t)
α−1(t − tm)dt
= 1
α + 1

(tm+1 − tk+1)
α+1 − (tm+1 − tk)
α+1
−h
α [(tm+1 − tk+1)
α − (tm+1 − tk)
α] . (16.95)
Using Eqs. (16.94) and (16.95) in Eq. (16.93), after simplification results in
I1 = hα

2(m + 1)α
α − (m + 1)α+1
α + 1 
Fm
−hα

(m + 1)α
α − (m + 1)α+1
α + 1 
Fm−1 . (16.96)
Similarly, I2 is worked out as
I2 = hα

mα
α − mα+1
α + 1 
Fm + hα mα+1
α + 1
Fm−1 . (16.97)
Using the obtained expressions of I1 and I2 in Eq. (16.91) leads to
um+1 − um = hα
Γ(α)
2(m + 1)α − mα
α
+
mα+1 − (m + 1)α+1
α + 1 
Fm
−
(m + 1)α
α
+
mα+1 − (m + 1)α+1
α + 1 
Fm−1

. (16.98)
Taking inverse Laplace transform on the above equation gives the solution u(x, t). This
algorithm is called a two-step Laplace Adams–Bashforth algorithm for the partial differential
Eq. (16.86) [63].
For the equation Dαx(t) = f(x(t), t) the left-side of Eq. (16.98) becomes x(tm+1)−x(tm)
and Fm = f(x(tm), t). In this case, the algorithm is termed as a two-step Adam–Bashforth
algorithm. The error in the algorithm is shown to be [63]
|Rα
m| ≤ hα+2
8Γ(α + 1) max 
F(2)(u, ξ)

((m + 1)α + mα) < +∞ , (16.99)
where ξ ∈ (0, tm+1). To find x(tm+1) using Eq. (16.98) the solution x(tm) and x(tm−1) are
required. That is, to find x2(= x(t2)) the solutions x1 and x0 are needed. x0 is the initial
condition. To find x1, the fractional Euler formula
x1 = x0 +
hα
Γ(α)
F(x0, t) (16.100)
can be used. Thus, knowing x0 and x1 the solutions at t2, t3, ..., tm+1 can be computed
employing (16.98).
Treating (16.98) as a predictor, in [68] the following an implicit method with the corrector
has been proposed:
x(tm+1) = x(tm) + hα
Γ(α)

(m + 1)α
α
+
mα+1 − (m + 1)α+1
α + 1 
Fm+1
+
hα
Γ(α)

−mα
α
+ (m + 1)α+1 − mα
α + 1 
Fm . (16.101)396 Fractional Order Ordinary Differential Equations
TABLE 16.8
The numerical solution xn of Dαx(t) = t, x(0) = 0 computed employing the two-step
Adam–Bashforth method with h = 0.001 for α = 0.5. The exact solution xe and the error
in the numerical solution |xe − xn| are also given.
t xn xe |xe − xn| t xn xe |xe − xn|
0.1 1.02376 1.02379 0.00003 0.6 1.34959 1.34962 0.00003
0.2 1.06726 1.06728 0.00002 0.7 1.44054 1.44057 0.00003
0.3 1.12358 1.12361 0.00003 0.8 1.53825 1.53827 0.00002
0.4 1.19028 1.19031 0.00003 0.9 1.64226 1.64228 0.00002
0.5 1.26594 1.26596 0.00002 1.0 1.75223 1.75225 0.00002
16.6.2 Numerical Examples
Consider the system (16.26) with x(0) = 1, α = 0.5 and h = 0.001. It is easy to develop a
C++ or Python code for solving a fractional differential equation using the formula (16.98).
Table 16.8 presents the xn, xe and |xe − xn| for ten values of t. The numerical solution is in
close agreement with the analytical solution.
The second example is the system (16.37). The numerical solutions of this system for
α = 0.5 and 0.75 along with the exact solutions are given in Table 16.9. The step size used
is h = 0.001.
TABLE 16.9
The numerical solution xn of Dαx(t) = −x(t), x(0) = 1 computed using the two-step
Adams–Bashforth algorithm with h = 0.001 for α = 0.5 and 0.75. xe denotes the exact
solution.
α = 0.5 α = 0.75
t xn xe xn xe
0.1 0.73338 0.72358 0.82912 0.82825
0.2 0.64945 0.64379 0.73252 0.73259
0.3 0.59444 0.59202 0.65916 0.66034
0.4 0.55332 0.55361 0.59971 0.60212
0.5 0.52052 0.52316 0.54989 0.55360
0.6 0.49331 0.49803 0.50722 0.51229
0.7 0.47013 0.47670 0.47013 0.47656
0.8 0.45000 0.45825 0.43750 0.44529
0.9 0.43224 0.44202 0.40853 0.41768
1.0 0.41639 0.42758 0.38263 0.39311Nonlinear Fractional Differential Equations 397
16.7 Nonlinear Fractional Differential Equations
So far, the applicability of the numerical methods for fractional differential equations is
discussed by considering certain linear fractional differential equations for which exact solu￾tions are known. It is important to analyze the applicability of these methods to nonlinear
systems. The backward-difference method is difficult to apply for nonlinear systems. The
other methods are suitable for nonlinear systems also. In this section, the behaviours of
certain nonlinear systems are explored by applying the fractional Euler method. For most
of the nonlinear systems, exact analytical solutions are not known, however, useful qualita￾tive behaviours can be extracted applying theoretical approaches. For autonomous systems,
that is equations not having explicit occurrence of the independent variable, time t, equi￾librium point analysis is useful. In this section, linear stability analysis is performed for
two nonlinear systems and the predictions are verified numerically. Then, the periodically
driven fractional Duffing oscillator is numerically solved and the occurrence of interesting
period-doubling route to chaotic dynamics is presented. To start with, let us review the
linear stability analysis [90].
16.7.1 Linear Stability Analysis
Consider a system of two-coupled differential equations, each of order α (0 <α< 1), of the
form
Dαx1 = f1(x1, x2), Dαx2 = f2(x1, x2), (16.102)
where x1 and x2 are functions of time t. The system (16.102) is an autonomous system.
This system can be rewritten as
DαX = F(X), X = (x1, x2)
T ∈ Rn, F = (f1, f2)
T . (16.103)
Any allowed solution of F(X) = 0 is called a fixed point or an equilibrium point of the system
and is denoted as X∗ = (x∗
1, x∗
2). The equilibrium points of (16.102) are thus obtained by
setting f1(x1, x2) = 0 and f2(x1, x2) = 0.
An equilibrium point X∗ is said to be stable if the neighbouring trajectories of it approach
it in the limit of t → ∞. If the neighouring trajectories of X∗ deviate or move away from it
in the long-time limit then X∗ is called as an unstable equilibrium point. If the above two
are not happening then X∗ is neutrally stable.
The stability condition for the fractional order system is determined in [90]. To find the
stability of X∗ let us slightly disturb it and write
x1 = x∗
1 + ξ1(t), x2 = x∗
2 + ξ2(t). (16.104)
Write the Taylor series expansion of f1 and f2 about (x∗
1, x∗
2) as
fi(x∗
1 + ξ1, x∗
2 + ξ2) ≈ fi(x∗
1, x∗
2) + ∂fi
∂x1




x∗
1,x∗
2
· ξ1 + ∂fi
∂x2




x∗
1,x∗
2
· ξ2
+ higher-order terms in (ξ1, ξ2), (16.105)
where i = 1, 2. Substitution of Eqs. (16.104) and (16.105) in Eqs. (16.102) gives
Dαξi(t) = ∂fi
∂x1




x∗
1,x∗
2
· ξ1 + ∂fi
∂x2




x∗
1,x∗
2
· ξ2 , i = 1, 2 (16.106)398 Fractional Order Ordinary Differential Equations
where the higher-order terms in (ξ1, ξ2) are neglected. This system can be rewritten as
Dαξ = Mξ, ξ =
 ξ1
ξ2

, M =
 a b
c d 
(16.107a)
and
a = ∂f1
∂x1




x∗
1,x∗
2
, b = ∂f1
∂x2




x∗
1,x∗
2
, c = ∂f2
∂x1




x∗
1,x∗
2
, d = ∂f2
∂x2




x∗
1,x∗
2
. (16.107b)
For the systems of the form (16.102) with α = 1, a detailed stability analysis predicts
that (x∗
1, x∗
2) becomes asymptotically stable if the real part of both the eigenvalues of the
matrix M are negative [91]. If λ1 and λ2 are the eigenvalues of M, B is the eigenvectors of
M and C =
 λ1 0
0 λ2

then MB = BC and M = BCB−1. Equation (16.107a) becomes,
Dαξ = Mξ = 
BCB−1
ξ, Dα 
B−1ξ

= C 
B−1ξ

. (16.108)
Defining η = B−1ξ =
 η1
η2

, the second subequation in Eqs. (16.108) takes the form
Dαη = Cη. That is,
Dαη1 = λ1η1 , Dαη2 = λ2η2 . (16.109)
This is a linear system and its solution is [92] (also see Problem 4 at the end of the present
chapter)
ηi(t) = ∞
n=1
λn
i t
nα
Γ(nα + 1)ηi(0) = Eα (λit
α) ηi(0), i = 1, 2 (16.110)
where Eα(λ, t) is the Mittag–Leffler function. The asymptotic stability of the system
(16.109) (that is, both |η1| → 0 and |η2| → 0 as t → ∞) has been analyzed by Matignon
[92]. If all the eigenvalues of M satisfy the condition
|arg(λi)| > απ/2 , i = 1, 2 (16.111)
then the solution ηi(t) → 0 as t → ∞. For a detailed proof of this see the ref. [93]. Write an
eigenvalue as λ = λR + iλI. Then, arg (λ = λR + iλI) is given by
arg (λR + iλI) =



tan−1(λI/λR), λR > 0,
tan−1(λI/λR) + π, λR < 0, λI ≥ 0,
tan−1(λI/λR) − π, λR < 0, λI < 0,
π/2, λR = 0, λI > 0,
−π/2, λR = 0, λI < 0,
undefined, λR = 0, λI = 0.
(16.112)
From (16.111) and (16.112) note that, as pointed out in [91], if the real part of both λ1 and
λ2 are negative then (x∗
1, x∗
2) becomes stable.
In general, for DαX = F(X) with X and F having n components, an equilibrium point
of this system is asymptotically stable if
|arg(λi)| > απ/2 , i = 1, 2,...,n. (16.113)
For the system Dαx = f(x) the stability condition for an equilibrium point is f
(x∗) < 0.Nonlinear Fractional Differential Equations 399
α = α =0.6 α =0.5 0.7
α =0.8
α =0.9
α =1
t
x(t) 0 1 2 3 4 5
2
1
0
FIGURE 16.3
The numerically computed solution of the nonlinear system (16.114) by the fractional Euler
method for six values of α with x(0) = 0. All the trajectories approach the equilibrium
point x∗
+ = 2.41421.
16.7.2 Stability Analysis of a System of the Form Dαx = f(x)
Consider the nonlinear system
Dαx = f(x)=1+2x − x2 , 0 < α ≤ 1 . (16.114)
For α = 1, it is an exactly solvable system with the solution (with x(0) = 0)
x(t)=1+ √
2 tanh 
√
2 t +
1
2
ln √2 − 1
√2+1 . (16.115)
The exact analytical solution is difficult to find for the system (16.114). Therefore, numeri￾cally solve it by the fractional Euler method and check whether the qualitative behaviour of
the numerical solution is in accordance with the linear stability analysis of the equilibrium
points of the system.
The equilibrium points of (16.114) are obtained by letting Dαx = 0. This gives two
equilibrium points x∗
± = 1 ± √2 = −0.41421 and 2.41421. Here, for an equilibrium point
to be stable the condition is f
(x∗) < 0. For x+, f
(x∗
+) = −2
√2 < 0. x∗
+ is thus stable.
The trajectories started in the neighbourhood of it asymptotically approach x∗
+. For x∗
−,
f
(x∗
−)=2√2 > 0. x∗
− is an unstable equilibrium point. In the neighbourhood of it the
trajectories diverge. Figure 16.3 shows the numerically computed solution for α = 1, 0.9,
0.8, 0.7, 0.6 and 0.5 with x(0) = 0 and h = 0.001. For each value of α, the solution x(t)
approaches to x∗
+ = 2.41421. The rate of convergence of x(t) depends on α and is clearly
seen in Fig. 16.3.
In Fig. 16.4, x(t) for α = 0.7 is plotted for five different initial values of x. For the first
trajectory from the bottom x(0) = −0.42. This initial condition is left to x∗
− = −0.41421
which is the unstable equilibrium point. Consequently, x(t) starting with x(0) = −0.42
moves away from it. For the second, third and fourth trajectories from bottom in Fig. 16.4
the values of x(0) are right to x∗
− and left to x∗
+. The corresponding trajectories move
away from x∗
− and attracted to the stable equilibrium point x∗
+. The top most trajectory is
corresponding to an initial condition right to x∗
+ and is also approaching x∗
+. The qualitative400 Fractional Order Ordinary Differential Equations
t
x(t)
0 1 2 3 4 5
3
2
1
0
-1
-2
FIGURE 16.4
The variation of x(t) of the system (16.114) for α = 0.7 and for different values of x(0). For
details see the text.
behaviour of the numerical solution of the system (16.114) computed by the fractional Euler
method is in agreement with the linear stability analysis of the equilibrium points of the
system.
16.7.3 A Fractional Order Duffing Oscillator
A prototype nonlinear oscillator is the Duffing oscillator given by [91]
D2x + dDx − x + x3 = f sin ωt. (16.116)
First, consider this system with f = 0. Suppose, a fractional order form of (16.116) is
D2αx + dDαx − x + x3 = 0 , 0 <α< 1 . (16.117)
This equation can be rewritten as
Dαx = y = f1(x, y), (16.118a)
Dαy = −dy + x − x3 = f2(x, y). (16.118b)
This system has three equilibrium points (x∗, y∗) = (0, 0), (±1, 0). The stability determining
eigenvalues are the eigenvalues of the matrix
M =
 ∂f1/∂x ∂f1/∂y
∂f2/∂x ∂f2/∂y  



x∗,y∗
=
 0 1
1 − 3x∗2 −d

. (16.119)
For (x∗, y∗) = (0, 0), the stability determining eigenvalues are
λ± = 1
2

−d ± d2 + 4 
. (16.120)
That is, λ− < 0 while λ+ > 0. The equilibrium point (0, 0) is unstable.Nonlinear Fractional Differential Equations 401
α =0.5
α =0.9
α =1
t
x(t)
302520151050
1.2
0.8
0.4
0
FIGURE 16.5
x(t) with t for the system (16.118) for three values of α. Here, h = 0.001, x(0) = 0.01,
y(0) = 0 and d = 0.3.
For the other two equilibrium points (±1, 0) the matrix M =
 0 1
−2 −d

and the
eigenvalues are
λ± = 1
2

−d ± d2 − 8

. (16.121)
For d2 < 8, λ± are complex conjugate with negative real part. The equilibrium points
are thus stable. In the neighbourhood of each of these equilibrium points the trajectories
spiral around it and reach it in the limit of t → ∞. The corresponding equilibrium point
is called a stable focus. For d2 > 8, the eigenvalues λ± are real negative and distinct and
the equilibrium points are said to be stable nodes. In this case, the trajectories in the
neighbourhood of them approach them along parabolic paths in the x − y phase space.
Equations (16.118) are solved by the fractional Euler method for d = 0.3 with step size
h = 0.001 and with the initial conditions x(0) = 0.01 and y(0) = 0. The chosen (x(0), y(0))
is close to the unstable equilibrium point (0, 0). Numerical solution is obtained for α = 1,
0.7 and 0.5. Figure 16.5 shows the numerical solutions. The numerical solutions x(t) diverge
out of x∗ = 0 and approach the stable focus point x∗ = 1 by exhibiting damped oscillation
as predicted by the linear stability analysis of the equilibrium points.
Next, choose d = 3 for which d2 > 8. The numerically computed solutions for α = 1,
0.9, 0.7 and 0.5 with x(0) = 0.01 and y(0) = 0 are presented in Fig. 16.6. As predicted
by the linear stability analysis the solutions x(t) approach x∗ = 1 along parabolic paths.
The effect of α on the rate of convergence of the solution to x∗ is clearly seen. As the
value of α decreases from 1 the rate of convergence of the solution to x∗ also decreases as
expected. The results in the Figs. 16.5 and 16.6 clearly indicate the very good agreement
of the qualitative behaviour of the numerical solution computed by the fractional Euler
method with the outcome of the linear stability analysis.
16.7.4 A Fractional Order Damped Duffing Oscillator
Suppose the damping term Dx in Eq. (16.116) is fractional order so that the equation is
D2x + dDαx − x + x3 = f sin ωt, α = 3/2 . (16.122)402 Fractional Order Ordinary Differential Equations
α =1
α =0.9
α =0.7
α =0.5
t
x(t)
1007550250 125 150
1
0.5
0
FIGURE 16.6
x(t) with t for the system (16.117) for four values of α. Here, h = 0.001, x(0) = 0.01,
y(0) = 0 and d = 3.
To numerically solve it, rewrite it in the form of (16.1) as
D1/2x1 = x2, (16.123a)
D1/2x2 = x3, (16.123b)
D1/2x3 = x4, (16.123c)
D1/2x4 = −dx4 + x1 − x3
1 + f sin ωt. (16.123d)
Case 1: f = 0.
For f = 0, this equation is an autonomous system and linear stability analysis gives
useful information of the dynamics of the system. A detailed analysis shows that there are
three equilibrium points (x1, x2, x3, x4) = (0, 0, 0, 0) and (x1, x2, x3, x4)=(±1, 0, 0, 0). The
equilibrium point (0, 0, 0, 0) is always unstable. (±1, 0, 0, 0) are stable foci for d2 < 8 and
stable node for d2 > 8. Equations (16.123) are solved by the fractional Euler method with
step size h = 0.001 and for t ∈ [0, 50]. The initial conditions are chosen near the origin
which is the unstable equilibrium point. In Fig. 16.7 two solutions for α = 1 and 0.5 are
plotted in the x1 − x3, that is (x1 − x˙ 1) phase space.
For the initial condition (−0.1, 0, 0, 0) chosen left to the origin the trajectories spiral
about the left equilibrium point (−1, 0, 0, 0) while for the initial condition (0.1, 0, 0, 0) right
to the origin the trajectories approach the right equilibrium point (1, 0, 0, 0). This result is
in accordance with the stability nature of the equilibrium points of the system.
Case 2: f = 0.
Finally, consider the system (16.123) with d = 0.5, ω = 1 and treat f as the control
parameter. For a range of values of f numerical solutions are computed with the initial
condition (1.2, 0, 0, 0) and with the step size in the fractional Euler method as h = 0.005.
For f = 0, as f sin ωt is an explicit time-dependent external forcing term, linear stability
analysis cannot be performed and the solution never ends up on any of the equilibrium
points determined for f = 0. The integer order system (16.116) exhibits a variety of regular
and complex dynamics when the parameter f or ω is varied from a small value. One of
the interesting dynamics is the period doubling route to chaotic dynamics. This nonlinear
dynamics is displayed by the fractional order Duffing oscillator also. Let us illustrate this
dynamics.Nonlinear Fractional Differential Equations 403
x1(t)
x
3(t)
-1 -0.5 10.50
0.5
0.25
0
-0.25
-0.5
FIGURE 16.7
The numerically computed solutions of the system (16.123) for d = 0.5 and for α = 1 and
0.5. The continuous and dashed trajectories represent the solution for α = 1 and α = 0.5,
respectively. Here, (x1(0), x2(0), x3(0), x4(0)) = (±0.1, 0, 0, 0).
In Figs. 16.5-16.7, a trajectory started from an initial condition approaches a stable
equilibrium point X∗ and once it reaches X∗ it remains there. This is the case for various
initial conditions in the neighbourhood of a stable equilibrium point. As a stable equilibrium
point attracts nearby trajectories, it is an attractor and is a point attractor. The part
of a trajectory between the initial state and the stable equilibrium point in Figs. 16.5-
16.7 is termed as transient. In studying the long-time behaviour of a dynamical system
the transient motion has to be discarded. In numerically solving Eq. (16.122), starting
with an initial condition, the solution is computed with the step size h = 0.005 and the
solution corresponding to t < 600 is left as a transient. Then, the nature of the solution
for t ∈ [600, 1000] is plotted in x1 − x3 phase space and the periodicity of the solution is
determined.
A. Period Doubling and Chaotic Dynamics
Figure 16.8 presents the plot of x1 versus x3 for some selected values of f where d = 0.5
and ω = 1. For f = 0.252 the orbit in Fig. 16.8a is closed and is a periodic orbit with period
T = 2π/ω. When f = 0.275 (see Fig. 16.8b) the orbit is also a closed one but the period
of it is 2T, that is, twice the period of the orbit corresponding to f = 0.252. A period-4T
solution is realized for f = 0.282 (Fig. 16.8c).
The orbit shown in Fig. 16.8d is clearly different from the orbits shown in Figs. 16.8a￾c. This is a nonperiodic orbit. An important characteristic property of this solution is
that it is highly sensitive to any small change in the initial condition. Essentially, two
solutions started from two nearby initial conditions diverge exponentially until they become
completely uncorrelated and long time future prediction is highly inaccurate. The distance
between two such trajectories neither decays to zero (as is the case for the periodic motions
shown in Figs. 16.8a-c) nor diverges continuously but varies irregularly with time. The
readers may verify this for the solution shown for f = 0.288. Such a solution is called a
chaotic solution.
For the f values in Figs. 16.8a-d the orbit existing for x1 > 0 is shown. For these values
of f there is another orbit confined to x1 < 0 and is not shown in Fig. 16.8. In Fig. 16.8e the
orbit coexisting with x1 < 0 is shown. As f is increased from 0.288 the two coexisting orbits404 Fractional Order Ordinary Differential Equations
(a) f = 0.252
x
3
1.2510.750.50.25
0.6
0.3
0
-0.3
-0.6
(c) f = 0.282
x
3
1.510.50
0.6
0.3
0
-0.3
-0.6
(e) f = 0.288
x1
x
3
-1.5 -1 -0.5 0
0.6
0.3
0
-0.3
-0.6
(b) f = 0.275
1.510.50
0.6
0.3
0
-0.3
-0.6
(d) f = 0.288
1.510.50
0.6
0.3
0
-0.3
-0.6
(e) f = 0.3
x1
-1.5 0 1.5
0.8
0.4
0
-0.4
-0.8
FIGURE 16.8
The numerically computed solution of the system (16.123) for d = 0.5, ω = 1 and for a few
values of f.
expand and at a value of f merge together and form a single chaotic orbit. An example of
such a chaotic orbit is presented in Fig. 16.8e for f = 0.3.
B. Bifurcation Diagram
In order to understand what is happening when the value of f is varied from 0 the
so-called bifurcation diagram is useful. It can be constructed as described below.
For the system (16.123) for a value of f collect the values of X(t) =
(x1(t), x2(t), x3(t), x4(t)) at integer multiples of t = T = 2π/ω. Leave first, say, 200 val￾ues of X as a transient and consider next 100 values of X. These values of X are called
Poincar´e points. Do this for f values in the interval f ∈ [fmin, fmax] with step size ∆f. For
a period-T orbit one value of X repeats. Similarly, for a period-mT orbits m values of X
repeat. For the nonperiodic solution, like a chaotic solution, the X values are nonrepeat￾ing but bounded. Plot x1 versus f. This plot will describe the qualitative change in the
behaviour of the solution of the system when the parameter f is varied.Concluding Remarks 405
t
x1
0 20 40 60 80
1.5
1
0.5
0
FIGURE 16.9
x(t) versus t for the system (16.123) for f = 0.288.
There is an alternative approach to draw a bifurcation diagram. X(t) oscillates with
time. The local maximum values of X(t) can be used for bifurcation diagram. Consider
the part of the solution for f = 0.288 shown in Fig. 16.9. The solid circles in this figure
represent the local maxima of x1(t). Collect these values of x1(t) after a sufficient transient.
How does one collect such values of x1(t)? When x3(t − h) (that is, ˙x1(t − h) > 0) and
x3(t) < 0 find the value of t = t
 at which x3(t

) = 0. This t
 value can be determined using
the interpolation formula
t
 = t − h − hx3(t − h)
x3(t) − x3(t − h)
. (16.124)
Then, using the values of x1(t−h), x1(t), t−h, t and t
 the value of x1(t

) can be determined
again using the interpolation method. x1(t

) is given by
x1(t

) = xmax
1 (t

) = x1(t − h)+(t
 − t + h)
x1(t) − x1(t − h)
h . (16.125)
For the system (16.123) 200 values of f in the interval f ∈ [0.25, 0.29] with ∆f = 0.0002
are chosen. For each value of f after leaving a sufficient transition 100 values of xmax
1
are collected. Figure 16.10 presents the plot of xmax
1 versus f. For f < 0.26 one value
of xmax
1 is repeating and this correponds to a period-T solution. For 0.26 <f< 0.2776
two values of xmax
1 are repeating. The solution is thus a periodic with period-2T. As f
is further varied, period-4T, 8T, ... solutions are developed. At f ≈ 0.2840 this period
doubling phenomenon accumulates and a chaotic solution is developed. In this case, xmax
1
is nonperiodic. In Fig. 16.10 the region of f where such solutions occur can be clearly seen.
The numerical study of the nonlinear systems considered in this section clearly indicates
the applicability of the fractional Euler method to nonlinear systems.
16.8 Concluding Remarks
Four methods for numerically solving initial-value problem of fractional order ordinary
differential equations are described and their applicability and efficacy are illustrated by406 Fractional Order Ordinary Differential Equations
f
xmax
1
0.25 0.26 0.27 0.28 0.29
1.40
1.35
1.30
1.25
FIGURE 16.10
Bifurcation diagram of the system (16.123) for d = 0.5 and ω = 1.
considering certain linear and nonlinear systems. Among the four methods considered the
fractional Euler method is a better choice. Its applicability is demonstrated by consider￾ing single-order and multi-order systems. Numerical predictions are found to be in close
agreement with the exact analytical solutions and the expected results in the cases of un￾known analytical solutions. Compared to integer order equations numerical computation of
solutions of even very simple fractional order equations involves enormous calculations in
increase in time due to the memory effect.
Methods have been proposed in the mathematics literature to solve time-delayed, dis￾tributed order and stiff fractional order differential equations. Methods for fractional order
systems subjected to boundary conditions are not considered in this book. For some methods
see the refs. [96-100].
16.9 Bibliography
[1] M. Rehman and R.A. Khan, Appl. Math. Model 60:2630, 2011.
[2] Y.A. Rossikhin and M.V. Shitikova, Appl. Mech. Rev. 63, 2010, 52 pages.
[3] F.B.M. Duarte and J.A.T. Machado, Nonl. Dyn. 29:342, 2002.
[4] O.P. Agrawal, Nonl. Dyn. 38:323, 2004.
[5] N. Engheta, IEEE Trans. Ant. Prop. 44:554, 1996.
[6] J.F. Gomez-Aguilar, R.F. Escobar-Jimenez, M.G. Lopez-Lopez and V.M. Alvarado￾Mart´ınez, J. Electro. Waves and Appl. 30:1937, 2016.
[7] C. Lederman, J.M. Roquejoffre and N. Wolanski, Ann. Mat. Pure Appl. 183:173,
2004.
[8] F.C. Meral, T.J. Royston and R.L. Magin, Commun. Nonl. Sci. Num. Simul.
15:939, 2010.
[9] R.L. Magin, Comput. Math. Appl. 59:1586, 2010.Bibliography 407
[10] K.B. Oldham, Adv. Eng. Soft. 41:9, 2010.
[11] F. Mainardi, Fractional Calculus: Some Basic Problems in Continuum and Sta￾tistical Mechanics. Springer, Vienna, 1997.
[12] D.A. Benson, S.W. Wheatcraft and M.M. Meerschaert, Water Resource Research
36:1403, 2000.
[13] Y. Pachepsky, D. Benson and W. Rawls, Soil Sci. Soc. Am. J. 4:1234, 2000.
[14] L. Zhou and H.M. Selim, Soil Sci. Soc. Am. J. 67:1079, 2003.
[15] M.M. Meerschaert and C. Tadjeran, J. Comp. Appl. Math. 172:65, 2004.
[16] S. Shen and F. Liu, ANZIAM J. 46E:C871, 2005.
[17] G. Huang, Q. Huang and H. Zhan, J. Contam. Hydr. 85:53, 2006.
[18] V.G. Ychuk, B. Datsko and V. Meleshko, J. Comput. Appl. Math. 220:215, 2008.
[19] E. Sousa, J. Comp. Phys. 228:4038, 2009.
[20] F. Mainardi, Chaos, Solitons & Fractals 7:1461, 1996.
[21] D.N. Slinn and J.J. Riley, J. Comput. Phys. 144:550, 1998.
[22] A. Atangana, Proceedings of the International Conference on Algebra and Applied
Analysis 6, 2012, 20 pages.
[23] X.J. Yang and D. Baleanu, Thermal Sci. 17:625, 2013.
[24] D. Baleanu, B. Guvenc, J.A. Tenreino-Machado, New Trends in Nanotechnology
and Fractional Calculus Applications. Springer, New York, 2010.
[25] T.J. Anastasio, Biol. Cybernet. 72:69, 1994.
[26] A. Le Mehaute and G. Crepy, Solid State Ionics 9-10:17, 1983.
[27] Y. Jiang, B. Zhang, X. Shu and Z. Wei, J. Adv. Res. 25:217, 2020.
[28] T. Kaczorek and K. Rogowski, Fractional Linear Systems and Electrical Circuits.
Springer, London, 2007.
[29] M. Sivarama Krishna, S. Das, K. Biswas and B. Goswami, IEEE Trans. Elect.
Dev. 58:4067, 2011.
[30] A.G. Radwan, IEEE J. Emer. Sel. Top. Circ. Sys. 3:2156, 2013.
[31] A. Alsaedi, J.J. Nieto and V. Venktesh, Adv. Mech. Eng. 7:1, 2015.
[32] J.I. Hidalgo-Reyes, J.F. Gomez-Aguilar, R.F. Escobar-Jimenez, V.M. Alvarado￾Martinez, M.G. Lopez-Lopez, Int. J. Circuit Theory and Appl. 47:1225, 2019.
[33] M.S. Semary, M. Fouda and A. Radwan, J. Adv. Res. 18:147, 2019.
[34] S. Kapoulea, G. Tsirimokou, C. Psychalinos and A.S. Elwakil, Circ. Syst. Signal
Process, 2019. https://doi.org/10.1007/s00034-019-01252-5.
[35] L. Debnath, IJMMS 54:3413, 2003.
[36] J.H. He, Comput. Meth. Appl. Mech. Eng. 167:57, 1998.
[37] N. Shawagfeh, Appl. Math. Comput. 131:517, 2002.
[38] Z. Odibat and S. Momani, Int. J. Nonlin. Sci. Numer. Simulat. 7:15, 2006.
[39] S. Momani and Z. Odibat, Phys. Lett. A 355:271, 2006.
[40] S. Momani and Z. Odibat, Appl. Math. Comput. 177:488, 2006.
[41] Z. Odibat and S. Momani, Appl. Math. Comput. 181:767, 2006.408 Fractional Order Ordinary Differential Equations
[42] S. Momani, Chaos, Solitons & Fractals 28:930, 2006.
[43] L. Blank, Numerical Analysis Report 287 , Manchester Centre for Computational
Mathematics, Manchester, 1996.
[44] K. Diethelm, Elec. Transact. Numer. Anal. 5:1, 1997.
[45] K. Diethelm and G. Walz, Numer. Algor. 16:231, 1997.
[46] K. Diethelm and A.D. Freed, On the solutions of nonlinear fractional differen￾tial equations used in the modeling of viscoplasticity. In Scientific Computing in
Chemical Engineering II − Computational Fluid Dynamics, Reaction Engineering
and Molecular Properties. F. Keil, W. Mackens, H. Voβ and J. Werther (Eds.).
Springer, Heidelberg, 1999. pages 217-224.
[47] N.J. Ford and A.C. Simpson, Numer. Algor. 26, 333 (2001).
[48] K. Diethelm, N.J. Ford and A.D. Freed, Nonl. Dyn. 29:3, 2002.
[49] K. Diethelm, N.J. Ford and A.D. Freed, Numer. Algor. 36:31, 2004.
[50] A.M.A. El-Sayed, A.E.M. El-Mesiry and H.A.A. El-Saka, Comput. Appl. Math.
23:33, 2004.
[51] M. Weilbeer, Efficient Numerical Methods for Fractional Differential Equations
and Their Analytical Background. Verlag, Nicht, Ermittelbar, 2005.
[52] S. Momani, Math. Comput. Simul. 70:110, 2005.
[53] K. Diethelm, N.J. Ford, A.D. Freed and Y. Luchko, Comput. Meth. Appl. Mech.
Eng. 194:743, 2005.
[54] K. Diethelm, J.M. Ford, N.J. Ford and M. Weilbeer, J. Comput. Appl. Math.
186:482, 2006.
[55] S. Momani and Z. Odibat, Chaos, Solitons & Fractals 31:1248, 2007.
[56] Z.M. Odibat and S. Momani, J. Appl. Math. & Inform. 26:15, 2008.
[57] C. Li and Y. Wang, Comput. Math. Appl. 57:1672, 2009.
[58] J. Peinado, J. Ibanez, E. Arias and V. Hernandez, Comput. Math. Appl. 60:3032,
2010.
[59] S. Esmaeili, M. Shamsi and Y. Luchko, Comput. Math. Appl. 62:918, 2011.
[60] J. Cao and C. Xu, J. Comput. Phys. 238:154, 2013.
[61] H.M. Baskonus and H. Bulut, Open Math. 13:547, 2015.
[62] R.B. Albadarneh, M. Zerqat and I.M. Batiha, Int. J. Pure Appl. Math. 106:859,
2016.
[63] R. Gnitchogna and A. Atangana, Numer. Meth. Part. Diff. Eqs. 34:1739, 2017.
[64] H.F. Ahmed, J. Egypt. Math. Soc. 26:38, 2018.
[65] H. Singh and H.M. Srivastava, Front. Phys. 8, 2020, article number 120.
[66] A. Daraghmeh, N. Qatanani and A. Saadeh, Appl. Math. 11:1100, 2020.
[67] H. Wang, F. Wu and D. Lei, AIMS Math. 6:5596, 2021.
[68] N.A. Zabidi, Z.A. Majid, A. Kilicman and Z.B. Ibrahim, Adv. Cont. Disc. Models
26, 2022; https://doi.org/10.1186/s13662-022-03697-6.
[69] C.W.H. Green and T. Yan, Foundations 2:839, 2022.
[70] M. Harker, Fractional Differential Equations: Numerical Methods for Applica￾tions. Springer, New York, 2022.Bibliography 409
[71] R. Garappa, Mathematics 6:16, 2018.
[72] A.K. Gr¨unwald, Z. Angrew. Math. Phys. 12:441, 1867.
[73] A.V. Letnikov, Mat. Sh. 3:1, 1868.
[74] I. Podlubny, Fractional Differential Equations: An Introduction to Fractional
Derivatives, Fractional Differential Equations, to Methods of Their Solution and
Some of Their Applications. Elsevier, Amsterdam, 1998.
[75] M.D. Ortigueira and F. Coito, Int. J. Theory Appl. 7:459, 2004.
[76] K.B. Oldham and J. Spanier, The Fractional Calculus Theory and Applications of
Differentiation and Integration to Arbitrary Order . Elsevier, Amsterdam, 1974.
[77] S.G. Samko, A.A. Kilbas and O.I. Marichev, Fractional Integrals and Derivatives.
Gordon and Breach Science Publishers, Lausanne, 1993.
[78] K. Diethelm, The Analysis of Fractional Differential Equations: An Application￾Oriented Exposition Using Differential Operators of Caputo Type, Springer,
Berlin, 2010.
[79] M. Caputo, Geophys. J. Int. 13:529, 1967.
[80] C. Li, D. Qian and Y.Q. Chen, Disc. Dyn. Nat. Soc. 2011, article ID 562494;
https://doi.org/10.1155/2011/562494.
[81] R. Hilfer, Threshold introduction to fractional derivatives. In Anomalous Trans￾port: Foundations and Applications. R. Kloges et al. (Eds.). Wiley-VCH, Wein￾heim, 2008.
[82] R. Herrmann, Fractional Calculus: An Introduction for Physicists. World Scien￾tific, Singapore, 2014.
[83] M. Caputo and M. Fabrizio, Prog. Fract. Differ. Appl. 1:1, 2015.
[84] D. Oliveira, E. Capelas, T. Machado and J. Ant´onio, Math. Probl. Eng. 2014;
doi:10.1155/2014/238459.
[85] Aslan Ismail, Math. Meth. Appl. Sci. 38:27, 2015.
[86] K.S. Miller and B. Ross, An Introduction to the Fractional Calculus and Fractional
Differential Equations. John Wiley and Sons, New York, 1993.
[87] Y. Luchko and R. Gorneflo, The Initial Value Problem for Some Fractional Dif￾ferential Equations with the Caputo Derivative. Fachbreich Mathematic and In￾formatik, Freic Universitat Berlin, 1998.
[88] S. Kazem, Int. J. Nonl. Sci. 16:3, 2013.
[89] E. Hairer, S.P. Norsett and G. Wanner, Solving Ordinary Differential Equations
I: Nonstiff Problems. Springer, Berlin, 1993.
[90] E. Ahmed, A.M.A. El-Sayed and H.A.A. El-Saka, J. Math. Anal. Appl. 325:542,
2007.
[91] M. Lakshmanan and S. Rajasekar, Nonlinear Dynamics: Integrability, Chaos and
Patterns. Springer, Berlin, 2002.
[92] D. Matignon, Comput. Eng. Syst. Appl. 2:963, 1996.
[93] D. Qian, C. Li, R.P. Agarwal and P.J.Y. Wong, Math. Comput. Model 52:862,
2010.
[94] Y.Y.Y. Noupoue, Y. Tandogdu and M. Awadalla, Adv. Difference Eqs. 2019-108,
2019, 13 pages.410 Fractional Order Ordinary Differential Equations
[95] I. Grigorenko and E. Grogorenko, Phys. Rev. Lett. 91:034101, 2003.
[96] N.J. Ford and M. Luisa Morgado, Fract. Cal. Appl. Anal. 14:554, 2011.
[97] M.V. Rehman and R.A. Khan, Appl. Math. Model. 36:894, 2012.
[98] Q.M. Al-Mdallah and M.I. Syam, Commun. Nonl. Sci. Numer. Simulat. 17:2299,
2012.
[99] W.K. Zahra and S.M. Elkholy, Numer. Algor. 59:373, 2012.
[100] A. Jajarmi and D. Baleanu, Front. Phys. 8:220, 2020.
16.10 Problems
16.1 Write the Gr¨unwald–Letnikov fractional integral of order α.
16.2 Prove that ωk = (−1)k
 α
k

=

1 − 1 + α
k

ωk−1 with ω0 = 1.
16.3 Represent the formula (16.21) in matrix form for all the nodes.
16.4 Define the Laplace transform of Dαx(t) for 0 <α< 1 as [sF(s) − x(0)]/s1−α
where F(s) is the Laplace transform of x(t). Then, find the solutions of the
equations Dαx(t) + ax(t) = 0, x(0) = x0 and Dαx(t) = t, x(0) = x0, 0 <α< 1.
16.5 Applying the backward-difference method and using a calculator find the solution
of Dαx(t)=e−t at t = 0.001 and 0.002 with x(0) = 0 and α = 0.5 and 0.75.
16.6 Using a calculator compute the solution of Dαx(t) = x(t) + t, x(0) = 1 and
α = 0.5 at t = 0.001 and 0.002 applying the backward-difference method.
16.7 Prove the following relations [56]:
(i) (InαDnαf) (t) −

I (n+1)αD(n+1)αf

(t)
= t
nα
Γ(nα + 1) (Dnαf) (0+).
(ii) f(t) = n
k=0
t
kα
Γ(kα + 1)

Dkαf

(0+)
+

D(n+1)αf

(ξ)
Γ[(n + 1)α + 1]t
(n+1)α, 0 ≤ ξ ≤ t.
16.8 Find the exact solution of Dαx(t) = t
β, β > 0, x(0) = 0, 0 <α< 1 applying
the Laplace transform technique. Then, numerically solve this equation by the
fractional Euler method for α = 0.5, β = 3 and for t ∈ [0, 1] and compare the
numerical solution with the exact solution.
16.9 For the equation D0.5x + x + 1 = 0, x(0) = 0 by the fractional Euler method
compute x(0.0001) and x(0.0002). Do the calculations using a calculator.
16.10 Using a calculator and applying the fractional Euler method find the solution of
D0.5x = x + t, x(0) = 1 at t = 0.001 and 0.002.
16.11 Applying the ABM method find the numerical solution of the equation D0.5x = t,
x(0) = 1 at t = 0.01 and 0.02. Use a calculator for the calculations.Problems 411
16.12 For the equation D0.25x = −x, x(0) = 1 compute x(0.01) and x(0.02) by the
ABM method and using a calculator.
16.13 Develop a program to solve Eq. (16.37) by the ABM method and verify the results
presented in Table 16.6.
16.14 Using a calculator and applying the fractional Euler method find the solution of
D0.5x = t, x(0) = 1 at t = 0.001 and 0.002.
16.15 For the equation D0.5x = −x, x(0) = 1 compute x(0.001) and x(0.002) by the
two-step Adams–Bashforth method and using a calculator.
16.16 The fractional order logistic differential equation is DαN = rN(1 − (N/k)), 0 <
α < 1, 0 < r ≤ 1 and k > 0 [94]. Show that the equilibrium point N∗ = 0 is always
unstable and N∗ = k is always stable. For α = 0.25, 0.5 and 0.75 numerically
solve this equation with different values of N(0) by the Euler method and verify
that N(t) → N∗ in the long-time limit.
16.17 Consider the stability determining eigenvalues of the equilibrium points (x∗, y∗) =
(±1, 0) of the system (16.118). Vary the value of d from 0 to 5 in step size 0.1
and compute the values of |arg(λ+)| and |arg(λ−)| using Eq. (16.112). Verify that
both |arg(λ±)| are > π/2 there by they are > απ/2 for 0 <α< 1 and so (±1, 0)
are stable equilibrium points.
16.18 The fractional order Lotka–Volterra predator-prey model equation is [90]
Dαx1 = x1(4 − ax1 − bx2), Dαx2 = x2(−d + cx1),
where r, a, b, c, d > 0. Verify that the equilibrium points of the system are
(x∗
1, x∗
2) = (0, 0), (r/a, 0) and (d/c,(rc−ad)/bc). Show that the equilibrium point
(0, 0) is always unstable and (r/a, 0) is stable for 0 < r < ad/c. For a value of
α and a = b = c = d = 1 solve the system by the fractional Euler method for
several initial conditions and plot the trajectories in the x1 − x2 phase space for
r = 0.5 and 1.5. Verify the predictions of linear stability analysis.
16.19 For the system Dαx = µ−x2, Dαy = −y with µ > 0 determine all the equilibrium
points and identify their stability. Verify the stability nature of the equilibrium
points by numerically solving the equations for various initial conditions by the
fractional Euler method.
16.20 The fractional order Lorenz equations are [95]
Dαx = σ(y − x), Dβy = rx − y − xzρ, Dγz = xy − bz ,
where r > 0, σ > 0, ρ > 0 and b > 0 are parameters. Choose ρ = 1, σ =
10, b = 8/3, α = β = γ. One of the equilibrium points of this equations is
(x∗, y∗, z∗) = (0, 0, 0). This equilibrium point is stable for r < 1 and unstable for
r > 1 as per the linear stability analysis. Numerically, solve these equations by
the fractional Euler method and draw the trajectories in the neighbourhood of
the equilibrium point (0, 0, 0) for r = 0.5 and r = 1.5 and verify the predictions
of the linear stability analysis.17
Fractional Order Partial Differential Equations
17.1 Introduction
Certain numerical methods for fractional order ordinary differential equations are described
in the previous chapter. The present chapter is devoted to the fractional order partial
differential equations. Consider the linear partial differential equations of the form
a(x, t) ∂m
∂tm u(x, t) + b(x, t) ∂n
∂xn u(x, t) = f(x, t), (17.1)
where x and t are space and time variables, respectively, and m, n = 0, 1, 2,... . Classifica￾tions of the equations of the form (17.1) for integer values of m and n are well-known in the
field of partial differential equations. For some details see Chapter 14. In the equations of
the form (17.1) the time fractional order derivatives are often treated as the Caputo type
or the Riemann–Liouville sense or the Riez sense [1,2]. For Eqs. (17.1) with f = 0, a and
b are constants the classification of them according to the noninteger values of m and n in
the interval [0, 2] is reported by Changpin Li and An Chen [2].
An excellent review on numerical methods of solving the fractional order linear partial
differential equations with the order in the interval [0, 2] is presented in the refs. [2,3].
A great deal of interest has been focused on diffusion equations and variants of diffu￾sion equations. Various finite-difference schemes for time-fractional diffusion, diffusion-wave,
advection-diffusion and diffusion-convection like equations are proposed and analyzed [4-
22]. Space-fractional diffusion and advection-dispersion equations with special emphasize
on finite-difference schemes are dealt [23-34]. Numerical methods for equations with both
space and time derivatives being fractional order have been reported [35-43]. Apart from
finite-difference schemes, other approaches, namely, Galerkin methods [44-49], space spectral
time Adams–Bashforth Moulton method [50], McCormack method [51], multi-step method
[52], kernel scheme [53], radial point interpolation method [54], semi-discrete finite element
method [55], Dufort-Frankel scheme [56], Crank-Nicolson method [57-58], Fourier method
[59] and higher-order approximation methods [60-66] are developed and tested. Interest has
been focused on fractional order nonlinear partial differential equations in the refs. [67-72].
In this present chapter, certain fractional order linear partial differential equations with
a time variable and one space variable, that is, equations with 1+1 dimension are considered.
Particularly, the following equations are taken for analysis:
1. Time-fractional order diffusion equation
2. Time-fractional order advection-diffusion equation
3. Time-fractional order wave equation
4. Time-fractional order damped wave equation
5. Time-fractional order Fisher equation
6. Space-fractional order diffusion equation
DOI: 10.1201/9781032649931-17 412Time-Fractional Diffusion Equation 413
For the time-fractional diffusion equation without an external force or the source term a
finite-difference scheme is developed and the stability condition for the scheme is obtained.
The scheme is verified by considering the system with the exact analytical solution. The sys￾tem in the presence of an external force is examined and approach of the numerical solution
towards an equilibrium solution is tested. Then, the advection-diffusion equation is taken
for analysis. Next, the time-fractional order wave equation, the damped wave equation and
the Fisher equation are discussed. For the space-fractional order diffusion equation with
appropriate initial and boundary conditions, a finite-difference scheme is arrived. Applica￾bility of the scheme is studied by considering a decaying solution and a nonzero equilibrium
solution.
17.2 Time-Fractional Diffusion Equation
The mean-square displacement of the anomalous diffusion is given by
x2(t) ∼ 2Kα
Γ(1 + α)
t
α, t → ∞. (17.2)
In Eq. (17.2) α is the anomalous diffusion exponent and Kα is the diffusion coefficient. When
α = 1 the diffusion is called Brownian and 0 <α< 1 corresponds to subdiffusion. Define
u(x, t) as the probability density of determining a particle at x at time t. The equation used
to model the diffusion process with α = 1 is
∂
∂tu(x, t) = D ∂2
∂x2 u(x, t), D = K1 . (17.3)
In the case of anomalous diffusion with 0 <α< 1 the relevant model is the fractional order
diffusion equation [73-76]
∂α
∂tα u(x, t) = Kα
∂2
∂x2 u(x, t), t ≥ 0 . (17.4)
This equation can be rewritten as
∂u
∂t = KαD1−α t
∂2u
∂x2 , u = u(x, t), (17.5)
where Dγ
t denotes γ-order time derivative. An explicit finite-difference algorithm for
Eq. (17.5) (or Eq. (17.4)) has been developed and its applicability was investigated by
Yuste and Acedo [4]. Their method is followed in this section for Eq. (17.5).
Fractional order diffusion-like equations are employed in the model of information diffu￾sion in a social network [77], diffusion of second messenger signalling molecule inosital-1,4,5-
trisphosphate (IP3) in spiny dendrites of Purkinje neurons (in neuroscience) [78] and diffu￾sion process due to high velocity contrasts in heterogeneous porous media [79]. In financial
mathematics, in the formalism of the risk-neutral approach, the exponential option pricing
models are governed by the fractional order diffusion equations [81]. In electrodynamics,
averaging of charge density in a special manner gives fractional order time-derivatives in
Maxwell equations.414 Fractional Order Partial Differential Equations
17.2.1 A Finite-Difference Scheme
Discretize the space and time variables as
xj = j∆x, tm = m∆t , (17.6)
where ∆x and ∆t are the step sizes of the variables x and t, respectively. Denote u(xj , tm)
as u(m)
j . The finite-difference scheme for numerically solving the diffusion equation ∂u/∂t =
D∂2u/∂x2 is
1
∆t

u(m+1)
j − u(m)
j

= D
(∆x)2

u(m)
j−1 − 2u(m)
j + u(m)
j+1
+ T(x, t), (17.7)
where the truncation term is T(x, t). For Eq. (17.5), an equation similar to Eq. (17.7) after
neglecting the truncation term is
1
∆t

u(m+1)
j − u(m)
j

= KαD1−α t
1
(∆x)2

u(m)
j−1 − 2u(m)
j + u(m)
j+1
. (17.8)
Treat D1−α t in Eq. (17.8) as the Riemann–Liouville fractional order derivative operator.
The Gr¨unwald–Letnikov (GL) form of D1−α t f(t) is [82-86] (refer Eq. (16.3))
D1−α t f(t) = lim∆t→0
1
(∆t)1−α
m
k=0
ω(1−α)
k f(t − k∆t), (17.9a)
where m is the integer part of t/∆t and
ω(1−α)
k = (−1)k
 1 − α
k

. (17.9b)
ω(γ)
k can be computed recursively as
ω(γ)
0 = 1, ω(γ)
k =

1 − γ + 1
k

ω(γ)
k−1 . (17.9c)
Use of the GL form of D1−α t in Eq. (17.8) leads to the finite-difference scheme [4]
u(m+1)
j = u(m)
j + Sα
m
k=0
ω(1−α)
k

u(m−k)
j−1 − 2u(m−k)
j + u(m−k)
j+1 
, (17.10a)
where
Sα = Kα(∆t)α
(∆x)2 (17.10b)
and m = 0, 1, 2,.... To implement the above finite-difference scheme to Eq. (17.5), for
m = 0 the values of u(0)
j are needed and are given by the initial condition u(x, 0). Notice
the major difference in the formulas given by Eqs. (17.3) and (17.5). For Eq. (17.3) u(m+1)
j
at (j, m + 1) is evaluated using the values of u at the mesh points (j − 1, m), (j, m) and
(j + 1, m). In contrast to this, for the time-fractional order diffusion equation the value of
u at (j, m + 1) is computed using the values of u at the mesh points (j, k), (j − 1, k) and
(j + 1, k), k = 0, 1, 2,...,m. That is, for each value of m, the value of k starts from 0.
Though the scheme can be straight-forwardly implemented through a computer program,
it is necessary to store all the estimates u(k)
j−1, u(k)
j and u(k)
j+1, k = 0, 1, 2,...,m to compute
u(m+1)
j .Time-Fractional Diffusion Equation 415
17.2.2 Stability of the Finite-Difference Scheme
Let us perform the von Neumann stability analysis (refer Subsection 14.5.6) and obtain the
condition for the stability of the scheme obtained [4].
Assume the solution as u(m)
j = ζmeiqj∆x with q as the spatial wave number and ζm as
the time-dependent term. Substitution of this u(m)
j in Eq. (17.10a) leads to
ζm+1eiqj∆x = ζmeiqj∆x + Sα
m
k=0
ω(1−α)
k ζm−keiqj∆x 
e−iq∆x − 2+eiq∆x
. (17.11)
That is,
ζm+1 = ζm − 4Sα sin2 (q∆x/2)m
k=0
ω(1−α)
k ζm−k . (17.12)
Let ζm+1 = ξζm. From this it is easy to note that ζm−k = ξ−kζm. With this, Eq. (17.12)
becomes (see Problem 17.2 at the end of the present chapter)
ξ = 1 − 4Sα sin2 (q∆x/2)m
k=0
ω(1−α)
k ξ−k . (17.13)
As ζm+1 = ξζm, the time part ζ grows with time if |ξ| > 1 for certain q implying unstable
aspect of the numerical solution. For stability substitute the extreme value of ξ as −1 and
that of sin2(q∆x/2) as 1 in Eq. (17.13) and find the condition on Sα. This results in
Sα ≤
1
2
m
k=0(−1)kω(1−α)
k
. (17.14)
In the limit of m → ∞ find the value of the summation term in Eq. (17.14) [4]. As
ω(1−α)
0 = 1, ω(1−α)
1 = α − 1, ω(1−α)
2 = (α − 1)α/2, ... (17.15)
the summation term becomes
m
k=0
(−ξ)
kω(1−α)
k = 1 − (α − 1)ξ +
1
2!(α − 1)αξ2 − ... = (1 − ξ)
1−α . (17.16)
When ξ = −1, the summation term is 21−α. Then, from Eq. (17.14)
Sα = Kα(∆t)α
(∆x)2 <
1
22−α (17.17)
which gives
∆t < (∆t)c =
 (∆x)2
22−αKα
1/α
. (17.18)
Table 17.1 presents the values of (∆t)c for some values of α and ∆x with Kα = 1.
Treating the so-called L1 approximation [87] for D1−α t an implicit scheme for the diffu￾sion equation has been proposed [5].416 Fractional Order Partial Differential Equations
TABLE 17.1
(∆t)c values for certain chosen values of α and ∆x for Kα = 1.
α ∆x (∆t)c α ∆x (∆t)c
1.00 1/40 0.0003125 0.70 1/25 0.0000280
0.90 1/40 0.0001180 0.60 1/10 0.0000920
0.75 1/25 0.0000590 0.50 1/10 0.0000125
17.2.3 Examples
Now, test the applicability of the scheme (17.10) by considering the system (17.4) with two
different types of solutions.
Example 1: Decaying Solution
Consider the system with absorbing boundaries and the initial conditions as
u(0, t) = u(1, t)=0, u(x, 0) = x(1 − x), 0 ≤ x ≤ 1 . (17.19)
The exact analytical solution for the given u(x, 0) can be determined by the variable sepa￾rable method. The exact solution is
u(x, t) = 8
π3
∞
n=0
1
(2n + 1)3 sin[(2n + 1)πx]Eα

−Kα(2n + 1)2π2t
α
, (17.20)
where Eα[.] is the Mittag–Leffler function (refer Eq. (16.110)). Figure 17.1a shows the
numerically computed solution (represented by solid circles) at t = 0.5 for four values of α
and Kα = 1 along with the exact solution (represented by continuous line). For α = 1, 0.9,
0.75 and 0.5 the values of (∆x, ∆t) used are (1/40, 0.00025), (1/40, 0.0001), (1/25, 0.00005)
and (1/10, 0.00001), respectively. The values of ∆t are < (∆t)c (see Table 17.1). In Fig. 17.1b
the numerically computed u(x, t) is shown for a few values of t. u(x, t) → 0 in the long time
limit. Very good agreement between the numerical solution and the exact solution is clearly
seen.
Example 2: An Equilibrium Solution
The finite-difference scheme (17.10) can be extended to the equation of the form
∂αu
∂tα = Kα
∂2u
∂x2 + f(x, t) (17.21)
with u(0, t) = u(1, t) = 0 and an appropriate initial condition. Rewrite this equation as
∂u
∂t = KαD1−α t
∂2u
∂x2 + D1−α t f(x, t). (17.22)
Making use of Eqs. (17.9), the finite-difference scheme for the above equation is
u(m+1)
j = u(m)
j + Sα
m
k=0
ω(1−α)
k

u(m−k)
j−1 − 2u(m−k)
j + u(m−k)
j+1 
+(∆t)
αm
k=0
ω(1−α)
k f(m−k)
j , m = 0, 1, 2, ... . (17.23)Time-Fractional Diffusion Equation 417
x
u(x, t)
α = 1
α = 0.9
α = 0.75
(a) α = 0.5
0.40.30.20.10 0.5 0.90.80.70.6 1
0.02
0.01
0
x
u(x, t)
t = 0.5
t = 0.15
t = 0.05
t = 0.01
t = 0.0 (b)
0.40.30.20.10 0.5 0.90.80.70.6 1
0.3
0.2
0.1
0
FIGURE 17.1
(a) u(x, t) of the diffusion equation computed using Eqs. (17.10) for four values of α at
t = 0.5. The solid circles are the numerical solutions and the continuous lines are the exact
solutions. (b) u(x, t) versus x for five values of t for α = 0.75 and Kα = 1. u(x, t) decreases
with increase in t.
If D1−α t f(x, t) = g(x, t) is easy to determine analytically then the last term in Eq. (17.23)
can be replaced by ∆tg(x, t)=∆tg(m)
j . What is the advantage of this?
What is a stable equilibrium solution u∗(x) of Eq. (17.21)? By a stable equilibrium
solution, it is meant that an initial wave profile chosen near u∗ approach it in the long-time
limit. Let us seek such a solution. For simplicity assume that Kα = 1. u∗(x) is the solution
of
u∗
xx + S(x)=0 . (17.24)
Choose f(x, t) = S(x) = sin πx. Then, Eq. (17.24) gives
u∗(x) = 1
π2 sin πx + ax + b , (17.25)
where a and b are to be determined by the given boundary conditions. The condition
u(0, t) = 0 gives b = 0 while u(1, t) = 0 gives a = 0. Therefore,
u∗(x) = 1
π2 sin πx. (17.26)
Then, the exact solution of Eq. (17.21) is
u(x, t) = 1
π2

1 − Eα

−π2t
α sin πx. (17.27)418 Fractional Order Partial Differential Equations
x
u(x, t)
t =0
t =0.005
t =0.015
t =0.035
t =0.1
t =1
0.40.30.20.10 0.5 0.90.80.70.6 1
0.1
0.05
0
FIGURE 17.2
Plot of u(x, t) for various values of t of the diffusion Eq. (17.21) with f(x, t) = sin πx. Here,
the solid circles are the numerical solutions and the continuous lines are the exact solution.
With f(x, t) = sin πx the term D1−α t sin πx is obtained as
D1−α t sin πx = 1
Γ(α)
∂
∂t  t
0
(t − τ )
α−1f(x, τ )dτ
= sin πx
Γ(α)
∂
∂t  t
0
(t − τ )
α−1dτ
= sin πx
Γ(α) t
α−1 . (17.28)
Then, replace the last term in Eq. (17.23) by ∆t(sin πx)t
α−1/Γ(α).
Equation (17.22) is numerically solved using the algorithm (17.23) with Kα = 1, ∆x =
1/25, ∆t = 0.00005 and α = 0.75. The result is shown in Fig. 17.2 for some selected
values of t. For sufficiently large values of π2t
α, the term Eα

−π2t
α
≈ 0 giving u(x, t) =
sin πx/π2. That is, as t increases u(x, t) approaches the equilibrium solution u∗ and is
evident in Fig. 17.2. Though the initial solution is 0 the solution u(x, t) evolves to the
nonzero stationary solution.
17.3 Time-Fractional Advection-Diffusion Equation
The time-fractional order advection-diffusion equation is [20]
∂αu
∂tα = Kα
∂2u
∂x2 − Vα
∂u
∂x + f(x, t) (17.29)
subjected to the conditions
u(x, 0) = g1(x), u(0, t) = g2(t), u(L, t) = g3(t), 0 ≤ x ≤ L . (17.30)
In Eq. (17.29) 0 < α ≤ 1, Kα > 0 is the diffusion coefficient and Vα is the advection
coefficient. In [20] making use of the rth degree interpolation method, some schemes forTime-Fractional Advection-Diffusion Equation 419
Caputo derivative of order α ∈ [0, 1] are proposed and are applied to Eq. (17.29). In the
present section, the finite-difference approach discussed in the previous section is extended
to Eq. (17.29).
With D1−α t as the Riemann–Liouville fractional derivative operator Eq. (17.29) can be
written as
∂u
∂t = KαD1−α t
∂2u
∂x2 − VαD1−α t
∂u
∂x + D1−α t f(x, t). (17.31)
Discretize x and t as in Eq. (17.6) and denote u(xj , tm) as um
j . For Eq. (17.31) the finite￾difference scheme with
∂
∂xu(m)
j = 1
2∆x

u(m)
j+1 − u(m)
j−1

(17.32)
is
1
∆t

u(m+1)
j − u(m)
j

= Kα
(∆x)2 D1−α t

u(m)
j−1 − 2u(m)
j + u(m)
j+1
− Vα
2∆x
D1−α t

u(m)
j+1 − u(m)
j−1

+D1−α t f(m)
j , m = 0, 1, 2, ... . (17.33)
Considering Eqs. (17.9) for D1−α t f the above equation takes the form
u(m+1)
j = u(m)
j + Sα
m
k=0
ω(1−α)
k

u(m−k)
j−1 − 2u(m−k)
j + u(m−k)
j+1 
−Rα
m
k=0
ω(1−α)
k

u(m−k)
j+1 − u(m−k)
j−1

+(∆t)
αm
k=0
ω(1−α)
k f(m−k)
j , m = 0, 1, 2, ... , (17.34a)
where
Sα = Kα(∆t)α
(∆x)2 , Rα = Vα(∆t)α
2∆x . (17.34b)
Performing the stability analysis, similar to the diffusion equation, to Eq. (17.29) with
f(x, t) = 0 gives the stability condition for the finite-difference scheme (17.34) as (see
Problem 17.8 at the end of the present chapter)
(∆t)2α
4(∆x)4

V 2
α (∆x)
2 + 16K2
α

≤
1
22−2α . (17.35)
For Vα∆x  1 the term V 2
α (∆x)2 can be neglected so that
∆t < (∆t)c =
 (∆x)2
22−αKα
1/α
. (17.36)
This condition is same as the condition (17.18) obtained for Eq. (17.4).420 Fractional Order Partial Differential Equations
x
E(x,
0.5)
0.00015
0.00010
0.00005
∆t =0.00002
∆t =0.00003
∆t =0.00004
∆t =0.00005
0 0.2 0.4 0.6 0.8 1
0
FIGURE 17.3
Plot of E(x, 0.5) = |ue(x, 0.5) − un(x, 0.5)|, where ue and un are the exact solution and the
numerically computed solution, respectively, of Eq (17.29) for four different values of ∆t.
Example:
For simplicity assume that Kα = Vα = 1 and the solution as u(x, t) = tex with 0 ≤ x ≤ 1,
u(x, 0) = 0, u(0, t) = t and u(1, t) = te. Substitution of this solution in Eq. (17.29) with
∂αu/∂tα = t
1−α/Γ(2 − α) gives f(x, t) = t
1−αex/Γ(2 − α). For ∆x = 1/25 and α = 0.75
the value of (∆t)c = 0.000058. Numerical u(x, t) is computed applying the finite-difference
scheme (17.34) for four values of ∆t and for t ≤ 0.5. Note that u(x, t) is independent of
α. The numerical solution and the error E in the numerical solution, that is the absolute
difference between the exact and the numerically computed solutions given by
E(x, t) = 
ue(x, t) − un(x, t)

 , (17.37)
where ue and un are the exact solution and the numerically computed solution, respectively,
are calculated at t = 0.5 for 0 ≤ x ≤ 1. The obtained E(x, 0.5) for chosen values of ∆t are
presented in Fig. 17.3. Notice that E < 10−3 and decreases with decrease in the value of
∆t. The advection-diffusion system can also possess an equilibrium solution because of the
presence of the external forcing term f(x, t). For an example, see Problem 17.10 at the end
of the present chapter).
17.4 Time-Fractional Wave Equation
The time-fractional wave equation is
∂αu
∂tα = ∂2u
∂x2 + f(x, t), 0 ≤ x ≤ π , (17.38)
where α ∈ [1, 2] and the relevant initial and boundary conditions are
u(x, 0) = ϕ(x), ut(x, 0) = φ(x), u(0, t) = g1(t), u(π, t) = g2(t). (17.39)
When α = 1 it is the classical diffusion equation. The equation with α = 2 is the wave equa￾tion and it governs sound wave propagation through ideal conducting media. For α ∈ [1, 2]Time-Fractional Wave Equation 421
it is thought to interpolate the diffusion and wave equations. Time fractional wave equation
describes the propagation of waves in viscoelastic media, particularly, in viscoelastic solids
with a power-law creep and oscillations of smart material cable. It also describes travel￾ling of sound waves in an inhomogeneous medium exhibiting power-law attenuation with
frequency [88,89].
Operation of the fractional integral operator D1−α t with α ∈ [1, 2] on Eq. (17.38) gives
∂u
∂t = ∂u
∂t



t=0
+ D1−α t
∂2u
∂x2 + D1−α t f(x, t). (17.40)
Discretize the variables x and t as in Eq. (17.6). Similar to the diffusion equation, the
finite-difference scheme for the wave equation is
u(m+1)
j = u(m)
j + ∆tφ(x) + Sα
m
k=0
ω(1−α)
k

u(m−k)
j−1 − 2u(m−k)
j + u(m−k)
j+1 
+(∆t)
αm
k=0
ω(1−α)
k f(m−k)
j , Sα = (∆t)α
(∆x)2 , (17.41)
and m = 0, 1, 2, .... The stability condition for the above scheme is same as that of the
diffusion Eq. (17.4), however, here α ∈ [1, 2]. That is,
∆t < (∆t)c =
(∆x)2
22−α
1/α
, α ∈ [1, 2] . (17.42)
Example: An Equilibrium Solution
The equilibrium solution u∗ is the solution of the equation ∂2u∗/∂x2 + f(x) = 0. For
f(x) = sin x, u∗ is sin x + ax + b. Assume that u(0, t) = g1(t) = 0 and u(π, t) = g2(t) = 0.
The first and the second conditions give b = 0 and a = 0, respectively, and thus u∗ = sin x.
For f(x, t) = sin x the last term in Eq. (17.40) can be easily evaluated. As α ∈ [1, 2], write
α =1+ β, β ∈ [0, 1]. Then,
D1−α t f(x, t) = RLD1−1−β
t f(x, t)
= sin x
Γ(β)
 t
0
(t − τ )
β−1dτ
= sin x
Γ(1 + β)
t
β
= sin x
Γ(α)
t
α−1 , α ∈ [1, 2]. (17.43)
The last term in Eq. (17.41) containing f(m−k)
j can be replaced by ∆tsin x tα−1/Γ(α).
Next, determine φ(x) in Eq. (17.41) which is given by ut(x, 0). As the wave equation is
linear its exact solution can be determined, for example, by the variable separable method.
Writing u(x, t) = U(t) sin x, the equation for U(t) is
dαU
dtα = −U + 1. (17.44)
Its solution is
U(t)=1+ Eα(−t
α), (17.45)422 Fractional Order Partial Differential Equations
x
u(x, t)
t =30,u∗
t =9.5
t =6
t =3
t =0
α = 1.75, ∆x = π/200, ∆t = 0.005
0 0.5 1 1.5 2 2.5 3
1.5
1
0.5
0
FIGURE 17.4
The numerically computed solution u(x, t) of the wave Eq. (17.38) for α = 1.75.
where Eα is the Mittag-Leffler function (refer Eq. (16.110)). Then,
u(x, t) = (1 + Eα(−t
α)) sin x
= sin x(1 + 1 − t
α/Γ(α + 1) + t
2α/Γ(2α + 1) − ...). (17.46)
This gives
ut(x, t) = sin x(−αtα−1/Γ(α + 1) + 2αt2α−1/Γ(2α + 1) − ...) (17.47)
and φ(x) = ut(x, 0) = 0.
Fix α = 1.75, ∆x = π/200 and ∆t = 0.005 whereas (∆t)c = 0.00786. The initial wave
profile is chosen as u(x, 0) = 0. Figure 17.4 shows the time evolution of the zero initial wave.
The solution appears as oscillating and in this figure at t = 30 the numerically computed
solution u(x, t) becomes u∗(x). In order to know what is happening, collect the value of,
for example, u(π/2, t) for a wide range of values of t. The result is the Fig. 17.5. u(π/2, t)
exhibits damped oscillation. Though the initial solution is zero because of the term f(x)
the system displays an equilibrium solution in the long-time limit.
t
u(π/2, t)
α = 1.75
0 5 10 15 20 25 30
1.5
1
0.5
0
FIGURE 17.5
Time variation of u(π/2, t) of the diffusion Eq. (17.38) for α = 1.75.Time-Fractional Damped Wave Equation 423
In the scheme given by Eq. (17.41) choose φ(x) = sin x and compute the numerical
solution and find whether it approaches u∗ = sin x or not. Explain your observation.
17.5 Time-Fractional Damped Wave Equation
The time-fractional damped wave equation in 1 + 1 dimension is
∂2u
∂t2 +
∂αu
∂tα − ∂2u
∂x2 = f(x, t), (17.48)
where 0 <α< 1, 0 ≤ x ≤ 1 and u(x, 0) = ψ(x), ut(x, 0) = φ(x) and u(0, t) = u(1, t) = 0.
Equation (17.48) with the additional term u(x, t) is called fractional telegraph equation.
For this equation, two finite-difference schemes have been developed [56]. In this section,
Eq. (17.48) is considered.
17.5.1 A Finite-Difference Scheme
Rewrite Eq. (17.48) as
∂u
∂t + D1−α t
∂2u
∂t2 − D1−α t
∂2u
∂x2 = D1−α t f(x, t). (17.49)
Define ∆x = 1/jx, xj = j∆x, j = 0, 1,...,jx, increment in t as ∆t, u(m)
j = u(xj , tm),
∂
∂tu(m)
j = 1
2∆t

u(m+1)
j − u(m−1)
j

, (17.50a)
∂2
∂x2 u(m)
j = 1
(∆x)2

u(m)
j+1 − 2u(m)
j + u(m)
j−1

, (17.50b)
∂2
∂t2 u(m)
j = 1
(∆t)2

u(m+1)
j − 2u(m)
j + u(m−1)
j

, (17.50c)
D1−α t f(m)
j = 1
(∆t)1−α
m
k=0
ω(1−α)
k f(m−k)
j . (17.50d)
Substitution of Eqs. (17.50) in Eq. (17.49) gives
u(m+1)
j = u(m−1)
j − 2(∆t)
α−2m
k=0
ω(1−α)
k

u(m+1−k)
j − 2u(m−k)
j + u(m−1−k)
j

+2 (∆t)α
(∆x)2
m
k=0
ω(1−α)
k

u(m−k)
j+1 − 2u(m−k)
j + u(m−k)
j−1

+2(∆t)
αm
k=0
ω(1−α)
k f(m−k)
j , m = 0, 1, 2, ... . (17.51)
When k = 0 the second term in the right-side of Eq. (17.51) contains the terms u(m+1)
j , u(m)
j
and u(m−1)
j . The quantity u(m+1)
j to be determined occurs in both sides of the equation.
Therefore, separate out the terms in this summation for k = 0 and so the starting value of424 Fractional Order Partial Differential Equations
k becomes 1. Then, bringing the u(m+1)
j term −2(∆t)α−2w(1−α)
0 u(m+1)
j to the right-side of
Eq. (17.51) leads to the finite-difference scheme
u(m+1)
j = 1
S1

u(m−1)
j − S2

−2u(m)
j + u(m−1)
j

−S3
m
k=1
ω(1−α)
k

u(m+1−k)
j − 2u(m−k)
j + u(m−1−k)
j

+S4
m
k=0
ω(1−α)
k

u(m−k)
j+1 − 2u(m−k)
j + u(m−k)
j−1

+S5
m
k=0
ω(1−α)
k f(m−k)
j

, m = 0, 1, 2, ... (17.52a)
where
S1 = 1 + 2(∆t)
α−2ω(1−α)
0 , S2 = S1 − 1, (17.52b)
S3 = 2(∆t)
α−2, S4 = 2 (∆t)α
(∆x)2 , S5 = 2(∆t)
α. (17.52c)
From Eqs. (17.52) note that to calculate u(2)
j the values of u(−1)
j , u(0)
j , u(1)
j , u(0)
j+1, u(1)
j+1
and u(1)
j−1 are needed. Except u(−1)
j and u(1)
j the values of all other quantities are given by
the initial and boundary conditions. To find u(−1)
j and u(1)
j consider the first three terms in
the Taylor series expansion of them about u(0)
j . This gives
u(1)
j = u(0)
j + ∆t(ut)
(0)
j +
1
2
(∆t)
2(utt)
(0)
j
= u(0)
j + ∆tφj +
1
2
(∆t)
2

−
∂αu
∂tα
(0)
j
+
∂2u
∂x2
(0)
j
+ f(0)
j

. (17.53)
As Dα
t u(m)
j = (1/(∆t)α)
m
k=0 ω(α)
k u(m−k)
j the expression for Dα
t u(0)
j is
Dα
t u(0)
j =
∂αu
∂tα
(0)
j
= 1
(∆t)α ω(α)
0 u(0)
j . (17.54)
Then, with ω(α)
0 = 1
u(1)
j = u(0)
j + ∆tφj − 1
2
(∆t)
2−αu(0)
j + (∆t)2
2(∆x)2

u(0)
j+1 − 2u(0)
j + u(0)
j−1

+
1
2
(∆t)
2f(0)
j (17.55)
and an expression for u(−1)
j can be obtained from
u(−1)
j = u(0)
j − ∆t(ut)
(0)
j +
1
2
(∆t)
2(utt)
(0)
j . (17.56)
17.5.2 Example
The exact solution of Eq. (17.48) for
f(x, t) =  t
1−α
Γ(2 − α) + π2t

sin πx (17.57)Time-Fractional Damped Wave Equation 425
x
u(x, t)
∆t =0.001
α =0.5, ∆x =0.01
t =0
t =10
t =5
t =1
0.40.30.20.10 0.5 0.90.80.70.6 1
10
5
0
FIGURE 17.6
The numerical solutions of Eq. (17.48) for three values of t computed employing the finite￾difference scheme (17.52) for α = 0.5. Continuous lines and the solid circles represent the
exact and the numerically computed solutions, respectively.
is u(x, t) = tsin πx. For the choice 0 ≤ x ≤ 1 the conditions are u(x, 0) = 0, u(0, t) = 0,
u(1, t) = 0 and ut(x, 0) = φ(x) = sin πx. Further, u(1)
j = u(0)
j + ∆tsin πxj and u(−1)
j =
u(0)
j − ∆tsin πxj . Choose ∆x = 0.01 and ∆t = 0.001. The numerical solutions computed
at t = 1, 5 and 10 are plotted in Fig. 17.6. The exact solution is independent of α. This is
verified from the numerical solution computed for several values of α. The error E in the
computed solution is given by Eq. (17.37). In Fig. 17.7 E at x = 0.5 and t = 10 for α = 0.5
is plotted as a function of the space step size ∆x. As u(x, t) is maximum at x = 0.5 the
error E at this value of x is considered in Fig. 17.7. E decreases with decrease in ∆x. For
∆x = 0.01 the error E(0.5, 10) is < 10−3.
Instead of Eq. (17.50a) the approximation
∂
∂tu(m)
j = 1
∆t

u(m+1)
j − u(m)
j

(17.58)
can be considered. Then, the finite-difference scheme is given by Eq. (17.52) with the first
term in the right-side of it is replaced by u(m)
j and with S1 to S5 without the multiplication
∆x
E(0.5, 10)
0.005 0.006 0.007 0.008 0.009 0.01
0.0008
0.0006
0.0004
0.0002
FIGURE 17.7
The variation of the error in the numerical solution at t = 10 and x = 0.5 for α = 0.5,
∆t = 0.001 with ∆x for the damped wave Eq. (17.48).426 Fractional Order Partial Differential Equations
factor 2. The numerical solution obtained with the approximation (17.58) is found to be
almost the same as the solution obtained with the approximation (17.50a). The reader may
verify this.
17.6 Time-Fractional Fisher Equation
The time-fractional order version of the Fisher Eq. (14.121) is
∂αu
∂tα + c
∂u
∂x = f(x, t). (17.59)
For simplicity choose c = 1 and α ∈ [0, 1]. As usual, rewrite this equation as
∂u
∂t = −cD1−α t
∂u
∂x + D1−α t f(x, t). (17.60)
17.6.1 Finite-Differences Schemes
With
∂u
∂t = 1
∆t

u(m+1)
j − u(m)
j

, ∂u
∂x = 1
2∆x

u(m)
j+1 − u(m)
j−1

, (17.61)
the finite-difference scheme for Eq. (17.60) is
u(m+1)
j = u(m)
j − Sα
m
k=0
ω(1−α)
k

u(m−k)
j+1 − u(m−k)
j−1

+(∆t)
αm
k=0
ω(1−α)
k f(m−k)
j , m = 0, 1,... (17.62a)
where
Sα = (∆t)α
2∆x
, ω(1−α)
0 = 1, ω(1−α)
k =

1 − 2 + α
k

ω(1−α)
k−1 . (17.62b)
One may choose
∂u
∂t = 1
2∆t

u(m+1)
j − u(m−1)
j

. (17.63)
In this case
u(m+1)
j = u(m−1)
j − S
α
m
k=0
ω(1−α)
k

u(m−k)
j+1 − u(m−k)
j−1

+2(∆t)
αm
k=0
ω(1−α)
k f(m−k)
j , m = 0, 1,... (17.64)
with S
α = (∆t)α/∆x. u(0)
j is given by the initial condition u(x, 0). Equation (17.64) gives
u(2)
j , u(3)
j , .... The values of u(1)
j need to be calculated. For this use
∂u
∂t = 1
∆t

u(m+1)
j − u(m)
j

(17.65)Diffusion Equation with Fractional Order Space Derivative 427
and obtain the approximation
u(1)
j = u(0)
j − Sαω(1−α)
0

u(0)
j+1 − u(0)
j−1

+ (∆t)
αω(1−α)
0 f(0)
j . (17.66)
It is easy to obtain the stability condition for the finite-difference schemes (17.62) and
(17.64).
17.6.2 Examples
For illustrative purpose, assume the solution of Eq. (17.59) as u(x, t) = t
αex and find f(x, t).
As
Dα
t u(x, t) = Γ(1 + α) ex (17.67)
Eq. (17.59) gives
f(x, t) = Γ(1 + α) ex + t
αex. (17.68)
D1−α t f(x, t) in Eq. (17.60) becomes
D1−α t f(x, t) = Γ(1 + α)
Γ(α) t
α−1ex + exD1−α t t
α . (17.69)
Then, in Eq. (17.62a) the last term in the right-side takes the form
Γ(1 + α)
Γ(α) t
α−1 m exj∆t + (∆t)
αexj
m
k=0
ω(1−α)
k t
α
m−k . (17.70)
The initial and the boundary conditions are
u(x, 0) = 0, u(xmin, t) = t
αexmin , u(xmax, t) = t
αexmax . (17.71)
Fix xmin = 0, xmax = 1, ∆x = 1/25 and ∆t = 0.00005. The numerically computed solution
at t = 2 employing the scheme (17.62) for α = 1, 0.75 and 0.5 is presented in Fig. 17.8a along
with the exact solution. Figure 17.8b shows u(x, t) at t = 0.5, 1, 1.5 and 2 for α = 0.75.
In Section 14.8, the Fisher Eq. (17.59) with α = 1 is considered with u(x, t) = sech(x−t).
Shape preserving and forwardly moving nature of the solution is realized in the numerical
solution also. For the Eq. (17.59) with α = 1 suppose u(x, 0) = sech(2x), x ∈ [−10, 15],
∆x = 1/25 and ∆t = 0.00005. The time evolution of this single hump wave is shown in
Fig. 17.9. Because the value of α is < 1, the system is a damped system and hence the wave
solution is spreading over space with decreasing amplitude with time.
17.7 Diffusion Equation with Fractional Order Space Derivative
In Section 17.2, diffusion equation with fractional order time derivative is considered. This
section is concerned with the diffusion equation with a fractional order space derivative.
The system is
∂u
∂t = ∂αu
∂xα + f(x, t), (17.72)
where u = u(x, t), α ∈ [1, 2], 0 ≤ x ≤ 1 and subjected to appropriate initial and boundary
conditions. Finite-difference schemes for Eq. (17.72) are obtained by treating Dα
x = ∂α/∂xα
as the Riemann–Liouville derivative operator [23] and Gr¨unwald–Lutnikov derivative oper￾ator [26].428 Fractional Order Partial Differential Equations
x
u(x, t)
α =0.5
α =0.75
α =1
(a) t = 2
0 0.25 0.5 0.75 1
5
4
3
2
1
x
u(x, t)
t =0.5 t =1
t =1.5
t =2
(b) α = 0.75
0 0.25 0.5 0.75 1
5
4
3
2
1
FIGURE 17.8
(a) u(x, t) of the Fisher Eq. (17.59) with f(x, t) = Γ(1 + α) ex + t
αex obtained using the
scheme (17.62). The symbols and the continuous lines represent the numerically computed
and the exact analytical solutions, respectively. (b) u(x, t) at four values of t for α = 0.75.
17.7.1 Dα as the Riemann–Liouville Derivative Operator
Dα
a,xF(x) is given by
Dα
a,xF(x) = 1
Γ(m − α)
∂m
∂xm
 x
a
(x − τ )
m−1−αF(τ )dτ , (17.73)
where m is an integer with m>α>m − 1. For α ∈ [1, 2] the value of m is 2. As
RLDα
0,xF(x) = CDα
0,xF(x) +
m
−1
k=0
F(k)
(0)xk−α
Γ(k + 1 − α)
, (17.74)
one can write, for α ∈ [1, 2],
RLDα
0,xF(x) = F(0)x−α
Γ(1 − α) +
F
(0)x1−α
Γ(2 − α) + CDα
0,xF(x). (17.75)
x
u(x, t)
t =4
t =2
t =1
t =0
-10 -5 0 5 10
1
0.8
0.6
0.4
0.2
0
FIGURE 17.9
The time evolution of u(x, 0) = sech(2x) according to Eq. (17.59) for α = 0.75.Diffusion Equation with Fractional Order Space Derivative 429
Let us discretize the derivative at x = xj as

RLDα
0,xF(x)

x=xj
= F(0)x−α
j
Γ(1 − α) +
F
(0)x1−α
j
Γ(2 − α)
+
1
Γ(2 − α)

j−1
k=0
 xj+1
xj
τ 1−αF(xj − τ )dτ. (17.76)
Approximating F(xj − τ ) on each interval [xj , xj+1] as [2]
1
(∆x)2 [F(xj − xk−1) − 2F(xj − xk) + F(xj − xk+1)] (17.77)
Eq. (17.76) becomes [23]

RLDα
0,xF(x)

x=xj
= F(0)x−α
j
Γ(1 − α) +
F
(0)x1−α
j
Γ(2 − α) + (∆x)−α
Γ(3 − α)
×

j−1
k=0
bk (Fj−k−1 − 2Fj−k + Fj−k+1), (17.78a)
where
bk = (k + 1)2−α − k2−α . (17.78b)
In Eq. (17.78a), for xj = 0, the first two terms in the right-side are singular and are to be
determined from the boundary conditions.
Returning to Eq. (17.72), approximate ∂u(m)
j /∂t as 
u(m+1)
j − u(m)
j

/∆t and Dα
x u(m)
j
by Eq. (17.78a). The result is
u(m+1)
j = u(m)
j +
∆t
Γ(1 − α)
u(m)
0 x−α
j +
∆t x1−α
j
Γ(2 − α)
ux|x=0,t=tm
+
∆t(∆x)−α
Γ(3 − α)

j−1
k=0
bk

u(m)
j−k−1 − 2u(m)
j−k + u(m)
j−k+1
+∆tf(m)
j . (17.79)
17.7.2 Appropriate Initial and Boundary Conditions
The singular terms, namely the second and the third terms in Eq. (17.79) can be avoided
by appropriate boundary conditions [23]. The boundary condition ux(0, t) = 0 removes the
singular term, the third term in the right-side of Eq. (17.79). In order to remove the other
singular term ∆tu(m)
0 x−α
j /Γ(1 − α) choose u(0, t) = 0. Then, the appropriate initial and
boundary conditions are
u(x, 0) = g(x), u(1, t)=0, u(0, t)=0, ux(0, t)=0 . (17.80)
The last two boundary conditions remove the singular terms in Eq. (17.79). Then, the
finite-difference scheme becomes
u(m+1)
j = u(m)
j +
∆t(∆x)−α
Γ(3 − α)

j−1
k=0
bk

u(m)
j−k−1 − 2u(m)
j−k + u(m)
j−k+1
+ ∆tf(m)
j . (17.81)430 Fractional Order Partial Differential Equations
The stability condition for the scheme given by Eq. (17.81) is ∆t < (∆t)c = (∆x)α [23].
In the case of u(x, t) = 0, the singular term ∆tu(m)
0 x−α
j /Γ(1 − α) is present. To remove
it, consider the modified version of Eq. (17.72) as
∂u
∂t = Dα
x (u(x, t) − u(0, t)) + f(x, t), (17.82)
The first singular term in the fractional derivative given by Eq. (17.78a) with F = u(x, t) −
u(0, t) is
F(0)x−α
Γ(1 − α) = [u(0, t) − u(0, t)] x−α
Γ(1 − α) = 0 . (17.83)
17.7.3 Determination of u(0, t) for t > 0
If u(0, t) = 0 for t > 0 and is unknown then an expression for it can be easily obtained
[23]. u(x = 0, t) at t = 0 is known from the given initial condition. Consider the solution
at t = ∆t. From the finite-difference formula note that u(∆t)
0 is not required for calculating
u(∆t)
j , j = 1, 2,...,jx − 1 but it is needed for calculating the solution at t = 2∆t.
u(∆t)
j for j = 1, 2,...,jx−1 can be calculated from the finite-difference scheme. Then, an
expression for u(∆t)
0 involving u(∆t)
1 and u(∆t)
2 can be arrived. From this expression, one can
write the expression for u(m+1)
0 as described below. For this purpose, consider the expansion
of u(∆t)
1 near x = 0 as
u(∆t)
1 = u(∆t)
0 + ∆x(ux)
(∆t)
0 + (∆x)
α
∂αu
∂xα
(∆t)
0
. (17.84)
As ux(0, t)=0
u(∆t)
1 = u(∆t)
0 + (∆x)
α
∂αu
∂xα
(∆t)
0
. (17.85)
Similarly,
u(∆t)
2 = u(∆t)
0 + 2α(∆x)
α
∂αu
∂xα
(∆t)
0
. (17.86)
This equation gives
(∆x)
α
∂αu
∂xα
(∆t)
0
= 1
2α

u(∆t)
2 − u(∆t)
0

. (17.87)
Use of this in Eq. (17.85) leads to the result
u(∆t)
0 = u(∆t)
1 − 2−αu(∆t)
2
1 − 2−α . (17.88)
In general,
u(t+∆t)
0 = u(t+∆t)
1 − 2−αu(t+∆t)
2
1 − 2−α , (17.89)
that is,
u(m+1)
0 = u(m+1)
1 − 2−αu(m+1)
2
1 − 2−α . (17.90)Diffusion Equation with Fractional Order Space Derivative 431
17.7.4 An Equivalent Alternative Form of Equation (17.81)
Reorganization of the sum in the second term in the right-side of Eq. (17.81) leads to an
equivalent form of this term as [23]

j−1
k=0
bk

u(m)
j−k−1 − 2u(m)
j−k + u(m)
j−k+1
= 
j
k=−1
Wku(m)
j−k , (17.91a)
where
W0 = 22−α − 3, W−1 = 1, (17.91b)
Wk = (k + 2)2−α − 3(k + 1)2−α + 3k2−α
−(k − 1)2−α , 1 ≤ k ≤ j − 2, (17.91c)
Wj−1 = −2j2−α + 3(j − 1)2−α − (j − 2)2−α , (17.91d)
Wj = j2−α − (j − 1)2−α . (17.91e)
Then,
u(m+1)
j = u(m)
j +
∆t(∆x)−α
Γ(3 − α)

j
k=−1
Wku(m)
j−k + ∆tf(m)
j . (17.92)
This formula is for Eq. (17.72) with the conditions given by Eq. (17.80). For Eq. (17.82)
with u(0, t) = 0 the finite-difference scheme is
u(m+1)
j = u(m)
j +
∆t(∆x)−α
Γ(3 − α)

j
k=−1
Wk

u(m)
j−k − u(m)
0

+ ∆tf(m)
j . (17.93)
17.7.5 Application of the Scheme (17.81)
Let us study the applicability of the finite-difference scheme (17.81) to the diffusion equation
by considering a few specific solutions u(x, t).
Example 1: Decaying Solution
Assume the solution of (17.72) as
u(x, t) = 16e−t
x2(1 − x)
2, 0 ≤ x ≤ 1. (17.94)
The initial and the boundary conditions (17.80) for this solution become
u(x, 0) = g(x) = 16x2(1 − x)
2, u(1, t)=0, u(0, t)=0, (17.95a)
ux(0, t) = 32e−t
(x − 3x2 + 2x3)x=0 = 0 . (17.95b)
Note that for the assumed solution u(0, t) = 0. f(x, t) in Eq. (17.72) for the solution (17.93)
is determined as
f(x, t) = −16e−t
x2(1 − x)
2 +
32e−t
Γ(2 − α)
 x2−α
Γ(3 − α) +
12x4−α
Γ(5 − α) − 6x3−α
Γ(4 − α)

.
(17.96)
Discretize the space variable x by dividing the interval [0, 1] into jx equal intervals
with grid points xj = j∆x, j = 0, 1,...,jx where ∆x = 1/jx. For ∆x = 1/40 = 0.025432 Fractional Order Partial Differential Equations
x
u(x, t)
t =3
t =1
t =0.4
α = 1.75 t =0
0.40.30.20.10 0.5 0.90.80.70.6 1
1
0.75
0.5
0.25
0
FIGURE 17.10
The numerically computed solution (marked by the symbols) and the exact analytical so￾lution (marked by the continuous curve) of Eq. (17.72) with f(x, t) given by Eq. (17.96).
For t = 0 the solid circles represent the analytical solution. The scheme (17.81) is used with
∆x = 0.025 and ∆t = 0.0001.
and α = 1.75 the value of (∆t)c is 0.00157. Fix the value of ∆t as 0.0001. Figure 17.10
presents the numerical solutions (symbols) at t = 0.4, 1 and 3 along with the exact solutions
(continuous curves) and the solution at t = 0. Closeness of the numerical solution with the
exact solution is clearly seen. The solution is independent of α. This is verified with the
numerical solution computed for several values of α. As expected, the solution decays to 0
in the long-time limit.
The influence of ∆x on the error in the numerical solution is studied. For α = 1.75 and
∆t = 0.0001 the maximum error in the numerical solution at t is defined as
Emax(t) = max.

ue(xj , t) − un(xj , t)

, j = 1, 2,...,jx − 1

, (17.97)
where ue is the exact solution and un is the numerically computed solution. Emax is com￾puted at t = 0.4 for a range of values of ∆x and the result is shown in Fig. 17.11. As
∆x decreases the error decreases almost linearly. Instead of Emax, another quantity called
average error , Eave, defined as [23]
Eave(t) = jx
j=0 (ue(xj , t) − un(xj , t))2
jx
j=0(ue(xj , t))2
1/2
(17.98)
can also be used to compare the numerical solution with the exact solution.
Example 2: Nonzero Equilibrium Solution
The solution given by Eq. (17.94) decays with time. For a suitable chosen f(x, t) the
system can admit a nonzero stationary solution [23]. Assume that f(x, t) is independent
of time and is S(x). For an equilibrium solution ∂u/∂t = 0. So, an equilibrium solution
u∗ of Eq. (17.72) is the solution of the equation Dα
x u∗ + S(x) = 0 where f(x, t) = S(x),
independent of time. For a chosen S(x) integration of this equation gives u∗. For illustrative
purpose, let S(x)=1 − x2−α [23]. Then,
u∗(x) = −D−α
0,x S(x) + ax + b , (17.99)Diffusion Equation with Fractional Order Space Derivative 433
∆x
Emax
α = 1.75, t = 0.4
0 0.005 0.01 0.015 0.02 0.025
0.025
0.02
0.015
0.01
0.005
0
FIGURE 17.11
Emax, the maximum error in the numerical solution for x ∈ [0, 1], versus the spatial step
size ∆x for Eq. (17.72) at t = 0.4 and α = 1.75.
where a and b are to be determined employing the boundary conditions ux(0, t) = u(1, t) =
0. D−α
0,x S(x) is obtained as
D−α
0,x S(x) = 1
Γ(α)
 x
0
(x − τ )
α−1 
1 − τ 2−α
dτ
= 1
Γ(α)
 x
0
(x − τ )
α−1dτ − 1
Γ(α)
 x
0
(x − τ )
α−1τ 2−αdτ
= − 1
Γ(1 + α)
(x − τ )
α

x
0 − 1
Γ(α)
L−1 Γ(3 − α)
s3−α
Γ(α)
sα
= 1
Γ(1 + α)
xα − Γ(3 − α)
2
x2 , (17.100)
where L−1 is the inverse Laplace transform. Then,
u∗(x) = − 1
Γ(1 + α)
xα + Γ(3 − α)
2
x2 + ax + b . (17.101)
Applying the boundary condition ux(0, t) = 0 gives a = 0. The other condition u(1, t)=0
gives
b = 1
Γ(1 + α) − Γ(3 − α)
2 . (17.102)
Therefore,
u∗(x) = 1
Γ(1 + α)
(1 − xα) − Γ(3 − α)
2

1 − x2
. (17.103)
For Eq. (17.72) with f(x, t) = S(x)=1 − x2−α and the initial condition u(x, 0) chosen not
too far away from the equilibrium solution u∗(x) the solution u(x, t) evolves towards u∗.
In the numerical solution fix α = 1.75, ∆x = 1/400 = 0.0025 and ∆t = 0.00001. u(x, t)
is computed for two choices of u(x, 0). u(m+1)
j for j = 1, 2,...,jx −1 are computed using the434 Fractional Order Partial Differential Equations
x
u(x, t)
u(x, 0) = 0.2u∗(x, 0)
u∗(x, 0)
t =5
t =0.5
t =0.2
t =0
(a) α = 1.75, ∆x = 0.0025, ∆t = 0.00001
0.40.30.20.10 0.5 0.90.80.70.6 1
0.2
0.15
0.1
0.05
0
x
u(x, t)
u(x, 0) = 2u∗(x, 0)
u∗(x, 0)
t =5
t =0.5
t =0.2 t =0, (b) α = 1.75, ∆x = 0.0025, ∆t = 0.00001
0.40.30.20.10 0.5 0.90.80.70.6 1
0.4
0.3
0.2
0.1
0
FIGURE 17.12
Plots of u(x, t) of the diffusion Eq. (17.72) for α = 1.75 and f(x, t)=1 − x2−α. In the
subplot (a) the initial wave profile is u(x, 0) = 0 while it is 2u∗(x) in the subplot (b). The
dashed line represents u∗(x).
scheme given by Eq. (17.81) and u(m+1)
0 is computed using Eq. (17.90) while u(m+1)
jx = 0.
Figure 17.12a presents the time evolution of u(x, 0) = 0.2u∗(x). Even for the initial solution
as zero solution, because f(x, t) is nonzero the solution u(x, t) is found to end up on the
equilibrium state u∗. In Fig. 17.12b the initial wave profile is u(x, 0) = 2u∗(x). In this
case, also u(x, t) becomes u∗ for sufficiently large values of t. u(x, t) approaches to u∗ with
increase in t. u(0.1, t) is computed for t ∈ [0, 6]. The result is shown in Fig. 17.13 for both
u(x, 0) = 0.2u∗(x) and u(x, 0) = 2u∗(x). u(0.1, t) asymptotically approaches to u∗(0.1)
and in general, u(x, t) → u∗(x) in the long time limit. The convergence to u∗ is a nonlinear
function of time. In Fig. 17.14 u(x, t) attained the equilibrium state for, say, t > 3. The
maximum error Emax in the numerical solution is calculated at t = 5 using Eq. (17.97) with
u(x, 0) = 2u∗(x) for some selected values of ∆x. In Fig. 17.14 Emax is found to decrease
with decrease in the value of ∆x.
17.7.6 A Semi-Implicit Scheme
A semi-implicit scheme is developed in [23]. In this scheme, the terms in the summation in
Eq. (17.93) forming a tridiagonal matrix are collected and used in the time advanced part.Diffusion Equation with Fractional Order Space Derivative 435
t
u(0.1, t)
u(x, 0) = 0.2u∗(x, 0)
u(x, 0) = 2u∗(x, 0)
α = 1.75, ∆x = 0.0025, ∆t = 0.00001
0 1 2 3 4 5
0.3
0.2
0.1
0
FIGURE 17.13
Time development of u(0.1, 0) for Eq. (17.72) with f(x, t)=1−x2−α. The horizontal dotted
line repesents u∗(0.1).
The scheme is given by
u(m+1)
j − ∆t

Wju(m+1)
j−1 + W0u(m+1)
j + W−1u(m+1)
j+1 
= u(m)
j + ∆t


j
k=2
Wk

u(m)
j−k − u(m)
0

− (W−1 + W0 + W1) u(m)
0 + f(m)
j

, (17.104)
where Wk’s are given by Eqs. (17.91b)-(17.91e).
∆x
Emax
α = 1.75, t = 5, ∆t = 0.00001
0.002 0.004 0.006 0.008 0.01
0.01
0.008
0.006
0.004
FIGURE 17.14
Variation of Emax given by Eq. (17.97) in the numerical solution at t = 5 of the diffusion
Eq. (17.72) for the case of u(x, 0) = 2u∗(x) with ∆x.436 Fractional Order Partial Differential Equations
17.7.7 Gr¨unwald–Letnikov Approximation of Fractional Order Space
Derivative
Fractional order derivatives can be discretized by the GL approximation. The standard GL
discrete approximation of ∂αu/∂xα for α ∈ [1, 2] with L ≤ x ≤ R is
∂αu
∂xα = 1
Γ(−α) lim
jx→∞
1
(∆x)α

jx
k=0
Γ(k − α)
Γ(k + 1) u(x − k∆x, t), (17.105)
where ∆x = (x − L)/jx. With ∆t being the increment step size of t the finite-difference
scheme for the diffusion Eq. (17.72) is
u(m+1)
j = u(m)
j +
∆t
(∆x)αΓ(−α)

j
k=0
Γ(k − α)
Γ(k + 1) u(m)
j−k + ∆tf(m)
j . (17.106)
This scheme is found to be unstable (see Problem 20 at the end of the present chapter) [24].
The shifted Gr¨unwald formula is appropriate for Eq. (17.72). For α ∈ [1, 2] this formula
for Dαf(x) is
Dαf(x) = lim jx→∞
1
(∆x)α

jx
k=0
gkf(x − (k − 1)∆x) + O(∆x), (17.107)
where gk = Γ(k − α)/(Γ(−α)Γ(k + 1)). Applying this shifted Gr¨unwald formula, the dis￾cretized version of Eq. (17.72) is [26]
u(m+1)
j = u(m)
j + β

j+1
k=0
gku(m)
j−k+1 + ∆tf(m)
j , (17.108)
where β = ∆t/(∆x)α. The above explicit Euler scheme is shown to be stable for ∆t ≤
(∆x)α/α [26].
17.8 Concluding Remarks
In this chapter, a few fractional order linear partial differential equations with and without
the external force f(x, t) are considered. Finite-difference schemes are developed and the
stability condition for the schemes are obtained. In the illustrative examples, the time step
size is chosen such that the stability condition is satisfied. A reader can choose the time
step as out of the stability region and notice the unstable nature of the numerical solution
developed by the numerical algorithm. By suitably choosing the external force f(x, t) one
can make the system to display nondecaying solution. How to choose f(x, t) for realizing a
nonzero stationary solution is illustrated.
For space-fractional partial differential equations one example is considered. Appropriate
finite-difference scheme for other linear partial differential equations can be obtained and
analyzed. The numerical analysis of fractional order nonlinear partial differential equations
is not well established at present and hence is not considered in the present book. For some
details one may refer to the refs. [67-72].Bibliography 437
17.9 Bibliography
[1] C.P. Li and F.H. Zeng, Numerical Methods for Fractional Calculus. Chapman
and Hall/CRC Press, Boca Raton, 2015.
[2] C. Li and A. Chen, Int. J. Comput. Math. 95:1048, 2018.
[3] C. Li and F. Zeng, Int. J. Bifur. & Chaos 22:1230014, 2012.
[4] S.B. Yuste and L. Acedo, SIAM J. Numer. Anal. 42:1862, 2005.
[5] T.A.M. Langlands an B.I. Henry, J. Comput. Phys. 205:719, 2005.
[6] Z.Z. Sun and X. Wu, Appl. Numer. Math. 56:193, 2006.
[7] Y.M. Lin and C.J. Xu, J. Comput. Phys. 225:1533, 2007.
[8] E. Sousa, J. Comput. Phys. 228:4038, 2009.
[9] M.R. Cui, J. Comput. Phys. 228:7792, 2009.
[10] R. Du, W.R. Cao and Z.Z. Sun, Appl. Math. Model. 34:2998, 2010.
[11] K. Mustapha and J. Al Mustawa, Numer. Algor. 61:525, 2012.
[12] H.M. Nasir, B.L.K. Gunawardana and H.M.N. Abeyrathna, Int. J. Appl. Phys.
Math. 3:237, 2013.
[13] J.F. Huang, Y.F. Tang, L. Vazquez and J.Y. Yang, Numer. Algor. 64:707, 2013.
[14] A.A. Alikhanov, J. Comput. Phys. 280:424, 2015.
[15] G.H. Gao, H.W. Sun and Z.Z. Sun, J. Comput. Phys. 280:510, 2015.
[16] Y.M. Wang, BIT Numer. Math. 55:1187, 2015.
[17] A. Chen and C. Li, Numer. Funct. Anal. Opt. 37(1):19, 2016.
[18] X. Hu, F. Liu, I. Turner and V. Anh, Numer. Algor. 72:393, 2016.
[19] H. Chen, S. Lu and W. Chen, J. Comput. Phys. 315:84, 2016.
[20] H. Li, J. Cao and C. Li, J. Comput. Appl. Math. 299:159, 2016.
[21] W. Bu, A. Xiao and W. Zeng, J. Sci. Comput. 72:422, 2017.
[22] S. Cheng, N. Du, H. Wang and Z. Yang, Fract. Frac. 6:525, 2022.
[23] V.E. Lynch, B.A. Carreras, D. del-Castillo-Negrete, K.M. Ferriera-Mejias and
H.R. Hicks, J. Comput. Phys. 192:406, 2003.
[24] M.M. Meerschaert and C. Tadjeran, J. Comput. Appl. Math. 172:65, 2004.
[25] M. Illic, F. Liu, I. Turner and V. Anh, Fract. Calc. Appl. Anal. 8:323, 2005.
[26] M.M. Meerschaert and C. Tadjeran, Appl. Numer. Math. 56:80, 2006.
[27] M. Illic, F. Liu, I. Turner and V. Anh, Fract. Calc. Appl. Anal. 9:333, 2006.
[28] Q. Yang, F. Liu and I. Turner, Appl. Math. Model. 34:200, 2010.
[29] H. Wang and T.S. Basu, SIAM J. Sci. Comput. 34:A2444, 2012.
[30] H. Hejazi, T. Moroney and F. Liu, J. Comput. Appl. Math. 255:684, 2014.
[31] M.H. Chen and W.H. Deng, Appl. Math. Model. 38:3244, 2014.
[32] J.F. Huang, N.M. Nie and Y.F. Tang, Sci. China Math. 57:1303, 2014.
[33] W.Y. Tian, H. Zhou and W.H. Deng, Math. Comput. 84:1703, 2015.
[34] H.F. Ding and C.P. Li, Fract. Calc. Appl. Anal. 20:722, 2017.438 Fractional Order Partial Differential Equations
[35] R. Metzler and T.F. Nonnenmacher, Chem. Phys. 284:67, 2002.
[36] M. Meerschaert, D. Benson, H.P. Scheffler and B. Baeumer, Phys. Rev. E 65:1103,
2002.
[37] W.H. Deng, SIAM J. Numer. Anal. 47:204, 2008.
[38] S.J. Shen, F. Liu and V.Anh, Numer. Algor. 56:383, 2011.
[39] M.H. Chen, W.H. Deng and Y.J. Wu, Appl. Numer. Math. 70:22, 2013.
[40] Y. Liu, Y. Du, H. Li, S. He and W. Cao, Comput. Math. Appl. 70:573, 2015.
[41] Q. Liu, F.H. Zeng and C.P. Li, Int. J. Comput. Math. 92:1439, 2015.
[42] L.B. Feng, P. Zhuang, F. Liu, I. Turner and Y.T. Gu, Numer. Algor. 72:749, 2016.
[43] H. Sun, Z.Z. Sun and G.H. Hao, Appl. Math. Comput. 281:356, 2016.
[44] W.H. Deng and J.S. Hesthaven, Math. Model. Numer. Anal. 47:1845, 2013.
[45] M. Li, D. Xu and M. Luo, J. Comput. Phys. 255:471, 2013.
[46] W.P. Bu, Y.F. Tang and J.Y. Yang, J. Comput. Phys. 276:26, 2014.
[47] K. Mustapha. B. Abdallah and K.M. Furati, SIAM J. Numer. Anal. 52:2512,
2014.
[48] B. Jin, R. Lazarov, Y. Liu and Z. Zhou, J. Comput. Phys. 281:825, 2015.
[49] Y. Liu, Y.W. Du, H. Li and J.F. Wang, J. Appl. Math. Comput. 47:103, 2015.
[50] A. Sohail, K. Maqbool and R. Ellahi, Numer. Meth. Part. Diff. Eqs. 34:19, 2017.
[51] A.R. Haghighi, A. Dadv and H.H. Ghejlo, Commun. Adv. Comput. Sci. Appl.
2014. Article ID Cascsa-00024, 11 pages.
[52] J.Y. Yang, J.F. Huang, D.M. Liang and Y.F. Tang, Appl. Math. Model. 38:3652,
2014.
[53] D. Baffet and J.S. Hesthaven, SIAM J. Numer. Anal. 55:496, 2017.
[54] V.R. Hosslini, E. Shivanian and W. Chen, J. Comput. Phys. 312:307, 2016.
[55] R. Lazarov and Z. Zhou, SIAM J. Numer. Anal. 51:445, 2013.
[56] M. Modanli, Adv. Diff. Eqs. 2018:333, 2018.
[57] C. Cem and M. Duman, J. Comput. Phys. 231:1743, 2012.
[58] W.P. Bu, Y.F. Tang, Y.C. Wu and J.Y. Yang, Appl. Math. Comput. 257:355,
2015.
[59] C.M. Chen, F. Liu, I. Turner and V. Anh, J. Comput. Phys. 227:886, 2007.
[60] Y.G. Jiang and J.T. Ma, J. Comput. Appl. Math. 235:3285, 2011.
[61] C.P. Li and H.F. Ding, Appl. Math. Model. 38:3802, 2014.
[62] M.H. Chen and W.H. Deng, SIAM J. Numer. Anal. 52:1418, 2014.
[63] H.W. Sun and Z.Z. Sun J. Comput. Phys. 298:337, 2015.
[64] M.H. Ding and C.P. Li, Numer. Meth. Part. Diff. Eqs. 32:213, 2016.
[65] H. Li, J. Cao and C. Li, J. Comput. Appl. Math. 299:159, 2016.
[66] Z. Li, Z. Liang and Y. Yan, J. Sci. Comput. 71:785, 2017.
[67] C.P. Li, Z.G. Zhao and Y.Q. Chen, Comput. Math. Appl. 62:855, 2011.
[68] F.H. Zeng, F. Liu, C.P. Li, K. Burrage, I. Turner and V. Anh, SIAM J. Numer.
Anal. 52:2599, 2014.Problems 439
[69] W. Cao, F.H. Zeng, Z. Zheng and G.E. Karniadakis, SIAM J. Sci. Comput.
38:A3070, 2016.
[70] C.P. Li, Q. Yi and A. Chen, J. Comput. Phys. 316:614, 2016.
[71] A.Q.M. Khaliq, X. Liang and K.M. Furati, Numer. Algor. 75:147, 2017.
[72] X. Liang, A.Q.M. Khaliq, H. Bhatt and K.M. Furati, Numer. Algor. 76:939, 2017.
[73] V. Balakrishnan, Physica A 132:569, 1985.
[74] W. Wyss, J. Math. Phys. 27:2782, 1986.
[75] W.R. Schneider and W. Wyss, J. Math. Phys. 30:134, 1989.
[76] R. Metzler and J. Klafter, Phys. Rep. 339:1, 2009.
[77] P. Sakrajda and M. Slawomir Wiraszka, Fractional Order Diffusion Model
for Social Networks. Proceedings of International Conference on Fractional
Differentiation and its Applications (ICFDA) 2018. Available at SSRN:
https://ssrn.com/abstract=3271330 or http://dx.doi.org/10.2139/ssrn.3271330.
[78] Nirupama Bhattacharya, Fractional Diffusion: Numerical Methods and Applica￾tions in Neuroscience. Ph.D. Thesis, University of California, San Diego, 2014.
[79] D. Benson, S. Wheatcraft and M. Meerschaert, Water Resour. Res. 36:1403, 2000.
[80] J.P. Aguilar, J. Korbel and Y. Luchko, Mathematics 7:796, 2019.
[81] A.V. Pskhu and S.Sh. Rekhviashvili, Theor. Math. Phys. 75:316, 2020.
[82] M. Weilbeer, Efficient Numerical Methods for Fractional Differential Equations
and Their Analytical Background. Verlag, Nicht, Ermittelbar, 2005.
[83] A.K. Gr¨unwald, Z. Angrew. Math. Phys. 12:441, 1867.
[84] A.V. Letnikov, Mat. Sh. 3:1, 1868.
[85] I. Podlubny, Fractional Differential Equations: An Introduction to Fractional
Derivatives, Fractional Differential Equations, to Methods of Their Solution and
Some of Their Applications. Elsevier, Amsterdam, 1998.
[86] M.D. Ortigueira and F. Coito, Int. J. Theory Appl. 7:459, 2004.
[87] K.B. Oldham and J. Spanier, The Fractional Calculus. Academic Press, New
York, 1974.
[88] P. Straka, M.M. Meerschaert, R.J. McGough and Y. Zhou, Fract. Cal. Appl. Anal.
16:262, 2013.
[89] T. Sandev and Z. Tomovski, Fractional wave equations. In: Fractional Equa￾tions and Models. Developments in Mathematics, vol 61, 2019. Springer, Cham.
https://doi.org/10.1007/978-3-030-29614-8−5.
[90] O.P. Agrawal, Z. Angew. Math. Mech. 83(4):265, 2003.
17.10 Problems
17.1 Show that when α = 1 Eq. (17.10a) becomes the classical finite-difference scheme
(17.7) of Eq. (17.3).
17.2 Substituting ζm+1 = ξζm in Eq. (17.12) arrive at the Eq. (17.13).440 Fractional Order Partial Differential Equations
17.3 For the diffusion Eq. (17.4) construct the exact analytical solution subjected to
the conditions given by (17.19) by the variable separable method.
17.4 Study the time evolution of the initial Gaussian wave function
u(x, 0) = 1
σ
√2π
e−(x−x0)2/(2σ2) , x ∈ [−20, 20]
according to the diffusion Eq. (17.4) with x0 = 0, σ = 0.5 and α = 0.75.
17.5 Consider Eq. (17.21) with f(x, t) = sin πx. Compute the numerical solution u(x, t)
with u(x, 0) = 0, u(0, t) = u(1, t) = 0, ∆x = 1/25 and ∆t = 0.00005. Plot
u(0.48, t) versus t and verify that as t increases from 0, u(0.48, t) approaches to
0.10112.
17.6 For the diffusion Eq. (17.21) with Kα = 1 choose u(x, t) = tsin 2πx. For this
solution find f(x, t). Compute the numerical solution for α = 0.75, ∆x = 1/25
and ∆t = 0.00005. Compare the numerical solution with the exact solution for
some selected values of t.
17.7 For the time-fractional diffusion Eq. (17.21) with f(x, t) = x(1 − x), u(0, t) =
u(1, t) = 0 and u(x, 0) = 0 determine the equilibrium solution u∗(x) and then
u(x, t). Numerically solve the Eq. (17.21) with ∆x = 0.05, ∆t = 0.0001 and
α = 0.75. Compare the numerical solution with the exact solution.
17.8 Perform the stability analysis for the finite-difference scheme (17.34) for the
advection-diffusion Eq. (17.29) and arrive the result given by Eq. (17.35).
17.9 The exact solution of Eq. (17.29) for Kα = Vα = 1 with f(x, t)=2x − 2 +
2t
2−α/Γ(3 − α) is u(x, t) = x2 + t
2, 0 ≤ x ≤ 1. Compute the numerical solution
u(x, t) with ∆x = 1/25, ∆t = 0.00005 at t = 0.5 for several values of α in the
interval [0, 1]. Compare the numerical solution with the exact solution.
17.10 For the advection-diffusion Eq. (17.29) with Kα = Vα = 1 and f(x, t)=3 − 2x
determine the equilibrium solution u∗(x). With u(x, 0) = 0 and u(x, 0) = 1.5u∗(x)
study the time evolution of the initial wave profile by suitably choosing ∆x and
∆t for α = 0.75. Plot u(x, t) as a function of t for a value of x and state the
observation.
17.11 For the wave Eq. (17.38) subjected to the conditions u(x, 0) = ut(x, 0) = 0 =
u(0, t) = u(π, t) = 0 and f(x, t) = sin x

Γ(2 + α)t + t
1+α
the exact solution is
u(x, t) = t
1+α sin x. With ∆x = π/200 compute the numerical solution at t = 0.5
for α = 1, 1.25, 1.5, 1.75 and 2. For each value of α find (∆t)c and then choose
∆t appropriately. Compare the numerical solution with the exact solution.
17.12 Consider the wave Eq. (17.38) with f(x, t) = sin x and driven by the external
time-periodic force F cos(2πt). With ∆x = π/200, ∆t = 0.005 and α = 1.75
compute the numerical solution and plot u(π/2, t) for t ∈ [0, 30]. Verify that
after a transient evolution, u(π/2, t) exhibits periodic oscillation with period 1.
Further, confirm the periodicity of u(x, t) by plotting the solution for t ∈ [30, 33].
17.13 Obtain the stability condition of the scheme (17.52) for the damped wave
Eq. (17.48).
17.14 For the damped wave Eq. (17.48) determine the exact analytical solution for
f(x, t) = sin x. Numerically compute the solution for a chosen value of α and
compare it with the exact solution.Problems 441
17.15 For the Fisher equation
∂αu
∂tα +
∂u
∂x = t
1−α
Γ(2 − α)
sin x + t cos x, x ∈ [0, 2π], α ∈ [0, 1]
determine the exact analytical solution. Applying the finite-difference scheme
(17.62) compute the numerical solutions for α = 1, 0.75 and 0.5 at t = 0.5. What
is the effect of α on u(x, t)? Compare the numerical solutions with the exact
solutions by plotting the error in the solution with x.
17.16 Show that for the assumed solution (17.94) of the diffusion Eq. (17.72) the f(x, t)
is given by Eq. (17.96).
17.17 Determine f(x, t) in the diffusion Eq.(17.72) corresponding to the solution
u(x, t)=e−t
x(1 − x). Then, numerically compute the solution for α = 1.75
and compare it with the exact solution at t = 0.5.
17.18 For the space-fractional diffusion Eq. (17.72) determine the stationary solution
u∗(x) for f(x, t) = x(1 − x) subjected to the conditions (17.80). Compute the
numerical solution for u(x, 0) = 0 and α = 1.75 and verify that it approaches to
u∗ in the long-time limit.
17.19 Assume the initial wave solution of the space-fractional diffusion Eq. (17.72) as
u(x, 0) = 1
σ
√2π
x2e−(x−x0)2/(2σ2) , x ∈ [−20, 20]
with x0 = 0 and σ = 0.5. Study the time evolution of this initial solution by
computing the numerical solution for α = 1.75.
17.20 Consider the finite-difference scheme (17.106) of the diffusion Eq. (17.72) ob￾tained using the standard Gr¨unwald approximation for the fractional order space
derivative. Assume that the perturbed value of u(0)
j as u(0)
j = u(0)
j + ξ
(0)
j and it
leads the perturbation in u(1)
j as u(1)
j = u(1)
j + ξ
(1)
j . Obtain an expression for ξ
(1)
j
in term of ξ
(0)
j and then ξ
(m)
j = µ(m)
j ξ
(0)
j [24]. Show that µ(m)
j is > 1 and hence
the scheme is unstable.18
Fourier Analysis and Power Spectrum
18.1 Introduction
When a given data set exhibits a periodic behaviour it is desired to construct an appropriate
trigonometric function through a curve fitting. In many engineering problems, signal analysis
and dynamical systems theory it is often required to know the various frequency components
present in the solutions. The frequencies present in a solution can be identified through a
spectral analysis. For a sufficiently large data set, one can construct an appropriate Fourier
series and Fourier transform.
Fourier analysis is a standard tool for decomposing periodic as well as non-periodic
solutions or non-periodic functions into an infinite number of sinusoidal functions such
as sine and cosine terms. Fourier series is an appropriate tool for approximating periodic
functions while the Fourier transform is the most suitable tool for non-periodic functions.
This chapter is devoted to the numerical construction of Fourier series and Fourier
transform. For a complex form of Fourier series expansion of a function, the analytical
calculation of the Fourier coefficients is reviewed. The numerical computation of the Fourier
coefficients by employing the composite trapezoidal rule is outlined with an example. As this
way of calculation of the Fourier coefficients is time-consuming the fast Fourier transform
technique is explained. Next, the power spectrum of a function is defined and its significance
is illustrated with an example. The power spectrum is determined exactly for a constant and
a periodic function. Then, the problem of discrete Fourier transform is introduced and the
computation of its frequency components by a simplified fast Fourier transform is described.
The features of the numerically computed power spectrum of periodic, quasiperiodic and
chaotic solutions are discussed. The power spectrum of a set of uniformly distributed random
numbers is presented.
18.2 Fourier Series
Problems involving oscillations or vibrations are common in physics, engineering and chem￾istry. A vibrating tuning fork, to and fro motion of a wall-clock pendulum and sound waves
are some examples of such systems. Mathematically, such oscillatory/vibrating processes
are described by appropriate periodic functions, often by sine or cosine functions. Musical
instruments give a fundamental tone accompanied by a number of overtones of frequencies
which are integral multiples of the fundamental frequency. The combination of the funda￾mental and overtones is generally a complicated periodic function. If the goal is to construct
the complicated function as a sum of terms corresponding to various overtones (harmonics)
including fundamental, it will require, in general, an infinite series of terms called Fourier
series.
DOI: 10.1201/9781032649931-18 442Fourier Series 443
The complex form of a Fourier series expansion of a function f(t) with period-T is given
by
f(t) = ∞
k=−∞
Ckeikωt , ω = 2π/T , (18.1)
where Ck is the Fourier coefficient corresponding to the wave number k. Ck provides
information about the various harmonics contributing to f(t) and their strengths. Let us
first outline the calculation of the Fourier coefficients.
For convenience write Eq. (18.1) as
f(t) = C0 +∞
k=1
(Ck + C−k) cos kωt +∞
k=1
i (Ck − C−k) sin kωt. (18.2)
Defining
ak = Ck + C−k , bk =i(Ck − C−k) , a0
2 = C0 (18.3)
Eq. (18.2) becomes
f(t) = a0
2 +∞
k=1
ak cos kωt +∞
k=1
bk sin kωt. (18.4)
The term a0/2 in the Fourier series allows the possibility that f(t) oscillates about an
average value other than zero. Substitution of ω = 2π/T in Eq. (18.4) gives
f(t) = a0
2 +∞
k=1
ak cos
2πk
T t +∞
k=1
bk sin
2πk
T t . (18.5)
The equations for a’s and b’s are obtained as follows.
Integration of Eq. (18.5) with respect to t from 0 to T leads to
 T
0
f(t) dt =
 T
0

a0
2 +∞
k=1
ak cos
2πk
T t +∞
k=1
bk sin
2πk
T t

dt
= a0
2
T . (18.6)
That is,
a0 = 2
T
 T
0
f(t) dt . (18.7a)
To determine ak multiply Eq. (18.5) by cos(2πk
t/T) and integrate from 0 to T:
 T
0
f(t) cos 2πk
T t dt =
 T
0

a0
2 +∞
k=1
ak cos
2πk
T t
+
∞
k=1
bk sin
2πk
T t

cos
2πk
T t dt
= ak
2
T. (18.8)444 Fourier Analysis and Power Spectrum
Therefore,
ak = 2
T
 T
0
f(t) cos 2πk
T t dt . (18.7b)
To determine bk’s multiply Eq. (18.5) by sin(2πk
t/T) and integrate from 0 to T. The result
is
 T
0
f(t) sin 2πk
T t dt = bk
2
T . (18.9)
That is,
bk = 2
T
 T
0
f(t) sin 2πk
T t dt . (18.7c)
When T = 2π with 0 ≤ t < 2π the Fourier series is written as
f(t) = a0
2 +∞
k=1
ak cos kt +∞
k=1
bk sin kt, (18.10a)
where
ak = 1
π
 2π
0
f(t) cos kt dt, k = 0, 1, 2, ... (18.10b)
bk = 1
π
 2π
0
f(t) sin kt dt, k = 1, 2, ... . (18.10c)
For f(t) of period-2π the Fourier series (18.1) is rewritten as
f(t) ≈ 2
∞
k=0
|Ck| cos (θk + kt) , (18.11a)
where
Ck = |Ck| eiθk and Ck = 1
2π
 2π
0
f(t)e−ikt dt . (18.11b)
In Eq. (18.11b), f(t) is expressed as a sum of simple harmonic oscillations. The kth mode
is
fk = 2 |Ck| cos (θk + kt) . (18.12)
fk is periodic with period 2π/k with phase angle θk. It has an amplitude 2|Ck|, frequency
k/2π and an angular frequency k. |Ck| is a measure of the extent to which a simple harmonic
motion of the angular frequency k is present in the total motion.
For a certain periodic forms of f(t) like rectangular, triangular and linear functions the
Fourier coefficients can be determined analytically by exactly integrating the integrals in
Eqs. (18.10). However, analytic integration is possible only for very limited functions. If
f(t) is given as a table of, say, N values or if it is difficult to evaluate the integrals in
Eqs. (18.10) analytically for a given analytical form of f(t) it is desirable to evaluate these
integrals numerically. The next section discusses the construction of the Fourier coefficients
numerically.Numerical Calculation of the Fourier Coefficients 445
18.3 Numerical Calculation of the Fourier Coefficients
Let f(t) is periodic with period-2π and the values of f(t) are known in the interval 0 to
2π at equidistant points tm = m∆t, m = 1, 2, ..., N with ∆t = 2π/N. Denote f(tm) as fm.
Application of the composite trapezoidal rule
 2π
0
f(t) dt ≈ ∆t

N
m=1
f(m∆t) (18.13)
to the integrals in Eqs. (18.10) results in
a0 = ∆t
π

N
m=1
fm , (18.14a)
ak = ∆t
π

N
m=1
fm cos k(m∆t), k = 1, 2,... (18.14b)
bk = ∆t
π

N
m=1
fm sin k(m∆t), k = 1, 2,... . (18.14c)
The above approximation is first-order in ∆t and hence numerically computed coefficients
may not coincide exactly with the true values of the coefficients. In practice, if the absolute
value of a Fourier coefficient is less than a pre-assumed small number δ (say 10−3) then
its value is assumed as zero. Equations (18.14) are used to compute Fourier coefficients of
certain periodic functions.
Example:
Compute the Fourier coefficients of the square-wave
f(t) =



α, 0 ≤ t ≤ π
−α, π < t < 2π.
(18.15)
The exact Fourier series of the above square-wave is
f(t) = α ∞
k=odd
4
kπ sin kt. (18.16)
The first few values of ak and bk were computed numerically using Eqs. (18.14) with N =
214 = 16384, α = 1 are given in Table 18.1. Table 18.2 presents the numerically computed
values of a0, a1 and b1 for various values of N. As N increases the values of a0, a1 and b1
(as well as other a’s and b’s) approach the true values.
The calculation of ak’s and bk’s using Eqs. (18.14) is time-consuming. These coefficients
can be calculated very quickly by the fast Fourier transform technique.
18.4 Fourier Transform and Power Spectrum
In order to express a function f(t) in a Fourier series, it has to be periodic with period
T, T being finite. For a nonperiodic function defined over infinite interval of t, one needs a446 Fourier Analysis and Power Spectrum
TABLE 18.1
The numerically calculated some of the Fourier coefficients (ak and bk) and their exact
values (denoted as ake and bke, respectively) for the square-wave given by Eq. (18.15),
where α = 1.
k ak ake bk bke
0 0.00000 0.00000
1 −0.00024 0.00000 1.27324 1.27324
2 0.00000 0.00000 0.00000 0.00000
3 −0.00024 0.00000 0.42441 0.42441
4 0.00000 0.00000 0.00000 0.00000
5 −0.00024 0.00000 0.25465 0.25465
6 0.00000 0.00000 0.00000 0.00000
7 −0.00024 0.00000 0.18189 0.18189
8 0.00000 0.00000 0.00000 0.00000
9 −0.00024 0.00000 0.14147 0.14147
10 0.00000 0.00000 0.00000 0.00000
11 −0.00024 0.00000 0.11575 0.11575
12 0.00000 0.00000 0.00000 0.00000
13 −0.00024 0.00000 0.09794 0.09794
14 0.00000 0.00000 0.00000 0.00000
15 −0.00024 0.00000 0.08488 0.08488
16 0.00000 0.00000 0.00000 0.00000
17 −0.00024 0.00000 0.07490 0.07490
18 0.00000 0.00000 0.00000 0.00000
19 −0.00024 0.00000 0.06701 0.06701
20 0.00000 0.00000 0.00000 0.00000
TABLE 18.2
Variations of the coefficients a0, a1 and b1 with N for the square-wave (Eq. (18.15)) with
α = 1. The exact values of a0, a1 and b1 are 0, 0 and 1.27324, respectively.
N a0 a1 b1 N a0 a1 b1
28 0 −0.01562 1.27318 212 0 −0.00098 1.27324
29 0 −0.00781 1.27322 213 0 −0.00049 1.27324
210 0 −0.00391 1.27324 214 0 −0.00024 1.27324
211 0 −0.00195 1.27324 215 0 −0.00012 1.27324
different representation called Fourier integral or Fourier transform. The Fourier integral is
thought of as the limiting case of a Fourier series of f(t) defined in the interval −T <t<T
as T → ∞. This representation forms the basis for the Fourier transform.Fourier Transform and Power Spectrum 447
The Fourier transform of a function f(t) is given by
¯f(Ω) = 1
√2π
 ∞
−∞
f(t)e−iΩt dt . (18.17)
Knowing ¯f(Ω) the corresponding function is obtained through the inverse transform
f(t) = 1
√2π
 ∞
−∞
¯f(Ω)eiΩt dΩ . (18.18)
Essentially, the Fourier transform decomposes a signal into complex exponential functions
of different frequencies. f(t) is thought of as a signal in time domain. In this case ¯f(Ω)
denotes the signal in frequency domain. If ¯f(Ω) is large at Ω = Ω then f(t) has a dominant
spectral component at Ω = Ω
. That is, the major part of f(t) is made of frequency Ω
.
When ¯f(Ω) = 0 or very small for Ω = Ω then f(t) does not contain the frequency Ω
. ¯f(Ω)
is generally a complex-valued function. Therefore, one can define a real-valued function by
taking the modulus square of ¯f(Ω) and call the real function as power spectrum P(Ω) of
f(t), that is,
P(Ω) = | ¯f(Ω)|
2 . (18.19)
18.4.1 Significance of ¯f(Ω) and Power Spectrum
Suppose that the inverse transform f(t) given by Eq. (18.18) is considered as the superpo￾sition of sinusoidal oscillations of all possible frequencies. Then, ¯f(Ω) gives the intensity of
f(t) in the interval Ω to Ω + dΩ where dΩ is small. For an oscillatory system the integral
 ∞
−∞
|x¯(Ω)|
2dΩ (18.20)
is interpreted as the total energy [1] where x(t) is the displacement of the system from the
equilibrium position or rest state. For example, the Hamiltonian or the energy of a linear
harmonic oscillator is
E = H = 1
2
v2 +
1
2
ω2
0x2 , (18.21)
where v = ˙x is the velocity and ω2
0 is a constant. The equation of motion of the oscillator is
x¨ + ω2
0x = 0 . (18.22)
The general solution of Eq. (18.22) is
x(t) = a1 cos ω0t + b1 sin ω0t (18.23)
which can be rewritten as
x(t) = C1eiω0t + C−1e−iω0t . (18.24)
Then,
E = −1
2
ω2
0

C1eiω0t − C−1e−iω0t
2
+
1
2
ω2
0

C1eiω0t + C−1e−iω0t
2
= 2ω2
0C1C−1
= 2ω2
0|C1|
2 . (18.25)
Energy is proportional to the square of the amplitude |C1|. Note that if x(t) is represented
by a Fourier series then the result is a series of squares |Ck|
2 instead of only one |C1|
2.448 Fourier Analysis and Power Spectrum
The set of quantities |Ck|
2 is called a discrete power spectrum. Comparison of Eqs. (18.11)
and (18.17) indicates that for a periodic function with period-2π (or T) Ck is the Fourier
transform of x(t).
Example 1:
Calculate the Fourier transform ¯f(Ω) and the power spectrum of f(t) = α, a constant.
Equation (18.17) becomes
¯f(Ω) = 1
√2π
 ∞
−∞
αe−iΩt dt = α
√2π
e−iΩt
−iΩ





∞
−∞
. (18.26)
Then,
P(Ω) = | ¯f(Ω)|
2 = α2
2π
e−iΩt
eiΩt
−iΩ × iΩ





∞
−∞
= α2
2πΩ2 . (18.27)
Thus, as Ω increases P(Ω) decays to zero.
Example 2:
Find the Fourier transform of f(t) = cos ωt.
The result is
¯f(Ω) = 1
√2π Real part of  ∞
−∞
eiωte−iΩt dt
= 1
√2π R.P. 2πδ(ω − Ω), (18.28)
where δ(ω − Ω) is the Dirac-delta function.
If f(t) is given for a discrete set of values of t then the resultant Fourier transform is
called a discrete Fourier transform [2]. For f = cos ωt from (18.28) note that the discrete
Fourier transform ¯f(Ω) has sharp peaks at Ω = ω and at integral multiples of Ω. The next
section deals with the problem of discrete Fourier transform.
18.5 Discrete Fourier Transform
Let us consider a function f(t) written in time series as fm = f(tm), where tm = m∆t,
m = 0, 1,...,N −1 and ∆t is the time duration with which the data are collected. Applying
the composite trapezoidal rule
 2π
0
y(t) dt ≈ 2π
N
N
−1
m=0
y (2πm/N) , (18.29)
where y(t) is a periodic function with period 2π, to the integral in Eq. (18.11b) gives
Ck = 1
N
N
−1
m=0
f (2πm/N) e−i2πkm/N . (18.30)Fast Fourier Transform 449
Now, represent a periodic orbit or a time series 
f p
1 , f p
2 ,...,f p
q
 with period q = 2p by the
Fourier expansion
f p
k = 
j
Cp
j ei2Ωjk , (18.31)
where the frequency Ωj = 2πj/2p, j = 0, 1,..., 2p−1. f p
k is called a discrete Fourier trans￾form [2] of the given time series. Note that the complex Fourier transform of eiωkt is
2πδ(Ω − ω). (18.32)
Therefore, the Fourier transform of the above periodic orbit is
¯f(Ω) = 2π

j
Cp
j δ(Ω − ωj ). (18.33)
Thus, the power spectrum
P(Ω) = | ¯f(Ω)|
2 (18.34)
will consist of delta functions at the frequencies ωj with amplitudes |Cp
j |
2. Later, computed
power spectra of different types of periodic and nonperiodic time series and their features
will be presented.
From Eq. (18.30) notice that the evaluation of a particular Ck needs N additions. There￾fore, for the calculation of N numbers of C’s totally O(N2) operations are required. Con￾sequently, a computer program will take an amount of computing time proportional to
N2. For 103 sample points, the number of operations required is O(106) and is a serious
time-consuming computation. That is, the straight forward calculation of discrete Fourier
transform is very inefficient. An alternate and efficient method is the fast Fourier transform
and is described in the next section.
18.6 Fast Fourier Transform
In order to reduce the number of arithmetic operations in the calculation of Ck, the fast
Fourier transform (FFT) algorithm was proposed by Cooley and Tukey [3]. In this method,
the number of arithmetic operations is only O(N log2 N). For 103 sample points the dis￾crete Fourier transform will need O(106) arithmetic operations while the FFT requires only
O(103 log2 103).
18.6.1 A Simplified Procedure
The FFT uses the advantages of symmetry properties of trigonometric functions at the
points of calculation. Essentially, the terms in the summation in Eq. (18.30) are rearranged
and the summation is performed in a hierarchical way as outlined in the following.
In Eq. (18.30) separate the odd and even terms as
Ck = 1
N
N/

2−1
m=0
f2me−i2π(2m)k/N +
1
N
N/

2−1
m=0
f2m+1e−i2π(2m+1)k/N
= uk + vke−i2πk/N , (18.35a)450 Fourier Analysis and Power Spectrum
where
uk = 1
N
N/

2−1
m=0
f2me−i2πmk/(N/2) , (18.35b)
vk = 1
N
N/

2−1
m=0
f2m+1e−i2πmk/(N/2) . (18.35c)
In Eq. (18.35) Ck is written into two summations. Each summation has N/2 terms. Repeat
the above process until there would be only two terms in each summation. This is possible
if N = 2M, where M is an integer.
In the case of 2M=3 = 8 data points f0, f1, ..., f7
Ck = 
7
m=0
fme−i2πmk/N
= 
3
m=0
f2me−i2π2mk/N + 
3
m=0
f2m+1e−i2π(2m+1)k/N
= 
1
m=0
f4me−i2π4mk/N + 
1
m=0
f4m+2e−i2π(4m+2)k/N
+ 
1
m=0
f4m+1e−i2π(4m+1)k/N + 
1
m=0
f4m+3e−i2π(4m+3)k/N .. (18.36)
There is another symmetry between the coefficients Ck for k < N/2 and k ≥ N/2, namely,
Ck = uk + vke−i2πk/N , (18.37a)
Ck+N/2 = uk − vkei2πk/N , k = 0, 1,..., N
2 − 1. (18.37b)
The above equations suggest that immediately after computing vkei2πk/N one can compute
both the coefficients Ck and Ck+N/2.
Once the summation in Eq. (18.30) is split then M times (with each summation con￾taining only two terms) the individual data points have to be added in pairs. But the points
in each pair at the first level of additions can be very far apart in the time series because of
grouping of even and odd terms in each level of splitting. For example, in Eq. (18.36) with
2M = 8 data points the first summation contains addition of the data points f0 and f4,
the second summation contains addition of the data points f2 and f6 and so on. To handle
this situation Cooley and Tukey suggested indexing the data string with binary numbers.
The advantage of this is that pair of the data in each summation is represented by a binary
number and its bit reversed order. For 8 data points the indexes representing the data are
0 to 7. In binary representation they are
000, 001, 010, 011, 100, 101, 110, 111. (18.38)
The bit reversed-order of this sequence is
000, 100, 010, 110, 001, 101, 011, 111, (18.39)
that is, in number system the sequence is 0, 4, 2, 6, 1, 5, 3, 7. Now, perform the first level
of addition between f0 and f4, f2 and f6, f1 and f5 and f3 and f7. After the bit reversalFast Fourier Transform 451
apply Eq. (18.37) repeatedly with the addition of pairs which are 2l−1 spaces apart. Here,
the index l indicates the level of additions.
The following is the systematic procedure of computing power spectrum for a real set
of data by FFT:
(1) Read the N = 2M data and store them in the variable CR (real part of the data).
Set CI’s (imaginary part of the data) zero.
(2) Rearrange the data in the bit reversed order.
(3) Perform the addition of pair of points obtained from step (2) at M levels and
obtain the Fourier coefficients.
(4) Calculate the power spectral density of the Fourier coefficients.
18.6.2 Advantage of the Fast Fourier Transform
To appreciate the feature of FFT algorithm, consider a case with N = 2M data points. In the
basic discrete Fourier transform equation N complex multiplications are needed to compute
one coefficient Ck. Since, there are N coefficients Ck the number of complex multiplications
is N2. In FFT algorithm after the bit reversal the number of levels of additions becomes
M and each level consists of N/2 additions. Therefore, the total number of operations is
M ·(N/2). Since M = log2 N the above number is (N/2) log2 N or O(N log2 N). Thus, the
total computing time in FFT is proportional to O(N log2 N) and is significantly less than
the direct implementation of discrete Fourier transform.
18.6.3 Fourier Coefficients from the Fast Fourier Transform
The Fourier coefficients a’s and b’s can be obtained from the coefficients C’s through
Eqs. (18.3). Calculation of Fourier series through FFT technique is much faster than the
calculation of it using Eqs. (18.14).
Table 18.3 gives the numerically calculated values of the Fourier coefficients for the
square-wave given by Eq. (18.15). Since the exact values of a0, ak’s and even bk’s are
zero, only the odd values of bk’s are given. The values of the coefficients a0, a1 and b1
are calculated for a few values of N, the number of data points. The values of the Fourier
coefficients converge to the exact values with increase in the number of data N. Table 18.4
presents the result.
Table 18.5 presents the Fourier coefficients for the function
f(t) =



t/π, 0 ≤ t ≤ π
0, π<t< 2π.
(18.40)
The exact Fourier series of (18.40) is given by
a0 = 1/2, ak =



0, k = 2, 4,...
−2/

k2π2
, k = 1, 3,...
(18.41a)
bk = −(−1)k
kπ , k = 1, 2,... . (18.41b)452 Fourier Analysis and Power Spectrum
TABLE 18.3
The numerically calculated Fourier coefficients of the square-wave, Eq. (18.15) (with α = 1),
through FFT. Here, a0 = 0, ak = 0.00024, k = 1, 2,... and bk = 0 for k = 2, 4,... . bkn and
bke denotes the numerically computed and the exact values of b’s.
k bkn bke k bkn bke
1 1.27324 1.27324 27 0.04716 0.04716
3 0.42441 0.42441 29 0.04390 0.04390
5 0.25465 0.25465 31 0.04107 0.04107
7 0.18189 0.18189 33 0.03858 0.03858
9 0.14147 0.14147 35 0.03638 0.03638
11 0.11575 0.11575 37 0.03441 0.03441
13 0.09794 0.09794 39 0.03265 0.03265
15 0.08488 0.08488 41 0.03105 0.03105
17 0.07490 0.07490 43 0.02961 0.02961
19 0.06701 0.06701 45 0.02829 0.02829
21 0.06063 0.06063 47 0.02709 0.02709
23 0.05536 0.05536 49 0.02598 0.02598
25 0.05093 0.05093
TABLE 18.4
The numerically computed values of the Fourier coefficients a0, a1 and b1 for a few values
of N for the square-wave, Eq. (18.15) (with α = 1).
N a0 a1 b1 N a0 a1 b1
28 0.01563 0.0 1.27318 212 0.00098 0.0 1.27324
29 0.00781 0.0 1.27322 213 0.00049 0.0 1.27324
210 0.00391 0.0 1.27324 214 0.00024 0.0 1.27324
211 0.00195 0.0 1.27324
18.6.4 Power Spectrum of Certain Functions
In real physical and engineering problems a time series x(t) can be thought of as a component
of the solution of the equation of motion of a system or a signal of an electrical system and
so on. A simple way to check for periodic and nonperiodic nature of x(t) is to compute its
power spectrum.
Let us present the power spectrum computed using FFT for various types of x(t). The
spectrum of a constant, that is x(t) = constant, contains a single peak at zero frequency.
The power spectrum of a periodic x(t) with frequency ω1 has Dirac-delta peaks at ω1 and its
various harmonics. This will be the case if the Fourier series of x(t) has an infinite number
of nonzero Fourier coefficients a’s and b’s. If the Fourier series of x(t) has only a finite
number of sinusoidal functions with frequencies ωi, i = 1, 2,...,n then the power spectrum
will have peaks only at the frequencies ωi.Fast Fourier Transform 453
TABLE 18.5
The numerically calculated Fourier coefficients of the function given by Eq. (18.40) through
FFT. akn and bkn are the numerically computed values of the Fourier coefficients. ake and
bke are their respective exact values.
k akn ake bkn bke
0 0.49994 0.50000
1 −0.20258 −0.20264 0.31831 0.31831
2 −0.00006 0.00000 −0.15915 −0.15915
3 −0.02245 −0.02252 0.10610 0.10610
4 −0.00006 0.00000 −0.07958 −0.07958
5 −0.00804 −0.00811 0.06366 0.06366
6 −0.00006 0.00000 −0.05305 −0.05305
7 −0.00407 −0.00414 0.04547 0.04547
8 −0.00006 0.00000 −0.03979 −0.03979
9 −0.00244 −0.00250 0.03537 0.03537
10 −0.00006 0.00000 −0.03183 −0.03183
11 −0.00161 −0.00167 0.02894 0.02894
12 −0.00006 0.00000 −0.02653 −0.02653
13 −0.00114 −0.00120 0.02449 0.02449
14 −0.00006 0.00000 −0.02274 −0.02274
15 −0.00084 −0.00090 0.02122 0.02122
For all the following examples, 214 (= 16384) data are used. The data are collected at
every integer multiple of the time interval 2π/100 unless otherwise specified.
A. Sinusoidal Time Series
Figure 18.1a presents the power spectrum of x(t) = sin 5t. Since, x(t) is a sinusoidal func￾tion with a single frequency ω = 5 the power spectrum shows a peak at ω(= Ω/2π) = 5.
The repetition period of sin 5t is 2π/5. Figure 18.1b depicts the power spectrum correspond￾ing to x(t) = sin 5t + 10 sin 12t. It has peaks at Ω = 5 and 12. The Fourier coefficients of
this x(t) are b5 = 1 and b12 = 10 and all other coefficients are 0. The amplitudes of the
peaks at ω = 5 and 12 in log10 scale are 0 and 2, respectively. The power spectrum gives
log10 
b2
5

= 0 and log10 
b2
12
= 2, that is, b5 = √
100 = 1 and b12 = √
102 = 10 and are in
fact the exact values of the Fourier coefficients. The point is that from the power spectrum
one can obtain the various frequency components present in the time series x(t) and as well
as their respective amplitudes.
B. Square-Wave
Next, consider the square-wave given by Eq. (18.15). Its exact Fourier series given by
Eq. (18.16) has an infinite number of Fourier components. Odd integer coefficients bk’s are
nonzero. Its power spectrum shown in Fig. 18.1c has peaks at the fundamental frequency
and odd integral multiples of the fundamental frequency. The period of the square-wave
considered here is 2π. The number of components of the power spectrum will double when
the period of x(t) is doubled. An example is given in Fig. 18.1d. This spectrum is for the454 Fourier Analysis and Power Spectrum
(a)
Ω × (1/2π)
Log10
P
0 10 20 30 40 50
0
-4
-8
(b)
Ω × (1/2π)
Log10
P
0 10 20 30 40 50
0
-4
-8
(c)
Ω × (1/2π)
Log10
P
0 5 10 15 20
0
-4
-8
(d)
Ω × (1/2π)
Log10
P
0 5 10 15 20
0
-4
-8
FIGURE 18.1
The power spectra of (a) x(t) = sin 5t, (b) x(t) = sin 5t + 10 sin 12t, (c) square-wave of
period 2π, Eq. (18.15), and (d) square-wave of period 4π, Eq. (18.42).
square-wave with period-4π defined as
f(t) =



α, 0 ≤ t ≤ 2π
−α, 2π<t< 4π,
(18.42)
where α = 1.Fast Fourier Transform 455
(a)
x
y
0.9 1 1.1 1.2
-0.9
-1.1
-1.3
(b)
Ω × (1/2π)
Log10
P
0 5 10 15 20
0
-5
-10
FIGURE 18.2
(a) A period-T solution of Eq. (18.43) and (b) the power spectrum of x(t). The values of
the parameters are fixed as β = 1, ν = 0.015, a = −1.02, b = −0.55 and ω1 = 1, ω2 = 0,
f1 = 0.1 and f2 = 0.
C. Periodic Time Series
Consider periodic, quasiperiodic and chaotic solutions of a differential equation. For an
illustrative purpose choose the Murali–Lakshmanan–Chua circuit model equation
x˙ = y − h(x), (18.43a)
y˙ = −β(1 + ν)y − βx + f1 sin ω1t + f2 sin ω2t , (18.43b)
where
h(x) = bx + 0.5(a − b)(|x + 1|−|x − 1|). (18.43c)
Equation (18.43) is solved numerically by the fourth-order Runge–Kutta method with time
step, say, 2π/100. Figure 18.2a shows the solution in the x − y plane for β = 1, ν = 0.015,
a = −1.02, b = −0.55 and ω1 = 1, ω2 = 0, f1 = 0.1 and f2 = 0. The solution corresponding
to first 1000 drive cycles, that is 1000 × (2π/100) time, is left as a transient. The solution
after the transient, is periodic with period T = 2π/ω1 = 2π. Its power spectrum is shown
in Fig. 18.2b. Dominant peaks occur at the frequencies that are integer multiples of 1/2π.
D. Quasiperiodic Orbit
A quasiperiodic orbit is written as
x(t) = φ (ω1t, ω2t, . . . , ωnt) , (18.44)456 Fourier Analysis and Power Spectrum
(a)
x
y
0.90.8 1.11 1.2
-0.8
-1
-1.2
(b)
Ω × (1/2π)
Log10
P
0 0.5 1 1.5 2
-2
-4
-6
-8
FIGURE 18.3
(a) A quasiperiodic solution of Eq. (18.43) and (b) the power spectrum of the component
x(t). The parameters are fixed as β = 1, ν = 0.015, a = −1.02, b = −0.55 and ω1 = 0.75,
ω2 = (√5 − 1)/2, f1 = 0.01 and f2 = 0.04.
where φ is periodic of period 2π, in each of the n(> 1) arguments. Further, the frequencies
(ω1, ω2,...,ωn) must have the following properties:
1. They are linearly independent, that is, there does not exist a nonzero set of
integers (l1, l2,...,ln) such that
l1ω1 + l2ω2 + ··· + lnωn = 0 . (18.45)
2. For each i, ωi = |l1ω1 + l2ω2 + ··· + lnωn| for some integers (l1, l2,...,ln), that
is, there is no rational relation between the ω’s.
A quasiperiodic orbit is essentially a sum of periodic orbits each of whose frequency is one
of the various sums and differences of the finite set of frequencies (ω1, ω2,...,ωn). Such
a quasiperiodic motion takes place on a n-dimensional torus T n. Suppose this torus is
an attracting set, that is nearby trajectories get attracted to it, then T n is said to be a
quasiperiodic attractor [4].
An example of a system with quasiperiodic solution is the Murali–Lakshmanan–Chua
circuit model Eq. (18.43). Fix β = 1, ν = 0.015, a = −1.02, b = −0.55 and ω1 = 0.75,
ω2 = (√5 − 1)/2, f1 = 0.01 and f2 = 0.04. Use the initial condition (x(0), y(0)) = (1, −1).
The solution after transient in x − y plane is shown in Fig. 18.3a. It is neither a definite
periodic motion nor an irregular orbit but a quasiperiodic orbit. The quasiperiodic char￾acter of the solution x(t) can be verified from its power spectrum. A quasiperiodic motionFast Fourier Transform 457
(a)
x
y
-1.5 0 1.5
1.5
0
-1.5
(b)
Ω × (1/2π)
Log10
P
0 1 2 3 4 5
0
-4
-8
FIGURE 18.4
(a) A chaotic solution of Eq. (18.43) for β = 1, ν = 0.015, a = −1.02, b = −0.55 and
ω1 = 0.75, ω2 = (√5 − 1)/2, f1 = 0.01 and f2 = 0.08. (b) The power spectrum of the
component x(t) of the solution shown in Fig. 18.4a.
with m rationally independent frequencies ω1, ω2,...,ωm will have Dirac-delta peaks at all
linear combinations of the basic frequencies. Figure 18.3b depicts the numerically computed
power spectrum using the data collected at t = (2π/100). In Fig. 18.3b, there are peaks
at Ω(×1/2π)=0.235, 0.38, 0.615, 0.85, 0.995 and 1.23. These data give two fundamental
frequencies Ω1 = 0.235 and Ω2 = 0.615 and these linear combinations Ω2 − Ω1 = 0.38,
Ω2 + Ω1 = 0.85, 2Ω2 − Ω1 = 0.995 and 2Ω2 = 1.23.
E. Chaotic Solution
Another interesting type of solution of a dynamical system is a chaotic solution. A non￾periodic solution of a nonlinear system with highly sensitive to initial condition is called
a chaotic solution [4]. By sensitive dependence on initial conditions it is meant that the
trajectories started from two nearby initial conditions diverge exponentially until they be￾come completely uncorrelated and future prediction becomes inaccurate. More precisely,
any small error δ0 in the specification of the initial state or round-off error in the numerical
computation can get amplified exponentially fast in a finite time interval and hence become
as large as the system size itself. Figure 18.4a shows the plot of the solution of Eq. (18.43)
for f1 = 0.01 and f2 = 0.08. The solution is not a definite periodic and not a quasiperiodic
but chaotic. The power spectrum of the chaotic solution x(t) is plotted in Fig. 18.4b. This
spectrum is quite different from that of the periodic and quasiperiodic solutions. It has con-458 Fourier Analysis and Power Spectrum
n
x
(a)
0 250 500 750 1000
1
0.5
0
(b)
Ω × (1/2π)
Log10
P
0 10 20 30 40 50
0
-5
-10
FIGURE 18.5
(a) A set of uniformly distributed random numbers in the interval [0, 1] and (b) its power
spectrum.
tinuous and broad-band nature. It is also quite common for spectrum of a chaotic solution
to contain spikes indicating the predominant frequencies of the solution.
F. Uniform Random Numbers
Finally, consider a set of random numbers uniformly distributed in the interval [0,1].
Figure 18.5a shows the random numbers generated using the Park and Miller method (see
Section 19.3.7). Its power spectrum shown in Fig. 18.5b is very noisy, a typical pattern for
a highly nonuniform signal. It has no dominant peak which implies that the sequence of
numbers considered has no periodicity.
18.7 Concluding Remarks
One of the advantages of Fourier series representation over some others, for example, with
Taylor series is that it may represent a discontinuous function also. Secondly, in the Fourier
series representation the odd and even functions are conveniently expressed as sine series
(ak = 0) and cosine series (bk = 0), respectively.
Fourier transform converts a signal from time domain into frequency domain. In FFT the
speed of the computation is greatly enhanced by decreasing the number of computations.
From the power spectrum one can obtain the various frequency components present in
the time series x(t) and as well as their respective amplitudes. The power spectrum is
not an invertible transform since it has no information about the phase. What is powerBibliography 459
spectral density and how is it obtained? There are numerous applications of FFT and power
spectrum analysis in everyday life. In recent years, neurofeedback power spectrum has been
utilized to diagnose many neuropsychiatric disorders. Changes due to gamma-ray exposure
can be detected by a power spectrum analysis. The power spectrum analysis is utilized to
seek patterns in the frequency characteristics of heart rate, blood pressure, central venous
pressure data, etc.
18.8 Bibliography
[1] E. Kreyszig, Advanced Engineering Mathematics. John Wiley, New York, 1999.
8th edition.
[2] W.H. Press, S.A. Teukolsky, W.T. Vetterling and B.P. Flannery, Numerical
Recipes in Fortran. Foundation Books, New Delhi, 1993. Indian edition.
[3] J.W Cooley and J.W. Tukey, Mathematics by Computation 19:297, 1965.
[4] M. Lakshmanan and S. Rajasekar, Nonlinear Dynamics: Integrability, Chaos and
Patterns. Springer, Berlin, 2003.
18.9 Problems
18.1 Some of the periodic functions, their mathematical representation and the Fourier
series are given below. Using the FFT algorithm compute numerically the Fourier
coefficients. Use the data in the interval [0, 2π] with ω = 1, f = 1 and T = 2π.
Sketch the graphs of the given functions and their corresponding Fourier series.
Draw the graph of the function using its Fourier series by considering first 2
terms, 5 terms, 10 terms and 20 terms. Prepare a report on the above. Compute
the power spectrum for each function and draw the power spectrum plot.
(a) Symmetric Saw-Tooth Wave
A symmetric saw-tooth wave is mathematically represented as
Fsst(t) =



4f t
T , (2n − 2) π
ω
≤ t < 4n − 3
2
 π
ω
−4f t
T + 2f, 4n − 3
2
 π
ω
≤ t < 4n − 1
2
 π
ω
4f t
T − 4f, 4n − 1
2
 π
ω
≤ t < 2nπ
ω ,
where T = 2π/ω is the period of the wave and n = 1, 2,... . The numerical460 Fourier Analysis and Power Spectrum
implementation of the wave is given by
Fsst(t) =



4f t
T , 0 ≤ t < π
2ω
−4f t
T + 2f, π
2ω
≤ t <
3π
2ω
4f t
T − 4f, 3π
2ω
≤ t <
2π
ω ,
where t is taken as mod(2π/ω). Its Fourier series is
Fsst(t) = 8f
π2
∞
n=1
(−1)(n+1)
(2n − 1)2 sin(2n − 1)ωt.
(b) Asymmetric Saw-Tooth Wave
The mathematical representation of an asymmetric saw-tooth wave form is
Fast(t) =



2f t
T , (2n − 2) π
ω
≤ t < (2n − 1) π
ω
2f t
T − 2f, (2n − 1) π
ω
≤ t < 2nπ
ω
, n = 1, 2,... .
The above wave form can be rewritten as
Fast(t) =



2f t
T , 0 ≤ t < π
ω
2f t
T − 2f, π
ω
≤ t <
2π
ω ,
where t is taken as mod(2π/ω). The Fourier series of Fast(t) is given by
Fast(t) = 2f
π
∞
n=1
(−1)n+1
n
sin nωt.
(c) Modulus of Sine Wave
The modulus of sine wave is given as
Fmsi(t) = f|sin(ωt/2)|.
Its Fourier series is
Fmsi(t) = 2f
π − 4f
π
∞
n=1
n
(4n2 − 1) cos nωt.
(d) Rectified Sine Wave
The mathematical representation of a rectified sine wave is
Frsi(t) =



f, (2n − 2) π
ω
≤ t < (2n − 1) π
ω
0, (2n − 1) π
ω
≤ t < 2nπ
ω
, n = 1, 2,... .Problems 461
The numerical implementation of the above wave form is
Frsi(t) =



f, 0 ≤ t < π
ω
0, π
ω
≤ t <
2π
ω .
The Fourier series of the rectified sine wave is
Frsi(t) = 2f
π − 4f
π
∞
n=1
1
(4n2 − 1) cos 2nωt
for 0 ≤ t < π/ω and 0 for π/ω ≤ t < 2π/ω.
18.2 Verify that the power spectrum of x(t) = 10 has a peak of magnitude 2 at the
zero frequency and 0 at all other frequencies.
18.3 An amplitude-modulated sine wave is given by
F(t)=(f + 2g cos ωt) sin Ωt.
Show that its Fourier series is
f sin Ωt + g sin(Ω + ω)t + g sin(Ω − ω)t .
Choose Ω = 3, ω = 2, f = 1 and g = 2. Compute the Fourier coefficients using
the FFT algorithm. Also, calculate its power spectrum and show that it has peaks
only at the frequencies 1, 3 and 5 (multiplied by 1/2π).
18.4 Consider the amplitude-modulated wave
F(t)=(f + 2g sin ωt) sin Ωt .
Show that its Fourier series is
F(t) = f sin Ωt + g cos(Ω − ω)t − g cos(Ω + ω)t .
Fix Ω = 5, ω = 4, f = 1 and g = 2. Compute the Fourier coefficients of F(t)
using the FFT algorithm. Then, obtain its power spectrum. Verify that it has
dominant peaks only at the frequencies 1, 5 and 9 (multiplied by 1/2π). Draw
the power spectrum plot.
18.5 Compute the Fourier series and the power spectrum P(Ω) of
f(t) = 10 + 10 sin t + cost + 5 sin 2t − √
75 cos 2t.
Verify that the power spectrum has peaks only at the frequencies 0, 1 and 2
(multiplied by 1/2π). Also, verify that the P(Ω) at these frequencies are all ≈ 100,
that is, log10 P(0) ≈ 2, log10 P(1) ≈ 2 and log10 P(2) ≈ 2.
18.6 The motion of a particle in a double-well potential driven by a periodic external
force is modelled by the equation
x¨ + αx˙ − ω2
0x + βx3 = f sin ωt.
Fix α = 0.5, ω2
0 = 1, β = 1, ω = 1 and x(0) = −1, ˙x(0) = 0. Using the fourth￾order Runge–Kutta integration method numerically solve the equation with time462 Fourier Analysis and Power Spectrum
step (2π/ω)/100 for f = 0.33. Leave x(t) for t = 0 to 200 × (2π/ω). Then, collect
212 values of x at t equal to every integer multiples of (2π/ω)/10. From its power
spectrum identify the frequency and period of the solution. Repeat the above
analysis for f = 0.35, 0.357 and 0.365. Write a short note on your result and
observation.
18.7 The Duffing–van der Pol oscillator equation is given by
x¨ − p

1 − x2
x˙ + ω2
0x + βx3 = f cos ωt.
Choose the parameters in the equation as p = 0.2, ω2
0 = 0.011, β = 1 and f = 1.
Obtain the numerical solutions after leaving x(t) for t = 0 to (2π/ω) × 200 as
a transient for ω = 0.92 and ω = 0.94. Plot x versus ˙x. Compute the power
spectra of the solutions and from them show that the solution for ω = 0.92 is
quasiperiodic and the solution for ω = 0.94 is chaotic.
18.8 Consider the Duffing–van der Pol equation in the previous problem with p = 0.2,
f = 0, ω2
0 = 0.011, β = 1. Obtain the long-time solution. From the power
spectrum identify the period of the solution.
18.9 Consider the quasiperiodically driven pendulum equation
x¨ + dx˙ + sin x = K + f (cos ω1t + cos ω2t)
with ω1 = (√5 − 1)/2, ω2 = 1, d = 3 and f = 0.55. For K = 1.33 obtain the
numerical solution x(t) after leaving a sufficient transient evolution. Compute
the power spectrum of the solution. Verify that the number of peaks, N(σ), in
the power spectrum exceeding a threshold amplitude σ scales as N(σ) ∝ σ−α,
1 <α< 2.
18.10 The two-dimensional map
xn+1 = xn + Ω − (K/2π) sin 2πxn + byn (mod 1),
yn+1 = byn − (K/2π) sin 2πxn
has a quasiperiodic solution for K = 0.8. Iterating the map and leaving the first
1000 iterations as a transient collect 214 values of (xn, yn). Plot xn versus yn. The
obtained power spectrum of the quasiperiodic solution shows that the number of
peaks N(σ) in the power spectrum exceeding a threshold amplitude σ scales as
N(σ) ∝ log(1/σ). (Hint: Compute log |P(Ω)/ max.P(Ω)| and then calculate N(σ)
for a range of values of σ. Draw a graph between N(σ) and log(1/σ). The data
will fall roughly on a straight-line).19
Random Numbers
19.1 Introduction
What are random variables? What are random numbers? An important concept in the
theory of probability is the definition of a (real) random variable. It is a set function X(ω)
and attaches a real number x to an outcome ω. The term random variable is commonly
used to emphasize that one does not know the specific value this variable will assume. If a
random variable X takes on values from a discrete set x1, x2, ... , xn it is called a discrete
random variable. A random variable X is said to be continuous if it takes on any value out
of a certain interval [a, b].
There is no proper and satisfactory definition of randomness. A sequence of numbers is
called a random, if it is generated by a random physical process. Physical processes such as
radioactivity decay, thermal noise in electronic devices, cosmic ray arrival times, etc., give
rise to a sequence of random numbers. Interestingly, it is possible to generate a random
number sequence employing simple algorithms which take very little time and memory.
Such numbers generated by a deterministic algorithm are called pseudo-random numbers
because they are predictable and reproducible.
A sequence of random numbers is often characterized by statistical quantities such as
mean, variance, moments and probability density function. The mean (or expectation value
or average or the first moment) of a discrete set of N random numbers, xi, i = 1, 2,,...,N,
is given by
x¯ = lim
N→∞
1
N

N
i=1
xi . (19.1)
Generally, a random sequence is not adequately described by just stating its mean value. It
is important to have some idea about how the numbers are dispersed about the mean. One
such measure is the variance. It is given by
σ2 = lim
N→∞
1
N

N
i=1
(xi − x¯)
2 . (19.2)
Its positive square-root is known as the standard deviation σ. The variance σ2 is the expected
value of the square of the deviation of xi from its mean value ¯x. Essentially, it characterizes
the spreading of the values xi around the mean value ¯x. The unit of mean and standard
deviation are dimensional quantities and have the same units as xi.
Another important quantity associated with a sequence of random numbers is the kth
moment defined as
mk = lim
N→∞
1
N

N
i=1
(xi − x¯)
k . (19.3)
DOI: 10.1201/9781032649931-19 463464 Random Numbers
For continuous distributions in the range [α, β]
x¯ = 1
β − α
 β
α
x dx (19.4)
and
σ2 = 1
β − α
 β
α

x − α + β
2
2
dx . (19.5)
The plan of this chapter is as follows. First, the importance of study of noise or random
numbers is presented. The probability density, mean and variance of a single uniformly dis￾tributed random variable in an interval are determined. A simple procedure for generating
a set of uniform random numbers is enumerated. Then, the Park and Miller’s method of
generation of uniform random numbers is described. The uniformity, autocorrelation and
periodicity tests for checking the randomness of a random sequence are discussed. Next,
the algorithms to generate random numbers with Gaussian distribution by the Box–Muller
formula and Ferna´ndez–Criado method are considered. Then, the inversion method, rejec￾tion technique and the Metropolis algorithm for generating random numbers with desired
distribution are explained with specific examples. Methods of developing Cauchy, L´evy and
dichotomous random numbers and their features are presented. Quantities to compare two
distributions are also defined.
19.2 Importance of Study of Noise and Random Numbers
Why should one study about noise? Is the study of noise important? What can one do
with random numbers? Study of noise is very crucial in laser systems, analog simulations
with electronic circuits, computer networks and in communication. Noise in the form of
random numbers is used in solving and simulating certain complex physical, biological and
engineering problems and processes. They play a key role in the Monte Carlo method (see
next chapter) which is a numerical method of solving mathematical problems by means
of random sampling. Presence of noise in electrical and mechanical devices and random
disturbances by an external environment in dynamical systems alter their normal functions.
Noise can be modelled by an appropriate pseudo-random sequence in the mathematical
modelling of systems driven by noise. Effect of noise can be studied theoretically without
performing actual experiments [1,2].
In poor weather conditions, millimetre waves offer a much greater penetration over
the visible spectrum through small dust particles (aerosols), rain and fog. Antenna arrays
capable of detecting millimetre waves can be constructed. This design utilizes radiometry
which is the science of using passive detection techniques to detect background radiation.
In this passive detection system, the signals are inherently noise. Thus, noise must be taken
into account when processing the antenna array signals for the desired application.
A common belief is that addition of noise to a system always degrades the quality of
the response. However, this is not the case always. It has been shown that the sensitivity
of muscle spindle receptors to a weak movement signal would be enhanced by adding noise
through the tendon of the parent muscle [3]. In hearing systems noise enhanced peripheral
sensory response has been demonstrated experimentally and theoretically [4–7]. Certain
nonlinear systems have shown that there is an optimal nonzero noise intensity which can
be added to a system to improve the response. This phenomenon is known as stochastic
resonance [8–11]. Another kind of noise-induced resonance is the coherence resonance [12-
15] and it has many practical applications.Uniform Random Numbers 465
Random numbers are useful to study various aspects of stochastic resonance in the
theoretical model equations. Study of stochastic resonance is quite interesting for biolog￾ical systems, especially in neurobiological systems since it provides a mechanism for such
systems to detect and process weak signals. Stochastic resonance has been found to play
a relevant role in several problems in biology, mammalian sensory systems, increment of
tactile capacity, visual perception, low-frequency effects and low-amplitude electromagnetic
fields. The presence of noise in nonlinear systems can give rise to many other fascinating phe￾nomena like – noise-enhanced stability, noise-induced intermittency, spiral dynamics, phase
transition, synchronization, pulse and pattern formation and noise-delayed extinction, a few
to mention.
19.3 Uniform Random Numbers
An abstract representation for a trial and its possible outcome is known as the sample space
denoted as S. A typical random number generator S consists of all integers between 0 and
231 − 1, a very large number. A point in space is called an event (or outcome), E. For
every E in S a non-negative number called the probability of E denoted as P(E), can be
assigned with 0 ≤ P(E) ≤ 1. A random variable X is a function on S with the following
two properties:
1. The values of it are real numbers denoted as x.
2. For every x the probability that the value of the function is less than or equal to
x (denoted as P(X ≤ x)) can be calculated.
19.3.1 Mean and Variance
Consider the distribution of a single random variable x assuming real values in the interval
say [α, β]. Let us denote the probability for x taking values between x and x+ dx as p(x)dx.
p(x) is called probability density of the random variable x. The total probability is
P(α ≤ x ≤ β) =  β
α
p(x)dx = 1 . (19.6)
Divide the interval [α, β] into N subintervals with N → ∞. Assume that the random
numbers are distributed with equal probability in each interval. Such random numbers are
called uniform random numbers. In this case the probability density p(x) = p = a constant.
What is the value of p
? Since P(α ≤ x ≤ β) = 1 Eq. (19.6) gives
1 =  β
α
p dx = p
 β
α
dx = p
(β − α)
or
p = 1
(β − α)
. (19.7)
The probability for the random variable taking values between x and x + dx is dx/(β − α)
and P(X ≤ x)=(x − α)/(β − α).
For uniform distribution
x¯ =
 β
α
xp(x) dx = 1
(β − α)
 β
α
x dx = (α + β)/2 (19.8)466 Random Numbers
and
σ2 =
 β
α
(x − x¯)
2p(x) dx
= 1
(β − α)
 β
α

x − α + β
2
2
dx
= 1
12(β − α)
2 . (19.9)
For a uniform random numbers in the interval [0, 1] the results are ¯x = 1/2 and σ2 = 1/12.
19.3.2 Generation of Uniform Random Numbers – Early History
How does one obtain a sequence of numbers which have the appearance of random? One can
think of the following:
1. Take a telephone directory, open a page and pick a number on that page. Then,
open another page and pick a number on that page and so on.
2. Write the numbers in pieces of paper, mix them in a box and pick one by one
from the box.
These are not of practical use.
The history of generating random numbers is interesting. In 1927, L.H.C. Tippetti pub￾lished a table of about 40, 000 random numbers (digits) taken at random from census reports.
In 1939 M.G. Kendall and B. Babington-Smith built a mathematical device to produce 105
random digits. Then, computers were introduced and the search for efficient algorithms to
generate random numbers began. In 1946 John von Neumann proposed a method of pro￾ducing random numbers using arithmetic operations of a computer. In 1955, the RAND
corporation published a table of a million random digits [16]. Is it possible to generate ran￾dom numbers by simply iterating an equation? This is interesting because in such a case it
is not necessary to store the data.
19.3.3 General Iterative Process
Computers themselves have a library routine that often has the name like ran or rand or
rnd to generate uniformly distributed random numbers in the interval [0, 1]. Such system￾supplied random numbers are generated by the recurrence relation
Ij+1 = aIj + c, mod(m), j ≥ 1 (19.10)
with the four magic numbers
m − the modulus with m > 0
a − the multiplier with 0 <a<m
c − the increment with 0 ≤ c ≤ m
I1 − the starting value with 0 ≤ I1 <m.
Start with an integer I1. Calculate aI1 + c which is also an integer. Divide it by m and
obtain the remainder and call it I2. Repeating the above procedure a sequence of integers
{I1, I2,...} can be computed. This is a sequence of random integers and is called a linear
congruential sequence. The sequence obtained when m = 1000, I1 = a = c = 89 is
89, 10, 979, 220, 669, 630, 159, ... .Uniform Random Numbers 467
TABLE 19.1
A few commonly used values of the parameters a, c and m in random number generators
[17].
a c m period
75 0 231 − 1 231 − 2
69069 0 232 230
1664524 1013904223 232 232
Equation (19.10) will repeat itself, with a period less than m. The sequence obtained when
m = 10, I1 = a = c = 7 is 7, 6, 9, 0, 7, 6, 9, 0, ... . The above sequence is periodic
with period 4. The desire is to find appropriate values for the parameters m, a, c and I1
to get a good linear congruential sequence. How does one choose good values for these four
parameters? In the following this aspect [17] is discussed.
19.3.4 Choice of the Modulus m
Let us consider the number m. The factors that influence the choice of m are
1. periodicity of the sequence and
2. speed of generation.
First, note that a periodic sequence cannot have more than m different numbers. So, m has
to be chosen as very large. Secondly, the value of m should be such that the calculation of
Ij+1 in Eq. (19.10) with mod m operation is fast. Essentially, the avoid of mod operation
would improve the speed of the calculation. For this purpose, set the value of m as the word
size of the computer.
19.3.5 Choice of the Multiplier a
The value of a is such that the period of the sequence is maximum. Remember that a
desirable criterion for the randomness of a sequence is the long period. As an example,
consider the choice a = c = 1, m = 6 and I1 = 5. Then, the random sequence is 5, 0, 1, 2,
3, 4, 5, 0, 1, ... . Its period is 6, that is, the value of m. The period cannot be longer than
m since the number of possible values are only m.
What are the possible values of a, c and I1 that give a period of length m? The answer
is given by the following theorem [17].
Theorem:
The period of the linear congruential sequence defined by the parameters m, a, c and the
initial value of I, I1, is m if and only if the following three conditions are satisfied:
1. c is relatively prime to m,
2. b = a − 1 is a multiple of p, for every prime p dividing m and
3. b is a multiple of 4, if m is a multiple of 4.
Table 19.1 shows a few commonly used values of the parameters in random number
generators.468 Random Numbers
19.3.6 A General Simple Generator for Random Numbers
For generating a sequence of random numbers using a computer the following procedure
gives a simple generator.
1. Choose an arbitrary value for the starting value of I, I1.
2. Choose the value of m as large as possible, say at least 230. The choice of m being
the computer word size makes the computation of aIj + c mod(m) as an efficient
one. More over round-off error should not be introduced in the computation of
(aIj + c) mod(m).
3. If m is a power of 2 then choose the value of a in such a way that a mod(8)= 5.
This choice of a along with the choice of c given in step 5 is to make sure that the
produced sequence will have m different values of I before the generator begins
to repeat the numbers.
4. The value of a should be between 0.01 m and 0.99 m and further it should not
have a regular pattern.
5. If a is a good multiplier then the value of c is immaterial. However, c should not
have any factor in common with m. It is better to choose c = 1 or c = a.
Instead of considering a random integer between 0 and m − 1 one may consider I as
a random fraction between 0 and 1. Then, the numbers generated are all in the interval
[0, 1]. It can be converted into any suitable range [−α, β]. Before using the sequence for
applications apply all existing tests to make sure that the sequence is sufficiently random.
19.3.7 Park and Miller’s Generator
Park and Miller proposed a generator based on the choices [18]
a = 75, c = 0, m = 231 − 1 = 2147483647. (19.11)
The maximum possible value of I is m−1. When the product of a and Ij is greater than the
maximum value of a 32-bit integer it is not possible to implement the Eqs. (19.10)–(19.11)
in a computer. However, the following suggestion of Schrage is useful. From the approximate
factorization of m given by
m = aq + r (19.12)
one can write
q = int [m/a] , r = m (mod a), (19.13)
where int denotes integer part. For r<q and 0 <I<m − 1 both aI (mod q) and r.int[I/q]
lie in the range 0, 1,...,m − 1. Write
aI (mod m) =



aI (mod q) − r.int[I/q], if it is > 0
aI (mod q) − r.int[I/q] + m, otherwise.
(19.14)
For simplicity, define
P1 = I (mod q), P2 = int[I/q] . (19.15)
Then,
aI (mod a) =



aP1 − rP2, if it is > 0
aP1 − rP2 + m, otherwise.
(19.16)Uniform Random Numbers 469
TABLE 19.2
First five random numbers generated using Eqs. (19.10) and (19.16). Here, m = 2147483647,
a = 75, r = 2836, q = 127773 and c = 0.
j Ij P1 P2 Ij+1 ran
0 3 3 0 50421 0.0000235
1 50421 50421 0 847425747 0.3946134
2 847425747 35211 6632 572982925 0.2668160
3 572982925 48793 4484 807347327 0.3759504
4 807347327 77513 6318 1284843143 0.5983017
Some useful choices of a, q and r are given below:
(i) a = 75, q = 127773, r = 2836.
(ii) a = 48271, q = 44488, r = 3399.
(iii) a = 69621, q = 30845, r = 23702.
The period of the numbers generated using the above procedure is 231 − 2 = 2147483646 ≈
2.1 × 109. The above procedure generates numbers in the interval [0, m]. Note that the
random numbers are now integers. The numbers generated can be converted into the range
[0, 1] by dividing them by m.
The following steps will generate a uniform random number in the interval [0, 1]:
m = 2147483647
a = 75
r = 2836
q = 127773
I = 3
P1 = mod(I,q)
P2 = int(I/q)
I = aP1 − rP2
if (I < 0) then I = I + m
ran = float(I)/float(m).
The value of ran is between 0 and 1. Repeating the lines 6 − 10 generates a sequence of
uniform random numbers. Table 19.2 gives the first few random numbers generated. Figure
19.1a depicts first 100 random numbers of the sequence generated. The mean and variance
obtained with 104 numbers are 0.5017 and 0.0833, respectively, whereas the exact values
are 0.5 and 0.0833 ... , respectively.
19.3.8 Numerical Calculation of Probability Distribution
To calculate the probability distribution of the random numbers, the numbers in the interval
[0, 1] are scaled into the range [0, L] with L = 50. This range is then divided into L-equal
subintervals. This simplifies the procedure to count the number of points occurring in each
subinterval.470 Random Numbers
(a)


0 25 50 75 100
1
0.5
0
(b)

 
0 0.25 0.5 0.75 1
0.03
0.02
0.01
0
FIGURE 19.1
(a) Plot of first 100 uniformly distributed random numbers x generated using the Park
and Miller algorithm. (b) The numerical probability distribution P(x) of 5 × 104 numbers
generated. The interval [0, 1] is divided into 50 equal subintervals. P(x) is almost constant.
For computational purpose, first the minimum, xmin, and the maximum, xmax, values
of generated numbers {xi} are calculated. For uniform random numbers in the interval [0, 1]
these values are 0 and 1, respectively. Next, the numbers in the interval [xmin, xmax] are
converted into the interval [0, L]. Then, the numbers are changed into integers. Now, if
the value of a number is K then it falls in the Kth subinterval. In this way, the number
of x’s falling in each subinterval is counted. Figure 19.1b shows the computed probability
distribution of the numbers where 104 numbers are used. For uniformly distributed numbers
the kth moments mk are finite. First few mk’s are given by
m0 = 1, m1 = ¯x, m2 = σ2 + m2
1 (19.17)
which can also be verified numerically.
Before going to study how to generate other types of random numbers let us present
how to analyze randomness of a set of random numbers in the next section.
19.4 Tests for Randomness
In the previous section how to generate a sequence of uniformly distributed random numbers
with a long period is described. Absence of periodicity alone does not guarantee that theGaussian (Normal) Distribution 471
sequence generated is useful for practical applications. Before using it for a practical purpose
it is necessary to know whether the sequence is sufficiently random. What are the quantitative
measures for randomness? Some unbiased tests are needed to know the randomness of a
sequence.
Randomness of a set of random numbers can be analyzed using the following tests:
1. Uniformity test.
2. Autocorrelation test.
3. Periodicity test.
4. Comparison of computed mean value with the expected one.
Let us consider the distribution of numbers in the interval [α, β] and divide the interval
into L equally spaced subintervals. For perfectly distributed numbers, the probability of
each interval is obtained from the expected or assumed probability density function. For
uniformly distributed numbers this is 1/L. Call this as expected probability Ei. For a given
set of numbers numerically calculate the probability in each subinterval and denote them
as Oi (observed). Then, the chi-square, χ2, quantity is given by
χ2 = 
L
i=1
(Ei − Oi)
2
Ei + Oi
. (19.18)
If the given set is perfectly distributed according to the assumed distribution then χ2 = 0.
Deviation from zero can be used to describe nonuniformity.
The autocorrelation function for a set of N numbers is defined as
C(k) =
N
i=1 xi xi+k − Nx¯2
N
i=1 x2
i − Nx¯2 , k = 0, 1, 2,...,N − 1 (19.19)
where ¯x is given by Eq. (19.1). For a sequence of truly random numbers C(0) = 1 and
C(k) = 0 for k > 0.
The third test is the checking of whether the sequence has a long period. The fourth
test is to compare the mean value of the sequence with the expected one. For a sequence of
N truly uniform random numbers in the interval [0, 1] the mean value ¯x of it is expected to
lie between 0.5 −  and 0.5 + . For large N, from the central-limit theorem, ¯x is Gaussian
with mean 0.5 and σ2 = 1/(12N). Suppose that p() is the probability for the numbers in
the interval [0.5 − , 0.5 + ]. If a sequence of N random numbers has an average that falls
inside (outside) the interval [0.5 − , 0.5 + ] then the conclusion is that it passes (fails) the
test at 5% level.
In practice, it is better to apply all the known tests to make sure that the sequence is
sufficiently random.
19.5 Gaussian (Normal) Distribution
A probability density function which is observed in many scientific processes is the Gaussian,
also called normal. It is characterized by the density function
G(x) = p(x) = 1
√2π σ e−(x−µ)2/(2σ2) , −∞ <x< ∞ (19.20)472 Random Numbers
  
  
  


630-3-6
1.5
1
0.5
0
FIGURE 19.2
The Gaussian distribution, Eq. (19.20), with µ = 0, and σ = 0.5, 1, 1.5.
where µ is the mean of x and σ2 is the variance of x. Figure 19.2 shows G(x) for µ = 0
and σ = 0.5, 1 and 1.5. The Gaussian function is of fundamental importance in many
physical and mathematical applications. The Gaussian random variable is utilized in digital
communication (particularly, to analyze transceiver systems), machine learning, statistical
modelling, data analysis, computer simulation, statistical sampling, cryptography and so
on. Gaussian noise is employed in the numerical investigation of the effect of fluctuation
of parameters and interaction of environments in the mathematical models of physical,
chemical, biological and engineering systems. For these, it is necessary to have an efficient
Gaussian random number generator. A very large number of techniques have been proposed
to generate Gaussian random numbers. In the following, two simple methods are described.
19.5.1 Box–Muller Algorithm
Box and Muller proposed a simple algorithm to generate Gaussian random numbers. Their
method is quite popular and called Box–Muller algorithm. The Box–Muller formula is [19]
x1 = µ + σ(−2 ln y1)
1/2 cos (2πy2) , (19.21)
where y1 and y2 are uniform random numbers in the interval [0, 1], µ and σ are, respectively,
the mean and the standard deviation of the Gaussian random numbers to be generated and
x1 is the generated number.
Let us derive the Box–Muller formula (19.21). Consider a generation of a pair of Gaussian
random numbers x and z. For simplicity set µ = 0 and σ = 1. The joint distribution of x
and z is
G(x, z) = p(x, z) = 1
2π e−(x2+z2)/2 , −∞ < x, z < ∞. (19.22)
Introducing the polar coordinates r = √
x2 + z2, θ = tan−1(z/x) write
p(x, z) dxdz = 1
2π e−r2/2 r drdθ . (19.23)Gaussian (Normal) Distribution 473
Here, r is distributed according to re−r2/2 whereas θ is uniformly distributed between 0
and 2π. Then, the distribution function for r is given by
F(r) =  r
0
r e−r2/2 dr = 1 − e−r2/2 = y1 , (19.24)
where y1’s are uniformly distributed random numbers in the interval [0, 1]. The above equa￾tion gives
r = −2 ln (1 − y1). (19.25)
Since the replacement of 1−y1 by y1 does not change the probability distribution Eq. (19.25)
can also be written as
r = −2 ln y1 . (19.26)
One can generate the random variable θ = 2πy2, where y2’s are uniformly distributed
random numbers in the interval [0, 1]. Hence, x and z can be generated using the formula
x = r cos θ = −2 ln y1 cos 2πy2 , (19.27)
z = r sin θ = −2 ln y1 sin 2πy2 . (19.28)
To get a Gaussian distribution with mean µ and variance σ2 rescale x as σx+µ and obtain
x = µ + σ
−2 ln y1 cos 2πy2 (19.29)
or
x = µ + σ
−2 ln y1 sin 2πy2 . (19.30)
Figure 19.3a shows the plot of first 100 Gaussian random numbers generated using
the above formula (19.29) with µ = 0 and σ = 1. Figure 19.3b depicts the probability
distribution P(x) of the generated numbers. 5 × 104 numbers are used to obtain this plot.
The interval [−5, 5] is divided into 50 equal subintervals in the calculation of P(x). The
moments, given by Eq. (19.3), of the Gaussian random numbers diverge with increase in
N.
19.5.2 Ferna´ndez–Criado Algorithm
Ferna´ndez and Criado [20] proposed an algorithm which is based on an N-particles closed
system interacting two at a time and conserving energy.
Consider N-particles each with same unit velocity: {vi = 1, i = 1, 2,..., N}. Randomly
choose two particles and denote them as i and j. Redefine their velocities as per the iteration
rule
vi(new) = 1
√2
[vi(old) + vj (old)] , (19.31a)
vj (new) = 1
√2
[vj (old) − vi(old)] . (19.31b)
Leave, for example, first 10N iterations or so as transients. Then, the velocities of the pair of
particles chosen in all further iterations are the required pair of Gaussian random numbers
with mean zero and variance unity. Gaussian random numbers of desired mean and variance
σ are then obtained by the transformation
x = µ + σv . (19.32)
This algorithm is found to be ten times faster than the Box–Muller algorithm.
How does one generate two-dimensional (that is, a sequence of pair of numbers (x, y))
random numbers with Gaussian distribution? (see Subsection 19.6.2 and Problem 19.29).474 Random Numbers
(a)


0 25 50 75 100
4
0
-4
(b)

 
-4 -2 0 2 4
0.1
0.05
0
FIGURE 19.3
(a) Plot of first 100 Gaussian random numbers generated using the Box–Muller method.
Here, µ = 0 and σ = 1. (b) The numerically computed probability distribution of 5 × 104
Gaussian random numbers. The interval [−5, 5] is divided into 50 equal subintervals. P(x)
is maximum at the mean ¯x = µ = 0.
19.6 Generation of Random Numbers with Desired Distribution
It is easy to generate a sequence of random numbers other than uniformly distributed num￾bers and Gaussian numbers, for example, numbers with exponential distribution, circular
probability density distribution, binomial distribution, etc. This can be achieved by em￾ploying the methods called random sampling techniques. Some of such methods are direct
inversion, rejection method and Metropolis. In the following, these methods are described to
obtain numbers with a chosen distribution. These methods can be easily applied to generate
an arbitrary distribution. For other methods of generating exponential distribution, readers
may refer to the refs. [21–26].
19.6.1 Method of Inversion
The method of inversion is based on the direct inversion of the cumulative probability
distribution of the random variable x. Therefore, first introduce it. Let a random variable
take values in the interval −∞ to ∞ and the probability density is p(x). The probabilityGeneration of Random Numbers with a Desired Distribution 475
that the value of x is less than or equal to a value x0 is
P(x ≤ x0) = F (x0) =  x0
−∞
p(x) dx (19.33)
with 0 ≤ F(x0) ≤ 1. The distribution function F(x0) is monotonic and nondecreasing
function of its argument. F(x0) is called a cumulative probability distribution or simply a
probability distribution function. The method of inversion is stated by the following theorem.
Theorem:
A random variable with probability density p(x) can be generated from the formula
x = F −1(y), (19.34)
where F −1(y) is the inverse function of F(x) and y is a uniformly distributed random
number.
Proof:
Let us prove that x given by Eq. (19.34) is distributed as per p(x). There is a one-to-one
correspondence between x and y. Particularly, the probability for x assuming values between
x and x + dx is the same for y and y + dy. The probability in this interval is dy (why?). As
y = F(x) the probability for x between x and x + dx is given by
dy = dF(x) = F
(x) dx = p(x) dx . (19.35)
Exponential Distribution
Choose the desired distribution as exponential of the form
p(x) =



α e−αx, x ≥ 0
0, x< 0.
(19.36)
Let us mention a few applications of the exponential distribution. This distribution and its
variants are used in queuing models, to model time duration between successive events such
as request arrivals to a device and failure of a device, the length of phone calls and sales
totals for customers, life span of electronic gadgets, shoppers at a shopping market, etc.
[24-25]. It is used in the Monte Carlo simulation of electronic distribution of a hydrogen
atom in various quantum states (see Chapter 20 in this book).
Consider the cumulative probability distribution defined as
F(x) =  x
−∞
p(x) dx . (19.37)
F(x) is a monotonic nondecreasing function of x with F(−∞) = 0 and F(∞) = 1. For the
function p(x) given by Eq. (19.36)
F(x) =  x
0
α e−αx dx = 1 − e−αx, x> 0. (19.38)
For the choice α = 1 the function F(x)=1 − e−x. Given a random number yi, one has
xi = F −1(yi) = − ln(1 − yi).
Figures 19.4a and 19.4b depict x versus p(x) and x versus F(x), respectively. Random
numbers with the distribution given by Eq. (19.36) are obtained as follows.476 Random Numbers
(a)


0 1 32 4 5
1
0.5
0 
(b)

 
0 1 32 4 5
1
0.5
0
FIGURE 19.4
Plots of (a) x versus p(x) (Eq. (19.36)) with α = 1 and (b) x versus F(x) (Eq. (19.38)).
(1) Choose a point on the y-axis of Fig. 19.4b randomly between 0 and 1.
(2) Draw a line parallel to x-axis and pass through the curve (dashed line in
Fig. 19.4b). Obtain the intersecting point on the curve and call the x-component
of it as x1.
(3) Repeating the steps (1) and (2) generates a sequence of numbers {xi, i =
1, 2,...} . These numbers have the chosen exponential distribution.
The above graphical scheme can be performed analytically as follows. Let {yi, i =
1, 2,...} be a sequence of numbers F(x) and are uniformly distributed between 0 and 1. The
probability that it falls between F(x) and F(x)+dF(x) is dF(x) which is equal to p(x)dx.
Hence, x = F −1(y) is distributed as per p(x). For exponential distribution F(x)=1 − e−x
and therefore x = − ln y. As both y and 1− y are uniformly distributed in the interval [0, 1]
one can set x = − ln y.
The above scheme can be easily implemented in computers. Figure 19.5a shows a plot
of first 100 numbers obtained. The numbers are usually generated in the interval [0, ∞]. For
5 × 104 numbers the numerically computed mean is 0.99 (the exact mean is 1/α = 1) and
the variance is 0.98 (the exact variance is (1/α)2 = 1). Figure 19.5b shows the numerically
computed probability distribution of the generated numbers.
The inversion technique can be used to generate random numbers with discrete distri￾butions also. For some details see the ref. [27].
19.6.2 Rejection Technique
Another useful random sampling method is the von Neumann accept-reject or simply rejec￾tion technique. It is a powerful technique to generate random numbers whose distribution
function is known and computable. This method does not rely on the analytic expression
of inverse distribution but it is less efficient.
Let f(x) be the desired distribution defined in the interval [α, β]. Select a suitable
bounding function g(x) such that cg(x) ≥ f(x) for all values of α ≤ x ≤ β. For theGeneration of Random Numbers with a Desired Distribution 477
(a)


0 25 50 75 100
10
5
0
(b)

 
0 5 10
0.2
0.1
0
FIGURE 19.5
(a) Plot of first 100 random numbers with exponential distribution generated by the method
of inversion. (b) The probability distribution of 5 × 105 numbers, where the interval [0, 12]
is divided into 50 equal subintervals.
exponential function
f(x) =



e−x, x ≥ 0
0, x< 0
(19.39)
one may choose the range [α, β] as [0, 10] or a suitable interval for practical purpose. cg(x) ≥
1 because f(x) ≤ 1 for x ≥ 0. Set c = 1 then g(x) ∈ [0, 1]. Choose g(x) and x as uniformly
distributed random numbers in the intervals [0, 1] and [0, 10], respectively. The scheme is
the following:
(1) Choose a value of x randomly and compute f(x).
(2) Choose a value of cg and call it y.
(3) If y>f(x) then go to step (1) otherwise x is a member of the chosen distribution
number.
Repeating the above procedure, number of random numbers with the distribution f(x) can
be obtained. The procedure is simply the following. Consider Fig. 19.6 where the dots are
the points with coordinate values (xi, yi), xi ∈ [0, 10] and yi ∈ [0, cg] (= [0, 1]). Discard the
points falling above the curve f(x). The x values of the points lying below the curve form
random numbers with exponential distribution. Here, cg(x) need not necessarily be defined
in [0, 1] and can be [0, 1.5] or [0, 2] or [0, > 1]. The number of points discarded to generate N478 Random Numbers


 
0 1 32 4 5
1
0.5
0
FIGURE 19.6
Description of rejection technique. The x-component of a point falling below the curve f(x)
is a number with the chosen exponential distribution.
numbers depends on the choice of cg(x). The number of discarded points will be minimum
if the maximum value of cg(x) is chosen as the maximum value of f(x).
Rejection technique is extendable to generate n-dimensional random numbers with a
chosen distribution. As an example, consider a two-dimensional distribution function f(x, y).
Define x and y as uniformly random numbers in the interval [a, b]. cg(z) is a suitable
bounding function which is a set of random numbers in the interval, say, [0, cgM] where cgM
is a number such that cgM ≥ f(x, y). Choose the values of x, y and cg(z). If cg(z) < f(x, y)
then accept the pair (x, y) otherwise leave. Repeating the above, a sequence of pair of
numbers (x, y) can be obtained and will obey the distribution f(x, y).
19.6.3 Metropolis Algorithm
Another simple technique of generating random numbers of a given distribution using uni￾form random numbers is the Metropolis sampling suggested by Metropolis and his collabo￾rators [28]. This algorithm is widely used in Monte Carlo simulation of models in statistical
physics.
The method is based on the concept of a random walk. In a typical one-dimensional
random walk, a walker or a point is started at a reference point, say, the origin. At any
instant of time, the point is allowed to move one step to the left or right. In the limit
t → ∞, the point will have visited all parts on a line. The point can spend much time in the
region where the value of the probability density function sampled is large. In this way the
set of positions of the points give a realization of the probability density function. This is
essentially because the probability of finding a point in a given region is same as the chosen
probability density function. Consequently, if all the positions of the points are collected
then the probability would follow the probability density function.
First, describe the technique for an arbitrary distribution f(x). Starting from an initial
arbitrarily chosen value x0 belonging to the value Ω of f(x) the Metropolis technique gen￾erates a Markov chain (a stochastic process in which past history has no influence on theGeneration of Random Numbers with a Desired Distribution 479
future if its present is specified) T of values x1, x2, ..., xn. All the xi’s are values Ω of f(x)
and for n → ∞, xn+1, xn+2, ... will have the given distribution f(x). Call {xn+1, xn+2,...}
as the desired states or values. Here, x1 is based on x0, x2 is based on x1 and so on. States are
generated through the transition probability W(xi → xi+1). For an asymptotic convergence
to the desired values, the sufficient (but not necessary) condition is that
f (xi) W (xi → xj ) = f (xj ) W (xj → xi). (19.40)
Equation (19.40) is called the detailed balance. This condition does not specify the transition
probabilities from one value to another. Metropolis algorithm refers to a particular choice
of W given by
W (xi → xj ) = T (xi → xj ) min [1, f (xj ) /f (xi)] , xi = xj (19.41)
where T (xi → xj ) = T (xj → xi). T must be a symmetric stochastic matrix with positive
elements but can be any distribution. Choose
T (xi → xj ) =



constant, for xj inside a certain
region around xi
0, otherwise.
(19.42)
Let us describe the implementation of the Metropolis sampling procedure for generating
Gaussian random numbers obeying the probability density given by Eq. (19.20).
(1) Set the parameters of the required distribution. For Gaussian distribution, for
simplicity, fix σ = 1 and µ = 0.
(2) Set the index i = 0 and specify the initial value of xi(= x0). A good choice of
initial value is that value for which the probability is maximum. For Gaussian
distribution, this is the case for x = 0 and hence choose xi = 0.
(3) Select a trial state xt randomly and with equal probability among the values Ω
of the required distribution. Define
xt = xi −  + 2 ξ, (19.43)
where ξ is a uniformly distributed random number in the interval [0, 1] and  is
an adjustable parameter, for example, say  = 3. xt is distributed uniformly in
the interval [xi − , xi + ] with xi as the centre.
(4) Calculate the ratio ω = f(xt)/f(xi).
(5) If ω ≥ 1 then set xi+1 = xt. When ω < 1, generate a (new) random number ξ
and if ξ ≤ ω set xi+1 = xt otherwise xi+1 = xi.
(6) Set i = i + 1 and go to the step (3).
It may be required to generate several values of x, starting from x0, before the sequence
x1, x2,... follows the desired distribution. Therefore, leave, for example, 103 random num￾bers generated and store the further obtained numbers.
A characteristic measure of the Metropolis algorithm is the acceptance ratio given by
AR = Number of accepted moves
Number of trials . (19.44)
AR can be adjusted by changing . When AR is too small then the value (of x) space or
region will be explored much slowly because only very small number of trials are accepted.480 Random Numbers
TABLE 19.3
The Gaussian random numbers are generated by the Metropolis algorithm.
i xi ξ xt ω ξ(new) xi+1
0 0.00 0.650 0.90 0.667 0.350 0.90
1 0.90 0.400 0.30 0.698 0.560 0.30
2 0.30 0.170 −1.68 0.255 0.910 0.30
3 0.30 0.610 0.96 0.659 0.390 0.96
4 0.96 0.890 3.30 0.002 0.001 3.30
5 3.30 0.250 0.55 199.100 −− 0.55
On the other hand, when AR is high, the trial moves are too small and in this case diffusion
through the space will be very slow. An optimal choice of  is the one which roughly accepts
50% of the trials.
Example:
The aim is to generate five Gaussian random numbers with mean zero and variance 1 using
the Metropolis technique. Choose  = 3 and assume that the following sequence of uniformly
distributed random numbers in the interval [0, 1] are given:
0.650, 0.350, 0.400, 0.560, 0.170, 0.910,
0.610, 0.390, 0.890, 0.001, 0.250, 0.440.
Since the Gaussian distribution has maximum probability about x = 0 choose x0 = 0. Table
19.3 presents the xi+1 for first five iterations.
19.7 Some Other Types of Random Numbers
Apart from uniform and Gaussian random noise, there are other types of noises such as
Cauchy, L´evy and coin-toss square-wave dichotomous are realized in certain physical sys￾tems. These noises are also used in the study of certain physical and engineering problems.
This section discusses the algorithms to generate them.
19.7.1 Cauchy Random Numbers
The Cauchy probability distribution is given by
PC(x) = a
π (a2 + x2)
, (19.45)
where a is a constant. It has applications in electrical and mechanical theories, measurement
and calibration problems and in representing the points of impact of straight-line of particles
emitted from a source [29,30]. It is used in the analysis of quality of service in IP networks
[31] and price fluctuation in research and development projects investments of governments
[32].Some Other Types of Random Numbers 481
Consider PC(x)dx = Py(u)dy where y is a set of uniformly distributed random numbers
in the interval [0, 1] and obtain
a dx
π (a2 + x2) = dy . (19.46)
Integration of the above equation gives
a
π
 x
−∞
1
a2 + x2 dx = y . (19.47)
That is,
1
π

tan−1 x
a
+ π
2

= y . (19.48)
From the above equation write
x = a tan π

y − 1
2

. (19.49)
Equation (19.49) can be used to generate random numbers x with the probability distribu￾tion PC(x) given by Eq. (19.45). Hundred random numbers generated using Eq. (19.49) with
a = 0.1 are plotted in Fig. 19.7a. The probability distribution of 5 × 105 Cauchy random
numbers is shown in Fig. 19.7b.
19.7.2 L´evy Random Numbers
L´evy [33] shown that the sum of n independent stochastic variable z with a probability dis￾tribution characterized by power-law tails P(z>u) ∝ |z|
−α, 0 < α ≤ 2 converges to a stable
process characterized by a probability density, which is now called a L´evy distribution. The
L´evy stable processes are characterized by the probability density with diverging moments.
The analytical form of the symmetrical L´evy stable distribution is known only for a few
special values of α. The choices α = 1 and 2 lead to Cauchy and Gaussian distributions,
respectively.
A L´evy noise can increase the mutual information or bit count of several feedbacks neu￾ron models that obey a general stochastic differential equation. Use of L´evy noise can benefit
subthreshold neuronal signal detection [34], enhance subthreshold synchronization of the su￾perparamagnetic tunnel junctions (spintronic nanodevices) [35], induce inverse stochastic
resonance [36] and stochastic resonance [37] in neuron models, leads to synchronization
of dynamical systems [38] and induce transport in multiple-well potential systems [39]. It
has also been used in stochastic Norovirus epidemic model [40], stochastic COVID-19 epi￾demic models [41] and epidemic systems with quarantine strategy [42] and so on. In certain
nonlinear systems the anomalous diffusion is found to show L´evy distribution.
The L´evy distribution is given by
Lα,γ(z) = Lα,γ(−z) = 1
π
 ∞
0
cos qz e−γqα
dq (19.50)
or
P(x) = P(−x) = 1
2π
 ∞
−∞
eiqz e−γ|q|
α
dq, (19.51)
where α and γ are two parameters characterizing the distribution. α defines the index of
the distribution and controls the scale properties of the stochastic process {x} and γ selects
the scale unit of the process [43].
Now, describe the method of Rosario Nunzio Mantegna [43] to generate L´evy random
numbers. The method of Mantegna consists of the following four steps.482 Random Numbers
(a)
j
x
0 25 50 75 100
2
0
-2
(b)
x
P(x)
-2 -1 0 1 2
0.15
0.1
0.05
0
FIGURE 19.7
(a) Hundred Cauchy random numbers with a = 0.1. (b) The numerically computed prob￾ability distribution of 5 × 105 Cauchy random numbers (marked by dots) and the exact
probability distribution (continuous curve).
Step 1:
With x and y being Gaussian random variables with their standard deviation given by
σx(α) =  Γ(1 + α) sin(πα/2)
Γ((1 + α)/2) α 2(α−1)/2
1/α
, σy = 1 , (19.52)
respectively, define a random variable v,
v = x
|y|
1/α . (19.53)
For |v|  0 the distribution of v is similar to the L´evy distribution (for a proof see the
ref. [43]). One can show that the probability distribution of v is
P(v) = 1
πσx σy
 ∞
0
y1/α exp 
− y2
2σ2
y
− v2y2/α
2σ2
x

dy . (19.54)
Figure 19.8 shows the plots of L1.5,1(v) and P(v) with σy = 1, σx = 0.696575. Both L1.5,1(v)
and P(v) are calculated numerically by integrating the integrals in Eqs. (19.50) and (19.54).
L1.5,1(v) and P(v) are different near the origin, however, they coincide for |v|  0.Some Other Types of Random Numbers 483
P(v)
L1.5,1(v)
v
L1.5,1(v), P(v)
151050-5-10-15
0.5
0.4
0.3
0.2
0.1
0
FIGURE 19.8
Plots of Lα,γ(v) and P(v) (Eq. (19.54)) where α = 1.5, γ = 1, σx = 0.696575 (Eq. (19.52))
and σy = 1. The two functions are almost coincident for |v| ≥ 10.
Step 2:
The second step is to ensure that the P(v) of the numerically generated {v} coincides all
over the range with the L´evy stable distribution of the same index α and the scale factor
γ. For this purpose consider
zn = 1
n1/α n
m=1
vm . (19.55)
The convergence of {zn} is quite slow. For a faster convergence, Mantegna proposed the
transformation from v to ω as
ω =

1+[K(α) − 1] e−v/C(α)

v , (19.56)
where C(α) and K(α) are two parameters to be determined. Next, introduce, zcn, weighted
average of n independent stochastic variables ω, as
zcn = 1
n1/α n
m=1
ωm . (19.57)
zcn quickly converges to a L´evy stable distribution.
Step 3:
The optimal value of K(α) can be determined analytically by requiring P(ω = 0) = Lα,1(0).
ω = K(α)v for ω close to the origin. Then, P(ω = 0) = Lα,1(0) is satisfied if
K(α) = P(v = 0)
Lα,1(0) . (19.58)484 Random Numbers
TABLE 19.4
Values of σx, K and C for some values of α with γ = 1 [43].
α σx(α) K(α) C(α)
0.8 1.139900 0.795112 2.4830
1.0 1.0 1.0
1.2 0.878829 1.205190 2.9410
1.4 0.759679 1.446470 2.8315
1.5 0.696575 1.59922 2.7370
1.6 0.628231 1.793610 2.6125
1.8 0.458638 2.501470 2.2060
Substitution of Lα,1(0) = Γ(1/α)/(πα) and Eq. (19.54) in the above equation yields
K(α) = αΓ((α + 1)/2α)
Γ(1/α)
 αΓ((α + 1)/2)
Γ(1 + α) sin(πα/2)1/α
. (19.59)
C(α) is the result of a polynomial fit of the values tabulated in the ref. [43]. The required
variable is z = C1/αzcn. Table 19.4 gives some of the values of the control parameters.
Step 4:
In the above, the value of the scale factor γ is set as 1. L´evy random numbers with γ = 1
can be obtained from zcn using the transformation
z = γ1/αzcn . (19.60)
Now, compare the numerically generated L´evy random numbers with the exact L´evy
stochastic variable. Figure 19.9a is a plot of hundred L´evy random numbers generated
using the above algorithm where α = 1.5, γ = 1, σx = 0.696575 (Eq. (19.52)), σy = 1,
K(α)=1.59922 (Eq. (19.59)), C(α)=2.737 (from the Table 19.4) and n = 1. Very few
generated numbers have very large magnitude say greater than 20. They can be discarded.
Probability distribution of the numerically generated random numbers is computed by di￾viding the interval [−20, 20] into 120 bins. Then, the probability density, PL is obtained
by dividing the probability distribution by the bin size used. Figure 19.9b shows PL of the
numerically generated 5 × 105 L´evy random numbers and the exact L´evy distribution Lα,γ.
Notice that the PL is in very good agreement with the Lα,γ all over the range of z. A
small discrepancy observed near the origin can be eliminated by using large values of n.
Figure 19.10 shows two probability densities calculated for γ = 0.01 and 100. In both cases,
numerical simulation is in very good agreement with the actual L´evy distribution.
For two other methods of generating L´evy random numbers, one may refer to the
refs. [18,44,45].
19.7.3 Dichotomous Noise
Dichotomous noise is characterized by whether it is on or off or whether it is up or down
[46–48]. Rich and complex phenomena are realized due to an interplay of the dichotomous
single-molecule gene noise with architecture of genetic networks. Bursting intermittency isSome Other Types of Random Numbers 485
(a)
j
z
0 25 50 75 100
20
10
0
-10
-20
(b)
z
PL, Lα,γ
-20 -10 0 10 20
0.3
0.2
0.1
0
FIGURE 19.9
(a) Hundred L´evy random numbers with the control parameters α = 1.5, γ = 1, σx =
0.696575 (Eq. (19.52)), σy = 1, K(α)=1.59922 (Eq. (19.59)), C(α)=2.737 (from the
Table 19.4) and n = 1. (b) The numerically computed probability density PL of 5 × 105
L´evy random numbers (marked by dots) and the exact probability density Lα,γ (continuous
curve) given by Eq. (19.45).
found to occur in the generation of mRNAs [49] and in coupled nonlinear oscillators [50]
due to dichotomous noise. Induced transitions in ZnSe interference filter with dichotomous
fluctuations in the incident power are experimentally detected [51]. Due to dichotomous
noise phase synchronization [52,53], hysteresis [54], patterning [55,56], improved resonance
[57] and hypersensitivity [58,59] are reported.
A dichotomous coin-toss square-wave noise can be expressed as
G(t) = Dan, [α + (n − 1)]t0 < t ≤ (α + n)t0, (19.61)
where n = ..., −2, −1, 0, 1, 2,... is a set of integers, α is a random variable uniformly
distributed between 0 and 1, an are independent random variables that take on the values
−1 and +1 with equal probability 0.5, t0 is a parameter of the process G(t) and D is the
amplitude of the noise. A rectangular pulse-wave of amplitude an and length t0 centred at
the coordinates tn = (α + n − 0.5)t0 has a Fourier transform
Gn(ω) = Dan

(2/ω) sin(ωt0/2) e−iωtn 
. (19.62)
The pulse itself can be expressed as a sum of harmonic terms approximating as closely as
desired the inverse Fourier transform of Gn(ω). Each realization of the coin-toss dichotomous
square-wave can be approximated arbitrarily closely by a superposition of such sums, which486 Random Numbers
(a)
z
PL, L1.5,0.01
-0.5 -0.25 0 0.25 0.5
8
6
4
2
0
(b)
z
PL, L1.5,100
-300 -200 -100 0 100 200 300
0.015
0.01
0.005
0
FIGURE 19.10
(a) The computed probability density PL of 5×105 L´evy random numbers (marked by dots)
and the exact probability density Lα,γ (continuous curve) given by Eq. (19.45). (a) γ = 0.01
and (b) γ = 100. The values of the parameters are α = 1.5, σx = 0.696575 (Eq. (19.52)),
σy = 1, K(α)=1.59922 (Eq. (19.59)), C(α)=2.737 (from the Table 19.4) and n = 1.
is itself a sum of harmonics, that is, a quasiperiodic function with parameters an. Figure
19.11 shows the numerically generated dichotomous coin-toss noise for t < 1000 with t0 =
100 and t is incremented in units of 1 and D = 1.
t
G(t)
0 200 400 600 800 1000
1
0
-1
FIGURE 19.11
The numerically generated dichotomous noise for t < 1000 with t0 = 100 and t is incre￾mented in units of 1 and D = 1.Skewness, Kurtosis and Students t Tests 487
19.8 Skewness, Kurtosis and Students t Tests
In this section, quantities useful to characterize an asymmetry of a distribution and com￾parison of two different distributions are defined.
19.8.1 Skewness
An asymmetry of distribution of random numbers around its mean can be characterized by
the skewness. It is defined as
S (x1, x2,...,xN ) = 1
N

N
i=1
xi − x¯
σ
3
, x¯ = 1
N

N
i=1
xi, (19.63)
where σ is the standard deviation of the given set of random numbers. It is a dimensionless
quantity. A distribution with a longer tail to the right of central maximum than to the left
said to be skewed to the right. For this distribution S is positive. A negative value of S
corresponds to a distribution with a longer tail to the left of central maximum than the
right. Such a distribution is called skewed to the left. For a perfectly symmetrical curve S
is zero.
19.8.2 Kurtosis
Kurtosis measures the degree of peakedness or flatness of a distribution relative to normal
distribution. Like skewness, the kurtosis is also a dimensionless quantity and is defined as
k (x1, x2,...,xN ) = 1
N

N
i=1

xi − x¯
σ
4
. (19.64)
A distribution with a relatively high peak is called leptokurtic. In this case kurtosis (k) is
> 0. A flat-topped distribution is called platykurtic for which k < 0. For normal distribution
k = 3.
19.8.3 Students t-Test
When two distributions have same variance but different mean values then the significance
of different mean values can be quantified by student’s t-test. It is a number between 0
and 1. Let A and B represent two distributions with xA and yB are the set of NA and NB
numbers, respectively. Denote ¯xA and ¯yB as mean values of the set numbers xA and yB,
respectively. Then, the student’s t is defined as
t = x¯A − y¯B
sD
, (19.65)
where the standard error sD of the difference of mean values is given by
sD =
 1
NA
+
1
NB
 NA
i=1 (xi − x¯A)
2 + NB
i=1 (yi − y¯B)
2
(NA + NB − 2) 1/2
. (19.66)
t can be computed as follows:
1. Compute ¯xA and ¯yB.488 Random Numbers
2. Compute (xi − x¯A)2 and (yi − y¯B)2.
3. Compute sD given by Eq. (19.66).
4. Compute t through Eq. (19.65).
Student’s is a pseudonym for W.S. Gosset, Biometrika 6(1908)1.
Whether two given distributions are different or same can be determined by computing
the chi-square–χ2 test quantity and is already discussed in Section 19.4.
19.9 Concluding Remarks
In this chapter, numerical algorithms to generate different types of random numbers are
discussed. The method of generating uniform random numbers is very simple. The methods
of generating other types of random numbers require uniform random numbers. The random
numbers play a key role in the Monte Carlo simulation of physical problems. Real physical,
mechanical and biological systems are subjected to some kind of noise. In nonlinear systems
the presence of even a noise of small strength can highly alter the response of the systems.
In some cases the presence of noise is beneficial. The influence of noise in a system can be
studied by adding appropriate noise in the mathematical model of the system. Now, one
may ask how to solve, for example, a differential equation, in the presence of a noise. This
is considered in Section 11.10.
19.10 Bibliography
[1] W. Horsthemke and R. Lefever, Noise-Induced Transitions. Springer, Berlin,
1984.
[2] J. Garcia-Ojalvo and J.M. Sancho, Noise in Spatially Extended Systems. Springer,
Berlin, 1999.
[3] P. Cordo, J.T. Inglis, S. Verschueren, J.J. Collins, D.M. Merfeld, S. Rosenblum,
S. Buckley and F. Moss, Nature 383:769, 1996.
[4] F. Jamarillo and K. Wiesenfeld, Chaos, Solitons & Fractals 11:1869, 2000.
[5] I.C. Gebeshuber, Chaos, Solitons & Fractals 11:1855, 2000.
[6] M. Chatterjee and M.E. Robert, Proc. SPIE 5110:348, 2000.
[7] Z.C. Long, F. Shao, Y.P. Zhang and Y.S. Qin, Phys. Lett. A 323:434, 2004.
[8] B. McNamara and K. Wiesenfeld, Phys. Rev. A 39:4854, 1989.
[9] P. Jung, Phys. Rep. 234:175, 1993.
[10] L. Gammaitoni, P. Hanggi, P. Jung and F. Marchesoni, Rev. Mod. Phys. 70:223,
1998.
[11] S. Rajasekar and M.A.F. Sanjuan, Nonlinear Resonances. Springer, New York,
2016.
[12] H. Gang, T. Ditzinger, C.Z. Ning and H. Haken, Phys. Rev. Lett. 71:807, 1993.
[13] A.S. Pikovsky and J. Kurths, Phys. Rev. Lett. 78:775, 1997.Bibliography 489
[14] B. Linder and L. Schimanwky-Geier, Phys. Rev. E 61:6103, 2000.
[15] C. Palenzuela, R. Toral, C.R. Mirasso, O. Calvo and J.D. Gunton, Europhys.
Lett. 56:347, 2001.
[16] RAND Corporation, A Million Random Digits with 100000 Normal Deviates. The
Free Press, Santa Monica, 1955.
[17] W. Jian-Sheng, C. Kan and Z. Fei, Computational Techniques in Theoretical
Physics. Lecture Notes, 1998.
[18] W.H. Press, S.A. Teukolsky, W.T. Vetterling and B.P. Flannery, Numerical
Recipes in Fortran. Foundation Books, New Delhi, 1993. Indian edition.
[19] G.E.P. Box and M.E. Muller, Ann. Math. Stat. 29:610, 1958.
[20] J.F. Ferna´ndez and C. Criado, Phys. Rev. E 60:3361, 1999.
[21] J.F. Ferna´ndez and R. Rivero, Comp. Phys. 10:83, 1996.
[22] C.S. Wallac, ACM Trans. Math. Software 22:119, 1996.
[23] J.H. Ahren and U. Dieter, Comm. Assoc. Mach. 15:873, 1972.
[25] J. Frost, Exponential distribution: Uses, parameters & examples; https://statistics
byjim.com/probability/exponential-distribution/ (accessed on 10 January 2022).
[26] G. Marsaglia, Ann. Math. Stat. 32:899, 1961.
[27] K.P.N. Murthy, Monte Carlo Basics. Indian Society for Radiation Physics,
Kalpakkam, 2000.
[28] N. Metropolis, A.W. Rosenbluth, M.N. Rosenbluth, A.H. Teller and E. Teller, J.
Chem. Phys. 21:1087, 1953.
[29] A. Alzaatreh, C. Lee, F. Famoye and I. Ghosh, Stat. Distri. Appl. 3:12, 2016.
[30] N.L. Johnson, S. Kotz and N. Balakrishnan, Continuous Univariate Distributions.
Volume-1. Wiley, New York, 1994.
[31] L. Rizo, Cauchy distributions for Jitter in IP networks; https://www.academia.
edu/5044387/Cauchy−Distribution−for−Jitter−in−IP−Networks (accessed on 10
January 2022).
[32] S. Casault, A.J. Groena and J.D. Linton, On the use of the Cauchy distri￾bution to describe price fluctuations in R&D and other forms of real assets;
https://idus.us.es/bitstream/handle/11441/58827/On%20the%20use%20of%20-
the%20Cauchy%20distribution.pdf?sequence=4&isAllowed=y (accessed on 10
January 2022).
[33] P. L´evy, Theorie de l’Addition des Variables Aleatories. Gauthier–Villars, Paris,
1937.
[34] A. Patel and B. Kosko, IEEE International Conference on Acoustics, Speech and
Signal Processing 3:1413, 2007.
[35] Z. Liao, K. Ma, S. Tang, Md. Shamum Sarker, H. Yamahara and H. Tabala, Res.
Phys. 27:104475, 2021.
[36] Y. Zhao and D. Li, Mod. Phys. Lett. 33:1950252, 2019.
[37] A. Patil and B. Kosko, IEEE Trans. Neural Netw. 19:1993, 2008.
[38] H. Zhou, Y. Li, W. Li and J. Feng, Appl. Anal. 101:2535, 2022.
[24] Exponential Distribution Examples in Real Life; https://studiousguy.com/exponential￾distribution-examples/ (accessed on 10 January 2022).490 Random Numbers
[39] Y. Li, Y. Xu, J. Kurths and X. Yue, Phys. Rev. E 94:042222, 2016.
[40] T. Cul, A. Din, P. Liu and A. Khan, Comp. Meth. Biomech. Biomed. Eng., 2022.
DOI: 10.1080/10255842.2022.2106784
[41] P. Liu, L. Huang, A. Din and X. Huang, J. Biol. Dyn. 16:236, 2022.
[42] Y. Sabbar, D. Kiouach and S.P. Rajasekar, Int. J. Dyn. Control 11:122, 2023.
[43] R.N. Mantegna, Phys. Rev. E 49:4677, 1994.
[44] J.M. Chambers, C.L. Mallows and B.W. Stuck, J. Am. Stat. Assoc. 71:340, 1976.
[45] M. Leccardi, Comparison of three algorithms for L´evy noise generation;
http://www.unipa.it/ocs/sito-strategico/relazioni/publicazioni−secondo−anno/-
AL1.pdf (accessed on 10 January 2022).
[46] R. Kapral and S.J. Fraser, J. Stat. Phys. 71:61, 1993.
[47] J.P. Porra, J. Masoliver and K. Lindenberg, Phys. Rev. E 48:951, 1993; 50:1985,
1994.
[48] Y.R. Sivathanu, C. Hagwood and E. Simiu, Phys. Rev. E 52:4669, 1995.
[49] D.A. Potoyana and P.G. Wolynes, J. Chem. Phys. 143:195101, 2015.
[50] S. Rajasekar, M.C. Valsakumar and S. Paulraj, Physica A 261:417, 1998.
[51] I. Broussell, I. L’Heureux and E. Fortin, Phys. Lett. A 225:85, 1997.
[52] M.R. Roussel and J. Wang, J. Phys. Chem. A 105:7371, 2001.
[53] T. Yamada, T. Horita, K. Ouchi and H. Fujisaka, Prog. Theor. Phys. 116:819,
2006.
[54] R. Mankin, A. Sauga, A. Ainsaar, A. Haljas and K. Paunel, Phys. Rev. E
69:061106, 2004.
[55] D. Das and D.S. Ray, Phys. Rev. E 87:062924, 2013.
[56] J. Buceta and K. Lindenberg, Phys. Rev. E 68:011103, 2003.
[57] R. Rozenfeld, A. Neiman and L. Schimansky-Geier, Phys. Rev. E 62:R3031, 2000.
[58] M.B. Tarlie and R.D. Astumian, Proc. Natl. Acad. Sci. U.S.A. 95:2039, 1998.
[59] S.L. Ginzburg and M.A. Pustovoit, Phys. Rev. Lett. 80:4840, 1998.
19.11 Problems
A. Uniform Random Numbers
19.1 Generate uniformly distributed random numbers in the intervals a) [−0.5, 0.5], b)
[1, 2] and c) [−1, −2]. For each sequence compute mean and variance and compare
them with the exact values.
19.2 For a sequence of random numbers generated by the Park–Miller method verify
that a) χ2 ≈ 0, b) C(k) ≈ 0, for k > 0, and c) the mean value passes the
randomness test at 5% level.
19.3 For a random number generator available in your computer check whether it
passes the randomness tests.Problems 491
19.4 For a sequence of uniformly distributed numbers in the interval [0, 1] verify that
kth moments mk are finite. Also, verify that m0 = 1, m1 = ¯x ≈ 0.5 and m2 =
σ2 + m2
1, where σ2 = 1/12.
19.5 Let y(N, x) = (1/N)
N
i=1 xi where xi are uniform random numbers in the inter￾val [0, 1]. Generate a large number of y’s with N = 2. Compute the probability
distribution of y, mean and variance. Repeat the above calculation for N = 5,
10, 50 and 100. Write a short note on your observation.
19.6 Write a single Python program which do all the following:
(a) Generate N = 5 × 104 uniformly distributed random numbers.
(b) Calculate mean and variance of the sequence.
(c) Calculate the probability distribution of the numbers.
19.7 Computer-generated random numbers are not really random. Why?
B. Gaussian Random Numbers
19.8 In the Box–Muller method instead of cosine function one may use sine function
(Eq. (19.30)) also. Generate Gaussian random numbers with sine function. Sketch
the probability distribution of the obtained numbers. Compare its mean and
variance with the assumed values.
19.9 Numerically, shows that the moments of Gaussian random numbers diverge with
N (the number of random numbers). (Compute kth moment as a function of N
and show that it diverges with N).
19.10 Verify that
(a) the sum YM = M
i=1 xi, where {xi} are Gaussian random numbers, is also a
Gaussian with mean Mµ and variance Mσ2,
(b) the mean and variance of the sum YM scaled by M are µ and σ2/M, respec￾tively, and
(c) the mean and variance of YM/
√
M are µ
√
M and σ2, respectively.
19.11 Compare the Box–Muller and the Ferna´ndez–Criado algorithms with special em￾phasis on mean, variance, probability distribution curve and CPU time taken.
Form a table showing these values for the number of random numbers N =
104, 2 × 104,..., 1 × 105.
19.12 A method of generating Gaussian random numbers using the central-limit theo￾rem is the following: Generate N uniform random numbers in the interval [0, 1].
Consider the sum
x =
N
i=1 yi

− (N/2)
N/12 .
Generate a large number of x’s. For large N the x’s form a sequence of Gaussian
random numbers with mean 0 and variance unity. Take N = 2, 3,... and demon￾strate the approach of x to Gaussian. Did you observe tails in the probability
distribution?
19.13 Write a single program which do all the following.492 Random Numbers
(a) Generates N = 5 × 104 normally distributed random numbers with mean =
0 and σ2 = 1,
(b) calculates the mean and variance of the obtained numbers and
(c) computes the probability distribution of the numbers.
19.14 Can you apply the method of inversion to generate Gaussian random numbers?
Why?
C. Random Sampling Techniques
19.15 Numerically, show that the moments
mk = 1
N

N
i=1
(xi − x¯)
k , k = 1, 2,..., 10
for the exponentially distributed numbers diverge.
19.16 Calculate the mean and variance of exponentially distributed 5 × 104 numbers
with [α, β] as a) [0, 4], b) [0, 5], c) [0, 6], d) [0, 10], e) [0, 20] and f) [0, 50].
What did you observe?
19.17 Count the number of discarded points in generating 104 numbers with exponential
distribution by rejection technique. Do this for the following bounding function
cg(x)=[α, β] = [0, 1], [0, 2], [0, 3], [0, 4] and [0, 5]. Study the dependence of the
discarded points on the parameter β.
19.18 Generate a sequence of random numbers with the distribution
f(x) =



√2 e−√2 x, x ≥ 0
0, x < 0.
Also, compute its mean, variance and probability distribution.
19.19 Generate a sequence of numbers with the distribution (1/
√2)e−√2 x, −∞ <x<
∞. What are the values of its mean and variance?
19.20 Generate a sequence of N random numbers with the exponential distribution,
Eq. (19.36). Sum them and divide by √
N. Call it as y. Generate a large number
of y. Then, compute their probability distribution. Compare it with the Gaussian
distribution of the same mean and variance.
19.21 Obtain a sequence of random numbers with the distribution
f(x) =



sin2 x, 0 ≤ x ≤ π
0, elsewhere
using the expression x = 2 sin−1 √y where y’s are uniform random numbers in
the interval [0, 1]. Verify that its mean is π/2 while the variance is ≈ 0.468. Draw
the probability distribution curve of the generated numbers.
19.22 Generate random numbers with the Gaussian distribution
f(x) = 1
σ
√2π e−(x−µ)2/(2σ2) ,Problems 493
where σ2 and µ are the variance and mean, respectively, with fixed mean and
variance. Fix the range of the Gaussian numbers as [−5, 5]. Compute the mean
and variance and sketch the probability distribution of the obtained numbers.
19.23 Obtain a sequence of random numbers with the circular probability distribution
density function
f(x) = 4
π
1 − x2 , 0 ≤ x ≤ 1.
(A convenient choice of the bounding function in the rejection technique is g(x) =
1, 0 ≤ x ≤ 1 and c is 4/π.) Verify that its mean is ≈ 0.43 and the variance is
≈ 0.074.
19.24 The Cauchy distribution is given by
f(x) = 1
π
a
a2 + x2 , −∞ <x< ∞
where a > 0 is a scale factor. Sketch the graph of f(x) for a = 1, −10 <x< 10.
Employing the rejection technique, generate random numbers with the Cauchy
distribution. Compare the numerically obtained distribution with the exact dis￾tribution.
19.25 Consider the Metropolis algorithm for generating Gaussian random numbers with
mean zero and variance 1. For each value of  in the interval 0 to 4 with step size
0.01 compute the acceptance ratio. Find the optimum value of  for which the
acceptance ratio is 0.5. Repeat the above for several values of variance.
19.26 Generate a sequence of random numbers with the following probability distribu￾tions:
(a) f(x) = x e−x.
(b) f(x) = xN e−x/N! (Poisson distribution).
(c) f(x) = MCN xN (1 − x)M−N (Binomial distribution).
19.27 The gamma density function is given by
f(x) = (
√
N )N
(N − 1)! xN−1 e−√N x , 0 ≤ x < ∞.
Its mean is √
N and variance is 1. For N = 2, 3 and 10 generate random numbers
with the above distribution in the interval [0, 10]. (For larger values of N the
above distribution is a bell-shaped form with a peak at x = N).
19.28 Write a short note on comparison of inverse and rejection methods.
19.29 Employing the rejection technique, generate a sequence containing 2000 pairs
of random numbers {(xi, yi)} with the following distributions and also plot the
numbers in the x − y plane.
(a) f(x, y)=e−(x+y) , x, y ∈ [0, 10].
(b) f(x, y)=e−(x2+y2) , x, y ∈ [−5, 5].
(c) f(x, y) = 1
1+(x2+y2) , x, y ∈ [−100, 100].
(d) f(x, y) = 4
π
2 − (x2 + y2), x, y ∈ [−1, 1].494 Random Numbers
D. Skewness
19.30 Write a Python program to read N numbers xi from an external data file and
then to compute the skewness of the distribution of xi.
19.31 Verify that for a set of Gaussian random numbers S ≈ 0.
19.32 Generate random numbers with the distribution xe−ax for several values of a. For
each value of a sketch the distribution curve. Calculate S as a function of a.
E. Kurtosis
19.33 Write a Python program to calculate k of a given set of N random numbers. Read
the data from an external data file.
19.34 Calculate k for Gaussian random numbers as a function of N. Choose N = 10,
20, 50, 100, 200, 500, 1000, 2000, 5000 and 10000. Verify that k → 0 as N → ∞.
19.35 Consider the two-dimensional map
xn+1 = yn , yn+1 = −xn − r sin yn + Γ mod 2π .
Fix r = 6.34 and (x0, y0) = (5, 3). For Γ = 0.3 generate N = 105 points. Verify
that the distribution of x is normal. Calculate k(N) and show that it becomes 0
for large N. For Γ = 0 verify that the distribution of x deviates from normal and
k diverges with N.
F. Students t-Test
19.36 Write a Python program to compute t for a given two sets of N random numbers.
19.37 Generate two sets of normal distributions each containing 105 numbers with the
same variance but with different means. Define d = ¯xA − y¯B. For several values
of d compute t and sketch the graph of t versus d.
G. Chi-Square Test
19.38 Write a Python program to compute χ2 of two given sets of random numbers.
For verifying the program see the next two problems.
19.39 Collect two sets of 105 numbers from the following distributions. Divide their
range into, say, 50 bins. Compute χ2 and verify that in all the cases χ2 ≈ 0.
(a) Uniform distribution.
(b) Gaussian distribution.
(c) x e−x distribution.
(d) e−x distribution.
19.40 Calculate χ2 for two sets of numbers with normal distribution but with different
mean and variance. Verify that χ2 is nonzero.20
Monte Carlo Technique
20.1 Introduction
Computer simulations are methods that try to model a physical process rather than solving
the equation that governs the physical process. Particularly, problems that are stochastic
in nature are most suitable for computer simulation. Monte Carlo method is of such a
type. The method has applications in traffic flow, stellar evolution, nuclear reactor design,
radiation cancer therapy, oil-well exploration and so on. The method uses random numbers
in a calculation that has the structure of stochastic process, a sequence of states whose
evolution is determined by random events. Generally, any problem analyzed or studied
with the use of random numbers and probability statistics is called a Monte Carlo method
or an experiment or a simulation. The Monte Carlo simulation can be applied to study
systems whose time evolution is not described by fully deterministic manner but behaves
stochastically.
Nicholas Metropolis gave the name Monte Carlo to random sampling techniques. The
term Monte Carlo is due to the name of the city of Monte Carlo (in Monaco which is
located along the French Riviera between Mediterranean Sea and France) famous for its
casino. The point is that the roulette is one of the simplest mechanical devices for generation
of random numbers. The Monte Carlo method was introduced by the great mathematical
physicists John von Neumann, Enrico Fermi and Stanislaw Ulam during 1949. There were
some isolated instances earlier where Monte Carlo method has been used in some form or the
other. The name Monte Carlo was initially applied to a class of mathematical methods used
for the development of nuclear weapons in Los Alamos in the 1940s. The earliest documented
use of random sampling to find solution of an integral is that of Comte de Buffon [see
Problem 12 at the end of the present chapter]. In India, P.C. Mahalanobis [1,2] exploited
random sampling technique to solve a variety of problems like the choice of optimum size
and shape of plots in experimental works, etc. Description of several Monte Carlo techniques
appeared in a paper by Kelvin [3]. He used random sampling to aid in evaluating certain
integrals in the kinetic theory of gases. In 1930s, Fermi made some numerical experiments
and are now called Monte Carlo calculations.
To understand the basic idea of the Monte Carlo method, assume that it is desired to
find the area of the plane S shown in Fig. 20.1. It may be an arbitrary figure specified
graphically or analytically. The area of S can be computed employing the Monte Carlo
method. For simplicity choose the square ABCD in Fig. 20.1 as a unit square. Generate a
set of uniformly distributed N random numbers (points) inside the square ABCD. Count
the number of points falling within the plane S and, say, it is NS. Geometrically the area
of S is approximately NS/N. Greater the value of N higher the accuracy of the estimate.
Essentially, if the geometry of the shape S is known then the ratio of the hits in the area S
and throws is the area of the shape. The above procedure is also called hit and miss method
[4].
DOI: 10.1201/9781032649931-20 495496 Monte Carlo Technique
D C
A B
S
x
y
FIGURE 20.1
An arbitrary plane S within a square ABCD.
There are three distinct features of the Monte Carlo method:
1. The structure of computation algorithm is simple. A program is developed to
perform only one random trial. This trial is repeated larger number of times with
each being independent of all the others. The results of all the trials are then
averaged.
2. The error is proportional to D/N where D is a constant and N is the number
of trials.
3. The simulation with two different sequences of random numbers, drawn from
same distribution with same characteristic properties such as mean and variance,
will not produce identical results but will give values which are close to each other
within some statistical error.
One should note that high accuracy cannot be obtained with the Monte Carlo approach.
It never gives an exact answer, rather its conclusions indicate that the answer is so and so,
within such and such an error, with such and such probability. In other words, it provides
an estimate of the value of the numbers sought in a given problem. It is computationally
effective, compared with deterministic methods for higher-dimensional problems. Generally,
a Monte Carlo simulation application has the following primary components:
1. A random number generator – An appropriate uniform random number generator
such as the Park and Miller (see Subsection 19.3.7) method is necessary.
2. Probability distribution function – The given physical process or phenomenon
must be described by one or more appropriate probability distribution functions.
3. Sampling rule – A description for sampling from the given probability distribution
function must be specified.
4. Error estimation – It is necessary to determine the variance of resulting data as
a function of number of trials.Evaluation of Definite Integrals 497
In this chapter, some simple applications of Monte Carlo method are described. Partic￾ularly, the following problems are considered:
1. Evaluation of definite integrals.
2. Square-root of real positive numbers.
3. Estimation of value of π.
4. Estimation of value of e.
5. Electronic distribution of hydrogen atoms.
6. Radioactive decay.
7. Diffusion of neutrons in a moderating material.
8. Percolation.
20.2 Evaluation of Definite Integrals
It is desired to compute the definite integral of the form
I =
 b
a
f(x) dx . (20.1)
A. Monte Carlo Estimator
Consider the integral
I =
 b
a
f(x)p(x) dx , (20.2)
where p(x) is a probability distribution function and
 b
a
p(x) dx = 1 . (20.3)
Then, the integral given by Eq. (20.2) can be thought of as the solution of the probabilistic
problem of computing the average of f(x), ¯f, over the function p(x). To compute ¯f, draw
a large number (N) of random numbers for x from the distribution function p(x). Then,
¯f = f ≈ 1
N

N
i=1
f (xi). (20.4)
¯f is called the Monte Carlo estimator for the integral in Eq. (20.1). Write
 1
0
f(x) dx ≈ 1
N

N
i=1
f (xi) = ¯f , (20.5)
where xi are uniformly distributed random numbers in the interval [0, 1]. Note that the
p(x) which appeared in the integral in Eq. (20.2) is absent in Eq. (20.5). What is the reason
for this? xi is distributed according to p(x). Obviously, there would be more points in the
region where p(xi) is large. But, if p(xi) is chosen as a uniform distribution then p(xi)=a
constant.498 Monte Carlo Technique
x
f (x 1 )
x 1 a b
(a)
x
f (x 2)
x 2 a b
(b)
FIGURE 20.2
Approximate area under the curve f(x) in the interval [a, b] with (a) x = x1 and (b) x = x2.
B. Connection Between ¯f and the Value of the Integral in Eq. (20.1)
Consider the Figs. 20.2a and b.  b
a f(x)dx represents the area under the curve f(x) in
the interval [a, b]. For x = x1 the value of the integral can be approximated by the area
of the shaded rectangle in Fig. 20.2a. For another value of x, say, x = x2 the value of the
integral can be approximated by the area of the rectangle in Fig. 20.2b. The area of the
rectangle in Fig. 20.2a underestimates the area below the curve (value of the integral) while
the area of the rectangle in Fig. 20.2b overestimates the area below the curve (value of the
integral). Averaging the area of rectangles formed by considering a large number of values of
x in the interval [a, b] gives an approximate value of the integral. The areas of the rectangles
for the case of x = x1 and x = x2 are (b − a)f(x1) and (b − a)f(x2), respectively. Then,
IN =
 b
a
f(x) dx = (b − a) ¯f , (20.6)
where ¯f is given by Eq. (20.4) with xi are uniformly distributed random numbers in the
range [a, b].
If the values of |a| and |b| are sufficiently small then xi can be chosen as uniformly
distributed random number in the interval [a, b]. That is, p(x) in Eq. (20.2) is a uniform
distribution. In the case of a = 0 and b = ∞, practically, it is difficult to generate random
numbers in the interval [0, ∞]. In this case choose p(x) as some other suitable function.
For example, in the integral  ∞
0
e−x cos x dx the function p(x) can be e−x whereas in
 ∞
0
e−x2
dx, it can be e−x2
.
C. The Monte Carlo Algorithm
The Monte Carlo algorithm of calculating the value of the integral (20.1) using (20.6) is
the following:
1. Generate N uniformly distributed random numbers xi, i = 1, 2,...,N in the
interval [a, b].
2. For each xi calculate f(xi).
3. Calculate the average value of f(xi), that is ¯f, using Eq. (20.4).
4. (b − a) ¯f is the value of the given integral I.Evaluation of Definite Integrals 499
D. Convergence of the Monte Carlo Estimate
In the limit N → ∞ the probability of getting IN = I is 1. That is,
P

lim
N→∞ IN = I

= 1 . (20.7)
The above theorem guarantees that in the limit of very large number of points the Monte
Carlo estimates converge to the exact answer. But it does not give the rate of convergence.
However, much more information is obtained from the central-limit theorem.
Define
σ2 =
 b
a
(f(x) − I)
2p(x) dx (20.8)
and obtain
σ2 =
 b
a

f 2p − 2fIp + I2p

dx
= f 2 − 2If + I2
= f 2 − 2I2 + I2
= f 2 − I2
= f 2−f
2. (20.9)
Also, define a random variable
yN =
√
N
σ (IN − 1). (20.10)
Further,
P {a ≤ yN < b} = 1
√2π
 b
a
e−y2/2 dy . (20.11)
Equation (20.11) implies that for very large N, IN is a Gaussian distribution with mean I
and variance σ2/N. In the limit N → ∞, IN becomes very narrow near I.
E. Error in the Monte Carlo Estimation
Let us calculate the error in the Monte Carlo estimation. Write
I = IN + , (20.12)
where  is the error. From Eq. (20.10)
|| = σ
√
N (20.13)
and the variance σ2 is given by
σ2 = N
N − 1

1
N

N
i=1

(f (xi))2 − ¯f 2


. (20.14)
For large N, drop −1 in N − 1 in the above equation and then obtain
|| = σ
√
N = 1
√
N

1
N

N
i=1

(f (xi))2 − ¯f 2

1/2
. (20.15)500 Monte Carlo Technique
TABLE 20.1
The Monte Carlo estimation of the integral  2
0
exdx. The exact value of the integral is
6.38906... . σ is the standard deviation error.
N value Integral value % of error σ
1000 6.16404 3.521 0.09727
2000 6.27201 1.832 0.06992
10000 6.41480 0.403 0.03187
50000 6.40922 0.316 0.01432
100000 6.38577 0.051 0.01003
200000 6.39625 0.113 0.00713
300000 6.39273 0.057 0.00583
400000 6.38591 0.049 0.00504
500000 6.38653 0.040 0.00451
The error in the Monte Carlo method decreases 1/
√
N as N increases.
F. Examples
Now, present two examples of numerical computation of definite integrals.
Example 1:
Evaluate the integral  2
0
exdx by employing the Monte Carlo method.
Since the integration is from 0 to 2 one has to use random numbers xi, x ∈ [0, 2]. A Python
or C++ program can be developed to calculate the value of I, the percentage of error in the
value of the integral and the standard deviation for various values of N. Table 20.1 presents
the computed value of the integral as a function of N. For a sufficiently large value of N
the computed value of the integral is close to the exact value 6.38906 ....
Example 2:
Evaluate the integral  1
0
1
1 + x
dx by employing the Monte Carlo method using the 5 random
numbers 0.501, 0.762, 0.243, 0.892, 0.123 (for the illustrative purpose only a few random
numbers are chosen). The exact value of the integral is ln(1 + x)|
1
0 = 0.69315... .
The value of the integral I = (b − a) ¯f = ¯f is obtained as
¯f = 1
5 [f(0.501) + f(0.762) + f(0.243) + f(0.892) + f(0.123)]
= 1
5 [0.66622 + 0.56754 + 0.80451 + 0.52854 + 0.89047]
= 3.45728
5
= 0.69146.nth Root of a Real Positive Number 501
TABLE 20.2
The Monte Carlo computation of √10. Here, N = 2k. The exact value of √10 is 3.16228 ....
k N √10 % of error
14 16384 3.15924 0.09604
15 32768 3.16064 0.05165
16 65536 3.16116 0.03524
17 131072 3.16080 0.04682
18 262144 3.16074 0.04875
19 524288 3.16171 0.01799
20 1048576 3.16167 0.01917
21 2097152 3.16223 0.00140
20.3 nth Root of a Real Positive Number
How does one estimate x1/n using Monte Carlo method? First, find two numbers between
which the root lies. Let these two numbers being N1 and N2 with N1 < N2. For simplicity,
these two numbers may be chosen as integers. The value of N2 is chosen such that (N2−1)n ≤
x ≤ Nn
2 . Then, N1 = N2 − 1. The procedure is summarized below.
1. Read the value of x and N.
2. Find N2 and then N1.
3. Generate N (sufficiently large) uniformly distributed random numbers y in the
interval [N1, N2].
4. Count the number of random numbers y satisfying the condition yn < x. Let this
number being Nc.
5. Calculate N1 + (Nc/N). This number is the value of x1/n.
Example:
Compute √10 by the Monte Carlo method.
For √10 the values of N1 and N2 are 3 and 4, respectively. Table 20.2 displays the computed
value of √10 as a function of N. If the nth root of a number is perfectly an integer then
the computed result would exactly coincide with the true value even for a small number of
samplings.
20.4 Estimation of π
Another application of Monte Carlo method is the calculation of π. The early history of
the calculation of π is interesting [5]. The value of π is known with high accuracy for a
long time. An Egyptian text known as the Rhind Papyrus dating to 1650 BC contains the502 Monte Carlo Technique
FIGURE 20.3
Archimedes idea of successive approximation of value of π. At nth stage, the value of π is
given by the formula (20.19).
statement that
area of a circle = area of the square with side
8/9 times the diameter of the circle . (20.16)
That is,
π
d
2
2
=
8
9
d
2
(20.17)
which gives
π = 256
81 = 3.16049 . (20.18)
A. Archimedes Method of Computation of π
Later Archimedes proposed a method which could give the value of π to any desired
accuracy. His idea was to take a circle and inscribe in it a series of regular polygons of more
and more sides as shown in Fig. 20.3. Remember that all sides of a regular polygon are
equal in length. The perimeter of the polygon is slightly less than the circumference of a
circle. However, the polygons approach the circle closer and closer as the number of sides
increases. Then, write
2πr = na, (20.19)
where r is the radius of the circle and a is the length of the side of the inscribed polygon
with n sides. Calculate π from the approximate formula
π = na
2r . (20.20)
Archimedes repeated the process with circumscribing polygons as shown in Fig. 20.3. Using
the inscribed and circumscribing polygons of 6 sides to 96 sides Archimedes calculated the
value of π in the range 3.1408 and 3.1428.
B. Monte Carlo Method of Computation of π
A Monte Carlo method of calculation of π is as follows, which is based on the relation
that area of a circle of radius r is πr2. Consider a square with size unity and its inscribedEstimation of π 503
S
C
x
y
0 1
1
0
FIGURE 20.4
S-square part of a C-circular curve.
circle of radius, r, unity as shown in Fig. 20.4. The ratio of the area of the (1/4)th of the
circle lying inside the square and the area of the square is π/4. One can also consider a
square with a full circle embedded in it. In this case, the above ratio is π. Generate N pairs
of uniform random numbers in the range [0, 1] and designate the ith pair as (xi, yi). xi and yi
are the random numbers for the coordinates x and y, respectively. If these points are placed
inside the square, some will fall within the circle and some will not. Count the number of
points that fall inside the quarter circle and call it as Nc. Whether a point (xi, yi) falls
inside the quarter circle can be easily identified. For a pair (xi, yi) calculate di = x2
i + y2
i .
If di < r(= 1) then it lies inside the quarter circle. Then, Nc/N is the ratio of the areas of
the quarter circle and the square, that is,
πr2/4
r2 = Nc
N . (20.21)
Thus,
π = 4Nc
N . (20.22)
Using the above formula the value of π is computed for N = 2k, k = 14, 15,..., 21. The
result is presented in the Table 20.3. The statistical convergence to the correct answer as the
sample size increases is seen. For some other methods for the estimation of π see Problems
3, 4 and 12 at the end of the present chapter.
C. A Simple Exercise
Let us present a quick illustration of the Monte Carlo estimation of the value of π.
Consider a quarter circle of radius r = 1 unit lying inside a square with sides 1 unit in the
x − y plane. Compute the value of π employing the Monte Carlo method using 10 pair of
random points (xi, yi) generated using the Ran function in a calculator.
By the Monte Carlo method, the value of π can be computed from π = 4Nc/N where N
is total number of pair of points (xi, yi) and Nc is the number of pair of points lying inside
the quarter circle of radius r = 1 unit. For a pair (xi, yi) if d2
i = x2
i + y2
i < 1 then it lies504 Monte Carlo Technique
TABLE 20.3
The Monte Carlo estimation of value of π by the ratio of area of a quarter circle and a
square. Here, N = 2k. N is the number of points in the square while Nc is the number of
points within the quarter circle.
kN Nc π value % of error
14 16384 12839 3.13452 0.22509
15 32768 25731 3.14099 0.01915
16 65536 51425 3.13873 0.09103
17 131072 102837 3.13834 0.10366
18 262144 205977 3.14296 0.04351
19 524288 411735 3.14129 0.00968
20 1048576 823762 3.14240 0.02578
21 2097152 1647233 3.14185 0.00811
inside the quarter circle. Table 20.4 is obtained with the set of random numbers generated
using a calculator. As Nc = 8 the value of π ≈ 4×8/10 = 3.2. Note that different realization
of set of ten pairs of random numbers will give different values π and in some cases the
result can be far from the actual value.
D. Standard Error
For a fixed value of k, if the computation of π (as well as e in the next section) is
repeated M times with N different sequences of uniformly distributed random numbers one
would get slightly different M values for π. The spread of the final result is specified by the
standard error of the final result. It is given by
S2
e = 1
M

M
i=1

Ri − R¯
2 , R¯ = 1
N

N
i=1
Ri , (20.23)
where Ri is the value of the quantity calculated in the ith repetition of the numerical scheme.
The standard error is similar to the standard deviation. What is the difference between these
two? The standard deviation describes the spread of the numerically computed values of
the quantity (for example, value of π or e or mean of a uniform distribution) with the
true value. The standard error, on the other hand, represents the spread of numerically
computed values with the mean value.
TABLE 20.4
The Monte Carlo estimation of value of π using only 10 pairs of uniformly distributed
random numbers.
xi yi d2
i Nc xi yi d2
i Nc
0.218 0.706 0.545 1 0.094 0.050 0.011 5
0.958 0.945 1.810 1 0.881 0.119 0.790 6
0.664 0.725 0.966 2 0.885 0.116 0.796 7
0.422 0.622 0.564 3 0.782 0.700 1.101 7
0.330 0.417 0.282 4 0.041 0.439 0.194 8Estimation of Value of e 505
20.5 Estimation of Value of e
The previous section presented an estimation of π by a Monte Carlo method. Another
naturally occurring irrational number in mathematics is the base of the natural logarithm
e. Mohazzabi [6] described three Monte Carlo algorithms for the estimation of e. Here, two
of them are described.
20.5.1 Dart Method
Let us consider a dart board, divide it into R equal size regions and N darts are thrown onto
it. The probability for a dart to strike a region is p = 1/R. What is the probability P(n) of
finding n darts in a given region out of N throws? It is given by the binomial distribution
P(n) = CN
n pnqN−n , (20.24)
where q = 1 − p and CN
n are the binomial coefficients N!/(n!(N − n)!). Therefore, the
probability of finding an empty region is
P(0) = qN = (1 − p)
N . (20.25)
When R is set to N then
P(0) = (1 − 1/N)
N . (20.26)
For large N, 1/N  1 and the series expansion of (1−1/N)N is approximated to the series
expansion of e−1 (verify). Further, for p  1, N  1 and R = N the binomial distribution
is approximated by the Poisson distribution:
P(n)=e−1/n! . (20.27)
Therefore, P(0) = P(1) = e−1.
From the above, the following algorithm to estimate e is obtained.
1. Throw a large number of darts, N, at a board which has been divided into N
equal size regions.
2. Count the empty cells. Denote this number as N0.
3. The ratio N0/N is e−1, that is, e = N/N0.
What is the expression for e if the number of cells (N1) with occupancy 1 is counted?
In a computer algorithm throw of a dart N times can be replaced by N uniformly dis￾tributed random numbers in the interval [1, N]. In this case, the algorithm is the following:
1. Divide the range [1, N] into N equal sizes.
2. Treat the N uniformly distributed random numbers in the interval [1, N] as N
points on a line.
3. Note down the number of points falling in each interval such as 1 − 2, 2 − 3, ...,
(N − 1) − N.
4. Count the number of empty intervals N0.
5. The ratio N/N0 is e.
It is easy to develop a program code to compute e as a function of N where N = 2k,
k = 14, 15,..., 21. Table 20.5 displays the obtained result.506 Monte Carlo Technique
TABLE 20.5
The Monte Carlo estimation of the value of e by a dart method. Here, N = 2k. The exact
value of e is 2.71828... .
kN N0 e value % of error
14 16384 6029 2.71853 0.02752
15 32768 12070 2.71483 0.12691
16 65536 24061 2.72374 0.20100
17 131072 48484 2.70341 0.54714
18 262144 96481 2.71705 0.04514
19 524288 193122 2.71480 0.12795
20 1048576 386234 2.71487 0.12536
21 2097152 771695 2.71759 0.02532
20.5.2 Derangement Method
Another simple method for estimation of e is a derangement method. Consider a permuta￾tion of N objects labelled as say {1, 2, 3,...,N}. The objects are shuffled once. Now, check
whether all the objects are moved from its original place. Such a permutation is called a
derangement of the objects. The number of derangement of N objects is given by
Nd = N!
 1
2! − 1
3! +
1
4! −··· + (−1)N 1
N!

. (20.28)
Since ex =1+ x + x2/2! + x3/3! + ··· the square bracket in the right-side of Eq. (20.28)
is approximated as e−1 so that Nd = N!/e. The number of permutations is N!. For a
reasonably large N the probability of finding the derangement is e−1. The following is a
systematic algorithm of this method.
(1) Consider an array of say N = 10 numbers. For simplicity and convenience choose
them as IX(i) = i, i = 1, 2,...,N. Set Nd = 0.
(2) Generate N uniformly distributed random numbers in the interval [1, 10]. Make
them as integer numbers again in the interval [1, 10]. Using these numbers the
values of IX(i) are exchanged as per the Step (3).
(3) First, change the content in IX(1). If the first random number is, for exam￾ple, 6 then exchange the contents in IX(1) and IX(6). If the second random
number is supposed, say, 5 then exchange IX(2) and IX(5). Repeat this for
IX(3),...,IX(N). Now, N number of IX’s are shuffled.
(4) Next, check for derangement. That is, check for IX(i) = i, i = 1, 2,...,N. If this
is realized then increment the value of Nd by one. If IX(i) = i for at least one of
the N values of i then no increment of Nd.
(5) Repeat the above procedure for a large number of times, say, NN = 105 times
and record the total derangement Nd.
Then, e = NN /Nd. Let this is a value of e in one trial. An average value of e can be obtained
over many trials. Table 20.6 displays the value of e in 10 trials. The average value of e is
2.71006 whereas the true value is 2.71828 .... The average value of e obtained in 20, 50, 100Electronic Distribution of Hydrogen Atom 507
TABLE 20.6
The Monte Carlo estimation of e by the derangement method. Nd is the number of derange￾ments observed in NN = 105 shuffling. The average value of e is 2.71006.
Trial Nd e value Trial Nd e value
1 36991 2.70336 6 36961 2.70555
2 36943 2.70687 7 36919 2.70863
3 36894 2.71047 8 36872 2.71209
4 36568 2.73463 9 36937 2.70731
5 36901 2.70995 10 37014 2.70168
and 500 trails are 2.7125, 2.7150, 2.7169 and 2.7180, respectively. Mohazzabi [6] computed
e with 105 shuffling and 103 trials. The obtained e value is 2.7181 ± 0.0002.
20.6 Electronic Distribution of Hydrogen Atom
In the previous sections, the features of Monte Carlo technique are illustrated in the calcula￾tion of values of nth root of positive numbers, π, e and evaluation of definite integrals. This
technique can also be applied to real physical problems. For example, it helps us visualize
electronic distribution of harmonic oscillator, scattering of particles from a potential, phase
transition and radiation transport studies and computer simulation of certain physical and
engineering problems. In this section, the application of Monte Carlo technique to the repre￾sentation of orbitals of hydrogen atom [7] is discussed. A few other interesting applications
of Monte Carlo technique are presented in the subsequent sections.
20.6.1 Hydrogen Atom and Its Eigenfunctions
The hydrogen atom is a two-particle system, consisting of the atomic nucleus of charge Ze
and an electron of charge −e. Let us use spherical polar coordinates system. If the nucleus
is assumed to remain in static then the potential energy of the system is −Ze2/r where r is
the distance between the electron and the nucleus. This potential is spherically symmetric
because V does not depend upon θ and φ and is only a function of r.
The Schr¨odinger equation of the system is

− 2
2µ
∇2 + V

ψ = Eψ , (20.29)
where µ = memn/(me + mn), me-mass of the electron and mn-mass of the nucleus. For
the construction of solution of Eq. (20.29), the meaning of eigenfunction, eigenstate and
quantum numbers refer refs. [8-10]. Its eigenfunctions are denoted as ψnlm where n, l and
m are quantum numbers and are given by
n = 1, 2,..., l = 0, 1,...,n − 1, m = −l, −l + 1,..., 0, 1, 2,...,l − 1,l. (20.30)
The position probability density for the electron is given by Pnlm = |ψnlm|
2. The first few508 Monte Carlo Technique
quantum energy eigenfunctions are given below where the Bohr radius a0 and Z are set to
unity:
nlm = 100 : ψ100 = 1
√π e−r .
nlm = 200 : ψ200 = 1
√32π (2 − r) e−r/2 .
nlm = 210 : ψ210 = 1
√32π z e−r/2 .
nlm = 211 : ψ211 = − 1
8
√π (x + iy) e−r/2 .
nlm = 300 : ψ300 = 1
9
√3π

27 − 18r + 2r2 e−r/3 .
nlm = 310 : ψ310 = 1
162√2π (6 − r) z e−r/3 .
nlm = 311 : ψ311 = − 1
81√π (6 − r)(x + iy) e−r/3 .
In the above eigenfunctions x, y, z are Cartesian coordinates. They can be replaced in terms
of r, θ, φ using spherical polar coordinates transformation. Here, r = x2 + y2 + z2.
20.6.2 Electronic Distribution in Various States
Consider the ground state eigenfunction ψ100. The probability distribution of electron in
this state is
P100 = |ψ100|
2 = 1
π e−2r . (20.31)
For the present discussion, P is chosen as e−2r and the factor 1/π is dropped. The mul￾tiplicative factors in other Pnlm’s are also dropped. Now, generate points (x, y, z) obeying
the distribution P100. The plot of the generated points in the x − y − z plane represents
the electronic distribution in the ground state nlm = 100. It is convenient to choose x − z
plane with y = 0. A sequence of pair of points x, z with the distribution P100 = e−2r can
be generated by the rejection technique described in Subsection 19.6.2. This is summarized
below:
(1) Set y = 0 and let x and z are a set of uniformly distributed random numbers in
the interval [a, b].
(2) Define a bounding function cg(x, z) such that the maximum of cg(x, z) is greater
than or equal to the maximum of P100(r). For P100 = e−2r, the maximum of
cg(x, z) denoted as cgm can be chosen as 1.
(3) Treat cg(x, z) as a sequence of random numbers in the interval [0, cgm].
(4) Select a number for each of the variables x, z and cg(x, z).
(5) Compute P100. If cg(x, z) < P100 then accept the pair (x, z). Otherwise reject it.
(6) Repeat Steps (4) and (5) and obtain large number, say, N = 2000 pair of points
(x, z). This sequence obeys the distribution P100.
The above procedure can also be used to obtain other distributions Pnlm. For each distri￾bution function the values of a, b, cgm have to be chosen properly. For the ground state the
values of a, b and cgm are set to −10, 10 and 1, respectively. Figure 20.5 displays the elec￾tronic distribution of hydrogen atom in its several states. The distributions correspondingElectronic Distribution of Hydrogen Atom 509
nlm = 100
x
z
-10 0 10
10
0
-10
nlm = 200
x
z
-20 0 20
20
0
-20
nlm = 210
x
z
-20 0 20
20
0
-20
nlm = 211
x
z
-20 0 20
20
0
-20
nlm = 300
x
z
-35 0 35
35
0
-35
nlm = 310
x
z
-35 0 35
35
0
-35
nlm = 311
x
z
-35 0 35
35
0
-35
FIGURE 20.5
The electronic distribution of the hydrogen atom in some of its states produced by 2000
points in x − z plane with y = 0.510 Monte Carlo Technique
to nlm = 100, 200, 300 exhibits spherical symmetry around their centre. The distributions
for nlm = 210, 211, 310, 311 show rotational symmetry around the z−axis. The influence
of applied uniform electric field along z−axis on the probability distribution called Stark
effect [8-10] can also be visualized by the Monte Carlo method.
20.7 Radioactive Decay
Another physical process that can be simulated by the Monte Carlo method is the radioac￾tive decay. This process is a purely random process.
A. Basic Idea
Let us start with a sample containing N0 unstable nuclei at t = 0 and assume that the
nuclei decay with a rate λ per second. The probability with which a nucleus decay in time
dt is p. That is, p = λdt and λdt  1. In nuclear physics, the rate of decay is governed by
the differential equation
dN
dt = −λN . (20.32)
Its solution is
N(t) = N0e−λt, (20.33)
where N(t) is the number of undecayed nuclei at time t.
At time t + dt the number of undecayed nuclei is
N(t + dt) = N0e−λ(t+dt)
. (20.34)
Then, the probability for a nucleus to decay in time dt, P, is
P = Number of nuclei decayed in time dt
Number of nuclei at time t
= N(t) − N(t + dt)
N(t)
= 1 − N(t + dt)
N(t)
= 1 − N0e−λ(t+dt)
N0e−λt
= 1 − e−λdt (20.35)
P = λdt for λdt  1. That is, the probability for a nucleus to decay in time dt is P =
λdt. The time varies uniformly. P = λdt varies uniformly with the time dt. As the total
probability of an event is 1, the probability for a nucleus to decay in time dt is ∈ [0, 1].
Therefore, to find whether a nucleus is decayed in time dt one can generate a uniform
random number in the interval [0, 1]. If the number is < P = λdt then assume that a
nucleus has decayed in time dt. If there are N nuclei then one can generate N uniform
random numbers in the interval [0, 1]. If N numbers are < λdt then N nuclei are decayed.
Why are uniform random numbers used in this simulation process?
B. Simulation Process
In the Monte Carlo approach, time t is divided into number of subintervals with an
increment, say, dt. The steps involved in the Monte Carlo simulation are given below.Radioactive Decay 511
λ = 0.5
λ = 0.25
t
N(t)
109876543210
100000
80000
60000
40000
20000
0
FIGURE 20.6
The number of undecayed nuclei N(t) versus t (in sec) for N(0) = N0 = 105, λ = 0.25 sec−1
and λ = 0.5 sec−1. The solid circles are the Monte Carlo simulation results while the con￾tinuous curves are given Eq. (20.33).
(1) Set, for example, t = 0 sec, N(t = 0) = N0 = 105, λ = 0.25 sec−1, dt = 0.01 sec
and tmax = 10 sec.
(2) Increment the time from t to t + dt.
(3) Set N(t) = N(t − dt).
(4) Generate a uniform random number r in the interval [0, 1]. If r<λdt then
decrement N(t) by 1, that is, N(t) → N(t) − 1.
(5) Repeat the Step (4) for N(t−dt) random numbers. The final N(t) is the number
of undecayed nuclei at time tsec.
(6) Repeating the Steps (2)−(5) one can compute N(t) for successive discrete values
of t.
Figure 20.6 shows N(t) as a function of t (in sec) for λ = 0.25 sec−1 and λ = 0.5 sec−1.
The time step size used is dt = 0.01. For each value of λ continuous curve represents
theoretically computed N(t) (Eq. (20.33)) while solid circles are the Monte Carlo simulation
predictions.
C. A Simple Exercise
Applying the Monte Carlo method compute the number of undecayed nuclei at time t = 1
and 2 units given that the number of undecayed nuclei at time t = 0 units is 10 and the
decay constant λ = 0.5 units. Use the uniform random numbers generated in a calculator.
At t = 0 units, N(0) = 10, λ = 0.5 units. To find N(0 + dt) generate 10 (N(0)) uniform
random numbers in the interval [0, 1]. If a number is < λdt = 0.5 × 1=0.5 then a nucleus
is decayed so decrease the number of undecayed nuclei by 1 from 10. Repeat this process
for the other random numbers also. The final number is the number of undecayed nuclei at
time = 1 unit. Table 20.7 is obtained at time t = 1 unit. From this table N(1) by the Monte
Carlo simulation is 5 while by the theory is 6. Proceed to compute N(2) at t = 2 units with
N(1) = 5 and by considering 5 random numbers. Table 20.8 is obtained at time t = 2 units.
N(2) by the Monte Carlo simulation is 4 while by the theory is 3.512 Monte Carlo Technique
TABLE 20.7
Number of undecayed nuclei at t = 1 units with N(0) = 10 and λ = 0.5 units.
Trial random λdt N(1) Trial random λdt N(1)
number number
1 0.561 0.5 10 6 0.441 0.5 8
2 0.796 10 7 0.355 7
3 0.573 10 8 0.262 6
4 0.497 9 9 0.291 5
5 0.668 9 10 0.631 5
TABLE 20.8
Number of undecayed nuclei at t = 2 units with N(0) = 10, N(1) = 5 and λ = 0.5 units.
Trial random λdt N(2) Trial random λdt N(2)
number number
1 0.593 0.5 5 4 0.376 0.5 4
2 0.845 5 5 0.699 4
3 0.592 5
20.8 Diffusion of Neutrons by a Moderating Material Slab
This section presents the study of diffusion of neutrons through a slab of a moderating
material by the Monte Carlo method [11]. Assume that slowing down of neutrons is due to
elastic collisions only and the nucleus is at rest in the laboratory frame. In the centre-of￾mass (CoM) system, a neutron with an energy E0, after scattering will have an energy E
and deflected by an angle θ. The ratio E/E0 is given by [11]
E
E0
=

1+2A cos θ + A2
(1 + A)
2 , (20.36)
where A is the mass number of the nucleus. The scattering angle θ in the CoM system and
the θL in the laboratory system are connected through the relation
cos θL = 1 + A cos θ
(1 + 2A cos θ + A2)
1/2 . (20.37)
The total mean free path is
1
Λ = 1
λc
+
1
λa
, (20.38)
where λc and λa are average elastic scattering free path and absorption free path, respec￾tively.
In the Monte Carlo simulation consider that a flux of neutrons with energy, say, E0 is
allowed to incident normally onto a homogeneous infinite plate of width d. After a numberPercolation 513
of scattering events the neutron may cross the plate or captured inside the plate or re￾flected. Denote PT, PA and PR as the transmission, absorption and reflection probabilities,
respectively. They are defined as
PT = NT
N , PA = NA
N , PR = NR
N , (20.39)
where NT, NA, NR and N are the number of neutrons crossing the plate, number of absorbed
neutrons, number of reflected neutrons and total number of neutrons, respectively. NT, NA
and NR can be computed using the Monte Carlo method.
The free path l, the diffusion angle φ and the scattering angle θ for each neutron can be
determined stochastically. If γ is a uniform random variable between 0 and 1 then
l = −Λ ln γ , (20.40a)
φ = 2πγ , (20.40b)
cos θ = 2γ − 1 . (20.40c)
The neutron motion can be simulated. Denote j as the trajectory number and k as the
collision number. For each neutron fix θL = 0 and assign a value for the initial coordinate
x0. Determine stochastically lk between two collisions and θk according to Eqs. (20.40). θk
is then expressed in terms of θL using Eq. (20.37). The new coordinate xk+1 is given by
xk+1 = xk + lk cos θL . (20.41)
After every collision calculate the position of the neutron and check the following possibili￾ties:
1. xk+1 > d: Terminate the calculation of the trajectory for the neutron and update
NT as NT = NT + 1.
2. xk+1 < 0: Terminate the calculation of the trajectory and update NR as NR =
NR + 1.
3. 0 ≤ xk+1 ≤ d: In this case in order to find whether the absorption of neutron
will take place or further elastic scattering will occur, generate another random
number, say, γ. If γ < Λ/λa then absorption of the neutron is happened and so
update NA as NA = NA + 1. Otherwise, the neutron is assumed to be scattered.
Then, use Eq. (20.40c) to compute another θ and then θL from Eq. (20.37) and l from
Eq. (20.40a). Repeat the above process number of times.
Now, present the simulation result for water. For water in the ref. [12] the values of
λc and λa are available as 1.1 cm and 170 cm, respectively. Further, A = 18.015 g/mol,
N = 105 and E0 = 1 MeV. Figure 20.7 presents the numerically computed PA, PR and PT
as a function of thickness d (in cm). PT decreases nonlinearly with d. PT ≈ 0 for sufficiently
large d. PR exhibits sigmoid type variation.
20.9 Percolation
Percolation is a phenomenon where a system exhibits a sudden change in its behaviour
when a parameter of the system attains a threshold value. This provides a model for
phase transitions and first-passage problems occurring in different areas of basic sciences
and engineering. The name originates from the mathematical similarity of such problems
with passage of coffee through a percolator.514 Monte Carlo Technique
PA
PR
PT
d (cm)
PR, PT, PA
76543210
1
0.8
0.6
0.4
0.2
0
FIGURE 20.7
PA, PR and PT of neutrons computed through the Monte Carlo simulation as a function of
d for water.
20.9.1 Computer Simulation
Consider a two-dimensional square lattice. Each lattice site is termed as participating or
nonparticipating with a probability p or 1−p, respectively. Algorithmically, one may set up
a two-dimensional array in a, say, C++ or Python program. Initially, all elements are set to
zero. Fix a value for p. The program now visits all the sites going successively through all
rows (or columns) and assigns a uniformly distributed random number R ∈ [0, 1]. If R ≤ p
then the site is set to 1. After having visited all sites, one realization or configuration is
generated. Since, only lattice sites are involved, this is called a site percolation model. If two
nearest (row- as well as column-wise) sites have the value 1, that is, they are participating,
connect them by a line (bond). This is done for the entire lattice. This is called a bond
percolation where adjacent sites are connected by bonds. A collection of participating sites
connected by nearest neighbour distance is defined as a cluster . That is, in general, all sites
rj which are connected to ri form a cluster containing ri.
The meaning and significance of the above can be understood by considering the follow￾ing examples.
1. Consider a lattice made up of a nonconducting material. Certain sites are replaced
by a conducting material like graphite. Then, the nearest conducting sites are
connected or made into contact.
2. Another simple configuration can be viewed by replacing participating sites by
magnetic atoms and nonparticipating sites by nonmagnetic atoms.
3. Consider a fluid trying to pass through a porous medium, for example, a rock.
The medium may have a number of small but random channels. It is desired
to know whether the fluid can flow through the medium. Fluid flow obviously
depends on the concentration and nature of the channels. When there are more
channels the fluid can pass through it. The particular concentration or situation
at which fluid flow becomes possible is called percolation threshold.
Figure 20.8a shows a configuration for p = 0.4. ‘+’ symbol represents participating sites
and dots represent nonparticipating sites. The nearest neighbour participating sites are
connected by lines denoting clusters. In Fig. 20.8a, one can clearly observe the following:Percolation 515
(a) p = 0.4 (b) p = 0.55
(c) p = 0.75
FIGURE 20.8
Percolation configuration for three values of p. The + symbol and the dots represent the par￾ticipating and the nonparticipating sites, respectively. The nearest neighbour participating
sites are connected by lines.
1. Some sites are participating.
2. Some sites are nonparticipating.
3. Some participating sites form a cluster.
4. Certain participating sites are isolated from the nearest neighbours.
Let us ask: Will a person starting from a site on the first column reach the last col￾umn continuously visiting participating sites without crossing the nonparticipating sites? In
Fig. 20.8a such a reach is not possible. This is because in this figure, even though there are
many clusters but all are scattered and disconnected. There is no path from first column
to last column. In a physical experiment the sites can be replaced by conducting materials
and in this case, one may ask whether conduction from one end to another occurs so that
a deflection in the ohmmeter is observed. Deflection does not occur for the configuration
shown in Fig. 20.8a.516 Monte Carlo Technique
20.9.2 Effect of p
What does happen if one increase p? As p increases, larger clusters will appear with small
ones merging with one another. Figure 20.8b depicts a configuration obtained for p = 0.55.
An interesting observation is that there exists a percolation, that is first and last columns
are connected. As p is further increased one can find more number of percolation. Figure
20.8c depicts a configuration for p = 0.75 where one can clearly see the effect of p.
Further, if p is very small, the clusters are small. The probability c(ri, rj ) that two sites
are in a same cluster, decays like e−α|ri−rj | as |ri − rj |→∞. Denote pc as the threshold
value at which first time a percolation occurs in all the realizations. From a detailed study
the following results are obtained:
1. The average size of the clusters increases with p. There are many small ones and
a few large ones.
2. As p → pc from a small value the average cluster size diverges. The mean cluster
size scales (pc − p)−ν as p → pc.
3. There is a broad distribution of cluster size at p = pc.
4. For p>pc there is an infinite cluster. The change of the lattice site from finite
clusters to an infinite cluster is the geometric analogue of a phase transition such
as when water changes from liquid to gas at a specific temperature and pressure.
5. Consider the root-mean-square distance ξ between pairs of sites belonging to the
same finite clusters. ξ is also called mean connectedness length. ξ is maximum at
pc and decays exponentially above and below it.
For more details about percolation the reader may read the refs. [13-19] and also see the
Problems 13–15 at the end of the present chapter.
20.10 Concluding Remarks
This last chapter introduced the Monte Carlo technique. The basic idea of this technique
is illustrated by considering a few simple mathematical problems. Then, the method is
applied to certain physical problems. The Monte Carlo methods play an important role in
computational science and engineering [20]. An interesting development during the past few
years is the use of Monte Carlo techniques to evaluate path integrals associated with field
theories, variational and diffusion Monte Carlo.
Monte Carlo methods have been employed to analyze security pricing [21], materials
studio applications for molecular simulation [22,23], radiological sciences [24], project man￾agement [25], medical physics [26], atmospheric optics [27], social sciences [28], finance en￾gineering [29], semiconductor device simulation [30] and simulations of x-ray image devices
[31]. For recent developments of the Monte Carlo methods refer to the refs. [32-34].
20.11 Bibliography
[1] P.C. Mahalanobis, Dialectia 8:95, 1954.
[2] C. Radhakrishna Rao, Statistics and Truth: Putting Chance To Work. World
Scientific, Singapore, 1997.Bibliography 517
[3] L. Kelvin, Phil. Mag. 2:1, 1901.
[4] T.A. Joy Woller, The Basics of Monte Carlo Simulations. Springer, Berlin, 1996.
[5] Eli Maor, e: The Story of a Number . University Press, Hyderabad, 1994.
[6] P. Mohazzabi, Am. J. Phys. 66:137, 1998.
[7] V.M. de Aquino, V.C. Aguilera-Navano, M. Goto and H. Itwamoto, Am. J. Phys.
69:788, 2001.
[8] J.L. Powell and B. Crasemann, Quantum Mechanics. Oxford and IBH Publi.
Co., New Delhi, 1961.
[9] P.M. Mathews and K. Venkatesan, Quantum Mechanics. Tata McGraw–Hill, New
Delhi, 1976.
[10] S. Rajasekar and R. Velusamy, Quantum Mechanics I: The Fundamentals. CRC
Press, Boca Raton, 2022.
[11] M.C. Capizzo, S. Nuzzo, R.M. Sperandeo Mineo and M. Zarcone, Eur. J. Phys.
26:85, 2005.
[12] E. Segre, Nuclei and Particles. Benjamin, New York, 1977.
[13] C. Vijayan and A. Arulgnanam, Phys. Edu. October−December, 1991. pp.229.
[14] D. Stauffer, Introduction to Percolation Theory. Taylor and Francis, London,
1985.
[15] G. Grimmet, Percolation. Springer, Berlin, 1999.
[16] G.A. Schwartz and S.J. Luduena, Am. J. Phys. 72:364, 2004.
[17] W. Jian Sheng, C. Kan and Z. Fei, Computational Techniques in Theoretical
Physics. Lecture Notes, 1998.
[18] M. Barma and R. Ramaswamy, J. Phys. A. Math. Gen. 19:L605, 1986.
[19] M.A. Dubson and J.C. Garland, Phys. Rev. B 32:7621, 1985.
[20] J.G. Amar, Comput. Sci. & Eng. March-April 2006. pp.9-19.
[21] P. Boyle, B. Broadie and P. Glasserman, J. Econ. Dyn. Control 21:1267, 1997.
[22] R.L.C. Akkermans, N.A. Spenley and S.H. Robertson, Molec. Simul. 39:1153,
2013.
[23] A. Rahbari, R. Hens, M. Ramdin, O.A. Moultos, D. Dubbeldam and T.J.H. Vlugt,
Mol. Simul. 47:804, 2021.
[24] R.L. Morin, Monte Carlo Simulation in the Radiological Sciences. CRC Press,
Boca Raton, 1988. Reissued in 2019.
[25] Y.H. Kwak and L. Ingall, Risk Manag. 9:44, 2007.
[26] P. Andreo, Phys. Med. Biol. 36:861, 1991.
[27] G.I. Marchuk, G.A. Mikhailov, M.A. Nazaraliev, R.A. Darbinjan, B.A. Kargin
and B.S. Elepov, The Monte Carlo Methods in Atmospheric Optics. Springer,
Berlin, 1980.
[28] T.M. Carsey and J.J. Harden, Monte Carlo Simulation and Resampling Methods
for Social Sciences. Sage, Los Angeles, 2014.
[29] P. Glasserman, Monte Carlo Methods in Financial Engineering. Springer, Berlin,
2003.518 Monte Carlo Technique
[30] C. Jacobon and P. Lugli, The Monte Carlo Methods for Semiconductor Device
Simulation. Springer, New York, 1989.
[31] H. Fuchs, L. Zimmermann, N. Reisz, M. Zeilinger, A. Ableitinger, D. Greorg and
P. Kuess, Z. Med. Phys. S0939-3889(22):00061-7, 2022.
[32] Special issue on Monte Carlo Simulation of Soft Matter Systems, Front. Phys. 9,
2021.
[33] A.A. Jaoude, The Monte Carlo Methods: Recent Advances, New Perspectives and
Applications. IntechOpen, London, 2022.
[34] A. Senova, A. Tobisova and R. Rozenberg, Sustainability 15:1006, 2023.
20.12 Problems
20.1 Applying a Monte Carlo method compute the area under the following curves:
a) f(x)=e−x2
, −5 ≤ x ≤ 5.
b) f(x)=e−x, 0 ≤ x ≤ 1.
c) f(x)=ex, 0 ≤ x ≤ 1.
d) f(x) = sin2 x, 0 ≤ x ≤ 2π.
e) f(x) = xe−x, 0 ≤ x ≤ 10.
20.2 The logarithm of a number p > 0 is not known. Employing a Monte Carlo method
ln p can be determined. For example, consider the integral
 1
0
 1
1 − x − p xp−1
1 − xp

dx .
Its value is ln p. Evaluate this integral by the Monte Carlo method and make an
estimate of ln p for p = 2, 3,..., 20.
20.3 Compute the value of π by evaluating the integral in the identity
4
 1
0
1
1 + x2 dx = π
by the Monte Carlo method.
20.4 Evaluate the integral in the relation
 ln 2
0
x
1 − e−x dx = π2
12
by the Monte Carlo method and compute the value of π.
20.5 The error function is defined as
erf(x) = 2
√π
 x
0
e−u2
du.
Compute erf(1) and erf(2) by evaluating the integral by the Monte Carlo method.Problems 519
20.6 Apply the Monte Carlo method to calculate the value of the integral
 ∞
0
x2e−x2
dx
(exact value is √π/4).
20.7 Prepare a comparative study of the dart and the derangement methods. Out of
these two methods which is efficient? Why?
20.8 Write a Python program to compute all n roots of a negative number.
20.9 Write a Python program to compute all n roots of a positive number.
20.10 Obtain the image representation of an electronic distribution in x − z plane with
y = 0 of hydrogen atom in the states
(a) (ψ200 + ψ210) /
√2 and (b) (ψ200 − ψ210) /
√2 .
20.11 A three-dimensional harmonic oscillator potential is given by
V (r) = 1
2
r2 = 1
2

x2 + y2 + z2
.
In spherical polar coordinates its eigenfunctions are given by
ψnlm = Ne−r2/2 rl Ll+1/2
k

r2
Y m
l (θ, φ).
The ψnlm for a few values of n, l, m are
ψ000 ∝ e−r2/2, ψ110 ∝ ze−r2/2, ψ200 ∝
3
2 − r2

e−r2/2,
ψ220 ∝ 
3z2 − r2
e−r2/2, ψ310 ∝ z
5
2 − r2

e−r2/2,
ψ400 ∝
15
8 − 5
2
r2 +
1
2
r4

e−r2/2 .
Obtain the probability distribution of a particle in the x − z plane with y = 0
for the states represented by the above functions. Write a short note on your
observation.
20.12 One of the oldest documented applications of Monte Carlo technique suggested
by Comte de Buffon in 1777 [George Louis Leclere Comte de Buffon, Essai
d’arithmatique morale, in Supplement ´a l’Histoire Naturella (de L’Imprimerie
Royale, Paris 1777) Vol.4] was Buffon’s needle. In this problem, a needle of
length l is thrown randomly onto a horizontal plane ruled with straight parallel
lines a distance d (d>l) apart. Buffon shown that the probability p of a needle
intersecting a line is given by
p =
 π
0
 l sin θ
0
1
dπ dAdθ = 2l
dπ .
Laplace [Marquis Pierre-Simon de Laplace, Theorie Analytique des Probabilities
in Oeuvres completes de Laplace (de L’Academie des Sciences, Paris, 1886), Vol.7,
part 2, pp.365-366] suggested that this technique could be used to evaluate π:
π = 2l/(dpn), where pn is the numerically computed probability for a needle to
intersect a line. Fix the values of l and d as 0.5 unit and 1 unit, respectively.520 Monte Carlo Technique
Consider a two-dimensional plane consisting of 105 lines drawn parallel to y-axis
with x−values 1, 2,..., 105. Write a program to compute pn taking the number of
throws as N = 212. Then, using the formula compute π. Repeat the calculation
for N = 213, 214,..., 224.
[Hint: Generate a random number x1 between 0 to 105. Treat this number x1 as
one end of the needle. The other end of the needle must be on the circumference
of a circle of radius l with the centre whose x value is simply x1. Now, generate
a number for the angle θ between 0 to 360 (or 0 to 2π). Then, the x−component
of the other end of the needle is x2 = x1 + l cos θ. ]
20.13 For a percolation problem with 20 × 20 lattice counts, the number of
a) nonparticipating sites,
b) participating sites but not a member of a cluster and
c) isolated clusters
as a function of p, for p ∈ [0, 1] with ∆p = 0.05. Describe the variation of these
quantities with p.
20.14 Consider a 20 × 20 lattice. Increase the value of p from 0 to 1 in steps of 0.01
and find the threshold value pc at which first time a percolation occurs. Then, for
p ≥ pc, each value of p count the number of percolation Np. Draw a graph between
p versus Np. Describe the trend in the figure. (The percolation configuration is
sensitive to the sequence of random numbers. If you got a percolation for chosen p
then repeat the experiment for several, say M, times for different random number
sequences. If you got at least one percolation in each trial then call the p as pc.
Otherwise do the above for p + ∆p.).
20.15 The threshold value pc is sensitive to the size n × n of the lattice. However,
as the value of n increases pc approach a constant value. Compute pc for n =
20, 40,..., 200 and sketch a graph between n versus pc. How does it approach a
constant value? What is that constant value?Answers to Some Selected Problems
Chapter 1 Preliminaries
1.1 (382.382)10 = 3 × 102 + 8 × 101 + 2 × 100 + 3 × 10−1 + 8 × 10−2 + 2 × 10−3.
1.2 11101.11101 = 29.90625.
1.3 1.9=1.1110011001 ... and is a never ending sequence.
1.4 (11010.11)2 = (26.75)10 = (32.6)8 = (1A.C)16.
1.5 e0.1
Taylor = 1.1 and the percentage of relative error is 0.47. e0.9
Taylor = 1.9 and the
percentage of relative error is 22.75.
1.6 f(0.8) = 2.049180328, f(0.80005) = 2.04958355, g(f(0.8)) = 6.248320476,
g(f(0.80005)) = 6.250376278. The error in f of the order of 10−4 gives an er￾ror in g of the order of 10−2. That is, the error propagates.
1.7 f(0.1) = 100, f(0.10005) = 99.90007 and |f(0.10005) − f(0.1)| = 0.09993. That
is, an approximation of the order of 10−5 in the value of x causes a change of
the order of 10−2 in the function. Here the divisor is a small number. Next,
f(10) = 0.01, f(10.00005) = 0.99999 × 10−2 and |f(10.00005) − f(10)| = 10−7.
When the divisor is a large number an approximation of the order of 10−5 in the
value of x causes an error of the order of 10−7 in the value of the function.
1.8 For a = 3, xn → x∗ = 0.66666 .... For a = 3.3 after few iterations {xn} becomes
a period-2 sequence. For a = 3.5 after a few iterations {xn} becomes a period-4
sequence. For a = 3.9 the sequence is nonperiodic.
Chapter 2 Solutions of Polynomial and Reciprocal Equations
2.2 (a) x = 0, 2, 3.
(b) x = 2, ±i.
(c) x = 1.27816, −1.13908 ± i1.61690.
2.4 α = 1.
2.5 x = −2/9, 2/3, −2.
2.8 (a) x = 1, (−3 ± √5 )/2.
(b) x = −2, −1/2, −1, −1.
(c) x = −1, (1 ± i
√3 )/2, (−3 ± √5 )/2.
2.9 x = −3, −2, −1, 1, 2.
Chapter 3 Solution of General Nonlinear Equations
3.1 (a) N = 4, x∗ = −0.00313.
(b) N = 3, x∗ = 0.56875.
3.3 It does not cross the x-axis.
521522 Answers to Some Selected Problems
3.4 (a) After 4 iterations x∗ = 0.
(b) After 3 iterations x∗ = 0.56714.
3.8 After 4 iterations the solution is √11 = 3.31662.
3.9 After 4 iterations (50)1/3 = 3.68403.
3.10 Consider the equation πx−1 = 0. With x0 = 0.2 after 2 iterations 1/π = 0.31831.
3.11 Denote the length of AC as x and the length of CB as 1 − x. Then AB/AC =
AC/CB is x2 + x − 1 = 0. Its positive root is 0.61803.
3.12 After 4 iterations t = 3.46574 sec.
3.13 After 4 iterations x = 1.73937.
3.14 With x0 = 0 units a) after 4 iterations x = −0.23873 units and b) after 4 iterations
x = −0.15878 units.
3.15 E = −0.52507 units.
3.16 After 4 iterations the result is 1.11010.
3.17 After 2 iterations VT = 1.44269 V.
3.18 After 4 iterations R = 1 Ω.
3.19 Define f = x5/(ex − 1) then the problem is to find the value of x at which
df /dx = 0. The exact value of x is 4.965.
3.20 No. Because f = 0 when x0 = 0.
3.22 xn+1 = (−x3
n + x2
n)/(−x2
n + 2xn + exn ).
3.23 T = 2.6180345 sec.
3.24 (a) After 4 iterations the roots are x∗ = (0.0 ± i 1.57080).
(b) After 4 iterations the roots are x∗ = (0.5 ± i 0.86603).
3.26 (a) After 4 iterations the root is (x∗, y∗) = (1, −2).
(b) After 1 iteration the root is (x∗, y∗) = (1, 3).
Chapter 4 Solution of Linear Systems AX = B
4.1 a) (x1, x2) = (1, −2).
b) (x1, x2) = (2, 3).
4.3 (x1, x2, x3) = (2, 3, −1).
4.4 (x1, x2, x3) = (2, 1, −3).
4.5 x = 1, y = 1.
4.6 a) (x1, x2, x3) = (1, 0, 1).
b) (x1, x2, x3) = (1, −3, −4).
4.9 a)


0.875 −0.6875 0.0625
−0.125 0.3125 0.0625
0.375 0.0625 −0.1875

 .
b)


−0.30769 0.03846 0.19231
0.53846 −0.19231 0.03846
0.46154 0.19231 −0.03846

 .
4.11 X = (4, 0, 11).Answers to Some Selected Problems 523
4.16 a) Exact solution is (x1, x2) = (1, −2).
Solution by Jacobi method:
Using computer – After 9 iterations (x1, x2) = (1, −2).
By hand calculation – After 2 iterations (x1, x2) = (1.03, −2.06).
Solution by Gauss–Seidel method:
Using computer – After 5 iterations (x1, x2) = (1, −2).
By hand calculation – After 2 iterations (x1, x2) = (1.01, −2.0).
b) Exact solution is (x1, x2, x3) = (1, 2, −3).
Solution by Jacobi method:
Using computer – After 10 iterations (x1, x2, x3) = (1, 2, −3).
By hand calculation – After 2 iterations (x1, x2, x3) = (1.06, 2.03, −3.05).
Solution by Gauss–Seidel method:
Using computer – After 7 iterations (x1, x2, x3) = (1, 2, −3).
By hand calculation – After 2 iterations (x1, x2, x3) = (1.02, 1.97, −3).
4.17 a) (x1, x2, x3) = (0, 1, 2).
b) (x1, x2, x3) = (2, 1, −1).
4.18 a) (x1, x2, x3) = α(1, 1, −1).
b) (x1, x2, x3) = α(3, 1, −2).
4.19 a) (x1, x2)=(−1, 2).
b) (x1, x2)=(−2, 2).
4.20 a) (x1, x2, x3)=(−0.5, 1, 1).
b) (x1, x2, x3) = (1, 1, −2).
Chapter 5 Curve Fitting
5.1 a = 0.5 V−1, b = 1.
5.2 V0 = 2m3, a = 0.0006 C−1.
5.3 a = −3.25 J/kg, b = 1052 J/(kg.C).
5.4 R = 100 ohms.
5.5 a = 0.00255 KJ/mol.K2
, b = 32.2307 KJ/mol.K.
5.6 N0 = 1000, λ = 0.75072/m.
5.7 a = 1.99884/eV, b = 0.49959.
5.8 q0 = 1 × 10−4 coulomb, τ = 0.24711 /sec.
5.9 N0 = 800, µ = −25.03827/m.
5.10 N0 = 9990, λ = 0.00995/hour.
5.11 a = 2.55779, b = −0.95522.
5.12 a = 0.52995, b = 1.92825.
5.13 a = −0.496 units, b = 9.81659 units.
5.14 a = −1.50127 units, b = 0.0641 units.
5.15 a = −1.75851, b = 0.43337.
5.16 a = 0.4999, b = 0.7.524 Answers to Some Selected Problems
5.17 K = 0.80084 volt, a = 0.50749 m.
5.18 a = 2.99865, b = 0.50285.
5.19 a = −1.38279, b = 1.60641.
5.20 a = 1.12564, b = 0.47436.
5.21 a = 0.8, b = 1.175.
5.22 a = −0.09965, b = 1.04421.
5.23 a = 3.00564, b = −1.99818.
5.24 a = 2, b = 0.5.
5.25 a = 3, b = 0.25.
5.26 a = −1, b = 3.
5.27 a = −0.5, b = 2.
5.28 a = 0.51105 MeV, b = 1.
5.29 a = −1, b = 1.
5.30 (a) A = 90 units, B = 0.5 units, C = 0.003 units.
5.31 (a) C1 = 5.30057 units, C2 = −0.00011 units, C3 = 0.00002 units.
5.32 C1 = 0.49851 units, C2 = −0.00217 units, C3 = 0.00002 units.
5.33 A = 3335.79992 units, B = 1.022 units.
5.34 C1 = 0, C2 = −7, C3 = 9, C4 = 4.
5.35 (a) y = 0.5 sin x.
(b) y = 1.0+0.5 sin x − 0.3 cos x.
Chapter 6 Interpolation and Extrapolation
6.1 a) f(−1.5) = 0.25162, |E1| = 0.04599.
b) f(2.5) = 0.29167, |E1| = 0.00926.
6.2 a) a1 = 1, a2 = 0.63212, a3 = 0.21332, a4 = 04450, P(−1.5) = 0.22015,
P(−3.1) = 0.03803.
b) a1 = 0, a2 = 0.89256, a3 = −0.32648, a4 = 0.13472, P(0.3) = 0.26247,
P(−0.1) = −0.10351.
6.5 h2 ≤ (8/e1)10−5, that is, h < 0.00542.
6.7 By Newton interpolation polynomial: a1 = 0.1, a2 = 0.1, a3 = 0.0, a4 = 0.0,
P(1.65) = 0.165 ampere.
By Gregory–Newton forward-difference method: P(1.65) = 0.165 ampere.
By Gregory–Newton backward-difference method: P(1.65) = 0.165 ampere.
6.8 By Newton interpolation polynomial: a1 = 0.0, a2 = 4.4, a3 = −0.8, a4 = 0.66667,
P(0.9) = 3.648 m.
By Gregory–Newton forward-difference method: P(0.9) = 3.648 m.
By Gregory–Newton backward-difference method: P(0.9) = 3.648 m.
6.9 By Newton interpolation polynomial: a1 = 2.108, a2 = 0.00110, a3 = 0.00001,
a4 = 0.0, P(100) = 2.11781 m3.
By Gregory–Newton forward-difference method: P(100) = 2.11781 m3.
By Gregory–Newton backward-difference method: P(100) = 2.11781 m3.Answers to Some Selected Problems 525
6.10 By Newton interpolation polynomial: a1 = 0.184, a2 = −0.305, a3 = 0.2625,
a4 = −0.25, P(0.6) = 0.15013.
By Gregory–Newton forward-difference method: P(0.6) = 0.15013.
By Gregory–Newton backward-difference method: P(0.6) = 0.15013.
6.13 For 6.7: At V = 1.65V; L1 = −0.05950, L2 = 0.77350, L3 = 0.33150, L4 =
−0.04550, P(1.65) = 0.165 ampere.
For 6.8: At t = 0.9 minute; L1 = −0.03200, L2 = 0.21600, L3 = 0.86400, L4 =
−0.04800, P(0.9) = 3.648 m.
For 6.9: At T = 100oC; L1 = 0.3125, L2 = 0.9375, L3 = −0.3125, L4 = 0.06250,
P(100) = 2.11781 m3.
For 6.10: At E = 0.6; L1 = 0.3125, L2 = 0.9375, L3 = −0.3125, L4 = −0.0625,
P(0.6) = 0.15012.
For 6.11: At 12 days; L1 = −0.056, L2 = −0.288, L3 = 1.008, L4 = 0.224,
P(12) = 0.12432 mg.
Chapter 7 Eigenvalues and Eigenvectors
7.1 a) i) After 3 iterations λ1 = 12.03478, X = (0.00506, 0.08309, 1).
After 17 iterations λ1 = 10, X = (0.01, 0.1, 1).
ii) After 3 iterations λsmall = 0.6625, X = (1, 0.56698, 0.3).
After 17 iterations λsmall = 0.5, X = (1, 0.5, 0.25).
b) i) After 3 iterations λ1 = 1, X = (1, 1, 1).
After 2 iterations λ1 = 1, X = (1, 1, 1).
ii) After 2 iterations λsmall = 1, X = (1, 1, 1).
After 2 iterations λsmall = 1, X = (1, 1, 1).
7.2 a) After 3 iterations λ1 = 2.80000, X = (1, 0.57143, 0.14286).
b) After 3 iterations λ1 = 3.83333, X = (0.65217, 1, 0.82609).
7.3 After 3 iterations λ = 2 and X = (1/8, 1, 1/8). X1 = (1/2, 1, 1/2), X2 =
(1/4, 1, 1/4) and X3 = (1/8, 1, 1/8). Hence, Xn = (1/2n, 1, 1/2n) giving X =
(0, 1, 0).
7.4 a) After 1 iteration λ = −1, 3 and X =
 1/
√2 1/
√2
−1/
√2 1/
√2

.
b) After 1 iteration λ = √2 ± 1 and X =
 1/
√2 1/
√2
−1/
√2 1/
√2

.
7.5 a) After 2 iterations λ = −1, 1, 5 and
X =


1/
√2 −1/2 1/2
0 1/
√2 1/
√2
−1/
√2 −1/2 1/2

.
b) After 1 iteration λ = −1, 1, 3 and
X =


1/
√201/
√2
01 0
−1/
√201/
√2

.526 Answers to Some Selected Problems
7.8 a) λ = 1, 3.
b) λ = −2.72545, 1.60147, 4.12398.
7.9 a) λ = 1.58579, 3, 4.41421.
b) λ = 1.58579, 3, 4.41421.
7.10 a) λ = 0.25761, 1.85228, 3.96828, 7.92184.
b) λ = −1.23607, 0.76393, 3.23607, 5.23607.
Chapter 8 Numerical Differentiation
8.4. Exact value of fx(0.2, 0.2) = −0.25761457.
fx(0.2, 0.2, h = 0.05) = −0.25698847. fx(0.2, 0.2, h = 0.1) = −0.25511928.
Exact value of fy(0.2, 0.2) = −1.28807283.
fy(0.2, 0.2, h = 0.05) = −1.29022069. fy(0.2, 0.2, h = 0.1) = −1.29667718.
8.5. Exact value of fx(0, 0) = 1.0.
fx(0, 0, h = 0.05) = 1.0. fx(0, 0, h = 0.1) = 1.0.
Exact value of fy(0, 0) = 0.
fy(0, 0, h = 0.05) = 0. fy(0, 0, h = 0.1) = 0.
8.6. x = 11.28000004 m/sec, v = 0.49999936 m/sec and the exact v = 0.5 m/sec.
8.7. x = −3.72500011 and ω2
0 = 4.04451693.
8.8. x = −0.200833483 m/sec and E = 0.98056708 J.
8.9. v(1.2) = −0.37249860 and L = 0.97288956.
8.10. f = (3/4h) [f (x0 + h) − f (x0 − h)] − (3/20h) [f (x0 + 2h) − f (x0 − 2h)]
+(1/60h) (f (x0 + 3h) − f (x0 − 3h)) − (4/21)h6f(7) (x0) .
8.11. D0.5(t
2 − t) values at t = 0.5 for h = 0.01, 0.001 and 0.0001 are −0.26642,
−0.26598 and −0.26596, respectively.
8.12. At t = 0.1, 0.2, 0.3, 0.4 and 0.5 the values of D0.5t
2 are 0.04758, 0.13457, 0.24722,
0.38061 and 0.53192, respectively.
8.13. At t = 0.1, 0.2, 0.3, 0.4 and 0.5 the values of D0.5t are 0.35682, 0.50463, 0.61804,
0.71365 and 0.79788, respectively.
8.14. The values of D1.5t
2 at t = 0.1, 0.2, 0.3, 0.4 and 0.5 are 0.71365, 1.00925, 1.23608,
1.42730 and 1.59577, respectively.
Chapter 9 Numerical Minimization of Functions
9.1. a) After 3 iterations the root of f
(x) = 0 is 2.45667. f(x) is locally minimum at
this point.
b) After 3 iterations the root of f
(x) = 0 is 1.01484. f(x) is locally maximum at
this point.
9.2. f(x) has a local minimum at x = −0.70705 while a local maximum at x = 0.70705.
9.3. The given function has local minima at x = ±1.23411 and a maximum at x = 0.
9.4. After 3 iterations x∗ = 0.01949 and f > 0 at this point.Answers to Some Selected Problems 527
9.6. The 1st iteration gives x0 = 0.1, x1 = −0.025, x2 = −0.15 and x∗
1 = −0.00016.
In the 2nd iteration ∆x less than 0.001 is realized therefore x∗
1 = −0.00016 is the
point of minimum. The exact value of x∗ is 0.
9.7. The first iteration gives x0 = 0.1, x1 = −0.9, x2 = −1.9 and x∗
1 = −0.98864.
The second iteration gives x0 = −0.98864, x1 = −0.48864, x2 = 0.01136 and
x∗
2 = −0.72294. The third iteration gives x0 = −0.72294, x1 = −0.69169, x2 =
−0.66044 and x∗
2 = −0.70717. In the fourth iteration ∆x < 0.001 is realized.
Therefore, x∗
1 = −0.70717 is the point of minimum.
9.8. a) After 5 iterations (x∗, y∗) = (0, 3.14159), ∆2 = 2 and fxx = 2.
b) After 5 iterations (x∗, y∗) = (0, 0), ∆2 = 4 and fxx = 2.
9.9. For (x0, y0) = (0.8, 0.8) after 6 iterations (x∗, y∗) = (1, 1) and ∆2 = 48, fxx = 8.
(x∗, y∗) is a minimum.
For (x0, y0) = (0.8, −0.8) after 12 iterations (x∗, y∗) = (1, −1) and ∆2 = 80,
fxx = 8. (x∗, y∗) is a minimum.
For (x0, y0)=(−0.8, 0.8) after 13 iterations (x∗, y∗)=(−1, 1) and ∆2 = 80,
fxx = 12. (x∗, y∗) is a minimum.
For (x0, y0)=(−0.8, −0.8) after 15 iterations (x∗, y∗)=(−1, −1) and ∆2 = 128,
fxx = 12. (x∗, y∗) is a minimum.
For (x0, y0) = (0.2, 0.2) after 4 iterations (x∗, y∗) = (0, 0) and ∆2 = 4, fxx = −2.
(x∗, y∗) is a maximum.
For (x0, y0) = (0.0, 0.7) after 3 iterations (x∗, y∗) = (0, 0.707107) and ∆2 = −12,
fxx = −3. Therefore, f does not have a local maximum at (x∗, y∗).
9.10. a) After 17 iterations (x∗, y∗) = (0.000309, 3.141850).
b) After 18 iterations (x∗, y∗) = (0.000717, 0.000276).
Chapter 10 Numerical Integration
In the following TR, S1/3, S3/8 and GL denote trapezoidal rule, Simpson’s 1/3-rule, Simp￾son’s 3/8-rule and two-point Gauss–Legendre rule, respectively. CTR, CS1/3 and CS3/8
denote composite trapezoidal rule, composite Simpson’s 1/3-rule and composite Simpson’s
3/8-rule, respectively. Further, exact value is the value of the given integral obtained by
direct integration or by composite Simpson’s 3/8-rule with large n.
10.2 (a) Exact value of the integral = sin 1 = 0.8414710.
TR: 0.7701512. S1/3: 0.8417721. S3/8: 0.8416044. GL: 0.8412698.
CTR: 0.8336651. CS1/3: 0.8414746. CS3/8: 0.8414726.
(b) Exact value of the integral = 0.7468241.
TR: 0.6839397. S1/3: 0.7471804. S3/8: 0.7469923. GL: 0.7465947.
CTR: 0.7399865. CS1/3: 0.7468304. CS3/8: 0.7468269.
10.3 (a) Exact value of the integral = 0.9460831.
TR: 0.9207355. S1/3: 0.9432914. S3/8: 0.9461109. GL: 0.9460411.
CTR: 0.9432914. CS1/3: 0.9460838. CS3/8: 0.9460834.
(b) Exact value of the integral = 1/12 = 0.0833333.
TR: 0.0000000. S1/3: 0.1308997. S3/8: 0.0956496. GL: 0.0459383.
CTR: 0.0850218. CS1/3: 0.0828822. CS3/8: 0.0831402.528 Answers to Some Selected Problems
10.4 Exact value of the integral = 0.25. TR: 0.125. S1/3: 0.25.
10.8 a) TR: 0.1666667. S1/3: 0.0083333. S3/8: 0.0037723. GL: 0.0055555.
b) TR: 0.1666667. S1/3: 0.0. S3/8: 0.0. GL: 0.0.
10.9 a) CTR: nopt = 129. CS1/3: nopt = 5. CS3/8: nopt = 2.
b) CTR: nopt = 223. CS1/3: nopt = 0. CS3/8: nopt = 0.
10.10 The exact value of the integral is 2.1779795.
CTR: nopt = 12. Therefore, n = 13. With this value of n the value of the integral
is 2.1786000.
CS1/3: nopt = 1. Therefore, n = 2. The value of the integral is 2.1780266.
CS3/8: nopt = 0. Therefore, n = 1. The value of the integral is 2.1783107.
10.14 Exact value of the integral = −0.0900633.
TR: 0.0. S1/3: 0.0. S3/8: −0.0937500. GL: −0.1726528.
CTR: −0.0833333. CS1/3: −0.0919278. CS3/8: −0.0908453.
10.15 Exact value of the integral = 3.6275987.
TR: 3.1415927. S1/3: 3.1415927. S3/8: 3.6853298. GL: 4.1094810.
CTR: 3.6249146. CS1/3: 3.6249146. CS3/8: 3.6279342.
10.16 Exact value of the integral = 2.6220576.
TR: 2.6815171. S1/3: 2.6039055. S3/8: 2.6146593. GL: 2.6349313.
CTR: 2.6220879. CS1/3: 2.6220474. CS3/8: 2.6220538.
10.17 Exact value of the integral = 0.7357589.
TR: 2.3504024. S1/3: 0.7834675. S3/8: 0.7573709. GL: 0.7043259.
CTR: 0.9343744. CS1/3: 0.7364409. CS3/8: 0.7360627.
10.18 Exact value of the integral = 1.7627472.
TR: 1.4142136. S1/3: 1.8047379. S3/8: 1.7765783. GL: 1.7320508.
CTR: 1.7363156. CS1/3: 1.7628166. CS3/8: 1.7627755.
10.19 Exact value of the integral = 0.5204999.
TR: 0.5017904. S1/3: 0.5206015. S3/8: 0.5205447. GL: 0.5204318.
CTR: 0.5184609. CS1/3: 0.5205011. CS3/8: 0.5205004.
10.20 Exact value of the integral = 0.1666667.
TR: 0.6666667. S1/3: 0.2222222. S3/8: 0.1978850. GL: 0.1384280.
CTR: 0.2499718. CS1/3: 0.1702858. CS3/8: 0.1684600.
10.21 Exact value of the integral = 2.4221121.
TR: 2.3561945. S1/3: 2.4411629. S3/8: 2.4302092. GL: 2.4088172.
CTR: 2.4219854. CS1/3: 2.4221542. CS3/8: 2.4221279.
10.22 Exact value of the integral = 0.5 ln 5 = 0.8047190.
TR: 0.4000000. S1/3: 0.8000000. S3/8: 0.8061538. GL: 0.8108108.
CTR: 0.7610256. CS1/3: 0.8053092. CS3/8: 0.8049718.Answers to Some Selected Problems 529
10.23 Exact value of the integral = 0.0750000.
TR: 0.0000000. S1/3: 0.0750000. S3/8: 0.0750000. GL: 0.0750000.
CTR: 0.0666667. CS1/3: 0.0750000. CS3/8: 0.0750000.
10.24 Exact value of the integral = 3.1415927.
TR: 2.5132741. S1/3: 2.5132741. S3/8: 3.1657588. GL: 3.9444714.
CTR: 3.0932605. CS1/3: 3.0932605. CS3/8: 3.1476207.
10.25 Exact value of the integral = 0.2000000.
TR: 0.3000000. S1/3: 0.2000000. S3/8: 0.2000000. GL: 0.2000000.
CTR: 0.2111111. CS1/3: 0.2000000. CS3/8: 0.2000000.
10.26 a) Exact value = 0.7468241. By GL 3-point method: 0.7468146.
b) Exact value = 0.6931472. By GL 3-point method: 0.6931217.
c) Exact value = 0.9460831. By GL 3-point method: 0.9460832.
10.27 The formula is
0.347858451 [f(0.8611363116) + f(−0.8611363116)]
+0.6521451549 [f(0.3399810436) + f(−0.3399810436)]
+
1
3472875f(8)(x
).
a) Exact value = 0.6931472. By the 4-point GL method: 0.6931490.
b) Exact value = 0.4352099. By the 4-point GL method: 0.4352117.
10.28 a) Exact value = 0.7468241.
By the 3-point Lobatto method: 0.7471805.
b) Exact value = 0.6931472.
By the 3-point Lobatto method: 0.6944445. 0.9461459.
10.29 a) Exact value = 0.7468241.
By the 3-point Radan method: 0.7472275.
b) Exact value = 0.6931472.
By the 3-point Radan method: 0.6933333.
10.30 I = 0.526745.
10.32 3.97807, 3.99927 and 3.99998.
10.33 0.02777, 0.02689 and 0.02688.
10.34 1.02436, 1.00784 and 1.00248.
Chapter 11 Ordinary Differential Equations – Initial-Value
Problems
For simplicity, the units of the variables are not mentioned in the answers.
11.5 At x = 0.1 erfEu = 1.2412171, erfRK2 = 1.2406557, erfRK4 = 1.2408421.
At x = 1 erfEu = 2.0060514, erfRK2 = 1.9703879, erfRK4 = 1.9710800.
11.6 At x = 0.1 CEu = 1.1000000, CRK2 = 1.0999938, CRK4 = 1.0999974.
At x = 1 CEu = 1.8272711, CRK2 = 1.7772711, CRK4 = 1.7798945.530 Answers to Some Selected Problems
11.7 At t = 0.1 xEx = 0.1003347, xEu = 0.1000000, xRK2 = 0.0995000,
xRK4 = 0.0996679.
11.8 At t = 0.1 xEx = 1.0996680, xEu = 1.1000000, xRK2 = 1.0995000,
xRK4 = 1.0996679.
11.9 At t = 0.1 xEx = 0.9128709, xEu = 0.9000000, xRK2 = 0.9135500,
xRK4 = 0.9128709.
11.10 At t = 0.1 xEx = 0.5378994, xEu = 0.5375000, xRK2 = 0.5378606,
xRK4 = 0.5378994.
11.11 At t = 0.1 xEx = 0.9003320, xEu = 0.9000000, xRK2 = 0.9005000,
xRK4 = 0.9003321.
11.12 At t = 0.1 xEx = 0.4534796, xEu = 0.4500000, xRK2 = 0.4535125,
xRK4 = 0.4534796.
11.13 At t = 0.1 xEu = 4.7500000, xRK2 = 4.7562500, xRK4 = 4.7561471.
11.14 At t = 0.1 xEu = 0.1900000, xRK2 = 0.1852502, xRK4 = 0.1854837.
11.15 At t = 0.1 xEx = 2.7145240, xEu = 2.7000000, xRK2 = 2.7150171,
xRK4 = 2.7145236.
11.16 At t = 0.1 vEu = 1.4550000, vRK2 = 1.3616488, vRK4 = 1.3821320.
11.17 At t = 0.1 xEx = 0.5124974, xEu = 0.5125000, xRK2 = 0.5124961,
xRK4 = 0.5124974.
11.18 At t = 0.1 qEx = 0.0380650, qEu = 0.0400000, qRK2 = 0.0380000,
qRK4 = 0.0380650.
11.19 At t = 0.1 iEx = 0.0195082, iEu = 0.0200000, iRK2 = 0.0195000,
iRK4 = 0.0195082.
11.20 At t = 0.1 xEx = 4.5717684, xEu = 4.5500000, xRK2 = 4.5725000,
xRK4 = 4.5717687.
11.21 At x = 0.1 IEx = 2.7145123, IEu = 2.7000000, IRK2 = 2.7150000,
IRK4 = 2.7145125.
11.22 At t = 0.1 TEx = 57.1451225, TEu = 57.0000000, TRK2 = 57.1500000,
TRK4 = 57.1451250.
11.23 At t = 0.1 xEx = 0.2000000, xEu = 0.2000000, xRK2 = 0.2000000,
xRK4 = 0.2000000.
x
Ex = 2.0000000, x
Eu = 2.0000000, x
RK2 = 2.0000000, x
RK4 = 2.0000000.
11.24 At t = 0.1 xEx = 0.1490000, x
Ex = 1.9800000,
xEu = 0.1000000, x
Eu = 1.9800000,
xRK2 = 0.1490000, x
RK2 = 1.9800000,
xRK4 = 0.1490000, x
RK4 = 1.9800000.
11.25 At t = 0.1 N1,Ex = 93.2393820, N2,Ex = 6.7129655,
N1,Eu = 93.0000000, N2,Eu = 7.0000000,
N1,RK2 = 93.2450000, N2,RK2 = 6.7060000,
N1,RK4 = 93.2393833, N2,RK4 = 6.7129639.Answers to Some Selected Problems 531
Chapter 12 Symplectic Integrators for Hamiltonian Systems
12.1 pn+1 =

1 +
1
2

pn +
1
2
hF

qn +
1
2
hpn

.
qn+1 = qn +
1
2
hpn + hpn+1 .
12.2 a1 + a2 + a3 = 1, b1 + b2 + b3 = 1, a1b2 + (a1 + a2) b3 = 1/2,
a1b2
1 + a2 (b1 + b2)
2 + a3 = 1/3, a2
1b2 + (a1 + a2)
2 b3 = 1/3 .
(a1, a2, a3, b1, b2, b3) = (2/3, −2/3, 1, 7/24, 3/4, −1/24) .
12.4 The exact solution at t = 0.1 is q(0.1) = 0.0998334 and p(0.1) = 0.9950042.
By the explicit Euler method: q(0.1) = 0.10, p(0.1) = 1, D = 0.0049986 and
H(0.1) = 0.505.
By the symplectic Euler method (updated q is used): q(0.1) = 0.1, p(0.1) = 0.99,
D = 0.0050069 and H(0.1) = 0.49505.
By the symplectic Euler method (updated p is used): q(0.1) = 0.1, p(0.1) = 1.0,
D = 0.0049986 and H(0.1) = 0.505.
By the St¨ormer–Verlet method: q(0.1) = 0.1, p(0.1) = 0.995, D = 0.0001666 and
H(0.1) = 0.5000125.
By the Candy–Rozmus method: q(0.1) = 0.0998331, p(0.1) = 0.9950042, D =
2.84466 × 10−7 and H(0.1) = 0.5.
12.6 By the explicit Euler method: q(0.1) = 0.10, p(0.1) = 1.0 and H(0.1) = 0.5050250.
By the symplectic Euler method (updated q is used): q(0.1) = 0.10, p(0.1) =
0.98990 and H(0.1) = 0.4949760.
By the symplectic Euler method (updated p is used): q(0.1) = 0.10, p(0.1) = 1.0
and H(0.1) = 0.5050250.
By the St¨ormer–Verlet method: q(0.1) = 0.10, p(0.1) = 0.994950 and H(0.1) =
0.4999878.
By the Candy–Rozmus method: q(0.1) = 0.0998325, p(0.1) = 0.9949793 and
H(0.1) = 0.50.
12.8 By the explicit Euler method: q(0.1) = 2.0, p(0.1) = −0.0909297 and H(0.1) =
0.4202809.
By the symplectic Euler method (updated q is used): q(0.1) = 2.0, p(0.1) =
−0.0909297 and H(0.1) = 0.4202809.
By the symplectic Euler method (updated p is used): q(0.1) = 1.9909070, p(0.1) =
−0.0909297 and H(0.1) = 0.4161469.
By the St¨ormer–Verlet method: q(0.1) = 1.9954535, p(0.1) = −0.0910239 and
H(0.1) = 0.4161511.
By the Candy–Rozmus method: q(0.1) = 1.9954519, p(0.1) = −0.0909929 and
H(0.1) = 0.4161469.
12.10 q(0.1) = 0.3484456, p(0.1) = 0.4681761.
Chapter 16 Fractional Order Ordinary Differential Equations
16.1 Replace α by −α in Eq. (16.3).532 Answers to Some Selected Problems
16.5 For α = 0.5, x(0.001) = 0.03159 and x(0.002) = 0.04735. For α = 0.75, x(0.001) =
0.00561 and x(0.002) = 0.00982.
16.6 x(0.001) = 0.45007 and x(0.002) = 0.47413.
16.9 x(0.001) = −0.0112 and x(0.0002) = −0.01572.
16.10 x(0.001) = 1.03656 and x(0.002) = 1.05300.
16.11 x(0.01) = 1.00075 and x(0.02) = 1.00213.
16.12 x(0.01) = 0.74849 and x(0.02) = 0.70298.
16.14 x(0.001) = 1.0 and x(0.002) = 1.00004.
16.15 x(0.001) = 0.98216 and x(0.002) = 0.96804.Index
absolute error, 5
Adams–Bashforth–Moulton method
condition for convergence, 253
formula, 249, 250
local truncation error, 249
advection-diffusion equation
time-fractional, 418
finite-difference scheme, 419
stability condition, 419
augmented matrix, 60
autocorrelation function, 471
back-substitution, 59
backward-difference
algorithm, 380
table, 133
bifurcation diagram, 404
bisection method
comparison with other methods, 50
efficiency index, 34
formula, 32
limitations, 33
order of convergence, 34
procedure, 31
bits, 1
Boole’s rule, 214
boundary
-value problem
definition, 220, 293
finite-difference method, 295
shooting method, 294
solving time-independent
Schr¨odinger equation, 299
condition
Cauchy, 310
Dirichlet, 310
Neumann, 310
Box–Muller method, 472
Buffon’s needle, 519
Caputo
–Fabrizio operator, 379
fractional order derivative, 174
operator, 378
Cauchy
boundary condition, 310
random numbers
an algorithm, 481
probability distribution, 480
chaotic solution, 403
definition, 457
power spectrum, 457
Chebyshev method
comparison with other methods, 50
efficiency index, 49
formula, 49
order of convergence, 49
procedure, 48
chi-square quantity, 471
chopping, 4
companion matrix, 148
composite
quadrature formulas, 199
Simpson’s 1/3-rule
error, 201
formula, 200
optimum values of n and h, 204
Simpson’s 3/8-rule
error, 202
formula, 201
optimum values of n and h, 204
trapezoidal rule
error, 200
formula, 200, 209
optimum values of n and h, 204
condition number, 81
corrector, 249
Cramer’s rule, 58
cumulative probability distribution, 475
curve fitting
definition, 93
least-squares fit
basic idea, 94
coefficient of determination, 97
criterion for the best fit, 95
exponential fit, 100
533534 Index
Gaussian function fit, 108
normal equations, 95
polynomial fit, 106
power-law fit, 101
sigmoid function fit, 103
straight-line fit, 96
trigonometric polynomial fit, 111
damped wave equation, 347
time-fractional, 423
a finite-difference scheme, 423
Descartes’ rule, 13
dichotomous random numbers
a formula, 485
Fourier transform, 485
differentiation(numerical)
definition, 168
first derivative
backward-difference formula, 169
central-difference formulas, 170
forward-difference formula, 169
Richardson extrapolation, 172
second derivative
central-difference formulas, 171
third derivative
central-difference formula, 172
forward-difference formula, 172
diffusion equation
space-fractional, 427
a finite-difference scheme, 429, 431
a semi-implicit scheme, 435
stability condition, 430
time-fractional, 413
a finite-difference scheme, 414
equilibrium solution, 417
stability condition, 415
Dirichlet boundary condition, 310
discrete random variable, 463
divided-differences
definition, 127
table, 127, 128
double integration, 209
efficiency index, 9
eigenfunction(s)
definition, 299, 300
of linear harmonic oscillator, 303
eigenvalue(s)
computation by
Jacobi method, 153
power method, 150
QL method, 159
Rutishauser method, 162
definition, 148, 300
dominant, 149
of linear harmonic oscillator, 301, 302
problem, 56
properties, 149
eigenvector(s)
definition, 148
dominant, 149
normalized, 149
properties, 149
equation(s)
advection-diffusion, 418
characteristic, 148
damped wave, 320, 347, 423
diffusion, 413, 416, 427
double root, 8
Fisher, 336, 426
for MLC circuit, 455
heat, 309, 322, 323, 347, 348
Helmholtz’s, 334
Laplace, 309, 327, 329, 348
linear, 30, 56
multiple root, 8
nonlinear, 30
normal, 95
parabolic, 348
Poisson, 334
polynomial, 13
reciprocal, 12, 27
root, 8
system of linear, 56
telegraph, 423
time-dependent Schr¨odinger, 336, 337
time-independent Schr¨odinger, 299
van der Pol oscillator, 111
wave, 309, 313, 316, 346, 347, 420
equilibrium point
definition, 397
stable, 397
unstable, 397
error
absolute, 5
average, 94
deviation, 94
discretization, 5, 315
in composite Simpson’s 1/3-rule, 201
in composite Simpson’s 3/8-rule, 202
in Gauss–Legendre rule, 207
in Gaussian formula, 205Index 535
in mid-point rule, 195
in Monte Carlo evaluation of a definite
integral, 500
in Newton–Cotes Formula, 194
in rectangle rule, 195
in Simpson’s 1/3-rule, 197
in Simpson’s 3/8-rule, 198
in trapezoidal rule, 196, 200
random, 5
relative, 5, 198
root-mean-square, 94
round-off, 3, 4
standard, 504
systematic, 5
tolerance, 9
truncation, 5
Euler
–Lagrange equations, 270
box scheme, 356
method
error analysis, 224
for harmonic oscillator, 278
formula, 222
global error, 225
implicit, 257, 277
improved algorithm, 230
local truncation error, 225
predictor-corrector formula, 230
semi-implicit, 258
stability condition, 229
event, 465
exponentially distributed random numbers
distribution function, 475–477
inversion method, 475
mean, 476
rejection technique, 476
variance, 476
extrapolation
definition, 123
Richardson method, 298
extreme value theorem, 8
false position method
comparison with other methods, 50
efficiency index, 36
formula, 35
limitations, 36
order of convergence, 36
procedure, 34
Ferna´ndez–Criado algorithm, 473
FFT
advantage, 451
of a chaotic solution, 457
of a quasiperiodic orbit, 456
of square-wave, 454
of uniform random numbers, 458
procedure, 449
finite-difference formula for
damped wave equation, 320
heat equation, 322
time-dependent Schr¨odinger equation,
338
wave equation, 314
Fisher equation
time-fractional, 426
finite-difference scheme, 426
floating point arithmetic, 2
forward-difference table, 132
Fourier
coefficient
definition, 443
from FFT, 451
integral, 446
series
definition, 443
of square-wave, 445, 446, 452
transform, 449
fractional order
advection-diffusion equation, 418
damped wave equation, 423
derivative
composite trapezoidal rule, 175, 177
definition, 174, 177
diffusion equation, 413, 427
Fisher equation, 426
integral operator, 210
integration
composite trapezoidal formula, 213
modified trapezoidal rule, 212
ordinary differential equations
Adams–Bashforth–Moulton method,
392
backward-difference algorithm, 380
Euler method, 387
two-step Adams–Bashforth
agorithm, 395
telegraph equation, 423
wave equation, 420
function
differentiable, 7
periodic, 6
Sturm, 14536 Index
gamma function, 174, 210
composite trapezoidal rule, 176
Gauss elimination method
augmented matrix, 60
definition, 60
modifications
partial pivoting, 62
pivoting, 62
number of arithmetic operations, 72
pivot element, 60, 62
pivot row, 60, 62
Gauss–Jordan elimination method
inverse of a matrix, 67
procedure, 65
Gauss–Legendre rule
error, 207
formula, 207, 218
Gauss–Seidel iterative method
convergence criteria, 75
formula, 74, 76
Gaussian
formula, 205
random numbers
Box–Muller formula, 472
distribution function, 471, 472
Ferna´ndez–Criado algorithm, 473
Metropolis algorithm, 479
geometric integrator, 269
global error for
Euler method, 225
fourth-order Runge–Kutta method, 239
second-order Runge–Kutta method,
234
Gr¨affe’s root square method, 20
Gr¨unwald–Letnikov operator, 377, 414
Hamiltonian, 270
Hamming method, 251, 265
harmonic oscillator
equations of motion, 270
exact solution, 272
Hamiltonian, 270
symplectic Euler algorithms, 277, 278
heat equation, 309, 322, 323, 347, 348
Crank–Nicholson formula, 325
finite-difference formula, 322
stability condition, 323
numerical solution, 323, 326
Helmholtz’s equation, 334
Hermite equation, 295
hit and miss method, 495
homogeneous linear equation, 56
ill-conditioned
linear system of equations, 80–82
matrix, 80
implicit formula
definition, 317
for stiff equation, 257, 258
for the heat equation, 325
for the wave equation, 318
inhomogeneous linear equation, 56
initial-value problem, 220, 293
integral
definite, 192
indefinite, 192
integrand, 192
intermediate-value theorem, 7
interpolation
Chebyshev, 123
cubic spline, 139
definition, 123
Gregory–Newton, 132, 133
Hermite, 123
Lagrange, 135, 136
Legendre polynomials, 123
Newton polynomial, 124, 128
rational function approximation, 142
inversion method, 474
iteration, 8
Jacobi
iterative method
convergence criteria, 75
formula, 74, 76
method, 153
rotation, 155
Jacobian matrix, 44, 255
KdV equation, 357
multi-symplectic
conservation law, 357
scheme, 358
ZK scheme, 363
Kurtosis, 487
L´evy random numbers
algorithm, 481
probability distribution, 481
Lagrange
polynomial, 79
interpolation
error term, 135, 136Index 537
formula, 135, 136
Lagrangian, 270
Laguere’s method, 26
Laplace equation, 309, 327, 329, 348
finite-difference formula, 327
iterative formula, 333
numerical solution, 330, 332, 334
leapfrog methods, 277
least-squares problem, 84
Legendre equation, 304
linear
congruential sequence, 466
equation, 30, 56
Lobatto rule, 218
local truncation error for
Adams–Bashforth–Moulton, 249
Euler method, 225
fourth-order Runge–Kutta method, 239
second-order Runge–Kutta method,
233
lower-triangular system, 89
M¨uller method
comparison with other methods, 50
efficiency index, 48
formula, 47
limitations, 48
order of convergence, 48
Maclaurin series, 7
mantissa, 2
Markov chain, 478
master polynomial, 79
matrix
augmented, 60
companion, 148
Jacobian, 44
orthogonal, 154
similarity, 154
transformation, 154
symmetric, 150, 153
tridiagonal, 159
Vandermonde, 78
mean
definition, 463
of uniform random numbers, 469
Metropolis algorithm, 478
mid-point rule
error, 195
formula, 195
Milne–Simpson method, 250, 265
minimum
of a two-dimensional function
a method using derivatives, 186
Nelder–Mead simplex method, 187
of an one-dimensional function
a method by solving f = 0, 183
a method enclosing the minimum,
183
definition, 183
Mittag–Leffler function, 383, 398
MLC circuit equation, 455
modified trapezoidal rule
for fractional integration, 212
moment, 463
Monte Carlo method
nth root of a real positive number, 501
application to hydrogen atom, 507, 513
Buffon’s needle problem, 519
definition, 464, 495
estimation of
π, 501
e by Dart method, 505
e by derangement method, 506
evaluation of definite integrals, 498
primary components, 496
multi
-step method, 247
-symplectic
conservation law, 353
integrator, 355
multiple root, 8
Nelder–Mead method, 187
Neumann boundary condition, 310
Newton polynomial interpolation
error bound, 125
formula, 124, 128
truncation error, 124, 128
Newton–Cotes formula, 193, 194
Newton–Raphson method
comparison with other methods, 50
for n-dimensional equations, 44
for one-dimensional equations
complex roots, 43
efficiency index, 43
iterative rule, 39
limitations, 42
multiple root, 42
order of convergence, 43
systematic procedure, 41
termination criteria, 40
for two-dimensional equations, 45538 Index
noise, 5
nonlinear equation, 30
normal equations, 95
numerical integration
Boole’s rule, 214
definition, 193
double, 209
Gauss–Legendre rule, 207, 218
Gaussian formula, 205
Lobatto rule, 218
mid-point rule, 195
Newton–Cotes formula, 193, 194
quadrature, 193
Radan rule, 218
rectangle rule, 195
Simpson’s 1/3-rule, 196, 200
Simpson’s 3/8-rule, 197, 201
trapezoidal rule, 196, 200
order of convergence, 9, 34
ordinary differential equation
Adams–Bashforth–Moulton method,
249, 250
boundary-value problem, 220, 293
definition, 220
eigenvalue problem, 220
Euler method, 221
finite-difference method, 295
Hamming method, 251
implicit Euler algorithms, 277
initial-value problem, 220
linear, 220
Milne–Simpson method, 250
multi-step method, 221, 247
nonlinear, 220
Rosenbrock method, 259
Runge–Kutta methods, 231
single-step method, 221
overflow, 4
Pad´e approximation, 140
Park–Miller’s method, 468
partial differential equation
definition, 220, 307
linear, 308
nonlinear, 308
quasilinear, 308
partial pivoting, 62
pendulum system
equations of motion, 270
Hamiltonian, 270
percolation
bond, 514
definition, 513
mean connectedness length, 516
site percolation model, 514
threshold, 514
periodic
function, 6
orbit, 403
phase space, 270
pivot
element, 62
equation, 62
row, 62
pivoting, 62
Poisson
distribution, 505
equation, 334
finite-difference formula, 335
numerical solution, 335
polynomial equations
definition, 13
Descartes’ rule, 13
general form, 13
Gr¨affe’s root square method, 20
Lagrange, 79, 136
Laguere’s method, 26
master, 79
Newton, 124
reciprocal type, 27
Sturm’s theorem, 14
power
method, 150
spectrum
by FFT, 449
definition, 447
discret, 448
of cos ωt, 448
of a chaotic solution, 457
of a constant, 448
of a quasiperiodic orbit, 456
of square-wave, 454
of uniform random numbers, 458
predictor, 230, 249
Preissman scheme, 372
probability density
binomial, 493
Cauchy, 480, 493
circular, 493
defintion, 465
gamma, 493Index 539
Gaussian, 471, 472
L´evy, 481
Poisson, 493
QL method, 159
quasi periodic orbit, 455
Radan rule, 218
random
error, 5
numbers
binomial, 493
Cauchy, 481, 493
circular, 493
definition, 463
dichotomous, 485, 486
gamma, 493
Gaussian, 472, 473, 480
importance, 464
L´evy, 484
linear congruential sequence, 466
Metropolis algorithm, 478
Poisson, 493
pseudo, 463
two-dimensional, 478
uniform, 465
sampling techniques, 474
variable
continuous, 463
discrete, 463
randomness
autocorrelation test, 471
uniformity test, 471
rational function
form, 140
interpolation, 142
reciprocal equations
definition, 27
properties of roots, 27
rectangle rule
error, 195
formula, 195
rejection technique, 476
relative error, 5, 198
residuals, 94
Richardson extrapolation, 172, 298
Riemann–Liouville operator, 378
Rolle’s theorem, 7
root-mean-square error, 94
roots of
an equation, 8
cubic equations, 16
polynomial equations
Gr¨affe’s root square method, 20
quadratic equations, 15
reciprocal equations, 27
Rosenbrock method, 259
round-off error, 3, 4
Runge–Kutta method(s)
−Fehlberg formula, 245
5 points Merson formula, 238
condition for
consistency, 244
convergence, 244
first-order, 232
fourth-order
formula, 237, 238, 253
geometrical description, 238
global error, 239
local truncation error, 239
stability condition, 239
second-order
formula, 233
global error, 234
local truncation error, 233
stability condition, 234
Rutishauser method, 162
Schr¨odinger equation, 299, 336, 337
finite-difference formula, 299, 338
secant method
comparison with other methods, 50
efficiency index, 39
formula, 37
limitations, 38
order of convergence, 39
procedure, 36
sequence
Sturm, 14
series
Fourier, 442
Maclaurin, 7
Taylor, 7
shooting method, 294
significant digits, 3
simplex, 187
Simpson’s
1/3-rule
error, 197, 201
formula, 197, 200
3/8-rule
error, 198, 202540 Index
formula, 197, 201
sine-Gordon equation, 364
a four-point scheme, 366
stability condition, 367
singular-value problem, 56, 84
skewness, 487
solution of linear equations
Cramer’s rule, 58
decomposition method, 68
for lower-triangular form, 58
for upper-triangular form, 58
Gauss elimination method, 60
Gauss–Jordan elimination method, 65
Gauss–Seidel iterative formula, 74
homogeneous type, 82
inhomogeneous type, 86
Jacobi iterative formula, 74
least-squares problem, 84
of Vandermonde matrix form, 78
singular-value problem, 84
triangular factorization method, 68
square-wave
Fourier series, 445, 446, 452
mathematical representation, 445
power spectrum, 454
stability
condition for
damped wave equation, 321
Euler method, 229
finite-difference formula, 316
fourth-order Runge–Kutta method,
239
heat equation, 323
second-order Runge–Kutta method,
234
wave equation, 316, 319
function, 228
standard
deviation, 463, 504
error, 504
stiff equation
definition, 255
implicit Euler method, 257
Rosenbrock method, 259
students t-test, 487
Sturm
functions, 14
sequence, 14
theorem, 14
successive over-relaxation, 333
symplectic
conservation law, 351
integrator, 269, 273
nth order Candy–Rozmus method,
285
adaptive St¨ormer–Verlet method,
289
Euler algorithms, 277, 288
first-order algorithm, 276
for harmonic oscillator, 278
Runge–Kutta type, 282
second-order method, 280
St¨ormer–Verlet method, 281, 288
manifold, 271, 351
map, 274, 275
systematic error, 5
Taylor series expansion, 169
theorem
intermediate-value, 7, 31
Rolle’s, 7, 125
Sturm’s, 14
trapezoidal rule
error, 196, 200
formula, 196, 200, 209
trigonometric polynomials, 111
truncation error, 5
underflow, 4
uniform random numbers
definition, 465
general procedure, 468
mean value, 466
Park and Miller’s algorithm, 468
probability density, 465
variance value, 466
uniformity test, 471
upper-triangular system, 58, 89
Vandermonde matrix, 78
variance
definition, 463
of uniform random numbers, 469
wave equation, 309, 313, 316, 346, 347
an implicit formula, 318
stability condition, 319
damped, 320, 347
finite-difference formula, 320
stability condition, 321
finite-difference formula, 314
numerical solution, 317, 320
stability condition, 316Index 541
time-fractional, 420
equilibrium solution, 421
finite-difference scheme, 421
stability condition, 421
zero of a function,
8
