The Information Retrieval Series
Probabilistic Indexing 
for Information 
Search and Retrieval 
in Large Collections 
of Handwritten 
Text Images 
Alejandro Héctor Toselli 
Joan Puigcerver 
Enrique Vidal The Information Retrieval Series
Volume 49
Series Editors
ChengXiang Zhai, University of Illinois, Urbana, IL, USA
Maarten de Rijke, University of Amsterdam, The Netherlands and Ahold Delhaize,
Zaandam, The Netherlands
Editorial Board Members
Nicholas J. Belkin, Rutgers University, New Brunswick, NJ, USA
Charles Clarke, University of Waterloo, Waterloo, ON, Canada
Diane Kelly, University of Tennessee at Knoxville, Knoxville, TN, USA
Fabrizio Sebastiani , Consiglio Nazionale delle Ricerche, Pisa, Italy Information Retrieval (IR) deals with access to and search in mostly unstructured 
information, in text, audio, and/or video, either from one large file or spread over 
separate and diverse sources, in static storage devices as well as on streaming data. 
It is part of both computer and information science, and uses techniques from e.g. 
mathematics, statistics, machine learning, database management, or computational 
linguistics. Information Retrieval is often at the core of networked applications, 
web-based data management, or large-scale data analysis.
The Information Retrieval Series presents monographs, edited collections, and 
advanced text books on topics of interest for researchers in academia and industry 
alike. Its focus is on the timely publication of state-of-the-art results at the forefront
of research and on theoretical foundations necessary to develop a deeper under￾standing of methods and approaches.
This series is abstracted/indexed in EI Compendex and Scopus.Alejandro Héctor Toselli • Joan Puigcerver 
Enrique Vidal
Probabilistic Indexing 
for Information 
Search and Retrieval 
in Large Collections 
of Handwritten 
Text Images Alejandro Héctor Toselli Joan Puigcerver 
Universitat Politècnica de València Google Research 
Valencia, Spain Zurich, Switzerland 
Enrique Vidal 
Universitat Politècnica de València 
Valencia, Spain 
ISSN 1871-7500 ISSN 2730-6836 (electronic) 
The Information Retrieval Series 
ISBN 978-3-031-55388-2 ISBN 978-3-031-55389-9 (eBook) 
https://doi.org/10.1007/978-3-031-55389-9
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland 
AG 2024 
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, 
whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, 
reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, 
and transmission or information storage and retrieval, electronic adaptation, computer software, or by 
similar or dissimilar methodology now known or hereafter developed. 
The use of general descriptive names, registered names, trademarks, service marks, etc. in this 
publication does not imply, even in the absence of a specific statement, that such names are exempt 
from the relevant protective laws and regulations and therefore free for general use. 
The publisher, the authors, and the editors are safe to assume that the advice and information in this 
book are believed to be true and accurate at the date of publication. Neither the publisher nor the 
authors or the editors give a warranty, expressed or implied, with respect to the material contained 
herein or for any errors or omissions that may have been made. The publisher remains neutral with 
regard to jurisdictional claims in published maps and institutional affiliations. 
This Springer imprint is published by the registered company Springer Nature Switzerland AG 
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland 
Paper in this product is recyclable. Preface
The technology presented in this book was developed by members of the Pattern
Recognition and Human Language Technology (PRHLT) research center of the
Universitat Politecnica de Val ` encia ` (UPV), Spain. PRHLT dates back to the decade
of 1990. In addition to the development of fundamental Pattern Recognition (PR)
methods, the main application field of PRHLT in those early years was Automatic
Speech Recognition (ASR). But the research activity was soon expanded to deal
also with Machine Translation (MT) and, later (around 2010), with Computer Vision
(CV) and Multimodal Interaction in PR.
Also at that time, a new research line was started on Document Analysis and
Handwritten Text Recognition (HTR). It became clear soon that the application
field of these technologies was not in nowadays handwritten documents, but in the
astronomical number of historical series of manuscripts that are held by archives
and libraries around the world. Not in vain it is often speculated that the amount
of original handwritten text existing in the world notably surpass the printed or
typewritten text, including native electronic documents!
When dealing with historical, handwritten text, a major issue emerges forcibly:
uncertainty. The concept of “ground truth” or “reference” transcript proves elusive:
If several paleography experts are asked to interpret a given piece of this kind
of handwritten text, what you generally get are several different interpretations –
nothing close to the single, unique transcript you would need as ground truth.
The reasons are plenty: Historical manuscripts are generally plagued with scrawled
scripts, extremely abridged and tangled abbreviations and archaic or outdated word
spellings. Moreover, punctuation is rarely used, or is used inconsistently, and reading￾order is often ambiguous, due to conflicting and even erratic layout. In addition,
there are many types of preservation-related degradations, such as lack of contrast
or support, dirt spots, severe show-through and bleed-through, etc.
Because of these issues, it promptly became very clear that the classical goal of
HTR technologies –given a text image, produce a unique transcript which is “as
correct as possible”– was a delusion. Therefore, suitable approaches were needed
that explicitly embraced the intrinsic uncertainty of the historical handwritten text.
vvi Preface
The first of these approaches was CATTI (for Computer Assisted Transcription of
Text Images):1 Rather than insisting in finding the best, or the most correct transcript,
just offer the users a bunch of likely alternatives and let them decide what is “best”
or “correct”. In CATTI, the process is interactive and driven by user feedback, so
alternative transcription hypotheses tend to follow what the user has previously
validated, suggested, or amended.
While CATTI proved useful to cost-effectively transcribe singular historical
manuscripts or small sets of specific text images, it is obviously not an option
when the historical collection considered amounts to hundreds of thousand, or mil￾lions of pages. In these cases, any approach that requires human expert intervention
quickly becomes not scalable, or plainly unrealistic.
Interestingly, when large series of manuscripts are considered, the primary interest
is generally not transcription, but making the series searchable. So a naive reasoning
became very popular to deal with this situation: first transcribe the whole series
automatically by means of a “good” HTR system, and then apply a suitable off-the￾shelf platform for plain text search. However, because of the intrinsic uncertainty
discussed above (and the sheer scale of the task), this naive idea is rather unrealistic;
transcription “errors” (or just unexpected interpretations) will be plenty and, what
is worse, it will be difficult to asses to which extent the obtained transcripts are
“correct”. Therefore, searching for information in such an uncontrollably noisy text
is typically prone to yield a disappointing search experience.
This is how our Probabilistic Indexing (PrIx) framework emerged. It explicitly
embraces the inherent ambiguities and uncertainty of historical handwritten text and
it capitalizes on this acquiescence to provide excellent search performance, even for
massive series of complex manuscripts. As discussed in this book, by adequately
modeling the uncertainty, PrIx methods prove very effective to actually allow users
find the information they are interested in.
Of course, once the information is found in a page or a section of some manuscript,
the relevant handwritten text can be automatically transcribed, if needed. Then the
user, or an expert paleographer, can revise these specific transcripts (maybe with
CATTI assistance) to ensure the required level of reliability. Or it may be also the
case that the target of the search process was not to get hold of specific text, but rather
to use the discovered information in further studies. In this case, we also discuss in
the book (Chapter 9) how the PrIx representation of a large collection of text images
can be advantageously used for this purpose.
PRHLT researchers developed PrIx (and HTR) technologies thanks to several
projects funded by National and European research programs. Perhaps the most
important projects were tranScriptorium and READ, followed by HIMANIS, and
Carabela. By the end of these projects (2020), we had released an open-source
HTR toolkit called PyLaia based on deep Convolutional-Recurrent Neural Networks
(which is nowadays considered one of the best HTR toolkits). On the other hand, the
fundamentals of the PrIx framework were already fully developed and a completely
operative set of PrIx tools and the corresponding workflow had been implemented
1 Reference [29] in Chapter 1.Preface vii
and fully tested in several large-scale series of historical manuscripts (these works
and results are described in Chapter 10 of this book).
In sum, by 2020 the PRHLT/UPV PrIx (and HTR) technology was fully mature
and ready to be applied in real indexing projects. The first of these projects was the
so called Finnish Court Records, where the PrIxs of more than one million page
images were successfully produced through a collaboration of the National Archives
of Finland, READ-coop and PRHLT/UPV. Yet, PRHLT as an academic research
center of the UPV, was not the most adequate institution to manage this kind of
projects which were mostly application-oriented. So, by the end of 2020, the UPV
promoted the creation of a spin-off company called tranSkriptorium IA SL (tS).
Since its creation, tS has been busy with important indexing projects both in
Spain and in other European countries. Given the very nature of PrIx, tS only deals
with large documentary series, say from a few tens of thousands to millions of text
images. Smaller series and singular transcription works are not considered for now
in the tS business plan.
Harnessing the PrIx workflow for real large-scale applications has proved fairly
complex. Best results are always achieved by training or at least fine-tuning the
PrIx models with the most representative images (and transcripts) of the series
considered. The amount of training images required to achieve satisfactory results
is also difficult to predict. Adequate management of these complexities usually
boils down to understanding what are the most significant writing style and layout
differences between images of different parts of a series of manuscripts. So the
whole process entails a multidisciplinary pipeline where expert paleographers are
needed in the center of a loop that we call expert-mediated active learning (outlined
in Appendix C of this book). This process ensures results that are perfectly adapted
to the particular needs of the archive or library users for each specific document
collection.
To manage these complexities, tS has developed a competitive multidisciplinary
team with the know-how and skills needed for an efficient and productive use of the
PrIx and HTR technologies inherited from PRHLT/UPV. This allows the company
to be already self-sustainable with production and service costs perfectly affordable
by most public archives and libraries.
Of course, the contents of this book could make things easier for individuals
or institutions that wish to develop their own workflows to process large series of
historical documents. In fact this is the purpose.
However, it is worth to consider cooperation rather than competition. There are
plenty of emerging applications of PrIx (and/or HTR) technologies for documentary
management in archives and libraries. And tS is keen to discuss interesting precom￾petitive collaborations to help developing this kind of applications. Moreover, tS, as
a spin-off of PRHLT/UPV enjoys a tight collaboration with PRHLT researchers who
go on doing fundamental research on open or basic problems of document analysis
and recognition. This allows cost-effective research on important issues that would
otherwise be prohibitive for small companies such as tS.viii Preface
Let us focus now on the contents of this book. It constitutes a comprehensive,
ordered and coherent compilation of our work on Probabilistic Indexing of Hand￾written Text Images during the last decade. Parts of this work have been presented
in several conferences and workshops in the field of document image processing, as
well as in a few journal papers.2 But a very significant part of the book stems from
the mostly unpublished PhD work3 of one of the authors, as well as from tutorial
and keynote presentations that had never been published in proceedings or journals.
The book is structured into 11 chapters and three appendices, as follows:
Chapter 1 (Introduction) exposes motivations for effective information retrieval so￾lutions to real-life searching on large collections of handwritten documents.
Fundamentals of Pattern Recognition, Statistical Decision Theory, and Hand￾written Text Recognition are briefly outlined, along with Information Retrieval
and a comprehensive account of the evaluation measures most commonly
adopted in this field.
Chapter 2 (State Of The Art) outlines the state of the art in the different fields related
with the book contents. Basic techniques and currently available solutions to
tackle the relevant problems are presented and compared. More technical details
of the most interesting approaches, however, will be presented in Chapter 7, once
the notation and the fundamental framework have been completely established.
Chapter 3 (Probabilistic Framework) presents, in a principled and comprehensive
manner, the approaches we propose for indexing (as opposed to “spotting”)
each region of a handwritten text image which is likely to contain a word.
Chapter 4 (Probabilistic Models for Handwritten Text) describes models that we
have adopted for handwritten text in images, namely hidden Markov models,
convolutional and recurrent neural networks and language models. This chapter
also provides full details of weighted finite-state transducer (WFST) concepts
and methods, needed in further chapters of the book. Under this point of view,
word or character graphs or lattices are reviewed in full detail.
Chapter 5 (Probabilistic Indexing for Fast and Effective Information Retrieval) in￾troduces and explain the set of techniques and algorithms developed to generate
image probabilistic indexes, to allow for fast search and retrieval of textual
information in the indexed images.
Chapter 6 (Empirical Validation of Probabilistic Indexing Methods) presents ex￾perimental evaluations of the proposed framework and algorithms on different
traditional benchmark datasets and compares them with other traditional ap￾proaches. Results on new, very much larger and realistic experimental datasets
are also included.
2 References [37, 36, 40] in Chapter 1.
3 Reference [27] in Chapter 1.Preface ix
Chapter 7 (Probabilistic Interpretation of Traditional KWS Approaches) reviews
the most popular KWS approaches, the majority of which based on heuristic
arguments, and provides probabilistic interpretation of these arguments.
Chapter 8 (Probabilistic Indexing Search Extensions) explains how a basic, word￾based PrIx can support classical free-text search tools, such as arbitrarily com￾plex Boolean multi-word (AND/OR/NOT) combinations and word sequences, as
well as wildcard and flexible or approximate spelling. Other extensions include
using entire words in queries to find also word instances that my be hyphenated
in unknown or even unexpected manners, as well as tabular queries to retrieve
geometrically structured information in handwritten tables.
Chapter 9 (Beyond Search Applications of PrIx) presents new methods that use
PrIx not only for searching, but also to deal with text analytics and other
related natural language processing and information extraction tasks, directly
on untranscribed collections of text images.
Chapter 10 (Large-scale Systems and Applications) shows how the proposed so￾lutions can be used to effectively index real, large collections of handwritten
document images, which typically are orders of magnitude larger than the tra￾ditional academic datasets. Several showcases of large-scale image collections
of historical manuscripts, which have been very successfully indexed using the
proposed approaches are also presented.
Chapter 11 (Conclusion and Outlook) summarize the contributions of this book and
suggests promising lines of future research.
Appendix A (The Probability Ranking Principle) provides Decision Theoretic back￾ground for the PrIx framework and the corresponding methods developed in
this book.
Appendix B (Weighted Finite State Transducers) outlines the algebra and basic
algorithms of WFSTs, which constitute the main supporting machinery for the
algorithms presented (mainly) in Chapter 5.
Appendix C (Text Image Document Collections and Datasets) presents details of the
text image collections and datasets used in the experiments reported throughout
the book and/or in the large-scale demonstrators presented in Chapter 10.
Valencia (Spain), Alejandro Hector Toselli ´
December, 2023 Joan Puigcerver
Enrique VidalAcknowledgements
The authors are deeply grateful to the following people and publicly funded projects
and grants, which have made possible the writting of several sections of this book.
Persons
Specific thanks are due to:
Name Contribution
Jose Andr ´ es´ Collaboration in the work of Secs. 8.4 and 9.4
Jose Ram ´ on Prieto Collaboration in the work of Sec. ´ 9.5
Dr. Carlos Alonso Collaboration in compiling datasets of Appendix C.9
Dr. Veronica Romero Collaboration in work of Sec. ´ 6.9 and Apps. C.8,C.9
Dr. Joan Andreu Sanchez Collaboration in the work of Sec. ´ 6.9 and Appendix C.2
Dr. Giorgos Sfikas For providing Fig. 7.3
In general, the authors also warmy acknowledge the help of all the current and former
PRHLT members who contributed with code development and in the realization of
many of the experiments reported throughout this book.
Projects
Thanks to the tranScriptorium and READ Projects for supporting the initial techno￾logical developments of PrIx and the compilation of many of the datasets described in
AppendixC. Moreover, thanks to the HIMANIS andCarabela Projectsfor supporting
the development of the large-scale PrIx demonstrators described in Chapter 10, and
the compilation of the datasets presented in Appendixes C.7 and C.9, respectively.
Grants
The work of one of the authors, Alejandro H. Toselli, is currently supported by
a Mar´ıa Zambrano Grant from the Spanish Ministerio de Universidades and the
European Union NextGenerationEU/PRTR.
xiContents
Acronyms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xix
List of Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxiii
1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.1 Motivation and Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2 Information Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.3 Pattern Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.4 Decision Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.5 Handwritten Text Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
1.6 Assessing Indexing and Search Performance . . . . . . . . . . . . . . . . . . . . 9
1.7 Handwritten Text Recognition and Probabilistic Indexing . . . . . . . . . 14
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2 State Of The Art . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.1 The field of a Hundred Names . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.2 Taxonomy of KWS approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
2.2.1 Segmentation Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
2.2.2 Retrieved Objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.2.3 Query Representation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.2.4 Training Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
2.3 Additional Significant Matters for KWS . . . . . . . . . . . . . . . . . . . . . . . . 25
2.3.1 Hyphenated Words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
2.3.2 Abbreviations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
2.3.3 Multiple-word Queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.4 State of the Art in HTR Models and Methods . . . . . . . . . . . . . . . . . . . 26
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
xiii
List of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxv
List of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxxi
.xiv Contents
3 Probabilistic Indexing (PrIx) Framework . . . . . . . . . . . . . . . . . . . . . . . . . 33
3.1 Pixel Level Textual Image Representation: 2-D Posteriorgram . . . . . 33
3.2 Image Regions for Keyword Indexing and Search . . . . . . . . . . . . . . . . 35
3.3 Position-independent PrIx . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
3.3.1 Naive Word Posterior Interpretation of 푃(푅 | 푥,푣) . . . . . . . . . 37
3.3.2 Proposed Approximations to 푃(푅 | 푥,푣) . . . . . . . . . . . . . . . . . 38
3.3.3 Estimating Image-Region RPs from Posteriorgrams . . . . . . . 39
3.3.4 Line-Region RP and 1-D Posteriorgram . . . . . . . . . . . . . . . . . 41
3.4 Position-independent PrIx and KWS from a HTR Viewpoint . . . . . . 42
3.4.1 Comparing the Image Processing and HTR Viewpoints . . . . 44
3.5 Position-dependent PrIx . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
3.5.1 Relevance of an Horizontal Coordinate Position . . . . . . . . . . 46
3.5.2 Relevance of a Segment of Text-line Image Region . . . . . . . . 48
3.5.3 Relevance of a Transcript Ordinal Position . . . . . . . . . . . . . . . 51
3.6 Query-by-Example Paradigm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
3.6.1 Position-independent RPs for Query by Example KWS . . . . . 52
3.6.2 Position-dependent RPs for Query by Example KWS . . . . . . 53
3.7 Relations among Position-Dependent and Independent RPs . . . . . . . 54
3.7.1 Equivalences of Positional RPs and other Posterior
Probabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
3.7.2 Computing Horizontal Coordinate RP from Segment RP . . . 56
3.7.3 Expected Values of Segments and Ordinal Positions . . . . . . . 56
3.7.4 RP Inequalities Based on Frechet ´ Bounds . . . . . . . . . . . . . . . . 58
3.8 PrIx Implementation Foreword . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
4 Probabilistic Models for Handwritten Text . . . . . . . . . . . . . . . . . . . . . . . . 65
4.1 Traditional Image Preprocessing and Feature Extraction . . . . . . . . . . 65
4.1.1 Image Preprocessing and Text Segmentation . . . . . . . . . . . . . 66
4.1.2 Text Line Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
4.1.3 Feature Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
4.2 Optical Modeling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
4.3 Hidden Markov Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
4.3.1 Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
4.3.2 HMM Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
4.3.3 HMMs for Optical Modeling in Handwritten Text
Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
4.4 Artificial Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
4.4.1 Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
4.4.2 Convolutional Layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
4.4.3 Recurrent Layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
4.4.4 CRNN Training Through Gradient Descent . . . . . . . . . . . . . . 83
4.4.5 Neural Networks for Handwritten Text . . . . . . . . . . . . . . . . . . 84
4.5 Key differences between HMMs and CRNNs with CTC . . . . . . . . . . 86
4.6 푁-gram Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87Contents xv
4.6.1 Combining the Output of a CRNN with a 푁-gram LM . . . . . 89
4.7 Weighted Finite State Transducers (WFST) . . . . . . . . . . . . . . . . . . . . . 90
4.7.1 The WFST Composition Operation . . . . . . . . . . . . . . . . . . . . . 92
4.7.2 Handling CTC by Means of Elementary WFST Operations . 92
4.7.3 Lattices Represented as WFST or WFSA . . . . . . . . . . . . . . . . 94
4.7.4 Normalization of Lattice Weights . . . . . . . . . . . . . . . . . . . . . . . 96
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
5 Probabilistic Indexing for Fast and Effective Information Retrieval . . 107
5.1 Lexicon-Based and Lexicon-Free PrIx . . . . . . . . . . . . . . . . . . . . . . . . . 107
5.2 Lexicon-based PrIx from Pixel-level Posteriorgrams . . . . . . . . . . . . . 108
5.3 Indexing Lexicon-based Lattices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
5.3.1 Position-independent Relevance . . . . . . . . . . . . . . . . . . . . . . . . 111
5.3.2 Lexicon-based Segment Relevance. . . . . . . . . . . . . . . . . . . . . . 114
5.3.3 Lexicon-based Horizontal Position Relevance . . . . . . . . . . . . 115
5.3.4 Lexicon-based Ordinal Position Relevance . . . . . . . . . . . . . . . 115
5.4 The Out-of-vocabulary Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
5.5 Indexing Lexicon-free Lattices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
5.5.1 From Character to Word Lattices . . . . . . . . . . . . . . . . . . . . . . . 119
5.6 Alternative Approaches for Lexicon-free PrIx . . . . . . . . . . . . . . . . . . . 123
5.6.1 Lexicon-free Segment Relevance . . . . . . . . . . . . . . . . . . . . . . . 123
5.6.2 Lexicon-free Ordinal Position Relevance . . . . . . . . . . . . . . . . . 132
5.7 Multi-word and Regular-Expression Queries . . . . . . . . . . . . . . . . . . . . 135
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
6 Empirical Validation of Probabilistic Indexing Methods . . . . . . . . . . . . 139
6.1 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
6.1.1 Evaluation Protocol: Image Regions, Query Sets and Metrics140
6.1.2 Datasets and Query Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
6.1.3 Statistical Models for Handwritten Text . . . . . . . . . . . . . . . . . . 142
6.2 Assessing Posteriorgram Methods for Lexicon-based PrIx . . . . . . . . 143
6.3 Comparing Position-Dependent RP Definitions . . . . . . . . . . . . . . . . . . 145
6.4 Evaluating Language Model Impact . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
6.4.1 Lexicon-based Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
6.4.2 Lexicon-free Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
6.4.3 Effect of the Optical and Character-label Prior Scales . . . . . . 153
6.5 Impact of Training-set Size and Data Augmentation . . . . . . . . . . . . . . 155
6.6 Correlation between Average Precision and HTR Error Rates . . . . . . 156
6.7 Results on Other Academic Benchmark Datasets . . . . . . . . . . . . . . . . 158
6.7.1 George Washington . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
6.7.2 Parzival . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
6.7.3 Comparison with Previous State-of-the-art Results . . . . . . . . 161
6.8 Comparing CRNN and HMM-GMM Optical Modeling . . . . . . . . . . . 163
6.9 Experiments for Real Indexing Projects . . . . . . . . . . . . . . . . . . . . . . . . 165
6.9.1 Passau . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167xvi Contents
6.9.2 Chancery (Himanis) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
6.9.3 Teatro del Siglo de Oro (TSO) . . . . . . . . . . . . . . . . . . . . . . . . . 168
6.9.4 Large Bentham Dataset (BEN4) . . . . . . . . . . . . . . . . . . . . . . . 169
6.9.5 Carabela . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
6.9.6 Finish Court Records (FCR) . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
6.10 Segmentation-free Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
6.10.1 ICDAR2015 Competition on Handwriting KWS . . . . . . . . . . 172
6.10.2 ICFHR2014 Competition on QbE Handwriting KWS . . . . . . 173
6.11 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
7 Probabilistic Interpretation of Traditional KWS Approaches. . . . . . . . 181
7.1 On the Spotting Versus Recognition Debate . . . . . . . . . . . . . . . . . . . . 181
7.2 Distance-based Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
7.2.1 Simplifying QbE RP for Word-segmented Image Regions . . 183
7.2.2 Distance-based Density Estimation . . . . . . . . . . . . . . . . . . . . . 184
7.2.3 Interpretation of Distance-based KWS: Empirical Results . . 188
7.3 PHOC-based Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192
7.3.1 Predicting the PHOC of a Word Image Region: PHOCNet . . 193
7.3.2 PHOC-based QbE KWS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
7.3.3 Probabilistic PHOCNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
7.3.4 PHOCNet Probabilistic Interpretation: Empirical Results . . . 194
7.3.5 Summary of Results of Distance– and PHOC–based Methods197
7.4 HMM-Filler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
7.4.1 HMM-Filler Probabilistic Interpretation: Experiments . . . . . 201
7.4.2 Fast HMM-Filler Computation using Character Lattices . . . . 203
7.5 BLSTM-CTC KWS. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206
7.5.1 BLSTM-CTC KWS Interpretation: Experimental Validation 209
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210
8 Probabilistic Indexing Search Extensions . . . . . . . . . . . . . . . . . . . . . . . . . 213
8.1 Multi-Word Boolean and Word-Sequence Queries . . . . . . . . . . . . . . . 213
8.1.1 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
8.2 Searching for Music Symbol Sequences . . . . . . . . . . . . . . . . . . . . . . . . 218
8.2.1 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
8.3 Structured Queries for Information Retrieval in Table Images . . . . . . 221
8.3.1 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222
8.4 Searching for Hyphenated Words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
8.4.1 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226
8.5 Approximate-Spelling and Wildcard Queries. . . . . . . . . . . . . . . . . . . . 227
8.5.1 Approximate-Spelling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
8.5.2 Wildcard Spelling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
8.5.3 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232Contents xvii
9 Beyond Search Applications of Probabilistic Indexing . . . . . . . . . . . . . . 233
9.1 Text Analytics Using PrIx . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
9.2 Estimating Word and Document Frequencies from PrIxs . . . . . . . . . 234
9.3 Zipf Curves, Running Words and Lexicon Size . . . . . . . . . . . . . . . . . . 235
9.3.1 Estimating Running Words and Lexicon Size: Results . . . . . . 237
9.4 Statistical Information Extraction from Text Images . . . . . . . . . . . . . . 238
9.4.1 Indexing Semantically Tagged Words and Named Entities . . 239
9.4.2 Statistical Information Extraction from Handwritten Forms . 241
9.5 Classification of Large Untranscribed Image Documents . . . . . . . . . . 242
9.5.1 Plaintext Document Classification . . . . . . . . . . . . . . . . . . . . . . 244
9.5.2 Estimating Text Features from Image PrIxs . . . . . . . . . . . . . . 245
9.5.3 Image Document Classification . . . . . . . . . . . . . . . . . . . . . . . . 246
9.5.4 Open Set Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246
9.5.5 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
9.5.6 Image Document Classification Concluding Remarks . . . . . . 253
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
10 Large-scale Systems and Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
10.1 Conceptual System Organization and Workflow . . . . . . . . . . . . . . . . . 255
10.1.1 PrIx Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
10.1.2 Spots Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
10.1.3 PrIx Search Engine and User Interface . . . . . . . . . . . . . . . . . . 258
10.2 Architecture Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
10.2.1 Web and Data Servers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
10.2.2 PrIx Server and Search Engine . . . . . . . . . . . . . . . . . . . . . . . . . 260
10.2.3 Web Client . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
10.3 Large-Scale Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
10.3.1 Tresor des ´ Chartes (Chancery) . . . . . . . . . . . . . . . . . . . . . . . . 264
10.3.2 Teatro del Siglo de Oro (TSO) . . . . . . . . . . . . . . . . . . . . . . . . . 266
10.3.3 Bentham Papers (Bentham) . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
10.3.4 Parcels from Indias and Cadiz Arc ´ hives (Carabela) . . . . . . 268
10.3.5 Finnish Court Records (FCR) . . . . . . . . . . . . . . . . . . . . . . . . . . 271
10.3.6 General Discussion on Large-Scale Applications . . . . . . . . . . 273
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275
11 Conclusion and Outlook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
11.1 Contribution Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
11.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281
A The Probability Ranking Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283
A.1 Ranking Multiple Relevant Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283
A.2 Evaluation Measures and Optimality . . . . . . . . . . . . . . . . . . . . . . . . . . 284
A.2.1 Precision-at-푘 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
A.2.2 Recall-at-푘 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286xviii Contents
A.2.3 Average Precision (AP) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
A.2.4 Discounted Cumulative Gain (DCG) . . . . . . . . . . . . . . . . . . . . 293
A.2.5 Normalized Discounted Cumulative Gain (NDCG) . . . . . . . . 295
A.3 Global and Mean Measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
B Weighted Finite State Transducers (WFST) . . . . . . . . . . . . . . . . . . . . . . . 299
B.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
B.2 Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300
B.3 WFST Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
B.3.1 Composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
B.3.2 Shortest Path and Distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304
B.4 Determinization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308
C Text Image Document Collections and Datasets . . . . . . . . . . . . . . . . . . . . 309
C.1 IAM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310
C.2 The Bentham Papers Collection and Datasets . . . . . . . . . . . . . . . . . . . 312
C.2.1 ICFHR-2014 Competition on HTR (BEN1) . . . . . . . . . . . . . . 312
C.2.2 ICFHR-2014 Competition on KWS (BEN2) . . . . . . . . . . . . . . 315
C.2.3 ICDAR-2015 Competition on KWS (BEN3) . . . . . . . . . . . . . . 317
C.2.4 Large Bentham Dataset used in [28] (BEN4) . . . . . . . . . . . . . 318
C.3 George Washington (GW) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319
C.4 Parzival (PAR) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322
C.5 Plantas (PLA) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323
C.6 Passau Parish Records (PAS) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325
C.7 Tresor des ´ Chartes and Chancery (CHA) . . . . . . . . . . . . . . . . . . . . . . 328
C.8 Spanish Golden Age Theater (TSO) . . . . . . . . . . . . . . . . . . . . . . . . . . . 330
C.9 Parcels from Indias and Cadiz Arc ´ hives: Carabela (CAR) . . . . . . . 332
C.10 Finnish Court Record (FCR) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 334
C.11 The Vorau-253 Sheet Music Manuscript and Dataset . . . . . . . . . . . . . 337
C.12 A Dataset for Multi-page Handwritten Deeds Classification . . . . . . . 339
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343Acronyms
Lists of Abbreviations
ANN Artificial Neural Network
AP (raw) Average Precision
BB Bounding Box
BLSTM Bi-directional Long-Short Term Memory
BOW Bag of Words
CBIDC Content Based Image Document Classification
CER Character Error Rate
CNN Convolutional Neural Network
CL Character Lattice
CRNN Convolutional-Recurrent Neural Network
CSC Closed Set Classification
CTC Connectionist Temporal Classification
DAG Directed Acyclic Graph
DC Documnet Classification
DCG Discounted Cumulative Gain
DFA Deterministic Finite Automaton
DL Deep Learning
DT Decission Theory
FFN Feed Forward Network
FSA Finite State Automaton or Automata
FST Finite State Transducer
gAP global Average Precision
GMM Gaussian Mixture Model
GT Ground Truth
xixxx Acronyms
HMM Hidden Markov Model
HTR Handwritten Text Recognition
IG Information Gain
IR Information Retrieval
KWS Keyword Spotting
LM Language Model
LSTM Long-Short Term Memory
mAP mean Average Precision
MDLSTM Multidimensional Long-Short Term Memory
MLE Maximum Likelihood Estimation
ML Machine Learning
MLP Multi-Layer Perceptron
NDCG Normalized Discounted Cumulative Gain
OCR Optical Character Recognition
OOV Out of Vocabulary
OSC Open Set Classification
PHOC Pyramid of Histograms of Characters
PR Pattern Recognition
PRP Probability Ranking Principle
PrIx Probabilistic Indexing (or Index)
PrIxs Probabilistic Indexes
QbE Query by Example
QbS Query by String
RNN Recurrent Neural Network
R–P Recall–Precision
RP Relevance Probability
WFSA Weighted Finite State Automaton or Automata
WFST Weighted Finite State Transducer
WER Word Error Rate
WG Word (or character) Graph or latticeAcronyms xxi
Mathematical Notation
Common notation rules adopted across all the Chapters
Symbol(s) Description
푎, 푏, . . . 훼, 훽, . . . Scalar variables, sequences, etc.
퐴, 퐵, . . . Random variables (sometimes also scalar constants or variables)
A, B, . . . Sets
풂, 풃, . . . Vectors
풂
푇 Transpose of a vector
(푎, 푏)
푇 Transpose: vertical vector with componnents 푎 and 푏
푨, 푩, . . . Matrices or tensors
[푨]푖, 푗 Element of a matrix (or tensor) at row 푖 and column 푗
푎1, . . . , 푎푇, 푎1:푇 A sequence of length 푇
푠
1
, . . . , 푠푁 A list of 푁 sequences
|A|, |푎| Size of the set A, length of the sequence 푎
def
= Used in equations to define a symbol or function
★
= The equality holds under some assumption
푎 ← 푏 In algorithms, the value of 푏 is assigned to the variable 푎
푧 ∈ A 푧 belongs to the set A
푣 ∈ 푤 푣 is one of the elements of the sequence 푤 ≡ 푤1:푀; i.e., ∃푘 : 푤푘 = 푣
푢 ⊂ 푤 푢 ≡ 푢1,퐾 is a substring of the sequence 푤 ≡ 푤1:푀, 퐾 ≤ 푀
푧 ⊑ 푍 The image region 푧 is within a larger image region 푍
푃(· · · ) Probability mass function
푝(· · · ) Probability density function
푃(· · · ; 휃) Parametric probability mass function with parameter 휃
푝(· · · ; 휃) Parametric probability density function with parameter 휃
PZ (· · · ), p
Z
(· · · ) Probabilities estimated, or computed, using 푍 (training set, or WG)
E[· · · ] Expected value of an expression
E[· · · | · · · ] Expected value of an expression conditioned on another
E푧 [· · · ] Expected value of a (conditioned) expression with respect to 푧xxii Acronyms
Meaning of mathematical symbols and expressions frequently used across Chapters
Symbol Typical usage
푥 Image region
풃 A word or character bounding box or segment within a line image
푣 A word (or its character sequence) – a query word in most cases
푤 A text; i.e., a sequence of words or characters
푐 A single character. Also, a sequence of CTC character-level labels
푅 Binary random variable for the relevance probability (RP)
푃(푅 | 푥, 푣) ≡ 푃(푅 = 1 | 푋 = 푥, 푄 = 푣): RP of 푥 for the query word 푣
휏 RP threshold
푒 An edge of a graph (usually a WFST), or of a path of a WFST
휙 A path (sequence of edges) of a graph (WFST)
휔(·) The weight of an edge 푒, or of a path 휙 (WFST)
푙푖(·) The input token(s) of an edge 푒 (or of a path 휙) (WFST)
푙표 (·) The output token(s) of an edge 푒 (or of a path 휙) (WFST)
푙(·) The token(s) of an edge 푒 (or of a path 휙) (WFSA)
푝(·) Departing (or “previous”) state of an edge 푒, or of a path 휙 (WFST)
푛(·) Ending (of “next”) state of an edge 푒, or of a path 휙 (WFST)
푎(푞) Alignment (horizontal coordinate) asociated with a state 푞 (WG)
F Set of final states of a WFST or a WFSA
P (푞) Set of edges leading to (or predecesors of) the state 푞 (WFST)
N (푞) Set of edges departing from (or next to) the state 푞 (WFST)
풩(·, ·) Nearest Neighbor(s) FunctionList of Algorithms
5.1 Compute a PrIx from Pixel-level Posteriorgram . . . . . . . . . . . . . . . . . . 110
5.2 Compute a PrIx for a word lattice of an image region . . . . . . . . . . . . . 112
5.3 Compute a PrIx for a word lattice of an image region (opt1) . . . . . . . . 113
5.4 Compute a PrIx for a word lattice of an image region (opt2) . . . . . . . . 113
5.5 Compute a segment PrIx from a word lattice of an image region . . . . 114
5.6 Disambiguate word lattice states . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
5.7 Compute an ordinal position PrIx from a word lattice . . . . . . . . . . . . . 117
5.8 Expansion of subpaths formed by labels of the same class in a WFSA 120
5.9 Edge alignment encoding as part of the output labels of a WFST . . . . 124
5.10 Disambiguate the input symbols of the states in a WFST . . . . . . . . . . . 125
5.11 Convert same-class subpaths into complete paths . . . . . . . . . . . . . . . . . 126
5.12 Keep only word alignments from a WFST yield by Alg. 5.11 . . . . . . . 127
5.13 Compute a word index for text segments based on character lattices. . 128
5.14 Disambiguate character lattice states. . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
5.15 Edge word count encoding in a lattice as the output labels of a WFST 133
5.16 Compute a PrIx for text ordinal positions based on a CL . . . . . . . . . . . 134
xxiii
7.1 Fast HMM-Filler Computation using CL . . . . . . . . . . . . . . . . . . . . . . . . 205List of Figures
1.1 Illustration of a probabilistic index of a text image from the
Bentham Papers collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.2 Retrieval operation scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
1.3 Illustration of R–P curves and gAP results for three typical IR
systems working on text images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
1.4 Evaluating PrIx performance is based on user-produced GT
reference transcripts. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.1 Accurate word segmentation is not always possible to perform . . . . . 19
2.2 Textual context is a useful aid to identify words in handwritten text . 20
2.3 Different types of objects to retrieve after a user query . . . . . . . . . . . . 22
2.4 Illustration of the different query paradigms used for KWS . . . . . . . . 23
3.1 Example of marginalization bounding boxes . . . . . . . . . . . . . . . . . . . . . 34
3.2 Example of 2-D posteriorgrams for a text image and keyword . . . . . . 36
3.3 Correlation between exact and approximate RPs obtained for line
regions and words of the Bentham test set . . . . . . . . . . . . . . . . . . . . . . 40
3.4 Example of 1-D posteriorgram using a contextual recognizer . . . . . . . 42
3.5 An example of text image with two likely transcripts . . . . . . . . . . . . . . 44
3.6 Example of relevant image horizontal coordinates for a given query . 46
3.7 Heat map representing the RP of the image columns . . . . . . . . . . . . . . 48
3.8 Example showing that multiple instances of the same keyword may
have overlapping segments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
3.9 Example of the relationship between position-independent and
position-dependent RPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
3.10 Diagram of the relationship among the different RPs for a fixed text
image and a keyword . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
3.11 PrIx workflow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
4.1 Page images from different collections used for HTR and KWS
experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
xxvxxvi List of Figures
4.2 Examples of skewed and slanted text lines . . . . . . . . . . . . . . . . . . . . . . 68
4.3 An example of a HMM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
4.4 Example of the HMM alignment for the letter “a” . . . . . . . . . . . . . . . . 72
4.5 Diagram of the mathematical model of an artificial neuron and an
illustration of a multilayer neural network . . . . . . . . . . . . . . . . . . . . . . . 76
4.6 Diagram of a two dimensional convolution operation . . . . . . . . . . . . . 77
4.7 Compact unrolled representation of a simple recurrent layer . . . . . . . 79
4.8 Unit diagrams of a simple recurrent layer and a LSTM layer . . . . . . . 79
4.9 Example of the probabilistic interpretation that the CTC makes of
the output of a CRNN, applied to a text image . . . . . . . . . . . . . . . . . . . 82
4.10 Coordinates in one-dimensional and two-dimensional input signals
of a multidimensional recurrent layer . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
4.11 Comparison of the outputs from a 2D-LSTM layer and a CNN
layer, both trained for a HTR task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
4.12 Diagram of the CRNN architecture proposed and used to model
the transcripts of a handwritten text line . . . . . . . . . . . . . . . . . . . . . . . . 86
4.13 An example of WFST . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
4.14 Example of WFST composition to model the full transcription
posterior 푃(푤 | 푥) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
4.15 Example of a lattice represented as a WFST and an equivalent
compact version . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
4.16 Example of edge-posterior graph normalization . . . . . . . . . . . . . . . . . . 98
4.17 Example of sentence-posterior graph normalization . . . . . . . . . . . . . . 99
5.1 Minimal Deterministic Automaton accepting all sequences
containing a particular word . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
5.2 Example of disambiguation of the word position associated to the
states of a lattice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
5.3 Illustrative example for Alg. 5.8 application . . . . . . . . . . . . . . . . . . . . . 122
5.4 Illustrative example for Alg. 5.13 applied on a small WFSA . . . . . . . . 130
5.4bis Illustrative example for Alg. 5.13 applied on a small WFSA (cont.) . . 131
5.5 Example of RP computation for a given regular expression . . . . . . . . 135
6.1 mAP and gAP evolution for increasing word 푛-gram order on the
IAM dataset. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
6.2 mAP and gAP evolution for increasing lexicon size, on the IAM
dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
6.3 Lexicon-free mAP and gAP evolution for increasing order of
character 푛-gram, on the IAM dataset . . . . . . . . . . . . . . . . . . . . . . . . . . 151
6.4 AP evolution with respect to the number of indexed segments per
line, on the IAM dataset. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
6.5 Lexicon-free mAP and gAP evolution on the IAM validation-set
with varying optical and prior scales (훾, 휂), using a character 8-gram 154
6.6 Lexicon-based mAP and gAP evolution on the IAM validation-set
with varying optical and prior scales (훾, 휂), using a word 3-gram . . . 155List of Figures xxvii
6.7 Lexicon-free mAP and gAP evolution on the GW validation-set
with varying optical and prior scales (훾, 휂), using a character 6-gram 155
6.8 AP evolution with respect to the number of lines used to train the
CRNN, on the IAM dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
6.9 Correlation between AP measures and Recognition Error Rates,
for the IAM test set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
6.10 mAP and gAP evolution for increasing order of the character
푛-gram, on the GW dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
6.11 AP evolution for increasing order of the character 푛-gram, on the
Parzival dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
6.12 R–P curves of HMMs, CRNNs and 1-best HTR transcripts, on the
Bentham and Plantas datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
6.13 gAP, mAP and R–P curves for six real indexing datasets: TSO,
Bentham, FCR, Chancery, Passau and Carabela collections . . . . 167
6.14 Example of spotted results in the ICFHR2014 H-KWS Competition . 176
7.1 An instance of the multi-variance problem of traditional
distance-based KWS and in a unit-normed space . . . . . . . . . . . . . . . . . 186
7.2 An instance of the multi-mode problem of traditional distance-based
KWS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
7.3 Work-flow diagram of the feature extraction process used to obtain
Zoning Aggregated Hypercolumn features . . . . . . . . . . . . . . . . . . . . . . . 189
7.4 gAP and mAP evolution for different values of the sharpness
hyperparameter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
7.5 PHOC example representation and percentage of words that share
the same PHOC representation in IAM and GW datasets . . . . . . . . . . 192
7.6 “Filler” and “keyword” model schemes used in the HMM-Filler
approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
7.7 Example of CL and posteriorgram produced by the decoding of
line image rendering the text “to be for” . . . . . . . . . . . . . . . . . . . . . . . . 204
8.1 Scatter plots and histograms of lower-upper bounds of Boolean
AND/OR word pair combinations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
8.2 R–P curve for Vorau-235 dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220
8.3 Example of geometric reasoning for the column-wise multi-word
structured query: “⟨NAMEN VERSTORBENEN, WOLF⟩”. . . . . . . . . . . . . . 221
8.4 R–P curve for Passau dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
8.5 Image region from a document of the FCR collection . . . . . . . . . . . . . 224
8.6 Examples of hyphenated words with different hyphenation symbols . 224
9.1 Plaintext Zipf curve of Bentham test-set GT transcripts . . . . . . . . . . . 236
9.2 Comparing the Zipf curves and lexicon sizes computed from
Bentham’s GT transcripts and from corresponding PrIxs . . . . . . . . . 237
9.3 Zipf curves of PAS, TSO, CAR and FCR computed for their GT
transcripts and estimated using their PrIxs . . . . . . . . . . . . . . . . . . . . . . 238xxviii List of Figures
9.4 Example of PrIx with tagged entries . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
9.5 Statistics of “Reason to travel” and “Jobs”. . . . . . . . . . . . . . . . . . . . . . . 241
9.6 Real example of visa information statistics estimated using PrIxs . . . 242
9.7 Age histogram for persons registered in the visa record collection,
estimated using the PrIxs of the visa images . . . . . . . . . . . . . . . . . . . . 242
9.8 Leaving-one-out classification error rate on two books with three
threshold-less MLP models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
10.1 Diagram of a PrIx-based Search and Retrieval System . . . . . . . . . . . . 255
10.2 Diagram of PrIx Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
10.3 Diagram of the PrIx Ingestion component . . . . . . . . . . . . . . . . . . . . . . 257
10.4 Diagram of PrIx search engine and user interface . . . . . . . . . . . . . . . . 258
10.5 Client–server architecture used by the large-scale demonstrators . . . . 259
10.6 Representation of the hierarchical index used in the large-scale
demonstrators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260
10.7 Search engine response time as a function of the number of
occurrences retrieved . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
10.8 Web client graphical interface at root of the hierarchy . . . . . . . . . . . . . 262
10.9 Web client at box (book) level of the hierarchy . . . . . . . . . . . . . . . . . . . 263
10.10 Web client at page level of the hierarchy . . . . . . . . . . . . . . . . . . . . . . . . 263
10.11 A view of the Chancery PrIx demonstrator showing a result for a
bilingual Boolean and word-sequence query . . . . . . . . . . . . . . . . . . . . . 265
10.12 A view of the TSO PrIx demonstrator showing a result for a
Boolean proximity-AND query . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
10.13 Results for a query using wildcard . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270
10.14 Two views of the FCR PrIx demonstrator showing results for a
given query . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272
10.15 PrIx-estimated Zipf curves of the complete collections Chancery,
Carabela, TSO, FCR and Bentham . . . . . . . . . . . . . . . . . . . . . . . . . . 274
B.1 An illustrative example of a WFST . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301
B.2 Example of the composition of two WFSTs . . . . . . . . . . . . . . . . . . . . . 304
B.3 Example of a cyclic WFST which admits applying the shortest
distance algorithm both in the Tropical and Log semirings. . . . . . . . . 304
B.4 Original WFST and two determinized versions operated on the
Tropical and Real semirings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306
C.1 Examples of a few pages and segmented lines extracted from the
IAM dataset. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310
C.2 Examples of Bentham Papers page images. . . . . . . . . . . . . . . . . . . . . 313
C.3 Examples of pages and segmented lines from the BEN1 Bentham
dataset. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314
C.4 Examples of the query images used in the ICFHR-2014
Competition on KWS. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316List of Figures xxix
C.5 Examples of page images extracted from the George Washington
dataset (GW). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 320
C.6 Examples of a few normalized and binarized line images used from
the line-level partition of the George Washington database. . . . . . . 320
C.7 Examples of pages and a couple of segmented (binarized and
normalized) text lines from the Parzival dataset. . . . . . . . . . . . . . . . . 322
C.8 Examples of page images and a close-up of some text lines from
the Plantas dataset (PLA). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324
C.9 Examples of page images of the Passau dataset (PAS). . . . . . . . . . . . 326
C.10 Examples of page images from the Chancery dataset (CHA) . . . . . . 329
C.11 Examples of TSO images. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331
C.12 Random examples of Carabela page images . . . . . . . . . . . . . . . . . . . 333
C.13 Examples of important difficulties exhibited by Carabela images . . 333
C.14 Examples of Finnish Court Records (FCR) dataset images and a
close-up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335
C.15 Two FCR image examples showing bounding boxes of HwFs . . . . . . 337
C.16 Examples of page images from VORAU-253 sheet music
manuscript. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338
C.17 Geometry-wise annotation of music symbols . . . . . . . . . . . . . . . . . . . . 339
C.18 Page image examples from JMDB 4949 and JMBD 4950 books . . . . 340
C.19 Example of a four-page deed of class Risk (RI) from the
JMBB-4949 book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341List of Tables
1.1 Loss matrix for the binary decision problem involved in KWS . . . . . . 8
1.2 Comparison between HTR and PrIx . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
3.1 Likely segments where the word “all” is written in Fig. 3.7 . . . . . . . . . . 49
3.2 Example highlighting the differences between the segment RPs . . . . . 50
3.3 Example highlighting the differences between ordinal RPs . . . . . . . . . 52
3.4 Example of expected segment boundaries computed from ordinal
position RPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
3.5 Example of expected ordinal positions computed from segment RPs . 57
4.1 Operations in Real and Tropical semirings used in WFSTs . . . . . . . . . 90
5.1 Example of RP computation for a multi-word query . . . . . . . . . . . . . . . 136
6.1 Main dataset features of: IAM, PAR, GW, BEN1, BEN2, BEN3 and
PLA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
6.2 Architecture of the CRNN used in the IAM experiments. . . . . . . . . . . 143
6.3 Architecture of the CRNN used in the BEN1 experiments . . . . . . . . . . 144
6.4 Interpolated gAP obtained on BEN1 dataset for various
posteriorgram-based approximations to the RP . . . . . . . . . . . . . . . . . . . 144
6.5 Lexicon-based/free APs on the IAM dataset for different RPs
evaluated in a line-level setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
6.6 Time needed by different algorithms to build the PrIx on the IAM
dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
6.7 Evolution of the total index size and indexing time for increasing
maximum number of spots per line, on the IAM dataset . . . . . . . . . . . 153
6.8 Architecture of the CRNN used in the GW experiments. . . . . . . . . . . . 159
6.9 Architecture of the CRNN used in the Parzival experiments . . . . . . . 160
6.10 AP results achieved by different QbS, line-level KWS approaches
on the IAM, GW and PAR datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
xxxixxxii List of Tables
6.11 Comparison of gAP results between using HMM and CRNN for
PrIxs on the BEN1 and PLA datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
6.12 gAP and indexing density for PLA and BEN1, using HMM and RNN 165
6.13 Main dataset features of: TSO, BEN4, FCR, PAS and CAR . . . . . . . . 166
6.14 Architecture of the CRNN used in the BEN4 experiments . . . . . . . . . . 166
6.15 Architecture of the CRNN used in the Passau experiments. . . . . . . . . 168
6.16 Architecture of the CRNN used in the TSO experiments. . . . . . . . . . . 169
6.17 Architecture of the CRNN used in the Carabela experiments. . . . . . 170
6.18 Comparison of various systems in the ICDAR2015 Competition on
KWS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172
6.19 Architecture of the CRNN used in ICFHR2014 Handwriting KWS
Competition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
6.20 Comparison of multiple systems and measures in the ICFHR2014
H-KWS Competition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
7.1 mAP and gAP on the GW dataset for different ranking strategies
based on the features extracted in [31] . . . . . . . . . . . . . . . . . . . . . . . . . . 191
7.2 Comparison of word-segmentation-based QbE KWS performance
achieved by different PHOC-based methods on the GW dataset . . . . . 196
7.3 Summary of the KWS results on the word-segmented GW dataset
for different approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
7.4 Line-level gAP results on IAM using HMM-GMM, and different
character 푛-grams and approximations to the RP. . . . . . . . . . . . . . . . . . 202
7.5 Preprocessing and average query times and total indexing times, for
classical and CL-based HMM-Filler KWS on the IAM dataset . . . . . . 206
7.6 Comparative gAP results between BLSTM-CTC KWS and the
probabilistic interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209
8.1 gAP for single-word and Boolean AND an OR queries on the BEN1
dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
8.2 Architecture of the CRNN used for optical modeling of sheet music
staves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220
8.3 AP figures obtained using Plain and HyW PrIxs for the query sets
AllWords and MaybeHyph on the FCR test set . . . . . . . . . . . . . . . . . 227
8.4 Exact, approximate- and wildcard-spelling retrieval gAP and mAP
achieved for the two query sets Q1 and Q2. . . . . . . . . . . . . . . . . . . . . . . 231
9.1 Running words and vocabulary sizes estimated from different PrIxs
of datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
9.2 Several statistics of JMBD 4949 and JMBD 4950 books . . . . . . . . . . . 248
9.3 Classification error rate of threshold-less methods . . . . . . . . . . . . . . . . 251
9.4 OSC classification + rejection error rate using PrIx and 2 048 words . 252
9.5 Rejection performance for bMLP-2 OSC with PrIx and 2 048 words . 253List of Tables xxxiii
10.1 Public URLs to access to the PrIx demonstrator for the manuscript
series “Tresor des Chartes ´ ” and corresponding statistics . . . . . . . . . . . 265
10.2 Public URL of the “Teatro del Siglo de Oro” PrIx demonstrator and
corresponding statistics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
10.3 Public URL of the Bentham demonstrator and corresponding PrIx
statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268
10.4 Public URL of the Carabela PrIx demonstrator and corresponding
statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
10.5 Public URL of the FCR PrIx demonstrator and corresponding
statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272
10.6 Summary of main features of the PrIx demonstrators . . . . . . . . . . . . . . 273
A.1 Example illustrating the calculation of the Global and Mean
Average Precision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296
B.1 Common semirings used in WFSTs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301
C.1 Statistics of the IAM dataset and partition used in the experiments. . . 311
C.2 Statistics of the queries used in the IAM dataset.. . . . . . . . . . . . . . . . . . 311
C.3 Statistics of the external corpora used in IAM experiments. . . . . . . . . 312
C.4 Statistics of the Bentham BEN1 dataset, as proposed in
ICFHR-2014 HTRtS and used in our experiments. . . . . . . . . . . . . . . . . 313
C.5 Queries for PrIx experiments on the BEN1 Bentham dataset . . . . . . . 314
C.6 Statistics of the query pools generated for page-level multi-word
experiments on the BEN1 Bentham dataset . . . . . . . . . . . . . . . . . . . . . 315
C.7 Statistics of the Bentham partition from the ICFHR-2014
Competition on KWS as used in our experiments. . . . . . . . . . . . . . . . . . 316
C.8 Keywords of the ICFHR-2014 KWS competition . . . . . . . . . . . . . . . . . 316
C.9 Statistics of the Bentham partition from the ICDAR-2015
Competition on KWS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318
C.10 Basic statistics of the relatively large BEN4 Bentham dataset . . . . . . 319
C.11 Statistics of the George Washington dataset (GW) . . . . . . . . . . . . . . 320
C.12 Statistics of the queries used in the George Washington dataset
(GW). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
C.13 Statistics of the query set used on the word-level experiments with
the George Washington dataset (GW). . . . . . . . . . . . . . . . . . . . . . . . . 321
C.14 Statistics of the Parzival (PAR) partition used in the experiments. . . 323
C.15 Statistics of the query set used in the Parzival database (PAR). . . . . . 323
C.16 Statistics of the partition of Plantas dataset . . . . . . . . . . . . . . . . . . . . . 325
C.17 Statistics of the query set used in the test partition of the Plantas
database. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325
C.18 Main forms of character transliteration applied to the PAS dataset. . . 327
C.19 Statistics of the Passau experimental dataset (PAS) . . . . . . . . . . . . . . . 327
C.20 Basic statistics and partition of the Chancery dataset (CHA) . . . . . . . 329xxxiv List of Tables
C.21 Basic statistics of the TSO experimental datasets and the
corresponding external text corpus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331
C.22 Basic statistics of the batches used to train and test the Carabela
statistical models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 334
C.23 Basic statistics of the Finnish Court Records (FCR) experimental
dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336
C.24 Statistics of the hyphenated lines and hyphenated word (prefix or
suffix) fragments of the FCR dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337
C.25 Statistics of the Vorau-253 dataset and partitions used in the
experiments. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339
C.26 Number of documents and page images for JMBD 4949 and
JMBD 4950: per class, per document & class, and totals. . . . . . . . . . . 342Chapter 1
Introduction
Abstract After an introduction to the motivation and background of the topics
considered in this book, the present chapter reviews the most important concepts of
the scientific areas upon which the rest of the book stands on. This includes Informa￾tion Retrieval (IR), Pattern Recognition (PR) and Statistical Decision Theory. The
chapter includes also a complete account of standard IR metrics, used in this book
to assess the performance of the proposed Probabilistic Indexing models and meth￾ods. We provide the general equations, as well algorithmic details that are seldom
found in the literature. Probabilistic Indexing is closely related with Handwritten
Text Recognition, which is also reviewed here, along with a comparison of the most
important similarities and differences between both technologies.
1.1 Motivation and Background
During thousands of years the human kind used handwriting to preserve and share
knowledge. With the invention of the printing press, by Johannes Gutenberg (circa
1439), the printing press allowed an incredible acceleration in the distribution of
information, and made it possible that segments of the population that had never
had access before, could start to gain it [23]. In the current digital era, with the
usage of computers and digital formats, information can be stored in a cheaper and
more convenient way than ever before. In addition, any person around the world with
access to a computer with Internet connection, can retrieve any piece of information,
even if it is stored anywhere else in the globe. This has the potential to bring a
true democratization of human knowledge, that was started with the invention of the
printing press in the 15th century.
As a matter of fact, digitization works carried out in the last decades by archives
and libraries world wide have produced massive quantities high resolution images
of historical manuscripts and early printed documents [15, 9, 19, 25]. Billions of
text images have been produced through these efforts, and this is only a minuscule
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 1
A. H. Toselli et al., Probabilistic Indexing for Information Search and Retrieval 
in Large Collections of Handwritten Text Images, The Information Retrieval Series 49, 
https://doi.org/10.1007/978-3-031-55389-9_1
 2 1 Introduction
part of the amount of manuscripts which are still waiting to be digitized. The aim
of manuscript digitization is not only to improve preservation, but also to make the
written documents easily accessible to interested scholars and general public.
However, access to the real wealth of these images, namely, their textual contents,
remains elusive and there is a fast growing interest in automated methods which
allow the users to search for relevant textual information contained in handwritten
text images.
In order to use classical plain-text indexing and search Information Retrieval
(IR) methods [22, 1, 16, 43], a first step would be to convert the handwritten
text images into digital text. But the image collections for which text indexing is
highly in demand are so large that the cost of manually transcribing these images
is entirely prohibitive, even by means of crowd-sourcing approaches [5, 32]. An
obvious alternative to manual transcription is to rely on automatic Handwritten Text
Recognition (HTR) [2, 42, 34].
The development of HTR started during the 1950s and 1960s and, in the last 50
years, the field of HTR has improved significantly. However, current state of the art
HTR systems achieve good transcription results only if perfect layout, line detection
and reading order are taken for granted – as it is generally the case in published results.
But real historical scanned documents prove elusive even to the most advanced HTR
technologies and, despite the great recent advances in the field [14, 30, 26], fully
automatic transcripts of the kind of historical images of interest lack the accuracy
required to enable useful plain-text indexing and search. Another possibility would
be to use computer-assisted transcription methods [29], but so far these methods
can not provide the huge human-effort reductions needed to render semiautomatic
transcription of large image collections feasible [35].
HTR accuracy becomes low on real historical text images for many reasons, in￾cluding unpredictable, erratic layouts, lines with uneven interline spacing and highly
variable skew, ambiguous, inconsistent or capricious reading order of layout ele￾ments, etc. All or most of these difficulties boil down to the intrinsic uncertainty
which underlies the interpretation (by machines and humans alike) of image strokes
into actual textual elements or glyphs and how to combine these glyphs into charac￾ters, words, paragraphs and, in the end, whole textual documents.
Interestingly, most or all of these problems disappear or become much less severe
if, rather than to achieve accurate word-by-word image transcripts, the goal is to
determine how likely is that a given word is or is not written in some indexable
image region, such as a text line, a text block or paragraph, or just a full page
image. This goal statement places our textual IR problem close to the field known
as Keyword Spotting (KWS), which emerged as an alternative to HTR in the 1990s.
A comprehensive survey on KWS approaches for text images1 can be found in [13]
and Chapters 2 and 7 of this book provide detailed insights about the most relevant
of these approaches.
1 Among the works cited in this survey, it is worth noting that many recent developments are
inspired in one form or the other in earlier KWS works in the field of automatic speech recognition
(ASR), such as [8, 6, 17, 7, 4, 18, 33]. This is also the case of the work presented in this book.1.1 Motivation and Background 3
Generally speaking, KWS aims at determining locations on a text image or image
collection which are likely to contain instances of the query words, without explicitly
transcribing the image(s). This is also the aim of a framework we have developed
during the last decade, called Probabilistic Indexing (PrIx) [37, 27, 36, 40], which
is the main topic of the present book.
PrIx explicitly adopts the IR point of view to develop search and retrieval methods
for untranscribed handwritten text images. However, in contrast with traditional
KWS, rather than focusing on searching for specific, given “keywords”, all the likely
locations of all the words which are deemed likely keyword candidates are holistically
determined and indexed, along with the corresponding likelihoods.
As we will see, HTR and PrIx can advantageously share statistical models and
training methods. However, it is important to realize that HTR and PrIx are funda￾mentally different problems, even if both may rely on identical probability distribu￾tions and models. The HTR decision rule attempts to obtain the best sequence of
words or characters (transcript) for a given image. Therefore the result epitomizes
just the mode of the distribution; once a transcript has been obtained, the distribution
itself can be safely discarded. In contrast, PrIx decisions are delayed to the query
phase and, for each decision, (an approximation to) the full distribution is used.
In other words, rather than aiming at single, unique interpretations of the textual
elements of an image (as in HTR), PrIx explicitly embraces the interpretation uncer￾tainty above mentioned so that a proper interpretation can be disambiguated using
the required additional knowledge as/when available. This obviously explains why
proper KWS and PrIx can always achieve better overall search results than those
provided by naive KWS based on plain HTR transcripts.
An indexing and search system can be evaluated by measuring its precision and
recall performance for a given (large) set of keywords. Precision is high if most of the
retrieved results are correct while recall is high if most of the existing correct results
are retrieved. In the case of naive indexing, based on automatic HTR transcripts,
precision and recall are fixed numbers, which are obviously closely correlated with
the accuracy of the recognized transcripts. In contrast, for a PrIx system based on
the likelihood that a keyword is written in an image region, arbitrary precision-recall
tradeoffs can be achieved by setting a threshold to decide whether the likelihood is
high enough or not. We refer to this flexible search and retrieval framework as the
“Precision-Recall Tradeoff Model”. Under this model, it becomes even more clear
that proper KWS and PrIx has the opportunity of achieving better results than naive
KWS based on HTR transcripts, as previously discussed.
Contributions of this book to the state of the art in handwritten image indexing
and search include: First, a sound probabilistic framework is presented which helps
understanding the relations between PrIx and other classical, maybe not-probabilistic
statements of KWS, and provides probabilistic interpretations to many of these
approaches. Second, the development of this framework makes it clear that word
recognition implicitly or explicitly underlies any proper formulation of KWS, and
suggests that the same statistical models and training methods successfully used
for HTR can advantageously used also for PrIx. Third, experiments carried out
using this approach on datasets traditionally used for KWS benchmarking yield4 1 Introduction
results significantly better than those previously published for these datasets. Fourth,
PrIx results on new, larger handwritten text image datasets are reported, showing
the great potential of the methods proposed in this book for accurate indexing
and textual search in large collections of handwritten documents. Fifth, extensions
and applications of PrIx are described that go far beyond basic textual information
retrieval; this includes, text analytics, search for information in structured documents,
and textual content based document classification.
1.2 Information Retrieval
As previously commented, the framework proposed in this book mainly adopts the
point of view of Information Retrieval, which aims to build systems that enable
users to meet their information needs by finding relevant information among large
collections of generally unstructured documents [22]. The academic field of IR was
originated almost in parallel to computers, back in the 1940s and 1950s, mainly
in order to organize companies and libraries catalogs [31]. With the popularization
and spread of the Internet, the field gained significant importance and has been the
central business of various companies (such as Yahoo, Google or the extinct Altavista
or Lycos).
The term information needs is very fuzzy and can be ambiguous in many cases.
For instance, when a user searches for “Paris” while she is planning her holiday
trip, she probably wishes to find nice hotels, the best flight fares and interesting
attractions. However, when some local Frenchwoman searches for “Paris” on the
Internet, she is probably expecting to find other types of information (e.g. the web
address of the City Hall, hospitals, etc.). The task of the IR system is to provide the
users with relevant answers for their information need.
When we place traditional KWS systems and literature under the IR goggles, the
definition of relevant information may seem trivial: a document (i.e. full page, text
line, or a specific page location) is relevant if, and only if, it contains an instance
of the queried keyword. However, in this case, other sources of ambiguity arise: in
particular, the textual content of the documents is unknown. This contrasts with the
above web search examples, where the very definition of relevance is ambiguous,
but the content of the documents is not.
Probability is the standard way of measuring different sources of uncertainty.
Thus, the approaches presented in this book can be seen as instances of proba￾bilistic IR methods [22], applied to manage the uncertainty underlying the textual
information conveyed by images of handwritten (or printed) documents.
Finally, we want to emphasize the fact that aiming at building indexes from text
images was one of the foundational goals of KWS [20, 21], as well as interpreting
KWS as an instance of Information Retrieval. However, the indexes that we build
are more closely related to the ones used by traditional search engines, which make
them very easy to use and integrate with existing IR systems.1.3 Pattern Recognition 5
In order to better illustrate our IR-oriented indexing goals, Fig. 1.1 shows an
example of one of the probabilistic KWS indexes that we intend to build, for a given
(segment of a) page containing handwritten text.
200
150
100
 50
0 100 200 300 400 500 600
Term Probability Likely location
2 0.929 1 36 20 31 71.21.2
21 0.064 1 36 24 31 71.21.2
It 0.982 33 36 27 31 71.21.2
If 0.012 33 36 26 31 71.21.2
matters 0.998 76 35 104 31 71.21.2
matter 0.011 77 36 93 31 71.21.2
· · ·
some 0.832 570 198 78 31 71.21.2
soner 0.016 576 198 83 31 71.21.2
Fig. 1.1: Illustration of a probabilistic index of a text image (071 021 002, from the
Bentham Papers collection – http://www.prhlt.upv.es/htr/bentham). Each entry
contains an indexed term (word), its relevance probability and its likely location (bounding
box and image ID) in the collection.
In this example, the index contains the list of candidate words likely written in the
image, each with its location (bounding box) and the location relevance probability.
For example, the system find it very likely that the words “It”, “matters” and “some”
are written in the corresponding locations, although it is less certain about the latter
and even less for other alternatives such as “matter”, “If” “sooner”, etc. At the end of
this book, the reader will be able to understand the approaches and algorithms used
to build this kind of index and the theory supporting them.
1.3 Pattern Recognition
As discussed in the previous section, PrIx can be interpreted as a form of IR
where the content of the documents is uncertain. Recall that we are dealing with
scanned document images containing text. Thus, under a probabilistic formulation,
we can only guess which text is likely to actually be written in such images. Pattern
Recognition (PR) is a time-honored discipline that deals with the inherent uncertainty
of recognition of patterns from arbitrary data [10, 11, 3]. Machine Learning (ML) is6 1 Introduction
one of the fundamental pillars of PR, even though some modern works on ML may
forget its origins.
As the title of this book suggest, it focuses on probabilistic (or statistical) PR
models. In fact, although many PR methods or tasks may not involve the explicit
computation of any probability or distribution, all of them can be properly interpreted
in these terms. Specifically, PR uses statistical methods to (1) learn the parameters
of the models from given training data, and (2) decide (or predict) optimal actions
or outcomes from new (“evaluation” or “test”) data.
Over the years, PR has proven to be a phenomenal approach to find general rules
to solve problems from examples. In practice, once we have trained our model from
data, we need to check that it actually generalizes well to previously unseen data
(produces the correct answer). This is an essential step to ensure that our methods
are not just memorizing the data supplied during training. Different algorithms and
models have different generalization properties. The number of model parameters,
the dimensionality of the data, the independence assumptions taken into consid￾eration, etc. are just a few aspects that affect the ability of a PR method to be
successful [11, 3, 24].
Both ML and PR ultimately relay on the Statistical Decision Theory to develop
their learning and decision methods.
1.4 Decision Theory
Decision Theory (DT) provides the reasoning underlying systems that have to decide,
under uncertainty, optimal strategies with respect some utility or loss function. In
PR, training methods have to decide the optimal set of parameters according to
some optimization criterion (e.g. maximizing a likelihood function, or minimizing
some least squared error). Similarly, in the operational (evaluation or test) phase, the
system hast to decide the optimal values of the output variables, according to some
criterion (e.g. minimize the expected classification error).
DT can also be advantageously invoked for IR problems. Given a query and some
arbitrary object to retrieve, the system needs to decide whether that object is relevant
(and should therefore be retrieved) for a given query, or not. In the Appendix A, DT
is called in to prove that the PrIx framework proposed and developed in this book
is optimal for a broad set of criteria typically adopted in KWS and many other IR
applications.
In its simplest form, DT considers a binary classification problem where the two
classes are the two possible answers to a yes/not question. As applied specifically
to our text-image IR task, the following general question is considered, regarding
a given query word 푣 and a certain image or image region 푥: “Is 푣 written in 푥?”
Associated with this question there is another one which might appear more complex:
“What are the locations (if any) of word 푣 within 푥?”. However, this can often be
answered as a byproduct of solving the main question. The probabilistic framework
proposed and developed in this book deals with these questions.1.4 Decision Theory 7
First, to model the decision probability associated to the above binary classifica￾tion problem we need a binary random variable which, following common notation in
the IR field, will be denoted 푅 (after “relevant”). This entails a slight reformulation
of the original question as: “is the image 푥 relevant for the query 푣?”, considering
that 푥 is relevant for 푣 if at least one instance of 푣 is rendered in 푥.
Second, we propose another random variable 푋 over the set of image regions. A
value of 푋 (i.e., an arbitrary image region), will be denoted as 푥. At this point we do
not need to consider what are the possible sizes or shapes of image regions (a page,
a paragraph, a line, a word-sized bounding box, etc.) and, until we need to be more
specific, we will simply use the term “image” for a value of 푋.
Finally, we introduce the random variable, 푄, over the set of all possible user
queries. An arbitrary value of 푄 would generally be denoted as 푞. The proposed
framework properly admits arbitrary types of queries: from single words, to boolean
word combinations [36], or even “example image patches”, as in “query by example”
KWS [39] (see also Chapters 2 and 8). However, to keep the presentation simple,
we start considering only conventional string search, where queries are individual
keywords. Therefore, from now on, a generic value of 푄 will be denoted as 푣.
We can now introduce the relevance probability (RP) distribution:2
푃(푅 = 1 | 푋 = 푥, 푄 = 푣) ≡ 푃(푅 | 푥, 푣) (1.1)
which denotes the probability that 푥 is relevant for the keyword 푣. Since 푅 is binary,
the relevance probability can be obviously interpreted as the statistical expectation
that 푣 is written in 푥. On the other hand, by definition, 푃(푅 | 푥, 푣) is the posterior
probability underlying the following 2-class classification problem:
Given 푣, classify each image 푥 into one of two classes: (1.2)
• 1 : 푣 is (one of the words) written (somewhere) in 푥
• 0 : 푣 does not appear in 푥
Using a loss matrix 휆 to weight the cost of each 0/1 decision, the resulting
decision theoretic Bayes’ or minimum expected risk rule amounts to classify 푥 into
the class 1 (“yes”) iff [10]:
푃(푅 | 푥,푣) > 휏 , 휏 =
휆10 − 휆00
휆01 − 휆11 + 휆10 − 휆00
(1.3)
Table 1.1 shows in detail the meaning of each component of the 휆 matrix. According
to Eq. (1.3), in the two-class case, this matrix reduces to just a single scalar threshold
휏. Under the precision-recall tradeoff model, this is exactly the threshold to be
adjusted in order to achieve the required tradeoffs.
In the following chapters of this book we explain how to compute relevance
distributions for given images.
2 To simplify notation, from now on we will generally write 푃(푅 · · · ) and 푃(푎 · · · ), rather than
푃(푅 = 1 · · · ) and 푃(퐴 = 푎 · · · ), respectively, except when the full notation helps enhancing
clarity and/or avoiding ambiguity.8 1 Introduction
Table 1.1: IR loss matrix. For instance, 휆10 is the loss incurred by classifying as “Not
relevant” an object which actually is “Relevant”.
Decision
Not relevant Relevant
Truth Not relevant 휆00 휆01
Relevant 휆10 휆11
1.5 Handwritten Text Recognition
It has already been commented that perhaps the simplest approach for textual IR in
text images is to first use HTR to transcribe the images and then use of-the-shelf IR
tools on the noisy automatic transcripts. Informally speaking, HTR aims to provide
automatic ways to transcribe digitized text images into a symbolic format that would
allow modern treatment of textual matters such as editing, indexing, and retrieval.
For a formal treatment, HTR is stated as the following PR problem:
Given an image region 푥, obtain a word sequence ˆ푤 such that:
푤ˆ = arg max
푤
푃(푤 | 푥) (1.4)
From a statistical DT point of view, the underlying loss function is 휆푤 푤′ = 0 iff
푤 = 푤
′
and, therefore, Eq. (1.4) is a minimum expected risk rule which minimizes
the statistical expectation of whole transcription error.
As will be discussed in detail in Chapter 4, modern approaches to HTR are based
on optical models which deal with how image strokes can be interpreted in terms of
text elements or glyphs, such as characters, and language models, which account for
how the text elements can be combined to form words and sentences. Both types of
models are learned from training examples.
The optimization of Eq. (1.4) has proven to be a computational hard problem and,
actually, no algorithm exists which solves it exactly. However good approximations
are available to solve a similar (albeit apparently more difficult) problem in which ˆ푤
is obtained from a pair (푤, ˆ 푎ˆ) of a word sequence and an alignment which maximize
the joint posterior 푃(푤, 푎 | 푥). Formally:
(푤, ˆ 푎ˆ) = arg max
푤,푎
푃(푤, 푎 | 푥) (1.5)
An alignment 푎 = 푎1, . . . , 푎푛, 푎푛+1 of a word sequence 푤 = 푤1, . . . , 푤푛, with a (line)
image region 푥, is a sequence of (horizontal) coordinates of 푥 which determine how
the image is segmented into words of 푤.
There are various algorithms with provide exact or approximate solutions to this
optimization problem. The best known and the one most effective and efficient is
the Viterbi algorithm, which provides an exact solution to Eq. (1.5), as well as very
fast approximations by means of an accelerating technique known as Viterbi beam
search [29].1.6 Assessing Indexing and Search Performance 9
According to DT, the ultimate goal of Eq. (1.4) (and Eq. (1.5) alike) is to achieve
a whole sentence HTR transcription error rate as low as possible. But, in practice,
HTR results are typically evaluated using more fine-grained measures. Most popular
metrics are Word Error Rate (WER) and Character Error Rate (CER), which are
defined as the number of insertion, deletion and substitution word/character errors
divided by the number of words/characters in the reference transcript.
WER and CER are considered very adequate to asses HTR systems where text￾line image regions are the basic recognition units. However, for recently proposed
end-to-end HTR approaches, where text line detection and recognition are done
simultaneously, other evaluation measures –such as those based on Bag-of-Words and
the Hungarian algorithm proposed in [41]– are required which are more informative
about the types of errors; that is, whether they are due to reading order issues or
recognition itself.
1.6 Assessing Indexing and Search Performance
The indexing and retrieval effectiveness of IR systems is generally assessed using
the standard measures of recall and interpolated precision [22].
In the PrIx framework, for a given query and relevance threshold, recall is the
ratio of relevant image regions (lines) correctly retrieved by the system (often called
hits), with respect to the total number of relevant regions existing in the image test
set. Precision, on the other hand, is the ratio of hits with respect to the total number of
(correctly or incorrectly) retrieved regions. Precision is high if most of the retrieved
results are correct while recall is high if most of the existing correct results are
retrieved.
The adequateness of these and other related measures to evaluate PrIx-based
systems is thoroughly discussed in Appendix A. In this section we review the most
important concepts and equations, along with the algorithmic details needed for
efficient computing of these metrics.
Let Q be a set of (word) queries and 휏 be a relevance threshold. The recall,
휌(푞, 휏), and the raw (non-interpolated) precision, 휋
′
(푞, 휏), for a given query 푞 ∈ Q
are defined as:
휌(푞, 휏) =
ℎ(푞, 휏)
푟(푞)
, 휋(푞, 휏) =
ℎ(푞, 휏)
푑(푞, 휏)
(1.6)
Here 푟(푞) is the number of test image regions which are relevant for 푞, according
to the ground-truth (GT), 푑(푞, 휏) is the number of regions retrieved or detected by
the system with relevance threshold 휏 and ℎ(푞, 휏) is the number of detected regions
which are actually relevant (also called hits). See Fig. 1.2.
The interrelated trade-off between recall and precision can be conveniently dis￾played as the so-called recall-precision (R–P) curve, 휋푞 (휌) [12]. Any IR system
should allow users to (more or less explicitly) regulate 휏 in order to choose the
R–P operating point which is most appropriate in each query. Good systems should10 1 Introduction
C
H (푞, 휏)
R (푞)
D (푞, 휏)
Fig. 1.2: Retrieval operation scheme. C is the whole collection or set of elements con￾sidered. For a given query, 푞, R (푞) is the set of 푟 (푞) elements that, according to the
reference GT, are relevant to 푞. For a certain relevance threshold 휏, D (푞, 휏) is the set of
푑(푞, 휏) elements detected by the system. Finally, H (푞, 휏) = R (푞) ∩ D (푞, 휏) is the set
of correctly detected elements or hits, with |H (푞, 휏) | = ℎ(푞, 휏).
achieve both high precision and high recall for a wide range of values of 휏. A
commonly accepted scalar measure which meets this intuition is the area under the
R–P curve, here denoted as 휋푞 and called (raw) average precision (AP) [44, 28]. In
addition, to consider all the queries in Q, the (raw) mean average precision (mAP,
denoted as 휋) is used:
휋푞 =
∫ 1
0
휋푞 (휌)푑휌 , 휋 =
1
|Q|
Õ
푞∈ Q
휋푞 (mAP) (1.7)
Obviously, the mAP is undefined if ∃ 푞 ∈ Q for which 휋푞 is undefined, which
happens if 푟(푞) = 0, that is, if no test-set image region is relevant for 푞. On the
other hand, Eq. (1.7) equally weights all the queries, thereby ignoring the different
amounts of relevant regions for different queries.
To circumvent both of these issues, a global averaging scheme can be adopted by
computing the total number of test image regions which are relevant for all 푞 ∈ Q,
the total number of regions detected with relevance threshold 휏 and the total number
of hits, respectively, as:
푟 =
Õ
푞∈ Q
푟(푞) , 푑(휏) =
Õ
푞∈ Q
푑(푞, 휏) , ℎ(휏) =
Õ
푞∈ Q
ℎ(푞, 휏) (1.8)
Then the overall recall and raw precision, and the (often preferred) global average
precision, 휋 (referred to as gAP or simply as AP), are defined as:
휌(휏) =
ℎ(휏)
푟
, 휋(휏) =
ℎ(휏)
푑(휏)
, 휋 =
∫ 1
0
휋(휌)푑휌 (gAP) (1.9)
The integral in Eq. (1.9), must be computed numerically. For large datasets and
many queries in Q, the cost of this computation can become excessive – as large as
푂(푁
2
), where 푁 = |Q×C|. However, it can be greatly accelerated as follows [44, 38].
First, for each 푞 ∈ Q and each 푥 ∈ C, compute the RP 푃(푅 | 푥, 푞) (Eq. 1.1) and sort1.6 Assessing Indexing and Search Performance 11
the list of 푁 pairs (푥, 푞) in decreasing order of RP. Let (푥푘, 푞푘) be the k-th pair of
this list and let 푑푘≡푘 and ℎ푘 be the number of regions detected and the number of
hits accumulated up to the 푘-th entry of the sorted list. Let finally 휋푘≡ ℎ푘/푘 be the
precision up to the 푘-th entry in the sorted list. Then, using the simplest rectangular
or Rimanian numerical integration, the gAP is given by:
휋 =
Õ
푁
푘=1
휋푘 (Δ휌)푘 =
1
푟
Õ
푁
푘=1
휋푘푔(푘) (1.10)
where 푔(푘) ∈ {0, 1} is the true relevance (given by the GT) of 푥푘 with respect to 푞푘.
The values of 휋푘, can be easily computed recursively as:
휋0
def
= 1 , 휋푘 =
1
푘
￾
(푘 − 1)휋푘−1 + 푔(푘)

, 1 ≤ 푘 ≤ 푁 (1.11)
The cost of this computation is now dominated by the 푂(푁log푁) complexity of the
sort step. This cost remains unchanged if the often preferred trapezoidal numerical
integration is used. In this case, Eq. (1.10) becomes:
휋 =
1
푟
Õ
푁
푘=1
휋푘−1 + 휋푘
2
푔(푘) (1.12)
Even with the global averaging scheme, raw precision can still be ill-defined
in some extreme cases and, moreover, raw R–P curves can present an undesired
distinctive saw-tooth shape [12]. Both of these issues are avoided by the so-called
interpolated precision, defined as:
휋
′
(휌) = max
휌
′
:휌
′≥휌
휋(휌
′
) (1.13)
Intuitive arguments in favor of 휋
′
(휌), which is often adopted in the literature, are
discussed in [22]. The same interpolation scheme can be applied to a single query
푞, resulting in the interpolated R–P curve 휋
′
푞
(휌). Then, the interpolated versions
of mAP and gAP are straightforwardly defined using 휋
′
푞
(휌) and 휋
′
(휌), rather than
휋푞 (휌) and 휋(휌), in Eqs. (1.7) and (1.9), respectively.
The adoption of the interpolated precision 휋
′
instead of the raw precision 휋,
prevents directly using Eqs. (1.10) and (1.12) for efficient computation. In this case,
it becomes necessary to explicitly compute the precision and recall up to the 푘-th
entry 휋푘, during a first, forward pass over the sorted list. Then the list is visited
backwards, applying Eq. (1.13) to compute the interpolated precision up the 푘-th
entry, denoted 휋
′
푘
. Finally the interpolated versions of Eqs. (1.10) and (1.12) are
computed by simply changing 휋푘 with 휋
′
푘
in these equations.
Eqs. (1.10,1.11,1.12) all relay on the values of 푘 that are assigned to each pair
(푥, 푞) by sorting these pairs according to their RP, 푃(푅 | 푥, 푞). So, a practical
problem arises as to what to do in case of RP ties. This problem can be very
relevant even for moderately large data and query sets, and more so for high R12 1 Introduction
values. Specifically, most of the highest RP values tend to be 1.0.3 Clearly, some
of these duplicated high-RP entries can be not-relevant (i.e., 푔(푘) = 0). Therefore,
depending on exactly where these entries are placed in the sorted list, the resulting
gAP values can be significantly higher or lower.
To fairly circumvent this problem, ties can be avoided by sorting with duplicate￾removing. But then, several changes are needed to Eqs. (1.10–1.12). To start with, the
upper limit 푁 of the sums in Eqs. (1.10,1.12) and the span of the recurrence (1.11) is
now 푁
′≤ 푁 and 푔(푘)∈{0, 1} is now 푔
′
(푘
′
)∈ N, re-defined as the total number of GT
1’s in the 푘
′
-th collapsed block of entries with the same RP value. Finally, Eq. (1.11)
becomes:
휋0
def
= 1 , 휋′
푘
=
푑푘
′−1휋푘
′−1 + 푔
′
(푘
′
)
푑푘
′−1 + 푚(푘
′
)
, 1 ≤ 푘
′ ≤ 푁
′
(1.14)
where 푚(푘
′
) is the number of duplicated entries collapsed in the 푘
′
-th block and, as
before, 푑푘
′ , is the accumulated number of detected regions up to the 푘
′
-th entry of
the sorted list. Note that now, 푑푘
′ is no longer just equal to 푘
′
, but it can be simply
computed incrementally as: 푑0
def
= 0, 푑푘
′ = 푑푘
′−1 + 푚(푘
′
), 1≤ 푘
′≤ 푁
′
.
The use of interpolated precision is particularly necessary for fair evaluation of IR
results of the naive 1-best KWS approach, where RP can only be 1 or 0, independently
of 휏. Therefore, in a raw R–P curve, just one R–P point, (휌0, 휋0), could be defined and
the resulting raw gAP would be 0, disallowing comparison with other approaches. In
contrast, the interpolated precision curve becomes 휋
′
(휌) =휋0 if 0≤ 휌 ≤ 휌0, 휋
′
(휌) = 0
otherwise, with a resulting interpolated gAP: 휋
′ = 휋0 · 휌0.
Fig. 1.3 illustrates the computation of a R–P curve and the corresponding inter￾polated gAP values for a typical IR system working on text images. If perfectly
correct text were indexed, we would get a single, “ideal” point with 휌0 = 휋0 = 1 and
gAP = 1.0. If automatic (typically noisy) HTR transcripts are naively indexed just
as plaintext, precision and recall are also fixed values, albeit not “ideal” (perhaps
something like 휌0 = 0.75, 휋0 = 0.8, with gAP = 0.6).
In contrast, PrIx allows for arbitrary precision-recall tradeoffs by setting a thresh￾old on the system confidence (relevance probability). This flexible precision-recall
tradeoff model obviously allows for better search and retrieval performance than
naive plaintext searching on automatic noisy transcripts.
It is worth to recall that precision/recall assessment requires GT reference data –
denoted 푔(·) in the above equations. In our case, such GT can be straightforwardly
derived from the same GT needed to evaluate HTR performance; namely, the ref￾erence transcripts of a selected test set of images. Fig. 1.4 illustrates the complete
assessment process and the user involvement in this process.
3 Up to reasonably rounding RP to, say a 6 decimal digits.1.6 Assessing Indexing and Search Performance 13
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
High relevance threshold τ
Low relevance threshold τ
τ
Precision (π)
Recall (ρ)
Perfect (gAP=1.0)
HTR Transcript (gAP=0.6)
PrIx (gAP=0.8)
Fig. 1.3: Illustration of R–P curves and gAP results for three typical IR systems working on
text images: an ideal system based on perfect transcripts, another based on plain indexing
of HTR transcripts and a PrIx system.
Selected test
 text Images
 Probabilistic
Indexing (PrIx)
abcd xyz abc xyz
abcd xyz abc xyz
abcd xyz abc xyz
abcd xyz abc xyz
abcd xyz abc xyz
abcd xyz abc xyz
abcd xyz abc xyz
abcd xyz abc xyz
abcd xyz abc xyz
 xyz abc xyz
abcd xyz abc xyz
abcd xyz abc xyz
abcd xyz abc xyz
abcd xyz abc xyz
abcd xyz abc xyz
abcd xyz abc xyz
abcd xyz abc xyz
abcd xyz abc xyz
abcd xyz abc xyz
 xyz abc xyz
abcd xyz abc xyz
abcd xyz abc xyz
abcd xyz abc xyz
abcd xyz abc xyz
abcd xyz abc xyz
abcd xyz abc xyz
abcd xyz abc xyz
abcd xyz abc xyz
abcd xyz abc xyz
 xyz abc xyz
Evaluation
Reference
Transcripts
Page Image
 PrIxs
Precision/Recall and
Average Precision
results
Fig. 1.4: Evaluating PrIx performance is based on user-produced GT reference transcripts.14 1 Introduction
1.7 Handwritten Text Recognition and Probabilistic Indexing
As previously discussed, PrIx and HTR are related technologies which can advan￾tageously share concepts and models. In fact, it can be empirically seen that when
PrIx and HTR share the same models, the conventional evaluation measures of PrIx
and HTR (gAP and WER) correlate graciously (see Sec. 6.6 of Chapter 6).
Nevertheless, it is important to highlight that HTR and PrIx are fundamentally
different technologies, with substantially different application goals. Table 1.2 sum￾marizes the most important similarities and differences between these technologies.
Table 1.2: Probabilistic indexes are not transcripts.
Automatic Transcription (HTR) Probabilistic Indexing (PrIx)
Generally comes after Layout Analysis and
Reading Order determination
Is generally agnostic to Layout structure and
Reading Order
Typically needs carefully detected lines Line detection helps, but only if accurate
The output is a best, unique (frail!) text
interpretation of the given image according to
the models used
For the same models, the output is a robust
probability distribution of words with their
positions in the images
The output is aimed to be in reading order (but
this is seldom achieved)
In general, Probabilistic Indexing is
reading-order agnostic
Provides plaintext output. If accuracy is high,
it can be directly used in many applications
In its basic form, does not provide any text
output; only images marked with word-sized
bounding boxes
Usually yields only fixed and comparatively
low precision-recall performance for the given
trained models
Allows flexible, user-controlled
precision-recall tradeoffs and search
performance is generally much better for the
same trained modelsReferences 15
References
1. Bache, R., Ballie, M., Crestani, F.: The likelihood property in general retrieval operations.
Information Sciences 234, 97 – 111 (2013)
2. Bazzi, I., Schwartz, R., Makhoul, J.: An Omnifont Open-Vocabulary OCR System for English
and Arabic. IEEE Trans. on Pattern Analysis and Machine Intelligence 21(6), 495–504 (1999)
3. Bishop, C.: Pattern Recognition and Machine Learning. Springer-Verlag New York (2006)
4. Can, D., Saraclar, M.: Lattice indexing for spoken term detection. IEEE Transactions on Audio,
Speech, and Language Processing 19(8), 2338–2347 (2011)
5. Causer, T., Wallace, V.: Building a volunteer community: results and findings from Transcribe
Bentham. Digital Humanities Quarterly 6(2) (2012)
6. Chelba, C., Silva, J., Acero, A.: Soft indexing of speech content for search in spoken documents.
Computer Speech & Language 21(3), 458 – 478 (2007)
7. Chia, T.K., Sim, K.C., Li, H., Ng, H.T.: Statistical lattice-based spoken document retrieval.
ACM Trans. Inf. Syst. 28(1), 2:1–2:30 (2010)
8. Christiansen, R., Rushforth, C.: Detecting and locating key words in continuous speech using
linear predictive coding. IEEE Trans. on ASSP 25(5), 361–367 (1977)
9. D’Orazio, D.: Oxford and Vatican libraries to digitize 1.5 million pages of ancient texts. The
Verge (2012). URL https://www.theverge.com/2012/4/15/2950260/oxford-v
atican-libraries-digitize-1-5-million-pages-ancient-texts. Visited on
17-Jun-2018
10. Duda, R.O., Hart, P.E.: Pattern Classification and Scene Analysis. J. Wiley and Sons, (1973)
11. Duda, R.O., Hart, P.E., Stork, D.G.: Pattern Classification, 2nd edition edn. Wiley-Interscience,
New York, NY, USA (2000)
12. Egghe, L.: The measures precision, recall, fallout and miss as a function of the number of
retrieved documents and their mutual interrelations. Inf. Proces. & Management 44(2), 856–
876 (2008)
13. Giotis, A.P., Sfikas, G., Gatos, B., Nikou, C.: A survey of document image word spotting
techniques. Pattern Recognition 68, 310–332 (2017)
14. Graves, A., Liwicki, M., Fernandez, S., Bertolami, R., Bunke, H., Schmidhuber, J.: A novel ´
connectionist system for unconstrained handwrting recognition. IEEE Trans. on PAMI 31(5),
855–868 (2009)
15. Jimenez, C.: British Library books go digital. BBC News (2007). URL http://news.bbc
.co.uk/2/hi/technology/7018210.stm. Visited on 17-Jun-2018
16. Joung, Y.J., Yang, L.W.: On character-based index schemes for complex wildcard search in
peer-to-peer networks. Information Sciences 272, 209 – 222 (2014)
17. Kohler, J., Larson, M., Jong de, F., Kraaij, W., R.J.F, O. (eds.): Searching Spontaneous conver￾sational speech workshop, proc. of the ACM SIGIR workshop of the 31th Annual International
SIGIR conference. Centre for Telematics and Inf. Tech., Enschede, The Netherlands (2008)
18. Lee, L.s., Glass, J., Lee, H.y., Chan, C.a.: Spoken content retrieval—beyond cascading speech
recognition with text retrieval. IEEE/ACM Transactions on Audio, Speech, and Language
Processing 23(9), 1389–1420 (2015)
19. Madrigal, A.C.: Norway Decided to Digitize All the Norwegian Books. The Atlantic (2013).
URL https://www.theatlantic.com/technology/archive/2013/12/norway-dec
ided-to-digitize-all-the-norwegian-books/282008/. Visited on 16-Jun-2018
20. Manmatha, R., Han, C., Riseman, E.M.: Word spotting: a new approach to indexing hand￾writing. In: Proceedings CVPR IEEE Computer Society Conference on Computer Vision and
Pattern Recognition, pp. 631–637 (1996). DOI 10.1109/CVPR.1996.517139
21. Manmatha, R., Han, C., Riseman, E.M., Croft, W.B.: Indexing Handwriting Using Word
Matching. In: Proceedings of the First ACM International Conference on Digital Libraries,
DL ’96, pp. 151–159. ACM, New York, NY, USA (1996). DOI 10.1145/226931.226960
22. Manning, C.D., Raghavan, P., Sch ¨utze, H.: Introduction to Information Retrieval. Cambridge
University Press, New York, NY, USA (2008)16 1 Introduction
23. McLuhan, M.: The Gutenberg Galaxy: The Making of Typographic Man. University of Toronto
Press (1962)
24. Murphy, K.P.: Machine learning: a probabilistic perspective. The MIT Press, Cambridge, MA
(2012)
25. Paniagua, E.: As´ı se digitaliza la Biblioteca Nacional de Espana. El Pa ˜ ´ıs (2018). URL
https://retina.elpais.com/retina/2018/01/30/innovacion/1517327412_5156
02.html. Visited on 16-Jun-2018
26. Puigcerver, J.: Are Multidimensional Recurrent Layers Really Necessary for Handwritten
Text Recognition? In: 2017 14th IAPR International Conference on Document Analysis and
Recognition (ICDAR), vol. 01, pp. 67–72 (2017). DOI 10.1109/ICDAR.2017.20
27. Puigcerver, J.: A probabilistic formulation of keyword spotting. Ph.D. thesis, Universitat
Politecnica de Val ` encia (2018) `
28. Robertson, S.: A new interpretation of average precision. In: Proceedings of the 31st annual
international ACM SIGIR conference on Research and development in information retrieval,
SIGIR, pp. 689–690. ACM, New York, NY, USA (2008)
29. Romero, V., Toselli, A.H., Vidal, E.: Multimodal Interactive Handwritten Text Transcription.
Perception and Artif. Intell. (MPAI). World Scientific, (2012)
30. Sanchez, J.A., Romero, V., Toselli, A.H., Vidal, E.: ICFHR2016 Competition on Handwritten ´
Text Recognition on the READ Dataset. In: Proc. of 15th ICFHR, pp. 630–635 (2016)
31. Sanderson, M., Croft, W.B.: The History of Information Retrieval Research. Proceedings of the
IEEE 100(Special Centennial Issue), 1444–1451 (2012). DOI 10.1109/JPROC.2012.2189916
32. Shuttleworth, S.: Old weather: Citizen scientists in the 19th and 21st centuries. Science
Museum Group Journal 3(3) (2016)
33. Tabibian, S., Akbari, A., Nasersharif, B.: Discriminative keyword spotting using triphones
information and n-best search. Information Sciences 423, 157 – 171 (2018)
34. Toselli, A.H., Juan, A., Gonzalez, J., Salvador, I., Vidal, E., Casacuberta, F., Keysers, D., Ney, ´
H.: Integrated handwriting recognition and interpretation using finite-state models. Int. Journal
of Pattern Recognition and Artificial Intelligence 18(04), 519–539 (2004)
35. Toselli, A.H., Leiva, L.A., Bordes-Cabrera, I., Hernandez-Tornero, C., Bosch, V., Vidal, E.: ´
Transcribing a 17th-century botanical manuscript: Longitudinal evaluation of document layout
detection and interactive transcription. Digital Scholarship in the Humanities 33(1), 173–202
(2018). DOI 10.1093/llc/fqw064. URL http://dx.doi.org/10.1093/llc/fqw064
36. Toselli, A.H., Vidal, E., Puigcerver, J., Noya-Garc´ıa, E.: Probabilistic multi-word spotting in
handwritten text images. Pattern Analysis and Applications (2019)
37. Toselli, A.H., Vidal, E., Romero, V., Frinken, V.: HMM Word-Graph Based Keyword Spotting
in Handwritten Document Images. Information Sciences 370(C), 497–518 (2016). DOI
10.1016/j.ins.2016.07.063
38. Turpin, A., Scholer, F.: User performance versus precision measures for simple search tasks.
In: Proceedings of the 29th annual international ACM SIGIR conference on Research and
development in information retrieval, pp. 11–18 (2006)
39. Vidal, E., Toselli, A.H., Puigcerver, J.: High performance Query-by-Example keyword spotting
using Query-by-String techniques. In: 2015 13th International Conference on Document
Analysis and Recognition (ICDAR), pp. 741–745 (2015). DOI 10.1109/ICDAR.2015.7333860
40. Vidal, E., Toselli, A.H., Puigcerver, J.: Lexicon-based probabilistic indexing of handwritten
text images. Neural Computing and Applications pp. 1–20 (2023)
41. Vidal, E., Toselli, A.H., R´ıos-Vila, A., Calvo-Zaragoza, J.: End-to-end page-level assessment
of handwritten text recognition. Pattern Recognition p. 109695 (2023). DOI https://doi.org/
10.1016/j.patcog.2023.109695
42. Vinciarelli, A., Bunke, H., Bengio, S.: Offline Recognition of Unconstrained Handwritten Texts
Using HMMs and Statistical Language Models. IEEE Transactions on Pattern Analysis and
Machine Intelligence 26(6), 709–720 (2004). DOI 10.1109/TPAMI.2004.14
43. Wu, M.S.: Modeling query-document dependencies with topic language models for information
retrieval. Information Sciences 312, 1 – 12 (2015)
44. Zhu, M.: Recall, Precision and Average Precision. Working Paper 2004-09 Department of
Statistics & Actuarial Science - University of Waterloo (2004)Chapter 2
State Of The Art
Abstract As discussed in the previous chapter, the PrIx framework explicitly adopts
the IR point of view, but it draws from many concepts and methods developed over
the last 50 years in the field of KWS, first for speech signals and more recently for text
images. A comprehensive survey of these approaches can be consulted in [23] and a
recent review in [36]. This chapter overviews the taxonomy and the most important
state-of-the-art approaches to KWS for text images. Detailed insights about the most
interesting and/or relevant of these approaches will be provided in Chapter 7 of this
book. This chapter also reviews the works carried out so far on certain issues which
are very significant for PrIx (and KWS alike) but which are very seldom considered
in the KWS literature; namely querying text images for hyphenated, abbreviated
and/or multiple words. Finally, since PrIx shares optical and language models with
HTR, the state of the art in HTR is also briefly outlined.
2.1 The field of a Hundred Names
The scientific literature is flooded with works that chase the aims described earlier.
Nevertheless, depending on the authors’ background or community, different names
are used for tackling virtually the same problem. One remarkable example is the name
Spoken Term Detection (or STD), which is widely used by the speech processing
community [57, 48, 35, 13, 30, 38, 47]. Although the name Keyword Spotting (or just
KWS) has been used in the past by the speech processing community as well [56, 84],
the former name has been broadly adopted in the recent years.
The contributions from the speech community are very significant, since they
tackled the problem earlier than the researchers interested in historical handwritten
documents [33, 14, 43, 44, 32]. As a matter of fact, some of the popular strategies to
perform keyword spotting for handwritten documents were adopted from the speech
community (see [20], for example).
This should not come as a surprise to the reader, since the handwritten text
recognition community has benefited for a long time from the developments made
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 17
A. H. Toselli et al., Probabilistic Indexing for Information Search and Retrieval 
in Large Collections of Handwritten Text Images, The Information Retrieval Series 49, 
https://doi.org/10.1007/978-3-031-55389-9_2 
 18 2 State Of The Art
by their speech recognition colleagues: the use of Hidden Markov and Gaussian
Mixture Models was first used for speech recognition [31], and then adopted by text
recognition researchers [37], and the same occurred with modern artificial neural
network architectures, such as the Long-Short Term Memories (see [24] and [26]).
Sometimes, even within the same community, two different names are used to
refer to the same problem. For instance, in the handwritten documents community,
many researchers prefer the term Word Spotting [43, 20]. And, just to make the things
a bit more confusing, some researchers have used these names to refer to different
problems (e.g. [11]).
In this book, we will use the term KWS which is the most popular in the literature
nowadays. However, if the reader wants to investigate other works with the same
or very similar aims, she should carefully review the literature related to all these
topics:
• Keyword Spotting
• Spoken Term Detection (only for speech signals)
• Word Spotting
• Word Detection
• (Speech or text image) Search
• (Speech or text image) Retrieval
• (Speech or text image) Indexing
• (Speech or text image) Mining
2.2 Taxonomy of KWS approaches
Different KWS systems and publications can be classified using multiple criteria. In
this section we aim to present the different categories that can be employed to classify
a particular solution, which will be useful later to understand the assumptions and
limitations of different approaches.
2.2.1 Segmentation Assumptions
One of the most important practical distinctions between different Keyword Spotting
systems is related to the assumptions that each system makes regarding the segmen￾tation of the original document images. Collections of real handwritten document
images have a large variability, and certain assumptions may be reasonable in some
cases but not others. Any assumption that one system makes, is a real limitation if
that assumption does not hold in reality.2.2 Taxonomy of KWS approaches 19
2.2.1.1 Word Segmentation
Many KWS approaches in the literature assume that is possible to have an accurate
segmentation of the words present in the documents [43, 1, 2, 64, 69, 49]. Generally,
these approaches work directly with the cropped bounding boxes of the words in
the document, and the query introduced by the user, which can be either a string or
another image (see Sec. 2.2.3). Many research assume that this allows to simplify
the problem of KWS and helps to determine the best-case performance.
However, we can identify the following problems in these:
1. Manual word segmentation is not practical. This is quite obvious, since we
aim to automate the processing of handwritten documents. It does not seem
reasonable that one of the steps involved needs of human labor to be completed.
In addition, one has to realize that accurate word segmentation into bounding
boxes or bounding polygons is a very tedious task (and thus, expensive).
2. Automatic word segmentation is not good enough. Some authors argue that,
although manual word segmentation is not practical for obvious reasons, current
automatic word segmentation approaches are good enough to perform word
spotting. While this could be true for some academic data sets, actual data from
real collections of historical documents clearly contradicts this hypothesis. See
Fig. 2.1, for example.
(a) RIMES experimental dataset
(b) Alcaraz manuscript
Fig. 2.1: Accurate word segmentation is not always possible to perform. While isolated
words are clearly identifiable in image (a), even expert human paleographers would face
problems segmenting the individual words in image (b).
3. Working with isolated word images discards useful information. Notice that,
if we try to identify whether a given query keyword is written in a particular
cropped word region, we are assuming that the word contained in this particular
region is independent of other words in the document. This assumption is
obviously false, and it has been extensively shown in the literature that, using20 2 State Of The Art
textual context information can substantially improve the results [46, 19, 74].
As an example of the importance of the context, see Fig. 2.2. In theory, word
segmentation does not necessarily discard context information, but virtually all
works assuming word segmentation ignore it.
Fig. 2.2: Textual context is a useful aid to identify words in handwritten text. Although
the word “House” cannot be read in the image, any reader fairly formed in politics (or
statistics), can infer it from the typical use of the English language or, at least, she would
guess that words like “automobile” are very unlikely in this context.
2.2.1.2 Line Segmentation
An important subset of papers in KWS assume that accurate line-segmentation is
feasible in practice [20]. These works typically use the machinery developed for
Spoken Term Detection, since the 푥-axis in images (or 푦-axis for vertical-oriented
text) can be interpreted as the time-axis in speech.
Of course, manual line segmentation is not feasible for real applications, for
the same reasons as manual word segmentation. However, current automatic line
segmentation approaches offer a very good accuracy in many historical collections,
and thus this is a less restrictive segmentation assumption in practice [40, 41, 28] .
One additional benefit that these systems offer, in front of most word segmentation￾based approaches, is that they are able to take into account textual context information
to improve the accuracy.
Nevertheless, the computation needed to take into account the context information
is not negligible. The time needed to process a text line typically grows exponentially
with the size of the textual context considered, unless some pruning and approxima￾tion strategies are used to accelerate the process. For instance, the number of states
in a 푛-gram language models, which directly affects the running time of keyword
spotting and text recognition systems using these, increases exponentially with the
context size 푛 (more details will be discussed in Chapter 4. Yet, this extra computa￾tional resources are only needed during a single step of the construction of the PrIx
index, as we will see in Chapter 5.2.2 Taxonomy of KWS approaches 21
2.2.1.3 Segmentation-free
Finally, the less restrictive scenario is where no segmentation assumption is made
whatsoever. There are many works that claim to follow a segmentation-free approach,
however they actually are divided into two clearly separated steps: first, the location
and segmentation of word-like areas of the image, and then deciding which of these
areas are relevant for the given query [59, 1, 51, 58].
Regardless of whether the system is implicitly or explicitly free of any segmen￾tation assumption, we believe that all methods should be evaluated at some point
under a fully automatic segmentation-free scenario. Mainly, because this will be
the real operating scenario once the systems are deployed in libraries and archives,
where millions of document pages have to be processed. Thus, despite the fact that
the methods presented in this book generally operate on segmented lines, we will
carry out some experiments where no manual segmentation of the pages is given
(see Chapter 6).
2.2.2 Retrieved Objects
Another important aspect of any KWS system is the type of the retrieved objects. In
practice, this is usually related to the previous subsection: most works that operate
under a line segmentation assumption retrieve relevant lines for a given query, and
systems working under a word segmentation assumption, typically retrieve relevant
word instances. Nevertheless, although this is the usual practice, it does not mean that
it is the only possible combination, nor the most recommended. Fig. 2.3 illustrates
the different types of retrieved objects described below.
2.2.2.1 Word Instances
As we mentioned before, a large portion of the keyword spotting systems described
in the literature operate under the word segmentation assumption: that is, the system
assumes that accurate word segmented regions have been extracted from the col￾lection of images. Thus, when a user gives a query keyword to the system, it will
provide a ranked list of the word regions that, according to the system, correspond
to the given keyword [43, 2, 64, 69, 29].
2.2.2.2 Lines
In a similar way than before, the system will retrieve full text line regions where it
believes that the user’s query keyword is written. This approach is followed also by
many works [20, 21, 53, 76]. The benefit of this approach is twofold: first, it gives
more context to the user to decide whether or not the retrieved object is actually of22 2 State Of The Art
(a) (b)
(c)
Fig. 2.3: Different types of objects to retrieve after a user query. For instance, if the user
searches for “Labour”, a keyword spotting system could choose to retrieve (a) individual
instances of this keyword, (b) lines where this word was written, or (c) whole pages or
paragraphs containing the keyword.
interest to the human labor required to produce the ground truth is much smaller.
Also, it provides with performance measure values highly correlated with those
systems retrieving word instances [82, 81].
2.2.2.3 Pages
Last, but not least, the spotting system could choose to retrieve full pages or para￾graphs containing multiple lines. This would give the user further context, and the
measuring of the quality of the results would resemble more the traditional appli￾cations of Information Retrieval. Remember, that the user is trying to spot some
keyword in our collection of documents because she needs to satisfy some need of
information, and it is highly probable that the required information cannot be found
in a single text line of our collection. Hence, it would make perfect sense that the
system reported full text pages or paragraphs.
However, because most keyword spotting works are focused at a very low level
(decide whether or not a word instance in a collection of images is the one that
the user was searching for), they disregard this scenario. There are some works that
evaluate their systems retrieving paragraphs, with more complex queries [81, 75].2.2 Taxonomy of KWS approaches 23
2.2.3 Query Representation
The user may interact with the system in different ways, in order to present the
query keyword. The two classical alternatives are the Query-by-String and Query￾by-Example paradigms. Fig. 2.4 shows a representation of the two paradigms, which
are explained next.
Query:
country
(a)
Query:
(b)
Fig. 2.4: Illustration of the different query paradigms used in the Keyword Spotting
literature. The yellow box represents the user input. In figure (a) the user types a query
string using her keyboard, while in figure (b) she uses an exemplar image containing the
keyword to search for.
2.2.3.1 Query-by-String
On the one hand, the Query-by-String (QbS) paradigm assumes that the query key￾word is presented to the system as an individual symbol part of a vocabulary lexicon
or, alternatively, as a sequence of characters of a given alphabet. This paradigm is
typically adopted in the speech community [56, 84, 57], and many of the handwriting
community works influenced by the former [20, 21, 76, 74].
2.2.3.2 Query-by-Example
On the other hand, the Query-by-Example (QbE) paradigm assumes that an exemplar
image, containing the query keyword of interest, is given to the system, and it
has to find the instances of the same keyword within the collection of document
images [43, 1, 72, 54, 64, 49].
Historically, in this case, KWS is seen as a particular instance of Content-based
Image Retrieval (CBIR) [77, 5, 66], since most researchers focusing on this paradigm
have a Computer Vision background, where CBIR has a long tradition.
In this book, we will focus mainly on the QbS paradigm, however our probabilistic
framework will also be applied to the QbE case. Without giving much further details,
it will be shown in successive chapters (see Chapter 7) that the QbE case only
introduces one additional hidden variable with respect to the QbS case, and this only
requires minor modifications to the algorithms.
One clear advantage of QbS in front of QbE is that the user only needs her
keyboard to search for any imaginable concept that she wishes, and a broader set24 2 State Of The Art
of queries (such as Boolean queries or arbitrary regular expressions) can be used.
Notice that the QbE is more restricted in this sense, since, in principle, we would
need at least one exemplar image for each of the keywords forming the query.
Nevertheless, the QbE paradigm also offers some advantages in front of some QbS
approaches. In particular, as we will discuss later (see Chapter 5), QbS approaches
relying on a closed lexicon are prone to the out-of-vocabulary problem, while QbE
approaches are essentially resistant to it.
2.2.4 Training Requirements
Finally, a fundamental distinction among different KWS systems is whether they
need human annotated training data to be built. This is a very important distinction,
since systems that do not need annotated data (i.e. unsupervised methods) are much
cheaper to build than those requiring large quantities of annotated samples (also
know as supervised methods).
2.2.4.1 Unsupervised
Initial works on KWS for historical documents typically fall into this category [33,
43, 32]. Virtually all the unsupervised approaches have been restricted to the query￾by-example paradigm, explained before. Typically, researchers apply some feature
extraction mechanism on the images (typically, pre-segmented word-shaped boxes)
in order to extract visual features that could discriminate similar images. Then, they
compare the features extracted from the collection of images against the features
obtained from the query, in order to rank them by some similarity (or dissimilarity)
measure. Recently, there are still works being published under this paradigm [72,
51, 54, 17, 86], although the popular trend is to use supervised methods.
2.2.4.2 Supervised
Because visually similar or dissimilar images do not necessary mean relevant or
irrelevant pairs of objects, researchers soon noticed that better quality results could
be obtained by using supervised methods. In fact, most recent successful KWS
methods are supervised algorithms [20, 21, 2, 69, 76].
This book focuses on supervised methods. As it will be shown, our framework
involves the probability distribution of the content of the text images (i.e. the posterior
probability over transcripts), and supervised methods excel in the task of learning
parametric models for these kind of distributions. However, experimental evaluation
with different amounts of supervised training data will be carried on, in order to
measure the amount of needed annotation.2.3 Additional Significant Matters for KWS 25
2.3 Additional Significant Matters for KWS
In the previous sections we have reviewed the most significant research topics that
have been widely considered in the field of KWS for text images. Most of the cited
works are mainly academically oriented and tend to overlook important features
which are actually needed in practical search interfaces aimed to find textual infor￾mation in real, large-scale collections of text images. Perhaps the most important
features that are largely overlooked are dealing with hyphenated and abbreviated
text and allowing combined, multi-word queries.
2.3.1 Hyphenated Words
Hyphenated words are very frequent in historical documents. For example, in many
large manuscript collections, such as the Finnish Court Records (see Appendix C),
at least one word is hyphenated every other text line, which results in about 10% of
the written tokens being fragments of hyphenated words.
Most works on HTR sidestep this problem and just try to accurately recognize the
prefix/suffix fragments of each hyphenated word [8, 71, 60, 70, 76]. Note that reliable
recognition of these fragments is problematic and has not been sufficiently studied so
far. However, if the aim is to transcribe text images, a sufficiently accurate character￾level recognition of the fragments (and maybe the hyphens themselves) might be an
admissible transcription result. This explains why only a handful of HTR works can
be found in the literature so far which explicitly deal with this problem [8, 71, 70].
Special mention, however, deserve recent works on line-segmentation-free, full-page
or full-paragraph HTR approaches [15, 16], where hyphenation fragments are paired
in training and the resulting systems prove able to transcribe hyphenated words into
their corresponding entire word forms.
Nevertheless, since our goal is to allow searching for words, users need to use
entire words in their queries. Clearly it completely unacceptable that users have
to figure out what are the possible fragments into which these words may happen
to have been broken when the documents were written! Works that deal with this
important practical problem are extremely rare. We only find [82, 88], in addition to
our own recent papers [80, 3]. These works will be reviewed in detail in Chapter 8
of this book.
2.3.2 Abbreviations
Abbreviations are intensively used in historical documents. For instance, in typical
Latin texts, about 40% of the written words are abbreviated. And abbreviations
seldom follow consistent, homogeneous rules. This becomes dramatically the case26 2 State Of The Art
in handwritten documents. Again, most works on HTR sidestep this problem and just
try to accurately recognize (at the character level) the abbreviations as such. This is
often referred to as diplomatic or paleographic transcription. As with hyphenation,
historical documents seldom apply consistent or homogeneous abbreviation rules
and, moreover, abbreviated tokens very frequently become ambiguous and one can
only tell which is the right way to expand an abbreviation by relying on textual
context provided by the surrounding text. In the field of HTR, it is only recently that
this problem is starting to be explicitly considered [83, 12, 61].
Also as in the case of hyphenated words, since our goal is to allow searching for
words, it becomes completely unacceptable that users have to figure out what are the
possible abbreviated forms of the words they wish to find. The specific problem of
searching for abbreviated words in text images does not seem to have been explicitly
considered so far, except for a few works of our own [6, 79, 61].
2.3.3 Multiple-word Queries
The vast majority of works on KWS for text images only consider single-word
queries. But in the practical application of this technology for information searching
it is necessary to allow for queries based on multiple words. In a real, large-scale
application, users expect to be able to use all the conventional search amenities
available when searching for information in electronic text (like the Internet). A
basic technology requirement is to provide support for word sequences and Boolean
AND/OR word combinations.
As previously commented, the field of KWS is more mature for speech docu￾ments (where it is often called spoken term detection) than for text images. Thus,
significant amount of work has been already carried out in this area to support the
most conventional forms of multiple-word queries [87, 62, 85, 13]. While all these
works are certainly interesting and useful, the approaches and techniques proposed
are not directly applicable to searching in text images.
Regarding works specifically dealing with text images, we can only cite one
paper [55] that describes a simplistic way to combine KWS outcomes for individual
keywords to obtain a response similar to that of an AND query. In addition, there are
a few papers of our own that explicitly deal with queries based on AND/OR word
combinations [50, 9, 75, 61] and word sequences [9, 10, 61]. These works will be
reviewed in detail in Chapter 8 of this book.
2.4 State of the Art in HTR Models and Methods
HTR approaches evolved from the first proposals based on detecting and recognizing
the constituent words/characters of a given handwritten text image [42, 63] to the
segmentation-free approaches based on Hidden Markov Models (HMM) that enable
the modeling of syntactic constraints through language models as in [4, 73, 45].2.4 State of the Art in HTR Models and Methods 27
Since 2009, the Recurrent Neural Networks” (RNN) and “Deep Feed-Forward
Neural Networks began to be key parts of the new recognition systems with amazing
results. In particular those Recognizers using the RNN types Bi-directional and
Multi-dimensional Long Short-Term Memory (LSTM) [27, 26], and trained with the
Connectionist Temporal Classification (CTC) [25] loss function far outperformed
those based on HMM. This trend was later consolidated with the introduction of
Convolutional Neural Networks (CNN) [22] for automatic feature extractions from
images, defining a new architecture type named Convolutional-Recurrent Neural
Network (CRNN) [65, 7, 68]. State-of-the-art HTR toolkits like PyLaia [52] and
Kraken [34] are based on this architecture type.
Lately, with the advent of Transformer models and their effective attention mech￾anism [78] (specially regarding the so-called Visual Transformers [18]), new HTR
approaches are emerging which achieve results comparable to those of CRNN on
traditional HTR datasets like IAM (Appendix C.1) or the simplest Bentham BEN1
dataset (Appendix C.2.1). These good results [39] were generally achieved with
pre-trained Transformer models fine-tuned on the specific dataset on which it is
evaluated. Furthermore, such pre-trained models have been fine-tuned on historical
manuscript datasets with fair results [67], but it is not entirely clear to what extent
this depends on the fact that original data used to pre-train the models included also
text samples of the same (or similar) language of the manuscript to be recognized.28 2 State Of The Art
References
1. Almazan, J., Gordo, A., Forn ´ es, A., Valveny, E.: Efficient Exemplar Word Spotting. In: ´
Proceedings of the British Machine Vision Conference, pp. 67.1–67.11. BMVA Press (2012).
DOI http://dx.doi.org/10.5244/C.26.67
2. Almazan, J., Gordo, A., Forn ´ es, A., Valveny, E.: Word Spotting and Recognition with Em- ´
bedded Attributes. IEEE Transactions on Pattern Analysis and Machine Intelligence 36(12),
2552–2566 (2014). DOI 10.1109/TPAMI.2014.2339814
3. Andres, J., Toselli, A.H., Vidal, E.: Search for hyphenated words in probabilistic indices: a ´
machine learning approach. In: 2023 International Conference on Document Analysis and
Recognition (ICDAR), pp. 269–285 (2023)
4. Bazzi, I., Schwartz, R., Makhoul, J.: An Omnifont Open-Vocabulary OCR System for English
and Arabic. IEEE Trans. on Pattern Analysis and Machine Intelligence 21(6), 495–504 (1999)
5. Bird, C.L., Chapman, S.G., Ibbotson, J.B.: Content-driven navigation of large databases. In:
IEEE Coll. on Intelligent Image Databases, pp. 13/1–13/5 (1996). DOI 10.1049/ic:19960751
6. Bluche, T., Hamel, S., Kermorvant, C., Puigcerver, J., Stutzmann, D., Toselli, A.H., Vidal,
E.: Preparatory KWS Experiments for Large-Scale Indexing of a Vast Medieval Manuscript
Collection in the HIMANIS Project. In: Proc. of 14th ICDAR (2017)
7. Bluche, T., Louradour, J., Messina, R.O.: Scan, attend and read: End-to-end handwritten para￾graph recognition with mdlstm attention. 14th IAPR International Conference on Document
Analysis and Recognition (ICDAR) 01, 1050–1055 (2016)
8. Bluche, T., Ney, H., Kermorvant, C.: The LIMSI handwriting recognition system for the HTRtS
2014 contest. In: 13th International Conference on Document Analysis and Recognition
(ICDAR), pp. 86–90. IEEE (2015)
9. Calvo-Zaragoza, J., Toselli, A.H., Vidal, E.: Probabilistic music-symbol spotting in handwritten
scores. In: 2018 16th International Conference on Frontiers in Handwriting Recognition
(ICFHR), pp. 558–563. IEEE (2018)
10. Calvo-Zaragoza, J., Toselli, A.H., Vidal, E., Sanchez, J.A.: Music symbol sequence indexing ´
in medieval plainchant manuscripts. In: 2019 International Conference on Document Analysis
and Recognition (ICDAR), pp. 882–887. IEEE (2019)
11. Cambria, E., Schuller, B., Xia, Y., Havasi, C.: New Avenues in Opinion Mining and Sentiment
Analysis. IEEE Intelligent Systems 28(2), 15–21 (2013). DOI 10.1109/MIS.2013.30
12. Camps, J.B., Vidal-Gorene, C., Vernet, M.: Handling heavily abbreviated manuscripts: Htr `
engines vs text normalisation approaches. ArXiv abs/2107.03450 (2021)
13. Can, D., Saraclar, M.: Lattice indexing for spoken term detection. IEEE Transactions on Audio,
Speech, and Language Processing 19(8), 2338–2347 (2011)
14. Chen, F.R., Wilcox, L.D., Bloomberg, D.S.: Word spotting in scanned images using hidden
Markov models. In: International Conference on Acoustics, Speech, and Signal Processing,
vol. 5, pp. 1–4 (1993). DOI 10.1109/ICASSP.1993.319732
15. Coquenet, D., Chatelain, C., Paquet, T.: SPAN: a simple predict & align network for hand￾written paragraph recognition. In: Document Analysis and Recognition–ICDAR 2021: 16th
International Conference, Lausanne, Switzerland, September 5–10, 2021, Proceedings, Part
III 16, pp. 70–84. Springer (2021)
16. Coquenet, D., Chatelain, C., Paquet, T.: DAN: a segmentation-free document attention network
for handwritten document recognition. IEEE Transactions on Pattern Analysis and Machine
Intelligence (2023)
17. Dey, S., Nicolaou, A., Llados, J., Pal, U.: Local Binary Pattern for Word Spotting in Handwritten ´
Historical Document. In: A. Robles-Kelly, M. Loog, B. Biggio, F. Escolano, R. Wilson (eds.)
Structural, Syntactic, and Statistical Pattern Recognition, pp. 574–583. Springer International
Publishing (2016)
18. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., De￾hghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth
16x16 words: Transformers for image recognition at scale. CoRR abs/2010.11929 (2020)References 29
19. Fischer, A., Frinken, V., Bunke, H., Suen, C.Y.: Improving HMM-Based Keyword Spotting
with Character Language Models. In: 2013 12th International Conference on Document
Analysis and Recognition, pp. 506–510 (2013). DOI 10.1109/ICDAR.2013.107
20. Fischer, A., Keller, A., Frinken, V., Bunke, H.: Lexicon-free handwritten word spotting using
character HMMs. Pattern Recognition Letters 33(7), 934 – 942 (2012). DOI 10.1016/j.patrec
.2011.09.009. Special Issue on Awards from ICPR 2010
21. Frinken, V., Fischer, A., Manmatha, R., Bunke, H.: A Novel Word Spotting Method Based on
Recurrent Neural Networks. IEEE Transactions on Pattern Analysis and Machine Intelligence
34(2), 211–224 (2012). DOI 10.1109/TPAMI.2011.113
22. Fukushima, K., Miyake, S.: Neocognitron: A Self-Organizing Neural Network Model for a
Mechanism of Visual Pattern Recognition. In: S.i. Amari, M.A. Arbib (eds.) Competition and
Cooperation in Neural Nets, pp. 267–285. Springer Berlin Heidelberg (1982)
23. Giotis, A.P., Sfikas, G., Gatos, B., Nikou, C.: A survey of document image word spotting
techniques. Pattern Recognition 68, 310–332 (2017)
24. Graves, A., Eck, D., Beringer, N., Schmidhuber, J.: Biologically Plausible Speech Recognition
with LSTM Neural Nets. In: A.J. Ijspeert, M. Murata, N. Wakamiya (eds.) Biologically Inspired
Approaches to Advanced Information Technology, pp. 127–136. Springer Berlin Heidelberg,
Berlin, Heidelberg (2004)
25. Graves, A., Fernandez, S., Gomez, F., Schmidhuber, J.: Connectionist Temporal Classification: ´
Labelling Unsegmented Sequence Data with Recurrent Neural Networks. In: Proceedings of
the 23rd International Conference on Machine Learning, ICML ’06, pp. 369–376. ACM, New
York, NY, USA (2006). DOI 10.1145/1143844.1143891
26. Graves, A., Liwicki, M., Fernandez, S., Bertolami, R., Bunke, H., Schmidhuber, J.: A novel ´
connectionist system for unconstrained handwrting recognition. IEEE Trans. on PAMI 31(5),
855–868 (2009)
27. Graves, A., Schmidhuber, J.: Offline handwriting recognition with multidimensional recurrent
neural networks. In: Proceedings of the 21st International Conference on Neural Information
Processing Systems, NIPS’08, p. 545–552. Curran Associates Inc., Red Hook, NY, USA (2008)
28. Gr ¨uning, T., Leifert, G., Strauss, T., Labahn, R.: A Robust and Binarization-Free Approach for
Text Line Detection in Historical Documents. In: 2017 14th IAPR International Conference
on Document Analysis and Recognition (ICDAR), vol. 01, pp. 236–241 (2017). DOI 10.110
9/ICDAR.2017.47
29. Gomez, L., Rusi ´ nol, M., Karatzas, D.: LSDE: Levenshtein Space Deep Embedding for Query- ˜
by-String Word Spotting. In: 2017 14th IAPR International Conference on Document Analysis
and Recognition (ICDAR), vol. 01, pp. 499–504 (2017). DOI 10.1109/ICDAR.2017.88
30. Hazen, T.J., Shen, W., White, C.: Query-by-example spoken term detection using phonetic
posteriorgram templates. In: 2009 IEEE Workshop on Automatic Speech Recognition Under￾standing, pp. 421–426 (2009). DOI 10.1109/ASRU.2009.5372889
31. Jelinek, F.: Continuous speech recognition by statistical methods. Proceedings of the IEEE
64(4), 532–556 (1976). DOI 10.1109/PROC.1976.10159
32. Keaton, P., Greenspan, H., Goodman, R.: Keyword spotting for cursive document retrieval. In:
Document Image Analysis, 1997. (DIA ’97) Proceedings., Workshop on, pp. 74–81 (1997).
DOI 10.1109/DIA.1997.627095
33. Khoubyari, S., Hull, J.J.: Keyword location in noisy document image. In: 2nd Annual Sympo￾sium on Document Analysis and Information Retrieval, pp. 217–231 (1993)
34. Kiessling, B., Kurin, G., Miller, M.T., Smail, K., Miller, M.: Advances and limitations in open
source arabic-script ocr: A case study. Digital Studies/Le champ numerique ´ 11(1) (2021)
35. Kohler, J., Larson, M., Jong de, F., Kraaij, W., R.J.F, O. (eds.): Searching Spontaneous conver￾sational speech workshop, proc. of the ACM SIGIR workshop of the 31th Annual International
SIGIR conference. Centre for Telematics and Inf. Tech., Enschede, The Netherlands (2008)
36. Kumari, L., Sharma, A.: A review of deep learning techniques in document image word
spotting. Archives of Computational Methods in Engineering pp. 1–22 (2021)
37. Kundu, A., He, Y., Bahl, P.: Recognition of handwritten word: First and second order hidden
Markov model based approach. Pattern Recognition 22(3), 283–297 (1989). DOI https:
//doi.org/10.1016/0031-3203(89)90076-930 2 State Of The Art
38. Lee, L.s., Glass, J., Lee, H.y., Chan, C.a.: Spoken content retrieval—beyond cascading speech
recognition with text retrieval. IEEE/ACM Transactions on Audio, Speech, and Language
Processing 23(9), 1389–1420 (2015)
39. Li, M., Lv, T., Cui, L., Lu, Y., Florencio, D.A.F., Zhang, C., Li, Z., Wei, F.: Trocr: Transformer- ˆ
based optical character recognition with pre-trained models. ArXiv abs/2109.10282 (2021)
40. Likforman-Sulem, L., Zahour, A., Taconet, B.: Text line segmentation of historical documents:
a survey. International Journal of Document Analysis and Recognition (IJDAR) 9(2), 123–138
(2007). DOI 10.1007/s10032-006-0023-z
41. Louloudis, G., Gatos, B., Pratikakis, I., Halatsis, C.: Text Line and Word Segmentation of
Handwritten Documents. Pattern Recogn. 42(12), 3169–3183 (2009). DOI 10.1016/j.patcog
.2008.12.016
42. Mahadevan, U., Nagabushnam, R.: Gap metrics for word separation in handwritten lines. In:
Proceedings of 3rd International Conference on Document Analysis and Recognition, vol. 1,
pp. 124–127 vol.1 (1995). DOI 10.1109/ICDAR.1995.598958
43. Manmatha, R., Han, C., Riseman, E.M.: Word spotting: a new approach to indexing hand￾writing. In: Proceedings CVPR IEEE Computer Society Conference on Computer Vision and
Pattern Recognition, pp. 631–637 (1996). DOI 10.1109/CVPR.1996.517139
44. Manmatha, R., Han, C., Riseman, E.M., Croft, W.B.: Indexing Handwriting Using Word
Matching. In: Proceedings of the First ACM International Conference on Digital Libraries,
DL ’96, pp. 151–159. ACM, New York, NY, USA (1996). DOI 10.1145/226931.226960
45. Marti, U.V., Bunke, H.: Handwritten sentence recognition. In: Proceedings 15th International
Conference on Pattern Recognition. ICPR-2000, vol. 3, pp. 463–466 vol.3 (2000). DOI
10.1109/ICPR.2000.903584
46. Marti, U.V., Bunke, H.: Using a statistical language model to improve the performance of an
HMM-based cursive handwriting recognition system. In: Hidden Markov Models, pp. 65–90.
World Scientific (2001). DOI 10.1142/9789812797605 0004
47. Mary, L., Deekshitha, G.: Searching speech databases: features, techniques and evaluation
measures. Springer (2018)
48. Miller, D.R., Kleber, M., Kao, C.L., Kimball, O., Colthurst, T., Lowe, S.A., Schwartz, R.M.,
Gish, H.: Rapid and accurate spoken term detection. In: 8th Annual Conference of the
International Speech Communication Association (2007)
49. Mondal, T., Ragot, N., Ramel, J.Y., Pal, U.: Flexible Sequence Matching technique: An effective
learning-free approach for word spotting. Pattern Recognition 60, 596–612 (2016). DOI
https://doi.org/10.1016/j.patcog.2016.05.011
50. Noya-Garc´ıa, E., Toselli, A.H., Vidal, E.: Simple and Effective Multi-word Query Spotting in
Handwritten Text Images, pp. 76–84. Springer International Publishing (2017)
51. Papandreou, A., Gatos, B., Zagoris, K.: An Adaptive Zoning Technique for Word Spotting
Using Dynamic Time Warping. In: 2016 12th IAPR Workshop on Document Analysis Systems
(DAS), pp. 387–392 (2016). DOI 10.1109/DAS.2016.79
52. Puigcerver, J.: Are Multidimensional Recurrent Layers Really Necessary for Handwritten
Text Recognition? In: 2017 14th IAPR International Conference on Document Analysis and
Recognition (ICDAR), vol. 01, pp. 67–72 (2017). DOI 10.1109/ICDAR.2017.20
53. Puigcerver, J., Toselli, A.H., Vidal, E.: Probabilistic interpretation and improvements to the
HMM-filler for handwritten keyword spotting. In: 13th Int. Conf. on Document Analysis and
Recognition (ICDAR), pp. 731–735 (2015). DOI 10.1109/ICDAR.2015.7333858
54. Retsinas, G., Louloudis, G., Stamatopoulos, N., Gatos, B.: Keyword Spotting in Handwritten
Documents Using Projections of Oriented Gradients. In: 2016 12th IAPR Workshop on
Document Analysis Systems (DAS), pp. 411–416 (2016). DOI 10.1109/DAS.2016.61
55. Riba, P., Almazan, J., Forn ´ es, A., Fern ´ andez-Mota, D., Valveny, E., Llad ´ os, J.: e-crowds: A ´
mobile platform for browsing and searching in historical demography-related manuscripts. In:
Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference on, pp.
228–233 (2014). DOI 10.1109/ICFHR.2014.46
56. Rohlicek, J.R., Russell, W., Roukos, S., Gish, H.: Continuous hidden Markov modeling for
speaker-independent word spotting. In: International Conference on Acoustics, Speech, and
Signal Processing, vol. 1, pp. 627–630 (1989). DOI 10.1109/ICASSP.1989.266505References 31
57. Rose, R.: Keyword detection in conversational speech utterances using hidden Markov model
based continuous speech recognition. Computer Speech & Language 9(4), 309–333 (1995).
DOI https://doi.org/10.1006/csla.1995.0015
58. Rothacker, L., Sudholt, S., Rusakov, E., Kasperidus, M., Fink, G.A.: Word hypotheses for
segmentation-free word spotting in historic document images. In: 2017 14th IAPR International
Conference on Document Analysis and Recognition (ICDAR), vol. 01, pp. 1174–1179 (2017).
DOI 10.1109/ICDAR.2017.194
59. Rusinol, M., Aldavert, D., Toledo, R., Llad ˜ os, J.: Browsing Heterogeneous Document Collec- ´
tions by a Segmentation- Free Word Spotting Method. In: 2011 International Conference on
Document Analysis and Recognition, pp. 63–67 (2011). DOI 10.1109/ICDAR.2011.22
60. Sanchez, J.A., Romero, V., Toselli, A.H., Villegas, M., Vidal, E.: A set of benchmarks for ´
handwritten text recognition on historical documents. Pattern Recognition 94, 122–134 (2019)
61. Sanchez, J.A., Vidal, E., Bosch, V.: Effective crowdsourcing in the EDT project with prob- ´
abilistic indexes. In: Document Analysis Systems: 15th IAPR International Workshop, DAS
2022, La Rochelle, France, May 22–25, 2022, Proceedings, pp. 291–305. Springer (2022)
62. Seide, F., Yu, P., Shi, Y.: Towards spoken-document retrieval for the enterprise: Approximate
word-lattice indexing with text indexers. In: 2007 IEEE Workshop on Automatic Speech
Recognition & Understanding (ASRU), pp. 629–634. IEEE (2007)
63. Seni, G., Cohen, E.: External word segmentation of off-line handwritten text lines. Pattern
Recognition 27(1), 41–52 (1994)
64. Sfikas, G., Retsinas, G., Gatos, B.: Zoning Aggregated Hypercolumns for Keyword Spotting.
In: 2016 15th International Conference on Frontiers in Handwriting Recognition (ICFHR), pp.
283–288 (2016). DOI 10.1109/ICFHR.2016.0061
65. Shi, B., Bai, X., Yao, C.: An end-to-end trainable neural network for image-based sequence
recognition and its application to scene text recognition. IEEE Trans. on Pattern Analysis and
Machine Intelligence 39(11), 2298–2304 (2017). DOI 10.1109/TPAMI.2016.2646371
66. Smeulders, A.W.M., Worring, M., Santini, S., Gupta, A., Jain, R.: Content-Based Image
Retrieval at the End of the Early Years. IEEE Trans. Pattern Anal. Mach. Intell. 22(12),
1349–1380 (2000). DOI 10.1109/34.895972
67. Strobel, P.B., Clematide, S., Volk, M., Hodel, T.: Transformer-based HTR for historical docu- ¨
ments. arXiv preprint arXiv:2203.11008 (2022)
68. Subramani, N., Matton, A., Greaves, M., Lam, A.: A survey of deep learning approaches for
ocr and document understanding. ArXiv abs/2011.13534 (2020)
69. Sudholt, S., Fink, G.A.: PHOCNet: A Deep Convolutional Neural Network for Word Spotting in
Handwritten Documents. In: 2016 15th International Conference on Frontiers in Handwriting
Recognition (ICFHR), pp. 277–282 (2016). DOI 10.1109/ICFHR.2016.0060
70. Swaileh, W., Lerouge, J., Paquet, T.: A unified french/english syllabic model for handwriting
recognition. In: 15th Int. Conf. on Frontiers in Handwriting Rec. (ICFHR), pp. 536–541 (2016)
71. Sanchez, J.A., Romero, V., Toselli, A.H., Vidal, E.: ICFHR2014 Competition on Hand- ´
written Text Recognition on Transcriptorium Datasets (HTRtS). In: 2014 14th Interna￾tional Conference on Frontiers in Handwriting Recognition, pp. 785–790 (2014). DOI
10.1109/ICFHR.2014.137
72. Tarafdar, A., Pal, U., Roy, P.P., Ragot, N., Ramel, J.: A Two-Stage Approach for Word Spotting
in Graphical Documents. In: 2013 12th International Conference on Document Analysis and
Recognition, pp. 319–323 (2013). DOI 10.1109/ICDAR.2013.71
73. Toselli, A.H., Juan, A., Gonzalez, J., Salvador, I., Vidal, E., Casacuberta, F., Keysers, D., Ney, ´
H.: Integrated handwriting recognition and interpretation using finite-state models. Int. Journal
of Pattern Recognition and Artificial Intelligence 18(04), 519–539 (2004)
74. Toselli, A.H., Puigcerver, J., Vidal, E.: Context-aware lattice based filler approach for key word
spotting in handwritten documents. In: 2015 13th Int. Conference on Document Analysis and
Recognition (ICDAR), pp. 736–740 (2015). DOI 10.1109/ICDAR.2015.7333859
75. Toselli, A.H., Vidal, E., Puigcerver, J., Noya-Garc´ıa, E.: Probabilistic multi-word spotting in
handwritten text images. Pattern Analysis and Applications (2019)32 2 State Of The Art
76. Toselli, A.H., Vidal, E., Romero, V., Frinken, V.: HMM Word-Graph Based Keyword Spotting
in Handwritten Document Images. Information Sciences 370(C), 497–518 (2016). DOI
10.1016/j.ins.2016.07.063
77. Toshikazu, K., Takio, K., Hiroyuki, S.: Intelligent Visual Interaction with Image Database
Systems : Toward the Multimedia Personal Interface. Journal of Information Processing 14(2),
134–143 (1991)
78. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L.,
Polosukhin, I.: Attention is all you need. CoRR abs/1706.03762 (2017)
79. Vidal, E., Romero, V., Toselli, A.H., Sanchez, J.A., Bosch, V., Quir ´ os, L., Bened ´ ´ı, J.M.,
Prieto, J.R., Pastor, M., Casacuberta, F., et al.: The carabela project and manuscript collection:
large-scale probabilistic indexing and content-based classification. In: 2020 17th International
Conference on Frontiers in Handwriting Recognition (ICFHR), pp. 85–90. IEEE (2020)
80. Vidal, E., Toselli, A.H.: Probabilistic indexing and search for hyphenated words. In: Document
Analysis and Recognition–ICDAR 2021: 16th International Conference, Proceedings, Part II
16, pp. 426–442. Springer (2021)
81. Villegas, M., M ¨uller, H., Garc´ıa Seco de Herrera, A., Schaer, R., Bromuri, S., Gilbert, A., Piras,
L., Wang, J., Yan, F., Ramisa, A., Dellandrea, E., Gaizauskas, R., Mikolajczyk, K., Puigcerver,
J., Toselli, A.H., Sanchez, J.A., Vidal, E.: General overview of imageclef at the clef 2016 labs. ´
In: N. Fuhr, P. Quaresma, T. Gonc¸alves, B. Larsen, K. Balog, C. Macdonald, L. Cappellato,
N. Ferro (eds.) Experimental IR Meets Multilinguality, Multimodality, and Interaction, pp.
267–285. Springer International Publishing, Cham (2016)
82. Villegas, M., Puigcerver, J., Toselli, A.H., Sanchez, J.A., Vidal, E.: Overview of the ImageCLEF ´
2016 Handwritten Scanned Document Retrieval Task. In: CLEF (Working Notes), pp. 233–253
(2016)
83. Villegas, M., Toselli, A.H., Romero, V., Vidal, E.: Exploiting Existing Modern Transcripts for
Historical Handwritten Text Recognition. In: 2016 15th International Conference on Frontiers
in Handwriting Recognition (ICFHR), pp. 66–71 (2016). DOI 10.1109/ICFHR.2016.0025
84. Weintraub, M.: Keyword-spotting using SRI’s DECIPHER large-vocabulary speech- recog￾nition system. In: 1993 IEEE International Conference on Acoustics, Speech, and Signal
Processing, vol. 2, pp. 463–466 (1993). DOI 10.1109/ICASSP.1993.319341
85. Yu, R.P., Thambiratnam, K., Seide, F.: Word-lattice based spoken-document indexing with
standard text indexers. In: Searching Spontaneous conversational speech workshop, SIGIR,
pp. 54–61. Citeseer (2008)
86. Zagoris, K., Pratikakis, I., Gatos, B.: Unsupervised Word Spotting in Historical Handwritten
Document Images Using Document-Oriented Local Features. IEEE Transactions on Image
Processing 26(8), 4032–4041 (2017). DOI 10.1109/TIP.2017.2700721
87. Zhou, Z.Y., Yu, P., Chelba, C., Seide, F.: Towards spoken-document retrieval for the internet:
Lattice indexing for large-scale web-search architectures. In: Proceedings of the Human
Language Technology Conference of the NAACL, Main Conference, pp. 415–422 (2006)
88. Ziran, Z., Pic, X., Innocenti, S.U., Mugnai, D., Marinai, S.: Text alignment in early printed
books combining deep learning and dynamic programming. Pattern Recognition Letters 133,
109–115 (2020). DOI https://doi.org/10.1016/j.patrec.2020.02.016Chapter 3
Probabilistic Indexing (PrIx) Framework
Abstract The proposed PrIx framework is formally presented in this chapter. In
short, PrIx aims at processing each text image in such a way that all the sets of strokes
in the image which can be reasonably interpreted as text elements, such as characters
and words, become symbolically represented; that is, represented like electronic
text. However, the primary concern of PrIx is to retain all the information needed
to also represent the intrinsic uncertainty which underlies text images, and more
specifically handwritten text images. A dual presentation is given. First, a “pure”
image processing viewpoint is adopted, where each text element in the images is
treated just as a small object that has to be somehow detected and identified. This
presentation will make it clear that PrIx, and KWS alike, essentially boil down to an
object recognition process, where the class’ posterior probability of each object has
to be estimated at each image location. Then PrIx will be developed in full detail
from another equivalent viewpoint where the underlying object recognition problem
is equated to HTR, thereby considering PrIx as a form of HTR which explicitly
retains image interpretation uncertainty.
3.1 Pixel Level Textual Image Representation: 2-D Posteriorgram
Words are here considered just as small objects that we wish to detect and identify.
Each of these objects, denoted by 푣, is assumed to belong to a large (open) set of
“object classes” which might be called (open) Vocabulary.
The posteriorgram of a text image 푥 and a (key)word 푣 is the probability that 푣
uniquely and completely appears in some bounding box containing the pixel (푖, 푗).
In mathematical notation:
푃(푄 = 푣 | 푋 = 푥, 퐿 = (푖, 푗)) ≡ 푃(푣 | 푥, 푖, 푗) , 1≤ 푖≤ 퐼, 1≤ 푗≤ 퐽, 푣 ∈푉 (3.1)
where 퐿 is a random variable over the set of locations (pixel coordinates) and 퐼, 퐽
are the horizontal and vertical dimensions of 푥, respectively. 푃(푣 | 푥, 푖, 푗) is a proper
probability distribution that is:
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 33
A. H. Toselli et al., Probabilistic Indexing for Information Search and Retrieval 
in Large Collections of Handwritten Text Images, The Information Retrieval Series 49, 
https://doi.org/10.1007/978-3-031-55389-9_3 
 34 3 Probabilistic Indexing (PrIx) Framework
Õ
푣
푃(푣 | 푥, 푖, 푗) = 1 , 1≤ 푖≤ 퐼, 1≤ 푗≤ 퐽 (3.2)
A simple way to compute 푃(푣 | 푥, 푖, 푗) is by considering that 푣 may have been
written in any possible bounding box1 풃 in B (푖, 푗), the set of all bounding boxes
which contain the pixel (푖, 푗):
푃(푣 | 푥, 푖, 푗) =
Õ
풃∈ B (푖, 푗)
푃(푣, 풃 | 푥, 푖, 푗)
=
Õ
풃∈ B (푖, 푗)
푃(풃 | 푥, 푖, 푗) 푃(푣 | 푥, 풃, 푖, 푗) (3.3)
where 푃(푣 | 푥, 풃, 푖, 푗) is the probability that 푣 is the (unique) word written in the box
풃 (which includes the pixel (푖, 푗)). Therefore it is conditionally independent of (푖, 푗)
given 풃, and Eq (3.3) simplifies to:
푃(푣 | 푥, 푖, 푗) =
Õ
풃∈ B (푖, 푗)
푃(풃 | 푥, 푖, 푗) 푃(푣 | 푥, 풃) (3.4)
This marginalization process is illustrated in Fig. 3.1 and Fig. 3.2 shows real results of
computing 푃(푣| 푥, 푖, 푗) in this way for an example image 푥 and a specific keyword 푣.
j
i
Fig. 3.1: Marginalization bounding boxes 풃 ∈ B (푖, 푗). For 푣 = ”matter”, the thick-line
box will provide the highest value of 푃(푣 | 푥, 풃), while most of the other boxes will
contribute only (very) low amounts to the sum.
The distribution 푃(풃 | 푥, 푖, 푗) of Eq. (3.4) should be interpreted as the probability
that some word (not necessarily 푣) is written in the image region delimited by the
bounding box 풃. Therefore, this probability should be high for word-shaped and
word-sized bounding boxes centered around the pixel (푖, 푗), like some of those
illustrated in Fig. 3.1. In contrast, it should be low for boxes which are too small, too
large, or are too off-center with respect to (푖, 푗). For simplicity, it can be assumed that
this distribution is uniform for all reasonably sized and shaped boxes around (푖, 푗)
(and null for all other boxes), and then just replace this distribution with a constant
in Eq. (3.4). Such a simplification encourages the peaks of the posteriorgram to be
rather flat, as in Fig. 3.2.
1 Vector notation, like 풃, will be generally adopted for bounding boxes. In a 2-D digital image, 풃
is generally assumed to be rectangle, represented in N
4
by the coordinates of two opposed corners.
Line image regions, considered later on, are seen as 1-D objects and 풃 = (푏1, 푏2 )
푇∈ N
2
is a
horizontal segment delimited by 푏1 and 푏2.3.2 Image Regions for Keyword Indexing and Search 35
On the other hand, the term 푃(푣| 푥, 풃), is exactly the posterior probability needed
by any system capable of recognizing a pre-segmented word image (i.e., a sub-image
of 푥 bounded by 풃). Actually, such an isolated word recognition task can be formally
written as the following classification problem:
푣ˆ = arg max
푣∈푉
푃(푣 | 푥, 풃) (3.5)
In general, any system capable of recognizing pre-segmented word images implicitly
or explicitly computes 푃(푣| 푥, 풃) and can thereby be used to obtain the posteriorgram
according to Eq. (3.4). For example, using a 푘-Nearest Neighbor classifier, it can be
approximated just as [3]: 푃(푣| 푥, 풃) = 푘푣/푘 (3.6)
where 푘푣 is the number of 푣–labeled prototypes out of the 푘 which are nearest to the
image in the bounding box 풃 of 푥.
Obviously, the better the classifier, the better the corresponding posteriorgram
estimates. This is illustrated in Fig. 3.2, which shows two examples of image posteri￾orgrams obtained according to Eq. (3.4) using two different word image recognizers.
In both cases, well trained optical hidden Markov models (HMM) were used to
compute 푃(푣 | 푥, 풃) ∀풃 ∈ B (푖, 푗). 푃0 (푣 | 푥, 푖, 푗) was obtained directly, using a plain,
context-agnostic optical recognizer, and 푃2 (푣 | 푥, 푖, 푗) was produced using a more
precise contextual word recognizer, additionally based on a well trained bi-gram. As
it can be seen, 푃0 values are only good for the two clear instances of “matter”, but
almost vanish for a third instance, probably because of the faint character “m”. Worse
still, 푃0 values are relatively high for the similar, but wrong word “matters”; in fact
very much higher than for the third, faint instance of the correct one. In contrast, the
contextual recognizer leads to high 푃2 values for all the three correct instances of
“matter”, even for the faint one, while the values for the wrong word are very low.
Clearly bi-grams such as “It matter” and “matter not” are unlikely, thereby
preventing 푃2 (푣 | 푥, 풃) to be high for any box 풃 around the word “matters”. On the
other hand, the bi-grams “the matter” and “matter of” are very likely, thereby
helping the optical recognizer to boost 푃2 (푣 | 푥, 풃) for boxes 풃 around the faint
instance of “matter”.
Pixel-level posteriorgrams could be directly used for keyword search: Given
a threshold 휏 ∈ [0, 1], a word 푣 ∈ 푉 is spotted in all image positions where
푃(푣| 푥, 푖, 푗) > 휏. Varying 휏, adequate precision–recall tradeoffs could be achieved.
3.2 Image Regions for Keyword Indexing and Search
Computing the full posteriorgram as in Eq. (3.4) for all the words of a large vocabu￾lary (as needed for indexing purposes) and all the pixels of each page image entails a
formidable amount of computation. The same can be said for the exorbitant amount
of memory which would be needed to explicitly store all the resulting posterior prob￾abilities. Therefore such a direct approach is obviously inappropriate for indexing
purposes and, moreover, it becomes unfeasible for the size of text image collections36 3 Probabilistic Indexing (PrIx) Framework
푃2 (푣 | 푥, 푖, 푗)
푃0 (푣 | 푥, 푖, 푗)
푥
Fig. 3.2: Identical optical HMMs were carefully trained in order to help computing two
2-D posteriorgrams, 푃0 and 푃2, for a text image 푥 and keyword 푣=“matter”. A context￾agnostic HMM+0-gram isolated word classifier was used to obtain 푃0. But much better
posterior estimates are offered by 푃2, obtained using a contextual, HMM+2-gram classifier.
considered in this work. Clearly, rather than working at the pixel level, some ade￾quately small image regions, 푥, which are indexable and suitable search targets for
users, need be defined to compute the RPs introduced in Chapter 1 (Sec. 1.4).
While these concerns are seldom discussed in the KWS literature, region pro￾posal [18] has been the focus of a number of studies in the object recognition
community, as well as in the field of document analysis – see e.g. [4], which deals
with graphic pattern spotting in historical documents and [12, 16, 23], which apply
region proposal neural networks to various document layout analysis tasks.
In the traditional KWS literature, word-sized regions are often considered by
default. This is reminiscent of segmentation-based KWS methods which required
previously cropped accurate word bounding boxes. However, as discussed in Chap￾ter 1, this is not realistic for large image collections. More importantly, by considering
isolated words, it is difficult for the underlying word recognizer to take advantage of
word linguistic contexts to achieve good spotting precision (as illustrated in Fig. 3.2).
At the other extreme we may consider whole page images, or relevant text blocks
thereof, as the search target image regions. While this can be sufficiently adequate
for many textual content retrieval applications, a page may typically contain many
instances of the word searched for and, on the other hand, users generally like to get
narrower responses to their queries.3.3 Position-independent PrIx 37
A particularly interesting intermediate search target level consists of line-shaped
regions. Lines are useful targets for indexing and search in practice and, in contrast
with word-sized image regions, lines generally provide sufficient linguistic context
to allow computing accurate word classification probabilities. Moreover, as will be
discussed later on, line region posteriorgrams can be very efficiently computed.
3.3 Position-independent PrIx
Let us now examine how to obtain the RP 푃(푅 | 푥, 푣) defined in Eq. (1.1) when 푥
is a suitable (typically a line-shaped or any other small) image region. We will say
that this scenario assumes a position-independent RP. In this section we examine
different approaches to compute the position-independent RP for an image region.
3.3.1 Naive Word Posterior Interpretation of 푷(푹 | 풙, 풗)
If 푥 were a word-sized, tight bounding box, then 푃(푣 | 푥) could be used as a proxy
for 푃(푅 | 푥, 푣) as:
푃(푅=1 | 푥, 푣) ≈ 푃(푣| 푥) (3.7)
푃(푅=0 | 푥, 푣) ≈Õ
푢 ≠ 푣
푃(푢 | 푥) = 1 − 푃(푣 | 푥)
As will be discussed in Chapter 7, this is in fact the approach implicitly or explicitly
adopted by all word-segmentation-based KWS methods. So it is not surprising that
researchers have tried to stretch this idea even if 푥 is not a tight word bounding box
(i.e., it may contain multiple words). In this case, however, the intuition behind the
classification problem underlying 푃(푣 | 푥) is unclear: How the (unique) “most likely
word” ˆ푣 = arg max푣 푃(푣 | 푥) should be interpreted? Moreover, 푃(푣 | 푥) sums up to
one for all 푣 ∈푉; but in keyword search, each word actually written in 푥 should have
high RP and, as discussed in Chapter 9, the sum should rather approach the expected
number of different words written in 푥.
In [31] (see also Chapter 6) we empirically study whether using 푃(푣| 푥) with line
image regions can still provide useful KWS performance. While the posterior un￾derlying any isolated word classifier can straightforwardly be used to obtain 푃(푣| 푥),
in the comparative experiments reported in [31] we tried to use the same underlying
probability distributions for all the methods. To this end, one can realize that 푃(푣| 푥)
can be readily obtained as a simple pixel-average of the posteriorgram as follows:
푃(푣 | 푥) =
Õ
푖 푗
푃(푣, 푖, 푗 | 푥) ≈ 1
퐼·퐽
Õ
푖 푗
푃(푣 | 푥, 푖, 푗) (3.8)
where 퐼· 퐽 is the number of pixels of 푥 and, for simplicity, possible positions (푖, 푗)
of words are assumed to be equiprobable.38 3 Probabilistic Indexing (PrIx) Framework
3.3.2 Proposed Approximations to 푷(푹 | 풙, 풗)
To start with, let the correct transcript of the sequence of words 푤 = 푤1, 푤2, . . . , 푤푛,
푤푘 ∈ 푉, 1 ≤ 푘 ≤ 푛, and let us abuse the notation and write 푣 ∈ 푤 to denote that
∃푘, 푤푘 = 푣.2 The definition of the class “1” in Eq. (1.2) can then be written as:
(푅 = 1) ≡ (푣 ∈ 푤) ≡ (푤1 =푣 ∨ 푤2 =푣 . . . ∨ 푤푛 =푣) (3.9)
Of course, if 푤 were known, the RP 푃(푅 | 푥, 푣) would trivially be 1 if 푣 ∈ 푤 and 0
otherwise. In KWS or PrIx, no transcripts are available, but an obvious, naive idea
is to approximate 푤 with a best HTR transcription hypothesis, ˆ푤(푥) (see Sec. 3.4).
This results in:
푃(푅 | 푥, 푣) ≈ (
1 if 푣 ∈ 푤ˆ(푥)
0 otherwise
(3.10)
While the simplicity of this idea makes it really enticing (and it has in fact become
quite popular), we anticipate that ˆ푤(푥) is seldom accurate enough in practice, and
this method generally yields poor precision-recall performance.
Therefore, we propose less simple but hopefully more accurate developments.
According to [29] and Eq. (3.9), 푃(푅 | 푥, 푣) can be exactly written as:
푃(푅 | 푥, 푣) =
Õ푛
푘=1
푃(푤푘 = 푣 | 푥)
−
Õ
푙<푘
푃(푤푘 = 푣, 푤푙 = 푣 | 푥)
+
Õ
푚<푙<푘
푃(푤푘 = 푣, 푤푙 = 푣, 푤푚 = 푣 | 푥)
. . .
(−1)
푛−1 푃(푤1 = 푣, . . . 푤푛 = 푣 | 푥) (3.11)
If the image regions are sufficiently small (e.g., line regions), it can reasonably
be expected that only one instance of each keyword may appear in each region. In
these cases, all the joint probabilities in Eq. (3.11) vanish and it simplifies to:
푃(푅 | 푥,푣) ≈ Õ푛
푘=1
푃(푤푘 =푣 | 푥)
def
=
Õ푛
푘=1
푃푘푣푥 (3.12)
where the terms 푃푘푣푥
def
= 푃(푤푘=푣 | 푥) have been introduced to simplify forthcoming
notation. A drawback of this approximation is that 푃(푅 | 푥, 푣) may become improper
since the sum can be greater than one if a keyword appears more than once in 푥.
To avoid this problem, rather than plainly ignoring the joint probabilities of
Eq. (3.11), they can be approximated by naive Bayes estimates:
2 In formal language theory, the clause 푣∈푤 is more usually written as 푤 = 푤
′
푣푤′′, where 푤
′
, 푤′′
are arbitrary word sequences, which may also be empty sequences.3.3 Position-independent PrIx 39
푃(푅 | 푥,푣) ≈ Õ푛
푘=1
푃푘푣푥
−
Õ
푙<푘
푃푘푣푥 푃푙푣푥
+
Õ
푚<푙<푘
푃푘푣푥 푃푙푣푥 푃푚푣푥
. . .
(−1)
푛−1 푃1푣푥 . . . 푃푛푣푥 (3.13)
Eq. (3.13) can be efficiently computed by dynamic programming according to the
following recurrence relation, which can be proved by simple induction:
푃(푅 | 푥, 푣) ≈ 푞(푛) (3.14)
where 푞(푘) =

푃1푣푥 if 푘 = 1
푃푘푣푥 + 푞(푘 − 1) (1 − 푃푘푣푥) if 푘 > 1
Finally, inspired by the Frechet’s bounds [ ´ 7, 10] (see also Sec. 3.7.4), another
approximation to Eq. (3.11) is proposed which does not suffer from the problem of
Eq. (3.12) and, moreover, is much simpler than Eq. (3.13/3.14).
푃(푅 | 푥,푣) ≈ max
1≤푘≤푛
푃푘푣푥 (3.15)
This approximation is intuitively appealing (see Figs. 3.2 and 3.4 for illustrations)
and, as will be seen in the experiments of Chapter 6, leads to one of the simplest and
most effective methods to obtain image region RPs.
3.3.3 Estimating Image-Region RPs from Posteriorgrams
In PrIx or in KWS no transcript of 푥 is available, but 푃푘푣푥 ≡ 푃(푤푘 = 푣 | 푥) can
be estimated from the posteriorgram for 푘 ∈ {1, 2, . . . }. To this end, we can divide
the whole region 푥, into 푛 (maybe slightly overlapping or disjoint) subregions or
blocks, 풃1, . . . 풃푘, . . . 풃푛, where a sufficiently high and wide (usually rather flat)
local maximum of 푃(푣 | 푥, 푖, 푗) is observed for some 푣 ∈ 푉 (see Fig. 3.2, where
푛 should be around 25, the number of likely words in 푥; or more concretely, the
unidimensional illustration of Fig. 3.4, where 푛 would be 9 or 10). Then:
푃푘푣푥 ≈ max
(푖, 푗) ∈풃푘
푃(푣 | 푥, 푖, 푗) (3.16)
This estimate can be used in any of the approximations given by the Eqs. (3.12),
(3.13/3.14) and (3.15), discussed in Sec. 3.3.2. Since only approximate local maxima
of 푃(푣| 푥, 푖, 푗) are required, simple maximum detection techniques can be used.
However, the performance of the resulting RPs will depend of one or two adjustable40 3 Probabilistic Indexing (PrIx) Framework
parameters (푛 and/or an adequate threshold) which need to be optimized using
validation data.
Eq. (3.15) deserves special attention. In this case, a closed-form expression for
the RP distribution can be straightforwardly derived:
푃(푅 | 푥, 푣) ≈ max
1≤푘≤푛
(푖, 푗) ∈풃푘
푃(푣 | 푥, 푖, 푗) = max
푖, 푗
푃(푣 | 푥, 푖, 푗) (3.17)
Interestingly, the number of subregions, 푛, and the subregions themselves, needed
in this derivation, finally become irrelevant and no extra adjustable parameters are
needed. On the other hand, since the maximization of Eq. (3.17) can be carried out
during the process of computing 푃(푣| 푥, 푖, 푗) itself, it does not add any computa￾tional cost to this process. Moreover, the precise more likely location of a spotted
word within a spotted region is straightforwardly obtained as a byproduct of the
maximization in Eq. (3.17).
One may argue that this approximation may be exceedingly rough, but it can be
empirically shown that it is very good in practice. Techniques discussed in Sec. 3.4
based on Eq. (3.24) can be used to compute (almost) exact RPs. In contrast to
Eq. (3.17), this approach does not allow to obtain precise locations of the spotted
words within the spotted regions and, moreover, it is much more complex and com￾putationally demanding. Nevertheless, for comparison purposes, we have afforded
it on the relatively small BEN1 Bentham test set described in Appendix C.2.1.
The results, summarized in Fig. 3.3, show that Eq. (3.17) provides practically exact
results for more than 99.5% of the (line) regions and words spotted in this dataset.
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
maxij P(R | x, v, i, j)
P(R | x, v)
1
10
100
1000
10000
100000
0 .08 .16 .24
Frequency
Approximation
error
Fig. 3.3: Correlation between exact and approximate RPs obtained for line regions and
words of theBenthamtest set. 푃(푅 | 푥, 푣) is generally well approximated by max푖 푗 푃(푅 |
푥, 푣, 푖, 푗) and the approximation error is almost null in more than 99.5% of the regions
and words spotted.3.3 Position-independent PrIx 41
The derivation of Eq. (3.17) from Eqs. (3.11), (3.15) and (3.16) provides a for￾mal explanation to a similar max heuristic which has been successfully used for
confidence estimation in several works of automatic speech and handwritten text
recognition [21, 22], and recently also for keyword indexing and search in [28].
The relative performance of all the proposed approximations was empirically stud￾ied in [31] (see also Chapter 6). The results confirm that a best choice is Eq. (3.17),
based on Eq. (3.15). This approach, not only provides (almost) the best approximation
to the ideal relevance probabilities (Eq. 3.11), but it also leads to simple implemen￾tations and, moreover, it straightforwardly allows to obtain accurate bounding boxes
for the spotted words within the spotted image regions.
3.3.4 Line-Region RP and 1-D Posteriorgram
At the beginning of Sec. 3.2, line image regions were suggested as particularly
adequate targets for keyword indexing and search. In connection with the discussion
in Sec. 3.1, the following two main advantages of these regions can be identified: A)
Line regions provide rich linguistic context which allows computing precise word
classification probabilities. And B) Line region posteriorgrams can be very efficiently
computed by smart choices of the sets of relevant marginalization boxes, B (푖, 푗) and
wise vertical subsampling.
For a line-shaped region, the relevant sets of marginalization boxes needed to
compute the posteriorgram according to Eq. (3.3) can be defined just by horizontal
segmentation. As discussed later, these sets can be accurately and efficiently obtained
as a byproduct of using a holistic, segmentation-free, context-aware handwritten
recognizer on the whole line image region.
Vertical subsampling, on the other hand, can be made by guessing a line height
and running, with some overlap, a vertical-sliding rectangular window of this height,
as in [1] (Fig. 3.2 was made in this way, with large overlap to achieve high vertical
resolution). However, in many cases, line detection and segmentation techniques [16,
23, 14, 9, 17, 19] can be accurate enough to allow for computation reductions and
improved precision. The possible lack of robustness of this approach can be alleviated
by means of over-segmentation [2].
In what follows of this section we regard (as in [13, 24, 25, 6, 5, 32, 28], among
others – see also [8]) line-shaped image regions as our target resolution level for
PrIx keyword indexing and search. As discussed in Sec. 3.3.3, once a line spot is
determined, the precise location(s) of the keyword searched for within the line can
also be obtained as a byproduct (or through postprocessing).
We assume that each handwritten page image has undergone basic preprocess￾ing steps including correction of overall page skew and other simple geometrical
distortions [20]. We do not need to assume that preprocessing includes any kind
of character or word segmentation. Line segmentation is not essentially needed ei￾ther [1] but, as discussed above, for effectiveness, efficiency and simplicity, text can
be assumed to be organized into distinguishable, roughly horizontal lines.42 3 Probabilistic Indexing (PrIx) Framework
A line image region 푥 can be processed “frame-wise” by extracting 푚 narrow
vertical boxes (or “frames”) at uniformly spaced horizontal positions. This way, 푥 can
be seen as a sequence of 푚 frames, effectively reducing 푥 to a 1-dimensional object.
The corresponding posteriorgram is also 1-D: 푃(푣 | 푥, 푖), 1 ≤ 푖≤ 푚, and Eq. (3.4)
becomes:
푃(푣 | 푥, 푖) =
Õ
풃∈S (푖)
푃(풃 | 푥, 푖) 푃(푣 | 푥, 풃) (3.18)
where S (푖) is the set of reasonably shifted and sized segments which contain the
frame 푖 and, as in Eq. (3.4), 푃(풃 | 푥, 푖) can be assumed uniform for all 풃 = (푏1, 푏2)
푇
such that 푏1 ≤ 푖 < 푏2 and replaced by an adequate constant.
A fairly complete real example of this kind of context-aware line image poste￾riorgram is shown in Fig. 3.4. As discussed previously, an important advantage of
line-level processing is that it allows to easily take into account the rich context
provided by words surrounding each query word.
 0.001
 0.01
 0.1
 1
 0 200 400 600 800 1000 1200 1400
vestidos con
con
con
trage de
de
cautivos s fue
un
del
hospital
despistado
don juan
Fig. 3.4: A 1-D posteriorgram obtained following the proposed approach, using a contex￾tual recognizer based on HMMs and bi-grams.
It is straightforward to rewrite the derivations and discussions of Sec. 3.3.3 to
obtain line-region RPs from 푃(푣 | 푥, 푖). In particular, Eq. (3.17) becomes:
푃(푅 | 푥,푣) ≈ max
푖
푃(푣 | 푥, 푖) (3.19)
3.4 Position-independent PrIx and KWS from a HTR Viewpoint
Many authors in the field of KWS consider that KWS and HTR are different problems
which require distinct methods. Aiming to shed light on this debate, KWS and PrIx
are now re-visited from a HTR point of view.3.4 Position-independent PrIx and KWS from a HTR Viewpoint 43
In Sec. 1.4 of Chapter 1, it was pointed out that KWS and PrIx essentially boil
down to answering the question: “is the word 푣 written in the text image region 푥?”.
As discussed in Sec. 3.3.2, a direct answer to this question would be to check whether
푣 appears in a word sequence 푤 which constitutes a transcript of 푥. But, since 푤 is
unknown, here it is considered the value of a new random variable, 푊, defined over
all the possible transcription hypotheses of 푥. This allows us to obtain the RP by
marginalization on 푊:
푃(푅 | 푥, 푣) =
Õ
푤
푃(푅, 푊 =푤 | 푥, 푣) ≡ Õ
푤
푃(푅, 푤 | 푥, 푣)
=
Õ
푤
푃(푅 | 푤, 푥, 푣) 푃(푤 | 푥, 푣) (3.20)
where 푤 ranges over the set of all sequences of words in 푉. Recall that our definition
of relevance is conditionally independent on the image itself when the transcript
is given; that is, 푃(푅 | 푥, 푣, 푤)
★
= 푃(푅 | 푣, 푤). In addition, we can assume that the
transcript of an image is conditionally independent on the query, when the image is
given; i.e. 푃(푤 | 푥, 푣)
★
= 푃(푤 | 푥). This assumption is actually true in practice, since
users can perform any kind of query, regardless of the text rendered in a particular
image. Therefore Eq. (3.20) simplifies to:
푃(푅 | 푥,푣) =
Õ
푤
푃(푅 | 푣, 푤) 푃(푤 | 푥) (3.21)
Now, we can split the sum over all possible 푤 into two disjoint sets of addends: those
for which 푣 ∈ 푤, and those for which 푣 ∉ 푤:
푃(푅 | 푥, 푣) =
Õ
푤:푣∈푤
푃(푅 | 푣, 푤) 푃(푤 | 푥) + Õ
푤:푣∉푤
푃(푅 | 푣, 푤) 푃(푤 | 푥) (3.22)
Observe that if 푤 is given, we know for sure whether the word 푣 is one of the words
in 푤, or not, and therefore:
푃(푅 | 푣, 푤) =

1 푣 ∈ 푤 (i.e., ∃푘 : 푤푘 = 푣)
0 otherwise
(3.23)
Thus, the RP finally simplifies to:
푃(푅 | 푥, 푣) =
Õ
푤:푣∈푤
푃(푤 | 푥) (3.24)
That is, now the RP can be properly computed using the probability that an arbitrary
word sequence 푤 is the transcript of the image region 푥. Interestingly, 푃(푤 | 푥) is
exactly the same distribution used by modern HTR systems, which provide a most
likely transcript of the given text image region 푥 according to the minimum Bayes
error criterion [3] (see Sec. 1.5 of Chapter 1); that is:
푤ˆ = arg max
푤
푃(푤 | 푥) (3.25)44 3 Probabilistic Indexing (PrIx) Framework
Note , however, that the sum of Eq. (3.24) considers multiple HTR decoding
hypotheses, not just the best one defined by Eq. (3.25). Each of these hypotheses
entails a corresponding word segmentation hypothesis, which implicitly provides
the marginalization boxes (segments in this 1-D case) discussed in Sec- 3.3.4.
Fig. 3.5 illustrates the application of Eq. (3.24) to a rather extreme case where the
underlying transcript of a text image is somehow ambiguous.
not / 0.8 all / 1.0 foxes / 1.0
no / 0.2
tall / 1.0 foxes / 1.0
Fig. 3.5: An image 푥 with two likely transcripts: 푤 = “not all foxes” and 푤
′ = “no tall
foxes”. Each is represented by a complete path through a weighted directed acyclic graph,
depicted below the image. The posteriors are the product of the weights through the path,
푃(푤 | 푥) = 0.8 and 푃(푤
′
| 푥) = 0.2.
On the one hand, the person that wrote the text of Fig. 3.5 may have intended to
write “not all foxes”, but she did not put enough space after the word “not”. On the
other hand, she perhaps intended to write “no tall foxes” and forgot to leave enough
space after the word “no”. In any case, one might argue that the first transcript is
more likely than the second one, since the adjective “tall” usually refers to people,
and foxes are animals not particularly tall after all. Eq. (3.24) can now be applied to
compute the RP of 푥 for any given query word:
푃(푅 | 푥, “not”) = 푃(푤 | 푥) = 0.8
푃(푅 | 푥, “no”) = 푃(푤
′
| 푥) = 0.2
푃(푅 | 푥, “all”) = 푃(푤 | 푥) = 0.8
푃(푅 | 푥, “tall”) = 푃(푤
′
| 푥) = 0.2
푃(푅 | 푥, “foxes”) = 푃(푤 | 푥) + 푃(푤
′
| 푥) = 1.0
3.4.1 Comparing the Image Processing and HTR Viewpoints
It is worth to compare the computation of the RP of a line image region, based on
Eq. (3.24) and word-sequence posterior probabilities, with the use of Eqs. (3.19, 3.18)
for posteriorgram-based computation of the same RP.
In the derivation of Eq. (3.18), words were considered as small visual objects to be
detected and recognized. Then, the computation of 푃(푅 | 푥, 푣) relied on single-word
posterior probabilities, generally given by an isolated-word recognition model, and
marginalized on a suitable set of word-sized segments of the given line image region.3.5 Position-dependent PrIx 45
In contrast, Eq. (3.24) relies on whole line transcript posterior probabilities, pro￾vided by holistic HTR models, and marginalizes on these transcription hypotheses.
Clearly, as compared with the local processing underlying the computation of word￾level posteriors, a holistic line image region processing offers much better or simpler
opportunities to leverage contextual lexical and syntactic regularities and constraints.
This generally leads to more accurate models and higher PrIx performance.
While methods are proposed in [28, 31] (briefly discussed later in Sec. 5.2) that
cleverly use HTR byproducts to indirectly obtain the single-word posteriors and
marginalization segments needed for Eq. (3.18), Eq. (3.24) clearly allows for more
direct and flexible RP computing, as discussed below.
Therefore, unless noticed otherwise, the HTR viewpoint will be the preferred one
in the rest of this book.
3.5 Position-dependent PrIx
So far in this Chapter, we have assumed that the RP of an image region 푥 for a word
푣 is all we need for PrIx and/or KWS. We have referred to such scenario as one in
which the RP was position-independent.
Clearly, if 푥 is sufficiently small, this is in fact all what is needed for most
information search applications. However, when indexing text images, it is desirable
to make it possible that IR and search applications provide the users not only with
a rough region where the query words are likely to appear, but also with the precise
position of each word within 푥. Moreover, if a word 푣 appears more than once in 푥, it
is desirable that the system provides all likely positions of 푣 in 푥, not just that of the
one most probable instance. To support this enhanced approach to PrIx, the concept
of position-dependent RP is introduced here the for different types of positions.
That is, in this section, we will study PrIx scenarios where we wish to determine
whether or not a given keyword, 푣, is written at a specific position of interest, 풃,
within the image region 푥. This is especially useful when the 푥 contains a significant
amount of text, such as a long line or a full page.
Notice that we are using the same vector notation 풃 for a position as we have used
for a bounding box or a segment in previous sections. This allows it to represent
arbitrarily defined positions. For instance, a position could be just the coordinates of
a pixel within the image region, or we could represent the left and right horizontal
coordinates of a text line region or segment – in these cases, 풃 ∈ N
2
. If the image
region 푥 contains multiple text lines we may want to denote a position as a conven￾tional 2-D word bounding box, that is, 풃 ∈ N
4
. Finally, we may want to represent
not a geometrical position within the image, but an ordinal position along the image
transcript, with, 풃 ∈ N. Specifically, here we will study three cases for the meaning
of 풃 within a text-line image region 푥:46 3 Probabilistic Indexing (PrIx) Framework
1. 풃 ∈ N represents a horizontal coordinate of 푥. In this case, we will just denote
the position as 푖.
2. 풃 ∈ N
2
represents a geometric segment (or interval) of 푥. In this case, we will
assume that we work with horizontally-oriented text and that the components
푏1, 푏2 of 풃 represent the left and right boundaries of the interval, respectively.
3. 풃 ∈ N represents a “logical” or ordinal position along a transcription hypothesis
of 푥. In this case, we will simply refer to the position using the variable 푘.
It is worth emphasizing that these definitions can be easily extended to fully
segmentation-free scenarios where 푥 represents a full page image, but we decided to
rely on the line-level assumption for practical reasons: modern methods for text line
detection and segmentation are quite robust and it is very common to work at this
level when processing handwritten documents.
3.5.1 Relevance of an Horizontal Coordinate Position
In this scenario, we want to determine whether or not a keyword 푣 is written at
a particular horizontal coordinate 푖 of the text line image 푥. This is similar to the
problem statement discussed in Sec.3.1 and to approaches studied e.g. in [11, 28].
We say that a word is written at 푖 if the tightest bounding box of the word rendering
in 푥 includes 푖. Fig. 3.6 gives an example to better understand the idea.
푖 푖
′
푖
′′
Fig. 3.6: Example showing two relevant horizontal coordinates (푖
′
and 푖
′′) of the given
image for the keyword “foxes”, and a non-relevant horizontal coordinate (푖). Intuitively, any
horizontal coordinate of the image belonging to the bounding box of the keyword within
the image will be relevant.
We can capture this intuition in a more formal statement:
푅 | 푥, 푣, 푖 def
=
(
1 푣 is written at horizontal coordinate 푖 of the line image 푥
0 otherwise (3.26)
As we did in previous sections, we need to formalize slightly better what we mean
by “a word is written at a horizontal coordinate”. To this end, in addition to using
a random variable 푊 to represent the distribution of transcription hypotheses, now
we need to introduce the concept of “segmentation” or alignment of a transcript 푤
of an image 푥 with (horizontal) coordinates of 푥 where each word in 푤 is rendered.
Alignments can be represented in many different ways. Here we introduce an addi￾tional random variable 퐴, over sequences of natural numbers. For a certain sequence
푤 = 푤1, . . . , 푤푛 with an arbitrary number of words 푛, let 푎 = 푎1, . . . , 푎푛, 푎푛+1 be a
sequence of horizontal coordinates or “word boundaries”. Each 푎푘 denotes the left
boundary of the word 푤푘 and 푎푛+1 is the right boundary of the last word 푤푛.3.5 Position-dependent PrIx 47
Now, following similar steps as in Eq. (3.20), we can marginalize over the random
variables 푊 and 퐴, to obtain an expression for the RP of 푥 for 푖 and 푣:
푃(푅 | 푥, 푣, 푖) =
Õ
푤
Õ
푎
푃(푅 | 푥, 푣, 푖, 푤, 푎) 푃(푤, 푎 | 푥, 푣, 푖)
★
=
Õ
푤
Õ
푎
푃(푅 | 푣, 푖, 푤, 푎) 푃(푤, 푎 | 푥) (3.27)
where, as before, we have assumed that the relevance does not depend on the image
itself when its transcript and its alignment are given, and that the transcript and its
alignment only depend on the image itself, not on the keyword or the horizontal
coordinate that the user may be interested in. Also, according to Eq. (3.26):
푃(푅 | 푣, 푖, 푤, 푎) =
(
1 ∃푘 : 푤푘 = 푣, 푎푘 ≤ 푖 < 푎푘+1
0 otherwise
(3.28)
That is, if both 푤 and 푎 are given, we know for sure whether the word 푣 matches some
word 푤푘 in 푤 and 푖 lies within segment [푎푘, 푎푘+1) assigned to 푤푘 by the alignment,
or not. Therefore, Eq. (3.27) simplifies to:
푃(푅 | 푥,푣, 푖) =
Õ
푤:
∃푘:푤푘=푣
Õ
푎:
푎푘 ≤ 푖<푎푘+1
푃(푤, 푎 | 푥) (3.29)
In short, this is the sum of all the joint probabilities of transcripts and alignments,
such that the transcript includes the word 푣 at some position, and the alignment of
that word includes the horizontal coordinate 푖. As discussed in Sec. 1.5 (Chapter 1),
it is exactly the joint probability 푃(푤, 푎 | 푥) the distribution used in HTR to obtain
a feasible approximation to the optimal transcript of 푥.
Now, we can use this equation to plot on top of a text line image, a “heat-map”
representing the RP of each horizontal coordinate of the image for a given keyword,
similar to the posteriorgram in Fig. 3.4 (and also the 2-D posteriorgram of Fig. 3.2).
For instance, consider again the example of Fig. 3.5. In this case, the weighted
directed acyclic graph will not only represent the text line image transcription hy￾potheses, but also their likely alignments. Each path through the new graph will
correspond to a pair (푤, 푎), where 푤 is a transcription hypothesis and 푎 its corre￾sponding alignment, and the weight of this path will represent the value of joint
probability 푃(푤, 푎 | 푥). Fig. 3.7 shows the new graph, with the heat-map for the
keyword “all” superimposed on the image. The graph edges are labeled with words
and weights, as before, and now the alignments are given by the initial horizontal
coordinate of each word segment, depicted at the departing node of each edge3.
According to Eq. (3.29), in order to compute the RP of an image horizontal
coordinate, 푖, for a given keyword 푣, we need to sum the posterior probability of
all alignments (i.e. paths in the graph) where the word written on top of horizontal
coordinate 푖 is equal to 푣. In the figure above, consider the horizontal coordinate
푖 = 45 (the process will be identical for any 푖 ∈ [31, 58)), and let’s consider the
3 This kind of graph is often called “Word Graph” or “Lattice”. See Chapter 4 and Sec. 3.8.48 3 Probabilistic Indexing (PrIx) Framework
0.7 0.8 0.1
0 100
31
28
23
20
64
58
not / 0.1
not / 0.7
no / 0.01
no / 0.19
all / 0.7
all / 0.3
all / 0.3
all / 0.7
tall / 1
tall / 1
foxes / 1
foxes / 1
Fig. 3.7: Heat map representing the RP of the image horizontal coordinates for the
keyword “all”. It was computed using the joint probabilities of transcription hypotheses and
alignments of the image, 푃(푤, 푎 | 푥), represented by the weighted directed acyclic graph,
(edges are labeled with words and probabilities and the numbers in the nodes represent
the alignments). In the heat-map, RP is represented with colors (green for highest, red for
lowest). For instance, for all the horizontal coordinates 푖 ∈ [31, 58), the RP is 0.8.
keyword “all”. There are four edges in the graph that traverse the horizontal coordinate
푖 = 45. We will denote each edge by the tuple (departing state, ending state, word,
probability). For each of them we compute its contribution to the sum in Eq. (3.29):
• (28, 58, all, 0.7), probability: 0.7 · 0.7 · 1.0 = 0.49
• (28, 64, all, 0.3), probability: 0.7 · 0.3 · 1.0 = 0.21
• (31, 64, all, 0.3), probability: 0.1 · 0.3 · 1.0 = 0.03
• (31, 58, all, 0.7), probability: 0.1 · 0.7 · 1.0 = 0.07
Thus, the RP of 푖 = 45 for the keyword “all” is the sum for the four edges, 0.8 –and
the same is value is obtained for any 푖 ∈ [31, 58), as shown in the figure.
3.5.2 Relevance of a Segment of Text-line Image Region
In the previous section, we presented a probability distribution that allowed us to
draw a “heat-map” in order to represent the likely location(s) of a given keyword in
a line image region. In this section, we will present an alternative distribution which
may be more useful in practice, when the aim is to build a search index (i.e., a PrIx)
of the words likely written in a collection of text images.
Specifically, we aim to determine whether or not the keyword 푣 is written exactly
in a particular segment 풃 of the text line 푥, delimited by horizontal coordinates 푏1
and 푏2; i.e., 풃 = (푏1, 푏2)
푇
. Intuitively, our definition of relevance would be:3.5 Position-dependent PrIx 49
푅 | 푥,푣, 풃
def
=
(
1 푣 is written between horizontal coordinates 푏1 and 푏2 of 푥
0 otherwise (3.30)
As in the previous section, we represent the alignments of the transcripts with the
text line image using again the random variables 푊 for transcripts 푤 = 푤1, . . . , 푤푛
and 퐴 for transcript alignments 푎 = 푎1, . . . , 푎푛, 푎푛+1. Then we follow the same steps
as before, to compute the RP of a text line segment 풃 as:
푃(푅 | 푥, 푣, 풃) =
Õ
푤
Õ
푎
푃(푅 | 푥, 푣, 풃, 푤, 푎) 푃(푤, 푎 | 푥, 푣, 풃)
★
=
Õ
푤
Õ
푎
푃(푅 | 푣, 풃, 푤, 푎) 푃(푤, 푎 | 푥)
=
Õ
푤,푎:
∃푘:푤푘=푣
(푎푘 ,푎푘+1 )= 풃
푇
푃(푤, 푎 | 푥) (3.31)
where, according to Eq. (3.30) we have taken into account that:
푃(푅 | 푣, 풃, 푤, 푎) =
(
1 ∃푘 : 푤푘 = 푣, (푎푘, 푎푘+1)
푇 = 풃
0 otherwise
(3.32)
Using Eq. (3.31) in the example of Fig. 3.7, now we can retrieve, for instance, the
likely segments where the word “all” is written, and their RPs (Table 3.1).
Table 3.1: Likely segments where the word “all” is written in the example of Fig. 3.7.
Segment RP Segment RP
[28, 58) 0.49 31–58 0.07
[28, 64) 0.21 31–64 0.03
From Table 3.1 one can notice that multiple alignment possibilities for the same
word instance tend to dilute the total probability mass. Intuitively, we would like to
provide the user with the multiple instances of the keyword within the text line, with
their best (or expected) alignment (i.e. segment).
To do so, one could try to sum the probabilities of all the overlapping segments
and keep the interval with the highest probability. Nevertheless, this may become
problematic since it is possible that multiple instances of the same word appear in the
text line with some overlapping segments. Fig. 3.8 (“Original graph”) highlights this
problem using (part of) a famous proverb from the Greek philosopher Parmenides
of Elea (born circa 515 BC).
Segment overlap can be avoided to some extent by merging adequate states and
edges of this graph. An idea to achieve this is to determinize the graph so that all the
paths whose corresponding word sequences are the same become merged (this will
be discussed in more detail in Appendix B, Sec.B.3 and Sec. 4.7.4 of Chapter 4).
For the above example, the result is shown in the bottom graph of Fig. 3.850 3 Probabilistic Indexing (PrIx) Framework
0 100
9
31
33
31
33
62
78
81
Original graph
t / 0.05
that / 0.35
that / 0.5
Hat / 0.1
hat / 0.4
hat / 0.6
that / 1
that / 1
that / 1
that / 1
is / 0.7
is / 0.3
is / 1
is / 1
0 100
9
33
31
33
62
78
Determinized & Normalized version
t / 0.05
that / 0.79
Hat / 0.16
hat / 1
that / 1
that / 1
that / 1
is / 1
is / 1
Fig. 3.8: Example showing that multiple instances of the same keyword (e.g. “that”) may
lead to overlapping segments in the graph of transcription hypotheses. Punctuation marks
are excluded to simplify the graph of transcription and segmentation hypotheses. The set of
hypotheses includes the correct transcript, “that that is is”, along with other hypotheses like
“Hat that is is” or “t hat that is is”. Notice the multiple instances and multiple hypotheses
of the word “that”. The hypothesis in the interval [0, 33) overlaps with another hypothesis
of the same instance in the interval [0, 31], as will as with another hypothesis of another
instance in the interval [31, 62). A determinized and normalized version of the graph is
also shown (see Chapter 4, Sec. 4.7 for details), where some overlaps have been resolved
by the merging operations entailed by determinization. Although the example may seem
artificial, it comes from Parmenides’ proverb: “That that is, is. That that is not, is not.”
Now we can compute the RP of some words and segments for the image of this
example, both using the original graph and its determinized and normalized version.
The results for two words (“that”, “is”) are shown in Table 3.2.
Table 3.2: Example highlighting the differences between segment RPs computed according
to Eq. (3.31) using the original set of alignments (given by the original graph of Fig. 3.8),
and those given by a determinized and normalized graph version (also in Fig. 3.8). RPs
computed using the determinized and normalized graph are more “concentrated”.
RP
Word Segment Original Det.&Norm.
that [0, 31) 0.35 0.00
that [0, 33) 0.50 0.79
that [31, 62) 0.47 0.16
that [33, 62) 0.53 0.84
RP
Word Segment Original Det.&Norm.
is [62, 78) 0.70 1.00
is [62, 81) 0.30 0.00
is [78, 100) 0.70 1.00
is [81, 100) 0.30 0.003.5 Position-dependent PrIx 51
As discussed later (Sec. 3.7.2), 푃(푅 | 푥, 푣, 푖) (Eq.3.29), can be computed from
푃(푅 | 푥,푣, 풃) (Eq. 3.31) by adding the RP of all the segments 풃 that contain the
horizontal coordinate 푖. On the other hand, the RP introduced in this section allows
us to extract other useful information, such as the statistical expectation of the number
of instances of a given keyword are written in the given text line (see Chapter 9).
3.5.3 Relevance of a Transcript Ordinal Position
As a better alternative to the distribution given by Eq. (3.31), in this section we pro￾pose a new relevance probability conditioned on the line image region 푥, the keyword
푣, and the ordinal position within the transcription word sequence. Intuitively, we
want to decide whether or not the 푘-th word (word at position 푘) of the text depicted
in the line image 푥 is relevant for the keyword 푣. Thus, intuitively, we define this
concept of relevance as:
푅 | 푥, 푣, 푘 def
=
(
1 푣 is the 푘-th word written in 푥
0 otherwise
(3.33)
In order to compute 푃(푅 | 푥, 푣, 푘), we marginalize again over 푊, representing the
transcription hypotheses of the text line 푥, and we obtain:
푃(푅 | 푥, 푣, 푘) =
Õ
푤
푃(푅 | 푥, 푣, 푘, 푤) 푃(푤 | 푥, 푣, 푘)
★
=
Õ
푤
푃(푅 | 푣, 푘, 푤) 푃(푤 | 푥) =
Õ
푤:푤푘=푣
푃(푤 | 푥) (3.34)
where, according to Eq. (3.33), we have taken into account that:
푃(푅 | 푣, 푘, 푤) =
(
1 푤푘 = 푣
0 otherwise
(3.35)
In short, we need to compute the sum of posterior probabilities of all the transcription
hypotheses 푤 of the text line 푥, such that the keyword, 푣, appears at the 푘-th position.
As an illustration of the differences between the segment RP, presented in
Sec. 3.5.2, and the ordinal position RP presented above, refer to Table 3.3. It shows
ordinal position RPs computed with Eq. (3.35) for the same text image and the same
words as in Fig. 3.8 and Table 3.2. The table also reports the RPs obtained with the
determinized and normalized version of the graph shown in Fig. 3.8 (bottom). In
contrast with segment RPs, in the case of ordinal position relevance the differences
with respect to those obtained using the original graph are much less significant.52 3 Probabilistic Indexing (PrIx) Framework
Table 3.3: Example highlighting the differences between ordinal position RPs computed
according to Eq. (3.34) using the original set of alignments (given by the original graph of
Fig. 3.8), and those given by a determinized and normalized graph version (also in Fig. 3.8).
Observe that, since in all transcription hypotheses (graph paths) there is an instance of the
word “is” at the ordinal position 4, the RP of the spot (“is”, 4) is 1.0. In contrast, the spots
(“is”, 3) and (“is”, 3) are not exactly 1.0 or 0.0. Unlike the segment RPs shown in Table 3.2,
the reported ordinal position RPs are much more “concentrated”, even with the original,
not determinized graph.
RP
Word Position Original Det.&Norm.
that 1 0.85 0.79
that 2 0.95 0.95
that 3 0.05 0.05
RP
Word Position Original Det.&Norm.
is 3 0.95 0.95
is 4 1.00 1.00
is 5 0.05 0.05
While some differences between segment and ordinal RPs are significant, it will
be shown later that they are much smaller in practice.
Eq. (3.34) is a promising tool for simple position-dependent PrIx, but an important
drawback is that it does not provide any explicit hint as to where a 푘-th word of the
transcription hypothesis of an image region may lay on the image. As discussed
above, in many PrIx IR applications, users typically expect the system to provide
not only the location of spotted image regions within the image collection, but also the
likely location of each keyword instance within this region. However, as discussed in
Sec. 3.7.3, it is easy to use ordinal-position RPs to compute the expected boundaries
of the segment where a given word is likely to appear.
3.6 Query-by-Example Paradigm
As already mentioned in Sec. 2.2.3, many researchers in the field of KWS tradi￾tionally focused on the Query-by-Example (QbE) paradigm where the query is not
presented in a textual form, but as an example image 푦 of some keyword. So far,
however, it was assumed that the keyword 푣 was given as a discrete symbol of a
vocabulary (or a sequence of discrete symbols of some alphabet).
Nevertheless, a proper probabilistic formulation for QbE can also be derived. The
key idea is to assume that some keyword 푣 is actually written in the query image 푦;
but, in contrast with the QbS formulation, it is unknown. So now 푣 is not given (it is
“hidden”) and should thereby be considered the value of a random variable (in fact
the same random variable 푄 introduced in Sec. 3.1).
3.6.1 Position-independent RPs for Query by Example KWS
First, a position independent form of RP is derived for a given image region 푥 and an
also given query image 푦. The expression for such RP is 푃(푅 = 1 | 푋 = 푥, 푌 = 푦),
where 푌 is a new random variable whose values are query images 푦. As in previous
sections, to simplify notation, this RP should be just written as 푃(푅 | 푥, 푦).3.6 Query-by-Example Paradigm 53
To derive a useful way of computing this RP, we can rely on marginalization over
all possible values of the hidden random variable 푣:
푃(푅 | 푥, 푦) =
Õ
푣
푃(푅 | 푥, 푦, 푣) 푃(푣 | 푥, 푦) (3.36)
Now, it can safely be assumed that transcript 푣 of the query image 푦 does
not depend on the text image region 푥; only on the query image itself. Likewise,
푃(푅 | 푥, 푦, 푣)
★
= 푃(푅 | 푥, 푣) because 푅 is conditionally independent of the image 푦 if
its transcript 푣 is given. So we can resort again to marginalization on all possible
transcripts 푤 of the image region 푥 (as in Eq. 3.20), to get:
푃(푅 | 푥, 푦) =
Õ
푣
Õ
푤
푃(푅 | 푣, 푤) 푃(푤 | 푥) 푃(푣 | 푦) (3.37)
As discussed earlier, observe that, according to the definition of relevance in
Eq. (3.23), 푃(푅 | 푣, 푤) = 1 for all the transcripts that include the query word 푣
(i.e. 푣 ∈ 푤, using the notation introduced earlier). Likewise, for all the transcripts
that not include the keyword, this probability will be zero. Thus, taking into account
also Eq. (3.24), Eq. (3.37) becomes::
푃(푅 | 푥, 푦) =
Õ
푣
Õ
푤:푣∈푤
푃(푤 | 푥) 푃(푣 | 푦) =
Õ
푣
푃(푣 | 푦)
Õ
푤:푣∈푤
푃(푤 | 푥)
=
Õ
푣
푃(푣 | 푦) 푃(푅 | 푥, 푣) (3.38)
This equation implies that in order to compute the relevance probability in the
QbE scenario (i.e. given a text image region 푥 and a query image 푦), we simply need
to compute the sum of the QbS relevance probabilities, for all possible transcripts of
the query image, weighted by their posteriors 푃(푣 | 푦). This becomes even simpler
if the sum in Eq. (3.38) is approximated by its dominating term:
푃(푅 | 푥, 푦) ≈ max
푣
푃(푣 | 푦) 푃(푅 | 푥, 푣) (3.39)
This computationally lighter approximation has empirically proved in [30] to produce
results similar to those of Eq. (3.38) (see Sec.6.10.2).
3.6.2 Position-dependent RPs for Query by Example KWS
The other relevance probabilities conditioned on some position that were presented
in Sec. 3.5 can also be straightforwardly adapted to the QbE scenario using the same
marginalization approach. In summary, the position-dependent relevance probabili￾ties under the QbE paradigm are:54 3 Probabilistic Indexing (PrIx) Framework
• Relevance of an image column:
푃(푅 | 푥, 푦, 푖) =
Õ
푣
푃(푣 | 푦) 푃(푅 | 푥, 푣, 푖)
| {z }
(A)
(3.40)
• Relevance of an image segment:
푃(푅 | 푥, 푦, 풃) =
Õ
푣
푃(푣 | 푦) 푃(푅 | 푥, 푣, 풃)
| {z }
(B)
(3.41)
• Relevance of a transcript position:
푃(푅 | 푥, 푦, 푘) =
Õ
푣
푃(푣 | 푦) 푃(푅 | 푥, 푣, 푘)
| {z }
(C)
(3.42)
Where (A), (B) and (C) are defined as in Eqs.(3.27), (3.31) and (3.34), respectively.
3.7 Relations among Position-Dependent and Independent RPs
We have introduced several RPs concerning different types of objects that may be
interesting for PrIx and IR.
1. 푃(푅 | 푥, 푣) can be used to retrieve any type of text image region, depending how
푥 is defined: individual segmented words, text lines, paragraphs or full pages.
2. 푃(푅 | 푥, 푣, 푖) can be used to retrieve individual horizontal coordinates within
text line images.
3. 푃(푅 | 푥, 푣, 풃) could retrieve word segments within a text line image.
4. 푃(푅 | 푥, 푣, 푘) can retrieve word positions within transcription hypotheses from
an arbitrary text image region (either a text line, or a paragraph, as long as its
transcript admits a sequential representation).
One might wonder what is the relationship between these RPs.4 Or, even more
practical, how can we use one of them to approximate or bound another.
For instance, suppose that we are designing a PrIx system to retrieve relevant text
lines given a keyword. Then, the obvious choice would be to use 푃(푅 | 푥, 푣), with 푥
representing an individual text line. However, if we want to show the user where the
keyword was spotted within the text line, we need to use a different probability to get,
for instance, the segment with the highest RP (i.e. 푃(푅 | 푥, 푣, 풃)). However, these
4 Notice that the same symbol 푅 has been used so far to denote the different relevance random
variables defined in Eqs. (3.23, 3.26, 3.30, 3.33). Strictly speaking, they should be denoted with
different symbols, such as, e.g., 푅, 푅푖
, 푅풃 , 푅푘. However, for the sake of simplicity, we will
continue to use just the basic notation, 푅, assuming that the strict meaning will be understood from
the context.3.7 Relations among Position-Dependent and Independent RPs 55
two probabilities answer two different questions. But, can we use one to address the
other with some guarantees?
In answering this question, we shall see useful connections between position
independent and dependent RPs and posteriorgrams and expected values of two
types of positions, as well as various upper and lower bounds among the different
RPs discussed in previous sections.
3.7.1 Equivalences of Positional RPs and other Posterior Probabilities
First we show that a (1-D) posteriorgram 푃(푣 | 푥, 푖), as defined in Eq. (3.18), can be
given by the RP 푃(푅 | 푥, 푣, 푖) of Eq. (3.29). To this end we proceed by marginalization
on any word 푣
′ which may appear in any transcript of 푥:5
푃(푅 | 푥, 푣, 푖) =
Õ
푣
′
푃(푅, 푣′
| 푥, 푣, 푖) =
Õ
푣
′
푃(푅 | 푥, 푣, 푖, 푣′
) 푃(푣
′
| 푥, 푣, 푖)
★
=
Õ
푣
′
푃(푅 | 푣, 푣′
) 푃(푣
′
| 푥, 푖) = 푃(푣 | 푥, 푖) (3.43)
where we have assumed that 푣
′
and 푣 are independent, 푅 is conditionally independent
of 푥 and 푖 given 푣, and it has be taken into account that 푃(푅 |푣, 푣′
) = 1 iff 푣 = 푣
′
.
On the other hand, we show that the segment RP 푃(푅 | 푥, 푣, 풃) given by Eq. (3.31)
is also the word posterior probability for the (word-sized) image region defined by
the given segment or bounding box 풃, 푃(푣 | 푥, 풃):
푃(푣 | 푥, 풃) =
Õ
푤,푎
푃(푤, 푎, 푣 | 푥, 풃) =
Õ
푤,푎
푃(푤, 푎 | 푥, 풃) 푃(푣 | 푥, 푤, 푎, 풃)
★
=
Õ
푤,푎:
∃푘:푤푘=푣
(푎푘 ,푎푘+1 )= 풃
푇
푃(푤, 푎 | 푥) ≡ 푃(푅 | 푥, 푣, 풃) (3.44)
We have assumed that 푃(푤 | 푥, 풃) is conditionally independent of 풃 given 푥, 푃(푣 |
푥, 푤, 푎풃) is conditionally independent of 푥 given 푤, 푎 and 풃, and 푃(푣 | 푤, 푎, 풃) is 1
iff ∃ 푘 : 푤푘 = 푣, (푎푘, 푎푘+1) = 풃
푇
(0 otherwise).
Finally, we show that the ordinal position RP 푃(푅 | 푥, 푣, 푘) given by Eq. (3.35) is
also the word posterior probability for a given ordinal position 푘, 푃(푣 | 푥, 푘):
푃(푣 | 푥, 푘) =
Õ
푤
푃(푤, 푣 | 푥, 푘) =
Õ
푤
푃(푤 | 푥, 푘) 푃(푣 | 푥, 푤, 푘)
★
=
Õ
푤:푤푘= 푣
푃(푤 | 푥) ≡ 푃(푅 | 푥, 푣, 푘) (3.45)
5 Beware that 푣 and 푣
′
are values of two different random variables56 3 Probabilistic Indexing (PrIx) Framework
Here it has been assumed that 푃(푤 | 푥, 푘) is conditionally independent of 푘 given 푥,
푃(푣 | 푥, 푤, 푘) is conditionally independent of 푥 given 푤 and 푘, and 푃(푣 | 푤, 푘) is 1
iff 푤푘 = 푣 (0 otherwise).
3.7.2 Computing Horizontal Coordinate RP from Segment RP
The RP of a horizontal coordinate 푖, 푃(푅 | 푥, 푣, 푖), can be obtained as a sum of the
RPs 푃(푅 | 푥, 푣, 풃) for all the segments 풃 which include 푖. This is directly derived
from Eqs. (3.29) and (3.31):
From Eq. (3.29): 푃(푅 | 푥,푣, 푖) =
Õ
푤,푎:∃푘,푤푘=푣
푎푘 ≤ 푖<푎푘+1
푃(푤, 푎 | 푥)
From Eq. (3.31): Õ
풃:
푏1≤푐≤푏2
푃(푅 | 푥,푣, 풃) =
Õ
풃:
푏1≤푐≤푏2
Õ
푤,푎:∃푘,푤푘=푣
(푎푘 ,푎푘+1 ) = 풃
푇
푃(푤, 푎 | 푥)
Therefore: 푃(푅 | 푥, 푣, 푖) =
Õ
풃:푏1≤ 푖≤푏2
푃(푅 | 푥, 푣, 풃) (3.46)
Eqs. (3.43) and (3.46) connect segment RPs with the computation of the (1-D)
posteriorgram by marginalization on bounding boxes discussed in Secs. 3.1 and 3.3.4.
3.7.3 Expected Values of Segments and Ordinal Positions
Here we obtain the expected values of the boundaries (푏1, 푏2) ≡ 풃
푇 of a segment
where a given word 푣, which is would appear in a transcript ordinal position 푘, is
likely to be found in the image 푥:
E[풃 | 푥, 푣, 푘] =
Õ
풃
푃(풃 | 푥, 푣, 푘) 풃 =
Õ
풃
풃
Õ
푤,푎
푃(풃, 푤, 푎 | 푥, 푣, 푘)
=
Õ
풃
풃
Õ
푤,푎
푃(풃, 푤, 푎, 푣 | 푥, 푘)
푃(푣 | 푥, 푘)
=
Õ
풃
풃
Õ
푤,푎
푃(푤, 푎 | 푥, 푘)
푃(푣 | 푥, 푘)
푃(풃,푣 | 푥, 푤, 푎, 푘) (3.47)
According to Eq. (3.45), 푃(푣 | 푥, 푘) = 푃(푅 | 푥, 푣, 푘). In addition, observe that:
푃(풃, 푣, | 푥, 푤, 푎, 푘)
★
= 푃(풃, 푣 | 푤, 푎, 푘) =
(
1 푤푘 = 푣, (푎푘, 푎푘+1) = 풃
푇
0 otherwise
(3.48)
Therefore, using Eq. (3.48), assuming 푃(푤, 푎 | 푥, 푘) is conditionally independent
of 푘 given 푥, and using Eq. (3.31), Eq. (3.47) becomes:3.7 Relations among Position-Dependent and Independent RPs 57
E[풃 | 푥,푣, 푘]
★
=
1
푃(푅 | 푥, 푣, 푘)
Õ
풃
풃
Õ
푤,푎:
∃푘:푤푘=푣
(푎푘 ,푎푘+1 )= 풃
푇
푃(푤, 푎 | 푥)
=
1
푃(푅 | 푥, 푣, 푘)
Õ
풃
풃 푃(푅 | 푥, 푣, 풃) (3.49)
Similar steps lead to the following expression for the expected value of the ordinal
position 푘 of a given word 푣 in a given segment 풃 of an image 푥:
E[푘 | 푥, 푣, 풃] =
1
푃(푅 | 푥, 푣, 풃)
Õ
푘
푘 푃(푅 | 푥, 푣, 푘) (3.50)
Tables 3.4 and 3.5 show, respectively, examples of expected segment boundaries
and ordinal word positions obtained using Eq. (3.49) and Eq. (3.50) for the example
of Fig. 3.8. The RPs are the same as those already reported in Tables 3.2 and 3.3.
Table 3.4: Examples of expected values of segment boundaries computed from ordinal
position RPs using Eq. (3.49) for the example of Fig. 3.8.
Original graph Determ. & Normaliz.
Word position RP E[segment] RP E[segment]
that 1 0.85 [0.0, 32.2) 0.79 [0.0, 33.0)
that 2 0.95 [32.1, 62.0) 0.95 [32.7, 62.0)
that 3 0.05 [32.2, 62.0) 0.05 [33.0, 62.0)
is 3 0.95 [62.0, 78.9) 0.95 [62.0, 78.0)
is 4 1.00 [78.1, 98.9) 1.00 [77.2, 98.9)
is 5 0.05 [78.9, 100.0) 0.05 [78.0, 100.0)
Table 3.5: Examples of expected values of word ordinal positions computed from segment
RPs using Eq. (3.50) for the example of Fig. 3.8. Note that, in this case, the expected ordinal
positions are very close to those in the corresponding maximum probability paths of the
graphs of Fig. 3.8.
Original graph Determ. & Norm.
Word segment RP E[ord. pos.] RP E[ord, pos.]
that [0, 31) 0.35 1.00 0.00 –
that [0, 33) 0.50 1.00 0.79 1.00
that [31, 62) 0.47 2.04 0.16 2.00
that [33, 62) 0.53 2.07 0.84 2.06
is [62, 78) 0.70 3.05 1.00 3.05
is [62, 81] 0.30 3.05 0.00 –
is [78, 100) 1.00 4.05 1.00 4.05
is [81, 100) 1.00 4.05 0.00 –58 3 Probabilistic Indexing (PrIx) Framework
3.7.4 RP Inequalities Based on Frechet Bounds ´
As we mentioned in Sec. 3.3, 푃(푅 | 푥, 푣) represents the RP independently of any
position of interest, while the rest of RPs are conditioned on different types of
locations (horizontal coordinates, horizontal coordinate intervals, or word positions).
Now, let’s assume that we are dealing with an arbitrary enumerable type of
location6, represented by the random variable 퐵. Then, we can interpret the position￾independent RP as a disjunctive probability over all possible locations. That is, the
position-independent relevance is equal to the position-dependent relevance of the
first location, or the second, or the third location, etc.
Indeed, recall that the position-independent relevance definition was:
푅 | 푥, 푣
def
=
(
1 푣 is written somewhere in 푥
0 otherwise
which, if the possible positions are 풃1, . . . , 풃푛, is equivalent to:
푅 | 푥, 푣
def
=



1



푣 is written at position 풃1 of 푥, OR
푣 is written at position 풃2 of 푥, OR
· · ·
푣 is written at position 풃푛 of 푥
0 otherwise
Fig. 3.9 shows a simplification of the generic position-independent and position￾dependent PrIx scenarios. In the figure, the big box represents the whole image
region, 푥, which contains three different locations 풃1, 풃2 and 풃3. In each location,
the possible transcript of each location is represented by the squares and circles and
their probability is represented by the fraction of squares/circles. For instance, in the
first location the probability of the word “circle” is equal to 1
6
.
For the sake of simplicity, in the example we will assume that the transcript of
each location is independent of the others. Thus, the transcript of each location would
be given by selecting a square/circle in each box. However, it is extremely important
to realize that this independence assumptions is obviously false in real scenarios. For
instance, when the positions represent horizontal coordinates of an image, the word
aligned to neighbor horizontal coordinates is obviously not independent. Likewise,
when the positions represent segments of the image, the text aligned to overlapping
and contiguous segments is also not independent.
Now, back to Fig. 3.9, we might wonder what is the RP of the word “circle” in
a specific location. For instance, 푃(푅 | 푥, “circle”, 풃1) = 푃(푊 =“circle” | 푥, 풃1) =
1
6
.
Likewise, the probabilities 푃(푅 | 푥, “circle”, 풃2) =
1
6
and 푃(푅 | 푥, “circle”, 풃3) =
3
6
.
On the other hand, according to our position-independent relevance, 푃(푅 |
푥, “circle”) is the probability that the transcript of at least one box is equal to “circle”.
6 Notice that all the types of positions described in Sec. 3.5 are actually enumerable and finite for a
given image (although the set of possible locations may be very large).3.7 Relations among Position-Dependent and Independent RPs 59
푥
풃1 풃2 풃3
Fig. 3.9: Example of the relationship between position-independent and position-dependent
RPs. A text region is represented by the biggest box, 푥, and several positions within
the region are presented by the smallest boxes (i.e. 풃1, 풃2, and 풃3). The squares and
circles inside each box represent the possible transcripts of the corresponding position.
Thus, the probability that the first position is relevant for the keyword “circle” is equal to
푃(푅 | 푥, “circle”, 풃1 ) =
1
6
. However, the probability that the whole region is relevant for
the keyword is equal to 푃(푅 | 푥, “circle”) =
47
72 ≈ 0.7. The Frechet inequalities tell us that ´
3
6
= 0.5 ≤ 푃(푅 | 푥, “circle”) ≤ 5
6
≈ 0.8.
This is equal to one minus the probability that none of the boxes’ transcripts is equal
to “circle”. Thus, 푃(푅 | 푥, “circle”) = 1 −
5
6
·
5
6
·
3
6
=
47
72 ≈ 0.7.
In our particular example, the position-independent RPs can be computed easily
from the position-dependent ones, because we assumed that the transcript of each
position was independent from the others. When this assumption does not hold, one
can use the Frechet inequalities of the logical disjunction ´ to find bounds for the
position-independent relevance.
In general, given a set of 푛 logical propositions {퐴푖
: 1 ≤ 푖 ≤ 푛} (e.g. the 푖-th
position of 푥 is relevant for the keyword 푣), the Frechet inequalities [ ´ 7, 10] are:
• Logical disjunction:
max
푖
푃(퐴푖) ≤ 푃(퐴푖 ∨ · · · ∨ 퐴푛) ≤ min 
1,
Õ
푖
푃(퐴푖)

(3.51)
• Logical conjunction:
max 
0,
Õ
푖
푃(퐴푖) − (푛 − 1)

≤ 푃(퐴푖 ∧ · · · ∧ 퐴푛) ≤ min
푖
푃(퐴푖) (3.52)
If Eq. (3.51) is applied to any pair of position-dependent and position-independent
probability distributions, we have:
max
풃
푃(푅 | 푥, 푣, 풃) ≤ 푃(푅 | 푥, 푣) ≤ min 
1,
Õ
풃
푃(푅 | 푥, 푣, 풃)

(3.53)
where 풃 is assumed to be any type of “position” as discussed in Sec. 3.5.
The lower bound on the RP has already been used in previous works on KWS ad
PrIx, as an heuristic to compute relevance scores for text lines [28, 26]. Moreover,
they will be also used in Sec. 8.1 of Chapter 8 to support multi-word, boolean
AND/OR/NOT queries using single-word PrIxs.60 3 Probabilistic Indexing (PrIx) Framework
Fig. 3.10 shows the relationship among the different RPs introduced earlier.
Frechet ´ inequalities give different lower bounds for 푃(푅 | 푥, 푣). The proofs of the
bounds in this figure are fairly straightforward. As an example, we sketch here only
the proof for:
max
풃
푃(푅 | 푥, 푣, 풃) ≤ max
푖
푃(푅 | 푥, 푣, 푖)
Let’s consider the segment 풃 ≡ (푏1, 푏2)
푇 with the highest RP for a fixed text
image 푥 and keyword 푣. According to Eq. (3.31), 푃(푅 | 푥, 푣, 풃) is equal to the sum
of the probabilities of all possible transcripts of 푥 such that the keyword 푣 starts at
horizontal coordinate 푏1 and ends at 푏2.
Similarly, 푃(푅 | 푥, 푣, 푖) is equal to the sum of the probability of all possible
transcripts of 푥 where the keyword 푣 is written in a segment containing the horizontal
coordinate 푖 (see Eq. (3.29)).
Now, suppose that a segment 풃
′ ≡ (푏
′
1
, 푏′
2
)
푇 overlaps with 풃, where the keyword
푣 is also written. For all horizontal coordinates in the overlapping region, their RP
will be greater than that of the individual segments (see Eq. (3.29)). And, obviously,
their RP is lower than or equal to the maximum among all horizontal coordinates.
푃(푅 | 푥, 푣)
푃(푅 | 푥, 푣, 푖)
푃(푅 | 푥, 푣, 풃)
푃(푅 | 푥, 푣, 푘)
max푖 ≤
max풃 ≤ max푖
max푘 ≤
Fig. 3.10: Diagram of the relationship among the different 4 RPs for a fixed text image
푥 and keyword 푣. The inequalities should be read in the direction of the arrows, e.g.,
max푘 푃(푅 | 푥, 푣, 푘) ≤ 푃(푅 | 푥, 푣). Note that the remarks of footnote 4
also apply here.
3.8 PrIx Implementation Foreword
The PrIx framework has been presented in this chapter according to two different but
related points of view. First an image-processing-oriented approach, where words
are essentially considered as small visual objects, has been discussed in Secs. 3.1–
3.4. Then the remaining sections have been devoted to present in greater detail
another viewpoint based on HTR models and concepts. In both cases, an essential
component is a word recognizer. However, while in the first case such a recognizer
is needed to compute single-word posterior probabilities, in the second we rely on
whole line-image transcript posteriors derived from a holistic HTR recognizer.3.8 PrIx Implementation Foreword 61
As previously commented, a recognizer working at a whole image region has
better opportunities to leverage contextual linguistic constraints than another which
just focus on individual words. In addition, as also previously noted, the approach
based on HTR models and concepts is more versatile and generally allows for
greater accuracy. Nevertheless, individual word posterior probabilities can also be
derived as byproducts of an HTR holistic processing of an image region. And this
has in fact been exploited in most of our works on posteriorgram-based KWS and
PrIx [28, 27, 31].
Implementation aspects of all the PrIx approaches discussed in this chapter
will be presented in Chapter 5, largely relaying on HTR models and methods de￾scribed in Chapter 4. A crucial computational tool is the word lattice or Word Graph
(WG) [15, 28], which can be obtained as a byproduct of solving Eq. (3.25) (or, more
specifically, Eq. (1.5) of Chapter 1) [20, 28]. A WG of a (line) image region, is a
very compact representation of a huge amount of alternative image transcription
results, including the probability of each of the (millions of) hypothesized words
and the corresponding word segmentation boundaries. WGs are very versatile and,
as discussed in Chapters 4 and 5, can easily provide all the probabilities required for
any of the PrIx formulations developed in this chapter. This makes it unnecessary
to use any kind of isolated word recognizer, even for posteriorgram-oriented PrIx.
Simplified WGs have already been used throughout Sec. 3.5 (Figs. 3.7 and 3.8), to
support examples of computations and properties of the different RPs.
An important issue which needs to be discussed before finishing this chapter is
how to actually detect the “spots” to be indexed through the proposed PrIx methods.
All the formulation of this chapter aims at computing the probability of a relevance
binary random variable, given a keyword and its location. For example, 푃(푅 | 푥, 푣, 풃)
is the probability that the location specified by the given (line) image region 푥 and
the geometric position 풃 within 푥 are relevant for the keyword 푣.
So the question arises as to how to determine which image regions, words and
specific positions have to be indexed along with the corresponding RPs computed
with the equations discussed in this Chapter.
Regarding the image regions 푥, in Sec. 3.2 we have argued that text-line regions,
which can be reliably detected using modern line detection techniques, are perhaps
the most adequate indexable image regions. So, it remains the question of which
words and specific positions within a line region have to be indexed. Again, as we
explain in Chapter 5 the WG of a line image provides a great deal of word sequence
and alignment hypotheses, from which all types of word and position hypotheses
can be easily derived.
To close this chapter, Fig. 3.11 shows the key components and methods com￾mented above and illustrates the proposed implementation workflow to produce the
PrIxs of a set of text images.62 3 Probabilistic Indexing (PrIx) Framework
Transcribed
images (GT)
Probabilistic Indexing (PrIx)
Text images
Optical & Lang.
 Models
Training
Computing spots:
 words, RPs & 
 Boinding boxes
Contextual word
recognizer (HTR)
char/word
 lattices
Page-level
PrIxs
Fig. 3.11: PrIx workflow. RPs and word bounding boxes are derived form word lattices
obtained using HTR optical and language models trained from GT transcribed images.References 63
References
1. Barrere, K., Toselli, A.H., Vidal, E.: Line segmentation free probabilistic keyword spotting and
indexing. In: Iberian Conf. on Pattern Recognition and Image Analysis, pp. 201–217. Springer
(2019)
2. Bluche, T., Moysset, B., Kermorvant, C.: Automatic line segmentation and ground-truth align￾ment of handwritten documents. In: Proc. of 14th ICFHR, pp. 667–672 (2014)
3. Duda, R.O., Hart, P.E.: Pattern Classification and Scene Analysis. J. Wiley and Sons, (1973)
4. En, S., Petitjean, C., Nicolas, S., Heutte, L., Jurie, F.: Region proposal for pattern spotting in
historical document images. In: Proc. of 15th ICFHR, pp. 367–372. IEEE (2016)
5. Fischer, A., Keller, A., Frinken, V., Bunke, H.: Lexicon-free handwritten word spotting using
character HMMs. Pattern Recognition Letters 33(7), 934 – 942 (2012). DOI 10.1016/j.patrec
.2011.09.009. Special Issue on Awards from ICPR 2010
6. Frinken, V., Fischer, A., Manmatha, R., Bunke, H.: A Novel Word Spotting Method Based on
Recurrent Neural Networks. IEEE Transactions on Pattern Analysis and Machine Intelligence
34(2), 211–224 (2012). DOI 10.1109/TPAMI.2011.113
7. Frechet, M.: G ´ en´ eralisation du th ´ eor ´ eme des probabilit ` es totales. Fundamenta Mathematicae ´
25(1), 379–387 (1935). URL http://eudml.org/doc/212798
8. Giotis, A.P., Sfikas, G., Gatos, B., Nikou, C.: A survey of document image word spotting
techniques. Pattern Recognition 68, 310–332 (2017)
9. Gr ¨uning, T., Leifert, G., Strauß, T., Michael, J., Labahn, R.: A two-stage method for text line
detection in historical documents. International Journal on Document Analysis and Recognition
(IJDAR) 22(3), 285–302 (2019)
10. Hailperin, T.: Best possible inequalities for the probability of a logical function of events. The
American Mathematical Monthly 72(4), 343–359 (1965)
11. Hazen, T.J., Shen, W., White, C.: Query-by-example spoken term detection using phonetic
posteriorgram templates. In: 2009 IEEE Workshop on Automatic Speech Recognition Under￾standing, pp. 421–426 (2009). DOI 10.1109/ASRU.2009.5372889
12. He, K., Gkioxari, G., Dollar, P., Girshick, R.: Mask r-cnn. In: Proceedings of the IEEE ´
international conference on computer vision, pp. 2961–2969 (2017)
13. Kolcz, A., Alspector, J., Augusteijn, M., Carlson, R., Viorel Popescu, G.: A line-oriented
approach to word spotting in handwritten documents. Pattern Analysis & Applications 3,
153–168 (2000)
14. Oliveira, S.A., Seguin, B., Kaplan, F.: dhsegment: A generic deep-learning approach for
document segmentation. In: 2018 16th International Conference on Frontiers in Handwriting
Recognition (ICFHR), pp. 7–12. IEEE (2018)
15. Ortmanns, S., Ney, H., Aubert, X.: A word graph algorithm for large vocabulary continuous
speech recognition. Computer Speech & Language 11(1), 43–72 (1997)
16. Prusty, A., Aitha, S., Trivedi, A., Sarvadevabhatla, R.K.: Indiscapes: Instance segmentation
networks for layout parsing of historical indic manuscripts. In: 2019 International Conference
on Document Analysis and Recognition (ICDAR), pp. 999–1006. IEEE (2019)
17. Quiros, L.: Multi-task handwritten document layout analysis. Preprint arXiv:1806.08852 ´
(2018)
18. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object detection with
region proposal networks. IEEE Trans. on PAMI 39(6), 1137–1149 (2017)
19. Renton, G., Soullard, Y., Chatelain, C., Adam, S., Kermorvant, C., Paquet, T.: Fully convolu￾tional network with dilated convolutions for handwritten text line segmentation. International
Journal on Document Analysis and Recognition (IJDAR) 21(3), 177–186 (2018)
20. Romero, V., Toselli, A.H., Vidal, E.: Multimodal Interactive Handwritten Text Transcription.
Perception and Artif. Intell. (MPAI). World Scientific, (2012)
21. Sanchis, A., Juan, A., Vidal, E.: A word-based naive bayes classifier for confidence estimation in
speech recognition. IEEE Trans. on Audio, Speech, and Language Processing 20(2), 565–574
(2012)64 3 Probabilistic Indexing (PrIx) Framework
22. Serrano, N., Gimenez, A., Civera, J., Sanchis, A., Juan, A.: Interactive handwriting recognition ´
with limited user effort. International Journal on Document Analysis and Recognition (IJDAR)
17(1), 47–59 (2014)
23. Studer, L., Alberti, M., Pondenkandath, V., Goktepe, P., Kolonko, T., Fischer, A., Liwicki, M.,
Ingold, R.: A comprehensive study of imagenet pre-training for historical document image
analysis. In: 2019 International Conference on Document Analysis and Recognition (ICDAR),
pp. 720–725. IEEE (2019)
24. Terasawa, K., Tanaka, Y.: Slit Style HOG Feature for Document Image Word Spotting. In: 2009
10th International Conference on Document Analysis and Recognition, pp. 116–120 (2009).
DOI 10.1109/ICDAR.2009.118
25. Thomas, S., Chatelain, C., Heutte, L., Paquet, T.: Alpha-Numerical Sequences Extractaction
in Handwritten Documents. In: Proc. of ICFHR, pp. 232–237 (2010)
26. Toselli, A.H., Puigcerver, J., Vidal, E.: Two Methods to Improve Confidence Scores for Lexicon￾Free Word Spotting in Handwritten Text. In: 2016 15th International Conference on Frontiers
in Handwriting Recognition (ICFHR), pp. 349–354 (2016)
27. Toselli, A.H., Vidal, E., Puigcerver, J., Noya-Garc´ıa, E.: Probabilistic multi-word spotting in
handwritten text images. Pattern Analysis and Applications (2019)
28. Toselli, A.H., Vidal, E., Romero, V., Frinken, V.: HMM Word-Graph Based Keyword Spotting
in Handwritten Document Images. Information Sciences 370(C), 497–518 (2016). DOI
10.1016/j.ins.2016.07.063
29. Ventsel, H.: Theorie des Probabilit ´ es. ´ Editions MIR. Moscow, (1973) ´
30. Vidal, E., Toselli, A.H., Puigcerver, J.: High performance Query-by-Example keyword spotting
using Query-by-String techniques. In: 2015 13th International Conference on Document
Analysis and Recognition (ICDAR), pp. 741–745 (2015). DOI 10.1109/ICDAR.2015.7333860
31. Vidal, E., Toselli, A.H., Puigcerver, J.: Lexicon-based probabilistic indexing of handwritten
text images. Neural Computing and Applications pp. 1–20 (2023)
32. Wshah, S., Kumar, G., Govindaraju, V.: Statistical script independent word spotting in offline
handwritten documents. Pattern Recognition 47(3), 1039–1050 (2014). DOI https://doi.org/
10.1016/j.patcog.2013.09.019Chapter 4
Probabilistic Models for Handwritten Text
Abstract As discussed in the previous chapter, PrIx (and KWS), can be fruitfully
seen under a handwritten text recognition (HTR) viewpoint. So, this chapter reviews
segmentation-free approaches to HTR and the corresponding probabilistic models.
Pre-processing steps used in traditional HTR workflows, such as handwriting style
normalization and hand-crafted feature extraction, are briefly outlined. More recent
techniques like Convolutional Neural Networks for feature extraction learning, and
neural-network-based text line detection, are reviewed in greater detail. Probabilis￾tic “optical models” for HTR are characterized by modeling simultaneously textual
contents and geometric positions (alignment) of handwritten strokes on images. We
review Recurrent Neural Networks (RNN) and, in particular, the Long-Short Term
Memory RNN model, along with the, so called, Connectionist Temporal Classifica￾tion loss function, which are currently considered state of the art. We also review the
time-honored Hidden Markov Models, not only because of their sound formalization
and paradigmatic role in the in the field of HTR, but also because they are still
useful nowadays as a convenient asset to combine all types of optical models with
linguistic constraints related with lexicon and syntax. These constraints have tradi￾tionally been modeled with 푁-grams, which can bust HTR (and PrIx) performance,
sometimes very significantly. 푁-grams are also thoroughly reviewed, including the
way they can seamlessly be integrated with HMMs or RNNs using Weighted Finite
State Transducers and the corresponding automata algebra. Finally, Word Graph (or
lattice) concepts and methods are explained in full detail since, as it was foretold in
the previous chapter, these graphs constitute a key tool for PrIx computation.
4.1 Traditional Image Preprocessing and Feature Extraction
Here we explain how the scanned page images are first processed in order to extract
meaningful regions containing text and to reduce the variability of these text regions
that is not useful to read their content. This is mainly aimed at HMM optical modeling
and recognition. As we shall see later, newer optical modeling methods such as the
Convolutional-Recurrent NNs (CRNN), require less of these steps.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 65
A. H. Toselli et al., Probabilistic Indexing for Information Search and Retrieval 
in Large Collections of Handwritten Text Images, The Information Retrieval Series 49, 
https://doi.org/10.1007/978-3-031-55389-9_4 
 66 4 Probabilistic Models for Handwritten Text
4.1.1 Image Preprocessing and Text Segmentation
Page-level reprocessing steps that can be useful for any subsequent modeling and
recognition approach, include the (attempt to) removal of bleed-through and other
kinds of ink noise, contrast normalization, and correction of rotated pages. Some
of these steps play a less significant role under controlled environments, but they
can be very important if the page images are not obtained using adequate scanning
equipment (e.g. if they are obtained using a hand-held camera or a cell phone).
First the different text regions in the document are located. Text regions are
typically formed by several text lines with a meaningful (sequential) reading order.
This problem is, by itself, quite challenging in real applications, mainly due to
the ambiguity of the concept “text region”. A survey on traditional approaches for
structure analysis of text documents is given in [62]. Nevertheless, as in many other
key aspects of text documents processing, methods based on ANN have become the
state-of-the-art in recent years [15, 17, 16, 43].
Next, individual text lines are isolated, which may be an even more difficult
task and is considered an open research problem, as shown by recent competi￾tions [24]. Common problems are the presence of diacritics, ascenders and descen￾ders non-horizontal lines, overlapping lines, etc. Works [57, 59] report a compari￾son on different approaches to tackle line segmentation, and recent relevant works
include [33, 84, 20]. Text line segmentation is considered a critical step in the pro￾cessing of historical handwritten images, since virtually all approaches, including
the models presented in this chapter, assume that text lines have been (more or less)
accurately segmented.
Fig. 4.1 shows three examples from different data sets used in text recognition
and KWS research. The easiest scenario, represented by the IAM dataset (see Ap￾pendix C.1) is depicted in Fig. 4.1 (a), already exhibits some ambiguity: one could
just separate the handwritten zones from the printed text areas, but one could also
argue that the printed areas are separated in three regions: header, description and
footer. Nevertheless, text line segmentation is quite straightforward in this collection,
since text lines are well separated by blank regions.
On the contrary, Figs. 4.1 (b) and 4.1 (c) show two data sets where text region
detection and line segmentation are considerably more challenging, since the main
text body is filled with annotations (see Fig. 4.1 (b)), and there are dozens of close-by
text regions (one for each cell in Fig. 4.1 (c)) with few text lines in each.
4.1.2 Text Line Normalization
Once the page images have been segmented into individual text lines, several nor￾malization steps can be applied to reduce handwriting variability, as discussed above.
Text lines seldom follow a strict horizontal (or vertical) orientation, especially in
handwritten documents written without constraints. Plus, quite often, handwritten4.1 Traditional Image Preprocessing and Feature Extraction 67
(a) IAM (b) Inquisicion´
(c) Passau
Fig. 4.1: Page images from different collections used for handwritten text recognition and
keyword spotting experiments.(a) shows a page from the IAM dataset (Appendix C.1),
with a quite easy structure since all pages in the collection are formed by two main text
regions: printed and handwritten text; (b) shows a page from the “Inquisicion” dataset, ´
where annotations are interleaved with the main text of the page; and (c) shows a page
from the Passau collection (Appendix C.6), which contains many tables and records with
a particular structure.
characters tend to be inclined towards the left or the right, producing the italic effect.
The first distortion is known as line skew, whereas the latter is known as slant.
To correct the skew of a text line, some methods estimate a single angle for the
whole text line. For instance, a popular approach is based on the horizontal projection
profile of the text line [86, 105]. A survey can be found in [47]. Other authors detect
several regions within the text line to apply different skewing corrections in each
segment [96, 104, 71].
Regarding the slant correction, virtually all methods find the angle between the
vertical axis of the text line and the strokes of text in the image. Then, an affine (shear)68 4 Probabilistic Models for Handwritten Text
transformation is applied to correct the angle. For instance [105, 70] try a range of
slant angles and choose the one that maximizes the variance of the histogram of the
pixel intensities.
Finally, in may also be convenient to normalize the size of the writing, since
it may vary significantly between different writers, documents, or even parts of
the same page. This normalization usually includes the height of the text, as well
as its thickness. A common solution is to scale all text line images to a fixed
height, preserving the aspect ratio. This works well when the height of the text line
is proportional to the height of the characters, but when the lines have not been
accurately segmented, this approach may be troublesome. Another approach that
has been proposed more recently is to use the first and second order moments over a
sliding window, to re-position the center of gravity in each window at the center of
the image and re-scale the size so that the second order moments are constant [55].
Fig. 4.2 depicts two examples of skewed and slanted text lines and the normalized
images after using the normalization software1 employed in some of the experiments
of Chapter 6.
(a) (b)
(c) (d)
Fig. 4.2: Fig. (a) shows a portion of a text page which includes skewed (and also slanted)
text lines. Fig. (b) shows a heavily slanted text. Figs. (c) and (d) show the text lines of each
segment after normalization.
4.1.3 Feature Extraction
Traditional HTR models need, as input, a sequence of vectors, each representing
local visual features at a given horizontal position of a line image. To this end, a
sequence of handcrafted feature vectors has to be extracted from the raw image
pixels. Although this is no longer needed for modern CRNN methods (as discussed
in Sec. 4.4), feature extraction is still needed to operate with traditional HMMs.
Many feature extraction alternatives have been proposed in the past. Here we
outline some of the most popular. It is extremely important to notice that the features
1 https://github.com/mauvilsa/textfeats4.3 Hidden Markov Models 69
employed are very often strongly dependent of the image processing steps carried
out before, meaning that if one processes the images in a different way than the one
proposed by the author of a feature extraction technique, the overall performance
may be poor (e.g. some features need the images to be binarized).
Many early methods relied on counting black and/or white pixels (after binariza￾tion). Works such as [63, 65, 7] use the number of black pixels in each image column;
and in [63, 7] the number of transitions between black and white pixels is used.
In addition to these simple features, higher-level features have also been used
in the literature. For instance, the average gray level, along with the horizontal and
vertical Gray-level derivatives, in rectangular cells of the image are used in [98, 30].
Other common features in the Computer Vision field such as Speeded Up Robust
Features (SURF) [6] and Scale-Invariant Feature Transform (SIFT) [60], have also
been used in the context of handwritten text modeling [109, 82].
4.2 Optical Modeling
In Chapter 3, we saw that one of the approaches to RP estimation, needed to tackle
PrIx in a principled way involves, essentially, the conditional probability over the
image transcripts. The term “Optical Modeling” is used to refer to the way this
probability is modeled and how to estimate the corresponding model parameters.
In this chapter, two families of models used to represent the text written in images
(and its alignment) are reviewed: Hidden Markov Models (HMMs) and Recurrent
Neural Networks(RNNs) with aConvolutionalfront-end stack (CRNN). Both models
have been widely and successfully used, both for HTR and PrIx
Note that we will not deal with Transformer models for optical modeling, which
are currently a hot topic of research and we consider it is still premature as a
consolidated technology in the HTR field. More importantly, for these models no
direct way is known at the moment to map the recognized characters/words onto
their geometric positions in the input images. According to Chapter 3, this is an
essential need for PrIx which, as we will see, is easily provided by HMMs with
Viterbi decoding and CRNN with CTC.
4.3 Hidden Markov Models
The concept of Hidden Markov Model (HMM) was introduced in the mid 20th.
century to model a sequence of states of some process such that, at each state, the
process produces a certain outcome. The outcomes are observable, but the states at
which these outcomes are produced are not. Therefore the state sequence is said to
be hidden, which gives the name to the model [49].70 4 Probabilistic Models for Handwritten Text
4.3.1 Description
Formally speaking, a HMM is probabilistic graphical models of the joint likelihood
푝(푋1 = 풙1, . . . , 푋푇 = 풙푇, 푆1 = 푠1, . . . , 푆푇 = 푠푇), where 푇 is an arbitrary integer, and
푆푖 and 푋푖
, 푖 = 1, . . . , 푇 are random variable sequences representing sequences of
HMM states and observed values emitted at these states, respectively. Even though
푇 is assumedly given, it is important to emphasize that, in general, the model does
not restrict in any way the length of the sequences, i.e. 푇 ∈ [1, ∞). To simplify
notation, we will write the observed-value and state random variable sequences
as 푋1:푇 ≡ 푋1, . . . , 푋푇 and 푆1:푇 ≡ 푆1, . . . , 푆푇, or just as 푋 and 푆 if 푇 can be
understood. Observable values are assumed to be multidimensional vectors (as usual
with continuous-density HMMs), and 푄, 푄′
and 푂, will be used as needed to denote
arbitrary random variables among 푆푖 and 푋푖
, 푖 = 1, . . . , 푇, respectively.
A HMM is thus typically defined using the following items:
• A set of emitting states Q and a special (non-emitting) final state F ∉ 푄.
Elements of Q will be denoted with the letter 푞 (and also 푝 or 푟, if needed).
• A probability distribution over initial states:2 푃(푆1 = 푞), ∀푞 ∈ Q.
• A probability distribution modeling the transition between two consecutive
states, 푞, 푞′
: 푃(푄
′ = 푞
′
| 푄 = 푞), ∀푞 ∈ Q, ∀푞
′ ∈ Q∪{F}.
• And a probability mass or density function describing the likelihood that an
observed value is emitted in each non-final state: 푝(푂 =풙 | 푄 =푞), ∀푞 ∈ Q.
Fig. 4.3 shows an example of ergodic HMM with three emitting states. The name
ergodic simply denotes that any possible transition between two states has a non-zero
probability mass.
We will denote a sequence of 푇 observed values as 풙1:푇 ≡ 풙1, . . . , 풙푇, or just 푥 if
it does not lead to ambiguity. Such a sequence is emitted by a sequence of 푇 states,
denoted as 푠1:푇 ≡ 푠1, . . . , 푠푇, or just 푠 for short, where it is assumed that there is
always a (푇 + 1)-th state 푠푇+1
def
= 퐹. Two assumptions are made for HMM state and
observation sequences:
1. The sequence of states is described by a first-order Markov process:
푃(푠1:푇)
★
= 푃(푠1)
Ö
푇
푡=2
푃(푠푡
| 푠푡−1) 푃(F | 푠푇) (4.1)
2. The value of the 푡-th observation only depends on the the state 푠푡
, which implies:
푝(풙1:푇 | 푠1:푇)
★
=
Ö
푇
푡=1
푝(풙푡
| 푠푡) (4.2)
2 It is possible to include the final state F in the set of possible initial states, in order to allow
sequences of length 0, but this is not the norm.4.3 Hidden Markov Models 71
a
푃(a)
b
푃(b)
c
푃(c)
F
푃(b | a)
푃(a | b)
푃(c | a
푃
)
(a | c)
푃(c | b)
푃(b | c)
푃(F | a)
푃(F | b)
푃(F | c)
푝(풙 | a)
푝(풙 | b)
푝(풙 | c)
Fig. 4.3: An example of a HMM. States labeled a, b, c ∈ Q and F, are represented by
circles, with the initial state probability 푃(푆1 = 푞), 푞 ∈ Q, written in them (observe
that the final state does not have an initial probability). Solid arcs represent the state
transitions between pairs of states 푞, 푞′ ∈ Q, with the corresponding transition probabilities
푃(푄′ = 푞
′
| 푄 = 푞). Dashed arcs represent emitted observations with the corresponding
density distributions 푝(푂 = 풙 | 푄 = 푞) for each emitting state, 푞 ∈ Q.
Note that the probability and density distributions do not depend on any particular
value of 푡, only on the state. That is:
∀푡, 푡′
∈ N, ∀풙 : 푠푡 = 푠푡
′ ⇒ 푃(풙 | 푠푡) = 푃(풙 | 푠푡
′ )
∀푡, 푡′
∈ N, ∀푞 ∈ Q : 푠푡 = 푠푡
′ ⇒ 푃(푞 | 푠푡) = 푃(푞 | 푠푡
′ )
(4.3)
Under these assumptions:
푝(풙1:푇, 푠1:푇) = 푝(풙1:푇 | 푠1:푇) 푃(푠1:푇)
★
=
Ö
푇
푡=1
푝(풙푡
| 푠푡) 푃(푠1)
Ö
푇
푡=2
푃(푠푡
| 푠푡−1) 푃(F | 푠푇) (4.4)
HMMs are sometimes called generative models, since they model the likelihood
of observed values, and can then be sampled to generate observation sequences.
4.3.2 HMM Training
Without hidden states (i.e., observation values are deterministically associated with
states), Markov models are called Markov chains. In this case, the state transition
distribution can be easily trained following the Maximum Likelihood Estimation
(MLE) criterion, by just computing the frequency of each transition for the training
observation sequences. However, because the hidden states, HMM training becomes72 4 Probabilistic Models for Handwritten Text
significantly more complex, requiring Expectation–Maximization methods for Gen￾erative Training and other sophisticated techniques for Discriminative Training. This
will be reviewed later, as applied to training HMM optical models.
4.3.3 HMMs for Optical Modeling in Handwritten Text Recognition
In order to reduce the number of free parameters of the model (and thus, reduce the
chances of training overfitting), some structural decisions are made in practice for
modeling handwritten text.
Typically, HMMs adopt a left-to-right topology, as depicted in Fig. 4.4. In a left￾to-right topology, each emitting state has two transitions: a self-loop and a transition
to the next state. Some authors add skip transitions so that samples shorter than the
number of emitting states can be processed by the HMM (otherwise, these sequences
would have a null likelihood) [28, 93].
F
0.7
0.3
0.4
0.6
0.4
0.6
0.6
0.4
Fig. 4.4: Example of the alignment produced by a character HMM modeling the letter “a”.
The HMM is composed of four states in a left-to-right topology, where only the leftmost
state has non-null initial-state probability. The values in the arcs represent the HMM
transition probabilities, 푃(푞 | 푞
′
), including the final-state probability for 푞 = F.
As depicted in Fig. 4.4, most works that need to model handwritten text choose to
use an individual HMM to represent each character in the alphabet. There are some
works that represent full words by a single HMM, however this becomes problematic
when dealing with vocabularies of a large number of words, since the number
of parameters required to estimate grows significantly. Notice that the number of
characters of the Latin and other western alphabets is typically in the 10–100s, while
the number of words that one typically encounters in large collections of texts is in
the 10 000–100 000s. This is very similar to the works in speech recognition, where
they typically use an individual HMM to represent a phoneme (or a tri-phoneme),
since this is the fundamental unit of speech.4.3 Hidden Markov Models 73
The number of states in each HMM can be fixed (i.e. all characters have the same
number of states), or can be variable, since the length of each character is expected
to be different from one class to the other. For instance characters like “i” or “l” are
typically much shorter (horizontally) than characters like “m” or “n”.
Finally, Gaussian Mixture Models (GMM) with diagonal covariance matrices are
the probability density functions most commonly used to model the state emissions,
푝(풙 | 푞). Although this is a common choice in the handwritten text and speech
community, it is not the only option, and models for Bernoulli density functions [35]
and for discrete observed values [46, 25, 35], have also been used in the past.
For a HMM state 푞 ∈ Q, the density of a diagonal GMM with 퐾 multivariate
Gaussians can be written as:
푝(풙 | 푞)
def
=
Õ
퐾
푘=1
휔푞,푘N (풙 | 흁푞,푘, diag(흈
2
푞,푘)) (4.5)
where 휔푞,푘 is the weight of the 푘-th mixture component, and 흁푞,푘 and diag(흈
2
푞,푘)
are, respectively, the mean and diagonal covariance matrix of the 푘-th Gaussian.
A very good analysis of different methods to optimize the topology of HMMs
for modeling handwritten text, including the number of states and the number of
components in the GMMs, is offered in [41]. Once the topology of a HMM has
been established, the model parameters can be estimated from a set of training data,
typically consisting of pairs of text-line images and their corresponding reference
transcripts. In general, (at least) a 푀-state left-to-right HMM with 퐾 Gaussians per
state is to be trained for each of the 푁 characters of the alphabet considered. Note
that the number of transitions with probability greater than 0 of these models is 2푀.
Thus, the following parameters have to be estimated: a) 푀푁 initial-state proba￾bilities, 푃(푞); b) 2푀푁 transition probabilities, 푃(푞 | 푞
′
); and c) 3퐷퐾푀푁 diagonal￾Gaussian parameters (mean vectors and diagonal covariances) to model 푝(풙 | 푞),
where 퐷 is the dimension of the feature vectors, 푞, 푞′ ∈ Q and Q is the union of the
(generally disjoint) sets of states of the 푁 HMMs.
Two main choices to estimate these HMM parameters are Generative and Dis￾criminative Training, discussed below.
Given a set of trained HMMs, a character-level3 transcription hypothesis ˆ푤 can be
obtained for any new text line image feature vector sequence 푥 by computing [49, 80]:
푤ˆ = arg max
푐
푃(푐 | 푥) ≈ arg max
푤∈W
max
푎∈A
푃(푐, 푎 | 푥) (4.6)
where W and A are sets of character sequences and text-to-image alignments
(cf. Secs. 1, 3). This computation, called “decoding”, can be very efficiently done
by the time-honored dynamic programming Viterbi algorithm [107], which can
be further accelerated using very effective, well-known beam-search techniques
[49, 80]. Moreover, Viterbi decoding seamlessly allows for additionally using lexicon
and syntax constraints, implemented as “Language Models” (cf. Sec. 4.6).
3 Transcripts will be denoted as 푤, whether they are word or character sequences, except when it
becomes needed to explicitly distinguish among both.74 4 Probabilistic Models for Handwritten Text
4.3.3.1 Generative Training
The problem is to find an optimal set of HMM parameters 휽
∗
that maximizes the
probability density of the feature vector sequences of the training data, given the
reference (character-level) transcripts. For a training set with 푚 independent training
pairs4 (푥푖
, 푤푖
), 1≤푖 ≤ 푚, this can be stated as the following optimization problem:
휽
★ = arg max
휽
Ö푚
푖=1
푝(푥푖
| 푤
푖
; 휽) (4.7)
Equivalently, a more convenient logarithmic expression can be given:
휽
∗ ★
= arg max
휽
Õ푚
푖=1
log 푝(푥푖
| 푤
푖
; 휽)
★
= arg max
휽
Õ푚
푖=1
logÕ
푠
푝(푥푖
, 푠 | 푤
푖
; 휽)
★
= arg max
휽
Õ푚
푖=1
logÕ
푠
푝(푥푖
| 푠; 휽)푃(푠 | 푤
푖
; 휽) (4.8)
The sum over 푠 in Eq. (4.8) corresponds to the marginalization of the density among
all sequences of states of the composite model of HMMs, which is one of the hidden
variables of the model.
Because closed-form solutions to the previous optimization problem cannot be
found for complex probabilistic models with hidden variables (such as HMMs and
Gaussian Mixture Models GMMs), the Expectation–Maximization algorithm [23]
(in particular, the Forward–Backward or Baum–Welch algorithm [4, 5] for HMM)
is used to find a local optimum of the parameters. The algorithm starts with an
arbitrary set of parameters and these are iteratively modified to increase log-density
until convergence. All popular HMM-based toolkits for speech recognition and HTR
implement these algorithms, or variations of them (such as the Viterbi training for
HMMs) [114, 72].
4.3.3.2 Discriminative Training
The previous approach finds a set of parameters for our probabilistic models that
(locally) maximizes the density of the feature vectors given the reference transcript
of the image. However, in reality we are usually in an “opposite scenario”, since the
image transcripts are unknown. Then, we would rather wish to find the transcript
with the maximum probability given the sequence of feature vectors of a line image.
4 To enumerate a set or a list of sequences, we will generally use upper-indexing (such as 푤
(푖) – or
just 푤
푖
if there is no ambiguity) to avoid confusing an 푖-th sequence of a list with the 푖-th element
of a sequence.4.4 Artificial Neural Networks 75
Once the HMMs and GMMs are trained, the latter problem is usually solved thanks
to the Bayes theorem, but when training our models in a generative way, we are
solving a different problem than the original one (find a probabilistic model that
accurately represents the transcripts of given images).
There exists a wide variety of estimation criteria in the context of HMMs in order
to train these models to have a better discriminative performance. The general idea
is to try to find a model that maximizes the probability of the reference transcript
given the feature vectors, although slightly different variations of this idea have also
been widely used.
For instance, the Maximum Mutual Information (MMI) criterion [3] maximizes
the mutual information between the sequences of feature vectors and the correspond￾ing transcripts; the Minimum Classification Error Rate (MCE) criterion [51] tries
to find a set of parameters that minimizes the expected number of wrongly tran￾scribed samples; the Minimum Word and Phone Error Rate (respectively, MWE and
MPE) [74] try to optimize the parameters of the model so that the expected number
of wrongly transcribed words (or phones in the case of speech signals) is minimized.
In this book, we have generally used the more traditional generative training (using
the Baum–Welch algorithm) in order to estimate the parameters of our HMMs (and
GMMs). However, in some cases we have also used the Minimum Character Error
Rate (i.e. analogous to the MPE in speech) to have better probabilistic models [99].
4.4 Artificial Neural Networks
During the last decade, Artificial Neural Networks (or ANNs) have become predom￾inant in many PR applications. In particular, Convolutional and Recurrent Neural
Networks (CNN, RNN and CRNN) have become a de facto standard to model hand￾written text.
4.4.1 Description
The fundamental components of a ANN are the (artificial) neurons, also known
as units. An artificial neuron, in a similar way to real neurons present in living
organisms, is connected to other neurons and produces an output given the inputs
from the connected cells, using an activation function 휎. The first definition of an
ANN appeared in the work of [66], and the first algorithm to adjust the weights of
the neurons was described in [81]. Fig. 4.5 (a) shows the mathematical model of the
neuron introduced by [66].
In order to solve challenging problems, the neurons are grouped together into
several layers of neurons, so that the neurons from one layer are connected to the76 4 Probabilistic Models for Handwritten Text
neurons of the next layer.5 The layers of neurons which are neither the input nor
the output are called hidden layers. Fig. 4.5 (b) shows an illustration of a multilayer
artificial neural networks with two hidden layers. This type of network is also known
as a fully connected multilayer network, since each neuron is connected to all the
units from the previous layer, as shown in the figure.
푥3 Σ 휎
푥2
푥1
· · ·
푥푛
휔1 푗
휔2 푗
휔3 푗
휔푛 푗
휎(
Í푛
푖=1
푥푖 휔푖 푗 )
(a) (b)
Fig. 4.5: The mathematical model of an artificial neuron (a) and an illustration of a multi￾layer neural network with two hidden layers(b), and three input and two output neurons.
It has been proven that, for several families of not-linear activation functions, a
multilayer neural network is a universal approximator. This means that, although
composed by rather simple units, ANNs can approximate any function arbitrarily
well. This was first proved for the Sigmoid activation function [21], defined as:
휎(푥) =
1
1 + exp(−푥)
Nevertheless, due to the characteristics of the algorithm used to adjust the parameters
of the model, other activation functions have been proposed that perform better in
practice: the hyperbolic tangent (denoted by tanh), the Rectifier Linear Unit (also
known as ReLU, and denoted by 푅) [42], and the leaky ReLU (denoted by 퐿푎,
typically with 푎 = 0.01) [61] are a few examples. The following equations describe
each of these activation functions.
tanh(푥) =
exp(푥) − exp(−푥)
exp(푥) + exp(−푥)
, 푅(푥) = max{0, 푥}, 퐿푎 (푥) =
(
푥 푥 > 0
푎 푥 otherwise
In classification problems, the output of the ANN typically represents the posterior
probability of a label given the input data [11]. Thus, the output units must have their
value in the range [0, 1] and the sum of all of their outputs must be equal to 1. For
that, the SoftMax function [12] is typically used:
푦 푗 = 휙푗(푥1, . . . , 푥푛) =
exp(푥 푗)
Í푛
푖=1
exp(푥푖)
(4.9)
where 푦 푗
is the 푗-th output of an ANN layer and 푥푖
, 1≤ 푖 ≤ 푛, are its inputs.
5 Typically, neurons also have a constant input called the bias, but we will omit it from the notation
for simplicity.4.4 Artificial Neural Networks 77
4.4.2 Convolutional Layers
Fully connected layers present two problems in practice. First, since the number of
connections is quadratic with the number of neurons, the number of neurons in each
layer has to be limited (since we need the number of parameters of the model to
be manageable, for training purposes). Thus, for instance, if we needed to process
images with our ANN, we first would need to get a fixed-size representation of our
images (e.g. by re-scaling them to a fixed size).
The second problem arises when the input data is high dimensional, but presents
strong local patterns between neighbor input units. For instance, consider a rather
small squared input image of 100 pixels height and width. Since each unit in a fully
connected layer would be connected to all input pixels, we would need 104 parameters
for each unit in the layer. However, the correlation between the pixel intensities of
far away coordinates is low. This makes fully connected layers very slow when
processing images with a medium or large size, and highly over-parameterized,
which may create or aggravate the problem of overfitting.6
Convolutional layers (CNNs) [32, 56] are one solution to the aforementioned
problems. They are similar to fully connected layers, but each neuron of a con￾volution layer is only connected to the neighbor units from the previous layer. In
addition, all units share the same parameters. Thus, the number of parameters does
not depend on the size of the input data, but only on the receptive field (i.e. the
size of the neighborhood around each unit). For instance, Fig. 4.6 shows a diagram
of a convolution layer operating on a input image of width and height of 4 pixels,
with a receptive field of 3 squared pixels. Each output pixel is the product of each
neighboring input pixel weighted by the parameters.
6 1 3 2
3 4 5 6
2 3 1 5
6 4 5 3
∗
−1 −2 −1
0 5 0
−2 0 −1
=
26 −6 1 0
−1 4 5 21
−4 −18 −26 −2
23 11 15 4
푿 훀 풀
Fig. 4.6: Diagram of a two dimensional convolution operation. The input image 푿 is
convolved with the weight matrix 훀 to produce the output image 풀. The value of each
output pixel is the weighted sum of the neighboring input pixels.
For multichannel images (e.g. RGB), for each output channel the convolution
operation typically weights all input channels from all neighboring pixels. Let 푿 be
the input image represented as a tensor7 of size (퐼, 퐽, 퐻) where 퐼 is the width, 퐽 is
6 Overfitting happens when a model is highly accurate for training data but highly inaccurate for
new data coming from the same distribution.
7 Tensors are a generalization of matrices for an arbitrary number of dimensions. For instance,
vectors are one-dimensional tensors, matrices are two-dimensional tensors and multichannel images
can be represented as three-dimensional tensors.78 4 Probabilistic Models for Handwritten Text
the height and 퐻 is the number of input channels. Then, if 푿 is convolved with a
receptive field of size (훾, 휆) and 퐾 output channels, the result will be an image of
size (퐼, 퐽, 퐾) described by the following equation:8
[풀]푖, 푗,푘 = [푿 ∗ 훀]푖, 푗,푘 =
Õ
훾−1
푖
′=0
Õ
휆−1
푗
′=0
Õ
퐻
ℎ=1
[푿]
푖+푖
′− ⌊ 훾
2
⌋, 푗+ 푗
′−⌊
휆
2
⌋,ℎ [훀]푖
′
, 푗′
,ℎ,푘 (4.10)
Here, the operator ⌊·⌋ denotes the floor operator. Very often, a padding value is
defined (usually 0) for pixels laying outside of the input images (e.g. the blue regions
outside the input matrix in Fig. 4.6).
Finally, the reader should be aware that, although convolutional layers were
first popularized to process images [56], they have also been used to process one￾dimensional sequences [45], videos [50], or even sparse graphs [22].
4.4.3 Recurrent Layers
A recurrent neural network layer [29] (RNN) is composed of units with an internal
state. RNNs were originally designed to process sequences of vectors instead of
the single vectors fully-connected neural networks deal with. At each “time-step”
(sequence index), the output of the layer depends on the current input and the
previous state. In its simplest form, the state of each unit is just its output. Thus,
for a sequence of 푇 elements of 푚-dimensional vectors, 풙1, . . . , 풙푇, the output of a
simple RNN, parameterized by the matrices 훀 ∈ R
푚×푛
and 푹 ∈ R
푛×푛
, is a sequence
of 푛-dimensional vectors, 풚1, . . . , 풚푇, given by the equation:9
푦푡, 푗 = 휎
Õ푚
푖=1
푥푡,푖[훀]푖, 푗 +
Õ푛
푖
′=1
푦푡−1,푖′ [푹]푖
′
, 푗
(4.11)
where 푦푡, 푗 is the 푗-th component of the output vector at time 푡 (i.e. 풚푡) and 푥푡,푖
is the 푖-th component of the input vector at time 푡 (i.e. 풙푡). Typically, at 푡 = 1 the
recurrent connection is ignored, which is equivalent to assume that the state at 푡 = 0
is 풚0 = 0. The previous equation can be simplified using vectorial notation (and
assuming 풙푡
, 풚푡 are row vectors) to describe all the units in a recurrent layer.
풚푡 = 휎(풙푡훀 + 풚푡−1푹) , 1 ≤ 푡 ≤ 푇 (4.12)
where the non-linear function 휎 is applied to its vector argument component-wise.
RNNs can be visualized as regular fully connected layers if one unrolls the
sequence of input and output vectors, as depicted in Fig. 4.7.
The sequence can be processed both from left-to-right and right-to-left directions.
This is referred to as a bidirectional layer (BRNN) [85]. After processing it in the two
directions, the two output vectors at each time-step are combined into a single one by
summing, averaging or concatenating them (among other combination strategies).
8 Typically, as in the case of fully connected layers, each neuron has an input bias, but we omitted
this from the notation for simplicity.
9 Recurrent units typically have a bias input too, but this has been omitted for simplicity.4.4 Artificial Neural Networks 79
풙
풚
훀
푹
(a)
풙1 풙2 풙3
풚1 풚2 풚3
훀 훀 훀
푹 푹
(b)
Fig. 4.7: Compact and unrolled representation of a simple recurrent layer. The connections
in the Fig. (a) are replicated at each time-step and the resulting network is equivalent to a
(deep) fully connected network with several layers, as shown in the Fig. (b).
4.4.3.1 Long Short-Term Memory layers
Simple RNN are conceptually powerful models. In fact, it has been argued that they
are Turing-complete machines [89], although in practice this theoretical capability
is quite limited by several reasons. Among others, the usual learning algorithms are
prone to vanishing and exploding gradient problems, which prevent the network to
learn dependencies in a large-span context and makes the training very unstable. The
Long Short-Term Memory (LSTM) units were proposed to address these issues [44].
These units control the flow of information between the units at different time-steps
using a set of gates. The original LSTMs were later improved with forget gates [34],
which can be used to “reset” the internal state of the units. Fig. 4.8 shows the details
of the simple recurrent units and the LSTM units (with forget gates).
Σ
≀
풙푡
풚푡−1
풚푡
(a) Simple recurrent unit
Σ
≀
⊙
Σ
≀
⊙
Σ 휎
Σ 휎
Σ 휎 ⊙
풙푡
풚푡−1
풔푡−1
풚푡
풔푡
Input gate
Output gate
Forget gate
(b) LSTM unit
Fig. 4.8: Detailed diagram of the units of a simple recurrent layer (Fig. (a)) and a LSTM
layer (Fig.(b)). The LSTM unit has two gates (input and forget gates) that control the input
to the unit’s state (푠푡), and an output gate that controls the output of the unit, at each
time-step.80 4 Probabilistic Models for Handwritten Text
Given a input sequence of 푇 푚-dimensional vectors, 풙1, . . . , 풙푇, the output se￾quence of 푛-dimensional vectors 풚1, . . . , 풚푇 provided by an LSTM layer is described
by the following set of (vectorial) equations.10
풂푡 = tanh(풙푡훀푎 + 풚푡−1푹푎)
풊푡 = 휎(풙푡훀푖 + 풚푡−1푹푖)
풐푡 = 휎(풙푡훀표 + 풚푡−1푹표)
풇푡 = 휎(풙푡훀 푓 + 풚푡−1푹푓 )
풔푡 = 풊푡 ⊙ 풂푡 + 풇푡 ⊙ 풔푡−1
풚푡 = 풐푡 ⊙ tanh(풔푡)
where the operator ⊙ denotes the Hadamard (or element-wise) product of vectors and
all the 훀 and 푹 parameter matrices are in R
푚×푛
and R
푛×푛
, respectively. As with the
simplest recurrent units, most implementations of LSTMs assume that 풚0 = 풔0 = 0.
LSTM layers are in the core of almost all state-of-the-art solutions to count￾less applications such as handwritten text recognition [39, 108, 77], speech recog￾nition [40, 38], machine translation [2, 19], language modeling [90, 91], image
captioning [113, 106], etc.
4.4.3.2 Estimating character-level posterior probabilities
A RNN produces a sequence of 푇 푑-dimensional vectors 푦 = 풚1:푇 for a (line) image
region 푥 represented as a sequence of 푑
′
-dimensional feature vectors 풙1:푇. These
vectors are obtained from 푥 through a series of CNN layers which extract relevant
text-stroke features and adapt the original image dimensions to those used by the
RNN stack. Therefore, each vector 풚푡
is expected to represent characteristics of the
text depicted in 푥, focused on some region of 푥 roughly associated with the position
푡 through the CNN stack.
To explicitly relate the RNN output 푦 with a sequence of words or characters
representing a image transcript, an additional layer is needed which, for each RNN
output 푡, estimates the posterior probability 푃(푐푡
| 푥), 1 ≤ 푖 ≤ 푇, where 푐푡 ∈ C is a
“character-level label”. This is generally implemented as a linear layer with 푇 output
vectors, each with 퐶 = |C| SoftMax units (i.e., 푇 퐶-class perceptrons).
This sequence of posterior probability vectors is sometimes referred to as “confi￾dence matrix” or, as we prefer, character-level posteriorgram of 푥. Fig. 4.9 (b) shows
an example of character-level posteriorgram for an image of the text “abba”.
4.4.3.3 Connectionist Temporal Classification
Let 푃(푐푡
| 푥; 휽), 1 ≤ 푖 ≤푇, be the character-level posteriorgram of 푥 produced by
a CNN+RNN stack (i.e. a CRNN), where 휽 represents the CRNN weights). If we
10 Once more, we have omitted the bias inputs in the first four equations to simplify the notation.4.4 Artificial Neural Networks 81
assume that the labels are position-wise independent, given the corresponding feature
vectors, then the probability of a label sequence 푐 = 푐1, . . . , 푐푇 can be written as:
푃(푐 | 푥; 휽)
★
=
Ö
푇
푡=1
푃(푐푡
| 푥; 휽) (4.13)
This model, assigns (in a stochastic manner) a label to each feature vector of the
sequence derived from 푥, much in the same way as a HMM assumes that each feature
vector is generated by some state.
We are using the term label, not any kind of linguistic term. In general the variable
푤 will be used in the sequel to represent a transcript as a sequence of linguistic units,
such as characters from a given alphabet or words from a vocabulary. But, without
loss of generality, for the moment let us assume these units are characters.
Thus, the remaining question is, how can we determine a distribution over tran￾scripts from the distribution over label sequences? Such a distribution is in fact the
one which can be learned from training data (pairs of line images and their tran￾scripts). Once trained, it is also needed to obtain an actual transcription hypothesis
for a given image (referred to as decoding).
To define a distribution over arbitrary-length transcripts, the Connectionist Tem￾poral Classification (CTC) [37] method is used. It assumes that the set of character￾level labels is essentially equal to the set of characters (alphabet), plus an additional
auxiliary label, typically called “CTC-blank” or “no-symbol”, denoted as “∅”. This
label is needed to allow transcripts shorter than the number of feature vectors (푇)
and to distinguish two consecutive equal characters in the transcript.
Using the CTC-blank, a sequence of labels can be mapped into a sequence of
characters in a deterministic way, using a simple function, Υ, called “CTC function”.
It takes a label sequence 푐 as input and produces a transcript 푤, as follows:11
1. Remove all consecutive repetitions of labels from 푐
(e.g.: 푐 = a, a, ∅, ∅, b, b, ∅, b, ∅, a, a, ∅ → a, ∅, b, ∅, b, ∅, a, ∅).
2. Remove all “CTC-blank” symbols to obtain 푤
(e.g.: a, ∅, b, ∅, b, ∅, a, ∅ → a, b, b, a ≡ 푤 ).
Now we can see how the label sequence distribution given by Eq. 4.13 can be
used to compute the posterior probability of a transcription hypothesis 푤, given a
(line) image 푥. To this end, we apply marginalization:
푃(푤 | 푥; 휽) =
Õ
푐
푃(푤, 푐 | 푥; 휽) =
Õ
푐
푃(푐 | 푥; 휽) 푃(푤 | 푐, 푥; 휽)
=
Õ
푐:Υ(푐)=푤
푃(푐 | 푥; 휽) =
Õ
푐:Υ(푐)=푤
Ö
푇
푡=1
푃(푐푡
| 푥; 휽) (4.14)
where we have taken into account that 푃(푤 | 푐, 푥; 휽) is simply a Dirac delta function,
whose value is 1 if the sequence of labels 푐 leads to the reference transcript (that is,
Υ(푐) = 푤) and 0 otherwise.
11 In general Υ is not injective or subjective, but it is a true (univalent) function, that is, Υ(푐) =푤
∧Υ(푐
′
) =푤 ⇒ 푐=푐
′
. It can be represented by a finite state transducer (see illustration in Fig. 4.14).82 4 Probabilistic Models for Handwritten Text
Fig. 4.9 shows an example of the probabilistic interpretation that the CTC makes
of the output of a CRNN. Notice that the posterior distribution can be represented as
a weighted automaton with 푇+1 states (recall, 푇 is the number of feature vectors). The
automaton has the simple form of a lattice because of the conditional independence
assumption made by the CTC algorithm. Moreover, thanks to this independence,
the sum of the probabilities of all label sequences that produce the given reference
sequence of characters can be computed efficiently, as discussed in Sec. 4.7.2 (see
illustration in Fig. 4.14).
(a) Input image
.
0.26 0.02 0.67 0.96 0.10 0.01 0.95 0.06 0.51 0.46 0.04 0.90
0.73 0.96 0.24 0.02 0.30 0.09 0.01 0.12 0.10 0.46 0.95 0.09
0.01 0.02 0.09 0.02 0.60 0.90 0.04 0.82 0.39 0.08 0.01 0.01
∅
a
b
(b) Character-level posteriorgram: sequence of label posterior vectors (푇 =12 SoftMax outputs).
. . .
∅/0.26
a/0.73
b/0.01
∅/0.02
a/0.96
b/0.02
∅/0.67
a/0.24
b/0.09
∅/0.10
a/0.30
b/0.60
∅/0.01
a/0.09
b/0.90
∅/0.90
a/0.09
b/0.01
(c) Probability distribution of the output labels represented as a weighted finite-state automaton.
Fig. 4.9: Example of the probabilistic interpretation that the CTC makes of the output of
a CRNN, applied to a text image. The probability of a given text 푤, is the sum of the
probabilities of all the label sequences 푐 such that Υ(푐) = 푤. For instance, 푃(“aa” | 푥) =
0.00006, 푃(“aba” | 푥) = 0.036 and 푃(“abba” | 푥) = 0.39.
The character (or word) sequence posterior probability given by Eq. (4.14), along
with the corresponding character-level lattice suggested in Fig.4.9 (c), is all what is
needed to compute all forms of PrIxs, as discussed in Chapter 3. Effective techniques
to carry out this computation efficiently will be explained in Chapter 5.
On the other hand, as usual in plain HTR, one may be only interested in a single
transcription hypothesis (or decoding) for each input image. In this case, statistical
decision theory dictates that an optimal transcript, ˆ푤, is one for which the posterior
probability 푃(푤 | 푥; 휽) is maximum. Thus, departing from Eq. (4.14), an optimal
decoding of 푥 is derived as follows:
푤ˆ = arg max
푤
푃(푤 | 푥; 휽) = arg max
푤
Õ
푐:Υ(푐)=푤
Ö
푇
푡=1
푃(푐푡
| 푥; 휽) (4.15)
This optimization can be carried out by dynamic programming. But even so, it entails
an important amount of computation. A cheaper, greedy, local-optimum decoding
of 푥 can be obtained by approximating the sum in Eq. (4.15) with the maximum:4.4 Artificial Neural Networks 83
푤ˆ ≈ arg max
푤
max
푐:Υ(푐)=푤
Ö
푇
푡=1
푃(푐푡
| 푥; 휽)
= arg max
푐,푤:푤=Υ(푐)
Ö
푇
푡=1
푃(푐푡
| 푥; 휽) = Υ￾
arg max
푐
Ö
푇
푡=1
푃(푐푡
| 푥; 휽)

= Υ(푐ˆ1, · · · , 푐ˆ푇), where ˆ푐푡 = arg max
푐
′
푃(푐푡
| 푥; 휽), 1 ≤ 푡 ≤ 푇 (4.16)
Note that, while the max approximation may be not very accurate in terms of
probability values, in many cases the ranking across different transcription hy￾potheses does not change significantly. For instance, in the example of Fig. 4.9,
the max approximation yields: 푃˜(“aa” | 푥) = 0.000006, 푃˜(“aba” | 푥) = 0.0028 and
푃˜(“abba” | 푥) = 0.038. In general, it is commonly acknowledged that, even though
Eq. (4.16) is suboptimal, it usually provides sufficiently good transcription results.
4.4.4 CRNN Training Through Gradient Descent
ANNs used to model the transcript of a handwritten image are typically discrimina￾tive, and directly model the posterior distribution of the transcript given the image.
Typically, the (log-)posterior probability of the reference transcript is maximized
during training. This corresponds to minimizing a loss equivalent to the Kullback–
Leibler divergence between a Dirac delta distribution representing the reference text,
푤
′
, and the probability distribution modeled by the CRNN, 푃(푤 | 푥; 휽) (Eq. 4.14).
For a training pair (푤
′
, 푥) this loss, often called “CTC loss”, is:
퐷KL ￾
푃(푤
′
| 푥) ∥ 푃(푤 | 푥; 휽)

= − log 푃(푤
′
| 푥; 휽)
= − log Õ
푐:Υ(푐)=푤′
Ö
푇
푡=1
푃(푐푡
| 푥; 휽) (4.17)
This loss reflects the discrepancy of the characters of the reference transcript 푤
′
with the corresponding probabilistic predictions of the CRNN, through the CTC
function. However, the backward error propagation associated with gradient descent
CRNN training requires a loss measured at the CRNN output, namely at the level of
the character-label posteriorgram, 푃(푐푡
| 푥; 휽), 1≤푡 ≤푇. To this end, the inverse of
the CTC function has to be used to propagate actual character loss backwards to the
CRNN output. All in all, computing this loss efficiently requires complex algorithmic
which essentially boils down to the same backward-forward EM algorithm used for
HMM training [49, 37].
Since all the CRNN components are differentiable, once the label loss at the
CRNN output is available, gradient descent based on Backpropagation (BP) [110, 83]
can be carried out as usual to iteratively update all the network parameters. Note that a
non-standard version of BP, known as Backpropagation Through Time (BPTT) [111]
is needed for the RNN layers. On the other hand, the classical method for iterati84 4 Probabilistic Models for Handwritten Text
parameter updating is Stochastic Gradient Descent (SGD) [79], but different alter￾natives are used very often, such as Adagrad [27], Adam [53], RMSProp [95], and
many others.
In the early days of ANNs, BP was unable to effectively train the parameters of
large neural networks with many stacked layers. Similarly, BPTT also had problems
learning with long sequences for tasks that required long time dependencies between
its outputs. However, better initialization of strategies [36], activation functions, types
of layers (LSTMs vs. simple RNNs) [44, 42], and gradient-based algorithms have
alleviated this issue significantly. Nowadays, it is possible to train neural networks
with dozens of stacked layers, over sequences of hundreds of elements.
4.4.5 Neural Networks for Handwritten Text
During many years RNNs based on Multidimensional LSTMs (MDLSTMs) domi￾nated the state-of-the-art solutions to model handwritten text [39, 1, 108]. MDLSTMs
are a variant of recurrent units designed to process signals of an arbitrary size and
an arbitrary number of dimensions. Observe that, for both simple RNNs and LSTMs
described before, the output at a given time-step depends on the current input and
the state of the layer at the previous time-step. However, it is not clear what “the
previous time-step” means with signals spanning across more than one axis. For
instance, if we process an image (a two-dimensional input) from the left-top to the
right-bottom corner, what would be the “previous” pixel? Multidimensional RNNs,
and particularly MDLSTMs, are designed so that at each coordinate, the output de￾pends on the input at the same coordinate and the outputs at the coordinates with a
delay equal to 1 in each dimension, as illustrated in Fig. 4.10. In addition, similarly to
bidirectional RNNs described earlier, multidimensional RNNs can process the input
signal in different directions (2퐷 different directions, for a signal with 퐷 axes).
(푡 −1) (푡) (푡+1)
(a)
(푖−1, 푗 −1) (푖−1, 푗) (푖−1, 푗+1)
(푖, 푗 −1) (푖, 푗) (푖, 푗+1)
(푖+1, 푗 −1) (푖+1, 푗) (푖+1, 푗+1)
(b)
Fig. 4.10: Coordinates in one-dimensional (Fig.(a)) and two-dimensional (Fig. (b)) input
signals of a multidimensional recurrent layer. In the one-dimensional image (e.g. a se￾quence) there is a single “previous” element when processing the sequence, for instance,
from left-to-right. However, in the two-dimensional signal there are two “previous” ele￾ments when processing the image from the top-left to the bottom-right corner.4.4 Artificial Neural Networks 85
Despite its wide adoption and success for handwritten text applications, in [75]
we argued that these powerful architectures were likely unnecessary to accurately
model handwritten text. There are two main observations behind this hypothesis.
First, notice that human languages have an intrinsic sequential nature. Long before
humans developed handwriting we communicated only using sounds, which are
continuous vibrations of the air (or other mediums) through time. In order to allow
for non-direct communication, humans later developed handwriting and represented
the different phonemes of their language using combinations of a set of graphemes
(such as characters) to be able to communicate using two-dimensional surfaces (such
as paper sheets, stones, seeds, etc.). Thus, although two-dimensional information
can be useful to model individual symbols (or a small group of symbols) in a given
handwritten alphabet, it should not be necessary to model the language itself, since
the latter has a sequential foundation.
Indeed, when one compares the output of a 2D-LSTM layer (a MDLSTM designed
for processing images) trained for a handwritten text recognition task, with that of a
comparable convolutional layer, the results are very similar, as shown in Fig. 4.11.
The figure suggests that the features learned by a 2D-LSTM layer use only a quite
small 2D context, which can be mimicked with a simple convolutional layer with a
small receptive field (3×3). In Fig. 4.11, the outputs of the 2D-LSTM show some
“volumetric” effects that cannot be replicated with the small receptive field used by
the convolutional layer. However, as shown in [75], this limitation does not hinder
the capability of the model to achieve the same accuracy as MDLSTMs.
(a)
(b)
Fig. 4.11: Comparison of the outputs of a 2D-LSTM layer trained for a handwritten text
recognition task (Fig. (a)), with that of a comparable convolutional layer (Fig. (b)).
Given these facts, we use an architecture composed of a stack of convolutional
(and pooling) layers followed by a stack of one-dimensional recurrent layers of
bidirectional BLSTMs, as illustrated in Fig. 4.12 and already more or less explicitly
anticipated in previous sections.
Pooling layers (in particular, Maximum pooling) are used very often to summarize
the output of neighboring pixels and, thus, reducing the size of the images as they
are processed by several layers of the neural network [78]. The BLSTMs layers in86 4 Probabilistic Models for Handwritten Text
our architecture process the images column-wise (i.e. all channels of all pixels of
a column form a single vector). A similar architecture was first used in [88] and
other variants have also been used more recently in different HTR applications [9].
As also anticipated in previous sections, we call this architecture Convolutional and
Recurrent Neural Network (CRNN).
Input
Conv
Maxpool
Concat
LSTM
Linear + SoftMax
Fig. 4.12: Diagram of the CRNN architecture we propose and used to model the transcripts
of a handwritten text line. It contains two convolutional blocks, followed by two BLSTM
layers, and a final linear layer. Convolutional layers (“Conv”) also include the ReLU
activation function. If a large number of convolutional blocks is used, not all of them
include the pooling layer (“Maxpool”) to avoid discarding too much information. The
concatenation layer (“Concat”) after the last convolutional block concatenates all channels
from all pixels of an individual column into a single vector. The other concatenation layers
append the outputs of a BLSTM layer in each direction.
4.5 Key differences between HMMs and CRNNs with CTC
As discussed in Sec. 4.4.3.3, the CRNN output at each time-step is an estimate of
the posterior probability distribution over a set of labels, given the input image. And
we argued that these labels play a similar role as the states in HMMs in that they
account for different alignments of label sequences with the input image. Moreover,
when the CTC function is applied to the CRNN output, we can interpret the final
output as a probability distribution over actual image transcription hypotheses.
Focusing only on the probabilistic model interpretation, observe that the CTC
makes a very naive assumption with respect to a given sequence of labels: it assumes
that the probability of the label at time 푡 depends only on the corresponding feature
vector, and not on the other vectors or labels. HMMs have a slightly weaker inde￾pendence assumption: the density of a feature vector only depends on the state that
emitted it, but the sequence of states follows a first-order Markov process (i.e. the
probability of each state depends on the state before).
One might think that, since the probabilistic model associated with CTC makes
stronger assumptions than those of the HMMs, the latter could perform better when
these assumptions do not hold. Yet, in reality, CRNN models trained with CTC4.6 푁-gram Language Models 87
typically perform better than HMM-based models for recognition tasks. How can
this be explained?
When the CTC is used on top of deep and recurrent neural networks, the param￾eters of this neural network, are adjusted to produce a sequence of feature vectors
that helps the CTC probabilistic output to represent the transcript of the image accu￾rately, even with its naive assumptions. An this applies also to the parameters of the
convolutional layers, responsible of feature extraction. In contrast, the feature vectors
in HMM-based methods are fixed during training, and the free parameters of the
probabilistic model are adjusted to maximize only the likelihood of the fixed feature
vectors (or the posterior of the reference transcript in discriminative training).
In [8, chapter 7], the similarities between CTC training (discussed above) and the
Forward–Backward algorithm applied to hybrid HMM-ANN models are analyzed.
Essentially, CTC training is equivalent to Forward–Backward training on a special
HMM model with a single state per character and an optional state between characters
(the “CTC-blank”), and no transition probabilities between states.
Although the independence assumption made by the CTC may not be critical
for achieving a decent recognition accuracy, combining the output of the neural
network with an external language model can improve the recognition results. For
PrIx applications, the independence assumption can be very harmful since we are
not only interested in obtaining the first-best transcription hypothesis, regardless of
its particular posterior probability. Instead, we need to have an accurate model for
the complete posterior probability distribution.
4.6 푵-gram Language Models
The effects of the independence assumption made CTC can be mitigated by combin￾ing the output distribution of the CRNN with an additional Language Model (LM)
which provides a prior distribution over transcripts; that is over all (meaningful)
sequences of words or characters of a given language, or collection of documents.
In the case of generative models such as HMMs, such a prior distribution is
essential. Given that HMMs (with GMMs) model the conditional density 푃(푥 | 푤),
the Bayes rule has to be applied to obtain a transcription hypothesis posterior:
푃(푤 | 푥) =
푝(푥, 푤)
푝(푥)
=
푝(푥 | 푤)푃(푤)
Í
푤′ 푝(푥 | 푤′
)푃(푤′
)
(4.18)
where 푃(푤) is the prior modeled by a LM.
One of the most popular choices to model a prior distribution over sequences
(such as 푤) is the 푛-gram, which was first proposed by Shannon to model the English
language [87]. An 푛-gram language model assumes that each symbol (word or
character) in a given sequence only depends on the previous 푛−1 symbols. Thus,88 4 Probabilistic Models for Handwritten Text
using the chain rule on the prior probability, one obtains:12
푃(푤) = 푃(푤1:퐿) =
Ö
퐿
푖=1
푃(푤푖
| 푤1, . . . , 푤푖−1)
★
=
Ö
퐿
푖=1
푃(푤푖
| 푤푖−푛+1, . . . , 푤푖−1)
def
= 푃(푤; 푛) (4.19)
Perhaps the most interesting feature of 푛-grams is that they are very easily learn￾able from training strings. All the parameters of a 푛-gram model can be maximum￾likelihood (MLE) estimated just as the relative frequency of the relevant events in
the training strings [49]. Let Σ be a vocabulary or alphabet and W a training sample
of strings over Σ. The parameters of a 푛-gram model, are estimated as:
푃(푣 | 푧; 푛) =
푓 (푧푣)
푓 (푧)
, 푣 ∈ Σ, 푧 ∈ Σ
<푛 (4.20)
where 푧푣 is the concatenation of 푧 and 푣 and, for any string 푧
′
, 푓 (푧
′
) is the number
of times13 the substring 푧
′
appears in the strings of W [102].
Plain MLE presents a serious issue when the context (푛) grows and the amount
of training data is kept constant. If the vocabulary size is |Σ|, there are |Σ|
푛 possible
different contexts. For instance, the modern English language is made of more than
170 000 words. Using a modest 3-gram model gives roughly 5·1015 different contexts,
thus the chance of missing some 푛-grams increases exponentially with 푛.
The simplest technique to address this issue is additive smoothing, which consists
in adding a small value to all counts, although more sophisticated (and preferred)
methods exists, such as discounting, back-off and interpolation with lower-order 푛￾grams. In [18] is given an excellent overview and empirical comparison of various
smoothing techniques. In particular, in our experiments we have used the modified
Kneser–Ney [54] or Witten–Bell [112] smoothing and interpolation.
푁-grams have been widely used in speech recognition [48, 52], HTR [65, 64],
machine translation [13, 14], and many other pattern recognition applications, includ￾ing KWS [31, 97]. In HTR, Eq. (4.18) is commonly used to obtain an (approximate)
optimal transcription hypothesis for a given text image 푥 as follows:
푤ˆ = arg max
푤
푃(푤 | 푥) ≈ arg max
푤
푝(푥 | 푤)
훾푃(푤; 푛) (4.21)
where the denominator of Eq. (4.18) has been omitted since it does not depend on 푤
and 훾 is a hyperparamenter which can be tuned empirically to balance the impact of
the term 푝(푥 | 푤), called “optical model”, with respect to the LM prior 푃(푤).
12 For the sake of simpler notation, if 푖≤1 the expression 푃(푧푗
|푧푖
, . . . , 푧푗−1 ) is assumed to denote
푃(푧푗
|푧1, . . . , 푧푗−1 ). If 푗 = 1, it is just 푃(푧1 | 휖 ) (휖 is the empty string), interpreted as 푃(푧).
13 For substrings shorter than 푛, 푓 (푧
′
) is understood as the number of times that 푧
′
appears as a
prefix of some string in W.4.6 푁-gram Language Models 89
4.6.1 Combining the Output of a CRNN with a 푵-gram LM
CRNN optical models trained with CTC directly represent the posterior probability
of transcripts given the input images. Thus, in theory, the underlying prior distribution
on transcripts (which what is explicitly modeled by a LM), is already implicitly
modeled by the CRNN and, in principle, LMs should not be necessary for CRNNs.
However, it has been observed that adequately combining (in a probabilistic man￾ner) the CRNN output with an additional LM does improve the HTR accuracy [75],
as well as the PrIx performance [100, 76, 103] (see also experiments in Chapter 6.
There are two reasons that explain this seemingly counter-intuitive fact. Firstly, the
independence assumptions of underlying CTC are different from those made by
푛-grams, especially when a large context is considered. Secondly, and most impor￾tantly, very often we have access to large amounts of additional text-only data. In such
case, a LM trained with the additional text may represent a richer prior than what
can be captured by the CRNN using only the limited amount of image-transcript
training pairs available.
For a probabilistically sound combination of both models, a density distribution
푝(푥 | 푐) has to be derived from the CRNN output distribution 푃(푐 | 푥), where 푐
denotes a sequence of character-level labels (Eq. 4.13). To this end, the following
heuristic was proposed and successfully used in [8, 10] (among others):
푝(푥 | 푐) =
푝(푐, 푥)
푃(푐)
≈
푃S (푐 | 푥)
푃S (푐)
휂
★
=
Ö
푇
푡=1
푃S (푐푡
| 푥)
푃S (푐푡)
휂
(4.22)
where S is a training set,푇 is the length of the label sequence 푐 and the character label
priors 푃S (푐푡) are estimated from values of character-level posteriorgrams produced
as a byproduct of CTC training with S [8]. The hyperparameter 휂 is generally tuned
using a validation set, so as to maximize the performance on the task considered.14
To obtain a transcription hypothesis for a test image 푥, the character label priors
푃S (·) (and Eq. (4.22)) can be used to transform the character-level posteriorgram
푃S (푐 | 푥) and its corresponding lattice (cf. Fig. 4.9) into a likelihood lattice. This
lattice can then be combined with other finite-state models (such as CTC, lexicae
and LMs) to build a complete system which provides the final transcript posterior
probability distribution needed for PrIx and also in HTR to obtain the most probable
transcription hypothesis for a given text image. This approach has been widely used
in the past in many works on HTR [26, 108, 75, 9].
Among many approaches available for model combination, here we will adopt a
very flexible methodology based on weighted finite-state transducers.
14 In practice, the factors 1/푃(푐푡 )
휂
in Eq. (4.22) can be safely ignored because their effect is largely
subsumed by the effect of the optical scale factor (훾 in Eq. (4.21)). This is supported by empirical
results shown in Chapter 6.90 4 Probabilistic Models for Handwritten Text
4.7 Weighted Finite State Transducers (WFST)
Most of the models discussed so far are or can be more or less explicitly interpreted
in terms of stochastic finite–state models [101, 102]. This is particularly the case
with HMMs and 푛-gram LMs and also the character-level posteriorgram produced
by a CRNN can be properly seen as a simple finite-state automaton (c.f., Fig. 4.9).
This is also the case for the CTC function, which can be easily casted into a simple
finite state transducer, as the one shown in Fig. 4.14 (e).
Therefore the probability distributions involved in many of the (often complex)
computations discussed so far can be assumedly given by a generic model known
as Weighted Finite State Transducer (WFST) [68]. This is particularly interesting
because there are plenty of fundamental results about WFST algebra, as well as
many practical, flexible and reasonably efficient computational tools to perform
useful operations with objects represented in terms of WFSTs.15 All the important
concepts of WFSTs and the operations they support are given in Appendix B.
A WFST is just a finite-state network where the nodes represent states, and the
edges are state transitions labeled with three elements: an input symbol from an input
alphabet Σ, an output symbol of an output alphabet Γ, and a weight which is typically
a real number. Both alphabets may include the empty string, denoted as 휖. States
may also have a final-state weight. The sets of states and edges are denoted as Q and
E, respectively, and Q includes a distinguished initial state, denoted as 푞0.16
The combination of edge and final-state weights is governed by two operations
of an adequate semiring, generically denoted as K. The operations are denoted ⊕
and ⊗, respectively called “sum” and “product”, with the corresponding neutral or
identity elements 0 and ¯ 1. In addition, a ¯ division operation, denoted as ⊘, is defined.
Two WFST semirings commonly used in the field of speech recognition and HTR
(and in this book) are shown in Table 4.1. They allow to implement various useful
algorithms using a small set of WFST operations. For instance, marginalization and
Viterbi decoding are equivalent to solving the shortest distance and the shortest-path
problems on a WFST in the Real and the Tropical semiring,17 respectively [67].
Table 4.1: Two semirings commonly used in WFSTs.
Semiring K 0¯ 1¯ 푎 ⊕ 푏 푎 ⊗ 푏 푎 ⊘ 푏
Real R 0 1 푎 + 푏 푎 · 푏 푎/푏
Tropical17 R ∪ {∞} ∞ 0 min{푎, 푏} 푎 + 푏 푎 − 푏
15 Two popular toolkits that offer WFST tools are OpenFST https://www.openfst.org and
Kaldi https://kaldi-asr.org , also based on OpenFST.
16 Here we use Q and 푞0 for homogeneity with HMMs, but in Appendix B they are denoted as 푉
and 푠0, respectively. Similarly, for the set of edges we use E rather than 퐸 used in the Appendix.
17 If used with stochastic models, − log–probabilities are assumed. Therefore, the weights resulting
from any WFST operation need to be converted back into probabilities. In what follows, we assume
these “formatting” steps are always done when Tropical semirings are used.4.7 Weighted Finite State Transducers (WFST) 91
An example of WFST is shown in Fig. 4.13, where Σ = {a, b, c, ab, cZc} and
Γ ={X, Y, ZZ, cZc}. Each edge 푒 ∈ E is labeled with an input from Σ, an output from
Γ, and a weight which, if omitted, is understood to have a default value (the neutral
element of the product in the assumed semiring). Inputs and outputs are separated
by a semicolon (“ : ”), and the weight by a slash (“ / ”). In general, state labels are not
shown, unless it becomes necessary for clarity.
푞0
0.1
a : X / 0.6
b : Y / 0.4
a : X / 0.7
a : X / 0.3
ab : 휖 / 0.3
c : Y / 0.7
휖 : ZZ / 0.2 c : Y
cZc
Fig. 4.13: An example of WFST. Input and output symbols are separated by “ : ”. The
output can be omitted if it is equal to the input, as in the edge labeled “cZc”. In this
example, edge weights belong to the Real semiring and, if omitted, the default value
1 (the neutral element of the product). The default final-state weight is 0 (the neutral
element of the sum), except for the double-circle (final) states where it is 1.0. Note
that WFSTs need not be stochastic, as it happens in this example, because the sum
of weights of transitions out of the bottom-most state, plus the final-state weight, is
2.3 ≠ 1.0. Examples of weighted input-output sequences modeled by this WFST are:
aac : XXY / 0.474, ba : YX / 0.012, aacZc : XXZZZZcZc / 0.0072, baabababc : YXY / 0.00756.
A function 휚(푞) : Q → K is used to denote the final-state weights. And five
functions are used to denote specific elements associated to an edge: 푝(푒), 푛(푒) :
E → Q are the previous (departing) and next (ending) states; 푙푖(푒) : E → Σ
and 푙표 (푒) : E → Γ are the input and output tokens; and 휔(푒) : E → K is the
edge weight. These functions are extended as needed to account for sequences of
states, edges (called “paths” or “sub-paths”), and input and output tokens, and the
corresponding weights, which are computed according the semiring considered (see
details in Appendix B). A path 휙 = 푒1, . . . , 푒푘, 푛(푒푖−1) = 푝(푒푖), 1 < 푖 ≤ 푘 is said
to be complete if it starts in 푞0 and ends in a state whose final-state weight is not 0;¯
that is, 푝(푒1) = 푞0, 휚(푛(푒푘)) ≠ 0. The partial weight of ¯ 휙 is computed as:
휔(휙) = 휔(푒1) ⊗ . . . ⊗ 휔(푒푘) (4.23)
and the full weight (only meaningful if the path is complete) as:
휔˜ (휙) = 휔(휙) ⊗ 휚(푛(푒푘)) (4.24)
In general a given pair of input-output token sequences may correspond to multiple
complete paths (for example, in Fig.4.13 there are two complete paths for aac :XXY).
Thus, of 푇 is s WFST, the weight of pair of input-output token sequences, 푤 ∈
Σ
★, 푤′ ∈ Γ
★, is computed as (see Appendix B for formal definitions of 푇 and 푇 (·, ·)):
푇 (푤, 푤′
) =
Ê
휙: 푙푖 (휙)=푤, 푙표 (휙)=푤′
휔˜ (휙) (4.25)92 4 Probabilistic Models for Handwritten Text
4.7.1 The WFST Composition Operation
The most important ans useful operations which can be defined on WFSTs are
described in Appendix B. Here we provide a summary of a fundamental operation
called composition. It can be used to create a complex WFST from simpler ones. It
is this operation what is often called for to combine different probabilistic models.
Let 푇 and 푇
′ be two WFSTs defined over a common semiring K and three
alphabets, Σ, Σ
′
, Γ, where Σ, Σ
′
are the input and output alphabets of 푇 and Σ
′
, Γ are
those of 푇
′
, respectively. And assume that the sumÉ
푤′∈Σ′★ 푇 (푤, 푤′
) ⊗푇
′
(푤
′
, 푤′′) is
well-defined in K for all pairs of sequences (푤, 푤′′) ∈ Σ
★×Γ
★. Then, the composition
of 푇 and 푇
′
is a WFST, denoted by 푇 ◦ 푇
′
, such that:
[푇 ◦ 푇
′
] (푤, 푤′′) =
Ê
푤′∈Σ′★
푇 (푤, 푤′
) ⊗ 푇
′
(푤
′
, 푤′′) (4.26)
The composition can be straightforwardly applied to combine distributions that
are directly given by stochastic finite-state models such as HMMs and 푛-grams. This
allows to easily join optical character HMMs with 푛-gram character LM to build
a compound finite-state model for the joint probability 푃(푥, 푤) of a transcription
hypothesis given a feature vector sequence representing the input (text line) image.
This and other WFST operations can be also used for decoding; that is, computing
(suitable approximations to) arg max푤푃(푤 | 푥). Moreover it is also possible to obtain
not only this single best transcript, but also a lattice or WG representing a discrete,
pruned approximation to 푃(푤 | 푥).
In addition, the composition operation can be used to handle in a flexible manner
the character-level posteriorgrams yield by a CRNN, as will be discussed hereafter.
4.7.2 Handling CTC by Means of Elementary WFST Operations
The so called CTC function was defined in Sec. 4.4.3.3 to map fixed-length sequences
of character-level labels into actual transcripts represented by real, variable-length
character sequences. The mapping, described in words in Sec. 4.4.3.3, was fairly
simple and, as illustrated in Fig. 4.14 (푇2) below, it can be easily implemented as a
simple HMM represented by a finite-state transducer. On the other hand, in Fig. 4.9 of
this chapter, we illustrated how a character-level posteriorgram 푃(푐푡
| 푥; 휽), 1≤푖 ≤푇,
yield by a CRNN (see Eq. 4.13) can be straightforwardly represented as a lattice,
which can also be directly implemented as a simple WFST.
The WFST composition operation can now be called for to put these two networks
together into a WFST that represents the transcription posterior 푃(푤 | 푥) of Eq. (4.14).
This is illustrated in Fig. 4.14. A CRNN output is represented by a WFST 푇1, the
CTC function HMM as 푇2 and the composition of these two WFST as 푇1 ◦ 푇2. The
posterior probability of a given transcription hypothesis 푤, can be computed as the
sum of weights of all paths of푇1◦푇2 such that the sequence of output symbols is 푤. For
instance, if 푤 =”ab” there are five of these paths, and the sum is 푃(ab | 푥) = 0.244.4.7 Weighted Finite State Transducers (WFST) 93
푃(푤 | 푥) can be also computed through a composition 푇1 ◦ 푇2 ◦ 푇3, where 푇3 is a
trivial WFST representing 푤. In this WFST, the only output string is “ab” and the
sum of the weights of all complete paths is 0.244, as expected.
The above weight computations assumed the Real semiring. If the Tropical semir￾ing is used instead, the sum becomes a maximization; that is, 푃(ab | 푥) ≈ 0.1. All
these computations can be efficiently done using the Forward or Backward algo￾rithms which, as mentioned before, simply amount to compute the “shortest distance”
in the Real or Tropical semirings (see Appendix B).
∅ / 0.5
a / 0.3
b / 0.2
∅ / 0.1
a / 0.5
b / 0.4
∅ / 0.2
a / 0.4
b / 0.4
(a) 푇1, character-level posteriorgram of an image 푥
푞0
∅ : 휖
a : a
b : b
∅ : 휖
a : 휖
b : b
∅ : 휖
a : a
b : 휖
(b) 푇2, CTC function
∅ : 휖 / 0.5
b : b / 0.2
a : a / 0.3
b : b / 0.4
∅ : 휖 / 0.1
a : a / 0.5
b : 휖 / 0.4
∅ : 휖 / 0.1
a : a / 0.5
b : b / 0.4
∅ : 휖 / 0.1
a : 휖 / 0.5
∅ : 휖 / 0.2
b : 휖 / 0.4 a : a / 0.4
∅ : 휖 / 0.2
b : b / 0.4
a : a / 0.4
∅ : 휖 / 0.2
b : b / 0.4
a : 휖 / 0.4
(c) 푇1 ◦ 푇2
a b
(d) 푇3, transcription hypothesis
∅ : 휖 / 0.5
a : a / 0.3
a : a / 0.5
a : 휖 / 0.5
∅ : 휖 / 0.1
b : b / 0.4
b : b / 0.4
b : b / 0.4
b : 휖 / 0.4
∅ : 휖 / 0.2
(e) 푇1 ◦ 푇2 ◦ 푇3
Fig. 4.14: Example of using WFST composition to model the full transcription posterior
푃(푤 | 푥) (represented by 푇1 ◦ 푇2), as well as to actually compute 푃(푤 | 푥) for a specific
transcription hypothesis 푤 = ab. In the compound WFST 푇1 ◦푇2 ◦푇3, each path corresponds
to one path in 푇1, one in 푇2 and the single path in 푇3), but the only output sequence is “ab”.
The posterior 푃(ab | 푥) is the sum of probabilities of the paths in 푇1 for which applying
the CTC function results in the sequence ab. This can now be directly computed as the sum
of the weights of all the paths in the compound WFST, which already embodies the CTC
function. Using the Real semiring, this sum is 0.244, while the sum is 0.1 if the Tropical
semiring is adopted, which yields the so called “Viterbi approximation”. In the latest case,
an optimal path ∅:휖, a:a, b:b is also obtained, which correspond to the path ∅:휖 , a, b in 푇1.94 4 Probabilistic Models for Handwritten Text
4.7.3 Lattices Represented as WFST or WFSA
As commented in previous sections, a word graph or character lattice (WG) can be
obtained as a byproduct of decoding an input image 푥, that is, solving or approaching
arg max푤 푃(푤 | 푥). Rather than a single best solution, a WG generally provides a
compact representation in the form of a graph or lattice of the joint distribution18
푝(푥, 푤) for a huge number of sequences 푤
′
for which 푝(푥, 푤′
) is largest.
In the literature, there are multiple definitions of WG. Some authors [58] simply
define it as a labeled, weighted, directed acyclic graph containing the transcription
hypotheses, and their (log-)probabilities 푃(푤 | 푥), or (log-)densities 푝(푥, 푤). Most of
the seminal works are applied to speech recognition, where it is sometimes useful to
represent the “time” at which a certain word was uttered [69]. In the traditional HTK
toolkit, lattices can contain character and/or word “time” alignment information, as
well as HMM or state-level alignment information [114].
Here we follow [73], where WGs are defined as WFSTs where the output symbols
are words (or characters, in lexicon-free models), and the input symbols represent the
most fine-grained hidden variable of the assumed probabilistic model. For instance,
for an HMM approach, each input symbol may represent an individual HMM state
(or a transition between two states, for convenience). And in the case of CRNNs, the
input symbols represent character-level labels, as in Fig. 4.14 (c).
Just as the aforementioned figure shows, 휖-symbols are used at the output to cope
with the fact the number of output symbols (e.g. words) is typically smaller than the
number of states transitions or labels. Epsilon symbols are not allowed at the input.
Accordingly, all paths leading to a given state have the same number of input symbols
and we can assign this number to each state as an “alignment” – that is, a “time” (in
speech) or a “horizontal coordinate” (in text). For instance, the WFST of Fig. 4.14 (c)
has a left-to-right state order and an alignment position can be assigned to each rank
of states: 0 to the first (initial) state and 1, 2 and 3 for the three remaining ranks
(columns), each rank corresponding to a frame in the character-level posteriorgram
of Fig. 4.14 (a). Such an alignment is depicted in Fig. 4.15 (a), below. In the sequel,
alignment information will be denoted as a function 푎 : Q → N .
In most cases, the input symbols are only needed to determine geometric positions
of the output tokens. Thus, we can represent the lattices in a more compact form
(using less states and edges). So, a compact lattice is defined as a WFSA representing
only the possible transcription hypotheses (sequences of words or characters) and
the alignment (positional coordinates) defined by the corresponding input frames.19
18 In the example of Fig. 4.14 the lattice corresponding to 푇1 ◦ 푇2 was directly derived from a
character-level posteriorgram supposedly provided by a CRNN. A property of such lattices is that
the sum of probabilities of all the paths is 1, which qualify it as a proper model of 푃(푤 | 푥). Under
this point of view, 푥 is given which implies 푝(푥) = 1. Therefore, stretching a bit the notation, the
same lattice can also be considered to represent a joint distribution 푝(푥, 푤) ≡ 푝(푥) 푃(푤 | 푥),
where 푝(푥) = 1. From now on, WGs or lattices will not be assumed to be normalized and thereby
they will be generally assumed to represent joint, rather than posterior distributions.
19 This definition is as implemented in https://kaldi-asr.org/doc/lattices.html.4.7 Weighted Finite State Transducers (WFST) 95
Fig. 4.15 (a) shows again the lattice from Fig. 4.14 (c), now with alignment infor￾mation 푎(푞), along with its equivalent compact version (Fig. 4.15 (b)). Notice that
input symbols are lost, but 푎(푞) is now shown as a number inside each state 푞.
0 1
1
1
2
2
2
3
3
3
∅ : 휖 / 0.5
b : b / 0.2
a : a / 0.3
b : b / 0.4
∅ : 휖 / 0.1
a : a / 0.5
b : 휖 / 0.4
∅ : 휖 / 0.1
a : a / 0.5
b : b / 0.4
∅ : 휖 / 0.1
a : 휖 / 0.5
∅ : 휖 / 0.2
b : 휖 / 0.4 a : a / 0.4
∅ : 휖 / 0.2
b : b / 0.4
a : a / 0.4
∅ : 휖 / 0.2
b : b / 0.4
a : 휖 / 0.4
(a) Lattice 푇
0
1
1
2
2
3
3
3
a / 0.3
b / 0.2
a / 0.04
b / 0.4
휖 / 0.333
a / 0.4
휖 / 0.833
휖 / 0.4
a / 0.5
b / 0.04
휖 / 0.8
b / 0.4
(b) Compact version of 푇 determinized in Tropical semiring
Fig. 4.15: Example of a lattice, represented as a WFST and an equivalent compact version
obtained under the Tropical semiring. In a nutshell, if more than one path provides the
seam sequence of output symbols, only the one with maximum weight is retained. path
provides the seam sequence of output symbols. Alignment information 푎(· ) is shown as a
number inside each state. The numbers correspond to horizontal coordinates of character
hypothesis boundaries. The information about the input symbols is lost in the compact
version, but alignment data are preserved.
Compact lattices are deterministic, as it is the case of Fig. 4.15 (b), where no
two edges labeled with the same token depart from any state. Therefore they are
also unambiguous (no two paths exist with the same token sequence). In this case,
determinization was performed on the Tropical semiring, keeping only the best paths
and the corresponding alignments. See Appendix B for details.
On the other hand, note that the original lattice (Fig. 4.15 (a)) was proper and
thereby stochastically consistent (path weights sum up to 1), so it could be properly
assumed to model 푃(푤 | 푥), as in the example of Fig. 4.14. But these properties are lost
in compact lattices, which usually are rather considered to model a joint distribution
(see footnote18), whose weights should be normalized, as discussed below.96 4 Probabilistic Models for Handwritten Text
4.7.4 Normalization of Lattice Weights
Many applications of WGs, like for example, computing PrIxs (see Chapter 5),
require that the values of edge weights are normalized in some way to make them
probabilistically meaningful. In general the weight of a WG edge is assumed to
represent a single word or character likelihood.20 So, the product of weights of the
edges of a complete path of an WG can be interpreted as an estimate of the joint
density 푝(푥, 푤, 푎), for a text line image 푥, a transcription hypothesis 푤 and a word
alignment 푎. This is directly the case for a WG produced using HMMs, but it also
applies to a WG obtained using the output of a CRNN combined with a (maybe
trivial) LM, as discussed in Sec. 4.6.1.
Before starting with the normalization methods, we need to introduce the relation￾ship between the HTR probabilistic framework (Eqs. 4.6, 4.18) and the information
provided by a WG obtained using the corresponding HTR models. Using the WFST
notation21 introduced in this section, the joint probability density 푝(푥, 푤, 푎) is ap￾proximated by a WG 퐺 as:
푝(푥, 푤, 푎) ≈ 휔˜ (휙)
def
= p
G
(푥, 푤, 푎) (4.27)
where 휙 = 푒1, . . . , 푒푘 is the unique complete path in 퐺 such that 푝(푒1) = 푞0 (the
initial state of 퐺), 푛(푒푘) is a final state 푞, such that 휚(푞) ≠ 0, and the word and ¯
alignment sequences are: 푤 = 푙(푒1), . . . , 푙(푒푘), 푎 = 푎(푛(푒1)), . . . , 푎(푛(푒푘)).
By marginalization on the alignments, the joint probability density 푝(푥, 푤) is
similarly approximated as:
푝(푥, 푤) ≈ Ê
휙: 푙(휙)=푤
휔˜ (휙)
def
= p
G
(푥, 푤) (4.28)
By further marginalization, the unconditional probability density 푝(푥) for an
image region 푥 is approximated by the accumulated score of all the WG paths:
푝(푥) ≈ Ê
푤
p
G
(푥, 푤) =
Ê
휙
휔˜ (휙)
def
= p
G
(푥) (4.29)
Then, the posteriors 푃(푤, 푎 | 푥) and 푃(푤 | 푥) can be approximated as:
푃(푤, 푎 | 푥) ≈ p
G
(푥, 푤, 푎) ⊘ p
G
(푥)
def
= PG (푤, 푎 | 푥) (4.30)
푃(푤 | 푥) ≈ p
G
(푥, 푤) ⊘ p
G
(푥)
def
= PG (푤 | 푥) (4.31)
Note that if 퐺 is deterministic22, each path 휙 determines a unique pair 푤, 푎.
Consequently, there is only one addend in Eq. (4.28) for the unique path 휙 such that
푙(휙) =푤 and, for the same reason, PG (푤, 푎 | 푥) ≡ PG (푤 | 푥).
20 In general, likelihoods are probability densities which are just real numbers, not necessarily
bounded in [0, 1].
21 Using the generic semiring operators ⊕, ⊗ and ⊘, rather than the conventional Real semiring
sum, product, and division, will allow us to conveniently interpret the results of this section as
needed in other chapters of this book.
22 Hereafter WGs will be assumed to be deterministic (and so unambiguous), unless noted otherwise.4.7 Weighted Finite State Transducers (WFST) 97
The Backward and Forward Algorithms
Two different normalizations of edge weights are considered here. In both cases, to
achieve computation efficiency the forward and backward functions are required. As
mentioned in previous parts of this chapter, by selecting the appropriate semirings,
these functions can be implemented as shortest distance WFST operations. Here
we present a traditional implementation from which interesting properties of the
normalizations based on these functions can be easily derived. To simplify handling
of subsets of states and edges of a WFST, we introduce the following notation:
F
def
={푞 ∈ Q : 휚(푞) ≠ 0¯ } will denote the set of “final states” and, for a state 푞,
P (푞)
def
={푒 ∈ E : 푛(푒) = 푞} and N (푞)
def
={푒 ∈ E : 푝(푒) = 푞} will denote the set of edges
leading to (or preceding) 푞 and those departing from (or next to) 푞, respectively.
The forward 훼(·) and backward 훽(·) functions are recursively described as:
훼(푞) =



1¯ Ê
if 푞 =푞0
푒∈ P (푞)
훼(푝(푒)) ⊗ 휔(푒) otherwise (4.32)
훽(푞) =



Ê
휚(푞) if 푞 ∈ F
푒∈N (푞)
휔(푒) ⊗ 훽(푛(푒)) otherwise (4.33)
Since a WG is a direct acyclic graph, the states can be assumed to be sorted in
topological order. Therefore, dynamic programming can be used to efficiently com￾pute these recurrences just by storing the incrementally computed values of 훼(푞)
and 훽(푞) in arrays indexed by the WG states 푞 ∈ Q. In what follows array or vector
notation will be generally used for these functions; that is 훼푞≡훼(푞) and 훽푞≡ 훽(푞).
As shown in [100], these 훼 and 훽 functions fulfill the following property:
푝퐺 (푥) =
Ê
휙∈Φ
휔˜ (휙) =
Ê
푞∈ F
훼푞 ⊗ 휚(푞) = 훽푞0
(4.34)
Edge-Posterior Normalization
This normalization is key to computing PrIx relevance probabilities, as we will see
later on. The edge posterior probability 휑(푒) of a given WG edge 푒 ∈ E is defined as:
휑(푒)
def
=
Ê
푤,푎:∃휙∈Φ,
푙(휙)=푤,푒∈휙
PG (푤, 푎 | 푥), 휑(푒)
★
=
Ê
푤:∃휙∈Φ,
푙(휙)=푤,푒∈휙
PG (푤 | 푥) (4.35)
where PG (푤, 푎 | 푥) and PG (푤 | 푥) are the posterior approximations given in Eqs. (4.28 -
4.30). The second 휑(푒) definition is given assuming deterministic WG. In words,
휑(푒) is a normalized sum of weights of all complete paths that include the edge 푒.98 4 Probabilistic Models for Handwritten Text
Fig. 4.16 shows how the edges of a WFSA are re-weighted according to edge￾posterior normalization, with operations in the Real semiring.
0 100
9
33
31
33
62
78
Original t / 0.05
that / 0.5
Hat / 0.1
hat / 0.6
that / 1
that / 1
that / 1
is / 0.7
is / 1
0 100
9
33
31
33
62
78
Edge-posterior normalized t / 0.05
that / 0.79
Hat / 0.16
hat / 0.05
that / 0.79
that / 0.16
that / 0.05
is / 1
is / 1
Fig. 4.16: Example of edge-posterior normalization. The original WFSA is a version of
the original graph in Fig. 3.8 (top), which has been determinized according to the Tropical
semiring (see Appendix B for details). The numbers in the states are horizontal word
boundary alignments. In the normalized WFST, for any position between 0 and 100, the
sum of weights of vertically stacked edges which “include” this position is exactly 1.0.
A WG normalized in this way fulfills the following properties [100]:
Property 4.1 (efficient dynamic programming computation)
휑(푒) =
￾
훼푝(푒) ⊗ 휔(푒) ⊗ 훽푛(푒)

⊘ 훽푞0
(4.36)
Property 4.2 (states are flow preserving)
Ê
푒∈ P (푞)
휑(푒) =
Ê
푒∈N (푞)
휑(푒), ∀푞 ∈ Q− F−{푞0} (4.37)
Property 4.3 (frame-level edge posterior consistency)
Ê
푒∈ E:
푎( 푝(푒) ) ≤푖<푎(푛(푒) )
휑(푒) = 1¯, ∀푖 : 푎(푞0) ≤ 푖 < 푎(푞
′
), 푞′
∈ F (4.38)
As discussed in Chapter 5, the last property is the most important one for efficient
PrIx computation. In words, it states that for whichever horizontal coordinate or
frame position 푖 along the line image width, the sum of normalized weights of edges
whose positional intervals (defined by the coordinates of their start and end states)
include that position, must be 1 (or just 1 if we operate in the ¯ Real semiring)4.7 Weighted Finite State Transducers (WFST) 99
Sentence-Posterior Normalization
The sentence-posterior normalization ensures that the complete path weights of all
WG paths are probabilistically consistent. It also preserves the rank order of the paths
according to their original weights. Therefore, this normalization is useful whenever
the WG needs be interpreted as a probabilistic generative model [94, 92].
The sentence posterior 휓(푒) for a given WG edge, 푒 ∈ E, is directly defined using
the backward function (Eq. 4.33) as:
휓(푒) =
￾
휔(푒) ⊗ 훽푛(푒)

⊘ 훽푝(푒) (4.39)
Fig. 4.17 shows how the original graph of Fig. 4.16 is re-weighted according to
sentence-posterior normalization with operations in the Real semiring.
0 100
9
33
31
33
62
78
Sentence-posterior normalized t / 0.05
that / 0.79
Hat / 0.16
hat / 1
that / 1
that / 1
that / 1
is / 1
is / 1
Fig. 4.17: Example showing the sentence-posterior normalized weights of the original
WFSA of Fig. 4.16. The sum of complete path weights is 1.0 and for each state, the sum of
weights for all its departing edges is 1.0.
If a WG is normalized in this way, it fulfills the following properties [94, 92]:
Property 4.4 (consistency of complete path weights)
Ê
휙∈Φ
Ì
푒∈휙
휓(푒) = 1¯ (4.40)
where Φ is the set of all complete WG paths.
Property 4.5 (in a deterministic WG, path weights are sentence posterior estimates)
Ì
푒∈휙: 푙(휙)=푤
휓(푒) = PG (푤 | 푥) (4.41)
Property 4.6 (the graph is proper)
Ê
푒∈N (푞)
휓(푒) = 1¯ ∀푞 ∈ Q − F (4.42)
That is, for each not-final state, the sum of the weights of all its edges must be 1 (or ¯
just 1.0 operating on the Real semiring)100 4 Probabilistic Models for Handwritten Text
References
1. B., Bluche, T., Knibbe, M., Benzeghiba, M.F., Messina, R., Louradour, J., Kermorvant, C.:
The A2iA Multi-lingual Text Recognition System at the Second Maurdor Evaluation. In:
14th International Conference on Frontiers in Handwriting Recognition, pp. 297–302 (2014).
DOI 10.1109/ICFHR.2014.57
2. Bahdanau, D., Cho, K., Bengio, Y.: Neural Machine Translation by Jointly Learning to Align
and Translate. CoRR abs/1409.0473 (2014). URL http://arxiv.org/abs/1409.0473
3. Bahl, L., Brown, P., de Souza, P., Mercer, R.: Maximum mutual information estimation of
hidden Markov model parameters for speech recognition. In: ICASSP ’86. IEEE International
Conference on Acoustics, Speech, and Signal Processing, vol. 11, pp. 49–52 (1986). DOI
10.1109/ICASSP.1986.1169179
4. Baum, L.E., Eagon, J.A.: An inequality with applications to statistical estimation for proba￾bilistic functions of Markov processes and to a model for ecology. Bulletin of the American
Mathematical Society 73(3), 360–363 (1967). URL https://projecteuclid.org:
443/euclid.bams/1183528841
5. Baum, L.E., Petrie, T., Soules, G., Weiss, N.: A maximization technique occurring in the
statistical analysis of probabilistic functions of markov chains. The Annals of Mathematical
Statistics 41(1), 164–171 (1970). DOI 10.1214/aoms/1177697196. URL https://doi.or
g/10.1214/aoms/1177697196
6. Bay, H., Tuytelaars, T., Van Gool, L.: SURF: Speeded Up Robust Features. In: A. Leonardis,
H. Bischof, A. Pinz (eds.) Computer Vision – ECCV 2006, pp. 404–417. Springer Berlin
Heidelberg, Berlin, Heidelberg (2006)
7. Bertolami, R., Bunke, H.: Hidden Markov model-based ensemble methods for offline hand￾written text line recognition. Pattern Recognition 41(11), 3452 – 3460 (2008). DOI
https://doi.org/10.1016/j.patcog.2008.04.003
8. Bluche, T.: Deep Neural Networks for Large Vocabulary Handwritten Text Recognition. Ph.D.
thesis, Universite Paris Sud - Paris XI (2015). URL ´ https://tel.archives-ouvertes.
fr/tel-01249405
9. Bluche, T., Messina, R.: Gated Convolutional Recurrent Neural Networks for Multilingual
Handwriting Recognition. In: 14th IAPR International Conference on Document Analysis
and Recognition (ICDAR), vol. 01, pp. 646–651 (2017). DOI 10.1109/ICDAR.2017.111
10. Bluche, T., Ney, H., Kermorvant, C.: The LIMSI handwriting recognition system for the
HTRtS 2014 contest. In: 13th International Conference on Document Analysis and Recogni￾tion (ICDAR), pp. 86–90. IEEE (2015)
11. Bourlard, H., Wellekens, C.J.: Links between markov models and multilayer perceptrons.
IEEE Transactions on Pattern Analysis and Machine Intelligence 12(12), 1167–1178 (1990).
DOI 10.1109/34.62605
12. Bridle, J.S.: Probabilistic Interpretation of Feedforward Classification Network Outputs, with
Relationships to Statistical Pattern Recognition. In: F.F. Soulie, J. H ´ erault (eds.) Neurocom- ´
puting, pp. 227–236. Springer Berlin Heidelberg, Berlin, Heidelberg (1990)
13. Brown, P.F., Cocke, J., Pietra, S.A.D., Pietra, V.J.D., Jelinek, F., Lafferty, J.D., Mercer, R.L.,
Roossin, P.S.: A Statistical Approach to Machine Translation. Comput. Linguist. 16(2), 79–85
(1990). URL http://dl.acm.org/citation.cfm?id=92858.92860
14. Brown, R., Frederking, R.: Applying statistical English language modeling to symbolic ma￾chine translation. In: Proceedings of the 6th International Conference on Theoretical and
Methodological Issues in Machine Translation (TMI-95), pp. 221–239 (1995)
15. Bukhari, S.S., Breuel, T.M., Asi, A., El-Sana, J.: Layout Analysis for Arabic Historical
Document Images Using Machine Learning. In: 2012 International Conference on Frontiers
in Handwriting Recognition, pp. 639–644 (2012). DOI 10.1109/ICFHR.2012.227
16. Chen, K., Seuret, M., Hennebert, J., Ingold, R.: Convolutional Neural Networks for Page
Segmentation of Historical Document Images. In: 2017 14th IAPR International Conference
on Document Analysis and Recognition (ICDAR), vol. 01, pp. 965–970 (2017). DOI
10.1109/ICDAR.2017.161References 101
17. Chen, K., Seuret, M., Liwicki, M., Hennebert, J., Ingold, R.: Page segmentation of historical
document images with convolutional autoencoders. In: 2015 13th International Conference
on Document Analysis and Recognition (ICDAR), pp. 1011–1015 (2015). DOI 10.1109/IC
DAR.2015.7333914
18. Chen, S.F., Goodman, J.: An empirical study of smoothing techniques for language modeling.
Computer Speech & Language 13(4), 359–394 (1999). DOI https://doi.org/10.1006/csla.1
999.0128
19. Cho, K., van Merrienboer, B., G ¨ulc¸ehre, C¸ ., Bougares, F., Schwenk, H., Bengio, Y.: Learning
Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.
CoRR abs/1406.1078 (2014). URL http://arxiv.org/abs/1406.1078
20. Cruz, F., Terrades, O.R.: A probabilistic framework for handwritten text line segmentation.
CoRR abs/1805.02536 (2018). URL http://arxiv.org/abs/1805.02536
21. Cybenko, G.: Approximation by superpositions of a sigmoidal function. Mathematics of
Control, Signals and Systems 2(4), 303–314 (1989). DOI 10.1007/BF02551274
22. Defferrard, M., Bresson, X., Vandergheynst, P.: Convolutional Neural Networks on Graphs
with Fast Localized Spectral Filtering. In: D.D. Lee, M. Sugiyama, U.V. Luxburg, I. Guyon,
R. Garnett (eds.) Advances in Neural Information Processing Systems 29, pp. 3844–3852.
Curran Associates, Inc. (2016)
23. Dempster, A.P., Laird, N.M., Rubin, D.B.: Maximum Likelihood from Incomplete Data via
the EM Algorithm. Journal of the Royal Statistical Society. Series B (Methodological) 39(1),
1–38 (1977). URL http://www.jstor.org/stable/2984875
24. Diem, M., Kleber, F., Fiel, S., Gr ¨uning, T., Gatos, B.: cBAD: ICDAR2017 Competition on
Baseline Detection. In: 2017 14th IAPR International Conference on Document Analysis and
Recognition (ICDAR), vol. 1, pp. 1355–1360 (2017). DOI 10.1109/ICDAR.2017.222
25. Digalakis, V., Tsakalidis, S., Harizakis, C., Neumeyer, L.: Efficient speech recognition using
subvector quantization and discrete-mixture HMMs. Computer Speech & Language 14(1),
33–46 (2000). DOI https://doi.org/10.1006/csla.1999.0134
26. Doetsch, P., Kozielski, M., Ney, H.: Fast and Robust Training of Recurrent Neural Networks
for Offline Handwriting Recognition. In: 2014 14th International Conference on Frontiers in
Handwriting Recognition, pp. 279–284 (2014). DOI 10.1109/ICFHR.2014.54
27. Duchi, J., Hazan, E., Singer, Y.: Adaptive Subgradient Methods for Online Learning and
Stochastic Optimization. Journal Machine Learning Research 12, 2121–2159 (2011)
28. El-Yacoubi, A., Sabourin, R., Suen, C.Y., Gilloux, M.: An HMM-Based Approach for Off￾Line Unconstrained Handwritten Word Modeling and Recognition. IEEE Trans. Pattern Anal.
Mach. Intell. 21(8), 752–760 (1999). DOI 10.1109/34.784288
29. Elman, J.L.: Finding structure in time. Cognitive Science 14(2), 179–211 (1990). DOI
https://doi.org/10.1016/0364-0213(90)90002-E. URL http://www.sciencedirect.co
m/science/article/pii/036402139090002E
30. Espana-Boquera, S., Castro-Bleda, M.J., Gorbe-Moya, J., Zamora-Martinez, F.: Improving
Offline Handwritten Text Recognition with Hybrid HMM/ANN Models. IEEE Transactions
on Pattern Analysis and Machine Intelligence 33(4), 767–779 (2011). DOI 10.1109/TPAM
I.2010.141
31. Fischer, A., Frinken, V., Bunke, H., Suen, C.Y.: Improving HMM-Based Keyword Spotting
with Character Language Models. In: 2013 12th International Conference on Document
Analysis and Recognition, pp. 506–510 (2013). DOI 10.1109/ICDAR.2013.107
32. Fukushima, K., Miyake, S.: Neocognitron: A Self-Organizing Neural Network Model for a
Mechanism of Visual Pattern Recognition. In: S.i. Amari, M.A. Arbib (eds.) Competition
and Cooperation in Neural Nets, pp. 267–285. Springer Berlin Heidelberg (1982)
33. Garz, A., Fischer, A., Sablatnig, R., Bunke, H.: Binarization-Free Text Line Segmentation for
Historical Documents Based on Interest Point Clustering. In: 2012 10th IAPR International
Workshop on Document Analysis Systems, pp. 95–99 (2012). DOI 10.1109/DAS.2012.23
34. Gers, F.A., Schmidhuber, J.A., Cummins, F.A.: Learning to Forget: Continual Prediction with
LSTM. Neural Comput. 12(10), 2451–2471 (2000). DOI 10.1162/089976600300015015102 4 Probabilistic Models for Handwritten Text
35. Gimenez, ´ A., Juan, A.: Embedded Bernoulli Mixture HMMs for Handwritten Word Recog￾nition. In: 2009 10th International Conference on Document Analysis and Recognition, pp.
896–900 (2009). DOI 10.1109/ICDAR.2009.66
36. Glorot, X., Bengio, Y.: Understanding the difficulty of training deep feedforward neural
networks. In: Y.W. Teh, M. Titterington (eds.) Proceedings of the Thirteenth International
Conference on Artificial Intelligence and Statistics, Proceedings of Machine Learning Re￾search, vol. 9, pp. 249–256. PMLR, Chia Laguna Resort, Sardinia, Italy (2010)
37. Graves, A., Fernandez, S., Gomez, F., Schmidhuber, J.: Connectionist Temporal Classifica- ´
tion: Labelling Unsegmented Sequence Data with Recurrent Neural Networks. In: Proceed￾ings of the 23rd International Conference on Machine Learning, ICML ’06, pp. 369–376.
ACM, New York, NY, USA (2006). DOI 10.1145/1143844.1143891
38. Graves, A., Jaitly, N., Mohamed, A.: Hybrid speech recognition with Deep Bidirectional
LSTM. In: 2013 IEEE Workshop on Automatic Speech Recognition and Understanding, pp.
273–278 (2013). DOI 10.1109/ASRU.2013.6707742
39. Graves, A., Liwicki, M., Fernandez, S., Bertolami, R., Bunke, H., Schmidhuber, J.: A novel ´
connectionist system for unconstrained handwrting recognition. IEEE Trans. on PAMI 31(5),
855–868 (2009)
40. Graves, A., Mohamed, A., Hinton, G.: Speech recognition with deep recurrent neural net￾works. In: 2013 IEEE International Conference on Acoustics, Speech and Signal Processing,
pp. 6645–6649 (2013). DOI 10.1109/ICASSP.2013.6638947
41. G ¨unter, S., Bunke, H.: HMM-based handwritten word recognition: on the optimization of the
number of states, training iterations and Gaussian components. Pattern Recognition 37(10),
2069–2079 (2004). DOI https://doi.org/10.1016/j.patcog.2004.04.006
42. Hahnloser, R.H.R., Sarpeshkar, R., Mahowald, M.A., Douglas, R.J., Seung, H.S.: Digital
selection and analogue amplification coexist in a cortex-inspired silicon circuit. Nature 405,
947–951 (2000). DOI 10.1038/35016072
43. He, D., Cohen, S., Price, B., Kifer, D., Giles, C.L.: Multi-Scale Multi-Task FCN for Semantic
Page Segmentation and Table Detection. In: 2017 14th IAPR International Conference
on Document Analysis and Recognition (ICDAR), vol. 01, pp. 254–261 (2017). DOI
10.1109/ICDAR.2017.50
44. Hochreiter, S., Schmidhuber, J.: Long Short-Term Memory. Neural Computation 9(8), 1735–
1780 (1997). DOI 10.1162/neco.1997.9.8.1735
45. Hu, B., Lu, Z., Li, H., Chen, Q.: Convolutional Neural Network Architectures for Matching
Natural Language Sentences. In: Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, K.Q.
Weinberger (eds.) Advances in Neural Information Processing Systems 27, pp. 2042–2050.
Curran Associates, Inc. (2014)
46. Huang, X., Hon, H., Hwang, M., Lee, K.: A comparative study of discrete, semicontinuous,
and continuous hidden Markov models. Computer Speech & Language 7(4), 359–368 (1993).
DOI https://doi.org/10.1006/csla.1993.1019
47. Hull, J.J.: Document image skew detection: Survey and annotated bibliography. In: Document
Analysis Systems II. Word Scientific, pp. 40–64. World Scientific (1998)
48. Jelinek, F.: Continuous speech recognition by statistical methods. Proceedings of the IEEE
64(4), 532–556 (1976). DOI 10.1109/PROC.1976.10159
49. Jelinek, F.: Statistical Methods for Speech Recognition. MIT Press (1998)
50. Ji, S., Xu, W., Yang, M., Yu, K.: 3D Convolutional Neural Networks for Human Action
Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence 35(1), 221–
231 (2013). DOI 10.1109/TPAMI.2012.59
51. Juang, B.H., Hou, W., Lee, C.H.: Minimum classification error rate methods for speech
recognition. IEEE Transactions on Speech and Audio Processing 5(3), 257–265 (1997).
DOI 10.1109/89.568732
52. Katz, S.: Estimation of probabilities from sparse data for the language model component of
a speech recognizer. IEEE Transactions on Acoustics, Speech, and Signal Processing 35(3),
400–401 (1987). DOI 10.1109/TASSP.1987.1165125References 103
53. Kingma, D.P., Ba, J.: Adam: A Method for Stochastic Optimization. CoRR abs/1412.6980
(2014). URL http://arxiv.org/abs/1412.6980
54. Kneser, R., Ney, H.: Improved backing-off for M-gram language modeling. In: 1995 Interna￾tional Conference on Acoustics, Speech, and Signal Processing, vol. 1, pp. 181–184 (1995).
DOI 10.1109/ICASSP.1995.479394
55. Kozielski, M., Forster, J., Ney, H.: Moment-Based Image Normalization for Handwritten Text
Recognition. In: 2012 International Conference on Frontiers in Handwriting Recognition,
pp. 256–261 (2012). DOI 10.1109/ICFHR.2012.236
56. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D.:
Backpropagation Applied to Handwritten Zip Code Recognition. Neural Computation 1(4),
541–551 (1989). DOI 10.1162/neco.1989.1.4.541
57. Likforman-Sulem, L., Zahour, A., Taconet, B.: Text line segmentation of historical documents:
a survey. International Journal of Document Analysis and Recognition (IJDAR) 9(2), 123–138
(2007). DOI 10.1007/s10032-006-0023-z
58. Ljolje, A., Pereira, F., Riley, M.: Efficient general lattice generation and rescoring. In:
Proceedings of the Sixth European Conference on Speech Communication and Technology
(EUROSPEECH’99), pp. 1251–1254 (1999)
59. Louloudis, G., Gatos, B., Pratikakis, I., Halatsis, C.: Text Line and Word Segmentation of
Handwritten Documents. Pattern Recogn. 42(12), 3169–3183 (2009). DOI 10.1016/j.patcog
.2008.12.016
60. Lowe, D.G.: Object recognition from local scale-invariant features. In: Proceedings of the
Seventh IEEE International Conference on Computer Vision, vol. 2, pp. 1150–1157 (1999).
DOI 10.1109/ICCV.1999.790410
61. Maas, A.L., Hannun, A.Y., Ng, A.Y.: Rectifier nonlinearities improve neural network acoustic
models. In: Proceedings of the 30th International Conference on Machine Learning, vol. 30,
p. 3 (2013)
62. Mao, S., Rosenfeld, A., Kanungo, T.: Document structure analysis algorithms: a literature
survey. In: T. Kanungo, E.H.B. Smith, J. Hu, P.B. Kantor (eds.) Document Recognition and
Retrieval X, vol. 5010, pp. 197 – 207. International Society for Optics and Photonics, SPIE
(2003). DOI 10.1117/12.476326. URL https://doi.org/10.1117/12.476326
63. Marti, U.V., Bunke, H.: Handwritten sentence recognition. In: Proceedings 15th International
Conference on Pattern Recognition. ICPR-2000, vol. 3, pp. 463–466 vol.3 (2000). DOI
10.1109/ICPR.2000.903584
64. Marti, U.V., Bunke, H.: On the influence of vocabulary size and language models in uncon￾strained handwritten text recognition. In: Proc. of Sixth International Conference on Docu￾ment Analysis and Recognition, pp. 260–265 (2001). DOI 10.1109/ICDAR.2001.953795
65. Marti, U.V., Bunke, H.: Using a statistical language model to improve the performance of an
HMM-based cursive handwriting recognition system. In: Hidden Markov Models, pp. 65–90.
World Scientific (2001). DOI 10.1142/9789812797605 0004
66. McCulloch, W.S., Pitts, W.: A logical calculus of the ideas immanent in nervous activity. The
bulletin of mathematical biophysics 5(4), 115–133 (1943). DOI 10.1007/BF02478259. URL
https://doi.org/10.1007/BF02478259
67. Mohri, M.: Semiring frameworks and algorithms for shortest-distance problems. Journal of
Automata, Languages and Combinatorics 7(3), 321–350 (2002)
68. Mohri, M., Pereira, F., Riley, M.: Speech Recognition with Weighted Finite-State Transducers,
pp. 559–584. Springer Berlin Heidelberg, Berlin, Heidelberg (2008). DOI 10.1007/978-3-5
40-49127-9 28
69. Ortmanns, S., Ney, H., Aubert, X.: A word graph algorithm for large vocabulary continuous
speech recognition. Computer Speech & Language 11(1), 43–72 (1997)
70. Pastor, M., Toselli, A., Vidal, E.: Projection Profile Based Algorithm for Slant Removal. In:
A. Campilho, M. Kamel (eds.) Image Analysis and Recognition, pp. 183–190. Springer Berlin
Heidelberg, Berlin, Heidelberg (2004)
71. Pesch, H., Hamdani, M., Forster, J., Ney, H.: Analysis of Preprocessing Techniques for Latin
Handwriting Recognition. In: 2012 International Conference on Frontiers in Handwriting
Recognition, pp. 280–284 (2012). DOI 10.1109/ICFHR.2012.179104 4 Probabilistic Models for Handwritten Text
72. Povey, D., Ghoshal, A., Boulianne, G., Burget, L., Glembek, O., Goel, N., Hannemann,
M., Motlicek, P., Qian, Y., Schwarz, P., Silovsky, J., Stemmer, G., Vesely, K.: The Kaldi
Speech Recognition Toolkit. In: IEEE 2011 Workshop on Automatic Speech Recognition
and Understanding. IEEE Signal Proc. Society (2011). IEEE Catalog No.: CFP11SRW-USB
73. Povey, D., Hannemann, M., Boulianne, G., Burget, L., Ghoshal, A., Janda, M., Karafiat, M., ´
Kombrink, S., Motl´ıcek, P., Qian, Y., Riedhammer, K., Vesel´y, K., Vu, N.T.: Generating exact ˇ
lattices in the WFST framework. In: 2012 IEEE Int. Conference on Acoustics, Speech and
Signal Processing (ICASSP), pp. 4213–4216 (2012). DOI 10.1109/ICASSP.2012.6288848
74. Povey, D., Woodland, P.C.: Minimum Phone Error and I-smoothing for improved discrimi￾native training. In: 2002 IEEE International Conference on Acoustics, Speech, and Signal
Processing, vol. 1, pp. 105–108 (2002). DOI 10.1109/ICASSP.2002.5743665
75. Puigcerver, J.: Are Multidimensional Recurrent Layers Really Necessary for Handwritten
Text Recognition? In: 2017 14th IAPR International Conference on Document Analysis and
Recognition (ICDAR), vol. 01, pp. 67–72 (2017). DOI 10.1109/ICDAR.2017.20
76. Puigcerver, J.: A probabilistic formulation of keyword spotting. Ph.D. thesis, Universitat
Politecnica de Val ` encia (2018) `
77. Puigcerver, J., Toselli, A.H., Vidal, E.: Querying out-of-vocabulary words in lexicon-based
keyword spotting. Neural Computing and Applications 28(9), 2373–2382 (2017). DOI
10.1007/s00521-016-2197-8
78. Riesenhuber, M., Poggio, T.: Hierarchical models of object recognition in cortex. Nature
Neuroscience 2, 1019–1025 (1999). DOI 10.1038/14819
79. Robbins, H., Monro, S.: A stochastic approximation method. The Annals of Mathematical
Statistics 22(3), 400–407 (1951). URL http://www.jstor.org/stable/2236626
80. Romero, V., Toselli, A.H., Vidal, E.: Multimodal Interactive Handwritten Text Transcription.
Perception and Artif. Intell. (MPAI). World Scientific, (2012)
81. Rosenblatt, F.: The Perceptron: A Probabilistic Model for Information Storage and Organiza￾tion in The Brain. Psychological Review pp. 65–386 (1958)
82. Rothacker, L., Vajda, S., Fink, G.A.: Bag-of-Features Representations for Offline Handwriting
Recognition Applied to Arabic Script. In: 2012 International Conference on Frontiers in
Handwriting Recognition, pp. 149–154 (2012). DOI 10.1109/ICFHR.2012.185
83. Rumelhart, D.E., Hinton, G.E., Williams, R.J.: Learning representations by back-propagating
errors. Nature 323, 533–536 (1986). DOI 10.1038/323533a0
84. Saabni, R., Asi, A., El-Sana, J.: Text line extraction for historical document images. Pattern
Recognition Letters 35, 23–33 (2014). DOI https://doi.org/10.1016/j.patrec.2013.07.007.
Frontiers in Handwriting Processing
85. Schuster, M., Paliwal, K.K.: Bidirectional recurrent neural networks. IEEE Transactions on
Signal Processing 45(11), 2673–2681 (1997). DOI 10.1109/78.650093
86. Senior, A.W., Robinson, A.J.: An off-line cursive handwriting recognition system. IEEE
Transactions on Pattern Analysis and Machine Intelligence 20(3), 309–321 (1998). DOI
10.1109/34.667887
87. Shannon, C.E.: Prediction and Entropy of Printed English. Bell System Technical Journal
30(1), 50–64 (1951). DOI 10.1002/j.1538-7305.1951.tb01366.x
88. Shi, B., Bai, X., Yao, C.: An end-to-end trainable neural network for image-based sequence
recognition and its application to scene text recognition. IEEE Trans. on Pattern Analysis and
Machine Intelligence 39(11), 2298–2304 (2017). DOI 10.1109/TPAMI.2016.2646371
89. Siegelmann, H.T.: Computation Beyond the Turing Limit. Science 268(5210), 545–548
(1995). DOI 10.1126/science.268.5210.545. URL http://science.sciencemag.org/c
ontent/268/5210/545
90. Sundermeyer, M., Schl¨uter, R., Ney, H.: LSTM neural networks for language modeling. In:
13th Annual Conference of the Int. Speech Communication Association, pp. 194–197 (2012)
91. Sutskever, I., Vinyals, O., Le, Q.V.: Sequence to Sequence Learning with Neural Networks.
In: Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, K.Q. Weinberger (eds.) Advances
in Neural Information Processing Systems 27, pp. 3104–3112. Curran Associates, Inc. (2014)References 105
92. Sanc ´ hez, J.A., Rocha, M.A., Romero, V., Villegas, M.: On the derivational entropy of left-to￾right probabilistic finite-state automata and hidden markov models. Computational Linguis￾tics 44(1), 17–37 (2018). DOI 10.1162/COLI a 00306
93. Tay, Y.H., Lallican, P.M., Khalid, M., Knerr, S., Viard-Gaudin, C.: An analytical handwritten
word recognition system with word-level discriminant training. In: Proceedings of Sixth
International Conference on Document Analysis and Recognition, pp. 726–730 (2001). DOI
10.1109/ICDAR.2001.953885
94. Thompson, R.: Determination of probabilistic grammars for functionally specified
probability-measure languages. IEEE Transactions on Computers C-23(6), 603–614 (1974).
DOI 10.1109/T-C.1974.224001
95. Tieleman, T., Hinton, G.: Lecture 6.5-RMSprop: Divide the gradient by a running average of
its recent magnitude. COURSERA: Neural networks for machine learning 4(2) (2012)
96. Toselli, A.H., Juan, A., Gonzalez, J., Salvador, I., Vidal, E., Casacuberta, F., Keysers, D., ´
Ney, H.: Integrated handwriting recognition and interpretation using finite-state models. Int.
Journal of Pattern Recognition and Artificial Intelligence 18(04), 519–539 (2004)
97. Toselli, A.H., Puigcerver, J., Vidal, E.: Context-aware lattice based filler approach for key
word spotting in handwritten documents. In: 2015 13th Int. Conference on Document Analysis
and Recognition (ICDAR), pp. 736–740 (2015). DOI 10.1109/ICDAR.2015.7333859
98. Toselli, A.H., Romero, V., Pastor, M., Vidal, E.: Multimodal interactive transcription of text
images. Pattern Recognition 43(5), 1814 – 1825 (2010). DOI https://doi.org/10.1016/j.patc
og.2009.11.019
99. Toselli, A.H., Vidal, E.: Handwritten text recognition results on the Bentham collection
with improved classical N-gram-HMM methods. In: Proceedings of the 3rd International
Workshop on Historical Document Imaging and Processing, pp. 15–22. ACM (2015)
100. Toselli, A.H., Vidal, E., Romero, V., Frinken, V.: HMM Word-Graph Based Keyword Spotting
in Handwritten Document Images. Information Sciences 370(C), 497–518 (2016). DOI
10.1016/j.ins.2016.07.063
101. Vidal, E., Thollard, F., De La Higuera, C., Casacuberta, F., Carrasco, R.C.: Probabilistic
finite-state machines-part i. IEEE Transactions on Pattern Analysis and Machine Intelligence
27(7), 1013–1025 (2005)
102. Vidal, E., Thollard, F., De La Higuera, C., Casacuberta, F., Carrasco, R.C.: Probabilistic finite￾state machines-part ii. IEEE Transactions on Pattern Analysis and Machine Intelligence 27(7),
1026–1039 (2005)
103. Vidal, E., Toselli, A.H., Puigcerver, J.: Lexicon-based probabilistic indexing of handwritten
text images. Neural Computing and Applications pp. 1–20 (2023)
104. Vinciarelli, A., Bunke, H., Bengio, S.: Offline Recognition of Unconstrained Handwritten
Texts Using HMMs and Statistical Language Models. IEEE Transactions on Pattern Analysis
and Machine Intelligence 26(6), 709–720 (2004). DOI 10.1109/TPAMI.2004.14
105. Vinciarelli, A., Luettin, J.: A new normalization technique for cursive handwritten words.
Pattern Recognition Letters 22(9), 1043 – 1050 (2001). DOI https://doi.org/10.1016/S016
7-8655(01)00042-3
106. Vinyals, O., Toshev, A., Bengio, S., Erhan, D.: Show and Tell: Lessons Learned from the
2015 MSCOCO Image Captioning Challenge. IEEE Transactions on Pattern Analysis and
Machine Intelligence 39(4), 652–663 (2017). DOI 10.1109/TPAMI.2016.2587640
107. Viterbi, A.: Error bounds for convolutional codes and an asymptotically optimum decoding
algorithm. IEEE Transactions on Information Theory 13(2), 260–269 (1967). DOI 10.1109/
TIT.1967.1054010
108. Voigtlaender, P., Doetsch, P., Ney, H.: Handwriting Recognition with Large Multidimensional
Long Short- Term Memory Recurrent Neural Networks. In: 15th Int. Conf. on Frontiers in
Handwriting Recognition (ICFHR), pp. 228–233 (2016). DOI 10.1109/ICFHR.2016.0052
109. Wang, S., Uchida, S., Liwicki, M.: Part-based method on handwritten texts. In: Proceedings
of the 21st International Conference on Pattern Recognition (ICPR2012), pp. 339–342 (2012)
110. Werbos, P.J.: Beyond Regression: New Tools for Prediction and Analysis in the Behavioral
Sciences. Ph.D. thesis, Harvard University, Cambridge, MA (1974)106 4 Probabilistic Models for Handwritten Text
111. Werbos, P.J.: Backpropagation through time: what it does and how to do it. Proceedings of
the IEEE 78(10), 1550–1560 (1990). DOI 10.1109/5.58337
112. Witten, I.H., Bell, T.C.: The zero-frequency problem: estimating the probabilities of novel
events in adaptive text compression. IEEE Transactions on Information Theory 37(4), 1085–
1094 (1991). DOI 10.1109/18.87000
113. You, Q., Jin, H., Wang, Z., Fang, C., Luo, J.: Image Captioning With Semantic Attention. In:
The IEEE Conf. on Computer Vision and Patt. Recognition (CVPR), pp. 4651–4659 (2016)
114. Young, S., Evermann, G., Gales, M., Hain, T., Kershaw, D., Liu, X., Moore, G., Odell, J.,
Ollason, D., Povey, D.: The HTK book. Tech. rep., Cambridge Univ. Engineering Dep. (2002)Chapter 5
Probabilistic Indexing for Fast and Effective
Information Retrieval
Abstract This chapter provides details for the implementation of the different ap￾proaches proposed for PrIx under the probabilistic framework presented in Chapter 3,
using the HTR models and tools described in Chapter 4. After a relatively brief pre￾sentation of the implementation of image-processing oriented methods, based on
pixel-level posteriorgrams, in the rest of the chapter efficient algorithms are pre￾sented in detail which can be used to produce PrIxs under the HTR point of view.
In both cases, the aim is to process large collections of text images off-line, so as
to allow fast response to on-line queries with very low computing time complexity.
To this end, for each image of the collection a series of sufficiently likely “spots” is
extracted. Each spot contains a word or a character sequence, called (pseudo)word,
along with the corresponding relevance probability and word position information.
Two main approaches are discussed: lexicon-based and lexicon-free. The former
allows simpler implementations and provides better search accuracy whenever the
adopted lexicon provides enough coverage of the expected query words. The latter
is not as accurate, but it is much more versatile since the words to be indexed are
automatically “discovered” in the very images being indexed.
5.1 Lexicon-Based and Lexicon-Free PrIx
In Chapter 3 we proposed a probabilistic framework for PrIx, which introduced
theory-grounded principles to rank a set of results given a user’s query. While this
framework supports traditional KWS principles and methods (c.f. Chapter 7), the
main aim was to deal with PrIx proper, aiming to provide very fast response at
query time for huge collections of text images. Fast query response requires a really
fast way to compute, approximate, or otherwise determine 푃(푅 | 푥, 푣, . . .). Actually,
the fastest way to obtain the values of 푃(푅 | 푥, 푣, . . .) in the on-line query phase,
is to precompute these RPs in advance (i.e., off-line) for all the words in a given,
potentially interesting lexicon. Specifically, a precomputed PrIx, essentially consists
of a list of spots of the form [푥, 푣, 푃] or [푥, 푣, 푙, 푃] where 푥 is an image (ID), 푣 is a
(pseudo-)word, 푃 is a RP and 푙 is some information about the location of 푣 within 푥.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 107
A. H. Toselli et al., Probabilistic Indexing for Information Search and Retrieval 
in Large Collections of Handwritten Text Images, The Information Retrieval Series 49, 
https://doi.org/10.1007/978-3-031-55389-9_5 
 108 5 Probabilistic Indexing for Fast and Effective Information Retrieval
The different forms of RPs discussed in Chapter 3 essentially need to estimate
a posterior distribution over words, 푣; either in a small word-sized image region or
bounding box 풃, or over the transcripts 푤 of a text image region. In the first case,
we need a model of the isolated-word recognition posterior 푃(푣 | 푥, 풃) and in the
second, a model of the word or character sequence distribution 푃(푤 | 푥).
While 푃(푣 | 푥, 풃) can be given by any simple isolated word recognition method
(e.g., 푘-nearest neighbors), in Chapter 3 we already argued that the required distribu￾tion can be advantageously obtained using, instead, state-of-the-art HTR models and
methods. Sec. 5.2, below, will outline how this can be done. Since this approach re￾quires all the words 푣 which are expected to appear in the text images to be explicitly
given, this approach belongs to the so-called Lexicon-based category.
In the second case, we saw in Chapter 4, and particularly in Sec. 4.7, how 푃(푤 | 푥)
can be efficiently represented using Weighted Finite State Transducers (WFST). To
this end, we could first build the representation of 푃(푤, 푥) as a WFST and then
compute 푃(푅 | 푥, 푣, . . .) using standard operations on WFST. Note, however, that
since this also assumes the set of relevant words 푣 to be known, the approach is also
Lexicon-based.
An alternative to lexicon-based is to compute the RP, not just for given words,
but for any character sequence modeled by 푃(푤 | 푥) which is sufficiently likely
to be an actual word. Probabilistically detecting likely words in an image amounts
to predicting where a word-separation token (such as a blank or a punctuation
mark) is expected to appear in a character-sequence hypothesis. Note that it is not
necessary that such word-separation tokens are actually rendered in the images (in
fact, in historical manuscripts, words are seldom separated from each other in clear
or consistent ways). In most cases, word separation is reasonably well predicted
just by training the HTR models with correct and consistent transcripts. We call
“(pseudo)-words” the word-like character sequences detected in this way.
By adequately using the word-separation probabilities, all (pseudo)-words that
are likely enough to be present in a image can be detected. Then, this information can
be simply stored in the PrIx spots, and let the users query the resulting pseudo-word
index without any lexicon restriction. This is the scenario referred to as Lexicon-free.
In this chapter, we will introduce different algorithms to build such indexes,
for each of the flavors of the relevance probability introduced in Chapter 3. These
algorithms are all based on standard WFST algorithms and it is thus important to
understand the operations and algorithms described in Sec. 4.7.
5.2 Lexicon-based PrIx from Pixel-level Posteriorgrams
Here we outline methods based on HTR models and WGs, which can be used to
efficiently compute the 1-D posteriorgram of a text-line image region according to
Eq. (3.18). Following [12, 14, 15], the main idea is to use a WG [5, 15], obtained as a
byproduct of solving Eq. (3.25) [10, 15]. As discussed in Chapter 4, a WG of a (line)
image region, 푥, is a very compact representation of a huge amount of alternative
image transcription results, including the probability of each of the (millions of)
hypothesized words and the corresponding word segmentation boundaries.5.2 Lexicon-based PrIx from Pixel-level Posteriorgrams 109
The posteriorgram 푃(푣| 푥, 푖) can be obtained from a WG of 푥 following essentially
the same arguments as in Eq. (3.4) or, more specifically, its 1-D version, Eq. (3.18):
푃(푣 | 푥, 푖) ≈ Õ
풃∈S (푖)
푃(풃 | 푥, 푖) 푃(푣 | 푥, 풃) (5.1)
where S (푖) is a set of reasonably shifted and sized segments which contain the
horizontal coordinate 푖. Now, this set is given by the multiple word segmentation
hypotheses associated with all the WG edges, 푒, labeled with the word 푣 and such that
푖 is within the segmentation boundaries specified in the departing and ending nodes
of 푒. These boundaries are generally very accurate, not only for the best hypothesis
of the WG (called the “1-best” transcript), but also for most edges associated with
high-likelihood WG paths. Moreover, edge likelihoods also embody the segment
probabilities of Eq. (5.1). In sum, WG boundaries and probabilities provide highly
informative data from which Eq. (5.1) can be accurately computed.
To formally derive how to compute 푃(푣 | 푥, 푖) from a WG of 푥, rather than directly
departing from Eq. (5.1), it is better to rely on the equivalence between the horizontal
coordinate RP 푃(푅 | 푥, 푣, 푖) and the posteriorgram 푃(푣 | 푥, 푖), proved in Eq. (3.43),
and depart from Eq. (3.29):
푃(푣 | 푥, 푖) =
Õ
푤:
∃푘: 푤푘=푣
Õ
푎:
푎푘≤푖<푎푘+1
푃(푤, 푎 | 푥)
=
Õ
푤,푎:
∃푘: 푤푘=푣,
푎푘≤푖<푎푘+1
푃(푤, 푎 | 푥) ≈ Õ
푤,푎:∃휙,∃푒∈휙
푙(휙)=푤,푙(푒)=푣,
푎( 푝(푒) ) ≤ 푖<푎(푛(푒) )
PG (푤, 푎 | 푥)
=
Õ
푒: 푙(푒)=푣,
푎( 푝(푒) ) ≤ 푖<푎(푛(푒) )
Õ
푤,푎:∃휙
푙(휙)=푤,푒∈휙
PG (푤, 푎 | 푥) =
Õ
푒: 푙(푒)=푣,
푎( 푝(푒) ) ≤ 푖<푎(푛(푒) )
휑(푒)
(5.2)
That is, the posteriorgram is obtained by accumulating the edge-posteriors, 휑(푒)
(defined by Eq. (4.35)), across all the WG edges labeled with 푣 and such that the
position 푖 lies within the segment corresponding to the alignment defined by the
departing and ending states of each of these edges.
Finally, the RP of 푥 for the query 푣 is computed as in Eq. (3.19):
푃(푅 | 푥,푣) ≈ max
푖
푃(푣 | 푥, 푖) (5.3)
Alg. 5.1 provides an efficient implementation1 of Eqs. (5.2, 5.3). Edge posteriors
휑(푒) are computed in step 9 using the 훼–훽 (forward–backward) functions efficiently
computed (by dynamic programming) in steps 4, 5, according to Eqs. (4.32, 4.33).
Following Eq. (5.2), in step 11 edge posteriors are incrementally accumulated in
a posteriorgram vector 풉. And using these posteriors, Eq. (5.3) is incrementally
1 In this and in all the other algorithms and examples of this section, the Real semiring is assumed.110 5 Probabilistic Indexing for Fast and Effective Information Retrieval
computed in step 12 for each word 푣 labeling the successively processed WG edges.
Finally, the PrIx of the given text-line image region is built in Steps 14, 15.
Algorithm 5.1 Compute a pixel-level 1-D posteriorgram from the WG of a text line
image and use this posteriorgram to obtain the PrIx of the image region.
Require: A compact lattice 퐴 = (Σ, Q, E, 푞0, 휚) obtained from an image region 푥
Require: The state alignment function of 퐴, 푎(· )
1: procedure PosteriorgramWordIndex(퐴, 푎)
2: h ← 0¯ ⊲ Initialize posteriorgram vector to hold 푃(푣 | 푥, 푖) ∀푣, 푖
3: 풖 ← 0¯ ⊲ Initialize maximization of Eq. (5.3) for all 푣 ∈ Σ
4: 휶 ← Forward(퐴) ⊲ Forward vector
5: 휷 ← Backward(퐴) ⊲ Backward vector
6: for all 푒 ∈ E : 푙(푒) ≠ 휖 , do
7: 푣 ← 푙(푒) ⊲ Word label of the edge
8: 푞, 푟 ← 푝(푒), 푛(푒) ⊲ Departing and ending states of the edge
9: 휑 ←
￾
훼푞 ⊗ 휔(푒) ⊗ 훽푟

⊘ 훽푞0
⊲ Edge posterior 휑(푒)
10: for 푖 ← 푎(푞) to 푎(푟 ) −1 do ⊲ Go over the horizontal positions of the edge segment
11: ℎ푣,푖 ← ℎ푣,푖 ⊕ 휑 ⊲ Update 푖-th posteriorgram entry for the word 푣
12: 푢푣 ← max(푢푣, ℎ푣,푖 ) ⊲ Update the highest posteriorgram pick for 푣 (Eq. (5.3))
13: 퐼 ← ∅ ⊲ Initialize PrIx (set of spots) derived from h
14: for all 푣 ∈ Σ do
15: MapInsert(퐼, [푥, 푣, 푢푣 ]) ⊲ Add spot for 푣 and the corresponding RP, 푣푣
return 퐼
Asymptotic Cost of Algorithm 5.1
The cost of lines 4 and 5 in Alg. 5.1 is O (|E)|. Then the cost of the outer for loop on
푒 is also proportional to |E |. The inner for loop on 푖 takes less than 퐿 steps, where 퐿
is the maximum length of a word in the image 푥. Since 퐿 can be considered a small
constant, the overall asymptotic cost of the outer loop is also O (|E)|. Finally the
loop on 푣 takes |Σ| steps and the overall time complexity of Alg. 5.1 is O (|E | + |Σ|).
The space complexity is O (|Q| + 푇 |Σ|) where 푇 is the horizontal size of the image
푥.2 See [15, 13] for more details, including real computing times of WG generation.
5.3 Indexing Lexicon-based Lattices
As in the previous section, here we are interested in building a PrIx that contains
whole-region relevance probabilities, for each word likely written in the image, out
of a given set of words (lexicon). That is, we continue under the lexicon-based
paradigm. However, starting in this section, we explicitly adopt a HTR point of view
and relay of WFST methods more intensively.
2 The memory space that would be taken to store the PrIx spots in the list 퐼 is not taken into
account, since they can be just printed out sequentially and no storage is actually required. This will
be similarly assumed in other algorithms of this section5.3 Indexing Lexicon-based Lattices 111
5.3.1 Position-independent Relevance
We have to compute, for all the words 푣 in a vocabulary Σ, the RP given by Eq. 3.24:
푃(푅 | 푥,푣) =
Õ
푤:푣∈푤
푃(푤 | 푥) (5.4)
Recall that with 푤 : 푣 ∈ 푤 we denote the set (here called “language”) Σ
★푣 Σ
★ of all
the transcripts (here called “sentences”) that contain the word 푣.
In Chapter 4 (Eqs. (4.28–4.31)), it was discussed how the posterior 푃(푤 | 푥) can
be adequately approximated using a WG obtained from the image region 푥. In the
same chapter it was also explained how to obtain a WG using HTR models and
methods and how WGs can advantageously be represented as WFSTs. Now, this
allows the RP of Eq. (5.4) to be computed easily and efficiently using basic WFST
operations.
First, we need to obtain the set of WG paths that contain the label 푣 in any of the
edges. To do this, we can compute the composition between the WG WFST and a
deterministic automaton accepting the language Σ
★푣 Σ
★, which we will refer to as
퐴푣. Fig. 5.1 represents the minimal deterministic automaton for such a language.
Σ − {푣}
푣
Σ
Fig. 5.1: Minimal Deterministic Automaton 퐴푣 accepting all sequences containing the
symbol 푣, that is the language Σ
★푣 Σ
★. When a FST representing the set of hypotheses of
a text image is composed with this automaton, the resulting FST includes all the original
hypotheses (and their costs) that include at least one instance of the symbol 푣.
The result of this composition will be a new WFST containing all paths from the
original WFST that contain at least one edge with the label 푣 in its output.3 Then,
we just need to compute the total sum of weights of all its paths, which is efficiently
done using the Backward algorithm described in Chapter 4, Sec. 4.7.4.
Then, to build a PrIx for the image region represented by a given WFST, we
can repeat this procedure for all the word labels in any of the edges of the WFST.
Alg. 5.2 describes this procedure.
Asymptotic Cost of Algorithm 5.2
Observe that the cost of steps 2 is O (|Q| + |E |), where |Q| is the number of states and
|E | the number of edges in the WFSA. Since the input WFST is trim, (i.e. all states
are accessible), then |E | ≥ |Q| − 1 and the asymptotic cost of this step is O (|E |).
On the other hand, if special labels are used to represent the Σ-edges in Fig. 5.1, and
3 It is crucial that 퐴푣 is deterministic in order to avoid duplicated paths in the resulting WFST.
However, it’s not required that the automaton is minimal.112 5 Probabilistic Indexing for Fast and Effective Information Retrieval
Algorithm 5.2 Compute a PrIx for the word compact lattice of a text image region.
Require: A compact lattice 퐴 = (Σ, Q, E, 푞0, 휚) obtained from an image region 푥.
1: procedure LatticeWordIndex(퐴)
2: 휷 ← Backward(퐴) ⊲ Vector indexed by the states of Q
3: 퐼 ← ∅ ⊲ PrIx: Set of spots, (푥, 푣, 푃(푅 | 푥, 푣) ), derived from 퐴
4: for all 푣 ∈ Σ do ⊲ Assume Σ encompasses only the words in the edges of 퐴
5: Let 퐴푣 be a deterministic automaton for the language Σ
★푣 Σ
★ (see Fig. 5.1).
6: 퐴
′ ← 퐴 ◦ 퐴푣 ⊲ Composition: Paths of 퐴
′
are those of 퐴 which contain 푣
7: Let 푞
′
0
be the initial state of 퐴
′
.
8: 휷
′ ← Backward(퐴
′
)
9: MapInsert(퐼, [푥, 푣, 훽′
푞
′
0
⊘ 훽푞0
]) ⊲ Divide by 훽푞0=푃퐺 (푥) for normalization
return 퐼
a special matching algorithm is used during composition (i.e. 휌-composition4) the
cost of the composition operation in line 6 is also O (|Q| + |E |) (or O (|E |) for short).
This step, along with another Backward computation whose cost is also O (|E |)), are
executed |Σ| times. Thus, the asymptotic time complexity of the entire algorithm
is O (|Σ| |E |). Regarding space costs, Both 휷 and 휷
′
require |Q| memory locations,
but this is dominated by the O (|E |) space required to store 퐴
′
. Thus, the overall
asymptotic space complexity is O (|E |).
A Lower Cost Alternative
A significant complexity of Alg. 5.2 comes from the need to build one automaton
for Σ
★푣 Σ
★ and to perform a complex composition operation, both for each word in
Σ. A simpler and more efficient alternative is to directly compute Eq. (5.4) by first
computing the sum of weights of paths of 퐴 which depart from every state 푞 and
contain 푣. Let 훽
′
(푞, 푣) be these sums. Then, the total weight of paths which contain
the word 푣 is just 훽
′
(푞0, 푣). Finally this total weight has to be normalized, as in
Eq. (4.31), by the total weight of all the paths in 퐴, which is directly given by 훽(푞0),
as in Alg. 5.2.
The first sum can be recursively computed in a similar way as the Backward
computation of Eq. (4.33), as follows:
BackwardWord(퐴, 푣, 푞)
(Compute the sum of weights of paths of 퐴 which depart from 푞 and contain 푣)
훽
′
(푞, 푣) =



0¯ Ê
if 푞 ∈ F
푒∈N (푞)
푙(푒)=푣
휔(푒) ⊗ 훽
￾
푛(푒)

⊕
Ê
푒∈N (푞)
푙(푒)≠푣
휔(푒) ⊗ 훽
′
￾
푛(푒), 푣
otherwise
4 https://www.openfst.org/twiki/bin/view/FST/FstAdvancedUsage#Matche5.3 Indexing Lexicon-based Lattices 113
If the states of 퐴 are sorted in topological order (as usual), dynamic programming
can be used to efficiently compute this recurrences just by storing the incrementally
computed values of 훽
′
(푞, 푣) in an array indexed by the states of 퐴, 푞 ∈ Q.
Now, to compute Eq. (5.4) for each 푣, Alg.5.3 computes 훽
′
(푞0, 푣) and normalizes
this weight by 훽(푞0).
Algorithm 5.3 Compute a PrIx for the word compact lattice of a text image region.
Require: A compact lattice 퐴 = (Σ, Q, E, 푞0, 휚) obtained from an image region 푥.
1: procedure LatticeWordIndex(퐴)
2: 휷 ← Backward(퐴) ⊲ Backward vector indexed by the states of Q
3: 퐼 ← ∅ ⊲ PrIx: Set of spots, (푥, 푣, 푃(푅 | 푥, 푣) ), derived from 퐴
4: for all 푣 ∈ Σ do ⊲ Assume Σ encompasses only the words in the edges of 퐴
5: 휷
′ ← BackwardWord(퐴, 푣) ⊲ Backward vector computed for paths containing 푣
6: MapInsert(퐼, [푥, 푣, 휷
′
푞0
⊘ 휷푞0
]) ⊲ Normalize weight and add spot to PrIx
return 퐼
The computing time of this algorithm is still proportional to the number of words
in Σ (which can be assumed to encompass only the words in the edges of 퐴). The
computing time can be further reduced by using additional memory. Specifically,
the BackwardWord recurrence can be fully computed by dynamic programming
for all (푞, 푣) ∈ Q × Σ by storing the corresponding 훽
′
-vales into an array indexed by
states and words. This leads to Alg. 5.4.
Algorithm 5.4 Compute a PrIx for the word compact lattice of a text image region.
Require: A compact lattice 퐴 = (Σ, Q, E, 푞0, 휚) obtained from an image region 푥.
1: procedure LatticeWordIndex(퐴)
2: 휷 ← Backward(퐴) ⊲ Backward vector indexed by the states of Q
3: 휷
′′ ← BackwardAllWords(퐴) ⊲ Backward matrix indexed by states and words of Q × Σ
4: 퐼 ← ∅ ⊲ PrIx: Set of spots, (푥, 푣, 푃(푅 | 푥, 푣) ), derived from 퐴
5: for all 푣 ∈ Σ do ⊲ Assume Σ encompasses only the words in the edges of 퐴
6: MapInsert(퐼, [푥, 푣, 휷
′′
푞0 ,푣 ⊘ 휷푞0
]) ⊲ Normalize weight and add spot to PrIx
return 퐼
Asymptotic Cost of Algorithm 5.4
The computing time of both BackwardWord and BackwardAllWords is in
O (|E |) and the forloop takes O (|Σ|). So the overall time complexity is in O (|E |+|Σ|).
The space complexity is dominated by the 휷
′′ matrix, which requires O (|Q| |Σ|)
memory space.114 5 Probabilistic Indexing for Fast and Effective Information Retrieval
5.3.2 Lexicon-based Segment Relevance
In Chapter 4, Sec. 3.5.2 we considered the case where it is required to highlight the
geometric location where a word instance is found within a text image region 푥. In
this scenario, we cannot use Alg. 5.2, since we need to generate PrIx spots of the
relevance probabilities given by Eq. 3.31, for all possible words and word instance
alignments.
In order to do this efficiently, we use again a WG obtained from 푥. As discussed
in Sec. 4.7.3, a WFST representing a WG is augmented with an alignment function
푎(·) that assigns a horizontal position coordinate to each state of the WFST. Recall
from Eq. (3.31) that the segment-aware RP is computed as:
푃(푅 | 푥,푣, 풃) =
Õ
푤:
∃ 푗:푤푗=푣
Õ
푎:
(푎푗
,푎푗+1 ) = 풃
푇
푃(푤, 푎 | 푥) (5.5)
where 푎 is a sequence of horizontal coordinates that represent an alignment between
a transcription hypothesis 푤 and the text line image 푥. For a given word 푣 and segment
bounded by the coordinates (푏1, 푏2) ≡ 풃
푇
, we need to sum the joint posteriors of all
the transcripts and alignments such that an instance of 푣 is in within 풃.
Alg. 5.5 computes this RP for all the words appearing in the WG. Since each edge
is associated with a particular word and alignment (푏1 and 푏2 are the coordinates
associated the edge’s departing and ending states, respectively), the algorithm simply
traverses all the edges in the lattice and accumulates the likelihood of all paths through
the edge, and finally normalizes the joint likelihoods into the required posterior
probabilities needed to build the PrIx spots.
Algorithm 5.5 Compute a PrIx for segments provided by a word compact lattice of
a text image region.
Require: A compact lattice 퐴 = (Σ, Q, E, 푞0, 휚) obtained from an image region 푥
Require: The state alignment function of 퐴, 푎(· )
1: procedure LatticeWordIndexSegment(퐴, 푎)
2: 휶 ← Forward(퐴) ⊲ Forward vector
3: 휷 ← Backward(퐴) ⊲ Backward vector
4: 퐼 ← ∅ ⊲ PrIx (set of spots) derived from 퐴
5: for all 푒 ∈ E : 푙(푒) ≠ 휖 , do
6: 푣 ← 푙(푒) ⊲ Word label of the edge
7: 푞, 푟 ← 푝(푒), 푛(푒) ⊲ Departing and ending states of the edge
8: 풃 ← (푎(푞), 푎(푟 ) )푇 ⊲ 푏1, 푏2: horizontal coordinates of states 푟 and 푠
9: 휑 ←
￾
훼푞 ⊗ 휔(푒) ⊗ 훽푟

⊘ 훽푞0
⊲ Edge posterior 휑(푒)
10: MapInsertOrSum(퐼, [푥, 푣, 풃, 휑]) ⊲ Add or update spot with segment-aware RP
return 퐼
The function MapInsertOrSum is used to check whether a spot is already in the
PrIx built so far. If it does not exist, it is inserted; otherwise, the new RP (휑) is just
added to the previous RP (using the sum semiring operator ⊕)5.3 Indexing Lexicon-based Lattices 115
Asymptotic Cost of Algorithm 5.5
As in previous algorithms, the asymptotic cost of lines 2 and 3 is in O (|E |) and
the storing the results takes O (|Q|) memory space. Then each of the |E | steps of
the for loop entails just constant complexity operations (independent of the WG
size). Regarding the space complexity, notice that now the list 퐼 needs to be held
in memory, because PrIx spots are incrementally updated by MapInsertOrSum.
Therefore, if 푀 is the number of PrIx spots indexed, the overall asymptotic time and
space complexities of Alg. 5.5 are O (|E |) and O(푀 + |Q|), respectively.
5.3.3 Lexicon-based Horizontal Position Relevance
In Sec. 3.5.1, a relevance probability conditioned on individual horizontal coordi￾nates was also presented (see Eq. 3.29):
푃(푅 | 푥,푣, 푖) =
Õ
푤:
∃ 푗:푤푗=푣
Õ
푎:
푎푗 ≤푖<푎푗+1
푃(푤, 푎 | 푥) (5.6)
Computing this RP can be done using an algorithm very similar to Alg. 5.5. The
difference is that when traversing an edge, we need an additional loop to increase
the accumulator corresponding to each horizontal position in the edge’s segment.
According to Eq. (3.43) of Chapter 3, the resulting values of 푃(푅 | 푥,푣, 푖) computed
in this way would the same as those of the posteriorgram ℎ푣,푖 computed for the same
line image 푥 in Alg. 5.1 (Sec. 5.2).
5.3.4 Lexicon-based Ordinal Position Relevance
The RP of a transcript ordinal position was presented in Sec. 3.5.3, Eq. (3.34):
푃(푅 | 푥,푣, 푘) =
Õ
푤:푤푘=푣
푃(푤 | 푥) (5.7)
An interesting feature of this approach is that it allows to consider the relevance of
word instances regardless of their multiple alignment hypotheses. In addition, as we
will see in this section, this relevance probabilities enable us to build positional PrIxs
very similar to the ones used in traditional IR systems, which allows a search engine
to easily operate with “phrase queries” (i.e. searching for sequences of words).
Disambiguating WG State Positions
In WGs or lattices represented as WFSA, all the edges entering a given state are
aligned to the same horizontal coordinate of the text line image. That is, all the
words in the edges that enter a particular state end at the same horizontal position.116 5 Probabilistic Indexing for Fast and Effective Information Retrieval
However, in general, the different edges entering a state could be part of paths whose
number of words is different, as in the example shown in Fig. 5.2.
푞0
푞
푞
′
she / 0.3
is / 1.0
she’s / 0.7
(푞0,0)
(푞,1)
(푞
′
,1)
(푞
′
she / 0.3
,2)
is / 1.0
she’s / 0.7
Fig. 5.2: Disambiguation of the word position associated to the states of a lattice. In the first
lattice states are ordered according the their horizontal coordinates given by the alignment
function 푎(· ). In the second lattice, all paths entering a state have the same number of
words. This way, we can associate an ordinal position to each state (and edge) of the WFSA.
State IDs shown within the states correspond to the notation used in Alg. 5.6.
Alg. 5.6 takes a WG and produces an equivalent WFSA such that all input paths
to each state have the same number of words. In short, the algorithm does that
by decoupling the states where there are input paths with different lengths. In this
algorithm (and in others from now on) the final-state weight function 휚(·) is assumed
to be represented as a vector, 흔. That is, we assume 휚푞 ≡ 휚(푞) ∀푞 ∈ Q.
Notice that, because the edges in the WG represent full words, we just need a
counter which is incremented each time a non-epsilon edge is traversed (see lines
12–13 of Alg. 5.6). When we reach a state with a different count than the previously
observed, we add that pair of state and count to the queue of pending target WFSA
states (lines 16–18 of Alg. 5.6).
Finally, observe that each state in the original WFSA will be added, at most,
퐾 times to the queue, where 퐾 is the maximum word count input degree (i.e. the
maximum number of different path lengths arriving to any state). Thus the running
time of the algorithm is O (퐾 |E |).
Building Ordinal Position PrIxs
Alg. 5.7 describes how to obtain an ordinal position PrIx from the WG of a line
image. The first step is to use Alg. 5.6 to disambiguate the length of the paths
arriving to each state. Once this step is complete, the remaining steps are similar to
those of Alg. 5.5, based on using the forward and backward functions.5.3 Indexing Lexicon-based Lattices 117
Algorithm 5.6 Disambiguate word lattice states to ensure that all paths entering a
state have the same word count.
Require: A compact lattice 퐴 = (Σ, Q, E, 푞0, 흔)
1: procedure LatticeWordDisambiguateWordCount(퐴)
2: Q
′ ← { (푞0, 0) } ⊲ States of the target WFSA
3: E
′ ← ∅ ⊲ Edges of the target WFSA
4: QueueInit(푆, (푞0, 0)) ⊲ Stack of pending states of the target WFSA
5: while 푆 ≠ ∅ do
6: (푞, 푘) ← QueuePop(푆) ⊲ 푘 is the word count of any path arriving to state 푞
7: 휚
′
(푞,푘) ← 휚푞 ⊲ Final-state weight for the new state (푞, 푘)
8: for all 푒 ∈ N (푞) do ⊲ For all edges departing from state 푞
9: 푣 ← 푙(푒) ⊲ Word label associated to the edge
10: 푞
′ ← 푛(푒) ⊲ Ending state of the edge
11: 푘
′ ← 푘
12: if 푣 ≠ 휖 then
13: 푘
′ ← 푘 + 1 ⊲ Increase word count
14: if (푞
′
, 푘′
) ∉ Q
′
then
15: Q
′ ← Q
′ ∪ { (푞
′
, 푘′
) } ⊲ Add the new state
16: QueuePush(푆, (푞
′
, 푘
′
))
17: E
′ ← E′ ∪
￾
(푞, 푘), (푞
′
, 푘′
), 푣, 휔(푒)
	
⊲ Add new edge
18: return 퐴
′ = (Σ, Q
′
, E
′
, (푞0, 0), 흔
′
)
Algorithm 5.7 Compute an ordinal position PrIx from a word compact lattice.
Require: A compact lattice 퐴 = (Σ, Q, E, 푞0, 흔) obtained from an image region 푥
1: procedure LatticeWordIndexPosition(퐴)
2: (Σ, Q
′
, E
′
, (푞0, 0), 흔
′
) ≡ 퐴
′ ← LatticeWordDisambiguateWordCount(퐴)
3: 휶 ← Forward(퐴
′
)
4: 휷 ← Backward(퐴
′
)
5: 퐼 ← ∅
6: for all 푒 ∈ E′
: 푙(푒) ≠ 휖 , do
7: 푣 ← 푙(푒) ⊲ Word label associated to the edge
8: (푞, 푘) ← 푝(푒) ⊲ Departing state of the edge
9: (푞
′
, 푘′
) ← 푛(푒) ⊲ Ending state of the edge
10: 휑 ←
￾
훼(푞,푘) ⊗ 휔(푒) ⊗ 훽(푞′
,푘′
)

⊘ 훽푞0
⊲ Edge posterior 휑(푒)
11: MapInsertOrSum(퐼, [푥, 푣, 푘′
, 휑])
12: return 퐼
Asymptotic Cost of Algorithm 5.7
As we discussed earlier, the cost of the first step is O (퐾 |E |). This dominates the cost
of the Backward and Forward algorithms, and traversing all edges, which is O (|E |).
Thus, if 푀 is the number of PrIx spots indexed, the worst case asymptotic time and
space complexities of Alg. 5.7 are O (퐾 |E |) and O (푀 + |Q|), respectivel118 5 Probabilistic Indexing for Fast and Effective Information Retrieval
5.4 The Out-of-vocabulary Problem
In the previous section, we described several algorithms that can be employed to
compute each one of the relevance probabilities introduced in Chapter 3. However,
as we already mentioned, all these algorithms assume that a word lattice representing
the set of transcription hypotheses of a text image region was given. In order to obtain
such lattices, researchers typically use a given (or “closed”) lexicon (and maybe an
푛-gram word language model), as discussed in Secs. 4.6 and 4.7 of Chapter 4.
Nonetheless, this assumption entails an important restriction, which may render
PrIx unpractical in many applications: the need to fix before-hand which words are
allowed to be used in queries to a given collection of documents. The only way to
ensure that all the potentially useful words (including names, dates, numbers, etc.)
are present in the lexicon is to “read” the entire collection before indexing it. Of
course, “reading” it is part of the problem we are trying to solve, thus we face a
chicken and egg situation. In practice, many systems are restricted to work with a
(typically very large) subset of words from the collection, usually extracted from the
GT reference transcripts used to train the statistical models described in Chapter 4.
Notice that when we restrict our statistical models to a given subset of words, by
definition any other possible word has a null prior probability, and thus its RP will be
zero and the resulting PrIx will never allow “spotting” such a word. On the contrary,
an excessively large lexicon may increase the chances of making avoidable mistakes
and, moreover, the system can become excessively slow.
This problem has been widely studied in the fields of Speech Recognition and Text
Recognition [1, 18, 16, 2], and it is a fundamental flaw of lexicon-based systems,
which affects other domains such as Statistical Machine Translation, Spoken Dialog
Systems, Image Captioning, etc. To circumvent the issue, some authors have proposed
using a combination of word and character-based models [17, 11, 4]. In the context
of KWS and PrIx, apart from the lexicon-free approaches that will be discussed
below, we have tried in the past to smooth sparse PrIxs to account for potential
out-of-vocabulary queries [7, 6, 8, 9].
In the following section we propose a different and more straightforward approach,
based on the two following principles:
1. PrIxs should be word-based, since word indexing allows for very fast searches
on large collections of documents (i.e. constant response time with respect to
the number of indexed documents, and independent of any lexicon size).
2. The approach should account for any word potentially written in the collection,
including proper names, foreign-language words, etc.
Hence, the proposed solutions will employ character lattices obtained from
lexicon-free statistical models (i.e. no closed lexicon is assumed), but will ma￾nipulate them to extract pseudo-words (and build a PrIx for them). These algorithms
assume that a word is any sequence of characters in-between some special delimiter
characters such as white-space, punctuation marks, etc.
This could be considered a serious limitation. For example, in early manuscripts
it was very common to write several words, or even complete lines, without lifting5.5 Indexing Lexicon-free Lattices 119
the quill from the paper (thereby resulting in text without any kind of optical word￾separating clues). However, it is important to understand that word delimiters are
not actually expected to appear in the images. In fact, the concatenation regularities
captured in the character-level optical and language models help solving this problem
very adequately. If training transcripts include the required word separators, (regard￾less whether they are actually rendered or not in the associated training images), the
statistical models usually assign significant probability to these word separators in
the right contexts, even if they do not appear at all in the test images.
In most languages, words can be decomposed as a sequence of characters. Gen￾erally speaking, vocabularies are very large; in fact, one may argue that in most real
languages they are not even bounded. But the number of characters in an alphabet
is always dramatically much smaller than the number of words that can be formed
in any language based on this alphabet. Thus, when training statistical models of
handwritten text, one may expect to have several or many instances of all interesting
characters, but one can by no means assume that all possible words were observed in
any training set. In this sense, lexicon-free models provide a simple and appropriate
way to solve the problem of out-of-vocabulary words.
5.5 Indexing Lexicon-free Lattices
Following the discussion in the previous section, note that lexicon-free (i.e. character)
lattices would prevent us from using the algorithms presented in the previous sections.
Clearly, the edges in the WFSA would no longer represent words, but individual
characters instead. Therefore, a first idea is to extract a word lattice (i.e., a “real”
WG) from a given character lattice (i.e., a character-level WG).
Recall that the goal of the algorithms in this section is to produce “pseudo-word”
PrIxs using character lattices obtained from given image regions using lexicon-free
character-level models. Ideally, pseudo-words should be sequences of characters that
are likely enough to form “real” words that appear in the text images considered.
5.5.1 From Character to Word Lattices
We first present an algorithm that converts a character lattice (whose edges represent
individual characters of transcription hypotheses) into a word lattice whose edges
represent full (pseudo-)words. Typically, the delimiter character that we use to sep￾arate words is the white-space symbol, but this is extended to support any subset
of characters that may act as word delimiters. To this end we introduce a function
Λ: Σ → K that assigns a class 휅 ∈K to each character of the alphabet Σ. Essentially,
only two classes are needed to tell whether or not a given character is a delimiter.
Alg. 5.8 takes a WFSA and produces an equivalent WFSA such that each edge in
the target WFSA is a subpath of the original WFSA formed by edges with the same120 5 Probabilistic Indexing for Fast and Effective Information Retrieval
label class. This way, in the target WFSA it will be easy to tell whether an edge is
labeled with a pseudo-word or a (sequence of) word-delimiter(s).
The two automata are equivalent, in the sense that all (and only) the complete
paths in the original WFSA are also complete paths in the target WFSA, with exactly
the same total weight. The equivalence is fundamental in order to preserve the
distribution over transcription hypotheses.
Algorithm 5.8 Expansion of subpaths formed by labels of the same class in a WFSA.
It obtains an equivalent target WFSA such that the edges represent subpaths in the
original WFSA formed by labels of the same class.
Require: A compact lattice 퐴 = (Σ, Q, E, 푞0, 흔)
Require: A function Λ: Σ → K. ⊲ The class of a character label 푐 ∈ Σ is Λ(푐) = 휅 ∈ K
1: procedure LatticeExpandSubpaths(퐴, Λ) ⊲ Class of the epsilon label
2: Σ
′ ← ∅ ⊲ Alphabet of the target WFSA
3: Q
′ ← { (푞0, Λ( 휖 ) } ⊲ States of the target WFSA
4: E
′ ← ∅ ⊲ Edges of the target WFSA
5: Z ← ∅ ⊲ Origin edges of subpaths, to avoid repetitions
6: StackInit(푆, (푞0, Λ( 휖 ), 푞0, Λ( 휖 ), 휖 , 1)) ⊲ Stack of pending subpaths
7: while StackNotEmpty(푆) do
8: (푞, 휅, 푞′
, 휅′
, 푣, 푡) ← StackPop(푆) ⊲ 푣 will become a word by char concatenation
9: 푧 ← 0 ⊲ Flag to check if a subpath ends in state (푞
′
, 휅′
)
10: for all 푒 ∈ N (푞
′
) do ⊲ For all edges departing from state 푞
′
11: 푐 ← 푙(푒) ⊲ Character label of the edge
12: 푞
′′ ← 푛(푒) ⊲ Ending state of the edge
13: 푢 ← 휔(푒) ⊲ Weight (likelihood) of the edge
14: 휅
′′ ← Λ(푐) ⊲ Class of the edge label, given by Λ
15: if 푐 = 휖 then ⊲ But, if the edge label is empty,
16: 휅
′′ ← 휅
′
⊲ the edge class is that of the predecessor
17: if 휅
′ = Λ( 휖 ) ∨ 휅
′ = 휅
′′ then ⊲ Keep expanding subpath
18: StackPush(푆, (푞, 휅, 푞′′, 휅′′, 푣·푐, 푡 ⊗ 푢)) ⊲ String concatenation: 푣·푐
19: else
20: 푧 ← 1 ⊲ Subpath ends in state (푞
′
, 휅′
)
21: if (푞
′
, 휅′
, 푒) ∉ Z then ⊲ New subpath from (푞
′
, 휅′
) through edge 푒
22: Q
′ ← Q′ ∪ { (푞
′
, 휅′
) }
23: Z ← Z ∪ { (푞
′
, 휅′
, 푒) }
24: StackPush(푆, (푞
′
, 휅′
, 푞′′, 휅′′, 푐, 푢))
25: if 푞 ≠ 푞
′ ∧ ( 휚푞′ ≠0¯ ∨ 푧 = 1) then ⊲ Subpath ends in (푞
′
, 휅′
)
26: Σ
′ ← Σ
′ ∪ {푣}
27: Q
′ ← Q′ ∪ { (푞
′
, 휅′
) }
28: E
′ ← E′ ∪
￾
(푞, 휅 ), (푞
′
, 휅′
), 푣, 푡	
29: for all (푞, 휅 ) ∈ Q′ do
30: 휚
′
(푞,휅) ← 휚푞 ⊲ Final-state weighs
31: return 퐴
′ = ( Q′
, E
′
, (푞0, Λ( 휖 ) ), 흔
′
)
Each state in the target WFSA is identified by a state of the original WFSA and
the class of the subpaths that arrive to that state. Hence, the initial state of the target
WFSA is a pair formed by the initial state of the original WFSA and the class of th5.5 Indexing Lexicon-free Lattices 121
휖-label (line 3 of Alg. 5.8), which is the class of the paths that do not contain any
symbol (in the case that the WFSA is not 휖-free). An initial empty subpath is added
to a stack that stores the pending subpaths that are being formed in the WFSA (line
6 of Alg. 5.8).
While there are subpaths in the stack, the top one is extracted and the edges
leaving from the last state in the path are considered. If the considered edge has a
label which is of the same class as the current subpath, it is extended with this edge
and added to the stack (line 18 of Alg. 5.8). On the contrary, if the edge is from a
different class, it means that a new subpath starts from the current state through that
edge. The new subpath is added to the stack, and the pair formed by the target state
and the traversed edge are added to the set Z to avoid expanding subpaths through
the same edge in the future (lines 21 and 23).
At the end of each iteration, if the last state of the current subpath is final or had
an outgoing edge of a different class, it means that the subpath ends in the current
state and, hence, an edge is added to represent the current subpath (lines 26–28).
Finally, once the stack has been emptied, all the states from the target automaton
are traversed and the final weight of each state is set to that of the corresponding
state in the original WFSA.
It is worth emphasizing that the word-level WFSA obtained as a result of Alg. 5.8
can be used with any of the lexicon-based algorithms presented in Sec. 5.3, adopting
as a “lexicon” the set of relevant pseudo-words “discovered”; that is the character
sequences associated with the subpaths of the same (regular) character class.
Illustrative Example of Algorithm 5.8
Fig. 5.3 shows an example of the result of Alg. 5.8 for a small character lattice. Notice
that the third and fifth states in the original automaton (Fig. 5.3 (a)) are duplicated
in the target (Fig. 5.3 (b)) because both have input edges with the two classes of
characters (regular characters and the delimiter).
The two WFSA are equivalent, as they represent the same sequences of characters
with the same sequence weights. In the original WFSA there are 12 accepted se￾quences (12 complete paths). In the target WFSA, the same sequences of characters
are accepted with the same weights. For instance, the sequence “b a b a” in the
original WFSA is also represented in the target with total weight equal to 0.072,
but there is a single edge in the target automaton to represent the whole sequence,
since it is formed exclusively by “regular” symbols. In contrast, the sequence of
characters “b @ b a”, represented by four edges in the original WFSA (also with
0.072 sequence weight), is represented by three edges in the target: the edge “b”
(with weight 0.6), the edge “@” (with weight 0.5), and the edge “b a” (with weight
0.24).
As a byproduct, this algorithm has discovered the following seven pseudo-word
hypotheses: “a, aaa, aaba, b, ba, baa, baba”.122 5 Probabilistic Indexing for Fast and Effective Information Retrieval
a / 0.4
b / 0.6
a / 0.5
@ / 0.5
@ / 0.7
b / 0.24
a / 1.0
a / 0.06
(a)
a / 0.4
b / 0.6 a a / 0.2
b a / 0.3
a a a / 0.012
b a a / 0.018
a a b a / 0.048
b a b a / 0.072
@ / 0.5
@ @ / 0.35
@ / 0.7
a / 0.06
b a / 0.24
(b)
Fig. 5.3: Example of Alg. 5.8. Fig.(a) shows the original character WFSA, where the sym￾bol “@” represents the character that separates words and symbols “a” and “b” represent
regular characters. The algorithm produces the automaton depicted in Fig. (b). Observe
that the 12 complete paths present in the original WFSA are exactly the same complete
paths that the target WFSA accepts, with the same weights. However, the edges in the target
WFSA are formed by subpaths of the instead of individual characters, formed by symbols
of the same class. As a result, seven pseudo-words hypotheses have been discovered: “a,
aaa, aaba, b, ba, baa, baba”.
Asymptotic Cost of Algorithm 5.8
Since Alg. 5.8 expands subpaths in the original WFSA, it is clear that the worst
case could have an exponential cost with the size of the graph. However, in practice,
it behaves very well because no complete paths in the original automaton is fully
expanded, thanks to the delimiter symbols.
A single state in the original WFSA may be “replicated” in the target word-level
WFSA, depending on the class of the edges arriving to that state. For instance, if
퐶푞 distinct classes arrive to the state 푞, there will be 퐶푞 replicas in the word-level
WFSA. Thus, the number of states in the resulting automaton is, at most, 퐶 |Q|,
where 퐶 = max푞 퐶푞.
Now, suppose that the maximum output degree of the original automaton is 퐷,
and the maximum length (in edges) of a subpath is 퐿. Then, since each state in the
target WFSA is the origin of a subpath, there will be, at most, 퐷
퐿
edges leaving each
of the output states. Then, the target WFSA will have at most 퐶 |Q| 퐷
퐿
edges.
Consequently, the worst-case asymptotic cost is essentially O (퐶 |Q| 퐷
퐿
). Nev￾ertheless, in practice we typically consider only two classes: delimiter characters
(white-space, punctuation symbols, etc.) and regular characters (letters, numbers,
etc.). Therefore, the cost can be (approximately) expressed as O (|Q| 퐷
퐿
).5.6 Alternative Approaches for Lexicon-free PrIx 123
5.6 Alternative Approaches for Lexicon-free PrIx
As mentioned above, the WFSA obtained by Alg. 5.8 from a character lattice is a true
word-level WG. This is very convenient, because it can then be straightforwardly
used in any of the lexicon-based PrIx algorithms discussed so far in this chapter.
However, the asymptotic cost of Alg. 5.8 is exponential with the length of the pseudo￾words. So, unless pseudo-word length is somehow limited, in some cases the cost of
this lattice conversion might become prohibitive and, moreover, explicitly expanding
lattice subpaths may result in very big WGs. Thus, in the coming subsections we
will develop alternative algorithmic solutions that avoid this explicit expansion.
5.6.1 Lexicon-free Segment Relevance
Here we are again interested in computing Eq. (5.5), as in Alg. 5.5, but now the set
of words is not given; instead, the RP is to be computed for pseudo-words which
have to be detected. This is approached through four basic steps, two of which are
well known algorithms available in any WFST software package.
First, the character lattice is disentangled so that all edges arriving to a given state
are of the same class, as in Alg. 5.6, with some ingredients from Alg. 5.8.
Next, the lattice is transformed so that the initial state is connected to every other
state which is the origin of a path of a non-delimiter class, and each state which is the
final of such path is connected to the final state. So, the complete paths correspond
now to the different pseudo-words to be indexed.
Then, an equivalent deterministic lattice is obtained, which sums probabilities of
multiple equivalent alignments of the same pseudo-word.
Finally a list of PrIx spots is produced for the 푛-best segments obtained from the
resulting lattice, where 푛 can be tuned to control the “density” of the obtained PrIx;
that is, the amount of pseudo-word hypotheses generated per real word of the image.
These steps are presented in the coming subsections and showcased in Figs. 5.4
and 5.4bis for a small, illustrative example of character compact lattice. We recall
that, by default, we are assuming the Real semiring (see footnote1
).
5.6.1.1 Encode Character Alignment
As we plan to relay as much as possible on standard WFST processing, an initial
preparation is to encode alignments as part of edge labels. Alg. 5.9 takes a character
compact lattice represented by an acyclic WFSA and the function mapping each state
to its aligned coordinate, and outputs an equivalent target WFST which encodes the
alignment information in the output labels of its edges. Fig. 5.4 (b) shows the result
of Alg. 5.9 applied to a small, illustrative compact lattice.124 5 Probabilistic Indexing for Fast and Effective Information Retrieval
Algorithm 5.9 Encode the alignment of each edge of a lattice as its output label.
The original label is considered as the input label and the alignment is encoded as a
tuple in the output label.
Require: A compact lattice 퐴 = (Σ, Q, E, 푞0, 흔)
Require: The state alignment function 푎(· ) of 퐴
1: procedure LatticeEncodeAlignment(퐴, 푎)
2: E
′ ← ∅ ⊲ New set of edges
3: Γ ← ∅ ⊲ Output alphabet
4: for all 푒 ∈ E do
5: 푞, 푞′ ← 푝(푒), 푛(푒) ⊲ Edge departing and ending states
6: 푐 ← 푙(푒) ⊲ Character label of the edge
7: 푢 ← 휔(푒) ⊲ Weight (likelihood) of the edge
8: 풃 ← (푎(푞), 푎(푞
′
) )푇 ⊲ Edge alignment segment
9: 퐸
′ ← 퐸
′ ∪ { (푞, 푞′
, 푐, 풃, 푢) } ⊲ Target edge with 풃 as the output token
10: Γ ← Γ ∪ {풃} ⊲ Add alignment segment to output alphabet
11: return 푇 = (Σ, Γ, Q, E
′
, 푞0, 흔) ⊲ The target WFST
5.6.1.2 Disambiguating the Input Class Associated to States
The first step, described in Alg. 5.10, consists in disambiguating the input class
associated to each state in the original lattice, so that all edges arriving to a particular
state are of the same label class (given by Λ, as in Alg. 5.8).
The original WFST is traversed from the initial state (whose class is the class of
the epsilon label) and add a new state to the target WFST each time we arrive to an
state of the original WFST with a different class (according to the label of the target
alphabet), continuing the expansion from that state. Fig. 5.4 (c) shows the result of
Alg. 5.10 applied to a WFST produced by Alg. 5.9 (LatticeEncodeAlignment).
The asymptotic cost is O (퐶 |E |), where |E | is the number of edges of the original
lattice and 퐶 is the maximum number of different input classes of edges arriving to
a state. The 퐶 factor is due to the fact that each original state is added this number
of times to the queue, and we add copies of all of its edges each time it is extracted
from the queue. But since typically 퐶 < 3, we can simply express the asymptotic
complexity just as O (|E |). Note that the resulting transducer can be larger than the
original one, but only by a small constant; so, if E
′
is the set of edges of the target
transducer, then |E′
| is O (|E |).
5.6.1.3 From Subpaths to Complete Paths
In this step, we transform the lattice so that the initial state is connected to every node
which is the start of a path corresponding to the non-delimiter class, and each state
which is the end of such a path is connected to the final state. More generally, we
transform the lattice so that each subpath of a particular class in the lattice becomes
a complete path in the modified lattice. When we do so, the complete paths in the
lattice no longer represent transcript hypotheses of the full image, but hypotheses
of individual segments (corresponding to a specific class). In addition, the weight5.6 Alternative Approaches for Lexicon-free PrIx 125
Algorithm 5.10 Disambiguate the input symbols of the edges of a WFST produced
by Alg. 5.9. The algorithm obtains a target, equivalent WFST such that the input
labels of all the edges incident to each state are of the same class.
Require: An acyclic WFST 푇 = (Σ, Γ, Q, E, 푞0, 흔)
Require: A function Λ: Σ → K. ⊲ The class of a character label 푐 ∈ Σ is Λ(푐) = 휅 ∈ K
1: procedure LatticeDisambiguateInputClass(푇, Λ)
2: Q
′ ← { (푞0, Λ( 휖 ) } ⊲ States of the target WFST
3: E
′ ← ∅ ⊲ Edges of the target WFST
4: QueueInit(푆, (푞0, Λ( 휖 ))) ⊲ Stack of pending subpaths
5: while QueueNotEmpty(푆) do
6: (푞, 휅 ) ← QueuePop(푆) ⊲ Process target state (푞, 휅 )
7: 휚
′
(푞,휅) ← 휚푞 ⊲ Final weight for target state (푞, 휅 )
8: for all 푒 ∈ N (푞) do ⊲ Edges leaving state 푞
9: 푐 ← 푙푖 (푒) ⊲ Input symbol (character)
10: 풃 ← 푙표 (푒) ⊲ Output “symbol” (alignment segment)
11: 푢 ← 휔(푒) ⊲ Weight (likelihood) of the edge
12: 푞
′ ← 푛(푒) ⊲ Destination state of the edge
13: 휅
′ ← Λ(푐) ⊲ Class of the edge is given by Λ
14: if 푐 = 휖 then
15: 휅
′ ← 휅 ⊲ Class of the edge is that of 푞
16: if (푞
′
, 휅′
) ∉ Q
′
then ⊲ Add target state (푞
′
, 휅
′
)
17: Q
′ ← Q′ ∪ { (푞
′
, 휅′
) }
18: QueuePush(푆, (푞
′
, 휅′
))
19: E
′ ← E′ ∪
￾
(푞, 휅 ), (푞
′
, 휅′
), 푐, 풃, 푢	
⊲ Add target edge
20: return 푇
′ = (Σ, Γ, Q
′
, E
′
, (푞0, Λ( 휖 )), 흔
′
)
(likelihood) of the complete path is equal to the sum of all complete paths which
share that subpath. Alg. 5.11 describes this procedure.
Fig. 5.4 (d)shows the result of Alg. 5.11 applied to a WFST produced by Alg. 5.10.
Observe that all complete paths in the target WFST correspond to some subpath in
the original WFSA (Fig. 5.4 (c)), and the weight of a complete path in the target is
equal to the sum of all paths containing that subpath in the original. Hence, we can
obtain the probability of a segmentation hypothesis as the total weight of a path.
The worst-case asymptotic time complexity of the algorithm is O (|E |). The
Forward and Backward calculations and the while loop share this cost. The space
requirement is O (|Q|), needed for the Forward and Backward procedures and also
for the stack 푆 and the set Z. But this is dominated by the space needed to store the
target WFST edges E
′
. Therefore, the asymptotic space complexity is also O (|E |).
5.6.1.4 From Character to Word Alignments
Recall that Alg. 5.9 encoded character alignment information as output labels of the
target WFST. Now, in order to make the algorithm comparable to Alg. 5.5, we need
to sum the likelihoods of all segments corresponding to the same word alignment,
regardless of the particular alignment of each individual character in this word126 5 Probabilistic Indexing for Fast and Effective Information Retrieval
Algorithm 5.11 Convert same-class subpaths with hypothesis segments in a charac￾ter lattice into complete paths.
Require: An acyclic WFST 푇 = (Σ, Γ, Q, E, 푞0, 흔) produced by Alg. 5.10. States of 푇 are pairs
(state, character-class), where character-class was given by the Λ(· ) function.
1: procedure LatticeConvertSubToCompletePath(푇)
2: (푞
′′, 휅′′) ← 푞0 ⊲ The class of 휖 is stored in 휅
′′. It will be needed later
3: 휶 ← Forward(푇)
4: 휷 ← Backward(푇)
5: E
′ ← ∅ ⊲ Edges of the target WFST
6: Z ← {푞0 }
7: QueueInit(푆, 푞0) ⊲ Initialize stack of pending states
8: while QueueNotEmpty(푆) do
9: (푞, 휅 ) ← QueuePop(푆)
10: 휚
′
(푞,휅) ← 휚(푞,휅) ⊲ Initialize final weight
11: for all 푒 ∈ N￾
(푞, 휅 )

do ⊲ Edges departing from (푞, 휅 )
12: 푐 ← 푙푖 (푒) ⊲ Input label (character)
13: 풃 ← 푙표 (푒) ⊲ Output label (alignment)
14: (푞
′
, 휅′
) ← 푛(푒) ⊲ Destination state
15: 푢 ← 휔(푒) ⊲ Weight (likelihood) of the edge
16: 푒
′ ← 푒 ⊲ Assume same class, then copy edge
17: if 휅 ≠ 휅
′′ ∧ 휅 ≠ 휅
′
then ⊲ States with different classes
18: 푒
′ ←
￾
푞0, (푞
′
, 휅′
), 푐, 풃, 훼(푞,휅) ⊗ 푢

⊲ Direct edge initial → (푞
′
, 휅′
)
19: 휚
′
(푞,휅) ← 휚
′
(푞,휅)
⊕ (푢 ⊗ 훽(푞′
,휅′
) ) ⊲ Update final weight of state (푞, 휅 )
20: E
′ ← E′ ∪ {푒
′
} ⊲ Add edge to the target WFST
21: if (푞
′
, 휅′
) ∉ Z then
22: Z ← Z ∪ { (푞
′
, 휅′
) }
23: QueuePush(푆, (푞
′
, 휅′
))
24: return 푇
′ = (Σ, Γ, Q, E
′
, 푞0, 흔
′
)
Before this sum can be computed, we need to extract the full word alignment,
rather than the individual character-level alignments. To this end, we recall that,
after Alg. 5.11, words start with edges departing from the initial state and end
with edges incident to a final state. Alg. 5.12 relies on this fact to remove the
alignment information from intermediate characters of a word, and keep only the
initial horizontal coordinate of the first character and the final coordinate of the last
character of the word. This way, all paths with the same word alignment will have
the same sequence of input and output labels in the resulting WFST.
Fig. 5.4bis (e) shows the result of Alg. 5.12 applied to a WFST produced by
Alg. 5.11. Notice that the original WFST is transformed so that all complete paths
representing the same word with the same word-level alignment (i.e. the word starts
and ends at the same horizontal coordinates) are represented by the same sequence
of input/output symbols. In particular, the input sequence represent the character
sequence and the output sequence provides the horizontal boundaries where the
word is hypothesized in the (line) image.
The running time and space complexity of the algorithm is in O (|E |), with respect
to the size of its original transducer. The resulting transducer can be larger than t5.6 Alternative Approaches for Lexicon-free PrIx 127
Algorithm 5.12 Keep only word alignments from a WFST yield by Alg. 5.11.
Require: An acyclic WFST 푇 = (Σ, Γ, Q, E, 푞0, 흔) produced by Alg. 5.11.
1: procedure KeepOnlyWordAlignment(푇)
2: Q
′ ← Q
3: E
′ ← ∅
4: for all 푒 ∈ E do
5: 푞, 푞′ ← 푝(푒), 푛(푒) ⊲ Departing and ending states of the edge
6: 푢 ← 휔(푒) ⊲ Weight (likelihood) of the edge
7: 푐 ← 푙푖 (푒) ⊲ Character (input label) of the edge
8: (푏1, 푏2 ) ≡ 풃
푇 ← 푙표 (푒) ⊲ Character alignment segment
9: if 푞 =푞0 ∧ 휚푞′ ≠0 then ⊲ Word with a single character
10: Q
′ ← Q′ ∪ { (푞, 푞′
) } ⊲ New auxiliary state
11: E
′ ← E′ ∪ { (푞, (푞, 푞′
), 푐, 푏1, 푢) }
12: E
′ ← E′ ∪ { ( (푞, 푞′
), 푞′
, 휖 , 푏2, 1) }
13: else if 푞 =푞0 then ⊲ First character of the word
14: E
′ ← E′ ∪ { (푞, 푞′
, 푐, 푏1, 푢) }
15: else if 휚푞′ ≠0 then ⊲ Last character of the word
16: E
′ ← E′ ∪ { (푞, 푞′
, 푐, 푏2, 푢) }
17: else ⊲ Intermediate character
18: E
′ ← E′ ∪ { (푞, 푞′
, 푐, 휖 , 푢) }
19: return 푇
′ = (Σ, N, Q
′
, E
′
, 푞0, 흔)
original one, but only by a small constant; that is, if E
′
is the set of edges of the
target transducer, then |E′
| is O (E).
5.6.1.5 Disambiguating WFST Paths through Automaton Determinization
The WFSTs yield by Alg. 5.12 may be ambiguous; that is, there may be more
than one path corresponding to the same (pseudo)-word and word-level alignment.
Fig. 5.4bis (e) showcases two of these ambiguous paths: “a:0 / 0.6, a:6 / 1.5” and
“a:0 / 0.5, a:6 / 2.0”. Clearly, if these paths have to be used to hypothesize the
(pseudo-)word “aa”, in the segment “(0,6)”, the weights have to be added into
1.9 and a total path weight of 1.9 · 0.8 = 1.52, taking into account the final-state
weight.
Unambiguity can be ensured by means of determinization of theWFST considered
as an automaton; that is, considering that input:output labels are joined into a single
symbols. This is a standard WFST function, generally available in most WFST
toolkits, which here will be referred to it as DeterminizeAsWFSA. Note that, in
order to ensure the sum of likelihoods, it must operate in the Real semiring.
1
Fig. 5.4bis (f) shows the result of applying this function to the WFST produced by
Alg. 5.12. Now, the above pair of ambiguous paths are merged into the single path
“a:0 / 0.6, a:6 / 3.167” whose total likelihood is 0.6 · 3.167 · 0.8 = 1.52, as required.128 5 Probabilistic Indexing for Fast and Effective Information Retrieval
5.6.1.6 푵-best Paths
In the WFST obtained after determinization, each complete path corresponds to the
hypothesis of a different (pseudo)-word in a different image segment and it can be
directly used to obtain one spot of the image PrIx. The only remaining steps are to
concatenate the characters of the path into a single (pseudo)-word and to normalize
the complete path weight by the total weight of the original compact lattice.
However, note that for a WFST obtained through the above steps from a real
character compact lattice of a given (line) image region, the number of possible
paths can grow exponentially with the size of the original lattice. In practice, billions
of paths can often be extracted even from the WFST obtained for a small image with
just a dozen of handwritten words. Therefore, the number of paths to extract has to
be somehow limited so as to achieve an adequate PrIx indexing density.
To this end we use another standard WFST function, here referred to as Best￾Paths, which determine the 푛-best paths (those with highest weight) of a WFST.
5.6.1.7 Indexing Words with Alignment from Character Lattices
Finally, we can put all the pieces together in order to extract a PrIx for (pseudo-)word
segments directly from a character lattice. Alg. 5.13 describes this procedure.
Algorithm 5.13 Compute a word segment PrIx based on a character lattice.
Require: A compact lattice 퐴 = (Σ, Q, E, 푞0, 흔) of an image 푥
Require: A function 푎(· ) which yields a horizontal coordinate associated to each state
Require: A function Λ(· ) which assigns a class to each character label
Require: The maximum number of words to index, 푛
1: procedure LatticeCharacterIndexSegment(퐴, 푎, Λ, 푛)
2: 휷 ← Backward(퐴)
3: 푇 ← LatticeEncodeAlignment(퐴, 푎)
4: 푇 ← LatticeDisambiguateInputClass(푇, Λ)
5: 푇 ← LatticeConvertSubToCompletePath(푇)
6: 푇 ← KeepOnlyWordAlignment(푇)
7: 푇 ← DeterminizeAsWFSA(푇)
8: 퐼 ← ∅
9: for all (푐, 푎, 푢) ∈ BestPaths(푇, 푛) do ⊲ 푢 is the total weight of each path
10: ⊲ 푐 ≡ 푐1, 푐2, . . . , 푐푚 is the sequence of path input symbols (pseudo-word characters)
11: ⊲ 푎 ≡ 푎1, 푎2, . . . , 푎푚 is the sequence of path output symbols (alignment boundaries)
12: 푣 ← 푐1 · 푐2 · . . . · 푐푚 ⊲ Concatenate characters to define a pseudo-word
13: 풃 ← (푎1, 푎푚)
푇 ⊲ Word image segment associated with 푣
14: MapInsert(퐼, [푥, 푣, 풃, 푢 ⊘ 훽푞0
]) ⊲ Normalize weight and generate PrIx spot
15: return 퐼5.6 Alternative Approaches for Lexicon-free PrIx 129
Fig. 5.4 illustrates all the steps of Alg. 5.13, applied to a simple character compact
lattice example. The 푛 = 9 best final PrIx spots produced in steps 8–14 are shown
in Fig. 5.4bis (g). No more than these 9 spots exist in this small example and they
constitute the word–segment PrIx discussed in Chapter 3, Sec. 3.5.1 – see Eq. (5.5)
in this chapter. It is worth noting that, for each horizontal coordinate 푖 ∈ [0, 15] the
sum of RPs for all the spots whose segments contain 푖 is exactly 1.0. This ensures
the “posterior-gram-like” consistency of the horizontal coordinate RP, 푃(푅 | 푥, 푣, 푖),
derived from these word–segment RPs as discussed in Sec. 3.5.1 – see Eq. (5.6) in
this chapter. These spots include two “pseudo-word” hypotheses which correspond
just to word-delimiters. These spots are shown here only to illustrate the consistency
of the computed RPs but, for most real applications, they are discarded in the final
image PrIx.
As discussed previously, if E is the set of edges of the original character lattice,
the sizes of all the transducers obtained through steps 1-6 are in O (|E |).
The Backward algorithm (line 2) has an asymptotic cost of O (|E |), which is the
same as Alg. 5.9 (line 3), Alg. 5.10 (line 4), Alg. 5.11 (line 5), and Alg. 5.12 (line 6),
and also the trivial RemoveDeimiterPaths (line 8).
The DeterminizeAsWFSA step (line 7) has worst-case asymptotic cost expo￾nential with |E |. This cost can be drastically reduced for WFSTs with special prop￾erties [3]. In our case, the WFSTs to be minimized are acyclic and, since their
paths correspond to (pseudo-)word character sequences, they are typically rather
short (see illustration in Fig. 5.4bis (e)). Probably due to such a simple structure,
in practice determinization is generally observed to rather run in O (|E |) time and
space.
Finally, for the BestPaths function, we use the OpenFST implementation, with
a time complexity of O (|Q| log|Q| + 푛|E |).
Warping it all up, the (observed) asymptotic cost of Alg. 5.13 can be expressed
as O (|E | + |Q| log|Q|).
In practice it proves sufficiently efficient to handle large real datasets. In the
experiments reported in Chapter 6 it can be seen that Alg. 5.13 takes just a few
seconds to generate word–segment PrIxs for whole data sets of hundreds of page
images. The algorithm has been also successfully used in real large-scale production
projects,5 including the Finnish Court Record collection (FCR), with more than one
million page images, whose PrIxs where produced in just a few weeks of processing
using a small CPU/GPU computing cluster.
5 See http://www.prhlt.upv.es/htr/PrIxDemos for a list of PrIx demonstrators.130 5 Probabilistic Indexing for Fast and Effective Information Retrieval
0 3
4
6 9
10
12 15
a / 0.6
a / 0.5
@ / 0.4
a / 1.5
b / 0.5
a / 2.0
b / 0.5 a / 2.0
@ / 0.8 a / 0.5 b / 0.9
c / 1.1
(a) Original character compact lattice.
The numbers inside the states are the horizontal coordinates given by the alignment function 푎(· )
a:(0,3) / 0.6
a:(0,4) / 0.5
@:(0,4) / 0.4
a:(3,6) / 1.5
b:(3,6) / 0.5 a:(4,6) / 2.0
b:(4,10) / 0.5 a:(10,15) / 2.0
@:(6,9) / 0.8 a:(9,12) / 0.5
b:(12,15) / 0.9
c:(12,15) / 1.1
(b) After LatticeEncodeAlignment (Alg. 5.9)
1.0 / 3.3 0.6 / 1.6
0.5 / 2.6
0.4 / 2.6
3.0 / 0.8 2.4 / 1.0
0.45 / 2.0
1.2 / 2.0 3.3 / 1.0
a:(0,3) / 0.6
a:(0,4) / 0.5
@:(0,4) / 0.4
a:(3,6) / 1.5
b:(3,6) / 0.5 a:(4,6) / 2.0
b:(4,10) / 0.5
a:(10,15) / 2.0
a:(4,6) / 2.0
b:(4,10) / 0.5
@:(6,9) / 0.8 a:(9,12) / 0.5 b:(12,15) / 0.9
c:(12,15) / 1.1
(c) After LatticeDisambiguateInputClass (Alg. 5.10). In red: 훼/훽 values for each state, needed in Alg. 5.11 (d)
2.6
0.8
a:(0,3) / 0.6
a:(0,4) / 0.5
b:(4,10) / 0.2
a:(3,6) / 1.5
b:(3,6) / 0.5
a:(4,6) / 2.0
b:(4,10) / 0.5 a:(10,15) / 2.0
b:(12,15) / 0.9
c:(12,15) / 1.1
a:(4,6) / 0.8
@:(0,4) / 0.4
@:(6,9) / 2.4
a:(9,12) / 1.2
(d) After LatticeConvertSubToCompletePath (Alg. 5.11)
Fig. 5.4: Example of successive results of Alg. 5.9, Alg. 5.10 and Alg. 5.11, applied to a
small character compact lattice.5.6 Alternative Approaches for Lexicon-free PrIx 131
2.6
0.8
a:(0,3) / 0.6
a:(0,4) / 0.5
b:(4,10) / 0.2
a:(3,6) / 1.5
b:(3,6) / 0.5
a:(4,6) / 2.0
b:(4,10) / 0.5 a:(10,15) / 2.0
b:(12,15) / 0.9
c:(12,15) / 1.1
a:(4,6) / 0.8
@:(0,4) / 0.4
@:(6,9) / 2.4
a:(9,12) / 1.2
(d) After LatticeConvertSubToCompletePath (Alg. 5.11)
2.6
0.8
a:0 / 0.6
a:0 / 0.5
b:4 / 0.2
a:6 / 1.5
b:6 / 0.5
a:6 / 2.0
b:휖 / 0.5
a:15 / 2.0
b:15 / 0.9
c:15 / 1.1
a:4 / 0.8
휖 :6 / 1.0
@:0 / 0.4
휖 :4 / 1.0
@:6 / 2.4
휖 :9 / 1.0
a:9 / 1.2
(e) After KeepOnlyWordAlignment (Alg. 5.12)
2.6
0.8
a:0 / 0.6
b:4 / 0.2
a:6 / 3.167
b:6 / 0.5
b:휖 / 0.417
a:15 / 2.0
b:15 / 0.9
c:15 / 1.1
a:4 / 0.8
휖 :6 / 1.0
@:0 / 0.4
휖 :4 / 1.0
@:6 / 2.4
휖 :9 / 1.0
a:9 / 1.2
(f) After DeterminizeAsWFSA
(g) List of P rIx spots derived in Alg. 5.13 from the 푛 = 9 best paths of (f)
Image 푣 푏1 푏2 RP
푥 @ 6 9 0.727
푥 aa 0 6 0.461
푥 ac 9 15 0.400
푥 ab 9 15 0.327
푥 @ 0 4 0.315
푥 a 4 6 0.194
푥 aba 0 15 0.152
푥 ba 4 15 0.121
푥 ab 0 6 0.073
Fig. 5.4bis: Example of successive results of Alg. 5.12, DeterminizeAsWFSA and
Alg. 5.13, applied to a small character compact lattice ((a), in the previous page). For
convenience, the first WFST (d) is the same as the last one of the previous page.132 5 Probabilistic Indexing for Fast and Effective Information Retrieval
5.6.2 Lexicon-free Ordinal Position Relevance
Here we develop a lexicon-free version of Alg. 5.7, discussed in Sec. 5.3.4.
First, we need a procedure to determine where the (pseudo-)words are within
a character-level transcription hypothesis. For this purpose, we relay again on the
notion of word delimiter characters discussed in the previous section. In addition, we
need to sum the likelihoods of all paths corresponding to the same word alignment.
5.6.2.1 Associating Ordinal Word Positions to States
First, as in Sec. 5.3.4, we have to transform the character lattice so we can associate
a unique ordinal word position to each lattice state. However, the problem is now
more difficult, since the edges represent individual characters instead of words, and
we cannot simply count the number of edges in the paths arriving to a given state.
In contrast, as we explained earlier, words are defined by a sequence of characters
between two delimiter characters. Thus, we need to keep track of the number of
times that we switch from a delimiter to a non-delimiter symbol in the paths that
enter a given state, and we need to decouple the states of the original WFSA so that
we produce an equivalent target WFSA with a constant number of transitions from
non-delimiter to delimiter characters in all entering paths.
Alg. 5.14 performs this operation, assuming that the original WFSA is produced
by Alg. 5.10. In addition, this algorithm takes a function Δ : K → {0, 1}, which
maps the class of a label/state to a binary value that denotes whether or not the
transition to the given class should increase the word count. For instance, take a path
with symbols “m y @ c a t @ i s @ b l a c k”, where the symbol “@” denotes the
delimiter character. When we move from any character to “@”, we must increase
the word count, but when we move from “@” to any other character we must not.
Thus, given the previous path the word count at each ordinal position of the sequence
would be “1 1 1 2 2 2 2 3 3 3 4 4 4 4 4 4”.
The asymptotic cost of this algorithm is O (퐾 |E |), where 퐾 is the maximum
word-count input degree and |E | is the number of edges of the original WFST.
Notice that, because the original WFST has to be pre-processed with Alg. 5.10, the
total asymptotic cost of both together is O (퐶 퐾 |E |), where 퐶 is the maximum class
input degree. The maximum count and class input degrees are the maximum number
of distinct classes and word counts entering a state.
5.6.2.2 Encode Word Counts
Similarly to what we did in Alg. 5.9, where we encoded the alignment of the charac￾ters as labels of the WFST, here we will encode the word count of each character (i.e.
the ordinal position of the word that the given character belongs to). The cost of this
algorithm is linear with the size of the given transducer. Se the details in Alg. 5.15,5.6 Alternative Approaches for Lexicon-free PrIx 133
Algorithm 5.14 Disambiguate states of a character lattice produced by Alg. 5.10 to
ensure that all paths entering any state have the same word count.
Require: An acyclic WFST 푇 = (Σ, Γ, Q, E, 푞0, 흔), produced by Alg. 5.10, where all the incident
edges to any state are of the same class. States of 푇 are pairs (푟, Λ(푟 ) ), where Λ(푟 ) ∈ K is
the char-class of 푟.
Require: A binary function Δ : K → {0, 1} which determines whether a character class is
considered a separator or not.
1: procedure LatticeCharDisambiguateWordCount(푇, Δ)
2: (푞
′′, 휅′′) ← 푞0 ⊲ Initial state 푞0 is a pair (푞
′′ =푟0, 휅′′ =Λ(푟0 ) )
3: Q
′ ← { (푞
′′, 휅′′
, 0) }
4: E
′ ← ∅ ⊲ Output edges
5: QueueInit(푆, (푞
′′
, 휅′′
, 0)) ⊲ Pending states stack; 0 encodes a null word count
6: while QueueNotEmpty(푆) do
7: (푞, 휅, 푘) ← QueuePop(푆) ⊲ (푞, 휅 ) ∈ Q and 푘 is a word count
8: for all 푒 ∈ N
￾
(푞, 휅 )

do ⊲ Edges leaving state (푞, 휅 )
9: (푞
′
, 휅′
) ← 푛(푒) ⊲ Destination state
10: 푐, 풃 ← 푙푖 (푒), 푙표 (푒) ⊲ Input and output labels
11: 푢 ← 휔(푒) ⊲ Weight of the edge
12: if 휅 ≠ 휅
′ ∧ Δ(휅
′
) =1 then
13: 푘
′ ← 푘 + 1 ⊲ Increase word count
14: else
15: 푘
′ ← 푘 ⊲ Keep word count
16: E
′ ← E′ ∪
￾
(푞, 휅, 푘), (푞
′
, 휅′
, 푘′
), 푐, 풃, 푢	
⊲ Add edge
17: if (푞
′
, 휅′
, 푘
′
) ∉ Q
′
then
18: Q
′ ← Q′ ∪ { (푞
′
, 휅′
, 푘′
) } ⊲ Add state
19: QueuePush(푆, (푞
′
, 휅
′
, 푘′
))
20: for all (푞, 휅, 푘) ∈ Q′ do
21: 휚
′
(푞,휅,푘) ← 휚(푞,휅) ⊲ Set final weight
22: return 푇
′ = (Σ, Γ, Q
′
, E
′
, (푞
′′
, 휅′′
, 0), 흔
′
)
Algorithm 5.15 Encode the word count of each state in a lattice, produced by
Alg. 5.14, as output labels of a WFST. Input labels of the lattice are preserved.
Require: An acyclic WFST 푇 = (Σ, Γ, Q, E, 푞0, 흔) produced by Alg. 5.14, where all paths
entering a state have the same character class and word count. The states of 푇 are triplets
(푟, Λ(푟 ), 푘), where Λ(푟 ) ∈ K is the char-class of 푟 and 푘 is a word count.
1: procedure LatticeEncodeWordCount(푇)
2: E
′ ← ∅
3: for all 푒 ∈ E do
4: (푞, 휅, 푘), (푞
′
, 휅′
, 푘′
) ← 푝(푒), 푛(푒) ⊲ Departing and ending states
5: 푐 ← 푙푖 (푒) ⊲ Label (character)
6: 푢 ← 휔(푒) ⊲ Weight (likelihood)
7: E
′ ← E′ ∪
￾
(푞, 휅, 푘), (푞
′
, 휅′
, 푘′
), 푐, 푘′
, 푢	
⊲ Output edge
8: return 푇
′ = (Σ, N, Q, E
′
, 푞0,134 5 Probabilistic Indexing for Fast and Effective Information Retrieval
5.6.2.3 Indexing Words with Positions from Character Lattices
Finally, all the pieces can be put together to build the ordinal position PrIx. Alg. 5.16
describes such procedure, which shares many steps in common with Alg. 5.13.
Algorithm 5.16 Compute a word ordinal position PrIx based on a character lattice.
Require: A compact lattice 퐴 = (Σ, Q, E, 푞0, 흔) of an image 푥
Require: A function Λ(· ) which assigns a class to each character label
Require: A function Δ which determines whether a character class is or is not a word separator
Require: The maximum number of words to index, 푛
1: procedure LatticeCharacterIndexPosition(퐴, Λ, Δ, 푛)
2: 휷 ← Backward(퐴)
3: 푇 ← LatticeDisambiguateInputClass(퐴, Λ)
4: 푇 ← LatticeDisambiguateWordCount(푇, Δ)
5: 푇 ← LatticeEncodeWordCount(푇)
6: 푇 ← LatticeConvertSubToCompletePath(푇)
7: 푇 ← DeterminizeAsWFSA(푇)
8: 퐼 ← ∅
9: for all (푐, 푘, 푢) ∈ BestPaths(푇, 푛) do ⊲ 푢 is the total weight of each path
10: ⊲ 푐 ≡ 푐1, 푐2, . . . , 푐푚 is the sequence of path input symbols (pseudo-word characters)
11: ⊲ 푘 ≡ 푘1, 푘2, . . . , 푘푚 is the sequence of path output symbols (ordinal positions)
12: 푣 ← 푐1 · 푐2 · . . . · 푐푚 ⊲ Concatenate characters to define a pseudo-word
13: MapInsert(퐼, [푥, 푣, 푘1, 푢 ⊘ 훽푞0
]) ⊲ Normalize weight and generate PrIx spot
14: return 퐼
First, the lattice states are disambiguated so that all labels entering a given state
are of the same class. Then, the states are also disentangled so that all paths entering
any state have the same word count (i.e. number of groups of sequences of char￾acters between delimiters). Afterwards, the word count (i.e. ordinal word position)
is encoded as part of the labels of a WFST. Later, the subpaths corresponding to
(pseudo-)words are isolated, so that a complete path in the resulting WFST repre￾sents a (pseudo-)word, in a specific ordinal position. Then, the resulting WFST is
determinized, so that the likelihoods of the equivalent pairs of word–position are
added. And finally, from the 푛 best paths, the 푛-best pairs (word, ordinal-position)
are indexed.
The total asymptotic cost is essentially the same as that of Alg. 5.13, that is
O(|E | + |Q| log|Q|). As in the case of Alg. 5.13, the cases where the exponential
behavior might appear are extremely rare. Our implementation for experimental
work (and for real production too), also extracts the best image alignment for each
pair (word, ordinal-position), thus the cost is usually about twice as much as that
of Alg. 5.13 alone. Nevertheless, as the results of the experiments in Sec. 6.3 show,
we are able to generate ordinal position PrIxs, including segment alignments, for
instance, in less than 12 seconds for a whole data set of 229 pages.5.7 Multi-word and Regular-Expression Queries 135
5.7 Multi-word and Regular-Expression Queries
PrIx was developed in Chap. 3 under the framework of Information Retrieval (IR)
and in Sec. 3.6, we explained how PrIx seamlessly copes not only with the QbS,
but also with the QbE scenario of KWS. However, so far we always adopted the
traditional KWS assumption here queries are individual keywords.
Nevertheless, the same probabilistic framework is also apt to tackle more complex
types of queries. Once the probability distribution of transcripts of a text image is
represented as a WFST (or, more specifically, as a WG), it is possible to use simple
WFST operations to properly compute RPs for any type of query, provided the query
itself can be represented also as a FST (or WFST).
FSTs are generalizations of Finite State Automata (FSA), and any Regular Ex￾pression can be represented by a FSA. Therefore, the proposed method should be
able to deal with any query that can be expressed in terms of a regular expression.
The example of Fig. 5.5 illustrates the great flexibility entailed by this approach. It
considers the following complex multi-word query:
푞 = (great ∨ neat) ∧ ¬(“not great” ∨ “not neat”)
which makes use of the Boolean operators ∧, ∨, ¬ (AND, OR, NOT) and literal
word strings (between quotes). This query can straightforwardly be represented by
a deterministic FSA such as the one shown in Fig. 5.5 (b). Now consider the WG of
Fig. 5.5 (a) which is assumed to have been obtained from some text image 푥.
this is / 0.8
is / 0.2
great / 0.7
neat / 0.2
bad / 0.1 not
(a) 푇1: WG of transcription hypotheses of an image 푥
great
neat
not
not
great
neat
great
neat
Σ
′
Σ
′
not
Σ
′
Σ
′
not
Σ
(b) 푇2: FSA of the regular-expression query 푞
this is / 0.8
great / 0.7
neat / 0.2
(c) 푇1 ◦ 푇2: Possible image transcripts that are relevant to the query
Fig. 5.5: Example of computation of RP for a given regular expression. Likely transcripts
of a text line image 푥 are represented as a WG in panel (a). Panel (b) shows a deterministic
FSA representing the regular-expression query, 푞 = (“great” ∨ “neat”) ∧ ¬(“not great” ∨
“not neat”). Σ
′
-transitions denote all labels in Σ not present in the other state’s transitions.
Panel (c) depicts the composition of the two previousWFSTs. The RP is the total probability
mass in the composite WFST; that is, 푃(푅 | 푥, 푞) = 0.72.136 5 Probabilistic Indexing for Fast and Effective Information Retrieval
Using Eq.(3.24) of Sec. 3.4, the position-independent RP of 푥 for 푞 is computed
as the sum of the posterior probabilities of all the word sequences of the WG of 푥
(푇1) that belong to the language of the FSA of 푞 (푇2). This sum we can efficiently
computed using essentially the same method described in Alg. 5.2 of Sec. 5.3.1. In a
nutshell, we first obtain the composition of the WG and the query FSA, 푇1 ◦ 푇2
def
= 푇3,
and then compute the total weight of all the paths in 푇3. That is, 푃(푅 | 푥, 푞) = 0.72.
It is worth pointing out, however, that this approach would require performing
a very computationally demanding WFST composition operation for each query
and each image region (typically a text line) of the collection considered. What
is worse, such an exorbitant computing effort would be needed at query time! In
addition, rather that simple PrIx spots, a system based on this idea would have to
store complex (and large) WGs. For example, to provide access to a large collection
such as FCR (with more than one million pages and 30 million text line regions –
see Chapter 10), such an approach would demand more than 300Gb storage. And
each user query would require 30 million WFST compositions – or around 30 days
of intensive (single-core) computing on a fast computer!
The scalable approach we propose to efficiently handle multi-word queries in
large text image collections will be presented and evaluated in Chapter 9. It is based
on building PrIxs of individual (pseudo-)words, as explained in previous sections,
and adequately structuring the resulting spots for fast retrieval. Then, at query time,
approximate RPs of Boolean word combinations are computed by using the simple
Frechet bounds ´ already seen in Sec. 3.7.4 of Chapter 3, Eqs. (3.51, 3.52). These
bounds only require light computing, which enables very fast system answers.
To gain some insight about the validity of this approach, Table 5.1 illustrates
how the Frechet approximation might deviate from the “exact” (but computationally ´
prohibitive) RP computed as discussed above. Positional RPs are computed for
the individual words and the various Boolean word combinations required for of the
example of Fig. 5.5. The Frechet approximations (and bounds) of these combinations ´
are also computed.
Table 5.1: Computing the RP for a multi-word query. The ultimate goal is to compute an
approximation to the RP for the query 푞 of the example in Fig. 5.5 (b). To this end, we use
positional RPs of the individual words (Table 5.1 (a)), computed from the WG of the image
푥, and then relay on the Frechet bounds to compute Boolean combination approximations. ´
(a) Positional P rIx
Term Pos. Prob.
“this” 1 1.00
“is” 2 1.00
“not” 3 0.20
“great” 3 0.56
“neat” 3 0.16
“bad” 3 0.08
“great” 4 0.14
“neat” 4 0.04
“bad” 4 0.02
(b) Exact and approximate RP of various multi-word queries
Relevance probability
Query Exact Approx. Bounds
푞1 = “great” ∨ “neat” 0.90 0.56 [0.56, 0.9]
푞2 = “not great” 0.14 0.14 [0, 0.14]
푞3 = “not neat” 0.04 0.04 [0, 0.04]
푞4 = 푞2 ∨ 푞3 0.18 0.14 [0.14, 0.18]
푞5 = ¬ 푞4 0.82 0.86 [0.82, 0.86]
푞 = 푞1 ∧ 푞5 0.72 0.56 [0.72, 0.82]5.7 Multi-word and Regular-Expression Queries 137
In Sec. 3.5.3 we explained how positional RPs are computed from a WG. For
instance, the RP for the word “this” at ordinal position 1 is equal to 1.0, because
all paths in the WG contain this word in that position. On the contrary, the RP for
“not” at position 3 is 0.2, since only one path (whose posterior probability is 0.2)
includes that word in that position. These “exact” RPs are shown in Table 5.1 (a) for
the individual words involved in the example.
Exact RPs and the corresponding approximate RPs based on the Frechet bounds ´
are shown in Table 5.1 (b) for the various Boolean word combinations6 entailed by
the query 푞. The RP of an OR query is approximated by the lower bound and that
of an AND by the upper bound. So for example, for “great” ∨ “neat” the bounds are
[max(0.56, 0.16, 0.14, 0.04), min(1, 0.56 + 0.16 + 0.14 + 0.04)] = [0.56, 0.9] and
the RP is approximated as 0.56.
Similarly for the AND (and word sequence) queries. Take for instance the query
“not great”. Its exact relevance probability is 0.14, since this is the posterior
probability of the unique path in the WG containing this sequence of words.
There is one entry for the word “not” in the index, at position 3, with proba￾bility 0.2, thus we need to take into account the entry of the word “great” at
position 4, with probability 0.14. Thus, the RP bounds for this word sequence
are [max{0, 0.2 + 0.14 − 1}, min{0.2, 0.14}] = [0, 0.14] and the approximate RP is
0.14. We proceed likewise for the final 푞, which consists in an AND combination of
“great” ∨ “neat” and¬(“not great” ∨ “not neat”). In this case, the bounds computed ac￾cording to exact RPs are [max{0, 0.90 + 0.82 − 1}, min{0.90, 0.82}] = [0.72, 0.82].
Using these bounds, the approximate RP would be 0.82, but in practice we can￾not relay on any exact RP and the previous approximations for the two terms
involved have to be used instead. That is, the “approximate bounds” would be
[max{0, 0.56 + 0.86 − 1}, min{0.56, 0.86}] = [0.42, 0.56] and the resulting approx￾imate RP is 0.56.
Before closing this section it is worth to insist that the example of Fig. 5.5 and
the corresponding RPs reported in Table 5.1 (b) were conceived just for educative
purposes. We will return to this topic in Chapter 8, where real multi-word queries
are considered using previously generated single-word PrIxs and formal empirical
results are presented to assess the practical validity of our approach.
6 Word sequences such as “not neat” and “not great” are handled using the positional information
associated with the position-dependent RP computation.138 5 Probabilistic Indexing for Fast and Effective Information Retrieval
References
1. Asadi, A., Schwartz, R., Makhoul, J.: Automatic modeling for adding new words to a large￾vocabulary continuous speech recognition system. In: International Conference on Acoustics,
Speech, and Signal Processing, vol. 1, pp. 305–308 (1991)
2. Bazzi, I.: Modelling out-of-vocabulary words for robust speech recognition. Ph.D. thesis,
Massachusetts Institute of Technology (2002)
3. Buchsbaum, A.L., Giancarlo, R., Westbrook, J.R.: On the determinization of weighted finite
automata. SIAM Journal on Computing 30(5), 1502–1531 (2000)
4. Kozielski, M., Rybach, D., Hahn, S., Schl¨uter, R., Ney, H.: Open vocabulary handwriting
recognition using combined word- level and character-level language models. In: International
Conference on Acoustics, Speech, and Signal Processing, pp. 8257–8261 (2013). DOI 10.110
9/ICASSP.2013.6639275
5. Ortmanns, S., Ney, H., Aubert, X.: A word graph algorithm for large vocabulary continuous
speech recognition. Computer Speech & Language 11(1), 43–72 (1997)
6. Puigcerver, J., Toselli, A.H., Vidal, E.: Word-Graph and Character-Lattice Combination for
KWS in Handwritten Documents. In: 2014 14th International Conference on Frontiers in
Handwriting Recognition, pp. 181–186 (2014). DOI 10.1109/ICFHR.2014.38
7. Puigcerver, J., Toselli, A.H., Vidal, E.: Word-Graph-Based Handwriting Keyword Spotting of
Out-of-Vocabulary Queries. In: 2014 22nd International Conference on Pattern Recognition,
pp. 2035–2040 (2014). DOI 10.1109/ICPR.2014.355
8. Puigcerver, J., Toselli, A.H., Vidal, E.: A New Smoothing Method for Lexicon-Based Hand￾written Text Keyword Spotting. In: R. Paredes, J.S. Cardoso, X.M. Pardo (eds.) Pattern
Recognition and Image Analysis, pp. 23–30. Springer International Publishing, Cham (2015)
9. Puigcerver, J., Toselli, A.H., Vidal, E.: Querying out-of-vocabulary words in lexicon-based
keyword spotting. Neural Computing and Applications 28(9), 2373–2382 (2017). DOI
10.1007/s00521-016-2197-8
10. Romero, V., Toselli, A.H., Vidal, E.: Multimodal Interactive Handwritten Text Transcription.
Perception and Artif. Intell. (MPAI). World Scientific, (2012)
11. Szoke, I., Burget, L., Cernocky, J., Fapso, M.: Sub-word modeling of out of vocabulary words in
spoken term detection. In: 2008 IEEE Spoken Language Technology Workshop, pp. 273–276
(2008). DOI 10.1109/SLT.2008.4777893
12. Toselli, A.H., Puigcerver, J., Vidal, E.: Two Methods to Improve Confidence Scores for Lexicon￾Free Word Spotting in Handwritten Text. In: 2016 15th International Conference on Frontiers
in Handwriting Recognition (ICFHR), pp. 349–354 (2016)
13. Toselli, A.H., Romero, V., Vidal, E.: Word graphs size impact on the performance of hand￾writing document applications. Neural Computing and Applications 28(9), 2477–2487 (2017)
14. Toselli, A.H., Vidal, E.: Fast HMM-Filler Approach for Key Word Spotting in Handwritten
Documents. In: 2013 12th International Conference on Document Analysis and Recognition,
pp. 501–505 (2013). DOI 10.1109/ICDAR.2013.106
15. Toselli, A.H., Vidal, E., Romero, V., Frinken, V.: HMM Word-Graph Based Keyword Spotting
in Handwritten Document Images. Information Sciences 370(C), 497–518 (2016). DOI
10.1016/j.ins.2016.07.063
16. Woodland, P.C., Johnson, S.E., Jourlin, P., Jones, K.S.: Effects of out of Vocabulary Words in
Spoken Document Retrieval. In: Proceedings of the 23rd Annual International ACM SIGIR
Conference on Research and Development in Information Retrieval, SIGIR ’00, pp. 372–374.
ACM, New York, NY, USA (2000). URL http://doi.acm.org/10.1145/345508.345661
17. Yazgan, A., Saraclar, M.: Hybrid language models for out of vocabulary word detection in
large vocabulary conversational speech recognition. In: 2004 IEEE International Conference
on Acoustics, Speech, and Signal Processing (ICASSP), vol. 01, pp. 745–748 (2004). DOI
10.1109/ICASSP.2004.1326093
18. Young, S.R.: Detecting misrecognitions and out-of-vocabulary words. In: Acoustics, Speech,
and Signal Processing, 1994. ICASSP-94., 1994 IEEE International Conference on, vol. 2, pp.
21–24 (1994). DOI 10.1109/ICASSP.1994.389728Chapter 6
Empirical Validation of Probabilistic Indexing
Methods
Abstract The proposed probabilistic framework and most of the specific approaches,
algorithms, assumptions, and claims discussed throughout the previous chapters,
require empirical validation. This is the main purpose of this chapter. In particular,
the most relevant questions that we aim to answer trough the experiments are:
1. As compared with the HTR-oriented formulation proposed in Sec. 3.4, how do
the various lexicon-based image-processing-oriented posteriorgram methods
discussed in Secs. 3.1 and 3.3.2 perform? This will be studied in Sec. 6.2.
2. As discussed in Chapter 3, text-lines are particularly interesting image regions
for indexing purposes. So, the question is, how the different RPs defined in that
chapter can advantageously be used under a line-level PrIx paradigm? This
will be developed in Sec. 6.3.
3. What is the impact of a language model on PrIx performance? Which general
approach is preferable, lexicon-based, or lexicon-free? These questions are
tackled in Sec. 6.4.
4. How does the amount of training examples affect PrIx performance? This is
studied in Sec. 6.5.
5. Given that both PrIx and HTR use the same underlying probability distributions,
is there a clear correlation between the performance on HTR and PrIx tasks?
This topic is examined in Sec. 6.6.
6. Since search is one of the main applications of both PrIx and KWS, how
does our PrIx methods compare with state-of-the-art KWS approaches? This
is studied at line-level in Sec. 6.7.
7. How much PrIx performance improvement can be expected by using the newer,
neural-network-based CRNN optical models with respect to adopting more
traditional statistical HMM models? We analyze this question in Sec. 6.8.
8. Can line-oriented PrIx RPs be used to tackle KWS under a segmentation-free
paradigm? This is empirically assessed in 6.10.1.
9. Is the approach proposed in Sec. 3.6 to compute the RP for a query image (rather
than a textual query) adequate to perform traditional QbE KWS? How does this
approach fares with respect to other segmentation-free QbE KWS methods?
This is studied in Sec. 6.10.2.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 139
A. H. Toselli et al., Probabilistic Indexing for Information Search and Retrieval 
in Large Collections of Handwritten Text Images, The Information Retrieval Series 49, 
https://doi.org/10.1007/978-3-031-55389-9_6 
 140 6 Empirical Validation of Probabilistic Indexing Methods
6.1 Experimental Setup
Assessment conventions, datasets and models generally adopted in the experiments
are outlined here.
6.1.1 Evaluation Protocol: Image Regions, Query Sets and Metrics
For simplicity, homogeneity and easy comparison with other approaches, the evalu￾ation protocol generally adopted in this chapter is line-oriented. Text line regions are
relatively easy to detect in images and it is fairly inexpensive to manually validate the
detection results so as to create a reliable GT of adequately indexable image regions.
In line-level evaluation, for each given query, the system has to determine the text
lines of a data set which are relevant to the query, regardless of the position or the
number of occurrences of the query word within the text line. Nevertheless, some
experiments under the fully segmentation-free paradigm are also reported, as well
as a couple of experiments under the word-segmentation assumption. In these cases,
not only the relevant textlines have to be detected, but also the specific positions
(e.g. bounding boxes) of each detected instance of the query word. The details of
this protocol will be given in Sec. 6.10.
The line-oriented protocol is exactly all what is needed for experiments where the
RP is defined at whole (line) region level, that is, 푃(푅 | 푥, 푣). However, assessment
is also needed for other more fine-grained, position-dependent RP definitions that
where discussed in Chapter 3. Yet, annotating a reference data set at the word
bounding-box level is extremely expensive, as compared with line-level annotation.
Conversely, line-level transcripts are routinely produced to create HTR reference data
for HTR training and evaluation. Therefore, the same GT can be straightforwardly
used also to evaluate KWS or PrIx performance, without incurring extra annotation
costs. As a compromise, position-dependent RP methods where indirectly assessed
by using each of these methods to approximate a position-independent RP. Then,
results where evaluated under the very convenient line-oriented protocol (see details
in Sec. 6.3). To evaluate a position-dependent method in a line-level setting, we only
need to determine whether or not the full text line is relevant. That is, we can simply
keep, for each word in each line, the position (i.e. horizontal coordinate, segment, or
ordinal position) with the maximum RP value, as suggested by the Frechet bounds ´
discussed in Sec. 3.7.
Another noteworthy aspect of the evaluation protocol is how to select an adequate
query set. Several criteria can be adopted to select keywords for PrIx and KWS
assessment. Clearly, a system may perform significantly better or worse depending
on the query words it is tested with and how these words are distributed on the test set.
Of course, the larger the keyword set, the more reliable the empirical results. Since
this book aims at indexing applications, testing with large query sets is mandatory.
With this in mind, the criterion adopted was to consider as keywords all (or most of)
the words that appear in the training transcripts of each dataset.6.1 Experimental Setup 141
It is important to remark that, in contrast with other keyword selection criteria
which adopt only test-set words for queries, in our case many keywords actually fail to
appear in any of the test images. We say that these keywords are non-relevant, while
the remaining ones are relevant. Trying to spot non-relevant words is challenging
since, depending on the system accuracy, similar relevant words may be erroneously
spotted, thereby leading to important precision degradations.
We typically report both Mean and Global Average Precision (mAP and gAP)
metrics, as defined in Eqs. (1.7, 1.12, 1.14), with interpolated precision computed
according to Eq. (1.13). Following [16], the interpolated precision is justified from
a practical point of view: one typically would look at a few more results if this
increases the percentage of relevant retrieved documents (that is, the precision of
the larger set is higher). In addition, the interpolated precision at a recall of 0 is
well-defined, whereas plain precision is not. On the other hand, recall that the AP is
the area below the R–P curve. In order to compute this area, the trapezoid integral
was used in most of the experiments. All AP results will be generally reported as
percentages. In addition, in some experiments we also plot the actual R–P curves to
give a deeper insight on the performance of the corresponding approach.
This protocol was uniformly followed in most experiments. However, some details
of this protocol may differ in certain cases for the sake of fair comparison with
previously published results. This will stated in the corresponding sections.
6.1.2 Datasets and Query Sets
Several datasets are adopted but most of the experiments (at line level) are based
only on the IAM corpus (see Appendix C.1). This is considered one of the seminal
datasets for HTR benchmarking and more recently, it was adopted also in KWS,
where it was considered to be much harder than other data sets, such as George
Washington (GW), Appendix C.3, or Parzival (PAR), Appendix C.4. Yet, results
will be also reported on other larger, more realistic (and complex) datasets like
Bentham, Appendix C.2, and Plantas (PLA), Appendix C.5. Bentham is a large
collection from which several experimental datasets have been proposed for different
purposes. Three of these datasets were used in the experiments of this chapter: BEN1,
defined for the ICFHR-2014 HTRtS competition, BEN2, defined for the ICFHR-2014
KWS competition and BEN3, defined for the ICDAR-2015 KWS competition.
Main features of these datasets are summarized in Table 6.1 and full details can
be consulted in the Appendix C. In all the cases, ground-truth line segmentation
was available, both for training and testing images, and it was used in all the line￾level experiments. Regarding GW (which is a very small set), the standard 4-fold
cross-validation scheme adopted by most other authors was also adopted here and
the figures in Table 6.1 are averages over the 4 folds. On the other hand, in two of
the Bentham datasets only very few query words are defined (as it was customary in
KWS experimental settings at the time). Here these datasets are only used in some
experiments where performance is measured so as it can be fairly and accurately
compared with results achieved by other segmentation-free or QbE KWS methods.142 6 Empirical Validation of Probabilistic Indexing Methods
Table 6.1: Main dataset features of: IAM, Parzival (PAR), George Washington (GW),
Bentham (BEN1: ICFHR-2014 HTRtS: , BEN2: ICFHR-2014 KWS, BEN3: ICDAR-2015
KWS) and Plantas (PLA). All numbers are thousands, except Words / line.
IAM PAR GW BEN1 BEN2 BEN3 PLA
Hands many three two several several several one
Training + validation lines 7.1 3.1 0.3 10.6 9.3 11.1 7.7
Test lines 0.9 1.3 0.2 0.9 1.3 – 11.8
Words / line 8.8 6.3 7.5 9.3 8.4 10.2 10.1
Training running words 53.8 † 14.0 2.5 86.1 70.2 84.1 67.9
Validation running words 8.6 5.7 1.2 13.0 8.2 12.8 9.8
Test running words 8.3 8.4 1.2 7.9 10.8 16.8 117.0
Train + validation lexicon size 7.8 † 3.2 0.9 8.7 8.3 8.3 11.0
Query set size 3.4 3.2 0.9 8.7 0.3 ‡ 0.2 10.9
Non-relevant keywords 2.3 2.0 0.7 3.8 0.0 0.0 9.4
† Additional text-only data used for language modeling:
3.3 million running words, with a lexicon of more than 100 K words.
‡ Specifically 290 images of 41 different words (aimed at QbE experiments).
The query set of each dataset was generally composed of all (or most of) the words
that appear in the corresponding training transcripts. For the benchmark datasets,
IAM, GW and PAR, this allows us to produce results which are comparable with those
of the best approaches previously published. Also for this reason, we adopt for IAM
the same query set used in [32], which contains not all but only the most frequent
IAM training words, excluding punctuation marks and stop words. Finally, about
one hundred words corresponding to numbers were not included in the query set for
Plantas. Table 6.1 shows the sizes of these query sets. The number of non-relevant
keywords is also reported. Note that the proportion of these (challenging) keywords
ranges from more than 40% to almost 90% of the keywords in the corresponding
query set. All the datasets are publicly available, along with the details needed to
produce results comparable with those reported in this book.1
In addition to these academic datasets, other more realistic datasets, aimed at real,
large-scale applications, will be described in Sec. 6.9 (Table 6.13).
6.1.3 Statistical Models for Handwritten Text
Most of the experiments described here were conducted using neural networks based
on convolutional and one-dimensional LSTMs layers (i.e. CRNNs), as described in
Sec. 4.4.5. In the IAM experiments, we used the simplest neural network architecture
presented in [19] and summarized in Table 6.2. For other data sets, the architecture
employed will be described in each case. Most of the experiments were carried out
using PyLaia,2 a toolkit specifically developed to cater to the needs of researchers
working with handwritten documents.
1 IAM, PAR, GW: http://www.fki.inf.unibe.ch/databases
Bentham: https://zenodo.org/record/44519#.YxCuDEhBzZ8
Plantas: https://zenodo.org/record/6608342#.YxCu-0hBzZ9
2 https://github.com/jpuigcerver/PyLaia6.2 Assessing Posteriorgram Methods for Lexicon-based PrIx 143
Table 6.2: Architecture of the CRNN used in the IAM experiments.
Block Configuration Values
Convolutional Number of layers 5
Activation LeakyReLU
Filters {16, 32, 48, 64, 80}
Filter sizes {3, 3, 3, 3, 3}
Max. pooling {2, 2, 2, 0, 0}
Dropout {0.0, 0.0, 0.2, 0.2, 0.2}
Batch normalization yes
Recurrent Number of layers 5
Type BLSTM
Units {256, 256, 256, 256, 256}
Dropout {0.5, 0.5, 0.5, 0.5, 0.5}
Output layer Units (characters plus ∅) 80
Dropout 0.5
In most of the experiments, for training, the RMSProp algorithm was adopted to
incrementally update the parameters of the neural network using the gradients of the
CTC loss on each batch of a fixed number of images. In the IAM case, as described
in [19], this number was set up to 16 with a fixed learning rate equal to 0.0003 and
the training was stopped when the Character Error Rate (CER) on the validation set
did not improve after 20 epochs. In the cases that a different setting was adopted, it
will be indicated so.
The Kaldi toolkit3 was utilized to combine the output distribution of the neural
network with the 푛-gram language models, as discussed in Chapter 4, Secs. 4.6.1
and 4.7.2–4.7.3. The 푛-gram models were trained typically using the OpenGrm
libraries, with Kneser–Ney smoothing and interpolation.
In addition, comparative experiments using traditional HMMs (with GMMs) were
also performed (see Sec. 6.8). Basically, we adopted a left-to-right HMM topology
for modeling each character. The number of states and Gaussian densities per state
were roughly set up taking into account the average number of frames (or feature
vectors) aligned to each character in the alphabet. HMM training was carried out
with the embedded Baum–Welch algorithm using the HTK toolkit [43], which was
also used to integrate the 푛-gram LMs and to perform the whole decoding process.
6.2 Assessing Posteriorgram Methods for Lexicon-based PrIx
These of experiments were aimed to evaluate and compare the different approxima￾tions to position-independent, lexicon-based RP 푃(푅 | 푥, 푣) proposed in Chapter 3,
Sec. 3.3.3. These approaches are compared with the (almost) exact computation of
the same RP discussed in Sec. 3.4. Only the Bentham (BEN1) dataset was used in
these experiments. CRNN optical modeling with architecture as described in Ta￾ble 6.3 was adopted. The training set transcripts were used to extract the lexicon and
to train a 2-gram word language model. See more details in [36].
3 http://kaldi-asr.org144 6 Empirical Validation of Probabilistic Indexing Methods
Table 6.3: CRNN architecture used in the experiments with Bentham (BEN1).
Block Configuration Values
Convolutional Number of layers 4
Activation LeakyReLU
Filters {16, 16, 32, 32}
Filter sizes {3, 3, 3, 3}
Max. pooling {2, 2, 2, 0}
Batch normalization no
Recurrent Number of layers 3
Type BLSTM
Units {256, 256, 256}
Dropout {0.5, 0.5, 0.5}
Output layer Units (characters plus ∅) 87
Dropout 0.5
Table 6.4 reports the gAP achieved. The approximations range from the roughest
ones given by Eq. (3.7) (along with Eq. (3.8)) and using plain 1-best HTR transcripts
(Eq. (3.10)), to the potentially most accurate, but also highly computationally de￾manding approximation, given by Eq. (3.24). In order to illustrate the challenges
entailed by trying to spot non-relevant keywords, gAP results using only relevant
queries are also shown in Table 6.4 (column gAP푟
).
Table 6.4: Bentham (BEN1) interpolated gAP (in %) for various posteriorgram-based
approximations to the RP 푃(푅 | 푥, 푣), using CRNN optical models. To show the impact
of non-relevant keywords, the column gAP푟
reports results using a reduced query set of
4.9K relevant-only keywords (about 56% of the full query set, see Table 6.1).
Approximations to 푃(푅 | 푥, 푣) gAP gAP푟
Eq. (3.8) – Line-region word posterior 78.2 88.4
Eq. (3.10) – 1-best transcripts 76.3 82.1
Eq. (3.12) – Sum (improper distribution) 87.9 91.8
Eq. (3.13) – Naive Bayes, by DP using Eq. (3.14) 91.3 95.0
Eq. (3.15) – Max (direct with Eq. (3.17)) 91.4 95.2
Eq. (3.24) – “Exact” (high computing cost, using Alg. 5.2) 91.3 95.0
The results achieved by Eqs. (3.13/3.14) and (3.15/3.17) are both practically
identical to those of Eq. (3.24) which, as commented above, entails an exceedingly
high computational cost and is reported here only as a reference point. As expected,
the other, rougher approximations (Eqs. (3.8, 3.10, 3.12)) are significantly worse, the
naive 1-best providing the worst performance.
Among the approximations considered, Eq. (3.15/3.17) is as good as the best
ones, and also the fastest and simplest, and it does not have any hyper-parameter
which needs to be tuned.6.3 Comparing Position-Dependent RP Definitions 145
6.3 Comparing Position-Dependent RP Definitions
In Sec. 3.7 we saw that all RPs presented through Chapter 3 are related. For instance,
the maximum position-dependent RP of an image region 푥 for a keyword 푣, is a
lower bound of the position-independent relevance probability, 푃(푅 | 푥, 푣) (see
Sec. 3.7.4 and Fig. 3.10). So this chapter considers additional approximations to
position-independent RP which can be derived from the different position-dependent
RPs discussed through Chapter 3 and can be easily evaluated under the convenient
line-level protocol. The interests of these experiments are two-fold: On the one hand,
the results will provide an indirect assessment of the different position-dependent
RPs studied in Chapter 3. On the other, as we saw in Sec. 5.3.1, creating an (exact)
position-independent PrIx has a much higher asymptotic cost than other position￾dependent RPs; therefore, it is obviously interesting to evaluate how accurate are the
approximations provided by these cheaper (and more fine-grained) alternatives to
position-independent RP.
Thus, in this section the (line-level) gAP and mAP performance of PrIxs created
for the IAM dataset, using each of the RPs introduced in Chapter 3 will be evaluated,
along with the processing time required in each case.
Both the lexicon-based and the lexicon-free approaches were considered. In the
first case, a word 3-gram LM was used, with a vocabulary of 50 000 words while, in
the second, the LM was a character 8-gram. In the lexicon-free case, some algorithms
need to set the maximum number of (pseudo-)words to index per line, and we fixed
this number to 100 (the impact of this tunable parameter is studied in Sec. 6.4.2).
The RPs and algorithms used to build the respective PrIxs were as follows:
• The position independent (line-level) RP, 푃(푅 | 푥, 푣), described in Sec. 3.3,
directly using the lexicon-based Alg. 5.2. In the lexicon-free case, the character
lattice was simply converted into a word lattice, by means of Alg. 5.8, and
finally the lexicon-based method was applied.
• (Maximum) horizontal coordinate RP, max푖 푃(푅 | 푥, 푣, 푖), introduced in
Sec. 3.5.1. No algorithm was explicitly described in Chapter 5, but the method
was briefly described in Sec. 5.3.3, where it was also mentioned that the outcome
would be the same as that of the posteriorgram in Alg. 5.1. In the lexicon-free
case, we first convert the lattice into a (pseudo-)word lattice, as in the previous
approach, and then use the lexicon-based algorithm.
• (Maximum) segment RP, max풃 푃(푅 | 푥, 푣, 풃), presented in Sec. 3.5.2. The
lexicon-based algorithm is described in Sec. 5.3.2, and the lexicon-free al￾gorithm in Sec. 5.6.1.
• (Maximum) ordinal position RP, max푘 푃(푅 | 푥, 푣, 푘), described in Sec. 3.5.3.
The lexicon-based algorithm is described in Sec. 5.3.4, and the lexicon-free
algorithm in Sec. 5.6.2.
Lexicon-free approaches need a set of delimiter characters to automatically es￾tablish (pseudo-)word hypotheses, as we explained in Sec. 5.5. The delimiters used
for IAM were: “#”, “&”, “ ( ”, “ ) ”, “ * ”, “ : ”, “ ; ”, “?” and the white space symbol.146 6 Empirical Validation of Probabilistic Indexing Methods
Table 6.5 summarizes the results. Observe that all lexicon-based approaches
behave very similarly and the same happens for the lexicon-free methods, although
both the mAP and gAP of the latter are higher. However, as we will see in Sec. 6.4.1,
lexicon-based results become sensibly better when the vocabulary size is increased.
Table 6.5: Line-level mAP and gAP for different RPs on the IAM dataset. Lexicon-based
and lexicon-free results are shown. In both cases, a CRNN and a 푛-gram language model
were combined to generate the lattices from which the PrIxs were built.
(a) 50 000 words lexicon-based, using a word 3-gram LM
Method mAP (%) gAP (%)
푃(푅 | 푥, 푣) 93.8 91.1
max푖 푃(푅 | 푥, 푣, 푖) 94.0 91.4
max풃 푃(푅 | 푥, 푣, 풃) 94.0 91.6
max푘 푃(푅 | 푥, 푣, 푘) 94.0 91.1
(b) Lexicon-free, using a character 8-gram LM
Method mAP (%) gAP (%)
푃(푅 | 푥, 푣) 95.7 93.0
max푖 푃(푅 | 푥, 푣, 푖) 95.4 92.8
max풃 푃(푅 | 푥, 푣, 풃) 95.5 92.9
max푘 푃(푅 | 푥, 푣, 푘) 95.4 92.7
In the lexicon-based case, it may come as a surprise that the performance for
푃(푅 | 푥,푣) is lower than that of the other lower bounds to this RP. After all, as
proved in Appendix A, ranking according to this position-independent RP should
yield the maximum mAP and gAP values according to the assumed line-level loss.
Nevertheless, recall that those proofs rely on the assumption that the RP distribution
used to rank the results is actually the real distribution of the data. But here we are
just using training-set estimates based on a (set of) statistical model(s).
Anyhow, the differences between methods are very small. Therefore, selecting
one of these RPs in practice, will mainly depend of the kind of PrIxs (dependent on
segment or ordinal position, or just position-independent) that are most suitable for
the application considered. Moreover, when better (lexicon-free) statistical models
are used, the performance is consistent with the theoretical results from Appendix A.
Table 6.6 reports the total time needed by each approach to build the corresponding
PrIx. These times include all the steps, from reading the lattices to writing the PrIx
to disk, for both the validation and test sets of the dataset.
Lexicon-based approaches are faster than the equivalent lexicon-free counterparts.
This is because character lattices are typically larger (have more edges and states)
than word lattices. In addition, remember that Alg. 5.8, used to convert character
lattices into word lattices in the first two lexicon-free approaches, has a worst-case
computational cost which may grow exponentially with the length of the pseudo￾word hypotheses. For most text lines, these methods are very fast (less than 0.1 sec￾onds/line), but for some extreme cases with a huge amount of edges, the exponential
time (and memory) is excessive and the expanded lattices had to be pruned. Another6.3 Comparing Position-Dependent RP Definitions 147
Table 6.6: Time needed by different algorithms to build IAM PrIxs for both the validation
and test sets (1 849 lines, 229 pages). Results using lexicon-based and lexicon-free ap￾proaches are shown. The total time needed to generate the respective lattices is also shown.
Experiments performed on a single core of an Intel Core i7-3820 CPU at 3.60GHz
(a) 50 000 words lexicon-based, using a word 3-gram LM
Method Time (sec.)
푃(푅 | 푥, 푣) 23.8
max푖 푃(푅 | 푥, 푣, 푖) 1.0
max풃 푃(푅 | 푥, 푣, 풃) 0.6
max푘 푃(푅 | 푥, 푣, 푘) 0.8
Lattice creation 105.8
(avg. size: 37.4 states, 108.5 edges, 2.8 · 107 paths)
(b) Lexicon-free, using a character 8-gram LM
Method Time (sec.)
푃(푅 | 푥, 푣) 1754.9
max푖 푃(푅 | 푥, 푣, 푖) 101.2
max풃 푃(푅 | 푥, 푣, 풃) 5.3
max푘 푃(푅 | 푥, 푣, 푘) 11. 4
Lattice creation 264.4
(avg. size: 506.2 states, 713.5 edges, 2.7 · 1012 paths)
alternative would be to first extract a set of candidate words to index (using one of
the faster lexicon-free approaches), and then compute the position-independent or
the horizontal coordinate RP only for these (pseudo-)words.
Nevertheless, given that the segment and position RPs obtain virtually the same
mAP and gAP than the position-independent or horizontal coordinate RPs, in real
application we can simply use one of these. As a matter of fact, these faster approaches
are also more convenient for real applications, since they yield a bounding box for
each indexed spot, and thereby support more advanced types of queries in a very
efficient manner, as we will see in Chapter 8, Sec. 8.1.
For the rest of the line-oriented experiments in this chapter, unless stated oth￾erwise, we will make use of the segment RP, 푃(푅 | 푥, 푣, 풃), since it is the fastest
method for both lexicon-based and lexicon-free approaches.
Taking into consideration both the time needed to generate the lattices and to com￾pute the RPs, we could process 229 pages in just 106.4 seconds (2.15 pages/second)
to produce the corresponding lexicon-based PrIxs; or in 269.7 seconds (0.85
pages/second) for lexicon-free PrIxs. These processing times were measured us￾ing a single core of a relatively modest CPU, while the whole indexing process can
be widely parallelized, since each text line can be processed independently.
Although lexicon-free approaches are slower than lexicon-based ones, they do
not suffer from the out-of-vocabulary problem, which is a real issue in applications
with scarce training samples (recall that a large amount of external data was used
here to estimate the IAM language models). Thus, in real large-scale scenarios, like
those discussed in Chapter 10), lexicon-free approaches are typically preferred.148 6 Empirical Validation of Probabilistic Indexing Methods
6.4 Evaluating Language Model Impact
This section is devoted to empirically analyze the effect of the language model in
PrIx performance. These experiments are all performed on the IAM dataset.
Line-level PrIxs are obtained using the maximum of the segment RP obtained
for each (pseudo-)word in each line. More specifically, the method referred to as
max풃 푃(푅 | 푥, 푣, 풃) in the previous section is adopted.
Regarding the hyperparameters needed in the combination of the output of the
neural networks and the 푛-gram language models, in each of the following experi￾ments, both the optical and the prior scales are tuned using Bayesian optimization.
Later, it will be shown that tuning both parameters is not critical to achieve very
competitive results, but here we have tuned both for empirical completeness.
6.4.1 Lexicon-based Models
In lexicon-based 푛-gram LMs each 푛-gram token represents a full word from a given
lexicon. The white space characters are modeled as part of the word. Specifically,
each word is modeled to start with a white space character.
It is important to highlight that no text tokenization has been performed before
training the LM. This step is usually done in HTR and many other applications in
order to reduce the effective lexicon size (e.g. words like “don’t”, and “aren’t” are
split into “do” + “n’t” and “are” + “n’t”, respectively). This typically helps improving
the HTR accuracy, but it introduces other problems in the case of PrIx. Thus, to
keep the experiments simple it was decided to train a LM with the original text data.
There are two hyperparameters to adjust in this regard. One is the length of the
LM context (푛, in the 푛-gram), and the other is the lexicon size.
푵-gram Order
First, the effect of the 푛-gram context length in PrIx performance is studied, with a
lexicon size fixed to 50 000 words. Fig. 6.1 shows the evolution of mAP and gAP for
increasing 푛, both for the validation and the test sets of the IAM dataset.
In the validation set, all models achieve a competitive AP, and the results are
almost identical for 푛 ≥ 2. Considering these results, the optimal choice is 푛 = 3,
with 92.4% gAP and 94.9% mAP for the validation set. Similar conclusions can be
drawn from the test-set results,4 with about 91.8% gAP and 94.3% mAP for 푛 ≥ 2.
4 While the statistical significance of these differences was not formally evaluated, it is very likely
that they not significant. This guess is based on the width of the confidence intervals from other
publications, which is typically about 1–2% in AP [25].6.4 Evaluating Language Model Impact 149
1 2 3 4 5
85
90
95
100
Word 푛-gram order (푛)
Average Precision (AP %)
Valid, mAP
Valid, gAP
Test, mAP
Test, gAP
Fig. 6.1: Evolution of mAP and gAP for increasing word LM order, 푛, on the IAM dataset.
The lexicon of all the LMs was restricted to 50 000 words. Both validation (Valid) and Test
set results are shown.
Lexicon Size
As previously discussed, the lexicon size is expected to have a significant impact on
the performance of lexicon-based PrIx approaches because any query word that is
not in the models lexicon will just get a null RP. This is empirically studied here.
Now, the 푛-gram context length was kept constant (푛 = 3) and performance was
evaluated for increasing lexicon sizes (10k, 20k, . . ., and 100k). Notice that such
large lexicon LMs could only be trained for the IAM task by using huge additional
sources of text, such as the Brown, LOB and Wellington datasets of (modern English)
electronic text. The lexicon size of the training partition of the IAM dataset itself is
only of 7 772 words (c.f. Table 6.1).
Fig. 6.2 shows the evolution of the mAP and gAP for increasing lexicon size, both
for the validation and test sets of the IAM dataset.
As expected, PrIx performance improves consistently with the number of words
in the lexicon. Best results, for both validation and test sets, are achieved using the
largest lexicon (100 000 words), with 95.1% mAP and 93.0% gAP for the test set.
Of course, the RP of any OOV query will always be zero, for any text line image.
However, OOVs affect the performance even if not queried. While the lattice is being
built, the image segment corresponding to an OOV word will be aligned with many
other (wrong) words, all with a very low likelihood. This may cause issues with the
beam pruning used during decoding (if a narrow beam is used, all terminal states
may be pruned; if the beam is increased, the lattice generation can be much slower),
and may introduce errors in the subsequent word hypotheses of the path, since the
probability of 푛 subsequent words is tied due to the context size of the 푛-grams.150 6 Empirical Validation of Probabilistic Indexing Methods
20 40 60 80 100
80
85
90
95
100
Lexicon size (in K-words)
Average Precision (AP %)
Valid, mAP
Valid, gAP
Test, mAP
Test, gAP
Fig. 6.2: Evolution of mAP and gAP for increasing lexicon size, on the IAM dataset. All
LMs were 3-grams. Results are shown both for the validation (Valid) and Test sets.
6.4.2 Lexicon-free Models
As discussed in previous parts of this book, lexicon-free PrIx approaches offer
promise to avoid many of the issues that lexicon-based approaches may present.
Thus this section investigates how the different settings related with the LM affect
the performance of a lexicon-free approach.
In order to train the character 푛-gram LM required for lexicon-free processing, the
original words in the training transcripts of the IAM dataset (as well as the Brown,
LOB, and Wellington) were considered just as character sequences, with a “white
space” symbol pre-appended to each sequence. This is just the same reference text
used to train the CRNN with the CTC algorithm.
푵-gram Order
The PrIx performance was evaluated for increasing order of the character 푛-gram
LM. As in some of the previous experiments, the 100-best (pseudo-)word segment
hypotheses are extracted from each text line. Later, we will see that this provides a
good trade-off between quality and speed.
Fig. 6.3 shows the results from this experiment, where the mAP and gAP are
plotted for increasing 푛-gram order. As expected, also in this lexicon-free case, the
results improve with the context length of the LM. The best validation results are
achieved with the character 9-grams (96.0% gAP, 96.8% mAP), although they are
almost identical to results obtained with 8-grams. Similar behavior is observed on
the test set, with 92.9% gAP and 95.5% mAP for 푛 ≥ 8.
Observe that in order to have a character 푛-gram that covers the same (or similar)
context as a word 푚-gram, we typically need a larger context. Since we included the6.4 Evaluating Language Model Impact 151
3 4 5 6 7 8 9
85
90
95
100
Character 푛-gram order (푛)
Average Precision (AP %)
Valid, mAP
Valid, gAP
Test, mAP
Test, gAP
Fig. 6.3: Evolution of lexicon-free gAP and mAP for increasing order of a character 푛-gram
LM, on the IAM dataset. Results are shown both for the validation (Valid) and test sets.
white space character as part of the word in the lexicon-based system, to cover the
same number of characters as a 푚-gram word model we need a character context of,
approximately, 푛 = 푚 · 푤 + (푚 − 1), where 푤 is the average number of characters
per word. For example, in the IAM dataset, the average number of characters per
word is 푤 = 4.1. Therefore, about 2 · 4.1 + 1 = 9.2 characters are needed to cover the
same context as a 2-gram word-based LM. This is a good rule of thumb to estimate
the required character context length that would approach the context modeling
capabilities of a word 푛-gram.
Anyhow, given that all these experiments are actually very fast to perform, one
can simply sweep across multiple context sizes, just as we did here.
It is worth noting that the results achieved with character 8-grams are similar
(actually slightly better) than those achieved with a very large lexicon and a 2-
gram or a 3-gram word LM. As already commented, in real applications where no
additional training data is available, character LMs are often preferred. Thus, for the
rest of the experiments, unless otherwise stated, we will use a lexicon-free approach.
Number of Indexed Spots per Line and PrIx Density
In all the previous lexicon-free experiments, only the 100-best pseudo-word segment
hypotheses (at most) where retained for each text line image, using the algorithm
described in Sec. 5.6.1. Note that this is a maximum number of indexed spots per line.
If the lattice of a particular text line contains few hypotheses (due to beam pruning)
the actual number may be smaller. In any case, this is a hyperparameter that can be
easily adjusted and it is worth studying how it affects the PrIx performance.152 6 Empirical Validation of Probabilistic Indexing Methods
Generally speaking, depending on the entropy of the true transcript posterior
distribution, 푃(푤 | 푥), and the expected length of the transcripts, the required number
of indexed spots may need to be increased to achieve a good AP.
In the IAM dataset, assuming the reference text represents the true transcript
posterior, the entropy of the true distribution is 0 (only one transcript per text line is
given). Regarding the expected length of the transcripts, among both validation and
test sets, each text line contains an average of 9.1 words, with a standard deviation of
2.2 words/line. Therefore, about 9 spots can be assumed to be a strict lower bound
on the number of spots to index per line.5
This parameter will directly affect the so-called PrIx density, defined as the
average number of spots used to index each actual word written in the document.
The above lower bound would correspond to a density of 1, which implicitly assumes
that no uncertainty exists in the interpretation of the text images and that the statistical
models used are perfect. Obviously, in real applications, an adequate density has to
be (empirically) established so as to cope with all the sources of uncertainty, while
also avoiding the resulting PrIxs to become exceedingly large.
Fig. 6.4 shows the evolution of the AP for increasing maximum number of indexed
spots per text line. The PrIx density of each point can be approximately determined
by dividing this number by 9.1, the average number of words per IAM text line.
Observe that once we set this parameter to 30 (which roughly corresponds to a
density of 3.3 spots per running word), we start obtaining very good results. The best
performance is achieved when this number is 100 (a density of 11 spots per running
word) or larger, with about 93% gAP and 96% mAP. Therefore, in the rest of the
experiments in this book this hyperparameter is set to 100.
These results are worth comparing with naively indexing the text resulting from
plain HTR transcripts of the text images, which should roughly correspond to setting
a PrIx density of 1 spot per running word (or about 9 spots per text line). The test-set
recognition error rates of the automatic transcripts yield by a HTR system based
exactly with the same statistical models as in the PrIx experiments are fairly low
(4.4% CER, and 11.7% WER). However, The gAP and mAP obtained by indexing
the words of these transcripts are comparatively poor: only 80.7% and 85.8%,
respectively. These numbers are in fact close to what could be expected for 9 spots
per line in Fig. 6.4 (1 spot per running word), and more than 10 points worse that
what PrIx achieves even with a low density of just 3.3 spots per running word.
Table 6.7 shows the evolution of the total sizes of the PrIxs (including both
the validation and test images),6 as well as the total time required to produce such
indexes, with the maximum number of indexed spots per line.
Observe that, even for the largest density (2 670 word hypotheses per running
word, obtained by extracting at most 24 300 spots per text line), we needed just about
15 seconds to generate a 1 MByte PrIx from the lattices of the 1 849 text lines (226
pages) that comprise both the validation and test sets. Yet, virtually the same gAP
5 In real scenarios, the expected number of words can be computed from the lattices.
6 Using a single core of an Intel Core i7-3820 CPU at 3.60GHz6.4 Evaluating Language Model Impact 153
10 100 1000 10000
85
90
95
100
Indexed Spots / Line
Average Precision (AP %)
Valid, mAP
Valid, gAP
Test, mAP
Test, gAP
Fig. 6.4: Evolution of the mAP and gAP with respect to the number of indexed segments
per line, on the IAM dataset. The figure shows the results for both the validation (Valid)
and Test sets.
Table 6.7: Evolution of the total index size and indexing time for increasing maximum
number of spots per line, on the IAM dataset. Size and time values correspond to processing
both the validation and test sets. AP values from Fig. 6.4 are also reported for convenience.
Max. spots/line 10 30 100 300 900 2700 8100 24300
Density (spots/RW) 1.1 3.3 11.0 33.0 98.9 296.7 890.1 2670.3
Size (kB) 18.5 51.1 116.0 206.8 399.9 498.5 729.2 1062.4
Time (s) 4.8 4.8 5.2 5.9 8.3 8.5 11.1 15.3
Test-set gAP (%) 84.7 92.3 92.9 93.0 92.8 93.0 93.0 93.0
Test-set mAP (%) 86.7 94.9 95.5 96.0 95.6 96.1 96.1 96.1
and mAP results can be obtained in about 5 seconds, by indexing (at most) 100 spots
per line into a 116 KByte PrIx.
6.4.3 Effect of the Optical and Character-label Prior Scales
In most works, the optical scale7, 훾, and the character-label prior scale, 휂 (see
Eq. (4.22)), are tuned independently, typically using grid search (e.g. [5, 39]). How￾ever it has been argued that the effects of both hyperparameters on the desired metric
(e.g. mAP or gAP) are not independent. Thus, in all our previous (and following)
experiments, we tuned both parameters as if they were independent.
In particular, we first generate lattices using 훾 =1 for different values of the prior
scale, 휂 ∈ [0.0, 0.1, . . . , 1.0]. Because the optical cost is stored separately in the
7 The optical scale plays a similar role to the grammar scale factor. The former scales the likelihoods
of the optical model (a CRNN in our case), while the latter scales the language model probabilities.154 6 Empirical Validation of Probabilistic Indexing Methods
lattice edges, we can adjust 훾 once the lattice is generated, avoiding to re-generate
lattices for each value of 훾 that we need to evaluate.
Then, we use Bayesian optimization using a Tree-structured Parzen Estimator
(TPE) [2] to find a pair (훾
∗
, 휂
∗
) that maximizes the mAP and gAP average8, for the
validation set in each experiment. We use a Bayesian optimization approach, instead
of simple grid search, because it generally makes the search faster (in our case, about
2–3 times faster).
Figs. 6.5 and 6.6 show the evolution of the mAP and the gAP with respect to
the optical and the prior scales, in the validation set of the IAM dataset. The color
of the heat map represents the value of the mAP (resp. gAP), the 푥-coordinate
represents the value of the optical scale (훾 ∈ [0.2, 0.4, . . . , 4]), and the 푦-coordinate
represents the value of the prior scale (휂 ∈ [0.0, 0.1, . . . , 1.0]). The sampled results
were interpolated to produce a smoother plot. Fig. 6.5 was produced using a 8-gram
character language model, and Fig 6.6 using a 3-gram word language model with a
vocabulary size of 50 000 words.
Observe that, in both figures, the optical scale has a more significant impact on
the performance, compared to the prior scale. In the case of the lexicon-free model,
if one fixes the optical scale to its optimum value, then all values of the prior scale
have a very similar performance. The mAP in the lexicon-based approach (see the
left plot in Fig. 6.6) is slightly more susceptible to the combined effect of the two
hyperparameters.
1 2 3 4
0
0.5
1
Optical Scale 훾
Prior Scale
휂
mAP
1 2 3 4
0
0.5
1
Optical Scale 훾
gAP
88
90
92
94
96
Fig. 6.5: Evolution of lexicon-free IAM validation-set mAP and gAP with the optical and
prior scales (훾, 휂), using a 8-gram character language model.
This behavior is not specific of the IAM dataset. Additional experiments with
other datasets show similar trends. For instance, Fig. 6.7 shows the results of the
same experiment in the George Washington dataset (see Sec.C.3).
These results suggest that the value of the prior scale is not critical to achieve very
good KWS results. In fact, one could even use a null prior scale, which is equivalent
to using the raw output distribution to perform the lattice generation, and still get very
good results. This is good news, because estimating the prior distribution requires
8 We could optimize only the value of the mAP or the gAP, or some other combination of the two
(e.g. geometric mean). However, for simplicity we decided to use the average.6.5 Impact of Training-set Size and Data Augmentation 155
1 2 3 4
0
0.5
1
Optical Scale 훾
Prior Scale
휂
mAP
1 2 3 4
0
0.5
1
Optical Scale 훾
gAP
88
90
92
94
96
Fig. 6.6: Evolution of lexicon-based IAM validation-set mAP and gAP with varying optical
and prior scales (훾, 휂), using a 3-gram word language model with a 50 000 words lexicon.
1 2 3 4
0
0.5
1
Optical Scale 훾
Prior Scale
휂
mAP
1 2 3 4
0
0.5
1
Optical Scale 훾
gAP
86
92
98
Fig. 6.7: Evolution of lexicon-free George Washington validation-set mAP and gAP
with varying optical and prior scales (훾, 휂), using a 6-gram character language model. The
figures show the average of the validation-sets of the four cross-validation folds.
processing all the training data with the neural network and accumulate the posteriors
across all frames, which takes extra processing time.
In any case, optimizing the prior scale together with the optical scale using a
Bayesian optimization approach is very fast as well (in our case, it took 5 minutes
for the IAM dataset and 8 minutes for the George Washington dataset).
6.5 Impact of Training-set Size and Data Augmentation
Here, we study how the number of samples (i.e. text lines) available for training
influences PrIx results. Recall that in order to train the probabilistic models, we
need segmented text lines with their transcripts. Thus, a solution which is able to
perform well enough even with less training data can save potentially significant
costs of ground-truth production.156 6 Empirical Validation of Probabilistic Indexing Methods
The partition of the IAM adopted in our experiments consists of 6 161 training,
920 validation, and 929 test lines (747, 116, and 110 pages, respectively) – see other
details in Table 6.1. However, the total number of transcribed lines in the dataset
amounts to 13 353 (1 539 pages). Thus, several text lines can be excluded from or
added to the original training set to study the evolution of mAP and gAP with the
number of training lines.
The experiments of this section use the same character 8-gram language model
used before, which was trained using three external large text-only datasets. The
influence of external text data has not studied because gathering this type of data
barely requires human supervision, in comparison to the training data needed to train
the CRNN or GMM-HMMs.
In addition, the effect of using artificial data augmentation is also studied. This
has been widely used for many PR and ML applications. In the handwritten text
domain, we showed in [19] that simple (but adequate) random affine distortions,
applied to the training images, can significantly boost HTR performance.
Here, a random affine matrix was generated for each training image. To this end,
the upper-left, upper-right, and bottom-left coordinates of the image are translated
in a random direction, such that the maximum translation is 30% of the image
height. Then, we computed the matrix of the affine transformation that maps the
three original coordinates to the new ones. This allows us to efficiently generate
arbitrary affine transformations which do not alter significantly the content of the
image. Our training software performs this operation on-the-fly, for each training
image independently.
Fig. 6.8 shows the evolution of mAP and gAP, both for the validation and the test
sets, for increasing number of line images used to train the CRNN. The right and left
plots show the results when data augmentation was and was not used, respectively.
Observe that performance typically improves with the number of training lines
used. Even using a very small subset of training lines (about 2 000), our approach
provides decent mAP and gAP (91.1% and 88.8%, respectively), which are higher
than any other previously reported results for line-level KWS on the IAM dataset
(see Sec. 6.7.3).
In addition, using the proposed random distortions for artificial training data
augmentation also improves the results in all cases. For instance, using the original
training set (6 161 lines), the results on the test set improve from 95.5% mAP and
92.9% gAP, to 96.7% mAP 94.9% gAP.
6.6 Correlation between Average Precision and HTR Error Rates
Recall that the proposed PrIx approach ultimately relies on the same probability
distribution, 푃(푤 | 푥), typically used in HTR tasks. Therefore, one might wonder
what is the correlation of the performance between the PrIx/KWS metrics, mAP and
gAP, and the Character and Word Error Rates (CER and WER), typically used to
assess HTR results.6.6 Correlation between Average Precision and HTR Error Rates 157
2 4 6 8 10 12
90
95
100
Training Lines ( ·103
)
Average Precision (AP %)
Data Augmentation: No
2 4 6 8 10 12
90
95
100
Training Lines ( ·103
)
Data Augmentation: Yes
Valid, mAP Valid, gAP Test, mAP Test, gAP
Fig. 6.8: Evolution of the mAP and gAP with respect to the number of lines used to train
the CRNN, on the IAM dataset. The left plot shows the evolution when no artificial data
augmentation is used, while random affine distortions were used in the right plot. The
results for both the validation (Valid) and Test sets are shown.
In order to study this correlation, we used the same lexicon-free model used in
previous sections, considering different orders of the 푛-gram language model, and
different values of the optical and prior scales. For each combination of hyperparam￾eters, we compute the CER, WER, mAP and gAP on the IAM test set. The results
are shown in the scatter plots of Fig. 6.9.
Certainly, extreme cases can be artificially constructed such that the error rates
are arbitrarily large, and the AP still be close to 100% (or vice versa). For instance,
suppose that our recognition system recognizes perfectly all the query keywords
(which are very infrequent, compared to the total amount of words), but not any
other word. Then, the recognition error rates will be arbitrarily high, but both mAP
and gAP will be equal to 100%.
Nevertheless, as Fig. 6.9 shows, a fairly linear relationship does exist in all the
cases, the stronger correlation being for WER–gAP and slightly less for WER–mAP.
Of course, this is happens because the RP is defined at word-level and the number
of character errors in a given word is irrelevant, provided it is greater than zero.
This linear relationship can be useful to estimate PrIx expected results, for a
given model, using results from an HTR experiment (which is typically easier to
conduct). First, one just needs to estimate the slope and bias of the linear function,
and then simply evaluate the probabilistic models on the HTR tasks to predict the
PrIx performance.158 6 Empirical Validation of Probabilistic Indexing Methods
4 6 8
85
90
95
CER
mAP
10 15 20 25
85
90
95
WER
4 6 8
80
85
90
95
CER
gAP
10 15 20 25
80
85
90
95
WER
Corr-Coef = −0.938 Corr-Coef = −0.983
Corr-Coef = −0.965 Corr-Coef = −0.992
Fig. 6.9: Correlation between AP measures and HTR CER/WER, for the IAM test set.
All points in the plots use the same CRNN, and a lexicon-free language model. However,
different values for the order of the 푛-gram, and for the optical and prior scales were used
to generate the plots.
6.7 Results on Other Academic Benchmark Datasets
In this section, we evaluate one of our lexicon-free approaches in other line-level
academic datasets: George Washington and Parzival. See details in Table 6.1
of Sec. 6.1.1 and in Appendix C, and recall that, due to its small size, 4-fold cross
validation experiments are carried out with George Washington.
In each dataset, we tuned the order of the character 푛-gram language model, as
well as the optical and prior scales, as discussed in previous sections. For each text
line, we extracted only the 100-best pseudo-word segments using the lexicon-free
approach described in Sec. 5.6.1. Finally, the results achieved in these experiments
are compared with loosely comparable state-of-the-art results previously reported
by other authors.6.7 Results on Other Academic Benchmark Datasets 159
6.7.1 George Washington
We trained a CRNN neural network similar to the one used in the experiments with
IAM. Table 6.8 provides the configuration details. The CTC loss was minimized
using the RMSProp algorithm, a learning rate equal to 3 · 10−4
and a batch size of
16 images, for about 230k updates of the parameters.9 In this specific, very small
Dataset, we observed that using batch normalization [12] in the convolutional layers
improved HTR results, so we also used this technique in the PrIx experiments. In
addition, since the data set is so small, we also used training data augmentation
through random affine image transformations.
Table 6.8: Architecture of the CRNN used in the George Washington experiments.
Block Configuration Values
Convolutional Number of layers 4
Activation LeakyReLU
Filters {16, 32, 64, 64}
Filter sizes {3, 3, 3, 3}
Max. pooling {2, 2, 2, 0}
Batch normalization yes
Recurrent Number of layers 4
Type BLSTM
Units {128, 128, 128, 128}
Dropout {0.5, 0.5, 0.5, 0.5}
Output layer Units (characters plus ∅) 72
Dropout 0.5
Fig. 6.10 shows the evolution of the mAP and gAP with respect to the order
of the character 푛-gram. The word delimiters used to generate the PrIxs were the
parentheses, “(” and “)”, and the white space symbol. Since this experiment uses
four-fold cross-validation, we tuned a unique optical and prior scale by averaging the
results across the four partitions.
The results are very similar across all values of 푛. In order to chose a single value
for 푛, we picked the one that maximized the average of the mAP and gAP in the
validation set. There, the 8-gram model achieves a mAP of 97.6% and a gAP of
93.5%, while in the test set it achieves 95.6% (mAP) and 94.6% (gAP). Note that
these numbers are the averages across the four cross-validation folds. These results
were achieved with an optical and prior scales equal to 0.85 and 0.80, respectively.
9 This is about 700 epochs through the training data of each cross-validation fold, which took about
2h:12m using a NVIDIA Titan X.160 6 Empirical Validation of Probabilistic Indexing Methods
4 5 6 7 8
90
95
100
Order of the character 푛-gram (푛)
Average Precision (AP %)
Valid, mAP
Valid, gAP
Test, mAP
Test, gAP
Fig. 6.10: Evolution of mAP and gAP for increasing order of the character 푛-gram language
model, on the George Washington dataset. The figure shows the average across the four￾folds for both the validation (Valid) and Test sets.
6.7.2 Parzival
The details of the CRNN architecture used for these experiments are provided in
Table 6.9. The CTC loss was minimized using the RMSProp algorithm, a learning
rate equal to 5·10−4
and a batch size of 16 images, for about 28k parameter updates10
In contrast with the previous section, here we did not use batch normalization, nor
training data augmentation.
Table 6.9: Architecture of the CRNN used in the Parzival experiments.
Block Configuration Values
Convolutional Number of layers 4
Activation ReLU
Filters {16, 16, 32, 32}
Filter sizes {3, 3, 3, 3}
Max. pooling {2, 2, 2, 0}
Batch normalization no
Recurrent Number layers 3
Type BLSTM
Units {256, 256, 256}
Dropout {0.5, 0.5, 0.5}
Output layer Units (characters plus ∅) 97
Dropout 0.5
10 This is about 200 epochs through, which took about 2h:08m using a NVIDIA Titan X.6.7 Results on Other Academic Benchmark Datasets 161
Once more, we tried different context sizes of the 푛-gram character language
model, adjusting for each of them both the optical and the prior scale. The word
delimiters used to generate the PrIxs were the dot character, the hyphen (respectively
“pt” and “eq”, as they appear in the training-set transcripts), and the white space
symbol. Fig. 6.11 shows the results.
4 5 6 7
96
98
100
Order of the character 푛-gram (푛)
Average Precision (AP %)
Valid, mAP
Valid, gAP
Test, mAP
Test, gAP
Fig. 6.11: Evolution of the mAP and gAP for increasing order of the character 푛-gram
language model, on the Parzival dataset. The figure shows the results for both the validation
(Valid) and Test sets.
Results are very similar for all 푛, slightly better for 푛 = 6. In the validation set,
this language model obtains 98.1% for both mAP and gAP (with an optical and prior
scales equal to 0.55 and 0.20, respectively). In the test set, it achieves 97.3% and
98.2% points of mAP and gAP, respectively.
6.7.3 Comparison with Previous State-of-the-art Results
To put our results in relation with previously published work, Table 6.10 summarizes
previous state-of-the-art results in the query-by-string (QbS), line-level KWS sce￾nario, obtained by other authors on the IAM, George Washington and Parzival
datasets. The following approaches have been considered:
The method presented in [27] uses Histogram of Gradients (HOG) and Dynamic
Time Warping (DTW); Works [8, 7] are based on the classical HMM-Filler KWS
method, including character language models to improve the results; in [14] they
use a Bayesian Logistic Regression classifier; [42] is a method based on the HMM￾Filler but with additional background modeling; [41, 40] uses deep belief neural
networks in combination of DTW and HMMs; [32] essentially adopts the lexicon-162 6 Empirical Validation of Probabilistic Indexing Methods
based posteriorgram approach discussed in Secs. 3.3.2 and 3.3.4 (Eq. 3.19), making
use of GMM-HMMs and a word language model to build the necessary statistical
models. Finally, [9] uses the BLSTM-CTC approach that will be briefly discussed
in Sec. 7.5.
In the case of the IAM dataset, we report in this table the results achieved by the
segment RP lexicon-free approach, using 8-gram character language model, with ar￾tificial data augmentation (i.e. using random affine distortions on the original training
images). For the other two datasets, the reported results are the ones described earlier
in this section.
The methods developed in this book significantly improved previous state-of-the￾art results, for the three datasets considered. However, note that the experimental
setups adopted in some of these works may vary significantly with respect to the
setups adopted in our experiments. In particular, the results marked with † were
obtained using smaller query sets, in some cases selected from the test (rather than
the training) partitions. Therefore, the results of Table 6.10 can only be considered
loosely comparable. Additional results on some of these datasets will be presented
in Chapter 7.
Table 6.10: AP (%) results achieved by different QbS, line-level KWS approaches on the
IAM, George Washington (GW) and Parzival (PAR) datasets, as of 2016. Reported
results that did not provide the first decimal point are marked with “?”.
Measure IAM GW PAR
HOG-DTW
[27]
mAP — 79.1† —
Classic Filler-HMM
[8]
mAP 68.9† 79.3 88.2
gAP 47.8† 62.1 85.5
BLSTM-CTC
[9]
gAP 78.? 85.? 94.?
2-gram Filler-HMM
[7]
gAP 55.1† 73.9 —
BLRC
[14]
mAP 49.0† — —
Filler-BGR
[42]
mAP 57.7† — —
CDBN-DTW
[41]
mAP — 67.4 62.4
gAP — 55.7 58.8
CDBN-HMM
[40]
mAP 72.4† 85.1 94.6
gAP 64.7† 71.2 92.3
HMM + 2-gram LM
[32]
gAP 72.? 77.? 90.?
PrIx mAP 96.7‡ 95.6 97.3
gAP 94.9‡ 94.6 98.2
† Smaller query set and/or number of evaluation lines.
‡ Artificial data augmentation.6.8 Comparing CRNN and HMM-GMM Optical Modeling 163
6.8 Comparing CRNN and HMM-GMM Optical Modeling
In all the experiments presented so far in this chapter only optical CRNN models
(combined with 푛-gram language models) were used. Nevertheless, the proposed
PrIx framework and methods seamlessly support other types of statistical models.
The only requirement is that the distribution 푃(푤 | 푥) or 푃(푤, 푥) can be represented
as a WFST, for any text (line) image 푥 and transcription hypothesis 푤.
For instance, as we explained in Sec. 4.3, classical HMMs (with GMMs) can also
be straightforwardly used to build the PrIxs. In fact, HMMs were our models of
choice at the early stages of the development of the PrIx technology.
This section is devoted to present experiments where PrIx performance using
CRNN and GMM–HMM optical models are compared. As we will see, PrIx based
on GMM-HMM models can also achieve very high gAP/mAP performance, even
though when these models are used for plain HTR, they yield significantly higher
WER than HTR based on CRNNs.
In these experiments we adopted two datasets, Bentham (BEN1) and Plantas
(see Table 6.1 of Sec. 6.1.1 and details in Appendix C), which are larger than those
used in previous sections and more realistically represent the kind of handwritten
historical collections which are the main focus of the PrIx framework here presented.
HMM training was carried out with the embedded Baum–Welch algorithm, as
discussed in Sec. 6.1.3, using the HTK toolkit [43]. A left-to-right HMM was used
for each character. The number of states and Gaussian densities per state were roughly
set up taking into account the average number of frames aligned to each character
in the alphabet and other data set features, and finally tuned using validation data.
Since the HTK toolkit does not offer good support for generating lattices with large
language models, only 2-gram word language models were used in these experiments.
More specific details about the HMM and the language model settings adopted for
each dataset are given in [31] and [36, 20] for Bentham and Plantas, respectively.
On the other hand, a common CRNN architecture was adopted for both datasets,
as described in Table 6.3 of Sec. 6.2. The only difference was the number of output
units (characters plus ∅) which was 77 for Plantas.
Global Average Precision (gAP) results for each dataset and type of optical
model are summarized in Table 6.11. Performances using raw 1-best HTR transcripts
(Eq. 3.10), based on the same models, are also reported.
Table 6.11: Comparison of gAP (in %) between using HMM and CRNN models for PrIx
and 1-best (1-b) HTR, on the Bentham and Plantas datasets.
Dataset HMM CRNN 1b-HMM 1b-CRNN
Bentham 90.7 91.4 74.0 76.3
Plantas 90.9 92.9 72.2 79.4
Fig. 6.12 shows the recall-precision (R–P) curves. The gAP values of Table 6.11
are the areas below these curves, as discussed in Sec. 1.6.164 6 Empirical Validation of Probabilistic Indexing Methods
0 0.2 0.4 0.6 0.8 1
0
0.2
0.4
0.6
0.8
1
Recall: 휌
Precision:
휋
Bentham
CRNN
HMM
1b-CRNN
0 0.2 0.4 0.6 0.8 1
0
0.2
0.4
0.6
0.8
1
Recall: 휌
Plantas
CRNN
HMM
1b-CRNN
Fig. 6.12: R–P curves of HMMs and CRNNs on the Bentham and Plantas datasets.
R–P curves are also shown for naive (non-probabilistic) indexes built from 1-best HTR
transcripts using the same CRNN optical and language models.
According to these results, using CRNNs in the place of HMMs only yields quite
modest gAP improvements. Since the gAP is very high in both cases, little gains can
be expected from using better statistical models. In contrast, the gAP achieved using
1-best transcripts is more than 12 perceptual points lower in all the cases.
Note, however, that the HMMs used in these experiments were the same as those
specifically developed and carefully tuned for each dataset in [31, 29], in order to
achieve excellent HTR performance. If less tightly tuned, vanilla HMMs are adopted,
HMM-based PrIx typically tends to yield worse performance than that achieved
with also vanilla CRNNs. For instance, the simple HMMs and hand-crafted feature
extraction methods proposed in [7] where used in [32] for PrIx. The gAP achieved
on the IAM dataset was 72.0%, while comparable (lexicon-based) gAP higher than
91% (see Table 6.5 (a)) is achieved using a CRNN with the architecture adopted in
all the IAM experiments of this chapter (Table 6.2).
Storage Efficiency
The work on the datasets used in the above experiments was aimed at actually index￾ing the full collections of Plantas11 and Bentham Papers, the latter encompassing
about 90 000 densely written handwritten images.12 Therefore, the issues studied
in Sec. 6.4.2 (Table 6.7), and the storage requirements in particular, were specially
relevant in this case too.
Recall that, as compared with most KWS techniques, PrIx methods rely on “off￾line” pre-computation of PrIxs that allow fast “on-line” search for information in
11 See http://bdh.bne.es/bnesearch/detalle/bdh0000140162
12 See https://www.ucl.ac.uk/bentham-project and the PrIx we created for the full
collection: https://www.prhlt.upv.es/htr/bentham6.9 Experiments for Real Indexing Projects 165
large collections of text images. Therefore, for large-scale indexing it is essential to
analyze how much information needs PrIx to pre-compute and store.
A rule of thumb, followed by our methods, is that PrIxs should be comparably
(much) smaller than the images for which they provide the indexing. For example for
Plantas and Bentham, the average size of a 300 dpi uncompressed page image is
about 30Mb. For the same datasets, PrIx produces an average of less than 3 000 spots
per page (according to Table 6.12, below and to [35]). And the size of a PrIx spot
is no more than 20 bytes, assuming we allot 8 characters per word on average, plus
4 and 8 bytes to store the relevance probability and the bounding box, respectively.
Therefore the ratio from (uncompressed) PrIx-size to Image-Size is less than 0.002.
Clearly, the size of an image depends on its resolution and many other factors.
Therefore, we prefer a less ambiguous analysis where PrIx size is instead compared
with the size of a perfect transcript of the image. As previously said, we refer to this
metric as PrIx density, defined as the ratio between the total number of PrIx spots
and the number of running words written in the indexed images. To put it another
way, it is the average number of word hypotheses that PrIx needs to retain to achieve
its AP performance. This metric, along with the AP values already given in Fig. 6.12,
is reported in Tab. 6.12 for Bentham and Plantas. The table also shows the values
for a naive indexing based on 1-best HTR transcripts. The density in this case is
obviously close to 1, but the AP is poor. In contrast, the gAP of PrIx is much better,
at the expense of a larger, but perfectly acceptable density.
Table 6.12: Search performance (gAP in %) and indexing density for Plantas and Ben￾tham, using the two optical modeling approaches, HMM and RNN, for PrIx and 1-best
(1Bst) HTR. Density (Den) is the ratio between the number of PrIx spots and the number
of running words in the ground-truth transcripts.
PrIx HMM PrIx RNN 1Best HMM 1Best RNN
gAP Dens gAP Dens gAP Dens gAP Dens
Bentham 90.7 7.8 91.4 7.9 74.0 1.0 76.3 0.9
Plantas 90.9 13.0 92.4 13.5 72.2 1.0 79.4 1.0
6.9 Experiments for Real Indexing Projects
In Chapter 10 we will describe several PrIx-based search and retrieval systems
developed for real, large-scale applications which involve historical manuscript col￾lections encompassing from tens of thousands up to more than one million pages.
Before developing each of these applications, preparatory experiments where carried
out on a selected, empirical dataset extracted from the corresponding text image col￾lection. These experiments were mainly aimed to determine adequate training data
and to predict the level of performance expected in each real, large-scale application.
The selected empirical datasets are described in detail in Appendix C and the main
features are summarized in Table 6.13 below. The experiments are presented here in
chronological order according to the dates they were performed.166 6 Empirical Validation of Probabilistic Indexing Methods
Table 6.13: Main dataset features of: “Teatro del Siglo de Oro” (TSO), “Large Bentham
Dataset” (BEN4), “Finnish Court Record” (FCR), Chancery (CHA), Passau (PAS) and
Carabela (CAR) datasets. All numbers are thousands, except Words / line.
PAS CHA TSO BEN4 CAR FCR
Hands many several several several many many
Training + Validation lines 29.3 6.1 11.9 23.9 17.3 26.0
Test lines 16.4 1.7 2.1 12.4 3.1 13.3
Average words per line 1.6 19.3 4.9 7.4 8.1 5.6
Train + validation running words 47.8 117.7 57.7 178.5 143.6 147.1
Test running words 26.7 33.1 10.2 † 89.9 † 21.8 73.8
Train + validation lexicon size 11.2 15.7 7.6 10.9 14.0 20.7
Test lexicon size 5.8 6.6 2.5 † 7.0 † 3.8 14.0
Query set size 5.7 6.5 2.5 7.0 3.8 10.4
† Additional text-only data used for language modeling: 3.3 and 7.8 millions of running
words for TSO and BEN4 respectively, with a lexicon of more than 60 K words.
Note that all the words of the query sets of these datasets are pertinent; that is, at
least one instance of each word is ensured to exist in the corresponding test set.
For these historical manuscript datasets, CRNNs and 푁-gram were used for
optical and language modeling respectively. For all (except Chancery, see below),
the HTR PyLaia toolkit [19] was used for optical modeling, which is based on the
PyTorch machine learning platform.
A similar CRNN architecture was adopted in all the experiments carried out using
PyLaia. Table 6.14 outlines this architecture for BEN4. In the coming subsections,
only the details which differ from this basic setting will be given for the experiments
with the other datasets.
Table 6.14: CRNN architecture used in experiments on the large Bentham(BEN4) dataset.
Block Configuration Values
Convolutional Number of layers 4
Activation LeakyReLU
Filters {12, 24, 48, 48}
Filter sizes {3, 3, 3, 3}
Max. pooling {2, 2, 0, 2}
Batch normalization {true, true, true, true}
Recurrent Number layers 3
Type BLSTM
Units {256, 256, 256}
Dropout {0.5, 0.5, 0.5}
Output layer Units (characters plus ∅) 68
Dropout 0.5
In all the experiments, (segment or ordinal) position-dependent PrIxs were com￾puted. However, as in previous sections, the evaluation was line-level, as discussed
in Secs. 6.1.1 and 6.3. The IR performance achieved in these experiments is reported
in Fig. 6.13. Specific details for each dataset are given in the coming subsections.6.9 Experiments for Real Indexing Projects 167
0 0.2 0.4 0.6 0.8 1
0
0.2
0.4
0.6
0.8
1
Recall: 휌
Precision:
휋
TSO
BEN4
FCR
CHA
PAS
CAR
PAS CHA TSO BEN4 CAR FCR
gAP 74.6 75.0 89.5 86.8 50.2 84.1
mAP 69.2 68.0 88.3 76.1 42.6 78.2
Fig. 6.13: PrIx R–P curves and corresponding gAP and mAP (in %), obtained in prepara￾tory laboratory experiments carried out on six datasets for real indexing projects: Passau
(PAS), Chancery (CHA), TSO, Large Bentham (BEN4), Carabela (CAR), and FCR.
6.9.1 Passau
This dataset was compiled by the Passau Diocesan Archives during the READ
project.13 It was initially aimed at developing a large scale PrIx demonstrator, even
though in the end the Archive did not wish to actually finish this implementation.
For model training, transcripts were previously transliterated according to what
is explained in Sec.C.6. Line images extracted from annotated positions in page
images were cleaned and contrast-enhanced using a Sauvola’s based method [23],
and normalized to 64-pixel height.
The optical model CRNN architecture was essentially the same described in
Table 6.14, with the few variations shown in Table 6.15. This model was trained with
the RMSProp method [28] on mini-batches of 32 examples, using a base learning
rate of 0.0005 to minimize the CTC cost function [10]. We stopped the training
procedure when the error on the development set did not decrease for 50 epochs.
13 See footnote 5 in Chapter 10.168 6 Empirical Validation of Probabilistic Indexing Methods
Table 6.15: Architecture of the CRNN used in the Passau experiments.
Block Configuration Values
Convolutional Filter sizes {7, 5, 3, 3}
Batch normalization {false, false, true, false}
Output layer Units (characters plus ∅) 100
For language modeling, a character 6-gram model was estimated with Kneser￾Ney back-off smoothing [13]. The trained CRNN, along with each character language
model, were subsequently used with the Kaldi decoder to produce CLs for all the
test-set image lines (see Table 6.13). One decoding pass with beam set to 15 was
carried out in all the cases. Finally, from the obtained CLs, a (segment) position￾dependent PrIx was produced using Alg. 5.13. The results were first reported in [15]
and here are summarized in Fig. 6.13.
6.9.2 Chancery (Himanis)
This dataset was compiled during the HIMANIS project14 as preparatory work to
develop the large-scale PrIx application described in Sec. 10.3.1. As explained in
Appendix C.7, text line images were semi-automatically detected and extracted and
then aligned with the GT transcripts (originally in XML-TEI format).
In this case, for optical modeling a CRNN architecture significantly different to
that of Table 6.14 was adopted. It consists of eight Gated-CNN layers followed by
two LSTM layers. A SoftMax output layer predicts the probabilities of 104 characters
and a CTC symbol. See [4] for details. The network was trained by gradient descent
with the RMSProp method [28], on mini-batches of 8 examples, using a base learning
rate of 0.001, to minimize the CTC objective function. The optimization process was
stopped when the error on the development set did not decrease for 20 epochs.
By using the training line transcripts, a character 5-gram model was estimated
with Kneser-Ney discounting with the IRSTLM Toolkit [6].
A WFST representing the character LM, was used in the decoding process to
generate required CLs. To this end, the Kaldi toolkit [17] decoder with beam search
(set to 20) was used, injecting in the edges of a such transducer the scores predicted
by the CRNN, scaled with character priors [1].
Once CLs were obtained, all the characters associated to their edges are diacritic￾and case-folded and then used by Alg. 5.13 to compute PrIx segment RPs. The
results were first reported in [3] and here are summarized in Fig. 6.13.
6.9.3 Teatro del Siglo de Oro (TSO)
This dataset was compiled during the READ projectas preparatory work to develop
the large-scale PrIx application described in Sec. 10.3.2.
14 See footnote 4 in Chapter 10.6.9 Experiments for Real Indexing Projects 169
As explained in Appendix C.8, TSO line images were detected and extracted from
color page images, using the system described in [11]. Next, they were normalized
to 64-pixel height, and their RGB color channels were processed independently for
cleanliness and contrast enhancement using a Sauvola’s based method [23]. Finally,
line-level reference transcripts were transliterated by removing diacritics (except the
“∼” of the Spanish “n”) and folding case. ˜
The optical model CRNN architecture was essentially the same described in
Table 6.14, with the few variations shown in Table 6.16. This model was trained
with the RMSProp method [28] on mini-batches of 16 examples, using a learning
rate of 0.0003 to minimize the CTC cost function [10]. The optimization procedure
was stopped when the error on the development set did not decrease for 50 epochs.
Table 6.16: Architecture of the CRNN used in the TSO experiments.
Block Configuration Values
Convolutional Filters {16, 32, 64, 96}
Max. pooling {2, 2, 2, 2}
Batch normalization {false, false, false, false}
Output layer Units (characters plus ∅) 61
A TSO 8-gram character language models was estimated with Kneser-Ney back￾off smoothing [13]) from the transliterated transcripts plus external related text
corpora encompassing about three million words (see Table 6.13).
The trained CRNN and language model were subsequently used with the Kaldi
decoder, with beam set to 15, to generate CLs for all the test-set image lines. From
these CLs, an ordinal position-dependent PrIx was produced using the Alg. 5.16.
Results were first reported in [30] and here are summarized in Fig. 6.13.
6.9.4 Large Bentham Dataset (BEN4)
This dataset, was also compiled during the READ project as preparatory work to
develop the large-scale PrIx application described in Sec. 10.3.3. It is described
in detail in Sec. C.2.4, as yet another dataset extracted from the Bentham Papers
collection, and it is significantly larger than the other Bentham datasets reported in
Table 6.1 and Secs. 6.8 and 6.10. Henceforward, this dataset will be referred to as
Large Bentham or just BEN4. Results on this dataset were published in [30].
Line images were detected and extracted from color page images using the system
described in [11] but, unlike TSO, they were then converted to gray-level and nor￾malized to 115-pixel height. Finally, they were processed for cleanliness and contrast
enhancement using a Sauvola’s based method [23].
The CRNN architecture used in this case was exactly the one outlined in optical
modeling Table 6.14. This model was trained with the RMSProp method [28] on
mini-batches of 24 examples, using a learning rate of 0.0005 to minimize the CTC
cost function [10]. The optimization procedure was stopped when the error on the
development set did not decrease for 50 epochs.170 6 Empirical Validation of Probabilistic Indexing Methods
An 8-gram character language model was trained with Kneser-Ney back-off
smoothing) [13] from the transliterated transcripts plus related external text cor￾pora encompassing about eight million words (see Table 6.13).
Finally, CLs were produced for all the test-set lines using the trained CRNN and
8-gram character language model using the Kaldi decoder (with beam set to 15).
Then, a segment position-dependent PrIx was obtained using the Alg. 5.13. Results
were first reported in [30] and here are summarized in Fig. 6.13.
6.9.5 Carabela
This dataset was compiled during the Carabela project15 as preparatory work to
develop the large-scale PrIx application described in Sec. 10.3.4.
Line images were detected and extracted from page images using the layout anal￾ysis tool P2PaLA [22] and detection errors were fixed manually. Line images were
cleaned and contrast-enhanced using a Sauvola’s based method [38], and normal￾ized to 64-pixel height. Finally, line-level reference transcripts were transliterated by
removing diacritics (except the “∼” of the Spanish “n”) and folding case. ˜
The optical model CRNN architecture was essentially the same described in
Table 6.14, with the few variations shown in Table 6.17. This model was trained
on samples from sets B[1-4] (see Table C.22) with the RMSProp method [28] on
mini-batches of 16 examples, using a learning rate of 0.0003 to minimize the CTC
cost function [10]. The optimization procedure was stopped when the error on the
validation set did not decrease for 50 epochs. On the other hand, for language mod￾eling, the SRILM toolkit [24] was used to train an 8-gram model on the transliterated
training transcripts. No additional, external text data was used in this case.
Table 6.17: Architecture of the CRNN used in the Carabela experiments.
Block Configuration Values
Convolutional Filters {16, 32, 64, 96}
Max. pooling {2, 2, 2, 2}
Batch normalization {false, false, false, false}
Output layer Units (characters plus ∅) 60
The language model, along with the trained CRNN, were subsequently used with
the Kaldi (Viterbi) decoder [17] to produce CLs for all line images of the 92 pages
of the set B5 (Table C.22). Finally, from these CLs, a ordinal position-dependent
PrIx was produced using Alg. 5.16. Results are were first reported in [33] and here
are summarized in Fig. 6.13.
These results were not as good as those of the other datasets of Table 6.13. But
these results might be somewhat pessimistic, because expert users of the Carabela
demonstrator (Sec. 10.3.4) generally made very satisfactory subjective assessments
of the system. As discussed in Sec. 10.3.4 and Appendix C.9, Carabela was one of
the most challenging sets of historical manuscripts we have ever considered.
15 See footnote 9 in Chapter 10.6.10 Segmentation-free Evaluation 171
6.9.6 Finish Court Records (FCR)
This dataset was compiled as preparatory work to develop the large-scale PrIx
application described in Sec. 10.3.4. As explained in Appendix C.10, FCR transcripts
were transliterated (by folding case and removing punctuation marks, diacritics and
non-ASCII symbols) and line images were cleaned and contrast-enhanced using a
Sauvola’s based method [23], and normalized to 115-pixel height.
The optical model CRNN architecture was the same described in Table 6.14,
with the only variation of the number output layer units, which in this case was 60
characters plus ∅. This model was trained using the RMSProp method [28], with
a mini-batch of 24 examples and a learning rate of 0.0003, to minimize the CTC
loss [10]. The optimization procedure was stopped when the error on the development
set did not decrease for 20 epochs. Moreover, in this case, data augmentation was
used, applying on the fly random affine transformations on each training line image.
This allows the optical model to generalize better on unseen data.
On the other hand, a 8-gram character language model with Kneser-Ney back-off
smoothing [13] was estimated from the transliterated training transcripts.The trained
CRNN and language model were subsequently used by the Kaldi decoder [17], with
a search beam of 15, to produce CLs for all the test-set line images. Finally, from
these CLs an ordinal position-dependent PrIx was produced using Alg. 5.16. Results
were first reported in [35] and here are summarized in Fig. 6.13.
6.10 Segmentation-free Evaluation
So far, all the experiments corresponded to documents with manually segmented
lines and the results were evaluated at line level (without the need of predicting
accurate spot bounding boxes. However, by using automatic text line segmentation
techniques [37], all the position-dependent RPs introduced in Chapter 3 can be
directly used in segmentation-free experiments.
In this section we will present results of approaches proposed in this book,
evaluated under a segmentation-free scenario. In particular, we conducted experi￾ments using the same datasets as two international competitions: the ICFHR2014
Handwritten Keyword Spotting Competition16, and the ICDAR2015 Competition on
Keyword Spotting for Handwritten Documents17. The later include a (main) track for
QbS KWS, which basically matches the main developments and aims of this book.
The former was explicitly stated only as QbE KWS, so we will present comparative
experiments using our QbE-based-on-QbS approach presented in Sec. 3.6. While
QbE is not the main focus of this book, these experiments are interesting to show
the flexibility and power of the proposed general formulation of PrIx.
The QbS experiments, comparable with ICDAR2015, are presented first, followed
by those of QbE comparable with ICFHR2014.
16 http://vc.ee.duth.gr/H-KWS2014/
17 http://transcriptorium.eu/˜icdar15kws/172 6 Empirical Validation of Probabilistic Indexing Methods
6.10.1 ICDAR2015 Competition on Handwriting KWS
The ICDAR2015 KWS competition [21] was organized in order to provide a bench￾mark to fairly compare different KWS approaches. Two tracks were created: a
training-free (for systems that do not need labeled training data), and a training￾based. In the training-based track, two sub-tracks were created: one following the
QbS paradigm and the other following the QbE. Here we focus on training-based
QbS, with the Bentham dataset referred to as BEN3 in Table 6.1.
In a segmentation-free scenario, the systems must provide a bounding box with
the specific localization of each spotted keyword in each page. Thus, an overlap
measure was used to decide whether a given bounding box is sufficiently close to a
reference one. In this competition, bounding-box overlap was measured by the so￾called intersection over the union ratio, defined as (퐴 ∩ 퐵) / (퐴 ∪ 퐵), where 퐴 and
퐵 are the areas of the reference and spotted bounding boxes, respectively. A match
is considered correct (hit) if the spotted word is correct and the overlapping ratio is
greater or equal than 0.7. In our experiments, the evaluation software provided by
the competition was used to assess bounding-box hypotheses and to measure gAP
and mAP. Evaluation was based on a very small set of 243 predefined query words.
Since the data used in this competition (BEN3) is based on the Bentham collec￾tion, we used the same CRNN architecture and the same training procedure for the
CRNN and the character LM.
To determine the spot bounding-boxes needed for evaluation, we first automati￾cally segmented the test pages into text lines, using a very simple Transkribus tool.18
Then, a lattice was generated for each text line, from which a segment-level proba￾bilistic index was created, as described in Sec. 5.6.1. Using the text line coordinates
obtained from the automatic segmentation and the indexed segments, the bound￾ing boxes of each spot (hypothesized pseudo-word) of each whole test page were
straightforwardly obtained. Finally, these bounding boxes were adequately trimmed
out using simple geometrical heuristics.
Table 6.18 shows the mAP and gAP19 of our approach, compared to the best
results of the two teams that participated in the training-based track.
Table 6.18: Comparison of various systems in the ICDAR2015 KWS Competition, based on
the BEN3 Bentham dataset. Results of gAP and mAP (in %) are given for bounding-box
overlap not higher than 0.7.
Query-by-String
mAP gAP
Team 1 85.3 87.1
Team 2 18.2 38.2
Ours 86.4 91.5
18 The one freely available at that time from: https://transkribus.eu
19 The gAP was not originally reported in the competition, but we report it here, since we had
access to the submissions.6.10 Segmentation-free Evaluation 173
“Team 1” used a 2D-LSTM approach fairly similar to BLSTM-CTC, described
in Sec. 7.5. “Team 2” used a combination of multiple BLSTM networks, with hand￾crafted features, and a KWS method similar to the HMM-Filler described in Sec. 7.4.
The lower performance achieved by “Team 2” is very likely due to problems
recovering accurate word bounding boxes, and due to the fact that they used hand￾crafted features to feed their neural networks. Our performance was somewhat better
than that of “Team 1” (the winner). Indeed, according to their description, they follow
an approach very similar to the BLSTM-CTC, but multiple segments were retrieved
per text line (up to 4). As we explain in Sec. 7.5, the BLSTM-CTC approach can be
interpreted under our probabilistic framework and it uses an optical model similar
to our CRNN model, thus it is no surprise that its performance approached ours.
As compared with most classical KWS methods, which perform all the computa￾tions at query time, our approach first builds a PrIx of (pseudo-)words that are likely
enough to be written in the test images — and this is done without knowing the query
set. So, at query time all the hard work has been done and it is thus dramatically
faster (almost instant response to any query).
In summary, it has been shown that PrIxs can be easily used in segmentation￾free scenarios, just by automatically detecting the text lines. This is crucial for real
applications, where exactly segmented text lines (or words) are never available. In
Chapter 10 we will report results of successful applications of this procedure to
large-scale collections (with several tens or hundreds of thousands of pages).
6.10.2 ICFHR2014 Competition on QbE Handwriting KWS
The ICFHR-2014 KWS competition [18] was based on the BEN2 Bentham dataset
and it only considered QbE KWS with a small query set of 290 keyword images, of
41 different words (see Tables 6.1 and C.8).
The aim was to validate empirically the statistical QbE formulation presented in
Sec. 3.6, with a CRNN optical model and a (lexicon-free) character 푛-gram LM,
using automatically segmented text line images. The results presented in this section
complement those published in [34], where GMM-HMM optical models, and a
(lexicon-based) 2-gram word LM were used.
Because no training data was provided in the original competition, we trained
our models on the reference data provided in the ICFHR2014 Competition on Hand￾written Text Recognition on tranScriptorium Datasets [26], also based on Bentham
images (350 page images for training and 50 for validation), from which we excluded
50 pages which were also used in the test set of the KWS competition. For more
detail about the dataset and query set refer to Appendix C.2.3. No additional training
data, nor data augmentation, was used to estimate any of the statistical models.
To assess the results of these experiments, the evaluation software provided by
the competition organizers was used. In contrast with most of the experiments in
this book, it did not use interpolated precision or the trapezoid integration method
to compute the Average Precision; and it only reported mAP (not gAP).174 6 Empirical Validation of Probabilistic Indexing Methods
Since we are in a segmentation-free scenario, the correctness of the spot bounding
boxes has to be assessed. The organizers of the competition adopted an overlapping
area ratio defined as (퐴 ∩ 퐵) / 퐴, where 퐴 and 퐵 are the reference and the detected
bounding boxes, respectively. A match is considered correct (hit) if the spotted word
is correct and the overlapping area ratio is greater than 0.7.20
The details about automatic text line segmentation can be seen in [34] and the
architecture of the CRNN model used in this experiment is described in Table 6.19.
The CRNN was trained for about 28k parameter updates, using RMSProp with a
5 · 10−4
learning rate and a batch size of 16. In addition, a 7-gram character LM was
trained from the reference transcripts of the training images.
Table 6.19: CRNN Architecture used in the ICFHR2014 Handwriting KWS Competition.
Block Configuration Values
Convolutional Number of layers 4
Activation ReLU
Filters {12, 24, 48, 48}
Filter sizes {7, 5, 3, 3}
Max. pooling {2, 2, 2, 0}
Batch normalization no
Recurrent Number of layers 3
Type BLSTM
Units {256, 256, 256}
Dropout {0.5, 0.5, 0.5}
Output layer Units (characters plus ∅) 62
Dropout 0.5
The relevant spots for each query image 푦 were obtained following the approach
described in Sec. 3.6 (Eq. 3.41). It combines a pre-computed segment-oriented PrIx
of the test images with the word posteriors 푃(푣 | 푦) for all indexed pseudo-words 푣.
The PrIx was built using the Alg. 5.13 described in Sec. 5.6.1.
The computation cost involved in this approach, can be reduced if the sum in 푣 of
Eq. (3.41) is limited to the set of the 푚 transcripts 푣 of a query image 푦 with highest
posterior probability, 푃(푣 | 푦). Moreover, it was observed that this can also (slightly)
improve performance in some cases. Therefore, a value of 푚 = 5 was chosen to
perform all the experiments. On the other hand, rather than using a line-level LM to
compute 푃(푣 | 푦), a character 7-gram for individual words was used.
Results are shown in Table 6.20. The four top rows reproduce the official score￾board for the segmentation-free task of this competition, and the row at the bottom
shows the results of our PrIx-based QbE approach. In addition to the mAP, other
performance metrics are also shown: the precision at 5 (P@5), and two normalized
20 We were aware that this measure can be easily fooled by producing spots encompassing the
whole page image. However, for the sake of fair comparison, we did not take advantage of this
shortcoming of the evaluation protocol.6.10 Segmentation-free Evaluation 175
discounted cumulative gain measures, one assuming a binary relevance judgment
(bNDCG) and the other assuming non-binary judgment (NDCG). Further details
about these evaluation measures are explained in the competition report [18].
Table 6.20: Comparison of multiple systems submitted to the original competition (“Team
1”, “Team 3”, . . . ) and our system, based on automatic text line segmentation and the PrIx
approach described in this book using a CRNN optical model and a 7-gram character LM.
P@5 bNDCG NDCG mAP
Team 1 61.1 64.0 65.7 41.9
Team 3 56.8 51.8 53.6 37.2
Team 4 34.1 36.3 37.6 20.9
Team 5 55.0 51.3 53.1 34.7
CRNN + char 7-gram 96.0 92.2 91.9 87.3
These results show a dramatic superiority of our probabilistic approach. For the
sake of illustration, Fig. 6.14 shows some examples of words spotted in one of the
competition test pages for a query image of the word “offence”. It is worth mentioning
that no further trimming was applied in this case to improve the bounding boxes of
the automatically obtained PrIx spots.
Finally, we must highlight that this particular competition violates the definition
of binary relevance that adopted in this book. In particular, they consider as relevant
some word regions of the evaluation pages that contain different words than that
written in the given query image. For example, for a query image with the text
“possess” written in it, some text regions in the test pages are considered relevant,
even though the text actually written in these are words like “possesst”, “possessed”
or “possession”.
Although one of the assumptions in our approach is clearly violated, our proba￾bilistic perspective still outperforms all the other (word-segmented distance-based)
heuristic solutions, by a significant margin.176 6 Empirical Validation of Probabilistic Indexing Methods
(a)
(b)
Fig. 6.14: A query image of the ICFHR2014 H-KWS Competition (a), and a fragment of
a test page (b) where multiple instances of this query were spotted by the proposed QbE
approach based on automatic text line segmentation and PrIx QbS.6.11 Summary 177
6.11 Summary
The main results of the experiments reported in this chapter are summarized here.
They answer each of the questions posed at the beginning of the chapter.
1. In Sec. 6.2 we show that the posteriorgram-based approach for computing the
relevance probability 푃(푅 | 푥, 푣) (Eq. 3.19 or 3.15) produces accurate results
at reasonable cost with respect to the other formulations proposed in Secs. 3.4.
2. Experiments in Sec. 6.3 show that line-region RPs can be effectively and effi￾ciently approximated by segment or ordinal position RPs. Algorithms described
in Sec. 5 can be used to construct the corresponding PrIxs in cost-effective way,
both for lexicon-based and lexicon-free models.
3. Sec. 6.4 argues that lexicon-based approaches, although usually faster than
lexicon-free, can produce worse results if they are not trained with a large lex￾icon. Thus, in real applications, we recommend using lexicon-free approaches
with a sufficiently large order 푛-gram language model (e.g. 푛 ∈ [6, 8]).
4. In Sec. 6.5 we saw that the number of labeled samples that are needed to train the
used neural networks is not excessively high. In fact, for the data set considered,
and using synthetic data augmentation, we got the same performance using only
about half the available training data.
5. As shown in Sec. 6.6, there is a clear linear correlation between HTR and PrIx
performances, in particular between WER and gAP. This allows alleviating
empirical costs since HTR experiments are much faster to perform, and we
know that WER improvements will faithfully translate into gAP (and into mAP
to a lesser extent).
6. In Sec. 6.7, we show that our approach produces state-of-the-art results for
several line-level KWS benchmarks, widely used in previous academic works.
7. Sec. 6.8, shows that more traditional HTR models, namely HMMs, can also be
effectively used in our framework. With well segmented text lines, they provide
PrIx performance close to that achieved using CRNN models. However we
also argued thatCRNNs are generally more robust and less sensitive to line
segmentation errors. Thus, we recommend CRNN optical models for real,
large-scale applications like those described in Chapter 10.
8. We showed in Sec. 6.10.1 that segmentation-free KWS can be easily achieved
by simply computing conventional PrIxs of automatically detected text line
images. This simple approach provides excellent results on the data sets of two
different competitions, using different methods for text line segmentation.
9. Finally, the experiments presented in Sec. 6.10.2, clearly show that conven￾tional PrIxs can be easily used, as proposed in Sec. 3.6, to perform traditional
segmentation-free QbE KWS– with a dramatic superiority over previous meth￾ods specifically developed for the QbE KWS task.178 6 Empirical Validation of Probabilistic Indexing Methods
References
1. B., Bluche, T., Knibbe, M., Benzeghiba, M.F., Messina, R., Louradour, J., Kermorvant, C.:
The A2iA Multi-lingual Text Recognition System at the Second Maurdor Evaluation. In:
14th International Conference on Frontiers in Handwriting Recognition, pp. 297–302 (2014).
DOI 10.1109/ICFHR.2014.57
2. Bergstra, J., Bardenet, R., Bengio, Y., Kegl, B.: Algorithms for Hyper-parameter Optimiza- ´
tion. In: Proceedings of the 24th International Conference on Neural Information Processing
Systems, NIPS’11, pp. 2546–2554. Curran Associates Inc., USA (2011)
3. Bluche, T., Hamel, S., Kermorvant, C., Puigcerver, J., Stutzmann, D., Toselli, A.H., Vidal,
E.: Preparatory KWS Experiments for Large-Scale Indexing of a Vast Medieval Manuscript
Collection in the HIMANIS Project. In: Proc. of 14th ICDAR (2017)
4. Bluche, T., Messina, R.: Gated Convolutional Recurrent Neural Networks for Multilingual
Handwriting Recognition. In: 14th IAPR International Conference on Document Analysis and
Recognition (ICDAR), vol. 01, pp. 646–651 (2017). DOI 10.1109/ICDAR.2017.111
5. Doetsch, P., Kozielski, M., Ney, H.: Fast and Robust Training of Recurrent Neural Networks
for Offline Handwriting Recognition. In: 2014 14th International Conference on Frontiers in
Handwriting Recognition, pp. 279–284 (2014). DOI 10.1109/ICFHR.2014.54
6. Federico, M., Bertoldi, N., Cettolo, M.: Irstlm: an open source toolkit for handling large scale
language models. In: INTERSPEECH, pp. 1618–1621. ISCA (2008)
7. Fischer, A., Frinken, V., Bunke, H., Suen, C.Y.: Improving HMM-Based Keyword Spotting
with Character Language Models. In: 2013 12th International Conference on Document
Analysis and Recognition, pp. 506–510 (2013). DOI 10.1109/ICDAR.2013.107
8. Fischer, A., Keller, A., Frinken, V., Bunke, H.: Lexicon-free handwritten word spotting using
character HMMs. Pattern Recognition Letters 33(7), 934 – 942 (2012). DOI 10.1016/j.patrec
.2011.09.009. Special Issue on Awards from ICPR 2010
9. Frinken, V., Fischer, A., Manmatha, R., Bunke, H.: A Novel Word Spotting Method Based on
Recurrent Neural Networks. IEEE Transactions on Pattern Analysis and Machine Intelligence
34(2), 211–224 (2012). DOI 10.1109/TPAMI.2011.113
10. Graves, A., Fernandez, S., Gomez, F., Schmidhuber, J.: Connectionist Temporal Classification: ´
Labelling Unsegmented Sequence Data with Recurrent Neural Networks. In: Proceedings of
the 23rd International Conference on Machine Learning, ICML ’06, pp. 369–376. ACM, New
York, NY, USA (2006). DOI 10.1145/1143844.1143891
11. Gr ¨uning, T., Leifert, G., Strauß, T., Michael, J., Labahn, R.: A two-stage method for text line
detection in historical documents. International Journal on Document Analysis and Recognition
(IJDAR) 22(3), 285–302 (2019)
12. Ioffe, S., Szegedy, C.: Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift. In: F. Bach, D. Blei (eds.) Proceedings of the 32nd International
Conference on Machine Learning, vol. 37, pp. 448–456. PMLR, Lille, France (2015)
13. Kneser, R., Ney, H.: Improved backing-off for M-gram language modeling. In: 1995 Interna￾tional Conference on Acoustics, Speech, and Signal Processing, vol. 1, pp. 181–184 (1995).
DOI 10.1109/ICASSP.1995.479394
14. Kumar, G., Govindaraju, V.: A Bayesian Approach to Script Independent Multilingual Keyword
Spotting. In: 2014 14th International Conference on Frontiers in Handwriting Recognition,
pp. 357–362 (2014). DOI 10.1109/ICFHR.2014.66
15. Lang, E., Puigcerver, J., Toselli, A.H., Vidal, E.: Probabilistic indexing and search for in￾formation extraction on handwritten german parish records. In: 2018 16th International
Conference on Frontiers in Handwriting Recognition (ICFHR), pp. 44–49 (2018). DOI
10.1109/ICFHR-2018.2018.00017
16. Manning, C.D., Raghavan, P., Sch ¨utze, H.: Introduction to Information Retrieval. Cambridge
University Press, New York, NY, USA (2008)
17. Povey, D., Ghoshal, A., Boulianne, G., Burget, L., Glembek, O., Goel, N., Hannemann, M.,
Motlicek, P., Qian, Y., Schwarz, P., Silovsky, J., Stemmer, G., Vesely, K.: The Kaldi SpeechReferences 179
Recognition Toolkit. In: IEEE 2011 Workshop on Automatic Speech Recognition and Under￾standing. IEEE Signal Proc. Society (2011). IEEE Catalog No.: CFP11SRW-USB
18. Pratikakis, I., Zagoris, K., Gatos, B., Louloudis, G., Stamatopoulos, N.: ICFHR 2014 Competi￾tion on Handwritten Keyword Spotting (H-KWS 2014). In: 2014 14th International Conference
on Frontiers in Handwriting Recognition, pp. 814–819 (2014). DOI 10.1109/ICFHR.2014.142
19. Puigcerver, J.: Are Multidimensional Recurrent Layers Really Necessary for Handwritten
Text Recognition? In: 2017 14th IAPR International Conference on Document Analysis and
Recognition (ICDAR), vol. 01, pp. 67–72 (2017). DOI 10.1109/ICDAR.2017.20
20. Puigcerver, J.: A probabilistic formulation of keyword spotting. Ph.D. thesis, Universitat
Politecnica de Val ` encia (2018) `
21. Puigcerver, J., Toselli, A.H., Vidal, E.: ICDAR2015 Competition on Keyword Spotting for
Handwritten Documents. In: 2015 13th International Conference on Document Analysis and
Recognition (ICDAR), pp. 1176–1180 (2015). DOI 10.1109/ICDAR.2015.7333946
22. Quiros, L.: Multi-task handwritten document layout analysis. Preprint arXiv:1806.08852 ´
(2018)
23. Sauvola, J., Pietikainen, M.: Adaptive document image binarization. Pattern Recognition ¨ 33,
225–236 (2000)
24. Stolcke, A.: Srilm - an extensible language modeling toolkit. In: INTERSPEECH. ISCA (2002)
25. Sudholt, S., Fink, G.A.: Attribute CNNs for word spotting in handwritten documents. Interna￾tional Journal on Document Analysis and Recognition (IJDAR) 21(3), 199–218 (2018). DOI
10.1007/s10032-018-0295-0. URL https://doi.org/10.1007/s10032-018-0295-0
26. Sanc ´ hez, J.A., Romero, V., Toselli, A.H., Vidal, E.: ICFHR2014 Competition on Hand￾written Text Recognition on Transcriptorium Datasets (HTRtS). In: 2014 14th Interna￾tional Conference on Frontiers in Handwriting Recognition, pp. 785–790 (2014). DOI
10.1109/ICFHR.2014.137
27. Terasawa, K., Tanaka, Y.: Slit Style HOG Feature for Document Image Word Spotting. In: 2009
10th International Conference on Document Analysis and Recognition, pp. 116–120 (2009).
DOI 10.1109/ICDAR.2009.118
28. Tieleman, T., Hinton, G.: Lecture 6.5-RMSprop: Divide the gradient by a running average of
its recent magnitude. COURSERA: Neural networks for machine learning 4(2) (2012)
29. Toselli, A.H., Leiva, L.A., Bordes-Cabrera, I., Hernandez-Tornero, C., Bosch, V., Vidal, E.: ´
Transcribing a 17th-century botanical manuscript: Longitudinal evaluation of document layout
detection and interactive transcription. Digital Scholarship in the Humanities 33(1), 173–202
(2018). DOI 10.1093/llc/fqw064. URL http://dx.doi.org/10.1093/llc/fqw064
30. Toselli, A.H., Romero, V., Sanchez, ´ J.A., Vidal, E.: Making two vast historical manuscript col￾lections searchable and extracting meaningful textual features through large-scale probabilistic
indexing. In: Int. Conf. on Document Anal. and Recogn. (ICDAR), pp. 108–113. IEEE (2019)
31. Toselli, A.H., Vidal, E.: Handwritten text recognition results on the Bentham collection with
improved classical N-gram-HMM methods. In: Proceedings of the 3rd International Workshop
on Historical Document Imaging and Processing, pp. 15–22. ACM (2015)
32. Toselli, A.H., Vidal, E., Romero, V., Frinken, V.: HMM Word-Graph Based Keyword Spotting
in Handwritten Document Images. Information Sciences 370(C), 497–518 (2016). DOI
10.1016/j.ins.2016.07.063
33. Vidal, E., Romero, V., Toselli, A.H., Sanchez, J.A., Bosch, V., Quir ´ os, L., Bened ´ ´ı, J.M.,
Prieto, J.R., Pastor, M., Casacuberta, F., et al.: The carabela project and manuscript collection:
large-scale probabilistic indexing and content-based classification. In: 2020 17th International
Conference on Frontiers in Handwriting Recognition (ICFHR), pp. 85–90. IEEE (2020)
34. Vidal, E., Toselli, A.H., Puigcerver, J.: High performance Query-by-Example keyword spotting
using Query-by-String techniques. In: 2015 13th International Conference on Document
Analysis and Recognition (ICDAR), pp. 741–745 (2015). DOI 10.1109/ICDAR.2015.7333860
35. Vidal, E., Toselli, A.H., Puigcerver, J.: A probabilistic framework for lexicon-based keyword
spotting in handwritten text images. Tech. Rep. arXiv:2104.04556 (2017–2021)
36. Vidal, E., Toselli, A.H., Puigcerver, J.: Lexicon-based probabilistic indexing of handwritten
text images. Neural Computing and Applications pp. 1–20 (2023)180 6 Empirical Validation of Probabilistic Indexing Methods
37. Villegas, M., Puigcerver, J., Toselli, A.H., Sanchez, J.A., Vidal, E.: Overview of the ImageCLEF ´
2016 Handwritten Scanned Document Retrieval Task. In: CLEF (Working Notes), pp. 233–253
(2016)
38. Villegas, M., Romero, V., Sanchez, J.A.: On the modification of binarization algorithms to ´
retain grayscale information for handwritten text recognition. In: Pattern Recognition and
Image Analysis: 7th Iberian Conference, IbPRIA 2015, Santiago de Compostela, Spain, June
17-19, 2015, Proceedings, pp. 208–215 (2015)
39. Voigtlaender, P., Doetsch, P., Ney, H.: Handwriting Recognition with Large Multidimensional
Long Short- Term Memory Recurrent Neural Networks. In: 15th Int. Conf. on Frontiers in
Handwriting Recognition (ICFHR), pp. 228–233 (2016). DOI 10.1109/ICFHR.2016.0052
40. Wicht, B., Fischer, A., Hennebert, J.: Deep learning features for handwritten keyword spotting.
In: 2016 23rd International Conference on Pattern Recognition (ICPR), pp. 3434–3439 (2016).
DOI 10.1109/ICPR.2016.7900165
41. Wicht, B., Fischer, A., Hennebert, J.: Keyword Spotting with Convolutional Deep Belief
Networks and Dynamic Time Warping. In: A.E. Villa, P. Masulli, A.J. Pons Rivero (eds.)
Artificial Neural Networks and Machine Learning – ICANN 2016, pp. 113–120. Springer
International Publishing, Cham (2016)
42. Wshah, S., Kumar, G., Govindaraju, V.: Statistical script independent word spotting in offline
handwritten documents. Pattern Recognition 47(3), 1039–1050 (2014). DOI https://doi.org/
10.1016/j.patcog.2013.09.019
43. Young, S., Evermann, G., Gales, M., Hain, T., Kershaw, D., Liu, X., Moore, G., Odell, J.,
Ollason, D., Povey, D.: The HTK book. Tech. rep., Cambridge Univ. Engineering Dep. (2002)Chapter 7
Probabilistic Interpretation of Traditional KWS
Approaches
AbstractIn this chapter traditional ideas and approaches for KWS are reviewed under
the probabilistic framework proposed in Chapter 3. Both QbE and QbS methods are
covered. In particular, we study schemes and techniques which have long been
considered among the most successful. This includes QbE Distance- and PHOC￾based methods, as well QbS HMM-Filler and BLSTM-CTC. Formal presentation
and developments are accompanied by simple illustrative examples and/or empirical
evaluations on real datasets.
7.1 On the Spotting Versus Recognition Debate
The concept of KWS was born around 1980 as a means to detect interesting words in
a stream of spoken utterances. More recently, the same concept has been adopted for
the detection of words in images of printed and handwritten documents. In both cases,
KWS soon went well beyond its original aim and it is now also seen as a means
to (probabilistically) index (all) the words of large collections of spoken, printed
or handwritten documents. Many researchers in document image processing have
historically claimed that KWS does not require any previous, explicit transcription
of documents into electronic text. Yet, a close relationship between plain (speech)
recognition and KWS eventually was commonly taken for granted in the field of
automatic speech recognition, mainly as a result of works driven by a series of NIST
competitions [14, 12, 6, 42].
In the field of document image processing, however, this relationship is still under
debate. A prevailing view is that KWS is just a retrieval problem, essentially unrelated
with image recognition, where only word image similarities matter. Another related
view is that, in case of limited (training) resources, it is better not to consider
KWS under the PR framework. This recognition-free viewpoint is clearly evinced
in sentences such as the one introducing a popular comprehensive survey on KWS
published not long ago [15]:
“[...] recognition-free retrieval, which is also known in the literature as word
spotting or keyword spotting, is the main subject of this study.”
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 181
A. H. Toselli et al., Probabilistic Indexing for Information Search and Retrieval 
in Large Collections of Handwritten Text Images, The Information Retrieval Series 49, 
https://doi.org/10.1007/978-3-031-55389-9_7
 182 7 Probabilistic Interpretation of Traditional KWS Approaches
In this chapter we instead argue that KWS does always more or less explicitly
entail some form of word recognition. By acknowledging this fact, we show that
standard PR and IR concepts and methods can advantageously be used to improve
KWS results. Moreover, this is also the case when (training) resources are limited or
not available.
We start discussing the most time-honored text image QbE methods, in which the
query is a word-sized image where an example of the keyword is written [15].
In the most popular KWS approaches of this kind, the text image collection where
keywords are to be searched for just consists of a collection of word-sized cropped
images, each one strictly containing a single word. This simplification is known
as word-segmentation-based KWS [15].1 The query image is generally represented
by a vector y ∈ R
퐷 (퐷-dimesional Euclidean space). Similarly, each word-cropped
collection image is also represented by a vector x ∈ R
퐷. Even though the actual word
written in an image does not explicitly matters in QbE KWS, words do implicitly serve
to identify (and cluster) word images. In fact QbE KWS performance is generally
evaluated using the words which label the images: a collection image is considered
to be relevant to a query image if both have the same label (word).
Under this category, we review classical distance-based methods in general and,
more specifically, the popular PHOC-based approach.
We show that the most usual formalizations and/or implementations of distance￾based methods exhibit two conceptual flaws which can be easily fixed when the
classical ideas are reformulated by acknowledging probabilistic and recognition￾based essential nature of the problem. By fixing these flaws, it is empirically shown
that important improvements are achieved.
Then the chapter focuses on two traditional approaches to word-segmentation-free
QbS KWS, known as HMM-Filler and BLSTM-CTC. To a large extent these methods
(HMM-Filler in particular) are reminiscent from similar proposals in the field of
speech processing. Both of these approaches are rather considered recognition￾based and they are much more closely related with the PrIx framework here proposed
(which explicitly assumes to be recognition-based). Once the relationship is formally
established, the advantages of fully adopting the PrIx viewpoint are shown both
conceptually and empirically.
7.2 Distance-based Methods
Many traditional approaches, predominantly in the QbE paradigm, interpret KWS as
a problem of finding images which are similar to the given query (image). In most
cases, fixed-size feature vectors are extracted from word-sized images (which have
been previously cropped, either manually, or automatically, or just using a suitable
1 It should be reminded, however, that this is a dramatic simplification of the original KWS concept,
where locating the positions of spotted words within unsegmented document images is perhaps the
most important and difficult part of the KWS problem.7.2 Distance-based Methods 183
sliding window approach). Then, these vectors are compared (using a suitable dis￾tance or dissimilarity measure) with another vector similarly obtained from the query
image (see [15] and e.g. [2, 26, 31]). Sometimes, word images are not represented
as single vectors, but as sequences of feature vector; in these cases distances over
sequences such as Dynamic Time Warping (DTW) are used (see e.g. [25, 27, 20, 21]).
This view of KWS is fairly common among researchers with a Computer Vision
background, perhaps because it is a predominant view in other related applications
such as Content-based Image Retrieval [41, 5, 33]. As commented in Sec. 7.1, many
of these researchers consider that their methods (based on well-designed heuristics)
are recognition-free, which purportedly has the advantage of being also completely
unsupervised or training-free.
Such a conception of KWS raises some concerns. First, one needs to establish a
(di-)similarity which adequately measures how close are two word images in terms
of their lexical meaning. Secondly, and most importantly, whether two images are
similar or dissimilar does not necessary imply that one is relevant or irrelevant
given the other as a query. In addition, while it may be accepted that some of these
methods do not perform explicit word recognition, they are still implicitly solving
a classification problem. Yet, as we show below, they are not solving the right
classification problem.
7.2.1 Simplifying QbE RP for Word-segmented Image Regions
As discussed in Chapter 3, the general probabilistic framework adopted in this
book mainly focus on indexing large collections of text images and, as such, it also
directly applies to QbS tasks of KWS. However, in Sec. 3.6 it was explained how the
proposed framework can be properly applied in the QbE paradigm. The main idea
was to consider that a query image is just the rendering of some unknown or “hidden”
word (understood as its textual representation). This approach was developed into
Eq. (3.38), through marginalization over all possible query words.
Now we are in a traditional word-segmented QbS KWS scenario, which entails
important simplifications. In particular, Eq. (3.38) takes an even simpler expression,
since now not only the query image 푦, but also the images to be queried, 푥, contain
a single word each. Recall from Sec. 3.3.1 (Eq. 3.7) that, if image regions are just
tightly cropped word bounding boxes, the RP 푃(푅 | 푥, 푣) is equivalent to the posterior
probability that the given keyword 푣 is the (only) word written in the cropped image
푥. Taking this into account, Eq. (3.38) becomes:
푃(푅 | 푥, 푦) =
Õ
푣
푃(푣 | 푦) 푃(푅 | 푥, 푣) =
Õ
푣
푃(푣 | 푦) 푃(푣 | 푥) (7.1)
It is worth reminding that, following decision theoretic results of Appendix A, if
푃(푣 | 푦) and 푃(푣 | 푥) are the true probability distributions, then ranking the collection
images according to this RP should provide optimal gAP and mAP results.184 7 Probabilistic Interpretation of Traditional KWS Approaches
7.2.2 Distance-based Density Estimation
The link between distances or, more generally, dissimilarity measures, and prob￾ability density functions is well established from the origins of PR. Any properly
defined dissimilarity (not strictly necessary a metric), 푑(·, ·), can be used to estimate
a density function 푝(풚) [32, 8, 4].
Here we adopt perhaps the most basic and simple approach, namely kernel-based
Parzen Windows:
푝(풚) =
1
푛 푏퐷
Õ
풙∈X
퐾

푑(풙, 풚)
푏

(7.2)
where X = {풙1, . . . , 풙푛} ⊂ R
퐷 is a set of 푛 prototype vectors, 퐾 is a kernel function2,
and 푏 ∈ R
>0
is a smoothing parameter called window bandwidth. Similarly, if
C = {1, . . . , 푚} is a set of 푚 classes, a class-conditional density can be estimated as:
푝(풚 | 푐) =
1
푛푐 푏퐷
Õ
풙∈X푐
퐾

푑(풙, 풚)
푏

, 1 ≤ 푐 ≤ 푚 (7.3)
where X푐 is the subset of prototypes of class 푐 and 푛푐 = |X푐 |.
Now, following the classical formulation of KWS, suppose that we have some
query keyword, represented by its feature vector 풚, and we have a collection of images
풙1, . . . , 풙푛 that we want to rank by similarity. These images typically correspond to
a set of 푚 < 푛 different words, which are considered to be the classes of C.
To cope with the simplest case of QbS KWS, we can stretch the Parzen Windows
density estimation to the limit and suppose that there are as many classes as collection
images (i.e. 푚 = 푛). In this case, the conditional density can be written as:
푝(풚 | 퐶 = 푖) =
1
푏퐷
퐾

푑(풙푖
, 풚)
푏

, 1 ≤ 푖 ≤ 푛 (7.4)
The class-conditional density of Eq. (7.4) would correspond to a generative model
where a query is assumedly generated from the distribution defined by just one of
the collection images. That is, each query image is considered as a version of a
certain word-cropped collection image, distorted according to the given vector image
representation, dissimilarity measure, kernel function and bandwidth parameter.
Typically, the kernel function 퐾 is monotonically decreasing with increasing
dissimilarity. If this is the case, for a given query 풚, sorting the collection images
in descending order of density, or in increasing order of dissimilarity, will produce
exactly the same ranking. More or less explicitly, this fact has actually driven perhaps
the largest amount of research work in the KWS community. However, this rather
naive idea has two fundamental drawbacks, that we have coined the multi-variance
and multi-mode problems.
2 A non-negative real-valued integrable function which integrates to 1.7.2 Distance-based Methods 185
7.2.2.1 The Multi-variance Problem
The first problem is due to ranking the collection images by simply using the raw
distances (or some monotonically equivalent function, like the density described
above). Observe that some subsets of collection images may have much larger vari￾ances than others. As a typical case, this happens with subsets of images of different
words – some words tend to be written very regularly across the collection, while
others may exhibit large variability.
While such variance differences do not impact mAP results, they becomes very
problematic when, instead of measuring retrieval performance of individual queries,
it is measured globally (i.e., with gAP), for a set of queries. In this case, some
dissimilar pairs will be ranked on top of other similar pairs just because the class of
the similar pairs happens to have a large variance. Thus, we refer to this issue as the
multi-variance problem.
A direct way to address this problem would be to figure out some technique to
estimate and use a different bandwidth parameter of Eq. (7.4) for each word. For
instance, for a Gaussian kernel, this parameter would be the Gaussian standard
deviation which could be easily estimated if some (training) labeled word images
were available. Nevertheless, we will consider here a less direct approach which does
not need any labeled image.
Fig. 7.1 (a) shows an illustrative instance of this problem. There are samples from
two classes (unknown by the KWS system): “red-square” and “blue-circle”. Col￾lection images are represented as dots labeled as 풙, and queries as 풚. The class
“red-square” has a much larger variance than the class blue-circle, hence the Eu￾clidean distance tends to be larger for the query 풚2 than for 풚1. Therefore, for the
raw negative distances, the pair (풙2, 풚1) is better ranked than (풙2, 풚2), resulting in a
relatively low gAP. As commented above, exactly the same result is obtained if the
density given by Eq. (7.4) is used instead of the raw negative distance.
As a reasonable alternative, one can compute the class-posterior of the collection
images given the query image:
푃(퐶 = 푖 | 풚) =
푃(풚 | 퐶 = 푖) 푃(푐)
Í푛
푖
′=1 푃(풚 | 푖
′
) 푃(퐶 = 푖
′
)
, 1 ≤ 푖 ≤ 푛 (7.5)
Now, let’s assume that the prior 푃(푐) is uniform and let a Gaussian kernel be used
for density estimation in Eq. (7.4), where we set 푠 = 푏
−1
. Then, Eq. (7.5) becomes:
푃(퐶 = 푖 | 풚) =
exp(−푠 푑2
(풙푖
, 풚))
Í푛
푖
′=1
exp(−푠 푑2
(풙푖
′ , 풚))
, 1 ≤ 푖 ≤ 푛 (7.6)
Since the denominator is independent of the class (퐶 = 푖, i.e., the word rendered in
the image 풙푖), for a fixed query 풚, Eq. (7.6) is proportional to exp(−푠 푑
2
(풙푖
, 풚)), and
therefore monotonous with −푑(풙푖
, 풚). Therefore, the mAP achieved using Eq. (7.6)
for any set of queries will be identical to that obtained using traditional distance￾based methods. However, the normalization of Eq. (7.6) provides a probabilistically
sound KWS score, which is bounded in [0, 1] and uniformly comparable across186 7 Probabilistic Interpretation of Traditional KWS Approaches
different queries. As illustrated in Fig. 7.1 (a) and in the experiments in Sec. 7.2.3,
this normalization entails significant gAP improvements with respect to using the
raw negative distance score.
풙1
풙2
풚1
풚2
mAP gAP
−푑(풙푐 , 풚 푗 ) 100% 83%
푃(퐶 =푖 | 풚 푗 ) 100% 100%
(a)
0
휋
4
휋
2
3휋
4
휋
5휋
4
3휋
2
7휋
4
0 1
풙1
풙2
풚1
풚2
(b)
Fig. 7.1: The color and shape of each point represent its true class (word), the letters 풙
and 풚 represent collection and query images, respectively. Panel (a) shows an instance of
the multi-variance problem of traditional distance-based KWS. The negative Euclidean
distance scores are: −푑(풙1, 풚1 ) = −1, −푑(풙2, 풚1 ) = −2, −푑(풙2, 풚2 ) = −4, −푑(풙1, 풚2 ) =
−5. Conversely, Eq. (7.6) (with 푠 = 1) yields: 푃(퐶 = 2 | 풚2 ) = 0.9991, 푃(퐶 = 1 | 풚1 ) =
0.9526, 푃(퐶 = 2 | 풚1 ) = 0.9526, 푃(퐶 = 1 | 풚2 ) = 0.0009. The gAP is 83% using raw
distances and 100% using class-posteriors. The mAP is identical in both cases because
they are monotonically related. Panel (b) shows an instance of the same problem in a
unit norm space. Similarly to (a), the class “red-square” has a much larger variance than
the class “blue-circle” and the Euclidean distances tend to be larger for the former class
than the latter. Therefore, the pair of (collection-image, query) samples (풙2, 풚1 ) is ranked
higher than (풙2, 풚2 ), resulting in a drop of the gAP.
As already mentioned, there are countless types of kernels. However, all our
experiments suggest that the choice of the kernel (with the required properties) is
not critical at all in order to achieve decent results as those reported in Sec. 7.2.3).
Finally, the reader may believe that normalizing the feature vectors would mitigate
the multi-variance problem. For instance, many authors use unit feature vectors
(e.g. [1, 31]). This limits the maximum distance between two vectors on the unit
hypersphere (for any norm, the distance between two unit vectors is always ≤ 2), but
the multi-variance effect can also appear, as shown in Fig. 7.1 (b). One can also see
in this example that other popular measures for unit-normed spaces, like the cosine
similarity, will also suffer exactly from the same problem. Thus, the multi-variance
problem is not due to a particular feature space or dissimilarity measure.
We could try to come up with some feature representation and dissimilarity mea￾sure that does not present this problem. However, it is not clear that such requirement
can be easily guaranteed across different data sets (languages, scripts, ages, etc.),
especially using unsupervised methods for feature extraction, which is the usual case
in traditional QbE KWS publications.
On the other hand, using class-conditional posteriors obtained from distance￾based density estimates, seems likely to adequately solve the multi-variance problem.7.2 Distance-based Methods 187
7.2.2.2 The Multi-mode Problem
In any case, we still may face an additional problem, caused by the naive generative
model presented earlier (and implicitly assumed by most traditional KWS methods).
Using the former class-posterior probability (or the raw distances) to rank the
collection images may present the so-called multi-mode problem. Note that in reality
we rarely have as many classes as collection images; instead, several collection
images typically share the same class label (i.e. there are multiple instances of the
same word). When one (or several) of the density functions conditioned on a class
are multi-modal, a given query will be close to the word images of the same mode,
but it may be far away from samples of other modes of the same class; farther than
from samples of other classes. This may happen, for instance, if one is considering
a collection of documents with multiple writers: it is reasonable to think that, for
some word label 푣 the density 푝(푥 | 푣) will have several modes (one for each writer).
In the example depicted in Fig. 7.2, there are two classes (words): “blue-circle”
and “red-square”. There are two collection images, 풙1, 풙2 of the first class and one,
풙3, of the second. And we consider only two queries, 풚1, 풚2, both from the “blue￾circle” class. In addition, we have a set of three labeled (training) samples 풛1, 풛2, 풛3
aimed at estimating the class (i.e. word) posteriors using a distance-based method.
Observe that the class-conditional density 푝(풙 | “blue-circle”) is bi-modal. There￾fore, as we just mentioned, a query such as 풚1 is only close to images of the same
mode (풙1 in the example), but it is farther away from collection images of another
mode of the same class (풙2 in the example) than from those from other classes, like
풙3. In this situation, traditional distance-based KWS will incorrectly rank 풙3 before
풙2. All in all, only 83% mAP and the same gAP are achieved. Using Eq. (7.6) does
not alleviate the trouble either, yielding the same poor gAP and mAP.
풙1 풙2
풙3
풛1 풛2
풛3
풚1 풚2
mAP gAP
−푑(풙푖
, 풚 푗 ) 83% 83%
푃(퐶 =푖 | 풚 푗 ) 83% 83%
푃(푅 | 풙푖
, 풚 푗 ) 100% 100%
Fig. 7.2: An instance of the multi-mode problem of traditional distance-based KWS (colors
and shapes are the same as in Fig. 7.1). Negative Euclidean distance scores: −푑(풙1, 풚1 ) =
−푑(풙2, 풚2 ) = −1, −푑(풙3, 풚1 ) = −푑(풙3, 풚2 ) = −
√
5, −푑(풙1, 풚2 ) = −푑(풙2, 풚1 ) = −3.
Posterior probabilities given by Eq. (7.6) (with 푠 = 1): 푃(퐶 = 1 | 풚1 ) = 푃(퐶 = 2 | 풚2 ) =
0.9817, 푃(퐶 =3 | 풚1 ) = 푃(퐶 =3 | 풚2 ) = 0.0180, 푃(퐶 =1 | 풚2 ) = 푃(퐶 =2 | 풚1 ) = 0.0003.
Relevance probabilities given by Eqs. (7.1) and (7.8) (with 푘 = 3, 푠 = 1): 푃(푅 | 풙1, 풚1 ) =
푃(푅 | 풙2, 풚2 ) = 푃(푅 | 풙1, 풚2 ) = 푃(푅 | 풙2, 풚1 ) = 0.9721, 푃(푅 | 풙3, 풚1 ) = 푃(푅 |
풙3, 풚2 ) = 0.0155. Ranking according to −푑(풙, 풚) or 푃(푐 | 풚) yields the same poor mAP
and gAP, but using 푃(푅 | 풙, 풚) does provide perfect results.188 7 Probabilistic Interpretation of Traditional KWS Approaches
It is worth noting that the posterior in Eq. (7.6) is not equivalent at all to the
(decision-theoretically optimal) RP of Eq. (7.1). According to this equation, the RP
of a collection image 풙 for a given query 풚 is the sum over all possible word labels
푣 of the product of the two word-posteriors 푃(푣 | 풙) and 푃(푣 | 풚).
Given that we have a few training samples, we are able to estimate these word
posteriors using the traditional 푘-nearest neighbor classifier. Let 풩(푘, 풙) and 풩(푘, 풚)
be the sets of 푘 training images which are nearest neighbors of 풙 and 풚, respectively,
according to the given distance 푑(·, ·). Let 풩(푘, 풙, 푣) and 풩(푘, 풚, 푣) be, respectively,
the subsets of images in 풩(푘, 풙) and 풩(푘, 풚) labeled with the word 푣 and let
푘 풙푣 = |풩(푘, 풙, 푣)|, 푘풚푣 = |풩(푘, 풚, 푣)| be the sizes of these sets. Then [7, 8]:
푃(푣 | 풙) =
푘 풙푣
푘
, 푃(푣 | 풚) =
푘풚푣
푘
(7.7)
These estimates are asymptotically optimal when 푛, 푘 → ∞, and 푘/푛 → 0, where
푛 is the number of total training samples [7]. However, to achieve best performance
in real, finite cases, a smoothed version of Eq. (7.7) is generally adopted to take into
account not only the number of neighbors, but also the actual distances involved.
To this end, we use the same Gaussian kernel as before, leading to the following
posterior probability estimates (which are identical to those in Eq. (7.7) for 푠 = 0):
푃(푣 | 풙) =
Í
풛∈풩(푘,풙,푣)
exp(−푠 푑2
(풛, 풙))
Í
풛
′∈풩(푘,풙)
exp(−푠 푑2
(풛
′
, 풙))
푃(푣 | 풚) =
Í
풛∈풩(푘,풚,푣)
exp(−푠 푑2
(풛, 풚))
Í
풛
′∈풩(푘,풚)
exp(−푠 푑2
(풛
′
, 풚))
(7.8)
This way, we are able to give a well-principled probabilistic approach to QbE
KWS, even using the original distances that were employed originally in a heuristic
manner. As a result, we overcome both the multi-variance and the multi-mode prob￾lems which plague traditional distance-based approaches to KWS. As we shall see
in Sec. 7.2.3, these problems are not mutually exclusive: a method can suffer from
both problems simultaneously.
The collection image posteriors that we proposed in Eq. (7.6) can mitigate the
multi-variance problem, but not the multi-mode one. However, the PrIx framework
presented in this book does provides a decision-theoretically optimal solution to both
problems – assuming the probabilities needed in Eq. (7.1) are properly estimated.
7.2.3 Interpretation of Distance-based KWS: Empirical Results
This section is devoted to empirically assess the probabilistic interpretations of
traditional distance-based methods presented above. A main purpose is to highlight
the aforementioned multi-variance and multi-mode problems of classical distance￾based QbE KWS and to show how these problems are overcome by assuming the
probabilistic, recognition-based viewpoints of our PrIx framework.7.2 Distance-based Methods 189
There are countless distance-based KWS works in the KWS literature. Unfor￾tunately, most of them do not share the source code of their methods. Hence, re￾implementing and exploring how the multi-variance and multi-mode problems affect
each of these works would be a hopeless task, out of the scope of this book. Thus,
we have chosen the work presented in [31] to illustrate the impact of these problems.
This work it is perfectly in line with distance-based KWS and, most importantly, the
source code of the reported experiments is publicly available in the Internet.3
The paper presents a QbE KWS system that operates on segmented words and
makes use of the so-called Zoning Aggregated Hypercolumn features, which is a
reasonably adequate fixed-dimension vectorial representation of a word image.
Each word image is partitioned into 6 vertical (overlapping) zones. Each zone
has the same height as the image and a variable width (which depends on the
configuration of the method). For each of these zones, pixel-level descriptors are
extracted using a convolutional network which was previously trained to perform
character recognition on “street-view” images (in particular, they use the network
from [17]). These descriptors, which are known as hypercolumns, are then aggregated
into a single vector per zone, and the vectors from all zones are concatenated to
produce a fixed-dimension, word-level feature vector. As a result, a vector of 1 536
features is obtained from each word image. These vectors are finally normalized
using the Euclidean norm. Fig. 7.3 describes this feature extraction work-flow.
To measure gAP and mAP, the Euclidean distance between the query image
feature vector and each of the collection-image vectors is computed, and collection
images are ranked according to increasing distance.
Fig. 7.3: Work-flow of the feature extraction process used to obtain Zoning Aggregated
Hypercolumn features. Figure kindly provided by Giorgos Sfikas.
3 https://github.com/sfikas/zah190 7 Probabilistic Interpretation of Traditional KWS Approaches
The experiments were carried out with the word-segmented George Washington
dataset. More specifically, the same (cross-validation) partition used in [3, 34] and
many others is adopted here. This dataset was chosen because both [31] and [34] used
it (and tuned their systems to work well on it). However, notice that the partition is not
exactly the same used in the original [31] work, since our evaluations sets contain
more queries. Thus our results do not exactly match those published in [31]. For
more details about the George Washington dataset and this partition in particular,
refer to Sec.C.3 of Appendix C.
We will use the gAP and mAP measures to compare three ranking alternatives.
In particular, we rank the 푛 collection images (i.e. all word-segmented images from
the test pages), represented by their feature vectors 풙푖
, 1 ≤ 푖 ≤ 푛, for each of the 푚
query images, 풚 푗
, 1 ≤ 푗 ≤ 푚, using these approaches:
1. Increasing order of the Euclidean distance ∥풙푖 − 풚 푗 ∥, as in the original work.
2. Decreasing order of the collection image posterior, 푃(퐶 =푖 | 풚 푗), as explained
in Sec. 7.2, and particularly using Eq. (7.6).
3. Decreasing order of the RP 푃(푅=1 | 풙푖
,풚 푗), using Eqs. (7.1) and (7.8).
Results
Both the collection image posteriors of Eq. (7.6) and the word posteriors of Eq. (7.8),
used to compute the RP, have a sharpness hyperparameter, 푠, that needs to be tuned.
Fig. 7.4 shows the mAP and gAP obtained for increasing sharpness values, using
each of the probabilistic approaches described before.
2
−1 2
1 2
3 2
5 2
7 2
9
0
20
40
60
80
100
Sharpness 푠
Average Precision (AP %)
푃(퐶 =푖 | 풚 푗 ), mAP
푃(퐶 =푖 | 풚 푗 ), gAP
푃(푅 | 풙푖
,풚 푗 ), mAP
푃(푅 | 풙푖
,풚 푗 ), gAP
Fig. 7.4: gAP and mAP for different values of the sharpness hyperparameter, 푠, used in
Eqs. (7.6) and (7.8) to estimate 푃(퐶 =푖 | 풚푗 ) and 푃(푅=1 | 풙푖
,풚푗 ), respectively.7.2 Distance-based Methods 191
In the case of the collection image posterior, 푃(퐶 = 푖 | 풚 푗), the mAP is always
67.1% because this parameter does not affect the relative order of the collection
images when compared to the same query image. In fact, this is the same mAP as
the one obtained by the original method (ranking in increasing Euclidean distance).
On the other hand, this parameter does affect both the mAP and the gAP when using
the RP 푃(푅 | 풙푖
,풚 푗). In order to compute the word posteriors of Eq. (7.8), involved
in the computation of the RP, we used all training examples available.
Given the results depicted in Fig. 7.4 we decided to use 푠 = 8 to compute
푃(퐶 =푖 | 풚 푗), and 푠=128 to compute (the word posteriors involved in) 푃(푅 | 풙푖
,풚 푗).
In Table 7.1 these results are compared with the original approach of simply ranking
according to the Euclidean distance. The table shows the mAP and gAP in each of
the cross-validation folds of the George Washington dataset, and their average.
Table 7.1: mAP and gAP achieved by different ranking strategies, based on the feature
vectors extracted by [31], on the word-segmented George Washington dataset. The results
are shown for each of the individual cross-validation folds, and the average.
Metric Order by CV1 CV2 CV3 CV4 Avg.
mAP (%)
− ∥ 풙푖 − 풚푗 ∥ 66.6 69.3 66.3 66.3 67.1
푃(퐶 =푖 | 풚푗 ) 66.6 69.3 66.3 66.3 67.1
푃(푅 | 풙푖
,풚푗 ) 89.0 90.5 89.0 87.4 89.0
gAP (%)
− ∥ 풙푖 − 풚푗 ∥ 32.3 29.0 28.0 28.7 29.2
푃(퐶 =푖 | 풚푗 ) 52.0 52.7 52.8 52.1 52.4
푃(푅 | 풙푖
,풚푗 ) 96.5 96.2 97.0 95.9 96.4
The results reported in Table 7.1 clearly corroborate the concerns raised in
Sec. 7.2: Plainly ranking the retrieved results according to some distance may not be
a good idea, since it is prone to both multi-variance and multi-mode problems.
The distances among all collection images can be normalized (as 푃(퐶 = 푖 | 풚 푗)
does) in order to solve the multi-variance issue. This does have a large impact on gAP
(but not on mAP, which remains unaffected). However, these results can be further
improved by actually computing a true RP (i.e. 푃(푅 | 풙푖
,풚 푗)) if some training labeled
word images are available. As we have seen during this book, ranking according to
the later probability (if well modeled) is the best strategy for the commonly adopted
mAP and gAP KWS performance measures.
Even though just a very simple (nearest-neighbors based) model of word posterior
probabilities have been used here, our probabilistic framework does reduce the gap
between classical, distance-based techniques and more advanced methods (such as
PHOC and CTC-based neural networks), as will be seen in the coming sections.
It is worth concluding that directly using other distances would probably not solve
the problem. In particular, as discussed in Sec. 7.2, and given that the feature vectors
used in the above experiments are unit normed, the popular cosine similarity would
have produced exactly the same mAP and gAP results.4
4 ∥ 풙 − 풚 ∥
2 = ∥ 풙∥
2 + ∥풚 ∥
2 − 2풙
⊺
풚 and ∥ 풙∥ = ∥풚 ∥ = 1 ⇒ ∥ 풙 − 풚 ∥
2 = 2 − 2풙
⊺
풚. Thus, ∥ 풙 − 풚 ∥
and 풙
⊺
풚 produce an equivalent (inverse) ranking, since one is a monotonic function of the other.192 7 Probabilistic Interpretation of Traditional KWS Approaches
7.3 PHOC-based Methods
A Pyramid of Histograms of Characters (PHOC) [3] is a hierarchical set of character
string binary histograms, where a histogram bin is “1” if the character is present in
the (sub)string and “0” if not. Here we prefer not to abuse the language and refer to
a binary histogram just as (a bit vector representation of) the set of characters (SoC)
which appear in the string. So the SoC of the string “acbbab” is {a,b,c} and that of
the string “a” is {a}. Let 푣 be a word and 푣푐 its character spelling. The first PHOC
level is the (single) SoC of 푣푐. At the 푙-th level, 푣푐 is split in 푙 disjoint substrings,
and a SoC is produced for each substring. Typically 푙 ≤ 5. Thus, if 푣푐 = “KOOKY”,
for example, its PHOC encompasses the SoCs shown in Fig. 7.5 (a).
For any word 푣 shorter than 6 characters, its highest PHOC level is a sequence
of SoCs containing at most one character each. So, the PHOC of 푣 is just 푣푐,
accompanied by superfluous information in the lower levels. For longer words, the
intrinsic redundancy of spelling in natural languages also results in last-level SoCs
which typically represent unique words. In any case, the lower-level SoCs contain
superfluous information. Fig. 7.5 (b) shows that such an (almost) strict uniqueness
of PHOC word representations actually happens in two of the most commonly used
data sets in KWS research (i.e. IAM and George Washington databases).
According to these observations, it can be asserted that the PHOC of a textual
word is just its character spelling and the PHOC of a word image (to be discussed
below) is essentially a character-level image transcript prediction.
Level Sets of characters (SoCs)
1 {K,O,Y}
2 {K,O} {K,O,Y}
3 {K,O} {O} {K,Y}
4 {K} {O} {K,O} {Y}
5 {K} {O} {O} {K} {Y}
(a)
1 2 3 4 5
0.1
1
10
100
148
14
0
0
0
3035
328
56
14
8
Pyramid Levels
% Words with shared PHOC
GW
IAM
(b)
Fig. 7.5: Panel (a): PHOC representation of the word “KOOKY” using a 5-level PHOC
(or sets of characters – SoCs). At the last level, since the length of the word is equal to
the number of SoCs, each set contains at most one character, thus the sequence of sets is
equivalent to the spelling of the word. Panel (b): percentage of words that share the PHOC
representation with other words in the word-segmented IAM and George Washington
(GW) data sets. For GW, a perfect one-to-one mapping is produced with only 3-level
PHOCs. For IAM, a perfect mapping would require 7 levels, but with 5-level PHOCs only
0.09% of the words share their PHOC representation. The exact number of words sharing
their PHOCs at each level is shown above the corresponding bar.7.3 PHOC-based Methods 193
7.3.1 Predicting the PHOC of a Word Image Region: PHOCNet
The PHOC representation of a (textual) word 푣 will be denoted as 풉(푣) ∈ B
퐷 where
퐷 is the total number of 0/1 components of all the PHOC “binary histograms” of 푣.
For a word image region 푥, let 푣(푥) be the single unknown word which is rendered
in 푥 and let 풉(푣(푥)) the PHOC of 푣(푥). Thus, a method is needed to predict (i.e.,
“recognize”) 풉(푥)
def
= 풉(푣(푥)), where the specific word involved becomes irrelevant.
To this end, Support Vector Machines (SVM) were originally adopted, using
Fisher feature vectors extracted from 푥. SVM outputs were then calibrated by means
of elaborate ad-hoc methods and interpreted as posterior probabilities [3]. The results
of [3] were later improved in [34, 35], using Convolutional Neural Networks (CNN)
to both extract deep image features and predict the PHOC components.
PHOCNet is trained to minimize the Binary Cross Entropy loss function, assum￾ing each PHOC component is independent of the others [34]. This assumption is
clearly dubious; in particular the low-level histograms are obviously correlated with
the upper level ones. Yet, the method still works very well in practice. For a given
input image 푥, a trained PHOCNet estimates the posterior probabilities:
푃(퐻푖 =1 | 푥)
def
= ℎˆ
푖(푥), 1 ≤ 푖 ≤ 퐷 (7.9)
where 퐻푖
is a binary random variable associated to the 푖-th PHOC component, ℎ푖(푥)
and ℎˆ
푖(푥) is the 푖-th component of the PHOCNet output vector, 풉ˆ (푥) ∈ R
퐷.
Clearly, because of the one-to-one PHOC–word correspondence that we have
asserted above, PHOCNet is essentially one more (among the many) proposed
character-based recognizers of isolated word images. However, it needs to predict
also the superfluous information in (the lower levels of) the PHOC. Therefore, we
argue that PHOCNet may be in disadvantage in terms of training demands with re￾spect to other less intricate, conventional text recognizers, which just have to predict
a sequential representation of 푣, namely its character spelling.
7.3.2 PHOC-based QbE KWS
For word-segmentation-based QbE KWS, 풉ˆ (푥) is obtained for each collection word
image 푥. Then, for a given query image 푦, 풉ˆ (푦) is similarly produced and the collec￾tion images are ranked according to the Bray–Curtis[34], or the cosine dissimilarities
between 풉ˆ (푥) and 풉ˆ (푦) [35].
Even if the PHOCNet approach can naturally be considered recognition-based, it
still overlooks the standard optimal-decision principles of PR and sticks to the most
traditional KWS view of just heuristically relying on dissimilarities between vector
representations of word images.
Of course, since PHOC KWS is just another distance-based method, the ideas
discussed in the previous section can be readily applied here to provide a proba￾bilistically sound use of the PHOC representation and dissimilarity. Nevertheless,
we explain below a more direct way to make the PHOC approach closer to the
decision-theoretically optimal QbE KWS framework introduced in Chapter 3.194 7 Probabilistic Interpretation of Traditional KWS Approaches
7.3.3 Probabilistic PHOCNet
As discussed earlier, an (almost) perfect one-to-one correspondence between words
푣 and their PHOC representations 풉(푣) can be safely assumed. Therefore, we can
simply replace 푣 with 풉 in Eq (7.1) and write:
푃(푅 | 푥, 푦)
★
=
Õ
풉
푃(풉 | 푥) 푃(풉 | 푦) (7.10)
where 풉 ≡ 풉(푣) ranges over all possible (or reasonable) PHOC representations of
words. The posterior probabilities 푃(풉 | 푥) and 푃(풉 | 푦) can be computed from the
PHOCNet output vectors, 풉(푥) and 풉(푦), respectively. To this end, just as assumed
for PHOCNet training, the 퐷 components of a PHOC can be assumed conditionally
independent and, using ℎˆ
푖(·) as defined in Eq. (7.9), we can write:
푃(풉 | 푥)
★
=
Ö
퐷
푖=1
푃(ℎ푖
| 푥) =
Ö
퐷
푖=1
ℎˆ
푖(풙)
ℎ푖
(1 − ℎˆ
푖(풙))1−ℎ푖
(7.11)
푃(풉 | 푦)
★
=
Ö
퐷
푖=1
푃(ℎ푖
| 푦) =
Ö
퐷
푖=1
ℎˆ
푖(풚)
ℎ푖
(1 − ℎˆ
푖(풚))1−ℎ푖
(7.12)
Putting it all together, Eq. (7.10) becomes:
푃(푅 | 푥, 푦)
★
=
Õ
풉
Ö
퐷
푖=1

ℎˆ
푖(풙) ℎˆ
푖(풚)
ℎ푖

￾
1 − ℎˆ
푖(풙)
 ￾1 − ℎˆ
푖(풚)

1−ℎ푖
(7.13)
This sum can be efficiently computed by dynamic programming in O (퐷), the same
asymptotic cost as that of the Bray–Curtis or the cosine dissimilarities used in
previous works on PHOC-based KWS [34, 35].
To summarize this section, we have derived an efficient way of computing a
probabilistically sound measure (i.e. the RP presented in Sec. 7.2.1), which can be
used to rank pairs of images, for word-segmented QbE scenarios, based on the PHOC
representation of words.
7.3.4 PHOCNet Probabilistic Interpretation: Empirical Results
In the previous section we have provided a probabilistic interpretation of PHOC￾based QbE KWS and derived a probabilistically-sound equation to compute the
RP from the PHOCNet output. This approach is more interpretable than the original
PHOC-based KWS formulations and enjoys theoretical guarantees of optimality (see
Appendix A7.3 PHOC-based Methods 195
The first goal of the experiments presented below is to show that the proposed
probabilistic interpretation of PHOC-based KWS actually achieves results as good
as the original (dissimilarity-based) PHOCNet approach.
The second goal seeks to check out the claimed equivalence of PHOCNet and
an isolated word recognizer, by comparing PHOC-based KWS results with those
of provided using a more conventional model of isolated word image transcript
posterior. To this end, we tried to use as many parts as possible from the original
TPP-PHOCNet architecture. In [34] they tried to replace the last layer from the
network with a word classifier (SoftMax CNN). However, the KWS results they
obtained were not as good as with the PHOCNet approach. Although they did not
compute a true RP using the SoftMax output as word posteriors, based on our
experience with lexicon-based approaches (see Sec. 6.4.1), we believe that building
a word classifier is not the best option due to the OOV problem. To avoid this
problem, rather than a word classifier, an lexicon-free model is needed. With this
purpose, we replaced the fully connected layers of the PHOCNet architecture with
a single recurrent layer, followed by a SoftMax layer to predict character labels,
and have used the CTC loss to train this network. With the network’s posteriors, we
generated lattices as described in Sec. 4.7.2, prune them for sake of efficiency with a
beam equal to 15, and then used the method presented in Sec. 7.2.1 to compute the
relevance probability. The latter step was performed simply by a composition of two
WFSTs and the Backward algorithm in the result to compute the sum of all paths
(see Eq. (4.33) and Sec.B.3.2).
In summary, we compare results obtained by three approaches:
1. Ranking the PHOCNet vectors of the collection images by increasing Bray–
Curtis measure. This approach will be referred to as PHOCNet (actually this
uses the TPP-PHOCNet architecture from [35]).
2. Ranking the vectors by decreasing RP, computed using our probabilistically
interpretation of the PHOC representation, Eq. (7.13). This will be referred to
as Probabilistic PHOCNet.
3. Ranking the collection images by decreasing RP, computed using a neural
network similar to TPP-PHOCNet, but trained lexicon-free with a character￾level CTC loss. This approach will be referred to as CTCNet.
For the PHOC-based methods 1 and 2, we used the TPP-PHOCNet architecture
described in [35]. It was re-implemented in PyTorch,5 since we later needed to
slightly adapt it to use the CTC loss. Since the authors published their source code,
we made sure to be using the same training loss (Binary Cross Entropy); learning
algorithm (Stochastic Gradient Descent, with batch size 10, learning rate equal to
10−4
, momentum equal to 0.9, and weight decay equal to 5 · 10−5
); and the same
data augmentation procedure.
We used the same evaluation measures as in [35]. That is, we computed the mAP,
without interpolated precision. In addition, we also computed the gAP, which is not
typically reported in PHOC-based works.
5 https://pytorch.org196 7 Probabilistic Interpretation of Traditional KWS Approaches
In order to compare these results with those of the previous section, we replicated
the experiments from [35] on the word-segmented George Washington dataset,
using exactly the same partitions, ground-truth transcripts and query sets, to make
sure that we made a fair comparison. Since the results of all methods are very close in
this data set, for each approach we trained 8 models with different random seeds for
each of the cross-validation fold of the database. The mAP and gAP values reported
below are averages over the 8 runs and the 4 cross-validation folds.
Results
For each approach, Table 7.2 shows the mAP and gAP averaged across the test-sets of
the four cross-validation folds of the word-segmented George Washington dataset.
Table 7.2: Comparison of word-segmentation-based QbE KWS performance (gAP and
mAP in %) achieved by different PHOC-based methods on the word-segmented George
Washington dataset. Reported results are averaged across 8 model-training random ini￾tializations and 4 cross-validation folds.
Method gAP mAP
PHOCNet (original) — 97.8
PHOCNet (ours) 98.4 97.7
Probabilistic PHOCNet 98.7 98.1
CTCNet 98.7 98.3
First, observe that our implementation of the TPP-PHOCNet achieves almost the
same mAP as in the original work (97.7% and 97.8%, respectively). The differences
are likely due to the random seeds used in each case, and due to the fact that some
operations executed in GPUs are non-deterministic. Our implementation achieves
98.4% gAP, while in [35] they did not report this figure.
The Probabilistic PHOCNet formulation achieves slightly better mAP and gAP:
98.1% and 98.7%, respectively. CTCNet provides an additional small mAP improve￾ment, achieving a value of 98.3%, while the gAP is the same as the Probalistic
PHOCNet. Note that CTCNet is just a version of our conventional CRNN models,
simplified to deal with isolated words (rather than continuous text). This explains
the better performance reported in this experiment (98.7 % gAP) with respect to the
word-segmentation-free CRNN results reported in Table 6.10 (94.6 % gAP for GW).
It is important to emphasize that we did not performed any fine-tuning for any of
the approaches, we simply used the same architecture and hyperparameters used in
the original TPP-PHOCNet paper. This means that the Probabilistic PHOCNet and
the CTCNet may be in a slight disadvantage in front of the original TPP-PHOCNet,
since in the later they presumably did tune these hyperparameters, as well as the
architecture of the neural network. Nevertheless our (lower bounded, in this sense)
mAP and gAP are still better than those of the PHOCNet, although the differences
are not significant, since all methods achieve very high values for both metrics.
One advantage of the CTCNet, which we have not exploited here, is that we could
have combined its output with an external (character) language model, as we did in7.3 PHOC-based Methods 197
previous sections. However, we decided not to do so to keep this experiment simple
and comparable with traditional word segmentation-based publications.
Moreover, the CTCNet has far less parameters than the PHOCNet (17 935 589 and
59 859 420, respectively), since all the fully connected layers in the latter have been
replaced with a single BLSTM, and a linear layer for the final mapping. This suggests
that indeed the PHOCNet is over-parameterized for the task. This is consistent with
our hypothesis that a given PHOC binary vector is a redundant representation of a
word spelling. Thus, by eliminating this redundancy we can reduce the number of
parameters needed by the model and still obtain better results.
7.3.5 Summary of Results of Distance– and PHOC–based Methods
Table 7.3 summarizes the results of the QbE KWS experiments performed using
segmented word images. That is: the results of the traditional distance-based meth￾ods and the corresponding probabilistic interpretations (described in Sec. 7.2); the
PHOC-based models, including our probabilistic interpretation and the CTC-based
method described in Sec. 7.3.
Table 7.3: Summary of QbE KWS results on the word-segmented George Washington
dataset, for different approaches described in this chapter. The first three rows are results
obtained following a traditional distance-based approach and our probabilistic interpreta￾tions thereof. The three middle rows were obtained using PHOC-based methods. And the
final row shows the result of a CRNN trained using the CTC algorithm.
Method mAP (%) gAP (%)
(Features from [31])
Raw distance − ∥ 풙푖 − 풚푗 ∥ 67.1 29.2
푃(퐶 = 푖 | 푌 = 풚푗 ) 67.1 52.4
푃(푅 | 푋 = 풙푖
, 푌 = 풚푗 ) 89.0 96.4
PHOCNet [35] 97.8 —
PHOCNet (ours) 97.7 98.4
Probabilistic PHOCNet 98.1 98.7
CTCNet 98.3 98.7
These results are related with those reported in Chapter 6 for experiments with the
line-level George Washington dataset (Appendix C.3). However the comparison
can not be fair, since the experiments in Chapter 6 where all for word-segmentation￾free QbS KWS, while those reported in this section correspond to QbE and are
word-segmentation-based, which is an important simplification of the KWS problem.
Applying the probabilistic formulation described in this book over the traditional
distance-based and PHOC-based approaches improves both the mAP and gAP. In
addition, using a CRNN model trained with the CTC loss also improves these re￾sults. The most remarkable result is the huge improvement achieved using a simple
distance-based method along with our probabilistic formulation, which significantly
reduces the gap between traditional techniques based on nearest-neighbors and mod￾ern approaches based on deep convolutional and/or recurrent neural networks.198 7 Probabilistic Interpretation of Traditional KWS Approaches
7.4 HMM-Filler
In this section the word-segmentation based KWS simplification is abandoned and
we go back to the more natural and application-oriented KWS assumptions adopted
in Chapter 6, which are word-segmentation-free and QbS.
One of the most popular, traditional QbS KWS methods was the so-called HMM￾Filler approach [10, 11]. It was considered among the most successful (yet expensive)
methods for word-segmentation-free KWS for text images. Earlier variants of this
method were applied to KWS on speech signals [28, 30, 29].
This approach models the text rendered in a text image using HMMs with diagonal
GMMs as the state emission probability density functions (see Sec. 4.3).
Each character is represented by an individual HMM and composite models are
used to build: a) a generic (or garbage) model, known as “garbage” or “filler HMM”,
and b) a specific “keyword HMM” for the keyword to be spotted. Fig 7.6 depicts
these two composite models. Each circular node represents the HMM model (with
several states and transitions) of an individual character in the alphabet (including
the white space symbol). The rectangular nodes represent copies of the garbage
model. Observe that the generative model represented by the filler can produce any
character sequence, while the keyword-specific model can only produce sequences
that contain at least one instance of the keyword (“word” in this example).
a
b
...
z
@
(a) Filler HMM: 휽 푓
w o r d
@ @
Filler Filler
(b) Keyword HMM: 휽푣 for 푣 = “word”
Fig. 7.6: Models used in the HMM-Filler approach: (a) “filler HMM”, 휽 푓
, and (b) “keyword
HMM”, 휽푣, built for the keyword 푣 = “word”. Circular nodes represent character HMM
models, including the withe space “@”, and the rectangular nodes represent copies of
the garbage or “filler” model. The filler accounts for any character sequence, while the
keyword-specific model only deals with text that contain at least one instance of “word”.
In [11] the following score was proposed to express how likely a query word, 푣, is
written in a text line image 푥 (assumedly represented as a feature vector sequence).
푆 푓 (푥, 푣)
def
=
1
|푣|
훾
￾
max
푤
log 푝(푥, 푤; 휽푣) − max
푤′
log 푝(푥, 푤′
; 휽 푓 )

(7.14)
where 푣 is the (character spelling of) a keyword, |푣| is the length of 푣, 푤, 푤′
are
arbitrary character sequences and 훾 is a tunable hyperparameter7.4 HMM-Filler 199
On the one hand, observe that log 푝(푥, 푤′
; 휽 푓 ) does not depend on the specific
query word, 푣. On the other hand, log 푝(푥, 푤; 휽푣) will (hopefully) be high if the
keyword 푣 is rendered at some location of the line image, while it will be low if not.
Thus, by comparing how large is the likelihood provided by the keyword-specific
model with respect to the keyword-agnostic (i.e. garbage) model, we can get a sense
of how likely is that the word 푣 is written somewhere in the image 푥.
The denominator in Eq. (7.14) was aimed to normalize the scores with respect to
the length of the query keyword. In general, models based on HMMs tend to give
larger likelihoods to shorter observation sequences. So, without this normalization,
long keywords would tend to have much lower scores than short ones, which damage
the gAP (see Sec. A.3). The term |푣| may refer to the length (number of characters)
of the keyword, or to the number o frames aligned with the keyword. In most
cases, the value of the hyperparameter 훾 is set to 1, like in the original HMM￾Filler publication, but other authors decided to tune it on a validation set to improve
system’s performance [37].
In [39], it was shown that the HMM-Filler score can be efficiently computed by
means of a (single) CL. First, a lattice from the text line is obtained using only the
“filler” model; then, the score of the best path (highest likelihood) in this lattice
which contains the keyword is divided by the score of the overall best lattice path
(which is just the right-hand part of the subtraction in Eq. (7.14)). This approach
produces almost exactly the same results as the original HMM-Filler, but the lattice
approach was very much faster (about 2 orders of magnitude).
Later, in [36, 37], instead of using the simple HMM-Filler model, such as that
of Fig. 7.6 (a), 푛-gram character language models were used to obtain better CLs.
It was shown that, as the order of the 푛-gram is increased, the results improved
substantially. Moreover, one can also observe that if one adjusts the parameter 훾 in
Eq. (7.14) using validation data, its value gets closer to 0 as one increases the order
of the language model (see the results in Sec. 7.4.1). This suggests that the length
normalization was only needed because of the extremely naive language models
initially adopted (in fact, the original HMM-Filler model assumed that all characters
sequences are equiprobable).
In addition, it was finally shown in [23] that the HMM-Filler score can actually
be interpreted as an approximation to the RP defined in Eq. (3.24) of Chapter 3. This
directly relates the HMM-Filler method with the probabilistic framework described
in this book. This relationship is formally discussed hereafter.
As noted before, the length normalization term of the original formulation be￾comes unnecessary when a large context is used to model the prior information of
the language. Thus, ignoring this term and applying the exponential function to both
sides of Eq. (7.14), it can be rewritten as:
exp 푆 푓 (푥, 푣) =
max푤 푝(푤, 푥; 휽푣)
max푤′ 푝(푤′
, 푥; 휽 푓 )
(7.15)200 7 Probabilistic Interpretation of Traditional KWS Approaches
Observe that all the character strings 푤 for which the keyword HMM likelihood
푝(푤, 푥; 휽푣) is greater than 0 contain at least one instance of the keyword 푣. Thus, in
the numerator, the maximum over all character strings is equal to the maximum over
all character strings 푤 such that6 푣 ⊆ 푤. Therefore:
exp 푆 푓 (푥, 푣) =
max푤:푣⊆푤 푝(푥, 푤; 휽푣)
max푤′ 푝(푥, 푤′
; 휽 푓 )
(7.16)
On the other hand, the RP of Eq. (3.24) can be rewritten in terms of the joint
likelihood of the image and the transcript, rather than the posterior, as follows:
푃(푅 | 푥, 푣) =
Õ
푤:푣⊆푤
푃(푤 | 푥) =
Í
푤:푣⊆푤 푝(푥, 푤)
푝(푥)
=
Í
푤:푣⊆푤 푝(푥, 푤)
Í
푤′ 푝(푥, 푤′
)
(7.17)
which, approximating the sums by the dominating addends, becomes:
푃(푅 | 푥, 푣) ≈ max푤:푣⊆푤 푝(푥, 푤)
max푤′ 푝(푥, 푤′
)
(7.18)
That is, the (exponential of the) HMM-Filler score is just an approximation to
the PrIx RP that the word 푣 is written in the image 푥. The main difference between
Eq. (7.16) and (7.18) is that the HMM-Filler approach uses two different models for
the joint likelihood, 휽푣 and 휽 푓
. In contrast, our formulation just assumes the same
model in both cases and this model does not need to be a naive “filler” model as the
one depicted in Fig. 7.6 (a) – in fact, improved results are expected by more accurately
modeling the real distribution 푝(푥, 푤), as in our works [36, 37], commented above.
On the other hand, Eq,(7.17) suggest that further improvements can be achieved
if rather than the max approximation, the real likelihood ratio is computed which, as
it has been shown, is exactly the same as the RP proposed in our PrIx formulation.
These improvements, which will be empirically checked in the next section, are
no surprise because the RP provides an optimal ranking, as formally proven in
Appendix A.
As inEq. (7.14) of the original HMM-Filler score, the length (number of character)
of the keyword can be used as a heuristic length normalization. That is, the retrieved
lines can be ranked in increasing order of:
푃
′
(푅 | 푥, 푣) ≈ 푃(푅 | 푥, 푣)
|푣|
−훾
(7.19)
where |푣| is the length of the keyword and 푃(푅 | 푥, 푣) is the RP approximated by
Eqs. (7.17) or (7.18).
6 Abusing the notation, here we understand that 푣 ⊆ 푤 means that the character sequence 푣 is a
substring of 푤.7.4 HMM-Filler 201
7.4.1 HMM-Filler Probabilistic Interpretation: Experiments
In this section, we describe the experiments carried out to highlight the probabilistic
interpretation of the HMM-Filler KWS method as well as the improvements achieved
by using the better approximations to the RP derived from this interpretation.
The experiments were performed on the line-level IAM dataset. That is, we
assumed that the text line images were given and the aim was localize the relevant
lines for a set of query words. We use the IAM partition used in the original
publications describing the HMM-Filler approach [10, 11, 9].
For each character in the training lexicon, a left-to-right HMM-GMM was used
in order to model the likelihood of the images. To allow comparison, the same
simple HMMs and hand-crafted feature extraction methods proposed in [9] where
used (these models were kindly provided by the authors). As discussed in that paper,
the number of states and the number of Gaussians per state were tuned in order to
minimize the CER for the validation partition.
Using these models, the classical HMM-Filler approach was implemented fol￾lowing Eq. (7.14). The probabilities were obtained with the Viterbi decoders of the
HTK [43] or iATROS [19] toolkits,7 using for the “filler” model character 푛-gram
language models of different orders. This approach was also implemented following
Eqs. (7.19) and (7.18), and a Tropical semiring instantiation of Alg. 7.1, described in
Sec. 7.4.2, below. The results of this (very much faster) implementation were essen￾tially identical to those of the classical HMM-Filler approach and are not reported
below for the sake of brevity (see [39] for details).
On the other hand, for our probabilistic interpretation, we followed Eqs. (7.19)
and (7.17), implemented using Alg. 7.1 instantiated in the Log semiring.
The CLs needed by the fast algorithm Alg. 7.1 were obtained using the HTK [43]
or iATROS [19] decoders, with the 푛-gram language models discussed below. To
control CL sizes, the maximum node input degree was limited to 30 edges, along with
conventional beam search decoding, grammar scale factor and character insertion
penalty parameters adjusted as discussed above.
Language model 푛-grams were trained on the well-known Lancaster-Oslo/Bergen
text corpus (LOB) [18]. Since the IAM corpus was built from the LOB, all sentences
used in the test set of the IAM dataset were excluded for 푛-gram training. The
grammar scale factor and character insertion penalty decoding parameters were
adjusted using the validation data to optimize the CER.
In each approach, an optimal length-normalization parameter, 훾
★, was determined
by maximizing the gAP on validation data.
7 iATROS was somewhat less efficient than HTK, but HTK could not deal with 푛-grams with 푛 > 2
(or even 푛 > 4 with re-scoring). On the other hand, iATROS lattices tend to produce slightly worse
results than those obtained with HTK. Therefore HTK+re-scoring was preferred for 푛 ≤ 4.202 7 Probabilistic Interpretation of Traditional KWS Approaches
Results and Discussion
Table 7.4 shows comparison results on the IAM dataset.8 HMM-Filler is the clas￾sical approach, while Exact-RP corresponds to our probabilistic interpretation (and
improvement). As mentioned above, to allow comparisons, the same HMMs and
IAM partitions as in [10] were used.9
Table 7.4: Line-level gAP results in IAM, using HMM-GMM, and different character
푛-grams and RP approximations. HMM-Filler refers to the classical approach, computed
with Eq. (7.14) – or Eqs. (7.19, 7.18) and Alg. 7.1 instantiated in Tropical semiring. Exact￾RP follows Eqs. (7.19, 7.17) and Alg. 7.1 instantiated in Log semiring. Optimal length
normalization parameter values, 훾
★, were determined in each case using the validation set.
0-gram 1-gram 2-gram 3-gram 4-gram 5-gram 6-gram
HMM-Filler gAP (훾 =훾
★) 40.2 42.9 45.3 48.3 50.0 50.5 50.8
훾
∗ 1.99 1.91 1.94 1.87 1.76 1.52 1.76
Exact-RP gAP (훾 =훾
★) 41.8 47.6 50.3 53.1 56.9 58.9 61.3
훾
∗ 1.68 1.83 1.83 1.91 1.73 1.59 1.38
First, we can observe that using better character language models (higher order
푛-grams) greatly improves the AP of the two methods. This is consistent with the
probabilistic reasoning followed through this book: the better the probabilistic models
of the handwritten text, the better KWS performance one should expect.
Second, observe that as the 푛-gram order is increased, the optimal value of
훾 decreases. This suggests that, as hypothesized above, the length normalization
heuristic is only necessary when poor probabilistic models are used.
Third, our HMM-Filler probabilistic interpretation (which has been shown to cor￾respond just to an exact computation of the RP) proves to be better than the classical
HMM-Filler in all the cases, as expected from the formal proofs in Appendix A.
Finally, note that for low-order 푛-grams, the two methods perform quite similar,
but when 푛 increases, the exact computation clearly shows its superiority. This is
explained because, if poor statistical models are used, the difference between com￾puting the exact probability or a not-so-good approximation is not that significant.
For similar reasons, it is worth to highlight that both the absolute and the relative
improvements with respect to the classical HMM-Filler increase with the 푛-gram
order. This suggests that not only good probabilistic models is important; ranking
the text lines with the right scores (namely, RPs) has a significant impact too.
8 Bear in mind that the experiments of this section were not aimed at achieved best KWS or PrIx
results. The only purpose was to allow fair and accurate comparison of the classical HMM-Filler
methods with our PrIx interpretation of these methods and to put forward the advantages entailed
by this interpretation. For this reason, the results reported here can not be compared at all with the
much better results of experiments with the IAM dataset presented in Chapter 6, which were aimed
at showing the full power of our PrIx proposals, mainly using CRNN optical models.
9 However, note that the absolute values of gAP reported in [11, 9] were significantly better because
in these works the set of query words were drastically limited to those appearing in both the training
and test sets of the IAM partition. With this departure from the original setting defined in [10],
the number of keywords was reduced from 3 421 down to just 882 in [11, 9]. In the experiments
presented here, we stick to the original query set (and dataset partition), as in [39, 24, 40, 37, 22].7.4 HMM-Filler 203
7.4.2 Fast HMM-Filler Computation using Character Lattices
In the classical HMM-Filler approach, two Viterbi decoding processes are needed
for each of the 푁 text-line image regions of the image collection considered. The
decoding with the Filler model can be carried out once for all in a pre-processing
step, but the decoding with the Word-specific model has to be carried out on-line for
each of the 푀 given keywords. This entails an important computational burden at
query time, O (푀 푁), which is clearly prohibitive in practice for large collections.10
In contrast, according to our probabilistic interpretation, no Word-specific model is
needed and all the computations can be carried out off-line, in a preprocessing step.
However, even with off-line processing, the computational burden is formidable,
because it has to be carried out for each (potential) query word. Nevertheless, using
CLs to compute the required probabilities (see Eq. 7.17), a fast algorithm similar to
Alg. 5.1 can be devised. It is worth mentioning that this algorithm, in contrast to
those described in Chapter 5 (aimed at large-scale indexing), does not actually aim at
pre-computing a PrIx, but rather at fast honoring queries “on-the-fly”, as it is more
generally assumed in traditional KWS.
Recall from Chapter 4 that a Character Lattice (CL) is a WFSA whose edge labels
are characters.11 Given a CL of an image region 푥, Eq. (7.17) can be approximated
by the CL-computed likelihoods, p
G
(푥, 푤) and p
G
(푥) (Eqs. 4.28, 4.29), as:
푃(푅 | 푥,푣) =
Í
푤:푣⊆푤 푝(푥, 푤)
푝(푥)
≈ exp  Ê
푤:푣⊆푤
log p
G
(푥, 푤) ⊘ log p
G
(푥)

(7.20)
The operators in this expression can be instantiated in the Log or the Tropical
semiring, depending on whether we want to deal with Eq. (7.17) or (7.18). Since
both semirings work with logarithmic values, the exp(·) function is applied to obtain
the required probability.
As discussed in Sec. 4.7.4, if a CL is obtained using adequate models and it is
large enough, the approximations p
G
(푥, 푐) and p
G
(푥) used in Eq. (7.20) are expected to
be sufficiently good for our probabilistic interpretation of the HMM-Filler approach.
The term p
G
(푥) needed in Eq. (7.20) can be very efficiently computed by means of
the backward (훽) scores introduced in Sec. 4.7.4 (see Eq. (4.34)). The new problem
here is how to efficiently compute the ⊕-sum of Eq. (7.20).
To this end, we resort to an extension of what was called edge posterior in
Eq. (4.35) (and Eq. (4.36)) of Sec. 4.7.4. Let a keyword 푣 = 푣1, 푣2, . . . , 푣푚 ≡ 푣1:푚
be a sequence of 푚 characters and let 푒 = 푒1, 푒2, . . . , 푒푚 be a CL sub-path such
that 푙(푒) = 푣. Thus, the edge posterior extension ˜휑(푒), named henceforward edge
10 E.g., in the full Bentham Papers (https://www.prhlt.upv.es/htr/bentham), 푁 and 푀
can be estimated at 2.5 million lines and 300 thousand words (lexicon), respectively [38]. That is
about 750 billion Viterbi decoding steps, which would require several years of intensive computing.
11 To better follow this section, we recommend to review WFST concepts notation, like 푝(· ), 푛(· )
and 푙(· ), or the weight computation of 휔(· ) and ˜휔(· ) for partial and complete paths respectively,
already presented in Sec. 4.7 for WGs, but here instantiated for CLs.204 7 Probabilistic Interpretation of Traditional KWS Approaches
sequence normalized score, is defined as:
휑˜(푒)
def
=
Ê
푤,푎:∃휙∈Φ
푙(휙)=푤,푒⊆휙
PG (푤, 푎 | 푥) =
Ê
휙:푒⊆휙
휔˜ (휙) ⊘ p
G
(푥)
=
￾
훼푝(푒) ⊗ 휔(푒) ⊗ 훽푛(푒)

⊘ 훽푞0
(7.21)
Basically, it is the normalized ⊕-sum over all complete paths 휙 which contain12 the
sub-path 푒. As Eq. (7.21) shows, ˜휑(푒) can be also efficiently computed by means of
the backward (훽) and forward (훼) scores defined by Eqs. (4.32) and (4.33):
Given 푣≡푣1:푚 and the image 푥, we define now a frame-level character sequence
score for each horizontal position (or frame) of 푥, 푖. It considers the contribution of
the edge-sequence normalized scores of all the CL sub-paths labeled with 푣, which
correspond to segmentation hypotheses that include the frame 푖; that is:
푆(푥, 푣, 푖)
def
=
Ê
푒:푙(푒)=푣,
푎( 푝(푒) ) ≤푖<푎(푛(푒) )
휑˜(푒) (7.22)
푆(푥, 푣, 푖) is a character-sequence extension to the single-word posteriorgram intro￾duced in Sec. 3.3.3 (see also [16, 40]). Fig. 7.7 shows an example of such a poste￾riorgram computed for the query 푣 = “to” from a CL produced by decoding a line
image rendering the text “to be for”.
b
t
f a
o @ b
k
e
e
e
@ f
f
t
a
o
o
r
r
@
a
o
t
@
f
t
e
S(x, “to”, i)
y
q1
q2 q3
q4
q5
q6
q7 q8
α(q6) β(q8)
α(q5)
β(q3)
α(q4)
α(q1)
a(q4)a(q1) a(q2) a(q3) a(q6)a(q5) a(q7) a(q8)
Fig. 7.7: Top: an image rendering the text “to be for”. Middle: A small, partial example
of CL of this image, where the symbol “@” stands for the white space character. The
edge sequence paths: { (푞1, 푞2 ), (푞2, 푞3 ) }, { (푞4, 푞2 ), (푞2, 푞3 ) }, { (푞5, 푞7 ), (푞7, 푞8 ) }
and { (푞6, 푞7 ), (푞7, 푞8 ) } (in dashed red lines) correspond to the query character sequence:
푣=“to”. Bottom: Corresponding character-sequence posteriorgram 푆(푥, 푣, 푖). Note that
the score 푆(푥, “to”, 푖) is not negligible in the interval where the word “for” appears in the
image. However is much lower than in the correct interval where “to” is actually written.
As discussed in [40], the RP can now be simply computed as:
12 We write this as 푒 ⊆ 휙, overloading again the operator ⊆, as in footnote (6)
7.4 HMM-Filler 205
푃(푅 | 푥,푣) ≈ exp ￾
max
푖
푆(푥, 푣, 푖)

(7.23)
This approach also provides the spotted word location associated to the maximum 푖
used to compute 푃(푅 | 푥, 푣); for instance, in the example shown by the Fig. 7.7, the
best scores of 푆(푥, 푣, 푖) for the keyword “to” span positions 푎(푞1) through 푎(푞3).
Alg. 7.1 shows how to compute the ⊕-sum efficiently for a given image region 푥
and query 푣. It can be seen as a generalization of Alg. 5.1 of Chapter 5, where the
posteriorgram is computed by summing up edge-sequence normalized scores rather
than edge posterior scores.
Algorithm 7.1 Compute the maximum score value of 푆(푥, 푣, 푖), ∀푖, from a CL of a
text line image 푥 and a query character sequence 푣≡푣1:푚.
Require: A deterministic character compact lattice 퐴 = (Σ, Q, E, 푞0, 휚)
Require: The state alignment function 푎(· ) of 퐴
Require: 푣1:푚 is a query-word character sequence of length 푚
1: procedure FrameLevelWordIndex(퐴, 푎, 푣1:푚)
2: 휶 ← Forward(퐴) ⊲ Forward vector of each state
3: 휷 ← Backward(퐴) ⊲ Backward vector of each state
4: StackInit(푆, ∅) ⊲ Stack of pending auxiliary tuples to process
5: 풉 ← 0¯ ⊲ Initialize posteriorgram vector to hold 푆(푥, 푣, 푖), ∀푖
6: 푢 ← 0¯ ⊲ Initialize maximization of Eq. (7.23)
7: for all 푒 ∈ E : 푙(푒) = 푣1 do ⊲ Look for all arcs whose labels are equal to 푣1
8: StackPush(푆, (훼푝(푒) ⊗ 휔(푒), 푎( 푝(푒) ), 푛(푒), 2))
9: while StackNotEmpty(푆) do
10: (휑, 푗, 푞, 푘) ← StackPop(푆)
11: if 푘 ≤ 푚 then ⊲ Progresively look for arcs of subsequent characters of 푣
12: for all 푒 ∈ N (푞) : 푙(푒) = 푣푘 do
13: StackPush(푆, (휑 ⊗ 휔(푒), 푗, 푛(푒), 푘+1))
14: else ⊲ Compute 휑 value and accumulate it in 풉
15: 휑 ← 휑 ⊗ 훽푞 ⊘ 훽푞0
16: for all 푖 ∈ [ 푗, 푎(푞) ) do
17: ℎ푖 ← ℎ푖 ⊕ 휑 ⊲ Update 푖-th posteriorgram entry for the word 푣
18: 푢 ← max(푢, ℎ푖 ) ⊲ Update the highest pick for 푣 (Eq. (7.23))
return exp(푢) ⊲ Apply the exponential of 푢 since it is in logarithm
The overall cost of computing the final RP given by Eq. (7.23), excluding CL gen￾eration and forward-backward computation, is determined by the costs of computing
the frame-level character sequence score 푆(푥, 푣, 푖) for all 푖, and the final maximization
of Eq. (7.23). Let 퐿 be the average length (in frames) of the subpaths corresponding
to 푣, and 퐵 = |E |/|Q| be the CL average branching factor. According to Alg. 7.1, in
the worst case the stack 푆 would contain |E | entries. For each entry, 푚·퐵 lookups
are to be performed to find at most only 푚 edges whose labels match with 푣푘 (this
is because 퐴 is deterministic) or, otherwise, 퐿·|E | steps are needed to populated the
vector 풉 and compute the maximum 푢. All in all, the worst-case time complexity cost
is 푂( (푚퐵+ 퐿)|E |). On the other hand, the worst case space complexity is 푂(|Q| +푇)
where 푇 is the horizontal size of the image 푥206 7 Probabilistic Interpretation of Traditional KWS Approaches
It is worth remarking that this approach for computing the 푃(푅 | 푥, 푣) is actually
an approximation, and there is another CL-based method, explained in [37], which
computes the exact probability. However, it involves a much higher computational
cost and the KWS performance achieved is almost exactly the same.
Empirical Evaluation of Alg. 7.1
Table 7.5 reports computing times13 for the classical (and “contextual”) HMM-Filler
method [9] and the fast CL-based version. Results are reported only for the lower￾order 푛-gram language models (0 ≤ 푛 ≤ 3). Values of gAP (from Table 7.4) are also
reported for the sake of completeness.
Table 7.5: Preprocessing and average query times and total indexing times, for classical
(REF) and CL-based (CL) HMM-Filler KWS on the IAM dataset. Values of gAP from
Table 7.4 are also reported.
0-gram 1-gram 2-gram
REF CL REF CL REF CL
Total preprocessing Time (min) 66.7 219.5 65.6 227.5 66.4 245.7
Average Query Time (min) 68.4 0.9 68.3 0.9 71.0 0.9
Total Indexing Time (days) 162.5 2.3 162.3 2.3 168.7 2.3
gAP (%) 40.2 42.9 45.3
Computing times are shown split into preprocessing and search (query) times, the
latter given in average minutes required for each single keyword search. The table also
shows total time (in days) needed to fully index the corpus with the corresponding
selected keywords (3 421 for IAM). We reported similar results in early papers such
as [39, 36, 37], were additional details are given.
Using the proposed method based on CLs, the average query response time is
reduced by a factor greater than 75 in all the cases. The corresponding overall time
that would be needed for indexing all the 3 421 selected keywords is also more than
70 times lesser in all the cases. Clearly, 2.3 days of (single-core) computing time
would be affordable using the proposed CL-based KWS method. In contrast, the
same process using the classical HMM-Filler approach would require more than 60
days, which would be overly prohibitive, even for this very small image collection.
7.5 BLSTM-CTC KWS
This approach was presented in [13] and for some time it was perhaps the most
successful technique for QbS KWS on the most popular benchmark KWS datasets at
that time. It was one of the first methods to approach KWS by using the BLSTM model
(see Sec. 4.4), which had already proved very successful for plain HTR transcription.
13 reported in terms of total elapsed time needed using a dedicated single core of a 64-bit Intel Core
Quad computer running at 2.83GHz.7.5 BLSTM-CTC KWS 207
The application of the original model to KWS required only a slight modification
of the classical CTC algorithm discussed in Sec. 4.4.3.3. For plain HTR, CTC is used
to compute the posterior probability of a character sequence from the sequence of
character-level label probabilities (called “character-level posteriorgram” in Chap￾ter 4) yield by the BLSTM for a given text image. The posterior probability of a
character sequence is computed by adding the probabilities of all CTC-alignments
for that sequence, using dynamic programming.
Instead of adding the probabilities of all alignments, the BLSTM-CTC KWS
approach keeps track of the alignment with the maximum posterior. In addition,
since the aim is not to obtain the best alignment of the full text line image, but only
to predict whether or not that image contains the given keyword, an special symbol
(namely, “∗”) is introduced which can be optionally matched (with probability 1)
against any character at the start and at the end of the desired keyword. Thus,
if 푣 = 푣1, 푣2, . . . , 푣푚 is (the character spelling of) a keyword, it is converted into
푣
′ = ∗, @, 푣1, 푣2, . . . , 푣푚, @, ∗, where “@′
’ represents a white space (or any other
word separator) and is added to ensure that only complete words are spotted, not
subwords. See [13] for full details.
In other words, this method searches for a best path through the character-level
posteriorgram provided by the BLSTM, such that the CTC decoding yields the
query character-sequence 푣. This, along with the use of the wildcard “∗” to ignore the
probability of other character-label subsequences (by assuming they have probability
1), is reminiscent of the HMM-Filler strategy, but using BLSTMs rather than HMMs
for optical modeling.
As we see it, using the notation of Sec. 4.4.3.3 (see Eq. (4.14)), the KWS score
computed by the algorithm proposed in [13] can be formally expressed as:
Ψ˜ (푣, 푥)
def
= max
1≤푖≤ 푗≤푇
max
푐푖:푗
Υ(푐푖:푗 )=푣
푃(푐푖:푗
| 푥) = max
푐푖:푗
Υ(푐푖:푗 )=푣
1≤푖≤ 푗≤푇
푃(푐푖:푗
| 푥) (7.24)
where 푣 ≡ 푣1:푚 is the keyword (including surrounding blanks 푣1 = 푣푚 = @), 푥 is a
text image represented by 푇 feature vectors or frames, Υ is the CTC function, and
푐푖:푗
is a substring of an arbitrary character-level label sequence 푐1:푇 ≡ 푐.
As discussed in Sec. 4.4.3.2, the sum of 푃(푐푡
| 푥) for all values of 푐푡
is 1, and
from Eq. (4.13), for any 푘, 푘′
, 1 ≤ 푘 ≤ 푘
′ ≤ 푇, the sum of the probabilities of all the
subsequences 푐푘:푘
′ is 1. Therefore, the term 푃(푐푖:푗
| 푥) of Eq. (7.24) can be rewritten
as:
푃(푐푖:푗
| 푥) =
 Õ
푐1:푖−1
푃(푐1:푖−1 | 푥)

푃(푐푖:푗
| 푥)
 Õ
푐푗+1:푇
푃(푐 푗+1:푇 | 푥)

=
Õ
푐1:푖−1
Õ
푐푗+1:푇
푃(푐1:푖−1 | 푥) 푃(푐푖:푗
| 푥) 푃(푐 푗+1:푇 | 푥)
★
=
Õ
푐1:푖−1
Õ
푐푗+1:푇
푃(푐1:푖−1, 푐푖:푗
, 푐 푗+1:푇 | 푥) =
Õ
푐1:푖−1
Õ
푐푗+1:푇
푃(푐 | 푥) (7.25)208 7 Probabilistic Interpretation of Traditional KWS Approaches
where in the third step it has been assumed that each label 푐푡
, 1≤푡 ≤푇 is independent
of the preceding and following labels, given the image 푥. Therefore, Eq. (7.24)
becomes:
Ψ˜ (푣, 푥)
★
= max
푐푖:푗
Υ(푐푖:푗 )=푣
1≤푖≤ 푗≤푇
Õ
푐1:푖−1
Õ
푐푗+1:푇
푃(푐 | 푥) (7.26)
Eqs. (7.24) or (7.26) compute the maximum probability of a subpath that is
CTC-aligned with 푣. But the maximization in these equations prevents a proper and
accurate probabilistic interpretation of the resulting score Ψ˜ (푣, 푥). However, if path
probabilities are added rather than maximized, from Eq. (7.26) we can write:14
Ψ˜ (푣, 푥) ≈ Õ
푐푖:푗
Υ(푐푖:푗 )=푣
1≤푖≤ 푗≤푇
Õ
푐1:푖−1
Õ
푐푗+1:푇
푃(푐 | 푥) =
Õ
푐: 푣⊆Υ(푐)
푃(푐 | 푥)
def = Ψ(푣, 푥) (7.27)
which more faithfully describes the probability aimed at. In fact, it can be easily seen
that Ψ(푣, 푥) is a true RP:
Ψ(푣, 푥) =
Õ
푐: 푣⊆Υ(푐)
푃(푐 | 푥) =
Õ
푐: 푣⊆Υ(푐)
Õ
푤
푃(푤, 푐 | 푥) =
Õ
푤: 푣⊆푤
Õ
푐
푃(푤, 푐 | 푥)
=
Õ
푤: 푣⊆푤
푃(푤 | 푥) = 푃(푅 | 푣, 푥) (7.28)
Here we have proceed as in Sec. 4.4.3.3, Eq. (4.14), but now marginalizing on the
possible transcripts 푤 of 푥 . This leads to the RP defined in Eq. (3.24) of Sec. 3.4.
Finally, due to essentially the same reasons as in the HMM-Filler (Sec. 7.4), the
following heuristic was introduces in [13] to mitigate the fact that long keywords
tend to produce lower scores than short ones:
푆CTC
def
=
1
|푣|
log Ψ˜ (푣, 푥) (7.29)
Notice that the BLSTM-CTC KWS model implicitly assumes that each label
푐푡
, 1≤푡 ≤푇 is independent of the preceding and following labels, given the image 푥.
Now suppose that a keyword has 10 characters. Thus, it needs to be aligned at least
with 10 character-level labels. If the probability of the correct label at each position
is for example 0.9, then the probability of the subpath corresponding to the keyword
drops to 0.9
10 ≈ 0.35. Again, this is only due to the independence assumptions of
the CTC model, and the use of 푛-gram language models (as in our approaches) in
addition to RNNs (or HMMs) largely makes such a heuristic unnecessary.
14 Note that when writing 푣 ⊆ Υ(푐1:푇 ) and 푣 ⊆ 푤, the operator “⊆” is overloaded as in footnote (6)
.7.5 BLSTM-CTC KWS 209
7.5.1 BLSTM-CTC KWS Interpretation: Experimental Validation
BLSTM-CTC KWS results were already reported in Table 6.10 of Sec. 6.7.3 for
three popular benchmarking datasets, IAM, GW, and PAR. Also, in Table 6.18 of
(Sec. 6.10.1 results on the Bentham dataset used in the ICDAR2015 KWS com￾petition were reported for a for a method very similar to BLSTM-CTC KWS (row
“Team 1”). These results are summarized in Table 7.6, in comparison with using the
RP computed as discussed in this book (Eqs. (3.24) or (7.28)).
Table 7.6: Comparative gAP results (in %) between BLSTM-CTC KWS and the prob￾abilistic interpretation proposed in this book. Results are reported for four benchmark
datasets (see Chapter 6).
Approach IAM GW PAR Bentham
BLSTM-CTC KWS 78 85 94 87
PrIx interpretation 95 95 98 92
These results clearly show the superiority of our probabilistic interpretation of the
ideas of [13]. Nevertheless, it should be taken into account that the improvements
were achieved in part thanks to the use of character LMs, which help better modeling
the underlying transcript posterior distribution. Note, however, that LMs can not be
straightforwardly used along with the methods proposed in [13].210 7 Probabilistic Interpretation of Traditional KWS Approaches
References
1. Aldavert, D., Rusinol, M., Toledo, R., Llad ˜ os, J.: A study of Bag-of-Visual-Words represen- ´
tations for handwritten keyword spotting. International Journal on Document Analysis and
Recognition (IJDAR) 18(3), 223–234 (2015). DOI 10.1007/s10032-015-0245-z
2. Almazan, J., Gordo, A., Forn ´ es, A., Valveny, E.: Efficient Exemplar Word Spotting. In: ´
Proceedings of the British Machine Vision Conference, pp. 67.1–67.11. BMVA Press (2012).
DOI http://dx.doi.org/10.5244/C.26.67
3. Almazan, J., Gordo, A., Forn ´ es, A., Valveny, E.: Word Spotting and Recognition with Em- ´
bedded Attributes. IEEE Transactions on Pattern Analysis and Machine Intelligence 36(12),
2552–2566 (2014). DOI 10.1109/TPAMI.2014.2339814
4. Biau, G., Devroye, L.: Weighted k-nearest neighbor density estimates. In: Lectures on the
Nearest Neighbor Method, pp. 43–51. Springer (2015)
5. Bird, C.L., Chapman, S.G., Ibbotson, J.B.: Content-driven navigation of large databases. In:
IEEE Coll. on Intelligent Image Databases, pp. 13/1–13/5 (1996). DOI 10.1049/ic:19960751
6. Chia, T.K., Sim, K.C., Li, H., Ng, H.T.: Statistical lattice-based spoken document retrieval.
ACM Trans. Inf. Syst. 28(1), 2:1–2:30 (2010)
7. Duda, R.O., Hart, P.E.: Pattern Classification and Scene Analysis. J. Wiley and Sons, (1973)
8. Duda, R.O., Hart, P.E., Stork, D.G.: Pattern Classification, 2nd edition edn. Wiley-Interscience,
New York, NY, USA (2000)
9. Fischer, A., Frinken, V., Bunke, H., Suen, C.Y.: Improving HMM-Based Keyword Spotting
with Character Language Models. In: 2013 12th International Conference on Document
Analysis and Recognition, pp. 506–510 (2013). DOI 10.1109/ICDAR.2013.107
10. Fischer, A., Keller, A., Frinken, V., Bunke, H.: HMM-based Word Spotting in Handwritten
Documents Using Subword Models. In: 20th International Conference on Pattern Recognition
(ICPR), pp. 3416 –3419 (2010). DOI 10.1109/ICPR.2010.834
11. Fischer, A., Keller, A., Frinken, V., Bunke, H.: Lexicon-free handwritten word spotting using
character HMMs. Pattern Recognition Letters 33(7), 934 – 942 (2012). DOI 10.1016/j.patrec
.2011.09.009. Special Issue on Awards from ICPR 2010
12. Fiscus, J.G., Ajot, J., Garofolo, J.S., Doddingtion, G.: Results of the 2006 spoken term de￾tection evaluation. In: Proceedings of ACM SIGIR Workshop on Searching Spontaneous
Conversational Speech, pp. 51–55 (2007)
13. Frinken, V., Fischer, A., Manmatha, R., Bunke, H.: A Novel Word Spotting Method Based on
Recurrent Neural Networks. IEEE Transactions on Pattern Analysis and Machine Intelligence
34(2), 211–224 (2012). DOI 10.1109/TPAMI.2011.113
14. Garofolo, J.S., Auzanne, C.G., Voorhees, E.M.: The trec spoken document retrieval track: A
success story. NIST special publication SP (246), 107–130 (2000)
15. Giotis, A.P., Sfikas, G., Gatos, B., Nikou, C.: A survey of document image word spotting
techniques. Pattern Recognition 68, 310–332 (2017)
16. Hazen, T.J., Shen, W., White, C.: Query-by-example spoken term detection using phonetic
posteriorgram templates. In: 2009 IEEE Workshop on Automatic Speech Recognition Under￾standing, pp. 421–426 (2009). DOI 10.1109/ASRU.2009.5372889
17. Jaderberg, M., Vedaldi, A., Zisserman, A.: Deep Features for Text Spotting. In: D. Fleet,
T. Pajdla, B. Schiele, T. Tuytelaars (eds.) Computer Vision – ECCV 2014, pp. 512–528.
Springer International Publishing (2014)
18. Johansson, S., Leech, G., Goodluck, H.: Manual of Information to Accompany the Lancaster￾Olso/Bergen Corpus of British English, for Use with Digital Computers. Tech. rep., Department
of English, University of Oslo (1978)
19. Lujan-Mares, M., Tamarit, V., Alabau, V., Mart ´ ´ınez-Hinarejos, C.D., Pastor, M., Sanchis, A.,
Toselli, A.H.: iATROS: A speech and handwritting recognition system. In: V Jornadas en
Tecnologıas del Habla (VJTH’2008), pp. 75–78 (2008)
20. Mondal, T., Ragot, N., Ramel, J.Y., Pal, U.: Flexible Sequence Matching technique: An effective
learning-free approach for word spotting. Pattern Recognition 60, 596–612 (2016). DOI
https://doi.org/10.1016/j.patcog.2016.05.011References 211
21. Mondal, T., Ragot, N., Ramel, J.Y., Pal, U.: Comparative study of conventional time series
matching techniques for word spotting. Pattern Recognition 73, 47–64 (2018). DOI https:
//doi.org/10.1016/j.patcog.2017.07.011
22. Puigcerver, J.: A probabilistic formulation of keyword spotting. Ph.D. thesis, Universitat
Politecnica de Val ` encia (2018) `
23. Puigcerver, J., Toselli, A.H., Vidal, E.: Probabilistic interpretation and improvements to the
HMM-filler for handwritten keyword spotting. In: 13th Int. Conf. on Document Analysis and
Recognition (ICDAR), pp. 731–735 (2015). DOI 10.1109/ICDAR.2015.7333858
24. Puigcerver, J., Toselli, A.H., Vidal, E.: Probabilistic interpretation and improvements to the
hmm-filler for handwritten keyword spotting. In: 2015 13th International Conference on
Document Analysis and Recognition (ICDAR), pp. 731–735. IEEE (2015)
25. Rath, T.M., Manmatha, R., Lavrenko, V.: A Search Engine for Historical Manuscript Images.
In: Proceedings of the 27th Annual International ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ’04, pp. 369–376. ACM, New York, NY, USA
(2004). DOI 10.1145/1008992.1009056
26. Retsinas, G., Louloudis, G., Stamatopoulos, N., Gatos, B.: Keyword Spotting in Handwritten
Documents Using Projections of Oriented Gradients. In: 2016 12th IAPR Workshop on
Document Analysis Systems (DAS), pp. 411–416 (2016). DOI 10.1109/DAS.2016.61
27. Rodriguez-Serrano, J.A., Perronnin, F., Llados, J., Sanchez, G.: A similarity measure be￾tween vector sequences with application to handwritten word image retrieval. In: 2009 IEEE
Conference on Computer Vision and Pattern Recognition, pp. 1722–1729 (2009). DOI
10.1109/CVPR.2009.5206783
28. Rohlicek, J.R., Russell, W., Roukos, S., Gish, H.: Continuous hidden Markov modeling for
speaker-independent word spotting. In: International Conference on Acoustics, Speech, and
Signal Processing, vol. 1, pp. 627–630 (1989). DOI 10.1109/ICASSP.1989.266505
29. Rose, R.: Keyword detection in conversational speech utterances using hidden Markov model
based continuous speech recognition. Computer Speech & Language 9(4), 309–333 (1995).
DOI https://doi.org/10.1006/csla.1995.0015
30. Rose, R.C., Paul, D.B.: A hidden Markov model based keyword recognition system. In:
International Conference on Acoustics, Speech, and Signal Processing, vol. 1, pp. 129–132
(1990). DOI 10.1109/ICASSP.1990.115555
31. Sfikas, G., Retsinas, G., Gatos, B.: Zoning Aggregated Hypercolumns for Keyword Spotting.
In: 2016 15th International Conference on Frontiers in Handwriting Recognition (ICFHR), pp.
283–288 (2016). DOI 10.1109/ICFHR.2016.0061
32. Silverman, B.W.: Density estimation for statistics and data analysis, vol. 26. CRC press (1986)
33. Smeulders, A.W.M., Worring, M., Santini, S., Gupta, A., Jain, R.: Content-Based Image
Retrieval at the End of the Early Years. IEEE Trans. Pattern Anal. Mach. Intell. 22(12),
1349–1380 (2000). DOI 10.1109/34.895972
34. Sudholt, S., Fink, G.A.: PHOCNet: A Deep Convolutional Neural Network for Word Spotting in
Handwritten Documents. In: 2016 15th International Conference on Frontiers in Handwriting
Recognition (ICFHR), pp. 277–282 (2016). DOI 10.1109/ICFHR.2016.0060
35. Sudholt, S., Fink, G.A.: Evaluating Word String Embeddings and Loss Functions for CNN￾Based Word Spotting. In: 2017 14th IAPR International Conference on Document Analysis
and Recognition (ICDAR), vol. 01, pp. 493–498 (2017). DOI 10.1109/ICDAR.2017.87
36. Toselli, A.H., Puigcerver, J., Vidal, E.: Context-aware lattice based filler approach for key word
spotting in handwritten documents. In: 2015 13th Int. Conference on Document Analysis and
Recognition (ICDAR), pp. 736–740 (2015). DOI 10.1109/ICDAR.2015.7333859
37. Toselli, A.H., Puigcerver, J., Vidal, E.: Two Methods to Improve Confidence Scores for Lexicon￾Free Word Spotting in Handwritten Text. In: 2016 15th International Conference on Frontiers
in Handwriting Recognition (ICFHR), pp. 349–354 (2016)
38. Toselli, A.H., Romero, V., Sanchez, J.A., Vidal, E.: Making two vast historical manuscript col- ´
lections searchable and extracting meaningful textual features through large-scale probabilistic
indexing. In: Int. Conf. on Document Anal. and Recogn. (ICDAR), pp. 108–113. IEEE (2019)212 7 Probabilistic Interpretation of Traditional KWS Approaches
39. Toselli, A.H., Vidal, E.: Fast HMM-Filler Approach for Key Word Spotting in Handwritten
Documents. In: 2013 12th International Conference on Document Analysis and Recognition,
pp. 501–505 (2013). DOI 10.1109/ICDAR.2013.106
40. Toselli, A.H., Vidal, E., Romero, V., Frinken, V.: HMM Word-Graph Based Keyword Spotting
in Handwritten Document Images. Information Sciences 370(C), 497–518 (2016). DOI
10.1016/j.ins.2016.07.063
41. Toshikazu, K., Takio, K., Hiroyuki, S.: Intelligent Visual Interaction with Image Database
Systems : Toward the Multimedia Personal Interface. Journal of Information Processing 14(2),
134–143 (1991)
42. Wang, C., Zhang, P.: Optimization of spoken term detection system. Journal of Applied
Mathematics 2012(548341), 1–8 (2012)
43. Young, S., Evermann, G., Gales, M., Hain, T., Kershaw, D., Liu, X., Moore, G., Odell, J.,
Ollason, D., Povey, D.: The HTK book. Tech. rep., Cambridge Univ. Engineering Dep. (2002)Chapter 8
Probabilistic Indexing Search Extensions
Abstract Except for a brief section at the end of Chapter 5, so far in this book PrIx
has been studied assuming queries are just single, individual words. However, in
the practical PrIx applications, users expect to be allowed to formulate usual forms
of free-text searching queries. Typically, these forms encompass multiple words,
presented as word sequences and/or logically combined using Boolean operators.
Moreover, flexible word spelling is also typically expected, mainly to cope with
the user incognizance of exactly how a given word could have been written in the
manuscripts – but also to deal with the system (or human) uncertainty as to which
pen strokes were actually used to depict some characters or words. In this chapter
we consider in detail these and other related topics, from a mostly practical point
of view which puts effectiveness and efficiency issues before perfect mathematical
formulation or exactness of the equations.
8.1 Multi-Word Boolean and Word-Sequence Queries
In this section we get back to the topic of multi-word query search in document
image collections, that was briefly considered in Sec. 5.7. That section comparatively
illustrated how the RPs of queries consisting in AND / OR word combinations can be
exactly computed using FSTs and CLs, and (in Table 5.1) how the exact RPs can be
approximated using single-word PrIxs and Frechet bounds ´ (explained in Sec. 3.7.4).
It was argued that the exact method entailed two main practical problems: computing
time, because of expensive WFST compositions, and large storage demands, because
of the need to store CLs, rather than simple PrIx spots. Here we further add that the
formal developments of Sec. 5.7 were confined to an assumedly small image region
(typically a line) for which a sufficiently small WG or character lattice is provided.
But, in practice, it is generally needed that Boolean expressions span whole page
images (or even larger, multi-page manuscript sections). Specially this latest issue
renders useless the exact RP computation approach discussed in Sec. 5.7, favoring
instead the approximations based on the Frechet bounds. ´
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 213
A. H. Toselli et al., Probabilistic Indexing for Information Search and Retrieval 
in Large Collections of Handwritten Text Images, The Information Retrieval Series 49, 
https://doi.org/10.1007/978-3-031-55389-9_8 
 214 8 Probabilistic Indexing Search Extensions
Thus, aiming at general search for information in large text image collections,
for simplicity and without loss of generality, we assume that Boolean multi-word
combinations span whole page-images. But note that, if needed, the same ideas and
methods would also work for larger, multi-page units, such as chapters, books, or
full collections.
Boolean multi-word search can be implemented using any of the word segment
or ordinal position PrIxs explained in Chapter 5. First, each word of the query is
searched for separately, obtaining a set of spots where each word is likely to appear
Then, set union, intersection and complement operations are applied to the resulting
single-word sets of spots to obtain the resulting set of spots that match the given
Boolean combined query. Yet, this still needs proper ways to combine the single￾word RPs into the overall RP of the Boolean query, and then check whether the
combined RP is higher than the given search threshold.
To this end, we call the Frechet bounds to obtain approximate RP values. First we ´
simplify the notation of the RP of a given image region 푥 and word 푣 as 푃(푅 | 푥, 푣) ≡
푃(푅푣), where the image region 푥 is assumed to be understood from the context.
Now, we are interested in queries that are Boolean combinations of several key￾words, 푣1, ..., 푣푀, using the three basic Boolean operators: OR, AND and NOT, respec￾tively denoted as “∨”, “∧” and “¬”. The relevance of a page image 푋 for an 푀-fold
AND query is then written as 푅푣1 ∧ 푅푣2
· · · ∧ 푅푣푀 , or just 푅1 ∧ 푅2 · · · ∧ 푅푀, for the
sake of further simplifying notation. Similarly, the event for an OR query is denoted
as 푅1 ∨ 푅2 · · · ∨ 푅푀.
As discussed in Sec. 3.7.4, the early works of Boole and Frec´ het [3, 9, 8], provide
simple bounds for probabilities of events combined with logical operators. Using the
notation adopted here, the bounds stated in Eqs. (3.51, 3.52) become:
max 
0,
Õ
푀
푖=1
푃(푅푖)−푀+1

≤ 푃(푅1 ∧ 푅2 · · · ∧ 푅푀) ≤ min
1≤푖≤푀
푃(푅푖) (8.1)
max
1≤푖≤푀
푃(푅푖) ≤ 푃(푅1 ∨ 푅2 · · · ∨ 푅푀) ≤ min 
1,
Õ
푀
푖=1
푃(푅푖)

(8.2)
In the illustrative examples of Sec. 5.7 (Table 5.1) it was suggested that, from these
bounds, the conjunction and disjunction combinations would be better approximated
by their upper and lower bounds, respectively; that is:
푃(푅1 ∧ 푅2 · · · ∧ 푅푀) ≈ min
1≤푖≤푀
푃(푅푖) (8.3)
푃(푅1 ∨ 푅2 · · · ∨ 푅푀) ≈ max
1≤푖≤푀
푃(푅푖) (8.4)
In addition, the relevance probability of the NOT operator applied to a Boolean query
combination, 퐵, is just computed as:
푃(¬퐵) = 1 − 푃(퐵) (8.5)
To have a better idea of how good are the approximations of Eqs. (8.3–8.4) in
practice, we computed the bounds (8.1) and (8.2), for a large number of random8.1 Multi-Word Boolean and Word-Sequence Queries 215
two-fold (i.e., 푀 = 2) AND and OR queries. The single-word RPs, 푃(푅푖), were those
obtained when computing the HMM-based PrIxs for the experiments described in
Sec. 6.8 with the BEN1 dataset (see Appendix C.2.1). Fig. 8.1 shows scatter plots of
these upper versus lower bounds (푏푙 vs 푏푢) for the AND and OR queries.
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
AND
Upper bound (bu)
Lower bound (bl)
0 0.2 0.4 0.6 0.8 1
OR
Lower bound (bl)
100
101
10
2
103
104
105
106
107
0 0.1 0.2 0.3 0.4 0.5
AND
Frequency
∆ = bu − bl
0 0.1 0.2 0.3 0.4 0.5
OR
∆ = bu − bl
Fig. 8.1: Scatter plots of lower versus upper bounds, 푏푙
, 푏푢, where 푏푙 = max ￾
0, 푃(푅1 ) +
푃(푅2 ) − 1

and 푏푢 = min ￾
푃(푅1 ), 푃(푅2 )

for word-pair AND queries (Eq. (8.1)); and
푏푙 = max ￾
푃(푅1 ), 푃(푅2 )

and 푏푢 = min ￾
1, 푃(푅1 ) + 푃(푅2 )

for word-pair OR queries
(Eq. (8.2)). Corresponding AND/OR frequency histograms of values of Δ = 푏푢 − 푏푙 are also
shown. The word pairs were randomly sampled from the pool described in Table C.6 of
Appendix C, with 푟 = 1 (equal number of test-set pertinent and non-pertinent words). For
more than 97% of the events, the upper-lower gap is very low. Δ ≤ 0.01.
Due to the constrains imposed by the inequalities (8.1) and (8.2), all points fall
within triangular regions. But what is more interesting is that most of points tend
to concentrate near to their corresponding diagonal edges. This implies that the
majority of pairs, 푏푙 and 푏푢 have similar values. This is more explicitly seen in the
two histograms of the gap between upper and lower bounds, Δ
def
= 푏푢 − 푏푙
, which show
that Δ is very close to zero (Δ ≤ 0.01) in more than 97% of AND and OR eve216 8 Probabilistic Indexing Search Extensions
This behavior is explained in part by the observed tendency of RP values to be
close to zero or to one. In this case, from Eqs. (8.1)) and (8.2) it is easy to see that,
if any of 푃(푅1) or 푃(푅2) is either close to one or to zero, the corresponding values
of Δ for 푅1 ∧ 푅2 and 푅1 ∨ 푅2 must be close to zero.
These results serve to empirically show that the approximation error of Eqs. (8.3–
8.4) is expected to be (very) small for the vast majority of possible AND / OR queries
that can be issued in a typical dataset such as BEN1.
Using Eqs. (8.3–8.4) and (8.5)), the approximate RP of any arbitrary Boolean
combination of single-word queries can be easily and very efficiently computed. For
example, to search for images containing both the words “cat” and “dog”, but none
of the words “mouse” or “rabbit” the relevance probability will be computed as:
푃(푅1 ∧ 푅2 ∧ ¬(푅3 ∨ 푅4)) ≈ min ￾
푃(푅1), 푃(푅2), 1 − max(푃(푅3), 푃(푅4))
where the events 푅1, 푅2, 푅3 and 푅4 correspond to the keywords “cat”, “dog”,
“mouse” and “rabbit”, respectively.
Using these approximations, word-sequence (or “phrase”) queries can also be
computed easily. In this case, additional position information is needed to ensure that
the words considered follow each-other in sequence. Therefore, a positional PrIx for
individual words is needed, which can be computed using the algorithms described
in Sec. 5.3.4 or Sec. 5.6.2 for lexicon-based or lexicon-free lattices, respectively.
This approach is applied below (Sec. 8.2) to support queries that represent melodic
patterns in images of sheet music medieval manuscripts.
8.1.1 Experiments
To validate the approaches described above, a series of experiments with AND / OR
queries were carried out. All the details of these experiments and the corresponding
results were reported in [15] and are summarized hereafter.
Since multi-word Boolean queries typically assume the page image as the target
search region, the evaluation in this experiment was done at page level. That is,
the goal was to determine whether or not a complete page is relevant for the given
(Boolean) query. A given page is relevant for a multi-word AND query if all the query
words are written in the page. Similarly, a page is relevant for an OR query if any of
the query words is written in the page. The corresponding RPs were computed using
the Frechet approximations described above. ´
The page-level partition of the BEN1 Bentham dataset described in Ap￾pendixC.2.1.2 was used. The single-word query set adopted in this partition consisted
of 3 293 words whose frequency of occurrence in the training set ranged from 2 to
10. This avoided including most stop words (generally with word frequencies greater
than 10) and also many (singleton) words that were unlikely to appear in the test set.
Multi-word queries were pairs of distinct words sampled from the single-word query
set. In total there were 5 420 278 possible word-pair queries, to be searched for in 33
test pages8.1 Multi-Word Boolean and Word-Sequence Queries 217
Despite the large number of queries, only a few had some relevant page in the
test set. In the case of the single-word queries, only 674 words appeared in the test
images. Similarly, for the multi-word AND and OR queries, only 1 992 007 and 11 784
had some relevant page, respectively. We call these queries pertinent, and the queries
without any relevant page in the test set, non-pertinent. For more details about these
query sets refer to Table C.6 in Appendix C.2.1.2.
In the original publication [15], we studied the performance for the different
query types (singletons, AND, and OR) as the ratio, 푟, of non-pertinent with respect
to pertinent queries increases. However, for the sake of brevity, here we only report
the results for 푟 = 0 (all queries were pertinent) and for 푟 = 1 (the same amount of
non-pertinent and pertinent queries).
HMMs and word 2-grams were used to compute (lexicon-based) PrIxs. The
models were essentially the same as those used in Sec. 6.8, but here we also applied
discriminative training to improve HMM training [17, 14, 15].
Since we operated at page level (without caring about the actual location of the
words within the page), we started computing a page-level single-word PrIx. To
this end, we first computed line-level word RPs and then relied once again on the
Frechet ´ OR approximation by considering that a page 푋 is relevant for a word 푣, if it
is relevant for the word in any line-region 푥 of 푋 [15]; that is,
푃(푅 | 푋,푣) = max
푥⊑푋
푃(푅 | 푥, 푣) (8.6)
Then, the RP for a word-pair Boolean query was simply computed by the “min” and
“max” Frechet approximations, as explained before. ´
Results are shown in Table 8.1 for the two types of query sets discussed above:
푟 = 0 and 푟 = 1 For 푟 = 0, the single-word, AND-word-pair and OR-word-pair query
set sizes were 674, 11 784 and 1 992 007, while for 푟 = 1 they were 1 348, 23 568
and 3 984 014 respectively. Only the gAP was measured, since the mAP cannot be
computed for non-pertinent query sets (see Sec. 1.6).
Table 8.1: PrIx gAP (%) obtained for single-word and Boolean AND an OR queries, using
the BEN1 Bentham dataset. The ratio 푟 = 0 corresponds to the a query set where all the
queries are pertinent and 푟 = 1 to a query set with the same number of non-pertinent and
pertinent queries.
Query set Single AND OR
r = 0 94.6 93.1 93.5
r = 1 92.7 91.9 90.9
As shown in the table, the performance was excellent for all types of queries
(single-word, AND, OR), in both query sets reported. The most conventional single￾word experiment corresponds to 푟 = 0 (all query words and word-pairs are relevant).
In this case, when moving from single-word search to Boolean queries, AP perfor￾mance remains essentially unchanged. The behavior is similar for 푟 = 1. But as 푟
was increased further, performance for single-word, OR and (to a lesser extent) AND
queries tended to fall noticeably [15].218 8 Probabilistic Indexing Search Extensions
It is important to understand, however, that from a practical point of view, even
a ratio of non-pertinent to pertinent queries equal to 1 is already quite unlikely:
it would correspond to the use of an information retrieval system where half of
the queries try to find information which does not exist in the indexed collection.
Therefore, practical performance is expected to be closer to the results for 푟 = 0.
8.2 Searching for Music Symbol Sequences
Many archives and libraries around the world safeguard vast collections of historical
sheet music manuscripts. Once these manuscripts are scanned into digital images,
indexing the resulting image collections for plaintext search can be done following
essentially the same PrIx workflow described throughout this book for text document
images [5, 7]. Examples of music sheet images of medieval plainchant manuscripts
can be seen in Appendix C.11.
One of the main differences between text and music notation is that the usual
concept of “word” in a text document does do not exist in sheet music documents.
Therefore, only individual music symbols (which typically amount to a few or
maybe a few dozens different symbols) can be indexed. That is, rather than indexing
the words (which here are unknown) the elementary symbols (or “alphabet”) is
directly indexed. And, of course, only positional indexing makes sense, since all
possible music symbols often appear in any single regular stave of a sheet music
image. Correspondingly, it makes little sense to search for individual music symbols,
because each of them is prone to be found in most or all staves of all sheet music
images of any given collection.
In fact, the objects of interest for IR in sheet music collections are not individual
music symbols, but music symbol sequences, which may be used to denote or specify
melodic patterns, which would loosely correspond in music to the concept of words
in text documents. But since these “words” are actually nonexistent, the search must
be resolved at query time by means of music symbol sequence queries.
As discussed in Sec. 8.1, using positional PrIxs, symbol sequence queries can
be fairly straightforwardly approached in general by means of Boolean AND symbol
combinations, along with position reasoning. This approach was first presented in [7],
along with the experimental work that will be outlined below.
Formally speaking, the fundamental question here is: given a staff image region
푥 and a music symbol sequence query 푠 = 푠1, . . . , 푠푚, is 푠 written in 푥? The staff
image 푥 is considered relevant to the query 푠 iff the answer is “yes”. The following
probabilistic formulation provides a principled way of estimating the RP that a staff
region 푥 is relevant for a query sequence 푠, 푃(푅 | 푥, 푠).
As in the previous section, let us first simplify the notation and denote the proba￾bility that the sequence 푠 is found starting at the 푘-th ordinal position of a transcript
of 푥, as 푃(푅푠,푘)
def
= 푃(푅 | 푥, 푠, 푘), where 푥 is assumed to be understood from the
context. Similarly, we denote the RP of a single symbol 푣 at a specific position 푘, as
푃(푅푣,푘)
def
= 푃(푅 | 푥, 푣, 푘). So, if 푣 is the i-th symbol of 푠, 푃(푅푠푖
,푘)
def
= 푃(푅 | 푥, 푠푖
, 푘).8.2 Searching for Music Symbol Sequences 219
Then, 푃(푅 | 푥, 푠) can be computed as the probability that 푠 is found in any of the
possible ordinal positions:
푃(푅 | 푥, 푠) = 푃(푅푠,1 ∨ . . . ∨ 푅푠,퐾−푚+1) ≈ max
1≤푘≤퐾−푚+1
푃(푅푠,푘)
where 퐾 is the maximum indexed ordinal position for 푥. On the other hand, 푃(푅푠,푘)
(the RP of the sequence 푠 starting at position 푘) can be computed as:
푃(푅푠,푘) = 푃(푅푠1,푘 ∧ . . . ∧ 푅푠푚,푘+푚−1) ≈ min
1≤푖≤푚
푃(푅푠푖
,푘+푖−1)
In these equations, OR and AND RPs are computed using the corresponding Frechet ´
approximations, as discussed in Sec. 8.1. Therefore:
푃(푅 | 푥, 푠) ≈ max
1≤푘≤퐾−푚+1
min
1≤푖≤푚
푃(푅푠푖
,푘+푖−1) (8.7)
In this equation, the terms 푃(푅푠푖
,푘+푖−1) ≡ 푃(푅 | 푥, 푠푖
, 푘+푖−1), 1 ≤ 푘 ≤ 퐾, 1 ≤ 푖 ≤ 푚,
are given by the corresponding single-symbol ordinal position-dependent PrIx spots
of 푥. Note that this formulation only computes the RP of one occurrence (the most
probable one) of the symbol sequence 푠 in each staff-region image 푥.
8.2.1 Experiments
To validate the proposed approach, experiments have been carried out on the histori￾cal sheet music manuscript referred to as the Vorau-253 dataset in Appendix.C.11.
First, staves were extracted following methods similar to those discussed in [4] and
the few segmentation errors were manually fixed. 1 097 of the resulting staves were
annotated with GT transcripts. From these, 1 000 staves were used for training and
97 were considered as the test set (see Table C.25).
To build the required symbol ordinal position-dependent PrIx, first a CRNN
optical model was trained on gray-scale staff images with a fixed height of 128
pixels, along with their corresponding GT transcripts (see details in Appendix C.11
and in [7]). Training was carried out with the TensorFlow deep-learning platform,
using the CRNN architecture detailed in Table 8.2 and a training set-up adopted
from a previous work [6].
In addition. a music-symbol 2-gram LM was trained from the training transcripts
with the smoothing strategy proposed in [10]. Then, using these models and the
Kaldi decoder, music-symbol lattices were produced for all the 97 test-set staff
images Finally, a symbol ordinal position PrIx was obtained using the Alg. 5.7.
The query set was established as follows: all the 615 sequences with lengths
ranging from 3 to 15 which appear in the test set more than once were selected as
symbol sequence queries. Shorter sequences or sequences that appear less than 3
times were deemed coincidental and not representing relevant melodic patterns of
the corpus.220 8 Probabilistic Indexing Search Extensions
Table 8.2: CRNN architecture used for optical modeling of sheet music staves.
Block Configuration Values
Convolutional Number of layers 4
Activation ReLU
Convolutional filters {64, 64, 128, 128}
Filter sizes {5×5, 5×5, 3×3, 3×3}
Max. pooling {2×2, 2×2, 2×1, 2×1}
Batch normalization no
Recurrent Number of layers 2
Type BLSTM
Units {256, 256}
Dropout {0.5, 0.5}
Output layer Units (characters plus ∅) 20
Dropout 0.0
Using Eq. (8.7) to compute the RPs for these 615 music sequences, the PrIx gAP
and mAP performance results were 85.6% and 92.3%, respectively. The correspond￾ing R–P curve is shown in Fig. 8.2.
0 0.2 0.4 0.6 0.8 1
0
0.2
0.4
0.6
0.8
1
Recall: 휌
Precision:
휋
gAP: 85.6% (mAP: 92.3%)
Fig. 8.2: R–P curve for music symbol sequence queries on the Vorau-235 dataset.
These good results predicted an excellent degree of search and retrieval usability
in practical applications which require a reliable way of finding melodic patterns in
untranscribed medieval plainchant manuscripts. This can be experienced first hand
in a demonstrator that is publicly available through the Internet.1
1 https://www.prhlt.upv.es/htr/music8.3 Structured Queries for Information Retrieval in Table Images 221
8.3 Structured Queries for Information Retrieval in Table Images
As seen in Sec. 8.1, a conventional PrIx (computed for individual words) provides
adequate support for queries consisting of Boolean (AND, OR, NOT) combinations
of multiple keywords. These queries were assumed to be position-independent at the
full page level. But, using ordinal position PrIxs, the Boolean multi-word queries
were easily extended to support word (or symbol) sequence queries, by just taking
into account the likely relative positions of the words involved in an AND query. Here
these ideas are further stretched to take into account likely geometrical positions
derived from segment-dependent PrIx (see Sec. 3.5.2 and 5.6).
This way we provide support for structured multi-word queries, aimed at infor￾mation retrieval in text images containing tabular data. The content of this section
is based on the work originally published in [11].
To ground the presentation on typical table images, we focus on untranscribed
handwritten tables of the Passau dataset (PAS, see Appendix C.6). On this image
collection, we aimed to support queries of the form:
“⟨column-heading, column-content⟩”
where column-heading is an AND combination of table heading words and
column-content is a (single) keyword. For example, the query “⟨NAMEN VER￾STORBENEN, WOLF⟩” (“deceased name, Wolf” in English), should retrieve the word
“WOLF” which appears in the third row of the first column of the image in Fig. 8.3.
Fig. 8.3: Example of geometric reasoning for the column-wise multi-word structured
query: “⟨NAMEN VERSTORBENEN, WOLF⟩”.
The retrieval process is carried out in two steps for each table image. First,
column-heading words with RP higher than the given threshold 휏 are retrieved.
Simple geometric restrictions are applied in this step: candidate spots must be close
enough to each other and all must loosely be located in upper regions of the image.
This is supported by the location information (word BBs) stored in the image PrIx.222 8 Probabilistic Indexing Search Extensions
Let H
def
={ℎ1, ℎ2 . . . , ℎ퐼 } be the set of column-heading query words. To simplify
notation as in previous sections, 푅ℎ푖
is understood as the Boolean event that ℎ푖
is
relevant for some image region. So, 푃(푅ℎ푖
)
def
= 푃(푅 | 푥, ℎ푖) denotes the RP of the
image 푥 for the heading word ℎ푖
. Note that repeated instances or spots of some of
these query words may be retrieved in the image region of a column-heading. Let
풃푖,1, . . . , 풃푖,퐽푖
denote the 퐽푖≥1 different BBs retrieved for the heading word ℎ푖
, and
let 푃(푅ℎ푖 푗 )
def
= 푃(푅 | 푥, ℎ푖
, 풃푖 푗) be the RP of the word ℎ푖
in the image location 풃푖 푗 of
푥. Then, according to Sec. 8.1, the RP of the AND combination for the words in H,
along with the OR combination of instances of each word, is computed as:
푃(푅H) = 푃(푅ℎ1 ∧ 푅ℎ2
· · · ∧ 푅ℎ퐼
) ≈ min
1≤푖≤퐼
푃(푅ℎ푖
) ≈ min
1≤푖≤퐼
max
1≤ 푗≤퐽푖
푃(푅ℎ푖 푗 ) (8.8)
In a second step, the column-content query word 푣 is searched for through the
column-wise regions delimited by the horizontal span of the spotted column-heading
word BBs (see Fig. 8.3). Again, only spots with RP higher then 휏 are retrieved and
the search is geometrically supported by the location information stored in the image
PrIx. Let 풃1, . . . , 풃퐾 be the image locations of the 퐾 spots retrieved for the column￾content word 푣 and let 푃(푅푣,푘)
def
= 푃(푅 | 푥, 푣, 풃푘) be the RP of the 푘-th spot. From
the discussion in Sec. 8.1, the RP of the column-content word 푣 in the considered
column is computed as:
푃(푅푣) ≈ max
1≤푘≤퐾
푃(푅푣,푘) (8.9)
Finally, again according to Sec. 8.1, the RP of a column-wise structured multi-word
query is approximated as:
푃(푅⟨H,푣⟩) = 푃(푅H ∧ 푅푣) ≈ min ￾
푃(푅H), 푃(푅푣)

(8.10)
Following the same ideas, more complex Boolean combinations can be straight￾forwardly supported. Row-wise and word-sequence combinations can be supported
too by using the location information of the spotted words. However, the study of
these more complex queries was left for future research.
8.3.1 Experiments
Empirical evaluation of structure multi-word query search was done using a lexicon￾free word segment PrIx. Specifically, we use here exactly the same PrIx produced
for the experiments with conventional, single-word queries presented in Sec. 6.9.1,
using the PAS dataset described in Appendix C.6.
In order to asses the methods discussed above for IR in table image collections, a
specific query set was provided by experts of the Passau Diocesan Archives with 363
queries of the form “⟨column-heading, column-content⟩”. The queries were
aimed at finding real information which was likely to exist in some of the Passau
test set tables (see Table C.19)8.3 Structured Queries for Information Retrieval in Table Images 223
Using Eqs. (8.8–8.10) to compute the RPs for these 363 structured queries, the
gAP and mAP performance results were 90.5% and 92.1%, respectively. The cor￾responding R–P curve is shown in Fig. 8.4, along with the R–P curve obtained with
the standard single-words query set used in the experiments of Sec. 6.9.1 (this curve
is also in Fig. 6.13).
0 0.2 0.4 0.6 0.8 1
0
0.2
0.4
0.6
0.8
1
Recall: 휌
Precision:
휋
Struct: gAP: 90.5% (mAP: 92.1%)
Plain: gAP: 74.6% (mAP: 69.2%)
Fig. 8.4: R–P curves for structured table queries on the Passau dataset (PAS). “Plain” is
the same curve of Fig. 6.13 (Sec. 6.9.1) for single-word queries on the PAS dataset.
Discussion
Both the gAP and mAP search performance results for structured multi-word queries
are much better than in the case of plain, single-word queries (see Fig. 6.13). Note that
the plain query set encompassed almost all the 5 801 test-set words (see Table C.19),
including most function words and many other rather “uninteresting” words. In
contrast, the structured queries were real queries issued by a real user aiming to
retrieve specific data from the text images. Therefore, while the conditions are not
properly comparable, we think the results obtained with natural queries should be
considered more realistic.
Note as well that the very structured nature of these queries might have also
contributed to achieve better performance: for this kind of queries, several words
must be successfully spotted in an image and geometric constraints further limit the
set of final candidate spots. This improves precision because many possible false
alarms just fail to survive the underlying relational and geometric constraints.
A real demonstrator of the indexing and search techniques developed and evalu￾ated in this section is publicly available through the Internet.2 It allows conventional
single-word and multiple-word Boolean queries, as well as the structured tabular
queries introduced in this section.
2 https://www.prhlt.upv.es/htr/passau224 8 Probabilistic Indexing Search Extensions
8.4 Searching for Hyphenated Words
One major problem when transcribing and/or indexing historical manuscripts is the
presence of hyphenated words. A hyphenated word (HyW) can be defined as a word
that, due to a line break, has been split into two text fragments (HwFs). The first
HwF of a HyW is referred to as its prefix of the second its suffix. Examples of HyWs
can be seen in Fig. 8.5. Words that are not hyphenated (all except those marked in
the example) are called entire words.
Fig. 8.5: Image region from a document of the Finnish Court Records (FCR) collection.
Prefix and suffix HwFs are marked with blue and orange BBs, respectively. All the other
text tokens are called entire words.
HyWs rise a variety of challenges: first, diverse hyphenation markup styles can be
found in historical manuscripts, making their recognition more difficult. Typically,
hyphenation is denoted using a special symbol (such as “-”) at the end of the prefix.
Nevertheless, this convention is rather seldom followed in historical documents: there
are documents where this mark is found at the beginning of the suffix, others where
it is both at the end of the prefix and the beginning of the suffix. Furthermore, other
hyphenation symbols such as “=”, “.”, “∼”, etc., are often used. And, in many cases,
no explicit hyphenation marks are written at all. Examples of some of hyphenation
symbols that are used (or are missing) in the FCR collection (Appendix.C.10) can
be seen in Fig. 8.6. In addition, the position(s) where the hyphenation symbol(s) is
(are) placed and the symbols themselves might vary, even on the same page!
Another difficulty is that historical handwriting seldom follow the syllable-based
hyphenation rules used in most modern languages. Depending on the language and
time when a text was written, it may happen that hyphenation rules were different, or
even that no word splitting rules existed at all. Some examples of hyphenated words
that do not follow modern hyphenation rules can be seen in Fig. 8.6.
a) b) c)
d) e)
Fig. 8.6: Examples of hyphenated words where different hyphenation symbols have been
employed: “.” in a), “∼” in b), “=” in c), “-” in d) and no hyphenation symbol in e).
Moreover, the first two HyWs do not follow nowadays hyphenation rules. The word written
in a) is “agande”, b) is “berorde”, c) is “Februari” and d) and e) are “hemman”.8.4 Searching for Hyphenated Words 225
Last but not least, since our objective is information retrieval, it is not admissible
that users have to figure out how exactly a query word may have been hyphenated
when the document was written, or that they have to type all the possible prefix/suffix
combinations of the entire word they want to find. Therefore, it is not enough to
recognize the HwFs: we need to join the HwFs to assemble the corresponding HyW,
so as to allow queries to be oblivious of hyphenation issues.
To deal with all these challenges, we capitalize on the following general ideas:
• Keep off hyphenation software or rules; instead, try to discover likely pre￾fix/suffix HwFs in the images themselves,
• Given pairs of pseudo-words which are likely prefix/suffix HwFs, compute the
relevance probabilities of their composed entire words.
These general ideas were first put forward in [16] and they are followed in this
section. However, here we propose an alternate problem formalization in which a
HyW is considered as a Boolean AND combination of two HwFs under geometrical
constraints on the BBs where these fragments are spotted. This view makes the HyW
search problem fairly similar to the problem of structured queries in table images,
considered in Sec. 8.3.
We aim to compute 푃(푅 | 푥, 푣) for a word 푣 that is hyphenated in an unknown
way. Let 푟 and 푠 be, respectively, two possible prefix and suffix HwFs of 푣. Note that
푣 is known (it is our query word), but not 푟 or 푠. However, it is sure that 푣 = 푟 푠.
Let 푃(푅 | 푥, 푟, 풃푟 ) and 푃(푅 | 푥, 푠, 풃푠) be the position-dependent RPs of the
image 푥 for the prefix and suffix HwFs 푟 and 푠 in the BBs 풃푟 and 풃푠, respectively.
Since both 푟 and 푠 must be present in 푥, the event that 푥 is relevant for 푣 can be
considered as an AND combination that 푥 is relevant for both 푟 and 푠 (that is, both
HwFs are written in 푥). But this is further conditioned by geometrical constraints of
the corresponding BBs, as discussed below.
Note that for different combinations of푟 and 푠, different instances or spots of both
HwFs may be retrieved. Let 풃푟1, . . . , 풃푟푀푟
and 풃푠1, . . . , 풃푠푀푠
be the 푀푟 and 푀푠
different BBs retrieved for the HwFs 푟 and 푠, respectively. Now let 푃(푅 | 푥, 푟, 풃푟 푖)
and 푃(푅 | 푥, 푠, 풃푠 푗), respectively, be the RPs of 푥 for 푟 and 푠 in the image locations
풃푟 푖 and 풃푠 푗. Therefore, from the discussion in Sec. 8.1:
푃(푅 | 푥,푣) ≈ max
푟,푠:
푟 푠=푣
max
1≤푖≤푀푟
1≤ 푗≤푀푠
min ￾
푃(푅 | 푥, 푟, 풃푟 푖), 푃(푅 | 푥, 푠, 풃푠 푗)

(8.11)
In general PrIx entries are of the form [푥, 푣, 푃푣, 풃푣], where 푃푣
def
= 푃(푅 | 푥, 푣, 풃푣)
is the RP that the word 푣 is written on image 푥 at position 풃푣. The computation
of Eq. (8.11) requires us to consider all pairs of BBs, (풃푟 푖, 풃푠 푗), which contain any
valid pair of HwFs 푟, 푠 of the word 푣. That is, we should consider all pairs of
spots ([푥, 푟, 푃푟 , 풃푟 푖], [푥, 푠, 푃푠, 풃푠 푗]) such that 푟 푠 = 푣 and 풃푟 푖, 풃푠 푗 lay in plausible
geometrical positions in 푥.
This computation may be too expensive to be carried out at query time; so we
propose to do it at indexing time. To this end, we should consider all the pseudo￾word spots of the PrIx of a page image 푥 as candidates to be 푟 and 푠. Then fo226 8 Probabilistic Indexing Search Extensions
each quadruplet (푟, 푠, 풃푟 푖, 풃푠 푗) considered in the right part of Eq. (8.11), a new spot
can be assembled of the form [푥, 푣, 푃푣, 풃푣], where 푃푣
def
= 푃(푅 | 푥, 푣) computed as in
Eq. (8.11), and 풃푣 would be the “union” of the two BBs 풃푟푖
, 풃푠 푗.
In order to avoid adding prohibitive amounts of (low probability and maybe
unwished) spots, we rely on the two following principles:
1. Accurately predict prefix/suffix HwFs. Rather than considering all spots of 푥,
we try to consider only those which likely correspond to prefix and suffix
HwFs. This can be achieved by adopting special forms of optical and language
modeling to allow distinguishing entire words from HwFs as described in [16].
Basically, for optical modeling, we employed the same CRNN model described
in Sec. 6.9.6, trained on annotated samples which included tagged HyWs. On
the other hand, for language modeling, a special character 푛-gram is used
which deals both with the probabilistic modeling of character sequences, and
with ensuring that a prefix HwF can appear only at the end of a line, while an
suffix HwF can appear only at the beginning of a line.
2. Apply BB geometric reasoning. We consider only pairs of spots whose BBs are
in plausible geometrical positions to constitute a HyW. To this end, a pair of
BBs, 풃푟 , 풃푠, must fulfill the following (heuristic) geometry constraints:
• 풃푠 should be a little below 풃푟 in 푥 (e.g., no more than 200 pixels),
• 풃푠 should be substantially on the left of 풃푟 in 푥 (e.g., at least 500 pixels).
Let [푥, 푟, 푃푟 , 풃푟 ], [푥, 푠, 푃푠, 풃푠] be a pair of spots such that푟 and 푠 are HwF prefix
and suffix predictions with non negligible RPs 푃푟 and 푃푠, respectively, and 풃푟 , 풃푠
meet the above geometric constraints. Then a new “entire-word spot” [푥, 푣, 푃푣, 풃푟 푠]
is produced, where 푣 is the concatenation of 푟 and 푠 (after removing the HwF tags)
and 푃푣 is the RP given by Eq. (8.11). The special “entire-word BB” 풃푟 푠 should be
understood as the union of two typically disjoint and distant BBs, 풃푟 and 풃푠. So, in
practice, rather than adding this special-form spot to the PrIx of 푥, two regular spots
are added; namely: [푥, 푣, 푃푣, 풃푟 ] and [푥, 푣, 푃푣, 풃푠].
8.4.1 Experiments
The above approach was empirically assessed in [16] using the FCRdataset described
in Appendix C.10. Plain PrIx performance results on this dataset, also initially
presented in [16], are summarized in Sec. 6.9.6 of this book.
To allow distinguishing the relative impact of entire and hyphenated word in￾stances on the results, two query sets were defined. One, named AllWords, was
a conventional query set composed of 10 416 words, derived from the test set by
excluding all HwFs and words with less than 2 characters. The other, called May￾beHyph, was a subset of AllWords consisting of 1 972 (entire) words for which at
least one instance in the test set is hyphenated (see Table C.23 of Appendix C.10).8.5 Approximate-Spelling and Wildcard Queries 227
Table 8.3 reports gAP and mAP figures obtained using the original PrIx (Plain
PrIx) and on the PrIx after adding the entire words produced from paired HwFs
(HyW PrIx) as discussed above..
Table 8.3: Plain and HyW PrIx keyword search AP figures (in %) for the query sets
AllWords and MaybeHyph on the FCR test set.
Query Sets Plain PrIx HyW PrIx
gAP mAP gAP mAP
AllWords 84.1 78.2 87.5 84.5
MaybeHyph 81.0 43.9 90.6 79.0
Discussion
The HyW PrIx approach clearly outperformed Plain PrIx for both query sets, both in
terms of AP and mAP. As expected, the AP/mAP improvements achieved by HyW
PrIx are more remarkable if we focus on searching only for potentially hyphen￾ated words. This is obviously explained because the query set MaybeHyph has a
much higher hyphenation rate than AllWords [16] Even though isolated HwFs are
hard to predict, the geometric constraints discussed before do help to correctly pair
prefix/suffix fragments, leading to the observed good search performance for HyWs.
These results suggested that a good practical search and retrieval experience was
expected when seamlessly searching for entire or hyphenated instances of words
using the methods here proposed. This can be experienced first hand with a demon￾strator which is publicly available through the Internet.3
Recently we have proposed a new, fully probabilistic formulation of the HyW
problem [2]. It integrates probabilistic models of geometric constraints, which are
learned from the training data, along with the rest of the models required for con￾ventional PrIx. The new approach provides better results and is much more robust
because it does not rely on any kind of heuristics.
8.5 Approximate-Spelling and Wildcard Queries
In previous sections query words were assumed to be tokens that have to exactly
match pseudo-words of a PrIx. Since both query words and pseudo-words are
represented by character strings, this implies that words are assumed to be perfectly
spelled: in the documents, in their PrIx spots, and in the queries. However, in the
practical PrIx applications, users often like to use flexible word spelling, not only
to cope with the user unfamiliarity about exactly how a given word could have been
written in the manuscripts, but also to deal with the system (or human) uncertainty
as to which pen strokes were actually used to depict some characters or word.
3 https://www.prhlt.upv.es/htr/fcr-hyp228 8 Probabilistic Indexing Search Extensions
Since each entry of the PrIx of a given document is essentially characterized by a
pseudo-word, all conventional search tools usually available for querying plain text
documents can potentially be applied to search for information in the PrIxs of a
collection of text images.
Conventional search tools typically include approximate (or “fuzzy”) spelling
and wildcards. These tools are generally considered to be remarkably useful search
assets in practice. This becomes especially important for searching in manuscripts
with inherent ambiguity and uncertainty, raised by archaic and/or inconsistent word
spelling and by often erratic use of abbreviations.
Both approximate spelling and wildcards can be easily represented in terms of
regular expressions which can readily be transformed into FSTs. Therefore, as dis￾cussed in Sec. 5.7, a first idea would be to compose these FSTs with the character
lattices of the original image regions (typically text lines) in order to compute the
RP of each image region for the query word, taking into account all the possi￾ble spelling transformations entailed by the approximate spelling and/or wildcard
operator specified.
However, as commented in that section, this approach would be prohibitively
expensive, both in terms of computing time and storage required. Therefore here
we present alternative methods which only require conventional PrIx and light
computing at query time.
8.5.1 Approximate-Spelling
The following description is partially based on the work presented in [1], which
considered approximate (also called “fuzzy” or “elastic”) spelling of any query
word. Since PrIxs are generally very large (as compared with plain text), computing
efficiency was a major concern.Without loss of generality, the work was restricted to a
single word. Of course, an approximately spelled word can then be straightforwardly
combined with other approximate- and/or exactly spelled words into arbitrarily
complex Boolean and sequence expressions, as outlined in Sec. 8.1.
An approximate spelling of a single word is specified by a base word, along
with an indication that some flexibility is allowed as to how the word is expected to
be spelled in the documents considered. We model this flexibility in terms of the
Levenshtein or edit distance [12], 푑(푣, 푣′
), which is the minimum number of edit
operations (i.e., single character insertions, substitutions or deletions) that need to
be applied to a word 푣 to produce another word 푣
′
. By specifying a maximum edit
distance, the degree of allowed flexibility can be easily controlled.
This flexibility is particularly useful to deal with historical text, where the evolu￾tion of the language through time generally leads to important uncertainties about
how a word we are interested in might have been spelled in the documents of a large
collection. In addition, it is also useful when script, writing style, and/or optical
image quality problems make it difficult (for machines and humans alike) to tell
which are the exact characters actually written. In these cases, PrIxs do generally
offer a large number of spelling alternatives. However, in many of these occasions,8.5 Approximate-Spelling and Wildcard Queries 229
the word form used in a conventional query may happen not to be among any of
the indexed alternatives, or it is one with very low RP. For any of these reasons, or
for all, approximate spelling constitute a powerful tool that do help users to retrieve
textual information which would be difficult to find otherwise.
An approximately-spelled word with edit distance threshold 휏 can be seen as a
word-level Boolean OR query, composed of all the spelling variations of the given
word allowed by applying up to 휏 edit operations. However, the number of possible
variations is generally huge. For instance, assuming an alphabet of 26 characters,
more than 75 000 spelling variations of the word “aptitude” are within an edit distance
of 휏=2 and more than 750 000 are within 휏=3. Luckily, we do not need to consider
all the variations, but only those which actually appear in the given PrIx (clearly,
the RP of all the others is null and would never be retrieved!). That is, given a base
word 푣 we can first search for all the indexed pseudo-words 푣
′
such that 푑(푣, 푣′
) ≤ 휏
and then issue a regular multi-word OR query with only these pseudo-words.
A Levenshtein automaton [13, 1] can be used for this purpose. Let 푞 = 푣∼휏 denote
an approximately-spelled word, with base word 푣 and threshold 휏, and let S be the set
of pseudo-words in the PrIx. The language accepted by the Levenshtein automaton
for 푞 = 푣 ∼ 휏 is 퐿(푣, 휏) = {푣
′
: 푑(푣, 푣′
) ≤ 휏} and let 퐿S (푣, 휏)
def
= 퐿(푣, 휏) ∩ S be the
set of PrIx pseudo-words in 퐿(푣, 휏).
Then 푣∼휏 is equivalent to the OR query “푣1 ∨ · · · ∨ 푣푛”, where 푣푖 ∈ 퐿푆 (푣, 휏), 1 ≤
푖 ≤ 푛 and, as discussed in Sec. 8.1, the RP of an image region 푥 for this query is
approximated as:
푃(푅 | 푥, 푣∼휏) ≈ max
1≤ 푗≤푛
푃(푅 | 푥, 푣푗) (8.12)
8.5.2 Wildcard Spelling
Now, we consider wildcard spelling, which allows us to search for words that have
specific prefixes and/or suffixes. Again, due to the typically large size of PrIxs,
computing efficiency is a major concern.
A wildcard spelling is understood as a prefix 푟, followed by the wildcard symbol
“∗” (which represents an arbitrary sequence of characters of any length) and a suffix
푠. Note that if 푟 is empty and 푠 is not, that wildcard will denote a suffix spelling,
while if 푠 is empty and 푟 is not, it will denote a prefix spelling.
Wildcards are particularly useful to search for words such that only a prefix, or a
suffix (or both) is known for sure. It is also helps to deal with historical text, given
the always present uncertainty due to the evolution of the language through time, the
use of abbreviations, etc. For any of these reasons, or for all of them, wildcards also
constitute a powerful tool that help the users to retrieve information that could not
be easily found otherwise.
The approach we propose to deal with wildcards is very similar to that proposed for
approximate spelling. A wildcard-spelled word can be seen as a Boolean OR query,
composed by all the possible character sequences starting with the prefix 푟 and
ending with the suffix 푠. Unfortunately, the number of possible character sequences
that match this pattern is infinite, which makes this problem intractable. However, as230 8 Probabilistic Indexing Search Extensions
in the case of approximate spelling, not all the possible character sequences need to
be considered; only those which appear in the given PrIx (clearly, the RP of all the
others is null and would never be retrieved). That is, given a prefix 푟 and a suffix 푠, we
can first search for all the indexed pseudo-words which start with 푟 and finish with 푠
and then perform a regular multi-word OR query only with these pseudo-words.
A simple FST could be straightforwardly used with this purpose. Let 푟 ∗ 푠 denote
a wildcard spelling with prefix 푟 and suffix 푠 and let S be the set of pseudo￾words in a given PrIx. The language accepted by the FST for this wildcard is
퐿(푟 ∗ 푠) = {푣
′
: 푣
′ =푟푤푠}, where 푤 is an arbitrary character sequence or maybe the
empty string. Let {푣1, . . . , 푣푛}
def
= 퐿(푟 ∗ 푠) ∩ S be the set of PrIx pseudo-words in
퐿(푟 ∗ 푠). Then, 푝∗푠 is equivalent to the OR query 푣1 ∨ 푣2 ∨ . . . ∨ 푣푛 and, as discussed
in Sec. 8.1, the RP of an image region 푥 is computed as:
푃(푅 | 푥, 푝 ∗ 푠) = max
1≤ 푗≤푛
푃(푅 | 푥, 푣푗) (8.13)
8.5.3 Experiments
The first experiments carried out to asses the proposed approach were based on the
Large Bentham dataset (BEN4, see Appendix C.2.4) and presented in [1]. Two
query sets, were used: The first one, referred as Q1, is the same conventional query
set of 6 953 words used in Table 6.13 for the BEN4 dataset
The second query set, referred to as Q2, was composed of 861 words from Q1,
selected so that their individual AP was zero with exact-word spelling; that is, they
were in the test-set GT but not in any of the PrIx spots of the line image where they
appear in the GT. This query set was explicitly aimed at assessing the capability of
approximate-spelling search to find information which would be impossible to find
using exact-spelling.
As in most experiments, evaluation was carried out at line level. To asses both the
accuracy and usefulness of approximate spelling search results, a relevant detected
entry (hit or true positive) was defined as an event where a query that matches the
pseudo-word of a PrIx spot, also matches one of the words of the GT transcript of
the text line geometrically associated to that spot. Otherwise. it is a false positive.
In the case of wildcards, on the other hand, the evaluation protocol defines
a relevant detected entry as an event where the prefix/suffix query that matches
the starting/ending fragment of a pseudo-word of a PrIx spot, also matches the
starting/ending fragment of one of the words of the GT transcript of the text line
geometrically associated to that spot.
Approximate spelling was assessed for two values of the edit distance threshold,
휏 = 1 and 휏 = 2 and wildcard spelling was tested for prefixes and suffixes of lengths
3, 4 and 5. Table 8.4 reports the gAP and mAP performance results achieved.8.5 Approximate-Spelling and Wildcard Queries 231
Table 8.4: Exact, approximate- and wildcard-spelling retrieval gAP and mAP performance
(in %) achieved for the two query sets Q1 and Q2.
Query set Q1 Q2
gAP mAP gAP mAP
Exact 86.8 76.1 00.0 00.0
Approximate, 휏 = 1 92.1 80.9 79.4 38.0
Approximate, 휏 = 2 96.0 85.0 86.9 64.8
Prefix, length= 3 91.5 84.7 90.9 74.9
Prefix, length= 4 88.9 82.3 87.4 59.9
Prefix, length= 5 87.6 81.1 82.5 45.6
Suffix, length= 3 87.7 82.6 87.6 75.5
Suffix, length= 4 85.0 79.8 84.8 61.8
Suffix, length= 5 80.9 78.0 79.6 44.5
Discussion
For Q1, both the approximate- and wildcard-spelling improved significantly the APs
with respect to exact-spelling. On the other hand, as obviously expected, the improve￾ment was huge for Q2, for which the exact-spelling APs were 0. The great success for
Q2 queries mainly stemmed from the frequent PrIx pseudo-word hypotheses which
were not exactly correct (or were correct but the words in the image were not spelled
exactly as expected), but were lexicographically close to the GT transcripts.
For wildcard-spelling, it was observed that the performance for both query sets
decreases with the length of the prefixes or suffixes; albeit the decrease is only
moderate. This was justified by a noticeable increase of false positives caused by a
sharp increase of alternative (OR) spellings.
This study shows the ability of approximate and wildcard spelling to help the
users dealing with the intrinsic uncertainty of handwritten text images - and more so
in images of historical documents. It also shows how these queries affect positively
the retrieval performance of the system. However, this type of queries may also
retrieve words that were not intended to be retrieved when the user made the query.
Approximate and wildcard spelling are available in all our recent PrIx demon￾strators, most of which are publicly available through the Internet.4
4 Many PrIx demonstrators are listed in https://www.prhlt.upv.es/htr/PrIx. Most of them
allow multi-word Boolean and sequence queries, along with approximate and wildcard spelling.232 8 Probabilistic Indexing Search Extensions
References
1. Andres, J., Toselli, A.H., Vidal, E.: Approximate search for keywords in handwritten text ´
images. In: S. Uchida, E. Barney, V. Eglin (eds.) Document Analysis Systems, pp. 367–381.
Springer International Publishing, Cham (2022)
2. Andres, J., Toselli, A.H., Vidal, E.: Search for hyphenated words in probabilistic indices: a ´
machine learning approach. In: 2023 International Conference on Document Analysis and
Recognition (ICDAR), pp. 269–285 (2023)
3. Boole, G.: An Investigation of the Laws of Thought on Which are Founded the Mathematical
Theories of Logic and Probabilities. Macmillan, New York (1854)
4. Bosch, V., Calvo-Zaragoza, J., Toselli, A.H., Vidal, E.: Sheet music statistical layout analysis.
In: 15th International Conference on Frontiers in Handwriting Recognition, ICFHR 2016,
Shenzhen, China, October 23-26, 2016, pp. 313–8 (2016)
5. Calvo-Zaragoza, J., Toselli, A.H., Vidal, E.: Probabilistic music-symbol spotting in handwritten
scores. In: 2018 16th International Conference on Frontiers in Handwriting Recognition
(ICFHR), pp. 558–563. IEEE (2018)
6. Calvo-Zaragoza, J., Toselli, A.H., Vidal, E.: Handwritten music recognition for mensural
notation with convolutional recurrent neural networks. Pattern Recognition Letters 128, 115–
121 (2019). DOI https://doi.org/10.1016/j.patrec.2019.08.021
7. Calvo-Zaragoza, J., Toselli, A.H., Vidal, E., Sanchez, J.A.: Music symbol sequence indexing ´
in medieval plainchant manuscripts. In: 2019 International Conference on Document Analysis
and Recognition (ICDAR), pp. 882–887. IEEE (2019)
8. Frechet, M.: Sur les tableaux de corr ´ elation dont les marges sont donn ´ ees. Ann. Univ. Lyon, ´
3ˆ e serie, Sciences, Sect. A 14, 53–77 (1951)
9. Frechet, M.: G ´ en´ eralisation du th ´ eor ´ eme des probabilit ` es totales. Fundamenta Mathematicae ´
25(1), 379–387 (1935). URL http://eudml.org/doc/212798
10. Kneser, R., Ney, H.: Improved backing-off for M-gram language modeling. In: 1995 Interna￾tional Conference on Acoustics, Speech, and Signal Processing, vol. 1, pp. 181–184 (1995).
DOI 10.1109/ICASSP.1995.479394
11. Lang, E., Puigcerver, J., Toselli, A.H., Vidal, E.: Probabilistic indexing and search for in￾formation extraction on handwritten german parish records. In: 2018 16th International
Conference on Frontiers in Handwriting Recognition (ICFHR), pp. 44–49 (2018). DOI
10.1109/ICFHR-2018.2018.00017
12. Levenshtein, V.I.: Binary codes capable of correcting deletions, insertions and reversals. Soviet
Physics Doklady 10(8), 707–710 (1966)
13. Schulz, K.U., Mihov, S.: Fast string correction with levenshtein automata. International Journal
on Document Analysis and Recognition 5(1), 67–85 (2002)
14. Toselli, A.H., Vidal, E.: Handwritten text recognition results on the Bentham collection with
improved classical N-gram-HMM methods. In: Proceedings of the 3rd International Workshop
on Historical Document Imaging and Processing, pp. 15–22. ACM (2015)
15. Toselli, A.H., Vidal, E., Puigcerver, J., Noya-Garc´ıa, E.: Probabilistic multi-word spotting in
handwritten text images. Pattern Analysis and Applications (2019)
16. Vidal, E., Toselli, A.H., Puigcerver, J.: A probabilistic framework for lexicon-based keyword
spotting in handwritten text images. Tech. Rep. arXiv:2104.04556 (2017–2021)
17. Young, S., Evermann, G., Gales, M., Hain, T., Kershaw, D., Liu, X., Moore, G., Odell, J.,
Ollason, D., Povey, D.: The HTK book. Tech. rep., Cambridge Univ. Engineering Dep. (2002)Chapter 9
Beyond Search Applications of Probabilistic
Indexing
Abstract PrIx development was originally driven by the need of searching for
textual information in large collections of untranscribed text images. The spots that
result from the PrIx process are not image transcripts, but they provide very rich
probabilistic information about the text rendered in the images and image regions or
locations. This chapter presents approaches to exploit this information to go beyond
information search applications. Specifically, we will present methods to use the
PrIx of an image or an image collection to deal with tasks that traditionally require
actual textual data such as electronic text. We will cover, in order, basic and advanced
text analytics, statistical information extraction and document image classification
by textual content.
9.1 Text Analytics Using PrIx
The primary usage of the PrIxs of a manuscript image collection is to allow fast
and accurate search for textual information in the images. However, the information
contained in a PrIx can be useful to estimate usual text features which have been
traditionally used to develop text analytics applications for documents consisting of
plain, electronic text. Now, using the PrIx information of an image collection, most
of these textual features can be easily estimated from text images without having to
resort to actual text image transcripts.
The most basic text analytics tasks entail computing running words and vocab￾ulary sizes, extracting actual vocabularies and computing Zipf curves, from which
interesting linguistic properties can be put forward. In addition, this chapter will cover
more advanced natural language tasks, such as (semantic) information extraction
and textual content-based classification of image documents.1
1 In coming sections, the term “image document” will be used to refer to a set of consecutive page
images from some larger archival unit or image collection.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 233
A. H. Toselli et al., Probabilistic Indexing for Information Search and Retrieval 
in Large Collections of Handwritten Text Images, The Information Retrieval Series 49, 
https://doi.org/10.1007/978-3-031-55389-9_9 
 234 9 Beyond Search Applications of Probabilistic Indexing
9.2 Estimating Word and Document Frequencies from PrIxs
As discussed in Chapter 3, the RP 푃(푅 | 푥,푣, 풃) of each entry of a position-dependent
PrIx, is the probability that the pseudo-word 푣 is written in the location 풃 (bounding
box or ordinal position, 푘) of the image region 푥. Since 푅 is a binary random variable,
푃(푅 | 푥, 푣, 풃) can also be properly seen as the statistical expectation that 푣 is written
in the location 풃. Therefore, the sum of RPs of all the pseudo-word spots indexed
from 푥 should approach the number of words written in 푥.
More formally, let 푤 be a word sequence corresponding to a transcription hy￾pothesis of 푥 and let 푛(푤) be the number of (running) words in 푤. Now, for a
pseudo-word 푣 and an ordinal position 푘 in 푤, let us define the indicator function
푔(푤, 푣, 푘)
def
= 1 iff 푤푘 = 푣 (0 otherwise). Therefore, Í
푣,푘 푔(푤, 푣, 푘) = 푛(푤). Then,
following Eq. (3.34), the expected value of 푛(푤) is:
E푤| 푥 [푛(푤)] =
Õ
푤
푛(푤)푃(푤 | 푥) =
Õ
푤
Õ
푣,푘
푔(푤, 푣, 푘)푃(푤 | 푥) =
Õ
푣,푘
Õ
푤:푤푘=푣
푃(푤 | 푥)
=
Õ
푣,푘
푃(푅 | 푥, 푣, 푘) (9.1)
The same2 can also be similarly derived from Eq. (3.31) for segment-position PrIxs:
E푤| 푥 [푛(푤)] ≈
Õ
푣, 풃
푃(푅 | 푥, 푣, 풃) (9.2)
Similar derivations allow us to compute the expected frequency (number of
occurrences) of a given word 푣 in an image region 푥:
E푤| 푥,푣 [푛(푤, 푣)] =
Õ
풃
푃(푅 | 푥, 푣, 풃) (9.3)
where 풃 can be just written as 푘 for ordinal-position PrIxs.
To avoid cumbersome notation, from now on E푤| 푥 [푛(푤)] and E푤| 푥,푣 [푛(푤, 푣)] will
be written just as 퐸[푛(푥)] and 퐸[푛(푣, 푥)], to be read as “expected number of words
written in 푥” and “expected frequency of occurrence of 푣 in 푥”, respectively.
Eqs. (9.1) or (9.2) can easily be extended to compute the expected number of
running words in a page image or image document 푋 or a full image collection, X:
퐸[푛(푋)] =
Õ
푥⊑푋
퐸[푛(푥)] ; 퐸[푛(X)] =
Õ
푋∈X
퐸[푛(푋)] (9.4)
where 푥 ⊑ 푋 is understood3 as the set of the set of (essentially disjoint) PrIx-indexed
image regions (lines, e.g.) of 푋.
2 Actually it is an approximation because, tn this case, we have to assume that all bounding boxes
풃 are disjoint, which might not be strictly true in general.
3 As in the previous section, the symbol “⊑” is used to denote geometrical inclusion.9.3 Zipf Curves, Running Words and Lexicon Size 235
Similarly, the frequency of use of a specific word 푣 in 푋, or in X, is estimated as:
퐸[푛(푣, 푋)] =
Õ
푥⊑푋
퐸[푛(푣, 푥)] ; 퐸[푛(푣, X)] =
Õ
푋∈X
퐸[푛(푣, 푋)] (9.5)
Finally, an interesting text statistics that will be used in coming sections is the
expected number of image documents in a collection X that contain a given word
푣, denoted as 퐸[푚(푣, X)]. Following similar arguments as used in [20] to compute
page-level RP (also in Chapter 8, Sec. 8.1), a reasonable estimate for this number is:
퐸[푚(푣, X)] ≈ Õ
푋∈X
max
푥⊑푋
푃(푅 | 푥, 푣) ≈ Õ
푋∈X
max
푥⊑푋, 풃⊑푥
푃(푅 | 푥, 푣, 풃) (9.6)
9.3 Zipf Curves, Running Words and Lexicon Size
The Zipf curve of a given plaintext depicts the frequency of each word as a function of
its rank in the decreasingly sorted list of word frequencies. In most natural languages,
where word occurrences are not balanced, this curve follows more or less accurately
the Zipf’s law, which states that in natural languages the frequency of any word is
inversely proportional to its rank [11]. If both axis are in logarithmic scale, the curve
becomes a decreasing straight line with a slope of about −45 degrees. As far as the
text diverges from that of an (occidental) natural language, the Zipf curve tends to
deviate from this “natural” shape.
Fig. 9.1 shows a Zipf curve computed from the GT transcripts of the Bentham
test set of the dataset described in Appendix C.2.4 (Table C.10). As it can be seen,
the curve does quite faithfully follow the natural Zipf’s law.
It is straightforward to see that the area under a Zipf curve is the number of running
words and the highest rank of a least frequent word (i.e., Frequency = 1) is just the
vocabulary size. In this case, the area under the curve amounts to 89 870 words
and the highest rank of a least frequent word is 6 988 different words. Both figures
exactly match the running words and vocabulary size values reported in Table C.10
of Appendix C.2.4.
Obviously, word frequencies can not be (directly) computed in text images. But
if the PrIx of an image document or collection is available, word frequencies can be
easily estimated as explained in Sec. 9.2. Then these estimates can be used as proxies
of the actual frequencies to produce an estimated Zipf curve.
Fig. 9.2 shows the same Zipf curve as in Fig. 9.1 computed using the GT transcripts
of the Bentham test set, along with another Zipf curve estimated from the PrIxs of
the corresponding dataset images. The estimated curve is very accurate down to
the estimated frequency = 1, which ratify the accuracy of the estimates provided by
Eq. (9.5), based on Eq. (9.3).
Now, the number of running words can be estimated as the area under the estimated
Zipf curve, which is exactly what is computed by Eq. (9.4). On the other hand, the
vocabulary size can be estimated as the rank for which the frequency = 휖, where 휖
can be determined empirically using validation data (or just set to a fixed value, such236 9 Beyond Search Applications of Probabilistic Indexing
as 0.5). As a byproduct of lexicon size estimation, an actual vocabulary is obtained,
each word accompanied with the corresponding estimated frequency of use.
1
10
100
1K
10K
1 10 100 1K 10K
BENTHAM Test-set GT transcripts
|V | = 6 988
Word frequency
Word rank
Fig. 9.1: Plaintext Zipf curve of Bentham test-set GT transcripts. The horizontal axis
is the range of each word in the list of words sorted according to their frequency (in the
vertical axis). The size of the vocabulary (lexicon) is the largest range of a word with
the lowest frequency (1) and the total number of (running) words is the area under the
curve (for frequency ≥ 1). For (occidental) natural languages the Zipf curve is expected to
approach the diagonal purple thin line.
Zipf curves similar to those of Fig. 9.2 are shown in Fig. 9.3 for other datasets.
Details of these datasets are reported in Appendix C: Secs. C.10, C.6, C.9 and C.8,
for FCR, Passau, Carabela and TSO, respectively.
The great accuracy of PrIx estimates is remarkable in all the cases. Regarding
how faithfully each Zipf curve follows the expected Zipf law for natural languages,
the best one is Carabela. It largely encompasses historical documents which relate
exploration stories in fairly plain (old) Spanish language. It follows TSO, composed
of Spanish Theater Golden-Age comedies. Most of the text is verse, which may
explain the small divergence from the Zipf law for a significant number of high
frequency words used in many rhymes. The next one is FCR, which fully consists
of court records written in Swedish. It contains lots of repetitive legal jargon that
most probably cause the increased notable divergence for a larger number of high￾frequency words. The most divergent, finally, is Passau. Half of this dataset are
parish registers written in rather natural (old) German language. But the other half
consists of large parish tables (a “language” that can hardly be considered “natural”).9.3 Zipf Curves, Running Words and Lexicon Size 237
<0.1
1
10
100
1K
10K
1 10 100 1K 10K 100K 1M 10M
frequency = 0.5
|V | = 6 988
|V | = 7 431 (estimated)
Word frequency
Word rank
BENTHAM Test-set PrIx estimates
BENTHAM Test-set GT transcripts
Fig. 9.2: Comparing the Zipf curve and lexicon size computed from Bentham’s GT
transcripts with corresponding estimates from PrIxs. The lexicon size is estimated as the
range of a word whose estimated frequency is 0.5.
9.3.1 Estimating Running Words and Lexicon Size: Results
As discussed above, Zipf curves estimated form PrIx can fairly straightforwardly
used to estimate the number of running words and the number of different words
(or lexicon size) of a set of text images. Crude estimates of the lexicon sizes by
means of a word frequency threshold are shown in all the plots of Fig. 9.3, beside
the corresponding true sizes according to the GT image transcripts. These values are
summarized in Table 9.1, along with true and estimated values of the corresponding
number of running words.
Table 9.1: Running words and vocabulary sizes for different datasets: “True”, according
to the GT reference transcripts and “Estimated”, from PrIxs. Relative error is also given
for each dataset. Lexicon sizes are estimated using an estimated frequency threshold of
0.5, except for Carabela where 0.3 was used.
Dataset Running Words Lexicon Size
True Estimated Error (%) True Estimated Error (%)
FCR 73 849 73 174 −0.9 13 855 13 831 −0.9
Bentham 89 870 83 235 −7.4 6 988 7 431 +6.3
Passau 26 709 26 155 −2.1 5 801 5 598 −3.5
Carabela 21 788 19 677 −9.7 3 837 3 720 −3.0
TSO 10 203 9 926 −2.7 2 545 2 503 −1.7238 9 Beyond Search Applications of Probabilistic Indexing
<0.1
1
10
100
1K
10K
1 10 100 1K 10K 100K 1M
frequency = 0.5 13 955 words
13 831 words
Word frequency
Word rank
FCR Test-set PrIx estimates
FCR Test-set GT transcripts
<0.1
1
10
100
1K
10K
1 10 100 1K 10K 100K 1M
frequency = 0.5 5 801 words
5 598 words
Word frequency
Word rank
PASSAU Test-set PrIx estimates
PASSAU Test-set GT transcripts
<0.1
1
10
100
1K
10K
1 10 100 1K 10K 100K 1M
frequency = 0.333
3 837 words
3 720 words
Word frequency
Word rank
CARABELA Test-set PrIx estimates
CARABELA Test-set GT transcripts
<0.1
1
10
100
1K
10K
1 10 100 1K 10K 100K 1M
frequency = 0.5 2 544 words
2 496 words
Word frequency
Word rank
TSO Test-set PrIx estimates
TSO Test-set GT transcripts
Fig. 9.3: From top to bottom, left to right: Zipf curves of Passau, TSO, Carabela and
FCR, computed for GT transcripts and estimated using PrIxs. Lexicon sizes were estimated
as the range of a word whose estimated frequency is 0.5 (except for Carabela, where this
threshold was set to 0.333 – using 0.5 the estimated size was 3 008 rather than 3 720).
These results are interesting by themselves and seem sufficiently accurate for
practical use. In addition, they indirectly asses the accurateness of the word frequency
estimates from PrIxs discussed in Sec. 9.2. It is worth reminding that a byproduct of
lexicon size estimation is a list of most likely unique (pseudo-)words (i.e., an actual
vocabulary) depicted in the whole image collection considered.
9.4 Statistical Information Extraction from Text Images
Throughout this book, and more specifically in Table 1.2, it has been made clear
that PrIxs are not transcripts. However, we have just seen that the rich probabilistic
information conveyed by PrIxs allows us not only very effective searching for textual
information, but also estimating interesting textual data such as Zipf curves, running
words and actual vocabularies.
In this section we go one step further and explain simple methods to mine useful
statistical information of meaningful facts described by the text depicted in a text
image or image collection.9.4 Statistical Information Extraction from Text Images 239
9.4.1 Indexing Semantically Tagged Words and Named Entities
Taking advantage of word contexts, PrIx models can be directly and easily trained
to produce pseudo-word hypotheses tagged with “Semantic” or Named Entity cate￾gories. This can be simply achieved by training the models with reference transcripts
where the relevant words are tagged as required. Clearly, PrIx optical and language
models, such as CRNNs and 푛-grams, are context-aware models that can be easily
configured to learn how to linguistically interpret character pen strokes taking into
account the surrounding linguistic context.
Tagged indexing makes it possible to distinguish words depending on their se￾mantic roles (including but not limited to Named-entities); for instance:4
• HERRERO<surname>, HERRERO<job>
• SORIA<surname>, SORIA<province>
• CASADO<surname>, CASADO<civilstate>
Once a PrIx with tagged pseudo-words is available, the tags can be straightfor￾wardly used in search queries. For example, we can issue queries such as:
1. CASADO<surname>
2. SORIA*
3. *<surname>
4. *<province>
5. HERRERO<job> && CASADO<civilstate>
to retrieve, respectively, relevant images with pseudo-words such as:
1. CASADO<surname>
2. SORIA, SORIA<surname>, SORIA<province>, SORIA<cityname>, etc.
3. HERRERO<surname>, SORIA<surname>, CASADO<surname>, GARCIA<surname>, etc.
4. SORIA<province>, MADRID<province>, VALENCIA<province>, etc.
5. HERRERO<job>, CASADO<civilstate>
That is, we retrieve (1) all persons whose surnames are “Casado”; (2) all instances
of the word “Soria”, either untagged, surname, province, or city; (3) all surnames;
(4) all provinces; and (5) married persons whose occupations are “herrero”.
This idea is particularly effective for images of hybrid pre-printed / handwritten
forms. The pre-printed text can be seen as attributes or concepts which convey se￾mantic context for the handwritten text (values). In general, the printed text is detected
and indexed with great accuracy and robustness. Therefore, printed words constitute
solid footholds that provide a robust context, as needed to reliably disambiguate the
tag hypotheses for the surrounding handwritten words.
Now we can go beyond searching and aim to actually extract statistical information
about meaningful facts described by text depicted in the images of a text image
collection. To this end, we rely on the word frequency estimation formulae discussed
in Sec. 9.1, above. But now, frequencies can be estimated not only for plain words,
but also for semantic categories and/or suitable combinations of plain words, tagged
words and category tags.
4 In English, “herrero” is “smith” and the civil state “casado” is “married”. “Casado” can also be
a surname. “Soria” can be a Spanish province, a city name or a surname.240 9 Beyond Search Applications of Probabilistic Indexing
We have applied this idea to a collection of visa records which consist of hybrid
printed/handwritten cards. As commented above, the printed text represents at￾tributes or concepts such as “name”, “surname”, “job”, “age”, “reasons to travel”,
“destination”, “date”, etc. These attribute words convey the required semantic con￾text for the corresponding handwritten text (values). Fig. 9.4 shows a fragment of
one of these cards and some of the PrIx entries for this image fragment.
...
Nombre 1.000 68 26 126 62
y 0.992 194 26 24 62
a 0.008 194 26 24 62
apellidos 1.000 230 26 129 62
Amelia<givenname> 0.774 361 26 321 62
Oundia<surname> 0.193 361 26 321 62
Amelin 0.033 361 26 321 62
...
De 0.888 75 289 48 69
Do˜na 0.100 75 289 56 69
D. 0.012 75 289 48 69
27<age> 0.830 111 289 92 69
21<age> 0.170 111 289 92 69
a˜nos 0.912 180 289 92 69
aanos 0.088 180 289 92 69
Estado 1.000 260 289 100 69
soltera<civilstate> 0.709 363 289 199 69
soltero<civilstate> 0.151 363 289 199 69
sal 0.140 363 289 199 69
Profesi´on 0.981 547 289 135 69
Profesi´onn 0.019 547 289 135 69
labores<job> 0.987 690 289 208 69
labor 0.013 690 289 208 69
...
Motivos 1.000 76 430 111 84
del 1.000 196 430 50 84
viaje 0.987 253 430 70 84
viaje<reasons> 0.013 253 430 70 84
familia<reasons> 0.845 292 430 261 84
fam<reasons> 0.080 292 430 250 84
familia 0.075 292 430 261 84
...
Serrano<surname> 0.995 1003 90 337 74
Serrrano<surname> 0.005 1003 90 337 74
Rico<surname> 0.999 1405 90 199 74
...
De 0.911 1007 289 46 69
Del 0.089 1007 289 46 69
22<age> 0.721 1041 289 107 69
222<age> 0.170 1041 289 107 69
2222<age> 0.100 1041 289 107 69
a˜nos 0.991 1106 289 69 69
pa˜nos 0.009 1106 289 69 69
Estado 1.000 1186 289 101 69
soltero<civilstate> 0.993 1287 289 184 69
San 0.007 1287 289 184 69
Profesi´on 1.000 1483 289 130 69
empleado<job> 0.996 1608 289 266 69
enscriptr<job> 0.004 1608 289 266 69
...
Motivos 1.000 1008 430 110 84
del 1.000 1130 430 55 84
viaje 1.000 1184 430 66 84
deberes<reasons> 0.835 1247 430 244 84
deportado<reasons> 0.105 1247 430 244 84
haberes<reasons> 0.060 1247 430 244 84
militares<reasons> 0.680 1512 430 396 84
milicia<reasons> 0.244 1512 430 396 84
nulilce 0.086 1512 430 396 84
...
Fig. 9.4: A typical visa card (above) and some spots of its tagged PrIx (below). Some
examples of tagged reference transcripts for the left record of this card are: “Nombre y
apellidos Amelia<givenname>”, “De 22<age> anos Estado solterara ˜ <civilstate> Pro￾fesion labores ´ <job>”, “Motivos del viaje familia<reasons>”. Using Eq. (9.5) in this
small example, the expected frequency of words such as “anos ˜ ” and “viaje” would be
0.912+0.991=1.903 and 0.987+1.000 =1.987, respectively, and the expected frequencies
of the civil states “soltero” and “soltera” would be 0.993+0.151=1.144 and 0.709.9.4 Statistical Information Extraction from Text Images 241
9.4.2 Statistical Information Extraction from Handwritten Forms
Following the above approach, estimated frequencies of the most frequent reasons
to travel and occupations (jobs) registered in the whole visa image collection are
reported in Fig. 9.5, where a few likely erroneous entries are marked in red.
Reason to travel
--------------------------- 0%
1670.0 familia
191.5 turismo turista
62.7 profesi´on
--------------------------- 75%
57.2 repatriado
34.0 esposa esposo
28.8 [deberes] militares
--------------------------- 80%
16.0 negocios negocio
8.0 servicio
4.9 a˜nos
4.8 comercio comercial
3.0 Deportado
--------------------------- 81%
3.0 compras comprar
2.7 padres padre
2.4 sur
2.2 caso
2.2 casa
2.4 navegar
2.0 hijo hijos
1.8 trabajo
1.0 salud
1.0 reunirse
1.0 posesi´on
1.0 patrones
--------------------------- 82%
467.0 [OTHER and ERRORS]
--------------------------- 100%
Jobs
------------------------------ 0%
1344.9 empleado
774.7 [sus Sus] labores
165.4 jornalero
------------------------------ 76%
75.4 comerciante comercio
22.5 artista Artista
16.3 marino Marino marinero
11.0 mozo
------------------------------ 80%
10.5 sacerdote
10.2 camarero
5.7 casado
5.7 abogado Abogado
------------------------------ 81%
5.6 estudiante
3.0 minero
3.0 labrador
3.0 escritor
3.0 engrasador
2.0 fogonero
2.0 Ling¨uista
1.8 mecanico ´
1.5 estado
1.3 autor
1.2 parado
1.2 oro
1.1 rentista
1.1 Vigo
1.0 viajante
1.0 timonel
1.0 pintor
------------------------------ 82%
543.6 [OTHER and ERRORS]
------------------------------ 100%
Fig. 9.5: Statistics of the “Reason to travel” and “Jobs” of persons registered in the visa
record collection, estimated using the PrIxs of the visa images.
Similar statistics are computed for the different civil states and ages and the
resulting estimated frequencies are displayed in the form of a pie chart and an
histogram in Figs. 9.6 and 9.7, respectively.
These statistics are fairly congruent with expectations derived from other in￾dependent historical sources. Nevertheless, while the results are encouraging, the
proposed approach still needs thorough exploration and assessment.242 9 Beyond Search Applications of Probabilistic Indexing
Civil State
---------------------------- 0%
1096.8 soltero 36%
1015.7 casado 34%
---------------------------- 70%
437.0 soltera 14%
209.0 casada 7%
---------------------------- 91%
120.5 viuda 4%
70.3 viudo 2%
21.7 c´elibe 1%
---------------------------- 98%
62.2 [OTHER and ERRORS] 2%
---------------------------- 100%
Fig. 9.6: Statistics of the civil state of persons registered in the visa record collection,
estimated using the PrIxs of the visa images.
Age (years old)
------------- 0%
1.4 <15
11.9 15-19
------------- 10%
126.3 20-24
129.0 25-29
286.6 30-34
------------- 50%
275.0 35-39
230.5 40-44
170.7 45-49
156.2 50-54
------------- 90%
61.2 55-59
71.4 60-64
------------- 99%
23.1 65-70
3.2 70-74
1.1 75+
------------- 100%
Fig. 9.7: Age histogram for persons registered in the visa record collection, estimated
using the PrIxs of the visa images.
9.5 Classification of Large Untranscribed Image Documents
Here we stretch further the use of word and document frequency estimates discussed
in Sec. 9.1 for content-based classification of large collections of untranscribed text
images. Classical approaches for plaintext Document Classification (DC) are mainly
based on word and document frequency counts. Since no plaintext is generally
available for text images, we capitalize on PrIx-based frequency estimates as proxies
of the true counts. In this section we summarize and consolidate our recent work in
this topic, parts of which have appeared in [21, 12, 4, 13].
Content-based classification of large manuscripts is a recurring task that arises in
the management of (historical) archives and libraries. Many of these manuscripts are
records of daily life affairs. Here we focus on historical notarial deeds, which make
up perhaps the vastest sort of documentary series in archives worldwide. Individual
deeds are generally piled up into large bundles or boxes, each typically containing
hundreds of deeds and thousands of page images. For series of documents so massive,
it is generally difficult or impossible for archives to provide detailed metadata that9.5 Classification of Large Untranscribed Image Documents 243
adequately describe the contents of each bundle, let alone of each individual deed.
And the class or “typology” of a manuscript is perhaps the most important –and
arduous to determine– datum to be included in the metadata.
Thereupon, bundles, boxes, books, or folders of manuscript images are called
“image bundles” or just “bundles”. A bundle may contain several, often many image
documents, also called “files”, “acts” – or “deeds” in the case of notarial image
documents considered in this chapter.
So, the task we are interested in is to classify a given untranscribed image doc￾ument, which may range from a few to a few tens or hundreds of handwritten text
images, into a set of classes or types, associated with the topics or (semantic) con￾tents conveyed by the text written in the images. We call this task Content Based
Image Document Classification (CBIDC).
Existing approaches for content-based DC assume documents are made up of elec￾tronic text, so characters, words and paragraphs are unambiguously given. Therefore
the current wisdom to address the proposed CBIDC task would be to first transcribe
the images and then apply off-the-self DC techniques. However, manual transcription
is not an option and, on the other hand, achieving sufficiently accurate automatic
transcripts is generally unfeasible or elusive for large sets of historical manuscripts.5
In our proposal, PrIx provides the probability distribution of likely written words,
from which statistical expectations of word and document frequencies are estimated.
These estimates are then used to compute well-known text features such as Informa￾tion Gain and Tf·Idf, which are in turn the inputs to a Neural Network Classifier.
Note that the CBIDC task here considered is very different from other related
tasks, which are often called with similar names. To name a few: Document Classi￾fication (DC, mentioned above, which only applies to unambiguous electronic text),
Content-based Image Classification (applied to single pictures of natural scenes –
not text), or Document Image Classification (where classes are associated with the
visual appearance or page layout of single images). See [12] for a more detailed dis￾cussion on these differences, as well as references to previous publications dealing
with related problems, but mainly aimed at printed text.
Note also that recent works on document classification, including those based
on multimodal approaches and visual transformers [18, 22] are far from being
applicable to our CBIDC task, where the nature and size of the textual visual objects
considered (maybe hundreds of page images) is very different and/or exceedingly
large as compared with the single-image objects considered in these works.
On the other hand, it is important to realize that document types do change over
the years and, in a realistic scenario, we need to handle image document of classes
that had never been seen before. In the traditional classification framework, all these
new image documents would be systematically misclassified. Therefore, to properly
deal with the proposed task, new image documents which are not of any known class
should be detected; that is, the system should refuse or reject their classification. To
explicitly address this full-fledged CBIDC problem we adopt the so-called Open Set
Classification (OSC) framework [17, 5, 9].
5 HTR word recognition accuracies as low as 40 − 60% are reported in [16, 14, 21] for historical
manuscripts similar to those considered in this work.244 9 Beyond Search Applications of Probabilistic Indexing
9.5.1 Plaintext Document Classification
If the plaintext of a document is given in some electronic form, words can be trivially
identified as discrete, unique elements. Then, the whole field of text analytics [10, 1]
is available to approach many document processing problems, including DC. Most
DC methods assume a document representation model known as vector model or Bag
of Words(BOW) [6, 10, 1]. In this model, the order of words in the text is ignored, and
a document is represented as a feature vector (also called “document embedding”)
indexed by the words of some vocabulary. Let D be a set of documents, 퐷 ∈ D
a document, and D ∈ R푁 its BOW representation, where 푁 = |푉|. For each word
푣 ∈ 푉, 퐷푣 ∈ R is the value of the 푣-th feature of D.
Each document is assumed to belong to a unique class 푐 out of a finite number of
classes, 퐶. The task is to predict the best class for any given document, 퐷. Among
many pattern classification models suitable for this task, from those studied in [12]
the Multi-Layer Perceptron (MLP) was the one most promising.
9.5.1.1 Feature Selection
Not all the words are equally helpful to predict the class of a document. Thus, a
classical first step in DC is to determine a good vocabulary, 푉푛, of reasonable size
푛 < 푁. One of the best ways to determine 푉푛 is to compute the Information Gain
(IG) of each word in 푉 and retain in 푉푛 only the 푛 words with highest IG.
Using the notation of [12], let 푡푣 be the value of a boolean random variable that
is True iff, for some random 퐷, the word 푣 appears in 퐷. So, 푃(푡푣) is the probability
that ∃퐷 ∈ D such that 푣 is used in 퐷, and 푃(푡푣) = 1 − 푃(푡푣) is the probability that
no document uses 푣. The IG of a word 푣 is then defined as:
IG(푣) = −
Õ
푐∈퐶
푃(푐) log 푃(푐)
+ 푃(푡푣)
Õ
푐∈퐶
푃(푐 | 푡푣) log 푝(푐 | 푡푣)
+ 푃(푡푣)
Õ
푐∈퐶
푃(푐 | 푡푣) log 푃(푐 | 푡푣) (9.7)
where 푃(푐) is the prior probability of class 푐, 푃(푐 | 푡푣) is the conditional probability
that a document belongs to class 푐, given that it contains the word 푣, and 푃(푐 | 푡푣)
is the conditional probability that a document belongs to class 푐, given that it does
not contain 푣. Note that the first addend of Eq. (9.7) does not depend on 푣 and can be
ignored to rank all 푣 ∈ 푉 in decreasing order of IG(푣).
Let 푀
def
= |D |. To estimate the relevant probabilities in Eq. (9.7), let 푚(푣, D) ≤ 푀
be the number of documents in D which contain 푣 and 푚푐 (푣, D) the number of doc￾uments of class 푐 which contain 푣. Correspondingly, 푀−푚(푣, D) and 푀푐−푚푐 (푣, D)
are the number of documents and the number documents of class 푐 that do not
contain 푣. Then, the probabilities used in Eq. (9.7) can be estimated as follows:9.5 Classification of Large Untranscribed Image Documents 245
푃(푡푣) =
푚(푣, D)
푀
푃(푡푣) =
푀 − 푚(푣, D)
푀
(9.8)
푃(푐 | 푡푣) =
푚푐 (푣, D)
푚(푣, D)
푃(푐 | 푡푣) =
푀푐 − 푚푐 (푣, D)
푀 − 푚(푣, D)
(9.9)
9.5.1.2 Feature Extraction
Using IG, a vocabulary 푉푛 of size 푛 ≤ 푁 can be defined by selecting the 푛 words
with highest IG. By attaching a (real-valued) feature to each 푣 ∈ 푉푛, a document 퐷
can be represented by a 푛-dimensional feature vector D ∈ R
푛
.
The value 퐷푣 of each feature 푣 is typically related with the frequency of 푣 in
퐷, denoted as 푛(푣, 퐷). However, absolute word frequencies can dramatically vary
with the size of the documents and normalized frequencies are generally preferred.
Let 푛(퐷) =
Í
푣∈푉푛
푛(푣, 퐷) be the total (or “running”) number of words in 퐷. The
normalized frequency of 푣 ∈ 푉푛, often called term frequency and denoted Tf(푣, 퐷),
is the ratio 푛(푣, 퐷) / 푛(퐷). Therefore, Tf(푣, 퐷) is a max-likelihood estimate of the
conditional probability of word 푣, given a document 퐷, 푃(푣| 퐷).
While Tf adequately deals with document size variability, it has been argued that
better DC accuracy can be achieved by further weighting each feature with a factor
that reflects its importance to predict the class of a document. Of course, IG could be
used for this purpose, but the so-called inverse document frequency (Idf) [15, 8, 2]
is argued to be preferable. Idf is defined as log(푀 / 푚(푣, D), which, according to
Eq. (9.8), can be written as − log 푃(푡푣).
Putting it all together, a document 퐷 is represented by a feature vector D. The
value of each feature, 퐷푣, is computed as the Tf·Idf of 퐷 and 푣; i.e., Tf(푣, 퐷),
weighted by Idf(푣):
퐷푣 = Tf·Idf(푣, 퐷) = Tf(푣, 퐷) · Idf(푣)
= 푃(푣| 퐷) log 1
푃(푡푣)
=
푛(푣, 퐷)
푛(퐷)
log 푀
푚(푣, D)
(9.10)
9.5.2 Estimating Text Features from Image PrIxs
Following the notation of previous sections, now a document 퐷 becomes an image
document 푋 and a document collection D becomes X. So now 푛(푋) is the number of
running words in the image document 푋 (typically encompassing several pages); i.e.,
푛(푋) ≡ 푛(퐷). Similarly, 푛(푣, 푋) ≡ 푛(푥, 퐷) is the frequency of the (pseudo-)word
푣 in 푋 and 푚(푣, X) ≡ 푚(푣, D) is the number of image documents in a collection
X which contain 푣. Of course, in text images, these counts are unknown but their
expected values can be computed as explained in 9.1. In addition, to compute the
IG, we need 푚푐 (푣, X), which can be estimated as in Eq. (9.13), but using only the
subset of image documents of class 푐, X푐. All in all, the required computations are
summarized here:246 9 Beyond Search Applications of Probabilistic Indexing
퐸[푛(푋)] =
Õ
푥⊑푋
Õ
푣,풃⊑푥
푃(푅 | 푥, 푣, 풃) (9.11)
퐸[푛(푣, 푋)] =
Õ
푥⊑푋
Õ
풃⊑푥
푃(푅 | 푥, 푣, 풃) (9.12)
퐸[푚(푣, X)] =
Õ
푋∈X
max
푥⊑푋, 풃⊑푥
푃(푅 | 푥, 푣, 풃) (9.13)
퐸[푚푐 (푣, X)] =
Õ
푋∈X푐
max
푥⊑푋, 풃⊑푥
푃(푅 | 푥, 푣, 풃) (9.14)
9.5.3 Image Document Classification
Let us first consider the most conventional Pattern Recognition (PR) classification
paradigm where each image document 푋 in X is assumed to belong to one of 퐶
known classes. We will refer to this setting as Closed Set Classification (CSC).
Using the Tf·Idf vector representation X of 푋, an optimal prediction of the class
of 푋 under the minimum-error risk statistical framework is [3]:
푐
★
(푋) = arg max
푐∈ {1,...,퐶}
푃(푐 | X) (9.15)
The posteriors 푃(푐 | X) can be computed following several well-known ap￾proaches, some of which were discussed and tested in [12, 4]. Following the results
reported in those papers, only the MLP is adopted in work described in this section.
The input to the MLP is X, the output is a SoftMax layer with 퐶 units, and training
is performed by backpropagation using the standard cross-entropy loss. Under these
conditions, it is well known that the each MLP output, 푐, approaches 푃(푐 | X),
1 ≤ 푐 ≤ 퐶. Thus Eq. (9.15) directly applies.
Such a CSC classifier is typically evaluated by its probability of error, estimated
as the Error Rate 푘푒/퐾, where 푘푒 is the number of wrong predictions made on a test
set of 퐾 image documents from the same 퐶 classes considered for training [3].
9.5.4 Open Set Classification
In the practical application of the methods discussed in this section, a complete
set of classes (i.e., typologies of notarial deeds, such as Will, Debenture, etc.) is
seldom known at the training time. Moreover, many of the classes represented in
the available GT often contain just one, or maybe a few samples (deeds) which are
hardly enough for training or testing. Clearly, these classes should be set aside in
the above CSC paradigm. But, in practice, new image documents do arrive which
need to be processed anyway and the classical CSC paradigm proves inadequate.
Instead, our problem naturally falls under the so called OSC (Open Set Classification)
framework [5, 9, 17, 19], where a larger number of (possibly unknown or uncertain)
classes, 퐶˜ > 퐶, is assumed to exist in X.9.5 Classification of Large Untranscribed Image Documents 247
Consider first a setup where the system can be trained with samples of all the 퐶
known classes plus an additional “Reject class” which encompasses the remaining
퐶˜ − 퐶 unknown classes. Clearly, all the GT classes with too few samples can be
properly included in this “class”. This is still a fairly traditional PR setting, which
amounts to training and classification with 퐶
′ = 퐶 + 1 classes [3]. Minimum error￾risk classification is also given by Eq. (9.15), changing 퐶 with 퐶
′
, and the traditional
Error Rate can still be reasonably used for OSC evaluation.
A different way to deal with unknown classes is to train the system using only
samples of the 퐶 known classes. A threshold 푡 is then needed to establish a class
posterior probability below which any test sample should be rejected; i.e., considered
to belong to a Reject Class (RJ). Formally, let 푄(X)
def
= max1≤푐≤퐶 푃(푐 | X). Then:
푐
★
(푋) =



arg max
푐∈ {1,...,퐶}
푃(푐 | X) if 푄(X) ≥ 푡
Reject otherwise
(9.16)
Following this scheme, several approaches can be used for OSC with RJ and
training with only the 퐶 known classes. In addition to directly using a MLP, trained
with Tf·Idf input vectors from the 퐶 known classes as discussed at the beginning of
this section, we have adapted the ideas of [19] to our OSC CBIDC task.
In the model proposed in [19], called one versus rest, the output layer of a neural
network is configured as a vector of 퐶 Sigmoid activation functions. That is, each
output 푐, 1 ≤ 푐 ≤ 퐶, corresponds to a Bernoulli distribution 푃(푏푐 | X), where 푏푐 is
the value of a binary random variable which is 1 if the class of 푋 is 푐 and 0 otherwise.
In our MLP architecture this idea simply leads to changing the SoftMax output layer
(which corresponds to a categorical distribution 푃(푐 | X)), with a 1-versus-rest layer,
using the corresponding 퐶–variate binary cross-entropy loss for training, as in [19].
This model will be referred to as binary-outputs MLP (bMLP).
If a single, fixed threshold 푡 can be assumed or somehow estimated,6 the bMLP
model can straightforwardly implement OSC with Reject, exactly as in Eq. (9.16),
by assuming that 푃(푐 | X), 1≤ 푐 ≤ 퐶, are the output probabilities yield by the model.
Also, the OSC Error Rate can be straightforwardly measured in the same way for
MLP and bMLP.
Nevertheless, letting the user adjust the reject threshold is a convenient, practical
option to help tailoring a trained system to the rejection needs of each specific batch
of test data. To assess rejection performance in this scenario, a Receiver Operating
Characteristic (ROC) curve [10] can be plotted to characterise the system for all
the possible thresholds. The area under this curve, called AUROC, is a commonly
accepted scalar measure that adequately assesses the system’s overall performance
for all reject thresholds. A ROC curve assumes binary decisions. In our case, the
task is to decide whether a test sample (deed image document) is or is not from one
of the 퐶 known classes.
6 We have considered using a heuristic method is proposed in [19], which can estimate a different
threshold per class. However, since no improvement has been observed using multiple thresholds,
the simpler single-threshold setting has been adopted in some of our experiments.248 9 Beyond Search Applications of Probabilistic Indexing
9.5.5 Experiments
In this section, we provide details of the dataset, the empirical framework adopted
for the experiments, and the achieved results.
9.5.5.1 Dataset
The dataset considered here is a small part of a large series of historical notarial
manuscripts held by the Spanish Archivo Historico Provincial de C ´ adiz ´ (AHPC).
See Appendix C.12 for details.
Two books of this series, MBD 4949 and JMBD 4950, dated 1723–1724, consti￾tute the present dataset.7 They were manually divided into sequential sections, each
corresponding to a single deed, which was then annotated with a class label. Apart
from these annotations, note that no typical GT data (such as text lines or transcripts)
are available for these manuscripts.
The classes of some deeds were not clear and, for many of the clearly identified
classes, only very few deeds were available. To allow the classification results to be
sufficiently reliable, only those classes having at least one deed in each book and six
deeds in total were taken into account. This way, 498 deeds were retained from 12
classes considered sufficiently represented and all the other, belonging to 29 unclear
or poorly represented classes, were collectively deemed to belong to a special “class”
called Reject (RJ).
The twelve well-represented classes are:Power of Attorney (PA), Letter of Payment
(LP), Debenture (DB), Lease (LE), Testament (TE), Sale (SA), Risk (RI), Census
(CS), Deposit (DP), Statement (ST), Cession (CN) and Treaty of Fact (TF). Table 9.2
shows how these classes are distributed in the dataset.
Table 9.2: Number of documents and page images for JMBD 4949 and JMBD 4950: per
class, per document & class, and totals.
Class Deeds Pages
ID’s Avg Min Max St-dev Total
PA 240 3.3 2 24 3.5 803
LP 72 4.8 2 30 5.4 345
DB 44 4.8 2 32 5.6 212
LE 32 4.8 2 16 2.6 152
TE 29 8.6 4 48 9.4 248
SA 21 22.9 4 122 29.8 480
RI 17 4.0 4 4 0.0 68
CS 12 11.5 2 26 9.0 138
DP 10 3.8 2 8 1.9 38
ST 9 2.4 2 4 0.8 22
CN 6 5.3 2 14 3.9 32
TF 6 5.3 4 8 1.9 32
Reject 57 9.2 2 70 12.2 526
Total 555 5.6 2 122 9.2 3096
7 To allow reproducibility, all the required data (and code) are publicly available from:
https://github.com/JoseRPrietoF/docClassPrIx9.5 Classification of Large Untranscribed Image Documents 249
The Closed Set machine learning task consists in training a model to classify a
deed known to belong to one of the 퐶 = 12 proper classes into one of these same
classes. The corresponding Open Set task is to also let the system reject samples
(deeds) from the remaining 29 classes. That is, 퐶˜ = 12 + 29 = 41 and the proportion
of known classes is 29.3%.
No training / test partition is specified for this dataset and the commonly adopted
empirical protocol is Leaving One (deed) Out (LOO).
9.5.5.2 Empirical settings
PrIxs typically contain huge amounts of different pseudo-word hypotheses. However,
many of these hypotheses have low relevance probability and most of the low￾probability pseudo-words are not real words. Therefore, as a first step, entries with
less than three characters, as well as those with too low RP (푃(푅 | 푥, 푣, 풃) < 0.1),
were pruned out. This reduced the original set of 809 787 different pseudo-words to
a vocabulary 푉 of 55 927 pseudo-words for the two bundles considered.
Then, as discussed in Sec. 9.5.1, the pseudo-words in 푉 were sorted by decreasing
IG and the first 푛 entries were selected to define a BOW vocabulary푉푛. Exponentially
increasing values of 푛 from 16 up to 16 384 were considered in the experiments.
Finally, a Tf·Idf 푛-dimensional vector was calculated for each deed, 푋 ∈ X. To
simplify experiments with different values of 푛, Tf·Idf(푣, 푋) was estimated just once
for each 푣 ∈ 푉, using a Tf normalising factor 퐸[푛(푋)] (see Eq. (9.11)) computed for
all 푣 ∈ 푉, rather than just 푣 ∈ 푉푛 for every 푛 considered in the experiments.8 Tf·Idf
deed vectors were further normalised by subtracting the mean and dividing by the
standard deviation, resulting in zero-mean and unit-variance input vectors.
Three MLP configurations with different numbers of layers were considered. In
all the cases, every layer except the last one is followed by batch normalisation
and ReLU activation functions [7]. Fairly conventional parameter initialization and
training procedures were adopted for both MLP and bMLP (see details in [13]).
The basic configuration was a plain퐶-class or퐶
′
-class perceptron where the input
is totally connected to each of the 퐶 = 12 or 퐶
′ = 13 neurons of the output layer
(hence no hidden layers are used). For the sake of simplifying the terminology, here
we consider such a model as a “0-hidden-layers MLP” and refer to it as MLP-0. The
next configuration, MLP-1, was a proper MLP with one hidden 128-neurons layer.
This layer was expected to do some kind of word clustering, hopefully improving
the classification ability of the output layer. Finally, an MLP-2 and a bMLP-2, with
two hidden 128-neurons layers, were tested. Deeper models were tried too, but they
did not yield significant improvements.
As discussed in Sec. 9.5.4, to use models trained only with the 12 known classes
for OSC, a threshold 푡 is required which has to be determined or somehow estimated.
8 퐸[푛(푋) ] is the expected number of running words in 푉, which can be larger than the same
estimate if only the words in 푉푛 are considered (in the summation on 푣 of Eq. (9.11)). For every 푛,
this normalising factor is thus the same for all the components of the Tf·Idf vectors, and it has not
been observed to significantly affect the classification results.250 9 Beyond Search Applications of Probabilistic Indexing
Two simple heuristic methods were considered. The first one is that proposed in [19],
which we compute as:
푡 = 1 −

1
퐾
Õ
푋
￾
1 − 푃푐ˇ (푋)
2
1/2
(9.17)
where the sum spans the 퐾 = 498 samples of known classes, 푃푐ˇ (푋) is 푃(푐ˇ(푋) | 푋)
for MLP or 푃(푏푐ˇ(푋)
| 푋) for bMLP, and ˇ푐(푋) is the correct (GT) class of 푋.
The second, rather crude heuristic comes from the observation that the exact
value of 푡 is not critical, provided it is around the average values of the max class
posteriors of the test samples. So the threshold can be just set to this average. While
this estimate is based on test sample posteriors, it is totally fair, since GT class labels
are not used at all.
As suggested by Table 9.2 above, all the deeds available in the books JMBD 4949
and JMBD 4950 are considered as a single dataset, which encompasses 498 samples
in 12 classes for CSC and 555 in 41 classes for OSC. Since these numbers are too
small for a reasonable, fixed training / test partition, a LOO protocol is adopted.
LOO entails certain issues in some of the experimental procedures. First, to
simplify the computation of IG and Tf·Idf, the calculations were performed only
once for all the classes and samples. Nevertheless, it has been observed that leaving
or not a single sample out hardly changes IG and Tf·Idf values significantly. Second,
for computing the threshold estimate of Eq. (9.17), the posterior probabilities of all
the 498 samples 푋 ∈ X of known classes have been used. While this simplification
breaks to some extent the test-set independence principle, it should be noted that the
values of these estimates are not critical, as will be discussed in Sec. 9.5.5.3.
9.5.5.3 Experiments and Results
Results using the methods presented in Sec. 9.5.3 and the dataset and empirical
settings discussed in Sec. 9.5.5 are reported below. First we focus on CSC and also
on OSC methods which rely on training with samples (deeds) of classes considered
unknown to avoid the need of a reject threshold. The second subsection is devoted
to OSC with models trained only with samples of known classes – which thereby
requires a threshold to reject test samples deemed not to belong to any known class.
Threshold-less Closed and Open Set Classification
Fig. 9.8 shows two sets of results all obtained according to Eq. (9.15) (Sec. 9.5.4).
First, traditional CSC results achieved using three MLP models trained and tested
only with samples of the 12 known classes. Then OSC with the same models but now
trained with samples of the 13 classes: 12 known proper classes plus a special Reject
class, which includes samples from 29 additional classes. In both cases, results are
shown for increasing dimension (number of IG-selected words) of the Tf·Idf image
document embeddings9.5 Classification of Large Untranscribed Image Documents 251
5
10
20
30
40
50
16 64 256 1024 4096 16384
Error rate (%)
Number of features (IG-selected words)
MLP-0 OSC
MLP-1 OSC
MLP-2 OSC
MLP-0 CSC
MLP-1 CSC
MLP-2 CSC
Fig. 9.8: Leaving-one-out classification error rate on JMBD 4949 and JMBD 4950 with
three threshold-less MLP models, both for CSC, training and testing with 12 known
classes, and OSC, training and testing with 12 known classes plus Reject (13 “classes”).
All the results are based on PrIx document and word frequency estimates. 95% confidence
intervals (not shown for clarity) are all smaller than ±4.4% and smaller than ±3.0% for all
the error rates below 15%.
CSC results are obviously better than those of their OSC counterparts. Under the
traditional CSC framework, these results suggest that, using MLP-1 and 512 words
(or more) for Tf·Idf representation, more than 93% of our image documents (deeds)
could be automatically tagged with the correct classes.
MLP-2 yields the best OSC and CSC results, with input image documents em￾bedded into a 2048-dimensional Tf·Idf vector space. The first column of Table 9.3
summarises these results.
Table 9.3: Classification error rate of threshold-less methods. CSC: training and testing
with 12 known classes; OSC: training and testing with 12 known plus a Reject “class”.
Results are shown for 푛 = 2048 words and both PrIx image representations and plaintext
HTR image transcripts.
Images represented as: PrIx HTR
Classifier MLP-2 bMLP-2 MLP-2
CSC (퐶 =12) 6.2 6.2 8.0
OSC (퐶′ =13) 10.5 11.0 12.3
Table 9.3 also reports comparable results using a bMLP-2 classifier (c.f.,
Sec. 9.5.4). Even though the bMLP-2 output layer and training loss do not aim
to maximise class discrimination, this model achieves almost the same results as
MLP-2. The classification accuracy achieved is remarkable, given the complexity of
the task: classify sets of images of untranscribed manuscripts (with as many as 122
images per set, see Table 9.2) into 12 (or 12 + 1) different classes, which only differ
from each other in nuances characterised by subtle combinations of words.252 9 Beyond Search Applications of Probabilistic Indexing
For completeness, Table 9.3 also reports results obtained with exactly the same
MLP-2 classifier, but using state-of-the-art HTR image transcripts [16, 14], rather
than PrIx, to represent the images. In this case, documents and word frequencies
needed for IG and Tf·Idf were naively computed using Eqs. (9.7–9.10) from the
noisy plain-text HTR output. As expected, these results fall short of those obtained
with the proposed approach, where document and word frequencies are estimated
(rather than computed) using PrIx image representations.
Threshold-based Open Set Classification and Rejection
Here models are trained only on samples of the 12 known classes, but the test set
includes samples both from these 12 classes and also from the other 29 classes
considered unknown. So the task entails both classification and rejection. OSC
Error Rates are reported in Table 9.4. As in the previous subsection, these error
rates include three types of errors: a) conventional known-class misclassification, b)
rejecting samples from known classes and c) failing to reject samples from unknown
classes. Given that noReject Classis trained, OSC must follow Eq. (9.16) (Sec. 9.5.4),
which requires a threshold 푡. Table 9.4 reports results for two fixed thresholds and for
another two thresholds, estimated as discussed in Sec. 9.5.5.2. An oracle threshold
is also included which was just determined as the one for which the Error Rate was
lowest on the test-set .
Table 9.4: OSC classification + rejection error rate using PrIx and 푛 = 2 048 words.
Models trained with 퐶 = 12 classes and tested with samples of all 퐶˜ = 41 classes (12
known plus 29 Reject “classes”), with different thresholds (푡). All 95% confidence intervals
within ±3.2%, or ±2.2% for the lowest error rates.
Threshold estimate MLP-2 (t) bMLP-2 (t)
Fixed 0.0 15.9 (0.00) 15.9 (0.00)
Fixed 0.5 15.3 (0.50) 16.4 (0.50)
1 − 휎 [19] 13.9 (0.76) 6.5 (0.75)
Average max class posterior 13.0 (0.97) 7.2 (0.94)
Best on test (“oracle”) 13.0 (0.98) 6.5 (0.75)
Results with the two estimated thresholds are similar and close to the oracle. In
fact, estimates are not critical and, for bMLP-2, similar error rates are observed for
any threshold in the range [0.70, 0.97].
Overall we can conclude that bMLP-2 provides excellent accuracy in full,
threshold-based OSC, very close to the best result achieved in basic CSC, but now
including also the duty of rejecting samples from unknown classes.
Table 9.5 shows the AUROC rejection results (see Sec. 9.5.4), which assess re￾jection performance taking into account all the possible thresholds. The table also
shows the Error Rates of the corresponding binary classification task (Reject – not￾Reject) for some thresholds. The rejection performance achieved by bMLP-2 is close
to perfect, which explains the OSC superiority of bMLP-2 discussed above.9.5 Classification of Large Untranscribed Image Documents 253
Table 9.5: Rejection performance for bMLP-2 OSC with PrIx and 푛 = 2 048 words.
Training with 퐶 = 12 classes, testing with samples of all 퐶˜ = 41 classes. AUROC values
(%) and rejection error rate (%) for various thresholds 푡.
Model Threshold (푡)
AUROC
0.00 0.50 0.75 0.76 0.94 0.97 0.98
MLP-2 10.3 9.7 9.2 9.2 9.4 10.8 10.3 92.2
bMLP-2 10.3 11.2 2.2 2.2 3.2 3.6 4.1 98.3
9.5.6 Image Document Classification Concluding Remarks
Plaintext Document Classification is a classical text analytics or natural language
processing task. Here it has been shown how this task can also be tackled for
untranscribed text image documents. To this end, the classically required word and
document counts are replaced by expected values of these counts estimated using
the PrIxs of the images. This task (here called CBIDC) is challenging because each
image document typically encompasses many images of handwritten text which are
hard to read, even by humans.
Our approach is cost-effective, because it does not need image transcripts. The
only ground truth needed for model training is the class label of each training
document and, once trained, the models provide accurate automatic CBIDC for new,
also untranscribed multi-page image documents.
It has been also shown that, using PrIxs, the classification models consistently
provide better results than using a popular, naive approach, where images are rep￾resented by their noisy automatic HTR transcripts. Our study includes both the
traditional classification viewpoint (CSC) and the Open Set (OSC) framework which
is much more realistic and close to practical requirements.
According to the experts who annotated the GT data used in our experiments, the
accuracy achieved is close to the limit of human-labeling uncertainty.254 9 Beyond Search Applications of Probabilistic Indexing
References
1. Aggarwal, C.C., Zhai, C.: Mining text data. Springer Science & Business Media (2012)
2. Aizawa, A.: An information-theoretic perspective of Tf–Tdf measures. Inf. Proc. & Manage￾ment 39(1), 45–65 (2003)
3. Duda, R.O., Hart, P.E.: Pattern Classification and Scene Analysis. J. Wiley and Sons, (1973)
4. Flores, J.J., Prieto, J.R., Garrido, D., Alonso, C., Vidal, E.: Classification of untranscribed
handwritten notarial documents by textual contents. In: A.J. Pinho, P. Georgieva, L.F. Teixeira,
J.A. Sanchez (eds.) Pattern Recognition and Image Analysis, pp. 14–26. Springer (2022) ´
5. Geng, C., Huang, S.J., Chen, S.: Recent advances in open set recognition: A survey. IEEE
Transactions on Pattern Analysis and Machine Intelligence 43(10), 3614–3631 (2021)
6. Ikonomakis, M., Kotsiantis, S., Tampakas, V.: Text classification using machine learning
techniques. WSEAS transactions on computers 4,8, 966–974 (2005)
7. Ioffe, S., Szegedy, C.: Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift. In: F. Bach, D. Blei (eds.) Proceedings of the 32nd International
Conference on Machine Learning, vol. 37, pp. 448–456. PMLR, Lille, France (2015)
8. Joachims, T.: A probabilistic analysis of the Rocchio algorithm with TFIDF for text catego￾rization. Tech. rep., Carnegie-mellon univ pittsburgh pa dept of computer science (1996)
9. Mahdavi, A., Carvalho, M.: A survey on open set recognition. In: 2021 IEEE Fourth Int. Conf.
on Artificial Intelligence and Knowledge Engineering (AIKE), pp. 37–44 (2021)
10. Manning, C.D., Raghavan, P., Sch ¨utze, H.: Introduction to Information Retrieval. Cambridge
University Press, New York, NY, USA (2008)
11. Manning, C.D., Sch ¨utze, H., et al.: Foundations of statistical natural language processing, vol.
999. MIT Press (1999)
12. Prieto, J.R., Bosch, V., Vidal, E., Alonso, C., Orcero, M.C., Marquez, L.: Textual-content￾based classification of bundles of untranscribed manuscript images. In: 2020 25th International
Conference on Pattern Recognition (ICPR), pp. 3162–3169 (2021)
13. Prieto, J.R., Flores, J.J., Vidal, E., Toselli, A.H.: Open set classification of untranscribed
handwritten text image documents. Pattern Recognition Letters 172, 113–120 (2023)
14. Romero, V., Toselli, A.H., Vidal, E., Sanchez, J.A., Alonso, C., Marqu ´ es, L.: Modern vs Diplo- ´
matic Transcripts for Historical Handwritten Text Tecognition. In: International Conference
on Image Analysis and Processing, pp. 103–114. Springer (2019)
15. Salton, G., Buckley, C.: Term-weighting approaches in automatic text retrieval. Inf. Proc. &
Management 24(5), 513/523 (1988)
16. Sanchez, J.A., Romero, V., Toselli, A.H., Villegas, M., Vidal, E.: A set of benchmarks for ´
handwritten text recognition on historical documents. Pattern Recognition 94, 122–134 (2019)
17. Scheirer, W.J., Jain, L.P., Boult, T.E.: Probability models for open set recognition. IEEE
Transactions on Pattern Analysis and Machine Intelligence 36(11), 2317–2324 (2014)
18. Sevim, S., Omurca, S.˙I., Ekinci, E.: Document image classification with vision transformers.
In: M.N. Seyman (ed.) Electrical and Computer Engineering, pp. 68–81. Springer (2022)
19. Shu, L., Xu, H., Liu, B.: DOC: Deep open classification of text documents. In: Proceedings
of the 2017 Conf. on Empirical Methods in Natural Language Processing, pp. 2911–2916.
Association for Computational Linguistics, Copenhagen, Denmark (2017)
20. Toselli, A.H., Vidal, E., Puigcerver, J., Noya-Garc´ıa, E.: Probabilistic multi-word spotting in
handwritten text images. Pattern Analysis and Applications (2019)
21. Vidal, E., Romero, V., Toselli, A.H., Sanchez, J.A., Bosch, V., Quir ´ os, L., Bened ´ ´ı, J.M.,
Prieto, J.R., Pastor, M., Casacuberta, F., et al.: The carabela project and manuscript collection:
large-scale probabilistic indexing and content-based classification. In: 2020 17th International
Conference on Frontiers in Handwriting Recognition (ICFHR), pp. 85–90. IEEE (2020)
22. Xu, Y., Xu, Y., Lv, T., Cui, L., Wei, F., Wang, G., Lu, Y., Florencio, D., Zhang, C., Che,
W., Zhang, M., Zhou, L.: LayoutLMv2: Multi-modal pre-training for visually-rich document
understanding. In: Proceedings of the 59th Annual Meeting of the Association for Computa￾tional Linguistics and the 11th International Joint Conference on Natural Language Processing
(Vol.1: Long Papers), pp. 2579–2591. Association for Computational Linguistics (2021)Chapter 10
Large-scale Systems and Applications
Abstract Here we outline how the PrIx technologies presented throughout this book
have been implemented into actual search and retrieval systems which provide fast
and effective access to the textual contents of many important series of historical
manuscripts. We cover in detail five of these systems, which support large series
provided by the French Archives Nationales, the Spanish Biblioteca Nacional and
Archivos Nacionales, the British UCL Library, and the Finnish Kansallisarkisto (Na￾tional Archives of Finland). All these large-scale demonstrators are publicly available
through the Internet and provide a broad array of searching tools. Specifically, wild￾cards and approximate or flexible spelling are supported at character level and at
word level, word-sequences and all kinds of multi-word Boolean combinations are
allowed, including geometrical-proximity conditioned conjunction (AND).
10.1 Conceptual System Organization and Workflow
A general overview of the main modules and workflow of a typical implementation
of a PrIx-based search and retrieval system is shown in Fig. 10.1.
Ingest
 Search engine
and User Interface Database
’Spots’
(PrIxs)
Probabilistic
Indexes images
Text
<KWindex pageID="FRCHANJJ_JJ070_0040R_A-chanceryDemo38.jpg">
<spot kw="A" s=1.000 x=872 y=230 w=53 h=29 /> <spot kw="A" s=1.000 x=844 y=72 w=23 h=47 /> <spot kw="AVENIR" s=1.000 x=1054 y=72 w=100 h=47 />
<spot kw="AVOIT" s=1.000 x=1135 y=181 w=82 h=32 />
<spot kw="CE" s=1.000 x=1303 y=72 w=40 h=47 /> <spot kw="COMME" s=1.000 x=1194 y=72 w=70 h=47 />
...
<spot kw="NOSTRE" s=1.000 x=1172 y=124 w=83 h=37 />
<spot kw="NOUS" s=1.000 x=445 y=181 w=74 h=32 />
<spot kw="NOUS" s=1.000 x=887 y=124 w=79 h=37 />
...
<spot kw="QUE" s=1.000 x=1134 y=72 w=53 h=47 /> <spot kw="QUELLE" s=1.000 x=752 y=124 w=83 h=37 />
<spot kw="RECEVEUR" s=1.000 x=1268 y=124 w=121 h=37 />
<spot kw="ROY" s=1.000 x=468 y=72 w=61 h=47 /> ...
<spot kw="TOUZ" s=1.000 x=878 y=72 w=65 h=47 />
<spot kw="UNE" s=1.000 x=739 y=181 w=53 h=32 /> <spot kw="VILLE" s=1.000 x=1289 y=181 w=66 h=32 />
<spot kw="FEU" s=0.999 x=547 y=230 w=35 h=29 /> <spot kw="MAISON" s=0.999 x=820 y=181 w=87 h=32 />
<spot kw="POIER" s=0.997 x=512 y=181 w=70 h=32 />
<spot kw="TENUZ" s=0.997 x=238 y=124 w=74 h=37 />
<spot kw="CRIER" s=0.996 x=195 y=181 w=70 h=32 />
<spot kw="TENANT" s=0.996 x=235 y=230 w=87 h=29 />
<spot kw="PROQUIN" s=0.982 x=624 y=230 w=138 h=29 />
<spot kw="LODUN" s=0.980 x=1388 y=181 w=74 h=32 />
<spot kw="PART" s=0.972 x=356 y=230 w=48 h=29 />
<spot kw="D’ARGENT" s=0.958 x=597 y=124 w=108 h=37 />
<spot kw="ESTIENNE" s=0.955 x=1047 y=181 w=104 h=32 />
<spot kw="AIT" s=0.953 x=1446 y=124 w=83 h=37 /> <spot kw="VENTE" s=0.914 x=300 y=181 w=74 h=32 />
<spot kw="TROIZ" s=0.902 x=147 y=230 w=91 h=29 />
<spot kw="DITE" s=0.895 x=632 y=181 w=44 h=32 />
<spot kw="PHILIPPES" s=0.885 x=103 y=72 w=113 h=47 />
...
<spot kw="METTRE" s=0.798 x=98 y=181 w=83 h=32 />
<spot kw="PART" s=0.667 x=990 y=72 w=40 h=47 />
<spot kw="CERTEINE" s=0.625 x=387 y=124 w=168 h=37 />
<spot kw="BOURS" s=0.614 x=1399 y=124 w=83 h=37 />
<spot kw="QUE" s=0.611 x=332 y=124 w=57 h=37 /> <spot kw="SATISFAIRE" s=0.584 x=1075 y=124 w=113 h=37 />
<spot kw="ESCUNE" s=0.450 x=1397 y=72 w=74 h=47 />
...
<spot kw="FAT" s=0.100 x=1523 y=124 w=185 h=37 /> <spot kw="CROIZ" s=0.087 x=147 y=230 w=91 h=29 />
<spot kw="JOURONDE" s=0.077 x=1516 y=181 w=203 h=32 />
...
<spot kw="MAVANT" s=0.001 x=1296 y=230 w=104 h=29 />
<spot kw="PHILIPS" s=0.001 x=105 y=72 w=117 h=47 />
<spot kw="SAATISFAIRE" s=0.001 x=1075 y=124 w=113 h=37 />
</KWindex>
 Probabilistic 
Indexing (PrIx)
Fig. 10.1: PrIx-based Search and Retrieval System Diagram.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 255
A. H. Toselli et al., Probabilistic Indexing for Information Search and Retrieval 
in Large Collections of Handwritten Text Images, The Information Retrieval Series 49, 
https://doi.org/10.1007/978-3-031-55389-9_10 
 256 10 Large-scale Systems and Applications
“Probabilistic Indexing (PrIx)” is the most important module which embodies
off-line pre-computation of PrIxs using the methods discussed in Chapters 3, 4 and 5.
Word-level PrIxs produced by this module are stored in an adequate database where
individual PrIx spots are indexed for fast retrieval according to the pseudo-word keys
and the RP of each spot. This is performed also off-line by an “Ingestion” module
which typically implements simple and computationally cheap processes. Finally,
the “Search engine and User Interface” model is in charge of on-line user query
analysis, searching the database for the requested information and presenting the
retrieved images. This module is also important because it should provide a friendly
user interface and must ensure short response times.
Additional details of each of these modules are given in the following subsections.
10.1.1 PrIx Components
Main components of a PrIx implementation were already introduced at the end of
Chapter 3, Fig. 3.11. For convenience, that figure is reproduced here (Fig. 10.2).
Transcribed
images (GT)
Probabilistic Indexing (PrIx)
Text images
Optical & Lang.
 Models
Training
Computing spots:
 words, RPs & 
 Boinding boxes
Contextual word
recognizer (HTR)
char/word
 lattices
Page-level
PrIxs
Fig. 10.2: PrIx Components.
This module entails two working phases: “Training” and PrIx proper. Since
all PrIx models and methods are based on ML, the starting point is a training
set of transcribed images which are representative of the text image collection being
considered. These training data are used to train both “Optical & Language Models”,
as discussed in Chapter 4. External text, of the same language(s) and with features
as those of the task in hand, can be also used to train the language model.
This phase is very much alike training conventional HTR models. In the current
implementations, it typically works at the text-line level and therefore requires some
(simple) form of layout analysis. Nevertheless, as compared with HTR, this process is
simpler and more robust since PrIx does not need any type of precise decomposition
of the image into related parts or blocks and only (relatively coarse) line detection is
needed. The training process yields as a result optical and language models which
currently are based on CRNNs and 푛-grams, as discussed in Chapter 4.
Using the trained models, a “Contextual word recognizer” is used to process all
the text images to be indexed. In this process, conventional HTR would only need
to produce a best transcript of each considered image region (typically a text line).
But, as discussed in Chapters 3 and 5, PrIx needs character lattices (CLs). So, as10.1 Conceptual System Organization and Workflow 257
compared with plain HTR, this step is much more complex and computationally
demanding – refer to Chapter 4 for details.
Finally, CLs are analyzed in the last module, using the methods presented in
Chapter 5 to extract the sets of PrIx spots of each page image.
All in all, in general, PrIx can require significant amounts of computation and,
for large image collections, decent computational resources are typically needed.
10.1.2 Spots Database
The sets of spots of all individual image PrIxs are compiled into a data structure,
adequate for fast operation of the search engine. This process, depicted in Fig. 10.3,
is called “Ingestion”.
Ingestion
 Spots
Database
Page-level
PrIxs
Char folding, word grouping,
index triming, data structuring
Fig. 10.3: PrIx Ingestion.
“Ingestion” is very much dependent on the specific database adopted to store
the PrIxs. Of course, a best approach is to implement an ad-hoc database, as we
have done for all the demonstrators described in the coming sections of this chapter.
This allows to support all the query types and search tools in the most efficient way,
including the proper handling of the probabilistic information.
Nevertheless, a perhaps simpler alternative is to relay on existing database man￾agement systems which provide the required features. For example, platforms such
as Elasticsearch1 and Solr2 have been successfully adopted in some cases to work
with PrIxs data. In any case, ingestion generally involves the following processes:
• Case and diacritics folding. In most cases, only a simplified spelling of pseudo￾words is indexed; typically all characters are converted into single (upper or
lower) case. Similarly, all (or most) nonessential diacritics are removed. When
this is carried out at this stage, it is referred to as late transliteration.
• Word grouping – e.g., to index lemmas rather than regular words
• Organize the page images and the corresponding spots according to the chosen
hierarchical structure.
• Trim the PrIxs to the desired indexing density. Density can be expressed as a
RP threshold, or as a number specifying how many spots per page, per image
region, or per running word should be indexed.
These are all simple, albeit not trivial processes which generally only require light
computing resources.
1 https://www.elastic.co
2 https://solr.apache.org258 10 Large-scale Systems and Applications
10.1.3 PrIx Search Engine and User Interface
In contrast with the previous modules, this one has to work on-line, when users are
actually interacting with the system to search for information in the text images.
Keyword Search engine
Query
analysis GUI retrived images
Display
Text images
search
Database
Fig. 10.4: PrIx Search engine and user interface.
Several key components are included in this module. First, the user interface
(GUI) which must support user-friendly graphical / textual specification of queries
and the desired precision-recall tradeoff.
Queries may be as simple as a single word or as complex as Boolean combination
of several words, each of which maybe only partially spelled, or with some spelling
flexibility specification. Therefore, the query has to be formatted as a formal language
sentence that can be parsed by the “Query analysis” component.
In turn, the parse tree resulting from this analysis has to be converted into a form
which the “Search engine” can understand. And this process is tightly related with the
database of choice. The search engine is also in charge of issuing the actual database
request for information and to selectively send this information to the “GUI” and
“Display” components. An important part of this information is probabilistic and the
engine has to adequately combine retrieved RPs, according to the parse tree derived
from the user query, into the RP of each image and image region to be shown to the
user.
Finally, the main task of the “Display” component is to actually fetch the relevant
images and present them to the user, adding the required geometrical and/or textual
information, such as bounding boxes and/or specific word transcription hypotheses
and RPs.
All these components, the “GUI” and “Display” in particular, are highly appli￾cation dependent. So they should be implemented so as to allow for a great deal of
flexibility and adaptability.10.2 Architecture Design 259
10.2 Architecture Design
Following the general scheme and workflow principles discussed in the previous
section, here we explain the specific architecture of the demonstrators we have im￾plemented for the large-scale manuscript collections to be presented in the coming
sections. It is assumed the PrIx of each image of the collection is already avail￾able. Therefore, with respect to the conceptual scheme and workflow discussed in
Sec. 10.1, this section mainly focuses on the “Search Engine” and “User Interface” –
i.e., it does not deal with the main block of Fig. 10.1, namely Probabilistic Indexing.
The design is oriented to Internet operation and follows a client–server architec￾ture made of four main components: the “Web client”, the “Web server”, the “Data
server” and the “PrIx server”, as shown in Fig. 10.5.
Web client Web server
PrIx server
Data server
Fig. 10.5: Client–server architecture used by the large-scale demonstrators. The web client
is any conventional web browser, the web server receives the user’s request and prepares
the response as HTML files. The PrIx server contains the probabilistic index, similar to
an “inverted index” used in most conventional search engines. The data server stores all
images and metadata of the documents in a simple database.
10.2.1 Web and Data Servers
The main task of the web server is to serve the HTML pages of the user interface
and prepare the responses to the user’s requests. A HTTP web server (Apache) is
used with PHP support. The web server connects to the other servers as required.
For instance, this service does not directly store the document images and meta￾data, which are stored in a separate database, handled by the data server (although
in practice, the data server and the web server often reside in the same machine).
Likewise, when the web server receives a query from the client, it redirects the
query to the PrIx server, and prepares the HTML pages that will present the relevant
results to the user.260 10 Large-scale Systems and Applications
10.2.2 PrIx Server and Search Engine
The PrIx server is the main component of the system and implements a probabilistic
(pseudo-)word index and the associated search engine, similar to the ones used by
conventional search engines. This server implements a “HTTP-based RESTful API”,
which is used by the web server to redirect the user’s queries.
The index has a conceptual hierarchical structure, as represented in Fig. 10.6. In
our current implementation, we actually use a hierarchy of hash tables to represent
this conceptual hierarchy.
...
...
...
Book-1 Book-2
Colection-1
Page-1 Page-2
Fig. 10.6: Representation of the hierarchical index used in the large-scale demonstrators.
At the top of the hierarchy, we use a hash table that maps pseudo-words to
the set of collections that contain (with an estimated probability greater than 0) at
least one instance of such pseudo-word. Then, two additional sets of hash tables
are used to map to the collection’s books that contain each term, and to relevant
pages within the book. Finally, for each page image, a (ranked) list of all relevant
pseudo-word instances within that page is stored, along with the corresponding RPs
and bounding-box coordinates.
The search engine implemented in this way is extremely time-efficient. It honors
usual queries in just a few milliseconds, regardless the size of the indexed image
collection. Fig. 10.7 shows the behavior of this search engine for 1 319 random single￾word queries to the full Chancery collection, with about one hundred thousand
page images, more than 47 million estimated running words and more than three
hundred million spots (see Sec. 10.3.1, below). The queries were sampled so as to
approximately achieve a uniform distribution of the number of occurrences of the
selected words in the full collection. Response time is below 10 milliseconds and
does not vary significantly for usual queries, which seldom entail requesting many
occurrences. Above 1 000 retrieved occurrences, response time grows almost linearly
with the number of retrieved spots, which is the optimal behavior expected.
Notice that using hash tables allows for extremely fast searches at any particular
level of the hierarchy, since accessing a particular level can be done in constant time10.2 Architecture Design 261
 1
 10
 100
 1000
 10000
1 10 100 1K 10K 100K 1M
Query response time (Miliseconds)
Number of Retrieved Spots per Query
Fig. 10.7: Search engine response time as a function of the number of occurrences
retrieved. Scatter plot for 1 319 random single-word queries.
(on average). However, this representation is not the most memory efficient one, since
the explicit implementation of the hierarchy has a significant memory overhead.
This memory footprint could be reduced significantly using a “linear” layout of
the hierarchy, for instance keeping the tuples (pseudo-word, collection, book, page)
sorted. Then, any level could be accessed in 푂(log 푛) (where 푛 is the total number
of indexed pages), which would result in virtually the same time as the nested hash
table architecture in real use cases, but much lower spatial cost.
An additional advantage of this linear layout is that, since pseudo-words would
be lexicographically ordered, we could support prefix queries very efficiently. Nev￾ertheless, our current implementation does not support this feature and wildcard
spelling is supported by a specialized software module.
10.2.3 Web Client
From the user perspective, the system is simply a web page, similar to a conventional
plain-text search engine, which users can access using their favorite web client
(Firefox, Chrome, Safari, etc.). This way, we can support users using multiple devices
and operating systems. The web client communicates through the web server with
the rest of the components.
The web interface consists of a search box where the user can type the query. The
user can also specify the maximum number of results that the system should retrieve
and/or a confidence threshold to filter out the results with a relevance probability
below that threshold.262 10 Large-scale Systems and Applications
Queries can be formed by individual words, or Boolean word combinations using
the logical AND, OR and NOT operators, which are represented, respectively, by the
characters “&&” (i.e. double ampersand), “||” (double vertical bar) and “-” (dash).
If the user types two consecutive words without any operators, the system interprets
it as a logical AND. Boolean expressions can be nested using parentheses (e.g.,
“(jail | | prison) && (clean | | easy)”). In addition, word sequence (or phrase) queries
are supported by our engine using the square brackets (e.g. [the white house]).
The web client offers different views, depending on the level of the hierarchical
index that the user is currently visiting (see the previous subsection). For a request
at the root of the hierarchy, all relevant collections are reported. Then one can look
into a specific collection and the set of relevant books of that collection will be
displayed. Next, the user can move to a specific book and the set of relevant pages
will be shown. Finally, the precise location of the relevant (pseudo-)words in a page
image are visualized.
The view at the root level is shown in Fig. 10.8. In an inner stage of the hierarchy,
the visual interface is as depicted in Fig. 10.9, which shows relevant pages for a
particular query within each chapter (box in the Bentham collection). At the leafs
of the hierarchy (pages), the interface is as seen in Fig. 10.10, where all the instances
of the relevant terms within the page are displayed. The color of each pseudo-word
bounding box indicates the confidence (RP) of the match (green: high, red: low).
Fig. 10.8: Web client graphical interface at root of the hierarchy, displaying the query:
“[ pain (and | | or) pleasure] && [pleasure (and | | or) pain ]”
which combines Boolean operators (∧ represented as && and ∨ as ||) and word-sequences
between square brackets [. . . ].10.2 Architecture Design 263
Fig. 10.9: Web client at box (book) level of the hierarchy, displaying the search results for:
“[ pain (and | | or) pleasure] && [pleasure (and | | or) pain ]”.
Fig. 10.10: Web client at page level of the hierarchy, displaying the search results for:
“[ pain (and | | or) pleasure] && [pleasure (and | | or) pain ]”.264 10 Large-scale Systems and Applications
10.3 Large-Scale Applications
The PrIx methods presented throughout this book were applied to actually process
several large or very large series of historical manuscripts so as to make them fully
textual-content searchable. The results are available through the Internet by means
of search and retrieval demonstration systems,3 implemented as described in the
previous sections of this chapter.
The preparatory training and evaluation experiments carried out before undertak￾ing the full production of the PrIxs of these manuscript collections are reported in
Sec. 6.9 of Chapter 6.
Details about these applications are reported in the coming subsections. They are
presented chronologically, according to the date they were produced.
10.3.1 Tresor des Chartes ( ´ Chancery)
PrIx processing of the large series of manuscripts known as the “Tresor des Chartes ´ ”,
was the main outcome of the HIMANIS project.4
The “Tresor des Chartes ´ ” collection is a challenging and particularly interesting
case study. This large and emblematic series is a key source of information to
understand the origin of centralized nation states in Europe, in the medieval age.
The preparatory experiments carried out before processing the full Chancery
collection are described in Sec. 6.9.2.
Medieval documents like Chancery, usually entail two important linguistic chal￾lenges. Namely, they are written in more than one language and they are heavily ab￾breviated (especially the text parts written in Latin). The latter problem is particularly
insidious because the (only) image transcripts generally available (for model training)
are modernized versions where all the abbreviations are completely expanded.
Nevertheless, thanks to the combination of powerful statistical models for hand￾written text (in particular CRNN) and the lexicon-free PrIx approaches presented in
this book (Sec. 5.6.1), the project was successful against these challenges, as shown
in the results reported in [3]. Lexicon-free was compared with lexicon-based PrIx,
as well as with deterministic indexing using 1-best HTR transcripts. Best results
where achieved for lexicon-free PrIx, while indexing HTR transcripts was clearly
inferior to both PrIx alternatives. The results achieved for abbreviations-only were
better than those of the full query set. This was explained because most abbreviated
words have, on average, more training instances than the average plain word [3].
The resulting PrIx contained about 366 million entries (spots). To produce this
index, about three million CLs were produced and later discarded. From the original
3 A list of URL links to these large-scale demonstrators, along with other for smaller but more
specific applications, can be consulted at www.prhlt.upv.es/htr/PrIx.
4 https://himanis.hypotheses.org/about,
https://eadh.org/projects/himanis,
https://www.heritageresearch-hub.eu/project/himanis10.3 Large-Scale Applications 265
366 million PrIx spots, those with RP not higher than 5 · 10−5 were pruned. This
way, the original PrIx density of 7.74 spots per estimated running word was reduced
to 6.05, without noticeable reduction in search accuracy (AP).
Final details of this demonstrator, including public access URLs, are reported
in Table 10.1. Fig. 10.11 shows a view of the Chancery demonstrator, with results
for the (multilingual) query: “[ feste Nostre Dame ] && [ tradicionem nemorum
predictorum ]” (“[Notre Dame party] && [the tradition of the foretold forests]”).
Table 10.1: Public URLs to access to the PrIx demonstrator for the manuscript series
“Tresor des Chartes ´ ” (Chancery), and corresponding statistics.
www.prhlt.upv.es/htr/chancery
http://himanis.huma-num.fr
Number of manuscripts 199
Number of page images 83 141
Number of spots 285 791 425
Average number of spots per page image 3 437
Estimated number of running words 47 256 202
Estimated lexicon size 1 462 688
Average estimated running words per page image 568
Density (spots per estimated running word) 6.05
Fig. 10.11: A view of the Chancery PrIx demonstrator showing a result for the bilingual
Boolean and word-sequence query: “[ feste Nostre Dame ] && [ tradicionem nemorum
predictorum ]”. Notice the spotted abbreviated forms of the Latin words “nemorum” and
“predictorum”, as well as the French word “Nostre”.266 10 Large-scale Systems and Applications
10.3.2 Teatro del Siglo de Oro (TSO)
The “Teatro del Siglo de Oro” (TSO) collection was indexed within the READ
project5, as one of its multiple large-scale demonstrators. The goal of the READ
project was to make archival material more accessible by developing and imple￾menting cutting-edge technology in HTR, KWS, Layout Analysis, and related fields.
TSO encompasses hundreds of comedy manuscripts, written (most of them in
verse) by many famous and anonymous Spanish writers (and copyists) between the
XVI and XVII centuries. This period corresponds to the Spanish Siglo de Oro6 of
Literature. The complete series of manuscripts of “Teatro del Siglo de Oro” held by
the Biblioteca Nacional de Espana˜ (BNE) encompasses hundreds of thousand page
images, although only 36 010 pages of 328 manuscripts have been processed so far.
The TSO manuscripts were written by hundreds of hands and they contain many
abbreviated words (e.g. most names of characters and locations in the scripts are
abbreviated). This makes the indexing process very challenging, in similar ways to
those faced in Chancery.
The preparatory empirical work carried out before processing the full TSO col￾lection is described in Sec. 6.9.3.
The public URL and statistics of the TSO PrIx demonstrator are shown in Ta￾ble 10.2. Fig. 10.12 shows a view of the TSO demonstrator, displaying results for a
Boolean proximity-AND query: “celestial &10& imperial”, aimed at finding rhymes
of the two words.
Table 10.2: Public URL of the “Teatro del Siglo de Oro” (TSO) PrIx demonstrator and
corresponding statistics.
www.prhlt.upv.es/htr/tso
Number of manuscripts 328
Number of pages 36 010
Number of spots 42 477 144
Average number of spots per page 1 180
Estimated number of running words 5 396 497
Estimated lexicon size 370 000
Average estimated running words per page 150
Density (spots per estimated running word) 7.87
5 https://cordis.europa.eu/project/id/674943, https://eadh.org/projects/read
6 Golden Age, in English.10.3 Large-Scale Applications 267
Fig. 10.12: A view of the TSO PrIx demonstrator showing a result for the Boolean
proximity-AND query “celestial &10& imperial”, aimed at finding rhymes of the two words.
10.3.3 Bentham Papers (Bentham)
Jeremy Bentham (1748–1832), was an influential British philosopher, jurist, and
social reformer. He left a huge amount of handwritten documents which are nowadays
considered of great interest by many scholars.7 The Bentham Papers series includes
several thousands of manuscripts written by Bentham, his correspondents and his
secretarial staff. The largest part of this series is held by the British University College
London (UCL) library, and a smaller part by the British Library. All in all the series
encompasses more than 90 000 page images, contained in 193 boxes.
A small portion of this series was first processed for KWS in the tranScriptorium
project.8 The PrIx of the whole series was finally produced, along with TSO, as
another of the READ large-scale demonstrators, commented in the previous section.
The resulting demonstrator is referred to as Bentham.
Several Bentham datasets have been proposed in the last decade. Most of them are
described in Appendix C.2. These datasets correspond to small, selected samples,
composed of page images of chronologically increasing complexity. Those most
popular in academic publications are also the easiest ones, initially proposed more
than 10 years ago. However, in order to train models adequate to produce PrIxs for
the full Bentham collection, we had to collect yet another dataset which included
representative samples of all the difficulties expected in the 89 911 page images to be
7 See the Bentham Project at https://www.ucl.ac.uk/bentham-project
8 See [8] and https://cordis.europa.eu/project/id/600707268 10 Large-scale Systems and Applications
processed. Thisincludes very difficult to read Bentham autographs, text written many
other hands, lack of contrast because fading ink, among many other. The resulting
dataset, referred to as “Large Bentham Dataset”, is described in Appendix C.2.4.
The preparatory empirical work prior to processing the full Bentham collection is
described in [11] and also in Sec. 6.9.4 of this book.
The public URL of the Bentham demonstrator and the corresponding PrIx statis￾tics are shown in Table 10.3. Some views of theBenthamdemonstrator, some of them
displaying spotting results were already shown in Figs. 10.8–10.10 (Sec. 10.2.3).
Table 10.3: Public URL of the Bentham demonstrator and corresponding PrIx statistics.
www.prhlt.upv.es/htr/bentham
Number of boxes 193
Number of pages 89 911
Number of spots 197 651 338
Average number of spots per page 2 198
Estimated number of running words 25 487 932
Estimated lexicon size 720 000
Average estimated running words per page 283
Density (spots per estimated running word) 7.75
10.3.4 Parcels from Indias and Cadiz Ar ´ chives (Carabela)
The Carabela collection consists of 125 000 images of manuscripts of interest
to underwater archaeology, compiled within the Carabela project [12].9 These
manuscripts were selected with the main purpose of stressing the PrIx technol￾ogy with the typical difficulties expected in large historical series of manuscripts.
Even if the selected collection is really large, it constitutes less than 1% of all the
relevant series held by the Spanish Archivo General de Indias (AGI) and Archivo
Histotico provincial de c ´ adiz ´ . See more details in Appendix C.9.
A basic objective of the Carabela project was to produce PrIxs for all the images
of the collection, in order to make it searchable by textual queries, as in previous
similar projects [2, 11].
Given our previous experience with other large manuscript series, this objective
was not expected to entail important research problems. However, the extreme com￾plexities of the Carabela text images made this objective daring by itself. Because
these complexities, described in some detail in Appendix C.9, Carabela was in fact
one of the most challenging sets of historical manuscripts we have ever considered.
Despite these issues, PrIxs were successfully produced for the full Carabela col￾lection and an effective search and retrieval system similar to the above described
demonstrators, was implemented.
9 www.prhlt.upv.es/htr/carabelaProject10.3 Large-Scale Applications 269
The preparatory experiments for this collection were described in [12] and are
summarized in Sec. 6.9.5. Even though the search performance achieved in these
experiments was not as good as in other difficult datasets, it can be considered
satisfactory given all the extreme complexities involved. Moreover, it is worth saying
that these empirical results might be somewhat pessimistic, in view of the very
satisfactory subjective assessment generally granted by real expert users when testing
the Carabela demonstrator in real information search and retrieval tasks.
The public URL and statistics of the Carabela PrIx demonstrator are shown
in Table 10.4. On the other hand, Fig. 10.13 shows results of the demonstrator for
the query: “( [ (tierra* | | isla*) Austral* ] | | [ Austral Incognita∼3 ] )”, aimed to
find information about the Spanish exploration of Austral lands in the 17th-18th
centuries.10
Table 10.4: Public URL of the Carabela PrIx demonstrator and corresponding statistics.
http://www.prhlt.upv.es/htr/carabela
Number of manuscripts 378
Number of images 125 311
Estimated Number of pages 133 189
Number of spots 485 693 217
Average number of spots per page 3 647
Estimated number of running words 25 583 700
Estimated lexicon size (Zipf thr. = 0.333) 745 000
Average estimated running words per page 192
Density (spots per estimated running word) 18.98
10 The English translation of the Spanish texts in Fig. 10.13 is: “[...] on October 19 of the said
last year, 1705. Because these islands are a small part of the said Austral Land and once they have
been conquered, it is not difficult to gain footing and penetrate little by little through the immense
mainland, which runs up to 34 degrees South, and it is even believed that it goes further ahead
towards the Antarctic pole” “[...] and is seen in the Map that was presented last year. Such
land is a part of the ”Austral Incognita” which was always the main attempt in the conquest and
population of these lands.”. These texts were written about 65 years before the claimed “discovery”
of Australia by Capt. Cook. in 1770. Probably the maps and other documents mentioned in these
manuscripts were seized by the British troops during their brief occupation of Manila 1762–1764
– see: https://en.wikipedia.org/wiki/British_occupation_of_Manila.270 10 Large-scale Systems and Applications
Fig. 10.13: Results for the query “([(tierra* | | isla*) Austral* ] | | [ Austral Incognita∼3])”
on Carabela. It makes use of wildcard and approximate spelling, Boolean OR and word
sequences, and was aimed at searching for information about the Spanish exploration of
Austral lands in the 17th-18th centuries.1010.3 Large-Scale Applications 271
10.3.5 Finnish Court Records (FCR)
The “Finnish Court Records” (FCR) encompass 785 manuscripts from the Renovated
District Court Records held by the Finnish Kansallisarkisto or National Archives of
Finland (NAF). Many of these manuscripts were scanned into double-page images,
amounting to 630 388 images of about one million pages in total. The manuscripts
date from the 18th century and consist of records of deeds, mortgages, traditional
life-annuity, among others. They were written by many hands mostly in Swedish.
In addition to the many hands and writing styles involved, one of the main
challenges of this collection stems from the heavy warping of many double-page
images. Another important challenge is, in point of fact, the very large proportion of
words that are hyphenated. Until recently, indexing and search for hyphenated words
was a pending problem which significantly hindered the search for important facts in
this (and other) collection(s). However, this problem is now adequately solved [13, 1],
as discussed in Sec. 8.4 of Chapter 8.
The work on FCR started during the last year of the READ project5
, but most
of the preparatory work and the PrIx processing proper was carried out within a
specific collaboration of NAF, READ-coop, and our PRHLT research center.11
In order to train models adequate to produce PrIxs for the full FCR collection, a
relatively large dataset had to be compiled, including representative samples of all
the difficulties expected in the huge amount of page images to be processed. The
resulting dataset is presented in Appendix C.10, and the preparatory empirical work
prior to processing the full FCR collection is described in Sec. 6.9.6 of this book.
After these experiments, additional images were transcribed by NAF and used to
train the final models. All in all, about one thousand transcribed page images were
used to train the final models used to produce the PrIxs for full collection of more
than one million handwritten pages.
The public URL and statistics of the FCR PrIx demonstrator are shown in Ta￾ble 10.5. On the other hand, Fig. 10.14 show results of the demonstrator for the
query: “(*ristina - Christina - Kristina - Ristina)”, aimed to find unusually spelled
forms of the very usual name Kristina or Christina (Ristina is another fairly usual
name). Among several spotted results, Cristina and Britachristina are shown in these
examples.
11 https://kansallisarkisto.fi/en,
https://readcoop.eu
http://www.prhlt.upv.es272 10 Large-scale Systems and Applications
Table 10.5: Public URL of the FCR PrIx demonstrator and corresponding statistics.
www.prhlt.upv.es/htr/fcr
Number of folders 68
Number of books 785
Number of images 634 667
Estimated Number of pages 1 027 464
Number of spots 2 853 730 871
Average number of spots per page 2 777
Estimated number of running words 239 123 000
Estimated lexicon size 1 967 000
Average estimated running words per page 233
Density (spots per estimated running word) 11.93
Fig. 10.14: Two views of the FCR PrIx demonstrator showing results for the query:
“(*ristina - Christina - Kristina - Ristina)”. It was aimed at finding unusual spellings of the
name Kristina, using wildcard spelling and the Boolean NOT operator (‘‘-’’). The two
occurrences shown are: “Cristina” and “Britachristina”.10.3 Large-Scale Applications 273
10.3.6 General Discussion on Large-Scale Applications
In the previous section five large-scale PrIx search and retrieval applications have
been described. They are chronologically presented in this chapter, but they have
different levels and types of complexity. In any case, the experience gained with the
first applications made it easier, or possible to deal with the others. It took about two
months to produce the PrIxs for the first system (Chancery), once the trained optical
and language models were satisfactory according to the preparatory empirical tests
(see Sec. 6.9). But production time was reduced to about a couple of weeks for TSO
and Bentham. Carabela took a bit more, because of the intricate complications
of this collection, and about one month was needed for FCR, because of its sheer
size. The required computing resources were significant, but not really prohibitive.
In all the cases, all the computation was carried out on a small computer cluster,
using only half a dozen CPUs and three modest GPUs (mainly for training in the
preparatory experiments).
Table 10.6 summarises the main features of these large-scale systems, and the
corresponding PrIx-estimated Zipf curves are shown in Fig. 10.15. All follow fairly
well the expected natural language shape, except FCR which deviated a bit probably
because of its rather formal language. As discussed in Sec. 9.3, the expected values
of the number of running words were computed as the area below each Zipf curve,
and lexicon sizes were estimated using the thresholds shown the figures.
Table 10.6: Summary of main features of the PrIx demonstrators. The number of running
words, the vocabulary size and the number of pages for Carabela and FCR are estimated
from the corresponding PrIxs.
Feature Chancery TSO Bentham Carabela FCR
Manuscripts 199 328 193 378 785
Pages (×103
) 83 36 90 133 1 027
Spots (×103
) 285 791 42 477 197 651 485 693 2 853 730
Running words (×103
) 47 256 5 396 25 488 25 584 239 123
Lexicon (×103
) 1 463 370 720 745 1 967
Density 6.1 7.9 7.8 19.0 11.9
At the time of writing, the five PrIx search and retrieval applications presented
in the previous subsections are fully operative and publicly accessible through the
Internet. These systems were initially intended only for demonstration purposes. But
soon after each of them was available, they started to receive numerous visits from
humanities scholars trying to actually find information about facts possibly described
in the corresponding manuscript series.
Longitudinal visit and usage statistics were reported in [9] for Chancery, in [11]
for Benthamand TSO and in [12] for Carabela. FCR, on the other hand, has mainly
been visited for testing and demonstration purposes. This is because, following
the collaboration agreement for the development of this application, READ-coop
implemented a specific search engine and user interface for the PrIxs produced by
PRHLT and this interface was installed at NAF (see footnote11). So real use in this
case was routed to the NAF web site from the very beginning.274 10 Large-scale Systems and Applications
<0.1
1
10
100
1K
10K
100K
1M
1 10 100 1K 10K 100K 1M 10M
frequency = 0.333
frequency = 0.5
Estimated frequency
Rank
CHANCERY
CARABELA
TSO
<0.1
1
10
100
1K
10K
100K
1M
1 10 100 1K 10K 100K 1M 10M
frequency = 0.5
Estimated frequency
Rank
FCR
BENTHAM
Fig. 10.15: PrIx-estimated Zipf curves of the complete collections Chancery, Carabela,
TSO, FCR and Bentham. Lexicon sizes were estimated as the rank of a word whose
estimated frequency is 0.5 (except for Carabela, where this threshold was set to 0.333).
Both Chancery and Bentham include the corresponding complete series of
manuscripts and, from the time they became available, both keep receiving regularly
many daily visits from scholars around the world.12
In contrast, as mentioned above, TSO is only a relatively small part of the large
Golden Age of Theater collection held by BNE. Therefore, it is only of limited
usefulness for humanity scholars working in this kind of documentary series and,
at the time being, it only gets casual visits from time to time. Nevertheless, TSO
became very popular soon after it was open to the public, when it was showcased13
in an exhibition about Lope de Vega and the Golden Age Theatre in the Digital Era,
which was organized by the Biblioteca Nacional de Espana˜ (BNE) in 2018–2019.14
Finally, while Carabela contains a large and complex set of manuscripts, this
is only a very tiny part of the huge series of the Archivo de Indias and related
archives and, therefore, it was never aimed at being a real search platform for
humanities scholars. However, it became a very media project15 when word spread
of the casual Carabela discovery of amazing information on the early Spanish
exploration of Austral lands, as mentioned in Sec. 10.3.4. Some time after, the
Spanish National Geographic Society (SGE) awarded a price16 to the Carabela
project for its contribution to help unveiling interesting unknown facts of History.
12 In the humanities literature, one can find papers, like [10, 5, 4, 6, 7], which warmly acknowledge
the important leap forward that our PrIx systems represent for historical research.
13 https://lopeyelteatro.bne.es/#section?id=4&point=2
https://lopeyelteatro.bne.es/video/INDEXACION%20MANUSCRITOS.m4v
14 https://lopeyelteatro.bne.es
www.bne.es/es/agenda-eventos-actividades/exposicion-lope-teatro-siglo-de-oro
15 www.prhlt.upv.es/htr/carabelaProject/news
www.fbbva.es/noticias/proyecto-carabela-una-herramienta-de-inteligencia-artificial-al-ser
vicio-de-la-investigacion-historica
www.imnovation-hub.com/digital-transformation/spaniards-discover-australia-ia-research/?_
adin=02021864894
16 https://sge.org/premios-sge/premio-iniciativa-empresa/premio-iniciativaempresa-sge-201
9-2020-proyecto-carabelaReferences 275
References
1. Andres, J., Toselli, A.H., Vidal, E.: Search for hyphenated words in probabilistic indices: a ´
machine learning approach. In: 2023 International Conference on Document Analysis and
Recognition (ICDAR), pp. 269–285 (2023)
2. Bluche, T., Hamel, S., Kermorvant, C., Puigcerver, J., Stutzmann, D., Toselli, A.H., Vidal,
E.: Preparatory KWS Experiments for Large-Scale Indexing of a Vast Medieval Manuscript
Collection in the HIMANIS Project. In: Proc. of 14th ICDAR (2017)
3. Bluche, T., Messina, R.: Gated Convolutional Recurrent Neural Networks for Multilingual
Handwriting Recognition. In: 14th IAPR International Conference on Document Analysis and
Recognition (ICDAR), vol. 01, pp. 646–651 (2017). DOI 10.1109/ICDAR.2017.111
4. Gelly-Perbellini, M.: A la recherche de la sorcellerie dans le tresor des chartes pp. 1–5 (2018). ´
URL https://himanis.hypotheses.org/470
5. Gelly-Perbellini, M.: Searching for witchcraft in the registers of the tresor ´ des chartes (french
royal archives) pp. 1–5 (2018). URL https://himanis.hypotheses.org/678
6. McDougall, S.: Pardoning infanticide in late medieval france. Law and History Review 39(2),
229–253 (2021)
7. McDougall, S.: Judging sexy women in late medieval france. postmedieval 13(1-2), 81–104
(2022)
8. Sanchez Peir ´ o, J.A., M¨uhlberger, G., Gatos, B., Schofield, P., Depuydt, K., Davis, R.M., ´
Vidal, E., De Does, J.: transcriptorium: a european project on handwritten text recognition pp.
227–2285 (2013)
9. Stutzmann, D.: HIMANIS users: an emerging community! pp. 1–2 (2018). URL https:
//himanis.hypotheses.org/305
10. Stutzmann, D., Moufflet, J.F., Hamel, S.: La researche en plein texte dans les sources
manuscrites medi ´ evales: enjeux et perspectives du projet HIMANIS pour l’ ´ edition ´
electronique. M ´ edi ´ evales pp. 67–96 (2017) ´
11. Toselli, A.H., Romero, V., Sanchez, J.A., Vidal, E.: Making two vast historical manuscript col- ´
lections searchable and extracting meaningful textual features through large-scale probabilistic
indexing. In: Int. Conf. on Document Anal. and Recogn. (ICDAR), pp. 108–113. IEEE (2019)
12. Vidal, E., Romero, V., Toselli, A.H., Sanchez, J.A., Bosch, V., Quir ´ os, L., Bened ´ ´ı, J.M.,
Prieto, J.R., Pastor, M., Casacuberta, F., et al.: The carabela project and manuscript collection:
large-scale probabilistic indexing and content-based classification. In: 2020 17th International
Conference on Frontiers in Handwriting Recognition (ICFHR), pp. 85–90. IEEE (2020)
13. Vidal, E., Toselli, A.H.: Probabilistic indexing and search for hyphenated words. In: Document
Analysis and Recognition–ICDAR 2021: 16th International Conference, Proceedings, Part II
16, pp. 426–442. Springer (2021)Chapter 11
Conclusion and Outlook
Abstract In this chapter the main contributions of this book are summarized. In
addition, for future works, we explain how the probabilistic indexing framework can
also be applied using the new, promising models for optical modeling in HTR which
are gaining increasing popularity. Finally, we also suggest how the ideas proposed
in this book could be applied to other domains, where images are not necessarily of
handwritten text.
11.1 Contribution Summary
The main contributions of this book are summarized below, organized into the
corresponding relevant topics.
Probabilistic Indexing Framework (PrIx)
After describing the many flavors of KWS (see Chapter 2), we have introduced a new
probabilistic formulation of KWS in Chapter 3. We presented KWS as an instance
of IR, where the relevant documents or pieces of documents (i.e. manuscript pages,
individual lines or image segments) have to be retrieved as a response to a user
query (represented by a string or maybe as an exemplar image). While the initial
focus was KWS, our formulation explicitly aims at indexing all the words that may
potentially appear in a text image collection, as opposed to spotting (“on-the-fly”)
just one or a few “keywords”. Therefore decided to refer to the proposed framework
as “Probabilistic Indexing” (PrIx).
The key idea was to define a binary random variable and the corresponding
“relevance probability” (RP), taking into account both the content of the document
and the query, as well as the stochastic nature of the content, given the images.
As complementary material, in Appendix A we prove mathematically that, under
the necessary assumptions (i.e. the correct probability distributions are used), our
framework is the best possible strategy (i.e. it is optimal) for many of the typically
used evaluation measures used across academia and industry, like the Average Pre￾cision, Normalized Discounted Cumulative Gain and Precision-at-푘 measures (both
Mean and Global).
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 277
A. H. Toselli et al., Probabilistic Indexing for Information Search and Retrieval 
in Large Collections of Handwritten Text Images, The Information Retrieval Series 49, 
https://doi.org/10.1007/978-3-031-55389-9_11 
 278 11 Conclusion and Outlook
In short, we have built a principled and robust probabilistic framework, that allows
very effective and efficient searching for words in text images, and have shown its
validity both theoretically and in practice.
Probabilistic Models of Handwritten Text
Because our framework works on top of probability distributions, we need good
statistical models to approximate these distributions; and which can be automatically
estimated from training data, (since the real distributions are never know in practice).
In Chapter 4 we have reviewed some of the traditional models used to represent
the content (transcript) of handwritten text documents, and we have proposed a new
neural network-based architecture that only uses convolutional and one-dimensional
recurrent layers. This architecture achieves a similar or better accuracy then previous
state-of-the-art models in a fraction of time (6 to 9 times faster).
Most importantly, we have explained how these models (and hopefully others
yet to come) can provide not only a “best” transcript of a text image, but a full
probability distribution over all possible transcripts (this can be understood as a
“stochastic transcription”). Moreover, we have explained how this distribution can
be represented using Weighted Finite State Transducers (WFST). This unifies the
representation and allows to use the algorithms we propose with arbitrary types of
probabilistic models, provided the corresponding distributions can be represented
by means of WFSTs.
Indexing Algorithms based on PrIx
Using Weighted Finite State Transducers to represent both the queries and the prob￾abilistic content of the document, in Chapter 5 we have developed several algorithms
that allow us to build PrIxs, based on the proposed framework.
These indexes, similar to the ones used by traditional search engines on plaintext
data (like Web pages, or PDF documents) enable us to scale our framework to very
large collections of documents, with a search time which is virtually constant with
respect to the index size (the number of terms and documents indexed).
Probabilistic Interpretation of KWS Methods
In Chapter 7, we have presented a probabilistic interpretation of some of the most
popular methods used in the KWS literature.
This interpretation enabled us to understand the assumptions and limitations of
these approaches, and we proposed different solutions to fix these limitations and to
thereby improve such methods. For instance, we studied (and improved) the tradi￾tional HMM-Filler, the BLSTM-CTC approach, a distance-based approach typically
used for segmentation-based QbE KWS, and the popular PHOCNet method. It has
been shown that, in several cases, the state-of-the-art results using such traditional
KWS methods can be improved if they are adjusted according to the ideas underlying
our PrIx framework.11.2 Future Work 279
Beyond Traditional and Academic KWS
Finally, we have used our probabilistic framework and algorithms to tackle informa￾tion search and extraction problems which are seldom considered in the academic
literature (of traditional KWS), but are of vital importance for real usage scenarios.
For example, in Chapters 8 and 9 we have presented how to tackle, in a principled
way, PrIx with Boolean and phrase queries, using the framework. Also, we have
suggested how to use the probabilistic formulation in other applications such as
Content-based IR.
Moreover, in Chapter 10 we have shown that our solutions can be applied, in an
effective and efficient manner, to large collections of documents, which are rarely
targeted in academic domains.
11.2 Future Work
Finally, we identify future lines of research and development that we have already
considered, or we think may be important.
Stochastic Definitions of Relevance
Throughout this book, we have assumed that the notion of relevance is of determin￾istic nature, given the content of the document and the query, and only depends on
these variables. Under the IR perspective, the challenging part was not in the notion
of relevance, but in the uncertainty regarding the textual content of the documents.
This is quite the opposite to the applications that the IR community typically
tackles, where the textual content of the documents is perfectly known (e.g. the text
of a Web page or a PDF document is given), but the concept of relevance has an
stochastic nature (e.g. we would not benefit from a search engine that would simply
retrieve all web pages that contain the words given in the query).
In order to improve the usefulness of our solutions, especially when dealing with
large collections of documents and queries regarding high-frequency words, we need
to investigate how to extend our models and algorithms to consider this new source
of uncertainty, in a principled, robust, and efficient manner.
Better Statistical Models and Training
Every improvement in the statistical models of the text content of the images will
have an impact on the final PrIx performance. Our preferred architecture, based on
Convolutional and Recurrent Neural Networks has done an excellent job to model
the text on text linesin several languages. However, we need to continue exploring for
better architectures, or even radically different models. Especially if we aim to tackle
more challenging documents with difficult layout analysis and line segmentation.
For instance, these later steps, which are currently performed as a preliminary step,
can also be modeled as stochastic processes, from which probabilistic models can
be constructed and integrated with our current solutions.280 11 Conclusion and Outlook
Actually, in this direction goes the last so-called End-to-End HTR systems, gener￾ally based on Transformer models with their attention mechanism [4]. Among these
systems stand out Document Attention Network (DAN) [1] and Document Under￾standing Transformer (DONUT) [2], which work at the page level by detecting and
transcribing text blocks with a simple layout. Just as CRNN, DAN and DONUT are
sequence-to-sequence models, where the uncertainty of the output transcripts can
by given by their produced token-level posteriorgrams, similar to the character-level
posteriorgrams outputted by CRNNs (see Sec. 4.4.3.3). Therefore, departing from
such token-level posteriorgrams, corresponding token lattices could be generated,
thereby allowing the algorithms of Chapter 5 to produce PrIxs for regions that go
beyond the realm of a text-line image.
Last but not least, the training data efficiency of our models could be further
improved. Although the human effort needed to produce labeled training data is
definitely worth it when indexing large collections of documents, new ways of
training the statistical models need to be explored, perhaps even using unlabeled
data. This would allow the use of PrIx by low-budgeted institutions, or in smaller
text image collections.
Probabilistic Framework Applied to Other Domains
Finally, we aim to explore other domains in which our framework can be applied
with minor modifications. For instance, one of this applications is Spoken Content
Retrieval. Wile this is considered nowadays to be a mature application field, we
believe the methods proposed in this book may lead to improved search and retrieval
performance.
But many other domains are worth exploring, like the retrieval of music scores,
music contents from raw audio files, cross-lingual IR, multi-modal IR, etc.
In addition, we also aim to use our probabilistic PrIx approach in order to aid
the process of transcribing (parts of) documents, in an iterative way. Once a page
or section of some manuscript which contains the information we are interested in
has been located by means of PrIx-based search, the relevant page or section can
be transcribed. This transcription process could be assisted by CATTI [3], but this
would require having available the (often large) word or character lattices originally
used to obtain the PrIx spots. So a more memory-efficient approach would be to
directly relay on the alternative word hypothesis contained in the indexed PrIx spots
to provide alternative transcripts in response to user feedback.
PrIx-mediated interactive transcription can also be used for cost-effective pro￾duction of training transcripts. To this end a small portion of documents can be
manually transcribed; then, the rest of text images can be probabilistically indexed
and the most likely word instances be presented to the user for verification in a single
step. Finally, PrIx models can be re-trained using the additional data, and the process
is iterated until the PrIx performance is considered satisfactory.References 281
References
1. Coquenet, D., Chatelain, C., Paquet, T.: DAN: a segmentation-free document attention network
for handwritten document recognition. IEEE Transactions on Pattern Analysis and Machine
Intelligence (2023)
2. Kim, G., Hong, T., Yim, M., Nam, J., Park, J., Yim, J., Hwang, W., Yun, S., Han, D., Park, S.:
Ocr-free document understanding transformer (2022)
3. Romero, V., Toselli, A.H., Vidal, E.: Multimodal Interactive Handwritten Text Transcription.
Perception and Artif. Intell. (MPAI). World Scientific, (2012)
4. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polo￾sukhin, I.: Attention is all you need. CoRR abs/1706.03762 (2017)Appendix A
The Probability Ranking Principle
Abstract Information Retrieval (IR) is thoroughly examined under the Statistical
Decision Theory to prove that the PrIx framework proposed and developed in this
book is optimal for a broad set of criteria typically adopted in KWS and many
other IR applications. The appendix presents developments which were originally
published in Joan Puigcerver’s PhD thesis [2] and it follows the so-called Probability
Ranking Principle, originally formulated by William S. Cooper [3]. The appendix
is mostly self-contained, and, in some cases, it may not follow the general style and
notation rules adopted in the main body of the book.
A.1 Ranking Multiple Relevant Images
Throughout this book different relevance probabilities are introduced and used to
decide whether or not a given image region (or a position within an image region),
is relevant for a given query. As discussed in Chapter 1, Sec. 1.4, these probabilities
allow a PrIx system to make optimal decisions under some binary classification loss.
In reality, the actual loss function depends on the user querying the system.
Thus, a given system should be able to perform well under multiple of these losses.
Thankfully, according to Eq. (1.3), a binary classification loss can be expressed
simply as a threshold value, that the user would set according to her preferences.
This means that evaluating the performance of a given PrIx system under multiple
binary classification losses, is equivalent to evaluating the quality of the ranking that
such system produces for a given list of documents and queries. In fact, this is how
users actually interact with traditional IR systems, such as web search engines.
Thus, we need to decide what is the optimal strategy to rank the set of documents
for a given query (or set of queries), so that some criterion is optimized (maximized,
or minimized).
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 283
A. H. Toselli et al., Probabilistic Indexing for Information Search and Retrieval 
in Large Collections of Handwritten Text Images, The Information Retrieval Series 49, 
https://doi.org/10.1007/978-3-031-55389-9 284 A The Probability Ranking Principle
A very well known strategy in the field of IR to tackle this problem is the so-called
Probability Ranking Principle (PRP), originally formulated by William S. Cooper,
and published in [3]:
If a reference retrieval system’s response to each request is a ranking of the documents in
the collection in order of decreasing probability of relevance to the user who submitted
the request, where the probabilities are estimated as accurately as possible on the basis
of whatever data have been made available to the system for this purpose, the overall
effectiveness of the system to its user will be the best that is obtainable on the basis of those
data.
In the following section, we shall see that this principle is actually optimal for a
wide range of quality measures as long as the following assumptions are met:
1. The relevance of a given object and query is independent of other objects and
other queries. That is, the joint distribution of conditional relevance probabilities
factorizes in:
푃(푅1, . . . , 푅푁 | 푋1, . . . , 푋푁 , 푉1, . . . , 푉푁 ) =
Ö
푁
푖=1
푃(푅푖
| 푋푖
, 푉푖)
2. The true relevance probability is known for all objects and queries in the
evaluation set. That is, given an evaluation set {(푟푖
, 푥푖
, 푣푖) : 1 ≤ 푖 ≤ 푁}, then:
푟푖
| 푥푖
, 풗푖 ∼ 푃(푅푖
| 푋 = 푥푖
, 푉 = 풗푖)
Notice that position-dependent PrIx and other IR instances, where the relevance
is conditioned on more variables, are also represented by the previous assumptions,
if one considers the random variable 푉 to be defined over the Cartesian product of
all the underlying variables.
Luckily, the first assumption holds in virtually all the existing KWS benchmarks.
Yet, it may be false in other IR applications. The second assumption, however, is
never correct, since the true distributions are unknown, and have to be estimated
from data.
In this appendix we will use the term “retrieved objects” to emphasize that our
results apply to any IR system where the aforementioned assumptions hold.
A.2 Evaluation Measures and Optimality
When stating that a certain strategy (i.e. decision rule) is optimal, one needs to state
with respect to which criterion that optimality holds. And, of course, then one has
to prove it. However, all papers that we could find that refer to the “optimality” of
the PRP cite the original publication [3]. Yet, this work only proves that the PRP
is optimal under the binary classification scenario (0–1 loss) presented in Eq. (1.4),
and for two of the most basic retrieval quality measures: the Precision-at-푘 and
Recall-at-푘.A.2 Evaluation Measures and Optimality 285
In this section, several popular measures to evaluate the quality of a ranking system
are introduced, and we prove that the PRP is the optimal strategy with respect to all
of them. More specifically, we present alternative proofs regarding the Precision-at-푘
and Recall-at-푘, and, more importantly, we present proofs of the optimality with the
very popular Average Precision (AP) and Discounted Cumulative Gain (DCG, for
both unnormalized and normalized versions).
In all cases, the proofs show that the PRP maximizes the expected value of the
corresponding measure. Following the principles of Decision Theory, we need to
optimize this value since the true relevance of each retrieved object for the respective
query is unknown.
Given an ordered list of 푁 elements, we will denote the relevance of the element
at position 푖 with the random variable 푅푖
. Thus, the sequence of random variables
for the 푁 elements is 푅1, . . . , 푅푁 .
Note that each element refers to some retrieved object for some particular query.
Thus, in the context of PrIx, the relevance probability of the 푖-th element in the list
would be conditioned on the corresponding image, keyword (and position within the
image, in position-dependent scenarios). Despite that, we will drop the conditioned
variables from the notation, for a clearer presentation.
The procedure to complete all proofs is almost identical: first, we will derive an
expression for the expected value of the corresponding quality measure, and then
we will show that such expected value can increase as long as the ranked list is not
ordered in decreasing order of the relevance probability.
A.2.1 Precision-at-풌
Definition A.1 Precision-at-푘 of a ranked list, sometimes abbreviated as P@푘, mea￾sures the fraction of the 푘-top retrieved elements that are actually relevant.
휋푘 (푅1, . . . , 푅푁 )
def
=
1
푘
Õ
푘
푖=1
푅푖 (A.1)
One can easily compute the expected value of the P@푘, taking into account
the independence assumption on the relevance probabilities, i.e. 푃(푅1, . . . , 푅푁 ) = Î
푖 푃(푅푖):
E [휋푘 (푅1, . . . , 푅푁 )]
★
=
1
푘
Õ
푘
푖=1
E [푅푖] =
1
푘
Õ
푘
푖=1
푃(푅푖 = 1) (A.2)
Theorem A.1 A sequence of elements ordered by decreasing relevance probability
(i.e. 푃(푅푖 = 1) ≥ 푃(푅푗 = 1), 1 ≤ 푖 < 푗 ≤ 푁) maximizes the expected value of the
Precision-at-푘, for any cut-off 1 ≤ 푘 ≤ 푁.286 A The Probability Ranking Principle
Proof. Notice that for any given 푘, any set of 푘 results with the largest sum of 푃(푅푖 =
1) maximizes Eq. (A.2). Thus, by sorting the 푁 results according to its decreasing
relevance probability, the 푘-top results maximize Eq. (A.2) for any possible value of
푘. ⊓⊔
A.2.2 Recall-at-풌
Definition A.2 Recall-at-푘 of a ranked list, sometimes abbreviated as R@푘, mea￾sures the fraction of all relevant elements that are found in the 푘-top of the retrieved
list.
휌푘 (푅1, . . . , 푅푁 )
def
=
(
0 푇 (푅1, . . . , 푅푁 ) = 0
1
푇 (푅1,...,푅푁 )
Í푘
푖=1 푅푖 otherwise
(A.3)
where 푇 (푅1, . . . , 푅푁 ) is the total number of relevant elements in the sequence
푅1, . . . , 푅푁 .
푇 (푅1, . . . , 푅푁 )
def
=
Õ
푁
푖=1
푅푖 (A.4)
In the previous definition, we chose to express 푇 as a function of 푅1, . . . , 푅푁 to
highlight the fact that 푇 is also a random variable whose value depends on those.
For the sake of clarity, we will just write 푇 to refer to 푇 (푅1, . . . , 푅푁 ). and 푇푘 to refer
to the number of relevant elements among the top-푘 of the retrieved list. Following
this notation, an alternative definition of Recall-at-푘 and Precision-at-푘 would be:
휌푘
def
=
(
0 푇 = 0
푇푘
푇
otherwise
; 휋푘
def
=
푇푘
푘
; 푇푘
def
=
Õ
푘
푖=1
푅푖 (A.5)
The calculation of the expected value of the Recall-at-푘 is slightly more tedious
than for the Precision-at-푘. The reason is that the denominator does not factorize
with respect to the expectation, since it is not a constant in this case. However, we
can use the law of total expectation to marginalize the expected value into a sum of
conditional expectations.
E [휌푘 (푅1, . . . , 푅푁 )] =
Õ
푁
푡=0
푃(푇 = 푡)E [휌푘 (푅1, . . . , 푅푁 ) | 푇 = 푡] (A.6)
Now, the value of 푇 is constant within each conditional expectation. Also, by
definition, when 푇 = 0, the Recall-at-푘 is always 0 (hence, it does not depend on
the ranking). Finally, using the linearity of the conditional expectation, the previous
equality can be rewritten as:A.2 Evaluation Measures and Optimality 287
E [휌푘 (푅1, . . . , 푅푁 )] =
Õ
푁
푡=1
푃(푇 = 푡)
푡
E
"Õ
푘
푖=1
푅푖
| 푇 = 푡
#
=
Õ
푁
푡=1
푃(푇 = 푡)
푡
Õ
푘
푖=1
E[푅푖
| 푇 = 푡] =
Õ
푁
푡=1
푃(푇 = 푡)
푡
푇푘 (푅1, . . . , 푅푁 | 푇 = 푡) (A.7)
With 푇푘 (푅1, . . . , 푅푁 | 푇 = 푡) defined as the expected number of relevant elements
within the top-푘 positions when the total number of relevant events is 푡.
푇푘 (푅1, . . . , 푅푁 | 푇 = 푡)
def
=
Õ
푘
푖=1
E[푅푖
| 푇 = 푡] (A.8)
When the sequence of relevant events can be inferred from the context, we will
simply write it as 푇푘 (푡). It is important to emphasize that this quantity is different
from 푇푘, which is not an expected value, but a random variable.
Lemma A.1 Given a ranked list with relevance variables 푅1, . . . , 푅푁 , and a cut-off
position 푘, if we swap any pair of elements at positions 푙 and 푚, for any value of
1 ≤ 푙 ≤ 푘 and 푘 < 푚 ≤ 푁, the difference in 푇푘 (푡) is equal to:
푃(푅푙 = 1) − 푃(푅푚 = 1)
For any value of 1 ≤ 푘 ≤ 푁 and 1 ≤ 푡 ≤ 푁.
Proof. Let’s suppose that the relevance variables of the original and the swapped se￾quences are, respectively, 푅1, . . . , 푅푁 and 푅
′
1
, . . . , 푅′
푁
, such that 푅푙 = 푅
′
푚, 푅푚 = 푅
′
푙
and 푅푖 = 푅
′
푖
, ∀푖 ∉ {푙, 푚}. We will use 푇푘 and 푇푘
′
to denote the expected value of
푇푘 (푡) and 푇
′
푘
(푡), respectively.
Thus, for the first sequence we have:
푇푘 = 푇푘 (푅1, . . . , 푅푁 | 푇 = 푡) =
Õ
푘
푖=1
E[푅푖
| 푇 = 푡] =
Õ
푙−1
푖=1
E[푅푖
| 푇 = 푡] + E[푅푙
| 푇 = 푡] + Õ
푘
푖=푙+1
E[푅푖
| 푇 = 푡] (A.9)
Taking into account that the two sequences only differ at positions 푙 and 푚, and
that 푇 = 푇
′
(i.e. the total number of relevant elements is identical), we have the
following expression for the second sequence:288 A The Probability Ranking Principle
푇푘
′
= 푇푘 (푅
′
1
, . . . , 푅′
푁
| 푇
′ = 푡) =
Õ
푙−1
푖=1
E[푅
′
푖
| 푇
′ = 푡] + E[푅
′
푙
| 푇
′ = 푡] + Õ
푘
푖=푙+1
E[푅
′
푖
| 푇
′ = 푡] =
Õ
푙−1
푖=1
E[푅푖
| 푇 = 푡] + E[푅푚 | 푇 = 푡] + Õ
푘
푖=푙+1
E[푅푖
| 푇 = 푡] (A.10)
Note that the expressions for 푇푘 and 푇푘
′
are almost identical, except for the 푙-th
position. The difference of the two is:
푇푘 − 푇푘
′
=
E[푅푙
| 푇 = 푡] − E[푅푚 | 푇 = 푡] =
푃(푅푙 = 1)E[1 | 푇 = 푡, 푅푙 = 1] − 푃(푅푚 = 1)E[1 | 푇 = 푡, 푅푚 = 1]
★
= (A.11)
푃(푅푙 = 1) − 푃(푅푚 = 1) (A.12)
The equality in Eq. (A.11) holds under the assumption that the relevance proba￾bility of an object and query is independent of the others. ⊓⊔
Theorem A.2 A sequence of elements ordered by decreasing relevance probability
(i.e. 푃(푅푖 = 1) ≥ 푃(푅푗 = 1), 1 ≤ 푖 < 푗 ≤ 푁) maximizes the expected value of the
Recall-at-푘, for any cut-off 1 ≤ 푘 ≤ 푁.
Proof. Let’s suppose that the relevance variables of the original sequence are
푅1, . . . , 푅푁 , and those of the modified sequence are 푅
′
1
, . . . , 푅′
푁
, such that 푅푙 = 푅
′
푚,
푅푚 = 푅
′
푙
and 푅푖 = 푅
′
푖
, ∀푖 ∉ {푙, 푚}.
Following Eq. (A.7), the expected values of the R@푘, for both sequences, are:
휌푘 =
Õ
푁
푡=1
푃(푇 = 푡)
푡
푇푘 (푅1, . . . , 푅푁 | 푇 = 푡) (A.13)
휌푘
′ =
Õ
푁
푡=1
푃(푇
′ = 푡)
푡
푇푘 (푅
′
1
, . . . , 푅′
푁
| 푇
′ = 푡) (A.14)
Taking into account that 푇 = 푇
′
and Eq. (A.1), the difference between the two is
equal to:
휌푘 − 휌푘
′ =
Õ
푁
푡=1
푃(푇 = 푡)
푡
(푇푘 (푡) − 푇
′
푘
(푡)) =
(푃(푅푙 = 1) − 푃(푅푚 = 1))Õ
푁
푡=1
푃(푇 = 푡)
푡
(A.15)A.2 Evaluation Measures and Optimality 289
Observe that the sum Í푁
푡=1
푃(푇=푡)
푡
is equal to zero if, and only if, 푃(푇 = 0) = 1
(i.e. it is impossible that any retrieved element is relevant). In such case, 푃(푅푙 =
1) = 푃(푅푚 = 1) = 0. In any other case, the previous sum is strictly positive and it
does not affect the sign of the difference in the expected value of the R@푘. Thus:
휌푘 − 휌푘



> 0 푃(푅푙 = 1) > 푃(푅푚 = 1)
= 0 푃(푅푙 = 1) = 푃(푅푚 = 1)
< 0 푃(푅푙 = 1) < 푃(푅푚 = 1)
(A.16)
Eq. (A.16) implies that, for any sequence of results, if we swap an arbitrary
element among the top-푘, with any other element not in the top-푘, we will improve
the expected value of the R@푘 if, and only if, the relevance probability of first one
is strictly lower than that of the latter.
Thus, if we sort the sequence of 푁 results by their decreasing relevance probability,
we guarantee that the expected value of R@푘 is the maximum for any 1 ≤ 푘 ≤ 푁. ⊓⊔
A.2.3 Average Precision (AP)
In general, the Average Precision (AP) is defined as the area under the Recall–
Precision curve (R–P).
AP def
=
∫ 1
0
휋(휌)푑휌 (A.17)
However, this definition cannot be applied in practice since the precision is not
a function of the recall, and, most importantly, neither the recall nor precision are
continuous. In practice, researchers use a different definition of the AP.
Definition A.3 The AP of a ranked list of elements with corresponding relevance
variables 푅1, . . . , 푅푁 is defined as:
AP(푅1, . . . , 푅푁 )
def
=
Õ
푁
푖=1
휋푖Δ휌푖 =
(
0 푇 = 0
1
푇
Í푁
푖=1
푅푖
푖
Í푖
푗=1 푅푗 otherwise
(A.18)
Here, Δ휌푖 = 휌푖 − 휌푖−1 (see Eq. (A.5)). The special case of 푇 = 0 is due to the fact
that 1
푇
is not defined.
Next, we calculate an expression for the expected value of the AP. Because 푇
depends on the relevance variables, we need to use the conditional expectation in
order to extract it from the expectation operator, as we did in Eq. (A.2.2).290 A The Probability Ranking Principle
E [AP(푅1, . . . , 푅푁 )] =
Õ
푁
푡=0
푃(푇 = 푡)E [AP(푅1, . . . , 푅푁 ) | 푇 = 푡] =
Õ
푁
푡=1
푃(푇 = 푡)
푡
E






Õ
푁
푖=1
푅푖
푖
Õ
푖
푗=1
푅푗
| 푇 = 푡






= (A.19)
Õ
푁
푡=1
푃(푇 = 푡)
푡
Õ
푁
푖=1
E[푅푖푇푖
| 푇 = 푡]
푖
= (A.20)
Õ
푁
푡=1
푃(푇 = 푡)
푡
푆(푅1, . . . , 푅푁 | 푇 = 푡) (A.21)
with
푆(푅1, . . . , 푅푁 | 푇 = 푡)
def
=
Õ
푁
푖=1
E[푅푖푇푖
| 푇 = 푡]
푖
(A.22)
The equality of (A.19) and (A.20) is due to the definition of 푇푖 (see Eq. (A.5)) and the
linearity of the expectation. For the sake of clarity, when the set of relevant variables
푅1, . . . , 푅푁 can be inferred from the context, we will use 푆(푡) as an alternative to
푆(푅1, . . . , 푅푁 | 푇 = 푡).
In addition, we can express 푆(푡) with respect to an auxiliary value 푆✘푘,푘✘+1 (푡),
which is equal to the former sum excluding the indexes at positions 푘 and 푘 + 1, by
simply using basic properties of the addition operation:
푆(푡) = 푆(푅1, . . . , 푅푁 | 푇 = 푡) =
Õ
푘−1
푖=1
E[푅푖푇푖
| 푇 = 푡]
푖
+
Õ
푁
푖=푘+2
E[푅푖푇푖
| 푇 = 푡]
푖
+
E[푅푘푇푘 | 푇 = 푡]
푘
+
E[푅푘+1푇푘+1 | 푇 = 푡]
푘 + 1
=
푆✘푘,푘✘+1 (푡) + E[푅푘푇푘 | 푇 = 푡]
푘
+
E[푅푘+1푇푘+1 | 푇 = 푡]
푘 + 1
(A.23)
Definition A.4 The random variable denoting the total number of relevant elements,
excluding the elements at positions 푖 and 푗, is represented by 푇✚푖, 푗 . It’s value is given
by the expression:
푇✚푖, 푗
def
=
Õ
1≤푘≤푁:
푘∉{푖, 푗 }
푅푘 = 푇 − 푅푖 − 푅푗
For any pair 1 ≤ 푖, 푗 ≤ 푁 such that 푖 ≠ 푗.
Lemma A.2 Given a ranked list with relevance variables 푅1, . . . , 푅푁 , if we swap
any pair of elements at positions 푘 and 푘 + 1, for any value of 1 ≤ 푘 < 푁, the
difference in the values of the sum 푆(푡) is equal to:A.2 Evaluation Measures and Optimality 291
푃(푅푘 = 1) − 푃(푅푘+1 = 1)
푘 (푘 + 1)
E[푇푘−1 + 1 | 푇✘푘,푘✘+1 = 푡 − 1]
For any value of 1 ≤ 푡 ≤ 푁.
Proof. Let’s consider two sequences of relevance variables 푅1, . . . , 푅푁 and
푅
′
1
, . . . , 푅′
푁
, such that 푅푘 = 푅
′
푘+1
, 푅푘+1 = 푅
′
푘
, and 푅푖 = 푅
′
푖
, ∀푖 ∉ {푘, 푘 + 1} (that is,
the two sequences are equal except that elements at positions 푘 and 푘 + 1 have been
swapped). We will use 푆 and 푆
′
to denote the value of the sum 푆(푡) for the first and
the second lists, respectively.
Thus, for the first sequence of relevant variables we have:
푆 = 푆(푅1, . . . , 푅푁 | 푇 = 푡) =
푆✘푘,푘✘+1 (푡) + E[푅푘푇푘 | 푇 = 푡]
푘
+
E[푅푘+1푇푘+1 | 푇 = 푡]
푘 + 1
=
푆✘푘,푘✘+1 (푡) + E[푅푘 (푇푘−1 + 푅푘) | 푇 = 푡]
푘
+
E[푅푘+1푇푘+1 | 푇 = 푡]
푘 + 1
(A.24)
And for the second sequence, we have:
푆
′ = 푆(푅
′
1
, . . . , 푅′
푁
| 푇
′ = 푡) =
푆
′✘푘,푘✘+1
(푡) +
E[푅
′
푘
(푇
′
푘−1
+ 푅
′
푘
) | 푇
′ = 푡]
푘
+
E[푅
′
푘+1
푇푘+1 | 푇
′ = 푡]
푘 + 1
=
푆✘푘,푘✘+1 (푡) + E[푅푘+1 (푇푘−1 + 푅푘+1) | 푇 = 푡]
푘
+
E[푅푘푇푘+1 | 푇 = 푡]
푘 + 1
(A.25)
The previous equations use the fact that 푆✘푘,푘✘+1 (푡) = 푆
′✘푘,푘✘+1
(푡) and 푇푘+1 = 푇
′
푘+1
,
since these values do not change when we swap the elements at positions 푘 and 푘 + 1
(review Eq, (A.5) and Eq. (A.23)).
Then, the difference 푆 − 푆
′
is equal to:
푆 − 푆
′ =
E[푅푘 (푇푘−1 + 푅푘) | 푇 = 푡]
푘
+
E[푅푘+1푇푘+1 | 푇 = 푡]
푘 + 1
−
E[푅푘+1 (푇푘−1 + 푅푘+1) | 푇 = 푡]
푘
+
E[푅푘푇푘+1 | 푇 = 푡]
푘 + 1
=
E[푅푘 (푇푘−1 + 푅푘) − 푅푘+1 (푇푘−1 + 푅푘+1) | 푇 = 푡]
푘
+
E[푅푘+1푇푘+1 − 푅푘푇푘+1 | 푇 = 푡]
푘 + 1
=
E[(푅푘 − 푅푘+1)푇푘−1 + 푅푘푅푘 − 푅푘+1푅푘+1 | 푇 = 푡]
푘
+ (A.26)
E[(푅푘+1 − 푅푘)푇푘+1 | 푇 = 푡]
푘 + 1
(A.27)292 A The Probability Ranking Principle
Now, let’s focus on the value of each expectation in the previous equation. First,
notice that if 푅푘 = 푅푘+1 the value of all expectations is canceled, and thus the
difference 푆 − 푆
′
is equal to zero. Using the fact that the relevance of an element
is independent of the others, Eq. (A.4) and basic properties of probabilities, the
expectation in Eq. (A.26) is equal to:
E[(푅푘 − 푅푘+1)푇푘−1 + 푅푘푅푘 − 푅푘+1푅푘+1 | 푇 = 푡] =
푃(푅푘 = 0)푃(푅푘+1 = 1)E[−푇푘−1 − 1 | 푇 = 푡, 푅푘 = 0, 푅푘+1 = 1] +
푃(푅푘 = 1)푃(푅푘+1 = 0)E[푇푘−1 + 1 | 푇 = 푡, 푅푘 = 1, 푅푘+1 = 0] =
−푃(푅푘 = 0)푃(푅푘+1 = 1)E[푇푘−1 + 1 | 푇 = 푡, 푅푘 = 0, 푅푘+1 = 1] +
푃(푅푘 = 1)푃(푅푘+1 = 0)E[푇푘−1 + 1 | 푇 = 푡, 푅푘 = 1, 푅푘+1 = 0] =
−푃(푅푘 = 0)푃(푅푘+1 = 1)E[푇푘−1 + 1 | 푇✘푘,푘✘+1 = 푡 − 1] +
푃(푅푘 = 1)푃(푅푘+1 = 0)E[푇푘−1 + 1 | 푇✘푘,푘✘+1 = 푡 − 1] =
(푃(푅푘 = 1) − 푃(푅푘+1 = 1))E[푇푘−1 + 1 | 푇✘푘,푘✘+1 = 푡 − 1] (A.28)
The last step is due to the fact that, for binary random variables:
푃(퐴 = 1)푃(퐵 = 0) − 푃(퐴 = 0)푃(퐵 = 1) = 푃(퐴 = 1) − 푃(퐵 = 1)
which can be easily derived given that 푃(퐴 = 0) = 1 − 푃(퐴 = 1), and respectively
for the random variable 퐵.
Similarly, for the second expectation we obtain:
E[(푅푘+1 − 푅푘)푇푘+1 | 푇 = 푡] =
푃(푅푘 = 0)푃(푅푘+1 = 1)E[푇푘−1 + 1 | 푇 = 푡, 푅푘 = 0, 푅푘+1 = 1] +
푃(푅푘 = 1)푃(푅푘+1 = 0)E[−푇푘−1 − 1 | 푇 = 푡, 푅푘 = 1, 푅푘+1 = 1] =
(푃(푅푘+1 = 1) − 푃(푅푘 = 1))E[푇푘−1 + 1 | 푇✘푘,푘✘+1 = 푡 − 1] (A.29)
Note that the value of E[(푅푘+1 − 푅푘)푇푘+1 | 푇 = 푡] is the same as that of E[(푅푘 −
푅푘+1)푇푘−1 + 푅푘푅푘 − 푅푘+1푅푘+1 | 푇 = 푡] with the opposite sign.
Finally, replacing Eq. (A.28) and Eq. (A.29) into Eq. (A.26) and Eq. (A.27), re￾spectively, and after a few basic algebra operations to simplify the expressions, the
result is:
푆 − 푆
′ =
푃(푅푘 = 1) − 푃(푅푘+1 = 1)
푘 (푘 + 1)
E[푇푘−1 + 1 | 푇✘푘,푘✘+1 = 푡 − 1] (A.30)
⊓⊔
Theorem A.3 A sequence of retrieved elements ordered by decreasing relevance
probability (i.e. 푃(푅푖 = 1) ≥ 푃(푅푗 = 1), 1 ≤ 푖 < 푗 ≤ 푁) maximizes the expected
value of the Average Precision.
Proof. Let’s consider two sequences of relevance variables 푅1, . . . , 푅푁 and
푅
′
1
, . . . , 푅′
푁
, and an arbitrary position, 1 ≤ 푘 < 푁, such that 푅푘 = 푅
′
푘+1
, 푅푘+1 = 푅
′
푘
,A.2 Evaluation Measures and Optimality 293
and 푅푖 = 푅
′
푖
, ∀푖 ∉ {푘, 푘 + 1} (that is, the two sequences are equal except that elements
at positions 푘 and 푘 + 1 have been swapped).
Following Eq. (A.22) and Eq. (A.23), the expected value of the AP for the first
and the second sequences are, respectively:
AP =
Õ
푁
푡=1
푃(푇 = 푡)
푡
푆(푡) ; AP′
=
Õ
푁
푡=1
푃(푇
′ = 푡)
푡
푆
′
(푡) (A.31)
where 푆(푡) = 푆(푅1, . . . , 푅푁 | 푇 = 푡) and 푆
′
(푡) = 푆(푅
′
1
, . . . , 푅′
푁
| 푇
′ = 푡).
Using the fact that 푇 = 푇
′
(since the two sequences contain the same elements,
just with a different order the total number of relevant elements is the same in both
cases), he difference of the two expected values results in:
AP − AP′
=
Õ
푇
푡=1
푃(푇 = 푡)
푡
(푆(푡) − 푆
′
(푡)) (A.32)
Taking into account Eq. (A.2), the previous equation is equivalent to:
AP − AP′
=
(푃(푅푘 = 1) − 푃(푅푘+1 = 1))
Õ
푇
푡=1
푃(푇 = 푡)E[푇푘−1 + 1 | 푇✘푘,푘✘+1 = 푡 − 1]
푡푘 (푘 + 1)
(A.33)
Observe that the sum is strictly greater than 0, since the expectation includes a
+1 and the rest of variables have values ≥ 0. Thus, its value does not alter the sign
of the difference AP − AP′
, which results in:
AP − AP′
=



> 0 푃(푅푘 = 1) > 푃(푅푘+1 = 1)
= 0 푃(푅푘 = 1) = 푃(푅푘+1 = 1)
< 0 푃(푅푘 = 1) < 푃(푅푘+1 = 1)
(A.34)
Eq. (A.34) implies that, for any sequence of results, if we swap any element at
position 푘 with its next neighbor (position 푘 + 1), the expected value of the AP will
increase if, and only if, the relevance probability of 푘 is lower than that of 푘 + 1.
Thus, if we sort the sequence of 푁 results by their decreasing relevance probability,
we guarantee that the expected value of the AP is the maximum. ⊓⊔
A.2.4 Discounted Cumulative Gain (DCG)
Definition A.5 The Discounted Cumulative Gain (DCG) of a sequence of elements
with relevance 푅푖 ∈ {0, 1} is defined as:294 A The Probability Ranking Principle
DCG(푅1, . . . , 푅푁 )
def
=
Õ
푁
푖=1
2
푅푖 − 1
log2
(푖 + 1)
(A.35)
Now, let’s compute the expected value of the DCG with respect to 푃(푅푖 = 1).
Again, the following expressions assume that the relevance of the elements are
independent.
E [DCG(푅1, . . . , 푅푁 )] =
Õ
푁
푖=1
E

2
푅푖 − 1
log2
(푖 + 1)

=
Õ
푁
푖=1
E[2
푅푖
] − 1
log2
(푖 + 1)
=
Õ
푁
푖=1
2
1푃(푅푖 = 1) + 2
0푃(푅푖 = 0) − 1
log2
(푖 + 1)
=
Õ
푁
푖=1
푃(푅푖 = 1)
log2
(푖 + 1)
(A.36)
Theorem A.4 A sequence of elements ordered by decreasing relevance probability
(i.e. 푃(푅푖 = 1) ≥ 푃(푅푗 = 1), 1 ≤ 푖 < 푗 ≤ 푁) maximizes the expected value of the
Discounted Cumulative Gain.
Proof. Let’s consider two sequences of relevance variables 푅1, . . . , 푅푁 and
푅
′
1
, . . . , 푅′
푁
, such that 푅푘 = 푅
′
푘+1
, 푅푘+1 = 푅
′
푘
, and 푅푖 = 푅
′
푖
, ∀푖 ∉ {푘, 푘 + 1} (that is,
the two sequences are equal except that elements at positions 푘 and 푘 + 1 have been
swapped). The expected value of the DCG for each sequence of variables is:
퐷퐶퐺 =
Õ
푁
푖=1
푃(푅푖 = 1)
log2
(푖 + 1)
; 퐷퐶퐺
′
=
Õ
푁
푖=1
푃(푅
′
푖
= 1)
log2
(푖 + 1)
(A.37)
Since the elements at any position 푖 ∉ {푘, 푘 + 1} are identical in both sequences,
and 푅푘 = 푅
′
푘+1
and 푅푘+1 = 푅
′
푘
, the difference of the two expected values is:
DCG − DCG′
=
푃(푅푘 = 1)
log2
(푘 + 1)
+
푃(푅푘+1 = 1)
log2
(푘 + 2)
−
푃(푅푘+1 = 1)
log2
(푘 + 1)
+
푃(푅푘 = 1)
log2
(푘 + 2)
=
(푃(푅푘 = 1) − 푃(푅푘+1 = 1)) 
1
log2
(푘 + 1)
−
1
log2
(푘 + 2)

=
(푃(푅푘 = 1) − 푃(푅푘+1 = 1)) log2
(푘 + 2) − log2
(푘 + 1)
log2
(2푘 + 3)
(A.38)A.2 Evaluation Measures and Optimality 295
Observe that the second term in the multiplication is always greater than 0. Hence,
this element does not affect the sign of the difference of the expected values, resulting
in:
DCG − DCG′



> 0 푃(푅푘 = 1) > 푃(푅푘+1 = 1)
= 0 푃(푅푘 = 1) = 푃(푅푘+1 = 1)
< 0 푃(푅푘 = 1) < 푃(푅푘+1 = 1)
(A.39)
This implies that, if the retrieved objects are not ordered by decreasing relevance
probability (i.e. ∃푘 : 푃(푅푘 = 1) < 푃(푅푘+1 = 1)), we can increase the expected
value of the DCG by swapping these two elements. We can repeat this operation
as long as the sequence of probabilities is not ordered. Once all the elements are
completely ordered by decreasing relevance, we cannot longer increase its expected
value, proving the theorem. ⊓⊔
A.2.5 Normalized Discounted Cumulative Gain (NDCG)
Notice that the DCG is not upper bounded, and thus sequences of different number of
elements cannot be relatively compared. In order to solve this issue, the Normalized
Discounted Cumulative Gain (NDCG) was defined.
Definition A.6 The NDCG of a sequence of retrieved elements is equal to the value
of its DCG divided by the Maximum DCG achieved by any possible ordering of the
sequence, MDCG(푅1, . . . , 푅푁 ).
NDCG(푅1, . . . , 푅푁 ) =
DCG(푅1, . . . , 푅푁 )
MDCG(푅1, . . . , 푅푁 )
(A.40)
Corollary A.1 The expected value of the NDCG for a sequence of elements ordered
by decreasing relevance probability (i.e. 푃(푅푖 = 1) ≥ 푃(푅푗 = 1), 1 ≤ 푖 < 푗 ≤ 푁) is
equal to 1, which is the maximum.
Proof.
E[NDCG(푅1, . . . , 푅푁 )] =
E

DCG(푅1, . . . , 푅푁 )
MDCG(푅1, . . . , 푅푁 )

=
∫ ∞
0
푃(MDCG = 푚)
푚
E[DCG | MDCG = 푚] 푑푚 (A.41)
Notice that Eq. (A.4) states that ordering the retrieved objects by decreasing
relevance probability maximizes the expected value of the DCG. Thus, if the Max￾imum DCG (i.e. MDCG) is equal to 푚, then for such a sequence of elements,
E[DCG | MDCG = 푚] = 푚.296 A The Probability Ranking Principle
E[NDCG(푅1, . . . , 푅푁 )] =
∫ ∞
0
푃(MDCG = 푚)
푚
푚 푑푚 =
∫ ∞
0
푃(MDCG = 푚)푑푚 = 1 (A.42)
The fact that this is the maximum value comes from the definition of NDCG
itself. ⊓⊔
A.3 Global and Mean Measures
In the KWS literature (and also in many other applications of IR), performance
measures of ranking systems can be divided into global and mean measures1. Global
measures, consider the retrieved elements of all queries used to evaluate the system
at the same time, while mean measures evaluate each query in isolation and later
average the results.
As an example, consider Eq. (A.1). A (global) ranking of retrieved elements
for different queries (푣1 and 푣2) is shown, with their relevance and the score (not
necessarily a probability) used to order the ranking list2. As the example shows,
depending on the value of the relevance variable of each element, the Global and
Mean AP can be quite different.
Table A.1: Example illustrating the calculation of the Global and Mean Average Precision
(AP). The elements of the ranking list are pairs of (keyword, region), sorted by their
decreasing score. 푅푖 and 푅
′
푖
show two hypothetical values of the relevance variable of each
element in the ranked list.
In the first case, the Global AP is equal to 1
3
+
2
9
+
3
12 =
29
36 , while the Mean AP is equal to
1 (since the AP of each individual keyword is 1).
In the second case, the Global AP is equal to 1
3
+
1
3
+
1
6
=
5
6
, while the Mean AP is equal
to 1
2

1
6
+ 1

=
7
12 (since the AP of 푣1 is equal to 1
6
and that of 푣2 is equal to 1).
(a) Ranking
Element 푅푖 푅
′
푖
Score
(푣2, 푥1 ) 1 1 3.9
(푣2, 푥3 ) 0 1 2.8
(푣1, 푥1 ) 1 0 1.7
(푣1, 푥2 ) 1 0 0.4
(푣2, 푥2 ) 0 0 -0.2
(푣1, 푥3 ) 0 1 -1.1
(b) Performance with 푅푖
Global AP 29
36 ≈ 0.81
Mean AP 1
2
(1 + 1) = 1
(c) Performance with 푅
′
푖
Global AP 5
6
≈ 0.83
Mean AP 1
2

1
6
+ 1

=
7
12 ≈0.58
1 In the literature, these two types of measures have been sometimes referred to as micro and macro
measures, respectively [1, 4].
2 The values of the scores are irrelevant for this particular example.References 297
Finally, observe that in Eq. (A.2) we did not make any assumption on the queries
that originated each element in the ranked list. Thus, it is fairly easy to prove that the
PRP is optimal for both the global and mean versions of the measures in the previous
section (see Eq. (A.5) below).
Theorem A.5 The Probability Ranking Principle is optimal with respect to the
Global and the Mean versions of the measures in Eq.(A.2), assuming that the
queries are independent.
Proof sketch. When we proved the optimality of the ranking principle for all the
previous performance measures, we did not make any assumption on the query that
originated each element in the ranked list.
Thus, if the ranking is globally ordered (considering all retrieved elements of all
queries), it will be optimal with respect the global measure of interest (e.g. the gAP).
In addition, if the ranked list is globally ordered, it is also locally ordered for the
elements corresponding to each individual query. Thus, the ranking will be optimal
for each individual query.
Finally, since the mean measure is just the average of the measure of each indi￾vidual query, and each of these are the optimal values, the mean measure of interest
(e.g. the mAP) is also optimal.
Thus, the probability ranking principle is optimal for both the global and mean
versions of the measures. ⊓⊔
References
1. Perronnin, F., Liu, Y., Renders, J.: A family of contextual measures of similarity between
distributions with application to image retrieval. In: 2009 IEEE Conference on Computer
Vision and Pattern Recognition, pp. 2358–2365 (2009). DOI 10.1109/CVPR.2009.5206505
2. Puigcerver, J.: A probabilistic formulation of keyword spotting. Ph.D. thesis, Universitat
Politecnica de Val ` encia (2018) `
3. Robertson, S.E.: The Probability Ranking Principle in IR. Journal of Documentation 33(4),
294–304 (1977). DOI 10.1108/eb026647. URL https://doi.org/10.1108/eb026647
4. Tsoumakas, G., Katakis, I., Vlahavas, I.: Mining Multi-label Data, pp. 667–685. Springer US,
Boston, MA (2010). DOI 10.1007/978-0-387-09823-4 34. URL https://doi.org/10.100
7/978-0-387-09823-4_34Appendix B
Weighted Finite State Transducers (WFST)
Abstract The algebra of WFSTs is a fundamental conceptual and computational tool
for many Artificial Intelligence, Pattern Recognition and Machine Learning tasks.
They are particularly relevant in Natural Language Processing, including Machine
Translation, Automatic Speech Recognition and Handwritten Text Recognition, to
name a few. In this book WFSTs play a crucial role in the algorithms proposed
to compute the various probability distributions involved in Probabilistic Indexing.
This appendix provides the fundamentals and most important algorithmic details of
WFSTs. The appendix is self-contained, and, in some cases, it may not follow the
general style and notation rules adopted in the main body of the book.
B.1 Introduction
A Finite State Transducer(FST) is an extension of a Finite State Automaton (FSA) [6]
which represents a “mapping” between strings of two languages. Here, we adopt the
formal definition of language as a set of sequences of symbols (strings) of a given
alphabet. Transducers were first introduced in [13], although the current formal
definition of a FST is not completely compatible with that of Shannon’s (which did
not formalize it). A FSA is a representation of regular (or rational) language, while
a FST is the formal representation of a rational relation [5].
HMMs, 푛-grams, the CTC probabilistic model of a CRNN, and many others can
all be represented as Weighted Finite State Transducers (WFSTs), and most of the
algorithms needed to estimate them, or to make inferences once the models have
been trained, can be formulated as algorithms operating on WFSTs [9]. The term
weighted simply means that each element of the relation represented by the FST has
some weight value associated.
WFSTs play a crucial role in Chapters 4 and 5 of this book, where the algorithms
needed to build PrIxs from text images are explained.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 299
A. H. Toselli et al., Probabilistic Indexing for Information Search and Retrieval 
in Large Collections of Handwritten Text Images, The Information Retrieval Series 49, 
https://doi.org/10.1007/978-3-031-55389-9 300 B Weighted Finite State Transducers (WFST)
B.2 Description
There are multiple ways of defining a WFST, in particular we will define a WFST,
푇, over a semiring K, as a 6-tuple 푇 = (Σ, Γ, 푉, 퐸, 푠0, 휚) where:
• Σ is the finite input alphabet.
• Γ is the finite output alphabet.
• 푉 is the finite set of states.
• 퐸 ⊆ 푉 × (Σ∪ {휖 }) × (Γ∪ {휖 }) ×K×푉 is the finite set of transitions (also known
as arcs or edges). 휖 is an special symbol indicating that no (input or output)
symbol was consumed during the transition.
• 푠0 ∈ 푉 is the initial state.
• 휚 : 푉 → K is the final weight function.
Notice that a Weighted Finite State Automaton (WFSA), is just a WFST where
the input and output labels of each transition are identical. Thus, we can define a
WFSA, 퐴, over a semiring K, as a 5-tuple 퐴 = (Σ, 푉, 퐸, 푠0, 휚).
A semiring K is an algebraic structure with operations ⊕ and ⊗ (called addition
and multiplication), with the following properties:
• (K, ⊕, 0¯) is a commutative monoid with identity element 0. That is: ¯ (푎⊕푏) ⊕푐 =
푎 ⊕ (푏 ⊕ 푐), 0¯ ⊕ 푎 = 푎 ⊕ 0¯ = 푎, and 푎 ⊕ 푏 = 푏 ⊕ 푎, ∀푎, 푏, 푐 ∈ K.
• (K, ⊗, 1¯) is a monoid with identity element 1.¯ (푎 ⊗ 푏) ⊗ 푐 = 푎 ⊗ (푏 ⊗ 푐), and
1¯ ⊗ 푎 = 푎 ⊗ 1¯ = 푎, ∀푎, 푏, 푐 ∈ K.
• The multiplication (⊗) distributes over the addition (⊕). That is: 푎 ⊗ (푏 ⊕ 푐) =
(푎 ⊗ 푏) ⊕ (푎 ⊗ 푐) (left-distributivity) and (푎 ⊕ 푏) ⊗ 푐 = (푎 ⊗ 푐) ⊕ (푏 ⊗ 푐)
(right-distributivity), ∀푎, 푏, 푐 ∈ K.
• The multiplication by 0 annihilates ¯ K. That is: 0¯ ⊗ 푎 = 푎 ⊗ 0¯ = 0,¯ ∀푎 ∈ K
In addition, in some semirings a division operation can be defined. A semiring is
called left divisible, right divisible or divisible when:
• Left divisible, iff ∀푎 ∈ K − {0¯ }, ∃푏 ∈ K such that 푏 ⊗ 푎 = 1, and ¯ 푏 is unique
(푏 is called the left-inverse of 푎).
• Right divisible, iff ∀푎 ∈ K − {0¯ }, ∃푐 ∈ K such that 푎 ⊗ 푐 = 1, and ¯ 푐 is unique
(푐 is called the right-inverse of 푎).
• Divisible, iff it is both left and right divisible and the left- and right-inverses
are equal, ∀푎 ∈ K − {0¯ }.
In a divisible semiring (left, right or both), we can define a corresponding inverse
operation to the multiplication, the division, represented by the operator ⊘. Common
semirings used in WFST are shown in Table B.1. The Tropical and Log semirings
are widely used in the field of speech recognition and HTR (and are used in the
Chapter 5), since they allow to define different operations and algorithms using aB.2 Description 301
small set of operations on WFST. For instance, marginalization and the Viterbi de￾coding are equivalent to solving the shortest distance and the shortest-path problems
on a WFST in the Log and the Tropical semiring, respectively [7].
Table B.1: Common semirings used in WFSTs.
Semiring K 0¯ 1¯ 푎 ⊕ 푏 푎 ⊗ 푏 푎 ⊘ 푏
Real R 0 1 푎 + 푏 푎 · 푏
푎
푏
Tropical R ∪ {∞} ∞ 0 min{푎, 푏} 푎 + 푏 푎 − 푏
Log R ∪ {∞} ∞ 0 − log(푒
−푎 + 푒
−푏
) 푎 + 푏 푎 − 푏
As we already saw multiple times in the previous chapters, WFSTs have a very
intuitive graphical representation. For example, Fig.B.1 represents a WFST with
input alphabet Σ = {a, b, c} and output alphabet Γ = {X, Y, Z}, five states 푉 =
{푠0, I, II, III, IV}, with initial state 푠0 and two final states (a state 푠 is final iff 휚(푠) ≠ 0)¯
with 휚(III) = 1 and 휚(IV) = 0. The set of transitions is represented by the labeled
arcs in the figure. For instance the transition (푠0, a, X, 2, I) ∈ 퐸 is represented by the
arc connecting states 푠0 and I labeled with a : X/2.
푠0 I
II
III / 1
IV
a : X / 2
b : Y / 1
a : X / 5
a : X / 3
b : Z / 1
c : Y / 1
b : Z / 2
c : Z
Fig. B.1: An example of a WFST.
Usually, the transitions and final weights which are equal to 1 (i.e. 0 in the ¯ Log or
Tropical semirings, as in the example) are omitted in the graphical representation.
For instance, in Fig.B.1, notice that the final state IV and the transition between
states III and IV do not show any weight.
For convenience, given an edge 푒 ∈ 퐸, in our algorithms we typically use the
function 푝(푒) to get the origin (or source, or previous) state, 푛(푒) to obtain the
destination (or next) state, 휔(푒) to get its weight, and 푙푖(푒) and 푙표 (푒) to return its
input and output symbols (or labels), respectively (in the case of a WFSA, we simply
use 푙(푒), since 푙푖(푒) = 푙표 (푒)).
A path 휙 = 푒1, . . . , 푒푘 is an element of 퐸
∗ with consecutive transitions:
푛(푒푖) = 푝(푒푖+1), 1 ≤ 푖 < 푘. The weight of a path 휔(휙) is the ⊗-product of
the weights of the constituent transitions: 휔(휙) = 휔(푒1) ⊗ 휔(푒2) ⊗ · · · ⊗ 휔(푒푘).
Similarly, the total weight of path, ˆ휔(휙), is equal to the weight of the path302 B Weighted Finite State Transducers (WFST)
and the final weight of the destination state of the last transition in the path:
휔ˆ (휙) = 휔(푒1) ⊗ 휔(푒2) ⊗ · · · ⊗ 휔(푒푘) ⊗ 휚(푛(푒푘)). Finally, the path label sequence
푙(휙) is defined as: 푙(휙) = 푙(푒1), . . . , 푙(푒푘). For instance, the weight of the path
휙 = (푠0, a, X, 2, I), (1, a, X, 3, III), in Fig.B.1, is equal to 휔(휙) = 2 ⊗ 3 = 2 + 3 = 5,
its total weight is ˆ휔(휙) = 2 ⊗ 3 ⊗ 1 = 2 + 3 + 1 = 6 and its output label sequence
푙(휙) = XX.
Any path that starts at the initial (or start) state (i.e. 푝(푒1) = 푠0) and has a total
weight different than 0 is a ¯ complete path. Equivalently, a complete path is a path
that starts in the initial state and ends in any final state. Sometimes, when we want
to emphasize that a path is not complete, we will say that it is a subpath.
An important distinction between WFSTs are whether they are acyclic or not. A
WFST is acyclic if there is no possible path in it that traverses the same state more than
once. For instance, the WFST in Fig.B.1 is not acyclic, since the path (I, a, X, 3, III),
(III, b, Z, 2, III) goes through the state III twice. It turns out that many problems related
to WFST have efficient solutions for acyclic WFST, or some conditions are always
met in such cases.
The rational relation defined by a (weighted) FST is given by its set of complete
paths. For instance, Fig.B.1 transduces (relates) the string a, a, b into (to) X, X, Z
with a weight equal to 2 ⊗ 3 ⊗ 2 ⊗ 1 = 8.
In the previous example, there was a single path representing the given relation￾ship, but in general there may exist multiple complete paths. Then, we say that the
FST is ambiguous. In such case, the weight of a given element of the relation (an
input–output sequence of symbols) is the ⊕-sum of all complete paths representing
that element. Thus, if 푇 is a WFST and (푥, 푦) ∈ Σ
∗ × Γ
∗
is an element of the relation
represented by the WFST, its weight1 is defined as:
푇 (푥, 푦) =
Ê
휙:푙푖 (휙)=푥,푙표 (휙)=푦
휔ˆ (휙) (B.1)
Observe that the WFST in Fig.B.1 is actually unambiguous since each complete
path in the WFST represents a different pair of input–output strings. Similarly to the
previous definitions of ambiguity, we can define equivalent properties only looking
at the input or output language of the WFST. For instance, we say that a WFST is
unambiguous on its input (output) if, and only if, for each input (output) sequence
of symbols there exists only a single path that accepts (produces) this string.
Another important distinction between WFSTs is whether they are functional or
not. A WFST is functional if each input string relates only to a single output string
(i.e. the relation that the WFST represents is a function). For instance, the WFST
in Fig. B.1 is not functional, since the input sequence a, a, b, c is related to both
output sequences X, X, Z, Y and X, X, Z, Z (with weights 2 + 5 + 1 + 1 + 0 = 9 and
2 + 3 + 2 + 0 + 0 = 7, respectively).
1 푇 was defined above as a WFST. Here the symbol 푇 is overloaded to also mean a function
푇 (푤, 푤′
) which yields the weight assigned by 푇 to a pair of input–output sequences 푤, 푤′
.B.3 WFST Operations 303
B.3 WFST Operations
Next, we briefly describe some of the most relevant operations on WFSTs which are
used to implement the algorithms described in this book.
B.3.1 Composition
Composition is one of the fundamental operations to create complex WFST from
simpler ones. This operation is used very often when different probabilistic models
have to be combined.
Let K be a commutative semiring (i.e. a semiring with the property that 푎 ⊗ 푏 =
푏 ⊗ 푎, ∀푎, 푏 ∈ K). Let 푇1 = (Σ, Ω, 푉1, 퐸1, 푠0, 휚1) and 푇2 = (Ω, Γ, 푉2, 퐸2, 푠′
0
, 휚2) be
two WFSTs defined over K, such that the output alphabet of 푇1 is equal to the input
alphabet of 푇2, and assume that the sum É
푧∈Ω∗ 푇1 (푥, 푧) ⊗ 푇2 (푧, 푦) is well-defined
and in K for all pairs of strings (푥, 푦) ∈ Σ
∗ × Γ
∗
. Then, the result of the composition
of 푇1 and 푇2 is a WFST, denoted by 푇1 ◦ 푇2, defined by:
[푇1 ◦ 푇2] (푥, 푦) =
Ê
푧∈Ω∗
푇1 (푥, 푧) ⊗ 푇2 (푧, 푦) (B.2)
Each state in the WFST 푇3 = 푇1 ◦ 푇2 is represented by a pair of states from 푇1 and
푇2. Without taking into account transitions with input or output 휖 symbols (which
require an special treatment), then:
(푠1, a, b, 푤, 푠2) ∈ 퐸1 ∧ (푠
′
1
, b, c, 푤′
, 푠′
2
) ∈ 퐸2 ⇒
( (푠1, 푠′
1
), a, c, 푤 ⊗ 푤
′
, (푠2, 푠′
2
)) ∈ 퐸3 (B.3)
Thankfully, an efficient algorithm to perform the composition of arbitrary WFSTs
exists [10, 9], which is a generalization of the classical state–pair construction for the
intersection of FSA [6]. This algorithm has an asymptotic cost which is essentially2
O (|푉1| |푉2|퐷1푀2), where |푉푖
| is the number of states, 퐷푖
is the maximum output
degree (maximum number of output edges from any state), and 푀푖
is the maximum
output multiplicity (maximum number of output edges with the same label) of the
푖-th WFST in the composition.
Fig. B.2 shows one of the many uses of the composition operation when handwrit￾ten text is modeled as the combination of different probabilistic models. In the figure,
a lexicon model, which maps characters into words, is composed with a language
model, which is used to represent the set of possible sentences (and their respective
probabilities). This is, of course, a very simple example of the composition operation.
In practice, the resulting WFST would be further composed with others, representing
other distributions of the composite model.
2 Depending on the implementation, the cost may be slightly different.304 B Weighted Finite State Transducers (WFST) , : , b : 휖 o : oil
r : bread
u : buy
i : 휖
e : 휖
y : 휖
a : 휖
d : 휖
l :
휖
(a) 푇1, a lexicon model
0.22
buy
bread / 0.36
oil / 1.2
, / 1.61
(b) 푇2, a language model
0.22
b : 휖 u : buy y : 휖
b : 휖
o : oil / 1.2
r : bread / 0.36 e : 휖 a : 휖
d : 휖
i : 휖 l : 휖
, : , / 1.61
(c) Composition of 푇1 and 푇2
Fig. B.2: Example of the composition of two WFSTs.
B.3.2 Shortest Path and Distance
The shortest path problem has been deeply studied in Computer Science. Generally
speaking, given a graph with several nodes the goal is to obtain the path with the
smallest total weight between a source node and the rest of the nodes in the graph. The
problem was first tackled in [4] for directed graphs with non-negative real weights.
Generally speaking, we say that the total weight of the shortest path from node 푠 to
node 푠
′
is the shortest distance from 푠 to 푠
′
.
The shortest path problem arises very often in speech and HTR applications.
Particularly, the Viterbi algorithm [14] finds the sequence of states in a probabilistic
model (e.g. a HMM or a combination of these) that maximizes the conditional density
of the observed data. Thus, if we are able to express a probabilistic model as a WFST,
we can then apply the generic shortest path algorithm to solve the same problem as
the Viterbi algorithm, by minimizing the negated (logarithm of the) likelihoods [9].
Take for instance the transducer represented in Fig.B.3 and assume that its weights
are in the Tropicalsemiring (i.e. 푎⊕푏 = min{푎, 푏}, 푎⊗푏 = 푎+푏). There are two paths
with a total minimum weight: 휙1 = (푠0, a, X, 1, I), with a total weight of 1 ⊗ 1 = 2;
and 휙2 = (푠0, a, X, 1, I), (I, c, Z, 1¯, I), with a total weight of 1 ⊗ 1¯ ⊗ 1 = 1 + 0 + 1 = 2.
푠0 I / 1 II / 1 a : X / 1
b : Y / 2
c : Z
Fig. B.3: Example of a cyclic WFST which admits applying the shortest distance algorithm
both in the Tropical and Log semirings.B.4 Determinization 305
The interesting particularity of WFSTs is that the shortest distance problem is not
defined in terms of finding the smallest total weight of the path between two states,
but in terms of computing the ⊕-sum of the weights of all paths between the two:
푑(푠, 푠′
) =
Ê
휙:푝(휙)=푠,푛(휙)=푠
′
휔(휙) (B.4)
where 푝(휙) and 푛(휙) here denote the initial and final states of the path 휙, respectively.
On the one hand, if we assume that the weights are in the Tropical semiring, the
⊕-sum defined above obtains the minimum weight (recall that 푎 ⊕ 푏 = min{푎, 푏}).
On the other hand, if we compute the same sum in the Log semiring, we obtain the
“log-sum-exp” of the weights (since 푎 ⊕ 푏 = − log(exp(−푎) + exp(−푏))). When
transducers are acyclic the ⊕-sum from one state to the rest, can be computed very
efficiently in O (|푉| + |퐸|).
For instance, back to Fig.B.3 the shortest distance between state 푠0 and 푠1 in the
Tropical semiring is 1, but the shortest distance in the Log semiring is equal to:
(1) ⊕ (1 ⊗ 2) ⊕ (1 ⊗ 2 ⊗ 2) ⊕ · · · =
Ê∞
푘=0
1 ⊗
푘 times
z }| {
2 ⊗ · · · ⊗ 2 ≈ 2.232 (B.5)
Although we can loop indefinitely on the state 푠1, the total cost in the Log semiring
finally converges. This is very convenient in text and speech applications, since
very common algorithms such as the Forward and Backward [2, 3], and the Viterbi
algorithms have the same implementation, but using different semirings.
Although further details of these operations are not required to understand the
algorithms presented in the Chapter 5, we recommend the reader to review the works
of [7, 11, 8, 9], if additional information is needed.
The single shortest path algorithm can be extended to obtain the 푛-best (or 푛￾shortest) paths, which is used in several of the PrIx algorithms described in this book.
The asymptotic temporal cost of the 푛-shortest paths algorithm is O (|푉| log|푉| +
푛|푉| + 푛|퐸|).
B.4 Determinization
A FST is said to be deterministic in its input (or in its output) if there is no node with
two output transitions with the same input (or output) label [8]. This is analogous to
the definition of deterministic automaton [6].
The determinization operation aims to find a deterministic WFST which is equiv￾alent to the one given. Equivalent means that it represents the same weighted rational
relation (i.e., the same probability distribution, in the case of probabilistic models).
Unlike in the unweighted case, some weighted transducers (and automata) are
not determinizable (i.e. there is not any equivalent WFST which is deterministic).
Luckily, in our case, determinization is typically used only on lattices, which are
acyclic WFSTs and thereby always determinizable [1].306 B Weighted Finite State Transducers (WFST)
A deterministic WFST has very interesting properties. In particular, a determin￾istic WFST (or automaton) is also unambiguous (in its input or output).
The determinization algorithm is an extension of the powerset construction [12],
used to determinize finite state automata. Thus, the worst case asymptotic cost is
exponential with the number of states in the WFST, i.e. O (exp(|푉|)). However, for
our purposes this worst case scenario is rarely reached in practice, as we will see
in the next chapter. In particular, determinization is used in this book to create the
probabilistic indexes from lexicon-free models (see Chapter 5, Sec. 5.5).
Fig. B.4 shows an original WFST (also shown in Chapter 3, Fig. 3.8) and two
determinized versions. As a stochastic FSA, this WFST is both proper and consistent
(the sum of probabilities of all the sentences is 1). However, it is ambiguous because,
for example, there are four paths associated with the word sequence “that that is is”.
The other two WFSTs are deterministic versions, obtained using the Tropical and
the Real semirings. Both deterministic WFSTs overcome all sentence ambiguities.
0 100
9
31
33
31
33
62
78
81
Original WFST
t / 0.05
that / 0.35
that / 0.5
Hat / 0.1
hat / 0.4
hat / 0.6
that / 1
that / 1
that / 1
that / 1
is / 0.7
is / 0.3
is / 1
is / 1
0 100
9
33
31
33
62
78
Tropical semiring determinized
t / 0.05
that / 0.5
Hat / 0.1
hat / 0.6
that / 1
that / 1
that / 1
is / 0.7
is / 1
0 100
9
32.18
31
32.20
62
78.9
Real semiring determinized
t / 0.05
that / 0.85
Hat / 0.1
hat / 1
that / 1
that / 1
that / 1
is / 1
is / 1
Fig. B.4: Example showing an original WFST containing several segmentation alternatives
for word sequences like “that that is is”, and two determinized versions on the Tropical
and Real semirings. In the determinized WFSTs, each path corresponds to a different word
sequence. Both edge probabilities and the alignments depend on the semiring used.B.4 Determinization 307
In the case of the Tropical semiring,3 from the four ambiguous paths mentioned
above, only the one with the best score (휔(휙) = 0.5·1 ·0.7·1 = 0.35) is kept, along
with the corresponding original alignment boundaries 푎 = 0, 33, 62, 78, 100. Note
that from a probabilistic point of view, this WFST is neither consistent or proper. So,
for many uses, it needs to be re-weighted according to some normalization criterion
(c.f., Chapter 4, Sec. 4.7.4).
The result using the Real semiring can be considered more interesting because it
is both proper and consistent and thereby also “sentence normalized”, according to
Eq. (4.41) of Chapter 4. Thanks to these properties, it does not lose probability mass
and so it reflects the original sentence distribution more accurately. However, in this
case different original paths are merged which corresponded to different alignments
and, thereby, the alignment boundaries of some states become not deterministic.
For instance, in the original WFST there are two different hypotheses, 31 and 33,
for the boundary between the two instances of the word that and the boundary to
be assigned to the single state that results from the merging of these hypotheses
becomes undefined.
In these cases, one can still provide single boundary hypotheses by estimating
an expected values of non-deterministic boundaries, as discussed in Sec. 3.7.3 of
Chapter 3. These are in fact the non-integer values shown in three of the states of the
Real semiring determinized WFST of Fig.B.4. This way, for the single path 휙 of the
sentence above considered, 푤= “that that is is”, we obtain 푃(푤) = 푃(휙) = 0.85 with
an expected alignment: 푎 = 0, 32.18, 62, 78.9, 100, or just 푎 ≈ 0, 32, 62, 79, 100, by
rounding decimals into integer boundaries.
While Real semiring determinization with expected values of non-deterministic
alignment boundaries can be considered a better option, the higher computing cost
and implementation complexity of this option, makes operation on Tropical semiring
the default option in practice (as it is actually the case for the popular OpenFST WFST
toolkit).
3 Note that the sum operator of the tropical semiring is min(· ) (see Table B.1). Therefore, to obtain
the maximum weight, the function − log(· ) has to be applied to the wights before determinization
and the final wights are obtained by applying the function exp(− (· ) ) to the resulting weights.308 B Weighted Finite State Transducers (WFST)
References
1. Allauzen, C., Mohri, M.: On the Determinizability of Weighted Automata and Transducers.
In: In Proceedings of the workshop Weighted Automata: Theory and Applications (WATA)
(2002)
2. Baum, L.E., Eagon, J.A.: An inequality with applications to statistical estimation for proba￾bilistic functions of Markov processes and to a model for ecology. Bulletin of the American
Mathematical Society 73(3), 360–363 (1967). URL https://projecteuclid.org:
443/euclid.bams/1183528841
3. Baum, L.E., Petrie, T., Soules, G., Weiss, N.: A maximization technique occurring in the
statistical analysis of probabilistic functions of markov chains. The Annals of Mathematical
Statistics 41(1), 164–171 (1970). DOI 10.1214/aoms/1177697196. URL https://doi.or
g/10.1214/aoms/1177697196
4. Dijkstra, E.W.: A Note on Two Problems in Connexion with Graphs. Numer. Math. 1(1), 269–
271 (1959). DOI 10.1007/BF01386390. URL http://dx.doi.org/10.1007/BF01386390
5. Eilenberg, S.: Automata, Languages, and Machines. Academic Press, Inc., Orlando, FL, USA
(1974)
6. Hopcroft, J.E., Motwani, R., Ullman, J.D.: Introduction to Automata Theory, Languages, and
Computation, 3rd edn. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA
(2006)
7. Mohri, M.: Semiring frameworks and algorithms for shortest-distance problems. Journal of
Automata, Languages and Combinatorics 7(3), 321–350 (2002)
8. Mohri, M.:Weighted Finite-State Transducer Algorithms. An Overview, pp. 551–563. Springer
Berlin Heidelberg, Berlin, Heidelberg (2004). DOI 10.1007/978-3-540-39886-8 29. URL
https://doi.org/10.1007/978-3-540-39886-8_29
9. Mohri, M., Pereira, F., Riley, M.: Speech Recognition with Weighted Finite-State Transducers,
pp. 559–584. Springer Berlin Heidelberg, Berlin, Heidelberg (2008). DOI 10.1007/978-3-5
40-49127-9 28
10. Mohri, M., Pereira, F.C.N., Riley, M.: Weighted Automata in Text and Speech Processing. In:
Proceedings of the 12th biennial European Conference on Artificial Intelligence (ECAI-96).
Workshop on Extended finite state models of language., pp. 1–5 (1996)
11. Mohri, M., Riley, M.: An Efficient Algorithm for the N-Best-Strings Problem. In: Proceedings
of the International Conference on Spoken Language Processing 2002 (ICSLP ’02) (2002)
12. Rabin, M.O., Scott, D.: Finite Automata and Their Decision Problems. IBM Journal of
Research and Development 3(2), 114–125 (1959). DOI 10.1147/rd.32.0114
13. Shannon, C.E.: A mathematical theory of communication. The Bell System Technical Journal
27(3), 379–423 (1948). DOI 10.1002/j.1538-7305.1948.tb01338.x
14. Viterbi, A.: Error bounds for convolutional codes and an asymptotically optimum decoding
algorithm. IEEE Transactions on Information Theory 13(2), 260–269 (1967). DOI 10.1109/
TIT.1967.1054010Appendix C
Text Image Document Collections and Datasets
Abstract The datasets used in experiments through this book are presented here in
some detail. The presentation follows the order in which the different datasets have
been used first. So, we will start with the popular IAM and the various Bentham
datasets, BEN1, BEN2, BEN3 and BEN4, derived from the large Bentham Papers
collection. Then another couple of popular small academic datasets, George Wash￾ington (GW) and Parzival will follow. Finally, we present a much less known set
of larger datasets, actually aimed at preparatory experiments for real, large-scale
PrIx applications. These include Plantas and Passau, which did not finally lead
to real applications, and Chancery, TSO, Carabela and FCR which were actually
used, along with Bentham Large (BEN4), to develop the large-scale demonstrators
presented in Chapter 10. In addition, two singular datasets are included: A medieval
sheet music dataset, called Vorau and a dataset of notarial records from the Cara￾bela collection, compiled for content-based document classification experiments,
which is only labeled into deed classes corresponding to multi-page documents (that
is, it lacks transcription GT, which would otherwise be exceedingly expensive for a
classification application).
General Remarks About How the Dataset Statistics Are Reported
There are two main styles of presentation of the datasets. One follows the PhD
thesis [20], for the datasets IAM, BEN1, BEN2, BEN3, GW, PAR and PLA, as
defined in Table 6.1 (Chapter 6). In all these cases, query sets were extracted from
the training/validation lexicons and, therefore, it is relevant to report how many query
words do appear in the test set (these words are called “pertinent” – see Secs. 6.9
and 8.1.1). The second style is generally used for the other datasets, BEN4, PAS,
CHA, TSO, CAR and FCR (as defined in Table 6.13), which were compiled in many
different works and projects, following rather heterogeneous criteria. Nevertheless,
in all these cases, the query sets were extracted from the corresponding test-set
lexicons and therefore all query words are guaranteed to be pertinent.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 309
A. H. Toselli et al., Probabilistic Indexing for Information Search and Retrieval 
in Large Collections of Handwritten Text Images, The Information Retrieval Series 49, 
https://doi.org/10.1007/978-3-031-55389-9 310 C Text Image Document Collections and Datasets
C.1 IAM
The IAMDB dataset (or simply IAM) was compiled by the Research Group on
Computer Vision and Artificial Intelligence (FKI) at the Institute of Computer Science
an Applied Mathematics (IAM), in Bern (Switzerland). The dataset as of October
2002 is described in [15]. It is publicly accessible1 and freely available upon request
for non-commercial research purposes.
The handwritten text images in this data set were produced by 500 different
persons that transcribed fragments of text from the Lancaster-Oslo/Bergen (LOB)
corpus [11]. Text from the LOB corpus was split into fragments of 3–6 sentences,
with at least 50 words each. No restrictions were imposed on the writing style or the
type of pen and thus, very different styles and sizes are present. Images were scanned
at 300dpi. Some pages and text lines are depicted in Fig.C.1.
Fig. C.1: Examples of a few pages and segmented lines extracted from the IAM dataset.
1 http://www.fki.inf.unibe.ch/databases/iam-handwriting-databaseC.1 IAM 311
The official partition of the employed version of the dataset (version 3.0) consists
of 747 training pages (6 161 text lines, from 283 writers), two validation sets of
105 pages (900 lines, from 46 writers) and 115 pages (940 text lines, from 43
writers), respectively, and a test set of 232 pages (1 861 lines, from 128 writers).
However, in this book, as well as in many other line-oriented PrIx and KWS works,
we use a different partition of the dataset, summarized in Table C.1. Notice that this
partition is also different from the one typically used in many recent HTR works
(e.g. [5, 34, 19]), although the training set is the same in all cases.
Table C.1: Statistics of the IAM dataset and partition used in the experiments.
Train Valid. Test Total
Writers 283 25 25 333
Pages 747 116 110 973
Lines 6 161 920 929 8 010
Words 53 767 8 599 8 315 70 681
Characters† 275 466 41 031 40 445 364 952
Lexicon size 7 772 2 450 2 492 9792
Alphabet size† 78 74 71 78
† Not including the white space or other auxiliary symbols.
The query set employed in this book was also used in previous PrIx publications
(e.g. [26, 29, 21] and many others). It consists of 3 421 words,2 most of which were
extracted from the training set, excluding English stop words. The same set of queries
is used in both the validation and test sets. Table C.2 shows some statistics of the
query set, on both the validation and test sets.
Table C.2: Statistics of the queries used in the IAM dataset.
Valid. Test
Queries 3 421 3 421
Pairs 3 147 320 3 178 109
Pertinent queries 1 053 1 101
Pertinent pairs 1 849 1 919
Following [2], three external text corpora are used to improve the language models.
Specifically, the full LOB corpus (except the sentences that appear in the original test
set of the IAM) [11], the Brown corpus [16] and the Wellington corpus [10]. These
three external corpora are typically used for IAM experiments in HTR and KWS
works. Table C.3 gives additional information of these text corpora. The three of
them contain full sentences much larger than the IAM text lines (actually, equivalent
to a full IAM page). We split the sentences in these data sets so that the distribution
of the number of words in each split “line” matches that of the lines in the IAM
dataset, in order to improve the 푛-gram models.
2 https://gist.github.com/jpuigcerver/a2c94a2196211aea6b2c08774f50f541312 C Text Image Document Collections and Datasets
Table C.3: Statistics of the external corpora used in IAM experiments.
LOB Brown Wellington
Words 1 119 904 1 045 213 1 144 401
Characters† 5 803 916 5 582 023 6 055 820
Lexicon size 52 724 53 115 58 919
† Not including the white space or other auxiliary symbols.
C.2 The Bentham Papers Collection and Datasets
The Benthamcollection includes several hundreds of manuscripts written by Jeremy
Bentham, an influential English philosopher, jurist, and social reformer in the XVIII
and XIX centuries, as well as his correspondents and many of his secretarial staff. A
portion of this collection was first indexed in the tranScriptorium project.3 The rest
of the collection was processed during the READ project.4 The entire collection is
composed of around 90 000 images, but only a relatively small portion of those have
been manually transcribed so far.
Many HTR and KWS publications and competitions have used different datasets
derived from this collection. In the following subsections we will provide details
of each Bentham dataset used in this book. The whole collection can be seen in
our large scale Bentham Papers demonstrator 5 (Chapter 10). Fig. C.2 shows a few
random images from this collection.
C.2.1 ICFHR-2014 Competition on HTR (BEN1)
This was the first dataset derived from Bentham Papers. It was a relatively small
dataset compiled in the TranScriptorium project for early HTR experiments. Images
were selected so as to avoid most of typical script complexities of the Bentham
collection (see Fig.C.2) and text lines were manually detected and extracted, so
as to circumvent layout difficulties. In this process, many marginalia were ignored.
Fig. C.3 shows typical examples of pages and extracted text lines from this dataset.
This dataset was adopted in the ICFHR-2014 Handwritten Text Recognition Com￾petition on a tranScriptorium Dataset (shortened as ICFHR-2014 HTRtS). It has been
used in this book for line-oriented KWS experiments. It encompasses a total of 433
pages (11 473 lines with nearly 110 000 running words and a vocabulary of more
than 9 700 different words. Table C.4 shows the main statistics of this dataset and
the train, validation and test partition that we adopted.
3 https://cordis.europa.eu/project/id/600707
4 https://cordis.europa.eu/project/id/674943
5 Publicly available since 2019 at https://www.prhlt.upv.es/htr/benthamC.2 The Bentham Papers Collection and Datasets 313
Fig. C.2: Examples of Bentham Papers page images.
Table C.4: Statistics of the Bentham BEN1 dataset, as proposed in ICFHR-2014 HTRtS
and used in our experiments.
Train Valid. Test Total
Pages 350 50 33 433
Lines 9 198 1 415 860 11 473
Running Words 86 075 12 962 7 868 106 905
Running Characters† 442 336 67 400 40 938 550 674
Lexicon size 8 658 2 709 1 946 9 716
Alphabet size† 91 91 91 91
† Not including the white space or other auxiliary symbols.
C.2.1.1 Line-level PrIx Experiments
This dataset was used in the line-level experiments of Secs. 6.2 and 6.8. Several
criteria can be assumed to select the keywords to be used in the assessment of a
PrIx (or KWS) system. Clearly, any given approach may perform better or worse
depending on the query words it is tested with, and how these words are distributed in
the test set. Of course, the larger the set of keywords, the more reliable the empirical
results. Moreover, since our approaches aim at indexing large collections, testing
with a large set of keywords is mandatory. As shown in Table C.5, we adopted all
the 8 658 different training-set words as queries.314 C Text Image Document Collections and Datasets
Fig. C.3: Examples of pages and segmented lines from the BEN1 Bentham dataset.
Table C.5: Queries for PrIx experiments on the BEN1 Bentham dataset. They are derived
from the training-set and, therefore, only a relatively small subset of 1 487 query words
(called “pertinent”) actually appear in some test-set line.
Queries 8 658
Pairs of query-text-lines 7 445 880
Pertinent queries 1 487
Pertinent pairs 6 727
Statistics of occurrence of query words in the test set are also shown in Table C.5.
Only a relatively small subset of 1 487 query words actually appear in any of the
test lines. These are called pertinent queries, while all the other queries are called
non-pertinent. Spotting or indexing non-pertinent keywords is challenging because
they can be easily confused with other words, leading to false alarms.
C.2.1.2 Multi-word Page-level PrIx Experiments
The BEN1 dataset was also used in the multi-word PrIx experiments described in
Sec. 8.1.1. However, in this case, the evaluation was performed at page level and the
query set was streamlined to allow for more natural boolean combinations. Specifi-C.2 The Bentham Papers Collection and Datasets 315
cally, the query set was limited to the 3 293 words whose frequency of occurrence in
the training set ranged from 2 to 10. This avoided to include most stop words (gen￾erally with word frequencies greater than 10) and also many words that are unlikely
to appear in the test-set. Despite this selection criteria, only a relatively small subset
of 674 (pertinent) query words actually appear in some test-set line.
In addition, to define multi-word queries, a pool consisting of all the 5 420 278
pairs of different words was compiled. As in the single-word case, not all the word￾pairs are pertinent. The maximum number of pertinent queries which can be com￾posed for each query type are reported in Table C.6, along with the other figures
mentioned above. The table also reports the equivalent statistics for the number of
pairs of (query, page) that can be formed.
Table C.6: Statistics of the query pools generated for page-level multi-word experiments on
the BEN1 Bentham dataset. 푟max the maximum ratio between non-pertinent and pertinent
queries.
Query type Total Pertinent 푟max
Queries
Single-word 3 293 674 3.89
AND 5 420 278 11 784 458.97
OR 5 420 278 1 992 007 1.72
Pairs
Single-word 108 669 836 128.99
AND 178 869 174 12 438 14 379.86
OR 178 869 174 2 739 674 64.29
We defined 푟 as the ratio between non-pertinent and pertinent queries. Therefore,
푟max denotes the maximum ratio that can be achieved with each pool of words or
word-pair boolean combinations.
C.2.2 ICFHR-2014 Competition on KWS (BEN2)
This was one of the datasets used in the ICFHR-2014 Handwritten Keyword Spotting
Competition.6 [17] Since the competition was targeted on training-free (and QbE)
KWS approaches, no training data was officially released. However, the test set of 50
page was extracted from the 400 pages available for training and validation, released
in the previous ICFHR-2014 HTRtS competition (i.e., the BEN1 Bentham dataset).
Thus, we excluded the 50 test pages from these 400 pages and used the manually
segmented and transcribed text lines provided by the HTRtS competition for training
and validation purposes. The set of pages used for training and validation is not
disjoint. We simply randomly sampled 10% of the text lines available for training to
create the validation set.
Table C.7 shows the main statistics from the BEN2 Bentham dataset, as used in
the ICFHR-2014 Competition on KWS. Notice that the alphabet size is much smaller
6 http://vc.ee.duth.gr/H-KWS2014/316 C Text Image Document Collections and Datasets
than that of the HTRtS competition (BEN1). This is because all the transcripts were
lowercase folded, since the competition on KWS was scoped in a QbE setting.
Table C.7: Statistics of the Bentham partition from the ICFHR-2014 Competition on KWS
as used in our experiments.
Train Valid. Test Total
Pages 320 30 50 400
Lines 8 337 973 1 303 10 613
Words 70 223 8 187 10 786 89 196
Characters† 308 507 35 946 46 644 391 097
Lexicon size 7 301 2 155 2 452 8 389
Alphabet size† 60 60 60 60
† Not including the white space or other auxiliary symbols.
In the experiments described in Sec. 6.10.2 we only targeted the (word) segment￾ation-free setting. To this end, we used both the manual test-set line segmentation
provided by the HTRtS competition and fully automatic segmentation, based on a
simple method known as Horizontal Projection Profile [14].
This is essentially the same setting as in [32], although there we used an additional
text corpora to improve the word 2-gram language model, while in this book we only
use the transcripts of the training images to train a character language model.
The query set defined for the ICFHR-2014 KWS competition was composed of
290 word images, of only 41 unique keywords. These keywords correspond to words
longer than 6 characters and written at least 5 times in the test set. Table C.8 shows
all the keywords selected, along with the corresponding frequency of occurrence.
The query images came from several writers, had different writing styles, font
sizes and noise. Some examples are shown in Fig.C.4.
Table C.8: The 41 different keywords of the ICFHR-2014 KWS competition, along with
the corresponding frequency of occurrence (#) in the test set.
Keyword # Keyword # Keyword # Keyword # Keyword #
received 5 together 5 offence 5 Exposition 5 property 5
punishment 5 instance 5 circumstance 5 knowledge 5 greater 5
expence 5 character 6 species 6 different 6 particular 6
certain 6 subsequent 6 mischief 6 instead 6 Governor 6
Prisoners 6 themselves 7 another 7 present 7 therefore 7
question 7 quantity 7 Provisions 7 sufficient 7 several 8
general 8 necessary 9 himself 9 nothing 9 according 9
intended 9 purpose 10 account 11 persons 11 against 11
without 15
Fig. C.4: Examples of the query images used in the ICFHR-2014 Competition on KWS.C.2 The Bentham Papers Collection and Datasets 317
It is important to realize that such a query set is orders of magnitude smaller (and
also very much simpler) that any of the other query sets considered in this book.
In the segmentation-free setting of the contest, these images have to be spotted in
images of pages written by the same or different writers and/or exhibiting different
writing variations. Therefore, there is an large amount of possible pairs (query,
bounding box) at page level. However, the test set only contains 2 850 reference
bounding boxes for the 290 query images (of 41 different words).
It worth noting that the rules of the ICFHR-2014 KWS competition violated
one of the assumptions made in our probabilistic formulation. In particular, some
of the relevant bounding box instances in the test pages for a particular query,
actually contain a different word written in them. For example, instances of words
like “altogether” or “characteristic” were considered relevant for query images with
the word “together” or “character” written on them, respectively.
C.2.3 ICDAR-2015 Competition on KWS (BEN3)
This dataset was proposed in the ICDAR-2015 Competition on Keyword Spotting
for Handwritten Documents,7 organized to provide a benchmark to fairly compare
different KWS approaches. BEN3 is also part of the Bentham Papers collection and
a subset of an HTR competition8 that was launched within the same ICDAR-2015
conference. Some of the pages used in previous HTR and KWS competitions were
also used in here, as part of the training and validation sets. In this book it was used
in the word-segmentation-free experiments described in Sec. 6.10.1.
The test set consisted of 70 document images (never used in any previous com￾petition), containing several problems to be addressed, such as writing by different
hands, styles, font-sizes, crossed-out words, etc. In addition, 10 document pages
were provided for validation purposes. The main block of text was extracted from
the original pages, so participants did not have to deal with marginalia. An additional
set of 423 document images, with manually segmented and transcribed text lines
was also provided to the participants competing in the training-based track.
Notice that the 10 validation pages provided in the competition (named Valid.-
I in Table C.9) were not transcribed or segmented into lines. Therefore, in our
experiments we used 10% of the training lines (30 pages) to tune some of the
hyperparameters of our model (we call this set Valid.-II). Some statistics of the
dataset, as used in our experiments, are given in Table C.9.
Since the full transcripts of the Valid.-I and test sets were not provided, the
reported number of words is only approximate, based on the number of word images
provided in the segmentation-based task. In addition, the provided transcripts were
normalized by lowercase folding and transliterating (as in BEN2).
7 https://www.prhlt.upv.es/htr/kwsContestIcdar15
8 https://www.prhlt.upv.es/htr/htrContestIcdar15318 C Text Image Document Collections and Datasets
Table C.9: Statistics of the Bentham partition from the ICDAR-2015 Competition on KWS
as used in our experiments. Values preceded with “∼” are approximate.
Valid.-I Test Train Valid.-II Total
Pages 10 70 393 30 503
Lines — — 10 013 1 131 11 144
Words ∼3 392 ∼16 840 84 071 9 359 ∼113 662
Characters† — — 366 596 40 588 407 184
Lexicon size — — 8 012 2 321 8 449
Alphabet size† — — 60 60 60
† Not including the white space or other auxiliary symbols.
Within the training-free track, two sub-tracks were defined: one for QbS and the
other the QbE. A total of 243 different keywords were selected as the query set,
with lengths ranging from 6 to 15 characters. Each keyword was represented by
up to 6 different example images, making a total of 1 421 query images. All query
words were written at least 4 times in the test set. In total, there were 1 867 relevant
bounding boxes to be retrieved for the 243 keywords (QbS scenario), and 11 006 for
the 1 421 query images (QbE scenario).
As it was the case in the ICFHR-2014 Competition on KWS (BEN2), all cas￾ing instances of a word were considered equivalent (hence our lowercase nor￾malization of the transcripts), but not plurals or derived words. For instance,
“therefore” and “Therefore” are considered the same keyword, but not “according”
and “accordingly”, nor “instance” and “instances”.
C.2.4 Large Bentham Dataset used in [28] (BEN4)
As of 2018, less than half of the Bentham Papers transcripts that were available
had been used in formal experiments or competitions. Therefore, aiming at the
large-scale demonstrator presented in Sec. 10.3.3 and the corresponding preparatory
experiments of Sec. 6.9.4, we compiled yet another “large” Bentham, referred to
in this book as BEN4. This dataset and the corresponding PrIx experiments were
first reported in [28]. In addition to the GT images, an external text corpus collected
from large sets of documents related with Bentham was also employed to train the
character 푛-gram language model.
Table C.10 shows details of this dataset and the training/test partition adopted, as
well as the external text data and the query set.
To give an idea about the challenge degree that this dataset may entail, the
paper [28] reports for the best case a WER of 17.3% (case folding and ignoring
punctuation marks wit 8-gram LM).C.3 George Washington (GW) 319
Table C.10: Basic statistics of the relatively large BEN4 Bentham dataset used in the
experiments of Sec. 6.9.4 and their corresponding external text corpora. The “†” mark
indicates that punctuation symbols are ignored.
Train-Val Test
Dataset
Pages 846 357
Lines 23 942 12 363
Running words 206 672 100 781
Running words † 178 522 89 870
Lexicon † 10 948 6 988
Character set size † 67 65
External
Text
Running Words 7 807 764 –
Lexicon 61 920 –
Query set size † – 6 953
C.3 George Washington (GW)
The George Washington dataset (sometimes referred to as Washington, or simply
GW), was created from the George Washington Papers at the Library of Congress.9.
These papers were written in the 18th century in English language, mainly by George
Washington himself. The GW dataset described here has been used in countless
academic publications and competitions. However, with only 4 902 running words,
it very small in absolute terms and is just a very minuscule fraction of the whole
George Washington Papers collection.
The GW dataset was used in seminal KWS publications, such as [23, 22, 6, 7, 1],
and it is still very popular among the KWS researchers. Note that different versions
of this dataset are used in the literature. Our experiments were performed using two
of these versions, as discussed below. Fig.C.5 shows examples of GW images.
Line-level Settings
The line-level partition is supported by the Research Group on Computer Vision
and Artificial Intelligence (FKI) at the Institute of Computer Science an Applied
Mathematics (IAM), in Bern (Switzerland).10 It was originally used in [6], and it
is based on the same 20 pages originally used in [13], although the word and line
segmentations, and the transcripts are of their own.
A detrimental characteristic of this partition was that they only provided the
binarized and normalized line and word images, which is a sub-optimal scenario for
modern models, like CRNNs. Nevertheless, we used this partition for compatibility
with many previous line-level KWS works. A few of these lines are shown in Fig.C.6.
9 www.loc.gov/collections/george-washington-papers/about-this-collection/
10 https://fki.tic.heia-fr.ch/databases/washington-database320 C Text Image Document Collections and Datasets
Fig. C.5: Examples of page images extracted from the George Washington dataset (GW).
Fig. C.6: Examples of a few normalized and binarized line images used from the line-level
partition of the George Washington database.
The 20 pages in this partition encompass only 656 text lines. Since this is a quite
small data set, four-fold cross-validation (CV) partitions were defined. In each CV
partition, 10 pages were used for training, 5 for validation and 5 for testing purposes.
The average statistics across the four folds of this partition are shown in Table C.11.
Notice that the average statistics in the validation and test sets are identical, due to
the cross-validation procedure.
Table C.11: Statistics of the George Washington dataset (GW) used in our line-level
experiments. Rows marked with † do not include white space or other auxiliary symbols.
Train (avg.) Valid/Test (avg.) Total
Pages – – 20
Lines 328.0 164.0 656
Running Words 2 451.0 1 225.5 4 902
Running Characters† 11 165.5 5 582.8 22 331
Lexicon size 898.0 538.8 1 466
Alphabet size† 70.0 66.5 70
† Not including the white space or other auxiliary symbols.C.3 George Washington (GW) 321
The alphabet size is different from that of the one originally published in [6],
since they considered some frequent words (e.g. the signature “G.W.”) as individual
symbols of their alphabet.
To perform PrIx (or KWS) experiments, the set of words in the training lexicon
of each cross-validation fold with at least one occurrence in the test set was used as
the query set. Thus, all queries are pertinent in the test set, although not necessarily
in the validation set of each fold. Table C.12 summarizes these statistics.
Table C.12: Statistics of the queries used in the George Washington dataset (GW).
Validation (average) Test (average)
Queries 104.5 104.5
Pairs 17 139.0 17 144.5
Pertinent queries 46.5 104.5
Pertinent pairs 108.0 194.3
Word-level Settings
Another popular partition of this data set is the one used in [1], which is also based
on [13]. This partition is typically used by KWS works with word-segmentation
needs, such as the distance and PHOC-based approaches that we compare with ours
in Secs. 7.2.3 and 7.3.4.
In this case, only a training and test sets are defined for each of the (four) cross￾validation folds, which are not the same as in the previous partition. The number of
words in each training and test partition is the same across the four folds.
Under this setting, all word images with more than one occurrence in the test set
were used as queries, and the rest of instances in the test set had to be retrieved by
the given KWS or PrIx method. The transcripts of the word images were converted
to lowercase, and punctuation marks were removed, so that a pair of images contain￾ing the same word but with different cases is considered pertinent. For evaluation
purposes, only pairs of distinct images are considered. Table C.13 summarizes the
statistics of this partition of the George Washington dataset.
Table C.13: Statistics of the query set used on the word-level experiments with the George
Washington dataset (GW).
Train Test Total
Query words 3 645 1 215 4 860
Pairs — 1 471 365 —
Pertinent pairs — 16 842 —322 C Text Image Document Collections and Datasets
C.4 Parzival (PAR)
This dataset contains images of medieval German manuscripts, from the 13th cen￾tury, written in Gothic script. The full Parzival collection consists of 16 books,
although only a tiny subset has been manually annotated by German transcribers.
Transcriptions are available for 47 pages, out of the 318 available in the full (tiny)
data set. At 4 477 total running words, this is yet another minuscule dataset that
became very popular for HTR and KWS experiments.
We used the partition released by the FKI research group. This release11 provides
the page images in color, scanned at 300dpi, as well as binarized and normalized
text line images. Fig.C.7 shows some examples of the pages and (pre-processed) text
line images available in the dataset. Observe that the binarization and normalization
carried out was quite severe and degraded the images significantly. Still, we used
this dataset in our experiments.
Fig. C.7: Examples of pages and a couple of segmented (binarized and normalized) text
lines from the Parzival dataset.
The statistics of the adopted partition are given in Table C.14. Notice that, on the
one hand, the 47 pages were not distributed disjointly among the train, validation,
and test sets, which makes this dataset even more easier. On the other hand, some
of the characters that appeared in the validation and test sets were not included in
the training set, meaning that these characters are badly modeled (if considered at
all) by the statistical models. This is the same partition used in many seminal KWS
papers such as [6, 7, 27].
11 www.fki.inf.unibe.ch/databases/iam-historical-document-database/parzival-databaseC.5 Plantas (PLA) 323
Table C.14: Statistics of the Parzival (PAR) partition used in the experiments.
Train Valid. Test Total
Pages 47 47 47 47
Lines 2 237 912 1 328 4 477
Running Words 14 042 5 671 8 407 28 120
Running Characters† 64 436 26 211 38 339 105 343
Lexicon size 3 221 1 753 2 305 4 936
Alphabet size† 89 79 81 95
† Not including the white space or other auxiliary symbols.
The official query set released by the FKI group was adopted. It consisted of 1 217
different keywords which appear in both the validation and the test sets. Table C.15
show the statistics of this query set. Notice that all the queries are pertinent in the
test set, meaning that at least one test line is relevant for each query word.
Table C.15: Statistics of the query set used in the Parzival database (PAR).
Valid. Test
Queries 1 217 1 217
Pairs 1 109 904 1 616 176
Pertinent queries 718 1 217
Pertinent pairs 3 533 5 764
C.5 Plantas (PLA)
The “Historia de Las Plantas” series of manuscripts was written in the XVII century
by the Spanish botanist Bernardo de Cienfuego. It encompasses seven single-hand
manuscripts, with close to 10 000 pages in total. This work never went to print, so
the manuscripts are the only source of information for researchers interested in the
botanical knowledge of the modern era.
Only the first volume of this series was used for experimentation and it constitutes
the dataset here referred to as Plantas (or just as PLA). While the original aim was
to eventually transcribe or index the full series, this project never went through and,
for the time being, only the PLA dataset was considered for GT production and
experimentation. Fig.C.8 show examples of images from this dataset.
Because an original objective was to produce a critical edition of the “Histo￾ria de Las Llantas” series, the GT reference transcripts were produced in a “dual”
style [25], which includes both diplomatic and modernized versions. For the ex￾periments reported in this book only the diplomatic transcripts were used, which
typographically transcribe as accurately as possible all significant characteristics
of the original manuscript, including spelling, punctuation marks, abbreviations,
crossed-out and inserted text, and other text alterations.324 C Text Image Document Collections and Datasets
In Table C.16, rows labeled with “Image”, report basic statistics of the line-level
annotated transcripts of Plantas dataset as it is used in this book. From the total
content of the manuscript (about 1 000 pages), these statistics exclude a “Table of
Content” and a large “Glossary”, which were available but not aligned at the line
level. Empty page images as well as those containing only drawings were excluded.
Full details about this dataset are provided in [25].
Extra text was extracted from the “Table of Contents” and “Glossary”. It amounts
to 2 645 running words which were used for language model training. This extra text
increased by about 1 000 words the training lexicon and helped reduce the amount
of OOV words in the validation and test sets, which was important since only PrIx
lexicon-based experiments were performed on this dataset.
Fig. C.8: Examples of page images and a close-up of some text lines from the Plantas
dataset (PLA).C.6 Passau Parish Records (PAS) 325
Table C.16: Statistics of the Plantas dataset. “Text” includes image transcripts plus extra
text extracted from the book “Table of Content” and a “Glossary”.
Train Valid. Test Total
Image
Pages 224 40 607 871
Lines 6 788 955 11 801 19 544
Running Words 67 912 9 753 117 029 194 694
Lexicon size 10 861 2 198 14 018 20 834
Alphabet size† 73 73 73 73
Text
Running Words 70 557 — — 197 339
Lexicon size† 11 890 — — 21 417
† Not including the white space or other auxiliary symbols.
The Plantas query set was defined on the extended lexicon (including the extra
text), excluding 958 words corresponding to numbers. This gave a total of 10 932
different query words. The statistics of this query set are reported in Table C.17.
Table C.17: Statistics of the query set used in the test partition of the Plantas database.
Queries 10 932
Pairs 129 008 532
Pertinent queries 4 888
Pertinent pairs 86 304
C.6 Passau Parish Records (PAS)
The Passau Parish Records collection, held by the Passau Diocesan Archives, con￾tains information about the parishioners who were baptized, who married and who
died within the geographic boundaries of the various parishes of the Diocese of
Passau. The scans originate from more than 100 pastoral districts with their own
record keeping, which started in the late 16th century by the order of the Church and
is carried out today. Overall, the collection encompasses more than 800 000 single
or double-page images.12
From this large collection, a small dataset of 289 images was compiled in the
READ project (see footnote 5 in Chapter 10). This dataset is referred to as Passau
or just PAS in this book.13 Its main purpose was to challenge HTR, KWS and PrIx
systems with selected typical difficulties of the whole collection. Even though the
PrIx results were encouraging (see Sec. 6.9.1), the processing of the whole collection
was not considered feasible (within the READ project).
12 Openly available at http://data.matricula-online.eu/de/deutschland/passau
13 Ground-truthed dataset publicly available at http://doi.org/10.5281/zenodo.1296322326 C Text Image Document Collections and Datasets
The Passau dataset samples the full collection and showcases the evolution of
three types of records over time. The images show a great variability in handwriting
styles, and the progress of record keeping over the centuries, with more and more
standardized table forms introduced in the early 19th century. Examples of images
from this dataset are shown in Fig.C.9.
Each record in the register books refers to a sacramental event and therefore
provides a reference to the person, who was baptized on a given day, a couple,
which was engaged and later married, or the person who died. Besides the names
of the individual, references to occupation, locations of living, priests and witnesses
or doctors are given. In the baptismal and wedding entries, also the names of the
parents are kept and, in death records, the illnesses and reason of death are recorded.
Fig. C.9: Examples of page images of the Passau dataset (PAS).
As other historical German record collections, this dataset contains a large number
of non-ASCII symbols. In addition, rather inconsistent and slightly different spelling
variations of the same word (e.g., accents, umlauts, underdots, tie bar, etc.) appear
in the images and in the GT reference record transcripts. The character set used for
the reference transcripts accounts a total of 263 UTF-8 different symbols, most of
which are/contain non-ASCII characters corresponding to archaic symbols used in
old German writing.
For keyword search experiments, every character or symbol is transliterated
by case folding and, if necessary, by removing diacritics and mapping non-ASCII
symbols onto their ASCII equivalents. Table C.18 shows the most relevant types of
character transliteration applied. This way,the number of different characters in the
reference transcripts was reduced to from 263 to 102.C.6 Passau Parish Records (PAS) 327
Table C.18: Main forms of character transliteration applied to the PAS dataset.
Remove Diacritics Non-ASCII to ASCII
ċ, č, c̾ C Æ, æ AE ĳ II ŋ EN
è, ē, ë E Œ, œ OE ß SS ƍ US
m̄ , m̌ , ṃ M p̖
, p̾ PRO đ DE δ DER
Table C.19 shows details of the 289 images of the PAS dataset used in this work.
179 images were selected for training, 21 for validation and the remaining 89 for
testing. The transliterated lexicon is also reported in Table C.19.
The column “Test” corresponds to the test-set images used for plain, single-word
PrIx experiments, while “TabTest” is a subset of “Test” containing only tabular data
images, many of them lying across two contiguous pages as in one of the examples
of Fig. C.9. It was used to measure the performance of structured multi-word queries
aimed at information retrieval from table images.
Table C.19: The Passau experimental dataset (PAS). All the text has been transliterated,
and the “†” mark indicates that punctuation symbols are ignored. The large differences in
running words with and without excluding punctuation marks are mainly due to abbreviated
words, which generally end with a dot. “TabTest” is the subset pure tables from “Test”.
Train+Val Test TabTest
Pages 200 89 44
Lines 29 314 16 376 11 710
Running words 72 848 37 354 21 027
Running words † 47 811 26 709 15 141
Lexicon size † 11 160 5 801 3 141
Alphabet size † 99 87 73
Query set sizes † – 5 725 363
All the transliterated test-set words longer than 1 character were used as key￾words, making a query set of 5 725 query words. On the other hand, for structured
multi-word tabular queries a set of 363 queries of the form “⟨column-heading,
column-content⟩” was provided by the Passau Diocesan Archives staff. The num￾ber of different words in the column-heading parts of these structured multi-word
queries range from 1 to 6, while all the column-content parts contain just 1 word.
To give an idea about the degree of challenge that this dataset entails, the paper [12]
reports for the best case a WER of 21.1% on the test set partition with transliteration
(using 6-gram LM).328 C Text Image Document Collections and Datasets
C.7 Tresor des Chartes and ´ Chancery (CHA)
The Tresor des Chartes ´ is a series of 199 large medieval manuscripts containing
registers produced by the French royal chancery from 1302 to 1483. The full series
encompasses more than 83 000 sizable pages, densely written mainly in Middle
French, Latin, and Early Spanish. The major part of this series is held by the French
Archives Nationales and a smaller part by the Bibliotheque Nationale de France ` .
This vast and iconic series bears witness to the rationalization of late medieval
administration and is a key source to our understanding of medieval Europe and the
rise of centralized nation state on the continent as a consequence of the long lasting
wars between France and England.
Making this series free-text searchable was the primary objective of the HIMANIS
project (see footnote 4 in Chapter 10). This objective was fully accomplished and a
PrIx search interface was setup which has been publicly available since 2017.14
The dataset compiled for the experimental work carried out prior to indexing
the whole collection is called “Chancery” or CHA for short. The experiments and
results with this dataset were reported in [3] and are summarized in Sec. 6.9.2 of this
book. The transcription GT was derived from the monumental text edition provided
by Paul Guerin [ ´ 9] which contains a (relatively small) set of transcripts of about
1 770 registers (called “acts”) from this series of manuscripts. These transcripts were
converted in XML-TEI format by the Ecole Nationale des Chartes15. The resulting
GT corresponds to 436 (partial) page images of these acts, which represent merely
0.67% of the complete Tresor des Chartes ´ series, but it is largely representative of
its diversity and the challenges it entails. Examples of Chancery page images are
shown in Fig. C.10, along with a trimmed act image with its modernized transcript.
In a first stage, text lines of theChancery images were semiautomatically detected
and aligned with the Guerin transcripts. The manual corrections, helped uncovering ´
a significant number of errors in the modernized text; nevertheless, some errors
remain marginally, both for the text and for the alignment. Basic statistics of the
dataset finally used for experimentation appear in Table C.20.
The query set for PrIx evaluation (see Table C.20), consisted of all the words
in the test set, excluding numbers and punctuation marks, making a total of 6 506
(diacritics- and case-folded) query words. A large proportion (about 33%) of these
keywords are expanded forms of words which in the images often appear abbreviated
in several ways (see details in [3]).
Finally, in order to give an idea about the degree of challenge that this dataset
entails, the paper [3] reports a WER around 33.4% on the test set partition (with a
5-gram LM, case folding and ignoring punctuation marks and diacritics).
14 See Sec. 10.3.1 and www.prhlt.upv.es/htr/chancery
15 http://corpus.enc.sorbonne.fr/actesroyauxdupoitou/.C.7 Tresor des Chartes and ´ Chancery (CHA) 329
Philippes, etc. Savoir faisons a touz presenz et avenir que, comme ou traiti ` e du mariage de nostre am ´ e et feal le seigneur de Harecourt ´
et de Ysabeaul, sa fame a present, file de nostre ame et feaul le seigneur de Partenay, certainnes convenances eussaint est ´ e traite- ´
-es et acordees entre les dites parties par leur consoil de leurs amis, et sus ycelles convenances eust est ´ e et fust entre eus aucun ´
descort, c’est assavoir que li diz sires de Harecourt dysoit et maintenoit que deux miles livres a tournois de rente annuelle et perpetu- `
-elle que li diz sires de Partenay li avoit promises et donnees pour cause dou dit mariage, ´ a avoir du dit seigneur de Harecourt apr ` es le `
deceps du dit seigneur de Partenay, ou cas que il auroit hoir ou hoirs males, et le cas se estoit j ˆ a offert, li devoient estre assises ` a Chas- `
Fig. C.10: Examples of page images from the Chancery dataset (CHA) and extract from
Paris, Archives Nationales, JJ 67, fol. 34v, n. 97 (1 May 1329). The GT, modernized
transcript of this extract is also shown.
Table C.20: Basic statistics and partition of the Chancery dataset (CHA). Whole-page
transcripts were not available, but the total amount of transcribed text is roughly equivalent
to 265 full pages. The “‡” mark indicates that diacritics- and case-folded word forms were
produced, and punctuation symbols are ignored.
Training Test
Acts 341 95
Lines 6 061 1 733
Running Words 135 006 37 901
Running Words ‡ 117 269 32 938
Lexicon size 19 809 8 019
Lexicon size ‡ 15 677 6 579
Alphabet Size 105 86
Alphabet Size ‡ 39 36
Query set size ‡ – 6 506330 C Text Image Document Collections and Datasets
C.8 Spanish Golden Age Theater (TSO)
The complete documentary series known as Teatro del Siglo de Oro,16 held by
the Biblioteca Nacional de Espana˜ (BNE),17 encompasses hundreds of comedy
manuscripts, written (most of them in verse) by many famous and anonymous
Spanish writers (and copyists) between the XVI and XVII centuries. A subset of
this series, comprising 36 010 pages of 328 manuscripts and referred to as TSO, was
indexed in the READ project (see footnote 5 in Chapter 10), leading to the public
PrIx search interface described in Sec. 10.3.2.18
The TSO collection contains two sub-collections, on the one hand, 182 come￾dies written by several famous Spanish writers (or copyists) such as Lope de Vega,
Calderon de la Barca ´ or Sanz de Pliegos, among others. On the other hand, there
are 146 manuscripts whose authors are unknown. Therefore, the number of differ￾ent hands and writing styles in the whole collection is very large. Moreover, the
TSO images entail many other difficulties including very variable layouts, many
abbreviations, archaic word forms and writing styles, and image quality difficulties
common to historical documents. Particularly important is the amount of images
with severe show-through and bleed-through problems, which often make it difficult
to distinguish between background and foreground. The combination of these prob￾lems makes the recognition of these documents a difficult process. All this makes this
collection very challenging, in new and/or similar ways as those faced in Chancery.
Some examples of this collection are shown in Fig.C.11.
From TSO, an experimental dataset was compiled to carry out the empirical
preparatory work prior to indexing the full TSO collection. To this end, 567 page
images of 4 manuscripts were selected and text lines were automatically detected in
these pages using the methods described in [8]. Then, detection results were manually
reviewed to check the correctness or/and amend errors. Finally, to create the TSO
dataset, existing (language-normalised, i.e., “modernized”) transcripts of the four
manuscripts were transliterated by case folding and removing most diacritics and
then manually aligned with the detected text lines. Table C.21 shows statistics and
partition of this experimental dataset, as it was used in the experiments first reported
in [28] and now summarized in Sec. 6.9.3 of this book. The table also shows the size
of the query set used for PrIx evaluation, which consisted of all the words in the test
set, excluding numbers and punctuation marks.
In addition, external text data was collected from transcripts of Lope de Vega’s
comedies provided by the ProLope research group,19 which can be used for language
model training. Details of this text-only corpus are also reported in Table C.21.
To give an idea about the challenge degree that this dataset may entail, paper [28]
reports for the best case a WER of 12.7% (case folding and ignoring punctuation
marks wit 8-gram LM).
16 Golden Age, in English.
17 http://www.bne.es
18 www.prhlt.upv.es/htr/tso
19 http://prolope.uab.catC.8 Spanish Golden Age Theater (TSO) 331
Fig. C.11: Examples of TSO images.
Table C.21: Basic statistics of the TSO experimental datasets and the corresponding
external text corpus. All the text has been transliterated, and the “†” mark indicates that
punctuation symbols are ignored.
Train-Val Test
Dataset
Pages 500 67
Lines 11 873 2 064
Running words 71 964 12 690
Running words † 57 690 10 203
Lexicon size † 7 576 2 544
Alphabet size † 60 60
External
Text
Running words † 3 325 146 –
Lexicon size † 60 930 –
Query set † – 2 522332 C Text Image Document Collections and Datasets
C.9 Parcels from Indias and Cadiz Archives: ´ Carabela (CAR)
The Carabela collection (or CAR for short) is composed of manuscripts related
with Spanish travels and naval commerce though the 15th to the 18th centuries.
Specifically it encompasses about 125 000 page images of interest for underwater
archaeology, distributed into two main parts as follow: The first part contains 30 765
pages, grouped into 328 folders20, from the Archivo General de Indias (AGI) in
Sevilla. The second part encompasses 94 546 pages, grouped into 50 folders, from
the Archivo Historico Provincial de C ´ adiz ´ (AHPC).21
These documents constitute just a tiny sample of the tens of millions of similar
documents that are preserved in these and other historical archives. The sampling
was made in the Carabela project with the main purpose of stressing the PrIx
technology with the typical difficulties expected in the large historical series of AGI,
AHPC and other similar historical archives. Examples of Carabela page images are
shown in Fig. C.12) and the full collection is available on the internet.22
The transcription of these images is generally very difficult, both for men and
machines. On the one hand, the documents do not follow any arrangement standard
and they typically have very variable writing styles with different degrees of com￾plexity. Moreover, the images frequently exhibit the typical problems of degradation
of the handwritten documents: loss of support and/or contrast, presence of dirt spots,
uneven lighting and severe bleed-through in many cases. A final, perhaps most im￾portant complication, mainly derived from the large time-span of the documents,
is that no uniform vocabulary is used across the collection. Specifically, most doc￾uments are replete of extremely abridged and tangled abbreviations and archaic or
outdated word forms. The combination of these problems makes the automatic (or
manual) recognition of these documents an extremely difficult task. Some examples
of these difficulties are shown in Fig.C.13.
Before undertaking the indexing of the whole Carabela collection,23 a dataset
was compiled. Given the expected difficulties to produce reference transcripts, the
process was split into five sequential phases or batches, B1 through B5. B1 was
manually selected and transcribed from scratch and was used only for initial training.
With the initial models, an interactive transcription tool, called Computer Assisted
Transcription of Text Images [24] (CATTI), was set up to help transcribing a large
number of random images from the collection. This allowed paleography experts to
browse these images and select for complete transcription another batch of samples,
B2, for which CATTI predictions were mostly poor. Then B2 was semi-automatically
transcribed by means of an CATTI. The process continued in this way until B4.
20 Each AGI folder contains a single record or file which may encompass a few, or a few tends,
hundreds or thousands pages.
21 The very low contrast of most AHPC images makes this sub-collection extremely difficult to
process, and also to read by expert paleographers.
22 At http://www.prhlt.upv.es/htr/carabela (see Sec. 10.3.4), with minor restrictions.
23 This was the main purpose of the Carabela project, as discussed in Sec. 10.3.4C.9 Parcels from Indias and Cadiz Archives: ´ Carabela (CAR) 333
Fig. C.12: Random examples of Carabela page images
a)
b)
c)
d)
e)
Fig. C.13: Examples of important difficulties exhibited by Carabela images: a) very
low contrast, b) extreme bleed-through, c) disparate writing styles, d) abridged and tangled
abbreviations and e) archaic or outdated word forms. In the examples from a) to d),
the written (full or abbreviated) word is “Capitan”. The words in e) are archaic forms of
“Jose”, “Cristobal” and “Jesucristo”. All these “spots” (along with many more thousands)
are found with confidence > 50% in the Carabela PrIxs, using the modern and non￾abbreviated word form queries “Capitan”, “Jose”, “Cristobal” and “Jesucristo”.
We call such a user-centered workflow “Expert-Mediated Active Learning”, be￾cause it is reminiscent to the classical active learning framework of ML. Rather than
letting the system do sample selection, the experts are empowered with interactive
tools to select the most productive samples for training.334 C Text Image Document Collections and Datasets
Finally, a test-only batch, B5, was selected by fully random sampling the complete
collection (excluding images of the previous batches). For comparison with other
datasets, B5 can be considered the test-set of Carabela and all the other batches
together can be seen as the training set.
The 650 page images that resulted from this process can be seen at a Carabela
web site.24 Table C.22 summarizes features of this dataset which was used for the
experiments first reported in [30] and summarized in Sec. 6.9.5 of this book.
Table C.22: Basic statistics of the batches used to train and test the Carabela statistical
models, along with automatic and computer-assisted transcription performance achieved.
These statistics were computed on case-folded transcripts that diacritics and punctuation
symbols were also removed.
Computer-assited incremental training Test
B1 B2 B3 B4 Total B5
Pages 179 162 100 117 558 92
Lines 4 999 5 850 2 730 3 677 17 256 3 129
Running words 38 728 45 542 20 164 39 199 143 633 21 788
Lexicon 6 314 8 657 6 160 6 895 13 984 3 837
Query set – – – – – 3 815
The final PrIx evaluation was done using batch B5, using as query set all the
3 815 words (see Table C.22) of its lexicon which were longer than one character.
To give some sort of idea about the large degree of challenge that this dataset
entails, the paper [30] reports for the best case a WER of 53.8% on the B5 partition
set (with case folding and removing diacritics and punctuation marks.).
C.10 Finnish Court Record (FCR)
The Finnish Court Records encompass thousands of manuscripts from the “Ren￾ovated District Court Records”, held by the Finnish Kansallisarkisto or National
Archives of Finland (NAF). Many of these manuscripts were scanned into double￾page images, amounting to more than one million pages. The manuscripts date from
the 18th century and consist of records of deeds, mortgages and traditional life￾annuity, among others. They were written by many hands mostly in Swedish. In
addition to its sheer size and the many hands and writing styles involved, another
main challenge of this collection stems from the heavy warping of many double-page
images (examples are shown in Figs.C.14 and C.15).
24 http://www.prhlt.upv.es/htr/carabelaDataset. This was the CATTI web platform
used to carry out the interactive transcription (but note that now CATTI is no longer active to avoid
further editing of image transcripts)C.10 Finnish Court Record (FCR) 335
Fig. C.14: Examples of Finnish Court Records (FCR) dataset images and a close-up
As discussed in Sec. 10.3.5, PrIxs were produced for about one million pages of
785 manuscripts of this collection and a corresponding search interface was made
available through the Internet.25 The preparatory empirical work for this indexing
project is reported in Sec. 6.9.6.
The dataset compiled for this work consists of a (mostly random) selection of 600
Finnish Court Recordsimages. Page layout and text line baselines were automatically
detected. Line detection was carried out with the P2Pala tool26 and (few) baselines
detection errors were manually fixed by NAF27 staff, who also accurately transcribed
the images line by line.
As in most previous datasets, ground-truth transcripts were transliterated. As in
the Passau dataset (See C.6 and [12]), case and diacritic folding was applied, and
some symbols were mapped onto ASCII equivalences to facilitate their typing on
standard keyboards. Table C.23 shows basic statistics of the resulting FCR dataset,
along with the training-test partition adopted in our experiments.
25 www.prhlt.upv.es/htr/fcr
26 https://github.com/lquirosd/P2PaLA
27 https://kansallisarkisto.fi/en336 C Text Image Document Collections and Datasets
Table C.23: Basic statistics of the Finnish Court Records (FCR) experimental dataset. All
the text has been transliterated, and the “†” mark indicates that punctuation symbols are
ignored.
Train + Val Test
Images 400 200
Lines 25 989 13 341
Running words 170 036 84 879
Running words † 147 118 73 849
Lexicon size † 20 710 13 955
Alphabet size † 59 59
Query set size † — 10 416
The query set adopted for PrIx experiments consisted of 10 416 words, derived
from the test set, excluding hyphenated-word fragments (see below) and words with
less than 2 characters.
To give an idea about the degree of challenge that this dataset may entail, the
paper [31] reports for the best case a WER of 23.0% on the test set partition of all
running words (case folding and removing punctuation marks and diacritics).
Dataset for Hyphenated-Word Experiments
Another important challenge of the Finnish Court Records is, in point of fact, the
very large proportion of words that are hyphenated. So far, this problem has been
ignored in all the other collections and datasets, where the lexicon is assumed to
be composed of all the blank-separated text tokens, including hyphenated word
fragments (HwFs). But the very large proportion of hyphenated words in FCR did
actually hinder significantly the usability – a reasonable search interface is expected
to accept entire-word queries and find these words regardless whether they are entire
or hyphenated. Fig.C.15 show typical examples of FCR images, where hyphenated
word fragments (HwFs) are highlighted.
This prompted the research first reported in [31] and summarized in Sec. 8.4
of this book. To support the empirical work of this research, the FCR dataset was
extended with the required GT information about hyphenation and HwFs. To this
end the original transcripts were carefully revised (by NAF staff) to include a special
symbol to mark a text-line ending character string when it is a (prefix) HwF (rather
than an entire word) and. Symmetrically, a character string at the beginning of a
text-line is also marked when it is a (suffix) HwF. Table C.24 shows statistics of
hyphenation and HwFs of the FCR dataset.
To allow distinguishing the relative impact of entire and hyphenated word in￾stances on the results, a smaller query set was defined by selecting, from the original
10 416 word query set (referred to as AllWords), only the 1 972 words for which
at least one instance in the test set is hyphenated. This more focused query set was
referred to as MaybeHyph.C.11 The Vorau-253 Sheet Music Manuscript and Dataset 337
Fig. C.15: Two FCRimage examples showing bounding boxes of HwFs. Colors correspond
to relevance probabilities of PrIx spots for these HwFs computed as described in Sec. 8.4
(red: low; green: high).
Table C.24: Statistics of the hyphenated lines and hyphenated word (prefix or suffix)
fragments (HwFs) of the FCR dataset. Proportions are relative to the corresponding total
numbers of lines or words in the dataset (see Table C.23). All the text was transliterated
and the punctuation marks ignored.
Dataset partition: Train-Val Test Proportion
Lines with HwFs 10 973 5 609 42%
Running HwFs 13 081 6 589 9%
HwF Lexicon 4 091 2 677 20%
MaybeHyph query set size — 1 972 19%
C.11 The Vorau-253 Sheet Music Manuscript and Dataset
The music manuscript, referred to as Cod. 253 of the Vorau Abbey Library28 (or
just Vorau-253), was written in German Gothic notation around year 1450. It was
provided to us by the musicology department of the Austrian Academy of Sciences.29
Vorau-253 contains 492 early sheet music page images, each containing eight
staves. Typically, staves are tetragrams (i.e., written in four lines), but there are also
a few hundred pentagram staves. Fig.C.16 shows example images of Vorau-253.
The full set of images can be seen though the PrIx search platform we developed for
this small collection.30
28 https://www.schloesserstrasse.com/en/vorau-abbey
29 https://www.oeaw.ac.at/acdh/research-units/musicology
30 www.prhlt.upv.es/htr/music338 C Text Image Document Collections and Datasets
Fig. C.16: Examples of page images from VORAU-253 sheet music manuscript.
From this image collection, a small number of staves (or parts of staves) had
been manually transcribed (by Austrian Academy of Sciences musicologists) into the
so-called “volpiano notation”. Later we converted these transcripts into a geometry￾wise notation. It specifies a note or music symbol by its vertical position in the
tetragram in terms of a letter (“L” for “line” and “S” for interline “space”) and a
number indicating the vertical position of the line or space it lays on. Fig.C.17
illustrates this notation and further details, including the conversion from volpiano,
can be consulted in [4].31 Notice that this notation (and volpiano alike) only cater
for the pitch of a note. But duration or time annotations were not needed because no
duration or timing was explicitly specified in this kind of early sheet music writing.
In addition to plain notes, the sheet music images contained other symbols, such
as the “flat” (alteration) and two clefs, c and f, which can also lay in any line of a staff.
In total 19 different symbols or “music characters” were needed. The statistics of the
resulting dataset and its training/test partition used in the experiments is presented
in Table C.25.
In the PrIx music symbol sequence search experiments first reported in [4] and
summarized in Sec. 8.2, two query sets are used: a trivial one, encompassing all the
19 symbols for single-symbol search, and a set of 615 music symbol sequences of
length ranging form 3 to 15 symbols. See [4] or Sec. 8.2 for details.
31 Recently both the music and the lyrics of the whole Vorau-253 manuscript have been transcribed
at the PRHLT research center [33]. Based on the new GT a new PrIx search demonstrator has been
developed which supports mixed music/lyrics queries: www.prhlt.upv.es/htr/musicaC.12 A Dataset for Multi-page Handwritten Deeds Classification 339
(a)
(b)
Fig. C.17: (a) Geometry-wise annotation of music symbols: L and S denote a symbol
on or between staff lines, respectively. (b) A portion of a staff and the corresponding
transcript into geometry-wise notation. The clef is annotated with a letter (c or f) and a
number indicating the line it lays on.
Table C.25: Statistics of the Vorau-253 dataset and partitions used in the experiments.
Train Test Total
(Partial) staves 1 000 97 1 097
Running music symbols 13 066 1 086 14 152
Different symbols (lexicon) 19 15 19
Single-symbol query set size 19 — —
Symbol-sequence query set size 615 — —
C.12 A Dataset for Multi-page Handwritten Deeds Classification
In contrast with previous datasets, the one considered here is not aimed at direct PrIx
experiments. It was compiled to study text-content-based multi image document
classification (in this case based on PrIx, as discussed in Sec. 9.5).
It is a small part of the 50 protocol books (or bundles) included in the Spanish
Archivo Historico Provincial de C ´ adiz ´ (AHPC) folder of the Carabela collection32
presented in Sec.C.9 of this appendix. Specifically, only two of these protocol
books are selected: JMBD 4949 and JMBD 4950, dated 1723-1724. Fig.C.18 shows
examples of page images of these books.
The books contain notarial deeds, which constitute precious historical sources
of important information about the relationships among persons, social classes, and
many other aspects of the evolution of society. One of the primary tasks for the study
of these deeds is their typological classification. The set of deed classes which can be
observed in collections as vast as those held by the AHPC is obviously “unbounded”,
because typological classes are expected to evolve over time. But for a short period
such as that of JMBD 4949 and JMBD 4950 it can be expected to find just a few
dozens different deed classes (see below).
32 See http://www.prhlt.upv.es/htr/carabela340 C Text Image Document Collections and Datasets
Fig. C.18: Examples of page images from JMDB 4949 and JMBD 4950.
Note that no typical GT annotations (such as text lines or transcripts) were avail￾able for these manuscripts. As explained below, only coarse-grained GT aimed at
bundle segmentation and deed classification were produced to compile this dataset.
The bundles were manually divided by experts into sequential segments or sec￾tions, each corresponding to a single deed, which was then annotated with a class
label. Each deed may contain from two to dozens of pages, and separating these
deeds is not straightforward. A first section of about 50 pages, which form a kind
of table of contents, was also identified in each book, but these sections are not
considered so far part of the dataset.
The experts found 95 deeds in JMBD 4949 and 260 in JMBD 4950, a total of
555 deeds, belonging to about 41 different types or classes. However, the classes
of some deeds were not clear and, for many of the clearly identified classes, only
very few deeds were available. To allow the classification results to be sufficiently
reliable, only those classes having at least one deed in each book and six deeds in
total were taken into account. This way, 498 deeds were retained from 12 classes
considered sufficiently represented and all the other, belonging to 29 unclear or
poorly represented classes, were collectively deemed to belong to a special “class”
called Reject (RJ).
The twelve well-represented classes are:Power of Attorney (PA), Letter of Payment
(LP), Debenture (DB), Lease (LE), Testament (TE), Sale (SA), Risk (RI), Census
(CS), Deposit (DP), Statement (ST), Cession (CN) and Treaty of Fact (TF). Fig. C.19
shows an example of four-page JMDB deed of the class Risk (RI). Details of the
resulting dataset are shown in Table C.26 and
The Closed Set machine learning task consists in training a model to classify a
deed known to belong to one of the 퐶 = 12 proper classes into one of these same
classes. The corresponding Open Set task is to also let the system reject samples
(deeds) from the remaining 29 classes. That is, 퐶˜ = 12 + 29 = 41 and the proportion
of known classes is 29.3%. Both tasks were considered in the experiments first
reported in [18] and summarized in Sec. 9.5 of this book.C.12 A Dataset for Multi-page Handwritten Deeds Classification 341
No training / test partition was specified for this dataset and the commonly adopted
empirical protocol is Leaving One (deed) Out.
Fig. C.19: Example of a four-page deed of class Risk (RI) from the JMBB-4949 book
A four-page deed of class Risk (RI) from the JMBB-4949 book of the AHPC.342 C Text Image Document Collections and Datasets
Table C.26: Number of documents and page images for JMBD 4949 and JMBD 4950:
per class, per document & class, and totals.
Class Deeds Pages
ID’s Avg Min Max St-dev Total
PA 240 3.3 2 24 3.5 803
LP 72 4.8 2 30 5.4 345
DB 44 4.8 2 32 5.6 212
LE 32 4.8 2 16 2.6 152
TE 29 8.6 4 48 9.4 248
SA 21 22.9 4 122 29.8 480
RI 17 4.0 4 4 0.0 68
CS 12 11.5 2 26 9.0 138
DP 10 3.8 2 8 1.9 38
ST 9 2.4 2 4 0.8 22
CN 6 5.3 2 14 3.9 32
TF 6 5.3 4 8 1.9 32
Reject 57 9.2 2 70 12.2 526
Total 555 5.6 2 122 9.2 3096References 343
References
1. Almazan, J., Gordo, A., Forn ´ es, A., Valveny, E.: Word Spotting and Recognition with Em- ´
bedded Attributes. IEEE Transactions on Pattern Analysis and Machine Intelligence 36(12),
2552–2566 (2014). DOI 10.1109/TPAMI.2014.2339814
2. Bertolami, R., Bunke, H.: Hidden Markov model-based ensemble methods for offline hand￾written text line recognition. Pattern Recognition 41(11), 3452 – 3460 (2008). DOI
https://doi.org/10.1016/j.patcog.2008.04.003
3. Bluche, T., Hamel, S., Kermorvant, C., Puigcerver, J., Stutzmann, D., Toselli, A.H., Vidal,
E.: Preparatory KWS Experiments for Large-Scale Indexing of a Vast Medieval Manuscript
Collection in the HIMANIS Project. In: Proc. of 14th ICDAR (2017)
4. Calvo-Zaragoza, J., Toselli, A.H., Vidal, E., Sanchez, J.A.: Music symbol sequence indexing ´
in medieval plainchant manuscripts. In: 2019 International Conference on Document Analysis
and Recognition (ICDAR), pp. 882–887. IEEE (2019)
5. Doetsch, P., Kozielski, M., Ney, H.: Fast and Robust Training of Recurrent Neural Networks
for Offline Handwriting Recognition. In: 2014 14th International Conference on Frontiers in
Handwriting Recognition, pp. 279–284 (2014). DOI 10.1109/ICFHR.2014.54
6. Fischer, A., Keller, A., Frinken, V., Bunke, H.: Lexicon-free handwritten word spotting using
character HMMs. Pattern Recognition Letters 33(7), 934 – 942 (2012). DOI 10.1016/j.patrec
.2011.09.009. Special Issue on Awards from ICPR 2010
7. Frinken, V., Fischer, A., Manmatha, R., Bunke, H.: A Novel Word Spotting Method Based on
Recurrent Neural Networks. IEEE Transactions on Pattern Analysis and Machine Intelligence
34(2), 211–224 (2012). DOI 10.1109/TPAMI.2011.113
8. Gr ¨uning, T., Leifert, G., Strauss, T., Labahn, R.: A Robust and Binarization-Free Approach for
Text Line Detection in Historical Documents. In: 2017 14th IAPR International Conference
on Document Analysis and Recognition (ICDAR), vol. 01, pp. 236–241 (2017). DOI 10.110
9/ICDAR.2017.47
9. Guerin, P., Celier, L.: Recueil des documents concernant le Poitou contenus dans les registres ´
de la chancellerie de France. Archives historiques du Poitou. Oudin, Poitiers (1881-1958)
10. Holmes, J., Johnson, G., Vine, B.: Guide to the Wellington corpus of spoken New Zealand En￾glish. School of Linguistics and Applied Language Studies, Victoria University of Wellington
Wellington (1998)
11. Johansson, S., Leech, G., Goodluck, H.: Manual of Information to Accompany the Lancaster￾Olso/Bergen Corpus of British English, for Use with Digital Computers. Tech. rep., Department
of English, University of Oslo (1978)
12. Lang, E., Puigcerver, J., Toselli, A.H., Vidal, E.: Probabilistic indexing and search for in￾formation extraction on handwritten german parish records. In: 2018 16th International
Conference on Frontiers in Handwriting Recognition (ICFHR), pp. 44–49 (2018). DOI
10.1109/ICFHR-2018.2018.00017
13. Lavrenko, V., Rath, T.M., Manmatha, R.: Holistic word recognition for handwritten historical
documents. In: First International Workshop on Document Image Analysis for Libraries, 2004.
Proceedings., pp. 278–287 (2004). DOI 10.1109/DIAL.2004.1263256
14. Likforman-Sulem, L., Zahour, A., Taconet, B.: Text line segmentation of historical documents:
a survey. International Journal of Document Analysis and Recognition (IJDAR) 9(2), 123–138
(2007). DOI 10.1007/s10032-006-0023-z
15. Marti, U.V., Bunke, H.: The IAM-database: an English sentence database for offline handwrit￾ing recognition. International Journal on Document Analysis and Recognition 5(1), 39–46
(2002). DOI 10.1007/s100320200071
16. Nelson, F.W., Kuc¸era, H.: Manual of information to accompany a standard corpus of present￾day edited American English, for use with digital computers. Tech. rep., Department of
Linguistics, Brown University (1964)
17. Pratikakis, I., Zagoris, K., Gatos, B., Louloudis, G., Stamatopoulos, N.: ICFHR 2014 Competi￾tion on Handwritten Keyword Spotting (H-KWS 2014). In: 2014 14th International Conference
on Frontiers in Handwriting Recognition, pp. 814–819 (2014). DOI 10.1109/ICFHR.2014.142344 C Text Image Document Collections and Datasets
18. Prieto, J.R., Flores, J.J., Vidal, E., Toselli, A.H.: Open set classification of untranscribed
handwritten text image documents. Pattern Recognition Letters 172, 113–120 (2023)
19. Puigcerver, J.: Are Multidimensional Recurrent Layers Really Necessary for Handwritten
Text Recognition? In: 2017 14th IAPR International Conference on Document Analysis and
Recognition (ICDAR), vol. 01, pp. 67–72 (2017). DOI 10.1109/ICDAR.2017.20
20. Puigcerver, J.: A probabilistic formulation of keyword spotting. Ph.D. thesis, Universitat
Politecnica de Val ` encia (2018) `
21. Puigcerver, J., Toselli, A.H., Vidal, E.: Probabilistic interpretation and improvements to the
HMM-filler for handwritten keyword spotting. In: 13th Int. Conf. on Document Analysis and
Recognition (ICDAR), pp. 731–735 (2015). DOI 10.1109/ICDAR.2015.7333858
22. Rath, T.M., Manmatha, R.: Word Spotting for Historical Documents. Int. Journal of Document
Analysis and Recognition (IJDAR) 9(2), 139–152 (2007). DOI 10.1007/s10032-006-0027-8
23. Rath, T.M., Manmatha, R., Lavrenko, V.: A Search Engine for Historical Manuscript Images.
In: Proceedings of the 27th Annual International ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ’04, pp. 369–376. ACM, New York, NY, USA
(2004). DOI 10.1145/1008992.1009056
24. Romero, V., Toselli, A.H., Vidal, E.: Multimodal Interactive Handwritten Text Transcription.
Perception and Artif. Intell. (MPAI). World Scientific, (2012)
25. Toselli, A.H., Leiva, L.A., Bordes-Cabrera, I., Hernandez-Tornero, C., Bosch, V., Vidal, E.: ´
Transcribing a 17th-century botanical manuscript: Longitudinal evaluation of document layout
detection and interactive transcription. Digital Scholarship in the Humanities 33(1), 173–202
(2018). DOI 10.1093/llc/fqw064. URL http://dx.doi.org/10.1093/llc/fqw064
26. Toselli, A.H., Puigcerver, J., Vidal, E.: Context-aware lattice based filler approach for key word
spotting in handwritten documents. In: 2015 13th Int. Conference on Document Analysis and
Recognition (ICDAR), pp. 736–740 (2015). DOI 10.1109/ICDAR.2015.7333859
27. Toselli, A.H., Puigcerver, J., Vidal, E.: Two Methods to Improve Confidence Scores for Lexicon￾Free Word Spotting in Handwritten Text. In: 2016 15th International Conference on Frontiers
in Handwriting Recognition (ICFHR), pp. 349–354 (2016)
28. Toselli, A.H., Romero, V., Sanchez, J.A., Vidal, E.: Making two vast historical manuscript col- ´
lections searchable and extracting meaningful textual features through large-scale probabilistic
indexing. In: Int. Conf. on Document Anal. and Recogn. (ICDAR), pp. 108–113. IEEE (2019)
29. Toselli, A.H., Vidal, E.: Fast HMM-Filler Approach for Key Word Spotting in Handwritten
Documents. In: 2013 12th International Conference on Document Analysis and Recognition,
pp. 501–505 (2013). DOI 10.1109/ICDAR.2013.106
30. Vidal, E., Romero, V., Toselli, A.H., Sanchez, J.A., Bosch, V., Quir ´ os, L., Bened ´ ´ı, J.M.,
Prieto, J.R., Pastor, M., Casacuberta, F., et al.: The carabela project and manuscript collection:
large-scale probabilistic indexing and content-based classification. In: 2020 17th International
Conference on Frontiers in Handwriting Recognition (ICFHR), pp. 85–90. IEEE (2020)
31. Vidal, E., Toselli, A.H.: Probabilistic indexing and search for hyphenated words. In: Document
Analysis and Recognition–ICDAR 2021: 16th International Conference, Proceedings, Part II
16, pp. 426–442. Springer (2021)
32. Vidal, E., Toselli, A.H., Puigcerver, J.: High performance Query-by-Example keyword spotting
using Query-by-String techniques. In: 2015 13th International Conference on Document
Analysis and Recognition (ICDAR), pp. 741–745 (2015). DOI 10.1109/ICDAR.2015.7333860
33. Villarreal-Ruiz, M., Sanchez, J.A.: Synchronous recognition of music images using coupled ´
n-gram models. In: In proceedings of the 23rd ACM Symposium on Document Engineering
(DocEng 2023) (2023)
34. Voigtlaender, P., Doetsch, P., Ney, H.: Handwriting Recognition with Large Multidimensional
Long Short- Term Memory Recurrent Neural Networks. In: 15th Int. Conf. on Frontiers in
Handwriting Recognition (ICFHR), pp. 228–233 (2016). DOI 10.1109/ICFHR.2016.0052
