Springer Monographs in Mathematics
Piernicola Bettiol
Richard B. Vinter
Principles 
of Dynamic 
OptimizationSpringer Monographs in Mathematics 
Editors-in-Chief 
Minhyong Kim, School of Mathematics, Korea Institute for Advanced Study, Seoul, 
South Korea 
International Centre for Mathematical Sciences, Edinburgh, UK 
Katrin Wendland, School of Mathematics, Trinity College Dublin, Dublin, Ireland 
Series Editors 
Sheldon Axler, Department of Mathematics, San Francisco State University, San 
Francisco, CA, USA 
Maria Chudnovsky, Department of Mathematics, Princeton University, Princeton, 
NJ, USA 
Tadahisa Funaki, Department of Mathematics, University of Tokyo, Tokyo, Japan 
Isabelle Gallagher, Département de Mathématiques et Applications, Ecole Normale 
Supérieure, Paris, France 
Sinan Güntürk, Courant Institute of Mathematical Sciences, New York University, 
New York, NY, USA 
Claude Le Bris, CERMICS, Ecole des Ponts ParisTech, Marne la Vallée, France 
Pascal Massart, Département de Mathématiques, Université de Paris-Sud, Orsay, 
France 
Alberto A. Pinto, Department of Mathematics, University of Porto, Porto, Portugal 
Gabriella Pinzari, Department of Mathematics, University of Padova, Padova, Italy 
Ken Ribet, Department of Mathematics, University of California, Berkeley, CA, 
USA 
René Schilling, Institute for Mathematical Stochastics, Technical University Dres￾den, Dresden, Germany 
Panagiotis Souganidis, Department of Mathematics, University of Chicago, 
Chicago, IL, USA 
Endre Süli, Mathematical Institute, University of Oxford, Oxford, UK 
Shmuel Weinberger, Department of Mathematics, University of Chicago, Chicago, 
IL, USA 
Boris Zilber, Mathematical Institute, University of Oxford, Oxford, UKThis series publishes advanced monographs giving well-written presentations of the 
“state-of-the-art” in fields of mathematical research that have acquired the maturity 
needed for such a treatment. They are sufficiently self-contained to be accessible to 
more than just the intimate specialists of the subject, and sufficiently comprehensive 
to remain valuable references for many years. Besides the current state of knowledge 
in its field, an SMM volume should ideally describe its relevance to and interaction 
with neighbouring fields of mathematics, and give pointers to future directions of 
research.Piernicola Bettiol • Richard B. Vinter 
Principles of Dynamic 
OptimizationPiernicola Bettiol 
Department of Mathematics 
University of Brest 
Brest, France 
Richard B. Vinter 
Electrical & Electronic Engineering 
Imperial College London 
London, UK 
ISSN 1439-7382 ISSN 2196-9922 (electronic) 
Springer Monographs in Mathematics 
ISBN 978-3-031-50088-6 ISBN 978-3-031-50089-3 (eBook) 
https://doi.org/10.1007/978-3-031-50089-3 
Mathematics Subject Classification: 49-02, 49K15, 49K21 
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland 
AG 2024 
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether 
the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse 
of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and 
transmission or information storage and retrieval, electronic adaptation, computer software, or by similar 
or dissimilar methodology now known or hereafter developed. 
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication 
does not imply, even in the absence of a specific statement, that such names are exempt from the relevant 
protective laws and regulations and therefore free for general use. 
The publisher, the authors, and the editors are safe to assume that the advice and information in this book 
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or 
the editors give a warranty, expressed or implied, with respect to the material contained herein or for any 
errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional 
claims in published maps and institutional affiliations. 
This Springer imprint is published by the registered company Springer Nature Switzerland AG 
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland 
If disposing of this product, please recycle the paper.To Francis Clarke, il miglior fabbroPreface 
What control strategy will transfer a space vehicle from one circular orbit to 
another in least time or, alternatively, with minimum fuel consumption? What 
should be the strategy for harvesting a renewable resource (a fish population, say) 
to maximize financial returns while satisfying sustainability constraints? In chemo￾immunotherapy for cancer, what treatment regime (concentration and frequency of 
cytotoxic drug doses) will minimize the tumour cell population while maintaining 
the blood cell population above a critical level? Mitigation strategies are available 
to counter an epidemic, including vaccination, livestock culling and host removal; 
how should we deploy these strategies while minimizing the social and economic 
costs involved? How should a batch distillation column be operated to maximize the 
yield, subject to specified constraints on product purity? 
There are a number of common features in these questions. First, they all concern 
phenomena where the relevant ‘state of nature’ (relating, for example, to the position 
of a space vehicle or the size of a diseased population) is dynamic, in the sense that 
it evolves with time. Second, the evolution of the state of nature, or state as we shall 
simply call it, is affected by the choice of a control strategy. Third, we can attach a 
cost to a control strategy and the evolving state to which it gives rise. The underlying 
problem is to choose a control strategy that minimizes the cost. 
In certain cases, problems in the classical calculus of variations (‘minimization 
of an integral functional over arcs and their derivatives’) match this description. 
Here, the independent variable is interpreted as time, the ‘state’ is the value of 
the arc at the current time and the control its rate of change. But techniques for 
their solution provided by this earlier theory fail to take account of the dynamic 
constraints that are so often encountered today, in engineering, applied science and 
economics. Here, by ‘dynamic constraints’ we mean the mathematical relations 
governing future evolution of the state, which will depend on the control strategy. 
Dynamic optimization is the name given to the systematic study of optimization 
problems with dynamic constraints. General study of optimization problems with 
dynamic constraints dates from the late 1950s, which saw several crucial advances, 
one conceptual and two technical. The conceptual advance, due by L. S. Pontryagin 
et al., was the realization that optimization problems where the dynamic constraint
viiviii Preface
took the form of a controlled differential equation covered a wide range of 
engineering control problems involving mechanical systems such as space vehicles 
and, furthermore, was amenable to analysis. As for the two technical advances, one 
was Pontryagin’s maximum principle, a set of necessary conditions for a control 
strategy to be optimal. The other was dynamic programming, a procedure initiated 
by R. Bellman, which reduces the search for an optimal strategy to finding the 
solution to a partial differential equation (the Hamilton Jacobi equation). 
‘Dynamic optimization’ is synonymous with ‘optimal control’. We have chosen 
the nomenclature dynamic optimization in this book, to convey the idea that 
optimization problems with general dynamic constraints merit study in their own 
right and that the field has widespread application, within and beyond engineering. 
We seek then to avoid the specificity of ‘optimal control’, a name introduced to 
describe a branch of control engineering, in which the control design objectives 
are expressed in terms minimizing a cost, rather than, say, in terms of stability and 
robustness requirements. 
From the mid-1970s, it became apparent that progress in the study of dynamic 
optimization problems was being impeded by a lack of suitable analytic tools 
for investigating local properties of functions which are nonsmooth, i.e. not 
differentiable in the traditional sense. Nonsmooth functions were encountered at first 
attempts to put Dynamic Programming on a rigorous footing, specifically attempts 
to relate value functions and solutions to the Hamilton Jacobi equation. It was 
found that, for many dynamic optimization problems of interest, the only ‘solutions’ 
to the Hamilton Jacobi equation have discontinuous derivatives. How should we 
interpret these solutions? New ideas were required to answer this question since the 
Hamilton Jacobi equation of dynamic optimization is a nonlinear partial differential 
equation for which traditional interpretations of generalized solutions, based on the 
distributions they define, are inadequate. 
Nonsmooth functions surfaced once again when efforts were made to extend 
the applicability of necessary conditions such as the maximum principle. A notable 
feature of the maximum principle (and one which distinguishes it from necessary 
conditions derivable using classical techniques) is that it can take account of 
pathwise constraints on values of the control functions. For some practical problems, 
the constraints on values of the control depend on the vector state variable. In flight 
mechanics, for example, the maximum and minimum thrust of a jet engine (a control 
variable) will depend on the altitude (a component of the state vector). The maxi￾mum principle in its original form is not, in general, valid for problems involving 
state-dependent control constraints. One way to derive necessary conditions for 
these problems, and others not covered by the maximum principle, is to reformulate 
them as generalized problems in the calculus of variations, the cost integrands for 
which include penalty terms to take account of the constraints. The reformulation 
comes at a price, however. To ensure equivalence with the original problems, it is 
necessary to employ penalty terms with discontinuous derivatives. So the route to 
necessary conditions via generalized problems in the calculus of variations can be 
followed only if we know how to adapt traditional necessary conditions to allow for 
nonsmooth cost integrands.Preface ix
Two important breakthroughs occurred in the 1970s. One was the end product 
of a long quest for effective, local descriptions of ‘non-smooth’ functions, based 
on generalizations of the concept of ‘subdifferentials’ of convex functions, to 
larger function classes. F. H. Clarke’s theory of generalized gradients, by achieving 
this goal, launched the field of nonsmooth analysis and provided a bridge to 
necessary conditions of optimality for nonsmooth variational problems (and in 
particular dynamic optimization problems reformulated as generalized problems in 
the calculus of variations). The other breakthrough, a somewhat later development, 
was the concept of viscosity solutions, due to M. G. Crandall and P.-L. Lions, which 
provides a framework for proving existence and uniqueness of generalized solutions 
to Hamilton Jacobi equations arising in dynamic optimization. 
Nonsmooth analysis and viscosity methods were introduced to overcome obsta￾cles in dynamic optimization. But they have come to have a significant impact 
on nonlinear analysis as a whole. Nonsmooth analysis provides an important new 
perspective: useful properties of functions, even differentiable functions, can be 
proved by examining related nondifferentiable functions, in the same way that 
trigonometric identities relating to real numbers can sometimes simply be derived 
by a temporary excursion into the field of complex numbers. Viscosity methods, 
on the other hand, provide a fruitful approach to studying generalized solutions 
to broad classes of nonlinear partial differential equations which extend beyond 
Hamilton Jacobi equations of dynamic optimization and their approximation for 
computational purposes. The calculus of variations (in its modern guise as dynamic 
optimization) continues to uphold a long tradition then, as a stimulus to research in 
other areas of mathematics. 
The main purpose of this book is to bring together as a single comprehen￾sive, up-to-date publication major advances in the theory dynamic optimization, 
with emphasis on those accomplished through the use of nonsmooth analytical 
techniques. Necessary conditions receive special attention. But other topics are 
covered as well. Material on the important topic of minimizer regularity provides 
a showcase for the application of nonsmooth necessary conditions to derive 
qualitative information about solutions to variational problems. The chapter on 
dynamic programming stands a little apart from other sections of the book, as it 
is complementary to mainstream research in the area based on viscosity methods 
(and which in any case is the subject matter of a number of substantial expository 
texts). Instead we concentrate on aspects of dynamic programming well matched 
to the analytic techniques of this book, notably the characterization (in terms 
of the Hamilton Jacobi equation) of extended-valued value functions associated 
with problems having endpoint and state constraints, inverse verification theorems, 
sensitivity relationships and links with the maximum principle. 
A subsidiary purpose is to meet the needs of readers with little prior exposure to 
modern dynamic optimization who seek quick answers to the questions: what are the 
main results, what were the deficiencies of the ‘classical’ theory and to what extent 
have they been overcome? Chapter 1 provides, for their benefit, a lengthy overview, 
in which analytical details are suppressed and the emphasis is placed instead on 
communicating the underlying ideas.x Preface
To render this book self-contained, preparatory chapters are included on nons￾mooth analysis, measurable multifunctions and differential inclusions. Much of this 
material is implicit in the books of R. T. Rockafellar and J. B. Wets [177] and Clarke 
et al. [85], and of J.-P. Aubin and H. Frankowska [14]. It is expected, however, that 
readers, whose main interest is in optimization rather than in broader application 
areas of nonsmooth analysis which require additional techniques, will find these 
chapters helpful, because of the strong focus on topics relevant to optimization. 
Dynamic optimization is a large field and the choice of material for this is 
necessary selective. The techniques used here to derive necessary conditions of 
optimality are, for the most part, within a tradition of research pioneered and 
developed by Clarke, Ioffe, Loewen, Mordukhovich, Rockafellar, Vinter and others, 
based on perturbation, elimination of constraints and passage to the limit. The 
necessary conditions are ‘state of the art’, as far as this tradition is concerned. 
Alternative approaches, based on set separation ideas, also make an appearance, 
but principally for comparison purposes and historical perspective. We do not enter 
into the topic of higher order necessary conditions nor computational aspects of 
dynamic optimization. 
This book is similar in structure and content to the 2000 book Optimal Control 
[194]. It brings up to date this earlier publication by, in many instances, providing 
new, simpler proofs of key theorems, where these have become available, and by 
broadening the applicability of the theory. We provide, for the first time in book 
form, recent improvements to necessary conditions of optimality for problems 
for dynamic optimization problems involving a differential inclusion constraint, 
referred to as the Ioffe refinement. It draws on recent research developments, 
unavailable at the time of the earlier publication, to provide a thorough discussion 
and analysis of necessary conditions in the form of Clarke’s Hamiltonian inclusion. 
The book includes new material on necessary conditions for problems with mixed 
state/control constraints and on problems with free end-times, drawing on latest 
research in these areas. Also included is a new framework for dynamic programming 
treating dynamic constraints with discontinuous time dependence. 
Brest, France Piernicola Bettiol 
London, UK Richard B. VinterAcknowledgements 
We wish to thank our colleagues at Imperial College, including Martin Clarke, 
David Angeli and Alessandro Astolfi, and at University of Brest, including Rainer 
Buckdahn, Chloé Jimenez, Vuk Milisic, Marc Quincampoix and Miloud Sadkane 
for creating working environments in which writing this book has been possible. 
Many people have influenced our thinking on the contents and presentation of this 
book, which is indeed the product of many years of collaboration with colleagues 
and graduate students in the control systems and applied analysis communities. We 
make special mention of the insights we have gained from Francis Clarke, whose 
pioneering work in nonsmooth analysis, perturbation methods and applications 
in nonsmooth optimization laid the foundation for the work reported here. An 
incomplete list of our collaborators is Zvi Artstein, Aram Arutyunov, Julien Bernis, 
Andrea Boccia, Frédéric Bonnans, Bernard Bonnard, Alberto Bressan, Arrigo 
Cellina, Giovanni Colombo, Maria do Rosário de Pinho, Asen Dontchev, Paola 
Falugi, Margarida Ferreira, Fernando Fontes, Helene Frankowska, Alexander Ioffe, 
Nathalie Khalil, Yuri Ledyaev, Philip Loewen, Carlo Mariconda, Helmut Maurer, 
Monica Motta, Michele Palladino, Fernando Lobo Pereiera, Franco Rampazzo, 
Alain Rapaport, Ralph Tyrrell Rockafellar, Javier Rosenblueth, Jérémy Rouot, 
Geraldo Silva, Peter Wolenski, and Harry Zheng. 
Finally, and most importantly, 
‘I express my profound gratitude to my wife Giorgia who has provided unwa￾vering and irreplaceable support in my projects, including this book. She and my 
wonderful children (Eloïse, Gabriele and Leonardo) with their presence, enthusiasm 
and understanding made it possible for me to dedicate time and effort to bring this 
book to fruition. I would like to extend my heartfelt appreciation and recognition to 
Richard; working alongside him has been an enlightening and inspiring experience.’ 
(Piernicola)
xixii Acknowledgements
‘My wife, Donna, and children, Magdalena, Becky and Hannah, have given me 
unconditional support and encouragement in everything I have wanted to do. This 
book was no exception. Writing a book like this takes time; I wish to express my 
deepest thanks to them, especially to Donna for giving me that time, generously and 
with such good grace. I add my thanks to Piernicola, both for his friendship and for 
his indispensable part in our fruitful mathematical collaboration.’ (Richard)Contents 
1 Overview..................................................................... 1 
1.1 Dynamic Optimization.............................................. 2 
1.2 The Calculus of Variations ......................................... 9 
1.3 Existence of Minimizers and Tonelli’s Direct Method ............ 22 
1.4 Sufficient Conditions and the Hamilton Jacobi Equation ......... 25 
1.5 The Maximum Principle ............................................ 30 
1.6 Dynamic Programming ............................................. 36 
1.7 Nonsmoothness ..................................................... 41 
1.8 Nonsmooth Analysis................................................ 45 
1.9 Nonsmooth Dynamic Optimization ................................ 59 
1.10 Epilogue ............................................................. 62 
1.11 Appendix: Proof of the Classical Maximum Principle ............ 66 
1.12 Exercises ............................................................ 86 
1.13 Notes for Chapter 1 ................................................. 89 
2 Set Convergence, Measurability and Existence of Minimizers ........ 91 
2.1 Introduction ......................................................... 91 
2.2 Convergence of Sets and Continuity of Multifunctions ........... 92 
2.3 Measurable Multifunctions ......................................... 95 
2.4 The Generalized Bolza Problem ................................... 107 
2.5 Exercises ............................................................ 115 
2.6 Notes for Chapter 2 ................................................. 116 
3 Variational Principles ...................................................... 119 
3.1 Introduction ......................................................... 120 
3.2 Exact Penalization .................................................. 121 
3.3 Ekeland’s Theorem ................................................. 123 
3.4 Quadratic Inf Convolution .......................................... 127 
3.5 Variational Principles with Smooth Perturbation Terms .......... 132 
3.6 Mini-Max Theorems................................................ 134 
3.7 Exercises ............................................................ 145 
3.8 Notes for Chapter 3 ................................................. 146
xiiixiv Contents
4 Nonsmooth Analysis........................................................ 149 
4.1 Introduction ......................................................... 150 
4.2 Normal Cones ....................................................... 151 
4.3 Subdifferentials ..................................................... 156 
4.4 Difference Quotient Representations .............................. 162 
4.5 Nonsmooth Mean Value Inequalities .............................. 167 
4.6 Characterization of Limiting Subgradients ........................ 173 
4.7 Subgradients of Lipschitz Continuous Functions ................. 177 
4.8 The Distance Function .............................................. 184 
4.9 Criteria for Lipschitz Continuity ................................... 190 
4.10 Relations Between Normal and Tangent Cones ................... 194 
4.11 Interior of Clarke’s Tangent Cone .................................. 201 
4.12 Appendix: Proximal Analysis in Hilbert Space ................... 203 
4.13 Exercises ............................................................ 213 
4.14 Notes for Chapters 4 and 5 ......................................... 214 
5 Subdifferential Calculus ................................................... 217 
5.1 Introduction ......................................................... 217 
5.2 A Marginal Function Principle ..................................... 220 
5.3 Partial Limiting Subgradients ...................................... 224 
5.4 A Sum Rule ......................................................... 226 
5.5 A Nonsmooth Chain Rule .......................................... 230 
5.6 Lagrange Multiplier Rules.......................................... 232 
5.7 Max Rule for an Infinite Family of Functions ..................... 236 
5.8 Exercises ............................................................ 238 
5.9 Notes for Chapter 5 ................................................. 240 
6 Differential Inclusions...................................................... 241 
6.1 Introduction ......................................................... 242 
6.2 Existence and Estimation of F Trajectories ....................... 243 
6.3 Perturbed Differential Inclusions................................... 256 
6.4 Existence of Minimizing F Trajectories........................... 261 
6.5 Relaxation ........................................................... 264 
6.6 Estimates on Trajectories Confined to a Closed Subset ........... 271 
6.7 Exercises ............................................................ 292 
6.8 Notes for Chapter 6 ................................................. 293 
7 The Maximum Principle ................................................... 295 
7.1 Introduction ......................................................... 295 
7.2 Clarke’s Nonsmooth Maximum Principle ......................... 297 
7.3 A Preliminary Maximum Principle, for Dynamic 
Optimization Problems with no End-Point Constraints........... 303 
7.4 Proof of Theorem 7.2.1 ............................................. 319 
7.5 Exercises ............................................................ 323 
7.6 Notes for Chapter 7 ................................................. 327Contents xv
8 The Generalized Euler-Lagrange and Hamiltonian Inclusion 
Conditions ................................................................... 333 
8.1 Introduction ......................................................... 334 
8.2 Pseudo Lipschitz Continuity ....................................... 338 
8.3 Unbounded Differential Inclusions ................................ 341 
8.4 The Generalized Euler Lagrange Condition ....................... 342 
8.5 Special Cases........................................................ 347 
8.6 Proof of Theorem 8.4.3 ............................................. 350 
8.7 The Hamiltonian Inclusion for Convex Velocity Sets............. 377 
8.8 The Hamiltonian Inclusion for Non-convex Velocity Sets........ 381 
8.9 Discussion and a Counter-Example ................................ 386 
8.10 Appendix: Dualization of the Euler Lagrange Inclusion .......... 391 
8.11 Exercises ............................................................ 404 
8.12 Notes for Chapter 8 ................................................. 406 
9 Free End-Time Problems .................................................. 411 
9.1 Introduction ......................................................... 412 
9.2 Lipschitz Time Dependence ........................................ 415 
9.3 Essential Values ..................................................... 424 
9.4 Measurable Time Dependence ..................................... 427 
9.5 Proof of Theorem 9.4.1 ............................................. 430 
9.6 A Free End-Time Maximum Principle............................. 448 
9.7 Appendix: Metrics on the Space of Free 
End-Time Trajectories .............................................. 463 
9.8 Exercises ............................................................ 466 
9.9 Notes for Chapter 9 ................................................. 468 
10 The Maximum Principle for Problems with Pathwise Constraints ... 471 
10.1 Introduction ......................................................... 472 
10.2 Problems with Pure State Constraints: Preliminary Discussion .. 474 
10.3 Convergence of Measures .......................................... 477 
10.4 The Maximum Principle (Pure State Constraints)................. 482 
10.5 Proof of Theorem 10.4.1............................................ 485 
10.6 Maximum Principles for Free End-Time Problems with 
State Constraints .................................................... 498 
10.7 Non-degenerate Conditions ........................................ 503 
10.8 Mixed Constraints .................................................. 521 
10.9 Exercises ............................................................ 537 
10.10 Notes for Chapter 10................................................ 543 
11 The Euler-Lagrange and Hamiltonian Inclusion Conditions 
in the Presence of State Constraints ...................................... 547 
11.1 Introduction ......................................................... 548 
11.2 The Euler Lagrange Inclusion ...................................... 549 
11.3 Proof of Theorem 11.2.1 ............................................ 552 
11.4 Free End-Time Problems with State Constraints .................. 567 
11.5 Non-degenerate Necessary Conditions ............................ 574xvi Contents
11.6 Exercises ............................................................ 588 
11.7 Notes for Chapter 11................................................ 589 
12 Regularity of Minimizers .................................................. 591 
12.1 Introduction ......................................................... 592 
12.2 Tonelli Regularity ................................................... 599 
12.3 Proof of The Generalized Tonelli Regularity Theorem ........... 604 
12.4 Lipschitz Continuous Minimizers.................................. 613 
12.5 Nonautonomous Variational Problems with State Constraints ... 619 
12.6 Bounded Controls................................................... 630 
12.7 Lipschitz Continuous Controls ..................................... 633 
12.8 Exercises ............................................................ 638 
12.9 Notes for Chapter 12................................................ 640 
13 Dynamic Programming .................................................... 643 
13.1 Introduction ......................................................... 644 
13.2 Invariance Theorems................................................ 654 
13.3 The Value Function and Generalized Solutions of the 
Hamilton Jacobi Equation .......................................... 667 
13.4 Local Verification Theorems ....................................... 685 
13.5 Costate Trajectories and Gradients of the Value Function ........ 697 
13.6 State Constrained Problems ........................................ 708 
13.7 Proofs of Theorems 13.6.1 and 13.6.2 ............................. 711 
13.8 Costate Trajectories and Gradients of the Value 
Functions for State-Constrained Problems ........................ 719 
13.9 Semiconcavity and the Value Function ............................ 723 
13.10 The Infinite Horizon Problem ...................................... 728 
13.11 The Minimum Time Problem ...................................... 735 
13.12 Viscosity Solutions of the Hamilton Jacobi Equation ............. 741 
13.13 A Comparison Theorem for Viscosity Solutions .................. 754 
13.14 Exercises ............................................................ 759 
13.15 Notes for Chapter 13................................................ 762 
References......................................................................... 769 
Index ............................................................................... 779Notation 
B Closed unit ball in Euclidean space
|x| Eulidean norm of x
a ∧ b min{a, b}
a ∨ b max{a, b}
R+ Non-negative real numbers
dC(x) Euclidean distance of x from C ◦
C, intC Interior of C
∂C, bdyC Boundary of C
C¯ Closure of C
co D Convex hull of D
co D Closure of the convex hull of D
NP
C (x) Proximal normal cone to C at x
NˆC(x) Strict normal cone to C at x
NC(x) Limiting normal cone to C at x
TC(x) Bouligand tangent cone to C at x
T¯
C(x) Clarke tangent cone to C at x
∂P f (x) Proximal subdifferential of f at x
∂f (x) ˆ Strict subdifferential of f at x
∂f (x) Limiting subdifferential of f at x
∂∞
P f (x) Asymptotic proximal subdifferential
of f at x
∂ˆ∞f (x) Asymptotic strict subdifferential
of f at x
∂∞f (x) Asymptotic limiting subdifferential
of f at x
dom f (Effective) domain of f
Gr F Graph of F
epi f Epigraph of f
xviixviii Notation
f 0(x; v) Generalized directional derivative of f
at x in the direction v
C(x) Indicator function of the set C
at the point x
∇f (x) Gradient vector of f at x
xi
C
→ x xi → x and xi ∈ C ∀i
xi
f
→ x xi → x and f (xi) → f (x)
suppμ Support of the measure μ
L Lebesgue subsets of I ⊂ R
Bk Borel subsets of Rk
L1(I ; Rn) Integrable functionsf : I → Rn
L1(S, T ) Integrable functions f : [S,T ] → R
W1,1(I ; Rn) Absolutely continuous functionsf : I → Rn
NBV ([S,T ]; Rn) Functions of bounded variationf : [S,T ] → Rn,right
continuous on (S, T )
H, H Hamiltonian, un-maximized HamiltonianChapter 1 
Overview 
Abstract Dynamic optimization emerged as a distinct field of research in the late 
1950’s, to address new kinds of optimization problems, in aerospace, economics and 
other areas. The distinctive feature of these problems was an underlying dynamic 
constraint, typically in the form of a controlled differential equation, which placed 
these problems beyond the scope of earlier variational techniques. Rapid advances 
were made in the 1970’s and 80’s, with the discovery of the maximum principle 
and methodologies (dynamic programming) that linked optimal strategies and the 
Hamilton Jacobi equation. These were the main elements in what, today, is known as 
the classical theory of dynamic optimization. While classical dynamic optimization 
was adequate for many applications, deficiencies became apparent, leading to a new 
body of theory in the 1980’s, including Clarke’s nonsmooth maximum principle 
and generalized solutions of Hamilton-Jacobi equations, based on techniques of 
nonsmooth analysis. 
The purpose of this overview chapter is twofold. First, it provides a self-contained 
exposition of the classical theory suitable for a first course in dynamic optimization 
(at undergraduate or graduate level). It includes motivating examples, a derivation 
of the classical maximum principle, optimality conditions of dynamic programming 
type expressed in terms of solutions to the Hamilton Jacobi equation, and extensive 
discussion. Second, it gives answers to the questions: what are the shortcomings of 
the classical theory and how are they surmounted by more recent developments? 
We argue that many of the deficiencies of the earlier theory arise from the lack 
of appropriate analytic techniques for constructing useful local approximations of 
non-differentiable functions and closed sets with irregular boundaries. We then 
cover rudiments of nonsmooth analysis, which was developed precisely for this 
purpose, and show how we can use it to derive new, improved optimality conditions, 
unshackled by the restrictive hypotheses of classical dynamic optimization. We 
thereby offer readers the ‘big picture’ in preparation for later chapters, and also 
to equip them better to understand the contemporary literature. 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
P. Bettiol, R. B. Vinter, Principles of Dynamic Optimization, Springer Monographs 
in Mathematics, https://doi.org/10.1007/978-3-031-50089-3_1
12 1 Overview
1.1 Dynamic Optimization 
Dynamic optimization emerged as a distinct field of research in the 1950’s, to 
address in a unified fashion optimization problems arising in scheduling and 
the control of engineering devices, beyond the reach of earlier analytical and 
computational techniques. This field was initially called optimal control, but this 
earlier name is increasingly giving way to dynamic optimization, to convey a wider 
range of potential application, beyond control engineering. Aerospace engineering 
is an important source of such problems, and the relevance of dynamic optimization 
to the American and Russian space programmes gave powerful initial impetus to 
research in this area. A simple example is: 
The Maximum Orbit Transfer Problem A rocket vehicle is in a circular orbit. 
What is the radius of the largest possible co-planar orbit to which it can be 
transferred over a fixed period of time? The motion of the vehicle during the 
manoeuvre is governed by the rocket thrust and by the rocket thrust orientation, 
both of which can vary with time. See the Fig. 1.1. The variables involved are 
r = radial distance of vehicle from attracting centre,
u = radial component of velocity,
v = tangential component of velocity,
m = mass of vehicle,
Tr = radial component of thrust,
Tt = tangential component of thrust.
Fig. 1.1 The maximum orbit 
transfer problem1.1 Dynamic Optimization 3
The constants are 
r0 = initial radial distance,
m0 = initial mass of vehicle,
γmax = maximum fuel consumption rate,
Tmax = maximum thrust,
μ = gravitational constant of attracting centre,
tf = duration of manoeuvre.
A precise formulation of the problem, based on an idealized point mass model of 
the space vehicle, is as follows: 
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize − r(tf )
over radial and tangential components of the thrust history,
(Tr(t), Tt(t)), 0 ≤ t ≤ tf , satisfying
r(t) ˙ = u,
u(t) ˙ = v2(t)/r(t) − μ/r2(t) + Tr(t)/m(t),
v(t) ˙ = −u(t)v(t)/r(t) + Tt(t)/m(t),
m(t) ˙ = −(γmax/Tmax )(T 2
r (t) + T 2
t (t))1/2,
(T 2
r (t) + T 2
t (t))1/2 ≤ Tmax,
m(0) = m0, r(0) = r0, u(0) = 0, v(0) = √μ/r0,
u(tf ) = 0, v(tf ) = μ/r(tf ).
Here r(t) ˙ denotes dr(t)/dt, etc. It is standard practice in dynamic optimization 
to formulate optimization problems as minimization problems. Accordingly, the 
problem of maximizing the radius of the terminal orbit r(tf ) is replaced by 
the equivalent problem of minimizing the ‘cost’ −r(tf ). Notice that knowledge 
of the control function or strategy (Tr(t), Tt(t)), 0 ≤ t ≤ tf permits us to 
calculate the cost −r(tf ): we solve the differential equations, for the specified 
boundary conditions at time t = 0, to obtain the corresponding state trajectory 
(r(t), u(t), v(t), m(t)), 0 ≤ t ≤ tf , and thence determine −r(tf ). The control 
strategy therefore has the role of choice variable in the optimization problem. We 
seek a control strategy which minimizes the cost, from among the control strategies 
whose associated state trajectories satisfy the specified boundary conditions at time 
t = tf . 
For the following values of relevant dimensionless parameters: 
Tmax/m0
μ/r2
0
= 0.1405, γmax
Tmax/
√μ/r0
= 0.07487, tf

r3/μ = 3.32 ,
the radius of the terminal circular orbit is 
r(tf ) = 1.5 r0.4 1 Overview
Fig. 1.2 A control strategy for the maximum orbit transfer problem 
In Fig. 1.2, the arrows indicate the magnitude and orientation of the thrust at times 
t = 0, 0.1tf , 0.2tf , ... , tf . As indicated, full thrust is maintained. The thrust 
is outward for (approximately) the first half of the manoeuvre and inward for the 
second. 
Suppose, for example, that the attracting centre is the Sun, the space vehicle 
weighs 10,000 lb, the initial radius is 1.50 million miles (the radius of a circle 
approximating the Earth’s orbit), the maximum thrust is 0.85 lb (i.e. a force equiva￾lent to the gravitational force on a 0.85 lb mass on the surface of the earth, which cor￾responds to Tmax = 3.778 N, the maximum rate of fuel consumption is 1.81 lb/day 
and the transit time is 193 days. Corresponding values of the constants are 
Tmax = 3.778 N, m0 = 4.536 × 103 kg,
r0 = 1.496 × 1011 m, γmax = 0.9496 × 10−5 kg s−1,
tf = 1.6675 × 107 s, μ = 1.32733 × 1020 m3 s−2.
Then the terminal radius of the orbit is 2.44 million miles. (This is the radius of a 
circle approximating the orbit of the planet Mars.) 
Numerical methods, inspired by necessary conditions of optimality akin to the 
maximum principle of Chap. 7, were used to generate the above control strategy.1.1 Dynamic Optimization 5
Optimal Control of a Growth/Consumption Model Dynamic optimization prob￾lems are encountered also in the field of economics. One example is the ‘growth 
versus consumption’ problem of neoclassical macro-economics, based on the 
Ramsey model of economic growth. The question here is, what balance should be 
struck between investment and consumption to maximize overall spending on social 
programmes over a fixed period time? A simple formulation of the problem is as 
follows. 
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize −  T
0 (1 − u(t))xα(t)dt
subject to
x(t) ˙ = −ax(t) + bxα(t)u(t) for a.e. t ∈ [0, T ],
u(t) ∈ [0, 1] for a.e. t. ∈ [0, T ] ,
x(t) ≥ 0 for all t ∈ [0, T ] ,
x(0) = x0 .
Here, a > 0, b > 0, x0 ≥ 0 and α ∈ (0, 1) are given constants and [0, T ] is a given 
interval. 
It has the following interpretation: x denotes global economic output. The rate of 
financial return r(x) from economic output x is modelled as 
r(x) = bxα .
The term −ax takes account of fixed costs reducing growth (wages, etc.). 
To describe the solution to this problem, we introduce the constants 
xˆ := αb
a
	 1
1−α
and Δ :=
1
aα
ln 1
1 − α
	
and also the state feedback function χ : [0, T ] × (0,∞) → [0, 1]: 
χ (t, x) :=
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
0 if x > y(t) ¯
1 if x < y(t) ¯
α if x = ¯y(t) and t ≤ T − Δ
0 if x = ¯y(t) and t>T − Δ ,
in which y¯ : (−∞, T ] → (0,∞) is the function 
y(t) ¯ := 

xˆ if t ≤ T − Δ
 b
a (1 − e−aα(T −t) 1
1−α if t>T − Δ .
Techniques of dynamic programming covered in Chap. 13 provide the following 
solution to this problem: 
Given arbitrary initial data (t0, x0) ∈ [0, T ] × (0,∞), the optimal output x∗is 
the unique solution in the space of Lipschitz continuous functions on [t0, T ] of the 
differential equation6 1 Overview

x˙∗(t) = −ax∗(t) + bx∗α(t)χ (t, x∗(t)) a.e. t ∈ [t0, T ],
x(t0) = x0 . (1.1.1) 
The optimal proportion of financial return for investment u∗is unique (w.r.t. the 
equivalence class of almost everywhere equal functions) and is given by 
u∗(t) = χ (t, x∗(t)), for a.e. t ∈ [t0, T ] .
Notice that the solution above is expressed in state feedback form; that is, the 
optimal control u∗ is expressed as a function of the current state. For any given 
initial state and time t0, the optimal state expressed as a function of time, i.e. in 
open loop form, is the solution to the ‘closed loop’ state equation (1.1.1) for the 
given initial state and time t0. We then obtain the optimal control as a function 
of time (open loop form) by plugging the optimal state trajectory into the state 
feedback function. Notice that the feedback form captures, within a single relation, 
the optimal strategies for every initial state x0 and time t0. 
Intuition would suggest that if, at the start of the time interval, economic output 
is low, the optimal control should have a first phase of maximum investment during 
which economic output builds up to some critical value, followed by a second 
phase of intermediate investment over which economic output is maintained and, 
finally, a third phase over which there is no investment because the remaining 
time is too small for the benefits of investment to show through. This is indeed 
the optimal control, with the qualification that, if the initial output is high, the 
optimal control is pure consumption in the first phase. There are also values of 
the initial investment and T such that there is no first phase or no first and second 
phase. Analysis is required, of course, to determine precisely the times separating 
the phases, the critical value of output and the proportion of financial return for 
investment required to maintain it; also to identify the situations when there are 
fewer than three phases. Optimal state trajectories, for various choices of initial 
data, are illustrated in Fig. 1.3. 
Optimal Control in Anti-Cancer Treatment 
We illustrate applications of dynamic optimization in medicine. Chemotherapy is 
a treatment aimed at destroying cancer cells by means of a cocktail of drugs, 
Fig. 1.3 Optimal trajectories for the consumption/growth problem1.1 Dynamic Optimization 7
administered either at specific times or continuously. It is typically part of a complex 
overarching treatment plan, in which chemotherapy is following up by procedures, 
surgical or drug-based, for inhibiting renewed tumour growth. 
Traditionally, chemotherapy treatments have been based on the maximum tol￾erated dose paradigm. But a side effect of chemotherapy, a ‘two-edged sword’, 
is damage to normal cells. Modern day treatments aim to improve outcomes by 
balancing destruction of cancer cells and suppression of side effects. Empirical 
design of treatment plans based on clinical trials is time consuming and extremely 
expensive. Mathematical models of the underlying pharmaco-dynamic processes 
involved have an important role, because they can be used to simulate on the 
computer the effects of different treatment strategies, simply and at low cost. 
Dynamic optimization is the appropriate tool for designing optimal treatment 
strategies based on these models. 
The following formulation of treatment planning as a dynamic optimization 
problem is taken from [188]. The underlying dynamic model involves the time￾varying state variable components c1, c2, n and w and the control variable u: 
c1 = concentration of administered anti-cancer drug in plasma,
c2 = active drug concentration at the tumour cellular level,
n = number of tumour cells,
w = number of white blood cells (WBCs),
u = drug dosage.
The evolution of the state variable components for some control strategy u(t), 0 ≤
t ≤ T , is governed by the differential equations over the fixed time interval [0, T ]
c˙1(t) = −(k1 + k2)c1(t) +
 1
V1

u(t)
c˙2(t) = k12  V1
V2

c1(t) − k2c2(t)
n(t) ˙ = Λψ(n(t)) − K max{c2(t) − Cmin, 0}
w(t) ˙ = rc − V w(t) − μw(t)c1(t)
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(1.1.2) 
in which ψ is the function ψ(n) := n loge
 θ
n

.
The initial conditions on state variable components are 
c1(0) = 0, c2(0) = 0, n(0) = n0 and w(0) = w0.
The first differential equation relates the administered drug concentration to the 
drug dosage. The second relates the active drug concentration to the administered 
drug concentration. The third is a Gompertz-type differential equation governing 
tumour growth with an exogenous term to account for the suppressive effects of 
the active drug concentration. The fourth determines how the WBC population, 
whose decrease reflects chemotherapy toxicity, responds to the administered drug 
concentration. 
The chosen values of parameters in the model are as in Table 1.1.8 1 Overview
Table 1.1 Values of the parameters in the model 
Par. Description Value 
V1 Volume of distribution in first compartment 25 litres 
V2 Volume of distribution in second compartment 15 litres 
k1 Process of drug elimination from plasma compartment 1.6 day−1
k12 Link process between two compartments 0.4 day−1
θ Largest tumour 1012
n0 Initial size of tumour at t = 0 30 × 109 cells 
Λ Gompertz growth parameter for tumour 3 × 10−3 day−1
Cmin Threshold below which no tumour cells are killed 0.0001 gml−1
K Rate of cell killing 30 g−1 litres day−1
μ Delayed toxicity of drug concentration on WBCs 80 g−1 litres day−1
w0 Initial physiology level of WBCs at t = 0 8 × 109 litres −1
V Nominal turnover constant 0.15 day−1
rc Rate of WBCs production 0.2 × 109 litre−1 day−1
Cmax Maximum allowable drug concentration 0.01 gml−1
WD Absolute leukopenia level 2 × 109 litres−1
T Terminal time 40 days 
The control problem is to minimize a weighted sum of the tumour volume and the 
total amount of drug, subject to upper and lower bounds at each time on drug toxicity 
and white blood cell population respectively, both of which affect the patient’s 
health. 
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize  T
0 (αn(t) + u(t))dt,
over control strategies u : [0, T ] → R
and state trajectories (c1, c2, n, w)
satisfying
u(t) ∈ [0, 1] for t ∈ [0, T ],
c1(t) ≤ Cmax for t ∈ [0, T ],
w(t) ≥ WD for t ∈ [0, T ].
The upper and lower bounds, Cmax and WD, are as given in Table 1.1. 
In [188] a combination of analytical and computational techniques are employed 
to determine a control strategy u¯ which satisfies necessary conditions of optimality, 
when the weighting factor is chosen to be α = (3/5) × 10−10. The control u¯ gives 
rise to a quite complicated, 5-subarc state trajectory structure, involving two short 
bang-bang pulses and three subarcs, in each of which u(t) takes a constant value. 
By neglecting the bang-bang pulses, we arrive at a simpler, and therefore more 
practical, drug treatment strategy, with only slightly increased cost. See Fig. 1.4. 
According to this strategy the dosage is held constant at a higher level over an initial 
period, reduced to a lower level for a subsequent period and finally reduced to 0 for 
the final period:1.2 The Calculus of Variations 9
Fig. 1.4 Optimized drug 
strategy 
u(t) ¯ =
⎧
⎨
⎩
u1 for 0 ≤ t<t1
u2 for t1 ≤ t<t2
0 for t2 ≤ t < 40.
Here, t1 = 2.2786 days, t2 = 25.855 days, u1 = 0.60254 and u2 = 0.33737. 
1.2 The Calculus of Variations 
From a mathematical perspective, dynamic optimization is an outgrowth of the 
calculus of variations (in one independent variable) that takes account of new kinds 
of constraints (differential equation constraints, pathwise constraints on control 
functions ‘parameterizing’ the differential equations, etc.) encountered in advanced 
engineering design and dynamic decision making. A number of key developments 
in dynamic optimization have resulted from marrying old ideas from the calculus of 
variations and modern analytical techniques. For purposes both of setting dynamic 
optimization in its historical context and of illuminating later developments in 
dynamic optimization, we pause to review relevant material from the classical 
calculus of variations. 
The basic problem in the calculus of variations is that of finding an arc x¯ which 
minimizes the value of an integral functional 
J (x) =
 T
S
L(t, x(t), x(t))dt ˙
over some class of arcs satisfying the boundary condition 
x(S) = x0 and x(T ) = x1.
Here [S,T ] is a given interval, L : [S,T ] × Rn × Rn → R is a given function, and 
x0 and x1 are given points in Rn.10 1 Overview
Fig. 1.5 The 
Brachistochrone problem 
xf 
f s 
x(s) 
0 
mg 
v 
s 
The Brachistochrone Problem An early example of such a problem was the 
brachistochrone problem circulated by Johann Bernoulli in the late seventeenth 
century. Positive numbers sf and xf are given. A frictionless bead, initially located 
at the point (0, 0), slides along a wire under the force of gravity. The wire, which 
is located in a fixed vertical plane, joins the points (0, 0) and (sf , xf ). What should 
the shape of the wire be, in order that the bead arrives at its destination, the point 
(sf , xf ), in minimum time? See Fig. 1.5. 
There are a number of possible formulations of this problem. We now describe 
one of them. Denote by s and x the horizontal and vertical distances of a point on the 
path of the bead (vertical distances are measured downward). We restrict attention 
to wires describable as the graph of a suitably regular function x(s), 0 ≤ s ≤ sf . 
For any such function x, the speed v(s) is related to the downward displacement 
x(s), when the horizontal displacement is s, according to 
mgx(s) = 1
2
mv2(s) (1.2.1) 
(‘loss of potential energy equals gain of kinetic energy’). For any s ∈ [0, sf ], we 
denote by t (s) the time elapsed when the position of the bead is (s, x(s)). If it is 
assumed that speed v is positive valued, the functions t and v are related by the 
equation 
v(s)
dt
ds(s) =

1 + |
dx
ds (s)|
2 , for t ∈ [0, sf ] .
Denote by tf the transit time: tf = t (sf ). The change of independent variable 
t (s) =  s
0 v−1(s'
)

1 + |dx(s'
)/ds)|
2 ds' now gives the following formula for tf : 
tf =
 tf
0
dt =
 sf
0

1 + |dx(s)/ds|
2
v(s)
ds.1.2 The Calculus of Variations 11
Using (1.2.1) to eliminate v(s), we arrive at a formula for the transit time: 
J (x) =
 sf
0
L(s, x(s), x(s))ds, ˙
in which 
L(s, x, w) :=

1 + |w|
2
√2gx .
The problem is to minimize J (x) over some class of arcs x satisfying 
x(0) = 0 and x(sf ) = xf .
This is an example of the basic problem of the calculus of variations, in which 
(S, x0) = (0, 0) and (T , x1) = (sf , xf ). Suppose that we seek a minimizer in the 
class of absolutely continuous arcs. It can be shown that the minimum time t∗ and 
the minimizing arc (x(t), s(t)), 0 ≤ t ≤ t∗ (expressed in parametric form with 
independent variable time t) are given by the formulae 
x(t) = a

1 − cosg
a t
	
and s(t) = a
g
a t − sing
a t
	
.
Here, a and t∗ are constants which uniquely satisfy the conditions 
x(t∗) = xf ,
s(t∗) = tf ,
0 ≤
g
a t
∗ ≤ 2π .
The minimizing curve is a cycloid, with infinite slope at the point of departure: it 
coincides with the locus of a point on the circumference of a disc of radius a, which 
rolls without slipping along a line of length tf . 
Problems of this kind, the minimization of integral functionals, may perhaps have 
initially attracted attention as individual curiosities. But throughout the eighteenth 
and nineteenth centuries their significance became increasingly evident, as the list 
lengthened of laws of physics which identified states of nature with minimizing 
curves and surfaces. Some examples of rules of the minimum are as follows: 
Fermat’s Principle in Optics The path of a light ray achieves a local minimum 
of the transit times over paths between specified end-points which visit the 
relevant reflecting and refracting boundaries. The principle predicts Snell’s Laws 
of Reflection and Refraction, and the curved paths of light rays in inhomogeneous 
media. See Fig. 1.6.12 1 Overview
Fig. 1.6 Fermat’s principle predicts Snell’s laws 
Dirichlet’s Principle Take a bounded, open set Ω ⊂ R2 with boundary ∂Ω, in 
which a static two-dimensional electric field is distributed. Denote by V (x) the 
voltage at point x ∈ Ω. Then V (x) satisfies Poisson’s equation 
ΔV (x) = 0 for x ∈ Ω
V (x) = V (x) ¯ for x ∈ ∂Ω.
Here, V¯ : ∂Ω → R is a given function, which supplies the boundary data. 
Dirichlet’s principle characterizes the solution to this partial differential equation 
as the solution of a minimization problem 

Minimize 
Ω ∇V (x) · ∇V (x)dx
over surfaces V satisfying V (x) = V (x) ¯ on ∂Ω.
This optimization problem involves finding a surface which minimizes a given 
integral functional. See Fig. 1.7. 
Dirichlet’s principle and its generalizations are important in many respects. They 
are powerful tools for the study of existence and regularity of solutions to boundary 
value problems. Furthermore, they point the way to Galerkin methods for computing 
solutions to partial differential equations, such as Poisson’s equation: the solution 
is approximated by the minimizer of the Dirichlet integral above over some finite 
dimensional subspace SN of the domain of the original optimization problem, 
spanned by a finite collection of ‘basis’ functions {φi}
N
i=1, 
SN = {
N
i=1
αiφi(x) : α ∈ RN }.
The widely used finite element methods are modern implementations of Galerkin’s 
method.1.2 The Calculus of Variations 13
Fig. 1.7 A minimizer for the Dirichlet integral 
The Action Principle Let x(t) be the vector of generalized coordinates of a 
conservative mechanical system. The action principle asserts that x(t) evolves in 
a manner to minimize (strictly speaking, to render stationary) the ‘action’, namely 

[T (x(t), x(t)) ˙ − V (x(t))]dt.
Here T (x, x)˙ is the kinetic energy and V (x) is the potential energy. Suppose, for 
example, x = (r, θ ), the polar coordinates of an object of mass m moving in a plane 
under the influence of a radial field (the origin is the centre of gravity of a body, 
massive in relation to the object). Then 
T (x, x)˙ = 1
2
m(r˙
2 + r2θ˙2)
and 
V (r) = K/r,
for some constant K. The action in this case is 
 1
2
m[ ˙r2(t) + r2(t)θ˙2(t)] − K/r(t)	
dt.
In this case, the premise ‘the action is minimized’ predicts the motion of a single 
planet about a much larger sun in a Newtonian universe. See Fig. 1.8. The action 
principle has proved a fruitful starting point for deriving the dynamical equations of 
complex interacting systems, and for studying their qualitative properties (existence 
of periodic orbits with prescribed energy, etc.).14 1 Overview
Fig. 1.8 The action principle 
predicts planetary motion 
Necessary Conditions 
Consider the optimization problem 
(CV)
⎧
⎨
⎩
Minimize J (x) :=  T
S L(t, x(t), x(t))dt ˙
over arcs x satisfying
x(S) = x0 and x(T ) = x1,
in which [S,T ] is a fixed interval, L : R × Rn × Rn → R is a given C2
function and x0 and x1 are given points in Rn. Precise formulation of problem (CV) 
requires us to specify the domain of the optimization problem. We take this to be 
W1,1([S,T ]; Rn), the class of absolutely continuous Rn valued arcs on [S,T ], for 
reasons which will be discussed presently. 
The systematic study of minimizers x¯ for this problem was initiated by Euler, 
whose seminal paper of 1744 provided the link with the equation: 
d
dt
Lv(t, x(t), ¯ ˙
x(t)) ¯ = Lx (t, x(t), ¯ ˙
x(t)). ¯ (1.2.2) 
(In this equation, Lx and Lv are the gradients of L(t, x, v) with respect to the second 
and third arguments respectively.) 
The Euler equation (1.2.2) is, under appropriate hypotheses, a necessary condi￾tion for an arc x¯ to be a minimizer. Notice that, if the minimizer x¯ is a C2 function, 
then the Euler equation is a second order, n-vector differential equation: 
Lvt(t, x(t), ¯ ˙
x(t)) ¯ +Lvx(t, x(t), ¯ ˙
x(t)) ¯ ·˙
x¯+Lvv(t, x(t), ¯ ˙
x(t)) ¯ ·
..
x (t) ¯ =Lx (t, x(t), ¯ ˙
x(t)). ¯
A standard technique for deriving the Euler equation is to reduce the problem 
to a scalar optimization problem, by consideration of a one-parameter family of 
variations. The calculus of variations, incidentally, owes its name to these ideas. 
(Variations of the minimizing arc cannot reduce the cost; conditions on minimizers 
are then derived by processing this information, with the help of a suitable calculus 
to derive necessary conditions of optimality). Because of its historical importance 
and its continuing influence on the derivation of necessary conditions, we now 
describe the technique in detail.1.2 The Calculus of Variations 15
Fix attention on a minimizer x¯. Further hypotheses are required to derive the 
Euler equation. We assume that there exists some number K such that 
|L(t, x, v) − L(t, y, w)| ≤ K(|x − y|+|v − w|) (1.2.3) 
for all x, y ∈ Rn and all v, w ∈ Rn. 
Take an arbitrary C1 arc y, which satisfies the homogeneous boundary conditions 
y(S) = y(T ) = 0.
Then, for any ϵ > 0, the ‘variation’ x+ϵy, which satisfies the end-point constraints, 
must have cost not less than that of x¯. It follows that 
ϵ−1[J (x¯ + ϵy) − J (x)¯ ] ≥ 0.
Otherwise expressed 
 T
S
ϵ−1[L(t, x(t) ¯ + ϵy(t), ˙
x(t) ¯ + ϵy(t)) ˙ − L(t, x(t), ¯ ˙
x(t)) ¯ ]dt ≥ 0.
Under hypothesis (1.2.3), the dominated convergence theorem permits us to pass to 
the limit under the integral sign. We thereby obtain the inequality 
 T
S
[Lx(t, x(t), ¯ ˙
x(t)) ¯ · y(t) + Lv(t, x(t), ¯ ˙
x(t)) ¯ · ˙y(t)]dt ≥ 0.
This relationship holds, we note, for all continuously differentiable functions y
satisfying the boundary conditions y(S) = 0 and y(T ) = 0. By homogeneity, the 
inequality can be replaced by equality. 
Now apply integration by parts to the first term on the left. This gives 
 T
S
[−  t
S
Lx (s, x(s), ¯ ˙
x(s))ds ¯ + Lv(t, x(t), ¯ ˙
x(t)) ¯ ]· ˙y(t)dt = 0.
Take any continuous function w : [S,T ] → Rn which satisfies 
 T
S
w(t)dt = 0. (1.2.4) 
Then the continuously differentiable arc y(t) ≡  t
S w(s)ds vanishes at the end￾times. Consequently 
 T
S
[−  t
S
Lx (s, x(s), ¯ ˙
x(s))ds ¯ + Lv(t, x(t), ¯ ˙
x(t)) ¯ ] · w(t)dt = 0, (1.2.5) 
a relationship which holds for all continuous arcs w satisfying (1.2.4). To advance 
the analysis, we require16 1 Overview
Lemma (Du Bois-Reymond) Take a function a ∈ L2([S,T ]; Rn). Suppose that 
 T
S
a(t) · w(t) dt = 0 (1.2.6) 
for every continuous function w which satisfies 
 T
S
w(t)dt = 0. (1.2.7) 
Then there exists some vector d ∈ Rn such that 
a(t) = d for a.e. t ∈ [S,T ].
Proof Write A for the subset of constant functions in the Hilbert space 
L2([S,T ]; Rn), endowed with the standard inner product 〈w, v〉L2 =  T
S w(t) ·
v(t) dt. We shall prove that A is the orthogonal complement, denoted by W⊥, of 
the subspace 
W := 
w ∈ L2([S,T ]; Rn) :
 T
S
w(t) dt = 0

.
Clearly, if a ∈ A then, for every w ∈ W, we have 〈a,w〉L2 = a ·
 T
S w(t) dt = 0. 
Therefore A ⊂ W⊥. 
Now, take any f ∈ W⊥. Set f¯ :=  T
S f (t) dt and consider w¯ := f −f¯. Observe 
that f¯ ∈ A and w¯ ∈ W. But, we know that A ⊂ W⊥ and, so, it follows that 
〈f ,¯ w¯〉L2 = 0. We deduce that 
0 = 〈f, w¯〉L2 − 〈f ,¯ w¯〉L2 = 〈f − f ,¯ w¯〉L2 = ||f − f¯||2
L2 .
Thus, f = f¯ a.e. on [S,T ], which implies f ∈ A. We conclude that W⊥ ⊂ A. The 
lemma is proved. ⨅⨆
Return now to the derivation of the Euler equation. We identify the function a of 
the lemma with 
t → −  t
S
Lx (s, x(s), ¯ ˙
x(s))ds ¯ + Lv(t, x(t), ¯ ˙
x(t)). ¯
Assume that t → Lx (t, x(t), ¯ ˙
x(t)) ¯ and t → Lv(t, x(t), ¯ ˙
x(t)) ¯ are integrable and 
square integrable functions respectively. Taking note of (1.2.5), we deduce from the 
lemma informs that there exists a vector d such that 
−
 t
S
Lx (s, x(s), ¯ ˙
x(s))ds ¯ + Lv(t, x(t), ¯ ˙
x(t)) ¯ = d , a.e. t ∈ [S,T ]. (1.2.8)1.2 The Calculus of Variations 17
Since Lx (t, x(t), ¯ ˙
x(t)) ¯ is integrable, it follows that t → Lv(t, x(t), ¯ ˙
x(t)) ¯ is 
almost everywhere equal to an absolutely continuous function and 
d
dt
Lv(t, x(t), ¯ ˙
x(t)) ¯ = Lx (t, x(t), ¯ ˙
x(t)), ¯ a.e. t ∈ [S,T ].
We have verified the Euler equation and given it a precise interpretation, when the 
domain of the optimization problem is the class of absolutely continuous arcs. 
The above analysis conflates arguments assembled over several centuries. The 
first step was to show that smooth minimizers x¯ satisfy the pointwise Euler 
equation. Euler’s original derivation made use of discrete approximation techniques. 
Lagrange’s alternative derivation introduced variational methods similar to those 
outlined above (though differing in the precise nature of the ‘integration by parts’ 
step). Erdmann subsequently discovered that, if the domain of the optimization 
problem is taken to be the class of piecewise C1 functions (i.e. absolutely continuous 
functions with piecewise continuous derivatives) then 
‘the function t → Lv(t, x(t), ¯ ˙
x(t)) ¯ has removable discontinuities’.
This condition is referred to as the first Erdmann condition. 
For piecewise C1 minimizers, the integral version of the Euler equation (1.2.8), 
was first regarded as a convenient way of combining the pointwise Euler equation 
and the first Erdmann condition. We shall refer to it as the Euler Lagrange condition. 
An analysis in which absolutely continuous minimizers substitute for piecewise C1
minimizers is an early twentieth century development, due to Tonelli. 
Another important property of minimizers can be derived in situations when L
is independent of the t (write L(x, v) in place of L(t, x, v)). In this ‘autonomous’ 
case the second order n vector differential equation of Euler can be integrated. There 
results a first order differential equation involving a ‘constant of integration’ c: 
Lv(x(t), ¯ ˙
x(t)) ¯ · ˙
x(t) ¯ − L(x(t), ¯ ˙
x(t)) ¯ = c . (1.2.9) 
This condition is referred to as the Second Erdmann condition or constancy of the 
Hamiltonian condition. It is easily deduced from the Euler Lagrange condition when 
x¯ is a C2 function. Fix t. We calculate in this case 
d
dt (Lv · ˙
x(t) ¯ − L) = d
dt
Lv · ˙
x(t) ¯ + Lv·
..
x (t) ¯ − Lx · ˙
x(t) ¯ − Lv·
..
x (t) ¯
= (
d
dt
Lv − Lx ) · ˙
x(t) ¯
= 0.
(In the above relationships, L, Lv, etc., are evaluated at (x(t), ¯ ˙
x(t)) ¯ ). Equa￾tion (1.2.9) follows.18 1 Overview
A more sophisticated analysis leads to an ‘almost everywhere’ version of this 
condition for autonomous problems, when the minimizer x¯ in question is assumed 
to be merely absolutely continuous. 
‘Variations’ of the type x(t) ¯ + ϵy(t) lead to the Euler Lagrange condition. 
Necessary conditions supplying additional information about minimizers have 
been derived by considering other kinds of variations. We note in particular the 
Weierstrass Condition or the maximization of the Hamiltonian condition: 
Lv(t, x(t), ¯ ˙
x(t)) ¯ · ˙
x(t) ¯ − L(t, x(t), ¯ ˙
x(t)) ¯
= max
v∈Rn
{Lv(t, x(t), ¯ ˙
x(t)) ¯ · v − L(t, x(t), v) ¯ }.
Suppressing the (t, x(t)) ¯ argument in the notation and expressing the Weierstrass 
condition as 
L(v) − L(˙
x(t)) ¯ ≥ Lv(˙
x)¯ · (v − ˙
x(t)) ¯ for all v ∈ Rn,
we see that it conveys no useful information when L(t, x, v) is convex with respect 
to the v variable: in this case it simply interprets Lv as a subgradient of L in the 
sense of convex analysis. In general however, it tells us that L coincides with its 
‘convexification’ (with respect to the velocity variable) along the optimal trajectory, 
i.e. 
L(t, x(t), ¯ ˙
x(t)) ¯ = L(t, ˜ x(t), ¯ ˙
x(t)). ¯
Here L(t, x, .) ˜ is the function with epigraph set co {epiL(t, x, .)}. See Fig. 1.9. 
v 
Fig. 1.9 The Weierstrass condition1.2 The Calculus of Variations 19
The above necessary conditions (the Euler Lagrange condition, the Weierstrass 
condition and the second Erdmann condition) have convenient formulations in terms 
of the ‘costate arc’ 
p(t) = Lv(t, x(t), ¯ ˙
x(t)). ¯
They are 
(p(t), p(t)) ˙ = Lx,v(t, x(t), ¯ ˙
x(t)), ¯ (1.2.10) 
p(t) · ˙
x(t) ¯ − L(t, x(t), ¯ ˙
x(t)) ¯ = max
v∈Rn [p(t) · v − L(t, x(t), v) ¯ ]
and, in the case when L does not depend on t, 
p(t) · ˙
x(t) ¯ − L(x(t), ¯ ˙
x(t)) ¯ = c
for some constant c. 
To explore the qualitative properties of minimizers it is often helpful to reduce the 
Euler Lagrange condition to a system of specially structured first order differential 
equations. This was first achieved by Hamilton for Lagrangians L arising in 
mechanics. In this analysis the Hamiltonian, 
H (t, x, p) := max
v∈Rn
{p · v − L(t, x, v)},
has an important role. 
Suppose that the right side has a unique maximizer vmax . This will depend on 
(t, x, p) and so we can use it to define a function χ : [S,T ] × Rn × Rn → Rn: 
χ (t, x, p) := vmax.
Then 
H (t, x, p) = p · χ (t, x, p) − L(t, x, χ (t, x, p))
and 
∇v(p · v − L(t, x, v))|v=χ (t,x,p) = 0.
The last relationship implies 
p = Lv(t, x, χ (t, x, p)). (1.2.11) 
(The mapping from x vectors to p vectors implicit in this relationship, for fixed t, is 
referred to as the Legendre transformation.) 
Now consider an arc x¯ and associated costate arc p which satisfy the Euler 
Lagrange and Weierstrass conditions, namely 
(p(t), p(t)) ˙ = ∇x,vL(t, x(t), ¯ ˙
x(t)) ¯ (1.2.12)20 1 Overview
and 
p(t) · ˙
x(t) ¯ − L(t, x(t), ¯ ˙
x(t)) ¯ = max v∈Rn {p(t) · v − L(t, x(t), v) ¯ }.
Since it is assumed that the ‘maximizer’ in the definition of the Hamiltonian is 
unique, it follows from the Weierstrass condition that 
˙
x(t) ¯ = χ (t, x(t), p(t)). ¯
Fix t. Let us assume that χ (t, ., ., ) is differentiable. Then we can calculate the 
gradients of H (t, ., .): 
Hx (t, x, p)|x= ¯x(t),p=p(t)
= ∇x (p · χ (t, x, p) − L(t, x, χ (t, x, p)))|x= ¯x(t),p=p(t)
= p · χx (t, x, p) − Lx (t, x, χ (t, x, p))
−Lv(t, x, χ (t, x, p)) · χx (t, x, p)|x= ¯x(t),p=p(t)
= (p−Lv(t, x, χ (t, x, p))) · χx (t, x, p)−Lx (t, x, χ (t, x, p))|x= ¯x(t),p=p(t).
= 0 − ˙p(t).
(The last step in the derivation of these relations makes use of (1.2.11) and (1.2.12).) 
We have evaluated the x-derivative of H: 
Hx (t, x(t), p(t)) ¯ = −˙p(t).
As for the p-derivative, we have 
Hp(t, x, p)|x= ¯x(t),p=p(t)
= ∇p(p · χ (t, x, p) − L(t, x, χ (t, x, p)))|x= ¯x(t),p=p(t)
= χ (t, x, p) + p · χp(t, x, p)
−Lv(t, x, χ (t, x, p)) · χp(t, x, p)|x= ¯x(t),p=p(t)
= ˙
x(t) ¯ + (p(t) − Lv(t, x(t), ¯ ˙
x(t))) ¯ · χp(t, x(t), p(t)) ¯
= ˙
x(t) ¯ + 0.
Combining these relations, we arrive at the system of first order differential 
equations of interest, namely the Hamilton condition 
(− ˙p(t), ˙
x(t)) ¯ = Hx,p(t, x(t), p(t)). ¯ (1.2.13) 
So far, we have limited attention to the problem of minimizing an integral 
functional of arcs with fixed end-points. A more general problem is that in which 
the arcs are constrained to satisfy the boundary condition 
x(S) = x0 and x(T ) ∈ C, (1.2.14)1.2 The Calculus of Variations 21
for some specified point x0 ∈ Rn and subset C ⊂ Rn. (The fixed end-point problem 
is a special case, in which C is chosen to be {x1}.) The above necessary conditions 
remain valid when we pass to this more general end-point constraint, since a 
minimizer x¯ over arcs satisfying (1.2.14) is also a minimizer for the fixed end-point 
problem in which C = {¯x(T )}. But something more is required: replacing the fixed 
end-point constraint by other types of end-point constraint which allow end-point 
variation introduces extra degrees of freedom into the optimization problem, which 
should be reflected in supplementary necessary conditions. These conditions are 
conveniently expressed in terms of a boundary condition on the costate arc p: 
− p(T ) is an outward normal to C at x(T ). ¯
They are collectively referred to as the transversality condition, because they 
assert that a vector composed from end-points of the costate arc is orthogonal or 
‘transverse’ to the tangent hyperplane to C at x(T ) ¯ . 
The transversality condition too can be derived by considering suitable ‘vari￾ations’ (variations which, on this occasion, allow perturbations of the end-point 
values of the minimizer x¯). In favourable circumstances the transversality condition 
combines with the boundary condition (1.2.14) to supply the appropriate number of 
2n boundary conditions to accompany the system of 2n Eq. (1.2.13). 
The following theorem brings together these classical conditions and gives 
precise hypotheses under which they are satisfied. 
Theorem 1.2.1 Let x¯ be a minimizer for 
⎧
⎨
⎩
Minimize J (x) :=  T
S L(t, x(t), x(t))dt ˙
over absolutely continuous arcs x satisfying
x(S) = x0 and x(T ) ∈ C,
for some given interval [S,T ], function L : [S,T ] × Rn × Rn → R, point x0 ∈ Rn
and closed set C ⊂ Rn. We list the hypotheses 
(i) L(., x, v) is measurable for each (x, v) and L(t, ., .) is continuously differen￾tiable for each t ∈ [S,T ], 
(ii) there exists k ∈ L1 and β > 0, ϵ > 0 such that 
|L(t, x, v) − L(t, x'
, v)| ≤ (k(t) + β|v|)|x − x'
|
for all x, x' ∈ ¯x(t) + ϵB, v ∈ Rn,
(iii) L(t,x,.) is convex for all (t, x) ∈ [S,T ] × Rn, 
(iv) H(t,.,.) is continuously differentiable for each t ∈ [S,T ] where 
H (t, x, p) := sup
v∈Rn
{p · v − L(t, x, v)}.
Suppose that hypotheses (i) and (ii) are satisfied. Then there exists an absolutely 
continuous arc p : [S,T ] → Rn satisfying the following conditions:22 1 Overview
The Euler Lagrange Condition 
(p(t), p(t)) ˙ = Lx,v(t, x(t), ¯ ˙
x(t)) ¯ a.e.,
The Weierstrass Condition 
p(t) · ˙
x(t) ¯ − L(t, x(t), ¯ ˙
x(t)) ¯ = max
v∈Rn {p(t) · v − L(t, x(t), v) ¯ } a.e.,
The Transversality Condition 
− p(T ) ∈ NC(x(T )). ¯
If additionally hypotheses (iii) and (iv) are satisfied, then p can be chosen also to 
satisfy 
Hamilton’s Condition 
(− ˙p(t), ˙
x(t)) ¯ = Hx,p(t, x(t), p(t)) ¯ a.e. .
Finally, if the additional hypothesis 
(v) L(t, x, v) is independent of t, 
is imposed, then the above assertions can be strengthened to require that also that 
p satisfies 
Constancy of the Hamiltonian Condition 
p(t) · ˙
x(t) ¯ − L(t, x(t), ¯ ˙
x(t)) ¯ = c a.e.
for some constant c. 
The theorem is a special case of general necessary conditions which will be 
proved in Chap. 8. 
In this theorem, the transversality condition is expressed in a way that makes 
sense when C is assumed to be a closed set. It makes reference to the ‘limiting 
normal cone’ NC(x(T )) ¯ . This will be defined in the overview of nonsmooth analysis 
provided in Sect. 1.8. 
1.3 Existence of Minimizers and Tonelli’s Direct Method 
Deriving the Euler Lagrange condition and related conditions would appear to 
reduce the problem of finding a minimizer to one merely of solving a differential 
equation. But this is to overlook the fact that satisfaction of the Euler Lagrange 
condition is only a necessary condition for optimality; we cannot be certain, without 
further analysis, that an arc satisfying these conditions is truly a minimizer.1.3 Existence of Minimizers and Tonelli’s Direct Method 23
Early in the twentieth century, Tonelli recognized the significance of establishing 
the existence of minimizers before using necessary conditions of optimality to try 
to identify them. Tonelli’s direct method for obtaining a minimizer consists of the 
following steps: 
Step 1: Show that a minimizer exists. 
Step 2: Search among arcs satisfying the necessary conditions for an arc with 
lowest cost. 
These steps, when successfully carried out, are guaranteed to yield a minimizer. 
Notice however that, if we neglect Step 1, then Step 2 alone can be positively 
misleading. The point is illustrated by the following example: 
⎧
⎨
⎩
Minimize  1
0 x(t)x˙2(t)dt
over absolutely continuous arcs x satisfying
x(0) = 0 and x(1) = 0.
This example is not precisely matched to the necessary conditions of Theorem 1.2.1 
because the integrand L fails to satisfy hypothesis (ii). It can nevertheless be shown 
that any Lipschitz continuous minimizer x¯ satisfies the Euler Lagrange, Weierstrass 
and Transversality conditions. In this case the Euler Lagrange condition takes the 
form: 
2
d
dt

x(t) ¯ ˙
x(t) ¯  = ˙
x¯2(t) a.e. .
Obviously the arc 
x¯ ≡ 0
satisfies the Euler equation and the specified end-point conditions. It is in fact 
the unique Lipschitz continuous function so doing. To see this note that, for any 
Lipschitz continuous arc y which vanishes at the end-times and satisfies the Euler 
Lagrange Condition, we have 
d2
dt2 y2(t) = ˙y2(t) a.e. .
It follows that, for some constant c, 
d
dt y2(t) = c +
 t
0
y˙
2(s)ds for all t.
Since y vanishes at the left end-point and is assumed to have bounded slope, we 
deduce from this last condition that c = 0. If y /≡ 0 then y˙ must be nonzero on a set 
of positive measure, in order to satisfy the end-point constraints. But then24 1 Overview
y(1)
2 = 0 +
 1
0
 s
0
y˙
2(t) dsdt > 0 ,
in violation of the end-point constraints. We have confirmed that x¯ ≡ 0 is the unique 
Lipschitz continuous arc satisfying the Euler Lagrange condition. 
Notice, however, that x¯ is not a minimizer, not even in a local sense. Indeed 
by choosing the parameter ϵ > 0 sufficiently small we can arrange that the arc 
t → −ϵt(1 − t), which has cost strictly less than 0, is arbitrarily close to x¯ with 
respect to the supremum norm. 
To summarize, in this example there is a unique Lipschitz continuous arc 
satisfying the constraints of the problem and also the Euler Lagrange, Weierstrass 
and transversality conditions. Yet it is not a minimizer. This is a case in which a 
naive belief in the power of necessary conditions to identify minimizers leads us 
astray. The pathological feature of this example is, of course, that there are no 
minimizers in the class of Lipschitz continuous functions. The collection of arcs 
satisfying necessary conditions for a Lipschitz continuous arc to be a minimizer 
will not therefore contain a minimizer, even though it is non-empty. 
Clearly, it makes sense to speak of a minimizer only if we specify the class 
of functions from which minimizers are chosen. Unfortunately, hypotheses under 
which we can guarantee existence of minimizers in the ‘elementary’ function 
spaces (C1 functions, piecewise C1 functions, etc.) are, for many purposes, 
unacceptably restrictive. One of Tonelli’s most significant contributions in the 
calculus of variations was to show that, by enlarging the domain of (CV) to include 
W1,1([S,T ]; Rn) arcs, the existence of minimizers could be guaranteed under 
broad, directly verifiable hypotheses on the Lagrangian L. An existence theorem, 
in the spirit of Tonelli’s pioneering work in this field, is: 
Theorem 1.3.1 Consider the minimization problem 
⎧
⎨
⎩
Minimize  T
S L(t, x(t), x(t))dt ˙
over x ∈ W1,1([S,T ]; Rn) satisfying
x(S) = x0 and x(T ) = x1,
for given [S,T ] ⊂ R, L : [S,T ] × Rn × Rn → R, x0 ∈ Rn and x1 ∈ Rn. Assume 
that 
(i) L is continuous, 
(ii) L(t, x, .) is convex for all t ∈ [S,T ] and x ∈ Rn, 
(iii) there exist constants c > 0, d > 0 and α > 0 such that 
L(t, x, v) ≥ c|v|
1+α − d for all t ∈ [S,T ], x ∈ Rn and v ∈ Rn.
Then there exists a minimizer (in the class W1,1([S,T ]; Rn). 
A more general version of this theorem will be proved in Chap. 2. 
Nowadays, existence of minimizers is recognized as an important topic in its own 
right, which gives insights into whether variational problems have been properly1.4 Sufficient Conditions and the Hamilton Jacobi Equation 25
formulated and also into the regularity properties of minimizers. But, as we have 
already observed, existence of minimizers was first studied as an ingredient in the 
direct method, i.e. to justify seeking a minimizer from the class of arcs satisfying 
the necessary conditions. 
Unfortunately, existence theorems such as Theorem 1.3.1 are not entirely ade￾quate for purposes of applying the direct method. The difficulty is this: implicit in 
the direct method is the assumption that a single set of hypotheses on the data for the 
optimization problem at hand, simultaneously guarantees existence of minimizers 
and also validity of the necessary conditions. Yet for certain variational problems of 
interest, even though existence of a minimizer x¯ is guaranteed by Theorem 1.3.1, it is 
not clear a priori that the hypotheses are satisfied under which necessary conditions, 
such as those listed in Theorem 1.2.1, are valid at x¯. For these problems we cannot 
be sure that a search over arcs satisfying the Euler Lagrange condition and related 
conditions will yield a minimizer. 
There are two ways out of this dilemma. One is to refine the existence theory 
by showing that W1,1 minimizers are confined to some subset of W1,1 for which 
the standard necessary conditions are valid. The other is to replace the standard 
necessary conditions by conditions which are valid under hypotheses similar to 
those of existence theory. Tonelli’s achievements along these lines, together with 
recent significant developments, provide much of the subject matter for Chap. 12. 
1.4 Sufficient Conditions and the Hamilton Jacobi Equation 
We have discussed pitfalls in seeking minimizers among arcs satisfying the nec￾essary conditions. The steps initiated by Tonelli for dealing with them, namely 
investigating hypotheses under which minimizers exist, is a relatively recent 
development. They were preceded by various procedures, some of a rather ad hoc 
nature, for testing whether an arc satisfying known necessary conditions, which 
we have somehow managed to find, is truly a minimizer. For certain classes of 
variational problems, an arc x¯ satisfying the necessary conditions of Theorem 1.2.1 
is a minimizer (at least in some ‘local’ sense) if it can be shown to satisfy also certain 
higher order conditions (such as the strengthened Legendre or Jacobi conditions). 
In other cases we might hope to carry out calculations, ‘completing the square’ 
or construction of a ‘verification function’ for example, which make it obvious 
that an arc we have obtained and have reason to believe is a minimizer, is truly a 
minimizer. This ragbag of ‘indirect’ procedures, none of which may apply, contrasts 
with Tonelli’s direct method, which is in some sense a more systematic approach to 
finding minimizers. 
However one ‘indirect’ method for confirming that a given arc is a minimizer 
figures prominently in current research and we discuss it here. It is based on the 
relationship between, on the one hand, the variational problem26 1 Overview
(Q)
⎧
⎨
⎩
Minimize  T
S L(t, x(t), x(t))dt ˙ + g(x(T ))
over Lipschitz continuous arcs x satisfying
x(S) = x0 and x(T ) ∈ Rn
(in which L : [S,T ] × Rn × Rn → R and g : Rn → R are given continuous 
functions and x0 is a given n-vector) and, on the other, the Hamilton Jacobi partial 
differential equation: 
(HJE)
⎧
⎨
⎩
φt(t, x) + minv∈Rn {φx (t, x) · v + L(t, x, v)} = 0
for all t ∈ (S, T ), x ∈ Rn
φ(T , x) = g(x) for all x ∈ Rn.
We shall sometimes find it convenient to express the first of these equations in terms 
of the Hamiltonian H: 
H (t, x, p) := sup
v∈Rn
{p · v − L(t, x, v)}
thus 
−φt(t, x)+H (t, x, −φx (t, x)) = 0 for all t ∈ (S, T ), x ∈ Rn.
(It is helpful in the present context to consider a variational problem in which the 
right end-point is unconstrained and a function of the terminal value of the arc is 
added to the cost.) 
The following proposition summarizes the approach, which is referred to as 
Carathéodory’s verification technique : 
Proposition 1.4.1 Let x¯ be a Lipschitz continuous arc satisfying x(S) ¯ = x0. 
Suppose that a continuously differentiable function φ : R × Rn → R can be found 
satisfying (HJE) and also 
φx (t, x(t)) ¯ · ˙
x(t) ¯ + L(t, x(t), ¯ ˙
x(t)) ¯
= Minv∈Rn {φx (t, x(t)) ¯ · v + L(t, x(t), v) ¯ } a.e. t ∈ [S,T ].
Then x¯ is a minimizer for (Q) and 
φ(S, x0) = inf(Q).
Proof Take any Lipschitz continuous arc x : [S,T ] → Rn for which x(S) = x0. 
Then t → φ(t, x(t)) is a Lipschitz continuous function and 
dφ
dt (t, x(t)) = φt(t, x(t)) + φx (t, x(t)) · ˙x(t) a.e. .1.4 Sufficient Conditions and the Hamilton Jacobi Equation 27
Now express φ(t, x(t)) as the integral of its derivative: 
φ(S, x0) = −  T
S
d
dt φ(t, x(t))dt + g(x(T ))
(we have used the facts that x(S) = x0 and φ(T , x) = g(x)) 
= −  T
S
[φt(t, x(t)) + φx (t, x(t)) · ˙x(t) + L(t, x(t), x(t)) ˙ ]dt
+
 T
S
L(t, x(t), x(t))dt ˙ + g(x(T )).
But the first term on the right satisfies 
−
 T
S
[φt(t, x(t)) + φx (t, x(t)) · ˙x(t) + L(t, x(t), x(t)) ˙ ]dt
≤ −  T
S
[φt(t, x(t)) + inf
v {φx (t, x(t)) · v + L(t, x(t), v)}]dt
= 0. (1.4.1) 
It follows that 
φ(S, x0) ≤
 T
S
L(t, x(t), x(t))dt ˙ + g(x(T )). (1.4.2) 
Now repeat the above arguments with x¯ in place of x. Since, by hypothesis, 
φx (t, x(t)) ¯ · ˙
x(t) ¯ + L(t, x(t), ¯ ˙
x(t)) ¯ = minv∈Rn {φx (t, x(t)) ¯ · v + L(t, x(t), v) ¯ },
we can replace inequality (1.4.1) by equality: 
 T
S
[−φt(t, x(t)) ¯ + φx (t, x(t)) ¯ · ˙
x(t) ¯ + L(t, x(t), ¯ ˙
x(t)) ¯ ]dt = 0.
Consequently 
φ(S, x0) =
 T
S
L(t, x(t), ¯ ˙
x(t))dt ¯ + g(x(T )). ¯ (1.4.3) 
We see from (1.4.2) and (1.4.3) that x¯ has cost φ(S, x0) and, on the other hand, 
any other Lipschitz continuous arc satisfying the constraints of problem (Q) has 
cost not less than φ(S, x0). It follows that φ(S, x0) is the minimum cost and x¯ is a 
minimizer. ⨅⨆28 1 Overview
In favourable circumstances, it is possible to find a continuously differentiable 
solution φ to the Hamilton Jacobi equation such that the function 
v → φx (t, x) · v + L(t, x, v)
has a unique minimizer (write it d(t, x) to emphasize that it will depend on (t, x)) 
and the differential equation 
x(t) ˙ = d(t, x(t)) a.e.
x(0) = x0
has a Lipschitz continuous solution x¯. Then, Proposition 1.4.1 informs us, x¯ must 
be a minimizer. 
Success of this method for finding minimizers hinges of course on finding a 
solution φ to the Hamilton Jacobi equation. Recall that, if a solution φ confirms the 
optimality of some putative minimizer, then φ(S, x0) coincides with the minimum 
cost for (Q). (This is the final assertion of Proposition 1.4.1). It follows that the 
natural candidate for solution to the Hamilton Jacobi equation is the value function 
V : [S,T ] × Rn → R for (Q): 
V (t, x) = inf(Qt,x ) .
Here, the right side denotes the infimum cost of a variant of problem (Q), in which 
the initial data (S, x0) is replaced by (t, x): 
(Qt,x )
⎧
⎨
⎩
Minimize  T
t L(s, y(s), y(s))ds ˙ + g(y(T ))
over Lipschitz continuous functions y satisfying
y(t) = x.
The following proposition gives precise conditions under which the value function 
is a solution to (HJE): 
Theorem 1.4.2 Let V be the value function for (Q). Suppose that 
(i) V is a continuously differentiable function, 
(ii) For each (t, x) ∈ [S,T ]×Rn, the optimization problem (Qt,x ) has a minimizer 
which is continuously differentiable. 
Then V is a solution to (HJE). 
(Here and elsewhere, we say a vector valued function defined on a closed subset 
D of a finite dimensional linear space is continuously differentiable if it is the 
restriction to D of some continuously differentiable function on a neighbourhood 
of D.)1.4 Sufficient Conditions and the Hamilton Jacobi Equation 29
Proof Take (t, x) ∈ [S,T ] × Rn, τ ∈ [t,T ], a continuously differentiable function 
y : [t,T ] → Rn and a continuously differentiable minimizer y¯ for (Qt,x ). Simple 
contradiction arguments lead to the following relationships 
V (t, x) ≤
 τ
t
L(s, y(s), y(s))ds ˙ + V (τ, y(τ ))
V (t, x) =
 τ
t
L(s, y(s), ¯ ˙
y(s))ds ¯ + V (τ, y(τ )) ¯
Take arbitrary v ∈ Rn, ϵ ∈ (0, T − t) and choose y(s) = x + (s − t)v. Then 
ϵ−1[V (t + ϵ, x + ϵv) − V (t, x)] + ϵ−1
 t+ϵ
t
L(s, x + sv, v)ds ≥ 0.
and 
ϵ−1[V (t + ϵ, x +
 t+ϵ
t
˙
y(s)ds) ¯ − V (t, x)] + ϵ−1
 t+ϵ
t
L(s, y(s), ¯ ˙
y(s))ds ¯ = 0.
Passage to the limit as ϵ ↓ 0 gives 
Vt(t, x) + Vx (t, x) · v + L(t, x, v) ≥ 0
and 
Vt(t, x) + Vx (t, x) · ˙
y(t) ¯ + L(t, x, ˙
y(t)) ¯ = 0.
Bearing in mind that v is arbitrary, we conclude that 
Vt(t, x) + min
v∈Rn
{Vx (t, x) · v + L(t, x, v)} = 0.
We have confirmed that V satisfies (HJE). The fact that V also satisfies the boundary 
condition V (T , x) = g(x) follows directly from the definition of the value function. 
⨅⨆
How might we find solutions to the Hamilton Jacobi equation? One approach, 
‘construction of a field of extremals’, is inspired by the above interpretation of 
the value function. The idea is to generate a family of continuously differentiable 
arcs {yt,x }, parameterized by points (t, x) ∈ [S,T ] × Rn, satisfying the necessary 
conditions. We then choose 
φ(t, x) =
 T
t
L(t, yt,x (s), y˙t,x (s))ds + g(yt,x (T )).30 1 Overview
If it turns out that φ is a continuously differentiable function and the extremal yt,x
really is a minimizer for (Qt,x ) (a property of which we have no a priori knowledge), 
then V ≡ φ and φ is a solution to the Hamilton Jacobi equation. 
The Hamilton Jacobi equation is a nonlinear partial differential equation of 
hyperbolic type. From a classical perspective, constructing a field of extremals 
(a procedure for building up a solution to a partial differential equation from the 
solutions to a family of ordinary differential equations) amounts to solving the 
Hamilton Jacobi equation by the method of characteristics. 
1.5 The Maximum Principle 
A convenient framework for studying minimization problems, encountered in the 
optimal selection of flight trajectories and other areas of advanced engineering 
design and dynamic decision making, is to regard them as special cases of the 
problem: 
(P )
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize g(x(T )) +  T
S L(t, x(t), u(t))dt
over measurable functions u : [S,T ] → Rm and
arcs x ∈ W1,1([S,T ]; Rn) satisfying
x(t) ˙ = f (t, x(t), u(t)) a.e.,
u(t) ∈ U (t) a.e.,
x(S) = x0 and x(T ) ∈ C,
the data for which comprise an interval [S,T ], functions f : [S,T ] × Rn × Rm →
Rn, L : [S,T ] × Rn × Rm → R and g : Rn → R, a point x0 ∈ Rn, a set C ⊂ Rn, 
and {U (t) ⊂ Rm : S ≤ t ≤ T } is a given family of sets. 
This formulation of the dynamic optimization problem (or variants on it in which, 
for example, the cost depends also on the left end-point, or the end-points of the time 
interval [S,T ] are included among the choice variables, or pathwise constraints are 
imposed on values of x) is referred to as the Pontryagin formulation. The importance 
of this formulation is that it embraces a wide range of significant optimization 
problems which are beyond the reach of traditional variational techniques and, at 
the same time, it is very well suited to the derivation of general necessary conditions 
of optimality. 
In (P), the n-vector dependent variable x is called the state. The function 
describing its time evolution, x(t), S ≤ t ≤ T , is called the state trajectory. 
The state trajectory depends on our choice of control function u(t), S ≤ t ≤ T . 
The object is to choose a control function u to minimize the value of the cost 
g(x(T )) +  T
S L(t, x(t), u(t))dt resulting from our choice of u. 
There follows a statement of the maximum principle, whose discovery by L. S. 
Pontryagin et al. in the 1950’s was an important milestone in the emergence of 
dynamic optimization as a distinct field of research.1.5 The Maximum Principle 31
Theorem 1.5.1 (Maximum Principle) Let (x,¯ u)¯ be a minimizer for (P). Assume 
that 
(i): g is continuously differentiable on a neighbourhood of x(T ) ¯ , 
(ii): C is a closed set, 
(iii): (f, L) is continuous, (f, L)(t, ., u) is continuously differentiable for each 
(t, u) and there exist ϵ > 0, k ∈ L1 and c ∈ L1 such that 
|(f, L)(t, x, u)−(f, L)(t, x'
, u)| ≤ k(t)|x−x'
| and |(f, L)(t, x, u)| ≤ c(t)
for all x, x' ∈ ¯x(t) + ϵB and u ∈ U, a.e., 
(iv): U (t) ≡ U is a Borel set. 
Then there exist an arc p ∈ W1,1([S,T ]; Rn) and λ ≥ 0, not both zero, such that 
the following conditions are satisfied: 
The Costate Equation 
− ˙p(t) = p(t) · fx (t, x(t), ¯ u(t)) ¯ − λLx (t, x(t), ¯ u(t)), ¯ a.e.,
The Generalized Weierstrass Condition 
p(t) · f (t, x(t), ¯ u(t)) ¯ − λL(t, x(t), ¯ u(t)) ¯
= max
u∈U
{p(t) · f (t, x(t), u) ¯ − λL(t, x(t), u) ¯ } a.e.,
The Transversality Condition 
− p(T ) = λ∇g(x(T )) ¯ + η
for some η ∈ NC(x(T )) ¯ . 
The limiting normal cone NC of the right end-point constraint set C featuring in 
the above transversality condition will be defined presently; in the case that C is a 
smooth manifold it reduces to the set of outward pointing normals. 
A Maximum Principle for Problems with Functional Endpoint Constraints In many 
applications, the endpoint constraint C set in problem (P) takes the form of a 
collection of functional inequality and equality constraints, thus 
C = {x : φj (x) ≤ 0, j = 1,...,nφ and ψi(x) = 0, i = 1,...,nψ }, (1.5.1) 
in which φ : Rn → Rnφ and ψ : Rn → Rnψ are given C1 functions. Can we, in this 
case, replace the transversality condition by a condition expressed directly in terms 
the constraint functions φ and ψ? We show how this can be done.32 1 Overview
Define the index set of active constraints 
I (x)¯ := {j : φj (x(T )) ¯ = 0}
and the ‘multiplier domain’ for the system of functional constraints defining C: 
Λ(x(T )) ¯ := {(λφ, λψ ) ∈ (R+)
nφ × Rnψ : λφ
j = 0 for all j /∈ I (x(T )) ¯ }.
Now assume the non-degeneracy condition 
λφ · φx (x(T )) ¯ + λψ · ∇ψ(x(T )) ¯ = 0
(λφ, λψ ) ∈ Λ(x(T )) ¯

=⇒ (λφ, λψ ) = 0 , (1.5.2) 
which requires the gradients of the active inequality constraint function components 
and all the equality constraint function components to be linearly independent 
(‘positively’ linearly independent w.r.t. to the active inequality constraint function 
components.) Then it can be shown that 
NC(x(T )) ¯ ⊂ {λφ · ∇φ(x(T )) ¯ + λψ · ∇ψ(x(T )) ¯ : (λφ, λψ ) ∈ Λ(x(T )) ¯ }.
These observations lead to the following version of the maximum principle: 
Theorem 1.5.2 (Maximum Principle for Functional Endpoint Constraints) Let 
(x,¯ u)¯ be a minimizer for (P) in the special case when C has the representation 
(1.5.1) for some C1 functions φ : Rn → Rnφ and ψ : Rn → Rnψ . Assume that 
(i): g is continuously differentiable on a neighbourhood of x(T ) ¯ , 
(ii): (f, L) is continuous, (f, L)(t, ., u) is continuously differentiable for each 
(t, u) and there exist ϵ > 0, k ∈ L1 and c ∈ L1 such that 
|(f, L)(t, x, u)−(f, L)(t, x'
, u)| ≤ k(t)|x−x'
| and |(f, L)(t, x, u)| ≤ c(t)
for all x, x' ∈ ¯x(t) + ϵB and u ∈ U, a.e. t ∈ [S,T ], 
(iii): U (t) ≡ U is a Borel set. 
Then there exist an arc p ∈ W1,1([S,T ]; Rn), λ ≥ 0 and (λφ, λψ ) ∈ Λ(x(T )) ¯ such 
that 
(a): (p, λ, λφ, λψ ) /= (0, 0, 0, 0), 
(b): − ˙p(t) = p(t) · fx (t, x(t), ¯ u(t)) ¯ − λLx (t, x(t), ¯ u(t)), ¯ a.e. t ∈ [S,T ], 
(c): p(t) · f (t, x(t), ¯ u(t)) ¯ − λL(t, x(t), ¯ u(t)) ¯ =
maxu∈U {p(t) · f (t, x(t), u) ¯ − λL(t, x(t), u) ¯ } a.e. t ∈ [S,T ], 
(d): −p(T ) = λgx (x(T )) ¯ + λφ · φx (x(T )) ¯ + λψ · ψx (x(T )) ¯ . 
(The validity of maximum principle in this form follows from the earlier stated 
maximum principle (Theorem 1.5.1) has been established in the preceding analysis, 
in the case when the non-degeneracy condition (1.5.2) is satisfied, If the data violates1.5 The Maximum Principle 33
the non-degeneracy condition, however, there exists (λ¯ φ, λ¯ ψ ) ∈ Λ(x(T )) ¯ such that 
(λ¯ φ, λ¯ ψ ) /= (0, 0); in this case the assertions of the theorem are still true, in a trivial 
sense, with p ≡ 0, λ = 0 and (λφ, λφ) = (λ¯ φ, λ¯ ψ ).) 
A self contained proof of the maximum principle for functional endpoint 
constraints is given in the Appendix to this chapter. 
Alternative statements of the maximum principle (either form) can be given in 
terms of the un-maximized Hamiltonian 
Hλ(t, x, p, u) := p · f (t, x, u) − λL(t, x, u).
The costate equation (augmented by the state equation x˙ = f ) and the generalized 
Weierstrass condition can be written 
(− ˙p(t), ˙
x(t)) ¯ = Hλ
x,p(t, x(t), p(t), ¯ u(t)) ¯ a.e.
and 
Hλ(t, x(t), p(t), ¯ u(t)) ¯ = max
u∈U
Hλ(t, x(t), p(t), u) ¯ a.e.,
a form of the conditions which emphasizes their affinity with Hamilton’s system of 
equations in the calculus of variations. 
In favorable circumstances, we are justified in setting the cost multiplier λ = 1, 
and the generalized Weierstrass condition permits us to express u as a function of x
and p
u = u∗(x, p).
The maximum principle then asserts that a minimizing state trajectory x¯ is the 
first component of a pair of absolutely continuous functions (x, p) ¯ satisfying the 
differential equation 
(− ˙p(t), ˙
x(t)) ¯ = Hλ=1
x,p (t, x(t), p(t), u ¯ ∗(x(t), p(t))) ¯ a.e. (1.5.3) 
together with the end-point conditions 
x(S) ¯ = x0, x(T ) ¯ ∈ C
and, for the general maximum principle Theorem 1.5.1, 
− p(T ) ∈ gx (x(T )) ¯ + NC(x(T )) . ¯
The minimizing control is given by the formula 
u(t) ¯ = u∗(x(t), p(t)). ¯34 1 Overview
Notice that the (vector) differential Eq. (1.5.3) is a system of 2n scalar, first order 
differential equations for (p, x)¯ . If C is a (n−k)-dimensional manifold, specified by 
k scalar functional constraints on the end-points of x¯, then the endpoint constraints 
on state trajectories impose n+k boundary conditions. The transversality condition 
on the right end-point of p imposes a further n − k conditions. Thus the ‘two point 
boundary value problem’ which we must solve to obtain (p, x)¯ and p has the ‘right’ 
number of end-point conditions, namely n + k + (n − k) = 2n. 
To explore the relationship between the maximum principle and classical 
conditions in the calculus of variations, let us consider the following refinement 
of the basic problem of the calculus of variations, in which a pathwise constraint on 
the velocity variable ‘x(t) ˙ ∈ U’ is imposed. (U is a given Borel subset of Rn.) 
⎧
⎪⎪⎨
⎪⎪⎩
Minimize  T
S L(t, x(t), x(t))dt ˙
over arcs xsatisfying
x(t) ˙ ∈ U,
x(S) = x0 and x(T ) ∈ C .
(1.5.4) 
This problem will be recognized as a special case of (P), in which the dynamic and 
pathwise control constraints are x(t) ˙ = u(t) and u(t) ∈ U, respectively. 
Let x¯ be a minimizer for (1.5.4). Under appropriate hypotheses, the maximum 
principle supplies λ ≥ 0 and a costate arc q satisfying 
(q, λ) /= (0, 0), (1.5.5) 
− ˙q(t) = −λLx (t, x(t), ¯ ˙
x(t)), ¯ (1.5.6) 
q(T ) = ξ '
for some ξ ' ∈ NC(x(T )) ¯ , and 
q(t) · ˙
x(t) ¯ − λL(t, x(t), ¯ ˙
x(t)) ¯ = max
v∈U
{q(t) · v − λL(t, x(t), v) ¯ }. (1.5.7) 
We now impose the ‘normality condition’: 
(CQ): λ can be chosen strictly positive. 
Two special cases, in either of which (CQ) is automatically satisfied, are 
(i) ˙
x(t) ¯ ∈ int U on a subset of [S,T ] having positive measure, 
(ii) x(T ) ¯ ∈ int C. 
(It is left to the reader to check that, if the condition λ > 0 is violated, then, in either 
case, (1.5.6) and (1.5.7) imply (q, λ) = (0, 0), in contradiction of (1.5.5).) 
In terms of p(t) := λ−1q(t) and ξ := λ−1ξ '
, these conditions can be expressed 
p(t) ˙ = Lx (t, x(t), ¯ ˙
x(t)) ¯ a.e. (1.5.8) 
−p(T ) = ξ , (1.5.9)1.5 The Maximum Principle 35
for some ξ ∈ NC(x(T )) ¯ and 
p(t) · ˙
x(t) ¯ − L(t, x(t), ¯ ˙
x(t)) ¯ = max
v∈U
{p(t) · v − L(t, x(t), v) ¯ }. (1.5.10) 
(We have used the fact that NC(x(T )) ¯ is a cone). When U = Rn, the Generalized 
Weierstrass Condition (1.5.10) implies 
p(t) = Lv(t, x(t), ¯ ˙
x(t)) ¯ a.e..
(Lv denotes the derivative of L(t, x, .).) This combines with (1.5.8) to give 
(p(t), p(t)) ˙ = Lx,v(t, x(t), ¯ ˙
x(t)) ¯ a.e..
We have shown that the maximum principle subsumes the Euler Lagrange condition 
and the Weierstrass condition. But is has much wider implications, because it covers 
problems with pathwise velocity constraints. 
The innovative aspects of the maximum principle, in relation to classical 
optimality conditions, are most clearly revealed when it is compared with the 
Hamilton condition. For Problem (1.5.4), the maximum principle associates with 
a minimizer x¯ a costate arc p satisfying relationships (1.5.8)–(1.5.10) above. These 
can be expressed in terms of the un-maximized Hamiltonian 
H(t, x, p, v) = p · v − L(t, x, v)
as 
(− ˙p(t), ˙
x(t)) ¯ = Hx,p(t, x(t), p(t), ¯ ˙
x(t)) ¯
and 
H(t, x(t), p(t), ¯ ˙
x(t)) ¯ = max
v∈U
{H(t, x(t), p(t), v) ¯ }.
On the other hand, classical conditions in the form of Hamilton’s equations, which 
are applicable in the case U = Rn are also a set of differential equations satisfied 
by x¯ and a costate arc p
(− ˙p(t), ˙
x(t)) ¯ = Hx,p(t, x(t), p(t)) , ¯
where H is the Hamiltonian 
H (t, x, p) = max
v∈Rn
{p · v − L(t, x, v)}.
Both sets of necessary conditions are intimately connected with the function H
and its supremum H with respect to the velocity variable. However there is a36 1 Overview
crucial difference. In Hamilton’s classical necessary conditions, the supremum￾taking operation is applied before the function H is differentiated and inserted into 
the vector differential equation for p and x¯. In the maximum principle applied 
to Problem (1.5.4), by contrast, supremum-taking is carried out only after the 
differential equations for p and x¯ have been assembled. 
One advantage of postponing supremum taking is that we can dispense with 
differentiability hypotheses on the Hamiltonian H (t, ., .). (If the data L(t, ., .) is 
differentiable, it follows immediately that H(t, ., ., .) is differentiable, as required 
for derivation of the maximum principle, but it does not follow in general that 
the derived function H (t, ., .) is differentiable, as required for derivation of the 
Hamilton condition.) 
Other significant advantages are that conditions (1.5.8)–(1.5.10) are valid in 
circumstances when the velocity x(t) ¯ is constrained to lie in some set U, and 
that they may be generalized to cover problems involving differential equation 
constraints. These are the features which make the maximum principle so well 
suited to problems in advanced engineering design, economics and elsewhere. 
1.6 Dynamic Programming 
We have seen how necessary conditions from the calculus of variations evolved 
into the maximum principle, to take account of pathwise constraints encountered in 
advanced engineering design and dynamic decision making. What about optimality 
conditions related to the Hamilton Jacobi equation? Here, too, long-established 
techniques in the calculus of variations can be adapted to cover present day 
applications. 
It is convenient to discuss these developments, referred to as dynamic program￾ming, in relation to the special case of the problem studied earlier, in which the right 
end-point of the state trajectory is unconstrained. 
(I )
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize J (x) :=  T
S L(t, x(t), u(t))dt + g(x(T ))
over measurable functions u : [S,T ] → Rm
and x ∈ W1,1([S,T ]; Rn) satisfying
x(t) ˙ = f (t, x(t), u(t)) a.e.,
u(t) ∈ U a.e.,
x(S) = x0,
the data for which comprise an interval [S,T ], a set U ⊂ Rm, a point x0 ∈ Rn
and functions f : [S,T ] × Rn × Rm → Rn, L : [S,T ] × Rn × Rm → R and 
g : Rn → R. 
It is assumed that f , L and g are continuously differentiable functions and that f
satisfies additional assumptions ensuring that, in particular, the differential equation 
y(s) ˙ = f (s, y(s), u(s)), y(t) = ξ , has a unique solution y on [t,T ] for an arbitrary 
control function u, initial time t ∈ [S,T ] and initial state ξ ∈ Rn.1.6 Dynamic Programming 37
The Hamilton Jacobi equation for this problem is: 
(HJE)'
⎧
⎨
⎩
φt(t, x) + minu∈U {φx(t, x) · f (t, ξ , u) + L(t, ξ , u)} = 0
for all (t, x) ∈ (S, T ) × Rn,
φ(T , x) = g(x) for all x ∈ Rn.
Finding a suitable solution to this equation is one possible approach to verifying 
optimality of a putative minimizer. This extension of Carathéodory’s verification 
technique for problem (I ) is summarized as the following optimality condition: the 
derivation is along very similar lines to the proof of Proposition 1.4.1. 
Theorem 1.6.1 Let (x,¯ u)¯ satisfy the constraints of the dynamic optimization 
problem (I ). Suppose that there exists φ ∈ C1 satisfying (HJE) ' and also 
φx (t, x(t)) ¯ · f (t, x(t), ¯ u(t)) ¯ + L(t, x(t), ¯ u(t)) ¯
= min
u∈U
{φx (t, x)¯ · f (t, x(t), u) ¯ + L(t, x(t), u) ¯ }. (1.6.1) 
Then 
(a) (x,¯ u)¯ is a minimizer for (I ), 
(b) φ(S, x0) is the minimum cost for (I ). 
The natural candidate for ‘verification function’ φ is the value function V :
[S,T ] × Rn → R for (I ). For (t, x) ∈ [S,T ] × Rn we define 
V (t, x) := inf(It,x )
where the right side indicates the infimum cost for a modified version of (I ), in 
which the initial time and state (S, x0) is replaced by (t, x): 
(It,x )
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize  T
t L(s, y(s), u(s))ds + g(x(T ))
over measurable functions u : [t,T ] → Rm
and y ∈ W1,1([t,T ]; Rn) satisfying
y(s) ˙ = f (s, y(s), u(s)) a.e. s ∈ [t,T ],
u(s) ∈ U a.e. s ∈ [t,T ],
x(t) = x.
Indeed, we can mimic the proof of Theorem 1.4.2 to show 
Theorem 1.6.2 Let V be the value function for (I ). Suppose that 
(i) V is a continuously differentiable function, 
(ii) For each (t, x) ∈ [S,T ] × Rn, the optimization problem (It,x ) has a minimizer 
with continuous control function. 
Then V is a solution to (HJE).38 1 Overview
One reason for the special significance of the value function in dynamic 
optimization is its role in the solution of the ‘optimal synthesis problem’. A feature 
of many engineering applications of dynamic optimization is that knowledge of an 
optimal control strategy is required for a variety of initial states and times. This is 
because the initial state typically describes the deviation of plant variables from their 
nominal values, due to disturbances; the requirements of designing a control system 
to correct this deviation, regardless of the magnitudes and time of occurrence of the 
disturbances, make necessary consideration of more than one initial state and time. 
The optimal synthesis problem is that of obtaining a solution to (It,x ) in feedback 
form such that the functional dependence involved is independent of the data point 
(t, x) ∈ [S,T ] × Rn. To be precise, we seek a function G : [S,T ] × Rn → Rm
such that the feedback equation 
u(s) = G(s, y(s)),
together with the dynamic constraints and initial condition, 

y(s) ˙ = f (s, y(s), u(s)) for a.e. s ∈ [τ,T ],
y(τ ) = ξ
can be solved to yield a minimizer (y, u) for (Iτ,ξ ). Furthermore, it is required that 
G is independent of (τ, ξ ). We mention that feedback implementation, which is a 
dominant theme in control engineering, has many significant benefits besides the 
scope it offers for treatment of arbitrary initial data. 
Consider again the dynamic programming sufficient condition of optimality. Let 
φ be a continuously differentiable function that satisfies (HJE)'
. For each (t, x)
define the set Q(s, x) ⊂ Rm
Q(t, x) := {v' : φx (t, x) · f (t, x, v'
) + L(t, x, v'
) =
min
v∈U
{φx (t, x) · f (t, x, v) + L(t, x, v)}.
Let us assume that Q is single valued and that, for each (t, x) ∈ [S,T ] × Rn, the 
equations 
y(s) ˙ = f (s, y(s), u(s)) a.e. s ∈ [t,T ],
u(s) = Q(s, y(s)) a.e. s ∈ [t,T ],
y(t) = x
have a solution y on [t,T ]. Then, by the sufficient condition, (y, u(s) = Q(s, x(s))
is a minimizer for (It,x ), whatever the initial data point (t, x) happens to be. 
Evidently, Q solves the synthesis problem. 
We next discuss the relationship between dynamic programming and the max￾imum principle. The question arises, if φ is a solution to (HJE)' and if (x,¯ u¯) is a 
minimizer for (I ), can we interpret costate arcs for (x,¯ u)¯ in terms of φ? The nature 
of the relationship can be surmised from the following calculations.1.6 Dynamic Programming 39
Assume that φ is twice continuously differentiable. We deduce from (HJE)' that, 
for each t ∈ [S,T ], 
φx (t, x(t)) ¯ · f (t, x(t), ¯ u(t)) ¯ + L(t, x(t), ¯ u(t)) ¯
= min
v∈U
{φx (t, x(t)) ¯ · f (t, x(t), v) ¯ + L(t, x(t), v) ¯ } (1.6.2) 
and 
φt(t, x(t)) ¯ + φx (t, x(t)) ¯ · f (t, x(t), ¯ u(t)) ¯ + L(t, x(t), ¯ u(t)) ¯ = 0 . (1.6.3) 
Since φ is assumed to be twice continuously differentiable, we deduce from (1.6.3) 
that 
φtx + φxx · f + φx · fx + Lx = 0. (1.6.4) 
(In this equation, all terms are evaluated at (t, x(t), ¯ u(t)) ¯ .) 
Now define the continuously differentiable functions p and h as follows: 
p(t) := −φx (t, x(t)), h(t) ¯ := φt(t, x(t)). ¯ (1.6.5) 
We have 
− ˙p(t) = φtx (t, x(t)) ¯ + φxx (t, x(t)) ¯ · f (t, x(t), ¯ u(t)) ¯
= −φx (t, x(t)) ¯ · fx (t, x(t), ¯ u(t)) ¯ − Lx (t, x(t), ¯ u(t)), ¯
by (1.6.4). It follows from (1.6.5) that 
− ˙p(t) = p(t) · fx (t, x(t), ¯ u(t)) ¯ − Lx (t, x(t), ¯ u(t)) ¯ for all t ∈ [S,T ]. (1.6.6) 
We deduce from the boundary condition for (HJE)' and the definition of p that 
− p(T ) ( = φx (T , x(T )) ¯ ) = ∇g(x(T )). ¯ (1.6.7) 
From (1.6.2) and (1.6.5) 
p(t) · f (t, x(t), ¯ u(t)) ¯ − L(t, x(t), ¯ u(t)) ¯
= max
v∈U
{p(t) · f (t, x(t), v) ¯ − L(t, x(t), v) ¯ }. (1.6.8) 
We deduce from (1.6.3) and (1.6.8) that 
h(t) = H (t, x(t), p(t)) ¯ (1.6.9)40 1 Overview
where, as usual, H is the Hamiltonian 
H (t, x, p) := max
v∈U
{p(t) · f (t, x(t), v) ¯ − L(t, x(t), v) ¯ }.
Conditions (1.6.6)–(1.6.9) tell us that, when a smooth verification function φ exists, 
then a form of the maximum principle is valid, in which the costate arc p is related 
to the gradient ∇φ of φ according to 
(H (t, x(t), p(t)), ¯ −p(t)) = ∇φ(t, x(t)) ¯ for all t ∈ [S,T ]. (1.6.10) 
As a proof of the maximum principle, the above analysis is deficient because the 
underlying hypothesis that there exists a smooth verification function is difficult to 
verify outside simple special cases. 
However the relationship (1.6.10) is of considerable interest, for the following 
reasons. It is sometimes useful to know how the minimum cost of a dynamic 
optimization problem depends on parameter values. In engineering applications 
for example, such knowledge helps us to assess the extent to which optimal 
performance is degraded by parameter drift (variation of parameter values due to 
ageing of components, temperature changes, etc.). If the parameters comprise the 
initial time and state components (t, x), the relevant information is supplied by the 
value function which, under favourable circumstances, can be obtained by solving 
the Hamilton Jacobi equation. However for high dimensional, nonlinear problems 
it is usually not feasible to solve the Hamilton Jacobi equation. On the other 
hand, computational schemes are available for calculating pairs of functions (x,¯ u)¯
satisfying the conditions of the maximum principle and accompanying costate arcs 
p, even for high dimensional problems. Of course the costate arc is calculated for 
the specified initial data (t, x) = (S, x0) and does not give a full picture of how the 
minimum cost depends on the initial data in general. The costate arc does however 
at least supply gradient (or ‘sensitivity’) information about this dependence near the 
nominal initial data (t, x) = (S, x0): in situations where there is a unique costate 
arc associated with a minimizer and the value function V is a smooth solution to the 
Hamilton Jacobi equation, we have from (1.6.10) that 
∇V (S, x0) = (H (S, x0, p(S)), −p(S)) .
A major weakness of classical dynamic programming, as summarized above, 
is that it is based on the hypothesis that the value function is continuously 
differentiable. (For some aspects of the classical theory, even stronger regularity 
properties must be invoked.) This hypothesis is violated in many cases of interest. 
The historical development of necessary conditions of optimality and of dynamic 
programming in dynamic optimization could not be more different. Necessary 
conditions made a rapid start, with the early conceptual breakthroughs involved in 
the formulation and rigorous derivation of the maximum principle. The maximum 
principle remains a key optimality condition to this day. Dynamic programming, by1.7 Nonsmoothness 41
contrast, was a slow beginner. Early developments (in part reviewed above) were 
fairly obvious extensions of well-known techniques in the calculus of variations. In 
the 1960’s dynamic programming was widely judged to lack a rigorous foundation: 
it aimed to provide a general approach to the solution of dynamic optimization prob￾lems yet dynamic programming, as originally formulated, depended on regularity 
properties of the value function which frequently it did not possess. The conceptual 
breakthroughs, required to elevate the status of dynamic programming from that of 
a useful heuristic tool to a cornerstone of modern dynamic optimization, did not 
occur until the 1970’s. They involved a complete re-appraisal of what we mean by 
‘solution’ to the Hamilton Jacobi equation. 
1.7 Nonsmoothness 
By the early 1970’s, it was widely recognized that attempts to broaden the 
applicability of available ‘first order’ necessary conditions and also to put dynamic 
programming on a rigorous footing were being impeded by a common obstacle: 
a lack of suitable techniques for analysing local properties of non-differentiable 
functions and of sets with non-differentiable boundaries. 
Consider first dynamic programming, where the need for new analytic techniques 
is most evident. This focuses attention on the relationship between dynamic 
optimization problems such as 
(I )
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize J (x) :=  T
S L(t, x(t), x(t))dt ˙ + g(x(T ))
over measurable functions u : [S,T ] → Rm
and absolutely continuous arcs x satisfying
x(t) ˙ = f (t, x(t), u(t)) a.e.,
u(t) ∈ U a.e.,
x(S) = x0
and the Hamilton Jacobi equation 
(HJE)'
⎧
⎨
⎩
φt(t, x) + minu∈U {φx (t, x) · f (t, x, u) + L(t, x, u))} = 0
for all (t, x) ∈ (S, T ) × Rn,
φ(T , x) = g(x) for all x ∈ Rn.
The elementary theory tells us that, if the value function V is continuously differ￾entiable (and various other hypotheses are satisfied) then V is a solution to (HJE)'
. 
One shortcoming is that it does not exclude the existence of other solutions to the 
(HJE)' and, therefore, does not supply a full characterization of the value function 
in terms of solutions to (HJE)'
. Another, more serious, shortcoming is the severity 
of the hypothesis ‘V is continuously differentiable’, which is extensively invoked42 1 Overview
in the elementary analysis. In illustration of problems in which this hypothesis is 
violated, consider the following example: 
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize x(1)
over measurable functions u : [0, 1] → R
and x ∈ W1,1([0, 1]; R) satisfying
x(t) ˙ = xu a.e.,
u(t) ∈ [−1, +1] a.e.,
x(0) = 0.
The Hamilton Jacobi equation in this case takes the form 

φt(t, x) − |φx (t, x)x| = 0 for all (t, x) ∈ (0, 1) × R,
φ(1, x) = x for all x ∈ R.
The value function is 
V (t, x) =

xe−(1−t) if x ≥ 0
xe+(1−t) if x < 0.
We see that V satisfies the Hamilton Jacobi equation on {(t, x) ∈ (0, 1) × R :
x /= 0}. However V cannot be said to be a classical solution because V is non￾differentiable on the subset {(t, x) ∈ (0, 1) × R : x = 0}. 
What is significant about this example is that the non-differentiability of the 
value function here encountered is by no means exceptional. It is a simple instance 
of a kind of dynamic optimization problem frequently encountered in engineering 
design, in which the dynamic constraint 
x˙ = f (t, x, u)
has right side affine in the u variable, the control variable is subject to simple 
magnitude constraints and the cost depends only on the terminal value of the state 
variable; for these problems, we expect the value function to be non-differentiable. 
How can the difficulties associated with non-differentiable value functions be 
overcome? Of course we can choose to define non-differentiable solutions of a given 
partial differential equation in a number of different ways. However the particular 
challenge in dynamic programming is to come up with a definition of solution, 
according to which the value function is the unique solution. 
Unfortunately we must reject traditional interpretations of non-differentiable 
solution to partial differential equations based on distributional derivatives, since 
the theory of ‘distribution sense solutions’ is essentially a linear theory which is ill￾matched to the nonlinear Hamilton Jacobi equation. It might be thought, on the other 
hand, that a definition based on ‘almost everywhere’ satisfaction of the Hamilton1.7 Nonsmoothness 43
Jacobi equation would meet our requirements. But such a definition is simply too 
coarse to provide a characterization of the value function under hypotheses of any 
generality. 
Clearing the ‘nondifferentiable value functions’ bottleneck in dynamic program￾ming, encountered in the late 1970’s, was a key advance in dynamic optimization. 
Success in relating the value function and the Hamilton Jacobi equation was 
ultimately achieved under hypotheses of considerable generality by introducing new 
solution concepts, ‘viscosity’ solutions and related notions of a generalized solution, 
based on a fresh look at local approximation of nondifferentiable functions. 
The need to ‘differentiate the un-differentiable’ arises also when we attempt to 
derive necessary conditions for dynamic optimization problems not covered by the 
maximum principle. Prominent among these are problems 
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize g(x(T ))
over measurable functions u : [S,T ] → Rm
and x ∈ W1,1([S,T ]; Rn) satisfying
x(t) ˙ = f (t, x(t), u(t)) a.e.,
u(t) ∈ U (t, x(t)) a.e.,
x(S) = x0 and x(T ) ∈ C ,
(1.7.1) 
involving a state dependent control constraint set U (t, x) ⊂ Rm. Problems 
with state dependent control constraint sets are encountered in flight mechanics 
applications where, for example, the control variable u(t) is the engine thrust. For 
large excursions of the state variables, it is necessary to take account of the fact 
that the upper and lower bounds on the thrust depend on atmospheric pressure and 
therefore on altitude. Since altitude is a state component, the control function is 
required to satisfy a constraint of the form 
u(t) ∈ U (t, x)
in which 
U (t, x) := {u : a−(t, x) ≤ u ≤ a+(t, x)]
and a−(t, x) and a+(t, x) are respectively the state dependent lower and upper 
bounds on the thrust at time t. In this simple case, it is possible to reduce to 
the state-independent control constraint case, by re-defining the problem. (The 
state dependence of the control constraint set can be absorbed into the dynamic 
constraint.) But in other cases, involving several interacting control variables, each 
subject to state-dependent constraints, this may no longer be possible or convenient. 
A natural framework for studying problems with state dependent control con￾straints is provided by:44 1 Overview
⎧
⎪⎪⎨
⎪⎪⎩
Minimize g(x(T ))
over arcs x ∈ W1,1 satisfying
x(t) ˙ ∈ F (t, x(t)) a.e.,
x(S) = x0 and x(T ) ∈ C .
(1.7.2) 
Now, the dynamic constraint takes the form of a differential inclusion: 
x(t) ˙ ∈ F (t, x(t)) a.e.
in which, for each (t, x), F (t, x) is a given subset of Rn. The perspective here is that, 
in dynamic optimization, the fundamental choice variables are state trajectories, 
not control function/state trajectory pairs, and the essential nature of the dynamic 
constraint is made explicit by identifying the set F (t, x(t)) of allowable values of 
x(t) ˙ . The earlier problem, involving a state dependent control constraint set can be 
fitted to this framework by choosing F to be the multifunction 
F (t, x) := {v = f (t, x, u) : u ∈ U (t, x)}. (1.7.3) 
Indeed it can be shown, under mild hypotheses on the data for problem (1.7.1), that 
the two problems (1.7.1) and (1.7.2) are equivalent for this choice of F in the sense 
that x¯ is a minimizer for (1.7.2) if and only if (x,¯ u)¯ is a minimizer for (1.7.1) (for 
some u¯). 
By what means can we derive necessary conditions for problem (1.7.2)? One 
approach is to seek necessary conditions for a related problem, from which the 
dynamic constraint has been eliminated by means of a penalty function: 
⎧
⎨
⎩
Minimize  T
S L(t, x(t), x(t))dt ˙ + g(x(T ))
over x ∈ W1,1([S,T ]; Rn) satisfying
x(S) = x0 and x(T ) ∈ C .
(1.7.4) 
Here 
L(t, x, v) := 
0 if v ∈ F (t, x)
+∞ if v /∈ F (t, x).
(Notice that problems (1.7.2) and (1.7.4) have the same cost at an arc x satisfying 
the constraints of (1.7.2). On the other hand, if x violates the constraints of (1.7.2), 
it is excluded from consideration as a minimizer for (1.7.4), since the penalty term 
in the cost ensures that it has infinite cost.) 
We have arrived at a problem in the calculus of variations, albeit one with 
discontinuous data. It is natural then to seek necessary conditions for a minimizer 
x¯ in the spirit of classical conditions, including the Euler Lagrange and Hamilton 
conditions: 
(p(t), p(t)) ˙ ∈ ‘∂x,v’ L(t, x(t), ¯ ˙
x(t)) ¯1.8 Nonsmooth Analysis 45
and 
(− ˙p(t), ˙
x(t)) ¯ ∈ ‘∂x,p’ H (t, x(t), p(t)) ¯
for some p. Here 
H (t, x, p) := max
v∈Rn
{p · v − L(t, x, v)}.
These conditions involve derivatives of the functions L and H. Yet, in the present 
context, L is discontinuous and H is, in general, nondifferentiable. How then should 
these relationships be interpreted? 
These difficulties were decisively overcome in the 1970’s and a new chapter 
of dynamic optimization was begun, with the introduction by F. H. Clarke, in his 
seminal paper [57], of new concepts of local approximation of non-differentiable 
functions and of a supporting calculus for applications in variational analysis. 
1.8 Nonsmooth Analysis 
Nonsmooth analysis is a branch of nonlinear analysis concerned with the local 
approximation of non-differentiable functions and of sets with non-differentiable 
boundaries. Since its inception in the early 1970’s, there has been a sustained and 
fruitful interplay between nonsmooth analysis and dynamic optimization. A famil￾iarity with nonsmooth analysis is, therefore, essential for an in-depth understanding 
of present day research in dynamic optimization. This section provides a brief, 
informal review of material on nonsmooth analysis, which will be covered in far 
greater detail in Chaps. 4 and 5. 
The guiding question is: 
How should we adapt classical concepts of outward normals to subsets 
of vector spaces with smooth boundaries and of gradients of differentiable 
functions, to cover situations in which the subsets have nonsmooth boundaries 
and the functions are nondifferentiable? 
There is no single answer to this question. The need to study local approxi￾mations of nonsmooth functions arises in many branches of analysis and a wide 
repertoire of techniques has been developed to meet the requirements of different 
applications. 
In this book emphasis is given to proximal normal vectors, proximal subgradients 
and limits of such vectors originating in Clarke’s early work [57] . This reflects their 
prominence in applications to dynamic optimization. 
Consider first generalizations of ‘outward normal vector’ to general closed sets 
(in a finite dimensional space).46 1 Overview
Fig. 1.10 A proximal normal 
vector 
y 
The Proximal Normal Cone Take a closed set C ⊂ Rk and a point x¯ ∈ C. A 
vector η ∈ Rk is said to be a proximal normal vector to C at x¯ if there exists M ≥ 0
such that 
η · (x − ¯x) ≤ M|x − ¯x|
2 for all x ∈ C. (1.8.1) 
The cone of all proximal normal vectors to C at x¯ is called the proximal normal 
cone to C at x¯ and is denoted by NP
C (x)¯ : 
NP
C (x)¯ := {η ∈ Rk : ∃ M ≥ 0 such that (1.8.1) is satisfied}.
Figure 1.10 provides a geometrical interpretation of proximal normal vectors: η
is a proximal normal vector to C at x¯ if their exists a point y ∈ Rk such that x¯ is the 
closest point to y in C and η is a scaled version of y − ¯x, i.e. there exists α ≥ 0 such 
that 
η = α(y − ¯x).
The Limiting Normal Cone Take a closed set C ⊂ Rk and a point x¯ ∈ C. A 
vector η is said to be a limiting normal vector to C at x¯ ∈ C if there exist sequences 
xi
C
→ ¯x and ηi → η such that 
ηi ∈ NP
C (xi) for all i.
The cone of limiting normal vectors to C at x¯ is denoted NC(x)¯ : 
NC(x)¯ := {η ∈ Rk : ∃ xi
C
→ x and ηi → η such that ηi ∈ NP
C (xi) for all i}.
(The notation xi
C
→ x indicates that xi → x and xi ∈ C for all i.) 
Figure 1.11 illustrates limiting normal cones at various points in a set with 
nonsmooth boundary. It can be shown that, if x¯ is a boundary point of C, then the 
limiting normal cone contains non-zero elements. Limiting normal cones are closed 
cones, but they are not necessarily convex. 
We consider next generalizations of the concept of ‘gradient’ to functions which 
are not differentiable.1.8 Nonsmooth Analysis 47
Fig. 1.11 Limiting normal 
cones 
. 
. 
x Set C
x normals 
normals 
Fig. 1.12 A geometric 
interpretation of proximal 
subgradients 
The Proximal Subdifferential Take an extended valued, lower semi-continuous 
function f : Rk → R ∪ {+∞} and a point x¯ ∈ dom {f }. A vector η ∈ Rk is said to 
be a proximal subgradient of f at x¯ if there exist ϵ > 0 and M ≥ 0 such that 
η · (x − ¯x) ≤ f (x) − f (x)¯ + M|x − ¯x|
2 (1.8.2) 
for all points x which satisfy |x − ¯x| ≤ ϵ.
The set of all proximal subgradients of f at x¯ is called the proximal subdifferential 
of f at x¯ and is denoted by ∂P f (x)¯ : 
∂P f (x)¯ := {there exist ϵ > 0 and M ≥ 0 such that (1.8.2) is satisfied }.
(The notation dom {f } denotes the set {y : f (y) < +∞ }.) 
Figure 1.12 provides a geometric interpretation of proximal subgradients: a 
proximal subgradient to f at x¯ is the slope at x = ¯x of a paraboloid, 
y = η · (x − ¯x) + f (x)¯ − M|x − ¯x|
2,
which coincides with f at x = ¯x and which lies on or below the graph of f on a 
neighbourhood of x¯.48 1 Overview
Fig. 1.13 Limiting subdifferentials 
The Limiting Subdifferential Take an extended valued, lower semi-continuous 
function f : Rk → R ∪ {+∞} and a point x¯ ∈ dom {f }. A vector η ∈ Rk is said 
to be a limiting subgradient of f at x¯ if there exist sequences xi
f
→ ¯x and ηi → η
such that 
ηi ∈ ∂P f (xi) for all i.
The set of all limiting subgradients of f at x¯ is called the limiting subdifferential 
and is denoted by ∂f (x)¯ : 
∂f (x)¯ := {η : ∃ xi
f
→ x and ηi → η such that ηi ∈ ∂P f (xi) for all i}.
(The notation xi
f
→ x indicates that xi → x and f (xi) → f (x) as i → ∞.) 
Figure 1.13 illustrates how the limiting subdifferential depends on the base point 
for a nonsmooth function. The limiting subdifferential is a closed set, but it need not 
be convex. It can happen that 
∂f (x)¯ /= −∂(−f )(x). ¯
This is because the inequality (1.8.2) in the definition of proximal subgradients does 
not treat positive and negative values of the function f in a symmetrical fashion. 
We mention that there are, in fact, a number of equivalent ways of defining 
limiting subgradients. As so often in mathematics, it is a matter of expository 
convenience what we regard as definitions and what we regard as consequences 
of these definitions. The ‘difference quotient’ definition above has been chosen for1.8 Nonsmooth Analysis 49
this review chapter, because it is often the simplest to use in applications. Another, 
equivalent, definition based on limiting normals to the epigraph set is chosen in 
Chap. 4 below, as a more convenient starting point for an in-depth analysis. 
We now list a number of important properties of limiting normal cones and 
limiting subdifferentials. 
(A): (Closed Graph) 
(i) Take a closed set C ⊂ Rk and a point x ∈ C. Then for any convergent 
sequences xi
C
→ x and ηi → η such that 
ηi ∈ NC(xi) for all i,
we have η ∈ NC(x). 
(ii) Take a lower semi-continuous function f : Rk → R ∪ {+∞} and a point 
x ∈ dom {f }. Then for any convergent sequences xi
f
→ x and ξi → ξ such 
that 
ξi ∈ ∂f (xi) for all i,
we have ξ ∈ ∂f (x). 
(B): (Links Between Limiting Normals and Subdifferentials) Take a closed set 
C ⊂ Rk and a point x¯ ∈ C. Then 
NC(x)¯ = ∂ΨC(x)¯
and 
NC(x)¯ ∩ B = ∂dC(x). ¯
Here, B is the closed unit Euclidean ball, centre the origin, in Rk, ΨC : Rk →
R ∪ {+∞} is the indicator function of the set C
ΨC(x) := 
0 if x ∈ C
+∞ if x /∈ C
and dC : Rk → R is the distance function of the set C
dC(x) := inf
y∈C
|x − y|.
It is well known that a Lipschitz continuous function f on Rk is differentiable 
almost everywhere with respect to Lebesgue measure. (This is Rademacher’s 
theorem.) It is natural then to ask, what is the relationship between the limiting 
subdifferential and limits of gradients at neighbouring points? F. H. Clarke provided50 1 Overview
an answer this question, by proving the following important representation of the 
limiting subdifferential (or, more precisely, its convex hull): 
(C): (Limits of Derivatives) Consider a lower semi-continuous function f : Rk →
R ∪ {+∞} and a point x ∈ dom f . Suppose that f is Lipschitz continuous on a 
neighbourhood of x¯. Then, for any subset S ⊂ Rk of zero k-dimensional Lebesgue 
measure, we have 
co ∂f (x)¯ = co {η : ∃ xi → x such that ∇f (xi) exists and
xi ∈/ S for all i and ∇f (xi) → η}.
Calculations of the limiting subdifferential for a specific function, based on 
the defining relations, is often a challenge, because of their indirect nature. If the 
function in question is Lipschitz continuous and we are content to estimate the 
limiting subdifferential ∂f (x)¯ by its convex hull co ∂f (x)¯ , then property (C) can 
simplify the task. A convenient feature of this characterization is that, in examining 
limits of neighbouring gradients, we are allowed to exclude gradients on an arbitrary 
null-set S. When f is piecewise smooth, for example, we can exploit this flexibility 
by hiding in S problematical points (on ridges, at singularities, etc.) and calculating 
co ∂f (x)¯ merely in terms of gradients of f on subdomains were it is smooth. 
Property (C) is significant also in other respects. It tells us, for example, that, if a 
lower semi-continuous function f : Rn → R∪ {+∞} is continuously differentiable 
near a point x¯, then ∂f (x)¯ = {∇f (x)¯ }. Since f is differentiable at x if and only if 
−f is differentiable at x and ∇f (x) = −∇(−f )(x)), we also deduce: 
(D): (Homogeniety of the Convexified Limiting Subdifferential) Consider a 
lower semi-continuous function f : Rk → R ∪ {+∞} and a point x¯ ∈ dom f . 
Suppose that f is Lipschitz continuous on a neighbourhood of x¯. Then 
co ∂f (x)¯ = −co ∂(−f )(x). ¯
Another useful property of Lipschitz continuous functions is 
(E): (Bounds on Limiting Subdifferentials) Consider a lower semi-continuous 
function f : Rk → R ∪ {+∞} and a point x ∈ dom f . Suppose that f is Lipschitz 
continuous on a neighbourhood of x¯ with Lipschitz constant K. Then 
∂f (x)¯ ⊂ KB.
A fundamental property of the gradient of a differentiable function is that it 
vanishes at a local minimizer of the function. The proximal subdifferential has a 
similar role in identifying possible minimizers: 
(F): (Limiting Subdifferentials at Minima) Take a lower semi-continuous func￾tion f : Rk → R ∪ {+∞} and a point x¯ ∈ dom f . Assume that, for some ϵ > 0
f (x)¯ ≤ f (x) for all x ∈ ¯x + ϵB.1.8 Nonsmooth Analysis 51
Then 
0 ∈ ∂P f (x). ¯
It follows that 0 ∈ ∂f (x)¯ . 
In applications, it is often necessary to estimate limiting subdifferentials of 
composite functions in terms of limiting subgradients of their constituent functions. 
Fortunately, an extensive calculus is available for this purpose. Some important 
calculus rules are as follows: 
(G): (Positive Homogeneity) Take a lower semi-continuous function f : Rk →
R ∪ {+∞}, a point x¯ ∈ dom f and α ≥ 0. Then 
∂(α f )(x)¯ = α∂f (x). ¯
(H): (Sum Rule) Consider a collection fi : Rk → R ∪ {+∞}, i = 1, .., n of lower 
semi-continuous extended valued functions and a point x¯ ∈ ∩i=1,..,ndom fi. Assume 
that all the fi’s except possibly one are Lipschitz continuous on a neighbourhood of 
x¯. Then the limiting subdifferential of (f1 +···+ fn)(x) = f1(x) +···+ fn(x)
satisfies 
∂(f1 + .. + fn)(x)¯ ⊂ ∂f1(x)¯ + .. + ∂fn(x). ¯
(I): (Max Rule) Consider a collection fi : Rk → R ∪ {+∞}, i = 1, .., n of 
lower semi-continuous extended valued functions and a point x¯ ∈ ∩i=1,..,ndom fi. 
Assume that all the fi’s, except possibly one, are Lipschitz continuous on a 
neighbourhood of x¯. Then the limiting subdifferential of (max{f1,...,fn})(x) =
max{f1(x), . . . , fn(x)} satisfies 
∂( max
i=1,..,n fi)(x)¯ ⊂ {n
i=1
αi∂fi(x)¯ : {α1,...,αn} ∈ (R+)
n, such that n
i=1
αi = 1
and αi = 0 for αi ∈/ I (x)¯ },
in which 
I (x)¯ := {i ∈ {1, .., n} : fi(x)¯ = max
j fj (x)¯ }.
(J): (Chain Rule) Take Lipschitz continuous functions G : Rk → Rm and g :
Rm → R and a point x¯ ∈ Rk. Then the limiting subdifferential ∂(g ◦ G)(x)¯ of the 
composite function x → g(G(x)) at x¯ satisfies: 
∂(g ◦ G)(x)¯ ⊂ {η : η ∈ ∂(γ · G)(x)¯ for some γ ∈ ∂g(G(x)) ¯ }.52 1 Overview
Many of these rules are extensions of familiar principles of differential calculus. 
(Notice however that, in most cases, equality has been replaced by set inclusion). 
An exception is the max rule above. The max rule is a truly nonsmooth principle; it 
has no parallel in traditional analysis because the operation of taking the pointwise 
maximum of a collection of smooth functions usually generates a nonsmooth 
function. (‘corners’ typically occur when there is a change in the function on which 
the maximum is achieved). 
We illustrate the use of the calculus rules by proving a multiplier rule for a 
mathematical programming problem with functional inequality constraints: 
⎧
⎨
⎩
Minimize f (x)
over x ∈ Rk satisfying
gi(x) ≤ 0, for i = 1, .., n.
(1.8.3) 
Here f : Rk → R and gi : Rk → R, i = 1, .., n, are given functions. 
Multiplier Rule I (Inequality Constraints) Let x¯ be a minimizer for problem 
(1.8.3). Assume that f , gi, i = 1, .., n, are Lipschitz continuous. 
Then there exist non-negative numbers λ0, λ1, .., λn such that 
n
i=0
λi = 1,
λi = 0 if gi(x) < ¯ 0 for i = 1, .., n
and 
0 ∈ λ0∂f (x)¯ +n
i=1
λi∂gi(x). ¯ (1.8.4) 
(A related, but sharper, multiplier rule will be proved in Chap. 5.) 
Proof Consider the function 
φ(x) = max{f (x) − f (x), g ¯ 1(x), .., gn(x)}.
We claim that x¯ minimizes φ over x ∈ Rk. If this were not the case, if would be 
possible to find x' such that 
φ(x'
) < φ(x)¯ = 0.
It would then follow that 
f (x'
) − f (x) < ¯ 0, g1(x'
) < 0, .., gn(x'
) < 0,
in contradiction of the minimality of x¯. The claim has been confirmed.1.8 Nonsmooth Analysis 53
According to Rule (F) 
0 ∈ ∂φ(x). ¯
We deduce from Rule (I) that there exist non-negative numbers λ0, .., λn such that 
n
i=0
λi = 1,
and 
0 ∈ λ0∂f (x)¯ +n
i=1
λi∂gi(x). ¯
Since, for index values i such that gi(x) < ¯ 0, we have 
gi(x) < φ( ¯ x). ¯
Rule (I) supplies the supplementary information that 
λi = 0 if gi(x) < ¯ 0.
The multiplier rule is proved. ⨅⨆
In the case when f, g1, .., gn are continuously differentiable, inclusion (1.8.4) 
implies 
0 = λ0∇f (x)¯ +n
i=1
λi∇gi(x), ¯
the traditional Fritz-John condition for problems with functional inequality con￾straints. Nonsmooth analysis supplies a new, simple proof of a classical theorem in 
the case when the functions involved are continuously differentiable and extends 
it, by supplying information even in the situations when the functions are not 
differentiable. Application of nonsmooth calculus rules to some derived, nonsmooth 
function (in this case φ) is a staple of ‘nonsmooth’ methodology. Notice that, in the 
above proof, we can expect φ to be nonsmooth, even if the functions from which it 
is constructed are smooth. 
Calculus rules are some of the tools of the trade in nonsmooth analysis. In 
many applications however, these are allied to variational principles. A ‘variational 
principle’ is an assertion that some quantity is minimized. Two which are widely 
used in nonsmooth analysis are the exact penalization theorem and Ekeland’s 
theorem. Suppose that the point x¯ ∈ Rk is a minimizer for the constrained 
minimization problem54 1 Overview

Minimize f (x) over x ∈ Rk
satisfying x ∈ C,
for which the data comprise a function f : Rk → R and a set C ⊂ Rk. For both 
analytical and computational reasons, it is often convenient to replace this by an 
unconstrained problem, in which the new cost 
f (x) + p(x)
is the sum of the former cost and a term p(x) which penalizes violation of the 
constraint x ∈ C. In the case that f is Lipschitz continuous, the exact penalization 
theorem says that x¯ remains a minimizer when the penalty function p is chosen to 
be the nonsmooth function 
p(x) = Kdˆ C(x),
for some constant Kˆ sufficiently large. Notice that the penalty term, while nons￾mooth, is Lipschitz continuous. 
The Exact Penalization Theorem Take a function f : Rk → R and a closed set 
C ⊂ Rk. Assume that f is Lipschitz continuous, with Lipschitz constant K. Let x¯
be a minimizer for 

Minimize f (x) over x ∈ Rk
satisfying x ∈ C.
Then, for any Kˆ ≥ K, x¯ is a minimizer also for the unconstrained problem 

Minimize f (x) + Kdˆ C(x)
over points x ∈ Rk.
A more general version of this theorem is proved in Chap. 3. 
One important consequence of the exact penalization theorem is that, if x¯ is a 
minimizer for the optimization problem of the theorem statement then, under the 
hypotheses of the exact penalization theorem, 
0 ∈ ∂f (x)¯ + NC(x). ¯
(It is a straightforward matter to deduce this necessary condition of optimality for 
an optimization problem with an ‘implicit’ constraint from the fact that x¯ also 
minimizes f (x) + KdC(x) over Rn and Rules (F), (H) and (B) above.) 
We turn next to Ekeland’s theorem. In plain terms, this states that, if a point x0
approximately minimizes a function f , then there is a point x¯ close to x0 which is a 
minimizer for some new function, obtained by adding a small perturbation term to 
the original function f . See Fig. 3.1 of Chap. 3.1.8 Nonsmooth Analysis 55
Ekeland’s Theorem Take a complete metric space (X, d), a lower semi￾continuous function f : X → R ∪ {+∞}, a point x¯ ∈ dom f and a number 
ϵ > 0 such that 
f (x)¯ ≤ inf
x∈X f (x) + ϵ.
Then there exists x' ∈ X such that 
d(x'
, x)¯ ≤ ϵ
1
2
and 
f (x) + ϵ
1
2 d(x, x'
)|x=x' = inf
x∈X
{f (x) + ϵ
1
2 d(x, x'
)}.
Ekeland’s theorem and the preceding nonsmooth calculus rules can be used, for 
example, to derive a multiplier rule for a mathematical programming problem 
involving a functional equality constraint: 
⎧
⎨
⎩
Minimize f (x)
over x ∈ Rk satisfying
G(x) = 0.
(1.8.5) 
The data for this problem comprise functions f : Rk → R and G : Rk → Rm. 
Multiplier Rule II (Equality Constraints) Let x¯ be a minimizer for problem 
(1.8.5). Assume that f and G are Lipschitz continuous. 
Then there exist number λ0 ≥ 0 and a vector λ1 ∈ Rm such that 
λ0 + |λ1| = 1
and 
0 ∈ λ0∂f (x)¯ + ∂(λ1 · G)(x). ¯ (1.8.6) 
Proof The simple proof above of the multiplier rule for inequality constraints 
suggests that equality constraints can be treated in the same way. We might hope 
to use the fact that, for the problem with equality constraints, x0 is an unconstrained 
minimizer of the function 
γ (x) := max{f (x) − f (x) , ¯ |G(x)|}.
By Rule (F), 0 ∈ ∂γ(x)¯ . We deduce from the Max Rule (I) that there exist 
multipliers λ0 ≥ 0 and ν1 ≥ 0 such that λ0 + ν1 = 1 and 
0 ∈ λ0∂f (x)¯ + ν1∂|G(x)| |x= ¯x . (1.8.7)56 1 Overview
We aim then to estimate vectors in ∂|G(x)| |x= ¯x using nonsmooth calculus rules, in 
such a manner as to deduce the asserted multiplier rule. The flaw in this approach 
is that 0 ∈ ∂|G(x)| |x= ¯x with the result that (1.8.7) is trivially satisfied by the 
multipliers (λ0, ν1) = (0, 1). In these circumstances, (1.8.7) cannot provide useful 
information about x¯. 
As shown by Clarke [60], Ekeland’s Theorem can be used to patch up the 
argument, however. The difficulty we have encountered is that 0 ∈ ∂|G(x)| |x= ¯x . 
The idea is to perturb the function γ in such a way that, along some sequence 
xi → ¯x, (1.8.7) is approximately satisfied at xi but, significantly, 0 ∈/ ∂|G(x)| |x=xi . 
The perturbed version of (1.8.7) is no longer degenerate and we can expect to 
recover the multiplier rule in the limit as i → ∞. 
Take a sequence ϵi ↓ 0. For each i define the function 
φi(x) := max{f (x) − f (x)¯ + ϵi, |G(x)|}.
Note that φi(x)¯ = ϵi and φi(x) ≥ 0 for all x. It follows that 
φi(x)¯ ≤ inf
x∈Rk
φ(x) + ϵi.
In view of the fact that φi is lower semi-continuous (indeed Lipschitz continuous) 
with respect to the metric induced by the Eulidean norm, we deduce from Ekeland’s 
theorem that there exists xi ∈ Rk such that 
|xi − ¯x| ≤ ϵ
1
2
i
and 
φ˜i(xi) ≤ inf
x∈Rk
φ˜i(x),
where 
φ˜i(x) := max{f (x) − f (x)¯ + ϵi, |G(x)|} + ϵ
1
2
i |x − xi|.
Since the function x → |x − xi| is Lipschitz continuous with Lipschitz constant 1, 
we know that its limiting subgradients are contained in the closed unit ball B. By 
Rules (E), (F) and (H) then, 
0 ∈ ∂φ˜i(xi) + ϵ
1
2
i B.
Otherwise expressed, 
0 ∈ zi + ϵ
1
2
i B1.8 Nonsmooth Analysis 57
for some point 
zi ∈ ∂ max{f (x) − f (x)¯ + ϵi, |G(x)|}|x=xi .
We now make an important observation: for each i, 
max{f (xi) − f (x)¯ + ϵi, |G(x)|} |x=xi > 0.
Indeed, if this were not the case, we would have f (xi) ≤ f (x)¯ − ϵi and G(xi) = 0, 
in violation of the optimality of x¯. 
It follows that 
|G(xi)| = 0 implies f (xi) − f (x)¯ + ϵi > |G(xi)|. (1.8.8) 
Now apply the max rule (I) to φ˜i. We deduce the existence of non-negative numbers 
λi
0 and νi
1 such that 
λi
0 + νi
1 = 1,
zi ∈ λi
0∂f (xi) + νi
1∂|G(xi)|.
Furthermore, by (1.8.8), 
|G(xi)| = 0 implies νi
1 = 0. (1.8.9) 
We note next that, if y' /= 0, then all limiting subgradients of y → |y| at y' have unit 
Euclidean norm. (This follows from the fact that ∇|y| = y/|y| for y /= 0 and rule 
(C)). We deduce from the chain rule and (1.8.9) that there exists a vector ai ∈ Rm
such that |ai| = 1 and 
zi ∈ λi
0∂f (xi) + νi
1∂(ai · G)(xi).
Writing λi
1 := νi
1ai, we have, by positive homogeniety, 
λi
0 + |λi
1| = 1 (1.8.10) 
and 
zi = λi
0∂f (xi) + ∂(λi
1 · G)(xi).
It follows that, for each i, there exist points 
γ i
0 ∈ ∂f (xi), γ i
1 ∈ ∂(λi
1 · G)(xi) and ei ∈ ϵ
1
2
i B58 1 Overview
such that 
0 = λi
0γ i
0 + γ i
1 + ei. (1.8.11) 
By rule (E), and in view of (1.8.10) and of the Lipschitz continuity of f and G, the 
sequences 
{γ i
0 }, {γ i
1 }, {λi
0} and {λi
1}
are bounded. We can therefore arrange, by extracting subsequences, that they have 
limits γ0, γ1, λ0, λ1 respectively. Since the λi
0’s are non-negative, it follows from 
(1.8.10) that 
λ0 ≥ 0 and λ0 + |λ1| = 1.
But ei → 0. We deduce from (1.8.11), in the limit as i → ∞, that 
0 = λ0γ0 + γ1.
It is a straightforward matter to deduce from the fact that the limiting subdifferential 
has closed graph, the sum rule (H) and Rule (E) that 
γ0 ∈ ∂f (x)¯ and γ1 ∈ ∂(λ1 · G)(x). ¯
If follows that 
0 ∈ λ0∂f (x)¯ + ∂(λ1 · G)(x). ¯
This is what we set out to show. ⨅⨆
In the case when f and g are continuously differentiable, condition (1.8.6) 
implies 
0 = λ0∇f (x)¯ + λ1∇G(x). ¯
Once again, the tools of nonsmooth analysis provide both an alternative derivation 
of a well-known multiplier rule in nonlinear programming for differentiable data 
and also extensions to nonsmooth cases. 
The above proof is somewhat complicated, but it is no more so (if we have the 
above outlined techniques of nonsmooth analysis at our disposal) than traditional 
proofs of Lagrange multiplier rules for problems with equality constraints (treating 
the smooth case), based on the classical inverse function theorem, say. The Lagrange 
multiplier rule for problems with equality constraints, even for differentiable data, 
is not an elementary result and we must expect to work for it.1.9 Nonsmooth Dynamic Optimization 59
Many known relations in classical analysis involving continuously differentiable 
functions (calculus rules and first order necessary conditions of optimality in 
nonlinear programming, for example) can be generalized, with the help of nons￾mooth analysis, to allow for functions that are merely Lipschitz continuous or even 
lower semi-continuous. Variational principles often play a key role in achieving 
these extensions. The proof of the Lagrange multiplier rule above is included in 
this review chapter as an illuminating prototype. The underlying ideas will be 
used repeatedly in future chapters to derive necessary conditions of optimality 
in dynamic optimization. In many applications, variational principles such as 
Ekeland’s theorem (which concern properties of nondifferentiable functions) carry 
the burden traditionally borne by the inverse function theorem and its relatives in 
classical analysis (which concern properties of differentiable functions). 
1.9 Nonsmooth Dynamic Optimization 
The analytical techniques of the preceding section were developed primarily for the 
purpose of applying them to dynamic optimization. We now briefly describe how 
they have been used to overcome fundamental difficulties earlier encountered in 
this field. 
Consider first necessary conditions. As discussed, the following optimization 
problem, in which the dynamic constraint takes the form of a differential inclusion, 
serves as a convenient paradigm for a broad class of dynamic optimization problems, 
including some problems to which the maximum principle is not directly applicable: 
⎧
⎪⎪⎨
⎪⎪⎩
Minimize g(x(T ))
over arcs x ∈ W1,1 satisfying
x(t) ˙ ∈ F (t, x(t)) a.e.,
x(S) = x0 and x(T ) ∈ C.
(1.9.1) 
Here, g : Rn × Rn → R is a given function, F : [S,T ] × Rn ⇝ Rn is a given 
multifunction and C ⊂ Rn is a given subset. We proposed reformulating it as a 
generalized problem in the calculus of variations: 
⎧
⎨
⎩
Minimize  T
S L(t, x(t), x(t))dt ˙ + g(x(T ))
over absolutely continuous arcs x satisfying
x(S) = x0 and x(T ) ∈ C,
in which L is the extended valued function 
L(t, x, v) := 
0 if v ∈ F (t, x)
+∞ if v /∈ F (t, x),60 1 Overview
and deriving generalizations of classical necessary conditions to take account of 
nonsmoothness of the data. This can be done, with recourse to nonsmooth analysis. 
The details are given in future chapters. 
A decisive advance in this direction was Clarke’s Hamiltonian inclusion. This 
was a generalization, to a nonsmooth setting, of Hamilton’s system of equations 
in the calculus of variations. Clarke’s proof of this condition, under an additional 
convexity hypothesis on the values of the multifunction F, was an early landmark 
in nonsmooth dynamic optimization. As usual we write 
H (t, x, p) = max
v∈F (t,x) p · v .
Theorem (Clarke’s Hamiltonian Inclusion) Let x¯ be a minimizer for problem 
(1.9.1). Assume that, for some ϵ > 0, the data for the problem satisfy: 
(a) F (t, x) is nonempty for each (t, x) ∈ [S,T ] × Rn, Gr F (t, .) is closed for each 
t ∈ [S,T ] and F is L × Bn measurable, 
(b) there exists c ∈ L1 such that 
F (t, x) ⊂ c(t)B ∀ x ∈ ¯x(t) + ϵB, a.e.,
(c) there exist k ∈ L1 such that 
F (t, x) ⊂ F (t, x'
) + k(t)|x − x'
|B ∀ x, x' ∈ ¯x(t) + ϵB, a.e.,
(d) C is closed and g is Lipschitz continuous on x(T ) ¯ + ϵB. 
Then there exist p ∈ W1,1([S,T ]; Rn) and λ ≥ 0 such that 
(i) (p, λ) /= (0, 0),
(ii) (− ˙p(t), ˙
x(t)) ¯ ∈ co ∂x,p H (t, x(t), p(t)) ¯ a.e. t ∈ [S,T ],
(iii) −p(T ) ∈ λ∂g(x(T )) ¯ + NC(x(T )). ¯
(The sigma algebra L × Bn, referred to in hypothesis (a), will be defined in Chap. 2. 
∂x,pH denotes the limiting subdifferential of H (t, ., .).) 
The essential feature of this extension is that Hamilton’s system of equations for 
(p, x)¯ has been replaced by a differential inclusion, involving the convex hull of 
limiting subdifferential of the Hamiltonian H with respect to the x and p variables. 
The limiting subdifferential and the limiting normal cone are required to give 
meaning to the transversality condition (iii). 
We stress that the Hamiltonian inclusion is an intrinsically nonsmooth condition. 
Even when the multifunction F is the velocity set of some smooth control system 
x(t) ˙ = f (t, x(t), u), u(t) ∈ U a.e.,1.9 Nonsmooth Dynamic Optimization 61
(‘smooth’ in the sense that f is a smooth function and U has a smooth boundary), 
in which case 
F (t, x) = f (t, x, U ),
we can still expect that H (t, ., .) will be nonsmooth and that the apparatus of 
nonsmooth analysis will be required to make sense of the Hamiltonian inclusion. 
Other nonsmooth necessary conditions are derived in future chapters. These 
include a generalization of the Euler Lagrange condition for problem (1.9.1) 
and a generalization of the maximum principle (‘Clarke’s nonsmooth maximum 
principle’) for the dynamic optimization problem of Sect. 1.5. 
Consider next dynamic programming. This, we recall, concerns the relationship 
between the value function V , which describes how the minimum cost of a dynamic 
optimization problem depends on the initial data, and the Hamilton Jacobi equation. 
For now, we focus on the dynamic optimization problem 
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize g(x(T ))
over measurable functions u and arcs x ∈ W1,1 satisfying
x(t) ˙ ∈ f (t, x(t), u(t)) a.e.,
u(t) ∈ U, a.e.,
x(S) = x0, x(T ) ∈ C,
to which corresponds the Hamilton Jacobi equation: 
⎧
⎨
⎩
φt(t, x) + minu∈U φx (t, x) · f (t, ξ , u) = 0
for all (t, x) ∈ (S, T ) × Rn
φ(T , x) = g(x) + ΨC(x) for all x ∈ Rn.
(ΨC denotes the indicator function of the set C.) 
It has been noted that the value function is often nonsmooth. It cannot therefore 
be identified with a classical solution to the Hamilton Jacobi equation above, 
under hypotheses of any generality. However the machinery of nonsmooth analysis 
provides a characterization of the value function in terms of the Hamilton Jacobi 
equation, even in cases when V is discontinuous. There are a number of ways to do 
this. One involves the concept of viscosity solution. Another is based on properties 
of proximal subgradients. In this book we follow this latter approach because it is 
better integrated with our treatment of other topics and because it is particularly well 
suited to problems with end-point constraints. 
Specifically, we shall show that, under mild hypotheses, V is the unique lower 
semi-continuous function which is a generalized solution of the Hamilton Jacobi 
equation. Here, ‘generalized solution’ means a function φ taking values in the 
extended real line R∪{+∞}, such that the following three conditions are satisfied: 
(i): For each (t, x) ∈ ((S, T ) × Rn) ∩ dom φ
η0 + inf v∈F (t,x) η1 · v = 0 ∀ (η0, η1) ∈ ∂P φ(t, x),62 1 Overview
(ii): φ(T , x) = g(x) + ΨC(x) ∀ x ∈ Rn,
(iii): φ(S, x) = lim inft'
↓S,x'
→x φ(t'
, x'
) ∀ x ∈ Rn,
φ(T , x) = lim inft'
↑T ,x'
→x φ(t'
, x'
) ∀ x ∈ Rn.
Of principle interest here is condition (i). (ii) merely reproduces the earlier 
boundary condition. (iii) is a regularity condition on φ, which is automatically 
satisfied when φ is continuous. 
If φ is a C1 function on (S, T ) × Rn, condition (i) implies that φ is a classical 
solution to the Hamilton Jacobi equation. This follows from the fact that, in this 
case, ∂P V (t, x) ⊂ {∇V (t, x)}. 
What is remarkable about this characterization is the unrestrictive nature of the 
conditions it imposes on V , for it to be a value function. Condition (i) merely 
requires us to test certain relationships on the subset of (S, T ) × Rn: 
D = {(t, x) ∈ ((S, T ) × Rn) ∩ dom V : ∂P V (t, x) = ∅} / ,
a subset which can be very small indeed (in relation to (S, T ) × Rn); it need not 
even be dense. 
Another deficiency of classical theory of dynamic programming is its failure to 
provided precise sensitivity information about the value function V , though our 
earlier formal calculations suggest that, if p is the adjoint arc associated with a 
minimizer (x,¯ u)¯ , then 
(max
u∈U p(t) · f (t, x, u), ¯ −p(t)) = ∇V (t, x(t)), ¯ a.e. t ∈ [S,T ]. (1.9.2) 
Indeed for many cases of interest V fails to be differentiable and (1.9.2) does not 
even make sense. Once again, nonsmooth analysis fills the gap. The sensitivity 
relation (1.9.2) can be given precise meaning as an inclusion in which the gradient 
∇V is replaced by a suitable ‘nonsmooth’ subdifferential, namely the convexified 
limiting subdifferential co ∂V . 
1.10 Epilogue 
Our overview of dynamic optimization is concluded. To summarize, dynamic 
optimization concerns the properties of minimizing arcs. Its roots are in the 
investigation of minimizers of integral functionals, which has been an active area 
of research for over 200 years, in the guise of the calculus of variations. The 
distinguishing feature of dynamic optimization is that it can take account of dynamic 
and pathwise constraints, of a nature encountered in areas of advance engineering 
design and operations such as path planning for space vehicles, and in other areas 
of dynamic decision making that include resource economics, epidemiology and 
climate change mitigation.1.10 Epilogue 63
Key early advances were the maximum principle and an intuitive understanding 
of the relationship between the value function and the Hamilton Jacobi equation 
of dynamic programming. The antecedents of dynamic programming in the branch 
of the calculus of variations known as field theory are apparent. On the other hand 
the nature of the maximum principle, and the techniques first employed to prove 
it, based on approximation of reachable sets, suggested that essentially new kinds 
of necessary conditions and analytical techniques were required to deal with the 
constraints arising in dynamic optimization problems. 
Subsequent developments in dynamic optimization, aimed at extending the 
range of application of available necessary conditions of optimality, emphasize 
its similarities rather than its differences with the calculus of variations. Here, 
the constraints of dynamic control are replaced by extended valued penalty terms 
in the integral functional to be minimized. Problems in dynamic optimization 
are thereby reformulated as extended problems in the calculus of variations with 
nonsmooth data. It is then possible to derive, in the context of dynamic optimization, 
optimality conditions of remarkable generality, analogous to classical necessary 
conditions in the calculus of variations, in which classical derivatives are replaced 
by ‘generalized’ derivatives of nonsmooth functions. Nonsmooth analysis, which 
gives meaning to generalized derivatives has, of course, had a key role in these 
developments. nonsmooth analysis, by supplying the appropriate machinery for 
interpreting generalized solutions to the Hamilton Jacobi equation, can be used 
also to clarify the relationship between the value function and the Hamilton Jacobi 
equation, hinted at in the 1950’s. 
This overview stresses developments in dynamic optimization which provide 
the background to material in this book. It omits a number of important topics, 
such as higher order conditions of optimality, optimality conditions for a wide 
variety of nonstandard dynamic optimization problems (impulsive control problems, 
problems involving functional differential equations, etc.), computational dynamic 
optimization and an analysis of the dependence of minimizers on parameters, such 
as that provided in [148]. It also emphasizes only one possible route to proving 
general necessary conditions of optimality, which highlights their affinity with clas￾sical conditions, and to providing a rigorous foundation for dynamic programming, 
based on the pathwise analysis and the application of invariance/viability theorems. 
A complete picture would include the ‘set separation’ approaches to deriving 
necessary conditions, associated with Neustadt [163], Halkin [121], Dubovitskii and 
Milyutin [96], Warga [204] and Sussmann [185], which aim at generalizations of the 
maximum principle, and viscosity solution methods to establish the link between the 
Hamilton Jacobi equation and the value function in dynamic programming. 
Dynamic optimization has been an active research field for over 60 years, much 
longer if you regard it as an outgrowth of the calculus of variations. Newcomers 
will expect, with reason, to find an armoury of easily applied techniques for 
computing optimal strategies and an extensive library of solved problems. This is 
not exactly what they will find. The truth is, solving dynamic optimization problems 
of scientific or engineering interest is often extremely difficult. Analytical tools of 
dynamic optimization, such as the maximum principle, provide equations satisfied64 1 Overview
by minimizers. They do not tell us how to solve them or, if we do succeed in solving 
them, that the ‘solutions’ are the right ones. Over two centuries ago Euler wrote 
of fluid mechanics (he could have commented thus equally about the calculus of 
variations) 
If it is not permitted to us to penetrate to a complete knowledge concerning 
the motion of fluids, it is not to mechanics or to the insufficiency of the known 
principles of motion, that we must attribute the cause. It is analysis itself which 
abandons us here. 
As regards dynamic optimization, the picture remains the same today. It is quite 
clear that general methodologies giving ‘complete knowledge’ of solutions to large 
classes of optimal control problems will never be forthcoming, if by ‘solution’ is 
meant formulae describing the solution in terms of standard functions. 
Even in cases when we can completely solve a variational problem by anal￾ysis, using general theory to find a minimizer for a specific problem is often a 
major undertaking. Often we find that for seemingly simple classical problems of 
longstanding interest, the hypotheses are violated, under which the general theory 
guarantees existence of minimizers and supplies information about them. In cases 
when optimality conditions are valid, the information they supply can be indirect 
and incomplete. 
Some of these points are illustrated by the brachistochrone problem of Sect. 1.2. 
It is easy to come up with a candidate x¯ for minimizing path by solving the Euler 
Lagrange Condition, namely a cycloid satisfying the boundary conditions. But is 
it truly a minimizer? If we adopt the formulation of Sect. 1.2, in which we seek 
a minimizer among curves describable as the graph of an absolutely continuous 
function in the vertical plane, the answer is yes. But this is by no means easy to 
establish. The cost integrand violates the Tonelli criteria for existence of minimizers 
(though other criteria can be invoked in this case to establish existence of a minimum 
time path). Then it is not even clear at the outset that minimizers will satisfy 
the Euler Lagrange condition because the standard Lipschitz continuity hypothesis 
commonly invoked to derive the condition, namely hypothesis (ii) of Theorem 1.2.1, 
is violated. Various arguments can be used to overcome these difficulties. (We can 
study initially a simpler problem in which, to eliminate the troublesome infinite 
slope at the left end-point, the ball is assumed to have a small initial velocity, or a 
pathwise constraint is imposed on the slope of the arcs considered, for example, and 
subsequently examine limits). But none of them are straightforward. Solution of the 
brachistochrone problem is discussed at length in, for example, [187, 191]. 
The brachistrochrone problem can be solved by analysis, albeit with some hard 
work. The maximal orbit transfer and cancer treatment problems of Sect. 1.1 are 
more typical of advanced engineering applications. We made no claims that this 
problem has been solved. But numerical schemes, inspired by modern day Lagrange 
multiplier rules and the maximum principle, generate the control strategy of Fig. 1.5, 
which, from a strictly rigorous viewpoint, is merely a candidate for an optimal 
strategy.1.10 Epilogue 65
It is only when we adopt a more realistic view of the scope of dynamic 
optimization, that we can truly appreciate the benefits of research in this field: 
Simple Problems While we cannot expect the theory of dynamic optimization 
alone to provide complete solutions to complex dynamic optimization problems of 
advanced engineering design, it does often provide optimal strategies for simplified 
versions of these problems. Optimal strategies for a simpler problem can serve as 
adequate close-to-optimal strategies (synonymously ‘suboptimal’ strategies) for the 
original problem. Alternatively, they can be used as initial guesses at an optimal 
strategy for use in numerical schemes to solve the original problem. This philosophy 
underlies some applications of dynamic optimization to resource economics, where 
simplifying assumptions about the underlying dynamics lead to problems which 
can be solved explicitly by analytical means [65]. In a more general context, many 
dynamic optimization problems of practical importance, in which there are only 
small excursions of the state variables, can be approximated by the linear quadratic 
control problem 
(LQ)
⎧
⎪⎪⎨
⎪⎪⎩
Minimize  T
0 (xT (t)Qx(t) + uT (t)Ru(t))dt + xT (T )Gx(T )
over u ∈ L2 and x ∈ W1,1 satisfying
x(t ˙ = Ax(t) + Bu(t) a.e.,
x(0) = x0,
with data the number T > 0, the n × n symmetric matrices Q ≥ 0 and G ≥ 0, the 
m × m symmetric matrix R > 0, the n × n matrix A, the n × m matrix B and the 
n-vector x0. The linear state equation here corresponds to linearization of nonlinear 
dynamics about an operating point. Weighting matrices in the cost are chosen 
to reflect economic criteria and to penalize constraint violations. In engineering 
applications, solving (LQ) (or related problems involving an approximate, linear, 
model) is a common ingredient in controller design. 
Numerical Methods For many dynamic optimization problems which cannot 
be solved by the application of analysis alone, we can seek a minimizer with 
the help of numerical methods. Among the successes of dynamic optimization 
theory is the impact it has had on computational dynamic optimization, in the 
areas of both algorithm design and algorithm convergence analysis. Ideas such as 
exact penalization, for example, Lagrange multiplier rules and techniques for local 
approximation of nonsmooth functions, so central to the theory, have taken root also 
in general computational procedures for solving dynamic optimization problems. 
Early contributors in this area were Polak and Mayne, who exploited nonsmooth 
constructs and analytic methods from dynamic optimization in a computational 
context [150]. (See [151, 166, 169] for additional references.) 
A widely used procedure for the computation of optimal strategies for a 
(continuous time) dynamic optimization problem involves several steps. First, we 
employ discretization methods, such as those discussed in Betts’ book [38], to 
reduce the problem to a non-linear program. This can be configured with the help66 1 Overview
of the Applied Modelling Programming Language (AMPL) [114] to interface with 
one of several available optimization solvers, such as the interior point optimization 
solver IPOPT, developed by Wächler and Biegler [202]. 
Global convergence analysis, which amounts to establishing that limit points 
satisfy first order conditions of optimality, is of course intimately connected with 
dynamic optimization theory. The significance of global convergence analysis is 
that, on the one hand, it makes explicit classes of problems for which the algorithm is 
effective and, on the other, it points to possible modifications to existing algorithms 
to increase the likelihood of convergence. 
Qualitative Properties Dynamic optimization can also provide useful qualitative 
information about minimizers. Often there are various ways of formulating an 
engineering design problem as an optimization problem. The theory of dynamic 
optimization provides simple criteria for existence of solutions, which, at the very 
least, can give useful insights into whether a sensible choice of optimization problem 
has been made. The theory also supplies a priori information about regularity of 
minimizers. This is of benefit for computation of optimal controls, since a ‘best 
choice’ of optimization algorithm can depend on minimizer regularity, as discussed, 
for example in [120]. Minimizer regularity is also relevant to the modelling of 
physical phenomena. In applications to nonlinear elasticity for example, existence 
of minimizers with infinite gradients has been linked to material failure and so 
an analysis identifying classes of problems for which minimizers have bounded 
derivatives gives insights into mechanisms of fracture [16]. Finally, and perhaps 
most significantly from the point of view of solving engineering problems, nec￾essary conditions can supply structural information which greatly simplifies the 
computation of optimal controls. In the computation of optimal flight trajectories, 
for instance, information obtained from known necessary conditions of optimality 
concerning bang-bang subarcs, relationships governing dependent variables on 
interior arcs, bounds on the number of switches, etc., is routinely used to reduce 
the computational burden [50]. 
1.11 Appendix: Proof of the Classical Maximum Principle 
The maximum principle is the cornerstone of dynamic optimization. In later 
chapters we shall derive general versions of the maximum principle for problems 
with non-differentiable data as well as extensions to allow for end-point constraint 
sets arbitrary closed sets, pathwise state constraints and other features. Even under￾standing the statement of the maximum principle in these general settings requires 
the apparatus of non-smooth analysis and the proofs are long and complicated. 
Yet many dynamic optimization problems encountered in applications are basic 
ones, in the sense that there are no pathwise state constraints, the data is continu￾ously differentiable w.r.t. the state variable state and the state end-point constraints 
take the form of smooth functional equality and inequality constraints. For basic1.11 Appendix: Proof of the Classical Maximum Principle 67
problems in this sense, the statement of the maximum problem simplifies and its 
derivation becomes more straightforward. 
In this appendix we give two proofs of the maximum principle for these basic 
dynamic optimization problems. The first is based on Clarke’s perturbation methods 
(supported by a variational principle), similar to those deployed in future chapters to 
derive necessary conditions of optimality for a broader class of problems but which, 
here, are employed in a much more direct fashion. The second proof is based on set 
separation methods. 
The proof of the maximum principle by perturbation methods is intended to 
serve the needs of readers who wish to gain an understanding of this important 
optimality condition and apply it to basic problems of dynamic optimization, based 
on a minimal use of non-standard analytical techniques. For other readers intending 
to enter more deeply into the field, this proof is an introduction to ideas for deriving 
necessary conditions used throughout this book, but which, here, take a simpler, 
more transparent form. 
The earliest proofs of the maximum principle, including that in L. S. Pontragin 
et al.’s book [167], are based, not on Clarke’s perturbation methods that are centre 
stage in this book, but on set separation methods. We include in this appendix 
a second proof of the maximum principle for the basic problem of dynamic 
optimization , based on these methods. This is partly to acknowledge their historical 
importance. But we have included it also because set separation ideas have a 
continuing influence on the modern day dynamic optimization research and a 
rounded understanding of the current research landscape requires familiarity with 
both perturbation and set separation approaches. 
Consider the following ‘basic’ dynamic optimization problem in which the end￾point state constraints take the form of functional inequalities and equalities: 
(P )
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize g(x(T )) +  T
S L(t, x(t), u(t))dt
over absolutely cont. functions x : [S,T ] → Rn
and meas. functions u : [S,T ] → Rm such that
x(t) ˙ = f (t, x(t), u(t)), a.e. t ∈ [S,T ],
u(t) ∈ U (t), a.e. t ∈ [S,T ],
x(S) = x0 and x(T ) ∈ C .
Here, f : [S,T ] × Rn × Rm → Rn, L : [S,T ] × Rn × Rm → R and g : Rn → R
are given functions, {U (t) ⊂ Rm : S ≤ t ≤ T } is a given family of sets, x0 is a 
given point in Rn and C is the set 
C = {x ∈ Rn : φ(x) ≤ 0 and ψ(x) = 0}
for given functions φ : Rn → Rnφ and ψ : Rn → Rnψ . 
As usual, we take an admissible process to be a pair of functions (x, u), 
comprising an absolutely continuous function x : [S,T ] → Rn (called a ‘state 
trajectory’) and a measurable function u : [S,T ] → Rm (called a ‘control’), if they68 1 Overview
satisfy the conditions listed in problem (P). An admissible process(x,¯ u)¯ is said to be 
an L∞ local minimizer if there exists a number β > 0 such that g(x(T )) ≥ g(x(T )) ¯
for all admissible processes (x, u) such that ||x − ¯x||L∞ ≤ β. Under circumstances 
when there is a unique state trajectory associated with each control u, we write this 
state trajectory xu. 
For problem (P), the maximum principle takes the following form: 
Theorem 1.11.1 (Maximum Principle) Take an L∞ local minimizer (x,¯ u)¯ . 
Assume, for some k ∈ L1, c ∈ L1 and ϵ > 0, 
(A): (f, L)(., x, .) is L × Bm measurable for each x ∈ Rn and {(t, u) ∈ [S,T ] ×
Rm : u ∈ U (t)} is L × Bm measurable, where L × Bm denotes the product 
σ-field generated by the Lebesgue subsets of [S,T ] and the Borel subsets of 
Rm, 
(B): g, φ and ψ are C1 on x(T ) ¯ + ϵB, and (f, L)(t, ., u) is C1 on x(t) ¯ + ϵB
for all u ∈ U (t), a.e. t ∈ [S,T ], 
(C): |(f, L)(t, x, u) − (f, L)(t, x'
, u)| ≤ k(t)|x − x'
| for all x, x' ∈ ¯x(t) + ϵB and 
u ∈ U (t), a.e. t ∈ [S,T ], 
(D): |(f, L)(t, x, u)| ≤ c(t) for all x ∈ ¯x(t) + ϵB and u ∈ U (t), a.e. t ∈ [S,T ]. 
Then there exist λ0 ≥ 0, λφ ∈ (R+)nφ , λψ ∈ Rnψ and p ∈ W1,1([S,T ]; Rn) such 
that λφ
k = 0 if φk(x(T )) < ¯ 0, k = 1,...,nφ and 
(i): (p, λ0, λφ, λψ ) /= (0, 0, 0, 0), 
(ii): − ˙p(t) = p(t) · ∇xf (t, x(t), ¯ u(t)) ¯ − λ0∇xL(t, x(t), ¯ u(t)), ¯ a.e. t ∈ [S,T ], 
(iii): −p(T ) = λ0gx (x(T )) ¯ + λφ · ∇φ1(x(T )) ¯ + λψ · ∇ψ(x(T )) ¯ , 
(iv): p(t) · f (t, x(t), ¯ u(t)) ¯ − λ0L(t, x(t), ¯ u(t)) ¯ =
max
u∈U (t)

p(t) · f (t, x(t), u) ¯ − λ0L(t, x(t), u) ¯

a.e. t ∈ [S,T ].
If there is no inequality end-point constraint, i.e. C = {x : ψ(x) = 0}, then these 
relations are valid, with λφ = 0 and ‘λφ · ∇φ(x(T )) ¯ ’ deleted from (iii). If there is 
no equality end-point constraint, i.e. C = {x : φ(x) ≤ 0}, then these relations are 
valid with λψ = 0 and ‘λψ · ∇ψ(x(T )) ¯ ’ deleted from (iii). 
A: Proof of the Maximum Principle by Clarke’s Perturbation Methods 
Preliminary Analysis The following lemma justifies the imposition of a number of 
additional, simplifying hypotheses: 
Lemma 1.11.2 (Hypothesis Reduction) Assume that the assertions of the theo￾rem, for some L∞ local minimizer (x,¯ u)¯ , are valid in the special case when 
(a): (C) and (D) are replaced by: there exist k ∈ L1 and c ∈ L1 such that 
(C)'
: |(f, L)(t, x, u)−(f, L)(t, x'
, u)| ≤ k(t) for all x, x' ∈ Rn and u ∈ U (t), a.e. 
t ∈ [S,T ], 
(D)'
: |(f, L)(t, x, u))| ≤ c(t) for all x ∈ Rn and u ∈ U (t), a.e. t ∈ [S,T ], 
and g, φ and ψ are globally Lipschitz continuous,1.11 Appendix: Proof of the Classical Maximum Principle 69
and 
(b): L ≡ 0, 
(c): φj (x(T )) ¯ = 0 for j = 1,...nφ; that is, we can assume that all the end-point 
inequality constraints are active. 
Then the assertions of the theorem are valid without these restrictions. 
Proof 
(a): Consider a modification of (P), in which (f, L)(t, x, u), (g, φ, ψ)(x) are 
replaced by their ‘truncations’ 
(f, L)(t, x(t) ¯ + trϵ (x − ¯x(t)), u), (g, φ, ψ)(x(T ) ¯ + trϵ (x − ¯x(T )))
in which trϵ (y) := 
y if |y| ≤ ϵ
ϵ|y|
−1y if |y| > ϵ . The stronger hypotheses are now satisfied 
(with a modified Lipschitz bound k and parameter ϵ > 0). The process (x,¯ u)¯
retains its status as an L∞ local minimizer and the statement of the necessary 
conditions is unaffected, when we change the data in this way. So applying 
the necessary conditions, restricted to problems for which the data is ‘globally’ 
Lipschitz continuous yields the same necessary conditions for the problem involving 
the original data. 
(b): Assume the assertions of the theorem are valid when L ≡ 0. Define y(t) ¯ :=
 t
0 L(t, x(s), ¯ u(s))ds ¯ . Then ((x,¯ y), ¯ u)¯ is an L∞ local minimizer for the problem 
with ‘augmented state’: 
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize g(x(T )) + y(T )
s.t.
(x(t), ˙ y(t)) ˙ = (f (t, x(t), u(t)), L(t, x(t), u(t)), a.e. t ∈ [S,T ],
u(t) ∈ U (t), a.e. t ∈ [S,T ]
(x(S), y(S)) = (x0, 0), φ(x(T )) ≤ 0 and ψ(x(T )) = 0 .
(This follows easily from the facts that the cost of a process ((x, y), u) for the 
augmented state problem is g(x(T )) + z(T ) = g(x(T )) +  T
0 L(s, x(s), u(s))ds, 
which is the cost of the process (x, u) for the original problem, and that ||x||L∞ ≤
||(x, y)||L∞.) 
The augmented state problem has no integral cost term so, under our assumption, 
we may apply the maximum principle. This yields a costate trajectory (p, q), 
whose components take values in Rn and R respectively, and Lagrange multipliers 
λ0, λφ, λψ . Because (f, L)(t, x, u) does not depend on the y variable, the costate 
equation tells us that q˙ ≡ 0. We can deduce from the transversality condition that 
−q(T ) = λ0∂y (g(x(T )) ¯ + ¯y(T )) = λ0. It follows that we can eliminate the q
variable from the necessary conditions by the substitution ‘q ≡ −λ0’. The necessary 
conditions for the state augmented problems, expressed in this way, are precisely the 
necessary conditions for the original problem.70 1 Overview
(c): We show that we can assume that φj (x(T )) ¯ = 0 for all j = 1,...nφ , If, to 
the contrary, φj (x(T )) < ¯ 0 for some j then, since φj is a continuous function, 
(x,¯ u)¯ continues to be an L∞ local minimizer when this constraint is removed 
from the problem formulation. The necessary conditions for the problem without 
this constraint are the same as the necessary conditions for the problem with the 
constraint, in which the multiplier λφ
j is set to 0. Notice that we can thereby arrange 
that the Lagrange multiplier λφ automatically satisfies the condition: λφ
k = 0 if 
φk(x(T )) < ¯ 0, k = 1,...,nφ. ⨅⨆
Henceforth, we assume that the stronger hypotheses (C)' and (D)' are imposed 
on the data, L ≡ 0 and all the end-point inequality constraints are satisfied at x(T ) ¯ . 
According to the preceding lemma, no loss of generality is involved. Define 
U := {u : [S,T ] → Rm : u is L-meas and u(t) ∈ U (t) a.e. t ∈ [S,T ]}.
Take any u ∈ U. By standard existence/uniqueness properties of differential equa￾tions under these strengthened hypotheses, there is a unique absolutely continuous 
solution to 

x(t) ˙ = f (t, x(t), u(t)) a.e. t ∈ [S,T ]
x(S) = x0 ,
which we can write, unambiguously, as xu. For u ∈ U define 
J (u) := g(xu(T )) .
Lemma 1.11.3 Take u, u' ∈ U. Then 
||xu − xu'
||L∞ ≤ 2K

A(u,u'
)
c(t)dt , (1.11.1) 
in which K := exp(
 T
S k(t)dt) and A(u, u'
) := {t ∈ [S,T ] : u(t) /= u'
(t)}. 
Proof For every t ∈ [S,T ], the continuous function t → xu(t) − xu'
(t) satisfies 
|xu(t) − xu'
(t)| ≤ 
[S,t]
(|f (s, xu(s), u(s)) − f (s, xu'
(s), u(s))|
+ |f (s, xu'
(s), u(s)) − f (s, xu'
(s), u'
(s))|)ds
≤

[S,t]
(k(s)|xu(s) − xu'
(s)|+|f (s, xu'
(s), u(s)) − f (s, xu'
(s), u'
(s))|)ds .1.11 Appendix: Proof of the Classical Maximum Principle 71
By Gronwall’s lemma (Lemma 6.2.4) 
||xu − xu'
||L∞ ≤ K
 T
S
|f (s, xu'
(s), u(s)) − f (s, xu'
(s), u'
(s))|ds
≤ 2K

A(u,u'
)
c(t)dt ,
in which K = exp(
 T
S k(t)dt). This is the required estimate. ⨅⨆
Lemma 1.11.4 (Needle Variations) Take any u, u' ∈ U. Write x' := xu'
. Take the 
point t
¯ ∈ (S, T ) to be such that t
¯ is a Lebesgue point of t → f (t, x'
(t), u(t)) −
f (t, x'
(t), u'
(t)), t → k(t) and t → c(t) anad u(¯ t)¯ ∈ U (t)¯ . (Such points comprise 
a set of full measure.) 
Let the absolutely continuous function p be the solution to 

− ˙p(t) = p(t) · ∇xf (t, x'
(t), u'
(t)) a.e. t ∈ [S,T ]
−p(T ) = ∇xg(x'
(T )) . (1.11.2) 
Take ϵi ↓ 0 such that ϵi < T − t
¯ for each i. For each t ∈ [S,T ] and i = 1, 2,...
define 
ui(t) := 
u(t) if t ∈ [t ,¯ t
¯ + ϵi]
u'
(t) if t /∈ [t ,¯ t
¯ + ϵi]
.
Then 
lim
i→∞ ϵ−1
i (J (ui) − J (u'
)) = −p(t)¯ · [f (t,x ¯ '
(t), u( ¯ t)) ¯ − f (t,x ¯ '
(t), u ¯ '
(t)) ¯ ].
(1.11.3) 
Proof Write xi := xui . Since xi and x' satisfy the dynamic constraint, we know 
that, for each i, 
J (ui) − J (u'
) = g(xi(T )) − g(x'
(T ))
+
 T
t
¯+ϵi
p(s) ·

x˙i(s) − ˙x'
(s) − f (s, xi(s), ui(s)) + f (s, x'
(s), u'
(s))
ds .
Performing an integration by parts (on the term  T
t
¯+ϵi p(s)·(x˙i(s)− ˙x'
(s))ds), noting 
the defining relations for p and also that 
xi(t
¯ + ϵi) − x'
(t
¯ + ϵi) =
 t
¯+ϵi
t
¯
(f (s, xi(s), ui(s)) − f (s, x'
(s), u'
(s)))ds,72 1 Overview
we arrive at the following identity for the integral term on the right of the previous 
equation 
 T
t
¯+ϵi
p(s) ·

x˙i(s) − ˙x'
(s) − f (s, xi(s), ui(s)) + f (s, x'
(s), u'
(s))
ds
= p(T ) · (xi(T ) − x'
(T )) − p(t
¯ + ϵi) · (xi(t
¯ + ϵi) − x'
(t
¯ + ϵi))
−
 T
t
¯+ϵi

p(s) ˙ · (xi(s) − x'
(s))
+p(s) · (f (s, xi(s), ui(s)) − f (s, x'
(s), u'
(s)))
ds
= −∇g(x'
(T )) · (xi(T ) − x'
(T ))
−p(t
¯ + ϵi) ·
 t
¯+ϵi
t
¯
(f (s, xi(s), ui(s)) − f (s, x'
(s), u'
(s)))ds
−
 T
t
¯+ϵi
p(s) ·

f (s, x(s), ui(s)) − f (s, x'
(s), u'
(s))
−∇xf (t, x'
(s), u'
(s)) · (xi(s) − x'
(s))
ds .
Assembling there relations and dividing across by ϵi yields: 
ϵ−1
i (J (ui) − J (u'
)) = a(1)
i + a(2)
i − a(3)
i , (1.11.4) 
in which 
a(1)
i := −p(t
¯ + ϵi) · ϵ−1
i
 t
¯+ϵi
t
¯ (f (s, xi(s), ui(s)) − f (s, x'
(s), u'
(s)))ds, 
a(2)
i := ϵ−1
i

g(xi(T )) − g(x'
(T )) − ∇g(x'
(T )) · (xi(T ) − x'
(T )
and 
a(3)
i :=  T
t
¯+ϵi �
(3)
i (s)ds, 
in which 
�
(3)
i (s) := p(s)·ϵ−1
i

f (s, xi(s), ui(s))−f (s, x'
(s), u'
(s))−∇xf (t, x'
(s), u(s'
))·
(xi(s) − x'
(s))
. 
Our next task is to examine the numbers a(1)
i , a(2)
i and a(3)
i , in the limit as i → ∞. 
Notice first that L-meas{t ∈ [S,T ] : ui(t) /= u'
(t)} → 0, as i → ∞. It follows 
from Lemma 1.11.3 that 
||xi − x'
||L∞ → 0, as i → ∞. (1.11.5) 
Since t
¯ is a Lebesgue point of c, we know that ϵ−1
i
 t
¯+ϵi
t
¯ c(t)dt, i = 1, 2,..., is a 
bounded sequence. It follows from Lemma 1.11.3 that, for some L > 0
ϵ−1||xi − x'
||L∞ ≤ L, for i = 1, 2,... (1.11.6)1.11 Appendix: Proof of the Classical Maximum Principle 73
For each i, 
|
 t
¯+ϵi
t
¯
(f (t, xi(s), ui(s)) − f (t, x'
(s), u'
(s)))ds|
= |  t
¯+ϵi
t
¯
(f (t, x'
(s), u(s)) − f (t, x'
(s), u'
(s)))ds| ≤ γi ,
where γi := ϵi ×  t
¯+ϵi
t
¯ c(s)ds. Using the facts that ϵ−1
i γi → 0 as i → 0, 
that p is continuous and that t
¯ is a Lebesgue point of s → f (s, x'
(s), u(s)) −
f (s, x'
(s), u'
(s)), we deduce from this estimate that 
a(1)
i → −p(t)¯ · (f (t,x ¯ '
(t), u( ¯ t)) ¯ − f (t,x ¯ '
(t), u ¯ '
(t))), ¯ as i → ∞. (1.11.7) 
Making use of the fact that g is continuously differentiable on a neighbourhood of 
x'
(T ), we deduce from the mean value theorem that 
a(2)
i → 0, as i → ∞. (1.11.8) 
Since, for a.e. t ∈ [S,T ], x → p'
(t) · f (t, x, u'
(t)) is continuously differentiable 
on a neighbourhood of x(t) ¯ , we can deduce from (1.11.5) and (1.11.6) and the mean 
value theorem that 
�
(3)
i (s) → 0, a.e. s ∈ [S,T ] .
The functions �
(3)
i , i = 1, 2,... are majorized by the common integrable function 
L(||p||L∞ + 1)k(s), in which L is the constant of (1.11.6). By the dominated 
convergence theorem 
a(3)
i → 0, as i → ∞. (1.11.9) 
It follows from (1.11.4), (1.11.7), (1.11.8) and (1.11.9) that the limit lim
i→∞ ϵ−1
i
(J (ui) − J (u'
)) exists and is given by (1.11.3). ⨅⨆
With these analytical tools, we are now ready to prove the maximum principle. 
Step 1: The Maximum Principle for a Problem with Free Right End-Point 
Proposition 1.11.5 Let (x,¯ u)¯ be an L∞ local minimizer for the special case of (P), 
in which C = Rn. Assume hypotheses (A), (B), (C) and (D) are satisfied. Then the 
maximum principle (Theorem 1.11.1) is valid with λ0 = 1 and terms in the theorem 
statement involving λφ and λψ deleted. 
Proof We can assume (C) and (D) have been strengthened to (C)' and (D)' and 
L ≡ 0. Let p be the solution to the costate Eq. (1.11.2), in which we take (x'
, u'
) =74 1 Overview
(x,¯ u)¯ . All the assertions of the proposition follow from the definition of p, with 
the choice λ0 = 1, with the exception of condition (iv). To prove this last condition 
suppose, to the contrary, that 
p(t) · f (t, x(t), ¯ u(t)) < ¯ max
u∈U (t) p(t) · f (t, x(t), u) ¯ , 
for all t in a subset of [S,T ] of positive measure. A measurable selection theorem 
(whose application is justified, given the hypothesized measurability properties of 
f and U) supplies a control function u ∈ U such that 
p(t) · f (t, x(t), ¯ u(t)) < p(t) ¯ · f (t, x(t), u(t)) ¯
for all t in a subset of [S,T ] of positive measure. This contradicts relation (1.11.3) 
in Lemma 1.11.4 . So condition (iv) is also satisfied. ⨅⨆
Step 2: Completion of the Proof 
Take ϵi ↓ 0. Define, for each i, the function 
di
(x) :=
((g(x) − g(x(T )) ¯ + ϵi) ∨ 0)2 +nφ
j=1
(φi(x) ∨ 0)2 + |ψ(x)|
2 ,
(1.11.10) 
in which a ∨ b := max{a, b}. 
Consider, for each i, the dynamic optimization problem 
(P )i
⎧
⎪⎪⎨
⎪⎪⎩
Minimize di
(x(T ))
s.t.
x(t) ˙ = (f (t, x(t), u(t)) and u(t) ∈ U (t), a.e. t ∈ [S,T ],
x(S) = x0 and ||x − ¯x||L∞ ≤ ϵ .
Define 
M := {u : [S,T ] → Rm : u is L-meas., u(t) ∈ U (t), a.e., and ||xu − ¯x||L∞ ≤ ϵ}.
Problem (P )i with the specified end-point constraint can be posed as an optimiza￾tion problem with domain control functions u ∈ M, namely 
Minimize {Ψi(u) : u ∈ M},
in which Ψi(u) := di
(xu(T )) .
Now equip M with the Ekeland metric 
dE (u, u'
) := L-meas{t ∈ [S,T ] : u(t) /= u'
(t)}.
M is a closed, non-empty set and Ψi is a continuous function M w.r.t. this metric. 
Notice that 
Ψi(u)¯ = ϵi and Ψi(u) ≥ 0, for all u ∈ M.1.11 Appendix: Proof of the Classical Maximum Principle 75
By Ekeland’s theorem, there exists ui ∈ M such that 
Ψi(ui) = min{Ψi(u) + ϵ
1
2
i dE (u, ui) ,u ∈ M}
and 
dE (ui, u)¯ ≤ ϵ
1
2
i . (1.11.11) 
Define 
mi(t, u) := 
1 if u /= ui(t)
0 if u = ui(t) .
Write xi := xui . It follows from Lemma 1.11.3 that 
||xi − ¯x||L∞ → 0, as i → ∞.
It is important to observe that, for i sufficiently large, 
di
(xi(T )) > 0 . (1.11.12) 
Indeed, the contrary assertion ‘di
(xi(T )) = 0’ implies that g(xi(T )) − g(x(T )) ¯ =
−ϵi, φ(xi(T )) ≤ 0 and ψ(xi(T )) = 0. Since ||xi − ¯x||L∞ → 0, as i → ∞, this 
violates the optimality of u¯ (relative to controls u ∈ U that are admissible and satisfy 
||xu − ¯x||L∞ ≤ β, for some arbitrary, pre-assigned β > 0). Referring back to the 
definition of di
, we see that di is C1 near xi(T ). 
Since 
L-meas{t ∈ [S,T ] : u /= ui(t)} =  T
S
mi(t, u(t))dt ,
the above minimization property implies: for sufficiently large i, (xi, ui) is an L∞
local minimizer for the dynamic optimization problem 
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
Minimize di
(x(T )) + ϵ
1
2
i
 T
S mi(t, u(t))dt
subject to
x(t) ˙ = f (t, x(t), u(t)) and u(t) ∈ U (t), a.e. t ∈ [S,T ],
x(S) = x0 .
This is a free right end-point problem to which the special case of the maximum 
principle, Proposition 1.11.5, is applicable. (Notice that the relevant hypotheses 
are satisfied, including the continuous differentiability of di on a neighbourhood 
of xi(T ).)76 1 Overview
Fix u ∈ U. We deduce the existence of pi ∈ W1,1 such that, for a.e. t ∈ [S,T ], 
− ˙pi(t) = pi(t) · fx (t, xi(t), ui(t)) , (1.11.13) 
pi(t) · f (t, xi(t), ui(t)) ≥ pi(t) · f (t, xi(t), u(t)) − ϵ
1
2
i (1.11.14) 
and 
− pi(T ) = di
x (xi(T )) . (1.11.15) 
We deduce from (1.11.10) and (1.11.12) that 
di
x (xi(T )) := (λ0
i , λφ
i , λψ
i ) · (gx (xi(T )), φx (xi(T )), ψx (xi(T ))),
in which 
(λ0
i , λφ
i , λψ
i ) :=
1
di(xi(T )) ((g(xi(T )) − g(x(t)) ¯ + ϵi) ∨ 0),
{(φj (xi(T )) ∨ 0)}
nφ
j=1, ψ(xi(T )) .
Observe that, by (1.11.10) and (1.11.12), we have 
di
(xi(T )) =

((g(xi(T )) − g(x(T )) ¯ + ϵi) ∨ 0)2
+
nφ
j=1
(φj (xi(T )) ∨ 0)2 + |ψ(xi(T ))|
2

> 0.
We see that 
λ0
i ≥ 0, λφ
i ∈ (R+)
nφ , λψ
i ∈ Rnψ and |(λ0
i , λφ
i , λψ
i )| = 1, for each i .
(1.11.16) 
Our next task is to pass to the limit, as i → ∞, in relations (1.11.13), (1.11.14), 
(1.11.15) and (1.11.16), which will be recognized as perturbed versions of assertions 
(i)–(iv) in the proposition statement. 
In view of (1.11.16), we can arrange, by subsequence extraction, that 
(λ0
i , λφ
i , λψ
i ) → (λ0, λφ, λψ )
for some (λ0
i , λφ
i , λψ
i ) such that λ0
i ≥ 0, λφ
i ∈ (R+)nφ , λψ
i ∈ Rnψ and 
|(λ0
i , λφ
i , λψ
i )| = 1.1.11 Appendix: Proof of the Classical Maximum Principle 77
We know from (1.11.11) that L-meas{t ∈ [S,T ] : ui(t) /= ¯u(t)} → 0 as 
i → ∞. We can therefore arrange, by subsequence extraction, that 
∞
i=0
L-meas{t ∈ [S,T ] : ui(t) = ¯ / u(t)} < ∞. (1.11.17) 
Define the nested sequence of sets 
Aj := {t ∈ [S,T ] : ui(t) = ¯ / u(t) for some i ≥ j }, j = 1, 2,...
and write 
A := ∩∞
j=1Aj .
In view of (1.11.17), L-meas{Aj } → 0, as j → ∞. It follows that A has measure 
0. Now define 
T := [S,T ] \A.
T is a measurable subset of [S,T ] having full measure. It has representation 
T := {t ∈ [S,T ] : there exists an integer i(t)
such that uj (t) = ¯u(t) for all j ≥ i(t)}.
From (1.11.13) and (1.11.16), the continuous function qi(s) := pi(T − s), 0 ≤ s ≤
T − S satisfies 
|qi(s))|≤|(λ0
i , λφ
i , λψ
i ) · (gx (xi(T )), φx (xi(T )), ψx (xi(T )))|+  s
0
|qi(s'
)| · αi(s'
)ds' ,
in which αi(s) := |∇xf (T −s, xi(T −s), ui(T −s))|. Since, for each i, ||αi||L∞ ≤
||k||L∞, it follows from Gronwall’s lemma that qi are uniformly bounded and 
therefore ||pi||L∞ ≤ c1 for some c1 > 0, independent of i. But then, from (1.11.13), 
the p˙i’s are dominated by the integrable function t → c1k(t). It can be deduced from 
Ascoli’s theorem that, along some subsequence, pi → p, uniformly, and p˙i → ˙p, 
in the weak L1 topology, for some absolutely continuous function p. 
For each t ∈ [S,T ], we deduce from (1.11.13) that 
pi(t) = (λ0
i , λφ
i , λψ
i ) · (gx (xi(T )), φx (xi(T )), ψx (xi(T )))|
−
 T
t
pi(s) · fx (s, xi(s), ui(s))ds.
The integrand is bounded by the integrable function t → c1 × k(t) and at all 
points t lying in the set T (a set of full measure), the integrand converges to 
s → p(s) · fx (s, x(s), ¯ u(s)) ¯ . Applying the dominated convergence theorem and78 1 Overview
using the convergence properties of {pi} and {(λ0
i , λφ
i , λψ
i )}, we can conclude that 
the limiting integrand is integrable and 
p(t) = (λ0, λφ, λψ ) −
 T
t
p(s) · fx (s, x(s), ¯ u(s))ds. ¯
Expressing this relation as a differential equation with boundary conditions we 
arrive at conditions (ii) and (iii) of the proposition statement. We have also shown 
that the non-triviality condition (i) is also satisfied. 
For every point t ∈ T , both sides of relation (1.11.14) converge and yield in the 
limit as i → ∞, 
p(t) · f (t, x(t), ¯ u(t)) ¯ ≥ p(t) · f (t, x(t), u(t)) . ¯ (1.11.18) 
(Recall that, here, u is an arbitrary element in U.) If assertion (iv) were false, then 
there would exist a subset of points t, of positive measure, on which 
p(t) · f (t, x(t), ¯ u(t)) < ¯ sup
u∈U (t)
p(t) · f (t, x(t), u) . ¯
We could then choose the control function u such that 
p(t) · f (t, x(t), ¯ u(t)) < p(t) ¯ · f (t, x(t), u(t)), ¯
for all t in a set of positive measure. This contradicts (1.11.18). Assertion (iv) is 
confirmed. □
B: Proof of the Maximum Principle by Set Separation Methods 
Take a control system 
(S)
⎧
⎨
⎩
x(t) ˙ = f (t, x(t), u(t)), a.e. t ∈ [S,T ],
u(t) ∈ U (t), a.e. t ∈ [S,T ],
x(S) = x0 ,
with data as in the dynamic optimization problem (P). Assume that f and U (t), 
S ≤ t ≤ T , satisfy hypotheses (A), (C) and (D) of Theorem 1.11.1. Fix a process 
(x,¯ u)¯ for (S) and β > 0. 
Take a function d : Rn → Rk and β > 0. Define the β-local reachable set for 
(S) at time T , relative d, to be 
Rd (T ) := {ξ ∈ Rk : ξ = d(xu(T )) for some u ∈ U such that ||xu − ¯x||L∞ ≤ β}.
Here, as before xu denotes the state trajectory for (S) corresponding to u ∈ U. 
The following theorem provides sufficient conditions for a point to be a boundary 
point of Rd (T ).1.11 Appendix: Proof of the Classical Maximum Principle 79
Theorem 1.11.6 (Maximum Principle for Boundary Processes) Let (x,¯ u)¯ be a 
process for (S) such that 
d(x(T )) ¯ ∈ bdy Rd (T ) .
Assume that f and U (t), S ≤ t ≤ T , satisfy hypotheses (A), (C) and (D) of 
Theorem 1.11.1 and there exists ϵ > 0 such that f (t, ., u) is C1 on x(t) ¯ + ϵB
for all u ∈ U (t), a.e. t ∈ [S,T ], and d is C1 on x(T ) ¯ +ϵB. Then there exist λ ∈ Rk
and p ∈ W1,1([S,T ]; Rn) such that λ /= 0 and 
(i): − ˙p(t) = p(t) · fx (t, x(t), ¯ u(t)) ¯ a.e. t ∈ [S,T ], 
(ii): p(t) · f (t, x(t), ¯ u(t)) ¯ = max
u∈U (t) p(t) · f (t, x(t), u), ¯ a.e. t ∈ [S,T ], 
(iii): −p(T ) = λ · dx (x(T )) ¯ . 
Before giving the proof of this theorem, we show that it leads to the maximum 
principle for the optimization problem (P), which we reproduce here as a corollary: 
Corollary 1.11.7 Let (x,¯ u)¯ be a an L∞ local minimizer for problem (P). Assume 
hypotheses (A)–(D) of Theorem 1.11.1. Then there exist λ0 ≥ 0 and λφ ∈ (R+)nφ
and λψ ∈ Rnψ and p ∈ W1,1([S,T ]; Rn) such that λφ
k = 0 if φk(x(T )) < ¯ 0, 
k = 1,...,nφ, and 
(i): (p, λ0, λφ, λψ ) /= (0, 0, 0, 0), 
(ii): − ˙p(t) = p(t) · fx (t, x(t), ¯ u(t)) ¯ − λ0Lx (t, x(t), ¯ u(t)), ¯ a.e. t ∈ [S,T ], 
(iii): −p(T ) = λ0gx (x(T )) ¯ + λφ · φ1
x (x(T )) ¯ + λψ · ψx (x(T )) ¯ , 
(iv): p(t) · f (t, x(t), ¯ u(t)) ¯ − λ0L(t, x(t), ¯ u(t)) ¯
= max
u∈U (t)

p(t) · f (t, x(t), u) ¯ − λ0L(t, x(t), u) ¯

a.e. t ∈ [S,T ].
Proof As we have shown in Section A, which provided the proof of the maximum 
principle by perturbational methods, it is required to proof the corollary only in the 
special case when L = 0 and the inequality end-point constraints are all active. 
Step 1: We prove the assertions of the maximum principle, with the exception of the 
non-negativity conditions on the cost and inequality constraint Lagrange multipliers. 
Choose the function d : Rn → R1+nφ+nψ in Theorem 1.11.6 to be 
d(x) := (g(x) − g(x(T )), φ(x), ψ(x)) ¯ for x ∈ Rn
and let β > 0 to be a number such that (x,¯ u)¯ is minimizing relative to admissible 
processes (x, u) such that ||x − ¯x||L∞ ≤ β. Then, for this β, 
(0, 0, 0) ∈ bdy Rd (T ). (1.11.19) 
This is because, otherwise, there would exist γ > 0 and a process (x, u) such that 
g(x(T )) − g(x(T )) ¯ = −γ , φ(x(T )) = 0, ψ(x(T )) = 0 and ||x − ¯x||L∞ ≤ β. This 
violates the local optimality of (x,¯ u)¯ .80 1 Overview
Assertions (i)–(iii) of the Corollary 1.11.7, for some (λ0, λφ, λψ ) /= (0, 0, 0), 
now follow from Theorem 1.11.6, with the exception of the non-negativity require￾ments: λ0 ≥ 0, λφ ∈ (R+)nφ and λψ ∈ Rnψ . 
Step 2: Consider the problem 
(Q)
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize g(x(T )) + z0(T )
s.t.
(x(t), ˙ z(t)) ˙ = (f (t, x(t), u(t)), v(t)), a.e. t ∈ [S,T ],
u(t) ∈ U (t), v(t) ∈ [0, 1]
nφ+1 a.e. t ∈ [S,T ],
(x(S), z(S)) = (0, 0), φ1(x(T )) + z1(T ), . . . , φnφ (x(T ))
+znφ (T ) = 0 and ψ(x(T )) = 0 .
Let v,¯ z¯ : [S,T ] → Rnφ+1 be the functions v¯ ≡ 0 and z¯ ≡ 0. 
The process ((x,¯ z), ( ¯ u,¯ v)) ¯ is clearly admissible for (Q). We claim that it is also 
an L∞ local minimizer for (Q). To justify this, suppose that this assertion is not true. 
Then, for any β > 0, we can find a process ((x, z), (u, v)) for (Q) such that 
g(x(T )) < g(x(T )) ¯ −
 T
0
v0(t)dt, φj (x(T )) = −  T
0
vj (t)dt,
for j = 1,...,nφ,
ψ(x(T )) = 0 and ||(x, y) − (x,¯ z)¯ ||∞
L ≤ β
and ||(x, z) − (x,¯ z)¯ ||L∞ ≤ β .
Taking note of the fact that the vj ’s are non-negative valued functions, we 
conclude that g(x(T )) < g(x(T )) ¯ , φj (x(T )) ≤ 0 for j = 1,...nφ, ψ(x(T )) = 0
and ||x − ¯x||L∞ ≤ β. Since β > 0 is arbitrary, this means that (x,¯ u)¯ cannot be an 
L∞ local minimizer for the original problem. We have confirmed the claim. 
Now apply the necessary conditions of Step 1, with reference to ((x,¯ z), ( ¯ u,¯ v)) ¯ . 
Write p and (q0, q = (q1,...,qnφ )) for costate function components corresponding 
to the x and z variables. Let λ0, λφ and λψ be the Lagrange multipliers associated 
with the cost and end-point constraints. Then 
(p, λ0, λφ, λψ ) /= (0, 0, 0, 0), 
− ˙p(t) = p(t) · fx (t, x(t), ¯ u(t)) ¯ and − (q˙0, q)(t) ˙ = 0, a.e. t ∈ [S,T ], 
−p(T ) = λ0gx (x(T )) ¯ + λφ · φ1
x (x(T )) ¯ + λψ · ψx (x(T )) ¯
and −(q0, q)(T ) = (λ0, λφ), 
and 
p(t) · f (t, x(t), ¯ u(t)) ¯ = max
u∈U (t) p(t) · f (t, x(t), u) ¯
+ nφ
j=0 max
vj∈[0,1]
qj (t)vj , for a.e. t ∈ [S,T ].1.11 Appendix: Proof of the Classical Maximum Principle 81
Setting u = ¯u(t) in the last condition yields the information that qj (t) ≤ 0, for 
j = 0,...,nφ, a.e. t ∈ [S,T ]. Since −q0 ≡ λ0 and −q ≡ λφ, we can conclude 
that λ0 ≥ 0 and λφ ≥ 0. Contained in the preceding conditions are all the assertions 
of the theorem statement. ⨅⨆
Proof of Theorem 1.11.6 (Maximum Principle for Boundary Processes) Take a 
collection of control functions uj ∈ U, j = 1,...,r. Define 
Λ(r) := {α ∈ (R+)
r : α1 + ... + αr ≤ 1}.
For any α ∈ Λ define x(α) to be the solution of 
(S)relaxed
⎧
⎪⎨
⎪⎩
x(t) ˙ = f (t, x(t), u(t)) ¯
+ r
j=1 αj

f (t, x(t), uj (t)) − f (t, x(t), u(t)) ¯

a.e.,
x(S) = x0 .
Notice that elements x(α) may fail to be state trajectories, but they are ‘relaxed’ state 
trajectories, in the sense that they satisfy the differential inclusion 
x˙(α)(t) ∈ co {f (t, x(α)(t), u) : u ∈ U (t)}, for a.e. t ∈ [S,T ] . (1.11.20) 
Here ‘co’ denotes ‘convex hull’. Define y(α) to be the solution of 
(S)lin
⎧
⎪⎨
⎪⎩
y(t) ˙ = fx (t, x(t), ¯ u(t))y(t) ¯
+ r
j=1 αj

f (t, x(t), u ¯ j (t)) − f (t, x(t), ¯ u(t)) ¯

a.e.,
y(S) = 0 .
Preliminaries 
The following estimate justifies the interpretation (S)lin as a local linearization of 
(S)relaxed: 
Lemma 1.11.8 (Basic Estimates) There exist K > 0 and a function θ : R+ → R+
such that lims↓0 θ (s) = 0 and, for all α ∈ Λ(r), 
||(x(α) − ¯x) − y(α)||L∞ ≤ |α|1 × θ (|α|1) (1.11.21) 
and 
||y(α)||L∞ ≤ K|α|1 , (1.11.22) 
in which |α|1 := 
j αj . 
The proof of this lemma, which is based on the definitions of x(α) and y(α)
as solutions to the differential equations (S)relaxed and (S)lin, respectively, and 
Gronwall’s lemma, is omitted.82 1 Overview
Relaxation theorems in control theory tells us that solutions to the differential 
inclusion (1.11.20) can be mapped into neighbouring state trajectories for (S). The 
proof of the maximum principle by set separation methods makes use of a relaxation 
theorem, in which the mapping, restricted to a finitely generated class of relaxed 
state trajectories, can be chosen to be continuous. 
Lemma 1.11.9 (Relaxation) For any given δ > 0, there exists a continuous 
mapping χδ : Λ(r) → C([S,T ]; Rn) such that, for each α ∈ Λ(r), 
(i): χδ(α) is a state trajectory for (S), 
and 
(ii): ||χδ(α) − x(α)||L∞ ≤ δ. 
The proof of this lemma, which is available in a number of standard texts, e.g. [21], 
is not reproduced here. 
With these preliminaries behind us, we are ready to return to the proof. The 
following set should be regarded as a first order approximation to the reachable set 
Rd (T )
Rapprox
d (T ) := {∇d(x(T )) ¯ · yu(T ) : u ∈ U},
in which yu denotes the solution of the linearized system 

y(t) ˙ = fx (t, x(t), ¯ u(t))y(t) ¯ + f (t, x(t), u(t)) ¯ − f (t, x(t), ¯ u(t)), ¯ a.e. t ∈ [S,T ]
y(S) = 0 .
The key step is to establish that 
‘0 ∈ int co Rapprox
d (T )’ implies ‘d(x(T )) ¯ ∈ int Rd (T )’. (1.11.23) 
To prove this relation, assume that 0 ∈ int co Rapprox
d (T ). It follows that, for some 
r0 > 0, r0B ⊂ coRapprox
d (T ). Take any ξ¯ ∈ r0B, such that ξ¯ /= 0. Then there exists 
a simplex S generated by 0 and the linearly independent vectors ξ1 ...,ξr in the 
(unconvexified) set Rapprox
g (T ) such that ξ¯ is interior to S. 
The vectors in S are in a one-to-one relation with the elements in Λ(r): 
S = {ξ ∈ Rr : ξ = F α, α ∈ Λ(r)},
in which F ∈ Rr×r is the invertible matrix F := row{ξ1,...,ξr}. 
We can deduce from the property that ξ¯ ∈ int S that there exists ρ > 0 such that 
ξ¯ + ρB ⊂ S. Define α(ξ ) := F −1ξ . Since ξ¯ + ρB ⊂ S, we know that 
α(ξ ) ∈ Λ(r), for all ξ ∈ ξ¯ + ρB ⊂ S .1.11 Appendix: Proof of the Classical Maximum Principle 83
For i = 1, 2,...,r, we can choose ui ∈ U such that 
ξi = dx (x(T ))y ¯ ui(T ), for i = 1,...,r.
Now let us assume that the arbitrary collection of control functions {u1,...,ur}, 
introduced at the at the beginning of the proof, have been chosen to be the control 
functions in the preceding relation. 
Take any ϵ ∈ (0, 1) and δ > 0. Now consider the mapping �ϵ,δ : ξ¯ + ρB → Rr
defined as follows: 
�ϵ,δ(ξ ) := ϵ−1

− d(χδ(ϵα(ξ ))(T )) + d(x(T )) ¯

+ ξ¯ + ∇d(x(T )) ¯ · y(ϵα(ξ )) .
Using the properties of the mapping χδ provided by Lemma 1.11.9 and also the 
local Lipschitz continuity of d, we can show that 
|�ϵ,δ(ξ )−ξ¯| ≤ ϵ−1
!
!
!−d(x(ϵα(ξ ))(T ))+d(x(T )) ¯ +∇d(x(T )) ¯ ·y(ϵα(ξ ))(T )
!
!
!+ϵ−1e(δ) ,
in which e : R+ → R+ is a mapping such that e(s) ↓ 0 as s ↓ 0. But then, 
|�ϵ,δ(ξ ) − ξ¯| ≤ ϵ−1
!
!
! − d(y(ϵα(ξ ))(T ) + ¯x(T ))
+ d(x(T )) ¯ + ∇d(x(T )) ¯ · y(ϵα(ξ ))(T )
!
!
!
+ ϵ−1e(δ) + kd θ (ϵ|α(ξ )|1) ,
in which θ is the modulus of continuity of Lemma 1.11.8 and kd is a Lipschitz 
constant for d. Employing the mean value function and Lemma 1.11.8 to estimate 
the first term on the right of this inequality and noting that |α|1 ≤ 1 for α ∈ Λ(r), 
we deduce that 
|�ϵ,δ(ξ ) − ξ¯| ≤ ϵ−1e(δ) + kd θ (ϵ) + Kθ1(Kϵ) .
(θ1 is the modulus of continuity for ∇d near x(T ) ¯ and K is the constant of Lemma 
1.11.8.) 
It follows that, by choosing ϵ > 0 and δ > 0 such that ϵ−1e(δ) + kd θ (ϵ) +
Kθ1(Kϵ) ≤ ρ, we can arrange that �ϵ,δ maps the closed ball ξ¯ + ρB into itself. We 
may also arrange, by making a further reduction in the size of ϵ, that, ||χδ(ϵα(ξ )) −
x¯||L∞ ≤ β for all ξ ∈ ξ¯+ρB. (See Lemma 1.11.8.) The mapping �ϵ,δ is continuous. 
We conclude from Schauder’s fixed point theorem that �ϵ,δ has a fixed point ξ . But, 
by definition of F, ξ = F α = ∇d(x(T )) ¯ · y(ϵα(ξ )). It follows from the fixed point 
property that 
d(χδ(ϵα(ξ )(T ))) = d(x(T )) ¯ + ϵξ . ¯84 1 Overview
We have shown that for every non-zero ξ¯ ∈ r0B there exists u ∈ U such that 
||xu − ¯x||L∞ ≤ β and d(xu(T )) = d(x(T )) ¯ + ϵξ¯. This assertion is obviously true, 
with u = ¯u, when ξ¯ = 0. It has been confirmed that d(x(T ))( ¯ = 0) is interior to 
Rd (T ). 
We now carry out the final step of the proof. Notice that the maximum principle 
can be equivalently expressed as follows: there exists a non-trivial vector λ such that 
λ·∇d(x(T )) ¯
 T
S
S(T , s)[f (s, x(s), u(s)) ¯ −f (s, x(s), ¯ u(s)) ¯ ]ds ≤ 0 , for all u ∈ U .
(1.11.24) 
Here, S(t, s) is the fundamental matrix for the linear differential equation y˙ =
∇xf (t, x(t), ¯ u(t))y(t) ¯ , that is S(., s) is the n × n matrix value function that 
satisfies, for each s ∈ [S,T ], the matrix differential equation d/dt S(t, s) =
∇xf (t, x(t), ¯ u(t))S(t, s) ¯ on [S,T ], with boundary condition S(s, s) = I . 
We can rewrite relation (1.11.24), using our earlier notation, as 
λ · ∇d(x(T ))y ¯ u(T ) ≤ 0 , for all u ∈ U . (1.11.25) 
We argue by contraposition. Suppose that the maximum principle is not satisfied. 
Then there exists no non-zero vector λ such that (1.11.25) is satisfied. This means 
that the point 0 is not separated from the convex set co Rapprox
d (T ). It follows from 
the separating hyperplane theorem that 0 is interior to co Rapprox
d (T ). But then, as 
we have seen, d(x(T )) is an interior point of Rd (T ). This means that, if d(x(T ))
is a boundary point of Rd (T ), then the maximum principle is valid. The proof is 
complete. ⨅⨆
Discussion 
We see in the preceding proofs two very different approaches to proving the 
maximum principle. To simplify the discussion, let us assume that the L∞ local 
minimizer of interest is a minimizer and that there is a unique state trajectory 
associated with each control, so we can regard the domain of the dynamic 
optimization problem (P) as the set of controls. The perturbation method can be 
summarized as follows: 
Clarke’s Perturbation Approach take a minimizer u¯ for the original problem (P). 
• Choose ϵ > 0. Find, with the help of Ekeland’s theorem, a control uϵ that is a 
minimizer for a perturbed problem (P)ϵ and whose proximity to the minimizing 
control u¯ is governed by the parameter ϵ. The perturbed problem is one for which 
it is easy to derive necessary condition of optimality; to be specific, the perturbed 
problem has no right end-point constraints. 
• Write down necessary conditions for the (P)ϵ , with reference to the minimizer uϵ . 
• Obtain necessary conditions for the original problem (P), with reference to u¯, 
by passing to the limit, as ϵ ↓ 0, in the necessary conditions for the perturbed 
problem.1.11 Appendix: Proof of the Classical Maximum Principle 85
Set Separation Approach Note that a minimizer u¯ defines a boundary point of the 
reachable set that comprises images of the terminal values of the state trajectories 
under a function having components the cost and equality constraint functions. 
• Construct a (convex) approximating reachable set. Use the Schauder fixed point 
theorem and a relaxation lemma to show that, if 0 is an interior point of the 
approximating reachable set, then u¯ must be associated with an interior point of 
the reachable set, i.e. u¯ does not define a boundary point of the reachable set. 
• Since ‘there exists a linear hyperplane supporting the approximating reachable 
set at 0’ implies ‘the maximum principle is valid’ and the approximation cone 
is convex, we can deduce from the separating hyperplane theorem that ‘the 
maximum principle is not valid’ implies ‘0 is interior to the approximating 
reachable set’. 
These relations combine to give a proof of the maximum principle in contrapositive 
form: ‘the maximum principle is not valid’ =⇒ ‘0 is an interior point of the 
approximating reachable set’ =⇒ ‘u¯ does not define an interior point of the 
reachable set’ =⇒ ‘u¯ is not a minimizer’. 
Note that the relative simplicity of the two proofs should not be judged by their 
length alone. This is because gradient calculations (‘needle’ variations) and detailed 
justification of reductions to special cases are included in the first proof, but not in 
the second. 
Both approaches are powerful ones, and extend to provide necessary conditions 
of optimality for a wide-range of dynamic optimization problems, including those 
with nonsmooth data and a variety of different pathwise constraints. As regards 
extensions based on perturbation methods, Clarke’s nonsmooth maximum principle 
of 1976 stands out for the unrestrictive nature of the hypotheses under which it 
was proved [59]. Other group of contributors to the field who employed such 
methods included Ioffe, Loewen, Rockafellar and Vinter. The body of work on 
extensions to the maximum principle, based on set separation methods, is large; 
a far from complete list of contributors include Pontryagin and his collaborators; 
also Gamkrelidze, Dubovistkii, Milyutin, Dmitruk, Osmolovski, Mordukhovich, 
Neustadt, Warga, Sussmann and Hestenes. It has often been, but not always, the case 
that advances in the field, based on one approach, have been matched by subsequent 
independent proofs based on the other approach. For example, necessary conditions 
for general problems with data which are not continuously differentiable w.r.t. 
the state variable (so-called nonsmooth problems) was introduced independently 
by Clarke, using perturbation methods, and Warga, who followed a set separation 
approach. Even when both approaches can be used to provide necessary conditions 
for a particular class of dynamic optimization problems, they differ often, in terms 
of transparency of the proofs and the relative strengths of the hypotheses that are 
invoked. Both approaches then are distinct and valuable weapons in the armoury of 
modern day research.86 1 Overview
1.12 Exercises 
1.1 Consider a generalization of the fixed left endpoint problem (P) of Sect. 1.5, in 
which the initial state is included in the choice variables: 
(P1)
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize g(x(S), x(T )) +  T
S L(t, x(t), u(t))dt
over absolutely cont. functions x : [S,T ] → Rn
and meas. functions u : [S,T ] → Rm such that
x(t) ˙ = f (t, x(t), u(t)), a.e. t ∈ [S,T ],
u(t) ∈ U (t), a.e. t ∈ [S,T ],
(x(S), x(T )) ∈ C .
(The data are as for fixed left end-point problem (P) of Sect. 1.5 except, now, g has 
domain Rn × Rn and C is a subset of Rn × Rn.) 
Taking as starting point the maximum principle (Theorem 1.5.1) for the fixed left 
end-point problem (P), derive the following maximum principle for (P1): 
Let (x,¯ u)¯ be an L∞ local minimizer for (P1). Assume f , L and U satisfy the 
hypotheses of Theorem 1.5.1, C is closed and g is C1 on a neighbourhood of 
(x(S), ¯ x(T ¯ )). Then there exist λ ≥ 0 and p ∈ W1,1([S, T ]; Rn) such that 
(i): (p, λ) /= (0, 0), 
(ii): − ˙p(t) = p(t) · ∇xf (t, x(t), ¯ u(t)) ¯ − λ∇xL(t, x(t), ¯ u(t)), ¯ a.e. t ∈ [S, T ], 
(iii): p(t) · f (t, x(t), ¯ u(t)) ¯ − λL(t, x(t), ¯ u(t)) ¯ =
max 
u∈U (t)

p(t)·f (t, x(t), ¯ u)−λL(t, x(t), ¯ u)
, a.e. t ∈ [S, T ], 
(iv): (p(S), −p(T )) ∈ λ∇g(x(S), ¯ x(T ¯ )) + NC(x(S), ¯ x(T ¯ )). 
Hint: Show that the process ((x¯1, x¯2),(u¯1, u¯2)) on [S − 1, T ], in which 
(x¯1, u¯1) ≡ (x(S), ¯ 0), and (x¯2, u¯2) =

(x(S), ¯ 0) if t ∈ [S − 1,S)
(x(t), ¯ u(t)) ¯ if t ∈ [S,T ]
is an L∞ local minimizer to the fixed left end-point problem 
(P1)
'
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize g(x1(T ), x2(T )) +  T
S−1

0 if t ∈ [S − 1,S)
L(t, x2(t), u2(t)) if t ∈ [S,T ]

dt
subject to
(x˙1(t), x˙2(t)) =

(u1, u1) if t ∈ [S − 1,S)
(0, f (t, x2(t), u2(t))) if t ∈ [S,T ] ,
u1(t) ∈

[−1, +1]
n if t ∈ [S − 1,S)
{0} if t ∈ [S,T ]
and u2(t) ∈

{0} if t ∈ [S − 1,S)
U (t) if t ∈ [S,T ]
a.e.
(x1(S − 1), x2(S − 1)) = (x(S), ¯ x(S)) ¯ and (x1(T ), x2(T )) ∈ C .
Now apply the fixed left end-point maximum principle.1.12 Exercises 87
1.2 (Catalytic Conversion Problem) This is a dynamic optimization problem 
arising in chemical engineering. Here, the independent variable is not time, but 
displacement s along a one-dimensional, isothermic catalytic reactor. A mixture of 
three chemicals A, B and C is propelled through a longitudinal catalytic converter. 
The objective is to maximize the mole fraction of the chemical C at some point 
along the reactor. Let x1, x2 and x3 denote the mole fractions of the three chemicals 
A, B and C, at displacement s along the reactor. Catalyst a promotes a reversible 
reaction between A and B. Catalyst b promotes an irreversible reaction that converts 
chemical B into C. Intuition suggests that the mixture should first be exposed to 
catalyst a (to convert A to B) and, further along the reactor, to expose it to catalyst 
b (to convert B into C). But this is incorrect. We can deduce from the maximum 
principle that the maximum yield of C is obtained by mixing the catalysts along the 
reactor. The problem can be formulated at follows: 
(P2)
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize − x3(L)
subject to
x˙1(s) = u(s)(k2x2(s) − k1x1(s))
x˙2(s) = u(s)(k1x1(s) − k2x2(s)) − (1 − u(s))k3x2(t)
x˙3(s) = (1 − u(s))k3x2(t)
⎫
⎬
⎭
a.e. s ∈ [0, L],
u(t) ∈ [0, 1] a.e. s ∈ [0, L],
(x1(0), x2(0), x3(0)) = (1, 0, 0),
(x1(L), x2(L), x3(L)) ∈ R3.
Here, the control u(s) is the proportion of catalyst a at displacement s along the 
reactor. k1 and k2 are the reaction constants of catalyst a and k3 is the reaction 
constant for catalyst b. 
Show, using the maximum principle, that, for L sufficiently large, the optimal 
catalyst mixture u(s) ¯ , 0 ≤ s ≤ L has the following structure: For some intermediate 
displacements 0 < S1 < S2 < L and some constant u∗ ∈ (0, 1) , 
u(s) ¯ =
⎧
⎨
⎩
1 if s ∈ [0, S1)
u∗ if s ∈ [S1, S2)
0 if s ∈ [S2, L] .
Remark 
This example illustrates that in applied dynamic optimization problems, the inde￾pendent variable need not have the interpretation ‘time’; here is it displacement 
along a reactor duct. 
1.3 [Linear Quadratic Control] This problem arises in control engineering where, 
typically, x and u describe deviations from a nominal state and control. The first 
order controlled differential equation is often a local linear approximation to a non￾linear dynamic constraint. The cost function penalizes large deviations of state and 
control.88 1 Overview
(LQ)
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize  T
0 (xT (t)Q(t)x(t) + uT (t)R(t)u(t))dt + xT (T )Gx(T )
over u ∈ L2 and x ∈ W1,1 satisfying
x(t) ˙ = A(t)x(t) + B(t)u(t) a.e. t ∈ [0, T ],
u(t) ∈ Rm, a.e. t ∈ [0, T ],
x(0) = x0,
with data the number T > 0, symmetric, essentially bounded, measurable matrix 
valued functions Q : [0, T ] → Rn×n, R : [0, T ] → Rm×m, a symmetric matrix 
G ∈ Rn×n, essentially bounded, measurable, matrix valued functions A : [0, T ] →
Rn×n and B : [0, T ] → Rn×m and x0 ∈ Rn. Assume that 
G ≥ 0 and there exists ϵ > 0 such that Q(t) ≥ 0, R(t) ≥ ϵIm×m for a.e. 
t ∈ [0, T ]. 
Use dynamic programming to show that the minimizing process (x,¯ u)¯ is 
generated by the linear feedback law 
u(t) ¯ = −R−1(t)BT (t)P (t)x(t) , ¯
in which P : [0, T ] → Rn×n is the unique absolutely continuous, symmetric matrix 
value function satisfying the Riccati equation 

−P (t) ˙ = AT (t)P (t) + P (t)A(t) + Q − P (t)B(t)R−1(t)BT (t)P (t) a.e..
P (T ) = G .
Hint: Seek a solution V to the Hamilton Jacobi equation having the structure 
V (t, x) = xT P (t)x. 
1.4 Adapt the analysis of Exercise 1.3, to determine the minimizing process, when 
we allow for an inhomogeneous term r : [0, T ] → Rn in the controlled differential 
equation, which now becomes 
x(t) ˙ = A(t)x(t) + B(t)u(t) + r(t) a.e. t ∈ [0, T ].
Hint: Seek a solution V to the Hamilton Jacobi equation of the form V (t, x) =
xT P (t)x + 2rT (t)x. 
1.5 (Infinite Horizon Control) Consider the problem 
(I )
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize lim infT →∞  T
0 L(x(t), u(t))dt
over locally absolutely cont. x : [0,∞) → Rn and meas. u : [0,∞) → Rm
such that
x(t) ˙ = f (x(t), u(t)), a.e. t ∈ [0,∞),
u(t) ∈ U, a.e. t ∈ [0,∞),
x(0) = x0.1.13 Notes for Chapter 1 89
The data comprise functions f : Rn × Rm → Rn, L : Rn × Rm → R, a compact 
set U ⊂ Rm and a point x0 ∈ Rn. It is assumed that f (., u) and L(., u) are C1 for 
all u ∈ U, and f, fx ,L,Lx are continuous functions. Let (x,¯ u)¯ be a minimizing 
process. 
Use the finite horizon maximum principle to show that there exist λ ≥ 0 and a 
locally absolutely continuous function p : [0,∞) → Rn, not both zero, such that 
− ˙p(t) = p(t) · fx (x(t), ¯ u(t)) ¯ − λLx (x(t), ¯ u(t))dt, ¯ a.e.t ∈ [0,∞)
p(t)·f (x(t), ¯ u(t)) ¯ −λL(x(t), ¯ u(t))dt ¯ = max 
u∈U

p(t)·f (x(t), ¯ u)−λL(x(t), ¯ u)dt
, 
a.e. t ∈ [0,∞). 
Hint: Take Ti → ∞. For each i, consider the corresponding finite horizon problem 
with underlying time interval [0, Ti] with cost  Ti
0 L(x(t), u(t))dt and with right 
endpoint constraint x(Ti) = ¯x(Ti). Show the restriction of (u,¯ x)¯ to [0, Ti] is 
a minimizer for this problem. Apply the (finite horizon) maximum principle, for 
each i. Prove the infinite horizon necessary conditions by scaling the multipliers, 
subsequence extraction and passage to the limit as Ti → ∞. 
1.6 Since problem (I) of question 5 has no terminal cost and terminal constraints, 
we might expect that the necessary conditions include a transversality condition of 
the type 
lim
T →∞ p(T ) = 0 .
This conjecture was shown to be false by Halkin, who provided the following 
counter example. Take the data for problem (I) to be n = m = 1, f (x, u) = (1−x)u, 
L(x, u) = −(1−x)u, U = [0, 1] and x0 = 0. Show that (x¯ ≡ (1−exp(−t), u¯ ≡ 1)
is a minimizer. Determine Lagrange multipliers (λ, p) for this problem. Show that 
the necessary conditions cannot be satisfied, when they are supplemented by the 
above boundary condition on the costate arc. 
1.13 Notes for Chapter 1 
This chapter maps out major themes of the book and links them to earlier 
developments in mathematics. It is not intended as a comprehensive historical 
review of dynamic optimization. 
For a more detailed exposition of the material in Sect. 1.2 on optimality condi￾tions in the calculus of variations we recommend Clarke [68] and Troutman [191]. 
L. C. Young’s inspirational, if individualistic, book [209] is notable for tracking 
the evolution of the classical theory into modern day dynamic optimization. The 
brachistochrone problem of Sect. 1.2 is analysed in many publications, including 
[191] and [209].90 1 Overview
From numerous older books on classical dynamic optimization, we single out 
for clarity and accessibility [21, 104, 132] and [210]. Warga [204] and Cesari [54] 
provide more detailed coverage, centred on their own contributions to the theory 
of existence of minimizers and necessary conditions. Bryson and Ho’s widely read 
book [49] (and also [48]), which follow a traditional applied mathematics approach 
to dynamic optimization, includes a wealth of engineering-inspired examples. Betts’ 
monograph [38] also provides many examples, while focusing on computational 
techniques for their solution. Shättler and Ledzewicz’s book [137] emphasizes 
geometric aspects of dynamic optimization . 
The two books [68] and [85] by Clarke et al. are source references on nonsmooth 
analysis, covering different approaches in both in finite and infinite dimensional 
vector spaces, including the one based on the proximal methods that feature 
prominently in this publication. Other authoritative books including material on 
nonsmooth analysis are those of Rockafellar and Wets [177] and Aubin and 
Frankowska [14]. As regards nonsmooth dynamic optimization, Clarke’s book [65], 
which had a major role in winning an audience for the field, remains a standard 
reference. Clarke’s more recent book [68] includes later developments in the field. 
See also [85]. Expository accounts of this material (and extensions) are to be found 
in [64] and [141]. 
Two proofs of the classical maximum principle are given. The proof based on 
older set separation methods is standard. (See, for example, [21] or [132]). The proof 
based on Clarke’s perturbation techniques is essentially that in [65], but exploits the 
simplifications that are possible when the data is smooth. See also [98]. 
The maximum orbital transfer problem discussed in Sect. 1.1 is a refinement of a 
problem first studied by Kopp and McGill, and reported in [49], to allow for thrusts 
of variable magnitude. See also [48].A second order, feasible directions algorithm 
due to R. Pytlak, documented in [168, 169] was used in the computations summa￾rized in the diagrams of Sect. 1.1. The solution to the growth versus consumption 
problem from economics is taken from [152], which contains references to earlier 
dynamic optimization based on the Ramsay growth model. Other applications of 
dynamic optimization theory to economics are to be found in, for example, [160]. 
The application of dynamic optimization to chemotherapy has been an active field 
since the late 1980’s. A notable early contribution was [161]. Ledzewicz and 
Schättler have been prominent among researchers in this field [138]. The example 
considered in this chapter is based on [188], following removal of a time delay from 
the dynamic model. The numerical results and graphs are taken from this paper. 
Many formulations of the dynamic optimization problem have been proposed, 
involving different cost functions, constraints and underlying dynamics. Methods 
of dynamic optimization have been used also to design integrated treatments for 
tumour angiogenesis and chemotherapy [139]. 
Examples of applications of the theory of dynamic optimization to other areas 
referred to in this chapter are to be found in [55] (resource economics) and [180] 
(epidemiology).Chapter 2 
Set Convergence, Measurability 
and Existence of Minimizers 
Abstract This chapter covers basic concepts and definitions concerning set con￾vergence, measurability and multifunctions. Multifunctions are mappings whose 
values are not single points in the range space but subsets of such points. They are 
widely encountered in dynamic optimization; for example, it is helpful to interpret 
a time dependent control constraint set as a multifunction and admissible control 
functions as selectors of that multifunction. The chapter provides proofs of key 
properties, with special emphasis on properties relevant to dynamic optimization. 
A prior study of measurability is an important step in establishing existence of 
solutions to dynamic optimization problems. The chapter concludes with a theorem 
concerning existence of minimizers for the generalized problem of Bolza. The 
underlying optimization problem resembles the problem of Bolza from the classical 
calculus of variations, involving integral and endpoint costs. But it is in fact a far 
broader problem formulation, because we depart from the classical framework by 
allowing the cost integrand and endpoint constraint functions to take values in the 
extended real line. This formulation subsumes a wide range of problems involving 
dynamic constraints (in the form of controlled differential equations or differential 
inclusions), which can be accommodated by the use of extended valued indicator 
functions added to the cost integrand. 
2.1 Introduction 
Early developments in variational analysis, which predated the measurability 
concepts introduced by Lebesgue, sought to provide information, such the Euler 
condition, about an arc that minimizes a given integral functional of the arc 
and its derivative. Here, ‘arc’ was understood to be an element in the class of 
continuous functions with piecewise continuous derivatives. When however, in the 
early twentieth century, the field of study broadened also to address the questions 
of existence, ‘when does a minimizing arc exist?’, the need arose to consider larger 
classes of arcs. In modern day dynamic optimization, it is customary to take the 
underlying family of arcs to be the space of absolutely continuous functions (or 
some subset of this space that takes account of endpoint and pathwise constraints) 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
P. Bettiol, R. B. Vinter, Principles of Dynamic Optimization, Springer Monographs 
in Mathematics, https://doi.org/10.1007/978-3-031-50089-3_2
9192 2 Set Convergence, Measurability and Existence of Minimizers
since, in this framework, it is possible to guarantee existence of minimizers under 
unrestrictive, directly verifiable hypotheses. So we must now consider minimizers 
that belong to the class of indefinite integrals of measurable/summable functions. 
The formulation of dynamic constraints as either controlled differential equations 
or differential inclusions requires us to consider, not just measurable functions 
but also measurable multifunctions and the interpretation of integrals of set-valued 
functions. 
This chapter supplies basic definitions and concepts from the theory of mea￾surable multifunctions, extensive use of which is made throughout this book. It 
also provides derivations of multifunction properties having special significance 
in dynamic optimization. Earlier we referred to the link between existence and 
measurability. Appropriately then, the chapter concludes with a theorem giving 
conditions guaranteeing existence of minimizers (in the class of absolutely contin￾uous functions) for the generalized problem of Bolza. The underlying optimization 
problem resembles the problem of Bolza from the classical calculus of variations, 
that is the variational problem with both integral and endpoint costs. But it is in fact 
a far broader problem formulation, because, in contrast to classical studies, we allow 
the cost integrand and endpoint constraint functions to take values in the extended 
real line (−∞,∞) ∪ {+∞}. (Hence our use of the qualifier ‘generalized’.) This 
formulation subsumes a wide variety of problems involving dynamic constraints (in 
the form of controlled differential equations or differential inclusions), which can 
be accommodated by the use of extended valued penalty functions added to the cost 
integrand. 
2.2 Convergence of Sets and Continuity of Multifunctions 
Take a sequence of sets {Ai} in Rn. There are number of ways of defining limit sets. 
For our purposes, ‘Kuratowski sense’ limit operations will be the most useful. The 
set 
lim inf
i→∞ Ai
(the Kuratowski lim inf) comprises all points x ∈ Rn satisfying the condition: there 
exists a sequence xi → x such that xi ∈ Ai for all i. 
The set 
lim sup
i→∞
Ai
(the Kuratowski lim sup) comprises all points x ∈ Rn satisfying the condition: there 
exist a subsequence {Aij } of {Ai} and a sequence xj → x such that xj ∈ Aij for 
all j .2.2 Convergence of Sets and Continuity of Multifunctions 93
lim infi→∞ Ai and lim supi→∞ Ai are (possibly empty) closed sets, related 
according to 
lim inf
i→∞ Ai ⊂ lim sup
i→∞
Ai.
In the event lim infi→∞ Ai and lim supi→∞ Ai coincide, we say that {Ai} has a limit 
(in the Kuratowski sense) and write 
lim
i→∞ Ai := lim inf
i→∞ Ai ( = lim sup
i→∞
Ai).
The sets lim infi→∞ Ai and lim supi→∞ Ai are succinctly expressed in terms of 
the distance function 
dA(x) = inf
y∈A |x − y|,
thus 
lim inf
i→∞ Ai = {x : lim sup
i→∞
dAi(x) = 0},
and 
lim sup
i→∞
Ai = {x : lim inf
i→∞ dAi(x) = 0}.
These concepts can be generalized to define limit points of multifunctions. Take 
a set D. A multifunction � : D ⇝ Rn is a mapping from D into the space of subsets 
of Rn. For each x ∈ D then, �(x) is a subset of Rn. In this way we obtain a family 
of sets {�(x) ⊂ Rn : x ∈ D}, parameterized by points x ∈ D. We shall often refer 
to a multifunction as convex, closed or non-empty depending on whether �(x) has 
the referred-to property for all x ∈ D. The graph of � is the set 
Gr � := {(x, ξ ) ∈ D × Rn : ξ ∈ �(x)}.
Consider now the case in which D ⊂ Rk. Fix a point x ∈ Rk. The set 
lim inf
y D
→x
�(y)
(the Kuratowski lim inf) comprises all points ξ satisfying the condition: correspond￾ing to any sequence yi
D
→ x, there exists a sequence ξi → ξ such that ξi ∈ �(yi)
for all i. The set 
lim sup
y D
→x
�(y)94 2 Set Convergence, Measurability and Existence of Minimizers
(the Kuratowski lim sup) comprises all points ξ satisfying the condition: there exist 
sequences yi
D
→ x and ξi → ξ such that ξi ∈ �(yi) for all i. (In the above, ‘yi
D
→ x’ 
means ‘yi → x and yi ∈ D for all i’.) 
If D is a neighbourhood of x we write lim infy→x �(y) in place of 
lim inf
y D
→x �(y), etc. 
Here, too, we have convenient characterizations of the limit sets in terms of the 
distance function on Rn: 
lim inf
y D
→x
�(y) = {ξ ∈ Rn : lim sup
y D
→x
d�(y)(ξ ) = 0}
and 
lim sup
y D
→x
�(y) = {ξ ∈ Rn : lim inf
y D
→x
d�(y)(ξ ) = 0}.
We observe that lim inf
y D
→x �(y) and lim sup
y D
→x �(y) are closed (possibly 
empty) sets, related according to 
lim inf
y D
→x
�(y) ⊂ lim sup
y D
→x
�(y).
When the lim inf
y D
→x �(y) and lim sup
y D
→x �(y) are equal, we say that the limit of 
� at x along D exists and we set 
lim
y D
→x
�(y) := lim inf
y D
→x
�(y) ( = lim sup
y D
→x
�(y)).
To reconcile these definitions, we note that, give a sequence of sets {Ai} in Rn, 
lim sup Ai = lim sup
y D
→x
�(y) etc.,
when we identify D with the subset {1, 1/2, 1/3,...} of the real line, choose x = 0
and define �(y) = Ai when y = i−1, i = 1, 2,... . 
Closely related concepts concern the (semi-)continuity of multifunctions. � :
D ⇝ Rn is said to be upper semi-continuous at x ∈ D if for any ϵ > 0 there exists 
r > 0 such that 
�(y) ⊂ �(x) + ϵB, for all y ∈ (x + rB) ∩ D,
and � is called upper semi-continuous if it is upper semi-continuous at x for all x ∈
D. � : D ⇝ Rn is said to be lower semi-continuous at x ∈ D if for any ξ ∈ �(x)2.3 Measurable Multifunctions 95
and any sequence xi
D
→ x, there exists a sequence ξi ∈ �(xi) such that ξi → ξ . �
is called lower semi-continuous if it is lower semi-continuous at x for all x ∈ D. 
Clearly � is lower semi-continuous at x ∈ D if and only if �(x) ⊂ lim inf
y D
→x �(y), 
Observe also that, if � is upper semi-continuous at x ∈ D, then 
lim sup
y D
→x
�(y) ⊂ �(x).
The multifunction � : D ⇝ Rn is continuous at x ∈ D if it both lower semi￾continuous and upper semi-continuous at x, and � is continuous if it is continuous 
at each x ∈ D. 
2.3 Measurable Multifunctions 
Recall that a measurable space (Ω, F) comprises a set Ω and a family F of subsets 
of Ω which is a σ-field, i.e. 
(i) ∅ ∈ F, 
(ii) F ∈ F implies that Ω\F ∈ F, 
(iii) F1, F2, ···∈ F implies ∪∞
i=1Fi ∈ F. 
Definition 2.3.1 Let (Ω, F) be a measurable space. Take a multifunction � : Ω ⇝
Rn. � is measurable when the set 
�−1(W ) = {x ∈ Ω : �(x) ∩ W /= ∅}
is F measurable for every open set W ⊂ Rn. 
Fix a Lebesgue subset I ⊂ R. Let L denote the Lebesgue subsets of I . If Ω is 
the set I then ‘� : I ⇝ Rn is measurable’ is taken to mean that the multifunction is 
L measurable. 
Denote by Bk the Borel subsets of Rk. The product σ-algebra L × Bk (that 
is the smallest σ-algebra of subsets of I × Rk that contains all product sets 
A × B with A ∈ L and B ∈ Bk) is often encountered in hypotheses invoked 
to guarantee measurability of multifunctions, validity of certain representations for 
multifunctions, etc. 
A first taste of such results is provided by: 
Proposition 2.3.2 Take an L × Bm measurable multifunction F : I × Rm ⇝ Rk
and a Lebesgue measurable function u : I → Rm. Then G : I ⇝ Rk defined by 
G(t) := F (t, u(t))
is an L measurable multifunction.96 2 Set Convergence, Measurability and Existence of Minimizers
Proof For an arbitrary choice of set A ∈ L and B ∈ Bm, the set 
{t ∈ I : (t, u(t)) ∈ A × B}
is a Lebesgue subset because it is expressible as A ∩ u−1(B) and u is Lebesgue 
measurable. Denote by D the family of subsets E ⊂ I × Rm for which the set 
{t ∈ I : (t, u(t)) ∈ E}
is Lebesgue measurable. D is a σ-field, as is easily checked. We have shown that it 
contains all product sets A × B with A ∈ L, B ∈ Bm. So D contains the σ-field 
L × Bm. 
Take any open set W ⊂ Rk. Then, since F is L × Bm measurable, E := {(t, u) :
F (t, u) ∩ W /= ∅} is L × Bm measurable. But then 
{t ∈ I : G(t, u(t)) ∩ W /= ∅} = {t ∈ I : (t, u(t)) ∈ E}
is a Lebesgue measurable set since E ∈ D. Bearing in mind that W is an arbitrary 
open set, we conclude that t ⇝ G(t) := F (t, u(t)) is a Lebesgue measurable 
multifunction. ⨅⨆
Specializing to the point valued case we obtain: 
Corollary 2.3.3 Consider a function g : I ×Rm → Rk and a Lebesgue measurable 
function u : I → Rm. Suppose that g is L × Bm measurable. Then the mapping 
t → g(t, u(t)) is Lebesgue measurable. 
Functions g(t, u) arising in dynamic optimization to which Corollary 2.3.3 
are often applied are composite functions of a nature covered by the following 
proposition. 
Proposition 2.3.4 Consider a function φ : I × Rn × Rm → Rk satisfying the 
following hypotheses: 
(a) φ(t, ·, u) is continuous for each (t, u) ∈ I × Rm, 
(b) φ(·, x, ·) is L × Bm measurable for each x ∈ Rn. 
Then for any Lebesgue measurable function x : I → Rn, the mapping (t, u) →
φ(t, x(t), u) is L × Bm measurable. 
Proof Let {rj } be an ordering of the set of n-vectors with rational coefficients. For 
each integer k define 
φk(t, u) := φ(t, rj , u)
where j is chosen (j will depend on k and t) such that 
|x(t) − rj | ≤ 1/k and |x(t) − ri| > 1/k for all i ∈ {1, 2.., j − 1}.
(These conditions uniquely define j .)2.3 Measurable Multifunctions 97
Since φ(t, ·, u) is continuous, 
φk(t, u) → φ(t, x(t), u) as k → ∞
for every (t, u) ∈ I × Rm. It suffices then to show that φk is L × Bm measurable for 
an arbitrary choice of k. 
For any open set V ⊂ Rk, 
φ−1
k (V ) = {(t, u) ∈ I × Rm : φk(t, u) ∈ V }
= ∪∞
j=1

{(t, u) ∈ I × Rm : φ(t, rj , u) ∈ V }
∩ {(t, u) ∈ I × Rm : |x(t) − rj | ≤ 1/k
and |x(t) − ri| > 1/k for i = 1, .., j − 1}).
Since the set on the right side is a countable union of L × Bm measurable sets, we 
have established that φk is a L × Bm measurable function. ⨅⨆
The L × Bm measurability hypothesis of Proposition 2.3.4 is unrestrictive. It is 
satisfied, for example, by the Carathéodory functions: 
Definition 2.3.5 A function g : I ×Rm → Rk is said to be a Carathéodory function 
if 
(a) g(·, u) is Lebesgue measurable for each u ∈ Rm, 
(b) g(t, ·) is continuous for each t ∈ I . 
Proposition 2.3.6 Consider a function g : I × Rm → Rk. Assume that g is a 
Carathéodory function. Then g is L × Bm measurable. 
Proof Let {r1, r2...} be an ordering of the set of m-vectors with rational compo￾nents. For every positive integer k, t ∈ I and u ∈ Rm, define 
gk(t, u) := g(t, rj ),
in which the integer j is uniquely defined by the relations: 
|rj − u| ≤ 1/k and |ri − u| > 1/k for i = 1, .., j − 1.
Since g(t, ·) is assumed to be continuous, we have 
gk(t, u) → g(t, u)
as k → ∞, for each fixed (t, u) ∈ I ×Rm. It suffices then to show that gk is L×Bm
measurable for each k. However this follows from the fact that, for any open set 
V ⊂ Rk, we have98 2 Set Convergence, Measurability and Existence of Minimizers
g−1
k (V ) = {(t, u) ∈ I × Rm : gk(t, u) ∈ V }
= ∪∞
j=1{(t, u) ∈ I × Rm : g(t, rj ) ∈ V , |u − rj | ≤ 1/k
and |u − ri| > 1/k, i = 1, .., j − 1}
= ∪∞
j=1({t ∈ I : g(t, rj ) ∈ V }×{u ∈ Rm : |u − rj | ≤ 1/k,
and |u − ri| > 1/k for i = 1, 2, .., j − 1})
and this last set is L × Bm measurable, since it is expressible as a countable union 
of sets of the form A × B with A ∈ L and B ∈ Bm. ⨅⨆
The preceding propositions combine incidentally to provide an answer to the 
following question concerning an appropriate framework for the formulation of 
variational problems: under what hypotheses on the function L : I ×Rn ×Rn → R
is the integrand of the Lagrange functional 

L(t, x(t), x(t))dt ˙
Lebesgue measurable for an arbitrary absolutely continuous arc x ∈ W1,1? The two 
preceding propositions guarantee that the integrand is Lebesgue measurable if 
(i) L(·, x, ·) is L × Bn measurable for each x ∈ Rn
and 
(ii) L(t, ·, u) is continuous for each (t, u) ∈ I × Rn. 
This is because Proposition 2.3.4 tells us that (t, u) → L(t, x(t), u) is L × Bn
measurable, in view of assumptions (i) and (ii), and since x : I → Rn is Lebesgue 
measurable. Corollary 2.3.3 permits to conclude, since t → ˙x(t) is Lebesgue 
measurable, that t → L(t, x(t), x(t)) ˙ is indeed Lebesgue measurable. 
The following theorem, a proof of which is to be found in [53], lists important 
characterizations of closed multifunctions which are measurable. (Throughout, I is 
a Lebesgue subset of R.) 
Theorem 2.3.7 Take a multifunction � : I ⇝ Rn and define D := {t ∈ I : �(t) /=
∅}. Assume that � is closed. Then the following statements are equivalent: 
(a) � is an L measurable multifunction, 
(b) Gr � is an L × Bn measurable set, 
(c) D is a Lebesgue subset of I and there exists a sequence {γk : D → Rn} of 
Lebesgue measurable functions such that 
�(t) = ∪∞
k=1{γk(t)} for all t ∈ D. (2.3.1)2.3 Measurable Multifunctions 99
The representation of a multifunction in terms of a countable family of Lebesgue 
measurable functions according to (2.3.1) is called the Castaing Representation 
of �. 
Our aim now is to establish the measurability of a number of frequently 
encountered multifunctions derived from other multifunctions. 
Proposition 2.3.8 Take a measurable space (Ω, F) and a measurable multifunc￾tion � : Ω ⇝ Rn. Then the multifunction �˜ : Ω ⇝ Rn is also measurable in each 
of the following cases: 
(i) �(y) ˜ := �(y) for all y ∈ Ω,
(ii) �(y) ˜ := co �(y) for all y ∈ Ω.
Proof 
(i): �˜ is measurable in this case since, for any open set W ∈ Rn, 
{y ∈ Ω : �(y) ∩ W /= ∅} = {y ∈ Ω : �(y) ∩ W /= ∅}.
(ii): Define the multifunction �(n+1) : Ω ⇝ Rn×(n+1) to be 
�(n+1)
(y) := �(y) ×···× �(y) for all y ∈ Ω.
Then �(n+1) is measurable, since {y ∈ Ω : �(n+1)
(y) ∩ W = ∅} / is obviously 
measurable for any set W in Rn×(n+1) which is a product of open sets of Rn, 
and therefore for any open set W, since an arbitrary open set in the product 
space can be expressed as a countable union of such sets. 
Define also 
Λ := 
(λ0, .., λn) : λi ≥ 0 for all i, n
i=0
λi = 1

.
Take any open set W in Rn and define 
W(n+1) := 
(w0,...wn) :

i
λiwi ∈ W, (λ0,...,λn) ∈ Λ

.
Obviously, W(n+1) is an open set. 
We must show that the set 
{y ∈ Ω : co �(y) ∩ W /= ∅}
is measurable. But this follows immediately from the facts that �˜ (n+1) is measurable 
and W(n+1) is open, since100 2 Set Convergence, Measurability and Existence of Minimizers
{y : co �(y) ∩ W /= ∅} = {y : �(n+1)
(y) ∩ W(n+1) /= ∅}.
⨅⨆
The next proposition concerns the measurability properties of limits of sequences 
of multifunctions. We make reference to Kuratowski sense limit operations, defined 
in Sect. 2.2. 
Theorem 2.3.9 Consider closed multifunctions �j : I ⇝ Rn, j = 1, 2,...
Assume that �j is L measurable for each j . Then the closed multifunction � :
I ⇝ Rn is also L measurable when � is defined in each of the following ways 
(a) �(t) := ∪j≥1�j (t), 
(b) �(t) := ∩j≥1�j (t), 
(c) �(t) := lim supj→∞ �j (t), 
(d) �(t) := lim infj→∞ �j (t). 
(c) and (d) imply in particular that if t → {�j (t)} has a limit as j → ∞ for almost 
every t , then t → limj→∞ �j (t) is measurable. 
Proof 
(a) ( �(t) = ∪j≥1�j (t) ) 
Take any open set W ⊂ Rn. Then, since W is an open set, we have 
{t ∈ I : �(t) ∩ W /= ∅} = {t ∈ I : ∪j≥1�j (t) ∩ W /= ∅}
= {t ∈ I : ∪j≥1�j (t) ∩ W /= ∅}
= ∪j≥1{t ∈ I : �j (t) ∩ W /= ∅}.
This establishes the measurability of t ⇝ �(t) since the set on the right side, a 
countable union of Lebesgue measurable sets, is Lebesgue measurable. 
(b) ( �(t) = ∩j≥1�j (t) ) 
In this case, 
Gr � = Gr {t ⇝ ∩j≥1�j (t)}=∩j≥1 Gr �j .
Gr � then is L × Bn measurable since each Gr �j is L × Bn measurable by 
Theorem 2.3.7. Now apply again Theorem 2.3.7. 
(c) ( �(t) = lim supj→∞ �j (t). ) 
The measurability of t ⇝ �(t) in this case follows from (a) and (b) and the 
following characterization of lim supj→∞ �j (t): 
lim sup
j→∞
�j (t) = ∩J≥1∪j≥J�j (t).
(d) ( �(t) = lim infj→∞ �j (t) )2.3 Measurable Multifunctions 101
Define 
�k
j (t) := �j (t) + (1/k)B.
Notice that �k
j is measurable since, for any closed set W ⊂ Rn, 
{t : �k
j (t) ∩ W = ∅} = { / t : �j (t) ∩ (W + (1/k)B) /= ∅}
and the set W + (1/k)B is closed. 
The measurability of � in this case too follows from (a) and (b) in view of the 
identity 
lim inf
j→∞ �j (t) = ∩k≥1∪J≥1 ∩j≥J �k
j (t).
⨅⨆
Proposition 2.3.10 Take a multifunction F : I × Rn ⇝ Rk and a Lebesgue 
measurable function x¯ : I → Rn. Assume that 
(a) for each x, F (·, x) : I ⇝ Rk is a L measurable, non-empty, closed 
multifunction, 
(b) for each t, F (t, ·) is continuous at x = ¯x(t), in the sense that 
yi → ¯x(t) implies F (t, x(t)) ¯ = lim
i→∞ F (t, yi).
Then G : I ⇝ Rk defined by 
G(t) := F (t, x(t)) ¯
is a closed L measurable multifunction. 
Proof Let {ri} be an ordering of n-vectors with rational entries. For each integer l
and for each t ∈ I , define 
Fl(t) := F (t, rj ),
in which j is chosen according to the rule 
| ¯x(t) − rj | ≤ 1/l and | ¯x(t) − ri| > 1/l for i = 1, .., j − 1.
In view of the continuity properties of F (t, ·), 
F (t, x(t)) ¯ = lim
l→∞ Fl(t).
By Theorem 2.3.9 then it suffices to show that Fl is measurable for arbitrary l.102 2 Set Convergence, Measurability and Existence of Minimizers
We observe however that 
Gr Fl = ∪j (Gr F (·, rj ) ∩ (Aj × Rk))
where 
Aj := {t :|¯x(t) − rj | ≤ 1/l, | ¯x(t) − ri| > 1/l for i = 1, .., j − 1}.
Since F (·, rj ) has L × Bk measurable graph (see Theorem 2.3.7) and x¯
is Lebesgue measurable, we see that Gr Fl is L × Bk measurable. Applying 
Theorem 2.3.7 again, we see that the closed multifunction Fl is measurable. ⨅⨆
The derivation of necessary conditions of optimality for dynamic optimization 
problems with non-smooth data will require us to establish measurability properties 
of multifunctions involving the limiting subdifferential ∂g(x)¯ of a lower semi￾continuous function g : Rk → R ∪ {+∞}, at a point x¯ ∈ dom g. The limiting 
subdifferential and its properties feature prominently in Chaps. 4 and 5. Here, we 
merely anticipate its definition. The definition is in two stages: first we define the 
proximal subdifferential ∂P
g (x) at a point x ∈ dom g: 
∂P g(x) := {ξ : there exists M such that
ξ · (x' − x) ≤ g(x'
) − g(x) + M|x' − x|
2 for all x' ∈ Rk}.
The limiting subdifferential is then obtained by ‘closing the graph’ of the multifunc￾tion x → ∂P g(x), thus 
∂g(x)¯ := lim sup
ϵ↓0
{ξ : ξ = ∂P g(x) for some x ∈ Rk such that
|(x, g(x)) − (x, g( ¯ x)) ¯ | ≤ ϵ}.
The following proposition concerns the measurability properties of the multifunc￾tion t → ∂xf (t, x(t)) ¯ , for given functions (t, x) → f (t, x) and t → ¯x(t), where 
∂x denotes the (partial) limiting subdifferential w.r.t. the x variable. 
Proposition 2.3.11 Take a function f : I × Rn → R such that 
(a) f (., x) : I → R is a L measurable for each x ∈ Rn, 
(b) f (t, .) : Rn → R is locally Lipschitz continuous for each t ∈ I . 
Take a Lebesgue measurable function x¯ : I → Rn. Consider the multifunction 
G : I ⇝ Rn defined by 
G(t) := ∂xf (t, x(t)). ¯
Then G and co G are closed L measurable multifunctions.2.3 Measurable Multifunctions 103
Proof Since the limiting subdifferential is closed it immediately follows that G and 
co G take closed values. We also observe that, owing to Proposition 2.3.8, it suffices 
to prove that G is a closed L measurable multifunction. For each integer i ≥ 1 we 
consider the function φi : I × Rn × Rn → R defined by 
φi(t, x, ξ ) := min
y∈i−1B
f (t, x + y) − f (t, x) − ξ · y + i|y|
2.
Observe that, since φi(t, x, ξ ) = infy∈(i−1B)∩Qn {f (t, x+y)−f (t, x)−ξ ·y+i|y|
2}, 
it is easy to see that, for each (x, ξ ) ∈ Rn × Rn, φi(., x, ξ ) is Lebesgue measurable. 
Moreover, since f (t, .) is locally Lipschitz continuous for each t ∈ I , we can easily 
deduce that φi(t, ., .) is locally Lipschitz. We claim that ξ ∈ ∂x,P f (t, x) if and only 
if φi(t, x, ξ ) = 0. Indeed, assume that ξ ∈ ∂x,P f (t, x), then, by the definition of 
(partial) proximal subdifferential we can find M > 0 and ϵ > 0 such that 
ξ · (x' − x) ≤ f (t, x'
) − f (t, x) + M|x' − x|
2, for all x' ∈ x + ϵB. (2.3.2) 
Then taking an integer i0 such that i0 ≥ max{1/ϵ, M} and setting x' = y − x we 
deduce that 
0 ≤ f (t, x + y) − f (t, x) − ξ · y + i0|y|
2, for all y ∈ i
−1
0 B, (2.3.3) 
and so φi(t, x, ξ ) = 0. Suppose now that φi0 (t, x, ξ ) = 0 for some i0 ≥ 1, 
which means that (2.3.3) is satisfied. Then, from the characterization of proximal 
subgradients, taking ϵ = i
−1
0 and M = i0, we obtain that ξ ∈ ∂x,P f (t, x). 
Take a closed set W ⊂ Rn. The proof will be complete if we show that the 
following set E is measurable: 
E := {t ∈ I | ∂xf (t, x(t)) ¯ ∩ W /= ∅}.
We observe that 
E = 
j≥1

i≥1
Eij
where, for each i ≥ 1 and j ≥ 1, Eij := {t ∈ I | Fij (t) /= ∅}, i.e. the (effective) 
domain of the multivalued function Fij : I ⇝ Rn × Rn defined as 
Fij (t) := {(x, ξ ) ∈ (x(t) ¯ + j−1B) × (W + j−1B) | φi(t, x, ξ ) = 0}.
Invoking Theorem 2.3.7 we can easily deduce that the multifunctions Fij ’s are 
measurable and therefore E is measurable. ⨅⨆
Take a multifunction � : I ⇝ Rk. We say that a function x : I → Rk is a 
measurable selection for � if104 2 Set Convergence, Measurability and Existence of Minimizers
(i) x is Lebesgue measurable, 
and 
(ii) x(t) ∈ �(t) a.e.. 
We obtain directly from Theorem 2.3.7 the following conditions for � to have a 
measurable selection: 
Theorem 2.3.12 Let � : I ⇝ Rk be a non-empty multifunction. Assume that � is 
closed and measurable. Then � has a measurable selection. 
(In fact Theorem 2.3.7 tells us rather more than this: not only does there exist a 
measurable selection under the stated hypotheses, but the measurable selections are 
sufficiently numerous to ‘fill out’ the values of the multifunction.) 
The above measurable selection theorem is inadequate for certain applications in 
which the multifunction is not closed. An important extension (see [203]) is 
Theorem 2.3.13 (Aumann’s Measurable Selection Theorem) Let � : I ⇝ Rk
be a non-empty multifunction. Assume that 
Gr � is L × Bk measurable.
Then � has a measurable selection. 
This can be regarded as a generalization of Theorem 2.3.12 since if � is closed 
and measurable then, by Theorem 2.3.7, Gr � is automatically L × Bk measurable. 
Of particularly significance in applications to dynamic optimization is the 
following measurable selection theorem involving the composition of a function 
and a multifunction. 
Theorem 2.3.14 (The Generalized Filippov Selection Theorem) Consider a 
non-empty multifunction U : I ⇝ Rm and a function g : I ×Rm → Rn satisfying 
(a) The set Gr U is L × Bm measurable, 
(b) The function g is L × Bm measurable. 
Then for any measurable function v : I → Rn, the multifunction U' : I ⇝ Rm
defined by 
U'
(t) := {u ∈ U (t) : g(t, u) = v(t)}
has L × Bm measurable graph. Furthermore, if 
v(t) ∈ {g(t, u) : u ∈ U (t)} a.e. (2.3.4) 
then there exists a measurable function u : I → Rm satisfying 
u(t) ∈ U (t) a.e., (2.3.5) 
g(t, u(t)) = v(t) a.e.. (2.3.6)2.3 Measurable Multifunctions 105
Notice that condition (2.3.4) is just a rephrasing of the hypothesis 
‘U'
(t) is non-empty for a.e. t ∈ I ’
and the final assertion can be expressed in measurable selection terms as 
‘The multifunction U' has a measurable selection’,
a fact which follows directly from Aumann’s selection theorem. 
We mention that the name Filippov’s selection theorem usually attaches to the 
final assertion of the theorem concerning existence of a measurable function u :
I → Rm satisfying (2.3.5) and (2.3.6) under (2.3.4) and strengthened forms of 
hypotheses (a) and (b), namely 
(a)' U is a measurable, closed multifunction, 
(b)' g is a Carathéodory function. 
Proof By redefining U'
(t) on a null-set if required, we can arrange that U'
(t) is 
non-empty for all t ∈ I . In view of the preceding discussion it is required to show 
merely that Gr U' is L × Bm measurable. But this follows directly from the relation 
Gr U' = φ−1({0}) ∩ Gr U
in which φ(t, u) := g(t, u) − v(t), since under the hypotheses both φ−1({0}) and 
Gr U are L × Bm measurable sets. ⨅⨆
The relevance of Filippov’s theorem in a control systems context is illustrated 
by the following application. Take a function f : [S,T ] × Rn × Rm → Rn and 
a multifunction U : [S,T ] ⇝ Rm. The class of state trajectories for the control 
system 
x(t) ˙ = f (t, x(t), u(t)) a.e. t ∈ [S,T ]
u(t) ∈ U (t) a.e. t ∈ [S,T ]
comprises absolutely continuous functions x : [S,T ] → Rn such that the above 
relations are satisfied for some measurable u : [S,T ] → Rm. We often need to view 
them as solutions of the differential inclusion x(t) ˙ ∈ F (t, x(t)) with 
F (t, x) := {f (t, x, u) : u ∈ U (t)}.
The question then arises whether the state trajectories for the control system are 
precisely the absolutely continuous functions x satisfying 
x(t) ˙ ∈ F (t, x(t)) a.e.. (2.3.7)106 2 Set Convergence, Measurability and Existence of Minimizers
Clearly a necessary condition for an absolutely continuous function to be a state 
trajectory for the control system is that (2.3.7) is satisfied. Filippov’s theorem tells 
us that (2.3.7) is also a sufficient condition (i.e. the differential inclusion provides 
an equivalent description of state trajectories) under the hypotheses: 
(i) f (·, x, ·) is L × Bm measurable and f (t, ·, u) is continuous 
(ii) Gr U is L × Bm measurable. 
To see this, apply the generalized Filippov selection theorem with g(t, u) =
f (t, x(t), u) and v = ˙x. (The relevant hypotheses, (b) and (2.3.4), are satisfied in 
view of Proposition 2.3.4 and since x(t) ˙ ∈ F (t, x(t)) a.e..) This yields a measurable 
function u : [S,T ] → Rm satisfying x(t) ˙ = f (t, x(t), u(t)) and u(t) ∈ U (t) a.e. 
thereby confirming that x is a state trajectory for the control system. 
Having once again an eye for future dynamic optimization applications, we now 
establish measurability of various derived functions and the existence of measurable 
selections for related multifunctions. The source of a number of useful results is the 
following theorem concerning the measurability of a ‘marginal’ function. 
Theorem 2.3.15 Consider a function g : I × Rk → R and a closed, non-empty 
multifunction � : I ⇝ Rk. Assume that 
(a) g is a Carathéodory function, 
(b) � is a measurable multifunction. 
Define the extended valued function η : I → R ∪ {−∞}
η(t) = inf
γ ∈�(t) g(t, γ ) for t ∈ I.
Then η is a Lebesgue measurable function. Furthermore, if we define 
I ' := {t ∈ I : inf
γ '
∈�(t) g(t, γ '
) = g(t, γ ) for some γ ∈ �(t)}
(i.e. I ' is the set of points t for which the infimum of g(t, ·) is achieved over �(t)) 
then I ' is a Lebesgue measurable set and there exists a measurable function γ :
I ' → Rk such that 
η(t) = g(t, γ (t)) a.e t ∈ I '
. (2.3.8) 
Proof Since � is closed, non-empty and measurable, it has a Castaing representa￾tion in terms of some countable family of measurable functions {γi : I → Rk}. 
Since g(t, ·) is continuous and {γi(t)} is dense in �(t)
η(t) = inf{g(t, γi(t)) : i an integer}
for all t ∈ I .2.4 The Generalized Bolza Problem 107
Now according to Proposition 2.3.6 and Corollary 2.3.3, t → g(t, γi(t)) is a 
measurable function. It follows from a well-known property of measurable functions 
that η, which we have expressed as the pointwise infimum of a countable family of 
measurable functions, is Lebesgue measurable and dom{η} := {t ∈ I : η(t) >
−∞} is a Lebesgue measurable set. 
Now apply the generalized Filippov selection theorem (identifying η with v
and � with U, and replacing I by dom{η}). If I ' = ∅, there is nothing to prove. 
Otherwise, since 
I ' = {t ∈ dom{η}:{γ ∈ �(t) : g(t, γ ) = η(t)} /= ∅},
I ' is a nonempty, Lebesgue measurable set and there exists a measurable function 
γ : I ' → Rk such that 
η(t) = g(t, γ (t)) a.e. t ∈ I '
.
⨅⨆
2.4 The Generalized Bolza Problem 
In Sect. 6.4 we shall give conditions for the existence of minimizers in the context 
of minimization problems over some class of arcs satisfying a given differential 
inclusion x(t) ˙ ∈ F (t, x(t)). These conditions restricted attention to problems for 
which the velocity sets F (t, x) are bounded. For traditional variational problems and 
also for many dynamic optimization problems of interest there are no constraints 
on permitted velocities. To deal in a unified manner with problems with bounded 
and unbounded velocity sets, it is convenient to adopt a new framework for the 
optimization problems involved, namely to regard them as special cases of the 
generalized Bolza problem: 
(GBP )
Minimize Λ(x) := l(x(S), x(T )) + 	 T
S L(t, x(t), x(t))dt ˙
over arcs x ∈ W1,1([S,T ]; Rn),
in which [S,T ] is a given interval, and l : Rn × Rn → R ∪ {+∞} and L : [S,T ] ×
Rn × Rn → R ∪ {+∞} are given extended valued functions. Provided we arrange 
that t → L(t, x(t), x(t)) ˙ is measurable and minorized by an integrable function 
for every x ∈ W1,1 (our hypotheses take care of this), Λ will be a well-defined 
R ∪ {+∞} valued functional on W1,1. 
Notice that the functions l and L are permitted to takes values +∞. So they 
can be used implicitly to take account of constraints. For example the ‘differential 
inclusion’ problem108 2 Set Convergence, Measurability and Existence of Minimizers
⎧
⎪⎪⎨
⎪⎪⎩
Minimize g(x(S), x(T ))
over arcs x ∈ W1,1([S,T ]; Rn) satisfying
x(t) ˙ ∈ F (t, x(t)),
(x(S), x(T )) ∈ C.
is a special case of the generalized Bolza problem in which 
L(t, x, v) =

0 if v ∈ F (t, x)
+∞ otherwise
and 
l(x0, x1) =

g(x0, x1) if (x0, x1) ∈ C
+∞ otherwise.
In existence theorems covering problems with unbounded velocity sets, super￾linear growth hypotheses on the cost integrand are typically invoked to compensate 
for unbounded velocity sets. The key advantage of the generalized Bolza problem 
as a vehicle for such theorems is that unrestrictive hypotheses ensuring existence 
of minimizers, which require coercivity of the cost integrand in precisely those 
‘directions’ in which velocities are unconstrained, can be economically expressed 
as conditions on the extended valued function L. 
Theorem 2.4.1 (Generalized Bolza Problem: Existence of Minimizers) Assume 
that the data for (GBP) satisfy the following hypotheses: 
(H1): l is lower semi-continuous and there exist a lower semi-continuous function 
l
0 : R+ → R satisfying 
limr↑+∞l
0(r) = +∞
and either 
l
0(|x0|) ≤ l(x0, x1) for all x0, x1
or 
l
0(|x1|) ≤ l(x0, x1) for all x0, x1,
(H2): L is L × Bn×n measurable, 
(H3): L(t, ., .) is lower semi-continuous for each t ∈ [S,T ], 
(H4): For each (t, x) ∈ [S,T ] × Rn, L(t, x, .) is convex and dom L(t, x, .) /= ∅, 
(H5): For all t ∈ [S,T ], x ∈ Rn and v ∈ Rn, 
L(t, x, v) ≥ θ (|v|) − α|x|,2.4 The Generalized Bolza Problem 109
for some α ≥ 0 and and some lower semi-continuous, convex function θ :
R+ → R+ satisfying 
limr↑+∞ θ (r)/r = +∞.
Then (GBP) has a minimizer. (We allow the possibility that Λ(x) = +∞ for all 
x ∈ W1,1. In this case all arcs x are regarded as minimizers.) 
Remark 
The proof of this theorem, which follows shortly, exploits properties of the 
Hamiltonian 
H (t, x, p) := supv∈Rn {p · v − L(t, x, v)}.
To a large extent then, the role of the growth condition (H5) is to ensure that the 
Hamiltonian has the required properties to furnish existence theorems. Loosely 
speaking, growth conditions on the Lagrangian translate into (one-sided) bounded￾ness conditions on the Hamiltonian. One direction for generalizing this theorem is 
to replace (H5) by less restrictive conditions imposed directly on the Hamiltonian. 
Rockafellar has shown that the conclusions of Theorem 2.4.1 remain valid when 
(H5) is replaced by: 
(H5)' For all t ∈ [S,T ] and x,p ∈ Rn
H (t, x, p) ≤ μ(t, p) + |x|(σ (t) + ρ(t)|p|),
for some integrable functions σ, ρ and some function μ such that μ(., p) is 
integrable for each p ∈ Rn. 
The ensuing analysis calls upon some properties of convex functions, which it 
is now convenient to summarize. Take any function f : Rn → R ∪ {+∞}; the 
conjugate of f is the function f ∗ : Rn → R ∪ {+∞}, defined by the Legendre￾Fenchel transformation: 
f ∗(y) := supx∈Rn {y · x − f (x)}.
We say that a function f : Rn → R ∪ {+∞} is proper if dom f = ∅ / . Important 
facts are that, for any proper lower semi-continuous, convex function f : Rn → R∪
{+∞}, its conjugate f ∗, too, is proper lower semi-continuous, convex. Furthermore, 
f can be recovered from f ∗ by means of a second application of the Legendre￾Fenchel transformation: 
f (x) = supy∈Rn

y · x − f ∗(y)
.110 2 Set Convergence, Measurability and Existence of Minimizers
One consequence of these relations is 
Proposition 2.4.2 (Jensen’s Inequality) Take any proper lower semi-continuous, 
convex function f : Rn → R ∪ {+∞}. Then, for any set I ⊂ R of positive measure 
and any v ∈ L1(I ; Rn), t → f (v(t)) is a measurable function, minorized by an 
integrable function, and 

I
f (v(t))dt ≥ |I |f

|I |
−1

I
v(t)dt
,
where |I | denotes the Lebesgue measure of I . 
Proof Take any y ∈ dom f ∗. Then, for each t ∈ I , 
f (v(t)) ≥ v(t) · y − f ∗(y).
It follows that the (measurable) function t → f (v(t)) is minorized by an integrable 
function. Also, 

I
f (v(t))dt ≥

I
v(t)dt
· y − f ∗(y)|I |
= |I |
|I |
−1

I
v(t)dt
· y − f ∗(y)
.
This inequality is valid for all y ∈ dom f ∗. Maximizing over dom f ∗ and noting 
that f is obtained from f ∗ by applying the Legendre-Fenchel transformation, we 
obtain 

I
f (v(t))dt ≥ |I |f

|I |
−1

I
v(t)dt
,
as claimed. ⨅⨆
Proof of Theorem 2.4.1 We assume that l
0(|x0|) ≤ l(x0, x1) for all (x0, x1). (The 
case l
0(|x1|) ≤ l(x0, x1) for all (x0, x1) is treated similarly.) Under the hypotheses, 
l
0 and θ are bounded below. By scaling and adding a constant to 	 Ldt + l (this 
does not effect the minimizers) we can arrange that l
0 ≥ 0 and θ ≥ 0. We can also 
arrange that the constant α is arbitrary small. 
Choose α such that eα|T −S|
|T − S|α < 1. 
Since θ has superlinear growth, we can define k : R+ → R+: 
k(β) := sup {r ≥ 0 : r = 0 or θ (r) ≤ βr}.
Step 1: Fix M ≥ 0. We show that the level set 
SM := {x ∈ W1,1 : Λ(x) ≤ M}2.4 The Generalized Bolza Problem 111
is weakly sequentially pre-compact, i.e. any sequence {xi} in SM has a subsequence 
which converges, with respect to the weak W1,1 topology, to some point in W1,1. 
Take any x ∈ SM and define the L1 function 
q(t) := L(t, x(t), x(t)). ˙
Then 
| ˙x(t)| ≤ k(1) + θ (| ˙x(t)|) ≤ k(1) + q(t) + α|x(t)| a.e.. (2.4.1) 
But, for each t ∈ [S,T ], 
 t
S
q(s)ds = Λ(x) −
 T
t
q(s)ds ≤ M −
 T
t
θ (| ˙x(s)|)ds + α
 T
t
|x(s)|ds
≤ M + α|T − S| |x(t)| +  T
t
(α|T − S|| ˙x(s)| − θ (| ˙x(s)|)) ds
≤ M + α|T − S| |x(t)| + α|T − S|
2k(α|T − S|).
It follows from (2.4.1) and Gronwall’s lemma that 
|x(t)| ≤ eα(t−S) 
|x(S)| +  t
S
(k(1) + q(s))ds
≤ eα(T −S) [|x(S)| + k(1)|T − S| + M
+α|T − S|
2k(α|T − S|) + α|T − S| |x(t)|

.
Therefore 
|x(t)| ≤ A|x(S)| + B (2.4.2) 
where the constants A and B (they do not depend on x) are 
A := 
1 − α|T − S|eα|T −S|
−1
eα|T −S|
and 
B := 
1 − α|T − S|eα|T −S|
−1 
k(1)|T − S| + M + α|T − S|
2k(α|T − S|)

eα(T −S).
We deduce from (2.4.2) and the fact that l(x0, x1) ≥ l
0(|x0|) that 
|x(S)| ≤ K, (2.4.3)112 2 Set Convergence, Measurability and Existence of Minimizers
where K > 0 is any constant (it can be chosen independent of x) such that 
l
0(r) − α(T − S)[Ar + B] > M for all r ≥ K.
Now, for any set I ⊂ [S,T ] of positive measure, Jensen’s inequality yields 
θ (|I |
−1

I
| ˙x(t)|dt) ≤ |I |
−1

I
θ (| ˙x(t)|)dt
≤ |I |
−1
 T
S
θ (| ˙x(t)|)dt
≤ |I |
−1
 T
S
L(t, x(t), x(t))dt ˙ + α
 T
S
|x(t)|dt
≤ |I |
−1(M + α(T − S)(AK + αB)).
We conclude that, if 	
I | ˙x(t)|dt > 0 , 
θ (	
I | ˙x(t)|dt/|I |)
	
I | ˙x(t)|dt/|I | ≤ (M + α(T − S)(AK + B))
	
I | ˙x(t)|dt .
Since θ has superlinear growth, it follows from this inequality that there exists a 
function ω : R+ → R+ (which does not depend on x) such that limσ↓0ω(σ ) = 0
and 

I
| ˙x(t)|dt ≤ ω(|I |) for all measurable I ⊂ [S,T ]. (2.4.4) 
Take any sequence {xi} in SM. Then, by (2.4.3), {xi(S)} is a bounded sequence. 
On the other hand, { ˙xi} is an equicontinuous sequence, by (2.4.4). Invoking the 
Dunford Pettis criterion for weak sequential compactness in L1 (Theorem 6.3.1), 
we deduce that, along a subsequence, 
xi(S) → x(S) and x˙i → ˙x weakly in L1.
Otherwise expressed, 
xi → x weakly in W1,1,
for some x ∈ W1,1. This is what we set out to prove. 
Step 2: Take an L × Bn function φ : [S,T ] × Rn → R ∪ {+∞} which satisfies the 
conditions: 
(a): For each t ∈ [S,T ], φ(t, .) is lower semi-continuous and finite at some point,2.4 The Generalized Bolza Problem 113
(b): For some p˜ ∈ L∞, the function t → φ(t, p(t)) ˜ is minorized by an integrable 
function. 
We shall show that 
 T
S
φ(t)dt ˆ = supp∈L∞
 T
S
φ(t, p(t))dt, (2.4.5) 
where 
φ(t) ˆ := supp∈Rnφ(t, p).
(Note that, under the hypotheses, φˆ is measurable and minorized by an integrable 
function. So the left side of (2.4.5) is well-defined. The right side is interpreted as 
the supremum of the specified integral over p’s such that the integrand is minorized 
by some integrable function.) 
For any p ∈ L∞, φ(t) ˆ ≥ φ(t, p(t)) for all t. It immediately follows that (2.4.5) 
holds, when ‘≥’ replaces ‘=’. 
It suffices then to validate (2.4.5) when ‘≤’ replaces ‘=’. To this end, choose any 
r ∈ Rn such that 
 T
S
φ(t)dt > r . ˆ
We can also choose K > 0 and ϵ > 0 such that, writing 
ˆ
φ(t) ˆ := min {φ(t), K ˆ },
we have 
 T
S
ˆ
φ(t)dt > r ˆ
and 
 T
S
( ˆ
φ(t) ˆ − ϵ)dt > r.
Define the multifunction 
�(t) := {p ∈ Rn : φ(t, p) > ˆ
φ(t) ˆ − ϵ}.
Under the hypotheses, � takes values non-empty (open) sets and Gr � is L × Bn
measurable. According to Aumann’s measurable selection theorem then, � has a 
measurable selection, which we write p¯.114 2 Set Convergence, Measurability and Existence of Minimizers
However, since t → φ(t, p(t)) ¯ and t → φ(t, p(t)) ˜ are minorized by integrable 
functions, we can find a measurable set E such that p¯ restricted to [S,T ] \ E is 
essentially bounded and 

[S,T ]\E
φ(t, p(t))dt ¯ +

E
φ(t, p(t))dt > r. ˜
It follows that 
 T
S
φ(t, p(t))dt > r,
in which p is the essentially bounded function 
p(t) := 
p(t) ˜ if t ∈ E,
p(t) ¯ otherwise.
Since r is an arbitrary strict lower bound on 	 T
S φ(t)dt ˆ , the desired inequality is 
confirmed. 
Step 3: We show that Λ is weakly sequentially lower semi-continuous (w.r.t. the 
W1,1 topology). 
Since weak W1,1 convergence implies uniform convergence, we deduce from 
the lower semicontinuity of l that x → l(x(S), x(T )) is weakly sequentially lower 
semi-continuous. It remains therefore to show that 
Λ(x) ˜ :=  T
S
L(t, x(t), x(t))dt ˙
is also weakly sequentially lower semi-continuous. Take any x ∈ W1,1. Then, since 
L(t, x(t), .) is a proper lower semi-continuous, convex function for each t, 
Λ(x) ˜ =
 T
S
supp∈Rn [p · ˙x(t) − H (t, x(t), p)] dt
= supp∈L∞
 T
S
[p(t) · ˙x(t) − H (t, x(t), p(t))] dt.
(We have used the results of Step 2 to justify the last equality. Note that the function 
φ(t, p) = p · ˙x(t) − H (t, x(t), p) satisfies the relevant hypotheses. In particular, 
φ(t, p(t)) ˜ is minorized by an integrable function for the choice p˜ ≡ 0.) 
For fixed p ∈ L∞, consider now the integral functional 
Λp(x) :=  T
S
[p(t) · ˙x(t) − H (t, x(t), p(t))] dt.
We claim that Λp is weakly sequentially lower semi-continuous.2.5 Exercises 115
To verify this assertion, take any weakly convergent sequence yi → y in W1,1. 
Then y˙i → ˙y weakly in L1 and yi → y uniformly. Since H (t, ., p(t)) is upper 
semi-continuous for each t and the functions t → −H (t, yi(t), p(t)) are minorized 
by a common integrable function (this last property follows from hypothesis (H5)), 
we deduce from Fatou’s lemma 
liminfi→∞Λp(yi) = lim infi→∞  T
S
[p(t) · ˙yi(t) − H (t, yi(t), p(t))] dt
≥
 T
S
[p(t) · ˙y(t) − H (t, y(t), p(t))] dt
= Λp(y).
Weak sequential lower semicontinuity of Λp is confirmed. 
We have shown that, for each x ∈ W1,1, 
Λ(x) ˜ = supp∈L∞Λp(x).
But the upper envelope of a family of weakly sequentially lower semi-continuous 
functionals on W1,1, is also weakly sequentially lower semi-continuous. It follows 
that Λ˜ is weakly sequentially lower semi-continuous. 
Conclusion We have shown in Steps 1 and 3 that Λ is sequentially lower semi￾continuous and that the level sets of Λ are sequentially compact with respect to 
the weak W1,1 topology. These properties guarantee existence of a minimizer. (We 
allow the possibility that Λ(x) = +∞ for all x. In this case all x’s are minimizers.) 
⨅⨆
2.5 Exercises 
2.1 Let U : [S, T ] ⇝ Rm be a non-empty multifunction. Write U := {u : [S, T ] →
Rm L − measurable : u(t) ∈ U (t), a.e. t ∈ [S, T ]}. Assume that U is nonempty. 
Consider the map dE : U × U → R+ defined by 
dE (u'
, u) := meas {t ∈ [S,T ] : u'
(t) /= u(t)}.
Show that dE is a metric on U and (U, dE ) is a complete metric space. 
2.2 Consider the control system 
(CS)
⎧
⎨
⎩
x(t) ˙ = f (t, x(t), u(t)) a.e. t ∈ [S,T ],
u(t) ∈ U (t), a.e. t ∈ [S,T ],
x(S) ∈ C.116 2 Set Convergence, Measurability and Existence of Minimizers
Assume that 
(a): f (., x, .) is L×Bm measurable for each x ∈ Rn, and Gr U = {(t, u) ∈ [S, T ]×
Rm : u ∈ U (t)} is a L × Bm measurable set, 
(b): for some kf , cf ∈ L1(S, T )
|f (t, x, u) − f (t, x'
, u)| ≤ kf (t)|x − x'
| and |f (t, x, u)| ≤ cf (t), for 
all x, x' ∈ Rn and u ∈ U (t), a.e. t ∈ [S, T ], 
(c): C ⊂ Rn is a closed set. 
Consider the (nonempty) set X := {admissible processes (x, u) for (CS)}, and the 
map dX : X × X → R+ defined by 
dX ((x'
, u'
), (x, u)) := |x'
(S) − x(S)| + meas {t ∈ [S,T ] : u'
(t) /= u(t)}.
Show that dX defines a metric on X and (X , dX ) is a complete metric space. 
2.3 Let U : [S, T ] ⇝ Rm be a non-empty multifunction such that Gr U is an 
L × Bm measurable set. Take an absolutely continuous strictly increasing function 
ψ : [S, T ]→[S, T ]. Show that the graph of multifunction U
 : [S, T ] ⇝ Rm, 
defined by U (s)  := U (ψ−1(s)), is an L × Bm measurable set. 
Remark 
The property expressed in this exercise has a role in some reduction techniques to 
simplify the analysis, employing transformations of the time variable (cf. the proof 
of Theorem 9.6.2). 
Hint: Show that, if ψ : [S, T ]→[S, T ] is a homeomorphism, then ψ(B) ∈ B
for all B ∈ B (B is the σ-algebra of Borel sets in [S, T ]). And prove that, if ψ :
[S, T ] → R is an absolutely continuous function, then L − meas (ψ(N )) = 0, for 
all N ⊂ [S, T ] such that L − meas (N ) = 0 (i.e. ψ enjoys the ‘Lusin property’). 
2.6 Notes for Chapter 2 
The ‘Kuratowski sense’ limit operations on sequences of sets of Sect. 2.2 are 
so called for they were employed in Kuratowski’s influential 1933 monograph, 
reprinted as [135]. But their definitions were anticipated in Painlevé’s earlier lecture 
notes. (See [177, Notes to Chapter 4].) Closely related concepts concern conver￾gence of set valued functions. A detailed discussion of the differing definitions 
and terminologies that have been proposed are also to be found in [177, Notes to 
Chapter 5]. We shall introduce various continuity concepts for set valued mappings 
(in addition to pointwise convergence in the Kuratowski sense) in subsequent 
chapters, as required.2.6 Notes for Chapter 2 117
Measure and integration theory of set valued functions on a measure space arose 
out of applications to control theory [103] economics ([15, 89]) and other fields. 
Castaing is credited with initiating a systematic study of the field, establishing 
different fundamental properties associated with the concept of multifunctions 
measurability in a very general framework, see [53]. These become equivalent when 
the multifunctions take values closed sets in Rn (cf. [53, Chapter III]), which is 
often, but not always, the case in applications to dynamic optimization. We follow 
Clarke [65], Ioffe and Tihomirov [132] and Rockafellar and Wets [177] who, to 
simplify certain constructions, adopted as defining property of a measurable set 
value function G the requirement that G−1(O) is measurable for all open sets 
O ⊂ Rn. We recommend [177, Chapter 14] for its up-to-date and comprehensive 
exposition of this material. 
For problems in the calculus of variations (one independent variable) Tonelli 
was the first to identify the key hypotheses for existence of minimizers, namely 
convexity and uniform, superlinear growth of the Lagrangian w.r.t. the velocity 
variable. See [189]. Tonelli considered smooth Lagrangians. Rockafellar’s formu￾lation and systemic investigation of the generalized problem of Bolza [174], in 
which the Lagrangian is permitted to be a nonsmooth, extended valued function 
was an important advance. Its significance is that it brings together, within a 
single framework a wide range of dynamic optimization problems, including those 
incorporating controlled differential equations, differential inclusions and mixed 
pathwise constraints. In particular, Rockafellar identified minimal properties of the 
Lagrangian, under which the integral of its evaluation along an arbitrary arc is well 
defined or under which level sets are pre-compact. (These properties are implicit 
in the hypotheses of Theorem 2.4.1, though they take a more complicated form 
(‘normal integrands’) when the Lagrangian is not convex in the velocity variable.) 
This framework has been successfully employed both as a basis for existence theory 
and in the derivation of necessary conditions of optimality.Chapter 3 
Variational Principles 
Abstract The term ‘variational principle’, which was formerly attached to laws 
of nature asserting that some quantity is minimized, is now used to describe any 
procedure in which a property of interest is shown to imply that some quantity 
is minimized. Variational principles, taken to mean ‘some function is minimized’, 
make sense even if the function is not differentiable, and the part they play in 
nonsmooth analysis is not then all that surprising. 
This chapter brings together a number of variational principles that feature 
prominently in nonsmooth analysis and its applications. It also introduces a 
regularization procedure ‘quadratic inf convolution’ that is frequently used hand￾in-hand with variational principles to reproduce, in a nonsmooth setting, formerly 
known properties of smooth functions. 
The first variational principle is the exact penalization theorem. It tells us that if 
a point minimizes a Lipschitz function over a closed set, then the point remains a 
minimizer of an unconstrained problem, in which the constraint is accommodated 
by nonsmooth penalty function (the distance function to the set scaled by the 
Lipschitz constant of the objective function). 
A lower semi-continuous function with domain a topological space may fail to 
have a minimizer if the domain is not compact. It is however possible to assure 
existence of a minimizer if we add to the function a suitable perturbation term. 
The remaining variational principles provide different procedures for constructing 
a perturbation term to ensure this property. The oldest of these, Ekeland’s theorem, 
features a nonsmooth perturbation. It not only ensures existence of a minimizer, 
but locates that minimizer near an approximate minimizer for the original problem 
and tells us we can control the size of the perturbation according to the accuracy 
of the approximation. For some problems the presence of a nonsmooth perturbed 
term (obtained after applying a variational principle) can be inconvenient. The 
Borwein/Preiss theorem can be regarded as a variant of Ekeland’s theorem, in 
which restrictions are placed on the class of functions to which it applies, but the 
perturbation term is smooth. Stegall’s theorem gives conditions under which we 
choose a linear perturbation term, but gives no information about the location of the 
minimizer for the perturbed problem. 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
P. Bettiol, R. B. Vinter, Principles of Dynamic Optimization, Springer Monographs 
in Mathematics, https://doi.org/10.1007/978-3-031-50089-3_3
119120 3 Variational Principles
The chapter ends with a section on mini-max theory. This area is of interest in its 
own right, in large part, because of its relevance to game theory. But it is included in 
this chapter because it also has important applications to optimization, for example 
in the derivation of Lagrange multiplier rules for constrained optimization problems, 
where the Lagrange multipliers are interpreted as secondary player in some two 
player game. 
3.1 Introduction 
The name ‘variational principle’ is traditionally attached to a law of nature asserting 
that some quantity is minimized. Examples are Dirichlet’s principle (the spatial 
distribution of an electrostatic field minimizes some quadratic functional), Fermat’s 
principle (a light ray follows a shortest path) or Hamilton’s principle of least action 
(the evolution of a dynamical system is an ‘extremum’ for the action functional). 
These principles are called ‘variational’ because working through their detailed 
implications entails solving problems in the calculus of variations. 
Nowadays the term ‘variational principle’ is used to describe any procedure in 
which a property of interest is shown to imply that some quantity is minimized. 
Variational principles in this broader sense are at the heart of nonsmooth analysis 
and its applications. It is no exaggeration to say that they have as significant a 
role in a nonsmooth setting as, say, the inverse function theorem and fixed point 
theorems do in traditional real analysis. Variational principles, ‘some function is 
minimized’, make sense even if the function is not differentiable, and the part they 
play in nonsmooth analysis is not therefore all that surprising. 
This chapter brings together a number of variational principles that feature 
prominently both in the fundamental theory of nonsmooth analysis and its applica￾tions. Take an extended valued function f : X → R ∪ {+∞}. A point x0 ∈ dom f
is said to be an ϵ-minimizer if 
f (x0) ≤ inf
x∈X
f (x) + ϵ.
The variational principles we consider all address the question of how to construct 
a modified version of the function f (taking the form of the original function with 
additive perturbation term), to ensure that the modified problem has a minimizer 
and, for certain of them, to locate this minimizer in a neighbourhood of a given ϵ￾minimizer, whose size is controlled by the parameter ϵ. They differ according to the 
assumed regularity of f , the nature of the underlying space X and the form of the 
perturbations. 
The first and simplest of these is the exact penalization theorem. Here X is a 
metric space and f is a Lipschitz continuous function. It asserts that if a point x0 is 
a minimizer of f over a subset C ⊂ X, then x0 is also a minimizer over the whole 
space of a modified function, which is the extension to all of X of the restriction 
of f to C. The additive perturbation term is the scaled distance function to the3.2 Exact Penalization 121
subset C. We also take the opportunity in this chapter to introduce the quadratic 
inf convolution operation, which permits us to approximate a Lipschitz continuous 
function by a function having a one-sided differentiability property and which has 
an important role throughout this book in passing from known optimality conditions 
for minimizers of continuously differentiable functions to analogous optimality 
conditions which are valid for functions that are merely Lipschitz continuous. 
The other variational principles covered in this chapter, the theorems of Ekeland, 
Borwein/Preiss and Stegall, all concern extended valued lower semi-continuous 
functions, but provide differing procedures for perturbing them to guarantee the 
existence of a minimizer, and for locating this minimizer. In Ekeland’s theorem, the 
domain X is allowed to be an arbitrary complete metric space and the perturbation 
term is non-differentiable. Non-differentiability can sometimes be an obstacle to 
analysis and, in such circumstances, the Borwein/Preiss theorem, in which the 
perturbation term is quadratic, may be preferred, even though the Borwein/Preiss 
theorem places a restriction on the underlying space X, which must be a real 
Hilbert space. Stegall’s theorem, in which X is also required to be a real Hilbert 
space, is less precise than the Borwein/Priess theorem because it fails to locate the 
minimizer of the perturbed problem. But it introduces a linear perturbation term 
that is advantageous in certain applications. 
The chapter concludes with a section on the theory of mini-max problems, also 
referred as ‘two person zero-sum games’. This field is of great intrinsic interest, 
with applications in economics, management science, population dynamics under 
evolutionary change, etc. But mini-max theory also has important applications to 
optimization (even thought optimization problems involve only a primary player), 
notably in the derivation of Lagrange multiplier rules for constrained optimization 
problems, in which context the Lagrange multipliers are interpreted as secondary 
player in some two player games. Thus the theory adds to our arsenal of techniques 
for studying optimization problems. 
3.2 Exact Penalization 
Take a metric space (X, m), a non-empty subset C ⊂ X and a function f : X → R. 
We can use the metric m to define a ‘distance function’ dC on X, with respect to the 
set C: 
dC(x) := inf
x'
∈C
m(x, x'
) for each x ∈ X.
It is easy to show that, for any x ∈ C, ‘x ∈ C’ implies ‘dC(x) = 0’ and, if C is a 
closed subset, the converse is also true. 
We say that the function f satisfies a Lipschitz condition on X (with Lipschitz 
constant K) if 
|f (x) − f (x'
)| ≤ Km(x, x'
) for all x, x' ∈ X.122 3 Variational Principles
Consider the problem of minimizing the function f : X → R over the set C ⊂
X. We would like to replace this problem (or at least approximate it) by a more 
amenable one involving no constraint; an approach of long standing is to drop the 
constraint, but to compensate for its absence by adding a ‘penalty term’ Kg(x) to 
the cost (K is the penalty parameter). The function g is chosen to be zero on C
and positive outside C. The larger K, the more severe the penalty for violating the 
constraint, so one would expect that solving the ‘penalized problem’ 
Minimize f (x) + Kg(x) over x ∈ X
for large K would yield a point x which approximately minimizes the cost for the 
original problem and approximately satisfies the constraint. Now suppose that C
is closed and f satisfies a Lipschitz condition on X. A remarkable feature of the 
distance function is that, if it is adopted as the penalty function, then penalization 
is ‘exact’, in the sense that a minimizer for the original problem is also a minimizer 
for the penalized problem. Justification of this assertion is provided by the following 
theorem. 
Theorem 3.2.1 (Exact Penalization Theorem) Let (X, m) be a metric space. Take 
a set C ⊂ X and a function f : X → R. Assume that f satisfies a Lipschitz 
condition on X with Lipschitz constant K. Let x¯ be a minimizer for the constrained 
minimization problem 
Minimize f (x) over points x ∈ X satisfying x ∈ C. (3.2.1) 
Choose any Kˆ ≥ K. Then x¯ is a minimizer also for the unconstrained minimization 
problem 
Minimize f (x) + Kdˆ C(x) over points x ∈ X. (3.2.2) 
If K>K ˆ and C is a closed set, then the converse assertion is also true: any 
minimizer x¯ for the unconstrained problem (3.2.2) is also a minimizer for the 
constrained problem (3.2.1) and so, in particular, x¯ ∈ C. 
Proof Let x¯ be a minimizer for (3.2.1) and let Kˆ ≥ K. Suppose that, contrary to 
the claims of the theorem, x¯ fails to be a minimizer for (3.2.2). Then there exist a 
point y ∈ X and ϵ > 0 such that f (y) + Kdˆ C(y) < f (x)¯ − Kϵˆ . Choose a point 
z ∈ C such that m(y, z) ≤ dC(y) + ϵ. Since Kˆ is a Lipschitz constant for f on X, 
f (z) ≤ f (y) + Km(y, z) ˆ ≤ f (y) + K(d ˆ C(y) + ϵ) < f (x). ¯
This is not possible since x¯ minimizes f over C. 
Suppose next that K>K ˆ and C is closed. Let x¯ be a minimizer for (3.2.2). 
Choose any ϵ > 0. Then we can find a point z ∈ C such that 
dC(x) > m( ¯ x, z) ¯ − Kˆ −1ϵ.3.3 Ekeland’s Theorem 123
We have 
f (z) ≤ f (x)¯ + Km(x, z) ¯
≤ f (x)¯ + KdC(x)¯ + (K/K)ϵ ˆ
< f(x)¯ + Kdˆ C(x)¯ − (Kˆ − K)dC(x)¯ + ϵ
≤ f (z) − (Kˆ − K)dC(x)¯ + ϵ.
It follows that (Kˆ − K)dC(x) < ϵ ¯ . Since ϵ > 0 is arbitrary, dC(x)¯ = 0. But then 
x¯ ∈ C, because C is closed. We deduce that f (c) ≥ f (x)¯ for all c ∈ C. In other 
words, x¯ is a minimizer for (3.2.1). ⨅⨆
3.3 Ekeland’s Theorem 
Take a complete metric space (X, d), a lower semi-continuous function f : X →
R ∪ {+∞}, a point x0 ∈ dom f and some ϵ > 0. 
Suppose that x0 is an ϵ-minimizer for f . This means 
f (x0) ≤ inf
x∈X f (x) + ϵ.
In these circumstances, as we shall see, there exists some x¯ ∈ dom f satisfying 
d(x, x ¯ 0) ≤ ϵ
1
2
which is a minimizer for the perturbed function 
fϵ (x) = f (x) + ϵ
1
2 d(x, x), ¯
i.e. 
fϵ (x)¯ = inf
x∈X fϵ (x).
This is a version of Ekeland’s variational principle. It tells us that we can perturb f
in such a way as to ensure that a minimizer x¯ for the perturbed problem exists. 
Furthermore we can arrange that both the distance of the minimizer x¯ for the 
perturbed function from x0 and also the perturbation term are small, if ϵ is small. 
The essential idea is captured by Fig. 3.1 Here X is R with the metric induced by 
the Euclidean norm, and f is a strictly monotone decreasing function. The function 
f has no minimizer ‘close’ to the ϵ-minimizer x0. (In fact f has no minimizers at 
all!) However an appropriately chosen point x¯, close to x0, is a minimizer for the 
perturbed function 
x → f (x) + ϵ
1
2 d(x, x). ¯124 3 Variational Principles
Fig. 3.1 Ekeland’s thoerem 
The perturbation term penalizes deviation from x¯; by raising the graph of the 
function away from x¯, we force x¯ to be a minimizer. In this example it is essential 
that the perturbation term be a nonsmooth function and also that we allow x¯ to be 
different from x0. 
Widespread use of this variational principle has been made in the field of 
optimization and, indeed, nonlinear analysis generally. Its main role in optimization 
has been to justify techniques for deriving necessary conditions for a minimization 
problem based on applying available necessary conditions to a simpler, perturbed 
problem and passage to the limit. The fact that it allows (X, d) to be an arbitrary 
complete metric space adds greatly to it flexibility. 
The variational principle summarized above is a special case of a slightly more 
general theorem in which, by making different choices of parameter α > 0 and 
λ > 0, we can trade off the size of the perturbation term and the distance of x¯ from 
x0. (The preceding version involves the choices λ = ϵ
1
2 and α = ϵ
1
2 .) 
Theorem 3.3.1 (Ekeland’s Variational Principle) Take a complete metric space 
(X, d), a lower semi-continuous function f : X → R ∪ {+∞}, a point x0 ∈ dom f
and numbers α > 0 and λ > 0. Assume that 
f (x0) ≤ inf
x∈X f (x) + λα. (3.3.1) 
Then there exists x¯ ∈ X such that 
(i) f (x)¯ ≤ f (x0), 
(ii) d(x0, x)¯ ≤ λ, 
(iii) f (x)¯ ≤ f (x) + αd(x, x)¯ for all x ∈ X. 
Proof It suffices to find some x¯ such that 
(a): f (x)¯ + αd(x0, x)¯ ≤ f (x0), 
and 
(b): f (x) < f (x) ¯ + αd(x, x)¯ for all x /= ¯x. 
Indeed (a) implies (i) and (b) implies (iii). Notice also that (3.3.1) and (a) imply 
f (x)¯ + αd(x0, x)¯ ≤ f (x0) ≤ inf
x∈X f (x) + λα ≤ f (x)¯ + λα.3.3 Ekeland’s Theorem 125
Since f (x)¯ is finite (from (a)) and α > 0 we conclude that d(x0, x)¯ ≤ λ. The 
remaining assertion (ii) is also confirmed. 
The proof of (a) and (b) is as follows. Define a multifunction T : X ⇝ X
T (x) := {y ∈ X : f (y) + αd(x, y) ≤ f (x)}.
Notice that for each x ∈ X, T (x) is a closed set. It is clear that 
x ∈ T (x) for all x ∈ X. (3.3.2) 
A further significant property of the multifunction T is 
y ∈ T (x) implies T (y) ⊂ T (x). (3.3.3) 
This is obviously true if x /∈ dom f (for then T (x) = X). So assume x ∈ dom f . 
Since y ∈ T (x), 
f (y) + αd(x, y) ≤ f (x).
Take any z ∈ T (y). Then 
f (z) + αd(z, y) ≤ f (y).
These inequalities, together with the triangle inequality imply 
f (y) + f (z) + αd(z, x) ≤ f (x) + f (y).
However f (y) < ∞, since f (x) < ∞. We conclude that 
f (z) + αd(z, x) ≤ f (x).
This inequality implies z ∈ T (x), which is the desired condition. 
Now define ξ : dom f → R ∪ {+∞}
ξ(x) := inf
y∈T (x) f (y). (3.3.4) 
We see that 
y ∈ T (x) implies ξ(x) ≤ f (x) − αd(x, y).
It follows that 
diam T (x) ≤ 2α−1(f (x) − ξ(x)) (3.3.5)126 3 Variational Principles
where 
diam T (x) = sup{d(y, y'
) : y ∈ T (x), y' ∈ T (x)}.
Now construct a sequence x0, x1,..., starting with the x0 of the theorem 
statement, as follows: for k = 0, 1, .. choose xk+1 ∈ T (xk) to satisfy 
f (xk+1) ≤ ξ(xk) + 2−k.
This is possible since, by definition, 
ξ(xk) = inf x∈T (xk )
f (x).
Fix k ≥ 0. Since xk+1 ∈ T (xk), we deduce from (3.3.3) that 
ξ(xk) ≤ ξ(xk+1).
On the other hand, property (3.3.2) implies 
ξ(x) ≤ f (x) for all x ∈ X.
It follows 
ξ(xk+1) ≤ f (xk+1) ≤ ξ(xk) + 2−k ≤ ξ(xk+1) + 2−k.
From the preceding inequalities we deduce that 
0 ≤ f (xk+1) − ξ(xk+1) ≤ 2−k.
Recalling (3.3.5) we see that 
diam T (xk+1) ≤ 2−k · 2α−1.
We have shown that {T (xk)} is a nested sequence of non-empty closed sets in X
whose diameters tend to zero as k → ∞. {xk} is a Cauchy sequence then and, since 
(X, d) is complete, xk → ¯x for some x¯ ∈ X. Furthermore we have 
∩∞
k=0 T (xk) = {¯x}. (3.3.6) 
It remains to show that, for this choice of x¯, properties (a) and (b) are satisfied. But 
(3.3.6) implies x¯ ∈ T (x0) which simply means 
f (x)¯ + ϵd(x0, x)¯ ≤ f (x0).3.4 Quadratic Inf Convolution 127
We have verified (a). On the other hand (3.3.3) and (3.3.6) imply 
T (x)¯ ⊂ ∩∞
k=0T (xk) = {¯x}.
It follows that if x /= ¯x then x /∈ T (x)¯ and so 
f (x) < f (x) ¯ + αd(x, x). ¯
We have verified (b). ⨅⨆
3.4 Quadratic Inf Convolution 
Situations are encountered throughout applied analysis, in which we would like to 
establish properties of some function g : Rk → R ∪ {+∞} by means of analytical 
techniques which would be applicable if the function satisfied certain conditions, 
but we are prevented from doing so because these conditions are violated. A 
standard approach to dealing with this difficulty is to construct a family of functions 
{gα : α > 0}, each member of which satisfies the required conditions, such that 
g can be identified, in some way, with limα→∞ gα. The idea is then to demonstrate 
the desired properties of g by verifying them for each α (by means of available 
techniques) and to show that the properties are preserved in the limit as α → ∞. 
A common manifestation of this difficulty is that a continuous function g : Rk →
R fails to have first (or higher) order derivatives. Here it is often useful to ‘mollify’ 
g, to achieve a higher order of differentiability, by convolving it with a smooth 
mollifier φα, depending on the parameter α > 0, which has the properties that it is 
non-negative valued, vanishes outside α−1B and 

... 
φα(x)dx1 ...dxk = 1 .
Thus we choose 
gα(x) =

... 
g(y)φα(x − y)dy1 ... dyk .
We refer to the operation of replacing g by gα as ‘integral convolution’. 
In later chapters of this book, where we investigate optimization problems 
involving a Lipschitz continuous function g, it will sometimes be convenient to 
employ techniques that are available only under the following conditions 
(i): for each x ∈ Rk, there exist η ∈ Rk and M > 0 such that 
g(y) − g(x) ≤ η · (x − y) + M|y − x|
2 for all y ∈ Rk128 3 Variational Principles
and 
(ii): the vector η can be identified as a proximal subgradient of g at some point y
near x. (Here, ‘proximal subgradient’ is understood as in Chap. 1). 
A vector η satisfying the inequality in condition (i) (for given x) is called a proximal 
supergradient of g at x. Another way of describing condition (i) is: the proximal 
subdifferential of x → −g(x) is non-empty at all points in its domain. A Lipschitz 
continuous function may fail to have a proximal superdifferential at some points in 
its domain. 
It turns out that the integral convolution procedure described above is not well 
suited to this purpose. This is because, although the integral mollification of g
is everywhere differentiable, it is not easy to link the derivatives of the mollified 
function with proximal subgradients of the original function g. 
It is helpful to consider, instead, a procedure, called inf convolution, in which the 
integral operation in replaced by a minimization operation. Now, the approximating 
functions {gα : α > 0} are taken to be: 
gα(x) := inf
y∈Rk
{g(y) + α|x − y|
2} . (3.4.1) 
We say that the function gα given by this formula is the quadratic inf convolution 
of g (with parameter α > 0). The family of functions {gα : α > 0} is elsewhere 
referred to as the Moreau-Yosida envelope of g. 
Key properties of the quadratic inf convolution operation are brought together in 
the following theorem. 
Theorem 3.4.1 (Quadratic Inf Convolution, I) Take a function g : Rk → R, 
with Lipschitz constant kg. Let gα be its quadratic inf convolution (with parameter 
α > 0). Take any x ∈ Rk. Then there exists y ∈ Rk such that 
(i): gα is Lipschitz continuous, with Lipschitz constant kg, 
(ii): gα(x) ≤ g(x) ≤ gα(x) + 1 
4α k2 
g, 
(iii): gα(x'
) − gα(x) ≤ 2α(x − y) · (x' − x) + α|x' − x|
2, for all x' ∈ Rk, 
(iv): 2α(x − y) ∈ ∂P g(y), 
(v): |y − x| ≤ α−1kg. 
Proof For all x, x' ∈ Rk we have 
gα(x) = inf
z∈Rk
{g(z) + α|x − z|
2} = inf
w∈Rk
{g(w − (x' − x)) + α|w − x'
|
2}
≤ inf
w∈Rk
{g(w) + α|x' − w|
2} + kg|x − x'
| = gα(x'
) + kg|x − x'
|.
Exchanging the roles of x and x' we obtain that |gα(x'
)−gα(x)| ≤ kg|x' −x| for all 
x, x' ∈ Rk confirming the fact that gα is Lipschitz continuous of Lipschitz constant 
kg. This is (i).3.4 Quadratic Inf Convolution 129
Observe that gα(x) = inf{g(y'
) + α|y' − x|
2 : y' ∈ Rk}≤ g(x). It follows that 
gα(x) ≤ g(x). Notice also that, for any y'
, 
g(y'
)+α|y'
−x|
2 ≥ g(x)−kg|y'
−x|+α|y'
−x|
2
≥ g(x)+ min{αd2−kgd : d ≥ 0} = g(x) − 1
4α
k2
g .
Taking the infimum of the left side over y' and recalling the definition of gα, we 
conclude that gα(x) ≥ g(x) − 1 
4α k2 
g. We have shown (ii). 
Next note that, for any y' such that |y' − x| > α−1kg, we have 
g(y'
) + α|y' − x|
2 ≥ g(x) − kg|y' − x| + α|y' − x|
2 > g(x) ≥ gα(x) .
It follows that there exists y ∈ Rn such that 
|y − x| ≤ α−1kg
and 
gα(x) = inf
y'
∈Rk

g(y'
) + α|y' − x|
2

= inf
{y'
:|y'
−x|≤α−1kg}

g(y'
) + α|y' − x|
2

= g(y) + α|y − x|
2 ,
since y' → g(y'
) + α|y' − x|
2 is continuous and the second infimum in the 
preceding relation is taken over a compact set. We have shown that the infimum 
in the definition of gα(x) is attained. 
Take any x' ∈ Rk. Then 
gα(x'
) − gα(x) ≤ g(y) + α|y − x'
|
2 − g(y) − α|y − x|
2
= 2α(x − y) · (x' − x) + α|x' − x|
2 .
We have confirmed (iii) and (v). 
It follows from the definition of gα that, for every y' ∈ Rk, 
g(y) + α|y − x|
2 ≤ g(y'
) + α|y' − x|
2 .
Rearranging this inequality, we arrive at the relation 
g(y'
) − g(y) ≥ α(|y − x|
2 − |y' − x|
2) = 2α(x − y) · (y' − y) − α|y' − y|
2 ,
which can be equivalently expressed 2α(x − y) ∈ ∂P g(y). We have confirmed (iv).
⨅⨆130 3 Variational Principles
The main purpose of the following example is to illustrate how quadratic inf 
convolution can be used in dynamic optimization, specifically to derive necessary 
conditions for problems with non-smooth data and, by so doing, to provide a 
foretaste of techniques employed extensively in subsequent chapters. Notice that the 
role of quadratic inf convolution in this example (and elsewhere in this book) is to 
construct a smooth optimization problem, approximating the original non-smooth 
problem of interest. The fact that such a smooth problem can be constructed can 
be viewed as some kind of variational principle. To this extent, the example links 
quadratic inf convolution to the main themes of this chapter. 
Example 
Consider the dynamic optimization problem 
(P )
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize g(x(T ))
over arcs x ∈ W1,1([S,T ]; Rn)
and meas. functions u : [S,T ] → Rm such that
x(t) ˙ = f (t, x(t), u(t)), a.e. t ∈ [S,T ],
u(t) ∈ U (t), a.e. t ∈ [S,T ],
x(S) = x0 .
in which f : [S, T ] × Rn × Rm → Rn and g : Rn → R are given functions, 
{U (t) ⊂ Rm : S ≤ t ≤ T } is a given family of sets and x0 is a given point in Rn. 
This is a special case of a dynamic optimization addressed in Chap. 1. 
Necessary conditions (in the form of a maximum principle), for a given process 
(x,¯ u)¯ to be an L∞ local minimizer, were provided by Theorem 1.11.1, under 
hypotheses that included the requirement that the terminal cost function g be 
continuously differentiable. We illustrate the application of Theorem 3.4.1 by 
showing that it can be used to derive a form of ‘’nonsmooth’ maximum principle, in 
which g is permitted to be an arbitrary Lipschitz continuous (with Lipschitz constant 
kg) but in which, in all otherwise respects, the hypotheses Theorem 1.11.1 remain 
in place. 
Take ϵi ↓ 0. For each i and let gi be the quadratic inf convolution of g (with 
parameter α = 1 
4ϵi
k2 
g). Then according to property (ii) of the inf convolution in 
Theorem 3.4.1 , the process (x,¯ u)¯ is an ϵi minimizer for a modified version of (P ), 
in which the terminal cost g is replaced by gi. Because there is a fixed left end-point 
there is, under the given hypotheses, a unique state trajectory xu corresponding to a 
given control function u. It follows from the assumption that (x,¯ u)¯ is an L∞ local 
minimizer for (P), that u¯ is an ϵi-minimizer for the problem 
Minimize {J (u) := gi(xu(T )) : u ∈ M},
in which, for some γ > 0, 
M := {u : u is a control for (P ) such that ||xu − ¯x||L∞ ≤ γ }.3.4 Quadratic Inf Convolution 131
The parameter γ takes account of the facts that (x,¯ u)¯ is only an L∞ local minimizer 
and that the hypotheses (B), (C) and (D) of Theorem 1.11.1 are local (not global) 
Lipschitz continuity and boundedness conditions on the velocity function f . 
Under the hypotheses of Theorem 1.11.1, M is a complete metric space for the 
distance function 
dE (u'
, u) := L-meas {t : u'
(t) /= u(t)}.
Furthermore J is continuous on M, with respect to this metric. It follows from 
Ekeland’s theorem that, for i sufficiently large, there exists ui ∈ M such that ui is 
a minimizer for 
Minimize {Ji(u) := gi(xu(T )) + ϵ
1
2
i
 T
S
mi(t, u(t))dt : u ∈ M,
in which mi(t, u) =
	
0 if u = ui(t)
1 if u /= ui(t) . (Notice that dE (u, ui) = 
 T
S mi(t, u(t))dt.) 
Write xi := xui . 
We also know that dE (ui, u)¯ ≤ ϵ
1 
2 
i , from which we conclude: 
xi → ¯x, uniformly, and L-meas. {t : ui(t) /= ¯u(t)} → 0 . (3.4.2) 
Next notice that, by properties (iii), (iv) and (v) of the quadratic inf convolution 
in Theorem 3.4.1, there exists 
ξi ∈ ∂P g(yi) for some yi ∈ xi(T ) +
4ϵi
kg
B,
such that 
	
gi(x) ≤ ˜gi(x) for all x ∈ Rk
gi(x) = ˜gi(x) if x = xi(T ).
Here g˜i is the quadratic function 
g˜i(x) := gi(xi(T )) + ξi · (x − xi(T )) +
k2
g
4ϵi
|x − xi(T )|
2 .
Observe that the function g˜i dominates the function g, and the graphs of these two 
functions touch at the point (xi(T ), gi(xi(T )). It follows that ui remains a minimizer 
when we replace the terminal cost function gi by g˜i. We have shown that (xi, ui) is 
an L∞ local minimizer for the problem132 3 Variational Principles
(Pi)
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize g˜i(x(T )) + ϵ
1
2
i

 T
S mi(t, u(t))dt
subject to
x(t) ˙ = f (t, x(t), u(t)), a.e. t ∈ [S,T ],
u(t) ∈ U (t), a.e. t ∈ [S,T ],
x(S) = x0 .
Now apply the special case of the ‘smooth’ maximum principle, when there is 
no right endpoint constraint to (Pi). (See Proposition 1.11.5.) This is permissible 
because the terminal cost function g˜ is now a continuously differentiable (indeed 
quadratic) function. We deduce the existence of an absolutely continuous function 
pi on [S, T ] such that 
(i): − ˙pi(t) = pi(t) · ∇xf (t, xi(t), ui(t)) a.e. t ∈ [S, T ], 
(ii): −pi(T ) = ξi for some ξi ∈ ∂P g(yi), 
(iii): pi(t)·f (t, xi(t), ui(t)) ≥ max 
u∈U (t)

pi(t)·f (t, xi(t), u)
−ϵ
1 
2 
i , a.e. t ∈ [S, T ].
We know that ||xi − ¯x||L∞ → 0 and L-meas {t : ui(t) /= ¯u(t)} → 0. A 
similar convergence analysis to that employed in the proof of the smooth maximum 
principle (Theorem 1.11.1) permits us to extract subsequences and pass to the limit 
in the above relations, as i → ∞. We arrange in particular that pi → p uniformly, 
for some absolutely continuous function p and ξi → ξ for some ξ ∈ Rn. We 
conclude: 
− p(T ) = ξ = lim
i→∞ ξi ∈∂g(x(T )) ¯
since, by the characterization of the limiting subdifferential (see Theorem 4.6.2) 
∂g(x(T )) ¯ := {η : there exist zi → ¯x(T ) and ηi → η
such that ηi ∈ ∂P g(zi) for all i}.
3.5 Variational Principles with Smooth Perturbation Terms 
Take a lower semi-continuous, extended valued function g : X → R ∪ {+∞} on 
a complete metric space (X, d). Take ϵ > 0 and a point x¯ ∈ X which is an an ϵ
minimizer of the function g over X, that is 
g(x)¯ ≤ inf
x∈X
g(x) + ϵ .
Recall that, according to Ekeland’s theorem, there exists a point y ∈ X such that 
d(y, x)¯ ≤ ϵ
1
2 (3.5.1)3.5 Variational Principles with Smooth Perturbation Terms 133
and 
x → g(x) + ϵ
1
2 d(y, x) is minimized over X at y .
A distinguishing feature of Ekeland’s theorem is that the additive perturbation term 
‘+ϵ
1
2 d(y, x)’ in the modified function is non-smooth. The lack of smoothness 
can be troublesome for some applications and, for this reason, other variational 
principles, involving more amenable perturbation terms, have been investigated. 
We now provide two variants on Ekeland’s theorem in this spirit. Both of them 
concern the properties of a lower semi-continuous function g : X → R ∪ {+∞} but 
now we restrict attention to the case when X is a real Hilbert space. (Write the inner 
product 〈x, y〉X and the induced norm ||x||X.) 
Take an ϵ minimizer x¯ for the function g. In a Hilbert space setting, Ekeland’s 
theorem supplies a minimizer y (close to x¯ with respect to the X norm, if ϵ is 
small) for a function with additive perturbation term +ϵ
1
2 ||x − y||X. To arrive at a 
variational principle with a smooth perturbation term, we might consider replacing 
the norm of x (about the base point y) by the square of the norm, thus +ϵ
1
2 ||x−y||2
X. 
To see that this is not possible, we have only to consider the example of the function 
g(x) := e−x on R. Indeed, 
x → e−x + K|x − y|
2 is not minimized at y for any point y or K > 0 .
The insight of Borwein and Preiss [43] was to understand that modifying the 
perturbation term in this way is possible, if we replace the base point y by a new 
base point z (close to x¯ if ϵ is small). 
Theorem 3.5.1 (Borwein and Preiss) Take a real Hilbert space X, a lower semi￾continuous function g : X → R ∪ {+∞}, which is bounded below, and ϵ > 0. 
Suppose that x¯ ∈ X such that g(x) < ¯ infx∈X g(x) + ϵ. Then, for any λ > 0 there 
exist points y,z ∈ X such that 
||z − ¯x||X < λ, ||y − z||X < λ, g(y) ≤ g(x)¯
and such that the function 
x → g(x) +
ϵ
λ2 ||x − z||2
X
attains a unique minimum at x = y. 
The second variant, like its predecessor, asserts the existence of a minimizer for a 
perturbed function, in which the perturbation term is a linear function of arbitrarily 
small slope. Notice however that the simplicity of the perturbation come at a price: 
it fails to provide any information about the proximity of the minimizer to a given ϵ
minimizer for the original, unperturbed, function.134 3 Variational Principles
Theorem 3.5.2 (Stegall’s Variational Principle) Consider a nonempty, closed 
and bounded subset Y of a real Hilbert space X and a lower semi-continuous 
function g : X → R ∪ {+∞} which is bounded below on Y . Suppose that 
Y ∩ dom g = ∅ / . Then there exists a dense set of points x ∈ X such that 
z → g(z) − 〈x,z〉X
attains a unique minimum over Y . 
Proofs of Theorems 3.5.1 and 3.5.2, based on proximal normal analysis in real 
Hilbert spaces, are given in the Appendix to Chap. 4. 
3.6 Mini-Max Theorems 
Take non-empty sets X and Y and a function F : X × Y → R. The central question 
addressed in this section is: under what circumstances is the relation 
inf
x∈X sup
y∈Y
F (x, y) = sup
y∈Y
inf
x∈X F (x, y) (3.6.1) 
valid? In other words, when do the operations of taking the supremum over Y and 
taking the infimum over X commute? 
Notice that, without the imposition of any additional hypotheses whatsoever, we 
are assured that the two sides of (3.6.1) are related by inequality: 
Proposition 3.6.1 Let X, Y and F be as above. Then 
inf
x∈X sup
y∈Y
F (x, y) ≥ sup
y∈Y
inf
x∈X F (x, y).
Proof For any fixed x' ∈ X and y' ∈ Y
sup
y∈Y
F (x'
, y) ≥ F (x'
, y'
) ≥ inf
x∈X F (x, y'
).
It follows that 
inf
x∈X sup
y∈Y
F (x, y) ≥ inf
x∈X F (x, y'
)
and consequently 
inf
x∈X sup
y∈Y
F (x, y) ≥ sup
y∈Y
inf
x∈X F (x, y).
⨅⨆3.6 Mini-Max Theorems 135
The challenge then is to establish when the reverse inequality holds: 
inf
x∈X sup
y∈Y
F (x, y) ≤ sup
y∈Y
inf
x∈X F (x, y)
for this combines with the assertion of Proposition 3.6.1 to give (3.6.1). 
A related question, and one of great independent interest, is whether there exists 
a point (x∗, y∗) ∈ X × Y satisfying 
sup
y∈Y
F (x∗, y) = F (x∗, y∗) = inf
x∈X F (x, y∗).
A pair (x∗, y∗) having this property is called a saddepoint. 
The connection is that existence of a saddlepoint is a sufficient condition for the 
commutability condition (3.6.1): 
Proposition 3.6.2 Suppose that (x∗, y∗) is a saddlepoint. Then 
inf
x∈X sup
y∈Y
F (x, y) = F (x∗, y∗) = sup
y∈Y
inf
x∈X F (x, y).
Furthermore, 
sup
y∈Y
F (x∗, y) = sup
y∈Y
inf
x∈X F (x, y) (3.6.2) 
and 
inf
x∈X F (x, y∗) = inf
x∈X sup
y∈Y
F (x, y). (3.6.3) 
Proof By definition of a saddlepoint and in view of Proposition 3.6.1 we have 
F (x∗, y∗) = sup
y∈Y
F (x∗, y) ≥ inf
x∈X sup
y∈Y
F (x, y)
≥ sup
y∈Y
inf
x∈X F (x, y) ≥ inf
x∈X F (x, y∗) = F (x∗, y∗).
These relations therefore hold with equality. The assertions of the proposition follow 
immediately. ⨅⨆
Relations (3.6.2) and (3.6.3) do in fact fully characterize a saddlepoint: 
Proposition 3.6.3 There exists a saddlepoint if and only if the following conditions 
both hold: 
(a) There exists x∗ ∈ X such that 
sup
y∈Y
F (x∗, y) = sup
y∈Y
inf
x∈X F (x, y),136 3 Variational Principles
(b) There exists y∗ ∈ Y such that 
inf
x∈X F (x, y∗) = inf
x∈X sup
y∈Y
F (x, y).
Furthermore, if (a) and (b) both hold, then (x∗, y∗) is a saddlepoint. 
Proof In view of the preceding proposition all we have to show is that, if (x∗, y∗)
satisfies conditions (a) and (b), then (x∗, y∗) is a saddlepoint. However (a) implies 
inf
x∈X sup
y∈Y
F (x, y) ≤ sup
y∈Y
F (x∗, y) = sup
y∈Y
inf
x∈X F (x, y).
This combines with the assertions of Proposition 3.6.1 to give 
inf
x∈X sup
y∈Y
F (x, y) = sup
y∈Y
inf
x∈X F (x, y).
So if (b) also holds, then 
F (x∗, y∗) ≤ sup
y∈Y
F (x∗, y) =
sup
y∈Y
inf
x∈X F (x, y) = inf
x∈X sup
y∈Y
F (x, y) = inf
x∈X F (x, y∗) ≤ F (x∗, y∗).
It follows that 
sup
y∈Y
F (x∗, y) = F (x∗, y∗) = inf
x∈X F (x, y∗).
This is the saddlepoint condition. ⨅⨆
Establishing existence of a saddlepoint is by no means straightforward and 
requires the imposition of stringent hypotheses (see Von Neumann’s mini-max the￾orem below). It turns out however that these hypotheses can be relaxed significantly 
if we are willing to settle for just one of the two ‘one-sided’ properties (a) and (b) 
characterizing existence of a saddlepoint. The one-sided mini-max theorem, giving 
hypotheses under which property (a) of Proposition 3.6.3 holds, has come to be 
recognized as a powerful analytic tool with important implications for optimization. 
Numerous applications will be made in future chapters. 
We make one final observation. It is that, while property (a) falls somewhat short 
of guaranteeing existence of a saddlepoint, it nonetheless implies the commutability 
of the infimum and supremum operations. Validity of this assertion is a by-product 
of the proof of Proposition 3.6.3. (Replacing F by −F so that ‘inf’ becomes ‘sup’ 
and vice versa, we arrive at an analogous statement in relation to property (b).)3.6 Mini-Max Theorems 137
Proposition 3.6.4 Suppose that either of the following conditions hold 
(a) there exists x∗ such that 
sup
y∈Y
F (x∗, y) = sup
y∈Y
inf
x∈X F (x, y),
(b) there exists y∗ such that 
inf
x∈X F (x, y∗) = inf
x∈X sup
y∈Y
F (x, y).
Then 
inf
x∈X sup
y∈Y
F (x, y) = sup
y∈Y
inf
x∈X F (x, y).
The main theorems of the section now follow. 
Theorem 3.6.5 (Aubin One-Sided Mini-Max Theorem) Consider a function F :
X × Y → R in which X is a subset of a linear space and Y is a subset of a 
topological linear space. Assume that 
(i) X and Y are convex sets, 
(ii) F (., y) is convex for every y ∈ Y , 
(iii) F (x, .) is concave and upper semi-continuous for every x ∈ X, 
(iv) Y is compact. 
Then there exists y∗ ∈ Y such that 
inf
x∈X F (x, y∗) = inf
x∈X sup
y∈Y
F (x, y).
This implies in particular (see Proposition 3.6.4) that 
inf
x∈X sup
y∈Y
F (x, y) = sup
y∈Y
inf
x∈X F (x, y).
Proof Define 
η− := sup
y∈Y
inf
x∈X F (x, y)
and 
η+ := inf
x∈X sup
y∈Y
F (x, y).138 3 Variational Principles
For any subset K ⊂ X write 
η−
K := sup
y∈Y
inf
x∈K F (x, y).
Now let S denote the class of subsets of X comprising only a finite number of points 
and define: 
η˜
− := inf
K∈S sup
y∈Y
inf
x∈K F (x, y) = inf
K∈S η−
K.
Step 1: We show that 
η− ≤ ˜η− ≤ η+.
Take any K ∈ S. Then 
sup
y∈Y
inf
x∈K F (x, y) ≥ sup
y∈Y
inf
x∈X F (x, y) = η−.
Taking the infimum of the left side over K ∈ S we obtain 
η˜
− ≥ η+.
On the other hand, for any x we have {x} ⊂ S whence 
sup
y∈Y
F (x, y) ≥ inf
K∈S sup
y∈Y
inf
x∈K F (x, y) = ˜η−.
Taking the infimum of the left side over x yields 
η+ ≥ ˜η−,
as required. 
Step 2: We show that there exists y∗ ∈ Y such that 
inf
x∈X F (x, y∗) ≥ ˜η−.
For each x ∈ X, define Ex ⊂ X to be the set 
Ex := {y ∈ Y : F (x, y) ≥ ˜η−}.
We must show that 
∩x∈X Ex /= ∅. (3.6.4)3.6 Mini-Max Theorems 139
Notice however that, because F (x, .) is an upper semi-continuous function, the 
‘level set’ Ex is compact for every x ∈ X. Property (3.6.4) will follow then if we 
can show that, for an arbitrary finite set {x1, .., xm} of points in X, 
∩m
i=1 Exi /= ∅.
Take any finite subset K = {x1,...,xm} ⊂ X. Then 
∩m
i=1 Exi = {y ∈ Y : min
i=1,..,m F (xi, y) ≥ ˜η−}.
But mini=1,..,m F (xi, .) is upper semi-continuous and its maximum value is there￾fore achieved over the compact set Y at some y∗ ∈ Y . We have 
inf
x∈K F (x, y∗) = sup
y∈Y
inf
x∈K F (x, y) ≥ ˜η−.
It follows that ∩m
i=1Exi contains the point y∗ and is therefore non-empty. 
Step 3: For a finite subset K = {x1, ..xm} ⊂ X, define 
ζK := inf
(λ1,...,λm)∈Σm sup
y∈Y
m
i
λiF (xi, y),
in which 
Σm := {(λ1, .., λm) : λi ≥ 0 for each i for all m
i=1
λi = 1}.
We show that 
ζK ≤ η−
K.
(Recall that 
η−
K := sup
y∈Y
inf
x∈K F (x, y) .)
Indeed suppose, contrary to the claim, that there exists α > 0 such that 
ζK − α > sup
y∈Y
inf
x∈K F (x, y).
This implies 
(ζK − α)(1, .., 1) /∈ C140 3 Variational Principles
where C ⊂ Rm is the subset 
C := {(ξ1,...,ξm) : there exists y ∈ Y
such that ξi ≤ F (xi, y) for i = 1, .., m}.
However it may be deduced from the concavity of the functions F (xi, .) and the 
convexity of the set Y , that C is a convex set. 
By the separation theorem there exists a non-zero vector λ ∈ Rm (we may assume 
that m
i=1 |λi| = 1) such that 
(ζK − α)(1, .., 1) · λ ≥ c · λ for all c ∈ C.
This inequality can be satisfied only if λi ≥ 0 for all i. It follows that λ ∈ m and 
(1, .., 1) · λ = 1. Inserting 
c := (F (x1, y), .., F (xm, y))
into this relation for arbitrary y ∈ Y gives 
ζK − α ≥ m
i=1
λiF (x, y).
Taking the supremum of the right side over y ∈ Y , we arrive at 
ζK − α ≥ sup
y∈Y
m
i=1
λiF (xi, y)
≥ inf
λ∈Σm sup
y∈Y
m
i=1
λiF (xi, y) = ζK.
From this contradiction we conclude that 
ζK ≤ η−
K.
Step 4: We show that 
η˜
− ≥ η+.
Take any finite set K = {x1, .., xm} and λ ∈ Σm. Define 
xλ := Σm
i=1λixi.3.6 Mini-Max Theorems 141
Since X is convex and F (., y) is a convex function for each y ∈ Y , we have 
sup
y∈Y
m
i=1
λiF (xi, y) ≥ sup
y∈Y
F (xλ, y) ≥ inf
x∈X sup
y∈Y
F (x, y) = η+.
Taking the infimum on the left over λ ∈ Σm we deduce that 
ζK ≥ η+.
But then, by Step 3, 
η−
K ≥ η+.
Taking the infimum of the left side over K ⊂ S we obtain 
η˜
− ≥ η+.
Conclusion 
Steps 1, 2 and 4 give 
η− ≤ ˜η− ≤ η+,
η−

= sup
y∈Y
inf
x∈X F (x, y)
≥ inf
x∈X F (x, y∗) ≥ ˜η−
for some y∗ ∈ Y and 
η˜
− ≥ η+ =

inf
x∈X sup
y∈Y
F (x, y)
.
We conclude that 
inf
x∈X F (x, y∗) = inf
x∈X sup
y∈Y
F (x, y).
This is what we set out to prove. ⨅⨆
Theorem 3.6.6 (Von Neumann Mini-Max Theorem) Consider a function F :
X × Y → R in which X and Y are subsets of topological linear spaces. Assume 
that 
(i) X and Y are convex, compact sets, 
(ii) F (., y) is convex and lower semi-continuous for every y ∈ Y , 
(iii) F (x, .) is concave and upper semi-continuous for every x ∈ X.142 3 Variational Principles
Then there exists an element (x∗, y∗) ∈ X × Y which is a saddlepoint for F, i.e. 
sup
y∈Y
F (x∗, y) = F (x∗, y∗) = inf
x∈X F (x, y∗). (3.6.5) 
This implies in particular (see Proposition 3.6.2) 
inf
x∈X sup
y∈Y
F (x, y) = sup
y∈Y
inf
x∈X F (x, y).
Proof Apply Theorem 3.6.5 to F (x, y) and also to −F (x, y) (in the latter case 
interchanging the roles of X and Y ). We deduce existence of some (x∗, y∗) ∈ X×Y
such that 
inf
x∈X F (x, y∗) = inf
x∈X sup
y∈Y
F (x, y)
and 
sup
y∈Y
inf
x∈X F (x, y) = sup
y∈Y
F (x∗, y).
In view of Proposition 3.6.4 it follows that 
F (x∗, y∗) ≤ sup
y∈Y
F (x∗, y) = inf
x∈X F (x, y∗) ≤ F (x∗, y∗).
This implies the saddlepoint condition (3.6.5). ⨅⨆
Compactness of the underlying sets X and Y (or at least of one of them) plays an 
essential part in proving the above mini-max theorems. The compactness hypotheses 
on X and Y can be replaced in certain circumstances by coercivity hypotheses on 
the ‘objective function’ F; here compactness of the level sets of certain constructed 
functions in some sense substitutes for that of X and Y . 
We illustrate the point by proving existence of a saddlepoint for an objective 
function F (x, y) neither of whose variables x or y are confined to compact sets. 
This particular objective function is of interest because, in Chap. 8, it will be used 
to establish a link between necessary conditions of optimality for a general class 
of dynamic optimization problems related, on the one hand, to the classical Euler 
Lagrange conditions and, on the other, Hamilton’s system of equation. 
Proposition 3.6.7 Take a lower semi-continuous, convex function h : Rn → R ∪
{+∞} such that dom h = ∅ / , a number σ > 0 and vectors x¯ ∈ Rn and y¯ ∈ Rn. 
F (x, y) := x · (y − ¯y) + σ|x − ¯x|
2 − h(y).3.6 Mini-Max Theorems 143
Then there exists a point (x∗, y∗) ∈ Rn × dom h such that 
F (x∗, y) ≤ F (x∗, y∗) ≤ F (x, y∗) for all x, y ∈ Rn.
Furthermore 
x∗ = ¯x − 1
2
σ −1(y∗ − ¯y).
Proof Translating (x,¯ y)¯ to the origin and replacing the function h by y → ¯x · y +
h(y + ¯y), we reduce consideration to the case when x¯ = ¯y = 0. 
Define D := dom h. By a basic property of lower semi-continuous R ∪ {+∞}
valued convex functions, h is minorized by an affine function. It follows that there 
exists α > 0 and β > 0 such that 
h(y) ≥ −α − β|y| for all y ∈ Rn. (3.6.6) 
Letψ : Rn → R ∪ {−∞} be the function 
ψ(y) := inf
x∈Rn F (x, y).
We explicity calculate 
ψ(y) = −1
4
σ −1|y|
2 − h(y) for all y ∈ Rn.
Notice that ψ(y'
) > −∞ for some y' ∈ Rn (since D /= ∅). Also, by (3.6.6), 
lim sup
|y|→∞
ψ(y) ≤ lim sup
|y|→∞
{−
1
4
σ −1|y|
2 + α + β|y|} = −∞.
It follows that the number d defined by 
d := sup
y∈Rn
ψ(y)
is finite and there exists K > 0 such that 
sup
y∈Rn
ψ(y) = sup
y∈KB∩D
ψ(y).
The right side is expressible as 
sup
y∈KB∩D
inf
x∈Rn F (x, y).144 3 Variational Principles
Applying the one-sided mini-max theorem (Theorem 3.6.5), we obtain a point y∗ ∈
D ∩ KB such that 
inf
x∈Rn F (x, y∗) = sup
y∈KB∩D
inf
x∈Rn F (x, y) = sup
y∈D
inf
x∈Rn F (x, y). (3.6.7) 
Now choose k > 0 such that 
− (k − 2β − α
σ k )σ k < d − 1 and β < k, (3.6.8) 
and define, for each y ∈ Rn, 
ψk(y) := inf
x∈kB F (x, y).
We easily calculate 
ψk(y) =
	
ψ(y) if |y| < 2σ k
−k|y| + σ k2 − h(y) if |y| ≥ 2σk.
Note however that, in view of (3.6.6) and (3.6.8), 
ψk(y) ≤ d − 1 = sup
y∈D
ψ(y) − 1 if |y| ≥ 2σk.
Since ψk majorizes ψ and ψk = ψ on 2σ kB, this last relation can be true only if 
sup
y∈D
ψ(y) = sup
y∈D
inf
x∈kB
F (x, y). (3.6.9) 
Now apply the one-sided mini-nax theorem (Theorem 3.6.5) to 
inf
x∈kB sup
y∈D
F (x, y).
This gives the existence of x∗ such that 
sup
y∈D
F (x∗, y) = sup
y∈D
inf
x∈kB
F (x, y) = sup
y∈D
inf
x∈Rn F (x, y) (3.6.10) 
by (3.6.9). But then, by Proposition 3.6.4, 
sup
y∈D
inf
x∈Rn F (x, y) = inf
x∈Rn sup
y∈D
F (x, y).
We conclude from (3.6.7) that 
inf
x∈Rn F (x, y∗) = inf
x∈Rn sup
y∈D
F (x, y). (3.6.11)3.7 Exercises 145
According to Proposition 3.6.3, assertions (3.6.10) and (3.6.11) imply that 
sup
y∈D
F (x∗, y) = F (x∗, y∗) = inf
x∈Rn F (x, y∗).
This is the saddlepoint property. 
Notice that, since y∗ ∈ D, x∗ minimizes the function 
x → F (x, y∗) = x · y∗ + σ|x|
2 − h(y∗)
over Rn. It follows that the gradient of F (., y∗) vanishes at x = x∗. We conclude 
that 
x∗ = − 1
2σ y∗.
This is the final property to be verified and the proof is complete. ⨅⨆
3.7 Exercises 
3.1 Use Ekeland’s theorem to prove Caristi’s fixed point theorem: 
‘Take a complete metric space V and a mapping f : V → V , such that 
dV (u, f (u)) ≤ φ(u) − φ(f (u)) for all u ∈ V
for some given lower semi-continuous, lower bounded function φ : V → R. Then 
there exists v¯ ∈ V such that f (v)¯ = ¯v.’ 
Hint: Apply Ekeland’s theorem (Theorem 3.3.1) to the function φ, with α = λ = 1 
2 , 
to find a point v such that φ(w) ≥ φ(v) − 1 
2 dV (v, w) for all w ∈ V . Use this 
inequality, when w = f (u), together with dV (u, f (u)) ≤ φ(u) − φ(f (u)), to show 
d(v, f (v)) = 0. 
3.2 Use Ekeland’s theorem to prove Takahashi’s minimization theorem: 
‘Let (X, d) be a complete metric space. Take a proper bounded below lower semi￾continuous function f : X → R ∪ {+∞}, which satisfies the following property: 
for every x ∈ dom f with f (x) > infx'
∈X f (x'
) there exists a point y ∈ dom f , 
y /= x, such that 
f (y) + d(x, y) ≤ f (x) .
Then f attains its minimum on X: there exists x¯ ∈ X such that f (x)¯ =
infx'
∈X f (x'
).’146 3 Variational Principles
3.3 Let (X, d) be a complete metric space. Take a function φ : X → X. The lower 
derivative of φ at x ∈ X in the direction of y ∈ X, written Dφ(x; y), is defined to 
be 
Dφ(x; y) := 
0 if y = x
lim infz→x,z∈(x,y) d(φ(z),φ(x))
d(z,x) otherwise ,
where (x, y) denotes the open interval between x and y: 
(x, y) := {z ∈ X : z /= x, z /= y for all d(x, z) + d(z, y) = d(x, y)} .
The map φ is called a weak directional contraction on X if φ is continuous and there 
exists a number σ ∈ [0, 1) such that 
Dφ(x; φ(x)) ≤ σ for all x ∈ X.
Show that every weak directional contraction on X has a fixed point. (This fixed 
point theorem is due to Clarke.) 
Hint: Consider the continuous function f : X → R+ defined to be f (x) :=
d(φ(x), x), for all x ∈ X. Apply Ekeland’s theorem (Theorem 3.3.1) to f and 
construct a sequence {xk}k≥1 in X such that Dφ(xk; φ(xk)) ≥ 1 − 1 
k , for all k ≥ 1. 
3.8 Notes for Chapter 3 
The important role of exact penalization in the derivation of necessary conditions 
for constrained optimization problems was early recognized by Clarke [60]. The 
proof of the exact penalization theorem, Theorem 3.2.1, is taken from [65], adapted 
to allow for an underlying space which is a (possibly incomplete) metric space in 
place of a Banach space. 
Ekeland’s variational principle was initially devised to show that approximate 
minimizers approximately satisfy necessary conditions of optimality [98]. Our proof 
is taken from [13]. An early application to dynamic optimization, which provided 
a pattern for later research into constrained optimization, was Clarke’s derivation 
of necessary conditions for nonsmooth dynamic optimization problems with end￾point constraints [59]. Since then the principle has been put to many and diverse 
uses in nonlinear analysis, some of which are described in [99]. The proofs of both 
the Borwein and Preiss and Stegall theorems (deferred to the appendix of Chap. 4), 
based on properties of quadratic inf convolutions in Hilbert spaces are those given 
by Clarke et al. [85]. 
The inf convolution technique, also referred to as Moreau-Yoshida regularization, 
was introduced in the 1960s, to approximate a convex function by a continuously 
differentiable function whose derivatives are related to those of the original function.3.8 Notes for Chapter 3 147
More recently, the properties of quadratic inf convolutions for non-convex problems 
have been used broadly in nonlinear analysis (including the proof of variational 
principles), the derivation of necessary conditions of optimality, Hamilton Jacobi 
analysis, Lyapunov theory and computational optimization. 
The father of mini-max results is von Neumann. Mini-max theorems appear, at 
first sight, rather specialized affairs, because they concern saddle properties of (from 
some perspectives) the rather narrow class of convex-concave functions. However 
they are powerful tools in non-convex optimization, because they can be used to 
convert the non-negativity property of the first variation (which can be interpreted as 
a one-sided mini-max property) into a multiplier rule. The centrepiece of Sect. 3.6, 
from this point of view, is Aubin’s one-sided mini-max theorem, which retains part 
of Von Neumann’s mini-max theorem under reduced hypotheses and which is well 
suited to such applications. We reproduce the proof in [8].Chapter 4 
Nonsmooth Analysis 
Abstract Local properties both of closed sets in topological vector spaces with 
smooth boundary and also of smooth functions on such spaces have traditionally 
been investigated via normal and tangent spaces (linear subspaces) and function 
derivatives (affine functions) respectively. This is no longer possible when the sets 
have nonsmooth boundaries and the functions are not differentiable in a traditional 
sense. Nonsmooth analysis provides techniques for the local approximation of 
closed sets with non-smooth boundaries and functions that are not differentiable. 
The key idea is to use, instead, cones and families of affine mappings in place 
of linear subspaces and affine mappings to achieve such approximations, in this 
more general setting. This and the following chapter provide a self-contained 
treatment of those aspects of nonsmooth analysis of special relevance to dynamic 
optimization. Key concepts are the limiting normal cone to a set (at a given base 
point) and the limiting subdifferential (at a point in the domain of the function). A 
number of approaches have been proposed. We follow Clarke in constructing the 
limiting normal cone as the cone comprising limits of proximal normal vectors at 
neighbouring points in the set. We work, for the most part, in the framework of 
real, finite dimensional vector spaces, though briefly describe how some concepts 
generalize to a Hilbert space setting. 
This chapter introduces different kinds of normal cones to a set. Each of 
these constructs gives rise to a related concept of subdifferential of a function, 
defined via the normal cone to the epigraph the function. We then provide useful 
‘finite difference’ representations of these subdifferentials and their asymptotic 
relatives. Properties of the limiting subdifferentials of locally Lipschitz functions 
are explored. Here, the distance function receives special attention. We also establish 
relations between the different kinds of normal and tangent cones that feature in the 
theory. 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
P. Bettiol, R. B. Vinter, Principles of Dynamic Optimization, Springer Monographs 
in Mathematics, https://doi.org/10.1007/978-3-031-50089-3_4
149150 4 Nonsmooth Analysis
4.1 Introduction 
Let x¯ ∈ Rk be a point in the manifold 
C := {x : gi(x) = 0 for i = 1, .., m}
in which gi : Rk → R, i = 1, .., m are given continuously differentiable functions 
such that ∇g1(x), . . . , ¯ ∇gm(x)¯ are linearly independent. Then the set of normal 
vectors to C at x¯ is 

m
i=1
λi∇gi(x)¯ : λ1,...,λm ∈ R

,
and its orthogonal complement (translated to x¯) 
x¯ + {y : y · ∇gi(x)¯ = 0 for i = 1, 2,...,m}
is an affine subspace of Rk which provides a local approximation to C ‘near’ x¯. 
If, on the other hand, we are given a continuously differentiable function f :
Rk → R and a point x¯, then 
x → f (x)¯ + ∇f (x)¯ · (x − ¯x)
is an affine function which approximates f near x¯. 
We see here how local approximations to smooth manifolds and functions 
are traditionally constructed: they take the form of affine subspaces for smooth 
manifolds and of affine functions for smooth functions. The importance of these 
approximations is that it is often possible to predict qualitative properties of smooth 
manifolds and functions from properties of their ‘affine’ approximations, which are 
in almost all cases simpler to investigate. A case in point is the inverse function 
theorem, which tells us that a continuously differentiable function f : Rk → Rk
is invertible on a neighbourhood of a point x¯ ∈ Rk if its affine approximation 
x → f (x)¯ + ∇f (x)¯ · (x − ¯x) is invertible. 
What general principles and techniques may be developed, governing the approx￾imation of sets and functions in topological vector spaces when the regularity and 
differentiability hypotheses underpinning traditional methods are violated? These 
are precisely the questions that nonsmooth analysis aims to answer. 
The key idea is to abandon the notion of affine approximation. In nonsmooth 
analysis, closed sets (and in particular manifolds) are approximated, not by affine 
subspaces, but by cones and functions are approximated, not by a single affine 
function, but by a family of affine functions. 
Once this idea is accepted, it is possible to extend to a nonsmooth setting many 
principles of traditional nonlinear analysis and to assemble a calculus of rules for 
estimating approximations to specific sets and functions of interest. There has been4.2 Normal Cones 151
a great deal of research activity in nonsmooth analysis since 1980s. A sophisticated 
and far-reaching theory has now been put together, the significance of which 
is confirmed by a growing body of applications in mathematical programming, 
variational analysis, dynamic optimization and other fields of applied nonlinear 
analysis. 
Our aim in this and the next chapter is to provide a self-contained treatment 
of those aspects of nonsmooth analysis of particular relevance to dynamic optimiza￾tion. In this chapter, basic constructs of nonsmooth analysis are defined and relations 
between them are explored. In the next, a ‘generalized calculus’ is developed. 
4.2 Normal Cones 
Take a closed set C ⊂ Rk and a point x ∈ C. We wish to give meaning to ‘normals’ 
to C at x, that is vectors which in some sense point out of C from the ‘basepoint’ 
x. We introduce three notions of normals: proximal normals, strict normals and 
limiting normals. 
Proximal normals are vectors which satisfy an outward pointing condition with 
quadratic error term. Proximal normals have a simple geometric interpretation and, 
for this reason, are often easy to characterize in specific applications. Strict normals 
are similarly defined, except an ‘order’ term replaces the quadratic error term in the 
defining condition. Limiting normals, as their name suggests, are limits of proximal 
normals at neighbouring basepoints; they can be equivalently defined as limits of 
strict normals at neighbouring basepoints, as we shall see. 
Limiting normals feature most prominently in the applications to optimization 
explored in this book, owing to their superior analytic properties. Many of these 
properties are consequences of the fact that the cone of limiting normals has closed 
graph, regarded as a set valued function of the basepoint. Proximal normals have 
the role of building blocks of limiting normals. Strict normals provide an alternative 
route to generating limiting normals via limit taking. 
Definition 4.2.1 Given a closed set C ⊂ Rk and a point x ∈ C, the proximal 
normal cone to C at x, written NP
C (x), is the set 
NP
C (x) := {p ∈ Rk : ∃ M > 0 such that condition (4.2.1) is satisfied}
p · (y − x) ≤ M|y − x|
2, for all y ∈ C. (4.2.1) 
Elements in NP
C (x) are called proximal normals to C at x. 
The defining condition (4.2.1) for a proximal normal, which is referred to as the 
‘proximal normal inequality for cones’, can be expressed as follows: there exists 
M > 0 such that 
|(x + (2M)−1p) − y| ≥ (2M)−1|p|, for all y ∈ C. (4.2.2)152 4 Nonsmooth Analysis
In terms of r = 2M and z = x + (2M)−1p, the condition amounts to: 
|z − x| = min{|z − y| : y ∈ C} and p = r(z − x).
These observations lead to the following geometric interpretation of proximal 
normals. 
Proposition 4.2.2 Take a closed set C ⊂ Rk and points x ∈ C and p ∈ Rk. Then p
is a proximal normal to C at x if and only if there exist a point z ∈ Rk and a scaling 
factor r > 0 such that 
|z − x| = min{|z − y| : y ∈ C} and p = r(z − x).
This proposition tells us that p is a non-zero proximal normal to C at x precisely 
when there is some z ∈ Rk lying outside C such that x is the closest point in C to 
z (with respect to the Euclidean distance function) and the vector p points in the 
direction (z − x). See Fig. 1.10. 
The derivation of many useful relations involving normal cones to a set C at x
involves limit taking with respect to the basepoint x. This is often not possible if 
the cones in question are proximal normal cones, because membership of proximal 
normal cones is not in general preserved under such operations. We can construct 
a normal cone from the proximal normal cone which does have the desired closure 
properties with respect to changes of the basepoint, by adding extra normals which 
are limits of proximal normals as nearby basepoints. In this way we arrive at the 
limiting normal cone. See Fig. 1.11. 
Definition 4.2.3 Given a closed set C ⊂ Rk and a point x ∈ C, the limiting normal 
cone to C at x, written NC(x), is the set 
NC(x) := 
p : there exists xi
C
→ x,pi → p such that pi ∈ NP
C (xi) for all i

Elements in NC(x) are called limiting normals to C at x. 
Another normal cone of interest is the strict normal cone: 
Definition 4.2.4 Given a closed set C ⊂ Rk and a point x ∈ C, the strict normal 
cone to C at x, written NˆC(x), is the set 
NˆC(x) =
⎧
⎨
⎩
p : lim sup
y C
→x
p · (y − x)
|y − x| ≤ 0
⎫
⎬
⎭ .
Elements in NˆC(x) are called strict normals to C at x. 
Otherwise expressed, a strict normal p to C at x is a vector satisfying 
p · (y − x) ≤ o(|y − x|) for all y ∈ C, (4.2.3)4.2 Normal Cones 153
for some function o : R+ → R+ such that limϵ↓0 o(ϵ)/ϵ = 0; in other words p
satisfies (to ‘within first order’) conditions asserting that it is an outward normal to 
a hyperplane supporting C at x. Notice that the defining relation (4.2.3) for strict 
normals is weaker than the defining relation (4.2.1) for proximal normals: for a 
strict normal p also to be a proximal normal, we stipulate that a special form of 
error modulus o(·) can be used, namely a quadratic function. It follows that 
NP
C (x) ⊂ NˆC(x).
Proximal normal cones and strict normal cones are distinct concepts: it is not 
difficult to supply examples in which 
NP
C (x)
strict
⊂ NˆC(x).
However limits of strict normals at basepoints converging to x generate the same 
set (NC(x)) as limits of proximal normals. It is for this reason that strict normals, 
in place of proximal normals, provide an alternative foundation on which to define 
and establish properties of limiting normal cones. 
Proposition 4.2.5 Take a closed set C ⊂ Rk and points x ∈ C and p ∈ Rk. Then 
the following assertions are equivalent: 
(i) p ∈ NC(x), 
(ii) there exist sequences xi
C
→ x and pi → p such that pi ∈ NˆC(xi) for all i. 
Proof (i) implies (ii), by the definition of NC(x) and since NP
C (x) ⊂ NˆC(x). 
To establish that (ii) implies (i), we show that strict normals can be suitably 
approximated by proximal normals. Fix q ∈ NˆC(y) for some y ∈ C. It suffices 
to demonstrate that sequences yi
C
→ y and qi → q may be found such that 
qi ∈ NP
C (yi) for all i. For then we can show, by constructing a suitable diagonal 
sequence, that if a vector p is the limit of a sequence of strict normals at 
neighbouring points, then it is also the limit of proximal normals; that is, it lies 
in the limiting normal cone. 
Take any ϵi ↓ 0. Then for each i, we can find a point yi which is a closest 
point to (y + ϵiq) in C. Since ϵiq → 0, we have that yi → y. For each i define 
qi := ϵ−1
i (y + ϵiq − yi). Then qi ∈ NC(yi). We see that qi = q + ϵ−1
i (y − yi), so 
the proof will be complete if we can show that ϵ−1
i (y − yi) → 0. However by the 
‘closest point’ property of yi, 
|(y + ϵiq) − yi|
2 ≤ |(y + ϵiq) − z|
2, for all z ∈ C.
Choosing z = y we arrive at 
0 ≥ |yi − (y + ϵiq)|
2 − ϵ2
i |q|
2 = |yi − y|
2 − 2ϵi(yi − y) · q.154 4 Nonsmooth Analysis
But q ∈ NˆC(y). It follows that 
|yi − y|
2 ≤ 2ϵi(yi − y) · q ≤ 2ϵi o(|yi − y|)
for some function o : R+ → R+ such that limϵ↓0 o(ϵ)/ϵ = 0. We conclude that 
ϵ−1
i |yi − y| → 0. ⨅⨆
Some elementary properties of the cones which have been introduced are now 
listed without proof. 
Proposition 4.2.6 Take a closed set C ⊂ Rk and a point x ∈ C. Then: 
(i) NP
C (x), NˆC(x) and NC(x) are all cones in Rk, containing {0} and 
NP
C (x) ⊂ NˆC(x) ⊂ NC(x),
(ii) NP
C (x) is convex (but possibly not closed), 
(iii) NˆC(x) is closed and convex, 
(iv) the set valued mapping NC(.) : C ⇝ Rk has closed graph, in the sense that, 
for any sequences yi
C
→ y and pi → p such that pi ∈ NC(yi) for all i, we 
have p ∈ NC(x). 
We note also 
Proposition 4.2.7 Take a closed set C ⊂ Rk and a point x ∈ C. Then: 
(i) x ∈ int{C} implies NC(x) = {0} and hence 
NP
C (x) = NˆC(x) = {0}.
(ii) x ∈ bdy{C} implies NC(x) contains non-zero elements. 
Proof (i) Suppose x ∈ int{C}. It follows from Proposition 4.2.2 that all proximal 
normals at points in C near to x are zero. So NC(x) = {0}. 
(ii) Assume x ∈ bdy{C}. Then there exists a sequence xi → x such that xi /∈ C
for all i. For each i select a closest point ci to xi in C. Then xi − ci /= 0 and 
ξi := |xi −ci|
−1(xi −ci) is a proximal normal to C at ci, of unit norm. But |x−ci| ≤
|x − xi|+|xi − ci| ≤ 2|x − xi|. So ci
C
→ x. Along a subsequence then ξi → ξ . The 
vector ξ has unit norm and belongs to ξ ∈ NC(x). ⨅⨆
We note also 
Proposition 4.2.8 Take closed subsets C1 ⊂ Rm and C2 ⊂ Rn, and a point 
(x1, x2) ∈ C1 × C2. Then 
NP
C1×C2
(x1, x2) = NP
C1
(x1) × NP
C2
(x2)
NˆC1×C2 (x1, x2) = NˆC1 (x1) × NˆC2 (x2)
NC1×C2 (x1, x2) = NC1 (x1) × NC2 (x2).4.2 Normal Cones 155
Proof The first two relations are direct consequences of the definitions of proximal 
normals and strict normals. The last relation is a consequence of the first relation and 
the manner in which limiting normals are obtained as limits of proximal normals. 
⨅⨆
In the convex case, the normal cones above all coincide with the normal cone in 
the sense of convex analysis. 
Proposition 4.2.9 Take a closed, convex set C ⊂ Rk and a point x¯ ∈ C. Then 
NP
C (x)¯ = NˆC(x)¯ = NC(x)¯
= {ξ : ξ · (x − ¯x) ≤ 0 for all x ∈ C}.
Proof For y ∈ C write 
S(y) := {ξ : ξ · (y' − y) ≤ 0 for all y' ∈ C}.
Since the conditions for membership of S(x)¯ are more severe than those for 
membership of NP
C (x)¯ , we know that 
S(x)¯ ⊂ NP
C (x). ¯
Take an arbitrary element ξ ∈ NP
C (x)¯ . By definition, there exists M ≥ 0 such that 
ξ · (x' − ¯x) ≤ M|x' − ¯x|
2,
for all x' ∈ C. Choose any x ∈ C. Since C is convex, x' = ϵx + (1 − ϵ)x¯ ∈ C for 
any ϵ ∈ (0, 1). Inserting this choice of x' into the above inequality, dividing across 
by ϵ and passing to the limit as ϵ ↓ 0 gives 
ξ · (x − ¯x) ≤ 0.
It follows that ξ ∈ S(x)¯ . This establishes that S(x)¯ ⊃NP
C (x)¯ . We conclude that 
S(x)¯ = NP
C (x). ¯ (4.2.4) 
Now, take sequences xi
C
→ ¯x and ξi → ξ such that ξi ∈ S(xi) for all i. It is easy to 
deduce that ξ ∈ S(x)¯ . It follows from (4.2.4) and the definition of NC(x)¯ that 
NC(x)¯ ⊂ S(x). ¯
The remaining relations in the proposition to be proved follow from Proposi￾tion 4.2.6 (i). ⨅⨆156 4 Nonsmooth Analysis
4.3 Subdifferentials 
Consider a function f : Rk → R and a point x ∈ Rk. Suppose that f is C2 (twice 
continuously differentiable). Then the gradient ∇f (x) of f at x is the unique vector 
ξ ∈ Rk such that, for some M > 0 and ϵ > 0, 
|f (y) − f (x) − ξ · (y − x)| ≤ M|y − x|
2 for all y ∈ x + ϵB. (4.3.1) 
This description evokes the familiar ‘difference quotient’ characterization of the 
gradient. The fact that we assume f is a C2 function permits us to use a quadratic 
error term on the right side. 
An alternative description of ∇f (x) is in terms of the epigraph of f , epi f , 
epi f := {(x, α) ∈ Rk × R : α ≥ f (x)}.
It can be shown that ∇f (x) is the unique vector ξ ∈ Rk such that, for some M ≥ 0, 
(ξ , −1) · ((y, α) − (x, f (x))) ≤ M(|y − x|
2 + |α − f (x)|
2) (4.3.2) 
for all (y, α) ∈ epi f . (By considering a first order Taylor expansion of f around x, 
with quadratic remainder term, we can show that, for some M > 0, this inequality 
is satisfied ‘locally’ when ξ = ∇f (x); the ‘extra’ error term |α − f (x)|
2 ensures 
that the inequality holds for all y, not just y’s near x). To see that this last condition 
does indeed identify ∇f (x), we note that (y, f (y)) ∈ epi f . Hence 
0 ≤ lim inf
y→x
f (y) − f (x) − ξ · (y − x)
|y − x|
= lim inf
y→x

f (y) − f (x) − ∇f (x) · (y − x)
|y − x| +
∇f (x) − ξ ) · (y − x)
|y − x|

= 0 + lim inf
y→x
(∇f (x) − ξ ) · (y − x)
|y − x|
= −|∇f (x) − ξ |.
It follows that ξ = ∇f (x). 
The reasons for introducing relation (4.3.2) may not be immediately apparent. 
All becomes clear however when it is interpreted in terms of normal cones: (4.3.2) 
can be equivalently stated 
(ξ , −1) ∈ NP
epi f (x, f (x)), (4.3.3) 
where, we recall, NP
epi f (x, f (x)) denotes the proximal normal cone. 
Up to this point, we have discussed different ways of looking at the gradient 
of a C2 function f . Our ulterior motive of course has been to come up with4.3 Subdifferentials 157
some description which makes sense when f is no longer ‘differentiable’ in the 
conventional sense. 
Two routes are open to us, to generalize either (4.3.1) (the analytical approach) 
or (4.3.3) (the geometric approach). As they stand, conditions (4.3.1) and (4.3.3) 
are often rather restrictive: even for Lipschitz continuous functions f there will be 
many points x at which the sets of ξ ’s satisfying either (4.3.1) or (4.3.3) will be 
empty. It is therefore often helpful to play variations on these themes, namely to 
consider limits of ξi’s satisfying (4.3.1) along a sequence of points xi → x, or to 
replace the quadratic error term M|y − x|
2 by an error modulus o(|y − x|) (if we 
are going down the analytical road), or to insert different kinds of normal cones in 
place of NP
epi f (x, f (x)) (if we want to take a more geometric approach). 
Different choices are available regarding what we deem to be definitions and 
what to be consequences of the definitions. The theory would appear to unfold most 
simply when the basic definitions are given in terms of normal cones to epigraphs, 
and this therefore is our chosen approach. 
To what class of functions should we aim to generalize the traditional notion of 
derivatives? The class of lower semi-continuous functions f : Rk → R ∪ {+∞}
is a natural choice here. This is because generalized derivatives will be defined in 
terms of normal cones to the epigraph set epi f , so we would like to arrange that the 
epigraph set is closed. The class of lower semi-continuous functions is precisely the 
class of functions with closed epigraph sets. 
Definition 4.3.1 Take a lower semi-continuous function f : Rk → R ∪ {+∞} and 
a point x ∈ dom f := {y ∈ Rk : f (y) < +∞}. 
(i) The proximal subdifferential of f at x, written ∂P f (x), is the set 
∂P f (x) := {ξ : (ξ , −1) ∈ NP
epi f (x, f (x))}.
Elements in ∂P f (x) are called proximal subgradients. 
(ii) The strict subdifferential of f at x, written ∂f (x) ˆ , is the set 
∂f (x) ˆ := {ξ : (ξ , −1) ∈ Nˆ
epi f (x, f (x))}.
Elements in ∂f (x) ˆ are called strict subgradients. 
(iii) The limiting subdifferential of f at x, written ∂f (x), is the set 
∂f (x) := {ξ : (ξ , −1) ∈ Nepi f (x, f (x))}.
Elements in ∂f (x) are called limiting subgradients. 
Proximal subgradients and limiting subdifferentials are illustrated in Fig. 1.12 
and Fig. 1.13 respectively.158 4 Nonsmooth Analysis
Notice that, since 
NP
C (x) ⊂ NˆC(x) ⊂ NC(x),
we have 
∂P f (x) ⊂ ∂f (x) ˆ ⊂ ∂f (x).
These different subdifferentials provide local information about a function f near a 
point x. The most noteworthy departure from classical real analysis is that, for lower 
semi-continuous functions, the subdifferentials are in general set valued. 
Example 
In each of the following cases, f is a function from R to R. 
(i) f (y) = |y| and x = 0. Then 
∂P f (x) = ∂f (x) ˆ = ∂f (x) = [−1, +1].
(ii) f (y) = −|y| and x = 0. Then 
∂P f (x) = ∂f (x) ˆ = ∅ and ∂f (x) = {−1} ∪ {+1}.
(iii) f (y) = |y|
1/2 and x = 0. Then 
∂P f (x) = ∂f (x) ˆ = ∂f (x) = (−∞, +∞).
(iv) f (y) = sgn{y}|y|
1/2 and x = 0. Then 
∂P f (x) = ∂f (x) ˆ = ∂f (x) = ∅.
Unbounded or empty subdifferentials (as occur in (iii) and (iv)) give warning 
that the slopes of the function on an arbitrary small ϵ-ball about the base point 
are unbounded. In the presence of such pathologies, we would like to know the 
direction of these arbitrarily large slopes. Information of this nature is conveyed 
by some new constructs; they are the ‘asymptotic’ analogues of the subdifferentials 
already defined. 
Definition 4.3.2 Take a lower semi-continuous function f : Rk → R ∪ {+∞} and 
a point x ∈ dom f . 
(i) The asymptotic proximal subdifferential of f at x, written ∂∞
P f (x), is 
∂∞
P f (x) := {ξ : (ξ , 0) ∈ NP
epi f (x, f (x))}.
Elements in ∂∞
P f (x) are called asymptotic proximal subgradients.4.3 Subdifferentials 159
(ii) The asymptotic strict subdifferential of f at x, written ∂ˆ∞f (x), is 
∂ˆ∞f (x) := {ξ : (ξ , 0) ∈ Nˆ epi f (x, f (x))}.
Elements in ∂ˆ∞f (x) are called asymptotic strict subgradients. 
(iii) The asymptotic limiting subdifferential of f at x, written ∂∞f (x), is 
∂∞f (x) := {ξ : (ξ , 0) ∈ Nepi f (x, f (x))}.
Elements in ∂∞f (x) are called asymptotic limiting subgradients. 
Example 
Consider the function f : R → R given by f (y) = sgn{y}|y|
1/2 and the point 
x = 0. As we have noted ∂f (x) = ∅. However ∂∞f (x) = [0,∞). The calculation 
of ∂∞f (x) has revealed to us that the ‘infinite’ slopes near x are positive. 
The asymptotic limiting subdifferential ∂∞f (x) is of interest also because of 
the information that non-zero asymptotic limiting subgradients give about ‘non￾Lipschitz behaviour’ of a function f near a point x ∈ dom f . It is to be expected 
then that the asymptotic limiting subdifferential of a Lipschitz continuous function 
is the trivial set {0}. 
Proposition 4.3.3 Take a lower semi-continuous function f : Rk → R ∪ {+∞}
and a point x¯ ∈ Rk. Assume that f is Lipschitz continuous on a neighbourhood of 
x¯ with Lipschitz constant K. Then: 
(i) ∂f (x)¯ ⊂ KB, 
(ii) ∂∞f (x)¯ = {0}. 
Proof Take a point 
(ξ , −λ) ∈ Nepi f (x,f ( ¯ x)). ¯
Then there exist convergent sequences 
(xi, αi)
epi f → (x,f ( ¯ x)) ¯ and (ξi, −λi) → (ξ , −λ)
such that 
(ξi, −λi) ∈ NP
epi f (xi, αi) for all i sufficiently large,
and such that f is Lipschitz continuous on a neighbourhood of xi with Lipschitz 
constant K. ⨅⨆
We know that αi = f (xi)+βi for some βi ≥ 0. It follows now from the definition 
of proximal subgradients that160 4 Nonsmooth Analysis
ξi · (x − xi) − λi(β + f (x) − βi − f (xi))
≤ M(|x − xi|
2 + |β − βi + f (x) − f (xi)|
2)
for all β ≥ 0 and x ∈ Rn. 
Setting x = xi we deduce 
− λi(β − βi) ≤ M|β − βi|
2 for all β ≥ 0.
This implies λi ≥ 0. Passing to the limit as i → ∞, we obtain λ ≥ 0. 
Setting β = βi we deduce from the Lipschitz continuity of f that 
ξi · (x − xi) ≤ λiK|x − xi| + (M + MK2)|x − xi|
2
for all x in a neighbourhood of xi. Choosing x = xi + rξi for r > 0 sufficiently 
small gives 
r|ξi|
2 ≤ rKλi|ξi| + M(1 + K2)r2|ξi|
2.
Dividing across by r and passing to the limit as r ↓ 0 gives 
|ξi|
2 ≤ Kλi|ξi|.
We deduce that 
|ξi| ≤ Kλi for each i.
Passing to the limit as i → ∞, we arrive at 
|ξ | ≤ Kλ.
It follows that ∂∞f (x)¯ = {0} and ∂f (x)¯ ⊂ KB. ⨅⨆
A converse of the above proposition is proved later in the chapter (Theorem 4.9.1 
and Corollary 4.9.2). 
Limiting subgradients and asymptotic limiting subgradients are defined in terms 
of the limiting normal cone to the epigraph set. Taking a reverse view, we can 
represent the limiting normal cone to the epigraph of a function by means of the 
subgradients of the function. 
Proposition 4.3.4 Take a lower semi-continuous function f : Rk → R ∪ {+∞}
and a point x ∈ dom f . Then: 
Nepi f (x, f (x)) = {(λξ , −λ) : λ > 0, ξ ∈ ∂f (x)} ∪ (∂∞f (x) × {0}).
Either ∂f (x) is non-empty or ∂∞f (x) contains non-zero elements.4.3 Subdifferentials 161
Proof The first part of the proposition merely describes how the limiting subdif￾ferential and asymptotic limiting subdifferential are constructed from the limiting 
normal cone. Since (x, f (x)) is a boundary point of epi f , Nepi f (x, f (x)) contains 
non-zero elements (see Proposition 4.2.7). The fact that either ∂f (x) is non-empty 
or ∂∞f (x) contains non-zero elements is an immediate consequence. ⨅⨆
The following ‘closure’ properties of the limiting subdifferential and the 
asymptotic limiting subdifferential follow from their representation in terms of 
Nepi f (x, f (x)) and from the fact that the set valued function Nepi f : epi f ⇝
Rk × R has closed graph. 
Proposition 4.3.5 Take a lower semi-continuous function f : Rk → R ∪ {+∞}
and a point x ∈ dom f . Then: 
(i) ∂f (x) is a closed set. Given sequences xi
f
→ x and ξi → ξ such that ξi ∈
∂f (xi) for all i, then ξ ∈ ∂f (x), 
(ii) ∂∞f (x) is a closed cone. Given sequences xi
f
→ x and ξi → ξ such that 
ξi ∈ ∂∞f (xi) for all i, then ξ ∈ ∂∞f (x). 
Finally we note that, for convex functions, the definitions of proximal, strict and 
limiting subdifferential all coincide with the subdifferential in the sense of convex 
analysis. 
Proposition 4.3.6 Take a lower semi-continuous, convex function f : Rk → R ∪
{+∞} and a point x¯ ∈ dom f . Then 
∂P f (x)¯ = ∂f( ˆ x)¯ = ∂f (x)¯
= {ξ : ξ · (x − ¯x) ≤ f (x) − f (x)¯ for all x ∈ Rk}.
Proof Define 
D(x)¯ = {ξ : ξ · (x − ¯x) ≤ f (x) − f (x)¯ for all x ∈ Rk}.
It is easy to see that D(x)¯ can be alternatively expressed in terms of the normal cone 
(in the sense of convex analysis) to epi f at (x,f ( ¯ x)¯ : 
D(x)¯ = {ξ : (ξ , −1) · ((x, α) − (x,f ( ¯ x))) ¯ ≤ 0 for all (x, α) ∈ epi f }.
All the assertions of the proposition now follow from Proposition 4.2.9 and the 
definitions of the proximal, strict and limiting normal cones. ⨅⨆162 4 Nonsmooth Analysis
4.4 Difference Quotient Representations 
Subgradients have been defined, somewhat indirectly, via normals to epigraph 
sets. This approach has certain advantages from the point of view of unifying the 
treatment of normal vectors and subgradients and of deriving a number of important 
properties of subgradients. This section provides alternative conditions for vectors to 
be proximal subgradients (and strict subgradients), in terms of limits of difference 
quotients. These often simplify the task of investigating the detailed properties of 
subgradients in specific cases. 
We start with a very useful characterization of proximal subgradients. 
Proposition 4.4.1 Take a lower semi-continuous function f : Rk → R∪{+∞} and 
points x ∈ dom f and ξ ∈ Rk. Then the following two statements are equivalent: 
(i) ξ ∈ ∂P f (x), 
(ii) there exist M > 0 and ϵ > 0 such that 
ξ · (y − x) ≤ f (y) − f (x) + M|y − x|
2 (4.4.1) 
for all y ∈ x + ϵB. 
The significance of condition (ii) (which is referred to as the proximal normal 
inequality for functions) is the absence of the term M|f (y) − f (x)|
2 from the right 
side of inequality (4.4.1), a term which you would expect to be required to provide 
a ‘difference quotient’ characterization of proximal subgradient. In situations when 
f is not continuous this troublesome error term complicates the analysis of limits 
of proximal normal vectors. Notice that (4.4.1) is required to hold only locally. 
Proof Assume (ii). Inequality (4.4.1) implies 
ξ · (y − x) ≤ α − f (x) + M[|y − x|
2 + |α − f (x)|
2]
for any (y, α) ∈ epi f ∩ [(x, f (x)) + ϵB]. The inequality can be written 
(ξ , −1) · ((y, α) − (x, f (x))) ≤ M|(y, α) − (x, f (x))|
2. (4.4.2) 
The geometric interpretation of this relation is that the open ball, with cen￾tre (x, f (x)) + (2M)−1(ξ , −1) and of radius (2M)−1|(ξ , −1)|, is disjoint from 
epi f ∩ [(x, f (x)) + ϵB], and the closure of this ball contains (x, f (x)). (See 
Proposition 4.2.2 and preceding comments). So for M˜ ≥ M sufficiently large, when 
we substitute M˜ in place of M, this open ball is contained in (x, f (x)) + ϵ
◦
B, and 
is consequently disjoint from epi f . (We denote by y + r
◦
B the open ball in Rk with 
centre y and of radius r > 0.) Inequality (4.4.2), which is valid when M is replaced 
by the larger M˜ , then tells that4.4 Difference Quotient Representations 163

(x, f (x)) + (2M) ˜ −1(ξ , −1) + (2M) ˜ −1|(ξ , −1)|B

∩ epi f = ∅.
Again appealing to the geometric interpretation of proximal normals, we deduce 
that (ξ , −1) ∈ NP
epi f (x, f (x)). We have shown that (i) is true. 
Assume (i). We may suppose that (x, f (x)) = (0, 0) (this amounts simply to 
translating the origin in Rk × R to (x, f (x))). 
By assumption there exists M > 0 such that 
(ξ , −1) · (y, α) ≤ M(|y|
2 + |α|
2) for all (y, α) ∈ epi f .
Set λ = (2M)−1. 
According to Proposition 4.2.2 and the preceding discussion (0, 0) + {s + ρ ◦
B}
is disjoint from epi f for 
s = (s1, s2) := (λξ , −λ) and ρ := λ

1 + |ξ |
2.
For each x ∈ s1 + ρ
◦
B consider the vertical line {x} × R. This penetrates 
the ball s + ρ
◦
B. Define the functionals φ+(x) and φ−(x) to take values the 
vertical coordinates of the points at which this line intercepts the upper and lower 
hemispheres respectively. 
For each x ∈ s1 + ρ
◦
B, φ+(x) > φ−(x) and consequently 
{x} × (φ−(x), φ+(x))
is a non-empty subset of s + ρ
◦
B. But {x}×[f (x),∞) ⊂ epi f . Since epi f and 
s + ρ
◦
B are disjoint, we conclude that 
f (x) ≥ φ+(x) for all x ∈ s1 + ρ
◦
B. (4.4.3) 
In particular this inequality holds for all x in the neighbourhood ϵ'
◦
B of the origin, 
in which ϵ' is the positive number 
ϵ' :=
1
2
(λ
1 + |ξ |
2 − λ|ξ |).
A simple calculation yields the following formula for φ+(x): 
φ+(x) =

λ2 + λ2|ξ |
2 − |x − λξ |
2 − λ164 4 Nonsmooth Analysis
for x ∈ ϵ'
◦
B. φ+(x) is analytic on ϵ'
◦
B. The gradient ∇φ+(0) and Hessian ∇2φ+(0)
at the origin are 
∇φ+(0) = ξ
∇2φ+(0) = −λ−1(I + ξ ξ T ).
Since λ = (2M)−1 and φ+(0) = 0, we have 
φ+(x) = φ+(0) + ∇φ+(0) · x +
1
2
x ·

∇2φ+(0)

(x) + o(x)
= ξ · x − M|x|
2 − M|ξ · x|
2 + o(x). (4.4.4) 
Here, o is some function which satisfies limx→0 |o(x)|/|x|
2 = 0. Now take any 
ϵ ∈ (0, ϵ'
) such that 
|o(x)| < M(1 + |ξ |
2)|x|
2, for all x ∈ ϵ
◦
B.
It follows then from (4.4.3) and (4.4.4) that 
ξ · x ≤ f (x) + M|x|
2 + M|ξ · x|
2 − o(x)
≤ f (x) + M(1 + |ξ |
2)|x|
2 + M(1 + |ξ |
2)|x|
2
= f (x) + M˜ |x|
2, for all x ∈ ϵ
◦
B,
where M˜ := 2M(1 + |ξ |
2). This is what we set out to prove. ⨅⨆
We note for future use a variant on the preceding characterization of proximal 
normals, valid in the Lipschitz case. 
Proposition 4.4.2 Take a Lipschitz continuous function f : Rk → R and points 
x ∈ Rk and ξ ∈ Rk. Then the following two statements are equivalent: 
(i) ξ ∈ ∂P f (x), 
(ii) there exists M > 0 such that 
ξ · (y − x) ≤ f (y) − f (x) + M|y − x|
2 (4.4.5) 
for all y ∈ Rk. 
The difference, of course, is that, now, the ‘proximal inequality’ (4.4.5) must be 
satisfied for all y’s in Rk, not just y’s in a neighbourhood of x. 
Proof Assume (ii). Then (i) follows from Proposition 4.4.1. 
Assume (i). Then we know from Proposition 4.4.1 that there exist M > 0 and ϵ > 0
such that (4.4.5) is valid for all y ∈ x + ϵB. Let K be a Lipschitz constant for f . 
Define N := ϵ−1(|ξ | + K). Then4.4 Difference Quotient Representations 165
ξ · (y − x) − (f (y) − f (x)) ≤ |ξ ||y − x| + K|y − x| ≤ N|y − x|
2 if y /∈ x + ϵB.
It follows that (4.4.5) is satisfied for all y when M is replaced by M˜ := max{M,N}. 
⨅⨆
The next result provides a ‘one-sided difference quotient’ characterization of the 
strict subdifferential. 
Proposition 4.4.3 Take a lower semi-continuous functions function f : Rk → R ∪
{+∞} and points x ∈ dom f and ξ ∈ Rk. Then the following two statements are 
equivalent: 
(i) ξ ∈ ∂f (x), ˆ
(ii) 
lim sup
y→x
ξ · (y − x) − (f (y) − f (x))
|y − x| ≤ 0. (4.4.6) 
Reference is made in the proof to the Bouligand tangent cone TC(x) to a closed 
set C ⊂ Rk at the point x ∈ C, defined as 
TC(x) := {η : there exists yi
C
→ y and ti ↓ 0 such that t
−1
i (yi − x) → η},
whose properties will be explored later in the chapter. 
Proof Assume (ii). Condition (4.4.6) can be expressed: 
ξ · (y − x) − (f (y) − f (x)) ≤ o(|y − x|) for all y ∈ Rk,
in which o(·) : R+ → R+ is a function satisfying o(ϵ)/ϵ → 0 as ϵ ↓ 0. It follows 
that 
(ξ , −1) · (z − (x, f (x))) ≤ o(|z − (x, f (x))|) for all z ∈ epi f.
Otherwise expressed 
(ξ , −1) ∈ Nˆ epi f (x, f (x)),
which implies (i). 
Assume now (i). Suppose that condition (4.4.6) is not satisfied. This means that 
there exists a number α > 0 and a sequence yi → x such that yi /= xi and 
ξ · (yi − x) − (f (yi) − f (x)) ≥ α|yi − x|, for all i. (4.4.7) 
This inequality can be rearranged to give 
f (yi) ≤ f (x) + ξ · (yi − x) − α|yi − x|, for all i. (4.4.8)166 4 Nonsmooth Analysis
We see that lim supi→∞ f (yi) ≤ f (x). Since f is lower semi-continuous, it follows 
that f (yi) → f (x) as i → ∞. 
Define zi := (yi, f (yi)), ti := |zi − (x, f (x))| and ηi := t
−1
i (zi − (x, f (x))). 
Observe that, for each i, |ηi| = 1. Following extraction of subsequences we have 
that ηi → η for some non-zero vector η. Notice that zi
epi f → (x, f (x)) and ti → 0. 
By definition of the Bouligand tangent cone then η ∈ Tepi f (x, f (x)). 
We claim that 
(ξ , −1) · η > 0.
If this is true the proof will be complete because, according to Theorem 4.10.4 
below, the strict normal cone Nˆ epi f (x, f (x)) is contained in the polar set of the 
tangent cone Tepi f (x, f (x)). This implies (ξ , −1) · η ≤ 0, a contradiction. 
To verify the claim, we make use of the following identity, which follows from 
the definition of ηi: 
(ξ , −1) · ηi = t
−1
i (ξ · (yi − x) − (f (yi) − f (x))). (4.4.9) 
Now (4.4.8) implies that 
lim sup
i→∞
|yi − x|
−1(f (yi) − f (x)) < ∞.
There are therefore two possibilities to be considered: 
(a) there exists K > 0 such that 
f (yi) − f (x) ≤ K|yi − x| for all i , (4.4.10) 
(b) along a subsequence 
lim
i→∞ |yi − x|
−1(f (yi) − f (x)) = −∞.
Assume (a) is true. By (4.4.8), (4.4.9) and (4.4.10), we have: 
(ξ , −1) · ηi ≥ t
−1
i α|yi − x| =
α|yi − x|
|(yi, f (yi)) − (x, f (x))|
≥ (1 + K2)
−1/2α > 0.
Since ηi → η, we must have (ξ , −1) · η > 0. 
Finally assume (b) is true. (Along the subsequence) t
−1
i |yi − x| → 0 and 
t
−1
i (f (yi)−f (x)) → −1. But then by (4.4.9), (ξ , −1)· η = limi(ξ , −1)· ηi = +1. 
This confirms the claim in case (b) also. □4.5 Nonsmooth Mean Value Inequalities 167
4.5 Nonsmooth Mean Value Inequalities 
We pause to establish some consequences of the preceding theory, namely general￾izations of the classical mean value theorem. The results of this section will be used, 
both to carry out a deeper investigation of subdifferentials, normal cones, etc. and 
also to provide information about solutions to optimization problems. 
It will be convenient to use the following notation. Given a point x¯ ∈ Rk and a 
subset Y ⊂ Rk, we denote by [ ¯x,Y ] the set 
[ ¯x,Y ] := {x : x = λx¯ + (1 − λ)y for some y ∈ Y and λ ∈ [0, 1]}.
(x,Y) ¯ denotes the related set, in which the qualifier ‘λ ∈ [0, 1]’ is replaced by 
‘λ ∈ (0, 1)’. 
If Y = {¯y} we write [ ¯x, y¯] in place of [ ¯x,Y ]. 
The classical mean value theorem tells us that if f : Rn → R is a C1 function 
on an open set containing the line segment [ ¯x, y¯] then, for some z¯ ∈ [¯x, y¯], 
∇f (z)¯ · (y¯ − ¯x) = f (y)¯ − f (x). ¯
We might ask whether there is an analogue of this result covering situations 
where, say, f is a Lipschitz continuous function, expressed in terms of a limiting 
subgradient. The answer to this question is no, as is evident from the following 
example. 
Example 
Define the function f : R → R to be 
f (x) = −|x|
and set x¯ = −1 and y¯ = +1. A simple calculation shows that 
∂f (x) ⊂ {−1} ∪ {+1} for all x ∈ R.
It follows that, for any z ∈ R and any ξ ∈ ∂f (z), 
0 = f (y)¯ − f (x)¯ /∈ ξ · (y¯ − ¯x).
The above example rules out a direct generalization of the mean value theorem 
involving limiting subgradients, to allow for the function f to be nonsmooth. We do 
notice however in this example that the inequality 
f (y)¯ − f (x)¯ ≤ ξ · (y¯ − ¯x)
is satisfied for some ξ ∈ ∂f (z)¯ with z¯ ∈ [¯x, y¯]. Possible choices are z¯ = −0.5 and 
ξ = +1.168 4 Nonsmooth Analysis
It is the ‘inequality’ version of the mean value theorem which may be extended 
to a nonsmooth setting. Nonsmooth mean value inequalities have many uses, as an 
aid to proving fundamental relations in nonsmooth analysis and in the analysis of 
solutions to optimization problems. The role of these theorems is usually to provide 
some upper bound and the fact that they take the form only of an inequality is seldom 
a significant shortcoming. 
The mean value inequality proved here is due to Clarke and Ledyaev [71]. 
Besides allowing the function concerned to be nonsmooth, it departs from the 
classical mean value theorem in one other respect. It is to assert that the inequality 
involved holds in some uniform sense. 
In the setup for this mean value inequality, the line segment [ ¯x, y¯] is replaced by 
the ‘generalized’ line segment joining a point x¯ to a compact, convex set Y ⊂ Rk, 
namely [ ¯x,Y ]. A natural extension of the smooth mean value inequality would be: 
for each y ∈ Y there exists z¯ ∈ [¯x,Y ], where 
[ ¯x,Y ] := {ϵx + (1 − ϵ)y : ϵ ∈ [0, 1] and y ∈ Y },
and ξ ∈ ∂f (z)¯ such that 
f (y) − f (x)¯ ≤ ξ · (y − ¯x).
What comes as a surprise is that, if we replace the left side by 
min
y∈Y f (y) − f (x), ¯
then the inequality is valid whatever choice is made of y in Y for the same ξ (a 
limiting subgradient of f evaluated at some point in [ ¯x,Y ]). This result, when first 
proved, was new even for smooth functions. 
The main step is to prove the following ‘approximate’ mean value inequality 
involving proximal subgradients. 
Theorem 4.5.1 (The Proximal Mean Value Inequality) Take a lower semi￾continuous function f : Rn → R ∪ {+∞}, a point x¯ ∈ dom f and a compact, 
convex set Y ⊂ Rn. Define rˆ ∈ R ∪ {+∞} to be 
rˆ := inf
y∈Y f (y) − f (x). ¯
Then for any finite r < rˆ and ϵ > 0, we can choose z¯ ∈ [¯x,Y ]+ϵB and ξ ∈ ∂P f (z)¯
such that 
r<ξ · (y − ¯x) for all y ∈ Y
and 
f (z)¯ − f (x)¯ ≤ max{r, 0}.4.5 Nonsmooth Mean Value Inequalities 169
Proof By translation of the origin in Rn × R we can arrange that x¯ = 0 and f (x)¯ =
0. Fix r < rˆ and ϵ > 0. We deduce from the compactness of Y and the lower 
semicontinuity of f that positive numbers δ, M and k may be chosen such that 
0 <δ<ϵ and 
f (z) ≥ −M for all z ∈ [0, Y ] + δB ,
min
y∈Y+δB
f (y) ≥ (r + ˆr)/2
and 
k > (M + |r| + 1)/δ2.
The proof hinges on examining the properties of the function g : [0, 1]×Rn×Rn →
R ∪ {+∞}: 
g(t, y, z) := f (z) + k|ty − z|
2 − tr.
Write 
S := Y × ([0, Y ] + δB).
Notice that, for any y ∈ Y , 
(0, y, 0) ∈ [0, 1] × S and g(0, y, 0) = 0.
Since S is compact and g is lower semi-continuous, the minimization problem 
Minimize g(t, y, z) over (t, y, z) ∈ [0, 1] × S
has a solution (t ,¯ y,¯ z)¯ and 
min
(t,y,z)∈[0,1]×S
g(t, y, z) ≤ 0.
This relation implies that 
g(t ,¯ y,¯ z)¯ ≤ 0, (4.5.1) 
and so 
f (z)¯ − max{0, r} ≤ 0.
In particular, f (z) < ¯ +∞. Let us show that 
z¯ ∈ [0, Y ] + δ int B.170 4 Nonsmooth Analysis
If z¯ does not satisfy this condition then |¯z − t
¯y¯| ≥ δ, because t
¯y¯ ∈ [0, Y ]. But then 
g(t ,¯ y,¯ z)¯ = f (z)¯ + k|t
¯y¯ − ¯z|
2 − tr > ¯ −M + (M + |r| + 1) − |r| > 1.
We have contradicted (4.5.1). 
We note next that 
t <¯ 1. (4.5.2) 
This is because if t
¯ = 1, it would follow that 
g(t ,¯ y,¯ z)¯ = f (z)¯ + k| ¯y − ¯z|
2 − r
≥

f (z)¯ + k| ¯y − ¯z|
2 − r if | ¯y − ¯z| ≤ δ
f (z)¯ + kδ2 − r if | ¯y − ¯z| > δ
≥ min{(r + ˆr)/2 − r, −M + M + |r| + 1 − r} > 0.
But then, in view of (4.5.1), (t ,¯ y,¯ z)¯ cannot be a minimizer. We deduce from this 
contradiction the validity of (4.5.2). 
We know that, for any z in the open set [0, Y ] + δ int B, 
g(t ,¯ y, z) ¯ ≥ g(t ,¯ y,¯ z). ¯
Using the identity 
|t
¯y¯ − z|
2 − |t
¯y¯ − ¯z|
2 = −2(t
¯y¯ − ¯z) · (z − ¯z) + |z − ¯z|
2,
we can write the preceding inequality as 
2k(t
¯y¯ − ¯z) · (z − ¯z) ≤ f (z) − f (z)¯ + k|z − ¯z|
2.
This holds, in particular, for all points z in some neighbourhood of z¯. Define 
ξ = 2k(t
¯y¯ − ¯z).
According to Proposition 4.4.1, 
ξ ∈ ∂P f (z). ¯
Two cases now need to be considered 
Case (a): t
¯ = 0. Here ξ = −2kz¯. Choose t ∈ (0, 1]. For any y ∈ Y we have 
0 ≤ g(t, y, z)¯ − g(0, y,¯ z)¯
= f (z)¯ + k|ty − ¯z|
2 − tr − f (z)¯ − k|¯z|
2
= t
2k|y|
2 − 2ktz¯ · y − tr.4.5 Nonsmooth Mean Value Inequalities 171
Dividing across the inequality by t and passing to the limit as t ↓ 0 we obtain 
ξ · y = −2kz¯ · y ≥ r.
This is the required relation. 
Case (b): t
¯ ∈ (0, 1). The fact that y¯ minimizes the quadratic, convex function 
y → g(t, y, ¯ z)¯ over the convex set Y implies 
2tk( ¯ t
¯y¯ − ¯z) · (y − ¯y) ≥ 0, for all y ∈ Y.
Since t >¯ 0, it follows that 
ξ · (y − ¯y) ≥ 0.
But t
¯ minimizes the quadratic function t → g(t, y,¯ z)¯ over (0, 1) so 
2k(t
¯y¯ − ¯z) · ¯y − r = 0.
It follows that ξ · ¯y = r. We conclude that 
ξ · y ≥ r.
The theorem is proved. ⨅⨆
Performing a simple exercise in limit taking now yields an ‘exact’ mean value 
inequality for Lipschitz functions in terms of limiting subdifferentials. 
Theorem 4.5.2 (The Lipschitz Case) Take a lower semi-continuous function f :
Rn → R ∪ {+∞}, a point x¯ ∈ Rn and a compact, convex set Y ⊂ Rn. Assume that, 
for some δ > 0, f is Lipschitz continuous on [ ¯x,Y ] + δB. 
Then there exists z¯ ∈ [¯x,Y ] and ξ ∈ ∂f (z)¯ such that 
min
y∈Y f (y) − f (x)¯ ≤ ξ · (y − ¯x) for all y ∈ Y
and 
f (z)¯ − f (x)¯ ≤ max{min
y∈Y f (y) − f (x), ¯ 0}.
Proof Choose sequences ϵi ↓ 0 and ri ↑ ˆr, where 
rˆ := min
y∈Y f (y) − f (x). ¯172 4 Nonsmooth Analysis
Now apply Theorem 4.5.1 with ϵ and r taken to be ϵi and ri respectively for i =
1, 2,... . For each i, we deduce the existence of some zi ∈ [¯x,Y ] + ϵiB and ξi ∈
∂P f (zi) such that 
ri < ξi · (y − ¯x) for all y ∈ Y (4.5.3) 
and 
f (zi) − f (x)¯ ≤ max{ ri, 0}. (4.5.4) 
{zi} and {ξi} are bounded sequences because Y is compact and f is Lipschitz 
continuous on [ ¯x,Y ] + δB (see Proposition 4.3.3). We may arrange then by 
extracting subsequences that zi → ¯z and ξi → ξ for some z¯ ∈ [¯x,Y ] and some 
ξ ∈ Rn. It follows now from Proposition 4.3.5 that ξ ∈ ∂f (z)¯ . The property that ξ
and z¯ satisfy the desired inequalities is now recovered from (4.5.3) and (4.5.4), in 
the limit as i → ∞. ⨅⨆
If we specialize to the case Y = {¯y} and substitute the convexified limiting 
subdifferential co ∂f in place of ∂f , then a full (two sided) mean value theorem 
is valid. This result was earlier proved by Lebourg. To prove it, we make use of 
properties of limiting subgradients, derived in the next chapter. 
Theorem 4.5.3 (A Two Sided Mean Value Theorem) Take a locally Lipschitz 
continuous function f : Rn → R and a line segment [ ¯x, y¯] ⊂ Rn. Then there 
exists z ∈ {ϵx¯ + (1 − ϵ)y¯ : 0 <ϵ< 1} and ξ ∈ co ∂f (z) such that 
f (y)¯ − f (x)¯ = ξ · (y¯ − ¯x).
Proof Consider the function g : R → R ∪ {+∞} defined by 
g(t) := f (x¯ + t (y¯ − ¯x)) − t (f (y)¯ − f (x)) ¯ + Ψ[0,1](t),
in which Ψ[0,1] denotes the indicator function of the set [0, 1]. (Ψ(t)[0,1] takes the 
value 0 or ∞ depending on whether, or not, t ∈ [0, 1].) Notice that g(0) = g(1) =
f (x)¯ . So either of the following conditions are satisfied: 
(a): g has a minimizer at some ‘interior’ point τ ∈ (x,¯ y)¯ , 
(b): −g has a minimizer at some ‘interior’ point σ ∈ (x,¯ y)¯ . 
Here (x,¯ y)¯ denotes the relative interior of [ ¯x, y¯]. 
Consider case (a). We deduce from the proximal normal inequality that 0 ∈
∂P g(τ ). It follows from the chain rule (see Chap. 5) that 
0 = ξ · (y¯ − ¯x) − (f (y)¯ − f (x)) ¯
for some ξ ∈ ∂f (x¯ + τ (y¯ − ¯x)) as required.4.6 Characterization of Limiting Subgradients 173
Consider case (b). The chain rule applied to −g now gives 
0 = ξ · (y¯ − ¯x) + (f (y)¯ − f (x)) ¯
for some ξ ∈ ∂(−f )(σ ). Since, however, 
co∂f (x) = −co∂(−f (x))
(see Theorem 4.7.5) we conclude that 
0 = ξ ' · (y¯ − ¯x) − (f (y)¯ − f (x)) ¯
for some ξ ' ∈ ∂f (σ ) in this case also. ⨅⨆
4.6 Characterization of Limiting Subgradients 
We now seek an analytic description of limiting subgradients (and their asymptotic 
relatives). Our goal is to describe these objects in terms of limits of proximal 
subgradients, for which an analytic characterization is already available. 
The fact that limiting subgradients can be expressed as limits of proximal sub￾gradients at neighbouring points is a straightforward consequence of the definition 
of limiting subgradients in terms of limiting normal vectors to the epigraph set and 
the property that limiting normal vectors are limits of proximal normal vectors. This 
is part (a) of Theorem 4.6.2 below. 
It is easy also to show that asymptotic limiting subgradients are expressible 
as limits of vectors which (apart from a positive scale factor) are either proximal 
subgradients or asymptotic proximal subgradients. There is, however, a more con￾venient description of asymptotic limiting subgradients which involves sequences 
of proximal subgradients alone. This is part (b) of Theorem 4.6.2. It has important 
implications because the validity of various subdifferential calculus rules centres 
on the properties of asymptotic subdifferentials of the functions involved, and any 
results which restrict the possible ways in which asymptotic limiting subdifferentials 
can arise simplify the analysis of pathological phenomena. 
As a first step we prove a lemma, whose conclusions may be summarized 
‘proximal normals to an epigraph set can be approximated by non-horizontal 
proximal normals’. 
Lemma 4.6.1 Take a lower semi-continuous function f : Rk → R∪{+∞}, a point 
x¯ ∈ dom f and a non-zero point ξ ∈ Rk such that 
(ξ , 0) ∈ NP
epi f (x,¯ α), ¯174 4 Nonsmooth Analysis
in which (x,¯ α)¯ ∈ epi f . Then there exist convergent sequences xi
f
→ ¯x, ξi → ξ and 
a sequence of positive numbers λi ↓ 0 such that 
(ξi, −λi) ∈ NP
epi f (xi, f (xi)) for all i.
Use is made in the proof of properties of the distance function derived later in 
the chapter. (See Lemmas 4.8.3 and 4.8.4 and Theorem 4.8.5 below.) 
Proof To begin the proof, we show that we can restrict attention to the case when 
α¯ = f (x)¯ . Indeed, since (ξ , 0) is a non-zero ‘horizontal’ proximal normal, there 
exist a point (x, α)¯ /∈ epi f and a number σ > 0 such that (x,¯ α)¯ is a closest point 
in epi f to (x, α)¯ and ξ = σ (x − ¯x). We can arrange that (x,¯ α)¯ is the unique closest 
point, and also that there exists some β > 0 such that (x,¯ α)¯ is the closest point in 
epi f to the point 
(x¯ + (1 + β)(x − ¯x), α)¯ = (x¯ + σ −1(1 + β)ξ , α). ¯
This can be achieved by replacing x by a point on the ‘open’ line segment joining 
x and x¯ and by increasing σ appropriately. Therefore, for all y ∈ dom f and a ≥ 0
we have 
|(y, f (y)+a)−(x¯+σ −1(1+β)ξ, α)¯ |≥|(x,¯ α)¯ −(x¯+σ −1(1+β)ξ, α)¯ | ≥ σ −1(1+β)|ξ |.
Taking a = (α¯ − f (x)) ¯ + a' in the above inequality, we obtain 
|(y, f (y)+a'
)−(x¯+σ −1(1+β)ξ , f (x)) ¯ | ≥ σ −1(1+β)|ξ |
for all y ∈ dom f, a' ≥ 0.
In view of the proximal normal characterization provided by Proposition 4.2.2, it 
follows that (ξ , 0) ∈ NP
epi f (x,f ( ¯ x)) ¯ . We have confirmed our claim. 
We may now deduce from Lemmas 4.8.3, 4.8.4 and Theorem 4.8.5 the following 
properties. 
First, 
(|ξ |
−1ξ , 0) ∈ ∂P depi f (x, f (x)). ¯
Second, if for some sequences (yi, ri) → (x, f (x)) ¯ and {(ξ '
i, −λ'
i)} in Rn × R
(ξ '
i, −λ'
i) ∈ ∂P depi f (yi, ri) for all i, (4.6.1) 
then there exists (xi, si), a unique closest point to (yi, ri) in epi f , such that 
(ξ '
i, −λ'
i) ∈ ∂P depi f (xi, si) for all i (4.6.2) 
(xi, si) → (x,f ( ¯ x)) ¯ (4.6.3)4.6 Characterization of Limiting Subgradients 175
and 
(ξ '
i, −λ'
i) → (|ξ |
−1ξ , 0). (4.6.4) 
We shall show presently that 
depi f (x, f (x)¯ − t) > depi f (x, f (x)) ¯ for all t > 0. (4.6.5) 
Assuming this to be the case, we take a sequence ti ↓ 0 and, for each i, apply the 
proximal mean value inequality (Theorem 4.5.1) to the function depi f with base 
point {(x, f (x)) ¯ } and ‘Y set’ {(x, f (x)¯ − ti)} . The theorem tells us that there exist 
sequences (yi, ri) → (x, f (x)) ¯ and {(ξ '
i, −λ'
i)} in Rn × R satisfying (4.6.1) and, 
for all i, 
0 < (ξ '
i, −λ'
i) · ((x, f (x)¯ − ti) − (x, f (x))). ¯
This inequality implies that λ'
i > 0 for all i. It follows from (4.6.2) that in fact 
si = f (xi). Equation (4.6.3) may therefore be replaced by xi
f
→ ¯x. 
Now define 
(ξi, −λi) := (|ξ |ξ '
i, −|ξ |λ'
i) for each i.
Note that, since ξ /= 0, λi > 0 for each i. We have from (4.6.2) and Proposition 4.8.2 
that 
(ξi, −λi) ∈ NP
epi f (xi, f (xi)) for each i
and, in view of (4.6.4) 
(ξi, −λi) → (ξ , 0) as i → ∞.
The sequences {xi},{ξi} and {λi} therefore have the properties asserted in the lemma. 
It remains to confirm (4.6.5). If it is untrue, there exists t > 0 such that 
depi f (x, f (x)¯ − t) ≤ depi f (x, f (x)). ¯
We may find (y, r) ∈ Rn × R with r ≥ f (y) such that 
depi f (x, f (x)¯ − t) = |(x, f (x)¯ − t) − (y, r)|.
Notice that y /= ¯x. Indeed if this were not the case then 
|(x, f (x)¯ − t) − (x, r) ¯ |=|(x − ¯x, (f (x)¯ − t − r))| > |x − ¯x| = depi f (x, f (x)), ¯176 4 Nonsmooth Analysis
since t > 0 and r ≥ f (x)¯ , in contradiction of our premise. It follows that 
depi f (x, f (x)) ¯ ≥ depi f (x, f (x)¯ − t)
= |(x, f (x)¯ − t) − (y, r)|=|(x, f (x)) ¯ − (y, r + t)|.
But (y, r +t) ∈ epi f since t ≥ 0. It follows that (y, r +t) is a closest point in epi f
to (x, f (x)) ¯ with y /= ¯x. This is impossible because (x,f ( ¯ x)) ¯ is the unique closest 
point. ⨅⨆
It is now a simple matter to prove: 
Theorem 4.6.2 (Asymptotic Description of Limiting Subgradients) Take a 
lower semi-continuous function f : Rk → R ∪ {+∞} and points x ∈ dom f
and ξ ∈ Rk. 
(a) The following conditions are equivalent: 
(i) ξ ∈ ∂f (x), 
(ii) there exist xi
f
→ x and ξi → ξ such that ξi ∈ ∂P f (xi) for all i. 
(b) The following conditions are equivalent: 
(iii) ξ ∈ ∂∞f (x), 
(iv) there exist xi
f
→ x, ti ↓ 0 and ξi → ξ such that t
−1
i ξi ∈ ∂P f (xi) for all i. 
Proof (a) Assume (i). Then there exist sequences (ξ '
i, −ti) → (ξ , −1) and zi
epi f →
(x, f (x)) such that (ξ '
i, −ti) ∈ NP
epi f (zi) for all i. We may assume ti > 0 for all 
i. We deduce from the proximal inequality that zi = (xi, f (xi)) for all i and hence 
xi
f
→ x. Define ξi := t
−1
i ξ '
i . Then ξi ∈ ∂P f (xi) for each i. Since ξi → ξ as 
i → ∞, (ii) follows. 
Assume (ii). For the given sequences {xi} and {ξi}, we have that (ξi, −1) ∈
NP
epi f (xi, f (xi)) for each i and (xi, f (xi)) → (x, f (x)) as i → ∞. It follows from 
the closure properties of the limiting normal cone that (ξ , −1) ∈ Nepi f (x, f (x). 
But then ξ ∈ ∂f (x). 
(b) Assume (iii). Suppose to begin with that ξ /= 0. By Lemma 4.6.1, there exist 
sequences (ξ '
i, −ti) → (ξ , 0) and xi
f
→ x such that ti > 0 and (ξ '
i, −ti) ∈
NP
epi f (xi, f (xi)) for each i. Define ξi := t
−1
i ξ '
i for each i. Then (ξi, −1) ∈
NP
epi f (xi, f (xi)), whence ξi ∈ ∂f (xi) for each i. We have shown (iv). 
Suppose on the other hand that ξ = 0. Since x ∈ dom f , it follows from Propo￾sition 4.2.7 that there exists (ξ '
, −β) /= (0, 0) such that (ξ '
, −β) ∈ Nepi f (x, f (x)). 
Since limiting normals are limits of proximal normals and proximal normals to 
epigraph sets have second coordinate a non-negative number, β ≥ 0. Whether or 
not β = 0, we deduce from part (a) of Theorem 4.6.2 or part (b) (in the case ξ ' /= 0) 
that there exist sequences xi
f
→ x, ξ '
i → ξ ' and βi → β such that4.7 Subgradients of Lipschitz Continuous Functions 177
βi > 0 and β−1
i ξ '
i ∈ ∂P f (xi) for all i.
Choose a sequence αi ↓ 0 such that 
αiβi → 0 as i → ∞.
Now set ξi = αiξ '
i , ti = αiβi for each i. 
We see that 
ξi → ξ(= 0), ti ↓ 0 and t
−1
i ξi ∈ ∂P f (xi) for all i.
We have shown (iv) in this case also. 
Suppose (iv). For the given sequences {xi}, {ti} and {ξi}, we have (ξi, −ti) ∈
NP
epi f (xi, f (xi)) for all i. Since (ξi, −ti) → (ξ , 0) and (xi, f (xi)) epi f → (x, f (x)), 
it follows (ξ , 0) ∈ NP
epi f x, f (x). This is (iii). ⨅⨆
A companion piece to the above theorem is a characterization of limiting 
subgradients and asymptotic limiting subgradients in terms of limits of strict (rather 
than proximal) subgradients. 
Theorem 4.6.3 Take a lower semi-continuous functions function f : Rk → R ∪
{+∞} and points x ∈ dom f and ξ ∈ Rk. 
(a) The following conditions are equivalent: 
(i) ξ ∈ ∂f (x), 
(ii) there exist xi
f
→ x and ξi → ξ such that ξi ∈ ∂f (x ˆ i), for all i. 
(b) The following conditions are equivalent: 
(iii) ξ ∈ ∂∞f (x), 
(iv) there exists xi
f
→ x, ti ↓ 0 and ξi → ξ such that t
−1
i ξi ∈ ∂f (x ˆ i) for all i. 
The most significant assertions in this theorem are the implications (i) ⇒ (ii) and 
(iii) ⇒ (iv); but these follow directly from Theorem 4.6.2 since proximal normals 
are certainly strict normals. The reverse implications are routine consequences of 
the fact that limiting normal vectors to epi f are limits of neighbouring strict normal 
vectors (see Proposition 4.2.5). 
4.7 Subgradients of Lipschitz Continuous Functions 
The techniques so far assembled provide local descriptions of Lipschitz continuous 
functions, since they are special cases of lower semi-continuous functions. But 
Lipschitz continuous functions are encountered so frequently in applications of178 4 Nonsmooth Analysis
nonsmooth analysis that it is important to exploit the extra structure they introduce 
and alternative approaches to local approximation. 
The Lipschitz continuity hypothesis is essentially a hypothesis that the function 
in question has bounded slope. It is hardly surprising then that the subdifferentials 
of Lipschitz continuous functions are bounded sets, a fact which follows easily from 
the preceding analysis. 
Proposition 4.7.1 Take a lower semi-continuous function f : Rk → R ∪ {+∞}
and a point x ∈ Rk. Assume that f is Lipschitz continuous on a neighbourhood of 
x with Lipschitz constant K. Then: 
(i) ∂f (x) is non-empty and ∂f (x) ⊂ KB, 
(ii) ∂∞f (x) = {0}. 
Proof Take ξ and y such that ξ ∈ ∂P f (y) and f is Lipschitz continuous on a 
neighbourhood of y. Then there exist M > 0 and ϵ > 0 such that 
ξ · (z − y) ≤ f (z) − f (y) + M|z − y|
2 for all z ∈ y + ϵB.
It follows that, for all λ > 0, sufficiently small 
λ|ξ |
2 ≤ f (y + λξ ) − f (y) + Mλ2|ξ |
2 ≤ λK|ξ | + Mλ2|ξ |
2.
Dividing across by λ and passing to the limit gives 
|ξ |
2 ≤ K|ξ |.
We conclude that |ξ | ≤ K. This shows that for all y sufficiently close to x, 
ξ ∈ ∂P f (y) implies |ξ | ≤ K.
The assertions of the proposition now follow from Theorem 4.6.2. ⨅⨆
The above proposition is a special case of more far-reaching results, covered by 
Corollary 4.9.2 below. 
We now examine an alternative approach to defining subdifferentials of Lip￾schitz continuous functions, based on convex approximations and duality ideas. 
It is important partly because of the insights it gives into the relation between 
subdifferentials of nonsmooth functions and their counterparts in convex analysis, 
but also because the approach will provide new representations of subdifferentials 
which are extremely useful in applications. 
The starting point is the definition of the (Clarke) generalized directional 
derivative of a locally Lipschitz continuous function. 
Definition 4.7.2 Take a function f : Rk → R and points x ∈ Rk and v ∈ Rk. 
Assume that f is Lipschitz continuous on a neighbourhood of x. The generalized 
directional derivative of f at x in the direction v, written f 0(x, v), is the number4.7 Subgradients of Lipschitz Continuous Functions 179
f 0(x, v) := lim sup
y→x,t↓0
t
−1[f (y + tv) − f (y)].
Notice that, because f is assumed merely to be Lipschitz continuous, the right 
side would not make sense if ‘lim’ replaced ‘limsup’. 
We list some salient properties of f 0(x, v): 
Proposition 4.7.3 Take a function f : Rk → R and a point x ∈ Rk. Assume that 
f is Lipschitz continuous on a neighbourhood of x with Lipschitz constant K. Then 
the function v → f 0(x, v) with domain Rk has the following properties: 
(i) it is finite valued, Lipschitz continuous with Lipschitz constant K, and positively 
homogeneous, in the sense that 
f 0(x, αv) = αf 0(x, v) for all v ∈ Rk and α ≥ 0,
(ii) it is convex. 
Proof (i) These properties are straightforward consequences of the definition of f 0
and the assumed Lipschitz continuity of f (near x). 
(ii) To show convexity of f 0(x, ·), take any v, w ∈ Rk. Since f 0(x, ·) is 
positively homogeneous, it suffices to show ‘sub-additivity’, namely f 0(x, v+w) ≤
f 0(x, v) + f 0(x, w). We know that there exist sequences yi → x and ti ↓ 0 such 
that 
f 0(x, v + w) = lim
i
{t
−1
i (f (yi + ti(v + w)) − f (yi))}.
But the term between braces on the right can be expressed 
t
−1
i (f (zi + tiv) − f (zi)) + t
−1
i (f (yi + tiw) − f (yi)),
in which zi = yi + tiw. Since zi → x, we conclude 
f 0(x, v + w) ≤ lim sup
z→x,t↓0
{t
−1(f (z + tv) − f (z))}
+ lim sup
y→x,t↓0
{t
−1(f (y + tw) − f (y))}
= f 0(x, v) + f 0(x, w),
as claimed. ⨅⨆
We have constructed a convex function f 0(x, ·) which approximates f ‘near’ x. 
What would be more natural then than to introduce a ‘subdifferential’, we write it 
∂f (x) ¯ , which is the subdifferential in the sense of convex analysis of the convex 
function v → f 0(x, v) at v = 0? Since f 0(x, 0) = 0, this approach gives180 4 Nonsmooth Analysis
∂f (x) ¯ := {ξ : f 0(x, v) ≥ ξ · v for all v ∈ Rk}.
∂f (x) ¯ is called the Clarke subdifferential (alternatively referred to as the Clarke 
generalized gradient) of f at x. ∂f (x) ¯ is a non-empty, compact, convex set (for 
fixed x). Elements in ∂f (x) ¯ are uniformly bounded in Euclidean norm by the 
Lipschitz constant of f on a neighbourhood of x. 
The generalized directional derivative can be interpreted as the support function 
of ∂f (x) ¯ : 
Proposition 4.7.4 Take a function f : Rk → R which is Lipschitz continuous on a 
neighbourhood of a point x ∈ Rk. Then 
f 0(x, v) = max{v · ξ : ξ ∈ ∂f (x) ¯ } for all v ∈ Rk.
Proof We have f 0(x, v) ≥ max{v · ξ : ξ ∈ ∂f (x) ¯ } for all v, by definition of ∂f (x) ¯ . 
Fix any v. It remains to show equality for this v. Choose ξ to be a subgradient (in 
the sense of convex analysis) to f 0(x, ·) at v. Then 
f 0(x, w) − f 0(x, v) ≥ ξ · (w − v) for all w ∈ Rk.
Take w = αw¯ for arbitrary w¯ ∈ Rk and α > 0. Since f 0(x, w) = αf 0(x, w)¯ , we 
may divide across the inequality by α and pass to the limit as α → ∞. This gives 
f 0(x, w)¯ ≥ ξ · ¯w. We conclude that ξ ∈ ∂f (x) ¯ . On the other hand, setting w = 0, 
we obtain ξ · v ≥ f 0(x, v). It follows f 0(x, v) = max{v · ξ : ξ ∈ ∂f (x) ¯ }. ⨅⨆
The Clarke subdifferential commutes with −1. 
Proposition 4.7.5 Take a function f : Rk → R which is Lipschitz continuous on a 
neighbourhood of a point x ∈ Rk. Then 
∂(¯ −f )(x) = −∂f (x). ¯
Proof It suffices to show that the two sets have the same support function, which 
(as we now know) are (−f )0(x, v) and f 0(x, −v) for any v. But 
(−f )0(x, v) = lim sup
x'
→x,t↓0
(−f )(x' + tv) − (−f )(x'
)
t ,
which, following the substitution y' = x' + tv, becomes 
(−f )0(x, v) = lim sup
y'
→x,t↓0
f (y' − tv) − f (y'
)
t = f 0(x, −v).
This is the required relation. ⨅⨆4.7 Subgradients of Lipschitz Continuous Functions 181
Note that assertions of Proposition 4.7.5 are in general false if the limiting 
subdifferential is substituted for the Clarke subdifferential. This point is illustrated 
by functions (i) and (ii) in Example 4.3. 
How does the Clarke subdifferential ∂f (x) ¯ fit in with our other nonsmooth 
constructs? It relates very neatly to the limiting subdifferential ∂f (x). 
Proposition 4.7.6 Take a lower semi-continuous functions function f : Rk → R
which is Lipschitz continuous on a neighbourhood of some point x ∈ Rk. Then 
∂f (x) ¯ = co ∂f (x).
Proof Since the two sets are closed convex sets, it suffices to show that they have 
the same support function, i.e. to show f 0(x, v) = max{v · ξ : ξ ∈ ∂f (x)} for all 
v ∈ Rk. 
(a): We show f 0(x, v) ≥ max{v · ξ : ξ ∈ ∂f (x)} for all v ∈ Rk. 
Choose any ξ ∈ ∂f (x) and any v ∈ Rk. Then there exist sequences xi
f
→ x and 
ξi → ξ such that ξi ∈ ∂P f (xi) for each i, by Theorem 4.6.2. We know that for each 
i there exist Mi > 0 and δi > 0 such that 
f (x) − f (xi) ≥ ξi · (x − xi) − Mi|x − xi|
2 for all x ∈ xi + δiB.
It follows that we can find a sequence ti ↓ 0 such that 
f (xi + tiv) − f (xi) ≥ tiξi · v − i
−1ti for all i.
Then 
f 0(x, v) ≥ lim sup
i→∞
t
−1
i (f (xi + tiv) − f (xi)) ≥ ξ · v.
Since ξ and v were arbitrary, we have 
f 0(x, v) ≥ max{v · ξ : ξ ∈ ∂f (x)} for all v ∈ Rk.
(b): We show f 0(x, v) ≤ max{v · ξ : ξ ∈ ∂f (x)} for all v ∈ Rk. 
Pick an arbitrary v ∈ Rk. Then sequences xi → x and ti ↓ 0 can be found such 
that 
f 0(x, v) = lim
i→∞ t
−1
i (f (xi + tiv) − f (xi)).
The mean value inequality (Theorem 4.5.2) applied to the function f with basepoint 
xi and ‘Y set’ {xi + tiv} for each i yields the following information: for each i there 
exists zi ∈ xi + ti|v|B and ξi ∈ ∂f (zi) such that 
f (xi + tiv) − f (xi) ≤ tiξi · v. (4.7.1)182 4 Nonsmooth Analysis
Clearly zi → x. Because f is Lipschitz continuous on a neighbourhood of x, the 
ξi’s are uniformly bounded. Along a subsequence then ξi → ξ , for some ξ ∈ Rk. 
By Theorem 4.6.2, ξ ∈ ∂f (x). Dividing across (4.7.1) by ti and passing to the limit 
gives 
f 0(x, v) ≤ ξ · v for some ξ ∈ ∂f (x).
We conclude that 
f 0(x, v) ≤ max{v · ξ : ξ ∈ ∂f (x)} for all v.
⨅⨆
Consider now a function f : Rk → R and a point x ∈ Rk such that f
is Lipschitz continuous on a neighbourhood of x. According to Rademacher’s 
theorem, f is differentiable almost everywhere on this neighbourhood (with respect 
to k-dimensional Lebesgue measure). Can we construct limiting subgradients 
of f at x as limits of neighbouring derivatives? The answer to this question, 
supplied by the following theorem, is a qualified yes: the convex hull of the set 
of limits of neighbouring derivatives coincides with the convex hull of the limiting 
subdifferential at x. The same is true, even if we exclude neighbouring derivatives 
on some specified subset of measure zero. This important theorem is of intrinsic 
interest (it relates classical and modern concepts of derivative in a very concrete 
way), but also provides a computational tool for the convex hull of the limiting 
subdifferential of great power. Often the simplest approach to calculating co ∂f (x), 
by far, is to look at limits of neighbouring derivatives. 
When a function f : Rk → R is Lipschitz continuous on a neighbourhood of x, 
the concepts of Gâteaux, Hadamard and Fréchet differentiability all coincide. For 
concreteness, when we say that a Lipschitz continuous function f is differentiable 
at x, we shall mean Gâteaux differentiable, i.e. there is a unique vector ξ ∈ Rk, the 
Gâteaux derivative ∇f (x) of f at x, such that 
lim
h↓0
h−1(f (x + hv) − f (x) − h ξ · v) = 0 for all v ∈ Rk. (4.7.2) 
Notice that if f is differentiable at x (and Lipschitz continuous on a neighbourhood 
of x) then 
∇f (x) ∈ co ∂f (x).
This is true since (4.7.2) (with ∇f (x) substituted in place of ξ ) implies f 0(x, v) ≥
ξ · v for every v, whence (by Proposition 4.7.6) ∇f (x) ∈ co ∂f (x). 
Theorem 4.7.7 Take a function f : Rk → R, a point x ∈ Rk and any subset 
Ω ⊂ Rk having Lebesgue measure zero. Assume that f is Lipschitz continuous on 
a neighbourhood of x. Then4.7 Subgradients of Lipschitz Continuous Functions 183
co ∂f (x) = co{ξ : ∃ xi → x, xi /∈ Ω, ∇f (xi) exists and ∇f (xi) → ξ }.
Proof We know that y ⇝ ∂f (y) has closed graph on a neighbourhood of x (see 
Proposition 4.3.5). However the vectors in ∂f (y) are uniformly bounded as y ranges 
over this neighbourhood, as may be deduced from Theorem 4.7.1. It follows from 
Carathéodory’s theorem that the set valued function y ⇝ co ∂f (y) has closed graph 
on a neighbourhood of x. But, we have observed, if f is differentiable at a point y
near x, that ∇f (y) ∈ co ∂f (y). It follows that S ⊂ co ∂f (x), where S is the set on 
the right side in the theorem statement. 
Both the set S and co ∂f (x) are compact, convex sets and, as we have shown, 
S ⊂ co ∂f (x). So it remains to establish that the support functions of the two sets 
satisfy the inequality: for any v /= 0
sup{ξ · v : ξ ∈ ∂f (x)} ≤ sup{ξ · v : ξ ∈ S}.
Choose an arbitrary vector v /= 0. In view of Proposition 4.7.4, it suffices to show 
that 
f 0(x, v) ≤ sup{ξ · v : ξ ∈ S}.
In fact we need only demonstrate that 
f 0(x, v) ≤ α, (4.7.3) 
where the number α is defined to be 
α := lim sup{∇f (y) · v : f is differentiable at y, y /∈ Ω, y → x}.
Choose any ϵ. Denote by S the subset of Rk on which f fails to be differentiable. 
By definition of α, there exists δ > 0 such that 
y ∈ x + δB and y /∈ Ω ∪ S implies ∇f (y) · v ≤ α + ϵ. (4.7.4) 
We suppose that δ > 0 has been chosen small enough that x + δB lies in the 
neighbourhood on which f is Lipschitz continuous. By Rademacher’s theorem then, 
Ω ∪ S has k dimensional Lebesgue measure zero in x + δB. For each y ∈ Rk, 
consider the line segments Ly := {y + tv : 0 < t < δ/(2|v|)}. Since Ω ∪ S has k 
dimensional Lebesgue measure 0 in x + δB, it follows from Fubini’s theorem that, 
for almost every y in x + (δ/2)B, the line segment Ly intersects Ω ∪ S on a set 
of zero one-dimensional Lebesgue measure. Take y to be any point in x + (δ/2)B
having this property, and take any t ∈ (0, δ/(2|v|)). Then 
f (y + tv) − f (y) =
 t
0
∇f (y + sv) · v ds.184 4 Nonsmooth Analysis
This formula is valid because f is Lipschitz continuous on 
{y + sv : 0 ≤ s ≤ t}.
Since y + sv ∈ x + δB and y + sv /∈ Ω ∪ S for almost all s ∈ (0,t), we conclude 
from (4.7.4) that 
f (y + tv) − f (y) ≤ t (α + ϵ).
This inequality holds for all y ∈ x + (δ/2)B, excluding a subset of measure zero 
and for all t ∈ (0, δ/(2|v|)). But then, since f is continuous, it is true for all y ∈
x + (δ/2)B, t ∈ (0, δ/(2|v|)). This implies that 
f 0(x, v) = lim sup
y→x,t↓0
t
−1[f (y + tv) − f (y)] ≤ α + ϵ.
Since ϵ > 0 was arbitrary, we conclude (4.7.3). The proof is complete. ⨅⨆
4.8 The Distance Function 
Take a set C ⊂ Rk. The distance function dC : Rk → R is 
dC(x) := inf{|x − y| : y ∈ C}.
The distance function features prominently in nonsmooth analysis. This is not 
surprising since a key building block is the proximal normal, whose very definition 
revolves around the concept of ‘closest point’ (a point which achieves the infimum 
in the definition of the distance function). 
Chapter 3 has already provided a foretaste of the distance function’s significance. 
Take a set A in Rn and a Lipschitz continuous function f : A → R. Take also a 
subset C ⊂ A. Then, for the ‘exact penalty’ parameter K chosen sufficiently large, 
a minimizer for the constrained optimization problem 
Minimize f (x) over points x ∈ A satisfying x ∈ C
is a minimizer also for the unconstrained problem 
Minimize f (x) + KdC(x) over points x ∈ A.
This is a consequence of the exact penalization theorem (Theorem 3.2.1), which 
permits us to trade off the complications associated with a constraint against an 
extra, nonsmooth, term in the cost.4.8 The Distance Function 185
Another important property of dC is that it provides a description of a set C in 
terms of a Lipschitz continuous function. 
Proposition 4.8.1 For any set C ⊂ Rk, the distance function dC : Rk → R is 
Lipschitz continuous with Lipschitz constant 1. 
Proof Take any x, y ∈ Rk. Choose ϵ > 0. By definition of dC, there exists z ∈ C
such that dC(y) ≥ |y − z| − ϵ. Then 
dC(x) ≤ |x − z|≤|x − y|+|y − z|≤|x − y| + dC(y) + ϵ.
But ϵ > 0 was arbitrary. So dC(x) − dC(y) ≤ |x − y|. Since the roles of x and y
are interchangeable, we conclude |dC(x) − dC(y)|≤|x − y|, which implies dC is 
Lipschitz continuous with Lipschitz constant 1. ⨅⨆
Our goal now is to describe the limiting subgradients of dC at a point in C. As 
usual, we start by studying proximal normals. 
Proposition 4.8.2 Take a closed set C ⊂ Rk and a point x ∈ C. Then 
∂P dC(x) = NP
C (x) ∩ B.
Proof 
(a): We show ∂P dC(x) ⊂ NP
C (x) ∩ B. 
Suppose ξ ∈ ∂P dC(x). Then, since dC is Lipschitz continuous with Lipschitz 
constant 1, by Proposition 4.4.2, there exists M > 0 such that 
ξ · (y − x) ≤ dC(y) − dC(x) + M|y − x|
2 for all y ∈ Rk.
Since dC vanishes on C, we have 
ξ · (y − x) ≤ M|y − x|
2 for all y ∈ C.
But this implies ξ ∈ NP
C (x). Hence |ξ | ≤ 1, by Proposition 4.3.3, so ξ ∈ NP
C (x)∩B. 
(b): We show ∂P dC(x) ⊃ NP
C (x) ∩ B. 
Take any ξ ∈ NP
C (x) ∩ B. We need to show that ξ ∈ ∂P dC(x). This is certainly true 
for ξ = 0 so we may assume ξ /= 0. We know that there exists M > 0 such that 
ξ · (y − x) ≤ M|y − x|
2 for all y ∈ C.
This fact can be expressed: 
C ∩ Q = ∅
where Q is the open ball 
Q := {y ∈ Rk : |(x + (2M)−1ξ − y| < (2M)−1|ξ |}.186 4 Nonsmooth Analysis
It follows that, for any y ∈ Q, 
dC(y) ≥ g(y) (4.8.1) 
in which 
g(v) := (2M)−1|ξ |−|v − x − (2M)−1ξ |.
(g(v) will be recognized as the distance of v from the surface of the ball.) Since dC
is nonnegative, however, and g is nonpositive on the complement of Q, (4.8.1) is 
actually valid for all y. 
The function g is analytic on a neighbourhood of x (since ξ /= 0). We calculate 
∇g(x) = |ξ |
−1ξ . It follows that there exist some α > 0 and ϵ > 0 such that 
g(y) − g(x) ≥ |ξ |
−1ξ · (y − x) − α|y − x|
2 for all y ∈ x + ϵB.
Since g(x) = dC(x) = 0, we deduce from (4.8.1) that 
dC(y) − dC(x) ≥ g(y) − g(x) ≥ |ξ |
−1ξ · (y − x) − α|y − x|
2
for all y ∈ x + ϵB. Noting however that |ξ | ≤ 1 and dC(x) = 0, we deduce that 
dC(y) − dC(x) ≥ |ξ |(dC(y) − dC(x)) ≥ ξ · (y − x) − α|ξ ||y − x|
2
for all y ∈ x +ϵB. From this we conclude (via Proposition 4.4.1) that ξ ∈ ∂P dC(x). 
⨅⨆
Our next objective is to derive a similar description of limiting normal cones in 
terms of limiting subdifferentials of the distance function. First of all however we 
need to assemble more information about proximal subgradients ξ ∈ ∂P dC(x) at 
basepoints x /∈ C. The following lemma tells us that ξ ∈ ∂P dC(x)¯ for some x¯ ∈ C, 
and a little bit more. 
Lemma 4.8.3 Take a closed set C ⊂ Rk, a point x /∈ C. 
(i): For any vector ξ ∈ ∂P dC(x), x has a unique closest point x¯ in C such that 
ξ = |x − ¯x|
−1(x − ¯x)
and therefore ξ ∈ ∂P dC(x)¯ . 
(ii): ∂dC(x) ⊂ {ξ : |ξ | = 1}.
Proof 
(i): Take any ξ ∈ ∂P dC(x). By Proposition 4.4.2, there exists M > 0 such that 
ξ · (y − x) ≤ dC(y) − dC(x) + M|y − x|
2 for all y ∈ Rk. (4.8.2)4.8 The Distance Function 187
Let x¯ be a closest point to x in C. Then 
|x − ¯x| = dC(x) (/= 0).
Evidently then x¯ is the closest point to x¯+(1−α)(x− ¯x for all α ∈ (0, 1), a property 
which can be expressed analytically as 
dC(x¯ + (1 − α)(x − ¯x)) = (1 − α)|x − ¯x| = (1 − α)dC(x).
Combining this relation with the proximal inequality (4.8.2), we deduce 
− α|x − ¯x| = dC(x¯ + (1 − α)(x − ¯x)) − dC(x)
≥ −αξ · (x − ¯x) − α2M|x − ¯x|
2 for all α ∈ (0, 1).
Divide across this inequality by α and pass to the limit as α ↓ 0. This gives 
|x − ¯x| ≤ ξ · (x − ¯x).
But, by Proposition 4.8.2, |ξ | ≤ 1. Since x − ¯x /= 0, we deduce that 
ξ = |x − ¯x|
−1(x − ¯x).
If y¯ is any other closest point to x in C, the above reasoning gives dC(x) = |x− ¯y| =
|x − ¯x| /= 0 and ξ = |x − ¯y|
−1(x − ¯y) = |x − ¯x|
−1(x − ¯x), from which we conclude 
x¯ = ¯y. 
Of course, the fact that ξ ∈ ∂P dC(x)¯ follows from Proposition 4.8.1. 
(ii): This assertion follows from (i), since C is closed and any limiting subgradient is 
the limit of a convergent sequence of proximal subgradients at neighbouring points 
to x in Rn\C. ⨅⨆
We note also the following partial converse of part (i) of the preceding proposi￾tion. 
Lemma 4.8.4 Take a closed set C ⊂ Rk and points x /∈ C and x¯ ∈ C. Assume 
that, for some α > 0, y = ¯x minimizes the function 
y →|¯x + (1 + α)(x − ¯x) − y|
over C. Define 
ξ := |x − ¯x|
−1(x − ¯x).
We have: 
(i) ξ ∈ ∂P dC(x),188 4 Nonsmooth Analysis
(ii) if {xi} and {ξi} are sequences such that xi → x as i → ∞ and ξi ∈ ∂P dC(xi)
for each i, then there exists a sequence of points {yi} in C such that ξi =
|xi − yi|
−1(xi − yi) for all i, and yi → ¯x and ξi → ξ as i → ∞. 
Proof We arrange, by translation of the origin, that x¯ = 0. 
(i): The minimizing property of x¯ can be expressed geometrically: the open ball 
(1 + α)x + (1 + α)|x|B is disjoint from C. It follows that for any point v ∈ Rk with 
|v − αx|≤|x|(1 + α)
dC(x + v) ≥ ψ(v)
where 
ψ(v) := (1 + α)|x|−|x + v − (1 + α)x| = (1 + α)|x|−|v − αx|.
(the function ψ measures the distance of the point x + v in the ball (1 + α)x + (1 +
α)|x|B to its boundary.) 
Notice that dC(x) = |x| = ψ(0), so 
dC(x + v) − dC(x) ≥ ψ(v) − ψ(0).
But ψ is analytic on α|x|
◦
B. (Notice x /= 0 since x /∈ C.) We calculate its gradient 
at v = 0 to be 
∇ψ(0) = |x|
−1x.
It follows that, for some M > 0 and ϵ > 0
dC(x + v) − dC(x) ≥ |x|
−1v · x − M|v|
2, for all v ∈ ϵB.
By Proposition 4.4.1, then 
|x|
−1x ∈ ∂P dC(x).
(ii): We begin by showing that x(¯ = 0) is the unique minimizer of y → |x −y| over 
C. Let y' be an arbitrary such minimizer. Since 0 ∈ C, |x − y'
|
2 ≤ |x|
2. Hence 
− 2x · y' + |y'
|
2 ≤ 0.
But the minimizing property of x(¯ = 0) tells us that 
|(1 + α)x − y'
|
2 ≥ (1 + α)2|x|
2,
and hence that 
− 2(1 + α)x · y' + |y'
|
2 ≥ 0.4.8 The Distance Function 189
These relations combine to give 
|y'
|
2 − (1 + α)|y'
|
2 ≥ 0.
This is possible only if y' = 0. So x(¯ = 0) is indeed the unique minimizer. 
Now take sequences {xi} and {ξi} with the stated properties. Since x /∈ C and C
is closed, we may assume xi /∈ C for all i. According to Lemma 4.8.3 then there 
exists a sequence {yi} in C such that 
dC(xi) = |xi − yi| (4.8.3) 
and 
ξi = |xi − yi|
−1(xi − yi). (4.8.4) 
Extract an arbitrary subsequence. {yi} is bounded. Along a further subsequence then 
we have yi → ¯y for some y¯ ∈ C. By the continuity of the distance function, we 
deduce from (4.8.3) that dC(x) = |x − ¯y|. In view of our earlier observations then, 
y¯ = 0. Noting (4.8.4), we see that 
yi → 0 and ξi → |x|
−1x = ξ. (4.8.5) 
Since the limits here are independent of the subsequence initially selected, (4.8.5) is 
in fact true for the original sequence. ⨅⨆
Simple limit taking procedures will now permit to relate the limiting normal cone 
to a set and the limiting subdifferential of the distance function. 
Theorem 4.8.5 Take a closed set C ⊂ Rk and a point x ∈ C. Then 
∂dC(x) = NC(x) ∩ B.
Proof 
(a): We show ∂dC(x) ⊂ NC(x) ∩ B.
Take a vector ξ ∈ ∂dC(x). Then there exist xi → x and ξi → ξ such that ξi ∈
∂P dC(xi) for all i (by Theorem 4.6.2). In consequence of Proposition 4.8.2 and 
Lemma 4.8.3, there exists a sequence of points {zi} in C such that zi
C
→ x, and 
ξi ∈ NP
C (zi) and |ξi| ≤ 1 for each i. But then by definition of N, ξ ∈ NC(x) ∩ B. 
(b): We show ∂dC(x) ⊃ NC(x) ∩ B.
Since NC(x) is a cone, generated by taking limits of proximal normals and x ⇝
∂dC(x) has closed graph, it suffices to show that NP
C (x) ∩ B ⊂ ∂P dC(x). But this 
property is supplied by Proposition 4.8.2. ⨅⨆190 4 Nonsmooth Analysis
4.9 Criteria for Lipschitz Continuity 
On occasions, we wish to establish Lipschitz continuity of some specific function. 
In the case when an explicit formula is available for evaluation of the function, 
we can expect to be able to assess whether this property holds directly. There are, 
too, certain elementary criteria for Lipschitz continuity which are sometimes of use: 
‘the pointwise supremum of a uniformly bounded family of Lipschitz continuous 
functions with common Lipschitz constant is Lipschitz continuous’, for example. 
But for certain implicitly defined functions, most notably for many of the ‘value’ 
functions considered in optimization, the task of establishing Lipschitz continuity on 
a neighbourhood of a basepoint can be a challenging one. Here conditions involving 
boundedness of subdifferentials are often useful. The following theorem makes 
precise the intuitive notion that lack of Lipschitz continuity should be reflected in 
the presence of ‘unbounded’ neighbouring derivatives. 
Theorem 4.9.1 Take a lower semi-continuous function f : Rk → R ∪ {+∞}, 
a point x ∈ dom f and a constant K > 0. Then the following conditions are 
equivalent: 
(i) f is Lipschitz continuous with Lipschitz constant K on some neighbourhood 
of x, 
(ii) there exists ϵ > 0 such that for every x ∈ Rk and ξ ∈ ∂P f (x) satisfying 
|x − ¯x| ≤ ϵ and |f (x) − f (x)¯ | ≤ ϵ,
we have 
|ξ | ≤ K.
The theorem tells us that, in order to check Lipschitz continuity, we have only to 
test boundedness of neighbouring proximal subgradients, at points where they are 
defined. 
This theorem and earlier derived properties of subdifferentials lead to an alterna￾tive criterion for Lipschitz continuity involving the asymptotic subdifferential. 
Corollary 4.9.2 Take a lower semi-continuous function f : Rk → R ∪ {+∞} and 
a point x¯ ∈ dom f . The following conditions are equivalent: 
(i) f is Lipschitz continuous on a neighbourhood of x¯, 
(ii) ∂∞f (x)¯ = {0}.
Proof Condition (i) implies condition (ii), by Proposition 4.7.1. To show the 
reverse implication, assume that (i) is not true. Then, by Theorem 4.9.1, there exist 
sequences xi
f
→ ¯x and {ξi} such that 
ξi ∈ ∂P f (xi) for each i4.9 Criteria for Lipschitz Continuity 191
and 
|ξi|→∞ as i → ∞.
For each i, we have 
(|ξi|
−1ξi, −|ξi|
−1) ∈ NP
epif (xi, f (xi)).
Along a subsequence however 
(|ξi|
−1ξi, −|ξi|
−1) → (ξ ,¯ 0)
for some ξ¯ with |ξ¯| = 1. In view of the closure properties of the limiting normal 
cone 
(ξ ,¯ 0) ∈ Nepif (x,f ( ¯ x)). ¯
This means that ξ¯ is a non-zero vector in ∂∞f (x)¯ = {0}. We have shown that (ii) is 
not true. ⨅⨆
As a preliminary step in the proof of Theorem 4.9.1, we establish some properties 
of proximal subgradients of a special ‘min’ function. 
Lemma 4.9.3 Take a lower semi-continuous function f : Rk → R∪{+∞}, x ∈ Rk
and β ∈ R. Define 
g(y) := min{f (y), β}.
Suppose that ξ is a non-zero vector such that 
ξ ∈ ∂P g(x).
Then 
f (x) ≤ β and ξ ∈ ∂P f (x).
Proof Since ξ ∈ ∂P g(x), there exists α > 0 such that 
min{f (x'
), β} − min{f (x), β} ≥ ξ · (x' − x) − M|x' − x|
2
for all x' ∈ x + αB. 
Suppose that f (x) > β. Then 
0 = β − β ≥ min{f (x'
), β} − min{f (x), β} ≥ ξ · (x' − x) − M|x' − x|
2192 4 Nonsmooth Analysis
for all x' ∈ x + αB. It follows that ξ = 0. But this is impossible since we have 
assumed ξ /= 0. We have shown that 
f (x) ≤ β.
Notice also that 
f (x'
) − f (x) ≥ min{f (x'
), β} − min{f (x), β}
≥ ξ · (x' − x) − M|x' − x|
2
for all x' ∈ x + αB. It follows that ξ ∈ ∂P f (x). ⨅⨆
Proof of Theorem 4.9.1 That condition (i) implies condition (ii) follows from 
Theorem 4.7.1. To prove the reverse implication, assume that f is not Lipschitz 
continuous with Lipschitz constant K on any neighbourhood of x¯. We shall show 
that condition (ii) cannot be satisfied. 
Choose any ϵ > 0. Since f is lower semi-continuous, we can find δ > 0, 0 <
δ<ϵ, such that 
z ∈ ¯x + δB implies f (z) > f (x)¯ − ϵ. (4.9.1) 
Under the hypothesis there exist sequences 
xi → ¯x, yi → ¯y
such that 
xi /= yi
and 
f (yi) − f (xi)>K|yi − xi| (4.9.2) 
for all i. (The case f (yi) = +∞ is permitted.) We can assume that for some 
subsequence (we do not relabel) 
f (xi) → f (x)¯ as i → ∞. (4.9.3) 
This is because if this property fails to hold then, in view of the lower semicontinuity 
of f , we have 
lim inf
i→∞ f (xi)> f(x). ¯
But |xi − ¯x| → 0. It follows that (4.9.2) remains valid (for all i sufficiently large) 
when we redefine xi := ¯x. Of course (4.9.3) is automatically satisfied.4.9 Criteria for Lipschitz Continuity 193
In view of these properties, we can choose a sequence ϵi ↓ 0 such that 
f (xi) − f (x)¯ ≤ ϵi (4.9.4) 
and 
K|yi − xi| < f(x)¯ − f (xi) + ϵi. (4.9.5) 
Fix a value of the index i such that 
|xi − ¯x| < 1
2 δ , |yi − ¯x| < 1
2 δ and ϵi < ϵ. (4.9.6) 
Define the function 
gi(y) := min{f (x)¯ + ϵi, f (y)}.
It follows from (4.9.2) and (4.9.5) that 
gi(yi) − gi(xi)
= min{f (x)¯ − f (xi) + ϵi, f (yi) − f (xi)} > K|yi − xi|.
Since gi is a lower semi-continuous function which is finite at xi and yi, the 
following conclusions can be drawn from this last relation and the mean value 
inequality (Theorem 4.5.1): there exist 
z ∈ ¯x + δB and ξ ∈ ∂P gi(z)
such that 
K|yi − xi| < ξ · (yi − xi).
But yi /= xi. It follows that 
|ξ | > K.
Recall however that δ<ϵ. So |z − ¯x| < ϵ. From (4.9.1) we deduce 
f (z) − f (x) > ¯ −ϵ.
Since the proximal subgradient ξ is non-zero, we deduce, from Lemma 4.9.3 and 
the preceding relations, that 
|z − ¯x| < ϵ , |f (z) − f (x)¯ | < ϵ and ξ ∈ ∂P f (z).
But ϵ is an arbitrary positive number. It follows that condition (ii) is not satisfied.
⨅⨆194 4 Nonsmooth Analysis
4.10 Relations Between Normal and Tangent Cones 
Up to this point, tangent vectors have put in only an occasional appearance. In 
this optimization oriented treatment of the theory, it is natural to emphasize the 
significance of normal vectors, since it is in terms of these objects (and their close 
relatives, subgradients) that Lagrange multiplier rules and other important principles 
of nonsmooth optimization are expressed. There is however a rich web of relations 
involving tangent vectors and normal vectors, as we now reveal. 
Take a closed set C ⊂ Rk and a point x ∈ C. With the help of the set limit 
notation of Sect. 2.1, we now review the various concepts of normal cone and their 
interrelation. 
The proximal normal cone to C at x, NP
C (x), is 
NP
C (x) := {ξ : ∃ M > 0 such that ξ · (y − x) ≤ M|y − x|
2 for all y ∈ C}.
The strict normal cone to C at x, NˆC(x), is 
NˆC(x) := {ξ : lim sup
y C
→x
|y − x|
−1ξ · (y − x) ≤ 0}.
The limiting normal cone is expressible in terms of our new notation as 
NC(x) = lim sup
y C
→x
NP
C (y).
Proposition 4.2.5 provides us also with the following representation of NC(x): 
NC(x) = lim sup
y C
→x
NˆC(y).
We recall that NP
C (x), NˆC(x) and NC(x) are all cones containing {0}. NP
C (x) is 
convex, NˆC(x) is closed and convex, NC(x) is closed and 
NP
C (x) ⊂ NˆC(x) ⊂ NC(x).
The tangent cones of primary interest here are the Bouligand and the Clarke 
tangent cones defined as follows 
Definition 4.10.1 Take a closed set C ⊂ Rk and a point x ∈ C. 
The Bouligand tangent cone to C at x ∈ C, written TC(x), is the set 
TC(x) := lim sup
t↓0
t
−1(C − x).4.10 Relations Between Normal and Tangent Cones 195
The Clarke tangent cone to C at x, written T¯
C(x), is the set 
T¯
C(x) := lim inf
t↓0,y C
→x
t
−1(C − y).
Equivalent ‘sequential’ definitions are as follows: 
TC(x) comprises vectors ξ corresponding to which there exist some sequence {ci}
in C and some sequence ti ↓ 0 such that t
−1
i (ci − x) → ξ . 
T¯
C(x) comprises vectors ξ such that for any sequences xi
C
→ x and ti ↓ 0 there 
exists a sequence {ci} in C such that t
−1
i (ci − xi) → ξ . 
We state without proof the following elementary properties. 
Proposition 4.10.2 Take a closed set C ⊂ Rk and a point x ∈ C. Then TC(x) and 
T¯
C(x) are closed cones containing the origin and T¯
C(x) ⊂ TC(x). 
A less obvious property is: 
Proposition 4.10.3 Take a closed set C ⊂ Rk and a point x ∈ C. Then T¯
C(x) is a 
convex set. 
Proof Take any u and v in T¯
C(x). Since T¯
C(x) is a cone, to establish convexity we 
must show that u + v ∈ T¯
C(x). Take any sequence xi
C
→ x and ti ↓ 0. Because 
u ∈ T¯
C(x), there exists a sequence of points ui → u such that xi + tiui ∈ C for 
each i. But xi + tiui → x, so, since v ∈ T¯
C(x) also, there exists a sequence of 
points vi → v such that (xi + tiui) + tivi ∈ C for each i. A rearrangement of this 
inclusion gives xi + ti(ui + vi) ∈ C. But this implies u + v ∈ T¯
C(x). ⨅⨆
Denote by S∗ the polar cone of a set S ⊂ Rk, namely 
S∗ := {ξ : ξ · x ≤ 0 for all x ∈ S}.
Theorem 4.10.4 Take a closed set C ⊂ Rk and a point x ∈ C. Then the strict 
normal cone NˆC(x) and the Bouligand tangent cone TC(x) are related according to 
NˆC(x) = TC(x)∗.
Proof 
(a): We show that NˆC(x) ⊂ TC(x)∗.
Take any ξ ∈ NˆC(x). Then 
ξ · (y − x) ≤ o(|y − x|) for all y ∈ C (4.10.1) 
for some o(·) : R+ → R+ which satisfies o(s)/s → 0 as s ↓ 0. Choose any v ∈
TC(x). Then there exists xi
C
→ x and ti ↓ 0 such that, if we define vi := t
−1
i (xi −x)
for each i, then vi → v. It is claimed that ξ · v ≤ 0; it will follow that ξ ∈ TC(x)∗.196 4 Nonsmooth Analysis
The claim is certainly valid when v = 0. So assume v /= 0. For each i then 
ξ · vi = t
−1
i ξ · (xi − x) ≤ |vi||xi − x|
−1o(|xi − x|)
by (4.10.1). Passing to the limit as i → ∞, we obtain ξ · v ≤ 0. 
(b): We show that NˆC(x) ⊃ TC(x)∗.
Suppose that ξ /∈ NˆC(x). Then there exist ϵ > 0 and xi
C
→ x such that 
ξ · (xi − x) > ϵ|xi − x| for all i.
Note that xi /= x for each i. Set ti = |xi −x| and vi := t
−1
i (xi −x). The vi’s all have 
unit length. Along a subsequence then, vi → v for some v ∈ TC(x) with |v| = 1. 
Along the subsequence we have 
ξ · vi = t
−1
i ξ · (xi − x) > ϵt−1
i |xi − x| = ϵ|vi| = ϵ.
In the limit, we obtain ξ · v ≥ ϵ. It follows ξ /∈ TC(x)∗. ⨅⨆
The next theorem tells us (among other things) that we get the Clarke tangent 
cone by applying the lim inf operation (with respect to the base point) to the 
Bouligand tangent cone. 
Theorem 4.10.5 Take a closed set C ⊂ Rk and a point x ∈ C. Then the Bouligand 
tangent cone TC(x), its closed convex hull and the Clarke tangent cone T¯
C(x) are 
related as follows: 
lim inf
y C
→x
co TC(y) = lim inf
y C
→x
TC(y) = T¯
C(x).
Proof Since TC(y) ⊂ co TC(y) for each y, it suffices to show 
(i) lim inf
y C
→x
co TC(y) ⊂ T¯
C(x), 
(ii) T¯
C(x) ⊂ lim inf
y C
→x TC(y), 
since these inclusions imply 
lim inf
y C
→x
coTC(y) ⊂ T¯
C(x) ⊂ lim inf
y C
→x
TC(y) ⊂ lim inf
y C
→x
coTC(y)
from which the required relations follow. 
We show 
lim inf
y C
→x
coTC(y) ⊂ T¯
C(x).4.10 Relations Between Normal and Tangent Cones 197
Take any v ∈ lim inf
y C
→x coTC(y). Let xi
C
→ x and ti ↓ 0 be arbitrary sequences. 
Suppose we are able to find a sequence ϵi ↓ 0 such that 
t
−1
i dC(xi + tiv) ≤ ϵi. (4.10.2) 
Then we have 
lim sup
i
t
−1
i dC(xi + tiv) = 0.
This implies v ∈ T¯
C(x) which is the required relation. 
It remains to show (4.10.2). For each i, we consider the function 
gi(t) := dC(xi + tv), 0 ≤ t ≤ ti.
Choose zi(t) ∈ arg min{|xi + tv − z| : z ∈ C}. Notice that, since xi ∈ C, 
|x − zi(t)|≤|xi + tv − zi(t)|+|xi − x + tv| ≤ 2ti|v|+|xi − x|
for 0 ≤ t ≤ ti and i = 1, 2,...
Evidently then 
sup
0≤t≤ti
|x − zi(t)| → 0 as i → ∞.
Since v ∈ lim inf
y C
→x coTC(y), there exist ϵi ↓ 0 and functions wi(·) : [0, ti] → Rn
such that 
wi(t) ∈ coTC(zi(t)) and |wi(t) − v| < ϵi for all t ∈ [0, ti].
Fix i. We verify (4.10.2) for the above choice of ϵi. 
Let us first investigate the properties of a point t ∈ [0, ti) at which gi > 0
and where gi is differentiable. To simplify the notation, write z for zi(t) and set 
p = xi + tv. We have for sufficiently small h > 0
gi(t + h) − gi(t) ≤ |xi + tv + hv − z|−|xi + tv − z|
= |p − z + hv|−|p − z|.
Since p /= z (gi(t) > 0, remember) the function u → |p −z+u| is differentiable at 
any point sufficiently close to the origin, in particular at hv (for h > 0 sufficiently 
small). In view of the convexity of the norm then the ‘subgradient inequality’ for 
convex functions gives 
gi(t + h) − gi(t) ≤ h|p − z + hv|
−1(p − z + hv) · v.198 4 Nonsmooth Analysis
We conclude that 
h−1(gi(t + h) − gi(t)) ≤
 p − z + hv
|p − z + hv|
− p − z
|p − z|

· v + p − z
|p − z|
· v
≤
 p − z + hv
|p − z + hv|
− p − z
|p − z|

· v
+ p − z
|p − z|
· wi + ϵi. (4.10.3) 
Since p − z ∈ NP
C (z) and wi(t) ∈ ¯coTC(z) and in view of the fact that 
NˆC(z) = T ∗
C(z) = (co¯ TC(z))∗,
it follows that 
(p − z) · wi(t) ≤ 0.
We therefore retain inequality (4.10.3) when we drop the second term on the right. 
Passing to the limit as h ↓ 0, we obtain 
d
dt gi(t) ≤ ϵi.
Since gi is Lipschitz continuous and therefore almost everywhere differentiable, this 
inequality is valid for all points t ∈ {s ∈ [0, ti) : gi(s) > 0} excluding a nullset. 
Condition (4.10.2) is automatically satisfied if gi(ti) = 0. Suppose then that 
gi(ti) > 0. Define 
τ := inf{t ∈ [0, ti] : gi(s) > 0 for all s ∈ [t,ti]}.
Since gi is continuous and gi(0) = 0, we conclude gi(τ ) = 0. It follows that 
gi(ti) = gi(τ ) +
 ti
τ
g˙i(σ )dσ ≤ 0 + tiϵi.
But then 
t
−1
i dC(xi + tiv)(= t
−1
i gi(ti)) ≤ ϵi.
Condition (4.10.2) is therefore satisfied in this case also. The proof is complete. 
We show that 
T¯
C(x) ⊂ lim inf
y C
→x
TC(y).4.10 Relations Between Normal and Tangent Cones 199
Take v ∈ T¯
C(x). Consider an arbitrary sequence xi
C
→ x. It follows from the 
definition of T¯
C(x) that there exists ϵi ↓ 0, ti ↓ 0 and Ni ↑ ∞ such that 
t
−1dC(xk + tv) ≤ ϵi for all 0 ≤ t ≤ ti, and k ≥ Ni.
Relabel {xNi}∞
i=1 as {xi} and {tNi}∞
i=1 as {ti}. Then for each i = 1, 2,... and for all 
t ∈ [0, ti], we have in particular 
t
−1dC(xi + tv) ≤ ϵi.
Fix i and t ∈ [0, ti], choose zi(t) ∈ C such that 
dC(xi + tv) = |xi + tv − zi(t)|.
Define 
vi(t) := t
−1(zi(t) − xi), for all t ∈ (0, ti], i = 1, 2,... (4.10.4) 
Then 
|vi(t) − v| ≤ ϵi for all t ∈ (0, ti], for i = 1, 2,...
Fix i, choose v¯i to be a cluster point of {vi(t)}t>0 at t = 0 (such v¯i exists since 
vi(·) is bounded on (0, ti]). We have | ¯vi − v| ≤ ϵi. By (4.10.4) and in view of the 
definition of TC(xi), we have that 
v¯i ∈ TC(xi) and v¯i → v.
Starting with an arbitrary sequence {xi} in C converging to x, we have found a 
subsequence {xin }∞
n=1 and a sequence { ¯vn} such that 
v¯n ∈ TC(xin ) for all n and v¯n → v as n → ∞.
It follows that v ∈ lim inf
y C
→x TC(y). ⨅⨆
Our next objective is to relate the Clarke tangent cone and the limiting normal 
cone. First a lemma is required on the interaction between limit taking and the 
construction of polar sets. 
Lemma 4.10.6 Take a set valued function S : Rk ⇝ Rn, a set C ⊂ Rk and a point 
x ∈ Rk. Assume that S(y) is a closed convex cone for all y. Then 
lim inf
y C
→x
S(y) = [lim sup
y C
→x
S(y)∗]
∗.200 4 Nonsmooth Analysis
Proof 
(a): We show that 
lim inf
y C
→x
S(y) ⊂ [lim sup
y C
→x
S(y)∗]
∗.
Take v ∈ lim inf
y C
→x S(y). Choose any ξ ∈ lim sup
y C
→x S(y)∗. Then there exist 
sequences xi
C
→ x and {ξi} in Rn such that 
ξi ∈ S(xi)
∗ for all i and ξi → ξ.
We may also choose a sequence vi → v such that vi ∈ S(xi) for each i. For each i
then we have ξi · vi ≤ 0. In the limit we get ξ · v ≤ 0. It follows that 
v ∈ [lim sup
y C
→x
S(y)∗]
∗.
(b): We show that 
lim inf
y C
→x
S(y) ⊃ [lim sup
y C
→x
S(y)∗]
∗.
Take v ∈ [lim sup
y C
→x S(y)∗]
∗. Assume, contrary to the assertions of the lemma that 
v /∈ lim inf
y C
→x S(y). Then there is some ϵ > 0 and a sequence xi
C
→ x such that 
(v + ϵB) ∩ S(xi) = ∅, for each i.
Applying the separation theorem we obtain, for each i, a vector ξi such that |ξi| = 1
and 
sup
w∈S(xi)
w · ξi ≤ inf w∈v+ϵB
w · ξi = v · ξi − ϵ .
Since S(xi) is a cone, we deduce that supw∈S(xi) w·ξi = 0 and so ξi ∈ S(xi)∗. Along 
a subsequence ξi → ξ for some ξ with |ξ | = 1. We have that ξ ∈ lim sup
y C
→x S(y)∗. 
We also have 0 ≤ v·ξi−ϵ. In the limit we arrive at v·ξ ≥ ϵ, from which we conclude 
that v /∈ [lim sup
y C
→x S(y)∗]
∗. This contradiction concludes the proof. ⨅⨆
Theorem 4.10.7 Take a closed set C ⊂ Rk and x ∈ C. The Clarke tangent cone 
T¯
C(x) and the limiting normal cone NC(x) are related according to 
T¯
C(x) = NC(x)∗.4.11 Interior of Clarke’s Tangent Cone 201
Proof Apply the preceding lemma to S(y) := coTC(y). This gives 
lim inf
y C
→x
co TC(y) = (lim sup
y C
→x
co TC(y)∗)
∗.
But lim inf
y C
→x coTC(y) = T¯
C(x) by Theorem 4.10.5. Also, for each y we have 
(coTC(y))∗ = TC(y)∗ = NˆC(y) by Theorem 4.10.4. But then, by Proposition 4.2.5, 
NC(x) = lim sup
y C
→x
(coTC(y))∗. Assembling those relations, we get T¯
C(x) =
NC(x)∗. ⨅⨆
The relations we have established between NP
C (x), NˆC(x), NC(x), T¯
C(x) and 
TC(x) associated with a closed set C ∈ Rk and point x ∈ C, are summarized in the 
following diagram, in which ‘ ∗ ’ denotes ‘take the polar cone’: 
TC(x) liminf
−→ T¯
C(x)
∗↓ ↑∗
NˆC(x) limsup
−→ NC(x) limsup
←− NP
C (x)
4.11 Interior of Clarke’s Tangent Cone 
A useful characterization of the interior of Clarke’s tangent cone is provided by the 
following theorem. 
Theorem 4.11.1 Take a closed set C ⊂ Rk and x ∈ C. the following statements 
are equivalent: 
(i) v ∈ int T¯
C(x), 
(ii) there exists ϵ > 0 such that 
dC(y + tw) ≤ dC(y), for all y ∈ x + ϵB, w ∈ v + ϵB and t ∈ [0, ϵ] ,
(4.11.1) 
(iii) there exists ϵ > 0 such that 
y + [0, ϵ](v + ϵB) ⊂ C, for all y ∈ (x + ϵB) ∩ C . (4.11.2) 
Proof Since (iii) is an immediate consequence of (ii), we shall show that ‘(i) ⇒ (ii)’ 
and ‘(iii) ⇒ (i)’. 
‘(i) ⇒ (ii)’. 
Assume first that v ∈ int T¯
C(x). If v = 0, then it means that T¯
C(x) = Rn. Using 
well-known polarity properties we deduce that {0} = (T¯
C(x))∗ = coNC(x), and so202 4 Nonsmooth Analysis
NC(x) = {0}. Then, from Proposition 4.2.7 it follows that x ∈ ◦
C. Then taking a 
suitably small ϵ > 0 we obtain (4.11.2). 
So, we can reduce attention to the case when 0 /= v ∈ int T¯
C(x). From 
Theorem 4.10.7 we know that T¯
C(x) = NC(x)∗, and so there exists ρ > 0 such 
that 
ξ · v ≤ −ρ , for all ξ ∈ NC(x).
Suppose now that (4.11.1) is not satisfied. Then, there would exist sequences yi→x, 
vi → v and ti ↓ 0 such that 
dC(yi + tivi) − dC(yi) > 0, for all i ≥ 1 .
Take any sequence ϵi ↓ 0. Then, applying the proximal mean value inequality 
(Theorem 4.5.1), we can find, for each i ≥ 1, a point xi ∈ [yi, yi + tivi] + ϵiB
and a vector ξi ∈ ∂P dC(xi) such that 
ξi · vi > 0.
Using Proposition 4.8.2 (when xi ∈ C) and Lemma 4.8.3 (when xi ∈/ C), we also 
know that ξi/|ξi| ∈ ∂P dC(xi) ∩ ∂B. Therefore, bearing in mind Theorem 4.6.2 
(which provides a characterization of the limiting subgradients) and Theorem 4.8.5, 
extracting a subsequence if necessary, we see that ξi converges to a vector ξ ∈
∂dC(x) = NC(x) ∩ B such that |ξ | = 1. As a consequence we would obtain that 
− ρ = −ρ|ξ | ≥ v · ξ = lim
i→∞ |ξi|
−1ξi · vi ≥ 0 ,
and thereby arrive at a contradiction. We deduce the validity of (4.11.1). 
‘(iii) ⇒ (i)’. 
Suppose now that (4.11.2) is in force. Then it follows that the generalized directional 
derivative d0
C(x, w) ≤ 0 for all w ∈ v + ϵB. In view of Propositions 4.7.4 and 4.7.6 
we have that, for all w ∈ v + ϵB, 
w · ξ ≤ 0, for all ξ ∈ co∂dC(x),
and, recalling that ∂dC(x) = NC(x) ∩ B (from Theorem 4.8.5), we deduce that 
w ∈ NC(x)∗ .
Since T¯
C(x) = NC(x)∗ (from Theorem 4.10.7), we conclude that w ∈ T¯
C(x) for all 
w ∈ v + ϵB and, therefore, v ∈ int T¯
C(x). ⨅⨆4.12 Appendix: Proximal Analysis in Hilbert Space 203
4.12 Appendix: Proximal Analysis in Hilbert Space 
Nonsmooth analysis has a pivotal role in this book, both in the derivation of first 
order necessary conditions of optimality and also in establishing the link between 
the value function of dynamic optimization and the Hamilton Jacobi equation. 
Despite the fact that the dynamic optimization problem is an optimization problem 
over function spaces (and is therefore infinite dimensional), the application of 
nonsmooth analysis for these purposes is based, almost exclusively, on properties of 
nonsmooth functions and sets with nonsmooth boundaries in the setting of a finite 
dimensional space. Many aspects of nonsmooth analysis, in which the underlying 
spaces are finite dimensional linear spaces, have analogues when the function 
domains and also the sets involved are Hilbert spaces. 
Nonsmooth analysis in a real Hilbert space setting does find useful applications 
to some areas of dynamic optimization. These include derivation of necessary 
conditions of optimality for distributed parameter control systems and sensitivity 
analysis relating to perturbations of parameters in a function space. While such 
topics are not covered in this book, it is of interest to know that many of the ideas in 
the chapter generalize to an infinite dimensional setting. We show, in this appendix, 
how proximal subgradients of functions on real Hilbert spaces can be defined and 
prove an important density theorem. This broader theory is then used to prove two 
variational principles stated in Chap. 3. 
Henceforth we take X to be a real Hilbert space endowed with a scalar product 
written 〈., .〉. We write ||.|| the associated norm and BX the closed unit ball in X; ◦
BX is the open unit ball in X. 
Definition 4.12.1 Take a lower semi-continuous function f : X → R ∪ {+∞} and 
a point x ∈ dom f := {y ∈ X : f (y) < +∞}. We say that ξ ∈ X is a proximal 
subgradient if there exist M > 0 and ϵ > 0 such that 
〈ξ,y − x〉 ≤ f (y) − f (x) + M||y − x||2, for all y ∈ x + ϵBX. (4.12.1) 
The proximal subdifferential of f at x, written ∂P f (x), is the set 
∂P f (x) := {ξ : there exist M > 0 and ϵ > 0 such that condition (4.12.1) is satisfied }.
Theorem 4.12.2 (Proximal Density) Take a real Hilbert space X and a lower 
semi-continuous function f : X → R ∪ {+∞}. Let x0 ∈ dom f and ϵ > 0
be given. Then there exists a point y ∈ x0 + ϵ
◦
BX satisfying ∂P f (y) /= ∅ and 
|f (y) − f (x0)| ≤ ϵ. 
Proof Since f is lower semi-continuous there exists δ ∈ (0, ϵ) such that 
f (x) ≥ f (x0) − ϵ, for all x ∈ x0 + δBX. (4.12.2)204 4 Nonsmooth Analysis
In the particular case when X is finite dimensional, say X = Rn, the proof becomes 
very simple. Indeed we define the function 
φ(x) :=  1
δ2−|x−x0|
2 if ∈ x0 + δ
◦
B
+∞ otherwise.
Observe that φ(x) → +∞ as |x − x0| ↑ δ, φ is lower semi-continuous, but φ is of 
class C2 on x0+δ
◦
B. Then, if we consider the function f +φ we obtain a lower semi￾continuous function which is bounded below on X = Rn. As a consequence f + φ
achieves the minimum value at a point y ∈ x0 +δB, and from the definition of φ we 
can also deduce that necessarily y ∈ x0 + δ
◦
B. From Proposition 5.1.1 and Lemma 
5.1.2 it follows that 0 ∈ ∂P (f +φ)(y) = ∂P f (y)+ {∇φ(y)}. Therefore −∇φ(y) ∈
∂P f (y) and so, in particular, we have ∂P f (y) /= ∅. Since y is a minimum for f +φ
and clearly φ(x0) ≤ φ(y) we also deduce that f (y) ≤ f (x0) + (φ(x0) − φ(y)) ≤
f (x0). In view of this inequality and (4.12.2) we obtain that |f (y) − f (x0)| ≤ ϵ. 
The proof is then complete when X = Rn. 
We proceed now with the general case. Take α > 2ϵ/δ2. We claim that there 
exists y0 ∈ x0 + δBX such that the function x → f (x) + α||x − y0||2 attains a 
minimum on x0 + δ
◦
BX at some point y ∈ x0 + δ
◦
BX, and f (y) ≤ f (x0). 
Write X0 := x0 + δBX and 
X1 := {x ∈ X0 : f (x) +
α
2
||x − x0||2 ≤ f (x0)}.
Observe that x0 ∈ X1, and since x → f (x)+ α
2 ||x−x0||2 is lower semi-continuous, 
X1 is closed. Moreover, if x ∈ X1, then from (4.12.2) and the choice of α we deduce 
that 
||x − x0||2 ≤
2
α
(f (x0) − f (x)) <
δ2
ϵ × ϵ = δ2,
and, so, 
X1 ⊂ x0 + δ
◦
BX ⊂ X0. (4.12.3) 
We can find x1 ∈ X1 such that 
f (x1) +
α
2
||x1 − x0||2 ≤ inf
x∈X1
{f (x) +
α
2
||x − x0||2} +
α
4 ,
and we define the closed set 
X2 := 
x ∈ X1 : f (x)+
α
2

||x−x0||2+||x−x1||2
2

≤ f (x1)+
α
2
||x1−x0||2

,4.12 Appendix: Proximal Analysis in Hilbert Space 205
which is nonempty since x1 ∈ X2. Using an induction argument we obtain a 
sequence of points {xk} and a sequence of sets {Xk} such that xk ∈ Xk for all 
k ≥ 0, and for each k ≥ 1 we have 
f (xk) +
α
2

k−1
i=0
||xk − xi||2
2i ≤ inf
x∈Xk
{f (x) +
α
2

k−1
i=0
||x − xi||2
2i } +
α
4k (4.12.4) 
and 
Xk := 
x ∈ Xk−1 : f (x)+
α
2

k−1
i=0
||x−xi||2
2i ≤ f (xk−1)+
α
2

k−1
i=0
||xk−1−xi||2
2i

.
(4.12.5) 
Clearly xk−1 ∈ Xk for all k ≥ 1, and so {Xk} is a nested sequence of nonempty 
closed sets. Moreover, for each k ≥ 1 given, from (4.12.4) and (4.12.5) we deduce 
that, for all x ∈ Xk+1, 
α
2
||x − xk||2
2k ≤ f (xk) +
α
2

k−1
i=0
||xk − xi||2
2i −

f (x) +
α
2

k−1
i=0
||x − xi||2
2i

≤
α
4k
and, so, 
sup
x∈Xk+1
||x − xk|| ≤ 21−k .
It follows that diam (Xk) (:= sup{||x − x'
|| : x, x' ∈ Xk}) → 0 as k → ∞. 
Bearing in mind that X is an Hilbert space (and therefore it is complete), we deduce 
that there exists a point y ∈ X such that ∩∞
k=0Xk = {y}. Observe that in particular 
we have y ∈ X1 ⊂ x0 + δ
◦
BX (recall that we have the inclusion (4.12.3)) and so 
||y −x0|| <δ<ϵ, and (invoking again the definition of X1) f (y) ≤ f (y)+ α
2 ||y −
x0||2 ≤ f (x0). This inequality combined with (4.12.2) yields |f (y) − f (x0)| ≤ ϵ. 
Set 
y0 :=
1
2
∞
i=0
xi
2i and c :=
1
2
∞
i=0
||xi||2
2i − ||y0||2 .
Observe that, for each x ∈ X, we have 
||x − y0||2 = 1
2
∞
i=0
||x − xi||2
2i − c . (4.12.6)206 4 Nonsmooth Analysis
Now we show that the function x → f (x) + α||x − y0||2 achieves its minimum 
value on x0 + δ
◦
BX at y. Indeed, consider the sequence of real numbers {βk} where, 
for each k ≥ 1, 
βk := f (xk) +
α
2

k
i=0
||xk − xi||2
2i .
We know that xk ∈ Xk for all k, so from (4.12.5) we deduce that the sequence {βk}
is nonincreasing. Now fix any x ∈ x0+δ
◦
BX with x /= y, and write kˆ the integer such 
that x ∈ Xkˆ \ Xkˆ+1. Take any k ≥ kˆ +1. Since y ∈ Xk+1 and {βk} is nonincreasing, 
from (4.12.5) it follows that 
f (y) +
α
2

k
i=0
||y − xi||2
2i ≤ f (xk) +
α
2

k
i=0
||xk − xi||2
2i
≤ f (xk−1) +
α
2

k−1
i=0
||xk−1 − xi||2
2i
≤ ...
≤ f (xkˆ) +
α
2

kˆ
i=0
||xkˆ − xi||2
2i . (4.12.7) 
Recalling that x ∈ Xkˆ \ Xkˆ+1, from (4.12.5) we also have that 
f (xkˆ) +
α
2

kˆ
i=0
||xkˆ − xi||2
2i < f (x) +
α
2

kˆ
i=0
||x − xi||2
2i
≤ f (x) +
α
2
∞
i=0
||x − xi||2
2i . (4.12.8) 
Using the inequalities (4.12.7) and (4.12.8), we obtain that, for all k ≥ kˆ + 1, 
f (y) +
α
2

k
i=0
||y − xi||2
2i ≤ f (x) +
α
2
∞
i=0
||x − xi||2
2i
and, letting k → ∞, we arrive at 
f (y) +
α
2
∞
i=0
||y − xi||2
2i ≤ f (x) +
α
2
∞
i=0
||x − xi||2
2i .4.12 Appendix: Proximal Analysis in Hilbert Space 207
Adding to both sides the quantity ‘−αc’ and invoking the relation (4.12.6) we 
deduce that 
f (y) + α||y − y0||2 ≤ f (x) + α||x − y0||2 ,
which, since x ∈ x0 + δ
◦
BX was an arbitrary point such that x /= y, confirms our 
claim. 
To conclude the proof, we observe that g(x) := α||x − y0||2 is of class C2 and 
that Dg(x) = 2α(x −y0). Therefore, since y is a minimum for f +g, it follows that 
0 ∈ ∂P (f + g)(y) = ∂P f (y) + {Dg(y)}, where Dg(y) is the Fréchet derivative of 
g at y (this can easily be shown by adapting the analysis in the proofs of Proposition 
5.1.1 and Lemma 5.1.2 to the case when X is an Hilbert space, in place of Rn). 
Therefore −2α(y − y0) ∈ ∂P f (y) and ∂P f (y) /= ∅. ⨅⨆
In Sect. 3.4 we introduced the quadratic inf convolution of a lower semi￾continuous function on a finite dimensional linear space. We may carry out a similar 
construction when the finite dimensional linear space is replaced by a real Hilbert 
space X. Now we define the quadratic inf convolution of a lower semi-continuous 
function g : X → R ∪ {+∞} to be 
gα(x) := inf
y∈X
{g(y) + α||x − y||2} . (4.12.9) 
We shall make use of the properties collected together in the following theorem: 
Theorem 4.12.3 (Inf-Convolution, II) Take a real Hilbert space X and a proper 
lower semi-continuous function g : X → R ∪ {+∞} which is bounded below by 
a constant c. Then, for each α > 0, gα : X → R is bounded below by c, and is 
Lipschitz on each bounded set of X (and in particular is finite valued). Now take 
α > 0 and a point x ∈ X is such that 
∂P gα(x) /= ∅.
Then there exists a point y ∈ X satisfying the following conditions, 
(i): if {yi} is a minimizing sequence for the infimum in (4.12.9), then limi→∞
yi = y, 
(ii): the infimum in (4.12.9) is attained uniquely at y, 
(iii): the Fréchet derivative Dgα(x) exists and Dgα(x) = 2α(x − y); furthermore, 
∂P g(x) = {2α(x − y)} and 
gα(x'
) − gα(x) ≤ 〈2α(x − y), x' − x〉 + α||x' − x||2, for all x' ∈ X,
(iv): 2α(x − y) ∈ ∂P g(y), 
(v): if, in addition, g is Lipschitz continuous with Lipschitz constant kg, then gα is 
Lipschitz continuous with the same Lipschitz constant kg.208 4 Nonsmooth Analysis
Proof Consider a proper lower semi-continuous function g : X → R ∪ {+∞}
which is bounded below by a constant c. Fix α > 0. From the definition of gα it 
immediately follows that gα(x) ≥ c, for all x ∈ X. Now, let B ⊂ X be a bounded 
set, and take R > 0 such that B ⊂ RBX. Fix any x0 ∈ dom g (/= ∅). Observe that 
gα(x) ≤ g(x0) + α||x0 − x||2, for all x ∈ X,
and therefore, restricting attention to the bounded set B, we have 
gα(x) ≤ g(x0) + α(||x0|| + R)2, for all x ∈ B.
Write N := g(x0) + α(||x0|| + R)2 and, for any δ > 0 define the (nonempty) set 
Cδ := {z ∈ X : there exists u ∈ B s.t. g(z) + α||u − z||2 ≤ N + δ}.
Since B is bounded and g is bounded below, it follows that Cδ is bounded in X. Fix 
an arbitrary δ > 0. Take any x, x' ∈ B. From the definition of gα we know that we 
can find z ∈ Cδ such that 
gα(x'
) ≥ g(z) + α||x' − z||2 − δ.
Therefore we obtain: 
gα(x) − gα(x'
) ≤ gα(x) − g(z) − α||x' − z||2 + δ
≤ g(z) + α||x − z||2 − g(z) − α||x' − z||2 + δ
= α||x − x'
||2 + 2α〈x − x'
, x' − z〉 + δ
≤ α||x − x'
||(||x − x'
|| + 2||x' − z||) + δ
≤ kB||x − x'
|| + δ ,
in which kB := 6αR + 2(N + δ + |c|) (≥ α sup{||y − y'
|| + 2||y' − z|| : y, y' ∈
B and z ∈ Cδ}). Exchanging the roles of x and x' and letting δ ↓ 0, we obtain that 
gα is Lipschitz continuous of Lipschitz constant kB on B. 
Suppose now that x ∈ X is such that ∂P g(x) /= ∅. Take a vector ξ ∈ ∂P g(x). 
From the definition of proximal subgradient, there exist M > 0 and ϵ > 0 such that 
〈ξ,x' − x〉 ≤ gα(x'
) − gα(x) + M||x' − x||2, for all x' ∈ x + ϵBX. (4.12.10) 
Consider a minimizing sequence {yi} for the infimum in (4.12.9). Then we can find 
a sequence δi ↓ 0, as i → ∞ such that 
gα(x) ≤ g(yi) + α||yi − x||2 = gα(x) + δ2
i . (4.12.11)4.12 Appendix: Proximal Analysis in Hilbert Space 209
Recalling the definition of gα we also know that 
gα(x'
) ≤ g(yi) + α||yi − x'
||2, for all x' ∈ x + ϵBX. (4.12.12) 
Using (4.12.10), (4.12.11) and (4.12.12), it follows that, for all x' ∈ x + ϵBX, 
〈ξ,x' − x〉 ≤ α||yi − x'
||2 − α||yi − x||2 + δ2
i + M||x' − x||2
= −2α〈yi, x' − x〉 + α||x'
||2 − α||x||2 + δ2
i + M||x' − x||2
= 2α〈x − yi, x' − x〉 + (M + α)||x' − x||2 + δ2
i ,
and so 
〈ξ − 2α(x − yi), x' − x〉 ≤ (M + α)||x' − x||2 + δ2
i , for all x' ∈ x + ϵBX.
(4.12.13) 
We claim that for all i large enough we have 
||ξ − 2α(x − yi)|| ≤ δi(M + α + 1) . (4.12.14) 
Indeed, for each i, either ξ = 2α(x − yi) (and then (4.12.14) is trivially valid) or 
ξ /= 2α(x − yi); in the latter case we take the point x' = x + δi ξ−2α(x−yi)
||ξ−2α(x−yi)|| , which 
for i large enough belongs to x + ϵBX, and so we can easily deduce (4.12.14) from 
(4.12.13). Now we define y := x − ξ
2α . Therefore, letting i → ∞ in (4.12.14), we 
obtain (i). 
Notice also that invoking (4.12.9), property (i), (4.12.11) and the lower semicon￾tinuity of g we see that 
gα(x) ≤ g(y) + α||y − x||2 ≤ lim inf
i→∞ (g(yi) + α||yi − x||2) = gα(x) ,
and so gα(x) = g(y) + α||y − x||2 which means that y achieves the minimum in 
(4.12.9). Moreover, y is necessarily unique, since, if w ∈ X is another minimizer 
for (4.12.9), then we can consider the minimizing sequence {wi ≡ w} and, from (i) 
we know that limi→∞ wi = y, which yields w = y. This confirms (ii). 
To see (iii) we first observe that for all x' ∈ X we have gα(x'
) ≤ g(y) +
α||x' − y||2, and taking x' = x, from the analysis above we also know that gα(x) =
g(y) + α||x − y||2. Then we obtain 
gα(x) − gα(x'
) ≥ 2α〈x' − x, x − y〉 − α||x' − x||2
and so 
gα(x'
) − gα(x) − 2α〈x' − x, x − y〉 ≤ α||x' − x||2. (4.12.15)210 4 Nonsmooth Analysis
Then, from (4.12.10) (with ξ = 2α(x − y)) and (4.12.15) we deduce 
|gα(x'
) − gα(x)−〈2α(x − y), x'
−x〉|
||x'
−x|| ≤ max{M,α}||x'
−x||, for all x' ∈ x+ϵBX,
which means that gα is Fréchet differentiable at x and Dgα(x) = 2α(x − y). We 
claim that ∂P g(x) = {2α(x − y)}={Dgα(x)}. Indeed, take any other vector ξˆ ∈
∂P g(x); from the definition of proximal subgradient we have, for some M >ˆ 0 and 
ϵ >ˆ 0, 
〈ξ,x ˆ ' − x〉 ≤ gα(x'
) − gα(x) + Mˆ ||x' − x||2, for all x'
∈x + ˆϵBX.
Then we obtain 
0 ≤ lim inf
x'
→x
gα(x'
) − gα(x) − 〈ξ,x ˆ ' − x〉
||x' − x||
≤ lim inf
x'
→x

gα(x'
) − gα(x) − 〈ξ,x' − x〉
||x' − x|| + 〈ξ − ξ,x ˆ ' − x〉
||x' − x|| 
= 0 + lim inf
x'
→x
〈ξ − ξ,x ˆ ' − x〉
||x' − x||
≤ −||ξ − ξˆ||,
that implies ξˆ = ξ , confirming the claim. To conclude with the proof of (iii), we 
observe that (4.12.15) yields also 
gα(x'
) − gα(x) ≤ 〈2α(x − y), x' − x〉 + α||x' − x||2, for all x' ∈ X.
We show now (iv). Observe that x' → ||x' − x||2 is of class C2, and D[x' →
||x' − x||2](y) = 2(y − x) and the function x' → φ(x'
) := g(x'
) + α||x' − x||2 has 
its minimum value at x' = y. Therefore, 0 ∈ ∂P φ(y) = ∂P g(y)+2α(y −x) (recall 
that the same arguments employed to derive Proposition 5.1.1 and Lemma 5.1.2 in 
the finite dimensional case can be easily extended to the case when X is a general 
Hilbert space). Then −2α(y − x) ∈ ∂P g(y). 
Assume now that g : X → R is Lipschitz continuous with Lipschitz constant kg. 
For all x, x' ∈ X we have 
gα(x) = inf
z∈X
{g(z) + α||x − z||2} = inf
w∈X
{g(w − (x' − x)) + α||w − x'
||2}
≤ inf
w∈X
{g(w) + α||w − x'
||2} + kg||x − x'
||
= gα(x'
) + kg||x − x'
||.4.12 Appendix: Proximal Analysis in Hilbert Space 211
Exchanging the roles of x and x' we obtain that |gα(x) − gα(x'
) ≤ kg||x − x'
||
for all x, x' ∈ X confirming the fact that gα is Lipschitz continuous of Lipschitz 
constant kg. This concludes the proof. ⨅⨆
These analytical tools can be used to prove the variational principle of Borwein 
and Preiss and also that of Stegall, stated in Chap. 3. 
Proof of Theorem 3.5.1 (Borwein and Preiss) Take a lower semi-continuous func￾tion f : X → R ∪ {+∞} which is bounded below (on the Hilbert space X), a point 
x0 ∈ dom f and a number ϵ > 0. Assuming that 
f (x0) < inf
x∈X f (x) + ϵ. (4.12.16) 
we want to show that, for any λ > 0, there exist x¯ and z in X such that 
(i) f (x)¯ ≤ f (x0), 
(ii) ||z − x0|| < λ and || ¯x − z|| < λ, 
(iii) the function x → f (x) + ϵ
λ2 ||x − z||2 has a unique minimizer on X at x = ¯x. 
Fix any λ > 0. Consider the inf-convolution of f with α := ϵ
λ2 : 
fα(x) := inf
y∈X
{f (y) +
ϵ
λ2 ||y − x||2} for all x ∈ X . (4.12.17) 
From Theorem 4.12.3 (on the inf-convolution properties) we know that fα is finite 
valued on X (so x0 ∈ dom fα = X). Owing to the proximal density theorem 
(Theorem 4.12.2) we can find a point z ∈ x0 + λ
◦
BX such that ∂P fα(z) = ∅ / and 
fα(z) ≤ fα(x0). (4.12.18) 
Now, property (ii) of Theorem 4.12.3 (on inf-convolution) guarantees the existence 
of a unique point x¯ ∈ X such that the infimum which provides the definition of 
fα(z) (cf. (4.12.17)) is attained: 
fα(z) = inf
y∈X
{f (y) +
ϵ
λ2 ||y − z||2} = f (x)¯ +
ϵ
λ2 || ¯x − z||2 . (4.12.19) 
This yields (iii). Moreover, since clearly fα ≤ f , from (4.12.18) and (4.12.19) we 
deduce also that 
f (x)¯ ≤ f (x)¯ +
ϵ
λ2 || ¯x − z||2 = fα(z) ≤ fα(x0) ≤ f (x0) , (4.12.20) 
confirming property (i). Concerning (ii), since from the analysis above we already 
have z ∈ x0 + λ
◦
BX, it remains to show only that || ¯x − z|| < λ. In view of (4.12.20) 
and (4.12.16) it follows that212 4 Nonsmooth Analysis
f (x)¯ +
ϵ
λ2 || ¯x − z||2 = fα(z) ≤ f (x0) < inf
x∈X f (x) + ϵ ,
which, subtracting the term f (x)¯ , implies that 
ϵ
λ2 || ¯x − z||2 < inf
x∈X f (x) − f (x)¯ + ϵ ≤ ϵ ,
and, so, || ¯x − z|| < λ. This concludes the proof. ⨅⨆
Proof of Theorem 3.5.2 (Stegall) Take a lower semi-continuous function f : X →
R ∪ {+∞} which is bounded below on a bounded subset Y of a real Hilbert space 
X. We assume that Y ∩ dom f = ∅ / . Our aim is to show that there exists a dense set 
of points x ∈ X such that 
z → f (z) − 〈x,z〉
attains a unique minimum over Y . 
Define the function 
ϕ(x) := inf
y∈X
{f (y) + ΨY (y) − 1
2
||y||2 +
1
2
||x − y||2} (4.12.21) 
where ΨY denotes the indicator function of the subset Y ⊂ X (recall that ΨY (z)
takes the value 0 if z ∈ Y , otherwise it takes the value +∞). Observe that ϕ is 
the inf-convolution of the function g(x) := f (x) + ΨY (x) − 1
2 ||x||2, x ∈ X, with 
α = 1
2 . Notice that g is bounded below since f is bounded below and Y is bounded. 
Moreover the term which defines ϕ (4.12.21) can be easily simplified to 
ϕ(x) = inf
y∈Y
{f (y) − 〈x, y〉} +
1
2
||x||2. (4.12.22) 
Fix x ∈ X. Observe that the set of points y ∈ X achieving an infimum in 
(4.12.21) or in (4.12.22) or also in the following expression 
inf
y∈Y
{f (y) − 〈x, y〉} (4.12.23) 
is the same and is contained in Y . Theorem 4.12.3 (on the inf-convolution proper￾ties) guarantees that dom ϕ = X. The proximal density theorem (Theorem 4.12.2) 
tells us that there exists a dense set D ⊂ X such that ∂P ϕ(x) /= ∅ for all x ∈ D. 
Invoking again Theorem 4.12.3 we have that for each x ∈ D the infimum in 
(4.12.21) is uniquely attained at a point yx ∈ Y . Therefore, for each x ∈ D, also the 
infimum in (4.12.23) is uniquely achieved at the (same) point yx ∈ Y . ⨅⨆4.13 Exercises 213
4.13 Exercises 
4.1 (This Exercise Provides Some Details of the Proofs of Theorems 4.7.7 and 5.7.1. 
Take a non-zero vector v ∈ Rn. Let Ω ⊂ Rn be a null-set w.r.t. n-dimensional 
Lebesgue measure. For x ∈ R, denote by Lx the line Lx := {x +tv : t ∈ R}. Show 
that there exists a subset S ⊂ Rn, whose complement in Rn has zero n-dimensional 
Lebesgue measure, with the following property: for any x ∈ S, Lx ∩ Ω has zero 
one-dimensional Lebesgue measure, i.e. the line Lx touches the set Ω on a set of 
zero one-dimensional Lebesgue measure. 
Hint: We can assume |v| = 1, since Ly is unaffected by scaling v. By considering 
the orthogonal (Lebesgue measure preserving) transformation x → T x, in which 
T ∈ Rn×n is an orthonormal matrix whose first column is the vector v, we can 
justify restricting attention to the case when v = (1, 0,..., 0). Define m : Rn → R
to be m(x) =

1 if x ∈ Ω
0 otherwise . Since m is the indicator function of the nullset Ω, m
is integrable and 
 ∞
−∞
...  ∞
−∞
m(x1,...,xn)dx1 ...dxn = 0 .
Define m1 : Rn−1 → R to be m1(x2,...,xn) :=  ∞
−∞ m(x1, x2,...,xn)dx1. By 
Fubini’s theorem, 
 ∞
−∞
...  ∞
−∞
m1(x2,...,xn)dx2 ...dxn = 0 .
But, then, there exists a set Mn−1, whose complement in Rn−1 has zero (n − 1)-
dimensional Lebesgue measure, such that m1(x2,...,xn) = 0 for all (x2,...,xn) ∈
Mn−1. Now define S ⊂ Rn to be 
S := R × Mn−1 .
Show that S is the complement of a set of n-dimensional Lebesgue measure in Rn
and has the stated intersection properties. 
4.2 Take a multifunction F : [S, T ] × Rn ⇝ Rn. Assume that F takes values 
compact sets. Consider the Hamiltonian 
H (t, x, p) := max
v∈F (t,x) p · v .
Show that 
(i): if F (t, .) is k(t)-Lipschitz, then x → H (t, x, p) is |p|k(t)-Lipschitz continu￾ous,214 4 Nonsmooth Analysis
(ii): if (ξ , η) ∈ co ∂x,pH (t, x, p), then η ∈ ∂pH (t, x, p), and H (t, x, p) = η · p; 
moreover, 
(iii): ∂pH (t, x, p) ⊂ co F (t, x). 
Remark 
The properties expressed in this exercise are used several times in the book (cf. 
Chaps. 8 and 11). Observe, in particular, that relations (ii) and (iii) can be used to 
derive the Weierstrass condition from the Hamiltonian inclusion or even the partially 
convexified Hamiltonian inclusion. 
Hint: For (ii) and (iii) use Theorem 4.7.7 and the fact that the map p → H (t, x, p)
is convex. 
4.3 Let C ⊂ Rk be a closed nonempty set. Take a point x ∈ C. Show that ξ ∈
NP
C (x) if and only if there exist M > 0 and ϵ > 0 such that 
ξ · (y − x) ≤ M|y − x|
2, for all y ∈ (x + ϵB) ∩ C .
Remark 
This exercise highlights a useful (cf. the proof of Lemma 8.6.3) local property of 
proximal normal vectors; the property stated in this exercise tells us also that 
ξ ∈ NP
C (x) if and only if ξ ∈ ∂P ΨC(x).
(ΨC is the indicator function of the set C.) 
4.14 Notes for Chapters 4 and 5 
Developments in convex analysis in 1960s, centred substantially on Rockafellar’s 
contributions, revealed that, for purposes of characterizing minimizers to convex 
optimization problems with possibly non-differentiable data, tangent cones, normal 
cones and set-valued subdifferentials can in many ways do the work of tangent 
spaces, co-tangent spaces and derivatives respectively in traditional analysis. 
A desire to reproduce some of these successes in a nonconvex setting was the 
impetus behind the field of nonsmooth analysis, initiated in the following decade. 
The breakthrough was the introduction by Clarke in his 1973 thesis [56] of various 
‘robust’ nonsmooth constructs, including the generalized gradient of a Lipschitz 
continuous function and what are now widely referred to as the Clarke normal cone 
and Clarke tangent cone to a closed set [56, 57, 65]. Numerous earlier definitions 
of ‘derivative’ of a nondifferentiable function had previously been proposed, but 
the generalized gradient was the first to stand out for its generality, its extensive 
calculus, geometric interpretations and the breadth of its applications, notably in the 
derivation of necessary conditions of optimality.4.14 Notes for Chapters 4 and 5 215
Once the idea of local approximation of nonsmooth functions and sets with 
nonsmooth boundaries by means of cones and set valued ‘gradients’ was out the 
bag, there was an explosion of definitions. Aubin and Frankowska’s book [14], 
which includes a systematic study of the ‘menagerie’ of tangent cones, obtained 
by attaching different qualifiers to the limit operations in their definitions, and 
derivatives which they induce by consideration of tangent cones to graphs of 
functions, is evidence of this. 
Chapters 4 and 5 concern those aspects of nonsmooth analysis required to support 
future chapters on necessary conditions and dynamic programming in dynamic 
optimization. Here, fortunately, we need to consider only relatively few constructs. 
Much of the material in these chapters focuses on the limiting normal cone (and 
the related limiting subdifferential of a lower semi-continuous function, defined 
via limiting normals to the epigraph set). But these chapters also feature a limited 
repertoire of tangent cones of relevance to our analysis. All material is more or less 
standard. Proofs of basic properties of limiting normals and subdifferentials follow, 
in many respects, those in [175] and [141] and ideas implicit in [65]. Proofs of the 
relations between the various normal and tangent cones considered here in many 
respects follow those in [14]. Material on generalized gradients in Sect. 4.7 is taken 
from [65]. Proofs of the results contained in Sect. 4.11 and in the Appendix this 
chapter follow those of Clarke et al. in [85]. 
The limiting normal cone was introduced by Mordukhovich in his 1976 paper 
[154], where it was used to formulate generalized transversality conditions in 
dynamic optimization. The limiting normal cone is also referred to as the approxi￾mate normal cone by Ioffe [123] and simply as the normal cone (by Mordukhovich 
[157] and, recently, by Rockafellar and Wets [177]). The names ‘co-derivative’ and 
‘subdifferential’ are also used for the limiting subdifferential, in [157] and [177] 
respectively. These constructs were studied in detail in papers of Mordukhovich, 
Kruger and Ioffe, including [134, 156] and [123]. More recent expository treatments 
are provided in [157, 177] and [141]. Proximal normal vectors, the generators of the 
limiting normal cone, were used earlier by Clarke to prove properties of (Clarke) 
normal cones and generalized gradients [57]. But focusing attention on limiting 
normal cones as interesting objects in their own right was a significant departure. 
The proximal normal cone, limiting normal cone and its convex hull the 
Clarke normal cone, together with their associated subdifferentials (the proximal 
subdifferential, limiting subdifferential and the generalized gradient respectively), 
all figure prominently in nonsmooth dynamic optimization. Of these constructs, 
the generalized gradient is well suited to the formulation of adjoint inclusions for 
nonsmooth dynamics, by virtue of its convexity properties. On the other hand, the 
limiting normal cone is a natural choice of normal cone to express transversality 
conditions for general endpoint constraint sets. The proximal subdifferential is a 
convenient vehicle for the interpretation of generalized solutions to the Hamilton 
Jacobi equation of dynamic programming. 
Lebourg [136] proved the first nonsmooth mean value theorem (the two sided 
mean value theorem for Lipschitz functions). Clarke and Ledyaev’s approximate 
mean value inequality [70], the uniform nature of which was novel even for216 4 Nonsmooth Analysis
smooth functions, has found numerous applications—in fixed point theorems, the 
interpretation of generalized solutions to partial differential inequalities and other 
areas. (See also [71] and [85].) Approximate generalizations of the mean value 
theorem were earlier investigated by Zagrodny [211] and Loewen [142]. 
We stick for the most part to nonsmooth analysis in finite dimensional spaces. 
This suffices for most topics in dynamic optimization in this book. However 
dynamic optimization problems are inherently infinite dimensional. The investiga￾tion of certain issues in dynamic optimization, such as sensitivity of the minimum 
cost to ‘infinite dimensional perturbations’ of the dynamics [63], as well as some 
alternative proofs of results in this book, involve the application of nonsmooth anal￾ysis in infinite dimensional spaces. Clarke has shown that his theory of generalized 
gradients can be developed, with the accompanying calculus largely intact, in the 
context of Lipschitz continuous functions on Banach spaces [65]. A number of 
researchers have been involved in building alternative frameworks for nonsmooth 
analysis in infinite dimensions, including Clarke, Borwein, Ioffe, Loewen and 
Mordukhovich. Borwein and Zhu has provided a helpful, detailed review [44], 
see also [45]. An appealing approach for its simplicity and broad applicability, 
followed by Clarke, Ledyaev, Stern and Wolenski [85], centres on proximal normals 
in Hilbert space. Proximal normal cones and proximal subdifferentials do not have 
a satisfactory exact calculus. They do however admit a rich fuzzy calculus. Fuzzy 
calculus, initiated by Ioffe (see [125]), provides rules, involving ϵ error terms which 
can be made arbitrarily small, for the estimation of proximal subdifferentials of 
composite functions. The idea is to retain ϵ terms in the general theory and to 
attempt to dispose of them only at the applications stage, using special features of 
the problem at hand. The Appendix (to Chap. 4) contains some material on proximal 
analysis in Hilbert spaces, as required to prove the Borwein and Preiss and Stegall 
variational principles of Chap. 3.Chapter 5 
Subdifferential Calculus 
Abstract The subject matter of this chapter is a calculus governing the limiting 
subdifferentials of composite functions, that is functions that are composed from 
lower semi-continuous extended valued functions and indicator functions of closed 
sets. It is remarkable the extent to which a subdifferential calculus can be developed, 
reproducing aspects of classical calculus governing differential properties of smooth 
functions, when the functions involved are no longer smooth. A distinctive feature of 
subdifferential calculus is that, typically, it does not provide precise representations 
of the limiting subdifferentials of composite functions, but only estimates sets for 
these limiting subdifferentials. We have already encountered a nonsmooth mean 
value theorem in the previous chapter. The subdifferential calculus presented in 
this chapter provides versions of the sum rule, and chain rule. It also provides a 
widely used ‘max rule’ for composite functions, which has no parallel classical 
calculus. To give a foretaste of techniques employed in later chapters, we derive a 
very general Lagrange multiplier in nonlinear programming, using subdifferential 
calculus in harness with a variational principle. 
5.1 Introduction 
In this chapter, we assemble a number of useful rules for calculating and estimating 
the limiting subdifferentials of composite functions in terms of their constituent 
mappings. A typical rule is the ‘sum rule’ 
∂(f + g)(x) ⊂ ∂f (x) + ∂g(x). (5.1.1) 
(The right side denotes, of course, the set of all vectors ξ expressible as ξ = η1 +η2
for some η1 ∈ ∂f (x) and η2 ∈ ∂g(x).) 
We notice at once that the rule is ‘one sided’: it takes the form of a set inclusion 
not a set equivalence. Fortunately the inclusion that comes naturally is in the helpful 
direction: we want an estimate of ∂(f + g) in terms of the subgradients of the 
(usually simpler) functions f and g of which f + g is constituted. It is inevitable 
that inclusions feature in these rules if they are to handle functions which are locally 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
P. Bettiol, R. B. Vinter, Principles of Dynamic Optimization, Springer Monographs 
in Mathematics, https://doi.org/10.1007/978-3-031-50089-3_5
217218 5 Subdifferential Calculus
Lipschitz continuous (or even less regular). This point is illustrated in the following 
example. 
Example 
Take f : R → R and g : R → R to be the Lipschitz continuous functions 
f (x) = min{0, x} and g(x) = − min{0, x}.
Then 
∂(f + g)(0) = {0}.
Yet 
∂f (0) = {0}∪{1} and ∂g(0) = [−1, 0].
We see that 
∂(f + g)(0)
strict
⊂ ∂f (0) + ∂g(0).
It is desirable to have calculus rules which apply to general lower semi￾continuous functions. But even a ‘one sided’ rule such as (5.1.1) may fail unless 
some non-degeneracy hypothesis is imposed. The precise nature of the hypotheses 
required will differ from rule to rule, but in each case they will eliminate certain 
kinds of interaction of the relevant asymptotic limiting subdifferentials. These 
nondegeneracy hypotheses are automatically satisfied when the functions involved 
are Lipschitz continuous. 
Example 
Take the function s : R → R to be 
s(x) := sgn {x}|x|
1
2
and define the functions f : R → R and g : R → R according to 
f (x) = s(x) and g(x) = −s(x).
We find that ∂f (0) = ∂g(0) = ∅ and ∂(f + g)(0) = {0}. So 
{0} = ∂(f + g)(0) /⊂ ∂f (0) + ∂g(0) = ∅.
Notice that ∂∞f (0) = [0,∞) and ∂∞g(0) = (−∞, 0]. The pathological aspect of 
this example is that there exist nonzero numbers a and b, such that a ∈ ∂∞f (0), 
b ∈ ∂∞g(0) and a + b = 0. (Take a = 1 and b = −1 for example.) The data then 
fail to satisfy the condition:5.1 Introduction 219
‘a ∈ ∂∞f (0), b ∈ ∂∞g(0) and a + b = 0’ implies ‘a = b = 0’,
which will turn out to be precisely the nondegeneracy hypothesis appropriate to the 
sum rule. 
For a concept of subdifferential to be useful in the field of optimization, we 
require it, at the very least, to have the property: ‘If f achieves its minimum value 
at x¯ ∈ dom f , then {0} is contained in the subdifferential of f at x¯’. Fortunately 
this is true even of the ‘tightest’ subdifferential we have introduced, the proximal 
subdifferential. 
Proposition 5.1.1 Take a lower semi-continuous function f : Rk → R ∪ {+∞}
and a point x ∈ dom f . Assume that x achieves the minimum value of f over a 
neighbourhood of x, then 
0 ∈ ∂P f (x).
Proof The fact that x is a local minimum means that there exists ϵ > 0 such that 
ξ · (y − x) ≤ f (y) − f (x) + M|y − x|
2 for all y ∈ x + ϵB
when we take ξ = 0 and any M ≥ 0. We conclude from Proposition 4.4.1 that 
0 ∈ ∂P f (x). ⨅⨆
To make a start, we shall need the following rudimentary sum rule: 
Lemma 5.1.2 Take functions f : Rk → R ∪ {+∞} and g : Rk → R, a closed set 
C ⊂ Rk and a point x ∈ (intC)∩(dom f ). Assume that f is lower semi-continuous 
and g is of class C2 on a neighbourhood of x. Then 
∂P (f + g + ΨC)(x) = ∂P f (x) + {∇g(x)},
∂(f + g + ΨC)(x) = ∂f (x) + {∇g(x)},
∂∞(f + g + ΨC)(x) = ∂∞f (x).
(As usual, ΨC denotes the indicator function of the set C.) 
Proof For some ϵ > 0 such that x + ϵB ⊂ intC, there exists m > 0 such that 
|g(x'
) − g(x) − ∇g(x) · (x' − x)| ≤ m|x' − x|
2
for all x' ∈ x + ϵB, since g is assumed C2 on a neighbourhood of x. Take any 
y ∈ x + ϵB, ξ ∈ Rk. Then, in consequence of this observation, there exists M > 0
and δ > 0 such that 
(ξ + ∇g(x)) · (x' − x) ≤ (f + g + ΨC)(x'
) − (f + g + ΨC)(x) + M|x' − x|
2220 5 Subdifferential Calculus
for all x' ∈ x + δB if and only if there exists M' > 0 and δ' > 0 such that 
ξ · (x' − x) ≤ f (x'
) − f (x) + M'
|x' − x|
2
for all x' ∈ x + δ'
B. 
It follows that ξ + ∇g(x) ∈ ∂P (f + g + ΨC)(x) if and only if ξ ∈ ∂P f (x). We 
have confirmed the first assertion of the lemma. 
The remaining assertions follow from Theorem 4.6.2, upon limit taking, when 
we note that x' f
→ x if and only if x' f +g+ΨC → x. ⨅⨆
5.2 A Marginal Function Principle 
Take a function F : Rk × Rl → R ∪ {+∞}. Let f : Rk → R ∪ {+∞} be the 
marginal function: 
f (x) := F (x, 0) for all x ∈ Rk
and let x¯ be a minimizer for f : 
f (x)¯ = min {f (x) : x ∈ Rk}.
In convex optimization there is a standard procedure for constructing a function 
g : Rl → R ∪ {−∞} such that 
f (x)¯ ≥ sup {g(v) : v ∈ Rl
}. (5.2.1) 
It is to introduce the Fenchel conjugate functional F∗ : Rk × Rl → R ∪ {+∞}: 
F∗(y, v) := sup {x · y + u · v − F (x, u) : (x, u) ∈ Rk × Rl
}
and set 
g(v) := −F∗(0, v).
The validity of the ‘weak duality’ condition (5.2.1) follows directly from the 
definition of F∗. If F is jointly convex in its arguments and satisfies an appropriate 
nondegeneracy hypothesis then condition (5.2.1) can be replaced by the stronger 
condition: there exists v¯ ∈ Rl such that 
f (x)¯ = sup{g(v) : v ∈ Rl
} = g(v). ¯ (5.2.2)5.2 A Marginal Function Principle 221
Assertions of this nature (‘duality principles’) have an important role in the 
derivation of optimality conditions in convex optimization and can be used as a 
starting point for developing a subdifferential calculus for convex functions. 
Now suppose that F is no longer convex. Then it will be possible to find v¯ such 
that (5.2.2) is true only under very special circumstances. Surprisingly however we 
can still salvage some ideas from the above constructions for analysing nonconvex 
functions. 
The key observation is that condition (5.2.2) can be expressed 
F (x,¯ 0) + F∗(0, v)¯ = ¯x · 0 + 0 · ¯v
or, alternatively (in terms of the subdifferential of F in the sense of convex analysis), 
(0, v)¯ ∈ ∂F(x,¯ 0). (5.2.3) 
The above duality principle can therefore be formulated as: there exists v¯ ∈ Rl such 
that (5.2.3) is satisfied. The advantage of writing the condition in this way is that 
it makes no reference to conjugate functionals and is suitable for generalization to 
situations where F is no longer convex. 
We now prove a theorem, a corollary of which will assert the following: Suppose 
x¯ is a minimizer for x → F (x, 0). Then, under a mild nondegeneracy hypothesis 
on F, there exists v¯ such that (5.2.3) is true, when ∂F is interpreted as a limiting 
subdifferential. As we have discussed, this ‘marginal function’ principle has a 
similar role in the derivation of nonsmooth calculus rules as that of duality principles 
in convex analysis. 
Theorem 5.2.1 (Marginal Function Principle) Take a lower semi-continuous 
function F : Rn × Rm → R ∪ {+∞} and a point (x,¯ u)¯ ∈ dom F. Assume that 
(i) There exists a bounded set K ⊂ Rn such that 
dom F (., u) ⊂ K for all u ∈ Rm,
(ii) x¯ is the unique minimizer of x → F (x, u)¯ . 
Define V : Rm → R ∪ {+∞} to be 
V (u) := min
x∈Rn F (x, u).
Then V is a lower semi-continuous function and 
∂V (u)¯ ⊂ {ξ : (0,ξ) ∈ ∂F(x,¯ u)¯ },
∂∞V (u)¯ ⊂ {ξ : (0,ξ) ∈ ∂∞F (x,¯ u)¯ }.222 5 Subdifferential Calculus
Proof We begin by establishing the following properties of F and V : 
(a): For every u ∈ Rm, x → F (x, u) has a minimizer (with infinite cost if 
dom F (., u) = ∅), 
(b): V is lower semi-continuous, 
(c): Given any sequences ui
V
→ ¯u and {xi} such that V (ui) = F (xi, ui) for each i, 
then (xi, ui) F
→ (x,¯ u)¯ . 
Take any u ∈ Rm. If dom F (., u) = ∅, then any x is a minimizer. If 
dom F (., u) /= ∅ then the search for the minimizer of F (., u) must be carried out 
over the non-empty set dom F (., u). But this set is compact, since F is lower semi￾continuous and dom F (., u) is assumed bounded. A minimizer exists then in this 
case too. We have shown (a). 
Take any sequence ui → u. We wish to show lim infi V (ui) ≥ V (u). If all but 
a finite number of the V (ui)’s are infinite there is nothing to prove. Otherwise we 
can replace the sequence by a subsequence along which V (ui) is finite. For each 
i choose a minimizer xi of the function x → F (x, ui). By (i), we can arrange by 
extracting a further subsequence that xi → x for some x ∈ Rm. We know then that 
(xi, ui) → (x, u). It follows now from the lower semicontinuity of F that 
lim inf
i
V (ui) = lim inf
i
F (xi, ui) ≥ F (x, u) ≥ V (u).
We have confirmed property (b). 
Take any sequence ui
V
→ ¯u and {xi} such that V (ui) = F (xi, ui) for each i. 
Then V (ui) < +∞ (for i sufficiently large), and the xi’s are confined to a compact 
set. Along a subsequence then, xi → x for some x ∈ Rn. By lower semicontinuity 
of F, 
V (u)¯ = lim
i
V (ui) = lim F (xi, ui) ≥ F (x, u)¯ ≥ V (u). ¯
It follows that 
F (xi, ui) → F (x, u). ¯
We see also that x = ¯x since F (., u)¯ has a unique minimizer. From the fact that 
the limits are independent of the subsequence extracted, we conclude that, for the 
original sequence, 
(xi, ui) F
→ (x,¯ u). ¯
Property (c) is confirmed. 
We are now ready to estimate subgradients. Let F, V and (x,¯ u)¯ be as in the 
theorem. Take any ξ ∈ ∂V (u)¯ . According to Theorem 4.6.2 there exist sequences5.2 A Marginal Function Principle 223
ui
V
→ ¯u and ξi → ξ such that ξi ∈ ∂P V (ui) for all i. For each i, there exist ϵi > 0
and Mi > 0 such that 
ξi · (u − ui) ≤ V (u) − V (ui) + Mi|u − ui|
2 for all u ∈ ui + ϵiB. (5.2.4) 
Let xi be a minimizer for x → F (x, ui). Then, since V (u) ≤ F (x, u) for all x and 
u, we deduce from (5.2.4) that 
(0, ξi) · (x − xi, u − ui) ≤ F (x, u) − F (xi, ui) + Mi|(x, u) − (xi, ui)|
2
for all (x, u) ∈ (xi, ui) + ϵiB. It follows that 
(0, ξi) ∈ ∂P F (xi, ui).
In view of property (c) above, (xi, ui) F
→ (x,¯ u)¯ . By Theorem 4.6.2 then 
(0,ξ) ∈ ∂F(x,¯ u). ¯
Next take ξ ∈ ∂∞V (u)¯ . We know that there exist sequences ui
V
→ ¯u, ξi → ξ
and ti ↓ 0 such that t
−1
i ξi ∈ ∂P V (ui) for all i. The preceding arguments yield 
t
−1
i (0, ξi) ∈ ∂P F (xi, ui).
Here xi is a minimizer for x → F (x, ui). Recalling that (xi, ui) F
→ (x,¯ u)¯ , we 
conclude that (0,ξ) ∈ ∂∞F (x,¯ u)¯ . ⨅⨆
Frequently the theorem is used in the form of the following corollary: 
Corollary 5.2.2 Take a lower semi-continuous function F : Rn×Rm → R∪{+∞}
and a point (x,¯ u)¯ ∈ dom F. Assume that x¯ minimizes x → F (x, u)¯ over some 
neighbourhood of x¯. Suppose that 
{η : (0, η) ∈ ∂∞F (x,¯ u)¯ }={0}.
Then there exists a point ξ ∈ Rm such that 
(0,ξ) ∈ ∂F(x,¯ u). ¯
Proof Since x¯ is a local minimizer of F (., u)¯ , there exists ϵ > 0 such that F (x,¯ u)¯ =
min{F (x, u)¯ : x ∈ ¯x + ϵB}. Set C := ¯x + ϵB. Choose any r > 0. Define F˜ :
Rn × Rm → R ∪ {+∞}: 
F (x, u) ˜ := F (x, u) + r|x − ¯x|
2 + ΨC(x)224 5 Subdifferential Calculus
Define also 
V (u) := inf
x F (x, u). ˜
The function F˜ satisfies the hypotheses of Theorem 5.2.1. Notice in particular that 
the ‘penalty term’ r|x − ¯x|
2 ensures that F (., ˜ u)¯ has a unique minimizer. Bearing in 
mind that the limiting subgradients and asymptotic limiting subgradients of F and 
F˜ at (x,¯ u)¯ coincide (Lemma 5.1.2), we deduce that 
∂V (u)¯ ⊂ {ξ : (0,ξ) ∈ ∂F(x,¯ u)¯ } and ∂∞V (u)¯ ⊂ {ξ : (0,ξ) ∈ ∂∞F (x,¯ u)¯ }.
But under the non-degeneracy hypothesis, ∂∞V (u)¯ = {0}. We know from 
Corollary 4.9.2 and Proposition 4.7.1 that ∂V (u)¯ is non-empty. It follows that there 
exists some ξ ∈ Rm such that (0,ξ) ∈ ∂F(x,¯ u)¯ . ⨅⨆
5.3 Partial Limiting Subgradients 
The first calculus rule relates the partial limiting subdifferential of a function of two 
variables and the projection of the ‘total’ limiting subdifferential on to the relevant 
coordinate. 
Theorem 5.3.1 (Partial Limiting Subgradients) Take a lower semi-continuous 
function f : Rn × Rm → R ∪ {+∞} and a point (x,¯ y)¯ ∈ dom f . Assume that 
(0, η) ∈ ∂∞f (x,¯ y)¯ implies η = 0.
Then: 
∂xf (x,¯ y)¯ ⊂ {ξ : there exists η such that (ξ , η) ∈ ∂f (x,¯ y)¯ }
∂∞
x f (x,¯ y)¯ ⊂ {ξ : there exists η such that (ξ , η) ∈ ∂∞f (x,¯ y)¯ }.
Proof Take any ξ ∈ ∂xf (x,¯ y)¯ . We know then that there exist ξi → ξ and xi
f (.,y)¯ → ¯x
such that ξi ∈ ∂P
x f (xi, y)¯ for all i. For each i then we may choose ϵi > 0 and 
Mi > 0 such that xi is the unique minimizer of Fi(., y)¯ , where 
Fi(x, y) := f (x, y) − ξi · (x − xi) + Mi|x − xi|
2 + Ψxi+ϵiB(x).
Corollary 5.2.2 and Lemma 5.1.2 applied to Fi yield: either there exists η' ∈ Rm
such that (ξi, η'
) ∈ ∂f (xi, y)¯ or there exists a non-zero η'' ∈ Rm such that (0, η'') ∈
∂∞f (xi, y)¯ . There are two possible situations:5.3 Partial Limiting Subgradients 225
(a): for an infinite number of i’s there exists ηi ∈ Rm such that 
(ξi, ηi) ∈ ∂f (xi, y)¯
(b): for an infinite number of i’s there exists ηi ∈ Rm such that |ηi| = 1 and 
(0, ηi) ∈ ∂∞f (xi, y)¯
(we can arrange by scaling that |ηi| = 1 since ∂∞f (xi, y)¯ is a cone). 
Case (b) however is never encountered. This is because, in case (b), the ηi’s which 
satisfy (0, ηi) ∈ ∂∞f (xi, y)¯ have an accumulation point η satisfying (0, η) ∈
∂∞f (x,¯ y)¯ and |η| = 1, in view of the fact that (xi, y)¯ f
→ (x,¯ y)¯ . This is in violation 
of the non-degeneracy hypothesis. 
It remains then to attend to (a). We restrict attention to a subsequence, thereby 
ensuring that ηi, with the stated properties, exists for each i. 
It may be assumed that {ηi} is a bounded sequence for, otherwise, we are in 
a situation where, following a further subsequence extraction, |ηi|→∞ and 
|ηi|
−1ηi → v for some v with |v| = 1. But then, for ti := |ηi|
−1, we have 
ti ↓ 0 and ti(ξi, ηi) → (0, v).
Since (xi, y)¯ f
→ (x,¯ y)¯ , we conclude 
(0, v) ∈ ∂∞f (x,¯ y). ¯
This is ruled out by the non-degeneracy hypothesis. We have confirmed that {ηi} is 
a bounded sequence. 
By extracting a subsequence we can arrange that ηi → η for some η ∈ Rm. 
Along the subsequence (ξi, ηi) ∈ ∂f (xi, y)¯ and (xi, y)¯ f
→ (x,¯ y)¯ . It follows that 
(ξ , η) ∈ ∂f (x,¯ y)¯ . We have shown that 
∂xf (x,¯ y)¯ ⊂ {ξ : there exists η such that (ξ , η) ∈ ∂f (x,¯ y)¯ }.
Verification of the estimate governing the asymptotic partial limiting subdifferential 
∂∞
x f (x,¯ y)¯ ⊂ {ξ : there exists η such that (ξ , η) ∈ ∂∞f (x,¯ y)¯ }
is along similar lines. A sketch of the arguments involved is as follows. Take any 
ξ ∈ ∂∞
x f (x,¯ y)¯ . We deduce from Theorem 4.6.2 that there exist ξi → ξ , xi
f (.,y)¯ → x
and ti ↓ 0 such that 
ξi ∈ ti∂P
x f (xi, y). ¯226 5 Subdifferential Calculus
Once again there are two possibilities to be considered: 
(a)'
: for an infinite number of i’s there exists ηi ∈ Rm such that (ξi, ηi) ∈
ti∂f (xi, y)¯ , 
(b)'
: for an infinite number of i’s there exists ηi ∈ Rm such that |ηi| = 1 and 
(0, ηi) ∈ ti∂∞f (xi, y). ¯
(b)' is analogous to (b), a possibility which, as we have noted, cannot occur. So we 
may assume (a)'
. We deduce from the non-degeneracy hypothesis that the ηi’s are 
bounded and so have an accumulation point η. We deduce (as before) the required 
relation: 
(ξ , η) ∈ ∂∞f (x,¯ y). ¯
⨅⨆
5.4 A Sum Rule 
A general sum rule is another consequence of the marginal function principle. 
Theorem 5.4.1 (Sum Rule) Take lower semi-continuous functions fi : Rn → R ∪
{+∞}, i = 1,...,m, and a point x¯ ∈ ∩idom fi. Define f = f1 +...+fm. Assume 
that 
vi ∈ ∂∞fi(x)¯ for i = 1,...,m and 
i
vi = 0 imply vi = 0 for all i.
Then 
∂f (x)¯ ⊂ ∂f1(x)¯ + ... + ∂fm(x)¯
and 
∂∞f (x)¯ ⊂ ∂∞f1(x)¯ + ... + ∂∞fm(x). ¯
Proof Consider the function F : Rn × (Rn)m → R ∪ {+∞} defined by 
F (x, y) := m
i=1
fi(x + yi) for y = (y1,...,ym).
Evidently ∂f (x)¯ = ∂xF (x,¯ 0). This formula relating limiting subgradients of f
and partial limiting subgradients of F provides the link with the marginal function 
principle.5.4 A Sum Rule 227
Take (ξ , η) ∈ ∂P F (x, y) at some point (x, y) ∈ dom F. Then there exists M > 0
such that 
F (x'
, y'
) − F (x, y) ≥ (ξ , η) · ((x'
, y'
) − (x, y)) − M|(x'
, y'
) − (x, y)|
2
for all (x'
, y'
) in some neighbourhood of (x, y). Expressed in terms of the fi’s this 
inequality informs us that 

i
[fi(x'
+y'
i)−fi(x+yi)] ≥ ξ ·(x'
−x)+

i
ηi ·(y'
i −yi)−M|(x'
, y'
)−(x, y)|
2.
Fix an index value j ∈ {1,...,m}. Set x' = x and y'
i = yi for i /= j in the above 
inequality. There results 
fj (x + y'
j ) − fj (x + yj ) ≥ ηj · (y'
j − yj ) − M|y'
j − yj |
2
for all y'
j ’s close to yj . It follows that ηj ∈ ∂P fj (x + yj ). 
Next, for any x' close to x, set y'
i = x + yi − x' for i = 1,...,m. This gives 
0 ≥ ξ · (x' − x) − (

i
ηi) · (x' − x) − (m + 1)M|x' − x|
2.
We conclude that ξ = 
i ηi. To summarize: 
∂P F (x, y) ⊂ {(η1 + ... + ηm, η) : η = (η1,...,ηm) and
ηi ∈ ∂P fi(x + yi) for i ∈ {1,...,m}}.
Now take any (ξ , η) ∈ ∂F(x,¯ 0). Then (ξ , η) = limk(ξk, ηk) for some sequences 
{(ξk, ηk)} and {(xk, yk)} such that (ξk, ηk) ∈ ∂P F (xk, yk) for each k, and 
(xk, yk) F
→ (x,¯ 0). As we have shown 
ηk
i ∈ ∂P fi(xk + yk
i ) for i = 1,...,m
and ξk = m
i=1 ηk
i . We claim also that 
fi(xk + yk
i ) → fi(x)¯ for each i.
To confirm this relation, consider 
γi := lim sup
k→∞
(fi(xk + yk
i ) − fi(x)) ¯ for i = 1, . . . , m.228 5 Subdifferential Calculus
Since the fi’s are lower semi-continuous, we must show that γi = 0 for each i. By 
extracting subsequences if necessary, we can arrange that 
γi = lim
k→∞(fi(xk + yk
i ) − fi(x)) ¯ for i = 1, . . . , m.
(‘lim’ has replaced ‘lim sup’ here). But since (xk, yk) F
→ (x,¯ 0), 
lim
k
m
i=1
fi(xk + yk
i ) = m
i=1
fi(x), ¯
whence 

i
lim inf
k→∞ (fi(xk + yk
i ) − fi(x)) ¯ ≤ 0.
By lower semicontinuity, each term in the summation is non-negative. It follows 
that, for each i, 
0 = lim inf
k→∞ (fi(xk + yk
i ) − fi(x)) ¯ = lim
k→∞(fi(xk + yk
i ) − fi(x)) ¯ = γi.
Our claim then is justified. 
Since 
xk + yk
i
f
→ ¯x, ηk
i → ηi as k → ∞,
ηk
i ∈ ∂P fi(xk + yk
i ) for all k,
we have 
ηi ∈ ∂fi(x)¯ for each i.
We have shown that 
∂F(x,¯ 0) ⊂ {(η1 + ... + ηm, η) :
η = (ηi,...,ηm) and ηi ∈ ∂fi(x)¯ for i ∈ {1,...,m}}.
Similar arguments based on the representation of ∂∞F in terms of scaled proximal 
subgradients at neighbouring points yield 
∂∞F (x,¯ 0) ⊂ {(η1 + ... + ηm, η) :
η = (ηi,...,ηm) and ηi ∈ ∂∞fi(x)¯ for each i}.5.4 A Sum Rule 229
Now apply Theorem 5.3.1 to F. Note that the non-degeneracy hypothesis of 
Theorem 5.3.1 is satisfied since, if (0, η) ∈ ∂∞F (x,¯ 0) and η /= 0, then 
i ηi = 0
and ηi ∈ ∂∞fi(x)¯ for each i, a possibility excluded by our hypotheses. Recalling 
that f (x) = F (x, 0) for all x ∈ Rn, we deduce that 
∂f (x)¯ = ∂xF (x,¯ 0) ⊂ ∂f1(x)¯ + ... + ∂fm(x)¯
and 
∂∞f (x)¯ = ∂∞
x F (x,¯ 0) ⊂ ∂∞f1(x)¯ + ... + ∂∞fm(x). ¯
⨅⨆
As a first application of the sum rule, we estimate the normal cone to the graph 
of a Lipschitz continuous map. 
Proposition 5.4.2 Take a lower semi-continuous map G : Rn → Rm and a point 
u ∈ Rn. Assume that G is Lipschitz continuous on a neighbourhood of u. Then 
NGr G(u, G(u)) ⊂ {(ξ , −η) : ξ ∈ ∂(η · G)(u), η ∈ Rm}.
Proof Take any (ξ , −η) ∈ NGr G(u, G(u)). Then (ξ , −η) = limi(ξi, −ηi) for 
sequences {(ξi, −ηi)} and ui
G
→ u such that 
(ξi, −ηi) ∈ NP
Gr G(ui, G(ui)) for all i.
For each i, there exists Mi > 0 such that 
(ξi, −ηi) · (u' − ui, G(u'
) − G(ui)) ≤ Mi|u' − ui|
2
for all u' close to ui. This inequality can be re-arranged to give 
ηi · G(u'
) ≥ ηi · G(ui) + ξi · (u' − ui) − Mi|u' − ui|
2,
from which we deduce that 
ξi ∈ ∂P (ηi · G)(ui) .
If follows from Proposition 4.7.1 and the sum rule (Theorem 5.4.1) that 
ξi ∈ ∂(η · G)(ui) + ∂((ηi − η) · G)(ui)
for each i. Also by Proposition 4.7.1. then, for sufficient large i, 
ξi ∈ ∂(η · G)(ui) + K|ηi − η|B.230 5 Subdifferential Calculus
Here K is a Lipschitz constant for G on a neighbourhood of u. Since the limiting 
normal cone is ‘robust’ under limit taking, we obtain ξ ∈ ∂(η · G)(u) in the limit as 
i → ∞. ⨅⨆
5.5 A Nonsmooth Chain Rule 
We now derive a far-reaching chain rule. 
Theorem 5.5.1 (A Chain Rule) Take a locally Lipschitz continuous function G :
Rn → Rm, a lower semi-continuous function g : Rm → R ∪ {+∞} and a point u¯ ∈
Rn such that G(u)¯ ∈ dom g. Define the lower semi-continuous function f (u) :=
g ◦ G(u). Assume that: 
The only vector η ∈ ∂∞g(G(u)) ¯ such that 0 ∈ ∂(η · G)(u)¯ is η = 0. 
Then 
∂f (u)¯ ⊂ {ξ : there exists η ∈ ∂g(G(u)) ¯ such that ξ ∈ ∂(η · G)(u)¯ } (5.5.1) 
and 
∂∞f (u)¯ ⊂ {ξ : there exists η ∈ ∂∞g(G(u)) ¯ such that ξ ∈ ∂(η · G)(u)¯ }.
(5.5.2) 
Otherwise expressed, inclusion (5.5.1) tells us that, given some ξ ∈ ∂f (u)¯ , we 
may find some η ∈ ∂g, evaluated at G(u)¯ , such that ξ ∈ ∂d(u)¯ , where d is the 
scalar-valued function d(u) := (η · G)(u). ((5.5.2) can be likewise interpreted.) 
Proof Define the lower semi-continuous function 
F (x, u) := g(x) + ΨGrG(u, x) + Ψ{ ¯u,G(u)¯ }+B(u, x). (5.5.3) 
We see that f (u) = minx F (x, u). Notice that (G(u), ¯ u)¯ ∈ dom F, dom F is 
bounded and G(u)¯ is the unique minimizer of F (., u)¯ . The scene is set then for 
applying Theorem 5.2.1. This tells us that 
∂f (u)¯ ⊂ {ξ : (0,ξ) ∈ ∂F (G(u), ¯ u)¯ }
and 
∂∞f (u)¯ ⊂ {ξ : (0,ξ) ∈ ∂∞F (G(u), ¯ u)¯ }.
Now the sum rule (Theorem 5.4.1) is applied to estimate the limiting subdiffer￾ential and asymptotic limiting subdifferential of F given by (5.5.3). The last term 
in (5.5.3) makes no contribution to the subdifferentials and can be ignored.5.5 A Nonsmooth Chain Rule 231
It is necessary to check the sum rule non-degeneracy condition. Now, as is easily 
shown, a general element v1 in the asymptotic limiting subdifferential of (x, u) →
g(x) at (G(u), ¯ u)¯ is expressible as v1 = (γ , 0) where γ ∈ ∂∞g(G(u)) ¯ . On the other 
hand, by Proposition 5.4.2, 
∂{(x'
, u'
) → ΨGr G(u'
, x'
)}(x, u)
= {(−η, ξ) : (ξ , −η) ∈ ∂ΨGr G(u, x)}
⊂ {(−η, ξ) : ξ ∈ ∂(η · G)(u)}.
So an element in ∂{(x, u) → ΨGr G(u, x)}(G(u), ¯ u)¯ is expressible as v2 = (−η, ξ)
for some ξ and η which satisfy ξ ∈ ∂(η ·G)(u)¯ . We must examine the consequences 
of v1 + v2 = 0. They are that ξ = 0 and 0 ∈ ∂(η · G)(u)¯ for some η ∈ ∂∞g(G(u)) ¯ . 
But then η = 0 under the present hypotheses. So v1 + v2 = 0 implies v1 = v2 = 0. 
The non-degeneracy hypothesis is therefore satisfied. The sum rule (Theorem 5.4.1) 
and previous considerations now give 
∂f (u)¯ ⊂ {ξ : (0,ξ) = (ν, 0) + (−η, ξ ), ν ∈ ∂g(G(u)), ξ ¯ ∈ ∂(η · G)(u)¯ }
= {ξ : there exists η ∈ ∂g(G(u)) ¯ such that ξ ∈ ∂(η · G)(u)¯ }.
Similarly 
∂∞f (u)¯ ⊂ {ξ : there exists η ∈ ∂∞g(G(u)) ¯ such that ξ ∈ ∂(η · G)(u)¯ }.
These are the relations we set out to prove. ⨅⨆
A number of important calculus rules are now obtainable as corollaries of the 
nonsmooth chain rule. 
Theorem 5.5.2 (Max Rule) Take locally Lipschitz continuous functions fi : Rn →
R, i = 1,...,m, and a point x¯ ∈ Rn. Define f (x) = maxi fi(x) and Λ := {λ =
(λ1,...,λm) ∈ Rm : λi ≥ 0,

i λi = 1}. Then 
∂f (x)¯ ⊂ {∂(m
i=1
λifi)(x)¯ : λ ∈ Λ, and λi = 0 if fi(x) < f ( ¯ x)¯ }.
Let us be clear about what this theorem tells us: given ξ ∈ ∂f (x)¯ , there exists a 
‘convex combination’ {λi} of ‘active’ fi’s such that 
ξ ∈ ∂(
i
λifi)(x). ¯
This implies (via the sum rule and in view of the positive homogeneity properties of 
the subdifferential)232 5 Subdifferential Calculus
ξ ∈

i
λi∂fi(x), ¯
another, slightly weaker, version of the max rule. 
Proof We readily calculate the limiting subgradient of g(y) := max{y1,...,ym}
for y = (y1,...,ym). It is 
∂g(y) = {λ = (λ1,...,λm) ∈ Λ : λi = 0 if yi < max
j yj }.
Now we apply the chain rule (Theorem 5.5.1) with this g and G(x) :=
(f1,...,fm)(x), noting that the non-degeneracy hypothesis is satisfied in view 
of Proposition 4.7.1. ⨅⨆
Theorem 5.5.3 (Product Rule) Take locally Lipschitz continuous functions fi :
Rn → R, i = 1,...,m, and a point x¯ ∈ Rn. Define f (x) = f1(x)f2(x) . . . fm(x). 
Then 
∂f (x)¯ ⊂ ∂(m
i
Πj/=ifj (x)f ¯ i)(x). ¯
This theorem tells us for example that, in the case m = 2, 
∂f (x)¯ ⊂ ∂(f1(x)f ¯ 2 + f2(x)f ¯ 1)(x). ¯
In the event that f1(x)¯ ≥ 0, f2(x)¯ ≥ 0, this condition implies 
∂f (x)¯ ⊂ f1(x)∂f ¯ 2(x)¯ + f2(x)∂f ¯ 1(x)¯
Proof Apply the chain rule with g(y) := Πiyi for y = (y1,...,ym) and G(x) :=
(f1,...,fm)(x) for x ∈ Rn. ⨅⨆
5.6 Lagrange Multiplier Rules 
Finally we make contact with optimization. A general theorem is proved, providing 
necessary conditions for a point to be a minimizer of a composite function. Choosing 
different ingredients for this function supplies a variety of Lagrange multiplier rules, 
one example of which we investigate in detail. 
Theorem 5.6.1 (Generalized Multiplier Rule) Take Lipschitz continuous func￾tions f : Rn → R and F : Rn → Rm, a lower semi-continuous function 
h : Rm → R ∪ {+∞}, a closed set C ⊂ Rn and a point x¯ ∈ C such that5.6 Lagrange Multiplier Rules 233
h(F (x)) < ¯ ∞. Define the lower semi-continuous function l : Rn → R ∪ {+∞}
to be 
l(x) := f (x) + h(F (x)) + ΨC(x).
Let x¯ attain the minimum of l. Assume that 
0 ∈ ∂(η · F )(x)¯ + NC(x)¯ for some η ∈ ∂∞h(F (x)) ¯ implies η = 0.
Then 
0 ∈ ∂f (x)¯ + ∂(η · F )(x)¯ + NC(x)¯ for some η ∈ ∂h(F (x)). ¯
Proof Choose g(x, y) := f (x) + h(y) + ΨC(x) and G(x) = (x, F (x)). Then x¯
minimizes l(x) := g(G(x)), and so 
0 ∈ ∂P l(x)¯ ⊂ ∂(g ◦ G)(x). ¯
Set y¯ = F (x)¯ . Now apply the chain rule (Theorem 5.5.1). To begin with, we need 
to check the non-degeneracy hypothesis. Take any 
η = (η1, η2) ∈ ∂∞g(x,¯ y)¯ = ∂∞(f (x) + h(y) + ΨC(x))|(x,y)=(x,¯ y)¯ .
It is a straightforward exercise to check that the conditions under which the sum 
rule (Theorem 5.4.1) supplies an estimate for (η1, η2) are satisfied. Consequently 
there exist η1 ∈ NC(x)¯ and η2 ∈ ∂∞h(F (x)) ¯ . We see that if 0 ∈ ∂(η · G)(x)¯ then 
0 ∈ η1 + ∂(η2 · F )(x)¯ . Under current hypotheses then η2 (and therefore also η1) is 
zero, so the hypotheses under which we may apply the chain rule are satisfied. 
A further application of the sum rule tells us that if (ξ , η) ∈ ∂g(G(x)) ¯ then 
ξ ∈ ∂f (x)¯ + NC(x)¯ and η ∈ ∂h(y)¯ . From the chain rule then 
0 ∈ ∂(g ◦ G)(x)¯ ⊂ ∂f (x)¯ + ∂(η · F )(x)¯ + NC(x)¯
for some η ∈ ∂h(F (x)) ¯ . This is what we set out to prove. ⨅⨆
In applications the function h in the generalized multiplier theorem is usually 
taken to be the indicator function of some closed set E (comprising allowable 
values for F (x)). In this case a minimizer x¯ for l is a minimizer for the constrained 
optimization problem 
Minimize{f (x) : F (x) ∈ E and x ∈ C}.
Now 
∂ψE(F (x)) ¯ = ∂∞ψE(F (x)) ¯ = NE(F (x)). ¯234 5 Subdifferential Calculus
It follows from Theorem 5.6.1 that, if 
0 ∈ ∂(η · F )(x)¯ + NC(x)¯ and η ∈ NE(F (x)) ¯ implies η = 0, (5.6.1) 
then there exists 
η ∈ NE(F (x)) ¯
such that 
0 ∈ ∂f (x)¯ + ∂(η · F )(x)¯ + NC(x). ¯
The vector η is a Lagrange multiplier, which must be directed into the normal cone 
NE(F (x)) ¯ of the constraint set E at F (x)¯ . The non-degeneracy condition (5.6.1) 
under which this Lagrange multiplier rule is valid requires, when F is a smooth 
function, that there does not exist a non-zero vector 
η = {η1,...,ηn}
such that 
 −η is a limiting normal to E at F (x)¯ and the linear combination 
i ηi∇Fi(x)¯ of gradients of the components of F at x¯ is a limiting normal to C
at x¯. This condition is a generalization of Mangasarian Fromovitz type constraint 
qualifications invoked in the mathematical programming literature. 
A widely studied optimization problem to which the above theorem is applicable 
is 
(NLP) Minimize f0(x) over x ∈ X ∩ C
where 
X := {x ∈ Rn : f1(x) ≤ 0,...,fp(x) ≤ 0, g1(x) = 0,...,gq (x) = 0}.
Here f0, f1,...,fp and g1,...,gq are all R-valued functions with domain Rn, and 
C is a closed subset of Rn. 
Theorem 5.6.2 (Lagrange Multiplier Rule) Let x¯ be a local minimizer for 
(NLP ). Assume that f0,...,fp and g1,...,gq are locally Lipschitz continuous 
functions. Then there exist λ0 ≥ 0, λ1 ≥ 0,...,λp ≥ 0 (with λ0 = 0 or 1) and real 
numbers γ1,...,γq such that 
λi = 0 if fi(x) < ¯ 0 for i = 1,... , p, (5.6.2) 

p
i=0
λi +
q
i=1
|γi| /= 05.6 Lagrange Multiplier Rules 235
and 
0 ∈ ∂[

p
i=0
λifi +
q
i=1
γigi](x)¯ + NC(x). ¯
Proof By replacing C by C ∩(x¯ +ϵB), for ϵ > 0 sufficiently small, we can arrange 
that x¯ is a minimizer of f0 over X ∩ C (NC(x)¯ is unaffected). We can arrange also 
that, if fi(x) < ¯ 0, for some i ∈ {1,...,p} then fi(x) < 0 for all x ∈ ¯x + ϵB. 
The constraint fi(x) < 0 is thereby rendered irrelevant and we can ignore it. Let us 
assume that index values corresponding to inequality constraints inactive at x¯ have 
been removed and we have labelled the remaining inequality constraint functions 
by indices in some new (possibly reduced) index set. The assertions of the theorem 
for the new index set (excluding the complementary slackness condition (5.6.2)) 
imply those for the original index set (including this last condition). Complementary 
slackness therefore takes care of itself, if we attend to the other assertions of the 
theorem. 
It is clear that (α,¯ x)¯ (where α¯ = f0(x)¯ ) is a minimizer for the optimization 
problem: 
Minimize {α : f0(x) − α ≤ 0 and x ∈ X ∩ C}.
Set 
f (α, x) = α , F (α, x) = (f0(x) − α, f1(x), . . . , fp(x), g1(x), . . . , gq (x))
and 
E = (−∞, 0]
p+1 × {0}
q .
We find NE(0) = M where 
M = 
(λ0, λ1,...,λp, γ1,...,γq ) : λi ≥ 0 for i ∈ {0,...,p}

.
Let us now examine the non-degeneracy hypothesis of Theorem 5.6.1 for the 
above identifications of f and F and for h = ΨE. If η ∈ ∂∞h(0), then 
η = (λ0, λ1,...,λp, γ1,...,γq ) ∈ M.
It follows that the relation 
0 ∈ ∂(η · F )(α,¯ x)¯ + {0} × NC(x)¯
can be written 
0 ∈ {−λ0} × ∂(
p
i=0
λifi +
q
i=1
γigi)(x)¯ + {0} × NC(x)¯236 5 Subdifferential Calculus
which implies 
0 ∈ {0} × ∂(
p
i=1
λifi +
q
i=1
γigi)(x)¯ + {0} × NC(x). ¯
It follows that if the non-degeneracy hypothesis is violated, then 
0 ∈ ∂(
p
i=1
λifi +
q
i=1
γigi)(x)¯ + NC(x)¯
for some nonzero (λ0, λ1,...,λp, γ1,...,γq ) ∈ M and λ0 = 0. The assertions of 
the theorem are valid then (albeit only in a degenerate sense). 
On the other hand, if the non-degeneracy hypothesis is satisfied then there exists 
η = (λ0,...,λp, γ1,...,γq ) ∈ M such that 
0 ∈ ∂f (α,¯ x)¯ + ∂(η · F )(α,¯ x)¯ + {0} × NC(x). ¯
Since ∂f (α,¯ x)¯ = {(1, 0,..., 0)}, we conclude 
0 ∈ ∂(
p
i=0
λifi +
q
i=1
γigi)(x)¯ + NC(x), ¯
for some (λ0, λ1,...,λp, γ1,...,γq ) ∈ M with λ0 = 1. 
⨅⨆
5.7 Max Rule for an Infinite Family of Functions 
We have proved a max rule estimating the subdifferential of the pointwise supremum 
of a finite family number of lower semi-continuous functions (see Theorem 5.5.2). 
What if the pointwise supremum is taken over a family of functions x → f (t, x)
indexed by points t ∈ T , where T is a set containing possibly an infinite number of 
points? The following theorem, due to Clarke, provides an answer to this question. 
It characterizes the convexified limiting subdifferential of x → supt∈T {f (t, x)}, i.e. 
the Clarke generalized gradient, in circumstances where no restrictions are place on 
the index set, under a finiteness hypothesis and when we require the functions to be 
uniformly Lipschitz continuous, regarding their x dependence. 
Theorem 5.7.1 Take a non-empty abstract set T , a function f : T × Rn → R and 
a point x ∈ Rn. Define 
fsup(x) := sup
t∈T
f (t, x), for x ∈ Rn .5.7 Max Rule for an Infinite Family of Functions 237
Assume that, for some open neighbourhood U of x, there exists K > 0 such that 
(H1): |f (t, x'') − f (t, x'
)| ≤ K|x'' − x'
| for all t ∈ T and x'', x' ∈ U, 
(H2): fsup(x'
) < ∞ for some x' ∈ U . 
Then, for any subset S ⊂ U of zero n-dimensional Lebesgue measure, 
co ∂fsup(x) ⊂ C,
where C := co { lim
i→∞∇xf (ti, xi) : ti ∈ T,xi /∈ S for all i and f (ti, xi) →
fsup(x)}.
Proof Notice that, in consequence of (H1), (H2) implies fsup(x'
) < ∞ for all 
x' ∈ U. It is easily checked that fsup is Lipschitz continuous on U (with Lipschitz 
constant K). Observe that the set C is nonempty and compact. 
Take any v ∈ Rn and ϵ > 0. Write 
m := max
ξ∈C
ξ · v .
Recalling that the Clarke directional derivative f 0
sup(x;.) is the support function 
of the closed convex set co ∂fsup(x) (Propositions 4.7.4 and 4.7.6), we see that the 
assertions of the theorem will have been confirmed if we can show 
f 0
sup(x; v) ≤ m + ϵ.
Since the function dependencies concerned are homogeneous, we can assume, 
without loss of generality, that |v| = 1. 
It follows from the definition of C that there exists δ > 0 such that x + 2δB ⊂ U
and, for any y ∈ x + 2δB and t ∈ T satisfying 
y /∈ S, ∇xf (t, y) exists and f (t, x) ≥ fsup(x) − δ,
we have 
∇xf (t, y) · v ≤ m + ϵ . (5.7.1) 
Take any t ∈ T such that f (t, x) ≥ fsup(t, x) − δ. Write Ω := {x' ∈ U :
∇xf (t, x'
) does not exist}. Now take y to be any point in the ball x + δB such that 
the intersection of the line segment [y, y +δv] with S ∪Ω has zero one-dimensional 
Lebesgue measure. (It can be deduced from Fubini’s theorem that the set of such 
y’s is a subset of x + δB with full n-dimensional Lebesgue measure.) 
Now take any λ ∈ (0, δ). It follows from (5.7.1) and the Lipschitz continuity of 
f (t, .) that 
f (t, y + λv) − f (t, y) =
 λ
0
∇xf (t, y + σv) · v dσ ≤ λ(m + ϵ). (5.7.2)238 5 Subdifferential Calculus
Since this relation is valid for y’s in a dense set, it follows from the continuity of 
f (t..) that it is also valid for all y ∈ x + δB. 
Now take r ∈ (0, δ) such that r2+4rK < δ. 
Claim: for every y ∈ x + rB and λ ∈ (0,r)
fsup(y + λv) − fsup(y) ≤ λ(m + ϵ) + λ2 .
This will mean that 
f 0
sup(x; v) = {lim sup
i→∞
λ−1
i (fsup(yi + λiv) − fsup(yi)) : λi ↓ 0, yi → y} ≤ m + ϵ
as required for completion of the proof. 
To validate the claim, take any y ∈ x + rB and λ ∈ (0,r). Let t be any point in 
T such that f (t, y + λv) ≥ fsup(y + λv) − λ2. Then 
f (t, x) ≥ f (t, y + λv) − K|x − y − λv|
≥ fsup(y + λv) − λ2 − K|x − y − λv|
≥ fsup(x) − λ2 − 2K|x − y − λv|
≥ fsup(x) − r2 − 4Kr, since |v| ≤ 1,
≥ fsup(x) − δ .
We have shown that t and λ chosen as above meet the criteria for validity of (5.7.2). 
It follows that 
fsup(y + λv) − fsup(y) ≤ f (t, y + λv) − f (t, y) + λ2 ≤ λ(m + ϵ) + λ2.
⨅⨆
5.8 Exercises 
5.1 Let A ⊂ Rn be a closed set with representation 
A = {x : φ(x) ≤ 0, ψ(x) = 0 and x ∈ Ω},
for some locally Lipschitz continuous functions φ : Rn → Rk1 and ψ : Rn → Rk2 
and some closed set Ω ⊂ Rn .5.8 Exercises 239
Take x¯ ∈ Rn and assume that 
{(μ, ν) ∈ (R+)
k1 × Rk2 : ∂

μ · φ + ν · ψ

(x)¯ ∩

− NΩ(x)¯

and μi = 0 if φi(x) < ¯ 0}={(0, 0)}.
Take η ∈ NA(x)¯ . Show that there exists (μ, ν) ∈ (R+)k1 × Rk2 such that μi =
0 if φi(x)¯ < 0 and 
η ∈ ∂

μ · φ + ν · ψ

(x)¯ + NΩ(x). ¯
Remark 
The relations derived in this exercise enable us to deduce first order necessary 
conditions of optimality for nonlinear programming problems involving inequality 
and equality state constraints, expressed in terms of Lagrange multipliers, from 
general necessary conditions, such as those provided by Theorem 5.6.2, expressed 
in terms of the limiting normal cone of the constraint set. 
5.2 (Nonsmooth Derivatives of Vector Valued Functions) Take a vector valued 
function G : Rn → Rk and x¯ ∈ Rn. Write G(x) = [g1(x), . . . , gk(x)]. Assume 
that G is Lipschitz continuous on a neighbourhood of x¯. The Clarke generalized 
Jacobian of G at x¯ is the subset of the space of k × n matrices defined by 
J CG(x)¯ := co {M ∈ Rk×n : ∃ xi → ¯x s.t. G is differentiable at xi
for each i and ∇G(xi) → M}
(i): Show that, for each p ∈ Rk, 
p · J CG(x)¯ = co ∂(p · G)(x). ¯
Hint: For each p, use the characterization of co ∂(p · G)(x)¯ provided by Theo￾rem 4.7.7, taking as ‘bad’ set points in Rn at which G fails to be differentiable. 
(ii): Show that 
J CG(x)¯ ⊂ co ∂g1(x)¯ × ... × co ∂gk(x). ¯
Give an example where this inclusion relation is strict. 
Remark 
The multifunction D∗G(x)¯ : Rk → Rn defined by 
D∗G(x)(p) ¯ := ∂(p · G)(x), ¯240 5 Subdifferential Calculus
the convex hull of whose values feature in the above representation of Clarke’s 
generalized Jacobian, is referred to as the coderivative of G at x¯ in [177]. It 
properties were systematically investigated in [123] and it features in the early 
nonsmooth constructions of Mordukhovich (See references in [159] and [160].) 
5.9 Notes for Chapter 5 
(See previous chapter)Chapter 6 
Differential Inclusions 
Abstract Differential inclusions are generalizations of first order, vector differ￾ential equations, in which set inclusion replaces equality and the right side is no 
longer a point valued, but a set valued function (‘multifunction’) of the current 
time and state. Differential inclusions feature prominently in dynamic optimization. 
They are used to formulate a dynamic constraint, thereby serving as an alternative 
to the controlled differential equation framework of the early literature. Even 
when the original dynamic constraint is of a different nature, it is often helpful 
(regarding the formulation of hypotheses for existence of minimizers, or under 
which nonsmooth optimality conditions can be derived) to consider the associated 
differential inclusion. 
Many properties of first order differential equations have analogues in the 
theory of differential inclusions. These include existence of global solutions with 
a specified initial value, when a Lipschitz continuity hypothesis is imposed on 
the differential inclusion (strictly speaking, on the associated multifunction), and 
sensitivity properties of solutions under data perturbations. Differential inclusions 
fail to have unique solutions even in the simplest cases, however. It is therefore 
of interest to consider the whole set of solutions for a given initial state and ask: 
is this set non-empty, is it compact and, if so, w.r.t. what topology, or is it stable 
under perturbations of the differential inclusion? This chapter provides answers to 
questions of this nature. 
The starting point is the generalized Filippov existence theorem, which asserts 
existence of solutions to Lipschitz continuous differential inclusions. But it supplies 
additional useful information, in the form of estimates of the distance of a solution 
from a given arc that is an approximate solution of the differential inclusion. This 
is followed by the compactness of trajectories theorem, which concerns properties 
of sequences of arcs with uniformly integrably bounded velocities, approximately 
satisfying the given differential inclusion with an increasing degree of accuracy. We 
find that accumulation points of such sequences (in the uniform topology) satisfy, 
not the nominal differential inclusion but its convexification, in which the values 
of the associated multifunction are replaced by their convex hulls. We deduce as 
a straightforward consequence that, under a Lipschitz continuity hypothesis, the 
set of solutions to a given differential inclusion is closed, provided the differential 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
P. Bettiol, R. B. Vinter, Principles of Dynamic Optimization, Springer Monographs 
in Mathematics, https://doi.org/10.1007/978-3-031-50089-3_6
241242 6 Differential Inclusions
inclusion is convex (that is, has values convex sets). If the differential inclusion 
fails to be convex, it can be shown that, nonetheless, an arbitrary solution to the 
convexified differential inclusion can be approximated uniformly closely (in the 
uniform norm) by solutions to the original, unconvexified, differential inclusion; 
this is the relaxation theorem. In the final section of the chapter, we introduce a 
state constraint. Conditions are given for existence of solutions that satisfy the state 
constraint. We also provide estimates of the distance of this solution from a nominal 
solution that violates the state constraint. Distance estimates of this nature are used 
extensively in the theory of state constrained dynamic optimization, to establish 
non-degeneracy of optimality conditions and stability of the infimum cost under 
data perturbations. 
6.1 Introduction 
A differential inclusion is a relation of the form 
x(t) ˙ ∈ F (t, x(t)) a.e. t ∈ I , (6.1.1) 
in which I is a given interval and F : I × Rn ⇝ Rn is a given multifunction. 
Absolutely continuous functions that satisfy this relation are called F trajectories. 
Differential inclusions feature prominently in modern treatments of dynamic opti￾mization. This has come about for several reasons. One is that condition (6.1.1), 
summarizing constraints on allowable velocities, provides a convenient framework 
for stating hypotheses under which dynamic optimization problems have solutions 
and optimality conditions may be derived. Another is that, even when we choose not 
to formulate a dynamic optimization problem in terms of a differential inclusion, 
in cases when the data is nonsmooth, often the very statement of optimality 
conditions makes reference to differential inclusions. It is convenient then at this 
stage to assemble important properties of multifunctions and differential inclusions 
of particular relevance in dynamic optimization. 
Differential inclusions are generalizations of (first order) differential equations. 
Many aspects of the theory of first order differential equations carry across to this 
more general setting. These include existence of solutions with a specified initial 
value and sensitivity properties of solutions under data perturbation. An exception 
of course is ‘uniqueness of solutions’. Indeed we can expect there to be a multiplicity 
of F trajectories having a given initial state since the differential inclusion merely 
imposes a constraint on possible values of the velocity (at each time and state), but 
may fail to specify that velocity. For this reason, important questions underlying 
the theory of differential inclusions concern properties of the whole family of F
trajectories, not just individual members. Is it non-empty, is it closed and, if so, 
w.r.t. which topology on the underlying function space? How does the family behave 
under perturbations? Is it non-empty if we require the F trajectories to satisfy a 
path-wise constraint? All of these questions are addressed in this chapter.6.2 Existence and Estimation of F Trajectories 243
Lipschitz continuity of the right side of a differential equation w.r.t. the state 
guarantees existence of global solutions and stability properties under perturbations. 
Likewise, Lipschitz continuity features prominently in the theory of differential 
inclusions as a hypothesis, to ensure non-emptiness of the class of F trajectories and 
serve as a starting point for a perturbational analysis. (Here, of course, ‘Lipschitz 
continuity’ of F w.r.t. to the state variable requires interpretation, since F is set 
valued.) 
The starting point of this chapter is the generalized Filippov existence theorem. 
This fundamental theorem tells us, as its name suggests, that an F trajectory exists 
(under a Lipschitz continuity hypothesis). But it is important also for the additional 
information it supplies, namely estimates on the distance of this F trajectory from a 
given absolutely continuous arc that may fail to satisfy the differential inclusion. 
At the heart of our investigation of the closure properties of F trajectories 
families is the compactness of trajectories theorem. This reveals limit behaviour 
of sequences of absolutely continuous functions, approximately satisfying the given 
differential inclusion x˙ ∈ F with an increasing degree of accuracy. A key feature 
here is convexity: under a uniform integrability hypothesis, bounded sequences 
of F trajectories have accumulation points (in the uniform topology) that satisfy, 
not the given differential inclusion x˙ ∈ F (t, x), but its convexification, namely 
x˙ ∈ co F (t, x). 
The compactness of trajectories theorem points directly to simple criteria for 
existence of minimizers, for dynamic optimization problems in which the dynamic 
constraint takes the form of the differential inclusion (6.1.1), when F takes values 
convex sets. If F fails to be convex valued, there may be no minimizers in a 
traditional sense. But existence of minimizers can still be guaranteed, if the domain 
of the dynamic optimization problem is enlarged to include also convexified F
trajectories. This is the theme of relaxation. 
In the final section of the chapter, we undertake a more refined study of the 
existence of F trajectories, now to take account of a (path-wise) state constraint. We 
give conditions which ensure the existence of an F trajectory satisfying the state 
constraint. We also provide estimates concerning the distance of this F trajectory 
from some nominal F trajectory that violates the state constraint. Distance estimates 
of this nature are used extensively in the theory of state constrained dynamic 
optimization. 
6.2 Existence and Estimation of F Trajectories 
Fix an interval [S,T ] and a relatively open set Ω ⊂ [S,T ] × Rn. For t ∈ [S,T ], 
define 
Ωt := {x : (t, x) ∈ Ω}.244 6 Differential Inclusions
Take a continuous function y : [S,T ] → Rn and ϵ > 0. Then the ϵ tube about y is 
the set 
T (y, ϵ) := {(t, x) ∈ [S,T ] × Rn : t ∈ [S,T ], |x − y(t)| ≤ ϵ}.
Consider a multifunction F : Ω ⇝ Rn. We say that an arc x ∈ W1,1([S,T ]; Rn)
is an F trajectory if Gr x ⊂ Ω and 
x(t) ˙ ∈ F (t, x(t)) a.e. t ∈ [S,T ].
Naturally we would like to know when F trajectories exist. We shall make 
extensive use of a local existence theorem which gives conditions under which an 
F trajectory exists near a nominal arc y ∈ W1,1([S,T ]; Rn). This theorem will 
also provide important supplementary information about how ‘close’ to y the F
trajectory x may be chosen. Just how close will depend on the extent to which the 
nominal arc y fails to satisfy the differential inclusion, as measured by the function 
ΛF , 
ΛF (y) :=  T
S
ρF (t, y(t), y(t))dt. ˙
Here 
ρF (t, x, v) := inf{|η − v| : η ∈ F (t, x)}.
(We shall make use of function ΛF (y) only when the integrand above is Lebesgue 
measurable and F is a closed multifunction; then ΛF (y) is a non-negative number 
which is zero if and only if y is an F trajectory.) 
We pause for a moment however to list some relevant properties of ρF which, 
among other things, give conditions under which the integral ΛF (y) is well-defined. 
Proposition 6.2.1 Take a multifunction F : [S,T ] × Rn ⇝ Rn. 
(a) Fix (t, x) ∈ dom F. Then 
|ρF (t, x, v) − ρF (t, x, v'
)|≤|v − v'
| for all v, v' ∈ Rn.
(b) Fix t ∈ [S,T ]. Suppose that, for some ϵ > 0 and k > 0, x¯ + ϵB ⊂ dom F and 
F (t, x) ⊂ F (t, x'
) + k|x − x'
|B for all x, x' ∈ ¯x + ϵB.
Then 
|ρF (t, x, v) − ρF (t, x'
, v'
)|≤|v − v'
| + k|x − x'
|,
for all v, v' ∈ Rn and x, x' ∈ ¯x + ϵB.6.2 Existence and Estimation of F Trajectories 245
(c) Assume that F is L × Bn measurable. Then for any Lebesgue measurable 
functions y : [S,T ] → Rn and v : [S,T ] → Rn such that Gr y ⊂ dom F, we 
have that t → ρF (t, y(t), v(t)) is a Lebesgue measurable function on [S,T ]. 
Proof 
(a) Choose any ϵ > 0 and v, v' ∈ Rn. Since F (t, x) /= ∅, there exists η ∈ F (t, x)
such that 
ρF (t, x, v) ≥ |v − η| − ϵ
≥ |v' − η|−|v − v'
| − ϵ
≥ ρF (t, x, v'
) − |v − v'
| − ϵ .
(The second line follows from the triangle inequality.) Since ϵ > 0 is arbitrary, and 
v and v' are interchangeable, it follows that 
|ρF (t, x, v'
) − ρF (t, x, v'
)|≤|v − v'
|.
(b) Choose any x, x' ∈ ¯x + ϵB and v, v' ∈ Rn. Take any δ. Since F (t, x'
) /= ∅, 
there exists η' ∈ F (t, x'
) such that 
ρF (t, x'
, v'
) > |v' − η'
| − δ.
Under the hypotheses, there exists η ∈ F (t, x) such that |η − η'
| ≤ k|x − x'
|. 
Of course, ρF (t, x, v) ≤ |v − η|. It follows from these relations and the triangle 
inequality that 
ρF (t, x, v) − ρF (t, x'
, v'
)
≤ |v − η|−|v' − η'
| + δ ≤ |v − v'
|+|v' − η|−|v' − η'
| + δ
≤ |v − v'
|+|η − η'
| + δ ≤ |v − v'
| + k|x − x'
| + δ.
Since the roles of (x, v) and (x'
, v'
) are interchangeable and δ > 0 is arbitrary, we 
conclude that 
|ρF (t, x, v) − ρF (t, x'
, v'
)|≤|v − v'
| + k|x − x'
|.
(c) For each v, the function t → ρF (t, y(t), v) is measurable. This follows from 
the identity 
{t ∈ [S,T ] : ρF (t, y(t), v) < α} = {t : F (t, y(t)) ∩ (v + α int B) /= ∅} ,
valid for any α ∈ R, and the fact that t ⇝ F (t, y(t)) is measurable (see 
Proposition 2.3.2 ).246 6 Differential Inclusions
By part (a) of the lemma, the function ρF (t, y(t), .) is continuous, for each t ∈
[S,T ]. It follows that (t, v) → ρF (t, y(t), v) is a Carathéodory function. But then 
t → ρF (t, y(t), v(t)) is (Lebesgue) measurable on [S,T ], by Proposition 2.3.2 and 
Corollary 2.3.3. ⨅⨆
The following Proposition, concerning regularity properties of projections onto 
continuous, convex multifunctions, will also be required. 
Proposition 6.2.2 Take a continuous multifunction � : [S,T ] ⇝ Rn and a function 
u : [S,T ] → Rn. Assume that 
(i) u is continuous, 
(ii) �(t) is non-empty, compact and convex for each t ∈ [S,T ] and � is continuous, 
i.e. there exists o : R+ → R+ with limα↓0 o(α) = 0 such that 
�(s) ⊂ �(t) + o(|t − s|)B for all t,s ∈ [S,T ].
Let uˆ : [S,T ] → Rn be the function defined according to 
|u(t) − ˆu(t)| = min
u'
∈�(t)
|u(t) − u'
| for all t.
(There is a unique minimizer for each t since �(t) is non-empty, closed and convex.) 
Then uˆ is a continuous function. 
Proof Suppose that uˆ is not continuous. Then there exist ϵ > 0 and sequences {si}
and {ti} in [S,T ] such that |ti − si| → 0 and 
| ˆu(ti) − ˆu(si)| > ϵ, for all i. (6.2.1) 
Since the multifunction � is compact valued and continuous and has bounded 
domain, Gr � is bounded. It follows that, for some K > 0, 
u(si), u(ti), u(s ˆ i), u(t ˆ i) ∈ KB.
Since � is continuous, there exist {yi} and {zi} such that 
yi ∈ �(ti), zi ∈ �(si), for all i (6.2.2) 
and 
|yi − ˆu(si)| → 0, |zi − ˆu(ti)| → 0 as i → ∞. (6.2.3) 
Since � is a convex multifunction, u(si) − ˆu(si) and u(ti) − ˆu(ti) are normal 
vectors to �(si) and �(ti) respectively. Noting (6.2.2), we deduce from the normal 
inequality for convex sets that6.2 Existence and Estimation of F Trajectories 247
(u(si) − ˆu(si)) · (zi − ˆu(si)) ≤ 0
(u(ti) − ˆu(ti)) · (yi − ˆu(ti)) ≤ 0.
It follows that 
(u(si) − ˆu(si)) · (u(t ˆ i) − ˆu(si)) ≤ (u(si) − ˆu(si)) · (u(t ˆ i) − zi)
(u(ti) − ˆu(ti)) · (u(s ˆ i) − ˆu(ti)) ≤ (u(ti) − ˆu(ti)) · (u(s ˆ i) − yi).
Adding these inequalities gives 
(u(si) − u(ti)) · (u(t ˆ i) − ˆu(si)) +|ˆu(ti) − ˆu(si)|
2
≤ (u(si) − ˆu(si)) · (u(t ˆ i) − zi) + (u(ti) − ˆu(ti)) · (u(s ˆ i) − yi).
We deduce that 
| ˆu(ti) − ˆu(si)|
2 ≤ |u(si) − u(ti)|·|ˆu(ti) − ˆu(si)|
+ |u(si) − ˆu(si)|·|ˆu(ti) − zi|+|u(ti) − ˆu(ti)|·|ˆu(si) − yi|.
It follows that 
| ˆu(ti) − ˆu(si)|
2 ≤ 2K(|u(si) − u(ti)|+|ˆu(ti) − zi|+|ˆu(si) − yi|).
But the right side has limit 0 as i → ∞, since u is continuous, and by (6.2.3). We 
conclude that | ˆu(ti) − ˆu(si)| → 0 as i → ∞. We have arrived at a contradiction of 
(6.2.1). uˆ must therefore be continuous. ⨅⨆
We shall refer to the ‘truncation’ function trϵ : Rn → Rn, defined to be 
trϵ (ξ ) := 
ξ if |ξ | ≤ ϵ
ϵ|ξ |
−1ξ if |ξ | > ϵ . (6.2.4) 
Recall that T (y, ϵ) denotes the ϵ tube about the arc y: 
T (y, ϵ) := {(t, x) ∈ [S,T ] × Rn : t ∈ [S,T ], |x − y(t)| ≤ ϵ}.
Theorem 6.2.3 (Generalized Filippov Existence Theorem) Let Ω be a relatively 
open set in [S,T ] × Rn. Take a multifunction F : Ω ⇝ Rn, an arc y ∈
W1,1([S,T ]; Rn), a point ξ ∈ Rn and ϵ ∈ (0, +∞)∪{+∞} such that T (y, ϵ) ⊂ Ω.
Assume that 
(i) F (t, x'
) is a closed, non-empty set for all (t, x'
) ∈ T (y, ϵ), and F is L × Bn
measurable,248 6 Differential Inclusions
(ii) there exists k ∈ L1([S,T ]; R) such that 
F (t, x'
) ⊂ F (t, x'') + k(t)|x' − x''|B (6.2.5) 
for all x'
, x'' ∈ y(t) + ϵB, a.e. t ∈ [S,T ]. 
Assume further that 
exp T
S
k(t)dt |ξ − y(S)| +  T
S
ρF (t, y(t), y(t))dt ˙

≤ ϵ. (6.2.6) 
Then there exists an F trajectory x satisfying x(S) = ξ such that for all t ∈
(S, T ]
||x − y||L∞(S,t) ≤ |x(S) − y(S)| +  t
S
| ˙x(s) − ˙y(s)| ds
≤ exp t
S
k(s)ds |ξ−y(S)|+  t
S
ρF (s, y(s), y(s))ds ˙

.(6.2.7) 
Now suppose that (i) and (ii) are replaced by the stronger hypotheses 
(i)' F (t, x'
) is a non-empty, compact, convex set for all (t, x'
) ∈ T (y, ϵ), 
(ii)' there exists a function o : R+ → R+ and k∞ > 0 such that limα↓0 o(α) = 0
and 
F (s'
, x'
) ⊂ F (s'', x'') + k∞|x' − x''|B + o(|s' − s''|)B (6.2.8) 
for all (s'
, x'
), (s'', x'') ∈ T (y, ϵ). 
Then, if y is continuously differentiable, x can be chosen also to be continuously 
differentiable. 
(If ϵ = +∞ then in the above hypotheses T (y, ϵ) and ϵB are interpreted as 
[S,T ] × Rn and Rn respectively, and the left side of condition (6.2.6) is required to 
be finite.) 
Remarks 
(i) The hypotheses of Theorem 6.2.3 do not require F to be convex valued. For 
many developments in dynamic optimization the requirement that F is convex 
is crucial; fortunately, proving this basic existence theorem is not one of them. 
(ii) The proof of Theorem 6.2.3 is by construction. The iterative procedure used 
is a generalization to differential inclusions of the well-known Picard iteration 
scheme for obtaining a solution to the differential equation 
x(t) ˙ = f (t, x(t)), x(S) = ξ .6.2 Existence and Estimation of F Trajectories 249
An initial guess y at a solution is made. It is then improved by ‘successive 
approximations’ x0, x1, x2.. .. These arcs are generated by the recursive equa￾tions 
xi+1(t) = ξ +
 t
S
f (s, xi(s))ds
with starting condition x0(t) = y(t) + (ξ − y(S)). 
Proof We may assume without loss of generality that ϵ = ∞. Indeed if ϵ = ¯ϵ for 
some finite ϵ¯ then we consider F˜ in place of F, where 
F (t, x) ˜ := F (t, y(t) + trϵ¯(x − y(t))).
(See (6.2.4) for definition of trϵ¯.) 
F˜ satisfies the hypotheses (in relation to y) with ϵ = ∞ and with the same 
k ∈ L1 as before. Of course 
F (t, x) ˜ = F (t, x) for x ∈ y(t) + ¯ϵB.
Now apply the ‘ϵ = +∞’ case of the theorem to F˜. This gives an F˜ trajectory x
such that x(S) = ξ and (6.2.7) is satisfied (when F˜ replaces F). If, however, 
K

|ξ − y(S)| +  T
S
ρF (t, y(t), y(t))dt ˙

≤ ¯ϵ,
where K := exp  T
S k(t)dt
, then the theorem tell us that 
||x − y||L∞ ≤ ¯ϵ,
and therefore x is an F trajectory because F (t, .) and F (t, .) ˜ coincide on y(t)+ ¯ϵB. 
This justifies setting ϵ = +∞. 
It suffices to consider only the case ξ = y(S). To show this, suppose that ξ /=
y(S). Replace the underlying time interval [S,T ] by [S − 1, T ] and ξ by ξ˜ = y(S). 
Replace also F by F˜ : [S − 1, T ] × Rn ⇝ Rn and y by y˜ : [S − 1, T ] → Rn, 
defined as follows: 
F (t, x) ˜ := 
F (t, x) for (t, x) ∈ [S,T ] × Rn
{ξ − y(S)} for (t, x) ∈ [S − 1,S) × Rn
and 
y(t) ˜ := 
y(t) for t ≥ S
y(S) for t < S.250 6 Differential Inclusions
Now apply the special case of the theorem to find x˜ ∈ W1,1([S −1, T ]; Rn) such 
that x(S ˜ − 1) = ˜y(S − 1) = ξ˜ ≡ y(S). Take x to be the restriction of x˜ to [S,T ]. 
We readily deduce that for all t ∈ (S, T ]
||˙
x˜ − ˙
y˜||L1([S−1;t];Rn) ≤ exp t
S
k(s)ds t
S−1
ρF˜(s, y(s), ˜ ˙
y(s))ds. ˜
Now, to derive the desired estimate, we have merely to note that 
||˙
x˜ − ˙
y˜||L1([S−1,t];Rn) = |ξ − y(S)| + || ˙x − ˙y||L1([S,t];Rn)
and 
 t
S−1
ρF˜(s, y(s), ˜ ˙
y(s))ds ˜ = |ξ − y(S)| +  t
S
ρF (s, y(s), y(s))ds. ˙
Henceforth, then, we assume that ϵ = +∞ and y(S) = ξ ; we must find an F
trajectory x such that x(S) = ξ and, for all t ∈ (S, T ], 
|| ˙x − ˙y||L1([S,t];Rn) ≤ exp t
S
k(s)ds t
S
ρF (s, y(s), y(s))ds. ˙
Write x0(t) = y(t). According to Theorem 2.3.12, we may choose a measurable 
function v1 satisfying 
v1(t) ∈ F (t, x0(t)) a.e. t ∈ [S,T ]
and 
ρF (t, x0(t), x˙0(t)) = |v1(t) − ˙x0(t)| a.e. t ∈ [S,T ].
This is because 
t ⇝ G(t) := {v ∈ F (t, x0(t)) : ρF (t, x0(t), x˙0(t)) = |v − ˙x0(t)|}
is a closed nonempty measurable multifunction. (We use Proposition 2.3.2 and the 
fact that 
(t, v) → g(t, v) := ρF (t, x0(t), x˙0(t)) − |v − ˙x0(t)|
is a Carathéodory function.) Under the hypotheses, t → ρF (t, x0(t), x˙0(t)) is 
integrable. Since x˙0 is integrable, v1 is integrable too. We may therefore define x1
according to6.2 Existence and Estimation of F Trajectories 251
x1(t) := y(S) +
 t
S
v1(s)ds.
Note that 
ρF (t, x0(t), x˙1(t)) = 0 a.e..
Again appealing to Theorem 2.3.12, we choose a measurable function v2
satisfying 
v2(t) ∈ F (t, x1(t)) a.e.
and 
|v2(t) − ˙x1(t)| = ρF (t, x1(t), x˙1(t)) a.e.. (6.2.9) 
In view of the Lipschitz continuity properties of ρF (t, ., v) and since x˙1 is 
integrable, we readily deduce from the integrability of t → ρF (t, x0(t), x˙0(t)) that 
t → ρF (t, x1(t), x˙1(t)) is also integrable. It then follows from (6.2.9) that v2 is 
integrable and we may define 
x2(t) = y(S) +
 t
S
v2(s)ds.
We proceed in this way to construct a sequence of absolutely continuous arcs {xm}
satisfying 
ρF (t, xm(t), x˙m+1(t)) = 0 a.e.
| ˙xm+1(t) − ˙xm(t)| = ρF (t, xm(t), x˙m(t)) a.e.
for m = 0, 1, 2 ... and 
ρF (t, x0(t), x˙0(t)) = ρF (t, y(t), y(t)) ˙ a.e..
Notice that 
||x1 − x0||L∞ ≤
 T
S
| ˙x1(t) − ˙x0(t)|dt
=
 T
S
ρF (t, x0(t), x˙0(t))dt = ΛF (y). (6.2.10) 
Applying Proposition 6.2.1, we deduce that, for m ≥ 1 and a.e. t,252 6 Differential Inclusions
| ˙xm+1(t) − ˙xm(t)| ≤ ρF (t, xm−1(t), x˙m(t)) + k(t)|xm(t) − xm−1(t)|
= k(t)|xm(t) − xm−1(t)|. (6.2.11) 
Since xm+1(S) = xm(S), it follows that, for all m ≥ 1 and a.e. t ∈ [S,T ], 
|xm+1(t) − xm(t)|
≤
 t
S
k(t1)|xm(t1) − xm−1(t1)|dt1
≤
 t
S
k(t1)
 t1
S
k(t2)...  tm−1
S
k(tm)|x1(tm) − x0(tm)|dtm ...dt2dt1
≤ Sm(t)ΛF (y), (6.2.12) 
in view of (6.2.10). Here 
Sm(t) :=  t
S
k(t1)
 t1
S
k(t2)...  tm−1
S
k(tm)dtm..dt2dt1.
The right side can be reduced, one indefinite integral at a time with the help of the 
integration by parts formula. There results: 
Sm(t) =
 t
S k(t)dtm
m! .
It follows from (6.2.10)–(6.2.12) that, for any integers M>N ≥ 0, and for all 
t ∈ (S, T ], we have 
|| ˙xM − ˙xN ||L1([S,t];Rn)
≤ || ˙xM − ˙xM−1||L1([S,t];Rn) + ... + || ˙xN+1 − ˙xN ||L1([S,t];Rn)
≤
⎡
⎢
⎣
 t
S k(s)dsM−1
(M − 1)! + .. +
 t
S k(s)dsN
N!
⎤
⎥
⎦
 t
S
ρF (s, y(s), y(s))ds ˙
≤
⎡
⎢
⎣
 T
S k(s)dsM−1
(M − 1)! + .. +
 T
S k(s)dsN
N!
⎤
⎥
⎦
ΛF (y). (6.2.13) 
(Here  t
S k(t)dtm
/m! := 1 when m = 0.) It is clear from this inequality that 
{ ˙xm} is a Cauchy sequence in L1. It follows that 
x˙m → v in L1,6.2 Existence and Estimation of F Trajectories 253
for some v ∈ L1. Define x ∈ W1,1 according to 
x(t) := ξ +
 t
S
v(s)ds.
Since 
||x − xm||L∞ ≤
 T
S
|v(s) − ˙xm(s)|ds
and x˙m → v in L1, we know that 
xm → x uniformly.
By extracting a subsequence (we do not relabel), we can arrange that 
x˙m → ˙x a.e..
Define O to be the subset of points t ∈ [S,T ] such that x˙m(t) ∈ F (t, xm−1(t)) for 
all index values m = 1, 2, .. and such that x˙m(t) → ˙x(t). Take any t ∈ O. Then 
x˙m(t) ∈ F (t, xm−1(t)).
Since F (t, .) has closed graph, we obtain in the limit 
x(t) ˙ ∈ F (t, x(t)).
But O has full measure. It follows that x is an F trajectory. 
Next observe that by setting N = 0 in inequality (6.2.13) , for all t ∈ (S, T ], we 
arrive at 
|| ˙xM − ˙y||L1([S,t];Rn) ≤ exp t
S
k(s)ds t
S
ρF (s, y(s), y(s))ds ˙
for M = 1, 2,... Since x˙M → ˙x in L1 as M → ∞ we deduce that , for all 
t ∈ (S, T ], 
|| ˙x − ˙y||L1([S,t];Rn) ≤ exp T
S
k(t)dt t
S
ρF (s, y(s), y(s))ds. ˙
This is the required estimate. 
It remains to prove that x can be chosen continuously differentiable when 
the ‘comparison function’ y is continuously differentiable, under the additional 
hypotheses.254 6 Differential Inclusions
Construct a sequence {xi} as above. Under the additional hypotheses, t →
F (t, x0(t)) is a continuous multifunction. In view of Proposition 6.2.2, x˙1 is a 
continuous function. (x˙1(t) is the projection of y(t) ˙ = ˙x0(t) onto F (t, x0(t))). 
Arguing inductively, we conclude that 
t → ˙xi(t) is continuous for all i ≥ 0.
Fix any i ≥ 0. For arbitrary t, x˙i+1(t) ∈ F (t, xi(t)) and x˙i+2(t) minimizes 
v → |˙xi+1(t) − v| over v ∈ F (t, xi+1(t)), by construction. Under the hypotheses, 
we can find w ∈ F (t, xi+1(t)) such that 
|w − ˙xi+1(t)| ≤ k∞|xi+1(t) − xi(t)| for all t.
Since xi+2(S) = xi+1(S) and i was chosen arbitrarily, we conclude that 
| ˙xi+2(t) − ˙xi+1(t)| ≤ k∞|xi+1(t) − xi(t)| for all t.
Now hypothesis (ii)' implies hypothesis (ii) with k(t) = k∞ for all t. By (6.2.12) 
then, for any integers M>N ≥ 2, we have 
|| ˙xM − ˙xN ||C ≤ k∞
⎡
⎢
⎣
 T
S k∞dtM−2
(M − 2)! + .. +
 T
S k∞dtN−1
(N − 1)!
⎤
⎥
⎦
ΛF (y).
It follows that x˙i is a Cauchy sequence in C. But C is complete, so the sequence 
has a strong C limit, some continuous function v. But v must coincide with x˙, the 
strong L1 limit of { ˙xi}, following adjustment on a null-set. It follows that x is a 
continuously differentiable function. ⨅⨆
Naturally, if we specialize to the point valued case, we recover an existence 
theorem for differential equations. In this important special case the solution is 
unique. We shall require 
Lemma 6.2.4 (Gronwall’s Lemma) Take an absolutely continuous function z :
[S,T ] → Rn. Assume that there exist non-negative integrable functions k and v
such that 
|
d
dt z(t)| ≤ k(t)|z(t)| + v(t) a.e. t ∈ [S,T ].
Then 
|z(t)| ≤ exp t
S
k(σ )dσ |z(S)| +  t
S
exp
−
 τ
S
k(σ )dσ
v(τ )dτ
for all t ∈ [S,T ].6.2 Existence and Estimation of F Trajectories 255
Proof Since z is absolutely continuous so too is t → |z(t)|. Let O ⊂ [S,T ] be the 
subset of points t such that z and |z| are both differentiable at t. O has full measure 
and, it is straightforward to show, 
d
dt |z(t)| ≤ |˙z(t)| for all t ∈ O.
Now define the absolutely continuous function 
η(t) := exp
−
 t
S
k(σ )dσ
|z(t)| for all t ∈ [S,T ].
Then for every t ∈ O we have 
η(t) ˙ = exp
−
 t
S
k(σ )dσ  d
dt |z(t)| − k(t)|z(t)|

≤ exp
−
 t
S
k(σ )dσ
[|˙z(t)| − k(t)|z(t)|]
≤ exp
−
 t
S
k(σ )dσ
v(t).
It follows that for each t ∈ [S,T ], 
η(t) ≤ η(S) +
 t
S
exp
−
 τ
S
k(σ )dσ
v(τ )dτ.
Since η(S) = |z(S)| and 
|z(t)| = exp t
S
k(σ )dσ
η(t),
we deduce 
|z(t)| ≤ exp t
S
k(σ )dσ |z(S)| +  t
S
exp
−
 τ
S
k(σ )dσ
v(τ )dτ
.
⨅⨆
Corollary 6.2.5 (ODE’s: Existence and Uniqueness of Solutions) Take a func￾tion f : [S,T ] × Rn → Rn, an arc y ∈ W1,1([S,T ]; Rn), ϵ ∈ (0,∞) ∪ {+∞}
and a point ξ ∈ Rn. (The case ϵ = +∞ is interpreted as in the statement of 
Theorem 6.2.2.) Assume that 
(i) f (., x) is measurable for each x ∈ Rn, 
(ii) there exists k ∈ L1 such that256 6 Differential Inclusions
|f (t, x'
) − f (t, x'')| ≤ k(t)|x' − x''|
for all x'
, x'' ∈ y(t) + ϵB, a.e. t ∈ [S,T ]. 
Assume further that 
K

|ξ − y(S)| +  T
S
| ˙y(t) − f (t, y(t))|dt
≤ ϵ,
where K := exp  T
S k(t)dt
. 
Then there exists a unique solution to the differential equation 
x(t) ˙ = f (t, x(t)), a.e. t ∈ [S,T ]
x(S) = ξ
which satisfies 
||x − y||L∞ ≤ |x(S) − y(S)| +  T
S
| ˙x(t) − ˙y(t)| dt
≤ K

|ξ − y(S)| +  T
S
| ˙y(t) − f (t, y(t))dt
.
Proof All the assertions of the corollary follow immediately from the generalized 
Filippov existence theorem (Theorem 6.2.3), with the exception of ‘uniqueness’. 
Suppose however that there are two solutions x' and x'' to the differential equation 
which satisfy ||x' − y||L∞ ≤ ϵ and ||x'' − y||L∞ ≤ ϵ. Define z(t) := x'
(t) − x''(t). 
Then under the hypotheses, for almost every t ∈ [S,T ], 
|˙z(t)|=|f (t, x'
(t)) − f (t, x''(t))|
≤ k(t)|x'
(t) − x''(t)| = k(t)|z(t)|.
Since z(S) = 0, it follows from Gronwall’s lemma (Lemma 6.2.4) that z ≡ 0. We 
conclude that x' = x''. Uniqueness is proved. ⨅⨆
6.3 Perturbed Differential Inclusions 
Consider a sequence of arcs whose elements satisfy perturbed versions of a 
‘nominal’ differential inclusion such that the perturbation terms in some sense tend 
to zero as we proceed along the sequence. When can we extract a subsequence with 
limit a solution to the nominal differential inclusion? As we shall see this is possible 
under unrestrictive hypotheses on the differential inclusion and on the nature of the6.3 Perturbed Differential Inclusions 257
perturbations. Necessary conditions in nonsmooth dynamic optimization are usually 
obtained by deriving necessary conditions for simpler, perturbed versions of the 
dynamic optimization problem of interest and passing to the limit. The significance 
of the results of this section is that they will justify the limit taking procedures. 
Use will be made of a characterization of subsets of L1 which are relatively, 
sequentially compact. 
Theorem 6.3.1 (Dunford Pettis Theorem) Let S be a bounded subset of 
L1([S,T ]; Rn). Then the following conditions are equivalent: 
(i) every sequence in S has a subsequence converging to some L1 function, with 
respect to the weak L1 topology, 
(ii) for every ϵ > 0 there exists δ > 0 such that for every measurable set D ⊂
[S,T ] and x ∈ S satisfying meas{D} < δ, we have 
D x(t)dt < ϵ. 
Proof See [97]. ⨅⨆
When condition (ii) above is satisfied, we shall say ‘the family of functions S is 
equi-integrable’. A simple criterion for equi-integrability (a sufficient condition to 
be precise) is that the family of functions is ‘uniformly integrably bounded’ in the 
sense that there exists an integrable function α ∈ L1 such that 
|x(t)| ≤ α(t) a.e. t ∈ [S,T ]
for all x ∈ S. 
We shall require also certain properties of the Hamiltonian H (t, x, p) associated 
with a given multifunction F : [S,T ] × Rn ⇝ Rn, defined at points (t, x, p) ∈
dom F × Rn. 
H (t, x, p) := sup
v∈F (t,x)
p · v. (6.3.1) 
Proposition 6.3.2 Consider a multifunction F : [S,T ] × Rn ⇝ Rn, which has 
values closed sets. 
(a) Fix (t, x) ∈ dom F. Assume that there exists c ≥ 0 such that F (t, x) ⊂ cB. 
Then 
|H (t, x, p)| ≤ c|p| for every p ∈ Rn,
and H (t, x, .) is Lipschitz continuous with Lipschitz constant c. 
(b) Fix t ∈ [S,T ]. Take convergent sequences xi → x and pi → p in Rn. Assume 
that there exists c ≥ 0 such that 
(t, xi) ∈ dom F and F (t, xi) ⊂ cB for all i ,
and that Gr F (t, .) is closed. Then258 6 Differential Inclusions
limsupi→∞H (t, xi, pi) ≤ H (t, x, p) .
(c) Take measurable functions x : [S,T ] → Rn, p : [S,T ] → Rn such that 
Gr x ⊂ dom F. Assume that 
F is L × Bn measurable.
Then t → H (t, x(t), p(t)) is a measurable function. 
Proof 
(a): Take (t, x) ∈ dom F. Choose any p, p' ∈ Rn. Since F (t, x) is a compact, 
non-empty set, there exists v ∈ F (t, x) such that 
H (t, x, p) = p · v .
Of course, H (t, x, p'
) ≥ p' · v. But then 
H (t, x, p) − H (t, x, p'
) ≤ (p − p'
) · v ≤ |v||p − p'
| .
Since |v| ≤ c, and the roles of p and p' can be interchanged, we deduce that 
H (t, x, p) − H (t, x, p'
) ≤ c|p − p'
| .
Notice, in particular, that |H (t, x, p)| ≤ c|p|, since H (t, x, 0) = 0. 
(b): Fix t ∈ [S,T ] and take any sequences xi → x and pi → p in Rn. 
lim supi H (t, xi, pi) can be replaced by limi H (t, xi, pi), following extraction of 
a suitable subsequence (we do not relabel). For each i, F (t, xi) is a nonempty 
compact set (since (t, xi) ∈ dom F); consequently, there exists vi ∈ F (t, xi) such 
that H (t, xi, pi) = pi · vi. But |vi| ≤ c for i = 1, 2,... . We can therefore arrange, 
by extracting another subsequence, that vi → v for some v ∈ Rn. Since Gr F (t, .)
is closed, v ∈ F (t, x) and so 
lim
i
H (t, xi, pi) = lim
i pi · vi = p · v ≤ H (t, x, p) .
(c): According to Proposition 2.3.2 and Theorem 2.3.12, we can find a measurable 
selection v of t ⇝ F (t, x(t)). Fix k > 0 and define for all (t, x) ∈ [S,T ] × Rn, 
F'
k(t, x) := F (t, x) ∩ (v(t) + kB)
and 
Fk(t, x) =

F'
k(t, x) if (t, x) ∈ dom F'
k
v(t) + kB otherwise .6.3 Perturbed Differential Inclusions 259
It is a straightforward exercise to show that Fk is L × Bn measurable. Since 
dom Fk = [S,T ] × Rn, we can define 
Hk(t, x, p) := max {p · v : v ∈ Fk(t, x)}
for all (t, x, p) ∈ [S,T ] × Rn × Rn. Clearly 
Hk(t, x(t), p(t)) → H (t, x(t, p(t))) as k → ∞,
for all t. It therefore suffices to show that t → Hk(t, x(t), p(t)) is measurable. 
However, by Corollary 2.3.3, this will be true if we can show that (t, (x, p)) →
Hk(t, (x, p)) is L × B2n measurable. 
Fix r ∈ R. We shall complete the proof by showing that 
D := 
(t, x, p) ∈ [S,T ] × Rn × Rn : Hk(t, x, p) ≥ r

is L × B2n measurable. Let {vi} be a dense subset of Rn. Take a sequence ϵj ↓ 0. It 
is easy to verify that D = D, where 
D := ∩∞
j=1 ∪∞
i=1

Ai,j × Bi,j 
,
in which 
Ai,j := 
(t, x) ∈ [S,T ] × Rn : (vi + ϵjB) ∩ Fk(t, x) /= ∅
and 
Bi,j := 
p ∈ Rn : p · vi > r − ϵj

.
(That D ⊂ D is obvious; to show that D ⊂ D, we exploit the fact that Gr Fk(t, .) is 
a compact set.) 
But D is obtainable from L × B2n measurable sets, by means of a countable 
number of union and intersection operations. It follows that D is L × B2n is 
measurable, as claimed. ⨅⨆
With these preliminaries behind us we are ready to answer the question posed at 
the beginning of the section. 
Theorem 6.3.3 (Compactness of Trajectories) Take a relatively open subset Ω ⊂
[S,T ] × Rn and a multifunction F : Ω ⇝ Rn. 
Assume that, for some closed multifunction X : [S,T ] ⇝ Rn such that Gr X ⊂
Ω, the following hypotheses are satisfied: 
(i) F is a closed, convex, non-empty multifunction, 
(ii) F is L × Bn measurable, 
(iii) for each t ∈ [S,T ], the graph of F (t, .) restricted to X(t) is closed.260 6 Differential Inclusions
Consider a sequence {xi} of W1,1([S,T ]; Rn) functions, a sequence {ri} in 
L1([S,T ]; R) such that ||ri||L1 → 0 as i → ∞ and a sequence {Ai} of measurable 
subsets of [S,T ] such that meas Ai → |T − S| as i → ∞. 
Suppose that: 
(iv) Gr xi ⊂ Gr X for all i, 
(v) { ˙xi} is a sequence of uniformly integrably bounded functions on [S,T ] and 
{xi(S)} is a bounded sequence, 
(vi) There exists c ∈ L1 such that 
F (t, xi(t)) ⊂ c(t)B
for a.e. t ∈ Ai and for i = 1, 2,... . 
Suppose further that 
x˙i(t) ∈ F (t, xi(t)) + ri(t)B a.e. t ∈ Ai.
Then along some subsequence (we do not relabel) 
xi → x uniformly and x˙i → ˙x weakly in L1
for some x ∈ W1,1([S,T ]; Rn) satisfying 
x(t) ˙ ∈ F (t, x(t)) a.e. t ∈ [S,T ].
Proof The x˙i’s are uniformly integrably bounded on [S,T ]. According to the 
Dunford Pettis theorem, we can arrange, by extracting a subsequence (we do not re￾label), that x˙i → v weakly in L1 for some L1 function v. Since {xi(S)} is a bounded 
sequence we may arrange by further subsequence extraction that xi(S) → ξ for 
some ξ ∈ Rn. Now define 
x(t) := ξ +
 t
S
v(s)ds.
By weak convergence, xi(t) → x(t) for every t ∈ [S,T ]. Clearly too x˙i → ˙x
weakly in L1. 
Now consider the Hamiltonian H (t, ξ , p) defined by (6.3.1) . Choose any p and 
any Lebesgue measurable subset V ⊂ [S,T ]. For almost every t ∈ V ∩Ai, we have 
H (t, xi(t), p) ≥ p · ˙xi(t) − ri(t)|p|.
Since all terms in this inequality are integrable, we deduce 

V ∩Ai
p · ˙xi(t)dt −

V ∩Ai
ri(t)|p|dt ≤

V ∩Ai
H (t, xi(t), p)dt.6.4 Existence of Minimizing F Trajectories 261
Because the x˙i’s are uniformly integrably bounded, meas[Ai]→|T − S|, x˙i → ˙x
weakly in L1 and ||ri||L1 → 0, we see that the left side of this relation has limit 

V p · ˙x(t). It follows that 

V
p · ˙x(t)dt ≤ lim sup
i→∞

V
χi(t)H (t, xi(t), p)dt.
Here χi denotes the characteristic function of the set Ai. 
Since χi(t)H (t, xi(t), p) is bounded above by c(t)|p|, we deduce from Fatou’s 
Lemma that 

V
p · ˙x(t) ≤

V
lim sup
i
χi(t)H (t, xi(t), p)dt.
From the upper semicontinuity properties of H then (see Proposition 6.3.2) 

V
(H (t, x(t), p) − p · ˙x(t)) dt ≥ 0.
Let {pi} be an ordering of the set of n-vectors having rational coefficients. Define 
D ⊂ [S,T ] to be the subset of points t ∈ [S,T ] such that t is a Lebesgue point 
of t → H (t, x(t), pi) − pi · ˙x(t) for all i. D is a set of full measure. For any 
t ∈ D ∩ [S,T ) and any i
H (t, x(t), pi) − pi · ˙x(t) =
lim
δ↓0
1
δ
 t+δ
t
[H (σ, z(σ, x(σ ), pi) − pi · ˙x(σ )] dσ ≥ 0 .
Since H (t, x(t), .) is continuous for each t, it follows that 
sup{p · e − p · ˙x(t) : e ∈ F (t, x(t))} ≥ 0 for all p ∈ Rn a.e. t ∈ [S,T ].
But F (t, x(t)) is closed and convex. We deduce with the help of the separation 
theorem that 
x(t) ˙ ∈ F (t, x(t)) a.e. t ∈ [S,T ].
We have confirmed that x is an F trajectory. ⨅⨆
6.4 Existence of Minimizing F Trajectories 
Take a relatively open subset Ω ⊂ [S,T ] × Rn, a multifunction F : Ω ⇝ Rn, 
a closed multifunction X : [S,T ] ⇝ Rn with the property that Gr X ⊂ Ω and a262 6 Differential Inclusions
closed set C ⊂ Rn × Rn . We define the set of admissible F trajectories (associated 
with the constraint sets X(t), S ≤ t ≤ T and C) to be 
RF (X, C) := 
x ∈ C([S,T ]; Rn) : x is an F trajectory,
x(t) ∈ X(t) for all t ∈ [S,T ] and (x(S), x(T )) ∈ C} .
We shall deduce from the results of the previous section the following criteria for 
compactness of the set of admissible F trajectories. 
Proposition 6.4.1 Take Ω, F, X and C as above. Assume that 
(i) F is a closed, L × Bn measurable multifunction, 
(ii) for each t ∈ [S,T ], the graph of F (t, .) restricted to X(t) is closed, 
(iii) there exist α ∈ L1 and β ∈ L1 such that 
F (t, x) ⊂ (α(t)|x| + β(t))B for all (t, x) ∈ Gr X,
(iv) either X(s) is bounded for some s ∈ [S,T ] or one of the following two sets 
C0 := 
x0 ∈ Rn : (x0, x1) ∈ C for some x1 ∈ Rn
C1 := 
x1 ∈ Rn : (x0, x1) ∈ C for some x0 ∈ Rn
is bounded. 
Assume further that 
(v) F (t, x) is convex for all (t, x) ∈ Gr X. 
Then RF (X, C) is compact with respect to the supremum norm topology. 
Proof Since the supremum norm topology is a metric topology, it suffices to 
prove sequential compactness. Accordingly, take any sequence of admissible F
trajectories {xi}. We must show that there exists an F trajectory x satisfying the 
constraints x(t) ∈ X(t) for all t ∈ [S,T ] and (x(S), x(T )) ∈ C such that 
xi → x uniformly
along some subsequence. But these conclusions can be drawn from Theorem 6.3.3 
provided we can show that the set RF (X, C) is bounded with respect to the 
supremum norm. By hypothesis (iv) however there exists k > 0 and s¯ ∈ [S,T ]
such that for any admissible F trajectory y we have 
|y(s)¯ | ≤ k.
By hypothesis (iii) 
| ˙y(t)| ≤ α(t)|y(t)| + β(t) a.e..6.4 Existence of Minimizing F Trajectories 263
It follows from Gronwall’s lemma (applied ‘backwards’ in time on the interval [S,s¯]
and ‘forwards’ on [¯s,T ]) that 
|y(t)| ≤ K for all t ∈ [S,T ]
where 
K = e||α||L1 
k + ||β||L1

.
We have confirmed that R(X, C) is bounded with respect to the supremum norm. 
⨅⨆
It is a simple step now to supply conditions for existence of solutions to the 
dynamic optimization problem 
(P )
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize g(x(S), x(T )) over x ∈ W1,1([S,T ]; Rn)
which satisfy
x(t) ˙ ∈ F (t, x(t)) a.e. t ∈ [S,T ],
x(t) ∈ X(t) for all t ∈ [S,T ],
and
(x(S), x(T )) ∈ C,
in which g : Rn × Rn → R is a given lower semi-continuous function. 
Indeed (P) can be equivalently formulated as a problem of seeking a minimizer 
of a lower semi-continuous function over a compact subset of C([S,T ]; Rn)
equipped with the supremum norm, namely 
Minimize ψ(y) over y ∈ RF (X, C)
where 
ψ(y) := g(y(S), y(T )).
(P) therefore has a minimizer provided RF (X, C) is non-empty. We have proved: 
Proposition 6.4.2 Take Ω, F, X, C and g as above. Assume that 
(i) F is a closed, L × Bn measurable multifunction, 
(ii) For each t ∈ [S,T ], the graph of F (t, .) restricted to X(t) is closed, 
(iii) There exist α ∈ L1 and β ∈ L1 such that 
F (t, x) ⊂ (α(t)|x| + β(t))B for all (t, x) ∈ Gr X,
(iv) Either X(s) is bounded for some s ∈ [S,T ] or one of the following two sets 
C0 := 
x0 ∈ Rn : (x0, x1) ∈ C for some x1 ∈ Rn264 6 Differential Inclusions
C1 := 
x1 ∈ Rn : (x0, x1) ∈ C for some x0 ∈ Rn
is bounded. 
Assume further that 
(a) The set of admissible F trajectories RF (X, C) is non-empty, 
(b) F (t, x) is convex for each (t, x) ∈ Gr X. 
Then (P) has a minimizer. 
6.5 Relaxation 
Suppose that an optimization problem of interest fails to have a minimizer. ‘Relax￾ation’ is the procedure of adding extra elements to the domain of the optimization 
problem to ensure existence of minimizers. 
For a relaxation scheme to be of interest it usually needs to be accompanied by 
the information that an element x¯ in the extended domain can be approximated by an 
element y in the original domain of the optimization problem (to the extent that we 
can arrange that the cost of y is arbitrarily close to that of x¯). In these circumstances, 
we can find a sub-optimal element for the original problem (i.e. one whose cost is 
arbitrarily close to the infimum cost) by finding a minimizer in the extended domain 
and approximating it. 
Relaxation will now be examined in connection with the optimization problem 
(P) of the preceding section, which for convenience we reproduce 
(P )
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize g(x(S), x(T )) over x ∈ W1,1([S,T ]; Rn)
which satisfy
x(t) ˙ ∈ F (t, x(t)) a.e. t ∈ [S,T ],
x(t) ∈ X(t) for all t ∈ [S,T ],
(x(S), x(T )) ∈ C .
We impose the hypotheses of Proposition 6.4.2 with the exception of the 
convexity hypothesis 
‘F (t, x) is convex for all (t, x) ∈ Gr X’.
In these circumstances (P) may fail to have a minimizer. The point is illustrated by 
the following example. 
Example 
Consider problem (P) when the state vector x is 2 dimensional. We write the 
components of the state vector x = (y, z). Set6.5 Relaxation 265
[S,T ]=[0, 1], Ω = [0, 1] × R2, X(t) = R2,
g((y0, z0), (y1, z1)) = z1,
C = {((y0, z0), (y1, z1)) : z0 = 0},
F (y, z) := ({−1} ∪ {+1}) × {|y|}.
Notice that, if an arc (y, z) satisfies the constraints, then 
z(t) =
 t
0
|y(s)|ds. (6.5.1) 
Evidently then this special case of (P) is a disguised version, arrived at through state 
augmentation, of the optimization problem 
(P )'
⎧
⎨
⎩
Minimize J (y) =  1
0 |y(s)|ds
over y ∈ W1,1([0, 1]; R) satisfying
y(t) ˙ ∈ {−1} ∪ {+1} a.e..
(If (y, z) is admissible for (P) then y and z are related by (6.5.1), y is admissible 
for (P)
' and the costs are the same; also, if y is admissible for (P)
' then (y, z), with 
z given by (6.5.1), is admissible for (P) and again the costs are the same.) 
However, as we shall see, (P)
' has no minimizers. It follows that, in this case, 
(P) fails to have a solution. The fact that all the hypotheses of Proposition 6.4.2 are 
satisfied with the exception of (b) confirms that the convexity hypothesis cannot be 
dispensed with. 
To show that (P)
' does not have a solution, notice first of all that 
J (y) ≥ 0 (6.5.2) 
for all arcs y satisfying the constraint of (P)
'
. Consider next the sequence of 
admissible arcs {yi}, 
yi(t) =
 t
0
vi(s)ds
where 
vi(s) =

+1 for s ∈ Ai ∩ [0, 1]
−1 for s /∈ Ai ∩ [0, 1]
and 
Ai = ∪∞
j=0[(2i)−12j, (2i)−1(2j + 1)].266 6 Differential Inclusions
An easy calculation yields: 
J (yi) = 2−(i+1) for i = 1, 2.. .
Since J (yi) → 0 as i → ∞ we conclude from (6.5.2) that the infimum cost is 0. If 
there exists a minimizer y¯ then 
J (y)¯ =
 1
0
| ¯y(s)|ds = 0.
This implies that y¯ ≡ 0. It follows that 
˙
y(t) ¯ = 0 a.e..
But then y¯ fails to satisfy the differential inclusion 
˙
y(t) ¯ ∈ ({−1} ∪ {+1}). a.e..
It follows that no minimizer exists. 
The pathological feature of the above problem is that the limit point y¯ of any 
minimizing sequence satisfies only the convexified differential inclusion 
˙
y(t) ¯ ∈ co ({−1} ∪ {+1}) a.e..
In light of this example, a natural relaxation procedure for us to adopt is to allow 
arcs which satisfy the convexified differential inclusion 
x(t) ˙ ∈ co F (t, x(t)) a.e. t ∈ [S,T ]. (6.5.3) 
Accordingly, an arc x ∈ W1,1 satisfying this dynamic constraint is called a ‘relaxed’ 
F trajectory. When it is necessary to emphasize the distinction with relaxed F
trajectories, we sometimes call F trajectories ‘ordinary’ F trajectories. 
As earlier discussed, for this concept of relaxed trajectory to be useful we need to 
know that arcs satisfying the ‘relaxed’ dynamic constraint (6.5.3) can be adequately 
approximated by arcs satisfying the original, unconvexified, constraint. That this 
can be done is a consequence of the generalized Filippov existence theorem and the 
following theorem of R.J. Aumann on integrals of multifunctions. 
Theorem 6.5.1 (Aumann’s Theorem) Take a Lebesgue measurable multifunction 
� : [S,T ] ⇝ Rn which is closed and non-empty. Assume that there exists c ∈ L1
such that 
�(t) ∈ c(t)B for all t ∈ [S,T ].6.5 Relaxation 267
Then 
 T
S
�(s)ds =
 T
S
co �(s)ds,
where 
 T
S
A(s)ds :=  T
S
γ (s)ds : γ is a Lebesgue measurable selection of A

for A = � and A = co �. 
The ground has now been prepared for: 
Theorem 6.5.2 (Relaxation Theorem) Take a relatively open set Ω ⊂ [S,T ]×Rn
and a L×Bn measurable multifunction F : Ω ⇝ Rn which is closed and non-empty. 
Assume that there exist k ∈ L1 and c ∈ L1 such that 
F (t, x'
) ⊂ F (t, x'') + k(t)B for all (t, x'
), (t, x'') ∈ Ω
and 
F (t, x) ⊂ c(t)B for all (t, x) ∈ Ω.
Take any relaxed F trajectory x with Gr x ⊂ Ω and any δ > 0. Then there exists an 
ordinary F trajectory y which satisfies y(S) = x(S) and 
max
t∈[S,T ]
|y(t) − x(t)| < δ.
Proof Choose ϵ > 0 such that T (x, 2ϵ) ⊂ Ω and let α be such that 
0 <α< min  ϵ
K ln K , ϵ,
δ
1 + K ln K

,
where K := exp(||k||L1 ). Let h > 0 be such that 

I
c(t)dt < α/2
for any subinterval I ⊂ [S,T ] of length no greater than h. 
Let {S = t0, t1, .., tk = T } be a partition of [S,T ] such that meas Ii < h for 
i = 1, 2, .., k where Ii := [ti−1, ti) for i = 1, .., k − 1 and Ik = [tk−1, tk]. The 
multifunction t ⇝ F (t, x(t)) is Lebesgue measurable (see Proposition 2.3.2) and 
satisfies 
F (t, x(t)) ⊂ c(t)B for all t ∈ [S,T ].268 6 Differential Inclusions
Recalling that x is a co F trajectory, we deduce from Aumann’s theorem (The￾orem 6.5.1) that there exist measurable functions fi : [S,T ] → Rn such that 
fi(t) ∈ F (t, x(t)) a.e. t ∈ Ii and 

Ii
fi(t)dt =

Ii
x(t)dt ˙
for i = 1, .., k. Define 
f (t) := 
k
i=1
fi(t)χIi(t)
where χIi denotes the characteristic function of Ii and set 
z(t) = x(S) +
 t
S
f (s)ds for t ∈ [S,T ].
Fix t ∈ [S,T ]. Then for some j ∈ {1, .., k}, 
|z(t) − x(t)|=|
j−1
i=1

Ii
(fi(σ ) − ˙x(σ ))dσ +

Ij∩[S,t]
(fj (σ ) − ˙x(σ ))dσ|
≤ 0 + 2

Ij
c(t)dt < α.
It follows that 
||z − x||L∞ < α. (6.5.4) 
Since α<ϵ, we have 
T (z, ϵ) ⊂ Ω.
Notice that, since z(t) ˙ ∈ F (t, x(t)) a.e. and in view of Proposition 6.2.1, 
ρF (t, z(t), z(t)) ˙ ≤ ρF (t, x(t), z(t)) ˙ + k(t)|x(t) − z(t)|
< k(t)α a.e..
We have 
ΛF (z) :=  T
S
ρF (t, z(t), z(t))dt < α ˙ ln K.6.5 Relaxation 269
Since αK ln K<ϵ, we deduce from the generalized Filippov existence theorem 
that there exists an F trajectory y such that y(S) = x(S) and 
||y − z||L∞ ≤ KΛF (z) < αK ln K.
By (6.5.4) then 
||y − x||L∞ ≤ ||y − z||L∞ + ||z − x||L∞ ≤ α(K ln K + 1).
Since however 
α(K ln K + 1) < δ,
we conclude that the F trajectory y satisfies 
||y − x||L∞ < δ.
⨅⨆
The optimization problem obtained when we replace the dynamic constraint x˙ ∈
F in (P) by x˙ ∈ co F will be denoted (P )relaxed. Minimizers for (P )relaxed will 
be called relaxed minimizers for (P). 
The following proposition provides information clarifying the relation between 
(P) and (P )relaxed. 
Proposition 6.5.3 Take Ω, F, X, C and g as above. Assume that 
(i) F is a compact, L × Bn measurable multifunction, 
(ii) For each t ∈ [S,T ], the graph of F (t, .) restricted to X(t) is closed, 
(iii) There exist α ∈ L1 and β ∈ L1 such that 
F (t, x) ⊂ (α(t)|x| + β(t))B for all (t, x) ∈ Gr X.
(iv) Either X(s) is bounded for some s ∈ [S,T ] or one of the following two sets 
C0 := 
x0 ∈ Rn : (x0, x1) ∈ C for some x1 ∈ Rn
C1 := 
x1 ∈ Rn : (x0, x1) ∈ C for some x0 ∈ Rn
is bounded. 
Assume further that the set of admissible F trajectories RF (X, C) is non-empty. 
Then (P )relaxed has a minimizer. 
If, in addition, we assume that 
(a) there exists k ∈ L1 such that270 6 Differential Inclusions
F (t, x) ⊂ F (t, x'
) + k(t)|x − x'
|B for all (t, x), (t, x'
) ∈ Ω,
(b) g is continuous, 
and for some relaxed minimizer x¯ and ϵ > 0
(c) 
x(t) ¯ + ϵB ⊂ X(t) for all t ∈ [S,T ], (6.5.5) 
(d) 
either (x(S) ¯ + ϵB) ×{¯x(T )} ⊂ C or { ¯x(S)} × (x(T ) ¯ + ϵB) ⊂ C. (6.5.6) 
Then 
inf(P )relaxed = inf(P ).
The right and left sides of the last relation denote the infimum cost for (P) and 
(P )relaxed respectively. 
Proof Existence of a minimizer for (P )relaxed follows immediately from Proposi￾tion 6.4.2 applied to the modified version of (P) in which co F replaces F. (Notice 
that co F inherits the measurability properties of F according to Proposition 2.3.8, 
as well as the linear growth properties.) 
Suppose that there exists a relaxed minimizer x¯ and ϵ > 0 such that 
x(t) ¯ + ϵB ⊂ X(t) for all t ∈ [S,T ]
and 
{ ¯x(S)} × (x(T ) ¯ + ϵB) ⊂ C.
(The case (x(S) ¯ + ϵB) × {¯x(T )} ⊂ C is treated by ‘reversing time’.) Take any 
α > 0. Then noting the continuity of the function g and applying the relaxation 
theorem, Theorem 6.5.2, we can find an ordinary F trajectory y¯ such that 
y(S) ¯ = ¯x(S), || ¯y − ¯x||L∞ < ϵ
and 
g(y(S), ¯ y(T )) < g( ¯ x(S), ¯ x(T )) ¯ + α.
Clearly y¯ satisfies the constraints 
(y(S), ¯ y(T )) ¯ ∈ C and y(t) ¯ ∈ X(t) for all t ∈ [S,T ].6.6 Estimates on Trajectories Confined to a Closed Subset 271
In other words, y¯ is an admissible (ordinary) F trajectory. It follows that 
inf(P )relaxed = g(x(S), ¯ x(T )) > g( ¯ y(S), ¯ y(T )) ¯ − α ≥ inf(P ) − α.
Since α > 0 is arbitrary and 
inf (P )relaxed ≤ inf(P ),
we conclude that 
inf(P )relaxed = inf(P ).
⨅⨆
Notice the crucial role of the ‘interiority’ hypotheses (6.5.5) and (6.5.6). If all 
relaxed minimizers violate these hypotheses then the relaxation theorem does not 
automatically imply that the infimum costs of (P) and (P )relaxed coincide; while 
it is true that any relaxed minimize x¯ can be uniformly approximated by an F
trajectory y, we cannot in general guarantee that y will satisfy the constraints for 
it to qualify as an admissible F trajectory. 
6.6 Estimates on Trajectories Confined to a Closed Subset 
In the final section of this chapter, we provide information about F trajectories 
x that satisfy a pathwise constraint x(t) ∈ A, for a given subset A of the state 
space. Conditions for the existence of such F trajectories is the subject matter of 
viability/invariance theory. The focus of this section is on providing estimates of the 
distance of a given F trajectory from the family of F trajectories that satisfy the state 
constraint. Such estimates, known as ‘distance estimates’, are important tools in the 
study of constrained dynamic optimization problems, where they are used both in 
dynamic programming and in the derivation of necessary conditions of optimality. 
Consider the state-constrained differential inclusion: 

x(t) ˙ ∈ F (t, x(t)) for a.e. t ∈ [S,T ]
x(t) ∈ A for all t ∈ [S,T ] , (6.6.1) 
in which [S, T ] is a given interval (T > S), F : [S, T ] × Rn ⇝ Rn is a given 
multifunction with closed, non-empty values, and A ⊂ Rn is a given closed set. We 
recall that, given a subinterval (possibly closed or left open) I ⊂ [S, T ], we refer to 
an absolutely continuous function x : I → Rn which satisfies x(t) ˙ ∈ F (t, x(t)) a.e. 
as an F trajectory (on I ). An F trajectory x on I is said to be ‘admissible’ (on I ) if 
x(t) ∈ A for all t ∈ I , and ‘strictly admissible’ (on I ) if x(t) ∈ int A for all t ∈ I .272 6 Differential Inclusions
Theorem 6.6.1 (Distance Estimate) Take a multifunction F : [S, T ] × Rn ⇝ Rn
and a closed set A ⊂ Rn. Fix r0 > 0. Assume that, for some constant c0 > 0, 
some functions c, kF ∈ L1(S, T ) and for R0 := e
 T
S c(t) dt(r0 + 1), the following 
hypotheses (H1), (H2), (H3), (IPC) and (BVL) are satisfied: 
(H1): F : [S, T ] × Rn ⇝ Rn takes closed, non-empty values, F (., x) is L￾measurable for all x ∈ Rn, 
(H2): (i) F (t, x) ⊂ c(t)(1 + |x|) B for all x ∈ Rn and for a.e. t ∈ [S, T ] ,
(ii) F (t, x) ⊂ c0 B for all (t, x) ∈ [S, T ] × R0B ,
(H3): F (t, x'
) ⊂ F (t, x) + kF (t)|x − x'
| B
for all x, x' ∈ R0B and a.e. t ∈ [S, T ] ,
(IPC): for each (t, x) ∈ [S, T ] × (R0B ∩ ∂A), 

lim inf
(t'
,x'
)
D
→(t,x)
co F (t'
, x'
)

∩ int T¯
A(x) /= ∅ ,
where D := [S, T ] × A, 
(BVL): F (., x) has bounded variation from the left uniformly over x ∈ (∂A +
η0B) ∩ R0B for some η0 > 0, in the following sense: there exists a non￾decreasing bounded variation function η : [S, T ] → R (called a ‘modulus 
of variation of F (., x)’) such that, for every [s, t]⊂[S, T ] and x ∈ (∂A+
η0B) ∩ R0B, 
F (s, x) ⊂ F (t, x) + (η(t) − η(s)) × B .
Then there exists a constant K > 0 with the following property: 
Given any interval [t0, t1]⊂[S, T ], any F trajectory xˆ on [t0, t1] with x(t ˆ 0) ∈
A ∩ 
e
 t
0 
S c(t) dt(r0 + 1) − 1

B, and any ρ > 0 such that 
ρ ≥ max{dA(x(t)) ˆ : t ∈ [t0, t1]} ,
we can find an F trajectory x on [t0, t1] such that x(t0) = ˆx(t0), 
x(t) ∈ int A for all t ∈ (t0, t1]
and 
|| ˆx − x||L∞(t0,t1) ≤ Kρ. (6.6.2) 
(Recall that T¯
A(x) denotes the Clarke tangent cone to A at x.) 
Such estimates are referred to as linear L∞ distance estimates for F trajectories 
confined to a closed set. They provide an upper bound on the distance of a given 
F trajectory xˆ from the set of F trajectories satisfying a state constraint, in terms 
of the expression maxt∈[t0,t1] dA(x(t)) ˆ , which can be interpreted as a measure of the6.6 Estimates on Trajectories Confined to a Closed Subset 273
state constraint violation by xˆ. The assertions of the theorem cover two cases, each 
of independent interest: 
Case A: max{dA(x(t)) ˆ : t ∈ [t0, t1]} > 0 (xˆ is not admissible). 
In this case, an F trajectory x with initial value x(t ˆ 0) and strictly admissible on 
(t0, t1] exists, which satisfies the linear distance estimate 
|| ˆx − x||L∞(t0,t1) ≤ K max{dA(x(t)) ˆ : t ∈ [t0, t1]} .
(This follows from the theorem statement, after setting ρ := max{dA(x(t)) ˆ : t ∈
[t0, t1]} .) 
Case B: max{dA(x(t)) ˆ : t ∈ [t0, t1]} = 0 (xˆ is admissible). 
In this case, for arbitrary ϵ > 0, there exists an F trajectory x, with initial value 
x(t ˆ 0) and strictly admissible on (t0, t1] such that 
|| ˆx − x||L∞(t0,t1) ≤ ϵ .
(This follows from the theorem statement, after setting ρ := ϵ/K .) 
The next theorem is a variant on the preceding distance estimate, in which it is 
asserted that, if F (t, x) has a two-sided bounded variation property w.r.t. the time 
variable (in place of the one-sided property required in Theorem 6.6.1), a linear 
L∞ estimate is still valid, under a less restrictive version of the inward pointing 
condition. 
Theorem 6.6.2 The assertions of Theorem 6.6.1 remain valid when assumptions 
(IPC) and (BVL) are replaced by the following hypotheses (IPC)' and (BV)A (for 
R0 := e
 T
S c(t) dt(r0 + 1)): 
(IPC)'
: for each t ∈ [S, T ), s ∈ (S, T ] and x ∈ R0B ∩ ∂A, 

lim inf
x' A
→x
lim sup
t'
↓t
co F (t'
, x'
)

∩ int T¯
A(x) = ∅ / ,

lim inf
x' A
→x
lim sup
s'
↑s
co F (s'
, x'
)

∩ int T¯
A(x) /= ∅ ,
(BV)A: F (., x) has bounded variation uniformly over x ∈ (∂A + η0B) ∩ R0B
for some η0 > 0, in the following sense: there exists a non-decreasing 
bounded variation function η : [S, T ] → R such that, for every [s, t] ⊂
[S, T ] and x ∈ (∂A + η0B) ∩ R0B, 
dH (F (s, x), F (t, x)) ≤ η(t) − η(s) .
Here, dH (A,B) is the Hausdorff distance function between two arbitrary non￾empty closed sets in Rn A and B:274 6 Differential Inclusions
dH (A, B) := max 
sup
a∈A
dB(a), sup
b∈B
dA(b)
. (6.6.3) 
As a preliminary step in the proof of Theorem 6.6.1, we establish some technical 
lemmas. 
The first lemma, whose proof exploits properties of the interior of Clarke’s 
tangent cone, allows us to select suitable inward pointing vectors which belong to 
the convex hull of the velocity set F or are limits of such vectors. 
Lemma 6.6.3 Suppose the multifunction F : [S, T ] × Rn ⇝ Rn and the closed set 
A satisfy hypothesis (IPC) (for some R0 > 0). Then there exist M > 0 , ϵ > 0 and 
η >¯ 0 with the following property: for any (t, x) ∈ [S, T ]×
(∂A+ ¯ηB)∩R0B∩A

, 
there exists v ∈ co F (t, x) ∩ MB such that 
y + [0, ϵ](v + ϵB) ⊂ A, for all y ∈ (x + ϵB) ∩ A. (6.6.4) 
Proof 
Step 1: We claim that for each (t, x) ∈ [S, T ] × 
R0B ∩ ∂A
there exist Mt,x > 0,
ϵt,x ∈ (0, 1) and δt,x ∈ (0, ϵt,x ] such that, given any (t'
, x'
) ∈ ((t, x) + δt,xB) ∩
([S, T ] × A), a vector v' ∈ co F (t'
, x'
) can be found such that |v'
| ≤ Mt,x and 
y' + [0, ϵt,x ](v' + ϵt,xB) ⊂ A, for all y' ∈ (x' + ϵt,xB) ∩ A .
Indeed, take any (t, x) ∈ [S, T ] × (R0B ∩ ∂A) and chose any vector 
v ∈

lim inf
(t'
,x'
)
[S,T ]×A −→ (t,x)
co F (t'
, x'
)

∩ int T¯
A(x) .
By the characterization of the interior of the Clarke tangent cone (see Theo￾rem 4.11.1), there exists ϵ ∈ (0, 1) such that 
y + [0, ϵ](v + 2ϵB) ⊂ A, for all y ∈ (x + 2ϵB) ∩ A . (6.6.5) 
On the other hand, by definition of the limit inferior operation, there exists δ ∈
(0, ϵ] such that, given any (t'
, x'
) ∈ ((t, x) + δB) ∩ ([S, T ] × A), there exists 
v' ∈ co F (t'
, x'
) satisfying |v − v'
| ≤ ϵ. Then, |v'
|≤|v| + 1(=: Mt,x ). 
Now take any y' ∈ (x' + ϵB) ∩ A. Then, since x' + ϵB ⊂ x + 2ϵB and v' ∈ v + ϵB, 
we may conclude from (6.6.5) that 
y' + [0, ϵ](v' + ϵB) ⊂ A, for all y' ∈ (x' + ϵB) ∩ A .
Step 2: By a standard compactness argument, we can find a finite number of points 
(ti, xi) ∈ [S, T ]×
R0B∩∂A
and numbers Mi > 0, ϵi > δi > 0, for i = 1,...,m, 
such that6.6 Estimates on Trajectories Confined to a Closed Subset 275

i=1,...,m

(ti, xi) + δi
◦
B

⊃ [S,T ] × 
R0B ∩ ∂A
, (6.6.6) 
and for each (t'
, x'
) ∈ ((ti, xi) + δiB) ∩ ([S, T ] × A), there exists a vector v' ∈
co F (t'
, x'
) such that |v'
| ≤ Mi and 
y' + [0, ϵi](v' + ϵiB) ⊂ A, for all y' ∈ (x' + ϵiB) ∩ A.
Notice also that there exists η¯ ∈ (0, mini=1,...,m δi) such that 

i=1,...,m

(ti, xi) + δi
◦
B

⊃ [S,T ] × 
(∂A + ¯ηB) ∩ R0B

,
otherwise we could find a sequence of points (sj , yj ) ∈ ([S, T ] × R0B) \

i=1,...,m 
(ti, xi) + δi
◦
B

such that (sj , yj ) → (s, y) ∈ [S, T ] × 
R0B ∩ ∂A
, 
which would contradict (6.6.6). 
To conclude we just take ϵ := mini=1,...,m ϵi, M := maxi=1,...,m Mi and the 
assertions of the lemma immediately follow. ⨅⨆
The following lemma summarizes some implications of the ‘bounded variation’ 
hypotheses (BVL) and (BV)A: 
Lemma 6.6.4 Take a multifunction F : [S, T ]×Rn ⇝ Rn that satisfies hypotheses 
(H1), (H2), (H3) and (BVL) for some R0 > 0. Take also any interval [a, b]⊂[S, T ]. 
Define F
 : [a, b] × Rn ⇝ Rn to be 
F (t, x)  :=
⎧
⎨
⎩
lim supt↓a F (t, x) if t = a
F (t, x) if t ∈ (a, b)
lim supt↑b F (t, x) if t = b ,
(6.6.7) 
for any x ∈ Rn. Then F
 takes values, closed non-empty sets, 
F (t, x)  ⊂ c0B, for all (t, x) ∈ [a, b] × R0B
and 
F (t, x)  ⊂ F (t, x  '
) + kF (t)|x − x'
|B for all x, x' ∈ R0B, and a.e. t ∈ [a, b] .
Furthermore 
F (s, x)  ⊂ F (t, x)  +(η(t) ˜ − ˜η(s))×B for all x ∈ (∂A+η0B)∩ R0B (6.6.8) 
for any [s, t]⊂[a, b], where276 6 Differential Inclusions
η(t) ˜ :=
⎧
⎨
⎩
0 for t = a
η(t) − η(a+) for t ∈ (a, b)
η(b−) − η(a+) for t = b ,
(6.6.9) 
in which η is the ‘modulus of variation’ in hypothesis (BVL). 
Assume, in addition, that also (BV)A is satisfied. Then, we can supplement the 
assertions above with the following extra information 
sup x∈(∂A+η0B)∩R0B
dH (F (s, x),  F (t, x))  ≤ ˜η(t) − ˜η(s) . (6.6.10) 
Proof Suppose first that assumptions (H1), (H2), (H3) and (BVL) are satisfied. We 
omit the proof of the assertions that the constructed F
takes values non-empty closed 
sets, and inherits the Lipschitz continuity and boundedness properties of the original 
multifunction F (with the same constants), since this is a straightforward exercise. 
Consider the final assertion. Take any [s, t]⊂[a, b]. We must show (6.6.8). This 
relation is obviously true for a < s < t < b. Consider the case [s, t]=[a, b]. 
(The remaining cases [s, t]=[a, t'
] or [t'
, b], for some t' ∈ (a, b) are similar, but 
simpler, to deal with.) Fix any x ∈ (∂A + η0B) ∩ R0B. Take any v ∈ F (a,  x) and 
we must find w ∈ F (b,  x) such that 
|v − w| ≤ η(b−) − η(a+) . (6.6.11) 
By definition of the ‘lim sup’ operation, there exist si ↓ a and vi → v such that 
vi ∈ F (si, x) for each i. Now take any sequence ti ↑ b. Then by the properties of 
F (t, x), which coincides with F (t,  x) for t ∈ (a, b) we have: for each x, there exists 
wi ∈ F (ti, x) such that 
|vi − wi| ≤ η(ti) − η(si) (6.6.12) 
The sequence {wi} is bounded, in view of hypothesis (H2). So, by restricting 
attention to a subsequence, we can arrange that wi → w for some w ∈ Rn. But 
then, again by definition of the ‘lim sup’ operation, w ∈ F (b,  x). Passing to the 
limit in (6.6.12), and noting that η, as a monotone function, has left and right limits 
everywhere, we arrive at (6.6.11). 
If the stronger assumption (BV)A is also satisfied, then (6.6.10) follows from the 
argument above in view of the symmetric property of the Hausdorff distance. The 
proof is complete. ⨅⨆
Proof of Theorem 6.6.1 We take F to be the multifunction in the statement of 
Theorem 6.6.1, satisfying (H1), (H2), (H3), (IPC) and (BVL) for some positive 
numbers r0, c0, η0 (and R0 := exp{
 T
S c(t) dt}(r0 + 1)), some integrable functions 
c and kF , and some non-decreasing function η.6.6 Estimates on Trajectories Confined to a Closed Subset 277
Remark 
We take note, for future use, of an implication of Lemmas 6.6.3 and 6.6.4. Take any 
[t0, t1]⊂[S, T ] and define F
 : [t0, t1] × Rn ⇝ Rn according to (6.6.7). Take an 
arbitrary point (t, x) ∈ [t0, t1]×
(∂A+η0B)∩R0B∩A

. Then there exists a vector 
v ∈ co F (t,  x) such that (6.6.4) is true for all y ∈ (x+ϵB)∩A. Here ϵ is the positive 
constant as in the lemma statement. This follows from the way (IPC) is formulated 
and the fact that, for any (t, x) ∈ [t0, t1] × Rn: 
lim inf
(t'
,x'
)
D
→(t,x)
co F (t'
, x'
) ⊂ lim inf
(t'
,x'
)
D
→(t,x)
co F (t  '
, x'
) ,
where D = [S, T ] × A. 
Henceforth, we take ϵ > 0, η¯ ∈ (0, η0) and M = c0 to be the constants referred 
to in Lemma 6.6.3: we can arrange (by reducing the size of η¯, if necessary) that 
η¯ ∈ (0, η0) and M = c0 are also the constants appearing in Lemma 6.6.3. (This 
is true since, if the statement of Lemma 6.6.3 is valid for some η¯ and M, then it 
remains valid for any lower positive value of the parameter η¯ and bigger value of 
the constant M.) 
The Lemma 6.6.5 below are aimed at simplifying the proof of Theorem 6.6.1, by 
showing that attention can be restricted to a special case. ⨅⨆
Lemma 6.6.5 (Hypothesis Reduction) Fix r0 > 0. Assume that, for δ > 0, ρ >¯ 0 
and γ > 0 sufficiently small, that the assertions of Theorem 6.6.1 are valid when 
conditions (H1), (H2), (H3), (IPC) and (BVL) are satisfied (for some constants 
r0, c0, η0, R0 := exp{
 T
S c(t) dt}(r0 + 1), some functions c, kF ∈ L1(S, T )
and some non-decreasing bounded variation function η), and under the additional 
hypotheses: 
(H2)'
: F (t, x) ⊂ c0 B for all (t, x) ∈ [S, T ] × Rn,
(H3)'
: F (t, x'
) ⊂ F (t, x) + kF (t)|x − x'
| B
for all x, x' ∈ Rn and a.e. t ∈ [S, T ] ,
(H4): F (t, x) is convex for all (t, x) ∈ [S, T ] × Rn, 
and when the following conditions are imposed on the reference F trajectory xˆ :
[t0, t1] → Rn, with x(t ˆ 0) ∈ A ∩ 
e
 t
0 
S c(t) dt(r0 + 1) − 1

B, and the positive number 
ρ ≥ max{dA(x(t)) ˆ : t ∈ [t0, t1]}: 
(i): t1 − t0 ≤ δ, 
(ii): ρ ≤ ¯ρ, 
(iii): η(t1) − η(t0) ≤ γ and η(t0) = 0.
Then the assertions are valid under (H1), (H2), (H3), (IPC) and (BVL) alone, 
without conditions (i)–(iii) on xˆ and ρ, and when the extra hypotheses (H2)'
, (H3)'
and (H4) are no longer required to be satisfied. 
Proof In what follows we shall make use of the following fact: given any r0 > 0 
and any interval [t0, t1]⊂[S, T ], for any F trajectory x on [t0, t1] with x(t0) ∈278 6 Differential Inclusions

e
 t
0 
S c(t) dt(r0 +1)−1

B (c is the summable function of assumption (H2)), standard 
a-priori estimates yield: x(t) ∈ 
e
 t
S c(s) ds(r0+1)−1

B (⊂ R0B) for all t ∈ [t0, t1]. 
Step 1: Assume that the assertions of the lemma are valid (with constant K) 
when conditions (H1), (H2), (H3), (IPC) and (BVL) are satisfied together with the 
additional hypotheses (H2)'
, (H3)' and (H4), and when it is assumed that xˆ on 
[t0, t1] satisfies conditions (i)–(iii). We show that they remain valid (with a modified 
K) even if condition (iii) is no longer required to be satisfied. 
Choose any γ > 0, δ > 0, ρ >¯ 0 and K > 0, such that the assertions of the 
lemma are valid. 
The function η is increasing, and can therefore be decomposed into the sum of 
a continuous function ηc and a countable family of functions {si} satisfying, for 
each i, 
si(t) = ai ×

1 if t>σi
0 if t<σi ,
in which {σi} is a sequence of distinct points in [S, T ] (the ‘jump times’) and {ai}
is a sequence of non-negative numbers (the ‘jumps’). (In the analysis to follow, we 
do not have to take account of the value of si at its jump time σi.) The collection of 
jumps {ai} satisfies 

i
ai < ∞ .
In view of this last relation, there exists a finite index set J ⊂ {1, 2,...} such that 

i /∈J
ai < γ/2 .
Since the jump times {σi} are distinct, there exists α >¯ 0 such that 
|σi − σj | > α¯ for i, j ∈ J, i /= j .
By reducing the size of δ > 0, if necessary, we can also ensure that 
ηc
(t'
) − ηc
(s'
) < γ/2
for all subinterval [s'
, t'
]⊂[S, T ] such that t' −s' ≤ δ. Now further reduce the size 
of δ, if necessary, to ensure also that δ < α¯. ⨅⨆
Take any interval [t0, t1]⊂[S, T ] and F trajectory xˆ on [t0, t1] with x(t ˆ 0) ∈
A ∩ 
e
 t
0 
S c(t) dt(r0 + 1) − 1

B such that conditions (i) and (ii) are satisfied, i.e. 
t1 − t0 ≤ δ and max{dA(x(t)) ˆ : t ∈ [s,t]} ≤ ¯ρ .6.6 Estimates on Trajectories Confined to a Closed Subset 279
Since we have arranged that δ < α¯, there is at most one jump time t
¯ = σj , with 
j ∈ J , located in the interval [t0, t1]. It follows that 
η(t
¯
−) − η(t+
0 ) ≤ γ and η(t−
1 ) − η(t
¯
+) ≤ γ . (6.6.13) 
Case 1: t
¯ ∈ (t0, t1). 
Define F
1 : [t0, t
¯] × Rn ⇝ Rn according to (6.6.7) and η˜1 according to (6.6.9) 
when [a, b]=[t0, t
¯]. Similarly, set F
2 : [t ,¯ t1] × Rn ⇝ Rn according to (6.6.7) and 
η˜2 according to (6.6.9) when [a, b]=[t ,¯ t1].
F
1 satisfies assumptions (H1), (H2), (H3), (IPC) and (BVL) together with the 
additional hypotheses (H2)'
, (H3)' and (H4) of F, restricted to [t0, t
¯] × Rn, with 
the same constants c0, R0 and η¯, and functions c, kF and η. Also, the assertions of 
Lemma 6.6.4 continue to be true, with the same constants ϵ > 0 and η¯, when F
1 
replaces F. By Lemma 6.6.4 and (6.6.13) however, the variation modulus η˜1 of F
1 
is such that 
η˜1(t)¯ − ˜η1(t0) = η(t
¯
−) − η(t+
0 )<γ and η˜1(t0) = 0.
We have shown that xˆ restricted to [t0, t
¯] satisfies not just conditions (i) and (ii), 
but also condition (iii), in the lemma statement, when xˆ is interpreted as a F
1 
trajectory. We have confirmed that F
1 satisfies all the conditions required in the 
lemma statement. This means that, if we take any ρ > 0 such that 
ρ ≥ max{dA(x(t)) ˆ : t ∈ [t0, t1]}
and ρ ≤ ¯ρ, there exists an F
1 trajectory x1 (which is also an F trajectory) satisfying 
x1(t0) = ˆx(t0) and 
|| ˆx − x1||L∞(t0,t)¯ ≤ Kρ.
Also, x1(t) ∈ int A for t ∈ (t0, t
¯]. By Filippov’s existence theorem (Theorem 6.2.3), 
there exists an F trajectory y on [t ,¯ t1] such that y(t)¯ = x1(t)¯ and 
||y − ˆx||L∞(t ,t ¯ 1) ≤ K1Kρ ,
where K1 := e
 T
S kF (t) dt. We see that 
max{dA(y(t)) : t ∈ [t0, t
¯]} ≤ (1 + K1K)ρ .
Define ρ1 > 0 to be 
ρ1 := (1 + K1K)ρ .280 6 Differential Inclusions
Arguing as above, now in relation to F
2, we conclude that there exists an F
2 
trajectory x2 on [t ,¯ t1] (which is also an F trajectory) such that 
||y − x2||L∞(t ,t ¯ 1) ≤ Kρ1 = (K1K + 1)Kρ ,
such that x2(t) ∈ int A for all t ∈ [t ,¯ t1]. (We have used the fact that y(t)¯ = x1(t)¯ ∈
int A.) But then, by the triangle inequality, 
|| ˆx − x2||L∞(t ,t ¯ 1) ≤ (1 + K1 + K1K)Kρ .
Now define x to be the F trajectory on [t0, t1] obtained by concatenating x1 (on 
[t0, t
¯]) and x2 (on [t ,¯ t1]). Then x(t0) = ˆx(t0) and x(t) ∈ int A for t ∈ (t0, t
¯]. 
Furthermore 
|| ˆx − x||L∞(t0,t1) ≤ max{K1Kρ , (1 + K1 + K1K)Kρ} = K'
ρ
where 
K' := (1 + K1 + K1K)K .
Case 2: t / ¯ ∈ (t0, t1). 
In this case, t
¯ coincides with either t0 or t1, so either [t0, t
¯] or [t ,¯ t1] degenerate to 
a single point. Invoking the hypothesis of the lemma just once yields an F trajectory 
x on [t0, t1] such that x(t0) = ˆx(t0), x(t) ∈ int A for t ∈ (t0, t1] and satisfying 
|| ˆx − x||L∞(t0,t1) ≤ Kρ ≤ max{K1Kρ , (1 + K1 + K1K)Kρ} = K'
ρ .
In either case, we have exhibited a new K, written K'
, with the desired properties. 
Step 2: Suppose that the assertions of the lemma are valid (with constant K) 
when conditions (H1), (H2), (H3), (IPC) and (BVL) are satisfied together with 
the additional hypotheses (H2)'
, (H3)' and (H4), and when it is assumed that xˆ
on [t0, t1] satisfies conditions (i) and (ii). We show that they remain valid (with a 
modified K) even if condition (ii) is no longer required to be satisfied. 
By assumption, the assertions are valid (with constant K) if ρ ≤ ¯ρ. Suppose 
that ρ > ρ¯. By the weak invariance theorem for time-varying systems (Theorem 
13.2.2), there exists some admissible F trajectory x on [t0, t1], with x(t0) = ˆx(t0). 
Now apply the special case of the theorem we assume to be valid, treating x as the 
reference trajectory, to justify replacing x by an F trajectory (we do not re-label) 
that is strictly admissible on (t0, t1]. Then by (H2)'
||x − ˆx||L∞ ≤ 2c0(T − S) ≤ 2ρ¯
−1c0(T − S) × ρ .
So the assertions of the theorem are valid, in absence of the condition (ii), with the 
larger constant K6.6 Estimates on Trajectories Confined to a Closed Subset 281
max{K, 2ρ¯
−1c0(T − S)} .
Step 3: Suppose that the assertions of the lemma are valid (with constant K) 
when conditions (H1), (H2), (H3), (IPC) and (BVL) are satisfied together with the 
additional hypotheses (H2)'
, (H3)' and (H4), and when it is assumed that xˆ on 
[t0, t1] satisfies condition (i). We show that they remain valid (with a modified K) 
even if condition (i) is violated. 
Observe that the constant c0 will be the same upper bound for the velocities of all the 
F trajectories considered in this step. Choose N0 to be the smallest integer such that 
N−1 
0 (T −S) ≤ δ. Let N be the smallest integer such that N−1(t1 −t0) ≤ δ. Observe 
that N ≤ N0 Write x0 = ˆx. Partition [t0, t1] as a family of N contiguous intervals 
{[t
i
0, ti
1]}N
i=1 with t1 
0 = t0 and t
N
1 = t1, each of length at most δ (< 1/c0). Now 
apply the special case of the theorem (in which condition (i) is assumed to hold) 
with xˆ|
[t1 
0 ,t1 
1 ] as reference trajectory, to yield an F trajectory x1 on [t1 
0 , t1 
1 ] such that 
x1 is strictly admissible on (t1 
0 , t1 
1 ], x1(t1 
0 ) = ˆx(t1 
0 ) and 
||x1 − x0||L∞(t1
0 ,t1
1 ) ≤ Kρ .
Invoking Filippov’s existence theorem (Theorem 6.2.3), we can extend x1 as an F
trajectory to [t1 
0 , tN
1 ] (we do not re-label) such that 
||x1 − x0||L∞(t1
0 ,tN
1 ) ≤ K1Kρ = K1K

(max{dA(x0(t)) : t ∈ [t
1
0 , tN
1 ]}) ∨ ρ

,
in which K1 := exp{
 T
S kF (t) dt}. Now apply the special case of the theorem 
(in which condition (i) is satisfied), taking as reference trajectory x1 restricted to 
[t2 
0 , t2 
1 ], to yield an F trajectory x2 on [t1 
0 , t2 
1 ] that is strictly admissible on (t1 
0 , t2 
1 ], 
which we extend to [t1 
0 , tN
1 ], and so on. We thereby generate a sequence of F
trajectories xi on [t
1 
0 , tN
1 ], i = 1,...,N, such that for each i = 1,...,N, xi is 
strictly admissible on (t1 
0 , ti
1] and 
||xi − xi−1||L∞(t1
0 ,tN
1 ) ≤ K1K

(max{dA(xi−1(t)) : t ∈ [t
1
0 , tN
1 ]}) ∨ ρ

.
We also have 
(max{dA(xi(t)) : t ∈ [t
1
0 , tN
1 ]}) ∨ ρ ≤

(max{dA(xi−1(t)) : t ∈ [t
1
0 , tN
1 ]}) ∨ ρ

+ ||xi − xi−1||L∞(t1
0 ,tN
1 ) .
Write x = xN . Then x is strictly admissible on (t1 
0 , tN
0 ] = (t0, t1]. Now, for i =
1,...,N, define ai := ||xi − xi−1||L∞(t1 
0 ,tN
1 ) and bi := (max{dA(xi(t)) : t ∈
[t1 
0 , tN
1 ]}) ∨ ρ. Then, from the preceding relations, we have, for i = 2,...,N
ai ≤ K1Kbi−1, bi ≤ bi−1 + ai.282 6 Differential Inclusions
Eliminating ai from these inequalities gives 
bi ≤ (1 + K1K)bi−1.
Hence 
bi ≤ (1 + K1K)i
b0
and 
ai ≤ K1K(1 + K1K)i−1b0 ≤ K1K(1 + K1K)i−1ρ.
But 
||x − ˆx||L∞(t0,t1) = ||xN − x0||L∞(t0,t1) ≤ 
N
i=1
||xi − xi−1||L∞(t0,t1)
≤


N
i=1
(1 + K1K)i−1

K1Kρ
=

(1 + K1K)N − 1

ρ
≤

(1 + K1K)N0 − 1

ρ .
It follows that 
||x − ˆx||L∞(t0,t1) ≤ Kρ , ¯
in which K¯ = (1 + K1K)N0 − 1. The assertions are therefore valid even if (i) is not 
satisfied, when we replace K by K¯ . 
Step 4: Assume that the assertions of the lemma are valid (with constant K) 
when conditions (H1), (H2), (H3), (IPC) and (BVL) are satisfied together with the 
additional hypotheses (H2)'
, (H3)' and (H4). We show that they remain valid even if 
(H4) is violated, i.e. F is not convex valued. 
Assume that the above hypotheses are satisfied, with the exception of (H4). Replace 
F by co F. Then the above hypotheses, including (H4), are satisfied. The special 
case of the theorem yields a constant K (independent of the choice of reference 
trajectory xˆ on [t0, t1]) and a co F trajectory x' : [t0, t1] → Rn, which is strictly 
admissible on (t0, t1], such that 
||x' − ˆx||L∞(t0,t1) ≤ Kρ.6.6 Estimates on Trajectories Confined to a Closed Subset 283
Choose a decreasing sequence {si} in (t0, t1], with s1 = t1, such that si ↓ t0. Since 
x' is strictly admissible on (t0, t1] we can find a sequence of positive numbers ϵi ∈
(0,ρ) such that ϵi ↓ 0 and, for i = 1, 2,...
x'
(σ ) + ϵiB ⊂ int A for all σ ∈ [si, t1] . (6.6.14) 
Take a sequence of positive numbers {αi}. (We shall place restrictions on the αi’s 
presently.) By the relaxation theorem Theorem 6.5.2 (which asserts the density, with 
respect to the L∞ norm, of the set of F trajectories with a fixed initial state in the 
set of co F trajectories, with the same initial state), there exists a sequence of F
trajectories xi : [si, t1] → Rn such that, for all integer i ≥ 2, we have xi(si) =
x'
(si) and 
||xi − x'
||L∞(si,t1) ≤ αi . (6.6.15) 
For each integer j ≥ 2, we construct an F trajectory yj : [sj , t1] → Rn as follows: 
yj restricted to [sj , sj−1] coincides with xj . yj restricted to [sj−1, sj−2] is an F
trajectory with initial state yj (sj−1), obtained by applying the Filippov existence 
theorem (Theorem 6.2.3) with reference trajectory xj−1, and so on, until yj has 
been constructed on the whole interval [sj , s1 = t1]. 
Now fix an integer j > 2. We deduce from Theorem 6.2.3 that, for each 2 ≤ i < j , 
||yj − xi||L∞(si,si−1) ≤ K1|yj (si) − x'
(si)|
|| ˙yj − ˙xi||L1(si,si−1) ≤ K1|yj (si) − x'
(si)| .
(Here K1 = exp{
 T
S kF (t)dt}. We have also used the fact that xi(si) = x'
(si)). 
From these relations and (6.6.15) it follows that for each 2 ≤ i < j and any integer 
m, we have 
||yj − x'
||L∞(si,si−1) ≤ 
j
k=i
Kk−i
1 αk (6.6.16) 
|| ˙yj+m − ˙yj ||L1(si,si−1) ≤ 2 
j

+m
k=i+1 
Kk−i
1 αk . (6.6.17) 
Notice that for each j ≥ 2, yj (sj ) = x'
(sj ). So we can extend each F trajectory yj
as a co F trajectory to all of [t0, t1], setting yj (σ ) = x'
(σ ) for σ ∈ [t0, sj ]. (We do 
not re-label.) 
Now choose the sequence {αk} to satisfy 
∞
k=i
Kk
1αk < ϵi/2, for all i ≥ 2 . (6.6.18) 
This condition is satisfied, in particular, if we assume that ϵi < 1/3, for all i ≥ 2, 
and we chose αk = (ϵk/K1)k.284 6 Differential Inclusions
Since the yi’s have initial value x(t ˆ 0) and in view of hypotheses (H1)-(H2), we 
can extract a subsequence (we do not re-label) converging uniformly to a co F
trajectory x on [t0, t1], with initial value x(t ˆ 0). We conclude from (6.6.14), (6.6.16) 
and (6.6.18) that x is strictly admissible on (t0, t1]. To see this, take any σ ∈ (t0, t1]
and note that σ ∈ (si, si−1] for some i ≥ 2. But then from (6.6.16) and (6.6.18) we 
have 
yj (σ ) ∈ x'
(σ ) +
ϵi
2
B ⊂ int A, for all j>i.
Since the yj ’s converge uniformly to x, 
x(σ ) ∈ x'
(σ ) +
ϵi
2
B ⊂ int A .
On the other hand, for each k ≥ 2 and for every i ≥ k, the yi’s, restricted to 
[sk, t1], are F trajectories, which, owing to (6.6.17), define a Cauchy sequence in 
W1,1(sk, sk−1). It may be deduced that the limiting co F trajectory x is actually an 
F trajectory. Finally we note that, since each ϵi ≤ ρ, 
|| ˆx − x||L∞(t0,t1) ≤ || ˆx − x'
||L∞(t0,t1) + ||x − x'
||L∞(t0,t1) ≤ Kρ , ¯
where K¯ = K + 1. This is the desired distance estimate, with the modified constant 
K¯ . 
Step 5: Suppose that the assertions of the lemma are valid (with constant K) 
when conditions (H1), (H2), (H3), (IPC) and (BVL) are satisfied together with 
the additional hypotheses (H2)'
, (H3)'
. We show that they remain valid (with the 
same K) under (H1), (H2), (H3) alone, and when (IPC) and (BVL) are satisfied (for 
R0 := exp{
 T
S c(t) dt}(r0 + 1)). 
Assume that (H1), (H2), (H3), and that (IPC) and (BVL) are satisfied with R0 =
e
 T
S c(t) dt(r0 + 1). We observe that, given any interval [t0, t1]⊂[S, T ], for any 
F trajectory x on [t0, t1] with x(t0) ∈ A ∩ 
e
 t
0 
S c(t) dt(r0 + 1) − 1

B, we have 
x(t) ∈ R0B for all t ∈ [t0, t1]. As a consequence, if (H2)' or (H3)' are violated 
we can redefine F (t, x) for |x| > R0 (employing for instance the multifunction 
FR0 (t, x) := F (t, trR0 (x)), where trR0 is the truncation function defined in (6.2.4)) 
so that (H2)' and (H3)' are satisfied together with condition (H1). ⨅⨆
End of the Proof of Theorem 6.6.1 Fix r0 > 0. In view of the preliminary analysis 
we may assume that the multifunction F and set A in the theorem statement 
satisfy conditions (H1), (H2), (H3), (IPC) and (BVL) together with the additional 
hypotheses (H2)'
, (H3)' and (H4) with constant c0 > 0 and function kF ∈ L1(S, T ). 
(The constant c0 bounds the velocities of F trajectories x on subintervals of [S, T ]
originating in 
e
 t
0 
S c(t) dt(r0+1)−1

B.) Let η be the modulus of variation appearing 
in (BVL) (and also in Lemma 6.6.3), and let ϵ > 0 and η >¯ 0 be the constants from 
Lemma 6.6.3.6.6 Estimates on Trajectories Confined to a Closed Subset 285
We know (see Lemma 6.6.3) that, given any (t, x) ∈ [S, T ] × 
(∂A + ¯ηB) ∩
R0B ∩ A

, v ∈ co F (t, x) can be found such that 
x' + [0, ϵ](v + ϵB) ⊂ A, for all x' ∈ (x + ϵB) ∩ A. (6.6.19) 
By hypothesis (BVL) 
F (s, x) ⊂ F (t, x) + (η(t) − η(s))B (6.6.20) 
for all points x ∈ (∂A + ¯ηB) ∩ R0B and subintervals [s, t]⊂[S, T ]. 
Let ω : [0, T − S]→[0,∞) be the function 
ω(α) := sup{

I
kF (s)ds}
where the supremum is taken over sub-intervals I ⊂ [S, T ] of length not greater 
than α. Since kF ∈ L1(S, T ), by properties of integrable functions, ω is well-defined 
on [0, T − S], and ω(α) → 0, as α ↓ 0. 
Fix k > 0 such that k>ϵ−1 and take constants δ > 0, ρ >¯ 0 and γ > 0 in such 
a manner that 
δ ≤ ϵ, ρ¯ + c0δ < ϵ, kρ < δ, ¯ ρ¯ ≤ ¯η, 4δc0 ≤ ¯η, (6.6.21) 
and 
eω(δ)(γ + ω(δ)c0)(T − S) < ϵ, 2eω(δ)(γ + ω(δ)c0) k < (kϵ − 1) .(6.6.22) 
The assertions of the theorem will be confirmed provided that we establish the 
existence a constant K > 0 such that, for any interval [t0, t1]⊂[S, T ], given any F
trajectory xˆ on [t0, t1] with x(t ˆ 0) ∈ A ∩ 
e
 t
0 
S c(t) dt(r0 + 1) − 1

B, and any ρ > 0 
satisfying ρ ≥ max{dA(x(t)) ˆ : t ∈ [t0, t1]}, we can find an admissible F trajectory 
x on [t0, t1] with x(t0) = ˆx(t0) and such that 
|| ˆx − x||L∞(t0,t1) ≤ K ρ
and 
x(t) ∈ int A for t ∈ (t0, t1] .
Owing to the reduction Lemma 6.6.5, we can restrict attention, without loss of 
generality, to the case in which F is convex valued, 
(i): t1 − t0 ≤ δ, 
(ii): ρ ≤ ¯ρ and 
(iii): η(t1) − η(t0) ≤ γ and η(t0) = 0.286 6 Differential Inclusions
Observe that, if x(t ˆ 0) ∈ 
A ∩ 
e
 t
0 
S c(t) dt(r0 + 1) − 1

B

\ (∂A + η¯
2 B), then 
the fifth condition in (6.6.21) implies that x = ˆx is an admissible F trajectory 
having the required properties. Therefore, it is not restrictive to assume also that 
x(t ˆ 0) ∈ (∂A + η¯
2 B) ∩ A ∩ 
e
 t
0 
S c(t) dt(r0 + 1) − 1

B. An immediate consequence 
(from the definition of R0) is that 
x(t ˆ 0) ∈ (∂A + η¯
2
B) ∩ R0B ∩ A.
Then, recalling the fact that the multifunction F can be considered convex valued, 
there exists a vector v ∈ F (t0, x(t ˆ 0)) satisfying property (6.6.19) for (t, x) =
(t0, x(t ˆ 0)). Now, consider the arc y : [t0, t1] → Rn such that y(t0) = ˆx(t0) and 
y(t) ˙ =

v if t ∈ [t0, (t0 + kρ) ∧ t1]
˙
x(t ˆ − kρ) if t ∈ (t0 + kρ , t1] and if ˙
x(t ˆ − kρ) exists .
If t0 + kρ < t1, then it immediately follows that, for all t ≥ t0 + kρ, 
y(t) = ˆx(t − kρ) + kρv . (6.6.23) 
Recalling that c0 constitutes an upper bound for the magnitude for both v and 
||˙
xˆ||L∞, we can deduce that 
|| ˆx − y||L∞(t0,t1) ≤ 2c0kρ . (6.6.24) 
In addition, from (6.6.20) we can also deduce that, for all s ∈ [t0, (t0 + kρ) ∧ t1], 
dF (s,y(s))(y(s)) ˙ ≤ dF (s,y(t0))(v) + kF (s)|y(s) − y(t0)|
≤ (η(s) − η(t0)) + kF (s)|y(s) − y(t0)|
≤ γ + kF (s)c0 (s − t0) . (6.6.25) 
Invoking Filippov’s existence theorem (Theorem 6.2.3) and taking into account 
condition (6.6.25), we can find an F trajectory x on [t0, (t0 + kρ) ∧ t1] with 
x(t0) = y(t0) and such that, for any t ∈ [t0, (t0 + kρ) ∧ t1]
||x − y||L∞(t0,t) ≤ eω(δ)(γ + ω(δ)c0) (t − t0) . (6.6.26) 
Consider now the case in which t0 + kρ < t1. Conditions (6.6.20), (6.6.21) 
and (6.6.23) imply that, for a.e. s ∈ [t0 + kρ , t1], 
dF (s,y(s))(y(s)) ˙ = dF (s,kρv+ ˆx(s−kρ))(˙
x(s ˆ − kρ))
≤ kF (s)c0kρ + dF (s,x(s ˆ −kρ))(˙
x(s ˆ − kρ))
≤ kF (s)c0kρ + (η(s) − η(s − kρ)) + dF (s−kρ ,x(s ˆ −kρ))(˙
x(s ˆ − kρ))
= kF (s)c0kρ+(η(s) − η(s − kρ)) + 0 .6.6 Estimates on Trajectories Confined to a Closed Subset 287
Then it follows that, for any t ∈ [t0 + kρ , t1], 
 t
t0+kρ
dF (s,y(s))(y(s)) ds ˙ ≤ (ω(δ)c0 + η(t1))kρ ≤ (ω(δ)c0 + γ )kρ .
Thus, the F trajectory x can be extended from [t0, (t0 + kρ) ∧ t1] to [t0, t1] by 
applying again Filippov’s theorem, in such a manner that 
||x − y||L∞(t0,t1) ≤ 2eω(δ)(γ + ω(δ)c0)kρ . (6.6.27) 
From (6.6.24) and (6.6.27) we deduce the required estimate: 
|| ˆx − x||L∞(t0,t1) ≤ Kρ
in which 
K := 2

c0 + eω(δ)(γ + ω(δ)c0)

k .
To complete the proof we must show that 
x(t) ∈ int A for t ∈ (t0, t1] .
There are two cases to consider (we are assuming that t0 + kρ < t1, otherwise only 
the first case occurs): 
Case 1: t ∈ (t0, t0 + kρ]. Since y(t) = ˆx(t0) + (t − t0)v and t − t0 ≤ ϵ, it follows 
from (6.6.19) that 
y(t) + (t − t0)ϵB = ˆx(t0) + (t − t0)(v + ϵB) ⊂ A .
Then, conditions (6.6.26) and (6.6.22) immediately yield x(t) ∈ int A, for all t ∈
(t0, t0 + kρ]. 
Case 2: t ∈ (t0 + kρ , t1]. Write z(t) a projection on A of the arc t → ˆx(t − kρ). It 
means that, for each t ∈ (t0 + kρ , t1], we select z(t) ∈ A such that 
| ˆx(t − kρ) − z(t)| = dA(x(t ˆ − kρ)) (≤ ρ) .
As a consequence, invoking (6.6.23), we obtain 
y(t) ∈ z(t) + kρv + ρB , (6.6.28) 
and, since | ˆx(t −kρ)− ˆx(t0)| ≤ c0(t1 −t0) for all t ∈ (t0 +kρ , t1], appealing once 
again to (6.6.21), we also have 
|z(t) − ˆx(t0)|≤ ¯ρ + c0δ < ϵ.288 6 Differential Inclusions
Thus bearing in mind (6.6.19) and (6.6.21), we see that 
z(t) + kρv + kρϵB ⊂ A ,
and, owing to (6.6.28), 
y(t) + (kϵ − 1)ρB ⊂ A .
Taking into account (6.6.22) and (6.6.27), we deduce that x(t) ∈ int A in this case 
as well, confirming all the assertions of the theorem. ⨅⨆
If the data of the state-constrained differential inclusion (6.6.1) satisfy condition 
(IPC), then, in consequence of ‘stability’ properties of the interior of Clarke tangent 
cone, Lemma 6.6.3 guarantees the existence of vectors in co F (t, x) pointing 
uniformly inward A, whenever x is close to the boundary of A. The crucial step in 
the proof of Theorem 6.6.2 is to show that, if we impose just condition (IPC)'
, which 
is weaker than condition (IPC), still we can obtain implications similar to those ones 
stated in Lemma 6.6.3. The next lemma ensures that the required properties are valid 
if, in addition, we are allowed to make use of condition (BV)A. 
Lemma 6.6.6 Suppose that the multifunction F : [S, T ]×Rn ⇝ Rn and the closed 
set A satisfy hypotheses (H1), (H2), (H3), (IPC)' and (BV)A for some R0 > 0. Then 
there exist ϵ > 0, η¯ ∈ (0, η0) and a finite time set {τj }j∈J ⊂ [S, T ] with the 
following property: for any (t, x) ∈ [S, T ] × 
(∂A + ¯ηB) ∩ R0B ∩ A

, we can find 
v ∈
⎧
⎨
⎩
co F (t, x) if t /∈ {τj }j∈J

lim supτ↓t co F (τ, x) if t ∈ {τj }j∈J ,
such that 
y + [0, ϵ](v + ϵB) ⊂ A, for all y ∈ (x + ϵB) ∩ A. (6.6.29) 
Proof Fix any (t, x) ∈ [S, T ] × (R0B ∩ ∂A). From (IPC)' we can find vectors 
v1 ∈

lim inf
x' A
→x
lim sup
t'
↓t
co F (t'
, x'
)

∩ int T¯
A(x) ,
and 
v2 ∈

lim inf
x' A
→x
lim sup
t'
↑t
co F (t'
, x'
)

∩ int T¯
A(x) .
In view of the characterization of the interior of the Clarke tangent cone (see 
Theorem 4.11.1), there exist γ ∈ (0, 1/2) and r ∈ (0, η0/4) such that6.6 Estimates on Trajectories Confined to a Closed Subset 289
y + [0, γ ](v1 + 2γB) ⊂ A , for all y ∈ (x + 2rB) ∩ A (6.6.30) 
and 
y + [0, γ ](v2 + 2γB) ⊂ A , for all y ∈ (x + 2rB) ∩ A . (6.6.31) 
From the definition of the lim inf operator (in the sense of Kuratowski), we can 
deduce the existence of r¯ ∈ (0,r) such that 
sup x'
∈(x+ ¯rB)∩A
lim inf
t'
↓t dco F (t'
,x'
)(v1) < γ
2 (6.6.32) 
and 
sup x'
∈(x+ ¯rB)∩A
lim inf
t'
↑t dco F (t'
,x'
)(v2) < γ
2 . (6.6.33) 
Taking into account assumption (BV)A and the same argument employed in the 
proof of Lemma 6.6.3, we denote η the modulus of variation of F, which can be 
decomposed into a sum of a continuous function ηc and a singular (discontinuous) 
component. Write {σi} and {ai} respectively the sequence of the countable distinct 
jump times and the sequence of the (countable) non-negative jumps of η in [S, T ]. 
We can find numbers δ > 0 and α >¯ 0 such that 
ηc
(t'
) − ηc
(s'
) < γ/2 (6.6.34) 
for each subinterval [s'
, t'
]⊂[S, T ] with t' − s' ≤ δ, and 

i /∈J
ai < γ/2 , (6.6.35) 
where J ⊂ {1, 2,...} is a finite index set for which 
|σi − σj | > α¯ for i, j ∈ J, i /= j .
Take δt ∈ (0, δ) such that ([t − δt, t + δt]\{t}) ∩ {σi : i ∈ J }=∅. Define F
1 :
[t −δt, t]×Rn ⇝ Rn according to (6.6.7) and η˜1 according to (6.6.9) when [a, b] =
[t − δt, t]. Similarly, define F˜
2 : [t, t + δt] × Rn ⇝ Rn according to (6.6.7) and η˜2 
according to (6.6.9) when [a, b]=[t, t +δt]. F˜
1 and F˜
2 satisfy the hypotheses (H1), 
(H2)'
, (H3)'
, (BV)A of F, restricted respectively to [t − δt, t] × Rn and [t, t + δt] ×
Rn, with the same constants c0 > 0 and η0 > 0, and function kF . Moreover, the 
assertions of Lemma 6.6.4 hold true when F˜
1 and F˜
2 replace F˜. Then, from (6.6.10), 
for all s' ∈ [t − δt, t], we have 
sup x∈(∂A+η0B)∩R0B
dH (F˜
1(s'
, x), F˜
1(t, x)) ≤ ˜η1(t) − ˜η1(s'
) (6.6.36)290 6 Differential Inclusions
and for all t' ∈ [t, t + δt], we have 
sup x∈(∂A+η0B)∩R0B
dH (F˜
2(t, x), F˜
2(t'
, x)) ≤ ˜η2(t'
) − ˜η2(t) . (6.6.37) 
Also conditions (6.6.32) and (6.6.33) can be rephrased in terms of F˜
1 and F˜
2: for all 
x' ∈ (x + ¯rB) ∩ A
(v1 + γ
2
B) ∩ co F˜
1(t, x'
) /= ∅ and (v2 + γ
2
B) ∩ co F˜
2(t, x'
) /= ∅ .
Thus, for all x' ∈ (x + ¯rB) ∩ A and for all s' ∈ [t − δt,t), there exists v' ∈
co F˜
1(s'
, x'
) such that 
|v1 − v'
| ≤ γ
2 + ˜η1(t) − ˜η1(s'
) ≤
3
2
γ ,
where, taking account of (6.6.34) and (6.6.35), we have used the following fact: 
η˜1(t) − ˜η1(s'
) ≤ η(t−) − η(s'+)<γ.
Similarly, for all x' ∈ (x + ¯rB) ∩ A and for all t' ∈ (t, t + δt], there exists w' ∈
co F˜
2(t'
, x'
) such that 
|v2 − w'
| ≤ γ
2 + ˜η2(t) − ˜η2(t'
) ≤
3
2
γ .
(Observe that in the relations above, F˜
1(s'
, x'
) = F (s'
, x'
) for all s' ∈ (t − δt,t), 
and F˜
2(t'
, x'
) = F (t'
, x'
) for all t' ∈ (t, t + δt).) 
Therefore, for all t' ∈ (t − δt, t + δt), t' /= t, and for all x' ∈ (x + ¯rB) ∩ A, we can 
find v' ∈ co F (t'
, x'
) such that 
y + [0, γ ](v' + γ
2
B) ⊂ A , for all y ∈ (x' + ¯rB) ∩ A .
And, if t' = t, for all x' ∈ (x + ¯rB) ∩ A, there exists v˜ ∈ co F˜
2(t, x'
) satisfying 
y + [0, γ ](v˜ + γ
2
B) ⊂ A , for all y ∈ (x' + ¯rB) ∩ A .
Set εt,x := min{δt,r,¯ γ /2}. Then we have proved the following property: for any 
(t, x) ∈ [S, T ] × (R0B ∩ ∂A) there exists εt,x > 0 such that for all (t'
, x'
) ∈
[S, T ] × A with |x − x'
| < εt,x and |t − t'
| < εt,x , we can find 
v' ∈
⎧
⎨
⎩
co F (t'
, x'
) if t' /= t
co F˜
2(t'
, x'
) if t' = t ,6.6 Estimates on Trajectories Confined to a Closed Subset 291
satisfying 
y + [0, εt,x ](v' + εt,xB) ⊂ A , for all y ∈ (x' + εt,xB) ∩ A .
By a standard compactness argument, we can find a finite number of points (ti, xi) ∈
[S, T ] × 
R0B ∩ ∂A
and numbers ϵi > 0, for i = 1,...,m, such that 

i=1,...,m

(ti, xi) + ϵi
◦
B

⊃ [S,T ] × 
R0B ∩ ∂A (6.6.38) 
and for each (t'
, x'
) ∈ ((ti, xi) + ϵiB) ∩ ([S, T ] × A), there exists a vector 
v' ∈
⎧
⎨
⎩
co F (t'
, x'
) if t' /= t
co F˜
2(t'
, x'
) if t' = t ,
such that 
y' + [0, ϵi](v' + ϵiB) ⊂ A, for all y' ∈ (x' + ϵiB) ∩ A.
Notice also that there exists η¯ ∈ (0, mini=1,...,m ϵi) (observe that η <¯ η0) such that 

i=1,...,m

(ti, xi) + ϵi
◦
B

⊃ [S,T ] × 
(∂A + ¯ηB) ∩ R0B

,
otherwise we could find a sequence of points (sj , yj ) ∈ ([S, T ] × R0B) \

i=1,...,m 
(ti, xi) + ϵi
◦
B

such that (sj , yj ) → (s, y) ∈ [S, T ] × 
R0B ∩ ∂A
, 
which would contradict (6.6.38). 
To conclude we just take ϵ := mini=1,...,m ϵi and the assertions of the lemma 
immediately follow. ⨅⨆
Proof of Theorem 6.6.2 In view of Lemma 6.6.6, we can define the following 
multifunction 
F (t, x) :=
⎧
⎨
⎩
F (t, x) if t /∈ {τj }j∈J
lim supτ↓t F (τ, x) if t ∈ {τj }j∈J .
Then, the approach used in the proof of Theorem 6.6.1 is applicable to the state￾constrained differential inclusion (6.6.1) when F
 replaces F. Since an F
 trajectory 
is also an F trajectory and vice-versa, we obtain the validity of the required 
properties. ⨅⨆292 6 Differential Inclusions
6.7 Exercises 
6.1 (Intermediate Minimizers) Take 1 ≤ s ≤ ∞. An admissible F trajectory 
x¯ for a dynamic optimization problem (with cost J , underlying time domain [S, T ]
and differential inclusion constraint x˙ ∈ F (t, x)) is said to be a W1,s local minimizer 
if there exists ϵ > 0 such that, for all admissible F trajectories such that ||x −
x¯||W1,s ≤ ϵ, we have J (x)¯ ≤ J (x). 
For r ∈ (1,∞) set 
φr(t) := t
1−1/r.
Consider 
(P )r
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize J (x) := x1(1)
subject to
(x˙1, x˙2, x˙3)(t) ∈ F (t, x(t)) := {(0, |x1(t)φr(t) + x3(t)|, e)
: e ∈ [−φ˙r(t), φ˙r(t)]}, a.e. t ∈ [0, 1] ,
(x(0), x(1)) ∈ (R × {0} × R)2 .
Notice that, for any r ∈ (1,∞), the data for (P )r satisfy the standard convexity, 
integrable boundedness and integrable Lipschitz continuity hypotheses invoked in 
Chap. 6 and subsequent chapters. 
Consider the admissible F trajectory x¯ ≡ 0. Take any r ∈ (1,∞) and s and s'
such that 
1 ≤ s<r ≤ s' ≤ ∞.
Show that x¯ is a W1,s'
local minimizer but not a W1,s local minimizer. 
6.2 (Infinite Horizon Problems) Consider the following dynamic optimization 
problem with discounted running cost over an infinite horizon: 
(P∞)
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize  ∞
0 e−λtL(x(t), u(t))dt
over x ∈ W1,1
loc([0,∞); Rn)
and measurable functions u : [0,∞) → Rm satisfying
y(t) ˙ = f (y(t), u(t)), a.e. t ∈ [0,∞),
u(t) ∈ U, a.e. t ∈ [0,∞),
y(0) = x0,
in which f : Rn × Rm → Rn and L : Rn × Rm → R are given functions, U ⊂ Rm
is a given set, λ > 0 and x0 ∈ Rn. Assume6.8 Notes for Chapter 6 293
(H1): λ > 0, f is continuous, U is compact and 
{(v, α) ∈ Rn × R : v ∈ f (x, u), α ≥ L(x, u)} is convex for each x ∈ Rn ,
(H2): there exist positive constants kf and cf such that 
|f (x, u) − f (y, u)| ≤ kf |x − y| and |f (x, u)| ≤ cf (1 + |x|)
for all x, y ∈ Rn and u ∈ Rm,
(H3): there exist positive constants kL and cL such that 
|L(x, u) − L(y, u)| ≤ kL|x − y| and |L(x, u)| ≤ cL
for all x, y ∈ Rn and u ∈ Rm.
Show that (P∞) has a minimizer (in W1,1 
loc). 
Remark 
The object of this exercise is to complete some of the details of the proof of 
Proposition 13.10.1 part (i). 
6.3 (Proximal Gronwall’s Inequality) Let f : [S, T ] → R be lower semi￾continuous function. Assume that there exists β ≥ 0 such that 
|ξ | ≤ βf (s) for all ξ ∈ ∂P f (s) and s ∈ [S,T ].
(Here, the definition of ∂P f (s) when s = S or T can be provided extending f to R
by setting values to +∞ outside [S, T ].) Show that 
(i): f is Lipschitz continuous on [S, T ] and, if β > 0, then f ≥ 0, 
(ii): f (t) ≤ f (S)eβ(t−S), for all t ∈ [S, T ], 
(iii): |f (t'
) − f (t)| ≤ f (t)
eβ|t'
−t| − 1

, for all t,t' ∈ [S, T ], 
(iv): there exits a constant α ≤ (eβ(T −S) + 1)
eβ(T −S) − 1 
T − S
such that 
|f (t2) − f (t1)| ≤ α f (t)|t2 − t1| ∀t,t1, t2 ∈ [S,T ].
6.8 Notes for Chapter 6 
There is a substantial literature on differential inclusions. Material in Sects. 6.2–6.4, 
which restricts attention largely to differential inclusions involving multifunctions 
which are Lipschitz continuous with respect to the state, only scratches the surface294 6 Differential Inclusions
of this extensive field. (See [11] and [93].) The important existence theorem 
Theorem 6.2.3 and accompanying estimates are essentially due to Filippov [102]. 
The proof given here is an adaptation of that in [11], to allow for measurable time 
dependence. The compactness of trajectories theorem Theorem 6.3.3 is a refinement 
of that in [65]. Implicit in this theorem are early ideas for establishing existence of 
optimal controls under a convexity hypothesis on the ‘velocity set’ associated with 
Tonelli, L. C. Young, Filippov, Gamkrelidze, Roxin and others. 
The concept of relaxation has its origins in the work of L. C. Young (see 
references in [209]), where it provided a framework for expanding the domain of 
problems in the calculus of variations to ensure ‘closure’ properties of families of 
trajectories, and thereby guarantee existence of minimizers. Relaxation for dynamic 
optimization associated with controlled differential equations was introduced by 
Warga [204] and by Gamkrelidze [118]. The related closure properties of families 
of solutions to a differential inclusion, which are the subject matter of Sect. 6.5 are 
a by-product of the theory of set valued integrals [15] and, specifically, relations 
between the integral of a set valued functions and the integral of its convex hull. 
More recently, the study of relaxation schemes has broadened out to apply to 
other classes of optimization problems, for example variational problems in several 
independent variables [7] and dynamic optimization problems with time delay 
[178]. 
Distance estimates for state constrained dynamic systems, of the kind covered 
in Sect. 6.6 originate in work of Soner [182], who proved an L∞ linear estimate 
for autonomous controlled differential equations and a state constraint with smooth 
boundary, under an ‘inward pointing’ condition. Subsequent research has provided 
extensions to allow for control systems described by time-varying differential 
inclusions and state constraint set take to be arbitrary closed sets. Theorems 6.6.1 
and 6.6.2 are a refinement of [30, Thms. 1 and 2] (and of [33]). There has been 
interest, too, in W1,1 distance estimates and applications. When we replace the L∞
norm by the stronger W1,1 norm, it is still possible to derive linear distance estimates 
under inward pointing conditions, but only if we impose additional hypotheses, 
concerning the time dependence of the differential inclusion F (t, x), the regularity 
of the boundary of the state constraint set or the precise nature of the inward pointing 
condition (cf. [32, 109]). It is possible, however, to derive super-linear W1,1 distance 
estimates under weaker hypotheses (see [47] and [23]). 
Distance estimates have been widely employed in the analysis of state con￾strained dynamic optimization problems: to ensure the validity of necessary opti￾mality conditions in normal or nondegenerate form (cf. [27, 36, 170]), to establish 
regularity properties of the value function as well as to justify interpreting the value 
function as a unique (generalized) solution of the Hamilton-Jacobi equation (cf. 
[23, 28, 108, 110, 111]), to derive sensitivity relations (cf. [35]), and as a key element 
in other regularity investigations (see, e.g., [86]). Some constructions (employed in 
the derivation of distance estimates) can be refined in such a manner they are also 
‘nonanticipative’, a property that is crucial in the differential games context (cf. 
[37]).Chapter 7 
The Maximum Principle 
Abstract The classical maximum principle was introduced in Chap. 1. This fun￾damental theorem is a set of necessary conditions of optimality for dynamic 
optimization problems with endpoint cost and endpoint constraints, in which the 
dynamic constraint takes the form of a controlled differential equation. It unifies 
earlier optimality conditions in the calculus of variations and extends them to take 
account of the dynamic constraint. 
This chapter provides a generalization of these conditions, known as the Clarke 
nonsmooth maximum principle, that covers problems in which the endpoint con￾straint and cost are expressed in terms of nonsmooth functions and general closed 
sets, and the right side of the controlled differential equation is nonsmooth w.r.t. the 
state variable. Special cases of interest and simple extensions are discussed in detail. 
The derivation of Clarke’s nonsmooth principle appearing in this chapter is based 
on his perturbation technique. The necessary conditions in their full generality 
are arrived at in stages, starting with necessary conditions for a simple problem 
with a smooth endpoint cost function and no endpoint constraints. Subsequent 
stages introduce refinements (additional constraints, nonsmoothness, removal of 
temporary simplifying hypotheses). The key idea is that, at each stage, when we 
seek necessary conditions for a newly refined version of the problem, we construct 
a minimizer to a perturbed problem, which is simpler and for which necessary 
conditions are available from the previous stage. Necessary conditions for the 
new problem are then obtained in the limit, from the necessary conditions for the 
perturbed problems. Construction of the perturbed problems is accomplished by 
various techniques, which include application of Ekeland’s theorem and the use of 
quadratic inf convolutions. Limit taking of the necessary conditions is carried out 
with the help of subdifferential calculus. 
7.1 Introduction 
In this chapter we derive a set of optimality conditions for dynamic optimization 
problems, known as the maximum principle. Many competing sets of optimality 
conditions are now available, but the maximum principle retains a special signif-
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
P. Bettiol, R. B. Vinter, Principles of Dynamic Optimization, Springer Monographs 
in Mathematics, https://doi.org/10.1007/978-3-031-50089-3_7
295296 7 The Maximum Principle
icance. An early version of the maximum principle due to Pontryagin et al. was 
after all the breakthrough marking the emergence of dynamic optimization as a 
distinct field of research. Also, whatever additional information about minimizers 
is provided by dynamic programming, higher order conditions and the analysis 
of the geometry of state trajectories, first order necessary conditions akin to 
the maximum principle remain the principal vehicles for the solution of specific 
dynamic optimization problems (either directly or indirectly via the computational 
procedures they inspire), or at least for generating ‘suspects’ for their solution. 
A number of first order necessary conditions will be derived in this book. We 
attend to the maximum principle at the outset, because it was the first set of neces￾sary conditions to handle in a satisfactory way constraints typically encountered in 
practical dynamic optimization problems. 
The dynamic optimization problem studied here is 
(P )
⎧
⎪⎪⎨
⎪⎪⎩
Minimize g(x(S), x(T ))
over x ∈ W1,1([S,T ]; Rn) and measurable u satisfying
x(t) ˙ = f (t, x(t), u(t)) and u(t) ∈ U (t) a.e. t ∈ [S,T ],
(x(S), x(T )) ∈ C,
the data for which comprise an interval [S,T ], functions g : Rn × Rn → R, f :
[S,T ]×Rn×Rm → Rn, a non-empty multifunction U : [S,T ] ⇝ Rm and a closed 
set C ⊂ Rn × Rn. 
A measurable function u : [S,T ] → Rm which satisfies 
u(t) ∈ U (t) a.e. t ∈ [S,T ]
is called a control function. The set of all control functions is written U. 
A process (x, u) comprises a function x ∈ W1,1([S,T ]; Rn) and a control 
function u such that x is a solution to the differential equation 
x(t) ˙ = f (t, x(t), u(t)) a.e. t ∈ [S,T ].
A state trajectory x is the first component of some process (x, u). A process (x, u) is 
said to be admissible for (P) if the state trajectory x satisfies the end-point constraint 
(x(S), x(T )) ∈ C.
The maximum principle and related necessary conditions of optimality are 
satisfied by all minimizers. But, as we shall see, these conditions are satisfied also 
by processes which are merely ‘local’ minimizers for (P). Different choices of 
topology on the set of processes give rise to different notions of local minimizer. 
The one adopted in this chapter is that of W1,1 local minimizer, though we make 
reference in the proofs also to L∞ local minimizers.7.2 Clarke’s Nonsmooth Maximum Principle 297
A admissible process (x,¯ u)¯ is 
(a): a W1,1 local minimizer if there exists ϵ > 0 such that 
g(x(S)), x(T )) ≥ g(x(S), ¯ x(T )), ¯
for all admissible processes (x, u) which satisfy ‖x − ¯x‖W1,1 ≤ ϵ.
(b): an L∞ local minimizer if there exists ϵ > 0 such that 
g(x(S)), x(T )) ≥ g(x(S), ¯ x(T )), ¯
for all admissible processes (x, u) which satisfy ‖x − ¯x‖L∞ ≤ ϵ.
The point in showing that the optimality conditions are valid for local minimizers 
is to focus attention on the limitations of these conditions: they may lead us to a 
local minimizer in place of a global minimizer of primary interest. The W1,1 norm 
is stronger than the L∞ norm and therefore the class of W1,1 local minimizers is 
larger than the class of L∞ local minimizers. It follows that, by choosing to work 
with W1,1 local minimizers, we are carrying out a more precise analysis of the 
local nature of the maximum principle than would be the case if we chose to derive 
conditions satisfied by L∞ local minimizers. 
7.2 Clarke’s Nonsmooth Maximum Principle 
We denote by H : [S,T ] × Rn × Rn × Rm → R the un-maximized Hamiltonian 
function, namely 
H(t, x, p, u) := p · f (t, x, u), for (t, x, p, u) ∈ [S,T ] × Rn × Rn × Rm.
Theorem 7.2.1 (Clarke, 1976) Let (x,¯ u)¯ be a W1,1 local minimizer for (P). 
Assume that, for some ϵ >¯ 0, the following hypotheses are satisfied: 
(H1) for fixed x, f (., x, .) is L×Bm measurable; there exists an L×Bm measurable 
function k : [S,T ] × Rm → R+ such that t → k(t, u(t)) ¯ is integrable and, 
for a.e. t ∈ [S,T ], 
|f (t, x, u) − f (t, x'
, u)| ≤ k(t, u)|x − x'
|
for all x, x' ∈ ¯x(t) + ¯ϵB and u ∈ U (t), 
(H2) Gr U is an L × Bm measurable set, 
(H3) g is Lipschitz continuous on (x(S), ¯ x(T )) ¯ + ¯ϵB.298 7 The Maximum Principle
Then there exist p ∈ W1,1([S,T ]; Rn) and λ ≥ 0 such that 
(a): (non-triviality condition) 
(p, λ) /= (0, 0), 
(b): (co-state inclusion) 
− ˙p(t) ∈ co ∂xH(t, x(t), p(t), ¯ u(t)) ¯ a.e. t ∈ [S,T ], 
(c): (Weierstrass condition) 
H(t, x(t), p(t), ¯ u(t)) ¯ = max
u∈U (t)
H(t, x(t), p(t), u) ¯ a.e. t ∈ [S,T ], 
(d): (transversality condition) 
(p(S), −p(T )) ∈ λ∂g(x(S), ¯ x(T )) ¯ + NC(x(S), ¯ x(T )) ¯ . 
Now assume, also, that 
f (t, x, u) and U (t) are independent of t.
Then, in addition to the above conditions, there exists a constant r such that 
(e): H(t, x(t), p(t), ¯ u(t)) ¯ = r a.e. t ∈ [S,T ] . 
(∂xH denotes the limiting subdifferential of H(t, ·, p, u) for fixed (t, p, u).) 
The proof of the theorem is deferred to a later section. 
Elements (p, λ) whose existence is asserted in the maximum principle are called 
Lagrange multipliers for (P ). The components p and λ are referred to as the co-state 
trajectory and cost multiplier respectively. 
Remarks 
(a): The co-state inclusion (condition (ii) in the theorem statement) can be stated in 
terms of the Clarke generalized Jacobian introduced in Exercise 5.2, that we 
recall here: 
take a point y ∈ Rn and a function φ : Rn → Rm which is Lipschitz continuous 
on a neighbourhood of y. Then the Clarke generalized Jacobian J Cφ(y) of φ
at y is the set of m × n matrices: 
J Cφ(y) := co{M ∈ Rm×n : ∃ yi → y such that
∇φ(yi) exists ∀i and ∇φ(yi) → M}.
A noteworthy property of the generalized Jacobian J Cφ(y) of function φ :
Rn → Rm, which is Lipschitz continuous on a neighbourhood of a point y, is 
that, for any vector r ∈ Rm, 
r · J Cφ(y) = co ∂(r · φ)(y)
(see Exercise 5.2). Here, ∂(r · φ)(y) is the limiting subdifferential of the 
function y → r · φ(y). It follows immediately that the adjoint inclusion can be 
equivalently written7.2 Clarke’s Nonsmooth Maximum Principle 299
− ˙p(t) ∈ p(t) · J C
x f (t, x(t), ¯ u(t)), ¯
in which J C
x f (t, x, u) denotes the generalized Jacobian with respect to the x
variable. This alternative, but equivalent, statement of the costate inclusion is 
sometimes encountered in the dynamic optimization literature. 
(b): We observe finally that the necessary conditions of Theorem 7.2.1 are homoge￾neous with respect to the Lagrange multipliers (p, λ). This means that if (p, λ)
serves as a set of Lagrange multipliers then, for any α > 0, (αp, αλ) also 
serves. Since (p, λ) /= 0, we can always arrange by choosing an appropriate α
that 
||p||L∞ + λ = 1 .
Scaling Lagrange multipliers in this way is often carried out to assist conver￾gence analysis. 
The above maximum principle is easily adapted to cover problems with the more 
general formulation: 
(P I )
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize g(x(S), x(T )) +  T
S L(t, x(t), u(t))dt
over x ∈ W1,1([S,T ]; Rn) and
measurable functions u : [S,T ] → Rm satisfying
x(t) ˙ = f (t, x(t), u(t)) a.e.,
u(t) ∈ U (t) a.e.,
(x(S), x(T )) ∈ C,
in which an integral term +  T
S L(t, x(t), u(t))dt has been added to the cost. This 
is accomplished by introducing a new state variable z which satisfies the relations 

z(t) ˙ = L(t, x(t), u(t))
z(S) = 0.
The ‘mixed’ cost can then be replaced by the pure end-point cost 
z(T ) + g(x(S), x(T )).
In this way we arrive at problem to which Theorem 7.2.1 is applicable. (Of course 
we have to make suitable additional assumptions about the function L to ensure the 
hypotheses of Theorem 7.2.1 are satisfied.) Applying the theorem and interpreting 
the conclusions in terms of the data for the original problem gives the following 
maximum principle for problem (PI): 
Corollary 7.2.2 (The Maximum Principle for End-Point and Integral Cost) 
Let (x,¯ u)¯ be a W1,1 local minimizer for (P I ). Assume that (H1) and (H2) of 
Theorem 7.2.1 and, in place of (H3), the hypothesis300 7 The Maximum Principle
(H3): hypothesis (H3) of Theorem 7.2.1 is satisfied when the R1+n-valued func￾tion (L, f ) is inserted in place of f . 
Then the assertions of Theorem 7.2.1 are valid, when the function H is replaced by 
Hλ(t, x, p, u) := p · f (t, x, u) − λL(t, x, u).
The procedure employed above, for reducing a problem with integral and endpoint 
cost terms to one with an endpoint cost term alone, is widely used. It is called state 
augmentation. 
Examples of the following dynamic optimization problem, in which the end￾point constraints are specified as functional constraints, are often encountered. 
(F EC)
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize g(x(S), x(T ))
over x ∈ W1,1([S,T ]; Rn) and
measurable functions u : [S,T ] → Rm satisfying
x(t) ˙ = f (t, x(t), u(t)) a.e.,
u(t) ∈ U (t) a.e.,
φi(x(S), x(T )) ≤ 0 for i = 1, 2, .., k1,
ψi(x(S), x(T )) = 0 for i = 1, 2, .., k2.
The new ingredients here are the constraint functions φi : Rn × Rn → R, i =
1, .., k1 and ψi : Rn × Rn → R, i = 1, .., k2. We permit the cases k1 = 0 (no 
inequality constraints) and k2 = 0 (no equality constraints). 
Of course the necessary conditions of Theorem 7.2.1 apply to the special case of 
(P), in which the endpoint constraint set is chosen to be 
C = {(x0, x1) : φi(x0, x1) ≤ 0 for i = 1,...,k1
and ψi(x0, x1) = 0 for i = 1 ...,k2 }. (7.2.1) 
But they are rather cumbersome to apply, since this involves construction of a 
normal cone to an end-point constraint set defined implicitly as an admissible region 
for a collection of functional inequality and equality constraints. It is convenient 
then to have at hand necessary conditions expressed directly in terms of the 
constraint functionals themselves, by means of additional Lagrange multipliers. 
Such conditions are provided by the next theorem. 
Theorem 7.2.3 (The Maximum Principle for Functional End-Point Con￾straints) Suppose that (x,¯ u)¯ is a W1,1 local minimizer for (FEC). Assume that 
hypotheses (H1)–(H3) of Theorem 7.2.1 (for the data of the special case of (P) in 
which the endpoint constraint set C takes the form (7.2.1)) are satisfied for some 
ϵ >¯ 0. Assume also that the functions φi, i = 1, k1, and ψi, i = 1, k2, defining C
are Lipschitz continuous on a neighbourhood of (x(S), ¯ x(T )) ¯ . 
Then there exist p ∈ W1,1, λ ≥ 0, and sets of numbers {αi ≥ 0}
k1
i=1 and {βi}
k2
i=1, 
such that7.2 Clarke’s Nonsmooth Maximum Principle 301
(a): ||p||L∞ + λ + k1
i=1 αi + k2
i=1 |βi| /= 0, 
(b): − ˙p(t) ∈ co ∂xH(t, x(t), p(t), ¯ u(t)) ¯ a.e. t ∈ [S,T ], 
(c): H(t, x(t), p(t), ¯ u(t)) ¯ = maxu∈U (t) H(t, x(t), p(t), u) ¯ a.e. t ∈ [S,T ], 
(d): (p(S), −p(T )) ∈ ∂
	
λg(x(S), ¯ x(T )) ¯ + k1
i=1 αiφi(x(S), ¯ x(T )) ¯
+ k2
i=1 βi∂ψi(x(S), ¯ x(T )) ¯


, 
and 
αi = 0 for all i ∈ {1, .., k1} such that φi(x(S), ¯ x(T )) < ¯ 0. 
(In the preceding relations, k
i=1 is interpreted as 0 if k = 0.) 
If, furthermore, 
f (t, x, u) and U (t) are independent of t,
then there exists a constant r such that 
(e) H(t, x(t), p(t), ¯ u(t)) ¯ = r a.e.. 
Note that if the functions 
g, φ1,...,φk1 , ψ1,...,ψk2
are continuously differentiable and if f (t, x, u) is continuously differentiable with 
respect to the x variable, then the ‘nonsmooth’ conditions (b) and (d) can be replaced 
by relations involving classical (Fréchet) derivatives 
− ˙p(t) = ∇xH(t, x(t), p(t), ¯ u(t)) ¯ a.e.
and 
(p(S), −p(T )) = λ∇g(x(S), ¯ x(T )) ¯ +
k1
i=1
αi∇φi(x(S), ¯ x(T )) ¯
+

k2
i=1
βi∇ψi(x(S), ¯ x(T )). ¯
This version of the maximum principle (addressing problems with functional 
end-point constraints and smooth data) does not require, for its formulation, the 
modern apparatus of subdifferentials etc. Many of the more recent alternative 
necessary conditions of optimality, such as the generalized Euler Lagrange and 
generalized Hamilton conditions which are the subject matter of Chap. 8, are, by 
contrast, inherently nonsmooth: even if the dynamic constraint originates as a 
differential equation parameterized by a control variable, with smooth right side, the 
statement of these necessary conditions involves constructs of nonsmooth analysis 
(outside rather special cases).302 7 The Maximum Principle
Proof of Theorem 7.2.3 We assume that k1 ≥ 1 and k2 ≥ 1 and that all the end￾point inequality constraints are active. There is no loss of generality in so doing. To 
see this, note that (x,¯ z¯0 ≡ 0, z¯1 ≡ 0, u)¯ is a W1,1 local minimizer for a modified 
version of problem (F EC) obtained by deleting the inactive end-point inequality 
constraints, replacing the state vector x by (x, z0, z1) ∈ Rn×R×R and by requiring 
the new state trajectory components z0 and z1 to satisfy the differential equations 
z˙0(t) = 0, z˙1(t) = 0
and the end-point constraints 
z0(T ) ≤ 0, z1(T ) = 0.
The additional hypotheses (non-emptiness of the sets of equality and inequality 
constraints and all inequality constraints are active) are met. If the assertions 
of the theorem were true in these circumstances, we would be able to find a 
co-state trajectory which we write (p, q0, q1) satisfying the conditions of the 
maximum principle with functional end-point constraints. But because z0 and z1 are 
unconstrained at t = S, we conclude from the transversality conditions that q0 ≡ 0
and q1 ≡ 0. The resulting conditions may be interpreted as the assertions of the 
theorem for the original problem when we associate with each inactive inequality 
end-point constraint a zero Lagrange multiplier. 
Define G : Rn × Rn → R × Rk1 × Rk2 according to 
G(x0, x1) := 	
g(x0, x1) − g(x(S), ¯ x(T )), ¯
φ1(x0, x1), . . . , φk1 (x0, x1), ψ1(x0, x1)...,ψk2 (x0, x1)


.
Now observe that (x,¯ y¯ ≡ 0, u)¯ is a W1,1 local minimizer for the dynamic 
optimization problem with state vector (x, y = (y0, .., yk1+k2 )) ∈ Rn × R1+k1+k2 : 
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize y0(T )
over x ∈ W1,1, (y0, .., yk1+k2 ) ∈ W1,1
and measurable functions u satisfying
x˙ = f, y˙0 = 0, .., y˙k1+k2 = 0, a.e.,
u(t) ∈ U (t) a.e.,
(x(S), x(T ), y(S)) ∈ Gr G,
yi(T ) ≤ 0 for i = 1, .., k1,
yi(T ) = 0 for i = k1 + 1, .., k1 + k2.
The assertions of Theorem 7.2.1, which are valid for this problem, are expressed in 
terms of a co-state trajectory, which we write 
(p, q ≡ (−α0, .., −αk1 , −β1, .., −βk2 )),7.3 A Preliminary Maximum Principle, for Dynamic Optimization Problems. . . 303
and a cost multiplier λ ≥ 0, not both zero. (Here, we have made use of the fact that, 
since f (t, x, u) does not depend on (y0,...yk1+k2 ) and from the co-state inclusion, 
q does not depend on t.) From the transversality condition of Theorem 7.2.1, we 
know 
(p(S), −p(T ), −(α0, .., αk1 , β1, .., βk2 )) ∈ NGr G(x(S), ¯ x(T ), ¯ 0,..., 0)
α0 = λ, α1, .., αk1 ≥ 0.
With the help of Proposition 5.4.2, however, we deduce that 
(p(S), −p(T )) ∈ ∂

λg +
k1
i=1
αiφi +
k2
i=1
βiψi

(x(S), ¯ x(T )). ¯
This is the transversality condition as it appears in the theorem statement. All the 
other assertions of the theorem, for ‘Lagrange multipliers’ p, α1, .., αk1 , β1, .., βk2
and λ as above, follow from Theorem 7.2.1. ⨅⨆
7.3 A Preliminary Maximum Principle, for Dynamic 
Optimization Problems with no End-Point Constraints 
The proof of the maximum principle, as stated in Theorem 7.2.1, is built up 
in several stages. The first stage, the conclusions of which are summarised as 
Proposition 7.3.1 below, is accomplished in this section. We prove a version of the 
maximum principle for a variant of the dynamic optimization problem (P), the most 
significant feature of which is the absence of end-point constraints. The ultimate 
goal will be to prove the full maximum principle, for problems with end-point 
constraints; to this end, we shall, in the next section, apply the proposition to each 
of a sequence of endpoint-free problems, constructed with the help of Ekeland’s 
theorem, to generate a sequence of sets of Lagrange multipliers which, in the 
limit, satisfy the conditions of the maximum principle for problems with end-point 
constraints. The dynamic optimization problem considered in this section, which, in 
anticipation of these applications, also includes an integral cost term to take account 
of perturbations terms introduced by the application of Ekeland’s theorem, is as 
follows: 
(F )
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
Minimize J (x, u) := g(x(S), x(T )) + 
[S,T ] L(t, u(t)) dt,
subject to
x(t) ˙ = f (t, x(t), u(t)) a.e. t ∈ [S,T ],
u(t) ∈ U (t) a.e. t ∈ [S,T ],304 7 The Maximum Principle
the data for which comprise an interval [S,T ], functions g : Rn × Rn → R, f :
[S,T ] × Rn × Rm → Rn, L : [S,T ] × Rm → R, a non-empty multifunction 
U : [S,T ] ⇝ Rm. 
A admissible process (x, u) for (F) will be interpreted now as a pair of functions 
in which x ∈ W1,1([S,T ]; Rn) and u is a measurable selector of U, such that 
t → L(t, u(t)) is integrable. The concepts of W1,1 local minimizers and L∞ local 
minimizers, in this context, have their obvious meanings. 
The following proposition will be recognized as a special case of Theorem 7.2.1, 
extended, by state augmentation, to allow for the integral cost term, and in which the 
cost multiplier λ = 1. Notice that the admissible process (x,¯ u)¯ under consideration 
is assumed to be an L∞ local minimizer, not merely a W1,1 minimizer. 
For given λ ≥ 0, we define the integral cost-related un-maximized Hamiltonian, 
Hλ : [S,T ] × Rn × Rn × Rm → R, to be 
Hλ(t, x, p, u) := p · f (t, x, u) − λL(t, u) .
Proposition 7.3.1 Let (x,¯ u)¯ be an L∞ local minimizer for (F). Assume that, for 
some ϵ >¯ 0, the following hypotheses are satisfied: 
(F1): Gr U is an L × Bm measurable set, f is an L × Bn×m measurable function 
and there exist integrable functions c0 : [S,T ] → R and k0 : [S,T ] → R
such that 
|f (t, x, u) − f (t, x'
, u)| ≤ k0(t)|x − x'
| and |f (t, x, u)| ≤ c0(t)
for all x, x' ∈ ¯x(t) + ¯ϵB, u ∈ U (t), a.e. t ∈ [S,T ], 
(F2): g is Lipschitz continuous on (x(S), ¯ x(T )) ¯ + ¯ϵB × ¯ϵB, 
(F3): L is a bounded, L × Bm measurable function. 
Then there exist p ∈ W1,1([S,T ]; Rn) such that, for λ = 1, 
(b): − ˙p(t) ∈ co ∂xHλ(t, x(t), p(t), ¯ u(t)) ¯ a.e. t ∈ [S,T ], 
(c): Hλ(t, x(t), p(t), ¯ u(t)) ¯ = maxu∈U (t) Hλ(t, x(t), p(t), u) ¯ a.e. t ∈ [S,T ], 
(d): (p(S), −p(T )) ∈ λ∂g(x(S), ¯ x(T )) ¯ . 
(These conditions omit stating explicitly that (p, λ) /= (0, 0), i.e. the ‘non-triviality 
of the Lagrange multipliers’ condition, because, in view of the fact that λ = 1, this 
condition is automatically satisfied.) 
Proof of Proposition 7.3.1 We note at the outset that, without loss of generality, we 
can assume that the (F1) and (F2) (which are of a local nature w.r.t. the x variable) 
have been replaced by the stronger, global, hypotheses 
(F1)'
: Gr U is an L × Bm measurable set, f is an L × Bn×m measurable function 
and there exist integrable functions k0 : [S,T ] → R and c0 : [S,T ] → R
such that 
|f (t, x, u) − f (t, x'
, u)| ≤ k0(t)|x − x'
| and |f (t, x, u)| ≤ c0(t)
for all x, x' ∈ Rn, u ∈ U (t), a.e. t ∈ [S,T ], 
(F2)'
: g is Lipschitz continuous (with Lipschitz constant kg) on Rn × Rn.7.3 A Preliminary Maximum Principle, for Dynamic Optimization Problems. . . 305
This is because, if the stronger hypotheses are violated, we can replace f (t, x, u)
and g(x0, x1) by the functions f (t, x, u) ˜ := f (t, x(t) ¯ + trϵ¯(x − ¯x(t)), u) and 
g(x ˜ 0, x1) := g(x(S) ¯ +trϵ¯(x− ¯x(S)), x(T ) ¯ +trϵ¯(x− ¯x(T ))), in which trϵ¯ : Rn → Rn
(the ‘truncation function’) is 
trϵ¯(x) := 
x if |x|≤ ¯ϵ
ϵ¯ 1
|x|
x otherwise.
Note that the truncation function trϵ¯ has the following properties: for any function 
d : Rr → R that is Lipschitz continuous on ϵ¯B (with Lipschitz constant kd ) 
• (d ◦ trϵ¯)(x) = d(x) for all x ∈ ¯ϵB
• (d ◦ trϵ¯) (x) ≤ sup
x∈ ¯ϵB
d(x) for all x ∈ Rr
• (d ◦ trϵ¯)(x) is Lipschitz continuous on Rr with Lipschitz constant kd . 
It will be clear from these properties that the modified functions f˜ and g˜ satisfy the 
stronger hypotheses, with the original integrable bounds and Lipschitz constants. 
Furthermore, (x,¯ u)¯ remains an L∞ local minimizer for the modified data and the 
assertions of the proposition, in terms of the original and modified functions, are 
equivalent. 
We can also arrange, without loss of generality, that the integrable function k0 in 
hypothesis (F1) satisfies k0 ≥ 1 by adding, if necessary, a positive constant. 
Step 1: We prove the proposition under the additional hypothesis that 
(S):
⎧
⎨
⎩
g(x0, x1) = �(x0, x1) + α|x0 − ζ |,
for some continuously differentiable function � : Rn × Rn → R, ζ ∈ Rn and
α ≥ 0.
Since (x,¯ u)¯ is an L∞ local minimizer for problem (F), there exists ϵ > 0, such that 
(x,¯ u)¯ is a minimizer for 
(Fϵ )
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize J (x, u) := �(x(S), x(T ))+α|x(S)− ¯x(S)|+
[S,T ]
L(t, u(t)) dt,
subject to
x(t) ˙ = f (t, x(t), u(t)) a.e. t ∈ [S,T ],
u(t) ∈ U (t) a.e. t ∈ [S,T ],
||x − ¯x||L∞ ≤ ϵ .
For each positive integer i, consider the related problem: 
(Fi)
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize Ji(x, y, u) := �(x(S), x(T )) + α|x(S) − ¯x(S)|
+ 
[S,T ] L(t, u(t)) dt + i × 
[S,T ] k0(t)|y(t) − x(t)|
2dt
over y ∈ L1
k0
([S,T ]; Rn) and meas. functions u s.t.
x(t) ˙ = f (t, y(t), u(t)), a.e. t ∈ [S,T ],
u(t) ∈ U (t), a.e. t ∈ [S,T ],
||x − ¯x||L∞ ≤ ϵ.306 7 The Maximum Principle
Notice that, to construct each (Fi) problem, we have changed the dynamic constraint 
from the controlled differential equation x˙ = f (t, x, u) to a new controlled 
differential equation x˙ = f (t, y, u), in which y in interpreted as a new, control￾like, variable; then, to take account of the possible discrepancy between x and y, we 
have introduced a quadratic penalty term, with penalty parameter i. 
In (Fi), k0 is the integrable bound appearing in the hypotheses of the proposition 
statement, and L1
k0
([a, b]; Rn) denotes the ‘weighted L1 space’ of measurable 
functions φ : [a, b] → Rn such that t → k0(t)φ(t) is an L1 function. Observe 
that the cost in problem (Fi) can be infinite, because y is allowed to be a weighted 
L1 function and the cost involves a weighted L2 norm. Write the infimum costs of 
(Fi) and (Fϵ ) as inf(Fi) and inf(Fϵ ), respectively. 
Lemma 7.3.2 
lim
i→∞ inf(Fi) = inf(Fϵ ) .
Proof It is obvious from the special structure of the (Fi)’s and (Fϵ ) that 
−∞ < inf(Fi) ≤ inf(Fj ) ≤ inf(Fϵ ) for any index values i<j. (7.3.1) 
Fix i > 0 and take any admissible process (x, y, u) for (Fi) such that Ji(x, y, u)
< ∞. (Such a admissible process, namely (x,¯ y¯ ≡ ¯x, u)¯ , exists.) 
By Filippov’s existence theorem, applied to the reference trajectory x and initial 
state ξ = x(S), there exists a admissible process (xi, u) for (Fϵ ) (with the same u) 
such that xi(S) = x(S) and 
||xi − x||L∞ ≤ K × (

[S,T ]
k0(t)|y(t) − x(t)|dt).
(K is a number that does not depend on our choice of (x, y, u).) Notice that, from 
Hölder’s inequality, we know that (

[S,T ] k0(t)|y(t)−x(t)|dt)2 ≤ (

[S,T ] k0(t)dt)× 
[S,T ] k0(t)|y(t) − x(t)|
2dt. Observe also that |g(x(S), x(T )) − g(xi(S), xi(T ))| ≤
kg||xi − x||L∞ in which kg is a Lipschitz constant for g. It follows that 
Ji(x, y, u) − Ji(xi, xi, u) = Ji(x, y, u) − J (xi, u)
≥

[S,T ]
k0(t) 	
i × |y(t) − x(t)|
2 − Kkg|y(t) − x(t)|


dt ,
≥

(

[S,T ]
k0(t)dt)−1 × i × z2 − Kkgz

, (z := 
[S,T ]
k0(t)|y(t) − x(t)|dt )
≥ min
z∈R

(

[S,T ]
k0(t)dt)−1 × i × z2 − Kkgz

≥ −γ 2 × i
−1 ,7.3 A Preliminary Maximum Principle, for Dynamic Optimization Problems. . . 307
in which γ 2 := 1
4 × (Kkg)2 × ||k0||L1 . Since (x, y, u) was chosen arbitrarily, we 
deduce 
inf(Fi) ≥ inf(Fϵ ) − γ 2 × i
−1 .
This inequality, combined with (7.3.1) tell us that lim
i→∞ inf(Fi) = inf(Fϵ ) . ⨅⨆
Now write problem (Fi) as 
Minimize {Ji(x, y, u) : (x, y, u) ∈ Aϵ },
in which 
Aϵ := {(x, y, u) : x ∈ W1,1([S,T ]; Rn), y ∈ L1
k0
([S,T ]; Rn),
u is a measurable selector of U, x(t) ˙ = f (t, y(t), u(t)) a.e.,
||x − ¯x||L∞ ≤ ϵ}.
Equip Aϵ with the metric 
dE ((x'
, y'
, u'
), (x, y, u)) :=
|x'
(S) − x(S)| + 
[S,T ]
k0(t)|y'
(t) − y(t)| dt
+ meas {t ∈ [S,T ] : u'
(t) /= u(t)}.
We shall use the following properties of Aϵ and dE , which are consequences of 
the hypotheses of the proposition statement: 
(i): dE defines a metric on Aϵ and (Aϵ , dE ) is a complete metric space, 
(ii): If (xj , yj , uj ) → (x, y, u) in (Aϵ , dE ), then ||xj − x||L∞ → 0 as j → ∞, 
(iii): The function Ji is lower semi-continuous on (Aϵ , dE ). 
We comment only on the proof of (iii). Note that the function Ji is a sum of terms. 
Taking note of property (ii) above, we see that the first three terms are continuous. 
So we need to attend only to the last term. Take (xj , yj , uj ) → (x, y, u) w.r.t. 
dE . Since k0 ≥ 1, we know yj → y in L1 and therefore, for a subsequence, 
yj → y, a.e.. Using these facts and Fatou’s lemma, which may be invoked because 
the integrands are non-negative, we have 
lim inf
j→∞ 
[(S,T ]
k0(t)|yj (t) − xj (t)|
2dt ≥

[S,T ]
lim inf
j→∞

k0(t)|yj (t) − xj (t)|
2)dt
=

[S,T ]
k0(t)|y(t) − x(t)|
2dt .308 7 The Maximum Principle
(These relations are valid, even if some of the integrals involved are infinite.) Since 
the lower limit is independent of the subsequence that is chosen, the above relation 
is true also for the original sequence. We have confirmed the lower semi-continuity 
of the third term and therefore of Ji. 
By Lemma 7.3.2, there exists γi ↓ 0 such that, for each i, (x,¯ y¯ ≡ ¯x, u)¯ is a 
γ 2
i minimizer for (Fi). According to Ekeland’s theorem, there exists, for each i, an 
element (xi, yi, ui) ∈ Aϵ , which is a minimizer for the optimization problem: 
(F˜
i) Minimize {Ji(x, y, u) + γidE ((x, y, u), (xi, yi, ui)) : (x, y, u) ∈ Aϵ },
and 
dE ((xi, yi, ui), (x,¯ x,¯ u)) ¯ ≤ γi . (7.3.2) 
We know from the properties of the metric dE that ||xi − ¯x||L∞ → 0, as i → ∞. 
Consequently, for i sufficiently large, 
||xi − ¯x||L∞ ≤ ϵ/2 .
Define mi : [S,T ] × Rm → R as 
mi(t, u) := 
0 if u = ui(t)
1 if u /= ui(t) . (7.3.3) 
The significance of this definition is that it permits us to write the perturbation term 
meas{t : u(t) /= ui(t)} as an integral term, since, for any control function u, 
meas{t : u(t) /= ui(t)} = 
[S,T ]
mi(t, u(t))dt.
The cost function for (F˜
i) can be written 
J˜
i(x, y, u) := i ×

[S,T ]
k0(t)|y(t) − x(t)|
2dt
+γi

[S,T ]
k0(t)|y(t) − yi(t)|dt +

[S,T ]
(L(t, u(t)) + γi mi(t, u(t)))dt
+ �(x(S), x(T )) + α|x(S) − ¯x(S)| + γi |x(S) − xi(S)| .
Fix i. By choosing i sufficiently large, we can arrange that ||xi − ¯x||L∞ ≤
ϵ/2. We now assemble, in the following lemma, a set of conditions, in the 
form of a maximum principle, consequent on the fact that (xi, yi, ui) is an L∞
local minimizer for a modification of (F )˜ i, in which we drop the constraint 
||x − ¯x||L∞ ≤ ϵ.7.3 A Preliminary Maximum Principle, for Dynamic Optimization Problems. . . 309
Lemma 7.3.3 Define the function p : [S,T ] → Rn according to 

− ˙p(t) = 2 × i × k0(t)(yi(t) − xi(t)) a.e. t ∈ [S,T ]
−p(T ) = ∇x1 �(xi(S), xi(T )) .
Then 
(b) '
: − ˙p(t) ∈ co ∂x (p(t) · f (t, yi(t), ui(t))) + γik0(t)B , a.e. t ∈ [S,T ],
(c)'
: Hλ=1(t, yi(t), p(t), ui(t)) ≥ sup
u∈U (t)
Hλ=1(t, yi(t), p(t), u) − γi ,
a.e. t ∈ [S,T ] , 
( d) '
: p(S) ∈ ∇x0 �(xi(S), xi(T )) + (γi + α)B, −p(T ) = ∇x1 �(xi(S), xi(T )). 
Proof Since J˜
i(xi, yi, ui) < +∞, we deduce from the presence of the penalty 
quadratic term in Ji that yi ∈ L2
k0
([S,T ]; Rn). Take δ > 0, ξ ∈ Rn such that |ξ | ≤
1, a function y ∈ L2
k0
([S,T ]; Rn) and a selector u of U. Let x be the corresponding 
state trajectory, with initial condition x(S) = xi
(S)+δξ . Assume that x(t) ∈ ¯x(t)+
ϵ/2B for all t ∈ [S,T ]. 
By ‘optimality’ of (xi, yi, ui) and since x˙ = f (t, y, u) and x˙i = f (t, xi, ui), we 
can show that 
δ−1

[S,T ]
	
i × k0(t) 	
|y(t) − x(t)|
2−|yi(t) − xi(t)|
2


+ γi × k0(t)|y(t) − yi(t)|


dt + δ−1
	
�(xi(S) + δξ, x(T )) − �(xi(S), xi(T ))
+ γiδ|ξ | + α(|xi(S) + δξ − ¯x(S)|−|xi(S) − ¯x(S)|)


+ δ−1

[S,T ]
	
(L(t, u(t)) − L(t, ui(t))) + γi(mi(t, u(t)) − mi(t, ui(t)))

dt
+ δ−1

[S,T ]
p(t) ·
	
x(t) ˙ − ˙xi(t) − [f (t, y(t), u(t)) − f (t, yi(t), ui(t))]


dt ≥ 0 .
(7.3.4) 
An integration by parts yields the identity 

[S,T ]
p(t) · (x(t) ˙ − ˙xi(t))dt = − 
[S,T ]
p(t) ˙ · (x(t) − xi(t))dt
+p(T ) · (x(T ) − xi(T )) − p(S) · (x(S) − xi(S)) .
Substituting this expression into (7.3.4), employing the expansion 
|y(t) − x(t)|
2 = |y(t) − xi(t)|
2 + |x(t) − xi(t)|
2
−2(yi(t) − xi(t)) · (x(t) − xi(t)) − 2(x(t) − xi(t)) · (y(t) − yi(t))310 7 The Maximum Principle
and using the estimates 
�(x(S), x(T )) − �(xi(S), xi(T )) − ∇x0 �(xi(S), xi(T )) · (x(S) − xi(S))
−∇x1 �(xi(S), xi(T )) · (x(T ) − xi(T )) ≤ θ
	
δ|ξ |+|x(T ) − xi(T )|


and
|x(S) − ¯x(S)|−|xi(S) − ¯x(S)| ≤ δ|ξ |,
(for some function θ : [0,∞) → [0,∞) such that limα↓0 s−1θ (s) = 0), we arrive 
at 
δ−1

[S,T ]
	
i × k0(t) 	
|y(t) − xi(t)|
2 − |yi(t) − xi(t)|
2


+γi × k0(t)|y(t) − yi(t)|


dt
+ (∇x0 l(xi(S), xi(T )) − p(S)) · ξ + (γi + α)|ξ |
+δ−1

[S,T ]
(L(t, u(t)) − L(t, ui(t))) + γi(mi(t, u(t)) − mi(t, ui(t))) dt
+ δ−1

[S,T ]
−p(t) · [f (t, y(t), u(t)) − f (t, yi(t), ui(t)))] dt
+ E1(y, x, yi, xi) + E2(y, x, yi, xi) ≥ 0 . (7.3.5) 
in which the first ‘error’ term E1(y, x, yi, xi) is 
E1(y, x, yi, xi) :=
δ−1

[S,T ]
	
− 2ik0(t)(yi(t) − xi(t)) − ˙p(t)

·
	
x(t) − xi(t)

dt
+δ−1 
∇x1 �(xi(S), xi(T )) + p(T )
· (x(T ) − xi(T )) ,
and the second ‘error’ term E2(y, x, yi, xi) is 
E2(y, x, yi, xi) :=
δ−1

[S,T ]
i × k0(t)	
|x(t) − xi(t)|
2 − 2(y(t) − yi(t)) · (x(t) − xi(t))

dt
+ δ−1
	
�(xi(S) + δξ , x(T )) − �(xi(S), xi(T ))
−∇�(xi(S), xi(T )) · (δξ , x(T ) − xi(T ))

.7.3 A Preliminary Maximum Principle, for Dynamic Optimization Problems. . . 311
We can estimate E2(y, x, yi, xi) as follows: 
|E2(y, x, yi, xi)| ≤ δ−1θ
	
δ|ξ |+|x(T ) − xi(T )|


+ δ−1 × i × ||k0||L1 × ||x − xi||2
L∞ + γi

[S,T ]
k0(t)|y(t) − yi(t)|dt
+ δ−1 × i ×

[S,T ]
2k0(t)|y(t) − yi(t)|dt × ||x − xi||L∞ . (7.3.6) 
Note that E1(y, x, yi, xi) ≡ 0, because of the defining relations for p. 
We now confirm the assertions of the lemma by examining inequality (7.3.5), for 
various choices of δ > 0, ξ , y and u. 
Confirmation of (d)'
: Notice that −p(T ) = ∇x1 �(xi(S), xi(T )), by definition of p. 
To verify the full transversality condition, take any ξ ∈ Rn such that |ξ | ≤ 1 and 
sequence δj ↓ 0. For each j let xj be the state trajectory corresponding to yi and 
ui, and with initial value xj (S) = xi(S) + δj ξ . We can deduce from Filippov’s 
existence theorem that 
||xj − xi||L∞ → 0, as j → ∞ (7.3.7) 
and there exists a number B such that, for j = 1, 2,..., 
δ−1
j ||xj − xi||L∞ ≤ B. (7.3.8) 
Now consider (7.3.5) when δ = δj , y = yi, u = ui and x = xj . From (7.3.7) 
and (7.3.8) we see that 
E2(yi, xj , yi, xi) → 0 as j → ∞.
We may pass to the limit as j → ∞, to obtain 

∇x0 �(xi(S), xi(T )) − p(S)
· ξ + (γi + α)|ξ | ≥ 0 .
Since this inequality is valid for every ξ ∈ Rn such that |ξ | ≤ 1, We can conclude 
that p(S) ∈ ∇x0 �(xi(S), xi(T )) + (γi + α)B. 
Confirmation of (b) '
: Choose any measurable function y : [S,T ] → Rn such that 
y(t) ∈ yi(t) + (1 + k0(t))−1B a.e. t . (7.3.9) 
Observe that y ∈ L2
k0
([S,T ]; Rn). Define 
S := {t
¯ ∈ (S, T ) : t
¯ is a Lebesgue point of
t → k0(t)(|y(t) − xi(t)|
2 − |yi(t) − xi(t)|
2 + γi|y(t) − yi(t)|)
and t → f (t, y(t), ui(t)) − f (t, yi(t), ui(t))}.312 7 The Maximum Principle
Take δj ↓ 0 and t
¯ ∈ S. For each j , t ∈ [S,T ], let 
yj (t) =

y(t) if t ∈ [t,( ¯ t
¯ + δj ) ∧ T ]
yi(t) if t /∈ [t,( ¯ t
¯ + δj ) ∧ T ] .
Write xj for the state trajectory corresponding to yj and ui, with initial value 
xi(S). For each j , consider (7.3.5) with δ = δj , ξ = 0, y = yj and u = u∗. Making 
use of (7.3.9), we can show that (7.3.7) and (7.3.8) are satisfied, and  k0(t)|yj (t)−
yi(t)|dt → 0 as j → ∞. It follows that 
E2(yj , xj , yi, xi) → 0 as j → ∞.
Since t
¯ ∈ S, we can pass to the limit in (7.3.5) as j → ∞, to obtain φ(t, y( ¯ t)) ¯ ≥ 0 , 
where 
φ(t, y) := k0(t) ×
	
i × (|y − xi(t)|
2 − |yi(t) − xi(t)|
2 + γi × |y − yi(t)|


−p(t) · (f (t, y, ui(t)) − f (t, yi(t), ui(t)).
Since y was chosen arbitrarily and S has full measure, we have shown that 

[S,T ]
φ(t, y(t))dt ≥ 0
for all measurable functions y’s satisfying y(t) ∈ yi(t) + (1 + k0(t))−1B for a.e. 
t ∈ [S,T ]. 
Take ϵk ↓ 0 and, for each k, consider the multifunction 
Yk(t) := {y ∈ yi(t)+(1+k0(t))−1B : φ(t, y) ≤ inf
y'
∈yi(t)+(1+k0(t))−1B
φ(t, y'
)+ϵk}.
Yk is a non-empty multifunction such that Gr{Yk} is an L × B measurable set. By 
Aumann’s theorem, Yk has a measurable selection y'
. From the preceding analysis, 

[S,T ] φ(t, y'
(t))dt ≥ 0. But then, for each k, 
0 ≤
 T
S
φ(t, y'
(t))dt ≤
 T
S
inf
y∈yi(t)+(1+k0(t))−1B
φ(t, y)dt + ϵk × |T − S|.
Since k is arbitrary, ϵk ↓ 0 and φ(t, yi(t)) = 0 a.e. 
 T
S
	
inf
y∈y∗(t)+(1+k0(t))−1B
φ(t, y) − φ(t, yi(t))

dt ≥ 0 .7.3 A Preliminary Maximum Principle, for Dynamic Optimization Problems. . . 313
From the fact that the integrand is non-positive, we deduce that 
φ(t, yi(t)) = min{φ(t, y) : y ∈ yi(t) + (1 + k0(t))−1B}, a.e. t ∈ [S,T ].
But then 
−2ik0(t)(yi(t)−xi(t)) ∈ ∂x (−p(t)·f (t, yi(t), ui(t)))+γik0(t)B a.e. t ∈ [S,T ] .
From the defining relations for the p’s we deduce 
p(t) ˙ ∈ ∂x (−p(t) · f (t, yi(t), ui(t))) + γik0(t)B, a.e t ∈ [S,T ].
This implies relation (b)'
. 
Confirmation of (c)'
: Take Mk ↑ ∞ and, for each k, define the mutifunction Uk :
[S,T ] ⇝ Rm
Uk(t) := U (t) ∩ {u ∈ Rm : |f (t, yi(t), u) − f (t, yi(t), ui(t))| ≤
Mk
1 + k0(t)
B}.
Fix k > 0. Choose an arbitrary selector uˆ of Uk and define 
S := {t
¯ ∈ (S, T ) : t
¯ is a Lebesgue point of t → (mi(t, u(t)) ˆ − mi(t, ui(t))
−Hλ=1(t, yi(t), p(t), u(t)) ˆ + Hλ=1(t, yi(t), p(t), ui(t))}.
Take δj ↓ 0 and t
¯ ∈ S. For each j , t ∈ [S,T ], let 
uj (t) := 
u(t) ˆ if t ∈ [t,( ¯ t
¯ + δj ) ∧ T ]
ui(t) if t /∈ [t,( ¯ t
¯ + δj ) ∧ T ] .
Write xj for the state trajectory corresponding to yi and uj , with initial value 
xi(S). For each j , consider (7.3.5) with δ = δj , ξ = 0, y = yi, and u = uj . We can 
show that (7.3.7) and (7.3.8) are satisfied. It follows that 
E2(yi(t), xj (t), yi(t), xi(t)) → 0 as j → ∞, a.e..
Since t
¯ ∈ S, we can pass to the limit in (7.3.5) as j → ∞, to obtain 
γi(mi(t ,¯ u(ˆ t)) ¯ − mi(t,u ¯ i(t)) ¯ − Hλ=1t,y ¯ i(t), p( ¯ t), u( ¯ t)) ¯
+Hλ=1(t,y ¯ i(t), p( ¯ t), u ¯ i(t)) ¯ ≥ 0 .314 7 The Maximum Principle
Since uˆ was chosen arbitrarily and S has full measure, we can conclude that 

[S,T ]
ψ(t, u(t))dt ≥ 0 for every selector u of Uk,
where 
ψ(t, u) := γi(mi(t, u) − mi(t, ui(t))) − Hλ=1(t, yi(t), p(t), u)
+ Hλ=1(t, yi(t), p(t), ui(t)) .
Take ϵh ↓ 0. Fix an integer h. Arguing as above, we can deduce, with the help of 
Aumann’s theorem, there exists a selector u' of Uk such that 
ψ(t, u'
(t)) ≤ inf u∈Uk (t)
ψ(t, u) + ϵh for a.e. t ∈ [S,T ].
(To ensure existence of this selector, we use the fact that inf u∈Uk (t)
ψ(t, u) > −∞ for 
each t.) 
According to the earlier analysis, 
[S,T ] ψ(t, u'
(t))dt ≥ 0. It follows that 

[S,T ]
inf u∈Uk (t)
ψ(t, u)dt ≥ −ϵh|T − S |.
Since this is true for any h, 

[S,T ]
inf u∈Uk (t)
ψ(t, u)dt ≥ 0.
But ψ(t, ui(t)) = 0. It follows that 

[S,T ]
	
inf u∈Uk (t)
ψ(t, u) − ψ(t, ui(t))

dt ≥ 0.
Noting that the integrand is non-positive, we deduce that 
ψ(t, ui(t)) = inf u∈Uk (t)
ψ(t, u) a.e..
We have shown that 
− γi mi(t, ui(t)) + Hλ=1(t, yi(t), p(t), ui(t))
= max
u∈Uk (t)
{−γi mi(t, u) + Hλ=1(t, y∗(t), p(t), u)}.7.3 A Preliminary Maximum Principle, for Dynamic Optimization Problems. . . 315
Since mi(t, ui(t)) = 0 and |mi(t, u)| ≤ 1 for all u, we deduce that 
Hλ=1(t, yi(t), p(t), ui(t)) ≥ max
u∈Uk (t)
{Hλ=1(t, yi(t), p(t), u)} − γi .
The preceding relation is valid on a set of full measure and 
U (t) = lim
k→∞ Uk(t) , for all t ∈ [S,T ] .
It follows that 
Hλ=1(t, yi(t), p(t), ui(t)) ≥ max
u∈U (t)
{Hλ=1(t, yi(t), p(t), u)} − γi , a.e. t ∈ [S,T ] .
The proof of the Lemma is complete. 
⨅⨆
We observe that Lemma 7.3.3 provides perturbed versions of the desired 
necessary conditions, in terms of a costate function, which we write pi to emphasize 
the i dependence. It remains to construct, in the limit as i → ∞, a costate trajectory 
p, with respect to which conditions in the special case of the maximum principle 
are satisfied. 
Notice that the relation (7.3.2) ensures that, along a subsequence, yi converges 
in L1
k0
, therefore in L1 (since k0 ≥ 1), and, consequently, a.e., to x(t) ¯ on [S,T ], xi
converges uniformly to x¯ and meas{t ∈ [S,T ] : ui(t) /= ¯u} → 0, as i → ∞. 
By (d)'
, the pi(T )’s are uniformly bounded. From (b)' and (A2), | ˙pi(t)| ≤
k0(t)|pi(t)| a.e.. But then, by Gronwall’s lemma, the pi’s are uniformly bounded 
in the L∞ norm and the p˙i’s uniformly integrably bounded. By Theorem 6.3.3 
(compactness of trajectories), the pi’s converge strongly in L∞ to some p ∈ W1,1, 
the p˙i’s converge weakly in L1 to p˙ and p satisfies 
− ˙p(t) ∈ co ∂x (p(t) · f (t, x(t), ¯ u(t))) ¯ a.e. t ∈ [S,T ],
Since the pi’s converge strongly in L∞, we deduce from (d)'
(p(S), −p(T )) ∈ ∇�(x(S), ¯ x(T )) ¯ + αB × {0} = ∂g(x(S), ¯ x(T )) ¯
Taking note of (7.3.2), we can arrange, by extracting a subsequence, that 
meas{t : ui(t) = ¯ / u(t) for all i sufficiently large } → 0, as i → ∞.
Using this property, we can deduce from (c)' that 
Hλ=1(t, x(t), p(t), ¯ u(t)) ¯ ≥ sup
u∈U (t)
Hλ=1(t, x(t), p(t), u) ¯ a.e. t ∈ [S,T ] ,316 7 The Maximum Principle
We have confirmed that all assertions of the proposition, and specified strengthened 
hypotheses on the structure and regularity of the end-point cost function g. 
Step 2: Completion of the Proof. 
The previous step validates the assertions of the Proposition, under an additional 
hypothesis concerning the end-point cost function g, namely: 
(S): g(x0), x1) = �(x0, x1) + α|x0 − ζ |, for some C1 function �, ζ ∈ Rn and α ≥ 0. 
Now suppose that g is merely Lipschitz continuous (with Lipschitz constant kg), 
in accordance with hypothesis (F2)'
. The final step is to show that the assertions 
remain true. 
For i = 1, 2,..., let gi be the quadratic inf convolution of g (with parameter 
α = i): 
gi
(z) := inf
y∈Rn×Rn
{g(y) + i × |y − z|
2} for z ∈ Rn × Rn . (7.3.10) 
Recall the key ‘quadratic inf convolution’ properties of gi
. Take any z ∈ Rn × Rn
and let y ∈ Rn × Rn be any vector achieving the infimum in (7.3.10) (one such 
vector exists). Let 
ηi
(z) := −2i(y − z) .
(i): gi is Lipschitz continuous with Lipschitz constant kg, 
(ii): g(z) ≥ gi
(z) ≥ g(z) − k2
g × i−1, 
(iii): gi
(z'
) − gi
(z) ≤ ηi
(z) · (z' − z) + i × |z' − z|
2 for all z' ∈ Rn × Rn, 
(iv): ηi
(z) ∈ ∂P g(y), 
(v): |y − z| ≤ kg × i−1 . 
Since (x,¯ u)¯ is an L∞ local minimizer for (F), (x,¯ u)¯ is a minimizer for 
(Q) : Minimize {J (x, u) : (x, u) ∈ Bϵ },
for some ϵ > 0, where 
J (x, u) := 
[S,T ]
L(t, u(t))dt + g(x(S), x(T )) ,
and Bϵ := {admissible processes (x, u) for (F ) such that ||x − ¯x||L∞ ≤ ϵ}.
For each i, consider the problem 
(Qi
) : Minimize {J i
(x, u) : (x, u) ∈ Bϵ }
in which 
J i
(x, u) := 
[S,T ]
L(t, u(t))dt + gi
(x(S), x(T )) .7.3 A Preliminary Maximum Principle, for Dynamic Optimization Problems. . . 317
Equip Bϵ with the metric 
dE ((x'
, u'
), (x, u)) := meas {t ∈ [S,T ] : u'
(t) /= u(t)}+|x'
(S) − x(S)| .
(7.3.11) 
It can be shown that, w.r.t. this metric, Bϵ is complete and J i is continuous on Bϵ . 
Now note that, from property (ii) of the quadratic inf convolution construction, 
(x,¯ u)¯ is a γ 2
i -minimizer for (Qi
), where γ 2
i := k2
g i−1. In consequence of Ekeland’s 
theorem, there exists (xi, ui) ∈ Bϵ which is a minimizer for (Q˜ i
): 
(Q˜ i
) Min {J i
(x, u) + γidE ((x, u), (xi, ui)) : (x, u) ∈ Bϵ }.
and 
dE ((xi, ui), (x,¯ u)) ¯ ≤ γi . (7.3.12) 
The cost function for (Q˜ i
) can be written 
J˜i
(x, u):= 
[S,T ]
(L(t, u(t))+γimi(t, u(t)))dt+gi
(x(S), x(T ))+γi|x(S)−xi(S)|.
(mi(t, u) was defined in (7.3.3)). But by properties (iii), (iv) and (v) of quadratic inf 
convolutions (see above), 
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
gi
(x0, x1) − gi
(xi(S), xi(T ))
≤ ηi
0 · (x0−xi(S))+ηi
1 · (x1−xi(T ))+i × (|x0−xi(S)|
2+|x1 − xi(T )|
2)
and, trivially,
gi
(xi(S), xi(T )) − gi
(xi(S), xi(T ))
= ηi
0 · (xi(S) − xi(S)) + ηi
1 · (xi(T ) − xi(T ))
+i × (|xi(S) − xi(S)|
2 + |xi(T ) − xi(T )|
2),
(7.3.13) 
in which 
(ηi
0, ηi
1) ∈ ∂P g(y0, y1) for some (y0, y1) ∈ (xi(S), xi(T )) + kg i
−1(B × B).
(7.3.14) 
In consequence of the ‘upper envelope’ property (7.3.13), (xi, ui) remains a 
minimizer for (Q˜ i
), when we replace the additive term ‘gi
(x(S), x(T ))’ in the cost 
function by the quadratic function 
ηi
0 ·(x(S)−xi(S))+ηi
1 ·(x(T )−xi(T ))+i ×(|x(S)−xi(S)|
2 +|x(T )−xi(T )|
2) .
In other words, (xi, ui) is a minimizer for 
(Q˜ i
0) : Minimize {J˜i
0(x, u) : (x, u) ∈ Bϵ },318 7 The Maximum Principle
in which 
J˜i
0(x, u) := 
[S,T ]
(L(t, u(t)) + γimi(t, u(t)))dt + ηi
0 · (x(S) − xi(S))
+ηi
1 · (x(T ) − xi(T )) + i × (|x(S) − xi(S)|
2
+|x(T ) − xi(T )|
2) + γi |x(S) − xi(S)| .
The data for Problem (Q˜ i
0) satisfies the hypotheses for the validity of the assertions 
of the Proposition, in the special case covered by Step 1. We conclude the existence 
of pi : [S,T ] → Rn Note, in particular, that, from (d'
), (pi(S), −pi(T )) ∈
(ηi
0, ηi
1) + γiB × {0} ⊂ ∂P g(y0, y1) + γiB × {0}. From (7.3.14) then, 
(pi(S), −pi(T )) ∈ (ηi
0, ηi
1) + γiB × {0}
⊂ ∪ z∈(xi(S),xi(T ))+kgi−1B×B
∂P g(z) + γiB × {0}, (7.3.15) 
which is a convenient perturbed version of (d), for limit taking. Next define, for each 
j , Ej := {t ∈ [S,T ] : uj (t) /= ¯u(t)}. From (7.3.12) we know that meas{Ej } → 0
as j → ∞, where meas denotes Lebesgue measure. By extracting a subsequence, 
we can arrange that ∞
j=1 meas{Ej } < ∞. But then 
∞
j=k
meas{Ej } → 0, as k → ∞. (7.3.16) 
It follows that meas ∪∞
j=k Ej → 0, as k → ∞. Now define T := [S,T ] \ 	
∩∞
k=1
∪∞
j=kEj


. Notice that T has representation 
T = {t ∈ [S,T ] : there exists N (t) s.t. ui(t) = ¯u(t) for all i ≥ N (t)}.
In view of (7.3.16), T is a subset of [S,T ] with full Lebesgue measure. It follows 
that 
ui(t) = ¯u(t) , for all i sufficiently large, a.e. t ∈ [S,T ]. (7.3.17) 
From (7.3.12) we also know that xi → ¯x, uniformly, as i → ∞. On the other hand, 
the pi’s are uniformly bounded with uniformly integrably bounded derivatives, in 
view of conditions (c)' and (d)' of Lemma 7.3.3. We may deduce from Ascoli’s 
theorem that, after a further subsequence extraction, pi converges uniformly to some 
W1,1 function p as i → ∞, and its derivative p˙i converges weakly in L1 to p˙. The 
validity of the co-state inclusion (b) (expressed in terms of the limiting p) now 
follows from the compactness of trajectories theorem Theorem 6.3.3. Condition (c)7.4 Proof of Theorem 7.2.1 319 
is obtained in the limit from (c)'
, in view of the convergence properties of the xi’s, 
pi’s and ui’s. Finally, (d) is obtained in the limit from (7.3.15). ⨅⨆
Notice the important role of the convergence property (7.3.17). It has permitted 
us to derive the costate inclusion and Weierstrass condition, as a limit of the 
perturbed versions of these conditions, despite the fact the u- dependence of 
f (t, x, u) and L(t, u) is possibly discontinuous. This would not have been the case, 
if we had, for example, chosen the metric, for application of Ekeland’s theorem in 
the final step of the proof, to be d((x'
, u'
), (x, u)) := ||u' −u||L1 +|x'
(S)−x(S)| in 
place of (7.3.11). The use of this alternative metric would result in the replacement 
of (7.3.17) by 
ui → u strongly in L1 and a.e.
and would not permit limit taking to give the desired necessary conditions, in the 
absence of additional continuity hypotheses concerning the u-dependence of the 
data. 
7.4 Proof of Theorem 7.2.1 
In this section we prove assertions (a)–(d) of the theorem. Proof of the remaining 
assertion (e), concerning additional properties of W1,1 minimizers, when U and 
f (t, x, u) do not depend on t, will be given in Chap. 9 (Sect. 9.6), as part of a broader 
study of necessary conditions when additional regularity hypotheses are imposed 
on the time dependence of the data and when the end-times are included among the 
choice variables. 
Step 1. (Proof of the Theorem Under Strengthed Hypotheses) 
In this step we establish validity of the assertions of the Theorem under the following 
additional hypothesis: 
(A): There exist integrable functions k0 : [S,T ] → R and c0 : [S,T ] → R such 
that 
(i): |f (t, x, u) − f (t, x'
, u)| ≤ k0(t)|x − x'
|
(ii): |f (t, x, u)| ≤ c0(t)
for all x, x' ∈ ¯x(t) + ¯ϵB, u ∈ U (t), a.e. t ∈ [S,T ]. 
and when (x,¯ u)¯ is an L∞ local minimizer for (P). 
Let ϵ ∈ (0, ϵ)¯ be such that (x,¯ u)¯ is a minimizer for (P ) w.r.t. admissible state 
trajectories x satisfying ||x − ¯x||L∞ ≤ ϵ. 
Take γi ↓ 0. For i = 1, 2 ..., consider the problem:320 7 The Maximum Principle
(Pi
1 )
⎧
⎪⎪⎨
⎪⎪⎩
Minimize J i
1(x, u) subject to
x(t) ˙ = f (t, x(t), u(t))
u(t) ∈ U (t) a.e. t ∈ [S,T ],
||x − ¯x||L∞ ≤ ϵ ,
in which J i
1(x, u) = max{g(x(S), x(T )) − g(x(T ), ¯ x(T )) ¯ + γ 2
i , dC(x(S), x(T ))}.
Since J i
1(x,¯ u)¯ = γ 2
i , and J i
1 is non-negative valued, (x,¯ u)¯ is a γi-minimizer. 
Let Bϵ denote the set of admissible processes for (Pi
1 ). Applying Ekeland’s 
theorem to the function J i
1 : Bϵ → R with metric dE given by (7.3.11), we are 
assured of the existence of a minimizer (xi, ui) for 
(P˜i
1 )
⎧
⎪⎪⎨
⎪⎪⎩
Minimize J˜i
1(x, u) subject to
x(t) ˙ = f (t, x(t), u(t))
u(t) ∈ U (t) a.e. t ∈ [S,T ],
||x − ¯x||L∞ ≤ ϵ ,
where 
J˜i
1(x, u) := J i
1(x, u) + γi(

[S,T ]
mi(t, u(t)) dt + |x(S) − xi(S)|) .
Furthermore 
dE ((xi, ui), (x,¯ u)) ¯ ≤ γi . (7.4.1) 
It can be deduced from (7.4.1) that ||xi − ¯x||L∞ → 0 as i → ∞ and therefore 
that, for i sufficiently large, (xi, ui) is an L∞ local minimizer for (P˜i
1 ), when the 
constraint ‘||x − ¯x||L∞ ≤ ϵ’ is removed. The data for this last problem satisfies the 
hypotheses of Proposition 7.3.1. We deduce the existence of a costate trajectory p
and with properties listed in the Proposition statement. From properties (b) and (c) 
we obtain immediately the conditions 
(b)'
: − ˙p(t) ∈ co ∂x p(t) · f (t, xi(t), ui(t)), a.e. t ∈ [S,T ],
(c)'
: H(t, xi(t), p(t), ui(t)) ≥ max
u∈U (t)
H(t, xi(t), p(t), u) − γi a.e. t ∈ [S,T ] .
Let us examine the implications of the transversality condition (d). In this 
connection, we take note of the following important strict inequality: 
max{g(xi(S), xi(T )) − g(x(S), ¯ x(T )) ¯ + γ 2
i , dC(xi(S), xi(T ))} > 0,
for i sufficiently large . (7.4.2) 
Indeed if this were not the case, we would have g(xi(S), xi(T )) − g(x(S), ¯ x(T )) ¯ ≤
−γ 2
i and dC(xi(S), xi(T )) = 0. This contradicts the L∞ local optimality of (x,¯ u)¯
for problem (P ).7.4 Proof of Theorem 7.2.1 321 
In consequence of the max rule for limiting subdifferentials, we know that 
∂ max{g(x¯i(S), x¯i(T )) − g(x(S), ¯ x(T )) ¯ + γ 2
i , dC(x¯i(S), x¯i(T ))}
⊂ λ∂g(x¯i(S), x¯i(T )) + (1 − λ)∂dC(x¯i(S), x¯i(T )) ,
for some λ ∈ [0, 1]. Moreover, in view of (7.4.2), 
‘1 − λ > 0’ =⇒
‘dC(x¯i(S), x¯i(T )) = max{g(x¯i(S), x¯i(T )) − g(x(S), ¯ x(T )) ¯ + γ 2
i ,
dC(x¯i(S), x¯i(T ))}’. (7.4.3) 
We deduce from condition (d) that 
(d)’: (p(S), −p(T )) ∈ λ∂g(xi(S), xi(T ))+(1−λ)∂dC(xi(S), xi(T ))+γiB× {0}.
We claim that 
(1 + kg)λ + |(p(S), p(T ))| ≥ 1 − γi . (7.4.4) 
This is obviously true if λ = 1. If, on the other hand, λ < 1, we know from (7.4.2) 
and (7.4.3) that dC(xi(S), xi(T )) > 0. But then ∂dC(xi(S), xi(T )) ⊂ ∂B. It follows 
then from (d)' that 
|(p(S), p(T ))| ≥ (1 − λ) − kgλ − γi .
Then (1 + kg)λ + |(p(S), p(T ))| ≥ 1 − γi, as asserted. 
To emphasize the fact that these relations depend on i, we change the notation 
for p and λ to pi and λi, respectively. Condition (7.4.1) ensures that, along a 
subsequence, 
ui(t) = ¯u(t) for all i sufficiently large, a.e. t ∈ [S,T ]
and also that xi → ¯x uniformly as i → ∞. The pi’s are a uniformly bounded 
sequence of absolutely continuous functions with uniformly integrably bounded 
derivatives. It follows that pi converges to an absolutely continuous function p, 
and p˙i converges to p˙ weakly in L1, following extraction of a further subsequence 
A similar convergence analysis to that employed in the proof of Proposition 7.3.1 
permits us to pass to the limit in relations (b)' − (d)'
, and thereby arrive at the 
assertions (b)-(d) of the theorem statement. We deduce from (7.4.4), in the limit, 
that (1 + kg)λ + |(p(S), p(T ))| ≥ 1. Hence λ + ||p||L∞ > 0. This is condition (a).322 7 The Maximum Principle
Step 2: Completion of the Proof 
We have confirmed the assertions of the theorem, but only when (x,¯ u)¯ is an L∞
local minimizer and the hypotheses (H1)–(H3) are supplemented by (A). It remains 
to remove both of these restrictions. 
Assume, first, that (x,¯ u)¯ is an L∞ local minimizer for (P) and that the assertions 
of the Theorem are known to be valid under hypotheses (H1)–(H3) and (A). We 
show that the assertions remain valid when (H1)–(H3) alone are satisfied. 
For each integer j = 1, 2,..., consider the dynamic optimization problem 
(Pj )
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize g(x(S), x(T ))
over x ∈ W1,1([S,T ]; Rn) and meas. functions u : [S,T ] → Rm,
such that
x(t) ˙ = f (t, x(t), u(t)) a.e. t ∈ [S,T ],
u(t) ∈ Uj (t) a.e. t ∈ [S,T ],
(x(S), x(T )) ∈ C ,
in which 
Uj (t) := {u ∈ U (t) : k(t, u) ≤ k(t, u(t)) ¯ + j, |f (t, x(t), u) ¯ |≤|˙
x(t) ¯ | + j }.
For each j , (x,¯ u)¯ remains an L∞ local minimizer of (Pj ). (H1)–(H3) are satisfied. 
But, because of the modified control constraint multifunction, the data for problem 
(Pj ) also satisfies the supplementary hypothesis (A), in which the integrable 
functions k(t, u(t)) ¯ + j and |˙
x(t) ¯ | + j + ¯ϵ × (k(t, u(t)) ¯ + j ) take the roles of k0(t)
and c0(t). For each j then, there exists (pj , λj ) such that conditions (a), (b) and (d) 
of the theorem statement are satisfied (when (λj , pj ) replaces (λ, p)), together with 
the restricted-sense Weierstrass condition: 
H(t, x(t), p ¯ j (t), u(t)) ¯ ≥ max
u∈Uj (t)
H(t, x(t), p ¯ j (t), u) for all t ∈ S , (7.4.5) 
where S ⊂ [S,T ] is a subset of full measure, which does not depend on j . 
For each j , we can normalize the non-zero Lagrange multipliers to satisfy λj +
|pj (S)| = 1. From the costate inclusion we deduce that | ˙pj (t)| ≤ k(t, u(t)) ¯ |pj (t)|, 
a.e.. It follows from Gronwall’s lemma that the pj ’s are uniformly bounded and, 
consequently, the p˙j ’s have a uniform integral bound. Along a subsequence then, 
λj → λ for some λ ∈ [0, 1] and pj → p strongly in L∞, for some p ∈ W1,1. 
Furthermore, p˙j → ˙p weakly in L1. Passing to the limit, as j → ∞, we deduce 
that λ + |p(S)| = 1. This implies condition (a) of the theorem statement. 
We recover also conditions (b) and (d) of the theorem statement (expressed in 
terms of the Lagrange multipliers (p, λ)), in the limit as j → ∞. Making use of the 
observation: 
for any t ∈ S and u ∈ U (t), u ∈ Uj (t) for j sufficiently large,
we recover also (c), from (7.4.5).7.5 Exercises 323
All the assertions of the theorem have been proved, under the hypotheses (H1)-
(H3), in the case when (x,¯ u)¯ is an L∞ local minimizer. Now suppose that (x,¯ u)¯ is 
merely a W1,1 minimizer. Then there exists ϵ > 0 be such that (x,¯ u)¯ is minimizing 
with respect to admissible processes (x, u) satisfying ||x − ¯x||W1,1 ≤ ϵ. 
We see that (z¯ ≡ 0, x,¯ u)¯ is a minimizer for the dynamic optimization problem 
with augmented state : 
(Paug)
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize g(x(S), x(T ))
over x ∈ W1,1([S,T ]; Rn), z ∈ W1,1(S, T ) and u a selector of U,
such that
(x(t), ˙ z(t)) ˙ =(f (t, x(t), u(t)), |f (t, x(t), u(t))−˙
x(t) ¯ |) a.e. t∈[S,T ],
u(t) ∈ U (t) a.e. t ∈ [S,T ],
(x(S), x(T )) ∈ C , z(S) = 0, |x(S) − ¯x(S)|+|z(T )| ≤ ϵ .
The data for problem (Paug) satisfies (H1)–(H3). But then, since (z,¯ x,¯ u)¯ is 
certainly an L∞ minimizer, we can apply the special case of the theorem already 
proved. We deduce the existence of a set of Lagrange multipliers (λ, p, r), in which 
the costate trajectory components p and r are associated with the state variables x
and z, satisfying the conditions of the maximum principle, expressed in terms of 
(λ, p, r) and (z,¯ x,¯ u)¯ . 
Since the constraint ‘|x(S) − ¯x(S)|+|z(T )| ≤ ϵ’ is inactive at (x(S), z(T )) ¯ , 
we can assume, when analysing the implications of the transversality condition 
associated with (Paug), that this constraint is absent; according, we deduce that 
r(T ) = 0 and (p(S), −p(T )) ∈ ∂g(x(S), ¯ x(T )) ¯ + NC(g(x(S), ¯ x(T )) ¯ . The latter 
relation is the transversality condition for (P). 
The dynamic constraint in (Paug) takes the form of a controlled differential 
equation that does not depend on the z variable; hence, r˙ ≡ 0. Since r(T ) = 0, 
we know then that r ≡ 0. Setting r ≡ 0 in all the other maximum principle relations 
for (Paug) yields the non-triviality condition, costate inclusion and Weierstrass 
condition for (P ). ⨅⨆
7.5 Exercises 
7.1 Consider the autonomous dynamic optimization problem 
(P7)
⎧
⎪⎪⎨
⎪⎪⎩
Minimize g(x(S), x(T ))
subject to
x(t) ˙ = f (x(t), u(t)) and u ∈ U, a.e. t ∈ [S,T ],
(x(S), x(T )) ∈ C .
with data: functions g : Rn × Rn → R, f : Rn × Rm → Rn and sets U ⊂ Rm and 
C ⊂ Rn × Rn. Assume324 7 The Maximum Principle
(a): f (., u) is C1 for each u ∈ U, f is continuous, U is compact, C is closed, g is 
locally Lipschitz continuous, 
(b): there exist cf > 0 and kf > 0 such that 
|f (x'
, u) − f (x, u)| ≤ kf |x' − x| and |f (x, u)| ≤ cf ,
for all x'
, x ∈ Rn, t ∈ [S,T ], u ∈ U .
Let (x,¯ u)¯ be an extremal for (P7), in the sense that there exists a costate trajectory 
and cost multiplier (p, λ) /= (0, 0) satisfying the costate equation, Weierstrass 
condition and transversality conditions of Theorem 7.2.1 along (x,¯ u)¯ . Confirm the 
‘constancy of the maximized Hamiltonian’ condition: there exists c ∈ R such that 
H (x(t), p(t)) ¯ = c, for all t ∈ [S,T ] ,
where H (x, p) := max 
u∈U p · f (x, u). 
Remark 
The implications of this exercise is that, for smooth data, constancy of the 
maximized Hamiltonian is a consequence of the standard extremality conditions 
and therefore not an independent condition. 
Hint: Show first that H (x, p) is locally Lipschitz continuous and, consequently, t →
H (x(t), ¯ p(t)) is absolutely continuous, as a composition of a Lipschitz continuous 
function and an absolutely continuous function. Deduce that the function has zero 
derivative at appropriate points t ∈ (S, T ) comprising a set of full measure, taking 
as starting point the relations 
p(t + ϵ) · f (x(t ¯ + ϵ), u(t ¯ + ϵ)) − p(t) · f (x(t), ¯ u(t)) ¯
=H (x(t ¯ + ϵ), p(t + ϵ)) − H (x(t), p(t)) ¯
for all ϵ (positive or negative), sufficiently small in magnitude. 
7.2 Let x¯ be a minimizer for the calculus of variations problem involving higher 
order derivatives 
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
Minimize  T
S L(t, x(t), D(1)
x(t), . . . , D(N )x(t))dt
over x ∈ WN ,1([S,T ]; Rn) s.t.
(x, D(1)
x,... ,D(N−1)
x)(S) = d0 and
(x, D(1)
x,... ,D(N−1)
x)(T ) = d1 ,
the data for which comprise an integer N > 0, a function L : R2+N → R, and 
N-vectors d0 and d1. Here, D(k)x denotes the k’th derivative of x. Assume 
(a): L is a C2 function and the minimizer x¯ is CN+1,7.5 Exercises 325
(b): there exists kL ∈ L1(S, T ) such that 
|∇x0,...,xN−1L(t, x0,...,xN−1, D(N )x(t)) ¯ |(x0,...,xN−1)=(x(t),D ¯ (1))(x(t),...,D ¯ (N−1)x)(t) ¯ |
≤ kL(t), for a.e. t ∈ [S,T ].
Derive the following higher order Euler equation 
∇x0L(t, x(t), . . . , D ¯ (N )x(t)) ¯ − d
dt
	
∇x1L(t, x(t), . . . , D ¯ (N )x(t)) ¯


+
...(−1)
N dN
dtN
	
∇xN L(t, x(t), . . . , D ¯ (N )x(t)) ¯


= 0, a.e. t ∈ [S,T ] .
Hint: Reformulate the problem as one with block states x, D(1)
x, ...,D(N−1)
x
and in which D(N )x is interpreted as a control variable. Show that this problem 
is normal (cost multiplier must be non-zero) and apply the maximum principle. This 
yields absolutely continuous functions (p0,...,pN−1) such that condition. 
(p˙0, p˙1 + p0, p˙2 + p1,..., p˙N−2 + pN−3, p˙N−1 + pN−2)(t)
= ∇x0,...,xN−1L(t, x(t), D ¯ (1)
x(t), . . . , D ¯ (N )x(t)), ¯ for a.e. t ∈ [S,T ].
Eliminate p0,...,pN−2 by differentiating across these equations. Combine the 
resulting equation for pN−1 with the Weierstrass condition to obtain the higher 
order Euler equation. 
7.3 (A Multiprocess Maximum Principle) Consider a collection of control sys￾tems: for i = 1,...,M
(Si)

x˙i(t) = fi(xi(t), ui(t)) a.e. t ∈ [si, ti],
ui(t) ∈ Ui, a.e. t ∈ [si, ti].
Here, for each i, fi : Rni × Rmi → Rni is a given function and Ui ⊂ Rmi is a given 
subset. Now consider the dynamic optimization problem 
(P7)
⎧
⎪⎪⎨
⎪⎪⎩
Minimize g({(si, xi(si), ti, xi(ti))}M
i=1)
subject to
([si, ti], xi, ui) is a (free end-time) process for (Si), i = 1,... ,M,
{(si, xi(si), ti, xi(ti))}M
i=1 ∈ C ,326 7 The Maximum Principle
in which g : ΠM
i=1(R × Rni × R × Rni) → R is a given locally Lipschitz function 
and C ⊂ ΠM
i=1(R × Rni × R × Rni) is a given closed set. 
Let {([¯si, t
¯i], x¯i, u¯i)}M
i=1 be a minimizing family of processes for this problem. 
Assume that the data for each system (Si), in relation to (x¯i, u¯i) satisfy the 
hypotheses of the free end-time nonsmooth maximum principle (Theorem 9.6.1). 
Assume furthermore, t
¯i > s¯i for each i. Derive the following maximum principle 
for ‘optimal multiple processes’: there exist λ ≥ 0 and {pi : [¯si, t
¯i] → Rni} such 
that 
(i): λ + M
i=1 ||p||L∞(s¯i, t
¯i) /= 0,
(ii): − ˙pi(t) ∈ co ∂x pi(t) · fi(x¯i(t), u¯i(t)), a.e. t ∈ [¯si, t
¯i], i = 1,...,M, 
(iii): pi(t) · ˙
x¯i(t) = max 
u∈Ui
pi(t) · fi(x¯i(t), u¯i(t)), a.e. t ∈ [¯si, t
¯i],
(iv): {(−hi, pi(s¯i), hi, −pi(t
¯i))} ∈ λ∂g({(s¯i, x¯i(s¯i), t
¯i, x¯i(t
¯i))})
+ NC({(s¯i, x¯i(s¯i), t
¯i, x¯i(t
¯i))}), 
in which hi ∈ R, for each i, is such that 
hi = sup
u∈Ui
pi(t) · fi(x¯i(t), u), a.e. t ∈ [¯si, t
¯i].
Hint: Consider the standard dynamic optimization problem in which the state 
and control variables are ((y1, τi), . . . ,(yM, τM)) and ((v1, w1), . . . , (vM, wM))
respectively, and the time domain is [0, 1]: 
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize g({(τi(0), yi(0), τi(1), yi(1))}M
i=1)
subject to:
(τ˙i(s), y˙i(s))=(t
¯i−¯si)wi(s)(1, fi(yi(s), vi(s))) a.e. s ∈ [0, 1], for i =1,..., M,
(v1,...,vM)(s) ∈ U1 × ... × UM, a.e. s ∈ [0, 1],
(w1,...,wM)(s) ∈ [0.5, 1.5] × ... × [0.5, 1.5], a.e. s ∈ [0, 1],
{(τi(0), yi(0), τi(1), yi(1))}M
i=1 ∈ C .
By considering changes of variable s → τi(0)+(t
¯i − ¯si)
 s
0 wi(s'
)ds'
, i = 1,...,M
(cf. Sect. 9.6), show that ({(y¯i, τ¯i),(v¯i), w¯i}) is a minimizer for this problem. Here, 
for each i, 
τ¯i(s) := ¯si + (t
¯i − ¯si)s, y¯i(s) := ¯xi(s¯i + (t
¯i − ¯si)s), v¯i(s)
:= ¯ui(s¯i + (t
¯i − ¯si)s)) and w¯i(s) ≡ 1.
Apply the maximum principle (Theorem 7.2.1) and interpret the resulting relations 
in terms of the data for the original problem. 
7.4 (Clarke’s Nonsmooth Maximum Principle for Boundary Points of Reach￾able Sets) Consider the control system 
(S) 
x(t) ˙ = f (t, x(t), u(t)) a.e. t ∈ [S,T ],
u(t) ∈ U (t), a.e. t ∈ [S,T ].7.6 Notes for Chapter 7 327
Let ψ : Rn × Rn → Rk. Define the endpoints reachable set, relative to ψ, to be 
Rψ := {ψ(x(S), x(T )) : (x, u) is a process for (S)}.
(A process is a pair comprising an absolutely continuous x and a measurable u
satisfying the constraints of (S)). Let (x,¯ u)¯ be a process such that 
ψ(x(S), ¯ x(T )) ¯ ⊂ ∂Rψ ,
where ∂Rψ denotes ‘boundary of Rψ ’. Assume that ψ is C1 on a neighbourhood 
of (x(S), ¯ x(T ¯ )). Assume also that, for some kf ∈ L1(S, T ) and ϵ > 0, 
(a): f (., x, .) is L × Bm measurable for each x ∈ Rn and {(t, u) ∈ [S, T ] × Rm :
u ∈ U (t)} is a L × Bm measurable set, 
(b): |f (t, x, u)−f (t, x'
, u)| ≤ kf (t)|x−x'
| for all x, x' ∈ ¯x(t)+ϵB and u ∈ U (t), 
a.e. t ∈ [S, T ]. 
Show that there exists p ∈ W1,1([S, T ]; Rn) such that 
(i): − ˙p(t) ∈ co ∂xp(t) · f (t, x(t), ¯ u(t)), ¯ a.e. t ∈ [S, T ], 
(ii): p(t) · f (t, x(t), ¯ u(t)) ¯ = max 
u∈U (t)
	
p(t) · f (t, x(t), ¯ u)

a.e. t ∈ [S, T ], 
(iii): (p(S), −p(T )) = ηDψ(x(S), ¯ x(T ¯ )), for some η ∈ Rk such that |η| = 1. 
Here, Dψ(x0, x1) denotes the Jacobian of ψ at (x0, x1). 
Hint: Since ψ(x(S), ¯ x(T ¯ )) is a boundary point, there exists a sequence of points 
{zi} in Rk \ Rφ such that zi → ψ(x(S), ¯ x(T ¯ )), as i → ∞. Then (x,¯ u)¯ is an αi
minimizer for 

Minimize |zi − ψ(x(S), x(T ))|
over processes (x, u) for (S),
for some αi ↓ 0. Now apply Ekeland’s theorem and Clarke’s nonsmooth maximum 
principle, and pass to the limit as i → ∞. 
7.6 Notes for Chapter 7 
The maximum principle came to prominence through the book [167], co-authored 
by Pontryagin, Boltyanskii, Gamkrelidze and Mischenko, published in Russian in 
1961, and in English translation in 1962. It is also referred to as the Pontryagin 
maximum principle because of Pontryagin’s role as leader of the research group 
at the Steklov Institute, Moscow, which achieved this advance. The first proof is 
attributed to Boltyanskii [42], however. There is a voluminous Russian literature 
on the original maximum principle and extensions, which we make no attempt to 
summarize here. We refer to Milyutin and Osmolovskii’s book [153], which covers 
the important contributions of Dubovitskii/Milyutin and their collaborators.328 7 The Maximum Principle
Prominent among researchers in the West who first entered this field were 
Neustadt, Warga and Halkin. (See the monographs [163, 164] and [204] and the 
paper [121].) These authors had their distinctive points of view—Neustadt’s and 
Halkin’s approach to deriving necessary conditions were close in spirit to that of 
Dubovitskii/Milyutin and Gamkrelidze, while Warga worked more in a Western 
tradition of variational analysis associated with L. C. Young and McShane. They all 
aimed, however, to axiomatize the proof of Pontryagin, Boltyanskii, Gamkrelidze 
and Mischenko [167]. The key idea is to show that if a (suitably chosen) convex 
approximation to some generalized ‘target set’ intersects the interior of a convex 
approximation to the reachable set at some control u∗, then the reachable set itself 
intersects the interior of the target set. This furnishes a contrapositive proof of 
the maximum principle because the first property is equivalent to ‘the maximum 
principle conditions are not satisfied at u∗’, while the second property implies 
‘u∗ is not a minimizer’. We regard this as a ‘dual’ approach—the assertion ‘the 
maximum principle conditions are not satisfied’ is essentially a statement about 
the non-existence of a hyperplane separating the convex approximations to the 
target and reachable sets. It is ‘axiomatic’ to the extent that attention focuses 
on abstract conditions on convex approximations of sets, consistent with these 
relations. The elegant mixed Lagrange multiplier rules of Ioffe and Tihomirov [132], 
yielding necessary conditions in dynamic optimization as direct corollaries, were 
also grounded in this idea. 
The challenge of deriving versions of the maximum principle covering problems 
with nonsmooth data was taken up in the 1970s. A landmark advance was 
Clarke’s nonsmooth maximum principle [59], treating fully nonsmooth problems 
with general endpoint constraints, published in 1976. (An earlier version, dealing 
with the ‘free endpoint’ case, earlier appeared in Clarke’s 1973 PhD thesis.) The 
techniques used by Clarke to prove his nonsmooth maximum principle [59] had a 
decidedly ‘primal’ flavour and were a marked departure from the methodologies 
of Dobovitski/Milyutin, Neustadt, Halkin and Warga. Clarke’s proof built on the 
fact that simple, variational arguments can be used to derive necessary conditions 
of optimality for problems with no right end-point constraints. He dealt with the 
troublesome right end-point constraint by finding a neighbouring process which 
is a minimizer for a perturbed optimization problem with free right end-point. 
Necessary conditions for the perturbed, free right end-point problem are invoked. 
(The perturbed problems needed to be constructed in a subtle manner, usually with 
the help of a variational principle, to ensure that the minimizing controls for the 
perturbed problems converge to the nominal control. Naive penalty methods will 
not do the trick.) Necessary conditions are then obtained for the original problem 
by passage to the limit. This general approach, deriving necessary conditions for 
a ‘difficult’ dynamic optimization problem via necessary conditions for a simpler 
perturbed problem and passing to the limit, is a very powerful one and adapts to 
other contexts; it has been used, for example, to obtain necessary conditions for 
dynamic optimization problems with impulsive control [181, 197], time delays [41], 
infinite dimensional state spaces [100, 140].7.6 Notes for Chapter 7 329
On the other hand, Warga extended the earlier dual approach to prove, indepen￾dently, another kind of nonsmooth maximum principle for dynamic optimization 
problems with data Lipschitz continuous with respect to the state variable x
[205, 207]. The role of gradients with respect to x in the co-state equation and 
transversality condition is here taken by another kind of ‘generalized’ derivative 
namely a derivate container. Because the dynamic optimization literature makes 
reference to this construct, we note here the definition: 
Definition Take a neighbourhood O of a point x¯ ∈ Rn and a Lipschitz continuous 
mapping Λ : O → Rm. A compact set L of linear maps from Rn to Rm is said to 
be a derivate container of Λ at x¯ if, for every ϵ > 0, there exists a neighbourhood 
Oϵ of x¯ and a sequence {Λj : Oϵ → Rm} of C1 maps such that 
(i): ∇Λj (x) ∈ L + ϵB for all j and x ∈ Oϵ
(ii): Λj (x) → Λ as j → ∞, uniformly over x ∈ Oϵ . 
The choice of derivate container can be tailored to a particular application. In 
some cases, this choice can be exercised to give a different, and more precise, 
transversality condition than that derived in this chapter. However for technical 
reasons to do with the fact that the convex hull of a derivate container to a Lipschitz 
continuous function contains no more information than the generalized Jacobian 
[206], the co-state inclusion is essentially the same. The dual approach also yielded 
a nonsmooth maximum principle [124], via a nonsmooth generalization of Ioffe and 
Tihomirov’s earlier mixed multiplier rule. 
Distinctive features of Clarke’s nonsmooth maximum principle are, first, the 
transparent link to the original optimality condition (if the date is smooth, we 
replace limiting subdifferentials by Fréchet derivatives and immediately recover the 
classical maximum principle). This link, while present of course, is less obvious 
in nonsmooth maximum principles of Warga, where reduction to the smooth case 
involves choice of derivate container and interpretation of an abstract multiplier 
rule. Second, the Clarke’s optimality condition is exceptional for the unrestrictive 
nature of the hypotheses that are invoked. This is true particularly in relation 
to the integrably bounded Lipschitz dependence of velocity function w.r.t. its 
x dependence. Clarke requires f (t, ., u) to be locally Lipschitz continuous on 
a neighbourhood of x(t) ¯ (the nominal state trajectory), but then hypothesizes 
integrability only of the function t → k(t, u(t)) ¯ (i.e. the Lipschitz bound evaluated 
along the nominal control u(t)) ¯ . This is a very weak Lipschitz continuity hypothesis 
for derivation of necessary conditions in dynamic optimization indeed and, for that 
reason, is widely employed to this day. 
The dual approach to proving the maximum principle has been revisited by 
Sussmann [185], using constructs related to Warga’s derivate containers. Let (x,¯ u)¯
be an optimal process. Scrutiny of the maximum principle conditions reveals that the 
co-state equation makes sense when we assume that the right side of the differential 
equation modelling the dynamics is ‘differentiable’ with respect to the state in some 
sense merely along the optimal control function. Sussman provides a version of 
maximum principle incorporating the Lojasiewicz refinement, namely a version of330 7 The Maximum Principle
the maximum principle in which this weaker differentiability hypothesis replaces 
the usual ones involving all control functions. This development was prefigured 
in earlier multiplier rules of Halkin [122] and Ioffe [126], who merely invoked 
differentiability of the data at (rather than near) the minimizer under consideration. 
Primal methods yield a simple, alternative proof of the Lojasiewicz refinement for 
free right end-point problems (and refinements), though not for general fixed end￾point problems. 
The price of achieving the Lojasiewicz refinement for fixed end-point dynamic 
optimization problems would appear to be a hypothesis requiring the solutions of 
the state equation to be unique for an arbitrary initial condition and control. The 
Lojasiewicz refinement tells us then that the maximum principle remains valid 
when a Lipschitz continuity hypothesis, resembling standard sufficient conditions 
for uniqueness of solutions to the state equation (for an arbitrary control and 
initial state), is replaced by the hypothesis of uniqueness of solutions to the state 
equation itself. This suggests that the uniqueness condition is in some sense more 
fundamental to the derivation of necessary conditions in dynamic optimization 
than Lipschitz continuity. On the other hand, examples of dynamic optimization 
problems can be constructed whose data satisfy the Lipschitz continuity hypotheses 
for derivation of Clarke’s nonsmooth maximum principle (these resemble, but 
are weaker than standard sufficient conditions for uniqueness), yet for which 
the uniqueness hypothesis is violated. The question of what hypothesis is the 
more ‘fundamental’ does not therefore have a simple answer. A counter-example 
provided by Bressan [46] points to fundamental limitations to the derivation by 
dual/set separation methods of versions of the maximum principle derived by primal 
methods, incorporating transversality conditions (expressed in terms of limiting 
normal cones to the end-point constraint set) and therefore, also, on the scope for 
unifying the two approaches [186]. 
In this chapter, the primal techniques pioneered by Clarke are used, once again, 
to proof his nonsmooth maximum principle. We first prove the necessary conditions 
in a special case in which the data are smooth and there are no end-point constraints, 
using elementary needle variation techniques. We extend these conditions to allow 
for nonsmooth data (but still no endpoint constraints). This is done by approximating 
the problem by a smooth problem, using inf convolution to regularize the cost (as 
in [67]) and a decoupling technique to replace the state variable in the dynamic 
constraint by a control-like variable, which eliminates the need to deal with 
nonsmoothness of the right side of the controlled differential equation (a decoupling 
technique also used in [67] and earlier in [200]); we recover necessary conditions by 
applying Ekeland’s theorem and passage to the limit. Finally we derive optimality 
conditions for the nonsmooth problem with end-point constraints, approximating 
it by a non-smooth problem with no end-point constraints, which we can now 
deal with, and making a second application of Ekeland’s theorem. This proof 
strategy is simpler and more direct than those employed Clarke’s paper and the 
subsequent monographs [65, 68] or [194], all of which arrive at the desired necessary 
conditions, by first of all proving necessary conditions (in the form of either Clarke’s 
Hamiltonian inclusion or the generalized Euler Lagrange inclusion) for a relaxed7.6 Notes for Chapter 7 331
version of original dynamic constraint formulated as a differential inclusion. By 
contrast, our approach avoids excursions into the theory of necessary conditions for 
differential inclusion problems altogether. It does not involve relaxation procedures 
or the rather complicated reduction procedures (including approximating velocity 
sets by sets containing a finite number of points) earlier employed. The underlying 
simplicity of the approach is evident from the way it is used to prove the classical 
maximum principle in the Appendix of Chap. 1.Chapter 8 
The Generalized Euler-Lagrange and 
Hamiltonian Inclusion Conditions 
Abstract The subject matter of this chapter is necessary conditions of optimality 
for differential inclusion problems, by which we mean dynamic optimization prob￾lems in which the dynamic constraint takes the form of a differential inclusion. We 
provide two sets of conditions. These are the generalized Euler Lagrange condition 
and a later refinement of a set of conditions known as Clarke’s Hamiltonian 
inclusion. As their names imply, these conditions result from reformulating the 
differential inclusion problem as a generalized Bolza problem in the calculus of vari￾ations, in which indicator functions taking account of the dynamic constraint appear 
in an extended valued Lagrangian, and deriving analogues of the similarly named 
classical conditions, valid in this more general setting. The conditions, expressed 
in terms of limiting subgradients and limiting normal cones, are inherently non￾smooth. 
We bring fully up to date the generalized Euler Lagrange condition, which has 
been significantly improved, since the time it was first derived in the 1980s under 
a convexity hypothesis on velocity sets. The generalized Euler Lagrange condition 
covered in this chapter, for problems with non-convex, unbounded velocity sets, 
builds on Clarke’s stratified conditions. It also incorporates a recent improvement in 
the Weierstrass condition, which we refer to as the Ioffe refinement. 
The first necessary condition to be proved for differential inclusion problems with 
convex velocity sets, was Clarke’s Hamiltonian inclusion. This chapter includes 
a subsequent refinement of this condition, the so-called partially convexified 
Hamiltonian inclusion. It is shown, by means of a duality theorem linking Euler 
Lagrange and Hamiltonian inclusions, that the partially convexified Hamiltonian 
inclusion is not an independent condition, but is in fact a consequence of the 
generalized Euler Lagrange condition. 
Questions regarding validity of the Hamiltonian inclusion, for differential inclu￾sion problems with possibly non-convex velocity sets, was the subject of speculation 
for many years. Current understanding of these issues is conveyed in this chapter. 
Clarke’s (fully convexified) Hamiltonian inclusion for problems with non-convex 
velocity sets is now known to hold, at local minimizers w.r.t. certain topologies. But 
the condition may not be valid for local minimizers w.r.t. other topologies. 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
P. Bettiol, R. B. Vinter, Principles of Dynamic Optimization, Springer Monographs 
in Mathematics, https://doi.org/10.1007/978-3-031-50089-3_8
333334 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
8.1 Introduction 
The distinguishing feature of dynamic optimization problems, as compared with 
traditional variational problems, is the presence of constraints on the velocity 
variable. We have seen in the previous chapter that necessary conditions in the 
form of a maximum principle can be derived when these constraints are formulated 
in terms of a controlled differential equation governing the evolution of the state 
variable: 
x(t) ˙ = f (t, x(t), u(t)) a.e., (8.1.1) 
u(t) ∈ U (t) a.e.. (8.1.2) 
Another approach to the derivation of necessary conditions in dynamic optimization 
is to focus attention on the constraints on the velocity variable implicit in the 
underlying dynamics of the problem. The choice variables are now taken to be arcs 
satisfying a differential inclusion 
x(t) ˙ ∈ F (t, x(t)) a.e..
This is a broader framework for studying dynamic optimization problems, since 
constraints described by a controlled differential equation (8.1.1) and (8.1.2) can be 
reformulated as a differential inclusion constraint by choosing 
F (t, x) := {f (t, x, u) : u ∈ U (t)}. (8.1.3) 
Our goal in this chapter is to derive general necessary conditions for a dynamic 
optimization problem in which the dynamic constraint takes the form, no longer of 
a controlled differential equation, but of a differential inclusion: 
(P )
⎧
⎪⎪⎨
⎪⎪⎩
Minimize g(x(S), x(T ))
over arcs x ∈ W1,1([S,T ]; Rn) satisfying
x(t) ˙ ∈ F (t, x(t)) a.e.,
(x(S), x(T )) ∈ C.
Here, [S,T ] is a given interval, g : Rn × Rn → R is a given function, F : [S,T ] ×
Rn ⇝ Rn is a given multifunction and C ⊂ Rn × Rn is a given set. 
Problem (P) can be regarded as a generalization of traditional variational 
problems (in one independent variable) to allow for nonsmooth data. This is because 
(P) can be expressed as 
(P )'
⎧
⎨
⎩
Minimize g(x(S), x(T )) +  T
S L(s, x(s), x(s))ds ˙
over arcs x ∈ W1,1([S,T ]; Rn) satisfying
(x(S), x(T )) ∈ C,8.1 Introduction 335
when we choose the cost integrand L to be 
L(t, x, v) := ΨGrF (t,.)(x, v). (8.1.4) 
(ΨG denotes, as usual, the indicator function of the set G that takes value 0 or +∞
at a point z in its domain, depending on whether z lies in G or its complement, 
respectively.) 
This is an equivalent formulation, in so far as the values of the cost of the new 
problem and of the one it replaces are the same for an arbitrary arc x ∈ W1,1
satisfying 
x(t) ˙ ∈ F (t, x(t)) a.e..
If, on the other hand, an arc x is not admissible for the original problem, i.e. 
x(t) / ˙ ∈ F (t, x(t))
for all t in a subset of positive measure, then the value of the cost for the new 
problem is infinite and so the arc x is effectively excluded from consideration as a 
minimizer. 
From this perspective, the maximum principle, which concerns an inherently 
nonsmooth problem, was a remarkable achievement. Pontryagin et al. circumvented 
the need to examine generalized derivatives of the nonsmooth Lagrangian or 
Hamiltonian, a general theory for which did not exist at the time, by structuring 
their necessary conditions around the smooth ‘unmaximized’ Hamiltonian function. 
By contrast, the necessary conditions now available for dynamic optimization 
problems with dynamics described by a differential inclusion rely heavily on 
nonsmooth analysis, regarding not merely the proof techniques employed to derive 
them, but their very statement. Indeed, since the 1980s, research in dynamic 
optimization and nonsmooth analysis has proceeded hand in hand. 
A revealing perspective on necessary conditions for dynamic optimization 
problems involving differential inclusions is to regard them as generalizations of 
conditions from the classical calculus of variations. If, in (P )'
, L were smooth and 
suitably regular, g smooth and C expressible in terms of smooth ‘nondegenerate’ 
functional inequality constraints, classical variational techniques would supply the 
following necessary conditions for x¯ to be a minimizer: there would exist p ∈
W1,1([S,T ]; Rn) such that 
(p(t), p(t)) ˙ = ∇x,vL(t, x(t), ¯ ˙
x(t)) ¯ a.e. t ∈ [S,T ], (8.1.5) 
p(t) · ˙
x(t) ¯ − L(t, x(t), ¯ ˙
x(t)) ¯
≥ p(t) · v − L(t, x(t), v) ¯ for all v ∈ Rn, a.e. t ∈ [S,T ] (8.1.6)336 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
and 
(p(S), −p(T )) ∈ ∇g(x(S), ¯ x(T )) ¯ + NC(x(S), ¯ x(T )). ¯ (8.1.7) 
Of course in the present context L, a characteristic function, is certainly not 
smooth. Nevertheless, under unrestrictive hypotheses on the data for problem (P), 
the following analogues of the above conditions can be derived: there exist λ ≥ 0
and p ∈ W1,1([S,T ]; Rn), not both zero, such that 
p(t) ˙ ∈ co{ξ : (ξ , p(t)) ∈ NGr F (t,.)(x(t), ¯ ˙
x(t)) ¯ } a.e. , (8.1.8) 
p(t) · ˙
x(t) ¯ ≥ p(t) · v for all v ∈ F (t, x(t)) ¯ a.e. (8.1.9) 
and 
(p(S), −p(T )) ∈ λ∇g(x(S), ¯ x(T )) ¯ + NC(x(S), ¯ x(T )). ¯ (8.1.10) 
These three conditions will be referred to collectively as the generalized Euler 
Lagrange condition because of the close affinity of (8.1.8) with the classical Euler 
Lagrange condition. (Condition (8.1.8) alone will sometimes be called the Euler 
Lagrange inclusion.) The relation between (8.1.8) and the classical condition is 
evident from the fact that, if L(t, ., .) is the indicator function of the closed set 
Gr F (t, .) and (x, v) ∈ Gr F (t, .), we have 
∂x,vL(t, x, v) = NGr F (t,.)(x, v).
Condition (8.1.8) may be interpreted then as a ‘partially convexified’, nonsmooth 
version of (8.1.5). Equation (8.1.10) differs from (8.1.7) only by the presence of 
a cost multiplier λ ≥ 0 (to take account of the possibility of certain kinds of 
degeneracy for variational problems involving end-point and velocity constraints). 
Finally (8.1.9) is simply another way of writing (8.1.6) when L(t, ., .) is interpreted 
as the indicator function of Gr F (t, .). 
Another point of departure from classical optimality conditions in the calculus of 
variations, for deriving necessary conditions of optimality in dynamic optimization 
when the dynamic constraint takes the form of a differential inclusion, is Hamilton’s 
system of equations: a minimizing state trajectory x¯ and associated costate trajectory 
p satisfy 
(− ˙p(t), ˙
x(t)) ¯ = ∇x,pH (t, x(t), p(t)) ¯ a.e..
Here H, which we refer to as the ‘maximized Hamiltonian’, is defined to be 
H (t, x, p) := p · χ (t, x, p) − L(t, x, χ (t, x, p))8.1 Introduction 337
in which χ (t, x, p) is a stationary point of v → p · v − L(t, x, v), i.e. 

p · v − ∇vL(t, x, v)	
	
	
v=χ (t,x,p) = 0.
(It is assumed the stationary point χ (t, x, p) exists and is unique and χ (t, ., .) is 
continuously differentiable.) This construction of H (t, x, .) from L(t, x, .) is known 
as the Legendre-Fenchel transformation. 
Since stationary points of v → p · v −L(t, x, v) are maximizers, when L(t, x, .)
is a convex, H can be equivalently defined as the Legendre-Fenchel transformation 
of L(t, x, .), namely 
H (t, x, p) = sup
v∈Rn
{p · v − L(t, x, v)}, (8.1.11) 
in the case that L(t, x, .) is a convex function. Even if L(t, x, .) fails to be 
convex, (8.1.11) provides a convenient definition of the maximized Hamilto￾nian, because it makes sense when the onerous conditions (concerning existence, 
uniqueness and continuous dependence of stationary points) for construction of 
the Legendre-Fenchel transformation are dropped and, more significantly, because 
this definition provides a basis for the derivation of optimality conditions for a 
broad class of dynamic optimization problems. In the case L(t, ., .) is the indicator 
function of Gr F (t, .), H (t, x, p) is 
H (t, x, p) = sup
v∈F (t,x)


p · v − L(t, x, v)
. (8.1.12) 
Taking (8.1.12) as definition of H, we might seek necessary conditions of optimality 
for (P ) that include the relation 
(− ˙p(t), ˙
x(t)) ¯ ∈ co ∂x,pH (t, x(t), p(t)) ¯ a.e. , (8.1.13) 
satisfied by the minimizing state trajectory and associated costate trajectory. Here, 
we acknowledge that H (t, ., .) may fail to be differentiable and consequently we 
employ, in place of the derivative, the set valued subdifferential co ∂x,pH. We would 
expect the Clarke generalized gradient (i.e. the convexified limiting subdifferential) 
to be involved, to ensure robustness of the solution of (8.1.13) under perturbations 
and limit taking. In the spirit of the generalized Euler Lagrange condition, we might 
hope to replace (8.1.13) by a refined, ‘partially convexified’ condition 
− ˙p(t) ∈ co {ξ ∈ Rn : (ξ , p(t)) ∈ ∂x,pH (t, x(t), p(t)) ¯ } a.e.. (8.1.14) 
The goal of this chapter is to derive necessary conditions of optimality for 
problem (P ), whose dynamic constraint is formulated as a differential inclusion. 
We provide two sets of conditions. The first includes the Euler Lagrange inclu￾sion (8.1.13). The second, in which (8.1.14) supplements (8.1.13), will be called the338 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
Hamiltonian inclusion. For dynamic optimization problems arising in engineering 
design, the dynamic constraint usually takes the form of a differential equation 
parameterized by a control function (8.1.1) and (8.1.2). We may think of the 
necessary conditions of this chapter, for problems involving differential inclusions, 
as ‘intrinsic’ conditions because they depend only on the set of admissible velocities 
to which the parameterized differential equation gives rise via (8.1.3). The question 
therefore needs to be addressed whether the intrinsic necessary conditions have 
anything to offer over the maximum principle which is, after all, specially tailored 
to these kinds of dynamic constraints. While it is true that the maximum principle 
is often more convenient to apply, the intrinsic conditions have the significant 
advantage that they cover certain dynamic optimization problems formulated in 
terms of a differential equation parameterized by control functions, in cases when 
the control constraint set U is state and time dependent, provided of course that the 
multifunction 
F (t, x) = {f (t, x, u) : u ∈ U (t, x)}
satisfies the hypotheses under which the intrinsic conditions are valid. By contrast, 
the classical maximum principle does not apply to problems with state dependent 
control constraint sets. It is true that there is an extensive literature on extensions 
of the maximum principle to allow for ‘mixed constraints’ (pathwise constraints 
involving both state and control variables). But, as we shall see in Sect. 10.8 of 
this book, necessary conditions for differential inclusion problems have provided 
powerful analytical tools for their derivation, under unrestrictive hypotheses. 
Even when the control constraint set is not state dependent, the maximum 
principle and the intrinsic conditions are distinct sets of necessary conditions 
which we can apply to gain information about optimal controls. Examples can 
be constructed where the maximum principle excludes candidates for being a 
minimizer when the intrinsic conditions fail to do so, and vice versa. 
8.2 Pseudo Lipschitz Continuity 
In this section we introduce two continuity concepts concerning multifunctions � :
Rn ⇝ Rm. These have special relevance to dynamic optimization problems in which 
the dynamic constraint takes the form of a differential inclusion x˙ ∈ F (t, x), when �
has the interpretation ‘� = F (t, .)’ (for fixed t) and we aim to formulate appropriate 
Lipschitz continuity hypotheses under which we can derive necessary conditions of 
optimality. 
Definition 8.2.1 (Pseudo-Lipschitz Continuity) Take a multifunction � : Rn ⇝
Rm and a point (x,¯ v)¯ ∈ Gr �. Take also numbers ϵ > 0, R > 0 and k ≥ 0. We say 
that � is pseudo-Lipschitz continuous near (x,¯ v)¯ (with parameters ϵ, R and k) if 
�(x'
) ∩ (v¯ + R
◦
B) ⊂ �(x) + k|x' − x|B for all x'
, x ∈ ¯x + ϵ B .8.2 Pseudo Lipschitz Continuity 339
Definition 8.2.2 (Bounded Slope Condition) Take a multifunction � : Rn ⇝ Rm
with closed graph G := Gr � and a point (x,¯ v)¯ ∈ G. Take also numbers ϵ > 0, 
R > 0 and k ≥ 0 . We say that � satisfies the bounded slope condition near (x,¯ v)¯
(with parameters ϵ, R and k), if 
(w, p) ∈ NP
G (x, v) =⇒ |w| ≤ k|p| , for all (x, v) ∈ (x,¯ v)¯ + ϵB × R
◦
B .
These definitions are more or less equivalent with the caveat that, passing from 
one to the other, we must modify the parameters ϵ and R involved. The following 
proposition, which makes precise this assertion, can be regarded as an analogue 
for multifunctions of the property of real valued functions, lower semi-continuous 
functions f on a finite dimensional linear space that are bounded below: such 
a function is Lipschitz continuous with Lipschitz constant at most k on some 
open neighbourhood of a base point z, if and only if its limiting subgradients are 
uniformly bounded by k on some neighbourhood of z. 
Proposition 8.2.3 Take a multifunction � : Rn ⇝ Rm with closed graph G :=
Gr �. Take (x,¯ v)¯ ∈ G and also ϵ > 0, R > 0 and k ≥ 0. 
(a): ‘ � is pseudo-Lipschitz continuous near (x,¯ v)¯ with parameters ϵ, R and k’ 
=⇒ ‘ � satisfies the bounded slope condition near (x,¯ v)¯ with parameters ϵ'
, 
R and k (as above)’, for any ϵ' ∈ (0, ϵ), 
(b): ‘ � satisfies the bounded slope condition near (x,¯ v)¯ with parameters ϵ, R and 
k’ =⇒ ‘ � is pseudo-Lipschitz continuous near (x,¯ v)¯ with parameters Rη
3k ∧
ϵ'
, (1 − η)R and k’, for any ϵ' ∈ (0, ϵ) and η ∈ (0, 1). 
Proof 
(a): Suppose � is pseudo-Lipschitz continuous near (x,¯ v)¯ with parameters ϵ > 0, 
R > 0 and k ≥ 0. Choose any ϵ' ∈ (0, ϵ). Take (x, v) ∈ G∩

(x¯ +ϵ'
B)×(v¯+R
◦
B)

and (w, p) ∈ NP
G (x, v). We must show that |w| ≤ k|p|. With this goal in mind, 
take δi ↓ 0 and, for each i, write xi = x + δiw. Then, for i sufficiently large, 
xi ∈ ¯x + ϵ B . Invoking the pseudo-Lipschitz continuity hypothesis, we can find, for 
each i sufficiently large, a point vi ∈ �(xi) such that |vi − v| ≤ k|xi − x| = kδi|w|. 
It follows from the proximal normal inequality that there exists M > 0 such that, 
for i sufficiently large, 
w · (xi − x) + p · (vi − v) ≤ M(|xi − x|
2 + |vi − v|
2) .
But then 
δi|w|
2 ≤ δik|p| |w| + M(1 + k2)|w|
2δ2
i .
Dividing across by δi and passing to the limit yields |w|
2 ≤ k|w||p|, which implies 
|w| ≤ k|p|. Assertion (a) has been verified.340 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
(b): Assume that � has the bounded slope property near (x,¯ v)¯ with parameters 
R, ϵ and k. Take any ϵ' ∈ (0, ϵ) and η ∈ (0, 1), x1, x2 ∈ ¯x +
 Rη
3k ∧ ϵ'

B and 
v1 ∈ �(x1) ∩ (v¯ + (1 − η)R ◦
B). Assertion (b) will follow immediately if we can 
verify 
Claim: There exists a point v2 ∈ �(x2) such that |v2 − v1| ≤ k|x1 − x2|. 
We apply the mean value inequality (Theorem 4.5) to the function f : Rn × Rn →
R ∪ {+∞}, set Y and base point (x1, v1), when 
f (x, v) := ΨG(x, v) and Y := {x2} × (v1 + k|x1 − x2|B).
Here ΨG is the indicator function of the set G (taking values 0 on G and +∞ on 
its complement). Suppose the claim is false. Then (v1 + k|x1 − x2|B) ∩ �(x2) = ∅. 
This implies that 
min v∈v1+k|x1−x2|B f (x2, v) − f (x1, v1) =+∞.
The mean value inequality now tells us that, for any real number r and ϵ >˜ 0, 
r ≤ w · (x2 − x1) + p · (v − v1), for all v ∈ v1 + k|x1 − x2|B . (8.2.1) 
Here, (w, p) is a point in ∂P f (z, e) and (z, e) is some point in Rn × Rn whose 
distance to co [{(x1, v1)} ∪ Y ] is not greater than ϵ˜. Notice, however, that 
co [Y ∪ {(x1, v1)}] ⊂ (co {x1, x2}) × (v1 + k|x1 − x2|B)
⊂ (x¯ + ϵ'
B) × (v¯ + ((1 − η) + η
3
)R ◦
B)) .
We can choose the positive number ϵ˜, independently of r, such that 
co [Y ∪ {(x1, v1)}] + ˜ϵB ⊂ (x¯ + ϵB) × (v¯ + R
◦
B) .
Observe that (z, e) is in the set where the bounded slope condition is satisfied. 
Since (w, p) ∈ ∂P f (z, e) ⊂ NP
G (z, e), it follows from our assumptions that |w| ≤
k|p|. So, from (8.2.1), 
r ≤ (|w| − k|p|)|x1 − x2| ≤ 0 .
This is not possible since r is an arbitrary positive number. This contradiction 
confirms the claim. ⨅⨆8.3 Unbounded Differential Inclusions 341
8.3 Unbounded Differential Inclusions 
Necessary conditions of optimality in the form of the maximum principle, applicable 
to dynamic optimization problems where the dynamic constraint takes the form of 
a controlled differential equation x(t) ˙ = f (t, x(t), u(t)), have been derived under 
the assumption that f (t, x, u) has certain Lipschitz continuity properties w.r.t. the 
x variable. Now that we are formulating the dynamic constraint as a differential 
inclusion x(t) ˙ ∈ F (t, x), some kind of hypothesis, of a Lipschitz continuity 
nature, must be imposed on F (t, x) regarding its x dependence, in order to derive 
necessary conditions in this new framework. What should this hypothesis be? An 
obvious choice, indeed one which was adopted in the early literature, was to require 
integrable Lipschitz continuity w.r.t. the Hausdorff metric, i.e., for a given reference 
trajectory x¯, 
There exist ϵ > 0 and k ∈ L1 such that 
dH (F (t, x), F (t, x'
)) ≤ k(t)|x − x'
| for all x, x' ∈ ¯x(t) + ϵB a.e..
Recall that the Hausdorff distance dH (A, B) between two closed non-empty subsets 
of Rn A and B is defined by (see (6.6.3)) 
dH (A, B) := max 
sup
a∈A
dB(a), sup
b∈B
dA(b)
.
An equivalent statement of the condition is: 
There exist ϵ > 0 and k ∈ L1 such that 
F (t, x'
) ⊂ F (t, x) + k(t)|x' − x|B for all x'
, x ∈ ¯x(t) + ϵB, a.e.. (8.3.1) 
For unbounded differential inclusions, i.e. in situations where the values of 
the multifunction F are, possibly, unbounded sets, condition (8.3.1) is usually 
overly restrictive. This is because, for two unbounded sets which are ‘close’ in an 
intuitive sense, the Hausdorff distance between them can be very large. Consider, 
for example, the multifunction F : R2 ⇝ R2 defined by 
F (x1, x2) := 

(v1, v2) ∈ R2 : v2 ≤ x1v1

. (8.3.2) 
Here, values of F are hypographs of linear functions whose slopes depend smoothly 
on the value of x = (x1, x2). A reasonable requirement of a set of necessary 
conditions for unbounded differential inclusions is that it should allow cases like 
this. Hypothesis (8.3.1) fails to meet this requirement, because 
dH (F (x'
), F (x)) = +∞ for x'
1 /= x1,342 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
and we are therefore compelled to seek less restrictive hypotheses. This was the 
motivation behind the imposition, in the later literature, of the less restrictive 
pseudo-Lipschitz continuity hypothesis (with integrable parameter k): 
There exists R > 0, ϵ > 0 and k ∈ L1 such that 
F (t, x'
) ∩ (x¯ + R ◦
B) ⊂ F (t, x) + k(t)|x' − x|B
for all x'
, x ∈ ¯x(t) + ϵB, a.e.. (8.3.3) 
Unfortunately, it is the case that pseudo-Lipschitz continuity does not permit the 
derivation of Euler Lagrange-type necessary conditions; either this hypothesis must 
be modified (replacing the parameter R by a time varying function, that we call 
a radius function that is coordinated, in some sense, with the integrable Lipschitz 
bound k) or the pseudo-Lipschitz continuity hypothesis must be supplemented by 
other hypotheses. The supplementary hypotheses, which are referred to as tempered 
growth conditions, basically require the distance of the set velocity F (t, x) from 
the nominal velocity ˙
x(t) ¯ to grow not too rapidly, as x deviates from the nominal 
velocity x¯. 
8.4 The Generalized Euler Lagrange Condition 
This section provides necessary conditions satisfied by W1,1 local minimizers, for 
the dynamic optimization problem (P ) of the introduction, namely: 
(P )
⎧
⎪⎪⎨
⎪⎪⎩
Minimize g(x(S), x(T ))
over arcs x ∈ W1,1([S,T ]; Rn) satisfying
x(t) ˙ ∈ F (t, x(t)) a.e.,
(x(S), x(T )) ∈ C.
Here [S,T ] is a given interval, g : Rn × Rn → R is a given function, F : [S,T ] ×
Rn ⇝ Rn is a given multifunction, and C ⊂ Rn × Rn is a given closed set. An 
admissible F trajectory x is an F trajectory that satisfies the constraints of (P ). 
The necessary conditions assert properties of an admissible arc x¯ that is a ‘local 
minimizer’ in a sense made precise by the following definition: 
Definition 8.4.1 For a specified multifunction B : [S,T ] ⇝ Rn such that B(t) is 
open for each t,a F trajectory x¯ is said to be a W1,1 local minimizer for (P ) relative 
to B if there exists β > 0 such that 
g(x(S), x(T )) ≥ g(x(S), ¯ x(T )) , ¯ (8.4.1) 
for all admissible trajectories such that x(t) ˙ ∈ ˙
x(t) ¯ + B(t), a.e. and 
||x − ¯x||W1,1 ≤ β. (8.4.2)8.4 The Generalized Euler Lagrange Condition 343
An admissible F trajectory such that, for some β > 0, (8.4.1) is true for all 
admissible F trajectories x satisfying (8.4.2) is called a W1,1 local minimizer for 
(P ). A W1,1 local minimizer can, of course, be interpreted as a W1,1 local minimizer 
relative to B, when B ≡ Rn. 
The centrepiece of the following necessary conditions is the Euler Lagrange 
inclusion. They also incorporate versions of the Weierstrass and transversality 
conditions. The Weierstrass condition employed here is rather subtle and merits 
prior discussion. Suppose initially that the multifunction B(t) ≡ Rn. The expected 
form of the Weierstrass condition is the relation 
p(t) · ˙
x(t) ¯ ≥ p(t) · e, for all e ∈ F (t, x(t)), ¯ a.e., (8.4.3) 
in which p is the costate arc. However the pseudo Lipschitz continuity hypoth￾esis (8.3.3), now expressed in terms of a possibly time-varying radius function 
R, places restrictions only on velocities e in F (t, x) ∩ (˙
x(t) ¯ + R(t) ◦
B) and, 
unsurprisingly, earlier efforts to weaken the restrictive hypothesis (8.3.3) focused on 
confirming the inequality in (8.4.3), only for velocities in the subset of F (t, x(t)) ¯ : 
F (t, x(t)) ¯ ∩ (˙
x(t) ¯ + R(t) ◦
B) . (8.4.4) 
(There are technical reasons, related to the fact that proof techniques for the 
necessary conditions involve replacing R(t)B by an inner approximation and 
passage to the limit, why we need to take the intersection in (8.4.4) with the open set ◦
B, not B itself.) But in fact we can do better than this and validate the Weierstrass 
condition over a larger set of velocities. For this purpose, we introduce the following 
definition, in which x¯ is a given F trajectory: 
Definition 8.4.2 The regular velocity set at time t is 
Ω0(t) := {e ∈ F (t, x(t)) ¯ : F (t, .) is pseudo-Lipschitz
continuous near (x(t), e) ¯ }. (8.4.5) 
Now suppose the nominal F trajectory x¯ is a W1,1 local minimizer relative to 
some multifunction B such that R(t) ◦
B ⊂ B(t) a.e.. The version of the necessary 
conditions below asserts that the Weierstrass condition is valid for all velocities e in 
co (Ω0(t) ∩ (˙
x(t) ¯ + B(t))) .
Notice that a version of the Weierstrass condition employing this set is an improve￾ment over one employing F (t, x(t)) ¯ ∩ (˙
x(t) ¯ + R(t) ◦
B). This is because, under 
the pseudo-Lipschitz continuity hypothesis (8.3.3), every point e ∈ F (t, x(t)) ¯ ∩
(˙
x(t) ¯ +R(t) ◦
B) lies in Ω0(t). From this fact and since R(t) ◦
B ⊂ B(t) we deduce that 
F (t, x(t)) ¯ ∩ (˙
x(t) ¯ + R(t) ◦
B) ⊂ Ω0(t) ∩ (˙
x(t) ¯ + B(t)).344 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
We remark that, for many problems of interest, classes of which we identify 
below, Ω0 actually coincides with F (t, x(t)) ¯ and, here, the Weierstrass condition 
can be expressed in terms of the entire velocity set F (t, x(t)) ¯ . 
Theorem 8.4.3 (The Generalized Euler Lagrange Condition) Take a measur￾able multifunction B : [S,T ] ⇝ Rn such that B(t) is open for a.e. t ∈ [S,T ]. 
Let x¯ be a W1,1 local minimizer for (P) relative to B. Assume that the following 
hypotheses are satisfied: 
(G1): g is Lipschitz continuous on a neighbourhood of (x(S), ¯ x(T )) ¯ and C is a 
closed set, 
(G2): F (t, x) is nonempty for each (t, x) ∈ [S,T ] × Rn, Gr F (t, .) is closed for 
each t ∈ [S,T ] and F is L × Bn measurable, 
(G3): there exist ϵ > 0 and a measurable function R : [S,T ] → (0,∞) ∪
{+∞} (a ‘radius function’) such that R(t) ◦
B ⊂ B(t) a.e. and the following 
conditions are satisfied: 
(a): (Pseudo-Lipschitz Continuity) There exists k ∈ L1(S, T ) such that 
F (t, x'
) ∩ (˙
x(t) ¯ + R(t) ◦
B) ⊂ F (t, x) + k(t)|x' − x|B,
for all x, x' ∈ ¯x(t) + ϵB, a.e. t ∈ [S,T ], (8.4.6) 
(b): (Tempered Growth) there exists r ∈ L1(S, T ), r0 > 0 and γ ∈ (0, 1) such 
that r0 ≤ r(t), γ −1r(t) ≤ R(t) a.e. and 
F (t, x) ∩ (˙
x(t) ¯ + r(t)B) = ∅ / for all x ∈ ¯x(t) + ϵB, a.e. t ∈ [S,T ]. 
Then there exist an arc p ∈ W1,1([S,T ]; Rn) and λ ≥ 0, satisfying the following 
conditions: 
(i): (p, λ) /= (0, 0), 
(ii): p(t) ˙ ∈ co{η : (η, p(t)) ∈ NGr F (t,.)(x(t), ¯ ˙
x(t)) ¯ } a.e. t ∈ [S,T ], 
(iii): (p(S), −p(T )) ∈ λ∂g(x(S), ¯ x(T )) ¯ + NC(x(S), ¯ x(T )) ¯ , 
(iv): p(t) · ˙
x(t) ¯ ≥ p(t) · v for all v ∈ co (Ω0(t) ∩ (˙
x(t) ¯ + B(t))) , a.e. t ∈ [S,T ]. 
Now assume, also, that B(t) and F (t, x) do not depend on t. Then, in addition to 
the above conditions, there exists a constant a such that 
(v): p(t) · ˙
x(t) ¯ = a , a.e. t ∈ [S,T ] . 
The theorem, a proof of which is given in Sect. 8.6 below, is noteworthy both for the 
unrestrictive nature of the hypotheses under which it is valid and also the precision 
of the costate inclusion (condition (ii)). We now elaborate on these points. 
Concerning these hypotheses, we have pointed out that these will unavoidably 
include a Lipschitz continuity assumption concerning the x dependence of the 
velocity set F (t, x) of some nature and, here, the pseudo-Lipschitz continuity 
hypothesis (G3)(a) is a natural choice, when the F takes values possibly unbounded 
sets. But in the statement of Theorem 8.4.3, the tempered growth hypothesis is also 
invoked. The presence of this supplementary hypothesis is, at first sight, surprising 
and might be thought to pose unnecessary limitations on the range of application of 
the theorem. We pause therefore to discuss its role.8.4 The Generalized Euler Lagrange Condition 345
Observe at the outset that the question of whether, or not, the tempered growth 
hypothesis is really required arises only when either the integrable Lipschitz bound 
k in the pseudo-Lipschitz continuity hypothesis (G3)(a) is an unbounded function or 
the radius function R is not strictly bounded away from 0. Indeed, in the case when 
k is essentially bounded above by a number k0 (we can always arrange, by adding a 
constant, that k0 > 0) and R is essentially bounded below by some positive number 
d > 0, (G3)(b) follows automatically from (G3)(a). (To verify (G3)(b), reduce the 
size of ϵ to arrange that k0ϵ<d, and choose r(t) ≡ k0ϵ and γ ∈ (0, 1) such that 
γ −1k0ϵ ≤ d.) 
When however k is not essentially bounded, the tempered growth hypothesis 
cannot be dropped. This is illustrated in the following example. 
Example 
⎧
⎪⎪⎨
⎪⎪⎩
Minimize − x2(1)
over arcs x ∈ W1,1([0, 1]; R2) satisfying
x(t) ˙ ∈ F (t, x(t)) a.e. t ∈ [0, 1]
x2(0) = 0,
in which 
F (t, x) := {(e1, e2) ∈ R2 | e1 = 0, e2 = k(t)x1}.
Here, k is any positive function in L1(0, 1) which is not essentially bounded. 
Take an arbitrary number R > 0. Then the F trajectory x¯ = (x¯1 = 0, x¯2 ≡ 0)) is 
a W1,1 local minimizer relative to the multifunction B ≡ R ◦
B. To see this, suppose 
to the contrary that there exists an admissible F trajectory x = (x1, x2) with lower 
cost and such that 
x(t) ˙ ∈ (0, 0) + R ◦
B a.e. t ∈ [0, 1]. (8.4.7) 
We have x1 > x¯1 = 0, whence x(t) ˙ = (0, x1k(t)) is not essentially bounded. This 
implies that, on a set of positive measure, x(t) / ˙ ∈ R ◦
B, in contradiction of (8.4.7). 
Note however that x¯ is not a W1,1 minimizer. 
Hypotheses (G1)–(G3)(a) of Theorem 8.4.3 are satisfied for the above choice of 
radius function. Observe, however, that (G3)(b) ‘tempered growth’ is violated. To 
see this, take any γ ∈ (0, 1), positive function r, strictly bounded away from 0 and 
such that γ −1r(t) ≤ R a.e., and (x1, x2) ∈ R2 such that x1 /= 0. Then for times t in 
a set of positive measure, we have 
F (t, x) ( = {0, x1k(t)}) /⊂ ˙
x(t) ¯ + γ −1rB .
Since this relation can be expressed F (t, x) ∩ (˙
x(t) ¯ + γ −1rB) = ∅, it is clear that 
(G3)(b) is not satisfied.346 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
Let us now examine the Euler Lagrange inclusion and transversality condition. 
They assert the existence of an arc p = (p1, p2) and λ ≥ 0, not both zero, satisfying: 
p˙2(t) = 0
p˙1(t) + p2(t)k(t) = 0 (8.4.8) 
and 
p1(0) = 0, p1(1) = 0 (8.4.9)
p2(1) = λ .
If λ = 0 then also p ≡ 0, a contradiction. If, on the other hand, λ > 0 then (8.4.8) 
implies 
p1(1) = −λ
 1
0
k(t) dt < 0
in contradiction of (8.4.9). This example tells us that, concerning those aspects of 
the theorem which deal with situations in which the nominal admissible F trajectory 
x¯ is merely a W1,1 minimizer relative to a given multifunction B, the assertions of 
the theorem (even those omitting the Weierstrass condition) are not true in general, if 
the tempered growth hypothesis is removed. But no examples have been identified, 
apparently, that test the need for the tempered growth hypothesis, when x¯ is a W1,1
local minimizer. 
Consider next the nature of differential inclusion (ii) which governs the costate 
trajectory p. This is a refined version, in which convexification is carried out with 
respect to just one variable, of the condition 
(p(t), p(t)) ˙ ∈ co NGr F (t,.)(x(t), ¯ ˙
x(t)), ¯ (8.4.10) 
in which convexification is carried out with respect to two variables. The Euler 
Lagrange inclusion (ii) is, of course, to be preferred because it provides more precise 
information about the costate trajectory. But it is in fact a significant improvement 
on (8.4.10) for the following reasons. As we shall see in Sect. 8.7, condition (ii) 
implies (for convex valued F’s) an alternative necessary condition, generalizing 
Hamilton’s system of equations. (The same cannot be said of condition (8.4.10).) 
Furthermore, the generalized Euler Lagrange condition for (P) has an important 
role as an analytical tool for the derivation of necessary conditions for dynamic 
optimization problems, of a nonstandard nature (problems involving free end-times, 
discontinuous state trajectories. etc.). Here it is usual practice to write down the 
necessary conditions for a suitable auxiliary dynamic optimization problem, which 
is a special case of (P), and to pass to the limit. Now the Euler Lagrange inclusion 
(ii), applied to the auxiliary problem, yields a costate trajectory p satisfying8.5 Special Cases 347
| ˙p(t)| ≤ k(t)|p(t)|
where k ∈ L1 is as in hypothesis (G3). This bound can be used to justify the use 
of weak compactness arguments to obtain a costate trajectory for the nonstandard 
problem when we pass to the limit. Condition (8.4.10) yields no such bound in 
general, curtailing its usefulness as an analytical tool. 
8.5 Special Cases 
In this section we demonstrate the unifying power of the ‘pseudo-Lipschitz con￾tinuity and tempered growth’ hypothesis (G3), in Theorem 8.4.3. We identify 
several sets of conditions, which have served, in past work, as hypotheses for more 
restrictive versions of the necessary conditions, that are now revealed as sufficient 
conditions for the validity of (G3). 
Proposition 8.5.1 Take multifunctions F : [S,T ] × Rn ⇝ Rn and B : [S,T ] ⇝
Rn. Let (G3) be the hypothesis of Theorem 8.4.3, that is: 
(G3): There exist ϵ > 0 and a measurable function R : [S,T ] → (0,∞)∪{+∞} (a 
‘radius function’) such that R(t) ◦
B ⊂ B(t) a.e. and the following conditions 
are satisfied, 
(a): (Pseudo-Lipschitz Continuity) There exists k ∈ L1(S, T ) such that 
F (t, x'
) ∩ (˙
x(t) ¯ + R(t) ◦
B) ⊂ F (t, x) + k(t)|x' − x|B,
for all x, x' ∈ ¯x(t) + ϵB, a.e. t ∈ [S,T ],
(b): (Tempered Growth) There exist r ∈ L1(S, T ), r0 > 0 and γ ∈ (0, 1) such 
that r0 ≤ r(t), γ −1r(t) ≤ R(t) a.e. and 
F (t, x) ∩ (˙
x(t) ¯ + r(t)B) /= ∅ for all x ∈ ¯x(t) + ϵB, a.e. t ∈ [S,T ]. 
Then (G3) is satisfied if any of the following hypotheses (G3)*, (G3)** or (G3)*** 
are satisfied. 
(G3)*: There exist ϵ > 0, k ∈ L1 and a measurable function R : [S,T ] →
(0,∞) ∪ {+∞} such that R(t) ◦
B ⊂ B(t) a.e., 
F (t, x'
) ∩ (˙
x(t) ¯ + R(t) ◦
B) ⊂ F (t, x) + k(t)|x' − x|B,
for all x, x' ∈ ¯x(t) + ϵB, a.e. (8.5.1) 
and either 
(a): k is strictly bounded away from 0 and there exists ω0 > 0 such that 
ω0k(t) ≤ R(t) a.e. t ∈ [S,T ] ,348 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
or 
(b): R is strictly bounded away form 0 and there exists a modulus of continuity 
θ such that 
dF (t,x)(˙
x(t)) ¯ ≤ θ (|x − ¯x(t)|) for all x ∈ ¯x(t) + ϵB, a.e. t ∈ [S,T ] .
(G3)∗∗: There exist ϵ > 0, k ∈ L1 (strictly bounded away from zero), a measurable 
function R : [S,T ] → (0,∞) ∪{∞} and ω0 > 0 such that ω0k(t) ≤ R(t)
and R(t) ◦
B ⊂ B(t), a.e. t ∈ [S,T ] and 
‘(w, p) ∈ NP
Gr F (t,.)(x, v)’ =⇒ ‘|w| ≤ k(t)|p|’,
for all v ∈ ˙
x(t) ¯ + R(t) ◦
B and x ∈ ¯x(t) + ϵ B, a.e. t ∈ [S,T ].
(G3)∗∗∗: B ≡ Rn. There exist ϵ > 0 , α > 0, non-negative measurable functions k
and β such that k and t → β(t)kα(t) are integrable on [S,T ] and, for 
each N ≥ 0, 
F (t, x'
) ∩ (˙
x(t) ¯ + NB) ⊂ F (t, x) + (k(t) + β(t)Nα)|x' − x|B,
for all x'
, x ∈ ¯x(t) + ϵB and a.e. t ∈ [S,T ]. 
The following corollary of Theorem 8.4.3 is an immediate consequence of this 
proposition. 
Corollary 8.5.2 The assertions of Theorem 8.4.3 remain valid when hypothesis 
(G3) of Theorem 8.4.3 is replaced by any of the hypotheses (G3)*, (G3)** or 
(G3)*** in Proposition 8.5.1. 
Remark 
We briefly discuss the more restrictive forms (G1)*, (G2)** and (G3)*** of 
hypothesis (G3) in turn: 
(G3)*: The first part of hypothesis (G3)* merely reproduces the ‘pseudo￾Lipschitz’ continuity condition of (G3). Part (a) of (G3)* is a sufficient condition for 
satisfaction of the ‘tempered growth’ condition in (G3) and provides the motivation 
behind the terminology ‘tempered growth’. It requires that the radius function has 
adequate growth, in a sense made precise, in relation to the integral Lipschitz bound 
k of the multifunction F. 
(G3)**: We know from Proposition 8.2.3 that, if F (t, .) satisfies the bounded 
slope condition, then it is pseudo-Lipschitz continuous (with modified parameters). 
(G3)** exploits this relation, to provide a restrictive version of (G3), in which a 
bounded slope condition replaces pseudo-Lipschitz continuity. 
(G3)***: (G3)*** combines within a single condition restrictive forms of 
conditions (a) and (b) in (G3). It requires F (t, .) to satisfy the pseudo-Lipschitz8.5 Special Cases 349
condition relative to an arbitrary constant radius function, R(t) = N, and places 
growth conditions on the associated integrable Lipschitz bounds, as N increases. 
Observe that, if (G3)*** is in force, then the regular velocity set is Ω0(t) =
F (t, x(t)) ¯ and then, since in this case B ≡ Rn, the Weierstrass condition becomes 
p(t) · ˙
x(t) ¯ ≥ p(t) · v, for all v ∈ F (t, x(t)), ¯ a.e. t ∈ [S,T ] .
Proof of Proposition 8.5.1 In stages (I), (II) and (III) of the proof, we show that 
(G3) is satisfied under hypotheses (G1)*, (G2)** and (G3)***, respectfully. 
(I): Suppose that (G3)∗ is satisfied. Assume, first, condition (a) in hypothesis (G3)∗. 
Take any γ ∈ (0, 1) and adjust the size of the parameter ϵ > 0 so that ϵ < γω0. 
Define r(t) := γ ω0k(t). Then, in consequence of condition (a) in (G3)*, we know 
that r is strictly bounded away from zero and γ −1r(t) ≤ R(t). 
For a.e. t ∈ [S,T ] and all x ∈ ¯x(t) + ϵB, we know from (8.5.1) that there exists 
a point v ∈ F (t, x) such that |v − ˙
x(t) ¯ | ≤ ϵ k(t). Since ϵ k(t) ≤ r(t), it follows that 
F (t, x) ∩

˙
x(t) ¯ + r(t)B

/= ∅ .
We have verified (G3)(b) in the case when (G3)∗(a) is satisfied. 
Now assume (b) in (G3)*. Take any γ ∈ (0, 1). Then there exists r0 > 0 such 
that γ −1r0 ≤ R(t) a.e.. By reducing the size of ϵ > 0, if required, we can arrange 
that θ (ϵ) ≤ r0. But then condition (b) in (G3)∗ tells us that, for an arbitrary point 
x ∈ ¯x(t) + ϵB, there exists e ∈ F (t, x) such that |e − ˙
x(t) ¯ | ≤ θ (|x − ¯x(t)|) ≤ r0. It 
follows that F (t, x) ∩ (˙
x(t) ¯ + r0B) /= ∅. We have, once again, verified (G3)(b) (for 
the constant r function r ≡ r0). 
(II): Suppose that (G3)∗∗ is satisfied. Choose any ϵ1 ∈ (0, ϵ) and η ∈ (0, 1). It 
follows from Proposition 8.2.3(b) that (G3)(a) is satisfied with parameters , ϵ' := ω0η
3 ∧ ϵ1, R'
(t) := (1 − η)ω0k(t) and k(t). 
We now show that (G3)(b) is also satisfied. Take γ ' ∈ (0, 1). By reducing the size 
of ϵ'
, if necessary, we can arrange that ϵ' ≤ γ '
(1 − η)ω0. Define r'
(t) = γ '
R'
(t). 
Note that r' is strictly bounded away from zero. Since, as we have shown, (G3)(a) 
is satisfied (with modified parameters), we know that, for any x ∈ ¯x(t) + ϵ'
B, there 
exists a point v ∈ F (t, x) satisfying |v − ˙
x(t) ¯ | ≤ k(t)ϵ' ≤ γ '
(1 − η)ω0k(t) =
γ '
R'
(t). We have shown that (G3)(b) is also satisfied. 
(III): We may arrange, by adding a constant if required, that k ≥ 1. Reduce the size 
of ϵ so that ϵ ∈ (0, 1). Choose the radius function R := k. Set r(t) = ϵk(t) and 
γ = ϵ. Then r is bounded away from zero, γ ∈ (0, 1) and γ −1r(t) ≤ R(t) a.e.. 
From hypothesis (G3)∗∗∗, in which we set N = k(t), we get 
F (t, x'
) ∩ (˙
x(t) ¯ + R(t) ◦
B) ⊂ F (t, x) + k(t) ˜ |x' − x| B
for all x'
, x ∈ ¯x(t) + ϵ B, a.e. t ∈ [S,T ]. Here k˜ is the integrable function k(t) ˜ :=
(k(t) + β(t)k(t)α).350 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
On the other hand, setting N = 0 in (G3)∗∗∗ yields the relation 
F (t, x) ∩ (˙
x(t) ¯ + ϵk(t)B) = ∅ / for every x ∈ ¯x(t) + ϵB, a.e. t ∈ [S,T ].
Recalling that r(t) = ϵk(t), we conclude that 
F (t, x) ∩ (˙
x(t) ¯ + r(t)B) = ∅ / ,for every x ∈ ¯x(t) + ϵB, a.e. t ∈ [S,T ]. ⨅⨆
8.6 Proof of Theorem 8.4.3 
This section provides a proof of all the assertions of the theorem, with the exception 
of property (vi), namely ‘constancy of t → p(t) · ˙
x(t) ¯ when B(t) and F (t, x)
do not depend on t’. The proof of (vi) is given in Chap. 9, as part of a broader 
investigation of dynamic optimization problems involving more general end-point 
constraints that those present in problem (P ). (See the comment following statement 
of Theorem 9.2.1.) 
Step 1 (Necessary Conditions for a Finite Lagrangian Problem) 
As a first step to proving Theorem 8.4.3, we establish necessary conditions of 
optimality for a variational problem of special structure, which is of interest 
principally because it provides a stepping stone to the derivation of necessary 
conditions for problems where the dynamic constraint is formulated as a differential 
inclusion. The variational problem, which makes reference to some trajectory x' ∈
W1,1([S,T ]; Rn), is as follows: 
(F L)
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
Minimize J (x) := 
�(x(S), x(T ))
∨
  T
S L(t, x(t), x(t))dt ˙

+  T
S L0(t, x(t), x(t))dt ˙ + δ |x(S) − x'
(S)|
over x ∈ W1,1([S,T ]; Rn) such that
x(t) ˙ ∈ D(t) a.e.
The data comprise an interval [S,T ], a number δ > 0, functions L : [S,T ] × Rn ×
Rn → R, L0 : [S,T ] × Rn × Rn → R and � : Rn × Rn → R, and a multifunction 
D : [S,T ] ⇝ Rn. 
Problems involving an integral cost, in which the dynamic constraint imposes 
a constraint on the velocity variables alone, are traditionally referred to as finite 
Lagrangian problems. (FL) can be regarded as a generalized finite Lagrangian 
problem, in which the primary integral cost term enters, not additively, but via a 
‘max’ operation. (The secondary integral cost term  L0dt and the left end-point 
cost term δ|x(S) − x'
(S)| are included in the problem formulation to aid future 
analysis, in which they will appear as perturbation terms arising from the application 
of Ekeland’s theorem.) A W1,1 local minimizer x' for (FL) is defined by analogy 
with earlier definitions, except that, now, we impose the additional requirement that 
t → (L, L0)(t, x'
(t), x˙'
(t)) is an integrable function.8.6 Proof of Theorem 8.4.3 351 
Proposition 8.6.1 Let x' be a W1,1 local minimizer for (FL). Assume that the 
following hypotheses are satisfied: there exist ϵ ∈ (0, 1), N > 0 and a non-negative 
function k˜ ∈ L1(S, T ) such that 
(FL0): D has closed values, Gr D is L × Bn measurable and D(t) ⊂ ˙x'
(t) + NB
a.e., 
(FL1): � is Lipschitz continuous on ϵ(B × B), 
(FL2): (L, L0)(., x, v) is L-measurable for each (x, v) ∈ Rn × Rn, 
(FL3): |(L, L0)(t, x, v) − (L, L0)(t, y, w)| ≤ k(t)( ˜ |x − y|+|v − w|),
for all x, y ∈ ϵB and v, w ∈ D(t), a.e. t ∈ [S,T ]. 
Assume, furthermore, that 
||x'
||W1,1 < ϵ.
Then there exists an arc p ∈ W1,1 and λ ∈ [0, 1] which satisfy 
(i): 
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
(a) : ˙p(t) ∈ (1 − λ)co ∂xL(t, x'
(t), x˙'
(t))
+co ∂xL0(t, x'
(t), x˙'
(t)) a.e. t ∈ [S,T ]
and
(b) : ˙p(t) ∈ co{η : (η, p(t)) ∈ (1 − λ)∂x,vL(t, x'
(t), x˙'
(t))
+∂x,vL0(t, x'
(t), x˙'
(t))}, a.e. t ∈ [S,T ] such that x˙'
(t) ∈ ◦
D(t)
(ii): (p(S), −p(T )) ∈ λ∂�(x'
(S), x'
(T )) + δB × {0}, 
(iii): p(t) · ˙x'
(t) − (1 − λ)L(t, x'
(t), x˙'
(t)) − L0(t, x'
(t), x˙'
(t)) ≥
p(t) · v − (1 − λ)L(t, x'
(t), v) − L0(t, x'
(t), v),
for all v ∈ D(t), a.e. t ∈ [S,T ].
Furthermore 
 T
S
L(t, x'
(t), x˙'
(t))dt < �(x'
(S), x'
(T )) =⇒ λ = 1. (8.6.1) 
Proof of Proposition 8.6.1 Since x' is a W1,1 local minimizer, there exists β > 0
such that x' is minimizing, when we add to (FL) the constraint 
|x(S) − x'
(S)| +  T
S
| ˙x(t) − ˙x'
(t)|dt ≤ β . (8.6.2) 
Bearing in mind that ||x'
||W1,1 < ϵ, we can arrange, by choosing β sufficiently 
small, that ||x||W1,1 < ϵ (and hence also that ||x||L∞ < ϵ), for all x satisfy￾ing (8.6.2). Here, ϵ is the constant appearing in (FL3). 
We note at the outset that hypotheses (FL1) and (FL3) can been replaced by the 
stronger, global, hypotheses:352 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
(FL1)'
: There exists c0 > 0 and k� > 0 such that 
|�(x0, x1) − �(y0, y1)| ≤ k�|(x0, x1) − (y0, y1)| and |�(x0, x1)| ≤ c0
for all (x0, x1), (y0, y1)) ∈ Rn × Rn, 
(FL3)'
: there exist non-negative k, c ∈ L1 such that 

|(L, L0)(t, x, v) − (L, L0)(t, y, w)| ≤ k(t)(|x − y|+|v − w|) and
|(L, L0)(t, x, v)| ≤ c(t)
for all x, y ∈ Rn and v, w ∈ D(t), a.e. t ∈ [S,T ]. 
This is because, if either (FL1)' or (FL3)' were not satisfied, we could replace 
� and (L, L0) by �'
(x0, x1) := �(πϵB(x0), πϵB(x1)) and (L'
, L'
0)(t, x, v) =
(L, L0)(t, πϵB(x), v) where πϵB is the projection onto ϵB. Then, if k� is the 
Lipschitz constant of � on ϵ(B × B), �' is Lipschitz continuous on Rn × Rn with 
Lipschitz constant k�, which coincides with � on a neighbourhood of (x'
(S), x'
(T )), 
and we can take c0 := |�(x'
(S), x'
(T ))| + 2
√2ϵk�. The new pair of inte￾grands satisfies (FL3), with integrable bounds k(t) = 2 ϵ−1 k(t) ˜ and c(t) :=
|(L, L0)(t, x'
(t), x˙'
(t))|+(2ϵ+N )k(t). Hypotheses (FL0) and (FL2) continue to be 
satisfied, following these changes in the data, and x' remains a W1,1 local minimizer. 
If we are able to establish existence of some p and λ such that conditions (i)–(iii) 
are satisfied for x' and the modified data �' and (L'
, L0
'
) at x'
, conditions (i)–(iii) 
continue to be satisfied for x' and the original data � and (L, L0) with the same p
and λ, because of the ‘local’ nature of these conditions (w.r.t. the x variable). So we 
may indeed assume (FL1)' and (FL3)'
, without loss of generality. 
By adding a constant if required, we can arrange k in (FL3)' satisfies k(t) ≥ 1, 
a.e.. 
Take a sequence Ki ↑ 0. Write L1
k for the Banach space of measurable functions 
w such that t → k(t)w(t) is integrable on [S,T ], equipped with the k-weighted L1
norm ||w||L1
k := ||kw||L1 . Write 
W := {(ξ , w, v) ∈ Rn × L1
k × L1
k : v(t) ∈ D(t) a.e., ||xξ,v − x'
||W1,1 ≤ β},
in which xξ,v(t) = ξ +  t
S v(s)ds. Define 
||(ξ , w, v)||k := |ξ | + ||kw||L1 + ||kv||L1 .
For each i, set 
J˜
i(ξ , w, v) := 
�(xξ,v(S), xξ,v(T ))
∨
  T
S
L(t, w(t), v(t))dt8.6 Proof of Theorem 8.4.3 353 
+
 T
S
L0(t, w(t), v(t))dt
+δ|xξ,v(S) − x'
(S)| + Ki
 T
S
k(t)|xξ,v(t) − w(t)|
2dt.
Notice that this cost function differs from that in (FL), by virtue of the facts that, 
in the cost integrands, the control-like variable w replaces the state variable x and 
that the discrepancy between w and x is accommodated by a quadratic penalty term, 
with penalty parameter Ki. ⨅⨆
Claim: For each i, (W,||.||k) is a complete metric space and J˜
i is lower semi￾continuous on (W,||.||k). There exists a sequence of non-negative numbers αi ↓ 0
such that, for each i, 
J˜
i(x'
(S), x'
, x˙'
) ≤ inf
(ξ ,w,v)∈W
J˜
i(ξ , w, v) + α2
i .
We verify the claim. Fix i. W is a subset of the Banach space Rn × L1
k × L1
k
with norm ||.||k. It suffices to show that the set is strongly closed and that J˜
i is 
lower semi-continuous on W. Take an arbitrary sequence (ξj , wj , vj ) → (ξ , w, v)
in (W,||.||k). Write xj := xξj ,vj . Then, since k ≥ 1 a.e., xj → xξ,v in W1,1. 
Restricting attention to a subsequence, we have vj (t) → v(t) a.e.. So v(t) ∈ D(t)
a.e. and ||xξ,v − x'
||W1,1 ≤ β. The limit point (ξ , w, v) satisfies the conditions 
confirming membership of W, and so W is strongly closed. This establishes that 
(W,||.||k) is complete. 
Take again an arbitrary sequence (ξj , wj , vj ) → (ξ , w, v) in (W,||.||k). Set 
σ0 := lim inf
j→∞ J˜
i(ξj , wj , vj ) .
Observe that, from (FL1)' and (FL3)'
, we have σ0 > −∞. By restricting attention 
to a subsequence, we can arrange that J˜
i(ξj , wj , vj ) → σ0 as j → ∞. Writing 
as before xj := xξj ,vj , we have xj → xξ,v strongly in W1,1. It follows that 
�(xj (S), xj (T )) → �(xξ,v(S), xξ,v(T )) and  T
S |vj − v|dt → 0 as i → ∞. 
Along a further subsequence (we do not relabel), wj (t) → w(t) and vj (t) → v(t)
a.e.. In view of the second condition in (FL3)'
, which ensures uniform integrable 
boundedness of the integrands (L, L0)(t, wj (t), vj (t)), j = 1, 2 ..., we have, by 
the dominated convergence theorem, that 
 T
S
(L, L0)(t, wj (t), vj (t))dt →
 T
S
(L, L0)(t, w(t), v(t))dt as k → ∞.
Bearing in mind that the integrands involved are non-negative, we deduce from 
Fatou’s lemma that354 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
lim inf
j→∞  T
S
k(t)|xj (t) − wj (t)|
2dt ≥
 T
S
lim inf
j→∞ k(t)|xj (t) − wj (t)|
2dt
=
 T
S
k(t)|xξ,v(t) − w(t)|
2dt.
(Note, we allow the possibility that some of these integrals are infinite.) It follows 
that 
σ0 ≥ lim inf
j→∞ 
�(xj (S), xj (T )) ∨
 T
S
L(t, wj (t), vj (t))dt
+ δ|xj (S) − x'
(S)| +  T
S
L0(t, wj (t), vj (t))dt
+ lim inf
j→∞ Ki
 T
S
k(t)|xj (t) − wj (t)|
2dt
≥ �(x(S), x(T )) ∨
 T
S
L(t, w(t), v(t))dt + δ|xξ,v(S) − x'
(S)|
+
 T
S
L0(t, w(t), v(t))dt + Ki
 T
S
k(t)|xξ,v(t) − w(t)|
2dt
= J˜
i(ξ , w, v) .
We conclude from this inequality that J˜
i is lower semi-continuous. The claim is 
confirmed. 
Now define 
α2
i := J˜
i(x'
(S), x'
, x˙'
) − inf
(ξ ,w,v)∈W
J˜
i(ξ , w, v).
(Since (x'
(S), x'
, x˙'
) ∈ W, the right side is nonnegative.) We show that αi ↓ 0 as 
i → ∞. 
Choose an arbitrary point (ξ , w, v) ∈ W such that J˜
i(ξ , w, v) < ∞. Define 
c˜ :=  T
S
k(t)|w(t) − xξ,v(t)|
2dt1
2
(< ∞) and d :=  T
S
k(t)dt1
2
.
Then 
J˜
i(ξ , w, v) ≥ J˜
i(ξ , xξ,v, v) − |  T
S
((L, L0)(t, w(t), v(t))
− (L, L0)(t, xξ,v(t), v(t)))dt| + Kic˜
2
≥ J˜
i(x'
(S), x'
, x˙'
) − 2
 T
S
k(t)|xξ,v(t) − w(t)|dt + Kic˜
28.6 Proof of Theorem 8.4.3 355 
(by the minimizing property of (x'
(S), x'
, x˙'
) and by (FL3)'
) 
≥ J˜
i(x'
(S), x'
, x˙'
) − ˜cd + Kic˜
2
(by the Hölder inequality) 
≥ J˜
i(x'
(S), x'
, x˙'
) + min{−2c'
d + Ki(c'
)
2 : c' ∈ R}
= J˜
i(x'
(S), x'
, x˙'
) − K−1
i d2.
It follows that 
0 ≤ α2
i

= J˜
i(x'
(S), x'
, x˙'
) − inf
W
J˜
i(ξ , w, v)
≤ K−1
i ×
 T
S
k(t)dt.
Since Ki ↑ ∞, we conclude that αi → 0 as i → ∞. By discarding initial points in 
the sequence, we can arrange that αi ≤ 1, for each i. 
Fix i. Since J˜
i is a lower semi-continuous function and (x'
(S), x'
, x˙'
) is an α2
i
minimizer for J˜
i over W, we know from Ekeland’s theorem (Theorem 3.3.1), that 
there exists (ξi, wi, vi) ∈ W which minimizes 
Ji(ξ , w, v) := J˜
i(ξ , w, v) + αi||(ξ , w, v) − (ξi, wi, vi)||k
over W, and 
||(ξi, wi, vi) − (x'
(S), x'
, x˙'
)||k ≤ αi, for each i . (8.6.3) 
Ekeland’s theorem also tells us that J˜
i(ξi, wi, vi) ≤ Ji(x'
(S), x'
, x˙'
)(< ∞). This 
implies that  T
S k(t)|xi(t)−wi(t)|
2dt < ∞. Here, xi := xξi,vi . Since xi is bounded, 
it follows that t → k(t)w2
i (t) is integrable. 
We can summarize the above discussion in control theoretic terms as follows: 
((xi, yi, zi), (vi, wi)) is a minimizer for the dynamic optimization problem 
(E)
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize �(x(S), x(T )) ∨ z(T )+ δ|x(S)−x'
(S)|
+  T
S L0(t, w(t), v(t))dt +αi

|x(S) − xi(S)|
+  T
S (k(t)|v(t) − vi(t)| + k(t)|w(t) − wi(t)|)dt
+Ki
 T
S k(t)|x(t) − w(t)|
2dt.
over arcs (x, y, z) ∈ W1,1 × W1,1 × W1,1 satisfying
x(t) ˙ = v(t), y(t) ˙ = |v(t) − ˙x'
(t)| , z(t) ˙ = L(t, w(t), v(t)), a.e.,
w(t) ∈ Rn, v(t) ∈ D(t) a.e.,
y(S) = z(S) = 0 and |x(S) − x'
(S)| + y(T ) ≤ β.
Here, yi(t) :=  t
S |vi(s)− ˙x'
(s)|ds, and zi(t) :=  t
S Li(s, wi(s), vi(s)))ds.356 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
(Strictly speaking, the above problem (E) is not equivalent to Min{Ji(ξ , w, v) :
(ξ , w, v) ∈ W} because it has domain comprising control functions (v, w) from the 
larger set L1× {meas. functions w : [S,T ] → Rn}. However the two problems are 
equivalent, in the sense of having a common set of minimizers since, for any control 
functions w, v /∈ L1
k ×L1
k the cost in (E) is +∞, as is easily shown, and such control 
functions cannot be candidates for minimizing controls.) 
The foregoing dynamic optimization problem is one to which the maximum 
principle, Theorem 7.2.1, is applicable, following the removal of integral cost terms 
by state augmentation. The relevant hypotheses are satisfied. (To check hypothesis 
(H1) in the maximum principle, we make use of the facts that t → k(t)w2
i (t) and 
t → k(t)vi(t) are integrable functions.) In consequence of (8.6.3), we know that, 
for some subsequence, 
||xi → x'
||L∞ → 0 and (vi, wi) → (x˙'
, x'
) in L1 and a.e..
This implies, in particular, that |xi(S)−x'
(S)|+yi(T ) → 0. It follows that the end￾point constraint ‘|xi(S)−x'
i(S)| + yi(T ) ≤ β’ is inactive at (xi, yi), for sufficiently 
large i. 
Because of the decoupled structure of the cost and dynamics, regarding the y and 
(x, z) state variable components, the costate trajectory component associated with y
must be zero; it therefore drops altogether out of the relations. Also, in consequence 
of the facts that there is no end-point constraint and the end-point cost function is 
Lischitz continuous near (xi(S), xi(T )), the maximum principle is valid with cost 
multiplier 1. 
Identifying pi ∈ W1,1 with the costate trajectory for the x variable, and −(1−λi)
with the (constant) costate trajectory associated with the z variable, we deduce, from 
the necessary conditions (and the max rule to estimate the limiting subdifferential 
∂(� ∨ z)) the following: there exist pi ∈ W1,1 and λi ∈ [0, 1] such that 
(A): − ˙pi(t) = −2Ki k(t)(xi(t) − wi(t)) a.e., 
(B): (pi(S), −pi(T )) ∈ λi∂�(xi(S), xi(T )) + (δ + αi)B × {0}, 
(C): the function hi(w, v) := pi(t) · v − (1 − λi)L(t, w, v) − L0(t, w, v)
− αik(t)(|v − vi(t)|+|w − wi(t)|) − Kik(t)|xi(t) − w|
2
achieves its maximum at (wi(t), vi(t)) over (w, v) ∈ Rn × D(t), a.e.. 
Notice that, if  T
S L(t, x'
(t), x˙'
(t))dt < �(x'
(S), x'
(T )), then  T
S L(t, xi(t),
x˙i(t))dt < �(xi(S)), xi(T )) for i sufficiently large. It follows from the max rule 
for subdifferentials that λi = 1. But then 
 T
S
L(t, x'
(t), x˙'
(t))dt < �(x'
(S), x'
(T )) =⇒ λi = 1 , for i sufficiently large .
(8.6.4) 
Next, we investigate the consequences of condition (C). Take any t ∈ [S,T ] at 
which this condition is satisfied. The specified function with constrained maximizer8.6 Proof of Theorem 8.4.3 357 
(wi(t), vi(t)) is Lipschitz continuous on a neighbourhood of (wi(t), vi(t)); write 
the Lipschitz constant ρi(t). It follows from the exact penalization principle that 
(wi(t), vi(t)) is also an unconstrained maximizer of the function 
(w, v) → pi(t) · v − (1 − λi)L(t, w, v) − L0(t, w, v)
− αik(t)(|v − vi(t)|+|w − wi(t)|) − Kik(t)|xi(t) − w|
2 − ρi(t)dD(t)(v) .
It follows that 
(0, pi(t)) ∈ ∂x,v((1 − λi)L + L0)(t, wi(t), vi(t))
+ αik(t)(B × B)+(2Ki k(t)(xi(t) − wi(t)), 0) + {0} × ρi(t)∂dD(t)(v).
This relation combines with (A) to give: 
(− ˙pi(t), pi(t)) ∈ ∂x,v((1 − λi)L + L0)(t, wi(t), vi(t))
+ αik(t)(B × B) + {0} × ρi(t)∂dD(t)(vi(t)) . (8.6.5) 
Fix v = vi(t). Then 
w → − (1 − λi)L(t, w, vi(t)) − L0(t, w, vi(t)) − αik(t)|w − wi(t)|
− Kik(t)|xi(t) − w|
2
achieves its maximum at wi(t) over all w ∈ Rn. In view of (A), this implies 
p˙i(t) ∈ ∂x ((1 − λi)L + L0)(t, wi(t), vi(t)) + αik(t)B. (8.6.6) 
Fix w = wi(t). Then v → pi(t) · v − (1 − λi)L(t, wi(t), v) − L0(t, wi(t), v) −
αik(t)|v − vi(t)| achieves its maximum at vi(t) over v ∈ D(t). This implies that, 
for a.e. t ∈ [S,T ], 
pi(t) · vi(t) − (1 − λi)L(t, wi(t), vi(t)) − L0(t, wi(t), vi(t))
≥ pi(t) · v − (1 − λi)L(t, wi(t), v) − L0(t, wi(t), v)
− αik(t)|v − vi(t)| for all v ∈ D(t). (8.6.7) 
Since (L, L0)(t, ., v) is Lipschitz continuous with Lipschitz constant k(t) for all v ∈
D(t), (8.6.6) implies that, for i sufficiently large, | ˙pi(t)| ≤ (2+αi)k(t). Noting that 
the pi(S)’s are uniformly bounded (see (B)), we can arrange, by limiting attention 
to a subsequence, that pi → p uniformly and p˙i → ˙p weakly in L1 for some 
p ∈ W1,1. We can also ensure that λi → λ for some λ ≥ 0. It follows from (8.6.4) 
that358 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
 T
S
L(t, x'
(t), x˙'
(t))dt < �(x'
(S)), x'
(T )) =⇒ λ = 1 . (8.6.8) 
We have confirmed condition (8.6.1). Next, notice that (8.6.7) implies in the limit 
p(t) · v'
(t) − (1 − λ)L(t, x'
(t), v'
(t)) − L0(t, x'
(t), v'
(t))
≥ p(t) · v − (1 − λ)L(t, x'
(t), v) − L0(t, x'
(t), v) for all v ∈ D(t) a.e..
This is (iii). (B) yields in the limit (p(S), −p(T )) ∈ λ∂�(x'
(S), x'
(T )) + δB × {0}, 
which is (ii). 
We next confirm (i)(b). Fix σ > 0. Define 
Fσ := {t ∈ [S,T ]: ˙x'
(t) + σB ⊂ D(t)}.
Define, for t ∈ [S,T ], 
Gσ (t) :=
⎧
⎪⎪⎨
⎪⎪⎩
{q ∈ Rn : (q, p(t)) ∈ 
(w,v)∈(x'
(t),x˙'
(t))+σB
∂x,v((1 − λ)L + L0)(t, w, v)
+σ k(t)(B × B)} if t ∈ Fσ
3k(t)B if t /∈ Fσ .
Define, finally, the monotone sequence of sets Eσ
i , i = 1, 2,..., 
Eσ
i := {t ∈ [S,T ]:|(wj (t), vj (t)) − (x'
(t), x˙'
(t))| ≤ σ for all j ≥ i}.
Since (wi, vi) → (x'
, x˙'
) a.e. 
meas 
[S,T ]\Eσ
i

→ 0 as i → ∞.
Taking note also of (8.6.5) and (8.6.6), we deduce that, for all i sufficiently large, 
p˙i(t) ∈ Gσ (t), for a.e. t ∈ Eσ
i (t) .
It now follows from the compactness of trajectories theorem (Theorem 6.3.3) that 
p(t) ˙ ∈ co Gσ (t), for a.e. t ∈ [S,T ] .
This relation is valid for all σ > 0. Taking account of the upper semicontinuity 
properties of the limiting subdifferential, we can show that, for a.e. t ∈ [S,T ], 
co Gσ (t), σ > 0, is a monotone family of sets and 
lim
σ↓0
co Gσ (t)= co ∂x,v((1−λ)L+L0)(t, x'
(t), x˙'
(t)), for a.e. t s.t. x˙'
(t) ∈ int D(t) .8.6 Proof of Theorem 8.4.3 359 
We have confirmed property (i)(b). A similar analysis, taking as starting point (8.6.6) 
in place of (8.6.5), yields (i)(a). All the assertions of Proposition 8.6.1 have been 
verified. The proof is complete. 
Step 2 (Hypothesis Reduction) 
We now show that, without loss of generality, it suffices to prove the assertions of 
Theorem 8.4.3 in the special case when some additional hypotheses are imposed on 
the data and the candidate admissible F trajectory x¯. The end result of this reduction 
step are summarized as Proposition 8.6.2 below. 
The following hypothesis is a strengthened form of (G3), in which the function 
R and r are replaced by constants. 
(G3)'
: There exist ϵ > 0, R ∈ (0,∞) , γ ∈ (0, 1) and k ∈ L1 such that R ◦
B ⊂ B(t)
a.e. and the following conditions are satisfied: 
(a): F (t, x'
) ∩ (˙
x(t) ¯ + R
◦
B) ⊂ F (t, x) + k(t)|x' − x|B,
for all x, x' ∈ ¯x(t) + ϵB, a.e. t ∈ [S,T ]. 
(b): F (t, x) ∩ (˙
x(t) ¯ + γ RB) /= ∅ for all x ∈ ¯x(t) + ϵB, a.e. t ∈ [S,T ]. 
Proposition 8.6.2 Assume the assertions of Theorem 8.4.3 are valid under the 
hypotheses(G1), (G2) and (G3)' and when x¯ ≡ 0. Then the assertions of Theo￾rem 8.4.3 are valid under hypotheses (G1)–(G3). 
Proof Suppose that the assertions of the theorem are known to be valid only under 
the additional assumption that x¯ ≡ 0. Then, if x¯ /≡ 0, we may consider a modified 
problem in which F (t, x), g(x0, x1) and C are replaced by ‘F (t, x(t) ¯ + x) − ˙
x(t) ¯ ’, 
‘g(x(S) ¯ + x0, x(T ) ¯ + x1)’ and ‘C − (x(S), ¯ x(T )) ¯ ’, respectively. For the modified 
problem, x ≡ 0 is a W1,1 local minimizer relative to the multifunction B. The 
modified data continue to satisfy (G1)–(G3) and the necessary conditions for the 
modified problem, which we know to be valid under our assumptions, imply those 
for the original problem. So we can assume that x¯ ≡ 0. 
Next suppose that the data satisfies (G3) (expressed in terms of R : [S,T ] →
(0,∞)∪+∞, r ∈ L1 and γ ∈ (0, 1)), with reference to x¯ ≡ 0 and the multifunction 
B, but when the assertions of the theorem are known to be valid only in the case 
when (G3) is replaced by the stronger hypothesis (G3)'
. In this situation, we consider 
the change of independent variable s = σ (t) where 
σ (t) :=  t
S
r(t'
)dt' .
σ is a strictly increasing function on its domain [S,T ] such that σ (S) = 0. Write 
τ := σ (T ). 
Let us now examine a new problem, which is an example of (P ), in which the 
underlying time interval [S,T ] is replaced by [0, τ ] and F is replaced by 
F (s, x) ˜ :=
1
(r ◦ σ −1)(s)F (σ −1(s), x) ,360 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
but, otherwise, the data remains the same. Take β > 0 to be such that x¯ ≡ 0 is 
minimizing with reference to admissible F trajectories x such that x˙ ∈ B(t) a.e. 
and ||x||W1,1 ≤ β. Now take y : [0, τ ] → Rn to be any admissible F˜ trajectory for 
the new problem such that ||y||W1,1 ≤ β and y(s) ˙ ∈ B(s) ˜ a.e., where 
B(s) ˜ :=
1
(r ◦ σ −1)(s)B(σ −1(s)) .
Define x := y ◦ σ. Then x is an F trajectory on [S,T ] satisfying (x(S), x(T )) =
(y(0), y(τ )) and x(t) ˙ ∈ B(t) a.e.. Since y is admissible for the new problem, it 
follows that x is admissible for the original problem, We have x(t) ˙ = (y˙ ◦ σ )(t)r(t)
a.e.. By Fubini’s theorem, 
 T
S
| ˙x(t)|dt =
 τ
0
| ˙y ◦ σ|(t)r(t)dt =
 τ
0
|y(s)|ds ≤ β .
But then, since x(S) = y(0), we know that ||x||W1,1 = ||y||W1,1 ≤ β. Now define 
y¯ ≡ 0. Since x¯ ≡ 0 is admissible for the original problem and x¯ ≡ 0 maps into 
y¯ ≡ 0 under the inverse change of independent variable, we know that y¯ ≡ 0 is 
admissible for the new problem. By the optimality properties of x¯ ≡ 0 and the 
above relations, 
g(y(0), y(τ )) = g(x(S), x(T )) ≥ g(x(S), ¯ x(T )) ¯ = g(y(¯ 0), y(τ )) . ¯
It follows that y¯ ≡ 0 is a W1,1 local minimizer for the new problem relative to B˜. 
The data for the new problem satisfies hypotheses (G1) and (G2) of Theo￾rem 8.4.3. We now show that hypothesis (G3)' is also satisfied. Since γ −1r(t) ≤
R(t) a.e., we know, from (G3), that, for all x, x' ∈ ϵB and a.e. s ∈ [0, τ ], 
F (σ −1(s), x) ∩ (γ −1 × r ◦ σ −1)(s) ◦
B ⊂ F (σ −1(s), x'
) + (k ◦ σ −1)(s)|x − x'
|B .
But then by definition of F˜, 

(r ◦ σ −1)(s) F (s, x) ˜ 
∩ (γ −1 × r ◦ σ −1)(s) ◦
B
⊂ (r ◦ σ −1)(s)F (σ ˜ −1(s), x'
) + (k ◦ σ −1)(s)|x − x'
|B .
It follows that 
F (s, x) ˜ ∩ (γ −1 ◦
B) ⊂ F (s, x ˜ '
) + k(s) ˜ |x − x'
|B for all x, x' ∈ ϵB and a.e. s ∈ [0, τ ],
(8.6.9)8.6 Proof of Theorem 8.4.3 361 
where k(s) ˜ := (k ◦ σ −1)(s)) 1
r◦σ −1(s). Notice that, in consequence of Fubini’s 
theorem, k˜ is an integrable function. Thus the data for the new problem satisfies 
condition (G3)'
(a), with reference to the nominal F˜ trajectory y¯ ≡ 0. 
We know from (G3)(b) that, for all x ∈ ϵB, F (t, x) ∩ (r(t)B) /= ∅ for a.e. t ∈
[S,T ]. But then, by definition of F˜, 
(r ◦ σ −1)(s) F (s, x) ˜ ∩ ((r ◦ σ −1)(s)B) /= ∅ , for all x ∈ ϵB, a.e. s ∈ [0, τ ].
It follows that 
F (s, x) ˜ ∩ B /= ∅ , for all x ∈ ϵB, a.e. s ∈ [0, τ ]. (8.6.10) 
Relations (8.6.9) and (8.6.10) confirm (G3)'
, with ϵ as before and the other 
parameters that we now write R' and γ ' given by R' := γ −1 and γ ' = γ , in which 
γ ∈ (0, 1) is the parameter in (G3) (for the data of the original problem). 
Since hypotheses (G1), (G2) and (G3)' are satisfied, we are justified, under 
our assumptions, in applying the necessary conditions of Theorem 8.4.3 to the 
new problem, with reference to the W1,1 minimizer y¯ ≡ 0 relative to B˜. It 
is straightforward to deduce the necessary conditions for the original problem, 
with reference to x¯ ≡ 0 and B, via the inverse change of independent variable 
t = σ −1(s), in which the Lagrange multipliers (p, λ) ˜ arising in the new problem 
are replaced by (p = ˜p ◦ σ −1, λ) in the original one. ⨅⨆
Henceforth in the proof, we may assume that the strengthened hypothesis (G3)'
(with parameters γ ∈ (0, 1) and R > 0 and integrable Lipschitz bound k) is satisfied 
since, as has been demonstrated, we may do so without loss of generality. 
Step 3 (Construction of an Integral Penalty Integrand) 
Our goal in this step is to construct an integral function, with integrand ρS(t, x, v), 
which penalizes violations of the dynamic constraint and restrictions on velocities 
associated with the multifunction B, namely 
x(t) ˙ ∈ F (t, x(t)) ∩ (˙
x(t) ¯ + B(t)) ,
in a suitable manner for the derivation of necessary conditions. 
In the earlier literature, which pre-dated consideration of constraints on velocities 
expressed in terms of the multifunction B, an integral penalty function with 
integrand constructed from the distance function to F (t, x), namely 
ρF (t, x, v) := inf{|v − e| : e ∈ F (t, x)}, (8.6.11) 
was widely used to reduce problem (P ) to an approximating finite Lagrangian 
problem, as a means to accomplishing this step. But when we introduce an 
additional constraint on velocities, effectively replacing F (t, x(t)) by F (t, x(t)) ∩
(˙
x(t) ¯ +B(t)), this traditional penalty integrand lacks the crucial regularity properties362 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
for the derivation of necessary conditions and an alternative, which we shall refer to 
as the modified penalty integrand, is required. 
Before constructing the modified penalty integrand and exploring its properties, 
we will find it helpful to have at hand useful information about the standard penalty 
integrand, assembled in the following lemma. 
Lemma 8.6.3 Take a multifunction � : Rn ⇝ Rn and a point (x,¯ v)¯ ∈ Gr �. 
Assume that � has values non-empty closed sets. Define the function ρ� : Rn ×
Rn → R to be 
ρ�(x, v) := inf{|v − y| : y ∈ �(x)}.
Assume that there exist ϵ > 0, r > 0, k > 0 and K > 0 such that 
�(x'
) ∩ (v¯ + r
◦
B) ⊂ �(x) + k|x' − x|B, for all x'
, x ∈ ¯x + ϵB. (8.6.12) 
and 
�(x) ∩ (v¯ + K|x − ¯x|B) /= ∅, for all x ∈ ¯x + ϵB. (8.6.13) 
Then: 
(a): For each x ∈ Rn, ρ�(x, .) is Lipschitz continuous with Lipschitz constant 
1. For each v ∈ ¯v + (r/4)B, ρ�(., v) is Lipschitz continuous on x¯ +
min{ϵ, r/(4K)}B with Lipschitz constant k. 
(b): Fix elements (x, v) ∈ Rn × Rn and (w, p) ∈ Rn × Rn such that |x − ¯x| <
min{ϵ, r/(3K)} and |v − ¯v| < r/3. Assume 
(w, p) ∈ ∂ρ�(x, v).
Then 
|w| ≤ k and |p| ≤ 1.
Furthermore, 
v ∈ �(x) implies (w, p) ∈ NGr �(x, v) .
(c): Fix elements (x, v) ∈ Rn × Rn and (w, p) ∈ Rn × Rn such that |x − ¯x| < ϵ
and |v − ¯v| < r, 
‘ v ∈ �(x) and (w, p) ∈ NGr �(x, v).’ =⇒ ‘ |w| ≤ k |p|’.
(It might be thought that (8.6.13) is a redundant hypothesis, since it is implied by the 
preceding hypothesis (8.6.12), with K replaced by k. The lemma needs sometimes8.6 Proof of Theorem 8.4.3 363 
to be applied, however, in situations where K<k and, in this case, the fact that 
the domain of Lipschitz continuity referred to in part (a) of the lemma is the set 
x¯ + min{ϵ, r/(4K)}B , not the smaller set x¯ + min{ϵ, r/(4k)}B, can be important.) 
Proof 
(a): For fixed x, ρ�(x, .) is the Euclidean distance function for the set �(x) and, by 
the properties of distance functions, is Lipschitz continuous with Lipschitz constant 
1. Fix v ∈ ¯v +(r/3)B. Choose any x'
, x ∈ ¯x +min{ϵ, r/(3K)}B. Let u' be a closest 
point to v in �(x'
) (and one such point exists), i.e., |v − u'
| = ρ�(x'
, v). According 
to (8.6.13) there exists e' ∈ �(x'
) such that |e' − ¯v| ≤ K|x' − ¯x| ≤ r/4. Then 
|u'
− ¯v|≤|v− ¯v|+|v−u'
|≤|v− ¯v|+|v−e'
| ≤ 2|v− ¯v|+|¯v−e'
| ≤
2
3
r +
1
4
r<r.
We have shown that u' ∈ ¯v + r
◦
B. By (8.6.12), there exists u ∈ �(x) such that 
|u' − u| ≤ k|x' − x|. It follows that 
ρ�(x, v) ≤ |v − u|≤|v − u'
|+|u' − u| ≤ ρ�(x'
, v) + k|x' − x|.
Since the variables x' and x are interchangeable, we conclude that 
|ρ�(x, v) − ρ�(x'
, v)| ≤ k|x − x'
|.
This is the desired Lipschitz continuity property. 
(b): We have shown that ρ� is Lipschitz continuous on the closed neighbourhood 
N of (x,¯ v)¯
N := (x,¯ v)¯ + min{ϵ, r/(4K)}B × (r/3)B (8.6.14) 
Close scrutiny of the proof of (a) reveals also that 
(x, v) ∈ N implies |u − ¯v|<r (8.6.15) 
for any point u ∈ �(x) such that |v − u| = min{|v − e| : e ∈ �(x)}. 
Fix (x, v) in the interior of N . Since (as we have shown) ρ� is Lipschitz 
continuous on a neighbourhood of (x, v), it is meaningful to talk of the limiting 
subdifferential ∂ρ�(x, v). Take any (w, p) ∈ ∂ρ�(x, v). We know that there exist 
(xi, vi) → (x, v), (wi, pi) → (w, p), ϵi ↓ 0 and a sequence of positive numbers 
Mi such that 
wi · (x' − xi) + pi · (v' − vi) ≤
ρ�(x'
, v'
) − ρ�(xi, vi) + Mi(|x' − xi|
2 + |v' − vi|
2) (8.6.16) 
for all (x'
, v'
) ∈ (xi, vi) + ϵiB.364 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
By part (a), ρ�(., vi) and ρ�(xi, .) are Lipschitz continuous (on appropriate 
neighbourhoods of (xi, vi)) with Lipschitz constants k and 1 respectively, for 
sufficiently large i. We easily deduce from (8.6.16) that |wi| ≤ k and |pi| ≤ 1. 
Passing to the limit as i → ∞ gives 
|w| ≤ k and |p| ≤ 1.
Now suppose that v ∈ �(x). For each i, again take ui to be a closest point to vi
in �(xi) (we allow the possibility that vi ∈ �(xi) for some values of i). It follows 
from (8.6.12) and (8.6.15) that ui → v as i → ∞. Fix i sufficiently large. Choose 
any (x'
, u'
) in Gr � close to (xi, ui). Now insert (x'
, v' = u' +vi −ui) into (8.6.16). 
We arrive at 
wi · (x' − xi) + pi · (u' − ui) ≤
ρ�(x'
, u' + vi − ui) − ρ�(xi, vi) + Mi(|x' − xi|
2 + |u' − ui|
2).
Notice however that 
ρ�(x'
, u' + vi − ui) ≤ |u' + vi − ui − u'
|=|vi − ui| = ρ�(xi, vi).
It follows that 
wi · (x' − xi) + pi · (u' − ui) ≤ Mi(|x' − xi|
2 + |u' − ui|
2)
for all (x'
, u'
) in Gr � sufficiently close to (xi, ui). This implies (wi, pi) ∈
NP
Gr �(xi, ui). However Gr �∩((x, v)+αB) is a closed set for some α > 0. Recalling 
that (xi, ui) Gr � → (x, v), we deduce from the closure properties of the normal cone 
that (w, p) ∈ NGr �(x, v). This is what we set out to prove. 
(c): Take arbitrary pairs of points (x, v) ∈ Gr �(x, v) and (w, p) ∈ NGr � with the 
asserted properties. Assume initially that 
(w, p) ∈ NP
Gr �(x, v) .
Then, from the proximal normal inequality, there exists M > 0 such that 
(x' − x) · w + (v' − v) · p ≤ M(|x' − x|
2 + |v' − v|
2)
for all (x'
, v'
) ∈ Gr �. Take δi ↓ 0 and, for each i, let xi = x +δiw. Since |v − ¯v| <
r and in view of (8.6.12) we know that, for each i sufficiently large, there exists 
vi ∈ �(x + δiw) such that |vi − v| ≤ δik|w|. Inserting x' = xi and v' = vi into the 
above relation yields 
δi|w|
2 ≤ |vi −v| |p| +δ2
i M(|w|
2 +δ−2
i |vi −v|
2) ≤ δik|w||p| +δ2
i M(1+k2)|w|
2 .8.6 Proof of Theorem 8.4.3 365 
Suppose w /= 0. Dividing across this relation by δi|w| and passing to the limit as 
i → 0 yields 
|w| ≤ k|p| (8.6.17) 
On the other hand, (8.6.17) is obviously true if w = 0. We have proved (c) in 
the special case when (w, p) is a proximal normal to Gr � at (x, v). The fact 
that (8.6.17) continues to be satisfied when (w, p) is merely a limiting normal vector 
is a direct consequence of the fact that an arbitrary limiting normal vector can be 
approximated by a proximal normal vector at a neighbouring point. ⨅⨆
Choose η ∈ (0, 1/2) such that (G3)' is satisfied, with γ = (1 − 2η). Fix N>R. 
Define 
EN (t) := {e ∈ Ω0(t) ∩ B(t) : R ≤ |e| ≤ N},
in which Ω0(t) is the regular velocity set relative to x¯ ≡ 0 and B, defined in (8.4.5), 
namely 
Ω0(t) := {e ∈ F (t, x(t)) ¯ : F (t, .) is pseudo-Lipschitz continuous near (x(t), e) ¯ }.
Making use of the compactness of the closed ball N B, we can construct a 
multifunction EN
discrete : [S,T ] ⇝ Rn such that 
(a): EN
discrete(t) is an empty or finite set for each t ∈ [S,T ], 
(b): EN
discrete is measurable, 
(c): EN
discrete(t) ⊂ EN (t) ⊂ EN
discrete(t) + N−1 B,
for each t ∈ [S,T ] such that EN (t) /= ∅ , 
(d): EN
discrete(t) ⊂ EN+1
discrete(t) for all integers N>R. 
Define 
θ (t) :=
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
inf{|e − e'
| : e, e' ∈ EN
discrete(t) and e /= e'
}
∧ inf{d∂B(t)(e) :}{ e ∈ EN
discrete(t)}
if EN
discrete(t) contains at least two elements,
+∞ otherwise .
(θ (t) is the minimum distance between distinct points in EN
discrete(t), when this set 
contains two or more points. Since EN
discrete(t) is a finite set, θ (t) > 0 for each 
t ∈ [S,T ].) 
Take δ ∈ (0, ηR). Define 
Eδ
regular(t) := {e ∈ EN
discrete(t) : F (t, .) is pseudo-Lipschitz continuous
near (x(t) ¯ = 0, e) (with parameters
ϵ ≥ δ, R ≥ δ and k ≤ δ−1) and θ (t) ≥ δ}.366 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
We see that Eδ
regular(t) = ∅ if θ (t) < δ. Observe also that Eδ
regular(t) ⊂
EN
discrete(t) ⊂ NB. Define 
Dδ
(t) := (1 − η)RB ∪ {e ∈ e0 +
1
3
δB : e0 ∈ Eδ
regular(t)},
in which the right side is interpreted as (1 − η)RB if Eδ
regular(t) = ∅. 
Since the distance between distinct points in Eδ
regular(t) is at least δ and δ ≤ η R, 
Dδ(t) is a finite union of disjoint, closed balls. These comprise (1 − η)R B and 
elements from a (possibly empty) collection of disjoint closed balls each with origin 
lying outside R ◦
B. Observe also that Dδ(t) ⊂ B(t) for a.e. t ∈ [S,T ]. 
Consider next the multifunction S : [S,T ] × Rn ⇝ Rn
S(t, x) := {(χ (|e|)e : e ∈ F (t, x)}, (8.6.18) 
in which χ : [0,∞) → [1,∞) is the function 
χ (d) := 1 +
4(1 − η)
ηR [d − (1 − η)R]
+ .
(φ+(d) := max{φ(d), 0} is the positive part of the function φ.) 
Lemma 8.6.4 The multifunction S takes values closed non-empty sets. 
Proof Fix (t, x) ∈ [S,T ] × Rn. The set S(t, x) is non-empty because F (t, x) is 
non-empty. To see that it is closed, take a sequence {ei} in F (t, x) such that the 
sequence {χ (|ei|)ei} converges in Rn. Because χ ≥ 1, this implies the {ei} is a 
bounded sequence. We can arrange then, by extraction of a subseqence, that ei → ¯e, 
for some e¯, which is an element in F (t, x), because the latter set is closed. Since χ
is continuous, χ (|ei|)ei → χ (| ¯e|)e¯, which lies in S(t, x). So S(t, x) is closed. ⨅⨆
The function ρS : [S,T ] × Rn × Rn → [0, +∞), interpreted as a modified 
penalty integrand, is defined to be 
ρS(t, x, v) := 
dS(t,x)(v) if |v| ≤ (1 − η)R ,
dF (t,x)(v) if |v| > (1 − η)R .
The modified penalty integrand has explicit representation 
ρS(t, x, v) =

inf{|v − χ (|e|)e| : e ∈ F (t, x)} if |v| ≤ (1 − η)R
inf{|v − e| : e ∈ F (t, x)} if |v| > (1 − η)R .
We shall make use of the following properties of ρS (listed in Lemma 8.6.5). 
These include the fact that ρS(t, ., .) coincides with ρF (t, ., .) on a neighbourhood 
of (x(t), ¯ ˙
x(t)) ( ¯ = (0, 0)), for a.e. t. (This is property (v).) Of course ρS ≥ 0; also, 
according to property (iii), for v confined to a suitable region, ρS(t, x, v) = 0 only8.6 Proof of Theorem 8.4.3 367 
if v ∈ F (t, x). This is a basic requirement of a useful penalty integrand for the 
velocity constraint. Notice, finally, the crucial Lipschitz continuity property (iv) on 
a ball about x(t) ¯ ≡ 0 uniformly w.r.t. t, which is, indeed, the raison d’être for this 
particular choice of modified penalty integrand. 
Lemma 8.6.5 Let η ∈ (0, 1/2) be such that (G1)–(G2) and (G3)' are satisfied 
with parameters γ = (1 − 2η), R and ϵ and integrable Lipschitz bound k. Take 
δ ∈ (0, η R). Then the function ρS (whose definition depends on R and η) has the 
following properties: 
(i): ρS(., x, v) is L-measurable for each (x, v) ∈ Rn × Rn, 
(ii): Take any (t, x) ∈ [S,T ] × Rn and v ∈ Dδ(t). Then ρS(t, x, .) is Lipschitz 
continuous with Lipschitz constant 1 on a relative neighbourhood of v in 
Dδ(t). If additionally we assume that v ∈ int{Dδ(t)}, ρS(t, x, v) > 0 and 
ξ ∈ ∂vρS(t, x, v), for some ξ ∈ Rn, then |ξ | = 1, 
(iii): For any x ∈ ϵ B and v ∈ Dδ(t) and a.e. t ∈ [S,T ], ρS(t, x, v) = 0 if and 
only if v ∈ F (t, x), 
(iv): For any t ∈ [S,T ], v ∈ Dδ(t), ρS(t, ., v) is Lipschitz continuous on ϵ˜B with 
Lipschitz constant k(t) ˜ , a.e. t ∈ [S,T ]. Here ϵ˜ and the integrable function k˜
are 
ϵ˜ =
1
4
δ2

∧ ϵ and k(t) ˜ = δ−1 ∨

1 +
12 × (1 − η)
η

k(t) , (8.6.19) 
(v): ρS(t, x, v) = ρF (t, x, v) for all x ∈ min{ϵ, 1
8k(t)(1 − η)R}B and v ∈ 1
8 (1 −
η)R B, a.e. t ∈ [S,T ]. 
In view of Lemma 8.6.3 and since (0, 0) ∈ Gr F (t, .) for a.e. t ∈ [S,T ], (v) 
implies 
(v)(a): ρS(t, ., v) is Lipschitz continuous on min{ ˜ϵ, 1
8k(t)(1 − η)R} B with 
Lipschitz constant k(t), for all v ∈ 1
8 (1 − η)R B, a.e. t ∈ [S,T ], 
(v)(b): If (w, p) ∈ ∂x,vρS(t, 0, 0) then (w, p) ∈ NGr F (t,.)(0, 0) and |w| ≤
k(t)|p|, for a.e. t ∈ [S,T ]. 
Proof Take any (t, x, v) ∈ [S,T ] × Rn × Rn. Since ρS(t, x, v) is the distance to 
some non-empty, closed set (either S(t, x) or F (t, x)), we know that there exists a 
point e¯ ∈ F (t, x) achieving the infimum in the definition of ρ(t, x, v). We shall use 
this fact below. 
Property (i) can be deduced from the continuity of χ and the L×Bn measurability 
of F. Consider (ii). Dδ(t) is the union on two disjoint sets D1 := (1 − η)RB and 
D2 := ∩N
j=1(ej + 1
3 δB), in which {e1,...,eN } is a finite collection of points in 
{e : |e| ≥ R}. Moreover, ρS(t, x, v) = dS(t,x)(v) for v ∈ D1 and ρS(t, x, v) =
dF (t,x)(v) for v ∈ D2. (ii) then follows from basic properties of the distance function 
to some non-empty, closed set (either S(t, x) or F (t, x)). 
Consider (iii). Fix (t, x) ∈ [S,T ] × ϵ B and v ∈ Dδ(t). Recall that Dδ(t) is 
expressible as a union of closed disjoint sets D1 = (1 − η)R B (on which ρS(t, x, .)
coincides with dS(t,x)(.) ) and a closed set D2 (on which ρS(t, x, .) coincides with368 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
dF (t,x)(.)). If |v| > (1 − η) R B then ρS(t, x, v) = dF (t,x)(v) and so v ∈ F (t, x)
if and only if ρS(t, x, v) = 0. So suppose |v| ≤ (1 − η)R. If ρS(t, x, v) = 0 then 
v = χ (|e|)e, for some e ∈ F (t, x). Since χ ≥ 1 and |v| ≤ (1 − η)RB we know 
e ∈ (1 − η)RB. But then v ∈ F (t, x). On the other hand, if v ∈ F (t, x) then 
ρS(t, x, v) ≤ |v −χ (|v|)v|=|v −v| = 0, since |v| ≤ (1−η)R. So ρS(t, x, v) = 0. 
Consider (iv). Take (t, x) ∈ [S,T ] ×ϵ B and v ∈ Dδ(t). We show that ρS(t, ., v)
is Lipschitz continuous on ϵ˜B, with Lipschitz constant k(t) ˜ , where ϵ >˜ 0 and k˜ are 
as in the lemma statement. 
Suppose first that |v| > (1 − η)R. In this case, v ∈ e0 + 1
3 δB, for some 
e0 ∈ Eδ
regular(t) and ρS(t, ., v) = dF (t,.)(v). Since F (t, .) is pseudo-Lipschitz 
continuous near (0, e0) with ϵ, R and k parameters taken to be δ, δ and δ−1
respectively, we know from Lemma 8.6.3 (for K = k = δ−1) that ρF (t, ., v) is 
Lipschitz continuous on ( 1
4 δ2 ∧ ϵ)B , with Lipschitz constant δ−1. ρS(t, ., v) =
ρF (t, ., v) is therefore Lipschitz continuous on ϵ˜ B with Lipschitz constant k(t) ˜ . 
We may assume then that v ∈ (1 − η)R B. Let e¯ ∈ F (t, x) achieve the infimum 
in the definition of ρS(t, x, v). Such a point exists, in consequence of Lemma 8.6.4. 
Claim: | ¯e| < R. 
Assume, to the contrary, that | ¯e| ≥ R. Then, from the definition of χ, χ (| ¯e|) ≥
1+4(1−η). From (G3)'
, there exists e0 ∈ F (t, x)∩(1−2η) R B. Then χ (|e0|) = 1. 
We have 
(1 + 4(1 − η))R ≤ |χ (| ¯e|)e¯|≤|v − χ (| ¯e|)e¯|+|v|
≤ |v − χ (|e0|)e0| + (1 − η)R = |v − e0| + (1 − η)R
≤ |v|+|e0| + (1 − η)R = (3 − 4η)R .
Since R > 0, this is a contradiction. We have verified the claim. 
Take any t ∈ [S,T ], x, x' ∈ ϵB and v ∈ (1 − η)R B. Let e ∈ F (t, x) and 
e' ∈ F (t, x'
) achieve the minimum in the definition of ρS(t, x, v) and ρS(t, x'
, v). 
We know from the earlier verified ‘claim’ that |e| < R and |e'
| < R. 
From (G3)'
(a), we can find e1 ∈ F (t, x'
) such that 
|e − e1| ≤ k(t)|x − x'
| .
If k(t)|x−x'
| ≥ (2−η)R, then, by (G3)'
(b), we can find e2 ∈ F (t, x'
)∩(1−2η)R B. 
Then 
|e − e2|≤|e|+|e2| < (2 − η)R ≤ k(t)|x − x'
| .
Now choose e˜ ∈ F (t, x'
) to be 
e˜ =

e1 if k(t)|x − x'
| < (2 − η)R
e2 if k(t)|x − x'
| ≥ (2 − η)R .8.6 Proof of Theorem 8.4.3 369 
From the preceding relations 
|e − ˜e| ≤ (2 − η)R ∧ k(t)|x − x'
| .
Notice that χ is Lipschitz continuous with Lipschitz constant 4(1 − η)/(ηR) and 
| ˜e|≤|e − ˜e|+|e| ≤ (3 − η)R .
We see also that χ (|e|) ≤ 1 + 4(1 − η)/R, since |e| < R. It follows from these 
relations that: 
ρS(t, x'
, v) − ρS(t, x, v) ≤ |v − χ (| ˜e|)e˜|−|v − χ (|e|)e|
≤ |χ (| ˜e|)e˜ − χ (|e|)e|
≤ |χ (| ˜e|)e˜ − χ (|e|)e˜|+|χ (|e|)e˜ − χ (|e|)e|
≤ |χ (| ˜e|) − χ (|e|)||˜e|+|χ (|e|)|| ˜e − e|
≤ k1(t)|x − x'
| ,
where k1 is the integrable function k1(t) =

1 + 12×(1−η)
η

k(t) ≤ k(t) . ˜
Since the roles of x and x' can be interchanged and x, x' are arbitrary points in 
ϵ B, we have shown that ρ(t, ., v) is Lipschitz continuous on ϵ B, with Lipschitz 
constant k(t) ˜ . We have confirmed (iv). 
Consider (v). Fix t ∈ [S,T ] such that the conditions in (G3)' is satisfied. Take 
any x ∈ min{ϵ, 1
8 (1 − η)Rk−1(t))}B and v ∈ 1
8 (1 − η)RB. It follows from (G3)'
(a) 
that there exists e0 ∈ F (t, x) such that |e0| ≤ k(t)|x| ≤ 1
8 (1 − η)R. In view of 
the preceding inequality, χ (|e0|) = 1. But then, because ρS(t, x, .) has Lipschitz 
constant 1 on (1 − η)R B, 
ρS(t, x, v) ≤ |v| + ρS(t, x, 0) ≤
1
8
(1 − η)R + |χ (|e0|)e0| =
1
4
(1 − η)R.
Since the infimum in the definition of ρS(t, x, v) is achieved, there exists e(1) ∈
F (t, x) such that 
|v − χ (|e(1)
|)e(1)
| = ρS(t, x, v) ≤
1
4
(1 − η)R .
But then |χ (|e(1)
|)e(1)
| ≤ 5
8 (1 − η)R. It follows that χ (|e(1)
|) = 1. Then 
ρS(t, x, v) = |v − χ (|e(1)
|)e(1)
|=|v − e(1)
| ≥ dF (t,x)(v) . (8.6.20) 
But the infimum in the definition of dF (t,x)(v) is also achieved. So there exists e(2) ∈
F (t, x) such that370 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
dF (t,x)(v) = |v − e(2)
|≤|v − e0|≤|v|+|e0| ≤
1
4
(1 − η)R .
Then |e(2)
| ≤ 5
8 (1 − η)R. Consequently, χ (|e(2)
|) = 1 and 
dF (t,x)(v) = |v − e(2)
|=|v − χ (|e(2)
|)e(2)
| ≥ ρS(t, x, v) . (8.6.21) 
Equations (8.6.20) and (8.6.21) combine to give ρS(t, x, v) = dF (t,x)(v). We have 
confirmed (v). The assertions (v)(a) and (v)(b) follow directly from (v) in the light 
of Lemma 8.6.3 and since (0, 0) ∈ Gr F (t, .). ⨅⨆
Step 4 (Completion of the Proof). 
Recall that, under the supplementary hypotheses, the W1,1 local minimizer for (P )
of interest is x¯ ≡ 0. Let β ∈ (0, ϵ) be such that x¯ is minimizing w.r.t. all admissible 
F trajectories x such that ||x − ¯x||W1,1 (= ||x||W1,1 ) ≤ β and x(t) ˙ ∈ ˙
x(t) ¯ +B(t) (=
B(t)) a.e. t ∈ [S,T ]. 
Take η ∈ (0, 1/2) such that the conditions in (G3)' are satisfied with γ = (1 −
2η). For t ∈ [S,T ] and e ∈ Dδ(t) define 
φ(t, e) :=  1
2 (|e| − (1 − 2η)R) ∨ 0 if e ∈ (1 − η)RB
1
2 (|e − e0| − δ/6) ∨ 0 if e ∈ e0 + 1
3 δ B for some e0 ∈ Eδ
regular(t) .
Note the following properties of φ, each of which is a simple consequence of the 
definition of this function: for any t ∈ [S,T ]
φ(t, e) = 0 if |e| ≤ (1 − 2η)R
φ(t, .) is locally Lipschitz continuous on Dδ(t)
with Lipschitz constant 1/2,
φ(t, e) ≥ ηR
2 ∧ δ
12 if e ∈ ∂Dδ(t) .
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(8.6.22) 
Take αi ↓ 0 and, for each i, consider the optimization problem: 
(Pi)
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
Minimize 
�i(x(S)), x(T )
∨
  T
S ρS(t, x(t), x(t))dt ˙

+  T
S φ(t, x(t))dt ˙
over arcs x ∈ W1,1 such that
x(t) ˙ ∈ Dδ(t) a.e. t ∈ [S,T ] ,
|x(S)| + 
[S,T ] | ˙x(t)|dt ≤ β .
Here, 
�i(x0, x1) := 
g(x0, x1) − g(0, 0) + α2
i

∨ dC(x0, x1)8.6 Proof of Theorem 8.4.3 371 
and ρS(t, x, v) is the modified penalty integrand of Step 3, with parameters η ∈
(0, 1/2) and δ > 0 as earlier chosen. 
We can formulate this problem as 
Minimize {Ji(x) : x ∈ S}
in which 
S := {x ∈ W1,1 : x satisfies the constraints of (Pi)}
and 
Ji(x) := 
�i(x(S)), x(T ))
∨
  T
S ρS(t, x(t), x(t))dt ˙

+  T
S φ(t, x(t))dt. ˙
S is complete w.r.t. the metric induced by the ||.||W1,1 norm on elements of S. Ji
is continuous w.r.t. to this metric. We see that x¯ ≡ 0 is an α2
i minimizer for (Pi). By 
Ekeland’s theorem, we may find xi ∈ S such that xi is a minimizer for 
Minimize {Ji(x) + αi

|x(S) − xi(S)| + 
[S,T ]
| ˙x(t) − ˙xi(t)|dt : x ∈ S}
and 
|xi(S)| + 
[S,T ]
| ˙xi(t)|dt ≤ αi . (8.6.23) 
Observe that 

�i(xi(S)), xi(T ))
∨
  T
S
ρS(t, xi(t), x˙i(t))dt
> 0 (8.6.24) 
for, otherwise, we would have 

g(xi(S), xi(T )) − g(0, 0) + α2
i

∨ dC(xi(S), xi(T ))
∨
  T
S
ρS(t, xi(t), x˙i(t))dt
= 0 .
This implies ρS(t, xi(t), x˙i(t)) = 0 a.e.. But then x(t) ˙ ∈ F (t, x(t)) a.e., in 
consequence of Lemma 8.6.5. The relation also tells us dC(xi(S), xi(T )) = 0, from 
which we conclude (xi(S), xi(T )) ∈ C, and also, g(xi(S), xi(T )) ≤ g(0, 0) − α2
i . 
Since xi ∈ S, we know that ||xi||W1,1 ≤ β. But then xi is an admissible F trajectory, 
with x˙i(t) ∈ B(t) for a.e. t ∈ [S,T ], that violates the W1,1 local optimality of x¯ ≡ 0. 
Equation (8.6.24) is confirmed.372 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
For i sufficiently large, ||xi||W1,1 < β. It follows that xi is a W1,1 local minimizer 
for the problem 
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
Minimize �i(x(S), x(T )) ∨
  T
S ρS(t, x(t), x(t))dt ˙

+ αi

|x(S) − xi(S)| +  T
S | ˙x(t) − ˙xi(t)|dt
+  T
S φ(t, x(t))dt ˙
over x ∈ W1,1 satisfying
x(t) ˙ ∈ Dδ(t) a.e. t ∈ [S,T ] .
This problem has the structure of the finite Lagrangian problem introduced in 
Step 1, a problem for which Proposition 8.6.1 provides necessary conditions of 
optimality. It is easy to check that the hypotheses for application of the proposition 
are satisfied, since φ(t, .) is Lipschitz continuous, uniformly w.r.t. t and in view of 
the properties of ρS established in Lemma 8.6.5. 
Using the max rule to estimate ∂�i, we deduce that there exist pi ∈ W1,1, 
λ¯i ∈ [0, 1] and γi ∈ [0, 1] which satisfy 
(A)'
: p˙i(t) ∈ co{η : (η, pi(t)) ∈ (1 − λ¯i)∂x,vρS(t, xi(t), x˙i(t))+{0} × αiB}
for a.e. t ∈ [S,T ] such that x˙i(t) ∈ (1 − 2η)R ◦
B
(B)'
: (pi(S), −pi(T )) ∈ λ¯iγi∂g(xi(S), xi(T ))
+ λ¯i(1 − γi)∂dC(xi(S), xi(T )) + αiB × {0}, 
(C)'
: pi(t) · ˙xi(t) − (1 − λ¯i)ρS(t, xi(t), x˙i(t)) − φ(t, x˙i(t)) ≥
pi(t) · v − (1 − λ¯i)ρS(t, xi(t), v) − φ(t, v) − αi|v − ˙xi(t)|,
for all v ∈ Dδ
(t), a.e. t ∈ [S,T ].
Observe however that 
λ¯i(1 − γi)∂dC(xi(S), xi(T )) = λ¯i(1 − γi)

∂dC(xi(S), xi(T )) ∩ ∂B

,
in which we interpret the right side as {0}, if λ¯i(1 − γi) = 0. This relation is 
true when dC(xi(S), xi(T )) > 0 since, in this case, ∂dC(xi(S), xi(T )) ⊂ ∂B, by 
basic properties of the distance function. On the other hand, it is also true when 
dC(xi(S), xi(T )) = 0 because, then, λ¯i(1 − γi) = 0 in view of (8.6.1), (8.6.24) 
and the max rule for limiting subdifferentials. But then (B)'
can be replaced by the 
stronger condition: 
(pi(S), −pi(T )) ∈ λ¯iγi∂g(xi(S), xi(T )) + λ¯i(1 − γi)

∂dC(xi(S), xi(T )) ∩ ∂B

+ αiB × {0}.8.6 Proof of Theorem 8.4.3 373 
Now write λi := λ¯iγi, λ(1)
i = (1−λ¯i) and λ(2)
i = λ¯i(1−γi). These non-negative 
numbers satisfy 
λi + λ(1)
i + λ(2)
i = 1.
In view of the preceding observations we can write (A)'
-(C)' as 
(A): p˙i(t) ∈ co{η : (η, pi(t)) ∈ λ(1)
i ∂x,vρS(t, xi(t), x˙i(t))+{0} × αiB}
for a.e. t ∈ [S,T ] such that x˙i(t) ∈ (1 − 2η)R ◦
B ,
(B): (pi(S), −pi(T )) ∈ λi∂g(xi(S), xi(T ))
+ λ(2)
i

∂dC(xi(S), xi(T )) ∩ ∂B

+ αiB × {0},
(C): pi(t) · ˙xi(t) − λ(1)
i ρS(t, xi(t), x˙i(t)) − φ(t, x˙i(t)) ≥
pi(t) · v − λ(1)
i ρS(t, xi(t), v) − φ(t, v) − αi|v − ˙xi(t)|,
for all v ∈ Dδ
(t), a.e. t ∈ [S,T ].
Proposition 8.6.1 also supplies the information that p˙i(t) ∈ λ(1)
i co ∂xρS(t, xi(t),
x˙i(t)) a.e.. Since ρS(t, ., x˙i(t)) is Lipschitz continuous with Lipschitz constant k(t) ˜ , 
where k˜ is the integrable Lipschitz constant of Lemma 8.6.5 (iv), we have 
| ˙pi(t)| ≤ λ(1)
i k(t) ˜ a.e. t ∈ [S,T ]. (8.6.25) 
From (B) we know that |pi(S)| ≤ kg + 1 + αi. It follows that the family of 
costate trajectories pi, i = 1, 2,... are uniformly bounded and their derivatives are 
uniformly integrably bounded. 
Our aim now is to establish a uniform positive lower bound on the magnitude 
of (pi, λi). We need to consider separately two possible cases, one of which must 
occur: 
(i): ρS(t, xi(t), x˙i(t)) = 0 a.e., 
(ii): ρS(t, xi(t), x˙i(t)) > 0 on a set of positive L-measure. 
Consider case (i). In consequence of (8.6.1), (8.6.24) and the max rule, we know 
that λi + λ(2)
i = 1. It follows then from condition (B) that 
|(pi(S), pi(T ))|≥−λikg + λ(2)
i − αi = 1 − (1 + kg)λi − αi.
Hence 
√
2||pi||L∞ + (1 + kg)λi ≥ 1 − αi .374 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
Consider case (ii). In this case there is a time t ∈ [S,T ] such that condition 
(C) is satisfied, ρS(t, xi(t), x˙i(t)) > 0 and x˙i(t) ∈ Dδ(t). We see that x˙i(t) /∈
F (t, xi(t)). This is obviously true if | ˙xi(t)|>(1 − η)R because, in this case, 
ρS(t, xi(t), x˙i(t)) = dF (t,xi(t))(x˙i(t))). So we can assume that | ˙xi(t)|≤(1 − η)R
and so χ (| ˙xi(t)|) = 1. If, contrary to our assertion, x˙i(t) ∈ F (t, xi(t)), it would 
follow that ρS(t, xi(t), x˙i(t)) ≤ |˙xi(t) − χ (| ˙xi(t)|)x˙i(t)| = |˙xi(t) − ˙xi(t)| = 0, 
which is a contradiction. 
To proceed with studying this case, we must distinguish two situations: 
(a): x˙i(t) ∈ ∂Dδ(t). 
In view of (G3)'
(b), there exists v0 ∈ F (t, xi(t)) such that |v0| ≤ (1 − 2η)R. Then 
φ(t, v0) = 0 and ρS(t, xi(t), v0) ≤ |v0 − χ (|v0|)v0|=|v0 − v0|) = 0. Since 
x˙i(t) ∈ ∂Dδ(t), 
it follows from (8.6.22) that 
φ(t, x˙i(t)) ≥ ηR
2
∧
δ
12 .
Using these relations, noting that ρS(t, xi(t), x˙i(t)) ≥ 0 and that | ˙xi(t)| ≤ N +
δ/3, and inserting v = v0 in condition (C) yields the inequality 
|pi(t)||˙xi(t) − v0| ≥ λ(1)
i ρS(t, xi(t), x˙i(t)) − 0 + φ(t, x˙i(t))
−0 − αi(N + δ/3 + (1 − 2η)R) .
≥ φ(t, x˙i(t)) − αi(N + δ/3 + (1 − 2η)R)
≥
ηR
2
∧
δ
12

− αi(N + δ/3 + (1 − 2η)R) .
We know | ˙xi(t)| ≤ N + δ/3 and |v0| ≤ (1 − 2η)R. It follows that 
||pi||L∞ ≥ (N + δ/3 + (1 − 2η)R)−1
ηR
2
∧
δ
12

− αi .
(b): x˙i(t) ∈ ◦
Dδ(t). 
Now, since x˙i(t) is an unconstrained local minimizer of v → −pi(t) · v +
λ(1)
i ρS(t, xi(t), v) + φ(t, v) + αi|v − ˙xi|, we have 
pi(t) ∈ λ(1)
i ∂vρS(t, xi(t), x˙i(t)) + ∂vφ(t, x˙i(t)) + αiB.
Since ρS(t, xi(t), x˙i(t)) > 0, we know from Lemma 8.6.5 that elements in 
∂vρS(t, xi(t), x˙i(t)) have unit length. Taking note also of the fact that φ(t, .)
is locally Lipschitz continuous on Dδ(t) with Lipschitz constant 1/2, whence 
∂vφ(t, x˙i(t)) ∈ (1/2)B, we deduce from the preceding relations that 
||pi||L∞ ≥ λ(1)
i − 1
2 − αi.8.6 Proof of Theorem 8.4.3 375 
From (B) we know 
√
2||pi||L∞ ≥ |(pi(S), −pi(T ))| ≥ λ(2)
i − λikg − αi = 1 − λ(1)
i − (1 + kg)λi − αi .
Adding the preceding inequalities we obtain 
(1 + √
2)||pi||L∞ + (1 + kg)λi ≥ 1 − 1
2 − 2αi = 1
2 − 2αi .
Combining the estimates relating to the cases (i), (ii)(a) and (ii)(b), we arrive at 
(1 + √
2)||pi||L∞ + (1 + kg)λi
≥ min{
1
2 − 2αi, (N + δ/3 + (1 − 2η)R)−1
ηR
2
∧
δ
12
− αi}. (8.6.26) 
This is the desired lower bound. 
We deduce from (8.6.23) that, along a subsequence, xi → ¯x ≡ 0 uniformly, and 
x˙i → ˙
x¯ ≡ 0 in L1 and a.e.. We have already observed that the p˙i’s are uniformly 
integrably bounded and the pi’s are uniformly bounded. By further restriction to a 
subsequence we can then arrange, in consequence of Ascoli’s theorem, that pi → p
uniformly and p˙i → ˙p weakly in L1. We can also arrange that λi → λ, λ(1)
i → λ(1)
and λ(2)
i → λ(2) , for some λ, λ(1)
, λ(2) ∈ [0, 1]. A convergence analysis along the 
lines of the proof of Proposition 8.6.1, permits us to pass to the limit in conditions 
(A)-(C) and thereby to obtain: 
(1 + √
2)||p||L∞ + (1 + kg)λ
≥ min{
1
2
, (N + δ/3 + (1 − 2η)R)−1
ηR
2
∧
δ
12
}, (8.6.27) 
p(t) ˙ ∈ co{η : (η, pi(t)) ∈ λ(1)
∂x,vρS(t, x(t), ¯ ˙
x(t)) ¯ } for a.e. t ∈ [S,T ],
which, according to Lemma 8.6.5 (v), implies 
p(t) ˙ ∈ co{η : (η, p(t)) ∈ NGr F (t,.)(x(t), ¯ ˙
x(t)) ¯ }, a.e. t ∈ [S,T ], (8.6.28) 
(p(S), −p(T )) ∈ λ∂g(x(S), ¯ x(T )) ¯ + NC(x(S), ¯ x(T )) , ¯ (8.6.29) 
and 
p(t)· ˙
x(t) ¯ −λ(1)
ρS(t, x(t), ¯ ˙
x(t)) ¯ −φ(t, ˙
x(t)) ¯ ≥ p(t)·v−λ(1)
ρS(t, x(t), v) ¯ −φ(t, v),
for all v ∈ Dδ(t) , a.e. t ∈ [S,T ].376 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
Observe that, for all points v ∈ (F (t, x(t)) ¯ ∩(1−2η)RB) ∪ Eδ
regular(t), we have 
v ∈ Dδ(t) and ρS(t, x(t), v) ¯ = φ(t, v) = 0. Consequently, the preceding relation 
implies 
p(t) · ˙
x(t) ¯ ≥ p(t) · v, for all v ∈ (F (t, x(t)) ¯ ∩ (1 − 2η)RB) ∪ Eδ
regular(t) .
(8.6.30) 
Lemma 8.6.5 (v)(b) provides the additional information that 
| ˙p(t)| ≤ k(t)|p(t)| for a.e. t ∈ [S,T ] . (8.6.31) 
In view of (8.6.27), we can arrange, by scaling the Lagrange multipliers, that 
||p||L∞ + λ = 1 . (8.6.32) 
Relations (8.6.28), (8.6.29), (8.6.30) and (8.6.32) combine to provide a restricted 
version of the theorem, in which the Weierstrass condition (8.6.30) is affirmed for 
velocities only in a subset of co (Ω0(t) ∩ B(t)). 
To derive the full Weierstrass condition, take a sequence δi ↓ 0. Then 
conditions (8.6.28), (8.6.29), (8.6.30) and (8.6.32) are valid for each i, for some 
Lagrange multipliers (p, λ) that we now label (pi, λi). From (8.6.31) and (8.6.32) 
we know the pi’s are uniformly bounded and have uniformly integrable derivatives. 
We may therefore arrange, by extracting subsequences, that pi → p (uniformly), 
p˙i → ˙p in the weak L1 topology and λi → λ, for some p ∈ W1,1 and λ ≥ 0. 
Relations (8.6.32), (8.6.29) and (8.6.28) (now expressed in terms of (pi, λi)) 
are preserved in the limit as i → ∞ (with multipliers (p, λ)). We now attend to 
the implications of relation (8.6.30). Let S ⊂ [S,T ] be the subset on which the 
Weierstrass condition (8.6.30), in which (pi, λi) replaces (p, λ), is satisfied for all 
values of i. S ⊂ [S,T ] is a subset of full measure. Take any t ∈ S. Notice that 
Eδi
regular(t), i = 1, 2,..., is an increasing sequence of sets and 
lim
i
Eδi
regular(t) = EN
discrete(t) .
We deduce from (8.6.30), in the limit, that p(t) · ˙
x(t) ¯ ≥ p(t) · v, for all v ∈
(F (t, x(t)) ¯ ∩ (1 − 2η)RB)) ∪ EN
discrete(t) for all t ∈ S. 
Now take Ni ↑ ∞ and ηi ↓ 0. We know that ENi
discrete, i = 1, 2,... is an 
increasing sequence of sets. Then, in consequence of the defining properties of 
EN
discrete(t), 
Ω0(t) ∩ B(t) ∩ {e ∈ Rn : |e| ≥ R} ⊂ lim
i
ENi
discrete(t) . (8.6.33) 
We also know that, for each i, F (t, x(t)) ¯ ∩ (1 − 2ηi)RB = Ω0(t) ∩ {e ∈ Rn :
|e| ≤ (1−2ηi)R}, i=1,2,. . . (this is because, for every point e ∈ F (t, x(t)) ¯ such that8.7 The Hamiltonian Inclusion for Convex Velocity Sets 377
|e| < R), F (t, .) is pseudo-Lipschitz continuous near (x(t), e) ¯ , in consequence of 
(G3)'
. Furthermore, this sequence of sets is increasing and 
Ω0(t) ∩ {e : |e| < R} ∩ B(t) = Ω0(t) ∩ {e : |e| < R}
⊂ lim
i

F (t, x(t)) ¯ ∩ {e ∈ Rn : < (1 − 2ηi)R}

. (8.6.34) 
(To obtain the first set identity we have used the fact that R
◦
B ⊂ B(t) a.e..) A 
similar analysis to that undertaken above ensures p can be chosen such that, for a.e. 
t ∈ [S,T ], 
p(t) · ˙
x(t) ¯ ≥ p(t) · v, (8.6.35) 
for all v ∈ lim
i

F (t, x(t)) ¯ ∩ {e ∈ Rn : < (1 − 2ηi)R}

∪ lim
i
ENi
discrete(t) .
But then, by (8.6.33) and (8.6.34), (8.6.35) is valid for all v ∈ Ω0(t) ∩ B(t). 
Since v → p(t) · v is a linear function, we may deduce that (8.6.35) is valid for 
all v ∈ co (Ω0(t) ∩ B(t)). This is the full Weierstrass condition of the theorem 
statement. The proof is concluded. ⨅⨆
8.7 The Hamiltonian Inclusion for Convex Velocity Sets 
We have seen how the Euler Lagrange condition from the classical calculus of 
variations generalizes to allow for a constraint on the velocity set, in the form of 
a differential inclusion. We show now that Hamilton’s system of equations also 
generalizes to this broader setting. The new condition will be called the Hamiltonian 
inclusion. Attention, for the present, is limited to problems in which the velocity sets 
are convex. For such problems, the Hamiltonian inclusion is implied by the earlier 
derived Euler Lagrange inclusion, in consequence of the following theorem. 
Theorem 8.7.1 (Dualization Theorem) Take a function L˜ : Rn × Rm → R ∪
{+∞} and points (x,¯ v)¯ ∈ dom L˜ and p¯ ∈ Rm. Let H (x, .) ˜ to be the conjugate 
function of L(x, .) ˜ , i.e. 
H (x, p) ˜ := sup{p · v − L(x, v) ˜ : v ∈ Rm}.
Assume that, for some neighbourhoods U, V and W of x¯, v¯ and L(˜ x,¯ v)¯ , the 
following hypotheses are satisfied: 
(H1): L(x, .) ˜ is convex for each x ∈ U, 
(H2): L(., .) ˜ is lower semi-continuous, 
(H3): there exists k > 0 such that 
epiL(x ˜ '
, .) ∩ (V × W ) ⊂ epiL(x ˜ '', .) + k|x' − x''|B
for all x'
, x'' ∈ U.378 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
Then 
{q : (q, p)¯ ∈ ∂L(˜ x,¯ v)¯ } ⊂ co{q : (−q, v)¯ ∈ ∂H (˜ x,¯ p)¯ }. (8.7.1) 
A proof of the theorem is given in the Appendix at the end of the chapter. 
Remarks 
1: Under the hypotheses of this theorem, it is not guaranteed that the function H˜
is lower semi-continuous. The limiting subdifferential ∂H (˜ z)¯ at the point z¯ =
(x,¯ p)¯ ∈ dom H was earlier defined only in the case H is lower semi-continuous. 
In the present context, ∂H (˜ z)¯ is taken to be the set of elements ξ having the 
following properties: there exist sequences of positive numbers {ϵi} and {σi} and 
convergent sequences zi
H
→ ¯z and ξi → ξ such that, for each i, 
H (z) ˜ − H (z ˜ i) ≥ ξi · (z − zi) − σi|z − zi|
2 for all z ∈ zi + ϵiB.
2: For purposes of dualizing the Euler Lagrange inclusion, only the set inclu￾sion (8.7.1) is required. We shall not use this fact here, but we note Rockafellar 
[176] has shown that under the hypotheses of Theorem 8.7.1, supplemented by 
the condition 
L˜ is epicontinuous,
the two sets considered do in fact coincide. (Epicontinuity is defined in the 
Appendix.) That is, 
co{q : (q, p)¯ ∈ ∂L(˜ x,¯ v)¯ } = co{q : (−q, v)¯ ∈ ∂H (˜ x,¯ p)¯ }.
Return now to problem (P ), posed in Sect. 8.4. Let x¯ be a W1,1 local minimizer 
relative to the multifunction B. We set L(x, v) ˜ := ΨGr F (t,.)(x, v), then the function 
H˜ in the theorem statement coincides with H (t, ., .), where H : [S,T ]×Rn×Rn →
R is the Hamiltonian function earlier employed in this chapter, namely: 
H (t, x, p) := sup
v∈F (t,x)
p · v .
It turns out that, for this identification of L˜ in Theorem 8.4.3, the hypotheses 
placed on L˜ in the dualization theorem are satisfied, for almost every t ∈ [S,T ], 
under the hypotheses invoked in Theorem 8.4.3, when we make the additional 
assumption that F takes values convex sets. The same is true of the earlier stated 
Corollary 8.5.2 of Theorem 8.4.3. This means that, for problems with convex 
velocity sets, we can supplement the Euler Lagrange inclusion by the partially 
convexified Hamiltonian inclusion 
p(t) ˙ ∈ co{−q : (q, ˙
x(t)) ¯ ∈ ∂H (t, x(t), p(t)) ¯ } a.e..8.7 The Hamiltonian Inclusion for Convex Velocity Sets 379
Notice that, when F takes values convex sets, we can also replace the Weierstrass 
condition in Theorem 8.4.3 
p(t) · ˙
x(t) ¯ ≥ p(t) · v, for all v ∈ co (Ω0(t) ∩ (˙
x(t) ¯ + B(t))) a.e. t ∈ [S,T ]
(8.7.2) 
by the seemingly stronger (but in fact equivalent) unrestricted Weierstrass condition 
p(t) · ˙
x(t) ¯ ≥ p(t) · v, for all v ∈ F (t, x(t)) , ¯ a.e. t ∈ [S,T ] . (8.7.3) 
To see this, suppose that (8.7.3) were not true. Then there would exist a subset of 
[S,T ] of positive measure on which (8.7.2) is satisfied but (8.7.3) is violated. For 
any time t in this set there exists a point v¯ ∈ F (t, x(t)) ¯ such that p(t) · ¯v > p(t) · ˙
x(t) ¯ . But, under the hypotheses of the Theorem 8.4.3, we know that the set Ω0(t) ∩
(˙
x(t) ¯ + B(t)) contains the neighbourhood of ˙
x(t) ¯ , ˙
x(t) ¯ + R(t) ◦
B. It is therefore 
possible to choose σ ∈ (0, 1) sufficiently small that v' := ˙
x(t) ¯ + σ (v¯ − ˙
x(t)) ¯ ∈
˙
x(t) ¯ +R(t) ◦
B. Since (by convexity of F (t, x(t)) ¯ ) v' ∈ F (t, x(t)) ¯ , it follows that v' ∈
F (t, x(t)) ¯ ∩Ω0(t)∩(˙
x(t) ¯ +B(t)) . But then, from (8.7.2), 0 ≥ p(t)·v'
−p(t)· ˙
x(t) ¯ =
σ (p(t) · ¯v − p(t) · ˙
x(t)) ¯ , which implies p(t) · ¯v − p(t) · ˙
x(t) ¯ ≤0 (see Exercise 4.2). 
This contradiction confirms the unrestricted Weierstrass condition (8.7.3). 
The following theorem is a direct consequence of these observations. It tells us 
that, under the additional hypothesis that F takes values convex sets, the assertions 
of Theorem 8.4.3 can be supplemented by the partially convexified Hamiltonian 
inclusion and the information that the Weierstrass condition is satisfied over the 
entire velocity set F (t, x(t)) ¯ . 
Theorem 8.7.2 (The Partially Convexified Hamiltonian Inclusion for Convex 
Velocity Sets) Take a measurable multifunction B : [S,T ] ⇝ Rn such that B(t)
is open for a.e. t ∈ [S,T ]. Let x¯ be a W1,1 local minimizer for (P) relative to B. 
Assume that the following hypotheses are satisfied: 
(G1): g is Lipschitz continuous on a neighbourhood of (x(S), x( ¯ T )) ¯ and C is 
closed, 
(G2): F (t, x) is nonempty for each (t, x) ∈ [S,T ] × Rn, Gr F (t, .) is closed for 
each t ∈ [S,T ] and F is L × Bn measurable, 
(G3): There exist a measurable function R : [S,T ] → (0,∞) ∪ {+∞} and ϵ > 0
such that R(t) ◦
B ⊂ B(t) a.e. and the following conditions are satisfied, 
(a): There exists k ∈ L1 such that 
F (t, x'
) ∩ (˙
x(t) ¯ + R(t) ◦
B) ⊂ F (t, x) + k(t)|x' − x|B,
for all x, x' ∈ ¯x(t) + ϵB, a.e. t ∈ [S,T ],
(b): There exist r ∈ L1(S, T ), r0 > 0 and γ ∈ (0, 1) such that r0 ≤ r(t), 
γ −1r(t) ≤ R(t) a.e. and 
F (t, x) ∩ (˙
x(t) ¯ + r(t)B) = ∅ / for all x ∈ ¯x(t) + ϵB, a.e. t ∈ [S,T ].380 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
Assume further that 
F (t, x) is convex, for all x ∈ ¯x(t) + ϵB, a.e. t ∈ [S,T ] .
Then there exist an arc p ∈ W1,1([S,T ]; Rn) and λ ≥ 0 such that 
(i): (p, λ) /= (0, 0), 
(ii): p(t) ˙ ∈ co{η : (η, p(t)) ∈ NGr F (t,.)(x(t), ¯ ˙
x(t)) ¯ }, a.e. t ∈ [S,T ], 
(iii): (p(S), −p(T )) ∈ λ∂g(x(S), ¯ x(T )) ¯ + NC(x(S), ¯ x(T )), ¯
(iv): p(t) · ˙
x(t) ¯ ≥ p(t) · v for all v ∈ F (t, x(t)), ¯ a.e. t ∈ [S,T ] . 
Condition (ii) implies 
(v): p(t) ˙ ∈ co{−ξ : (ξ , ˙
x(t)) ¯ ∈ ∂x,pH (t, x(t), p(t)) ¯ }, a.e. t ∈ [S,T ]. 
Now assume, also, that 
F (t, x) does not depend on t.
(In this case we write F (x) in place of F (t, x).) Then, in addition to the above 
conditions, there exists a constant r such that 
(vi): p(t) · ˙
x(t) ( ¯ = maxv∈F (x(t)) ¯ p(t) · v) = r a.e. t ∈ [S,T ] . 
The following corollary gives alternative hypotheses for the validity of the 
preceding necessary conditions. The assertions of the corollary are an immediate 
consequence of Proposition 8.5.1. the Hamiltonian Dualization Theorem (Theo￾rem 8.7.1) and, finally, the preceding observation that the local Weierstrass condition 
implies the global Weierstrass condition when F (t, x) is convex valued. 
Corollary 8.7.3 The assertions of Theorem 8.7.2 remain valid when hypothesis 
(G3) of Theorem 8.7.2 is replaced by either (G3)* or (G3)**. If x¯ is a W1,1 local 
minimizer (i.e. we can take B(t) = Rn) then (G3) can be replaced by (G3)***. 
Here, 
(G3)*: There exist a measurable function R : [S,T ] → (S, T ) ∪ {+∞}, strictly 
bounded away from 0, k ∈ L1 and ϵ > 0 such that 
F (t, x'
) ∩ (˙
x(t) ¯ + R(t) ◦
B) ⊂ F (t, x) + k(t)|x' − x|B,
for all x, x' ∈ ¯x(t) + ϵB, a.e.
and either 
(a): k is strictly bounded away from zero and there exists ω0 > 0 such that 
ω0k(t)B ⊂ R(t) a.e. t ∈ [S,T ],8.8 The Hamiltonian Inclusion for Non-convex Velocity Sets 381
or 
(b): There exists a modulus of continuity θ such that 
dF (t,x)(˙
x(t)) ¯ ≤ θ (|x − ¯x(t)|) for all x ∈ ¯x(t) + ϵB, a.e. t ∈ [S,T ] .
(G3)∗∗: There exist a measurable function R : [S,T ] → (S, T ) ∪ {+∞}, strictly 
bounded away from 0, k ∈ L1 and ϵ > 0 such that R(t) ◦
B ⊂ B(t), a.e. 
t ∈ [S,T ] and 
‘(w, p) ∈ NF (t, .)(x, v)’ =⇒ ‘|w| ≤ k(t)|p|’
for all x ∈ ¯x(t) + ϵ B and v ∈ R(t) ◦
B a.e. t ∈ [S,T ],
and either 
(a): k is strictly bounded away from zero and there exists ω0 > 0 such that 
such that ω0k(t) ≤ R(t) a.e. t ∈ [S,T ], 
or 
(b): There exists a modulus of continuity θ such that 
dF (t,x)(˙
x(t)) ¯ ≤ θ (|x − ¯x(t)|) for all x ∈ ¯x(t) + ¯ϵB, a.e. t ∈ [S,T ] ,
(G3)∗∗∗: There exist α > 0, ϵ > 0 and non-negative measurable functions k and β
such that k and t → β(t)kα(t) are integrable and, for each N ≥ 0, 
F (t, x'
) ∩ (˙
x(t) ¯ + NB) ⊂ F (t, x) + (k(t) + β(t)Nα)|x' − x|B,
for all x'
, x ∈ ¯x(t) + ϵB, a.e. t ∈ [S,T ]. 
8.8 The Hamiltonian Inclusion for Non-convex Velocity Sets 
We have proved necessary conditions for a W1,1 local minimizer x¯, relative to a 
multifunction B, incorporating the partially convexified Hamiltonian inclusion 
p(t) ˙ ∈ co{−ξ : (ξ , ˙
x(t)) ¯ ∈ ∂x,pH (t, x, p(t)) ¯ } a.e. t ∈ [S,T ] ,
only in the case when the velocity sets F (t, x) are convex. Are they valid, in the 
absence of convexity? An answer to this question, provided by F.H. Clarke, is 
a qualified ‘yes’; necessary conditions incorporating a Hamiltonian inclusion for 
problems with possibly non-convex velocity sets can be proved provided, first, we 
replace the partially convexified Hamiltonian inclusion by the less precise, fully 
convexified Hamiltonian inclusion and, second, we assume that x¯ is an L∞ local 
minimizer, not merely a W1,1 local minimizer.382 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
Theorem 8.8.1 (Hamiltonian Inclusion for Non-convex F ) Let x¯ be an L∞ local 
minimizer for (P ). Assume 
(G1): g is Lipschitz continuous on a neighbourhood of (x(S), ¯ x(T )) ¯ and C is 
closed, 
(G2): F (t, x) is nonempty for each (t, x) ∈ [S,T ] × Rn, Gr F (t, .) is closed for 
each t ∈ [S,T ] and F is L × Bn measurable, 
(G3)'
: there exist ϵ > 0 and integrable functions k and c such that 
F (t, x'
) ⊂ F (t, x) + k(t)|x' − x|B and F (t, x) ⊂ c(t)B
for all x'
, x ∈ ¯x(t) + ϵB, a.e. t ∈ [S,T ].
Then there exist p ∈ W1,1([S,T ]; Rn) and λ ≥ 0, such that 
(i): (p, λ) /= (0, 0), 
(ii): (− ˙p(t), ˙
x(t)) ¯ ∈ co ∂x,pH (t, x(t), p(t)) , ¯ a.e. t ∈ [S,T ] ,
(iii): (p(S), −p(T )) ∈ λ∂g(x(S), ¯ x(T )) ¯ + NC(x(S), ¯ x(T )) . ¯
Remark 
It is unnecessary to add the Weierstrass condition ((iv) of Theorem 8.7.2) to 
the above conditions of Theorem 8.8.1 because it is implied by the Hamiltonian 
inclusion (ii) of Theorem 8.8.1 (see Exercise 4.2). 
Proof We may assume, without loss of generality that the functions c and k in (G3)'
are essentially bounded. (This is because, if not, we can introduce a change of time 
variable s = σ (t) :=  t
S (k(t'
) + c(t'
))dt'
, after it has been arranged, by addition of 
a constant, that k + c are strictly bounded away from 0. This transformation renders 
the two integrable bounds essentially bounded, in the corresponding hypotheses for 
the transformed problem; we then prove the stated assertions for the transformed 
problem, which imply the same assertions for the original problem, as in the proof 
of Proposition 8.6.2.) By translation of the minimizer x¯ to the origin, we can arrange 
that x¯ ≡ 0. Reducing the size of ϵ > 0 in (G3)'
, we can also arrange that the 
L∞ local minimizer x¯ minimizes g(x(S), x(T )) over all admissible F trajectories 
satisfying ||x − ¯x||L∞ ≤ ϵ, where ϵ > 0 is the constant of hypothesis (G3)'
. Finally, 
by redefining g outside a neighbourhood of (x(S), ¯ x(T )) ¯ , we can arrange that g is 
globally Lipschitz continuous. 
Now take αi ↓ 0 and, for each i = 1, 2,..., define 
�i(x0, x1) := 
g(x0, x1) − g(0, 0) + αi

∨ dC(x0, x1)
and consider the optimization problem 
Minimize {Ji(x) : x ∈ S}8.8 The Hamiltonian Inclusion for Non-convex Velocity Sets 383
in which 
Ji(x) := �i(x(S), x(T )) +
 T
S
|x(t) − ¯x(t)|
2dt
and 
S := {x ∈ W1,2 : ˙x(t) ∈ F (t, x(t)) a.e. and ||x − ¯x||L∞ ≤ ϵ/2}.
Here W1,2 denotes the Hilbert space of absolutely continuous functions with square 
integrable derivatives on [S,T ], with the inner product 〈x, x'
〉 := x(S) · x'
(S) +
 T
S x(t) ˙ · ˙x'
(t)dt. 
Since c is essentially bounded, elements x in S, together with their derivatives 
x˙, are uniformly essentially bounded. We can therefore find constants k1 > 0 and 
k2 > 0 such that, for any x ∈ S, 
|g(x(S), x(T ))| ≤ k1, and ||x||L∞ ≤ k2||x||W1,2 .
(The last inequality is valid because the embedding W1,2([S,T ]; Rn) ⊂
L∞([S,T ]; Rn) is continuous.) 
S is nonempty, bounded and strongly closed in W1,2, and Ji is continuous on S, 
w.r.t. the strong W1,2 topology and is bounded below on S. We may therefore apply 
Stegall’s theorem (Theorem 3.5.2) to the proper, lower semi-continuous function 
Ф : W1,2 → R ∪ {+∞}: 
Ф(x) := 
Ji(x) if x ∈ S
+∞ otherwise ,
defined on the Hilbert space W1,2. We thereby obtain sequences {(di, ai)} in 
Rn × L2([S,T ]; Rn) and xi ∈ S, such that |di|
2 + ||ai||2
L2 → 0 and xi minimizes 
x → Ji(x) + di · x(S) +
 T
S
ai(t) · ˙x(t)dt
over x ∈ S for each i. The sequence xi is uniformly bounded, with uniformly 
integrably bounded derivatives. It follows that, along a subsequence, xi → x'
uniformly on [S,T ] and x˙i → ˙x'
, weakly in L1, for some x' ∈ W1,1([S,T ]; Rn). 
By optimality of xi, and bearing in mind that x¯ ≡ 0 ∈ S, we have 
αi = Ji(x)¯ ≥ �i(xi(S), xi(T )) +
 T
S
|xi(t)|
2dt + di · xi(S) +
 T
S
ai(t) · ˙xi(t)dt384 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
for each i. Passing to the limit, as i → ∞ on both sides of this relation gives 
0 ≥

g(x'
(S), x'
(T )) − g(0, 0)

∨ dC(x'
(S), x'
(T ))
+
 T
S
|x'
(t)|
2dt ≥
 T
S
|x'
(t)|
2dt .
It follows that x' ≡ 0. Notice also that, for each i, 
�i(xi(S), xi(T )) > 0 , (8.8.1) 
since, otherwise, g(xi(S), xi(T )) ≤ g(x(S), ¯ x(T )) ¯ − αi, (xi(S), xi(T )) ∈ C and 
||xi − ¯x||L∞ ≤ ϵ; these conditions violate the L∞ optimality of x¯. 
Let us now investigate the properties of xi, in the limit as i → ∞. We have 
established that xi → ¯x in L∞ and (di, ai) → (0, 0) in Rn × L2. By extracting 
subsequences (we do not re-label), we can arrange that ai → 0 a.e.. Write, for 
t ∈ [S,T ], 
zi(t) :=  t
S
(ai(s) · ˙xi(s) + |xi(s)|
2)ds .
The minimizing properties of the sequence {xi} can be expressed as follows: for 
each i sufficiently large, (xi, zi) is an L∞ local minimizer for 
⎧
⎪⎪⎨
⎪⎪⎩
Minimize �i(x(S), x(T )) + z(T ) + di · x(S)
over absolutely continuous functions (x, z) : [S,T ] → Rn+1 satisfying
(x(t), ˙ z(t)) ˙ ∈ {(e, ai(t) · e) : e ∈ F (t, x(t))}+{(0, |x(t)− ¯x(t)|
2)} a.e. t ∈ [S,T ] ,
z(S) = 0 .
Since this is a free right end-point problem, we know from the relaxation theorem 
(Theorem 6.5.2) that (xi, zi) is also an L∞ local minimizer for the relaxed version of 
the above problem, in which F (t, x) is replaced by co F (t, x). It is straightforward 
to check that the hypotheses of Theorem 8.7.2 are satisfied by the data for the relaxed 
version, with reference to the L∞ local minimizer (xi, zi) and the multifunction 
B ≡ Rn (take the parameters R ≡ 2||c||L∞, r ≡ r0 := ||c||L∞, γ = 1/2, ϵ˜ =
ϵ/4, k(t) ˜ = √2 max{k(t); |ai(t)|k(t) + 2ϵ}). In terms of the costate vector block 
elements, denoted (p, −r), the Hamiltonian is 
H (t, x, p, r) ˜ = max
e∈F (t,x)
{(p − rai(t)) · e} − r|x − ¯x(t)|
2
= H (t, x, p − rai(t)) − r|x − ¯x(t)|
2 . (8.8.2) 
Take any i. Let (pi, −ri) be the costate trajectory. Since H (t, ., ., .) ˜ does not 
depend on z, r˙i(t) = 0. But ri(T ) = 1, by the transversality condition. We conclude 
that ri ≡ 1. The Hamiltonian inclusion (condition (v) of Theorem 8.7.2) tells us that8.8 The Hamiltonian Inclusion for Non-convex Velocity Sets 385
(p˙i(t),r˙i(t)) ∈ co {(ξ , ζ ) : ((ξ , ζ ), (xi(t), zi(t)))
∈ ∂x,z,p,rH (t, x ˜ i(t), pi(t), ri(t))} a.e.. (8.8.3) 
An analysis of limiting subgradients of H (t, ., ., .) ˜ at (xi(t), pi(t), ri(t)), char￾acterized as limits of neighbouring proximal subgradients, with the help also of 
Lemma 5.1, permits us to deduce from (8.8.2) and (8.8.3) that 
(− ˙pi(t), x˙i(t)) ∈ co ∂x,pH (t, xi(t), pi(t) − ai(t))
+ 2|xi(t) − ¯x(t)|B × {0}; a.e.. (8.8.4) 
From the transversality condition and the max rule for subdifferentials we know 
that there exists λi ∈ [0, 1] such that 
(pi(S), −pi(T )) ∈ λi∂g(xi(S), xi(T )) + (1 − λi)∂dC(xi(S), xi(T )) + diB × {0}.
This relation can be strengthened to 
(pi(S), −pi(T )) ∈ λi∂g(xi(S), xi(T ))
+ (1 − λi)(∂dC(xi(S), xi(T )) ∩ ∂B) + diB × {0}, (8.8.5) 
in which the set (1 − λi)∂dC(xi(S), xi(T )) ∩ ∂B is interpreted as {0} if (1 − λi) =
0. This strengthened form of the transversality condition is obviously true in the 
case (1 − λi) = 0, in view of the above interpretation. If, on the other hand, (1 −
λi) > 0, it follows from (8.8.1) and the sum rule that dC(xi(S), xi(T )) > 0. But 
then (8.8.5) is true because, in this case, all elements in ∂dC have unit Euclidean 
norm. Since, by (8.8.4) and (8.8.5), pi and its derivatives p˙i are uniformly bounded, 
we can arrange, by subsequence extraction, that 
pi → p uniformly, and p˙i → ˙p weakly in L1 ,
for some absolutely continuous function p. We can also arrange that λi → λ, for 
some λ ∈ [0, 1]. 
A similar convergence analysis to that earlier employed, which makes use, now, 
of the facts that ai → 0 a.e. and di → 0, yields, in the limit, 
(− ˙p(t), ˙
x(t)) ¯ ∈ co ∂x,pH (t, x(t), p(t)), ¯ a.e. t ∈ [S,T ] .
We have proved the Hamiltonian inclusion. From (8.8.5) we obtain 
(p(S), −p(T )) ∈ λ∂g(x(S), ¯ x(T )) ¯ + (1 − λ)(∂dC(x(S), ¯ x(T )) ¯ ∩ ∂B) .
This relation implies 
(p(S), −p(T )) ∈ λ∂g(x(S), ¯ x(T )) ¯ + NC(x(S), ¯ x(T )) , ¯386 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
which is the desired transversality condition. We deduce also from the relation that 
|(p(S), −p(T ))| ≥ (1 − λ) − λkg, in which kg is a Lipschitz constant for g. It 
follows that 
√
2||p||L∞ + (1 + kg)λ ≥ 1 .
We have verified the Lagrange multiplier non-triviality condition. The proof is 
complete. ⨅⨆
8.9 Discussion and a Counter-Example 
This Section provides a counter-example that gives insights into different forms 
of the Hamiltonian inclusion condition. We begin however with some historical 
context. Here, two versions of the optimality condition come into play: 
The fully convexified Hamiltonian inclusion: 
(p(t), ˙ ˙
x(t)) ¯ ∈ co ∂x,pH (t, x(t), p(t)), ¯ a.e.
and 
The partially convexified Hamiltonian inclusion: 
p(t) ˙ ∈ co{q : (q, x(t)) ¯ ∈ ∂x,pH (t, x(t), p(t)) ¯ }, a.e..
In the above relations, H is, as usual, the Hamiltonian 
H (t, x, p) := max
e∈F (t,x) p · e .
(Note the change of terminology: the formerly named ‘Hamiltonian inclusion’ of 
Sect. 8.7 is now referred to as the ‘partially convexified Hamiltonian inclusion’, 
to distinguish it from the fully convexified version.) We assume, for simplicity of 
discussion, that the data (in relation to nominal F trajectory x¯ under consideration) 
satisfies the hypotheses of Theorem 8.8.1. 
The first set of necessary conditions for differential inclusion problems, involving 
the Hamiltonian was proved by F. H. Clarke in 1973. The time-line of the original 
discovery and subsequent refinements up to 2005 is as follows: 
1973: Clarke’s (fully convexified) Hamiltonian inclusion is valid for 
L∞ local minimizers under the convexity hypothesis (C) (of 
Theorem 8.7.2). (Clarke [56]). 
1996: The partially convexified Hamiltonian inclusion is valid for 
W1,1 local minimizers under the convexity hypothesis (C). 
(Loewen/Rockafellar [144]). 
2005: The fully convexified Hamiltonian inclusion is valid for L∞ local 
minimizers in the absence of the convexity hypothesis (Clarke [67]).8.9 Discussion and a Counter-Example 387
This sequence of improvements were in part motivated by a trend in refinements 
of necessary conditions, in the form of the generalized Euler Lagrange condition, 
which were first proved for L∞ local minimizers under the convexity hypothesis 
(C), then for W1,1 local minimizers under the convexity hypothesis and, finally, for 
W1,1-minimizers in the absence of (C). While these developments were underway, 
it seemed reasonable to assume that such a trend could be reproduced also in 
refinements of necessary conditions involving subgradients of the Hamiltonian. 
Looking at the time-line, we see optimality conditions involving the Hamiltonian 
were first proved only under the convexity hypothesis (C). From the mid-1970s until 
its resolution by Clarke 25 years later in 2005, the search was on for an answer to 
the question: is the Hamiltonian inclusion valid, and in what form, in the absence of 
the convexity hypothesis (C)? 
The reasons why a proof of a version of the Hamiltonian inclusion for the 
nominal trajectory x¯, in the absence of the convexity hypothesis (C), was so long 
coming is fairly easy to understand. The obvious approach is to attempt to bootstrap 
a proof for problems with non-convex velocities from one requiring the convexity 
hypothesis is as follows. First, we approximate the dynamic optimization problem 
by a problem with no right end-point constraint, whose closeness of approximation 
is calibrated by a parameter α. We then apply Ekeland’s theorem to construct 
a perturbation to the approximate problem that has a minimizer xα, converging 
to x¯ as α ↓ 0. Because of the absence of a right endpoint constraint and in 
consequence of the relaxation theorem, xα remains a minimizer when we replace 
F (t, x), in the perturbed problem, by its convex hull co F (t, x). We can then appeal 
to known necessary conditions, applicable when the velocity sets are convex, to 
write down necessary conditions satisfied by x¯α, and obtain necessary conditions 
for x¯, expressed in terms of the Hamiltonian, in the limit as α ↓ 0. If this proof 
strategy is to be followed in situations when the nominal trajectory x¯ is a W1,1 local 
minimizer, a natural choice of topology on the space of admissible F trajectories 
for the perturbed problem is the W1,1 topology. Because of the presence of the 
Ekeland perturbation term in the cost of the perturbed problem, associated with this 
topology, the necessary conditions for the perturbed problem involves the perturbed 
Hamiltonian 
Hα(t, x, p) := max
e∈F (t,x)

p · e − α|e − ˙xα(t)|

.
The proof strategy requires us to recovery a solution of the unperturbed Hamiltonian 
inclusion (in fully or partially convexified form) as the limit of solutions of the 
perturbed Hamiltonian inclusions; this in turn requires the following upper semi￾continuity property: 
lim sup
(x'
,p'
)→(x,p),α↓0
∂x,p Hα(t, x'
, p'
) ⊂ ∂x,p H (t, x, p) . (8.9.1) 
It is precisely at this point that the proof technique fails, because (8.9.1) is not true 
in general, if F is not convex valued. In order to circumvent this difficulty, Clarke388 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
made use of a different variational principle, namely Stegall’s theorem, which gives 
rise to a perturbed Hamiltonian with better stability properties, in place of Ekeland’s 
theorem, as detailed in the proof of Theorem 8.8.1 in Sect. 8.8. 
The latest development described in the timeline above was Clarke’s proof of the 
fully convexified Hamiltonian inclusion for L∞ local minimizers, in the absence 
of the convexity hypothesis (C). It leaves open the question of whether, in these 
necessary conditions we can, in general, replace the fully convexified Hamiltonian 
inclusion by the more precise partially convexified Hamiltonian inclusion and 
whether such optimality conditions are valid for the larger class of W1,1 local 
minimizers. 
The following example sheds light on such queries. It tells us that, in the absence 
of the convexity hypothesis (C), the fully convexified Hamiltonian inclusion is not 
valid, in general, when the nominal F trajectory is merely a W1,1 local minimizer 
(not an L∞ local minimizer). Since the partially convexified Hamiltonian inclusion 
is a more precise condition than the fully convexified Hamiltonian inclusion, 
the above statement remains correct, when we substitute ‘partially convexified 
Hamiltonian inclusion’ for ‘fully convexified Hamiltonian inclusion’. 
Example 
Let θ be the function θ (r) := min{r, 0}. Consider the optimization problem: 
(E)
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize 1
2 x1(1) − x2(1)
over absolutely continuous functions x = (x1, x2, x3) satisfying
⎡
⎣
x˙1(t)
x˙2(t)
x˙3(t)
⎤
⎦ ∈
⎡
⎣
0
x1(t)
1
⎤
⎦ ∪
⎡
⎣
0
x1(t)
−1
⎤
⎦ ∪
⎡
⎣
0
θ (x1)
0
⎤
⎦ , a.e. t ∈ [0, 1],
(x1(0), x2(0), x3(0)) ∈ [0,∞) × {0}×{0},
(x1(1), x2(1), x3(1)) ∈ R × R × R .
This will be recognized as a special case of (P ) in which F (t, x) = F (x) is 
F (x) =
⎡
⎣
0
x1
1
⎤
⎦ ∪
⎡
⎣
0
x1
−1
⎤
⎦ ∪
⎡
⎣
0
θ (x1)
0
⎤
⎦
and 
g(x0, x1) = 1
2
x1
1 − x1
2 , C = ([0,∞) × {0}×{0}) × R3 .
Salient features of this example are summarized in the following proposition.8.9 Discussion and a Counter-Example 389
Proposition 8.9.1 The data of problem (E) satisfy hypotheses (G1)–(G3) of Theo￾rem 8.8.1, with reference to the admissible F trajectory (x¯ ≡ (0, 0, 0)), and 
(a): (x¯ ≡ (0, 0, 0)) is a W1,1 local minimizer, 
(b): (x¯ ≡ (0, 0, 0)) is not an L∞ local minimizer, 
(c): There does not exist non-zero (λ, p) such that conditions (i)-(iii) in the 
statement of Theorem 8.8.1 are all satisfied, with reference to x¯. 
Proof 
(a): Take any γ ∈ (0, 1/2) and any admissible F trajectory (x1, x2, x3) such that 
||x||W1,1 ≤ γ .
Then x1 is constant (write the constant value x1) and, in view of the left end-point 
constraint, x1 ≥ 0. Since x2(0) = 0, we deduce from x(t) ˙ ∈ F (t, x(t)) that 
x2(1) =

I
x1 dt +

[0,1]\I
0 dt = x1 × meas{I }
and 
| ˙x3(t)| = 
1 a.e. t ∈ I
0 a.e. t ∈ [0, 1]\I .
Here, I := {t ∈ [0, 1]: ˙x3(t) /= 0}. 
It follows that 
|| ˙x3||L1 = meas{I } .
Since x3(0) = 0, we know || ˙x3||L1 = ||x3||W1,1 . But then 
meas{I } = ||x3||W1,1 ≤ ||x||W1,1 ≤ γ ,
and so x2(1) ≤ γ x1. We conclude that 
g(x(0), x(1)) = 1
2
x1 − x2(1) ≥ x1(1/2 − γ ) ≥ 0 .
We have shown that x has cost not lower than that of x¯. 
(b): Take any γ > 0 and write α := γ /√3. Let sw be the switching function 
sw(t) := 
+1 for t ∈ [2k − 2, 2k − 1)
−1 for t ∈ [2k − 1, 2k) , for k = 1, 2,...
Now let x = (x1, x2, x3) be the admissible F trajectory defined by the relations390 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions

(x˙1(t), x˙2(t), x˙3(t)) = (0, α, sw(Mγ t)) for t ∈ [0, 1] ,
(x1(0), x2(0), x3(0)) = (α, 0, 0) ,
in which Mγ is an even positive integer such that Mγ ≥ √3/γ .
We see immediately that (x1(1), x2(1)) = (α, α). A simple calculation (‘inte￾grate the switching function’) yields 
|x3(t)| ≤ γ /√
3 for all t .
Since |x1(t)| and |x2(t)| are bounded by α = γ /√3, we have 
||x||L∞ ≤ (3 ×

γ /√
3
2
)
1
2 = γ .
Also, 
g(x(0), x(1)) = 1
2
x1(1) − x2(1) = (
1
2 − 1)α < 0 .
On the other hand, the F trajectory x¯, which is admissible, has cost 
g(x(¯ 0), x(¯ 1)) = 0. It follows that there exists an admissible F trajectory arbitrarily 
close to x¯ ≡ 0 w.r.t. the L∞ norm, with lower cost, i.e. x¯ is not an L∞ local 
minimizer. The F trajectory x will be recognized as a ‘chattering’ approximation to 
the admissible co-F trajectory with constant velocity (0, α, 0). 
(c): Suppose that, contrary to the assertion, there exists (p, λ) satisfying conditions 
(i)-(iii) in the statement of Theorem 8.8.1, with reference to x¯ ≡ 0. In view of 
hypotheses (G1)–(G3), which are satisfied by the data for (E), and since the right 
end-points of admissible F trajectories are free, we must have λ > 0. By scaling 
(λ, p), we can arrange that λ = 1. The transversality conditions give: 
p1(0) ≤ 0, p1(1) = −1
2
, p2(1) = 1 and p3(1) = 0 .
The Hamiltonian is 
H (t, x, p) = max {(p2x1 + |p3|), p2 × θ (x1)} .
The Hamiltonian inclusion yields p˙2 ≡ 0, whence p2 ≡ 1. But then, for each 
t ∈ [0, 1], we have on a neighbourhood of (x(t), p(t)) ¯ , 
H (t, x, p) = max {(p2x1 + |p3|), (p2 × min{x1, 0})} = p2x1 + |p3| .
It follows that 
co ∂x,pH (t, x(t), p(t)) ¯ = {((p2(t), 0, 0), (0, 0,β) ∈ R6 : β ∈ [−1, +1]}.8.10 Appendix: Dualization of the Euler Lagrange Inclusion 391
Using this information, we deduce from the Hamiltonian inclusion that p˙1 =
−p2 ≡ −1. Since p1(1) = −1
2 , it follows that p1(0) = +1
2 . But this contradicts 
p1(0) ≤ 0. We have confirmed that the Hamiltonian inclusion conditions are not 
satisfied. ⨅⨆
Remark 
Because x¯ is a W1,1 local minimizer, it must satisfy the Euler Lagrange inclusion 
(ii), transversality condition (iii) and Weierstrass condition (iv) in Theorem 8.4.3. 
With reference to the Euler Lagrange inclusion, the costate trajectory components 
(p1, p2, p3) satisfy similar conditions to those associated with the Hamiltonian 
inclusion, with this crucial difference: Now p˙1(t) ∈ [−p2(t), 0], yielding merely the 
information that p1(0) ≥ −1
2 . This condition is too weak to establish a contradiction 
with the transversality condition. 
8.10 Appendix: Dualization of the Euler Lagrange Inclusion 
Take a function L˜ : Rn ×Rm → R∪ {+∞} and points (x,¯ v)¯ ∈ dom L˜ and p¯ ∈ Rm. 
Define H˜ : Rn × Rm → R ∪ {−∞} ∪ {+∞} to be the conjugate functional of 
L(x, v) ˜ with respect to v: 
H (x, p) ˜ := sup
v∈Rm
{p · v − L(x, v) ˜ }.
Our goal is to verify the following inclusion 
{q : (q, p)¯ ∈ ∂L(˜ x,¯ v)¯ } ⊂ co{q : (−q, v)¯ ∈ ∂H (˜ x,¯ p)¯ }, (8.10.1) 
under unrestrictive hypotheses on the function L˜. Notice that, since the set on the 
right is convex, (8.10.1) immediately implies 
co{q : (q, p)¯ ∈ ∂L(˜ x,¯ v)¯ } ⊂ co{q : (−q, v)¯ ∈ ∂H (˜ x,¯ p)¯ }. (8.10.2) 
The validity of (8.10.1) will be proved under a range of hypotheses. Our ultimate 
objective is to prove the relation under the hypotheses of dualization theorem, 
Theorem 8.7.1, and thereby to justify incorporating the Hamiltonian inclusion into 
the necessary conditions of Theorem 8.7.2. 
To prepare the ground, it is necessary to introduce some continuity concepts for 
the function L˜ that focus on the properties of the multifunction x ⇝ epiL(x, .) ˜ . 
Definition 8.10.1 Take a function L˜ : Rn × Rm → R ∪ {+∞}. We say that L˜ is 
epicontinuous if, for each x ∈ Rn and each xi → x, we have 
lim
i→∞ epiL(x ˜ i, .) = epiL(x, .). ˜392 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
The defining properties for epicontinuity can be expressed directly in terms of 
L˜. The proof of the following characterization is straightforward and is therefore 
omitted: 
Proposition 8.10.2 Take a function L˜ : Rn × Rm → R ∪ {+∞}. Then L˜ is 
epicontinuous if and only if 
(a) Given any point (x, v) and any sequence (xi, vi) → (x, v), we have 
L(x, v) ˜ ≤ lim inf
i→∞ L(x ˜ i, vi),
and 
(b) Given any point (x, v) and any sequence xi → x, there exists a sequence vi →
v such that 
L(x, v) ˜ ≥ lim sup
i→∞
L(x ˜ i, vi).
Definition 8.10.3 Take a function L˜ : Rn × Rm → R ∪ {+∞} and a point (x,¯ v)¯ ∈
dom L˜. We say that L˜ is epicontinuous near (x,¯ v)¯ if there exist neighbourhoods U, 
V and W of x¯, v¯ and L(x,¯ v)¯ with the property: 
(a) given any point ((x, v),L(u, v)) ˜ ∈ U × V × W and any sequence (xi, vi) →
(x, v), we have 
L(x, v) ˜ ≤ lim inf
i→∞ L(x ˜ i, vi),
and 
(b) given any (x, v,L(x, v)) ˜ ∈ U × V × W and any sequence xi → x, a sequence 
vi → v can be found such that 
L(x, v) ˜ ≥ lim sup
i→∞
L(x ˜ i, vi).
Definition 8.10.4 Given a function L˜ : Rn × Rm → R ∪ {+∞} and a point 
(x,¯ v)¯ ∈ dom L˜, we say that L˜ is locally epi-Lipschitz near (x,¯ v)¯ if there exist 
neighbourhoods U, V and W of (x,¯ v)¯ and L(x,¯ v)¯ respectively and k > 0 such that 
epiL(x ˜ '
, .) ∩ (V × W ) ⊂ epiL(x ˜ '', .) + k|x' − x''|B
for all x'
, x'' ∈ U.8.10 Appendix: Dualization of the Euler Lagrange Inclusion 393
Observe that L˜ is locally epi-Lipschitz near (x,¯ v)¯ if and only if the multifunction 
x ⇝ L(x, .) ˜ is pseudo-Lipschitz near 
x, ( ¯ v,¯ L(˜ x,¯ v)) ¯ 
. 
In the first version of the dualization theorem provided here, a Lipschitz 
continuity hypothesis on the data is imposed via the conjugate functional H (x, p) ˜ : 
Theorem 8.10.5 Take a function L˜ : Rn × Rm → R ∪ {+∞} and points (x,¯ v)¯ ∈
dom L and p¯ ∈ Rm. Assume that, for some neighbourhoods U and P of x¯ and p¯
respectively, the following hypotheses are satisfied: 
(H1): L(x, .) ˜ is convex for each x ∈ U, 
(H2): L˜ is lower semi-continuous and epicontinuous near (x,¯ v)¯ , 
(H3): H (., p) ˜ is Lipschitz continuous on U, uniformly with respect to all p’s in P. 
Then 
{q : (q, p)¯ ∈ ∂L(˜ x,¯ v)¯ } ⊂ co{q : (−q, v)¯ ∈ ∂H (˜ x,¯ p)¯ }.
Proof In view of (H3) we can arrange, by reducing the size of U and P if required, 
that H˜ is finite-valued and continuous on U × P. From (H3) it can also be deduced 
that the set 
{q ∈ Rn : (q, v'
) ∈ ∂H (x ˜ '
, p'
)}
is uniformly bounded as (x'
, p'
, v'
) ranges over U × P × Rm. In view of (H2), we 
can arrange by shrinking the neighbourhood U if necessary, and choosing suitable 
neighbourhoods V and W of v¯ and L(˜ x,¯ v)¯ respectively, that the following assertions 
are valid: given any (x, v) ∈ U × V such that L(x, v) ˜ ∈ W and any sequence 
xi → x, there exists a sequence vi → v such that 
lim sup
i
L(x ˜ i, vi) ≤ L(x, v). ˜
These facts will be used presently. Take a point q¯ ∈ Rn which satisfies 
(q,¯ p)¯ ∈ ∂L(˜ x,¯ v). ¯
We must show 
− ¯q ∈ co{q : (q, v)¯ ∈ ∂H (˜ x,¯ p)¯ }.
We note at the outset, however, that it suffices to treat only the case when (q,¯ p)¯
is a proximal normal: 
(q,¯ p)¯ ∈ ∂PL(˜ x,¯ v). ¯ (8.10.3)394 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
This is because if merely (q,¯ p)¯ ∈ ∂L(˜ x,¯ v)¯ , there exist sequences (xi, vi) L˜
→ (x,¯ v)¯
and (qi, pi) → (q,¯ p)¯ such that (qi, pi) ∈ ∂PL(x ˜ i, vi) for each i. Applying the 
special case of the theorem gives 
− qi ∈ co{q : (q, vi) ∈ ∂H (x ˜ i, pi)}
for each i sufficiently large. 
Since H˜ is continuous on U × P, we have (xi, pi) H
→ (x,¯ p)¯ . We then 
deduce from the uniform boundedness of the sets {q : (q, vi) ∈ ∂H (x ˜ i, pi)} and 
Carathéodory’s theorem that 
− ¯q ∈ co{q : (q, v)¯ ∈ ∂H (˜ x,¯ p)¯ }.
This confirms that, without loss of generality, we can assume (8.10.3). 
By modifying the neighbourhoods U and V if required, we can arrange that V is 
compact and, for some constant σ > 0, 
M(x, v) ≥ 0 for all (x, v) ∈ (x,¯ v)¯ + U × V
where 
M(x, v) := L(x, v) ˜ − L(˜ x,¯ v)¯ − ¯q · (x − ¯x) − ¯p · (v − ¯v)
+ σ|x − ¯x|
2 + σ|v − ¯v|
2. (8.10.4) 
Since M(x, .) is lower semi-continuous and strictly convex, for each x ∈ U there 
exists a unique minimizer vx over the compact set V . We can deduce from (H2) that 
lim sup
x→ ¯x
M(x, vx ) ≤ 0.
By the lower semicontinuity of M, and since v¯ is the unique minimizer for M(x, .) ¯
over V , we have 
lim
x→ ¯x
vx = ¯v. (8.10.5) 
For all x sufficiently close to x¯ then, vx is interior to V and so, since M(x, .) is 
convex, vx is the unique global minimizer. For all such x
0 ≤ M(x, vx ) = min
v∈Rm M(x, v). (8.10.6) 
From (8.10.4), v¯ minimizes 
v → L(˜ x, v) ¯ + σ|v − ¯v|
2 − ¯p · (v − ¯v).8.10 Appendix: Dualization of the Euler Lagrange Inclusion 395
Since L(˜ x, .) ¯ is convex, this implies that p¯ is a subgradient of L(˜ x, .) ¯ , in the sense 
of convex analysis. But then p · ¯v − L(˜ x,¯ v)¯ ≥ max{p · v − L(˜ x, v) ¯ : v ∈ Rm}. 
Taking note of the definition of H, we conclude that 
L(˜ x,¯ v)¯ = ¯p · ¯v − H (˜ x,¯ p). ¯ (8.10.7) 
Representing L(x, .) ˜ at the conjugate function of H (x, .) ˜ , we obtain 
min
v∈Rm M(x, v)
= min
v∈Rm


supp∈Rn [p · v − H (x, p) ˜ ] − L(˜ x,¯ v)¯ − ¯q · (x − ¯x)
− ¯p · (v − ¯v) + σ|x − ¯x|
2 + σ|v − ¯v|
2

= min
v∈Rm sup
p∈Rm
Kx (v, p) + H (˜ x,¯ p)¯ − ¯q · (x − ¯x) + σ|x − ¯x|
2
by (8.10.7). Here 
Kx (v, p) := (p − ¯p) · v + σ|v − ¯v|
2 − H (x, p). ˜
It follows from Proposition 3.4.7 (a version of the mini-max theorem for non￾compact domains) that 
min
v sup
p
Kx (v, p) = min
v Kx (v, px )
= (px − ¯p) · ¯v − (4σ )−1|px − ¯p|
2 − H (x, p ˜ x ),
in which 
px := ¯p − 2σ (vx − ¯v). (8.10.8) 
We also have that 
min
v sup
p'
Kx (v, p'
) ≥ min
v Kx (v, p) = (p − ¯p) · ¯v − (4σ )−1|p − ¯p|
2 − H (x, p) , ˜
for any p. From (8.10.6) we deduce that 
0 ≤ M(x, vx ) = −(4σ )−1|px − ¯p|
2 − H (x, p ˜ x )
− ¯q · (x − ¯x) + σ|x − ¯x|
2 + (px − ¯p) · ¯v + H (˜ x,¯ p)¯
and 
0 = M(x,¯ v)¯ ≥ −(4σ )−1|p − ¯p|
2 − H (˜ x, p) ¯ + (p − ¯p) · ¯v + H (˜ x,¯ p)¯396 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
for all p. It follows from these last two inequalities that 
H (˜ x,¯ p) − H (x, ˜ px ) + (4σ )−1(|p − ¯p|
2 − |px − ¯p|
2)
+ (px − ¯p) · ¯v − ¯q · (x − ¯x) + σ|x − ¯x|
2 ≥ 0, (8.10.9) 
for all x near x¯ and all p. 
For fixed x close to x¯, define the function φx (z), whose argument z is partitioned 
as z = (y, p), to be 
φx (z) := H (y, p) ˜ − H (x, p ˜ x ) + (4σ )−1(|p − ¯p|
2 − |px − ¯p|
2)
v¯ · (px − p) − ¯q · (x − y) + σ|x − y|
2.
In consequence of the definition of φx , and also by (8.10.9), 
φx (x, p) ¯ ≥ 0 for all p and φ(x, px ) = 0. (8.10.10) 
By (8.10.5) and (8.10.8) we know that px → ¯p as x → ¯x. We can choose sequences 
ϵi ↓ 0 and δi ↓ 0, therefore, which satisfy ϵiδ−1
i → 0 as i → ∞ and 
|x − ¯x| ≤ ϵi implies |px − ¯p| ≤ δi/2.
Fix i. For each x ∈ ¯x + ϵiB, x /= ¯x, apply the generalized mean value inequality 
theorem (Theorem 4.5.1) to φx with 
z0 := (x, px ) and Z = {(x, p) ¯ : p ∈ ¯p + δiB}.
This supplies (ζ '
(x), η'
(x)) ∈ ∂P H (y ˜ '
(x), p'
(x)) for some (y'
(x), p'
(x)) ∈ Rn ×
Rm such that 
|y'
(x) − x| ≤ 2ϵi, |p'
(x) − ¯p| ≤ ϵi + δi
and (in view of (8.10.10)) 
−ϵi|x − ¯x|≤−ϵi|x − ¯x| + inf
z∈Z φx (z) − φx (x, px )
≤ [ζ '
(x) + ¯q + 2σ (y'
(x) − x)] · (x¯ − x)
+ [η'
(x) − ¯v + (2σ )−1(p'
(x) − ¯p)] · (p − px )
for all p ∈ ¯p + δiB. 
Since |px − ¯p| < δi/2, taking the minimum over p in the final term on the right, 
we arrive at 
−ϵi|x − ¯x|≤[ζ '
(x) + ¯q + 2σ (y'
(x) − x)] · (x¯ − x)
− (δi/2)|η'
(x) − ¯v + (2σ )−1(p'
(x) − ¯p)|. (8.10.11) 
This last inequality is valid for all x ∈ ¯x + ϵiB such that x = ¯ / x.8.10 Appendix: Dualization of the Euler Lagrange Inclusion 397
Now, |ζ '
(x)| is bounded on x¯ + ϵiB by a constant independent of i and of our 
choice of y'
(x), p'
(x) (see the remarks at the beginning of the proof). It follows that 
there exists K, independent of i, such that, for all x ∈ ¯x + ϵiB, x /= ¯x, 
− ϵ2
i ≤ (K +|¯q| + 4σ ϵi)ϵi − (δi/2)|η'
(x) − ¯v| + (δi/2)(2σ )−1(ϵi + δi).
Since ϵiδ−1
i → 0 as i → ∞ there exists γi ↓ 0 such that 
sup
x∈ ¯x+ϵiB
|η'
(x) − ¯v| < γi. for each i . (8.10.12) 
Inequality (8.10.11) also tells us that 
− ϵi|x − ¯x|≤[ζ '
(x) + ¯q + 2σ (y'
(x) − x)] · (x¯ − x)
≤ (ζ '
(x) + ¯q) · (x¯ − x) + 4σ ϵi|x − ¯x|
for all x ∈ ¯x + ϵiB. This means that 
0 ≤ (ζ '
(x) + ¯q) · (x¯ − x) + max e∈ϵi(1+4σ )B
e · (x¯ − x) (8.10.13) 
for all x ∈ ¯x + ϵiB. (8.10.12) and (8.10.13) yield 
sup
ζ∈ ¯q+Si
ζ · (x¯ − x) ≥ 0 (8.10.14) 
for all x ∈ ¯x + ϵiB. Here 
Si := {ζ ' : (ζ '
, v'
) ∈ ∂P H (x ˜ '
, p'
) + (ϵi(1 + 4σ )B) × (γiB),
|x' − x| ≤ 2ϵi, |p' − ¯p| ≤ ϵi + δi}.
We conclude from (8.10.14) that 
− ¯q ∈ coSi.
But in view of the remarks at the beginning of the proof, the Si’s are bounded sets 
and H˜ is continuous on a neighbourhood of (x,¯ p)¯ . From Carathéodory’s theorem 
and the closure properties of ∂H˜ we deduce that 
− ¯q ∈ co{ζ : (ζ, v)¯ : (ζ, v)¯ ∈ ∂H (˜ x,¯ p)¯ }.
The proof is complete. ⨅⨆
The hypotheses in the above version of the dualization theorem include the 
requirement that the dual function H (., p) ˜ is Lipschitz continuous near x¯, in some 
uniform sense. We wish to replace it by the condition that L˜ is locally epi-Lipschitz 
near (x,¯ v)¯ . Unfortunately, the local epi-Lipschitz condition for L˜ concerns the398 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
behaviour of L˜ on a neighbourhood of (x,¯ v)¯ ∈ dom L˜, yet the values of H˜ are 
affected by the global properties of L˜. We cannot therefore expect, in general, to 
guarantee regularity properties of H˜ by hypothesizing L˜ is locally epi-Lipschitz. 
The local epi-Lipschitz property does however ensure that, for some ϵ > 0, the 
related function Hϵ has useful Lipschitz continuity properties, where 
H˜ϵ (x, p) := sup{p · v − L(x, v) ˜ : v ∈ ¯v + ϵB}. (8.10.15) 
Here ϵ is some parameter. Hϵ will be recognized as the conjugate function, with 
respect to the second variable, of the ‘localization’ of L: 
L˜ ϵ (x, v) := 
L(x, v) ˜ if v ∈ ¯v + ϵB
+∞ otherwise .
Relevant properties of H˜ϵ are listed in following proposition. 
Proposition 8.10.6 Take a function L˜ : Rn×Rm → R∪{+∞} and a point (x,¯ v)¯ ∈
dom L˜. Assume that 
(i): L(x, .) ˜ is convex for each x in a neighbourhood of x¯, 
(ii): L˜ is lower semi-continuous, 
(iii): L˜ is locally epi-Lipschitz near (x,¯ v)¯ . 
Then for each c > 0, there exists ϵ > 0 and β > 0 such that Hϵ (., p) is Lipschitz 
continuous on x¯ + βB, uniformly with respect to all p’s in cB. 
Proof Hypothesis (iii) can be expressed: there exist neighbourhoods U, V and W
of x¯, v¯ and L(˜ x,¯ v)¯ respectively and k > 0 such that 
epiL(x ˜ '
, .) ∩ (V × W ) ⊂ epiL(x ˜ '', .) + k|x' − x''|B (8.10.16) 
for all x'
, x'' ∈ U. By increasing k if necessary, we can arrange that k > 1. Choose 
α > 0 such that L(˜ x,¯ v)¯ + αB ⊂ W. 
Assume that the assertions of the proposition are false. We will show that this 
leads to a contradiction. Under this assumption, there exists c > 0 such that for all 
β > 0 and ϵ > 0, the function Hϵ (., p) is not Lipschitz continuous on x¯ + βB, 
uniformly with respect to all p’s in cB. We can therefore find a sequence βi ↓ 0
and a positive constant ϵ such that, for all i, Hϵ (., p) is not Lipschitz continuous on 
x¯ + βB uniformly with respect to p ∈ cB and 
(a) the set K := (x¯ + βiB) × (v¯ + ϵB) is included in U × V , 
(b) kβi < ϵ, 
(c) if (x, v) is in K, then L(x, v) > ˜ L(˜ x,¯ v)¯ − α, 
(d) 2cϵ < α/2. 
(We have invoked hypothesis (ii) to ensure (c).)8.10 Appendix: Dualization of the Euler Lagrange Inclusion 399
Fix i. Since L˜ is lower semi-continuous on U × Rm, finite at (x,¯ v)¯ and nowhere 
takes the value −∞, L˜ is bounded below on the compact subset K. By adding a 
constant to L˜ (this does not affect the assertions of the proposition), we can arrange 
that L(x, v) ˜ ≥ 0 for all (x, v) in K. Now for each x in x¯ +βiB, we have by (8.10.16) 
that 
(v,¯ L(˜ x,¯ v)) ¯ ∈ epiL(˜ x,¯ ·) ∩ (V × W ) ⊂ epiL(x, ˜ ·) + k| ¯x − x|B.
Hence, in particular, there exists v in Rm such that |v− ¯v| ≤ k|x − ¯x| < kβi < ϵ and 
L(x, v) ˜ ≤ L(˜ x,¯ v)¯ + k| ¯x − x| < +∞. This fact, combined with the non-negativity 
of L˜ on K, establishes that H˜ϵ (x, p) is finite for all p in Rm and for all x in x¯ +βiB. 
By the contradiction hypothesis, we can find points xi and yi in x¯ + βiB (with 
xi /= yi) and pi in cB such that 
H˜ϵ (xi, pi) − H˜ϵ (yi, pi) < −i|xi − yi|. (8.10.17) 
But a lower semi-continuous function which is locally epi-Lipschitz near a point is 
locally epi-continuous near the point. Since {xi} converges to x¯, it follows that there 
exists a sequence {vi} converging to v¯ such that lim supi→∞ L(x ˜ i, vi) ≤ L(˜ x,¯ v)¯ . 
We can assume i sufficiently large that vi is in v¯ + ϵB and such that L(x ˜ i, vi) <
L(˜ x,¯ v)¯ + α/2 (by definition of lim sup). 
Now since v → L(y ˜ i, v) is lower semi-continuous on Rm (by (ii)) and since 
v¯ + ϵB is a compact set, we have by definition of the real number H˜ϵ (yi, pi) that 
there exists wi in v¯ + ϵB such that H˜ϵ (yi, pi) = pi · wi − L(y ˜ i, wi). By (8.10.17), 
this implies that for all v in v¯ + ϵB, 
pi · v − L(x ˜ i, v) − pi · wi + L(y ˜ i, wi) < −i|xi − yi|. (8.10.18) 
Of course (wi,L(y ˜ i, wi)) belongs to epiL(y ˜ i, · ). Let us show that it also 
belongs to V × W. Certainly, wi belongs to V . Now since vi is in v¯ + ϵB, using 
inequality (8.10.18), we obtain 
pi · vi − L(x ˜ i, vi) − pi · wi + L(y ˜ i, wi) < −i|xi − yi| ≤ 0 ,
which implies that 
L(y ˜ i, wi) ≤ L(x ˜ i, vi) − pi · (vi − ¯v) + pi · (wi − ¯v) ≤ L(˜ x,¯ v)¯ +
α
2 + 2cϵ .
Since (yi, wi) is in K, we deduce from (c) and (d) that L(y ˜ i, wi) is in W. We can 
now use (8.10.16) (with x' = yi and x'' = xi) to find v˜i in Rm such that 
 | ˜vi − wi| ≤ k|xi − yi|
L(x ˜ i, v˜i) − L(y ˜ i, wi) ≤ k|xi − yi| . (8.10.19) 
There are two cases to consider:400 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
Case A: | ˜vi − ¯v| ≤ ϵB for an infinite number of index values. 
In this case, by extracting a subsequence if necessary, we can arrange that this 
assertion is valid for all i. Inserting v = ˜vi into (8.10.18) gives 
pi · ˜vi − L(x ˜ i, v˜i) − pi · wi + L(y ˜ i, wi) < −i|xi − yi| .
Hence by (8.10.19), we have 
− (|pi| + 1)k|xi − yi| < −i|xi − yi| ,
which is impossible for large i, because |pi| < c and xi /= yi. 
Case B: | ˜vi − ¯v| > ϵ for all i sufficiently large. 
In this case, we deduce from (8.10.16), in which we set x' = ¯x and x'' = xi, that, 
for i sufficiently large, ui ∈ Rm can be found such that 
 |ui − ¯v| ≤ k|xi − ¯x| < kβi < ϵ,
L(x ˜ i, ui) − L(˜ x,¯ v)¯ ≤ k|xi − ¯x| . (8.10.20) 
Define v'
i in v¯ + ϵB according to 
v'
i := μiui + (1 − μi)v˜i,
where μi ∈ [0, 1] is chosen such that |v'
i − ¯v| = ϵ (this is possible since |ui − ¯v| < ϵ
and | ˜vi − ¯v| > ϵ). We see that, for i large enough, 
ϵ = |v'
i − ¯v|=|μi(ui − ¯v) + (1 − μi)(v˜i − ¯v)|
≤ μi|ui − ¯v| + (1 − μi)(| ˜vi − wi|+|wi − ¯v|)
≤ μik|xi − ¯x| + (1 − μi)(k|xi − yi| + ϵ) .
It follows that, for i sufficiently large, 
μi(ϵ + k|xi − yi| − k|xi − ¯x|) ≤ k|xi − yi|.
But xi → ¯x and |xi −yi| → 0. Noting that, for i sufficiently large, ϵ −k|xi − ¯x| > 0
(this follows from (b)), we have, for i sufficiently large, that 
μi ≤
2k
ϵ
|xi − yi|. (8.10.21) 
Since v'
i is in v¯ + ϵB, we may insert it into (8.10.18), to get 
pi ·(μiui + (1 − μi)v˜i − wi) − L(x ˜ i, v'
i) + L(y ˜ i, wi) < −i|xi − yi|, (8.10.22) 
for all i sufficiently large. By convexity, we obtain 
L(x ˜ i, v'
i) ≤ μiL(x ˜ i, ui) + (1 − μi)L(x ˜ i, v˜i) .8.10 Appendix: Dualization of the Euler Lagrange Inclusion 401
Hence, we have, for i sufficiently large, 
− L(x ˜ i, v'
i) ≥ −μiL(x ˜ i, ui) − (1 − μi)L(x ˜ i, v˜i)
≥ −μi(L(˜ x,¯ v)¯ + k|xi − ¯x|) − (1 − μi)L(x ˜ i, v˜i)
≥ −μi(L(˜ x,¯ v)¯ + k|xi − ¯x|) − (1 − μi)(L(y ˜ i, wi) + k|xi − yi|)
≥ −μi(L(˜ x,¯ v)¯ + k|xi − ¯x|) − L(y ˜ i, wi) − k|xi − yi|.
To derive these relations, we have used (8.10.19) and (8.10.20), and noted that, 
since (yi, wi) is in K, μi(L(y ˜ i, wi) + k|xi − yi|) ≥ 0. This inequality combines 
with (8.10.22) to give 
μipi · (ui − ˜vi) + pi · (v˜i − wi) − μi(L(˜ x,¯ v)¯ + k|xi − ¯x|)
− k|xi − yi| < −i|xi − yi|.
We deduce from (8.10.19) that, for i sufficiently large, 
− μic|ui − ˜vi| − ck|xi − yi| − μi(L(˜ x,¯ v)¯ + k|xi − ¯x|)
− k|xi − yi| < −i|xi − yi|.
It follows from (8.10.21) 
−2k
ϵ
[c|ui − ˜vi| + L(˜ x,¯ v)¯ + k|xi − ¯x|] × |xi − yi|
− (c + 1)k|xi − yi| < −i|xi − yi|.
Since {ui} and { ˜vi} are bounded sequences and {xi} converges to x¯, this is 
impossible. 
In both cases (A) and (B), we have arrive at a contradiction. The assertions of the 
Proposition must therefore be true. ⨅⨆
The link that Proposition 8.10.6 provides between epi-Lipschitz hypotheses on L˜
and Lipschitz continuity properties of H˜ϵ are now used to prove the following 
generalization of Theorem 8.10.5 
Theorem 8.10.7 Take a function L˜ : Rn × Rm → R ∪ {+∞} and points (x,¯ v)¯ ∈
dom L˜ and p¯ ∈ Rm. Assume that, for some neighbourhoods U and P of x¯ and p¯
respectively, the following hypotheses are satisfied: 
(H1): L(x, .) ˜ is convex for each x in a neighbourhood of x¯, 
(H2): L˜ is lower semi-continuous and epicontinuous near (x,¯ v)¯ , 
(H3): For some ϵ > 0, H˜ϵ (., p) (defined in (8.10.15)) is Lipschitz continuous on 
U, uniformly with respect to all p’s in P.402 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
Then 
{q : (q, p)¯ ∈ ∂L(˜ x,¯ v)¯ } ⊂ co{q : (−q, v)¯ ∈ ∂H (˜ x,¯ p)¯ }.
Proof Take a point q¯ such that (q,¯ p)¯ ∈ ∂L(˜ x,¯ v)¯ . We must show that 
− ¯q ∈ co{q ∈ Rn : (q, v)¯ ∈ ∂H (˜ x,¯ p)¯ }.
We observe that 
(q,¯ p)¯ ∈ ∂L˜ ϵ (x,¯ v), ¯
since L˜ and L˜ ϵ coincide on a neighbourhood of (x,¯ v)¯ . 
But the hypotheses of the preceding theorem (Theorem 8.10.5) are satisfied when 
L˜ ϵ replaces L˜. We deduce that 
q¯ ∈ co{q : (−q, v)¯ ∈ ∂H˜ϵ (x,¯ p)¯ }.
It follows from Carathéodory’s theorem and the closure properties of the proximal 
subdifferential that q¯ is a convex combination of (n + 1) points of the form 
q = lim
i qi
for some sequences vi → ¯v and (xi, pi) → (x,¯ p)¯ such that 
(−qi, vi) ∈ ∂P H˜ϵ (xi, pi) for all i. (8.10.23) 
Take such a point q. Let {vi}, {(xi, pi)} be sequences associated with q as above. 
Claim: 
H˜ϵ (x,¯ p)¯ = H (˜ x,¯ p)¯ (8.10.24) 
and 
H˜ϵ (xi, pi) = H (x ˜ i, pi) for all i sufficiently large. (8.10.25) 
Let us verify these assertions. (8.10.23) implies that, for each i, σi > 0 can be found 
such that 
H˜ϵ (x, p) − H˜ϵ (xi, pi) ≥ −qi · (x − xi) + vi · (p − pi) − σi|(x, p) − (xi, pi)|
2
(8.10.26) 
for all (x, p) in some neighbourhood of (xi, pi). It follows that, for each i, 
H˜ϵ (xi, p) − H˜ϵ (xi, pi) ≥ vi · (p − pi) − σi|p − pi|
2,
for all p in some neighbourhood of pi.8.10 Appendix: Dualization of the Euler Lagrange Inclusion 403
Since H˜ϵ (x, .) is convex, we deduce that 
vi ∈ ∂H˜ϵ (xi, .)(pi),
in which ∂H˜ϵ denotes the subdifferential of convex analysis. But this implies that 
(from Fenchel’s inequality) 
H˜ϵ (xi, pi) = pi · vi − L˜ ϵ (xi, vi) = pi · vi − L(x ˜ i, vi), (8.10.27) 
for i sufficiently large. 
For i sufficiently large, the maximum of the concave function v → pi · v −
L(x ˜ i, v) over v¯ + ϵB is achieved at the point vi ∈ ¯v + ϵ
◦
B. We conclude that vi
achieves the maximum of this function over all v ∈ Rn. It follows that (8.10.25) is 
true. 
Since H˜ϵ is continuous on a neighbourhood of (x,¯ p)¯ and (xi, vi) → (x,¯ v)¯ , it 
follows from (8.10.27) and the lower semicontinuity of L˜ that 
H˜ϵ (x,¯ p)¯ = ¯p · ¯v − lim
i
L(x ˜ i, vi) ≤ ¯p · ¯v − L(˜ x,¯ v)¯ ≤ H˜ϵ (x,¯ p). ¯
We conclude that v → ¯p·v−L(˜ x, v) ¯ has a local maximum at v = ¯v. Since −L(˜ x, .) ¯
is concave, this maximum is in fact a global maximum. We deduce (8.10.24). The 
claim is verified. 
Since H˜ϵ (x, p) ≤ H (x, p) ˜ for all (x, p), we deduce from (8.10.26) and (8.10.25) 
that, for each i sufficiently large, there exists σi > 0 such that 
H (x, p) ˜ − H (x ˜ i, pi) ≥ −qi · (x − xi) + vi · (p − pi) − σi|(x, p) − (xi, pi)|
2
for all (x, p) in some neighbourhood of (xi, pi). This implies 
(−qi, vi) ∈ ∂P H (x ˜ i, pi)
for all i sufficiently large. Since H˜ϵ is continuous on a neighbourhood of (x,¯ p)¯ , 
(8.10.24) and (8.10.25) imply 
lim
i
H (x ˜ i, pi) = lim
i
H˜ϵ (xi, pi) = H˜ϵ (x,¯ p)¯ = H (˜ x,¯ p). ¯
We deduce from these relations that q satisfies 
(−q, v)¯ ∈ ∂H (˜ x,¯ p). ¯
But q¯ is expressible as a convex combination of such q’s. It follows that 
q¯ ∈ co{q : (−q, v)¯ ∈ ∂H (˜ x,¯ p)¯ }.
This is what we set out to prove. ⨅⨆404 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
We conclude from Proposition 8.10.6 and Theorem 8.10.7 the validity of the 
following assertions: 
Take a function L˜ : Rn × Rm → R ∪ {+∞} and points (x,¯ v)¯ ∈ dom L˜ and 
p¯ ∈ Rm. Assume that, for some neighbourhood U of x¯, the following hypotheses 
are satisfied: 
(H1): L(x, .) ˜ is convex for each x ∈ U, 
(H2): L˜ is lower semi-continuous, 
(H3): L˜ is locally epi-Lipschitz near (x,¯ v)¯ . 
Then 
{q : (q, p)¯ ∈ ∂L(˜ x,¯ v)¯ } ⊂ co{q : (−q, v)¯ ∈ ∂H (˜ x,¯ p)¯ }. (8.10.28) 
The properties listed above, together, will be recognised as a statement of the 
dualization theorem (Theorem 8.7.1). The proof is complete. ⨅⨆
8.11 Exercises 
8.1 (Kaskosz Lojasiewicz-Type Necessary Conditions [133]) Take an interval 
[S, T ], a multifunction F : [S, T ] × Rn ⇝ Rn and x¯ ∈ W1,1([S, T ]; Rn). Assume 
that F is L × Bn measurable, takes values closed, convex sets and, furthermore, F
has the representation 
F (t, x) := {f (t, x) : f ∈ F}, for all (t, x) ∈ [S,T ] × Rn .
for some family of functions F with the following properties: 
˙
x(t) ¯ = f (t, ¯ x(t)), ¯ a.e., for some f¯ ∈ F
and there exist ϵ > 0, kF > 0 and cF > 0 such that, for each f ∈ F, 
(a): f is L × Bn measurable ,
(b): |f (t, x)| ≤ cF and |f (t, x'
) − f (t, x)| ≤ kF |x' − x|, for all x'
, x ∈ ¯x(t) +
ϵB and t ∈ [S, T ].
Let x¯ be an L∞ local minimizer for 
⎧
⎪⎪⎨
⎪⎪⎩
Minimize g(x(S), x(T ))
over x ∈ W1,1([S,T ]; Rn) such that
x(t) ˙ ∈ F (t, x(t)), for a.e. t ∈ [S,T ]
(x(S), x(T )) ∈ C .
in which g : Rn × Rn → R is a given function and C ⊂ Rn × Rn is a given 
closed set. Assume that g is Lipschitz continuous on (x(S), ¯ x(T ¯ )) + ϵB.8.11 Exercises 405
Show that there exist p ∈ W1,1([S, T ]; Rn) and λ ≥ 0, not both zero, such that 
(i): − ˙p(t) ∈ co ∂xp(t) · f (t, x(t)) ¯ , a.e., 
(ii): p(t) · ˙
x(t) ¯ = max e∈F (t,x(t)) ¯ p(t) · e a.e., 
(iii): (p(S), −p(T )) ∈ λg(x(S), ¯ x(T ¯ )) + NC(x(S), ¯ x(T ¯ )). 
Hint: For each α ∈ (0, 1] show that x¯ remains an L∞ local minimizer when, in 
the problem formulation F (t, x) is replaced by (f (t, ¯ x) − α[f (t, ¯ x) − F (t, x)]). 
Apply Clarke’s Hamiltonian inclusion conditions to the modified problem, to obtain 
perturbed version of the state necessary conditions. Pass to the limit as α ↓ 0. (See 
[145].) 
8.2 Let x¯ be a W1,s local minimizer (see Exercise 6.1 for the definition), for some 
s ∈ [1,∞), for the dynamic optimization problem 
(P )
⎧
⎪⎪⎨
⎪⎪⎩
Minimize g(x(S), x(T ))
over arcs x ∈ W1,1([S,T ]; Rn) satisfying
x(t) ˙ ∈ F (t, x(t)) a.e. t ∈ [S,T ],
(x(S), x(T )) ∈ C.
Assume that 
(H1): g is Lipschitz continuous on a neighbourhood of (x(S), ¯ x(¯ T )) ¯ and C is 
closed, 
(H2): F (t, x) is nonempty for each (t, x) ∈ [S, T ] × Rn, Gr F (t, .) is closed for 
each t ∈ [S, T ] and F is L × Bn measurable, 
(H3): there exist integrable functions c, k ∈ L1(S, T ) such that 
F (t, x) ⊂ c(t)B for all x ∈ Rn, a.e. t ∈ [S,T ] and
F (t, x'
) ⊂ F (t, x) + k(t)|x' − x|B for all x'
, x ∈ Rn, a.e. t ∈ [S,T ].
Show that there exist an arc p ∈ W1,1([S, T ]; Rn) and λ ≥ 0 such that 
(i) (p, λ) /= (0, 0), 
(ii) p(t) ˙ ∈ co{η : (η, p(t)) ∈ NGr F (t,.)(x(t), ¯ ˙
x(t)) ¯ }, a.e. t ∈ [S, T ], 
(iii) (p(S), −p(T )) ∈ λ∂g(x(S), ¯ x(T ¯ )) + NC(x(S), ¯ x(T ¯ )),
(iv) p(t) · ˙
x(t) ¯ ≥ p(t) · v for all v ∈ F (t, x(t)), ¯ a.e. t ∈ [S, T ] . 
If, in addition, F (t, x) is convex for each (t, x) ∈ [S, T ] × Rn, then condition (ii) 
implies 
(v) p(t) ˙ ∈ co{−ξ : (ξ , ˙
x(t)) ¯ ∈ ∂x,pH (t, x,¯ p(t))}, a.e. t ∈ [S, T ]. 
Hint: Consider the auxiliary dynamic optimization problem406 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
(P ) 
⎧
⎪⎪⎨
⎪⎪⎩
Minimize g(x(S), x(T ))
over arcs (x, y) ∈ W1,1([S,T ]; Rn+1) satisfying
(x(t), ˙ y(t)) ˙ ∈ F (t, x(t))  a.e. t ∈ [S,T ],
(x(S), x(T )) ∈ C and (y(S), y(T )) ∈ {0} × ϵB,
where ϵ > 0 is a suitable number and F
 = {(v, |˙
x(t) ¯ − v|
s) : v ∈ F (t, x)}. 
Observe that (x,¯ y¯ ≡ 0) is an L∞ local minimizer for problem (P )  and apply 
Theorem 8.4.3. 
8.12 Notes for Chapter 8 
Rockafellar’s optimality conditions for fully convex problems of Bolza type 
revealed how the classical Euler Lagrange condition and Hamilton’s system 
of equations can be interpreted in terms of subdifferentials (in the sense of 
convex analysis) of extended valued Lagrangians and Hamiltonian functions 
[172, 173]. Despite the limitations of the ‘fully convex’ setting (which, in effect, 
restricts attention to dynamic optimization problems with affine dynamics and cost 
integrands jointly convex with respect to state and velocity variables), Rockafellar’s 
conditions provided the template for subsequent necessary conditions of the kind 
covered in this chapter. 
The breakthrough into necessary conditions for broad classes of dynamic 
optimization problems formulated in terms of a differential inclusion was Clarke’s 
Hamiltonian inclusion of 1976 [58]: 
(− ˙p, ˙
x)¯ ∈ co ∂H (t, x, p). ¯ (8.12.1) 
A feature of Clarke’s derivation, distinguishing it from early necessary conditions 
for ‘differential inclusion’ problems, was the natural and intrinsic nature of the 
hypotheses imposed on the velocity set F (t, x). F (t, x) was required to be merely 
compact, convex valued, integrably bounded, measurable in time and integrably 
Lipschitz continuous in the state variable, with respect to the Hausdorff metric. 
Clarke also derived, under the above hypotheses, an Euler Lagrange type 
condition for problems reducible to free right end-point problems [61]: 
(p, p) ˙ ∈ coNGr F (t,.)(x,¯ ˙
x). ¯ (8.12.2) 
Subsequently, Loewen and Rockafellar [143] established the validity of the gener￾alized Euler Lagrange condition 
p˙ ∈ co{q : (q, p) ∈ NGr F (t,.)(x,¯ ˙
x)¯ }. (8.12.3)8.12 Notes for Chapter 8 407
This condition, prefigured by related, weaker, conditions of Mordukhovich [155] 
(later elaborated in [158]) and Smirnov [183], improves on (8.12.2), because it 
involves convexification only w.r.t. to the first coordinate, not both coordinates. 
Loewen and Rockafellar also provided necessary conditions involving a sharper 
version of Clarke’s Hamiltonian inclusion, namely 
p˙ ∈ co{q : (−q, ˙
x)¯ ∈ ∂H (t, x, p) ¯ }, (8.12.4) 
which, like (8.12.3), involves convexification only w.r.t. the first coordinate. 
Rockafellar [176] gave conditions under which the Euler Lagrange inclu￾sion (8.12.3) (for convex valued velocity sets F (t, x)) and the Hamiltonian inclu￾sion (8.12.4) are in fact equivalent. Conditions for the one way implication ‘(8.12.3) 
=⇒ (8.12.4)’, under weaker hypotheses (suitably matched to the derivation 
of necessary conditions of optimality, under unrestrictive hypotheses), were later 
provided by Bessis, Ledyaev and Vinter [24]. Thus, in the case of convex velocity 
sets, the second condition is subsumed in the first. 
Subsequently, Loewen and Rockafellar highlighted the unsatisfactory nature 
of traditional ‘Hausdorff metric’ Lipschitz continuity hypotheses, for applications 
involving unbounded F (t, x)’s. They introduced in [143] a more appropriate set of 
hypotheses for such multifunctions, which included pseudo-Lipschitz continuity. 
The foregoing discussion relates to dynamic optimization problems with convex 
valued F (t, x)’s. Of course the maximum principle, which predates all these 
developments, applies to problems with possibly nonconvex velocity sets, in the 
case that the dynamic constraint can be parameterized as a control-dependent 
differential equation. Necessary conditions for problems with nonconvex F (t, x)’s 
have long been available under hypotheses (‘calmness’, ‘controllability’, or assump￾tions about the nature of the end-point constraints), the implications of which 
are that minimizers are minimizers also for related problems in which F (t, x) is 
replaced by its convex hull. In such cases, necessary conditions can be derived from 
known necessary conditions for the case F (t, x) is convex, or by studying local 
approximations of the mapping of the initial state into the set of admissible state 
trajectories as in [106]. But the derivation of necessary conditions for problems with 
nonconvex F (t, x)’s, under unrestrictive hypotheses, required some new ideas and 
was a later chapter in the story. 
We observe that, for convex F (t, x)’s, the generalized Weierstrass condition 
p · ˙
x¯ := max
v∈F (t,x)¯ p · v
is merely a consequence of the generalized Euler Lagrange condition (8.12.3). For 
nonconvex F (t, x)’s however, it is a distinct condition which can have an important 
role in the elimination of putative minimizers. 
Mordukhovich, using discrete approximation techniques, established the validity 
of the Euler Lagrange inclusion, for non-convex valued, bounded F (t, x)’s, a.e. 
continuous in t and Lipschitz continuous in x [158], but only under the hypothesis408 8 The Generalized Euler-Lagrange and Hamiltonian Inclusion Conditions
that the nominal state trajectory is a local minimizer of the associated ‘relaxed’ 
problem. (Furthermore, almost everywhere continuity of the data w.r.t. time was 
required in this paper and the condition was not accompanied by the Weierstrass 
condition, since this condition is not manifested in discrete time approximations). 
Mordukhovich constructions were subsequently elaborated in [159, Part 2]. 
The validity of the Euler Lagrange inclusion for measurably time dependent, 
possibly nonconvex valued F (t, x)’s satisfying epi-Lipschitz continuity hypothesis, 
accompanied by the Weierstrass condition was proved by Ioffe [127], building on 
earlier joint work with Rockafellar [131], treating the finite Lagrangian problem. 
Vinter and Zheng [199, 200] provided a simple, independent derivation of the 
condition for non-convex F (t, x), based on application of the classical ‘smooth’ 
maximum principle to an approximating optimization problem with state free 
dynamics and passage to the limit. 
As we have observed, it was a significant advance to replace Lipschitz conti￾nuity hypotheses by a less restrictive pseudo-Lipschitz continuity hypothesis, in 
the derivation of the generalized Euler Lagrange condition for problems having 
unbounded velocity sets F (t, x). However this was not the end of the story, 
because the precise form of the pseudo-Lipschitz condition chosen involves setting 
parameters that govern how fast a certain Lipschitz bound grows as you move away 
from the nominal velocity. The appropriate choice of these parameters, to convey 
as much information as possible, depends on the special case under consideration 
[127, 143, 194]. In 2005, Clarke published a version of (8.12.3), Clarke’s stratified 
Euler Lagrange condition, in which he hypothesized the pseudo-Lipschitz condition 
supplemented by an extra condition called the ‘tempered growth’ condition [67]. 
This not only subsumed earlier special cases: the conditions placed on the pseudo￾Lipschitz continuity parameters correspond to choosing different stratification 
procedures. It also clarified the nature of supplementary conditions required. 
A feature of Clarke’s stratified Euler Lagrange condition is that it incorporates 
only a partial form of the Weierstrass condition. To be specific, it asserts maxi￾mization of the Hamiltonian not over the entire velocity set at the nominal state 
F (t, x(t)) ¯ , but over only a subset. Subsequently, employing techniques formalized 
in [128], Ioffe identifies additional points in F (t, x(t)) ¯ with respect to which the 
Weierstrass condition is valid (the ‘Ioffe refinement’) [129]. 
In our discussion of necessary conditions for ‘nonconvex’ differential inclusion 
problems we have, so far, dwelt exclusively on the generalized Euler Lagrange 
condition. What about the Hamiltonian inclusion (8.12.4)? This later condition is 
not automatically satisfied because the implication ‘(8.12.3) =⇒ (8.12.4)’ has 
been proved only for convex velocity sets F (t, x). It has been a a longstanding open 
question, posed by Clarke in the 1970’s, whether Clarke’s Hamiltonian inclusion is 
valid when F (t, x) is not convex. The question was resolved by Clarke himself 
in 2005, when he established this optimality condition was indeed valid when 
F (t, x) is not convex, but only for L∞ local minimizers and under Lipschitz, not 
pseudo-Lipschitz, hypotheses on the data [67]. A counter-example showing that 
Clarke’s Hamiltonian inclusion (and therefore also the stronger partially convexified8.12 Notes for Chapter 8 409
Hamiltonian inclusion) is not always valid for W1,1 local minimizers is due to Vinter 
[195]. Ioffe [130] has recently proved Clarke’s Hamiltonian inclusion under less 
restrictive hypotheses. 
This chapter brings the generalized Euler Lagrange condition fully up to date. 
The centrepiece is Clarke’s ‘stratified’ form of the condition proved under pseudo￾Lipschitz and tempered growth hypotheses, with the Ioffe refinement. The derivation 
involves first proving necessary conditions for a finite Lagrangian problem as in 
[67] but our chosen Lagrangian is a constructed as a scaled distance to the velocity 
set, in place of the distance to an embedding of the velocity set in a higher 
dimensional space (Clarke’s ‘lifting’ technique). This facilitates proof of the Ioffe 
refinement. The proof that the generalized Euler Lagrange conditions subsumes 
the generalized Hamiltonian inclusion for convex F (t, x)’s follows that in [24]. 
Further discussion concerning the validity of the Clarke’s Hamiltonian inclusion 
for nonconvex F (t, x)’s can be found in [195].Chapter 9 
Free End-Time Problems 
Abstract This chapter provides necessary conditions of optimality for free end￾time dynamic optimization problems, that is problems in which the left and right 
end-times are included among the choice variables. Minimum time problems, in 
which the aim is to drive the state from an initial state to a target set in the state 
space in minimum time, are important examples of such problems. We shall see 
that the earlier derived necessary conditions for fixed end-point problems (Clarke’s 
nonsmooth maximum principle when the dynamic constraint is a controlled differ￾ential equation and either the generalized Euler Lagrange condition or a condition 
expressed in terms of the Hamiltonian for differential inclusion problems) can be 
supplemented by extra conditions to take account of the enlargement of the set of 
choice variables to include the end-times. It turns out that these extra conditions 
are boundary conditions on the Hamiltonian evaluated along the minimizing state 
and co-state trajectories. But here we have a problem because, for problems with 
measurable time dependence, the Hamiltonian is only almost everywhere defined 
and conditions involving point evaluation of the Hamiltonian at the optimal end￾times require interpretation. 
The necessary conditions featuring in this chapter fall into two groups. For prob￾lems in the first group, the data is assumed to be Lipschitz continuous w.r.t. time; 
in this situation the Hamiltonian (evaluated along the minimizing state and co-state 
trajectories) is Lipschitz continuous. So the boundary conditions on this function can 
be interpreted in the obvious way. For this group of problems, necessary conditions 
(including the classically interpreted extra boundary conditions on the Hamiltonian), 
can be derived by using a family of re-parameterizations of the time variable to 
reduce the free end-time problem to a fix end-time problem, to which we apply 
fixed end-time necessary conditions from earlier chapters. 
For problems in the second group we allow the data to be merely measurable 
w.r.t. time. Here it is still possible to derive free end-time necessary conditions. But 
now the boundary conditions are interpreted as conditions involving the ‘essential 
values’ (strictly speaking, the super and sub essential values) of the Hamiltonian. 
Essential values associated with a given function are set-valued functions that reduce 
to a point-valued function coinciding with the original function, when the original 
function is continuous. They provide a means of interpreting boundary conditions 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
P. Bettiol, R. B. Vinter, Principles of Dynamic Optimization, Springer Monographs 
in Mathematics, https://doi.org/10.1007/978-3-031-50089-3_9
411412 9 Free End-Time Problems
on the Hamiltonian, because essential values are invariant under changes to the 
original function on a nullset. To prove the necessary conditions, for this group 
of problems, we build up the proofs in stages, providing proofs of the necessary 
conditions under progressively less stringent hypotheses. A simple variation of the 
end-times in the first state establishes the required boundary condition (expressed 
in terms of essential values). Robustness properties of essential values then ensure 
that these boundary conditions are retained as we progress through all the stages. In 
this chapter we make use of new, refined concepts of ‘essential value’ that allow for 
more precise forms of the necessary conditions than appears in the earlier literature. 
9.1 Introduction 
Our investigation of the properties of optimal strategies has, up till now, been 
confined to dynamic optimization problems for which the underlying time interval 
[S,T ] has been fixed. In this chapter, we address a broader class of problems in 
which the end-times S and T are included among the choice variables. We refer to 
such problems as free end-time problems. We shall show that the earlier derived 
necessary conditions for fixed end-point problems can be supplemented to take 
account of the enlargement of the set of choice variables to include the end-times. 
We do so in situations where the dynamic constraint takes the form either of either 
a differential inclusion or a controlled differential equation. The initial discussion 
takes as starting point the differential inclusion formulation. Consider: 
(F T )
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize g(S, x(S), T , x(T ))
over intervals [S,T ] and arcs x ∈ W1,1([S,T ]; Rn)
satisfying
x(t) ˙ ∈ F (t, x(t)) a.e. t ∈ [S,T ],
(S, x(S), T , x(T )) ∈ C.
Here g : R × Rn × R × Rn → R is a given function, F : R × Rn ⇝ Rn is a given 
multifunction, and C ⊂ R × Rn × R × Rn is a given set. 
Now, an arc x ∈ W1,1([S,T ]; Rn) will usually be denoted ([S,T ], x), to 
emphasize the underlying time-interval [S,T ]. In the present context, an arc 
([S,T ], x), where x is an F trajectory on [S,T ] that satisfies the end-point 
constraint (S, x(S), T , x(T )) ∈ C, is referred to as an admissible F trajectory. 
Important special cases of (F T ) are minimum time problems, in which the 
time duration of a manoeuvre is the quantity we aim to minimize. Many practical 
dynamic optimization problems are of this nature. For example, a strategy for the 
attitude control of an orbiting satellite is ‘bang-bang’ control, based on elimination 
of the deviation of the satellite’s orientation from its nominal value in minimum 
time. Dynamic optimization problems associated with evasion or pursuit are often 
minimum time problems—escape from, or catch, your adversary as quickly as 
possible.9.1 Introduction 413
Take a minimizer ([S,¯ T¯], x)¯ for (F T ). Then x¯ is a minimizer for the related 
dynamic optimization problem, in which the end-times are frozen at S¯ and T¯. So 
the minimizer certainly satisfies fixed end-time necessary conditions, such as those 
derived in Chap. 8. But additional information about the multipliers λ ≥ 0 and 
p ∈ W ([S,¯ T¯]; Rn), featuring in the fixed end-time necessary conditions, is required 
to take account of the extra degrees of freedom which have been introduced into 
the optimization problem. The addition information comes in the form of boundary 
conditions on the Hamiltonian 
H (t, x, p) := max
v∈F (t,x) p · v (9.1.1) 
(evaluated along t → (x(t), p(t) ¯ ). These boundary conditions are implicit in 
a generalized transversality condition for free end-time dynamic optimization 
problems, to be derived in this chapter: 
(−ξ0, p(S), ξ ¯ 1, −p(T )) ¯ ∈ λ∂g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ + NC(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯
(9.1.2) 
in which 
ξ0 = H (t, x(t), p(t)) ¯ |
t=S¯ and ξ1 = H (t, x(t), p(t)) ¯ |
t=T¯ .
In one extreme case (fixed end-times), C and g are of the form 
C = {(S, x ¯ 0, T,x ¯ 1) : (x0, x1) ∈ C˜} and g(S, x0,T,x1) = ˜g(x0, x1),
for some set C˜ ⊂ Rn × Rn, fixed interval [S,¯ T¯] and function g˜ : Rn × Rn → R. 
Here, (9.1.2) reduces to the familiar ‘fixed end-time’ transversality condition 
(p(S), ¯ −p(T )) ¯ ∈ λ∂g(˜ x(¯ S), ¯ x(¯ T )) ¯ + NC˜(x(¯ S), ¯ x(¯ T )) ¯
and, appropriately, conveys no information about the optimal end-times. 
In another extreme case (unconstrained end-times), C and g are expressible as 
C = {(α, x0, β, x1) : (α, β) ∈ R2, (x0, x1) ∈ C˜} and g(S, x0,T,x1) = ˜g(x0, x1) ,
for some C˜ ⊂ Rn × Rn and some g˜ : Rn × Rn → R. In this case the boundary 
conditions on the Hamiltonian are decoupled from the generalized transversality 
condition and are simply 
H (t, x(t), p(t)) ¯ |
t=S¯ = 0 and H (t, x(t), p(t)) ¯ |
t=T¯ = 0.
The interpretation of these boundary conditions on the Hamiltonian is not entirely 
straightforward however, for the following reason. The set of state trajectories 
associated with the differential inclusion x(t) ˙ ∈ F (t, x(t)) is unaffected by arbitrary414 9 Free End-Time Problems
modifications on a null-set of the ‘point to multifunction’ mapping t → F (t, .) . We 
can therefore regard the dynamic optimization problem as an optimization problem 
over state trajectories corresponding to an equivalence class of F (t, .)’s which differ 
only on a nullset. The boundary conditions on the Hamiltonian are meaningless in 
this context, because the values of t → H (t, x(t), p(t)) ¯ at t = S¯ and t = T¯ are not 
the same across F’s in an equivalence class. 
When the time dependence of F (t, x) is in some sense Lipschitz continuous, 
the dilemma of how to define boundary conditions on the Hamiltonian is resolved 
by showing that the equivalence class of functions almost everywhere equal to 
t → H (t, x(t), p(t)) ¯ has a continuous representative r. We can then express the 
boundary condition in terms of r, i.e. condition (9.1.2) is satisfied with 
(ξ0, ξ1) = (r(S), r( ¯ T )). ¯
The analysis supplies additional information about the Hamiltonian evaluated along 
t → (x(t), p(t)) ¯ (or, more precisely, about r): this reduces to the well-known 
condition that the Hamiltonian is a.e. equal to a constant in the case when F is 
independent of t. 
For measurably time dependent data, there is no convenient representative of the 
equivalence class, in terms of which the boundary conditions on the Hamiltonian 
can conveniently be expressed. In this more general setting, we take a different 
approach. This involves replacing point evaluation of the Hamiltonian by another 
operation, namely calculating the ‘essential values’ of the Hamiltonian. Taking 
essential values or, to be more precise sub and super essential values, is a gener￾alization of point evaluation which, significantly, is unaffected by modifications on 
a null-set. 
We derive conditions that are satisfied not merely by minimizers for (F T ) but 
by W1,1 local minimizers. In the present circumstances, the notion of a ‘W1,1 local 
minimizer’ must be modified, to take account of the fact that the underlying time 
interval [S,T ] is a choice variable. 
Given a multifunction B : (−∞, +∞) ⇝ Rn, we say that an admissible F
trajectory ([S,¯ T¯], x) is a W1,1 local minimizer for (F T ) relative to B if there exists 
some β > 0 such that 
g(S, x(S), T , x(T )) ≥ g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯
for all admissible F trajectories ([S,T ], x) satisfying the following properties: 
x(t) ˙ ∈ ˙
x(t) ¯ + B(t) for a.e. t ∈ [S,¯ T¯]∩[S,T ] and 
d(([S,T ], x), ([S,¯ T¯], x)) ¯ ≤ β. (9.1.3) 
Here, d is the following distance function on the space of absolutely continuous, Rn
valued functions x on a finite time interval: given two such functions ([S,T ], x) and 
([S'
, T '
], x'
)9.2 Lipschitz Time Dependence 415
d(([S,T ], x), ([S'
, T '
], x'
)) := |S − S'
|+|T − T '
|
+|x(S) − x'
(S'
)| +  T ∨T '
S∧S'
| ˙xe(s) − ˙x'
e(s)|ds, (9.1.4) 
in which, as usual, S ∧S' := min{S,S'
} and T ∨T ' := max{T,T '
}. Here xe denotes 
the extension to (−∞, +∞) of the function x : [S,T ] → Rn on the finite interval 
[S,T ], defined as follows: 
xe(t) :=
⎧
⎨
⎩
x(S) if t<S
x(t) if t ∈ [S,T ]
x(T ) if t>T.
Notice that, according to this definition, x˙e(t) = 0, for a.e. t /∈ [S,T ]. 
The subscript e indicating ‘extension to (−∞, +∞)’ is often omitted, for brevity, 
for instance in integrals where the domains of functions defining the integrand 
are smaller than the interval of integration. In all such cases, it is understood 
that the functions have been replaced by their extensions, as defined above, 
prior to evaluation of the integral; for example, the integral in (9.1.4) is written 
 T ∨T '
S∧S' | ˙x(t) − ˙x'
(t)|dt. The distance function (9.1.4) is a metric on the space of 
absolutely continuous, Rn valued functions on finite intervals, which we refer to as 
the W1,1 metric on this space. Obviously, it coincides with the standard W1,1 metric 
when evaluated at two functions that have the same domain. 
9.2 Lipschitz Time Dependence 
In this section we derive necessary conditions for the free end-time problem (F T ), 
when a Lipschitz continuity hypothesis is invoked, regarding the time dependence 
of F (t, x). The key feature of the analysis is a transformation of the independent 
variable, which generates information about the Hamiltonian evaluated along the 
minimizing state trajectory and associated costate trajectory. This we now briefly 
review. 
Suppose that ([S,¯ T¯], x)¯ is a W1,1 local minimizer (F T ). Consider a new fixed 
end-time problem (we shall call it the ‘transformed’ problem): 
(C)
'
⎧
⎪⎪⎨
⎪⎪⎩
Minimize g(τ (S), y( ¯ S), τ ( ¯ T ), y( ¯ T )) ¯
over (τ, y) ∈ W1,1([S,¯ T¯]; R1+n) satisfying
(τ (t), ˙ y(t)) ˙ ∈ {(w, wv) : w ∈ [1−ρ , 1+ρ], v ∈ F (τ (t), y(t))}, a.e. t ∈ [S,¯ T¯],
(τ (S), y( ¯ S), τ ( ¯ T ), y( ¯ T )) ¯ ∈ C.
Here, ρ ∈ (0, 1/2) is a given number. The significance of (C)' is that there exists 
a transformation G which carries absolutely continuous arcs (τ, y) satisfying the416 9 Free End-Time Problems
constraints of problem (C)' into admissible F trajectories for the original problem 
(FT). Furthermore this transformation preserves the value of the cost. To be precise, 
the transformation is 
G(τ, y) = ([τ (S), τ ( ¯ T )¯ ], y ◦ ψ−1),
in which ψ : [S,¯ T¯]→[S,T )] is the strictly increasing function 
ψ(s) = τ (S)¯ +
 s
S¯ w(σ )dσ, for s ∈ [S,¯ T¯] ,
S = τ (S)¯ and T = ψ(T ). ¯ Here, w : [S,¯ T¯]→[1 − ρ , 1 + ρ] is some measurable 
function such that 
(τ (s), ˙ y(s)) ˙ ∈ w(s)({1} × F (τ (s), y(s)) a.e. s ∈ [S,¯ T¯].
It follows that, since (τ (s) ¯ ≡ s, x)¯ transforms into ([S,¯ T¯], x)¯ , (τ ,¯ x)¯ is a minimizer 
for fixed end-time problem (C)'
. 
Under appropriate hypotheses on the data for (F T ), the data for (C)' satisfy the 
hypotheses for application of the necessary conditions of Chap. 8. These supply a 
cost multiplier λ ≥ 0 and adjoint arc components −r and p, corresponding to the 
state components τ and y. Of various conditions satisfied by r and p, particular 
interest attaches to the Weierstrass and transversality conditions: 
−r(t) + p(t) · ˙
x(t) ¯ = (9.2.1)
max{w(−r(t) + p(t) · v) : v ∈ F (t, x(t)), w ¯ ∈ [1 − ρ , 1 + ρ]} a.e. t
and 
(−r(S), p( ¯ S), r( ¯ T ), ¯ −p(T )) ¯ ∈
λ∂g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ + NC(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )). ¯ (9.2.2) 
Fixing w = 1, we recover from (9.2.1) the Weierstrass Condition 
p(t) · ˙
x(t) ¯ = max{p(t) · v : v ∈ F (t, x(t)) ¯ }
for (F T ). However, further information is now obtained if we fix v = ˙
x(t) ¯ , namely 
p(t) · ˙
x(t) ( ¯ = H (t, x(t), p(t)) ) ¯ = r(t) a.e.,
(where H (t, x, p) is the Hamiltonian (9.1.1)). We see from this relation and (9.2.2) 
that the transversality condition provides information about the ‘boundary values’ 
of the Hamiltonian evaluated along (x, p) ¯ . The boundary values are interpreted as9.2 Lipschitz Time Dependence 417
the end-points of some absolutely continuous function which coincides with the 
Hamiltonian almost everywhere. 
Of course this analysis is justified only when the data for (C)' satisfy the 
hypotheses for application of suitable necessary conditions of optimality. Since 
the independent variable t in (F T ) becomes a state variable τ in (C)' and since 
currently available necessary conditions require the data to be, in some generalized 
sense, ‘differentiable’ with respect to the state variable, the analysis will effectively 
be limited to problems for which the data for (FT) is regular regarding also its time 
dependence. 
The following theorem provides necessary conditions of optimality for free end￾time problems. The derivation is based on the preceding ideas, but modified to allow 
for F’s which are unbounded. This complicating feature necessitates constraining 
w to lie in some time dependent set w(t) ∈ [1 − ρ(t), 1 + ρ(t)]). The use of 
proof techniques that treat time as a state-like variable affects the nature of the 
regular velocity set, in terms of which the Weierstrass condition is expressed. The 
appropriate definition of the regular velocity set, written Ω˜ 0(t) and called the (t, x)-
regular velocity set, now becomes 
Ω˜ 0(t) := {e ∈ F (t, x(t)) ¯ : F is pseudo−Lipschitz continuous near((t, x(t)), e) ¯ }.
(9.2.3) 
(Notice that, in this definition, points in the regular velocity set are identified via the 
pseudo-Lipschitz continuity property of F jointly w.r.t. the t and x variables, not the 
x variable alone.) 
Theorem 9.2.1 (Free-Time Necessary Conditions: Lipschitz Time Dependence) 
Let ([S,¯ T¯], x)¯ be a W1,1 local minimizer for (FT) (relative to B ≡ Rn) such that 
T¯ − S >¯ 0. Assume that the following hypotheses are satisfied: 
(G1): g is Lipschitz continuous on a neighbourhood of (S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ and C is 
a closed set, 
(G2): F (t, x) is non-empty for all (t, x) ∈ R × Rn and Gr F is closed, 
(G3): There exist ϵ > 0 and a measurable function R : [S,¯ T¯] → (0,∞) ∪ {+∞}
(a ‘radius function’) such that the following conditions are satisfied: 
(a): (Pseudo-Lipschitz Continuity) There exists k ∈ L1(S,¯ T )¯ such that 
F (t'
, x'
) ∩ (˙
x(t) ¯ + R(t) ◦
B) ⊂ F (t'', x'') + kF (t)|(t'
, x'
) − (t'', x'')|B ,
for all (t'
, x'
), (t'', x'') ∈ (t, x(t)) ¯ + ϵB, a.e. t ∈ [S,¯ T¯] ,
(b): (Tempered Growth) There exist r ∈ L1(S,¯ T )¯ , r0 > 0 and γ ∈ (0, 1) such 
that r0 ≤ r(t), γ −1r(t) ≤ R(t) a.e. t ∈ [S,¯ T¯] and 
F (t'
, x'
) ∩ (˙
x(t) ¯ + r(t)B) /= ∅, for all (t'
, x'
) ∈ (t, x(t)) ¯ + ϵB, 
a.e. t ∈ [S,¯ T¯].418 9 Free End-Time Problems
Then there exist absolutely continuous arcs p ∈ W1,1([S,¯ T¯]; Rn) and a ∈
W1,1([S,¯ T¯]; R) and a number λ ≥ 0 such that 
(i): (p, λ) /= (0, 0), 
(ii): (− ˙a(t), p(t)) ˙ ∈ co{(ζ, η) : ((ζ, η), p(t)) ∈ NGr F ((t, x(t)), ¯ ˙
x(t)) ¯ },
a.e. t∈[S,¯ T¯],
(iii): (−a(S), p( ¯ S), a( ¯ T ), ¯ −p(T )) ¯ ∈ λ∂g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ +NC(S,¯ x(¯ S), ¯ T,¯ x(¯ T )) ¯ , 
(iv): p(t) · ˙
x(t) ¯ ≥ p(t) · v for all v ∈ co Ω˜ 0(t) , a.e. t ∈ [S,¯ T¯], 
(v): a(t) = p(t) · ˙
x(t) ¯ a.e. t ∈ [S,¯ T¯]. 
If the initial time S¯ is fixed, i.e. C = {(S, x ¯ 0,T,x1) : (x0,T,x1) ∈ C˜}, for some 
closed set C˜ ⊂ Rn × R × Rn, and g(S, x ¯ 0,T,x1) = ˜g(x0,T,x1) for some g˜ :
Rn × R × Rn → R, then the above conditions are satisfied when (iii) is replaced 
by 
(iii)'
: (p(S), a( ¯ T ), ¯ −p(T )) ¯ ∈ λ∂g(x(¯ S), ¯ T ,¯ x(¯ T )) ¯ + NC˜(x(¯ S), ¯ T ,¯ x(¯ T )) ¯ . 
The assertions of the theorem can be similarly modified when the final time T¯ is 
fixed. 
Remark 
(The Autonomous Case) An important aspect of this theorem is its implications for 
autonomous problems (problems for which F (t, x) is independent of t; write F (x)
in place of F (t, x)). In this case, condition (ii) of the theorem implies that the adjoint 
arc p satisfies 
p(t) ˙ ∈ co{η : (η, p(t)) ∈ NGr F (x(t), ¯ ˙
x(t)) ¯ } a.e.
(the Euler Lagrange Inclusion) and a(t) = c a.e. for some c ∈ R. Then, from (v), 
p(t) · ˙
x(t) ¯ = c a.e. t ∈ [S,¯ T¯].
Information about the constant c is provided by the transversality condition (iii). 
Recall how, in Chap. 8, Theorem 8.4.3 (the Euler Lagrange inclusion) served 
as a starting point for deriving necessary conditions of optimality, under other 
hypotheses, listed in Proposition 8.5.1, that replace the ‘pseudo Lipschitz continuity 
and tempered growth’ hypothesis in the theorem statement. Analogues of each these 
hypotheses can be employed, in place of hypothesis (G3) in Theorem 9.2.1. In the 
following corollary, which is an immediate consequence of Proposition 8.5.1 of 
Chap. 8, we focus merely on the version of Theorem 9.2.1 that features the third 
alternative hypothesis, namely (G3)***, of Proposition 8.5.1. 
Corollary 9.2.2 The assertions of Theorem 9.2.1 remain valid when hypothesis 
(G3) is replaced by: 
(G3)***: There exist numbers α > 0 and ϵ > 0, and non-negative measurable 
functions k and β such that k and t → β(t)kα(t) are integrable and, for each 
N ≥ 0,9.2 Lipschitz Time Dependence 419
F (t'', x'') ∩ (˙
x(t) ¯ + NB) ⊂ F (t'
, x'
) + kN (t)|(t'', x'') − (t'
, x'
)|B, 
for all (t'', x''), (t'
, x'
) ∈ (t, x(t)) ¯ + ϵB, a.e. t ∈ [S,¯ T¯]
where kN (t) := k(t) + β(t)Nα .
In this case, the Weierstrass condition (iv) becomes 
p(t) · ˙
x(t) ¯ ≥ p(t) · v, for all v ∈ F (t, x(t)), ¯ a.e. t ∈ [S,¯ T¯] .
Proof of Theorem 9.2.1 Assume that ([S,¯ T¯], x)¯ is a W1,1 local minimizer for (FT) 
(relative to B ≡ Rn) and (G1)–(G3) are satisfied. We know then that there exists 
β > 0 such that ([S,¯ T¯], x)¯ is a minimizer with respect to all F trajectories 
([S,T ], x) satisfying the constraints of (F T ) and 
d(([S,T ], x), ([S,¯ T¯], x)) ( ¯ = |S − S¯|+|T − T¯|+|x(S) − ¯x(S)¯ |
+
 T ∨T¯
S∧S¯ | ˙x(t) − ˙
x(t) ¯ |dt ) ≤ β . (9.2.4) 
Take α > 0 such that 
α<1
2
	
∧
 1
2r0
	
and
1 + α
1 − αr0
≤
2
1 + γ . (9.2.5) 
Here, γ > 0 and r0 > 0 are the parameters of hypothesis (G3)(b). Define 
ρα(t) :=
αr0
1 + |˙
x(t) ¯ |
, t ∈ [S,¯ T¯] .
Note that ρα(t) ∈ (0, 1/2). Now consider the following fixed end-time dynamic 
optimization problem (the ‘transformed’ problem): 
(T )
⎧
⎪⎪⎨
⎪⎪⎩
Minimize g(τ (S), y( ¯ S), τ ( ¯ T ), y( ¯ T )) ¯
over (τ, y) ∈ W1,1([S,¯ T¯]; R1+n) satisfying
(τ (s), ˙ y(s)) ˙ ∈ F (s, τ (s), y(s)) ˜ a.e., s ∈ [S,¯ T¯]
(τ (S), y( ¯ S), τ ( ¯ T ), y( ¯ T )) ¯ ∈ C .
Here F˜ : R × R × Rn ⇝ R × Rn is the multifunction 
F (s, τ, y) ˜ := {(w, wv) : w ∈ [1 − ρα(s), 1 + ρα(s)], v ∈ F (τ, y)} .
Claim: (τ (s) ¯ := s, x)¯ is a W1,1 local minimizer for the fixed end-time problem (T). 
We confirm the claim. Take β' > 0, and let (τ, y) be any admissible F˜ trajectory 
such that420 9 Free End-Time Problems
|(τ (S), y( ¯ S)) ¯ − (τ (¯ S), ¯ x(¯ S)) ¯ | +  T¯
S¯ |(τ (s), ˙ y(s)) ˙ − (1, ˙
x(s)) ¯ |ds ≤ β' . (9.2.6) 
In consequence of the generalized Filippov selection theorem (Theorem 2.3.14), 
there exist measurable functions w and v such that 
w(s) ∈ [1 − ρα(s), 1 + ρα(s)] and v(s) ∈ F (τ (s), y(s)) a.e.,
τ (s) ˙ = w(s), y(s) ˙ = w(s)v(s) a.e..
Consider now the transformation ψ : [S,¯ T¯]→[S,T ], 
ψ(s) := τ (S)¯ +
 s
S¯ w(σ )dσ , (9.2.7) 
S := τ (S)¯ and T := τ (S)¯ +  T¯
S¯ w(σ )dσ. The function ψ is strictly increasing 
and invertible, and ψ and ψ−1 are absolutely continuous functions with essentially 
bounded derivatives. It follows that x(t) := (y ◦ψ−1)(t) is an absolutely continuous 
function. We deduce from Lemma 9.7.1 of the Appendix that β' > 0 in (9.2.6) 
and α > 0 in (9.2.5) can be chosen independent of our selection of (τ, y)
satisfying (9.2.6), such that 
d(([S,T ], x), ([S,¯ T¯], x)) ¯ ≤ β , (9.2.8) 
where β > 0 is as in (9.2.4). Take any t ∈ [S,T ] and let s := ψ−1(t). We deduce 
from Fubini’s theorem and the chain rule that, for each t ∈ [S,T ], 
 t
S
(v ◦ ψ−1)(t'
)dt' =
 s
S¯ v(s'
)w(s'
)ds' =
 s
S¯ y(s ˙ '
)ds'
=
 s
S¯
d
ds(x ◦ ψ)(s'
)ds' =
 s
S¯
(x˙ ◦ ψ)(s'
)
d
ds ψ(s'
)ds'
=
 s
S¯
(x˙ ◦ ψ)(s'
)w(s'
)ds' =
 t
S
x(t ˙ '
)dt'
.
It follows that x(t) ˙ = v ◦ ψ−1(t) a.e.. But then, since ψ and ψ−1 map Lebesgue 
null-sets into null-sets, 
x(t) ˙ = (v ◦ ψ−1)(t) ∈ F ((τ ◦ ψ−1)(t), (y ◦ ψ−1)(t)) = F (t, x(t)) a.e..
From the fact that (τ (s), y(s)) = (ψ(s), (x ◦ ψ)(s)) for all s ∈ [S,¯ T¯] we deduce 
that S = τ (S)¯ , T = τ (T )¯ , x(S) = y(τ (S)) ¯ and x(T ) = y(τ (T )) ¯ . We have shown 
that ([S,T ], x) is an admissible F trajectory for problem (FT). Since 
(S, x(S), T , x(T )) = (τ (S), y( ¯ S), τ ( ¯ T ), y( ¯ T )) , ¯9.2 Lipschitz Time Dependence 421
we can deduce from the W1,1 local optimality of ([S,¯ T¯], x)¯ that 
g(τ (S), y( ¯ S), τ ( ¯ T ), y( ¯ T )) ¯ = g(S, x(S), T , x(T ))
≥ g(S,¯ x(S), ¯ T ,¯ x(T )) ¯ = g(τ (¯ S), ¯ x(¯ S), ¯ τ (¯ T ), ¯ x(¯ T )) . ¯
Since (τ, y) was an arbitrary admissible F˜ trajectory satisfying (9.2.6), we conclude 
that (τ ,¯ x)¯ is a W1,1 local minimizer for (T). The claim is confirmed. Simple changes 
to the preceding analysis (in which the change of independent variable is now taken 
to be t := S¯+ s
S¯ w(σ )dσ) validate the claim, also in the case when the left end-time 
S¯ is fixed, described at the end of the theorem statement. The case when the right 
end-time is fixed is similarly treated. 
We aim next to applying the necessary conditions of Chap. 8 to (τ (s) ≡ s, x)¯ , 
regarded as a W1,1 local minimizer. First, let us check that the data for (T) satisfies 
the hypotheses (G1), (G2) and (G3) of Theorem 8.4.3 of Chap. 8. 
The data for problem (T) satisfies hypotheses (G1), (G2) of Theorem 8.4.3, 
in simple consequence of our assumption that the data of problem (FT) satisfies 
hypotheses (G1) and (G2) of Theorem 9.2.1. In the analysis below, we will refer 
to the integrable Lipschitz bound k, the radius function R and the parameters r, 
r0, γ ∈ (0, 1) appearing in (G3) of Theorem 9.2.1. We now show that (G3) is also 
satisfied when, in this hypothesis, the function r and parameters r0 and ϵ > 0 remain 
the same, but R, γ and k are replaced by R˜, γ˜ and k˜, where 
R(t) ˜ =
1 + γ
2
	
R(t), γ˜ :=
2γ
1 + γ
and k(t) ˜ = √
2(1 + αr0)k(t) .
Notice that γ˜ ∈ (0, 1) and k˜ ∈ L1; furthermore 
r0 ≤ r(t) ≤ γ R(t) ≤ γ
 2
1 + γ
	
R(t) ˜ = ˜γ R(t). ˜
Consider (G3)(a). For a.e. t ∈ [S,¯ T¯] take (τ, x), (τ '
, x'
) ∈ (t, x(t)) ¯ + ϵB and 
(w, wv) ∈ F (t, τ, x) ˜ ∩ ((1, ˙
x)(t)) ¯ + R(t) ˜ ◦
B). Then |(w, wv) − (1, ˙
x)(t) ¯ |< R(t) ˜ . It 
follows that |v − w−1 ˙
x(t) ¯ |< w−1R(t) ˜ . Hence 
|v − ˙
x(t) ¯ |≤|w−1 − 1| |˙
x(t) ¯ | + w−1R(t) . ˜
But |w−1−1| |˙
x(t) ¯ | ≤ 1
w
 αr0
1+|˙
x(t) ¯ |
	
×|˙
x(t) ¯ | ≤ αr0
w and |w−1| ≤ (1−αr0)−1. Noting 
also that r0 ≤ R(t) ˜ , we see that 
|v − ˙
x(t) ¯ |<
1 + α
1 − αr0
R(t) ˜ ≤
2
1 + γ
R(t) ˜ = R(t) .422 9 Free End-Time Problems
Since v ∈ F (τ, x) ∩ (˙
x(t) ¯ + R(t) ◦
B), we know from (G3)(a) that there exists v' ∈
F (τ '
, x'
) such that |v' − v| ≤ k(t)(|x − x'
|+|τ − τ '
|). But then, since w ∈ [1 −
ρα(t), 1 + ρα(t)], (w, wv'
) is an element in F (t, τ ˜ '
, x'
) and 
|(w, wv'
) − (w, wv)| = w|v' − v|≤|w| k(t)(|x − x'
|+|τ − τ '
|)
≤ (1 + αr0)k(t)(|x − x'
|+|τ − τ '
|) ≤ k(t) ˜ |(τ, x) − (τ '
, x'
)| .
The existence of a point (w, wv'
) satisfying this relation implies 
F (t, τ, x) ˜ ∩ ((1, ˙
x(t)) ¯ + R(t) ˜ ◦
B) ⊂ F (t, τ ˜ '
, x'
) + k(t) ˜ |(τ, x) − (τ '
, x'
)|B .
We have shown that (G3)(a) is satisfied with the integrable Lipschitz bound k(t) ˜ . 
Take any (τ, x) ∈ (t, x(t)) ¯ + ϵB. Then, by (G3)(b), we know that there exists 
v ∈ F (τ, x) such that |v − ˙
x(t) ¯ | ≤ r(t). But then the element (1, v) ∈ F (t, τ, x) ˜
satisfies |(1, v) − (1, ˙
x(t)) ¯ | ≤ r(t). We have shown 
F (t, τ, x) ˜ ∩ (˙
x(t) ¯ + r(t)B) /= ∅ .
Since, as we have already observed, r0 ≤ r(t) ≤ ˜γ R(t) ˜ , we have confirmed that 
(G3)(b) is also satisfied. 
The preceding analysis justifies applying Theorem 8.4.3 to (T), with reference to 
the W1,1 local minimizer (τ¯ ≡ 1, x)¯ . We deduce that there exist λ ≥ 0, and costate 
trajectory components −a and p such that 
(λ, (a, p)) /= (0, (0, 0)), (9.2.9) 
(− ˙a(s), p(s)) ˙ ∈ co{(η1, η2) : ((η1, η2), (−a(s), p(s)) (9.2.10)
∈ NGrF (s,.,.) ˜ ((s, x(s)), ( ¯ 1, ˙
x(s))) ¯ }, a.e. s ∈ [S,¯ T¯]
and 
w[−a(s) + p(s) · v]≤−a(s) + p(s) · ˙
x(s) ¯
for all (w, v) ∈ Ω0(t) , a.e. t ∈ [S,¯ T¯]. (9.2.11) 
Here, Ω0 : [S,¯ T¯] ⇝ R1+n (the regular velocity set for F˜ relative to (τ¯ ≡ t, x)¯ ) is 
the multifunction 
Ω0(t) := {(e0, e1) ∈ F (t, t, ˜ x(t)) ¯ : F (t, ., .) ˜ is pseudo Lipschitz
continuous near ((t, x(t)), (e ¯ 0, e1))} for t ∈ [S,¯ T¯] .9.2 Lipschitz Time Dependence 423
We deduce from the tranversality condition that 
(−a(S), p( ¯ S), ¯ +a(T ), ¯ −p(T )) ¯ ∈ NC(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯
+ λ∂g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) . ¯ (9.2.12) 
A careful analysis of the Euler Lagrange inclusion (9.2.10) and applying basic 
properties of proximal normals, yields the relation 
(− ˙a(t), p(t)) ˙ ∈ co {(η1, η2) : ((η1, η2), p(t))
∈ NGrF ((t, x(t)), ¯ ˙
x(t)) ¯ 
 a.e.. (9.2.13) 
Let us now explore the implications of the Weierstrass condition (9.2.11). It is a 
straightforward exercise to deduce from hypothesis (G3) that 
{(w, w ˙
x(t)) ¯ | |w − 1| ≤ ρα(t)} ⊂ Ω0(t), for a.e. t ∈ [S,¯ T¯] (9.2.14) 
and 
{(1, v) : v ∈ Ω˜ 0(t)} ⊂ Ω0(t), for a.e. t ∈ [S,¯ T¯] , (9.2.15) 
in which Ω˜ 0(t) is (t, x)-regular velocity set defined by (9.2.3). 
It follows from (9.2.11) and (9.2.15) and the linearity of v → p(s) · v that, for 
a.e. t ∈ [S,¯ T¯], 
p(t) · ˙
x(t) ¯ = max{p(t) · v : v ∈ co Ω˜ 0(t)}. (9.2.16) 
On the other hand, it follows from (9.2.11) and (9.2.14) that w → w(−a(t) + p(t)· ˙
x(t)) ¯ is maximized over w ∈ [1 − ρα(t), 1 + ρα(t)] at w = 1. We conclude that 
a(t) = p(t) · ˙
x(t), ¯ a.e. t ∈ [S,¯ T¯] . (9.2.17) 
This last condition implies that a ≡ 0 if p ≡ 0. Since (λ, a, p) /= (0, 0, 0), it 
follows that 
(λ, p) /= (0, 0) . (9.2.18) 
Reviewing relations (9.2.12), (9.2.13), (9.2.16) and (9.2.18), we see that the 
assertions of the theorem have been validated, in the case that ([S,¯ T¯], x)¯ is a W1,1
local minimizer, under hypotheses (G1)–(G3). 
The final assertions of the theorem, concerning problems in which the left 
end-time is fixed are proved by a simple modification of the preceding analysis, 
in which the transformation ψ(s) = S¯ +  s
S¯ w(s'
)ds' replaces (9.2.7) and 
(τ (S), y( ¯ S), τ ( ¯ T ), y( ¯ T )) ¯ ∈ {S¯} × C˜ replaces the end-point constraint in (T).
⨅⨆424 9 Free End-Time Problems
9.3 Essential Values 
The operation of taking the ‘essential value’ (or, to be more specific, the sub or super 
essential value) of a given real valued function on the real line is a generalization 
of point evaluation of a continuous function. Its most important property is that 
the essential values of a function are unaltered if the function is adjusted on a set 
of Lebesgue measure zero. This will be exploited to interpret necessary conditions 
for free end-time dynamic optimization problems with measurably time dependent 
data, specifically to make sense of boundary conditions on the Hamiltonian. 
Definition 9.3.1 Take an closed interval [S,T ] ⊂ R, an integrable function a :
[S,T ] → R and a point t ∈ (S, T ). 
(a): The sub essential value of f at t
¯ is the set 
sub-ess t→t
¯ a(t) :=

ζ ∈ R : ∃ti → t
¯ and ζi → ζ s.t.
lim sup
ϵ↓0
ϵ−1
 ti
ti−ϵ
a(s)ds ≤ ζi ≤ lim inf ϵ↓0
 ti+ϵ
ti
a(s)ds, for each i

.
(b): The super essential value of a at t is the set 
super-ess t→t
¯
a(t) :=

ζ ∈ R : ∃ti → t
¯ and ζi → ζ s.t.
lim sup
ϵ↓0
ϵ−1
 ti+ϵ
ti
a(s)ds ≤ ζi ≤ lim inf ϵ↓0
 ti
ti−ϵ
a(s)ds, for each i

.
If a(t, x) is a function of two variables, sub-ess t→t
¯ a(t, x) denotes the sub essential 
value of t → a(t, x), for fixed x, etc. 
The operations of taking sub and super essential values generate multifunctions 
t → sub-ess t→t
¯ a(t) and t → super-ess t→t
¯
a(t)
taking values closed, possibly unbounded, sets. These multifunctions are of interest 
in dynamic optimization primarily because of the role they play in describing the 
sub- and super subdifferentials of an indefinite integral function. 
Proposition 9.3.2 Take an interval [S,T ] ⊂ R, an integrable function a :
[S,T ] → R and a point t
¯ ∈ (S, T ). Fix any S' ∈ [S,T ] and define the function 
ψ : [S,T ] → R to be9.3 Essential Values 425
ψ(t) :=  t
S'
a(s)ds .
Then 
∂ψ(t)¯ = sub-ess t→t
¯ a(t) and −∂(−ψ)(t)¯ = super-ess t→t
¯
a(t) .
In words, ‘the limiting subdifferential of ψ at t
¯ is the sub essential value of a at t
¯’ 
and ‘the limiting super differential of ψ at t
¯ is the super essential value of a at t
¯’. 
Observe that, since the sub essential value of a and the super essential value of a at 
a point t
¯ ∈ (S, T ) do not depend on S' ∈ [S,T ], the sub- and super differentials 
of the indefinite integral function ψ at t
¯ do not depend on the choice of the point 
S' ∈ [S,T ] used to define ψ, either. 
Proof We provide the proof of only the first assertion, since that of the second is 
the same (following a reversal of inequalities). 
It can be deduced from the definition of strict subdifferential, Proposition 4.4.3 
and Theorem 4.6.3 that ζ ∈ ∂ψ(t)¯ if and only if there exists sequences ti → t
¯ and 
ζi → ζ such that, for each i, 
lim inf ϵ↓0
ϵ−1(ψ(ti + ϵ) − ψ(ti)) ≥ ζi and lim inf ϵ↓0
ϵ−1(ψ(ti − ϵ) − ψ(ti)) ≥ −ζi .
It now follows from the definition of ψ that this last condition is equivalent to 
lim sup
ϵ↓0
ϵ−1
 ti
ti−ϵ
a(s)ds ≤ ζi ≤ lim inf ϵ↓0
ϵ−1
 ti+ϵ
ti
a(s)ds .
We have shown that ζ ∈ ∂ψ(t)¯ if and only if ζ ∈ sub-ess t→t
¯ a(t).
⨅⨆
The following proposition provides further information about sub and super essen￾tial values of a. It tells us, in particular, that these sets are unaffected by changing 
the values of a on a (Lebesgue) nullset. It also provides estimates of sub and super 
essential values in terms of a multifunction which, for some applications, is simpler 
to calculate while supplying an adequate approximation. 
Proposition 9.3.3 Take an interval [S,T ] ⊂ R, and two integrable functions a, b :
[S,T ] → R and t
¯ ∈ (S, T ). Then 
(a): If a(t) = b(t) a.e., then 
sub-ess t→t
¯ a(t) = sub-ess t→t
¯ b(t) and super-ess t→t
¯
a(t) = super-ess t→t
¯
b(t),426 9 Free End-Time Problems
(b): sub-ess t→t
¯ a(t) ⊂ [inf
ϵ↓0

ess inf
t∈[t
¯−ϵ,t
¯+ϵ]
a(t)	
,sup
ϵ↓0

ess sup
t∈[t
¯−ϵ,t
¯+ϵ]
a(t)	
]
and 
super-ess t→t
¯
a(t) ⊂ [inf
ϵ↓0

ess inf
t∈[t
¯−ϵ,t
¯+ϵ]
a(t)	
,sup
ϵ↓0

ess sup
t∈[t
¯−ϵ,t
¯+ϵ]
a(t)	
],
(c): If a is essentially bounded on a neighbourhood of t
¯ then 
co sub-ess t→t
¯ a(t) = co super-ess t→t
¯
a(t)
= [inf
ϵ↓0

ess inf
t∈[t
¯−ϵ,t
¯+ϵ]
a(t)	
,sup
ϵ↓0

ess sup
t∈[t
¯−ϵ,t
¯+ϵ]
a(t)	
] .
Proof Properties (a) and (b) are straightforward consequences of the definitions. 
We attend only to the proof of (c). Write ζ + := lim
ϵ↓0

ess sup
s∈[t
¯−ϵ,t
¯+ϵ]
a(s)	
and ζ − :=
lim
ϵ↓0

ess inf
s∈[t
¯−ϵ,t
¯+ϵ]
a(s)	
. (ζ + and ζ − are finite numbers because of the ‘essential 
boundedness’ hypothesis.) Denote by S the Lebesgue points of a : [S,T ] → R. 
Since S has full Lebsgue measure, there exists a sequence of points ti → t
¯such that 
ti ∈ S for each i and a(ti) → ζ + as i → ∞. 
For each i, by the Lebesgue point property, a(ti) = lim
ϵ↓0
ϵ−1  ti+ϵ
ti a(s)ds =
lim
ϵ↓0
ϵ−1  ti
ti−ϵ a(s)ds. But then, for each i, 
lim sup
ϵ↓0
ϵ−1
 ti
ti−ϵ
a(s)ds ≤ a(ti) ≤ lim inf ϵ↓0
ϵ−1
 ti+ϵ
ti
a(s)ds
It follows that ζ + = lim
i→∞a(ti) ∈ sub-ess t→t
¯ a(t). Likewise we show that ζ − ∈
sub-ess t→t
¯ a(t). 
But then [ζ −, ζ +] ⊂ co
sub-ess t→t
¯ a(t)	
. Combining this inclusion with property 
(b), we conclude that [ζ −, ζ +] = co
sub-ess t→t
¯ a(t)	
. The proof that [ζ −, ζ +] =
co
super-ess t→t
¯
a(t)	
is similar.
⨅⨆
The following closure properties of the sub and super essential values follow 
directly from the above representation of the limiting sub- and super essential values 
and the analogous closure properties of sub- and super-differentials. 
Proposition 9.3.4 Take an interval [S,T ] ⊂ R, a set A ⊂ Rn and a function 
a : [S,T ] × A → R. Assume that9.4 Measurable Time Dependence 427
(a): t → a(t, x) is integrable on [S,T ] for each x ∈ A, 
(b): x → a(t, x) is continuous on A, uniformly over t ∈ [S,T ]. 
Then, for each (t, x) ¯ ∈ (S, T ) × A, 
(i): sub-ess t→t
¯ a(t, x) is closed, 
(ii): For any sequences ti → t
¯, xi
A
→ x and ζi → ζ such that ζi ∈ sub-ess t→ti
a(ti, xi)
for each i, we have 
ζ ∈ sub-ess t→t
¯ a(t, x) ,
(iii): For any sequences ti → t
¯, xi
A
→ x and ζi → ζ such that ζi ∈
super-ess t→ti
a(ti, xi) for each i, we have 
ζ ∈ super-ess t→t
¯
a(t, x) .
9.4 Measurable Time Dependence 
In this section, we derive necessary conditions for (F T ), under hypotheses which 
require the differential inclusion to have right side F (t, x) which is merely 
measurable with regard to time. The motivation for treating the measurable time 
dependence case is partly to unify the theory of necessary conditions for fixed and 
free end-time dynamic optimization problems. A framework which requires the 
dynamic constraint to be merely measurable with respect to time is widely adopted 
for fixed end-time problems. Why should extra regularity be required for free end￾time problems? 
But there are also practical reasons for developing a theory of free end-time 
problems, which allows the ‘dynamic constraint’ to be discontinuous with respect to 
time. Dynamic optimization problems arising in resource economics, for example, 
typically require us to find harvesting/investment strategies to minimize a cost which 
involves an integral cost of the form 
−
 T
0
c(t, x(t), u(t))dt.
Here, c is a given function representing the rate of return on harvesting effort and 
investment. It is natural in this context to consider problems in which T is a choice 
variable; the company can choose the harvesting period. Special cases are of interest, 
in which c is discontinuous with respect to time, to take account, for example,428 9 Free End-Time Problems
of abrupt changes in interest rates (reflecting, perhaps, penalties incurred for late 
completion). When the integral cost term is absorbed into the dynamics by means 
of ‘state augmentation’ the resulting dynamics will be discontinuous with respect 
to time. Furthermore, since we can expect an optimal strategy to terminate at the 
instant when there is an abrupt, unfavourable change in the rate of return, it is not 
satisfactory to develop a theory in which it is assumed that the optimal end-time 
occurs at a point of continuity of the data; rather we should allow for the possibility 
that discontinuities and optimal end-times interact. 
The necessary conditions in the following theorem bring together versions of the 
Euler Lagrange inclusion, the Weierstrass condition and the transversality condition 
for free end-time problems, where the dynamic constraint takes the form of a 
differential inclusion. Here, the formulation of the Weierstrass condition involves, 
for a.e. t ∈ [S,¯ T¯], the regular velocity set Ω0(t) for the differential inclusion, 
relative to the nominal F trajectory x¯: 
Ω0(t) := {e ∈ F (t, x(t)) ¯ : F (t, .) is pseudo Lipschitz
continuous near (x(t), e) ¯ }, for t ∈ [S,¯ T¯] .
and the transversality condition is expressed in terms of essential values of the 
Hamiltonian at the optimal end-times. The Hamiltonian H (t, x, p) is, as usual, 
taken to be: 
H (t, x, p) = sup
v∈F (t,x)
p · v.
Theorem 9.4.1 (Free End-Time Generalized Euler Lagrange Condition: Mea￾surable Time Dependence) Take a measurable multifunction B : R ⇝ Rn such 
that B(t) is open for a.e. t ∈ R. Let x¯ be a W1,1 local minimizer for (F T ) relative to 
B. Assume that, for some ϵ > 0 and σ ∈ (0, (T¯ − S)/ ¯ 2), the following hypotheses 
are satisfied: 
(G1): g is Lipschitz continuous on a neighbourhood of (S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ and C is 
a closed set, 
(G2): F (t, x) is a non-empty for all (t, x) ∈ R × Rn, Gr F (t, .) is closed for each 
t ∈ R and F is L × Bn measurable, 
(G3): There exists a measurable function R : [S¯ − σ, T¯ + σ] → (0,∞) ∪ {+∞}
(a ‘radius function’), such that R(t) ◦
B ⊂ B(t) a.e. t ∈ [S¯ − σ, T¯ + σ] and 
the following conditions are satisfied, 
(a): (Pseudo-Lipschitz Continuity) There exists k ∈ L1(S¯ − σ, T¯ + σ ) such that 
F (t, x'
) ∩ (˙
x(t) ¯ + R(t) ◦
B) ⊂ F (t, x) + k(t)|x' − x|B,
for all x, x' ∈ ¯x(t) + ϵB, a.e. t ∈ [S¯ − σ, T¯ + σ],9.4 Measurable Time Dependence 429
(b): (Tempered Growth) There existr ∈ L1(S¯ − σ, T¯ + σ ),r0 > 0 and γ ∈ (0, 1)
such that r0 ≤ r(t), γ −1r(t) ≤ R(t), a.e. t ∈ [S¯ − σ, T¯ + σ], and 
F (t, x) ∩ (˙
x(t) ¯ + r(t)B) /= ∅ for all x ∈ ¯x(t) + ϵB, a.e. t ∈ [S¯ − σ, T¯ + σ], 
(G4): There exists cF ≥ 0 such that, for a.e. t ∈ [S¯ − σ, S¯ + σ]∪[T¯ − σ, T¯ + σ], 
F (t, x) ⊂ cFB, for all x ∈ ¯x(t) + ϵB . (9.4.1) 
Then there exist p ∈ W1,1([S,¯ T¯]; Rn) and λ ≥ 0 such that 
(i): (p, λ) /= (0, 0), 
(ii): p(t) ˙ ∈ co{ζ : (ζ, p(t)) ∈ NGrF (t,·)(x(t), ¯ ˙
x(t)) ¯ }, a.e. t ∈ [S,¯ T¯], 
(iii): there exist ξ0 ∈ sub-esst→S¯H (t, x(¯ S), p( ¯ S)) ¯ and 
ξ1∈ super-esst→T¯ H (t, x(¯ T ), p( ¯ T )) ¯ such that 
(−ξ0, p(S), ξ ¯ 1, −p(T )) ¯ ∈ λ∂g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ + NC(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) , ¯
(iv): p(t) · ˙
x(t) ¯ ≥ p(t) · v, for all v ∈ co (Ω0(t) ∩ (˙
x(t) ¯ + B(t))) ,
a.e. t ∈ [S,¯ T¯]. 
If the initial time S¯ is fixed, i.e. C = {(S, x ¯ 0,T,x1) : (x0,T,x1) ∈ C˜}, for some 
closed set C˜ ⊂ Rn × R × Rn, and g(S, x0,T,x1) = ˜g(x0,T,x1) for some g˜ :
Rn × R × Rn → R, then the above conditions are satisfied when (iii) is replaced 
by: there exists ξ1 ∈ super-esst→T¯ H (t, x(¯ T ), p( ¯ T )) ¯ such that 
(iii)'
: (p(S), ξ ¯ 1, −p(T )) ¯ ∈λ∂g(˜ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ + NC˜(x(¯ S), ¯ T ,¯ x(¯ T )) ¯ , 
and when condition (9.4.1) in (G4) is satisfied only for a.e. t ∈ [T¯ − σ, T¯ + σ]. The 
assertions of the theorem can be similarly modified when the final time T¯ is fixed. 
We can use Proposition 8.5.1 of Chap. 8 immediately to obtain alternative 
versions of Theorem 9.4.1 in which the ‘pseudo Lipschitz continuity’ with ‘tem￾pered growth’ hypothesis (G3) is replaced by alternative, stronger hypotheses. 
We record here only one such corollary of Theorem 9.4.1, based on the third 
hypothesis in Proposition 8.5.1, where (G3) is replaced by the requirement that F is 
pseudo Lipschitz continuous for arbitrary constant radius functions R and that the 
associated integrable Lipschitz bounds have polynomial growth w.r.t. R. 
Corollary 9.4.2 The assertions of Theorem 9.4.1 remain valid when hypothesis 
(G3) is replaced by: 
(G3)***: There exist numbers α > 0 and ϵ > 0, and non-negative measurable 
functions k and β such that k and t → β(t)kα(t) are integrable and, for 
each N ≥ 0
F (t, x'') ∩ (˙
x(t) ¯ + NB) ⊂ F (t, x'
) + kN (t)|x'' − x'
|B, 
for all x'
, x'' ∈ ¯x(t) + ϵB and for a.e. t ∈ [S¯ − σ, T¯ + σ], 
where kN (t) := k(t) + β(t)Nα .430 9 Free End-Time Problems
In this case, the Weierstrass condition (iv) becomes 
p(t) · ˙
x(t) ¯ ≥ p(t) · v, for all v ∈ F (t, x(t)), ¯ a.e. t ∈ [S,¯ T¯] .
9.5 Proof of Theorem 9.4.1 
Preliminaries 
We show, by means of a straightforward adaptation of the proof of Proposition 8.6.2 
on Chap. 8, that, without loss of generality, (G1) and (G3) can be replaced by the 
stronger hypotheses: 
(G1)'
: g is Lipschitz continuous on R × Rn × R × Rn and C is a closed set, 
(G3)'
: There exist ϵ > 0, R ∈ (0,∞) , γ ∈ (0, 1) and k ∈ L1(S¯ − σ, T¯ + σ ) such 
that R ◦
B ⊂ B(t) a.e. t ∈ [S¯ − σ, T¯ + σ] and the following conditions are 
satisfied, 
(a): F (t, x'
) ∩ (˙
x(t) ¯ + R
◦
B) ⊂ F (t, x) + k(t)|x − x'
|B, for all x, x' ∈ ¯x(t) +
ϵB, a.e. t ∈ [S¯ − σ, T¯ + σ], 
(b): F (t, x)∩(˙
x(t) ¯ +γ RB) = ∅ / for all x ∈ ¯x(t)+ϵB, a.e. t ∈ [S¯ −σ, T¯ +σ]. 
We can arrange, by translation to the origin, that the W1,1 local minimizer of interest 
is ([S,¯ T¯], x¯ ≡ 0). We can also ensure, by redefining g on a neighbourhood of the 
optimal end-points, that g is (globally) Lipschitz continuous. 
To prove Theorem 9.4.1, we will follow a by now familiar pattern of analysis, 
a key step in which is to apply known necessary conditions to a minimizer 
([S'
, T '
], x'
) for a preliminary dynamic optimization problem, with a perturbation 
term in the cost that results from a choice of metric topology on the space of 
admissible F trajectories and application of Ekeland’s theorem. For the free end￾time problems here under consideration, we employ the metric d (see (9.1.4)), which 
gives rise to a cost perturbation term α d(([S,T ], x), ([S'
, T '
], x'
)), where α > 0
and 
d(([S,T ], x), ([S'
, T '
], x'
)) = |S − S'
|+|T − T '
|+|x(S) − x'
(S'
)|
+
 T ∨T '
S∧S'
| ˙xe(s) − ˙x'
e(s)|ds.
(We emphasize through our use of the subscript e notation that function extensions 
are used, to interpret the integral on the right side.) Here we encounter a difficulty: 
the perturbed cost is not of the right form for the application of earlier-derived 
necessary conditions, because it involves the integral  T ∨T '
S∧S' , not  T
S . The following 
lemma, whose simple proof we omit, overcomes this difficulty:9.5 Proof of Theorem 9.4.1 431 
Lemma 9.5.1 Take an interval [S,¯ T¯] and a number σ ∈ (0, (T¯ − S)/ ¯ 2). Now take 
intervals [S'
, T '
], [S,T ]⊂[S¯ − σ, T¯ + σ] and absolutely continuous functions 
x' : [S'
, T '
] → Rn and x : [S,T ] → Rn. Assume that, for some c >¯ 0, 
| ˙xe(t)|≤ ¯c and |˙
x¯e(t)|≤ ¯c for a.e. t ∈ [S¯ − σ, S¯ + σ]∪[T¯ − σ, T¯ + σ] .
Then 
 T '
∨T
S'
∧S
| ˙x'
e(s) − ˙xe(s)|ds ≤
 T ∧T '
S∨S'
| ˙x'
(s)− ˙x(s)|ds+2c¯

|S − S'
|+|T − T '
|
	
.
Since [S ∨ S'
, T ∧ T '
]⊂[S,T ], the lemma implies that, for absolutely continuous 
functions x, x' with the specified domains and satisfying the specified conditions, 
d(([S,T ], x), ([S'
, T '
], x'
)) ≤
 T
S
| ˙x(s)− ˙x'
e(s)|ds+(1+2c)¯

|S−S'
|+|T −T '
|
	
.
Since this relation holds with equality when ([S,T ], x) = ([S'
, T '
], x'
), we 
conclude that ([S'
, T '
], x'
) remains a minimizer when the perturbation term in the 
cost is replaced by a scaled version of 
 T
S
| ˙x'
(s) − ˙x(s)|ds + 2c¯

|S' − S|+|T ' − T |
	
.
Step 1 (A Free End-Point Problem) Our goal is to extend the Euler Lagrange-type 
conditions of Chap. 8 to cover problems with free end-times, now expressed in terms 
of essential values of the Hamiltonian at the optimal end-times. This is accomplished 
in several steps. First we derive necessary conditions for a problem in which there 
are no constraints on the end-times and end-states, and the differential inclusion 
F (t, .) is ‘globally’ Lipschitz continuous, with integrable Lipschitz constant, in the 
sense that the pseudo-Lipschitz condition is satisfied with radius function R ≡ +∞. 
Take a function g : R×Rn×R×Rn → R and a multifunction F : R×Rn ⇝ Rn. 
Consider the following free end-point dynamic optimization problem: 
(F EP )
⎧
⎨
⎩
Minimize g(S, x(S), T , x(T ))
over [S,T ] ⊂ R and x ∈ W1,1([S,T ]; Rn) satisfying
x(t) ˙ ∈ F (t, x(t)) a.e. t ∈ [S,T ] .
Proposition 9.5.2 Let ([S'
, T '
], x'
) be a W1,1 local minimizer for (FEP). Assume 
that, for some ϵ' > 0 and σ' ∈ (0, (T ' − S'
)/2), 
(FEP1)'
: g is Lipschitz continuous, 
(FEP2)'
: F (t, x) is a non-empty, closed set for all (t, x) ∈ R × Rn, Gr F (t, .) is 
closed for each t ∈ R and F is L × Bn measurable,432 9 Free End-Time Problems
(FEP3)'
: There exists a non-negative function k ∈ L1(S' − σ'
, T ' + σ'
) such that, 
for a.e. t ∈ [S' − σ'
, T ' + σ'
], 
F (t, x) ⊂ F (t, y) + k(t)|x − y|B , for all x, y ∈ x'
(t) + ϵ'
B,
(FEP4)'
: There exists cF ≥ 0 such that, for a.e. t ∈ [S' − σ'
, S' + σ'
]∪[T ' −
σ'
, T ' + σ'
], 
F (t, x) ⊂ cFB , for all x ∈ x'
(t) + ϵ'
B .
Then there exists p ∈ W1,1([S'
, T '
]; Rn) such that 
(A): p(t) ˙ ∈ co{ζ : (ζ, p(t)) ∈ NGr F (t,·)(x'
(t), x˙'
(t))}, a.e. t ∈ [S'
, T '
], 
(B): p(t) · ˙x'
(t) ≥ p(t) · v for all v ∈ F (t, x'
(t)), a.e. t ∈ [S'
, T '
], 
(C): there exist ξ0 ∈ sub-esst→S'H (t, x'
(S'
), p(S'
)) and 
ξ1 ∈ super-esst→T ' (t, x'
(T '
), p(T '
)) such that 
(−ξ0, p(S'
), ξ1, −p(T '
)) ∈ ∂g(S'
, x'
(S'
), T '
, x'
(T '
)) .
Proof Assume that β > 0 is a parameter such that the W1,1 local minimizer 
([S'
, T '
], x'
) is minimizing w.r.t. admissible F trajectories satisfying 
d(([S,T ], x), ([S'
, T '
], x'
)) ≤ β .
(Here d is the distance function (9.1.4)). 
Hypothesis Reduction We now show that, without loss of generality, we can 
restrict attention to the special case when (FEP3)' is replaced by the following 
stronger hypothesis, in which the non-negative function k is replaced by a con￾stant: 
(FEP3)∗: There exists a constant kF > 0 such that, for a.e. t ∈ [S' − σ'
, T ' + σ'
], 
F (t, x) ⊂ F (t, y) + kF |x − y|B ,
for all x, y ∈ x'
(t) + ϵ'
B. 
Indeed, it is not restrictive to assume that k(t) ≥ 1 for a.e. t ∈ [S' − σ'
, T ' + σ'
]
(otherwise we can always replace k by k∨1). Consider the change of the independent 
variable s = ˆσ (t), where σˆ : R → R is defined by 
σ (t) ˆ := 
S' − σ' + T '
−S'
+2σ'
‖k‖L1
 t
S'
−σ' k(τ )dτ for all t ∈ [S' − σ'
, T ' + σ'
]
t otherwise.
(9.5.1)9.5 Proof of Theorem 9.4.1 433 
(Here, ‖k‖L1 = ‖k‖L1(S'
−σ'
,T '
+σ'
).) Notice that σˆ is a strictly increasing function 
such that σ (S ˆ ' − σ'
) = S' − σ' and σ (T ˆ ' + σ'
) = T ' + σ'
. We can then consider a 
new problem (FEP) in which F is replaced by 
F (s, y) ˆ :=
‖k‖L1
(T ' − S' + 2σ'
) × (k ◦ ˆσ −1)(s)F (σˆ −1(s), y), for all (s, y) ∈ R×Rn,
and for which ([Sˆ := ˆσ −1(S'
), Tˆ := ˆσ −1(T '
)], xˆ := x' ◦ ˆσ −1) is a W1,1 local 
minimizer. Observe also that Fˆ satisfies (FEP3)∗ with kF = ‖k‖L1
T '
−S'
+2σ' . It is 
straightforward to deduce the necessary conditions for the original problem, with 
reference to the minimizer ([S'
, T '
], x'
), via the inverse change of independent 
variable s = ˆσ (t), in which the Lagrange multiplier pˆ ∈ W1,1([S,ˆ Tˆ]; Rn) arising 
in the new problem is replaced by p = ˆp ◦ ˆσ in the original one. 
Consider first the special case in which g has the structure 
g(S, x0,T,x1) = ˜g(S, x0,T,x1) + e(S, x0,T,x1) ,
for some twice differentiable function g˜ and some Lipschitz continuous function 
e, with Lipschitz constant ke. We shall prove a coarser version of the necessary 
conditions in the proposition statement, in which condition (C) is replaced by 
(−ξ0, p(S'
), ξ1, −p(T '
)) ∈∇˜g(S'
, x'
(S'
), T '
, x'
(T '
)) + 2
√
2ke(1 + c2
F )
1
2 B .
Note, first of all, that x' is a W1,1 local minimizer for a version of problem (FEP) 
in which the underlying time interval is fixed at [S'
, T '
]. The hypotheses of Theo￾rem 8.4.3 of Chap. 8 are satisfied. We deduce existence of p ∈ W1,1([S'
, T '
]; Rn)
satisfying conditions (A) and (B) and the following transversality condition: 
(p(S'
), −p(T '
)) ∈ ∇x0,x1g(S ˜ '
, x'
(S'
), T '
, x'
(T '
)) + keB . (9.5.2) 
We immediately see that d(([S'
, T ' − s], x'
), ([S'
, T '
], x'
)) = s. But then, for s ∈
[0, β], by W1,1 local optimality of ([S'
, T '
], x'
), ([S'
, T ' − s], x'
) must have cost 
not less than ([S'
, T '
], x'
). It follows that 
g(S ˜ '
, x'
(S'
), T ' − s, x'
(T ' − s)) + e(S'
, x'
(S'
), T ' − s, x'
(T ' − s))
≥ ˜g(S'
, x'
(S'
), T '
, x'
(T '
)) + e(S'
, x'
(S'
), T '
, x'
(T '
)) .
Since g˜ is a C2 function, there exists r1 > 0 such that for all s sufficiently small 
0 ≥ ˜g(S'
, x'
(S'
), T '
, x'
(T '
)) − ˜g(S'
, x'
(S'
), T ' − s, x'
(T ' − s)) − ke(1 + c2
F )
1
2 s
=  T '
T '
−s

∇T g(S ˜ '
, x'
(S'
), t, x'
(t)) + ∇x1g(S ˜ '
, x'
(S'
), t, x'
(t)) · ˙x'
(t)	
dt
−ke(1 + c2
F )
1
2 s
≥ ∇T g(S ˜ '
, x'
(S'
), T '
, x'
(T '
))s −  T '
T '
−s H (t, x'
(T '
), p(T '
))dt
−r1s2 − ke(cF + (1 + c2
F )
1
2 )s.434 9 Free End-Time Problems
To derive the final inequality in the above relation, we have used the following facts: 
−p(T '
) ∈ ∇x1g˜ (S'
, x(S'
), T '
, x(T '
)) + keB; moreover, by (FEP3)∗ and (FEP4)'
, 
we have, for a.e. t ∈ [T ' − s, T '
], 
x˙'
(t) ∈ F (t, x'
(T '
)) + kF |x'
(t) − x'
(T '
)|B ⊂ F (t, x'
(T '
)) + kF cF sB,
and so, for a.e. t ∈ [T ' − s, T '
], 
p(T '
) · ˙x'
(t) ≤ max{p(T '
) · v : v ∈ F (t, x'
(T '
))} + kF cF |p(T '
)|s
= H (t, x'
(T '
), p(T '
)) + kF cF |p(T '
)|s .
We have shown that 
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
T ' is a local minimum over [S'
, T '
] of
T → −∇T g(S ˜ '
, x'
(S'
), T '
, x'
(T '
))(T ' − T )
−  T '
T (−H )(t, x'
(T '
), p(T '
)) dt
+ r1(T − T '
)2 + ke(cF + (1 + c2
F )
1
2 )|T − T '
| .
(9.5.3) 
Next, select a measurable function ξ : [T '
, T ' + σ'
] → Rn such that ξ(t) ∈
F (t, x'
(T '
)) a.e. and 
p(T '
) · ξ(t) = max v∈F (t,x'
(T '
))
p(T '
) · v, a.e. t ∈ [T '
, T ' + s'
].
We deduce, from Filippov’s existence theorem (Theorem 6.2.3) that there exist s' ∈
(0, σ'
) and an F trajectory y on [T '
, T ' + s'
] such that y(T '
) = x'
(T '
) and 
 T '
+s
T '
| ˙y(t) − ξ(t)|dt ≤ (1/2)ekF s
kF cF s2, for all s ∈ [0, s'
]. (9.5.4) 
We now construct an F trajectory ([S'
, T ' + s'
], x) by concatenating ([S'
, T '
], x'
)
and ([T '
, T ' + s'
], y). For s' ∈ (0, σ'
) sufficiently small, d(([S'
, T ' +
s], x), ([S'
, T '
], x'
)) ≤ β for all s ∈ [0, s'
], by Lemma 9.5.1. In view of the 
W1,1 local optimality of ([S'
, T '
], x'
), for all s ∈ [0, s'
] the cost of ([S'
, T ' + s], x)
exceeds that of ([S'
, T '
], x'
). It follows that 
0 ≤ (g˜ + e)(S'
, x'
(S'
), T ' + s, y(T ' + s)) − (g˜ + e)(S'
, x'
(S'
), T '
, y(T '
))
≤
 T '
+s
T '

∇T g(S ˜ '
, x'
(S'
), t, x(t)) + ∇x1g(S ˜ '
, x'
(S'
), t, x(t)) · ˙x(t)	
dt
+ke(1 + c2
F )
1
2 s , for all s ∈ [0, s'
].9.5 Proof of Theorem 9.4.1 435 
Arguing as in the analysis preceding (9.5.3), we can show that, for some constant 
r2 ≥ r1 and for all s ∈ [0, s'
], 
 T '
+s
T '

∇T g(S ˜ '
, x'
(S'
), t, x(t)) + ∇x1g(S ˜ '
, x'
(S'
), t, x(t)) · ˙x(t)	
dt
≤ (∇T g(S ˜ '
, x'
(S'
), T '
, x(T '
))s +
 T '
+s
T '
(−H )(t, x'
(T '
), p(T '
)) dt
+(1 + (1/2)ekF s'
kF cF )r2s2 + kecF s .
We have shown, for some s' ∈ (0, σ'
), 
⎧
⎪⎨
⎪⎩
T ' is a local minimum over [T '
, T ' + s'
] of
T → ∇T g(S ˜ '
, x'
(S'
), T '
, x'
(T '
))(T − T '
) +  T
T '(−H )(t, x'
(T '
), p(T '
)) dt
+(1 + (1/2)ekF s'
kF cF )r2|T − T '
|
2 + ke(cF + (1 + c2
F )
1
2 )|T − T '
|.
(9.5.5) 
Interpreting, as usual, the integral  T
T '(−H )(t, x'
(T '
), p(T '
))dt as 
−  T '
T (−H )(t, x'
(T '
), p(T '
))dt for T <T '
, we deduce from (9.5.3) and (9.5.5) 
that 
⎧
⎪⎨
⎪⎩
T ' is a local minimum over [S'
, T ' + s'
] of
T → ∇T g(S ˜ '
, x'
(S'
), T '
, x'
(T '
))(T − T '
) +  T
T '(−H )(t, x'
(T '
), p(T '
)) dt
+(1 + (1/2)ekF s'
kF cF )r2(T − T '
)2 + ke(cF + (1 + c2
F )
1
2 )|T − T '
| .
Applying Proposition 9.3.2, in which we take the indefinite integral function ψ to be 
ψ(t) :=  t
T '(−H )(s, x'
(T '
), p(T '
))ds, t ∈ [S'
, T ' + s'
], noting that 0 is a limiting 
subgradient of a function at a minimizing point and also applying the sum rule for 
limiting subdifferentials of Lipschitz functions, we deduce that 
− ∇T g(S ˜ '
, x'
(S'
), T '
, x'
(T '
)) ∈ sub-ess
t→T ' (−H )(t, x(T ¯ '
), p(T '
))
+ ke(cF +(1+c2
F )
1
2 )B.
A similar analysis, in which we take the left end-time, yields 
− ∇Sg(S ˜ '
, x'
(S'
), T '
, x'
(T '
)) ∈ sub-esst→S'H (t, x'
(S'
), p(S'
))
+ ke(cF +(1+c2
F )
1
2 )B.
In view of (9.5.2) and Proposition 9.3.2, we have confirmed that there exist 
elements ξ0 ∈ sub-esst→S'H (t, x'
(S'
), p(S'
)) and 
ξ1 ∈ super-esst→T 'H (t, x'
(T '
), p(T '
)) = −sub-esst→T '(−H )(t, x'
(T '
), p(T '
))436 9 Free End-Time Problems
such that 
(−ξ0, p(S'
), ξ1, −p(T '
)) ∈∇˜g(S'
, x'
(S'
), T '
, x'
(T '
)) + 2
√
2ke(1 + c2
F )
1
2 B.
Recall that, up to this point in the proof, we have assumed the cost function g
has a special structure. We now drop this assumption; henceforth we assume that g
is an arbitrary Lipschitz continuous function. 
For i = 1, 2,... let gi be the quadratic inf convolution of g (with parameter 
α = i). Since ([S'
, T '
], x'
) is a W1,1 local minimizer for (FEP), there exists β > 0
such that ([S'
, T '
], x'
) is a minimizer for (FEP), when we append the constraint 
d(([S,T ], x), ([S'
, T '
], x'
)) ≤ β. Consider, for each i, the following variant on 
(FEP), in which we replace g by gi and add a distance constraint: 
(F EP )i
⎧
⎪⎪⎨
⎪⎪⎩
Minimize gi(S, x(S), T , x(T ))
over [S,T ] ⊂ R and x ∈ W1,1([S,T ]; Rn) satisfying
x(t) ˙ ∈ F (t, x(t)) a.e.
d(([S,T ], x), ([S'
, T '
], x'
)) ≤ β .
Write αi := kg/
√i, where kg is a Lipschitz constant for g. According to the 
properties of quadratic inf convolutions, ([S'
, T '
], x'
) is an α2
i minimizer for this 
problem. 
We can express the preceding problem as 
Minimize {Ji([S,T ], x) : ([S,T ], x) ∈ M}
where M denotes the space of admissible arcs ([S,T ], x) for problem (FEP)i
and Ji([S,T ], x) := gi(S, x(S), T , x(S)). Ji is continuous on the closed set M, 
equipped with the d metric topology. It follows then from Ekeland’s theorem 
that there exists an element ([Si, Ti], xi) in M that minimizes Ji([S,T ], x) +
αid([S,T ], x), ([Si, Ti], xi) over M and 
d(([Si, Ti], xi), ([S'
, T '
], x'
)) ≤ αi . (9.5.6) 
For i sufficiently large, αi < β/2. Then, by the triangle inequality applied to the 
metric d, ([Si, Ti], xi) remains a minimizer for (FEP)i, when the distance constraint 
in the definition of M is replaced by d(([S,T ], x), ([Si, Ti], xi)) ≤ β/2. It now 
follows from Lemma 9.5.1 that ([Si, Ti], xi) is a minimizer for 
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize gi(S, x(S), T , x(T ))
+ αi(
 T
S | ˙x(t) − ˙xi(t)|dt + |x(S) − xi(Si)|
+(1 + 2cF )(|S − Si|+|T − Ti|)),
over [S,T ] ⊂ R and x ∈ W1,1([S,T ]; Rn) satisfying
x(t) ˙ ∈ F (t, x(t)), a.e. t ∈ [S,T ],
d(([S,T ], x), ([Si, Ti], xi)) ≤ β/2, .9.5 Proof of Theorem 9.4.1 437 
We have shown that ([Si, Ti], xi, yi ≡ 0) is a W1,1 local minimizer for 
(F EP )'
i
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize gi(S, x(S), T , x(T )))
+ αi(y(T ) − y(S) + |x(S) − xi(Si)|
+ (1 + 2cF )(|S − Si|+|T − Ti|))
over [S,T ] ⊂ R and (x, y) ∈ W1,1([S,T ]; Rn+1) satisfying
(x(t), ˙ y(t)) ˙ ∈ F (t, x(t)) ˜ a.e.,
in which F˜ : R × Rn ⇝ Rn+1 is the multifunction 
F (t, x) ˜ := {(v, |v − ˙xi(t)|) : v ∈ F (t, x)} for (t, x) ∈ R × Rn .
By properties of quadratic inf-convolutions, there exists a quadratic function g˜i such 
that 
∇ ˜gi(Si, xi(Si), Ti, xi(Ti)) ∈ ∂P g(zi) ,
for some zi ∈ (Si, xi(Si), Ti, xi(Ti)) + (kg/i)B, and 

g˜i(z) ≥ gi(z) for all z ∈ R × Rn × R × Rn
g˜i(z) = gi(z) when z = (S'
, x'
(S'
), T '
, x'
(T '
)) .
It follows that ([Si, Ti], xi, yi ≡ 0) is also a W1,1 local minimizer for a variant of 
(FEP)'
i, in which the quadratic function g˜i replaces gi, namely 
⎧
⎪⎪⎨
⎪⎪⎩
Minimize g˜i(S, x(S), T , x(T )))
+αi(y(T ) − y(S) + |x(S) − xi(Si)| + (1 + 2cF )(|S − Si|+|T − Ti|))
over [S,T ] ⊂ R and (x, y) ∈ W1,1([S,T ]; Rn+1) satisfying
(x(t), ˙ y(t)) ˙ ∈ F (t, x(t)) ˜ a.e..
This problem is an example of the special case of (FEP), for which the 
previous analysis of Step 1 provides necessary conditions of optimality. The relevant 
hypotheses are satisfied. To interpret these conditions, we note that, given a proximal 
normal vector (ξ , (η, η0)) ∈ NP
Gr F (t,.) ˜ (x, ( ˜ v,˜ | ˜v − ˙xi(t)|)), there exists M > 0 such 
that (x,˜ v)˜ is a local minimizer of the function 
(x, v) → −ξ ·x−η·v−η0|v− ˙xi(t)|+M(|x− ˜x|
2+|v− ˜v|
2+(|v− ˙xi(t)|−| ˜v− ˙xi(t)|)
2)
over (x, v) ∈ Gr F (t, .). But then, by the exact penalization theorem (Theorem 
3.2.1), there exists a constant K > 0 such that (x,˜ v)˜ is a local minimizer of the 
function 
(x, v) → −ξ · x − η · v − η0|v − ˙xi(t)| + KdGr F (t,.)(x, v)
+ M(|x − ˜x|
2 + |v − ˜v|
2 + (|v − ˙xi(t)|−|˜v − ˙xi(t)|)
2) .438 9 Free End-Time Problems
Since NGr F (t,.) = {λ∂dGr F (t,.) : λ ≥ 0}, it follows that 
(ξ , η) ∈ NGr F (t,.)(x,˜ v)˜ + |η0|B . (9.5.7) 
Consideration of limits of proximal normal vectors to Gr F˜ yields the information 
that 
(ξ , η, η0) ∈ NGr F (t,.) ˜ (x, v, |v − ˙xi(t)|)
=⇒ (ξ , η) ∈ NGr F (t,.)(x, v) + |η0|B . (9.5.8) 
Now apply the earlier derived necessary conditions of Proposition 9.5.2. Notice that, 
in consequence of the Euler Lagrange inclusion and the transversality condition, the 
costate arc component associated with the y state component is a constant and takes 
value −αi. We deduce that there exists pi ∈ W1,1([S'
, T '
]; Rn) such that 
(A)'
: p˙i(t) ∈ co{ζ : (ζ, pi(t)) ∈ NGr F (t,·)(xi(t), x˙i(t))} + αiB, a.e. t ∈ [Si, Ti], 
(B)'
: pi(t)· ˙xi(t)+αi|v− ˙xi(t)| ≥ pi(t)·v for all v ∈ F (t, xi(t)), a.e. ∈ [Si, Ti], 
(C)'
: there exist ξ i
0 ∈ sub-esst→SiH (t, xi(Si), p(Si)) and 
ξ i
1 ∈ super-esst→Ti H (t, xi(Ti), p(Ti)) such that 
(−ξ i
0, pi(Si), ξ i
1, −pi(Ti)) ∈ ∂g(zi) + 2
√
2αi(1 + 2cF )
2B .
We use our ‘constant extrapolation’ convention to extend the domains of 
the pi’s to the entire real line. We deduce from (A)' and (C)' that the pi’s 
are uniformly bounded with uniformly integrably bounded derivatives. We 
can arrange then, by subsequence extraction, that pi → p uniformly and 
p˙i → ˙p weakly in L1 for some p ∈ W1,1. The sequences {ξ i
0} and {ξ i
i }
are bounded. A further subsequence extraction ensures that they have limits 
ξ0 and ξ1 respectively. We know however that ([Si, Ti], xi) → ([S'
, T '
], x'
)
w.r.t. the d metric and zi → (S'
, x'
(S'
), T '
, x'
(T '
)) as i → ∞. A by now 
familiar convergence analysis permits us to pass to the limit as i → ∞ in 
conditions (A)'
–(C)' and arrive at the relations asserted in the Proposition statement. 
The novel feature of the analysis, as compared with treatment of fixed time 
problems is that we now make use of the stability, under limit taking, of relations 
expressed in terms of the sub and super essential values. We note specifically 
that, Proposition 9.3.4, the conditions ξ i
0 ∈ sub-esst→SiH (t, xi(Si), p(Si))
and ξ i
1 ∈ super-esst→Ti
H (t, xi(Ti), p(Ti)), for each i, imply that ξ0 ∈
sub-esst→S'H (t, x'
(S'
), p(S'
)) and ξ1 ∈ super-esst→T 'H (t, x'
(T '
), p(T '
)).
⨅⨆
Step 2 (Completion of the Proof) Let β > 0 be such that the W1,1 local minimizer 
([S,¯ T¯], x)¯ relative to B is minimizing w.r.t. admissible F trajectories ([S,T ], x)
that satisfy d(([S,T ], x), ([S,¯ T¯], x)) ¯ ≤ β and x(t) ˙ ∈ ˙
x(t) ¯ + B(t) for a.e. t ∈
[S,¯ T¯]∩[S,T ]. Since we may impose (G3)'
, it may be assumed that x¯ ≡ 0.9.5 Proof of Theorem 9.4.1 439 
We shall make use, once again, of the modified integral penalty function 
ρS(t, x, v) earlier encountered to prove the fixed end-time necessary conditions of 
Chap. 8. 
Choose η ∈ (0, 1/2) such that (G3)' is satisfied with γ = (1 − 2η). Fix N>R. 
For each t ∈ [S¯ − σ, T¯ + σ] define 
EN (t) := {e ∈ Ω0(t)∩B(t) : R ≤ |e| ≤ N}.
Here, Ω0(t) is the regular velocity set relative to ([S,¯ T¯], x)¯ , namely 
Ω0(t) := {e ∈ F (t, x(t)) ¯ : F (t, .) is pseudo-Lipschitz near (x(t), e) ¯ }.
We can construct a multifunction EN
discrete : [S¯ − σ, T¯ + σ] ⇝ Rn such that 
(a): EN
discrete is measurable, 
and, for each t ∈ [S¯ − σ, T¯ + σ], 
(b): EN
discrete(t) is an empty or finite set, 
(c): EN
discrete(t) ⊂ EN (t) ⊂ EN
discrete(t) + N−1 B, for each t ∈ [S¯ − σ, T¯ + σ]
such that EN (t) /= ∅, 
(d): EN
discrete(t) ⊂ EN+1
discrete(t) for all integers N>R. 
Define 
θ (t) :=
⎧
⎪⎪⎨
⎪⎪⎩
min{|e−e'
| : e, e' ∈EN
discrete(t) and e /= e'
}∧inf{d∂B(t)(e) : e∈EN
discrete(t)}
if EN
discrete(t) contains at least two elements,
+∞ otherwise .
Take δ ∈ (0, ηR). Define 
Eδ
regular(t) := {e ∈ EN
discrete(t) : F (t, .) is pseudo-Lipschitz near (e, 0)
(with parameters R ≥ δ, ϵ ≥ δ and k ≤ δ−1) and θ (t) ≥ δ} .
By construction, Eδ
regular(t) ⊂ EN
discrete(t) ⊂ NB. Define Dδ : [S¯ −σ, T¯ +σ] ⇝
Rn as 
Dδ
(t) := (1 − η)RB ∪ {e ∈ e0 +
1
3
δB : e0 ∈ Eδ
regular(t)}
in which the right side is interpreted as (1 − η)RB if Eδ
regular(t) = ∅.440 9 Free End-Time Problems
Dδ(t) is a finite union of disjoint, closed balls. These comprise (1 − η)RB and 
elements from a (possibly empty) collection of disjoint closed balls each with origin 
outside R
◦
B. We also have that Dδ(t) ⊂ B(t) for a.e. t ∈ [S¯ − σ, T¯ + σ]. 
Now consider the multifunction S : [S¯ − σ, T¯ + σ] × Rn ⇝ Rn
S(t, x) := {(χ (|e|)e : e ∈ F (t, x)},
in which χ : [0,∞) → [1,∞) is the function 
χ (d) := 1 +
4(1 − η)
ηR [d − (1 − η)R]
+ .
Define ρS : [S¯ − σ, T¯ + σ] × Rn × Rn → [0,∞) to be 
ρS(t, x, v) := 
dS(t,x)(v) if |v| ≤ (1 − η)R ,
dF (t,x)(v) if |v| > (1 − η)R .
We refer to Lemma 8.6.5 of Chap. 8 for relevant properties of ρS(t, x, v). 
For t ∈ [S¯ − σ, T¯ + σ] and e ∈ Dδ(t) define 
φ(t, e) :=  1
2 (|e| − (1 − 2η)R) ∨ 0 if e ∈ (1 − η)RB
1
2 (|e − e0| − δ/6) ∨ 0 if e ∈ e0 + 1
3 δ B for some e0 ∈ Eδ
regular(t) .
Note the following properties of φ, each of which is a simple consequence of the 
definition of this function: for any t ∈ [S,¯ T¯]
φ(t, e) = 0 if |e| ≤ (1 − 2η)R
φ(t, .) is locally Lipschitz continuous on Dδ(t)
with Lipschitz constant 1/2,
φ(t, e) ≥ ηR
2 ∧ δ
12 if e ∈ ∂Dδ(t) .
⎫
⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎭
(9.5.9) 
Take αi ↓ 0 and, for each i, consider the optimization problem: 
(Pi)
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
Minimize 
�i(S, x(S), T , x(T ))	
∨
  T¯−σ
S¯+σ ρS(t, x(t), x(t))dt ˙
	
+  T¯−σ
S¯+σ φ(t, x(t))dt ˙
over arcs x ∈ W1,1 such that
x(t) ˙ ∈ Dδ(t)
d(([S,T ], x), ([S,¯ T¯], x)) ¯ ≤ β .9.5 Proof of Theorem 9.4.1 441 
Here, 
�i(S, x0,T,x1) := 
g(S, x0,T,x1) − g(S,¯ 0, T ,¯ 0) + α2
i
	
∨ dC(S, x0,T,x1)
and ρS(t, x, v) is the modified penalty integrand, with parameters η ∈ (0, 1/2), 
δ > 0 and N, as earlier chosen. 
We can formulate this problem as 
Minimize {Ji(([S,T ], x)) : ([S,T ], x) ∈ S},
in which S is the class of admissible arcs ([S,T ], x) for (Pi) and 
Ji(([S,T ], x)) := 
�i(S, x(S)), T , x(T ))	
∨
  T¯−σ
S¯+σ
ρS(t, x(t), x(t))dt ˙
	
+
 T¯−σ
S¯+σ
φ(t, x(t))dt . ˙
S is complete w.r.t. the metric d. We see that ([S,¯ T¯], x¯ ≡ 0) is an α2
i minimizer for 
(Pi). By Ekeland’s theorem, we can find ([Si, Ti], xi) ∈ S such that ([Si, Ti], xi) is 
a minimizer for 
Minimize {Ji(([S,T ], x)) + αid(([S,T ], x), ([Si, Ti], xi))
s.t. ([S,T ], x) ∈ S} (9.5.10) 
and 
d(([Si, Ti], xi), ([S,¯ T¯], x)) ¯ ≤ αi . (9.5.11) 
Note that 

�i(Si, xi(Si), Ti, xi(Ti))	
∨
  T¯−σ
S¯+σ
ρS(t, xi(t), x˙i(t))dt	
> 0 , (9.5.12) 
for, otherwise, we would have 

g(Si, xi(Si), Ti, xi(Ti)) − g(S,¯ x(¯ S)¯ = 0, T ,¯ x(¯ T )¯ = 0) + α2
i
	
∨ dC(Si, xi(Si), Ti, xi(Ti)) ∨
  T¯−σ
S¯+σ
ρS(t, xi(t), x˙i(t))dt	
= 0 .
This implies ρS(t, xi(t), x˙i(t)) = 0 a.e. t ∈ [S¯ + σ, T¯ − σ]. But then 
x˙i(t) ∈ F (t, xi(t)) a.e., by properties of the modified distance function. The442 9 Free End-Time Problems
relation also tells us dC(Si, xi(Si), Ti, xi(Ti)) = 0, from which we conclude 
(Si, xi(Si), Ti, xi(Ti)) ∈ C and, also, g(Si, xi(Si), Ti, xi(Ti))) ≤ g(S,¯ 0, T ,¯ 0)−α2
i . 
Since ([Si, Ti], xi) ∈ S, we know that d(([Si, Ti], xi), ([S,¯ T¯], x)) ¯ ≤ β. But then 
([Si, Ti], xi) is an admissible F trajectory, satisfying x(t) ˙ ∈ B(t) for a.e. t ∈
[S,¯ T¯]∩[Si, Ti], that violates the W1,1 local optimality of ([S,¯ T¯], x¯ ≡ 0). (9.5.12) 
is confirmed. 
For i sufficiently large we have αi ≤ σ∧(β/2). Then [Si, Ti]⊂[S¯−σ, T¯+σ] and 
([Si, Ti], xi) remains a minimizer for problem (9.5.10), when the distance constraint 
is replaced by d(([S,T ], x), ([Si, Ti], xi)) ≤ β/2. In view of Lemma 9.5.1, 
([Si, Ti], xi) continues to be a minimizer when the cost Ji([S,T ], x) in (9.5.10) 
is replaced by 
J˜
i([S,T ], x) := Ji([S,T ], x) +

�i(S, x(S), T , x(T ))	
∨
  T¯−σ
S¯+σ
ρS(t, x(t), x(t))dt ˙
	
+
 T¯−σ
S¯+σ
φ(t, x(t))dt ˙ + αi
  T
S
| ˙x(t)− ˙xi(t)|dt
+|x(S)−xi(Si)| + (1 + 2cF )(|S − Si|+|T − Ti|)
	
.
We know then that ([Si, Ti], (xi, yi(t) :=  t
Si ρS(s, xi(s), x˙i(s))ds, zi(t) :=
 t
Si φ(s, x˙i(s))ds)) is a W1,1 local minimizer for the problem 
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize 
g(S, x(S), T , x(T )) − g(S,¯ 0, T ,¯ 0) + α2
i
	
∨ dC(S, x(S), T , x(T ))
∨(y(T ) − y(S))
+αi|x(S) − xi(Si)| + αi(1 + 2cF )(|S − Si|+|T − Ti|)
+(z(T ) − z(S))
over ([S,T ], (x, y, z)) ∈ W1,1([S,T ]; Rn) satisfying
(x(t), ˙ y(t), ˙ z(t)) ˙ ∈ F (t, x(t)) , ˜
in which F˜ : R × Rn ⇝ Rn × R × R is the multifunction: 
F (t, x) ˜ =

{(v, ρS(t, x, v), αi|v − ˙xi(t))| + φ(t, v) : v ∈ Dδ(t))} if t ∈ [S¯ − δ, T¯ + δ]
{(0, 0, 0)} otherwise .
We have arrived at a problem in which the endpoints are not constrained and to 
which the necessary conditions of Step 1 (Proposition 9.5.2) are applicable. It is 
a straightforward exercise to show that the relevant hypotheses are satisfied, for 
i sufficiently large, when we identify ([S'
, T '
], x'
) with ([Si, Ti], xi). Note that9.5 Proof of Theorem 9.4.1 443 
hypothesis (FEP4)' is satisfied, in consequence of (G4) in Theorem 9.4.1, for 
suitably small parameters δ' > 0 and ϵ' > 0, since (by (9.5.11)) Si → S¯, Ti → T¯
and ||xi = ¯x||L∞ → 0, as i → ∞. 
Using the sum rule to estimate the subdifferential of the end-point cost term, 
and by examining the properties of proximal normal vectors to Gr F (t, .) ˜ at 
neighbouring points to (xi(t), x˙i(t)), we deduce the following information, in which 
the costate trajectory component associated with xi is denoted pi. (Note that the 
costate components associated with y and z are both constant). There exist non￾negative numbers λi, λ(1)
i and λ(2)
i satisfying λi + λ(1)
i + λ(2)
i = 1 such that 
(a): p˙i(t) ∈ co{η : (η, pi(t)) ∈ λ(1)
i ∂x,vρS(t, xi(t), x˙i(t)) +

{0} × αiB
	
+ {0} × ∂vφ(t, x˙i(t))}
for a.e. t ∈ [Si, Ti] such that x˙i(t) ∈ Dδ(t) ,
(b)'
: there exist ξ i
0 ∈ sub-esst→SiH (t, xi(Si), pi(Si)) and 
ξ i
1 ∈ super-esst→Ti H (t, xi(Ti), pi(Ti)) such that 
(−ξ i
0, pi(Si), ξ i
1, −pi(Ti)) ∈ λi∂g(Si, xi(Si), Ti, xi(Ti))
+ λ(2)
i ∂dC(Si, xi(S), Ti, xi(Ti)) + αi(1 + 2(1 + c2
F )
2)
1
2 B × {0},
(c): pi(t) · ˙xi(t) − λ(1)
i ρS(t, xi(t), x˙i(t)) − φ(t, x˙i(t)) ≥
pi(t) · v − λ(1)
i ρS(t, xi(t), v) − φ(t, v) − αi|v − ˙xi(t)| , 
for all v ∈ Dδ(t), a.e. t ∈ [Si, Ti]. 
Observe however that 
λ(2)
i ∂dC(ξ i
0, xi(Si), ξ i
1, xi(Ti)) = λ(2)
i

∂dC(xi(S), xi(T )) ∩ ∂B
	
,
in which we interpret the right side as {0}, if λ(2)
i = 0. This relation is true when 
dC(Si, xi(Si), Ti, xi(Ti)) > 0 since, according to properties of the subdifferential 
of the distance function, ∂dC(Si, xi(Si), Ti, xi(Ti)) ⊂ ∂B in this case. It is also true 
when dC(Si, xi(Si), Ti, xi(Ti)) = 0 because in this case we have also λ(2)
i = 0 in 
view of (9.5.12) and the max rule for subdifferentials. But then (b)'
can be replaced 
by the stronger condition: 
(b): (−ξ i
0, pi(Si), ξ i
1, −pi(Ti)) ∈ λi∂g(Si, xi(S), Ti, xi(Ti))
+ λ(2)
i

∂dC(Si, xi(S), Ti, xi(Ti)) ∩ ∂B
	
+ αi

1 + 2(1 + 2cF )2 B × {0}. 
We can conclude from condition (a) above and the Lipschitz regularity of the 
modified penalty integrand ρS(t, ., .) (see properties (ii) and (iv) established in 
Lemma 8.6.5 of Chap. 8) that pi also satisfies 
| ˙pi(t)| ≤ k(t)( ˜ |pi(t)| + 2), a.e. t ∈ [Si, Ti] . (9.5.13)444 9 Free End-Time Problems
Here, k˜ is the integrable function of property (iv) of Lemma 8.6.5 applied to the 
modified penalty integrand ρS. 
From (b) we know that |pi(S)| ≤ kg + 1 + αi. It follows then from (9.5.13) 
that the family of costate trajectories components pi, i = 1, 2,... are uniformly 
bounded and their derivatives are uniformly integrably bounded. 
To proceed, we need to establish a uniform positive lower bound on the 
magnitude of (pi, λi). Let us examine the two possible cases that can arise: 
(i): ρS(t, xi(t), x˙i(t)) = 0 a.e. t ∈ [Si, Ti], 
(ii): ρS(t, xi(t), x˙i(t)) > 0 on a subset of [Si, Ti], which has positive L-measure. 
Consider (i). In this case, zi(Ti) − zi(Si) = 0 and so, in view of (9.5.12), 
zi(Ti) − zi(Si) < 
g(Si, xi(Si), Ti, x(Ti)) − g(S,¯ 0, T ,¯ 0) + α2
i
	
∨ dC(Si, xi(Si), Ti, x(Ti)) .
But then, from the sum rule, λ(1)
i = 0 and therefore λ(2)
i = 1 − λi. It follows then 
from (b) that 
|(−ξ i
0, pi(Si), ξ i
1, −pi(Ti))| ≥ 1 − (1 + kg)λi − αi

1 + 2(1 + 2cF )2 .
Since |ξ i
0| ≤ cF |pi(Si)| and |ξ i
1| ≤ cF |pi(Ti)|, we deduce 
√
2(1 + c2
F )
1
2 ||pi||L∞ + (1 + kg)λi ≥ 1 − αi

2 + (1 + 2cF )2 .
Consider case (ii). In this case there is a time t ∈ [Si, Ti] such that the first 
relation in condition (c) is satisfied, ρS(t, xi(t), x˙i(t)) > 0 and x˙i(t) ∈ Dδ(t). 
We claim that x˙i(t) /∈ F (t, xi(t)). This is certainly true if | ˙xi(t)| ≥ (1 − η)R
because, in this case, ρS(t, xi(t), x˙i(t)) = dF (t,xi(t))(x˙i(t))). So we may assume that 
| ˙xi(t)| < (1 − η)R. Then χ (| ˙xi(t)|) = 1. This means that, if x˙i(t) ∈ F (t, xi(t)), 
then ρS(t, xi(t), x˙i(t)) ≤ |˙xi(t) − χ (x˙i(t)|)x˙i(t)| = |˙xi(t) − ˙xi(t)| = 0. From this 
contradiction it follows that x˙i(t) /∈ F (t, xi(t)). 
To proceed further, we must now distinguish two situations: 
First case: x˙i(t) ∈ ∂Dδ(t). 
In view of (G3)'
, there exists v0 ∈ F (t, xi(t)) such that |v0| ≤ (1 − 2η)R. Then 
φ(t, v0) = 0 and ρS(t, xi(t), v0) ≤ |v0 − χ (|v0|)v0|=|v0 − v0|) = 0. Since 
x˙i(t) ∈ ∂Dδ(t), it follows from property (9.5.9) of the function φ that 
φ(t, x˙i(t)) ≥ ηR
2
∧
δ
12 .
Using these relations, noting that ρS(t, xi(t), x˙i(t)) ≥ 0 and | ˙xi(t)| ≤ N + δ/3, 
and inserting v = v0 in the first relation in (c), yields the inequality9.5 Proof of Theorem 9.4.1 445 
|pi(t)| |(x˙i(t) − v0)| ≥ λ(1)
i ρS(t, xi(t), x˙i(t)) − 0 + φ(t, x˙i(t))
−0 − αi(N + δ/3 + (1 − 2)ηR) .
≥ φ(t, x˙i(t)) − αi(N + δ/3 + (1 − 2η)R)
≥
ηR
2
∧
δ
12
	
− αi(N + (1 − 2η)R) .
Since |v0| ≤ (1 − 2η)R, it follows that 
||pi||L∞ ≥ (N + δ/3 + (1 − 2η)R)−1
ηR
2
∧
δ
12
	
− αi .
Second Case: x˙i(t) ∈ ◦
Dδ(t). 
Now, since x˙i(t) is an unconstrained local minimizer of v → −pi(t) · v +
λ(1)
i ρS(t, xi(t), v) + φ(t, v) + αi|v − ˙xi|, we have 
pi(t) ∈ λ(1)
i ∂vρS(t, xi(t), x˙i(t)) + ∂vφ(t, x˙i(t)) + αiB.
But ρS(t, xi(t), x˙i(t)) > 0. It follows from property (ii) of Lemma 8.6.5 applied 
on the modified penalty integrand that elements in ∂vρS(t, xi(t), x˙i(t)) have unit 
length. Taking note also of the fact that φ(t, .) is Lipschitz continuous with 
Lipschitz constant 1/2, whence ∂vφ(t, x˙i(t)) ∈ (1/2)B, we deduce from the 
preceding relations that 
||pi||L∞ ≥ λ(1)
i − 1
2 − αi.
From (b) we know 
√
2(1 + c2
F )
1
2 ||pi||L∞ ≥ |(−ξ 0
i , pi(Si), ξ 1
i , −pi(Ti))|
≥ λ(2)
i − λikg − αi

2 + (1 + 2cF )2
= 1 − λ(1)
i − (1 + kg)λi − αi

1 + 2(1 + 2cF )2 .
The preceding inequalities yield 
(1 + √
2(1 + c2
F )
1
2 )||pi||L∞ + (1 + kg)λi ≥
1
2 − αi(1 + 
2 + (1 + 2cF )2) .
Combining the estimates relating to both cases (i) and (ii), we arrive at 
(1 + √
2(1 + c2
F )
1
2 )||pi||L∞ + (1 + kg)λi
≥ min{
1
2
, (N+
δ
3
+(1 − 2η)R)−1
ηR
2
∧
δ
12
	
−αi(1 + 
1 + 2(1 + 2cF )2)}.446 9 Free End-Time Problems
This is the desired lower bound. 
We deduce from (9.5.11) that, along a subsequence, xi → ¯x ≡ 0 uniformly, and 
x˙i → ˙
x¯ ≡ 0 in L1 and a.e.. We have already observed that the p˙i’s are uniformly 
integrably bounded and the pi’s are uniformly bounded. By further restriction to a 
subsequence we can then arrange, in consequence of Ascoli’s Theorem, that pi → p
uniformly and p˙i → ˙p weakly in L1. We can also arrange that λi → λ, λ(1)
i → λ(1)
and λ(2)
i → λ(2) , for some λ, λ(1)
, λ(2) ∈ [0, 1] and ξ i
0 → ξ0 and ξ i
1 → ξ1 for such 
that (owing to Proposition 9.3.4) 
ξ0 ∈ sub-esst→S¯H (t, x(¯ S), p( ¯ S)) ¯ and ξ1 ∈ super-esst→T¯ H (t, x(¯ T ), p( ¯ T )) ¯
for some ξ0, ξ1 ∈ R. A convergence analysis along the lines of the proof of 
Proposition 8.6.1 of Chap. 8 permits us to pass to the limit in conditions (b)–(c) 
above and thereby to obtain: 
(1 + √
2(1 + c2
F )
1
2 )||p||L∞ + (1 + kg)λ
≥ min{
1
2
, (N+
δ
3 + (1 − 2η)R)−1
ηR
2
∧
δ
12
	
}, (9.5.14) 
p(t) ˙ ∈ co{η : (η, p(t)) ∈ NGr F (t,.)(x(t), ¯ ˙
x(t)) ¯ }, a.e. t ∈ [S,¯ T¯], (9.5.15) 
(−ξ0, p(S), ξ ¯ 1, −p(T )) ¯ ∈ λi∂g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯
+ NC(S,¯ x(S), ¯ T ,¯ x(¯ T )), ¯ (9.5.16) 
p(t) · ˙
x(t) ¯ − λ(1)
ρS(t, x(t), ¯ ˙
x(t)) ¯ − φ(t, ˙
x(t)) ¯ ≥ p(t) · v
−λ(1)
ρS(t, x(t), v) ¯ − φ(t, v) ,
for all v ∈ Dδ(t), a.e. t ∈ [S,¯ T¯]. 
Observe that, for all points v ∈ (F (t, x(t)) ¯ ∩ (1 − 2η)RB) ∪ Eδ
regular(t), we 
have v ∈ Dδ(t), v ∈ F (t, x(t)) ¯ and ρS(t, x(t), v) ¯ = φ(t, v) = 0. So the preceding 
relation implies 
p(t) · ˙
x(t) ¯ ≥ p(t) · v (9.5.17) 
for all v ∈ (F (t, x(t)) ¯ ∩ (1 − 2η)RB) ∪ Eδ
regular(t), a.e. t ∈ [S,¯ T¯]. 
Notice that we have made use of property (v)(b) in Lemma 8.6.5 applied to the 
modified penalty integrand to express relation (9.5.15) in terms of the normal cone 
to the graph NGr F (t.,) in place of the modified distance function ρS(t, ., .). We also 
know that 
| ˙p(t)| ≤ k(t)|p(t)|, a.e. t ∈ [S,¯ T¯] , (9.5.18)9.5 Proof of Theorem 9.4.1 447 
This follows from property (v)(b) of ρS(t, ., .). In view of (9.5.14), we can arrange, 
by scaling the Lagrange multipliers, that 
||p||L∞ + λ = 1 . (9.5.19) 
Relations (9.5.15), (9.5.16), (9.5.17) and (9.5.19) combine to provide a restricted 
version of the theorem (under all the hypotheses including (G4)), in which the 
Weierstrass condition (iv)'
is affirmed for velocities only in the subset (F (t, x(t)) ¯ ∩
(1 − 2η)RB) ∪ Eδ
regular(t) of F (t, x(t)) ¯ . To derive the full Weierstrass condition, 
we first take a sequence δi ↓ 0. Relations (9.5.15), (9.5.16), (9.5.17) and (9.5.19) 
are valid for each i, for some Lagrange multipliers (p, λ) that we now label 
(pi, λi). From (9.5.16) and (9.5.18) we know the pi’s are uniformly bounded and 
have uniformly integrable derivatives. We may therefore arrange, by extracting 
subsequences, that pi → p (uniformly), p˙i → ˙p in the weak L1 topology and 
λi → λ, for some p ∈ W1,1 and λ ≥ 0. 
Relations (9.5.19), (9.5.15 ) and (9.5.16) (now expressed in terms of (pi, λi)) are 
preserved in the limit as i → ∞ (with multipliers (p, λ)). Using the fact that 
lim
i
Eδi
regular(t) = EN
discrete(t) , a.e. t ∈ [S,¯ T¯] ,
we can show that, in the limit, (9.5.17) yields p(t) · ˙
x(t) ¯ ≥ p(t) · v, for all v ∈
(F (t, x(t)) ¯ ∩ (1 − 2η)RB) ∪ EN
discrete(t), a.e. t ∈ [S,¯ T¯]. 
Now take Ni ↑ ∞ and ηi ↓ 0. Using the facts that 
Ω0(t) ∩ B(t) ⊂ lim
i

F (t, x(t)) ¯ ∩ {e ∈ Rn : < (1 − 2ηi)R}
	
∪ lim
i
ENi
discrete(t)
and noting the linearity of the mapping v → p(t) · v, we deduce from (9.5.17), in 
the limit, that 
p(t) · ˙
x(t) ¯ ≥ p(t) · v, for all v ∈ co (Ω0(t) ∩ B(t)), a.e. t ∈ [S,¯ T¯] . (9.5.20) 
This is the full Weierstrass condition of the theorem statement. 
It remains only to prove the additional assertions that appear at the end of the 
theorem, concerning the relaxation of hypothesis (G4) when, say, the left end-time 
is fixed. In this case a simplified version of Step 1 of the proof, in which only the 
right end-time is varied, yields the transversality condition 

(p(S), ξ ¯ 1, −p(T '
)) ∈ ˜g(x'
(S), T ¯ '
, x'
(T '
)) ,
ξ1 ∈ super-esst→T 'H (t, x'
(T '
), p(T '
)) ,
for a version of the free end-point problem (FEP), in which, now, the end-point 
vector (x(S), T , x(T )) ¯ is ‘free’. (The part of hypothesis (G4) requiring that the 
velocity set F (t, .) is bounded, uniformly over times in a neighbourhood of S¯, is448 9 Free End-Time Problems
only required when we analyse perturbations to the left endpoint and can therefore 
be dropped.) Then, in Step 2 where we apply the necessary conditions of Step 1 
to a sequence of ancillary dynamic optimization problems and pass to the limit, 
the ancillary problems are now taken to have a fixed left end-time. Limit taking, 
which no longer requires aspects of (G4) relating to the left end-time, yields the 
transversality condition (iii)' in the theorem statement.
⨅⨆
9.6 A Free End-Time Maximum Principle 
We have derived necessary conditions for free end-time dynamic optimization 
problems in which the dynamic constraint is expressed as a differential inclusion. 
We now show that the analysis can be adapted to provide necessary conditions for 
free end-time problems, in the form of a maximum principle, when the dynamics 
are modelled instead by a control dependent differential equation: 
(F T )'
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize g(S, x(S), T , x(T ))
over intervals [S,T ], arcs x ∈ W1,1([S,T ]; Rn)
and measurable functions u : [S,T ] → Rm satisfying
x(t) ˙ = f (t, x(t), u(t)) a.e. t ∈ [S,T ],
u(t) ∈ U (t) a.e. t ∈ [S,T ],
and
(S, x(S), T , x(T )) ∈ C .
The data for this problem comprise functions g : R1+n+1+n → R and f : R ×
Rn × Rm → Rn, a non-empty multifunction U : R ⇝ Rm and a closed set C ⊂
R1+n+1+n. 
Earlier terminology is modified to emphasize that the end-points of the underly￾ing time interval [S,T ] are now choice variables. A process is taken to be a triple 
([S,T ], x, u) in which [S,T ] is an interval, x is an element in W1,1([S,T ]; Rn) (the 
state trajectory) and u (the control function) is a measurable Rm valued function on 
[S,T ] satisfying, for a.e. t ∈ [S,T ], 

x(t) ˙ = f (t, x(t), u(t))
u(t) ∈ U (t).
A process ([S,¯ T¯], x,¯ u)¯ , which satisfies the constraints of (F T )
' is said to be a W1,1
local minimizer if there exists δ' > 0 such that 
g(S, x(S), T , x(T )) ≥ g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯
for every process ([S,T ], x, u) which satisfies the constraints of (F T )
' and also9.6 A Free End-Time Maximum Principle 449
d(([S,T ], x), ([S,¯ T¯], x)) ¯ ≤ δ'
.
Here, d is the metric (9.1.4). 
Let H denote the un-maximized Hamiltonian function for (F T )
'
: 
H(t, x, p, u) := p · f (t, x, u).
Theorem 9.6.1 (Free End-Time Maximum Principle: Lipschitz Time Depen￾dence) Let ([S,¯ T¯], x,¯ u)¯ be a W1,1 local minimizer for (F T )
'
. Assume that 
(H1): g is Lipschitz continuous on a neighbourhood of (S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ and C is 
a closed set, 
(H2): For fixed x, f (., x, .) is L×Bm measurable. There exist ϵ > 0 and a function 
kf : [S,¯ T¯] × Rm → R such that t → k(t, u(t)) ¯ is integrable and, for a.e. 
t ∈ [S,¯ T¯], 
|f (t'', x'', u) − f (t'
, x'
, u)| ≤ kf (t, u) |(t'', x'') − (t'
, x'
)|
for all (t'', x''), (t'
, x'
) ∈ (t, x(t)) ¯ + ϵB, a.e. t ∈ [S,¯ T¯], 
(H3): U (t) = U for all t ∈ R, for some Borel set U ⊂ Rm. 
Then there exist p ∈ W1,1([S,¯ T¯]; Rn), a ∈ W1,1([S,¯ T¯]; R) and λ ≥ 0 such 
that 
(i): (p, λ) /= (0, 0), 
(ii): (a(t), ˙ − ˙p(t)) ∈ co ∂t,xH(t, x(t), p(t), ¯ u(t)) ¯ a.e. t ∈ [S,¯ T¯], 
(iii): (−a(S), p( ¯ S), ¯ +a(T ), ¯ −p(T )) ∈ λ∂g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯
+NC(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ , 
(iv): H(t, x(t), p(t), ¯ u(t)) ¯ = maxu∈U H(t, x(t), p(t), u) ¯ a.e. t ∈ [S,¯ T¯], 
(v): H(t, x(t), p(t), ¯ u(t)) ¯ = a(t) a.e. t ∈ [S,¯ T¯] . 
If the initial time S¯ is fixed, i.e. C = {(S, x ¯ 0,T,x1) : (x0,T,x1) ∈ C˜}, for some 
closed set C˜ ⊂ Rn × R × Rn, and g(S, x0,T,x1) = ˜g(x0,T,x1) for some g˜ :
Rn × R × Rn → R, then the above conditions are satisfied when (iii) is replaced 
by: 
(iii)'
: (p(S), a( ¯ T ), ¯ −p(T )) ¯ ∈ λ∂g(˜ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ + NC˜(x(¯ S), ¯ T ,¯ x(¯ T )) ¯ . 
The assertions of the theorem can be similarly modified when the final time T¯ is 
fixed. 
Proof Choose δ such that ([S,¯ T¯], x,¯ u)¯ is a minimizer for (F T )
'
, with respect to 
processes ([S,T ], x, u) satisfying the constraints of (F T )
' and also 
d(([S,T ], x), ([S,¯ T¯], x)) < δ. ¯
Take α ∈ (0, 1/2) and consider the dynamic optimization problem on the fixed time 
interval [S,¯ T¯]:450 9 Free End-Time Problems
(R)
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize g(τ (S), y( ¯ S), τ ( ¯ T ), y( ¯ T )) ¯
over processes ((τ, y), (v, w)) on [S,¯ T¯] satisfying
(τ (s), ˙ y(s)) ˙ = f (τ (s), y(s), v(s), w(s)) ˜ a.e. s ∈ [S,¯ T¯],
v(s) ∈ U a.e.,
w(s) ∈ [1 − α, 1 + α] a.e.,
(τ (S), y( ¯ S), τ ( ¯ T ), y( ¯ T )) ¯ ∈ C,
where 
f (τ, y, v, w) ˜ := (w, wf (τ, y, v)) .
We claim that ((τ (s) ¯ ≡ s, x), ( ¯ u,¯ w¯ ≡ 1)) is a W1,1 local minimizer for (R). 
To confirm this, take β' > 0 and let ((τ, y), (v, w)) be any process (on [S,¯ T¯]) that 
is admissible for problem (R) such that 
|(τ (S), y( ¯ S)) ¯ − (τ (¯ S)¯ ≡ S,¯ x(¯ S)) ¯ | +  T¯
S¯ |(τ (s), ˙ y(s)) ˙ − (1, ˙
x(s)) ¯ |ds ≤ β'
(9.6.1) 
Write S := τ (S)¯ and T := τ (T )¯ and consider the transformation ψ : [S,¯ T¯] →
[S,T ]: 
ψ(s) := τ (S)¯ +
 s
S¯ w(s'
)ds' (= τ (s)).
We find that ψ is a strictly increasing, Lipschitz continuous function, with Lipschitz 
continuous inverse. It can be deduced that x : [S,T ] → Rn and u : [S,T ] → Rm
defined by 
x := y ◦ ψ−1 and u := v ◦ ψ−1
are absolutely continuous and measurable functions respectively which satisfy 
(S, x(S), T , x(T )) = (τ (S), x( ¯ S), τ ( ¯ T ), x( ¯ T )), ¯
x(t) ˙ = f (t, x(t), u(t)) a.e. t ∈ [S,T ],
u(t) ∈ U a.e. t ∈ [S,T ] .
It follows from the foregoing relationships that ([S,T ], x, u) is an admissible 
process for the original dynamic optimization problem. Further, according to 
Lemma 9.7.1 and (9.6.1), we can arrange, by choosing β' > 0 and α sufficiently 
small, that 
d(([S,T ], x), ([S,¯ T¯], x)) ¯ ≤ δ.9.6 A Free End-Time Maximum Principle 451
In view of the minimizing properties of ([S,¯ T¯], x,¯ u)¯ , we have 
g(τ (S), y( ¯ S), τ ( ¯ T ), y( ¯ T )) ¯ = g(S, x(S), T , x(T ))
≥ g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ = g(τ (¯ S), ¯ y(¯ S), τ ( ¯ T ), ¯ y(¯ T )). ¯
It follows that the process ((τ ,¯ x), ( ¯ u,¯ w(s) ¯ ≡ 1)) is a minimizer for (R), as claimed. 
The hypotheses are satisfied under which the necessary conditions of Theorem 7.2.1 
can be applied to (R), with reference to the minimizer ((τ (s) ¯ ≡ s, x), ( ¯ u,¯ w¯ ≡ 1)). 
We deduce that there exist p ∈ W1,1([S,¯ T¯]; Rn), a ∈ W1,1([S,¯ T¯]; R) and λ ≥ 0, 
not all zero, such that 
(a(s), ˙ − ˙p(s)) ∈ co ∂t,xH(s, x(s), p(s), ¯ u(s)) ¯ a.e.,
(−a(S), p( ¯ S), a( ¯ T ), ¯ −p(T )) ¯ ∈ λ∂g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ + NC(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯
and 
p(s) · f (s, x(s), ¯ u(s)) ¯ − a(s) ≥ (p(s) · f (s, x(s), u) ¯ − a(s))w
for all u ∈ U and w ∈ [1 − α, 1 + α] a.e..
The last relationship implies 
H(s, x(s), p(s), ¯ u(s)) ¯ = max
u∈U
H(s, x(s), p(s), u) ¯ a.e.,
H(s, x(s), p(s), ¯ u(s)) ¯ = a(s) a.e..
All the assertions of the theorem have been confirmed, in the case that neither 
of the end-times are fixed. The refinements referred to at the end of the theorem 
statement, when either end-point is fixed, is achieved by obvious changes to the 
analysis.
⨅⨆
We denote by H : R × Rn × Rn → R the (maximized) Hamiltonian function for 
(F T )
'
: 
H (t, x, p) := sup
u∈U (t)
p · f (t, x, u).
Theorem 9.6.2 (The Free End-Time Maximum Principle: Measurable Time 
Dependence) Let ([S,¯ T¯], x,¯ u)¯ be a W1,1 local minimizer for (F T )
' such that 
T¯ − S >¯ 0. Assume that there exist ϵ > 0 and σ ∈ (0, (T¯ − S)/ ¯ 2) such that 
(H1): g is Lipschitz continuous on a neighbourhood of (S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ and C is 
a closed set, 
(H2): Gr U is a L × Bm measurable set,452 9 Free End-Time Problems
(H3): For each x ∈ Rn, f (., x, .) is L × Bm measurable. There exists a function 
kf : R×Rm → R such that t → kf (t, u(t)) ¯ is integrable on [T¯ −σ, T¯ +σ]
and 
|f (t, x'
, u) − f (t'
, x'', u)| ≤ kf (t, u)|x' − x''|
for all x'
, x'' ∈ ¯xe(t) + ϵB and u ∈ U (t), a.e. t ∈ [S¯ − σ, T¯ + σ], 
(H4): There exists c¯ ≥ 0 such that, for a.e. t ∈ [S¯ − σ, S¯ + σ]∪[T¯ − σ, T¯ + σ], 
|f (t, x, u)|≤ ¯c
for all x ∈ ¯xe(t) + ϵB and u ∈ U (t). 
Then there exist p ∈ W1,1([S,¯ T¯]; Rn) and a real number λ ≥ 0 such that 
(i): (p, λ) /= (0, 0), 
(ii): − ˙p(t) ∈ co∂xH(t, x(t), p(t), ¯ u(t)) ¯ a.e. t ∈ [S,¯ T¯], 
(iii): there exist ξ0 ∈ sub-esst→S¯H (t, x(¯ S), p( ¯ S)) ¯ , and 
ξ1 ∈ super-esst→T¯ H (t, x(¯ T ), p( ¯ T )) ¯ such that 
(−ξ0, p(S), ξ ¯ 1, −p(T )) ¯ ∈ λ∂g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ + NC(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ , 
(iv): H(t, x(t), p(t), ¯ u(t)) ¯ = maxu∈U (t) H(t, x(t), p(t), u) ¯ a.e. t ∈ [S,¯ T¯]. 
If the initial time S¯ is fixed, i.e. C = {(S, x ¯ 0,T,x1) : (x0,T,x1) ∈ C˜}, for some 
closed set C˜ ⊂ Rn × R × Rn, and g(S, x0,T,x1) = ˜g(x0,T,x1) for some g˜ :
Rn × R × Rn → R, then the above conditions are satisfied when (iii) is replaced 
by: there exists ξ1 ∈ super-esst→T¯ H (t, x(¯ T ), p( ¯ T )) ¯ such that 
(iii)'
: (p(S), ξ ¯ 1, −p(T )) ¯ ∈ λ∂g(˜ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ + NC˜(x(¯ S), ¯ T ,¯ x(¯ T )) ¯ , 
and when, in hypothesis (H4), the stated conditions are required to hold only for 
t ∈ [T¯ − σ, T¯ + σ]. The assertions of the theorem can be similarly modified when 
the final time T¯ is fixed. 
Proof 
Step 1 (A Free End-Point Problem): 
(F ET )'
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize g(S, x(S), T , x(T ))
over intervals [S,T ] ⊂ R, and arcs x ∈ W1,1([S,T ]; Rn)
and measurable functions u : [S,T ] → Rm satisfying
x(t) ˙ = f (t, x(t), u(t)) a.e. t ∈ [S,T ],
u(t) ∈ U (t) a.e. t ∈ [S,T ] ,
|T − T '
|, |S − S'
| ≤ σ' .
The following proposition provides necessary conditions for problem (FET)'
, 
in which [S'
, T '
] is a given interval and σ' > 0 is a given constant such that 
σ' < (T ' − S'
)/2.9.6 A Free End-Time Maximum Principle 453
Proposition 9.6.3 Let ([S'
, T '
], x'
, u'
) be a global minimizer for (FET)'
. Assume 
that, for some ϵ' > 0 and σ' ∈ (0, (T ' − S'
)/2), 
(FET1)'
: g is Lipschitz continuous, 
(FET2)'
: Gr U is a L × Bm measurable set, 
(FET3)'
: For each x ∈ Rn, f (., x, .) is L × Bm measurable. There exists kf ∈
L1(S' − σ'
, T ' + σ'
) such that 
|f (t, x, u) − f (t, x1, u)| ≤ kf (t)|x − x1|
for all x, x1 ∈ x'
e(t) + ϵ'
B and u ∈ U (t), a.e. t ∈ [S' − σ'
, T ' + σ'
], 
(FET4)'
: There exists c¯ ≥ 0 such that, for a.e. t ∈ [S' − σ'
, S' + σ'
]∪[T ' − σ'
,
T ' + σ'
], 
|f (t, x, u)|≤ ¯c
for all x ∈ x'
e(t) + ϵ'
B and u ∈ U (t). 
Then there exists p ∈ W1,1([S'
, T '
]; Rn) such that 
(i): − ˙p(t) ∈ co∂xH(t, x'
(t), p(t), u'
(t)) a.e. t ∈ [S'
, T '
], 
(ii): there exist ξ0 ∈ sub-esst→S'H (t, x'
(S'
), p(S'
)) and 
ξ1 ∈ super-esst→T ' H (t, x'
(T '
), p(T '
)) such that 
(−ξ0, p(S'
), ξ1, −p(T '
)) ∈ ∂g(S'
, x'
(S'
), T '
, x'
(T '
)),
(iii): H(t, x'
(t), p(t), u'
(t)) = maxu∈U (t) H(t, x'
(t), p(t), u) a.e. t ∈ [S'
, T '
].
The proof is based on perturbational methods, in which we employ the following 
distance function on admissible processes for (FET)'
: given admissible processes 
([S,T ], x, u) and ([S'
, T '
], x'
, u'
) for (FET)' we define 
dcontrol(([S,T ], x, u), ([S'
, T '
], x'
, u'
)) := |x(S) − x'
(S'
)|+ (9.6.2)
meas{t ∈ [S,T ]∩[S'
, T '
] : u(t) /= u'
(t)}+|S − S'
|+|T − T '
| .
The required properties of this distance function are summarized in the following 
lemma, whose straightforward proof we omit. 
Lemma 9.6.4 Assume that, for some interval [S'
, T '
] and σ' ∈ (0, (T ' − S'
)/2), 
hypotheses (FET2)'
- (FET4)' of the proposition are satisfied. Let 
M = { admissible processes ([S,T ], x, u) for (FET)' } .
Then (M, dcontrol) is a complete metric space.454 9 Free End-Time Problems
Proof of Proposition 9.6.3 Observe that, without loss of generality, we can reduce 
attention to the case when the following strengthen form of hypothesis (FET3)'
, in 
which the non-negative function kf is replaced by a constant: 
(FET3)∗: There exists a constant k >¯ 0 such that, for a.e. t ∈ [S' − σ'
, T ' + σ'
], 
|f (t, x, u) − f (t, x1, u)| ≤ k¯|x − x1|
for all x, x1 ∈ x'
e(t) + ϵ'
B. 
Indeed, we can consider the change of the independent variable s = ˆσ (t), where 
σˆ : R → R is defined as in the proof of Proposition 9.5.2, replacing in (9.5.1) the 
non-negative function k by kˆ
f (which can be supposed to be greater than 1 for all 
t ∈ [S' − σ'
, T ' + σ'
]). We obtain a new problem (FET)' in which the function f
and the multifunction U are respectively replaced by 
f (s, x, u) ˆ :=
‖kf ‖L1
(T ' − S' + 2σ'
) × (kf ◦ ˆσ −1)(s)f (σˆ −1(s), x, u),
for all (s, x, u) ∈ R × Rn × Rm,
and 
U (s) ˆ := U (σˆ −1(s)), for all s ∈ R.
Observe that the process ([Sˆ := ˆσ −1(S'
), Tˆ := ˆσ −1(T '
)], xˆ := x' ◦ σ −1, uˆ :=
u' ◦ σ −1) is a global minimizer for this new problem, and that fˆ satisfies (FEP3)∗
with k¯ = ‖kf ‖L1
T '
−S'
+2σ' . Notice also that, since σˆ is an absolutely continuous strictly 
continuous function on [S' − σ'
, T ' + σ'
], Gr Uˆ remains a L × Bm measurable set 
(cf. Exercise 2.3). It is then easy to derive the necessary conditions for the original 
problem, with reference to the minimizer ([S'
, T '
], x'
, u'
), via the inverse change 
of independent variable s = ˆσ (t), where the Lagrange multiplier pˆ obtained in the 
new problem is replaced by p = ˆp ◦ ˆσ in the original one. 
Consider first the special case of (FET)'
, in which g has the structure 
g(S, x0,T,x1) = ˜g(S, x0,T,x1) + e(S, x0,T,x1) ,
for some twice differentiable function g˜ and some Lipschitz continuous function e. 
We shall prove a less precise version of the necessary conditions in the proposition 
statement, in which (iii) is replaced by 
(−ξ0, p(S'
), ξ1, −p(T '
)) ∈ ∂g(S'
, x'
(S'
), T '
, x'
(T '
)) + 2
√
2ke(1 + ¯c2)
1
2 B,
where ke is a Lipschitz constant for e.9.6 A Free End-Time Maximum Principle 455
(x'
, u'
) is a minimizer for a version of problem (FET)'
, in which the underlying 
time interval is fixed at [S'
, T '
]. The hypotheses of Theorem 7.2.1 of Chap. 7 
are satisfied. In consequence of the fact that the problem has no state endpoint 
constraints, the cost multiplier can be set to 1. It follows that there exists p ∈
W1,1([S'
, T '
]; Rn) satisfying conditions (i), (iii) and the following tranversality 
condition: 
(p(S'
), −p(T '
)) ∈ ∇x0,x1g(S ˜ '
, x'
(S'
), T '
, x'
(T '
)) + keB . (9.6.3) 
For s ∈ (0, σ'
], ([S'
, T ' − s], x'
, u'
) is admissible and 
g(S ˜ '
, x'
(S'
), T ' − s, x'
(T ' − s)) + e(S'
, x'
(S'
), T ' − s, x'
(T ' − s))
≥ ˜g(S'
, x'
(S'
), T '
, x'
(T '
)) + e(S'
, x'
(S'
), T '
, x'
(T '
)) .
Since g˜ is a C2 function, there exists r1 > 0, that does not depend on s, such that 
for all s ∈ (0, σ'
]
0 ≥ ˜g(S'
, x'
(S'
), T '
, x'
(T '
)) − ˜g(S'
, x'
(S'
), T ' − s, x'
(T ' − s)) − ke(1 + ¯c2)
1
2 s
=  T '
T '
−s

∇T g(S ˜ '
, x'
(S'
), t, x'
(t)) + ∇x1g(S ˜ '
, x'
(S'
), t, x'
(t)) · ˙x'
(t)	
dt
−ke(1 + ¯c2)
1
2 s
≥ ∇T g(S ˜ '
, x'
(S'
), T '
, x'
(T '
))s −  T '
T '
−s H (t, x'
(T '
), p(T '
)) dt − r1s2
−ke(c¯ + (1 + ¯c2)
1
2 )s.
To derive the final inequality in the above relation, we have used the facts that 
−p(T '
) ∈ ∇x1g˜ (S'
, x(S'
), T '
, x(T '
)) + keB and that (from (FET3)∗ and (FET4)'
) 
we have 
p(T '
) · ˙x'
(t) ≤ H (t, x'
(T '
), p(T '
)) + |p(T '
)|k¯cs, ¯ a.e. t ∈ [T ' − s, T '
].
We have shown that 
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
T ' is a local minimum over [S'
, T '
] of
T → −∇T g(S ˜ '
, x'
(S'
), T '
, x'
(T '
))(T ' − T )
−  T '
T (−H )(t, x'
(T '
), p(T '
)) dt
+r1(T − T '
)2 + ke(c¯ + (1 + ¯c2)
1
2 )|T − T '
| .
(9.6.4) 
Next, for each integer k ≥ 1 we can select a measurable function uk : [T '
, T ' +
σ'
] → Rm such that uk(t) ∈ U (t) for a.e. t ∈ [T '
, T ' + σ'
] and 
p(T '
)·f (t, x'
(T '
), uk(t)) ≥ sup
u∈U (t)
p(T '
)·f (t, x'
(T '
), u)−1
k
, a.e. t ∈ [T '
, T '
+σ'
].456 9 Free End-Time Problems
By Corollary 6.2.5 of Filippov’s existence theorem (Theorem 6.2.3), for each k, 
there exists a process (yk, uk) on [T '
, T ' + σ'
] such that yk(T '
) = x'
(T '
); from 
(FET3)∗ and (FET4)' we also have 
 T '
+s
T '
|f (t, yk(t), uk(t)) − f (t, x'
(T '
), uk(t))|dt ≤ k¯cs¯ 2, for all s ∈ [0, σ'
].
(9.6.5) 
We now construct a sequence of admissible processes ([S'
, T ' + σ'
], x'
k, u'
k) by 
concatenating ([S'
, T '
], x'
, u'
) and ([T '
, T ' + σ'
], yk, uk). By optimality, 
0 ≤ (g˜ + e)(S'
, x'
k(S'
), T ' + s, x'
k(T ' + s)) − (g˜ + e)(S'
, x'
(S'
), T '
, x'
(T '
))
≤ ˜g(S'
, x'
(S'
), T ' + s, yk(T ' + s)) − ˜g(S'
, x'
(S'
), T '
, x'
(T '
)) + ke(1 + ¯c2)
1
2 s
=
 T '
+s
T '

∇T g(S ˜ '
, x'
(S'
), t, yk(t)) + ∇x1g(S ˜ '
, x'
(S'
), t, yk(t)) · ˙yk(t)	
dt
+ke(1 + ¯c2)
1
2 s ,
for all s ∈ [0, σ'
]. Since g˜ is twice continuously differentiable, there exists r1 > 0
(which can be assumed to be same constant appearing in (9.6.4)) independent of s, 
such that 
0 ≤
 T '
+s
T '

∇T g(S ˜ '
, x'
(S'
), t, yk(t)) + ∇x1g(S ˜ '
, x'
(S'
), t, yk(t)) · ˙yk(t)	
dt
+ke(1 + ¯c2)
1
2 s
≤ ∇T g(S ˜ '
, x'
(S'
), T '
, x'
(T '
))s +
 T '
+s
T '
∇x1g(S ˜ '
, x'
(S'
), T '
, x'
(T '
))
·f (t, x'
(T '
), uk(t)) dt + r1s2 + ke(1 + ¯c2)
1
2 s .
Recalling that −p(T '
) ∈ ∇x1g˜ (S'
, x(S'
), T '
, x(T '
)) + keB, from (9.6.5) we 
deduce that for all k
∇x1g(S ˜ '
, x'
(S'
), T '
, x'
(T '
)) · f (t, x'
(T '
), uk(t))
≤ (−H )(t, x'
(T '
), p(T '
)) +
1
k + kec¯ a.e. t ∈ [T '
, T ' + s].
Therefore, combining the preceding two relations and taking the limit as k →
+∞, we arrive at 
0 ≤ ∇T g(S ˜ '
, x'
(S'
), T '
, x'
(T '
))s
+
 T '
+s
T '
(−H )(t, x'
(T '
), p(T '
)) dt + r1s2 + ke(c¯ + (1 + ¯c2)
1
2 )s .9.6 A Free End-Time Maximum Principle 457
We have shown that 
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
T ' is a local minimum over [T '
, T ' + σ'
] of
T → ∇T g(S ˜ '
, x'
(S'
), T '
, x'
(T '
))(T − T '
)
+  T
T '(−H )(t, x'
(T '
), p(T '
)) dt
+r1|T − T '
|
2 + ke(c¯ + (1 + ¯c2)
1
2 )|T − T '
|.
(9.6.6) 
Interpreting, as usual, the integral  T
T '(−H )(t, x'
(T '
), p(T '
))dt as 
−  T '
T (−H ) (t, x'
(T '
), p(T '
))dt for T <T '
, we deduce from (9.6.4) and (9.6.6) 
that 
⎧
⎪⎨
⎪⎩
T ' is a local minimum over [S'
, T ' + σ'
] of
T → ∇T g(S ˜ '
, x'
(S'
), T '
, x'
(T '
))(T − T '
) +  T
T '(−H )(t, x'
(T '
), p(T '
)) dt
+r1(T − T '
)2 + ke(c¯ + (1 + ¯c2)
1
2 )|T − T '
| .
Applying Proposition 9.3.2, in which we take the indefinite integral function ψ to be 
ψ(t) :=  t
T '(−H )(s, x'
(T '
), p(T '
))ds, t ∈ [S'
, T ' + σ'
], noting that 0 is a limiting 
subgradient of a function at a minimizing point and also applying the sum rule for 
limiting subdifferentials of Lipschitz functions, we deduce that 
−∇T g(S ˜ '
, x'
(S'
), T '
, x'
(T '
))∈sub-ess
t→T ' (−H )(t, x'
(T '
), p(T '
))+ke(c¯+(1+ ¯c2)
1
2 )B .
Then, using Proposition 9.3.2, we obtain that 
∇T g(S ˜ '
, x'
(S'
), T '
, x'
(T '
)) ∈ super-ess
t→T '
H (t, x'
(T '
), p(T '
)) + ke(c¯ + (1 + ¯c2)
1
2 )B .
A similar analysis, in which we vary the left end-time, yields 
− ∇Sg(S ˜ '
, x'
(S'
), T '
, x'
(T '
)) ∈ sub-esst→S'H (t, x'
(S'
), p(S'
))+ke(c¯ +(1+ ¯c2)
1
2 )B .
In view of (9.6.3), we have confirmed that there exist elements ξ0 ∈
sub-esst→S'H (t, x'
(S'
), p(S'
)) and ξ1 ∈ super-esst→T 'H (t, x'
(T '
), p(T '
)) such 
that 
(−ξ0, p(S'
), ξ1, −p(T '
)) ∈∇˜g(S'
, x'
(S'
), T '
, x'
(T '
)) + 2
√
2ke(1 + ¯c2)
1
2 B.
We have proved a special case of the proposition (with weakened transversality 
condition), when g has a special structure. We now assume that g is an arbitrary 
Lipschitz continuous function. 
For i = 1, 2,... let gi be the quadratic inf convolution of g (with parameter 
α = i). Consider, for each i, the following variant of (FET)'
, in which we replace g
by gi:458 9 Free End-Time Problems
(F ET )i
'
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize gi(S, x(S), T , x(T )))
over intervals [S,T ], arcs x ∈ W1,1([S,T ]; Rn)
and measurable functions u : [S,T ] → Rm satisfying
x(t) ˙ = f (t, x(t), u(t)) a.e. t ∈ [S,T ],
u(t) ∈ U (t) a.e. t ∈ [S,T ],
|T − T '
|, |S − S'
| ≤ σ' .
Write αi := kg/
√i, where kg is a Lipschitz constant for g. According to the 
properties of quadratic inf convolutions, ([S'
, T '
], x'
, u'
) is an α2
i minimizer for this 
problem. 
We can express the preceding problem as 
Minimize {Ji([S,T ], x, u):= gi(S, x(S), T , x(T ))) : ([S,T ], x, u) ∈ M} .
(M was defined in the statement of Lemma 9.6.4.) According to this lemma, 
(M, dcontrol) is a complete metric space. Ji is continuous on (M, dcontrol). 
It follows then from Ekeland’s theorem that, for each i, there exists an ele￾ment ([Si, Ti], xi, ui) ∈ M that minimizes Ji([S,T ], x, u) + αidcontrol(([S,T ],
x, u), ([Si, Ti], xi, ui) over M and 
dcontrol(([Si, Ti], xi, ui), ([S'
, T '
], x'
, u'
)) ≤ αi .
This implies that xie → x'
e uniformly, Si → S' and Ti → T '
. Fix a measurable 
selection u˜ : [S' − σ'
, T ' + σ'
] → Rm such that u(t) ˜ ∈ U (t) for a.e. t ∈
[S' −σ'
, T ' +σ'
]. We write u'
i the measurable extension of the control ui defined on 
[Si, Ti]⊂[S' −σ'
, T ' +σ'
] to the interval [S' −σ'
, T ' +σ'
] obtained concatenating 
ui with u˜: u'
i(t) := 
ui(t) if t ∈ [Si, Ti]
u(t) ˜ if t ∈ [S' − σ'
, T ' + σ'
]\[Si, Ti]
. Noting that, for each 
interval [S,T ]⊂[S' − σ'
, T ' + σ'
], 
meas{t ∈ [S,T ]∩[Si, Ti] : u(t) /= ui(t)}
≤ meas{t ∈ [S,T ] : u(t) /= u'
i(t)} (9.6.7) 
(with equality if [S,T ]=[Si, Ti]), we see that ([Si, Ti], xi, ui) is a minimizer for 
(F ET )i
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize gi(S, x(S), T , x(T ))) + αi(
 T
S mi(t, u(t))dt
+|S − Si|+|T − Ti|+|x(S) − xi(Si)|)
over [S,T ] ⊂ R and x ∈ W1,1([S,T ]; Rn)
and measurable functions u : [S,T ] → Rm satisfying
x(t) ˙ ∈ f (t, x(t), u(t)) and u(t) ∈ U (t), a.e. t ∈ [S,T ] ,
|T − T '
|, |S − S'
| ≤ σ' ,
in which mi(t, u) := 
0 if u = u'
i(t)
1 if u /= u'
i(t) .9.6 A Free End-Time Maximum Principle 459
By properties of the quadratic inf convolution, there exists a quadratic function 
g˜i such that 
∇ ˜gi(Si, xi(Si), Ti, xi(Ti)) ∈ ∂g(ei) ,
for some ei ∈ (Si, xi(Si), Ti, xi(Ti)) + (kg/i)B, and 

g˜i(e) ≥ gi(e) for all e ∈ R × Rn × R × Rn
g˜i(e) = gi(e) when e = (S'
, x'
(S'
), T '
, x'
(T '
)) .
It follows that ([Si, Ti], xi, ui) is a minimizer for a variant of (FET)i, in which 
the quadratic function g˜i replaces gi, namely 
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize g˜i(S, x(S), T , x(T ))) + αi(
 T
S mi(t, u(t))dt
+|x(S) − xi(Si)|+|S − Si|+|T − Ti|)
over [S,T ] ⊂ R and x ∈ W1,1([S,T ]; Rn)
and measurable functions u : [S,T ] → Rm satisfying
x(t) ˙ ∈ f (t, x(t), u(t)) and u(t) ∈ U (t), a.e. t ∈ [S,T ] ,
|T − T '
|, |S − S'
| ≤ σ' .
Since Si → S' and Ti → T '
, we can arrange (by eliminating initial sequence terms) 
that |Si − S'
|, |Ti − T '
| ≤ σ'
/2. But then ([Si, Ti], xi, ui) remains a minimizer, 
when we replace the end-time constraint by |T − Ti|, |S − Si| ≤ σ'
/2. Absorbing 
the integral term in the cost into the dynamic constraint via state augmentation, we 
arrive at an example of the special case of (FET)'
, for which analysis employed in 
the first part of the proof provides necessary conditions of optimality. The relevant 
hypotheses are satisfied, when σ'
/2 replaces the parameter σ'
. It follows that there 
exists pi ∈ W1,1([Si, Ti]; Rn) such that 
(A)'
: − ˙pi(t) ∈ co ∂x [pi(t) · f (t, xi(t), ui(t))] a.e. t ∈ [Si, Ti], 
(B)'
: pi(t) · f (t, xi(t), ui(t)) ≥ pi(t) · f (t, xi(t), u) − αi for all u ∈ U (t)
a.e. t ∈ [Si, Ti], 
(C)'
: there exist ξ i
0 ∈ sub-esst→SiH (t, xi(Si), p(Si)) and 
ξ i
1 ∈ super-esst→Ti H (t, xi(Ti), p(Ti)) such that 
(−ξ i
0, pi(Si), ξ i
1, −pi(Ti) ∈ ∂g(ei) + 2
√2αi(1 + ¯c2)
1
2 B . 
We now use our ‘constant extrapolation’ convention to extend the domains of 
the pi’s to the entire real line. We deduce from (A)' and (C)' that the pi’s are 
uniformly bounded with uniformly integrably bounded derivatives. We can arrange 
then, by subsequence extraction, that pi → p uniformly and p˙i → ˙p weakly 
in L1 for some p ∈ W1,1. The sequences {ξ i
0} and {ξ i
1} are bounded. A further 
subsequence extraction ensures that they have limits ξ0 and ξ1 respectively. We 
know however that ([Si, Ti], xi, ui) → ([S'
, T '
], x'
, u'
) w.r.t. the dcontrol metric 
and ei → (S'
, x'
(S'
), T '
, x'
(T '
)). A by now familiar convergence analysis permits460 9 Free End-Time Problems
us to pass to the limit as i → ∞ in conditions (A)' - (C)' and arrive at the relations 
asserted in the Proposition statement. As earlier, we make use of the stability, under 
limit taking, properties of the limiting process, expressed in terms of the sub- and 
super essential values.
⨅⨆
Step 2 (Completion of the Proof) By imitating the analysis in Chap. 7, it is a 
straightforward task to show that the assertions of the theorem are true in general, 
if we can demonstrate their validity under the additional hypothesis: 
(A): There exist integrable functions k0 : [S¯ − σ, T¯ + σ] → R, c0 : [S¯ − σ, T¯ +
σ] → R, and a constant c¯ ≥ 0 such that 
(i): |f (t, x, u) − f (t, x'
, u)| ≤ k0(t)|x − x'
|, for all x, x' ∈ ¯x(t) + ϵB, 
u ∈ U (t), a.e. t ∈ [S¯ − σ, T¯ + σ], 
(ii): |f (t, x, u)| ≤ c0(t), for all x ∈ ¯x(t) + ϵB, u ∈ U (t), 
a.e. t ∈ [S¯ − σ, T¯ + σ], 
(iii): |f (t, x, u)|≤ ¯c for all x ∈ ¯x(t) + ϵB, u ∈ U (t), a.e. 
t ∈ [S¯ − σ, S¯ + σ]∪[T¯ − σ, T¯ + σ]. 
Observe that when we apply the ‘reduction technique’ which allows us to restrict 
attention to (A), the constant c¯ appearing in (A) (iii) is the constant of hypothesis 
(H4) of the theorem statement. Therefore, in the limit taking procedures we can 
make use of the robustness properties of sub and super essential values. 
We can also assume, without loss of generality, that ([S,¯ T¯], x,¯ u)¯ is 
a (global) minimizer for (FT)'
. To see this, assume that ([S,¯ T¯], x,¯ u)¯ is 
minimizer merely w.r.t. candidate admissible processes ([S,T ], x, u) satisfying 
d(([S,T ], x), ([S,¯ T¯], x)) ¯ ≤ β, for some β > 0. Then ([S,¯ T¯], x,¯ u)¯ is a (global) 
minimizer for (FT)' when we add the constraint: 
 T
S
|f (t, x(t), u(t)) − f (t, x(t), ¯ u(t)) ¯ |dt
+ |x(S) − ¯x(S)¯ | + (1 + 2c)( ¯ |T − T¯|+|S − S¯|) ≤ β .
By applying the special case of the theorem, in which ([S,¯ T¯], x,¯ u)¯ is a 
minimizer to this modified problem, following absorption of the integral term in the 
added constraint into the dynamics via state augmentation, and noting the added 
constraint is inactive, we validate the assertions of the theorem for the original 
problem, when ([S,¯ T¯], x,¯ u)¯ is merely a W1,1 local minimizer. 
Take αi ↓ 0 and, for each i, define 
�i(S, x0,T,xi) := max{g(S, x(S), T , x(T )) − g(S,¯ x(S), ¯ T ,¯ x(T )) ¯ + α2
i ,
dC(S, x(S), T , x(T ))}.9.6 A Free End-Time Maximum Principle 461
Now consider the problem: 
(Pi
1 )
⎧
⎪⎪⎨
⎪⎪⎩
Minimize J i
1([S,T ], x, u) subject to
x(t) ˙ = f (t, x(t), u(t)), a.e. t ∈ [S,T ],
u(t) ∈ U (t), a.e. t ∈ [S,T ],
|S − S¯|, |T − T¯| ≤ σ
in which J i
1([S,T ], x, u) := �i(S, x(S), T , x(T )). Since J i
1(([S,¯ T¯], x,¯ u)¯ = α2
i ,
and J i
1 is non-negative valued, ([S,¯ T¯], x,¯ u)¯ is an α2
i -minimizer. 
Let M denote the set of admissible processes ([S,T ], x, u) for (Pi
1 ). Observe 
that the data characterizing the admissible processes for (Pi
1 )satisfy the assumptions 
of the data which characterize the admissible processes for (F ET )'
, and, so, from 
Lemma 9.6.4 we know that (M, dcontrol) is a complete metric space. Notice also 
that the function J i
1 : M → R is continuous w.r.t. the metric dcontrol. By Ekeland’s 
theorem, we are assured of the existence of a minimizer ([Si, Ti], xi, ui) for a modi￾fication of the above optimization problem, in which J i
1([S,T ], x, u) is replaced by 
J i
1([S,T ], x, u) + dcontrol(([S,T ], x, u), ([Si, Ti], xi, ui)). Furthermore 
dcontrol(([Si, Ti], xi, ui), ([S,¯ T¯], x,¯ u)) ¯ ≤ αi . (9.6.8) 
Similarly as before, we fix a measurable selection u˜ : [S¯ − σ, T¯ + σ] → Rm
such that u(t) ˜ ∈ U (t) for a.e. t ∈ [S¯ − σ, T¯ + σ], and we write u˜i the measurable 
extension to [S¯ −σ, T¯ +σ] of the control ui (defined on [Si, Ti]⊂[S¯ −σ, T¯ +σ]), 
which is obtained as follows: 
u˜i(t) := 
ui(t) if t ∈ [Si, Ti]
u(t) ˜ if t ∈ [S¯ − σ, T¯ + σ]\[Si, Ti] .
Taking account of inequality (9.6.7), we conclude that ([Si, Ti], xi, ui) is a 
minimizer for 
(P˜i
1 )
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize �i(S, x(S), T , x(T )) + αi(
 T
S mi(t, u(t))dt
+|x(S) − xi(Si)|+|S − Si|+|T − Ti|)
over [S,T ] ⊂ R and x ∈ W1,1([S,T ]; Rn)
and measurable functions u : [S,T ] → Rm satisfying
x(t) ˙ ∈ f (t, x(t), u(t)) and u(t) ∈ U (t), a.e. t ∈ [S,T ] ,
|S − S¯|, |T − T¯| ≤ σ ,
in which mi(t, u) := 
0 if u = ˜ui(t)
1 if u /= ˜ui(t) . 
It can be deduced from (9.6.8) that ||xi − ¯x||L∞ → 0 as i → ∞ (remember our 
‘extension’ convention for interpreting such relations) and |Ti − T¯|, |Ti − T¯| → 0. 
It follows that, for i sufficiently large, (xi, ui) is minimizer for (P˜i
1 ), when the end-462 9 Free End-Time Problems
time constraint is replaced by ‘|Ti−T¯|, |Ti−T¯| ≤ σ/2’. Now absorb the integral cost 
term into the dynamics by state augmentation. The data for the problem we thereby 
obtain satisfies the hypotheses of Proposition 9.6.3. We deduce the existence of a 
costate trajectory pi such that 
(b)' :−˙pi(t) ∈ co ∂x pi(t) · f (t, xi(t), ui(t)), a.e. t ∈ [Si, Ti],
(c)'
: pi(t) · f (t, xi(t), ui(t)) ≥ pi(t) · f (t, xi(t), u) − αi for all u ∈ U (t)
a.e. t ∈ [Si, Ti], 
(d)'': there exist ξ i
0 ∈ sub-esst→SiH (t, xi(Si), p(Si)) and 
ξ i
1 ∈ super-esst→Ti H (t, xi(Ti), p(Ti)) such that 
(−ξ i
0, pi(Si), ξ i
1, −pi(Ti)) ∈ ∂�i(Si, xi(Si), Ti, xi(Ti)) + √
3αi B .
Let us examine the implications of the transversality condition (d)''. We observe 
that, for i sufficiently large: 
max{g(Si, xi(Si), Ti, xi(Ti)) − g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯
+ α2
i , dC(Si, xi(Si), Ti, xi(Ti))} > 0 . (9.6.9) 
Indeed if this were not the case, we would have g(Si, xi(Si), Ti, xi(Ti)) −
g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ ≤ −α2
i and dC(Si, xi(Si), Ti, xi(Ti)) = 0. This contradicts 
the L∞ local optimality of ([S,¯ T¯], x,¯ u)¯ for problem (F T )'
. In consequence of the 
max rule for limiting subdifferentials, there exists λi ∈ [0, 1] such that 
∂�i(Si, xi(S), Ti, xi(T )) ⊂ λi∂g(Si, xi(S), Ti, xi(T ))
+(1 − λi)∂dC(Si, xi(S), Ti, xi(T )) .
A familiar argument, based on the max rule for limiting subdifferentials and limiting 
subgradient properties of the distance function, allows us to deduce from (9.6.9) 
that, in the preceding relation, ∂dC can be replaced by ∂dC ∩ ∂B, thus: 
(d)'
: (−ξ i
0, pi(Si), ξ i
1, −pi(Ti)) ∈ λi∂g(Si, xi(S), Ti, xi(T ))
+ (1 − λi)

∂dC(Si, xi(S), Ti, xi(T )) ∩ ∂B
	
+ √3αiB .
Since |ξ i
0|, | ξ i
1|≤ ¯c||pi||L∞ we deduce from the preceding relation that 2(1 +
c)¯ ||pi||L∞ ≥ (1 − λi) − λikg − √3αi. Here kg is a Lipschitz constant for g and c¯ is 
the constant of hypothesis (H4) in the theorem statement. It follows that 
(a)'
: 2(1 + ¯c)||pi||L∞ + (1 + kg)λi ≥ 1 − √3αi .
Under the strengthened hypotheses on f (t, x, u) introduced at the beginning 
of this step of the proof (‘uniform Lipschitz continuity w.r.t. x with integrable 
Lipschitz bound’), we can deduce from (b)' and (d)'
, with the help of Gronwall’s 
lemma, that the pi’s (extended to all of (−∞,∞) by constant extrapolation) are 
uniformly bounded, with uniformly integrably bounded derivatives. But then, along 
a subsequence, pi → p uniformly and p˙i → pi, weakly in L1, for some absolutely9.7 Appendix: Metrics on the Space of Free End-Time Trajectories 463
continuous function p. We can also arrange, by further subsequence extraction, that 
λi → λ, ξ i
0 → ξ0 and ξ i
1 → ξ1. 
We will recognize in conditions (a)'
–(d)'
, perturbed versions of the desired 
conditions. A standard convergence analysis permits us to pass to the limit as 
i → ∞. We thereby recover the conditions asserted in the theorem statement.
⨅⨆
9.7 Appendix: Metrics on the Space of Free End-Time 
Trajectories 
At several points of this chapter, we use the technique of time re-parameterization 
to reduce free end-time, Lipschitz time-dependent, dynamic optimization problems, 
to fixed end-time dynamic optimization problems, to which we apply earlier derived 
necessary conditions for W1,1 local optimality. (Here, W1,1 local optimality is 
interpreted in terms of the d metric, defined in (9.1.4).) The application of the 
re-parameterization technique for the study of W1,1 local minimizers is possible 
then, only if we can show that there exists a W1,1 neighbourhood of the nominal 
arc x¯ in the space of re-parameterized F trajectories that maps into a specified 
W1,1 neighbourhood of x¯ in the space of original F trajectories, under the inverse 
parameterization. The following lemma provides this step. 
Lemma 9.7.1 Take an interval [S,¯ T¯] and x¯ ∈ W1,1([S,¯ T¯]; Rn). Choose any β >
0. Then there exist β' > 0 and α' ∈ (0, 1/2) with the following properties: 
For any S ∈ R and w : [S,¯ T¯] → R, such that w(t) ∈ [1 − α'
, 1 + α'
] a.e., let 
ψ : [S,¯ T¯]→[S,T ] be the monotone, onto, Lipschitz continuous mapping, with 
Lipschitz continuous inverse 
ψ(s) = S +
 s
S¯ w(τ )dτ, for s ∈ [S,¯ T¯] ,
where T = ψ(T )¯ . Take any y ∈ W1,1([S,¯ T¯]; Rn) such that 
 T¯
S¯ | ˙y(s) − ˙
x(s) ¯ |ds + |y(S)¯ − ¯x(S)¯ |+|S − S¯| ≤ β' . (9.7.1) 
Now define x ∈ W1,1([S,T ]; Rn) to be the function x = y ◦ ψ−1. Then 
 T ∨T¯
S∧S¯ | ˙x(t) − ˙
x(t) ¯ |dt + |x(S) − ¯x(S)¯ |+|S − S¯|+|T − T¯| ≤ β . (9.7.2) 
(Recall that, according to our convention, ˙
x(s) ¯ = 0 for s /∈ [S,¯ T¯], etc. ) 
Proof Take α' ∈ (0, 1/2), γ > 0 and β' > 0. Take S ∈ R and any measurable 
function w : [S,¯ T¯] → R such that w(t) ∈ [1 − α'
, 1 + α'
] a.e.. Let ψ : [S,¯ T¯] →464 9 Free End-Time Problems
[S, T ] be as defined in the Lemma statement. Take any y ∈ W1,1([S,¯ T¯]; Rn) such 
that (9.7.1) is satisfied and write x = y ◦ ψ−1. 
It is a simple exercise to show that 
|ψ(s) − s|≤|S − S¯| + α' × |T¯ − S¯|, for all s ∈ [S,¯ T¯] . (9.7.3) 
Let a : R → Rn be any twice continuously differentiable function, with uniformly 
bounded second derivative, such that a(S)¯ = ¯x(S)¯ and 

(−∞,+∞)
| ˙a(t) − ˙
x(t) ¯ |dt ≤ γ. (9.7.4) 
(We use our extension convention to interpret the integral.) Define the non-negative 
function ω¯ : [0,∞) → [0,∞): 
ω(r) ¯ := sup{

E
|˙
x(t) ¯ |dt | E ⊂ R is measurable and meas{E} ≤ r} .
From well-known properties of integrable functions, we have ω(r) ¯ → 0, as r ↓ 0. 
Define 
e1 := || ˙a||L1α'
e2 := (1 + α'
)||..
a||L∞(|S − S¯| + α'
|T¯ − S¯|)|T¯ − S¯|
e3 := ¯ω(|S − S¯|+|T − T¯|).
Since x ◦ ψ(s) = y(s) we know that (x˙ ◦ ψ)(s)w(s) = ˙y(s). Since |w(t) − 1| ≤ α'
, 
it follows from (9.7.4) that 
 T¯
S¯ | ˙y(s) − ˙
x(s) ¯ |ds ≥
 T¯
S¯ |(x˙ ◦ ψ)(s)w(s) − ˙a(s)|ds − γ
≥
 T¯
S¯ w(s)|(x˙ ◦ ψ)(s) − ˙a(s)|ds − γ − e1
≥
 T¯
S¯ w(s)|(x˙ ◦ ψ)(s) − (a˙ ◦ ψ)(s)|ds − γ − e1 − e2
=
 T
S
| ˙x(t) − ˙a(t)|dt − γ − e1 − e2
(we have made a change of independent variable)
≥
 T
S
| ˙x(t) − ˙
x(t) ¯ |dt − 2γ − e1 − e2
≥
 T ∨T¯
S∧S¯ | ˙x(t) − ˙
x(t) ¯ |dt − 2γ − e1 − e2 − e3.9.7 Appendix: Metrics on the Space of Free End-Time Trajectories 465
We conclude that 
 T ∨T¯
S∧S¯ | ˙x(t) − ˙
x(t) ¯ |dt − ((1 + α'
)||..
a||L∞|T¯ − S¯|)|S − S¯|
≤
 T¯
S¯ | ˙y(s) − ˙
x(s) ¯ |ds + 2γ + (|| ˙a||L1 + (1 + α'
)||..
a||L∞|T¯ − S¯|
2)α'
+ ¯ω(|S − S¯|+|T − T¯|) .
Since x(S) = y(S)¯ and, according to (9.7.3), |T − T¯|≤|S − S¯| + α'
|T¯ − S¯|, it 
follows from the preceding inequalities that 
 T ∨T¯
S∧S¯ | ˙x(t) − ˙
x(t) ¯ |dt + |x(S) − ¯x(S)¯ |+|S − S¯|+|T − T¯|
≤
 T¯
S¯ | ˙y(s) − ˙
x(s) ¯ |ds + |y(S) − ¯x(S)¯ |
+ (1 + (1 + α'
)||..
a||L∞|T¯ − S¯|)|S−S¯|2γ
+(|| ˙a||L1 + (1 + α'
)||..
a||L∞|T¯ − S¯|
2)α'
+ ¯ω(|S − S¯|+|T − T¯|) + |T − T¯|
≤
 T¯
S¯ | ˙y(s) − ˙
x(s) ¯ |ds + |y(S) − ¯x(S)¯ |
+ (2 + (1 + α'
)||..
a||L∞|T¯ − S¯|)|S−S¯| + 2γ
+(|| ˙a||L1 + (1 + α'
)||..
a||L∞|T¯ − S¯|
2 + |T¯ − S¯|)α'
+ ¯ω(2|S − S¯| + α'
|T¯ − S¯|) .
Now choose β'
, γ and α' such that 
(2 + (1 + α'
)||..
a||L∞|T¯ − S¯|)β' + 2γ
+(|| ˙a||L1+(1 + α'
)||..
a||L∞|T¯−S¯|
2+|T¯ − S¯|)α'
+ ¯ω(2β' + α'
|T¯ − S¯|)≤β .
Then 
 T ∨T¯
S∧S¯ | ˙x(t) − ˙
x(t) ¯ |dt + |x(S) − ¯x(S)¯ |+|S − S¯|+|T − T¯| ≤ β .
This is the desired inequality.
⨅⨆466 9 Free End-Time Problems
9.8 Exercises 
9.1 (Alternative Definitions for Essential Values) Take an integrable function a :
[S, T ] → R and t
¯ ∈ (S, T ). Fix any S' ∈ [S, T ]. Define the absolutely continuous 
function ψ : [S, T ] → R to be 
ψ(t) :=  t
S'
a(s)ds .
We know from Prop 9.3.2 that the limiting sub an super differentials of ψ at t
¯ can 
be represented in terms of the sub and super essential values of the integrand a at 
t
¯ (and do not depend on the choice of S' ∈ [S, T ] to define the indefinite integral 
function), thus 
∂ψ(t)¯ = sub-ess t→t
¯ a(t) and − ∂(−ψ)(t)¯ = super-ess t→t
¯
a(t) ,
We recall the definitions of sub and super essential values: 
sub-ess t→t
¯ a(t) :=

ζ ∈ R : ∃ti → t
¯ and ζi → ζ s.t.
lim sup
ϵ↓0
ϵ−1
 ti
ti−ϵ
a(s)ds ≤ ζi ≤ lim inf ϵ↓0
ϵ−1
 ti+ϵ
ti
a(s)ds, for each i

;
super-ess t→t
¯
a(t) :=

ζ ∈ R : ∃ti → t
¯ and ζi → ζ s.t.
lim sup
ϵ↓0
ϵ−1
 ti+ϵ
ti
a(s)ds ≤ ζi ≤ lim inf ϵ↓0
 ti
ti−ϵ
a(s)ds, for each i

.
These relations permit us to derive first order necessary conditions of optimality, 
expressed in terms of sub and super essential values of the Hamiltonian evaluated 
along the optimal state and costate trajectories. However sub and super essential 
values were predated, regarding their role in necessary conditions, by related 
concepts: 
Essential Value 
ess 
t→t
¯
a(t) := 
limϵ↓0 ess inf
t
¯−ϵ≤t
¯≤t+ϵ
a(t), limϵ↓0 ess sup
t
¯−ϵ≤t
¯≤t+ϵ
a(t)
,9.8 Exercises 467
Lebesgue Essential Value 
L-ess 
t→t
¯ a(t) := 
ζ ∈ R :
L-meas{s ∈ [S, T ]:|a(s)−ζ | ≤ ϵ, |s−t
¯| ≤ ϵ} > 0 for all ϵ > 0

.
(A): Show that ∂ψ(t)¯ ⊂ ess 
t→t
¯
a(t).
(B): Show that, if a is essentially bounded on a neighbourhood of t
¯, then ∂ψ(t)¯ ⊂
coL-ess 
t→t
¯ a(t) = ess 
t→t
¯
a(t).
(C): Consider the essentially bounded functions 
a1(t) := 
−1 if t ∈ [0, 1
2 ]
+1 if t ∈ ( 1
2 , 1]
and a2 := 
+1 if t ∈ [0, 1
2 ]
−1 if t ∈ ( 1
2 , 1] ,
and write ψ1 and ψ2 for the corresponding indefinite integral functions. Set 
t
¯ = 1/2. Show that 
L-ess
t→t
¯ a1(t)
strict
⊂ ∂ψ1(t)¯ and ∂ψ2(t)¯ strict
⊂ ess
t→t
¯
a2(t).
Remark 
This exercise shows that the concepts ‘essential value’, ‘Lebesgue essential value’ 
and its convex hull all fail precisely to capture precisely the elements in the limiting 
subdifferential of indefinite integral functionals. 
9.2 (Strengthened Boundary Condition on Maximized Hamiltonian) Consider 
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize g(x(T ))
over T > 0, x ∈ W1,1([0, T ]; R) and meas u : [0, T ] → R
s.t.
x(t) ˙ ∈ F (t) a.e. t ∈ [0, T ],
x(0) = 0 ,
in which 
g(x) := −x and F (t) := 
[−2, −1] if t < 1
[0, 1] if t ≥ 1 .
Write H (t, p) := max{p · v : v ∈ F (t)}. Take the admissible F trajectory ([0, T¯ =
1], x(t) ¯ ≡ −t). 
The free-time related component of the necessary conditions of Theorem 9.4.1 is 
the boundary condition468 9 Free End-Time Problems
0 ∈ super-ess
t→T¯
H (t, p(T )) ¯ (9.8.1) 
in which p is the co-state trajectory. This condition replaces previous boundary 
conditions in, say, [194] 
0 ∈ ess
t→T¯
H (t, p(T )). ¯ (9.8.2) 
Show that ([0, T¯ = 1], x(t) ¯ ≡ −t)
(i): is not an extremal with reference to the ‘super essential value’ boundary 
condition (9.8.1), 
(ii): is an extremal with reference to the standard ‘essential value’ boundary 
condition (9.8.2). 
i.e. the refined free time necessary condition Theorem 9.4.1 excludes ([0, T¯ =
1], x(t) ¯ ≡ −t) as a candidate for minimizer, but previous free time necessary 
conditions fail to do so. 
9.9 Notes for Chapter 9 
In this chapter we have derived two kinds of necessary conditions for free end-time 
dynamic optimization problems or, to be more precise, problems for which the end￾times are included among the choice variables. 
Free end-time necessary conditions of the first kind give information about the 
nature of the Hamiltonian on the interior and at the end-points of the optimal 
time interval under consideration, in particular ‘constancy of the Hamiltonian’ for 
autonomous dynamics. They are restricted to problems with dynamic constraints 
which are Lipschitz continuous with respect to time. The approach followed here, 
to introduce a change of independent variable which replaces the free end-time 
problem by a fixed end-time problem and to apply the fixed end-time necessary 
conditions to the transformed problem, is substantially that employed by Clarke 
[65]. The underlying idea is implicit in earlier literature, however. (See, e.g., [96].) 
The fact that necessary conditions of the first kind (including constancy of the 
Hamiltonian) are valid for arcs which are merely W1,1 local minimizers, has been a 
matter of some past speculation. It is proved here apparently for the first time. 
Free time necessary conditions of the second kind give information about the 
Hamiltonian merely at the optimal end-times, but cover problems for which the time 
dependence of the dynamic constraint is no longer assumed Lipschitz continuous. 
They are typically derived by reworking the proofs of the related fixed end-time 
necessary conditions to allow explicitly for variations of end-times as well as state 
trajectories.9.9 Notes for Chapter 9 469
In the case of continuous time dependence, free time necessary conditions 
including boundary conditions on the Hamiltonian are evident in the early Russian 
literature. They are also to be found in Berkovitz’ book [21]. 
Free time necessary conditions for problems with measurably time dependent 
data were introduced to the Western literature by Clarke and Vinter [79, 80], based 
on the concepts of ‘essential value’ and ‘Lebesgue essential value’ (see Exercise 9.1) 
of an almost everywhere defined function, to make sense of the boundary condition 
on the Hamiltonian in this case. (These ideas had precursors in the Russian literature 
[96].) The Lebesgue essential value is a classical concept (cf. [212]), aimed at 
identifying ‘significant’ points in the range of a function defined on a measure space. 
Extension to problems with state constraints were carried out by Clarke, Loewen 
and Vinter [83] and by Rowland and Vinter [179], who employed a more refined 
analysis to allow for active state constraints at optimal end-times. The improved 
necessary conditions of this chapter, involving refinements of essential value 
concepts, namely sub and super essential values, is a very recent development in 
the theory [29].Chapter 10 
The Maximum Principle for Problems 
with Pathwise Constraints 
Abstract This chapter provides necessary conditions of optimality for dynamic 
optimization problems involving pathwise constraints. Attention is directed at 
problems in which the dynamic constraint takes the form of a controlled differential 
equation. Two kinds of problems are considered. In the first kind, the pathwise 
constraint is imposed on the state trajectories (the ‘pure state constraint’ problem) 
and, in the second kind, it is imposed on both state trajectories and control functions 
(the ‘mixed constraint’ problem). For each kind, the necessary conditions resemble 
the maximum principle, but modified to take account of the pathwise constraint via 
additional Lagrange multipliers. 
Concerning pure state constraint problems, we consider pathwise scalar func￾tional inequality constraints. It might seem restrictive to limit attention to a scalar 
inequality constraint. But the constraint function, which is assumed merely to be 
upper semi-continuous and Lipschitz continuous w.r.t. to the state variable, can 
be constructed to embrace vector inequality constraints, set inclusion constraints 
and other constraints of interest. Simple examples illustrate that it is not possible, 
in general, to accommodate a pure state constraint by means of a simple inte￾grable Lagrange multiplier. Instead we must do so, using a measure multiplier. 
The presence of a measure multiplier gives rise to a measure driven differential 
inclusion (differential equation in the smooth case) for the co-state trajectory, 
with discontinuous solutions. The discontinuous nature of the co-state trajectory is 
somewhat disguised in the standard formulations of the necessary conditions such 
as those given in this chapter, because these conditions are expressed in terms of 
a modified, absolutely continuous co-state trajectory, obtained by subtracting off 
the singular component from the ‘true’ co-state trajectory. Dealing with measure 
multipliers (ensuring stability of key relations under perturbations, for example) 
required additional analytic tools. These are provided in a separated section near 
the beginning of the chapter. The derivation of necessary conditions is based on 
introducing an integral penalty term relating to the pathwise constraint. With the 
help of Ekeland’s theorem, we find a minimizer to a perturbed version of the 
penalized problem, close to the original minimizer. The necessary conditions for 
the perturbed problem are then interpreted as a perturbation version of the desired 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
P. Bettiol, R. B. Vinter, Principles of Dynamic Optimization, Springer Monographs 
in Mathematics, https://doi.org/10.1007/978-3-031-50089-3_10
471472 10 The Maximum Principle for Problems with Pathwise Constraints
conditions. Passage to the limit, as the parameters controlling the perturbations 
vanish, completes the derivation. 
We also address the degeneracy issue associated with necessary conditions for 
pure state constraint problems. This is connected with the fact that, for certain 
problems of interest, in which the pure state constraint is active at either of the 
end-times, the standard necessary conditions are trivial, in the sense that they are 
satisfied by all admissible state trajectory/control pairs. Following on from work 
of Arutyunov and Aseev, we show, under additional hypotheses, that this form of 
degeneracy can be eliminated. 
We adopt a general formulation of the mixed state constraint problem which 
captures, as special cases, other formulations appearing in the literature, including 
formulations involving combined functional inequality/equality constraints and set 
inclusion relations. We seek necessary conditions in which, typically, the pathwise 
constraint is accommodated by an absolutely continuous Lagrange multiplier. 
The hypotheses we must impose to justify employing multipliers of this nature, 
which require some kind of stability of the set of controls satisfying the mixed 
constraint as the state varies, are violated for pure state constraint problems. Thus 
necessary conditions for mixed constrained problems do not subsume those for pure 
state constraints. Following Clarke and de Pinho, we derive conditions for mixed 
constraint problems by introducing a differential inclusion which incorporates the 
control constraint, the mixed constraint and the dynamic constraint. The desired 
necessary conditions are obtained by applying the generalized Euler Lagrange 
conditions of Chap. 8 to the resulting differential inclusion problem. 
We consider also formulations of both pure state and mixed constraint problems, 
in which the end-times are included among the choice variables (‘free time’ 
problems). In each case, we provide the extra necessary conditions associated with 
the free end-times. 
10.1 Introduction 
We return to the framework of Chap. 7, in which the dynamic constraint takes 
the form of a differential equation parameterized by control functions. The goal 
is to extend the earlier derived necessary conditions of optimality, in the form of 
a maximum principle, to allow for additional pathwise constraints on state and 
control variables. The material in this chapter follows two well-established research 
traditions. 
The first tradition concerns necessary conditions for problems with a pathwise 
constraint on the state variable x alone, which are referred to as problems with pure 
state constraints. A common formulation, and one which we adopt in this chapter, 
is a functional inequality 
h(t, x(t)) ≤ 0 for all t ∈ [S,T ] ,10.1 Introduction 473
for a specified scalar valued function h. This appears somewhat restrictive but, 
as we show, many kinds of pure state constraints are covered by the formulation, 
including multiple functional inequality constraints, implicit constraints and con￾straints imposed on a closed subset of the underlying time interval. It was early 
apparent, even for the simplest of examples, that a straightforward extension of the 
standard maximum principle, expressed in terms of an absolutely continuous costate 
trajectory, in which the pathwise constraint is accommodated by an absolutely 
continuous state constraint Lagrange multiplier, cannot be achieved in a setting 
of significant generality. The extension can be successfully carried however, if we 
look for the state constraint Lagrange multiplier in the larger class of continuous 
linear functionals on C(S, T ), represented by a Borel measure. We derive necessary 
conditions with measure multipliers of this nature, for problems with non-smooth 
data. 
The second tradition concerns pathwise constraints on both the state variable x
and the control variable u. Such constraints are called mixed constraints. A simple 
formulation is 
φ(t, x(t), u(t)) ≤ 0 and ψ(t, x(t), u(t)) = 0, a.e. t ∈ [S,T ],
for specified functions φ and ψ (‘functional inequality and equality’ mixed con￾straints). We can exploit the fact that the constraint functionals also involve the 
control variable, now to derive broadly applicable necessary conditions of opti￾mality, in which absolutely continuous multipliers are associated with the pathwise 
constraints. Hypotheses on the data that are applied to make this possible typically 
require that projections of the (x, u) dependent constraint sets on the u variable are 
stable in some sense, under perturbations of the x variable. Such hypotheses are 
typically not satisfied by pure state constraints. So we see that the theory for mixed 
constraint problems (when we require the Lagrange multipliers to be absolutely 
continuous) is not subsumed in the pure state constraints theory. 
Until recently, the literature on necessary conditions for mixed constraint 
problems was a panoply of special cases, treating different ways of formulating the 
pathwise constraints differing over the precise nature of the necessary conditions. A 
significant unification was achieved by Clarke and de Pinho, following the approach: 
construct an equivalent dynamic optimization problem with dynamic constraint a 
differential inclusion whose right side is obtained by combining the dynamic and 
pathwise constraints, apply the generalized Euler Lagrange conditions and express 
these conditions directly in terms of the data for the original problem. We follow 
their approach to derive necessary conditions for a general formulation of the 
mixed constraints and then examine their implications for numerous special cases 
of interest.474 10 The Maximum Principle for Problems with Pathwise Constraints
10.2 Problems with Pure State Constraints: Preliminary 
Discussion 
We consider the problem 
(P )
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize g(x(S), x(T ))
over x ∈ W1,1([S,T ]; Rn) and measurable functionsu satisfying
x(t) ˙ = f (t, x(t), u(t)) a.e.,
u(t) ∈ U (t) a.e.,
h(t, x(t)) ≤ 0 for all t ∈ [S,T ],
(x(S), x(T )) ∈ C,
the data for which comprise: an interval [S,T ], functions g : Rn × Rn → R, 
f : [S,T ] × Rn × Rm → Rn and h : [S,T ] × Rn → R, a multifunction U :
[S,T ] ⇝ Rm and a closed set C ⊂ Rn × Rn. The new ingredient in problem (P) is 
the pathwise state constraint: 
h(t, x(t)) ≤ 0 for all t ∈ [S,T ]. (10.2.1) 
We have chosen to formulate the state constraint in problem (P) as a scalar 
functional inequality constraint, partly because this is a kind of state constraint 
frequently encountered in engineering applications and partly because it is a 
convenient starting point for deriving necessary conditions for other types of state 
constraints (multiple state constraints, implicit state constraints, etc.) of interest. 
As before, we refer to a measurable function u : [S,T ] → Rm which satisfies 
u(t) ∈ U (t) a.e., as a control function.A process (x, u) comprises a control function 
u and an arc x ∈ W1,1([S,T ]; Rn) which is a solution to the differential equation 
x(t) ˙ = f (t, x(t), u(t)) a.e..A state trajectory x is the first component of some 
process (x, u). A process (x, u) is said to be admissible if the state trajectory 
x satisfies the end-point constraint (x(S), x(T )) ∈ C and the state constraint 
h(t, x(t)) ≤ 0, for all t ∈ [S,T ]. 
Consistent with earlier terminology, a process (x,¯ u)¯ is said to be a W1,1local 
minimizer if there exists δ > 0 such that the process (x,¯ u)¯ minimizes g(x(S), x(T ))
over all admissible processes (x, u) satisfying 
||x − ¯x||W1,1 ≤ δ.
If the W1,1 norm is replaced by the L∞ norm, (x,¯ u)¯ is called an L∞ local 
minimizer. 
What effect does the state constraint have on necessary conditions of optimality? 
We might expect that it can be accommodated by a Lagrange multiplier term 
 T
S
h(s, x(s))m(s)ds10.2 Problems with Pure State Constraints: Preliminary Discussion 475
added to the cost. That is to say, if (x,¯ u)¯ is a minimizer for (P), then for some 
appropriately chosen, non-negative valued function m satisfying the complementary 
slackness condition : 
m(t) = 0 for t ∈ {s : h(s, x(s)) < ¯ 0}, (10.2.2) 
(x,¯ u)¯ satisfies first order necessary conditions of optimality also for the state￾constraint free problem 
⎧
⎨
⎩
Minimize g(x(S), x(T )) +  T
S h(t, x(t))m(t)dt
over x ∈ W1,1([S,T ]; Rn) and measurable functions u satisfying
x˙ = f a.e., u(t) ∈ U (t) a.e. and (x(S), x(T )) ∈ C.
Reducing this problem to one with no integral cost term by state augmentation and 
applying the state constraint-free maximum principle we arrive at the following set 
of conditions: there exist q ∈ W1,1([S,T ]; Rn) and λ ≥ 0 such that 
(λ, q, m) /= (0, 0, 0), (10.2.3) 
− ˙q(t) = q(t)fx (t, x(t), ¯ u(t)) ¯ − λhx(t, x(t))m(t) ¯ a.e., (10.2.4) 
(q(S), −q(T )) ∈ NC(x(S), ¯ x(T ¯ )) + λ∇g(x(S), ¯ x(T ¯ )), (10.2.5) 
q(t) · f (t, x(t), ¯ u(t)) ¯ = max 
u∈U (t) q(t) · f (t, x(t), ¯ u) a.e.. (10.2.6) 
These relations capture the essential character of necessary conditions for problems 
with pure state constraints, though one modification is required: in order to derive 
broadly applicable necessary conditions we must allow the Lagrange multiplier m
(the precise nature of which has not been specified) to be the ‘derivative of a function 
of bounded variation’. That is to say, the conditions assert the existence of a non￾decreasing function of bounded variation ν : [S,T ] → R such that the preceding 
relations apply with (10.2.2), (10.2.3) and (10.2.4) replaced by 
ν is constant on any subinterval of {t : h(t, x(t)) < 0}, (10.2.7) 
(λ, q, ν) /= (0, 0, 0), (10.2.8) 
and, for all t ∈ (S, T ], 
− q(t) = −q(S) +
 t
S
q(s)fx (s, x(s), ¯ u(s))ds ¯ −

[S,t]
hx (s, x(s))dν(s). ¯
(10.2.9) 
(We have absorbed λ into the state constraint multiplier: dν = λm(t)dt.) This 
is not surprising since, if h is a continuous function, problem (P) can be set up476 10 The Maximum Principle for Problems with Pathwise Constraints
as an optimization problem over pairs of elements (x, u) satisfying (among other 
conditions) the constraint 
G(x) ∈ P −,
where G : W1,1([S,T ]; Rn) → C([S,T ]; R) is the function 
G(x)(t) := h(t, x(t))
and P − is the cone of non-positive valued continuous functions on [S,T ]. One 
expects for such problems a multiplier rule to apply, involving a multiplier ξ in 
the negative polar cone of P −, regarded as a subset of the topological dual space 
C([S,T ]; R), which satisfies 
〈ξ , G(x)¯ 〉 = 0.
Such a multiplier is represented by a non-decreasing function ν of bounded variation 
according to 
〈ξ,y〉 =  T
S
y(s)dν(s) for all y ∈ C([S,T ]; R),
in which the integral is interpreted in the Stieltjes sense. The anticipated relations 
then are (10.2.5)–(10.2.9), in which λm(t) has been replaced by dν(t), for some 
non-decreasing function of bounded variation ν. 
Of course the costate arc q, which satisfies the integral equation (10.2.9), is a 
function of bounded variation itself. It is customary to aim for necessary conditions 
which differ from those just outlined in one small respect. A further modification to 
the necessary conditions, which is really just a matter of redefinition, is motivated by 
the desire to supply relations involving an absolutely continuous costate trajectory. 
The new costate trajectory, p, is obtained simply by subtracting the ‘troublesome’ 
bit off q: 
p(t) := 
q(S) if t = S
q(t) − 
[S,t] hx (s, x(s))dν(s) ¯ if t ∈ (S, T ] .
Conversely, q can be expressed in terms of p as 
q(t) := 
p(S) if t = S
p(t) + 
[S,t] hx (s, x(s))dν(s) ¯ if t ∈ (S, T ] . (10.2.10) 
The conditions now become: there exist p ∈ W1,1([S,T ]; Rn), λ ≥ 0 and a non￾decreasing function of bounded variation ν such that10.3 Convergence of Measures 477
(NC)
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
(λ, p, ν) /= 0,
− ˙p(t) = q(t) · fx (t, x(t), ¯ u(t)) ¯ a.e.,
(p(S), −q(T )) ∈ λ∇g(x(S), ¯ x(T )) ¯ + NC(x(S), ¯ x(T )), ¯
q(t) · f (t, x(t), ¯ u(t)) ¯ = max
u∈U (t) q(t) · f (t, x(t), u) ¯ a.e.,
ν is constant on any subinterval of {t : h(t, x(t)) < ¯ 0}.
In these conditions, the function q is determined from p and ν according 
to (10.2.10). 
The final touch is to express the conditions in terms of the regular Borel measure 
μ on [S,T ] associated with the function of bounded variation ν: μ is the unique 
regular Borel measure such that μ(I ) = 
I dν(t) for all closed sub-intervals I ⊂
[S,T ]. The change then is to replace  hx dν(s) by  hx dμ(s). 
In this chapter, we shall validate a generalization of the necessary condition of 
optimality (NC) above, to allow for non-smooth data. Now, the functions f (t, ., u)
and h(t, .) will be assumed to be merely Lipschitz continuous. The limiting 
subdifferential ∂xf (t, x, u) appears in place of the classical derivative fx (t, x, u), 
etc. and we shall replace hx (s, x(s)) ¯ by a selector γ of some suitably defined 
subdifferential of x → h(t, x). For convenience, we shall refer to (p, λ, μ, γ )
as ‘Lagrange multipliers’ for (P). (Strictly speaking, the Lagrange multipliers are 
(q, λ, μ)) – those associated with the dynamic constraint x˙ = f (t, x, u), the cost 
and the state constraint respectively.) The statement and proof of the nonsmooth 
maximum principle with pure state constraints is preceded by one section providing 
information about measure convergence that is required in the analysis. 
10.3 Convergence of Measures 
As earlier discussed, the presence of state constraints requires us to consider 
‘Lagrange multipliers’ that are elements in the topological dual C∗([S,T ]; Rk) of 
the space of continuous functions C([S,T ]; Rk) with supremum norm. (Here [S,T ]
is a given interval.) The norm on C∗([S,T ], Rk), written ‖μ‖TV, is the induced 
norm. The set of elements in C∗([S,T ]; R) taking non-negative values on non￾negative valued functions in C([S,T ]; R) is denoted C⊕(S, T ). 
As is well-known, elements μ ∈ C∗([S,T ]; Rk) can be identified with the 
set of finite regular vector-valued measures on the Borel subsets of [S,T ]. We 
loosely refer then to elements μ ∈ C∗([S,T ]; Rk) as ‘measures’. Notice that, for 
μ ∈ C⊕(S, T ), ‖μ‖TV, as defined above, coincides with the total variation of μ, 
[S,T ] dμ(s), as the notation would suggest. 
The support of a measure μ ∈ C∗([S,T ]; Rk), written supp{μ}, is the smallest 
closed subset A ⊂ [S,T ] with the property that for all relatively open subsets B ⊂
[S,T ] \ A we have μ(B) = 0.478 10 The Maximum Principle for Problems with Pathwise Constraints
Given μ ∈ C⊕(S, T ), a μ-continuity set is a Borel subset B ⊂ [S,T ] for which 
μ(bdyB) = 0. Take μ ∈ C⊕(S, T ). Then there is a countable set S ⊂ (S, T ), such 
that all sets of the form [a, b], [a, b), (a, b] with a, b ∈ ([S,T ]\S) are μ-continuity 
sets. 
Given a weak∗ convergent sequence μi → μ in C∗([S,T ]; Rk), there exists a 
countable subset S ⊂ (S, T ) such that 

[S,t]
dμi(s) →

[S,t]
dμ(s)
for all t ∈ ([S,T ] \ S). 
Take a weak∗ convergent sequence μi → μ in C⊕(S, T ). Then 

B
dμ(t) ≤ lim inf
i→∞ 
B
dμi(t)
for any relatively open subset B ⊂ [S,T ]. Also, 

B
h(t)dμ(t) = lim
i→∞ 
B
h(t)dμi(t)
for any h ∈ C([S,T ]; Rk) and any μ-continuity set B. 
Take closed subsets A and Ai, i = 1, 2,... of [S,T ] × Rn. We denote by A(.) :
[S,T ] ⇝ Rn the multifunction 
A(t) := {a : (t, a) ∈ A}.
The multifunctions Ai(.) are likewise defined. The following proposition will have 
an important role in justifying limit taking in ‘measure’ relations of the kind 
ηi = γiμi, i = 1, 2,...,
in which the sequence of Borel measurable functions {γi} satisfies 
γi(t) ∈ Ai(t) μi - a.e..
Conditions will be given under which we can conclude 
η0 = γ0μ0
where η0 and μ0 are weak∗ limits of {ηi} and {μi} respectively and γ0 is a Borel 
measurable function satisfying 
γ0(t) ∈ A(t) μ0 - a.e.. (10.3.1)10.3 Convergence of Measures 479
Proposition 10.3.1 Take a weak∗ convergent sequence {μi} in C⊕(S, T ),a 
sequence of Borel measurable functions {γi : [S,T ] → Rn} and a sequence of 
closed sets {Ai} in [S,T ] × Rn. Take also a closed set A in [S,T ] × Rn and a 
measure μ0 ∈ C⊕(S, T ). 
Assume that A(t) is convex for each t ∈ dom A and that the sets A and 
A1, A2,... are uniformly bounded. Assume further that 
lim sup
i→∞
Ai ⊂ A,
γi(t) ∈ Ai(t) μi - a.e. for i = 1, 2,...
and 
μi → μ0 weakly∗.
Define ηi ∈ C∗([S,T ]; Rk)
ηi := γiμi.
Then, along a subsequence, 
ηi → η0 weakly∗,
for some η0 ∈ C∗([S,T ]; Rk) such that 
η0 = γ0μ0,
in which γ0 is a Borel measurable function which satisfies 
γ0(t) ∈ A(t) μ0 - a.e..
Proof Since the sets Ai, i = 1, 2,..., are uniformly bounded there exists some 
constant K such that 
|γi(t)| ≤ K μi - a.e. t ∈ [S,T ].
But {μi} is a weak∗ convergent sequence. It follows that {‖ηi‖TV} is a bounded 
sequence. Along a subsequence then, ηi → η0 (weakly∗) for some η0 ∈
C∗([S,T ]; Rk). Given any φ ∈ C([S,T ]; Rn)
|

[S,T ]
φ(t)dη0(t)| = lim
i
|

[S,T ]
φ(t)γi(t)dμi(t)|480 10 The Maximum Principle for Problems with Pathwise Constraints
≤ K lim 
i

[S,T ]
|φ(t)|dμi(t)
= K

[S,T ]
|φ(t)|dμ0(t),
by weak∗ convergence. This inequality, which holds for every continuous φ, implies 
that η0 is absolutely continuous with respect to μ0. By the Radon-Nikodym theorem 
then, there exists an Rn valued Borel measurable and a μ0-integrable function γ0 :
[S,T ] → Rn such that 

E
dη0(t) =

E
γ0(t)dμ0(t)
for all Borel subsets E ⊂ [S,T ]. 
For each i we express the n-vector valued measure ηi in terms of its components 
ηi = (ηi1,...,ηin). Let ηij = η+
ij −η−
ij , j = 1,...,n, be the Jordan decomposition 
of ηij . For each j , {η+
ij }∞
i=1 and {η−
ij }∞
i=1 are bounded in total variation, since the ηi’s 
are bounded in total variation. By limiting attention to a further subsequence then, 
we can arrange that limi η+
ij = η+
j , limi η−
ij = η−
j , for some η+
j , η−
j ∈ C⊕(S, T )
and j = 1,...,n. (Here limits are interpreted as weak∗ limits.) Since, for each 
φ = (φ1,...,φn) ∈ C([S,T ]; Rn), 

φ(t) · dη0(t) = lim
i

φ(t) · dηi(t)
= lim
i
 	
j
φj (t)[dη+
ij − dη−
ij ](t)
=
 	
j
φj (t)[dη+
j − dη−
j ](t),
we see that η0 = ((η+
1 − η−
1 ), . . . , (η+
n − η−
n )). Let C denote the class of Borel sets 
which are continuity sets of η+
1 ,...,η+
n , η−
1 ,...,η−
n and μ0. Then for any E ⊂ C
and h ∈ C([S,T ]; Rn)

E
dηi(t) →

E
dη0(t) and 
E
h(t)dμi(t) →

E
h(t)dμ0(t). (10.3.2) 
Fix a positive integer j . Define the set Aj ⊂ [S,T ] × Rn to be 
Aj := (A + j−1B) ∩ ([S,T ] × Rn).
Then, since the sets Ai, i = 1, 2,..., are uniformly bounded and lim supi→∞ Ai ⊂
A, we have Ai ⊂ Aj , for all i sufficiently large.10.3 Convergence of Measures 481
Take any relatively open set E ⊂ [S,T ] \ dom Aj (.). Then, since ηi = γiμi and 
supp{ηi} ⊂ dom Aj (.), for i sufficiently large, we have 
0 ≥ lim inf
i→∞ 
E
dη+
ij (t) ≥

E
dη+
j (t) ≥ 0.
It follows that η+
j (E) = 0, j = 1,...,n. Likewise, we show that η−
j (E) = 0, j =
1,...,n. 
We conclude that η0(E) = 0 for all open sets E ⊂ [S,T ] \ dom Aj (.). This 
means that 
supp {η0} ⊂ dom Aj (.) .
Fix q ∈ Rn. Then, for r > 0 sufficiently large, the function 
sq (t) := 

max{q · d : d ∈ Aj (t)} if Aj (t) = ∅ /
r otherwise
is upper semi-continuous and bounded on [S,T ]. By a well-known property of 
upper semi-continuous functions, there exists a sequence of continuous functions 
{ck
q : [S,T ] → R}∞
k=1 such that 
sq (t) ≤ ck
q (t) for all t ∈ [S,T ], k = 1, 2,...
and 
sq (t) = lim
k→∞ ck
q (t) for all t ∈ [S,T ]. (10.3.3) 
Choose any E ⊂ C. For each i sufficiently large, 
q ·

E
dηi(t) =

E
q · γi(t)dμi(t)
=

E∩dom Aj (·)
q · γi(t)dμi(t)
≤

E
ck
q (t)dμi(t).
By (10.3.2) and since ck
q is continuous, we obtain in the limit as i → ∞

E
q · γ0(t)dμ0(t) ≤

E
ck
q (t)dμ0(t).482 10 The Maximum Principle for Problems with Pathwise Constraints
Since C generates the Borel sets, we readily deduce that this inequality is valid for 
all Borel sets E. It follows that, 
q · γ0(t) ≤ ck
q (t) μ0 - a.e..
From (10.3.3) then, 
q · γ0(t) ≤ sq (t) (= max{q · d : d ∈ Aj (t)}) μ0 - a.e..
By σ-additivity, this last relation holds for all q belonging to some countable, dense 
subset of Rn. Since Aj is a bounded set, the mapping q → ck
q (t) is continuous for 
each t ∈ dom Aj . We conclude that it is true for all q ∈ Rm. But then, since Aj (t)
is closed and convex for each t ∈ dom Aj , we have 
γ0(t) ∈ Aj (t) μ0 - a.e..
Finally we observe that, since A is closed set, 
γ0(t) ∈ ∩jAj (t) = A(t) μ0 - a.e..
We have shown that (along some subsequence) ηi → η0 (weakly∗) for some η0
which can be expressed η0 = γ0μ0, where γ0 is a Borel measurable function 
satisfying 
γ0(t) ∈ A(t) μ0 - a.e..
This is what we set out to prove. ⨅⨆
10.4 The Maximum Principle (Pure State Constraints) 
In this section we state necessary conditions in the form of a maximum principle 
for an dynamic optimization problem with pure state constraints (P), posed in the 
introduction. By NBV ([S,T ]; Rn) we denote the space of Rn-valued functions of 
bounded variation defined on [S,T ] which are ‘normalized’ in the sense that they 
are right-continuous on (S, T ). Define, the un-maximized Hamiltonian H : [S,T ]×
Rn × Rn × Rm → R
H(t, x, p, u) := p · f (t, x, u) . (10.4.1) 
Theorem 10.4.1 (Maximum Principle for Problems with Pure State Con￾straints) Let (x,¯ u)¯ be a W1,1 local minimizer for (P). Assume that, for some 
ϵ >¯ 0, the following hypotheses are satisfied:10.4 The Maximum Principle (Pure State Constraints) 483
(H1) For fixed x, f (., x, .) is L × Bm measurable. There exists an L × Bm
measurable function k : [S,T ] × Rm → [0,∞) such that t → k(t, u(t)) ¯
is integrable and, for a.e. t ∈ [S,T ], 
|f (t, x, u) − f (t, x'
, u)| ≤ k(t, u)|x − x'
|
for all x, x' ∈ ¯x(t) + ¯ϵB and u ∈ U (t), 
(H2): The set Gr U is L × Bm measurable, 
(H3): g is Lipschitz continuous on (x(S), ¯ x(T )) ¯ + ¯ϵB and C is a closed subset of 
Rn×n, 
(H4): h(., x) is upper semi-continuous for each x ∈ Rn and there exists kh > 0
such that 
|h(t, x) − h(t, x'
)| ≤ kh|x − x'
| for all x, x' ∈ ¯x(t) + ¯ϵB, t ∈ [S,T ] .
Then there exist p ∈ W1,1([S,T ]; Rn), μ ∈ C⊕(S, T ), a bounded Borel 
measurable function γ : [S,T ] → Rn and λ ≥ 0, satisfying the following 
conditions, in which q ∈ NBV ([S,T ]; Rn) is the function 
q(t) := 
p(S) if t = S
p(t) + 
[S,t] γ (s)dμ(s) if t ∈ (S, T ] .
(a): (p, μ, λ) /= (0, 0, 0), 
(b): − ˙p(t) ∈ co ∂xH(t, x(t), q(t), ¯ u(t)) ¯ a.e. t ∈ [S,T ], 
(c): H(t, x(t), q(t), ¯ u(t)) ¯ = max
u∈U (t)
H(t, x(t), q(t), u) ¯ a.e. t ∈ [S,T ], 
(d): (q(S), −q(T )) ∈ λ∂g(x(S), ¯ x(T )) ¯ + NC(x(S), ¯ x(T )) ¯ , 
(e): supp{μ}⊂{t ∈ [S,T ] : h(t, x(t)) ¯ = 0} and γ (t) ∈ ∂>
x h(t, x(t)) ¯
for μ-a.e. t ∈ [S,T ] ,
where 
∂>
x h(t, x(t)) ¯ := co lim sup {∂xh(ti, xi) :
ti → t,xi → ¯x(t) and h(ti, xi) > 0 for each i}.
Now assume, also, that 
f (t, x, u), h(t, x) and U (t) are independent of t.
Then, in addition to the above conditions, there exists a constant r such that 
(f): H(t, x(t), q(t), ¯ u(t)) ¯ = r a.e..484 10 The Maximum Principle for Problems with Pathwise Constraints
Remarks 
The state constraint formulation ‘h(t, x(t)) ≤ 0’, in (P ) encompasses a number of 
special cases of interest. Key features of this formulation, contributing to its broad 
applicability, are that h(t, x) is permitted to be merely Lipschitz continuous w.r.t. 
the x variable and merely upper semi-continuous w.r.t. the t variable. Consider the 
following examples. 
(i): Multiple state constraints hk(t, x(t)) ≤ 0 for t ∈ [S,T ], k = 1,...,M, 
in which the hk(t, x)’s are Lipschitz continuous w.r.t. x, can be accom￾modated by setting h(t, x) := maxk{hk(t, x)}. (The ‘composite’ state 
constraint function h(t, x) retains Lipschitz continuity w.r.t. x.) The max 
rule of nonsmooth calculus can then be used to express the necessary 
conditions directly in terms of limiting subdifferentials of the constituent 
hk(t, x)’s. 
(ii): Consider an implicit state constraint x(t) ∈ A, for t ∈ [S,T ], in which A ⊂
Rn is a given closed set. Let us assume the following regularity condition 
on the boundary of the A is satisfied: 
int T¯
A(x) /= ∅ for all x ∈ ∂A. (10.4.2) 
Here the necessary conditions are valid in a modified form where, in 
condition (e), the Borel measurable function γ is now required to satisfy 
γ (t) ∈ co (NA(x(t)) ¯ ∩ {ξ ∈ Rn : |ξ | = 1}) .
These modified conditions can be derived from Theorem 10.4.1 by setting 
h(t, x) = dA(x), where dA is the distance function to the set A, with the 
help of Proposition 4.8.2 and Lemma 4.8.3. Observe that, by the polarity 
relation T¯
A(x) = (NC(x))∗ (cf. Theorem 4.10.7), condition (10.4.2) 
implies that co NA(x(t)) ¯ is pointed. Recall that a cone K ⊂ Rn is said 
to be ‘pointed’ when the relation d1 +···+ dk = 0 is never satisfied with 
di ∈ K unless di = 0 for all i = 1,...,k. 
(iii): The requirement that h(t, x) is merely upper semi-continuous w.r.t. the t
variable adds useful flexibility to the formulation of the state constraint, 
because it permits us to cover cases when a state constraint is imposed 
only at times lying in a closed subset I ⊂ [S,T ]. In such cases we replace 
‘h(t, x(t)) ≤ 0 for all t ∈ I ’, by ‘h(t, x(t)) ˜ ≤ 0 for all t ∈ [S,T ]’, where 
h(t, x) ˜ =


h(t, x) if t ∈ I
−R if t /∈ I ,
in which R is a suitably large positive number.10.5 Proof of Theorem 10.4.1 485 
10.5 Proof of Theorem 10.4.1 
We omit the proof of the constancy of the Hamiltonian condition (f) for autonomous 
problems, since it follows from Theorem 10.6.1. 
We first confirm the assertions of the theorem, in the special case when the 
following supplementary hypotheses are satisfied. (ϵ¯ is as in (H1) and (x,¯ u)¯ is 
the admissible process of interest.) 
(A1): (x,¯ u)¯ is an L∞ local minimizer (not merely a W1,1 local minimizer), 
(A2): There exist integrable functions c0 : [S, T ] → R and k0 : [S, T ] → R such 
that 
(i): |f (t, x, u) − f (t, x'
, u)| ≤ k0(t)|x − x'
|, 
(ii): |f (t, x, u)| ≤ c0(t), 
for all x, x' ∈ ¯x(t) + ¯ϵ B, u ∈ U (t), a.e. t ∈ [S, T ]. 
(A3): h(., x) is piecewise constant in the following sense: there exists a uniform 
partition of {t0 = S, t1,...,tN = T } and Lipschitz continuous functions 
hk : Rn → Rn, for k = 0,...,N − 1, such that, for each x ∈ Rn, we have 
h(t, x) =
⎧
⎪⎪⎨
⎪⎪⎩
h0(x) if t ∈ [S,t1),
hk(x) if t ∈ (tk, tk+1) for some k ∈ {1,...,N − 2},
hk−1(x) ∨ hk(x) if t = tk for some k ∈ {1,...,N − 1},
hN−1(x) if t ∈ (tN−1, T ].
The final step of the proof will be to show that the assertions of the theorem remain 
valid, when the supplementary hypotheses are removed. 
Assume then that the W1,1 local minimizer (x,¯ u)¯ for (P) is actually an L∞ local 
minimizer (this is a consequence of supplementary hypothesis (A1)) and that, for 
some ϵ >¯ 0, (H1)–(H4) and (A2)–(A3) are satisfied. Since (x,¯ u)¯ is an L∞ local 
minimizer, we know that there exists ϵ ∈ (0, ϵ/¯ 2) such that (x,¯ u)¯ is a minimizer 
for (P), w.r.t. processes (x, u) satisfying ||x − ¯x||L∞ ≤ ϵ. 
Take ϵi ↓ 0 and, for each i, consider the problem 
(Pi)
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize Ji(x, u) := gi(x(S), x(T ), max
t∈[S,T ]
h(t, x(t)))
subject to
x(t) ˙ = f (t, x(t), u(t)), a.e. t ∈ [S,T ],
u(t) ∈ U (t) a.e. t ∈ [S,T ],
||x − ¯x||L∞ ≤ ϵ/2 .
Here 
gi(x0, x1, z) := 
g(x0, x1) − g(x(S), ¯ x(T )) ¯ + ϵ2
i

∨ dC(x0, x1) ∨ z .486 10 The Maximum Principle for Problems with Pathwise Constraints
For each i, Ji is a continuous function on the complete metric space M
M := {admissible processes(x, u) for (Pi) satisfying ||x − ¯x||L∞ ≤ ϵ/2 },
with metric 
dE ((x, u), (x'
, u'
)) := |x(S) − x'
(S)| + L-meas{t ∈ [S,T ] : u(t) /= u'
(t)}.
(10.5.1) 
Fix i. Notice that, because the cost is non-negative, and the process (x,¯ u)¯ has cost
ϵ2 
i , (x,¯ u)¯ is an ϵ2 
i minimizer of Ji over M. We deduce from Ekeland’s theorem that 
there exists a process (xi, ui) which is a minimizer for 
(P'
i)
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize J '
i(x, u)
subject to
x(t) ˙ = f (t, x(t), u(t)), a.e. t ∈ [S,T ],
u(t) ∈ U (t) a.e. t ∈ [S,T ],
||x − ¯x||L∞ ≤ ϵ/2 ,
where 
J '
i(x, u) := gi(x(S), x(T ), max
t∈[S,T ]
h(t, x(t)))
+ ϵi

|x(S) − xi(S)| + L-meas{t ∈ [S,T ] : u(t) /= ui(t)}

.
Furthermore 
dE ((xi, ui), (x,¯ u)) ¯ ≤ ϵi . (10.5.2) 
Taking note of hypothesis (A2), we deduce from Filippov’s existence theorem that 
||xi − ¯x||L∞ → 0 and L -meas{t : ui(t) /= ¯u(t)} → 0 as i → ∞. (10.5.3) 
It is obvious that gi(x(S), x(T ), max 
t∈[S,T ]
h(t, x(t))) ≥ 0, for all state trajectories x, 
since the distance function is non-negative valued. Note, also, that 
gi(xi(S), xi(T ), max
t∈[S,T ]
h(t, xi(t))) > 0, for all i sufficiently large. (10.5.4) 
This reason is that, if ‘=’ replaces ‘>’ in the above relation, then g(xi(S), xi(T )) ≤
g(x(S), ¯ x(T ¯ )) − ϵ2 
i , (xi(S), xi(T )) ∈ C and h(t, xi(t)) ≤ 0, for all t ∈ [S, T ]. 
Furthermore, ||xi − ¯x||L∞ ≤ ϵ/2. This contradicts the L∞ local optimality of (x,¯ u)¯ . 
There are two cases that can arise: 
Case 1: max 
t∈[S,T ]
h(t, xi(t)) > 0, for at most a finite number of index values i,10.5 Proof of Theorem 10.4.1 487 
Case 2: max 
t∈[S,T ]
h(t, xi(t)) > 0, for an infinite number of index values i. 
Consider first Case 1. We can arrange, by excluding initial terms in the sequence, 
that 
max
t∈[S,T ]
h(t, xi(t)) ≤ 0 for all i .
Taking note of (10.5.4), we see that (xi, ui) is an L∞ local minimizer for 
(Qi)
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize (g(x(S), x(T )) − g(x(S), ¯ x(T )) ¯ + ϵ2
i ) ∨ dC(x(S), x(T ))
+ϵi

|x(S) − xi(S)| + 
[S,T ] mi(t, u(t))dt
subject to
x(t) ˙ = f (t, x(t), u(t)), a.e. t ∈ [S,T ],
u(t) ∈ U (t) a.e. t ∈ [S,T ],
in which mi(t, u) := 

0 if u = ui(t)
1 otherwise .
In view of (10.5.4) and since h(t, xi(t)) ≤ 0 for t ∈ [S, T ], 
(g(xi(S), xi(T )) − g(x(S), ¯ x(T )) ¯ + ϵ2
i ) ∨ dC(xi(S), xi(T )) > 0 . (10.5.5) 
Consider now the dynamic optimization problem obtained by removing the state 
constraint ‘h(t, x(t)) ≤ 0’ from the original problem (P ). Problem (Qi) above 
can be interpreted as a perturbation of this ‘state-constraint free’ problem. It was 
precisely this perturbation which, together with (10.5.3) and (10.5.5), was used 
to prove the state-constraint free maximum principle in Chap. 7, in the presence 
of the supplementary hypotheses (A1)–(A3). Our previous analysis (that is step 
1 of Chap. 7, Sect. 7.4) can then be reproduced to prove all the assertions of the 
maximum principle, Theorem 10.4.1, with state constraint multiplier μ set to be 
zero. This deals with Case 1. 
We now turn to Case 2. We can arrange, by selecting a subsequence, that 
max
t∈[S,T ]
h(t, xi(t)) > 0 , for all i . (10.5.6) 
Fix i. For arbitrary K > 0, consider the optimization problem 
(P K
i )
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize J K
i (x, u, z)
over x ∈ W1,1([S,T ]; Rn), measurable functions u and z ∈ R satisfying
x(t) ˙ = f (t, x(t), u(t)), a.e. t ∈ [S,T ],
u(t) ∈ U (t), a.e. t ∈ [S,T ],
||x − ¯x||L∞ ≤ ϵ/2 ,488 10 The Maximum Principle for Problems with Pathwise Constraints
in which 
J K
i (x, u, z) := gi(x(S), x(T ), z) + ϵidE ((x, u), (xi, ui))
+ K
 T
S
(h(t, x(t)) − z) ∨ 0)
2dt .
Notice that, for each K and any (x, u), we have J K
i (x, u, z = maxt∈[S,T ] h(t,
x(t))) = J '
i(x, u). This implies that, for each i, inf(P K
i ) ≤ inf(P'
i). According to 
the following lemma, the two infima coincide in the limit as K → ∞, however. 
Lemma 10.5.1 For i = 1, 2,..., 
lim
K→∞ inf(P K
i ) = inf(P'
i) .
Proof Fix i. Assume that assertion of the lemma is false. Since the mapping K →
inf(P K
i ) is non-decreasing, we may deduce the existence of ρ >¯ 0 and sequences 
Kj ↑ ∞ and {(xj , uj , zj )} in M × R such that, for each j , 
gi(xj (S), xj (T ), zj ) + ϵidE ((xj , uj ), (xi, ui))
+Kj

[S,T ]
((h(t, xj (t)) − zj ) ∨ 0))2dt
< gi(xi(S), xi(T ), max
t∈[S,T ]
h(t, xi(t)))− ¯ρ , for j=1, 2,... (10.5.7) 
Notice, at the outset, that we can assume 
max
t∈[S,T ]
h(t, xj (t)) − zj ≥ 0 for all j . (10.5.8) 
To see this, suppose that, for some j , this condition is violated, i.e. max{h(t, xj (t)) :
t ∈ [S, T ]} < zj . Now replace zj by the lower number max{h(t, xj (t)) : t ∈
[S, T ]}. We see that, following this change, (10.5.8) is satisfied. Furthermore, the 
strict inequality (10.5.7) is preserved; this is because the right side of this relation is 
unaffected. On the other hand, the value of the integral term on the left is reduced 
to zero while the first term on the left cannot increase, owing to the monotonicity 
of z → gi(x0, x1, z). We have confirmed that, without loss of generality, we may 
assume (10.5.8). Since the xj ’s are uniformly bounded in L∞, the zj ’s are uniformly 
bounded above. It is also clear from the non-negativity of the first two terms and the 
structure of the integrand of the integral on the left side of (10.5.7) that the zj ’s are 
also bounded below. It follows that {zj } is a bounded sequence. Again, from the 
non-negativity of the first two terms and since Kj → ∞ as j → ∞, we deduce that 

[S,T ]
(h(t, xj (t)) − zj ) ∨ 0)
2dt → 0, as j → ∞. (10.5.9)10.5 Proof of Theorem 10.4.1 489 
Fix j . Let t
¯ be a point at which (h(t, xj (t))−zj ) achieves its maximum over [S, T ]. 
Using the facts that the xj ’s are uniformly bounded w.r.t. the L∞ norm and the 
zj ’s are bounded and invoking hypothesis (H4) and (A3), we see that there exists 
α¯ ∈ (0, |T −S|
2N ) such that 
((h(t, xj (t)) − zj ) ∨ 0)
2 ≥
1
2
((h(t,x ¯ j (t)) ¯ − zj ) ∨ 0)
2,
for all t ∈ [S,T ] s.t. either t ∈ [t
¯ − ¯α,t
¯] or t ∈ [t ,¯ t
¯ + ¯α] .
Here 1/N is the ‘mesh-size’ parameter of piecewise continuous function h(., x). 
(See supplementary hypothesis (A3).) Note the role of (A3) in establishing this 
crucial inequality. But then 
max
t∈[S,T ]
h(t, xj (t)) − zj
= h(t,x ¯ j (t)) ¯ − zj ≤ (2/α)¯ 1
2
 
[S,T ]
((h(t, xj (t)) − zj ) ∨ 0)
2dt1
2 .
It follows now from (10.5.8) and (10.5.9) that 
max
t∈[S,T ]
h(t, xj (t)) − zj → 0, as j → ∞.
Since gi(x0, x1, .) is Lipschitz continuous (with Lipschitz constant 1) we deduce 
from (10.5.7) that, for j sufficiently large, 
gi(xj (S), xj (T ), max
t∈[S,T ]
h(t, xj (t))) + ϵidE ((xj , uj ), (xi, ui))
< gi(xi(S), xi(T ), max
t∈[S,T ]
h(t, xi(t))) − ¯ρ/2 , for j = 1, 2,... (10.5.10) 
On the other hand, by the optimality of (xi, ui) we have 
gi(xj (S), xj (T ), max
t∈[S,T ]
h(t, xj (t))) + ϵidE ((xj , uj ), (xi, ui))
≥ gi(xi(S), xi(T ), max
t∈[S,T ]
h(t, xi(t))) .
This contradicts the optimality of (xi, ui). The lemma is proved. ⨅⨆
In view of (10.5.6), and the continuity of the embedding (x, u) → x from M to 
L∞, we can find a sequence ρi ↓ 0 such that, for each i, 
(x, u, z) ∈ M × R
dE ((x, u), (xi, ui)) ≤ ρi
|z − maxt∈[S,T ] h(t, xi(t))| ≤ ρi
⎫
⎬
⎭
=⇒ gi(x(S), x(T ), z) > 0 and z > 0 .
(10.5.11)490 10 The Maximum Principle for Problems with Pathwise Constraints
According to the lemma, we can choose Ki ↑ ∞ such that, for each i, (xi, ui, zi :=
maxt∈[S,T ] h(t, xi(t))) is a ρ2 
i -minimizer for (P Ki
i ). Ekeland’s theorem then tells us 
that there exists (x'
i, u'
i, z'
i ≡ z'
i) ∈ M × R that is an L∞ local minimizer for (Q˜ i): 
(Q˜ i)
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize gi(x(S), x(T ), z(T ))+ϵi

|x(S)−xi(S)|+ 
[S,T ] mi(t, u(t))dt
+ρi

|x(S) − x'
i(S)|+|z(S) − z'
i(S)| + 
[S,T ]
(m'
i(t, u(t))dt
+Ki

[S,T ]
((h(t, x(t)) − z(t)) ∨ 0)2dt
over (z, x) ∈ W1,1([S,T ]; R1+n), meas. u satisfying
(z(t), ˙ x(t)) ˙ = (0, f (t, x(t), u(t))), a.e. t ∈ [S,T ]
u(t) ∈ U (t) a.e. t ∈ [S,T ] .
Here, m'
i(t, u) := 

0 if u = u'
i(t)
1 otherwise 
. Furthermore 
dE ((x'
i, u'
i), (xi, ui)) + |z'
i − max
t∈[S,T ]
h(t, xi(t))| ≤ ρi . (10.5.12) 
This relation combines with (10.5.2) to tell us that, as i → ∞, 
||x'
i − ¯x||L∞ → 0, z'
i → max
t∈[S,T ]
h(t, x(t)) ¯
and L − meas{t ∈ [S,T ] : u'
i(t) = ¯ / u(t)} → 0 .
It follows from (10.5.11) that 
gi(x'
i(S), x'
i(T ), z'
i) > 0 and z'
i > 0 , for each i . (10.5.13) 
Now, for each i, apply the earlier derived state-constraint free version of Clarke’s 
nonsmooth maximum principle of Chap. 7 to the problem (Q˜ i), with reference to 
the L∞ local minimizer (z'
i, x'
i, u'
i), noting that the relevant hypotheses are satisfied. 
Since (Q˜ i) has no endpoint constraints, we can set the cost multiplier to 1. We are 
assured then of the existence of elements −ri ∈ W1,1 and qi ∈ W1,1 (interpreted as 
costate components associated with the state variable components z and x variables 
respectively) and a Borel measurable function γi : [S, T ] → Rn such that 
− ˙qi(t) ∈ co ∂x qi(t) · f (t, x'
i(t), u'
i(t)) − γi(t)(dμi/dt)(t) a.e. t ∈ [S,T ],
(10.5.14) 
and 
γi(t) ∈ ∂xh(t, x'
i(t)) μi-a.e. t ∈ [S,T ], (10.5.15) 
qi(t)· f (t, x'
i(t), u'
i(t)) ≥ sup
u∈U (t)
qi(t)· f (t, x'
i(t), u) − 2(ϵi + ρi), a.e. t ∈ [S,T ],
(10.5.16)10.5 Proof of Theorem 10.4.1 491 
and 
(qi(S), −qi(T ), ri(T )) ∈ ∂gi(x'
i(S), x'
i(T ), z'
i(T )) + (ϵi + ρi)B × {0}×{0},
(10.5.17) 
Furthermore, r˙i = (dμi/dt) a.e. and ri(S) ∈ ρiB. This implies that 
ri(T ) ∈

[S,T ]
(dμi/ds)(s)ds + ρiB . (10.5.18) 
Here μi is a (positive) measure associated with distribution: 
μi([S,t]) = 2Ki

[S,t]
((h(s, x'
i(s)) − z'
i) ∨ 0)ds . (10.5.19) 
It follows from (10.5.19) that 
supp {μi}⊂{t ∈ [S,T ] : h(t, x'
i(t)) ≥ z'
i}. (10.5.20) 
Recalling the definition of gi, namely 
gi(x0, x1, z) := 
g(x0, x1) − gi(x0, x1) + ϵi

∨ dC(x0, x1) ∨ z ,
and taking note of max rule for subdifferentials, we deduce from (10.5.17) that 
(qi(S), −qi(T ), ri(T )) ∈ λi∂g(x'
i(S), x'
i(T )) × {0}
+ λ(1)
i ∂dC(x'
i(S), x'
i(T )) × {0} +
λ(2)
i {(0, 0, 1)} + (ϵi + ρi)B × {0}×{0},
in which λi, λ(1)
i λ(2)
i are non-negative numbers such that λi + λ(1)
i + λ(2)
i = 1. The 
relation can be replaced by the following more precise condition: 
(qi(S), − qi(T ), ri(T )) ∈ λi∂g(x'
i(S), x'
i(T )) × {0}
+ λ(1)
i

∂dC(x'
i(S), x'
i(T )) ∩ ∂B)

× {0}
+λ(2)
i {(0, 0, 1)} + (ϵi + ρi)B × {0}×{0}, (10.5.21) 
in which, we recall, ∂B denotes the boundary of the unit ball in Euclidean space. 
We check this assertion. The two relations are equivalent when λ(1)
i = 0. 
So we may assume that λ(1)
i > 0. But then, according to the max rule, 
the ‘dC((x'
i(S), x'
i(T )))’ term must achieve the maximum in the definition of 
gi(x'
i(S), x'
i(T ), z'
i). Since, however, gi(x'
i(S), x'
i(T ), z'
i) > 0 (see (10.5.13)), we 
conclude that dC((x'
i(S), x'
i(T ))) > 0. We know from Lemma 4.8.3 that492 10 The Maximum Principle for Problems with Pathwise Constraints
ξ ∈ ∂dC(z) and z /∈ C =⇒ ξ ∈ ∂dC(z) ∩ ∂B .
(10.5.21) follows from the preceding relation. From (10.5.21), 
(qi(S), −qi(T )) ∈ λi∂g(x'
i(S), x'
i(T )) + NC(x'
i(S), x'
i(T ))∩B + (ϵi + ρi)B × {0}.
(10.5.22) 
From (10.5.18) and (10.5.21), and since λ(2)
i ≤ 1, we see that 
|(qi(S), −qi(T ))| ≤ 1 + kg + (ϵi + ρi) , ||μi||TV ≤ 1 + ρi (10.5.23) 
and λ(2)
i ≤ ||μi||TV + ρi. (10.5.21) also tells us that λ(1)
i ≤ |(qi(S), −qi(T ))| +
kgλi + (ρi + ϵi). Since λi + λ(1)
i + λ(2)
i = 1, it follows from these relations that 
(1 + kg)λi + |(qi(S), −qi(T ))| + ||μi||T V ≥ 1 − (ϵi + 2ρi) (10.5.24) 
Now define the function pi ∈ W1,1([S, T ]; Rn) to be 
pi(t) := qi(t) −

[S,t]
γi(s)(dμi(s)/ds)(s)ds for t ∈ [S,T ] .
Condition (10.5.14) can be expressed in terms of pi: 
− ˙pi(t) ∈ co ∂x qi(t) · f (t, x'
i(t), u'
i(t)) a.e. t ∈ [S,T ] . (10.5.25) 
We summarize our findings so far: for i = 1, 2,..., there exist pi ∈ W1,1, 
μi ∈ C⊕(S, T ), a Borel measurable function γi and λi ≥ 0 satisfying con￾ditions (10.5.24), (10.5.25), (10.5.22), (10.5.16), (10.5.15) and (10.5.20) which 
imply: 
(a'
): (1 + kg)λi + |(qi(S), −qi(T ))| + ||μi||T V ≥ 1 − (ϵi + 2ρi), 
(b'
): − ˙pi(t) ∈ co ∂x qi(s) · f (t, x'
i(t), u'
i(t)) a.e., 
(c'
): qi(t) · f (t, x'
i(t), u'
i(t)) ≥ sup 
u∈U (t)
qi(t) · f (t, x'
i(t), u) − 2(ϵi + ρi), a.e., 
(d'
): (qi(S), −qi(T )) ∈ λi∂g(x'
i(S), x'
i(T )) +

NC(x'
i(S), x'
i(T ))) ∩ B + (ϵi + ρi)B × {0}, 
(e'
): γi(t) ∈ ∂xh(t, x'
i(t)), μi-a.e. and 
supp {μi}⊂{t ∈ [S, T ] : h(t, x'
i(t)) ≥ z'
i}. 
These conditions will be recognized as perturbed versions of conditions (a)–(e) in 
the maximum principle Theorem 10.4.1. The next step is to capture (a)–(e) from 
(a'
)–(e'
), in the limit as i → ∞. 
In view of (10.5.2) and (10.5.12), we have ||x'
i − ¯x||L∞ → 0 as i → ∞ and we 
can arrange, by subsequence extraction, that 
L-meas{t : u'
i(t) = ¯u(t) for all i sufficiently large}=|T − S| . (10.5.26)10.5 Proof of Theorem 10.4.1 493 
From (10.5.23) we deduce that {||μi||T V }, {qi(S)} and {pi(S)} are bounded 
sequences. By (b'
) then, and in consequence of Filippov’s existence theorem, 
the qi’s are uniformly bounded, the pi’s are uniformly bounded and the p˙i’s 
are uniformly integrably bounded. Invoking Proposition 10.3.1, we deduce that, 
following a subsequence extraction, 
||pi − p||L∞ → 0, and μi → μ, γiμi → γ μ weakly∗
as i → ∞. Furthermore on some subset S1 ⊂ [S, T ] of full L-measure we have 
qi(t) → q(t) for all t ∈ S1 ∪ {S}∪{T } (10.5.27) 
as i → ∞, where 
q(t) := p(t) +

[S,t]
γ (s)(dμ(s)/ds)(s)ds for t ∈ (S, T ] ,
for some p ∈ W1,1([S, T ]; Rn), q ∈ NBV ([S, T ]; Rn) and μ ∈ C⊕(S, T ), and 
some Borel measurable function γ . 
Bearing in mind that z'
i > 0 for each i and x
'
i → ¯x uniformly, we deduce from 
(e'
) that, for some δi ↓ 0, 
γi(t) ∈ ∪{∂xh(t, y) : y ∈ ¯x(t) + δiB and h(t, y) > 0}, μi-a.e. t ∈ [S,T ] .
We can deduce from this relation, with the help of Proposition 10.3.1 that 
γ (t) ∈ ∂>
x h(t, x(t)) , μ ¯ -a.e. t ∈ [S,T ] .
Taking note of (10.5.26) and (10.5.27), we deduce from (c'
) and (d'
) that 
q(t) · f (t, x(t), ¯ u(t)) ¯ ≥ sup
u∈U (t)
q(t) · f (t, x(t), u), ¯ a.e.,
and 
(q(S), −q(T )) ∈ λ∂g(x(S), ¯ x(T )) ¯ + NC(x(S), ¯ x(T )) . ¯
From (a'
) we conclude that: (1+kg)λ+ |(q(S), q(T ))| + ||μ||T V ≥ 1. This implies 
that 
(λ, p, μ) /= (0, 0, 0) ,
since, when μ = 0, p = q. All the assertions of the state constrained maximum 
principle have been confirmed.494 10 The Maximum Principle for Problems with Pathwise Constraints
Up to this point in our analysis, we have assumed that (x,¯ u)¯ is an L∞ local 
minimizer and when supplementary hypothesis (A1), (A2) and (A3) are imposed. It 
remains to lift these restrictions. 
We deal first with (A3), the hypothesis that imposed additional structure on the 
state constraint function h. Assume then (A1) and (A2) are satisfied, but not (A3). 
Take sequence of integers Nj ↑ ∞. For each j , let {t
j
0 = S, t
j
1 ,...,t j
Nj−1, t j
Nj = T }
be a uniform partition of [S, T ]. Now define 
hj (t, x) =
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
max{h(t, x) : t ∈ [S,t j
1 ]} if t ∈ [S,t j
0 ),
max{h(t, x) : t ∈ [t
j
k , t j
k+1]} if t ∈ (t j
k , t j
k+1) for some
k ∈ {1,...,Nj − 2},
max{h(t, x) : t ∈ [t
j
k−1, t j
k+1]} if t = t
j
k for some
k ∈ {1,...,Nj − 1},
max{h(t, x) : t ∈ [t
j
Nj−1, t j
Nj
]} if t ∈ (t j
Nj−1, t j
Nj
] .
Taking note of hypotheses (H4) and (A2), we can show 
sup
t∈[S,T ]
h(t, x(t)) ≤ sup
t∈[S,T ]
hj (t, x(t)) ≤ sup
t∈[S,T ]
h(t, x(t)) + δj , (10.5.28) 
for all state trajectories x satisfying ||x − ¯x||L∞ ≤ ϵ/2, where δj := kh × θ (2|T −
S|/Nj ) and 
θ (α) := sup{

I
c0(t)dt : Lebesgue sets I ⊂ [S,T ] s.t. L-meas{I } ≤ α}.
Here kh and c0 are as in (H4) and (A2) respectively. Notice that δj ↓ 0 as i → ∞. 
We now go back to an earlier stage of the proof. Recall, we specified a sequence
ϵi ↓ 0 and invoked Ekeland’s theorem to obtain a process (xi, ui), for each i, that 
was a minimizer for (P'
i) which we reproduce: 
(P'
i)
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize J '
i(x, u)
subject to
x(t) ˙ = f (t, x(t), u(t)), a.e. t ∈ [S,T ],
u(t) ∈ U (t) a.e. t ∈ [S,T ],
||x − ¯x||L∞ ≤ ϵ/2 ,
where 
J '
i(x, u) :=gi(x(S), x(T ), max
t∈[S,T ]
h(t, x(t)))
+ ϵi

|x(S) − xi(S)| + L-meas{t ∈ [S,T ] : u(t) /= ui(t)}
10.5 Proof of Theorem 10.4.1 495 
and 
dE ((xi, ui), (x,¯ u)) ¯ ≤ ϵi .
We observed, furthermore, that 
||xi − ¯x||L∞ → 0 and L-meas{t : ui(t) /= ¯u(t)} → 0 as i → ∞
and, for some ρi ↓ 0 and αi ↓ 0, 
(x, u) ∈ M
dE ((x, u), (xi, ui)) ≤ ρi

=⇒ gi(x(S), x(T ), max
t∈[S,T ]
h(t, x(t)))> αi .
(10.5.29) 
(This was relation (10.5.11).) We now take a new direction in the analysis. 
According to (10.5.28), we can choose, for each i, an integer j (i) such that 
sup
t∈[S,T ]
h(t, x(t)) ≤ sup
t∈[S,T ]
hj (i)(t, x(t)) ≤ sup
t∈[S,T ]
h(t, x(t))+ρ2
i , (10.5.30) 
for all state trajectories x satisfying ||x − ¯x||L∞ ≤ ϵ/2. 
Consider a variant on (P'
i), in which hj (i) replaces h: 
(Q'
i)
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize gi(x(S), x(T ), max
t∈[S,T ]
hj (i)(t, x(t)))
+ϵi

|x(S) − xi(S)| + 
[S,T ] mi(t, u(t))dt
over x ∈ W1,1 and meas. functions u satisfying
x(t) ˙ = f (t, x(t), u(t)), a.e. t ∈ [S,T ]
u(t) ∈ U (t) a.e. t ∈ [S,T ] ,
||x − ¯x||L∞ ≤ ϵ/2
where mi(t, u) := 

0 if u = ui(t)
1 otherwise . It will be clear from (10.5.30) that (xi, ui) is a 
ρ2 
i minimizer for (Q'
i). Invoking Ekeland’s theorem, we can find, for i sufficiently 
large, an L∞ local minimizer (x'
i, z'
i ≡ max 
t∈[S,T ]
hj (i)(t, x'
i(t)), u'
i(t)) for the problem 
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize gi(x(S), x(T ), z(T )) + ϵi

|x(S) − xi(S)| + 
[S,T ] mi(t, u(t))dt
+ρi

|x(S) − x'
i(S)| + 
[S,T ] m'
i(t, u(t))dt
over x,z ∈ W1,1 and meas. functions u satisfying
(x(t), ˙ z(t)) ˙ = (f (t, x(t), u(t)), 0), a.e. t ∈ [S,T ]
u(t) ∈ U (t) a.e. t ∈ [S,T ] ,
hj (i)(t, x(t)) − z ≤ 0 for all t ∈ [S,T ]496 10 The Maximum Principle for Problems with Pathwise Constraints
in which, once again, m'
i(t, u) := 

0 if u = u'
i(t) ,
1 otherwise 
and 
dE ((x'
i, u'
i), (xi, ui))≤ ρi .
But then, by (10.5.29) and since hi(j ) ≥ h and z → gi(x0, x1, z) is an increasing 
function, we have 
gi(x'
i(S), x'
i(T ), z'
i = max
t∈[S,T ]
hi(j )(t, x'
i(t))) > αi . (10.5.31) 
Fix i. Taking note of the fact that the state constraint function hj (i) satisfies 
the supplementary hypothesis (A3), we may apply the special case of the state 
constrained maximum principle already proved. This asserts existence of multipliers 
(pi, −ri) ∈ W1,1 (associated with the states x and z respectively), μi ∈ C⊕(S, T ), 
a bounded Borel measurable function γi : [S, T ] → Rn and λ¯i ≥ 0, satisfying the 
following conditions: 
(A): (λ¯i, pi, ri, μi) /= (0, 0, 0, 0), 
(B): − ˙pi(t) ∈ co ∂x (qi(t) · f (t, x'
i(t), u'
i(t))) and r˙i(t) ≡ 0 a.e. t ∈ [S, T ], 
(C): qi(t) · f (t, x'
i(t), u'
i(t)) ≥ max 
u∈U (t)qi(t) · f (t, x'
i(t), u) − λ¯i × 2(ϵi + ρi)
a.e. t ∈ [S, T ], 
(D): 
qi(S), −qi(T ),ri(T ) + 
[S,T ] dμi(t)
∈ λ¯i∂gi(x'
i(S), x'
i(T ), z'
i), r(S) = 0, 
(E): supp{μi}⊂{t ∈ [S, T ] : hj (i)(t, x'
i(t)) = z'
i} and 
γi(t) ∈ ∂>
x hj (i)(t, x'
i(t)) for μi-a.e. t ∈ [S, T ] ,
in which qi(t) := 
pi(S) if t = S
pi(t) + 
[S,t] γi(s)dμi(s) if t ∈ (S, T ] .
(B) and (D) imply that ri ≡ 0. We see also from these conditions that λ¯i /= 0 for, 
otherwise, (pi, ri, μi) = (0, 0, 0), in violation of (A). Scaling the multipliers, we 
can arrange then that λ¯i = 1. We can therefore replace (B), (C) and (D) by 
(B'
): − ˙pi(t) ∈ co ∂x (qi(t) · f (t, x'
i(t), u'
i(t))) a.e. t ∈ [S, T ], 
(C'
): qi(t) · f (t, x'
i(t), q(t), u'
i(t)) ≥ max 
u∈U (t)qi(t) · f (t, x'
i(t), u) − 2(ϵi + ρi)
a.e. t ∈ [S, T ]
and

qi(S), −qi(T ), 
[S,T ] dμi(t)
∈ ∂gi(x'
i(S), x'
i(T ), z'
i). 
Following the pattern on our earlier analysis, we can deduce from the preceding 
relation and (10.5.31) that there exist λi ≥ 0, λ(1)
i ≥ 0 and λ(2)
i ≥ 0 such that 
λi + λ(1)
i + λ(2)
i = 1 and10.5 Proof of Theorem 10.4.1 497 
(qi(S), − qi(T ), 
[S,T ]
dμi(t)) ∈ λi∂g(x'
i(S), x'
i(T )) × {0}
+λ(1)
i

∂dC(x'
i(S), x'
i(T )) ∩ ∂B)

× {0}
+λ(2)
i {(0, 0, 1)}+(ϵi + ρi)B × {0}×{0}. (10.5.32) 
This implies 
(D'
) : (qi(S), −qi(T ) ∈ λi∂g(x'
i(S), x'
i(T )) × {0}
+ NC(x'
i(S), x'
i(T )) ∩ B × {0} + (ϵi + ρi)B × {0}.
We see from (10.5.32) that λ(2)
i = 
[S,T ] dμi(s) = ||μi||TV. In view of (10.5.31), 
sup
t∈[S,T ]
hj (i)(t, x'
i(t))< αi =⇒ μi = 0 . (10.5.33) 
We can also deduce from the relation and the definition of qi that 
√
2(||pi||L∞ + ||μi||TV) ≥ |(qi(S), qi(T ))| ≥ λ(1)
i − kgλi − ϵi − ρi .
Then, since λ(1)
i = 1 − ||μi||TV − λi, 
√
2(||pi||L∞ + ||μi||TV) ≥ |(qi(S), qi(T ))| ≥ 1 − ||μi||TV − (1 + kg)λi − ϵi − ρi .
It follows that 
√
2 ||pi||L∞ + (1 + √
2)||μi||TV + (1 + kg)λi ≥ 1 − ϵi − ρi . (10.5.34) 
We know that λi ≤ 1. Equation (10.5.32) implies that {pi(S)} and {||μi||TV}
are bounded sequences. In consequence of (A2), {pi} is a bounded sequence in 
L∞, with integrably bounded derivatives. We may arrange then, by extracting 
subsequences that pi → p and p˙i → ˙p, μi → μ (in the appropriate topologies), 
for some p ∈ W1,1([S, T ]; Rn), μ ∈ C⊕(S, T ) and λ ≥ 0. 
Now take any point t in the support of μi. It follows from (10.5.33) and the max 
rule that λ(2)
i = 0 and so 
sup
t∈[S,T ]
hj (i)(t, x'
i(t)) ≥ αi .
Write δi := |T − S|/Nj (i). We deduce from the max rule (Theorem 5.7.1) that 
∂x hj (i)(t, x'
i(t)) ⊂ ∩ϵ'
>0 co {ξ ∈ ∂xh(s, y) : (10.5.35)
s ∈ [t − 2δi, t + 2δi]∩[S,T ], y ∈ x'
i(t) + ϵ'
B, h(s, y) ≥ αi − ϵ'
}.
(Confirm this relation first for proximal subgradients of x → h(t, x) using the 
proximal subgradient inequality. It extends to limiting subgradients by the density498 10 The Maximum Principle for Problems with Pathwise Constraints
properties of proximal subgradients.) This implies 
supp {μi}⊂{t ∈ [S,T ] : max s∈[t−2δi,t+2δi]
h(s, x'
i(t)) > 0}. (10.5.36) 
But x'
i → ¯x uniformly. It therefore follows from Proposition 10.3.1, together with 
condition (E), (10.5.36) and condition (10.5.35), which is valid for all t ∈ supp {μi}, 
that 

[0,t]
γi(s)dμi(s) →

[0,t]
γ (s)dμ(s) for all t ∈ [S,T ]
for some Borel measurable function γ such that 
γ (t) ∈ ∩ϵ'
>0 co {ξ ∈ ∂xh(s, y) :
s ∈ [t − ϵ'
, t + ϵ'
]∩[S,T ], y ∈ ¯x(t) + ϵ'
B, h(s, y) > 0}
= ∂>
x h(t, x(t)), μ ¯ -a.e.
and supp {μ}⊂{t ∈ [S, T ] : h(t, x(t)) ¯ = 0}. Condition (e) of the theorem 
statement is confirmed. 
Passing to the limit in (10.5.34), as i → ∞, yields (p, μ, λ) /= (0, 0, 0). This 
is condition (a) of the theorem statement. The remaining conditions (b) – (d) are 
obtained from (B'
) – (D'
), in the limit as j → ∞. 
Finally let us attend to the removal of the supplementary hypotheses (A1) and 
(A2). This is accomplished as in the final step of the proof of the state-constraint free 
maximum principle of Chap. 7. To be more precise, a ‘state augmentation’ argument 
is used to verify that the assertions of the theorem remain valid when (x,¯ u)¯ is merely 
a W1,1, not an L∞, local minimizer. (This disposes of (A1).) If (A2) is violated we 
consider a variant of problem (P) of the introduction in which U (t) is replaced by 
the ‘inner approximation’ 
Uj (t) := {u ∈ U (t) : k(t, u) ≤ k(t, u(t)) ¯ + j, |f (t, x(t), u) ¯ |≤|˙
x¯| + j }.
Here, j is a positive integer. For each j , the special case of the theorem, already 
proved, can be applied. Validity of the assertions, in the absence of (A2), is now 
proved by passage to the limit. ⨅⨆
10.6 Maximum Principles for Free End-Time Problems with 
State Constraints 
We provide in this section extensions of the state constrained maximum principle, to 
allow for free end-times. As in our earlier treatment of free end-time free problems 
with free end-times in Chap. 9 (where state constraints were absent), the extra10.6 Maximum Principles for Free End-Time Problems with State Constraints 499
relations corresponding to the free end-time take the form of boundary conditions 
on the maximized Hamiltonian. 
Consider the following state constrained problem with free end-times: 
(F T )
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize g(S, x(S), T , x(T ))
over [S,T ] ⊂ R, x ∈ W1,1([S,T ]; Rn)
and measurable functions u : [S,T ] → Rm satisfying
x(t) ˙ = f (t, x(t), u(t)) a.e. t ∈ [S,T ],
u(t) ∈ U (t) a.e. t ∈ [S,T ],
h(t, x(t)) ≤ 0 for all t ∈ [S,T ],
(S, x(S), T , x(T )) ∈ C,
the data for which comprise: functions g : R × Rn × R × Rn → R, f : R ×
Rn × Rm → Rn and h : R × Rn → R, a multifunction U : R ⇝ Rm and a set 
C ⊂ R × Rn × R × Rn. 
A process is a triple ([S,T ], x, u) comprising an interval [S,T ],a 
W1,1([S,T ]; Rn) function x and a Lebesgue measurable function u : [S,T ] → Rm
such that x(t) ˙ = f (t, x(t), u(t)) and u(t) ∈ U (t), a.e. t ∈ [S,T ]. The process is 
said to be admissible if additionally the elements satisfy the constraints of problem 
(F T ). An admissible process ([S,¯ T¯], x,¯ u)¯ is called a W1,1 minimizer relative if 
there exists β > 0 such that 
g(S, x(S), T , x(T )) ≥ g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯
for all admissible processes ([S,T ], x, u) such that 
d(([S,T ], x), ([S,¯ T¯], x)) ¯ ≤ β .
Here, d is the distance function employed in Chap. 9 (cf.(9.1.4)), which can be 
written also as follows: 
d(([S,T ], x), ([S'
, T '
], x'
)) := |S − S'
|+|T − T '
|+|x(S) − x'
(S)|
+
 +∞
−∞
| ˙xe(t) − ˙x'
e(t)|dt.
(Here, and elsewhere, the subscript e attached to ‘state’ like variables such as x
indicates extension by constant extrapolation and, attached to ‘velocity’ related 
variables such as x˙ or u denotes the extension by zero.) 
Following the pattern of Chap. 9, we treat the Lipschitz time dependence and the 
measurable time dependence cases separately. Define the maximized Hamiltonian 
H (t, x, p) = sup
u∈U (t)
p · f (t, x, u) .500 10 The Maximum Principle for Problems with Pathwise Constraints
Theorem 10.6.1 (State Constrained, Free End-Time Problems with Lipschitz 
Time Dependence) Let ([S,¯ T¯], x,¯ u)¯ be a W1,1 local minimizer for (FT). Assume 
that, for some ϵ >¯ 0, the following hypotheses are satisfied: 
(H1): g is Lipschitz continuous on a neighbourhood of (S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ and C is 
a closed set, 
(H2): U (t) = U for all t ∈ R, for some Borel set U ⊂ Rm, 
(H3): For fixed x, f (., x, .) is L × Bm measurable. There exists a function kf :
[S,¯ T¯] × Rm → R such that t → k(t, u(t)) ¯ is integrable on [S,¯ T¯] and 
|f (t'', x'', u) − f (t'
, x'
, u)| ≤ kf (t, u) |(t'', x'') − (t'
, x'
)|
for all (t'', x''), (t'
, x'
) ∈ (t, x(t)) ¯ + ¯ϵB and u ∈ U, a.e. t ∈ [S,¯ T¯], 
(H4): h is upper semi-continuous and there exists a constant kh such that, for all 
t ∈ [S,¯ T¯], 
|h(t'
, x'
)−h(t'', x'')| ≤ kh|(t'
, x'
) − (t'', x'')|
for all (t'
, x'
), (t'', x'') ∈ (t, x(t)) ¯ + ¯ϵB .
Then there exist absolutely continuous arcs e ∈ W1,1([S,¯ T¯]; R) and p ∈
W1,1([S,¯ T¯]; Rn), λ ≥ 0, a measure μ ∈ C⊕(S,¯ T )¯ and a Borel measurable 
function γ = (γ0, γ1) : [S,¯ T¯] → R1+n, such that the following conditions, in 
which (r, q) ∈ NBV ([S,¯ T¯]; R1+n) is the function 
(−r(t), q(t)) := 
(−e(S), p( ¯ S)) ¯ if t = S¯
(−e(t), p(t)) + 
[S,t ¯ ] γ (s)dμ(s) if t ∈ (S,¯ T¯] , (10.6.1) 
are satisfied: 
(i) (p, μ, λ) /= (0, 0, 0),
(ii) (e(t), ˙ − ˙p(t)) ∈ co ∂t,x q(t) · f (t, x(t), ¯ u(t)) ¯ a.e. t ∈ [S,¯ T¯], 
(iii) q(t) · f (t, x(t), ¯ u(t)) ¯ ≥ q(t) · f (t, x(t), u), ¯ for all u ∈ U, a.e. t ∈[S,¯ T¯],
(iv) (−r(S), q( ¯ S), r( ¯ T ), ¯ −q(T )) ¯ ∈ λ∂g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ +NC˜(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ , 
(v) γ (t) ∈ ∂>h(t, x(t)) μ ¯ - a.e. t ∈ [S,¯ T¯], 
(vi) r(t) = H (t, x(t), q(t)) ¯ for a.e. t ∈ [S,¯ T¯], 
in which 
∂>h(t, x(t)) ¯
:= co lim sup {∂h(ti, xi) : ti → t,xi → ¯x(t) and h(ti, xi) > 0 for each i}.
The proof, which we omit, involves reformulating (FT) as a fixed time problem 
by means of a change of independent variable (as in the proof of Theorem 9.6.1). 
We then apply known necessary conditions to the fixed time problem (necessary10.6 Maximum Principles for Free End-Time Problems with State Constraints 501
conditions for state constrained problems Theorem 10.4.1 in this case) and interpret 
these conditions in terms of the data for the original problem. 
Remark 
The Autonomous Case: If f (t, x, u) and h(t, x) do not depend on t, then (10.6.1) 
and conditions (ii) and (v) in the theorem statement imply that r is a constant 
function and γ0 = 0. Condition (vi) then takes the form: there exists a number c
such that 
H (x(t), q(t)) ¯ = c, a.e. t ∈ [S,¯ T¯] .
If for admissible processes ([S,T ], x, u) either the initial time S is unconstrained 
and g does not depend on S, or the final time T is unconstrained and g does not 
depend on T , then the constant c = 0. 
Necessary conditions can also be derived for problems with measurably time 
dependent data. For such problems, the boundary condition on the maximized 
Hamiltonian is interpreted in the ‘sub and super essential value’ sense, introduced 
in Chap. 9. Recall that, given an integrable function a : [S,T ] → R and a point 
t ∈ (S, T ), we have (see Definition 9.3.1): 
(a): the sub essential value of f at t
¯ is the set 
sub-ess t→t
¯ a(t) := 
ζ ∈ R : ∃ti → t
¯ and ζi → ζ s.t.
lim sup
ϵ↓0
ϵ−1
 ti
ti−ϵ
a(s)ds ≤ ζi ≤ lim inf ϵ↓0
 ti+ϵ
ti
a(s)ds, for each i

.
(b): the super essential value of a at t is the set 
super-ess t→t
¯
a(t) := 
ζ ∈ R : ∃ti → t
¯ and ζi → ζ s.t.
lim sup
ϵ↓0
ϵ−1
 ti+ϵ
ti
a(s)ds ≤ ζi ≤ lim inf ϵ↓0
 ti
ti−ϵ
a(s)ds, for each i

.
Theorem 10.6.2 (The Free End-Time Maximum Principle: Measurable Time 
Dependence) Let ([S,¯ T¯], x,¯ u)¯ be a W1,1 local minimizer for (F T ) such that T¯ −
S >¯ 0. Assume that there exist ϵ > 0 and σ ∈ (0, (T¯ − S)/ ¯ 2) such that 
(H1): g is Lipschitz continuous on a neighbourhood of (S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ and C is 
a closed set, 
(H2): Gr U is a L × Bm measurable set, 
(H3): For each x ∈ Rn, f (., x, .) is L × Bm measurable. There exists a function 
kf : R × Rm → R such that t → kf (t, u(t)) ¯ is integrable on [S¯ − σ, T¯ + σ]
and502 10 The Maximum Principle for Problems with Pathwise Constraints
|f (t, x'
, u) − f (t, x, u)| ≤ kf (t, u)|x' − x|
for all x'
, x ∈ ¯xe(t) + ϵB and u ∈ U (t), a.e. t ∈ [S¯ − σ, T¯ + σ], 
(H4): There exists c¯ ≥ 0 such that, for a.e. t ∈ [S¯ − σ, S¯ + σ]∪[T¯ − σ, T¯ + σ], 
|f (t, x, u)|≤ ¯c
for all x ∈ ¯xe(t) + ϵB and u ∈ U (t), 
(H5): h is upper semi-continuous and there exists a constant kh such that, 
|h(t, x) − h(t, x'
)| ≤ kh|x − x'
|
for all x, x' ∈ ¯xe(t) + ϵB, for all t ∈ [S¯ − σ, T¯ + σ]. Furthermore 
h(S,¯ x(¯ S)) < ¯ 0 and h(T ,¯ x(¯ T )) < ¯ 0. (10.6.2) 
Then there exist p ∈ W1,1([S,¯ T¯]; Rn), λ ≥ 0, a measure μ ∈ C⊕(S,¯ T )¯ and a 
Borel measurable function γ : [S,¯ T¯] → Rn, such that the following conditions, in 
which q ∈ NBV ([S,¯ T¯]; Rn) is the function 
q(t) := 
p(S)¯ if t = S¯
p(t) + 
[S,t ¯ ] γ (s)dμ(s) if t ∈ (S,¯ T¯] , (10.6.3) 
are satisfied: 
(i) (p, μ, λ) /= (0, 0, 0),
(ii) − ˙p(t) ∈ co ∂x q(t) · f (t, x(t), ¯ u(t)) ¯ a.e. t ∈ [S,¯ T¯], 
(iii) (−ξ0,q(S), ξ ¯ 1, −q(T )) ¯ ∈ λ∂g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ + NC(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ , 
for some ξ0 ∈ sub-esst→S¯H (t, x(¯ S), p( ¯ S)) ¯
and ξ1 ∈ super-esst→T¯ H (t, x(¯ T ), p( ¯ T )) ¯ , 
(iv) q(t) · f (t, x(t), ¯ u(t)) ¯ ≥ q(t) · f (t, x(t), u), ¯
for all u ∈ U (t), a.e. t ∈ [S,¯ T¯], 
(v) γ (t) ∈ ∂>
x h(t, x(t)) μ ¯ - a.e. t ∈ [S,¯ T¯]. 
If the initial time S¯ is fixed, i.e. C = {(S, x ¯ 0,T,x1) : (x0,T,x1) ∈ C˜}, for some 
closed set C˜ ⊂ Rn × R × Rn, and g(S, x0,T,x1) = ˜g(x0,T,x1) for some g˜ :
Rn × R × Rn → R, then the above conditions are satisfied when (iii) is replaced 
by: there exists ξ1 ∈ super-esst→T¯ H (t, x(¯ T ), p( ¯ T )) ¯ such that 
(iii)'
: (p(S), ξ ¯ 1, −p(T )) ¯ ∈ λ∂g(˜ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ + NC˜(x(¯ S), ¯ T ,¯ x(¯ T )) ¯ , 
and when, in hypothesis (H4), the stated conditions are required to hold only for 
t ∈ [T¯ − σ, T¯ + σ], and (10.6.2) in (H5) is replaced by ‘h(T ,¯ x(¯ T )) < ¯ 0’ . The 
assertions of the theorem can be similarly modified when the final time T¯ is fixed.10.7 Non-degenerate Conditions 503
The proof of this theorem, which we omit, follows the pattern of its fixed 
end-times counterpart Theorem 10.4.1. The proof of this earlier stated theorem 
was based on an application of Clarke’s nonsmooth maximum principle (for fixed 
end-times) to a series of state constraint-free problems approximating the original 
problem, in which the state constrained is replaced by an integral penalty term, and 
passage to the limit. The difference now is that, at the start of the proof, we apply 
the free end-time necessary conditions of Theorem 9.6.2. The extra information 
provided by this theorem, expressed in terms of sub and super essential values of 
the maximized Hamiltonian, is preserved under limit taking and manifests itself as 
free end-time transversality condition (v) above. 
10.7 Non-degenerate Conditions 
We have seen how necessary conditions for state constraint-free problems can be 
adapted to a state constrained setting, by incorporating extra ‘measure multiplier’ 
terms associated with the state constraint. 
These necessary conditions provide useful information about minimizers in many 
cases. But there are cases of interest where they are unsatisfactory. In this section we 
elaborate on this point, and show how the necessary conditions can be supplemented 
to broaden their applicability. 
Consider, once again, the free end-time dynamic optimization problem with 
pathwise state constraints: 
(F T P )
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize g(S, x(S), T , x(T ))
over [S,T ] ⊂ R, x ∈ W1,1([S,T ]; Rn)
and measurable u : [S,T ] → Rm satisfying
x(t) ˙ = f (t, x(t), u(t)) a.e. t ∈ [S,T ],
u(t) ∈ U a.e. t ∈ [S,T ],
h(t, x(t)) ≤ 0 for all t ∈ [S,T ],
(S, x(S), T , x(T )) ∈ C,
the data for which comprise: an interval [S,T ], functions g : R × Rn × R × Rn →
R, f : R × Rn × Rm → Rn and h : R × Rn → R, and sets U ⊂ Rm and 
C ⊂ R × Rn × R × Rn. 
Subsequent analysis will address problem (F T P) in greater generality. But for 
clarity of exposition, we temporarily restrict attention to a special case in which 
f (t, x, u) and h(t, x) are independent of t. (Accordingly, we suppress t in the 
notation, writing f (x, u) for f (t, x, u) and h(x) for h(t, x)). We also assume that 
f (., u) (for all u ∈ Rm), g and h are continuously differentiable functions, f is 
continuous, the underlying time interval and the left end-point are fixed, i.e. 
C = {S¯}×{x0}×{T¯} × C1,504 10 The Maximum Principle for Problems with Pathwise Constraints
for some [S,¯ T¯] ⊂ R, x0 ∈ Rn (T >¯ S¯) and C1 ⊂ Rn, and g depends only on the 
right end-point of the state trajectory. 
Take a minimizer x¯ for (F T P). Theorem 10.4.1 provides necessary conditions 
of optimality. These assert the existence of an absolutely continuous arc p ∈
W1,1([S,¯ T¯]; Rn), λ ≥ 0, a measure μ ∈ C⊕(S,¯ T )¯ and r ∈ R such that the 
following conditions are satisfied: 
(i) (λ, p, μ) /= (0, 0, 0),
(ii) − ˙p(t) = (p(t) + 
[S,t ¯ ] hx (x(s))dμ(s)) ¯ · fx (x(t), ¯ u(t)) ¯ a.e. t ∈ [S,¯ T¯], 
(iii) u → (p(t) + 
[S,t ¯ ] hx (x(s))dμ(s)) ¯ · f (x(t), u) ¯ is maximized
over u ∈ U at u(t), ¯ a.e. t ∈ [S,¯ T¯], 
(iv) −(p(T )¯ + 
[S,¯ T¯] hx (x(s))dμ(s)) ¯ ∈ λgx (x(¯ T )) ¯ + NC1 (x(¯ T )) ¯ , 
(v) suppμ ⊂ {t : h(x(t)) ¯ = 0}, 
(vi) (p(t) + 
[S,t ¯ ] hx (x(s))dμ(s)) ¯ · f (x(t), ¯ u(t)) ¯ = r, for a.e. t ∈ (S,¯ T). ¯
Notice that (vi) is an ‘almost everywhere’ condition. A case when the deficiencies 
of these necessary conditions are particularly in evidence is that when the initial 
state is fixed and lies in the boundary of the state constraint region. To explore this 
phenomenon, we suppose that 
h(x0) = 0. (10.7.1) 
Here, we find that conditions (i)–(vi) are satisfied (for some p, λ and μ) when x¯ is 
any arc satisfying the constraints of (F T P). A possible choice of multipliers is 
(p ≡ −hx (S), μ ¯ = δ{S¯}, λ = 0) (10.7.2) 
(δ{S¯} denotes the unit measure concentrated at {S¯}.) Provided hx (S)¯ /= 0, these 
multipliers are non-zero. Condition (v) is satisfied, by (10.7.2). The remaining 
conditions (i)–(iv) and (vi) are satisfied since 

(S,t ¯ ]
hx (x(s))dμ(s) ¯ = 0 for t ∈ (S,¯ T ). ¯
The fact that the necessary conditions (i)–(vi) are automatically satisfied by all 
admissible arcs renders them useless as necessary conditions of optimality in the 
case (10.7.1). 
How should we deal with the degeneracy phenomenon? Extra necessary condi￾tions are clearly required, which (in the case (10.7.1)) eliminate the uninteresting 
multipliers (10.7.2). 
There are now a number of ways to do this. We focus on an extra condition to 
eliminate degeneracy, originating in the work of Arutyunov, Aseev and Blagodat￾Skikh [5]. In the fixed end-point, autonomous case now under consideration, this 
asserts that the constancy of the Hamiltonian condition (v) on the open interval 
(S,¯ T )¯ extends to the end-points, i.e. condition (vi) can be supplemented by10.7 Non-degenerate Conditions 505
sup
u∈U
p(S)¯ · f (x(¯ S), u) ¯ = r, (10.7.3) 
sup
u∈U
(p(T )¯ +

[S,¯ T¯]
hx (x(s))dμ(s)) ¯ · f (x(¯ T ), u)) ¯ = r,
where r is the constant of condition (vi). 
The significance of these conditions becomes evident when we impose the 
constraint qualification 
hx (x0) · f (x0, u) < 0 for some u ∈ U. (10.7.4) 
This is a condition on the data, requiring the existence of a control value driving the 
state into the interior of the state constraint set at x0. 
The constancy of the Hamiltonian condition, strengthened in this way, eliminates 
the degenerate multipliers (10.7.2). Indeed, for this choice of multipliers, p(t) +

[S,t ¯ ] hx (x(s))dμ(s) ≡ 0 for t ∈ (S,¯ T¯], so r = 0 by (vi). But then, in view 
of (10.7.3) and (10.7.4), we arrive at the contradiction: 
0 = sup
u∈U
p(S)¯ · f (x(¯ S), u) ¯ = − inf
u∈U
hx (x0) · f (x0, u) > 0.
The following theorem is the end result of elaborating these ideas, to allow for 
time dependent data and free end-times. 
Consider the problem (F T P). A process ([S,T ], x, u) comprises an interval 
[S,T ] ⊂ R (T >S), x ∈ W1,1([S,T ]; Rn) and a measurable function u : [S,T ] →
Rm such that x(t) ˙ ∈ f (t, x(t), u(t)) and u(t) ∈ U, a.e. t ∈ [S,T ]. The process is 
said to be admissible if (S, x(S), T , x(S)) ∈ C and h(t, x(t)) ≤ 0, for all t ∈ [S,T ]. 
An admissible process ([S,¯ T¯], x,¯ u)¯ is an L∞ local minimizer for (FTP) if there 
exists some β > 0 such that 
g(S, x(S), T , x(T )) ≥ g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯
for all admissible processes ([S,T ], x, u) such that 
|S − S¯|+|T − T¯| + ||xe − ¯xe||L∞ ≤ β.
Here, xe denotes the extension of x : [S,T ] → Rn, by constant extrapolation to the 
left and right. 
Write 
H (t, x, p) := sup
u∈U
p · f (t, x, u) (10.7.5) 
and 
C˜ := {(t0, x0, t1, xi) ∈ C : h(t0, x0) ≤ 0 and h(t1, x1) ≤ 0}.506 10 The Maximum Principle for Problems with Pathwise Constraints
Theorem 10.7.1 Let ([S,¯ T¯], x,¯ u)¯ be an L∞ local minimizer for (F T P). Assume 
that, for some ϵ > 0, 
(H1) g is Lipschitz continuous on a neighbourhood of (S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ and C is 
a closed set, 
(H2) U ⊂ Rm is a Borel measurable set, (t, u) → f (t, x, u) is L×Bm measurable 
for fixed x, and there exist kf > 0, cf > 0 such that 


|f (t'', x'', u) − f (t'
, x'
, u)| ≤ kf (|t'' − t'
|+|x'' − x'
|),
|f (t'
, x'
, u)| ≤ cf ,
for all u ∈ U, (t'', x''), (t'
, x'
) ∈ (t, x(t)) ¯ + ϵB and t ∈ [S,¯ T¯], 
(H3) there exists kh > 0 such that 
|h(t'', x'') − h(t'
, x'
)| ≤ kh(|(t'', x'') − (t'
, x'
)|),
for all (t'', x''), (t'
, x'
) ∈ (t, x(t)) ¯ + ϵB and for all t ∈ [S,¯ T¯] .
Then there exist absolutely continuous arcs e ∈ W1,1([S,¯ T¯]; R) and p ∈
W1,1([S,¯ T¯]; Rn), λ ≥ 0, a measure μ ∈ C⊕(S,¯ T )¯ and a Borel measurable 
function γ = (γ0, γ1) : [S,¯ T¯] → R1+n, such that the following conditions, in 
which (r, q) ∈ NBV ([S,T ]; R × Rn) is the function 
(−r(t), q(t)) := 
(−e(S), p( ¯ S)) ¯ if t = S¯
(−e(t), p(t)) + 
[S,t ¯ ] γ (s)dμ(s) if t ∈ (S,¯ T¯] , (10.7.6) 
are satisfied: 
(i) (p, μ, λ) /= (0, 0, 0),
(ii) (e(t), ˙ − ˙p(t)) ∈ co ∂t,x q(t) · f (t, x(t), ¯ u(t)) ¯ a.e. t ∈ [S,¯ T¯], 
(iii) q(t)· f (t, x(t), ¯ u(t)) ¯ ≥ q(t)· f (t, x(t), u), ¯ for all u ∈ U, a.e. t ∈ [S,¯ T¯],
(iv) (−r(S), q( ¯ S), r( ¯ T ), ¯ −q(T )) ¯ ∈ λ∂g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯
+ NC˜(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ , 
(v) γ (t) ∈ ∂>h(t, x(t)) μ ¯ - a.e. t ∈ [S,¯ T¯], 
(vi) r(S)¯ = H (S,¯ x(¯ S), q( ¯ S)) ¯ , 
(vii) r(t) = H (t, x(t), q(t)) ¯ for a.e. t ∈ (S,¯ T )¯ , 
(viii) r(T )¯ = H (T ,¯ x(¯ T ), q( ¯ T )) ¯ . 
Remarks 
(a): The ‘value added’ of this theorem, compared with earlier necessary conditions 
for problems with pure state constraints provided in Sect. 10.6 is, as anticipated 
in the introductory comments, the additional boundary conditions (vi) and (viii) 
on the maximized Hamiltonian. 
(b): The transversality condition (iv) is perhaps not quite what one might expect, 
because it is expressed in terms of a modified endpoint constraint set C˜, 
obtained by augmenting the original endpoint constraints by the pathwise state 
constraints at the end-times. This refinement is essential to the proof. Of course10.7 Non-degenerate Conditions 507
the transversality conditions involving C˜ and C coincide when either we are 
considering fixed end-point problems (C = {(S0, x0, T0, x1)}) or when the 
pathwise constraint is inactive at the optimal end-times. 
In the event we introduce a constraint qualification, regarding the interaction 
of the dynamics and the state constraint functional at the optimal end-times, the 
additional boundary conditions on the maximized Hamiltonian serve to strengthen 
the non-triviality condition (i) and thereby to eliminate the trivial set of Lagrange 
multipliers alluded to above. 
Corollary 10.7.2 Let ([S,¯ T¯], x,¯ u)¯ be an L∞ local minimizer for (F T P). Assume 
hypotheses (H1)–(H3) of Theorem 10.7.1. Assume also that the following constraint 
qualification is satisfied: 
γ '
0 + inf
u∈U γ '
1 · f (S,¯ x(¯ S), u) < ¯ 0 and γ ''
0 + sup
u∈U
γ ''
1 · f (T ,¯ x(¯ T ), u) > ¯ 0
(10.7.7) 
for all (γ '
0, γ '
1) ∈ ∂>h(S,¯ x(¯ S)) ¯ and all (γ ''
0 , γ ''
1 ) ∈ ∂>h(T ,¯ x(¯ T )) ¯ . 
Then there exist absolutely continuous arcs e ∈ W1,1([S,¯ T¯]; R) and p ∈
W1,1([S,¯ T¯]; Rn), λ ≥ 0, a measure μ ∈ C⊕(S,¯ T )¯ and a Borel measurable 
function γ = (γ0, γ1) : [S,¯ T¯] → R1+n such that conditions (ii)–(viii) of 
Theorem 10.7.1 are satisfied and also the following strengthened version of the non￾triviality condition (i): 
(i)' : λ + ||q||L∞ + 
(S,¯ T )¯ dμ(s) /= 0. 
Notice that condition (i)' of the corollary is not satisfied when (λ, p, μ) is chosen 
according to (10.7.2). Thus, the corollary provides hypotheses under which there 
exists another, ‘non-degenerate’, family of Lagrange multipliers, for any problem in 
which either of the terminal states is fixed and is located in the boundary of the state 
constraint set. 
Proof of Corollary 10.7.2 By Theorem 10.7.1 there exist multipliers (λ, p, μ), not 
all zero, and associated functions q and r, satisfying conditions (ii)–(viii). We 
must show that, under the constraint qualification (10.7.7), condition (i)' is also 
satisfied. Suppose, to the contrary, that (i)' fails to hold. Then λ = 0, q(s) =
0 for a.e. s ∈ (S,¯ T )¯ and there exist non-negative numbers α, β, not both zero, such 
that μ = αδ{S¯} + βδ{T¯}. We shall assume α > 0. (The case ‘β > 0’ is treated 
analogously.) 
Since q(t) = 0 a.e., we know that r(t) = H (t, x(t), q(t)) ¯ = 0 a.e.. Then, from 
(v) and (10.7.6), there exists (γ0, γ1) ∈ ∂>h(S,¯ x(¯ S)) ¯ such that 
r(S)¯ = +αγ0 and q(S)¯ = −αγ1 .
It follows then from condition (vi) that 
α γ0 = r(S)¯ = H (S,¯ x(¯ S), q( ¯ S)) ¯ = −α inf
u∈U γ1 · f (S,¯ x(¯ S), u) . ¯508 10 The Maximum Principle for Problems with Pathwise Constraints
Consequently, 
α(γ0 + inf
u∈U γ1 · f (S,¯ x(¯ S), u)) ¯ = 0.
But this relation contradicts the constraint qualification. The corollary is 
confirmed. ⨅⨆
Proof of Theorem 10.7.1 Let ϵ' > 0 be such that ([S,¯ T¯], x,¯ u)¯ is a minimizer for 
(F T P), with respect to arcs x satisfying 
|S − S¯|+|T − T¯|+‖xe − ¯xe‖L∞ ≤ ϵ'
.
Fix ρ ∈ (0, 1/2). Consider the ‘re-parameterized’ dynamic optimization problem 
(R)
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize g((τ, y)(s0), (τ, y)(s1))
subject to ([s0, s1], (τ, y), (v, w)) for the control system
⎧
⎨
⎩
y(s) ˙ = (1 + w(s))f (τ (s), y(s), v(s)) a.e. s ∈ [s0, s1],
τ (s) ˙ = 1 + w(s) a.e. s ∈ [s0, s1],
(v(s), w(s)) ∈ U × [−ρ,ρ] a.e. s ∈ [s0, s1],
subject to
h(τ (s), y(s)) ≤ 0 for all s ∈ [s0, s1],
((τ, y)(s0), (τ, y)(s1)) ∈ C,˜
|s0 − S¯|+|s1 − T¯|+‖ye − ¯xe‖L∞ ≤ ϵ'',
where ϵ'' ∈ (0, ϵ'
/2). Notice that the last constraint in problem (R) is inactive, with 
reference to any process ([s'
0, s'
1], y'
, u'
) such that |s
'
0 − S¯|+|s
'
1 − T¯| + ||y'
e −
x¯e||L∞ < ϵ'' and so the end-times s0 and s1 of the independent variable s are 
locally unconstrained. Subsequent analysis will exploit this crucial fact. Proof of the 
following lemma is along similar lines to that of Theorem 9.6.1 (and Lemma 9.7.1) 
and is therefore omitted. It is based on the change of independent variable ψ(s) =
S + 
[s0,s]
(1 + w(σ ))dσ, s ∈ [s0, s1] (for given s0, s1 and w : [s0, s1] → [−ρ,ρ]). 
Lemma 10.7.3 We can choose ρ ∈ (0, 1/2) and ϵ'' ∈ (0, ϵ'
/2) sufficiently small 
such that ([S,¯ T¯], (τ (s) ¯ ≡ s, y), ( ¯ u,¯ w¯ ≡ 0)) is an L∞ (global) minimizer for 
problem (R). 
Take ρi ↓ 0. For each i define 
gi(τ0, y0, τ1, y1) := (g(τ0, y0, τ1, y1) − g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ + ρ2
i ) ∨ 0 .
Consider the problem10.7 Non-degenerate Conditions 509
(Ri)
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize Ji(([s0, s1], (τ, y), (v, w))) := gi(τ (s0), y(s0), τ (s1), y(s1))
∨ max
s∈[s0,s1]
h(τ (s), y(s)))
over ([s0, s1], (τ, y), (v, w)) subject to
y(s) ˙ = (1 + w(s))f (τ (s), y(s), v(s)) a.e. s ∈ [s0, s1],
τ (s) ˙ = 1 + w(s) a.e. s ∈ [s0, s1],
(v(s), w(s)) ∈ U × [−ρ,ρ] a.e. s ∈ [s0, s1],
(τ (s0), y(s0), τ (s1), y(s1)) ∈ C,˜
|s0 − S¯|+|s1 − T¯|+‖ye − ¯xe‖L∞ ≤ ϵ'',
where ρ ∈ (0, 1/2) and ϵ'' ∈ (0, ϵ'
/2) are taken according to Lemma 10.7.3. Define 
M := {processes ([s0, s1], (τ, y), (v, w)) for (Ri)}.
Ji is a continuous function on the complete metric space (M, dM), when we 
define 
dM(([s'
0, s'
1], (τ '
, y'
), (v'
, w'
)), ([s0, s1], (τ, y), (v, w)))
:= |τ '
(s'
0) − τ (s0)|+|y'
(s'
0) − y(s0)|
+|s'
0 − s0|+|s'
1 − s1| + L − meas{s ∈ R : v'
e(s) /= ve(s)}+||w'
e − we||L1 .
Here, ve(s) := 

v(s) if s ∈ [s0, s1]
0 otherwise . v'
e(s), we(s), w'
e(s) are likewise defined. 
(Notice that a different extension convention is employed with the controls (u, w)
(‘zero extrapolation’) compared with the state (τ, y) ‘constant extrapolation’.) 
Clearly, ([S,¯ T¯], (τ (s) ¯ ≡ s, x), ( ¯ u,¯ w(s) ¯ ≡ 0)) is a ρ2
i -minimizer for problem 
(Ri). We can therefore conclude from Ekeland’s theorem that there exists ([s0i, s1i],
(τi, yi), (vi, wi)) ∈ M, which is a minimizer for 
Minimize {J '
i([s0, s1], (τ, y), (v, w)) : ([s0, s1], (τ, y), (v, w)) ∈ M}. (10.7.8) 
Here 
J '
i([s0, s1], (τ, y), (v, w)) := Ji([s0, s1], (τ, y), (v, w))
+ρidM(([s0, s1], (τ, y), (v, w)), ([s0i, s1i], (τi, yi), (vi, wi))).
Furthermore, 
dM(([s0i, s1i], (τi, yi), (vi, wi)), ([S,¯ T¯], (x,¯ τ (s) ¯ ≡ s), (v,¯ w¯ ≡ 0))) ≤ ρi
(10.7.9) 
and 
J '
i([s0i, s1i], (τi, yi), (vi, wi)) ≤ J '
i([S,¯ T¯], (x,¯ τ (s) ¯ ≡ s), (v,¯ w¯ ≡ 0)) .510 10 The Maximum Principle for Problems with Pathwise Constraints
Relation (10.7.9) implies 
|τi(s0i) − S¯|+|yi(s0i) − ¯x(S)¯ |+|s0i − S¯|+|s1i − T¯|
+ L − meas{s ∈ R : vie(s) = ¯ / ue(s)} + ||wi||L1 → 0 as i → ∞.
(10.7.10) 
We can deduce, with the help of Gronwall’s lemma, that 
||(τie, yie) − (τ¯e, x¯e)||L∞ → 0 as i → ∞ .
Note also that 
gi(τi(s0i), yi(s0i), τi(s1i), yi(s1i)) ∨ max t∈[s0i,s1i]
h(τi(t), yi(t))) > 0,
for all i sufficiently large. (10.7.11) 
This reason is that, if ‘=’ replaces ‘>’ in the above relation, then 
g(τi(s0i), yi(s0i), τi(s1i), yi(s1i)) ≤ g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ − ρ2
i ,
(τi(s0i), yi(s0i), τi(s1i), yi(s1i)) ∈ C˜ and h(τi(t), yi(t)) ≤ 0, for all t ∈ [s0i, s1i]. 
Furthermore, |s0i − S¯|+|s1i − T¯| + ||yie − ¯xe||L∞ ≤ ϵ''. This contradicts the 
optimality of ([S,¯ T¯], x,¯ u)¯ for problem (R). 
Observe next that ([s0i, s1i], (τi, yi), (vi, wi)) is a global minimizer for 
(R'
i)
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize J ''
i (([s0, s1], (τ, y), (v, w)))
subject to
y(s) ˙ = (1 + w(s))f (τ (s), y(s), v(s)) a.e. s ∈ [s0, s1],
τ (s) ˙ = 1 + w(s) a.e. s ∈ [s0, s1],
(v(s), w(s)) ∈ U × [−ρ,ρ] a.e. s ∈ [s0, s1],
(τ (s0), y(s0), τ (s1), y(s1)) ∈ C,˜
|s0 − S¯|+|s1 − T¯|+‖ye − ¯xe‖L∞ ≤ ϵ'',
in which 
J ''
i (([s0, s1], (τ, y), (v, w))):=gi(τ (s0), y(s0), τ (s1), y(s1)) ∨ max
s∈[s0,s1]
h(τ (s), y(s))
+ρi

|τ (s0) − τi(s0i)|+|y(s0) − yi(s0i)| + 4|s0 − s0i| + 4|s1 − s1i|
+  s1
s0 mi(s, v(s))ds +  s1
s0 |w(s) − wie(s)|ds
.
Here, mi(s, v) := 

0 if v = vie(s)
1 otherwise . Use of the cost function appearing in this 
problem formulation, in place of J '
i in problem (10.7.8), is justified by the following 
relations:10.7 Non-degenerate Conditions 511
||we − wie||L1 ≤
 s1
s0
|w(s) − wie(s)|ds + 2(|s0 − s0i|+|s1 − s1i|) (10.7.12) 
and 
L − meas{s ∈ R : ve(s) /= vie(s)} ≤  s1
s0
mi(s, v(s))ds + |s0 − s0i|+|s1 − s1i| .
(10.7.13) 
There are two cases to consider. 
Case 1: max s∈[s0i,s1i]
h(τi(s), yi(s)) > 0 for at most a finite number of index values i, 
Case 2: max s∈[s0i,s1i]
h(τi(s), yi(s)) > 0, for an infinite number of index values i. 
Consider first Case 1. We can arrange, by excluding initial terms in the sequence, 
that 
max s∈[s0i,s1i]
h(τi(s), yi(s)) ≤ 0 for all i .
Taking note of (10.7.11), we see that ([s0i, s1i], (τi, yi), (vi, wi)) is an L∞ mini￾mizer for 
(R
'
i)
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize J˜
i(([s0, s1], (τ, y), (v, w)))
subject to
y(s) ˙ = (1 + w(s))f (τ (s), y(s), v(s)) a.e. s ∈ [s0, s1],
τ (s) ˙ = 1 + w(s) a.e. s ∈ [s0, s1],
(v(s), w(s)) ∈ U × [−ρ,ρ] a.e. s ∈ [s0, s1],
(τ (s0), y(s0), τ (s1), y(s1)) ∈ C,˜
|s0 − S¯|+|s1 − T¯|+‖ye − ¯xe‖L∞ ≤ ϵ'',
in which 
J˜
i([s0, s1], (τ, y), (v, w)) := gi((τ (s0), y(s0), τ (s1), y(s1))
+ρi

|τ (s0) − τi(s0i)|+|y(s0) − yi(s0i)| + 4|s0 − s0i| + 4|s1 − s1i| +
 s1
s0
mi(s, v(s))ds +
 s1
s0
|w(s) − wie(s)|ds
.
From (10.7.11) and since h(τi(t), yi(t)) ≤ 0 for t ∈ [si, ti], 
gi(τi(s0i), yi(s0i), τi(s1i), yi(s1i))
= g(τi(s0i), yi(s0i), τi(s1i), yi(s1i)) − g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ + ρ2
i ) .
(10.7.14)512 10 The Maximum Principle for Problems with Pathwise Constraints
In view of (10.7.10), we can find a Lebesgue set T ⊂ (S,¯ T )¯ with the following 
properties. Following a further extraction of subsequences we have, for each s ∈ T , 
vi(s) = ¯u(s), −ρ<wi(s) < ρ for all i suff. large and wi(s) → 0, as i → ∞.
(10.7.15) 
For i sufficiently large, we may apply the free end-time maximum principle 
Theorem 9.6.2 to this problem. We may thereby conclude that there exist λi ≥ 0, 
ri ∈ W1,1([s0i, s1i]; R) and qi ∈ W1,1([s0i, s1i]; Rn) such that 
λi + ||qi||L∞ = 1, (10.7.16) 
(r˙i(s), − ˙qi(s)) ∈ co ∂t,x (qi(s) · f (τi(s), yi(s), vi(s)))(1 + wi(s)),
a.e. s ∈ [s0i, s1i], (10.7.17) 
(v, w) → (1 + w)(qi(s) · f (τi(s), yi(s), v) − ri(s))
− ρiλi(mi(s, v) + |w − wie(s)|)
is maximized over U × [−ρ , ρ] at (vi(s), wi(s)),
a.e. s ∈ [s0i, s1i], (10.7.18) 
(−ri(s0i), qi(s0i),ri(s1i), −qi(s1i)) ∈ λi∂gi(τi(s0i), yi(s0i), τi(s1i), yi(s1i))
+ NC˜(τi(s0i), yi(s0i), τi(s1i), yi(s1i)) + √
2λiρiB × {(0, 0)} (10.7.19) 
and 
sup 
(v,w)∈U×[−ρ ,ρ]
(qi(s) · f (τi(s), yi(s), v) − ri(s))(1 + w)
∈ ρiλi(5 + 2ρ)[−1, +1], for s = s0i and s1i . (10.7.20) 
Condition (10.7.20) results from the free end-time transversality condition for 
the optimal endtimes, when we take account of the fact the end-times s0 and 
s1 in problem (R
'
i) are locally unconstrained, with reference to the minimizer 
([s0i, s1i], (τi, yi), (vi, wi)), for i sufficiently large. Notice that the estimation of 
the essential values of the maximized Hamiltonian at the end-times takes account of 
the error terms involving mi(s, v) and |w − wie|. 
(10.7.20) implies 
sup
v∈U
qi(s) · f (τi(s), yi(s), v) − ri(s) ∈ ρiλi(5 + 2ρ)(1 − ρ)−1[−1, +1],
for s = s0i and s1i . (10.7.21) 
By removing a null set of points from T , we can arrange that conditions (10.7.17) 
and (10.7.18) are satisfied all points s ∈ T , i = 1, 2,...10.7 Non-degenerate Conditions 513
Take any s ∈ T . Considering first minimization w.r.t. the w variable and then the 
v variable, we deduce from (10.7.15) and (10.7.18) that, for i sufficiently large, 
qi(s) · f (τi(s), yi(s), vi(s)) − ri(s) ∈ ρiλiB (10.7.22) 
and 
qi(s) · f (τi(s), yi(s), vi(s)) ≥ sup
v∈U
qi(s) · f (τi(s), yi(s), v) − (1 − ρ)−1ρiλi .
(10.7.23) 
But then 
sup
v∈U
(qi(s) · f (τi(s), yi(s), v) − ri(s)) ∈ (2 − ρ)(1 − ρ)−1ρiλiB . (10.7.24) 
We deduce from (10.7.14), (10.7.19), properties of the distance function and the 
max rule for limiting subdifferentials that there exists λi ∈ [0, 1] such that 
(−ri(s0i), qi(s0i), ri(s1i), −qi(s1i)) ∈ λi∂g(τi(s0i), yi(s0i), τi(s1i), yi(s1i))
+ NC˜(τi(s0i), yi(s0i), τi(s1i), yi(s1i))
+ √
2ρiλiB × {(0, 0)}.
(10.7.25) 
Relations (10.7.16), (10.7.17), (10.7.21), (10.7.22), (10.7.24) and (10.7.25) will 
be recognized as perturbed versions of the desired necessary conditions, in which 
μi = 0 and therefore (ei, pi) = (ri, qi). After extracting a further subsequence, 
we are assured of the existence of q ∈ W1, r ∈ W1,1 and λ ∈ [0, 1] such that 
(rie, qie) → (re, qe) uniformly, (r˙ie, q˙ie) → (re, qe) weakly in L1 and λi → λ. We 
can now employ a standard convergence analysis to show that the asserted relations 
of the theorem are correct, with λ, q(= p) and r(= e) as above. 
We now turn to Case 2. We can arrange, by selecting a subsequence, that 
max s∈[s0i,s1i]
h(τi(s), yi(s)) > 0 , for all i . (10.7.26) 
Fix i. For arbitrary K > 0, consider the problem 
(RK
i )
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize J K
i ([s0, s1], (τ, y), (v, w), z)
over [s0, s1] ⊂ R, (τ, y) ∈ W1,1, measurable (v, w) and z ∈ R
satisfying
y(s) ˙ = (1 + w(s))f (τ (s), y(s), v(s)) a.e. s ∈ [s0, s1],
τ (s) ˙ = 1 + w(s) a.e. s ∈ [s0, s1],
(v(s), w(s)) ∈ U × [−ρ,ρ] a.e. s ∈ [s0, s1],
(τ (s0), y(s0), τ (s1), y(s1)) ∈ C,˜
|s0 − S¯|+|s1 − T¯|+‖ye − ¯xe‖L∞ ≤ ϵ'',514 10 The Maximum Principle for Problems with Pathwise Constraints
in which 
J K
i ([s0, s1], (τ, y), (v, w), z) := gi(τ (s0), y(s0), τ (s1), y(s1)) ∨ z
+ ρi

|τ (s0) − τi(s0i)|+|y(s0) − yi(s0i)| + 4|s0 − s0i| + 4|s1 − s1i|
+
 s1
s0
mi(s, v(s))ds +
 s1
s0
|w(s) − wie(s)|ds
+ K
 s1
s0
(h(τ (s), y(s)) − z) ∨ 0)
2ds .
Notice that, for each K and any ([s0, s1], (τ, y), (v, w)), we have 
J K
i

[s0, s1], (τ, y), (v, w), z = max
s∈[s0,s1]
h(τ (s), y(s))
= J
''
i ([s0, s1], (τ, y), (v, w)) .
This implies that, for each i, inf(RK
i ) ≤ inf(R'
i). According to the following 
lemma, the two infima coincide in the limit as K → ∞, however. 
Lemma 10.7.4 For i = 1, 2,..., 
lim
K→∞ inf(RK
i ) = inf(R'
i) .
Proof Fix i. Assume that the lemma is false. Since the function K → inf(RK
i ) is 
non-decreasing, we may deduce the existence of ρ >¯ 0 and sequences Kj ↑ ∞ and 
{([s
j
0 , sj
1 ], (τ j , yj ), (vj , wj )), zj } in M × R such that, for each j , 
gi(τ j (sj
0 ), yj (sj
0 ), τ j (sj
1 ), yj (sj
1 )) ∨ zj
+ρi

|τ j (sj
0 ) − τi(s0i)|+|yj (sj
0 ) − yi(s0i)| + 4|s
j
0 − s0i| + 4|s
j
1 − s1i|
+
 s
j
1
s
j
0
mi(s, vj (s))ds +
 s
j
1
s
j
0
|wj (s) − wie(s)|ds
+Kj
 s
j
1
s
j
0
((h(τ j (s), yj (s)) − zj ) ∨ 0)
2ds
< gi(τi(s0i), yi(s0i), τi(s1i), yi(s1i)) ∨ max s∈[s0i,s1i]
h(τi(s), yi(s)) − ¯ρ ,
for j = 1, 2,... (10.7.27)10.7 Non-degenerate Conditions 515
Notice, at the outset, that we can assume 
max
s∈[s
j
0 ,sj
1 ]
h(τ j (s), yj (s)) − zj ≥ 0 for all j . (10.7.28) 
To see this, suppose that, for some j , this condition is violated, i.e. max{ h(τ j (s),
yj (s)) : s ∈ [s
j
0 , sj
1 ]} < zj . Now replace zj by the lower number 
max{ h(τ j (s), yj (s)) : s ∈ [s
j
0 , sj
1 ]}. We see that, following this change, (10.7.28) 
is satisfied. Furthermore, the strict inequality (10.7.27) is preserved. This is because 
the right side of this relation is unaffected, while the left side can only be reduced 
by this change. We have confirmed that, without loss of generality, we may 
assume (10.7.28). Since the (τ j , yj )’s are uniformly bounded in L∞, the zj ’s 
are uniformly bounded above. It is also clear from the non-negativity of the first two 
terms and the structure of the integrand of the integral on the left side of (10.7.27) 
that the zj ’s are also bounded below. It follows that {zj } is a bounded sequence. 
Again, from the non-negativity of the first two terms and since Kj → ∞ as j → ∞, 
we deduce that 

[s
j
0 ,sj
1 ]
((h(τ j (s), yj (s)) − zj ) ∨ 0)
2ds → 0, as j → ∞. (10.7.29) 
Fix j . Let s¯ be a point at which (h(τ j (s), yj (s)) − zj ) achieves its maximum 
over [s
j
0 , sj
1 ]. Using the facts that the functions s → ((h(τ j (s), yj (s)) − zj ) ∨ 0)2
are uniformly Lipschitz continuous we can find α >¯ 0 independent of j such that 
((h(τ j (s), yj (s)) − zj ) ∨ 0)
2 ≥
1
2
((h(τ j (s), y ¯ j (s)) ¯ − zj ) ∨ 0)
2,
for all s ∈ [s
j
0 , sj
1 ] ∩ [¯s − ¯α,s¯ + ¯α] .
But then 
max
s∈[s
j
0 ,sj
1 ]
(h(τ j (s), yj (s)) − zj ) ∨ 0 = (h(τ j (s), y ¯ j (s)) ¯ − zj ) ∨ 0
≤ (2/α)¯ 1
2
 
[s
j
0 ,sj
1 ]
(h(τ j (t), yj (t)) − zj ) ∨ 0)
2dt1
2 .
It follows now from (10.7.28) and (10.7.29) that 
max
s∈[s
j
0 ,sj
1 ]
h(τ j (s), yj (s)) − zj → 0, as j → ∞.
Since z → gi(s0, x0, s1, x1) ∨ z is Lipschitz continuous (with Lipschitz constant 1) 
we deduce from this relation that, for j sufficiently large,516 10 The Maximum Principle for Problems with Pathwise Constraints
gi(τ j (sj
0 ), yj (sj
0 ), τ j (sj
1 ), yj (sj
1 )) ∨ max
s∈[s
j
0 ,sj
1 ]
h(τ j (s), yj (s))
+ ρi

|τ j (sj
0 ) − τ j (sj
0 )|+|yj (sj
0 ) − yj (sj
0 )| + 4|s
j
0 − s0i| + 4|s
j
1 − s1i|
+
 s
j
1
s
j
0
mi(s, vj (s))ds +
 s
j
1
s
j
0
|wj (s) − wie(s)|ds
< gi(τi(s0i), yi(s0i), τi(s1i), yi(s1i)) ∨ max s∈[s0i,s1i]
h(τi(s), yi(s)) − ¯ρ/2 .
(10.7.30) 
This contradicts the optimality of ([s0i, s1i], (τi, yi), (vi, wi)) for problem (R'
i). The 
lemma is proved. ⨅⨆
In view of (10.7.26), and the continuity of the map ([s0, s1], (τ, y), (v, w)) → (τ, y)
from M to L∞, we can find a sequence ρ'
i ↓ 0 such that, for each i, ρ'
i ≤ 1/2 and 
([s0, s1], (τ, y), (v, w)) ∈ M, z ∈ R
dM(([s0, s1], (τ, y), (v, w)), ([si0, si1], (τi, yi), (vi, wi))) ≤ ρ
'
i
|z − maxs∈[s0i,s1i] h(τi(s), yi(s))| ≤ ρ
'
i
⎫
⎬
⎭
(10.7.31)
=⇒ gi(τ (s0), y(s0), τ (s1), y(s1)) ∨ z > 0 and z > 0 .
According to the lemma, we can choose Ki ↑ ∞ such that, for each i, 
([si0, si1], (τi, yi), (vi, wi), zi := maxs∈[s0i,s1i] h(τi(s), yi(s))) is a ρ
'
2
i -minimizer 
for (P Ki
i ). Ekeland’s theorem, together with relations (10.7.12) and (10.7.13), then 
tell us that there exists (([s'
0i, s'
1i], (τ '
i, y'
i), (v'
i, w'
i)), z'
i) ∈ M × R that is an L∞
minimizer for: 
(Qi)
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize gi(τ (s0), y(s0), τ (s1), y(s1)) ∨ z(s1) + ρ'
i|z(s0) − z'
i|
+Ki
 s1
s0 ((h(τ (s), y(s)) − z(s)) ∨ 0)2ds
+ρi

|τ (s0) − τi(s0i)|+|y(s0) − yi(s0i)| + 4|s0 − s0i|
+4|s1 − s1i| +  s1
s0 mi(s, v(s))ds +  s1
s0 |w(s) − wie(s)|ds
+ρ'
i

|τ (s0) − τ '
i(s'
0i
)|+|y(s0) − y'
i(s'
0i)| + 4|s0 − s'
0i|
+4|s1 − s'
1i| +  s1
s0 m'
i(s, v(s))ds +  s1
s0 |w(s) − w'
ie(s)|ds
over [s0, s1] ⊂ R, (τ, y, z) ∈ W1,1(s0, s1), meas. (v, w) satisfying
y(s) ˙ = (1 + w(s))f (τ (s), y(s), v(s)) a.e. s ∈ [s0, s1],
τ (s) ˙ = 1 + w(s) a.e. s ∈ [s0, s1],
z(s) ˙ = 0 a.e. s ∈ [s0, s1],
(v(s), w(s)) ∈ U × [−ρ,ρ] a.e. s ∈ [s0, s1],
(τ (s0), y(s0), τ (s1), y(s1)) ∈ C,˜
|s0 − S¯|+|s1 − T¯|+‖ye − ¯xe‖L∞ ≤ ϵ'' .10.7 Non-degenerate Conditions 517
Here, m'
i(t, v) := 

0 if v = v'
ie(t)
1 otherwise
. Furthermore 
dM(([s'
0i, s'
1i], (τ '
i, y'
i), (v'
i, w'
i)), ([s0i, s1i], (τi, yi), (vi, wi)))
+|z'
i − max s∈[s0i,s1i]
h(τi(s), yi(s))| ≤ ρ'
i . (10.7.32) 
and so we have 
||(τ '
ie, y'
ie) − (τe, yie)||L∞ → 0, |z'
i − max s∈[s0i,s1i]
h(τi(s), yi(s))| → 0,
|s'
0i − s0i| → 0, |s'
1i − s1i| → 0
and L − meas{s ∈ R : v'
ie(s) /= vie(s)} → 0,
||w'
ie − wie||L1 → 0 . (10.7.33) 
This relation combines with (10.7.9) to tell us that, as i → ∞, 
||(τ '
ie, y'
ie) − (τ¯e, y¯ie)||L∞ → 0, z'
i → max
s∈[S,¯ T¯]
h(s, x(s)), s ¯ '
0i → S, s ¯ '
1i → T¯
and L − meas{s ∈ R : v'
ie(s) /= ¯ue(s)} → 0, ||w'
i||L1 → 0 .
It follows that, for a subsequence, w'
ie(s) → 0 a.e.. We deduce from (10.7.31) that 
gi(τ '
i(s'
0i), y'
i(s'
0i), τ '
i(s'
1i), y'
i(s'
1i)) ∨ z'
i > 0 and z'
i > 0 , for each i . (10.7.34) 
Ekeland’s theorem also tells us that 
Ki
 s'
1i
s'
0i
((h(τ '
i(s), y'
i(s)) − z'
i) ∨ 0)
2ds (+ non-negative terms)
≤ gi(τi(s0i), yi(s0i), τi(s1i), yi(s1i)) ∨ max s∈[s0i,s1i]
h(τi(s), yi(s))
+ρ'
i| max s∈[s0i,s1i]
h(τi(s), yi(s)) − z'
i|
+ρ'
i

|τi(s0i) − τ '
i(s'
0i)|+|yi(s0i) − y'
i(s'
0i)| + 4|s0i − s'
0i|
+4|s1i − s'
1i| +  s1i
s0i
m'
i(s, vi(s))ds +
 s1i
s0i
|wi(s) − w'
ie(s)|ds
.
The number on the right side of this relation has value 0, in the limit as i → ∞, by 
(10.7.33). It follows that 
Ki
 s'
1i
s'
0i
((h(τ '
i(s), y'
i(s)) − z'
i) ∨ 0)
2ds → 0, as i → ∞.518 10 The Maximum Principle for Problems with Pathwise Constraints
Now, for each i sufficiently large, apply the free end-time version of the maximum 
principle (Theorem 9.6.2) to the problem (Qi), with reference to the L∞ minimizer 
(([s'
0i, s'
1i], (τ '
i, y'
i), (v'
i, w'
i)), z'
i), noting that the relevant hypotheses are satisfied. 
We are assured then of the existence of λ˜i ∈ [0, 1] and also of elements −di ∈ W1,1, 
−ri ∈ W1,1 and qi ∈ W1,1 (interpreted as costate components associated with 
the state variable components z and τ and y variables respectively) and a Borel 
measurable function γi = (γ0i, γ1i) : [s'
0i
, s'
1i] → R1+n such that 
(λ˜i, qi) /= (0, 0), (10.7.35)
(r˙i(s), − ˙qi(s)) ∈ co ∂t,x (qi(s) · f (τ '
i(s), y'
i(s), v'
i(s)))(1 + w'
i(s))
−γi(s)(dμi/ds)(s), a.e. s ∈ [s
'
0i, s'
1i], (10.7.36)
γi(s) ∈ ∂h(τ '
i(s), y'
i(s)), μi − a.e. s ∈ [s
'
0i, s'
1i],
d˙
i(s) = (dμi/ds)(s), a.e. and di(s'
0i) ∈ ρ
'
iλ˜iB, (10.7.37)
(v, w) → (1 + w)(qi(s) · f (τ '
i(s), y'
i(s), v) − ri(s))
−ρiλ˜i(mi(s, v) + |w − wie(s)|) − ρ
'
iλ˜i(m'
i(s, v) + |w − w'
ie(s)|)
is maximized over U × [−ρ,ρ] at (v'
i(s), w'
i(s)), a.e. s ∈ [s
'
0i, s'
1i],(10.7.38)
((−ri(s'
0i), qi(s'
0i), ri(s'
1i), −qi(s'
1i)), di(s'
1i)) ∈
λ˜i∂

gi(τ '
i(s'
0i), y'
i(s'
0i
), τ '
i(s'
1i), yi(s'
1i)) ∨ z
'
i(s'
1i))
+(ξi, 0) + √
2(ρi + ρ
'
i)λ˜iB × {0},
for some ξi ∈ NC˜(τ '
i(s'
0i), y'
i(s'
0i), τ '
i(s'
1i), y'
i(s'
1i)), (10.7.39)
sup
(v,w)∈U×[−ρ ,ρ]
(qi(s) · f (τ '
i(s), y'
i(s), v)) − ri(s))(1 + w)
−λ˜iKi(h(τ '
i(s), y'
i(s)) − z
'
i(s)) ∨ 0)
2
∈ (ρi + ρ
'
i)λ˜i(5 + 2ρ)[−1, +1], for s = s
'
0i and s
'
1i. (10.7.40) 
Here μi is the (positive) measure on the Borel sets of the real line, with support in 
[s'
0i, s'
1i] and distribution: 
μi([s'
0i, s]) = 2Kiλ˜i

[s'
0i,s]
(h(τ '
i(σ ), y'
i(σ )) − z'
i(σ )) ∨ 0)dσ . (10.7.41) 
Clearly, 
supp {μi}⊂{s ∈ [s'
0i, s'
1i] : h(τ '
i(s), y'
i(s))) ≥ z'
i(s)}. (10.7.42)10.7 Non-degenerate Conditions 519
Relation (10.7.40) results from the free end-time transversality condition for the 
optimal end-times s'
0i and s'
1i in problem (Qi), when we take account of the 
fact the end-times are locally unconstrained, with reference to the minimizer 
(([s'
0i
, s'
1i], (τ '
i, y'
i), (v'
i, w'
i)), z'
i), for i sufficiently large. But Ki((h(τ '
i(s), y'
i(s)) −
z'
i(s)) ∨ 0)2 = 0 for s = s'
0i, s'
1i, since z'
i(s)(≡ z'
i) > 0 and h(τ '
i(s), y'
i(s)) ≤ 0 for 
s = s'
0i and s = s'
1i. (We use here the fact that, for elements (τ0, y0, τ1, y1) ∈ C˜, we 
know h(τ0, y0) ≤ 0 and h(τ1, y1) ≤ 0.) (10.7.40) therefore implies that, for s = s'
0i
and s'
1i, 
sup
v∈U
qi(s) · f (τ '
i(s), y'
i(s), v)) − ri(s) ∈ (ρi + ρ'
i)λ˜i
5 + 2ρ
1 − ρ

[−1, +1].
(10.7.43) 
We deduce from (10.7.31) and (10.7.39), with the help of the sum rule, that there 
exists νi ∈ [0, 1] such that 
(−ri(s'
0i), qi(s'
0i), ri(s'
1i), −qi(s'
1i)) ∈
λ˜iνi∂g(τ '
i(s'
0i), y'
i(s'
0i
), τ '
i(s'
1i), y'
i(s'
1i)) + ξi + √
2(ρi + ρ'
i)λ˜iB ,
di(s'
1i) = λ˜i(1 − νi) and 
||μi||T V =

[s'
0i,s'
1i]
(dμ/ds)ds ∈ λ˜i(1 − νi) + ρ'
iλ˜iB . (10.7.44) 
We can find a Lebesgue set S ⊂ (S,¯ T )¯ of full measure, with the following 
properties. Following a further extraction of subsequences we have: for each s ∈
S, (10.7.36) and (10.7.38) are valid for i sufficiently large and 
vi(s) = ¯u(s), −ρ<wi(s) < ρ for all i suff. large and wi(s) → 0, as i → ∞.
(10.7.45) 
Take any s ∈ S. Considering first minimization w.r.t. the w variable and then the v
variable, we deduce from (10.7.38) that, for i sufficiently large, 
qi(s) · f (τ '
i(s), y'
i(s), v'
i(s)) − ri(s) ∈ (ρi + ρ'
i)λ˜iB
and 
qi(s) · f (τ '
i(s), y'
i(s), v'
i(s)) ≥ sup
v∈U
qi(s) · f (τ '
i(s), y'
i(s), v))
− (1 − ρ)−1(ρi + ρ'
i)λ˜i . (10.7.46) 
But then 
sup
u∈U
qi(s)·f (τ '
i(s), y'
i(s), v)−ri(s) ∈ (2−ρ)(1−ρ)−1(ρi +ρ'
i)λ˜iB . (10.7.47)520 10 The Maximum Principle for Problems with Pathwise Constraints
Now define the function (ei, pi) ∈ W1,1([s'
0i, s'
1i]; R1+n) according to 
(−ei, pi)(s) := (−ri, qi)(s) −

[s'
0i,s]
γi(σ )(dμi(σ )/dσ )(σ )dσ for s ∈ [s'
0i, s'
1i] .
Condition (10.7.36) can be expressed in terms of (ei, pi): 
(ei, − ˙pi)(s) ∈ co ∂t,x ((−r, qi)(s) · f (τ '
i(s), y'
i(s), v'
i(s)))(1 + w'
i(s))
a.e. s ∈ [s'
0i, s'
1i] . (10.7.48) 
Notice next that (λ˜i, ξi) /= (0, 0). This is because, otherwise, q = 0, in 
consequence of (10.7.39), (10.7.36) and Gronwall’s lemma (Lemma 6.2.4): this 
contradicts (10.7.35). Now set λi := λ˜iνi. We see from (10.7.44) that ||μi||T V +
λi ≥ (1 − ρ'
i)λ˜i. It follows that (λi, ξi, μi) /= (0, 0, 0). Now normalize qi, pi, ξi, 
λi and λ˜i, dividing each by the positive number ||μi||T V + λi + |ξi|. (We do not 
re-label.) Then, automatically, ||μi||T V +λi +|ξi| = 1 . Observe also that, following 
relabelling, λ˜i ≤ (1 − ρ'
i)−1 ≤ 2, since ρ'
i ≤ 1/2 for each i. 
Assembling all these relations and recalling the definition of the maximized 
Hamiltonian, we arrive at the following perturbed versions of the desired conditions: 
for each i, 
||μi||T V + λi + |ξi| = 1 , (10.7.49) 
(ei, − ˙pi)(s) ∈ co ∂t,x ((−r, qi)(s) · f (τ '
i(s), y'
i(s), v'
i(s)))(1 + w'
i(s)),
a.e. s ∈ [s'
0i, s'
1i], (10.7.50) 
(−ri(s'
0i), qi(s'
0i),ri(s'
1i), −qi(s'
1i)) ∈
λi∂g(τ '
i(s'
0i), y'
i(s'
0i), τ '
i(s'
1i), y'
i(s'
1i) + ξi + √
2(ρi + ρ'
i)B, (10.7.51) 
ξi ∈ NC˜(τ '
i(s'
0i), y'
i(s'
0i), τ '
i(s'
1i), y'
i(s'
1i), (10.7.52) 
H (τ '
i(s), y'
i(s), qi(s)) − ri(s) ∈ (ρi + ρ'
i)
5 + 2ρ
1 − ρ

[−1, +1],
for s = s0i and s1i ,
(−ri, qi)(s) = (−ei, pi)(s) +

[s'
0i,s]
γi(σ )(dμi(σ )/dσ )(σ )dσ
for s ∈ [s'
0i, s'
1i] . (10.7.53) 
Also, for each s ∈ S and i sufficiently large: 
qi(s) · f (τ '
i(s), y'
i(s), v'
i(s)) ≥ sup
v∈U
qi(s) · f (τ '
i(s), y'
i(s), v))
−2(1 − ρ)−1(ρi + ρ'
i) , (10.7.54)10.8 Mixed Constraints 521
H (τ '
i(s), y'
i(s), qi(s))−ri(s) ∈ (4−2ρ)(1−ρ)−1(ρi+ρ'
i)B . (10.7.55) 
Relations (10.7.49)–(10.7.52) ensure that the sequences with elements μi, (ei, pi), 
γi, (e˙i, p˙i), λi and ξi, are uniformly bounded (w.r.t. the total variation norm, 
the uniform norm, the L1 norm and Euclidean norms, respectively). Observe that 
[s'
0i, s'
1i]⊂[S¯ − ϵ'', T¯ + ϵ''] for all i. Therefore, we can consider the extension of 
elements (ei, pi) and (e˙i, p˙i) on the compact interval [S¯ − ϵ'', T¯ + ϵ''] according 
to the preceding convention (by ‘constant extrapolation’ for the state arcs (ei, pi)’s 
and by ‘zero extrapolation’ for their velocities (ei, pi)’s). For each i (sufficiently 
large) we shall write μie the Borel measure on [S¯ − ϵ'', T¯ + ϵ''] defined by 
μie(E) := μi(E ∩ [S¯ − ϵ'', T¯ + ϵ'']),
for all Borel subsets E ⊂ [S¯ − ϵ'', T¯ + ϵ'']. The Borel measurable functions γi’s 
are extended by ‘constant extrapolation’ on [S¯ − ϵ'', T¯ + ϵ'']. Consequently their 
extensions satisfy the property 
γie(s) ∈ ∂h(τ '
ie(s), y'
ie(s)), μie − a.e. s ∈ [S¯ − ϵ'', T¯ + ϵ''] .
It follows that, after subsequences have been extracted, there exist limit points μ, 
e, e˙, p, p˙, γ , λ and ξ with respect to the appropriate topologies (the limit measure 
and functions are eventually considered restricted to [S,¯ T¯]). But then there exists 
a function of bounded variation (r, q) such that (rie, qie)(s) → (re, pe)(s) on a 
subset of [S,¯ T¯] of full Lebesgue measure, including the endpoints S¯ and T¯. (The 
subscript ‘e’ here means ‘extension by constant extrapolation’.) We can also arrange 
that ξi → ξ , for some ξ ∈ N˜C(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ . We deduce finally, with the help 
of a standard convergence analysis, the validity of all the assertions in the theorem 
statement, by passing to the limit in (10.7.49)–(10.7.55) as i → ∞. Notice, in 
particular that ||μ||T V + λ + |ξ | = 1. This implies the non-triviality condition 
(λ, p, μ) /= (0, 0, 0) since ‘(λ, p, μ) = (0, 0, 0)’ implies ξ /= 0 and hence p /= 0, 
which is not possible. To arrive at condition (v) of the theorem statement we have 
used Proposition 10.3.1 identifying the sets Ai and A of this proposition as follows 
Ai = {(t, (ζ0, ζ1)) : (ζ0, ζ1) ∈ ∂h(τ '
ie(t), y'
ie(t)), t ∈ [S¯ − ϵ'', T¯ + ϵ'']}
and 
A = {(t, (ζ0, ζ1)) : (ζ0, ζ1) ∈ ∂>h(t, x¯e(t)), t ∈ [S¯ − ϵ'', T¯ + ϵ'']}. ⨅⨆
10.8 Mixed Constraints 
The necessary conditions of optimality derived in Chap. 7 cover problems involving 
a pathwise constraint on control functions. In this chapter we have extended 
these necessary conditions to allow for an addition pathwise constraint on state522 10 The Maximum Principle for Problems with Pathwise Constraints
trajectories, ‘h(t, x(t)) ≤ 0’. But dynamic optimization problems of interest arise, 
with pathwise constraints on control and state variables, which do not separate out 
into distinct pathwise constraints for each of the variables. Such problems are called 
‘mixed constraint’ problems. An important example is the case when the control 
constraint set is state dependent; here the control constraint u(t) ∈ U (t) of the 
dynamic optimization problem treated in Chap. 7 is replaced by u(t) ∈ U (t, x(t))
where, now, the control constraint set at time t, U (t, x), is x dependent. Such 
constraints arise in a number of application areas, including aeronautical control, 
where the thrust may be a control variable and the upper limit on the thrust will 
depend on altitude, a state variable component. In this section we provide necessary 
conditions of optimality for problems subject to a rather general form of mixed 
constraint and then explore the special form these conditions take in a classical 
setting, where the pathwise constraints are a collection of inequality and equality 
constraints on the mixed variables. 
Our approach to deriving necessary conditions for dynamic optimization prob￾lems with mixed constraints will be to reformulate the problem as one in which the 
dynamic constraint is a differential inclusion with velocity set F (t, x), obtained by 
combining the controlled differential equation with the mixed constraint. Necessary 
conditions for the mixed constraint problem can then be obtained by applying 
the Euler Lagrange-type conditions of Chap. 8 to the reformulated problem. This 
approach requires that F (t, x) has the requisite regularity properties to make 
possible this application of the Euler Lagrange-type conditions. What hypotheses 
must be imposed, when the mixed constraint arises from a state-dependent control 
constraint set, u ∈ U (t, x)? The Euler Lagrange-type conditions are valid under 
hypotheses that include ‘F (t, .) has bounded slope’ or, equivalently, ‘F (t, .) is 
pseudo-Lipschitz continuous’ (see Proposition 8.2.3). We can arrange that the 
multifunction F in the reformulation of the mixed constraint problem satisfies this 
hypothesis by imposing conditions on the data of the mixed constraint problem that 
include: 
U (t, .) has the bounded slope property .
In what follows, we will consider different formulations of the mixed constraint and 
the precise nature of the bounded slope hypothesis imposed will vary according to 
our choice. This is because we prefer to impose hypotheses directly on the data in 
terms of which the mixed constraint is formulated rather than on the corresponding 
state dependent control constraint set, which is only implicitly defined. 
Notice that the ‘bounded slope’ hypotheses will not be satisfied by the data of the 
problem posed in Sect. 10.2, involving a separated ‘pure’ state constraint h(t, x) ≤ 0
and control constraint u ∈ U (t). This is because the corresponding state dependent 
control constraint set is 
U (t, x) := 

U (t) if h(t, x)) ≤ 0
∅ if h(t, x)) > 0 .10.8 Mixed Constraints 523
The multifunction U (t, .) will be discontinuous at points in the boundary of the 
state constraint set A(t) := {x ∈ Rn : h(t, x) ≤ 0}, so it will certainly not 
satisfy the pseudo Lipschitz condition, or the equivalent bounded slope condition. 
The essential difference between problems involving a pure state constraint and 
those with mixed constraints treated in this chapter is reflected in the nature of the 
necessary conditions: for pure constraints these involve ‘true’ costate trajectories q
that are discontinuous functions of bounded variation, whereas costate trajectories 
for mixed constraint problems, derived below, are absolutely continuous. 
Consider the following problem, in which the mixed constraint has the general 
formulation: 
φ(t, x(t), u(t)) ∈ Ф(t) and u(t) ∈ U (t) a.e. t ∈ [S,T ] , (10.8.1) 
for a given function φ and a given multifunction Ф. 
(G)
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize g(x(S), x(T ))
over x ∈ W1,1([S,T ]; Rn) and measurable functions u : [S,T ] → Rm
such that
x(t) ˙ = f (t, x(t), u(t)) a.e. t ∈ [S,T ],
φ(t, x(t), u(t)) ∈ Ф(t) and u(t) ∈ U (t) a.e. t ∈ [S,T ]
(x(S), x(T )) ∈ C ,
the data for which comprise an interval [S,T ], integers n > 0, m > 0, functions 
g : Rn × Rn → R, f : [S,T ] × Rn × Rm → Rn and φ : [S,T ] × Rn × Rm → Rκ , 
a set C ⊂ Rn × Rn and multifunctions U : [S,T ] ⇝ Rm and Ф : [S,T ] ⇝ Rκ . 
An admissible process for (G) is a pair of functions (x, u) satisfying the specified 
constraints. We say that (x,¯ u)¯ is a W1,1 local minimizer, when there exists β > 0
such that (x,¯ u)¯ is minimizing over all admissible processes (x, u) such that ||x −
x¯||W1,1 ≤ β. 
For each time t, we define the set of admissible (x, u) pairs, S(t) ⊂ Rn × Rm to 
be 
S(t) := {(x, u) ∈ Rn × Rm) : φ(t, x, u) ∈ Ф(t) and u ∈ U (t)}. (10.8.2) 
We also define Sϵ,R(t), the localization of S(t) about (x(t), ¯ u(t)) ¯ , in which (x,¯ u)¯ is 
a nominal admissible process and ϵ > 0 and R > 0 are specified parameters: 
Sϵ,R(t) := {(x, u) ∈ S(t) : |x − ¯x(t)| ≤ ϵ and |u − ¯u(t)| ≤ R}.
The set of admissible controls at state x(t) ¯ , Ω(t), is the set 
Ω(t) = {u ∈ Rm : (x(t), u) ¯ ∈ S(t)}.524 10 The Maximum Principle for Problems with Pathwise Constraints
For purposes of formulating the appropriate ‘bounded slope’ hypo thesis to derive 
necessary conditions for problem (G), we introduce the following ‘bounded slope’ 
condition: 
(BS)K
t,x,u :
λ ∈ NФ(t)(φ(t, x, u)) and
(α, β) ∈ ∂(λ · φ(t, ., .))(x, u) + {0} × NU (t)(u)
=⇒ |λ| ≤ K|β| ,
for given K > 0 and (t, x, u) ∈ [S,T ] × Rn × Rm such that (x, u) ∈ S(t). 
The necessary conditions will include an unrestrictive form of the Weierstrass 
condition, expressed in terms of a subset Ω0(t) ⊂ Ω(t). Ω0(t), the set of regular 
admissible control values at x(t) ¯ , is defined to be 
Ω0(t) := {u ∈ Rm : (x(t), u) ¯ ∈ S(t) and there exists ρ > 0 such that
(BS)K=ρ−1
t,x'
,u' is satisfied for all points (x'
, u'
) in a neighbourhood of
S(t) ∩

(x(t), u) ¯ + ρB × ρB

relative to S(t)}. (10.8.3) 
Theorem 10.8.1 (Necessary Conditions for a General Problem with Mixed 
Constraints) Let (x,¯ u)¯ be a W1,1 local minimizer for (G). Assume that, for some 
ϵ > 0, r0 > 0 and measurable function R : [S,T ] → R ∪ {+∞}, such that 
R(t) > r0 for a.e. t ∈ [S,T ], the following hypotheses are satisfied 
(H1): g is Lipschitz continuous on (x(S), ¯ x(T )) ¯ + ϵ(B × B) and C is closed, 
(H2): for each x ∈ Rn, f (., x, .) is L × Bm measurable; U is a measurable 
multifunction that takes values non-empty, closed sets, for a.e. t ∈ [S,T ], 
f (t, ., .) is locally Lipschitz continuous and there exist measurable functions 
k
f
x and k
f
u such that, for a.e. t ∈ [S,T ], 
|f (t, x1, u1) − f (t, x2, u2)| ≤ k
f
x (t)|x1 − x2| + k
f
u (t)|u1 − u2|
for all (x1, u1) and (x2, u2) in a neighbourhood of Sϵ,R(t)(t), 
(H3): Ф is a Lebesgue measurable multifunction taking values closed, non-empty 
subsets of Rκ ; for each x ∈ Rn, φ(., x, .) is L × Bm measurable; for a.e. t ∈
[S,T ], φ(t, ., .) is locally Lipschitz continuous and there exist measurable 
functions k
φ
x and k
φ
u such that, for a.e. t ∈ [S,T ], 
|φ(t, x1, u1) − φ(t, x2, u2)| ≤ kφ
x (t)|x1 − x2| + kφ
u (t)|u1 − u2|
for all (x1, u1) and (x2, u2) in a neighbourhood of Sϵ,R(t)(t), 
(BS): There exists a measurable function K : [S,T ]→R such that, for a.e. 
t ∈ [S,T ], condition (BS)K(t)
t,x,u is satisfied for all (x, u) in a neighbourhood 
of Sϵ,R(t)(t) relative to S(t). 
Assume also the compatibility condition: for some γ > 0,10.8 Mixed Constraints 525
(C): k
f
x and t → K(t)kφ
x (t)kf
u (t) are integrable, and K(t)kφ
x (t) > 0 and 
R(t) ≥ K(t)kφ
x (t)γ , a.e.. 
Then there exist p ∈ W1,1([S,T ]; Rn) and λ0 ≥ 0 such that 
(a): (p, λ0) /= (0, 0), 
(b): (− ˙p(t), 0) ∈ co ∂x,u(p(t) · f (t, x(t), ¯ u(t))) ¯
−co {ξ ∈ ∂x,u(λ · φ(t, x(t), ¯ u(t))) ¯ : λ ∈ NФ(t)(φ(t, x(t), ¯ u(t))) ¯ } −
{0} × co NU (t)(u(t)) ¯ a.e. t ∈ [S,T ], 
(c): (p(S), −p(T )) ∈ λ0∂g(x(S), ¯ x(T )) ¯ + NC(x(S), ¯ x(T )) ¯ , 
(d): p(t)·f (t, x(t), ¯ u(t)) ¯ ≥ p(t)·f (t, x(t), u) ¯ for all u ∈ Ω0(t), a.e. t ∈ [S,T ]. 
If in addition φ(t, ., .) is continuously differentiable near (x(t), ¯ u(t)) ¯ a.e. t ∈ [S,T ], 
then condition (b) can be replaced by 
(b) '
: (− ˙p(t), 0) ∈ co ∂{p(t) · f (t, ., .) − λ(t) · φ(t, ., .)}(x(t), ¯ u(t)) ¯
−{0} × co NU (t)(u(t)) ¯ a.e., 
in which λ : [S,T ] → Rκ is an integrable function that satisfies 
λ(t) ∈ co NФ(t)(φ(t, x(t), ¯ u(t))) ¯ a.e.. (10.8.4) 
Furthermore, for K as in (BS), 
|λ(t)| ≤ K(t)kφ
x (t)kf
u (t)|p(t)| a.e..
A proof of Theorem 10.8.1 is given below in this section. 
The version of the mixed constraint that has received most attention is a 
constraint that is a combination of inequality and equality functional constraints 
imposed on values of processes (x, u), together with a general set constraint on the 
values of the control function u: 
h(1)
(t, x(t), u(t)) ≤ 0 and h(2)
(t, x(t), u(t)) = 0 and u(t) ∈ U (t), a.e. t ∈ [S,T ] ,
for given functions h(1) and h(2) and a given multifunction U. The problem now of 
interest is: 
(M)
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize g(x(S), x(T ))
over x ∈ W1,1([S,T ]; Rn) and measurable functions u : [S,T ] → Rm
such that
x(t) ˙ = f (t, x(t), u(t)), a.e. t ∈ [S,T ],
h(1)
(t, x(t), u(t)) ≤ 0 and h(2)
(t, x(t), u(t)) = 0 and u(t) ∈ U (t),
a.e. t ∈ [S,T ],
(x(S), x(T )) ∈ C ,
the data for which comprise an interval [S,T ], integers n > 0, m > 0, functions 
g : Rn × Rn → R, f : [S,T ] × Rn × Rm → Rn, h(1) : [S,T ] × Rn × Rm → Rκ1526 10 The Maximum Principle for Problems with Pathwise Constraints
and h(2) : [S,T ] × Rn × Rm → Rκ2 , a set C ⊂ Rn × Rn and a multifunction 
U : [S,T ] ⇝ Rm taking values non-empty, closed sets. 
The next theorem provides necessary conditions for problem (M). The assertions 
of this theorem can be deduced directly from Theorem 10.8.1, by making the 
following identifications 
Ф(t) =
κ1    (−∞, 0] × ... × (−∞, 0] ×
κ2    {0} × ... × {0} and φ = (h(1)
, h(2)
)
(10.8.5) 
and noting that, with these choices for Ф and φ, 
NФ(t)((h(1)
, h(2)
)(t, x, u)) = {(λ1, λ2) ∈ (R+)
κ1 × Rκ2 : λ1 · h(1)
(t, x, u) = 0},
for any (t, x, u) ∈ [S,T ]×Rn×Rm such that h(1)
(t, x, u) ≤ 0 and h(2)
(t, x, u) = 0. 
As we pass from problem (G) to problem (M), we will continue to employ the 
notation S(t), Sϵ,R(t), Ω(t) and its subset Ω0(t), when we identify Ф(t) and φ(t)
according to (10.8.5). For problem (M) then, 
S(t) = {(x, u) : h(1)
(t, x, u) ≤ 0, h(2)
(t, x, u) = 0, u ∈ U (t)},
Sϵ,R(t) = {(x, u) : h(1)
(t, x, u) ≤ 0, h(2)
(t, x, u) = 0, u ∈ U (t),
|x − ¯x(t)| ≤ ϵ, |u − ¯u(t)| ≤ R},
Ω(t) = {u ∈ Rm : (x(t), u) ¯ ∈ S(t)}.
The condition (BS)K
t,x,u, earlier employed to formulate the ‘bounded slope’ hypoth￾esis associated with the necessary conditions for the general problem (G), is now 
replaced by: 
(BS∗)K
t,x,u : 
λ1 ∈ (R+)κ1 , λ2 ∈ Rκ1 ,
λ1 · h(1)
(t, x, u) = 0, η ∈ NU (t)(u)
=⇒ |(λ1, λ2)| ≤ K|∇u(λ1 · h(1) + λ2 · h(2)
)(t, x, u) + η| ,
in which (t, x, u) is a specified point in [S,T ] × Rn × Rm such that (x, u) ∈ S(t). 
For each t ∈ [S,T ], the regular control constraint set Ω0(t) at x(t) ¯ , correspond￾ing to this bounded slope condition is: 
Ω∗
0(t) := {u ∈ Rm : (x(t), u) ¯ ∈ S(t) and there exists ρ > 0 such that
(BS∗)
K=ρ−1
t,x'
,u' is satisfied for all points (x'
, u'
) in a neighbourhood of
S(t) ∩

(x(t), u) ¯ + ρB × ρB

relative to S(t)}.
Theorem 10.8.2 (Mixed Functional Constraints: I) Let (x,¯ u)¯ be a W1,1 local 
minimizer for (M). Assume, for some ϵ > 0 and r0 > 0 and measurable function10.8 Mixed Constraints 527
R : [S,T ] → Rm ∪ {+∞}, such that R(t) > r0 for all t ∈ [S,T ], the following 
hypotheses are satisfied. 
(H1): g is Lipschitz continuous on (x(S), ¯ x(T )) ¯ + ϵ(B × B) and C is closed, 
(H2): for each x ∈ Rn, f (., x, .) is L × Bm measurable; U is a measurable 
multifunction that takes values closed, non-empty sets; for a.e. t ∈ [S,T ], 
f (t, ., .) is locally Lipschitz continuous and there exist measurable functions 
k
f
x and k
f
u such that, for a.e. t ∈ [S,T ], 
|f (t, x1, u1) − f (t, x2, u2)| ≤ k
f
x (t)|x1 − x2| + k
f
u (t)|u1 − u2|
for all (x1, u1) and (x2, u2) in a neighbourhood of Sϵ,R(t)(t), 
(H3): for each x ∈ Rn, h(1)
(., x, .) and h(2)
(., x, .) are L × Bm measurable; for 
a.e. t ∈ [S,T ], h(1)
(t, ., .) and h(2)
(t, ., .) are continuously differentiable and 
there exist measurable functions kh
x and kh
u such that, for a.e. t ∈ [S,T ], 
|(h(1)
, h2)(t, x1, u1) − (h(1)
, h2)(t, x2, u2)|
≤ kh
x (t)|x1 − x2| + kh
u(t)|u1 − u2|
for all (x1, u1) and (x2, u2) in a neighbourhood of Sϵ,R(t)(t), 
(BS∗): there exists a measurable function K : [S,T ] → R such that, for a.e. 
t ∈ [S,T ], 
(BS∗)
K(t)
t,x,u is satisfied for all (x, u) in a neighbourhood of Sϵ,R(t)(t). 
Assume also that, for some γ > 0, 
(C): k
f
x and t → K(t) kh
x (t) kf
u (t) are integrable, and K(t)kh
x (t) > 0 and 
R(t) ≥ K(t)kh
x (t)γ , a.e.. 
Then there exist p ∈ W1,1([S,T ]; Rn), λ0 ≥ 0 and integrable functions λ1 :
[S,T ] → (R+)κ1 and λ2 : [S,T ] → Rκ2 such that 
λ1(t) · h(1)
(t, x(t), ¯ u(t)) ¯ = 0, a.e.
and 
(a): (p, λ0) /= (0, 0), 
(b): (− ˙p(t), 0) ∈ co ∂x,u(p(t) · f (t, x(t), ¯ u(t))) ¯ − {0} × co NU (t)(u(t)) ¯
−λ1(t)·∇x,uh(1)
(t, x(t), ¯ u(t)) ¯ −λ2(t)·∇x,uh(2)
(t, x(t), ¯ u(t)) ¯
a.e. t ∈ [S,T ], 
(c): (p(S), −p(T )) ∈ λ0∂g(x(S), ¯ x(T )) ¯ + NC(x(S), ¯ x(T )) ¯ , 
(d): p(t) · f (t, x,¯ u(t)) ¯ ≥ p(t) · f (t, x, u) ¯ for all u ∈ Ω∗
0(t), a.e. t ∈ [S,T ].
Furthermore, for K as in (BS∗), 
|(λ1(t), λ2(t))| ≤ K(t)kh
x (t)kf
u (t)|p(t)| a.e..528 10 The Maximum Principle for Problems with Pathwise Constraints
Remarks 
(i): Notice that, when the data is continuously differentiable w.r.t x, condition (b) 
above reduces to 
− ˙p(t) = ∇xHλ1,λ2
(t, x(t), ¯ u(t), p(t)) ¯ a.e
0 ∈ ∇uHλ1,λ2
(t, x(t), ¯ u(t), p(t)) ¯ − co NU (t)(u(t)) ¯ a.e.,
in which 
Hλ1,λ2
(t, x, u, p) := p · f (t, x, u) − λ1(t) · h(1)
(t, x(t) ¯ u(t)) ¯
− λ2(t) · h(2)
(t, x(t) ¯ u(t)) . ¯
These relations will be recognized as the standard costate equation for the 
costate function coupled with a weak (local) version of the Weierstrass 
condition, expressed in terms of an augmented Hamiltonian with Lagrange 
multiplier terms to take account of the pathwise inequality and equality 
constraints. The functions λ1 and λ2 have the interpretation of Lagrange mul￾tipliers for the two constraints. Further information of Weierstrass condition 
type is supplied by condition (d) in the corollary statement. 
(ii): Of course, if hypotheses (H2), (H3) and (BS∗) are strengthened to require that 
all the conditions of (BS∗) are satisfied for every (t, x, u) such that (x, u) ∈
S(t), |x − ¯x(t)| ≤ ϵ (that is, we remove the condition ‘|u − ¯u(t)| ≤ R(t)’), 
then the set Ω0(t) coincides with Ω(t) and the Weierstrass condition (e) is 
valid with Ω(t) replacing Ω0(t). 
This theorem can be used as a starting point for deriving necessary conditions of 
optimality for problems with mixed pathwise constraints, under different hypothe￾ses. We provide just one example of a variant on Theorem 10.8.2, in which we 
replace the bounded slope condition (BS∗)K
t,x,u of Theorem 10.8.2 by the simpler, 
and less restrictive condition: 
(BS∗∗)t,x,u : 
(λ1, λ2) ∈ (R+)κ1 × Rκ2 , λ1 · h(1)
(t, x, u) = 0, ν ∈ NU (u)
∇u(λ1 · h(1) + λ2 · h(2)
)(t, x, u) + ν = 0

=⇒ (λ1, λ2) = (0, 0), 
while imposing additional hypotheses on the regularity of the mixed constraint 
functions, the control constraint set and the nominal control function. The condition 
in (BS∗∗)t,x,u is frequently encountered in the field of nonlinear programming 
(NLP) in connection the constraint set 
Q(t, x) := {u ∈ Rm : h(1)
(t, x, u) ≤ 0 and h(2)
(t, x, u) = 0 and u ∈ U (t),10.8 Mixed Constraints 529
for fixed (t, x), where it is referred to as the Magasarian Fromowitz condition; in the 
(NLP) context, it has a role as a sufficient condition ensuring regularity properties 
of the multifunction (t, x) → Q(t, x) and normality of extremals of optimization 
problems with u ∈ Q(t, x) as constraint. 
The set of regular controls Ω0(t) at x(t) ¯ , based on this new bounded slope 
condition, is 
Ω∗∗
0 (t) := {u ∈ Rm : (x(t), u) ¯ ∈ S(t) and there exists ρ > 0 such that
(BS∗∗)t,x'
,u' is satisfied for all points (x'
, u'
) in a neighbourhood of
S(t) ∩

(x(t), u) ¯ + ρB × ρB

relative to S(t)}.
Theorem 10.8.3 (Mixed Functional Constraints: II) Let (x,¯ u)¯ be a W1,1 local 
minimizer for (M). Assume that, for some ϵ > 0, 
(H1): g is Lipschitz continuous on a neighbourhood of (x(S), ¯ x(T )) ¯ . t → U (t) is 
a constant multifunction (write its value U), 
(H2): for each x ∈ Rn, f (., x, .) is L × Bm measurable, and there exist integrable 
functions k
f
x and k
f
u such that, for a.e. t ∈ [S,T ], 
|f (t, x1, u1) − f (t, x2, u2)| ≤ k
f
x (t)|x1 − x2| + k
f
u (t)|u1 − u2|
for all (x1, u1) and (x2, u2) in a neighbourhood of S(t) such that | x' −
x(t) ¯ | ≤ ϵ, |u' − ¯u| ≤ ϵ, 
(H3): for each x ∈ Rn, h(1)
(., x, .) and h(2)
(., x, .) are L × Bm measurable; for 
a.e. t ∈ [S,T ], h(1)
(t, ., .) and h(2)
(t, ., .) are continuously differentiable and 
there exists a constant kh such that, for a.e. t ∈ [S,T ], 
|(h(1)
, h(2)
)(t, x1, u1) − (h(1)
, h(2)
)(t, x2, u2)|
≤ kh(|x1 − x2|+|u1 − u2|)
for all (x1, u1) and (x2, u2) in a neighbourhood of Sϵ,ϵ (t), 
(BS∗∗): (BS)∗∗
t,x,u is satisfied, for every point (t, x, u) in 
closure {(t, x, u) ∈ [S,T ] × Rn × Rm : (x, u) ∈ Sϵ,ϵ (t)}. 
(Here u(t) ¯ is interpreted as some version of the equivalence class of bounded, 
a.e. equal functions.) 
(B): u¯ is essentially bounded. 
Then there exist p ∈ W1,1([S,T ]; Rn), λ0 ≥ 0 and integrable functions λ1 :
[S,T ] → (R+)κ1 and λ2 : [S,T ] → Rκ2 such that 
λ1(t) · h(1)
(t, x(t), ¯ u(t)) ¯ = 0, a.e.530 10 The Maximum Principle for Problems with Pathwise Constraints
and 
(a): (p, λ0) /= (0, 0), 
(b): (− ˙p(t), 0) ∈ co ∂{p(t) · f (t, ., .)}(x(t), ¯ u(t)) ¯ − {0} × co NU (u(t)) ¯
−λ(1)
(t) · ∇x,uh(1)
(t, x(t), ¯ u(t)) ¯ − λ(2)
(t) · ∇x,uh(2)
(t, x(t), ¯ u(t)) ¯ a.e., 
(c): (p(S), −p(T )) ∈ λ0∂g(x(S), ¯ x(T )) ¯ + NC(x(S), ¯ x(T )) ¯ , 
(d): p(t) · f (t, x,¯ u(t)) ¯ ≥ p(t) · f (t, x, u) ¯ for all u ∈ Ω∗∗
0 (t), a.e. t ∈ [S,T ].
Proof of Theorem 10.8.3 We shall verify: 
Claim: Under hypotheses of Theorem 10.8.3 (Functional Mixed Constraints: II), 
hypothesis (BS∗) of Theorem 10.8.2 (Functional Mixed Constraints: I), in which 
R ≡ ϵ, is valid for some constant K function. Furthermore, Ω∗∗
0 (t) ⊂ Ω∗
0(t), where 
Ω∗∗
0 (t) and Ω∗
0(t) are sets of regular admissible control values at x(t) ¯ corresponding 
to the bounded slope conditions (BS∗∗)t,x,u and (BS∗)K
t,x,u, respectively. 
The assertions of Theorem 10.8.3 will follow directly from Theorem 10.8.2 
applied to the special case when, for the given parameter ϵ > 0, the function R
is taken to be R ≡ ϵ, since the verification of the claim will ensure that (BS∗)
is satisfied, while all the other hypotheses in Theorem 10.8.3 are the same as, 
or stronger than, their counterparts in Theorem 10.8.2. (Note that, since K and 
the Lipschitz bound kh are constant functions, the compatibility hypothesis (C) in 
Theorem 10.8.2 is automatically satisfied.) The assertions of Theorem 10.8.3 merely 
reproduce those of Theorem 10.8.2, except that, in condition (d) of Theorem 10.8.3, 
Ω∗
0 has been replaced by Ω∗∗
0 (t).This substitution is justified since, according to the 
claim above, Ω∗∗
0 (t) ⊂ Ω∗
0(t). 
To verify the first claim suppose, to the contrary, it is false. We shall show this 
leads to a contradiction. Write 
Λ(t, x, u) := {(λ1, λ2) ∈ (R+)
κ1 × Rκ1 : λ1 · h(1)
(t, x, u) = 0}.
Under our assumption, there exists Ki ↑ ∞ such that, for each i, we can find 
(t, xi, ui) ∈ Sϵ,R=ϵ (t) and (λ1
i , λ2
i ) ∈ Λ(t, xi, ui) and νi ∈ NU (ui) such that 
Ki|(λ1
i , λ2
i ) · (∇u(h(1)
, h(2)
)(t, xi, ui)) + νi| < |(λ1
i , λ2
i )| . (10.8.6) 
Since u¯ (or, more precisely, the relevant selection from the equivalence class) 
is bounded, the sequence {(t, xi, ui)} is bounded. Notice that, from (10.8.6), 
(λ1
i , λ2
i ) /= 0. By dividing across (10.8.6) by (|(λ1
i , λ2
i )|+|νi|) and relabelling 
(λ1
i , λ2
i )/(|(λ1
i , λ2
i )|+|νi|) as (λ1
i , λ2
i ) and relabelling νi/(|(λ1
i , λ2
i )|+|νi|) as νi
(normalization), we can arrange that the variables in relation (10.8.6) satisfy 
|(λ1
i , λ2
i )|+|νi| = 1 . (10.8.7) 
Notice that {(λ1
i , λ2
i )} is a bounded sequence and, for each i, νi, after the preceding 
normalization step, continues to satisfy νi ∈ NU (ui). By extracting subsequences10.8 Mixed Constraints 531
then, we can arrange that (t, xi, ui) → (t, x, u), (λ1
i , λ2
i ) → (λ1, λ2) and νi → ν, 
for some (t, x, u) ∈ [S,T ] × Rn × Rm, (λ1, λ2) ∈ Λ(t, x, u). It is easy to deduce 
from the assumed regularity properties of h(1) and h(2) that (x, u) ∈ S(t) and ν ∈
NU (u). Because the left side of (10.8.6) is bounded and Ki ↑ ∞, we can conclude 
that |(λ1
i , λ2
i ) · (∇u(h(1)
, h(2)
)(t, xi, ui)) + νi| → 0 and so 
|(λ1, λ2) · (∇u(h(1)
, h(2)
)(t, x, u)) + ν| = 0 . (10.8.8) 
But then, from (BS∗∗)t,x,u, we deduce that (λ1, λ2) = (0, 0) and, therefore, 
by (10.8.8) we obtain also |ν| = 0. This contradicts condition (10.8.7). The proof of 
the first assertion is complete. 
A similar sequential compactness argument can be used to show that Ω∗∗
0 ⊂ Ω∗
0. 
Since the reverse inclusion is obviously true, we have Ω∗∗
0 = Ω∗
0.
⨅⨆
Proof of Theorem 10.8.1 Let ε > 0 be such that (x,¯ u)¯ is a minimizer for (G) with 
respect to admissible processes (x, u) such that 
||x − ¯x||W1,1 ≤ ε .
Take S to be the multifunction of (10.8.2). Noting that, from the compatibility 
hypothesis (C), k
φ
x (t)K(t) > 0 a.e., we can define: 
c(t) := (kf
x (t) + kφ
x (t)K(t)kf
u (t))/(kφ
x (t)K(t)) . (10.8.9) 
Now define the multifunction F : [S,T ] × Rn ⇝ Rn+1+m to be 
F (t, x) := {(f (t, x, u), Δf (t, x, u), θ (t, u))
: there exists u ∈ Rm s.t. (x, u) ∈ S(t)}
in which Δf (t, x, u) := |f (t, x, u) − f (t, x(t), ¯ u(t)) ¯ | and 
θ (t, u) := c(t)(u − ¯u(t)) .
Now consider the optimization problem 
(G)'
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize g(x(S), x(T )) over (x, y, z) ∈ W1,1
such that
(x(t), ˙ y(t), ˙ z(t)) ˙ ∈ F (t, x(t)), a.e.
and
(x(S), x(T )) ∈ C, y(S) = |x(S) − ¯x(S)|, z(S) = 0 and |y(T )| ≤ ε.
We see that (x,¯ 0, 0) is a minimizer for this problem. Indeed if (x, y, z) is any 
admissible F trajectory, we can, in consequence of the constraints of problem (G)'
, 
select a measurable function u such that (x(t), u(t)) ∈ S(t) for a.e. t ∈ [S,T ]532 10 The Maximum Principle for Problems with Pathwise Constraints
and ||x − ¯x||W1,1 ≤ ε. But then (x, u) is an admissible process for (G) such 
||x − ¯x||W1,1 ≤ ε. Since (x,¯ u)¯ is a W1,1 local minimizer for (G), (x,¯ 0, 0) is a 
minimizer for (G)'
. 
Our intention is to apply the necessary conditions of Chap. 8 to problem (G)'
. 
This necessitates some preliminary subgradient calculations, the results of which 
are summarized in the following Lemma. 
Lemma 10.8.4 Take any point t ∈ [S,T ] in the set of full measure such that the 
conditions in hypotheses (H2), (H3) and (BS) are satisfied. Take an arbitrary point 
u ∈ Ω0(t). Assume that for some set of positive parameters (ϵ'
, r'
, K'
) such that 
λ ∈ NФ(t)(φ(t, x'
, u'
)) and
(ψ, ζ ) ∈ ∂(λ · φ(t, ., .))(x'
, u'
) + {0} × NU (t)(u'
)

=⇒ |λ| ≤ K'
|ζ |
(10.8.10) 
for all (x'
, u'
) in a neighbourhood of {(x'', u'') ∈ S(t) : |x''− ¯x(t)| ≤ ϵ'
, |u''−u| ≤
r'
} relative to S(t). 
Write k
f '
x , k
f '
u , k
φ'
x and k
φ'
u for constants such that 
|f (t, x1, u1) − f (t, x2, u2)| ≤ k
f '
x |x1 − x2| + k
f '
u |u1 − u2|
|φ(t, x1, u1) − φ(t, x2, u2)| ≤ kφ'
x |x1 − x2| + kφ'
u |u1 − u2|
for all points (x1, u1), (x2, u2) in a neighbourhood of {(x'
, u'
) ∈ S(t) : |x' −
x(t) ¯ | ≤ ϵ'
, |u' − u| ≤ r'
} relative to S(t). 
Take (x, u) ∈ S(t) such that (x, u) ∈ (x(t), ¯ u(t)) ¯ + ϵ'
2 B × r'
2 B and 
(α, β, σ, τ ) ∈ NGr F(t,.)(x, f (t, x, u), Δf (t, x, u), θ (t, u)) . (10.8.11) 
Then 
(i): there exist η ∈ NФ(t)(φ(t, x, u)) and ν ∈ NU (t)(u) such that 
(α, −ν) ∈ ∂x,u(−β · f (t, x, u) − σΔf (t, x, u)
− τ · θ (t, u)) + ∂x,u(η · φ(t, x, u)), (10.8.12) 
furthermore, 
|α| ≤ (kf '
x + K'
kφ'
x k
f '
u )(|β|+|σ|) + K'
kφ'
x c(t)|τ |
and |η| ≤ K'
(kf '
u (|β|+|σ|) + c(t)|τ |) . (10.8.13) 
(ii): Now take u = ¯u(t) and (ϵ'
, r'
, K'
) = (ϵ, R(t), K(t)). Then (10.8.13) can be 
equivalently stated 
√
3 × |α| ≤ KF (|β|+|σ|+|τ |) and |η| ≤ K(t)(kf
u (t)(|β|+|σ|) + c(t)|τ |) ,10.8 Mixed Constraints 533
in which 
KF (t) := √
3 × (kf
x (t) + K(t)kφ
x (t)kf
u (t)) . (10.8.14) 
Proof In the ensuing analysis the t component of points (t, x, u) in the domains 
of f , Δf , φ and θ, is suppressed, to simplify the notation. Consider the 
assertions of part (i) of the lemma. We begin by dealing with the special 
case, in which (α, β, σ, τ ) is a proximal normal vector to Gr F (t, .) at 
(x, f (x, u), Δf (x, u), θ (u)). We deduce from the proximal normal inequality 
that, for some constant C > 0, (x, u, y := φ(t, x, y)) is a local minimizer for the 
optimization problem 
⎧
⎨
⎩
Minimize −α · x'
−β · f (x'
, u'
)−σ Δf (x'
, u'
)−τ · θ (u'
)+C |(x'
, u'
)−(x, u)|
2
over (x'
, u'
, y'
) ∈ Rm × Rm × Rκ such that
y' = φ(x'
, u'
), y' ∈ Ф(t) and u' ∈ U (t) .
(Notice that we have exploited the local Lipschitz continuity of f (t, ., .), Δf (t, ., .)
and θ (t, .) to justify the inclusion of a quadratic term in the cost only involving the 
|(x'
, u'
) − (x, u)|
2 quadratic error term.) Now apply the Lagrange multiplier rule 
(Theorem 5.6.2). This gives λ0 ∈ {0, 1} and vectors ν ∈ NU (t)(u) and η ∈ Rκ such 
that (η, λ0) /= (0, 0) and 
(0, 0, 0) ∈ λ0(−α, 0, 0) + λ0∂x,u(−β · f (x, u) − σ Δf (x, u) − τ · θ (u)) × {0}
+(0, 0, −η) + ∂x,u(η · φ(x, u)) + (0, ν, 0) + {(0, 0)} × NФ(t)(φ(x, u)) .
Consideration of the third coordinate reveals that η ∈ NФ(t)(φ(x, u)). We deduce 
that 
(λ0α, −ν) ∈ λ0∂x,u(−β · f (x, u) − σ Δf (x, u) − τ · θ (u)) + ∂x,u(η · φ(x, u)) .
If λ0 = 0, then (0, −ν) ∈ ∂x,u(η · φ(x, u)). This relation violates (10.8.10) since, 
when λ0 = 0, we must have η /= 0. So we can assume that λ0 = 1. It follows that 
(α, −ν) ∈ ∂x,u(−β · f (x, u)−σ Δf (x, u)−τ · θ (u))+∂x,u(η · φ(x, u)) .
(10.8.15) 
We see that there exists (ψ, ζ ) ∈ ∂x,u(−β · f (x, u) − σ Δf (x, u) − τ · θ (u)) such 
that 
(α − ψ, −ν − ζ ) ∈ ∂x,u(η · φ(x, u)) .
But then, by (10.8.10), |η| ≤ K'
|ζ |. Estimating the magnitude of ζ with the help of 
the Lipschitz continuity properties of f (t, ., .) etc., we arrive at 
|η| ≤ K' ×

k
f '
u (|β|+|σ|) + c(t)|τ |

. (10.8.16)534 10 The Maximum Principle for Problems with Pathwise Constraints
We deduce from (10.8.15) that 
|α| ≤ k
f '
x (|β|+|σ|) + K'
kφ'
x

k
f '
u (|β|+|σ|) + c(t)|τ |

, (10.8.17) 
|ν| ≤ k
f '
u (|β|+|σ|) + c(t)|τ | + K'
kφ'
u

k
f '
u (|β|+|σ|) + c(t)|τ |

. (10.8.18) 
(10.8.15), (10.8.16) and (10.8.17) are the required relations. We have confirmed the 
assertions in part (i), when (α, β, σ, τ ) is a proximal normal vector to NGr F(t,.). It 
remains to consider the case when (α, β, σ, τ ) is merely a (limiting) normal vector. 
Now we can find a sequences of points (xi, ui) → (x, u) and (αi, βi, σi, τi) →
(α, β, σ, τ ). such that, for each i, 
(αi, βi, σi, τi) ∈ NP
Gr F(t,.)(xi, f (xi, ui), Δf (xi, ui), θ (ui)) .
For each i sufficiently large, (xi, ui) lies in the relevant neighbourhood of the base 
point (x, u) and, arguing as above, we can show versions of (10.8.12) and (10.8.13) 
are valid, in which (xi, ui), and (αi, βi, σi, τi) replace (x, u) and (α, β, σ, τ )
respectively, for some νi ∈ NU (t)(ui) and ηi ∈ NФ(t)(φ(xi, ui)). We deduce from 
the relation (10.8.13) (strictly speaking, its version at the perturbed base point) that 
{ηi} is a bounded sequence. It then follows from (10.8.18), that {νi} is also a bounded 
sequence. Extracting a subsequence, we can arrange that ηi → η and νi → ν, for 
some vectors η and ν. We deduce from the closure properties of the limiting normal 
cone and the continuity of φ(t, ., .) that η ∈ NФ(t)(φ(x, u)) and ν ∈ NU (t)(u). We 
recover relations (10.8.11), (10.8.12) and (10.8.13), in the limit as i → ∞. 
The assertions of part (ii) of the lemma, concerning the case when u = ¯u(t), 
(ϵ'
, r'
, K'
) = (ϵ, R(t), K(t)), follow immediately from the substitution of the for￾mula for c(t), provided by (10.8.9), into (10.8.13). Since, now, (kf '
x , kf '
u , kφ'
x , kφ'
u ) =
(kf
x (t), kf
u (t), kφ
x (t), kφ
u (t)), this gives 
|α| ≤ k
f
x (t)(|β|+|σ|) + K(t)kφ
x (t)
k
f
u (t)(|β|+|σ|) + c(t)|τ |

.
= (kf
x (t)+K(t)kφ
x (t)kf
u (t))(|β|+|σ|+|τ |)=(
√
3)
−1KF (t)(|β|+|σ|+|τ |) ,
which is the desired relation. ⨅⨆
We are now ready to apply the necessary conditions of Theorem 8.4.3 to problem 
(G)' with reference to the minimizer (x,¯ 0, 0). We do so when the necessary 
conditions take the modified form of Corollary 8.5.2, in which (G1) and (G2)
(of Theorem 8.4.3) are imposed, but when the ‘pseudo Lipschitz continuity and 
tempered growth hypothesis’ (G3) is replaced by the bounded slope (and associated 
compatibility condition) hypothesis (G3)∗∗. 
(G1) is obviously satisfied. Consider (G2). Fix t ∈ [S,T ] and take any 
convergent sequence (xi, ei) → (x, e) in Gr F (t, .). It follows that, for each i, 
there exists ui ∈ U (t) such that ei = (f (t, xi, ui), Δf (t, xi, ui), c(t) (ui − ¯u(t)))10.8 Mixed Constraints 535
and φ(t, xi, ui) ∈ Ф(t). Since c(t) > 0, {ui} is a bounded sequence. But then, 
for a subsequence, ui → u for some u ∈ Rm. By the continuity properties 
of f (t, ., .) and φ(t, ., .) and since U (t) and Ф(t) are closed sets, we know that 
e = (f (t, x, u), Δf (t, x, u), c(t)(u − ¯u(t))), u ∈ U (t) and φ(t, x, u) ∈ Ф(t). We 
have shown that (x, e) ∈ F (t, x). This confirms that Gr F (t, .) is closed. It can be 
shown that F (., x) is measurable. 
To check (G3)∗∗, we set RF (t) := c(t) R(t) and we take B ≡ Rn. Fix 
t ∈ [S,T ]. Take any x ∈ ¯x(t) + ϵB and e ∈ F (t, x) such that |e −
(f (t, x(t), ¯ u(t)), ¯ 0, 0)|< RF (t). Then there exists u ∈ U (t) such that |u −
u(t) ¯ |< R(t), e = (f (t, x, u), Δf (t, x, u), c(t)(u − ¯u)) and φ(t, x, u) ∈ Ф(t). Now 
take any (α, β, σ, τ ) ∈ NGr F(t,.)(e). Then, according to Lemma 10.8.4, 
|α| ≤ (
√
3)
−1 KF (t)(|β|+|σ|+|τ |) ≤ KF (t)|(β, σ, τ )| . (10.8.19) 
Under the hypotheses of Theorem 10.8.1, k
f
x (t) + K(t)kφ
x (t)kf
u (t) is integrable 
and there exists γ > 0 such that R(t) ≥ K(t)kφ
x (t)γ . It follows that KF (t) =
(
√3)(kf
x (t) + K(t)kφ
x (t)kf
u (t)) is integrable and 
RF (t) = c(t)R(t) ≥ (kf
x (t) + kφ
x (t)K(t)kf
u (t))γ =KF (t)γ ,˜ (10.8.20) 
in which γ˜ := γ /√3. Relations (10.8.19) and (10.8.20) confirm that hypothesis 
(G3)∗∗ is satisfied. 
Since the relevant hypotheses are satisfied, we deduce from Cor. 8.5.2 that there 
exist (p, r, s) ∈ W1,1 (the costate trajectories corresponding to (x, y, z)) and λ0, 
not all zero, such that 
(A): (p,˙ r,˙ s)(t) ˙ ∈ co{ξ : (ξ , (p(t), r(t), s(t))) ∈ NGr F (t,.)(x(t), ( ¯ ˙
x(t), ¯ 0, 0))}
a.e. t ∈ [S,T ], 
(B): (p(S), −p(T )) ∈ λ0∂g(x(S), ¯ x(T )) ¯ + NC(x(S), ¯ x(T )) ¯ , 
(C): p(t) · ˙
x(t) ¯ ≥ (p(t), r(t), s(t)) · e for all e ∈ co Ω˜ 0(t) a.e. t ∈ [S,T ]
in which 
Ω˜ 0(t) := {e ∈ F (t, x(t)) ¯ : F (t, .)
is pseudo-Lipschitz continuous near ((x(t), ¯ 0, 0), e)}.
Since F does not depend on the (y, z) variables, we deduce from Corollary 8.5.2 
that r and s are constant functions. In view of the fact that the endpoint constraint 
(y(T ), z(T )) ∈ εB × Rn on these variables is inactive at (y, z) = (y,¯ z)¯ , 
Corollary 8.5.2 also tells us that (r, s) ≡ 0. (B) is the desired transversality 
condition. We deduce from condition (A) and Lemma 10.8.4 that, for a.e. t ∈ [S,T ], 
p(t) ˙ lies in the convex hull of the set of vectors ξ such that 
(ξ , 0) ∈ ∂x,u
−p(t)·f (t, x(t), ¯ u(t)) ¯

+∂x,u
η·φ(t, x(t), ¯ u(t)) ¯

+{0}×NU (t)(u(t)) ¯536 10 The Maximum Principle for Problems with Pathwise Constraints
for some η ∈ NФ(t)(φ(t, x(t), ¯ u(t)) ¯ such that |η| ≤ K(t)kf
u (t)|p(t)|. From the 
Carathéodory representation theorem, there exists a simplex (γ0,...,γn) in Rn, and 
for j = 0,...,n, points ξj ∈ Rn, ηj ∈ NФ(t)(φ(t, x(t), ¯ u(t)) ¯ and νj ∈ NU (t)(u(t)) ¯
such that p(t) ˙ = 
j γj ξj and, for each j , 
(ξj , 0) ∈ ∂x,u
− p(t)· f (t, x(t), ¯ u(t)) ¯

+ ∂x,u
ηj · φ(t, x(t), ¯ u(t)) ¯

+ {0}×{νj }.
and |ηj | ≤ K(t)kf
u (t)|p(t)|. Scaling by the simplex parameters and summing yields 
the relation 
(p(t), ˙ 0) = (
	
j
γj ξj , 0)
∈ co ∂x,u
− p(t) · f (t, x(t), ¯ u(t)) ¯

+{ξ ∈ co ∂x,u η · φ(t, x(t), ¯ u(t)) ¯ : η ∈ NФ(t)(φ(t, x(t), ¯ u(t)) ¯
and |η| ≤ K(t)kf
u (t)|p(t)|}
+{0} × co NU (t)(u(t)) . ¯
Using the sum rule for subdifferentials and noting that, for a locally Lipschitz 
continuous function d, co ∂(−d) = −co ∂d, we deduce that 
(− ˙p(t), 0) ∈ co ∂x,u p · f (t, x(t), ¯ u(t)) ¯ − {0} × co NU (t)(u(t)) ¯
−{ξ ∈ co ∂x,u η · φ(t, x(t), ¯ u(t)) ¯ : η ∈ NФ(t)(φ(t, x(t), ¯ u(t)) ¯
and |η| ≤ K(t)kf
u (t)|p(t)|}.
We have confirmed the costate inclusion. Consider finally the consequences of 
the Weierstrass condition (C). Fix t in a suitable subset of [S,T ] of full measure 
and take any point u ∈ Ω0(t). (Recall the definition of Ω0(t) provided by (10.8.3).) 
Then there exists ρ > 0 such that 
λ ∈ NФ(t)(φ(t, x'
, u'
)) and
(α, β) ∈ ∂(λ · φ(t, ., .))(x'
, u'
) + {0} × NU (t)(u'
)

=⇒ |λ| ≤ ρ−1|β|
(10.8.21) 
for all points (x'
, u'
) in a neighbourhood of {(x'', u'') ∈ S(t) : |x'' − ¯x(t)| ≤
ρ , |u'' − u| ≤ ρ} relative to S(t). 
Set e := (f (t, x(t), u), Δf (t, ¯ x(t), u), θ (t, u)) ¯ . Write k
f '
x , k
f '
u , k
φ'
x and k
φ'
u for 
constants such that 
|f (t, x1, u1) − f (t, x2, u2)| ≤ k
f '
x |x1 − x2| + k
f '
u |u1 − u2|,
|φ(t, x1, u1) − φ(t, x2, u2)| ≤ kφ'
x |x1 − x2| + kφ'
u |u1 − u2|10.9 Exercises 537
for all points (x1, u1), (x2, u2) in a neighbourhood of {(x'
, u'
) ∈ S(t) : |x'
− ¯x(t)| ≤
ρ , |u' − u| ≤ ρ}. Now let δ ∈ (0, ρ] be such that 
(
√
3 × δ)−1 ≥ (kf '
x + ρ−1(kφ'
x k
f '
u + kφ'
x c(t))) and δ ≤ c(t)ρ .
Take any x' ∈ ¯x(t) + δB, e' ∈ F (t, x) ∩ (e + δB) and (α, β, σ, τ ) such that 
(α, β, σ, τ ) ∈ NGr F(t,.)(x'
, e'
) .
Then e' = (f (t, x'
, u'
), Δf (t, x'
, u'
), θ (t, u'
)) for some u' such that (x'
, u'
) ∈ S(t). 
We have c(t)|u' − u| ≤ δ ≤ c(t)ρ and hence |u' − u| ≤ ρ . From Lemma 10.8.4 
|α| ≤ (kf '
x + ρ−1(kφ'
x k
f '
u + kφ'
x c(t)))(|β|+|σ|+|τ |)
≤ (
√
3 × δ)−1(|β|+|σ|+|τ |) ≤ δ−1|(β, σ, τ )| .
We known that F (t, .) satisfies the bounded slope condition near (x(t), ¯ 0, 0),
(f (t, x(t), u), Δf (t, ¯ x(t), u), ¯ θ (t, u)). It follows from Proposition 8.2.3 that F (t, .)
is pseudo-Lipschitz near (x(t), ¯ 0, 0), (f (t, x(t), u), ¯ Δf (t, x(t), u), θ (t, u)) ¯ . We 
have shown that (f (t, x(t), u), Δf (t, ¯ x(t), u), θ (t, u)) ¯ ∈ Ω˜ 0(t). But then, by 
condition (C), 
p(t) · (f (t, x, u)) ¯ ≥ p(t) · f (t, x(t), ¯ u(t)) . ¯
We have confirmed the Weierstrass condition. The proof is complete.
⨅⨆
10.9 Exercises 
10.1 (State Constrained Maximum Principle in Gamkrelidze Form) Let (x,¯ u)¯
be a minimizer for the state constrained problem 
⎧
⎪⎪⎨
⎪⎪⎩
Minimize g(x(S), x(T ))
subject to x(t) ˙ = f (x(t), u(t)), u(t) ∈ U a.e.
h(x(t)) ≤ 0 for all t ∈ [S,T ]
(x(S), x(T )) ∈ C .
with data functions f : Rn × Rm → Rn, g : Rn × Rn → R, h : Rn → R and sets 
U ⊂ Rm and C ⊂ Rn × Rn. 
Assume that hypotheses (H1)–(H3) of Theorem 10.4.1 are satisfied. Assume 
further that g is C1, f (., u(t)) ¯ is C1 a.e. and h is C2. Show that there exist 
p ∈ W1,1([S, T ]; Rn), μ ∈ C⊕(S, T ) and λ ≥ 0 such that538 10 The Maximum Principle for Problems with Pathwise Constraints
(i): (p, μ, λ) /= (0, 0, 0), 
(ii): − ˙p(t) = 
p(t) + 
[S,t] dμ(s)hx (x(t)) ¯ 
· fx (x(t), ¯ u(t)) ¯
+ 
[S,t] dμ(s)hxx (x(t)) ¯ · f (x(t), ¯ u(t)) ¯ , 
(iii): u → (p(t) + 
[S,t] dμ(s)hx (x(t))) ¯ · f (x(t), ¯ u) is maximized over U at u =
u(t) ¯ . a.e., 
(iv): supp {μ}⊂{t : h(x(t)) ¯ = 0}, 
(v): (p(S), −(p(T ) + 
[S,T ] dμ(t)hx (x(T ¯ ))) ∈ λ∇g(x(S), ¯ x(T ¯ ))
+NC(x(S), ¯ x(T ¯ )). 
Hint: Show that (x,¯ z(t) ¯ := h(x(t)), ¯ u(t)) ¯ is a minimizer for 
⎧
⎪⎪⎨
⎪⎪⎩
Minimize g(x(S), x(T ))
subject to (x(t), ˙ z(t)) ˙ = (f (x(t), u(t)), hx (x(t))·f (x(t), u(t)), u ∈ U a.e.
z(t) ≤ 0 for all t ∈ [S,T ]
(x(S), x(T ), z(T )) ∈ {(x0, x1, z1) : (x0, x1) ∈ C and z1 ≥ h(x1)}.
Now apply the standard state constrained maximum principle (Theorem 10.4.1) to 
this problem. 
10.2 Show that if the outward pointing condition 
max
u∈U
hx (x(t)) ¯ · f (x(t), u) > ¯ 0 for all t ∈ [S,T ]
is satisfied and h(x(T ¯ )) < 0, then the non-trivially condition (i) in Exercise 10.1 
can be replaced by the stronger condition 
(i)'
: (p, λ) /= (0, 0), 
Remark 
This example illustrates how the Gamkrelidze form of the necessary conditions can 
be used to strengthen non-trivially conditions, concerning the Lagrange multipliers, 
under additional hypotheses. 
10.3 Consider the problem 
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize  T
S L(x(t), x(t))dt ˙
over x ∈ W1,1([S,T ]; Rn) s.t.
x(t) ˙ ∈ U for a.e. t ∈ [S,T ],
h(x(t)) ≤ 0 for all t ∈ I,
x(S) = x0,
for which the data comprise an interval [S, T ], functions L : Rn × Rn → R and 
h : Rn → R, a closed set I ⊂ [S, T ] and a point x0 ∈ Rn. Let x¯ be a minimizer. 
Assume that 
(A): L and h are continuously differentiable, 
(B): U is a compact, convex set and L(x, .) is convex for each x ∈ Rn.10.9 Exercises 539
Define 
I (x)¯ := {t ∈ I : h(x(t)) ¯ = 0}.
(a): Show that 
 T
S
[Lx (x(t), ¯ ˙
x(t)) ¯ · yv(t) + (L(x(t), v(t)) ¯ − L(x(t), ¯ ˙
x(t))) ¯ ] dt
∨

max
t∈I (x)¯
hx (x(t)) ¯ ·yv(t)
≥ 0,
for all measurable functions v such that v(t) ∈ U, a.e. t ∈ [S, T ], in which 
yv(t) :=  t
S (v(s) − ˙
x(t))ds ¯ . 
(b): Deduce from the Aubin one-sided minimax theorem (Theorem 3.6.5) that there 
exist λ ≥ 0 and μ ∈ C⊕(S, T ) such that supp μ ⊂ I (x)¯ , λ + ||μ||TV = 1 and 
 T
S
[λ Lx (x(t), ¯ ˙
x(t)) ¯ · yv(t) + λ (L(x(t), v(t)) ¯ − L(x(t), ¯ ˙
x(t)) ¯ ] dt
+

[S,T ]
yv(t)·hx (x(t))dμ(t) ¯ ≥ 0,
for all measurable functions v such that v(t) ∈ U, a.e. t ∈ [S, T ]. 
(c): Let p ∈ W1,1 be defined by 

p(t) ˙ = λLx (x(t), ¯ ˙
x(t)), ¯ a.e. t ∈ [S,T ],
−p(T ) = 
[S,T ] hx (x(t))dμ(t) . ¯
Show that 

p(t) +

[S,t]
hx (x(s))dμ(s) ¯

· ˙
x(t) ¯ − λ L(x(t), ¯ ˙
x(t)) ¯ =
max
v∈U

p(t) +

[S,t]
hx (x(s))dμ(s) ¯

· v + λ L(x(t), ¯ ˙
x(t)), ¯ a.e. t ∈ [S,T ].
Remark 
The Aubin one-sided minimax theorem has been used to derive necessary conditions 
for state constrained dynamic optimization problems. (See e.g. [65] and [194].) This 
exercise illustrates the analysis involved, for a simple velocity constrained problem 
in the calculus of variations; it is easily generalized to allow for end-point costs and 
end-point constraints and a different dynamic constraint. A notable feature of this 
approach is that it can simply accommodate a state constraint imposed only on an540 10 The Maximum Principle for Problems with Pathwise Constraints
arbitrary closed subset I of the underlying time interval [S, T ], including cases in 
which I is an isolated point t
¯. The penalty approach employed in this chapter, in 
which we take account of the state constraint by adding the penalty term 
+ K

I
(h(x(t))) ∨ 0)
2dt
and passage to the limit as K ↑ ∞, while offering some advantages in other respects 
(avoiding the need for convexification, for example), cannot be used directly in this 
‘isolated point case’, because when I has Lebesgue measure zero, the penalty term 
automatically vanishes. It is for this reason that, in applying the penalty approach 
to problems in which the state constraint is imposed on a subset of I (or, more 
generally, in which h(t,x) is t dependent and possibly discontinuous), we need to 
introduce an extra step, in which h(t,x) is approximated by a function which is 
piecewise constant w.r.t. t. 
10.4 (Free End-Time Mixed Constrained Maximum Principle: Measurable 
Time Dependence) Let ([S,¯ T¯], x,¯ u)¯ be a W1,1 local minimizer (in the sense 
expressed in Sect. 9.6 of Chap. 9, and w.r.t. some parameter δ > 0) for the free 
end-time mixed constrained problem 
(F T M)
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize g(S, x(S), T , x(T ))
over intervals [S,T ], arcs x ∈ W1,1([S,T ]; Rn)
and measurable functions u : [S,T ] → Rm satisfying
x(t) ˙ = f (t, x(t), u(t)) a.e. t ∈ [S,T ],
φ(t, x(t), u(t)) ∈ Ф(t) and u(t) ∈ U (t) a.e. t ∈ [S,T ]
u(t) ∈ U (t) a.e. t ∈ [S,T ],
and
(S, x(S), T , x(T )) ∈ C .
The data for this problem comprise functions g : R1+n+1+n → R, f : R × Rn ×
Rm → Rn and φ : R × Rn × Rm → Rκ , non-empty multifunctions U : R ⇝ Rm
and Ф : R ⇝ Rκ , and a closed set C ⊂ R1+n+1+n. 
Assume that, for some ϵ > 0, σ ∈ (0, (T¯ − S)/ ¯ 2), r0 > 0 and measurable 
function R : [S¯ − σ, T¯ + σ] → R ∪ {+∞}, such that R(t) > r0 for a.e. t ∈
[S¯ − σ, T¯ + σ], the following hypotheses are satisfied: 
(H1): g is Lipschitz continuous on a neighbourhood of (S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ and C is 
a closed set, 
(H2): U is a measurable multifunction that takes values non-empty, closed subsets 
of Rm; Ф is a Lebesgue measurable multifunction taking values closed, non￾empty subsets of Rκ ,10.9 Exercises 541
(H3): for each x ∈ Rn, f (., x, .) is L × Bm measurable; there exist measurable 
functions on R k
f
x and k
f
u such that, for a.e. t ∈ [S¯ − σ, T¯ + σ], 
|f (t, x1, u1) − f (t, x2, u2)| ≤ k
f
x (t)|x1 − x2| + k
f
u (t)|u1 − u2|
for all (x1, u1) and (x2, u2) in a neighbourhood of Sϵ,R(t)
e (t), 
(H4): for each x ∈ Rn, φ(., x, .) is L × Bm measurable; there exist measurable 
functions on R k
φ
x and k
φ
u such that, for a.e. t ∈ [S¯ − σ, T¯ + σ], 
|φ(t, x1, u1) − φ(t, x2, u2)| ≤ kφ
x (t)|x1 − x2| + kφ
u (t)|u1 − u2|
for all (x1, u1) and (x2, u2) in a neighbourhood of Sϵ,R(t)
e (t), 
(BS): there exists a measurable function K : R → R such that, for a.e. t ∈ [S¯ −
σ, T¯ + σ], condition (BS)K(t)
t,x,u is satisfied for all (x, u) in a neighbourhood 
of Sϵ,R(t)
e (t) relative to S(t), 
(H5): there exists c¯ ≥ 0 such that, for a.e. t ∈ [S¯ − σ, S¯ + σ]∪[T¯ − σ, T¯ + σ], 
|f (t, x, u)|≤ ¯c, |u|≤ ¯c, |(kf
x (t) + kφ
x (t)K(t)kf
u (t))/(kφ
x (t)K(t))|≤ ¯c,
for all x ∈ ¯xe(t) + ϵB and u ∈ U (t) such that φ(t, x, u) ∈ Ф(t). 
Assume also the compatibility condition: for some γ > 0, 
(C): k
f
x and t → K(t)kφ
x (t)kf
u (t) are integrable on [S¯−σ, T¯ +σ], and K(t)kφ
x (t) >
0 and R(t) ≥ K(t)kφ
x (t)γ , a.e. on [S¯ − σ, T¯ + σ]. 
Show that there exist p ∈ W1,1([S,¯ T¯]; Rn) and a real number λ0 ≥ 0 such that 
(i): (p, λ0) /= (0, 0), 
(ii): (− ˙p(t), 0) ∈ co ∂x,u(p(t) · f (t, x(t), ¯ u(t))) ¯
−co {ξ ∈ ∂x,u(λ · φ(t, x(t), ¯ u(t))) ¯ : λ ∈ NФ(t)(φ(t, x(t), ¯ u(t))) ¯ }
− {0} × co NU (t)(u(t)) ¯ a.e. t ∈ [S,¯ T¯], 
(iii): there exist ξ0 ∈ sub-esst→S¯H (t, x(¯ S), ¯ p(S)) ¯ , 
and ξ1 ∈ super-esst→T¯ H (t, x(¯ T ), ¯ p(T )) ¯ such that 
(−ξ0, p(S), ¯ ξ1, −p(T )) ¯ ∈ λ0∂g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ +NC(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ , 
(iv): p(t)·f (t, x(t), ¯ u(t)) ¯ ≥ p(t)·f (t, x(t), ¯ u), for all u ∈ Ω0(t), a.e. t ∈ [S,¯ T¯].
Here, 
Sϵ,R
e (t) := {(x, u) ∈ S(t) : |x − ¯xe(t)| ≤ ϵ and |u − ¯u(t)| ≤ R}.
Hint: Consider the free end-time optimization problem542 10 The Maximum Principle for Problems with Pathwise Constraints
(F T M)'
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize g(S, x(S), T , x(T ))
over [S,T ] ⊂ R and (x, y, z) ∈ W1,1([S,T ]; Rn+1+m) s.t.
(x(t), ˙ y(t), ˙ z(t)) ˙ ∈ F (t, x(t)), ˜ a.e. t ∈ [S,T ]
and
(S, x(S), T , x(T )) ∈ C, y(S) = |x(S) − ¯x(S)|, z(S) = 0
and |y(T )| ≤ δ,
where F˜ is a suitable extended (on R) version of the multifunction F defined in the 
proof of Theorem 10.8.1. Observe that ([S,¯ T¯], x,¯ 0, 0) is a local minimizer for this 
problem and apply Theorem 9.4.1. 
10.5 (Free End-Time Mixed Constrained Maximum Principle: Lipschitz Time 
Dependence) Let ([S,¯ T¯], x,¯ u)¯ be a W1,1 local minimizer (in the sense expressed 
in Sect. 9.6 of Chap. 9, and w.r.t. some parameter δ > 0) for the free end-time 
mixed constrained problem (F T M) of Exercise 10.4. Suppose that condition (H1) 
of Exercise 10.4 is satisfied. Assume also that, for some constants ϵ > 0, r0 > 0 
and integrable function R : [S,¯ T¯] → R+, such that R(t) > r0 for a.e. t ∈ [S,¯ T¯], 
the following hypotheses are satisfied: 
(H2)'
: U (t) ≡ U for all t ∈ R, for some non-empty, closed set U ⊂ Rm, Ф(t) ≡ Ф
for all t ∈ R, for some non-empty, closed set Ф ⊂ Rκ , 
(H3)'
: for each x ∈ Rn, f (., x, .) is L × Bm measurable; there exists a constant 
kf > 0 such that, for a.e. t ∈ [S,¯ T¯], 
|f (t1, x1, u1) − f (t2, x2, u2)| ≤ kf (|t1 − t2|+|x1 − x2|+|u1 − u2|)
for all t1, t2 ∈ [t − ϵ, t + ϵ], (x1, u1) and (x2, u2) in a neighbourhood of 
Sϵ,R(t)(t), 
(H4)'
: for each x ∈ Rn, φ(., x, .) is L × Bm measurable; there exists a constant 
kφ > 0 such that, for a.e. t ∈ [S,¯ T¯], 
|φ(s1, x1, u1) − φ(s2, x2, u2)| ≤ kφ(|s1 − s2|+|x1 − x2|+|u1 − u2|)
for all s1, s2 ∈ [t − ϵ, t + ϵ], (x1, u1) and (x2, u2) in a neighbourhood of Sϵ,R(t)(t), 
(BS)'
: there exists a measurable function K : [S,¯ T¯] → R+ such that, for a.e. 
t ∈ [S,¯ T¯], 
λ ∈ NФ(φ(s, x, u)) and 
(αt, αx ,β) ∈ ∂(λ · φ)(s, x, u) + {0, 0} × NU (u)
=⇒ |λ| ≤ K(t)|β| ,
for all (s, x, u) in a neighbourhood of Sϵ,R(t)(t) relative to S.10.10 Notes for Chapter 10 543
Assume also the compatibility condition: for some γ > 0, 
(C)'
: t → K(t) × (|˙
x(t) ¯ | + R(t)) is integrable on [S,¯ T¯], and K(t)kφ
x (t) > 0 and 
R(t) ≥ K(t)kφ
x (t)γ , a.e. on [S,¯ T¯]. 
Show that there exist p ∈ W1,1([S,¯ T¯]; Rn), a ∈ W1,1([S,¯ T¯]; R) and a real 
number λ0 ≥ 0 such that 
(i): ((p, a), λ0) /= (0, 0, 0), 
(ii): ((a(s), ˙ − ˙p(s)), 0) ∈ co ∂t,x,u(p(s) · f (s, x(s), ¯ u(s))) ¯
−co {ξ ∈ ∂t,x,u(λ · φ(s, x(s), ¯ u(s))) ¯ : λ ∈ NФ(φ(s, x(s), ¯ u(s))) ¯ }
− {0} × co NU (u(t)) ¯ a.e. s ∈ [S,¯ T¯], 
(iii): (−a(S), ¯ p(S), ¯ a(T ), ¯ −p(T )) ¯ ∈ λ0∂g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯
+NC(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ , 
(iv): p(t) · f (t, x,¯ u(t)) ¯ = maxu∈U p(t) · f (t, x(t), ¯ u) a.e. t ∈ [S,¯ T¯], 
(v): p(t) · f (t, x,¯ u(t)) ¯ = a(t) a.e. t ∈ [S,¯ T¯] . 
Here, 
Sϵ,R(t) := {(s, x, u) ∈ S : (s, x) ∈ −(t, x(t)) ¯ + ϵB and u ∈ ¯u(t) + R(t)B},
S := {(s, x, u) ∈ R × Rn × Rm : φ(s, x, u) ∈ Ф and u ∈ U}.
Hint: Apply the technique used in the proof of Theorem 9.6.1 to construct an auxil￾iary fixed time interval mixed constraints problem, and observe that Theorem 10.8.1 
is applicable to this auxiliary problem. 
10.6 Show that the assertions of the special case of Theorem 10.8.1 remain valid 
when the condition ‘φ(t, ., .)’ is continuously differentiable near (x(t), ¯ u(t)) ¯ a.e. 
t ∈ [S, T ]’ is replaced by the less restrictive hypothesis ‘φ(t, ., .)’ is strictly 
differentiable at (x(t), ¯ u(t)) ¯ a.e. t ∈ [S, T ]’. Here, a function ϕ : Rn → Rk is 
said to be strictly differentiable at a point z¯ if there exists a (continuous) linear map 
Dsϕ(z)¯ : Rn → Rk such that for each v ∈ Rn we have 
lim
h↓0, z→¯z
h−1 [ϕ(z + hv) − ϕ(x)] = Dsϕ(z)(v) , ¯
and provided the convergence is uniform for v in compact sets (see [65]). 
10.10 Notes for Chapter 10 
Proposition 10.3.1, whose role is to ensure the preservation of necessary conditions 
involving measure multipliers under limit taking, for pure state constrained prob￾lems, is taken from [196]. Additional material on convergence properties of Borel 
measures can be found in [39].544 10 The Maximum Principle for Problems with Pathwise Constraints
Necessary conditions for state constrained dynamic optimization problems with 
optimal state trajectories lying completely in the boundary of the state constraint 
region go back to the earliest days of dynamic optimization and appear in the 
monograph of Pontryagin, Boltyanskii, Gamkrelidze and Mischenko [167]. An early 
version of the maximum principle for dynamic optimization problems with state 
constraints, which made allowance for a finite number of boundary and interior 
arcs was obtained by Gamkrelidze [117], under strong regularity hypotheses on the 
optimal control. Dubovitskii and Milyutin and their research collaborators have been 
prominent contributors to the theory of necessary conditions for problems with both 
pure state and mixed pathwise constraints, starting in the 1960s [96]. Their later 
work is covered in the monograph [153] and review article [91]. See also [147] and 
[90]. 
Nonsmooth maximum principles for pure state constrained problems were 
proved by Warga [207], Vinter and Pappas [196], Clarke [65] and Ioffe [124]. 
The maximum principle for problems involving a functional inequality constraint, 
proved in this chapter, is a refinement of Clarke’s necessary conditions for problems 
with state constraints in [65]. Proof techniques employed in this book involve 
replacing the pure state constraint by an integral penalty term, an idea which was 
introduced in [196]. Necessary conditions for free end-time problems were given in 
[201]. 
The degeneracy phenomenon, namely the fact that standard necessary conditions 
of optimality convey no useful information for certain state constrained dynamic 
optimization problems of interest, was first investigated by Arutyunov, Aseev and 
Blagodatskikh, see [5]. It was the subject of numerous subsequent publications in 
the Russian, and more recently, the Western literature [194]. We refer to Arutyunov’s 
book [2], and also to [3] and [4], for overviews and further references. The key 
idea here was that, for autonomous problems, the costate function can be chosen 
such that the constancy of the Hamiltonian condition, previously known to hold at 
interior times can be extended to the end-times: this property coupled with an inward 
pointing hypothesis can be used to derive non-degenerate forms of the maximum 
principle. The non-degenerate necessary conditions in this chapter (Theorem 10.7.1) 
are new; they improve on earlier conditions in, say, [2], because they allow general 
endpoint constraints and data that is nonsmooth w.r.t. the state variable. 
The conditions referred to above are restricted to problems whose data is 
Lipschitz continuous w.r.t. time. The non-degenerate conditions of Ferreira and 
Vinter [101] allow measurable time dependence but place restrictions on the nature 
of the optimal state trajectories near the endtimes, which are difficult to test in 
practice. Non-degenerate necessary conditions, in the form of the generalized Euler 
Lagrange inclusion, derived by Rampazzo and Vinter [170], do not pre-suppose 
any structural properties of minimizers and allow measurable time dependence, 
nonconvex velocity sets and general end-point constraints. 
The approach followed in this chapter to deriving necessary conditions of 
optimality for mixed problems is that pioneered by Clarke and de Pinho [69]: 
apply the generalized Euler Lagrange condition to a ‘differential inclusion’ problem, 
in which the differential inclusion results from merging the dynamic and mixed10.10 Notes for Chapter 10 545
constraints and express these conditions directly in terms of the data for the original 
problem. The necessary conditions in this chapter include an improved Weierstrass 
condition, obtained by applying the generalized Euler Lagrange condition with the 
Ioffe refinement to the differential inclusion problem. Following [69], we show how 
these conditions unify earlier necessary conditions for mixed constraint problems in 
the literature. Clarke, Ledyaev and de Pinho [87] showed how the approach can be 
extended to provide necessary conditions for mixed constraint problems not covered 
by their original theory. 
In this chapter, we have treated pure and mixed constraints separately. But the 
two kinds of constraints can be treated simultaneously [40], once again, using the 
Clarke/de Pinho approach; now we apply the pure state constrained version of the 
generalized Euler Lagrange condition, a derivation of which is given in Chap. 11, 
to the related differential inclusion problem. These conditions feature a measure 
multiplier for the pure state constraint and an absolutely continuous multiplier for 
the mixed constraint. Dmitruk [90] has also derived (smooth) necessary conditions 
of this nature. We mention that, in the Russian literature, versions of the maximum 
principle have been derived for problems involving irregular mixed constraints, that 
is where the pathwise constraint set cannot be expressed as the intersection of a pure 
state constraint set and a mixed constraint set satisfying, say, the hypotheses of [92].Chapter 11 
The Euler-Lagrange and Hamiltonian 
Inclusion Conditions in the Presence of 
State Constraints 
Abstract This chapter concerns necessary conditions of optimality for dynamic 
optimization problems with pathwise state constraints, when the dynamic constraint 
takes the form of a differential inclusion. Here, the pathwise constraint is expressed 
as a time-dependent scalar functional inequality constraint. Through redefinition of 
the state constraint function, which is merely required to be upper semicontinuous 
and Lipschitz continuous w.r.t. the state variable, we can subsume within this 
framework other formulations of the pathwise state constraint (vector inequality 
constraints, combined equality/inequality constraints, set inclusion constraints, etc.) 
We have seen in Chap. 10, which concerned necessary conditions for path￾constrained dynamic optimization problems involving controlled differential inclu￾sions, how it is not possible, in general, to accommodate the state constraint by 
means of an absolutely continuous Lagrange multiplier; instead we must introduce 
a measure multiplier. The same is true when, as in this chapter, we substitute a 
differential inclusion for a controlled differential equation in the problem formu￾lation. Once again, the necessary conditions involve a costate trajectory that is 
discontinuous, though we hide this fact, expressing the conditions in terms of a 
modified, absolutely continuous, co-state trajectory, obtained by subtracting off the 
singular component from the ‘true’ co-state trajectory. 
The necessary conditions for state constrained differential inclusion problems 
appearing in this chapter resemble the generalized Euler Lagrange condition of 
Chap. 8, but now including a measure multiplier associated with the state constraint. 
They are derived under unrestrictive conditions that allow the velocity sets to be 
non-convex and unbounded. Under an additional hypothesis that the velocity sets 
are convex, we can prove, with the help of the duality theorem relating Euler 
Lagrange and Hamiltonian conditions, also a Hamiltonian version of the condition. 
The generalized Euler Lagrange condition for state constrained problems of this 
chapter incorporates the stratified conditions of Clarke and the Ioffe refinement. The 
proof technique mimics that used in Chap. 8, where the full necessary conditions 
were built up in stages, starting with an application of the maximum principle to 
a simple version of the problem. The difference is that, now, we use the state￾constrained maximum principle in the first stage. We also address the degeneracy 
issue associated with necessary conditions for pure state constraint problems. 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
P. Bettiol, R. B. Vinter, Principles of Dynamic Optimization, Springer Monographs 
in Mathematics, https://doi.org/10.1007/978-3-031-50089-3_11
547548 11 The Euler-Lagrange and Hamiltonian Inclusion Conditions in the Presence of. . .
We showed in Chap. 10 that, for state constrained problems involving controlled 
differential equations, standard necessary conditions are sometimes degenerate 
when the state constraint is active at either end-time. The same is true when the 
dynamic constraint takes the form of a differential inclusion. This chapter also 
provides non-degenerate necessary conditions, under extra hypotheses, covering 
these situations. 
Formulations of the state constrained differential inclusion problem, in which the 
end-times are included among the choice variables, are also considered. The extra 
necessary conditions associated with the free end-times take, as usual, the form of 
boundary conditions on the Hamiltonian evaluated along the minimizing state and 
costate trajectories. 
11.1 Introduction 
In this chapter, we broaden our earlier investigation, in Chap. 8, of necessary 
conditions for dynamic optimization problems, in which the dynamic constraint is 
formulated as a differential inclusion. Now, we introduce a pathwise state constraint 
h(t, x(t)) ≤ 0 for all t ∈ [S,T ].
Attention focuses then on: 
(P )
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize g(x(S), x(T ))
over absolutely continuous arcs x : [S,T ] → Rn satisfying
x(t) ˙ ∈ F (t, x(t)) a.e.,
(x(S), x(T )) ∈ C,
h(t, x(t)) ≤ 0 for all t ∈ [S,T ].
Here [S,T ] ⊂ R is a given interval, g : Rn ×Rn → R and h : [S,T ]×Rn → R are 
given functions, F : [S,T ] × Rn ⇝ Rn is a given multifunction, and C ⊂ Rn × Rn
is a given set. We shall also study a free end-time version of this problem. 
Admissible F trajectories for (P ) are W1,1 arcs x : [S,T ] → Rn that satisfy 
the constraints of problem (P ), which now include the state constraint. Given a 
multifunction B : [S,T ] ⇝ Rn, we say that an admissible F trajectory x¯ is a W1,1
local minimizer relative to B if there exists β > 0 such that 
g(x(S), x(T )) ≥ g(x(S), ¯ x(T )) ¯
for all admissible F trajectories x such that ||x− ¯x||W1,1 ≤ β and x(t) ˙ ∈ ˙
x(t) ¯ +B(t), 
a.e.. 
As we shall see, variational techniques employed in Chap. 8 can be adapted to 
take account of the pathwise state constraint. These techniques can be used, together 
with the methods of Chap. 9, also to derive necessary conditions of optimality for 
the free end-time problem.11.2 The Euler Lagrange Inclusion 549
In Chap. 10, concerning state constrained problems with dynamic constraint 
formulated as a controlled differential equation, we drew attention to the fact 
that, for certain problems of interest (including problems in which the end-states 
are fixed and located in the state constraint boundary) the standard necessary 
conditions are automatically satisfied by any admissible process. The remedy to 
this ‘degeneracy’ phenomenon was to derive supplementary boundary conditions on 
the maximized Hamiltonian along the optimal state trajectory. We shall show, also 
for state constrained problems with dynamic constraint formulated as a differential 
inclusion, degeneracy can be eliminated by introducing additional hypotheses that 
include boundary conditions on the maximized Hamiltonian. 
11.2 The Euler Lagrange Inclusion 
This section provides necessary conditions for the state constrained dynamic 
optimization problem (P ) formulated above. These take the form of the generalized 
Euler Lagrange inclusion and accompanying conditions. 
As in Chap. 8, the following subset of the velocity set F in problem (P ), 
evaluated along a nominal F trajectory x¯, is used to formulate the Weierstrass 
condition: the regular velocity set relative to x¯ is the multifunction Ω0 : [S,T ] ⇝
Rn: 
Ω0(t) := {e ∈ F (t, x(t)) ¯ : F (t, .) is pseudo-Lipschitz continuous near (x(t), e) ¯ }.
(11.2.1) 
(The ‘pseudo-Lipschitz continuity near (x(t), e) ¯ ’ condition was defined in Chap. 8, 
Definition 8.2.1: it requires that there exists ϵ' > 0, R' > 0, k' > 0 such that 
F (t, x)∩(e+R' B) ⊂ F (t, x'
)+k'
|x −x'
|B for all x, x' ∈ ¯x(t)+ϵ'
B.) Consistent 
with earlier notation, we write C⊕(S, T ) for the subset of the topological dual of 
C(S, T ) comprising elements that take non-negative values on the space of non￾negative functions in C(S, T ). By NBV ([S,T ]; Rn) we denote the space of Rn￾valued functions of bounded variation defined on [S,T ] which are right-continuous 
on (S, T ). (See Chap. 10, Sect. 10.2 for related discussion.) 
Theorem 11.2.1 (The Euler Lagrange Inclusion with State Constraints) Take 
a measurable multifunction B : [S,T ] ⇝ Rn such that B(t) is open for a.e. t ∈
[S,T ]. Let x¯ be a W1,1 local minimizer for (P) relative to B. Assume that, for some 
ϵ > 0, the following hypotheses are satisfied: 
(G1): g is Lipschitz continuous on a neighbourhood of (x(S), ¯ x(T )) ¯ . h(., x) is 
upper semi-continuous for each x ∈ Rn and there exists kh > 0 such that 
|h(t, x)−h(t, x'
)| ≤ kh|x −x'
| for all x, x' ∈ ¯x(t)+ϵB, for all t ∈ [S,T ],550 11 The Euler-Lagrange and Hamiltonian Inclusion Conditions in the Presence of. . .
(G2): F (t, x) is nonempty for each (t, x) ∈ [S,T ] × Rn, Gr F (t, .) is closed for 
each t ∈ [S,T ] and F is L × Bn measurable, 
(G3): There exists a measurable function R : [S,T ] → (0,∞) ∪ {+∞} (a ‘radius 
function’) such that R(t) ◦
B ⊂ B(t) a.e. and the following conditions are 
satisfied, 
(a): (Pseudo-Lipschitz Continuity) There exists k ∈ L1 such that 
F (t, x'
) ∩ (˙
x(t) ¯ + R(t) ◦
B) ⊂ F (t, x) + k(t)|x' − x|B,
for all x'
, x ∈ ¯x(t) + ϵB, a.e. t ∈ [S,T ], (11.2.2) 
(b): (Tempered Growth) There exist r ∈ L1, r0 > 0 and γ ∈ (0, 1) such that 
r0 ≤ r(t), γ −1r(t) ≤ R(t) a.e. and 
F (t, x) ∩ (˙
x(t) ¯ + r(t)B) /= ∅ for all x ∈ ¯x(t) + ϵB, a.e. t ∈ [S,T ]. 
Then there exist an arc p ∈ W1,1([S,T ]; Rn), μ ∈ C⊕(S, T ), a bounded 
Borel measurable function γ : [S,T ] → Rn and λ ≥ 0, satisfying the following 
conditions, in which q ∈ NBV ([S,T ]; Rn) is the function 
q(t) =

p(S) if t = S
p(t) + 
[S,t] γ (s)dμ(s) if t ∈ (S, T ] , (11.2.3) 
(i) (λ, p, μ) /= (0, 0, 0), 
(ii) p(t) ˙ ∈ co{η : (η, q(t)) ∈ NGr F (t,.)(x(t), ¯ ˙
x(t)) ¯ } , a.e. t ∈ [S,T ], 
(iii) (q(S), −q(T )) ∈ λ∂g(x(S), ¯ x(T )) ¯ + NC(x(S), ¯ x(T )) ¯ , 
(iv) q(t) · ˙
x(t) ¯ ≥ q(t) · v for all v ∈ co(Ω0(t) ∩ (˙
x(t) ¯ + B(t))) , a.e. t ∈ [S,T ], 
(v) supp{μ}⊂{t ∈ [S,T ] : h(t, x(t)) ¯ = 0}
and γ (t) ∈ ∂>
x h(t, x(t)), μ ¯ - a.e. t ∈ [S,T ]. 
Here, 
∂>
x h(t,x(t)) ¯ := co lim sup {∂xh(ti, xi) : (11.2.4) 
ti → t, xi → ¯x(t) and h(ti, xi) > 0 for each i}.
Remark 
(i): The ‘complementary slackness’ condition ‘supp{μ}⊂{t ∈ [S,T ] :
h(t, x(t)) ¯ = 0}’ in (v) is included for emphasis. It is in fact implied by the 
relation γ (t) ∈ ∂>
x h(t, x(t)) ¯ , μ-a.e. t ∈ [S,T ] characterizing the function γ , 
since this relation only makes sense if ∂>
x h(t, x(t)) ¯ is non-empty μ a.e. and 
since ∂>
x h(t, x(t)) ¯ is empty at all times t at which h(t, x(t)) < ¯ 0. 
(ii): In consequence of the unrestrictive nature of hypotheses placed on the state 
constraint function, the necessary conditions cover a range of state constrained11.2 The Euler Lagrange Inclusion 551
dynamic optimization problems, in which the state constraint is formulated in 
different ways. Examples include multiple functional inequality constraints, 
functional inequality constraints imposed on a fixed closed subset of [S,T ]
or implicit constraints x(t) ∈ A(t) for all t ∈ [S,T ], for some given 
multifunction A : [S,T ] ⇝ Rn. We refer to the remarks following the 
statement of Theorem 10.4.1, for information about how these other types of 
state constraints are subsumed in the framework of problem (P ), and how they 
affect the statement of the necessary conditions. 
(iii): (Convex Velocity Sets) Assume that 
F (t, x) is convex, for all x ∈ ¯x(t) + ϵB, a.e. t ∈ [S,T ] .
Then, in consequence of the dualization theorem (Theorem 8.7.1), the gen￾eralized Euler Lagrange condition (ii) in the theorem statement implies the 
Hamiltonian inclusion (see Theorem 8.7.2), namely 
p(t) ˙ ∈ co{−ξ : (ξ , ˙
x(t)) ¯ ∈ ∂x,pH (t, x(t), q(t)) ¯ }, a.e. t ∈ [S,T ].
Furthermore, the Weierstrass condition (iv) is valid in a stronger form, in 
which F (t, x(t)) ¯ replaces the set co(Ω0(t)∩(˙
x(t) ¯ +B(t))). This last assertion 
is confirmed in the discussion that precedes the statement of Theorem 8.7.2 in 
Chap. 8. (See also Example 4.2.) 
As in Chap. 8, we can use the ‘pseudo Lipschitz continuity and tempered growth’ 
hypothesis (G3) in Theorem 11.2.1 as a springboard for proving other versions of 
the theorem in which (G3) is replaced by alternative hypotheses which are, perhaps, 
more easily verified in particular applications. In consequence of Proposition 8.5.1, 
any of the sets of conditions (G3)*–(G3)*** listed in that proposition can be inserted 
in place of (G3), to give different versions of Theorem 11.2.1. One example of 
necessary conditions obtained in this way is: 
Corollary 11.2.2 The assertions of Theorem 11.2.1 remain valid when x¯ is a W1,1
local minimizer (i.e. B ≡ Rn) and hypothesis (G3) is replaced by: 
(G3)∗∗∗: There exist α > 0 and non-negative measurable functions k and β such 
that k and t → β(t)kα(t) are integrable on [S,T ] and, for each N ≥ 0, 
F (t, x'
) ∩ (˙
x(t) ¯ + NB) ⊂ F (t, x) + (k(t) + β(t)Nα)|x' − x|B,
for all x'
, x ∈ ¯x(t) + ϵB and a.e. t ∈ [S,T ]. 
Furthermore, the regular velocity set Ω0(t) = F (t, x(t)) ¯ and, in consequence, 
the Weierstrass condition (iv) takes the form 
q(t) · ˙
x(t) ¯ ≥ q(t) · v for all v ∈ F (t, x(t)) ¯ a.e. t ∈ [S,T ].552 11 The Euler-Lagrange and Hamiltonian Inclusion Conditions in the Presence of. . .
11.3 Proof of Theorem 11.2.1 
Step 1 (Necessary Conditions for a Finite Lagrangian Problem) 
We begin by deriving necessary conditions for a W1,1 local minimizer x' for the 
following problem: 
(F L)
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
Minimize J (x) :=

�(x(S), x(T ))	
∨
  T
S L(t, x(t), x(t))dt ˙
	
∨

max
t∈[S,T ]
h(t, x(t))	
+  T
S L0(t, x(t), x(t))dt ˙ + δ |x(S) − x'
(S)|
over x ∈ W1,1([S,T ]; Rn) such that
x(t) ˙ ∈ D(t) a.e. t ∈ [S,T ] .
The data comprise an interval [S,T ], a number δ > 0, functions L : [S,T ] × Rn ×
Rn → R, L0 : [S,T ]×Rn×Rn → R, � : Rn×Rn → R and h : [S,T ]×Rn → R, 
and a multifunction D : [S,T ] ⇝ Rn. 
A W1,1 local minimizer x' for (FL) is defined by analogy with our earlier 
definition for (P ), except that, now, we impose the additional requirement that 
t → (L,L0)(t, x'
(t), x˙'
(t)) is integrable. 
Proposition 11.3.1 Let x' be a W1,1 local minimizer for (FL). Assume that the 
following hypotheses are satisfied: there exist ϵ > 0, N > 0, kh > 0 and a non￾negative function k˜ ∈ L1(S, T ) such that 
(FL0): D has closed values, Gr D is L × Bn measurable and D(t) ⊂ ˙x'
(t) + NB
a.e., 
(FL1): � is Lipschitz continuous on a neighbourhood of (0, 0). h(., x) is upper 
semi-continuous for each x ∈ Rn such that 
|h(t, x) − h(t, y)| ≤ kh|x − y| for all x, y ∈ ¯x(t) + ϵB, t ∈ [S,T ],
(FL2): (L,L0)(., x, v) is L-measurable for each (x, v) ∈ Rn × Rn, 
(FL3): |(L,L0)(t, y, w) − (L,L0)(t, x, v)| ≤ k(t)( ˜ |y − x|+|w − v|),
for all y, x ∈ ϵB and w, v ∈ D(t), a.e. t ∈ [S, T ]. 
Assume, furthermore, that 
||x'
||W1,1 < ϵ.
Then there exist an arc p ∈ W1,1([S, T ]; Rn), μ ∈ C⊕(S, T ), a bounded Borel 
measurable function γ : [S, T ] → Rn, λ ≥ 0 and λ(1) ≥ 0, satisfying the following 
conditions, in which q ∈ NBV ([S, T ]; Rn) is the function 
q(t) =

p(S) if t = S
p(t) + 
[S,t] γ (s)dμ(s) if t ∈ (S, T ] .11.3 Proof of Theorem 11.2.1 553 
(i): λ + λ(1) + ||μ||TV = 1, 
(ii): 
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
(a) : ˙p(t)∈λ(1)
co ∂xL(t, x'
(t), x˙'
(t))+co ∂xL0(t, x'
(t), x˙'
(t)) a.e. t∈[S, T ]
and 
(b) : ˙p(t)∈co{η :(η, q(t))∈λ(1)
∂x,vL(t, x'
(t), x˙'
(t))+∂x,vL0(t, x'
(t), x˙'
(t))},
for a.e. such that x˙'
(t) ∈ ◦
D(t)
(iii) (q(S), −q(T )) ∈ λ∂�(x'
(S), x'
(T )) + δB × {0}, 
(iv) q(t) · ˙x'
(t) − λ(1)
L(t, x'
(t), x˙'
(t)) − L0(t, x'
(t), x˙'
(t)) ≥
q(t) · v − λ(1)
L(t, x'
(t), v) − L0(t, x'
(t), v), 
for all v ∈ D(t), a.e. t ∈ [S, T ], 
(v) supp{μ}⊂{t ∈ [S, T ] : h(t, x'
(t)) = max 
t∈[S,T ]
h(t, x'
(t))}
and γ (t) ∈ ∂>
x h(t, x'
(t)) for μ-a.e. t ∈ [S, T ]. 
Furthermore 

 T
S
L(t, x'
(t), x˙'
(t))dt < �(x'
(S), x'
(T )) ∨ max
t∈[S,T ]
h(t, x'
(t)) =⇒ λ(1) = 0 ,
(11.3.1) 
max
t∈[S,T ]
h(t, x'
(t)) < �(x'
(S), x'
(T )) ∨

 T
S
L(t, x'
(t), x˙'
(t))dt =⇒ μ = 0
(11.3.2) 
and 
�(x'
(S), x'
(T )) < max
t∈[S,T ]
h(t, x'
(t)) ∨

 T
S
L(t, x'
(t), x˙'
(t))dt =⇒ λ = 0 .
(11.3.3) 
Proof of Proposition 11.3.1 Since x' is a W1,1 local minimizer, there exists β > 0 
such that x' is minimizing, when we add to (FL) the constraint 
|x(S) − x'
(S)| + 
 T
S
| ˙x(t) − ˙x'
(t)|dt ≤ β . (11.3.4) 
Bearing in mind that ||x'
||W1,1 < ϵ, we can arrange, by choosing β sufficiently 
small, that ||x||W1,1 < ϵ (and hence also that ||x||L∞ < ϵ), for all x satisfy￾ing (11.3.4). Here, ϵ is the constant appearing in proposition statement. 
As justified in the proof of Proposition 8.6.1 in Chap. 8 (the analysis is unaffected 
by the presence of a pathwise state constraint), we may assume that � is Lipschitz 
continuous, h(t, .) is Lipschitz continuous with Lipschitz constant kh for each t ∈
[S, T ] and (FL3) has been replaced by the hypothesis: 
(FL3)'
: there exist non-negative functions k, c ∈ L1 such that k(t) ≥ 1 a.e. and

|(L,L0)(t, y, w) − (L,L0)(t, x, v)| ≤ k(t)(|y − x|+|w − v|) and 
(L,L0)(t, x, v)| ≤ c(t)
for all y, x ∈ Rn and w, v ∈ D(t), a.e. t ∈ [S, T ] .554 11 The Euler-Lagrange and Hamiltonian Inclusion Conditions in the Presence of. . .
Take a sequence Ki ↑ ∞. Write L1 
k for the Banach space of measurable functions 
w such that t → k(t)w(t) is integrable on [S, T ], equipped with the k-weighted L1 
norm ||w||L1 
k := ||kw||L1 . Write 
W := {(ξ , w, v) ∈ Rn × L1
k × L1
k : v(t) ∈ D(t) a.e., ||xξ,v−x'
||W1,1 ≤ β},
in which xξ,v(t) = ξ +  t
S v(s)ds. Define 
||(ξ , w, v)||k := |ξ | + ||kw||L1 + ||kv||L1 .
For each i, set 
J˜
i(ξ , w, v) :=

�(xξ,v(S), xξ,v(T ))	
∨
 

L(t, w(t), v(t))dt	
∨

max
t∈[S,T ]
h(t, x(t))	
+


L0(t, w(t), v(t))dt + δ|xξ,v, (S) − x'
(S)|
+Ki


k(t)|xξ,v(t) − w(t)|
2dt.
Taking note of the fact that the mapping (ξ , v) → max 
t∈[S,T ]
h(t, xξ,v(t)) is a 
continuous function on (W, ||.||k), we can verify, by a simple adaptation of the ‘state 
constraint-free’ analysis in Chap. 8: 
Claim: For each i, (W, ||.||k) is a complete metric space and J˜
i is lower semi￾continuous on (W, ||.||k). There exists a sequence of non-negative numbers αi ↓ 0 
such that, for each i, 
J˜
i(x'
(S), x'
, x˙'
) ≤ inf
(ξ ,w,v)∈W
J˜
i(ξ , w, v) + α2
i .
By Ekeland’s theorem (Theorem 3.3.1), there exists (ξi, wi, vi) ∈ W which 
minimizes 
Ji(ξ , w, v) := J˜
i(ξ , w, v) + αi||(ξ , w, v) − (ξi, wi, vi)||k
over W. Furthermore 
||(ξi, wi, vi) − (x'
(S), x'
, x˙'
)||k ≤ αi, for each i . (11.3.5)11.3 Proof of Theorem 11.2.1 555 
We also know from Ekeland’s theorem that J˜
i(ξi, wi, vi) ≤ Ji(x'
(S), x'
, x˙'
)(< ∞). 
This implies that  k(t)|xi(t) − wi(t)|
2dt < ∞, where xi := xξi,vi . Since xi is 
bounded, it follows that t → k(t)w2 
i (t) is integrable. 
Taking note of the fact that the mapping z → �(x0, x1) ∨ z is monotone, we may 
deduce from the preceding analysis that ((xi, yi, ri, zi),(vi, wi)) is a minimizer for 
the dynamic optimization problem 
(Ei)
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize �(x(S), x(T )) ∨ r(T ) ∨ z(T ) + δ|x(S) − x'
(S)|
+  L0(t, w(t), v(t))dt + αi

|x(S) − xi(S)| +  k(t)(|v(t) − vi(t)|
+|w(t) − wi(t)|)dt	
+ Ki
 k(t)|x(t) − w(t)|
2dt.
over arcs (x, y, r, z) ∈ W1,1 × W1,1 × W1,1 × W1,1 satisfying
x(t) ˙ = v(t), y(t) ˙ = |v(t) − ˙x'
(t)| ,r(t) ˙ = L(t, w(t), v(t)), z(t) ˙ = 0 a.e.,
w(t) ∈ Rn, v(t) ∈ D(t) a.e.,
h(t, x(t)) − z(t) ≤ 0 for all t ∈ [S,T ],
y(S) = r(S) = 0 and |x(S) − x'
(S)| + y(T ) ≤ β.
Here 
yi(t) :=  t
S |vi(s)− ˙xi(s)|ds, zi ≡ max 
t∈[S,T ]
h(t, xi(t)))
and ri(t) :=  t
S Li(s, wi(s), vi(s)))ds. 
(Notice that the problem (Ei) is not strictly equivalent to Min{Ji(ξ , w, v) :
(ξ , w, v) ∈ W} because it has domain comprising control functions (v, w) from 
the larger set L1 × {meas. functions w : [S, T ] → Rn}. However the two problems 
are equivalent, in the sense of having a common set of minimizers since, for any 
control functions (w, v) ∈/ L1 
k × L1 
k the cost in (Ei) is +∞, as is easily shown, and 
such control functions cannot be candidates for minimizing controls.) 
In consequence of (11.3.5), we know that, for some subsequence, 
||xi → x'
||L∞ → 0 and (vi, wi) → (x˙'
, x'
) in L1 and a.e.
and yi(.) = 
[S,.] |vi(s) − ˙x'
(s)|ds → 0 uniformly. It follows that, for i sufficiently 
large, |xi(S)−x'
(S)|+ T
S | ˙xi(t)− ˙x'
(t)|dt < β . This tells as that the final constraint 
in problem (Ei) is not active. 
Fix i. The foregoing dynamic optimization problem (Ei) is one to which the state 
constrained maximum principle Theorem 10.4.1 is applicable, following absorption 
of the integral cost term into the dynamic constraint by state augmentation. The 
relevant hypotheses are satisfied. (To check hypothesis (H1) in the maximum 
principle, we make use of the facts that t → k(t)w2 
i (t) and t → k(t)vi(t) are 
integrable functions.) 
Because the inequality constraint is inactive, the costate trajectory component 
associated with y must be zero. Let us identify pi, p(1)
i and p(2)
i as the costate556 11 The Euler-Lagrange and Hamiltonian Inclusion Conditions in the Presence of. . .
trajectories associated with the remaining states x, r and z. (p(1)
i and p(2)
i are 
constant arcs.) Write λ(c) for the cost multiplier. A preliminary analysis of the 
costate inclusion and the transversality condition of the state constrained maximum 
principle based on the max rule, tells us that, for some μi ∈ C⊕(S, T ), bounded 
Borel measurable function γi : [S, T ] → Rn and λ(c) ≥ 0, λi ≥ 0 λ(1)
i ≥ 0 and 
λ(2)
i ≥ 0 such that λi + λ(1)
i + λ(2)
i = 1 the following relations hold: 
(pi(S), −pi(T ) −


[S,T ]
γi(s)dμi(s))
= λiλ(c)∂�(xi(S), xi(T )) + (δ + αi)λ(c)B × {0},
− ˙pi(t) = −2λ(c)Kik(t)(xi(t) − wi(t)), a.e. t ∈ [S,T ] ,
−p(1) = λ(1)
i λ(c), p(2) = 0 and 

[S,T ]
dμi(s) = λ(2)
i λ(c) .
Furthermore, (λ(c), pi, p(1)
i , p(2)
i , μi) /= (0, 0, 0, 0, 0). We now observe that λ(c) /=
0. Indeed, if this were not true, it would follow from the above relations and 
Gronwall’s lemma that (pi, p(1)
i , p(2)
i , μi) = (0, 0, 0, 0), which contradicts the 
fact that costate arcs cannot all vanish. Since λ(c) /= 0, we can scale the Lagrange 
multipliers to ensure that λ(c) = 1. Notice then that ||μi||T V = 
[S,T ] dμi(s) = λ(2)
i . 
We have, after making these changes, 
(A): λi + λ(1)
i + ||μi||T V = 1, 
(B): − ˙pi(t) = −2Ki k(t)(xi(t) − wi(t)), a.e. t ∈ [S, T ], 
(C): (qi(S), −qi(T )) ∈ λi∂�(xi(S), xi(T )) + (δ + αi)B × {0}, 
(D): (w, v) → qi(t) · v − λ(1)
i L(t, w, v) − L0(t, w, v)
−αik(t)(|v − vi(t)|+|w − wi(t)|) − Kik(t)|xi(t) − w|
2 
achieves its maximum at (wi(t), vi(t)) over (w, v) ∈ Rn × D(t), 
a.e. t ∈ [S, T ], 
(E): supp{μi}⊂{t ∈ [S, T ] : h(t, xi(t)) = max 
t∈[S,T ]
h(t, xi(t))} and 
γ (t) ∈ ∂>
x h(t, xi(t)) for μi-a.e. t ∈ [S, T ] .
Here, qi(t) =

pi(S) if t = S
pi(t) + 
[S,t] γi(s)dμi(s) if t ∈ (S, T ] .
Next we investigate the consequences of condition (D). Take any t ∈ [S, T ] at 
which this condition is satisfied. The specified function with constrained maximizer 
(wi(t), vi(t)) is Lipschitz continuous on a neighbourhood of (wi(t), vi(t)); write 
the Lipschitz constant ρi(t). According to the exact penalization principle then, 
(wi(t), vi(t)) is also an unconstrained maximizer of the function: 
(w, v) → qi(t) · v − λ(1)
i L(t, w, v)
−L0(t, w, v) − αik(t)(|v − vi(t)|+|w − wi(t)|)
−Kik(t)|xi(t) − w|
2 − ρi(t)dD(t)(v).11.3 Proof of Theorem 11.2.1 557 
It follows that 
(0, qi(t)) ∈ λ(1)
i ∂x,vL(t, wi(t), vi(t)) + ∂x,vL0(t, wi(t), vi(t))
+αik(t)(B × B)+(2Ki k(t)(xi(t) − wi(t)), 0) + {0} × ρi(t)∂dD(t)(v). 
This relation combines with (B) to give: 
(p˙i(t), qi(t)) ∈ λ(1)
i ∂x,vL(t, wi(t), vi(t)) + ∂x,vL0(t, wi(t), vi(t))
+αik(t)(B × B) + {0} × ρi(t)∂dD(t)(vi(t)) . (11.3.6) 
Fix v = vi(t). Then 
w → −λ(1)
i L(t, w, vi(t))−L0(t, w, vi(t))−αik(t)|w−wi(t)|−Kik(t)|xi(t)−w|
2
achieves its maximum at wi(t) over all w ∈ Rn. In view of (B), this implies 
p˙i(t) ∈ λ(1)
i ∂xL(t, wi(t), vi(t)) + ∂xL0(t, wi(t), vi(t)) + αik(t)B. (11.3.7) 
Fix w = wi(t). Then 
v → qi(t) · v − λ(1)
i L(t, wi(t), v) − L0(t, wi(t), v) − αik(t)|v − vi(t)|
achieves its maximum at vi(t) over v ∈ D(t). We see that, for a.e. t ∈ [S, T ], 
qi(t) · vi(t) − λ(1)
i L(t, wi(t), vi(t)) − L0(t, wi(t), vi(t))
≥ qi(t) · v−λ(1)
i L(t, wi(t), v)−L0(t, wi(t), v)−αik(t)|v−vi(t)| for all v∈D(t).
(11.3.8) 
Since (L,L0)(t, ., v) is Lipschitz continuous with Lipschitz constant k(t) for all 
v ∈ D(t), (11.3.7) implies that, for i sufficiently large, | ˙pi(t)| ≤ (2+αi)k(t). Noting 
that the pi(S)’s, λi’s, λ(1)
i ’s and ||μi||T V ’s are uniformly bounded (see (A) and (C)), 
we can arrange, by subsequence extraction, that pi → p uniformly, p˙i → ˙p weakly 
in L1, μi → μ weakly∗, dqi → dq weakly∗, λi → λ, λ(1)
i → λ(1)
, some p ∈ W1,1, 
μ ∈ C⊕(S, T ), λ ≥ 0, λ(1) ≥ 0 and q ∈ NBV ([S, T ]; Rn) satisfying 
q(t) =

p(S) if t = S
p(t) + 
[S,t] γ (s)dμ(s) if t ∈ (S, T ] .
From (A) 
λ + λ(1) + ||μ||T V = 1 .
This is condition (i) of the proposition statement. 
Notice next that, if  T
S L(t, x'
(t), x˙'
(t))dt<
�(x'
(S), x'
(T ))	
∨

max 
t∈[S,T ]
h(t, x'
(t))	
, 
we know that558 11 The Euler-Lagrange and Hamiltonian Inclusion Conditions in the Presence of. . .

 T
S
L(t, xi(t), x˙i(t))dt < 
�(xi(S), xi(T ))	
∨

max
t∈[S,T ]
h(t, xi(t))	
for all i sufficiently large.
But then, by the sum rule, λ(1)
i = 0 for all i sufficiently large. It follows that λ(1) =
0. This confirms (11.3.1). Likewise we show, with the help of the sum rule, that 
max
t∈[S,T ]
h(t, xi(t)) < �(x'
(S), x'
(T )) ∨

 T
S
L(t, x'
(t), x˙'
(t))dt =⇒ μ = 0,
�(x'
(S), x'
(T )) < max
t∈[S,T ]
h(t, x'
(t)) ∨

 T
S
L(t, x'
(t), x˙'
(t))dt =⇒ λ = 0 .
We have confirmed (11.3.1), (11.3.2) and (11.3.3). Now observe that, in the 
limit, (11.3.8) implies 
q(t) · v'
(t) − λ(1)
L(t, x'
(t), v'
(t)) − L0(t, x'
(t), v'
(t))
≥ q(t) · v − λ(1)
L(t, x'
(t), v) − L0(t, x'
(t), v) for all v ∈ D(t) a.e..
This is (iv). (C) yields, in the limit, (q(S), −q(T )) ∈ λ∂�(x'
(S), x'
(T ))+δB× {0}, 
which is (iii). With the help of Proposition 10.3.1 of Chap. 10, we deduce from (E) 
the existence of a Borel measurable function γ such that 
supp{μ}⊂{t ∈ [S, T ] : h(t, x'
(t)) = max 
t∈[S,T ]
h(t, x'
(t))} and 
γ (t) ∈ ∂>
x h(t, x'
(t)) for μ-a.e. t ∈ [S, T ] .
We have confirmed (v). Notice next that, since dqi → dq in the weak* topology and 
qi(S) → q(S), we know that qi(t) → q(t) for all times t ∈ [S, T ] in some set of full 
measure that contains S and T . Using this fact, we can carry out an almost identical 
analysis to that appearing in the proof Proposition 8.6.1 (necessary conditions for a 
generalized finite Lagrangian problem without a pathwise state constraint) to pass 
to the limit as i → ∞ in (11.3.6) and obtain the two relations 
p(t) ˙ ∈ co{η : (η, q(t)) ∈ λ(1)
∂x,vL(t, x'
(t), x˙'
(t)) + ∂x,vL0(t, x'
(t), x˙'
(t)), 
a.e. t ∈ [S, T ] such that x˙'
(t) ∈ ◦
D(t), 
p(t) ˙ ∈ λ(1)
co ∂xL(t, x'
(t), x˙'
(t)) + co ∂xL0(t, x'
(t), x˙'
(t)) a.e. t ∈ [S,T ] .
This is condition (ii). All the assertions of Proposition 11.3.1 have been verified. The 
proof is complete. ⨅⨆11.3 Proof of Theorem 11.2.1 559 
Step 2 (Hypothesis Reduction) 
Analogous arguments to those used in Chap. 8 to prove the Euler Lagrange inclusion 
and related conditions, in the case that there are no state constraints, permit us to 
impose, without loss of generality, some additional hypotheses, as is asserted in the 
following proposition. 
Proposition 11.3.2 Assume the assertions of Theorem 11.2.1 are valid under the 
hypotheses: (G1), (G2) and (G3)'
, and when x¯ ≡ 0. Then the assertions of 
Theorem 11.2.1 are valid under hypotheses (G1)–(G3) alone. 
Here, (G3)'
, for the given parameter ϵ > 0, is the hypothesis 
(G3)'
: there exist R > 0, k ∈ L1 and γ ' ∈ (0, 1) such that R ◦
B ⊂ B(t) for a.e. 
t ∈ [S, T ] and the following conditions are satisfied: 
(i): F (t, x'
) ∩ (˙
x(t) ¯ + R ◦
B) ⊂ F (t, x) + k(t)|x' − x|B for all x'
, x ∈ ϵB,
a.e. t ∈ [S, T ], 
(ii): F (t, x) ∩ (˙
x(¯ t) + γ '
R B) /= ∅ for all x ∈ ϵB, a.e. t ∈ [S, T ]. 
Step 3 (An Integral Penalty Function) 
We shall make use of the modified integral penalty function, earlier used to derive 
the Euler Lagrange-type conditions in the absence of state constraints, to take 
account of the dynamic constraint also in a state constrained setting. 
Choose η ∈ (0, 1/2) such that (G3)
'
is satisfied with γ ' = (1 − 2η). Fix N > R
and define 
EN (t) := {e ∈ Ω0(t)∩B(t) : R ≤ |e| ≤ N},
in which Ω0(t) is the regular velocity set (11.2.1), namely 
Ω0(t) := {e ∈ F (t, x(t)) ¯ : F (t, .) is pseudo-Lipschitz near (x(t), e) ¯ }.
We can construct a multifunction EN
discrete : [S, T ] ⇝ Rn such that 
(a): EN
discrete(t) is an empty or finite set for each t ∈ [S, T ], 
(b): EN
discrete(t) is measurable, 
(c): EN
discrete(t) ⊂ EN (t) ⊂ EN
discrete(t) + N−1 B, for each t ∈ [S, T ] such that 
EN (t) /= ∅, 
(d): EN
discrete(t) ⊂ EN+1 
discrete(t) for all integers N > R. 
Define 
θ (t) :=
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
min{|e − e'
| : e, e' ∈ EN
discrete(t) and e /= e'
}
∧ inf{d∂B(t)(e) : e ∈ EN
discrete(t)}
if EN
discrete(t) contains at least two elements,
+∞ otherwise .560 11 The Euler-Lagrange and Hamiltonian Inclusion Conditions in the Presence of. . .
For δ ∈ (0, ηR), define 
Eδ
regular(t) := {e ∈ EN
discrete(t) : F (t, .)is pseudo-Lipschitz near (e, 0)
(with parameters R ≥ δ, ϵ ≥ δ and k ≤ δ−1) and θ (t) ≥ δ}.
By construction, Eδ
regular(t) ⊂ EN
discrete(t) ⊂ NB. Define 
Dδ
(t) := (1 − η)RB ∪ {e ∈ e0 +
1
3
δB : e0 ∈ Eδ
regular(t)}
in which the right side is interpreted as (1 − η)RB if Eδ
regular(t) = ∅. 
Dδ(t) is a finite union of disjoint, closed balls. These comprise (1 − η)RB and 
elements from a (possibly empty) collection of disjoint closed balls each with origin 
outside R
◦
B. Moreover, Dδ(t) ⊂ B(t) for a.e. t ∈ [S, T ]. 
Now consider the multifunction S : [S, T ] × Rn ⇝ Rn
S(t, x) := {(χ (|e|)e : e ∈ F (t, x)}, (11.3.9) 
in which χ : [0,∞) → [1,∞) is the function 
χ (d) := 1 +
4(1 − η)
ηR [d − (1 − η)R]
+ .
Define ρS : [S, T ] × Rn × Rn → [0,∞) to be 
ρS(t, x, v) := 
dS(t,x)(v) if |v| ≤ (1 − η)R ,
dF (t,x)(v) if |v| > (1 − η)R .
Relevant properties of ρS are assembled in Lemma 8.6.5 of Chap. 8. 
Step 4 (Completion of the Proof) 
Under the supplementary hypothesis (G3)'
, the W1,1 local minimizer for (P ) of 
interest is x¯ ≡ 0. Let β ∈ (0, ϵ) be such that x¯ is minimizing w.r.t. all admissible F
trajectories x such that ||x − ¯x||W1,1 (= ||x||W1,1 ) ≤ β. Take η > 0 and δ > 0 as in 
Step 3. 
For t ∈ [S, T ] and e ∈ Dδ(t) define 
φ(t, e) :=  1
2 (|e| − (1 − 2η)R) ∨ 0 if e ∈ (1 − η)RB
1
2 (|e − e0| − δ/6) ∨ 0 if e ∈ e0 + 1
3 δ B for some e0 ∈ Eδ
regular(t) .
Note the following properties of φ, each of which is a simple consequence of the 
definition of this function: for any t ∈ [S, T ]11.3 Proof of Theorem 11.2.1 561 
φ(t, e) = 0 if |e| ≤ (1 − 2η)R
φ(t, .) is locally Lipschitz continuous on Dδ(t)
with Lipschitz constant 1/2
φ(t, e) ≥ ηR
2 ∧ δ
12 if e ∈ ∂Dδ(t) .
⎫
⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎭
(11.3.10) 
Take αi ↓ 0 and, for each i, consider the optimization problem: 
(Pi)
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize

�i(x(S)), x(T ))	
∨
  T
S ρS(t, x(t), x(t))dt ˙
	
∨

max t∈h(t,x(t))
h(t, x(t))	
+  T
S φ(t, x(t))dt ˙
over arcs x ∈ W1,1 such that
x(t) ˙ ∈ Dδ(t) a.e. t ∈ [S,T ] ,
|x(S)| + 
[S,T ] | ˙x(t)|dt ≤ β .
Here, 
�i(x0, x1) := 
g(x0, x1) − g(0, 0) + α2
i
	
∨ dC(x0, x1)
and ρS(t, x, v) is the modified penalty integrand of Step 3, with parameters η ∈
(0, 1/2) and δ > 0 as earlier chosen. This problem can be expressed as 
Minimize {Ji(x) : x ∈ S}
in which 
S := {x ∈ W1,1 : x satisfies the constraints of (Pi)}
and 
Ji(x) := 
�i(x(S)), x(T ))	
∨
 
 T
S
ρS(t, x(t), x(t))dt ˙
	
∨

max
t∈[S,T ]
h(t, x(t))	
+

 T
S
φ(t, x(t))dt. ˙
S is complete w.r.t. the metric induced by the ||.||W1,1 norm on elements of S. Ji is 
continuous w.r.t. to this metric. Since x¯ ≡ 0 is an α2 
i minimizer for (Pi) we can, by 
Ekeland’s theorem, find xi ∈ S such that xi is a minimizer for 
Minimize {Ji(x) + αi

|x(S) − xi(S)| + 

[S,T ]
| ˙x(t) − ˙xi(t)|dt : x ∈ S}562 11 The Euler-Lagrange and Hamiltonian Inclusion Conditions in the Presence of. . .
and 
|xi(S)| + 

[S,T ]
| ˙xi(t)|dt ≤ αi . (11.3.11) 
We note that 

�i(xi(S)), xi(T ))	
∨
 
 T
S
ρS(t, xi(t), x˙i(t))dt	
∨

max
t∈[S,T ]
h(t, x(t))	
> 0
(11.3.12) 
for, otherwise, we would have 

g(xi(S), xi(T )) − g(0, 0) + ϵ2
i
	
∨ dC(xi(S), xi(T ))
∨
 
 T
S
ρS(t, xi(t), x˙i(t))dt	
∨

max
t∈[S,T ]
h(t, x(t))	
= 0 .
This implies ρS(t, xi(t), x˙i(t)) = 0 a.e.. But then x˙i(t) ∈ F (t, xi(t)) a.e., in 
consequence of Lemma 8.6.5 of Chap. 8. By definition of Dδ(t), x˙i(t) ∈ B(t). 
The preceding relations also tell us that dC(xi(S), xi(T )) = 0, which implies 
(xi(S), xi(T )) ∈ C, maxt∈[S,T ] h(t, x(t))dt ≤ 0 and g(xi(S), xi(T )) ≤ g(0, 0) −
α2 
i . Since xi ∈ S, we know that ||xi||W1,1 ≤ β. But then xi is an admissible 
F trajectory, with x˙i(t) ∈ B(t) for a.e. t ∈ [S, T ], that violates the W1,1 local 
optimality of x¯ ≡ 0. (11.3.12) is confirmed. 
We have shown that xi is a W1,1 local minimizer for the problem 
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
Minimize �i(x(S), x(T )) ∨
  T
S ρS(t, x(t), x(t))dt ˙
	
∨

max
t∈[S,T ]
h(t, x(t))	
+ αi

|x(S) − xi(S)| +  T
S | ˙x(t) − ˙xi(t)|dt	
+  T
S φ(t, x(t))dt ˙
over x ∈ W1,1 satisfying
x(t) ˙ ∈ Dδ(t) a.e. t ∈ [S,T ] .
This problem is an example of the finite Lagrangian problem of Step 1. The 
hypotheses are satisfied for the application of Proposition 11.3.1. 
Using the max rule to estimate ∂�i, we deduce that there exist pi ∈ W1,1, μi ∈
C⊕(S, T ), a bounded Borel measurable function γi : [S, T ] → Rn λ¯i ∈ [0, 1], 
λ(1)
i ∈ [0, 1] and σi ∈ [0, 1] such that the following conditions are satisfied, in 
which qi ∈ NBV ([S, T ]; Rn) is the function 
qi(t) =

pi(S) if t = S
pi(t) + 
[S,t] γi(s)dμi(s) if t ∈ (S, T ] ,
(A)'
: λ¯i + λ(1)
i + ||μi||T V = 1 ,11.3 Proof of Theorem 11.2.1 563 
(B)'
: p˙i(t) ∈ co{η : (η, qi(t)) ∈ λ(1)
i ∂x,vρS(t, xi(t), x˙i(t))+αiB × {0}}, 
for a.e. t ∈ [S, T ] such that either x˙i(t) ∈ ˙
x(t) ¯ + (1 − 2η)R ◦
B, 
(C)'
: (qi(S), −qi(T )) ∈ λ¯iσi∂g(xi(S), xi(T )) + λ¯i(1 − σi)∂dC(xi(S), xi(T ))
+ αiB × {0}, 
(D)'
: qi(t) · ˙xi(t) − λ(1)
i ρS(t, xi(t), x˙i(t)) − φ(t, x˙i(t)) ≥
qi(t) · v − λ(1)
i ρS(t, xi(t), v) − φ(t, v) − αi|v − ˙xi(t)|, 
for all v ∈ Dδ(t) , a.e. t ∈ [S, T ], 
(E)'
: supp{μi}⊂{t ∈ [S, T ] : h(t, x¯i(t)) = max 
t∈[S,T ]
h(t, xi(t))} and 
γi(t) ∈ ∂>
x h(t, xi(t)) for μi - a.e. t ∈ [S, T ]. 
Note that 
λ¯i(1−σi)∂dC(xi(S), xi(T )) = λ¯i(1−σi)

∂dC(xi(S), xi(T ))∩∂B
	
, (11.3.13) 
in which we interpret the right side of (11.3.13) as {0}, if λ¯i(1 − σi) = 0. This is 
because, if (xi(S), xi(T )) ∈/ C, then ∂dC(xi(S), xi(T )) ⊂ ∂B. If, on the other hand, 
(xi(S), xi(T )) ∈ C then dC(xi(S), xi(T ))=0. In view of (11.3.12), the max rule and 
condition (11.3.3) of Proposition 11.3.1, either λ¯i = 0 or 1 − σi = 0; in both these 
cases λ¯i(1 − σi) = 0, then, (11.3.13) is true. 
Notice also that, from (11.3.12) and condition (11.3.2) of Proposition 11.3.1, 
maxt∈[S,T ] h(t, xi(t)) ≤ 0 =⇒ μi = 0. It follows that ∂xh>(t, xi(t)) =
co ∂xh(t, xi(t)) at all points t in the support of μi and 
supp{μi}⊂{t ∈ [S,T ] : h(t, x¯i(t)) > 0}.
Now write λi := σiλ¯i and λ(2) := (1 − σi)λ¯i. Then λi ∈ [0, 1], λ(2) ∈ [0, 1] and 
λi + λ(1)
i + λ(2)
i + ||μi||T V = λ¯i + λ(1) + ||μi||T V = 1 .
Furthermore, we can deduce from (11.3.12), and condition (11.3.1) in Proposi￾tion 11.3.1 
‘


[S,T ]
ρS(t, xi(t), x˙i(t))dt = 0’ =⇒ ‘λ(1)
i = 0’ . (11.3.14) 
With the benefit of the preceding analysis, we can replace (A)'
–(E)' by 
(A): λi+λ(1)
i +λ(2)
i +||μi||T V = 1 and λ(1)
i = 0 if 
[S,T ] ρS(t, xi(t), x˙i(t))dt = 0, 
(B): p˙i(t) ∈ co{η : (η, qi(t)) ∈ λ(1)
i ∂x,vρS(t, xi(t), x˙i(t))+{0} × αiB}
for a.e. t ∈ [S, T ] such that x˙i(t) ∈ ˙
x(t) ¯ + (1 − 2η)R ◦
B,564 11 The Euler-Lagrange and Hamiltonian Inclusion Conditions in the Presence of. . .
(C): (qi(S), −qi(T )) ∈ λi∂g(xi(S), xi(T )) + λ(2)
i (∂dC(xi(S), xi(T )) ∩ ∂B )
+αiB × {0}, 
(D): qi(t) · ˙xi(t) − λ(1)
i ρS(t, xi(t), x˙i(t)) − φ(t, x˙i(t)) ≥
qi(t) · v − λ(1)
i ρS(t, xi(t), v) − φ(t, v) − αi|v − ˙xi(t)|, for all v ∈ Dδ(t), 
a.e. t ∈ [S, T ], 
(E): supp{μi}⊂{t ∈ [S, T ] : h(t, xi(t)) > 0} and γi(t) ∈ co ∂>
x h(t, xi(t)) for 
μi - a.e.. t ∈ [S, T ]. 
We also know from condition (ii) (b) of Proposition 11.3.1 that 
p˙i(t) ∈ λ(1)
i ∂xρS(t, xi(t), x˙i(t))t ∈ [S,T ]. a.e. (11.3.15) 
Since ρS(t, ., x˙i(t)) is Lipschitz continuous with Lipschitz constant k(t) ˜ , where k˜ is 
the integrable function of Lemma 8.6.5 (iv), we have 
| ˙pi(t)| ≤ λ(1)
i k(t) ˜ a.e. t ∈ [S,T ].
From (C) we have |pi(S)| ≤ kg +1+αi. It follows from Gronwall’s lemma and the 
preceding two relations that the family of costate trajectories pi, i = 1, 2,... are 
uniformly bounded and their derivatives are uniformly integrably bounded. 
We next seek to establish a uniform positive lower bound on the magnitude of 
(qi, λi, μi). We need to consider separately two possible cases, one of which must 
occur: 
(i): ρS(t, xi(t), x˙i(t)) = 0 a.e. 
(ii): ρS(t, xi(t), x˙i(t)) > 0 on a set of positive L-measure. 
Consider case (i). We know, from (11.3.14), that λ(1)
i = 0. But then, from conditions 
(A) and (C), and since √2||qi||L∞ ≥ |(qi(S), qi(T ))|, 
√
2||qi||L∞ + (1 + kg)λi + ||μ||T V ≥ 1 − αi.
Consider case (ii). In this case there is a time t ∈ [S, T ] such that condition 
(C) is satisfied, ρS(t, xi(t), x˙i(t)) > 0 and x˙i(t) ∈ Dδ(t)B. We see that x˙i(t) ∈/
F (t, xi(t)). This is obviously true if | ˙xi(t)|> (1 − η)R because, in this case, 
ρS(t, xi(t), x˙i(t)) = dF (t,xi(t))(x˙i(t))). So we can assume that | ˙xi(t)| ≤ (1 − η)R. 
Suppose, contrary to our assertion, x˙i(t) ∈ F (t, xi(t)). Then χ (| ˙xi(t)|) = 1. It 
follows that ρS(t, xi(t), x˙i(t)) ≤ |˙xi(t) − χ (| ˙xi(t)|)x˙i(t)| = |˙xi(t) − ˙xi(t)| = 0, 
which is a contradiction. 
We now distinguish the two possible situations: 
(a): x˙i(t) ∈ ∂Dδ(t). 
In view of (G3)'
, there exists v0 ∈ F (t, xi(t)) such that |v0| ≤ (1 − 2η)R. 
Then φ(t, v0) = 0 and ρS(t, xi(t), v0) ≤ |v0 − χ (|v0|)v0|=|v0 − v0|) = 0.11.3 Proof of Theorem 11.2.1 565 
Since x˙i(t) ∈ ∂Dδ(t), it follows from (11.3.10) that 
φ(t, x˙i(t)) ≥ (ηR/2) ∧ (δ/12) .
Using these relations, noting that ρS(t, xi(t), x˙i(t)) ≥ 0 and that | ˙xi(t)| ≤ N + δ/3, 
and inserting v = v0 in condition (C) yields the inequality 
|qi(t)||˙xi(t) − v0| ≥ λ(1)
i ρS(t, xi(t), x˙i(t)) − 0 + φ(t, x˙i(t))
−0 − αi(N + δ/3(1 − 2η)R) .
≥ φ(t, x˙i(t)) − αi(N + δ/3 + (1 − 2η)R)
≥ (ηR/2) ∧ (δ/12) − αi(N + δ/3 + (1 − 2η)R) .
Since | ˙xi(t)| ≤ N+δ/3 and |v0| ≤ (1 − 2η)R, it follows that 
||qi||L∞ ≥ (N + δ/3 + (1 − 2η)R)−1

(ηR/2) ∧ (δ/12)
	
− αi .
(b): x˙i(t) ∈ ◦
Dδ(t). 
Now, since x˙i(t) is an unconstrained local minimizer of v → −qi(t) · v +
λ(1)
i ρ(t, xi(t), v) + φ(t, v) + αi|v − ˙xi|, we have 
qi(t) ∈ λ(1)
i ∂vρS(t, xi(t), x˙i(t)) + ∂vφ(t, x˙i(t)) + αiB.
Because we have assumed ρS(t, xi(t), x˙i(t)) > 0, we know that elements in 
∂vρS(t, xi(t), x˙i(t)) have unit length. Taking note also of the fact that φ(t, .) is 
Lipschitz continuous with Lipschitz constant 1/2, whence ∂vφ(t, x˙i(t)) ∈ (1/2)B, 
we deduce from the preceding relations that 
||qi||L∞ ≥ λ(1)
i − 1
2 − αi.
However, from (A) and (C), 
√
2||qi||L∞ ≥ λ(2) − kgλi − αi = −(1 + kg)λi − λ(1)
i − ||μi||T V + 1 − αi .
Adding these inequalities yields 
(1 + √
2)||qi||L∞ + (1 + kg)λi + ||μi||T V ≥ 1 − 1/2 − αi = 1/2 − 2αi .
Combining the estimates relating to the cases (i), (ii)(a) and (ii)(b), we obtain566 11 The Euler-Lagrange and Hamiltonian Inclusion Conditions in the Presence of. . .
(1 + √
2)||qi||L∞ + (1 + kg)λi + ||μi||T V
≥ min{
1
2 − 2αi ; (N + δ/3 + (1 − 2η)R)−1 ((ηR/2) ∧ (δ/12)) −αi}.
(11.3.16) 
This is the desired lower bound. 
We deduce from (11.3.11) that, along a subsequence, xi → ¯x ≡ 0 uniformly, and 
x˙i → ˙
x¯ ≡ 0 in L1 and a.e.. We have already observed that the p˙i’s are uniformly 
integrably bounded and the pi’s are uniformly bounded. By further restriction to a 
subsequence we can then arrange, in consequence of Ascoli’s theorem, that pi → p
uniformly and p˙i → ˙p weakly in L1. We can also arrange that λi → λ, λ(1)
i → λ(1)
and λ(2)
i → λ(2)
, for some λ ∈ [0, 1], λ(1) ∈ [0, 1] and λ(2) ∈ [0, 1], μi → μ
weakly∗ for some μ ∈ C⊕(S, T ) and qi(t) → q(t), for all t in some set of full 
Lebesgue measure (including the times S and T ), for some q ∈ NBV ([S, T ]; Rn). 
q is related to p and μ according to (11.2.3), for some Borel measurable function γ
satisfying γ (t) ∈ ∂>
x h(t, x(t)) ¯ for μ - a.e.. 
A convergence analysis along the lines of the proof of Proposition 11.3.1, permits 
us to pass to the limit in conditions (B)–(E). and thereby to obtain: 
p(t) ˙ ∈ co{η : (η, q(t)) ∈ λ(1)
∂x,vρS(t, x(t), ¯ ˙
x(t)) ¯ }, a.e. t ∈ [S,T ], (11.3.17) 
and also 
(C)''
: (q(S), −q(T )) ∈ λ∂g(x(S), ¯ x(T ¯ )) + NC(x(S), ¯ x(T ¯ )), 
(E)''
: supp{μ}⊂{t ∈ [S, T ] : h(t, x(t)) ¯ = 0},
γ (t) ∈ ∂>
x h(t, x(t)) ¯ for μ - a.e. t ∈ [S, T ]
and 
q(t) · ˙
x(t) ¯ − λ(1)
ρS(t, x(t), ¯ ˙
x(t)) ¯ − φ(t, ˙
x(t)) ¯
≥ p(t) · v − λ(1)
i ρS(t, x(t), ¯ v) − φ(t, v), for all v ∈ Dδ(t) , a.e. t ∈ [S, T ]. 
Observe that, for all points v ∈ (F (t, x(t)) ¯ ∩ (1 − 2η)RB)) ∪ Eδ
regular(t), we have 
v ∈ Dδ(t) and ρS(t, x(t), ¯ v) = φ(t, v) = 0. Consequently, the preceding relation 
implies 
(D)''
: q(t) · ˙
x(t) ¯ ≥ q(t) · v,
for all v ∈ (F (t, x(t)) ¯ ∩ (1 − 2η)RB)) ∪ Eδ
regular(t) a.e. t ∈ [S, T ]. 
From Lemma 8.6.5 of Chap. 8 and from (B) it follows that 
(B)'': p(t) ˙ ∈ co{η : (η, q(t)) ∈ NGrF (t,.)(x(t), ¯ ˙
x(t)) ¯ }, a.e. t ∈ [S, T ]
and 
| ˙p(t)| ≤ k(t)|q(t)| for a.e. t ∈ [S,T ] . (11.3.18)11.4 Free End-Time Problems with State Constraints 567
From (11.3.16), 
(1 + √
2)||q||L∞ + (1 + kg)λ + ||μ||T V
≥ min{
1
2
, (N + δ/3 + (1 − 2Rη))−1 ((ηR/2) ∧ (δ/12))} > 0 .
Since q = p if μ = 0, we deduce that (p, λ, μ) /= (0, 0, 0). We can therefore 
arrange, by scaling the Lagrange multipliers, that 
(A)''
: ||p||L∞ + λ + ||μ||T V = 1 .
Conditions (A)''–(E)'' above provide a restricted form of theorem, in which the 
Weierstrass condition (D)'' is affirmed only for velocities in the subset (recalling 
that x¯ ≡ 0) 
(F (t, x(t)) ¯ ∩ (1 − 2η)RB)) ∪ EN
regular(t) ⊂ co (Ω0(t) ∩ B(t)) .
To recover the full Weierstrass condition, we employ an analogous proof technique 
to that one of Chap. 8. The first step is to validate the condition for all v ∈
(F(t, x(t)) ¯ ∩ (1 − 2η)RB)) ∪ EN
discrete(t), by taking a sequence δi ↓ 0, carrying 
out the above constructions for each i and passing to the limit, using the fact that, 
for a.e. t ∈ [S, T ], 
lim
i
Eδi
regular(t) = EN
discrete(t) .
The second step is to validate it for all v ∈ Ω0(t) ∩ B(t), by taking sequences 
Ni ↑ ∞ and ηi ↓ 0 and using the fact that, for a.e. t ∈ [S, T ], 
Ω0(t) ∩ B(t) ⊂ lim
i→∞

(F (t, x(t)) ¯ ∩ (1 − 2ηi)RB)) ∪ ENi
discrete(t)	
.
Finally, we can replace Ω0(t) ∩ B(t) by co (Ω0(t) ∩ B(t)), using the linearity of the 
mapping e → q(t) · e. The proof is complete.
⨅⨆
11.4 Free End-Time Problems with State Constraints 
This section supplies necessary conditions that are ‘state constrained’ analogues of 
the free end-time necessary conditions of Chap. 9. As before, the extra conditions 
to take account of the free end-times are boundary conditions on the Hamiltonian. 
Consider the following state constrained problem with free end-times:568 11 The Euler-Lagrange and Hamiltonian Inclusion Conditions in the Presence of. . .
(F T )
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize g(S, x(S), T , x(T ))
over intervals [S,T ] and arcs x ∈ W1,1([S,T ]; Rn) satisfying
x(t) ˙ ∈ F (t, x(t)) a.e. t ∈ [S,T ],
(S, x(S), T , x(T )) ∈ C,
h(t, x(t)) ≤ 0 for all t ∈ [S,T ].
Here g : R × Rn × R × Rn → R and h : R × Rn → R are given functions, 
F : R × Rn ⇝ Rn is a given multifunction, and C ⊂ R × Rn × R × Rn is a 
given closed set. F trajectories for (FT) are now written ([S,T ], x) to emphasize the 
underlying time interval [S,T ] of the arc x. F trajectories that satisfy the constraints 
of (FT) are said to be admissible. 
Take a multifunction B : (−∞, +∞) → Rn. An admissible F trajectory 
([S,¯ T¯], x)¯ is called a W1,1 minimizer relative to B if there exists β > 0 such 
that 
g(S, x(S), T , x(T )) ≥ g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯
for all admissible F trajectories such that x(t) ˙ ∈ ˙
x(t) ¯ +B(t) a.e. t ∈ [S,¯ T¯]∩[S,T ]
and 
d(([S,T ], x), ([S,¯ T¯], x)) ¯ ≤ β .
Here, d is the distance function employed in Chap. 9: 
d(([S,T ], x), ([S'
, T '
], x'
)) :=
|x(S) − x'
(S)| + 
 +∞
−∞
| ˙xe(t) − ˙x'
e(t)|dt + |S − S'
|+|T − T '
|.
(Recall that xe denotes the extension, by constant extrapolation to left and right, of 
x : [S,T ] → Rn, etc. Often, for brevity, the subscript ‘e’ is omitted; any integral in 
which the domain of an integrand is not compatible with the limits of integration is 
interpreted in terms of this extension convention.) 
As in Chap. 9, we treat the Lipschitz time dependence and the measurable time 
dependence cases separately. Define, as usual, 
H (t, x, p) = sup
v∈F (t,x)
p · v .
Theorem 11.4.1 (State Constrained, Free End-Time Problems with Lipschitz 
Time Dependence) Let ([S,¯ T¯], x)¯ be a W1,1 local minimizer for (FT) such that 
T¯ − S >¯ 0. Assume that, for some ϵ > 0, the following hypotheses are satisfied: 
(G1): g is Lipschitz continuous on a neighbourhood of (S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ and C is 
a closed set,11.4 Free End-Time Problems with State Constraints 569
(G2): F (t, x) is non-empty for all (t, x) ∈ R × Rn and Gr F is closed, 
(G3): There exist ϵ > 0 and a measurable function R : [S,¯ T¯] → (0,∞) ∪ {+∞}
(a ‘radius function’) such that the following conditions are satisfied: 
(a): (Pseudo-Lipschitz Continuity) There exists k ∈ L1(S,¯ T )¯ such that 
F (t'
, x'
) ∩ (˙
x(t) ¯ + R(t) ◦
B) ⊂ F (t'', x'') + kF (t)|(t'
, x'
) − (t'', x'')|B ,
for all (t'
, x'
), (t'', x'') ∈ (t, x(t)) ¯ + ϵB, a.e. t ∈ [S,¯ T¯] ,
(b): (Tempered Growth) There exists r ∈ L1(S,¯ T )¯ , r0 > 0 and γ ∈ (0, 1) such 
that r0 ≤ r(t), γ −1r(t) ≤ R(t) a.e. t ∈ [S,¯ T¯] and 
F (t'
, x'
)∩(˙
x(t) ¯ +r(t)B) /= ∅, for all (t'
, x'
) ∈ (t, x(t)) ¯ +ϵB, a.e. t ∈ [S,¯ T¯], 
(G4): h is upper semi-continuous and there exists a constant kh such that, 
for all t ∈ [S,¯ T¯], 
|h(t'
, x'
) − h(t'', x'')| ≤ kh|(t'
, x'
) − (t'', x'')|
for all (t'
, x'
), (t'', x'') ∈ (t, x(t)) ¯ + ϵB .
Then there exist absolutely continuous arcs 
a ∈ W1,1([S,¯ T¯]; R) and p ∈ W1,1([S,¯ T¯]; Rn),
a non-negative number λ, a measure μ ∈ C⊕(S,¯ T )¯ and a Borel measurable 
function γ = (γ0, γ1) : [S,¯ T¯] → R1+n such that the following conditions are 
satisfied, in which 
q(t) := 
p(S)¯ if t = S¯
p(t) + 
[S,t] γ (s)dμ(s) if t ∈ (S,¯ T¯] .
(i) λ + ‖p‖L∞ + ‖μ‖T V = 1,
(ii) (− ˙a(t), p(t)) ˙ ∈ co{(α, β) : (α, β, q(t)) ∈ NGr F (t, x(t), ¯ ˙
x(t)) ¯ },
a.e. t ∈ [S,¯ T¯],
(iii) q(t) · ˙
x(t) ¯ ≥ q(t) · v for all v ∈ coΩ˜ 0(t) , a.e. t ∈ [S,¯ T¯],
(iv) (−a(S), q( ¯ S), a( ¯ T )¯ − 
[S,¯ T¯] γ0(s)dμ(s), −q(T )) ¯
∈ λ∂g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ + NC(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ , 
(v) a(t) = 
[S,t) ¯ γ0(s)dμ(s) + H (t, x(t), q(t)) ¯ , a.e. t ∈ [S,¯ T¯], 
(vi) suppμ ⊂ {t : h(t, x(t)) ¯ = 0} and γ (t) ∈ ∂>h(t, x(t)), μ ¯ - a.e. t ∈ [S,¯ T¯]. 
If the initial time S¯ is fixed, i.e. C = {(S, x ¯ 0,T,x1) : (x0,T,x1) ∈ C˜}, for some 
closed set C˜ ⊂ Rn × R × Rn, and g(S, x ¯ 0,T,x1) = ˜g(x0,T,x1) for some g˜ :
Rn × R × Rn → R, then the above conditions are satisfied when (iii) is replaced 
by 
(iii)'
: (q(S), a( ¯ T ), ¯ −q(T )) ¯ ∈ λ∂g(˜ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ + NC(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ .570 11 The Euler-Lagrange and Hamiltonian Inclusion Conditions in the Presence of. . .
The assertions of the theorem can be similarly modified when the final time T¯ is 
fixed. Here, 
∂>h(t, x(t)) ¯ := co lim sup {∂h(ti, xi) : ti → t,xi → ¯x(t) and h(ti, xi) > 0 for each i},
and 
Ω˜ 0(t) := {e ∈ F (t, x(t)) ¯ : F is pseudo−Lipschitz continuous near((t, x(t)), e) ¯ }.
The proof closely patterns that of Theorem 9.2.1 and is therefore omitted. The 
essential idea is to reformulate (F T ) as a fixed time problem by means of a 
change of independent variable, apply known necessary conditions to the fixed time 
problem (necessary conditions for state constrained problems in this case) and to 
interpret these conditions in terms of the original problem. 
Remark 
The Autonomous Case: If F (t, x) and h(t, x) do not depend on t, conditions (ii) and 
(v) in the theorem statement imply that a(.) is a constant and γ0 = 0. Condition (v) 
then takes the form: there exists a number c such that 
H (t, x(t), p(t) ¯ +


[S,t ¯ ]
γ1(s)dμ(s)) = c, a.e. t ∈ [S,¯ T¯] .
If for admissible F trajectories ([S,T ], x) either the initial time S is unconstrained 
and g does not depend on S, or the final time T is unconstrained and g does not 
depend on T , then the constant c = 0. 
We can use Proposition 8.5.1 of Chap. 8 to prove other versions of Theorem 11.4.1, 
in which hypothesis (G3) is replaced by alternative, stronger, hypotheses. For 
example, choosing the third set of conditions in Proposition 8.5.1 serving this 
purpose, we arrive at: 
Corollary 11.4.2 The assertions of Theorem 11.4.1 remain valid when x¯ is a W1,1
local minimizer (i.e. B ≡ Rn) and hypothesis is (G3) is replaced by: 
(G3)∗∗∗: There exist ϵ > 0, α > 0 and non-negative measurable functions k and 
β such that k and t → β(t)kα(t) are integrable on [S,¯ T¯] and, for each 
N ≥ 0, 
F (t'
, x'
) ∩ (˙
x(t) ¯ + NB) ⊂ F (t'', x'') + (k(t) + β(t)Nα)|x' − x''|B,
for all (t'
, x'
), (t'', x'') ∈ (t, x(t)) ¯ + ϵB and a.e. t ∈ [S,¯ T¯]. 
In this case, the Weierstrass condition (iii) of Theorem 11.4.1 becomes 
p(t) · ˙
x(t) ¯ ≥ p(t) · v, for all v ∈ F (t, x(t)), ¯ a.e. t ∈ [S,¯ T¯].11.4 Free End-Time Problems with State Constraints 571
Necessary conditions can also be derived for problems with measurably time 
dependent data. For such problems, the boundary condition on the Hamiltonian 
is interpreted in the ‘sub and super essential values’ sense, introduced in Chap. 9. 
Recall that, given an integrable function a : [S,T ] → R and a point t ∈ (S, T ), we 
have (see Definition 9.3.1): 
(a): the sub essential value of f at t
¯ is the set 
sub-ess t→t
¯ a(t) := 
ζ ∈ R : ∃ti → t
¯ and ζi → ζ s.t.
lim sup
ϵ↓0
ϵ−1

 ti
ti−ϵ
a(s)ds ≤ ζi ≤ lim inf ϵ↓0

 ti+ϵ
ti
a(s)ds, for each i

.
(b): the super essential value of a at t is the set 
super-ess t→t
¯
a(t) := 
ζ ∈ R : ∃ti → t
¯ and ζi → ζ s.t.
lim sup
ϵ↓0
ϵ−1

 ti+ϵ
ti
a(s)ds ≤ ζi ≤ lim inf ϵ↓0

 ti
ti−ϵ
a(s)ds, for each i

.
Theorem 11.4.3 (State Constrained, Free End-Time Problems with Measur￾able Time Dependence) Take a measurable multifunction B : R ⇝ Rn such that 
B(t) is open for a.e. t ∈ R. Let ([S,¯ T¯], x)¯ be a W1,1 local minimizer for (FT) 
relative to B such that T¯ − S >¯ 0. Assume that, for some ϵ > 0 and σ > 0, the 
data satisfy the following hypotheses: 
(G1): g is Lipschitz continuous on a neighbourhood of (S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ and C is 
a closed set, 
(G2): F (t, x) is nonempty for each (t, x) ∈ R × Rn, Gr F (t, .) is closed for each 
t ∈ R and F is L × Bn measurable, 
(G3): There exists a measurable function R : [S¯ − σ, T¯ + σ] → (0,∞) ∪ {+∞}
(a ‘radius function’), such that R(t) ◦
B ⊂ B(t) a.e. t ∈ [S¯ − σ, T¯ + σ] and 
the following conditions are satisfied: 
(a): (Pseudo-Lipschitz Continuity) There exists k ∈ L1(S¯ − σ, T¯ + σ ) such that 
F (t, x'
) ∩ (˙
x(t) ¯ + R(t) ◦
B) ⊂ F (t, x) + k(t)|x' − x|B,
for all x, x' ∈ ¯x(t) + ϵB, a.e. t ∈ [S¯ − σ, T¯ + σ],
(11.4.1) 
(b): (Tempered Growth) There exist r ∈ L1(S¯ − σ, T¯ + σ ), r0 > 0 and α ∈ (0, 1)
such that r0 ≤ r(t), α−1r(t) ≤ R(t), a.e. t ∈ [S¯ − σ, T¯ + σ], and 
F (t, x) ∩ (˙
x(t) ¯ + r(t)B) /= ∅ for all x ∈ ¯x(t) + ϵB, a.e. t ∈ [S¯ − σ, T¯ + σ], 
(G4): There exists cF ≥ 0 such that, for a.e. t ∈ [S¯ − σ, S¯ + σ]∪[T¯ − σ, T¯ + σ], 
F (t, x) ⊂ cFB, for all x ∈ ¯x(t) + ϵB . (11.4.2)572 11 The Euler-Lagrange and Hamiltonian Inclusion Conditions in the Presence of. . .
(G5): h is upper semi-continuous and there exists a constant kh such that, for a.e. 
t ∈ [S¯ − σ, T¯ + σ]
|h(t, x) − h(t, x'
)| ≤ kh|x − x'
|
for all x, x' ∈ ¯xe(t) + ϵB. 
Furthermore, 
h(S,¯ x(¯ S)) < ¯ 0 and h(T ,¯ x(¯ T )) < ¯ 0. (11.4.3) 
Then there exist an absolutely continuous arc p ∈ W1,1([S,¯ T¯]; Rn), real 
numbers λ ≥ 0, ξ0, ξ1, a measure μ ∈ C⊕(S,¯ T )¯ and a μ-integrable function 
γ : [S,¯ T¯] → Rn such that the following conditions are satisfied, in which 
q(t) =

p(S)¯ if t = S¯
p(t) + 
[S,t] γ (s)dμ(s) if t ∈ (S,¯ T¯] .
(i) λ + ‖p‖L∞ + ‖μ‖T V = 1,
(ii) p(t) ˙ ∈ co{ζ : (ζ, q(t)) ∈ NGrF (t,·)(x(t), ¯ ˙
x(t)) ¯ }, a.e. t ∈ [S,¯ T¯],
(iii) (−ξ0,q(S), ξ ¯ 1, −q(T )) ¯ ∈ λ∂g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ + NC(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ , 
(iv) q(t) · ˙
x(t) ¯ ≥ q(t) · v for all v ∈ co(Ω0(t) ∩ (˙
x(t) ¯ + B(t))) a.e. t ∈ [S,¯ T¯],
(v) γ (t) ∈ ∂>
x h(t, x(t)) μ ¯ - a.e. t ∈ [S,¯ T¯], 
(vi) ξ0 ∈ sub-esst→S¯H (t, x(¯ S), q( ¯ S)) ¯ , 
(vii) ξ1 ∈ super-esst→T¯ H (t, x(¯ T ), q( ¯ T )) ¯ . 
(In condition (i), Ω0(t) is as defined by (11.2.1).) 
If the initial time is fixed (i.e. C = {(S, x ¯ 0,T,x1) : (x0,T,x1) ∈ C˜} for some set 
C˜) and g(S, x0,T,x1) = ˜g(x0,T,x1), for some function g˜ : Rn × R × Rn → R, 
then the above assertions (except (vi)) remain true when condition (11.4.2) in (G4) 
is satisfied only for a.e. t ∈ [T¯ − σ, T¯ + σ] and (11.4.3) in (G5) is replaced by 
‘h(T ,¯ x(¯ T )) < ¯ 0’ ; in this case, (iii) can be written 
(q(S), ξ ¯ 1, −q(T )) ¯ ∈ λ∂g(˜ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ + NC˜(x(¯ S), ¯ T ,¯ x(¯ T )). ¯
Hypotheses (G4)–(G5) and the assertions of the theorem are modified similarly, 
when the right end-time is fixed. 
Remark 
(Convex Velocity Sets): If 
F (t, x) is convex, for all x ∈ ¯x(t) + ϵB, a.e. t ∈ [S,T ] ,
then, according to the dualization theorem (Theorem 8.7.1), the generalized Euler 
Lagrange condition (ii) implies the Hamiltonian inclusion, namely11.4 Free End-Time Problems with State Constraints 573
p(t) ˙ ∈ co{−ξ : (ξ , ˙
x(t)) ¯ ∈ ∂x,pH (t, x, q(t)) ¯ }, a.e. t ∈ [S,T ];
also, the Weierstrass condition (iii) is valid in a stronger form, in which F (t, x(t)) ¯
replaces the set co(Ω0(t) ∩ (˙
x(t) ¯ + B(t))). 
The proof of Theorem 11.4.3 (providing necessary conditions for state con￾strained, free end-time problems and measurably time depend data) is along similar 
lines to that its fixed end-times counterpart Theorem 11.2.1 and, for this reason, we 
merely describe here its main features. The proof comprises four steps. In Step 1 we 
prove necessary conditions for a free time version of the finite Lagrangian problem. 
These necessary conditions, which incorporate a transversality condition expressed 
in terms of sub and super essential values of the Hamiltonian, are proved by the 
techniques used in the proof of the free end-time necessary condition Theorem 9.4.1 
of Chap. 9. (This involves, first, proving the necessary conditions when the endpoint 
cost is continuously differentiable, in which case the transversality conditions are 
deduced from fixed time necessary conditions by applying variations to the end￾times; the analysis is then extended to cover Lipschitz continuous end-point cost 
functions, by means of ‘inf convolution’ arguments and an application of Ekeland’s 
theorem). Step 2 (hypothesis reduction) and Step 3 (construction of an integral 
penalty function) are exactly the same as in the fixed end-time setting. In Step 
4 (completion of the proof) we apply the finite Lagrangian necessary conditions 
of Step 1 to a sequence of intermediate dynamic optimization problems involving 
the integral penalty function of Step 3. The intermediate problems are the same 
as those employed in Step 4 of the proof the fixed end-time necessary conditions 
(Theorem 11.2.1) except that, now, the end-times are free. The necessary conditions 
of Theorem 11.4.3 (in a restricted sense) are obtained by passing to the limit in the 
necessary for the intermediate problem. The full necessary conditions are finally 
obtained by another passage to the limit, now involving the parameters used to 
define the integral penalty function. 
Other versions of Theorem 11.4.3 can be proved with the help of Proposi￾tion 8.5.1 of Chap. 8, in which hypothesis (G3) is replaced by other hypotheses. 
For example 
Corollary 11.4.4 The assertions of Theorem 11.4.3 remain valid when x¯ is a W1,1
local minimizer (i.e. B ≡ Rn) and hypothesis (G3) is replaced by: 
(G3)∗∗∗: There exist α > 0, ϵ > 0 and non-negative measurable functions k and β
such that k and t → β(t)kα(t) are integrable and, for each N ≥ 0, 
F (t, x'
) ∩ (˙
x(t) ¯ + NB) ⊂ F (t, x) + (k(t) + β(t)Nα)|x' − x|B,
for all x'
, x ∈ ¯x(t) + ϵB and a.e. t ∈ [S¯ − σ, T¯ + σ]. 
In this case, the Weierstrass condition (iii) of Theorem 11.4.4 becomes 
p(t) · ˙
x(t) ¯ ≥ p(t) · v, for all v ∈ F (t, x(t)), ¯ a.e. t ∈ [S,¯ T¯].574 11 The Euler-Lagrange and Hamiltonian Inclusion Conditions in the Presence of. . .
11.5 Non-degenerate Necessary Conditions 
The preceding Euler Lagrange-type necessary conditions, for state constrained 
dynamic optimization problems, provide useful information about minimizers in 
many cases. But there are cases when they are degenerate, notably when the initial 
or terminal state is fixed and either of these fixed end-states are located in the 
state constraint set boundary. Suppose, indeed, that we restrict attention (for clarity 
of exposition) to a special case in which F and h are independent of t, h is a 
continuously differentiable function, g depends only on the right endpoint of the 
state trajectory, the underlying time interval and the left endpoint are fixed, i.e. 
C = {S¯}×{x0}×{T¯} × C1,
for some [S,¯ T¯] ⊂ R, x0 ∈ Rn and C1 ⊂ Rn, and 
h(x0) = 0.
For such problems, conditions (i)–(vi) of Theorem 11.4.1 are satisfied by any 
admissible F trajectory x¯, with Lagrange multipliers taken to be 
(p ≡ −∇h(x(¯ S)), μ ¯ = δ'
{S¯}
, λ = 0) . (11.5.1) 
Provided ∇h(x(¯ S)) ¯ = ∇h(x0) /= 0, these multipliers are non-zero. In these cases, 
the Euler Lagrange type necessary conditions fail to give useful information about 
minimizers. 
We have already explored this ‘degeneracy’ phenomenon in Chap. 10, in connec￾tion with the state-constrained maximum principle. The earlier discussion is equally 
relevant to Euler Lagrange-type necessary conditions for state constrained problems 
in which the dynamic constraint is a differential inclusion. The remedy will be 
the same as before. It will be to supplement the necessary conditions with extra 
conditions, asserting that the regularity properties of the maximized Hamiltonian 
on the interior of the optimal time interval [S,¯ T¯] extend to the end-times. In the 
presence of an appropriate constraint qualification, these extra conditions exclude 
the degenerate Lagrange multipliers (11.5.1) and tell us that there must exist other 
Lagrange multiplier choices with possibly non-trivial information content. 
Consider, once again, the free end-time dynamic optimization problem with 
pathwise state constraints: 
(F T )
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize g(S, x(S), T , x(T ))
over [S,T ] ⊂ R, x ∈ W1,1([S,T ]; Rn)
satisfying
x(t) ˙ ∈ F (t, x(t)) a.e. t ∈ [S,T ],
h(t, x(t)) ≤ 0 for all t ∈ [S,T ],
(S, x(S), T , x(T )) ∈ C,11.5 Non-degenerate Necessary Conditions 575
the data for which comprise: functions g : R×Rn×R×Rn → R and h : R×Rn →
R, a multifunction F : R × Rn ⇝ Rn and a set C ⊂ R × Rn × R × Rn. 
Consistent with earlier terminology (cf. Chap. 9), an F trajectory ([S,T ], x) is a 
pair comprising an interval [S,T ] ⊂ R (T >S) and an arc x ∈ W1,1([S,T ]; Rn)
such that x(t) ˙ ∈ F (t, x(t)) a.e. t ∈ [S,T ]. The F trajectory is said to be admissible 
if (S, x(S), T , x(S)) ∈ C and h(t, x(t)) ≤ 0, for all t ∈ [S,T ]. An admissible 
F trajectory ([S,¯ T¯], x)¯ is an L∞ local minimizer for (F T ) if there exists some 
β > 0 such that g(S, x(S), T , x(T )) ≥ g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ for all admissible F
trajectories ([S,T ], x) such that |S − S¯|+|T − T¯| + ||xe − ¯xe||L∞ ≤ β. Here, xe
denotes the extension of x : [S,T ] → Rn, by constant extrapolation to the left and 
right. 
Write 
H (t, x, p) := sup
v∈F (t,x)
p · v (11.5.2) 
and 
C˜ := {(t0, x0, t1, xi) ∈ C : h(t0, x0) ≤ 0 and h(t1, x1) ≤ 0}.
Theorem 11.5.1 Let ([S,¯ T¯], x)¯ be an L∞ local minimizer for (F T ). Assume that, 
for some ϵ > 0, 
(H1) g is Lipschitz continuous on a neighbourhood of (S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ and C is 
a closed set, 
(H2) F takes values non-empty, compact sets, and there exist Kf > 0, cf > 0 such 

that 
F (t'
, x'
) ⊂ F (t'', x'') + kF (|t' − t''|+|x' − x''|)B,
F (t'
, x'
) ⊂ cFB,
for all (t'
, x'
), (t'', x'') ∈ (t, x(t)) ¯ + ϵB, for all t ∈ [S,¯ T¯], 
(H3) h is Lipschitz continuous on a neighbourhood of Gr x¯. 
Then there exist absolutely continuous arcs e ∈ W1,1([S,¯ T¯]; R) and p ∈
W1,1([S,¯ T¯]; Rn), λ ≥ 0, a measure μ ∈ C⊕(S,¯ T )¯ and a Borel measurable 
function γ = (γ0, γ1) : [S,¯ T¯] → R1+n such that the following relations, in which 
(r, q) ∈ NBV ([S,¯ T¯]; R1+n) is the function 
(−r(t), q(t)) := 
(−e(S), p( ¯ S)) ¯ if t = S¯
(−e(t), p(t)) + 
[S,t ¯ ] γ (s)dμ(s) if t ∈ (S,¯ T¯] , (11.5.3) 
are satisfied: 
(i) (λ, p, μ) /= (0, 0, 0), 
(ii) γ (t) ∈ ∂>h(t, x(t)) μ ¯ - a.e. t ∈ [S,¯ T¯], 
(iii) (− ˙e(t), p(t)) ˙ ∈ co{(ζ, η) : ((ζ, η), q(t)) ∈ NGr F ((t, x(t)), ¯ ˙
x(t)) ¯ },
a.e. t ∈ [S,¯ T¯],
(iv) q(t) · ˙
x(t) ¯ = sup v∈F (t,x(t)) ¯
q(t) · v, a.e. t ∈ [S,¯ T¯],576 11 The Euler-Lagrange and Hamiltonian Inclusion Conditions in the Presence of. . .
(v) (−r(S), q( ¯ S), r( ¯ T ), ¯ −q(T )) ¯ ∈ λ∂g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯
+ NC˜(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ , 
(vi) r(S)¯ = H (S,¯ x(¯ S), q( ¯ S)) ¯ , 
(vii) r(t) = H (t, x(t), q(t)) ¯ , for all t ∈ (S,¯ T )¯ , 
(viii) r(T )¯ = H (T ,¯ x(¯ T ), q( ¯ T )) ¯ . 
Remarks 
(a): The extra information provided by this theorem is of course the pair of 
conditions (vi) and (viii), asserting that condition (vii) extends to the end￾times. Notice that, in contrast to the non-degenerate state constrained maximum 
principle of Chap. 10, condition (vii) is an ‘everywhere’, not an ‘almost 
everywhere’ condition on the open interval (S,¯ T )¯ . The ‘almost everywhere’ 
condition can be strengthened to the ‘everywhere’ version, in consequence of 
the facts that H is a continuous function and q is a bounded variation function 
with everywhere left and right limits. 
(b): Notice that, as with the non-degenerate state-constrained maximum principle, 
the transversality condition (v) is expressed in terms of the modified end-point 
constraint set C˜, in place of C. 
Corollary 11.5.2 Let ([S,¯ T¯], x)¯ be an L∞ local minimizer for (F T ). Assume 
hypotheses (H1)–(H3) of Theorem 11.5.1. Assume also that the following constraint 
qualification is satisfied: 
γ '
0 + min v∈F (S,¯ x(¯ S)) ¯ γ '
1 · v < 0 and γ ''
0 + max v∈F (T ,¯ x(¯ T )) ¯ γ ''
1 · v > 0
for all (γ '
0, γ '
1) ∈ ∂>h(S,¯ x(¯ S)) ¯ and all (γ ''
0 , γ ''
1 ) ∈ ∂>h(T ,¯ x(¯ T )) ¯ . 
Then there exist absolutely continuous arcs e ∈ W1,1([S,¯ T¯]; R) and p ∈
W1,1([S,¯ T¯]; Rn), λ ≥ 0, a measure μ ∈ C⊕(S,¯ T )¯ and a Borel measurable 
function γ = (γ0, γ1) : [S,¯ T¯] → R1+n such that conditions (ii)–(viii) of 
Theorem 11.5.1 are satisfied and also the following strengthened version of the non￾triviality condition (i): 
(i)'
: λ + ||q||L∞ + 
(S,¯ T )¯ dμ(s)ds /= 0. 
Notice that, for problems with a fixed left endpoint discussed above, the degenerate 
choice of multipliers (11.5.1), in which q(t)(= p(t) + 
[S,t ¯ ] hx dμ(s)ds) = 0 for 
t ∈ (S,¯ T )¯ , violates the strengthened non-triviality condition (i)' in the corollary. 
Proof of Corollary 11.5.2 The proof is analogous to that of Corollary 10.7.2 of 
Chap. 10. 
Proof of Theorem 11.5.1 Let ϵ' > 0 be such that ([S,¯ T¯], x)¯ is a minimizer for 
(F T ), with respect to arcs ([S,T ], x) satisfying 
|S − S¯|+|T − T¯|+‖xe − ¯xe‖L∞ ≤ ϵ'
.11.5 Non-degenerate Necessary Conditions 577
Fix ρ ∈ (0, 1/2). Consider the ‘re-parameterized’ dynamic optimization problem 
(R)
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize g((τ, y)(s0), (τ, y)(s1))
over intervals [s0, s1] ⊂ R and (τ, y) ∈ W1,1([s0, s1]; R1+n)
satisfying
(τ (s), ˙ y(s)) ˙ ∈ {(w, wv) : w ∈ [1 − ρ , 1 + ρ], v ∈ F (τ (s), y(s))},
a.e. s ∈ [s0, s1],
subject to
h(τ (s), y(s)) ≤ 0 for all s ∈ [s0, s1],
((τ, y)(s0), (τ, y)(s1)) ∈ C,˜
|s0 − S¯|+|s1 − T¯|+‖ye − ¯xe‖L∞ ≤ ϵ'' ,
in which ϵ'' ∈ (0, ϵ'
/2). Set F (τ, y)  := {(w, wv) : w ∈ [1−ρ , 1+ρ], v ∈ F (τ, y)}. 
As in the proof of Theorem 10.7.1 the last constraint in problem (R) is inactive, with 
reference to any F
 trajectory ([s0, s1], (τ, y)) for (R) such that |s0 −S¯|+|s1 −T¯| +
||ye − ¯xe||L∞ < ϵ'' and so the end-times s0 and s1 of the independent variable s are 
locally unconstrained. This fact will play a crucial role in subsequent analysis. 
A change of independent variable analysis, similar to that employed in the proof 
of Theorem 9.6.1 (and Lemma 9.7.1), permits us to conclude: 
Lemma 11.5.3 We can choose ρ ∈ (0, 1/2) and ϵ'' ∈ (0, ϵ'
/2) sufficiently small 
such that ([S,¯ T¯], (τ (s) ¯ ≡ s, y)) ¯ is an L∞ local minimizer for problem (R). 
Take ρi ↓ 0. For each i define 
gi(τ0, y0, τ1, y1) := (g(τ0, y0, τ1, y1) − g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ + ρ2
i ) ∨ 0 .
Consider the problem 
(Ri)
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize Ji(([s0, s1], (τ, y))) := gi(τ (s0), y(s0), τ (s1), y(s1))
∨ max
s∈[s0,s1]
h(τ (s), y(s)))
subject to
(τ (s), ˙ y(s)) ˙ ∈ {(w, wv) : w ∈ [1 − ρ , 1 + ρ], v ∈ F (τ (s), y(s))},
a.e. s ∈ [s0, s1],
(τ (s0), y(s0), τ (s1), y(s1)) ∈ C,˜
|s0 − S¯|+|s1 − T¯|+‖ye − ¯xe‖L∞ ≤ ϵ''.
Define 
M := {F˜ trajectories ([s0, s1], (τ, y)) for (Ri)}. 
Ji is a continuous function on the complete metric space (M, dM), when we 
define 
dM(([s'
0, s'
1], (τ '
, y'
)), ([s0, s1], (τ, y))) := |τ '
(s'
0) − τ (s0)|+|y'
(s'
0) − y(s0)|
+|s'
0 − s0|+|s'
1 − s1| + || ˙τ '
e − ˙τe||L1 + || ˙y'
e − ˙ye||L1 .578 11 The Euler-Lagrange and Hamiltonian Inclusion Conditions in the Presence of. . .
Clearly, ([S,¯ T¯], (τ (s) ¯ ≡ s, x)) ¯ is a ρ2
i -minimizer for problem (Ri). We can 
therefore conclude from Ekeland’s theorem that there exists ([s0i, s1i], (τi, yi)) ∈
M, which is a minimizer for 
Minimize {J '
i(([s0, s1], (τ, y))) : ([s0, s1], (τ, y)) ∈ M}. (11.5.4) 
Here 
J '
i(([s0, s1], (τ, y))) := Ji(([s0, s1], (τ, y))) +
ρidM(([s0, s1], (τ, y)), ([s0i, s1i], (τi, yi))).
Furthermore, 
dM(([s0i, s1i], (τi, yi)), ([S,¯ T¯], (τ (s) ¯ ≡ s, x))) ¯ ≤ ρi (11.5.5) 
and 
J '
i(([s0i, s1i], (τi, yi))) ≤ J '
i(([S,¯ T¯], (τ (s) ¯ ≡ s, x))) . ¯
We can find Lebesgue measurable functions wi : [s0i, s1i] → R and vi :
[s0i, s1i] → Rn such that 
wi(s) ∈ [1 − ρ , 1 + ρ], vi(s) ∈ F (τi(s), yi(s))
and (τ˙i(s), y˙i(s)) = (wi(s), wi(s)vi(s)), a.e..
Relation (11.5.5) implies that, for a subsequence, 
|τi(s0i) − S¯| ,|yi(s0i) − ¯x(S)¯ |, |s0i − S¯|, |s1i − T¯| → 0 and
||(wie − χ[S,¯ T¯](= ˙
τ¯e), vie − ˙
x¯e)||L1 → 0 as i → ∞. (11.5.6) 
(The subscripts for the velocity related variables wie, vie, etc., indicate extension 
by 0, not by constant extrapolation as for the state-related variables. χ[S,¯ T¯] is the 
characteristic function of the set [S,¯ T¯].) Furthermore (wie(s) − χ[S,¯ T¯](s), vie(s) −
˙
x¯e(s)) → (0, 0) a.e.. We can deduce, with the help of Gronwall’s lemma that 
||(τie, yie) − (τ¯e, x¯e)||L∞ → 0 as i → ∞ .
Note also that 
gi(τi(s0i), yi(s0i), τi(s1i), yi(s1i)) ∨ max t∈[s0i,s1i]
h(τi(t), yi(t))) > 0,
for all i sufficiently large. (11.5.7)11.5 Non-degenerate Necessary Conditions 579
This reason is that, if ‘=’ replaces ‘>’ in the above relation, then 
g(τi(s0i), yi(s0i), τi(s1i), yi(s1i)) ≤ g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ − ρ2
i ,
(τi(s0i), yi(s0i), τi(s1i), yi(s1i)) ∈ C˜ and h(τi(t), yi(t)) ≤ 0, for all t ∈ [s0i, s1i]. 
Furthermore, ||yie − ¯xe||L∞ ≤ ϵ'
. This contradicts the L∞ local optimality of 
([S,¯ T¯], x)¯ for (F T ). 
Observe next that ([s0i, s1i], (τi, yi)) is a global minimizer for 
(R'
i)
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize J ''
i (([s0, s1], (τ, y)))
subject to
(τ (s), ˙ y(s)) ˙ ∈ {(w, wv) : w ∈ [1 − ρ , 1 + ρ], v ∈ F (τ (s), y(s))},
a.e. s ∈ [s0, s1],
(τ (s0), y(s0), τ (s1), y(s1)) ∈ C,˜
|s0 − S¯|+|s1 − T¯|+‖ye − ¯xe‖L∞ ≤ ϵ'',
in which 
J ''
i (([s0, s1], (τ, y))) := gi(τ (s0), y(s0), τ (s1), y(s1)) ∨ max
s∈[s0,s1]
h(τ (s), y(s))
+ρi

|τ (s0) − τi(s0i)|+|y(s0) − yi(s0i)| + c(|s0 − s0i|+|s1 − s1i|)+
 s1
s0 | ˙τe(s) − ˙τie(s)|ds +  s1
s0 | ˙ye(s) − ˙yie(s)|ds	
.
Here, c := 1 + max{2ρ , 4ρcF }. 
Use of the cost function appearing in this problem formulation, in place of J '
i in 
problem (11.5.4), is justified by the following relations: 
|| ˙ye − ˙yie||L1 ≤  s1
s0 | ˙ye(s) − ˙yie(s)|ds + 4ρcF (|s0 − s01|+|s1 − si1|),
|| ˙τe − ˙τie||L1 ≤  s1
s0 | ˙τe(s) − ˙τie(s)|ds + 2ρ(|s0 − s01|+|s1 − si1|) .
(11.5.8) 
There are two cases to consider. 
Case 1: max s∈[s0i,s1i]
h(τi(s), yi(s)) > 0 for at most a finite number of index values i, 
Case 2: max s∈[s0i,s1i]
h(τi(s), yi(s)) > 0, for an infinite number of index values i. 
Consider first Case 1. We can arrange, by excluding initial terms in the sequence, 
that 
max s∈[s0i,s1i]
h(τi(s), yi(s)) ≤ 0 for all i .
Taking note of (11.5.7), we see that ([s0i, s1i], (τi, yi)) is an L∞ local minimizer for580 11 The Euler-Lagrange and Hamiltonian Inclusion Conditions in the Presence of. . .
(R
i)
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize J˜
i(([s0, s1], (τ, y)))
subject to (τ (s), ˙ y(s)) ˙ ∈ {(w, wv) : w ∈ [1 − ρ , 1 + ρ], v ∈ F (τ (s), y(s))},
a.e. s ∈ [s0, s1],
(τ (s0), y(s0), τ (s1), y(s1)) ∈ C,˜
|s0 − S¯|+|s1 − T¯|+‖ye − ¯xe‖L∞ ≤ ϵ'',
in which 
J˜
i(([s0, s1], (τ, y), (v, w))) := gi((τ (s0), y(s0), τ (s1), y(s1))
+ρi

|τ (s0) − τi(s0i)|+|y(s0) − yi(s0i)| + c(|s0 − s0i|+|s1 − s1i|)
+

 s1
s0
| ˙τ (s) − ˙τie(s)|ds +

 s1
s0
| ˙y(s) − ˙yie(s)|ds	
.
From (11.5.7) and since h(τi(t), yi(t)) ≤ 0 for t ∈ [si, ti], 
gi(τi(s0i), yi(s0i), τi(s1i), yi(s1i)) = g(τi(s0i), yi(s0i), τi(s1i), yi(s1i))
− g(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ + ρ2
i . (11.5.9) 
For i sufficiently large, we may apply the free end-time Euler Lagrange necessary 
conditions Theorem 9.4.1 of Chap. 9 to this problem. (Since these optimality 
conditions apply to problems with no running cost it is required, as a first 
step, to reformulate the above problem as a running cost-free problem by state 
augmentation.) Taking account of (11.5.9), we thereby conclude that there exist 
λi ≥ 0, ri ∈ W1,1([s0i, s1i]; R) and qi ∈ W1,1([s0i, s1i]; Rn) such that 
λi + ||qi||L∞ = 1, (11.5.10)
(− ˙ri(s), q˙i(s)) ∈ co{(ζ, η) : ((ζ, η), qi(s)) ∈ NGr F ((τi(s), yi(s)), vi(s))
+{0} × (|wi(s) − 1| |qi(s)| + λiρi(1 + ρ))B}, a.e. s ∈ [s0i, s1i],
(v, w) → w(qi(s) · v − ri(s)) − ρiλi(|w − wi(s)|+|wv − wi(s)vi(s)|)
is maximized over (11.5.11)
(v, w) ∈ F (τi(s), yi(s)) × [1 − ρ , 1 + ρ] at (wi(s), vi(s)),
a.e. s ∈ [s0i, s1i], (11.5.12)
(−ri(s0i), qi(s0i), ri(s1i), −qi(s1i)) ∈ λi∂g(τi(s0i), yi(s0i), τi(s1i), yi(s1i))
+NC˜(τi(s0i), yi(s0i), τi(s1i), yi(s1i)) + √
2 λiρi B × {(0, 0)} (11.5.13)
and, for s = s0i and s1i ,
sup
(v,w)∈F (τi(s),yi(s))×[1−ρ ,1+ρ]
(qi(s) · v − ri(s))w ∈ ρiλic1[−1, +1]. (11.5.14) 
Here, c1 = 2((1 + ρ)(1 + cF ) + c/2).11.5 Non-degenerate Necessary Conditions 581
Equation (11.5.14) results from the free-time transversality condition for the opti￾mal end-times, when we take account of the fact the end-times s0 and s1 in problem 
(R
i) are locally unconstrained, with reference to the minimizer ([s0i, s1i], (τi, yi)), 
for i sufficiently large. Notice that the estimation of the sub and super essential 
values of the maximized Hamiltonian at the end-times takes account of the integral 
error term ρi

(| ˙τ − ˙τie|+|˙y − ˙yie|)ds in the cost. 
(11.5.14) implies 
sup v∈F (τi(s),yi(s))
qi(s) · v − ri(s) ∈ ρiλi(1 − ρ)−1c1[−1, +1],
for s = s0i and s1i . (11.5.15) 
In view of (11.5.6), we can find a Lebesgue set T ⊂ (S,¯ T )¯ with the following 
properties. Following a further extraction of subsequences we have, for each s ∈ T , 
1 − ρ<wi(s) < 1 + ρ for all i suff. large and (11.5.16) 
(wi(s) − 1, wi(s)vi(s) − ˙
x(s)) ¯ → (0, 0), as i → ∞.
By removing a null set of points from T , we can arrange that conditions (11.5.11) 
and (11.5.12) are satisfied all points s ∈ T , i = 1, 2,...
Take any s ∈ T . Considering first minimization w.r.t. the w variable and then the 
v variable, we deduce from (11.5.16) and (11.5.12) that, for i sufficiently large, 
qi(s) · vi(s) − ri(s) ∈ ρiλi(1 + cF )B (11.5.17) 
and 
qi(s) · vi(s) ≥ sup v∈F (τi(s),yi(s))
qi(s) · v − 2cF (1 − ρ)−1ρiλi . (11.5.18) 
But then 
sup v∈F (τi(s),yi(s))
qi(s) · v − ri(s) ∈ ρiλic2B . (11.5.19) 
Here, c2 := (1 + cF ) + 2cF (1 − ρ)−1. 
Relations (11.5.10), (11.5.11), (11.5.13), (11.5.15), (11.5.17) and (11.5.19) will 
be recognized as perturbed versions of the desired necessary conditions, in which 
μi = 0 and therefore (ei, pi) = (ri, qi). After extracting a further subsequence, 
we are assured of the existence of q ∈ W1,1, r ∈ W1,1 and λ ∈ [0, 1] such that 
(rie, qie) → (re, qe) uniformly, (r˙ie, q˙ie) → (re, qe) weakly in L1 and λi → λ. We 
can now employ a standard convergence analysis to show that the asserted relations 
of the theorem are correct, with λ, q(= p) and r(= e) as above.582 11 The Euler-Lagrange and Hamiltonian Inclusion Conditions in the Presence of. . .
We now turn to Case 2. We can arrange, by selecting a subsequence, that 
max s∈[s0i,s1i]
h(τi(s), yi(s)) > 0 , for all i . (11.5.20) 
Fix i. For arbitrary K > 0, consider the problem 
(RK
i )
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize J K
i (([s0, s1], (τ, y), z))
over [s0, s1] ⊂ R, (τ, y) ∈ W1,1 and z ∈ R satisfying
(τ (s), ˙ y(s)) ˙ ∈ {(w, wv) : w ∈ [1 − ρ , 1 + ρ], v ∈ F (τ (s), y(s))},
a.e. s ∈ [s0, s1],
(τ (s0), y(s0), τ (s1), y(s1)) ∈ C,˜
|s0 − S¯|+|s1 − T¯|+‖ye − ¯xe‖L∞ ≤ ϵ'',
in which 
J K
i (([s0, s1], (τ, y), z)) := gi(τ (s0), y(s0), τ (s1), y(s1)) ∨ z
+ ρi

|τ (s0) − τi(s0i)|+|yi(s0) − y(s0i)| + c(|s0 − s0i|+|s1 − s1i|)
+

 s1
s0
| ˙τ (s) − ˙τie(s)|ds +

 s1
s0
| ˙y(s) − ˙yie(s)|ds	
+ K

 s1
s0
((h(τ (s), y(s)) − z) ∨ 0)
2ds .
Notice that, for each K and any ([s0, s1], (τ, y)), we have 
J K
i

([s0, s1], (τ, y), z = max
s∈[s0,s1]
h(τ (s), y(s)))
= J
''
i (([s0, s1], (τ, y))) .
This implies that, for each i, inf(RK
i ) ≤ inf(R'
i). An analysis similar to that 
employed in the proof of Lemma 10.7.4, Chap. 10, establishes that the two infima 
coincide in the limit as K → ∞. Thus: 
Lemma 11.5.4 For i = 1, 2,..., 
lim
K→∞ inf(RK
i ) = inf(R'
i) .
In view of (11.5.20), and the continuity of the map ([s0, s1], (τ, y)) → (τ, y)
from M to L∞, we can find a sequence ρ'
i ↓ 0 such that, for each i, ρ'
i ≤ 1/2 and 
([s0, s1], (τ, y)) ∈ M, z ∈ R
dM(([s0, s1], (τ, y)), ([si0, si1], (τi, yi))) ≤ ρ
'
i
|z − maxs∈[s0i,s1i] h(τi(s), yi(s))| ≤ ρ
'
i
⎫
⎬
⎭
(11.5.21)
=⇒ gi(τ (s0), y(s0), τ (s1), y(s1)) ∨ z > 0 and z > 0 .11.5 Non-degenerate Necessary Conditions 583
According to the lemma, we can choose Ki ↑ ∞ such that, for each i, 
([si0, si1], (τi, yi), zi := maxs∈[s0i,s1i] h(τi(s), yi(s))) is a ρ
'
2
i -minimizer for (P Ki
i ). 
Ekeland’s theorem, together with relations (11.5.8), then tell us that there exists 
([s'
0i, s'
1i], (τ '
i, y'
i), z'
i) ∈ M × R that is an L∞ minimizer for: 
(Qi)
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize gi(τ (s0), y(s0), τ (s1), y(s1)) ∨ z(s1) + ρ'
i|z(s0) − z'
i|
+Ki
 s1
s0 ((h(τ (s), y(s)) − z(s)) ∨ 0)2ds
+ρi

|τ (s0) − τi(s0i)|+|y(s0) − yi(s0i)| + c(|s0 − s0i|+|s1 − s1i|)
+  s1
s0 | ˙τ (s) − ˙τie(s)|ds +  s1
s0 | ˙y(s) − ˙yie(s)|ds	
+ρ'
i

|τ (s0) − τ '
i(s'
0i
)|+|y(s0) − y'
i(s'
0i
)| + c(|s0 − s'
0i
|+|s1 − s'
1i
|)
+  s1
s0 | ˙τ (s) − ˙τ '
ie(s)|ds +  s1
s0 | ˙y(s) − ˙y'
ie(s)|ds	
over [s0, s1] ⊂ R, (τ, y, z) ∈ W1,1([s0, s1]; R1+n+1) satisfying
(τ (s), ˙ y(s), ˙ z(s)) ˙ ∈ {(w, wv, 0) : w ∈ [1 − ρ , 1 + ρ], v ∈ F (τ (s), y(s))},
a.e. s ∈ [s0, s1],
(τ (s0), y(s0), τ (s1), y(s1)) ∈ C,˜
|s0 − S¯|+|s1 − T¯|+‖ye − ¯xe‖L∞ ≤ ϵ'' .
Furthermore 
dM(([s'
0i, s'
1i], (τ '
i, y'
i)), ([s0i, s1i], (τi, yi)))
+ |z'
i − max s∈[s0i,s1i]
h(τi(s), yi(s))| ≤ ρ'
i . (11.5.22) 
We can find Lebesgue measurable functions w'
i : [s'
0i
, s'
1i] → R and v'
i :
[s'
0i, s'
1i] → Rn such that 
w'
i(s) ∈ [1 − ρ , 1 + ρ], v'
i(s) ∈ F (τ '
i(s), y'
i(s))
and (τ˙'
i(s), y˙
'
i(s)) = (w'
i(s), w'
i(s)v'
i(s)), a.e. .
From (11.5.22) it follows that 
|τ '
i(s'
0i) − τi(s0i)|, |y'
i(s'
0i) − yi(s0i)|, |s'
0i − s0i|, |s'
1i − s1i| → 0 and
||(w'
ie − wie, v'
ie − vie)||L1 → 0 as i → ∞. (11.5.23) 
We can deduce, with the help of Gronwall’s lemma, that 
||(τ '
ie, y'
ie) − (τie, yie)||L∞ → 0 . (11.5.24) 
Relations (11.5.5) and (11.5.23) imply that 
|τ '
i(s'
0i) − S¯|, |y'
i(s'
0i) − ¯x(S)¯ |, |s'
0i − S¯|, |s'
1i − T¯| → 0 and
||(w'
ie − χ[S,¯ T¯](= ˙
τ¯e), v'
ie − ˙
x¯e)||L1 → 0 as i → ∞.584 11 The Euler-Lagrange and Hamiltonian Inclusion Conditions in the Presence of. . .
Furthermore, for a subsequence, (w'
ie(s) − χ[S,¯ T¯](s), v'
ie(s) − ˙
x¯e(s)) → (0, 0) a.e.. 
We deduce from (11.5.21) and (11.5.22) that 
gi(τ '
i(s'
0i), y'
i(s'
0i), τ '
i(s'
1i), y'
i(s'
1i)) ∨ z'
i > 0 and z'
i > 0 , for each i . (11.5.25) 
Ekeland’s theorem also tells us that 
Ki

 s'
1i
s'
0i
((h(τ '
i(s), y'
i(s)) − z'
i) ∨ 0)
2ds (+ non-negative terms)
≤ gi(τi(s0i), yi(s0i), τi(s1i), yi(s1i)) ∨ max s∈[s0i,s1i]
h(τi(s), yi(s))
+ρ'
i| max s∈[s0i,s1i]
h(τi(s), yi(s)) − z'
i|
+ρ'
i

|τi(s0i) − τ '
i(s'
0i)|+|yi(s0i) − y'
i(s'
0i)| + c(|s0i − s'
0i|+|s1i − s'
1i|)
+

 s1i
s0i
| ˙τi(s) − ˙τ '
ie(s)|ds +

 s1i
s0i
| ˙yi(s) − ˙y'
ie(s)|ds	
.
The right side of this relation has value 0, in the limit as i → ∞, by (11.5.23) and 
(11.5.24). It follows that 
Ki

 s'
1i
s'
0i
((h(τ '
i(s), y'
i(s)) − z'
i) ∨ 0)
2ds → 0, as → ∞.
Now, for each i sufficiently large, apply the free end-time Euler Lagrange condition 
Theorem 9.4.1 (Chap. 9) to the problem (Qi), with reference to the L∞ minimizer 
(([s'
0i, s'
1i], (τ '
i, y'
i)), z'
i), noting that (following state augmentation and modification 
of the differential inclusion constraint to accommodate the integral cost terms) the 
relevant hypotheses are satisfied. We are assured then of the existence of λ˜i ∈ [0, 1]
and also of elements −di ∈ W1,1, −ri ∈ W1,1 and qi ∈ W1,1 (interpreted as costate 
components associated with the state variable components z and τ and y variables 
respectively) and a Borel measurable function γi = (γ0i, γ1i) : [s'
0i
, s'
1i] → R1+n
such that 
(λ˜i, qi) /= (0, 0), (11.5.26)
(− ˙ri(s), q˙i(s)) ∈ co{(ζ, η) : ((ζ, η), qi(s))
∈ NGr F ((τ '
i(s), y'
i(s)), v'
i(s)) (11.5.27)
+{0} × (|w'
i(s) − 1| |qi(s)| + λ˜i(ρi + ρ'
i)(1 + ρ))B}
+γi(s)(dμi/ds)(s), a.e. s ∈ [s'
0i, s'
1i]
γi(s) ∈ ∂h(τ '
i(s), y'
i(s)), μi − a.e. s ∈ [s'
0i, s'
1i],11.5 Non-degenerate Necessary Conditions 585
d˙
i(s) = (dμi/ds)(s), a.e. and di(s'
0i) ∈ ρ'
iλ˜iB, (11.5.28)
(v, w) → w(qi(s) · v − ri(s)) − ρiλ˜i(|w − wi(s)|
+|wv − wi(s)vi(s)|) − ρ'
iλ˜i(|w − w'
i(s)|+|wv − w'
i(s)v'
i(s)|)
is maximized over F (τ '
i(s), y'
i(s)) × [1 − ρ , 1 + ρ]
at (v'
i(s), w'
i(s)), a.e. s ∈ [s'
0i, s'
1i], (11.5.29)
((−ri(s0i), qi(s0i), ri(s1i), −qi(s1i)), di(s1i)) ∈
λ˜i∂

gi(τ '
i(s'
0i), y'
i(s'
0i
), τ '
i(s'
1i), yi(s1i)) ∨ z'
i(s'
1i)
	
+ (ξi, 0)
+
√
2 λ˜i(ρi + ρ'
i)B × {(0, 0)},
for some ξi ∈ NC˜(τ '
i(s'
0i), y'
i(s'
0i), τ '
i(s1i), y'
i(s'
1i)), (11.5.30)
sup
(v,w)∈F (τ '
i(s),y'
i(s))×[1−ρ ,1+ρ]
(qi(s) · v − ri(s))w
−λ˜iKi((h(τ '
i(s), y'
i(s)) − z'
i(s)) ∨ 0)
2 ∈ (ρi + ρ'
i)λ˜ic1[−1, +1],
for s = s'
0i and s'
1i. (11.5.31) 
Here, c1 := 2((1+ρ)(1+cF )+c/2) (as in Case 1) and μi is the (positive) measure 
on the Borel sets of the real line, with support in [s'
0i, s'
1i] and distribution: 
μi([s'
0i, s]) = 2Kiλ˜i


[s'
0i,s]
(h(τ '
i(σ ), y'
i(σ )) − z'
i) ∨ 0)dσ . (11.5.32) 
We see that 
supp {μi}⊂{s ∈ [s'
0i, s'
1i] : h(τ '
i(s), y'
i(s))) ≥ z'
i}. (11.5.33) 
Relation (11.5.31) results from the free-time transversality condition for the optimal 
end-times s'
0i and s'
1i in problem (Qi). But Ki((h(τ '
i(s), y'
i(s)) − z'
i(s)) ∨ 0)2 = 0
for s = s'
0i, s'
1i, since z'
i(s) > 0 and h(τ '
i(s), y'
i(s)) ≤ 0 for s = s0i and s1i. (We 
use here the fact that, for elements (τ0, y0, τ1, y1) ∈ C˜, we know h(τ0, y0) ≤ 0 and 
h(τ1, y1) ≤ 0.) (11.5.31) therefore implies that, for s = s'
0i and s'
0i, 
sup v∈F (τ '
i(s),y'
i(s))
qi(s) · v − ri(s) ∈ (ρi + ρ'
i)λ˜i
 c1
1 − ρ

[−1, +1]. (11.5.34) 
We deduce from (11.5.21) and (11.5.30), with the help of the max and sum rules, 
that there exists νi ∈ [0, 1] such that586 11 The Euler-Lagrange and Hamiltonian Inclusion Conditions in the Presence of. . .
(−ri(s'
0i), qi(s'
0i), ri(s'
1i), −qi(s'
1i)) ∈
λ˜iνi∂g(τ '
i(s'
0i), y'
i(s'
0i
), τ '
i(s'
1i), y'
i(s'
1i)) + ξi + √
2 λ˜i(ρi + ρ'
i)B
for some ξi ∈ NC˜(τ '
i(s'
0i), y'
i(s'
0i), τ '
i(s'
1i), y'
i(s'
1i)) (11.5.35) 
and 
||μi||T V =


[s'
0i,s'
1i]
(dμ/ds)ds ∈ λ˜i(1 − νi) + ρ'
iλ˜iB . (11.5.36) 
We can find a Lebesgue set S ⊂ (S,¯ T )¯ of full measure, with the following 
properties. Following a further extraction of subsequences we have: for each s ∈
S, (11.5.27) and (11.5.29) are valid, 
1 − ρ<wi(s) < 1 + ρ for all i suff. large and (11.5.37) 
(w'
ie(s), v'
ie(s)) → (1, ˙
x(s)), ¯ as i → ∞.
Take any s ∈ S. Considering first minimization w.r.t. the w variable and then the 
v variable, we deduce from (11.5.29) that, for i sufficiently large, 
qi(s) · v'
i(s) − ri(s) ∈ (ρi + ρ'
i)λ˜i(1 + cF )B
and 
qi(s) · v'
i(s) ≥ sup v∈F (τ '
i(s),τ '
i(s))
qi(s) · v − 2(1 − ρ)−1(ρi + ρ'
i)λ˜i(ρ + (1 + ρ)cF ) .
(11.5.38) 
But then 
H (τ '
i(s), y'
i(s), ri(s), qi(s))
= sup v∈F (τ '
i(s),τ '
i(s))
qi(s) · v − ri(s)	
∈ c3(ρi + ρ'
i)λ˜iB,
(11.5.39) 
in which c3 := (1 + cF ) + 2(1 − ρ)−1(ρ + (1 + ρ)cF ). Now define the function 
(ei, pi) ∈ W1,1([s'
0i
, s'
1i]; R1+n) according to 
(−ei, pi)(s) := (−ri, qi)(s) −


[s'
0i,s]
γi(σ )(dμi(σ )/dσ )(σ )dσ for s ∈ [s'
0i, s'
1i] .
(11.5.40) 
Condition (11.5.27) can be expressed in terms of (ei, pi): 
(− ˙ei(s), p˙i(s)) ∈ co{(ζ, η) : ((ζ, η), qi(s)) ∈ NGr F ((τ '
i(s), y'
i(s)), v'
i(s))
+{0} × (|w'
i(s) − 1| |qi(s)| + λ˜i(ρi + ρ'
i)(1 + ρ))B},
a.e. s ∈ [s'
0i, s'
1i] . (11.5.41)11.5 Non-degenerate Necessary Conditions 587
Notice next the (λ˜i, ξi) /= (0, 0). This is because, otherwise, qi = 0, in consequence 
of (11.5.28), (11.5.30) and Gronwall’s lemma: this contradicts (11.5.26). Now set 
λi := λ˜iνi. We see from (11.5.36) that ||μi||T V + λi ≥ (1 − ρ'
i)λ˜i. It follows that 
(λi, ξi, μi) /= (0, 0, 0). 
Normalize qi, pi, ξi, λi and λ˜i, dividing each by the positive number ||μi||T V +
λi + |ξi|. (We do not re-label.) Then, automatically, 
||μi||T V + λi + |ξi| = 1 . (11.5.42) 
Observe also that, following relabelling, λ˜i ≤ (1 − ρ'
i)−1 ≤ 2, since ρ'
i ≤ 1/2 for 
each i. 
Replacing λ˜iνi by λi in the transversality condition (11.5.35), we recognize rela￾tions (11.5.34), (11.5.35), (11.5.39), (11.5.40), (11.5.41) and (11.5.42) as perturbed 
versions of the desired conditions. The relations imply that the sequences with 
elements μi, (ei, pi), γi, (e,˙ p˙i), λi and ξi, are uniformly bounded (w.r.t. the total 
variation norm, the uniform norm, the L1 norm and Euclidean norms, respectively). 
Observe that [s'
0i, s'
1i]⊂[S¯ − ϵ'', T¯ + ϵ''] for all i. Therefore, we can consider the 
extension of elements (ei, pi) and (e˙i, p˙i) on the compact interval [S¯ − ϵ'', T¯ + ϵ'']
according to the preceding convention (by ‘constant extrapolation’ for the state 
arcs (ei, pi)’s and by ‘zero extrapolation’ for their velocities (ei, pi)’s). For each 
i (sufficiently large) we shall write μie the Borel measure on [S¯ − ϵ'', T¯ + ϵ'']
defined by 
μie(E) := μi(E ∩ [S¯ − ϵ'', T¯ + ϵ'']),
for all Borel subsets E ⊂ [S¯ − ϵ'', T¯ + ϵ'']. The Borel measurable functions γi’s 
are extended by ‘constant extrapolation’ on [S¯ − ϵ'', T¯ + ϵ'']. Consequently their 
extension satisfy the property 
γie(s) ∈ ∂h(τ '
ie(s), y'
ie(s)), μie − a.e. s ∈ [S¯ − ϵ'', T¯ + ϵ''] .
It follows that, after subsequences have been extracted, there exist limit points μ, 
e, e˙, p, p˙, γ , λ and ξ with respect to the appropriate topologies. But then there 
exists a function of bounded variation (r, q) such that (rie, qie)(s) → (re, pe)(s)
on a subset of [S,¯ T¯] of full Lebesgue measure, including the endpoints S¯ and T¯. 
(The subscript ‘e’ here means ‘extension by constant extrapolation’.) We can also 
arrange that ξi → ξ , for some ξ ∈ N˜C(S,¯ x(¯ S), ¯ T ,¯ x(¯ T )) ¯ . We deduce finally, with 
the help of a standard convergence analysis, the validity of all the assertions in 
the theorem statement, by passing to the limit as i → ∞. Notice, in particular that 
||μ||T V +λ+|ξ | = 1. This implies the non-triviality condition (λ, p, μ) /= (0, 0, 0)
since ‘(λ, p, μ) = (0, 0, 0)’ implies ξ /= 0 and hence p /= 0, which is not possible.
⨅⨆588 11 The Euler-Lagrange and Hamiltonian Inclusion Conditions in the Presence of. . .
11.6 Exercises 
11.1 Theorem 8.8.1 provides necessary conditions in the form of Clarke’s Hamil￾tonian inclusion for problems with dynamic constraints formulated as a differential 
inclusion. Consider a generalization of the optimization problem considered in 
Sect. 8.8, now to include the pathwise state constraint h(t, x(t)) ≤ 0, for all 
t ∈ [S, T ] :
(P11)
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize g(x(S), x(T ))
over x ∈ W1,1([S,T ]; Rn) such that
x(t) ˙ ∈ F (t, x(t)), for a.e. t ∈ [S,T ]
h(t, x(t)) ≤ 0, for all t ∈ [S,T ]
(x(S), x(T )) ∈ C .
Let x¯ be an L∞ local minimizer for (P11). Assume that, for some ϵ > 0 conditions 
(G1), (G2) and (G3)' of Theorem 8.8.1 are satisfied. Suppose also that h(., x) is 
upper semi-continuous for each x ∈ Rn and there exists kh > 0 such that 
|h(t, x) − h(t, x'
)| ≤ kh|x − x'
| for all x, x' ∈ ¯x(t) + ¯ϵB, t ∈ [S,T ] .
Then, prove that there exist p ∈ W1,1([S, T ]; Rn), μ ∈ C⊕(S, T ), a bounded 
Borel measurable function γ : [S, T ] → Rn and λ ≥ 0, satisfying the following 
conditions, in which q ∈ NBV ([S, T ]; Rn) is the function 
q(t) := 
p(S) if t = S
p(t) + 
[S,t] γ (s)dμ(s) if t ∈ (S, T ] .
(i): (λ, p, μ) /= (0, 0, 0), 
(ii): (− ˙p(t), ˙
x(t)) ¯ ∈ co ∂x,pH (t, x(t), ¯ q(t)), a.e. t ∈ [S, T ] ,
(iii): (q(S), −q(T )) ∈ λ∂g(x(S), ¯ x(T ¯ )) + NC(x(S), ¯ x(T ¯ )),
(iv): q(t) · ˙
x(t) ¯ ≥ q(t) · v for all v ∈ F (t, x(t)) ¯ a.e. t ∈ [S, T ], 
(v): supp{μ}⊂{t ∈ [S, T ] : h(t, x(t)) ¯ = 0} and γ (t) ∈ ∂>
x h(t, x(t)) ¯
for μ-a.e. t ∈ [S, T ] .
Hint: Pattern the earlier ‘state constraint-free’ analysis, employed in the proof of 
Theorem 8.8.1 and based on Stegall’s theorem, but now use the necessary conditions 
of Theorem 11.2.1 (and the remarks after Theorem 11.2.1), which allow for state 
constraints.11.7 Notes for Chapter 11 589
11.7 Notes for Chapter 11 
Clarke [65] first proved the Hamiltonian inclusion for dynamic optimization 
problems with pathwise state constraints and dynamics modelled by a differential 
inclusion with convex valued right side (‘convex velocity sets’). The validity of the 
generalized Euler Lagrange inclusion and the generalized Hamiltonian inclusion, 
also for convex velocity sets, was established by Loewen and Rockafellar [143]. 
The generalized Euler Lagrange inclusion, for state constrained problems with 
unbounded, possibly nonconvex, velocity sets satisfying a one sided Lipschitz 
continuity condition, was first derived by Vinter and Zheng [200]. An improved 
version of this theorem, proved under unrestrictive epi-Lipschitz continuity and 
tempered growth hypotheses on F (t, x) (as in Clarke’s stratified conditions for state￾constraint free problems [67]), is due to Bettiol et al. [34]. Recently, Ioffe [129] 
achieved a further improvement to the optimality condition, by incorporating his 
refinement. 
This chapter provides the latest version of the Euler Lagrange inclusion, includ￾ing the Ioffe refinement and proved under unrestrictive ‘stratified’ hypotheses. The 
proof technique is similar to that used in Chap. 8, to derive necessary conditions for 
state constraint-free problems. That is, necessary conditions for the original problem 
are obtained as the limit of necessary conditions for a ‘finite Lagrangian’ problem, 
constructed from the scaled distance function. Necessary conditions for the finite 
Lagrangian problem, in which a state constraint is present, are derived, now with 
the help of the smooth state constrained maximum principle. 
Free end-time necessary conditions for state constrained differential inclusion 
problems, with data measurable in time, were first derived by Clarke, Loewen 
and Vinter [83]. Refinements, to allow for state constraints which are active at the 
optimal end-times, were investigated in [179]. 
The non-degeneracy phenomenon was earlier investigated in Chap. 10, in the 
context of state-constrained dynamic optimization problems involving a controlled 
differential equation. The phenomenon is encountered also for problems involving a 
differential inclusion. Non-degenerate forms of the Hamiltonian inclusion for such 
problems were derived by Aseev [6] and Vinter [194] under the assumption that the 
velocity set is convex. The necessary conditions in this chapter (Theorem 11.5.1) 
are new [31]; they improve on earlier conditions because they allow the velocity set 
to be non-convex and they incorporate the (partially convexified) Euler Lagrange 
inclusion.Chapter 12 
Regularity of Minimizers 
Abstract This chapter provides conditions under which minimizers for dynamic 
optimization problems possess regularity properties of interest, such as Lipschitz 
continuity or boundedness of higher derivatives. We discuss the significance of 
regularity analysis, regarding the derivation of necessary conditions of optimality, 
the selection of numerical schemes for the computation of optimal strategies and the 
implementation of such strategies. 
Tonelli identified conditions (Tonelli existence hypotheses), under which the 
basic problem in the calculus of variations has a minimizer in the class of absolutely 
continuous functions that satisfy specified end-point conditions. Tonelli’s existence 
hypotheses do not guarantee, however, that minimizers satisfy the Euler Lagrange 
condition; this is a set-back to solution techniques based on seeking a minimizer 
among arcs satisfying standard necessary conditions of optimality. For problems 
with smooth Lagrangians and in which the arcs are scalar valued, Tonelli showed 
however that, under his existence hypotheses, the velocity of the minimizing 
arcs is locally essentially bounded at all times in an open set of full Lebesgue 
measure. This regularity property can be exploited, to show that minimizers 
satisfy the Euler Lagrange condition under additional hypotheses. In sections of 
this chapter dealing with Tonelli regularity, we cover subsequent extensions of 
Tonelli’s original discoveries: the velocities of minimizers are essentially locally 
bounded on a relatively open subset of full measure of the reference time interval, 
under Tonelli’s existence hypotheses, even when we allow vector valued arcs and 
nonsmooth Lagrangians. We show that this extended Tonelli regularity property 
leads to improved criteria for validity of the Euler Lagrange condition. The proof 
of these extensions, which is based on the construction of auxiliary Lagrangians, 
is a showcase for application of nonsmooth methods, which introduce a new 
flexibility into the use of Tonelli’s methods, by allowing us to construct nonsmooth 
auxiliary Lagrangians. These aspects of Tonelli regularity theory relate to calculus 
of variations problems. We briefly consider extensions of the theory gives conditions 
for the essential boundedness of the minimizing control, for dynamic optimization 
problems associated with a linear control system. 
The chapter also includes material on another approach to establishing regularity 
of minimizers, in circumstances when standard necessary conditions are not valid 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
P. Bettiol, R. B. Vinter, Principles of Dynamic Optimization, Springer Monographs 
in Mathematics, https://doi.org/10.1007/978-3-031-50089-3_12
591592 12 Regularity of Minimizers
and therefore cannot be (directly) used in the regularity analysis. This is based 
on reparameterization of the minimizer by a change of independent variable. 
Applications of this approach, when first introduced, were restricted to autonomous 
problems satisfying the superlinear growth conditions in the Tonelli existence 
hypotheses. We take account of recent extensions of reparameterization methods, 
which can be used to establish regularity properties of minimizers, specifically 
Lipchitz continuity, in some situations where the problem concerned is non￾autonomous and the superlinear growth hypothesis is replaced by a less restrictive, 
slow growth hypothesis. 
In the final section of this chapter, we restrict attention to certain dynamic 
optimization problems with dynamic constraint a controlled differential equation, 
where minimizers are known to satisfy the maximum principle. Here we show how 
these necessary conditions can be applied directly to establish Lipschitz continuity 
of state trajectories. 
12.1 Introduction 
In this chapter we seek information about regularity of minimizers. When do 
minimizing arcs have essentially bounded derivatives, higher order derivatives or 
other qualitative properties of interest in applications? 
Minimizer regularity has a number of important implications. In the computation 
of minimizers for example, the choice of efficient discretization schemes and 
numerical procedures depends on minimizer differentiability. Again, in control 
engineering applications, where an dynamic optimization strategy is implemented 
digitally, the quality of the approximation of an ideal ‘continuous time’ strategy 
depends on minimizer regularity. Minimizer regularity can also help us to predict 
natural phenomena, governed by variational principles. In particular, studying the 
regularity of minimizers for variational problems arising in nonlinear elasticity, 
gives insights into mechanisms for material failure. 
We begin however by focusing on the fundamental role of regularity theory, 
discussed in Chap. 1, to provide a solid foundation for Tonelli’s direct method. 
Recall that this involves the following steps: 
Step 1: Establish that the problem has a minimizer. 
Step 2: Derive necessary conditions for an arc to be a minimizer. 
Step 3: Search among ‘extremals’ (arcs satisfying the necessary conditions) for 
an arc with minimum cost; this will be a minimizer. 
The direct method fails when it is applied to any class of dynamic optimization 
problems for which minimizers exist, but when necessary conditions of optimality 
are available only under hypotheses which the minimizers may possibly violate. An 
example of this phenomenon is given in Chap. 1. In this example, finding an arc 
which satisfies the necessary conditions yields a unique extremal. However this 
extremal is not a minimizer, because the minimizers are not located among the 
extremals.12.1 Introduction 593
Regularity analysis enters the picture because it helps us to identify classes 
of problems, for which all minimizers satisfy known necessary conditions of 
optimality. It thereby justifies searching for minimizers among extremals. 
At the same time, we anticipate difficulties in investigating minimizer regularity, 
with a view to justifying the direct method. An obvious approach is to draw 
conclusions about minimizer regularity from the necessary conditions. Yet, to justify 
the direct method, we need to establish regularity of precisely those minimizers for 
which satisfaction of the necessary conditions is not guaranteed a priori. 
Much of this chapter is devoted to deriving regularity properties of solutions to 
the basic problem in the calculus of variations, concerning the minimization of an 
integral functional (with finite Lagrangian) over absolutely continuous vector valued 
arcs with fixed end-points: 
(BP )
⎧
⎨
⎩
Minimize  T
S L(t, x(t), x(t))dt ˙
over x ∈ W1,1([S,T ]; Rn) satisfying
(x(S), x(T )) = (x0, x1).
Here, [S,T ] is a fixed interval with T >S, L : [S,T ] × Rn × Rn → R is a given 
function and x0, x1 are given n-vectors. 
No apology is made for limiting attention to fixed end-point problems. If an arc x¯
is a minimizer for any other kind of end-point condition, then x¯ is a minimizer also 
for the related fixed end-point problem with x0 := ¯x(S), x1 := ¯x(T ). So regularity 
properties for fixed end-point problems imply the same regularity for all other kinds 
of boundary conditions. 
On the other hand, the fact that the formulation (BP) covers only dynamic 
optimization problems for which the velocity variable in unconstrained (traditional 
variational problems) is a genuine shortcoming. We remedy it, to some extent, by 
studying some generalizations in the closing sections of the chapter, to allow for 
dynamic constraints. 
Our starting point is the following well-known set of conditions (HE1)–(HE3) 
for the existence of minimizers to (BP). Because of their close affinity to conditions 
considered by Tonelli in his pioneering work on existence of minimizers, we refer 
to them as the Tonelli existence hypotheses. 
Theorem 12.1.1 (Tonelli Existence Theorem) Assume that the data for (BP) 
satisfy the following hypotheses 
(HE1) (Convexity, etc.) L is bounded on bounded sets, L(., x, v) is measurable 
for each (x, v) and L(t, x, .) is convex for each (t, x), 
(HE2) (Uniform Local Lipschitz Continuity) For each N > 0, there exist kN >
0 such that 
|L(t, x, u) − L(t, x'
, u'
)| ≤ kN |(x, v) − (x'
, v'
)|
for all (x, v), (x'
, v'
) ∈ NB, a.e. t ∈ [S,T ],594 12 Regularity of Minimizers
(HE3) (Coercivity) There exist α ≥ 0 and a convex function θ : [0,∞) →
[0,∞) satisfying limr→∞ θ (r)/r = +∞ such that 
L(t, x, v) ≥ θ (|v|) − α|x| for all (x, v) ∈ Rn × Rn, a.e. t ∈ [S,T ].
Then (BP) has a minimizer. 
This is a special case of the existence theorem for the generalized Bolza problem 
proved in Chap. 2. (See Theorem 2.4.1.) 
Chapter 8 provides the following necessary conditions of optimality for (BP): 
Proposition 12.1.2 Let x¯ be an L∞ local minimizer for (BP). Assume that, in 
addition to (HE1)–(HE2), there exists k ∈ L1 and ϵ > 0 such that 
|L(t, x, v) − L(t, x'
, u'
)| ≤ k(t)|(x, v) − (x'
, v'
)| (12.1.1) 
for all (x, v), (x'
, v'
) ∈ (x(t), ¯ ˙
x(t)) ¯ + ϵB, a.e.. 
Then, there exists an arc p ∈ W1,1([S,T ]; Rn) such that 
p(t) ˙ ∈ co {q : (q, p(t)) ∈ ∂L(t, x(t), ¯ ˙
x(t)) ¯ } a.e. (12.1.2) 
(where ∂L denotes the limiting subdifferential of L(t, ., .)) and 
p(t) · ˙
x(t) ¯ − L(t, x(t), ¯ ˙
x(t)) ¯ = max
v∈Rn
{p(t) · v − L(t, x(t), v) ¯ } a.e.. (12.1.3) 
Furthermore, if L is independent of t (write L(x, v) in place of L(t, x, v)), then 
there exists a constant a such that 
a = p(t) · ˙
x(t) ¯ − L(x(t), ¯ ˙
x(t)) ¯ a.e..
Note that, since L(t, x, .) is a convex function and the subdifferential for convex 
finite valued functions on Rn coincides with the limiting subdifferential, (12.1.3) 
implies 
p(t) ∈ ∂vL(t, x(t), ¯ ˙
x(t)) ¯ a.e..
Proof The arc 
x,¯ z(t) ¯ =  t
S L(s, x(s), ¯ ˙
x(s))ds ¯

is an L∞ local minimizer for the 
optimal dynamic optimization problem 
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize z(T )
over (x, z) ∈ W1,1([S,T ]; Rn+1) satisfying
(x(t), ˙ z(t)) ˙ ∈ F (t, x(t), z(t)),
(x(S), z(S)) = (x0, 0),
(x(T ), z(T )) ∈ {x1} × R.12.1 Introduction 595
Here, 
F (t, x, z) := {(v, L(t, x, v)) : v ∈ ˙
x(t) ¯ + B}
The hypotheses hold under which the generalized Euler Lagrange condition (Theo￾rem 8.4.3) is valid (observe that condition (G3)*** in Proposition 8.5.1 is satisfied 
and so Corollary 8.5.2 guarantees the validity of the necessary conditions of 
Theorem 8.4.3). We deduce the existence of an arc (p, r) ∈ W1,1([S,T ]; Rn+1)
and a constant λ ≥ 0 such that ((p, r), λ) /= (0, 0), r(T ) = λ and 
(p(t), ˙ r(t)) ˙ ∈ co{(q, w) : (q, w, p(t), r(t))
∈ NGr F (t,.)(x(t), ¯ z(t), ¯ ˙
x(t), ¯ ˙
z(t)) ¯ } a.e. t ∈ [S,T ].
By considering limits of proximal normal vectors we readily deduce that 
‘(q, w, p, −r) ∈ NGr F (t,.)(x,¯ z,¯ ˙
x,¯ ˙
z)¯ ’ implies ‘w = 0’.
and therefore r ≡ λ (≥ 0). Clearly, λ = 0 would imply that (p ≡ 0, r ≡ 0). It 
follows that necessarily we have r ≡ λ > 0. We can therefore arrange by scaling 
the multipliers that r = λ = 1. The analysis of limits of proximal normals vectors 
now provides the following implication 
‘(q, 0, p, −1) ∈ NGr F (t,.)(x,¯ z,¯ ˙
x,¯ ˙
z)¯ ’ implies ‘(q, p)
∈ ∂L(t, x,¯ ˙
x)¯ and p ∈ ∂vL(t, x,¯ ˙
x)¯ ’.
So we deduce the validity of (12.1.1), (12.1.2) and, since v → L(t, x, v) is convex, 
∂vL(t, x(t), ¯ ˙
x(t)) ¯ coincides with the subdifferential of v → L(t, x(t), v) ¯ (at v = ˙
x(t) ¯ ) in the sense of the convex analysis. It follows that 
p(t) · (v − ˙
x(t)) ¯ ≤ L(t, x(t), v) ¯ − L(t, x(t), ¯ ˙
x(t)), ¯ for all v ∈ Rn, a.e. t ∈ [S,T ],
confirming (12.1.3). If L is independent of t, Corollary 8.5.2 (cf. condition (v) of 
Theorem 8.4.3) also tells us that p can be chosen to satisfy 
a = p(t) · ˙
x(t) ¯ − λL(x(t), ¯ ˙
x(t)) ¯ a.e. t ∈ [S,T ],
for some constant a. Then, the assertions of the proposition are all confirmed. 
⨅⨆
Proposition 12.1.2 falls short of requirements, from the point of view of 
identifying a large class of problems, of type (BP), for which there exist minimizers 
and for which all minimizers satisfy known necessary conditions of optimality. This 
is because of the ‘extra’ hypothesis (12.1.1) therein invoked, to justify the necessary 
conditions. (Note that hypothesis (HE2) does not imply (12.1.1) when ˙
x¯ fails to be 
essentially bounded.)596 12 Regularity of Minimizers
At the outset, we need to ask whether the presence of the troublesome hypothesis 
(12.1.1) in Proposition 12.1.2 merely reflects a weakness in our analysis and can be 
dispensed with in situations in which the Tonelli existence hypotheses are satisfied, 
or whether it points to genuine restrictions on classes of problems for which the 
optimality conditions of Proposition 12.1.2 can be confirmed. 
This question was resolved by an example of Ball and Mizel, exposing the gap 
between the Tonelli existence Hypotheses and hypotheses needed for the derivation 
of Euler Lagrange-type necessary conditions. 
Example (Ball Mizel) 
⎧
⎨
⎩
Minimize  1
0
	
rx˙2(t) + (x3(t) − t2)2x˙14(t)

dt
over x ∈ W1,1([0, 1]; R) satisfying
x(0) = 0, x(1) = k.
Here, r > 0 and k > 0 are constants, linked by the relation 
r = (2k/3)
12(1 − k3)(13k3 − 7).
It can be shown that there exists ϵ > 0 such that, for all k ∈ (1 − ϵ, 1), the arc 
x(t) ¯ := kt2/3 (12.1.4) 
is the unique minimizer for this problem. (This is by no means a straightforward 
undertaking!) 
It is a simple matter to check by direct substitution that x¯ satisfies, a.e., a 
pointwise version of the Euler condition: 
d/dt Lv(t, x(t), ¯ ˙
x(t)) ¯ = Lx (t, x(t), ¯ ˙
x(t)), ¯
in which 
L(t, x, v) = rv2 + (x3 − t
2)
2v14.
However this cannot be expressed as an Euler Lagrange type condition in terms of 
an absolutely continuous adjoint arc p, 
(p(t), p(t)) ˙ = ∇L((t, x(t), ¯ ˙
x(t))), ¯
because the only candidate for adjoint arc p, namely 
p(t) = −αt−1/3
(for some α /= 0 depending on k), is not an absolutely continuous function.12.1 Introduction 597
The train of thought behind this example, incidently, is that the Lagrangian (x3 −
t2)2m| ˙x|
2k, for any integers m > 0 and k > 0, has a minimizer x(t) = t2/3 with 
unbounded derivative. It fails however to exhibit ‘bad’ behaviour under the Tonelli 
existence hypotheses, because the coercivity condition (HE3) is violated. To ensure 
satisfaction also of (HE3) we now add a small quadratic coercive term ‘ϵ| ˙x|
2 to the 
Lagrangian. It then turns out that the values of m and k can be adjusted to ensure 
that x(t) = kt3/2 is a pointwise solution to the Euler equation, for some k ∈ (0, 1). 
The Ball Mizel example is of interest not only for confirming that the Tonelli 
existence hypotheses alone fail to ensure validity of standard necessary conditions 
such as the Euler Lagrange condition. It also helps us to predict the nature of 
minimizers under these hypotheses. 
A salient feature of the minimizing arc (12.1.4) is that the minimizer is in some 
sense badly behaved only on some small subset of the underlying time interval 
[0, 1], namely {0}. 
It is a remarkable fact that all minimizers to (BP) share this property. The 
key result of the chapter, the generalized Tonelli regularity theorem, makes precise 
this assertion: it says that, merely under the Tonelli existence hypotheses, the bad 
behaviour of each minimizer x¯ is confined to a closed set of zero measure. In this 
context, t
¯ is defined to be a point of bad behaviour for x¯ if x¯ fails to be Lipschitz 
continuous on any neighbourhood of t
¯ in [S,T ]. 
We commented earlier on an inherent difficulty in establishing regularity prop￾erties of minimizers, namely that we would like to exploit regularity implications 
of standard necessary conditions, but cannot do this in interesting cases when the 
hypotheses, under which these necessary conditions can been derived, are violated. 
This brings us to one of the most valuable aspects of the generalized Tonelli regu￾larity theorem. It permits us to derive a weakened, ‘local’ version of the generalized 
Euler Lagrange condition merely under the Tonelli existence hypotheses. We can 
combine information about minimizers inherent in this optimality condition with 
additional hypotheses on the data for (BP), to establish refined regularity properties 
for special classes of problems. 
The role of the generalized Tonelli regularity theorem to generate refined 
regularity theorems for special classes of problems is investigated at length in 
Sect. 12.4. A particularly striking result, first proved via the generalized Tonelli 
regularity theorem, is Proposition 12.4.2 below: 
Suppose that L(t, x, v) is independent of t. Then, under the Tonelli 
existence hypotheses, all minimizers for (BP) are Lipschitz continuous. 
To paraphrase, existence of points of bad behaviour is a phenomenon associated 
solely with non-autonomous problems. 
A related topic to regularity analysis is investigation of the so-called Lavrentiev 
phenomenon. This is the surprising fact that, for certain variational problems 
posed over spaces of arcs x : [S,T ] → Rn, the infimum cost over the space 
of Lipschitz continuous functions is strictly less that the infimum cost over the598 12 Regularity of Minimizers
space of absolutely continuous functions; surprising because the Lipschitz functions 
comprise a dense subset of the space of absolutely continuous functions, with 
respect to the strong W1,1 topology. Typical applications of the generalized Tonelli 
regularity theorem identify classes of problems for which minimizers over the class 
of absolutely continuous functions are in fact Lipschitz continuous. The link is of 
course that all such applications inform us about special cases of (BP) for which 
the Lavrentiev phenomenon cannot occur. 
The concluding sections concern regularity of minimizers for optimization 
problems with dynamic and pathwise constraints. We indicate how the analysis 
underlying the generalized Tonelli regularity theorem can, in some cases, be 
extended to cover dynamic optimization problems involving linear dynamic con￾straints. We also examine some situations in which the application of ‘standard’ 
necessary conditions for dynamic optimization problems with dynamic constraints 
and pathwise state constraints leads directly to regularity information. 
The choice of topics in this chapter on minimizer regularity is motivated in part 
by a desire to illustrate the benefits of applying nonsmooth analysis, in smooth as 
well as nonsmooth settings. To the authors’ knowledge, the only available proof of 
the generalized Tonelli regularity theorem, for vector valued arcs, is via nonsmooth 
analysis, even when the data is smooth. 
The proof techniques involve the construction of an auxiliary Lagrangian and 
the comparison of minimizers for the origin and auxiliary Lagrangians on suitable 
subintervals. Since we are unable to apply the standard necessary conditions to 
minimizers for the original problem, we establish regularity properties indirectly, 
by applying them to the problem with the auxiliary Lagrangian, which does 
satisfy the relevant hypotheses. So everything hinges on the construction of the 
auxiliary Lagrangian. This is most simply carried out by taking convex hulls of 
certain epigraph sets, operations which generate nonsmooth functions. Nonsmooth 
necessary conditions are then required, to analyse the solutions to the auxiliary 
problem, even if the original Lagrangian is smooth. 
In the n = 1 case (scalar valued arcs) and for strictly convex Lagrangians, Tonelli 
showed how to construct a smooth auxiliary Lagrangian with suitable properties. 
But the task of carrying out such an exercise for vector valued arcs would appear to 
be a formidable one and, since nonsmooth necessary conditions are now available, 
unnecessary. 
In this chapter we also consider another approach to establishing regularity 
of minimizers, in situations when standard necessary conditions are not available 
and therefore cannot be (directly) exploited for regularity analysis. The approach 
involves introducing a family of reparameterizations of the minimizer by a means 
of transformations of independent variable. A variational analysis, in which ‘vari￾ations’ are generated by these transformations, can be used to extract regularity 
information about the minimizer. Applications of this approach were restricted to 
autonomous problems satisfying the superlinear growth conditions in the Tonelli 
existence hypotheses. This chapter supplies recent extensions of reparameterization 
methods, which can be used to establish Lipschitz continuity of minimizers, in some12.2 Tonelli Regularity 599
situations where the problem concerned is non-autonomous and the superlinear 
growth hypothesis is replaced by a less restrictive, slow growth hypothesis. 
In the final section of this chapter, we restrict attention to certain dynamic 
optimization problems with dynamic constraint a controlled differential equation, 
where minimizers are known to satisfy the maximum principle. Here we how show 
these necessary conditions can be applied directly to establish Lipschitz continuity 
of state trajectories. 
12.2 Tonelli Regularity 
In a landmark paper published in 1915, Tonelli [189] developed a technique for 
establishing regularity properties of solutions to (BP), under hypotheses similar to 
(HE1)–(HE3), based on the construction of auxiliary Lagrangians. The regularity 
properties of minimizers proved in this chapter, which go beyond Tonelli’s results 
in a number of significant respects, are the fruits of combining Tonelli’s technique 
and modern nonsmooth analytical methods. 
A cornerstone of the Tonelli’s regularity analysis is the concept of a regular point: 
Definition 12.2.1 Take a function y : [S,T ] → Rn and a point τ ∈ [S,T ]. We say 
τ is a regular point of y if 
lim inf
S≤si≤τ≤t
i≤T
si→τ,ti→τ,si/=ti
|y(ti) − y(si)|
|ti − si|
< +∞. (12.2.1) 
Otherwise expressed, τ is a regular point of y if there exist si → τ , ti → τ and 
K > 0 such that, for all i
si /= ti and S ≤ si < ti ≤ T
and 
|y(ti) − y(si)| ≤ K|ti − si|.
Because the definition involves limits of difference quotients, it would at first 
sight appear that assuming y is regular at t comes close to assuming that it is 
Lipschitz continuous near t. In fact the definition of ‘regular point’ is much less 
restrictive than this. For example, t = 0 is a regular point of y(s) = |s|
1
2 , 
−1 ≤ s ≤ +1, because 
|y(ti) − y(si)|
|ti − si| = 0 for all i,600 12 Regularity of Minimizers
when we choose si = −i−1 and ti = +i−1 for i = 1, 2,... Yet this function has 
unbounded slope near t = 0. 
Theorem 12.2.2 (Generalized Tonelli Regularity Theorem) Assume that the 
Tonelli existence hypotheses (HE1)–(HE3) are satisfied. A solution to (BP) exists. 
Let x¯ be any L∞ local minimizer for (BP). Take any regular point τ for x¯. Then 
there exists a relatively open subinterval I ⊂ [S,T ] with the properties: τ ∈ I and 
the restriction of x¯ to I is Lipschitz continuous. 
We defer proof of the theorem until the next section. Here we examine some of 
its implications. 
Take an arbitrary L∞ local minimizer x¯ for (BP). Let D be the subset of [S,T ]
comprising points at which x¯ is differentiable. Since x¯ is an absolutely continuous 
function, it is differentiable almost everywhere. Consequently D has full measure. 
Take any t ∈ D. Then, since x¯ is differentiable at t, 
lim
i
| ¯x(ti) − ¯x(si)|
|ti − si|
< +∞
for some sequences ti → t and si → s such that S ≤ si ≤ t ≤ ti ≤ T and ti > si
for each i. We see that t is a regular point of x¯. According to the preceding theorem, 
we can choose a relatively open interval It ⊂ [S,T ], containing t, such that x is 
Lipschitz continuous on It . 
Now set 
Ω = ∪t∈DIt .
Ω is a relatively open set because it is a union of relatively open sets. It has full 
measure because the subset D has full measure. We see also that x¯ is locally 
Lipschitz continuous on Ω, in the sense that for any t ∈ Ω we can find a 
neighbourhood of t (It serves the purpose) on which x¯ is Lipschitz continuous. 
We have drawn the following important conclusions from Theorem 12.2.2: 
Corollary 12.2.3 Assume the Tonelli existence hypotheses (HE1)–(HE3) are satis￾fied. Take any L∞ local minimizer x¯ for (BP). Then there exists a relatively open 
set of full measure, Ω, such that x¯ is locally Lipschitz continuous on Ω. 
For a given absolutely continuous function x¯, let Ωmax(x)¯ be the union of all 
relatively open sets Ω with the property that x¯ is locally Lipschitz continuous on 
Ω. Then Ωmax is a relatively open subset of [S,T ], of full measure, and x¯ is locally 
Lipschitz continuous on Ωmax(x)¯ . We define 
S := [S,T ]\Ωmax(x)¯
to be the Tonelli set for x¯. The Tonelli set, which we think of as the set of times at 
which x¯ exhibits bad behaviour can be alternatively described as the set of points t
in [S,T ] such that x¯ has unbounded slope in the vacinity of t.12.2 Tonelli Regularity 601
Summarizing and extending these results, we arrive at 
Theorem 12.2.4 Assume that the data for (BP) satisfy the Tonelli existence 
hypotheses (HE1)–(HE3). Take any L∞ local minimizer x¯. Then the Tonelli set S
for x¯ is a (possibly empty) closed set of zero measure. We have: 
(i) Given any closed subinterval I ⊂ [S,T ]\S, x¯ is Lipschitz continuous on I , 
(ii) Given any closed interval I ⊂ [S,T ]\S, there exists some p ∈ W1,1(I ; Rn)
such that, for a.e. t ∈ I , 
p(t) ˙ ∈ co {q : (q, p(t)) ∈ ∂L(t, x(t), ¯ ˙
x(t)) ¯ }
and 
p(t) · ˙
x(t) ¯ − L(t, x(t), ¯ ˙
x(t)) ¯ = max
v∈Rn {p(t) · v − L(t, x(t), v) ¯ },
(iii) Suppose in addition to (HE1)–(HE3) that, for each t ∈ [S,T ] and w ∈ Rn, 
L(., x(t), w) ¯ is continuous at t and L(t, x(t), .) ¯ is strictly convex. Then x¯ is 
continuously differentiable on [S,T ]\S, 
(iv) Suppose in addition to the hypotheses of (iii) that, for each t ∈ [S,T ], the 
function L is Cr on a neighbourhood of (t, x(t), ¯ ˙
x(t)) ¯ (for some integer r ≥ 2)
and Lvv(t, x(t), ¯ ˙
x(t)) > ¯ 0. Then x¯ is r-times continuously differentiable on 
[S,T ]\S. 
In (iii) and (iv), the assertion ‘x¯ is Cr on [S, b)’ is taken to mean that x¯ is of class 
Cr on the open set (S, b) and all derivatives of order r or less have limits as t ↓ S. 
We interpret ‘x¯ is Cr on (b, T ], etc., likewise. 
Proof We know already that S is a closed set of zero measure, outside of which x¯
is locally Lipschitz continuous. 
(i): Take a closed interval I ⊂ [S,T ]\S, Since x¯ is locally Lipschitz continuous on 
the compact interval I , x¯ is (globally) Lipschitz continuous on I , 
(ii): Take any closed interval [a, b]⊂[S,T ]\S. Then x¯ restricted to [a, b] is 
an L∞ local minimizer for (BP) with end-point constraints x(a) = ¯x(a) and 
x(b) = ¯x(b). Because x¯ is Lipschitz continuous on [a, b] and the relevant Lipschitz 
continuity hypothesis (12.1.1) is satisfied, the asserted necessary conditions follow 
from Proposition 12.1.2, 
(iii): Take any point t in the relatively open set [S,T ]\S. Then there exists some 
relatively open interval containing t, with end-points a and b and such that [a, b] ⊂
[S,T ]\S. We know L is Lipschitz continuous on [a, b] and that there exists p ∈
W1,1([a, b]; Rn) such that, for a.e. t ∈ [a, b], 
p(t)· ˙
x(t) ¯ −L(t, x(t), ¯ ˙
x(t)) ¯ ≥ p(t)· v −L(t, x(t), v) ¯ for all v ∈ Rn. (12.2.2)602 12 Regularity of Minimizers
If the bounded function ˙
x¯ is not almost everywhere equal to a continuous function 
on [a, b] there exists r ∈ [a, b] and sequences {ci} and {di} converging to r in [a, b], 
such that (12.2.2) is satisfied for t = r and t = ci, t = di, i = 1, 2,... , the limits 
α := lim
i→∞
˙
x(c ¯ i), β = lim
i→∞
˙
x(d ¯ i)
exist and α /= β. 
Fix v ∈ Rn. Under the additional hypotheses we have 
L(r, x(r), v) ¯ = lim
i→∞ L(ci, x(c ¯ i), v),
L(r, x(r), α) ¯ = lim
i→∞ L(ci, x(c ¯ i), ˙
x(c ¯ i)).
Setting t = ci in (12.2.2) and passing to the limit as i → ∞ gives 
p(r) · v − L(r, x(r), v) ¯ ≤ p(r) · α − L(r, x(r), α). ¯
Using the same arguments in relation to the sequence {di}, we arrive at 
p(r) · v − L(r, x(r), v) ¯ ≤ p(r) · β − L(r, x(r), β). ¯
These inequalities are satisfied for arbitrary points v ∈ Rn. We have shown that the 
function 
v → p(r) · v − L(r, x(r), v) ¯
achieves its maximum at the points v = α and v = β. Since it is strictly concave, we 
deduce that α = β. From this contradiction we conclude that an arbitrary point t ∈
[S,T ]\S is contained in a relatively open interval on which ˙
x¯ is almost everywhere 
equal to a continuous function. It follows that ˙
x¯ is continuous on [S,T ]\S. 
(iv): Take any point s ∈ [S,T ]\S. Then s is contained in some relatively 
open subinterval of [S,T ] with end-points a and b, such that [a, b]⊂[S,T ]\S. 
x¯, restricted to [a, b], is an L∞ local minimizer to (BP) for ‘end-point data’ 
(a, x(a), b, ¯ x(b)) ¯ . Since, by (iii), this subarc has essentially bounded derivative, we 
deduce from Proposition 12.1.2 the existence of a vector d such that 
Lv(t, x(t), ¯ ˙
x(t)) ¯ = d +
 t
a
Lx (σ, x(σ ), ¯ ˙
x(σ ))dσ ¯ (12.2.3) 
for all t ∈ [a, b]. The right side is C1 and so is (t, v) → Lv(t, x(t), v) ¯ . Since 
Lvv > 0, it follows from the implicit function Theorem that ˙
x¯ is C1, from which it 
follows that x¯ is C2. (12.2.3) therefore implies 
d2x(t)/dt ¯ 2 = [Lvv]
−1{Lx − Lvt − Lvx · ˙
x(t) ¯ }, (12.2.4)12.2 Tonelli Regularity 603
on (a, b). (The derivatives are evaluated at (t, x(t), ¯ ˙
x(t)) ¯ .) It follows that d2x/dt ¯ 2
is continuous (and has limits at a and b). Now suppose we know that x¯ is Cr−1
on [a, b] and that L is Cr (for some integer r ≥ 2). The right side of (12.2.4) is 
Cr−2, and therefore d2x/dt ¯ 2 is Cr−2. It follows that x¯ restricted to I is Cr (and if 
dr−1x/dt ¯ r−1 has limits at the end-points of I , then so does drx/dt ¯ r.) The fact that 
x¯ is Cr on a relatively open subinterval containing s now follows by induction. ⨅⨆
Thus far, we have placed no restrictions on the dimension n of the state 
variable. If n = 1, then minimizing arcs have the surprising property that they 
are differentiable at every point in [S,T ], even on their Tonelli sets. To justify such 
assertions however, we must allow derivatives which take values +∞ or −∞. 
Theorem 12.2.5 Assume that, in addition to the Tonelli Existence Hypotheses 
(HE1)–(HE3), for each t ∈ [S,T ] and w ∈ Rn L(., x(t), w) ¯ is continuous at t
and L(t, x(t), .) ¯ is strictly convex. Suppose further that 
n = 1.
Let x¯ be a minimizer. Then x¯ is everywhere differentiable on [S,T ], in the sense that 
the following limit exists (finite or infinite) for each τ ∈ [S,T ]: 
lim
t→τ
a≤t≤b
x(t) ¯ − ¯x(τ )
t − τ . (12.2.5) 
Proof If τ is such that the left side of (12.2.1) is finite then, as we have shown in 
(iii) of Theorem 12.2.4, x¯ is C1 near τ . In this case, the limit (12.2.5) exists and is 
finite. So we may assume that the left side of (12.2.1) is infinite. 
Suppose that τ = S. The limit (12.2.5) can fail to exist only if 
lim supt↓S
x(t) ¯ − ¯x(S)
t − S = +∞ and lim inft↓S
x(t) ¯ − ¯x(S)
t − S
= −∞.
(12.2.6) 
But since x¯ is continuous, we deduce from these two relations that there exist points 
t, arbitrarily close to S, with x(t) ¯ = ¯x(S). It follows that the left side of (12.2.1) is 
finite, a contradiction. We show similarly that limit (12.2.5) exists also when τ = T . 
It remains to consider the case when τ lies in (S, T ) and the left side of (12.2.1) 
is infinite. Reasoning as above, we justify restricting attention to the case when 
limt↓τ
x(t) ¯ − ¯x(τ )
t − τ
= +∞ and lims↑τ
x(s) ¯ − ¯x(τ )
s − τ
= −∞. (12.2.7) 
(The related case, in which the limits from right and left are −∞ and +∞
respectively, is treated analogously.) 
Fix ϵ > 0 such that [τ −ϵ, τ +ϵ]⊂[S,T ]. We claim that there exists δ > 0 with 
the following property: corresponding to any r ∈ (0, δ), a point s ∈ (τ −ϵ, τ) can be604 12 Regularity of Minimizers
found such that x(s) ¯ = ¯x(τ ) + r. Indeed, if this were not the case, we could deduce 
from the continuity of x¯ that x(s) ¯ ≤ ¯x(τ ) for all s ∈ (τ − ϵ, τ). This contradicts the 
second condition in (12.2.7). 
Arguing in similar fashion, we can show that (possibly after reducing the size of 
δ) there exists t ∈ (τ, τ +ϵ) and r ∈ (0, δ) for which x(t) ¯ = ¯x(τ )+r, for otherwise 
we obtain a contradiction of the first condition in (12.2.7). We know that τ ∈ (s, t). 
Also, x(s) ¯ = ¯x(t) and |t − s| ≤ 2ϵ. Since ϵ is an arbitrary positive number, the left 
side of (12.2.1) is zero, again a contradiction. ⨅⨆
12.3 Proof of The Generalized Tonelli Regularity Theorem 
We prove Theorem 12.2.2. Existence of minimizers is assured by Theorem 12.1.1. 
Take any L∞ local minimizer x¯. Note at the outset that, without loss of generality, 
we can assume: 
(HE4): θ : [0,∞) → R is a non-negative valued, non-decreasing function. 
Indeed, if θ fails to satisfy this condition we can replace it by 
θ (r) ˜ := inf{θ (r'
) : r' ≥ r} − inf{θ (r'
) : r' ≥ 0}.
It is a straightforward task to deduce from the convexity and superlinear growth of 
L(t, x, .) that this new function meets the requirements of hypothesis (HE3) and 
also satisfies (HE4), for the Lagrangian 
L(t, x, v) − inf{θ (r'
) : r' ≥ 0}.
Since minimizers are unaffected by the addition of a constant to L, we have 
confirmed that we can add (HE4) to the hypotheses. 
We can also assume 
(HE5): In condition (HE3), the inequality is strict and α = 0.
To see this, take k > 0 such that || ¯x||L∞ < k. Consider a new problem, in which L
in problem (BP) is replaced by 
L'
(t, x, v) := max{L(t, x, v); −αk + θ (|v|)} + αk + 1.
Hypotheses (HE1)–(HE2) continue to be satisfied (with θ = θ˜ and α = 0). However 
the inequality in (HE3) is now strict. We have L' ≥ L + αk + 1 everywhere and 
L'
(t, x, v) = L(t, x, v) + αk + 1 for all v ∈ Rn and points (t, x) in some tube 
about x¯. So x¯ remains an L∞ local minimizer and the assertions of the theorem 
for L' imply those for the original L. This confirms that we can add (HE5) to the 
hypotheses.12.3 Proof of The Generalized Tonelli Regularity Theorem 605
Take a regular point τ ∈ [S,T ] of x¯. Then there exist sequences si → τ , ti → τ
such that S ≤ si ≤ τ ≤ ti ≤ T for all i, and 
limi→∞
| ¯x(ti) − ¯x(si)|
|ti − si|
< ∞. (12.3.1) 
Since x¯ is continuous, we can arrange (by decreasing each si and increasing each ti
if necessary) that, for each i, ti > τ if τ<T and si < τ if τ>S. This means that, 
for each i, τ is contained in a relatively open subinterval of [S,T ] with end-points 
si and ti. 
For each i let y : [si, ti] → Rn be the linear interpolation of the end-points of x¯
restricted to [si, ti]: 
yi(t) := ¯x(si) +
t − si
|ti − si|
(x(t ¯ i) − ¯x(si)).
Lemma 12.3.1 There exist constants R0 and M such that 
|| ¯x||L∞ ≤ M, ||yi||L∞ ≤ M for all i
and 
|| ˙yi||L∞ < R0 for all i.
Furthermore, if {zi ∈ W1,1([si, ti]; Rn)} is any sequence of arcs such that, for each 
i, 
zi(si) = ¯x(si) and zi(ti) = ¯x(ti)
and 
1
2
 ti
si
θ (|˙zi(t)|)dt ≤
 ti
si
L(t, yi(t), y˙i(t))dt,
then 
||zi||L∞ ≤ M for all i.
Proof The fact that M and R0 can be chosen to satisfy all the stated conditions is 
obvious, with the exception of ‘||zi||L∞ ≤ M’. To show that this condition can also 
be satisfied, we choose α > 0 such that θ (α'
)>α' whenever α' ≥ α. Then, for any 
i and t ∈ [si, ti],606 12 Regularity of Minimizers
|zi(t)|≤|zi(si)| +  ti
si
|˙zi(s)|ds
≤ |zi(si)| + α|ti − si| +  ti
si
θ (|˙zi(s)|)ds
≤ |zi(si)| + α|ti − si| + 2
 ti
si
L(t, yi(s), y˙i(s))ds.
We have now merely to note that all terms on the right side of this inequality are 
bounded by a constant which does not depend on i or t ∈ [si, ti]. ⨅⨆
Define 
c0 := max{|L(t, x, v)| : t ∈ [S,T ], |x| ≤ M and |v| ≤ R0},
and the function d : R → R
d(σ ) := inf{|v| : p ∈ ∂vL(t, x, v), t ∈ [S,T ], |x| ≤ M, |p| ≥ σ}.
We deduce from (HE2) that 
limσ→∞d(σ ) = +∞. (12.3.2) 
Fix ϵ > 0. Choose R1 ≥ R0 to satisfy 
θ ◦ d
θ (r'
)
2r' − c0
r' − 2ϵ

> 2c0 (12.3.3) 
whenever r' ≥ R1, t ∈ [S,T ] and |x| ≤ M. This is possible by (12.3.2) and since θ
has superlinear growth. Define 
c1 := max{|L(t, x, v)| : t ∈ [S,T ], |x| ≤ M, |v| ≤ R1} (12.3.4) 
and 
σ1 := max{|ξ | : ξ ∈ ∂vL(t, x, v)|, t ∈ [S,T ], |x| ≤ M, |v| ≤ R1}. (12.3.5) 
Choose R2 > R1 such that 
1
2
θ (r) ≥ σ1(R1 + r)+c1 if r ≥ R2. (12.3.6) 
Set 
φ(v) :=
1
2
max {θ (|v|); θ (R2)}12.3 Proof of The Generalized Tonelli Regularity Theorem 607
and, for each (t, x) ∈ [S,T ] × Rn and v ∈ Rn, define 
L(t, x, v) ˜ := inf 	
α : (v, α) ∈ co 
epiφ ∪ epi 	
L(t, x, .) + ΨR2B


 .
The expression on the right summarizes the following construction: 
L(t, x, .) ˜ is the function with epigraph set E, where E is the convex hull 
of the unions of the epigraph sets of φ and of the function v→L(t, x, v), 
restricted to the ball R2B. 
An alternative representation is as follows: 
L(t, x, v) ˜ = inf{λL(t, x, u) + (1 − λ)φ(w) :
0 ≤ λ ≤ 1, |u| ≤ R2 and λu + (1 − λ)w = v}.
This ‘auxiliary Lagrangian’ L˜ has the following properties: 
Lemma 12.3.2 
(a) L(t, x, v) ˜ is locally bounded, measurable in t and convex in v, 
(b) L(t, x, v) ˜ is locally Lipschitz continuous in (x, v) uniformly in t ∈ [S,T ], 
(c) L(t, x, v) ˜ ≥ θ (|v|)/2 for all (t, x, v), 
(d) for all t ∈ [S,T ] and x ∈ MB we have 
L(t, x, v) ˜ = L(t, x, v) if |v| ≤ R1,
L(t, x, v) ˜ ≤ L(t, x, v) if |v| ≤ R2,
L(t, x, v) < L(t, x, v) ˜ if |v| > R2,
(e) For (t, x) ∈ [S,T ] × Rn
L(t, x, v) ˜ = θ (|v|)/2 if |v| ≥ R2.
Proof 
(a): L˜ is convex in v, by construction. It is locally bounded since 0 ≤ L˜ ≤ φ and φ is 
locally bounded. To see that L˜ is measurable in t we use the fact that, for fixed x and 
v, L˜ can be expressed as a pointwise infimum of a countable family of measurable 
functions 
t → λL(t, x, v) + (1 − λ)φ(w)
obtained by allowing (λ, v, w) to range over a countable dense subset of [S,T ] ×
Rn × Rn. (a) has been proved.608 12 Regularity of Minimizers
(c): This property follows from the facts that both L and φ satisfy the desired 
inequality and that v → 1
2 θ (|v|) is a convex function. 
(e): Take arbitrary points w' ∈ Rn , |w'
| > R2 and (t, x) ∈ [S,T ] × Rn. Choose 
ζ ∈ ∂φ(w'
). The subgradient inequality for convex functions gives 
φ(w) − φ(w'
) − ζ · (w − w'
) ≥ 0 for all w ∈ Rn. (12.3.7) 
Since θ is continuous and strictly increasing on [R2,∞), φ(w) and 1
2 θ (|w|) coincide 
on a neighbourhood of w = w'
. It follows that ζ is a subgradient also of w → 1
2 θ (|w|) at w = w' and, for all u ∈ Rn and (t, x) ∈ [S,T ] × Rn we have 
L(t, x, u)−φ(w'
)−ζ ·(u−w'
) ≥
1
2
θ (|u|)− 1
2
θ (|w'
|)−ζ ·(u−w'
) ≥ 0. (12.3.8) 
From (12.3.7) and (12.3.8) we deduce that 
L(t, x, v) ˜ = inf{λL(t, x, u) + (1 − λ)φ(w) :
λ ∈ [0, 1], |u| ≤ R2, v = λu + (1 − λ)w}
≥ φ(w'
) − ζ · (v − w'
).
Setting v = w' we see that L(t, x, w ˜ '
) ≥ φ(w'
). But then L(t, x, v) ˜ = 1
2 θ (|v|) in 
the region {v : |v| > R2}, since φ and v → 1
2 θ (|v|) coincide here and φ majorizes 
L˜. This remains true in the region {v : |v| ≥ R2}, by the continuity properties of 
convex functions. 
(b): Take any k1 > 0. Let K be a Lipschitz constant for x → L(t, x, v) uniformly 
valid for t ∈ [S,T ], |v| ≤ R2 and |x| ≤ k1. Take x1, x2 ∈ Rn such that 
|x1| and |x2| ≤ k1. Then for any δ > 0, t ∈ [S,T ] and w we can choose u, 
w and λ in the definition of L˜ such that 
L(t, x ˜ 1, v) ≤ λL(t, x1, u) + (1 − λ)φ(w)
≤ λL(t, x2, u) + K|x1 − x2| + (1 − λ)φ(w)
≤ L(t, x ˜ 2, v) + K|x1 − x2| + δ.
Since x1 and x2 are interchangeable and δ > 0 is arbitrary, it follows that x →
L(t, x, v) ˜ has Lipschitz constant at most K on k1B, for all t ∈ [S,T ] and v ∈ R2B. 
Now take k2 ≥ R2. We shall show that v → L(t, x, v) ˜ is Lipschitz continuous in 
the region k2B, uniformly over (t, x) ∈ [S,T ] × Rn. It will follow that L˜ is locally 
Lipschitz continuous jointly in the variables x, v, uniformly in t ∈ [S,T ], since it 
has this property with respect to these variables individually. 
Choose (t, x) ∈ [S,T ] × Rn, and let v → p · v + q be an arbitrary, non-constant 
affine function which is majorized by v → L(t, x, v) ˜ . By (e), we must have12.3 Proof of The Generalized Tonelli Regularity Theorem 609
p · v + q ≤
1
2
θ (|v|)
for all v such that |v| ≥ k2. Setting v = (k2 + 1)p/|p| we obtain 
(k2 + 1)|p| + q ≤
1
2
θ (k2 + 1). (12.3.9) 
However since L˜ ≥ 0 we also have 
L(t, x, v) ˜ − p · v − q ≥ −|p|k2 − q (12.3.10) 
for v ∈ k2B. Equations (12.3.9) and (12.3.10) yield 
L(t, x, v) ˜ − p · v − q ≥ +|p| −
1
2
θ (k2 + 1)
for v ∈ k2B. Set K1 := 1
2 θ (k2 + 1) + 1. It follows that 
L(t, x, v) ˜ − p · v − q ≥ 1 (12.3.11) 
for v ∈ k2B and |p| ≥ K1. 
Now the function v → L(t, x, v) ˜ is expressible as the pointwise supremum of 
affine functionals majorized by L˜. However inequality (12.3.11) tells us that, to 
evaluate the pointwise supremum in the region |v| ≤ k2, we can restrict attention 
to affine functions with Lipschitz constant at most K1. It follows then that v →
L(t, x, v) ˜ has Lipschitz constant at most K1 in this region, uniformly with respect 
to (t, x) ∈ [S,T ] × Rn. 
(d): The cases |v| ≥ R2 and R2 > |v| > R1 follow from (e) since L majorizes L˜ and 
L strictly majorizes u → 1
2 θ (|u|). It remains to show that L(t, x, v) = L(t, x, v) ˜
for all (t, x) ∈ [S,T ] × MB and v ∈ R1B. 
Take (t, x) and v as above and choose ζ ∈ ∂vL(t, x, v). By (12.3.6) and the 
definition of the constants c1 and σ1 (see (12.3.4) and (12.3.5)) we have that 
φ(w) ≥
1
2
θ (|w|) ≥ σ1(R1 + |w|) + c1 ≥ L(t, x, v) + ζ · (w − v),
for all points w which satisfy |w| ≥ R2. On the other hand, we also know that 
φ(w) ≥
1
2
θ (R2) ≥ σ1(R1 + R2) + c1 ≥ L(t, x, v) + ζ · (w − v)
for all points w which satisfy |w| ≤ R2. By the subgradient inequality however 
L(t, z, u) − L(t, z, v) − ζ · (u − v) ≥ 0610 12 Regularity of Minimizers
for all u ∈ Rn. Scaling and adding these inequalities, we arrive at 
L(t, x, v ˜ '
) = inf{λL(t, x, u) + (1 − λ)φ(w) :
0 ≤ λ ≤ 1, |u| ≤ R2, v' = λu + (1 − λ)w}
≤ L(t, x, v) + ζ · (v' − v)
for all points v' ∈ Rn. Setting v' = v yields L(t, x, v) ˜ ≥ L(t, x, v). Since however 
L majorizes L˜ we can replace inequality here by equality. This is what we set out to 
prove. ⨅⨆
Consider the optimization problems (Pi), i = 1, 2 ... , 
(Pi)
⎧
⎪⎨
⎪⎩
Minimize  ti
si L(t, x(t), ˜ x(t))dt ˙
over x ∈ W1,1([si, ti]; Rn) satisfying
x(si) = ¯x(si), x(ti) = ¯x(ti).
In view of properties (a)–(c) of the auxiliary Lagrangian, we deduce from Theo￾rem 12.1.1 that (Pi) has a minimizer, which we denote by xi, for each i. Notice that, 
for each i, 
1
2
 ti
si
θ (| ˙xi(s)|)ds ≤
 ti
si
L(t, x ˜ i(t), x˙i(t))dt
≤
 ti
si
L(t, y ˜ i(t), y˙i(t))dt
≤
 ti
si
L(t, yi(t), y˙i(t))dt,
by properties (c) and (d) of L˜ and since || ˙yi||L∞ ≤ R1. 
We conclude from Lemma 12.3.1 that 
||xi||L∞ ≤ M for all i.
Observe next that (xi, zi(t) =  t
si L(s, x ˜ i(s), x˙i(s))ds) is a solution to the 
dynamic optimization problem: 
⎧
⎪⎪⎨
⎪⎪⎩
Minimize z(ti)
over (x, z) ∈ W1,1([si, ti]; Rn+1) satisfying
(x(t), ˙ z(t)) ˙ ∈ F (t, x(t), z(t)) a.e. t ∈ [si, ti],
x(si) = ¯x(si), z(si) = 0, x(ti) = ¯x(ti),
where F (t, x, z) := epiL(t, x, .) ( ˜ = {(v, α) ∈ Rn × R : α ≥ L(t, x, v), v ˜ ∈
Rn}). It is straightforward to confirm the hypotheses under which the necessary12.3 Proof of The Generalized Tonelli Regularity Theorem 611
conditions of Theorem 8.7.2 (the Hamiltonian inclusion for convex velocity sets) 
are valid, with reference to the minimizer (xi, zi). 
Notice the crucial role of property (e) of L˜ which, together with (b), ensures that 
L˜ satisfies the Lipschitz continuity hypothesis: there exists β > 0 such that 
|L(t, x, v) ˜ − L(t, x ˜ '
, v)| ≤ β|x − x'
| (12.3.12) 
for all v ∈ Rn and all (t, x), (t, x'
) in some tube about xi. 
We deduce existence of an arc pi ∈ W1,1 such that 
p˙i(t) ∈ co{−ξ : (ξ , x˙i(t), z˙i(t)) ∈ ∂x,p,qH (t, x ˜ i(t), pi(t), −1)}
(12.3.13)
z˙i(t) = L(t, x ˜ i(t), x˙i(t)) a.e. t ∈ [si, ti],
where H (t, x, p, q) ˜ := sup(v,α)∈epiL(t,x,.) ˜ (p, q) · (v, α). Moreover, the Weierstrass 
condition tells us that 
pi(t) · ˙xi(t) − λL(t, x ˜ i(t), x˙i(t)) ≥ max
v∈Rn
{pi(t) · v − L(t, x ˜ i(t), v)}, (12.3.14) 
which yields 
pi(t) ∈ ∂vL(t, x ˜ i(t), x˙i(t)) a.e. t ∈ [si, ti]. (12.3.15) 
Observe that from (12.3.12) and (12.3.13) we deduce that | ˙pi(t)| ≤ β a.e. t ∈
[si, ti], and so 
|pi(ti) − pi(si)| ≤ β|ti − si|
for all i. Let i0 be the smallest integer such that 
β|tj − sj | ≤ ϵ for all j ≥ i0.
Choose any i ≥ i0. Then, by (12.3.15), 
pi(si) ∈ ∂vL(t, x ˜ i(t), x˙i(t)) + ϵB a.e. t ∈ [si, ti]. (12.3.16) 
We claim that 
|| ˙xi||L∞ ≤ R1. (12.3.17) 
Indeed, assume to the contrary that for all points t
¯ in some subset of [si, ti] of 
positive measure we have612 12 Regularity of Minimizers
| ˙xi(t)¯ | > R1.
According to (12.3.16) then, we can arrange that 
L(˜ t,x ¯ i(t), ¯ 0) − L(˜ t,x ¯ i(t), ¯ x˙i(t)) ¯ ≥ −pi(si) · ˙xi(t)¯ − ϵ| ˙xi(t)¯ |.
So, 
|pi(si)|·|˙xi(t)¯ | ≥ L(˜ t,x ¯ i(t), ¯ x˙i(t)) ¯ − L(˜ t,x ¯ i(t), ¯ 0) − ϵ| ˙xi(t)¯ |.
It follows that 
|pi(si)| ≥ θ (r)
2r − c0
r − ϵ,
for some r>R1. By (12.3.16), for a.e. t ∈ [si, ti]
| ˙xi(t)| ≥ d(|pi(si)| − ϵ) ≥ d
θ (r)
2r − c0
r − 2ϵ

.
Since θ is a monotone function, recalling also (12.3.3), we obtain 
1
2
θ (| ˙xi(t)|) ≥
1
2
θ ◦ d
θ (r)
2r − c0
r − 2ϵ

> c0 a.e..
It follows now from properties (c) and (d) of L˜ (see Lemma 12.3.2) that 
 ti
si
L(t, x ˜ i(t), x˙i(t))dt ≥
1
2
 ti
si
θ (| ˙xi(t)|)dt
> |ti − si|c0
≥
 ti
si
L(t, yi(t), y˙i(t))dt
=
 ti
si
L(t, y ˜ i(t), y˙i(t))dt.
But this contradicts the optimality of xi. Condition (12.3.17) is verified. 
We claim finally that 
||˙
x(t) ¯ ||L∞([si,ti];Rn) ≤ R2. (12.3.18) 
This will imply that ˙
x¯ is locally essentially bounded on [si, ti]. 
Suppose, to the contrary, that there exists a subset D ⊂ [si, ti] of positive measure 
such that12.4 Lipschitz Continuous Minimizers 613
|˙
x(t) ¯ | > R2 for all t ∈ D.
Since || ¯x||L∞ ≤ M, we have from property (d) of L that 
L(t, ˜ x(t), ¯ ˙
x(t)) < L(t, ¯ x(t), ¯ ˙
x(t)) ¯ for t ∈ D. (12.3.19) 
Then 
 ti
si
L(t, x(t), ¯ ˙
x(t))dt ¯ ≤
 ti
si
L(t, xi(t), x˙i(t))dt
(by optimality of x¯) 
=
 ti
si
L(t, x ˜ i(t), x˙i(t))dt
(since || ˙xi||L∞([si,ti];Rn) ≤ R1 and by property (d) of L˜) 
≤
 ti
si
L(t, ˜ x(t), ¯ ˙
x(t))dt ¯
(by optimality of xi) 
<
 ti
si
L(t, x(t), ¯ ˙
x(t))dt ¯
(by (12.3.19) and since L ≥ L˜). 
From this contradiction, we deduce that (12.3.18) is true. We have confirmed that ˙
x¯ is essentially bounded on the relatively open subinterval [si, ti] containing τ . 
12.4 Lipschitz Continuous Minimizers 
Our aim in this section is to explore the implications of the generalized Tonelli 
regularity theorem for particular classes of problems. The idea is to provide a more 
detailed, qualitative description of minimizers than that of Sect. 12.2, when the 
Tonelli existence hypotheses are supplemented by additional hypotheses. 
The generalized Tonelli regularity theorem is a very fruitful source of refined 
regularity theorems, supplying information about the Tonelli set in special cases. It 
can be used, for example, to show that for a large class of problems with polynomial 
Lagrangians, the Tonelli set is a countable set with a finite number of accumulation 
points [77].614 12 Regularity of Minimizers
We concentrate here, however, on just one application area: identifying hypothe￾ses which, when added to the Tonelli existence hypotheses, assure that all minimiz￾ers of (BP) over the class of absolutely continuous functions are, in fact, Lipschitz 
continuous. An equivalent property is that the Tonelli set is empty. 
The significance of establishing that a minimizer for (BP) is Lipschitz contin￾uous is that, on one hand, the hypotheses are then met under which the standard 
necessary conditions, such as those summarized as Proposition 12.1.2, can be used 
to investigate minimizers in detail. One the other, it rules out pathological behaviour, 
which might otherwise give rise to difficulties in the computation of minimizers, 
associated with the Lavrentiev phenomenon. 
We shall make repeated use of the following lemma, which gives sufficient 
conditions under which an interval on which a minimizer is Lipschitz continuous 
can be extended to include an end-point of [S,T ]. 
Lemma 12.4.1 Assume the Tonelli Existence Hypotheses (HE1)–(HE3). Let x¯ be a 
minimizer for (BP) and let S be the Tonelli set of x¯. Take t
¯ ∈ [S,T )\S. Suppose 
that there exists k > 0 with the following property: for any t' ∈ (t,T ) ¯ such that 
[t,t ¯ '
]⊂[S,T ]\S, there exists p ∈ W1,1([t,t ¯ '
]; Rn) satisfying 
p(t) ˙ ∈ co {q : (q, p(t)) ∈ ∂L(t, x(t), ¯ ˙
x(t)) ¯ } a.e.,
|p(t)| ≤ k for all t ∈ [t,t'
].
Then ˙
x¯ is Lipschitz continuous on [t,T ¯ ]. 
Proof Define 
τmax := sup{τ ∈ (t,T ¯ ] : ˙
x¯ is essentially bounded on [t,τ ¯ ]}.
(The set over which the supremum is taken is non-empty because of Theo￾rem 12.2.2.) 
Choose a sequence {ti} in (t,τ ¯ max) such that ti ↑ τmax. According to the 
hypotheses, for each i there exists pi ∈ W1,1([t,t ¯ i]; Rn) and k ≥ 0 such that 
p˙i(t) ∈ co {q : (q, pi(t)) ∈ ∂L(t, x(t), ¯ ˙
x(t)) ¯ } a.e. t ∈ [t,t ¯ i] (12.4.1) 
and 
|pi(t)| ≤ k for all t ∈ [t,t ¯ i]. (12.4.2) 
Since L(t, x, .) is convex, (12.4.1) implies that 
pi(t) ∈ ∂vL(t, x(t), ¯ ˙
x(t)) ¯ a.e. t ∈ [t,t ¯ i]. (12.4.3) 
We claim that ˙
x¯ is essentially bounded on [t,τ ¯ max]. If this were not the case, there 
would exist a point σi in (t,t ¯ i) for i = 1, 2,... , such that σi ↑ τmax, (12.4.3) is12.4 Lipschitz Continuous Minimizers 615
satisfied at t = σi and 
| ˙xi(σi)| → +∞. (12.4.4) 
Since L(σi, x(σ ¯ i), .) is convex, (12.4.3) implies 
pi(σi) · ˙
x(σ ¯ i) − L(σi, x(σ ¯ i), ˙
x(σ ¯ i)) ≥ min|v|≤1{pi(σi) · v − L(σi, x(σ ¯ i), v)}
≥ −|pi(σi)| − α
for i = 1, 2,... , where 
α := sup{L(σ, x(σ ), v) ¯ : σ ∈ [S,T ], |v| ≤ 1}.
It follows that, for i = 1, 2,...
L(σi, x(σ ¯ i), ˙
x(σ ¯ i))/|˙
x(σ ¯ i)| ≤ (|pi(σi)|(1 + |˙
x(σ ¯ i)| + α)/|˙
x(σ ¯ i)|.
As i → ∞, the left side of this inequality has limit +∞ by (12.4.4). But the right 
side is bounded above, in view of (12.4.2). From this contradiction we deduce that ˙
x¯ is essentially bounded on [t,τ ¯ max]. 
If τmax < T , then τmax is a regular point of x¯ and ˙
x¯ is essentially bounded on 
a relatively open neighbourhood of τmax in [S,T ]. This contradicts the defining 
property of τmax. It follows that ˙
x¯ is essentially bounded on all of [t,T ¯ ]. ⨅⨆
Our first application of the lemma is to show that, if L does not depend on time 
(the ‘autonomous’ case), then all minimizers are Lipschitz continuous. 
Proposition 12.4.2 Assume that, in addition to the Tonelli Existence Hypotheses 
(HE1)–(HE3), L(t, x, v) is independent of t. Then all L∞ local minimizers for (P) 
are Lipschitz continuous. 
Proof Write L(x, v) in place of L(t, x, v). Take an L∞ local minimizer x¯. Choose 
a regular point t
¯ ∈ (S, T ) of x¯. (This is possible since the regular points have full 
measure.) We show that ˙
x¯ is essentially bounded on [t,T ¯ ]. A similar argument can 
be used to confirm that ˙
x¯ is essentially bounded on [S, t
¯]. It will follow that x¯ is 
Lipschitz continuous on [S,T ]. 
Take any t' ∈ (t,T ¯ ] such that ˙
x¯ is essentially bounded on [t,t ¯ '
]. Because L
is independent of t, we can apply the necessary conditions of Proposition 12.1.2, 
including the constancy of the Hamiltonian condition, to the optimal subarc x¯
restricted to [t,t ¯ '
]. We know then that there exist an arc p ∈ W1,1([t,t ¯ '
]; Rn) and a 
number c such that, for a.e. t ∈ [t,t ¯ '
], 
c = p(t) · ˙
x(t) ¯ − L(x(t), ¯ ˙
x(t)) ¯ (12.4.5) 
p(t) ∈ ∂vL(x(t), ¯ ˙
x(t)). ¯ (12.4.6)616 12 Regularity of Minimizers
Fix ϵ > 0 such that ˙
x¯ is essentially bounded on [t ,¯ t
¯ + ϵ]. It follows from the 
local Lipschitz continuity of L and (12.4.6) that there exists a constant k1 > 0, 
independent of t'
, such that 
|p(t)| ≤ k1 a.e. t ∈ [t,t ¯ '
]∩[t ,¯ t
¯ + ϵ].
But then (12.4.5) implies that there exists k2 (independent of t'
) such that 
|c| ≤ k2.
We deduce from (12.4.5) and (12.4.6) that, for a.e. t ∈ [t,t ¯ '
], 
c = p(t) · ˙
x(t) ¯ − L(x(t), ¯ ˙
x(t)) ¯
≥ max|v|≤1{p(t) · v − L(x(t), v) ¯ }
≥ |p(t)| − α,
where α, defined by 
α := inf{L(x(t), v) ¯ : S ≤ t ≤ T , |v| ≤ 1},
does not depend on t'
. 
Since p is continuous, we have 
|p(t)| ≤ α + c for all t ∈ [t,t ¯ '
].
We see that p is bounded on [t,t ¯ '
] by a constant which does not depend on t'
. It 
follows from Lemma 12.4.1 that ˙
x¯ is essentially bounded on [t,T ¯ ]. ⨅⨆
Another case when the Lemma 12.4.1 can be used to establish Lipschitz 
continuity of minimizers is when L(t, x, v) is convex, jointly in (x, v): 
Proposition 12.4.3 Assume, in addition to the Tonelli Existence Hypotheses 
(HE1)–(HE3), that L(t, x, v) is convex in (x, v) for each t ∈ [S,T ]. Then all 
L∞ local minimizers for (BP) are Lipschitz continuous. 
Proof Take an L∞ local minimizer x¯. Choose a regular point τ ∈ (S, T ) of x¯. As 
in the proof of the previous proposition, we content ourselves with showing that ˙
x¯
is essentially bounded on [τ,T ]. The demonstration that the same is true on [S,τ ]
is along precisely similar lines. 
Take any t' ∈ (t,T ¯ ] such that ˙
x¯ is essentially bounded on [t,t ¯ '
]. Observe next 
that (x,¯ z(t) ¯ ≡  t
t
¯ L(s, ˜ x(s), ¯ ˙
x(s))ds) ¯ is a solution to the dynamic optimization 
problem:12.4 Lipschitz Continuous Minimizers 617
⎧
⎪⎪⎨
⎪⎪⎩
Minimize z(ti)
over (x, z) ∈ W1,1([t,t ¯ '
]; Rn+1) satisfying
(x(t), ˙ z(t)) ˙ ∈ F (t, x(t), z(t)) a.e. t ∈ [t,t ¯ '
],
x(t)¯ = ¯x(t), z( ¯ t)¯ = 0, x(t'
) = ¯x(t'
),
where F (t, x, z) := epiL(t, x, .). We know from Theorem 8.4.3 that there exists 
p ∈ W1,1([t,t ¯ '
]; Rn) such that 
p(t) ˙ ∈ co{η : (η, 0, p(t), −1) ∈ NGr F (t,.)(x(t), ¯ z(t), ¯ ˙
x(t), ¯ ˙
z(t)) ¯ } a.e. t ∈ [t,t ¯ '
].
(12.4.7) 
Since L(t, ., .) is convex, we deduce that Gr F (t, .) is convex, and so from (12.4.7) 
it follows that 
(p(t), p(t)) ˙ ∈ ∂L(t, x(t), ¯ ˙
x(t)) ¯ a.e. t ∈ [t,t ¯ '
], (12.4.8) 
where ∂L(t, ., .) is the subdifferential in the usual sense of convex analysis. 
Since ˙
x¯ is essentially bounded on a neighbourhood of t
¯, L(t, ., .) is locally 
Lipschitz continuous uniformly in t and p is continuous, (12.4.8) implies that there 
exists k1 > 0 (independent of t'
) such that 
|p(t)¯ | ≤ k1. (12.4.9) 
Since the ‘convex’ subdifferential is employed, (12.4.8) implies that, for a.e. t ∈
[t,t ¯ '
], 
L(t, y, v) − L(t, x(t), ¯ ˙
x(t)) ¯
≥ (y − ¯x(t)) · ˙p(t) + (v − ˙
x(t)) ¯ · p(t) for all y ∈ Rn, v ∈ Rn.
By examining the implications of this inequality when y = ¯x + u and v = 0, for an 
arbitrary unit vector u, we deduce that, for a.e. t ∈ [t,t ¯ '
], 
| ˙p(t)| = max|u|≤1p˙ · u
≤ max|u|≤1L(t, x(t) ¯ + u, 0) − L(t, x(t), ¯ ˙
x(t)) ¯ + p(t) · ˙
x(t). ¯
It follows that, for some integrable functions, γ1 and γ2 which do not depend on t'
, 
| ˙p(t)| ≤ γ1(t)|p(t)| + γ2(t) a.e. t ∈ [t,t ¯ '
].
We deduce from this inequality and (12.4.9), with the help of Gronwall’s lemma 
(Lemma 6.2.4), that there exists k2 > 0 (independent of t'
) such that 
|p(t)| ≤ k2 for all t ∈ [t,t ¯ '
].618 12 Regularity of Minimizers
It follows now from Lemma 12.4.1 that ˙
x¯ is essentially bounded on [t,T ¯ ]. ⨅⨆
Finally, we illustrate how the Tonelli regularity theorem can be used to reduce 
the hypotheses under which Euler Lagrange type necessary conditions of optimality 
have traditionally been derived. As in the previous applications, this is done via the 
intermediary of Lemma 12.4.1. 
Proposition 12.4.4 Take an L∞ local minimizer x¯ for (BP). Assume that, in 
addition to the Tonelli Existence Hypotheses (HE1)–(HE3), there exist integrable 
functions c and γ such that, for a.e. t ∈ [S,T ], 
supξ∈Px [co∂L](t)|ξ | ≤ c(t)infη∈∂vL|η| + γ (t), (12.4.10) 
where Px [co∂L](t) denotes the projection of co∂L onto the first coordinate 
Px [co∂L](t) := {ξ : (ξ , η) ∈ co ∂L(t, x(t), ¯ ˙
x(t)) ¯ for some η ∈ Rn}
and ∂vL is evaluated at (t, x(t), ¯ ˙
x(t)) ¯ . 
Then x¯ is Lipschitz continuous. 
Notice that the supplementary hypothesis (12.4.10) reduces to 
|Lx (t, x(t), ¯ ˙
x(t)) ¯ | ≤ c(t)|Lv(t, x(t), ¯ ˙
x(t)) ¯ | + γ (t) a.e. t ∈ [S,T ]
when L is smooth. 
This hypothesis is less restrictive than that invoked in standard necessary 
conditions, such as those of Proposition 12.1.2, in two respects. Firstly the presence 
of the non-negative term c(t)|Lv| on the right side reduces the severity of the 
inequality. Secondly, the inequality is required to hold precisely along the L∞ local 
minimizer x¯, not over a tube about x¯. 
Proof Choose a regular point τ ∈ (S, T ) of x¯. As usual, we show merely that ˙
x¯ is 
essentially bounded on [τ,T ], since a similar analysis can be used to demonstrate 
the essential boundedness of ˙
x¯ also on [S,τ ]. 
Take any t' ∈ (t,T ¯ ] such that ˙
x¯ is essentially bounded on [t,t ¯ '
]. The necessary 
conditions of Proposition 12.1.2 supply p ∈ W1,1([t,t ¯ '
]; Rn) such that 
p(t) ˙ ∈ co{q : (q, p(t)) ∈ ∂L(t, x(t), ¯ ˙
x(t)) ¯ } a.e. t ∈ [t,t ¯ '
].
This implies that, for a.e. t ∈ [t,t ¯ '
], 
p(t) ˙ ∈ Px [co∂L](t)
and 
p(t) ∈ ∂vL(t, x(t), ¯ ˙
x(t)) ¯ (12.4.11)12.5 Nonautonomous Variational Problems with State Constraints 619
Since L(t, ., .) is locally Lipschitz continuous uniformly in t, x¯ is essentially 
bounded on a neighbourhood of t
¯ and p is continuous, we deduce from (12.4.11), 
that there exists some k1 (independent of t'
), such that 
|p(t)¯ | ≤ k1.
It follows from supplementary hypothesis (12.4.10) that 
| ˙p(t)| ≤ c(t)|p(t)| + γ (t) a.e. t ∈ [t,t ¯ '
].
We now deduce from Gronwall’s lemma (Lemma 6.2.4) that there exists k2 > 0
(independent of t'
) such that 
|p(t)| ≤ k2 for all t ∈ [t,t ¯ '
].
It follows from Lemma 12.4.1 that ˙
x¯ is essentially bounded on [t,T ¯ ]. ⨅⨆
12.5 Nonautonomous Variational Problems with State 
Constraints 
In the preceding section, we applied the generalized Tonelli regularity theorem 
Theorem 12.2.2 to show that minimizers of the basic problem (BP) over the space 
of absolutely continuous functions are Lipschitz continuous in certain cases of 
interest. 
An alternative approach to showing Lipschitz continuity of minimizers is 
based on an application of the maximum principle and time re-parameterization 
techniques. This has the merit of simplicity. Besides, the hypotheses which it is 
necessary to impose are weaker in some instances and the methods extend to cover 
problems with pathwise state constraints. 
Consider the optimization problem 
(CV )
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize  T
S L(t, x(t), x(t))dt ˙
over arcs x ∈ W1,1([S,T ]; Rn) satisfying
x(S) = x0, x(T ) = x1,
x(t) ˙ ∈ C a.e. t ∈ [S,T ],
x(t) ∈ A for all t ∈ [S,T ],
the data for which comprise an interval [S,T ], a function L : [S,T ]×Rn×Rn → R, 
points x0, x1 ∈ Rn, a cone C ⊂ Rn, and a set A ⊂ Rn. We lay stress on the presence 
of the pathwise state constraint 
x(t) ∈ A for all t ∈ [S,T ].620 12 Regularity of Minimizers
Given a given reference arc x¯ ∈ W1,1([S,T ]; Rn), we shall consider the 
following conditions, in which k : [S,T ] × (0, +∞) → R is a L × B1 measurable 
function such that k(., 1) ∈ L1(S, T ): 
(A1): there exists ε∗ > 0 such that for a.e. t ∈ [S,T ] and for all σ > 0 we have 
|L(s2, x(t), σ ¯ ˙
x(t)) ¯ − L(s1, x(t), σ ¯ ˙
x(t)) ¯ | ≤ k(t, σ )|s2 − s1| (12.5.1) 
whenever s1, s2 ∈ [t − ε∗, t + ε∗]∩[S,T ], 
(A2): for every selection Q(t, v) of ∂˜
rL(t, x(t), rv) ¯ r=1, we can find c >
ess inf{|˙
x(t) ¯ | : t ∈ [S,T ]} and a negligible set N such that, 
lim
R→+∞
sup
|v|≥R,v∈C
∂˜
rL(t,x(t),rv) ¯ r=1/=∅
t∈[S,T ]\N
{L(t, x(t), v) ¯ − Q(t, v)} +  T
S
k(s, 1)ds
< inf
|v|<c,v∈C
∂˜
rL(t,x(t),rv) ¯ r=1/=∅
t∈[S,T ]\N
{L(t, x(t), v) ¯ − Q(t, v)}. (12.5.2) 
Here, given a point (t, x, v) ∈ [S,T ] × Rn × Rn such that L(t, x, v) < +∞, we 
denote by ∂˜
rL(t, x, rv)r=1 the subdifferential in the sense of convex analysis of the 
function (0, +∞) ϶ r ⍿→ L(t, x, rv) at r = 1: 
∂˜
rL(t, x, rv)r=1 := {q ∈ R : q × (r − 1) ≤ L(t, x, rv)
− L(t, x, v) for all r > 0}. (12.5.3) 
We shall interpret condition (12.5.2) to be satisfied whenever the term on the left 
side of the inequality takes value ‘−∞’ or in situations where there exists R0 > 0
such that ∂˜
rL(t, x(t), rv) ¯ r=1 = ∅ for all |v| ≥ R0. 
Theorem 12.5.1 Let x¯ be a W1,1 local minimizer for (CV). Assume that L is 
Borel measurable and hypotheses (A1) and (A2) are satisfied. Then x¯ is Lipschitz 
continuous. 
A proof of Theorem 12.5.1 is provided at the end of this section. 
Corollary 12.5.2 Let x¯ be a W1,1 local minimizer for (CV). Assume that 
(i) L is Borel measurable and satisfies (A1) of Theorem 12.5.1, 
(ii) L is bounded on bounded subsets of [S,T ] × A × C, 
(iii) there exists an increasing function θ : [0,∞) → [0,∞) such that 
limr→∞θ (r)/r = +∞12.5 Nonautonomous Variational Problems with State Constraints 621
and a constant α such that 
L(t, x, v) > θ (|v|) − α|v| for all (x, v) ∈ Rn × Rn, a.e. t ∈ [S,T ].
Then x¯ is Lipschitz continuous. 
Remark 
Observe that, if L is independent of t, then (A1) is automatically satisfied 
(with k = 0). Therefore, an immediate consequence of Theorem 12.5.1 is that, 
in the ‘autonomous case’, if the Lagrangian is merely Borel measurable and 
condition (A2) is satisfied (or, in particular, conditions (ii)–(iii) of Corollary 12.5.2 
are satisfied), then a W1,1 local minimizer for (CV) is always Lipschitz 
continuous. 
Proof of Corollary 12.5.2 Consider any Q(t, v) ∈ ∂˜
rL(t, x(t), rv) ¯ r=1 where 0 /=
v ∈ C and t ∈ [S,T ]. Then 
L

t, x(t), rv ¯

− L(t, x(t), v) ¯ ≥ Q(t, v)(r − 1), for all r > 0. (12.5.4) 
Taking r = 1
|v|
(> 0) in (12.5.4) we obtain 
L

t, x(t), ¯ v
|v|

≥ L(t, x(t), v) ¯ + Q(t, v) 1
|v|
− 1

≥

L(t, x(t), v) ¯ − Q(t, v)1 − 1
|v|

+
1
|v|
L(t, x(t), v). ¯
Bearing in mind conditions (ii) and (iii), and the fact that C is a cone (so
v
|v|
remains 
in C), we deduce, when |v| ≥ 2, that 
M − θ (|v|)
|v| + α ≥
1
2

L(t, x(t), v) ¯ − Q(t, v)
, a.e. t ∈ [S,T ],
for a suitable constant M. Therefore, given a selection Q(t, v) of ∂˜
rL(t, x(t), rv) ¯ r=1
there exists a subset N ⊂ [S,T ] of zero measure such that 
lim
R→+∞
sup
|v|≥R,v∈C
∂˜
rL(t,x(t),rv) ¯ r=1/=∅
t∈[S,T ]\N
{L(t, x(t), v) ¯ − Q(t, v)}
≤ lim
R→+∞
sup
|v|≥R
2

M − θ (|v|)
|v| + α

= −∞.622 12 Regularity of Minimizers
It follows that condition (A2) is satisfied (for any selection Q(t, v) and for any 
c > ess inf{|˙
x(t) ¯ | : t ∈ [S,T ]}), and so we can apply Theorem 12.5.1 since all the 
required hypotheses are in force. ⨅⨆
As a preliminary step in the proof of Theorem 12.5.1, we establish a technical 
lemma and Theorem 12.5.4 below which provides a Weierstrass-type variational 
inequality: this plays a central role in the proof of Theorem 12.5.1. 
Lemma 12.5.3 Let {yh}h∈N be a sequence of functions in W1,1([S,T ]; R) satisfy￾ing the following conditions: 
(i) yh(S) = S and yh(T ) = T , for all h ∈ N, 
(ii) there exists ω > 0 such that, for every h ∈ N, y˙h(t) ≥ ω for a.e. t ∈ [S,T ], 
(iii) the sequence {yh} converges to y(t) ¯ := t in W1,1([S,T ]; R). 
Then, for every x¯ ∈ W1,1([S,T ]; Rn), the sequence { ¯x ◦y−1
h } admits a subsequence 
which converges to x¯ in W1,1([S,T ]; Rn). 
Proof Observe that the sequence of the inverse functions {y−1
h } converges to 
y(t) ¯ := t in W1,1([S,T ]; R). Indeed, recalling that yh(S) = S = ¯y(S), by means 
of the change of variable t := yh(s), for each h we obtain: 
‖y−1
h − ¯y‖W1,1 = ‖(d/dt)(y−1
h ) − 1‖L1
=
 T
S





1 − ˙yh(y−1
h (t))
y˙h(y−1
h (t))





dt =
 T
S




1 − ˙yh(s)
y˙h(s)




y˙h(s) ds
= ‖˙yh − 1‖L1 = ‖yh − ¯y‖W1,1 .
Therefore, extracting a subsequence (we do not relabel), we obtain that the 
sequence {(d/dt)(y−1
h )} converges to ˙
y¯ ≡ 1 almost everywhere on [S,T ]. 
For every h ∈ N let φh be the linear operator φh : W1,1([S,T ]; Rn) →
W1,1([S,T ]; Rn) defined by 
φh(w) := w ◦ y−1
h .
The lemma statement is confirmed if we show that {φh(x)¯ } converges to x¯ in 
W1,1([S,T ]; Rn). To see this, notice first of all that, for all w ∈ W1,1([S,T ]; Rn), 
we have 
(d/dt)φh(w)(t) = (w˙ ◦ y−1
h (t))(d/dt)(y−1
h )(t) a.e. t ∈ [S,T ], (12.5.5) 
and so 
lim
h→+∞
(d/dt)φh(w)(t) = ˙w(t) a.e. t ∈ [S,T ].
Moreover, applying the change of variable s := y−1
h (t) we deduce that12.5 Nonautonomous Variational Problems with State Constraints 623
‖(d/dt)φh(w)‖L1 =
 T
S
| ˙w(y−1
h (t))|[(d/dt)(y−1
h )(t)] dt
=
 T
S
| ˙w(s)| ds = ‖˙w‖L1
and, consequently, 
‖φh(w)‖W1,1 = |φh(w)(S)|+‖(d/dt)φh(w)‖L1 ≤ ‖w‖W1,1 .
It follows that the operators φh’s are equi-bounded. 
Now, we claim that, if w ∈ C2([S,T ]; Rn), then the sequence φh(w) converges 
to w in W1,1([S,T ]; Rn). Indeed, since the family of functions {y−1
h }h is equi￾Lipschitz continuous (with Lipschitz constant 1/ω), from (12.5.5) it follows that 
|(d/dt)φh(w)(t)| ≤
1
ω‖ ˙w‖L∞ a.e. t ∈ [S,T ].
The dominated convergence theorem implies the convergence of {(d/dt)φh(u)} to 
w˙ in L1 as h → +∞, and, recalling that φh(w)(S) = w(S) for all h, we deduce 
that ‖φh(w) − w‖W1,1 → 0 as h → +∞, confirming the claim above. 
Now, consider a sequence {wk} in C2([S,T ]; Rn) that converges to x¯ w.r.t. the 
W1,1 norm. For each h, k ∈ N we have: 
φh(x)¯ − ¯x = (φh(x)¯ − φh(wk)) + (φh(wk) − wk) + (wk − ¯x).
The analysis above tells us that: 
‖φh(x)¯ − ¯x‖W1,1 ≤ ‖φh(x)¯ − φh(wk)‖W1,1 + ‖φh(wk) − wk‖W1,1 + ‖wk − ¯x‖W1,1
≤ 2‖wk − ¯x‖W1,1 + ‖φh(wk) − wk‖W1,1 ,
and passing to the limit as h → +∞ first, and then as k → +∞, we deduce that 
φh(x)¯ → ¯x as h → +∞. 
This concludes the proof of the lemma. ⨅⨆
Theorem 12.5.4 Let x¯ be a W1,1 local minimizer for (CV). Assume that L is 
Borel measurable and satisfies assumption (A1). Then there exists an arc p ∈
W1,1([S,T ]; R) such that: 
L

t, x(t), ¯ ˙
x(t) ¯
u

u − L(t, x(t), ¯ ˙
x(t)) ¯ ≥ p(t)(u − 1)
for all u > 0, a.e. t ∈ [S,T ], (12.5.6) 
p(t) ˙ ∈ co ∂tL(t, x(t), ¯ ˙
x(t)) ¯ a.e. t ∈ [S,T ] (12.5.7)624 12 Regularity of Minimizers
and 
| ˙p(t)| ≤ k(t, 1) a.e. t ∈ [S,T ]. (12.5.8) 
Proof In what follows we fix an integer j ≥ 2 and we take y(t) ¯ := t. We prove 
first the validity of (12.5.6) for ‘u ≥ 1/j ’ instead of ‘u > 0’ for some absolutely 
continuous arc pj which satisfies also (12.5.7) and (12.5.8). Then a compactness 
argument, applied to the sequence of arcs {pj }, will allow us to pass to the case 
‘u > 0’ in a limit-taking procedure. 
Step 1. We introduce the following auxiliary Lagrangian: for all t ∈ [S,T ], y ∈ R
and u ∈ R we set 
�(t, y, v) :=
⎧
⎨
⎩
L


y, x(t), ¯ ˙
x(t) ¯
u

u if u ≥ 1/j and ˙
x(t) ¯ exists,
0 otherwise.
where L
 is the Borel extension of L to R × Rn × Rn defined by 
L(t, x, v)  :=
⎧
⎪⎪⎨
⎪⎪⎩
L(S, x, v) if t ≤ S
L(t, x, v) if t ∈ [S,T ]
L(T , x, v) if t ≥ T ,
for all x,v ∈ Rn. Consider the following auxiliary dynamic optimization problem: 
(P )
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize  T
S �(t, y(t), y(t))dt ˙
over measurable functions u : [S,T ] → R and arcs
y ∈ W1,1([S,T ]; R) satisfying
y(t) ˙ = u a.e. t ∈ [S,T ],
u(t) ∈ [1/j,∞) a.e. t ∈ [S,T ],
y(S) = S, y(T ) = T ,
Observe that the function �(t, y, u) is L × B1 × B1 measurable, therefore, from 
Corollary 2.3.3 it follows that the map t ⍿→ �(t, y(t), u(t)) is Lebesgue measurable 
for every pair (y, v) of Lebesgue measurable functions. 
The next lemma establishes that if x¯ is a W1,1 local minimizer for (CV) then 
(y(t) ¯ := t, u(t) ¯ ≡ 1) is a W1,1 local minimizer for (P). 
Lemma 12.5.5 If x¯ is a W1,1 local minimizer for (CV), then the process (y(t) ¯ :=
t, u(t) ¯ ≡ 1) is a W1,1 local minimizer for (P).12.5 Nonautonomous Variational Problems with State Constraints 625
Proof Assume that x¯ is a W1,1 local minimizer for (CV), which means that there 
exists ε > 0 such that 
 T
S
L(t, x(t), ¯ ˙
x(t)) dt ¯ ≤
 T
S
L(t, x(t), x(t)) dt, ˙
for all admissible arcs x for (CV) such that ‖x − ¯x‖W1,1 ≤ ε. We claim that there 
exists ρ > 0 for which, for every y ∈ W1,1([S,T ]; R) such that y(S) = S, y(T ) =
T , y˙ ≥ 1/j a.e. on [S,T ] and ‖y − ¯y‖W1,1 ≤ ρ, we have 
‖ ¯x ◦ y−1 − ¯x‖W1,1 ≤ ε.
Indeed, suppose that the claim is false. Then we can find a sequence {yh} of 
functions in W1,1([S,T ]; R) such that, for all h, y˙h ≥ 1/j a.e. on [S,T ] and 
yh(S) = S, yh(T ) = T , ‖yh − ¯y‖W1,1 ≤
1
h + 1
, but ‖ ¯x ◦ y−1
h − ¯x‖W1,1 > ε.
On the other hand we know from Lemma 12.5.3 (taking ω = 1/j ) that a subse￾quence of { ¯x ◦ y−1
h }h converges to x¯ in W1,1([S,T ]; R), arriving at a contradiction 
of our premise. 
Now, let (y, u) be any process satisfying the constraints of the optimal problem 
(P) such that ‖y − ¯y‖W1,1 ≤ ρ. Since y(t) ˙ = u(t) ≥ 1/j almost everywhere on 
[S,T ], we deduce that y : [S,T ]→[S,T ] is strictly increasing and 
|y(t2) − y(t1)| =




 t2
t1
y(t) dt ˙




≥
1
j
|t2 − t1| for all t1, t2 ∈ [S,T ].
Therefore y is invertible and its inverse y−1 is Lipschitz continuous with Lipschitz 
constant j . We consider the absolutely continuous function 
x(τ ) := ¯x(y−1(τ )), for all τ ∈ [S,T ].
Notice that x(S) = ¯x(S), x(T ) = ¯x(T ), and x(τ ) ∈ ¯x([S,T ]) ⊂ A. Moreover, ˙
x(y ¯ −1(τ )) and (d/dτ )(y−1)(τ ) exist for almost every τ ∈ [S,T ]. We deduce that 
(d/dτ )(y−1)(τ ) = 1
y(y ˙ −1(τ )), and x(τ ) ˙ = ˙
x(y ¯ −1(τ ))
y(y ˙ −1(τ ))
∈ C, a.e. τ ∈ [S,T ],
and, so, x is an admissible arc for problem (CV) such that ‖x− ¯x‖W1,1 ≤ ε. (The fact 
that x is absolutely continuous and the validity of the chain rule for the derivative 
follows from the fact that y−1 is a monotone absolutely continuous function.) It 
follows that 
 T
S
L(τ, x(τ ), x(τ )) dτ ˙ ≥
 T
S
L(t, x(t), ¯ ˙
x(t)) dt. ¯ (12.5.9)626 12 Regularity of Minimizers
Of course we have 
 T
S
L(t, x(t), ¯ ˙
x(t)) dt ¯ =
 T
S
�(t, y(t) ¯ = t, u(t) ¯ ≡ 1)dt.
Now, using the change of variables t = y−1(τ ) on the left term of (12.5.9), we 
deduce that 
 T
S
L

τ, x(τ ), x(τ ) ˙ 
dτ =
 T
S
L

τ, x(y ¯ −1(τ )), ˙
x(y ¯ −1(τ ))
y(y ˙ −1(τ ))

dτ
=
 T
S
L

y(t), x(t), ¯ ˙
x(t) ¯
y(t) ˙

y(t) dt ˙
=
 T
S
�(t, y(t), y(t)) dt. ˙
This confirms the lemma statement. ⨅⨆
Step 2. Set 
k∗(t, u) :=
⎧
⎨
⎩
k

t,
1
u

u if (t, u) ∈ [S,T ]×[1/j, +∞)
0 if (t, u) ∈ [S,T ] × (−∞, 1/j ) ,
where k is the L × B1 measurable function provided by assumption (A1). Observe 
that k∗ is L × B1 measurable and k∗(., u¯ ≡ 1)(= k(., 1)) ∈ L1(S, T ). Moreover, 
for almost every t ∈ [S,T ], we have 
|�(t, y2, u) − �(t, y1, u)|
≤ k∗(t, u)|y2 − y1|, for all y1, y2 ∈ [t − ε∗, t + ε∗] and u ≥ 1/j.
(12.5.10) 
Indeed, let t be such that (12.5.1) holds. For any y1, y2 ∈ [t − ε∗, t + ε∗] and 
u ≥ 1/j we have 
|�(t, y2, u) − �(t, y1, u)| =




L


y2, x(t), ¯ ˙
x(t) ¯
u

− L


y1, x(t), ¯ ˙
x(t) ¯
u



 u.
(12.5.11) 
We claim that 




L


y2, x(t), ¯ ˙
x(t) ¯
u

− L


y1, x(t), ¯ ˙
x(t) ¯
u




≤ k

t,
1
u

|y2 − y1|. (12.5.12)12.5 Nonautonomous Variational Problems with State Constraints 627
Observe that it is not restrictive to assume that ε∗ < (T −S)/2 and y1 ≤ y2. Clearly 
the claim is trivial if y1 ≤ y2 ≤ S or T ≤ y1 ≤ y2. So we can restrict attention to 
the following three cases. 
Case 1: y1 ≤ S ≤ y2. Then, we have 




L


y2, x(t), ¯ ˙
x(t) ¯
u

− L


y1, x(t), ¯ ˙
x(t) ¯
u




=




L

y2, x(t), ¯ ˙
x(t) ¯
u

− L

S, x(t), ¯ ˙
x(t) ¯
u




≤ k

t,
1
u

|y2 − S| ≤ k

t,
1
u

|y2 − y1|.
Case 2: S ≤ y1 ≤ y2 ≤ T . Then L


yi, x(t), ¯ ˙
x(t) ¯
u

= L

yi, x(t), ¯ ˙
x(t) ¯
u

, for 
i = 1, 2, and (12.5.12) follows directly from (12.5.1). 
Case 3: y1 ≤ T ≤ y2. In this case the analysis can be carried out in a similar way 
to that one outlined in case 1. The validity of (12.5.10) is a direct consequence of 
(12.5.11) and (12.5.12). 
As a consequence, taking (y(t) ¯ = t, u(t) ¯ ≡ 1) as a reference W1,1 local 
minimizer, since all the required hypotheses are satisfied we can apply the maximum 
principle Theorem 7.2.1 to the above dynamic optimization problem (P), after 
reducing it to a (right) end-point cost problem by state augmentation. 
The (un-maximized) Hamiltonian, which may depend also on the cost multiplier 
λ ≥ 0, takes the form: 
Hλ(t, y, p, u) := pu − λ�(t, y, u).
We deduce the existence of an absolutely continuous function pj : [S,T ] → R and 
(normalizing if necessary) a number λ ∈ {0, 1} satisfying the nontriviality condition 
(pj (t), λ) /= (0, 0) for all t ∈ [S,T ], (12.5.13) 
the co-state inclusion 
− ˙pj (t) ∈ co ∂yHλ(t, y(t), p ¯ j (t), u(t)) ¯
= −λco ∂y �

t, y(t), ¯ u(t) ¯

a.e. t ∈ [S,T ], (12.5.14) 
and the Weierstrass condition 
Hλ(t, y(t), p ¯ j (t), u(t)) ¯ = max
u≥1/j
Hλ(t, y(t), p ¯ j (t), u) a.e. t ∈ [S,T ].
(12.5.15)628 12 Regularity of Minimizers
Taking into account that y(t) ¯ = t and u(t) ¯ ≡ 1, (12.5.15) yields 
pj (t) − λ�(t, t, 1) ≥ pj (t)u − λ�
t,t,u
for all u ≥ 1/j, a.e. t ∈ [S,T ],
which, in terms of the reference Lagrangian L, becomes 
pj (t) − λL(t, x(t), ¯ ˙
x(t)) ¯ ≥ pj (t)u
− λL
t, x(t), ¯ ˙
x(t) ¯
u

u for all u ≥ 1/j, a.e. t ∈ [S,T ]. (12.5.16) 
If λ = 0, then from (12.5.16) we would obtain that, for almost every t ∈ [S,T ], 
pj (t) ≥ pj (t)u for all u ≥ 1/j,
which would imply that pj (t) = 0, contradicting (12.5.13). Thus we necessarily 
have λ = 1, and from (12.5.14) and (12.5.16) it follows that 
L

t, x(t), ¯ ˙
x(t) ¯
u

u − L(t, x(t), ¯ ˙
x(t)) ¯ ≥ pj (t)(u − 1)
for all u ≥ 1/j, a.e. t ∈ [S,T ], (12.5.17) 
p˙j (t) ∈ co ∂tL(t, x(t), ¯ ˙
x(t)) ¯ a.e. t ∈ [S,T ], (12.5.18) 
and 
| ˙pj (t)| ≤ k(t, 1) a.e. t ∈ [S,T ]. (12.5.19) 
Step 3. To conclude the proof of the theorem, we must show that there exists an 
absolutely continuous function p ∈ W1,1([S,T ]; R) such that 
L

t, x(t), ¯ ˙
x(t) ¯
u

u − L(t, x(t), ¯ ˙
x(t)) ¯ ≥ p(t)(u − 1)
for all u > 0, a.e. t ∈ [S,T ], (12.5.20) 
p(t) ˙ ∈ co ∂tL(t, x(t), ¯ ˙
x(t)) ¯ and | ˙p(t)| ≤ k(t, 1)
a.e. t ∈ [S,T ]. (12.5.21) 
First observe that, employing a standard countable additivity argument, there exists 
a set of full measure E ⊂ [S,T ] such that (12.5.17) holds for every t ∈ E and for 
every j ≥ 2. Fix t0 ∈ E. Let u1, u2 ≥ 1/2 such that u1 < 1 < u2. By applying12.5 Nonautonomous Variational Problems with State Constraints 629
(12.5.17) respectively taking u = u1 and u = u2, we obtain the following relations, 
which are valid for all j ≥ 2: 
1
1 − u1

L(t0, x(t ¯ 0), ˙
x(t ¯ 0)) − L

t0, x(t ¯ 0), ˙
x(t ¯ 0)
u1

u1

≤ pj (t0), (12.5.22) 
pj (t0) ≤
1
u2 − 1

L

t0, x(t ¯ 0), ˙
x(t ¯ 0)
u2

u2 − L(t0, x(t ¯ 0), ˙
x(t ¯ 0)
. (12.5.23) 
Therefore the sequence {pj (t0)}j is bounded and, using also (12.5.19), we deduce 
that {pj (S)}j is a bounded sequence. Then the compactness of trajectories theorem 
(Theorem 6.3.3) guarantees that along a subsequence (we do not relabel) pj
converges weakly in W1,1 to an arc p ∈ W1,1([S,T ]; R) satisfying (12.5.21). Fix 
any t ∈ E and let u > 0. Take N ≥ 1 with 1/N < u. Since (12.5.17) is valid 
for every j ≥ 2, passing to the limit as j → +∞, we obtain that the inequality in 
(12.5.20) is satisfied for every t ∈ E. Recalling that E ⊂ [S,T ] is of full measure 
we can conclude that (12.5.20) is confirmed. 
Proof of Theorem 12.5.1 Since we assume that (A1) is satisfied, from Theo￾rem 12.5.4 we know that we can find an absolutely continuous function p ∈
W1,1([S,T ]; R) satisfying properties (12.5.6), (12.5.7) and (12.5.8). Let t ∈ [S,T ]
be a point such that the Weierstrass-type condition (12.5.6) is satisfied. The change 
of variable r = 1
u gives 
L(t, x(t), r ¯ ˙
x(t)) ¯ 1
r − L(t, x(t), ¯ ˙
x(t)) ¯
≥ p(t)1
r − 1

, for all r > 0, a.e. t ∈ [S,T ].
(12.5.24) 
Multiplying both terms of (12.5.24) by r we obtain 
L(t, x(t), r ¯ ˙
x(t)) ¯ − rL(t, x(t), ¯ ˙
x(t)) ¯
≥ p(t)(1 − r) for all r > 0, a.e. t ∈ [S,T ]. (12.5.25) 
Adding (r − 1)L(t, x(t), ¯ ˙
x(t)) ¯ to both terms of (12.5.25) we deduce that 
L(t, x(t), r ¯ ˙
x(t)) ¯ − L(t, x(t), ¯ ˙
x(t)) ¯
≥ 
L(t, x(t), r ¯ ˙
x(t)) ¯ − p(t)
(r − 1) for all r > 0, a.e. t ∈ [S,T ].
This means that L(t, x(t), ¯ ˙
x(t)) ¯ − q(t) ∈ ∂˜
rL(t, x(t), r ¯ ˙
x(t)) ¯ r=1 (see (12.5.3) for 
the definition of ∂˜
rL). Setting q(t) := L(t, x(t), ¯ ˙
x(t)) ¯ − p(t), it follows that630 12 Regularity of Minimizers

L(t, x(t), ¯ ˙
x(t)) ¯ − q(t) = p(t),
q(t) ∈ ∂˜
rL(t, x(t), r ¯ ˙
x(t)) ¯ r=1
a.e. t ∈ [S,T ]. (12.5.26) 
Let Q(t, v) be a selection of ∂˜
rL(t, x(t), rv) ¯ r=1 (at those points where the latter set 
is nonempty) such that Q(t, ˙
x(t)) ¯ = q(t) a.e. on [S,T ]. 
Recall now that we are assuming also (A2). So, if ∂˜
rL(t, x(t), rv) ¯ r=1 = ∅ or is 
not defined when |v| > R0, for some R0 > 0, then (12.5.26) implies that ‖ ˙
x¯‖L∞ ≤
R0. Otherwise, there exist c ∈ R and a negligible set N such that (12.5.2) holds. 
Then we can find a set E0 ⊂ [S,T ] of positive measure such that, for each point 
t0 ∈ E0, |˙
x(t ¯ 0)| < c. Let p, q be as in (12.5.26). Fixing a point t0 ∈ E0 \ N , we 
have 
p(t) = L(t, x(t), ¯ ˙
x(t)) ¯ − q(t) = p(t0) +
 t
t0
p(s) ds ˙ a.e. t ∈ [S,T ].
Since ˙
x(t ¯ 0) ∈ C it follows that 
p(t) ≥ inf
|v|<c,v∈C
∂˜
rL(τ,x(τ ),rv) ¯ r=1/=∅
τ∈[S,T ]\N
{L(τ, x(τ ), v) ¯ − Q(τ, v)} −  T
S
k(s, 1) ds.
From condition (A2) we can find R >¯ 0 such that, for a.e. t ∈ [S,T ], 
p(t) = L(t, x(t), ¯ ˙
x(t)) ¯ − q(t) > sup
|v|≥R,v ¯ ∈C
∂˜
rL(t,x(t),rv) ¯ r=1/=∅
{L(t, x(t), v) ¯ − Q(t, v)}.
We conclude that |˙
x(t) ¯ | ≤ R¯ a.e. t ∈ [S,T ]. ⨅⨆
12.6 Bounded Controls 
Up to now, we have restricted attention to regularity properties of minimizers for 
variational problems with no dynamic constraints. Our investigations have centred 
on conditions under which derivatives are locally essentially bounded on some 
relatively open subset of full measure and on the implications of such conditions. 
What regularity properties of minimizers can be established for problems with 
nonlinear dynamic constraints, under standard hypotheses guaranteeing existence 
of a minimizer? General results are lacking. Our earlier analysis adapts however to 
yield conditions for optimal controls to be locally essentially bounded (and hence 
for optimal state trajectories to have locally essentially bounded derivatives) in the 
case of a time invariant linear dynamic constraint: 
x(t) ˙ = Ax(t) + Bu(t) + d(t).12.6 Bounded Controls 631
Consider the problem: 
(L)
⎧
⎪⎪⎨
⎪⎪⎩
Minimize  T
S L(t, x(t), u(t))dt
over x ∈ W1,1([S,T ]; Rn) and measurable u : [S,T ] → Rm satisfying
x(t) ˙ = Ax(t) + Bu(t) + d(t) a.e.,
x(S) = x0, x(T ) = x1,
the data for which comprise an interval [S,T ], functions L : [S,T ]×Rn×Rm → R
and d : [S,T ] → Rn, matrices A ∈ Rn×n and B ∈ Rn×m and points x0, x1 ∈ Rn. 
Theorem 12.6.1 Suppose that the data for (L) satisfy the following hypotheses 
(H1): L(t, x, u) is bounded on bounded sets, measurable in t and convex in u. d is 
an integrable function, 
(H2): For each bounded set M ⊂ Rn × Rm, there exists a constant K such that for 
all t ∈ [S,T ] and (x1, u1), (x2, u2) ∈ M
|L(t, x1, u1) − L(t, x2, u2)| ≤ K|(x1 − x2, u1 − u2)|,
(H3): There exist a number c ≥ 0 and a convex function θ : [0,∞) → [0,∞) such 
that θ (r)/r → ∞ as r → ∞ and 
L(t, x, v) ≥ −c|x| + θ (|v|)
for all (t, x, v) ∈ [S,T ] × Rn × Rm. 
Suppose that there exists a process that satisfies the end-point constraints for (L). 
Take any minimizer (x,¯ u)¯ . (Under the hypotheses a minimizer exists.) Then there 
exists a closed subset Ω ⊂ [S,T ] of zero measure with the following property: for 
any t' ∈ [S,T ]\Ω, u¯ is essentially bounded on a relative neighbourhood of t'
. 
Furthermore, there exists a measurable function p : [S,T ] → Rn which is locally 
Lipschitz continuous on [S,T ]\Ω, such that 
− ˙p(t) ∈ p(t)A − co ∂xL(t, x(t), ¯ u(t)) ¯ a.e.
and 
H(t, x(t), ¯ u(t), p(t)) ¯ = max
u∈Rm H(t, x(t), u, p(t)) ¯ a.e.
where H denotes 
H(t, x, u, p) := p · (Ax + Bu) − L(t, x, u)
and ∂xL denotes the limiting subgradient with respect to x.632 12 Regularity of Minimizers
We deduce from Theorem 2.4.1 that (L) has a minimizer. However the earlier 
derived maximum principle, Theorem 7.2.1, cannot be applied to this problem, 
because the hypotheses of Theorem 12.6.1, which are tailored to the requirements 
of existence theory, do not imply those of Theorem 7.2.1. Theorem 12.6.1 asserts 
validity of a weaker form of maximum principle, in which the adjoint arc is not 
required to be absolutely continuous, but is required instead to be merely locally 
Lipschitz continuous on a relatively open subset of [S,T ], of full measure. 
Theorem 12.6.1 is proved by reducing problem (L) to a variational problem 
without a dynamic constraint, but in which the cost integrand depends on x and 
its higher derivatives Dx,... ,Dn˜−1x, for some n˜ ≤ n, and then carrying out a 
similar, but more intricate, analysis to that of Sect. 12.3. Details are given in [82]. 
Not surprisingly, Theorem 12.6.1 serves as a stepping stone to proving essential 
boundedness of optimal controls in special cases. One such case we now consider. 
In applications of dynamic optimization to control system design, it is usually 
necessary to take account of magnitude constraints on control variables, which 
reflect actuator limitations, safety considerations or permissible regions of the 
control variable space for validity of the dynamic model. The presence of constraints 
can, however, greatly complicate the computation of optimal controls. The standard 
technique for bypassing these difficulties is to drop the constraint and to add, instead, 
a term ϵ|u|
r to the cost integrand which penalizes excessive control action. In 
quadratic cost control it is known that inclusion of this penalty term ensures that 
optimal controls are bounded, for any ϵ > 0 and r = 2. Furthermore, the larger 
ϵ, the smaller is the uniform bound on optimal controls. This is shown by direct 
calculation, an approach which is not possible for (L) in general. We can however 
use Theorem 12.6.1 to assess how large the exponent r in the penalty term must be, 
at least to ensure boundedness of controls. 
Proposition 12.6.2 Let (x,¯ u)¯ be a minimizer for (L). Assume that hypotheses 
(H1)–(H3) of Theorem 12.6.1 are satisfied and that L has the form 
L(t, x, u) = L1(t, x, u) + ϵ|u|
r
in which L1 : [S,T ] × Rn × Rm → R is a given non-negative valued function and 
r ≥ 1 and ϵ > 0 are given numbers. Suppose further that: 
Given any compact set D ⊂ [S,T ] × Rn, there exists a number c such that 
max{|a|+|b| : (a, b) ∈ co∂L(t, x, u)} ≤ c(1 + |u|
r
)
for all (t, x) ∈ D and u ∈ Rm. 
Then u¯ is essentially bounded on [S,T ]. 
A similar analysis to that of Sect. 12.4 can be used to prove the proposition. See [82] 
for details.12.7 Lipschitz Continuous Controls 633
Recall the Ball Mizel example, which can be reformulated as an dynamic 
optimization problem: 
⎧
⎪⎪⎨
⎪⎪⎩
Minimize  1
0
	
(x3(t) − t2)2|u(t)|
14 + ϵ|u(t)|
2


dt
over x ∈ W1,1([0, 1]; R) and measurable u : [0, 1] → R satisfying
x(t) ˙ = u(t) a.e.,
x(S) = 0, x(1) = k.
It follows from the discussion of Sect. 12.1, that there is a unique optimal control for 
this problem, namely the unbounded function u(t) = kt−1/3, for suitable choices 
of the constants k > 0 and ϵ > 0. Proposition 12.6.2 provides an engineering 
perspective on this problem. It tells us thatr = 2 is too small a value for the exponent 
in the ‘penalty term’ ϵ|u(t)|
r, to ensure boundedness of optimal controls. On the 
other hand, it is a straightforward matter to check by applying Proposition 12.6.2 
that optimal controls are bounded if the penalty term is taken to be 
ϵ
 T
S
|u(t)|
r
dt
for any ϵ > 0 and r ≥ 14. 
12.7 Lipschitz Continuous Controls 
In certain cases, necessary conditions of optimality can be used directly to establish 
regularity properties of minimizers. Indeed, examining implications of known 
necessary conditions is the traditional approach to regularity analysis. A simple 
example is Hilbert’s proof that arcs satisfying the Euler Lagrange condition for 
the basic problem in the calculus of variations are automatically of class Cr if the 
Lagrangian L(t, x, v) is of class Cr (r ≥ 2) and strictly convex in v. (The main 
steps are reproduced in the proof above of assertion (iv) of Theorem 12.2.4.) We 
illustrate the method by giving conditions under which a control function satisfying 
the state constrained maximum principle of Chap. 10 is Lipschitz continuous. 
Consider the dynamic optimization problem 
(R)
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize l(x(S), x(T )) +  T
S [L(t, x(t)) + 1
2uT (t)Ru(t)]dt
over arcs x ∈ W1,1([S,T ]; Rn) and
measurable functions u : [S,T ] → Rm satisfying
x(t) ˙ = f (t, x(t)) + G(t, x(t))u(t) a.e.,
h(x(t)) ≤ 0 for all t ∈ [S,T ],
(x(S), x(T )) ∈ C,634 12 Regularity of Minimizers
with data an interval [S,T ], functions L : [S,T ] × Rn → R, l : Rn × Rn → R, 
f : [S,T ] × Rn → Rn, G : [S,T ] × Rn → Rn×m, h : Rn → R, a closed set 
C ⊂ Rn × Rn and a symmetric m × m matrix R. 
It is to be expected that control functions for (R) satisfying the maximum 
principle (with non-zero cost multiplier) are Lipschitz continuous, when the cor￾responding state trajectories are interior. In this case, the regularity property follows 
directly from the generalized Weierstrass condition and strict convexity of the Un￾maximized Hamiltonian with respect to the control variable. The fact that optimal 
controls are Lipschitz continuous also for problems with active state constraints 
comes, on the other hand, as something of a surprise. It means that, for the class 
of problems here considered, optimal state trajectories do not instantly change 
direction when they strike the boundary of the state constraint set. 
Write 
H(t, x, p, u) := p · [f (t, x) + G(t, x)u]−[L(t, x) + (1/2)uT Ru]
and, for x ∈ W1,1, 
I (x) := {t ∈ [S,T ] : h(x(t)) = 0}.
We say that a process (x,¯ u)¯ , satisfying the constraints of (R) is a normal extremal 
if there exist p ∈ W1,1([S,T ]; Rn) and μ ∈ C∗(S, T ) such that 
− ˙p(t) ∈ ∂xH(t, x(t), p(t) ¯ +

[S,t]
∇h(x(s))dμ(s), ¯ u(t)) ¯ a.e.,
(p(S), −[p(T ) +

[S,T ]
∇h(x(t))dμ(t) ¯ ]) ∈ ∂l(x(S), ¯ x(T )) ¯ + NC(x(S), ¯ x(T )), ¯
supp {μ} ⊂ I (x), ¯
H(t, x(t), p(t) ¯ +

[S,t]
∇h(x(s))dμ(s), ¯ u(t)) ¯ (12.7.1)
= max
u∈Rn H(t, x(t), p(t) ¯ +

[S,t]
∇h(x(s))dμ(s), u). ¯ a.e..
In other words, a normal extremal is an admissible process, for which the maximum 
principle is satisfied with cost multiplier λ = 1. 
Theorem 12.7.1 (Lipschitz Continuity of Optimal Controls) Take a normal 
extremal (x,¯ u)¯ for (R). Assume that 
(H1): L, f, G and l are locally Lipschitz continuous,12.7 Lipschitz Continuous Controls 635
(H2): h is of class C1,1, i.e. h is everywhere differentiable with a derivative which 
is locally Lipschitz continuous, 
(H3): ∇hT (x(t))G(t, ¯ x(t)) ¯ /= 0 for all t ∈ I (x)¯ , 
(H4): R is positive definite. 
Then u¯ is Lipschitz continuous. 
Remarks 
(i): The proof to follow can be adapted to allow for multiple state constraints. 
Extensions are also possible to cover problems in which the cost integrand is 
strictly convex in the control variable, but possibly non-quadratic. 
(ii): The normality hypothesis in Theorem 12.7.1 is of an intrinsic nature. Suffi￾cient conditions, open to direct verification, can be given for normality. These 
typically require that local approximations to the dynamics are controllable 
in some sense, by means of controls which maintain strict feasibility of the 
pathwise state constraint. 
(iii): Hager [120] and Malanowski [146] have shown that optimal controls are 
Lipschitz continuous (and have estimated the Lipschitz constants of optimal 
controls and Lagrange multipliers) for certain classes of problems involving 
control constraints as well as state constraints. Theorem 12.7.1 does not cover 
problems with control constraints but, on the other hand, departs from [120] 
and [146] by allowing nonsmooth data and also initial states which lie in the 
state constraint boundary. 
Proof In view of the special structure of the Hamiltonian, we deduce from (12.7.1) 
that 
u(t) ¯ = R−1GT (t, x(t)) ¯

p(t) +

[S,t]
∇h(x(s))dμ(s) ¯

a.e.. (12.7.2) 
It is immediately evident from this expression that u¯ is essentially bounded. It 
follows also that u¯ can be chosen to have left and right limits on (S, T ) and to be 
continuous at t = S and t = T . We deduce from the state and adjoint equation that 
p and x are both Lipschitz continuous. 
Step 1: We show that μ has no atoms in (S, T ) and, for any t ∈ (S, T ) ∩ I (x)¯ , 
∇hT (x(t)) ¯

f + GR−1GT

p(t) +

[S,t]
∇h(x(s))dμ(s) ¯
 = 0. (12.7.3) 
(In this relation, f and G are evaluated at (t, x(t)) ¯ .) 
Take any point t ∈ (S, T ) ∩ I (x)¯ . Then the fact that x(t) ¯ satisfies the state 
constraint permits us to conclude that, for all δ > 0 sufficiently small, 
δ−1(h(x(t ¯ + δ)) − h(x(t))) ¯ ≤ 0,636 12 Regularity of Minimizers
δ−1(h(x(t)) ¯ − h(x(t ¯ − δ))) ≥ 0.
Passing to the limit as δ ↓ 0, we deduce that 
∇hT (x(t)) ¯

f + GR−1GT

p(t) +

[S,t]
∇h(x(s))dμ(s) ¯
 ≤ 0,
∇hT (x(t)) ¯

f + GR−1GT

p(t) +

[S,t)
∇h(x(s))dμ(s) ¯
 ≥ 0.
Subtracting these inequalities gives 
∇hT (x(t))G(t, ¯ x(t))R ¯ −1GT (t, x(t)) ¯ ∇h(x(t))μ( ¯ {t}) ≤ 0.
Since ∇hT (x(t))G(t, ¯ x(t))R ¯ −1GT (t, x(t)) ¯ ∇h(x(t)) > ¯ 0, it follows that 
μ({t}) = 0.
These relations also imply (12.7.3). Since the support of μ is contained in I (x)¯ , we 
conclude that μ has no atoms in (S, T ). 
Step 2: We show that t → 
[S,t] dμ(s) is Lipschitz continuous on (S, T ). As u¯ is 
continuous at t = S and t = T , the Lipschitz continuity of u¯ on [S,T ] then follows 
directly from (12.7.2). 
Assume to the contrary that the function is not Lipschitz continuous on (S, T ). 
Then there exist Ki ↑ ∞ and a sequence of intervals {[si, ti]} in (S, T ) such that, 
for each i, 
si /= ti and  ti
si
dμ(s) = Ki|ti − si|. (12.7.4) 
Since supp {μ} ⊂ I (x)¯ , it follows that [si, ti] ∩ I (x)¯ = ∅ / . Furthermore, we can 
arrange by increasing si and decreasing ti if necessary that 
si, ti ∈ (S, T ) ∩ I (x). ¯
In view of (12.7.4), we can ensure by subsequence extraction that either 
(A):  si+t
i 2 si dμ(s) ≥ 1
2
 ti
si dμ(s), for all i
or 
(B):  ti
si+t
i 2
dμ(s) ≥ 1
2
 ti
si dμ(s), for all i.
Assume first (A). Under the hypotheses and since ∇h is continuous, there exists 
β > 0 such that, for each i sufficiently large, 
∇hT (x(t))GR ¯ −1GT ∇h(x(s)) > β ¯ for all s,t ∈ [si, ti].12.7 Lipschitz Continuous Controls 637
Since h(x(s ¯ i)) = 0, 
h(x(t ¯ i)) = 0 +  ti
si
d
dt h(x(s))ds ¯
=  ti
si ∇hT (x(t)) ¯

f + GR−1GT 
p(t) + 
[S,t] ∇h(x(s))dμ(s) ¯
 dt
=  ti
si
[Di(t) + Ei(t)]dt,
where 
Di(t) := ∇hT (x(t)) ¯

f + GR−1GT

p(t) +

[S,si]
∇h(x(s))dμ(s) ¯

and 
Ei(t) := ∇hT (x(t))GR ¯ −1GT

(si,t]
∇h(x(s))dμ(s). ¯
Under the hypotheses, the functions Di : [si, ti] → R, i = 1,... , are Lipschitz 
continuous with a common local Lipschitz constant (write it K). Also, by (12.7.3), 
Di(si) = 0 for all i.
It follows that 
 ti
si
Di(t)dt =
 ti
si
(ti − t)
d
dt
Di(t) dt ≥ −K (ti − si)2
2 .
Also, 
 ti
si Ei(t)dt =  ti
si ∇hT (x(t))GR ¯ −1GT 
(si,t]
∇h(x(s))dμ(s)dt ¯
≥ β
 ti
si
 t
si dμ(s)dt
= β[−  ti
si
(t − si)dμ(t) + |ti − si|
 ti
si dμ(t)]
= β
 ti
si
(ti − t)dμ(t).
But by (A), 
 ti
si
(ti − t)dμ(t) =  si+t
i 2 si (ti − t)dμ(t) +  ti
si+t
i 2
(ti − t)dμ(t)
≥ 
ti − si+ti
2
  si+t
i 2 si dμ(t) + 0
≥ ti−si
2
Ki
2 (ti − si).
Therefore, 
h(x(t ¯ i)) ≥ −K (ti − si)2
2 + β
Ki
4 (ti − si)
2, for all i.638 12 Regularity of Minimizers
Since Ki ↑ ∞ it follows that h(x(t ¯ i)) > 0, for i sufficiently large. This contradicts 
the fact that x¯ satisfies the state constraint. 
Similar reasoning leads to a contradiction in case (B) also. Specifically, we 
examine the properties of the functions 
D˜i(t) := ∇hT (x(t)) ¯

f (t, x(t)) ¯ + GR−1GT

p(t) +

[S,ti]
∇h(x(s))dμ(s) ¯

and 
E˜i(t) := −∇hT (x(t))GR ¯ −1GT

(t,ti]
∇h(x(s))dμ(s) ¯
in place of Di and Ei and show that, for i sufficiently large, 
 ti
si
[D˜i + E˜i]dt < 0 .
Since hj (x(s ¯ i)) = 0 we have 
hi(x(t ¯ i)) = 0 −
 ti
si
[D˜i + E˜i]dt > 0,
which is not possible. ⨅⨆
12.8 Exercises 
12.1 Let x¯ ∈ W1,1([S, T ]; Rn) be a minimizer for 
⎧
⎪⎨
⎪⎩
Minimize  T
S L(t, x(t), x(t))dt ˙
over x ∈ W1,1 s.t.
x(S) = x0 and x(T ) = x1
in which L is a given function and x0 and x1 are given points in Rn. Assume that 
(i): L is a Ck function (k ≥ 2), 
(ii): ∇2 
vL(t, x(t), ¯ v) is positive definite, for all (t, v) ∈ [S, T ] × Rn. 
Suppose it is known that x¯ ∈ W1,∞([S, T ]; Rn). Show that x¯ ∈ Ck([S, T ]; Rn). 
Hint: Use the Euler Lagrange condition (Theorem 1.2.1) to show that x¯ ∈
Cj−1([S, T ]; Rn) implies x¯ ∈ Cj ([S, T ]; Rn), for j = 2,...,k.12.8 Exercises 639
12.2 Let x¯ be a minimizer for the calculus of variations problem involving a second 
order derivative: 
⎧
⎪⎨
⎪⎩
Minimize  T
S L(x(t), x(t), ˙ x(t))dt ¨
over x ∈ W2,1([S,T ]; Rn) s.t.
(x, x)(S) ˙ = (x0, v0) and (x, x)(T ) ˙ = (x1, v1) ,
in which L : Rn × Rn × Rn → R is a given function and (x0, v0) and (x1, v1) are 
given points in Rn × Rn. Assume 
(i): L is locally Lipschitz continuous, 
(ii): w → L(x, v, w) is convex each fixed (x, v), 
(iii): there exists kL ∈ L1 such that 
∂xL(x(t), ¯ ˙
x(t), ¯ ¨
x)(t) ¯ |x= ¯x(t) ∈ kL(t)B, for a.e. t ∈ [S,T ],
(iv): there exists θ : (0,∞) → (0,∞) such that θ (0) = 0, lim r↑∞ θ (r)/r = ∞ and 
L(x, v, w) ≥ θ (|w|) for all (x, v, w) ∈ Rn × Rn.
Show that ¨
x¯ is essentially bounded. 
Hint: Reformulate the problem as one involving a first order linear dynamic 
constraint, by state augmentation. Now use a similar time transformation technique 
to that one employed in Sect. 12.5 to identify x¯ is a minimizer for an dynamic 
optimization problem for which necessary conditions of optimality are known. Then 
deduce essential boundedness of ¨
x¯ from these necessary conditions. 
12.3 (A Nonsmooth Erdmann – Du Bois-Reymond Condition) Let x¯ be a W1,1 
local minimizer for the optimization problem 
(CV )
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize  T
S L(t, x(t), x(t))dt ˙
over arcs x ∈ W1,1([S,T ]; Rn) satisfying
x(S) = x0, x(T ) = x1,
x(t) ˙ ∈ C a.e. t ∈ [S,T ],
x(t) ∈ A for all t ∈ [S,T ],
the data for which comprise an interval [S, T ], a function L : [S, T ]×Rn×Rn → R, 
points x0, x1 ∈ Rn, a cone C ⊂ Rn, and a set A ⊂ Rn. Assume that the Lagrangian 
L is Borel measurable and satisfies assumption (A1) of Theorem 12.5.4. Show that 
there exist an arc p ∈ W1,1([S, T ]; R) and an integrable function q : [S, T ] → R
such that640 12 Regularity of Minimizers

L(t, x(t), ¯ ˙
x(t)) ¯ − q(t) = p(t),
q(t) ∈ ∂˜
rL(t, x(t), r ¯ ˙
x(t)) ¯ r=1
a.e. t ∈ [S,T ] . (12.8.1) 
Show that if, in particular, L = L(x, v) is autonomous and just Borel measurable, 
then condition (12.8.1) is satisfied and p is a constant. (Recall that ∂˜
rL(t, x,rv)r=1 
denotes the subdifferential in the sense of convex analysis of the function (0, +∞) ϶
r ⍿→ L(t, x,rv) at r = 1, see (12.5.3).) 
Hint: Use conditions (12.5.6) and (12.5.7) provided by Theorem 12.5.4 and the 
change of variable r = 1/u (u > 0). 
12.4 Show that Theorems 12.5.1, 12.5.4, Corollary 12.5.2 and the result stated in 
Exercise 12.3 can be extended to a W1,s local minimizer x¯ for problem (CV) of 
Exercise 12.3 (or of Sect. 12.5), when s ∈ [1,∞). (For the definition of ‘W1,s local 
minimizer’ see Exercise 6.1.) 
Hint: Extend Lemmas 12.5.3 and 12.5.5 to the case in which x¯ is a W1,s arc. 
12.9 Notes for Chapter 12 
Regularity properties of minimizers for variational problems in one independent 
variable were studied extensively by Tonelli [190] in the early years of the 20th 
century, for inherent interest, no doubt, but also motivated by a desire to fill the 
gap between hypotheses for existence of minimizers and hypotheses needed to 
derive first order necessary conditions of optimality and thereby to validate the 
direct method. The regularity issue has remained a central one in multidimensional 
calculus of variations (see [115]). However this aspect of Tonelli’s legacy, notably 
the discovery that under the hypotheses of existence theory bad behaviour can 
be confined to a closed set of zero measure (the Tonelli set), was unaccountably 
overlooked when one dimensional calculus of variations evolved into dynamic 
optimization. Interest in Tonelli regularity was reawakened by Ball and Mizel, who 
saw potential applications to the field of nonlinear elasticity in which material 
failure can be associated with the existence of non-empty Tonelli sets. Ball and 
Mizel studied the structure of Tonelli sets and gave the first examples of problems 
satisfying the hypotheses of existence theory and yet having non-empty Tonelli sets 
[16], including the Ball Mizel example of Sect. 11.1. A brief proof by Vinter and 
Clarke that the Ball Mizel example exhibits the pathological behaviour of interest, 
based on the construction of a nonsmooth verification function, appears in [74]. 
In a series of papers Clarke and Vinter brought together Tonelli’s proof tech￾niques, based on the application of necessary conditions to suitably regular auxiliary 
Lagrangians, and methods of nonsmooth analysis, to explore further the properties 
of minimizers under the hypotheses of existence theory. The scope for constructing 
nonsmooth Lagrangians adds greatly to the flexibility of the approach. [75] general￾izes Tonelli’s earlier results [189] on the structure of Tonelli sets, to allow for vector12.9 Notes for Chapter 12 641
valued arcs, nonsmooth Lagrangians and to eliminate the need for strict convexity. 
[75] supplied the first proof that minimizers for autonomous problems satisfying the 
hypotheses of existence theory are Lipschitz continuous. Other sufficient conditions 
for Lipschitz continuity are proved, a sample of which are reproduced in this chapter. 
[76] concerns properties of the Tonelli set for noncoercive problems. In [77] it 
is established that Tonelli sets for polynomial Lagrangians are countable, with 
a finite number of accumulation points. Generalizations to variational problems 
involving higher derivatives and to dynamic optimization problems with affine 
dynamic constraints are provided in [81] and [82] respectively. 
The use of time reparameterization to supply an independent proof that min￾imizers for autonomous problems satisfying the hypotheses of Tonelli Existence 
Theory are Lipschitz continuous originates with Ambrosio, Ascenzi and Buttazzo 
[1]. A streamlined proof of this regularity property for solutions to autonomous 
problems was devised by Clarke [68]. Theorem 12.5.1 is a recent departure, due 
to Bettiol and Mariconda, based on this approach but applicable to non-autonomous 
problems [25, 26]. Hypothesis (A1) in Theorem 12.5.1 is a local Lipschitz condition 
of the map s ⍿→ L(s, x(t), σ ¯ ˙
x(t)) ¯ for a.e. t ∈ [S,T ] and for all σ > 0 (for 
a reference minimizer x¯), and represents a nonsmooth extension of the classical 
Cesari condition (S) [54, §2.7 A]. Assumption (A2) is a variation on a condition 
which was first introduced by Clarke in [66] (for Lagrangians L(t, x, v) that are 
convex in the velocity variable v); hypothesis (A2) is satisfied not only when the 
Lagrangian is coercive (w.r.t. v) but also in many situations in which the Lagrangian 
has merely a slow growth condition (see papers [25, 26] and [149] for a discussion, 
generalizations also to the case of extended valued Lagrangians, and related results 
such as the avoidance of the ‘Lavrentiev phenomenon’). 
The final section of the Chapter concerns the direct application of necessary con￾ditions to establish regularity of optimal controls. A significant early advance was 
Hager’s proof of Lipschitz continuity of optimal controls, for dynamic optimization 
problems with affine dynamics, a smooth, coercive cost integrand jointly convex 
with respect to state and control variables, and with unilateral state and control 
constraints satisfying an independence condition [120]. Extensions to allow for non￾linear dynamics were carried out by Malanowski [146]. A simple, independent proof 
of Hager’s regularity theorem, in the case of linear quadratic problems with affine 
state constraints, based on discrete approximations, was provided by Dontchev 
and Hager [94]. A more refined regularity analysis of this class of problems was 
undertaken by Dontchev and Kolmanovsky [95], who have given conditions for 
optimal controls to be piecewise analytic. The assertions of Theorem 12.7.1 are 
those of Malanowski’s regularity theorem in the case of a single state constraint, no 
control constraints and a quadratic control term in the cost. The proof, which allows 
nonsmooth data and a milder constraint qualification, strong normality, is new.Chapter 13 
Dynamic Programming 
Abstract Dynamic programming is an approach to solving dynamic optimization 
problems, centred on properties of the value function, that is the minimum cost 
parameterized by the initial time and state. For the dynamic optimization problems 
considered in this book the value function is a solution to the Hamilton Jacobi 
equation (HJE). The central question is, how should we define ‘solution’ in such 
a manner that the HJE has a unique solution and that this solution coincides 
with the value function? Other matters of interest opened up by this approach 
include techniques for verifying the optimality of a putative minimizer, feedback 
representation of optimal strategies and computational methods, some of which are 
discussed in this chapter. The goal of representing the value function as the unique 
solution, appropriately defined, of the HJE has been arrived at along two different 
paths. The first involves viscosity solutions, as introduced by Crandall and Lions. 
With this solution concept it is possible to show directly, and without consideration 
of state trajectories, that the Hamilton Jacobi equation has a unique solution. The 
second path is system theoretic, in the sense that it is intimately connected with 
properties of state trajectories; invariance theorems are employed to show that a 
solution to the Hamilton Jacobi equation provides a lower bound to the cost of an 
arbitrary state trajectory and this lower bound is achieved by some state trajectory. 
This chapter is an up to date treatment of dynamic programming that places 
emphasis on the system theoretic point of view. We consider dynamic optimization 
problems for which the value function is a, possibly discontinuous, lower semi￾continuous function. Various, equivalent, definitions of ‘solution’ of HJE are 
involved, prominent among which is that of proximal solution of Clarke. The 
chapter begins with a study of system invariance leading, on the one hand, to 
conditions under which there exists a state trajectory satisfying a given pathwise 
constraint and, on the other, to conditions under which all state trajectories have 
this property. The desired characterization of the value function as the unique 
proximal solution of the HJE results from applying the invariance theorems to an 
extended control system with pathwise constraint set constructed from an epigraph 
set, corresponding to the proximal solution of the HJE under consideration. This 
link between the value function and proximal solutions to HJE is established for 
various formulations of the dynamic optimization problem. Problems with finite 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
P. Bettiol, R. B. Vinter, Principles of Dynamic Optimization, Springer Monographs 
in Mathematics, https://doi.org/10.1007/978-3-031-50089-3_13
643644 13 Dynamic Programming
time horizons, discounted-cost problems with an infinite horizon, minimum time 
problems and problems with pathwise state constraints all make their appearance. 
In the earlier literature a full characterization of the value function as the unique 
lower semi-continuous solution of HJE was achieved only for continuously time￾dependent dynamics. A notable feature of theory presented in this chapter, the result 
of recent research, is that we allow discontinuous time dependence of the dynamics. 
Other topics are covered in this chapter. These include verification techniques of 
dynamic programming type and the role of semiconcavity in dynamic programming. 
A rounded exposition on the subject of dynamic programming necessarily 
embraces both viscosity solution and system theoretic methods. The final sections 
include discussion, comparing and contrasting the methods. Finally a simple proof 
of comparison theorem relating to an infinite horizon problem is given, to convey 
the flavour of viscosity techniques. 
13.1 Introduction 
Consider the dynamic optimization problem: 
(P )
⎧
⎪⎪⎨
⎪⎪⎩
Minimize g(x(T ))
over arcs x ∈ W1,1([S,T ]; Rn) satisfying
x(t) ˙ ∈ F (t, x(t)) a.e.,
x(S) = x0,
the data for which comprise an interval [S,T ] ⊂ R, a function g : Rn → R∪{+∞}, 
a multifunction F : [S,T ] × Rn ⇝ Rn and a point x0 ∈ Rn. 
Notice that the cost function g is allowed to take value +∞. Implicit in this 
formulation then is the end-point constraint: 
x(T ) ∈ C,
where 
C := {x ∈ Rn : g(x) < +∞}.
Dynamic programming, as it relates to the above problem, concerns the relation 
between, on the one hand, minimizers and the infimum cost of (P) and, on the other, 
solutions to the Hamilton Jacobi equation: 
φt (t, x) + min v∈F (t,x) φx (t, x) · v = 0 for all (t, x) ∈ D (13.1.1)
φ(T , x) = g(x) for all x ∈ D1. (13.1.2) 
Here, D and D1 are given subsets of [S,T ] × Rn and Rn respectively.13.1 Introduction 645
Equation (13.1.1) can alternatively be expressed in terms of the Hamiltonian 
H (t, x, p) := sup
v∈F (t,x)
p · v,
thus 
− φt (t, x) + H (t, x, −φx (t, x)) = 0 for all (t, x) ∈ D.
The link between the dynamic optimization problem (P) and the Hamilton Jacobi 
equation is the value function V : [S,T ] → R ∪ {+∞}: for each (t, x) ∈ [S,T ] ×
Rn, V (t, x) is defined to be the infimum cost for the problem 
(Pt,x )
⎧
⎪⎪⎨
⎪⎪⎩
Minimize g(y(T ))
over arcs y ∈ W1,1([t,T ]; Rn) satisfying
y(s) ˙ ∈ F (s, y(s)) a.e.,
y(t) = x,
in which (t, x) replaces the initial data (S, x0) in (P). We write this relation 
V (t, x) = inf(Pt,x ).
The elementary theory of dynamic programming (see Chap. 1) tells us that, if V is a 
C1 function then, under appropriate hypotheses on the data for (P), V is a solution 
to (13.1.1) and (13.1.2) when D = (S, T ) × Rn and D1 = Rn. 
Of course knowledge of the value function V provides the minimum cost for (P): 
it is simply V evaluated at (S, x0). But in favourable circumstances, it also supplies 
information about minimizers. Suppose that V is a C1 function and also that, for 
each (t, x), 
χ (t, x) := {v ∈ F (t, x) : Vx (t, x) · v = min v'
∈F (t,x)
Vx (t, x) · v'
} (13.1.3) 
is non-empty and single valued. Finally suppose that 
y(s) ˙ = χ (s, y(s)), y(t) = x (13.1.4) 
has a W1,1 solution y on [t,T ], for any (t, x) ∈ [S,T ] × Rn. Then y is a minimizer 
for (Pt,x ). Indeed, the calculation: 
V (t, x) = V (T , y(T )) −
 T
t
d
ds V (s, y(s))ds
= V (T , y(T )) −
 T
t
[Vt(s, y(s)) + Vx (s, y(s)) · χ (s, y(s))]ds646 13 Dynamic Programming
= V (T , y(T )) −
 T
t
[Vt(s, y(s)) + min v∈F (t,x)
Vx (s, y(s)) · v]ds
= V (T , y(T )) − 0 = g(y(T ))
shows that y has cost V (t, x), i.e. y is a minimizer for (Pt,x ). In particular, selecting 
(t, x) = (S, x0) and solving the differential Eq. (13.1.4) yields a minimizer for (P). 
An advantage to this approach is that it supplies the minimizer in ‘feedback’ form, 
favoured in control engineering applications. In a sense then, dynamic programming 
reduces the dynamic optimization problem to one of solving a partial differential 
equation. 
In more recent dynamic programming research, the role of the Hamilton Jacobi 
equation in characterizing the value function is emphasized. But the value function 
as an object of interest in its own right is a relative newcomer to variational 
analysis. Traditionally, the Hamilton Jacobi equation has had a different role: that 
of providing sufficient conditions of optimality, to test whether a putative minimizer 
(arrived at by finding an arc which satisfies some set of necessary conditions, say) 
is truly a minimizer. Using the Hamilton Jacobi equation in this spirit is called the 
Carathéodory method. The essential character of the approach is captured by the 
following sufficient condition of optimality: 
Let x¯ be an arc which satisfies the constraints of (P). Then x¯ is an L∞ local 
minimizer if, for some ϵ > 0, a C1 function1 φ : T (x,ϵ) ¯ → R can be found 
satisfying the Hamilton Jacobi equation (13.1.1) and (13.1.2) with D = int T (x,ϵ) ¯
and D1 = (x(T ) ¯ + ϵB) ∩ dom g and if 
φ(S, x0) = g(x(T )). ¯ (13.1.5) 
In the above, T (x,ϵ) ¯ denotes the ϵ-tube about x¯: 
T (x,ϵ) ¯ := {(t, y) ∈ [S,T ] × Rn : y ∈ ¯x(t) + ϵB}.
To justify these assertions, we have merely to note that, for any arc x satisfying the 
constraints of (P) and also the condition ||x − ¯x||L∞ < ϵ, we have 
φ(S, x0) = φ(T , x(T )) −
 T
t
d
ds φ(s, x(s))ds
= φ(T , x(T )) −
 T
t
[φt(s, x(s)) + φx (s, x(s)) · ˙x(s)]ds
≤ g(x(T )) + 0.
1 Given a closed set A ⊂ Rk and a function ψ : A → Rm, we say that ψ is a C1 function if it is 
continuous, if it is of class C1 on the interior of A and if ∇ψ extends, as a continuous function, to 
all of A. 13.1 Introduction 647
This inequality combines with (13.1.5) to confirm the L∞ local optimality of x¯. 
Appropriately, functions φ used in this way are called verification functions. The 
application of the Carathéodory method is illustrated, for example, in L C Young’s 
book [209], where it is used to solve a number of classical problems in the calculus 
of variations. For many of these problems, it is comparatively straightforward 
to determine a candidate for minimizer by solving Euler’s equation (a necessary 
condition for an arc to be a minimizer). Confirming this arc is truly a minimizer 
can be accomplished in favourable circumstances by constructing a verification 
function. 
An important feature of verification functions is that they do not have to be 
the value function for (P). In many cases, finite verification functions serve to 
confirm the local minimality of a particular arc, even when the value function is 
infinite at some points in its domain. This flexibility can simplify the task of finding 
verification functions in specific applications. 
How have these early, elementary ideas evolved? Consider the relation between 
the Hamilton Jacobi equation and the value function. The first issue to be settled, 
if we are to regard solving the Hamilton Jacobi equation as a means to generating 
the value function, is whether the Hamilton Jacobi equation has a unique solution 
which coincides with the value function. This question is a challenging one because, 
for many dynamic optimization problems of interest, the value function is not 
continuously differentiable. A simple example of this phenomenon was discussed in 
Chap. 1. At the outset then we must come up with a suitable concept of generalized 
solution to the Hamilton Jacobi equation. Minimal requirements are: 
1. The (possibly non-differentiable) value function is a generalized solution, 
and 
2. There exists a unique generalized solution. 
A plausible approach to providing the desired characterization of the value function 
is based on the following solution concept: a locally Lipschitz continuous function 
V : [S,T ] × Rn is an almost everywhere solution of (HJE) if 
− φt(t, x) + H (t, x, −φx (t, x)) = 0
at almost all points (t, x) ∈ (S, T ) × Rn, w.r.t. Lebesgue measure, at which V is 
Fréchet differentiable, and also 
φ(T , x) = g(x), for all x ∈ Rn .
Almost everywhere solutions might be expected to furnish a satisfactory theory 
since, by Rademacher’s theorem, a locally Lipschitz continuous function is almost 
everywhere Frechét differentiable. But unfortunately, this is not the case, as is 
illustrated by the following example, due to F. H. Clarke [68], in which there are 
multiple almost everywhere solutions.648 13 Dynamic Programming
Example 
Consider 
⎧
⎪⎪⎨
⎪⎪⎩
Minimize g(x(1)) := |x(1)|
over arcs x ∈ W1,1([0, 1]; R) s.t.
x(t) ˙ ∈ [−1, +1] a.e. t ∈ [0, 1]
x(0) = x0 ,
where x0 ∈ R is a given state. The value function is 
V (t, x) = max{|x| + t − 1, 0} for all (t, x) ∈ [0, 1] × R .
It is easy to check that V is a Lipschitz continuous function that is an almost 
everywhere solution to (HJE). However it is not uniquely so. Another, distinct, 
Lipschitz continuous function that is also an almost everywhere solution is 
V (t, x) ˜ = |x| + t − 1, for all (t, x) ∈ [S,T ] × Rn .
Notice however that 
V (t, x) ˜ ≤ V (t, x), for all (t, x) ∈ [S,T ] × Rn .
We shall show (Proposition 13.4.3, below) that this inequality is true for any locally 
Lipschitz continuous function that is an almost everywhere solution of (HJE). Our 
analysis has revealed that, while the almost everywhere solution concept is not 
useful when it comes to characterizing the value function, this class of solutions 
do at least provide lower bounds on the value function. 
The lesson learned from Example 13.1 is that we must look elsewhere for a 
suitable definition of ‘solution’. Viscosity solutions [88], whose appearance on the 
scene in the 1980’s marked an important advance in the theory of dynamic program￾ming, met all the requirements, in situations were the value function is continuous. 
(Extensions allowing for discontinuous value functions ‘lower semi-continuous 
viscosity’ solutions were subsequently introduced [19].) The key ingredients in the 
definition are as follows: 
A continuous function φ : [S,T ] × Rn → R is said to be a viscosity solution of 
− φt(t, x) + H (t, x, −φx (t, x)) = 0 for all (t, x) ∈ (S, T ) × Rn (13.1.6) 
if it satisfies the following conditions: 
(a) for any point (t, x) ∈ (S, T ) × Rn and any C1 function w : R × Rn → R such 
that (t'
, x'
) → φ(t'
, x'
) − w(t'
, x'
) has a local minimum at (t, x) we have 
− ξ 0 + H (t, x, −ξ 1) ≥ 0
where (ξ 0, ξ 1) = ∇w(t, x),13.1 Introduction 649
(b) for any point (t, x) ∈ (S, T ) × Rn and any C1 function w : R × Rn → R such 
that (t'
, x'
) → φ(t'
, x'
) − w(t'
, x'
) has a local maximum at (t,x) we have 
− ξ 0 + H (t, x, −ξ 1) ≤ 0
where (ξ 0, ξ 1) = ∇w(t, x). 
If a function φ satisfies condition (a) is said to be a viscosity supersolution (of 
(13.1.6)). If it satisfies (b), it is said to be a viscosity subsolution of (13.1.6). 
The theory of viscosity solutions employs directly the defining relation of super￾and sub- viscosity solutions to derive comparison relations of the following kind. 
Comparison Principle: Take viscosity super- and sub-solutions W+ and W−
respectively. Then 
W+(t, x) ≥ W−(t, x) for all (t, x) ∈ {T } × Rn
=⇒ W+(t, x) ≥ W−(t, x) for all (t, x) ∈ [S,T ] × Rn
Take any viscosity solution W (i.e. W is simultaneously a super- and sub-solution) 
satisfying the boundary condition W (T , .) = g. Then, by the comparison principle, 
W (t, x) ≥ V (t, x) and V (t, x) ≥ W (t, x) for all (t, x) ∈ [S,T ] × Rn
(13.1.7) 
which implies W = V . Thus the comparison principle tells us a little bit more than 
‘V is the unique solution to the Hamilton Jacobi equation satisfying the boundary 
condition’. 
In parallel with these developments, efforts were made to establish relation 
(13.1.7), governing the value function V , for an arbitrary ‘generalized’ solution to 
the Hamilton Jacobi equation by linking this relation to invariance properties of 
differential inclusions. This alternative approach has been variously referred to as 
the control theoretic approach, because it exploits properties of the class of state 
trajectories associated with the underlying control system, the viability approach, 
because it is based on viability/invariance theorems from the theory of differential 
inclusions and the generalized solutions approach, since it involves an interpretation 
of ‘solution’ to the Hamilton Jacobi equation in an appropriate ‘generalized’ sense, 
that is somewhat different to that employed in the viscosity solutions literature. 
When we turn attention to problems with right end-point constraints it is no 
longer tenable to assume that the value function is continuous. The value function is 
lower semi-continuous, however, under unrestrictive, verifiable hypotheses on the 
data. This suggests that we should aim to characterize value functions in terms 
of lower semi-continuous solutions to the Hamilton Jacobi equation, appropriately 
defined. 
The centerpiece of this chapter is a control theoretic framework for studying 
lower semi-continuous ‘solutions’ to the Hamilton Jacobi equation which relates 
them to the corresponding value function. Fundamental to our approach is the fol-650 13 Dynamic Programming
lowing concept of generalized solution to the Hamilton Jacobi equation, introduced 
by Clarke et al. [84]. 
A lower semi-continuous function φ : [S,T ] × Rn → R ∪ {+∞} is said to be a 
proximal solution to the Hamilton-Jacobi equation if 
(a)' at each point (t, x) ∈ ((S, T )×Rn)∩dom φ such that ∂P φ(t, x) is non-empty, 
− ξ 0 + H (t, x, −ξ 1) = 0 for all (ξ 0, ξ 1) ∈ ∂P φ(t, x).
Notice that condition (a) in the earlier definition of viscosity solution implies 
− ξ 0 + H (t, x, −ξ 1) ≥ 0 for all (ξ 0, ξ 1) ∈ ∂P φ(t, x)
at every point (t, x) ∈ (S, T ) × Rn such that ∂P φ(t, x) is non-empty, because 
(ξ 0, ξ 1) ∈ ∂P φ(t, x) means that (t, x) is a local minimizer of φ − w, where w is 
the quadratic function 
w(t'
, x'
) = (t'
, x'
) · (ξ 0, ξ 1) − M(|(t'
, x'
) − (t, x)|
2),
for some M > 0. 
We see that the definition of proximal solution (the solution concept employed 
according to the generalized solutions approach) is arrived by discarding condition 
(b) in the definition of viscosity solutions, relaxing condition (a) somewhat (the 
‘generalized solutions’ approach only requires consideration of quadratic test 
functions w) and replacing inequality by equality. 
If we wish to allow for F (t, x)’s that are possibly discontinuous w.r.t. the t
variable, it is natural to seek a characterization of the value function in terms 
of the unique generalized solution to the Hamilton-Jacobi equation in an ‘almost 
everywhere w.r.t. time’ sense, i.e., when we replace conditions (a)' above by: there 
exists a subset T ⊂ [S,T ] of zero Lebesgue measure such that 
(a)'' at each point (t, x) ∈ (((S, T )\T ) × Rn) ∩ dom φ such that ∂P φ(t, x) is non￾empty, 
− ξ 0 + H (t, x, −ξ 1) = 0 for all (ξ 0, ξ 1) ∈ ∂P φ(t, x).
This is not possible2 within a theory aiming to show the value function is the unique 
lower semi-continuous function that is a generalized solution of the Hamilton Jacobi
2 Frankowska, Plaskacz and Rzezuchowski [113] have shown, however, that, for problems in 
which F is measurably time dependent, the value function is the unique lower Dini solution of 
the Hamilton Jacobi equation, in the class of lower semi-continuous functions that are also epi￾continuous. 13.1 Introduction 651
equation, in the above ‘almost everywhere sense’. The following example confirms 
this assertion. 
Example 
Consider 
⎧
⎪⎪⎨
⎪⎪⎩
Minimize g(x(1)) := x(1)
over arcs x ∈ W1,1([0, 1]; R) s.t.
x(t) ˙ = 0 a.e. t ∈ [0, 1]
x(0) = x0 ,
where x0 ∈ R is a given state. The value function is V (t, x) = x for all (t, x) ∈
[0, 1] × R. However 
W (t, x) := 
x − 1 if t ≤ 1
2
x if t > 1
2
is also a lower semi-continuous function that satisfies the ‘almost everywhere’ 
condition (a)'
. (We choose T = { 1
2 }, to exclude consideration of the troublesome 
point 1
2 at which W (t, x) fails to satisfy the original conditions (a).) This confirms 
that the value function is not the unique lower semi-continuous function satisfying 
the almost everywhere conditions (a)'
. 
Our analysis will cover situations in which the velocity set is discontinuous w.r.t. 
time. We restrict the nature of discontinuities considered, however, by requiring 
that the multifunction t → F (t, x) has everywhere left and right limits and is 
continuous on the complement of a set of measure zero. A special case covered by 
this hypothesis is when t → F (t, x) has bounded variation w.r.t. time (many time 
discontinuities encountered in applied dynamic optimization are of this nature). 
In this chapter the link between lower semi-continuous solutions to the Hamilton 
Jacobi equation and value functions is achieved, as a by-product of theorems on the 
invariance properties of solutions to differential inclusions. This control theoretic 
approach is consistent with analytical techniques employed elsewhere in this book, 
and illustrates further areas of application of generalized subdifferentials. 
We mention that the invariance theorems on which the analysis is based are of 
great independent interest and have been applied in many areas of dynamic systems 
theory and in nonlinear analysis. (The systematic use of invariance theorems, such 
as those proved in this chapter, is called ‘viability theory’). 
Of course showing that the value function is a generalized solution of the 
Hamilton Jacobi equation falls somewhat short of obtaining detailed information 
about minimizers for (P). What is required is an analysis of the feedback map 
χ of (13.1.3), or some nonsmooth analogue of it, and also means of interpreting 
solutions to the ‘closed loop equation’ x˙ ∈ χ (t, x) (we can expect the right side to 
be multivalued) which generates minimizers for (P). This important area of study,652 13 Dynamic Programming
Optimal Synthesis as it is called, requires rather different analytical techniques to 
those used elsewhere in this book and will not be entered into here. 
Another direction of research into Carathéodory’s method concerns the inverse 
problem: what concept of verification function should we adopt in order that, 
corresponding to every local minimizer, there exists a verification function? This 
area of research aims at simplifying the task of finding a verification function to 
confirm the optimality of a specified putative minimizer: the smaller the class of 
verification functions, one of which can be guaranteed to confirm the optimality 
of a given minimizer, the narrower will be the search and therefore the ‘better’ 
the inverse theorem. The main result in this area, proved in this chapter, is that, 
under a mild non-degeneracy hypothesis, there always exists a Lipschitz continuous 
verification function, even in situations when there are right end-point constraints 
and when the value function (which is after all the obvious choice for verification 
function) is not even continuous! 
Another topic covered in this chapter is the interpretation of costate arcs in terms 
of generalized gradients of value functions. Connections are thereby made between 
the necessary conditions of earlier chapters and dynamic programming. 
The relevant relations were sketched in Chap. 1, where the costate arcs were 
associated with the maximum principle. We review them, in the context of the 
dynamic optimization problem (P). Now, gradients of the value function are related 
to the costate arc appearing in the generalized Euler Lagrange condition. 
Let x¯ be a minimizer. To simplify the analysis, let us suppose that V is a C2
function, that g is a C1 function and that F (t, .) has closed graph. We deduce from 
the fact that g coincides with V (T , .) that 
Vx (T , x(T )) ¯ = gx (x(T )). ¯
Since x¯ is a minimizer, we have V (S, x0) = g(x(T )) ¯ = V (T , x(T )) ¯ . It then follows 
from (13.1.1) and the identity 
V (S, x0) = V (T , x(T )) ¯ −
 T
S
{Vt(t, x(t)) ¯ + Vx (t, x(t)) ¯ · ˙
x(t) ¯ }dt
that 
Vt(t, x(t)) ¯ + Vx (t, x(t)) ¯ · ˙
x(t) ¯ = 0 =
min[Vt(t, x) + Vx (t, x) · v : x ∈ Rn, v ∈ F (t, x)] a.e..
Clearly, 
− Vx (t, x(t)) ¯ · ˙
x(t) ¯ = max v∈F (t,x(t)) ¯ (−Vx (t, x(t))) ¯ · v a.e..
Furthermore, from the multiplier rule (Theorem 5.6.1), we have that13.1 Introduction 653
− (Vtx (t, x(t)) ¯ + Vxx (t, x(t)) ¯ · ˙
x(t), V ¯ x (t, x(t))) ¯ ∈ NGr F (t,.)(x(t), ¯ ˙
x(t)) ¯ a.e..
Now define 
p(t) := −Vx (t, x(t)), ¯
h(t) := max v∈F (t,x(t)) ¯ p(t) · v ( = p(t) · ˙
x(t)). ¯
Since p(t) ˙ = −Vtx (t, x(t)) ¯ −Vxx (t, x(t)) ¯ · ˙
x(t) ¯ we deduce from these relations that 
(p(t), p(t)) ˙ ∈ NGr F (t,.)(x(t), ¯ ˙
x(t)) ¯ a.e.,
p(t) · ˙
x(t) ¯ = max v∈F (t,x(t)) ¯ p(t) · v a.e.,
−p(T ) = gx (x(T )), ¯
(h(t), −p(t)) = ∇V (t, x(t)) ¯ a.e.. (13.1.8) 
We have arrived at set of necessary conditions including the Euler Lagrange 
inclusion, in which the Hamiltonian, evaluated along the optimal trajectory, and 
the costate arc are interpreted in terms of gradients of the value function. In general, 
V will not be a C2 function and the above arguments cannot be justified. We will 
however derive, by means of a more sophisticated analysis, necessary conditions 
of optimality involving an costate arc p which satisfies a nonsmooth version of 
(13.1.8), namely p satisfies (at the same time) the following ‘partial’ and ‘full’ 
sensitivity relations: 
− p(t) ∈ co ∂xV (t, x(t)) ¯ a.e. (13.1.9) 
(h(t), −p(t)) ∈ co ∂V (t, x(t)) ¯ a.e.. (13.1.10) 
We conclude this introduction with two elementary relations satisfied by the value 
function for (P), ones to which frequent reference will be made. Take points s1, 
s2 ∈ [S,T ] with s1 ≤ s2, a minimizer x¯ : [s1, T ] → Rn (for problem (Ps1,x(s ¯ 1)) and 
also an F trajectory x : [s1, T ] → Rn. Then 
V (s1, x(s1)) ≤ V (s2, x(s2)) (13.1.11) 
V (s1, x(s ¯ 1)) = V (s2, x(s ¯ 2)). (13.1.12) 
These relations are collectively referred to as the principle of optimality, which can 
be paraphrased as 
The value function for (P) is non-decreasing along an arbitrary F trajectory, 
and is constant along a minimizing F trajectory.654 13 Dynamic Programming
We validate this principle. Suppose that (13.1.11) is false. Then there exists ϵ > 0
such that 
V (s1, x(s1)) > V (s2, x(s2)) + ϵ.
By definition of V , there exists an F trajectory z : [s2, T ] → Rn such that z(s2) =
x(s2) and 
V (s2, x(s2)) > g(z(T )) − ϵ/2.
But then the F trajectory y : [S,T ] → Rn, defined as 
y(t) =

x(t) s1 ≤ t<s2
z(t) s2 ≤ t ≤ T
satisfies 
V (s1, x(s1)) ≤ g(y(T )) = g(z(T )) < V (s2, x(s2)) + ϵ/2
< V (s1, x(s1)) − ϵ/2.
From this contradiction we deduce that (13.1.11) is true. 
As for (13.1.12), we note that, again by definition of V , 
V (s2, x(s ¯ 2)) ≤ g(x(T )) ¯ = V (s1, x(s ¯ 1)).
This combines with condition (13.1.11) to give condition (13.1.12). 
13.2 Invariance Theorems 
Invariance theorems concern solutions to a differential inclusion, which satisfy 
a specified constraint. Theorems giving conditions for existence of at least one 
solution satisfying the constraint are called weak invariance theorems or viability 
theorems. Those asserting that all solutions satisfy the constraint are called strong 
invariance theorems. 
These theorems have far reaching implications, in stability theory, dynamic 
programming, robust controller design and differential games (to name but a few 
applications areas!). Here, however, we concentrate on versions of the theorems 
suitable for characterizing value functions in dynamic optimzation. 
The starting point for all the important results in this section is a theorem 
providing conditions on an ‘autonomous’ multifunction F : Rn ⇝ Rn and a closed 
set D ⊂ Rn such that, for a given initial state x0 in D, the differential inclusion and 
accompanying constraint:13.2 Invariance Theorems 655
⎧
⎨
⎩
x˙ ∈ F (x) a.e. t ∈ [S,∞)
x(S) = x0
x(t) ∈ D for all t ∈ [S,∞)
have a locally absolutely continuous solution. Typically, the hypotheses invoked in 
theorems of this nature include the requirement that, corresponding to any point 
x ∈ D, F (x) contains a tangent vector to D, i.e. there is an admissible velocity 
pointing into the set D. In the following theorem, this ‘inward pointing’ condition 
has a dual formulation in terms of normal vectors, rather than tangent vectors: for 
every point x ∈ D such that NP
D (x) is non-empty, 
min v∈F (t,x) ξ · v ≤ 0, for all ξ ∈ NP
D (x).
The condition imposes restrictions on (F , D) only at points x in D: 
D := {x ∈ D : NP
D (x) /= ∅}.
D can be a rather sparse subset of D; it is surprising that such an economical 
condition suffices to guarantee existence of an F trajectory satisfying the constraint. 
Theorem 13.2.1 (Weak Invariance Theorem for Autonomous Systems) Take a 
multifunction F : Rn ⇝ Rn and a closed set D ⊂ Rn. Assume: 
(i) F (x) is a non-empty, convex set for each x ∈ D, 
(ii) Gr F is closed at every x ∈ D, 
xi → x, vi → v, xi ∈ D, vi ∈ F (xi) ⇒ v ∈ F (x),
(iii) There exists c > 0 such that 
F (x) ⊂ c(1 + |x|)B for all x ∈ D.
Assume further that, for every x ∈ D such that NP
D (x) /= {0}, 
min
v∈F (x) ζ · v ≤ 0 for all ζ ∈ NP
D (x). (13.2.1) 
Then, given any x0 ∈ D and S ∈ R, there exists a locally absolutely continuous 
function x : [S,∞) → Rn satisfying 
⎧
⎨
⎩
x(t) ˙ ∈ F (x(t)) a.e. t ∈ [S, +∞),
x(S) = x0,
x(t) ∈ D for all t ∈ [S, +∞).656 13 Dynamic Programming
Proof It suffices to prove existence of an arc x satisfying the stated conditions on 
an arbitrary finite interval [S,T ], since we can then generate an arc on [0, +∞) by 
concatenating a countable number of such arcs. 
Observe that only the points on the boundary of D are really involved in condition 
(13.2.1), since, when x ∈ intD, we have NP
D (x) = {0} and, in this case, (13.2.1) is 
automatically satisfied. Moreover, the hypothesis (iii) in the theorem statement can 
be replaced by the stronger hypothesis: 
(H'
) There exists K > 0 such that F (x) ⊂ KB for all x ∈ Rn, 
without loss of generality. To see this, choose any r satisfying 
r > 
ec|T −S| (|x0| + c|T − S|)
	
.
Define 
F (x) ˜ := 
F (x) if |x| ≤ r
co ((F (rx/|x|) ∪ {0}) if |x| > r.
F˜ and D satisfy the hypotheses of the theorem statement, and also (H'
), in which 
K = c(1+r). If the assertions of the theorem are valid under the stronger hypotheses 
then, there exists an F˜ trajectory z such that z(S) = x0 and z(t) ∈ D for all t ∈
[S,T ]. But 
|˙z| ≤ c(1 + |z|),
so by Gronwall’s lemma (Lemma 6.2.4) 
|z(t)| ≤ ec|T −S| (|x0| + c|T − S|) < r.
Since F and F˜ coincide on D ∩ rB, z is also an F trajectory. It follows that the 
assertions of the ‘finite interval’ version of the theorem are true with x = z. So we 
can assume (H'
). 
Fix an integer m > 0. Let {t0 = S,t1,...,tm = T } be a uniform partition of 
[S,T ]. Set hm := |T − S|/m. 
Define sequences {xm
0 ,...,xm
m}, {vm
0 ,...,vm
m−1} and {ym
0 ,...,ym
m−1} recur￾sively as follows: take ym
0 = xm
0 = x0. If, for some i ∈ {0,...,m − 1}, xm
i is 
given then choose ym
i ∈ D to satisfy 
|xm
i − ym
i | = inf{|xm
i − y| | y ∈ D}.
We now make use of the fact that, since ym
i is a closest point to xm
i in D, and 
xm
i − ym
i ∈ NP
D (ym
i ) whenever xm
i ∈/ D. Under the hypotheses then, vm
i ∈ F (ym
i )
can be chosen to satisfy13.2 Invariance Theorems 657
vm
i · (xm
i − ym
i ) ≤ 0, (13.2.2) 
indeed, in this choice, when xm
i ∈ D, we can take any vm
i ∈ F (xm
i ), otherwise we 
use condition (13.2.1). Finally, set 
xm
i+1 := xm
i + hmvm
i .
Now we define zm : [S,T ] → Rn to be the polygonal arc whose graph is the 
linear interpolant between the node points {(tm
0 , xm
0 ), . . . , (tm
m , xm
m)}. We have, for 
i = 0, 1,...,m − 1, 
zm(t) = xm
i + (t − t
m
i )vm
i for all t ∈ [t
m
i , tm
i+1].
Since |vm
0 | ≤ K and xm
0 ∈ D, 
d2
D(xm
1 ) ≤ |xm
1 − xm
0 |
2 = h2
m|v0|
2 ≤ K2(hm)
2. (13.2.3) 
Fix i ∈ {1,...,m}. Since ym
i ∈ D, we have 
d2
D(xm
i ) ≤ |xm
i − ym
i−1|
2
= |xm
i−1 − ym
i−1|
2 + |xm
i − xm
i−1|
2 + 2


xm
i−1 − ym
i−1

·


xm
i − xm
i−1

≤ d2
D(xm
i−1) + K2(hm)
2 + 0 (13.2.4) 
(by (13.2.2)). 
Relations (13.2.3) and (13.2.4), which are valid for for arbitrary i ∈ {1,...,m}, 
combine to tell us that 
d2
D(xm
i ) ≤ mK2(hm)
2 = |T − S|
2K2m−1, (13.2.5) 
for i = 1,...,m. 
The functions {zm}∞
m=1 are uniformly bounded and have common Lipschitz 
constant K. By the compactness of trajectories theorem (Theorem 6.3.3), there 
exists a Lipschitz continuous arc z such that, following the extraction of a suitable 
subsequence, we have 
zm → z uniformly as m → ∞ (13.2.6) 
and 
z˙
m → ˙z weakly in L1.
In view of (13.2.5) and the continuity of the distance function, we have658 13 Dynamic Programming
dD(z(t)) = 0 for all t ∈ [S,T ].
Since D is closed, z(t) ∈ D for all t ∈ [S,T ]. 
It remains to check that z is an F trajectory. Suppose to the contrary that z(t) / ˙ ∈
F (z(t)) on a set of positive measure. We deduce from the upper semi-continuous of 
F that there exists δ > 0 such that 
z(t) / ˙ ∈ Fδ(t)
on a set of positive measure. Here 
Fδ(t) := co {v : v ∈ F (ξ ) for some ξ ∈ z(t) + δB} + δB.
Arguing as in the proof of the compactness of trajectories theorem (Theorem 6.3.3), 
employing well-known properties on the support functions, we can find some p ∈
Rn and a subset A ⊂ [S,T ] of positive measure such that 
p · ˙z(t) > h(t) for all t ∈ A. (13.2.7) 
Here, h is the bounded, measurable function 
h(t) = max
v∈Fδ (t) p · v.
By (13.2.6) however, 
z˙
m(t) ∈ Fδ(t) a.e.
for all m sufficiently large. This implies that 
p · ˙zm(t) ≤ h(t) a.e.
for all m sufficiently large. By weak convergence of {˙zm} however, 

A
h(t)dt ≥ lim
m→∞ 
A
p · ˙zm(t)dt =

A
p · ˙z(t)dt.
This contradicts (13.2.7). It follows that z is an F trajectory. ⨅⨆
Theorem 13.2.2 (Weak Invariance Theorem for Time Varying Systems) Take 
multifunctions F : [S,T ] × Rn ⇝ Rn, P : [S,T ] ⇝ Rn. Take also S¯ ∈ [S,T )and 
a point x0 ∈ P (S)¯ . Assume that 
(i) Gr F is closed and F takes values non-empty convex sets,13.2 Invariance Theorems 659
(ii) There exist c > 0 such that 
F (t, x) ⊂ c (1 + |x|) B for all (t, x) ∈ Gr P ,
(iii) Gr P is closed and, if S¯ = S, 
x0 ∈ lim supt↓S¯ P (t) . (13.2.8) 
(iv) for every (t, x) ∈ Gr P ∩ ((S, T ) × Rn) such that NP
Gr P (t, x) /= {0}, 
ζ 0 + min
e∈F (t,x) ζ 1 · e ≤ 0 for all (ζ 0, ζ 1) ∈ NP
Gr P (t, x).
Then there exists an absolutely continuous function x satisfying 
⎧
⎨
⎩
x(t) ˙ ∈ F (t, x(t)) a.e. t ∈ [S,T ¯ ],
x(S) = x0,
x(t) ∈ P (t) for all t ∈ [S,T ¯ ].
Proof Define F˜ : R × Rn ⇝ R1+n, P˜ : R ⇝ Rn and D˜ ⊂ R1+n to be 
F (t, x) ˜ :=
⎧
⎨
⎩
{1} × F (t, x) for S<t<T
co ({(0, 0)} ∪ ({1} × F (S, x))) for t ≤ S
co ({(0, 0)} ∪ ({1} × F (T , x))) for t ≥ T ,
P (t) ˜ :=
⎧
⎨
⎩
P (t) for S<t<T
P (S) for t ≤ S
P (T ) for t ≥ T
and 
D˜ := Gr P . ˜
If S¯ = S, according to (13.2.8), we can choose sequences {t
i
0} ⊂ (S, T ) and {xi
0} ⊂
Rn such that t
i
0 → S¯ = S, xi
0 → x0 and xi
0 ∈ P (ti
0) for all i. In the case ‘S¯ ∈ (S, T )’ 
we choose t
i
0 = S¯ and xi
0 = x0 for all i. 
Fix i and apply the autonomous weak invariance theorem (Theorem 13.2.1) to 
(F ,˜ D)˜ , with initial time t
i
0 and initial state (ti
0, xi
0). (It is a straightforward matter to 
check that the relevant hypotheses are satisfied.) This supplies a locally absolutely 
continuous arc (τ i
, xi
) : [t
i
0, T ] → R × Rn such that 
⎧
⎨
⎩
(τ˙i
(t), x˙i
(t)) ∈ F (τ ˜ i
(t), xi
(t)) a.e. t ∈ [t
i
0, +∞)
(τ i
, xi
)(ti
0) = (ti
0, xi
0)
(τ i
(t), xi
(t)) ∈ D˜ for all t ∈ [t
i
0, +∞)660 13 Dynamic Programming
Clearly, 
0 ≤ ˙τ i
(t) ≤ 1.
It follows that 
S<ti
0 ≤ τ i
(t) < T a.e. t ∈ [t
i
0, T ).
By definition of F˜ then, 
τ˙
i
(t) = 1 a.e. t ∈ [t
i
0, T ].
We conclude that τ i
(t) = t for all t ∈ [t
i
0, T ], whence xi satisfies 
x˙i
(t) ∈ F (t, xi
(t)) a.e. t ∈ [t
i
0, T ]
xi
(t) ∈ P (t) for all t ∈ [t
i
0, T ].
If S¯ ∈ (S, T ), then the proof is completed by taking the F trajectory x = xi
. So 
we continue considering the case S¯ = S. Now extend the domain of xi to [S,¯ +∞)
by constant extrapolation from the right and restrict the resulting function to [S,T ¯ ]. 
This last function we henceforth denote xi. 
Consider the sequence of arcs xi : [S,T ¯ ] → Rn. The xi’s are uniformly bounded 
and the x˙i’s are uniformly integrably bounded. According to the compactness of 
trajectories theorem (Theorem 6.3.3), there exists an absolutely continuous arc x, 
which is an F trajectory on [S,T ] and 
xi(t) → x(t) for all t ∈ [S,T ].
Because the xi’s are equicontinuous, xi(ti
0) = xi
0 for each i and P has closed graph, 
it follows that the F trajectory x satisfies the conditions x(S)¯ = x0 and x(t) ∈ P (t)
for all t ∈ [S,T ¯ ]. This is what we set out to prove. ⨅⨆
We now consider strong invariance properties of differential inclusions. In 
contrast to weak invariance theorems, in which the existence of some F trajectory 
is asserted under regularity hypotheses on F which merely require F to have closed 
graph, strong invariance theorems typically invoke stronger, ‘Lipschitz’ regularity 
hypotheses concerning F. 
The first strong invariance theorem we state covers autonomous differential 
inclusions and constraints. The proof, due to Clarke and Ledyaev, is remarkable for 
its simplicity and for the fact that it covers cases in which F is possibly non-convex 
valued and fails even to have closed values. 
Theorem 13.2.3 (Strong Invariance Theorem for Autonomous Problems) Take a 
multifunction F : Rn ⇝ Rn and a closed set D ⊂ Rn. Assume that13.2 Invariance Theorems 661
(i) F takes values non-empty sets, 
(ii) there exists K > 0 such that 
F (x'
) ⊂ F (x'') + K|x' − x''|B for all x'
, x'' ∈ Rn,
(iii) for each x ∈ D such that NP
D (x) is non-empty 
sup
v∈F (x)
ζ · v ≤ 0 for all ζ ∈ NP
D (x). (13.2.9) 
Then for any Lipschitz continuous function x : [S,T ] → Rn satisfying 

x(t) ˙ ∈ F (x(t)) a.e. t ∈ [S,T ]
x(S) = x0
we have 
x(t) ∈ D for all t ∈ [S,T ].
Proof Since x is Lipschitz continuous and d2
D is locally Lipschitz continuous, the 
function s → d2
D(x(s)) is Lipschitz continuous. Choose a point t ∈ (S, T ) such that 
x and s → d2
D(x(s)) are differentiable at t and also x(t) ˙ ∈ F (x(t)). The set of such 
points has full measure. 
Let z be a closest point to x(t) in D. Then z ∈ D and x(t) − z ∈ NP
D (z). 
Under the hypotheses, there exists v ∈ F (z) such that 
|v − ˙x(t)| ≤ K|x(t) − z|.
In view of (13.2.9), 
(x(t) − z) · v ≤ 0.
Since 
(x(t) − z) · ˙x(t) = (x(t) − z) · v + (x(t) − z) · (x(t) ˙ − v)
≤ 0 + |x(t) − z|·|˙x(t) − v| ≤ K|x(t) − z|
2 ,
it follows that 
(x(t) − z) · ˙x(t) ≤ Kd2
D(x(t)). (13.2.10) 
Now 
d2
D(x(t)) = |x(t) − z|
2662 13 Dynamic Programming
and, for any δ ∈ (0, T − t), 
d2
D(x(t + δ)) = inf
y∈D
|x(t + δ) − y|
2 ≤ |x(t + δ) − z|
2.
We conclude that 
(d/dt)d2
D(x(t)) = lim
δ↓0
δ−1

d2
D(x(t + δ)) − d2
D(x(t))	
≤ lim
δ↓0
δ−1 [ψ(x(t + δ)) − ψ(x(t))]
= ∇ψ · ˙x(t) = 2(x(t) − z(t)) · ˙x(t)
in which ψ : Rn → R is the analytic function 
ψ(x) := |x − z|
2.
We deduce from (13.2.10) that 
(d/dt)d2
D(x(t)) ≤ 2Kd2
D(x(t)).
This inequality holds for almost every t ∈ [S,T ]. Since d2
D(x(S)) = 0, we conclude 
from Gronwall’s lemma (Lemma 6.2.4) that 
d2
D(x(t)) = 0 for all t ∈ [S,T ].
Since D is closed, this implies that x(t) ∈ D for all t ∈ [S,T ]. ⨅⨆
The autonomous strong invariance Theorem 13.2.3 can be extended to cover 
time dependent multifunctions F, by means of state augmentation. This is possible 
however only if F is Lipschitz continuous with respect to the time variable 
(since state augmentation accords the time variable the status of a state variable 
component, and therefore requires that F satisfies the same regularity hypotheses 
with respect to the time variable as it does with respect to the original state variable). 
The following strong invariance theorem does not require F to be Lipschitz 
continuous with respect to the time variable (though it invokes additional hypotheses 
on F in other respects). 
Theorem 13.2.4 (Strong Invariance Theorem for Time Varying Systems) Take 
multifunctions F : [S,T ] × Rn ⇝ Rn and P : [S,T ] ⇝ Rn. Take also S¯ ∈ [S,T )
and an F trajectory x¯ : [S,T ¯ ] → Rn such that 
x(¯ S)¯ ∈ P (S). ¯13.2 Invariance Theorems 663
Assume that, for some constant δ > 0, 
(i) F takes values non-empty, closed, convex sets, and F (., x) is L(S, T )-
measurable for all x ∈ Rn, 
(ii) There exists c > 0 such that 
F (t, x) ⊂ cB for all x ∈ ¯x(t) + δB, a.e. t ∈ [S,T ¯ ] ,
(iii) there exists a non-negative function kF ∈ L1 such that t → 1/(1 + kF (t)) is 
a.e. equal to a continuous function and 
F (t, x'
) ⊂ F (t, x) + kF (t)|x − x'
| B for all x, x' ∈ ¯x(t) + δB,
for a.e. t ∈ [S,T ¯ ] ,
(iv) for each s ∈ [S,T ) ¯ and t ∈ (S,T ¯ ] the following one-sided set-valued limits 
exist and are non-empty: 
F (s+, x) := lim
s'
↓s
F (s'
, x) for all x ∈ ¯x(s) + δB and
F (t−, x) := lim
t'
↑t
F (t'
, x) for all x ∈ ¯x(t) + δB ,
and there exists a set S ⊂ (S,T ) ¯ of full Lebesgue measure such that, for 
every t ∈ S, 
s → F (s, x) is continuous at t for each x ∈ ¯x(t) + δB,
(v) Gr P is closed and, if S¯ = S, 
x(¯ S)¯ ∈ lim supt↓S¯ P (t),
(vi) for every (t, x) ∈ Gr P ∩ Tδ(x)¯ ∩ ((S, T ) × Rn) at which NP
Gr P (t, x) /= ∅, 
ζ 0+

max v∈F (t−,x)
ζ 1·v

∧

max v∈F (t+,x)
ζ 1·v

≤ 0 for all (ζ 0, ζ 1) ∈ NP
Gr P (t, x) ,
in which Tδ(x)¯ := {(t, x) ∈ [S,T ¯ ]:|x − ¯x(t)| ≤ δ}. 
Then 
x(t) ∈ P (t) for all t ∈ [S,T ¯ ].664 13 Dynamic Programming
Proof We reserve for future use the following facts: by reducing the size of δ we 
can arrange that 
F (t−, x'
) ⊂ F (t−, x) + kF (t)|x − x'
| B
for all x, x' ∈ ¯x(t) + δB and t ∈ (S, T ]
and
F (t+, x'
) ⊂ F (t+, x) + kF (t)|x − x'
| B
for all x, x' ∈ ¯x(t) + δB and t ∈ [S,T )
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
(13.2.11) 
(These relations are simple consequences of hypotheses (iii) and (iv).) 
We simplify the analysis by making, at the outset, a hypothesis reduction: we 
may assume, without loss of generality, 
(HS): kF is a constant function, 
To justify imposing the first extra hypothesis (HS), suppose that kF is not a 
constant function. Introduce a change of independent variable s = σ (t), where 
σ (t) =
 t
S¯
(1 + kF (t'
))dt'
.
σ is a monotone increasing function that maps [S,T ] onto [0, σ1], where σ1 =  T
S¯ (1 + kF (t'
))dt'
. Define y(s) := (x¯ ◦ σ −1)(s) for s ∈ [0, σ1]. y is a G trajectory 
on [σ0, σ1], where G : [0, σ1] × Rn ⇝ Rn is the multifunction 
G(s, y) := (r ◦ σ −1)(s) F (σ −1(s), y) .
Here, r is the continuous selector of the equivalence class of almost everywhere 
equal functions t → 1/(1 + kF (t)). (See hypothesis (iii).) Now define the 
multifunction Q : [0, σ1] ⇝ Rn to be Q(s) := P (σ −1(s)). We find that the 
pair of multifunctions (G, Q) satisfies the hypotheses of the theorem statement, 
with time variable s ∈ [0, σ1], in which the parameters c and δ remain the 
same, kF is replaced by the measurable function s → (kF ◦σ −1)(s)
1+(kF ◦σ −1)(s) (which is 
dominated by the constant function taking value 1) and the subset S ⊂ [S,T ¯ ] of 
full Lebesgue measure is replaced by the subset σ (S) ⊂ [0, σ¯ ] of full Lebesgue 
measure. Since the transformed data satisfies the hypotheses of the theorem with 
constant Lipschitz bound (in fact, we can take the constant to be 1), we deduce 
from the special case of the theorem that y(s) ∈ Q(s) for all s ∈ [0, σ1]. But then 
x(t) = (y ◦ σ )(t) ∈ Q(σ (t)) = P (t), for all t ∈ [S,T ¯ ], as required. 
The key step in the proof is to confirm the following: 
Claim: For any subset [t0, tf ]⊂[S,T ] such that |tf − t0| ≤ δ/(8c), 
‘x(t ¯ 0) ∈ P (t0)’ implies ‘x(t) ¯ ∈ P (t) for all t ∈ [t0, tf ]’.13.2 Invariance Theorems 665
Notice that, if this claim is justified, then the assertions of the theorem are confirmed. 
To see this, we take a sequence αi ↓ 0 and, for each i, consider a partition Pi =
{t0 = S,... ,t ¯ N = tf } of [S,T −αi] such that diam Pi ≤ δ/(8c). Invoking the claim 
successively on the intervals [t0, t1],...,[tN−1, tN ], we deduce that x(t) ∈ P (t) for 
all t ∈ [t0, T¯ − αi]. But then, since x¯ is continuous and P has closed graph, we can 
conclude that x(t) ∈ P (t) for all t ∈ [S,T ¯ ], as required. 
It remains then to confirm the claim, when the hypotheses are supplemented by 
(HS). Take a subinterval [t0, tf ]⊂[S,T ] such that |tf − t0| < δ/(8c). Then, by 
hypothesis (v), we can find sequences {si} and {ξi} in [t0, tf ) and Rn respectively 
such that 
(a): si ↓ t0, ξi → ¯x(t0) and xi ∈ P (si) for all i, if t0 = S, 
(b): si = t0 and ξi = ¯x(t0) for all i, if t0 > S. 
Take ϵi ↓ 0 such that ϵi ∈ (0, δ/4), for each i. Since x¯ is continuous we can 
arrange, by eliminating terms in the sequence, that |ξi − ¯x(si)| < ϵi. 
Fix i. Now let yi : [si, tf ] → Rn be a continuously differentiable function such 
that yi(si) = ξi and 
|ξi − ¯x(si)| + || ˙yi − ˙
x¯||L1(si,tf ) ≤ ϵi . (13.2.12) 
Notice that ||yi − ¯x||L∞(si,tf ) ≤ ϵi(≤ δ/4) . Define the multifunctions 
F˜ −
i (t, x) := {v ∈ F (t−, x) : |v − ˙yi(t)| ≤ kF |x − yi(t)|
+ dF (t−,yi(t))(y˙i(t)) ∨ dF (t+,yi(t))(y˙i(t))},
F˜ +
i (t, x) := {v ∈ F (t+, x) : |v − ˙yi(t)| ≤ kF |x − yi(t)|
+ dF (t−,yi(t))(y˙i(t)) ∨ dF (t+,yi(t))(y˙i(t))}.
F˜
i(t, x) := F˜ −
i (t, x) ∪ F˜ +
i (t, x)
Now define �i : [si, tf ] ⇝ R1+n: 
�i(t, x) := 
{1} × co F˜
i(t, x)} if t ∈ (si, tf ) and |x − yi(t)| ≤ δ/2
co {(0, 0)} ∪ ({1} × cB) otherwise .
and D := Gr P .
We next apply the autonomous weak invariance Theorem 13.2.1 to (�i,D), when 
the underlying time interval is taken to be [si, tf ] and the initial state is (si, xi). To 
justify this, we must check that the hypotheses are satisfied. 
We show first that F˜ +
i (t, x) and F˜ +
i (t, x) are both non-empty sets. In view of the 
definition, we need to check this property only when |x − yi(t)| ≤ δ/2 and t is any 
point in [si, tf ]. Take e ∈ F (t−, yi(t)) such that | ˙yi(t) − e| = dF (t−,yi(t))(y˙i(t)).666 13 Dynamic Programming
Then, by (13.2.11), there exists v ∈ F (t−, x) such that |v − e| ≤ kF |x − yi(t)|. It 
follows that 
|v − ˙yi(t)|≤|v − e|+|e − ˙yi(t)| ≤ kF |x − yi(t)| + dF (t−,yi(t))(y˙i(t)) .
Clearly, v satisfies conditions for membership of the set F˜ −
i (t, x); this set then is 
non-empty. A similar analysis establishes non-emptiness of F˜
i(t+, x). Using the 
fact that the function t → dF (t−,yi(t))(y˙i(t)) ∨ dF (t+,yi(t))(y˙i(t)) is upper semi￾continuous, it is easy to show that the multifunction F˜
i has closed graph. The 
multifunction �i then, too, takes values non-empty convex sets and has closed graph. 
It remains to check the inward pointing hypothesis. This will be satisfied if we 
can show 
ζ 0 + min v∈co F˜
i(t,x)
ζ 1 · v ≤ 0 for all x ∈ yi(t) + (δ/2)B, t ∈ [si, tf ].
Let v− and v+ be minimizers of v → ζ 1 · v over the non-empty, compact sets 
F˜ −
i (t, x) and F˜ +
i (t, x), respectively. Then, since v− ∈ F (t−, x) and v+ ∈ F (t+, x), 
min
v∈F˜
i(t,x)
ζ 1 · v =

ζ 1 · v−

∧

ζ 1 · v+

≤ ( max v∈F (t−,x)
ζ 1 · v) ∧ ( max v∈F (t+,x)
ζ 1 · v)
So, in consequence of hypothesis (vi) in the theorem statement, 
ζ 0 + min v∈co F (t,x) ˜ ζ 1 · v = ζ 0 + min
v∈F˜
i(t,x)
ζ 1 · v
≤ ζ 0 +

max v∈F (t−,x)
ζ 1 · v

∧

max v∈F (t+,x)
ζ 1 · v

≤ 0.
We have arrived at the desired inequality. 
The autonomous weak invariance Theorem 13.2.1 now tells us that there exists 
a �i trajectory (τi, xi) on [si, tf ] such that τi(si) = si, xi(si) = ¯x(si) and 
(τi(t), xi(t)) ∈ �i(t) for all t ∈ [si, T ]. From the properties of F and the definition 
of �i we know that τ˙i = 1 and | ˙xi(t)| ≤ c, a.e.. But then 
xi(t) ∈ P (t) for all t ∈ [si, tf ] . (13.2.13) 
In view of (13.2.12), since |tf − si| < δ/(8c) and since, both xi and x¯ are F
trajectories (and consequently | ˙xi(t)| and |˙
x(t) ¯ | have magnitudes bounded by c), 
we have, for all t ∈ [si, tf ], 
|xi(t) − yi(t)| ≤|xi(t) − ¯x(t)|+|¯x(t) − yi(t)|
≤
 tf
si
| ˙xi(t) − ˙
x(t) ¯ |dt + |ξi − ¯x(si)|+|¯x(t) − yi(t)|
< δ/4 + δ/4 + δ/4 = 3δ/4 .13.3 The Value Function and Generalized Solutions of the Hamilton Jacobi. . . 667
It now follows from the definition of �i that x˙i(t) ∈ F (t, xi(t)), a.e. t ∈ [si, tf ]
and, recalling also hypothesis (iv), 
| ˙xi(t) − ˙yi(t)| ≤ kF |xi(t) − yi(t)| + dF (t,yi(t))(y˙i(t)), a.e. t ∈ [si, tf ] .
Since dF (t,x)(v) is Lipschitz continuous w.r.t. to x and v, with Lipschitz constants 
kF and 1 respectively, we know however that 
dF (t,yi(t))(y˙i(t)) ≤ dF (t,x(t)) ¯ (˙
x(t)) ¯ + kF |yi(t) − ¯x(t)|+|˙yi(t) − ˙
x(t) ¯ |
≤ 0 + kF |yi(t) − ¯x(t)|+|˙yi(t) − ˙
x(t) ¯ |
It now follows from (13.2.12) that 
 tf
si
dF (t,yi(t))(y˙i(t))dt ≤ (1 + kF |T − S|) × ϵi .
But then, by Gronwall’s lemma, 
|xi(t) − ¯x(t)| ≤ ekF |T −S|
|ξi − ¯x(si)| + ekF |T −S|
(1 + kF |T − S|) × ϵi
for all t ∈ [si, tf ] . (13.2.14) 
Let xe
i be the extension of xi to [t0, tf ], by constant extrapolation to the left. By 
the compactness of trajectories theorem, xe
i converges, uniformly, to an F trajectory 
x∗ on [t0, tf ]. It follows from (13.2.14) that xe
i → x∗, uniformly on [t0, tf ], and 
x∗ = ¯x. Since P has closed graph, we conclude from (13.2.13) that x(t) ¯ ∈ P (t) for 
all t ∈ [t0, tf ]. The claim has been confirmed and the proof is complete. ⨅⨆
13.3 The Value Function and Generalized Solutions of the 
Hamilton Jacobi Equation 
With the apparatus of invariance now at hand, we are ready to return to the dynamic 
optimization problem: 
(P )
⎧
⎪⎪⎨
⎪⎪⎩
Minimize g(x(T ))
over arcs x ∈ W1,1([S,T ]; Rn) satisfying
x(t) ˙ ∈ F (t, x(t)) a.e.,
x(S) = x0,
in which g : Rn → R ∪ {+∞} is a given function, [S,T ] is a given interval, 
F : [S,T ] × Rn ⇝ Rn is a given multifunction and x0 ∈ Rn a given point.668 13 Dynamic Programming
Recall that the value function for (P), V : [S,T ] × Rn → R ∪ {+∞}, is the 
function 
V (t, x) := inf(Pt,x ) for all (t, x) ∈ [S,T ] × Rn,
in which inf(Pt,x ) denotes the infimum cost of the optimization problem: 
(Pt,x )
⎧
⎪⎪⎨
⎪⎪⎩
Minimize g(y(T ))
over arcs y ∈ W1,1([t,T ]; Rn) satisfying
y(s) ˙ ∈ F (t, y(s)) a.e.,
x(t) = x,
thus 
V (t, x) := inf(Pt,x ), for all (t, x) ∈ [S,T ] × Rn.
Our goal is to characterize V as a generalized solution, appropriately defined, to the 
Hamilton Jacobi equation: 
φt(t, x) + min v∈F (t,x) φx (t, x) · v = 0 for all (t, x) ∈ (S, T ) × Rn (13.3.1) 
φ(T , x) = g(x) for all x ∈ Rn, (13.3.2) 
in various senses. 
To achieve this, we use the properties of two new constructs from nonsmooth 
analysis. 
Definition 13.3.1 Consider a set D ⊂ Rk, a function ϕ : D → R ∪ {+∞}, a point 
x ∈ dom ϕ and a vector d ∈ Rk. The lower Dini (directional) derivative of ϕ at x in 
the direction d ∈ Rk is defined to be: 
D↑ϕ(x; d) := lim inf
h↓0, e→d
h−1 [ϕ(x + he) − ϕ(x)] .
(For purposes of evaluating the limits on the right side, ϕ(x + he) is assigned the 
value +∞ when x + he /∈ D.) 
The following lemma, which provides alternative characterizations of the lower Dini 
derivative in the case D=[S,T ] × Rn and also the case when ϕ is locally Lipschitz 
continuous, is a straightforward consequence of the definitions and is stated without 
proof. (The differences are in the choice of sequences used to evaluate the ‘lim inf’.) 
Lemma 13.3.2 Take a function ϕ : [S,T ] × Rn → R ∪ {+∞}, a point (t, x) ∈
([S,T ] × Rn) ∩ dom ϕ and a vector v ∈ Rn. We have 
(i) D↑ϕ((t, x);(1, v))
= lim inf{h−1(ϕ(t + h, x + hw) − ϕ(t, x) : h ↓ 0, w → v},13.3 The Value Function and Generalized Solutions of the Hamilton Jacobi. . . 669
(ii) if ϕ is Lipschitz continuous on a neighbourhood of (t, x), then 
D↑ϕ((t, x);(1, v)) = lim inf{h−1(ϕ(t + h, x + hv) − ϕ(t, x)) : h ↓ 0},
(iii) if ϕ is Fréchet differentiable at (t, x) ∈ (S, T ) × Rn, then 
D↑ϕ((t, x); d) = ∇ϕ(t, x) · d, for all d ∈ Rn+1.
We now make precise two notions of ‘generalized solution’ to the Hamilton 
Jacobi equation (13.3.1)–(13.3.2), when the multifunction F of the underlying 
differential inclusion is possibly discontinuous with respect to time, but is required 
to have left and right limits: for each s ∈ [S,T ), t ∈ (S, T ] and x ∈ Rn, the 
following one-sided set-valued limits (in the Kuratowski sense) are non-empty: 
F (s+, x) := lim
s'
↓s
F (s'
, x) and F (t−, x) := lim
t'
↑t
F (t'
, x) . (13.3.3) 
Definition 13.3.3 A function φ : [S,T ]×Rn → R∪{+∞} is a lower Dini solution 
to (13.3.1)–(13.3.2) if 
(i) infv∈F (t+,x) D↑φ((t, x);(1, v)) ≤ 0
for all (t, x) ∈ ([S,T ) × Rn) ∩ dom V ,
(ii) supv∈F (t−,x) D↑φ((t, x);(−1, −v)) ≤ 0
for all (t, x) ∈ ((S, T ] × Rn) ∩ dom V ,
(iii) φ(T , x) = g(x) for all x ∈ Rn.
Definition 13.3.4 A lower semi-continuous function φ : [S,T ]×Rn → R∪ {+∞}
is a proximal solution to (13.3.1)–(13.3.2) if 
(i) for all (t, x) ∈ ((S, T ) × Rn) ∩ dom φ, (ξ 0, ξ 1) ∈ ∂P φ(t, x)
ξ 0 + inf v∈F (t+,x)
ξ 1 · v ≤ 0,
(ii) for all (t, x) ∈ ((S, T ) × Rn) ∩ dom φ, (ξ 0, ξ 1) ∈ ∂P φ(t, x)
ξ 0 + inf v∈F (t−,x)
ξ 1 · v ≥ 0,
(iii) for all x ∈ Rn, 
lim inf
{(t'
,x'
)→(S,x):t'
>S}
φ(t'
, x'
) = φ(S, x)
and 
lim inf
{(t'
,x'
)→(T ,x):t'
<T }
φ(t'
, x'
) = φ(T , x) = g(x).670 13 Dynamic Programming
These definitions are consistent with the classical notion of ‘solution’ to (13.3.1), in 
the case that t → F (t, x) is continuous, since any C1 function φ : [S,T ]×Rn → R
satisfying 
φt(t, x) + min v∈F (t,x) φx (t, x) · v = 0 for all (t, x) ∈ (S, T ) × Rn,
is automatically a lower Dini solution and also a proximal solution in the sense just 
defined. 
The following proposition relates these two solution concepts for the Hamilton 
Jacobi equation (13.3.1)–(13.3.2). 
Proposition 13.3.5 Take a lower semi-continuous function φ : [S,T ] ×Rn → R ∪
{+∞}. Concerning the data for problem (P), we assume that the (Kuratowski) limits 
(13.3.3) exist and are non-empty sets, for all x ∈ Rn, s ∈ (S, T ] and t ∈ [S,T ) and 
F (t+, x) is a compact set, for all (t, x) ∈ (S, T ) × Rn. Then 
‘φ is a lower Dini solution to (13.3.1)–(13.3.2)’ implies ‘φ is proximal solution 
to (13.3.1)–(13.3.2)’. 
Proof Let φ be a lower semi-continuous function which is also a lower Dini solution 
to (13.3.1)–(13.3.2). Take any x ∈ Rn. Then the condition 
φ(S, x) ≥ lim inf
t'
↓S,x'
→x
φ(t'
, x'
) (13.3.4) 
is certainly satisfied if (S, x) /∈ dom φ. Suppose that (S, x) ∈ dom φ. By 
assumption, F (S+, x) is non-empty. It follows from the definition of lower Dini 
solution that there exist a vector v ∈ F (S+, x) and sequences hi ↓ 0 and vi → v, 
vi ∈ Rn for all i, such that 
lim
i
h−1
i (φ(S + hi, x + hivi) − φ(S, x)) ≤ 0.
But then 
φ(S, x) ≥ lim
i φ(ti, xi),
where, for each i, ti = S + hi and xi = x + hivi. Thus (13.3.4) has been confirmed 
in this case too. Since φ is lower semi-continuous, the reverse inequality also holds. 
We conclude that 
φ(S, x) = lim inf
t'
↓S,x'
→x
φ(t'
, x'
).
A similar analysis, in which we take as starting point a vector v in the non-empty 
set F (T −, x) and use the second defining relation of lower Dini solutions, yields13.3 The Value Function and Generalized Solutions of the Hamilton Jacobi. . . 671
lim inf
t'
↑T ,x'
→x
φ(t'
, x'
) = φ(T , x) = g(x) .
Now suppose that (t, x) ∈ ((S, T ) × Rn) ∩ dom φ and (ξ 0, ξ 1) ∈ ∂P φ(t, x). This 
means that there exist M > 0 and ϵ > 0 such that 
ξ 0(t' − t) + ξ 1 · (x' − x) ≤ φ(t'
, x'
) − φ(t, x) + M(|t
' − t|
2 + |x' − x|
2)
for all (t'
, x'
) ∈ (t, x) + ϵB.
Since φ is a lower Dini solution of (13.3.1)–(13.3.2) and F (t+, x) is compact, there 
exist sequences hi ↓ 0 and {vi} in Rn such that vi → ¯v for some v¯ ∈ F (t+, x) and 
lim
i
h−1
i (φ(t + hi, x + hivi) − φ(t, x)) ≤ 0.
Setting (t'
, x'
) = (t + hi, x + hivi), we see that, for i sufficiently large, 
ξ 0 + ξ 1 · vi ≤ h−1
i (φ(t + hi, x + hivi) − φ(t, x)) + Mhi(1 + |vi|
2).
Since vi → ¯v and v¯ ∈ F (t+, x), we obtain, in the limit as i → ∞, that 
ξ 0 + inf v∈F (t+,x)
ξ 1 · v ≤ ξ 0 + ξ 1 · ¯v = ξ 0 + lim
i
ξ 1 · vi ≤ 0. (13.3.5) 
Now for any given v ∈ F (t−, x), since φ a lower Dini solution of (13.3.1)– 
(13.3.2), there exist sequences hi ↓ 0 and {vi} in Rn, such that vi → v and 
lim sup
i→∞
h−1
i (φ(t − hi, x − hi, vi) − φ(t, x)) ≤ 0.
According the definition of ∂P φ(t, x), setting (t'
, x'
) = (t −hi, x−hivi), we obtain 
− (ξ 0 + ξ 1 · vi) ≤ h−1
i (φ(t − hi, x − hi, vi) − φ(t, x)) + Mhi(1 + |vi|
2),
for i sufficiently large. Since vi → v, we deduce from these relations that 
ξ 0 + ξ 1 · v ≥ 0.
Since v was an arbitrary element in F (t−, x), we have shown that 
inf v∈F (t−,x)
( ξ 0 + ξ 1 · v ) ≥ 0,
The proof is concluded. ⨅⨆
The following proposition assembles some useful regularity properties about the 
value function:672 13 Dynamic Programming
Proposition 13.3.6 Assume that the data for (P) satisfies the following hypothe￾ses 
(i): F : [S,T ] × Rn ⇝ Rn takes closed non-empty values, F is L × Bn￾measurable, the graph of F (t, .) is closed for each t ∈ [S,T ], 
(ii): there exists c ∈ L1(S, T ) such that 
F (t, x) ⊂ c(t)(1 + |x|) B for all x ∈ Rn and for a.e. t ∈ [S,T ] ,
(iii): g : Rn → R ∪ {+∞} is lower semi-continuous. 
Then 
(a): Then V (t, x) > −∞ for all (t, x) ∈ [S,T ] × Rn, 
(b): If in addition F takes convex values, then V is lower semi-continuous, 
(c): If in addition, for every R0 > 0, there exists kF ∈ L1(S, T ) such that 
F (t, x'
) ⊂ F (t, x)+kF (t)|x−x'
| B for all x, x' ∈ R0B and a.e. t ∈ [S,T ]
and g is continuous, then V is continuous, 
(d): If in addition, for every R0 > 0, there exists kF ∈ L1(S, T ) and c0 > 0 such 
that 
F (t, x'
) ⊂ F (t, x)+kF (t)|x−x'
| B for all x, x' ∈ R0B and a.e. t ∈ [S,T ]
F (t, x) ⊂ c0 B for all (t, x) ∈ [S,T ] × R0B ,
and g is locally Lipschitz continuous, then V is locally Lipschitz continuous. 
Proof 
(a): Take any (t, x) ∈ [S,T ] × Rn. It follows from hypothesis (ii) that, for any 
state trajectory on y on [t,T ] such that y(t) = x, there exists a number r > 0, 
independent of y, such that |y(T )| ≤ r. But V (t, x) ≥ infx'
∈rB g(x'
). Since g :
Rn → R ∪ {+∞} is lower semi-continuous function, its infimum value on the 
compact set rB cannot take value −∞. It follows that V (t, x) > −∞. 
(b): Fix (t, x) ∈ [S,T ] × Rn. Then, by Proposition 6.4.1, we know that (Pt,x ) has 
a minimizer y. (We allow the possibly that (Pt,x ) has infinite cost). It follows that 
V (t, x) ( = g(y(T )) ) > −∞.
Now take any sequence {(ti, xi)} in [S,T ] × Rn, such that (ti, xi) → (t, x) and 
limi V (ti, xi) exists. To prove that V is lower semi-continuous we must show that 
V (t, x) ≤ lim
i
V (ti, xi).13.3 The Value Function and Generalized Solutions of the Hamilton Jacobi. . . 673
For each i, (Pti,xi) has a minimizer, which we write yi : [ti, T ] → Rn. Of course 
yi(ti) = xi. Extend yi to all [S,T ] by constant extrapolation from the right on 
[S,ti]. From hypothesis (ii) and Gronwall’s lemma (Lemma 6.2.4), all F trajectories 
having initial data in [S,T ] × (|x| + 1)B evolve in [S,T ] × R0B, where R0 :=
e
 T
S c(t) dt(|x| + 2). Observe that it is not restrictive to assume that xi ∈ (|x| +
1)B for all i. Then, we deduce that the yi’s are uniformly integrably bounded and 
their derivatives are uniformly integrably bounded. The compactness of trajectories 
theorem (Theorem 6.3.3) ensures that, following extraction of a subsequence, we 
have yi → y, uniformly, for some absolutely continuous arc whose restriction to 
[t,T ] is an F trajectory. Since the y˙i’s are uniformly integrably bounded, 
y(t) = lim
i yi(ti) = lim
i xi = x.
But then, 
V (t, x) ≤ g(y(T )) ≤ lim
i g(yi(T )) = lim
i
V (ti, xi).
It follows that V is lower semi-continuous. 
(c): Fix any r0 > 0. Notice that, from hypothesis (ii) and Gronwall’s lemma 
(Lemma 6.2.4), for any τ ∈ [S,T ] and any F trajectory x on [τ,T ] such that x(τ ) ∈
r0B we have: x(t) ∈ R0B for all t ∈ [t,T ], where R0 := exp{
 T
S c(t) dt}(r0 + 1). 
Take two points (t1, x1), (t2, x2) ∈ [S,T ] × Rn such that x1, x2 ∈ r0B. It is not 
restrictive to assume that t1 ≤ t2. Fix any ε > 0. Then there exists an F trajectory 
yε such that yε(t1) = x1 and 
V (t1, x1) ≥ g(yε(T )) − ε . (13.3.6) 
We know that |x2 − yε(t2)|≤|x1 − x2| + (1 + R0)
 t2
t1 c(s) ds. Consider the point 
(t2, yε(t2)). From the Filippov’s existence theorem (Theorem 6.2.3) there exists an 
F trajectory y2 on [t2, T ] such that y2(t2) = x2 and 
‖y2 − yε‖L∞(t2,T ) ≤ K |x2 − yε(t2)| ≤ K (|x1 − x2| + (1 + R0)
 t2
t1
c(s) ds) ,
(13.3.7) 
where K := e
 T
S kF (s) ds. Write ωg the modulus of continuity of g. Therefore, from 
(13.3.6) and (13.3.7) we have 
V (t2, x2) − V (t1, x1) ≤ g(y2(T )) − g(xε(T )) + ε
≤ ωg(|y2(T ) − xε(T )|) + ε
≤ ωg(K (|x1 − x2| + (1 + R0)
 t2
t1
c(s) ds)) + ε .674 13 Dynamic Programming
We can also find an F trajectory zε on [t2, T ] such that zε(t2) = x2 and 
V (t2, x2) ≥ g(zε(T )) − ε . (13.3.8) 
Consider now an F trajectory z1 on [t1, t2] (if t1 < t2) and extend it by means of 
Filippov’s existence theorem to an F trajectory on [t1, T ] still written z1 such that 
‖z1 − zε‖L∞(t2,T ) ≤ K |x2 − z1(t2)| ≤ K (|x2 − x1| + (1 + R0)
 t2
t1
c(s) ds) .
(13.3.9) 
So, arguing as above, we obtain the reverse inequality 
V (t1, x1) − V (t2, x2) ≤ ωg(K (|x1 − x2| + (1 + R0)
 t2
t1
c(s) ds)) + ε.
and, taking the limit as ε tends to zero, it follows that 
|V (t1, x1) − V (t2, x2)| ≤ ωg(K (|x1 − x2| + (1 + R0)
 t2
t1
c(s) ds)) .
Clearly this inequality is valid whenever we fix any x1 ∈ r0int B and take an 
arbitrary x2 ∈ r0int B in a neighbourhood of x1, implying the continuity of V at 
(t1, x1). This confirms assertion (b) of the proposition. 
(d): We employ the same argument used in (c), taking into account that g is local 
Lipschitz continuous and that F (t, x) ⊂ c0 B for all (t, x) ∈ [S,T ] × R0B. Write 
kg the Lipschitz constant of g on R0B. Then, we have 
|V (t1, x1) − V (t2, x2)| ≤ kg
√
2c0(1 + R0)K |(t1, x1) − (t2, x2)| .
⨅⨆
We are now ready to identify the value function as the unique lower semi￾continuous function that is a solution to the Hamilton Jacobi equation (13.3.1)– 
(13.3.2), in either the lower Dini or proximal sense. 
Theorem 13.3.7 (Characterization of Lower semi-continuous Value Functions) 
Assume that the data for (P) satisfies the following hypotheses: 
(H1): F : [S,T ] × Rn ⇝ Rn takes closed, convex, non-empty values, F (., x) is 
L(S, T )-measurable for all x ∈ Rn, 
(H2): (i) there exists c ∈ L1(S, T ) such that 
F (t, x) ⊂ c(t)(1 + |x|) B for all x ∈ Rn and for a.e. t ∈ [S,T ] ,
and13.3 The Value Function and Generalized Solutions of the Hamilton Jacobi. . . 675
(ii) for every R0 > 0, there exists c0 > 0 such that 
F (t, x) ⊂ c0 B for all (t, x) ∈ [S,T ] × R0B ,
(H3): for every R0 > 0, there exist kF ∈ L1(S, T ) and a modulus of continuity 
ωF : R+ → R+ such that t → 1/(1 + kF (t)) is almost everywhere equal to a 
continuous function, 
(i) F (t, x'
) ⊂ F (t, x) + kF (t)|x − x'
| B,
for all x, x' ∈ R0B and a.e. t ∈ [S,T ], 
and 
(ii) dH (F (t, x'
), F (t, x)) ≤ ωF (|x − x'
|)
for all x, x' ∈ R0B for all t ∈ [S,T ], 
(H4): (i) for each s ∈ [S,T ), t ∈ (S, T ] and x ∈ Rn the following one-sided 
set-valued limits exist and are non-empty: 
F (s+, x) := lim s'
↓s, x'
→x
F (s'
, x'
) and F (t−, x) := lim t'
↑t,x'
→x
F (t'
, x'
) ,
(ii) and there exists a subset S ⊂ (S, T ) of full Lebesgue measure such that, 
s → F (s, x) is continuous at s = t, for all t ∈ S and x ∈ Rn.
(H5): g : Rn → R ∪ {+∞} is lower semi-continuous. 
Take a function V : [S,T ] × Rn → R ∪ {+∞}. Then, assertions (a)–(c) below are 
equivalent: 
(a) V is the value function for (P), 
(b) V is lower semi-continuous on [S,T ] × Rn and 
(i) for all (t, x) ∈ ([S,T ) × Rn) ∩ dom V
inf v∈F (t+,x)
D↑V ((t, x);(1, v)) ≤ 0,
(ii) for all (t, x) ∈ ((S, T ] × Rn) ∩ dom V
sup v∈F (t−,x)
D↑V ((t, x);(−1, −v)) ≤ 0,676 13 Dynamic Programming
(iii) for all x ∈ Rn
V (T , x) = g(x).
(c) V is lower semi-continuous on [S,T ] × Rn and 
(i) for all (t, x) ∈ ((S, T ) × Rn) ∩ dom V , (ξ 0, ξ 1) ∈ ∂P V (t, x)
ξ 0 + inf v∈F (t+,x)
ξ 1 · v ≤ 0,
(ii) for all (t, x) ∈ ((S, T ) × Rn) ∩ dom V , (ξ 0, ξ 1) ∈ ∂P V (t, x)
ξ 0 + inf v∈F (t−,x)
ξ 1 · v ≥ 0,
(iii) for all x ∈ Rn, 
lim inf
{(t'
,x'
)→(S,x):t'
>S}
V (t'
, x'
) = V (S, x)
and 
lim inf
{(t'
,x'
)→(T ,x):t'
<T }
V (t'
, x'
) = V (T , x) = g(x).
Proof The proof structure will be to demonstrate ‘(a) ⇒ (b)’, ‘(b) ⇒ (c)’ and ‘(c) 
⇒ (a)’. In Proposition 13.3.5 it has already been shown that ‘(b) ⇒ (c)’. It remains 
to supply the missing links: ‘(a) ⇒ (b)’ and ‘(c) ⇒ (a)’. The key step of the proof is 
‘(c) ⇒ (a)’. This involves showing that, for an arbitrary point (t, x) in the domain of 
a function V satisfying condition (c), (A): V (t, x) is the cost of some state trajectory 
originating from (t, x) and (B): V (t, x) is a lower bound on the cost of an arbitrary 
state trajectory. To demonstrate these properties of V we use a weak invariance 
theorem to construct the state trajectory x with property (A) and a strong invariance 
theorem to confirm the lower bound property (B) of V . 
‘(a) ⇒ (b)’ 
The lower semicontinuity of V follows from Proposition 13.3.6. Take any point 
(t, x) ∈ dom V ∩ ([S,T ) × Rn). Choose r0 > 0 such that r0 > |x| + 1. 
Consider a minimizer y for (Pt,x ), which exists under the stated hypotheses. 
Observe also that by hypothesis (H2) y(s) ∈ R0B, for all s ∈ [t,T ], where 
R0 := exp{
 T
S c(s)ds}(r0 + 1). Invoking the principle of optimality for the value 
function, we have: 
V (t + δ, y(t + δ)) − V (t, x) = 0 (13.3.10)13.3 The Value Function and Generalized Solutions of the Hamilton Jacobi. . . 677
for all δ ∈ (0, T − t]. Using the fact that y is an F trajectory, we also have 
δ−1(y(t + δ) − y(t)) = δ−1
 t+δ
t
y(s)ds ˙
for all δ ∈ (0, T − t], and 
y(s) ˙ ∈ F (s, y(s)) a.e. s ∈ [t,T ] .
In view of (H4), this implies 
y(s) ˙ ∈ F (s+, y(s)) a.e. s ∈ [t,T ] .
Now take an arbitrary sequence δi ↓ 0. For each i we write 
vi := δ−1
i
 t+δi
t
y(s)ds. ˙
Then, condition (H2)(ii) implies that y is Lipschitz continuous, and therefore {vi} is 
a bounded sequence. Extracting a subsequence (we do not relabel), we can arrange 
that 
vi → ¯v as i → ∞
for some v¯ ∈ Rn. Fix any p ∈ Rn, and observe that 
p · vi = δ−1
i
 t+δi
t
p · ˙y(s) ds ≤ δ−1
i
 t+δi
t
max v∈F (s+,y(s))
p · v ds .
Noting that s → maxv∈F (s+,y(s)) p · v is a right-continuous function (at s = t), we 
can pass to the limit as i → ∞ in this relation to deduce: 
p · ¯v ≤ max v∈F (t+,x)
p · v . (13.3.11) 
But (13.3.11) is valid for all p ∈ Rn and F (t+, x) is convex. It follows that v¯ ∈
F (t+, x). 
For each i, we have y(t + δi) = x + δivi. Then, from (13.3.10), we deduce that 
δ−1
i [V (t + δi, x + δivi) − V (t, x)] = 0.
Since vi → ¯v and v¯ ∈ F (t+, x), we obtain that 
inf v∈F (t+,x)
D↑V ((t, x), (1, v)) ≤ lim
i
δ−1
i [V (t + δi, x + δivi) − V (t, x)] = 0 ,678 13 Dynamic Programming
confirming condition (b)(i). 
Now, fix any (t, x) ∈ dom V ∩ (S, T ] × Rn. Choose any v˜ ∈ F (t−, x). Take 
a sequence δi ↓ 0 such that δi < |t − S| for all i. Set y(s) := x + (s − t)v˜
for all s ∈ [S,t]. In consequence of the generalized Filippov existence theorem 
(Theorem 6.2.3), there exists, for each i sufficiently large, an F trajectory zi on 
[t − δi, t] such that zi(t) = x and 
|zi(t − δi) − [x − δiv˜]| ≤ K
 t
t−δi
dF (s,y(s))(v) ds ˜ for all i,
in which K := exp{
 T
S kF (s) ds} is a number that does not depend on i. Then, 
| ˜v − vi| ≤ Kδ−1
i
 t
t−δi
dF (s,y(s))(v) ds , ˜
where 
vi := δ−1
i (x − zi(t − δi)) for each i.
Since F (t, .) is continuous (from (H3)) and v˜ ∈ F (t−, x), it follows that 
lim
s↑t
dF (s,y(s))(v)˜ = 0 .
Then, we deduce that vi → ˜v as i → ∞. Applying the principle of optimality, 
V (t − δi, x − δivi) ≤ V (t, zi(t)) for all i ,
an so 
δ−1
i (V (t − δi, x − δivi) − V (t, x)) ≤ 0 for all i .
This condition yields 
D↑V ((t, x), (−1, − ˜v)) (≤ lim sup
i→∞
δ−1
i (V (t − δi, x − δivi) − V (t, x))) ≤ 0 .
Since v˜ was an arbitrary vector in F (t−, x), we deduce that 
sup v∈F (t−,x)
D↑V ((t, x), (−1, −v)) ≤ 0 ,
confirming (b)(ii). 
‘(c) ⇒ (a)’13.3 The Value Function and Generalized Solutions of the Hamilton Jacobi. . . 679
Suppose that V is a lower semi-continuous function that satisfies conditions c(i)– 
(iii) of the theorem statement. We shall show that V is the value function. We shall 
do so initially under the assumption that (H2) and (H3) have been replaced by the 
more restrictive hypotheses 
(H2)'
: there exists c0 > 0 such that F (t, x) ⊂ c0 B for all (t, x) ∈ [S,T ] × Rn,
(H3)'
: there exist kF ∈ L1(S, T ) such that t → 1/(1 + kF (t)) is almost 
everywhere equal to a continuous function and 
F (t, x'
) ⊂ F (t, x) + kF (t)|x − x'
| B
for all x, x' ∈ Rn and a.e. t ∈ [S,T ] .
We shall show that it suffices to assume (H2) and (H3) at a later stage of the proof. 
Take an arbitrary function V : [S,T ] × Rn → R ∪ {+∞} satisfying condition 
(c) and an arbitrary point (t ,¯ x)¯ ∈ [S,T ) × Rn. Our aim is to prove that 
V (t ,¯ x)¯ = inf(Pt
¯,x¯).
Step 1 We show that 
V (t ,¯ x)¯ ≥ inf(Pt ,¯ x¯) .
To prove this inequality, it suffices to find an F trajectory x¯ on [t,T ¯ ] which satisfies 
x(t)¯ = ¯x, and 
V (t ,¯ x)¯ ≥ g(x(T )) . ¯
We can assume that V (t ,¯ x) < ¯ +∞, since otherwise the inequality above is 
automatically satisfied. Define the multifunction � : [S,T ] × Rn+1 ⇝ Rn+2 to be 
�(τ, x, a) :=
⎧
⎪⎪⎨
⎪⎪⎩
co ({(0, 0, 0)} ∪ ({1} × F (S+, x) × {0})) if τ = S
{1} × co {F (τ −, x) ∪ F (τ +, x)}×{0} if τ ∈ (S, T )
co ({(0, 0, 0)} ∪ ({1} × F (T −, x) × {0})) if τ = T .
Consider the following differential inclusion and state constraint: 
(S)
⎧
⎨
⎩
(τ ,˙ x,˙ a)˙ ∈ �(τ, x, a) a.e. t ∈ [t,T ¯ ]
(τ (t), x(t), a(t)) ∈ epi V for all t ∈ [t,T ¯ ]
(τ (t), x( ¯ t), a( ¯ t)) ¯ = (t ,¯ x,V ( ¯ t ,¯ x)). ¯
Our intention is to apply the autonomous weak invariance theorem (Theorem 13.2.1) 
to this system. But first we must check that the relevant hypotheses are satisfied. 
Assumptions (H1), (H2)'
, (H3)' and (H4) guarantee that Gr � is closed at every680 13 Dynamic Programming
point of D := epi V , � takes values non-empty convex sets and satisfies 
�(τ, x, a) ⊂ (c0 + 1) B for all (τ, x, a) ∈ [S,T ] × Rn × R .
(Here, c0 is as in (H2)'
.) It remains to show that the ‘inward pointing condition’ is 
satisfied. Take any (τ, x, α) ∈ epi V and any ξ ∈ NP
epi V (τ, x, α). By the nature of 
proximal normal vectors to epigraph sets, ξ = (ξ 0, ξ 1, −λ), for some λ ≥ 0. We 
must show 
min w∈�(τ,x,α)(ξ 0, ξ 1, −λ) · w ≤ 0 . (13.3.12) 
Observe that, if τ = S or τ = T , (13.3.12) is automatically true, since 
min w∈�(τ,x,α) ξ · w ≤ ξ · (0, 0, 0) = 0 .
So we can assume that S<τ<T . Note that we need to check (13.3.12) only 
when α = V (τ, x). This is because, in the case α > V (τ, x), ‘(ξ 0, ξ 1, −λ) ∈
NP
epi V (τ, x, α)’ implies λ = 0 and (ξ 0, ξ 1, 0) ∈ NP
epi V (τ, x, V (τ, x)). (These 
facts are simple consequences of definition of the proximal normal cone and of 
the structure of epigraph sets.) We shall show: 
ξ 0 + min v∈F (t−,x)∪F (τ+,x)
ξ 1 · v ≤ 0 . (13.3.13) 
This implies 
min w∈�(τ,x,α)(ξ 0, ξ 1, −λ) · w ≤ ξ 0 + min v∈co {F (τ−,x)∪F (τ+,x)}
ξ 1 · v ≤ 0 .
which is the inward pointing condition (13.3.12). To check (13.3.13) we need to 
consider two cases. 
Case 1: λ > 0. In this case ((1/λ)ξ 0, (1/λ)ξ 1, −1) ∈ NP
epi V ((τ, x), V (τ, x)). It 
follows that 
((1/λ)ξ 0, (1/λ)ξ 1) ∈ ∂P V (τ, x) .
But then, by (c)(i), (1/λ) 

ξ 0 + minv∈F (τ+,x) ξ 1 · v
 ≤ 0 . This implies (13.3.13). 
Case 2: λ = 0. In this case, (ξ 0, ξ 1) ∈ ∂∞
P V (t, x) and we know from 
Theorem 4.6.2 that there exist (ξ 0
i , ξ 1
i ) → (ξ 0, ξ 1), λi ↓ 0 and (ti, xi) → (t, x)
such that, for each i, 
(λ−1
i ξ 0
i , λ−1
i ξ 1
i ) ∈ ∂P V (ti, xi) .13.3 The Value Function and Generalized Solutions of the Hamilton Jacobi. . . 681
Then, by condition (c)(i), for each i there exists vi ∈ F (t+
i , xi) such that 
ξ 0
i + ξ 1
i · vi. ≤ 0 .
But {vi} is a bounded sequence. We can therefore arrange, by extracting a 
subsequence we do not relabel), that vi → v, for some v ∈ Rn. Since (t, x) ⇝
F (t−, x) ∪ F (t+, x) is an upper semi-continuous multifunction, it follows that 
v ∈ F (t−, x) ∪ F (t+, x). So, in the limit as i → ∞, we obtain 
0 ≥ ξ 0 + ξ 1 · v ≥ ξ 0 + inf v∈F (t−,x)∪F (t+,x)
ξ 1 · v.
We have confirmed (13.3.13) in this case too. We are justified then in applying the 
autonomous weak invariance Theorem 13.2.1 to system (S). 
Suppose first that t>S ¯ . We deduce the existence of a � trajectory (τ, x, a) on 
[t,T ¯ ] such that (τ (t), x(t), a(t)) ∈ epi V , for all t ∈ [t,T ¯ ]. Taking note of the fact 
t>S ¯ (this condition excludes the possibility that τ ≡ S) and also (H4)(ii), we 
deduce from the structure of the multifunction � that x is a F trajectory, τ (t) ≡ t
and a is a constant (we write it also a) such that and a ≥ V (t, x(t)), for all t ∈ [t,T ¯ ]. 
Then, since a(t)¯ = V (t ,¯ x)¯ , we have 
g(x(T )) ¯ = V (T , x(T )) ¯ ≤ ¯a(T ) = ¯a(t)¯ = V (t ,¯ x) . ¯
If, on the other hand, t
¯ = S, we can, in view of (c)(iii), take a sequence (Si, xi) →
(S, x)¯ such that Si ↓ S and limi→∞ V (Si, xi) = V (S, x)¯ . Repeating the preceding 
analysis, we show, for each i ≥ 0, that there exists an F trajectory yi such that 
V (Si, xi) ≥ g(yi(T )) . (13.3.14) 
Extending yi to all [S,T ] by constant extrapolation from the right on [S,Si], with 
the help of the compactness of trajectories theorem (Theorem 6.3.3) we can arrange, 
by extracting a suitable subsequence, that yi → ¯y uniformly, for some F trajectory 
y¯ such that y(¯ t)¯ = ¯x. Since g is lower semi-continuous, we may pass to the limit as 
i → ∞ in the preceding relation and deduce that 
V (S, x)¯ = lim inf
i→∞ V (Si, xi) ≥ lim inf
i→∞ g(yi(T )) ≥ g(y(T )) . ¯
This completes step 1. 
Step 2 We now prove that 
V (t ,¯ x)¯ ≤ inf(Pt
¯,x¯) .
Take an arbitrary F trajectory y on [t,T ¯ ] such that y(t)¯ = ¯x. We shall establish that 
V (t ,¯ x)¯ ≤ g(y(T )) .682 13 Dynamic Programming
We can assume, without loss of generality, that g(y(T )) < +∞ for, otherwise, the 
inequality above holds automatically. 
Define 
y(s) ˜ := y(T − s), F (s, x) ˜ := −F (T − s, x) and V (s, x) ˜ := V (T − s, x)
for (s, x) ∈ [0, T − t
¯] × Rn. It is a straightforward matter to check that y˜ satisfies 
 ˙
y(s) ˜ ∈ F (s, ˜ y(s)) ˜ a.e. t ∈ [0, T − t
¯]
y(˜ 0) = y(T ).
We readily deduce from the fact that V satisfies condition (c)(ii) that 
ξ 0 + inf v∈F (t−,x)
ξ 1 · v ≥ 0, (13.3.15) 
for all (t, x) ∈ ((S, T ) × Rn) ∩ dom V , and all (ξ 0, ξ 1) ∈ ∂P V (t, x). Condition 
(c)(iii) implies that 
V (˜ 0, y(˜ 0)) = lim inf s'
↓0,x'
→ ˜y(0)
V (s ˜ '
, x'
). (13.3.16) 
Now define the multifunctions �˜ : [0, T −t
¯] ×Rn+1 ⇝ Rn+1 and P˜ : [0, T −t
¯] ⇝
Rn+1 to be 
�(s, (z, a)) ˜ := F (s, z) ˜ × {0},
P (s) ˜ := {(x, α) : V (s, x) ˜ ≤ α}
respectively. We shall apply the strong invariance theorem (Theorem 13.2.4) to the 
differential inclusion and accompanying constraint 
⎧
⎨
⎩
(z,˙ a)˙ ∈ �(s, (z, a)) ˜ a.e s ∈ [0, T − t
¯],
z(0) = y(T ), a(0) = V (˜ 0, y(˜ 0)),
(z(s), a(s)) ∈ P (s) ˜ for all s ∈ [0, T − t
¯].
(The boundary condition on a makes sense, since V (˜ 0, y(˜ 0)) ( = g(y(T ))) <
+∞.) To do so, we need first to check that the relevant hypotheses are satisfied. 
According to (13.3.16), we can find si ↓ 0, xi → y(T ) such that V (s ˜ i, xi) →
V (˜ 0, y(T )). Writing ai := V (s ˜ i, xi), we see that (xi, ai) → (y(T ), V (˜ 0, y(T )))
and (si, (xi, ai)) ∈ P (s ˜ i) for all i. This confirms that 
(z(0), a(0)) ∈ lim sup
s'
↓0
P (s ˜ '
).
�˜ and P˜ both clearly possess the required closure and regularity properties for 
application of the strong invariance theorem (Theorem 13.2.4).13.3 The Value Function and Generalized Solutions of the Hamilton Jacobi. . . 683
It remains to check the ‘inward pointing’ hypothesis. With this objective in mind, 
we consider any point (s, (z, α)) ∈ Gr P˜ such that 0 <s<T − S, and any vector 
(ζ 0, ζ 1, −λ) ∈ NP
Gr P˜(s, (z, α)). (13.3.17) 
We must show that 
ζ 0 +

maxw∈�(s ˜ −,(z,α)) ζ 1 · v

∧

maxw∈�(s ˜ +,(z,α)) ζ 1 · w

≤ 0
for all (ζ 0, ζ 1, −λ) ∈ NP
Gr P˜(s, (z, α)).
Taking note of the structure of �˜ , this last condition can be expressed as 
ζ 0 +

max
v˜∈F (s ˜ −,z)
ζ 1 · ˜v

∧

max
v˜∈F (s ˜ +,z)
ζ 1 · ˜v

≤ 0. (13.3.18) 
Since Gr P˜ is an epigraph set, λ ≥ 0. We deduce from (13.3.17) that 
(ζ 0, ζ 1, −λ) ∈ NP
epi V˜(s, z, α).
As noted at the similar stage of the analysis in Step 1, we need to check (13.3.18) 
only when α = V (s, z) ˜ . We observe also that (ζ 0, ζ 1, −λ) ∈ NP
epi V˜(s, z, a) if and 
only if (−ζ 0, ζ 1, −λ) ∈ NP
epi V (T − s, z, a). It follows that (13.3.18) is equivalent 
to the following condition: for every (t, z) ∈ (t,T ) ¯ × Rn and (ξ 0, ξ 1, −λ) ∈
NP
epi V (t, z, V (t, z)) (with λ ≥ 0) we have: 

max v˜∈−F (t−,z)
(−ξ 0 + ξ 1 · ˜v)
∧

max v˜∈−F (t+,z)
(−ξ 0 + ξ 1 · ˜v)
≤ 0 .
which is equivalent to 

min v∈F (t−,z)
(ξ 0 + ξ 1 · v)
∨

min v∈F (t+,z)
(ξ 0 + ξ 1 · v)
≥ 0 . (13.3.19) 
Consider first the case λ > 0. Then λ−1(ξ 0, ξ 1) ∈ ∂P V (t, z) and so, from condition 
c(ii), 
ξ 0 + min v∈F (t−,z)
ξ 1 · v ≥ 0 ,
which implies (13.3.18). If λ = 0, there exist (ξ 0
i , ξ 1
i ) → (ξ 0, ξ 1), λi ↓ 0 and 
(ti, zi) → (t, z) such that, for each i, 
(λ−1
i ξ 0
i , λ−1
i ξ 1
i ) ∈ ∂P V (ti, zi) .684 13 Dynamic Programming
But then, by condition (c)(ii), 
ξ 0
i + min v∈F (t−
i ,zi)
ξi · v ≥ 0 .
By extracting a subsequence we can arrange that, either ti ≤ t for all i, or ti > t for 
all i. Since (ti, zi) → (t, z) and in consequence of Hypothesis (H4)(i), we can pass 
to the limit as i → ∞ in the preceding relation to obtain 
ξ 0 + min v∈F (t−,z)
ξ 1 · v ≥ 0
if ti ≤ t for all i. Other other hand, if ti > t for all t, passage to the limit gives 
ξ 0 + min v∈F (t+,z)
ξ 1 · v ≥ 0 .
In either case then, (13.3.19) is verified. 
The strong invariance theorem (Theorem 13.2.4) tells us that the �˜ trajectory 
(y, a ˜ ≡ V (˜ 0, y(˜ 0))) satisfies 
(y(s), a(s)) ˜ ∈ P (s) ˜ for all s ∈ [0, T − t
¯].
Setting s = T − t
¯, we deduce that 
V (˜ 0, 0˜) = a(0) = a(T − t)¯ ≥ V (T ˜ − t ,¯ y(T ˜ − t)). ¯
Since V (T , y(T )) = V (˜ 0, y(˜ 0)) and V (t, y( ¯ t)) ¯ = V (T ˜ − t ,¯ y(T ˜ − t)) ¯ , it follows 
that 
g(y(T )) = V (T , y(T )) ≥ V (t, y( ¯ t)) ¯ = V (t ,¯ x). ¯
This is the desired inequality and completes Step 2. 
Conclusion of Proof Steps 1 and 2 above combine to tell us that 
V (t ,¯ x)¯ = inf(Pt ,¯ x¯) (13.3.20) 
at the arbitrary point (t ,¯ x)¯ ∈ [S,T ] × Rn. This confirms ‘(c) ⇒ (a)’, under the 
added hypotheses (H2)' and (H3)'
. 
To validate (13.3.20) when either (H2)' or (H3)' are not satisfied, we choose 
r0 > | ¯x| and consider the multifunction 
F (s, y) ˜ := 
F (s, y) if |y| ≤ R0
F (s, R0y/|y|) if |y| > R0 ,13.4 Local Verification Theorems 685
in which R0 = exp{
 T
S c(t)dt}(1 + r0). The multifunction F˜ satisfies hypotheses 
(H1), (H2)'
, (H3)' and (H4). The preceding analysis tell us that V (t, x) is the 
infimum cost for the modified dynamic optimization problem, in which F˜ replaces 
F, and for initial data (t, x), where V is any lower semi-continuous function that 
satisfies conditions (c)(i)–(c)(iii) (in relation to the multifunction F˜), restricted to 
[S,T ]×R0B. But, by standard a priori estimates, the F trajectories emanating from 
(t, x) also evolve in [S,T ] × R0B. It follows that the families of state trajectories 
are the same for the two multifunctions F and F˜. From this we conclude that, for 
arbitrary initial data (t, x) ∈ [S,T ] × r0B, the infimum costs are the same for the 
two multifunctions. Also the classes of lower semi-continuous functions satisfying 
hypotheses (c)(i)–(c)(iii) on [S,T ] × R0B are the same, because F and F˜ coincide 
on this set. It follows that V (t, x) is the minimum cost for the original dynamic 
optimization problem (for an initial data point (t, x) ∈ [S,T ]×r0B), where V is any 
lower semi-continuous function satisfying conditions (c)(i)–(c)(iii) (for the original 
multifunction F). It follows that (13.3.20) is valid, since (t ,¯ x)¯ ∈ [S,T ] × r0B. 
⨅⨆
13.4 Local Verification Theorems 
A traditional role of the Hamilton Jacobi equation in variational analysis has been 
to provide sufficient conditions of optimality. Consider the dynamic optimization 
problem 
(Q)
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize g(x(T ))
over arcs x ∈ W1,1([S,T ]; Rn) satisfying
x(t) ˙ ∈ F (t, x(t)) a.e.,
x(S) = x0,
x(T ) ∈ C,
the data for which comprise: a function g : Rn → R, a subinterval [S,T ] ⊂ R,a 
multifunction F : [S,T ] × Rn ⇝ Rn, a point x0 ∈ Rn and a closed set C ⊂ Rn. 
It is convenient at this stage to adopt a formulation in which the terminal cost 
function g is assumed to be finite valued and constraints on terminal values of state 
trajectories are expressed as a set inclusion x(T ) ∈ C. 
Take an arc x ∈ W1,1 satisfying the constraints of problem (Q). A smooth local 
verification function for x¯ is a C1 function φ : [S,T ]×Rn → R with the properties: 
there exists δ > 0 such that 
φt(t, x) + min v∈F (t,x) φx (t, x) · v ≥ 0 for all (t, x) ∈ int T (x, δ) ¯
φ(T , x) ≤ g(x) for all x ∈ C ∩ (x(T ) ¯ + δB)
φ(0, x0) = g(x(T )). ¯686 13 Dynamic Programming
Here T (x, δ) ¯ is the δ tube about x¯: 
T (x, δ) ¯ := 
(t, x) ∈ [S,T ] × Rn : |x − ¯x(t)| ≤ δ

.
Existence of a smooth verification function for x¯ is a sufficient condition for 
x¯ to be an L∞ local minimizer for (Q). It is of interest to know how broad the 
class of dynamic optimization problems is, for which the local optimality of local 
minimizers can be confirmed by some verification function. Inverse verification 
theorems, which provide conditions for the existence of verification functions 
associated with a minimizer address this issue. 
Unfortunately continuously differentiable verification theorems provide a rather 
restrictive framework for inverse verification theorems. For dynamic optimization 
problems whose solutions exhibit ‘bang bang’ behaviour indeed, non-existence of 
continuously differentiable verification functions is to be expected. 
Clearly, we need a less restrictive concept of smooth local verification functions, 
which can still be used in sufficient conditions of optimality, but whose existence 
can be guaranteed under unrestrictive hypotheses. 
These considerations lead to the following definition, which makes reference to 
the data for problem (Q) above and an arc x¯ ∈ W1,1([S,T ] × Rn). 
Definition 13.4.1 A function φ : T (x, δ) ¯ → R ∪ {+∞} is called a lower semi￾continuous local verification function for x¯ (with parameter δ > 0) if φ is lower 
semi-continuous and the following conditions are satisfied: 
(i) For every (t, x) ∈ (int T (x, δ)) ¯ ∩ dom φ such that ∂P φ(t, x) is non-empty, 
ξ 0 + inf v∈F (t−,x)
ξ 1 · v ≥ 0 for all (ξ 0, ξ 1) ∈ ∂P φ(t, x),
(ii) φ(T , x) ≤ g(x) for all x ∈ C,
(iii) lim inft'
↑T ,x'
→x φ(t'
, x'
) = φ(T , x) for all x ∈ C ∩ (x(T ) ¯ + δB),
(iv) φ(S, x0) = g(x(T )) ¯ . 
The next proposition tells us that sufficient conditions for local minimality can 
be formulated in terms of lower semi-continuous local verification functions. Fur￾thermore, the existence of a lower semi-continuous verification function confirming 
optimality of a given local minimizer is guaranteed, under unrestrictive hypotheses 
on the data. 
Proposition 13.4.2 Let the arc x¯ ∈ W1,1([S,T ]; Rn) satisfy the constraints of 
problem (Q). Assume that, for some ϵ > 0, we have 
(i) F takes values non-empty, closed, convex sets, and F (., x) is L(S, T )-
measurable for all x ∈ Rn, 
(ii) There exists c > 0 such that 
F (t, x) ⊂ cB for all x ∈ ¯x(t) + δB, a.e. t ∈ [S,T ],13.4 Local Verification Theorems 687
(iii) there exist kF ∈ L1(S, T ) and a modulus of continuity ωF : R+ → R+ such 
that t → 1/(1 + kF (t)) is almost everywhere equal to a continuous function, 
F (t, x'
) ⊂ F (t, x) + kF (t)|x − x'
| B
for all x, x' ∈ ¯x(t) + δB, for a.e. t ∈ [S,T ] ,
and 
dH (F (t, x'
), F (t, x)) ≤ ωF (|x − x'
|)
for all x, x' ∈ ¯x(t) + δB, and for all t ∈ [S,T ] ,
(iv) for each s ∈ [S,T ) and t ∈ (S, T ] the following one-sided set-valued limits 
exist and are non-empty: 
F (s+, x) := lim s'
↓s, x'
→x
F (s'
, x'
) for all x ∈ ¯x(s) + δB and
F (t−, x) := lim t'
↑t,x'
→x
F (t'
, x'
) for all x ∈ ¯x(t) + δB ,
and there exists a subset S ⊂ (S, T ) of full Lebesgue measure such that 
s → F (s, x) is continuous at s = t, for all t ∈ S and x(t) ¯ + δB,
(v) g is lower semi-continuous on x(T ) ¯ + ϵB. 
We have 
(a) Suppose that there exists a lower semi-continuous local verification function for 
x¯ (with parameter δ > 0). Then x¯ is an L∞ local minimizer for (Q). 
Conversely, 
(b) Suppose that x¯ is an L∞ local minimizer for (Q) and that g is bounded on 
x(T ) ¯ +ϵB. Then there exists a lower semi-continuous local verification function 
for x¯. 
Proof 
(a): Suppose that there exists a lower semi-continuous local verification function 
for x¯ (with parameter δ). Reduce the size of δ if necessary, to ensure that δ<ϵ. 
Let x be any F trajectory satisfying the constraints for (Q) and such that ||x −
x¯||L∞ ≤ δ/2. It is required to show that 
g(x(T )) ≥ g(x(T )). ¯
Apply the strong invariance theorem to the differential inclusion688 13 Dynamic Programming
 (z,˙ a)˙ ∈ −F (T − s, z) × {0} a.e. s ∈ [0, T − S]
(z(0), a(0)) = (x(T ), φ(T , x(T )))
and the accompanying constraint 
(z(s), a(s)) ∈ {(z, α) ∈ Rn × R : φ(T − s, z) ≤ α and |z − ¯x(T − s)| ≤ δ}.
This gives 
φ(S, x0) ≤ φ(T , x(T )).
The arguments involved are almost identical to those employed in the proof of 
Theorem 13.3.7 (see step 2 of ‘(c) ⇒ (a)’). But then 
φ(S, x0) ( ≤ φ(T , x(T )) ) ≤ g(x(T )).
Since φ(S, x0) = g(x(T )) ¯ , x¯ is an L∞ local minimizer as claimed. 
(b): Now suppose that x¯ is an L∞ local minimizer. Choose ϵ1 ∈ (0, ϵ) such that x¯
is a minimizer with respect to arcs x satisfying the constraints of (Q) together with 
||x − ¯x||L∞ ≤ ϵ1. Choose ϵ' ∈ (0, ϵ1). Now consider the dynamic optimization 
problem (P )˜ (this involves a constant k, whose value will be set presently): 
(P )˜
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize g(x(T )) ˜ + ky(T )
over arcs (x, y) ∈ W1,1([S,T ]; Rn+1) satisfying
(x(t), ˙ y(t)) ˙ ∈ F (t, x(t)) ˜ × {L(t, x(t)) ˜ } a.e.,
(x(S), y(S) = (x0, 0),
(x(T ), y(T )) ∈ C × R.
Here g˜ and F˜ are ‘localized’ versions of g and F, namely 
g(x) ˜ := g(x(T ) ¯ + trϵ1 (x − ¯x(T ))), F (t, x) ˜ := F (t, x(t) ¯ + trϵ1 (x − ¯x(t)))
and 
L(t, x) ˜ := max{|x − ¯x(t)| − ϵ'
, 0}
in which 
trα(z) := 
z if |z| ≤ α
αz/|z| if |z| > α.
Choose constants κ > 0 and k > 0 which satisfy13.4 Local Verification Theorems 689
κ > max{|g(x)| : x ∈ ¯x(T ) + ϵ1B},
k > |ϵ1 − ϵ'
|
−1 |ϵ1 + ϵ'
|
−116cκ .
Claim: x¯ is a global minimizer for (P˜). 
To verify this, take any other arc satisfying the constraints of (P˜). It must be shown 
that: 
g(x(T )) ˜ + k
 T
S
L(t, x(t))dt ˜ ≥ ˜g(x(T )) ¯ + k
 T
S
L(t, ˜ x(t))dt ( ¯ = g(x(T ))). ¯
There are two cases to consider: 
(A): ||x − ¯x||L∞ < ϵ1. In this case, F (t, x(t)) ˜ = F (t, x(t)) for all t ∈ [S,T ] and 
g(x(T )) ˜ = g(x(T )). So x is actually an F trajectory. But then, by local optimality 
of x¯, we have 
g(x(T )) ˜ + k
 T
S
L(t, x(t))dt ˜
≥ g(x(T )) ≥ g(x(T )) ¯ = ˜g(x(T )) ¯ + k
 T
S
L(t, ˜ x(t))dt, ¯
as required. 
(B): There exists t
¯ ∈ [S,T ] such that 
|x(t)¯ − ¯x(t)¯ | = ϵ1.
In this case, since t → |x(t) − ¯x(t)| is Lipschitz continuous with Lipschitz constant 
2c and |x(S) − ¯x(S)| = 0, we have 
meas{t | |x(t) − ¯x(t)| ≥ (ϵ' + ϵ1)/2} ≥ (ϵ1 − ϵ'
)/(4c) .
But then 
g(x(T )) ˜ + k
 T
S
L(t, x(t))dt ˜
= ˜g(x(T )) + k
 T
S
max{|x(t) − ¯x(t)| − ϵ'
, 0}dt
≥ −κ + (k/8c)(ϵ1 + ϵ'
)(ϵ1 − ϵ'
)
> κ ≥ g(x(T )). ¯
The inequality holds in this case too then; the claim is confirmed. 
Now let V (t, x, y) ˜ be the value function for (P˜). Since (P˜) is, in effect, a 
reformulation of an dynamic optimization problem with end-point and integral690 13 Dynamic Programming
cost terms as an dynamic optimization problem with end-point cost term alone, 
V (t, x, y) ˜ can be expressed 
V (t, x, y) ˜ = φ(t, x) + y
for some function φ : [S,T ] × Rn → R ∪ {+∞}. 
Notice that F ≡ F˜ and L˜ ≡ 0 on T (x,ϵ ¯ '
) and g˜ ≡ g on x(T ) ¯ + ϵ'
B. Now 
we use information about value functions supplied by Theorem 13.3.7, in which the 
terminal constraint functional is taken to be the extended valued function 
g(x(T )) ˜ + ky(T ) + ΨC(x(T )).
We deduce that φ is lower semi-continuous and, for every (t, x) ∈ (int T (x,ϵ ¯ '
)) ∩
dom φ, such that ∂P φ(t, x) is non-empty, 
ξ 0 + inf v∈F (t−,x)
ξ 1 · v ≥ 0 for all (ξ 0, ξ 1) ∈ ∂P V (t, x)
φ(T , x) = g(x) for all x ∈ (x(T ) ¯ + ϵ'
B) ∩ C,
lim inf
t'
↑T ,x'
→x
φ(t'
, x'
) = φ(T , x) for all x ∈ C ∩ (x(T ) ¯ + ϵ'
B).
Finally, since x¯ is a minimizer for (P˜), 
φ(S, x0) = g(x(T )). ¯
We have confirmed that there exists some lower semi-continuous, local verification 
function for x¯, namely φ. ⨅⨆
In many cases, when a candidate local verification function φ is constructed from 
a field of extremals, on some subset E of [S,T ] × Rn, φ is Lipschitz continuous 
and continuously differentiable on a open subset of E of full Lebesgue measure. 
The following proposition serves as a convenient aid to verify that φ is indeed a 
local verification function: we have merely to check that φ is a classical solution 
of the Hamilton Jacobi equation at each point on a neighbourhood of which it is 
continuously differentiable. 
Proposition 13.4.3 Consider problem (Q) involving the data g,F, x0 and C. Take 
an admissible F trajectory x¯ and a multifunction E : [S,T ] ⇝ Rn such that x(t) ¯ ∈
E(t), for all t ∈ [S,T ]. Assume that 
(i) there exists c > 0 such that 
F (t, x) ⊂ cB for all (t, x) ∈ Gr E ,
(ii) there exists a function ψ : [S,T ] × Rn → R that is Lipschitz continuous on 
bounded sets and, for some δ > 0, satisfies13.4 Local Verification Theorems 691
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
∇tψ(t, x) + min v∈F (t,x)
{∇xψ(t, x) · v} ≥ 0
for a.e. point (t, x) ∈ [S,T ] × Rn at which
ψ is differentiable and x ∈ E(t) + δB,
ψ(T , x) = g(x) for all x ∈ C ,
(13.4.1) 
(iii) ψ(S, x0) = g(x(T )) ¯ . 
Then, for any admissible F trajectory x on [S,T ] such that x(S) = x0 and x(t) ∈
E(t) for all t ∈ [S,T ], we have 
g(x(T )) ≥ g(x(T )) . ¯ (13.4.2) 
Proof Take any F trajectory x on [S,T ] such that x(S) = x0 and x(t) ∈ E(t) for 
all t ∈ [S,T ]. From (i) we know that the F trajectory x is Lipschitz continuous 
on [S,T ] and, therefore, remains confined in a ball R0B (for some radius R0 > 0). 
Owing to (ii) ψ is Lipschitz continuous on the bounded set [S,T ] ×R0B. It follows 
that t → ψ(t, x(t)) is a Lipschitz continuous function on [S,T ], and, so, it is a.e. 
differentiable and 
ψ(S, x(S)) = ψ(T , x(T )) −
 T
S
(d/dt)ψ(t, x(t))dt .
Now define N ⊂ (S, T ) to be the subset comprising points t in (S, T ) such at 
d/dt ψ(t, x(t)) exists, dx(t)/dt exists, x(t) ˙ ∈ F (t, x(t)) and t is a Lebesgue point 
of x˙. The subset N has full Lebesgue measure. 
Take any t ∈ N and hi ↓ 0. Then, for each i sufficiently large, there exists, by 
Lebourg’s two-sided mean value theorem (Theorem 4.5.3), λi ∈ [0, 1] and 
(ζ 0
i , ζ 1
i ) ∈ co ∂ψ(t + λihi, x(t) + λi(x(t + hi) − x(t)))
such that 
h−1
i [ψ(t + hi, x(t + hi)) − ψ(t, x(t))] = (ζ 0
i , ζ 1
i ) · (1, h−1
i
 t+hi
t
x(s)ds) . ˙
(13.4.3) 
The sequence {(ζ 0
i , ζ 1
i )} is bounded, since ψ is Lipschitz continuous on bounded 
sets. We can therefore arrange, by extracting a subsequence, that, for some (ζ 0, ζ 1), 
(ζ 0
i , ζ 1
i ) → (ζ 0, ζ 1), as i → ∞.
By the upper semicontinuity properties of the Clarke subdifferential co ∂ψ, we know 
that (ζ 0, ζ 1) ∈ co ∂ψ(t, x(t)) .692 13 Dynamic Programming
Since the left side of (13.4.3) converges to (d/dt) ψ(t, x(t)) and t is a Lebesgue 
point of x˙, we deduce from (13.4.3) that 
(d/dt)ψ(t, x(t)) = lim
h↓0
h−1[ψ(t +h, x(t +h))−ψ(t, x(t))] = (ζ 0, ζ 1)·(1, x(t)) . ˙
(13.4.4) 
In consequence of Theorem 4.7.7, there exist, for k = 0, 1,...,n + 1, αk ∈ [0, 1]
and a sequence of points (t k
j , xk
j ) → (t, x(t)), at each point along which ψ is 
differentiable, such that n+1
k=0 αk = 1 and, as j → ∞, 

n+1
k=0
(∇tψ(t k
j , xk
j ), ∇xψ(t k
j , xk
j ))
· (1, x(t)) ˙ → (ζ 0, ζ 1) · (1, x(t)) . ˙
Since x(t) ˙ ∈ F (t, x(t)), it follows from our assumptions concerning ψ that 
∇tψ(t k
j , xk
j ) + ∇xψ(t k
j , xk
j ) · ˙x(t) ≥ 0 for each j .
The preceding two relations yield, in the limit as j → ∞, the information 
(ζ 0, ζ 1) · (1, x(t)) ˙ ≥ 0 .
But then, from (13.4.4), (d/dt)ψ(t, x(t)) ≥ 0. Since this relation is true for all t’s 
in a set of full measure, it follows that 
ψ(S, x(S)) = ψ(T , x(T )) −
 T
S
d/dt ψ(t, x(t))dt ≤ g(x(T )) + 0.
Bearing in mind that this relation is valid for any admissible F trajectory and noting 
that, by assumption (iii) ψ(S, x(S)) = g(x(T )) ¯ , we deduce that 
g(x(T )) ¯ ≤ g(x(T )).
This concludes the proof. ⨅⨆
Proposition 13.4.2 tells us that an L∞ local minimizer can, in principle, be 
confirmed as such by some lower semi-continuous, local verification function. There 
follows an inverse verification theorem which aims to simplifying the task of finding 
a local verification function. It gives conditions under which the search can be 
confined to the class of functions which are Lipschitz continuous on a tube about 
the putative minimizer x¯. An important point is that we can establish existence of 
Lipschitz continuous local verification functions, even in the presence of an end￾point constraint 
x(T ) ∈ C.13.4 Local Verification Theorems 693
In such cases, Lipschitz continuous functions are typically distinct from the value 
function V for the original problem. (If, for example an L∞ local minimizer x¯ is 
such that x(T ) ¯ ∈ ∂C, then the value function will take value +∞ at some points 
in any tube about x¯ and so certainly cannot serve as a Lipschitz continuous local 
verification function.) 
The conditions under which there exists a Lipschitz continuous local verification 
function include a constraint qualification, requiring that necessary conditions of 
optimality, in the form of the Hamiltonian inclusion, are ‘normal’ at the locally 
minimizing arc x¯ under examination. 
(CQ) For any p ∈ W1,1([S,T ]; Rn) and λ ≥ 0 such that 
⎧
⎨
⎩
||p||L∞ + λ /= 0,
p(t) ˙ ∈ co {−q : (q, ˙
x(t)) ¯ ∈ ∂H (t, x(t), p(t)) ¯ } a.e.,
−p(T ) ∈ λ∂g(x(T )) ¯ + NC(x(T )), ¯
we have λ /= 0. 
Here, as usual, H (t, x, p) is the Hamiltonian 
H (t, x, p) = max
v∈F (t,x) p · v.
Theorem 13.4.4 (Existence of Lipschitz Continuous Verification Functions) 
Let the arc x¯ ∈ W1,1([S,T ]; Rn) satisfy the constraints of problem (Q). Assume 
that, for some ϵ > 0, 
(i) F takes values non-empty, closed, convex sets, and F (., x) is L(S, T )-
measurable for all x ∈ Rn, 
(ii) There exists c > 0 such that 
F (t, x) ⊂ cB for all x ∈ ¯x(t) + δB, a.e. t ∈ [S,T ],
(iii) there exist kF ∈ L1(S, T ) and a modulus of continuity ωF : R+ → R+ such 
that t → 1/(1 + kF (t)) is almost everywhere equal to a continuous function, 
F (t, x'
) ⊂ F (t, x) + kF (t)|x − x'
| B
for all x, x' ∈ ¯x(t) + δB, for a.e. t ∈ [S,T ] ,
and 
dH (F (t, x'
), F (t, x)) ≤ ωF (|x − x'
|)
for all x, x' ∈ ¯x(t) + δB, and for all t ∈ [S,T ] ,694 13 Dynamic Programming
(iv) for each s ∈ [S,T ) and t ∈ (S, T ] the following one-sided set-valued limits 
exist and are non-empty: 
F (s+, x) := lim s'
↓s, x'
→x
F (s'
, x'
) for all x ∈ ¯x(s) + δB and
F (t−, x) := lim t'
↑t,x'
→x
F (t'
, x'
) for all x ∈ ¯x(t) + δB ,
and there exists a subset S ⊂ (S, T ) of full Lebesgue measure such that 
s → F (s, x) is continuous at s = t, for all t ∈ S and x(t) ¯ + δB,
(v) g is Lipschitz continuous on x(T ) ¯ + ϵB. 
We have 
(a) If there exists a Lipschitz continuous local verification function for x¯, then x¯ is 
an L∞ local minimizer for (Q), 
(b) If x¯ is an L∞ local minimizer and (CQ) is satisfied, then there exists a Lipschitz 
continuous local verification function for x¯. 
Proof 
(a): This is, of course, just a special case of Proposition 13.4.2, already proved. 
(b): Let x¯ be an L∞ local minimizer for (Q). We shall construct a Lipschitz 
continuous local verification function for x¯. 
Choose a constant ϵ¯ ∈ (0, ϵ), such that x¯ is a minimizer with respect to all arcs 
satisfying the constraints of (Q) and also ||x − ¯x||L∞ ≤ ¯ϵ. Let κ be a constant such 
that |g(x)| ≤ κ for x ∈ ¯x(T ) + ¯ϵB. Take any sequence ϵi ↓ 0 such that ϵi ∈ (0, ϵ)¯
for all i. Take also sequences ki ↑ +∞, Ki ↑ +∞ such that 
ki > 64cκϵ−2
i for each i and Ki/ki → ∞ as i → ∞.
For each i, define 
Li(t, x) := max{|x − ¯x(t)| − ϵi/2, 0}.
Define also 
g(x) ˜ := g(x(T ) ¯ + trϵ¯(x − ¯x(T )), F (t, x) ˜ := F (t, x(t) ¯ + trϵ¯(x − ¯x(t))
For each i, consider the dynamic optimization problem (P˜
i):13.4 Local Verification Theorems 695
(P˜
i)
⎧
⎪⎪⎨
⎪⎪⎩
Minimize g(x(T )) ˜ + kiz(T ) + KidC(x(T ))
over arcs (x, z) ∈ W1,1([S,T ]; Rn+1) satisfying
(x(t), ˙ z(t)) ˙ ∈ F (t, x(t)) ˜ × {Li(t, x(t))} a.e.
(x(S), z(S)) = (x0, 0).
We shall show presently that, 
(x, z(t) ¯ ≡ 0) is a minimizer for (Pi0 ) (13.4.5) 
for some index value i0. 
Completion of the proof is then a straightforward matter. Indeed, let V (t, x, z)
be the value function for (P˜
i0 ). Clearly 
V (t, x, z) = φ(t, x) + z
for some function φ : [S,T ] × Rn → R ∪ {+∞}. We also note that 
F (t, x) ˜ = F (t, x) and L(t, x) = 0 for (t, x) ∈ T (x,ϵ ¯ i0 /2)
and 
g(x) ˜ + Ki0 dC(x) = g(x) for x ∈ C ∩ (x(T ) ¯ + ϵi0 /2B).
The information about value functions supplied by Theorem 13.3.7, applied to 
V (t, x, z), tells us that φ is a local verification function for (Pi0 ). (Pi0 ) has a 
Lipschitz continuous terminal cost function, however, and there is no right end￾point constraint. It follows from Proposition 13.3.6 that V (and therefore φ also) is 
locally Lipschitz continuous. This is what we set out to prove. 
It remains then to confirm (13.4.5). Here we make use of a contradiction 
argument. Suppose that, for each i, x¯ is not a minimizer for (Pi). 
Under the hypotheses it is known that, for each i, (Pi) has a minimizer: write it 
(xi, t → ki
 t
S
Li(s, xi(s))ds).
Because it is assumed that x¯ is not a minimizer for (Pi), we have 
g(x ˜ i(T )) + ki
 T
S
Li(t, xi(t))dt + KidC(xi(T )) < g(˜ x(T )) ¯ + 0 + 0 = g(x(T )). ¯
(13.4.6) 
We now note two important properties of xi: 
||xi − ¯x||L∞ ≤ ϵi (13.4.7) 
xi(T ) ∈/ C. (13.4.8)696 13 Dynamic Programming
Indeed, if (13.4.7) is not true, then there exists t
¯ ∈ [S,T ] such that |xi(t)¯ − ¯x(t)¯ | =
ϵi. Since t → |xi(t) − ¯x(t)| is Lipschitz continuous with Lipschitz constant 2c, it 
follows that 
meas{t ∈ [S,T ]:|xi(t) − ¯x(t)| > (3/4)ϵi} ≥ ϵi/(8c).
But then 
g(x ˜ i(T )) + ki
 T
S
Li(t, xi(t))dt + KidC(xi(T ))
> −κ + kiϵ2
i /(32c) > κ ≥ g(x(T )), ¯
in contradiction of (13.4.6). So (13.4.7) is true. 
Suppose (13.4.8) is not true. Then xi satisfies the constraints of (Q) and also 
||xi − ¯x|| ≤ ¯ϵ. So, by local optimality of x¯, 
g(xi(T )) ≥ g(x(T ). ¯
But then, in view of (13.4.6), 
g(x ˜ i(T )) + ki
 T
S
Li(t, xi(t))dt + KidC(xi(T ))
> g(x ˜ i(t)) + 0 + 0 = g(xi(T )) ≥ g(x(T )). ¯
This contradicts (13.4.6). So (13.4.8) is true. 
Now apply the necessary conditions of Theorem 8.7.2 (the Hamiltonian inclusion 
for convex velocity sets) to problem (P˜
i), with reference to the L∞ local minimizer 
xi. These provide a costate arc pi ∈ W1,1([S,T ]; Rn) and λi ≥ 0 satisfying: 
||pi||L∞ + λi = 1,
p(t) ˙ ∈ co {−q | (q, x˙i(t)) ∈ ∂H (t, xi(t), pi(t))
+λiki/Kico ∂L(t, xi(t))} a.e.,
−pi(T ) ∈ λi(1/Ki)∂g(x ˜ i(T )) + λi∂dC(xi(T )).
Here, H (t, x, p) is the Hamiltonian for the original problem. Notice that, before 
applying the necessary conditions, we have scaled the cost by (1/Ki). 
Since xi(T ) /∈ C, 
∂dC(xi(t)) ∈ {ξ ∈ Rn : |ξ | = 1}.13.5 Costate Trajectories and Gradients of the Value Function 697
We deduce that 
|pi(T )| ≥ λi(1 − kg/Ki).
(kg is a Lipschitz constant for g on x(T ) ¯ + ¯ϵB.) 
A by now familiar convergence analysis can be used to justify passing to the limit 
in the above relations: along a subsequence, pi → p uniformly and λi → α, for 
some p ∈ W1,1([S,T ]; Rn) and α ≥ 0, satisfying 
||p||L∞ + α = 1, (13.4.9)
p(t) ˙ ∈ co {−q | (q, ˙
x(t)) ¯ ∈ ∂H (t, x(t), p(t)) ¯ } a.e.,
−p(T ) ∈ α∂dC(x(T )) ¯ ⊂ NC(x(T )), ¯
||p||L∞ ≥ α.
Notice that p /= 0 since, if p = 0, then α = 0 which contradicts (13.4.9). We have 
exhibited a costate trajectory violating the constraint qualification (CQ). It follows 
that the original claim, namely that x¯ is minimizer for (Pi) for some i, is true. The 
proof is complete. ⨅⨆
13.5 Costate Trajectories and Gradients of the Value 
Function 
In this section we again consider the dynamic optimization problem of Sect. 13.1: 
(P )
⎧
⎪⎪⎨
⎪⎪⎩
Minimize g(x(T ))
over arcs x ∈ W1,1([S,T ]; Rn) satisfying
x(t) ˙ ∈ F (t, x(t)) a.e.,
x(S) = x0,
the data for which comprise a function g : Rn → R, an interval [S,T ] ⊂ R,a 
multifunction F : [S,T ] × Rn ⇝ Rn and a vector x0 ∈ Rn. 
Note, however, that g is taken to be finite valued. End-point constraints are 
therefore excluded. 
Let x¯ be a minimizer. Then, under appropriate hypotheses on the data, the 
necessary conditions of Theorem 8.4.3 assert the existence of a costate trajectory 
p ∈ W1,1([S,T ]; Rn) such that 
p(t) ˙ ∈ co {q : (q, p(t)) ∈ NGr F (t,.)(x(t), ¯ ˙
x(t)) ¯ } a.e.,
p(t) · ˙
x(t) ¯ = h(t) a.e.,
−p(T ) ∈ ∂g(x(T )), ¯698 13 Dynamic Programming
where 
h(t) := max v∈F (t,x(t)) ¯ p(t) · v .
(Since there are no right end-point constraints, we are justified in assuming that the 
cost multiplier is λ = 1.) 
Let V be the value function for (P). We shall invoke hypotheses under which V
is locally Lipschitz continuous. Earlier discussion leads us to expect that the costate 
trajectory p (and h) will be related to V according to 
(h(t), −p(t)) ∈ co ∂V (t, x(t)) ¯ a.e. t ∈ (S, T ). (13.5.1) 
(To be more precise, we would expect this inclusion to hold for some kind of subd￾ifferential of V ; with hindsight we adopt the convexified limiting subdifferential.) 
The aim of this section is to confirm the ‘sensitivity relation’ (13.5.1). Before 
entering into the details of the arguments involved, we briefly examine two 
different approaches which hold out hope of simpler proof techniques, if only 
to dismiss them. The first is to take an arbitrary selector of the multifunction 
t → co∂V (t, x(t)) ¯ , partitioned as (h, −p), such that p is absolutely continuous, 
and attempt to show that it is a costate trajectory. The other is to take an arbitrary 
costate trajectory and Hamiltonian (evaluated along this costate trajectory and state 
trajectory x¯) and attempt to show that they satisfy the sensitivity relation (13.5.1). 
Their appeal is that they involve simply checking the properties of a plausible 
candidate for a costate trajectory. The inadequacies of these approaches, at least 
for problems with nonsmooth data, are made evident by the following example, 
which reveals that, on the one hand, a pair of functions (h, −p) chosen from the 
set of selectors for t → co∂V (t, x(t)) ¯ , in which p is absolutely continuous, may 
fail to generate costate trajectories and, on the other, costate trajectories may fail to 
generate a pair of functions that satisfy the sensitivity relation. 
Example 
⎧
⎨
⎩
Minimize g(x(1)) over x ∈ W1,1([0, 1]; R) satisfying
x(t) ˙ ∈ {x(t)u : u ∈ [0, 1]},
x(0) = 0,
where 
g(ξ ) := 
−ξ if ξ > 0
−e0.5ξ if ξ ≤ 0 .
The only admissible arc for this problem is x(t) ¯ ≡ 0. This then is the minimizer. 
The value function V is easily calculated:13.5 Costate Trajectories and Gradients of the Value Function 699
V (t, ξ ) =

−e(1−t)ξ if ξ > 0
−e0.5ξ if ξ ≤ 0 .
Evidently 
co ∂V (t, 0) = {0} × co {−e0.5, −e1−t
} .
The Hamiltonian is 
H (t, x, p) = max{px, 0}.
For this problem a costate trajectory is any absolutely continuous function p which 
satisfies 
− ˙p(t) = α(t)p(t) a.e. t ∈ [0, 1] (13.5.2) 
and 
p(1) ∈ [1, e0.5], (13.5.3) 
for some measurable function α : [0, 1]→[0, 1]. 
Notice that p ≡ e0.5 is a costate trajectory (it corresponds to the choice α ≡ 0) 
which satisfies 
(H (t, x(t), p(t)), ¯ −p(t)) ∈ co ∂V (t, x(t)) ¯ for all t ∈ [0, 1].
So, for this example there is a costate trajectory satisfying the anticipated relation. 
However, p1 ≡ 1 is also a costate trajectory (it too corresponds to the choice 
α ≡ 0) with the property that 
(H (t, x(t), p ¯ 1(t)), −p1(t)) /∈ co ∂V (t, x(t)) ¯ for all t ∈ [0, 1).
This means that a costate trajectory exists which, on a set of full measure, fails to 
satisfy the sensitivity relation. 
On the other hand, for any number ω > 0, p2 given by 
p2(t) = e0.5 + (e1−t − e0.5)sin(ωt)
is a continuously differentiable function which satisfies 
(H (t, x(t), p ¯ 2(t)), −p2(t)) ∈ co ∂V (t, x(t)) ¯ for all t ∈ [0, 1].
We note however that 
dp2/dt (0) = ωe0.5(e0.5 − 1).700 13 Dynamic Programming
By Gronwall’s lemma (Lemma 6.2.4), if p2 is also a solution to (13.5.2) and (13.5.3) 
then p2(0) ≤ e1.5. Hence p˙2(0) ≤ e1.5. It follows that p2 cannot be a costate 
trajectory if ω > e(e0.5 − 1)−1. 
We have shown that there also exists an absolutely continuous function which, 
combined with associated function h, satisfies the sensitivity relation but which is 
not a costate trajectory. 
In order to pick out a special costate trajectory which, with its associated function 
h, satisfies the sensitivity relation, even in situations such as that described in the 
above example, we make use of quite different techniques, the flavour of which we 
now attempt to convey. The underlying ideas become more transparent when we 
switch to problems having a traditional controlled differential equation formulation 
and when we assume that the value function V is a C1 solution to the Hamilton 
Jacobi equation. Consider for the time being then: 
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize g(x(T )) over x ∈ W1,1([S,T ]; Rn)
and measurable functions u : [S,T ] → Rn satisfying
x˙ = f (t, x(t), u(t)) a.e. [S,T ],
u(t) ∈ U a.e. [S,T ],
x(S) = x0,
where [S,T ] is a given time interval, g : Rn → R and f : [S,T ] ×Rn ×Rm → Rn
are given functions, U ⊂ Rm is a given set and x0 ∈ Rn is a given point. 
Let (x,¯ u)¯ be a minimizer. Under appropriate hypotheses, the value function 
satisfies 
Vt(t, ξ ) + Vx (t, ξ ) · f (t, ξ , u) ≥ 0 (13.5.4)
for all (t, ξ ) ∈ (S, T ) × Rn and u ∈ U ,
and V (T , x) = g(x) for all x ∈ Rn. Also, 
Vt(t, x(t)) ¯ + Vx (t, x(t)) ¯ · f (t, x(t), ¯ u(t)) ¯ = 0 a.e. t ∈ [S,T ]. (13.5.5) 
Let u : [S,T ] → Rm, v : [S,T ] → Rn and w : [S,T ] → R be arbitrary 
measurable functions and x be an absolutely continuous arc such that 
x(t) ˙ = (1 + w(t))(f (t, x(t), u(t)) + v(t)) a.e.
(u(t), w(t), v(t)) ∈ U × B × [−1, +1] a.e..
Inserting (ξ , u) = (x(t), u(t)) into (13.5.4), multiplying across the inequality by 
the nonnegative number (1+w(t)) and adding and subtracting terms, we obtain the 
inequality13.5 Costate Trajectories and Gradients of the Value Function 701
Vt(t, x(t)) + Vx (t, x(t)) · (f (t, x(t), u(t)) + v(t))(1 + w(t))
+ η(t, x(t), v(t), w(t)) ≥ 0 a.e..
Here, η is the function 
η(t, x, v, w) := −(1 + w)Vx (t, x) · v + wVt .
Noting that the first two terms on the left of this inequality can be written 
dV (t, x(t))/dt and also the boundary condition V (T , .) = g, we deduce that 
J (x, u, v, w) ≥ 0,
in which J is defined to be the functional 
J (x, u, v, w) :=
g(x(T )) − V (S, x(S)) +
 T
S
η(t, x(t), v(t), w(t))dt.
Similar reasoning applied to (13.5.5) gives 
J (x,¯ u,¯ v¯ ≡ 0, w¯ ≡ 0) = 0.
We deduce that (x,¯ u,¯ v¯ ≡ 0, w¯ ≡ 0) is an L∞ local minimizer for the optimization 
problem: 
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize g(x(T )) − V (S, x(S)) +  T
S η(t, x(t), v(t), w(t))dt
over x ∈ W1,1 and measurable functions (u, v, w) : [S,T ] → Rm+n+1
satisfying
x(t) ˙ = (1 + w(t))(f (t, x(t), u(t)) + v(t)) a.e.,
(u(t), v(t), w(t)) ∈ U × B × [−1, +1] a.e..
This is an dynamic optimization problem to which the maximum principle is 
applicable, with reference to the L∞ local minimizer (x, ( ¯ u,¯ v¯ ≡ 0, w¯ ≡ 0)). It 
turns out that the a costate trajectory p for this derived problem associated with 
the cost multiplier λ = 1 (we can make such a choice of cost multiplier since the 
dynamic optimization problem has free right end-point) is also a costate trajectory 
for the original dynamic optimization problem. But the Weierstrass condition for 
the derived problem gives us the extra information that, for almost every t, 
d(t, v(t) ¯ = 0, w(t) ¯ = 0) = max{d(t, v, w) : |v| ≤ 1, |w| ≤ 1},
where702 13 Dynamic Programming
d(t, v, w) := (1 + w)[p(t) · f (t, x(t), ¯ u(t)) ¯ − Vt t (t, x(t)) ¯ ]
+ (1 + w)[p(t) + Vx (t, x(t)) ¯ ] · v.
In particular, d(t, 0, 0) ≤ d(t, 0, w) for every w ∈ [−1, 1] and d(t, 0, 0) ≤
d(t, v, 0) for every v ∈ B, from which we conclude that 
(p(t) · f (t, x(t), ¯ u(t)), ¯ −p(t)) = (Vt(t, x(t)), V ¯ x (t, x(t))) ¯ a.e..
This is precisely the sensitivity relation. 
What we have done is to modify the problem so that (x,¯ u)¯ becomes a minimizer 
with respect to a richer class of controls. It is reasonable to expect that necessary 
conditions of optimality for the derived problem will convey more information about 
(x,¯ u)¯ than those for the original problem; the extra information is the sensitivity 
relation. 
The reader will be justifiably sceptical at this stage as to whether these elementary 
arguments can be modified to handle situations where V is possibly nonsmooth, 
since then the cost integrand in the derived problem, which involves derivatives of 
V , may be discontinuous with respect to the state variable and altogether unsuitable 
for application of the standard first order necessary conditions. We shall get round 
this difficulty by replacing the offending function η(t, x, v, w) by an approximation 
which does not depend on the state variable at all. 
We are ready to prove 
Theorem 13.5.1 (Sensitivity Relations) Let x¯ be an L∞ local minimizer for (P). 
Assume that the following hypotheses are satisfied: 
(a): F takes values non-empty, closed sets, F is L×Bn-measurable, and F (., x) is 
continuous from the right on [S,T ), for each x ∈ Rn, 
(b): there exists c > 0 such that 
F (t, x) ⊂ c(1 + |x|)B for all (t, x) ∈ [S,T ] × Rn,
(c): there exists kF ∈ L1(S, T ) such that 
F (t, x) ⊂ F (t, x'
) + kF (t)|x − x'
|B, for all x, x' ∈ Rn, a.e. t ∈ [S,T ] ,
(d): g is locally Lipschitz continuous. 
Let V be the value function for problem (P). Then there exists p ∈ W1,1([S,T ]; Rn)
such that 
(i): p(t) ˙ ∈ co {ξ : (ξ , p(t)) ∈ NGr F (t,.)(x(t), ¯ ˙
x(t)) ¯ } a.e. t ∈ [S,T ], 
(ii): p(t) · ˙
x(t) ¯ = maxv∈F (t,x(t)) ¯ p(t) · v a.e. t ∈ [S,T ], 
(iii): −p(T ) ∈ ∂g(x(T )) ¯ , 
(iv): p(S) ∈ ∂x (−V )(S, x(S)) ¯ .13.5 Costate Trajectories and Gradients of the Value Function 703
Furthermore, 
(H (t, x(t), p(t)), ¯ −p(t)) ∈ co ∂V (t, x(t)) ¯ a.e. t ∈ [S,T ]. (13.5.6) 
Remark 
(a): Notice that the sensitivity relation (13.5.6) implies 
− p(t) ∈ Projx co ∂V (t, x(t)) ¯ a.e. t ∈ [S,T ], (13.5.7) 
where Projx denotes ‘projection onto the x block coordinate’. It is possible 
to show [35], though we do not do so here, that condition (13.5.6) can be 
supplemented by the (partial) sensitivity relation: 
− p(t) ∈ co ∂xV (t, x(t)) ¯ a.e. t ∈ [S,T ]. (13.5.8) 
Conditions (13.5.6) and (13.5.8) are distinct necessary conditions since, in 
some situations, 
Projx {∂V (t, x)} /⊂ ∂0
xV (t, x) .
For instance for f : R+ × R → R given by f (t, x) = |x + 1 − t|−|x|
we have f (1, x) = 0 for all x and therefore ∂xf (1, 0) = {0}. On the other 
hand, ∂0f (1, 0) is equal to the convex hull of the set {±(1, 0), ±(1, −2)}. Thus, 
Projx {∂f (1, 0)} /⊂ ∂xf (1, 0). 
(b): The assertions of Theorem 13.5.1 remain valid (with possibly different p) 
if we replace the Euler Lagrange inclusion (i) with the partially convexified 
Hamiltonian inclusion: 
(i)' p(t) ˙ ∈ co {ξ : (−ξ , ˙
x(t)) ¯ ∈ ∂x,pH (t, x(t), p(t)) ¯ } for a.e. t ∈ [S,T ]. 
To show this, we note that if F is replaced by its convex hull co F the 
hypotheses invoked in Theorem 13.5.1 are still valid. Furthermore, x¯ remains 
a minimizer and the value function is unaffected by the change. Now apply 
Theorem 13.5.1 with co F in place of F. This yields the assertions of the 
theorem (possibly with a different p), except that, now, co F appears in place of 
F in condition (i). But this modified condition implies the partially convexified 
Hamiltonian inclusion, in consequence of the dualization Theorem 8.7.1 of 
Chap. 8. 
Proof According to Proposition 13.3.6, V is a locally Lipschitz continuous func￾tion. For ϵ ∈ (0, 1) we define Gϵ : [S,T ] ⇝ R1+n and σϵ : R × Rn × R → R to 
be 
Gϵ (t) := {(α, β) : R1+n : (α, β) ∈ co ∂V (s, y)
for some (s, y) ∈ ((t, x(t)) ¯ + ϵB) ∩ ((S, T ) × Rn)}704 13 Dynamic Programming
and 
σϵ (t, v, w) := sup
(α,β)∈Gϵ (t)
(α, β) · (w, −(1 + w)v). (13.5.9) 
It is a straightforward matter to check that, for fixed ϵ ∈ (0, 1), σϵ is bounded on 
bounded sets, σϵ (., v, w) is measurable for all (v, w) ∈ Rn × R, and (by known 
properties on limiting subdifferentials and support functions) σϵ (t, ., .) is locally 
Lipschitz continuous for each t ∈ [S,T ]. 
Now consider the following dynamic optimization problem: 
(Pϵ )
⎧
⎪⎪⎨
⎪⎪⎩
Minimize J (x(S), x(T ), y(T ))
over arcs (x, y, v, w) ∈ W1,1([S,T ]; Rn+1+n+1) satisfying
(x(t), ˙ y(t), ˙ v(t), ˙ w(t)) ˙ ∈ F˜
ϵ (t, x(t)), a.e. t ∈ [S,T ]
(x(S), y(S), v(S), w(S)) ∈ Rn × {0}×{0}×{0}
in which 
F˜
ϵ (t, x) := {((e + v)(1 + w), σϵ (t, v, w), a, b) : e ∈ F (t, x), a ∈ ϵB, b ∈ ϵB}
and 
J (x0, x1, y1) := g(x1) − V (S, x0) + y1.
We verify: 
Claim: For any ϵ ∈ (0, 1), (x,¯ y¯ ≡ 0, v¯ ≡ 0, w¯ ≡ 0) is an L∞ local minimizer for 
(Pϵ ). 
To confirm the claim, take any arc (x, y, v, w) satisfying the constraints of (Pϵ )
and such that ||(x, y, v, w) − (x,¯ y,¯ v,¯ w)¯ ||L∞ ≤ ϵ/2. Notice to begin with that at 
every point t ∈ (S, T ) which is a Lebesgue point of s → ˙x(s) and which is a 
differentiability point of the Lipschitz continuous function s → V (s, x(s)) (such 
points comprise a set of full measure) we have 
d/dtV (t, x(t)) = lim
h↓0
h−1[V (t + h, x +
 t+h
t
x(s)ds) ˙ − V (t, x)],
= lim
h↓0
h−1[V (t + h, x + ˙x(t)h) − V (t, x)]
≥ D↑V ((t, x(t));(1, x(t))). ˙ (13.5.10) 
Since (x, y, v, w) satisfies the constraints of (Pϵ ), it can be deduced from the 
measurable selection theorem that there exist measurable functions e, a and b such 
that, for all points t ∈ (S, T ) in a set of full measure, we have13.5 Costate Trajectories and Gradients of the Value Function 705
y(t) ˙ = σϵ (t, v(t), w(t)),
x(t) ˙ = (e(t) + v(t))(1 + w(t)),
e(t) ∈ F (t, x(t)),
|v(t)|, |w(t)| ≤ ϵ|T − S|.
Since V is locally Lipschitz continuous, we deduce from Lemma 13.3.2(ii), the 
continuity of F (., x) from the right, hypothesis (c) and the principle of optimality 
that, for a.e. t ∈ [S,T ], 
D↑V ((t, x(t));(1, e(t))) = lim inf
h↓0
[V (t + h, x + he(t)) − V (t, x)]h−1 ≥ 0 .
Then, for all points t ∈ (S, T ) in a set of full measure, the following relations, in 
which we write x(t), e(t), v(t) and w(t) briefly as x, e, v and w, are valid: 
0 ≤ lim inf
h↓0
[V (t + h, x + he) − V (t, x)]h−1(1 + w)
(we have used the fact that (1 + w) > 0)
≤ lim inf
h↓0
[V (t + h(1 + w), x + h(1 + w)e) − V (t, x)]h−1
(by positive homogeneity)
≤ lim inf
h↓0
[V (t + h, x + h(e + v)(1 + w)) − V (t, x)]h−1
+ lim sup
h↓0
[V (th + hw, xh − hv(1 + w)) − V (th, xh)]h−1
(in which th := t + h and xh := x + h(e + v)(1 + w))
≤ D↑V ((t, x);(1, (e + v)(1 + w))) + D0V ((t, x);(w, −v(1 + w))).
In the final expression D0V denotes the generalized directional derivative. 
We deduce from the fact that the generalized directional derivative D0V is the 
support function of co ∂V (observe that, since V is locally Lipschitz continuous, we 
can invoke Prop. 4.7.4) that 
D0V (t, x);(w, −v(1 + w))
≤ sup
(α,β)∈Gϵ (t)
(α, β) · (w, −v(1 + w)) = σϵ (t, v, w).
It follows 
D↑V ((t, x(t));(1, (e(t) + v(t))(1 + w(t)))
+σϵ (t, v(t), w(t)) ≥ 0 a.e. t ∈ [S,T ].706 13 Dynamic Programming
But then, since V (T , .) = g and in view of (13.5.10), 
J (x(S), x(T ), y(T )) = g(x(T )) − V (S, x(S)) +
 T
S
σϵ (t, v(t), w(t))dt
= V (T , x(T )) − V (S, x(S)) +
 T
S
σϵ (t, v(t), w(t))dt
=
 T
S
[D↑V ((t, x(t));(1, x(t))) ˙ + σϵ (t, v(t), w(t))]dt
≥ 0 = J (x(S), ¯ x(T ), ¯ y(T )). ¯
The claim is verified. 
We now apply the necessary conditions of Theorem 8.4.3 to (Pϵ ), with reference 
to the minimizer (x,¯ y¯ ≡ 0, v¯ ≡ 0, w¯ ≡ 0). Since the problem has free right end￾point, we may set the cost multiplier λ = 1. It follows that there exists a costate 
trajectory p˜ = (px , py , pv, pw) (associated with the state arc (x,¯ y¯ ≡ 0, v¯ ≡
0, w¯ ≡ 0)) such that 
˙
p(t) ˜ ∈ co {η : (η, p(t)) ˜ ∈ NGr F˜
ϵ (t,.)((x(t), ¯ 0, 0, 0), (˙
x(t), ¯ 0, 0, 0)) a.e.,
(13.5.11) 
px (t) · ˙
x(t) ¯ = max {px (t) · e : e ∈ F (t, x(t)) ¯ } a.e., (13.5.12) 
− ˜p(T ) ∈ ∂g(x(T ¯ )) × {1}×{0}×{0} (13.5.13) 
px (S) ∈ ∂x (−V )(S, x(S)). ¯
An analysis of proximal normals and strict normals approximating vectors in the 
limiting normal cone NGr F˜
ϵ permits us to conclude from (13.5.11) that ηy = 0, 
pv = 0, pw = 0, 
(ηx , px ) ∈ NGr F (t,.)(x(t), ¯ ˙
x(t)), ¯
and 
(ηv + px , ηw + px · ˙
x(t)) ¯ ∈ −py co∂v,wσϵ (t, v(t) ¯ = 0, w(t) ¯ = 0) . (13.5.14) 
Then from (13.5.11)–(13.5.13) we deduce also that p˙v ≡ 0, p˙w ≡ 0, py ≡ −1, and, 
writing p = px , we obtain 
p(t) ˙ ∈ co {q : (q, p(t)) ∈ NGr F (t,.)(x(t), ¯ ˙
x(t)) ¯ }
and13.5 Costate Trajectories and Gradients of the Value Function 707
− p(T ) ∈ ∂g(x(T )), p(S) ¯ ∈ ∂x (−V )(S, x(S)). ¯
Denote by H the Hamiltonian for the original problem, namely 
H (t, x, p) := max
v∈F (t,x) p · v .
Now, recalling the definition of σϵ , from (13.5.14) it follows that 
(H (t, x(t), p(t)), ¯ −p(t)) ∈ co Gϵ (t). (13.5.15) 
Thus far, ϵ ∈ (0, 1) has been treated as a constant. Now choose a sequence ϵi ↓ 0. 
For each i, write pi in place of p and ϵi in place of ϵ in the above relations. It can 
be deduced from (13.5.15) and the local Lipschitz continuity of V that {pi}i≥1 is 
bounded in L∞([S,T ]; Rn). We can also obtain from Lemma 7.2.3 that 
| ˙pi(t)| ≤ kF (t)|pi(t)| for a.e. t ∈ [S,T ].
Consequently, the pi’s are uniformly bounded and p˙i’s are uniformly integrably 
bounded. So, by extracting a subsequence if necessary (without relabelling), we can 
arrange that pi → p uniformly, for some absolutely continuous function p, and p˙i
converges to p˙ weakly in L1([S,T ]; Rn). 
A by now familiar convergence analysis can be used to show that the following 
conditions are satisfied 
p(t) ˙ ∈ co {q : (q, p(t)) ∈ NGr F (t,.)(x(t), ¯ ˙
x(t)) ¯ }, a.e. (13.5.16) 
p(t) · ˙
x(t) ¯ = max v∈F (t,x(t)) ¯ p(t) · v a.e. (13.5.17) 
−p(T ) ∈ ∂g(x(T ¯ )),
−p(S) ∈ ∂x (−V )(S, x(S)). ¯
Write H (t, x(t), p(t)) ¯ briefly as h(t). From (13.5.15) we obtain in the limit the 
information that, for some subinterval S ⊂ [S,T ] of full measure, 
(h(t), −p(t)) ∈ ∩ϵ'
>0 Gϵ'(t) for all t ∈ S. (13.5.18) 
Finally, we use these relations to show that 
(h(t), −p(t)) ∈ co ∂V (t, x(t)) ¯ for all t ∈ S. (13.5.19) 
This will complete the proof. 
Suppose to the contrary that (13.5.19) is false at some t ∈ S. Then we can strictly 
separate the point (h(t), −p(t)) and the closed convex set co ∂V (t, x(t)) ¯ . In other 
words, there exist α ∈ R, β ∈ Rn and γ > 0 such that708 13 Dynamic Programming
αh(t) − p(t) · β − γ > max{ατ + ξ · β : (τ, ξ ) ∈ co ∂V (t, x(t)) ¯ }
= D0V ((t, x(t)) ¯ ;(α, β)).
Again we have used here the fact that D0V is the support function of co ∂V . 
However D0V is upper semi-continuous with respect to all its arguments. It 
follows that, for some ϵ1 > 0, 
αh(t) − p(t) · β − γ /2 > D0V ((s, y);(α, β))
for all points (s, y) ∈ ((t, x(t)) ¯ + δ1B) ∩ ([S,T ] × Rn) We conclude that 
αh(t) − p(t) · β − γ /2 > sup
(τ,ξ )∈Gϵ1 (t)
ατ + ξ · β.
But then 
(h(t), −p(t)) /∈ co Gϵ1 (t),
contradicting (13.5.18). It follows that (13.5.19) is true. ⨅⨆
13.6 State Constrained Problems 
This section provides characterizations of the value function in terms of solutions 
to the Hamilton Jacobi equation, when the formulation of the underlying dynamic 
optimization problem is broadened to include a pathwise state constraint. We also 
show the earlier established relations between costate trajectories and gradients of 
the value function continue to apply in this wider context. Consider then: 
(SC)
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize g(x(T ))
over arcs x ∈ W1,1([S,T ]; Rn) satisfying
x(t) ˙ ∈ F (t, x(t)) a.e.,
x(t) ∈ A for all t ∈ [S,T ],
x(S) = x0,
with data an interval [S,T ] ⊂ R, a function g : Rn → R ∪ {+∞}, a multifunction 
F : [S,T ] × Rn ⇝ Rn, a closed set A ⊂ Rn (the ‘state constraint set’) and a point 
x0 ∈ Rn. 
Write (SCt,x ) for problem (SC) when the initial data (t, x) replaces (S, x0). 
Denote by V : [S,T ] × A → R ∪ {+∞} the value function for (SC). That is, 
for each (t, x) ∈ [S,T ] × A, V (t, x) is the infimum cost of (SCt,x ). The cost of 
any state trajectory y violating the state constraint, y(s) ∈ A for all s ∈ [t,T ], is13.6 State Constrained Problems 709
taken to be +∞. Notice that, consistent with this interpretation, V takes value +∞
at points (t, x) such that x /∈ A. 
It is to be expected that the value function V for (S) will be the unique solution 
(appropriately defined) to the Hamilton Jacobi equation 
Vt + min v∈F (t,x)
Vx · v = 0 for (t, x) ∈ (S, T ) × A,
accompanied by a boundary condition on {T } ×A and also a boundary condition on 
(S, T ) × ∂A. 
Under the hypotheses here imposed, it turns out that the appropriate boundary 
condition on (S, T ) × ∂A is a boundary inequality 
Vt + min v∈F (t,x)
Vx · v ≤ 0 for (t, x) ∈ (S, T ) × ∂A,
suitably interpreted. 
The following theorems tell us that a characterization of the value function 
for state constrained problems along the above lines is possible, when certain 
compatibility hypotheses are satisfied, concerning the interaction of the state 
constraint set A and the velocity set F. These compatibility hypotheses, which allow 
alternative, ‘inward-pointing’ or ‘outward-pointing’ versions, require that there exist 
velocities driving the state strictly away from the boundary of the state constraint set. 
Earlier sections, in which state constraints are not present, have covered problems 
in which the velocity set F (t, x) is possibly discontinuous w.r.t. t. Specifically, we 
have hypothesized ‘t → F (t, x) has left and right limits and is almost everywhere 
continuous’. Passing to problems with state constraints, we shall now require that 
F (t, x), as a function of t, has ‘bounded variation’ (BV); the (BV) hypothesis is 
a strengthening of the earlier t-dependence hypothesis that, nonetheless, allows t
discontinuity. 
(BV): For each R0 > 0, F (., x) has bounded variation uniformly over x ∈ R0B, 
in the following sense: there exists a bounded variation function η : [S,T ] → R
such that, for every [s,t]⊂[S,T ] and x ∈ R0B, 
dH (F (s, x), F (t, x)) ≤ η(t) − η(s) .
As before, the left and right limit sets of F (., x), namely F (t+, x) and F (t−, x), 
have the following meaning: for all s ∈ [S,T ) and t ∈ (S, T ], and for all x ∈ Rn
F (s+, x) := lim s'
↓s, x'
→x
F (s'
, x'
) and F (t−, x) := lim t'
↑t,x'
→x
F (t'
, x'
) .
Theorem 13.6.1 (Characterization of Value Functions for State Constrained 
Problems (I): Outward-Pointing Condition) Consider (SCS,x0 ). Assume (H1), 
(H2)(i), (H3), (H4)(i) and (H5) of Theorem 13.3.7 and (BV). Suppose in addition 
that710 13 Dynamic Programming
(CQ)outward : for each s ∈ [S,T ), t ∈ (S, T ] and x ∈ ∂A, 
F (t−, x) ∩ 

− int TA(x) /= ∅ and F (s+, x) ∩ 

− int TA(x) /= ∅ .
Take a function V : [S,T ] × Rn → R ∪ {+∞}. Then assertions (a)–(c) below are 
equivalent: 
(a) V is the value function for (SCS,x0 ), 
(b) V is lower semi-continuous on [S,T ] × Rn, V (t, x) = +∞ if x /∈ A, and 
(i) for all (t, x) ∈ ([S,T ) × A) ∩ dom V
inf v∈F (t+,x)
D↑V ((t, x);(1, v)) ≤ 0,
(ii) for all (t, x) ∈ ((S, T ] × int A) ∩ dom V
sup v∈F (t−,x)
D↑V ((t, x);(−1, −v)) ≤ 0,
(iii) for all x ∈ A
lim inf
{(t'
,x'
)→(T ,x):t'
<T ,x'
∈int A}
V (t'
, x'
) = V (T , x) = g(x).
(c) V is lower semi-continuous on [S,T ] × Rn, V (t, x) = +∞ if x /∈ A, and 
(i) for all (t, x) ∈ ((S, T ) × A) ∩ dom V , (ξ 0, ξ 1) ∈ ∂P V (t, x)
ξ 0 + inf v∈F (t+,x)
ξ 1 · v ≤ 0,
(ii) for all (t, x) ∈ ((S, T ) × int A) ∩ dom V , (ξ 0, ξ 1) ∈ ∂P V (t, x)
ξ 0 + inf v∈F (t−,x)
ξ 1 · v ≥ 0,
(iii) for all x ∈ A, 
lim inf
{(t'
,x'
)→(S,x):t'
>S}
V (t'
, x'
) = V (S, x)
and 
lim inf
{(t'
,x'
)→(T ,x):t'
<T , x'
∈int A}}
V (t'
, x'
) = V (T , x) = g(x).13.7 Proofs of Theorems 13.6.1 and 13.6.2 711 
Theorem 13.6.2 (Characterization of Value Functions for State Constrained 
Problems (II): Inward-Pointing Condition) Consider (SCS,x0 ). Assume (H1), 
(H2)(i), (H3), (H4)(i) and (H5) of Theorem 13.3.7 and (BV). Suppose in addition 
g is continuous on A and 
(CQ)inward : for each s ∈ [S,T ), t ∈ (S, T ] and x ∈ ∂A, 
F (t−, x) ∩ int TA(x) /= ∅ and F (s+, x) ∩ int TA(x) /= ∅ .
Take a function V : [S,T ] × Rn → R ∪ {+∞}. Then the assertions (a)–(c) of 
Theorem 13.6.1 remain equivalent. 
Remarks 
1. Theorems 13.6.1 and 13.6.2 differ principally according to whether an ‘inward 
pointing’ and ‘outward pointing’ constraint qualification is invoked. Note how￾ever that Theorem 13.6.2 is more restrictive, to the extent that it requires ‘g
is continuous on A’, a hypothesis that automatically excludes problems with 
endpoint constraints. 
2. It can been shown that, under the hypotheses of Theorem 13.6.2, including the 
‘inward pointing condition’, the value function is continuous on [S,T ] × A. 
Notice however that both of the characterizations, (b) and (c), are in terms of 
lower semi-continuous functions. Thus the theorem not only identifies the value 
function as the unique generalized solution to the Hamilton-Jacobi equation, but 
gives the extra information that there are no ‘hidden’ generalized solutions in the 
larger class of lower semi-continuous functions. 
13.7 Proofs of Theorems 13.6.1 and 13.6.2 
We shall make repeatedly use of the ‘L∞ distance estimate theorem’ Theorem 6.6.2 
for state constrained differential inclusions. We observe that among the hypotheses 
invoked in Theorems 13.6.1 and 13.6.2 we assume that the multifunction F takes 
convex values and satisfies condition (H4)(i) (of Theorem 13.3.7); in consequence, 
the inward pointing condition (IPC)' of Theorem 6.6.2 can now be stated in the 
simpler form: 
(CQ)'
: for each s ∈ [S,T ), t ∈ (S, T ] and x ∈ RB ∩ ∂A, 
F (t−, x) ∩ int TA(x) /= ∅ and F (s+, x) ∩ int TA(x) /= ∅ .
The following proposition assembles some useful regularity properties of the 
value function. Notice in particular that the value function inherits the Lipschitz 
regularity of the final cost function, a property that follows from the preceding L∞
distance estimate.712 13 Dynamic Programming
Proposition 13.7.1 Let V be the value function for (SC). Assume that the following 
hypotheses are satisfied 
(i): F : [S,T ]×Rn ⇝ Rn takes closed non-empty values, F is L×Bn-measurable, 
the graph of F (t, .) is closed for each t ∈ [S,T ], 
(ii) there exists c ∈ L1(S, T ) such that 
F (t, x) ⊂ c(t)(1 + |x|) B for all x ∈ Rn and for a.e. t ∈ [S,T ],
(iii) g : Rn → R ∪ {+∞} is lower semi-continuous. 
(a) Then V (t, x) > −∞ for all (t, x) ∈ [S,T ] × Rn, 
(b) If in addition F takes convex values, then V is lower semi-continuous and 
V (t, x) > −∞ for all (t, x) ∈ [S,T ] × Rn, 
(c) If, in addition, the hypotheses (H1), (H2), (H3)(i) of Theorem 13.3.7 together 
with (BV) and (CQ)' are satisfied and g is locally Lipschitz continuous on A
(resp. continuous on A), then V is locally Lipschitz continuous on [S,T ] × A
(resp. continuous on [S,T ] × A). 
Proof 
(a) and (b) The proof is exactly as in the proof of Proposition 13.3.6. We highlight 
only the fact that, even though the extensions to [S,T ] of the minimizers yi, i =
1, 2,... might not to be admissible, the limiting F trajectory y is admissible. 
(c) We show here that V is locally Lipschitz continuous on [S,T ] × A when g
is locally Lipschitz continuous on A, since the case continuous on [S,T ] × A can 
be similarly proved (see Proposition 13.3.6). Observe that all the hypotheses of the 
distance estimate theorem (Theorem 6.6.2) are satisfied. Fix r0 > 0. Notice that for 
any τ ∈ [S,T ] and any F trajectory x on [τ,T ] such that x(τ ) ∈ r0B we have: 
x(t) ∈ R0B for all t ∈ [t,T ], where R0 := exp{
 T
S c(t) dt}(r0 + 1). Fix two points 
(t1, x1), (t2, x2) ∈ [S,T ] × A such that x1, x2 ∈ A ∩ r0B. It is not restrictive to 
assume that t1 ≤ t2. Fix any ε > 0. Then there exists an admissible F trajectory xε
such that xε(t1) = x1 and 
V (t1, x1) ≥ g(xε(T )) − ε . (13.7.1) 
Consider the point (t2, xε(t2)). From the Filippov’s existence theorem (Theo￾rem 6.2.3) there exists an F trajectory (not necessarily admissible) xˆ on [t2, T ] such 
that x(t ˆ 2) = x2 and 
‖ ˆx − xε‖L∞(t2,T ) ≤ e
 T
S kF (s) ds |x2 − xε(t2)‖ . (13.7.2) 
Then, by Theorem 6.6.2, there exists an admissible F trajectory x such that x(t2) =
x2 satisfying the estimate 
‖x − ˆx‖L∞(t2,T ) ≤ Kmax{dA(x(t)) ˆ : t ∈ [t2, T ]}. (13.7.3)13.7 Proofs of Theorems 13.6.1 and 13.6.2 713 
But max{dA(x(t)) ˆ : t ∈ [t2, T ]} ≤ ‖ ˆx − xε‖L∞(t2,T ) and |x2 − xε(t2)|≤|x1 −
x2| +c0|t1 −t2|, where c0 > 0 is a constant provided by (H2)(ii). From (13.7.2) and 
(13.7.3), it follows that 
‖x − xε‖L∞(t2,T ) ≤ ‖ˆx − xε‖L∞(t2,T ) + ‖x − ˆx‖L∞(t2,T )
≤ (K + 1) ‖ ˆx − xε‖L∞(t2,T )
≤ √
2c0(K + 1)e
 T
S kF (s) ds|z1 − z2| .
Write kg the Lipschitz constant of g on R0B. Therefore we have 
V (t2, x2) − V (t1, x1) ≤ g(x(T )) − g(xε(T )) + ε
≤ kg ‖x − xε‖L∞(t2,T ) + ε
≤ K0 |(t1, x1) − (t2, x2)| + ε ,
where the constant K0 := kg
√2c0(K + 1)e
 T
S kF (s) ds is a constant which does not 
depend on ε, (t1, x1) or (t2, x2). The reverse inequality 
V (t1, x1) − V (t2, x2) ≤ K0 |(t1, x1) − (t2, x2)| + ε
is easily obtained by means of similar arguments. The assertion (c) of the proposi￾tion now follows, by taking the limit as ε tends to zero. 
⨅⨆
Proof of Theorem 13.6.1 
‘(a) ⇒ (b)’. Let V be the value function. Using the facts that whenever x¯ is a 
minimizer for (Pt,x ), the function s → V (s, x(s)) ¯ is constant on [t,T ] and, if 
x is an admissible F trajectory satisfying the initial condition x(t) = x, then 
s → V (s, x(s)) is non-decreasing on [t,T ], we can reproduce the analysis in the 
state constraints-free case to confirm (b)(i) and (b)(ii). (Observe that the analysis 
employed in the proof of Theorem 13.3.7 can also be used to prove (b)(ii), since in 
this case (t, x) ∈ ((S, T ] × int A) and, therefore, there exists δ >¯ 0 such that any F
trajectory emanating from x is admissible on [t,t + δ¯].) 
It remains to prove (b)(iii). Take any x ∈ A. Since V is lower semi-continuous, 
we have that 
lim inf
{(t'
,x'
)→(T ,x):t'
<T ,x'
∈int A}
V (t'
, x'
) ≥ V (T , x) .
To complete the verification of condition (b)(iii), consider the differential inclusion 

y(s) ˙ ∈ F (s, y(s))  for a.e. s ∈ [0, T − S]
y(0) = x ,714 13 Dynamic Programming
where F (s, y)  := −F (T − s, y). There exists an admissible F
 trajectory z˜ on 
[0, T − S] such that z(˜ 0) = x and 
z(s) ˜ ∈ int A, for all s ∈ (0, T − S] .
(To justify this assertion we note that, by Filippov’s existence theorem, there exists 
an F
 trajectory zˆ on [0, T −S] such that z(ˆ 0) = x. The existence of the F
 trajectory 
z˜ with the stated properties is then established by applying Theorem 6.6.2 with 
reference trajectory zˆ.) Notice next that y := ˜z(T − .) is an admissible F trajectory 
on [S,T ] such that y(T ) = x and 
y(t) ∈ int A, for all t ∈ [S,T ).
By the principle of optimality, however, V (t, x(t)) ≤ V (T , x(T )) = V (t, x) for all 
t ∈ [S,T ]. We deduce from the preceding relations that 
V (T , x) ≤ lim inf
{(t'
,x'
)→(T ,x):t'
<T ,x'
∈int A}
V (t'
, x'
) ≤ lim inf
t↑T
V (t, x(t))≤V (T , x)=g(x).
This is condition (b)(iii). 
‘(b) ⇒ (c)’. The relations ‘(b)(i) ⇒ (c)(i)’ and ‘(b)(ii) ⇒ (c)(ii)’ have already 
been established in the proof of Theorem 13.3.7, the analysis in which is the same 
whether a state constraint is present or not. To complete the proof we have merely 
to note that the first relation in (c)(iii) is an immediate consequence of (b)(i). 
‘(c) ⇒ (a)’. In view of our convention that F trajectories that violate the state 
constraint have cost +∞, it is clear that the validity of this relation will be a 
consequence of completing the following two steps, in which V : [S,T ] × Rn →
R ∪ {+∞} in an arbitrary lower semi-continuous function satisfying condition (c) 
and (t ,¯ x)¯ is an arbitrary point on [S,T ] × Rn. 
Step 1 We show 
V (t ,¯ x)¯ ≥ g(x(T )) ¯
for some admissible F trajectory x¯ on [t,T ¯ ] such that x(¯ t)¯ = ¯x. 
Step 2 We show 
V (t ,¯ x)¯ ≤ g(x(T )),
in which x is an arbitrary admissible F trajectory on [t,T ¯ ] such that x(t)¯ = ¯x. 
Step 1 has already been accomplished. Indeed scrutiny of the proof of Theo￾rem 13.3.7, reveals that the analysis leading to the desired conclusions is the same, 
whether or not state constraints are present. We attend then only to Step 2.13.7 Proofs of Theorems 13.6.1 and 13.6.2 715 
We can also assume that g(x(T )) < +∞ since, otherwise, the inequality above 
is automatically satisfied. From the second relation in (c)(iii) we have 
lim inf
{(t'
,x'
)→(T ,x(T )):t'
<T ,x'
∈int A}
V (t'
, x'
) = V (T , x(T )) = g(x(T )).
There exists, in consequence, a sequence {(ti, ξi)} in (t,T ) ¯ × int A such that 
V (ti, ξi) → g(x(T )), ti ↑ T and ξi → x(T ), as i → +∞.
(13.7.4) 
Notice that, for each i, the arc s → x(T − s) is an admissible F
 trajectory on 
[T − ti, T − t
¯], where F (s, y)  := −F (T − s, y). Applying the Filippov existence 
theorem to the differential inclusion 

y(s) ˙ ∈ F (s, y(s))  for a.e. s ∈ [T − ti, T − t
¯]
y(T − ti) = ξi ,
we obtain a sequence of F
 trajectories yi : [t,t ¯ i] → Rn, i = 1, 2,..., such that, 
for all i sufficiently large, yi(ti) = ξi and 
||yi − x(T − .)||L∞(T¯−ti,T −t)¯ ≤ exp{
 T
S
kF (t) dt}|x(ti) − ξi| . (13.7.5) 
Since x is an admissible F trajectory, it follows that 
max{dA(yi(t)) | t ∈ [t,t ¯ i]} ≤ exp{
 T
S
kF (t) dt}|x(ti) − ξi| . (13.7.6) 
Write ρi := exp{
 T
S kF (t) dt}|x(ti) − ξi| and notice that ρi ↓ 0 as i → ∞. 
Application of Theorem 6.6.2 to the differential inclusion y˙ ∈ F˜ yields a sequence 
of admissible F trajectories zi : [t,t ¯ i] → Rn, i = 1, 2,..., such that, for each i,
zi(T − ti) = ξi
zi(s) ∈ int A, for all s ∈ [T − ti, T − t
¯]
and 
||yi − zi||L∞(t ,t ¯ i) ≤ Kρ  i ,
in which K is some number that does not depend on i. This inequality combines 
with (13.7.6) to give 
||zi − x(T − .)||L∞(T −ti,T −t)¯ ≤ (K + 1)ρi .716 13 Dynamic Programming
With the help of Filippov’s existence theorem (in which we take as reference 
trajectory s → x(T − s) restricted to [0, T − ti]), we can extend each zi as an 
F
 trajectory to all of [0, T − t
¯]. (The extension is also written ‘zi’.) It follows from 
the Filippov existence theorem that the extended F
 trajectories satisfy: 
||zi − x(T − .)||L∞(0,T −t)¯ ≤ (K + 1)ρi . (13.7.7) 
Furthermore, for some sequence δi ↓ 0, 
zi(s) + δiB ⊂ int A for all s ∈ [T − ti, T − t
¯], (13.7.8) 
and 
V (ti, zi(T − ti)) = V (ti, ξi) → g(x(T )) , as i → ∞ .
Fix i. We now employ a similar analysis to that used in Step 2 of the proof of 
the relation ‘(c) ⇒ (a)’ in Theorem 13.3.7, based on the strong invariance theorem 
(Theorem 13.2.4): we consider the constrained differential inclusion 
⎧
⎨
⎩
(z,˙ a)˙ ∈ �(s, (z, a)) ˜ a.e s ∈ [T − ti, T − t
¯],
z(T − ti) = ξi, a(T − ti) = V (ti, ξi),
(z(s), a(s)) ∈ P (s) ˜ for all s ∈ [T − ti, T − t
¯],
in which � : [T − ti, T − t
¯] × Rn+1 ⇝ Rn+2 and P˜ : [T − ti, T − T¯] ⇝ Rn+1 are 
�(s, (z, a)) ˜ := −F (T −s, z)× {0}, for all (s, (z, a)) ∈ [T −ti, T −T¯] ×Rn+1
P (s) ˜ := {(z, α) : V (T − s, z) ≤ α}, for all s ∈ [T − ti, T − T¯],
respectively. The multifunctions �˜ and P˜ both have the required closure and 
regularity for application of the strong invariance theorem (Theorem 13.2.4). Take 
as reference trajectory (zi, ai ≡ V (ti, ξi)). Since T −ti > 0 and (zi(T −ti), ai(T −
ti)) = (ξi, V (ti, ξi)) ∈ P (T ˜ − ti) the boundary condition, in which the initial time 
S¯ is interpreted as S¯ = T − ti, is also valid. Concerning the ‘inward pointing’ 
hypothesis, we deduce from (c)(ii) and (13.7.8), employing the same analysis 
as used in the proof of Theorem 13.3.7 (step 2 of ‘(c) ⇒ (a)’), that condition 
(vi) of Theorem 13.2.4 is certainly satisfied on a δi-tube around the �˜ trajectory 
(zi, ai ≡ V (ti, ξi)). So, from Theorem 13.3.7, 
(zi(s), V (ti, ξi)) ∈ P (s) ˜ for all s ∈ [T − ti, T − t
¯].
Setting s = T − t
¯ yields 
V (t,z ¯ i(T − t)) ¯ ≤ V (T − ti, ξi), for each i . (13.7.9)13.7 Proofs of Theorems 13.6.1 and 13.6.2 717 
From (13.7.4), (13.7.7), (13.7.9) and the lower semi-continuous of V , we finally 
arrive 
V (t, x( ¯ t)) ¯ ≤ lim inf
i→∞ V (t,z ¯ i(T − t)) ¯ ≤ lim
i→∞ V (ti, ξi) = g(x(T )).
This concludes Step 2. ⨅⨆
Proof of Theorem 13.6.2 The proof is a modification of that of Theorem 13.6.1, 
to take account of the fact that we have replaced the outward pointing constraint 
qualification by an inward pointing one. The analysis will require that g is 
continuous on A. 
The analysis required to show (a) ⇒ (b) ⇒ (c) for Theorem 13.6.1 is the same 
as that employed in the proof of Theorem 13.6.2, but simpler because the boundary 
condition 
lim inf
{(t'
,x'
)→(T ,x):t'
<T ,x'
∈int A}
V (t'
, x'
) = V (T , x),
is an immediate consequence of the continuity of the value function, a property 
established in Proposition 13.7.1. (The outward pointing constraint qualification was 
used in this part of the proof of Theorem 13.6.1 only to show that the value function 
satisfied this condition and is not required here.) 
It remains then to show ‘(c) ⇒ (a)’. As before, this involves the two steps: for 
any given, (t ,¯ x)¯ ∈ [S,T ] × Rn. 
Step 1 We show 
V (t ,¯ x)¯ ≥ g(x(T )) ¯
for some admissible F trajectory x¯ on [t,T ¯ ] such that x(¯ t)¯ = ¯x. 
Step 2 We show 
V (t ,¯ x)¯ ≤ g(x(T )),
in which x is an arbitrary admissible F trajectory on [t,T ¯ ] such that x(t)¯ = ¯x. 
Step 1 (which does not involve the constraint qualification) is exactly the same 
as in the proof of Theorem 13.6.1. It remains only to attend to Step 2. Take then any 
admissible F trajectory x on [t,T ¯ ] emanating from x¯. Invoking the inward point 
constraint qualification and applying Theorem 6.6.2, we can construct a sequence 
of admissible F trajectories {xi} on [t,T ¯ ] such that xi(t)¯ = ¯x and 
||xi − x||L∞(t ,T ) ¯ → 0 as i → ∞, (13.7.10) 
xi(t) ∈ int A, for all t ∈ (t,T ¯ ] and i = 1, 2,....718 13 Dynamic Programming
Take a sequence ti ↓ t
¯ and note that, for each i sufficient large, there exists δi > 0
such that 
xi(t) + δiB ⊂ int A, for all t ∈ [ti, T ] . (13.7.11) 
Take the multivalued function F (s, y)  := −F (T − s, y) (already used in step 2 
of the proof of Theorem 13.6.1), and consider the F
 trajectories zi on [0, T − t
¯], 
defined as zi(s) := xi(T − s). From (13.7.11) we deduce that 
zi(s) + δiB ⊂ int A, for all s ∈ [0, T − ti] . (13.7.12) 
Fix i and consider the constrained differential inclusion 
⎧
⎨
⎩
(z,˙ a)˙ ∈ �(s, (z, a)) ˜ a.e s ∈ [0, T − ti],
z(0) = xi(T ), a(0) = V (T , xi(T )),
(z(s), a(s)) ∈ P (s) ˜ for all s ∈ [0, T − ti],
where � : [0, T − t
¯] × Rn+1 ⇝ Rn+2 and P˜ : [0, T − t
¯] ⇝ Rn+1 are defined as 
follows 
�(s, (z, a)) ˜ := F (s, z) ˜ × {0}, for all (s, (z, a)) ∈ [0, T − t
¯] × Rn+1
P (s) ˜ := {(x, α) : V (T − s, x) ≤ α}, for all s ∈ [0, T − t
¯] .
Observe that condition (c)(iii) guarantees that (z(0), a(0)) = (xi(T ), V (T , xi(T )))
∈ lim sups↓0 P (s) ˜ . Note also that �˜ and P˜ satisfy the hypotheses of the strong 
invariance theorem (Theorem 13.2.4). Moreover, by means of a similar analysis to 
that in Step 2 of the proof of Theorem 13.3.7, in which we show ‘(c) ⇒ (a)’, we 
can deduce from (c)(ii) and (13.7.12) that the ‘inward pointing’ hypothesis (vi) of 
Theorem 13.2.4 is satisfied on a δi-tube around the �˜ trajectory (zi, a ≡ V (ti, ξi)). 
The strong invariance theorem is therefore applicable. It follows that 
(zi(s), a(s) ≡ V (ti, ξi)) ∈ P (s) ˜ for all s ∈ [0, T − ti].
As a consequence, for each i, we have: 
V (T , xi(T )) ≥ V (ti, zi(T − ti)) ( = V (ti, xi(ti))) . (13.7.13) 
Noting (13.7.10), (13.7.9) and (c)(iii), and using the facts that V is lower semi￾continuous and that g is continuous on A, we conclude: 
V (t, x( ¯ t)) ¯ ≤ lim inf
i→∞ V (ti, xi(ti)) ≤ lim
i→∞ V (T , xi(T ))
= lim
i→∞ g(xi(T )) = g(x(T )).
This completes Step 2. ⨅⨆13.8 Costate Trajectories and Gradients of the Value Functions for State. . . 719
13.8 Costate Trajectories and Gradients of the Value 
Functions for State-Constrained Problems 
In Sect. 13.5, we showed that the costate trajectory p, appearing in the generalized 
Euler Lagrange condition, with reference to a minimizing state trajectory x, could 
be chosen to satisfy the ‘sensitivity relation’ 
(H (t, x(t), p(t)), ¯ −p(t)) ∈ co ∂V (t, x(t)) ¯ a.e. t ∈ [S,T ].
Is a version of this relation valid, when a state constraint ‘x(t) ∈ A for all t ∈ [S,T ]’ 
is added to the underlying problem formulation? The answer is qualified ‘yes’: it is 
indeed valid, but only if we interpret ‘costate arc’ appropriately for this context. 
We provide motivation behind the correct interpretation. For ease of exposition, 
we assume that the dynamic constraint takes the form of a controlled differential 
equation. 

x(t) ˙ = f (t, x(t), u(t))
u(t) ∈ U
and the state constraint has the functional representation 
A := {x : h(x) ≤ 0},
for some C1 function h : Rn → R. 
Recall the discussion at the beginning of Chap. 10, concerning the manner in 
which necessary conditions of optimality should be modified to accommodate a 
state constraint x(t) ∈ A. We pointed out that if the necessary conditions for the 
state constrained problem correspond to necessary conditions for a related problem 
in which the state constrained is removed and, instead, a term 
 T
S
h(x(t))dν(t) ¯
(in which the integrator dν is interpreted as the Lagrange multiplier for the state 
constraint) is added to the cost, then the Weierstrass and transversality conditions 
are satisfied for a costate trajectory q of bounded variation, that satisfies the costate 
equation 
− dq(t) = q(t)fx (s, x(t), u(t))dt ¯ − hx (x(s))dν(t) . ¯
The set of optimality conditions can be expressed in terms of modified costate 
trajectory720 13 Dynamic Programming
p(t) := q(t) −

hx dν ,
that satisfies the modified costate equation 
− ˙p(t) = (p(t) +

[S,t]
hx (x(s))dν(t))f ¯ x (s, x(t), u(t))dt . ¯
Of course, the distinction between q and p disappears if there are no state constraints 
because, then, ν ≡ 0. 
It can be argued that the costate trajectory q is more fundamental than the 
modified costate trajectory p and introduction of the modified costate trajectory 
is merely a cosmetic step to simplify the statement of the necessary conditions. 
This is because q, and not p, can be interpreted as the sensitivity of the cost to 
perturbations of the dynamic constraint, a property that was identified Clarke and 
Loewen [72]. Not surprisingly then, the sensitivity relations for state constrained 
problems, providing costate interpretations of the gradients of the value function 
involve the original costate trajectory q and not the modified costate trajectory p. 
Theorem 13.8.1 (Sensitivity Relations in the Presence of State Constraints) 
Consider problem (SC) (of Sect. 13.6). Denote by V : [S,T ] × Rn → R ∪ {+∞}
the value function. Assume that 
(a): F takes values in the space of closed, non-empty sets. F (., x) is Lebesgue 
measurable for each x ∈ Rn, 
(b): there exists c > 0 such that 
F (t, x) ⊂ c(1 + |x|)B for all (t, x) ∈ [S,T ] × Rn,
(c): there exists kF ∈ L1(S, T ) such that 
F (t, x) ⊂ F (t, x'
) + kF (t)|x − x'
|B, for all x, x' ∈ Rn, a.e. t ∈ [S,T ],
(d): g is locally Lipschitz continuous, 
(e): A is a nonempty closed set, 
and also: 
(BV): for each R0 > 0, F (., x) has bounded variation uniformly over x ∈ R0B, 
in the following sense: there exists a bounded variation function η : [S,T ] → R
such that, for every [s,t]⊂[S,T ] and x ∈ R0B, 
dH (F (s, x), F (t, x)) ≤ η(t) − η(s),
(IPC): (Inward Pointing Condition) for each t ∈ [S,T ), s ∈ (S, T ] and x ∈ ∂A13.8 Costate Trajectories and Gradients of the Value Functions for State. . . 721

lim inf
x' A
→x
co F (t+, x'
)

∩ int TA(x) /= ∅ ,

lim inf
x' A
→x
co F (s−, x'
)

∩ int TA(x) = ∅ / .
Let x¯ be an L∞ local minimizer for (SC). Then there exist p ∈ W1,1([S,T ]; Rn), 
μ ∈ C⊕(S, T ), a bounded Borel measurable function γ : [S,T ] → Rn and λ ≥ 0, 
satisfying the following conditions, in which q ∈ NBV ([S,T ]; Rn) is the function 
q(t) := 
p(S) if t = S
p(t) + 
[S,t] γ (s)dμ(s) if t ∈ (S, T ] .
(i): γ (t) ∈ (co NA(x(t)) ¯ ) ∩ B μ − a.e. t ∈ [S,T ], 
(ii): p(t) ˙ ∈ co {ξ : (ξ , q(t)) ∈ NGr{F (t,.)}(x(t), ¯ ˙
x(t)) ¯ } a.e. t ∈ [S,T ], 
(iii): −q(T ) ∈ ∂g(x(T )), q(S) ¯ ∈ ∂(−V + ΨA)(x(S)) ¯ . 
in which ΨA(x) := 0 if x ∈ A, and takes value +∞ if x /∈ A; 
(iv): q(t) · ˙
x(t) ¯ = maxv∈F (t,x(t)) ¯ q(t) · v a.e.t ∈ [S,T ]. 
Furthermore, 
(H (t, x(t), q(t)), ¯ −q(t)) ∈ ∂0V (t, x(t)) ¯ a.e. t ∈ (S, T ] , (13.8.1) 
in which, for (t, x) ∈ [S,T ] × A, 
∂0V (t, x) := ∩ϵ>0 co ∪{(t'
,x'
) ∈ ((t,x)+ϵB) ∩ [S,T ]×int A} ∂V (t'
, x'
) .
Remarks 
(a): The ‘full’ sensitivity relation (13.8.1) can be supplemented by the ‘partial’ 
sensitivity relation 
− q(t) ∈ ∂0
xV (t, x(t)) ¯ for a.e. t ∈ (S, T ] , (13.8.2) 
in which, for (t, x) ∈ [S,T ] × A, 
∂0
xV (t, x) := ∩ϵ>0 co ∪{x' ∈ (x+ϵB) ∩ int A} ∂xV (t, x'
) ,
(13.8.1) and (13.8.2) are distinct conditions; see the discussion following the 
statement of Theorem 13.5.1. 
(b): The two sensitivity relations (13.8.1) and (13.8.2) are more precise sensi￾tivity relations than those expressed in terms of the Clarke subdifferential 
co ∂V (t, x(t)) ¯ and partial Clarke subdifferential co ∂xV (t, x(t)) ¯ , because the722 13 Dynamic Programming
definitions of ∂0V (t, x(t)) ¯ and ∂0
xV (t, x(t)) ¯ involve limit taking from within 
the interior of the state constraint set. Indeed 
∂0
xV (t, x) = co ∂xV (t, x) and ∂0V (t, x) = co ∂V (t, x)
for (t, x) ∈ [S,T ] × int A and 
∂0
xV (t, x)
strict
⊂ co ∂xV (t, x) and ∂0V (t, x)
strict
⊂ co ∂V (t, x)
for (t, x) ∈ [S,T ] × ∂A. 
(c): The assertions of Theorem 13.8.1 remain valid (with possibly different p, 
μ and γ ) if we replace the Euler Lagrange inclusion (ii) with the partially 
convexified Hamiltonian inclusion: 
(ii)' p(t) ˙ ∈ co {ξ : (−ξ , ˙
x(t)) ¯ ∈ ∂x,pH (t, x(t), q(t)) ¯ } for a.e. t ∈ [S,T ]. 
Proof (Sketch) We employ the penalty function σ A
ϵ : [S,T ]×Rn×R → R defined 
by 
σ A
ϵ (t, v, w) := sup
(α,β)∈GA
ϵ (t)
(α, β) · (w, −(1 + w)v),
in which GA
ϵ : [S,T ] ⇝ R1+n to be 
GA
ϵ (t) := {(α, β) : R1+n : (α, β) ∈ co ∂V (s, y)
for some (s, y) ∈ ((t, x(t)) ¯ + ϵB) ∩ ((S, T ) × Rn) s.t. y ∈ int A}
and ϵ > 0 is a (small) parameter. (σ A
ϵ coincides with the penalty function σϵ
appearing in the proof of Theorem 13.5.1 in the no state-constraints case, that is 
when A = Rn.) Now consider the following dynamic optimization problem: 
(AP )
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Minimize g(x(T )) − V (S, x(S)) +  T
S σ A
ϵ (t, v(t), w(t))dt
over arcs x satisfying
x(t) ˙ = {(e + v(t)(1 + w(t)) : e ∈ F (t, x(t))} a.e.
for some functions v : [S,T ] → ϵB, w : [S,T ] → ϵB ,
(x(S), v(S), w(S)) ∈ Rn × {0}×{0}
x(t) ∈ A for all t ∈ [S,T ] .
(At this stage, we are intentionally vague about the function spaces from which the 
functions v and w must be chosen in the specification of the dynamic constraint.) 
Let us recall that, in the proof of the state constraint-free sensitivity relation 
Theorem 13.5.1, we showed that x¯ (and perturbation functions v¯ ≡ 0, w¯ ≡ 0) 
was a minimizer for the ‘auxiliary problem’ (AP ). By applying standard necessary 
conditions to (AP ) and passage to the limit as ϵ ↓ 0, we show that the generalized 
Euler Lagrange conditions are valid for the original problem, but now augmented13.9 Semiconcavity and the Value Function 723
by the sensitivity relation. This basic idea is retained, when we take account of the 
state constraint, but the details of how exactly it is applied change. Once again, it 
can be shown that (x,¯ v¯ ≡ 0, w¯ ≡ 0) is a minimizer for (AP ). This is because 
the proof that x¯ (combined with v¯ ≡ 0 and w¯ ≡ 0) is a minimizer for (AP )
involves showing that, given an admissible state trajectory for (AP ) (associated with 
some function pair (v, w)) we can find an admissible state trajectory (associated 
with the same function pair (v, w)) that is interior to A on (S, T ]. The distance 
estimate of Theorem 6.6.1 that is used to establish the existence of this interior state 
trajectory is valid only under the inward pointing hypothesis and when restrictions 
are placed on the regularity of v and w. (The inward pointing hypothesis is also 
required to establish that V is locally Lipschitz continuous on A.) It is then possible 
to show that x¯ satisfies the usual generalized Euler Lagrange conditions, but now 
supplemented by the stated sensitivity relation, by applying the known necessary 
conditions to (AP ), with reference to the minimizer (x,¯ v¯ ≡ 0, w¯ ≡ 0). Full 
details of the proof of a closely related ‘sensitivity’ theorem, in which the bounded 
variation hypothesis (BV) in theorem statement is replaced by a stronger ‘absolutely 
continuous’ hypothesis, appears in [35]; the substance of the proof remains the same 
under the (BV) hypothesis. ⨅⨆
13.9 Semiconcavity and the Value Function 
In this section, we introduce an important property of the value function, called 
semiconcavity, and discuss its significance. 
Definition 13.9.1 Take set K ⊂ Rk and x¯ ∈ K. We say that a function φ : Rk → R
is semiconcave near x¯ relative to K, if there exist some ϵ > 0 and a modulus of 
continuity θ : [0,∞) → [0,∞) such that 
λφ(x) + (1 − λ)φ(x'
) − φ(λx + (1 − λ)x'
) ≤ λ(1 − λ)|x − x'
| × θ (|x − x'
|) ,
for all x, x' ∈ (x¯ + ϵB) ∩ K and λ ∈ [0, 1] . 
The statement ‘φ is semiconcave on K’ is interpreted as‘ φ is semiconcave near 
all points x in K, relative to K’. If K = Rk, we omit the qualifier ‘relative to K’. 
We say that φ is ‘locally semiconcave’ if it is semiconcave on K for every compact 
subset K ⊂ Rk. 
Recall the definition of ‘φ is concave near x¯’ : for some ϵ > 0, 
λφ(x) + (1 − λ)φ(x'
) − φ(λx + (1 − λ)x'
) ≤ 0,
for all x, x'
, ∈ ¯x + ϵB and λ ∈ [0, 1]. Thus, semiconcavity is a weakened version of 
concavity (on a neighbourhood of a given base point), in which ‘0’ on the right side 
of the defining inequality is replaced by a scaled, superlinear error term.724 13 Dynamic Programming
The following proposition identifies one situation in which a function is semi￾concave near a point in its domain. 
Proposition 13.9.2 Take a point x¯ ∈ Rk and a function φ : Rk → R that is 
continuously differentiable on a neighbourhood of x¯. Then φ is semiconcave near x¯. 
Proof Let ϵ > 0 be such that φ is continuously differentiable on an open set 
containing x¯ + ϵB. Take any λ ∈ [0, 1] and x, x' ∈ ¯x + ϵB. Then, by the mean 
value theorem, there exists t,t' ∈ [0, 1] such that 
φ(λx + (1 − λ)x'
) = φ(x) + ∇φ(x + t (1 − λ)(x' − x))(1 − λ)(x' − x)
and 
φ(λx + (1 − λ)x'
) = φ(x'
) + ∇φ(x' + t
'
λ(x − x'
))λ(x − x'
).
Multiplying across the first and second equalities by λ and (1 − λ) respectively and 
addition yields 
φ(λx + (1 − λ)x'
) = λφ(x) + (1 − λ)φ(x'
)
+(∇φ(x + t (1 − λ)(x' − x))
−∇φ(x' + t
'
λ(x − x'
)))λ(1 − λ)(x' − x).
It follows that 
λφ(x) + (1 − λ)φ(x'
) − φ(λx + (1 − λ)x'
) ≤ λ(1 − λ)|x − x'
| × θ (|x − x'
|) ,
where θ is a modulus of continuity of ∇φ on x¯ + ϵB . ⨅⨆
If we know that a function is semiconcave, we can draw a number of useful 
conclusions about subgradients to the function. These are well-documented in 
the literature (See [52]). Of special significance for dynamic programming is the 
following property: 
Proposition 13.9.3 Take point x¯ ∈ Rk and a function φ : Rk → R. Assume that φ
is semiconcave near x¯ and , for some ξ ∈ Rk, 
ξ ∈ ∂P φ(x). ¯
Then φ is Fréchet differentiable at x¯ and ∇φ(x)¯ = {ξ }. 
Proof Take ξ ∈ ∂P φ(x)¯ . By definition of the proximal subgradient, there exists 
ϵ > 0 and M > 0 such that 
ξ · d ≤ φ(x¯ + d) − φ(x)¯ + M|d|
2, for all d ∈ ϵB . (13.9.1)13.9 Semiconcavity and the Value Function 725
Using the fact that φ is semiconcave near x¯, we can arrange (by reducing the size of 
ϵ and choosing and appropriate continuity modulus θ) that, for all d ∈ ϵB, 
φ(x) ( ¯ = φ(
1
2
(x¯ + d) +
1
2
(x¯ − d)) ≥
1
2
φ(x¯ + d) +
1
2
φ(x¯ − d) − 1
2
|d| × θ (2|d|) ,
This inequality can be equivalently expressed: 
φ(x)¯ − φ(x¯ − d) ≥ φ(x¯ + d) − φ(x)¯ − |d| × θ (2|d|) . (13.9.2) 
We deduce from (13.9.1) and (13.9.2) that 
ξ · d ≥ φ(x)¯ − φ(x¯ − d) − M|d|
2 ≥ φ(x¯ + d) − φ(x)¯
−|d| × θ1(|d|) , for all d ∈ ϵB ,
in which θ1 is the continuity modulus θ1(r) := M|r| + θ (2r). Combining this 
inequality with (13.9.1) yields 
|ξ · d − (φ(x¯ + d) − φ(x)) ¯ |≤|d| × θ2(|d|), for all d ∈ ϵB ,
in which θ2 is the continuity modulus θ2(r) := M|r|
2 + θ1(r). We have confirmed 
that η is Fréchet differentiable at x¯. ⨅⨆
We now bring the value function into the picture. Recall that the pointwise 
infimum of a family of concave functions, which have a uniform lower bound, is also 
concave. As we have shown, smooth functions are semiconcave. We might expect 
then that, under appropriate hypotheses, the value function, which after all is defined 
as a pointwise infimum of more regular functions, is semiconcave. This is indeed the 
case for smooth dynamic optimization problems with no right endpoint constraints, 
in which the dynamic constraint takes the form of a controlled differential equation. 
We restrict attention henceforth to the dynamic optimization problem: 
(Q)
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize g(x(T ))
over arcs x ∈ W1,1([S,T ]; Rn) satisfying
x(t) ˙ ∈ f (t, x(t), u(t)) a.e.,
u(t) ∈ U a.e.,
x(S) = x0,
the data for which comprise an interval [S,T ] ⊂ R, a function g : Rn → R,a 
function f : [S,T ] × Rn × Rm → Rn, a (nonempty) subset U ⊂ Rm and a point 
x0 ∈ Rn. 
The value function V : [S,T ] × Rn → R for (Q) is defined in the usual way: 
V (t, x) := inf{Qt,x }, where the right side denotes the infimum cost of a modified 
version of problem (Q), in which the generic initial data point (t, x) replaces (S, x0).726 13 Dynamic Programming
Proposition 13.9.4 Let V be the value function for problem (Q). Assume that 
(H1): f is continuous, the set {f (t, x, u) : u ∈ U} is closed for all (t, x) ∈
[S,T ] × Rn, and there exists cf > 0 such that |f (t, x, u)| ≤ cf (1 + |x|)
for all (t, x, u) ∈ [S,T ] × Rn × U, 
(H2): there exists kf >0 such that |f (t, x, u)−f (t'
, x'
, u)| ≤ kf (|t−t'
|+|x−x'
|)
for all t,t' ∈ [S,T ], x, x' ∈ Rn and u ∈ U, 
(H3): for each R > 0 there exists a continuity modulus θ : [0,∞) → [0,∞)
such that 
|λf (t, x, u) + (1 − λ)f (t, x'
, u) − f (t, λx + (1 − λ)x'
, u)|
≤ λ(1 − λ)|x − x'
| × θ (|x − x'
|)
for all t ∈ [S,T ], x, x' ∈ ¯x + RB and u ∈ U, 
(H4): g is locally Lipschitz continuous and locally semiconcave. 
Then V is locally Lipschitz continuous and locally semiconcave on [S,T ] × Rn. 
A proof of this proposition, which we omit, is given in [52]. The analysis involved 
is similar to that used earlier in this chapter to prove local Lipschitz continuity 
properties of the value function, but adapted to exploit hypotheses (H3) and (H4). 
Remark 
Hypothesis (H3) is satisfied in the special case when f (t, ., u) is continuously 
differentiable for each t ∈ [S,T ], u ∈ U and, for each R > 0, there exists a 
continuity modulus θ : [0,∞) → [0,∞) such that 
|∇xf (t, x1, u) − ∇xf (t, x2, u)| ≤ θ (|x1 − x2|)
for all t ∈ [S,T ], x1, x2 ∈ RB and u ∈ U .
We shall say that a locally Lipschitz function φ : [S,T ] × Rn → R is a classical 
Lipschitz solution to the Hamiltonian Jacobi equation when 
(H J E)Q
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
∇tφ(t, x) + inf
u∈U
∇xφ(t, x) · f (t, x, u) = 0
for all points (t, x) ∈ (S, T ) × Rn
at which φ is Fréchet differentiable
φ(T , x) = g(x), for all x ∈ Rn .
(Notice that the notion of classical Lipschitz solution is similar to, but slightly 
different from, that of ‘almost everywhere solution’ referred to in the introductory 
discussion to this chapter. The latter requires satisfaction of the Hamilton Jacobi 
equation only at almost every point at which φ is Frechêt differentiable. But to ´
qualify as classical Lipschitz solution, it must satisfy the equation at every such 
point.)13.9 Semiconcavity and the Value Function 727
It is now a simple step to provide a characterization of the value function in 
terms of locally Lipschitz continuous, semiconcave functions on [S,T ] × Rn that 
are classical Lipschitz solutions to (H J E)Q. 
Theorem 13.9.5 Consider problem (Q). Assume hypotheses (H1)–(H4) of Propo￾sition 13.9.4. Then the value function V is the unique classical Lipschitz solution to 
(H J E)Q, in the class of functions φ : [S,T ] × Rn → R that are locally Lipschitz 
continuous and semiconcave on [S,T ] × Rn. 
Proof Notice to begin with that, in consequence of the relaxation theorem (Theo￾rem 6.5.2), the value function V for problem (Q) coincides with the value function 
for its relaxed counterpart: 
(Q)relaxed
⎧
⎪⎪⎨
⎪⎪⎩
Minimize g(x(T ))
over arcs x ∈ W1,1([S,T ]; Rn) satisfying
x(t) ˙ ∈ co F (t, x(t)) a.e.,
x(S) = x0,
in which co F (t, x) := {n
i=0 λif (t, x, ui) : λ ∈ Λn and u0,...,un ∈ U}. 
Here Λn := {λ0,...,λn} ∈ (R+)n+1 : n
i=0 = 1}. The data for (Q)relaxed
satisfies the hypotheses of Theorem 13.3.7. 
We know from Proposition 13.9.4 that the value function V belongs to the class 
of locally Lipschitz continuous functions that are semiconcave on [S,T ] × Rn. 
From Theorem 13.3.7, V is a lower Dini solution to (H J E)Q. This implies that, 
for any point (t, x) ∈ (S, T ) × Rn at which V is Fréchet differentiable (cf. 
Lemma 13.3.2(iii)), 
− ∇tV (t, x) − ∇xV (t, x) · f (t, x, u) = D↑((t, x);(−1, −f (t, x, u))) ≤ 0
for all u ∈ U and 
∇tV (t, x) +n
i=0
λi∇xV (t, x) · f (t, x, ui) = D↑((t, x);(1,
n
i=0
λif (t, x, ui))) ≤ 0
for some u0,...,un ∈ U and λ ∈ Λn. We deduce from the preceding relation, by 
means of a simple contradiction argument, that 
∇tV (t, x) +n
i=0
λi∇xV (t, x) · f (t, x, u)˜ ≤ 0
for some u˜ ∈ U. But then ∇tV (t, x) + minu∈U ∇xV (t, x) · f (t, x, u) = 0. Since, 
by definition of the value function, V (T , x) = g(x) for all x ∈ Rn, we have shown 
that V is a classical Lipschitz solution to the Hamilton Jacobi equation for (Q).728 13 Dynamic Programming
It remains then to show that any locally Lipschitz continuous, semiconcave 
function φ on [S,T ] × Rn that is a classical Lipschitz solution φ to (H J E)Q must 
be the value function. 
Take such a function φ. Let (t, x) ∈ (S, T ) × Rn and a vector ξ be such that 
(ξ0, ξ1) ∈ ∂P φ(t, x) .
Then, according to Proposition 13.9.3, φ is Fréchet differentiable at (t, x) and 
(ξ0, ξ1) = ∇φ(t, x). Since φ is a classical Lipschitz solution to (H J E)Q, it follows 
that 
ξ0 + inf
u∈U
ξ1 · f (t, x, u) = 0 .
The function φ is continuous and therefore automatically satisfies the relevant 
boundary conditions. It follows that V is a proximal solution to the Hamilton Jacobi 
equation for (Q). But then, according to Theorem 13.3.7, φ = V . This is what we 
set out to show. ⨅⨆
Remark 
In the introduction, we raised the question of whether the value function V could 
be identified as the unique locally Lipchitz function that satisfies the Hamilton 
Jacobi equation at almost all points at which the function is Fréchet differentiable. 
Example 13.1 of the introductory discussion tells us the answer is ‘no’; here 
we constructed a locally Lipschitz continuous function, different from the value 
function, which satisfies both the relevant boundary condition and the Hamilton 
Jacobi equation at all points of Fréchet differentiability. We observe that this 
example is not in conflict with Theorem 13.9.5, because the extra solution V˜ is 
not semiconcave on [S,T ] × Rn. 
13.10 The Infinite Horizon Problem 
In this section we study the following dynamic optimization problem, with dis￾counted running cost over an infinite horizon: 
(P∞)(x0)
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize  ∞
0 e−λtL(x(t), u(t))dt
over x ∈ W1,1
loc([0,∞); Rn) and
measurable functions u : [0,∞) → Rm satisfying
y(t) ˙ = f (y(t), u(t)), a.e. t ∈ [0,∞),
u(t) ∈ U, a.e. t ∈ [0,∞),
y(0) = x0,13.10 The Infinite Horizon Problem 729
in which f : Rn × Rm → Rn and L : Rn × Rm → R are given functions, U ⊂ Rm
is a given set, x0 ∈ Rn and λ > 0. 
Given t0 ∈ R, a pair of functions (x : [t0,∞) → Rn, u : [t0,∞) → Rm) is said 
to be a process on [t0,∞) if x is locally absolutely continuous, u is measurable, 
x(t) ˙ = f (x(t), u(t)) and u(t) ∈ U, for a.e. t ∈ [t0,∞). If t0 = 0 we simply say 
‘process’. 
Our earlier study of finite horizon problems would suggest that dynamic 
programming for infinite horizon problems should focus on properties of the value 
function W : R × Rn × R → R, generated by the family of perturbed problems 
in which the initial value of the time, state and the accumulated cost are arbitrary 
points (t0, x0, y0) ∈ R × Rn × R: 
(Q∞)(t0, x0, y0)

Minimize y0 +  ∞
t0 e−λsL(x(s), u(s))ds
over processes (x, u) on [t0,∞) such that x(t0) = x0.
Thus, 
W (t0, x0, y0) := inf{y0 +
 ∞
t0
e−λsL(x(s), u(s))ds :
(x, u) is a process on [t0,∞) s.t. x(t0) = x0}.
Notice that functions f and L and the set U do not depend on the independent 
variable t. This means that, if (x, u) is a process on [0,∞), then t → (x(t−h), u(t−
h)) is also a process, now on [h,∞), for any time shift h. Consequently 
W (t0, x0, y0) = y0 + ϵ−λt0V (x0) for all (t0, x0, y0) ∈ R × Rn × R,
where V : Rn → R is the function 
V (x0) := inf{
 ∞
0
e−λsL(y(s), u(s))ds :
(y, u) is a process on [0,∞) s.t. y(t) = x0}.
As we shall see, V is associated with the Hamilton Jacobi equation 
− λV (x) + inf
u∈U

∇V (x) · f (x, u) + L(x, u)
= 0 . (13.10.1) 
The function V provides the same information as W, but more economically, since 
it is a function of the state variable alone. For this reason, dynamic programming for 
infinite horizon problems with a discounted integral cost, in which f , L and U are 
independent of time, is typically centred on the one-parameter value function V and 
on efforts to identify this function as a unique solution (in some sense) to (13.10.1). 
We shall impose the following hypotheses730 13 Dynamic Programming
(H1): λ > 0, f is continuous, U is compact and 
{(v, α) ∈ Rn × R : v ∈ f (x, u), α ≥ L(x, u), for some u ∈ U}
is convex for each x ∈ Rn ,
(H2): there exist positive constants kf and cf such that 
|f (x, u) − f (y, u)| ≤ kf |x − y| and |f (x, u)| ≤ cf (1 + |x|)
for all x, y ∈ Rn and u ∈ Rm,
(H3): there exist positive constants kL and cL such that 
|L(x, u) − L(y, u)| ≤ kL|x − y| and |L(x, u)| ≤ cL
for all x, y ∈ Rn and u ∈ Rm.
Take t0 ∈ R, x0 ∈ Rn and a arbitrary measurable u : [t0,∞) → Rm such that 
u(t) ∈ U, a.e.. Then, in consequence of (H2), there exists a unique locally 
absolutely continuous x : [t0,∞) such that (x, u) is a process on [t0,∞)
satisfying x(t0) = x0. 
In more general treatments of the infinite horizon problem, the definition of the 
integral in the cost requires special attention, in situations where the integrand is 
not integrable for some processes. Note that, under our hypotheses, such difficulties 
do not arise, because, for any control process (x, u) on [0,∞), the absolute value 
of integrand in dominated by the integrable function t → cLe−λt . Note also that, 
under these hypotheses, the cost admits the uniform bound 
|
 ∞
0
e−λtL(x(t), u(t))dt| ≤ cL λ−1 for all processes (x, u).
This means that V is well defined (as an infimum over a non-empty set) and V is 
bounded. 
Proposition 13.10.1 Assume (H1)–(H3). 
(i): For each x0 ∈ Rn (P∞)(x0) has a minimizing process on [0,∞), 
(ii): For any x0 ∈ Rn, T ≥ 0 and process (x, u) such that x(0) = x0 we have 
V (x(0)) ≤
 T
0
e−λsL(x(s), u(s))ds + e−λT V (x(T )) ,
(iii): For any x0 ∈ R, T ≥ 0 and minimizing process (x,¯ u)¯ for (P∞)(x0) we have 
V (x(¯ 0)) =
 T
0
e−λsL(x(s), ¯ u(s))ds ¯ + e−λT V (x(T )) , ¯13.10 The Infinite Horizon Problem 731
Proof The proofs of (ii) and (iii) (a statement of the principle of optimality 
for infinite horizon problems) are based on simple contradiction arguments. We 
therefore attend only to the proof of (i). 
(i) Let (xi, ui) be a minimizing sequence for (P∞)(x0). Then t → (xi(t), yi :=
 t
0 e−λsL(xi(s), ui(s))ds) is a solution to the differential inclusion 
(x,˙ y)˙ ∈ F (t, x), ˜ a.e. t ∈ [0,∞) , (x(0), y(0)) = (x0, 0) ,
in which 
F (t, x) ˜ := {(f (x, u), α) : for some α ∈ e−λt[L(x, u), cL] and u ∈ U}.
(13.10.2) 
The restrictions of the functions (xi, yi) to any interval [0, T ] , i = 1, 2,... is 
uniformly bounded with uniformly integrably bounded derivatives. We can use this 
property to extract subsequences to ensure uniform convergence of the functions 
and weak L1 convergence of their derivatives on each interval in a sequence of 
finite intervals [0, Tj ], Tj ↑ ∞, and then extract an appropriate diagonal sequence 
to ensure that the limiting function does not depend on the index j . In this way, and 
with the help of Theorem 6.3.3 (notice that, because of (H1), the relevant convexity 
hypothesis is satisfied), we can find a solution to the differential inclusion (x,¯ y)¯
on [0,∞) with the following properties. (˙
x,¯ ˙
y)¯ is an integrable function such that 
(˙
x,¯ ˙
y)¯ = (f (x(t), ¯ u(t)), ¯ α(t)) ¯ , u(t) ¯ ∈ U a.e. t ∈ [0,∞), for measurable functions 
α¯ and u¯ such that α(t) ¯ ≥ e−λtL(x(t), ¯ u(t)) ¯ a.e. t ∈ [0,∞), (x,¯ u)¯ is a process 
satisfying x(¯ 0) = x0 and 
lim
i→∞  ∞
0
e−λtL(xi(t), ui(t))dt ≥
 ∞
0
e−λtL(x(t), ¯ u(t))dt. ¯
The last relation confirms that (x,¯ u)¯ is a minimizer for (P∞)(x0). 
⨅⨆
The following proposition provides regularity properties of the value function V
and tells us that it is a proximal solution to the Hamilton Jacobi equation (13.10.1). 
Proposition 13.10.2 Assume (H1)–(H3). 
(i): V : Rn → R is a bounded, uniformly continuous function, 
(ii): for any x ∈ Rn such that ∂P V (x) = ∅ / , 
− λV (x) + inf
u∈U
(η · f (x, u) + L(x, u)) = 0 for all η ∈ ∂P V (x) .
Proof 
(i): We have already observed that V is a bounded function. We show that V is also 
uniformly continuous.732 13 Dynamic Programming
We may assume, without loss of generality, that kf > λ. Take any δ > 0 and x0 ∈
Rn. For ϵ > 0, choose any y0 ∈ x0 + ϵB. Then V (x0) =  ∞
0 e−λtL(x(t), u(t))dt
for some process (x, u) such that x(0) = x0 (a minimizer for (P∞)(x0)). It follows 
from Cor. 6.2.5 of Filippov’s existence theorem (Theorem 6.2.3) that there exists 
y ∈ W1,1(0,∞) such that (y, u) is a process satisfying y(0) = y0. Take T > 0. We 
have 
V (x0) ≥
 T
0
e−λtL(x(t), u(t))dt − λ−1cLe−λT
≥
 T
0
e−λtL(y(t), u(t))dt − kL|x0 − y0|
 T
0
e(kf −λ)tdt − λ−1cLe−λT
=
 T
0
e−λtL(y(t), u(t))dt − kL|x0
−y0|(kf − λ)−1(e(kf −λ)T − 1) − λ−1cLe−λT
=
 ∞
0
e−λtL(y(t), u(t))dt − kL|x0 − y0|(kf − λ)−1(e(kf −λ)T − 1)
−2λ−1cLe−λT
≥ V (y0) − kL|x0 − y0|(kf − λ)−1(e(kf −λ)T − 1) − 2λ−1cLe−λT .
This means that, if we choose T such that 2λ−1cLe−λT ≤ δ/2 and then adjust the 
size of ϵ > 0 such that 
kL(kf − λ)−1(e(kf −λ)T − 1)ϵ ≤ δ/2,
then V (x0)−V (y0) ≥ −δ. Reversing the roles of x0 and y0 gives V (y0)−V (x0) ≥
−δ. So |V (x0)−V (y0)| ≤ δ. Notice that the choice of ϵ > 0 to achieve this estimate 
did not depend on x0. It follows that V is uniformly continuous. 
(ii): Take any x0 ∈ Rn such that ∂P V (x0) is non-empty. Let η ∈ ∂P V (x0). 
Fix any v ∈ U. Take h0 > 0 sufficiently small and write y˜ for the solution 
to the equation y(t) ˙ = f (y(t), v) ˜ a.e. t ∈ [0, h0] such that y(0) = x0, where 
f (y, v) ˜ := −f (y, v). Define x(t) := ˜y(−t) for all t ∈ [−h0, 0]. Observe that 
x(−h0) = ˜y(h0), x(0) = ˜y(0) and x(t) ˙ = f (x(t), v), a.e. t ∈ [−h0, 0]. We 
extend the process (x, v) (defined on [−h0, 0]) to a process (still written (x, v)) 
on [−h0,∞). Using the fact that η is a proximal subgradient and applying (ii) of 
Proposition 13.10.1 to the process (x, v) with T = h, we deduce the existence of 
c > 0 such that, for all h ∈ [0, h0],13.10 The Infinite Horizon Problem 733
η ·
 −h
0
f (x(s), v)ds − ch2
≤ V (x0 +
 −h
0
f (x(s), v)ds) − V (x0)
= V (x(−h)) − e−λhV (x(0)) − (1 − e−λh)V (x0)
≤ eλh  h
0
e−λsL(x(s − h), v)ds − (1 − e−λh)V (x0) .
Dividing across this inequality by h and passing to the limit as h ↓ 0 gives 
− λV (x0) + η · f (x0, v) + L(x0, v) ≥ 0 . (13.10.3) 
Now let (x,¯ u)¯ be a minimizing process for (P∞)(x0). Apply (iii) of Proposi￾tion 13.10.1 to the process (x,¯ u)¯ with T = h. Again using also the fact that η is 
a proximal subgradient, we deduce the existence of c > 0 such that, for h > 0
sufficiently small, 
η ·
 h
0
f (x(s), ¯ u(s))ds ¯ − ch2 ≤ V (x0 +
 h
0
f (x(s), ¯ u(s))ds) ¯ − V (x0)
= e−λhV (x0 +
 h
0
f (x(s), ¯ u(s))ds) ¯ − V (x0) + (1 − e−λh)V (x0
+
 h
0
f (x(s), ¯ u(s))ds) ¯
= −  h
0
e−λsL(x(s), ¯ u(s))ds ¯ + (1 − e−λh)V (x0 +
 h
0
f (x(s), ¯ u(s))ds) . ¯
(13.10.4) 
We deduce from the convexity hypothesis (H1) that there exists hi ↓ 0 and v¯ ∈ U
such that 
h−1
i
 hi
0 f (x(s), ¯ u(s))ds ¯ → f (x0, v)¯ and
limi→∞ h−1
i
 hi
0 e−λsL(x(s), ¯ u(s))ds ¯ ≥ L(x0, v). ¯
Set h = hi in (13.10.4). Dividing across (13.10.4) by hi and passing to the limit as 
i → ∞ gives 
− λV (x0) + η · f (x0, v)¯ + L(x0, v)¯ ≤ 0 . (13.10.5) 
We conclude from (13.10.3), which is satisfied for all v ∈ U, and from (13.10.5), 
which is valid for some v¯ ∈ U, that734 13 Dynamic Programming
− λV (x0) + inf
v∈U

η · f (x0, v) + L(x0, v)
= 0 .
Since this last relation is true for any x0 ∈ Rn at which ∂P V (x) /= ∅ and for any 
η ∈ ∂P V (x) = ∅ / , we have confirmed that V is a proximal solution to the Hamilton 
Jacobi equation. ⨅⨆
Theorem 13.10.3 Assume (H1)–(H3). Then the value function V is the unique 
bounded continuous function such that, for each x ∈ Rn such that ∂P V (x) /= ∅, 
− λV (x) + inf
u∈U

η · f (x, u) + L(x, u)
= 0 for all η ∈ ∂P V (x) .
Proof We have shown that the value function V is a continuous, bounded proximal 
solution to the Hamilton Jacobi equation. It remains then to prove that it is the 
unique such solution. 
Let V ' be an arbitrary continuous, bounded proximal solution of the Hamilton 
Jacobi equation. Take any x0 ∈ Rn, y0 ∈ R and T > 0. Let W : (−∞,∞) × Rn ×
R → R be the function 
W (t, x, y) := y + e−λtV '
(x) .
Now consider the problem 
(R∞)(x0, y0)
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
Minimize y(T ) + e−λT V '
(x(T ))
over (x, y) ∈ W1,1
loc([0,∞); Rn+1) satisfying
(x,˙ y)(t) ˙ ∈ {(f (x(t), u), α) :
α ∈ e−λt[L(x(t), u), cL] and u ∈ U},
x(0) = x0 and y(0) = y0 .
Take any point (t, x, y) ∈ R × Rn × R at which ∂P W (t, x, y) is non-empty and 
(ξ0, ξ1,ξ) ∈ ∂P W (t, x, y). It follows from the definition of W that ∂P V '
(x) /=
∅ and (ξ0, ξ1, ξ2) = (−λeλtV '
(x), eλtη, 1) for some η ∈ ∂P V '
(x). But V ' is a 
proximal solution to the Hamilton Jacobi equation so 
− λV '
(x) + inf
u∈U

η · u + L(x, u)
= 0 .
It follows that 
− e−λtλV '
(x) + inf
u∈U
{e−λtη · v(1) + v(2) : (v(1)
, v(2)
) ∈ F (t, y, x) ˜ } = 0 .
(F˜ was defined in (13.10.2)). 
But then 
ξ0 + min{ξ1 · v(1) + ξ2v(2) : (v(1)
, v(2)
) ∈ F (t, y, x) ˜ } = 0 .13.11 The Minimum Time Problem 735
We have confirmed that W is a proximal solution of the Hamilton Jacobi equation 
for problem (R∞)(x0, y0). Notice also that W also satisfies the right boundary 
condition 
W (T , x, y) = y + e−λT V '
(x) for all x, y ∈ Rn .
But we know from Theorem 13.3.7 that W is then the value function for problem 
(R∞)(x0, y0). It follows in particular that 
W (0, x0, 0) = inf{
 T
0
e−λtL(x(t), u(t))dt
+ e−λT V '
(x(T )) : (x, u) is a process s.t. x(0) = x0}.
We can now deduce from the boundedness of V ' and L the uniform bound 
|
 T
0
e−λtL(x(t), u(t))dt + e−λT V '
(x(T ))
−
 ∞
0
e−λtL(x(t), u(t))dt| ≤ (c0 + cLλ−1)e−λT ,
over all processes (x, u) on [0,∞) such that x(0) = x0, in which c0 := sup
x∈Rn
V '
. 
Since this estimate is valid for all T > 0, we can conclude that 
V '
(x0) = W (0, x0, 0)
= inf{
 ∞
0
e−λtL(x(t), u(t))dt : (x, u) is a process on [0,∞) s.t. x(0)
= x0}.
We have shown that V ' is the value function for (P∞)(x0). ⨅⨆
13.11 The Minimum Time Problem 
In minimum time control, the goal is to find a control strategy that drives the given 
initial state of a control system to a specified ‘target’ set in the state space in 
minimum time. In this section we treat the following version of the minimum time 
problem in which the dynamic constraint takes the form of a differential inclusion. 
For each x ∈ Rn consider then736 13 Dynamic Programming
(Px )
⎧
⎪⎪⎨
⎪⎪⎩
Minimize T
over T ≥ 0 and x ∈ W1,1([0, T ]; Rn) satisfying
y(t) ˙ ∈ F (y(t)), a.e. t ∈ [0, T ),
y(0) = x and y(T ) ∈ S.
in which F : Rn ⇝ Rn is a given multifunction and S ⊂ Rn is a given closed set . 
The value function T : Rn → R ∪ +∞ is 
T (x) := inf{(Px )}, for each x ∈ Rn .
In other words, T (x) is the infimum cost of problem (Px ). (We interpret the infimum 
cost T (x) = +∞ if there is no F trajectory y, starting at x, that reaches S in finite 
time.) 
Throughout this section, we shall refer to the following hypotheses: 
(H1): F (x) is a closed, convex, non-empty set for each x ∈ Rn, 
(H2): there exist constants kF and cF such that 
F (x'
) ⊂ F (x) + kF |x − x'
|B and F (x) ⊂ cFB for all x'
, x ∈ Rn .
Proposition 13.11.1 (Properties of the Value Function) Let T be the value 
function. Assume (H1) and (H2). Then 
(a): (Px ) has a minimizer for each x (we allow the possibility that inf{Px } = +∞, 
i.e. there are no F trajectories starting at the specified x and arriving at S in 
finite time). T is non-negative valued and lower semi-continuous, 
(b): Take any x ∈ Rn. Then 
1 + min
v∈F (x)ξ · v ≥ 0, for all ξ ∈ ∂P T (x),
(c): Take any x ∈ Rn \ S. Then 
1 + min
v∈F (x)ξ · v ≤ 0, for all ξ ∈ ∂P T (x).
Proof 
(a): Obviously the value function is non-negative valued. Existence of a minimizers 
for (Px ) and the lower semicontinuity of the value function are proved by a 
straightforward adaptation (to allow for processes with variable end-times) of the 
proof of Proposition 6.5.3 and Proposition 13.3.6 . 
(b): We can assume T (x) < +∞ since otherwise ∂P T (x) = ∅. Take any v ∈ F (x), 
ξ ∈ ∂P T (x) and ϵ > 0. Then, in consequence of Filippov’s existence theorem, there 
exists an F trajectory y : [−ϵ, 0] → Rn such that y(0) = x and13.11 The Minimum Time Problem 737
ϵ−1(y(0) − y(−ϵ)) → v, as ϵ ↓ 0 .
A simple contradiction argument tells us that T (y(−ϵ)) < +∞ and 
T (y(−ϵ)) − T (x) ≤ ϵ.
(This inequality can be regarded as part the principle of optimality for the minimum 
time problem.) Then by the definition of proximal subgradients and the Lipschitz 
continuity of y, there exists M > 0 such that, for ϵ > 0 sufficiently small, 
ξ · (y(−ϵ) − y(0)) ≤ T (y(−ϵ)) − T (y(0)) + Mϵ2 ≤ ϵ + Mϵ2 .
Dividing across by ϵ, using the preceding relations and passing to the limit as ϵ ↓ 0
yields 
− ξ · v = lim
ϵ↓0
ξ · ϵ−1(y(−ϵ) − y(0)) ≤ 1
Since v was an arbitrary point on F (x), we have shown that 1 + min
v∈F (x)ξ · v ≥ 0. 
(c): We can, once again, assume without loss of generality that T (x) < +∞. Since 
x /∈ S, it follows that (Px ) has a minimizer y : [0, T ] → Rn such that T > 0.A 
simple contradiction tells us that there exists ϵ ↓ 0 such that, for each i
T (y(0)) = T (y(ϵi)) + ϵi .
(This equality is the other part of the principle of optimality.) Since y is Lipschitz 
continuous, {ϵ−1
i (y(ϵi)−y(0))} is a bounded sequence. After extraction of a suitable 
subsequence then 
ϵ−1
i (y(ϵi) − y(0)) → ¯v, as i → ∞,
for some v¯ ∈ Rn. A straightforward contradiction argument, based on the convexity 
of F (x), the regularity properties of F and application of the separation theorem, 
tells us that v ∈ F (x) (see the proof of Theorem 13.3.7). 
Take any ξ ∈ ∂P T (x). By definition of proximal subgradients and the Lipschitz 
continuity of y, there exists M > 0 such that, for all i sufficiently large, 
ξ · (y(ϵi) − y(0)) ≤ T (y(ϵi)) − T (y(0)) + Mϵ2
i = −ϵi + Mϵ2
i .
The preceding relations tell us that 
1 + ξ · ¯v = 1 + lim
i→∞ϵ−1
i (y(ϵi)) − y(0)) ≤ 0.
We have shown that, for some v¯ ∈ F (x), 1 + ξ · ¯v ≤ 0. ⨅⨆738 13 Dynamic Programming
Theorem 13.11.2 Consider problem (Px ). Assume (H1) and (H2). The value 
function T is the unique function V that satisfies the following conditions 
(i): V is lower semi-continuous and bounded below, 
(ii): for every x ∈ Rn \ S, 
1 + min
v∈F (x)ξ · v = 0, for all ξ ∈ ∂P V (x),
(iii): for every x ∈ S, 
1 + min
v∈F (x)ξ · v ≥ 0, for all ξ ∈ ∂P V (x),
(iv): V (x) = 0, for x ∈ S. 
Remarks 
(A): This theorem, due to Wolenski and Zhuang [208] and building on earlier 
work by Clarke et al. [84], is remarkable for the unrestrictive nature of the 
hypotheses that are invoked. The target set is required merely to be a closed 
subset of the state-space. In these circumstances the value function may be 
discontinuous and take infinite values at points in its domain. To achieve a 
dynamic programming type characterization of the value function, we might 
expect the need for additional hypotheses involving the regularity of the 
boundary of the target set or concerning the way the target set interacts with 
the dynamic constraint. But any such hypotheses are entirely absent from the 
theorem statement. 
(B): Condition (iii) is automatically satisfied on the interior of S, so it is of interest 
only on the boundary of this set ∂S. It is helpful to regard (ii) as the statement 
that the value function satisfies the Hamilton Jacobi equation for the minimum 
time problem (in the proximal normal sense), with domain the open set Rn \ S
and to interpret condition (iii) as boundary condition on the boundary points 
of Rn \ S. 
Proof We have already shown that the value function T : Rn → R∪{+∞} satisfies 
conditions (i)–(iv). Now take any function V satisfying these conditions. It remains 
to show that V (x) = T (x) for all x ∈ Rn. To prove the theorem it suffices to 
complete the following two steps which, together, tell us that V (x) = inf{(Px )} =
T (x). 
Step 1: Take any x ∈ Rn. We show that 
V (x) ≥ inf{(Px )}.13.11 The Minimum Time Problem 739
We can assume that x ∈ Rn \ S since the relation is obviously satisfied for x ∈ S. 
We can further assume that V (x) < +∞ for otherwise there is nothing to prove. 
Define W (y, τ ) := V (y) + τ . To carry out this step, we consider the following 
control system with pathwise state constraint: 
⎧
⎨
⎩
(y(t), ˙ τ (t), ˙ a(t)) ˙ ∈ F (y(t)), ˜ a.e. t ∈ [0,∞),
(y(t), τ (t), a(t)) ∈ epi{W} for all t ∈ [0,∞),
y(0) = x,τ(0) = 0 and a(0) = W (x, τ ).
Here, 
F (y) ˜ := 
F (y) × {1}×{0} if y ∈ Rn \ S,
cFB × [−1, 1] × [−1, 1] if y ∈ S .
F˜ is a bounded, upper semi-continuous multifunction with values convex sets and 
its graph is closed. We now examine the inward pointing condition for application 
of a weak invariance theorem. Take any (y, τ, a) ∈ epi W and (η0, η1, −λ) ∈
NP
epi W (y, τ, a). Since the constraint set is an epigraph set, λ ≥ 0. We must show 
that 
min
e∈F (y,τ ) ˜ (η0, η1, −λ) · e ≤ 0 .
This is obviously true for y ∈ S because, in this case, F (y, τ ) ˜ contains 0 as an 
interior point. So assume that y ∈ Rn \ S. Then F (y) ˜ = F (y) × {1}×{0}. 
Suppose first that λ > 0. Then (η0/λ, η1/λ, −1) ∈ NP
epi W (y, τ, a), which (owing 
to Lemma 5.1.2) implies that (η0/λ) ∈ ∂P V (y) and η1/λ = 1. By condition (ii) in 
the theorem statement then, min
v∈F (y) (η0/λ) · v + η1/λ ≤ 0. It follows 
min
e∈F (y,τ ) ˜ (η0, η1, −λ) · e = λ min
v∈F (y)

(η0/λ) · v + η1/λ + 0

≤ 0 .
The other case to be considered is that when λ = 0. We show the inward pointing 
condition is true also in this case, by means of the earlier used technique of 
approximating ‘vertical’ proximal subgradients to the epigraph of V by non-vertical 
ones (cf. the proof of Theorem 13.3.7 ). 
All the hypotheses have been verified under which we can apply the weak 
invariance Theorem 13.2.1 for autonomous state-constrained systems. This tells 
us that there exists an F˜ trajectory (y, τ, a) : [0,∞) → Rn × R × R such 
(y, τ, a)(0) = (x, 0, V (x)) and a(t) ≥ W (y(t), τ (t)) for all t ≥ 0. 
Recall that x ∈ Rn \ S. Define T¯ := inf{T ' : y(T '
) /∈ Rn \ S}. (We allow 
T¯ = ∞, i.e. the case when y(t) ∈ Rn \ S for all t ≥ 0.) Take any finite T ' < T¯. 
Then, for a.e. t ∈ [0, T '
], y(t) ˙ ∈ F (y(t)), τ (t) = t and a(t) = 0. Thus y is an F
trajectory on [0, T¯] and, since (y(t)), τ (t), a(t)) ∈ epi{W} for all t ∈ [0, T '
]740 13 Dynamic Programming
V (x) = W (x, 0) = a(0) = a(T '
) ≥ W (y(T '
), τ (T '
)) = V (y(T '
)) + T ' .
Since V is bounded below, the T '
’s satisfying this relation must be uniformly 
bounded, This implies that T¯ is finite. Then, since y is continuous, y(T )¯ ∈ S and so 
V (y(T )) ¯ = 0. Recalling that V is lower semi-continuous, it follows that 
V (x) ≥ lim inf
T '
↑T¯ V (y(T '
)) + T¯ ≥ 0 + T¯ ≥ inf{(Px )}.
Step 2: Take any x ∈ Rn. We show that 
V (x) ≤ inf{(Px )}.
We can assume that inf{(Px )} < ∞ for, otherwise, there is nothing to prove. Take 
an arbitrary F trajectory y : [0, T¯] → Rn such that y(0) = x and y(T )¯ ∈ S. 
Define W (y, τ ) := V (y) + τ . Consider the control system: 
⎧
⎨
⎩
(z(s), ˙ τ (s), ˙ a(s)) ˙ ∈ −F (z(s)) × {−1}×{0}, a.e. t ∈ [0, T¯],
(y(s), τ (s), a(s)) ∈ epi{W} for all t ∈ [0, T¯],
z(0) = y(T ), τ ( ¯ 0) = T¯ and a(0) = T . ¯
We verify the inward pointing condition for application of a strong invariance 
theorem. Take any (z, τ, a) ∈ epi W and (η0, η1, −λ) ∈ NP
epi W (z, τ, a). Then 
λ ≥ 0. We must show that 
sup e∈(−F (z))×{−1}×{0}
(η0, η1, −λ) · e ≤ 0 .
We limit attention to the case λ > 0, since the case ‘λ = 0’ can be treated 
by looking at limits of non-vertical proximal gradients. Then (η0/λ, η1/λ, −1) ∈
NP
epi W (y, τ, a), which implies that η0/λ ∈ ∂P V (z) and η1/λ = 1. By conditions 
(ii) and (iii) in the theorem statement then, (η0/λ)· v' + η1/λ ≥ 0 for all v' ∈ F (z). 
It follows 
sup e∈(−F )(y)×{−1}×{0}
(η0, η1, −λ) · e = λ sup
v'
∈F (y)

− (η0/λ) · v' − η1/λ) + 0

≤ 0 .
The inward pointing condition has been confirmed. Notice also that the closed￾valued multifunction −F × {−1}×{0} has the requisite Lipschitz continuity 
property. 
Now apply the strong invariance Theorem 13.2.3 for autonomous state￾constrained systems with reference to the −F × {−1}×{0} trajectory 
(z(s), τ (s), a(s)) := (y(T¯ − s), T¯ − s, T )¯ . Notice that z(0) = y(T )¯ , τ (0) = T¯
and a(0) = T¯ = V (y(T )) ¯ + T¯ = W (y(T ), ¯ T )¯ . So (z(0), τ (0), a(0)) ∈ epi W. The 
theorem tells us that a(s) ≥ W (z(s), τ (s)) for all s ∈ [0, T¯]. But then13.12 Viscosity Solutions of the Hamilton Jacobi Equation 741
T¯ = a(0) = a(T )¯ ≥ W (z(T ), τ ( ¯ T )) ¯ = V (y(0)) + 0 = V (x) .
Since T¯ = inf{T ' : y(T '
) /∈ Rn \ S} is the cost of an arbitrary F trajectory y on 
[0, T¯] such that y(0) = x and y(T )¯ ∈ S, we have V (x) < ∞ and 
V (x) ≤ inf{(Px )}.
This completes the proof. ⨅⨆
13.12 Viscosity Solutions of the Hamilton Jacobi Equation 
Historically, the characterization of the value function for dynamic optimization 
problems in terms of viscosity solutions to the Hamilton Jacobi equation preceded 
those involving generalized solutions (by which we mean either lower Dini solutions 
or proximal solutions) developed in the previous sections. The viscosity solutions 
approach aims to derive comparison principles, uniqueness of solutions for appro￾priate boundary data, and stability of solutions under approximation, for Hamilton 
Jacobi equations, broadly interpreted. The proof techniques employed are based on 
a direct analysis of the Hamilton Jacobi equation and, apart from the proof that the 
value function is a viscosity solution of the Hamilton Jacobi equation that satisfies 
the specified boundary conditions, makes no use of properties of state trajectories. 
By contrast, the control theoretic approach pursued in this chapter is based, as we 
have seen, on a study of the invariance properties of state trajectories. The viscosity 
solutions approach is more general, in the sense that it can be applied to certain 
Hamilton Jacobi equations, even higher order partial differential equations, which 
are not associated with any deterministic dynamic optimization problem. On the 
other hand, the control theoretic approach has certain advantages, notably in the 
way it can take account of constraints, some of which we discuss in this section. 
The viscosity solutions approach is in widespread use. It is of interest then to 
examine how the information it supplies about dynamic optimization problems 
compares with that obtained by the constructive methods of this chapter. The 
purpose of this section is to make such comparisons. 
The Hamilton Jacobi equation (HJE) for problem (P) of the introduction can be 
written 
− Wt(t, x) + H (t, x, −Wx (t, x)) = 0. (13.12.1) 
in which H is, as usual, the Hamiltonian function 
H (t, x, p) := max
e∈F (t,x) p · e .742 13 Dynamic Programming
In the viscosity solutions literature, interpretation of (possibly discontinuous) 
solutions to (HJE) in the class locally bounded (possibly discontinuous) functions 
W, is based on the following definitions, in which W∗ and W∗ (referred to as the 
upper semi-continuous envelope and the lower semi-continuous envelope of W, 
respectively) are the functions: 
W∗(t, x) := lim sup
(t'
,x'
)→(t,x)
W (t'
, x'
) and W∗(t, x)
:= lim inf
(t'
,x'
)→(t,x)
W (t'
, x'
), for all (t, x) ∈ (S, T ) × Rn .
Definition 13.12.1 
(i): W is a viscosity supersolution of (13.12.1) if, for any point (t, x) ∈ (S, T ) ×
Rn and any C1 function ψ : R × Rn → R such that (t'
, x'
) → W∗(t'
, x'
) −
ψ(t'
, x'
) has a local minimum at (t, x), we have 
− ψt(t, x) + H (t, x, −ψx (t, x)) ≥ 0
(ii): W is a viscosity subsolution of (13.12.1) if, for any point (t, x) ∈ (S, T ) × Rn
and any C1 function ψ : R × Rn → R such that (t'
, x'
) → W∗(t'
, x'
) −
w(t'
, x'
) has a local maximum at (t, x), we have 
− ψt(t, x) + H (t, x, −ψx (t, x)) ≤ 0
(iii): W is a viscosity solution of (13.12.1) if it is both a viscosity supersolution and 
a viscosity subsolution. If W is a lower semi-continuous function satisfying 
conditions (i) and (ii) in the above definition, we say that W is a lower semi￾continuous viscosity solution. (Notice, we can replace W∗ by W in condition 
(ii) when W is lower semi-continuous, since, in this case, the two functions 
coincide.) 
Under suitable hypotheses on the data for problem (P) including a continuity 
requirement on F (t, x) regarding its t dependence, it is known that the value 
function for (P) is indeed the unique lower semi-continuous viscosity solution to 
(13.12.1) satisfying appropriate boundary conditions. 
The following theorem provides a characterization of this nature. While, as we 
have remarked, such characterizations are associated with the viscosity solutions 
literature, we offer an independent proof, based on constructive methods. We allow 
the multifunction F (t, x) to be discontinuous w.r.t. the t variable; supersolutions 
must therefore be interpreted in terms of the limit set F (t+, x) (in place of F (t, x)). 
We write H + for the modified Hamiltonian functions, in which F (t, x) is replaced 
by F (t+, x):13.12 Viscosity Solutions of the Hamilton Jacobi Equation 743
H +(t, x, p) := max e∈F (t+,x)
p · e .
Theorem 13.12.2 (Viscosity Solution Characterization of Lower semi-continuous 
Value Functions) Assume (H1)–(H5) of Theorem 13.3.7. Suppose, in addition, that 
(HS) g is locally bounded and satisfies (g∗)∗ = g. 
Take a lower semi-continuous, locally bounded function V : [S,T ] → R. 
Then V is the value function for (P ) if and only if V is a lower semi-continuous 
viscosity solution of (13.12.1) in the following sense: V is lower semi-continuous 
and 
(i) (V is a viscosity supersolution) for any point (t, x) ∈ (S, T ) × Rn and any C1
function ψ : R × Rn → R such that 
(t'
, x'
) → V (t'
, x'
) − ψ(t'
, x'
)
has a local minimum at (t, x) we have 
− ψt(t, x) + H +(t, x, −ψx (t, x)) ≥ 0 ,
(ii) (V is a viscosity subsolution) for any point (t, x) ∈ (S, T ) × Rn and any C1
function ψ : R × Rn → R such that 
(t'
, x'
) → V ∗(t'
, x'
) − ψ(t'
, x'
)
has a local maximum at (t, x) we have 
− ψt(t, x) + H +(t, x, −ψx (t, x)) ≤ 0 ,
(iii) (boundary conditions) For all x ∈ Rn, 
lim inf
{(t'
,x'
)→(S,x):t'
>S}
V (t'
, x'
) = V (S, x) ,
V (T , x) = g(x) and V ∗(T , x) = g∗(x).
Remarks 
(a): We take this opportunity to compare the ‘generalized solutions’ and ‘vis￾cosity solutions’ characterizations of the value function, provided by Theo￾rems 13.3.7 and 13.12.2. Dynamic optimization problems with discontinuous 
value functions are of interest primarily, as we have already remarked, because 
they include problems with a right endpoint constraint x(T ) ∈ C, in which C is 
a given closed, strict subset of Rn. In the generalized solutions characterization 
of Theorem 13.3.7, we accommodate such a constraint by replacing the 
original cost g by an extended valued cost744 13 Dynamic Programming
g(1)
(x) := 
g(x) if x ∈ C
+∞ otherwise
In the framework of Theorem 13.12.2, we can still deal with a right end￾point constraint, even though we now require the end-point cost function to 
be bounded on bounded sets; in place of g(1) we employ 
g(2)
(x) := 
g(x) if x ∈ C
+K otherwise
(If the velocity set F is bounded on bounded sets, we can arrange that the value 
function of the problem with cost function g(2) will coincide with that of the 
original problem on an arbitrary bounded domain, by choosing the constant K
sufficiently large.) A more serious issue is that, to apply Theorem 13.12.2, we 
need to assume that 
(g∗)∗ = g . (13.12.2) 
Notice that the function g(2) fails to satisfy this hypothesis if the set C has 
empty interior or, indeed, if intC /= C. This means that Theorem 13.12.2 
cannot be applied to dynamic optimization problems with a fixed endpoint 
(C = {x1}, for some x1). This difficulty does not arise, however, in the appli￾cation of Theorem 13.3.7, which does not require imposition of hypothesis 
(13.12.2). 
The following example shows that hypothesis (13.12.2) cannot be removed 
from the hypotheses of Theorem 13.12.2. 
⎧
⎨
⎩
Minimize g(x(1))
subject to
x(t) ˙ ∈ [−1, +1] a.e. t ∈ [0, 1]
in which 
g(x) := 
0 if x ∈ C
1 otherwise
and C := {0}. The value function is 
V (t, x) =

0 if |x| ≤ 1 − t
1 otherwise.
The data for this problem satisfy the hypotheses of Theorem 13.3.7 and, 
accordingly, V , as given above, is the unique lower semi-continuous function 
satisfying conditions of this theorem. Notice however that the function g in the13.12 Viscosity Solutions of the Hamilton Jacobi Equation 745
example fails to satisfy (13.12.2) and the function V is not the unique function 
that satisfies conditions (i)–(iii) of Theorem 13.12.2; the function 
V (t, x) ˜ =

0 if x = 0
1 otherwise
also satisfies the conditions. 
(b): In the case when the end-point cost function satisfies hypothesis (HS), 
Theorems 13.12.2 and 13.3.7 provide two different descriptions of the value 
function, in terms of viscosity solutions and proximal solutions to the Hamilton 
Jacobi equation. It follows that, under this additional hypothesis, a lower semi￾continuous function is a viscosity solution to the Hamilton Jacobi equation if 
and only if it is a proximal solution. The equivalence of these solution concepts 
was partly anticipated by Barron and Jensen [19] (see also [18]), who proposed 
a definition of generalized solution to the Hamilton Jacobi equation for lower 
semi-continuous functions, similar to that of ‘proximal’ solution, in which 
the strict subdifferential replaces the proximal subdifferential; they showed 
directly that continuous functions which were solutions to the Hamilton Jacobi 
equation in this sense are also viscosity solutions, in the sense of Def. 13.12.1. 
(c): The role of the right limits (see the definition of H +) in the value function 
viscosity solution characterization of Theorem 13.12.2 is crucial: this charac￾terization would become in general false if we try to substitute an Hamiltonian 
defined in terms of the left limit set F (t−, x) in place of H + (cf. Exercise 13.3). 
Proof The proof has the following structure. Assume (H1)–(H5). In Step 1 we show 
that, if g is locally bounded (in this step we do not require (g∗)∗ = g), then the 
value function satisfies conditions (i), (ii) and (iii) of the theorem statement. Steps 
2 and 3 are devoted to showing that a lower semi-continuous function satisfying 
conditions (i), (ii) and (iii) is the value function. More precisely, in Step 2 we prove 
that if V is a lower semi-continuous function, which satisfies conditions (i) and (iii), 
then ‘inf(Pt,x ) ≤ V (t, x)’. In Step 3 we show that if V is a lower semi-continuous 
locally bounded function satisfying conditions (ii) and (iii) and, in addition, (HS), 
then ‘inf(Pt,x ) ≥ V (t, x)’. 
Step 1 Let V be the value function. Assume that, in addition to (H1)–(H5), g is 
local bounded. We deduce from Proposition 13.3.6 that V is lower semi-continuous 
and locally bounded and so, in particular, dom V = [S,T ] × Rn. 
Fix any point (t, x) ∈ (S, T ) × Rn. Take a test function ψ such that V − ψ
achieves a local minimum at (t, x). Then, since ψ is continuously differentiable, for 
all (t'
, x'
) in a neighbourhood of (t, x), we have 
∇tψ(t, x)(t' − t) + ∇xψ(t, x) · (x' − x) ≤ V (t'
, x'
)
− V (t, x) + o(|(t' − t,x' − x)|) (13.12.3)746 13 Dynamic Programming
in which o : R+ → R+ is a function satisfying o(ϵ)/ϵ → 0 as ϵ ↓ 0. Arguing as in 
the proof of Theorem 13.3.7, we can find sequences hi ↓ 0 and vi → ¯v, for some 
v¯ ∈ F (t+, x), such that 
lim
i→+∞
h−1
i (V (t + hi, x + hivi) − V (t, x)) ≤ 0.
Taking (t'
, x'
) = (t + hi, x + hivi) in (13.12.3), and dividing across by hi, we 
obtain, for i sufficiently large, 
∇tψ(t, x)+∇xψ(t, x)·vi ≤ h−1
i (V (t+hi, x+hivi)−V (t, x))+h−1
i o(hi

1 + |vi|
2).
Therefore, in the limit as i → ∞, it follows that 
∇tψ(t, x) + inf v∈F (t+,x)
∇xψ(t, x) · v ≤ ∇tψ(t, x) + ∇xψ(t, x) · ¯v ≤ 0.
We have confirmed that the value function satisfies condition (i). 
Fix (t, x) ∈ (S, T ) × Rn. There exists a sequence of points (ti, xi) ∈ (S, T ) ×
Rn \ {(t, x)} such that (ti, xi) → (t, x) as i → ∞ and 
lim
i→∞ V (ti, xi) = V ∗(t, x). (13.12.4) 
We claim that we can restrict attention to the case when ti > t for all i. Indeed, 
either we can extract a subsequence (we do not relabel) such that ti > t for all 
i, or there exists i0 such that ti ≤ t for all i ≥ i0. In this latter case, we take a 
strictly decreasing sequence of elements τi ∈ (t, T ] such that τi ↓ t. Fix any i ≥ i0, 
and consider any F trajectory xi ∈ W1,1([ti, T ]; Rn) such that xi(ti) = xi. By the 
principle of optimality, 
V (ti, xi) ≤ V (τi, xi(τi)), for all i.
Since V ∗ is upper semi-continuous, we deduce: 
V ∗(t, x) = lim sup
i→∞
V (ti, xi) ≤ lim sup
i→∞
V (τi, xi(τi))
≤ lim sup
i→∞
V ∗(τi, xi(τi)) ≤ V ∗(t, x).
Hence, lim supi→∞ V (τi, xi(τi)) = V ∗(t, x). But then, along a subsequence (again, 
we do not relabel), 
lim
i→∞ V (τi, xi(τi)) = V ∗(t, x).13.12 Viscosity Solutions of the Hamilton Jacobi Equation 747
We have shown that, in the latter case, we can replace the original sequence {(ti, xi)}
by a new sequence {(τi, xi(τi))} such that τi > t for all i and (13.12.4) is satisfied. 
The claim is confirmed. 
Now take any v˜ ∈ F (t+, x). Then there exists a sequence of vectors vi such that 
vi ∈ F (t+
i , xi) and limi→∞ vi = ˜v. For every i ∈ N, define the arc 
yi(s) := xi + (s − ti)vi, for all s ∈ [ti, T ].
It follows from the Filippov existence theorem that, for each i, there exists an F
trajectory zi such that zi(ti) = xi and, for any h ∈ (0, T − ti], 
‖zi − yi‖L∞([ti,ti+h],Rn) ≤ K
 ti+h
ti
dF (s,yi(s))(vi)ds
,
where K = exp  T
S kF (s)ds
. In consequence of hypothesis (H2)(i), we can 
choose R0 > 0 such that, for each i ∈ N, |yi(s)| ≤ R0 for all s ∈ [ti, T ]. Observe 
also that, from (H2)(ii), | ˙yi(s)| ≤ c0 for a.e. s ∈ [ti, T ], for each i. 
Define, for each i, δi = max{|V (ti, xi) − V ∗(t, x)|, |xi − x|, |ti − t|}. Take a 
strictly decreasing sequence hi ↓ 0 such that, for each i, hi ≥ √δi. 
Fix i, and let wi := 1
hi
 ti+hi
ti z˙i(s)ds. Note that 
|vi − wi| ≤ K
 ti+hi
ti
dF (s,yi(s))(vi)ds
≤ Kθi(hi),
where 
θi(h) :=
⎧
⎨
⎩
sup{dH (F (s, y), F (t+
i , xi)) : if h /= 0,
0 < s − ti ≤ hand|xi − y| ≤ c0h}
0 otherwise.
There exists τ ∈ [ti, ti + hi] and z ∈ Rn such that |xi − z| ≤ c0hi and 
θi(hi) ≤ dH (F (τ, z), F (t+
i , xi)) +
1
i + 1
.
It follows that 
θi(hi) ≤ dH (F (τ, z), F (t+, x)) + dH (F (t+, x), F (t+
i , xi)) +
1
i + 1
.
Notice that: 
τ − t ≤ (τ − ti) + (ti − t) ≤ hi + h2
i
and |x − z|≤|z − xi|+|xi − x| ≤ hic0 + h2
i .748 13 Dynamic Programming
We deduce that 
θi(hi) ≤ sup 
dH (F (s, y), F (t+, x)) : 0 < s − t ≤ hi + h2
i
and |y − x| ≤ c0hi + h2
i
 
+ sup 
dH (F (t+, x), F (s+, y)) : 0 < s − t ≤ h2
i
and |y − x| ≤ h2
i
 
+(1 + i)−1.
This implies that θi(hi) → 0, as i → ∞. Since, for each i, | ˜v − wi| ≤ θi(hi) +
|vi − ˜v|, it follows that 
wi → ˜v, as i → ∞.
Define, for each i, ei := 1 − t−ti
hi and w˜i := wi − x−xi
hi . Since (ei, w˜i) → (1, v)˜ , as 
i → ∞, we have 
lim sup
i→∞
h−1
i

V ∗(t + hiei, x + hiw˜i) − V ∗(t, x)
= lim sup
i→∞
h−1
i

V ∗(ti + hi, xi + hiwi) − V ∗(t, x)
≥ lim sup
i→∞
h−1
i

V (ti + hi, xi + hiwi) − V ∗(t, x)
.
Fix i. Then 
V (ti+hi, xi+hiwi)−V ∗(t, x) ≥ V (ti+hi, xi+hiwi)−V (ti, xi)−δi. (13.12.5) 
From the principle of optimality, 
V (ti + hi, xi + hiwi) − V (ti, xi) ≥ 0.
Dividing across inequality (13.12.5) by hi and passing to the limit, while recalling 
δi
hi ≤ √δi for each i, yields: 
lim sup
i→∞
h−1
i

V ∗(t + hiei, x + hiw˜i) − V ∗(t, x)
≥ 0. (13.12.6) 
Now, take a test function ψ such that V ∗−ψ achieves a local maximum at (t, x). 
Then, for all (t'
, x'
) in a neighbourhood of (t, x), we obtain 
V ∗(t'
, x'
) − V ∗(t, x) − ∇tψ(t, x)(t' − t) − ∇xψ(t, x) · (x' − x) ≤ o(|y − x|)
(13.12.7)13.12 Viscosity Solutions of the Hamilton Jacobi Equation 749
where o : R+ → R+ is a function such that o(ϵ)/ϵ → 0 as ϵ ↓ 0. Setting (t'
, x'
) =
(t + hiei, x + hiw˜i) in (13.12.7) and dividing across by hi yields 
h−1
i (V ∗(t + hiei, x + hiw˜i) − V ∗(t, x)) − ∇tψ(t, x)ei − ∇xψ(t, x) · ˜wi
≤ h−1
i o(hi|(ei, w˜i)|).
It follows from these relations, in the limit as i → ∞, that 
− ∇tψ(t, x) − ∇xψ(t, x) · ˜v ≤ 0.
This inequality is valid for all points v˜ ∈ F (t+, x). We have confirmed condition 
(ii). 
Consider, finally, (iii). The first condition can be verified by the same argument as 
that employed in the proof Theorem 13.3.7. It remains to prove that V ∗(T , .) = g∗. 
It follows immediately from the relation V (T , .) = g that V ∗(T , x) ≥ g∗(x) for 
every x ∈ Rn. We show that the converse inequality is also true. 
Fix x ∈ Rn. There exists a sequence {(ti, xi)} in [S,T ]×Rn \{(T , x)} converging 
to (T , x) such that: 
lim
i→∞ V (ti, xi) = lim sup
(t,y)→(T ,x)
V (t, y) = V ∗(T , x).
For every i, there exists an F trajectory xi ∈ W1,1([ti, T ]; Rn) such that xi(ti) =
xi. By the principle of optimality, 
V (ti, xi) ≤ V (T , xi(T )) = g(xi(T )).
Since xi(T ) → x as i → ∞, we deduce that 
V ∗(T , x) ≤ lim sup
i→∞
g(xi(T )) ≤ lim sup
y→x
g(y) = g∗(x).
We have shown that V ∗(T , x) ≤ g∗(x). This relation combines with the earlier 
inequality to yield V ∗(T , x) = g∗(x). 
Step 2 Assume that V is a lower semi-continuous locally bounded function 
satisfying conditions (i) and (iii). Take any point (t, x) ∈ (S, T ) × Rn and 
(ξ 0, ξ 1) ∈ ∂P V (t, x). Then there exists M > 0 such that the test function 
ψ(s, y) := ξ 0(s − t) + ξ 1(y − x)M|(s, y) − (t, x)|
2 is a test function such that 
V (s, y) − ψ(s, y) is minimized at (t, x). It follows from condition (i) that 
ξ 0 + min v∈F (t+,x)
ξ 1 · v ≤ 0, for all (ξ 0, ξ 1) ∈ ∂P V (t, x).
Thus V satisfies (c)(i) of Theorem 13.3.7. Since V also satisfies condition (iii), we 
can use the same arguments employed in the proof of Theorem 13.3.7 to show that750 13 Dynamic Programming
inf(Pt ,¯ x¯) ≤ V (t ,¯ x), ¯ for every (t ,¯ x)¯ ∈ [S,T ] × Rn.
Step 3 Assume that V is a lower semi-continuous, locally bounded function 
satisfying conditions (ii) and (iii). Suppose also that (g∗)∗ = g. Fix (t ,¯ x)¯ ∈
[S,T ) × Rn. Let x ∈ W1,1([t,T ¯ ]; Rn) be an F trajectory such that x(t )¯ = ¯x. 
We want to prove prove that: 
V (t ,¯ x)¯ ≤ g(x(T )).
First observe that we can find a sequence of points ξj ∈ Rn such that ξj → x(T )
as j → ∞ and 
lim
j→+∞
g∗(ξj ) = (g∗)∗(x(T )).
In consequence of the Filippov existence theorem, there exists, for each j , an F
trajectory xj on [t,T ¯ ] such that xj (T ) = ξj and 
||xj − x||L∞ ≤ K|ξj − x(T )| for all j,
in which K := exp{
 T
S kF (s) ds}. Write x¯j := xj (t)¯ . 
Fix j . Consider the system 
⎧
⎨
⎩
(x,˙ b)˙ ∈ �(t, (x, b)) a.e. t ∈ [t,T ¯ ]
(x(t), b(t)) ∈ P (t) for all t ∈ [t,T ¯ ],
(x(t), b( ¯ t)) ¯ = (x¯j , V ∗(t ,¯ x¯j )) ,
(13.12.8) 
where the multifunctions � : [t,T ¯ ] × Rn+1 ⇝ Rn+1 and P : [t,T ¯ ] ⇝ Rn+1 are 
defined as follows 
�(t, (x, b)) := F (t, x) × {0},
P (t) := {(x, β) : V ∗(t, x) ≥ β}
respectively. We shall apply the strong invariance theorem (Theorem 13.2.4) to the 
constrained differential inclusion (13.12.8). The multifunctions � and P clearly 
possess the necessary closure and regularity properties for this application of the 
theorem. Let us check the hypothesis 
(x¯j , V ∗(S, x¯j )) ∈ lim sup
s'
↓S
P (s'
). (13.12.9) 
According to the definition of the upper semi-continuous envelope function, there 
exists a sequence of points {(si, xi)} in [S,T ] such that13.12 Viscosity Solutions of the Hamilton Jacobi Equation 751
(si, xi) → (S, x¯j ) and V (si, xi) → V ∗(S, x¯j ), as i → ∞. (13.12.10) 
Notice that we can assume si > S for each i. Indeed, if si = S for some i, then, 
invoking the first condition of (iii), we can find a sequence of points {(sk
i , xk
i )}∞
k=1 in 
(S, T ] such that (sk
i , xk
i ) → (si, xi) and V (sj
i , xk
i ) → V (si, xi), as j → ∞. We can 
then replace the element (si, xi) by some element from the sequence {(sk
i , xk
i }∞
j=1, 
such that (13.12.10) continues to be satisfied but, now, si > S, for each i. 
We know that (xi, V (si, xi)) ∈ P (si) for each i, since V ∗ ≥ V . This 
fact combines with (13.12.10) to yield the information that (x¯j , V ∗(S, x¯j )) ∈
lim supi→∞ P (si). Since si > S for each i, we deduce (13.12.9). 
It remains to check the ‘inward pointing’ hypothesis. Take any point (s, (x, β)) ∈
Gr P, satisfying S<s<T , and any vector (ζ 0, ζ 1, λ) ∈ NP
Gr P (s, (x, β)). It 
follows from the proximal normal inequality that 
(ζ 0, ζ 1, λ) ∈ NP
hyp V ∗ (s, x, β).
We must show that 
ζ 0 + min 
max w∈�(s−,(x,β))
ζ 1 · v , max w∈�(s+,(x,β))
ζ 1 · w
!
≤ 0 .
In view of the definition of �, this relation can be equivalently expressed: 
ζ 0 + min 
max v∈F (s−,x)
ζ 1 · v; max v∈F (s+,x)
ζ 1 · v
!
≤ 0 . (13.12.11) 
Since Gr P is the hypograph of an upper semi-continuous function, the proximal 
normal vector (ζ 0, ζ 1, λ) must be such that λ ≥ 0. Notice also that (s, x, β) is an 
interior point to the hypograph, if β<V ∗(s, x); in this case (ζ 0, ζ 1) = (0, 0) and 
(13.12.11) is automatically satisfied. So we can assume that β = V ∗(s, x). We need 
consider two distinct cases. 
λ > 0: in this case, λ−1(ζ 0, ζ 1) ∈ ∂P (−V ∗)(s, x). Then, by the proximal 
normal inequality for lower semi-continuous functions, there exist M > 0 with 
the following properties: the test function ψ(s'
, x'
) := −λ−1ζ 0(s' − s) − λ−1ζ 1 ·
(x' − x) + M|(s'
, x'
) − (s, x)|
2 is such that V ∗ − ψ has a local maximum at (s, x)
and (∇tψ, ∇xψ)(s, x) = −λ−1(ζ 0, ζ 1). It follows from condition (ii) that 
λ−1ζ 0 + sup v∈F (s+,x)
λ−1ζ 1 · v = −∇tψ(s, x) − min v∈F (s+,x)∇xψ(s,x)·v
≤ 0.
This implies (13.12.11). 
λ = 0: in this case, there exist sequences (ζ 0
i , ζ 1
i ) → (ζ 0, ζ 1), λi ↓ 0 and 
(si, xi) → (s, x) such that, for each i ≥ 0, 
(λ−1
i ζ 0
i , λ−1
i ζ 1
i ) ∈ ∂P (−V ∗)(si, xi) .752 13 Dynamic Programming
Considering the test function ψi(s'
, x'
) := −λ−1
i ζ 0
i (s' − s) − λ−1ζ 1
i · (x' − x) +
Mi|(s'
, x'
) − (s, x)|
2, for suitably chosen Mi > 0, we can show, as in the preceding 
case, that 
ζ 0
i + sup
v∈F (s+
i ,xi)
ζ 1
i · v ≤ 0.
By extracting suitable subsequences and arguing as in the proof of Theorem 13.3.7 
we deduce that (13.12.11) is verified also in this case. 
We have verified that the hypotheses are satisfied for application of the strong 
invariance theorem (Theorem 13.2.4) is applicable to the differential inclusion and 
constraint (13.12.8) and therefore may conclude that the � trajectory (xj , bj ≡
V (t ,¯ x¯j )) satisfies 
(xj (t), bj (t)) ∈ P (t) for all t ∈ [t,T ¯ ].
Taking t = T , we see that 
V ∗(t ,¯ x¯j ) ≤ V ∗(T , xj (T )) = g∗(xj (T )) (= g∗(ξj )), for every j.
(13.12.12) 
Since ‖xj − x‖L∞ → 0 as j → ∞, V ≤ V ∗, and V is lower semi-continuous, we 
may pass to the limit in (13.12.12) to obtain 
V (t ,¯ x)¯ ≤ lim inf
j→∞ g∗(ξj ).
Since limj→+∞ g∗(ξj ) = (g∗)∗(x(T )) and (g∗)∗ = g, it follows that 
V (t ,¯ x)¯ ≤ g(x(T )).
Recalling that x was an arbitrary F trajectory such that x(t )¯ = ¯x, we deduce that 
V (t ,¯ x)¯ ≤ inf(Pt
¯,x¯).
Step 3 is complete. 
⨅⨆
Theorem 13.12.3 (Viscosity Solution Characterization of Value Functions for 
State Constrained Problems: Inward/Outward-Pointing Condition) Consider 
(SC). Assume (H1), (H2)(i), (H3), (H4)(i) and (H5) of Theorem 13.3.7, (BV), and 
(CQ)outward and (CQ)inward of Theorems 13.6.1 and 13.6.2. Suppose, in 
addition, that g|A is locally bounded and satisfies ((g|A)∗)∗ = g|A. Take a lower 
semi-continuous, locally bounded function V : [S,T ] × Rn → R such that 
V (t, x) = +∞ when x /∈ A.13.12 Viscosity Solutions of the Hamilton Jacobi Equation 753
Then V is the value function for (SC) if and only if V is a locally bounded 
function on [S,T ] × A, lower semi-continuous constrained viscosity solution of 
(13.12.1) in the following sense: 
(i) (V is a viscosity supersolution) for any point (t, x) ∈ (S, T ) × A and any C1
function ψ : R × Rn → R such that 
(t'
, x'
) → V (t'
, x'
) − ψ(t'
, x'
)
has a local minimum at (t, x) (relative to [S,T ] × A) we have 
− ψt(t, x) + H +(t, x, −ψx (t, x)) ≥ 0 ,
(ii) (V is a viscosity subsolution) for any point (t, x) ∈ (S, T ) × int A and any C1
function ψ : R × Rn → R such that 
(t'
, x'
) → V ∗(t'
, x'
) − ψ(t'
, x'
)
has a local maximum at (t, x) (relative to [S,T ] × A) we have 
− ψt(t, x) + H +(t, x, −ψx (t, x)) ≤ 0 ,
(iii) for all x ∈ A
lim inf
{(t'
,x'
)→(S,x)|t'
>S}
V (t'
, x'
) = V (S, x),
(V|[S,T ]×A)
∗(T , x) = (g|A)
∗(x) and V (T , x) = g(x).
Proof (Sketch) The proof is based on the following two relations: ‘the value 
function is a constrained viscosity solution (i.e. satisfies (i), (ii) and (iii) of (13.12.1) 
’ and ‘if V is a locally bounded function on [S,T ] × A which is also a lower semi￾continuous constrained viscosity solution of (13.12.1) then V is the value function’. 
The first relation is actually valid even if the outward pointing constraint 
qualification and condition ‘((g|A)∗)∗ = g|A’ are not in force. Indeed, making 
use also of the distance estimate of Theorem 6.6.2, one can prove the following 
proposition. ⨅⨆
Proposition 13.12.4 Assume (H1), (H2)(i), (H3), (H4)(i) and (H5) of Theo￾rem 13.3.7, (BV), and (CQ)inward. Suppose, in addition, that g|A is locally 
bounded. Then V satisfies (i), (ii) and (iii) of Theorem 13.12.3. 
To complete the proof of Theorem 13.12.3 one must show the second relation. 
With this objective in mind we assume that (H1), (H2)(i), (H3) and (H5) of 
Theorem 13.3.7 and (BV) are satisfied, and consider a lower semi-continuous 
function V : [S,T ] × Rn → R ∪ {+∞}, locally bounded function on754 13 Dynamic Programming
[S,T ] × A, such that V (t, x) = +∞ whenever x /∈ A. One can prove the following 
properties: 
(A) if V satisfies (d)(i) and V (T , x) = g(x) for all x ∈ A, then inf(Pt,x ) ≤ V (t, x)
for any (t, x) ∈ [S,T ] × A, 
(B) if, in addition, (CQ)outward holds, g is a locally bounded on A such that 
((g|A)∗)∗ = g|A, and V satisfies (d)(ii) and (d)(iii), then inf(Pt,x ) ≥ V (t, x)
for any (t, x) ∈ [S,T ] × A. 
It is not difficult to see that the proof of (A) can be reduced to the analysis 
employed in the proof of Thm 13.6.1. On the other hand, the proof of (B) requires a 
different construction of several sequences of arcs, taking into account the condition 
‘((g|A)∗)∗ = g|A’, and the notions of lower/upper semi-continuous envelopes. 
13.13 A Comparison Theorem for Viscosity Solutions 
A key feature of viscosity solutions methods is to provide direct proofs of 
comparison theorems, which in turn lead to uniqueness of viscosity solutions 
to the Hamilton Jacobi equation of dynamic programming. This contrasts with 
approaches to proving uniqueness employed earlier in this chapter, based on 
invariance properties of state trajectories, where the proof that the Hamilton Jacobi 
equation has a unique generalized solution and that it coincides the value function 
comes as a combined package. Since the viscosity solutions methods are used to 
prove uniqueness alone, it is unsurprising then that they are capable of establishing 
this property, under hypotheses that are in some respects weaker than those required 
by the system theoretic approach. Indeed, viscosity solutions methods can be used 
to prove uniqueness of viscosity solutions, even when the Hamilton Jacobi equation 
does not arise from a dynamic optimization problem at all. 
Our aim in this section is to give the flavour of viscosity solutions methods, 
by proving a comparison theorem and examining its implications. The comparison 
theorem concerns the Hamilton Jacobi equation connected with the infinite horizon 
problem, investigated earlier in the chapter by means of system theoretic techniques. 
Comparison theorems can be proved also for finite horizon problems, but the 
viscosity solutions techniques involved are more complicated. 
For given x ∈ Rn, consider then 
(P∞)(x)
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Minimize  ∞
0 e−λtL(x(t), u(t))dt
over x ∈ W1,1
loc([0,∞); Rn) and
measurable functions u : [0,∞) → Rm satisfying
y(t) ˙ = f (y(t), u(t)), a.e. t ∈ [0,∞),
u(t) ∈ U, a.e. t ∈ [0,∞),
y(0) = x,13.13 A Comparison Theorem for Viscosity Solutions 755
in which f : Rn × Rm → Rn and L : Rn × Rm → R are given functions, U ⊂ Rm
is a given set and λ > 0. 
The value function V : Rn → R associated with this problem is 
V (x) := inf  ∞
0
e−λtL(y(t), u(t))dt,
in which the infimum is taken over infinite horizon processes (y, u) for (P∞)(x) (as 
earlier defined) satisfying the specified constraints and such that t → L(y(t), u(t))
is integrable. 
Depending on the hypotheses imposed on the data for (P)∞, the value function 
V a ‘solution’ (in the classical, viscosity or proximal sense) to the Hamilton Jacobi 
equation 
λv(x) + H (x, vx (x)) = 0, for all x ∈ Rn, (13.13.1) 
in which the function H : Rn × Rn → R is the function 
H (x, p) := sup
u∈U

− p · f (x, u) − L(x, u)
. (13.13.2) 
In this section we deal with value functions which, under the standing assumptions, 
turn out to be continuous. Accordingly, we introduce the notion of viscosity solution 
of the Hamilton Jacobi equation (13.13.1) in a continuous context. We shall say that 
a continuous function v : Rn → R is a viscosity subsolution of (13.13.1) if, for any 
C1 function φ : Rn → R, we have 
λv(x) + H (x, ∇φ(x)) ≤ 0, for all x ∈ Rn
at any local maximum point x¯ of v − φ. A continuous function v : Rn → R is a 
viscosity supersolution of (13.13.1) if, for any C1 function φ : Rn → R, we have 
λv(x) + H (x, ∇φ(x)) ≥ 0, for all x ∈ Rn
at any local minimum point x¯ of v−φ. v is a viscosity solution if it is simultaneously 
a sub- and supersolution. 
We refer to the following hypotheses. Note that they are expressed directly in 
terms of the function H appearing in the Hamilton Jacobi equation, consistent 
with the goal of proving properties of viscosity solutions without reference to the 
underlying control system. 
(A1): There exists a modulus of continuity ω1 : R+ → R+ such that 
|H (x, p)−H (y, p)| ≤ ω1(|x−y|(1+|p|)), for all x, y ∈ Rn and p ∈ Rn.756 13 Dynamic Programming
(A2): For all R > 0 there exists a modulus of continuity ω2 : R+ → R+ such that 
|H (x, p) − H (x, q)| ≤ ω2(|p − q|), for all x ∈ RB and p, q ∈ Rn.
To achieve a characterization of the value function for (P∞)(x) based on Theo￾rem 13.13.1 below, we need first to show that the value function is a bounded and 
continuous viscosity solution of (13.13.1). This is accomplished by the following 
proposition. 
Proposition 13.13.1 Assume (H1)–(H3) (of Sect. 13.10). Then, the value function 
V : Rn → R is a bounded, uniformly continuous function that is a viscosity solution 
of (13.13.1). 
Proof From Proposition 13.10.2 we already know that V is a bounded, uniformly 
continuous function. So, here, we only show that V is a viscosity solution of 
(13.13.1). 
Consider a C1 function φ : Rn → R and a point x0 ∈ Rn which is a local 
maximum point for V − φ. Therefore, there exists δ > 0 such that φ(x0) − φ(x) ≤
V (x0) − V (x) for all x ∈ x0 + δB. Take any v ∈ U. Let (x, u ≡ v) be the process 
on [0,∞) satisfying x(0) = x0. For h0 > 0 sufficiently small x(t) ∈ x0 + δB for all 
t ∈ [0, h0]. It follows that 
φ(x0) − φ(x(t)) ≤ V (x0) − V (x(t)), for all t ∈ [0, h0].
Applying (ii) of Proposition 13.10.1 to the process (x, u ≡ v), we deduce that, for 
all t ∈ [0, h0], 
φ(x0) − φ(x(t)) ≤
 t
0
e−λsL(x(s), v)ds + (e−λt − 1)V (x(t)) .
Dividing across this inequality by t ∈ (0, h0] and passing to the limit as t ↓ 0 we 
obtain 
− ∇φ(x0) · f (x0, v) ≤ L(x0, v) − λV (x0) .
Taking into account that v ∈ U is arbitrary, we conclude that 
λV (x0) + sup
v∈U

− ∇φ(x0) · f (x0, v) − L(x0, v)
≤ 0 .
This confirms that V is a viscosity subsolution. 
Now, we prove that V is a viscosity supersolution. Take any a C1 function φ :
Rn → R and a point x0 ∈ Rn that is a local minimum for V − φ: we can find 
δ > 0 such that V (x0) − φ(x0) ≤ V (x) − φ(x) for all x ∈ x0 + δB. Let (x,¯ u)¯ be 
a minimizing process for (P∞)(x0). There exists h0 > 0 sufficiently small x(h) ¯ ∈
x0 + δB for all h ∈ [0, h0]. Apply (iii) of Proposition 13.10.1 to the process (x,¯ u)¯
with T = h. We deduce that, for all h ∈ [0, h0],13.13 A Comparison Theorem for Viscosity Solutions 757
0 ≤ φ(x0) − φ(x(h)) ¯ + V (x(h)) ¯ − V (x0)
= φ(x0) − φ(x(h)) ¯ −
 h
0
e−λsL(x(s), ¯ u(s))ds ¯
+ (1 − e−λh)V (x(h)) . ¯ (13.13.3) 
By means of arguments similar to those used in the proof of (ii) of Proposi￾tion 13.10.2, we deduce that there exists hi ↓ 0 and v¯ ∈ U such that 
h−1
i
 hi
0 ∇φ(x(s)) ¯ · f (x(s), ¯ u(s))ds ¯ → ∇φ(x0) · f (x0, v)¯ and
limi→∞ h−1
i
 hi
0 e−λsL(x(s), ¯ u(s))ds ¯ ≥ L(x0, v). ¯
Setting h = hi in (13.13.3), dividing across (13.13.3) by hi and passing to the limit 
as i → ∞ yields 
0 ≤ −∇φ(x0) · f (x0, v)¯ − L(x0, v)¯ + λV (x0) .
We conclude that V is also a viscosity supersolution. We have confirmed that V
is a viscosity solution to the Hamilton Jacobi equation (13.13.1) and the proof is 
complete. ⨅⨆
Theorem 13.13.2 (Comparison Theorem) Assume (A1) and (A2). Take bounded 
continuous functions v1, v2 : Rn → R such that v1 is a viscosity subsolution and v2
is a viscosity supersolution of (13.13.1). Then 
v1(x) ≤ v2(x) for all x ∈ Rn .
In the case that v1, v2 are viscosity solutions, v1, v2 are viscosity super- and 
subsolutions respectively. Reversing the roles of v1, v2 we deduce from the theorem 
also that v1(x) − v2(x) ≥ 0 for all x and so v1 = v2. We have shown: 
Corollary 13.13.3 Assume (A1) and (A2). The there is at most one viscosity 
solution to (13.13.1) in the class of bounded continuous functions. 
Remarks 
(a): Notice that (A1) and (A2) are always satisfied when (H2) and (H3) (of 
Sect. 13.10) are in force, 
(b): The function H derived from the data for the dynamic optimization problem 
according to (13.13.2) automatically satisfies the convexity condition 
H (x, .) is convex for all x ∈ Rn ,
in consequence of the fact that it is defined via a supremum operation. The fact 
that this convexity condition is absent from the hypotheses of Theorem 13.13.2 
is highly significant because it means that Theorem 13.13.2 can be applied758 13 Dynamic Programming
to zero-sum two player differential games, with dynamic constraint x˙ =
f (x, u, v), (u, v) ∈ U × V and running cost L(x, u, v) involving the control 
variables u and v of the two players. Here the value V, parameterized by the 
initial state x is, under appropriate hypotheses, the unique viscosity solution of 
the Hamilton Jacobi equation (13.13.2) when, now, H is defined to be 
H (x, p) := inf
v∈V sup
u∈U

− p · f (x, u, v) − L(x, u, v)
.
Proof of Theorem 13.13.2 Suppose the assertions of the theorem are false. Then 
there exists a point x˜ ∈ Rn and δ > 0 such that v1(x)˜ − v2(x) > δ ˜ . 
Define the function g : Rn → R to be g(x) = 1
2 loge(1 + |x|
2). Let β > 0 be a 
number that satisfies 
λ−1ω2(2β) ≤ δ/4 and βg(x)˜ ≤ δ/4.
For given ϵ > 0 define the function Ф : Rn × Rn → R to be 
Ф(x, y) := v1(x) − v2(y) − |x − y|
2
2ϵ − β(g(x) + g(y)) .
The function Ф is continuous. In consequence of the facts that v1 − v2 is a bounded 
function and g is a non-negative function such that lim|x|→∞ g(x) = ∞, we know 
that Ф has compact level sets. It follows that Ф has a maximizer over Rn × Rn, 
which we write (xϵ , yϵ ). We have 
v1(xϵ ) − v2(yϵ ) ≥ Ф(xϵ , yϵ ) ≥ Ф(x,˜ x)˜
= v1(x)˜ − v2(x)˜ − 2βg(x)˜ ≥ δ − δ/2 = δ/2 . (13.13.4) 
Since Ф(xϵ , yϵ ) ≥ 0, we have 
β(g(xϵ ) + g(yϵ )) + |xϵ − yϵ |
2
2ϵ
≤ v1(xϵ ) − v2(yϵ ) ≤ C,
in which C := sup{|v1(x) − v2(x)| : x ∈ Rn} < ∞.
It follows from this relation that |xϵ − yϵ | → 0 as ϵ ↓ 0 .
Since g is a coercive, non-negative function, we can also deduce from this 
relation that there exists R > 0, independent of ϵ, such that |(xϵ , yϵ )| ≤ R. 
Since (xϵ , yϵ ) is a maximizer for Ф, 2Ф(xϵ , yϵ ) ≥ Ф(xϵ , xϵ ) + Ф(yϵ , yϵ ). 
Expanding this inequality and cancelling terms gives 
|xϵ − yϵ |
2
ϵ
≤ v1(xϵ ) − v1(yϵ ) + v2(xϵ ) − v2(yϵ ) .13.14 Exercises 759
Now the continuous functions v1, v2 are uniformly continuous on the compact set 
RB. But then, in consequence of the preceding relation and the fact |xϵ − yϵ | → 0, 
|xϵ − yϵ |
2
ϵ → 0 as ϵ ↓ 0 . (13.13.5) 
Now define the C1 test functions 
φ(x) :=
|x − yϵ |
2
2ϵ
+ βg(x) and ψ(y) := −
|y − xϵ |
2
2ϵ − βg(y) .
From the facts that Ф(x, yϵ ) is maximized at x = xϵ and −Ф(xϵ , y) is minimized at 
yϵ we deduce that v1 − φ is maximized at xϵ and v2 − ψ is minimized at yϵ . Since 
v1 and v2 are viscosity sub- and supersolutions of (13.13.1) respectively, we know 
then that 
λv1(xϵ ) + H (xϵ , ϵ−1(xϵ − yϵ ) + β∇g(xϵ ) ≤ 0
and 
λv2(yϵ ) + H (yϵ , ϵ−1(xϵ − yϵ ) − β∇g(yϵ ) ≥ 0.
Subtracting these inequalities, dividing across by λ, taking note of hypotheses (A1) 
and (A2), and observing that |∇g(x)| ≤ 1 for all x, we see that 
v1(xϵ ) − v2(yϵ ) ≤ λ−1ω1(|xϵ − yϵ |(ϵ−1|xϵ − yϵ | + β) + λ−1ω2(2β).
(Here we use that fact that the modulus of continuity ω2 is taken with respect to the 
radius R and that |(xϵ , yϵ )| ≤ R.) In view of (13.13.5) we can arrange, by choosing 
ϵ sufficiently small, that 
λ−1ω1(|xϵ − yϵ |(ϵ−1|xϵ − yϵ | + β) < δ/4.
Since λ−1ω2(2β) ≤ δ/4, we can conclude that 
v1(xϵ ) − v2(yϵ ) < δ/2 .
We have arrived at a contradiction of (13.13.4) and the proof is complete. ⨅⨆
13.14 Exercises 
13.1 (Problems with Running Costs.) Consider the dynamic optimization problem 
with running cost, parameterized by the initial state x0 ∈ Rn:760 13 Dynamic Programming
(P )(x0)
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
Minimize g(x(T )) +  T
S L(t, x(t), x(t))dt ˙
over x ∈ W1,1([S,T ]; Rn) satisfying
x(t) ˙ ∈ F (t, x(t)), a.e. t ∈ [S,T ],
x(S) = x0,
in which [S, T ] is a given interval, F : [S, T ] × Rn ⇝ Rn is a given multifunction 
and g : Rn → R ∪ {+∞} and L : [S, T ] × Rn × Rn → R are given functions. 
Assume 
(H1): g : Rn → R ∪ {+∞} is lower semi-continuous, 
(H2): F is continuous and takes closed, non-empty values. L is continuous and 
{(v, α) ∈ Rn × R : v ∈ F (t, x) and α ≥ L(t, x, v)}
is convex for all (t, x) ∈ [S,T ] × Rn,
(H3): there exists c > 0 such that 
|v| ≤ c and |L(t, x, v)| ≤ c for all (t, x, v) ∈ Gr F, 
(H4): there exists k > 0 such that 
F (t, x'
) ⊂ F (t, x) + k|x − x'
| B, for all x, x' ∈ Rn and t ∈ [S, T ],
and 
|L(t, x'
, v) − L(t, x, v)| ≤ k|x' − x| ,
for all x'
, x ∈ Rn, t ∈ [S, T ] and v ∈ Rn.
Show that a lower semi-continuous function V : [S, T ] × Rn → R ∪ {+∞} is 
the value function if and only if 
(i) for all (t, x) ∈ ((S, T ) × Rn) ∩ dom V , (ξ 0, ξ 1) ∈ ∂P V (t, x)
ξ 0 + inf v∈F (t,x)
{ξ 1 · v + L(t, x, v)} = 0,
(ii) for all x ∈ Rn, 
lim inf
{(t'
,x'
)→(S,x):t'
>S}
V (t'
, x'
) = V (S, x)
and 
lim inf
{(t'
,x'
)→(T ,x):t'
<T }
V (t'
, x'
) = V (T , x) = g(x).
Hint: Reformulate the problem as one without running cost, by means of state 
augmentation.13.14 Exercises 761
(P )(x0, z0)
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Minimize g(x(T )) + z(T )
over (x, z) ∈ W1,1 satisfying
(x,˙ z)(t) ˙ ∈ {(v, α) : v ∈ F (t, x) and α ∈ [L(t, x, v), c] }
a.e. t ∈ [S,T ],
x(S) = x0, z(S) = z0 .
Characterize the value function W (t, x, z) for the state augmented problem, using 
the theory of Sect. 13.3, noting that it has structure W (t, x, z) = V (t, x) + z, where 
V is the value function of the original problem. 
13.2 (Decrease Properties of Control Systems.) Consider the control system 

x(t) ˙ ∈ F (x(t)), a.e. t ∈ [0, +∞),
x(0) = x0 ,
in which F : Rn ⇝ Rn is a given multifunction and x0 ∈ Rn. Assume 
(H1): F is continuous and takes closed convex non-empty values, 
(H2): there exist constants c > 0 and k > 0 such that 
F (x) ⊂ cB and F (x'
) ⊂ F (x) + k|x − x'
| B,for all x, x' ∈ Rn.
Take any continuous function V : Rn → R. 
(i) (Weak Decrease.) Assume, for all x ∈ Rn, 
min
v∈F (x)
v · ξ ≤ 0, for all ξ ∈ ∂P V (x) .
Show that there exists an F trajectory x on [0,∞) such that x(0) = x0 and 
V (x(t)) ≤ V (x0) for all t ∈ [0,∞) .
(ii) (Strong Decrease.) Assume 
min
v∈F (x)
v · ξ ≥ 0, for all ξ ∈ ∂P V (x), x ∈ Rn .
Show that for all F trajectories x on [0,∞) such that x(0) = x0 and 
V (x(t)) ≤ V (x0) .
Hint: (i) Apply the weak invariance theorem for autonomous systems to the state 
augmented control system762 13 Dynamic Programming
⎧
⎨
⎩
(x(t)), ˙ a(t)) ˙ ∈ F (x(t)) × {0}, a.e. t ∈ [0,∞)
(x(t), a(t)) ∈ epi V for all t ∈ [0,∞)
x(0) = x0, z(0) = V (x0) .
(ii) Take any F trajectory x on [0,∞) such that x(0) = x0. Fix any T >0. Apply 
the strong invariance theorem for autonomous systems to the above extended-state 
control system, restricted to the time interval [0, T ], in reverse time. 
13.3 Consider the dynamic optimization problem 
(Pt0,x0 )
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
Minimize g(x(1)) +  1
t0 L(t, x(t), x(t))dt ˙
over arcs x ∈ W1,1([t0, 1]; R) such that
x(t) ˙ ∈ F (t) for a.e. t ∈ [t0, 1],
x(t0) = x0 ,
where t0 ∈ [0, 1], x0 ∈ R, 
F (t) := [−1, 1], if 0 ≤ t ≤ 1
2 ,
[−1
2 , 1
2 ], if 1
2 < t ≤ 1,
g(x) := 1 + x, if x > 0,
x, if x ≤ 0,
and 
L(t, x, v) := (v + 1)2, if 0 < t ≤ 1
2 ,
(v + 1
2 )2 + 2, if 1
2 < t ≤ 1.
(i) Write the explicit expression of the value function V : [0, 1] × R → R. 
(ii) Show that V satisfies conditions (b)–(c) of Theorem 13.3.7 and the characteri￾zation provided by Theorem 13.12.2. 
(iii) Take the point (t0, x0) = ( 1 
2 , 1 
4 ) and show that the characterization provided by 
Theorem 13.12.2 must involve the right limits in the Hamiltonian (that means 
F (t+, x) and L(t+, x, v)), for otherwise, if the limits were taken from the other 
side, the characterization of Theorem 13.12.2 would not be satisfied. 
13.15 Notes for Chapter 13 
Dynamic programming, initiated by Richard Bellman [20], is a large field and our 
focus in this chapter is restricted to some of the more important topics. These include 
the relation between the value function and generalized solutions of the Hamilton13.15 Notes for Chapter 13 763
Jacobi equation for deterministic dynamic optimization problems (over a finite or 
infinite horizon), extensions to allow for state constraints, minimum time problems, 
existence of verification functions and the interpretation of costate trajectories as 
gradients of value functions. 
In his 1942 paper, Nagumo studied conditions under which a differential equation 
has a solution which evolves in a given closed set [162]. Extensions to differential 
inclusions, supplying on the one hand conditions for the existence of a solution to a 
differential equation evolving in a given closed set and, on the other, conditions 
for all solutions to evolve in the set, have been of continuing interest since the 
1970’s. Following Clarke et al. [84], we refer to them as weak and strong invariance 
theorems respectively. These are the main tools used in this chapter to characterize 
value functions as generalized solutions to the Hamilton Jacobi equation. 
Weak invariance theorems can be regarded as existence theorems for solutions 
to differential inclusions on general closed domains—a viewpoint stressed by 
Deimling [93]. Weak invariance theorems are widely referred to as viability 
theorems (Aubin’s terminology). They are the cornerstone of viability theory, in 
which broad issues are addressed, relating to existence of invariant trajectories 
(nature of ‘viability domains’, etc.). 
Nagumo showed that, under an inward pointing hypothesis expressed in terms 
of the Bouligand tangent cone, there exists an invariant trajectory for a differential 
equation which can be constructed as a limit of polygonal arcs satisfying the given 
state constraint at mesh points. An abstract compactness argument assures the 
existence of node points for the polygonal arcs, with appropriate limiting properties. 
Generalizations to differential inclusions were considered by a number of 
authors. Under an inward pointing hypothesis involving the Clarke tangent cone, 
existence of invariant trajectories (possessing also a monotonicity property) was 
established by Clarke [57] for Lipschitz multifunctions and for continuous mul￾tifunctions by Aubin-Clarke [12]. Weak invariance theorems for upper semi￾continuous, convex-valued multifunctions (and, more generally, for multifunctions 
‘with memory’) were proved by Haddad [119], under an inward pointing hypothesis 
involving the Bouligand tangent cone. 
The Bouligand contingent cone condition is given a central role in Aubin’s book 
[10]. This is because it is in some sense necessary for weak invariance and because 
it can be shown to be equivalent to various, apparently less restrictive, conditions 
which arise in applications. 
We follow a different approach to building up sets of conditions for weak invari￾ance, mapped out by Clarke et al. in [84]. It is to hypothesize an inward pointing 
condition, expressed in term of the proximal normal cone of the state constraint 
set (wherever it is non-empty), to construct a polygonal arc on a uniform mesh by 
‘proximal aiming’ and obtain an invariant arc in the limit as the mesh size tends 
to zero. The advantage of the proximal aiming approach is simplicity, regarding 
both the proof of weak invariance theorems and also the investigation of alternative 
sets of conditions for weak invariance. In earlier proofs of weak invariance, taking 
inspiration from Nagumo’s original ideas, abstract compactness arguments are 
required to select a special, non-uniform mesh for construction of polygonal arcs,764 13 Dynamic Programming
to ensure convergence. The proximal aiming approach, by contrast, is ‘robust’ 
regarding mesh selection—we can, for convenience, choose a uniform mesh. The 
other point is that proximal aiming yields weak invariance theorems under an inward 
pointing hypothesis formulated in terms of the proximal normal cone, which yield as 
direct corollaries weak invariance under other commonly encountered formulations 
of the hypothesis, because the proximal normal formulation is the least restrictive 
among them. 
The proof given here of the strong invariance theorem for time-dependent 
systems, based on application of the weak invariance theorem to a modified 
differential inclusion which in some sense penalizes deviations from the arc under 
consideration, draws on ideas from [84], adapted to allow for discontinuous time 
dependence. A simple, alternative proof based on ‘Lipschitz’ parameterizations, as 
in [107], can also be given. 
Gonzalez’ paper [116] establishing that locally Lipschitz value functions are 
almost everywhere solutions of the Hamilton Jacobi equation is typical of the early 
literature linking nonsmooth value functions and the Hamilton Jacobi equation. 
Clarke et al. [62] and Offin [165] showed that locally Lipschitz continuous value 
functions are generalized solutions of the Hamilton Jacobi equation, not in an almost 
every sense but according to a new definition of generalized solution (based on 
generalized gradients) that looked ahead to the ‘pointwise’ concepts of generalized 
solution which now dominate the field. Information about uniqueness of solutions 
was lacking at this stage, though a characterization of locally Lipschitz continuous 
value functions in terms of the Hamilton Jacobi equation was achieved, namely as 
the upper envelope of almost everywhere solutions of the Hamilton Jacobi inequality 
[116]. 
In a key advance [88], Crandall and Lions introduced the concept of viscosity 
solutions to Hamilton Jacobi equation in the class of uniformly continuous functions 
together with analytical techniques for establishing uniqueness of such solutions. 
The Hamilton Jacobi equations considered embraced not merely those arising in 
dynamic optimization, but also in differential games and the second order equa￾tions of stochastic dynamic optimization. An important subsequent development 
[19] was Barron and Jensen’s characterization of lower semi-continuous, finite￾valued value functions as unique ‘lower semi-continuous viscosity solutions’ to the 
Hamilton Jacobi equation of dynamic optimization. These authors adopted a simple 
new ‘single differential’ definition of viscosity solution to replace former defini￾tions involving two partial differential inequalities. Bardi and Caputzo Docetta’s 
monograph [17] provides extensive coverage of viscosity methods in dynamic 
optimization and differential games and of computational methods. (See also [105]). 
The viscosity approach is to prove directly that the relevant Hamilton Jacobi 
equation has a unique solution (in the viscosity sense). It is then shown, as a separate 
undertaking, that the value function is such solution. 
An alternative approach is to use invariance theorems to show directly that an 
arbitrary generalized solution simultaneously majorizes and minorizes the value 
function and, therefore, coincides with it. This approach is ‘system theoretic’, to 
the extent that it involves the construction of state trajectories and the analysis of13.15 Notes for Chapter 13 765
the monotonicity properties of state trajectories. It has its roots in interpretations 
of ‘generalized’ solutions to Hamilton Jacobi equations and Lyapunov inequalities 
by Clarke (later distilled in [73]) and Aubin [9], predating viscosity solutions, used 
to establish basic properties of verification functions and Lyapunov functions in a 
nonsmooth setting, and also in early work (referenced in Subbotin’s book [184]) in 
the games theory literature. 
Frankowska, in [107], gave an independent system theoretic proof of Barron and 
Jensen’s characterization of lower semi-continuous value functions (for F (t, x)’s 
continuous w.r.t. time), based on applications of a weak invariance theorem forward 
in time and a strong invariance theorem backward in time. She also provided 
alternative Hamilton Jacobi equation solution concepts that can be used in this 
characterization. 
In Sect. 13.3, we follow the system theoretic approach, to achieve a characteriza￾tion of lower semi-continuous value functions. The analysis is based, as in [107], on 
application of invariance theorems in forward and backward time. But, because our 
analysis is based on the invariance theorems of Clarke et al. [84], in which inward 
pointing conditions are expressed in terms of proximal normals to the constraint 
set, the characterization is in terms of ‘proximal’ solutions to the Hamilton Jacobi 
equation. (The class of lower semi-continuous functions that are proximal solutions 
is larger that of lower semi-continuous viscosity solutions.) 
Initial advances in the application of system theoretic methods to link lower 
semi-continuous value functions and generalized solutions to the Hamilton Jacobi 
equation, were based on the assumption that the t dependence of the velocity set 
F (t, x) was continuous. 
The value function characterizations in this chapter make use of recent research 
by Bernis et al. [23] (building on [28]), in which t → F (t, x) is assumed to belong to 
the class of discontinuous multifunctions that have bounded variation. As shown by 
Frankowska et al. [112, 113], it is possible to establish a characterization of lower 
semi-continuous value functions, for problems in which t → F (t, x) is merely 
measurable. But this characterization is only partial, because it is expressed only in 
terms of a subset of the class of generalized solutions, namely solutions that also 
satisfy a regularity condition (‘epicontinuity’). See also [171, 198]. 
Inverse verification theorems give conditions under which the existence of a ver￾ification function is guaranteed, confirming the optimality of a putative minimizer. 
In contrast to investigations of the relation of the value function and solutions to 
the Hamilton Jacobi equation, where we seek as unrestrictive conditions on value 
functions as possible for validity of the stated characterizations, improvements to 
inverse verification theorems are achieved by restricting the class of verification 
functions considered: they narrow the search for a verification function to suit a 
particular application. The link between normality hypotheses and the existence of 
Lipschitz continuous verification functions for dynamic optimization problems with 
end-point constraints was established in [62] and elaborated in [73]. 
No mention is made in this chapter of a powerful approach to the derivation of 
inverse verification theorems, based on the application of convex analysis. It allows 
free end-times and general end-point and pathwise state constraints. The approach,766 13 Dynamic Programming
which has roots in L.C. Young’s theory of flows for parametric problems in the 
calculus of variations, has been employed by Fleming, Ioffe, Klötzler, Vinter and 
others. (See [193].) Here, in effect, a dual problem to the dynamic optimization 
problem at hand is set up, in which a Hamilton Jacobi inequality features as a 
constraint. In this context, a verification theorem involves a sequence of smooth 
functions (a maximizing sequence for the dual problem) satisfying a Hamilton 
Jacobi inequality. 
The sensitivity relation, providing an interpretation of the costate trajectory, the 
Hamiltonian and gradients of the value function, is implicit in the early heuristic 
‘dynamic programming’ proof of the maximum principle outlined in Chap. 1.A 
rigorous analysis, confirming the sensitivity relation for some costate trajectory, 
selected from the set of all possible costate trajectories satisfying nonsmooth 
necessary conditions, and the Hamiltonian is given in [192]. An earlier proof of the 
sensitivity relation involving the costate variable alone appears in [78]. Improved 
versions of these relations are given in [35]. 
Soner [182] achieved a characterization of the value functions for dynamic 
optimization problems with state constraints as unique continuous viscosity-type 
solutions to the ‘steady state’ Hamilton Jacobi equation. An inward pointing 
hypothesis is invoked both to assist the uniqueness analysis and to ensure the value 
function has the required continuity properties. 
The system theoretic approach was followed by Frankowska and Vinter [111], to 
characterize lower semi-continuous as unique proximal solutions to the Hamilton 
Jacobi equation under an outward pointing condition, in the case F (t, x) is 
continuous in the time variable. (See also [112].) Bettiol and Vinter [28] provided a 
similar characterization for problems in which t → F (t, x) is discontinuous, with 
bounded variation. This characterization was subsequently refined in [23], avoiding 
using asymptotic proximal subgradients and covering problems involving a running 
cost is only continuous w.r.t. the state variable. 
The discussion of semi-concavity in Sect. 13.9 focuses on its role as an additional 
regularity condition to ensure the uniqueness of Lipschitz continuous, almost 
everywhere solutions to the value function. This however only scratches the 
surface. Broader implications of value function semi-concavity are addressed in, 
for example, [52] and [51]. 
Study of the Hamilton Jacobi equation for dynamic optimization problems with 
infinite horizon has been undertaken mainly in the context of viscosity solutions. 
Soner’s paper [182], which allow for state constraints, was an important early 
contribution. (See [17] and the references therein.) In Sect. 13.10 we see how control 
theoretic techniques developed for finite horizon problems can be simply adapted 
also to allow for infinite horizons. 
Minimum time problems comprise perhaps the most extensively covered sub￾class of dynamic optimization problems in which the end-time is a choice variable 
(‘free end-time’ problems). Studies of the minimum time function (the value 
function in this case) centred on the Hamilton Jacobi equation feature prominently 
the viscosity solutions literature (see [17] and the references therein.) Sect. 13.11, 
in which control theoretic methods are used to provide a Hamilton Jacobi equation13.15 Notes for Chapter 13 767
characterization of the minimum time problem in the absence of any controllability 
hypotheses, makes use of ideas in [208]. 
In Sect. 13.12, relations between viscosity solutions methods and system theo￾retic methods are explored (see [22] for an extension to the case in which we have 
also an integral cost which is merely continuous w.r.t. the state variable). Section 
13.13 conveys the flavour of viscosity solution methods by applying them to prove 
the uniqueness of viscosity solutions to an infinite horizon problem. This section 
draws on material from [17].References 
1. L. Ambrosio, O. Ascenzi, G. Buttazzo, Lipschitz regularity for minimizers of integral 
functionals with highly discontinuous integrands. J. Math. Anal. Appl. 142, 301–316 (1989) 
2. A. Arutyunov, Optimality Conditions: Abnormal and Degenerate Problems, vol. 526 
(Springer, Science and Business Media, New York, 2000) 
3. A.V. Arutyunov, S.M. Aseev, Investigation of the degeneracy phenomenon of the maximum 
principle for optimal control problems with state constraints. SIAM J. Control Optim. 35, 
930–952 (1997) 
4. A. Arutyunov, D. Karamzin, A Survey on regularity conditions for state-constrained optimal 
control problems and the non-degenerate maximum principle. J. Optim. Theory Appl. 184, 
697–723 (2020) 
5. A.V. Arutyunov, S.M. Aseev, V.I. Blagodatskikh, First order necessary conditions in the 
problem of optimal control of a differential inclusion with phase constraints. Russ. Acad. 
Sci. Sb. Math. 79, 117–139 (1994) 
6. S.M. Aseev, Method of Smooth Approximations in the Theory of Necessary Conditions for 
Differential Inclusions. Mathematics, vol. 61 (Izvestiya, Moscow, 1997) 
7. H. Attouch, Variational Convergence for Functions and Operators. Applicable Mathematics 
Series (Pitman, London, 1984) 
8. J.-P. Aubin, Applied Functional Analysis (Wiley Interscience, New York, 1978) 
9. J.-P. Aubin, Contingent Derivatives of Set-Valued Maps and Existence of Solutions to 
Nonlinear Inclusions and Differential Inclusions. Advances in Mathematics, Supplementary 
Studies, ed. by L. Nachbin (1981), pp. 160–232 
10. J.-P. Aubin, Viability Theory (Birkhäuser, Boston, 1991) 
11. J.-P. Aubin, A. Cellina, Differential Inclusions (Springer-Verlag, Berlin, 1984) 
12. J.-P. Aubin, F.H. Clarke, Monotone invariant solutions to differential inclusions. J. Lond. 
Math. Soc. 16, 357–366 (1977) 
13. J.-P. Aubin, I. Ekeland, Applied Nonlinear Analysis (Wiley-Interscience, New York, 1984) 
14. J.-P. Aubin, H. Frankowska, Set-Valued Analysis (Birkhäuser, Boston, 1990) 
15. R.J. Aumann, Integrals of set-valued functions. J. Math. Anal. Appl. 12, 15–26 (1967) 
16. J. Ball, V. Mizel, One-dimensional variational problems whose minimizers do not satisfy the 
Euler-Lagrange equation. Arch. Rat. Mech. Anal. 90, 325–388 (1985) 
17. M. Bardi, I. Capuzzo-Dolcetta, Optimal Control and Viscosity Solutions of Hamilton-Jacobi 
Equations (Birkhäuser, Boston, 1997) 
18. G. Barles, Solutions de Viscosité des Equations de Hamilton-Jacobi. Mathématiques et 
Applications, vol. 17 (Springer, Paris, 1994) 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
P. Bettiol, R. B. Vinter, Principles of Dynamic Optimization, Springer Monographs 
in Mathematics, https://doi.org/10.1007/978-3-031-50089-3
769770 References
19. E.N. Barron, R. Jensen, Semicontinuous viscosity solutions for Hamilton-Jacobi equations 
with convex Hamiltonians. Commun. Partial Differ. Equ. 15, 1713–1742 (1990) 
20. R. Bellman, Dynamic Programming (Princeton University Press, Princeton, 1957) 
21. L.D. Berkovitz, Optimal Control Theory (Springer-Verlag, New York, 1974) 
22. J. Bernis, P. Bettiol, Hamilton-Jacobi equation for state constrained Bolza problems with 
discontinuous time dependence: a characterization of the value function. J. Convex Anal. 
30(2), 591–614 (2023) 
23. J. Bernis, P. Bettiol, R.B. Vinter, Solutions to the Hamilton-Jacobi equation for state 
constrained Bolza problems with discontinuous time dependence. J. Differ. Equ. 341, 589– 
619 (2022) 
24. D. Bessis, Y.S. Ledyaev, R.B. Vinter, Dualization of the Euler and Hamiltonian inclusions. 
Nonlinear Anal. 43, 861–882 (2001) 
25. P. Bettiol, C. Mariconda, A new variational inequality in the calculus of variations and 
Lipschitz regularity of minimizers. J. Differ. Equ. 268(5), 2332–2367 (2020) 
26. P. Bettiol, C. Mariconda, A Du Bois-Reymond convex inclusion for nonautonomous problems 
of the calculus of variations and regularity of minimizers. Appl. Math. Optim. 83, 2083–2107 
(2021) 
27. P. Bettiol, R.B. Vinter, Trajectories satisfying a smooth state constraint: improved estimates. 
IEEE Trans. Automat. Control 56(5), 1090–1096 (2011). 
28. P. Bettiol, R. B. Vinter, The Hamilton Jacobi equation for optimal control problems with 
discontinuous time dependence. SIAM J. Control Optim. 55, 1199–1225 (2017) 
29. P. Bettiol, R.B. Vinter, L∞ Estimates on trajectories confined to a closed subset, for control 
systems with bounded time variation. Math. Program. Ser. B Math. Program. 168(1–2), 201– 
228 (2018) 
30. P. Bettiol, R.B. Vinter, Improved first order necessary conditions for dynamic optimization 
problems with free end-times (to appear) 
31. P. Bettiol, R.B. Vinter, Non-degenerate necessary conditions for non-convex differential 
inclusions (to appear) 
32. P. Bettiol, A. Bressan, R. Vinter, On trajectories satisfying a state constraint: W1,1 estimates 
and counterexamples. SIAM J. Control Optim. Equ. 48, 2267–2281 (2011) 
33. P. Bettiol, H. Frankowska, R.B. Vinter, L∞ estimates on trajectories confined to a closed 
subset. J. Differ. Equ. 252, 1912–1933 (2012) 
34. P. Bettiol, A. Boccia, R.B. Vinter, Stratified necessary conditions for differential inclusions 
with state constraints. SIAM J. Control Optim. 51(5), 3903–3917 (2013) 
35. P. Bettiol, H. Frankowska, R.B. Vinter, Improved sensitivity relations in state constrained 
optimal control. Appl. Math. Optim. 71(2), 353–377 (2015) 
36. P. Bettiol, N. Khalil, R.B. Vinter, Normality of generalized Euler-Lagrange conditions for 
state constrained optimal control problems. J. Convex Anal. 23(1), 291–311 (2016) 
37. P. Bettiol, M. Quincampoix, R.B. Vinter, Existence and characterization of the values of two 
player differential games with state constraints. Appl. Math. Optim. 80(3), 765–799 (2019) 
38. J.T. Betts, Practical Methods for Optimal Control and Estimation Using Nonlinear Program￾ming, 2nd edn. (Cambridge University Press, New York, 2009) 
39. P. Billingsley, Convergence of Probability Measures (John Wiley and Sons, New York, 1968) 
40. A. Boccia, M.D.R. de Pinho, R.B. Vinter, Optimal control problems with mixed and pure 
state constraints. SIAM J. Control Optim. 54, 3061–3083 (2016) 
41. A. Boccia, R.B. Vinter, The maximum principle for optimal control problems with time 
delays. SIAM J. Control Optim. 55, 2905–2935 (2017) 
42. V.G. Boltyanskii, The maximum principle in the theory of optimal processes (in Russian). 
Dokl. Akad. Nauk SSSR, 119, 1070–1073 (1958) 
43. J.M. Borwein, D. Preiss, A smooth variational principle with applications to subdifferentiabil￾ity and to differentiability of convex functions. Trans. Am. Math. Soc. 303, 517–527 (1987) 
44. J.M. Borwein, Q.J. Zhu, A survey of subdifferential calculus with applications. Nonlinear 
Anal. Theory Methods Appl. 38(6), 687–773 (1999)References 771
45. J.M. Borwein, Q.J. Zhu, Techniques of Variational Analysis. CMS Books in Mathemat￾ics/Ouvrages de Mathématiques de la SMC, vol. 20 (Springer, New York, 2005) 
46. A. Bressan, On the intersection of a Clarke cone with a Boltyanskii cone. SIAM J. Control 
Optim. 45, 2054–2064 (2007) 
47. A. Bressan, G. Facchi, Trajectories of differential inclusions with state constraints. J. Differ. 
Equ. 250(4), 2267–2281 (2011) 
48. A.E. Bryson, Dynamic Optimization (Addison Wesley, New York, 1999) 
49. A.E. Bryson, Y.-C. Ho, Applied Optimal Control (Blaisdell, New York, 1969). And (in revised 
addition) Halstead Press (a division of John Wiley and Sons), New York, 1975 
50. R. Bulirsch, F. Montrone, H.J. Pesch, Abort landing in the presence of windshear as a 
minimax optimal problem, Part 1: necessary conditions and Part 2: multiple shooting and 
homotopy. J. Opt. Theory Appl. 70, 1–23, 223–254 (1991) 
51. P. Cannarsa, L. Rifford, Semiconcavity results for optimal control problems admitting no 
singular minimizing controls. Ann. I. H. Poincaré 52, 773–802 (2008) 
52. P. Cannarsa, C. Sinestrari, Semiconcave Functions, Hamilton-Jacobi Equations, and Optimal 
Control (Birkhäuser, New York, 2004) 
53. C. Castaing, M. Valadier, Convex Analysis and Measurable Multifunctions. Springer Lecture 
Notes in Mathematics, vol. 580 (Springer-Verlag, New York, 1977) 
54. L. Cesari, Optimization - Theory and Applications: Problems with Ordinary Differential 
Equations (Springer Verlag, New York, 1983) 
55. C.W. Clark, Mathematical Bioeconomics: The Optimal Management of Renewable Resources 
(Wiley, New York, 1990) 
56. F.H. Clarke, Necessary Conditions for Nonsmooth Problems in Optimal Control and the 
Calculus of Variations. Ph.D. Dissertation, University of Seattle, Washington, 1973 
57. F.H. Clarke, Generalized gradients and applications. Trans. Am. Math. Soc. 205, 247–262 
(1975) 
58. F.H. Clarke, Necessary conditions for a general control problem, in Calculus of Variations 
and Control Theory, ed. by D.L. Russell (Academic Press, New York, 1976), pp. 259–278 
59. F.H. Clarke, The maximum principle under minimal hypotheses. SIAM J. Control Optim. 14, 
1078–1091 (1976) 
60. F.H. Clarke, A new approach to Lagrange multipliers. Math. Oper. Res. 1, 165–174 (1976) 
61. F.H. Clarke, Optimal solutions to differential inclusions. J. Optim. Theory Appl. 19, 469–478 
(1976) 
62. F.H. Clarke, The applicability of the Hamilton-Jacobi verification technique, in Proceedings 
of the 10th IFIP Conference on New York, 1981, ed. by R.F. Drenick, F. Kozin. System 
Modeling and Optimization Series, vol. 38 (Springer-Verlag, New York, 1982), pp. 88–94 
63. F.H. Clarke, Perturbed optimal control problems. IEEE Trans. Automat. Control 31, 535–542 
(1986) 
64. F.H. Clarke, Methods of dynamic and nonsmooth optimization, in CBMS/NSF Regional 
Conference Series in Applied Mathematics, vol. 57 (SIAM, Philadelphia, 1989) 
65. F.H. Clarke, Optimization and Nonsmooth Analysis (Wiley-Interscience, New York, 1983). 
Reprinted as vol. 5 of Classics in Applied Mathematics, SIAM, Philadelphia, PA, 1990 
66. F.H. Clarke, An indirect method in the calculus of variations. Trans. Am. Math. Soc. 336, 
655–673 (1993) 
67. F.H. Clarke, Necessary conditions in dynamic optimization. Mem. Am. Math. Soc. 173 (2005) 
68. F.H. Clarke, Functional Analysis, Calculus of Variations and Optimal Control. Graduate Texts 
in Mathematics, vol. 264 (Springer Verlag, New York, 2013) 
69. F.H. Clarke, M.D.R. de Pinho, Optimal control problems with mixed constraints. SIAM J. 
Control Optim. 48(7), 4500–4524 (2010) 
70. F.H. Clarke, Y.S. Ledyaev, Mean value inequalities. Proc. Am. Math. Soc. 122, 1075–1083 
(1994) 
71. F.H. Clarke, Y.S. Ledyaev, Mean value inequalities in Hilbert space. Trans. Am. Math. Soc. 
344, 307–324 (1994)772 References
72. F.H. Clarke, P.D. Loewen, The value function in optimal control: sensitivity, controllability 
and time-optimality. SIAM J. Control Optim. 24, 243–263 (1986) 
73. F.H. Clarke, R.B. Vinter, Local optimality conditions and Lipschitzian solutions to the 
Hamilton-Jacobi equation. SIAM J. Control Optim. 21, 856–870 (1983) 
74. F.H. Clarke, R.B. Vinter, On the conditions under which the Euler equation or the maximum 
principle hold. Appl. Math. Optim. 12, 73–79 (1984) 
75. F.H. Clarke, R.B. Vinter, Regularity properties of solutions to the basic problem in the calculus 
of variations. Trans. Am. Math. Soc. 289, 73–98 (1985) 
76. F.H. Clarke, R.B. Vinter, Existence and regularity in the small in the calculus of variations. J. 
Differ. Equ. 59, 336–354 (1985) 
77. F.H. Clarke, R.B. Vinter, Regularity of solutions to variational problems with polynomial 
Lagrangians. Bull. Polish Acad. Sci. 34, 73–81 (1986) 
78. F.H. Clarke, R.B. Vinter, The relationship between the maximum principle and dynamic 
programming. SIAM J. Control Optim. 25, 1291–1311 (1987) 
79. F.H. Clarke, R.B. Vinter, Optimal multiprocesses. SIAM J. Control Optim. 27, 1072–1091 
(1989) 
80. F.H. Clarke, R.B. Vinter, Applications of optimal multiprocesses. SIAM J. Control Optim. 
27, 1048–1071 (1989) 
81. F.H. Clarke, R.B. Vinter, A regularity theory for problems in the calculus of variations with 
higher order derivatives. Trans. Am. Math. Soc. 320, 227–251 (1990) 
82. F.H. Clarke, R.B. Vinter, Regularity properties of optimal controls. SIAM J. Control Optim. 
28, 980–997 (1990) 
83. F.H. Clarke, P.D. Loewen, R.B. Vinter, Differential inclusions with free time. Ann. l’Inst. 
Henri Poincaré (An. Nonlin.) 5, 573–593 (1989) 
84. F.H. Clarke, Y.S. Ledyaev, R.J. Stern, P.R. Wolenski, Qualitative properties of trajectories of 
control systems: a survey. J. Dyn. Control Syst. 1, 1–47 (1995) 
85. F.H. Clarke, Y.S. Ledyaev, R.J. Stern, P.R. Wolenski, Nonsmooth Analysis and Control 
Theory. Graduate Texts in Mathematics, vol. 178 (Springer-Verlag, New York, 1998) 
86. F.H. Clarke, L. Rifford, R.J. Stern, Feedback in state constrained optimal control. ESAIM 
Control Optim. Calc. Var. 7, 97–133 (2002) 
87. F.H. Clarke, Y. Ledyaev, M.D.R. de Pinho, An extension of the Schwarzkopf multiplier rule 
in optimal control. SIAM J. Control Optim. 49(2), 599–610 (2011) 
88. M.G. Crandall, P.L. Lions, Viscosity solutions of Hamilton-Jacobi equations. Trans. Am. 
Math. Soc. 277, 1–42 (1983) 
89. G. Debreu. Integration of correspondences, in Proceedings of the Fifth Berkeley Symposium 
on Mathematical Statistics and Probability II, ed. by L. LeCam, J. Neyman, E. Scott 
(University of California Press, Berkeley, California, 1967), pp. 351–372 
90. A.V. Dmitruk, Maximum principle for a general optimal control problem with state and 
regular mixed constraints. Comput. Math. Model. 4, 364–377 (1993) 
91. A.V. Dmitruk, On the development of Pontryagin’s maximum principle in the works of A.Ya. 
Dubovitskii and A.A. Milyutin. Control Cybern. 38(4A), 923–957 (2009) 
92. A.V. Dmitruk, N.P. Osmolovskii, Local minimum principle for an optimal control problem 
with a nonregular mixed constraint. SIAM J. Control Optim. 60(4), 1919–1941 (2022) 
93. K. Diemling, Multivalued Differential Equations (de Gruyter, Berlin, 1992) 
94. A.L. Dontchev, W.W. Hager, A new approach to Lipschitz continuity in state constrained 
optimal control. Syst. Control Lett. 35, 137–143 (1998) 
95. A.L. Dontchev, I. Kolmanovsky, On regularity of optimal control. Recent developments in 
optimization, in Proceedings of the French-German Conference on Optimization, ed. by 
R. Durier, C. Michelot. Lecture Notes in Economics and Mathematical Systems, vol. 429 
(Springer Verlag, Berlin, 1995), pp. 125–135 
96. A.J. Dubovitskii, A.A. Milyutin, Extremal problems in the presence of restrictions. U.S.S.R. 
Comput. Math. Math. Phys. 5, 1–80 (1965) 
97. N. Dunford, J.T. Schwartz, Linear Operators. Part I: General Theory (Interscience, London, 
1958). Reissued by Wiley-Interscience (Wiley Classics Library), 1988References 773
98. I. Ekeland, On the variational principle. J. Math. Anal. Appl. 47, 324–353 (1974) 
99. I. Ekeland, Nonconvex minimization problems. Bull. Am. Math. Soc. (N.S.) 1, 443–474 
(1979) 
100. H.O. Fattorini, Infinite-Dimensional Optimization and Control Theory (Cambridge University 
Press, Cambridge, 1999) 
101. M.M.A. Ferreria, R.B. Vinter, When is the maximum principle for state-constrained problems 
degenerate? J. Math. Anal. Appl. 187, 432–467 (1994) 
102. A.F. Filippov, On certain questions in the theory of optimal control. J. SIAM Control (A) 1, 
76–84 (1962) 
103. A. Filippov, Classical solutions of differential equations with multivalued right-hand side. 
SIAM J. Control 5, 609–621 (1967) 
104. W.H. Fleming, R.W. Rishel, Deterministic and Stochastic Optimal Control (Springer Verlag, 
New York, 1975) 
105. W.H. Fleming, H.M. Soner, Controlled Markov Processes and Viscosity Solutions (Springer 
Verlag, New York, 1993) 
106. H. Frankowska, The maximum principle for an optimal solution to a differential inclusion 
with end point constraints. SIAM J. Control Optim. 25, 145–157 (1987) 
107. H. Frankowska, Lower semicontinuous solutions of the Hamilton-Jacobi equation. SIAM J. 
Control Optim. 31, 257–272 (1993) 
108. H. Frankowska, M. Mazzola. Discontinuous solutions of Hamilton-Jacobi-Bellman equation 
under state constraints. Calc. Var. Partial Differ. Equ. 46, 725–747 (2013) 
109. H. Frankowska, M. Mazzola. On relations of the adjoint state to the value function for optimal 
control problems with state constraints. Nonlinear Differ. Equ. Appl. 20, 361–383 (2013) 
110. H. Frankowska, F. Rampazzo, Filippov’s and Filippov-Wazewski’s theorems on closed 
domains. J. Differ. Equ. 161, 449–478 (2000) 
111. H. Frankowska, R.B. Vinter, Existence of neighbouring feasible trajectories: applications 
to dynamic programming for state constrained optimal control problems. J. Optim. Theory 
Applic. 104(1), 21–40 (2000) 
112. H. Frankowska, M. Plaskacz, Semicontinuous solutions of Hamilton Jacobi equations with 
state constraints, in Differential Inclusions and Optimal Control. Lecture Notes in Nonlinear 
Analysis, vol. 2, ed. by J. Andres, L. Gorniewicz, P. Nistri (Juliusz Schauder Center for 
Nonlinear Studies, Toruá, 1998), pp. 145–161 
113. H. Frankowska, M. Plaskacz, T. Rzezuchowski, Measurable viability theorems and the 
Hamilton-Jacobi-Bellman equation. J. Differ. Equ. 116, 265–305 (1995) 
114. D. Gay, The AMPL modeling language: An aid to formulating and solving optimization 
problems. Numer. Anal. Optim. 57, 95–116 (2015) 
115. M. Giaquinta, Multiple Integrals in the Calculus of Variations and Nonlinear Elliptic Systems 
(Princeton University Press, Princeton, 1983) 
116. M.R. Gonzalez, Sur l’existence d’une solution maximale de l’equation de Hamilton Jacobi. 
C. R. Acad. Sci. 282, 1287–1280 (1976) 
117. R.V. Gramkrelidze, Optimal control processes with restricted phase coordinates (in Russian). 
Izv. Akad. Nauk SSSR Ser. Math. 24, 315–356 (1960) 
118. R.V. Gramkrelidze, On sliding optimal regimes. Soviet Math. Dokl. 3, 559–561 (1962) 
119. G. Haddad, Monotone trajectories of differential inclusions and functional differential 
inclusions with memory. Isr. J. Math. 39, 83–100 (1981) 
120. W.W. Hager, Lipschitz continuity for constrained processes. SIAM J. Control and Optim. 17, 
321–338 (1979) 
121. H. Halkin, On the necessary condition for the optimal control of nonlinear systems. J. Analyse 
Math. 12, 1–82 (1964) 
122. H. Halkin, Implicit functions and optimization problems without continuous differentiability 
of the data. SIAM J. Control 12, 239–236 (1974) 
123. A.D. Ioffe, Approximate subdifferentials and applications I: the finite dimensional theory. 
Trans. Am. Math. Soc. 281, 389–416 (1984)774 References
124. A.D. Ioffe, Necessary conditions in nonsmooth optimization. Math. Oper. Res. 9, 159–189 
(1984) 
125. A.D. Ioffe, Proximal analysis and approximate subdifferentials. J. Lond. Math. Soc. 41, 175– 
192 (1990) 
126. A.D. Ioffe, A Lagrange multiplier rule with small convex-valued subdifferentials for non￾smooth problems of mathematical programming involving equality and non-functional 
constraints. Math. Prog. 72, 137–145 (1993) 
127. A.D. Ioffe, Euler-Lagrange and Hamiltonian formalisms in dynamic optimization. Trans. Am. 
Math. Soc. 349, 2871–2900 (1997) 
128. A.D. Ioffe, Variational Analysis of Regular Mappings (Springer-Verlag, Berlin, 2017) 
129. A.D. Ioffe, On generalized Bolza problem and its application to dynamic optimization. J. 
Optim. Theory Appl. 182, 285–309 (2019) 
130. A.D. Ioffe, Maximum principles for optimal control problems with differential inclusions. 
SIAM J. Control Optim. (to appear) 
131. A.D. Ioffe, R.T. Rockafellar, The Euler and Weierstrass conditions for nonsmooth variational 
problems. Calc. Var. Partial Differ. Equ. 4, 59–87 (1996) 
132. A.D. Ioffe, V.M Tihomirov, Theory of Extremal Problems (North-Holland, Amsterdam, 1979) 
133. B. Kaskosz, S. Lojasiewicz, A maximum principle for generalized control systems Nonlinear 
Anal. Theory, Methods Appl. 19, 109–130 (1992) 
134. A.Y. Kruger, Properties of generalized differentials. Siberian Math. J. 26, 822–832 (1985) 
135. K. Kuratowski. Topologie, I and II. Panstowowe Wyd Nauk, Warsaw. (Academic Press, New 
York, 1966) 
136. G. Lebourg, Valeur moyenne pour gradient généralisé. C. R. Acad. Sci. Paris 281, 795–797 
(1975) 
137. U. Ledzewicz, H. Shättler, Geometric Optimal Control Theory, Methods and Examples. Series 
on Interdisciplinary and Applied Mathematics, vol. 38 (Springer, Berlin, 2012) 
138. U. Ledzewicz, H. Shättler, Optimal Control for Mathematical Models of Cancer Therapies. 
Series on Interdisciplinary and Applied Mathematics, vol. 42 (Springer, Berlin, 2015) 
139. U. Ledzewicz, H. Maurer, H. Schättler, Optimal and suboptimal protocols for a mathematical 
model for tumor anti-angiogenesis in combination with chemotherapy. Math. Biosci. Eng. 
8(2), 307–328 (2011) 
140. X. Li, J. Yong, Optimal Control Theory for Infinite Dimensional Systems (Birkhäuser, Boston, 
1995) 
141. P.D. Loewen, Optimal Control via Nonsmooth Analysis, in CRM Proceedings of the Lecture 
Notes, vol. 2 (American Mathematical Society, Providence, 1993) 
142. P.D. Loewen, A mean value theorem for Fréchet subgradients. Nonlinear Anal. Theory 
Methods Appl. 23, 1365–1381 (1995) 
143. P.D. Loewen, R.T. Rockafellar, Optimal control of unbounded differential inclusions. SIAM 
J. Control Optim. 32, 442–470 (1994) 
144. P.D. Loewen, R.T. Rockafellar, New necessary conditions for the generalized problem of 
Bolza. SIAM J. Control Optim. 34, 1496–1511 (1996) 
145. P.D. Loewen, R.B. Vinter, Pontryagin-type necessary conditions for differential inclusion 
problems. Syst. Control Lett. 9, 263–265 (1987) 
146. K. Malanowski, On the regularity of solutions to optimal control problems for systems linear 
with respect to control variable. Arch. Auto. i Telemech. 23, 227–241 (1978) 
147. K. Makowski, L.W. Neustadt, Optimal Control Problems with mixed control-phase variable 
equality and inequality constraints. SIAM J. Control 12, 184–228 (1974) 
148. K. Malanowski, H. Maurer, Sensitivity analysis for state constrained optimal control prob￾lems. Discrete Contin. Dyn. Syst. 4, 241–272 (1998) 
149. C. Mariconda, Equi-Lipschitz minimizing trajectories for non coercive, discontinuous, non 
convex Bolza controlled-linear optimal control problems. Trans. Am. Math. Soc. Ser. B 8, 
899–947 (2021) 
150. D.Q. Mayne, E. Polak, An exact penalty function algorithm for control problems with control 
and terminal equality constraints, Parts I and II. J. Optim. Theory Appl. 32, 211–246, 345–363 
(1980)References 775
151. D.Q. Mayne, R.B. Vinter, First-order necessary conditions in optimal control. J. Optim. 
Theory Appl. 189, 716–743 (2021) 
152. K. Miao, R.B. Vinter, Optimal control of a growth/consumption model, in Optimal Control, 
Applications and Methods, vol. 42 (2021), pp. 1672–1688 
153. A.A. Milyutin, N.P. Osmolovskii, Calculus of Variations and Optimal Control. Translations 
of Mathematical Monographs (American Mathematical Society, Providence, 1998) 
154. B.S. Mordukhovich, Maximum principle in the optimal time control problem with non￾smooth constraints. Prikl. Math. Mech. 40, 1004–1023 (1976) 
155. B.S. Mordukhovich, Metric approximations and necessary optimality conditions for general 
classes of nonsmooth extremal problems. Soviet Math. Doklady 22, 526–530 (1980) 
156. B.S. Mordukhovich, Complete characterization of openness, metric regularity, and Lipschitz 
properties of multifunctions. Trans. Am. Math. Soc. 340, 1–36 (1993) 
157. B.S. Mordukhovich, Generalized differential calculus for nonsmooth and set-valued map￾pings. J. Math. Anal. Appl. 183, 250–282 (1994) 
158. B.S. Mordukhovich, Discrete approximations and refined Euler-Lagrange conditions for non￾convex differential inclusions. SIAM J. Control Optim. 33, 882–915 (1995) 
159. B.S. Mordukhovich, Variational Analysis and Generalized Differentiation, vols. I and II 
(Springer Verlag, Berlin, 2006) 
160. B.S. Mordukhovich, Variational Analysis and Applications. Springer Monographs in Mathe￾matics (2018) 
161. J. Murray, Some optimal control problems in cancer chemotherapy with a toxicity limit. Math. 
Biosci. 100(1), 49–67 (1990) 
162. M. Nagumo, Uber die lage der integralkurven gewöhnlicher differentialgleichungen. Proc. 
Phys. Math. Soc. Jpn. 24, 551–559 (1942) 
163. L.W. Neustadt, A general theory of extremals. J. Comput. Sci. 3, 57–92 (1969) 
164. L.W. Neustadt, Optimization (Princeton University Press, Princeton, 1976) 
165. D. Offin, A Hamilton-Jacobi approach to the differential inclusion problem. M.Sc. Thesis, 
University of British Columbia, Canada, 1979 
166. E. Polak, Optimization: Algorithms and Consistent Approximations (Springer Verlag, New 
York, 1997) 
167. L.S. Pontryagin, V.G. Boltyanskii, R.V. Gamkrelidze, E.F. Mischenko, The Mathematical 
Theory of Optimal Processes (Transl.), ed. by K.N. Tririgoff, L.W. Neustadt (Wiley, New 
York, 1962) 
168. R. Pytlak, Runge-Kutta based procedure for optimal control of differential-algebraic equa￾tions. J. Opt. Theory Appl. 97, 675–705 (1998) 
169. R. Pytlak, R.B. Vinter, A feasible directions algorithm for optimal control problems with state 
and control constraints: convergence analysis. SIAM J. Control Optim. 36, 1999–2019 (1998) 
170. F. Rampazzo, R.B. Vinter, Nondegenerate necessary conditions for nonconvex optimal control 
problems with state constraints. SIAM J. Control Optim. 39(4), 989–1007 (2000) 
171. A.E. Rapaport, R.B. Vinter, Invariance properties of time measurable differential inclusions 
and dynamic programming. J. Dyn. Control Syst. 2, 423–448 (1996) 
172. R.T. Rockafellar, Generalized Hamiltonian equations for convex problems of Lagrange. Pac. 
J. Math. 33, 411–428 (1970) 
173. R.T. Rockafellar, Existence and duality theorems for convex problems of Bolza. Trans. Am. 
Math. Soc. 159, 1–40 (1971) 
174. R.T. Rockafellar, Existence theorems for general control problems of Bolza and Lagrange. 
Adv. Math. 15, 312–333 (1975) 
175. R.T. Rockafellar, Proximal subgradients, marginal values and augmented Lagrangians in 
nonconvex optimization. Math. Oper. Res. 6, 424–436 (1982) 
176. R.T. Rockafellar, Equivalent subgradient versions of Hamiltonian and Euler-Lagrange equa￾tions in variational analysis. SIAM J. Control Optim. 34, 1300–1314 (1996) 
177. R.T. Rockafellar, R.J.-B. Wets, Variational Analysis. Grundlehren der Mathematischen 
Wissenschaften, vol. 317 (Springer-Verlag, New York, 1998)776 References
178. J. Rosenblueth, R.B. Vinter, Relaxation procedures for time delay systems. J. Math. Anal. 
Appl. 162, 542–563 (1991) 
179. J.D.L. Rowland, R.B. Vinter, Dynamic optimization problems with free time and active state 
constraints. SIAM J. Control Optim. 31, 677–697 (1993) 
180. O. Sharomi, T. Malik, Optimal control in epidemiology. Ann. Oper. Res. 251, 55–71 (2017) 
181. G.N. Silva, R.B. Vinter, Necessary conditions for optimal impulsive control problems. SIAM 
J. Control Optim. 35, 1829–1846 (1998) 
182. H.M. Soner, Optimal control with state-space constraints. SIAM J. Control Optim. 24, 552– 
561 (1986) 
183. G.V. Smirnov, Discrete approximations and optimal solutions to differential inclusions. 
Cybernetics 27, 101–107 (1991) 
184. A.I. Subbotin, Generalized Solutions of First-Order PDEs (Birkhäuser, Boston, 1995) 
185. H.J. Sussmann, Geometry and optimal control, in Mathematical Control Theory, ed. by J. 
Baillieul, J.C. Willems (Springer Verlag, New York, 1999), pp. 140–194 
186. H.J. Sussmann, Set separation, approximating multicones, and the Lipschitz maximum 
principle. J. Differ. Equ. 243, 448–488 (2007) 
187. H.J. Sussmann, J.C. Willems, 300 years of optimal control: from the brachystochrone to the 
maximum principle. IEEE Control Syst. Mag. 17(3), 32–44 (1997) 
188. P.L. Tan, H. Maurer, J. Kanesan, J.H. Chuah, Optimal control of cancer chemotherapy with 
delays and state constraints. J. Optim. Theory Appl. 194(3), 749–770 (2022) 
189. L. Tonelli, Sur une méthode direct du calcul des variations. Rend. Circ. Math. Palermo 39, 
233–264 (1915) 
190. L. Tonelli, Fondamenti di Calcolo delle Variazioni, vols. 1 and 2 (Zanichelli, Bologna, 
1921/1923) 
191. J. L. Troutman, Variational Calculus with Elementary Convexity (Springer-Verlag, New York, 
1983) 
192. R.B. Vinter, New results on the relationship between dynamic programming and the 
maximum principle. Math. Control Signals Syst. 1, 97–105 (1988) 
193. R.B. Vinter, Convex duality and nonlinear optimal control. SIAM J. Control Optim. 31, 518– 
538 (1993) 
194. R.B. Vinter, Optimal Control (Birkhäuser, Boston, 2000) 
195. R.B. Vinter, The Hamiltonian inclusion for non-convex velocity sets. SIAM J. Control and 
Optim. 52(2), 1237–125 (2014) 
196. R.B. Vinter, G. Pappas, A maximum principle for non-smooth optimal control problems with 
state constraints. J. Math. Anal. Appl. 89, 212–232 (1982) 
197. R.B. Vinter, F.L. Pereira, A maximum principle for optimal processes with discontinuous 
trajectories. SIAM J. Control Optim. 26, 205–229 (1988) 
198. R.B. Vinter, P. Wolenski, Hamilton Jacobi theory for optimal control problems with data 
measurable in time. SIAM J. Control Optim. 28, 1404–1419 (1990) 
199. R.B. Vinter, H. Zheng, The extended Euler-Lagrange condition for nonconvex variational 
problems. SIAM J. Control Optim. 35, 56–77 (1997) 
200. R.B. Vinter, H. Zheng, Necessary conditions for optimal control problems with state 
constraints. Trans. Am. Math. Soc. 350, 1181–1204 (1998) 
201. R.B. Vinter, H. Zheng, Necessary conditions for free end-time, measurably time dependent 
optimal control problems with state constraints. J. Set Valued Anal. 8, 1–19 (2000) 
202. A. Wächler, L. Biegler, On the implementation of a primal-dual interior point filter line search 
algorithm for large-scale nonlinear programming. Math. Program. 106, 25–57 (2006) 
203. D.H. Wagner, Survey of measurable selection theorems. SIAM. J. Control Optim. 15, 859– 
903 (1977) 
204. J. Warga, Optimal Control of Differential and Functional Equations (Academic Press, New 
York, 1972) 
205. J. Warga, Derivate containers, inverse functions and controllability in Calculus of Variations 
and Control Theory, ed. by D.L. Russell (Academic Press, New York, 1976)References 777
206. J. Warga, Fat homeomorphisms and unbounded derivate containers. J. Math. Anal. Appl. 81, 
545–560 (1981) 
207. J. Warga, Optimization and controllability without differentiability assumptions. SIAM J. 
Control Optim. 21, 239–260 (1983) 
208. P.R. Wolenski, Y. Zhuang, Proximal analysis and the minimal time function. SIAM J. Control 
Optim. 36(3), 1048–1072 (1998) 
209. L.C. Young, Lectures on the Calculus of Variations and Optimal Control Theory (Saunders, 
Philadelphia, 1991) 
210. J. Zabzyk, Mathematical Control Theory: An Introduction (Birkhäuser, Boston, 1992) 
211. D. Zagrodny, Approximate mean value theorem for upper subderivatives. Nonlinear Anal. 
Theory Methods Appl. 12, 1413–1428 (1988) 
212. J. Zimmer, Essential Results of Functional Analysis (University of Chicago Press, Chicago, 
1990)Index 
A 
Action principle, 13, 120 
Adjoint equation, 30 
Aumann 
integrals of multifunctions, 266 
measurable selection theorem, 104 
B 
Bolza problem, generalized, 107 
Borwein and Preiss’ theorem, 133 
Brachistochrone problem, 10 
C 
Calculus of variations, 9 
Carathéodory 
verification technique, 26 
Carathéodory function, 97 
Caristi’s fixed point theorem, 145 
Castaing Representation, 99 
Chain rule, 51 
nonsmooth, 230 
Compactness of trajectories theorem, 259 
Complementary slackness condition, 475 
Conjugate of a function, 109 
Constraint qualification, Mangasarian 
Fromovitz, 234 
Continuity, -μ, 478 
D 
Decrease Properties of Control Systems, 761 
Directional derivative 
generalized, 178 
lower dini, 668 
Dirichlet’s principle, 12, 120 
Distance estimate theorem, 271 
Distance function, 184–189 
Dualization, 391 
Du Bois-Reymond Lemma, 15 
Dunford Pettis theorem, 257 
Dynamic programming, 36–41, 759 
E 
Ekeland’s theorem, 53, 54, 123–127 
Elasticity, nonlinear, 66 
Epicontinuous function, 391 
near a point, 392 
Epi-Lipschitz function, local, 392 
Erdmann 
first condition, 17 
second condition, 17 
Erdmann – Du Bois-Reymond Condition 
nonsmooth, 639 
Essential values, 424–427, 466 
Euler equation, 14 
higher order, 325 
Euler Lagrange condition, 17, 21 
generalized, free end-time, 428 
Exact penalization, 54, 121 
Existence of minimizers, 22–593 
F 
Fermat’s principle, 120 
Field of extremals, 29 
© The Editor(s) (if applicable) and The Author(s), under exclusive license to 
Springer Nature Switzerland AG 2024 
P. Bettiol, R. B. Vinter, Principles of Dynamic Optimization, Springer Monographs 
in Mathematics, https://doi.org/10.1007/978-3-031-50089-3
779780 Index
Filippov 
Generalized Existence Theorem, 247 
Generalized selection theorem, 104 
selection theorem, 105 
Finite element methods, 12 
Free end-time problems, 412 
state constraints, 498, 567 
Fuzzy calculus, 216 
G 
Galerkin methods, 12 
Gronwall’s inequality, 254 
Proximal, 293 
H 
Hamilton condition, 20, 21 
Hamiltonian, 18 
constancy condition, 17 
inclusion, 60 
un-maximized, 33 
Hamilton Jacobi equation, viii, 25–30, 667 
lower Dini solutions, 669 
proximal solutions, 669 
Hausdorff distance function, 273 
I 
Inf convolution, 128, 207 
Infinite Horizon Control, 88 
Infinite horizon problems, 728 
minimizers existence, 292 
Invariance 
Strong, Autonomous, 660 
Strong, Time Varying, 662 
Weak, Autonomous, 655 
Weak, Time Varying, 658 
J 
Jacobi condition, 25 
Jensen’s inequality, 110 
L 
Lagrange multiplier rule, 232–236 
Lavrentiev phenomenon, 597 
Lebesgue essential value, 466 
Legendre condition, 25 
Legendre-Fenchel transformation, 109 
Legendre transformation, 19 
Lipschitz continuity, criteria, 190, 194 
Lower semi-continuous envelope, 742 
M 
Magasarian Fromowitz condition, 529 
Marginal function, 106, 220 
Maximum orbit transfer problem, 2 
Maximum principle, 30–36, 68, 295 
Clarke nonsmooth, 297 
Clarke’s nonsmooth, boundary points of 
reachable sets, 326 
free end-time, 448 
Gamkrelidze Form, 537 
multiprocess, 325 
Max rule, 51 
finite family of functions, 231 
infinite family of functions, 236 
Mean value inequality, 167–173 
proximal, 168 
two sided, 172 
Measures, weak∗ convergence, 477 
Minimax theorem, 134–145 
Aubin one-sided, 137 
Von Neumann, 136 
Minimizer 
L∞ local, 68, 474 
W1,1 local, 474 
W1,1 local minimizer relative to a 
multifunction B, 342 
W1,1 local, free end-time, 414 
W1,1 local, free end-time process, 448 
W1,s local, 405 
intermediate, 292 
Moreau-Yosida envelope, 128 
Multifunction 
lower/upper semi-continuous, 94 
measurable, 95 
measurable selection, 103 
Multiplier rule, 52, 55 
N 
Necessary conditions 
Kaskosz Lojasiewicz-Type, 404 
mixed constraints, 524 
mixed constraints, free end-time, 540 
non-degenerate, 574 
Nonsmooth analysis, 45, 59, 150–216 
optimal control, 41Index 781
Normal cone, 151–155 
of convex analysis, 155 
limiting, 46, 152, 194 
proximal, 45, 151 
strict, 152, 194 
P 
Picard iteration, 248 
Poisson’s equation, 12 
Principle of optimality, 653 
Product Rule, 232 
Proper function, 109 
Proximal normal inequality, 151, 162 
R 
Rademacher’s theorem, 49 
Regularity, 592–638 
Tonelli, 598, 600 
Regular point, tonelli, 599 
Relaxation theorem, 267 
Running cost problem, 759 
S 
Saddlepoint, 135 
Sensitivity, 40, 698 
Set Convergence, 92 
Snell’s Laws, 11 
State augmentation, 299 
Stegall’s theorem, 133 
Strictly Differentiable Function, 543 
Subdifferential, 156–161 
asymptotic limiting, 158 
asymptotic proximal, 158 
asymptotic strict, 158 
characterization, 173–177 
Clarke, 180 
of convex analysis, 161 
limiting, 47, 157 
limiting, indefinite integral function, 425 
partial limiting, 224 
proximal, 47, 157 
strict, 157 
Sub essential value, 424 
Sum rule, 51, 226 
Superdifferential 
limiting, indefinite integral function, 425 
Super essential value, 424 
Synthesis, optimal, 38, 652 
T 
Takahashi’s minimization theorem, 145 
Tangent cone, 194–201 
bouligand, 194 
clarke, 194 
clarke, interior, 201 
Tonelli, direct method, 22–25 
Tonelli set, 600 
Transversality condition, 21 
Truncation function, 247 
Two point boundary value problem, 34 
U 
Upper semi-continuous envelope, 742 
V 
Value function, 28, 645 
Verification function, 647 
Verification theorem, 685–697 
Viscosity solutions, 741 
comparison theorem, 757 
W 
Weierstrass condition, 18, 21
