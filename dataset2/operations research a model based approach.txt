Operations 
Research
H. A. Eiselt
Carl-Louis Sandblom
A Model-Based Approach
Third Edition
Springer Texts in Business and EconomicsSpringer Texts in Business and EconomicsSpringer Texts in Business and Economics (STBE) delivers high-quality instruc￾tional content for undergraduates and graduates in all areas of Business/Management
Science and Economics. The series is comprised of self-contained books with a
broad and comprehensive coverage that are suitable for class as well as for individual
self-study. All texts are authored by established experts in their fields and offer a
solid methodological background, often accompanied by problems and exercises.H. A. Eiselt • Carl-Louis Sandblom
Operations Research
A Model-Based Approach
Third EditionH. A. Eiselt
Faculty of Business Administration
University of New Brunswick
Fredericton, NB, Canada
Carl-Louis Sandblom
Department of Industrial Engineering
Dalhousie University
Halifax, NS, Canada
ISSN 2192-4333 ISSN 2192-4341 (electronic)
Springer Texts in Business and Economics
ISBN 978-3-030-97161-8 ISBN 978-3-030-97162-5 (eBook)
https://doi.org/10.1007/978-3-030-97162-5
# The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland
AG 2012, 2022
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether
the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of
illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and
transmission or information storage and retrieval, electronic adaptation, computer software, or by
similar or dissimilar methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a specific statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or
the editors give a warranty, expressed or implied, with respect to the material contained herein or for any
errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional
claims in published maps and institutional affiliations.
This Springer imprint is published by the registered company Springer Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, SwitzerlandOnce upon a time, there was a little boy who
liked nothing better than to sit in his yard and
look at the moon after it rose. One day, he
asked his dad: “Daddy, I have been looking at
the moon for many weeks now, and it always
looks the same. Is this all there is to it then?”
“Hmm,” replied his father and pondered the
situation. The next day, his father returned to
his son, who was again looking at the moon.
He handed him a book. “Here son,” he said.
“This is a book with pictures that show you
the moon from many different angles. This
may explain what the moon is all about.” The
little boy smiled as he read the book. It was a
good book.
Jesús PeregrinoPreface
Since the 1960s, operations research (or, alternatively, management science) has
become an indispensable tool in scientific management. In simple words, its goal on
the strategic and tactical levels is to aid in the decision-making process and, on the
operational level, automate decision making. Its tools are algorithms, procedures that
create and improve solutions to a point at which optimal or, at least, satisfactory
solutions have been found.
While many texts on the subject emphasize methods, the focus of this book is on
the applications of operations research in practice. Typically, a topic is introduced by
means of a description of its applications, a model is formulated, and its solution is
presented. Then the solution is discussed and its implications for decision making are
outlined. We have attempted to maximize the understanding of the topics by using
intuitive reasoning while keeping mathematical notation and the description of
techniques to a minimum. The exercises are designed to fully explore the material
covered in the chapters, without resorting to mind-numbing repetitions and
trivialization.
The book is designed for (typically second year) students of business manage￾ment and industrial engineering. With the appropriate deletions, the material can be
used for a one-semester course in the subject, while the complete material will be
sufficient for a full-year course. Parts of the book require some, albeit rudimentary,
knowledge of differential calculus. The reasoning and explanations are intuitive
throughout. Each algorithm is followed by a numerical example that shows in detail
how the method progresses. After presenting the applications and the techniques,
each chapter ends with a number of fully solved examples that review the concepts
covered in the chapter. Some more technical material and additional practice
problems have been taken out and are available at the file-sharing site https://drive.
google.com/drive/folders/1yCvgkZ4lrF_EruXQqJsZO5kWkcoM5xvE & the mirror
site drive.google.com/drive/folders/17iILwG52uqwWxCMQa74csG4lEWtS4KTA.
In addition to clarifications and modifications as well as corrections of some typos,
the third edition includes new chapters on nonlinear programming and on reliability,
material that is typically covered in courses on industrial engineering. Furthermore,
some of the previously existing material has been streamlined.
It is our pleasure to thank all the people who have made this volume possible.
Special thanks are due to Mrs. Jialin Yan and Mrs. Ramya Prakash. Their
viiencouragement and assistance are much appreciated. Thanks also to the Buddha
Man for his meticulous typing and to Mr. Logan Geraghty for his help with the
figures. Without the help of all of these individuals, this book would not have seen
the light of day. We like to thank all of them.
Fredericton, NB, Canada H. A. Eiselt
Halifax, NS, Canada C.-L. Sandblom
viii PrefaceSymbols and Definitions
The list below includes the special symbols used in the book. While we have made
every possible attempt to keep notation to a minimum, some symbolism will be
required.
Notation
2 : Element of
⊆: Subset
⊂: Proper subset
[ : Union of sets
\ : Intersection of sets
∅: Empty set
|S|: Cardinality of the set S
x 2 [a, b]: a  x  b
x 2 [a, b[: a  x < b
x 2 ]a, b]: a < x  b
x 2 ]a, b[: a < x < b
Fig. 1 Non-continuous functions
ixdxe : Ceiling of x, the smallest integer greater than or equal to x
bxc : Floor of x, the largest integer smaller than or equal to x
|x| : Absolute value of x
a: = a + b: Valuation, a is replaced by a + b
Definition 1: In nonmathematical terms, we will call a function continuous, if it
can be drawn without lifting the pen from the paper. In other words, it does not
contain holes, jump discontinuities, or vertical asymptotes, as shown in Fig. 1.
Definition 2: A function is differentiable, if its derivative exists at all points (thus
excluding noncontinuous functions, V-shaped functions or those with cusps, and
functions with vertical tangent lines).
x Symbols and DefinitionsContents
1 Introduction to Operations Research ....................... 1
1.1 The Nature and History of Operations Research . . . . . . . . . . . 1
1.2 The Main Elements of Operations Research . ............. 4
1.3 The Modeling Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
Reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2 Linear Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.1 Introduction to Linear Programming . . . . . . . . . . . . . . . . . . . . 15
2.2 Applications of Linear Programming . . . . . . . . . . . . . . . . . . . 20
2.2.1 Production Planning . . . . . . . . . . . . . . . . . . . . . . . . . 20
2.2.2 Diet Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
2.2.3 Allocation Problems . . . . . . . . . . . . . . . . . . . . . . . . . 28
2.2.4 Blending Problems . . . . . . . . . . . . . . . . . . . . . . . . . . 34
2.2.5 Transportation and Assignment Problems . . . . . . . . . . 38
2.2.6 Dynamic Production-Inventory Models . . . . . . . . . . . 45
2.2.7 Employee Scheduling . . . . . . . . . . . . . . . . . . . . . . . . 49
2.2.8 Cutting Stock Problems . . . . . . . . . . . . . . . . . . . . . . 52
2.3 Graphical Representation and Solution . . . . . . . . . . . . . . . . . . 70
2.3.1 The Graphical Solution Method . . . . . . . . . . . . . . . . . 71
2.3.2 Special Cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
2.4 Postoptimality Analyses . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
2.4.1 Graphical Sensitivity Analyses . . . . . . . . . . . . . . . . . 90
2.4.2 Optimal Solution and Sensitivity Analyses
on a Printout . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
2.5 Duality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
3 Multiobjective Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
3.1 Vector Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
3.2 Solution Approaches to Vector Optimization Problems . . . . . . 131
3.3 Goal Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
xi4 Nonlinear Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
4.2 Applications of Nonlinear Programming . . . . . . . . . . . . . . . . . 146
4.2.1 Forestry Logging . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
4.2.2 Maximizing Tax Revenues . . . . . . . . . . . . . . . . . . . . 147
4.2.3 Optimizing Inventory and Queuing Models . . . . . . . . 148
4.3 Properties of Nonlinear Programming Problems . . . . . . . . . . . 149
4.4 Solution Methods for Nonlinear Optimization . . . . . . . . . . . . . 152
4.4.1 Newton’s Method for Unconstrained Optimization . . . 153
4.4.2 Constrained Optimization . . . . . . . . . . . . . . . . . . . . . 154
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
5 Integer Linear Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
5.1 Definitions and Basic Concepts . . . . . . . . . . . . . . . . . . . . . . . 162
5.2 Applications of Integer Programming . . . . . . . . . . . . . . . . . . . 170
5.2.1 Diet Problems Revisited . . . . . . . . . . . . . . . . . . . . . . 172
5.2.2 Land Use . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
5.2.3 Modeling Fixed Charges . . . . . . . . . . . . . . . . . . . . . . 175
5.2.4 Workload Balancing . . . . . . . . . . . . . . . . . . . . . . . . . 179
5.2.5 Political Districting . . . . . . . . . . . . . . . . . . . . . . . . . . 181
5.3 Solution Methods for Integer Programming Problems . . . . . . . 183
5.3.1 Cutting Plane Methods . . . . . . . . . . . . . . . . . . . . . . . 183
5.3.2 Branch-and-Bound Methods . . . . . . . . . . . . . . . . . . . 188
5.3.3 Heuristic Methods . . . . . . . . . . . . . . . . . . . . . . . . . . 194
6 Network Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
6.1 Definitions and Conventions . . . . . . . . . . . . . . . . . . . . . . . . . 215
6.2 Social Network Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
6.3 Network Flow Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220
6.4 Shortest Path Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
6.5 Spanning Tree Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238
6.6 Routing Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
6.6.1 Arc Routing Problems . . . . . . . . . . . . . . . . . . . . . . . 241
6.6.2 Node Routing Problems . . . . . . . . . . . . . . . . . . . . . . 249
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
7 Location Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
7.1 The Major Elements of Location Problems . . . . . . . . . . . . . . . 267
7.2 Covering Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270
7.2.1 The Location Set Covering Problem . . . . . . . . . . . . . 271
7.2.2 The Maximal Covering Location Problem . . . . . . . . . 274
7.3 Center Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
7.3.1 1-Center Problems . . . . . . . . . . . . . . . . . . . . . . . . . . 278
7.3.2 p-Center Problems . . . . . . . . . . . . . . . . . . . . . . . . . . 280
7.4 Median Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281
7.4.1 Minisum Problems in the Plane . . . . . . . . . . . . . . . . . 281
xii Contents7.4.2 Minisum Problems in Networks . . . . . . . . . . . . . . . . 285
7.5 Other Location Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302
8 Project Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
8.1 The Critical Path Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304
8.2 Project Acceleration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312
8.3 Project Planning with Resources . . . . . . . . . . . . . . . . . . . . . . 317
8.4 The PERT Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 320
Reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332
9 Machine Scheduling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333
9.1 Basic Concepts of Machine Scheduling . . . . . . . . . . . . . . . . . 334
9.2 Single-Machine Scheduling Models . . . . . . . . . . . . . . . . . . . . 336
9.3 Parallel Machine Scheduling Models . . . . . . . . . . . . . . . . . . . 340
9.4 Dedicated Machine Scheduling Models . . . . . . . . . . . . . . . . . 345
Reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352
10 Decision Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
10.1 Introduction to Decision Analysis . . . . . . . . . . . . . . . . . . . . . . 353
10.2 Visualizations of Decision Problems . . . . . . . . . . . . . . . . . . . . 355
10.3 Decision Rules Under Uncertainty and Risk . . . . . . . . . . . . . . 357
10.4 Sensitivity Analyses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363
10.5 Decision Trees and the Value of Information . . . . . . . . . . . . . 366
10.6 Utility Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
11 Multiattribute Decision Making . . . . . . . . . . . . . . . . . . . . . . . . . . 385
11.1 The General Model and a Generic Solution Method . . . . . . . . 385
11.2 TOPSIS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390
11.3 The Analytic Hierarchy Process . . . . . . . . . . . . . . . . . . . . . . . 391
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399
12 Inventory Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401
12.1 Basic Concepts in Inventory Planning . . . . . . . . . . . . . . . . . . 401
12.2 The Economic Order Quantity (EOQ) Model . . . . . . . . . . . . . 404
12.3 The Economic Order Quantity with Positive Lead Time . . . . . 407
12.4 The Economic Order Quantity with Backorders . . . . . . . . . . . 410
12.5 The Economic Order Quantity with Quantity Discounts . . . . . . 412
12.6 The Production Lot Size Model . . . . . . . . . . . . . . . . . . . . . . . 414
12.7 The Economic Order Quantity with Stochastic Lead Time
Demand . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 417
12.7.1 A Model that Optimizes the Reorder Point . . . . . . . . . 418
12.7.2 A Stochastic Model with Simultaneous
Computation of Order Quantity and Reorder Point . . . 419
12.8 Extensions of the Basic Inventory Models . . . . . . . . . . . . . . . 420
Reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 424
Contents xiii13 Stochastic Processes and Markov Chains . . . . . . . . . . . . . . . . . . . 425
13.1 Basic Ideas and Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . 425
13.2 Steady-State Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 430
13.3 Decision Making with Markov Chains . . . . . . . . . . . . . . . . . . 431
14 Reliability Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 439
14.1 Fundamentals of Reliability Systems . . . . . . . . . . . . . . . . . . . 440
14.2 Time Aspects of Reliability . . . . . . . . . . . . . . . . . . . . . . . . . . 445
14.3 Failure Rates and the Hazard Function . . . . . . . . . . . . . . . . . . 447
14.4 Redundancy and Standby Systems . . . . . . . . . . . . . . . . . . . . . 450
14.5 Estimating Reliability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451
Reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455
15 Waiting Line Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457
15.1 Basic Queuing Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458
15.2 Optimization in Queuing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 466
16 Simulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 473
16.1 Introduction to Simulation . . . . . . . . . . . . . . . . . . . . . . . . . . . 474
16.2 Random Numbers and Their Generation . . . . . . . . . . . . . . . . . 475
16.3 Examples of Simulations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 479
16.3.1 Simulation of a Waiting Line System . . . . . . . . . . . . . 480
16.3.2 Simulation of an Inventory System . . . . . . . . . . . . . . 483
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 494
17 Heuristic Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 495
Reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 504
Appendices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 505
Appendix A: Vectors and Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . 505
Appendix B: Systems of Simultaneous Linear Equations . . . . . . . . . . 506
Appendix C: Probability and Statistics . . . . . . . . . . . . . . . . . . . . . . . 509
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 515
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 517
xiv ContentsIntroduction to Operations Research 1
In its first section, this introductory chapter first introduces operations research as a
discipline. It defines its function and then traces its roots to its beginnings. The
second section highlights some of the main elements of operations research and
discusses a number of potential difficulties and pitfalls. Finally, the third section of
this chapter suggests an eight-step procedure for the modeling process.
1.1 The Nature and History of Operations Research
The subject matter, operations research or management science (even though there
may be philosophical differences, we use the two terms interchangeably), has been
defined by many researchers in the field. Definitions range from “a scientific
approach to decision making” to “the use of quantitative tools for systems that
originate from real life,” “scientific decision making,” and others. In the
mid-1970s, the Operations Research Society of America (then one of the two large
professional societies in the field) defined the subject matter as follows:
Operations Research is concerned with scientifically deciding how to best design and
operate man-machine systems usually under conditions requiring the allocation of scarce
resources.
Today, the Institute for Operations Research and Management Science
(INFORMS) markets operations research as the “science of better.” What all of
this essentially means is that the science uses indeed quantitative techniques to make
and prepare decisions, by determining the most efficient way to act under given
circumstances. In other words, rather than throwing large amounts of resources (such
as money) at a problem, operations research will determine ways to do things more
efficiently.
Rather than being restricted to being a toolkit for quantitative planners, operations
research is much more: it is a way of thinking that does not just “do things,” but,
# The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
H. A. Eiselt, C.-L. Sandblom, Operations Research, Springer Texts in Business and
Economics, https://doi.org/10.1007/978-3-030-97162-5_1
1during each step of the way, attempts to do them more efficiently: the waitress, who
provides coffee refills along the way rather than making special trips; the personnel
manager, who (re-) assigns employees so as to either minimize the number of
employees needed or schedule employees to shifts, so as to make them more
pleasant; the municipal planner, who incorporates the typically widely diverging
goals and objectives of multiple constituents or stakeholders when locating a new
sewage treatment plant; and the project manager, who has to coordinate many
different and independent activities. All of these individuals can benefit from the
large variety of tools provided by operations research.
There are a number of obstacles that stand in the way of the extensive use of
operations research in practice. One of those obstacles is awareness. If managers
were to be able to realize that a problem may possibly benefit from the use of a
quantitative analysis, it does not matter at all, whether or not they can perform the
analyses themselves: there are plenty of specialists out there to do the job. The first
step, though, requires someone to simply realize that operations research could be
applied to a problem that presently requires a solution. This is the reason why we
have written this book with a strong focus on applications.
When trying to find out where you are and where you are going, it is always a
good idea to determine where you come from. The next few paragraphs will
highlight some of the main milestones to operations research. Clearly, space
limitations require us to cut many corners. We would like to refer to the eminently
readable history of operations research by Assad and Gass (2011).
What are usually considered to be early contributions are usually advances in
mathematics or statistics: Diophantine’s discourses on integer solutions to linear
equations in the third century AD are related to integer programming; Euler’s work
on the Königsberg bridge problem in 1736 is the first occurrence of graph theory;
Pascal, Bernoulli, and Bayes have made major advances in statistics. All results
found by these and many other scientists have put down a mathematical and
statistical foundation, on which operations research (and many other disciplines)
can rest comfortably.
While many authors credit the advances in the military in World War II to the
birth of operations research, we believe that the groundwork was laid considerably
earlier. F.W. Taylor is often called the “father of scientific management,” when he
performed his time studies in 1881. His main question was “what is the best way to
do a job,” which could very well be the motto of operations research. Henry L. Gantt
introduced bar charts, “Gantt charts” in today’s parlance, for scheduling problems,
and Agner Krarup Erlang introduced the discipline of queuing in 1909 when
working at the Copenhagen Telephone Exchange. Another contribution in the
early days was made by F.W. Harris in 1913, when he developed the “economic
order quantity” for inventory management, a result that is so robust that it is, in one
way or another, used to this day. All of these individuals would today be referred to
as industrial engineers, as their main concern was the smooth functioning of
industrial processes.
It is hardly surprising that these early contributions occurred at a time that saw
more complex industrial processes (the assembly line is but one example), a
2 1 Introduction to Operations Researchtremendous increase in the division of labor, and with it the need for coordination of
activities.
Later notable work was performed by John von Neumann in the 1920s, when he
introduced the theory of games to the world. In this context, the theory of games
must be understood as the study of competitive situations. As such, it is little surprise
that economists are among the main users of game theory. Leontief’s input–output
models and Kantorovich’s mathematical planning models for the Soviet economy
were main contributions in the 1930s. The 1940s saw Hitchcock’s transportation
problem, Stigler’s diet planning, and the aforementioned advances based on military
applications.
However, the main event occurred in August of 1947 when George Bernard
Dantzig developed what is now called the simplex method for linear programming.
Arguably, no other event has influenced the science of operations research more than
this development. Other main developments are due to John F Nash, who extended
von Neumann’s results in game theory and proved some main theorems, Bellman’s
dynamic programming principle in 1950, and Kuhn and Tucker’s optimality
conditions for nonlinear optimization problems in 1951 (which was later discovered
to be a reinvention of work by Karush in 1939). The year 1951 saw not only the first
full publication of Dantzig’s simplex method in the open literature, but also the first
computer-based simplex method. As a matter of fact, the advances in computing
hardware and software had a tremendous impact on the advances of operations
research. Without the progress made in computer sciences, operations research
would not have been able to gain the status it has today.
New results keep pouring in. Starting in 1950, the Operational Research Quar￾terly (later renamed the Journal of the Operational Research Society) was the first
journal in the field published in the UK. The first American journal followed in 1952
with the Journal of the Operations Research Society of America. Today, many
countries have their own operations research journals. To name a few, there are
the European Journal of Operational Research (the largest operations research
journal by size, close to 10,000 pages per year), INFOR (Canada), the OR Spectrum
(Germany), TOP (Spain), the Central European Journal of Operations Research
(Austria), the Yugoslav Journal of Operations Research (even though the country of
Yugoslavia no longer exists), OPSEARCH (India), Pesquisa Operacional (Brazil),
and many more. In addition, there are many specialist journals, such as Computers &
Operations Research, Mathematical Programming, Management Science, Naval
Research Logistics, and many others.
And wherever there is a national journal, more often than not there is a national
society behind it. Each of these societies has an annual meeting, where researchers
present their latest findings. Again, in addition to these national conferences, there
are meetings devoted to special topics such as optimization, logistics, supply chains,
location, transportation, scheduling, and many more. It should be apparent by now
that the number of contributions can only be described as vast.
1.1 The Nature and History of Operations Research 31.2 The Main Elements of Operations Research
This section will briefly explain the main elements of operations research. Essen￾tially, operations research is concerned with quantitative (mathematical) models and
their solution. This is actually where some people make a distinction: they claim that
while management science is mostly concerned with models, operations research
deals mostly with solution methods. The model that we build for a given scenario is a
(hopefully close) picture of reality and consists of mathematical expressions and
relations intended to describe the problem at hand. A model will never include all
components a real situation does: features such as some of the choice criteria used by
consumers are not fully observable and will have to be ignored; decision makers’
risk aversion, while it may be included to some degree in a decision-making model,
is not completely explainable and will have to be estimated or ignored. It is very
important to distinguish between the original real-life problem and the mathematical
model that is built. The step from the problem to the model entails gains and losses:
while we are losing some information, we gain solvability and gain insights into the
structure of the problem. As one of the founders of the science once wrote “the
purpose of mathematical programming [a subtopic of operations research, eds.] is
insight, not numbers.”
The next issue to be decided upon is the level of aggregation. In conjunction with
the decision maker the modeler will have to decide what to include in the model and
what to leave out. Comprehensive models are nice and avoid problems with subop￾timal solutions, but they are large, labor-intensive, and thus expensive. It depends on
the specific situation what level is appropriate. The aforementioned suboptimal
solutions may occur, if we were to optimize for only one department of a firm. For
instance, the manager of a shipping department of a large firm that manufactured
construction equipment consistently hired employees through a temp agency, even
though this was much more expensive than hiring people directly. However, tempo￾rary employees were paid by headquarters, while the pay for regular employees
came directly out of the manager’s budget. Thus, hiring temporary employees was
optimal for the manager, even though it cost the firm much more money.
The usual way to describe situations for operations research models is to first list
everything we want to do, and everything that we have to do and respect. What we
want to achieve (high profit, high levels of customer satisfaction, a large market
share, low production costs, or similar) is summarized in our objective(s). On the
other hand, all requirements (such as budget limitations, limitations of a firm’s
capabilities with respect to manpower, knowledge base, available machinery, and
others) are summarized in the constraints. While in most operations research tools,
objectives and constraints are clearly separated, this is not always true in reality.
Consider, for instance, a simple budget constraint. It will state that the amount of
money we can spend in, say, a month cannot exceed the amount of money that we
have. We can write this as a formal constraint, as long as we are aware of the fact that
a constraint in operations research is considered absolute. This means that whatever
the solution technique is, it will not consider any solution that violates this constraint.
In other words, it will consider the constraint as hard. In practice, however, the
4 1 Introduction to Operations Researchconstraint may not be that hard. We could, for instance, take out a loan and
temporarily spend more than we have. While this is necessarily a temporary measure
and we would like to avoid it, it is possible in practice. Written as a constraint,
however, it is not possible. There are techniques that address this problem by
formulating the models differently, allowing short-term deficits, but trying to mini￾mize this. Notice that doing so transforms what we used to clearly consider a
constraint into part of the objective function.
Another important issue to deal with is the issue of measurement. While many of
the features of a model are easily measurable (profits, travel time, processing times,
the length and width of a stretch of road to be paved are typical examples), others
may not be. Particularly in models involving human psychological and sociological
factors regarding the public sector, it is not always clear how to measure entities in
the model. For instance, when locating a desirable public facility, how do we
measure accessibility of the location? We may express this feature in terms of
average distance to the potential clients of this facility, but other issues such as
opening hours or parking may be just as relevant. A particular difficulty is presented
when dealing with models that include such nebulous concepts as “fairness.” Take
the issue of a simple speeding ticket. The penalty is supposed to hurt the speeder and
thus make an impression. An obvious constraint is that the law must treat everybody
equally. So whoever speeds will receive the same penalty, say $100. Such a penalty
will, of course, hurt people with a small annual income a lot more than somebody
who makes several hundred thousand dollars a year. Thus, while the penalty is equal,
the pain is not. Alternatively, one could assess the penalty as, say, 1% of the monthly
net income, thus trying to distribute the pain equally (it still does not, as a $10
penalty for somebody with a $1000 monthly net income hurts more than a $1000
penalty for somebody with a $100,000 monthly income). This would lead to
ridiculous penalties, such as $50,000 for speeding and, in the final analysis, is
nonsensical as it negates any incentive to earn more, as prices, fines, and other
expenses are adjusted accordingly. We leave decisions regarding this issue to the
decision makers; the purpose of our discussion was to highlight the difficulty
expressing some features of a model quantitatively.
Typically, in many problems, public and private, the objective is ill-defined or
fuzzy. This will require the decision maker to formulate a surrogate or proxy
expression instead. For instance, the measurable criterion “profit” may be a proxy
for the “well-being of the company.” Again, moving from the true objective to a
proxy involves gains and losses: we lose as the proxy expression does not do exactly
what we want our objective to do, but we gain by obtaining something that is
measurable and thus can be included in a quantitative model. The choice of a suitable
proxy expression is crucial for the applicability of the model.
Each operations research model will consist of mathematical expressions involv￾ing parameters and variables. While parameters are fixed and given numbers that we
know (or can determine), but that are not under our control, variables are numbers
that we do not know (but would like to know), and which are under our direct
jurisdiction. Consider a few examples. The estimated demand for a product is a
parameter, while the number of units that we make is a variable. The amount of beef
1.2 The Main Elements of Operations Research 5we put in each can of chili is under our jurisdiction and thus a variable, while the
nutritional content of beans in the can is not under our control and thus a parameter.
The type of truck we use for a shipment and the route the truck takes are variables,
while the location of our customer’s warehouse is a parameter. The purpose of the
solution method is then to determine the actual values of the variables, i.e., determine
the production level, the quantity of beef in a can, and the type of truck and route that
a truck takes.
As far as solution techniques are concerned, we distinguish between exact and
heuristic solution techniques. An exact technique will find a solution that respects all
constraints included in the model and optimizes the objective specified by the
decision maker. Note that such a solution is actually optimal for the model but not
necessarily for the problem: if the model is only a very rough approximation of the
problem, the solution that is labeled “optimal” will not be very useful to the decision
maker, as it is optimal only for the model, not the problem. Solutions obtained by
heuristic or approximation methods are typically much easier to find, but, as the
name implies, may not necessarily be the best (or even very good for that matter).
For more details, readers are referred to Chap. 17. Solution techniques come in two
versions: they are either closed-form solutions or iterative algorithms. A closed-form
solution is essentially a formula or a set of formulas: we input the parameters and use
the formula(s) to obtain values for our variables. Few models exist for which closed￾form solutions exist. Most models require the use of iterative algorithms. (As an
aside, the name algorithm derives from the Persian mathematician Al-Khwarizmi,
who worked and published in algebra around 820 AD.) An iterative algorithm starts
with an iteration counter, initially set to 0, and a solution x
0
, possibly a guess, the
solution that is presently employed, or some other solution, and then performs a test
that checks whether or not the solution satisfies certain criteria (such as feasibility or
optimality). If the solution passes the test, it is accepted and the algorithm terminates;
otherwise, the present solution is modified by the algorithm, a new solution x
k
(typically an improved guess) is generated, the iteration counter is increased by
one, and the new solution returns to the test. At this point, we are in a loop, which is
repeated, until an acceptable solution has been found. Each loop that involves the
test and an improvement constitutes an iteration of the algorithm. Many modern
large-scale problems require thousands, if not millions, of iterations to find a
solution. This explains why the use of high-speed digital computers is crucial for
the solution of nowadays models. The flow chart in Fig. 1.1 may explain the main
ideas.
The above discussion has made frequent mention of the concept of solution. It is
important to realize that a solution is a set of instructions. In production planning, it
will tell the decision maker what quantities to produce, in diet planning it will tell the
chefs what meals to prepare (e.g., in seniors’ residences), and in transportation
planning it will tell the dispatcher which trucks and which drivers to dispatch
where, and with what loads. Associated with each solution is a value of the objective
function, a value that tells the planner how much money will be made if a certain
production plan is adopted, how much it will cost, if a certain meal schedule is
followed, and what are the consequences if we schedule trucks and drivers in a
6 1 Introduction to Operations Researchcertain way. As it has no part in reconstructing a specific course of action, we will
consider the value of the objective function the consequence of the solution, but not
part of it.
There are four major concerns or phases when applying any operations research
model. They are
1. Feasibility (can we do this?)
2. Optimality (is this the best we can do with what we have?)
3. Sensitivity (what happens if some of the input parameters or conditions beyond
our control change)
4. Implementability (is the solution that we have obtained something that we can
actually do?)
We will explain the first three phases in a very simple numerical
Example A company faces a demand for its product. The magnitude q of the
demand is a function of the price p, both of which are to be set by the company. It
is known that the price-quantity relation is p ¼ 16  2q, meaning that starting at $16
per unit, each unit increase of the demand decreases the price all customers are
willing to pay for the product by $2. Clearly, the function is defined only for
quantities no larger than 8, as for quantities higher than 8, the price would be
negative. In addition to the general quantity-independent fixed costs of $20, the
cost function is C(q) ¼ ½q2
. The company wants to maximize its profit.
We assume that the company will set the price p and the quantity q at such values
that the market is cleared, i.e., the given price p will cause customers to purchase
exactly the produced amount q. In view of the price-quantity relation, this means that
fixing the value of one of the two variables will determine the value of the other one;
Solution 
x0. Set k:=0.
Does xk satisfy 
the desired 
property?
Determine 
xk+1 = f(xk
) 
and set 
k:= k + 1
Yes
No
Stop 
Fig. 1.1 Steps of an iterative
algorithm
1.2 The Main Elements of Operations Research 7in this sense there is only one variable whose value the company can freely set. In
our discussion below, we select this to be the quantity q. In order to formulate the
problem, we will employ a very simple version of what is known as the decomposi￾tion principle. Starting with a large component of the problem that we cannot model
per se, it subdivides this component into smaller and smaller pieces, until we are able
to find expressions for it. In this example, “profit” is such a component. We first
decompose profit by using its definition as revenue minus costs. Now we have to
deal with these two components separately. First take revenue. Again, we decom￾pose the entity by using the definition “revenue equals price times quantity,” which
leaves us with these two expressions. At this point, we are able to deal with them
directly, as we know that the unknown quantity has been defined as q and the price is
p ¼ 16  2q. Now consider costs. Decomposing costs, we obtain fixed and variable
costs as its two components. The variable costs are ½q2
, and, for simplicity, we will
assume fixed cost of 0 for now. We can then put together the profit function (the
composition phase) as
P qð Þ¼ ð Þ 16  2q q  ½q2 ¼ 2:5q2 þ 16q:
To facilitate our discussion, Fig. 1.2 provides a plot of the price-quantity relation
(the solid line), the revenue function R(q) ¼ (16  2q)q (the dashed line), the cost
function C(q) ¼ ½q2 (the dash-dotted line), and the profit function P(q) (the dotted
line).
We can now explain the first three major concerns by using this example. First
consider feasibility. Let us assume that the planner considers it mandatory that we
Fig. 1.2 Revenue, cost, and profit functions of example
8 1 Introduction to Operations Researchare not making losses so that any solution that provides a loss is not feasible. (This is
not generally true: losses obviously occur and are generally unrelated to conditions
of feasibility.) This leaves us with a break-even analysis, i.e., an analysis that
determines the types of production plans that generate a nonnegative profit. This
condition is P(q) ¼ 0, and in Fig. 1.2, we are looking for the quantities at which the
profit function intersects the abscissa. In our example, this occurs at the quantity
q ¼ 6.4 (the highest quantity that generates a nonnegative profit). The reason for this
somewhat counterintuitive result is that higher quantities result in (1) lower prices
and (2) progressively higher costs due to the quadratic cost function. Introducing
nowfixed costs of, say, 20,the break-even relation isthenP(q)¼16q2.5q220¼0,
or q  1.7 or 4.7. At this point, a positive profit can only be made for quantities
between 1.7 and 4.7. Increasing the fixed costs further to, say, 30, no quantity will
result in a positive profit.
Next consider the issue of optimality. The quantity that generates the maximal
profit is the highest point of the profit curve and it occurs at the quantity q ¼ 3.2. The
unit price of our product at that point equals p ¼ $9.6 and the resulting profit is
$25.6.
Finally, let us examine what happens if some of the input parameters change. We
note that the optimal solution will be at q ¼ 3.2, regardless of the fixed cost. In
Fig. 1.2, the fixed costs are merely a line parallel to the abscissa that moves up if the
costs increase. In doing so, the area of positive profits shrinks. While the optimal
quantity does not change, the profit at optimum does: with 0 fixed costs in our
example, the profit at optimum is Pð Þ¼ q 25:6 while it shrinks to 5.6 if the fixed
costs are $20.
Another sensitivity analysis could ask what happens if the variable cost function
in the original model (we typically do not consider compounded changes) were to
change to the linear function C(q) ¼ 4q. The break-even analysis determines a zero
profit for q ¼ 6, i.e., we will have positive profits if we make at most 6 units (recall
that there is an upper bound of q  8 in order to ensure positive prices). The optimal
quantity is now q ¼ 3, with a price at optimum of p ¼ 10, a revenue of Rð Þ¼ q 30,
costs (with zero fixed costs) of Cð Þ¼ q 12 , and thus a profit of Pð Þ¼ q 18:
Introducing again fixed costs of, say, $20 will eliminate positive profits altogether.
The last sensitivity analysis—again starting from the original situation with zero
fixed costs—asks what happens if the price-quantity relation changes to p ¼ 16  1q.
Again, another inverted parabola is the profit function, and the break-even point,
here again the maximal quantity that guarantees positive profits, is at q ¼ 10.6667.
Optimality is achieved for q ¼ 5:3333 with an associated profit of Pð Þ¼ q 42:6667.
In general, in this modification, there is a larger range of quantities that guarantee a
positive profit (the reason is that the price drops more slowly than in the original
case). Since the cost function has not changed, the profit will also be higher.
In general, it is always a good idea to compare the results provided by a
mathematical solver with those the decision maker or analyst comes up with intui￾tively. If optimized solution and intuition do not match, it is important to carefully
check the model, as one of them will likely be wrong. Which one depends on the
1.2 The Main Elements of Operations Research 9experience of the person involved and the care with which the model was
formulated.
1.3 The Modeling Process
This section outlines some of the major steps that will be followed when
formulating, solving, and implementing an operations research model. Clearly,
each situation has its own idiosyncrasies and difficulties, but some general ideas
are common to all of them. We will present the main eight steps below.
Step 1: Problem recognition In order to build a successful model, the first step is for
someone to realize that it is not “business as usual,” and that it is simply no longer
good enough to follow the old “we have always done it like that” and, its sister
expression in crime, “we have never done it like that.” Problem recognition does not
only include the realization that things are not what they were thought to be, but also
what the potential for improvement actually is. The decision maker has to keep in
mind that building a model is a lengthy and expensive process that is only worth
undertaking if there appears to be significant potential for improvement. This step
takes a manager who is fully familiar with the actual situation in the firm and, at the
very least, somewhat familiar with what operations research can do.
Step 2: Authorization to model This step will require the analyst, who is in charge
of model development, to convince management of the need to produce a model for
(a part of) the operation. This “sale” of potential benefits to those who eventually
have to pay for it is obviously crucial. It requires very good communication skills on
the part of the analyst. Often, analysts make the mistake to get lost in their technical
lingo, which not only annoys decision makers, but greatly reduces the chances of
obtaining permission to model. Avoiding this pitfall requires also that the analyst
clearly understands the mindset and way of thinking of the decision makers.
Step 3: Model building and data collection This is a step, in which this book can
help with the model building. The first step of the modeler has to decide on the scope
and the level of aggregation. Is it necessary to get into small details of the operations,
or it is sufficient to adopt a macro view? This decision will have to be made in
conjunction with the decision maker(s) and maybe has already been done in the
previous step before the model building idea was developed and presented. Finding
out what is actually important may take quite a while, but spending some time on it is
definitely no waste. Once the level of aggregation has been decided upon, the
modeler will determine who the relevant stakeholders are and what their objectives
are. This is the time to decide on appropriate surrogate criteria. In addition to find out
about from management what these objectives are, it will be necessary to learn about
the constraints from the shop floor. This type of information is typically unknown in
the corner offices, and it will be of tremendous benefit to the modeler and his model
if such information is collected directly at the source. Some analysts went to great
10 1 Introduction to Operations Researchlengths to accomplish this. Gene Woolsey wrote about his modeling and data
collection adventures in the practice journal Interfaces with stories that involved
him getting a job as a worker in the unit he was supposed to model and seeing first￾hand what the actual problems were. In addition to obtaining much better data than
those typically available in offices, he also got to know the workers and their
concerns. This would come as a significant benefit later on when the results of his
modeling efforts would be implemented: the suggested solution would take these
concerns into consideration and greatly enhance chances of worker acceptance of the
changes, rather than resistance and passive boycotts of any suggested change. Once
the data have been collected, the formal model will be built. This typically starts with
a listing of the assumptions and simplifications that are made. This is important for
the decision maker later on, when the time comes to accept or reject the
recommendations made by the model. It is a lot better for the decision maker to
see right away that a model is not really applicable and not implement it, rather than
to implement it and having to live with the (dire) consequences later, just because
some assumptions did not apply in the given situation. Since circumstances may
change over time, the assumptions should be checked periodically with the decision
maker, so as to avoid wasting time. Then the decision variables are defined based on
what the decision maker(s) would like to know.
Step 4: Solution of the model This is a step in which the modeler’s technical
knowledge is required. Here, we use the appropriate computer software, and docu￾ment the model properly so as to allow future users to more or less easily take over
and use the model again without having to go through the entire process again. There
are a few pitfalls. One of them is to use some software “just because we already have
it.” If the software does what we need, then it is perfectly all right to use it, but
changing the model to suit existing software is a highly questionable procedure.
Analysts have to keep in mind that a few thousand dollars for needed software are
very little compared to the costs incurred by the model building team and the
potential benefit. Another issue to keep in mind is that modeling and model solution
nowadays—in contrast to the situation some decades ago—is an interactive process.
In “the olden days,” computational power was severely limited and expensive. As a
result, modelers tried to develop the entire model as well as possible, then had it
solved, and, when it came back with errors (which it always did), fixed the errors and
resubmitted it. This required a lot of foresight, a lot of thought, and long and arduous
searches for the errors. Nowadays, computing power is ubiquitous and cheap. A
result is that modeling is typically done in an interactive fashion. A fairly small part
of the model is developed first, then solved, and if there are any errors, they will be
easy to find, as the model is still small. Once the analyst is happy with the modeled
part, additional parts are added. The revised model is solved, and again, error
detection is easy as anything wrong must be related to the new part. This process
goes back and forth, until the entire model is developed.
Step 5: Model validation In this step, the analyst will determine whether or not the
solution to the model that was obtained in the previous step does make sense in the
1.3 The Modeling Process 11context of the real problem. If this is not the case, the model is not (yet) a faithful
representation of reality and it must be changed. In such a case, the process will
shuttle between Steps 3 and 5. It is always a good idea to determine if the solution
leads to a major change as compared to the present solution (if any). If this is the
case, it is rather unlikely that the new solution will be adopted by the decision
makers. It is also useful at this stage to include a number of sensitivity analyses in the
package prepared for the next step.
Step 6: Model presentation In this step, the analyst(s) will “sell” the solution to
management. In trying to convince management to buy into the newly developed
plan, it helps a great deal if the assumptions are clearly stated, the solution is clearly
presented (and at least tentatively checked against reality in the previous step), and
some alternative scenarios are also presented. As a matter of fact, in all cases other
than the lowest operational level, on which solutions are more or less automatically
implemented, there are decision makers, whose job is to make decisions, not to
accept or reject decisions from analysts. This means that the main function of
operations research is not so much decision making, but preparing the decision.
Presenting some workable decision alternatives is usually a good idea. This is a
crucial step, which does not only decide about the future of the model, but possibly
the future of the modelers themselves: no firm will keep employees whom it does not
see contributing to the benefit of the organization, and producing models that are not
used provides no benefit. As a matter of fact, among the models that decision makers
actually asked to be built, only a surprisingly small portion was ever implemented.
This is highly detrimental to the firm and their employees in particular and the
profession as a whole.
Step 7: Implementation Given the acceptance by the decision makers (most likely
with some modifications of the solution), the task is now to translate the model
recommendations into practice. At this point it will help a great deal if those who
have to live with the recommendation and the changes—the employees and
workers—will accept the solution rather than sabotage it. If the modeler created
goodwill in Step 3 of this procedure, listened to the concerns of those involved, and
included them in the modeling process as far as possible, chances of acceptance are
much enhanced. In addition, as the solution of the model is implemented, it is crucial
to monitor the implementation each step of the way, so that it is possible to adjust the
solution in case some unexpected changes occur.
Step 8: Monitoring and Control This phase of the process is largely overlooked. It
includes the timely comparison of the plan and reality. Reality changes, and the plan
should be adjusted accordingly. As the famous saying in the United States Marine
Corps goes, “improvise, adapt, overcome.” Furthermore, the more frequently
adjustments can and are being made, the less dramatic they will have to be. As an
example, assume that an individual has planned his budget for the upcoming year.
Suppose that $1200 have been set aside for the purpose of entertainment. If the
individual monitors the situation monthly, he may find by the end of January that he
12 1 Introduction to Operations Researchhas already spent $500 on entertainment purposes. This leaves $700 for the remain￾der of the year, or $63.64 for each of the remaining 11 months. This is considerably
less than the $100 that were planned for each month, but is still not too dramatic.
Monitoring only each second month, the individual has not noticed that he spends
way too much money and behaves in February in the same way as in January by
spending another $500. Checking the situation by the end of February, there are only
$200 for each of the remaining months, or $20 for each month. This is a much more
severe decrease from the originally planned $100 per month and will be much more
difficult to adhere to.
Reference
Assad AA, Gass SI (2011) Profiles in operations research: pioneers and innovators. Springer,
New York
Reference 13Linear Programming 2
This chapter will introduce linear programming, one of the most powerful tools in
operations research. We first provide a short account of the history of the field,
followed by a discussion of the main assumptions and some features of linear
programming. Thus equipped, we then venture into some of the many applications
that can be modeled with linear programming. This is followed by a discussion of the
underlying graphical concepts and a discussion of the interpretation of the solution
with many examples of sensitivity analyses. Each of the sections in this chapter is
really a chapter in its own right. We have kept them under the umbrella of the chapter
“Linear Programming” so as to emphasize that they belong together rather than
being separate entities.
2.1 Introduction to Linear Programming
The purpose of this section is to provide a short introduction to linear programming
problems. We first present a short historical account of the topic, followed by the
main assumptions of linear programming problems, and finally some details about
the optimization problems under discussion.
As already discussed in Chap. 1, linear programming problems were originally
developed by Leonid Vitaliyevich Kantorovich (later Nobel prize laureate) in the
1930s before being first described by George Bernard Dantzig in 1947. His findings
that included his “simplex method for linear programming” were presented in
1949 at a conference for research in economics, and some of the papers, including
Dantzig’s work, were published in 1951 in a volume edited by (later Nobel prize
laureate) Tjalling C. Koopmans. Much work was done in the early years. The first
programmed solution code based on Dantzig’s simplex method was already devel￾oped in 1951, and many applications were first discussed in those years, among them
the blending of aviation gasolines and trim loss problems. The dual simplex method
by Lemke and the Hungarian method for assignment problems were also described
and published in the 1950s. In 1963, Dantzig’s book “Linear Programming and
# The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
H. A. Eiselt, C.-L. Sandblom, Operations Research, Springer Texts in Business and
Economics, https://doi.org/10.1007/978-3-030-97162-5_2
15Extensions” was published, an “instant classic.” The year 1979 saw the publication
of a paper by Leonid Genrikhovich Khachiyan, whose “ellipsoid method” made
quite a stir. However, while it has some remarkable theoretical properties, it failed to
perform well in practice and is not in use nowadays. On the other hand, Narendra
Karmarkar’s interior point method, first described in 1984, has slowly made some
inroads. To this day, though, Dantzig’s simplex method is the method of choice for
the vast majority of all linear programming problems that are solved.
In order to structure our discussion, first consider a general mathematical pro￾gramming problem. It can be written as the problem
P : Max z ¼ f xð Þ
s:t: g xð ÞR b,
where P stands for problem (sometimes we have multiple problems under consider￾ation, in which case we will write P1, P2, and so forth), f(x) is the objective function,
which is to be maximized, and z is the objective function value (e.g., profit, market
share, sales, or cost) associated with the present solution x. This objective is to be
optimized “subject to” (or “s.t.” for short) the constraints g(x) R b. We will come to
them and their meaning again later. The use of objectives and constraints goes back
to the fundamental economic principle: either maximize the outcome for a given
input or minimize the needed input for a required output. A typical example is for
students to either decide to study a certain number of hours for a course (the input)
and then try to maximize the grade in that course (the output), or they determined
what output (grade) they are happy with and then minimize the effort (hours) to get
there.
Linear programming problems are special cases of the above formulation. In
particular, we have three fundamental assumptions in linear programming. They are:
1. Deterministic property
2. divisibility, and
3. linearity
Consider first the deterministic property. Simply put, it requires that all
parameters are assumed to be known with certainty. (By the way, the antonym of
deterministic is probabilistic or stochastic.) While this assumption appears reason￾able in some instances, it is not in others. Consider these examples. While we know
exactly how many machines we have for the processing of semi-finished products,
these machines may fail unexpectedly, making their actual capacities probabilistic.
Similarly, we know the magnitude of contracted sales, but we have only rough
estimates concerning next month’s additional demand. Given that much of the world
is probabilistic, how can we possibly apply linear programming without making
simplifications that may be so major so as to render the results unusable? (Remember
the old adage: “if a model does not fit the problem, don’t use it.”) In general, there are
two ways to circumvent the problem. One would be to resort to techniques that do
16 2 Linear Programmingnot make that assumption—such as stochastic programming, a very complex field
for which the available software is much less powerful than for deterministic linear
programming—or to circumvent the difficulty by using sensitivity analyses. This is
the procedure of choice for many decision makers. Suppose we have estimated next
month’s demand to be 500 units, but we are not sure about it. One way to handle this
is to solve the problem with a demand of 500 units. Once the optimal solution is
known, we attempt to find out what happens if the demand were to change to values
other than 500. (Notice the words “what—if” that always indicate sensitivity
analyses.) For instance, we can change the demand to, say 490, resolve the problem
and find out what happens to our solution. This process can be repeated to the
relevant values of the demand. Once this has been accomplished, the decision maker
will have a clear idea what the effects on different levels of demand are on the
solution of the problem. The limitations of this approach are obvious: the method is
valid only if a small number of uncertain parameters exist, but it will drown the
decision maker in massive amounts of data, if much of the information in the model
is uncertain.
Next consider the issue of divisibility. Divisibility is ensured, if we allow the
variables in the model to be any real number (even though we will restrict ourselves
to rational numbers most of the time). Most importantly, divisibility allows variables
to be noninteger. While many instances do not allow it, this assumption is typically
less of a problem that we may first think. Suppose that our model includes a variable
that expresses the number of cans of corn that we make and sell. It is obvious that this
number must be integer, as partial cans cannot be sold. The main question is, though:
who cares if we make 1,345,827 or 1,345,826 cans? Many of the numbers in our
model will be approximations anyway, so that it is completely irrelevant if the
number of cans in the optimal solution is integer or not. Simple rounding will
solve the problem. On the other hand, suppose that the model under consideration
includes a variable that expresses the number of houses to be built in a subdivision.
Simply rounding a noninteger solution up or down may result in solutions that may
not be as good as possible (or, depending on the constraints, not even feasible). If
integrality of a variable is essential, users are well-advised to resort to techniques
from integer programming, discussed in Chap. 5 of this volume.
Finally, the issue of linearity. (For a discussion of the issue of linearity, see
Appendix B of this book.) The assumption in linear programming is that all
expressions, the objective function as well as all constraints, are linear. One of the
underlying assumptions that leads to linearity is proportionality. As an example, if
we purchase one pound of potatoes for, say, 50¢, then proportionality means that
2 lbs will cost $1, 3 lbs will cost $1.50, and so forth. So far, our costs are proportional
to the quantity and the cost function is 0.5x with $0.5 being the per-unit price of the
potatoes and x denoting the quantities of potatoes we purchase. This is part of a linear
function. However, if the quantities we purchase become large, the merchant may
offer us a rebate, which will make the function nonlinear (or piecewise linear).
Having stated all of the main assumptions, we can now focus on the individual
components of linear programming problems. First consider the objective function.
In all of mathematical programming, objective functions either maximize a function
2.1 Introduction to Linear Programming 17or they minimize it. Often, the objective expresses the wishes of the decision maker
in monetary terms, but nonmonetary objectives are possible and common in
applications in engineering and in the public sector. Note that if the objective is
Max z ¼ f(x), it can alternatively and equivalently be written as Min z ¼ f(x).
Suppose that originally, the idea was to maximize profit, then the alternative
objective is to minimize losses. In other words, each maximization function can be
rewritten as a minimization function and vice versa. This eliminates the need for
separate methods or treatment of problems with minimization and maximization
objective.
Consider now the constraints. Rather than writing them in the very general form
g(x) R b that is often used in mathematical programming, we will write all constraints
as LHS R RHS. What this means is that the left-hand side LHS is in some desired
relation, denoted by R, to the right-hand side RHS. First, linear programming accepts
relations of the type , ¼, and . These relations compare a feature of the solution at
hand with a proscribed standard. For instance, the number of pallets of corn shipped
out of a warehouse (the reality, what “is”) on the left-hand side is compared to the
quantity that our customers actually wanted (the stipulated target, the “should”) on
the right-hand side. Similarly, we can compare our actual expenditures on the left￾hand side with the targeted expenditures (i.e., our budget) on the right-hand side.
Typically, constraints are formulated so as to have a linear function on the left-hand
side and a single parameter on the right-hand side. This bears some resemblance to
hypothesis testing, in which we compare reality (some statistic from a sample) with a
standard from a theoretical distribution.
We will defer our discussion of the many facets of constraints to Sect. 2.1 of this
book, where a variety of applications of linear programming are discussed. At this
point, we will only mention a few general principles that relate to constraints. As
already discussed in Chap. 1, constraints are “hard” in the sense that the linear
programming solver will return a “there exists no feasible solution” message in case
the set of constraints does not allow a solution to be found. This means that
constraints are absolute and must be satisfied exactly. In other words, given that
we have $100, any plan that includes expenditures of say $100.01 will not be
considered as feasible. In reality, often constraints are much softer. This will have
to be considered in the formulation, for instance, by formulating constraints more
loosely, e.g., consider the possibility of taking out a loan to increase the budget, or by
allowing solutions that do not fully satisfy a customer’s demand. Another important
advice for modeling is to avoid equations whenever possible, using inequalities
instead. Linear programming is well equipped to handle equations from a technical
point of view, but equations are very restrictive and often lead to infeasibilities. We
will return to this subject when we discuss applications and the graphical solution
method.
In order to more fully explain the structure and features of constraints, we
consider a very small numerical example. Suppose that we can manufacture com￾puter speakers for the domestic market and speakers for export. The assembly of a
domestic speaker takes 10 sec each, while an export speaker takes 12 sec. The
capacity of the machine on which both speakers are assembled is 20 hrs
18 2 Linear Programming(or 72,000 sec). Our distributor has placed orders for 2500 domestic speakers and
3000 export speakers but is prepared to take more. Each domestic speaker nets us
$40, while each export speaker contributes $45 to our overall profit.
Defining x1 and x2 as the number of domestic and export speakers made and sold,
respectively, we can formulate the linear programming problem as
P : Max z ¼ 40x1 þ 45x2
s:t: 10x1 þ 12x2  72, 000
x1  2500
x2  3000:
Whenever an inequality of either “” or “” type is inputted, the solver will
automatically add an additional variable to create an equation. These variables have
an important interpretation. In particular, whenever we have a constraint of type
LHS  RHS, the solver automatically adds a slack variable S to the left-hand side of
the constraint in order to transform the inequality to an equation. It then uses the
constraint LHS + S ¼ RHS with S  0. As an example of what a slack variable means
and why this procedure is valid, consider a simple budget constraint. In such a
constraint, the left-hand side expresses the amount of money we actually spend,
while the right-hand side specifies the amount of money that is available, so that the
constraint reads “the amount of money spent must be less or equal than the amount
of money available.” The slack variable is then the difference between left- and
right-hand side. In our budget constraint, the rewritten constraint then states that the
amount of money used (the original LHS) plus the amount of money unused (the
slack S) equals the amount of money available (the original RHS).
A similar procedure is used for inequalities of the “” type. Here, the original
constraint LHS  RHS is transformed into an equation by subtracting a surplus or
excess variable E from the left-hand side so as to arrive at the reformulated constraint
LHS  E ¼ RHS. For instance, in a production requirement, where the left-hand side
expresses the actual production, while the right-hand side value specifies the smallest
quantity that must be made, the excess variable specifies the amount by which the
present solution exceeds the requirement.
Consider now the above numerical example and suppose someone has suggested
the solution x1 ¼ 2800 and x2 ¼ 3200. First of all, this indicates that we have decided
to manufacture 2800 speakers for the domestic and 3200 speakers for the export
market. By plugging these values into the objective function, we can determine the
profit level for this solution as z ¼ 40(2800) + 45(3200) ¼ $256,000. We can also
check if this solution is feasible: the plan requires 10(2800) + 12(3200) ¼ 66,400 sec
to make, which is less than the available assembly time. As a matter of fact, given
that we have 72,000 sec on the assembly machine and this solution uses 66,400 sec,
there is a slack capacity of 72,000  66,400 ¼ 5600 sec. The next two demand
constraints are also satisfied: since we make 2800, this is more than we need to
2.1 Introduction to Linear Programming 19(actually, an excess of 2800  2500 ¼ 300 units); similarly, 3200 units for the export
market are 3200  3000 ¼ 200 units in excess of the requirement.
As an aside, the solution discussed in the previous paragraph is not optimal. At
optimum, we make 3600 units for the domestic market and 3000 units for the export
market. This results in zero slack capacity for the assembly machine and a surplus of
1100 units for the domestic market and a zero surplus for the export market. The
profit at optimum is $279,000.
2.2 Applications of Linear Programming
This section presents a variety of linear programming applications. Each of these
applications is a prototype, in the sense that “real” applications in practice will build
upon these formulations by adding lots of “bells and whistles.” However, the major
use of learning about these bare-bones formulations is to understand the type of
formulations they present and the way the variables are defined, which is typical for
the particular type of application.
The following subsections describe in detail how some scenarios can be modeled.
They may be simplistic, but the models presented here include the most important
features of the application under consideration.
2.2.1 Production Planning
The formulation that we present in this section is a very basic prototype of produc￾tion planning models. It is probably the simplest possible problem to be formulated,
and the purpose to present it at this point is to introduce some of the basic ideas of
modeling. In this context, suppose that there are three products we can manufacture.
For simplicity, we will simply refer to them as P1, P2, and P3, respectively. The three
products sell for $20, $15, and $17 per unit, respectively. These products are
manufactured on two machines M1 and M2. Each of the two products has to be
processed on both machines. At this level, the order in which to process the products
on the machines is irrelevant. The two machines have capacities of 9 and 8 hrs,
respectively, within the planning period. After that time, the machines will require
regular maintenance, a task for which they have to be shut off and are no longer
available. The processing times of the three products on the two machines
(in minutes per quantity unit) are shown in Table 2.1.
It is important to understand that the machine capacities and their usage are meant
in the following way. Suppose there is a clock attached to the machine that shows the
amount of time that is still available on the machine before the next scheduled
Table 2.1 Processing
times of the products on the
machines
P1 P2 P3
M1 354
M2 613
20 2 Linear Programmingmaintenance. Initially, the clock on the first machine shows 9 hrs or 540 min. Suppose
now that four units of P2 are processed on M1. Given that the processing time per unit
is 5 min, the available time decreases by 20 min from 540 to 520. This continues until
one of the machines has no capacity left. The process is considered automatically in
the formulation, we present it here so as to stress the way time is managed here. It is
often misunderstood that a capacity of, say, 8 hrs means a workday and the objective is
to plan what product is processed at what time on which machine. This type of micro
planning is discussed in detail in Chap. 9 of this volume.
The production costs are $120 per hour of machine time on M1, and $90 per hour
of machine time on M2. In other words, operating time costs $2 and $1.50 per minute
on the two respective machines. Given the operating times, the unit processing costs
of the three machines on M1 are $6, $10, and $8, respectively, while the unit
operating costs on M2 are $9, $1.50, and $4.50, respectively. This results in overall
unit processing costs of $15, $11.50, and $12.50 for each of the three products,
respectively. Considering the selling prices, we have profit contributions of $5.00,
$3.50, and $4.50 per unit of each of the three products.
The problem can then be formulated so as to maximize the overall profit, while
respecting the capacity constraints of the two machines. In order to formulate the
problem, we first need to define the variables. In general, variables are defined, so
that they can provide “shop floor answers,” i.e., numbers that tell the user of the
solution what to do. In this example, we would like to know the number of products
that we will manufacture. How much time we will use on the two machines in the
process does not have to be defined as a variable, since it is a direct consequence of
how many products we will make. The same holds for the profit, which is also a
consequence of what we do.
Defining x1, x2, and x3 as the number of units of the respective products P1, P2,
and P3 that we will manufacture and sell (note that we assume here that everything
we make can also be sold!), we can then start to formulate the objective function.
Knowing that the objective is to maximize the profit, which is the sum of profits that
derive from the making and selling of products, we consider one such term at a time.
For instance, making and selling one unit of P1 nets $5, and we have now decided to
make x1 units of it. This means that the profit that derives from the making and
selling of P1 will be 5x1. Similar expressions can be derived for the other two
products, resulting in the objective function shown in the formulation below.
Next, consider the constraints. We need to formulate one constraint for each
machine. Each of these constraints will state that the actual usage of the resource
called processing time does not exceed the processing time that is available. The time
available is known, so the right-hand side of the constraints is easy. In order to be
more specific, we will deal with the first machine and formulate the actual time that is
used on M1. First, we note that the time will be consumed by the making of the three
products. Consider one such product at a time, say P1. We know that it takes 3 min to
process one unit of P1 on M1, and we have decided to make x1 units of P1. This
means that the processing time on M1 that is used for the making of P1 is 3x1
minutes. Similarly, we will use a total of 5x2 minutes on M1 to make all the units of
P2 that we will manufacture, and we need 4x3 minutes on M1 to make all of P3. The
2.2 Applications of Linear Programming 21sum of these processing times now must be less than or equal to the available time of
540 min. This is the capacity constraint for M1.
The capacity constraint for the second machine is formulated in similar fashion.
Finally, we have to ensure that we are making nonnegative numbers of the
products—after all, we are in the business of manufacturing and selling the products,
rather than buying them, which is what negative values of the variables would
indicate in this context. The formulation can then be written as the problem
P : Max z ¼ 5x1 þ 3:5x2 þ 4:5x3
s:t: 3x1 þ 5x2 þ 4x3  540
6x1 þ 1x2 þ 3x3  480
x1, x2, x3  0:
Solving the problem, the optimal solution indicates that the decision maker
should make and sell 20 units of P1, no units of P2, and 120 units of P3. The
associated total profit is $640.
2.2.2 Diet Problems
The diet problem is not only among the first linear programming problems to be
solved, but it is arguably also the most intuitive model. As a matter of fact, the
problem was studied in 1939 by the (later Nobel laureate) George Stigler in the
context of determining a cost-minimal nutritious food for “an active economist who
lives in a large city.” Incidentally, he included 77 different foods in his model. His
solution, while not optimized, was found to be quite close to the optimal solution for
his data. An interesting account of the history of the diet problem is found in Garner
Garille and Gass (2001).
In general, two versions of the problem can be thought of. They are roughly
equivalent to the fundamental economic principle, applied to the determination of a
diet. The two criteria are cost and nutritional value, where cost is an input factor,
while nutritional value is an output. Thus, we can either try to minimize the cost (the
input), while guaranteeing at least a certain and predetermined nutritional value (the
output), or we can attempt to maximize the nutritional value (the output), while using
no more than a prespecified dollar value (the input). In order to determine which of
the two versions is more suitable, consider this. In the latter approach, the constraint
is a simple resource constraint that states that the total amount of money spent on
food cannot exceed the decision maker’s budget. However, the objective function is
more complex. It is stated to maximize the nutritional value. The problem with this is
that the measure “nutritional value” is not a simple number, it is what is usually
referred to as multidimensional measure, as it comprises a large number of measures:
protein, carbohydrates, fats, vitamins, minerals, etc. And it is obviously not mean￾ingful to simply add those different units together so as to obtain a single
one-dimensional measure.
22 2 Linear ProgrammingOn the other hand, the former version of the problem is much easier to handle.
The objective is then a simple (one-dimensional) cost minimization, and the
“nutritional content” is relegated to the constraints. And this is where the
requirements can be handled quite easily: for each nutritional component, we can
formulate one or two constraints, an upper and/or a lower bound. As a simple
numerical example, consider three foodstuffs, e.g., hamburger, fries, and cheesecake
(indicating that we are using the term “diet” in the widest possible sense). As far as
nutrients are concerned, we consider only calories and fat in our example. Table 2.2
shows the nutritional contents of the foodstuffs, the nutritional requirements, and the
prices per serving of the foods.
Note that our numerical example expresses the calories in terms of the usual
calories (kCal) and we have an upper and a lower bound for it. (Note that in practice,
the recommended caloric intake depends on gender and age of an individual, as well
as other factors.) On the other hand, the fat content is expressed in terms of the
recommended daily requirements, and it is an upper bound.
The problem in the cost minimization version can then be formulated as follows.
First, we define the variables. As usual, the variables are defined depending on what
the decision maker would like to know but does not at the moment. In this model, the
decision maker’s goal is to determine the quantities of the individual foods that are to
be included in the daily diet, so that we will define variables x1, x2, and x3 as the
quantities of hamburgers, medium fries, and cheesecakes in the diet. As long as we
use well-defined “servings” as units and use the same units for each food in the
objective and all constraints, there is no problem inadvertently adding apples and
oranges, or hamburgers and fries for that matter. What we are adding are their
nutritional contents, as we show in this example.
First consider the objective function. The general idea is to minimize the costs of
all foodstuffs in the diet. Consider one of these foods at a time. As shown in
Table 2.2, each hamburger costs $1.59. Since there will be x1 hamburgers in the
diet, the total costs incurred by hamburgers in the diet are 1.59x1. Similarly, the costs
that can be attributed to fries and cheesecake are 2.19x2 and 2.99x3, respectively, so
that the objective function is the sum of these three terms.
Let us now formulate the constraints. In doing so, we consider one requirement at
a time. First, we focus on the lower bound of the constraints. What we would like to
express is that the caloric content of the diet should be at least 1800 calories. The
calories in the diet derive from the three foods. The number of calories in the diet that
derive from hamburgers are 250x1, the number of calories from fries are 380x2, and
the calories from cheesecake are 257x3. Adding up these three expressions results in
Table 2.2 Input data for the sample problem
Hamburger
3½ oz
Medium fries
4 oz
Cheesecake
2.8 oz Nutritional requirement
Calories 250 380 257 [1800; 2200]
Fat 13% 31% 28% 100%
Cost per serving $1.59 $2.19 $2.99
2.2 Applications of Linear Programming 23the total number of calories that we actually have in the diet. This number should not
fall short of 1800 (the first constraint), and it should also not exceed 2200 (the second
constraint). Note that both of these constraints have the same left-hand side.
The constraint that regulates the fat content of the diet is constructed similarly.
Overall, the constraint should state that the content of fat in the diet should not
exceed 100% of the recommended daily value. Where does the fat in the diet
come from? In this example, it derives from hamburgers (for a total of 13x1), fries
(for a total of 31x2), and cheesecake (for a total of 28x3). This total fat content should
then not exceed 100%.
In summary, the problem can then be formulated as follows.
P : Min z ¼ 1:59x1 þ 2:19x2 þ 2:99x3
s:t:250x1 þ 380x2 þ 257x3  1800 calories, lower bound ð Þ
250x1 þ 380x2 þ 257x3  2200 calories, upper bound ð Þ
13x1 þ 31x2 þ 28x3  100 fat, upper bound ð Þ
x1, x2, x3  0: ð Þ nonnegativity constraints
In addition to the lower and upper bounds on the caloric intake and the upper
bound on the fat content of the diet, we include the nonnegativity constraints on the
quantities of foods included in the diet, as the idea is to eat rather than regurgitate.
Incidentally, the optimal solution to this formulation specifies that the planner eats
61=3 hamburgers, ½ serving of fries, and no cheesecake. Doing so will provide 100%
of the allowable fat intake, generate 1800 calories, and cost $11.32.
While this solution may be optimal with respect to the problem as specified
above, it is clearly riddled with a number of problems. The number of hamburgers in
the diet is obviously much too high for all but the most determined junk food junkies.
It will require additional constraints or the introduction of other foodstuffs which are
added in the loop that attempts to reconcile the solution of the model with the real
problem of finding a cost-effective diet. In order to illustrate the modeling process,
we use a somewhat larger problem with real data. The data of the eight foods
included and the 11 nutrients is found in Table 2.3.
As far as the daily nutritional requirements are concerned, we have identified the
following parameters. The diet should include
• Between 1800 and 2200 calories
• No more than 78 g of fat
• No more than 300 mg of cholesterol
• No more than 2300 mg of sodium
• Between 225 and 325 g of carbohydrates
• At least 28 g of fiber
• At least 50 g or protein, and
• At least 1300 mg of calcium
• At least 18 mg of iron
24 2 Linear ProgrammingTable 2.3 Nutritional contents and prices of foods in the model
Pasta
Tomato
juice Clam chowder
Beef
chuck
Cheddar
cheese
Orange
juice Bologna Potato chips
Required
quantity
Weight 16 oz 8 fl. oz 16 fl. oz 16 oz 16 oz 8 fl. oz 16 oz 16 oz
Calories 563 41 316 1396 1824 112 1392 2400 2 [1800,
2200]
Fat 2.6 g 0.1 g 9 g 90 g 150.4 g 0.6 g 112 153.6 g
78
Cholesterol 0 mg 0 mg 18 mg 372 mg 480 mg 0 mg 272 32 mg
300
Sodium 12.8 mg 653 mg 3353 mg 220 mg 2816 mg 1 mg 3344 2832 mg
2300
Carbs 119 g 10.3 g 39.2 g 0 g 6.4 g 25.4 g 25.6 233.6 g 2 [225, 325]
Fiber 12.8 g 1 g 2.7 g 0 g 0 g 0.6 g 0 24 g
28
Protein 24.3 g 1.8 g 18 g 136 g 113.6 g 1.7 g 68.8 36.8 g
50
Calcium 67 mg 24 mg 149 mg 72 mg 3264 mg 26 mg 384 320 mg
1300
Iron 6.4 mg 1 mg 5.4 mg 12 mg 5 mg 1.4 mg 5 mg 8 mg
18
Potassium 198 mg 556 mg 410 mg 1088 mg 448 mg 496 mg 1424 6032 mg
4700
Vitamin C 0 mg 44 mg 9 mg 0 mg 0 mg 123 mg 5 mg 176 mg
90 mg
Price per
serving
$2.30 for
16 oz
61¢ per 8 fl.
oz
$2.24 for 16 fl.
oz
$8.87 per
lb
$3.36 per
16 oz
46¢ for 8 fl.
oz
$1 for
16 oz
$4.84 for
16 oz
2.2 Applications of Linear Programming 25• At least 4700 mg of potassium, and
• At least 90 mg of Vitamin C
The nutritional values of eight foods are shown in Table 2.3. Note that prices and
nutritional contents of all drinks have been normalized to 8 fl. oz, while the servings
of all foods are shown in 16 oz. This is not necessary but has been done in order to
allow easy comparisons between foods.
We first define variables x1, ..., x8 for the number of servings of the eight foods
outlined in Table 2.3. Following the ideas developed above for the small problem,
we can now formulate the diet problem. The formulation is as follows.
P : Min z ¼ 2:3x1 þ :61x2 þ 2:24x3 þ 8:87x4 þ 3:36x5 þ :46x6 þ 1x7 þ 4:84x8;
s.t.
563x1 þ 41x2 þ 316x3 þ 1396x4 þ 1824x5 þ 112x6 þ 1392x7 þ 2400x8  1800
ð Þ Calories, lower bound
563x1 þ 41x2 þ 316x3 þ 1396x4 þ 1824x5 þ 112x6 þ 1392x7 þ 2400x8  2200
ð Þ Calories, upper bound
2:6x1 þ 0:1x2 þ 9x3 þ 90x4 þ 150:4x5 þ 0:6x6 þ 112x7 þ 153:6x8  78
ð Þ Fat
18x3 þ 372x4 þ 480x5 þ 272x7 þ 32x8  300
ð Þ Cholesterol
12:8x1 þ 653x2 þ 3, 353x3 þ 220x4 þ 2816x5 þ 1x6 þ 3344x7 þ 2832x8  2300
ð Þ Sodium
119x1 þ 10:3x2 þ 39:2x3 þ 6:4x5 þ 25:4x6 þ 25:6x7 þ 233:6x8  225
ð Þ Carbohydrates, lower bound
119x1 þ 10:3x2 þ 39:2x3 þ 6:4x5 þ 25:4x6 þ 25:6x7 þ 233:6x8  325
ð Þ Carbohydrates, upper bound
12:8x1 þ 1x2 þ 2:7x3 þ 0:6x6 þ 24x8  28
ð Þ Fiber
24:3x1 þ 1:8x2 þ 18x3 þ 136x4 þ 113:6x5 þ 1:7x6 þ 68:8x7 þ 36:8x8  50
ð Þ Protein
26 2 Linear Programming67x1 þ 24x2 þ 149x3 þ 72x4 þ 3264x5 þ 26x6 þ 384x7 þ 320x8  1300
ð Þ Calcium
6:4x1 þ 1x2 þ 5:4x3 þ 12x4 þ 5x5 þ 1:4x6 þ 5x7 þ 8x8  18
ð Þ Iron
198x1 þ 556x2 þ 410x3 þ 1088x4 þ 448x5 þ 496x6 þ 1424x7 þ 6032x8  4700
ð Þ Potassium
44x2 þ 9x3 þ 123x6 þ 5x7 þ 176x8  90
ð Þ Vitamin C
x1, x2, ... , x8  0:
ð Þ Nonnegativity constraints
Solving the model, we find that there is no feasible solution. Debugging the
model shows that the sodium, fiber, calcium, and potassium constraints are incom￾patible. An obvious course of action is then to change the model to include a food
that is low in sodium, and high in fiber, calcium, and potassium. In this example, we
have chosen whole milk (3.25% fat). The nutritional values for milk are shown in the
second column of Table 2.4. The number of servings of milk in the diet is defined as
x9, and the appropriate terms are added to the constraints, i.e., adding 0.22x9 to the
objective function, and 146x9 to the left-hand sides of the first two constraints, 8x9 to
the fat constraint, and so forth.
After the addition of milk, there still exists no feasible solution. The incompatible
constraints are now sodium, carbs upper bound, fiber, calcium, and potassium (the
nutrients that were incompatible before are shown in italics). As a potential remedy,
we add apples with skin as the tenth food; see the third column of Table 2.4. The
quantity of apples in the diet is denoted by the new variable x10, and the appropriate
Table 2.4 Nutritional
content of additional foods Criterion Milk Apples
Weight 8 fl. oz 16 oz
Calories 146 213
Fat 8 g 0.7 g
Cholesterol 32 mg 0 mg
Sodium 96 mg 5 mg
Carbs 12.8 g 56.5 g
Fiber 0 g 9.8 g
Protein 8 g 1.1 g
Calcium 272 mg 25 mg
Iron 8 mg 1 mg
Potassium 349 mg 439 mg
Vitamin C 0 mg 18 mg
Price per serving 22 ¢ for 8 fl. oz 87¢ per lb
2.2 Applications of Linear Programming 27additions to the model are made: add 0.87x10 to the objective function, add 213x10 to
the left-hand sides of the first two calorie constraints, add 0.7x10 to the left-hand side
of the fat constraint, etc.
This time we do obtain a solution to the problem, which is shown in Table 2.5 as
“Solution 0.”
By any reasonable standard, this is a weird diet with lots of milk. Limiting the
milk intake to no more than two servings (that’s still one pint per day), we add the
constraint x9  2, and obtain solution 1, again shown in Table 2.5.
This diet looks a little better, but two pounds of apples a day seems like an
overkill. It is also apparent that the restriction on milk has resulted in a 16% increase
of costs. This is, of course, not surprising, as each new nonredundant constraint
further restricts the feasible set, so that the new solution cannot possibly be better
(cheaper) and is typically worse (more expensive) than its predecessor.
Restricting the quantity of apples in the diet to one pound, i.e., one serving, we
obtain Solution 2, shown again in Table 2.5. Note that the costs have increased by
another 22%. However, for the first time, this combination of foods resembles
something that could be called a realistic diet, even though the large quantity of
orange juice is less than ideal. In this solution, fat, the upper bound on carbohydrates,
fiber, calcium, and potassium as well as the bounds on milk and apples are tight, i.e.,
the actual intake in terms of nutrients equals the respective bounds in the constraints.
As a matter of fact, whenever left- and right-hand sides equal each other in a
solution, the constraint is referred to as a bottleneck. The identification of bottlenecks
is crucial in optimization, as a bottleneck indicates that a resource is at its limit and as
such deserves particular attention. If, for instance, successive solutions to a problem
such as those above for a diet problem show that, say, the sodium upper limit
constraint is a bottleneck in many of the solutions, then it would probably be a
good idea to search for additional foodstuffs that are low in sodium.
Back to the problem at hand, it would probably be a good idea at this point to
include foods that are low on carbs and fat, and rich in fiber, calcium, and potassium.
It may also be a good idea to limit the liquids in the diet. Example: If we were to limit
the total liquid intake to four servings (that’s one quart, as each serving is 8 oz), we
would write x2 + x3 + x6 + x9  4. If this constraint were to be introduced at this point,
we would again be in a situation, in which no feasible solution exists.
We terminate our discussion at this point, which is not to suggest that the present
diet is reasonable: the purpose of this section was to introduce the reader to the
iterative nature of the modeling process that repeatedly revises the model based on
the present solution.
2.2.3 Allocation Problems
Allocation problems are one of the most prominent areas of application in linear
programming. All models in this class have in common that they deal with the
allocation of scarce resources to (economic) activities. At times, more than one
scarce resource exists, in which case the modeler can choose any one of them as the
28 2 Linear ProgrammingTable 2.5 Solutions after model modifications
Solution
# Pasta
Tomato
juice
Clam
chowder
Beef
chuck
Cheddar
cheese
Orange
juice Bologna
Potato
chips Milk Apples Cost
0 0 0 0 0 0 0.31 0 0.0057 9.37 2.82 $4.69
1 0 0 0 0 0.167 3.4 0 0.22 2 2.12 $5.46
2 0.81 0.46 0 0 0.15 3.53 0 0.22 2 1 $6.66
2.2 Applications of Linear Programming 29basis for the mathematical formulation. This will be further elaborated upon below.
Due to the many different types of allocation problems, we will present two such
scenarios below.
First consider an investment allocation problem. Problems of this nature were first
formulated by Markowitz in the early 1950s as nonlinear optimization problems.
Here, we will discuss a linear version of the problem. In this problem, the decision
maker has to decide how much of the scarce resource (money) to allocate to different
types of investments. As we will see below, this observation already indicates how to
define the variables.
In our numerical example, the investor has $300,000 that can be invested. In
addition to the money at hand, it is possible to borrow up to $100,000 at 12%
interest. This money can be used for leveraging (borrowing to invest). The investor
has narrowed down the choices to six alternatives, shown in Table 2.6. The table also
shows the expected annual interest or dividend for the investment alternatives, the
expected annual increase of the value of the investment, and an indication of the risk
of the investment (per dollar).
We will consider two versions of the problem: the first version attempts to
maximize the expected value of the assets at the end of the planning period
(1 year), while the second version minimizes the total risk of the investment.
First consider version 1. The value of the assets after 1 year equals today’s value
of the investment plus the expected interest or dividend plus the expected change in
value within a year minus the amount of money that was borrowed (principal and
interest). In addition to the restricted availability of money already mentioned above,
the decision maker faces the following constraints:
• The expected value of assets (exclusive interest) at the end of the planning period
should be at least 7% higher than at the beginning.
• Invest at least 50% of all the money actually invested in stocks and bonds
combined.
• Invest no more than 20% of total amount available (excluding the amount
borrowed) in real estate and silver combined, and.
• The average risk of the portfolio should not exceed 10.
Table 2.6 Types of investments and their features
Investment
type
Expected annual interest/
dividend
Expected annual increase
in value
Average risk per
dollar
Real estate 0% 18% 20
Silver 0% 10% 12
Savings
account
2% 0% 1
Blue chip
stocks
3% 6% 7
Bonds 4% 0% 3
Hi-tech
stocks
0% 20% 30
30 2 Linear ProgrammingIn order to formulate the appropriate model, we first must define the problem
variables. The scarce resource in this application is money, so that we define xj as the
dollar amount invested in the j-th alternative. Here, x1 will denote the money
invested in real estate, x2 the money invested in silver, and so forth.
We can then formulate the objective function. Each dollar invested in real estate
will produce no interest or dividend, but will gain an expected 18%, so that the
investment of 1x1 will have appreciated to 1.18x1. A dollar invested in silver and a
savings account will appreciate to $1.10 and $1.02, respectively. A $1 investment in
blue chip stocks is expected to be worth $1.06 after a year plus a dividend of 3¢,
making it worth $1.09. The remaining investments are dealt with in a similar fashion.
From our revenue, we must deduct the amount borrowed (x7) plus the interest to be
paid on the borrowed amount (0.12x7), making it 1.12x7.
Consider now the constraints. An obvious restriction is that the investor cannot
invest more money than is available. The amount invested is nothing but the sum of
all individual investments, i.e., x1 + x2 + x3 + x4 + x5 + x6, while the amount that is
available is the sum of the $300,000 at hand plus the amount that is borrowed, viz.,
x7. This is the budget constraint (2.1). Another straightforward restriction limits the
amount that can be borrowed to $100,000. This constraint is shown in (2.2).
The constraint that requires the invested money to show a growth of at least 7%
can now be formulated as follows. The actual value of the assets at the end of the
planning period uses the expected annual gains from Table 2.6, resulting in 1.18x1,
1.10x2, etc. The required increase of the invested money by at least 7% is the product
of 1.07 (the principal plus the required increase) and the invested amount, which is
the sum of the first six variables. This is shown in Constraint (2.3).
The constraints (2.4) and (2.5) model the requirements that at least 50% of the
money invested must be invested in stocks and bonds combined, and that no more
than 20% of the money available can be invested in real estate and silver combined.
Note the difference between the two constraints: while in (2.4), we are dealing with a
portion of the amount actually invested (the sum of the first six variables), constraint
(2.5) refers to the total amount available to the investor (exclusive the amount that
may be borrowed), which equals $300,000.
The average risk of the portfolio equals the total risk divided by the amount
invested, i.e., 20x1þ12x2þx3þ7x4þ3x5þ30x6
x1þx2þx3þx4þx5þx6 , which is not to exceed a value of 10.
Multiplying the inequality by the (nonzero) denominator results in relation (2.6).
The nonnegativity constraints (2.7) conclude the formulation. Version 1 of the
model can then be summarized as follows.
P : Max z ¼ 1:18x1 þ 1:10x2 þ 1:02x3 þ 1:09x4 þ 1:04x5 þ 1:20x6  1:12x7
s:t: x1 þ x2 þ x3 þ x4 þ x5 þ x6  300, 000 þ x7 ð2:1Þ
x7  100, 000 ð2:2Þ
2.2 Applications of Linear Programming 311:18x1 þ 1:10x2 þ 1:00x3 þ 1:06x4 þ 1:00x5 þ 1:20x6
 1:07ð Þ x1 þ x2 þ x3 þ x4 þ x5 þ x6
ð2:3Þ
x4 þ x5 þ x6  0:5ð Þð x1 þ x2 þ x3 þ x4 þ x5 þ x6 2:4Þ
x1 þ x2  0:2 300, 000 ðÞ ð2:5Þ
20x1 þ 12x2 þ x3 þ 7x4 þ 3x5 þ 30x6  10ð Þð x1 þ x2 þ x3 þ x4 þ x5 þ x6 2:6Þ
x1, x2, ... , x7  0: ð2:7Þ
Table 2.7 shows the optimal solution of this model with a 12% interest on
borrowed money (as formulated above), and a slight modification with 10% interest
on the amount we borrow.
The resulting profits of the two versions that maximize the expected return turn
out to be $332,973.9, and $333,408.7, respectively. The average risk is 10 in both
cases.
Version 2 of the investment model is very similar. It deletes the risk constraint
(2.6) from the formulation and uses its left-hand side (the actual risk of the portfolio)
in a minimization objective. Furthermore, we require an appreciation of at least 7%
on the available money (exclusive the borrowed amount), i.e., $321,000 at the end of
the planning period. A summary of the optimal solutions is shown in Table 2.7. At
12% interest, Version 2 generates a return (after interest payment) of $328,110. The
risk of this solution is only 2.
Note the jump that occurs in the amount of money borrowed: apparently, given an
interest of 10% on borrowed money, it is still worthwhile to borrow and invest, while
an increase to 12% is prohibitive, so that no money is borrowed, and no leveraging
occurs. Also note the very different solution provided by Version 2 of the model.
Another allocation problem is found in the allocation of manpower—the scarce
resource in this context—to tasks. In this specific application, we deal with
allocating police officers to districts. Clearly, not all districts in a city are created
Table 2.7 Optimal solutions to the different versions of the investment allocation model
Investment
type
Version 1 with 12% interest
on borrowed money
Version 1 with 10% interest
on borrowed money Version 2
Real estate 60,000 60,000 0
Silver 0 0 0
Savings
account
0 0 160,500
Blue chip
stocks
234,782.61 321,739.13 0
Bonds 0 0 160,500
Hi-tech
stocks
5,217.39 18,260.87 0
Amount
borrowed
0 100,000 21,000
32 2 Linear Programmingequal, so that the impact a single policeman provides to a district will be different for
different areas. In the numerical illustration below, a city has a total of 67 police
officers to be allocated among five districts A, B, ..., and E. Table 2.8 shows the
degree of protection offered to a district for each officer assigned to the district.
For example, if six police officers are assigned to district A, then the degree of
protection is 6(3) ¼ 18. Suppose that due to the intricacies of the job, it is not
possible to hire additional police officers in the short run. Actually, each police
officer who is not assigned to one of the districts can be hired out to a private security
company for a fee of $100 per officer and day. Other than the house taxes (a flat fee
for our purposes that does not have to be included in the problem), we assume that
hiring out police officers is the council’s only income. Council wants to set up a
linear programming problem that maximizes the department’s income (due to hiring
out police officers). The following restrictions have to be observed:
• Each district must have at least the minimum protection as specified in the table
above.
• The average protection is at least 50, and.
• We have to allocate at least 50% more officers to districts A, B, and C combined
than to districts D and E combined.
Formulating the problem, we have to define the variables first. In this context, the
scarce resource consists of police officers, so that we can define xj as the number of
police officers assigned to district j. For simplicity, we use xA, xB, xC, xD, and xE for
the five districts under consideration. For the same reason, we ignore the fact that the
number of police offers assigned to a district will have to be an integer number. The
objective function then maximizes the product of the revenue for a police officer
offers hired out for a day ($100) and the number of police offers hired out. The
number of police officers hired out for private security work equals the difference
between the number of police officers available (67) and the number of officers
assigned to the five districts (the sum of all five variables).
The first constraint simply requires that we cannot assign more police officers
than we have. Constraints (2.9)–(2.13) ensure that each district receives at least the
minimum protection required in Table 2.8. Constraint (2.14) requires that the
average protection in the five districts is at least 50, and constraint (2.15) ensures
that the first three districts have at least twice as many police officers allocated to
them as have the last two. As usual, the nonnegativity constraints complete the
formulation. The model can then be written as follows.
Table 2.8 Protection provided by police officers and smallest allowable protection
ABCDE
Additional marginal protection per officer 3 7 10 5 4
Lowest acceptable protection 40 50 70 60 40
2.2 Applications of Linear Programming 33P : Max z ¼ 100 67 ½   ð Þ xA þ xB þ xC þ xD þ xE
s:t: xA þ xB þ xC þ xD þ xE  67 ð2:8Þ
3xA  40 ð2:9Þ
7xB  50 ð2:10Þ
10xC  70 ð2:11Þ
5xD  60 ð2:12Þ
4xE  40 ð2:13Þ
ð Þ 3xA þ 7xB þ 10xC þ 5xD þ 4xE =5  50 ð2:14Þ
xA þ xB þ xC  1:5ð Þð xD þ xE 2:15Þ
xA, xB, xC, xD, xE  0: ð2:16Þ
Solving the problem reveals that the number of police officers allocated to the five
districts are 131=3 , 122=3 , 7, 12, and 10, respectively. This means that a total of
55 police officers are allocated, leaving 12 police officers to be hired out, so that city
council’s daily income from this allocation is $1200. Furthermore, this allocation
results in protection levels of 40, 882=3, 70, 60, and 40. In other words, all districts
except the second receive the minimal protection required, while district B receives
(882=3)/50  1.77 times the protection that is minimally needed.
2.2.4 Blending Problems
All blending problems have in common that they take a number of given ingredients
or raw materials and blend them in certain proportions to the final products. In other
words, in the process we are creating something new by mixing existing materials.
Typical examples of blending are coffees, teas, whiskeys, tobaccos, perfumes,
gasoline, and similar products. Clearly, there are some rules for the blending process.
For instance, in order to ensure a specific taste, it may be required that a blend
includes at least a certain proportion of a raw material.
The process is shown in Fig. 2.1. On the left, there are m buckets with given
quantities of raw materials, while on the right, there are n empty buckets that are to
be filled with known quantities of the blends. In the blending process, we take, one at
a time, a certain number of scoops from each of the raw materials and transfer them
into the buckets on the right. Once sufficient quantities have been transferred to the
“Product” buckets on the right, all that is left to do is stir, package, and sell.
This figure not only demonstrates the actual process, but it also allows us to see
how the variables in blending problems are to be defined. What we need to know in
34 2 Linear Programmingany specific blending problem is how many “scoops” of each raw material goes into
each of the “Product” buckets. In more general terms, we define xij as the quantity of
raw material i that goes into product j.
As a numerical example, suppose that we are to blend two table wines from the
Moselle region in Germany. The two blends are the Filzener Hexenhammer and the
Leiwener Hosenscheisser. Both products are blends of wines from three white
grapes, viz., Riesling, Müller-Thurgau, and Silvaner. The original wines are avail￾able in quantities of 10,000, 5000, and 6000 gallons at a cost of $8, $6, and $5 per
gallon. The estimated demands for the two blends are 7000 and 8000 gallons (our
customers will purchase these or any higher amounts from us) and the estimated
sales prices are $16 and $18 per gallon, respectively.
The rules that have to be followed when blending the two wines are summarized
in Table 2.9. The meaning of these figures is best explained by some of the numbers.
For instance, the interval [.45; .55] in the Riesling row and the Filzener
Hexenhammer column of the table indicates that at least 45% and at most 55% of
the Filzener Hexenhammer blend must be Riesling. Note that the Silvaner content of
Fig. 2.1 Blending ingredients
Table 2.9 Blending rules
for the wines
Basic wines
Blends
Filzener
Hexenhammer
Leiwener
Hosenscheisser
Riesling [.45; .55] [.20; .50]
Müller-Thurgau [.10; .15] [.10; .60]
Silvaner [.35; .35] [.30; .40]
2.2 Applications of Linear Programming 35the Hexenhammer must be exactly 35%. It is also noteworthy that while the ranges
for the Hexenhammer blend are quite tight (which usually indicates a well-controlled
and high-quality product), the ranges for the Hosenscheisser are very wide, clearly
indicating a cheap product that consists of, loosely speaking, more or less whatever
happens to be cheap and available.
As discussed above, we need six variables in this example. They are x11 (the
quantity of Riesling in the Hexenhammer), x12 (the quantity of Riesling in the
Hosenscheisser), x21 (the quantity of Müller-Thurgau in the Hexenhammer), and
x22, x31, and x32 which are defined analogously. Before formulating the objective, it
is beneficial to first determine the quantities of the basic wines that are used in the
blending process and the quantities of the blends that are made in the process. For the
time being, let us assume that there are no losses in the process (e.g., spillage or
thirsty employees).
Consider the basic wines and their uses first. Each of those wines is either put into
the Hexenhammer or the Hosenscheisser blend—that is all we can do with them in
this context. Thus, the quantity of Riesling that we use is x11 + x12, the quantity of
Müller-Thurgau used in the process is x21 + x22, and the quantity of Silvaner that is
used is x31 + x32. Similarly, we determine the quantities of the blends that are
produced. The quantity of each blend that is made in the process is nothing but the
sum of its ingredients. In our example, the quantity of Hexenhammer that we blend
equals x11 + x21 + x31, and the quantity of Hosenscheisser we make is x12 + x22 + x32.
The objective function is then to maximize profit, which, in turn equals the differ￾ence between revenues from the two blends and the costs of the three basic wines. It
is shown as relation (2.17) below.
As far as constraints go, we have three different types of constraints. First there
are the supply constraints that require us not to use more of the basic wines than we
can get, secondly there are the demand constraints that state that we have to make at
least as many gallons of the blends as our customers are estimated to demand, and
thirdly and finally, there are the blending constraints.
Since the quantities of the ingredients (the basic wines) and the products (the
blends) have already been determined above, the first two types of constraints are
easy to formulate. They are shown as constraint groups (2.18) and (2.19) in the
formulation below. The blending constraints (group 2.20) are formulated as follows.
Consider the content of Riesling in Hexenhammer. One of the blending constraints
will state that the quantity of Riesling in Hexenhammer (which is x11) must be at least
45% of the total quantity of Hexenhammer, which has already been determined as
x11 + x21 + x31. In other words, we can write x11  .45(x11 + x21 + x31). This is the
lower bound of this combination of basic wine and blend. The upper bound is
formulated in similar fashion as x11  .55(x11 + x21 + x31). This is repeated for all
combinations of basic wines and blends. The complete formulation is then
P : Max z ¼ ½  16ð Þþ x11 þ x21 þ x31 18ð Þ x12 þ x22 þ x32
½  8ð Þþ x11 þ x12 6ð Þþ x21 þ x22 5ð Þ x31 þ x32
ð2:17Þ
36 2 Linear Programmings:t: x11 þ x12  10, 000
x21 þ x22  5000
x31 þ x32  6000
ð2:18Þ
x11 þ x21 þ x31  7000
x12 þ x22 þ x32  8000 ð2:19Þ
x11  :45ð Þ x11 þ x21 þ x31
x21  :1ð Þ x11 þ x21 þ x31
x31  :35ð Þ x11 þ x21 þ x31
x12  :2ð Þ x12 þ x22 þ x32
x22  :1ð Þ x12 þ x22 þ x32
x32  :3ð Þ x12 þ x22 þ x32
x11  :55ð Þ x11 þ x21 þ x31
x21  :15ð Þ x11 þ x21 þ x31
x31  :35ð Þ x11 þ x21 þ x31
x12  :5ð Þ x12 þ x22 þ x32
x22  :6ð Þ x12 þ x22 þ x32
x32  :4ð Þ x12 þ x22 þ x32
ð2:20Þ
x11, x12, x21, x22, x31, x32  0:
The optimal solution of the problem uses 3850 gallons of Riesling, 700 gallons of
Müller-Thurgau, and 2450 gallons of Silvaner in the Hexenhammer, so that a total of
7000 gallons of that blend are made. Note that this is exactly the quantity that is
desired. Similarly, we use 3983.33 gallons of Riesling, 4300 gallons of Müller￾Thurgau, and 3550 gallons of Silvaner in the Hosenscheisser blend, making it a total
of 11,833.33 gallons of that blend. This is considerably more than the minimum of
8000 gallons that were required. Also note that in the process, we use a total of
7833.33 gallons of Riesling (with 2166.67 gallons left over), we use all of the 5000
gallons of Müller-Thurgau and all of the 6000 gallons of Silvaner that are available
to us. The overall profit is $202,333.33.
It is easy to add other relevant constraints. Suppose that we need to control the
alcoholic content of the blends as well. For that purpose, assume that the Riesling has
an alcohol content of 8%, Müller-Thurgau has 7.5%, and Silvaner has 6%, and it is
desired that the Hexenhammer has at least 7.3% alcohol content. Note that the
2.2 Applications of Linear Programming 37present solution contains .08(3850) + .075(700) + .06(2450) ¼ 507.5 gallons of
alcohol. Given that we are making 7000 gallons of the blend, this means that the
present alcohol content of Hexenhammer is 507.5/7000 ¼ 7.25%, which is not
sufficient. The constraint requiring at least 7.3% can be written as .08x11 + .075x21
+ .06x31  .073(x11 + x21 + x31). The new optimal solution does not change the
quantity or composition of Hosenscheisser, but dramatically changes the composi￾tion of Hexenhammer. It now contains 5916.67 gallons Riesling, 2366.67 gallons of
Müller-Thurgau, and 3350 gallons of Silvaner. The profit has decreased to
$198,466.7, roughly a 2% decrease.
2.2.5 Transportation and Assignment Problems
Transportation problems have been introduced into the discussion by Hitchcock in
1941, thus predating the advent of linear programming by half a dozen years. It must
be understood that there is not a single model that encompasses all, or even most,
transportation scenarios. What we call “the” transportation problem in this context is
a very simple prototype, which exhibits some basic structures that are typically
inherent in transportation problems. The structure has three essential components.
On the one hand, there are origins, where certain quantities of a single homogeneous
good are available (so that one unit of the good is exactly the same, regardless from
which origin it is taken). We can think of the origins as warehouses at which goods
are stored. For simplicity, we will use the terms warehouse and origin interchange￾ably. Given that there is a total of m origins, we assume that there is a known supply
of si at origin i.
The second set of components concerns the destinations. This is where given
quantities of the good are needed. We will refer to them as either destinations or
customers. We assume that there are n destinations, and suppose that there is a
known demand for our product at a magnitude of dj at destination j. So far, we have
existing goods on one side, and the need for goods (or, equivalently, existing
demand), on the other. The task is now to ship the existing goods from the origins
to the destinations, so as to respect the existing quantities at the origins, satisfy the
demand at the destinations, and organize the shipments as efficiently as possible. As
a matter of fact, we can visualize the structure of the problem by considering the
graph used in our discussion of blending problems (Fig. 2.1). Rather than input
factors on the left and blended products on the right, we have origins and
destinations, and instead of taking a number of ladles full of raw materials and put
them into a bucket for a blended product, we ship a number of units from a
warehouse to a customer. The visuals and the story are very different, but the
mathematical structure is the same.
When requiring the efficiency of the shipments, we will need a criterion by which
to measure efficiency. As usual, we will employ a monetary criterion. Here, an
obvious choice is the cost of transportation. In order to introduce cost components,
we assume that there is a cost of cij to ship a single unit of the good from origin i to
destination j. Fixed unit cost of transportation mean a linear transportation cost
38 2 Linear Programmingfunction, as one unit shipped from i to j costs cij, two units cost 2cij, and so forth. The
assumption of linearity of the cost function is crucial to our model. Assume that we
are shipping pallets of bottles from regional distribution centers to supermarkets.
Suppose now that the capacity of a truck is 40 pallets. Then the transportation cost
for one pallet is one trip, the costs for two pallets are one trip (except for the cost of
loading and unloading the same as the cost for a single pallet), the costs for three
pallets are still mainly the costs of a single trip, etc. In other words, it costs about the
same to ship one pallet or up to 40 pallets. Shipping pallet number 41, though,
requires a second trip, causing the costs to jump to the equivalent of two trips. The
result is a step function with jumps at the truck capacities. One way to “restore” the
linearity assumption in this model is to measure the number of units shipped in terms
of truckloads. Disregarding the differences in loading and unloading costs for
different numbers of pallets, the cost function now includes points on a linear
function (those with an integer number of truckloads). It is very important to note
that the transportation network in this basic problem allows only direct transporta￾tion from an origin to a destination. It is not permitted to first ship from one origin to
another to consolidate the load, to use round trips (similar to multi-stop shopping), or
similar non-direct routes.
Before formulating a small example, it is necessary to distinguish between
balanced and unbalanced transportation problems. A transportation problem is called
balanced, if the sum of all supplies equals the sum of all demands. In balanced
transportation problems, it will be possible to empty all the warehouses and satisfy
all demands of our customers. In unbalanced transportation problems, we either
have more units than customers want (the total supply exceeds the total demand), or
we need more units than we have (the total demand exceeds the total supply). In the
former case, we will be able to satisfy all demand, but some units will remain
unshipped at some of the warehouses, while in the latter case, we will empty all
the warehouses, but some demand will remain unsatisfied. We will discuss the
balanced case first and then offer modifications that deal with unbalanced problems.
Consider a transportation problem with two origins and three destinations. The
supplies at the origins are 30 and 20 units, respectively, while at the destinations,
there are demands of 15, 25, and 10, respectively. Clearly, the total supply and the
total demand both equal 50, making the problem balanced. The unit transportation
costs are shown in the matrix C below, where an element in row i and column
j shows the value cij. For instance, shipping one unit from origin 1 to destination
3 costs $4.
C ¼ 174
235  :
The three types of parameters—the supplies, the demands, and the unit transpor￾tation costs—completely describe the transportation problem. The model will mini￾mize the total transportation costs, while ensuring that supplies and demands are
respected. In order to formulate the problem, we define decision variables xij that
denote the quantity that is shipped (directly) from origin i to destination j. The
2.2 Applications of Linear Programming 39objective function is then composed as follows. First of all, we compute the total
transportation costs along each origin–destination connection. As an example, in our
numerical illustration, the link between origin 2 and destination 1 carries unit
transportation costs of $2 and the quantity we ship on that link is x21, making the
total transportation cost on that connection 2x21. Adding the expressions on all links
results in the overall total cost, which is then to be minimized in the objective
function of the model as shown below.
Next, consider the constraints of the problem. Generally speaking, there will be
two sets of constraints: the first set ensures (for balanced problems such as this
example) that the flow of goods out of each origin exactly equals the quantity that is
available at that origin. The second set of constraints requires that the flow of goods
that is received at a customer site equals the demand of that customer. As an
example, for the first set of constraints, consider the second origin. From this origin,
goods can be shipped (directly) to destination 1 (the quantity shipped on this link is
x21), to destination 2 (the quantity is x22), and to destination 3 (with a quantity of x23).
Consequently, the total quantity shipped out of origin 2 is x21 + x22 + x23, which is
supposed to equal 20 units, the quantity available at origin 2. The remaining supply
constraints are formulated similarly.
We are now able to formulate the demand constraints. As a numerical example,
consider customer (or destination) 3. The goods that the customer receives either
come from origin 1 (a quantity of x13) and from origin 2 (a quantity of x23). As a
result, the total quantity received at destination 3 is x13 + x23, which should equal the
demand of 10 at that point. Again, the other demand constraints are formulated in
similar fashion. The complete formulation is then as follows:
P : Min z ¼ 1x11 þ 7x12 þ 4x13 þ 2x21 þ 3x22 þ 5x23
s:t: x11 þ x12 þ x13 ¼ 30
x21 þ x22 þ x23 ¼ 20
x11 þ x21 ¼ 15
x12 þ x22 ¼ 25
x13 þ x23 ¼ 10
x11, x12, x13, x21, x22, x23  0:
The problem above is written in a way that makes the underlying structure clearly
visible. It is apparent that each variable appears exactly twice in the constraints, once
in a supply constraint and once in a demand constraint. The structure is very special
(the matrix of coefficients is called totally unimodular), which ensures that as long as
all supplies and demands are integer, one can prove that there will exist at least one
optimal solution to the problem that is also integer. The special structure of the
problem has given rise to specialized solution techniques, such as the MODI
(MOdified DIstribution) method. With increasing computational power, their impor￾tance has diminished, so that we have chosen not to include them in this book, but
instead describe them on the file-sharing site associated with this book. Another
feature of the problem should be mentioned, even though we will not exploit it here.
40 2 Linear ProgrammingGiven m origins and n destinations, the problem will have mn variables and (m + n)
constraints. However, one constraint is redundant (more specifically: linearly depen￾dent). This can be seen in the above formulation by adding all supply constraints and
then subtracting the first two demand constraints. The result will be the third demand
constraint. As a result, we will have only (m+n1) independent constraints to be
considered. This number features prominently in the aforementioned special solution
methods.
The optimal solution to our example can be shown in the following transportation
plan T. In row i and column j, it shows the optimal value of the variable xij.
T ¼ 15 5 10
0 20 0  :
In other words, we ship 15 units from origin 1 to destination 1, 5 units from origin
1 to destination 2, 10 units from origin 1 to destination 3, and all 20 units that are
available at origin 2 directly to destination 2. It is easy to ascertain (by multiplying
the elements of T with the corresponding elements of C and adding them up) that the
total transportation cost at optimality is z ¼ 150. It is also worth mentioning that
each nondegenerate solution to a transportation problem has exactly (m+n1)
variables at a strictly positive level. In case of degeneracy, there may be even
fewer positive variables.
As far as extensions go, we will first look into unbalanced problems. First of all,
given any unbalanced problem, we can no longer formulate the problem with all
equation constraints, as total supply and demand are no longer equal. In case of the
total supply exceeding the total demand, we will not distribute the entire supply to
our customers, so that some supply will be left over. This means that we can
formulate the demand constraints as equations (which pull the total quantity of
demand from the supplies into the network) as we have in a balanced problem,
while we write the supply constraints as less-or-equal-than constraints to ensure that
we do not ship more than we have out of the origins. That way, the customer demand
is satisfied everywhere, while some units are left over in one or more of the origins.
In the above example, let the supplies be 30 and 23 rather than 30 and 20. The
optimal solution will ship 27 units out of origin 1 and 23 out of origin 2, leaving three
units unassigned at origin 1.
The case in which the total supply falls short of the total demand is dealt with
similarly. Here, we will not be able to satisfy the entire demand, but in order to come
as close as possible of doing so, we will use our entire supply. This means that we
formulate the supply constraints as equations (thus pushing the entire supply into the
network), while the demand constraints will be written as less-than-or-equal
constraints. In that way, the total supply is shipped to customers, which will still
leave some unsatisfied demand at one or more customers. In the above original
example, suppose that the demands are now 15, 25, and 12, respectively, rather than
15, 25, and 10. The optimal solution ships 15 units to destination 1, 23 units to
destination 2, and 12 units to destination 3, so that customer 2 will be left with an
unsatisfied demand of 2 units. Other extensions of the basic model may include
2.2 Applications of Linear Programming 41penalties for unsatisfied customers (loss of goodwill) and units left over in a
warehouse (inventory costs).
Two interesting extensions are called reshipments and overshipments. Both
modifications can be interpreted as some type of sensitivity analysis, in that we
change some of the existing assumptions. In the case of reshipments, we use the
same transportation network, but allow transportation on routes that are not direct. In
other words, a reshipment would be given if we were not to send a unit from, say,
origin 1 to destination 1 directly, but ship it from origin 1 to destination 3 (or some
other destination) first, then back to, say, origin 2, and from there on to destination
1. Clearly, in order to actually use reshipments (or back-and-forth shipments), it
must be advantageous to do so. Consider again our example above. At optimum, we
ship five units from origin 1 to destination 2. To ship a single unit on that route, it
costs $7. Instead, we could ship up to five units from origin 1 to destination 1 for $1,
back to origin 2 for an additional $2, and from there to destination 2 for an additional
$3. Hence, it costs $6 to ship a single unit on this somewhat circuitous route, $1 less
than on the direct connection. Reshipping five units this way will save 5(1) ¼ $5 for
total transportation costs of $145. While reshipping may use routes that go back-and￾forth multiple times, it is unlikely that such routes will exist in practice. The
mathematical formulations of reshipments use absolute values of variables, as the
value of a variable such as xij ¼ 5 indicates that five units are shipped back from
destination j to origin i. This is not really problematic, but it makes the formulation
somewhat more unwieldy.
Overshipments are another way of improving on the optimal solution to the basic
problem by changing the assumptions somewhat. The idea is to allow additional
flow through the transportation network, which, paradoxically, may actually reduce
costs. Again, in our example consider the possibility to add one unit of supply to
origin 2 (for a total of 21), and one unit of demand to destination 1 (for a total of 16).
The optimal solution to that problem will move 51, rather than 50, units through the
transportation network at a cost of 147, a decrease of $3 from the original solution.
Such a decrease is possible in this example, because we now ship one additional unit
on the link from origin 1 to destination 1 (costing an additional $1), and on the
connection from origin 2 to destination 2 (costing an extra $3), but we can now
transport one less unit on the expensive link from origin 1 to destination 2 (which
saves $7). This explains the net savings of +1 + 3  7 ¼ $3. While reshipments
were easy to implement (just modify the plan of what is shipped where),
overshipments are considerably more difficult to apply. In order to benefit from
overshipments, we need additional units at the right origin, and we have to convince
at least one customer to accept more units than originally demanded, which is
typically done by transferring some of the savings to the customer willing to take
more than originally desired.
Other extensions of the basic problem have been discussed as well. One such
extension includes capacities applied to the transportation links. Another, quite
natural, modification includes not only direct shipments as used here, but allows
transshipment points, at which the goods may be unloaded, temporarily stored, and
reloaded onto other trucks. In some practical applications, such transshipment points
42 2 Linear Programmingdo not just allow the consolidation of the loads, but also permit changing transporta￾tion modes, e.g., from truck to rail. Capacity constraints on the transportation points
are a further natural feature to be included in a model. While some of these
extensions may be incorporated in network models (see Chap. 6 of this volume),
planners will generally resort to standard linear programming formulations to
include all the desired features in the model.
A variety of other applications of “transportation problems” exist, some having
absolutely nothing to do with shipping units from one place to another. One such
example are the dynamic production–inventory models in Sect. 2.2.6. Here, the
“origins” represent the periods of productions, while the “destinations” are the
periods of consumption (or demand). A link from origin i to destination j exists, if
i  j. The constraints then require that the outflows of the origins do not exceed the
production capacities, while the inflows of the destinations must be at least as large
as the known demand. Other applications assign groups of workers to shifts, or
differently equipped military units to potential targets. Although the structure of the
mathematical models may be identical, they still represent completely different
physical situations.
While transportation problems have a specialized structure, assignment problems
are even more specialized. Consider a set of n employees that are to be assigned to
n tasks. We can use no more than 100% of an employee time in the allocation, and to
each task, we must assign 100% of one or more employees’ time. Each allocation
bears a certain cost. As an example, consider typists who, quite naturally, have
different abilities. Suppose that one of the tasks involves technical typing. In order to
perform the task, some typists may already know how to do it and can perform the
type of task quite efficiently (meaning that there will be low costs of assigning this
employee to the task), while other typists may have to be trained, necessitating
higher assignment costs. The problem is then to assign employees’ time to tasks, so
as to minimize the overall assignment costs.
In order to formulate the problem, we again first define the appropriate variables.
Here, we define xij as the percentage of worker i’s time that is assigned to task j. In
order to explain the way the model is formulated, suppose that we have three
employees and three tasks. The assignment/training costs are shown in the cost
matrix
C ¼
431
853
262
2
6
4
3
7
5:
If, for instance, we will use 20% of employee 1’s time for task 2, then the costs are
.2(3) ¼ .6. The objective function will then minimize the sum of all assignment
costs.
As far as the constraints of the model go, we have one set of constraints that
specify that the sum of proportions of an employee’s time must add up to 100%.
Similarly, the sum of proportions of employees’ time devoted to any one task must
2.2 Applications of Linear Programming 43also add up to 100%. Given this structure, the problem can then be formulated as
follows.
P : Min z ¼ 4x11 þ 3x12 þ 1x13 þ 8x21 þ 5x22 þ 3x23 þ 2x31 þ 6x32 þ 2x33
s:t: x11 þ x12 þ x13 ¼ 1
x21 þ x22 þ x23 ¼ 1
x31 þ x32 þ x33 ¼ 1
x11 þ x21 þ x31 ¼ 1
x12 þ x22 þ x32 ¼ 1
x13 þ x23 þ x33 ¼ 1
x11, x12, x13, x21, x22, x23, x31, x32, x33  0:
It becomes apparent that the assignment is a very close relative of the transporta￾tion problem discussed above. More specifically, since the constraints and the
objective are exactly the same, we can view assignment problems as transportation
problems with all supplies and demands equal to one. Given that, there will be at
least one optimal solution to the problem that has all variables integer. It follows that
at least one optimal solution has all variables either equal to zero or equal to one. In
many publications, the variables are assumed to be zero or one from the start, but this
feature is really not an assumption of the general problem, but a consequence of the
structure of the problem. It does, however, provide for an easier statement of the
problem: the variables indicate whether or not an employee is assigned to a task
(they equal one if he is and zero if not), and the constraints state that each employee
is assigned to exactly one task, and each task is performed by exactly one employee.
Similar to the transportation problem, specialized algorithms were developed for
the assignment problem. Of particular mention is the “Hungarian Method,” a
technique based upon a combinatorial theorem by the Hungarian mathematician
Jenö Egerváry. Again, due to the generally available increased computational power,
the problem can easily be solved as a general linear programming problem, and the
importance of specialized methods has diminished, and we will not discuss it here,
but relegate it to the website associated with this book.
Assignment problems have a number of applications, some not obviously related
to assignments. The earliest story reported by G.B. Dantzig in his 1963 book on
linear programming refers to it as the “marriage problem.” The story is that a father
has a number of daughters whom he wants to marry off. (It would work the same
way with sons, in case this were desired.) There are a number of prospects for the
matching, but each particular match requires a certain amount of dowry based on the
(in-) compatibility of the couple. The thrifty father’s overall objective is to minimize
the total amount of dowry he will have to pay.
The marriage story in its original form does not appear to be among the prime
applications of assignment problems, though. Instead, decision makers may attempt
to match entities such as sports teams with the objective of maximizing the
audience’s appeal (and with it revenue, as appealing games—those with
44 2 Linear Programminglong-standing rivalries or those among teams with close standings—tend to attract
larger crowds.) Assignment problems are also very important in the solution of
problems such as the traveling salesman problem, see Chap. 6 of this volume.
There are some well-known and well-documented extensions to assignment
problems, such as generalized assignment problems and quadratic assignment
problems. Both types of extensions are not only very difficult from a computational
point of view, but also beyond the scope of this volume.
2.2.6 Dynamic Production-Inventory Models
This section describes models, in which decision makers do not only have to answer
the “how many” question as we have seen in many of the previous applications, but
they also require an answer to the question “when” to produce. In the simplest case,
assume we only consider a single product. Furthermore, suppose that the time frame
of interest has been subdivided into small time units, in which production occurs.
Throughout this section, we will refer to these units as “months.” Within each
month, production occurs, and customers take out products based on their demand.
Based on the production capacities, it may now not be possible to satisfy the
demand in each month by the production in the same month. In order to avoid
undersupplying our customers, we can produce more than the demand indicates
during the earlier months of the planning period and keep the surplus in stock. This
will, of course, cause inventory holding costs to be incurred. Given that the produc￾tion costs may vary between the months, it may actually be preferable to manufac￾ture goods earlier in the planning period rather than later, but the decision will
depend on the relation between the production costs and the inventory holding costs.
We will leave these decisions to the optimizer and our model.
Before presenting a numerical example, it is important to discuss the exact
sequence of events within each month. At the beginning of each month, we take
stock. Since nothing has happened to the inventory between this point in time and
the end of the previous month (we assume that no theft, spoilage, spillage, etc.
occurs; such events could be included, if desired), the inventory level at the begin￾ning of the month will denote the number of units carried over from the previous
month, which, in turn, will determine the inventory holding costs. Then production
occurs. After the desired production quantity is made, customers take products out of
our warehouse according to the estimated demand. Whatever is left after that will be
carried over to the next month, and the process begins anew.
As a numerical illustration, consider the following scenario. The planning period
ranges from the beginning of January of some year and ends at the end of April. The
estimated demand, production capacity, and unit production costs are shown in
Table 2.10.
In addition, it costs 5¢ to carry over one unit from the end of January to the
beginning of February, 15¢ to carry over one unit from the end of February to the
beginning of March, and another 15¢ to hold one unit in stock between the end of
March and the beginning of April. The decision maker has an opening inventory of
2.2 Applications of Linear Programming 4520 units in the beginning of the planning period and desires to have nothing left at the
end of the planning period.
In order to formulate the model, we quite naturally need two types of variables,
one for production and the other for inventory. Denote the production variables by
x1, x2, x3, and x4, which are defined as the quantities to be manufactured in months
1, 2, 3, and 4, respectively. Similarly, we define the parameters d1, d2, d3, and d4 as
the demand in periods 1, 2, 3, and 4, respectively. Before defining the inventory
variables, we need to decide at which point to measure the inventory level. Given our
problem description above, we may decide to count inventory at the beginning of
each period. Alternatively, it is possible to determine the inventory level at the end of
a period, which we leave as exercise in Problem 3 at the end of this section. Here, we
denote by I1, I2, I3, I4, and I5 the variables that denote the inventory levels at the
beginning of the periods 1–5. Note that the inventory levels I1 and I5 are not
variables, but parameters whose numbers we know: as outlined above, the opening
inventory is I1 ¼ 20, and the closing inventory must be I5 ¼ 0.
The objective function is then a simple cost minimization function that consists of
two main components, the production costs and the inventory costs. As far as
constraints go, there are two types. First, there are the simple production capacity
constraints, which specify that in no period can we produce more than our capacity
allows. Secondly, there are the inventory balancing constraints. They state that the
inventory level at the beginning of period t equals the inventory level at the
beginning of the previous period t1 plus our production in the previous period
minus the demand in the previous period. Formally, we can write It ¼ It1 + xt1
 dt1 for t ¼ 2–5. These constraints are the same as the usual balancing constraints
in accounting, which state that what you have in your account today equals what you
had yesterday plus the deposits yesterday minus yesterday’s withdrawals. Note that
the inventory balancing constraints implicitly include the requirement that the
demand must be satisfied, so that separate constraints to that effect are not needed.
Our model can then be formulated as follows.
Table 2.10 Parameters for the dynamic production—inventory model
Month 1
(January)
Month 2
(February)
Month 3
(March)
Month 4
(April)
Estimated demand 80 70 130 150
Production capacity 120 140 150 140
Unit production cost $1.00 $1.10 $1.20 $1.25
46 2 Linear ProgrammingP : Min z ¼ 1x1 þ 1:1x2 þ 1:2x3 þ 1:25x4 þ :05I2 þ :15I3 þ :15I4
s:t:x1  120
x2  140
x3  150
x4  140
I2 ¼ I1 þ x1  80 or, as ð Þ I1 ¼ 20, x1  I2 ¼ 60
I3 þ I2 þ x2 ¼ 70
I4 þ I3 þ x2 ¼ 130
I5 þ I4 þ x4 ¼ 150 or, as ð Þ I5 ¼ 0, x4 þ I4 ¼ 150
x1, x2, x3, x4, I2, I3, I4  0:
Notice that the model does not include constraints It + xt  dt to ensure that
sufficient products are available to satisfy the demand. The reason is that the
nonnegativity constraints of the inventory variables guarantees that sufficient
products are available. This is no longer the case if the demand occurs in a period
before the production; if so, the additional constraints are needed.
The optimal production schedule has us manufacture 120, 10, 140 and 140 units
of the product in the 4 months, and the inventories carried over between months
1 and 2, 2 and 3, and 3 and 4 are 60, 0, and 10 units, respectively. The sum of the
production and inventory costs is $478.50.
The solution makes intuitive sense, as the low production level in February is a
result of the lower production costs in January and the low inventory costs between
January and February. On the other hand, the inventory carrying costs are significant
after February, so that inventories only occur between March and April, and these
are necessary as the April demand exceeds the production capacity in that month.
A potential extension of the model may consider warehouse capacities. In other
words, we may impose limits on the number of units we can keep in stock. Such
constraints are easily incorporated in this formulation. If in our numerical example
the largest possible inventory levels between months 1 and 2, months 2 and 3, and
months 3 and 4 are 40, 50, and 50 units, respectively, we add the constraints
I2  40
I3  50
I4  50.
With these additional constraints, the production levels in the four periods are
revised to 100, 30, 140, and 140 units, respectively, so that the inventory levels
between the periods are 40, 0 and 10 units, respectively. The total costs for this
system then (marginally) increase to $479.50. Note that this formulation assumes
that warehouse capacity is only used “overnight.” If production takes place in the
morning and sales in the afternoon, then we need to store not only It units in period t,
but It + xt for all periods under consideration.
2.2 Applications of Linear Programming 47This problem can also be formulated in an alternative fashion. This may not be the
best way to formulate the problem, but it nicely demonstrates that the same scenario
may be formulated in very different ways. Rather than using separate variables for
production and inventory, we can define double-subscripted variables xij that indi￾cate how many units of the product were manufactured in month i for use in month j.
Such a formulation will require some additional preprocessing. For instance, in order
to determine the objective function coefficient for the variable x14 in the numerical
example in this section, we need to add the production costs in month 1 (when the
product is made) and the inventory holding costs from month 1 to 2, those from
month 2 to 3, and those from month 3 to 4 for a total of $1 ¼ .05 + .15 + .15 ¼ $1.35.
The other coefficients in the objective function are determined similarly.
In addition, there will be two sets of constraints. The first are again the constraints
that require the production capacities to be respected. For instance, the total produc￾tion in Month 1 will be x11 + x12 + x13 + x14, the production in Month 2 will be x22
+ x23 + x24, and similar for the remaining 2 months. The second set of constraints that
are needed are then the demand constraints. The number of units available in, say,
Month 3 is x13 + x23 + x33 and this number must be at least as large as the demand in
that month. The problem can then be formulated as follows.
P : Min z ¼ 1x11 þ 1:05x12 þ 1:2x13 þ 1:35x14 þ 1:1x22 þ 1:25x23 þ 1:4x24
þ1:2x33 þ 1:35x34 þ 1:25x44
s:t: x11 þ x12 þ x13 þ x14  120
x22 þ x23 þ x24  140
x33 þ x34  150
x44  140
x11  60
January’
s demand is reduced by the available opening inventory  
x12 þ x22  70
x13 þ x23 þ x33  130
x14 þ x24 þ x34 þ x44  150
x11, x12, x13, x14, x22, x23, x24, x33, x34, x44  0:
Table 2.11 Optimal solu￾tion of the production￾inventory problem
Month 1 Month 2 Month 3 Month 4
Month 1 60 60 0 0
Month 2 – 10 0 0
Month 3 – – 130 10
Month 4 –– – 140
48 2 Linear ProgrammingIt is not difficult to see that the structure of this formulation is again the same as that
of the transportation problem. However, it must be ensured that the “consumption
month,” i.e., the month in which the product is sold, is not before the “production
month,” i.e., the month, in which the product is made. In other words, all variables, in
which the first subscript is smaller than the second subscript, must be forced to zero.
The optimal solution can best be summarized in a table such as that shown in
Table 2.11.
The cost at optimum is $478.50, obviously the same as in the other formulation.
The actual production levels in the 4 months can be determined by adding the values
of the variables in the rows, while the sums in the columns result in the months’
demand. It is also a good idea to plot the inventory changes on a timeline that lists the
opening inventory of each month, adds the production within the month, and then
subtracts the demand later that month. This is shown in Fig. 2.2.
Hence, the inventory levels at the beginning of the periods 2, 3, and 4 are
60, 0, and 10 units, respectively; again, reflecting the same result obtained earlier.
Incorporating limits on the warehouse capacity is also easy in this formulation.
The number of units put in stock between January and February is x12 + x13 + x14, the
level of stock between February and March is x23 + x24, and the level between March
and April is x34. All that needs to be done is to require these expressions not to
exceed 40, 50, and 50, respectively, and we will again obtain the same result
computed earlier for the formulation with explicit inventory variables.
Which of the formulations is used depends on the preferences of the user. The
former model with the explicit inventory variables has the advantage of having 2n
variables, given again n months within the planning period, while the latter model with
the double-subscripted variables requires ½n2 variables. However, since the value of
n is typically quite small and modern linear programming solvers can easily deal with
formulations that have hundreds of thousands of variables, this should not be a concern.
2.2.7 Employee Scheduling
In contrast to the applications discussed up to this point, the models described in this,
and the next section require that all variables are integer. However, since this feature
is just a simple addition to the formulation, we have chosen to discuss these models
in this chapter.
The employee scheduling model in this section is, in some sense, also an alloca￾tion problem. However, it has its own character, so that it justifies its own section.
Consider a recurring situation in which employees have to be assigned to shifts. The
number of employees required to be on the job varies throughout the day during a
Fig. 2.2 Production and inventory levels
2.2 Applications of Linear Programming 49variety of time slots. For instance, a bus route will require significant service during
the early morning and afternoon rush hours, while there will not be much service
during lunch hour or late at night. Similar requirements exist for nurses, pilots,
cashiers in grocery stores, and similar scenarios.
The difficulty with this problem is that we are typically not able to hire casual
labor whenever needed, but we will have to use permanent employees. Conse￾quently, the objective of the problem is to use the smallest number of employees
(a proxy for costs) and still be able to staff the position(s) throughout the day.
In our numerical example, assume that a regular shift is 8 hrs and assume that
there are 4-hr time segments during which personnel requirements have been
observed. The personnel requirements during the 4-hr time slots are shown in
Table 2.12 using a 24-hr clock.
Assume that shift work can start every 4 hrs at 6 AM, 10 AM, and so forth, and that
an employee cannot work on more than one shift. Our decision is then how many
employees to hire, who should start their respective shifts at each of these points in
time. This means that we can define variables x06, x10, x14, x18, x22 and x02 as the
number of employees who start their shift at 6 AM, 10 AM, 2 PM, and so forth. The total
number of employees required is then the sum of all of these variables. As far as the
constraints go, we have to require that a sufficient number of employees is present
during each time slot. Consider, for instance, the time slot between 1400 and
1800 hrs, during which at least 19 employees are needed. The employees working
during this time slot are those whose shift starts at 1000 hrs plus those who start
working at 1400 hrs. This means that during this time slot x10 + x14 employees will
be working, a number that must be at least 19. Similar constraints have to be
formulated for all six-time slots. The formulation can then be written as follows,
where again, we ignore the necessary integrality requirements for reasons of
simplicity.
Table 2.12 Personnel requirements during 4-hr time slots
Shift
0600–
1000
1000–
1400
1400–
1800
1800–
2200
2200–
0200
0200–
0600
Required number of
employees
17 9 19 12 5 8
50 2 Linear ProgrammingP : Min z ¼ x06 þ x10 þ x14 þ x18 þ x22 þ x02
s:t: x06 þ x02  17
x06 þ x10  9
x10 þ x14  19
x14 þ x18  12
x18 þ x22  5
x22 þ x02  8
x06, x10, x14, x18, x22, x02  0:
Problems of this type typically have multiple optimal solutions. The problem as
formulated has an optimal solution that requires a total of 41 employees. The starting
times of their shifts are shown in Table 2.13.
Note that this solution has the exact number of required employees during all time
slots, except for the time 1000–1400, where only 9 employees are needed, while
21 employees are available. In other words, there are 12 employees idle between
10 AM and 2 PM.
As an extension of the above model, assume that it is now possible to start the
employees’ shifts each 2 hrs rather than each 4 hrs. Similarly, the time requirements
are known for 2-hr rather than 4-hr segments throughout the day. For instance, the
17 employees that were needed between 6 AM and 10 AM in the above problem will
be required only between 6 AM and 8 AM, while between 8 AM and 10 AM only
11 employees are needed. The personnel requirements during the 2-hr time slots are
shown in Table 2.14. Note that the larger requirement of each two adjacent time slots
that correspond to a 4-hr time slot in the above example equals the requirement of
that 4-hrs slot. In that sense, we are using the same example, just a finer grid.
The problem can then be formulated as follows.
Table 2.13 Starting times of employees in optimal solution for 4-hr time slots
Start of shift 0600 1000 1400 1800 2200 0200
Number of employees 14 7 12 0 5 3
Table 2.14 Personnel requirements during 2-hr time slots
Shift
0600–
0800
0800–
1000
1000–
1200
1200–
1400
1400–
1600
1600–
1800
Required number of
employees
17 11 9 7 13 19
Shift
1800–
2000
2000–
2200
2200–
2400
2400–
0200
0200–
0400
0400–
0600
Required number of
employees
12 8 5 3 3 8
2.2 Applications of Linear Programming 51P : Min z ¼ x06 þ x08 þ x10 þ x12 þ x14 þ x16 þ x18 þ x20 þ x22 þ x24 þ x02 þ x04
s:t: x06 x24 þ x02 þ x04  17
x06 þ x08 x02 þ x04  11
x06 þ x08 þ x10 x04  9
x06 þ x08 þ x10 þ x12  7
x08 þ x10 þ x12 þ x14  13
x10 þ x12 þ x14 þ x16  19
x12 þ x14 þ x16 þ x18  12
x14 þ x16 þ x18 þ x20  8
x16 þ x18 þ x20 þ x22  5
x18 þ x20 þ x22 þ x24  3
x20 þ x22 þ x24 þ x02  3
x22 þ x24 þ x02 þ x04  8
x06, x08, x10, x12, x14, x16, x18, x20, x22, x24, x02, x04  0:
The optimal solution of this problem requires now only 36 employees, and the
starting times are shown in Table 2.15.
Particularly noteworthy are the more than 12% savings in the number of
employees that must be hired. In general, it is not surprising that the finer grid
used here provides a solution that is at least as good as that with 4-hrs time slots. The
reason is that the previous solution can still be implemented, and it would still
provide a feasible solution. However, with the additional possible starting times,
there are additional possibilities that may—and in this case do—allow us to find a
better solution.
2.2.8 Cutting Stock Problems
In this model, the variables quite naturally must assume integer values. Cutting stock
problems (or, alternatively, stock cutting or trim loss problems) are among the early
applications of integer linear programming. The first studies concerned paper rolls,
whose width is fixed, but which can be cut to desired lengths. The decision maker
then has a number of larger rolls of paper of given length, which he has to cut down
to smaller rolls that are in demand. This is what is called a one-dimensional problem,
as only the length of the rolls is cut.
Table 2.15 Starting times of employees in optimal solution for 2-hr time slots
Start of shift 06 08 10 12 14 16 18 20 22 24 02 04
Number of
employees
0 0 7 4 3 5 0 0 0 3 0 14
52 2 Linear ProgrammingIn order to explain the formulation, suppose that a home improvement store
carries wooden rods in a standard profile and width. They presently have two
lengths, 12 ft and 10 ft. In particular, they have twenty 12 ft rods and twenty-five
10 ft rods in their warehouse. Management anticipates a need for sixty 8 ft rods, forty
5 ft rods, and seventy-five 3 ft rods. In order to obtain the desired lengths, we can
either cut existing rods at a cost of 50¢ per cut, or purchase new rods at a cost of $2,
$1.50, and $1.10 for the 8 ft, 5 ft, and 3 ft rods, respectively.
There are two common types of objectives. Management could either attempt to
minimize the waste produced in the process or could minimize the costs incurred in
the process. Minimizing waste is a popular option, yet it is nontrivial from a
conceptual point of view. A small piece, say a 2 ft rod, cannot be used and is
considered a complete waste. A larger piece, say a 4 ft rod, however, while twice the
size of a 2 ft rod, but may be no waste at all, as it might be used to satisfy some future
demand for 4 ft sizes, even if it is not used in this planning period. Since minimizing
waste is really just a proxy for cost, we will simply minimize the total cost incurred
in the process.
In order to formulate the problem, it is mandatory that we first devise a cutting
plan. A cutting plan will include all meaningful cutting patterns. Patterns that are
undesirable, either because they produce too much waste, are too difficult to cut, or
for some other reason, are simply not included in the cutting plan.
The cutting plan for this example is shown in Fig. 2.3, where the meaningful
cutting patters are numbered 1–8 from top to bottom.
We can now define the decision variable y1 as the number of times that pattern 1 is
cut, and similarly for the remaining seven patterns. This takes care of the cutting we
have to do. In addition, we also require variables v1, v2, and v3 that determine the
12 ft
10 ft
8 ft 3 ft
3 ft
3 ft 3 ft
3 ft
3 ft 3 ft 3 ft
3 ft 3 ft
8 ft
3 ft
5 ft
5 ft
5 ft 5 ft
5 ft
5 ft
Fig. 2.3 Cutting patterns
2.2 Applications of Linear Programming 53number of 8 ft, 5 ft, and 3 ft rods that we purchase in addition to cutting longer rods
to the required sizes. Consequently, the objective function consists of two major
components, the cost of cutting and the cost of purchasing. Consider first the cutting
costs. Pattern 1 requires two cuts, so that each time pattern 1 is cut, it will cost $1.
Given that pattern 1 is cut y1 times, the cost contribution of the first pattern is 1y1.
Similarly, pattern 8 requires three cuts or $1.50 each time it is cut. Since pattern 8 is
cut y8 times, its cost contribution is 1.5y8. Adding the purchasing costs for the rods
that are newly bought, the objective function can then be formulated as
Min z ¼ 1y1 þ 1y2 þ 1:5y3 þ 1:5y4 þ 0:5y5 þ 0:5y6 þ 1y7 þ 1:5y8 ð Þ
þð Þ 2v1 þ 1:5v2 þ 1:1v3 ,
with the terms in the first bracket being the cutting costs, while the terms in the
second bracket are the purchasing costs. As far as the constraints are concerned,
there will be two types, viz., supply and demand constraints.
Consider first the supply constraints. In words, they state that the number of
patterns cut from an existing length cannot exceed the number of rods that are
available. Constraints of this type have to be formulated for each existing length.
Consider first the 12 ft length. It is used in patterns 1, 2, 3, and 4, which, as we
already know, are cut y1, y2, y3, and y4 times, respectively. Given that we have
twenty 12 ft rods available, we can formulate the supply constraint for the 12 ft
length as
y1 þ y2 þ y3 þ y4  20: ð2:21Þ
Similarly, the supply constraint for the 10 ft rods is
y5 þ y6 þ y7 þ y8  25: ð2:22Þ
The demand constraints are somewhat more difficult to formulate. As an exam￾ple, consider the first required length of 8 ft. It is produced by patterns 1 and 5. Each
time we cut pattern 1, we generate one 8 ft rod. Since we cut this pattern y1 times, the
number of 8 ft rods produced by cutting pattern 1 is 1y1. Similarly, since each time
pattern 5 is cut, we generate a single 8 ft rod, we make a total of 1y5 8 ft rods by
cutting pattern 5. Since the only other way to obtain 8 ft rods is to purchase them
(and we already have decided to buy v1 of them), the total number of 8 ft rods that we
will have is 1y1 + 1y5 + v1, a number that must be large enough to satisfy the demand
of 60 units. The demand constraint for the 8 ft rods can thus be written as
y1 þ y5 þ v1  60: ð2:23Þ
As far as 5 ft rods are concerned, the cutting plan reveals that they are generated
by patterns 2, 3, 6 and 7. Since each time pattern 1 is cut, we produce two 5 ft rods,
and as pattern 1 is cut y1 times, we will produce a total of 2y1 5 ft rods by cutting
pattern 1. Applying similar arguments for the other three patterns that generate 5 ft
rods, the constraint for these rods is
54 2 Linear Programming2y2 þ 1y3 þ 2y6 þ 1y7 þ v2  40: ð2:24Þ
Finally, consider the 3 ft rods. They are generated by patterns 1, 3, 4, 7 and
8. Each time one of these patterns is cut, we generate 1, 2, 4, 1, and 3 of the required
3 ft rods. As a result, we can write the constraint for the 3 ft rods as
1y1 þ 2y3 þ 4y4 þ 1y7 þ 3y8 þ v3  75: ð2:25Þ
The cutting stock problem can then be written as
Min z ¼ 1y1 þ 1y2 þ 1:5y3 þ 1:5y4 þ 0:5y5 þ 0:5y6 þ 1y7 þ 1:5y8
þ2v1 þ 1:5v2 þ 1:1v3
s:t: constraints 2ð Þ :21 ð Þ 2:25
y1, y2, ... , y8; v1, v2, v3  0 and integer:
Solving the problem results in y1 ¼ 2, y2 ¼ 0, y3 ¼ 0, y4 ¼ 18, y5 ¼ 5, y6 ¼ 20,
y7 ¼ 0, and y8 ¼ 0, as well as v1 ¼ 53, v2 ¼ 0, and v3 ¼ 1. This leaves none of the
existing rods left over, and the demand is exactly satisfied.
Solving the same problems with demands of 20, 15, and 18 for the 8 ft, 5 ft, and
3 ft rods results in y1 ¼ 6, y2 ¼ 0, y3 ¼ 0, y4 ¼ 3, y5 ¼ 14, y6 ¼ 8, y7 ¼ 0, and
y8 ¼ 8, as well as v1 ¼ 0, v2 ¼ 0, and v3 ¼ 0. In this case, nothing is purchased, we
have eleven 12 ft rods and three of the existing 10 ft rods left over, and the demand is
exactly satisfied for the 8 ft and 3 ft rods, while one 5 ft rod is cut but not used.
As expected, things get more complicated when cutting is possible in two
dimensions. However, it is not the formulation that becomes more difficult, but the
cutting plan that may now include many patterns. Just imagine cutting an irregular
piece of fabric that may be shifted and tilted by infinitesimally small amounts to any
of its sides, resulting in infinitely many patterns. To simplify matters, assume that the
patterns are regular. As a numerical example, consider a single board of plywood of
size 5 ft by 8 ft, and assume that we need two types of boards, type T1 is of size
2 ft  4 ft, and type T2, which measures 3 ft square. Two of the many possible
patterns are then shown in Fig. 2.4, where the shaded parts indicate waste.
Fig. 2.4 Cutting patterns for boards of plywood
2.2 Applications of Linear Programming 55When evaluating these patterns, we could resort to the amount of waste that is
generated. The first pattern uses 34 sq. ft out of the given 40 sq. ft for a usage rate of
85%. Pattern 2, in contrast, uses only 32 sq. ft for a usage rate of 80%, so it appears
that pattern 1 is more efficient. This is, however, not necessarily the case. Pattern 2 is
easy to cut: Adjust the saw once to a 1 ft width, cut off the shaded part at the bottom,
readjust the machine to a 2 ft cutting width, and continue to cut the four desired T1
pieces. Things are much more complicated when cutting pattern 1. First of all, we
should point out that only so-called guillotine cuts are feasible in many applications.
These are cuts that cut the existing piece all the way through. This restriction is
important, as non-guillotine cuts will result in many operator errors unless we use
automated methods such as laser cutting. Using only guillotine cuts, it appears best
to cut pattern 1 by first adjusting the machine to a 2 ft width, cut the T1 piece on the
right with the piece of waste at the bottom, then turn the remaining board and cut the
T1 piece at the bottom (this way, we do not have to readjust the machine all the time),
then readjust the machine to a 3 ft cutting width and cut the top left piece in the
middle, generating the two T2 pieces. And then we have to readjust the machine
again to cut off the waste at the bottom of the two T1 pieces. It should have become
clear that while this pattern uses more of the existing board, it is much more
complicated and thus costly to cut. A simple example, which demonstrates that
not all patterns can be cut by guillotine cuts is shown in the middle figure in Fig. 2.5
in Problem 11 at the end of this chapter.
Exercises
Problem 1 (production planning): Solve a variant of the standard production
planning problem. Three products P1, P2, and P3 are manufactured on two machines
M1 and M2. Each of the products must be processed on both machines in arbitrary
order. The unit profits of the products are $18, $12, and $6, respectively, and the
machine capacities are 24 and 16 hrs per planning period. Table 2.16 indicates how
many units of the products can be made each hour.
Fig. 2.5 Cutting patterns
Table 2.16 Hourly pro￾duction capabilities of the
two machines
P1 P2 P3
M1 3 5 10
M2 6 4 12
56 2 Linear ProgrammingIn addition, it is required that at least ten units of the second product are made.
Formulate a profit-maximizing linear programming model for this problem.
Solution: Defining x1, x2, and x3 as the quantities of the products to be made, the
objective function below is formulated as usual. However, before we formulate the
constraints, we have to adjust the units. The entries in Table 2.16 are expressed in
terms of quantity units per hour. Multiplying them by the variables as usual would
result in the meaningless units (quantity units)2 per hour. Instead, we need to convert
the entries in the table to hours per quantity units or, more conveniently, minutes per
quantity unit. Multiplying by the variables (measured in quantity units), we obtain
minutes used in the production, which can then be related to the capacities. The
formulation can then be written as follows.
P : Max z ¼ 18x1 þ 12x2 þ 6x3
s:t: 20x1 þ 12x2 þ 6x3  1440
10x1 þ 15x2 þ 5x3  960
x2  10
x1, x2, x3  0:
Incidentally, the optimal solution prescribes that 43.5, 10, and 75 units of the
respective products are made for a total profit of $1353.
Problem 2 (allocation of time to courses): A student is planning the coming
semester. In particular, he is attempting to allocate the weekly number of hours of
study to the individual courses he is taking. Each hour of study will increase his mark
by a certain quantity (starting at zero). Table 2.17 shows the marginal improvements
of the marks given each hour of study (per week) as well as the marks required for
passing the course.
For example, if our student were to allocate 15 hrs (per week) to marketing, then
his final mark is expected to be 15(5) ¼ 75, which means passing the course.
The student’s objective is to minimize the total number of hours studied. In
addition, the following constraints have been identified:
• A passing grade should be achieved in each course.
• Obtain an average grade of at least 64.
• Suppose that the student has the option to flip hamburgers at McDonalds in his
spare time. This job pays $10 per hour. Assuming that the student has a total of
80 hrs available for study and flipping, he must also make at least $100 per week.
Table 2.17 Input data for Problem 2
Marketing
Organizational
behavior Accounting
Operations
research Finance
Marginal
improvement of mark
5 4.5 5.5 3.5 5.5
Marks required for
passing the course
50 55 60 50 50
2.2 Applications of Linear Programming 57• The number of hours allocated to operations research should be at least 20% of
the number of hours allocated to the other four subjects combined.
Solution: Given that time is the scarce resource, we define xj as the number of
hours allocated to studying the j-th subject, j ¼ 1, ..., 5. The objective as well as the
individual and overall passing requirements and the need to spend at least 20% of his
studies on operations research are formulated in a straightforward fashion. The need
to make at least $100 in the hamburger shop is formulated by first determining the
hours used for hamburger flipping, which is the number of hours available overall
(here 80) minus the hours used for studying (the sum of variables). These hours are
then multiplied by the hourly wage of $10, which is then the amount of money made.
This amount is then required to be at least $100. Another important—and frequently
forgotten—part of this type of problem is the inclusion of constraints that limit the
grades to 100. As a matter of fact, if these constraints were omitted, our student
would aim for passing grades in most courses and allocate a large number of hours to
one course, in which good marks are easy to obtain, so that he receives in excess of
100 marks. Clearly, this is not possible, making the additional limitations necessary.
P : Min z ¼ x1 þ x2 þ x3 þ x4 þ x5
s:t: 5x1  50
4:5x2  55
5:5x3  60
3:5x4  50
5:5x5  40
5x1  100
4:5x2  100
5:5x3  100
3:5x4  100
5:5x5  100
5x1 þ 4:5x2 þ 5:5x3 þ 3:5x4 þ 5:5x5  5 64 ð Þ
10 80 ½   ð Þ x1 þ x2 þ x3 þ x4 þ x5 100
x4  :2ð Þ x1 þ x2 þ x3 þ x5
x1, x2, x3, x4, x5  0:
If the problem were solved, we find that the student studies a total of about
66½ hrs: 10 hrs for Marketing, 12.22 hrs for Organizational Behavior, 18.18 hrs for
Accounting, 14.29 hrs for Operations Research, and 11.82 hrs for Finance, resulting
in marks of 50, 55, 100, 50, and 65 in the five areas, i.e., minimum passing grades in
58 2 Linear ProgrammingMarketing, Organizational Behavior, and Operations Research, while the 100 marks
in Accounting and the 65 marks in Finance are significantly better than the minimum
requirements.
Problem 3 (reformulation of the dynamic production-inventory problem):
Formulate the problem in Sect. 2.2.6 with inventory variables that are defined at the
end of the period.
Solution: Define now I0, I1, I2, I3, and I4 as the inventory levels at the end of
periods 0 (the beginning of the planning period), 1, 2, 3, and 4. We can then use the
same formulation provided in Sect. 2.2.6, except that we need to replace It by It1 for
t ¼ 1, 2, 3, and 4. Doing so results in the formulation
P : Min z ¼ 1x1 þ 1:1x2 þ 1:2x3 þ 1:25x4 þ :05I1 þ :15I2 þ :15I3
s:t: x1  120
x2  140
x3  150
x4  140
I1 ¼ I0 þ x1  80 or, as ð Þ I0 ¼ 20, x1  I1 ¼ 60
I2 þ I1 þ x2 ¼ 70
I3 þ I2 þ x3 ¼ 130
I4 þ I3 þ x4 ¼ 150 or, as ð Þ I4 ¼ 0, x4 þ I3 ¼ 150
x1, x2, x3, x4, I1, I2, I3  0:
Using an interpretation that reflects the inventory variables as defined here, the
solution is again the same as before.
Problem 4 (a two-product production–inventory model): A firm manufactures
two products. Their production capacities for the two products, unit production
costs, and estimated demands are shown in Table 2.18.
The opening inventories of the two products are 0 and 10 units, respectively. At
the end of Month 3, we do not want any inventories left. The inventory carrying costs
are 20¢ per unit of product A and 50¢ for each unit of product B. These costs are
incurred whenever one unit of a product is carried over from 1 month to the next. The
total inventory levels (for both products combined) from Month 1 to Month 2 should
not exceed 40 units, while the total inventory level between Months 2 and 3 should
Table 2.18 Parameters for the sample problem
Month 1 Month 2 Month 3
Product
A
Product
B
Product
A
Product
B
Product
A
Product
B
Production capacity 70 40 80 30 80 10
Unit production cost $3.10 $10.50 $3.20 $10.80 $3.80 $12.00
Estimated demand dA,
dB
50 30 60 10 100 40
2.2 Applications of Linear Programming 59not exceed 50 units. Formulate a linear programming model for finding a cost￾minimizing production plan.
Solution: The decision variables are xA1, xA2, and xA3 as the production quantities
of product A in the months 1, 2, and 3, and xB1, xB2, and xB3 as the production
quantities of product B in the 3 months. In addition, the inventory levels at the
beginning of periods 1, 2, 3, and 4 (where the inventory level at the beginning of
period 4 equals the inventory level at the end of period 3) for the two products are
defined as IA1, IA2, IA3, and IA4, and IB1, IB2, IB3, and IB4, respectively.
The formulation of the problem is then
P : Min z ¼ 3:1xA1 þ 3:2xA2 þ 3:8xA3 þ 10:5xB1 þ 10:8xB2 þ 12xB3
þ:2IA2 þ :2IA3 þ :5IB2 þ :5IB3
s:t: xA1  70
xA2  80
xA3  80
xB1  40
xB2  30
xB3  10
IA1 ¼ 0
IB1 ¼ 10
IA2 ¼ IA1 þ xA1  50
IA3 ¼ IA2 þ xA2  60
IA4 ¼ IA3 þ xA3  100
IB2 ¼ IB1 þ xB1  30
IB3 ¼ IB2 þ xB2  10
IB4 ¼ IB3 þ xB3  40
IA4 ¼ 0
IB4 ¼ 0
IA2 þ IB2  40
60 2 Linear ProgrammingIA3 þ IB3  50
xA1, xA2, xA3, xB1, xB2, xB3, IA1, IA2, IA3, IA4, IB1, IB2, IB3, IB4  0:
The optimal solution is shown in Table 2.19. The associated costs are $1498.
Problem 5 (blending of tobaccos): A large tobacco manufacturer has the choice
of buying four tobacco types Virginia, Burley, Latakia, and Kentucky. Once suffi￾cient quantities have been purchased, they will make three blends of pipe tobacco,
viz., Sweet Smell, Brown Lung, and Black Death. The following information is
available.
• The four types of tobacco cost $3, $6, $5, and $2 per pound (in the order they
were mentioned).
• The final blends are sold by the 4 oz pouch, i.e. there are four pouches per pound.
• The blends sell for $7, $9, and $12 per pouch (in the above order).
• Sweet Smell consists of 20% Virginia, 50% Burley, and 30% Latakia, Brown
Lung is blended from 40% Latakia and equal proportions of the remaining
tobaccos, and Black Death is 80% Kentucky and 20% Latakia.
• The four tobaccos are available in limited quantities. We may purchase up to
300 lbs of Virginia, 500 lbs of Burley, 100 lbs of Latakia, and 50 lbs of Kentucky.
• Our customers have placed orders for exactly 500 pouches of Sweet Smell and
400 pouches of Brown Lung. There are no firm orders for the expensive Black
Death, but we are certain to be able to sell between 80 and 120 pouches.
(a) Formulate a linear programming problem for the above situation. Define the
variables clearly.
(b) Assume that there is a 5% loss in the blending process. Explain the changes in
the formulation.
Solution:
(a) As usual, the variables are denoted by xij and defined as the quantity of i-th raw
tobacco in j-th blend. The problem is very similar to that in Sect. 2.2.6. The only
Table 2.19 Optimal solution of Problem 4
Month 1 Month 2 Month 3
P1 IA1 xA1 dA1 IA2 xA2 dA2 IA3 xA3 dA3 IA4
0 +50 50 ¼0 20 +80 100 ¼0
0 +80 60 ¼20
P2 IB1 xB1 dB1 IB2 xB2 dB2 IB3 xB3 dB3 IB4
10 +30 30 ¼10 30 +10 40 ¼0
10 +30 10 ¼30
2.2 Applications of Linear Programming 61major difference is that the raw materials are measured in pounds, while the
products are sold by the pouch. As four pouches make a pound, we need to
convert pouches to pounds by multiplying the quantities of the products by
4. The problem can then be formulated as:
P : Max z ¼ 7ð Þ x11 þ x21 þ x31 þ x41 4 þ 9ð Þ x12 þ x22 þ x32 þ x42 4
þ12ð Þ x13 þ x23 þ x33 þ x43 4  3ð Þ x11 þ x12 þ x13 6ð Þ x21 þ x22 þ x23
5ð Þ x31 þ x32 þ x33 2ð Þ x41 þ x42 þ x43
s:t: x11 þ x12 þ x13  300
x21 þ x22 þ x23  500
x31 þ x32 þ x33  100
x41 þ x42 þ x43  50
4ð Þ¼ x11 þ x21 þ x31 þ x41 500
4ð Þ¼ x12 þ x22 þ x32 þ x42 400
4ð Þ x13 þ x23 þ x33 þ x43 120
4ð Þ x13 þ x23 þ x33 þ x43 80
x11 ¼ 0:2ð Þ x11 þ x21 þ x31 þ x41
x21 ¼ 0:5ð Þ x11 þ x21 þ x31 þ x41
x31 ¼ 0:3ð Þ x11 þ x21 þ x31 þ x41
x12 ¼ 0:2ð Þ x12 þ x22 þ x32 þ x42
x22 ¼ 0:2ð Þ x12 þ x22 þ x32 þ x42
x32 ¼ 0:4ð Þ x12 þ x22 þ x32 þ x42
x42 ¼ 0:2ð Þ x12 þ x22 þ x32 þ x42
x33 ¼ 0:2ð Þ x13 þ x23 þ x33 þ x43
x43 ¼ 0:8ð Þ x13 þ x23 þ x33 þ x43
x11, x12, x13, x21, x22, x23, x31, x32, x33, x41, x42, x43  0:
The optimal solution purchases 45, 82.5, 83.3, and 44 lbs of the respective raw
tobaccos, and produces 500, 400, and 120 pouches of the three tobacco blends.
The profit is $7404.50.
(b) Suppose that there is a 5% loss in the blending process. This can easily be
accounted for as follows. Everywhere the quantity of a product is referred to, it is
replaced by the quantity multiplied by 1  5% ¼ 0.95. In this formulation, we
replace (x11 + x21 + x31 + x41) by (x11 + x21 + x31 + x41)(.95) and similar for (x12
62 2 Linear Programming+ x22 + x32 + x42) and (x13 + x23 + x33 + x43) in the objective function, and the
second set of constraints. It would also be easy to include different losses for
different products. The solution here purchases 47.37, 86.84, 87.89, and
46.31 lbs of the respective raw tobaccos, and sells again 500, 400, and
120 pouches of the three tobacco blends. The profit is now $7344.74, about a
1% decrease as compared to the case without losses.
Problem 6 (Blending of gasolines): Table 2.20 describes components in a petro￾leum refinery that can be used to blend gasoline:
The purpose is to blend two types of gasoline, Regular and Premium, so as to
minimize the overall costs required to satisfy the demand. The Regular brand
consists of Naphta, Hydrocrackate, and Reformate, while Premium consists of
Naphta, Hydrocrackate, and Alkylate. The total contracted demand for gasoline is
80,000 barrels.
• Regular: The octane number must be at least 87 and the vapor pressure cannot
exceed 7.2.
• Premium: The octane number must be at least 91 and the vapor pressure cannot
exceed 6.8.
Assume that there are no losses in the blending process and that all quantities
blend linearly by quantity, and formulate a cost-minimizing linear programming
model for this situation.
Solution: Given the four “raw materials” Naphta, Hydrocrackate, Reformate,
and Alkylate along with the two products Regular and Premium, we can define the
variables xij as the quantity of raw material i in product j.
P : Min z ¼ 53ð Þþ x11 þ x12 48ð Þþ x21 þ x22 62x31 þ 69x42
s:t: x11 þ x12  30, 000 availability of ð Þ Naphta
x21 þ x22  45, 000 availability ð Þ Hydrocrackate
x31  20, 000 availability of ð Þ Reformate
x42  15, 000 availability of ð Þ Alkylate
Table 2.20 Input data for Problem 6
Component
Availability
(in barrels)
Octane
number
Vapor
pressure Cost per barrel
Naphta 30,000 85 10 $53
Hydrocrackate 45,000 79 4 $48
Reformate 20,000 101 9 $62
Alkylate 15,000 108 5 $69
2.2 Applications of Linear Programming 63x11 þ x12 þ x21 þ x22 þ x31 þ x42 ¼ 80, 000 demand ð Þ
85x11 þ 79x21 þ 101x31  87ð Þ x11 þ x21 þ x31 ð Þ octane Regular
10x11 þ 4x21 þ 9x31  7:2ð Þ x11 þ x21 þ x31 ð Þ vapor pressure Regular
85x12 þ 79x22 þ 108x42  91ð Þ x12 þ x22 þ x42 ð Þ octane Premium
10x12 þ 4x22 þ 5x42  6:8 ð Þ x12 þ x22 þ x42 ð Þ vapor pressure Premium
x11, x21, x31, x12, x22, x42  0:
The optimal solution produces a total of 80,000 barrels, 70,833.33 regular and
9166.67 barrels premium. Other than the resource Reformate, none of the resources
is fully used. The octane constraints are both binding, so is the vapor pressure
constraint for regular. The vapor pressure constraint for premium has some slack.
The total profit is $4,305,211.
Problem 7 (blending with exact requirements): A fish processing plant makes
two types of fish sticks, the Scrumptious Skipper and the Delicious Sailor. The
Skipper consists of exactly 30% pollock, 40% haddock, and 30% sole, while the
Sailor contains 30% pollock, 20% haddock, and 50% sole. A one-pound package of
the Skipper sells for $2.50, while a one-pound pack of the Sailor retails for $3.50.
There are 4000 lbs of pollock, 3000 lbs of haddock, and 3000 lbs of sole available in
the plant. Formulate a profit-maximizing linear programming problem for this
situation.
Solution: The usual thought would be to define the variables xij as the quantity of
the i-th type of fish in the j-th type of fish sticks. The formulation is then
P : Max z ¼ 2:5ð Þþ x11 þ x21 þ x31 3:5ð Þ x12 þ x22 þ x32
s:t:x11 þ x12  4000
x21 þ x22  3000
x31 þ x32  3000
x11 ¼ :3ð Þ x11 þ x21 þ x31
x21 ¼ :4ð Þ x11 þ x21 þ x31
x31 ¼ :3ð Þ x11 þ x21 þ x31
x12 ¼ :3ð Þ x12 þ x22 þ x32
x22 ¼ :2ð Þ x12 þ x22 þ x32
64 2 Linear Programmingx32 ¼ :5ð Þ x12 þ x22 þ x32
x11, x12, x21, x22, x31, x32  0:
While this is a correct formulation, it is too large for what it does. Due to the fact
that the blending requirements have to be satisfied exactly, we could simply formu￾late variables x1 and x2 as the quantities of the two fish stick packages that we make,
and obtain the simpler formulation
P’ : Max z ¼ 2:5x1 þ 3:5x2
s:t: :3x1 þ :3x2  4000
:4x1 þ :2x2  3000
:3x1 þ :5x2  3000
x1, x2  0:
In both cases, we make 6428.57 packages of the Skipper and 2142.86 packs of the
Sailor for a profit of $23,571.43. This shorter formulation is possible as there is a
fixed relation between the number of packages of the two products and the quantity
of the fish input (e.g., the quantity of pollock in the packages of Skipper is exactly 0.3
times the quantity of Skipper packages). This was not the case in Problems 5 and 6.
Problem 8 (a transportation problem): Consider a school district’s problem to
assign student from different villages to central schools. Typically, with the closing
of small neighborhood schools of the “little red schoolhouse” type and the establish￾ment of larger centralized schools, it has become necessary to bus the students to the
schools (as these distances would even make Abe Lincoln take the bus). The
objective is to ensure that all students must be able to take a bus, and school
capacities cannot be violated.
Suppose there are three villages with 30, 50, and 20 students each. The two
centralized schools have capacities of 70 and 60 students, respectively. The distances
between the villages and the schools are shown in the matrix C below.
C ¼
20 15
40 30
60 20
2
6
4
3
7
5:
(a) Formulate a transportation problem for minimizing the total mileage traveled by
the students.
(b) Suppose that the buses available to the district each have a capacity of
35 students. Formulate constraints to ensure that there are no overfilled buses.
(c) In addition to the constraints under (b), it is now required that each school is
filled to at least 75% capacity.
2.2 Applications of Linear Programming 65Solution:
(a) We first define variables, so that xij denotes the number of students bused from
village i to school j. The model can then be formulated as follows.
P : Min z ¼ 20x11 þ 15x12 þ 40x21 þ 30x22 þ 60x31 þ 20x32
s:t: x11 þ x12 ¼ 30
x21 þ x22 ¼ 50
x31 þ x32 ¼ 20
x11 þ x21 þ x31  70
x12 þ x22 þ x32  60
x11, x12, x21, x22, x31, x32  0:
Incidentally, the solution is summarized in the optimal transportation plan
T ¼
30 0
10 40
0 20
2
6
4
3
7
5:
The total mileage to bus all students to schools is 2600, and while the second school
is filled to capacity, the first houses only 40 students, well shy of its capacity
of 70.
(b) The most obvious way to formulate this constraint is to impose capacities on all
routes, i.e., write six additional constraints x11  35, x12  35, ..., x32  35. This
is, however, unnecessary, as only buses leading out of the second village could
possibly have more students than the bus capacity allows. Hence it is sufficient
to add the two constraints x21  35 and x22  35. The new solution has the
transportation plan
T ¼
25 5
15 35
0 20
2
6
4
3
7
5
requiring a total mileage of 2625, a very minor increase from the original 2600.
66 2 Linear Programming(c) Filling the schools to at least 75% of capacity requires the two schools to house at
least 70(.75) ¼ 52.5 and 60(.75) ¼ 45 students. Since integrality is required and
these are lower bounds on the number of students, we have to round up the first
number to 53. We then add the constraints
x11 þ x21 þ x31  53
x12 þ x22 þ x32  45
to the problem. The optimal solution is then shown in the transportation plan
T ¼
30 0
23 27
0 20
2
6
4
3
7
5
which has an associated total mileage of 2730.
Problem 9 (an assignment problem): The manager of an experimental farm
faces the task of trying out three fertilizers, each based on some of the main
ingredients in commercial products: Phosphorus, Biosolids (mostly domestic
septage), and Nitrogen. Each of the fields is to be planted with one of the three
crops, viz., Wheat, Rye, and Corn. The crops react differently to the fertilizers, and
small-scale lab tests have suggested the yields shown in the matrix A, in which aij
indicates the expected yield of crop i given that only fertilizer j is used. The fertilizers
and yields blend linearly, e.g., if the field of wheat were fertilized with 30%
phosphorus and 70% biosolids, the yield would be 0.3(5) + 0.7(1) ¼ 2.2. The
manager needs to ensure that no more than 100% of the available quantity of
fertilizers is used and that each crop receives 100% fertilization.
A ¼
517
345
632
2
6
4
3
7
5:
(a) Determine an optimal fertilization plan that maximizes the yield.
(b) Suppose that a new product Potassium became available, which results in yields
of 4, 6, and 6 if applied to the three respective crops. Does that revelation have
any effect on the fertilization plan?
2.2 Applications of Linear Programming 67Solution:
(a) Define xij as the proportion of fertilizer j that is applied to the field with crop i.
The problem can then be written as
P : Max z ¼ 5x11 þ x12 þ 7x13 þ 3x21 þ 4x22 þ 5x23 þ 6x31 þ 3x32 þ 2x33
s:t: x11 þ x12 þ x13 ¼ 1
x21 þ x22 þ x23 ¼ 1
x31 þ x32 þ x33 ¼ 1
x11 þ x21 þ x31 ¼ 1
x12 þ x22 þ x32 ¼ 1
x13 þ x23 þ x33 ¼ 1
xij  0 for all i, j:
The optimal solution is x13 ¼ x22 ¼ x31 ¼ 1, and xij ¼ 0 otherwise. In other words,
wheat is fertilized entirely with nitrogen, rye only with biosolids, and corn
exclusively with phosphorus. The expected yield is 17.
(b) Adding the new fertilizer potassium changes the formulation as follows (the
additions are shown in boldface):
P : Max z ¼ 5x11 þ x12 þ 7x13 þ 4x14 þ 3x21 þ 4x22 þ 5x23 þ 6x24 þ 6x31 þ 3x32
þ2x33 þ 6x34
s:t: x11 þ x12 þ x13 þ x14 ¼ 1
x21 þ x22 þ x23 þ x24 ¼ 1
x31 þ x32 þ x33 þ x34 ¼ 1
x11 þ x21 þ x31  1
x12 þ x22 þ x32  1
x13 þ x23 þ x33  1
x14 þ x24 þ x34  1
68 2 Linear Programmingxij  0 for all i, j:
The new optimal solution is similar to the previous solution, except that it replaces
the biosolids applied to rye by potassium. The expected yield increases by close
to 12% to 19.
Problem 10 (a one-dimensional cutting stock problem): A home building store
faces the following problem. Its customers demand half-inch plywood in the sizes
4 ft  3 ft, 4 ft  5 ft, and 4 ft  6 ft. Customer demand for these three sizes is
estimated to be at least 20, 50, and 40, respectively, and customers are prepared to
pay $7, $9, and $10 for each of these sheets, respectively. The store must generate
these sizes by cutting up standard 4 ft  8 ft sheets, each of which costs them $6. Up
to 100 such sheets are available. Furthermore, each cut costs the store $1.50.
Formulate a model that indicates to the decision maker how to cut up the 4 ft
 8 ft sheets so as to maximize his profit. Clearly show the cutting patterns and
define your variables properly.
Solution: Since the widths of the desired sheets are all 4 ft, we only have to
consider a single dimension. The cutting plan includes patterns with two 3 ft  4 ft
boards (and the resulting 2 ft  4 ft piece of waste), one 3 ft  4 ft board and a 5 ft
 4 ft board without any waste, and a single 6 ft  4 ft board with the resulting 2 ft
 4 ft piece of waste. The number of cuts performed according to these three patterns
are denoted by y1, y2, and y3, respectively. The problem can then be formulated as
P : Max z ¼ 7 2y1 þ y2 ð Þþ 9 y2 ð Þþ 10 y3 ½  ð Þ 6 y1 þ y2 þ y3 ½  1:5 2y1 þ y2 þ y3 ½ 
¼ 5y1 þ 8:5y2 þ 2:5y3
s:t: 2y1 þ y2  20 generate at least twenty 4 ft ð Þ  3 ft sheets
y2  50 generate at least fifty 4 ft ð Þ  5 ft sheets
y3  40 generate at least forty 4 ft ð Þ  6 ft sheets
y1 þ y2 þ y3  100 up to 100 sheets of wood are available ð Þ
y1 y2, y3  0 and integer
The optimal solution does not cut the first pattern and uses the second and third
pattern 60, and 40 times, respectively, resulting in a profit of $610. Changing the
demands to at least 20 (as is) for the first size, exactly 50 for the second, and at most
40 for the third size, we only cut the second pattern 100 times.
Problem 11 (a two-dimensional cutting stock problem): A planner has
30 sheets of plywood of size 10 ft  10 ft. They presently need 20 sheets in the
shape of disks of diameter 5 ft as well as 15 sheets of plywood in the shape of
4 ft  6 ft rectangles. The cutting patterns that are considered by the decision maker
are shown in Fig. 2.5.
2.2 Applications of Linear Programming 69It costs $2.00 to cut a disk and $1.50 to cut a rectangle. This price includes all
required cuts and is independent of the pattern the shape is cut from. Alternatively,
we could purchase a disk at $4.50 and a rectangle for $3.00 each.
Formulate a linear programming problem that
• Minimizes the cost of obtaining the required shapes
• Does not use more 10 ft  10 ft sheets than are available
• Produces the required numbers of disks and rectangles, and
• Ensures that the cutting results in no more than 30% of waste.
Define all variables clearly.
Solution: Define yj, j ¼ 1, 2, 3 as the number of times the j-th pattern is cut. In
addition, define variables v4 and v5 as the number of disks and rectangles that are
purchased. The problem can then be formulated as:
P : Min z ¼ 8y1 þ 6y2 þ 5:5y3 þ 4:5v4 þ 3v5
s:t: y1 þ y2 þ y3  30
4y1 þ 2y3 þ v4  20
4y2 þ 1y3 þ v5  15
:2146y1 þ :04y2 þ :3673y3  :3 y1 þ y2 þ y3 ð Þ
y1, y2, y3, v4, v5  0 and integer,
where the coefficients on the left-hand side of the last constraint are the proportions
of waste given a 10 ft  10 ft sheet.
2.3 Graphical Representation and Solution
Having described a variety of different applications of linear programming
problems, this section will first demonstrate how linear programming problems
can be represented graphically. We then discuss in some detail a graphical solution
method. This is followed by a discussion of a number of special cases that may occur
in the modeling and solution processes, and how to react to them as an analyst.
Throughout this chapter, we will restrict ourselves to the case of two variables in
order to accommodate easy graphing. This does, of course, mean that the technique
we describe in this section is not made for the solution of realistic problems that
typically have tens of thousands of variables. Much rather, the purpose of this
discussion is to create an understanding of what happens in the solution of linear
programming problems and what the difficulties are, regardless of the size of the
problem.
The first subsection will demonstrate how constraints and objective functions can
be graphed and how the problem can be solved graphically. Based on the under￾standing of this material, Sect. 2.3.2 will then discuss a number of special situations
70 2 Linear Programmingthat may occur in the solution process and how to deal with the resulting messages
from the solver.
2.3.1 The Graphical Solution Method
As discussed in the introduction to linear programming, each model consists of an
objective function and a number of constraints. This subsection will first demon￾strate how to plot constraints, and then show how to deal with objective functions,
and then put it all together in the graphical solution method.
Assuming that we have two variables x1 and x2, a constraint could be a linear
function such as 3x1 + 2x2  6. In order to plot this constraint, it is easiest to first
consider the associated equation 3x1 + 2x2 ¼ 6. It is known that the line in two
dimensions is uniquely determined by two points. In order to do so, we can simply
set either of the variables to any value we like and solve for the other variable,
resulting in one of the required points. Repeating this step with a different value will
result in a second point. The straight line that leads through both of these points is
then the set of all points that satisfy the equation.
In our example, setting x1 ¼ 0 leads to 2x2 ¼ 6 or x2 ¼ 3, so that the first point is
(x1, x2) ¼ (0, 3). The second point can be obtained by setting x2 ¼ 0, which leads
directly to 3x1 ¼ 6, or, equivalently, x1 ¼ 2. As a result, our second point is (x1, x2)
¼ (2, 0). The straight line in Fig. 2.6 is the set of points that satisfy 3x1 + 2x2 ¼ 6.
So far, we have determined that an equation is represented in two dimensions as a
straight line. (Note that in a single dimension an equation is just a point.) In three
dimensions, an equation would be represented by a plane, so that in general, we
Fig. 2.6 Hyperplane and
halfspaces
2.3 Graphical Representation and Solution 71speak about an equation in any number of dimensions being represented by a
hyperplane.
Back in our two-dimensional example, recall that the constraint in question is the
inequality 3x1 + 2x2  6, so that not only the set of points on the line are addresses.
As a matter of fact, a  or  inequality will refer to the set of points on the line and
all points in one of the two halfplanes generated by the line. The obvious question is
then which of the two halfplanes is addressed by the constraint in question. Some
people might believe that the halfplanes that belong to  inequalities are below the
line, while those of  are above the line. This is not true, as each  inequality can be
rewritten as an equivalent  inequality. In our example, the constraint 3x1 + 2x2  6
is equivalent to its counterpart 3x1  2x2  6. Both constraints define exactly the
same set of points.
A simple way to determine the proper halfplane is to choose any point that is not
located on the line we have plotted and determine whether or not it satisfies the
constraint in question. If so, then the point is located on the proper side of the line,
otherwise the halfplane is on the other side. In our example, consider, for instance,
the origin as a point. Its coordinates are (0, 0), so that the constraint 3x1 + 2x2  6
reduces to 0  6, which is correct. This means that the origin is on the “correct” side
of the line, which allows us to determine the halfplane as being on the lower left side
of the line. Had we chosen the point, say, (4, 2) instead, our constraint would have
been 3(4) + 2(2)  6 or 16  6, which is wrong, meaning that the point (4, 2) is
located on the “wrong” side of the line. As a matter of fact, the set of points on the
line and in the halfplane to the upper right of the line is determined by the constraint
3x1 + 2x2  6. Figure 2.6 shows both the hyperplane and both halfplanes for all three
types of constraints allowed in linear programming: ¼, , and . When graphing,
the halfplane is always indicated by a hyperplane and a small flag at its end that
indicates the appropriate halfplane.
Given that an equation is represented by a straight line in two and a plane in three
dimensions, an inequality is represented by a halfplane in two dimensions and half
the space in three dimensions (the separating line is given by the associated equation
and the other half of the space is defined by the same inequality but with inverted
inequality sign). This has led to the term halfspace that, in contrast to halfplane,
which applies only to two dimensions, applies to the representation of an inequality
of the  or  type in any number of dimensions.
At this point, we are able to plot the hyperplane or halfspace for each constraint in
a given problem. In order to determine the feasible set (also called feasible region or
set of feasible solutions), we first need to define a solution as feasible, if it satisfies all
of the given constraints. This is not really a restriction, as if we do not want a
constraint to be satisfied, why include it in the problem in the first place? Given that
all constraints must be satisfied, the feasible set is then the intersection of the
halfspaces and/or hyperplanes that correspond to all constraints of the problem.
As a numerical illustration, consider the following numerical example.
72 2 Linear ProgrammingP : Max z ¼ 2x1 þ 3x2
s:t: x1 þ 4x2  12 ðIÞ
2x1  x2  2 ðIIÞ
5x1 þ 3x2  15 ðIIIÞ
4x1 þ 6x2  6 ðIVÞ
x1  0 ðVÞ
x2  0: ðVIÞ
The feasible set determined by the constraints of problem P, is shown as the
shaded area in Fig. 2.7.
In our two-dimensional space, the feasible set is a linearly bounded polygon
(in general, referred to as a polytope). It consists of the boundary with its linear
segments and corner points (frequently referred to as extreme points) A, B, ..., F, as
well as the interior. At each of the extreme points, at least two constraints are
satisfied as equations. These constraints are usually referred to as binding (or tight)
at this point. In our example, at point A, the constraints IV and V are satisfied as
equations, at point B, constraints II and V are satisfied as equations, at point C,
constraints I and II are satisfied as equations, and so forth.
If desired, we can then determine the exact coordinates of all extreme points by
solving a system of simultaneous linear equations. For instance, for point A, the
Fig. 2.7 Feasible region
2.3 Graphical Representation and Solution 73system of simultaneous linear equations includes all constraints satisfied as
equations at this point. Here, these are the equations based on constraints IV and
V, so that the system is
4x1 þ 6x2 ¼ 6 and
x1 ¼ 0:
Replacing x1 ¼ 0 in the first equation and solving for x2, we obtain x2 ¼ 1, so that
the coordinates are (x1, x2) ¼ (0, 1).
Similarly, consider point C. At his point, the constraints I and II are binding, so
that we have the set of simultaneous linear equations
x1 þ 4x2 ¼ 12
2x1  x2 ¼ 2:
A system like this can be solved by any of the pertinent methods, see Appendix B
of this volume. One (albeit somewhat awkward) possibility is to use the substitution
technique. Here, we solve the first equation for x1, resulting in x1 ¼ 12  4x2. We
then replace x1 by this expression in the second equation, so that we obtain
2(12  4x2)  x2 ¼ 2. Solving this equation for x2 results in x2 ¼ 26
9  2:8889.
Replacing x2 by this value in x1 ¼ 12  4x2 and solving for x1 results in x1 ¼ 4
9 
0:4444. Table 2.21 shows the points, the constraints that are satisfied as equations at
that point, and their exact coordinates.
In n dimensions, each extreme point is determined by the intersection of at least
n hyperplanes, so that we have to solve a system of at least n simultaneous linear
equations in n variables to determine the coordinates for each of these extreme
points.
Consider now the objective function. To simplify matters, we will at first ignore
the constraints and deal exclusively with the objective function and its representa￾tion, before we combine objective function and constraints in the graphical solution
method.
For now, consider the objective function Max z ¼ 2x1 + 5x2. Ignoring the
maximization for a moment, we have 2x1 + 5x2 ¼ z, which is nothing but a regular
constraint with an unknown right-hand side value z. As discussed above, for any
Table 2.21 Coordinates of extreme points of sample problem
Point Constraints binding at point Coordinates (x1, x2) z-value
A IV, V (0, 1) 3
B II, V (0, 2) 6
C I, II 4
9 , 2 8
9
   ð Þ :4444, 2:8889 9.5556
D I, III 1 7
17, 2 11
17
   ð Þ 1:4118, 2:6471 10.7649
E III, VI (3, 0) 6
F IV, VI (1½, 0) 3
74 2 Linear Programmingvalue of z, we have an equation that can be represented by a hyperplane in the space
of variables, here the (x1, x2) space. Fig. 2.8 shows these lines for values of z ¼ 5,
10, 15, and 20.
Depending on the type of objective function under consideration, these lines are
usually referred to as iso-profit lines, iso-cost lines, or simply contour lines. Their
name derives from the fact that all points on any one of these lines have the same
value of the objective function. (You may be familiar with the contour lines on
topographic maps. Those lines are really iso-altitude lines, or lines of equal altitude.)
In other words, given the objective function under consideration, all points on the
line labeled z ¼ 5 are considered equally good by the decision maker. Similarly, a
decision maker will consider all points on the line z ¼ 10 as equally good―but better
than those on the z ¼ 5 line. In this example, the value of the objective function gets
better in the northeasterly direction.
It is then possible to construct a vector that points into the direction, in which the
objective function improves. This is the so-called gradient of the objective function,
sometimes also referred to as the direction of the objective function. Formally, a
gradient is the vector of partial derivatives, but here it is sufficient to think of it as the
direction in which the solutions get better. The gradient is constructed as follows,
where we use again our numerical example with the objective Max z ¼ 2x1 + 5x2.
Each term of the objective function can be thought of as the combination of the step
direction and the step length. Here, x1 means move to the right, and the coefficient
2 tells us to move 2 steps into that direction. The next term indicates that we should
move 5 steps into the x2 direction. Starting at an arbitrary point, we first move 2 steps
into the x1 direction, followed by 5 steps into the x2 direction. The starting point is
then connected to the end point, resulting in the gradient. Usually, we start these
moves at the origin, but this is not necessary.
Fig. 2.8 Iso-profit lines
2.3 Graphical Representation and Solution 75Observe that the gradient of the objective function is perpendicular to the
iso-profit lines. Once we have the gradient, it is not necessary to explicitly plot
any of the iso-profit lines. (In more than two dimensions, the gradient is a ray that is
orthogonal―the generalization of perpendicular to n dimensions―to the iso-profit
hyperplanes.) From a practical point of view, we can plot the gradient of the
objective function and then push the perpendicular iso-profit lines as much into its
direction as possible―the farther we push, the higher the profit.
Before putting it all together and describing the graphical solution technique,
some properties of the objective function are worth mentioning. Suppose that in the
above objective function each of the terms is measured in dollars. Assume now that
we have decided to measure the profit in Indian rupees instead. Suppose that the
present exchange rate is 75 rupees per dollar, so that the objective function is now
Max z0 ¼ 50x1 + 375x2. Plotting this objective, we find that while the gradient is
much longer, the direction of the objective function is exactly the same. As we will
see later, such a change of currency will result in exactly the same solution as the
original objective function, only the objective value changes: z0 will be 75 times the
value of z.
Another point of interest concerns minimization functions. What if the objective
function minimizes some costs, e.g., Min z ¼ 3x1 + 7x2? No special procedure is
needed, as we can simply transform the minimization objective into an equivalent
maximization objective by multiplying it by a negative number, e.g., (1). This will
result in the equivalent objective Max z ¼ 3x1 7x2. As far as the gradient of this
function is concerned, it leads from the origin 3 steps into the x1 direction (i.e.,
three steps to the left), followed by 7 steps into the x2 direction (i.e., 7 steps down).
Everything else remains exactly the same, the value of the objective function
improves (i.e., gets smaller in case of a minimization function) as we shift the
iso-cost lines more and more into the direction of the gradient. Figure 2.9 shows
the gradients for the following objective functions:
(a) Max z1 ¼ 4x1  3x2
(b) Max z2 ¼  x1 + 3x2
(c) Min z3 ¼  2x1  x2
(d) Min z4 ¼ 2x1 + 3x2
It is worthwhile to notice that if we have one function such as z1 ¼ 4x1  3x2 in
the above example, maximizing the function leads to a gradient, which points into a
southeasterly direction. Minimizing the same function leads into the northwest,
diametrically opposed to the maximization of the same function.
We are now able to describe the complete graphical solution technique. After
determining the feasible set, we plot the gradient of the objective function and move
its iso-profit lines into the direction of the gradient, until we hit the last feasible point.
While there are solutions with better objective function values beyond this point,
none of them is feasible. Thus, the last feasible point into the direction of the gradient
is the optimal point.
76 2 Linear ProgrammingIt is apparent that in this procedure, points in the interior of the feasible set cannot
be optimal; any optimal solution will be on the boundary of the feasible set. In fact,
Dantzig has proved his famous corner point theorem, which we will state here
formally.
Theorem (Corner Point Theorem, Dantzig) At least one optimal solution is
located at an extreme point of the feasible set.
The graphical solution method will identify such a corner point, whose exact
coordinates we will have to determine next. As demonstrated earlier in this section,
this is done by way of solving a system of simultaneous linear equations. Once the
exact coordinates of the optimal solution point have been determined, all that is left
to do is to determine the quality of the solution, as measured by the objective
function. This is accomplished by replacing the variables in the objective function
by their optimal values and thus computing the z-value.
We can summarize the procedure in the following steps:
Step 1: Graph the constraints and determine the set of feasible solutions.
Step 2: Plot the gradient of the objective function.
Step 3: Apply the graphical solution technique that pushes iso-profit lines into the
direction of the gradient until the last feasible point is reached. This is the optimal
solution x.
Step 4: Determine which constraints are satisfied as equations at x ¼ ð Þ x1, x2 . Write
them as equations and solve the resulting system of simultaneous linear equations
for the exact coordinates of the optimal solution.
Fig. 2.9 Objective function
gradients
2.3 Graphical Representation and Solution 77Step 5: Use the coordinates of the optimal point in the objective functions and
compute the value of the objective function.
Applying the first two steps of this procedure to the problem stated in the
beginning of this subsection, we obtain the graph in Fig. 2.10. Pushing now the
iso-profit lines (some of which are shown) into the direction of the gradient, we find
that the last feasible point on our way into a northeasterly direction is the extreme
point D. This is the optimal point x. The constraints I and III are binding at this point,
so that we solve the system of simultaneous linear equations that consists of relations
I and III written as equations, i.e.,
x1 þ 4x2 ¼ 12
5x1 þ 3x2 ¼ 15:
The optimal solution is ð Þ¼ x1, x2 1 7
17, 2 11
17    ð Þ 1:4118, 2:6471 , and the
associated value of the objective function is z ¼ 10 13
17  10:7647. It can be shown
that this solution is not only optimal but is the unique optimal solution to the
problem. Sometimes, an optimal point is found, such that at least one of its
neighboring extreme points has the same value of the objective function, and as
such is also optimal. This would happen in our problem, if the same objective
function were to be minimized rather than maximized. In this case, we would find
the points A and F both as optimal solution points with z ¼ 3. More about this issue
can be found in the next subsection on special cases.
While the graphical method as demonstrated above is an exact method
(no approximations were made in the process), its use is to explain the main concepts
and difficulties involved in solving linear programming problems. The reason is that
practical problems have not two or three, but tens of thousands of variables, making
graphing impossible. Since the graphical solution technique uses the exact pictorial
knowledge of the feasible set, it will be necessary to find an algebraic technique that
Fig. 2.10 Graphical solution
technique
78 2 Linear Programmingis independent of the graphical image. Dantzig’s simplex method is such a tool.
Rather than moving through the feasible space directly to the optimal solution, the
simplex method is an incremental technique that starts with a feasible solution
(which can be determined by “some” technique), improves it, tests whether or not
an optimal solution has been found, and if not, increases the solution further. It does
so by moving on the boundary of the feasible set from one extreme point to an
adjacent extreme point. The method also belongs to the class of feasible (and
improving) direction methods. This means that a step from a (feasible) extreme
point to an adjacent extreme point is only made if the solution remains feasible and
the value of the objective function improves in the process. A feature of the feasible
set, called convexity, guarantees that if a point is found none of whose neighbors has
a better z-value than the one we are presently at, this is an overall (i.e., global)
optimal solution.
To demonstrate a simplex path, i.e., the sequence of extreme points generated and
examined by the simplex method, consider again the example of Fig. 2.10 and
assume that we have “somehow” determined point A as a starting point. Point
A has two neighboring extreme points F and B. Both are feasible, so that moves
are possible. However, while the move from A to B improves the value of the
objective function as B is on a higher iso-profit line, the move from A to F will
leave the value of the objective function unchanged. (This is one of the special cases
discussed in the next subsection.) Since we are looking for improvements, the
simplex method will move to point B. At that point, we have again two neighboring
extreme points, viz., A and C. While a move from B to A retains feasibility of the
solution, the z-value would decrease, disallowing such move. On the other hand,
moving from B to C maintains feasibility and improves the value of the objective
function, so that the simplex method makes this move. At point C, we have again
two neighbors, which are B and D. Moving to B is not allowed, as this would
decrease the z-value. On the other hand, a move to D not only maintains feasibility,
but also increases the value of the objective function. The neighboring extreme
points at point D are C and E. Moving either way will keep the solution feasible, but
in both cases, the value of the objective function will decrease. At this point, the
method terminates with the message that point D is an optimal solution.
While examples have been constructed in which the simplex algorithm performs
very poorly, the average performance of the algorithm has been excellent. Given a
problem with m constraints, there is consensus that on average, the simplex algo￾rithm needs to examine only 1½m extreme points. In each step, we need to examine
an extreme point, which means we must solve a system of simultaneous linear
equations. Traditionally, computational details of this method, which is considered
to be one of the ten top algorithms of the twentieth century, have been included in
texts such as this. Given the abundance of software (some of it even free) and the fact
that users do not need to know details about how the method functions, we will not
discuss it in this book. For a full treatment, interested readers are referred to Eiselt
and Sandblom (2007), the file-sharing site that accompanies this book, or one of the
many books available on the subject.
2.3 Graphical Representation and Solution 79Finally, we would like to address the question why, given the tremendous
computing power of today’s equipment, we do not simply enumerate all extreme
points, determine their exact coordinates and their objective values, and then choose
the one with the best objective value (meaning the highest value for maximization
and lowest value for minimization problems), which then will be the optimal
solution. Given Dantzig’s corner point theorem, the procedure is certainly valid in
that it will find an optimal solution. However, as the example below will clearly
demonstrate, it is of no practical value.
As an example, consider a problem whose constraints are 0  x1  1, 0  x2  1,
and so forth for all n variables. With two variables, the feasible set is a square with
the four corner points (x1, x2) ¼ (0, 0), (1, 0), (1, 1), and (0, 1). With three variables,
the set is a unit cube with eight corner points (x1, x2, x3) ¼ (0, 0, 0), (0, 0, 1), (0, 1, 0),
(1, 0, 0), (0, 1, 1), (1, 0, 1), (1, 1, 0), and (1, 1, 1). Continuing in this fashion, we will
have a feasible set in the shape of a hypercube with 2n extreme points. Given a very
small practical problem with only n ¼ 100 variables, there are 2100  1030 corner
points. Given the fastest machines today that can deal with more than 1015 floating
point operations per second (called 1 petaflop), and assuming that one such operation
can determine one extreme point (it cannot: we would need to solve a system of
simultaneous linear equation with 100 variables and 100 equations), we would still
need close to 670,000 years to solve the problem. Considering the fact that practical
linear programming problems do not have 100 variables but maybe hundreds of
thousands of variables, this shows the complete uselessness of enumeration
techniques for linear programming problems.
2.3.2 Special Cases
This subsection discusses incidents that can occur when a linear programming
problem has been formulated and submitted for solution. The first two cases are
really error messages to the modeler. They require immediate intervention, as the
solver will not be able to continue. The last three cases are of a more technical nature,
it is good to know about them, but no user intervention is required. Below, we will
discuss each of these issues graphically and illustrate it by means of a numerical
example. In Sect. 2.4.2, we describe how these special cases appear on the computer
printout of an optimal solution.
1. There exists no feasible solution.
The nonexistence of a feasible solution is related by the solver to the analyst.
Whenever that happens, this must be taken as an error message. This is also the time
for the user to intervene, as the solver is not able to act any further. The error message
indicates that constraints are too tight, meaning that there is a contradiction among
the constraints. This message is very often―but by no means exclusively―received
by inexperienced analysts, who include too many constraints in their model, some of
which refer to situations they wish to happen rather than those that must happen. It is
80 2 Linear Programmingvery important to understand that in a model, the constraints are absolute, meaning
that they cannot be violated.
As an illustration, consider the following numerical example. Since this special
case is caused exclusively by the constraints, we will not include an objective
function in our description. Suppose that the feasible set is determined by the
following set of constraints:
P : 2x1 þ 3x2  7 ðIÞ
x1 þ x2  3 ðIIÞ
x1  0 ðIIIÞ
x2  0: ðIVÞ
A graphical representation of the problem is shown in Fig. 2.11. Clearly, there is a
contradiction between the constraints. To see this algebraically, rewrite constraint II
as x2  3 + x1, which, as constraint III requires that x1  0, implies that x2  3.
Similarly, constraint I can be rewritten as 3x2  7  2x1. As x1  0 per constraint III,
this implies that 3x2  7 or, equivalently, x2  21=3. This is an obvious contradiction
to the requirement that x2  3.
The question often arises as to which of the constraints actually causes
the nonexistence of feasible solutions. To investigate this question, consider again
the example in Fig. 2.11. If constraint I were to be deleted, the feasible set would be
the cone that is constructed by constraints II and III with the vertex at (0, 3). If
constraint II were to be deleted, the feasible set is the triangle with vertices at (0, 0),
(3½, 0), and (0, 21=3). If constraint III were to be deleted, the feasible set would be the
set that is determined by the halfspaces of constraints I, II, and IV, which has its
Fig. 2.11 Contradictory
constraints
2.3 Graphical Representation and Solution 81vertices at the points (.4, 2.6) and (3.5, 0). Finally, if constraint IV were to be
deleted, there would still be no feasible solution.
In summary, we have seen that the deletion of any one of the constraints I, II, and
III causes infeasibility to disappear. This means that the question “which constraint
causes the infeasibility” is indeed the wrong question: it is not a single constraint that
causes infeasibility, but the incompatibility of a number of constraints, here I, II, and
III. Many commercial solvers will not only provide the decision maker with the
“There exists no feasible solution” message, but also offer further help. Typically,
this help comes in the form of an identification of the set of constraints that causes
the infeasibility. If we were to add the constraints x1  2 and x2  2 to the region
pictured in Fig. 2.7 with constraints I–VI, there would be no feasible solution, and the
solver would notify the analyst that the constraints 5x1 + 3x2  15, x1  2, and x2  2
together cause the infeasibility.
The next question is then how to deal with infeasibilities should they occur. The
answer is that the planner has to decide which of the variables involved should be
“loosened up.” As an example of a budget constraint, loosening up such a “”
constraint would be accomplished by increasing the right-hand side value. In other
words, if one of the constraints that causes the infeasibility is a budget constraint,
then increasing the amount of cash available will make the problem less stringent.
Similarly, if a customer requires at least, say, 100 units of a product and we must
supply at least that many units to the customer, such a “” constraint would be made
looser by reducing this number by convincing the customer to accept a somewhat
smaller quantity. Even equations can be relaxed somewhat. As an example, consider
the equation 2x1 + 5x2 ¼ 6, in which the left-hand side indicates the quantity of
certain critical nutrients or medicines that an individual consumes. Any equation can
be expressed as two opposing inequality constraints, in our case as 2x1 + 5x2  6 and
2x1 + 5x2  6. Relaxing these two constraints means allowing a bit less than 6 and a
bit more than 6 units of that nutrient in the diet, which can be achieved by changing
the right-hand side values by some small value. For instance, instead of the original
constraints, we could use 2x1 + 5x2  6.1 and 2x1 + 5x2  5.8, thus allowing a certain
bandwidth within which there are feasible solutions. These are so-called interval
constraints that marginally increase the size of the problem but are much easier to
deal with from a computational and modeling point of view.
Often, however, it is not advisable to change the right-hand side value of a single
constraint, as a very significant change may be required to achieve feasibility. It is
often much easier to make a number of smaller changes on multiple right-hand side
values. As an example, consider the example in Fig. 2.11. Changing the right-hand
side of constraint II alone would require us to reduce the original value of 3 by more
than 22% down to 21=3 before a feasible solution could be obtained. Similarly, the
right-hand side value of constraint I must be increased from its original value of 7 to
at least 9 before a feasible solution can be obtained; that is an increase of more than
28%. Alternatively, we could increase the right-hand side value of constraint I by
14% to 8 and simultaneously reduce the right-hand side value of constraint II by 11%
down to 2.67 and obtain a feasible solution with these smaller changes that may be
easier to implement.
82 2 Linear Programming2. Unbounded “optimal” solutions.
The existence of unbounded “optimal” solutions is, in some sense, the opposite of
nonexisting feasible solutions. This is true in the sense that in the previous case, the
constraints were too tight and had to be loosened up, they are too loose here and
require tightening. Most frequently, this case occurs if some constraints have been
forgotten. Again, it is an error message that requires intervention from the analyst.
As an illustration of this case, consider the following numerical example.
P : Max z ¼ 2x1 þ x2
s:t: x1  x2  2 ðIÞ
2x1 þ x2  1 ðIIÞ
x1, x2  0:
The graphical representation of this problem is shown in Fig. 2.12. Using the
graphical solution method, we notice that we can increase the value of the objective
function to arbitrary levels by following the direction pointed out by the gradient of
the objective function. (In case you should encounter arbitrarily high profits in
practice, remember that you learned it from us first. We are happy with 10% of
infinity.) Clearly, this cannot occur in practice, which is why we use the word
“optimal” in quotes. In order to resolve the issue, it helps if the analyst asks how
such arbitrarily good solutions can be obtained. For instance, in a production
problem we would determine that we make money by making and selling products:
given that machine capacities and demand are limited, profits are limited, too. So it
would benefit the modeler to investigate whether or not the appropriate constraints
have been included.
Fig. 2.12 Unbounded
“optimal” solutions
2.3 Graphical Representation and Solution 83At first glance, it appears that the reason for the existence of unbounded “optimal”
solutions is the fact that the feasible set is not bounded in one direction (in our
example, the northeast). However, if we leave the feasible set in our example
unchanged, but modified the objective function from its original Max z ¼ 2x1 + x2
to Min z ¼ 2x1 + x2, the graphical solution method would continue to use the same
contour lines, but the gradient of the new objective function would point into a
southwesterly direction, diametrically opposed to its previous direction. The (finite)
optimal solution is then found at the origin.
This small example illustrates that a feasible set that is unbounded in one
direction is a necessary, but not sufficient, condition for the existence of unbounded
“optimal” solutions. In addition, the gradient of the objective function must also
point “towards that opening.” Here, we will leave this imprecise wording as it is;
suffice it to mention that exact mathematical conditions for the existence of
unbounded “optimal” solutions do exist.
3. Dual Degeneracy.
This is the first of those special cases that an analyst should know about, but that
does not require intervention. Formally, dual degeneracy occurs if two adjacent
extreme points of the feasible set have the same value of the objective function. An
example is the problem shown in Fig. 2.10. As already discussed in the context of
simplex paths, the point A has the same value of the objective function as its
neighboring point F. That in itself is not really noteworthy. The situation, however,
changes if dual degeneracy occurs at optimum. By definition, if we have an optimal
solution and another extreme point with the same z-value exists, that point must also
be optimal.
An example for this is again Fig. 2.10, but with the original objective function
changed from Max z ¼ 2x1 + 3x2 to Min z ¼ 2x1 + 3x2. This changes the gradient of
the objective function by 180, leaving us with the points A and F as optimal
solutions. Whenever two neighboring extreme points are optimal, all points between
them are also optimal, i.e., dual degeneracy at optimum means the existence of
alternative optimal solutions. In this example, the optimal coordinates of the points
A and F are (0, 1) and (1½, 0), respectively, both having an objective value of z ¼ 3.
Some points on the line segment between these two points are x ¼ ð Þ ¾, ½ , 9
8 , 1
4
 ,
and 3
8 , 3
4
 . These points have an objective value of z ¼ 3 and thus are optimal as
well. However, none of them is an extreme point, and the simplex method will not
generate them.
Note that the fact that points that are not corner points of the feasible set are
optimal does not invalidate Dantzig’s corner point theorem. All the theorem states is
that at least one optimal solution is at an extreme point. This allows for the
possibility of non-extreme points being optimal, but only if there is another optimal
extreme point as well.
84 2 Linear Programming4. Redundancy.
Redundancy is an issue that relates to individual constraints. In particular, a
constraint is said to be redundant, if it can be deleted from the set of constraints
without changing the feasible set. A more detailed definition will distinguish
between constraints that exhibit strong redundancy and those that are weakly
redundant. While a weakly redundant constraint belongs to a constraint whose
hyperplane shares at least one point with the feasible set, this is not the case for
strongly redundant constraints.
As an illustration, consider the following numerical example (without an objec￾tive function, as it does not play a role in the discussion of redundancy).
P : 3x1 þ x2  8 ðIÞ
x1  2 ðIIÞ
x1  x2  3 ðIIIÞ
x1 þ 2x2  6 ðIVÞ
x1, x2  0:
Figure 2.13 depicts the feasible set of this problem.
Constraint III is obviously redundant as its removal does not change the shaded
area that symbolizes the feasible set. As a matter of fact, it is strongly redundant as its
bordering hyperplane does not touch the feasible set at any point. In contrast,
consider constraint I. It is also redundant (but just about so), so that its removal
likewise does not change the feasible set. However, it is only weakly redundant, as it
touches the feasible set at the point (2, 2). Had constraint I be instead 3x1 + x2  8.1,
it would have been strongly redundant, had it been 3x1 + x2  7.95, it would not have
been redundant at all, as it would have cut off the point (2, 2) and thus shaped the
feasible set.
Fig. 2.13 Redundancy and
primal degeneracy
2.3 Graphical Representation and Solution 85By their very definition, redundant constraints can be deleted from the problem
formulation without changing anything. The problem is that there is no easy general
way to recognize whether or not a constraint is redundant. Again, the graphical
representation is deceiving as in it, redundancy can easily be detected. However, if
all we have is a set of constraints that we cannot plot, detection of redundancy is a
different, and much more complicated, matter. As a matter of fact, determining
whether or not a constraint is redundant is as difficult as solving the existing linear
programming in the first place. And this is why we can, and pretty much have to,
ignore the issue and have the solver compute optimal solutions, regardless of
whether or not redundant constraints exist in the formulation.
5. Primal Degeneracy.
While the issue of primal degeneracy is of very little, if any, practical concern, it
is a very important theoretical matter. If primal degeneracy is not properly taken care
of, the simplex method may “cycle,” meaning that it generates the same points over
and over again without ever reaching an optimal solution. In graphical terms, primal
degeneracy occurs in two dimensions, if more than two planes of constraint intersect
at a single point. In Fig. 2.13, the point (2, 2) exhibits degeneracy. As in two
dimensions, the intersection of any two straight lines uniquely determines a point,
we can think of points with primal degeneracy as “overdetermined.” In
n dimensions, primal degeneracy occurs if the hyperplanes of more than
n constraints intersect at one point. Any modern code will include a module that
deals with degeneracy, so that this is of no concern to users.
Exercises
Problem 1 (graphing constraints and objective, graphical solution method):
Consider the following linear programming problem.
Max z ¼ x1 þ x2
s:t: 5x1 þ 2x2  10 ðIÞ
x2  1 ðIIÞ
3x1 þ 5x2  15 ðIIIÞ
x1, x2  0:
(a) Graph the constraints, determine the feasible set, and use the graphical solution
method to determine the optimal solution point. Which constraints are binding at
optimum? Compute the exact coordinates at optimum and calculate the value of
the objective function at optimum.
86 2 Linear Programming(b) What if the objective were Min z ¼ 3x1  x2? Plot the new objective, use the
graphical solution method, determine the optimal solution point, its coordinates,
and the value of the objective function at optimum.
(c) Repeat (b) with the objective function Min z ¼ x1  2x2.
Solution:
The objective functions for (a), (b), and (c) are shown in Fig. 2.14. The optimal
solutions are indicated by the points xa ,xb , and xc , respectively. The exact
coordinates are xa ¼ ½  1:0526, 2:3684 with za ¼ 3:4211 , xb ¼ ½  1:6, 1 with zb ¼
5:8, and xc ¼ ½  0, 3 with zc ¼ 6. The binding constraints at the three points are
I and III, I and II, and III and the nonnegativity constraint x1  0, respectively.
Problem 2 (graphing constraints and objective, graphical solution method):
Consider the following linear programming problem.
Max z ¼ 3x1 þ x2
s:t: 6x1 þ 5x2  30 ðIÞ
x1 þ 2x2  4 ðIIÞ
x2  5 ðIIIÞ
x1, x2  0:
Fig. 2.14 Graph for
Problem 1
2.3 Graphical Representation and Solution 87(a) Graph the constraints, determine the feasible set, and use the graphical solution
method to determine the optimal solution point. Which constraints are binding at
optimum? Compute the exact coordinates at optimum and calculate the value of
the objective function at optimum.
(b) Repeat (a) with the objective function Min z ¼ x2.
(c) What happens if constraint I were of the type “” instead?
Solution:
(a) The feasible set is the shaded area in Fig. 2.15, and the optimal solution point is
xa ¼ ½  6, 5 with za ¼ 23. Constraints II and III are binding at optimum.
(b) The objective leads “straight down” as shown in the above figure. The optimal
solution point is xb ¼ ½  2:3529, 3:1765 with the objective value zb ¼ 3:1765. At
this point, the constraints I and II are binding.
(c) The feasible set would no longer be the shaded region, but the quadrilateral
indicated by (c). Given the objective in (a), the optimal solution is again xb.
Problem 3 (Dual degeneracy at optimum, alternative optimal solutions):
Consider the following linear programming problem:
P : Min z ¼ x1 þ x2
s:t: x1  4 ð ÞI
x1 þ x2  7 ð Þ II
x1, x2  0: ð Þ III and ð Þ IV
Fig. 2.15 Graph for Problem 2
88 2 Linear Programming(a) Graph the feasible set and the gradient of the objective function. Which, if any,
of the five special cases does apply?
(b) Consider the same problem but change constraint 2 from x1 + x2  7 to x1 + x2
 7. What is the feasible set now? Which, if any, of the five special cases does
apply?
Solution:
(a) The feasible set is shown as the lightly colored area in Fig. 2.16. The neighbor￾ing solutions x ¼ (4, 3) and x ¼ (0, 7) both have the objective value z ¼ 7. The
(along with all points on the line segment between them) are optimal. In other
words, we have dual degeneracy at optimum.
(b) The feasible set is now the dark gray section in Fig. 2.16. We still have dual
degeneracy, but no longer at optimum, which is now at x ¼ ð Þ 0, 0 .
Problem 4 (Primal degeneracy): Consider the following linear programming
problem:
P : Min z ¼ 3x1 þ x2
s:t: x1  5 ð ÞI
x1 þ x2  5 ð Þ II
x1, x2  0: ð Þ III and ð Þ IV
Graph the feasible set and the gradient of the objective function. Which, if any, of
the five special cases does apply?
Solution: The feasible set and the objective function of the problem are shown in
Fig. 2.17. It is apparent that there is primal degeneracy at the point x ¼ (5, 0), where
Fig. 2.16 Dual degeneracy
and alternative optimal
solutions
2.3 Graphical Representation and Solution 89the three hyperplanes I, II, and IV intersect. Note that constraint IV is also weakly
redundant.
2.4 Postoptimality Analyses
This section will investigate what can be thought of as the third phase in linear
programming. In particular, it asks: “What happens, if...?” In simple words, we will
examine what happens, if there is some change in some of the components of the
linear programming problem that was formulated and solved earlier. The first
subsection will explore what it means graphically when we make the proposed
change, while the second subsection puts this knowledge to work and examines
what managerial consequences the anticipated changes will have.
2.4.1 Graphical Sensitivity Analyses
Recall that one of the main assumptions of linear programming is the deterministic
property. In other words, we assume that the structure of the problem and all of the
parameters of the problem are assumed with certainty. In virtually all realistic cases,
this is a troubling assumption: prices may or may not be known in advance, demand
may be uncertain, machine capacities (due to unforeseen breakdowns), employees
may call in sick, thus changing the availability of manpower, and so forth. How then
can we justify using linear programming at all?
Using postoptimality analyses can be seen as a “trick” that allows us to get away
with assuming that the deterministic property holds, while it actually may not. As an
example, suppose that we are facing an uncertain demand, which, as past experience
indicates, ranges from, say, 80 to 130 units. Furthermore, suppose that most of the
time the demand is somewhere about 110 units. The idea is now to temporarily fix
the demand at the level of 110 and solve the problem. Once this has been done, we
perform sensitivity analyses by asking: “What happens, if the demand were to
increase (from 110) by 10 (to 120)?” “What if it increases by 20 (to 130)?” “What
Fig. 2.17 Primal degeneracy
90 2 Linear Programmingif it decreases by 10, 20, or 30 units?” Information of this type can be obtained either
by setting the demand to 120, 130, or any of the other values the decision maker is
interested in and actually resolving the problem, or by gleaning the information from
the printout. This information will then indicate how sensitive the solution is to
changes in the input parameters. While some problems are not sensitive at all with
respect to some changes, meaning that even significant changes in the original
demand, prices, or other parameters do not result in major (or even any) changes
of the solution, others are very sensitive, so that even minor changes in the input
parameters change the solution a great deal. There is nothing we can do about it, but
it is very important information to the decision maker. If it is known that a model is
very sensitive to changes, the decision maker will have to be very cautious and
monitor the process closely, obtaining additional and updated information at many
steps along the way. This is not as necessary in problems that are rather insensitive to
changes.
We distinguish between two different types of sensitivity analyses. The first type
deals with structural changes, meaning the addition and deletion of variables and
constraints. Changes of that nature are major and often dealt with by resolving the
problem altogether. The second type of sensitivity analyses involves parameter
changes. In these cases, only some of the numbers in the model change. Typically,
we deal with ceteris paribus changes, i.e., we determine what happens if one number
changes, while all other parameters remain unchanged. The advantage of such an
analysis is that it separates the different changes and analyzes their effects. If we
were to analyze simultaneous changes of a number of parameters, we would not be
able to easily specify what actually causes the observed effect, e.g., the increase or
decrease in the total costs.
We will first look at the changes that may occur when we either add or delete a
variable or a constraint. The addition of variables and constraints is an issue during
the modeling process when a model is built. Furthermore, it may occur after the
problem has been solved when opportunities and requirements are added to the
problem as time passes. Similarly, it may happen that over time some activities or
possibilities no longer exist, or constraints have become obsolete. Whenever possi￾ble, we will deal with these cases on an intuitive level, which is pretty much all that
can be done short of resolving the problem.
Consider the addition of a variable. In graphical terms, the addition of a variable
corresponds to the increase of the dimensionality of the problem. It is useful to
understand a variable as an opportunity, i.e., an additional activity that we may or
may not undertake. Having the additional variable allows us to choose the level at
which we engage in the activity (i.e., the value the variable assumes), while not
including the variable in the model is the same as setting its activity level or value
equal to zero. In this sense, adding a variable is the same as adding an opportunity.
Doing so allows us to increase the level of the new possible activity from a zero level
to any value within the constraints. The immediate conclusion is that the addition of
a variable can never result in a deterioration of the objective function value
(a decrease in maximization functions or an increase in minimization functions),
but possibly an improvement. Formally, defining the original problem as Porig with
2.4 Postoptimality Analyses 91its optimal objective value zorig and the problem with the added variables as Paddvar
and its optimal objective value as zaddvar, we know that zorig  zaddvar in problems
with maximization objective and zorig  zaddvar in problems with minimization
objective. Along similar lines, if Porig has unbounded “optimal” solutions, then so
does the modified problem Paddvar. On the other hand, if the original problem Porig
has no feasible solution, the added variable may or may not allow Paddvar to have
feasible solutions.
There is little that can be said in general beyond this. Often, if variables are added
in the modeling process, it may be useful not to start solving the new problem from
scratch but to start with the previous optimal solution, which usually only requires a
few additional steps to reach the optimal solution of the new problem. However,
with the advances of optimization software, it often takes only seconds to resolve a
problem, so that such considerations are no longer as important as they used to be.
The deletion of a variable can be discussed in analogous fashion. Again, let the
original problem be Porig, while the formulation without the now deleted variable is
Pdelvar. The objective function values of the two problems are defined as zorig
andzdelvar, respectively. Deleting a variable is now equivalent to deleting an oppor￾tunity, or, more formally, forcing the value of a variable to zero. If the value of the
variable that is to be deleted equaled zero in the optimal solution of Porig, then the
variable can be deleted without any change of the solution and zorig ¼ zdelvar in both,
maximization and minimization problems. The main reason for this result is that we
are no longer allowing an activity that we did not engage in in the first place. On the
other hand, if the value of the variable that is to be deleted was positive in the original
problem Porig, the deletion of this variable will deprive us from a worthwhile
activity, which lets the objective value deteriorate. In other words, we have zorig 
zdelvar for maximization and zorig  zdelvar for minimization problems. It is also
straightforward that if Porig is unbounded, then Pdelvar may or may not be unbounded
as well. Furthermore, if Porig has no feasible solution, then neither does Pdelvar.
The addition of a constraint is an issue that we can much more easily analyze, as
it allows us to visualize the situation in a graph as the dimensionality of the problem
does not change. Again, we restrict ourselves to small problems with only two
variables, but the conclusions are valid for any general formulation. Consider
again some arbitrary original maximization problem Porig and assume that a con￾straint is added, resulting in Paddcon. By definition, adding a constraint means that the
resulting problem Paddcon is more restricted, so that its objective value zaddcon  zorig
in maximization problems and zaddcon  zorig in minimization problems. More
specifically, if the new constraint is satisfied by the optimal solution of Porig, then
this solution will also be optimal for the new problem Paddcon. On the other hand, if
the old optimal solution violates the new constraint, then the optimal solution of
Paddcon is different from the optimal solution of Porig and the objective value will be
the same (in case of alternative optimal solutions) or be worse than before. Further￾more, if the original problem has unbounded “optimal” solutions, then the problem
with the new constraint may or may not have bounded optimal solutions. If the
92 2 Linear Programmingoriginal problem has no feasible solutions, then adding a constraint, which makes the
problem even more constrained, will consequently not result in feasible solutions.
Finally, consider the deletion of a constraint. Again, assume that the problem
Porig has been solved, resulting in the objective value zorig. The problem without one
or more of the constraints will be referred to as Pdelcon and its optimal value of the
objective function is zdelcon . It is apparent that the new problem Pdelcon is less
restricted than the original problem Porig, so that zdelcon  zorig holds for maximiza￾tion problems, while zdelcon  zorig holds for minimization problems. As in the case
of constraint additions, we can distinguish between two cases: either the constraint
that is deleted was binding at optimum before it was deleted, or it was not. In case it
was binding (and not weakly redundant), then it was, generally speaking, a con￾straint that held back the solution and with its removal, better solutions may exist. On
the other hand, if the constraint was not binding, then it did not restrict the solution in
the original problem, so that its removal cannot result in better solutions. Again, if
unbounded “optimal” solutions existed in Porig, then the removal of a constraint
cannot change that regardless if it is binding or not. If Porig did not have feasible
solutions, the deletion of a constraint may or may not result in the problem Pdelcon
having feasible solutions.
Next consider parameter changes. In order to classify such changes, consider the
following simple linear programming problem
P : Max z ¼ 5x1 þ 6x2
s:t: x1  2x2  2
3x1 þ 4x2  12
x1, x2  0:
This model, as well as any other linear programming problem, includes three
different types of parameters. The first are the objective function coefficients (typi￾cally denoted by c1, c2, ... with the subscript indicating the variable the coefficient is
associated with), which are the numbers found in the objective function (here the
numbers c1 ¼ 5 and c2 ¼ 6). Depending on the application, they may be referred to
as unit profits, cost coefficients, or similar names. The second type of parameter are
the right-hand side values (which we usually denote by b1, b2, ... with the subscript
indicating the number of the constraint we are dealing with). In our example, these
are the values b1 ¼ 2 and b2 ¼ 12. Again, depending on the specific applications, the
right-hand side values may be referred to as resource availabilities, demands,
inventory levels, or similar names. Finally, there are the left-hand side coefficients,
which sometimes are called technological coefficients a11, a12, ..., a21, a22, ... with
the first subscript denoting the number of the row or constraint and the second
subscript standing for the number of the column or variable. In our example, the
technological coefficients are a11 ¼ 1, a12 ¼ 2, a21 ¼ 3, and a22 ¼ 4. Depending on
the application, these values may symbolize the processing times of a product on a
machine, the content of nutrients of a food item, the interest rate of a specific
investment, or similar values. In this book, we will investigate changes of the
2.4 Postoptimality Analyses 93objective function coefficients and changes of the right-hand side values. For
changes of the left-hand side parameters, we suggest to simply resolve the problem.
First, consider changes of the objective function coefficients. In order to explore
what these parameter changes cause, we first look only at the objective function and
ignore the constraints. To facilitate our arguments, consider the objective function
Max z ¼ 3x1 + 2x2. The gradient of the objective is shown in Fig. 2.18 and is labeled
(3, 2). Suppose now that we want to examine changes of c1, the value associated with
the variable x1. If this number, whose original value is “3,” decreases to, say, “2,” the
gradient tilts in a counterclockwise direction to a position shown as (2, 2). If, on the
other hand, c1 were to increase to, say, “4,” then the gradient will tilt in a clockwise
direction to the position shown as (4, 2). If c1 were to further increase to a value of
“5,” the gradient further tilts in a clockwise direction to the position shown as (5, 2).
We see that the increase of an objective function coefficient in a maximization
function symbolizes the fact that the activity that corresponds to its associated
variable has become more attractive or profitable. Thus, the gradient of the objective
function is drawn more into that direction. In our example, we see that as c1 increases
from 3 to 4 and then to 5, the gradient is pulled further and further into the x1
direction, i.e., to the right.
We note that since all the changes occur in the objective function, the feasible set
will remain unaffected by these changes. This means that if there are no feasible
solutions before the change, then there will be no feasible solutions after the change.
On the other hand, the case of unbounded “optimal” solutions is different, as it does
depend not only on the feasible set, but also on the gradient of the objective function.
As a simple example, consider the following linear programming problem.
P : Max z ¼ 2x1 þ 1x2
s:t: x1  x2  2
x1, x2  0:
This problem is shown in Fig. 2.19, in which the gradient of the objective
function is labeled by its coefficient as (2, 1). Clearly, there are unbounded “optimal”
Fig. 2.18 Changing the
coefficients of the objective
function
94 2 Linear Programmingsolutions to the problem. However, if c2 decreases from its present value of “1” to
less than “2” [the gradient is shown as (2, 2)], the problem has a unique finite
optimal solution at x1 ¼ 2 and x2 ¼ 0 with an objective function value of z ¼ 4,
clearly a finite value.
We are now able to incorporate constraints in our discussion. In order to do so,
consider the following linear programming problem:
P : Max z ¼ 1x1 þ 2x2
s:t: x2  3
3x1 þ 2x2  11
x1  x2  2
x1, x2  0:
The problem can be visualized in the graph in Fig. 2.20.
Using the graphical solution technique (or any commercial solver), we determine
that the point B is the unique optimal solution with coordinates ð Þ¼ x1, x2 12=3 ð Þ , 3 .
Suppose now that we want to examine the sensitivity of the solution with respect to
c2, the objective function coefficient of x2. If we were to increase the value that
presently equals 2 to some higher value, our previous discussion indicates that the
gradient of the objective function tilts in a counterclockwise direction. This does not,
however, have any effect on the solution, which stays at point B. As a matter of fact,
no finite value of c2, regardless of how large, will change the solution, which remains
at point B (meaning that the solution is very insensitive to increases of this
Fig. 2.19 Unbounded
“optimal” solutions
2.4 Postoptimality Analyses 95coefficient). Clearly, since x2 ¼ 3 at optimum, the value of the objective function
will change as the activity that the variable x2 symbolizes becomes more and more
valuable.
Consider now a decrease of c2. Graphically, this means that the gradient of the
objective function tilts in a clockwise direction. For small changes, the optimal
solution remains at point B. However, once c2 reaches the value of 2=3, point B is still
optimal, but so is point C with coordinates (x1, x2) ¼ (3, 1) (and all non-extreme
points on the line segment between these two points). This is clearly a case of dual
degeneracy at optimum, i.e., alternative optimal solutions. Once c2 decreases below
the value of 2=3, point C is the unique optimal solution. Point C remains optimal until
c2 reaches the value of 1. At this point, points C and D are both optimal, again a
case of alternative optimal solutions. If c2 drops below 1, point D with coordinates
(x1, x2) ¼ (2, 0) remains optimal, regardless how small the coefficient is.
We summarize the effects of the changes of c2 in Table 2.22.
Similar analyses can be performed for each of the given objective function
coefficients. Without further comments, Table 2.23 shows different ranges of c1
and the resulting optimal solution points, their coordinates, and their values of the
objective function.
So far, we have only looked at the effects of individual changes on the optimal
solution. There is, however, an interesting result that considers the effects of
Fig. 2.20 Graphical solution
technique
Table 2.22 Optimal solutions ð Þ x1, x2 and objective values z for different values of c2
Range of c2 ]1, 1[ 1 ]1, 2=3[ 2=3 ]2=3, +1[
Optimal solution point D D and CC C and B B
Optimal coordinates
ð Þ x1, x2
(2, 0) (2, 0) and
(3, 1)
(3, 1) (3, 1) and
(12=3, 3)
(12=3, 3)
Optimal objective value z 2 2 3+ c2 32=3 12=3 + 3c2
96 2 Linear Programmingsimultaneous changes of several objective function coefficients. The rule is called
the 100 percent rule and it can be stated as follows.
100% Rule As long as the sum of the absolute values of the percentage increases or
decreases of the objective function coefficients is no more than 100%, the optimal
solution point remains optimal.
Formally, we denote the largest allowable increase of an objective function
coefficient cj by Δc j, while the largest allowable decrease of an objective function
coefficient cj is denoted by Δc j. In our example, the optimal solution for the original
objective function Max z ¼ 1x1 + 2x2 was ð Þ¼ x1, x2 12=3 ð Þ , 3 . As shown in
Table 2.23, this solution remains optimal as long as c1 (whose original value is
c1 ¼ 1) does not increase by more than Δc1 ¼ 2 to the upper limit of the range at
c1 ¼ 3. Similarly, the solution remains optimal as long as c1 does not decrease by
more than Δc1 ¼ 1 to the lower end of the range at c1 ¼ 0. Similarly, we obtain the
values Δc2 ¼ þ1 and Δc2 ¼ 11=3. Suppose now that we want to investigate the
effect of a simultaneous increase of the value of c1 by Δc1 and a decrease of the value
of c2 by Δc2, the 100% rule then states that the optimal solution ð Þ¼ x1, x2 12=3 ð Þ , 3
remains optimal, as long as the sum of actual increases in relation to their respective
values Δc j plus the sum of actual decreases in relation to their respective values Δc j
does not exceed 100% ¼ 1. In our example, we obtain jΔc1j
Δc1
þ jΔc2j
Δc2 ¼ jΔc1j
2 þ jΔc2j
11
3
 1.
For instance, if we were to face a simultaneous increase of c1 by ½ and a decrease of
c2 by ½, the condition tells us that ½
2 þ ½
11
3
¼ 1=4 þ 3=8 ¼ 5
8 < 1, so that the optimal
solution remains optimal. On the other hand, if the increase of c1 were ¾ and the
decrease of c2 were 1, then we would have 3=4
2 þ 1
11
3
¼ 3
8 þ 3
4 ¼ 9
8 > 1, so that the
optimal solution will change.
A similar argument can be applied to a simultaneous decrease of c1 and increase
of c2, or simultaneous increases or decreases of both cost coefficients. Increasing
both cost coefficients simultaneously presents an interesting special case. As there is
no finite upper bound on c2, the 100% rule reduces to the regular limit on c1 and no
limit on c2.
Consider now changes of a single right-hand side value bi. Again, we will first
examine the effects such a change on the constraint itself before investigating what
Table 2.23 Optimal solutions ð Þ x1, x2 and objective values z for different values of c1
Range of c1 ]1, 0[ 0 ]0, 3[ 3 ]3, 1[
Optimal solution point A A and BB B and C C
Optimal coordinates
ð Þ x1, x2
(0, 3) (0, 3) and
(12=3, 3)
(12=3, 3) (12=3, 3) and
(3, 1)
(3, 1)
Optimal objective value z 6 6 6+12=3c1 11 2 + 3c1
2.4 Postoptimality Analyses 97happens to optimal solutions. As a numerical example, consider the constraint 2x1
+ 3x2  6. The resulting hyperplane and halfspace are shown in Fig. 2.21, labeled as
b1 ¼ 6.
If we were to modify the right-hand side value to, say, b1 ¼ 9, the hyperplane and
halfspace would shift in parallel fashion to the position shown in Fig. 2.21 by b1 ¼ 9.
A further increase to b1 ¼ 12 is shown in the figure as well. Decreases of the right￾hand side value in our example would again result in a hyperplane and/or halfspace
shifting in parallel fashion, but now in a southwesterly direction.
Given this result, we are now able to examine changes of right-hand side values in
a linear programming problem. Before doing so, we note that such changes do in no
way affect the objective function, but they may change the feasible set.
As an illustration, consider again the numerical example that was used to discuss
changes of the objective function. For convenience, we restate the model here.
P : Max z ¼ 1x1 þ 2x2
s:t: x2  3 ðIÞ
3x1 þ 2x2  11 ðIIÞ
x1  x2  2 ðIIIÞ
x1, x2  0:
Figure 2.22 shows the feasible set (the shaded area) and the gradient of the
objective function. The extreme points of the feasible set are 0, A, B, C, and D.
The optimal solution is again at the point B with coordinates ð Þ¼ x1, x2 12=3 ð Þ , 3 and
value of the objective function z ¼ 72=3
Consider now changes of the second right-hand side value b2. If b2 increases from
its present value of 11, then the hyperplane and halfspace of this constraint will shift
into a northeasterly direction. For instance, if b2 ¼ 15, the feasible set is now
Fig. 2.21 Changing a right￾hand side value
98 2 Linear Programmingenlarged by the set with the extreme points B, C, B1
, and C1
. This has a direct effect
on the optimal solution point, which has moved from its original position at point
B to its new position at point B1
. Note that while the optimal coordinates of the
optimal point have changed, one thing has not: the point is still determined by the
intersection of the hyperplanes that belong to constraints I and II, the latter now with
its new right-hand side value. Such a point is referred to as a basis point or simply a
basis. We observe here that while the basis has not changed, the optimal solution has
moved to a new location, albeit still at the intersection of hyperplanes I and II. A
further increase of b2 has the effect of changing the feasible set to 0, A, B2
, C2
, and D,
with an optimal solution at point B2
. Again, the basis has not changed (the point is
still determined as the intersection of hyperplanes I and II), but its location has
moved further to the right. A further increase to b2 ¼ 21 results in a feasible set with
extreme points 0, A, B3 ¼ C3
, and D. The optimal solution is now at points B3 ¼ C3
.
We observe that at this point, hyperplanes I, II, and III now intersect at the same
point, causing primal degeneracy. Any further increase of b2 will not change the
feasible set any further, constraint II is now redundant.
Return now to the original value of b2 ¼ 11 and slowly decrease this value. For
instance, for b2 ¼ 9 the feasible set has shrunk to 0, A, B4
, C4
, and D with an optimal
solution at B4
. A further decrease to b2 ¼ 6 results in the feasible set with extreme
points 0, A ¼ B5
, and D ¼ C5
, indicating that primal degeneracy now occurs at
A ¼ B5
, and D ¼ C5
. The optimal solution is now at A ¼ B5
. A further decrease to
b2 ¼ 3 results in a feasible set with extreme points 0, A6
, and D6 with an optimal
solution at A6
. A further decrease to b2 ¼ 0 causes the feasible set to consist of only
the origin, which, as it is the only feasible point, is now also optimal. Any further
decrease of b2 will cause the problem not to have any feasible solutions. Our results
are summarized in Table 2.25.
Fig. 2.22 Changing optimal points based on different right-hand side values
2.4 Postoptimality Analyses 99In general, our discussion has revealed that reducing a right-hand side value for a
less-than-or-equal constraint from +1 to 1 causes the constraint to be redundant
at first, then essential (shaping the feasible set, but possibly not changing the optimal
solution), binding (shaping the feasible set and any change of the right-hand side
value, regardless how small, changes the optimal solution), to so strong so as to
cause the nonexistence of feasible solutions. Not each case has to go through all of
these phases. For instance, changing the value of b2 will result in the constraint first
being redundant, then essential and binding, and then infeasible.
The ranges for changes of b1 and b3 can be similarly obtained and are shown in
Tables 2.24 and 2.25, respectively. We must note, though, that the interpretation of
these intervals is different from that in the case of changes in the objective function.
While a range of cj of, say, [3, 7] indicates that as long as cj is anywhere between
3 and 7, the optimal solution will not change, the same interval for bi will indicate
that as long as bi is within this range, the optimal basis will not change. In other
words, for all changes within this range, the same constraints will be binding at
optimum. The optimal solution point will however change. This makes the ranges
for right-hand side values more difficult to interpret for managers.
Finally in this section, we want to apply the 100% rule to right-hand side changes.
The rule itself is the same as that used for changes of objective function coefficients.
Here, with right-hand side values of b1 ¼ 3, b2 ¼ 11, and b3 ¼ 2, we have obtained
intervals of [1, 5½], [6, 21], and [11=3, 1[, see Tables 2.24, 2.25, and 2.26. In other
words, we have Δb1 ¼ 2, Δb1 ¼ 2½, Δb2 ¼ 5, Δb2 ¼ 10, Δb3 ¼ 3 1
3
, and Δb3 ¼ 1
Table 2.25 Optimal solution and objective values of sample problem for various values of b2
Range of b2 ]–1, 0[ 0 ]0, 6[ 6 ]6, 21[ 21 ]21, 1[
Optimal
solution
point
There
exists
no
feasible
solution
0 A7
, ..., A A ¼ B5 B5
, B4
, B, B1
,
B2
, B3
B3 ¼ C3 B3 ¼ C3
Optimal
coordinates
ð Þ x1, x2
(0, 0) (0, 1=2b2) (0, 3) (1=3b2  2, 3) (5, 3) (5, 3)
Optimal
objective
value z
0 b2 6 4 þ 1=3b2 11 11
Table 2.24 Optimal solution and objective values of sample problem for various values of b1
Range of
b1 ]–1, 0[ 0 ]0, 1[ 1 ]1, 5½ [ 51=2 ]51=2, 1[
Optimal
coordinates
There
exists no
feasible
solution
(2, 0) (2+b1, b1) (3, 1) (2=3b1 þ 32=3, b1) (0, 51=2) (0, 51=2)
Optimal
objective
value z
2 2+3b1 5 71=3  1=3b1 11 11
100 2 Linear ProgrammingWith anticipated changes of the right-hand side values from their present values to
b1 ¼ 4½, b2 ¼ 7, and b3 unchanged at 2, we obtain Δb1 ¼ 1½, Δb2 ¼ 4, and
Δb3 ¼ 0. We can then compute jΔb1j
Δb1
þ jΔb2j
Δb2 ¼ 1½
2½ þ 4
5 ¼ 7
5 > 1, so that these changes
violate the 100% rule and will not only cause the solution to change, but also the
basis, i.e., different constraints will be binding after the change.
As a different example, assume that b1 decreases from its present value of 3 to 2½
(meaning that Δb1 ¼ ½), b2 increases by Δb2 ¼ 2 to its new value of b2 ¼ 13, and
the third right-hand side value decreases by Δb3 ¼ 1 from its present value of 2 to
its new value of b3 ¼ 1. The condition is then jΔb1j
Δb1 þ jΔb2j
Δb2
þ jΔb3j
Δb3 ¼ ½
2 þ 2
10 þ 1
10=3 ¼
3
4 < 1, satisfying the 100% rule, so that after this change, the same constraints will be
binding. Again, the solution may (and most likely will) change.
2.4.2 Optimal Solution and Sensitivity Analyses on a Printout
In this section, we will first provide a simulated printout that is typical for what
analysts obtain from a computer upon solving a linear programming problem. We
then explain the different features and the information provided by the printout. In
the second part of this section, we use a linear programming problem, provide the
printout, and answer a number of questions relevant to the decision maker.
In order to do so, we consider the following linear programming problem:
P : Max z ¼ 3x1  x2
s:t: x1  x2 ¼ 2
2x1 þ 3 x2  16
5x1 þ x2  15
x1, x2  0:
The standard printout for the problem is shown in Table 2.27, while information
concerning sensitivity analyses is provided in Table 2.28.
The Summary of Results shown in Table 2.27 subdivides in three parts. The first
part shows the optimal value of the objective function, in our example z ¼ 10:8. The
second part shows the optimal values of the decision variables as well as their
opportunity costs. In our examples, the information provided indicates that at
optimum, x1 ¼ 4:4 and x2 ¼ 2:4 . The opportunity cost (sometimes also called
Table 2.26 Optimal solution and objective values of sample problem to various values of b3
Range of b3 ]1, 3[ 3 ] 3, 12=3[ 11=3 ]11=3, 1[
Optimal coordinates
ð Þ x1, x2
There exists
no feasible
solution
(0, 3) (3+b3, 3) (12=3, 3) (12=3, 3)
Optimal objective
value z
6 9+ b3 72=3 72=3
2.4 Postoptimality Analyses 101reduced cost) of a variable indicates how far the price or cost of a variable is away
from being included in the optimal solution. In our example, both opportunity costs
are zero, as both variables are included in the solution with a positive value. Suppose
now that there were a variable in a profit-maximizing problem, whose price is $5 and
whose opportunity costs are 3. Furthermore, suppose that the variable equals zero at
optimum, i.e., it is not included in the optimal solution with a positive value. The
main reason for the variable not being part of the optimal solution is that it is not
profitable enough. More specifically, its price is not high enough. The opportunity
cost now indicates that the lowest price at which the variable would be included in
the solution is its present price plus the opportunity cost, in our example $5 + $3
¼ $8. In other words, $8 is the lowest price at which we consider including the
variable in an optimal solution. The interpretation of opportunity cost in cost
minimization problems is similar. In those problems, it indicates by how much the
cost of a variable has to decrease, before it will become part of an optimal solution.
Consider now the third and last part of the Summary of Results that is headed by
the words “slack or excess variable.” As discussed earlier in this book, the solver has
automatically added a slack variable to the left-hand side of each  inequality and
subtracted an excess or surplus variable from the left-hand side of each  inequality.
The printout first provides the number of the constraint these variables are added
to, followed by a column that specifies the type of constraint (and with it the type of
additional variable) that was automatically added. Here, we use the abbreviations EQ
for an equation, LE for a “” constraint, and GE for a “” constraint. The optimal
values of the slack, excess, and artificial variables are provided in the next column. In
our example, they are 0, 0, and 9.4. These numbers indicate that the variable
Table 2.27 Summary of Results for sample problem
SUMMARY OF RESULTS
VALUE OF THE OBJECTIVE FUNCTION 10.8000
DECISION VALUE AT OPPORTUNITY
VARIABLE OPTIMUM COST
X1 4.4000 0.0000
X2 2.4000 0.0000
SLACK/EXCESS CONSTRAINT OPTIMAL SHADOW PRICE
VARIABLE TYPE VALUE
CONSTRAINT 1: EQ 0.0000 2.2000
CONSTRAINT 2: LE 0.0000 0.4000
CONSTRAINT 3: GE 9.4000 0.0000
102 2 Linear Programmingassociated with the first constraint has an optimal value of 0. This is obvious, as the
first constraint is an equation, which does not allow left- and right-hand sides to
differ. The second constraint is of type “,” meaning that the system automatically
added a slack variable to the left-hand side of the relation. The optimal value of this
slack variable is 0, indicating that the constraint is actually satisfied as an equation at
optimum, meaning that the constraint is binding at optimum. A constraint that is
binding at optimum constitutes a bottleneck of the problem. Bottlenecks require
special attention as any changes in the right-hand side value of such equations will
immediately result in changes of the optimal solution. Finally, the third constraint is
of type “” and thus had an excess variable subtracted from its left-hand side. The
optimal value of this excess variable equals 9.4, indicating that at optimum, the left￾hand side exceeded the right-hand side value by 9.4. This constraint does not
constitute a bottleneck, the constraint is not binding at optimum, so that smaller
changes of the third right-hand side value will leave the solution unchanged.
The last column in Table 2.27 shows the shadow prices associated with the
constraints. The shadow price associated with a constraint indicates the change of
the value of the objective function, given that the right-hand side value of that
Table 2.28 Sensitivity Analyses for sample problem
SENSITIVITY ANALYSES
COEFFICIENTS OF THE OBJECTIVE FUNCTION
VARIABLE LOWEST ORIGINAL HIGHEST 
ALLOWABLE VALUE ALLOWABLE
VALUE VALUE
X1 1.0000 3.0000 INFINITY
X2 –3.0000 –1.0000 INFINITY
RIGHT-HAND SIDE VALUES
CONSTRAINT LOWEST ORIGINAL HIGHEST
NUMBER ALLOWABLE VALUE ALLOWABLE
VALUE VALUE
CONSTRAINT 1 –1.6154 2.0000 8.0000
CONSTRAINT 2 8.1667 16.0000 INFINITY
CONSTRAINT 3 –INFINITY 15.0000 24.4000
2.4 Postoptimality Analyses 103constraint increases by one unit. It does not provide any information about how the
solution itself, i.e., the values of the variables, would change, though.
In our example, the shadow price of the first constraint (the equation) equals 2.2.
This means that as the first right-hand side value b1 increases from its original value
of 2 by one unit to 3, the value of the objective function increases from 10.8 by 2.2 to
13. Similarly, the increase of the second right-hand side value by one unit from its
original value of 16 to 17 will result in a new optimal objective value that is 0.4
higher than the original objective value of 10.8, i.e., z ¼ 11:2. Finally, the third
constraint has a shadow price of 0. This indicates that a change of the third right￾hand side value by one unit will neither change the optimal solution, nor the value of
the objective function at optimum. This is a result of the fact that in the present
solution, the third left-hand side already exceeds the right-hand side value by 9.4,
thus the constraint is not binding at optimum, so that smaller changes of the right￾hand side value will have no effect on the solution.
It is also possible to identify some of the “special cases” on the printout. In case
no feasible solution exists, some solvers will provide the analyst with the numbers of
the constraints that are incompatible and thus cause the infeasibility. Such a message
greatly simplifies the work of the analyst, who can then immediately attempt to
loosen up some of the constraints that are involved in the problem. In the case of
unbounded “optimal” solutions, a simple message to that effect will appear. Dual
degeneracy can be detected only if it occurs at optimum, as intermediate solutions
are not displayed. In this case, alternative optimal solutions exist, and they can be
detected on the printout, if a variable that is not included in the solution, i.e., it has a
zero value, has an opportunity cost of zero as well. The case of primal degeneracy
can also be detected if it occurs at the optimal solution. If it does, there will be strictly
less than m (the number of constraints, not counting the nonnegativity constraints)
variables (decision, slack, and excess variables) with a positive value.
Additional information is available when choosing the “Sensitivity Analyses”
option. The simulated printout for our example is shown in Table 2.28.
The Sensitivity Analyses option consists of two parts. The upper part headed by
Coefficients of the Objective Function analyzes changes of the objective function
coefficients cj, while the part headed by Right-Hand Side Values provides informa￾tion concerning changes of the right-hand side values bi.
In Table 2.28, the line headed by x1 specifies the original value of c1 ¼ 3, and also
the interval ¼ [1, 1[. This interval indicates that as long as the value of c1 is equal or
larger than 1, the optimal solution shown in Table 2.27 will remain optimal.
Similarly, we can interpret the numbers in the row headed by x2. The original
value of the objective function coefficient of this variable is c2 ¼ 1. The range
specified here is ¼ [3, 1[, and it indicates that as long as c2 is 3 or larger, the
solution shown in Table 2.27 will remain optimal.
Consider now the bottom part of Table 2.28. The row headed by “Constraint 1”
specifies the original right-hand side value of the first constraint (which was b1 ¼ 2),
as well as the interval [1.6154, 8.0000]. This interval indicates that as long as the
first right-hand side value remains between these bounds, the optimal basis will
remain unchanged. In other words, within this range, the same constraints remain
104 2 Linear Programmingbinding at optimum. The solution, however, may very well change, even within this
range. However, it is not possible to determine from the printout what these changes
will be. The remaining rows for the other two constraints are interpreted in a similar
fashion.
The remainder of this section will present a linear programming problem, formu￾late it, and then interpret the results shown in a printout. The production planning
model we will use throughout the remainder of this section is as follows.
Example: A footwear manufacturer is planning next year’s product line. Part of
that line are three types of hiking boots, called “Walker,” “Hiker,” and “Back￾packer.” Among the many raw materials used in the production are three particularly
important and costly materials: NOwater (a lining that waterproofs the boots),
Fabrinsula (fabric insulation for warmth), and Lugster (lug) soles. Currently,
10,000 sq. ft of NOwater are available at $20 per sq. ft. Similarly, the manufacturer
has access to up to 5000 oz of Fabrinsula at $15 per ounce and up to 7000 pairs of
Lugster soles at $10 per pair. The requirements for manufacturing the different types
of boots are given in Table 2.29.
Contracts have been signed for the delivery of at least 3000 pairs of “Walkers,” at
least 2000 pairs of “Hikers,” and at least 1000 pairs of “Backpackers.” Customers are
prepared to purchase additional quantities should they become available. The
agreed-upon prices are $40, $65, and $110 per pair of the respective types. (Note:
The unit profits in the objective function in the formulation below have been
computed as the price minus the costs for the required amounts of NOwater,
Fabrinsula, and Lugster, as will be explained below.)
In order to formulate the problem, we first define variables xj as the quantity of the
three types of boots that we made and sell. The formulation of the problem is then as
follows.
P : Max z ¼ 12:5x1 þ 25x2 þ 30x3
s:t:½x1 þ x2 þ 2½x3  10, 000 NOwater availability ð Þ
½x1 þ 2=3x2 þ 11=3x3  5, 000 Fabrinsula availability ð Þ
x1 þ x2 þ x3  7, 000 Lugster soles availability ð Þ
x1  3, 000 Walker requirement ð Þ
x2  2, 000 Hiker requirement ð Þ
x3  1, 000 Backpacker requirement ð Þ
x1, x2, x3  0:
Table 2.29 Data for shoe
production problem Input material
Product
Walker Hiker Backpacker
NOwater (sq. ft per pair) 1/2 1 21
/2
Fabrinsula (oz per pair) 1/2 2/3 4/3
Lugster (pair of soles) 1 1 1
2.4 Postoptimality Analyses 105Before we provide the printout and continue with our interpretations, some
comments regarding the formulation are in order. While the constraints are straight￾forward, the objective function coefficients have been obtained as follows. Consider
the “Walker” boots. They sell for $40 a pair, from which we have to deduct the costs
of ½ sq. ft of NOwater ($10), ½ oz of Fabrinsula ($7.50) and one pair of soles ($10),
leaving us with a per-unit profit of $12.50. This is the coefficient found in the above
formulation. The unit profits of the other two boots are computed in a similar
fashion.
Tables 2.30 and 2.31 provide the usual printouts with the sensitivity option.
The following is a simulated dialog between an analyst, who modeled the
problem and provides the information from the printouts and the decision maker,
who asks the questions that are of managerial interest.
Q1: How many pairs of the boots should we manufacture and what will be the
associated profit?
A1: The solution suggests that we make 3000 pairs of Walkers, 2750 pairs of Hikers, and
1250 pairs of Backpackers. Given this production plan, we can expect a profit of $143,750.
Table 2.30 Summary of Results for shoe production problem
SUMMARY OF RESULTS
VALUE OF THE OBJECTIVE FUNCTION 143,750.0000
DECISION VALUE AT OPPORTUNITY
VARIABLE OPTIMUM COST
WALKER 3,000.0000 0.0000
HIKER 2,750.0750 0.0000
BACKPACKER 1,250.0000 0.0000
SLACK/EXCESS CONSTRAINT OPTIMAL SHADOW PRICE
VARIABLE TYPE VALUE
NOWATER: LE 2,625.0000 0.0000
FABRINSULA: LE 0.0000 7.5000
LUGSTER: LE 0.0000 20.0000
WALKER: GE 0.0000 -11.2500
HIKER: GE 750.0000 0.0000
BACKPACKER: GE 250.0000 0.0000
106 2 Linear ProgrammingQ2: You are undoubtedly aware of the fact that NOwater, Fabrinsula, and Lugster
are critical resources. How many of these do we use in the suggested plan and
how much is left over?
A2: The printout tells me that we will have 2625 sq. ft of NOwater left over, while Fabrinsula
and Lugster soles are both completely used. The latter two are obvious bottlenecks in the
process. In other words, we are using 7375 sq. ft of NOwater, the complete supply of 5000 oz
of Fabrinsula, and all of the 7000 pairs of Lugster soles.
Q3: I just was informed by our marketing research group that the Hiker boots are
very popular and just about all of our customers would be prepared to pay an
additional $10 to get a pair of these. Would such a price hike change the optimal
solution?
A3 (aside to himself): Ahh, a sensitivity analysis on the objective function coefficient c. The
range within which the present optimal solution remains optimal is [16, 30]. A price
Table 2.31 Sensitivity Analyses for shoe production problem
SENSITIVITY ANALYSES
COEFFICIENTS OF THE OBJECTIVE FUNCTION
VARIABLE LOWEST ORIGINAL HIGHEST 
ALLOWABLE VALUE ALLOWABLE
VALUE VALUE
WALKER –INFINITY 12.50 23.75
HIKER 16.00 25.00 30.00
BACKPACKER 25.00 30.00 50.00
RIGHT-HAND SIDE VALUES
CONSTRAINT LOWEST ORIGINAL HIGHEST
ALLOWABLE VALUE ALLOWABLE
VALUE VALUE
NOWATER: 7,375.0000 10,000.00 INFINITY
FABRINSULA: 4,833.3333 5,000.00 5,500.00
LUGSTER: 6,625.00 7,000.00 7,250.00
WALKER: 2,000.00 3,000.00 3,600.00
HIKER: –INFINITY 2,000.00 2,750.00
BACKPACKER: –INFINITY 1,000.00 1,250.00
2.4 Postoptimality Analyses 107increase by $10 leads to a price of $35 rather than the original $25, which is not in the
interval, thus the solution will change.
(to the Decision Maker) If we hike the price by more than $5, our solution will change. If
you need more details, I have to resolve the problem with the new price.
Q4: No no, that’s OK. But how about this. We are presently making 3000 Walkers,
just enough to satisfy one of our requirements. I wonder if it would be worthwhile
to lower the price. Would that lead to increased sales?
A4 (to himself): Another sensitivity analysis on an objective function coefficient. Given the
present unit profit of $12.50 for a pair of Walkers, the Sensitivity Analyses part of the
printout indicates that as long as the unit profit remains in the interval ]−1, 23.75], the
optimal solution will not change.
(to the Decision maker): It does not matter by how much you decrease the price of
Walkers, we will not sell any more of them.
Q5: All right then. An interesting thing happened the other day. I met a salesman
who offered me an additional 100 oz of Fabrinsula for $8.50 per ounce. Should
we purchase that? You told me that we used up all the Fabrinsula that we have.
What if I squeeze the man a bit and get it for $5? Will we take it for that price?
And what will happen to our profit?
A5 (to himself): A sensitivity analysis on the second right-hand side value b2. The Summary
of Results tells me that the shadow price of Fabrinsula is $7.50, so we should not purchase it
for more than that. Also, the basis will not change if we buy up to 500 ounces of Fabrinsula.
(to the Decision Maker): No, don’t buy it for $8.50. As a matter of fact, you should not
pay more than $7.50 for an extra ounce of Fabrinsula. If you can get it for $5, you can buy
up to 500 oz of it. For each ounce that you get on top of what we have, our profit will
increase by $2.50. What happens beyond 500 extra ounces, though, requires additional
calculations.
Q6: Wonderful. I have also considered an alternative, though. In one of our trade
magazines, I just read an offer for Lugster Soles that we could get for $19 a pair.
Would you consider that?
A6 (to himself): The shadow price for Lugster Soles is $20, and the Sensitivity Analyses tell
me that this holds for an increase of up to 250 pairs.
(to the Decision Maker): If you can get a pair for $19, get them. They are worth $20 to
us, so for each extra pair, we make $1 in addition to our usual profit. You can get up to 250
pairs.
Q7: Thank you so much, Mr. Analyst. Your advice was very helpful. (Putters around
with his cell phone). Wait a minute―stop the presses! Our production manager
just texted me that 200 pairs of Lugster soles have been damaged in our
warehouse and can no longer be used. What are we going to do? How much is
that going to cost me?
A7 (to himself): The optimal basis remains unchanged as long as we have between 6625 and
7250 pairs of soles. So 200 pairs less will not change the basis, but it will change the
108 2 Linear Programmingsolution. And since the shadow price for Lugster Soles is $20, our profit will decrease by 200
(20) = $4000.
(to the Decision Maker): I have good news and bad news. The bad news is that our profit
will decrease by $4000, which is 2.78%. After solving the problem again with the new data, I
found that not all is bad. The good news is that we do not have to rearrange the production
plan altogether. We should still make 3000 pairs of Walkers, but we should make 400 pairs
less of the Hikers, and 200 pairs more of the Backpackers.
The Decision Maker: I knew it. Why are they doing this to me? (Disappears into
the bowels of the Administration Building).
Exercises
Problem 1 (a diet problem): A planner considers designing a diet that consists of
Coke, garlic fingers, spring rolls, pita pockets, and apples. As far as nutrients go, the
planner includes calories, riboflavin, and vitamin A, of which at most 1500, at least
98, and at least 27 must be included in the diet. The cost minimization problem was
subsequently formulated as follows:
Min z ¼ :9x1 þ 2:8x2 þ 3:2x3 þ 5:6x4 þ 3:6x5
s:t: 80x1 þ 310x2 þ 340x3 þ 460x4 þ 20x5  1500
2x1 þ 9x2 þ 25x3 þ 16x4 þ 5x5  98
5x2 þ 4x3 þ 3x4 þ 10x5  27
x1, x2, x3, x4, x5  0
The printout of the problem is shown in Table 2.32.
(a) What does the diet of the planner consist of? (Indicate type of food and
quantity).
(b) How much does the diet cost?
(c) How many calories does the diet include? How much riboflavin? How much
vitamin A?
(d) What is the highest price that will put Coke into the solution?
(e) What is the effect of increasing the riboflavin requirement from 98 to 99?
Solution:
(a) The diet consists of no Coke, no garlic fingers, 3.67 spring rolls, no pita pockets,
and 1.23 apples.
(b) The cost of the diet is $16.19.
(c) The diet includes 1500  226.26 ¼ 1273.74 calories, 98 + 0 ¼ 98 units of
riboflavin, and 27 + 0 ¼ 27 units of vitamin A.
2.4 Postoptimality Analyses 109(d) Presently, Coke costs 90¢ which is apparently too much for it to be included in
the solution. Its opportunity cost is 74.7¢, so that its price will have be decrease
by at least that amount. In other words, Coke will be included in the diet if its
price is no higher than 90  74.7 ¼ 15.3¢.
(e) The cost will increase by 7.65¢.
Problem 2 (an investment problem): An investment agency has been asked to
advise one of its clients how to invest all of his $100,000 among the four assets
shown in Table 2.33.
The client would like as high an annual return as is possible to receive while
incurring of an average of no more than 2.5 risk units per dollar invested. The
Table 2.32 Summary of Results for diet problem
SUMMARY OF RESULTS
VALUE OF THE OBJECTIVE FUNCTION 16.18609
DECISION VALUE AT OPPORTUNITY
VARIABLE OPTIMUM COST
COKE 0.0000 0.7470
GARLIC FINGERS 0.0000 0.5026
SPRING ROLLS 3.6739 0.0000
PITA POCKETS 0.0000 3.4104
APPLES 1.2304 0.0000
SLACK/EXCESS CONSTRAINT OPTIMAL SHADOW PRICE
VARIABLE TYPE VALUE
CALORIES: LE 226.2609 0.0000
RIBOFLAVIN: GE 0.0000 –0.0765
VITAMIN A: GE 0.0000 –0.3217
Table 2.33 Risk and rate of return for the sample problem
Assets Units of risk per Dollar invested Expected rate of return
Northern Mines Shares 4 .15
Bucklin Automobiles 3.5 .11
Royal Bank Shares 2 .07
EE Savings Bonds 1 .05
110 2 Linear Programmingamount invested in Royal Bank cannot exceed $40,000. Furthermore, the investment
in Automobiles and Banks combined must be at least $20,000.
Defining x1, x2, x3, and x4 for the amount invested in the four alternatives, the
problem can be formulated as follows.
P : Max z ¼ :15x1 þ :11x2 þ :07x3 þ :05x4
s:t: 1:5x1 þ x2  :5x3  1:5x4  0
x1 þ x2 þ x3 x4  100, 000
x3  40, 000
x2 þ x3  20, 000
x1, x2, x3, x4  0:
The printout is shown in Tables 2.34 and 2.35.
(a) How much is invested in each of the alternatives and what is the average rate of
return?
(b) Identify the bottlenecks in the investment plan.
(c) If we could borrow some additional funds at 11%, would it be worth our while?
Explain in one short sentence.
Table 2.34 Summary of Results for Problem 2
SUMMARY OF RESULTS
VALUE OF THE OBJECTIVE FUNCTION 9,733.333
DECISION VALUE AT OPPORTUNITY
VARIABLE OPTIMUM COST
MINES 43,333.33 0.0000
BUCKLIN 0.0000 0.01
ROYAL BANK 20,000.00 0.0000
SAVINGS 36,666.67 0.0000
SLACK/EXCESS CONSTRAINT OPTIMAL SHADOW PRICE
VARIABLE TYPE VALUE
RISK LE 0.0000 0.0333
INVESTMENT LE 0.0000 0.1000
BANK LIMIT LE 20,000.00 0.0000
BANK & AUTO GE 0.0000 –0.0133
2.4 Postoptimality Analyses 111(d) There is a rumor that the return of the Royal Bank will increase to 8%. Will this
change the investor’s plans? What if it decreases to 5.5%? Explain in one short
sentence.
Solution:
(a) The solution prescribes investments as follows: Mines: $43,333.33, Automo￾bile: $0, Bank: $20,000, and EE Savings: $36,666.66. The average rate of return
is 9.7333%.
(b) The risk constraint (constraint 1), the total investment level (constraint 2), and
the requirement to invest at least $20,000 in “Auto and Bank” (constraint 4) are
all tight.
(c) The shadow price of total investment (constraint 2) is 0.1, i.e., the benefit of an
extra dollar is 10¢. Given the interest rate of 11% on loans, it is not worthwhile
to borrow.
Table 2.35 Sensitivity Analyses for Problem 2
SENSITIVITY ANALYSES
COEFFICIENTS OF THE OBJECTIVE FUNCTION
VARIABLE LOWEST ORIGINAL HIGHEST 
ALLOWABLE VALUE ALLOWABLE
VALUE VALUE
MINES 0.1300 0.1500 INFINITY
BUCKLIN –INFINITY 0.1100 0.1200
ROYAL BANK 0.0600 0.0700 0.0833
SAVINGS 0.0300 0.0500 0.0700
RIGHT-HAND SIDE VALUES
CONSTRAINT LOWEST ORIGINAL HIGHEST
ALLOWABLE VALUE ALLOWABLE
VALUE VALUE
RISK –130,000.00 0.0000 110,000.00
INVESTMENT 26,666.67 100,000.00 INFINITY
BANK LIMIT 20,000.00 40,000.00 INFINITY
BANK & AUTO 0.00 20,000.00 40,000.00
112 2 Linear Programming(d) Sensitivity on objective function coefficients. The range for the bank shares
extends from 6% to 8.33%, so an increase from 7% to 8% will not change the
investment plan. A decrease to 5.5% will.
Problem 3 (a transportation problem): Consider a transportation problem with
two origins (warehouses) and three destinations (customers). The supplies at the
origins are 60 and 80 units, respectively, while the demand at the destinations is
exactly 30, 50, and 40, respectively. The problem has been formulated as follows:
P : Min z ¼ 3x11 þ 7x12 þ 4x13 þ 9x21 þ 2x22 þ 5x23
s:t: x11 þ x12 þ x13  60
x21 þ x22 þ x23  80
x11 þ x21 ¼ 30
x12 þ x22 ¼ 50
x13 þ x23 ¼ 40
x11, x12, x13, x21, x22, x23  0:
The printout is shown in Tables 2.36 and 2.37.
(a) What is the shipment plan and what are the associated costs?
(b) Which of the origins are fully used and which have still some units of the
product in them (and how many)?
(c) What if the per-unit-cost of a shipment from origin 2 to destination 3 were to
increase by $2, would that change the optimal solution? What if the cost were to
decrease by $2?
(d) What would happen if the number of units available at the first origin were to be
reduced by one unit? (Include cost considerations).
(e) What if we were offered extra units delivered to origin 2 at a rate of $2 per unit?
Solution:
(a) The shipments from the Origin 1 to the three destinations are 30, 0, and
30, while the shipments from Origin 2 to the three destinations are 0, 50, and
10. The total transportation costs are $360.
(b) All units in Origin 1 are shipped out, but 20 units are left over in Origin 2.
(c) The range for the unit transportation costs c23 is [4, 10]. An increase by $2 puts
the unit transportation cost at 5 + 2 ¼ $7, which is in the interval. As a result,
there will be no change in the optimal transportation plan, but the costs will
increase by 2x23 ¼ 20. A decrease of $2 would put the unit transportation cost at
5  2 ¼ 3, which is outside of the interval, so that the optimal transportation
plan (and its costs) will change.
(d) As the constraint that belongs to Origin 1 has zero slack, the solution will
change. As the shadow price is 1, the total cost will increase by $1.
2.4 Postoptimality Analyses 113(e) There are still 20 units left at Origin 2, so that no additional units are needed, and
we decline the offer.
2.5 Duality
This section explores some aspects of duality theory, the theory behind linear
programming that explores how and why solution methods work. Given the scope
of this book, we will restrict ourselves to some of the relations and interpretations,
without getting into technical details. For more details, interested readers are referred
to the standard advanced texts such as Dantzig (1963), Dantzig and Thapa (1997), or
Eiselt and Sandblom (2007).
To every linear programming problem, which we will call a primal problem, we
can now assign another linear programming problem, called the dual problem. The
variables in the primal problem are also referred to as the primal variables, while the
variables in the dual problem are the dual variables.
In order to simplify our discussion, we will base our arguments on a production
problem similar to that presented in Sect. 2.2.1 when we introduced linear
Table 2.36 Summary of Results for Problem 3
SUMMARY OF RESULTS
VALUE OF THE OBJECTIVE FUNCTION 360.0000
DECISION VALUE AT OPPORTUNITY
VARIABLE OPTIMUM COST
X11 30.0000 0.0000
X12 0.0000 6.0000
X13 30.0000 0.0000
X21 0.0000 5.0000
X22 50.0000 0.0000
X23 10.0000 0.0000
SLACK/EXCESS CONSTRAINT OPTIMAL SHADOW PRICE
VARIABLE TYPE VALUE
ORIGIN 1 LE 0.0000 1.0000
ORIGIN 2 LE 20.0000 0.0000
DESTINATION 1 EQ 0.0000 -4.0000
DESTINATION 2 EQ 0.0000 -2.0000
DESTINATION 3 EQ 0.0000 -5.0000
114 2 Linear Programmingprogramming. In this case, there are two products, floorboards, and spindles, which
are processed on three machines: a saw, a router, and a sander. The unit profits of the
two products are $1 per floorboard and $4 per spindle. The machines have capacities
(in seconds) of 50,000, 450,000, and 600,000. It takes 5 sec to saw the floorboard or
the spindle. Thirty seconds are needed to process a floorboard on the router, while a
spindle takes 90 sec on this machine. One floorboard requires 20 sec on the sander,
while a spindle needs 100 sec for processing. The objective of the firm is to
maximize its profits from the floorboard and spindle production.
Defining variables x1 and x2 as the number of floorboards and spindles that we
make and sell, respectively, we can formulate the primal problem as follows.
Table 2.37 Sensitivity Analyses for Problem 3
SENSITIVITY ANALYSES
COEFFICIENTS OF THE OBJECTIVE FUNCTION
VARIABLE LOWEST ORIGINAL HIGHEST 
ALLOWABLE VALUE ALLOWABLE
VALUE VALUE
X11 –INFINITY 3.00 8.00
X12 1.00 7.00 INFINITY
X13 –1.00 4.00 5.00
X21 4.00 9.00 INFINITY
X22 -INFINITY 2.00 8.00
X23 4.00 5.00 10.00
RIGHT-HAND SIDE VALUES
CONSTRAINT LOWEST ORIGINAL HIGHEST
ALLOWABLE VALUE ALLOWABLE
VALUE VALUE
ORIGIN 1 40.00 60.00 70
ORIGIN 2 60.00 80.00 INFINITY
DESTINATION 1 20.00 30.00 50.00
DESTINATION 2 0.00 50.00 70.00
DESTINATION 3 30.00 40.00 60.00
2.5 Duality 115P : Max z ¼ 1x1 þ 4x2 ð Þ max profit
s:t: 5x1 þ 5x2  50, 000 saw ð Þ
30x1 þ 90x2  450, 000 router ð Þ
20x1 þ 100x2  600, 000 sander ð Þ
x1, x2, 0: ð Þ nonnegativity
In order to explain the dual problem, we will refer to the firm that has the primal
problem as its planning model as the Manufacturer. Suppose now that there is
another company that, for reasons to become clear as we proceed, we will refer to
as the Lessor. The Lessor actually owns the saws, routers, and sanders, which he can
use to either manufacture floor boards and spindles himself or lease out the machine
time to the Manufacturer or Lessee. The task for the Manufacturer is now to set up a
pricing system that will minimize its own overall production costs, while making it
interesting to the Lessor to rent machine time to the Manufacturer rather than make
the products himself. Note that the Lessor will not rent out an hour here or there, he
either rents the entire time—50,000 sec on the saw, 450,000 sec on the router, and
600,000 sec on the sander—or not at all.
In order to determine such a pricing system, the Manufacturer will set up a
pricing system that defines u1 as the price per second on the saw, u2 as the price per
second on the router, and u3 as the price per second on the sander. Clearly, the
Manufacturer wishes to find prices that will minimize the total cost of leasing the
equipment. Leasing the machine time on the saw will cost u1 dollars per second, and
the time to lease is 50,000 sec, and similar for the other two machines. Hence the
objective is to minimize 50,000u1 + 450,000u2 + 600,000u3. The next task is to fix
the value of the price u1 so as to make it interesting to the Lessor to rent out the
machine time rather than running the manufacturing procedure himself. This can be
achieved by ensuring that the price charged for the rental time required to make one
unit of a product is at least as much as it would cost by making and selling the
product directly. In order to illustrate, consider the floorboards. It takes 5 sec on the
saw to make it, which, given the pricing system, is evaluated at a cost of 5u1. In
addition, we also use time on the router and the sander, and these times are evaluated
at costs of 30u2 and 20u3, respectively. The sum of all of these costs—the rental cost
equivalent to making one floorboard—should be at least as much as the profit of
making a floorboard. If it were not, the Lessor might as well go into the
manufacturing business himself. This means that we have to require that 5u1
+ 30u2 + 20u3  1, where the right-hand side corresponds to the $1 profit for
making one floor board. A similar constraint needs to be written for the spindles,
again in order to make it as least as attractive for the Lessor to rent out machine time
rather than manufacture himself. That constraint is 5u1 + 90u2 + 100u3  4. Adding
the fact that the prices must all be nonnegative, we now have formulated a complete
dual problem, which we will call PD. Below, we show the primal problem and the
dual problem next to each other.
116 2 Linear ProgrammingP : Max z ¼ 1x1 þ 4x2 PD : Min zD ¼ 50, 000u1 þ 450, 000u2 þ 600, 000u3
s:t: 5x1 þ 5x2  50, 000 s:t: 5u1 þ 30u2 þ 20u3  1
30x1 þ 90x2  450, 000 5u1 þ 90u2 þ 100u3  4
20x1 þ 100x2  600, 000 u1, u2, u3  0:
x1, x2  0:
Before we continue discussing the relations between a primal and its associated
dual problem, we would like to point out that the roles of variables and constraints in
the two problems are exchanged. The objective function coefficients of the primal
are found on the right-hand sides of the dual, while the right-hand side values of the
primal are coefficients in the objective function of the dual. Similarly, the techno￾logical coefficients on the left-hand sides are the same, but with rows and columns
exchanged.
Table 2.38a, b show the simulated printout of the optimal solutions of the primal
and dual problem, respectively. While it is optimal for the Manufacturer to make no
floorboards and 5000 spindles (for a total profit of $20,000), the Lessor should
charge nothing for the saw and the sander, but 4.44¢ for each second of the router.
Given that he leases the machines to the Manufacturer rather than making
floorboards and spindles himself, his profit from the leasing will then be $20,000.
Comparing the primal and dual solutions, we also note that the optimal values of
the primal variables are found in the dual solution as shadow prices of resources,
while the optimal values of the primal slack and excess variables are the opportunity
costs of the dual variables. Similarly, the shadow prices of the primal resources equal
the optimal values of the dual decision variables, and the opportunity costs of the
primal variables equal the optimal values of the slack and excess variables in the
dual. This means, of course, that we can solve either the primal or the dual solution
and have both optimal solutions available in one Summary of Results.
In order to explain some further relations between the primal and its associated
dual problem, let us multiply each primal constraint with its associated dual variable
Table 2.38 Optimal solution of primal and dual problem
2.5 Duality 117and each dual constraint with the primal variable it is associated with. (Note that due
to the nonnegativity constraints on the variables, the inequalities do not change.) We
then obtain the following system S:
S : z ¼ 1x1 þ 4x2ð Þ the primal objective function
5x1u1 þ 5x2u1  50, 000u1 ð Þ the first primal constraint multiplied by u1
30x1u2 þ 90x2u2  450, 000u2 ð Þ the second primal constraint multiplied by u2
20x1u3 þ 100x2u3  600, 000u3 ð Þ the third primal constraint multiplied by u3
5u1x1 þ 30u2x1 þ 20u3x1  1x1 ð Þ the first dual constraint multiplied by x1
5u1x2 þ 90u2x2 þ 100u3x2  4x2 ð Þ the second dual constraint multiplied by x1
zD ¼ 50, 000u1 þ 450, 000u2 þ 600, 000u3ð Þ the dual objective function
x1, x2  0, u1, u2, u3  0: ð Þ nonnegativity constraints
Adding all primal constraints in system S, we obtain 5x1u1 + 5x2u1 + 30x1u2
+ 90x2u2 + 20x1u3 + 100x2u3  50,000u1 + 450,000u2 + 600,000u3, where we note
that the right-hand side value equals zD. Similarly, adding all dual constraints in
system S, we obtain 5u1x1 + 30u2x1 + 20u3x1 + 5u1x2 + 90u2x2 + 100u3x2  1x1
+ 4x2, where we note that the right-hand side value of the aggregate constraint equals
the primal value of the objective function z. Putting the two expressions together, we
obtain zD ¼ 50,000u1 + 450,000u2 + 600,000u3  1x1 + 4x2 ¼ z, or simply zD  z.
This property is commonly referred to as weak duality. Given that the primal
problem maximizes z and the dual problem minimizes zD, the two values will
move towards each other and be equal when they reach an optimal solution. This
situation is shown in Fig. 2.23.
In order for the two objective values to be equal at optimum, it is necessary that
all of the primal inequalities multiplied by their respective dual variables and all dual
constraints multiplied by their respective primal variables are satisfied as equations.
In our example, this means that if x1 , x2 , x3 is an optimal solution of the primal
problem and u1, u2 is an optimal solution of the dual problem, then
5x1u1 þ 30x2u1 ¼ 50, 000u1,
30x1u2 þ 90x2u2 ¼ 450, 000u2, and
20x1u3 þ 100x2u3 ¼ 600, 000u3 for the primal constraints, and
z zD z, zD for feasible solutions 
Fig. 2.23 Weak duality
118 2 Linear Programming50u1x1 þ 30u2x1 þ 20u3x1 ¼ 1x1, and
5u1x2 þ 90u2x2 þ 100u3x2 ¼ 4x2 for the dual constraints:
If we were to define slack variables S1, S2, and S3 for the three primal constraints
and excess variables ED
1 and ED
2 for the two dual constraints, and their optimal values
are also indicated by a bar over the variables, we can rewrite the above equations as
S1u1 ¼ 0, S2u2 ¼ 0, and S3u3 ¼ 0 for the primal problem, and ED
1 x1 ¼ 0 and
ED
2 x2 ¼ 0 for the dual problem. These conditions are usually called (weak) comple￾mentary slackness conditions.
These conditions mean that if an inequality constraint is not satisfied as an
equation at optimum, then its dual variable must be equal to zero. If a constraint is
satisfied as an equation, then its dual variable may be zero or positive. In terms of our
example, this means that if we do not fully use a resource (here: machine capacities),
then the dual variable (the shadow price of the resource) must equal zero. If, on the
other hand, a resource is fully used, then the dual variable may be positive. Similarly,
if a variable is positive, then its opportunity cost must be zero; if a variable is zero,
then its opportunity cost may be positive. This corresponds with the interpretation
we provided in Sect. 2.5 on postoptimality analyses.
It is worth noting that the dual of the dual problem is again the primal problem.
The remainder of this section will demonstrate some further relations between a
pair of primal and dual problems. In general, we have three possible cases:
1. The primal problem and its dual both have finite optimal solutions, in which case
z ¼ zD.
2. One of the two problems has no feasible solution, while the other has unbounded
“optimal” solutions.
3. Both problems have no feasible solutions.
We already have an example of the first case: the numerical illustration used
throughout this section belongs into that category.
In order to demonstrate the second case, consider the following pair of primal
(P) and dual (PD) problems:
P : Max z ¼ 3x1 þ 2x2 PD : Min zD ¼ u1 þ 2u2
s:t: x1  x2  1 s:t: u1  2u2  3
2x1 þ 1x2  2 u1 þ u2  2
x1, x2  0 u1, u2  0:
The graphical representations are shown in Fig. 2.24a (for P) and b (for PD). It is
apparent that while the primal problem has unbounded “optimal” solutions, its dual
problem has no feasible solution.
Finally, consider the third case, in which neither problem has a feasible solution.
As a numerical example, consider the following pair of primal (P) and dual (PD)
problems:
2.5 Duality 119P : Max z ¼ x1 þ 3x2 PD : Min zD ¼ u1  2u2
s:t: 2x1  4x2  1 s:t: 2u1  u2  1
x1 þ 2x2  2 4u1 þ 2u2  3
x1, x2  0 u1, u2  0:
The graphical representation of the two problems is shown in Fig. 2.25a (for P)
and b (for PD). It is apparent that the constraints in the primal and the dual problems
are parallel to each other, so that the feasible set is empty.
Fig. 2.24 Pairs of dual programs
Fig. 2.25 Primal and dual both lack feasible solutions
120 2 Linear ProgrammingExercises
Problem 1 (setting up the dual problem): Consider the following primal linear
programming problem P:
P : Min z ¼ 3x1 þ 4x2
s:t: x1 þ 2x2 ¼ 5
5x1  x2  2
x1, x2  0:
Set up the dual problem PD.
Solution: Since we have not provided any rules for the transformation other than
for maximization problems with “” constraints, so we simply bring the problem P
into that form. Changing a minimization problem to a maximization problem and
changing the direction of an inequality is standard. As far as the equation x1 + 2x2
¼ 5 is concerned, we replace it by two opposing inequalities x1 + 2x2  5 and x1
+ 2x2  5. We can then define an equivalent version of the problem description P as
P0 : Max  z ¼ 3x1  4x2
s:t: x1 þ 2x2  5
x1  2x2  5
5x1 þ x2  2
x1, x2  0:
The dual of this problem is then
P00
D : Min  zD ¼ 5u1  5u2  2u3
s:t: 2u1  u2  5u3  3
2u1  2u2 þ u3  4
u1, u2, u3  0:
If desired, we can clean up this problem a bit and write it in its equivalent form
P0
D : Max zD ¼ 5u1 þ 5u2 þ 2u3
s:t: u1  u2  5u3  3
2u1 þ 2u2  u3  4
u1, u2, u3  0:
We notice that in the dual problem, the variables u1 and u2 always appear with the
same coefficients but with opposite signs. We may thus define a new variable u12
¼ u1  u2, which can then replace the original variables u1 and u2. Note that the new
variable u12 is now unrestricted in sign. We then obtain the dual
2.5 Duality 121PD : Max zD ¼ 5u12 þ 2u3
s:t: u12  5u3  3
2u12  u3  4
u3  0:
Problem 2 (duality for a single-constraint primal problem): Consider the
following single-constraint primal linear programming problem:
P : Max z ¼ 2x1 þ 3x2 þ 8x3 þ 7x4 þ 6x5
s:t: x1 þ 6x2 þ 4x3 þ 3x4 þ 3x5  11
x1, x2, x3, x4, x5  0:
(a) Formulate the dual problem PD. How many variables does it have?
(b) Show that PD can be solved by simple inspection. State the optimal solution and
objective function value of PD. Explain why z ¼ zD.
(c) Using duality relationships, find the unique optimal solution to P.
Solution:
(a) The problem under consideration is a continuous knapsack problem similar to
those discussed in Chap. 5 of this book. Since there is only a single constraint in
the primal problem, the dual problem features only a single variable. The dual is
PD : Min zD ¼ 11u
s:t: u  2
6u  3
4u  8
3u  7
3u  6
u  0:
(b) With zD ¼ 11u, the optimal value of the variable u must be as small as possible.
Considering the lower bounds specified in the constraints, we determine that
u ¼ max 2, 3
6 , 8
4 , 7
3 , 6
3 , 0   ¼ 21=3, so that zD ¼ 11u ¼ 252=3. Since P has feasi￾ble solutions (for instance, x1 ¼ x2 ¼ ...¼ x5 ¼ 0), and zD exists, z must exist as
well and z ¼ zD ¼ 252=3.
(c) Since 3u  7 is the only constraint in the dual problem PD that is tight (binding)
at optimum, the excess variables of all other dual constraints are strictly positive
at optimum. The complementary slackness conditions then require that x1 ¼
x2 ¼ x3 ¼ x5 ¼ 0. Since u ¼ 21=3 > 0, the slack variable of the corresponding
122 2 Linear Programmingprimal constraint must be zero at optimum, again due to complementary slack￾ness. The primal constraint is then binding at optimum, so that 3x4 ¼ 11 or
x4 ¼ 32=3.
References
Dantzig GB (1963) Linear programming and extensions. Princeton University Press, Princeton, NJ
Dantzig GB, Thapa MN (1997) Linear programming: introduction. Springer, New York
Eiselt HA, Sandblom C-L (2007) Linear programming and its applications. Springer, Berlin
Garner Garille S, Gass SI (2001) Stigler’s diet problem revisited. Oper Res 49:1–13
References 123Multiobjective Programming 3
As diverse as the problems in the previous chapters have been, they share one
common feature: they all have one single objective function and the result is an
optimal solution (or multiple optima, in case of dual degeneracy). However, the
concept of optimality applies only in case of a single objective. If we state that
something is “the best” or optimal, we always have an objective in mind: the fastest
car, the most comfortable vehicle, the automobile that is cheapest to operate, and so
forth. Whenever a second or even more objectives are included in a problem, the
concept of optimality no longer applies. For instance, if the top speed of a vehicle
and its gas mileage are relevant concerns, then the comparison between a car, whose
speed may top 110 miles per hour and which gives 20 miles to the gallon (highway
rating) and a vehicle that can go up to 90 miles per hour and which gives 25 miles to
the gallon is no longer a simple one: the former car is faster at the expense of fuel
efficiency. It will now depend on the decision maker which of the two criteria is
considered more important. In other words, the decision maker will―sooner or
later―have to specify a tradeoff between the criteria. This is the type of problems
considered in this chapter.
There are two main reasons that recommend multiobjective optimization, despite
its added conceptual and technical difficulties. First, consider a simple linear pro￾gramming problem. For mathematical reasons, the number of variables that have
positive values in an optimal solution cannot exceed the number of constraints. For
instance, if we had a diet problem with, say, 10,000 foods but only two constraints,
then an optimal diet will have no more than two foodstuffs. This is clearly extreme
(not to be confused with the concept of extreme points, which is different, albeit
related). We would like to point out that this “extreme” property is particular to linear
programming; it does not occur in integer linear or nonlinear programming. Sec￾ondly, solutions found with single-objective optimization methods tend to be
extreme, as, by definition, they have only a single concern. For instance, if
the task is to schedule employees efficiently with a cost minimization objective,
then the solutions will completely disregard the convenience of the schedule to the
employees and other issues, provided, of course, that there are no constraints to that
# The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
H. A. Eiselt, C.-L. Sandblom, Operations Research, Springer Texts in Business and
Economics, https://doi.org/10.1007/978-3-030-97162-5_3
125effect. This is different when multiple objectives are included; their very essence is
the concept of compromise.
While the terminology in this field is not quite standardized, we typically refer to
problems with multiple concerns (objective functions or criteria) as multicriteria
decision-making (MCDM) problems. This big umbrella has two main subcategories:
problems with multiple objectives and an infinite number of possible decisions (and
linear functions) which are referred to as multiobjective (linear) programming
problems (MOLP), while problems with multiple evaluation criteria and a finite
number of possible decisions are usually called multiattribute decision-making
(MADM) problems. In this chapter, we will exclusively deal with MOLP, while
Chap. 11 discusses MADM problems.
Interestingly enough, MOLP have been around almost as long as linear program￾ming problems. Similarly, the replacement of the concept of optimality by the (much
weaker) concept of pareto-optimality has been known to economists for more than
100 years. (It is named after the Italian economist Vilfredo Pareto, 1848–1923.) In
the sections below, we discuss two of the main approaches to multiobjective linear
programming: the vector optimization problem and goal programming problems.
Their main distinction is rooted in the decision maker’s input. In vector optimization
problems, the decision maker does not provide any input regarding the tradeoff
between objectives. This means, of course, that many pareto-optimal solutions will
be generated, which then must be compared manually by the decision maker―a
process, in which he will have to use some (probably implicit) tradeoffs. Another
possibility is to openly define tradeoffs, which then allows the analyst to reduce the
problem to a standard linear programming problem. The problem with this approach
is that it will likely be impossible for any decision maker to specify with any degree
of certainty that one objective is, say, 2.7 times as important as another. Such an
approach will have to rely very heavily on sensitivity analyses. Finally, we present
goal programming, an approach whose main feature is that it blurs the distinction
between objectives and constraints.
3.1 Vector Optimization
The most logical way to introduce vector optimization problems starts with linear
programming and then extends the analysis to multiple objectives. As usual, we will
give preference to intuitive reasoning based on graphical arguments.
As far as the formal statement of a vector optimization problem is concerned,
consider the following example that we will also use in this in the next section.
Max z1 ¼ 3x1 þ x2
Max z2 ¼ 2x1 þ x2
126 3 Multiobjective Programmings:t:  x1 þ x2  3 ðIÞ
x2  4 ðIIÞ
x1 þ x2  6 ðIIIÞ
x1  5 ðIVÞ
x1, x2  0:
First of all, the name “vector optimization” stems from the fact that rather than a
single objective, we have a “vector” of objectives.
For now, consider a single objective, whose gradient of the objective function and
iso-profit line through some point “x” are shown in Fig. 3.1.
To recap from our discussion in linear programming, the “z” in Fig. 3.1 indicates
the gradient of the objective function, while the line with flags “+++” and “”
is the iso-profit line through an arbitrary point bx. This iso-profit line subdivides the
space into two halfspaces: all points in the halfspace flagged with “+++” have
objective function values better (i.e., higher for maximization problems and lower
for minimization problems) than the point bx, while all points in the halfspace flagged
with “– –.–” have objective function values worse (i.e., lower for maximization and
higher for minimization) than bx.
Consider now a problem with two objective functions. We can then take an
arbitrary point bx, anchor both objective function gradients at that point, and plot
iso-profit lines through it. This is shown in Fig. 3.2.
Figure 3.2 is based on a problem with two objectives, whose gradients are shown
by the arrows marked with z1 and z2, respectively, and their two iso-profit lines. The
two iso-profit lines subdivide the plane into four parts, labeled with C++, C+, C+
,
and C . In the following analysis, we will compare points in these four parts of the
plane with bx.
First consider any point in the set labeled C . As compared to bx, this point will
be worse than bx with respective to the first objective and also worse with respect to
Fig. 3.1 Single objective
function
3.1 Vector Optimization 127the second objective, as it is located in the intersection of the two halfplanes flagged
with “.” This means that if we had realized point bx, there would be no reason
to move to any point in the set C , as the new point will be worse than bx with
respect to both objectives. Next, consider a point in the set C+
. Here, things are a bit
more difficult, as any point in that set will be worse than bx with respect to the first
objective but better than bx with respect to the second objective. This means that
points in C+ are not comparable to bx.
A similar argument applies to all points in the set C+. All points in this set are
better than bx with respect to the first objective but worse than bx with respect to the
second objective. So again, no comparison is possible. Finally, consider the set C++.
Any point in this set is better than bx with respect to both objectives, so that we would
move out of the present solution bx into C++ whenever possible, i.e., if not restricted to
do so by the constraints. As all points in C++ are better than bx, we will call C++ the
improvement cone (rooted at bx).
Before using this concept to solve vector optimization problems, it is useful to
discuss the relation between objectives. Suppose that two objectives are very similar,
for example, Max z1 ¼ 2x1 + 5x2 and Max z2 ¼ 2x1 + 6x2. It is apparent that there is
only little conflict between the two. Such a case is shown in Fig. 3.3a, where the
angle between the two gradients is very small. This results in an improvement cone
with a very large angle. Actually, in the extreme case of two identical objectives (i.e.,
no conflict), the improvement cone is then the same as that shown in Fig. 3.1 with the
halfplane labeled “+ + +.”
On the other hand, consider two objectives that show extensive conflict. This
situation is depicted in Fig. 3.3b. Here, the angle between the two gradients is large,
and the angle of the improvement cone is very small, indicating that there is only
limited potential for improvement. In the limiting case, one gradient would be
diametrically opposed to the other, a case of total conflict. In such a case, the
improvement cone is empty and there is no potential for any improvement.
However, it may still be possible in such a case to find a compromise after all.
One possibility to do so is to introduce additional criteria. As an example, consider a
couple who is planning this year’s vacation. Suppose that the husband would like to
maximize the amount of time the couple spends on the beach to relax (and watch
other people), the wife would like to minimize the time on the beach and go boating
Fig. 3.2 Two objective
functions
128 3 Multiobjective Programminginstead, something the husband has absolutely no interest in. (A somewhat similar
situation has been dealt with in game theory under the name “battle of the sexes”).
This has escalated to a major fight, and as it is, there is no room for compromise.
Suppose now that we introduce another activity, e.g., visiting events, festivals, zoos,
or museums, which we will call sites and events, something both are interested in, at
least to some extent. The objective functions given two dimensions are shown in
Fig. 3.4, indicating that there is now an actual possibility for compromise. Similar
situations occur in labor negotiations, in which management wants to minimize the
amount spent on wages and salaries, while labor wants to maximize it. Issues added
to the list of topics to be negotiated could include concerns such as work conditions,
something both parties are interested in.
Fig. 3.3 Little (a) and extensive (b) conflict between objectives
Fig. 3.4 Possibility for compromise
3.1 Vector Optimization 129Back to the improvement cone. We stated above that whenever possible, we
would try to move out of an existing point bx into the improvement cone. The only
thing that could prevent us from doing so are the constraints. In case it is possible to
move out of a feasible point bx to another feasible point in the improvement cone
rooted at bx, we should do so, indicating that the point bx is not a point to be considered
further, as there are other points that are better than bx with respect to all objectives. In
other words, if the intersection of the improvement cone and the feasible set includes
at least one more point other than bx, then bx does no longer have to be considered.
On the other hand, if moving out of bx into its improvement cone will always result
in the loss of feasibility, then the point bx is called nondominated (or, alternatively,
noninferior, efficient, or pareto-optimal). We will use these terms interchangeably.
The collection of all noninferior points is called the nondominated frontier. It is
apparent that points in the interior of the feasible set cannot be nondominated, as it is
possible to move out of them into any direction and stay feasible, at least for some
sufficiently small distance. This is shown in Fig. 3.5, in which the feasible set of our
example problem is shaded, and the extreme points are 0, A, B, C, D, and E. The
gradients of the two objectives are labeled z1 and z2, and improvement cones are
shown anchored at all extreme points as well as at some interior point bx. From any of
these points, we will try to move into its improvement cone and determine whether
or not it is possible to do so and stay within the feasible region. Clearly, at the interior
point bx, this is feasible.
Consider now the line segments that border the feasible set. All points on the line
segment (O, A) are dominated, as it is possible from any of them to move into the
improvement cone and stay feasible. The same is true for all points on the line
segment (O, E) and the segment (D, E). On the other hand, all points on each of the
line segments (A, B), (B, C), and (C, D) is nondominated, as, the moment that we
move into the improvement cone constructed with the point at its vertex, we lose
feasibility. The result of our analysis is then the nondominated frontier, shown in
Fig. 3.5 by bold lines.
It is now possible to prove that the nondominated set is connected. The union of
all nondominated line segments is the nondominated frontier. All points on this
Fig. 3.5 Improvement cones
and nondominated frontier
130 3 Multiobjective Programmingfrontier are of interest to the decision maker, whose task is now to determine which
of these solutions to choose. The choice will depend on criteria other than those
already included in the model.
3.2 Solution Approaches to Vector Optimization Problems
This section will examine techniques that can be used to approximate the efficient
frontier. While it is possible to use a modified simplex method to determine all
extreme points on the efficient frontier, this is not only a lengthy process, but also
something that leaves the decision maker with tons of solutions to manually com￾pare. This is clearly out of the question. As a result, analysts typically determine a
few solutions, and based on the decision maker’s response to those, will generate
more solutions that attempt to reflect the decision maker’s comments.
Two methods for this purpose stand out. One is the weighting method, and the
other is called the constraint method. The basic idea of the weighting method is to
first assign positive weights w1, w2, ..., wp to the p given objectives, and then
aggregate them into a single new composite objective. The result is then a linear
programming problem that can easily be solved. It is then possible to prove that an
optimal solution to this linear programming problem is always a point on the
nondominated frontier.
As a numerical illustration, consider the example introduced in the previous
section. Recall that the two objectives were Max z1 ¼ 3x1 + x2 and Max z2
¼ 2x1 + x2. Suppose that we choose w1 ¼ 5 and w2 ¼ 1. This means that one
unit of whatever the first objective measures is considered five times as important
than one unit of what the second objective measures. Using these weights, the
composite objective is then Max z ¼ w1z1 + w2z2 ¼ 5(3x1 + x2) + 1(2x1 + x2)
¼ 13x1 + 6x2. Using this objective in conjunction with the constraints of the
problems results in the optimal solution x1 ¼ 5, x2 ¼ 1 (which is point D in
Fig. 3.5), with an objective value of z ¼ 71 . For our purposes, the value of the
aggregated objective function is of no interest, it is more useful to take the solution
obtained in the optimization and insert the optimal values into the two individual
objective functions, resulting in z1 ¼ 16 and z2 ¼ 9. Table 3.1 provides a listing of
different selected weight combinations and the nondominated points they generate.
Note that all nondominated solutions that this technique generates are at extreme
points.
Table 3.1 Nondominated
solutions generated by the
weighting method
(w1, w2) z ¼ w1z1 + w2z2 (x1, x2) ð Þ z1, z2
(5, 1) 13x1 + 6x2 D ¼ (5, 1) (16, 9)
(3, 1) 7x1 + 4x2 D ¼ (5, 1) (16, 9)
(1, 1) x1 + 2x2 C ¼ (2, 4) (10, 0)
(1, 3) 3x1 + 4x2 B ¼ (1, 4) (7, 2)
(1, 5) 7x1 + 6x2 A ¼ (0, 3) (3, 3)
3.2 Solution Approaches to Vector Optimization Problems 131A graphical representation for our numerical problem is shown in Fig. 3.6. Notice
the objectives z1 and z2 as limits, the gradients of all of their selected combinations
are between them. In particular, the gradients shown here are z1, those generated by
the weight combinations (5, 1), (3, 1), (1, 1), (1, 3), (1, 5), and z2, in counterclock￾wise direction.
The second approach to generate nondominated solutions is the constraint
method. It can be described as a technique that keeps one of the objective functions,
while using the others as constraints with variable right-hand side values. This is
done by first designating one of the p objectives as objective, while all others are
reformulated as constraints. Each objective of the type “Max zk” will be rewritten as
a constraint zk  bk with a yet-to-be-determined value of bk, while each original
objective of the type “Min zk” will be rewritten as a constraint of the type zk  bk with
variable values of bk. The resulting linear programming can then easily be solved.
The solution process is repeated for a number of combinations of selected values bk,
k ¼ 1, ..., p. All solutions that are generated in this fashion are nondominated.
However, these solutions are not necessarily extreme points of the feasible set.
As an illustration, consider again the example introduced in the previous section.
We will (arbitrarily) retain the first objective as an objective and reformulate the
second objective as a constraint. The problem can then be written as
Max z1 ¼ 3x1 þ x2
s:t:  x1 þ x2  3
x2  4
x1 þ x2  6
x1  5
2x1 þ x2  b2
x1, x2  0:
Fig. 3.6 The weighting
method
132 3 Multiobjective ProgrammingAt this point, we can solve the problem for a variety of values of b2. A summary
of solutions for some chosen values is displayed in Table 3.2.
For b2 ¼ 0, we obtain point C in Fig. 3.5, and for b2 ¼ 1, 2, ..., 8, points
between C and D are found, while b2 ¼ 9 and b2 ¼ 10 result in point D. Note that
this selection of b2 values misses the nondominated solutions A and B. This is why
both, the weighting method and the constraint methods, are referred to as approxi￾mation methods. As a matter of fact, using b2 ¼ 1 results in the solution ð Þ¼ x1, x2
11=2 ð Þ , 4 , which is the point halfway between B and C, a value of b2 ¼ 2 generates the
solution ð Þ¼ x1, x2 ð Þ 1, 4 , which is point B, a value of b2 ¼ 3 results in a solution
ð Þ¼ x1, x2 ð Þ 0, 3 , which is point A, and for values in excess of b4 ¼ 4, the problem
has no feasible solution. Figure 3.7 shows the feasible set of the problem and the
constraints based on the second objective for various values of b2. The bold points
are the resulting optimal solutions.
3.3 Goal Programming
When introducing goal programming, it is useful to return to the basic discussion in
Chap. 1 about constraints and objective functions. Recall that constraints express
requirements that must be satisfied, while objective functions are for requirements
that should be satisfied, if possible. While this distinction appears clear, the differ￾ence between “required” and “desired” is blurred in reality. Consider a simple
budget constraint that expresses the condition that we cannot spend more than we
have. While it may not be a wise choice, we could consider the possibility to borrow
money. Consider a variety of similar examples: All offices must fit into the space that
we own―but we can rent some more space. Do not use more employees than are
available―but that is what temp agencies are there for. Payments are due on a
specific date―but we may be able to defer them. All of this is introduced into the
discussion to demonstrate that many requirements are much softer than they appear.
And this is why modelers should take precautions before formulating constraints, as
Table 3.2 Nondominated
solutions determined with
the constraint method
b2 (x1, x2) z1
5 No feasible solution
0 (2, 4) 10
1 (21=3, 32=3) 102=3
2 (22=3, 31=3) 111=3
3 (3, 3) 12
4 (31=3, 22=3 ) 122=3
5 (32=3, 21=3 ) 131=3
6 (4, 2) 14
7 (41=3, 12=3) 142=3
8 (42=3, 11=3) 151=3
9 (5, 1) 16
10 (5, 1) 16
3.3 Goal Programming 133they are absolute: if they cannot be satisfied by the given data, the solver will return a
message indicating that there is no feasible solution.
Goal programming is one tool that attempts to deal with “soft constraints.” The
general idea was developed by the later Nobel laureate Herbert A. Simon in 1957,
who introduced the concept of satisficing, a composite word that joins the concepts
of “satisfy” and “suffice.” The concept as applied to goal programming works as
follows. All regular (absolute) constraints are written as usual. Continuing, a soft
constraint is first formulated as a regular constraint. It is then reformulated as a goal
constraint with the help of a target value (or aspiration level) and deviational
variables. The target value is a number that expresses how many resources we
have, what output we would like to achieve, or similar measures. The nonnegative
deviational variables dþ
ℓ and d
ℓ measure over- and underachievements, respectively.
The way to formulate goal constraints is to first write the requirement as a regular
constraint, and then reformulate it as shown in Table 3.3.
The deviational variable d
ℓ is similar to a slack variable, and the deviational
variable dþ
ℓ resembles an excess variable. While it appears counterintuitive that slack
and excess variables should appear in the same constraint, it really is not. As an
example, consider a budget constraint that states that the actual expenditures should
not exceed the available amount. First of all, the appearance of slack and surplus in
Fig. 3.7 The constraint method
Table 3.3 Formulation of soft constraints as goal constraints
Desired situation Formulation of goal constraint Contribution to the objective function
LHS  RHS
LHS ¼ RHS
LHS  RHS
LHS + d
ℓ  dþ
ℓ ¼ RHS
d
ℓ , dþ
ℓ  0
Min dþ
ℓ
Min d
ℓ þ dþ
ℓ
Min d
ℓ
134 3 Multiobjective Programmingthis constraint indicates that the actual expenditures may be smaller than the avail￾able budget (underspending) or may exceed the actual budget (overspending).
Secondly, technical reasons will ensure that at most one of the deviational variables
can be positive, so that we cannot have over- and underspending at the same time.
As a numerical example, suppose that we have $100 that we can spend on two
items, food and entertainment (panem et circensis, as the Romans would have it).
The amounts spent on the two items are x1 and x2, respectively, and the budget
constraint would be x1 + x2  100. Reformulating it as a goal constraint leads to
x1 þ x2 þ d
ℓ  dþ
ℓ ¼ 100. Suppose now that we have decided to spend $60 of food
and $30 on entertainment. This means that the goal constraint is then 90 þ d
ℓ 
dþ
ℓ ¼ 100 or d
ℓ  dþ
ℓ ¼ 10. Given that the deviational variables must satisfy the
nonnegativity constraints, and that at most one of them can be positive, this implies
that d
ℓ ¼ 10. The meaning is that the present budget is “underachieved” by $10, or,
in more standard terms, there are $10 left over.
If, on the other hand, we spend a total of, say, $90 on food and $30 on entertain￾ment, the goal constraint reads 120 þ d
ℓ  dþ
ℓ ¼ 100 or d
ℓ  dþ
ℓ ¼ 20. Again,
given the nonnegativity of the deviational variables, the result is that dþ
ℓ ¼ 20 ,
indicating an “overachievement” (or, similarly, overspending) of the budget by $20.
The last column in Table 3.3 then indicates the contribution to the overall
objective made by the deviational variables introduced in a goal constraint. In the
aforementioned budget constraint, the relation “actual amount spent  amount
available” was desired, so that goal programming, after rewriting the constraint as
a goal constraint, will minimize the overachievement, i.e. overspending. Note that
this approach does justice to the “softness” of the budget constraint by allowing
overspending but trying to minimize it. In practice, absolute or rigid constraints and
soft constraints can often be distinguished by the way the requirements are worded.
A telltale sign is the expression “if possible.” Whenever it is appended to a
requirement, it clearly indicates that formulation as a goal constraint is in order.
The next issue is then how to aggregate the deviational constraints into a single
objective function. The original version of goal programming has multiple levels,
each of them assumed to be infinitely more important than the next. This structure
has been criticized profusely in the literature, even though the principle is common,
even in linear programming: the absolute constraints are infinitely more important
than the objective. This can be seen that if a constraint cannot be satisfied, we will
obtain the signal “there exists no feasible solution” from the solver, regardless how
good the objective function value might or could be.
In this book, we will restrict ourselves to a single level, on which we aggregate the
deviational variables similar to the way we aggregated objective functions in the
weighting method in the previous section of this chapter. The problem with such a
procedure is commensurability. For instance, if one deviational variable expresses
the overexpenditure of the budget (measured in dollars), while another expresses the
underuse of manpower (measured in the number of employees), we cannot simply
add these two together. By using weights, we express a tradeoff between the units, so
that a weight, which is multiplied by the overexpenditure of the budget, will have to
3.3 Goal Programming 135express the importance of one dollar of overexpenditure in relation to the underuse of
one employee.
In order to illustrate the modeling process, consider the following numerical
Example The owner of a chain of jewelry stores has to decide how to distribute
parts of a new shipment of diamonds to five stores in a region. The first three stores
of the chain are located in shopping malls. The following conditions have to be
observed.
Absolutely necessary:
(a) Allocate between 1000 and 1200 carats in total to the five stores.
(b) Store 5 must receive at least 300 carats of diamonds.
Desired properties of the allocation:
(c) The stores in the malls should receive at least 80% of all the diamonds, if
possible.
(d) The allocations to the stores in the malls should be equal to each other, if
possible.
(e) The probabilities of theft in the stores have been estimated to be 0.1%, 0.1%, 9%,
2%, and 3%, respectively. The owner would like to minimize the expected loss.
Requirement (e) takes priority in the list and is considered to be 25 times as
important as requirement (d), which, in turn, is considered twice as important as (c).
In order to formulate the problem, we first define decision variables x1, x2, ..., x5
as the quantity of diamonds allocated to stores 1, 2, 3, 4, and 5, respectively. The
absolute constraints (a) and (b) can then be written as
x1 þ x2 þ x3 þ x4 þ x5  1000 ð3:1Þ
x1 þ x2 þ x3 þ x4 þ x5  1200 ð3:2Þ
x5  300: ð3:3Þ
Consider now requirement (c). Written as a constraint, the requirement can be
formulated as x1 + x2 + x3  0.8(x1 + x2 + x3 + x4 + x5) or, equivalently, as
0:2x1 þ 0:2x2 þ 0:2x3  0:8x4  0:8x5  0:
Rewriting the requirement as a goal constraint with deviational variables, we
obtain
0:2x1 þ 0:2x2 þ 0:2x3  0:8x4  0:8x5 þ d
1  dþ
1 ¼ 0 ð3:4Þ
with Min d
1 as the contribution to the objective function.
136 3 Multiobjective ProgrammingNext consider requirement (d). The average allocation to a store in the mall is
1
3 ð Þ x1 þ x2 þ x3 , so that we would like to see x1 ¼ 1
3 ð Þ x1 þ x2 þ x3 , x2 ¼ 1
3 
ð Þ x1 þ x2 þ x3 , and x3 ¼ 1
3 ð Þ x1 þ x2 þ x3 . Rewriting the first of these constraints
results in 2
3 x1  1
3 x2  1
3 x3 ¼ 0. As a goal constraint, we write
2
3 x1  1
3 x2  1
3 x3 þ d
2  dþ
2 ¼ 0 ð3:5Þ
with the objective function contribution Min d
2 þ dþ
2 . Similarly, we obtain the other
two goal constraints
 1
3 x1 þ
2
3 x2  1
3 x3 þ d
3  dþ
3 ¼ 0 ð3:6Þ
and
 1
3 x1  1
3 x2 þ
2
3 x3 þ d
4  dþ
4 ¼ 0 ð3:7Þ
with the objective function contributions Min d
3 þ dþ
3 and Min d
4 þ dþ
4 ,
respectively.
Finally, consider requirement (e) The original objective is written as
Min z ¼ 0:001x1 þ 0:001x2 þ 0:09x3 þ 0:02x4 þ 0:03x5:
Setting the expected loss at some unattainably low level, e.g., z ¼ 0, we can then
require that the expected loss is no larger than that level, if possible (which it cannot,
so that we minimize the overachievement). This is then written as
0:001x1 þ 0:001x2 þ 0:09x3 þ 0:02x4 þ 0:03x5 þ d
5  dþ
5 ¼ 0 ð3:8Þ
with the objective function contribution Min dþ
5 .
The problem can then be written as
Min z ¼ 50dþ
5 þ 2 d
2 þ dþ
2
  þ 2 d
3 þ dþ
3
  þ 2 d
4 þ dþ
4
  þ d
1
s.t. constraints (3.1)–(3.8) and the nonnegativity constraints for all variables.
The optimal solution allocates 466.69 carats of diamonds to store 1, another
233.31 carats of diamonds to store 2, no diamonds to stores 3 and 4, and the
minimally required 300 carats to store 5. The total allocation is 1000 carats, the
lower bound of the (absolute) constraints. The goal constraint with the highest
weight (the expected loss) is measured by the overachievement dþ
5 , whose optimal
value is 9.7. The second-ranking goal constraint that attempts to make the mall
allocations equal is violated by 466.67 carats, and the third-ranking goal constraint
that wants at least 80% of the allocations in malls is violated by 10 percentage points.
Exercises
Problem 1 (improvement cone): Consider the following two vector optimization
problems (to simplify matters, the constraints have been ignored):
3.3 Goal Programming 137(a) P1 : Max z1 ¼ 5x1 + 2x2, Max z2 ¼  2x1  2x2, Max z3 ¼ 3x1, and
(b) P2 : Max z1 ¼ x2, Max z2 ¼ 4x1  x2, Min z3 ¼ x1 + x2.
Plot each of these two problems individually and determine the
improvement cone.
Solution: The improvement cone for (a) is shown in Fig. 3.8a, the improvement
cone for (b) is empty as shown in Fig. 3.8b.
Problem 2 (nondominated frontier and composite objective, graphical):
Consider the following linear programming problem:
P : Max z1 ¼ x1 þ 2x2
s:t:  2x1 þ x2  2
x1 þ x2  5
x1  3
x1, x2  0:
(a) Plot the constraints and determine the feasible set.
(b) Graph the gradient of the objective function and use the graphical solution
technique to determine the optimal point. Compute the exact coordinates of
the optimal point and its value of the objective function.
(c) Consider a second objective function Max z2 ¼ 2x1  x2. Ignoring the first
objective, what is the optimal point? Compute its exact coordinates and its value
of the objective function.
(d) Determine the nondominated frontier given the two objectives.
(e) Use the two objectives above to construct the composite objective function with
weights w1 ¼ ¾ and w2 ¼ ¼. What is the optimal solution with this objective?
Fig. 3.8 Improvement cones
138 3 Multiobjective ProgrammingSolution:
(a) The solutions are based on Fig. 3.9.
(b) The exact coordinates of the optimal solution are x1 ¼ ð Þ 1, 4 with value of the
objective function z1 ¼ 9:
(c) The optimal solution with the second objective is x2 ¼ ð Þ 3, 0 with z2 ¼ 6:
(d) The efficient frontier is shown by the bold line in Fig. 3.9.
(e) The composite objective function is Max z ¼ 5
4 x1 þ 5
4 x2 , it is shown as the
gradient labeled “z” in Fig. 3.9. The optimal solution with the composite
objective is x ¼ ð Þ 3, 2 , and the values of the objective functions at that point
are z1 ¼ 7 and z2 ¼ 4.
Problem 3 (vector optimization, nondominated frontier): Consider the follow￾ing vector optimization problem:
P : Min z1 ¼ x1 þ x2
Max z2 ¼ 2x1 þ x2
s:t: x1  3
x2  2
x1 þ x2  1
x1, x2  0:
Fig. 3.9 Composite
objective function and
efficient frontier
3.3 Goal Programming 139(a) Graph the directions of both objective functions and determine the
improvement cone.
(b) Graph the constraints and clearly indicate the feasible set. Plot the improvement
cone at each outside boundary of the feasible set and determine the efficient
frontier. Clearly describe the efficient frontier.
Solution:
(a) Figure 3.10 shows the gradients of the two objective functions and the
improvement cone.
(b) Figure 3.11 shows the feasible set (the edges of the set are shaded in gray), the
improvement cones at each boundary segment of the feasible set. The
nondominated set is shown as the bold line segments (0, 0) to (3, 0) and (3, 0)
to (0, 2).
Problem 4 (weighting method): Consider again the vector optimization problem
in Problem 3.
(a) Use the weighting method with weight combinations w ¼ (5, 1), (3, 1), (1, 1),
(1, 3), and (1, 5) to determine nondominated solutions.
(b) Use the constraint method by keeping the first objective and using the second
objective as constraint with a variable right-hand side value b2.
(c) Repeat (b) by keeping the second objective and using the first objective as a
constraint with variable right-hand side b1.
Solution:
(a) The different weight combinations result in the solutions shown below.
Weights w Composite objective Solution x
5, 1 Max z ¼ –3x1 – 4x2 0, 0
3, 1 Max z ¼ –1x1 – 2x2 0, 0
1, 1 Max z ¼ 1x1 3, 0 and 3, 2
1, 3 Max z ¼ 5x1 + 2x2 3, 2
1, 5 Max z ¼ 9x1 + 4x2 3, 2
Fig. 3.10 Solution for
Problems 3(a) and 3(c)
140 3 Multiobjective Programming(b) The method generates the noninferior solutions shown below for various values
of b2 in the constraint 2x1 + x2  b2.
b2 x z1
10 0, 0 0
0 0, 0 0
5 2.5, 0 2.5
6 3, 0 3
7 3, 1 4
8 3, 2 5
9 No feasible solution
(c) The method generates the noninferior solutions shown below for various values
of b1 in the constraint x1 + x2  b1.
b1 x z2
10 3, 2 8
5 3, 2 8
4 3, 1 7
3 3, 0 6
2 2, 0 4
1 1, 0 2
0 0, 0 0
1 No feasible solution
Problem 5 (goal programming formulation): A good B is to be blended from
three ingredients I1, I2, and I3. Firm requirements dictate that at least 100 lbs of B are
to be blended, and that the average cost of the blend per pound do not exceed $2.80,
given that one pound of the three ingredients costs $5, $3, and $2 for I1, I2, and I3,
respectively. In addition, it would be desirable if 20% of B were to be I1. Similarly,
Fig. 3.11 Improvement
cones and nondominated
frontier
3.3 Goal Programming 141the decision maker would like to have B consist of no more than 50% of the cheap
ingredient I3, if possible. This desirable feature is about half as important as the
former desirable feature. Formulate as a goal programming problem.
Solution: As there is only a single product, we only need variables with a single
subscript. Define variables x1, x2, and x3 as the quantities of the three respective
ingredients in the product. The constraints can then be written as
x1 þ x2 þ x3  100 and
5x1 þ 3x2 þ 2x3  2:8ð Þ x1 þ x2 þ x3 :
The first of the two desirable properties, written as a constraint is
x1  0:2ð Þ x1 þ x2 þ x3 :
Rewriting as a goal constraint, we obtain
x1 þ d
1  dþ
1 ¼ :2ð Þ x1 þ x2 þ x3
with objective function contribution Min d
1 . The latter desirable property, written as
a constraint, is
x3  :5ð Þ x1 þ x2 þ x3 :
Rewritten as a goal constraint, we obtain
x3 þ d
2  dþ
2 ¼ :5ð Þ x1 þ x2 þ x3
with objective function contribution Min dþ
2 . The objective function is
Min z ¼ 2d
1 þ dþ
2 :
Solving the problem results in 15, 35, and 50 lbs of the three ingredients being
used, so that exactly 100 lbs are blended, whose price is exactly equal to the required
value of 2.8. It is apparent that the product includes 15% I1, 5% short of the desired
target. On the other hand, the upper limit of 50% I3 is satisfied as equation.
142 3 Multiobjective ProgrammingNonlinear Programming 4
In the Introduction to Linear Programming in Sect. 2.1 in this volume, we outlined
that the objective function(s) and the constraints in linear programming are assumed
to be linear functions in the variables. In this chapter, we drop this assumption and
only assume divisibility and the deterministic property. Given that, we can view
nonlinear programming as a generalization of linear programming. Another impor￾tant distinction between linear and nonlinear programming is that in nonlinear
programming, constraints are not necessarily needed to ensure finite optima as is
the case in linear programming. For instance, the nonlinear objective
Min z ¼ x2  6x þ 4y2  16y þ 21
has the optimal solution ð Þ¼ x, y ð Þ 3, 2 , which is at the center of the ellipse with
major axis of 4 and minor axis of 2. Given this, it stands to reason to first investigate
unconstrained nonlinear optimization, before constraints are then introduced later.
We will start our discussion with an introductory section, followed by a number
of applications of nonlinear programming. The third section focuses on formal
aspects, and Sect. 4.4 describes techniques for the solution of unconstrained and
constrained nonlinear programs. An in-depth discussion of nonlinear programming
can be found in Eiselt and Sandblom (2019).
4.1 Introduction
The historical roots of nonlinear programming are difficult to isolate since the study
of many nonlinear mathematical problems can be put into the context of nonlinear
optimization. Noteworthy are the Newton (1643–1727) method for finding roots of
polynomials (used in unconstrained optimization, to be discussed in Sect. 4.4 of this
chapter), as well as the introduction of Lagrange multipliers (Lagrange, 1736–1813)
for constrained problems (closely related to the dual variables in linear program￾ming). As a separate field of study, nonlinear optimization developed quickly since
# The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
H. A. Eiselt, C.-L. Sandblom, Operations Research, Springer Texts in Business and
Economics, https://doi.org/10.1007/978-3-030-97162-5_4
143the 1950s, roughly in lockstep with linear programming. Particularly noteworthy are
the contributions by Kuhn and Tucker (1951) with some of their work based on a
contribution by Karush (1939); see also Davidon (1959), Fletcher and Powell
(1963), and many others.
To formalize our discussion, we will write a general nonlinear optimization
problem as
Min z ¼ f xð Þ 1, x2, ... , xn
s:t:gið Þ x1, x2, ... , xn Ri bi for i ¼ 1, ... , m,
where f and g are some functions, Ri are relations of the type , ¼, or , and bi are
right-hand side constants. Clearly, in case the functions f and gi, i ¼ 1, ..., m are all
linear, we are dealing with a linear programming problem. To simplify the notation,
we will write nonlinear programming problems as Min z ¼ f(x), s.t. g(x) R b. Note
that we typically write nonlinear programs as minimization problems, whereas we
usually maximize in linear programming. Also, we usually assume that the relations
in nonlinear programming are of the type . This is not restrictive, of course, as each
maximization problem can be converted into an equivalent minimization problem,
just as  constraints can be converted to equivalent  constraints. Some specific
transformations are as follows:
• If a variable xj is required to be nonnegative, we simply add the constraint –xj  0,
• a constraint gi(x)  bi with a nonzero right-hand side bi can be rewritten as
gi(x)  bi  0,
• a constraint g(x) ¼ 0 can be replaced by the two opposing constraints g(x)  0
and –g(x)  0.
As a very simple example of a nonlinear optimization model, consider a single
product, whose price depends on the quantity that is produced and sold. More
specifically, let p denote the unit price of the product, let a and b be two positive
parameters, and denote by x the quantity that is produced and sold. The price–
quantity relation could then be p ¼ a  bx, indicating that the more of the good we
produce, the lower its unit price will be. The revenue function is then R(x) ¼ (a  bx)
x, which, if coupled with a simple cost function C(x) ¼ cx with per-unit costs of c,
results in the profit function P ¼ (a  bx)x  cx, where we have to ensure that we
only consider quantities, which are (1) nonnegative, and (2) for which the price
remains nonnegative. This results in the nonlinear optimization problem
P : Max z ¼ ð Þ a  c x  bx2
s:t: 0  x  a=b:
The problem can be visualized in Fig. 4.1, where R(x) is the revenue function,
C(x) the cost function, and z ¼ P(x) denotes the profit function.
144 4 Nonlinear ProgrammingSimple differential calculus reveals that the point of maximal revenue is bx ¼ a
2b,
whereas the point of maximal profit is x ¼ ac
2b , which is always lower than the point
of maximal revenue. Furthermore, in order to ensure nonnegative profits, we need to
have 0  x  ac
b .
Another simple example concerns the problem of designing the shape of an open
cylindrical tank, given a prescribed volume V. Assuming that the material for the
bottom of the tank and its sides is the same, we wish to determine the shape of the
tank that minimizes the material used. In order to formulate the problem, denote by
x the radius and by y the height of the cylinder. This can be visualized in Fig. 4.2.
The volume of the cylinder can then be expressed as V ¼ πx
2
y. The area of the
bottom is πx
2
, and the area of the side is 2πxy, so that the problem can be written as
Fig. 4.1 Revenue and profit functions
Fig. 4.2 Open
cylindrical tank
4.1 Introduction 145P : Min z ¼ πx2 þ 2πxy
s:t: πx2
y ¼ V,
x, y  0,
which is a nonlinear optimization problem with the two variables x and y. Solving
the equality constraint for y and replacing the variable in the objective function
leads to the objective Min z ¼ πx
2 + 2Vx1
. The function has a unique optimum
at x ¼
ffiffiffi
V
π
3
q
. Substituting this value in the volume equation, we obtain y ¼ x ¼
ffiffiffi
V
π
3
q
.
In other words, for an optimal open cylinder, its height should equal its radius. The
next section will examine a number of applications that demonstrate the flexibility
and usefulness of nonlinear formulations.
4.2 Applications of Nonlinear Programming
This section presents some selected applications of nonlinear programming. They
are somewhat closer to reality as compared to the examples presented in the
introductory section. For a more detailed discussion, readers are referred to Eiselt
and Sandblom (2019).
4.2.1 Forestry Logging
One of the major concerns of woodlot management deals with the question of when
harvesting should take place. The optimal time to harvest will depend on the type of
tree we are dealing with, and the soil and climate conditions. To simplify matters, we
assume that clearcutting is used, and monoculture has been chosen, i.e. we will
replant with a single species. We also assume that our objective is to maximize the
average annual biomass harvested from the trees, taken over an infinite planning
horizon. As soon as the parcel is harvested, we will replant, and growth starts again.
Although simplified, the model should shed some light on the planning process in
forest management. Typically, the useful biomass of a tree develops over time
according to an “S”-shaped sigmoidal function such as the one shown in Fig. 4.3.
Starting from zero, the biomass grows first slowly, then rapidly, until growth slows
down, then reaches a peak, and then decay sets in until nothing is left.
Denoting by f(t) a tree’s biomass at a time t, the objective here is to determine the
optimal age t of a tree, when logging should take place. Since we are interested in the
annual production of biomass, averaged over time, our optimization problem is
146 4 Nonlinear ProgrammingP : Max z t>0
¼ f tð Þ=t:
One can show that an optimal solution t to problem P must satisfy the equation
t f 0
ðÞ¼ t fð Þt and that t has an easy geometric interpretation: its value is determined
by constructing a straight line through the origin of Fig. 4.3 that touches the growth
function and has the smallest possible slope. This point maximizes the biomass (the
value on the ordinate) divided by time (the value on the abscissa), which is precisely
the tangent of the straight line we constructed. No straight line with a larger slope
touches the growth function and can thus not be realized. Denoting the time, at
which a tree obtains the maximal biomass by bt (the maximum of the growth
function), then it becomes clear that t < bt, i.e. the best time to harvest is before a
tree has reached its maximal growth. The reason is the slower growth near the point
of maximum biomass.
This type of optimization model occurs in other contexts as well, e.g., the
purchase and maintenance of equipment that deteriorates over time (e.g., school
buses), for which we want to determine the optimal time of replacement.
4.2.2 Maximizing Tax Revenues
Consider a state, in which taxpayers are levied a flat income tax rate x, with x ranging
from 0 (no income tax at all) to 1 (i.e., 100%, viz., confiscation of income). The state
now wishes to determine the tax rate x that maximizes the state’s income R(x).
Clearly, the shape of the function R(x) is crucial. All we know at this point is that
f(t)
t
85 90 95 100
100
105
95
90
85
80
75
70
65
60
55
50
45
40
45 50 55 60 65 70 75 80
t
40
35
30
25
20
15
10
5
−5 5 10 15 20 25 30 35
− t
∧
Fig. 4.3 Biomass of a tree over time
4.2 Applications of Nonlinear Programming 147R(0) ¼ 0, i.e., with a tax rate of zero, the state’s income is zero as well, and that
R(1) ¼ 0, i.e., with a tax rate of 100%, the state’s income will also be zero, as there is
no longer any reason for an individual to work. Small tax rates are not much of a
disincentive to work, whereas for larger tax rates, individuals will choose to spend
their time doing other things, i.e. spend time with their families or including
moonlighting. The shape of the state’s revenue curve will roughly resemble that of
an inverted parabola with a maximal point somewhere between x ¼ 0 and x ¼ 1. The
function R(x) is called the Laffer curve, named after the economist Arthur Laffer,
who famously drew it in 1974 on a napkin (which is on display at the National
Museum of American History in Washington, DC), when discussing American
taxation policy. A Laffer curve is shown in Fig. 4.4.
Without presenting any technical details, the Laffer curve can be constructed by
considering the amount of time an individual is prepared to work as opposed to
spend in leisure pursuits for any given tax rate. Once the curve has been constructed
(this is the difficult part), the state’s optimization problem is then
P : Max z ¼ R xð Þ:
Assume now that x is the unique optimal taxation rate. If the current taxation rate
is less than x, an increase of the taxation rate (to a maximum at x) will increase the
state’s income, while if the present rate of taxation is higher than x, any further
increase will actually lower the state’s income. We should note that there is no
universal optimal taxation rate, different people and nations have different
acceptances of pain.
4.2.3 Optimizing Inventory and Queuing Models
Inventory models, to be discussed in Chap. 12, deal with the problem of determining
the optimal size x of an order, so as to minimize some total cost function TC(x),
which includes the costs of placing an order and receiving the shipment, on the one
hand, and the cost of keeping units in stock. Deferring the discussion of details to the
chapter on the subject, we attempt to solve the optimization problem
Fig. 4.4 Laffer curve
148 4 Nonlinear ProgrammingP : Min TC xð Þ¼ a1
x þ b1x,
where x denotes the order quantity, the parameter a1 is proportional to the unit
ordering costs, and the parameter b1 is proportional to the unit holding cost. The
unconstrained problem can be solved by differential calculus, resulting in x ¼ ffiffiffiffiffiffiffiffiffiffiffi
a1=b1
p . This is the economic order quantity (EOQ), which will be properly
introduced in the chapter on inventory management.
One extension to the basic model allows backorders, which leads to the
two-variable unconstrained problem
P : Min z ¼ a2
x þ b2
ð Þ x  y
2
x þ c2
y2
x ,
with y denoting the maximal allowable shortage and a2, b2, and c2 being known
constants. Partial differentiation (differentiation with respect to one variable at a
time) and setting the expression to zero then determines the optimal solution x ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffi
a2
c2
b2þc2
b2
q
and y ¼
ffiffiffiffiffiffiffiffiffiffiffiffiffiffi
a2
c2
b2
b2þc2
q
. For a full discussion, see Chap. 12 of this volume.
A final application is found in the area of queuing. A more detailed discussion is
found in Chap. 15 of this book. Suppose that on average, λ customers arrive in 1 hr at
a single service station. Based on training and other factors, the service station is
manned by an agent, who can deal with x customers per hour (the service rate). In
order to be feasible, x > λ is required. Knowing that the expected number of
customers in the system is λ
xλ , a cost of ax for the service station (capturing the
fact that a faster server is more expensive) and unit costs c for keeping a customer in
the system for an hour, minimizing the total cost of the system results in the problem
P : Min TC xð Þ¼ ax þ cλ
x  λ , s:t: x > λ:
Using again differential calculus, we find that the unique optimal solution is given
by x ¼ λ þ
ffiffiffi
cλ
a
q
, which also satisfies the single constraint x > λ.
4.3 Properties of Nonlinear Programming Problems
In dealing with a nonlinear optimization problem, there are two important questions
to be answered.
• Given a feasible solution, how can we test it for optimality? and
• How do we find an optimal solution?
We will relegate a discussion of the second question to the next section and
discuss here how to perform an optimality test. First, consider the unconstrained
4.3 Properties of Nonlinear Programming Problems 149case, i.e., the case, in which a problem has an objective function but no
constraints. For simplicity, assume that we are minimizing a differentiable function
f(x) of a single variable x. A fundamental result from differential calculus is then
that if x satisfies f 0
ð Þ¼ x 0 and f 00ð Þx > 0, then the function f has a local minimum
(for a definition, see, e.g., Chap. 17) at x. As an example, consider the function
f(x) ¼ x
3  3x, pictured in Fig. 4.5.
This function has a local minimum at x ¼ 1, for which fð Þ¼ x fð Þ¼ 1 2, but it is
apparent that there are values of x, for which its functional value is less than 2, for
instance, x ¼ 3, for which f(3) ¼ 18. In fact, the function f(x) does not even
have a global minimum (a smallest value that the function f can take), since with ever
smaller negative values of x, there is no lower bound on the value of f(x). However,
for a convex function (i.e., a function, with the property that all points on a linear line
segment—the chord—between any of its two points is located above the function
itself, see, e.g., Fig. 4.6), any local minimum will necessarily be global. In our case,
the function is convex in [0, 1[, where x ¼ 1 is the minimal point.
In many situations that involve nonconvex functions we will consider that part of
the function in the region within which it is convex. For functions of several
variables, there are results similar to and extensions of the one-dimensional case.
Consider now the case, in which constraints are present. For pedagogical reasons,
we will assume that the objective function is quadratic, and the constraints are linear.
Such a case is referred to as a quadratic programming problem. Furthermore, let us
consider a problem with two variables and a convex objective. Momentarily
disregarding the constraints, the overall minimal point is bx, and we define the optimal
solution that respects the constraints by x . Figure 4.7 then shows three possible
cases: Either bx is located in the feasible region (so that bx ¼ x), in which case the
constraints are irrelevant as they are satisfied anyway, or x is located on the boundary
of the feasible region, or x is at a corner point of the feasible set. The latter case
occurs in linear programming.
A test that determines the optimality of a solution is provided by the so-called
Karush–Kuhn–Tucker (KKT) conditions. They are necessary and sufficient
conditions for the optimality of a convex differentiable nonlinear programming
problem.
Fig. 4.5 The function
f(x) ¼ x
3  3x
150 4 Nonlinear ProgrammingIn the simplest case, we start with a problem of the type Min z ¼ f(x1, x2, ..., xn),
s.t. gi(x1, x2, ..., xn) ¼ 0 for all i, where f is a convex function and gi denotes the
difference between the left-hand side and the right-hand side of the i-th constraint,
where all m constraints are assumed to be equations. We then set up the Lagrangean
function L(x1, x2, ..., xn; u1, u2, ..., um), which is the sum of the minimization
objective minus the sum of the constraints, each of which is weighted by a
Lagrangean multiplier ui. The KKT necessary optimality conditions in this case
simply state that the derivative of the Lagrangean function with respect to each of the
variables including the Lagrangean multipliers must equal zero. Solving this system
of simultaneous equations then finds an optimal solution.
Fig. 4.6 Convex function and global minimum at x
Fig. 4.7 Different cases for quadratic programming
4.3 Properties of Nonlinear Programming Problems 151As an example, consider the problem
P : Min z ¼ 4x2
1  6x1x2 þ 3x2
2  12x1  6x2
s:t: x1 þ x2  12 ¼ 0:
Note that in this formulation, the variables are unconstrained, i.e., they may be
positive or negative. The Lagrangean function is then
L xð Þ¼ 1, x2, u 4x2
1  6x1x2 þ 3x2
2  12x1  6x2  ð Þ x1 þ x2  12 u:
The KKT conditions are then
8x1  6x2  12  u ¼ 0,
6x1 þ 6x2  6  u ¼ 0, and
x1 þ x2  12 ¼ 0,
where the last equation is nothing but the original constraint. Solving this system of
simultaneous linear equations results in the solution x1 ¼ 5:7692, x2 ¼ 6:2308 with
an objective value of z ¼ 72:6923.
The Karush–Kuhn–Tucker conditions can also be formulated for the considerably
more general case, in which the constraints are inequalities rather than equations. A
rich and fruitful theory can then be built around these conditions involving sensitiv￾ity, postoptimality, and duality aspects. Such considerations are beyond the scope of
this book and readers are referred to specialized texts on the subject, see, e.g., Eiselt
and Sandblom (2019).
As mentioned earlier, many other solution techniques exist. Of special interest
may be SUMT (Sequential Unconstrained Minimization Technique) methods, whose
basic idea is to eliminate the constraints in a model by allowing them to be violated,
albeit with a penalty term in the objective function. This idea is reminiscent of goal
programming, where we minimize the over- and/or underachievement of a constraint
in the objective function as well; see, e.g., Sect. 3.3 in this volume.
4.4 Solution Methods for Nonlinear Optimization
Unlike linear programming, where there are only a few practically relevant solution
techniques, there are myriads of different solution methods for nonlinear optimiza￾tion problems. Many algorithms exist for constrained and unconstrained problems,
most tailored to a specific type of nonlinear function. This volume describes two
techniques, Newton’s method for unconstrained minimization and the Frank and
Wolfe method for linearly constrained convex models.
152 4 Nonlinear Programming4.4.1 Newton’s Method for Unconstrained Optimization
Consider the unconstrained problem
P : Min z ¼ f xð Þ,
where f is a single-variable, twice differentiable convex function. Our description of
Newton’s method will rely heavily on geometrical arguments; details are shown in
Fig. 4.8. The general idea is to generate a sequence of points x1, x2, x3, ..., which are
estimates of the true optimal solution x. Since x is characterized by f 0
ð Þ¼ x 0, we
attempt to solve this equation.
Given some starting estimate x1, we construct the line ‘1 as a tangent to the curve
f
0
(x) at the point (x1, f
0
(x1)). The next estimate of x is at x2, where the line ‘1 intersects
the abscissa. Pursuing this argument further, we find the next approximation x3 at the
intersection of the tangent line ‘2 with the abscissa. This process terminates when￾ever the steps between successive approximations are sufficiently small, or some
other termination criterion is satisfied. The general iteration formula is
xkþ1 ¼ xk  f 0
ð Þ xk
f 00ð Þ xk
:
As a numerical example, consider again the function f(x) ¼ x
3  3x in Fig. 4.5. As
f
0
(x) ¼ 3x
2  3 and f
00
(x) ¼ 6x, Newton’s iteration formula is
xkþ1 ¼ xk  3xk
2  3
6xk
¼ 1=2xk þ
1
2xk
:
y =f’(x) y =f’(x1)
(x1,f’(x1))
(x2,f’(x2))
x
− x3
l2
l1
x2 x1
x
Fig. 4.8 Newton’s
minimization method
4.4 Solution Methods for Nonlinear Optimization 153Arbitrarily starting with the initial estimate x1 ¼ ½, we obtain x2 ¼ 1.25,
x3 ¼ 1.025, x4 ¼ 1.0003049, and we see that the sequence converges to the true
value of 1 very rapidly.
We should note that Newton’s method for minimization problems described
above is equivalent to the Newton–Raphson method for solving equations—here,
we actually solve the equation f
0
(x) ¼ 0 for x.
4.4.2 Constrained Optimization
This section describes the Frank and Wolfe (1956) method for the solution of
linearly constrained optimization problems. To formalize, consider the problem
P : Min z ¼ f xð Þ 1, x2, ... , xn
s:t: a11x1 þ a12x2 þ ⋯ þ a1nxn  b1,
a21x1 þ a22x2 þ ⋯ þ a2nxn  b2,
⋮
am1x1 þ am2x2 þ ⋯ þ amnxn  bm:
For simplicity, we will consider the case of two variables, the extension to
n variables is straightforward. We will assume that the function f is convex and
differentiable, and that the feasible set is bounded. Each iteration of the method
proceeds in two steps. The first step linearizes the objective function in the vicinity of
the present interior point, and then solves the resulting linear programming problem,
while in the second step we solve a single-dimensional interpolation problem that
finds the next improved trial point.
Starting with an arbitrary initial interior feasible point x1
1, x1
2
 , we first solve the
auxiliary linear programming problem P‘
, which is obtained by linearizing the
objective function in the vicinity of the initial interior point x1
1, x1
2
 , i.e., we solve
Pℓ
1 : Min z
ℓ
1 ¼ ∂f x1
1, x1
2
 
∂x1
 x1 þ
∂f x1
1, x1
2
 
∂x2
 x2
s:t: the given constraints,
where, as usual, ∂f/∂x denotes the partial derivative of the function f with respect to
the variable x. This is a linear programming problem whose solution we denote by
ex
1
1,ex
1
2
  , which by virtue of Dantzig’s theorem, must be on the boundary of the
feasible set. Let us assume the problem is nondegenerate, so that we have an optimal
solution at an extreme point.
The next step performs an interpolation search between the points x1
1, x1
2
  and
ex
1
1,ex
1
2
 , i.e. we determine the point on the line segment between these two points
that has the smallest value of the original objective function f. This is accomplished
by solving the one-dimensional interpolation minimization problem
154 4 Nonlinear ProgrammingPλ
1 : Min
0λ1
z
λ
1 ¼ f λx1
1 þ ð Þ 1  λ ex
1
1, λx1
2 þ ð Þ 1  λ ex
1
2
 :
Denoting the solution to this problem by λ1, we then define the next estimate
x2
1, x2
2
  of an optimal solution to P by
x2
1≔λ1x1
1 þ ð Þ 1  λ1 ex
1
1 and
x2
2≔λ1x1
2 þ ð Þ 1  λ1 ex
1
2
and the process is repeated, until some stop criterion is satisfied.
As an illustration of the procedure, we will use the convex optimization problem
P : Min z ¼ x2
1 þ x2
2 þ
16
x1
 4x2
s:t:  2x1 þ x2  0
x1 þ x2  3
x1, x2  0:
Disregarding the constraints, the unconstrained optimal solution is ð Þ¼ bx1bx2
ð Þ 2, 2 , which violates the second constraint, i.e. it is not a feasible point. Hence,
the true optimal solution must be located on the boundary of the feasible set. We now
arbitrarily select x1
1, x1
2
  ¼ ð Þ 1, 1 as our interior starting point. We find ∂f
∂x1 ¼
2x1  16
x2
1
and ∂f
∂x2 ¼ 2x2  4, so that the first auxiliary linear programming problem is
Pℓ
1 : Min z
ℓ
1 ¼ 14x1  2x2
s:t:  2x1 þ x2  0
x1 þ x2  3
x1, x2  0,
which has the unique optimal solution ex
1
1 ¼ 3 and ex
1
2 ¼ 0 . Therefore, the first
interpolation problem is
Pλ
1 : Min
0λ1
z
λ
1 ¼ fð Þ λ þ ð Þ 1  λ 3, λ :
We find that zλ
1 ¼ fð Þ¼ 3  2λ, λ ð Þ 3  2λ 2 þ λ2 þ 16
32λ  4λ , which attains its
minimal value at λ1 ¼ 0.604. Therefore, the next estimate of the optimal solution will
be x2
1 ¼ 1λ1 þ 3 1ð Þ¼  λ1 1:792, and x2
2 ¼ 1λ1 þ 0 1ð Þ¼  λ1 0:604:
The next iteration computes ∂f
∂x1 ¼ ð Þ2 ð Þ 1:792 16
1:792  1:3985 and ∂f
∂x2 ¼
ð Þ2 ð Þ 0:604 4 ¼ 2:792; the second auxiliary linear programming problem is then
4.4 Solution Methods for Nonlinear Optimization 155Pℓ
2 : Min z
ℓ
2 ¼ 1:3985x1  2:792x2
s:t:  2x1 þ x2  0
x1 þ x2  3
x1, x2  0,
which has the unique optimal solution ex
2
1 ¼ 1 and ex
2
2 ¼ 2. The second interpolation
problem is then
Pλ
2 : Min
0λ1
z
λ
2 ¼ fð Þ 1:792λ þ ð Þ 1  λ 1, 0:604λ þ ð Þ 1  λ 2 ,
so that zλ
2 ¼ fð Þ 1 þ 0:792λ, 2  1:596λ , and the process continues as shown in
Fig. 4.9. It should be noted that the convergence to the true optimal solution at
x1 ¼ 1:7728 and x2 ¼ 1:2272 with z ¼ 8:7653 on the boundary of the feasible
region is rather slow.
Exercises
Problem 1 (Lagrangean function and KKT conditions): Consider the problem
P : Min z ¼ ð Þ x1  1 2 þ ð Þ x2  2 2 þ ð Þ x3  3 2
s:t: x2
1 þ x2
2 þ x2
3 ¼ 1
and solve it, using the Karush–Kuhn–Tucker conditions.
Solution: The Lagrangean function is
Fig. 4.9 Example for the
Frank and Wolfe method
156 4 Nonlinear ProgrammingL xð Þ¼ 1, x2, x3, u ð Þ x1  1 2 þ ð Þ x2  2 2 þ ð Þ x3  3 2  x2
1 þ x2
2 þ x2
3  1  u,
so that the Karush–Kuhn–Tucker conditions can be written as
2ð Þ x1  1 2x1u ¼ 0,
2ð Þ x2  2 2x2u ¼ 0,
2ð Þ x3  3 2x3u ¼ 0,
x2
1 þ x2
2 þ x2
3  1 ¼ 0,
which can be simplified to x1 ¼ 1
1u , x2 ¼ 2
1u , x3 ¼ 3
1u , and x2
1 þ x2
2 þ x2
3 ¼ 1 ,
which has the solution x1 ¼ 1ffiffiffiffi
14 p , x2 ¼ 2ffiffiffiffi
14 p , x3 ¼ 3ffiffiffiffi
14 p (as well as the negative values
thereof). Inserting each of the two solutions into the objective function, we determine
that the problem has a unique optimal solution with all values of the variables being
positive, i.e., x1 ¼ 1= ffiffiffiffiffi
14 p  0:26726, x2 ¼ 2= ffiffiffiffiffi
14 p  0:53452, and x3 ¼
3= ffiffiffiffiffi
14 p  0:80178 with the objective function value z ¼ 7:5167.
Problem 2 (Solve a one-dimensional unconstrained optimization problem):
Using the Newton method, find the solution to the unconstrained problem
P : Min z ¼ f xð Þ¼ x3
6  2x2  ½x þ π,
using the starting point x1 ¼ 5 and follow the process until the method converges,
using four decimal places for accuracy.
Solution: Since f
0
(x) ¼ ½x
2  4x  ½ and f
00
(x) ¼ x  4, Newton’s iteration
formula will take the form xkþ1 ¼ xk  f 0
ð Þ xk
f 00ð Þ xk ¼ xk  ½x2
k4xk½
xk4 ¼ x2
kþ1
2xk8 . Setting
x1 ¼ 5, we obtain x2 ¼ 13, x3 ¼ 9.4444, x4 ¼ 8.2834, x5 ¼ 8.1261, x6 ¼ 8.1231,
and x7 ¼ 8.1231, and the method has converged.
Problem 3 (Solve a convex constrained minimization problem): Consider the
following constrained nonlinear optimization problem
P : Min z ¼ 2x1  x2 þ
3
2x1
þ x2
2
2x1
s:t:  4x1 þ x2  0
6  x1  10
x2  0:
Solve the problem P via the Frank and Wolfe method, starting with the interior
feasible point x1
1, x1
2
  ¼ ð Þ 8, 15 and perform two complete iterations, i.e. compute
the points x2
1, x2
2
  and x3
1, x3
2
 .
4.4 Solution Methods for Nonlinear Optimization 157Solution: We find ∂f
∂x1 ¼ 2  3
2x2
1
 x2
2
2x2
1
and ∂f
∂x2 ¼ 1 þ x2
x1 , and therefore the
unconstrained minimal point for the problem P is ð Þ¼ bx1,bx2 ð Þ 1, 1 , which is not
feasible. On the other hand, we can easily check that x1
1, x1
2
  ¼ ð Þ 8, 15 is an interior
feasible point, see Fig. 4.10.
At the point x1
1, x1
2
  ¼ ð Þ 8, 15 , we obtain ∂f
∂x1 ¼ 7=32 ∂f
∂x2 ¼ 7=8, so that after
scaling the objective function, the first auxiliary linear programming problem is
Pℓ
1 : Min z
ℓ
1 ¼ x1 þ 4x2
s:t:  4x1 þ x2  0
6  x1  10
x2  0,
which has the unique optimal solution ex
1
1,ex
1
2
  ¼ ð Þ 6, 0 , which is the point located in
the southwest corner of the feasible region. The interpolation search is then
Pλ
1 : Min
0λ1
z
λ
1 ¼ fð Þ 6 þ 2λ, 15λ ,
which has the solution λ1 ¼ 0.3473, so that x2
1, x2
2
  ¼ ð Þ 6:6946, 5:2095 . At this
point, we find that ∂f
∂x1 ¼ 1:664 and ∂f
∂x2 ¼ 0:222. The resulting second auxiliary
problem is then
Pℓ
2 : Min z
ℓ
2 ¼ 1:664x1  0:222x2
s:t:  4x1 þ x2  0
6  x1  10
x2  0,
Fig. 4.10 Illustration of the
Frank and Wolfe problem
158 4 Nonlinear Programmingwhich has a unique optimal solution ex
2
1,ex
2
2
  ¼ ð Þ 6, 24 in the northwest corner of the
feasible set. The interpolation search is then
Pλ
1 : Min
0λ1
z
λ
2 ¼ fð Þ 6 þ 0:6946λ, 24  18:7905λ ,
which has a solution λ2 ¼ 0.90602, from which we calculate x3
1, x3
2
  ¼
ð Þ 6:6293, 6:9754 , at which time we stop, having completed two full iterations of
the technique. It should be noted that the true optimal solution is ð Þ¼ x1, x2 ð Þ 6, 6 .
References
Davidon WC (1959) Variable metric method for minimization. Research and Development Report
ANL-5990 (Rev.) Argonne National Laboratory, U. S. Atomic Energy Commission
Eiselt HA, Sandblom C-L (2019) Nonlinear programming. Springer, Berlin
Fletcher R, Powell MJD (1963) A rapidly convergent descent method for minimization. Comput J
6:163–168
Frank M, Wolfe P (1956) An algorithm for quadratic programming. Nav Res Logist Q 3:95–110
Karush W (1939) Minima of functions of several variables with inequalities as side conditions. MS
Thesis, Department of Mathematics, University of Chicago, IL
Kuhn HW, Tucker AW (1951) Nonlinear programming. In: Proceedings of 2nd Berkeley sympo￾sium. University of California Press, Berkeley, CA, pp 481–492
References 159Integer Linear Programming 5
Not too long after more and more applications of linear programming were devel￾oped it became apparent that in some of these applications, the variables would not
be able to attain just any (nonnegative) value but should be integers. As a simple
example, if a variable has been defined to denote the number of cans of beans
manufactured in the planning period, then surely it would make no sense to make,
say, 1,305,557.3 cans: the last 0.3 cans would have to be rounded up or down. While
this may be an acceptable practice when dealing with this application (after all, it
makes very little difference whether or not we make 0.3 cans more or less), in other
applications this may make a huge difference. For instance, assigning airplanes to
routes or trucks to deliveries may very well make the difference between gain and
loss. Furthermore, simply rounding up or down a noninteger (usually referred to as a
continuous solution) will not necessarily result in an optimal integer solution. We
will demonstrate this fact below.
Even though the difference may be blurry, it may be useful to distinguish between
variables that are naturally required to be integer (such as the number of trucks to be
used for deliveries, the number of work crews dispatched to a construction site, or
the number of drums of hazardous material shipped from one site to another), and the
so-called logical variables that also must be integer and are introduced for logical
reasons. Below, we will provide a number of examples of the latter.
Integer programming problems were first discussed by Ralph E. Gomory in the
1950s, who also devised a solution technique for them. Gomory’s class of
techniques is called cutting plane techniques, and we will describe their basic idea
below. A subsequent breakthrough is the 1961 contribution by Land and Doig,
whose branch-and-bound idea remains the basis of most solution methods for
integer programming problems to this day.
# The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
H. A. Eiselt, C.-L. Sandblom, Operations Research, Springer Texts in Business and
Economics, https://doi.org/10.1007/978-3-030-97162-5_5
1615.1 Definitions and Basic Concepts
In order to define integer programming problems, we will start with a standard linear
programming problem. As an example, consider the problem
P1 : Max z ¼ 10x1 þ 11x2
s:t: 3x1 þ 5x2  15
5x1 þ 2x2  10
x1, x2  0:
The only difference between the standard linear programming problem above and
an integer programming problem is that some or all of the variables, in addition to be
required to be nonnegative, are also required to be integer. It is very helpful to think
of the integrality condition as an additional requirement. All-integer programming
problems that require some, but not all, variables, to be integer, are called mixed￾integer linear programming (or MILPs) problems, while problems, in which all
variables are required to assume integer values, are called all-integer linear pro￾gramming (or AILP) problems.
If we replace the two nonnegativity constraints in the above example by x1 
0, and x2  0 and integer, then we have a mixed-integer programming problem,
which we may call P2. Similarly, if we replace the nonnegativity constraints in P1 by
the conditions x1  0 and integer and x2  0, we have another mixed-integer linear
programming problem P3. Finally, if we replace in P1 both nonnegativity constraints
by x1  0 and integer as well as x2  0 and integer, we then have the all-integer
linear programming problem P4.
The graphical representations of the feasible sets of the four problems P1, P2, P3,
and P4 are shown in Figs. 5.1a–d. Figure 5.1a shows the linear programming
problem P1 with the feasible set shaded. The mixed-integer linear programming
problem (MILP) P2 is shown in Fig. 5.1b; here, only the bold horizontal bars are
feasible, as only those guarantee that the variable x2 is integer, while in addition,
respect the given constraints. Similarly, only the bold vertical bars in Fig. 5.1c are
feasible for the mixed-integer linear programming problem P3. Finally, the feasible
set of the all-integer linear programming (AILP) problem P4 consists exclusively of
the grid points shown in Fig. 5.1d. Clearly, the feasible set of this problem is
smallest, as it is the most constrained. Incidentally, the optimal solutions of the
four problems are as follows:
P1 : x ¼ ð Þ 1:0526, 2:3684 with z ¼ 36:5790
P2 : x ¼ ð Þ 1:2, 2 with z ¼ 34,
P3 : x ¼ ð Þ 1, 2:4 with z ¼ 36:4, and
P4 : x ¼ ð Þ 0, 3 with z ¼ 33:
Again, it is apparent that moving from the least restricted problem P1 on to the
more restricted problems P2 and P3 to the most restricted problem P4, the values of
162 5 Integer Linear Programmingthe objective function are getting worse, i.e., they decrease in maximization
problems, while they would increase in minimization problems. Also, even in this
small example, we notice that simple rounding of the continuous solution of P1 does
not result in the all-integer solution of P4.
Furthermore, it is important to realize that none of the above integer programming
problems has its optimal solution at an extreme point, which is always the case in
linear programming (recall Dantzig’s corner point theorem) from Sect. 2.3.1.
In order to highlight the differences between linear and integer linear optimiza￾tion, consider the following all-integer linear programming problem P5:
Fig. 5.1 (a)–(d) Feasible sets for P1, P2, P3, and P4
5.1 Definitions and Basic Concepts 163P5 : Max z ¼ 2x1 þ x2
s:t: 7x1 þ 48x2  84
x1 þ 12x2  3
x1, x2  0 and integer:
The two solid lines in Fig. 5.2 show the two constraints (ignore the broken line for
now), and the bold dots indicate the all-integer solutions. The optimal solution of the
linear programming problem without the integrality requirements is x ¼
ð Þ 6:5455, 0:7955 with z ¼ 13:8864, while the optimal solution of the all-integer
programming problem is x ¼ ð Þ 5, 1 with z ¼ 11. This is a solution that cannot be
obtained by simple rounding.
Worse yet, if we were to change the right-hand side value of the second constraint
from “3” to “13,” one of the constraints in Fig. 5.2 moves in parallel fashion to the
broken line. The feasible set for the linear programming problem without integrality
conditions is the triangle with the vertices (0, 11=12), (0, 1¾), and the point marked
with x0
LP , which has coordinates (2.9091, 1.3258) and a value of the objective
function of 7.1439. The all-integer linear programming problem does, however,
have no feasible solution (notice that there are no bold dots in the triangle described
above).
Finally, consider the feasible set generated by the following constraints:
x1 þ x2  10
5x1 þ 3x2  15
x1  6
x2  7
x1, x2  0:
The feasible set is shown in Fig. 5.3.
X2
2
1
−1 1
−
−3 5 10 X1
XLP
XLP
Fig. 5.2 Graph for Problem P5
164 5 Integer Linear ProgrammingInspection reveals that the feasible set has extreme points (3, 0), (6, 0), (6, 4),
(3, 7), (0, 7), and (0, 5). In other words, all corner points of the feasible set are
integer-valued. If we were to solve a linear programming problem with this feasible
set and any arbitrary objective function, at least one optimal solution would be―as
proved by Dantzig’s “corner point theorem”―one of these points, and thus integer
without us actually requiring it. There are some large classes of linear programming
formulations that fall into this category, i.e. having integer optimal solutions without
explicitly including integer requirements. This is a very appealing feature, since
linear programming problems are generally much easier to solve than their integer
programming counterparts. We will encounter this type of problem in the chapter on
network models.
The remainder of this section will discuss the relations between the objective
function values of linear programming problems and those of their integer linear
programming problems. In order to facilitate the discussion, we need to introduce
some terminology. Consider any mixed- or all-integer linear programming problem
P. Its linear programming relaxation Prel is the very same problem P except with all
of the integrality conditions deleted. Suppose that P is a maximization problem, and
its optimal value of the objective function is zIP . Let now zLP denote the optimal
value of the objective function of its linear programming relaxation, then we find that
zIP  zLP:
The reason for this relation is easy to see. Starting with the relaxation, we obtain
the integer programming problem by adding constraints, namely the integrality
constraints. And whenever constraints are added to a problem, the optimal value
of the objective function gets worse, i.e. lower in case of maximization problems and
higher in case of minimization problems. And that is exactly what is captured by the
above relation. Note that the inequality is reversed in case of minimization problems.
In other words, the linear programming relaxation provides an upper bound on the
optimal value of the objective function of the integer programming problem in case
Fig. 5.3 Feasible set with
integer-valued extreme points
5.1 Definitions and Basic Concepts 165of maximization problems, while it is a lower bound in case of minimization
problems.
As a numerical example, consider the all-integer linear programming problem P5
introduced earlier in this section. The value of the objective function of the
all-integer solution was found to be zIP ¼ 11, while the linear programming relaxa￾tion had an optimal objective value of zLP ¼ 13:8864.
Not only does the numerical example satisfy the inequality shown above as
expected, but it also leads us to define the gap between the objective values of an
integer programming problem and its linear programming relaxation. Formally, the
absolute integrality gap is defined as the difference j j zLP  zIP , while the relative
integrality gap is defined as j j zLPzIP
max f g j j zLP , j j zIP with the relative gap defined as 0, in case
zLP ¼ zIP ¼ 0. In case we want to normalize the measure, we can use the expression
j j zLPzIP
max f g j j zLP , j j zIP , j j zLPzIP , which is always defined in the [0, 1] interval.
In the numerical example, the absolute integrality gap is 13.8864  11 ¼ 2.8864,
while the relative integrality gap is 2.8864/13.8864 ¼ .2079. The relative integrality
gap is usually a good indicator of the degree of difficulty of the problem. Any
problem with a relative integrality gap in excess of 0.1 or 10% is fairly difficult,
while problems with integrality gaps in excess of 0.5 or 50% are typically really
difficult. The integrality gap can, of course, only be computed after the problem and
its relaxations have been solved, thus diminishing its usefulness. However, often
good approximations for zIP are known (or zIP is replaced by the best-known integer
solution) and zLP is usually not too difficult to compute.
A related expression is the optimality gap, which, in case a problem was not or
could not be solved to optimality, measures how far the best-known solution
deviates from the best-known bound (upper for maximization and lower for minimi￾zation). The actual expression replaces zLP and zIP by the best-known bound and
best-known integer solution in the above expression.
While the linear programming relaxation is usually the first step to solve an
integer programming problem, other types of relaxations exist. Most prominent
among them is the so-called Lagrangean relaxation PLagr. In order to set up this
relaxed problem (which we assume has a maximization objective), we first select
some or all of the given structural constraints. These constraints, usually called
dualized constraints, are rearranged to be in the form LHS  0 and then multiplied
by some preselected nonnegative constants (referred to as the Lagrangean
multipliers or dual variables) and subtracted from the objective function. Further￾more, the dualized constraints are removed from the set of constraints.
In order to illustrate the process, we will again refer to problem P4, repeated here
for convenience:
166 5 Integer Linear ProgrammingP4 : Max z ¼ 10x1 þ 11x2
s:t: 3x1 þ 5x2  15
5x1 þ 2x2  10
x1, x2  0 and integer:
First we set up its Lagrangean relaxation by dualizing the first constraint, using
the Lagrangean multiplier bu1 ¼ 2. The relaxed problem is then
P1
Lagr : Max z
1
Lagr ¼ 10x1 þ 11x2  2 3ð Þ x1 þ 5x2  15
s:t: 5x1 þ 2x2  10
x1, x2  0 and integer:
This problem has an optimal solution bx1 ¼ 2, bx2 ¼ 0 with a value of the objective
function of bz
1
Lagr ¼ 38.
Alternatively, if we were to dualize the second constraint instead with, say, a
Lagrangean multiplier of bu2 ¼ 3, we would obtain a different relaxed problem, viz.,
P2
Lagr : Max z
2
Lagr ¼ 10x1 þ 11x2  3 5ð Þ x1 þ 2x2  10
s:t: 3x1 þ 5x2  15
x1, x2  0 and integer:
The optimal solution for this problem is bx1 ¼ 0 , bx2 ¼ 3 with a value of the
objective function of bz
2
Lagr ¼ 45.
Yet another possibility would be to dualize both constraints, which would leave
us with a problem that maximizes the objective function 10x1 þ 11x2 
bu1 ð Þ 3x1 þ 5x2  15 bu2 ð Þ 5x1 þ 2x2  10 subject to only the nonnegativity
constraints and the integrality requirements for both variables. If we were to choose
multipliers bu1 ¼ 2 and bu2 ¼ 1 , the objective function reduces to Max z3
Lagr ¼
x1  x2 þ 40, which has an optimal solution bx1 ¼ 0 and bx2 ¼ 0 with objective
function value bz
3
Lagr ¼ 40 . On the other hand, choosing multipliers bu1 ¼ 1 and
bu2 ¼ 2 results in a problem with an objective function Max z4
Lagr ¼ 3x1 þ 2x2 þ
35, which has unbounded “optimal” solutions (as the variable x2 can be chosen
arbitrarily large).
It is easy to show that the optimal value of the objective function bzLagr of a
Lagrangean relaxation is always greater than or equal to the value of the objective
function z of the original (maximization) problem. The main idea is this. Dualizing
any subset of constraints of the type LHS  0 increases the feasible set, allowing
solutions with higher value of the original objective function. In addition to the
removal of some constraints from the problem, the objective function of the
Lagrangean problem subtracts bu LHS from the original objective. By definition,
bu  0 and LHS  0, so that their product is a nonpositive number, which is
subtracted, so that the value of the objective function can be larger than that of the
original problem.
5.1 Definitions and Basic Concepts 167It has become apparent in the above discussion that the choice of the Lagrangean
multipliers is crucial in determining how close the gap between bzLagr and z, the
optimal objective function value of the original problem P, will be. In the context of
linear programming, we can show that by selecting the Lagrangean multipliers to be
equal to the optimal values of the dual variables, the inequality bzLagr  z is actually
satisfied as an equation, i.e. bzLagr ¼ z. We will exploit this idea further in a heuristic
method in Sect. 5.3.3 below.
In some problems, integrality does not occur naturally but is a result of the way
we must formulate constraints. Some of these formulations use the so-called logical
variables, which are zero-one variables that are introduced to model logical
implications that cannot be modeled in the usual variables.
In order to explain some of the intricacies, assume that we are facing a
manufacturing problem, in which one of the tasks is to decide which k of the
p possible machines we should lease or purchase. Defining the variables is easy:
For each of the p machines, we define a binary variable yj, j ¼ 1, ..., p, which
assumes a value of one, if we purchase/lease the machine and zero otherwise. This
part of the formulation is straightforward. However, we need to ensure that the
constraints related to each of the machines, e.g., capacity constraints and others, only
apply if we have purchased/leased the machine, otherwise they are obviously
irrelevant. This can be achieved as follows. If there exists a constraint relating to
any one of the p machines, we add to/subtract from the right-hand side a specific
amount as follows: If the original constraint is LHS  RHS, we reformulate it as
LHS  RHS + (y – 1)M, and if the original constraint were LHS  RHS, we
reformulate it as LHS  RHS – (y – 1)M, where M is a sufficiently large number.
The reason for this reformulation is as follows. Suppose that we purchase/lease a
machine. The variable associated with this machine then assumes a value of y ¼ 1, in
which case the constraints for this machine appear after the reformulation exactly as
before, since the added/subtracted term equals zero. In other words, the machine
constraints must hold when we purchase/lease the machine. Suppose now that the
solution does not include a specific machine. This means that the variable y assumes
a value of 0, and the reformulation guarantees that the constraints, which belong to
this machine are redundant. This is the idea behind the reformulation.
In general, we distinguish between different classes of logical constraints, i.e.,
constraints involving logical variables. In case of a dichotomy, i.e., either one
constraint holds or another constraint, we talk about either-or constraints, or, in
case that we need to choose k out of p machines, we refer to “k out of p constraints.”
Clearly, for k ¼ 1 and p ¼ 2, either-or constraints result as a special case. Another
class is that of conditional constraints. They can be spotted by their unique wording
“if this, then that.” Examples of these conditions will follow.
One simple way to formulate problems involving logical constraints uses a small
table that enumerates all possible solutions of the variables or activities involved in a
problem and determines whether or not they are acceptable. This will allow us to
decide which solutions to exclude. In order to demonstrate this, consider first the
case of two zero-one variables y1 and y2 which take the value of one if the
168 5 Integer Linear Programmingcorresponding activity is undertaken and zero if it is not. There are only four possible
solutions and they are shown in Table 5.1.
Table 5.2 shows the exclusion constraints that force the exclusion of one specific
solution without affecting any of the others.
Similarly, in the case of three variables (i.e., activities), we have the following
eight solutions, shown in Table 5.3.
Table 5.4 shows how to formulate a logical constraint causing a single solution to
be excluded.
Note that the wording offered in Table 5.4 is not necessarily unique. For instance,
in case Solution 1 is to be excluded, this could also be based on the statement “If y1
and y2 are both not chosen, then y3 must be chosen.” Or, for instance, if Solution 3a is
to be excluded, we could state that “If y2 and y3 are both chosen, then y1 must be
chosen as well.”
In case more than a single solution is to be excluded, we simply combine the
pertinent logical constraints. As an example, consider the case of two variables and
exclude the solutions #1 and #2a. This task can be achieved by employing the two
constraints y1 + y2  1 and y1  y2. Simple inspection reveals that these two
constraints can be collapsed into the single constraint y1 ¼ 1. In such a case, we
Table 5.1 All solutions
for two binary variables/
activities
# y1 y2
10 0
2a 0 1
2b 1 0
31 1
Table 5.2 Exclude a single solution in case of two variables
Solution to be
excluded Formulation Wording
1 y1 + y2  1 “At least one of the two activities needs to be chosen”
2a y1  y2 “If activity y2 is chosen, then activity y1 must also be
chosen”
2b y1  y2 “If activity y1 is chosen, then activity y2 must also be
chosen”
3 y1 + y2  1 “At most one of the activities can be chosen”
Table 5.3 All solutions
for three binary variables/
activities
# y1 y2 y3
10 0 0
2a 0 0 1
2b 0 1 0
2c 1 0 0
3a 0 1 1
3b 1 0 1
3c 1 1 0
41 1 1
5.1 Definitions and Basic Concepts 169could simply replace y1 by the value of 1 everywhere in the problem and thus reduce
the size of the model.
Another logical constraint may require that an activity y0 can only be chosen if at
least k activities in a set J ¼ {y1, y2, ..., yp} are also chosen. This can be
formulated as
y0  1
k y1 þ y2 þ ... þ yp
 :
5.2 Applications of Integer Programming
This section will introduce some classes and principles of applications of integer
programming problems. The applications discussed here are not simply the ones that
require integrality (those have been discussed in Sect. 2.2 of this volume), but those,
in which integrality takes center stage. Those are the applications that require zero￾one variables. The simplest integer programming problems in this class are the
so-called knapsack problems. Formally, knapsack problems have an objective and
a single constraint. The story behind the name can be told as follows. A backpacker
wants to decide which items to take into the woods. Items to be considered include a
tent, a sleeping bag, a stove, stove fuel, map, compass, and so forth. With each item,
the backpacker associates two numbers: one that expresses the usefulness or value of
the item, and the other its weight. The problem is then to choose items, so as to
maximize their total value to the backpacker, while the total weight should not
exceed a prespecified limit.
Table 5.4 Exclude a single solution in case of three variables
Solution to
be excluded Formulation Wording
1 y1 + y2 + y3  1 “At least one of the three activities must be chosen”
2a y1 + y2  y3 “If neither y1 nor y2 are chosen, then y3 cannot be chosen
either”
2b y1 + y3  y2 “If neither y1 nor y3 are chosen, then y2 cannot be chosen
either”
2c y2 + y3  y1 “If neither y2 nor y3 are chosen, then y1 cannot be chosen
either”
3a y1 + y2 + y3  1 “If y1 is not chosen, then y2 and y3 cannot both be chosen”
3b y1  y2 + y3  1 “If y2 is not chosen, then y1 and y3 cannot both be chosen”
3c y1 + y2  y3  1 “If y3 is not chosen, then y1 and y2 cannot both be chosen”
4 y1 + y2 + y3  2 “No more than two of the three activities can be chosen”
170 5 Integer Linear ProgrammingThere are a number of different versions of knapsack problems. One version only
allows the backpacker to either pack an item or leave it at home, while another
version permits the hiker to take any (integer) number of units of an item. The
definition of variables is similar regardless of the application. In case we can only
decide to either pack an item or not, we will define a logical variable for item j, such
that yj ¼ 1, if we include the item in our pack, and 0 otherwise. In contrast, if we are
allowed to take any number of units of item j, we will define variables yj that are
defined as the number of items of type j that we include in our pack, so that yj  0
and integer. (As a matter of fact, in the former case, we can also think of the variable
in terms of the number of items taken, only that this number is restricted to no more
than one.) Note that in order to distinguish integer variables from those that do not
have to satisfy integrality, we will use yj for integer variables and xj for continuous
variables.
Knapsack problems occur in a variety of guises. For continuous knapsack or
cargo loading problems (i.e., those, in which the variables do not have to be integers
or even zeroes and ones), an example is provided in Problem 2 in Sect. 2.5. Another
popular example of a knapsack problem is capital budgeting. As a numerical
illustration, suppose that a developer can engage in five different projects, viz., a
high rise building, a shopping mall, an amusement park, a warehouse, and an airport.
The expected profit contributions and resource consumption (e.g., the number of
construction workers needed for the respective projects) are shown in Table 5.5.
Assume now that the developer has seven work crews at his disposal and suppose
that it is not possible to hire additional work crews. We can then define variables
yj ¼ 1, if the developer is to engage in project j, and 0 otherwise (clearly, engaging in
partial projects is not feasible and none of the projects can be performed more than
once), so that the problem can be formulated as follows:
P : Max z ¼ 10y1 þ 6y2 þ 12y3 þ 2y4 þ 7y5
s:t: 4y1 þ 2y2 þ 5y3 þ 1y4 þ 3y5  7
y1, y2, y3, y4, y5 ¼ 0 or 1:
The linear programming relaxation that includes just the objective, the single
constraint, and the nonnegativity constraints, has the optimal solution y2 ¼ 3:5 ,
y1 ¼ y3 ¼ y4 ¼ y5 ¼ 0 with the objective value z ¼ 21 . Adding upper bounds
y1  1, y2  1, y3  1, y4  1, and y5  1 results in y1 ¼ y2 ¼ 1, y3 ¼ 0:8, and
y4 ¼ y5 ¼ 0 with an objective value of z ¼ 18:4. Finally, requiring integrality of all
Table 5.5 Profit contributions and resource consumption of projects
High
rise
Shopping
mall
Amusement
park Warehouses Airport
Profit contribution
(in m$)
10 6 12 2 7
Resource consumption 4 2 5 1 3
5.2 Applications of Integer Programming 171variables, we obtain the optimal solutions y1 ¼ y2 ¼ y4 ¼ 1, y3 ¼ y5 ¼ 0 with the
objective value z ¼ 18.
Many beginners might think that knapsack problems―and particularly zero-one
knapsack problems―are so simple that modern computational equipment must
surely be able to solve such problems by simply enumerating all solutions, without
having to resort to complicated algorithms. Nothing could be further from the truth.
In order to demonstrate this, let us compute the number of different solutions of a
zero-one integer programming problem. A problem with a single variable has two
solutions, as the variable can assume the values of zero and one. With two variables,
we have four solutions: (0, 0), (0, 1), (1, 0), and (1, 1). With 3 variables, we already
have 8 different solutions, with 4 variables, there are 16 different solutions, and so
on. As a matter of fact, adding a single variable will double the number of solutions
of the problem. The reason is this: the variable that we add can have a value of zero
or one. If it were zero, together with all possible solutions of the other variables, we
have just as many solutions as before. The same applies if the new variable were
equal to one, so that adding a variable doubles the number of solutions. This means
that in case of n variables, we will have 2n different solutions that have to be
examined. This means that for n ¼ 10, we have 1024 solutions, for n ¼ 20, there
will be about a million solutions, for n ¼ 30 there is a billion, for n ¼ 40 a trillion,
and so forth. Clearly, even if a computer could examine ten quadrillion solutions
within a single second, (which is pretty much the limit by today’s standards), a
problem with 100 variables (a tiny problem by today’s standards) would require
more than 4 million years for the computer to examine all solutions. For most
business problems, such a time frame appears excessive.
It is no wonder that many users resort to heuristic algorithms for the solution of
integer programming problems. We will discuss exact and heuristic solution
techniques in Sect. 5.3 of this chapter.
5.2.1 Diet Problems Revisited
First, recall the standard diet problem in linear programming. To simplify matters,
consider only two foodstuffs and a single nutrient. The quantities of the two foods
are defined as x1 and x2, respectively, and at least five units of the nutrient are
required in the diet. We assume that the problem has been formulated as follows:
P : Min z ¼ 3x1 þ 4x2
s:t: x1 þ 2x2  5
x1, x2  0:
Suppose now that the additional requirement is that if food 1 is included in the
diet (in any quantity), then food 2 should not be. (One reason for this may be
incompatibilities due to taste such as ice cream and mustard, or unfortunate side
effects of incompatible foods, such as water and green apples, or, worse, yoghurt and
yeast.) This is a conditional constraint of the type “if food 1 is included, then food
172 5 Integer Linear Programming2 should not be.” We first must define logical zero-one variables, one for each
foodstuff. These new variables y1 (and y2) are defined as being one, if food 1 (food 2)
is included in the diet, and zero otherwise. We will need these variables in addition
to the variables x1 and x2 that denote the quantities of the two foods that are included
in the diet.
For instance, the first row in Table 5.6 includes neither of the two foods, and
while it may leave us hungry, it does not violate the condition. Here, we find that
only the solution that has y1 and y2 both equal to one (the case in which both foods
are in the diet) is prohibited. We can now use the exclusion constraints introduced
earlier.
Eliminating this solution from consideration is achieved by writing the constraint
y1 þ y2  1:
This surely excludes only the case of both foods in the diet and adding this
constraint to our formulation should do it. However, it does not. The reason is that by
adding this constraint to the formulation, our new problem now has two sets of
constraints; one that includes only the continuous variables x1 and x2, and another,
completely separate part that includes only the variables y1 and y2. This would allow
the two types of variables to change their values independent of each other, for
instance, allowing solutions that have y1 ¼ 0 and x1 ¼ 7. This does not make sense,
as y1 ¼ 0 states that “food 1 is not included in the diet,” while x1 ¼ 7 says that “there
are 7 units of food 1 in the diet.”
The remedy is to include additional linking constraints, i.e. constraints that
include the continuous variables x1 and x2 as well as the logical variables y1 and
y2. In this problem, the linking constraints are
x1  My1 and
x2  My2,
where M is a “sufficiently large” constant. In order to understand the workings of
these linking constraints, consider the first of these constraints and use the two
possible solutions y1 ¼ 1 and y1 ¼ 0. If y1 ¼ 1, then the constraint reads x1  M,
which, with M being sufficiently large, is a redundant constraint that does not affect
the solution. If, on the other hand, y1 ¼ 0, then the constraint reads x1  0, which, in
conjunction with the nonnegativity constraint x1  0, forces x1 to be equal to zero.
This is exactly the desired effect, as if food 1 is not included in the diet (i.e., y1 ¼ 0),
then its quantity in the diet (x1) must be zero as well. And this is what these linking
constraints guarantee.
Table 5.6 Decision table
for the diet problem y1 y2 OK?
0 0 Yes
0 1 Yes
1 0 Yes
1 1 No
5.2 Applications of Integer Programming 173Consider now a few extensions of the model. If, for instance, it would not be
acceptable to have a diet without any food (i.e., we were to consider the solution
y1 ¼ y2 ¼ 0 unacceptable), then the constraint y1 + y2 ¼ 1 would guarantee that while
both foods cannot be together in the diet, at least one of them has to be.
Consider now the diet problem with the different conditional constraint “if food
1 is included in the diet, then food 2 must be included in the diet as well.” The
decision table for this condition is shown in Table 5.7.
For this scenario, the constraint
y1  y2
must be included (together with the linking constraints). The only solution that this
constraints excludes is y ¼ [0, 1], which is the desired effect.
5.2.2 Land Use
The next example deals with land use. Suppose that a land owner owns a parcel of
land that he has to decide what to do with. He has narrowed down his decisions
to two: sell stumpage, i.e., harvest the land, or build an animal sanctuary, but not
both. Here, we will formulate only this aspect of the model and ignore all other
considerations. We will need a decision variable for each possible decision, so that
we define y1 ¼ 1, if we decide to harvest the parcel, and 0 otherwise, and y2 ¼ 1, if
we decide to build an animal sanctuary, and 0 otherwise. The decision table for this
problem is then shown in Table 5.8.
Once formalized as done here, we see that the situation is the same as in the diet
problem by not allowing both options at the same time, which is modeled as y1
+ y2  1. Since this problem does not appear to need quantitative variables x1 and x2
as was the case in the diet problem, we do not need linking variables, as there is
nothing to link.
Things may get much more complicated when more options exist. Suppose now
that for the parcel in question, three choices have been identified: Harvest (decision
Table 5.7 Decision table
for the modified diet
problem
y1 y2 OK?
0 0 Yes
0 1 Yes
1 0 No
1 1 Yes
Table 5.8 Decision table
for the simple land use
problem
y1 y2 OK?
0 0 Yes
0 1 Yes
1 0 Yes
1 1 No
174 5 Integer Linear Programmingvariable y1), build a sanctuary (decision variable y2), or allow the building of a
municipal well (decision variable y3). As in the previous land use example, it is not
possible to harvest and have a sanctuary at the same time in the parcel in questions.
Furthermore, the parcel cannot be harvested if there is a municipal well on the parcel,
while we could very well have a well and a sanctuary on the same parcel. The
decision table for this extended problem is shown in Table 5.9.
The modeling of this situation is considerably more complicated than that in
the previous examples. As a matter of fact, we need two constraints to ensure that the
bottom three solutions in Table 5.9 are excluded from consideration. Using the
exclusion constraints in Table 5.4 at the end of Sect. 5.1, we can exclude the bottom
two solutions from consideration by requiring y1 + y2  y3  1 and y1 + y2 + y3  2.
We could add the two constraints, resulting in 2y1 + 2y2  3, or alternatively, y1 + y2
 1.5, which, because both remaining variables are zero or one, can be tightened to
y1 + y2  1. The third solution from the bottom can be eliminated by using the
constraint y1  y2 + y3  1 as shown in Table 5.4. Instead, we may write y1 + y3  1,
as this constraint additionally eliminates the bottom solution, which is not desired
either (even though it was already excluded). In summary, we find that adding the
constraints y1 + y2  1 and y1 + y3  1 leaves the first five solutions as options and
disallows the last three, which is precisely what the decision maker had in mind.
5.2.3 Modeling Fixed Charges
The purpose of this section is to introduce decision models, which provide options
regarding the choice of machines. One of the pertinent tasks is to ensure that
machine-related constraints apply only to those machines that are purchased or
leased. As an example, consider a publishing company that intends to produce its
annual lineup of operations research texts. This year, they have the books by Gabby
and Blabby (GB), Huff, Fluff, and Stuff (HFS), and the “Real OR” (ROR) texts. As
usual nowadays, authors are required to do everything except for the printing,
binding, and the subsequent marketing. A number of different machines are avail￾able for printing and binding, and from each type of machine, exactly one must be
chosen. The three printing machines under consideration are P1, P2, and P3, while
Table 5.9 Decision table
for the extended land use
problem
y1 y2 y3 OK?
0 0 0 Yes
0 0 1 Yes
0 1 0 Yes
1 0 0 Yes
0 1 1 Yes
1 0 1 No
1 1 0 No
1 1 1 No
5.2 Applications of Integer Programming 175the two binding machines are B4 and B5. The processing times for the different books
on the respective machines in minutes per book are shown in Table 5.10.
The capacities of the three printing machines are 120, 100, and 110 hrs (7200,
6000, and 6600 min), respectively, in the planning period. Similarly, the capacities
of the binding machines are 3331=3 and 300 hrs, respectively (or 20,000 and
18,000 min). The costs to lease the machines are independent of the number of
books made with them. They are $10,000, $8000, $9000, $20,000, and $23,000,
respectively. The profit contributions of the three books (other than the leasing costs)
have been identified as $40, $60, and $70, respectively. It has also been determined
that the publishing house should produce at least 500 copies of the landmark ROR
book in order to maintain a good academic image.
We can formulate the problem by first defining variables x1, x2, and x3 to indicate
the number of books of the three types that are manufactured and sold. (As usual in
single-period models, we assume that all units, which are manufactured can also be
sold.) In addition, we also need zero-one variables that show whether or not a
machine is leased. In particular, we define binary variables y1, y2, ..., y5 that assume
a value of one, if a machine is leased, and 0 otherwise. The objective function then
consists of two parts: the sum of the profit contributions of the individual books
(which are 40x1, 60x2, and 70x3), and the sum of the leasing costs, which are
10,000y1, 8000y2, 9000y3, 20,000y4, and 23,000y5, respectively.
First the easy constraints: making at least 500 copies of ROR is modeled as
x3  500, and the fact that we need exactly one printing and one binding machine can
be written as y1 + y2 + y3 ¼ 1 and y4 + y5 ¼ 1, respectively.
Consider now the capacity constraints of the machines. As usual, they are written
as (machine usage)  (machine capacity). For example, for the first printing
machine, the capacity constraint is 3x1 + 2x2 + 4x3  7200. The problem here is
that this constraint must hold only if the machine is actually leased. If it is not leased,
the constraint can be ignored. The way to model this is the same technique that was
applied in the diet problem in Sect. 5.2.1 to write the linking constraints. If the first
machine is not leased, y1 ¼ 0, and the capacity constraint is made redundant, by
having a sufficiently large right-hand side value. (Again, we will use the very large
value M as introduced in linear programming.) However, if the first machine is
leased, we have y1 ¼ 1, and the right-hand side of the capacity constraint should be
7200, the actual capacity of the first machine. We can formulate this by writing the
right-hand side as 7200 + M(1  y1).
To demonstrate the validity of this formulation, let y1 ¼ 0. In this case the right￾hand side value is 7200 + M(1  y1) ¼ 7200 + M, so that the constraint has a very
large right-hand side, making it redundant. On the other hand, if we do lease the first
Table 5.10 Processing
times for printing and bind￾ing machines
Printing Binding
P1 P2 P3 B4 B5
GB 3 6 4 10 10
HFS 2 3 3 12 11
ROR 4 5 5 15 14
176 5 Integer Linear Programmingmachine and y1 ¼ 1, the right-hand side value equals 7200 + M(1  y1) ¼ 7200,
which is the actual capacity of the first machine. The capacity constraints for the
other machines are constructed similarly. The formulation can then be written as
follows:
P : Max z ¼ 40x1 þ 60x2 þ 70x3
10, 000y1  8000y2  9000y3  20, 000y4  23, 000y5
s:t: 3x1 þ 2x2 þ 4x3  7200 þ M 1  y1 ð Þ
6x1 þ 3x2 þ 5x3  6000 þ M 1  y2 ð Þ
4x1 þ 3x2 þ 5x3  6600 þ M 1  y3 ð Þ
10x1 þ 12x2 þ 15x3  20, 000 þ M 1  y4 ð Þ
10x1 þ 11x2 þ 14x3  18, 000 þ M 1  y5 ð Þ
x3  500
y1 þ y2 þ y3 ¼ 1
y4 þ y5 ¼ 1
x1, x2, x3  0 and integer
y1, y2, y3, y4, y5 ¼ 0 or 1:
Using a large value for the constant M (here, we use M ¼ 1,000,000), multiplying
the brackets and sorting the variables, we obtain constraints such as 3x1 + 2x2 + 4x3
+ 1,000,000y1  1,007,200 for the first constraint, and similar for the other four
capacity constraints. The solution of this all-integer programming problem is y2 ¼
y4 ¼ 1 and y1 ¼ y3 ¼ y5 ¼ 0 (i.e., we lease the second printing and the first binding
machine), and make x1 ¼ 0 GB books, x2 ¼ 1039 HFS books, and x3 ¼ 502 ROR
books. The profit associated with this plan is $69,480. Note that the slack capacities
on the printout will indicate huge (and meaningless) values for the machines that are
not leased. This is due to the fact that their right-hand side values have the artificially
high value of M ¼ 1,000,000, from which some nonexistent usage is subtracted.
Suppose now that we allow more than one printing machine and/or more than one
binding machine to be used. This will make the model considerably more complex.
In particular, we will have to define additional variables xij, which denote the number
of books of type i that are processed on machine j. The model can then be formulated
as follows:
P : Max z ¼ 40x1 þ 60x2 þ 70x3
10, 000y1  8000y2  9000y3  20, 000y4  23, 000y5
5.2 Applications of Integer Programming 177s:t: x1 ¼ x11 þ x12 þ x13
x2 ¼ x21 þ x22 þ x23
x3 ¼ x31 þ x32 þ x33
3x11 þ 2x21 þ 4x31  7200y1
6x12 þ 3x22 þ 5x32  6000y2
4x13 þ 3x23 þ 5x33  6600y3
10x14 þ 12x24 þ 15x34  20, 000y4
10x15 þ 11x25 þ 14x35  10, 000y5
x3  500
x11 þ x12 þ x13 ¼ x14 þ x15
x21 þ x22 þ x23 ¼ x24 þ x25
x31 þ x32 þ x33 ¼ x34 þ x35
y1, ... , y5 ¼ 0 or 1
x1, x2, x3; x11, x12, ... , x35  0 and integer:
We wish to point out that the former model can be written as a special case of this
formulation. All we need to do is to include the two constraints y1 + y2 + y3 ¼ 1 and
y4 + y5 ¼ 1.
The objective function has not changed in comparison to the previous model. The
first three constraints link the variables xij and the variables xi with each other. In
other words, they define the number of units of the products that are actually made.
The next five constraints are the capacity constraints that restrict the number of units
that we can make by the machine capacities (which are zero if we do not lease the
machine). The next constraint ensures that we make at least 500 units of the ROR
book. Finally, the last three structural constraints require that the number of books of
each type that we print must also be bound. Note that we could, of course, replace all
variables xj by xij if so desired.
The optimal solution has the decision maker lease the first printing machine and
both binding machines. As before, we will make no GB books, but now we make
2600 HFS books, and 500 ROR books. In other words, the only significant change is
the increase of HFS books from 1039 to 2600. This results in a doubling of the profit
from the original $69,480 to $138,000.
178 5 Integer Linear Programming5.2.4 Workload Balancing
The problem presented in this subsection deals with the allocation of tasks to
employees, so as to ensure that none of the employees is overworked, while others
are partially idle. We assume that tasks cannot be split, meaning that once an
employee starts a job, he will have to finish it. (This scenario is reminiscent of the
bin packing problem, see, e.g., Chap. 17.) Due to their different backgrounds and
training, a job will take different amounts of time if different employees perform
it. There are three workers W1, W2, and W3, who will have to perform tasks T1, ...,
T5. Table 5.11 shows the processing times (in hours) for all worker–task
combinations.
In order to formulate the problem, we need to introduce zero-one variables, which
are defined as yij ¼ 1, if employee Wi is assigned to task Tj, and zero otherwise. The
only constraints of the model ensure that each task is assigned to exactly one
employee. Formally, we can write
y1j þ y2j þ y3j ¼ 1 for all j ¼ 1, ... , 5:
The more contentious issue concerns the objective function. First, we note that
the actual working time of the employees can be written as
w1 ¼ 5y11 þ 1y12 þ 9y13 þ 4y14 þ 9y15,
w2 ¼ 4y21 þ 3y22 þ 8y23 þ 3y24 þ 8y25, and
w3 ¼ 7y31 þ 5y32 þ 6y33 þ 4y34 þ 7y35,
where the new variables w1, w2, and w3 denote that time that employees W1, W2, and
W3 are busy working on the tasks. One possibility to ensure fairness in the solution is
to attempt to make the longest working time as short as possible. In other words, the
employee who works longest should have the shortest working hours possible.
Formally for our problem, we can write this objective as
Min z ¼ max f g w1,w2,w3 :
Clearly, this is not part of a linear programming problem the way we have defined
it. However, minimax objective functions of this type can easily be reformulated,
resulting in standard linear (or integer) programming problems. This is done by
introducing a single new variable, say z, which measures the highest workload of any
employee, i.e., the maximum of the right-hand side of the above objective. We will
then minimize this highest workload, but we must ensure that none of the actual
workloads is higher, which is done by introducing the three constraints z  w1,
Table 5.11 Processing
times for worker–task
combinations
T1 T2 T3 T4 T5
W1 51949
W2 43838
W3 75647
5.2 Applications of Integer Programming 179z  w2, and z  w3. Replacing w1, w2, and w3 by the functions of yij, we can then
write the model as
P : Min z
s:t: z  5y11 þ 1y12 þ 9y13 þ 4y14 þ 9y15
z  4y21 þ 3y22 þ 8y23 þ 3y24 þ 8y25
z  7y31 þ 5y32 þ 6y33 þ 4y34 þ 7y35
y11 þ y21 þ y31 ¼ 1
y12 þ y22 þ y32 ¼ 1
y13 þ y23 þ y33 ¼ 1
y14 þ y24 þ y34 ¼ 1
y15 þ y25 þ y35 ¼ 1
yij ¼ 0 or 1 for i ¼ 1, 2, 3; j ¼ 1, ... , 5:
One solution will have worker W1 work on tasks T2 and T5, worker W2 works on
tasks T1 and T4, while worker W3 is assigned task T3. The resulting workloads for the
three employees are then 10, 7, and 6 hrs, respectively (with the variable z ¼ 10
denoting the highest of the workloads). Note that other solutions exist, e.g., worker
W1 works on tasks T1, T2, and 4, worker W2 works on task T5, and worker W3 works
on task T3, so that the workloads are 10, 8, and 6, respectively. Again, the highest
workload is z ¼ 10 hrs. The total time the employees work is 24, 1 hr more than in
the previous solution.
Other objective functions to ensure fairness have been used in the literature. For
instance, we could define a variable w ¼ w1 + w2 + w3 as the total workload by all of
the employees, so that ideally, each employee would work 1=3w. We could not
minimize the sum of deviations from this workload. Since positive and negative
deviations will cancel out, we have to either minimize the sum of absolute values of
the deviations or square the deviations (as done to derive the variance in statistics,
which is the sum of squared deviations from the mean). This objective would then be
Min z ¼ ð Þ 1=3w  w1
2 þ ð Þ 1=3w  w2
2 þ ð Þ 1=3w  w3
2
:
Unfortunately, this objective is nonlinear and the resulting problem will then be
nonlinear and integer, a very difficult combination. Solving this problem results in
workloads of 9, 11, and 11 hrs with a total work time of 31 hrs (much higher than in
the previous solutions), but with a much smaller variance.
In general, fairness (or “equity”) objectives should normally be combined with
efficiency objectives. If not, a solution such as workloads of 12, 12, and 12 hrs for
three employees would be preferred to workloads of 4, 9, and 8 hrs, because the
180 5 Integer Linear Programmingformer solution is “more equal,” even though each employee would gain by moving
from the former to the latter solution.
5.2.5 Political Districting
An application of considerable importance and controversy deals with the redesign
of electoral districts. The controversy arises from the fact that each party will attempt
to redesign the districts, so that it will win (possibly by a small margin) in as many
districts as possible, while “in return,” it accepts to lose big in a few districts. Here,
we provide a simple prototype model for the problem. Consider an area with eight
regions, which are to be put together into three electoral districts, so that a voter in
each of these districts wielded approximately the same power as any other voter, thus
satisfying the principle of “one person, one vote.” The regions are shown in Fig. 5.4,
and the number of eligible voters in the regions are 65, 29, 95, 30, 52, 57, 21, and
41, respectively, for a total of 390 registered voters. Given that the task is to
aggregate regions to three electoral districts, ideally, each district comprises
130 voters. This is the basis of the formulation.
Defining binary variables xij which assume a value of 1, if region i is in district j,
and 0 otherwise, as well as deviational variables (as in goal programming, see Sect.
3.3 of this book) dþ
j and d
j as district j’s deviation below (above) the ideal number
of voters. We can then formulate the problem as
Fig. 5.4 Regions in the reapportionment problem
5.2 Applications of Integer Programming 181P : Min z ¼ dþ
1 þ d
1 þ dþ
2 þ d
2 þ dþ
3 þ d
3
s:t: x11 þ x12 þ x13 ¼ 1
x21 þ x22 þ x23 ¼ 1
⋮
x81 þ x82 þ x83 ¼ 1
65x11 þ 29x21 þ 95x31 þ 30x41 þ 52x51 þ 57x61 þ 21x71 þ 41x81 ¼ 130  dþ
1 þ d
1
65x12 þ 29x22 þ 95x32 þ 30x42 þ 52x52 þ 57x62 þ 21x72 þ 41x82 ¼ dþ
2 þ d
2
65x13 þ 29x23 þ 95x33 þ 30x43 þ 52x53 þ 57x63 þ 21x73 þ 41x83 ¼ dþ
3 þ d
3
xij ¼ 0 or 1 for all i, j:
The optimal solution has x11 ¼ x21 ¼ x81 ¼ 1, x32 ¼ x42 ¼ 1, and x53 ¼ x63 ¼ x73
¼ 1, i.e., the first district comprises Regions 1, 2, and 8, the second includes Regions
3 and 4, and the third district consist of Regions 5, 6, and 7, so that the number of
voters in the respective districts is 135, 125, and 130, respectively with a total
deviation of 10 from the idea size of 130. This is a nice result, except for the fact
that the first district is not contiguous, Region 8 is not directly connected to Regions
1 and 2. We can then formulate the conditional constraint “If regions 2 and 8 are in a
district, then region 5 must be in the district as well.” Rather than formulating this
only for the first district (which currently violates this condition), we formulate it for
all three districts, as otherwise, the solver could simply switch the first and the
second district and the same problem would appear, this time for the second district.
Hence, we formulate
x21 þ x81  x51  1,
x22 þ x82  x52  1, and
x23 þ x83  x53  1:
Adding these constraints to the formulation results in a solution, in which District
1 now consists of Regions 3 and 8 (not contiguous), District 2 comprises Regions
5, 6, and 7 (snake-like, but contiguous), and District 3 includes Regions 1, 2, and
4 (contiguous, but a weird shape). Due to the additional constraints, the sizes of the
districts are now 136, 130, and 124, up from 10 to 12.
Adding conditional constraints that require that if Regions 3 and 8 are in a district,
then Region 5 (or, alternatively, Regions 5 and 7 or Regions 4, 6, and 7) must be in
the same district, we add constraints
x31 þ x81  x51  1,
x32 þ x82  x52  1, and
x33 þ x83  x53  1:
The solution now has District 1 contain Regions 1, 4, and 8 (not contiguous),
District 2 has Regions 2 and 3, and District 3 includes 5, 6, and 7 (which is long and
182 5 Integer Linear Programmingnot very wide). The number of voters in the districts are now 136, 124, and 130, so
that the total deviation is still 12.
This process then continues until a contiguous solution has been found. It is also
possible to continue the process in order to avoid unpleasant shapes, such as District
3 (Regions 5, 6, and 7) in the present solution. We could accomplish this by
requiring that “if Regions 5 and 7 are in one district, then Region 8 cannot be in
the same district,” or similar constraints.
5.3 Solution Methods for Integer Programming Problems
This section will examine some of the techniques that can and are used to solve
integer programming problems. The first subsection will briefly describe the ideas
behind cutting plane methods. The second subsection thoroughly discusses the basic
features of branch-and-bound techniques, and the third subsection illustrates some
heuristic methods and how they may apply to some integer programming problems.
5.3.1 Cutting Plane Methods
As mentioned in the introduction to this chapter, the first exact techniques for the
solution of integer programming problems were the so-called cutting plane
techniques. Their general idea may be explained as follows. Recall that some linear
programming problems always have optimal integer solutions, such as the assign￾ment problem and the transportation problem (the latter only if all supplies and
demands are integer-valued). Given Dantzig’s corner point theorem (see Sect. 2.3 of
this book), this means that the feasible set of those two problems (and others like
them) has corner points, all of which are integer-valued. Clearly, very few
formulations have this property. If it were now feasible to find a restricted feasible
set that includes all-integer solutions of the original feasible set but has all of its
extreme points integer-valued (something usually called a convex hull), then we
could simply solve the linear programming relaxation of the problem with this
restricted feasible set and then automatically obtain an integer solution. Unfortu￾nately, obtaining the convex hull of a feasible set is very difficult and the suggested
approach is not computationally viable.
However, it is not necessary to have the entire convex hull at our disposal. And
this is where cutting plane methods come in. The idea of cutting plane methods is to
locally approximate the convex hull. This is done as follows. We first solve the linear
programming relaxation of the given integer programming problem. If the optimal
solution happens to be integer, we are done. Suppose now that it is not. We then
formulate a cutting plane, i.e., an additional constraint that does two things: (1) it
must cut off (i.e., make infeasible) the present optimal solution, while (2) it cannot
cut off any feasible integer point.
As a numerical illustration, consider the following all-integer programming
problem:
5.3 Solution Methods for Integer Programming Problems 183P : Max z ¼ y1 þ y2
s:t: 3y1 þ 2y2  6
y1 þ 3y2  3
y1, y2  0 and integer:
The shaded area in Fig. 5.5 shows the feasible set of the linear programming
relaxation, and the point shown as yLP ¼ ð Þ 12=7, 3=7 is the optimal solution of the
linear programming relaxation. The triangle shown by the broken lines that connect
the points (0, 0), (2, 0), and (0, 1) is the convex hull of the feasible set for the integer
programming problem. The dotted line shows the boundary of the cutting plane 5y1
+ 10y2  12. Plugging in the coordinates of yLP, we see that this point violates the
condition of the cutting plane, as 90=7 ¼ 12 6
7 ≰12 . This is also apparent in the
graph. On the other hand, all four feasible integer points satisfy the condition, so that
the condition is indeed a cutting plane.
While cutting planes appear to be a very good idea, their computational perfor￾mance for general integer programming problems has been disappointing. In partic￾ular, the slices that are cut off tend to become very small, requiring a very large
number of additional constraints to be added in the process, which adds to the size of
the problem and the degree of difficulty.
Below we will demonstrate this fact for one specific type of cut, a so-called
Dantzig cut. We have chosen this particular type of cutting plane, as it does not
require any knowledge beyond the solution that is typically provided by a solver.
Our discussion is designed to provide a general idea about cutting planes, it is not our
intention to suggest the practical use of this particular type of cutting plane which
turned out to be rather inefficient. Other, more efficient, cutting planes work on the
same principle.
Fig. 5.5 Illustration of the
cutting plane method
184 5 Integer Linear ProgrammingAgain, let us start with the general idea. Suppose that we have an all-integer linear
programming problem in which the set of variables includes all slack and excess
variables, so that all constraints are written as equations. Let there be n nonnegative
variables (including the slack and excess variables) and m structural equation
constraints. Furthermore, assume that the present optimal solution (starting with
the linear programming relaxation) has at least one variable assume a noninteger
value. At this point, we can separate the variables into two disjoint sets B and N,
where B includes all variables that presently have a positive value, while N includes
all variables that are presently zero. If the solution is nondegenerate, the set
B includes exactly m variables, and the set N includes exactly (n  m) variables.
In case of primal degeneracy, the set N will include more than (n  m) variables, in
which case we define N as any (n  m) variables that are presently zero.
A Dantzig cut is now an additional constraint that requires that the sum of all
variables in the set N is at least “1.” The validity of this constraint as a cutting plane
(i.e., a constraint that makes the present solution infeasible, but that does not
invalidate any feasible integer points) can be shown as follows. First of all, as in
the present solution all variables in the set N equal zero, this solution does not satisfy
the requirement of the new constraint, so that the cutting plane does cut off the
present solution. On the other hand, any feasible solution to the original integer
problem will need to have at least one variable in N assume a positive value, which,
since this is an all-integer optimization problem, must be at least one. Hence, the sum
of all the variables that are presently at the zero level, must be at least one, hence
Dantzig’s cut.
Once the cutting plane has been formulated and is added to the formulation, the
problem is solved again. Note that in practice, it is not required to use a “cold start,”
i.e. start the optimization from scratch. Much rather, it is typically a good idea to use
a “warm start,” in which the cutting plane is added to the formulation and the solver
continues from the previously optimal solution. At this point, we do not have to
concern ourselves with such technical details. Note that each time a cutting plane is
added to the system, the value of the objective function either stays the same (in case
of dual degeneracy) or decreases (in maximization problems, it increases in minimi￾zation problems). This is simply due to the fact that the feasible set shrinks each time
a cut is added. An important part of the development of a cutting plane is the proof
that eventually, the sequential addition of cutting plane results in an integer optimal
solution. Such a proof is not available for Dantzig cuts, meaning that it is not
guaranteed that the process will eventually terminate with an integer solution.
We will now explain the procedure by means of a numerical
5.3 Solution Methods for Integer Programming Problems 185Example Consider the following all-integer programming problem:
P : Max z ¼ 3y1 þ 2y2
s:t: 3y1 þ 7y2  22 ðI Þ
5y1 þ 3y2  17 ðII Þ
y1  2 ðIII Þ
y1, y2  0 and integer:
Adding slack variables S1 and S2 to the left-hand sides of the first two constraints
and subtracting an excess variable E3 from the left-hand side of the third constraint,
we obtain the following formulation with n ¼ 5 variables and m ¼ 3 structural
constraints:
P : Max z ¼ 3y1 þ 2y2
s:t: 3y1 þ 7y2 þ S1 ¼ 22
5y1 þ 3y2 þ S2 ¼ 17
y1  E3 ¼ 2
y1, y2, S1, S2, E3  0 and integer:
Solving the linear programming relaxation of this problem, we obtain the optimal
solutions y1 ¼ 2:0385, y2 ¼ 2:2692, S1 ¼ S2 ¼ 0, and E3 ¼ 0:0385 with an objec￾tive value z ¼ 10:65385. The solution is nondegenerate with the variables S1 and S2
being included in the set N, so that the first Dantzig cut is
S1 þ S2  1
(or, given that the original constraints imply that S1 ¼ 22  3y1  7y2 and S2 ¼ 17
 5y1  3y3, the cut could also be written as 8y1 + 10y2  38, but we will continue
with the original version of the cut). Subtracting a new excess variable E4 from the
left-hand side of this cut, we obtain S1 + S2  E4 ¼ 1. Adding this cut to the problem
and solving it again, we obtain the new solution y1 ¼ 2:1538, y2 ¼ 2:0769, S1 ¼ 1,
S2 ¼ E4 ¼ 0 , and E3 ¼ 0:1538 with an objective value z ¼ 10:61539 . Clearly,
another cut is required. The sequence of cutting planes that are generated in the
process is shown in Table 5.12.
It is apparent that even in this very small problem, a large number of cutting
planes are required to solve the problem. Computational experience with Dantzig’s
cutting planes confirms this. Figure 5.6 shows that the cutting planes in this example
only slice off tiny parts of the feasible set. Different cuts, particularly the so-called
deep cuts perform much better, but can still not compete with branch-and-bound
methods, at least not for general (unstructured) integer programming problems.
One possible modification in the above example could have used the objective
function to derive a cut. The idea is this. Given the fact that all variables must be
integer, the value of the objective function z ¼ 3y1 + 2y2 must also be integer. Given
186 5 Integer Linear ProgrammingTable 5.12 Cutting planes generated in the example
Optimal solution Cutting plane
y1 ¼ 2:0385, y2 ¼ 2:2692, S1 ¼ 0, S2 ¼ 0, E3 ¼ 0:0385 with z ¼ 10:65385
(optimal solution of the linear programming relaxation).
S1 + S2  1 or
S1 + S2  E4 ¼
1
y1 ¼ 2:1538, y2 ¼ 2:0769, S1 ¼ 1, S2 ¼ 0, E3 ¼ 0:1538,E4 ¼ 0 with z ¼
10:61539.
S2 + E4  1 or
S2 + E4  E5 ¼
1
y1 ¼ 2:2692, y2 ¼ 1:8846, S1 ¼ 2, S2 ¼ 0, E3 ¼ 0:2692, E4 ¼ 1, E5 ¼ 0 with
z ¼ 10:5769.
S2 + E5  1 or
S2 + E5  E6 ¼
1
y1 ¼ 2:3846, y2 ¼ 1:6923, S1 ¼ 3, S2 ¼ 0, E3 ¼ 0:3846, E4 ¼ 2, E5 ¼ 1,
E6 ¼ 0 with z ¼ 10:53846.
S2 + E6  1 or
S2 + E6  E7 ¼
1
y1 ¼ 2:5, y2 ¼ 1:5, S1 ¼ 4, S2 ¼ 0, E3 ¼ 0:5, E4 ¼ 3, E5 ¼ 2, E6 ¼ 1, E7 ¼ 0
with z ¼ 10:5.
S2 + E7  1 or
S2 + E7  E8 ¼
1
y1 ¼ 2:6154, y2 ¼ 1:3077, S1 ¼ 5, S2 ¼ 0, E3 ¼ 0:6154, E4 ¼ 4, E5 ¼ 3,
E6 ¼ 2, E7 ¼ 1, E8 ¼ 0 with z ¼ 10:46154.
S2 + E8  1 or
S2 + E8  E9 ¼
1
y1 ¼ 2:7308, y2 ¼ 1:1154, S1 ¼ 6, S2 ¼ 0, E3 ¼ 0:7308, E4 ¼ 5, E5 ¼ 4,
E6 ¼ 3, E7 ¼ 2, E8 ¼ 1, E9 ¼ 0 with z ¼ 10:42308.
S2 + E9  1 or
S2 + E9  E10
¼ 1
⋮ ⋮
y1 ¼ 2, y2 ¼ 2, S1 ¼ 2, S2 ¼ 1, E3 ¼ 0 with z ¼ 10 (optimal all-integer
solution)
Fig. 5.6 Cuts with Dantzig’s method
5.3 Solution Methods for Integer Programming Problems 187that we have just solved the linear programming relaxation of the problem that has an
objective value of z ¼ 10:65385, we know that z  10 must hold. We can then
define the cutting plane 3y1 + 2y2 + S4 ¼ 10. Solving the problem with this added
constraint results in the solution y1 ¼ 3:3333, y2 ¼ 0, S1 ¼ 12, S2 ¼ 0:3333, E3 ¼
1:3333, and S4 ¼ 0 with an objective value of z ¼ 10. (Since the objective value has
not changed, we presently encounter dual degeneracy.)
The next cutting plane is then y2 + S4  1, or, in its proper form, 3y1 + y2 + S5 ¼ 9.
The resulting solution has y1 ¼ 2:6667, y2 ¼ 1, S1 ¼ 7, S2 ¼ 0:6667, E3 ¼ 0:667,
S4 ¼ S5 ¼ 0, again with an objective value of z ¼ 10:
The next cut is S4 + S5  1, or, rewritten in terms of the original variables and the
new slack variable S6, it is written as 6y1 + 3y2 + S6 ¼ 18. The optimal solution is
then y1 ¼ y2 ¼ 2, S1 ¼ 2, S2 ¼ 1, E3 ¼ S4 ¼ 0, S5 ¼ 1, S6 ¼ 0, with a value of the
objective function of z ¼ 10. This solution is an integer optimum.
5.3.2 Branch-and-Bound Methods
In contrast to cutting plane methods, the concept of branch-and-bound in its many
variations is nowadays accepted as the universal solver for mixed- and all-integer
linear programming problems. The basic idea is simple. We first solve again the
linear programming relaxation. Again, if its coordinates of the optimal solution point
satisfy the required integrality conditions, the process terminates. Suppose now that
this is not the case. We then select any variable that is required to be integer but, at
the present solution, is not. As an example, let the variable y7 ¼ 3.4 at the present
solution. Given that y7 is required to be integer, the present solution does not satisfy
all integrality constraints. We will then subdivide the given problem (often called the
“parent”) into two new problems (the “children”), a process usually referred to as
“divide and conquer” (or, by some critics, “double your trouble”). Each of the two
children contains exactly the same constraints as its parent plus one single additional
constraint. One child (often called the “left child” based on the graphical depiction)
has the additional constraint y7  3, while the other child (the “right child”) has the
additional constraint y7  4. The union of the feasible sets of the two problems
includes all feasible points of the original problem, except all of those, for which
3 < y7 < 4. The reason that we can actually cut out such a “corridor” without
deleting important parts of the feasible set is that none of the solutions that we
eliminate from consideration is feasible, in that none of them contains any point that
has an integer value for the variable y7.
Continuing in this way, we will build up what is known as a solution tree. Each
“node” of the tree, shown as a small box, represents a problem formulation and a
solution. Once we decide to work on a node or solution, we “branch” from it,
meaning we generate its children by adding a constraint as explained above. Once
we have branched from a node, it is no longer active. Also, nodes that represent
integer solutions and those that represent problem formulations that do not have
feasible solutions are also not active. This is best explained in terms of a small
numerical example.
188 5 Integer Linear ProgrammingConsider the following all-integer optimization problem:
P : Max z ¼ 5y1 þ 9y2
s:t: 5y1 þ 11y2  94 Constraint I
10y1 þ 6y2  87 Constraint II
y1, y2  0 and integer:
The graphical representation of the problem is shown in Fig. 5.7, where the
feasible set of the linear programming relaxation is the shaded area. The optimal
solution of the linear programming relaxation is depicted as the point yLP . The
intermediate results of our computations will be displayed and collected in the
solution tree shown in Fig. 5.8. At present, only the top node, known as the root
of the tree, is known. At this point, this is also the only active node.
The first step is to examine the solution. In this example, both variables are
supposed to be integer and neither of them is. This means that we can start working
on either variable. Here, we choose to start working on y2. (Alternatively, we could
have started working on y1. This would have resulted in a completely different
solution tree―which one is smaller and easier cannot be said in advance―which is
shown in Fig. 5.9.) At present, y2 ¼ 6.3125, so that we will eliminate the corridor
6 < y2 < 7, so that our first branching, shown in the solution tree in Fig. 5.8 branches
from the root tree to the two children, the one on the left with the additional
constraint y2  6, and the one on the right with the additional constraint y2  7. In
Fig. 5.7 Graphical representation of the Branch-and-Bound Method
5.3 Solution Methods for Integer Programming Problems 189Fig. 5.7, the feasible set of the right child is now the triangle with vertices at (0, 7),
(0, 8.5455), and y2 ¼ ð Þ 3:4, 7 . Using the objective function, the optimal solution of
this subproblem is y2 . On the other hand, the feasible set of the left child is the
trapezoid with corner points (0, 0), (0, 6), y1 ¼ ð Þ 5:1, 6 , and (8.7, 0). The optimal
solution of this subproblem is y1.
At this point we have two active nodes with solutions y1 and y2
. Among all active
nodes, we always choose the one with the best value of the objective function (i.e.,
highest for maximization problems and lowest for minimization problems) to exam￾ine and further work on, if necessary. In this example, it is the node with the solution
y2 (now considered the parent). The solution y2 has values of 3.4 and 7 for the two
variables, meaning that while the variable y2 is integer as required, the variable y1 is
not and we will have to work on it. This means that from this solution, we will
perform our second branching that adds the constraint y1  3 to the left child, and
y1  4 to the right child. The additional constraints are shown in Fig. 5.7 as short
lines that will have to be considered in conjunction with the triangle (the feasible set
of the parent) derived earlier. This results in a feasible set of the left child that is the
trapezoid with corner points (0, 6), (0, 8.5455), y3 ¼ ð Þ 3, 7:1818 , and (3, 6), the
optimal solution of which is y3 . On the other hand, the right child has an empty
feasible set, as the intersection of the triangle of its parent and the constraint y1  4 is
empty. This is now indicated in the solution tree in Fig. 5.8. At this point we have
two active nodes: the nodes labeled with solutions y1 and y3 . All other nodes
developed so far have either already been branched from or have no feasible solution
and thus need no longer to be considered.
Fig. 5.8 Branch-and-Bound solution tree
190 5 Integer Linear ProgrammingThe node with the best (here: highest) value of the objective functions value is y3
,
so that our work continues here. Now the variable y2 ¼ 7.1818 is again noninteger
and we will branch on it. Considering this node as the present parent, we branch to
the children by using the additional constraints y2  7 and y2  8, respectively. The
feasible set of the left child is the line segment with end points (0, 7) and (3, 7). The
optimal solution of this child is y4 ¼ ð Þ 3, 7 with a value of the objective function of
78. This is our first integer solution so far in the tree. The feasible set of the right
child is the triangle with vertices (0, 8), (0, 8.5455), and y5
, with y5 being its optimal
solution, whose objective value is also 78.
At this point, the active nodes are y1
, y4
, and y5
. Inspection reveals that y1 has the
highest value of the objective function, so that work will continue here, regardless of
the fact that we have already found an integer solution (that solution may not be the
best). This node is now temporarily considered the parent. In this solution, y1 ¼ 5.1,
which violates the required integrality condition.
Recall that the feasible solution of this parent is the shaded area below y2  6.
Branching y1  5 and y1  6, we obtain the two lightly shaded feasible sets: the
rectangle with vertices (0, 0), (0, 6), y6 ¼ ð Þ 5, 6 , and (5, 0) for the left child, and the
triangle with vertices (6, 0), y7 ¼ ð Þ 6, 4:5 , and (8.7, 0) for the right child. The
respective optimal solutions are y6 and y7 , whose values of the objective function
are 79 and 70.5, respectively.
At this point, we have four active nodes with solutions y4 , y5 , y6 , and y7 . The
node with the best value of the objective function is y6
, whose solution is y6 ¼ ð Þ 5, 6
and objective value 79. Work will continue on this node. Inspection reveals that all
integrality conditions are now satisfied, and no other active node has an objective
function value this large, so that y6 represents an optimal solution. Since no other
active node has an equal objective value, this optimal solution is also unique, so that
the problem is fully solved.
We have seen that the branch-and-bound method solves integer programming
problems by a sequence of linear programming problems. It is not at all uncommon
that one integer programming problem requires the solution of hundreds of
thousands of linear programming problems, all similar, but differing by a few
constraints. Consider again the solution tree in Fig. 5.8 for some additional
considerations. For instance, the additional constraints to be considered at a node
are the constraints at all branchings from the root of the tree to the node under
consideration. For instance, the node with optimal solution y5 represents a problem
with the given objective function, all constraints of the linear programming relaxa￾tion (here: constraints I and II), as well as the additional constraints y2  7, y1  3,
and y2  8. An immediate consequence of this principle leads to the observation that
as you move on some path down the tree, the values of the objective function of the
problems either stay the same or get worse (i.e., go down for maximization problems
and go up for minimization problems). This occurs as by adding constraints, the
feasible set gets smaller, which cannot result in better objective values.
It is also apparent that even a very small problem with only two variables can
have sizeable solution trees. When using a computer with linear programming
software installed, it is possible to practice by using the computer to solve the linear
5.3 Solution Methods for Integer Programming Problems 191programming problems at the nodes of the solution tree, while constructing the
solution tree manually. In order to solve the integer programming problem in such
“computer-assisted” fashion, it is necessary to learn how to edit the linear program￾ming problems. For instance, return from the problem with the solution y5 to y1
requires moving up to the root of the tree (thus removing the constraints y2  8, y1
 3, and y2  7, and then moving down to the node labeled y1 by adding the
constraint y2  6).
For practice, readers may either use the graphical approach or the computer￾assisted approach to solve the same problem but start branching at the root node on
the variable y1 rather than y2. This will result in the solution tree shown in Fig. 5.9.
Note that this tree requires six branchings and the optimal solution is found at the
node labeled y9.
It is also important to realize that the very same procedure can be used to solve
mixed-integer linear programming problems. Whenever the best active node has
been chosen, we have to consider what is required to be integer and what is not.
Branching is then done only on variables that are required to be integer. As an
illustration, use the same example as above, except that y1  0, while y2  0 and
integer. Again, we first solve the linear programming relaxation and find the root
node of the solution tree. Now as y1 ¼ 4.9125, it satisfies the nonnegativity
constraint, which is all that is required of this variable. On the other hand,
y2 ¼ 6.3125, which satisfies the nonnegativity constraint, but not the integrality
constraint, so we must branch on y2 (there is no choice as in the all-integer problem).
The branching is identical to the first branching shown in Fig. 5.8. At this point, y2 is
the better solution with y1 ¼ 3.4, y2 ¼ 7 with an objective value of 80. At this point,
Fig. 5.9 Alternative Branch-and-Bound solution tree
192 5 Integer Linear Programmingthe procedure terminates, since y1 is nonnegative as required, and y2 is also nonneg￾ative and integer, as required; hence y2 is the optimal solution for this problem.
This is also the time to demonstrate what the term “bound” in branch-and-bound
actually means. The idea is that during the procedure, the upper and lower bounds on
the optimal objective value of the all- or mixed-integer programming problem are
constantly tightened. Consider the tree shown in Fig. 5.9. Initially, as shown in the
first section of this chapter, we only know that zIP  zLP, i.e. the optimal objective
value of the integer problem is no better than 81.375. For maximization problems,
the upper bound on zIP is always the objective value of the best-known active node,
while the lower bound is the objective value of the best-known integer solution.
Hence, initially zIP 1 81:375. After the first branching, the upper bound has
decreased to 80.5455. After the second branching, the upper bound is reduced to
80.5, and the lower bound is now 74, as the integer solution y3 is now known.
Branching 3 further reduces the upper bound to 80, branching 4 reduces the upper
bound to 79.6364, branching 5 reduces the upper bound to 79.5, while the lower
bound is now increased to 78, as the integer solution y7 has become known. Finally,
after branching 6 the upper and lower bounds coincide at 79, which terminates the
process.
Another aspect of the bounding procedure is that any active node with an
objective function value worse (i.e., smaller for maximization and larger for mini￾mization problems) than the best integer solution found so far can be removed from
the set of active nodes. Branching from such a node could obviously never produce a
child with an integer solution that might be optimal.
We will now demonstrate how to deal with cases in which the mixed- or
all-integer programming problem has no feasible solution. As an illustration, con￾sider the all-integer programming problem
P : Max z ¼ y1 þ 4y2
s:t: 28y1 þ 7y2  49
30y1  6y2  36
y1, y2  0 and integer:
The solution tree for this problem, given that branching commences with y2, is
shown in Fig. 5.10. Branching 1 results in one child of the root having no feasible
solution, while the other has a noninteger solution. Branching on this sole active
node, we obtain two children, both of whom have no feasible solutions. At this point,
there are no further active nodes, and no solution has been found. The tree that is
obtained if we were to start branching on y1 is even smaller: the two children of the
root of the tree both have no feasible solution.
5.3 Solution Methods for Integer Programming Problems 1935.3.3 Heuristic Methods
It will have become clear in the above discussion that for very large problems, exact
methods may not be able to solve a given integer linear programming problem
within a reasonable time frame. Surely, whenever an exact method such as the
branch-and-bound technique described in the previous section has found an integer
solution, one could terminate computations, even though the integer solution may
not be optimal. As a matter of fact, while such a procedure will result in a feasible
solution, it may be far from optimal. Instead, users often use heuristic techniques to
find (hopefully reasonably good) solutions quickly. This section will describe such a
technique.
In order to illustrate the technique, consider the following knapsack problem:
P : Max z ¼ 12y1 þ 20y2 þ 31y3 þ 17y4 þ 24y5 þ 29y6
s:t: 2y1 þ 4y2 þ 6y3 þ 3y4 þ 5y5 þ 5y6  19
y1, y2, y3, y4, y5, y6 ¼ 0 or 1:
In order to employ the Greedy Method (sometimes colloquially referred to as
“best bang for the buck” method), we first need to rank the variables in nonincreas￾ing order of their “value” to us. Rather than simply using the coefficients in the
objective function to rank the variables, we compute the “value per weight” ratios for
each product by dividing the contribution to the objective function by the coefficient
Fig. 5.10 Solution tree for problem with no feasible integer solution
194 5 Integer Linear Programmingin the constraint. We then rank the variables in nonincreasing order of these ratios as
shown in Table 5.13.
Starting with all variables set to zero, the Greedy algorithm will now increase the
values of variables one by one, starting with the highest rank, as long as resources are
available.
In Step 1, we set y1 :¼ 1, which consumes 2 resource units and contributes 12 units
to the objective,
in Step 2, we set y6 :¼ 1, which consumes 5 resource units and contributes 29 units to
the objective,
in Step 3, we set y4 :¼ 1, which consumes 3 resource units and contributes 17 units to
the objective,
in Step 4, we set y3 :¼ 1, which consumes 6 resource units and contributes 31 units to
the objective,
in Step 5, we set y2 :¼ 1, which consumes 4 resource units. Stop and backtrack, i.e.,
re-set the variable y2 :¼ 0, as this latest assignment exceeds the availability of
resources.
In summary, the solution at the termination of the Greedy algorithm is
y ¼ y1, y2, y3, y4, y5, y6 ½ ¼ ½  1, 0, 1, 1, 0, 1
which uses 16 resource units and has a value of the objective function of z(y) ¼ 89.
In the second phase, we use a simple improvement heuristic of the “interchange”
or “swap” type. This heuristic method raises one arbitrarily chosen variable from a
level of zero to one, while decreasing another also arbitrarily chosen variable from
one to zero. Whenever such an exchange is feasible and increases the value of the
objective function, it becomes our new starting point. The procedure is repeated until
no further improvements are possible. Note that given the present solution, we have
an extra 3 resource units available. In the tables below, we list the variable that leaves
the solution (i.e., the variable whose value is reduced from its present value of one to
zero), the entering variable (i.e., the variable whose value is increased from its
present value of zero to one), the resulting new solution, the marginal resource
usage ΔR, and the change Δz of the objective function that results from the swap.
Table 5.13 “Value per weight” of the individual items
Variable y1 y2 y3 y4 y5 y6
Value per
weight
12/2 ¼
6
20/4 ¼
5
31/6 ¼
5.1667
17/3 ¼
5.6667
24/5 ¼
4.8
29/5 ¼
5.8
Rank 1 5 4 3 6 2
Table 5.14 First swap move
Leaving variable Entering variable New solution ΔR Δz
y1 y2 0, 1, 1, 1, 0, 1 2+4 ¼ +2 12 + 20 ¼ + 8
5.3 Solution Methods for Integer Programming Problems 195It is apparent that the first swap move in Table 5.14 results in an improvement, so
that the solution y ¼ [0, 1, 1, 1, 0, 1] becomes the new basis with resource
consumption of 18 (so that we could use one additional resource unit) and objective
value of z(y) ¼ 97. Starting with this solution, we perform again pairwise exchanges
shown in Table 5.15.
Again, it was possible to improve the solution. The new solution is y ¼ [0, 0, 1,
1, 1, 1], which uses all 19 resource units that are available, and has a value of the
objective function of z(y) ¼ 101. With this new benchmark solution, we start another
series of potential improvements, shown in Table 5.16.
At this point, all feasible pairwise exchanges result in decreases of the value of the
objective function, so that the procedure terminates. Note that the fact that all
resource units happen to be used is a coincidence.
It is also worth pointing out that the Greedy algorithm alone without the second￾phase Swap procedure may result in very poor solutions. As an example, consider
the problem
P : Max z ¼ 10y1 þ 8y2 þ 7y3
s:t: 54y1 þ 48y2 þ 47y3  100
y1, y2, y3 ¼ 0 or 1:
Table 5.15 Second swap move
Leaving variable Entering variable New solution ΔR Δz
y2 y1 1, 0, 1, 1, 0, 1 4+2 ¼ 2 20 + 12 ¼ 8
y2 y5 0, 0, 1, 1, 1, 1 4+5 ¼ 1 20 + 24 ¼ +4
Table 5.16 Third swap move
Leaving
variable
Entering
variable New solution ΔR Δz
y3 y1 1, 0, 0, 1, 1, 1  6+2 ¼ 4 31+12 ¼ 19
y3 y2 0, 1, 0, 1, 1, 1 6+4 ¼ 2 31+20 ¼ 11
y4 y1 1, 0, 1, 0, 1, 1  3+2 ¼  1 17+12 ¼ 5
y4 y2 0, 1, 1, 0, 1, 1 3+4 ¼ +1: infeasible
y5 y1 1, 0, 1, 1, 0, 1 5+2 ¼ 3 24+12 ¼ 12
y5 y2 0, 1, 1, 1, 0, 1 5+4 ¼ 1 24+20 ¼ 4
y6 y1 1, 0, 1, 1, 1, 0 5+2 ¼ 3 29+12 ¼ 17
y6 y2 0, 1, 1, 1, 1, 0 5+4 ¼ 1 29+20 ¼ 9
196 5 Integer Linear ProgrammingThe ranking of the variables is y1, y2, and y3 in that order. Setting the highest￾ranked variable y1 to 1 consumes 54 resource units and achieves an objective value
of z ¼ 10. No other variables can be set to one, as only 46 resource units remain.
However, the solution y ¼ [0, 1, 1] has an objective value of z ¼ 15, much superior
to the solution obtained by the Greedy algorithm without subsequent Swap proce￾dure. To see that Greedy alone without any improvement follow-up may reach an
objective value of only half the optimum, consider the problem
P : Max z ¼ 1:01y1 þ 2y2
s:t: 1:001y1 þ 2y2  2
y1, y2 ¼ 0 or 1:
The Greedy algorithm will find the solution y ¼ [1, 0] with the objective value of
z ¼ 1.01, while the optimal solution is y ¼ [0, 1] with the value of the objecive
function z ¼ 2.
Another type of heuristic method can sometimes be advantageous, using
Lagrangean relaxation, which we described in Sect. 5.1. Since we are maximizing
the Lagrangean objective function zLagr, the optimization process tends to favor
solutions that are feasible for PIP, and this tendency will be stronger with larger
values of bui  0; for very large values of bui, the resulting solutions will therefore be
feasible as long as feasible solutions to PIP exist. Note that very high values of the
Lagrangean multipliers will put a very high emphasis on feasibility, while the
original objective function will be largely ignored. The art is to find a balance
between feasibility and the objective value.
A heuristic procedure that exploits this feature commences with some arbitrary
value of bui  0, and the corresponding relaxation PLagr is solved. If the solution
point is feasible for PIP, the value of bui  0 will be reduced and PLagr is solved again.
If, on the other hand, the solution point is not feasible for PIP, the value of bui  0 will
be increased, and PLagr is solved again.
Although we have described the procedure based on dualizing a single constraint,
it is not difficult to generalize the technique so as to dualize any number of
constraints.
As a numerical illustration of the method, we will use the assignment problem
described in Sect. 2.2.5 but change it to a maximization problem. For convenience,
we restate the problem here as a maximization problem. It is then
5.3 Solution Methods for Integer Programming Problems 197P : Max z ¼ 4x11 þ 3x12 þ 1x13 þ 8x21 þ 5x22 þ 3x23 þ 2x31 þ 6x32 þ 2x33
s:t: x11 þ x12 þ x13 ¼ 1
x21 þ x22 þ x23 ¼ 1
x31 þ x32 þ x33 ¼ 1
x11 þ x21 þ x31 ¼ 1
x12 þ x22 þ x32 ¼ 1
x13 þ x23 þ x33 ¼ 1
x11, x12, x13, x21, x22, x23, x31, x32, x33  0 and integer:
In addition to the usual constraints, we add the constraint
2x11 þ 3x12 þ x13 þ 4x21 þ 6x23 þ 5x32 þ 2x33  8:
In many practical applications of the assignment problem, an additional constraint
of this type may occur, expressing some kind of capacity, weight, or budget
restriction. Without it, the problem can very easily be solved either by using a
specialized algorithm, or simply using some all-purpose software. However, in the
presence of additional constraints, many nice features of the problem may be lost,
and these features made the problem easy to solve. With this in mind, we will refer to
such additional constraints as the complicating constraints. An obvious way to deal
with this complication is to dualize the complicating constraint and solve the
resulting assignment problem.
Consider again our numerical example. We first solve the reduced problem Pred,
i.e., the assignment problem without the complicating constraint, and obtain the
unique optimal solution ex13 ¼ ex21 ¼ ex32 ¼ 1, and exij ¼ 0 otherwise. The associated
value of the objective function is ez ¼ 15. (Note that this solution does not satisfy the
complicating constraint.) If we dualize the additional constraint so as to write
buð Þ 2x11 þ 3x12 þ x13 þ 4x21 þ 6x23 þ 5x32 þ 2x33  8 , we can consider this as a
penalty term that is added to the objective function of the original problem. The
reason is that the expression in brackets will be positive for solution points that
violate the additional constraint, so that the penalty term will make a negative
contribution to the objective function to the relaxed problem, and the penalty will
be bigger with larger values of the multiplier bu.
Using the dual variable bu ¼ 1=2 and solving the Lagrangean relaxation, we obtain
the solution bx13 ¼ bx21 ¼ bx32 ¼ 1, and bxij ¼ 0 otherwise. This is the same solution we
have obtained for the assignment problem without the complicating constraint, so
that the additional constraint is still violated.
We take this as an indication for the fact that the dual variable must be set at a
higher value than bu ¼ 1=2, so we arbitrarily set bu≔5. The unique optimal solution to
the relaxed problem is then bx13 ¼ bx22 ¼ bx31 ¼ 1 and bxij ¼ 0 otherwise. This solution
satisfies the additional constraint and is therefore feasible to the original problem
with an objective value of z ¼ 8.
198 5 Integer Linear ProgrammingAt this point we may investigate whether or not to reduce the penalty, as by
decreasing the value of the penalty term bu ¼ 5, we will improve (i.e., increase for
this maximization problem) the value of the objective function, while keeping the
solution feasible for the original problem. Since bu ¼ 1=2 was too small and bu ¼ 5
may have been too big, we can try a value in between such as bu ¼ 1. The resulting
relaxed problem then has the unique optimal solution bx11 ¼ bx22 ¼ bx33 ¼ 1 , and
bxij ¼ 0 otherwise. This solution is feasible for the original problem with an objective
value of z ¼ 11. We could now choose penalty values between ½ and 1 and continue.
(Actually, the solution we found here happens to be optimal.)
Alternatively, we could have dualized not the additional constraint and solved the
relaxation (an assignment problem), but we could have dualized all six assignment
problem constraints and solved the Lagrangean relaxation, which would be a
knapsack problem. Since we are maximizing and since all coefficients are nonnega￾tive, we may write the six assignment constraints as less-than-or-equal constraints. If
we were to proceed in this fashion, our numerical example would read as follows:
PLagr : Max zLagr ¼ 4x11 þ 3x12 þ 1x13 þ 8x21 þ 5x22 þ 3x23 þ 2x31 þ 6x32 þ 2x33
bu1ð Þ x11 þ x12 þ x13  1 bu2ð Þ x21 þ x22 þ x23  1 bu3ð Þ x31 þ x32 þ x33  1
bu4ð Þ x11 þ x21 þ x31  1 bu5ð Þ x12 þ x22 þ x32  1 bu6ð Þ x13 þ x23 þ x33  1
s:t: 2x11 þ 3x12 þ x13 þ 4x21 þ 6x23 þ 5x32 þ 2x33  8
xij ¼ 0 or 1 for all i, j:
Searching for suitable values for the dual variables bu1, bu2, ... ,bu6 is now consid￾erably more difficult since values of the six variables must be chosen simultaneously.
Furthermore, the relaxation is an integer problem that, albeit it has only a single
constraint, may not be all that easy to solve. On the other hand, dualizing only the
complicating constraint as done earlier leaves us with an assignment problem that
automatically has integer solution, so that all we have to do is solve a linear
programming problem.
Continuing with this approach, we may choose bu1 ¼ bu2 ¼ ... ¼ bu6 ¼ ½ and
obtain the objective function zLagr ¼ 3x11 + 2x12 + 7x21 + 4x22 + 2x23 + x31 + 5x32
+ x33 + 3. Maximizing this objective function subject to the budget constraint and the
zero-one specifications of the variables results in the solution bx11 ¼ bx21 ¼ bx22 ¼
bx31 ¼ bx33 ¼ 1, and bxij ¼ 0 otherwise. Since this solution violates the second, third,
and fourth of the six assignment constraints, we can increase the penalty parameters
for the violated constraints to, say, bu2 ¼ bu3 ¼ bu4 ¼ 5 . In the resulting knapsack
problem, most objective function coefficients are negative, leading to the solution
bx12 ¼ bx32 ¼ 1 and bxij ¼ 0 otherwise. This solution violates only the fifth constraint
in the original problem. At this point, we could just increase the value of bu5 in an
attempt to satisfy the fifth constraint and the process will continue.
The above example demonstrates that (1) it is not always apparent beforehand
which approach (i.e., dualizing which constraints) is preferable, and (2) which values
of the dual variables will result in quick convergence. Actually, choosing good
values for the dual variable is more of an art than a science.
5.3 Solution Methods for Integer Programming Problems 199We should also point out that the practical benefit of using Lagrangean relaxation
for solving an assignment problem with a complicating constraint will only be
present if the assignment problem is of a reasonable size. For instance, for n ¼ 15,
the assignment problem has more than 1012 solutions, making enumeration of all
feasible solutions impractical, so that a specialized algorithm is needed.
Exercises
Problem 1 (neighborhood constraints in forestry modeling): This model
concerns planning in forestry. In particular, suppose that a landowner has a number
of parcels, which, for the sake of simplicity, can be thought of as a regular grid. The
idea is now to plan which of the parcels should be harvested. Once it has been
decided to harvest a certain parcel, it will be clearcut. One restriction is to ensure that
neighboring parcels should not be harvested, so as to ensure avoiding huge clearcut
areas that foster erosion and do not sustain wildlife. Suppose that the parcels are
arranged and numbered as shown in Fig. 5.11.
Describe how to model the neighborhood constraints to ensure that neighboring
parcels are not harvested.
Solution: Define binary variables yj that assume a value of 1, if parcel j is
harvested and 0 otherwise. The constraints we are interested in can then be expressed
as conditional constraints of the type “if parcel j is harvested, then its neighbor
k cannot be harvested,” which will then have to be expressed for all neighbors of
each parcel. As an example, consider the neighboring parcels 1 and 2. Table 5.17
shows the four cases.
It is apparent that the only case that is prohibited is when both neighboring parcels
are harvested at the same time. As a result, the constraint that prevents the undesir￾able case from happening, we formulate as y1 + y2  1. This type of constraint has to
be formulated for all pairs of neighbors, e.g. parcels 1 and 2 (as we already did),
parcels 1 and 5 (as y1 + y5  1), parcels 2 and 3 (as y2 + y3  1), parcels 2 and 6 (as y2
+ y6  1), and so forth. For the above problem, there are no less than 24 such
neighborhood constraints. If parcels that only share a corner are also considered
neighbors (such as parcels 1 and 6 or parcels 8 and 11), that will add another
18 constraints, as y1 + y6  1, y8 + y11  1, and so forth.
1 2 3 4 
5 6 7 8 
9 10 11 12 
13 14 15 16 
Fig. 5.11 Arrangement and
numbering of parcels for
Problem 1
Table 5.17 Decision table y1 y2 OK?
0 0 Yes
0 1 Yes
1 0 Yes
1 1 No
200 5 Integer Linear ProgrammingProblem 2 (a warehouse distribution problem): A decision maker has pres￾ently three warehouses with capacities of 30, 10, and 50 units, respectively.
Operating the three warehouses incurs fixed operating costs of $25, $50, and $45,
respectively. The first warehouse could be expanded by up to 20 units for a cost of $1
per-unit capacity. It is also possible to close any of the existing warehouses, in which
case no operating costs are incurred at that site. Furthermore, two additional sites
have been identified, where new warehouses might be opened. The costs to open the
two new warehouses are $15 and $25, and their respective capacities are 20 and
30 units.
As far as demand goes, there are three customers with demands of 20, 60, and
40 units. These demands have to be satisfied exactly. The unit transportation costs
between the existing and potential warehouses and the customers are shown in
Table 5.18, where the first three supply points refer to the existing warehouses,
while the last two supply points symbolize the potential new warehouses. Formulate
and solve an integer programming problem that minimizes the total costs.
Solution: In order to formulate the problem, we need to define a number of
variables. First of all, we define binary variables y1, y2, y3, y4, and y5, which assume a
value of one, if the warehouse is kept open (for the first three existing warehouses) or
is newly opened in case of the last two warehouses. Furthermore, we define the
continuous variable w, which denotes the number of capacity units, by which the first
warehouse is expanded. In addition, we will define the usual continuous variables xij
as the quantity that is shipped from warehouse i to customer j.
The objective function is then the sum of facility costs (operating costs for the
existing warehouses and opening costs for the planned new warehouses), expansion
costs for the first warehouse, and transportation costs. The facility costs are, of
course, only incurred if the facilities are actually opened, so that we have 25y1 + 50y2
+ 45y3 for the existing warehouses and 15y4 + 25y5 for the new warehouses. The
expansion costs for the first warehouse are 1w. Finally, the transportation costs are
3x11 + 7x12 + 4x13 + 2x21 + ...+ 9x53.
Consider now the constraints. First there are the capacity constraints of the
warehouses. For instance, the second warehouse has a capacity of 10, provided we
keep it open. The variable that determines this is y2, so that the constraint states that
the flow out of warehouse 2 cannot exceed the capacity of the warehouse, or x21 + x22
+ x23  10y2. The constraints for the other existing or planned warehouses, except
for the first, are similar. The capacity of the first warehouse equals 30y1 plus the
capacity of the expansion (if any), which is w. We also have to specify that the
expansion will only be undertaken if the warehouse is kept open, as it is meaningless
Table 5.18 Unit transpor￾tation costs
Customer 1 Customer 2 Customer 3
Warehouse 1 3 7 4
Warehouse 2 2 6 8
Warehouse 3 9 3 4
New warehouse 1 5 6 4
New warehouse 2 7 3 9
5.3 Solution Methods for Integer Programming Problems 201to decide to close warehouse 1 and then expand its capacity. This is written as
w  20y1. The reason is that if the warehouse is kept open, we have y1 ¼ 1, so that
the constraint states that we can expand its capacity of 20 units. On the other hand, if
warehouse 1 is closed, y1 ¼ 0 and the constraint states that w  0, meaning that no
expansion is possible. The demand constraints then require the inflow into the
customer sites to be equal to the demand at the site. The complete formulation is
then as follows:
P : Min z ¼ 25y1 þ 50y2 þ 45y3 þ 15y4 þ 25y5 þ 1w
þ3x11 þ 7x12 þ 4x13 þ 2x21 þ 6x22 þ 8x23 þ 9x31 þ 3x32 þ 4x33
þ5x41 þ 6x42 þ 4x43 þ 7x51 þ 3x52 þ 9x53
s:t: x11 þ x12 þ x13  30y1 þ w
x21 þ x22 þ x23  10y2
x31 þ x32 þ x33  50y3
x41 þ x42 þ x43  20y4
x51 þ x52 þ x53  30y5
w  20y1
x11 þ x21 þ x31 þ x41 þ x51 ¼ 20
x12 þ x22 þ x32 þ x42 þ x52 ¼ 60
x13 þ x23 þ x33 þ x43 þ x53 ¼ 40
xij  0 for all i, j; w  0
y1, ... , y5 ¼ 0 or 1:
Solving the problem results in a decision to keep warehouses 1 and 3 open, while
closing warehouse 2. In addition, warehouse 5 will be opened, while warehouse 4 is
not. It was also decided to expand the capacity of warehouse 1 by 10 units. The
shipments are as follows: from warehouse 1, we send 20 units to the first and 20 units
to the third customer, from warehouse 3 we send 30 units to customer 2 and 20 units
to customer 3, and from the newly opened warehouse 5 we ship 30 units to customer
2. The total cost of the operations is $505.
Problem 3 (choosing vehicles for a display): An entrepreneur wants to put up an
exhibit featuring some antique cars. The vehicles potentially available are a Bugatti,
Cadillac, Cobra, Corvette, Pierce Arrow, and Studebaker. The impact of displaying
the individual vehicles has been estimated in terms of the number of people who
would make a special trip to see a vehicle as 58, 37, 42, 40, 55, and 33, respectively.
The budget of the organizer is $15,000, and the costs to transport the automobiles to
the venue (they are presently located at different sites) and the costs of their
insurance (depending on the vehicles’ estimated value) are $6000, $4000, $3800,
202 5 Integer Linear Programming$4200, $5500, and $3200, respectively. The obvious idea is to choose vehicles for
the exhibit, so as to maximize the total impact, while staying within the budget. In
addition, there are some further requirements.
• Choose at least three vehicles for the exhibit.
• If a Corvette is included in the exhibit, then a Cobra must also be included.
• If a Bugatti is not included in the show, then a Cadillac must be included.
Solution: We first we define binary variables y1, y2, y3, y4, y5, and y6 that assume a
value of one, if the first, second, ..., sixth vehicle is included in the exhibit, and
0 otherwise. The formulation of the objective function and the budget constraint is
straightforward. Consider now the additional requirements. The number of vehicles
included in the exhibit is expressed as the sum of all variables, so that we can write y1
+ y2 + y3 + y4 + y5 + y6  3.
The next step is the conditional constraint “if Corvette, then Cobra,” or, more
formally, “if y4 ¼ 1, then y3 ¼ 1 as well.” The possible solutions and their
acceptability are shown in Table 5.19.
The undesirable solution y4 ¼ 1 and y3 ¼ 0 can be avoided by including the
constraint y4  y3, which violates the undesirable solution, while it is valid in the
other three solutions.
Consider now the last requirement. The conditional constraint is “if not Bugatti,
then Cadillac,” or, equivalently, “if y1 ¼ 0, then y2 ¼ 1.” Again, consider the
decision table shown in Table 5.20.
The only solution that violates the condition is the one that has neither of the two
vehicles in the exhibit. In other words, at least one of the two vehicles must be in the
exhibit, so we can formulate y1 + y2  1.
The formulation of the entire problem is then
Table 5.19 Decision table
for the second requirement
in Problem 5
y4 y3 OK?
0 0 Yes
0 1 Yes
1 0 No
1 1 Yes
Table 5.20 Decision table
for the third requirement in
Problem 5
y1 y2 OK?
0 0 No
0 1 Yes
1 0 Yes
1 1 Yes
5.3 Solution Methods for Integer Programming Problems 203P : Max z ¼ 58y1 þ 37y2 þ 42y3 þ 40y4 þ 55y5 þ 33y6
s:t: 6, 000y1 þ 4, 000y2 þ 3, 800y3 þ 4, 200y4 þ 5, 500y5 þ 3, 200y6  15, 000
y1 þ y2 þ y3 þ y4 þ y5 þ y6  3
y4  y3
y1 þ y2  1
y1, y2, y3, y4, y5, y6 ¼ 0 or 1:
Table 5.21 displays solutions and objective values for a large variety of budgets.
It is apparent that there is no feasible solution for any budget strictly less than 11, as
we have to include at least three vehicles in the exhibit, and the three least expensive
cars, the sixth, third, and second vehicle, cost 3200 + 3800 + 4000 ¼ $11,000. On
the other hand, exhibiting all vehicles costs $26,700, so that any budget at or above
this level will enable the organizer to exhibit all vehicles. Also notice the “granular￾ity” of the solutions: an increase in the budget by $1000 results in an increase in the
objective value by 7 (if increasing the budget from $11,000 to $12,000), by 14 (if the
budget is increased from $12,000 to $13,000), by 7 if the budget is increased from
$13,000 to $14,000, and so forth.
Problem 4 (cutting plane method): Consider the following all-integer linear
programming problem:
P : Min z ¼ 3y1 þ 5y2
s:t: 9y1 þ 12y2  122
2y1 þ y2  6
y1  4
y1, y2  0 and integer:
Solve the linear programming relaxation of the problem and then sequentially
introduce cutting planes to solve the problem for eight steps or to optimality,
whichever comes first.
Table 5.21 Solutions of the car exhibit problem for different budgets
Budget
(in $10,000) 11 12 13 14 15 16 17 18 19
Cars included 2,
3, 6
2,
3, 4
1,
3, 6
1,
3, 4
1,
5, 6
1,
3, 5
1, 2, 3,
6
1, 2, 3,
4
1, 3, 5,
6
z-value 112 119 133 140 146 155 170 177 188
Budget
(in $10,000) 20 21 22 23 24 25 26 27
Cars included 1,
3, 4,
5
2,
3, 4,
5, 6
1,
2, 3,
4, 6
1,
3, 4,
5, 6
1,
2, 3,
4, 5
1,
2, 3,
4, 5
1,
2, 3,
4, 5
1, 2, 3,
4, 5, 6
z-value 195 207 210 228 232 232 232 265
204 5 Integer Linear ProgrammingSolution: When we reformulate the cuts, we write them as equations with the
added excess variables in the rightmost column of Table 5.22. This is to indicate
which excess variable was used in which cut, which is useful when formulating cuts
later in the process. However, when adding the cut to the problem, we write the
constraint without the excess variable as an inequality, since the solver will auto￾matically add slack and excess variables.
The successive cutting planes generated for this problem are shown in Table 5.22.
Problem 5 (solving a problem via branch-and-bound): Consider the following
all-integer optimization problem:
Table 5.22 Cutting planes for Problem 6
# Solution Cutting plane
1 Optimal solution of the LP relaxation: y1 ¼ 4, y2 ¼ 7:1667, E1 ¼ 0,
E2 ¼ 9:1667, S3 ¼ 0 with z ¼ 47:8333
E1 + S3  1, or,
equivalently,
8y1 + 12y2  E4 ¼
119
2 y1 ¼ 4, y2 ¼ 7:25, E1 ¼ 1, E2 ¼ 9:25, S3 ¼ 0, E4 ¼ 0 with z ¼
48:25
S3 + E4  1, or,
equivalently,
7y1 + 12y2  E5 ¼
116
3 y1 ¼ 3, y2 ¼ 7:9167, E1 ¼ 0, E2 ¼ 7:9167, S3 ¼ 1, E4 ¼ 0, E5 ¼ 0
with z ¼ 48:5833. Note primal degeneracy! Arbitrarily choose E1
and E4.
E1 + E4  1, or,
equivalently,
17y1 + 24y2  E6 ¼
242
4 y1 ¼ 3:3333, y2 ¼ 7:7222, E1 ¼ 0:6667, E2 ¼ 8:3889, S3 ¼
0:6667, E4 ¼ 0:3333, E5 ¼ 0, E6 ¼ 0 with z ¼ 48:6111.
E5 + E6  1, or,
equivalently,
31y1 + 48y2  E7 ¼
475
5 y1 ¼ 3:6667, y2 ¼ 7:5278, E1 ¼ 1:3333, E2 ¼ 8:8611, S3 ¼
0:3333, E4 ¼ 0:6667, E5 ¼ 0,E6 ¼ 1, E7 ¼ 0 with z ¼ 48:6389.
E5 + E7  1, or,
equivalently,
38y1 + 60y2  E8 ¼
592
6 y1 ¼ 4, y2 ¼ 7:3333, E1 ¼ 1:1250, E2 ¼ 9:3333, S3 ¼ 0, E4 ¼ 1,
E5 ¼ 0, E6 ¼ 2, E7 ¼ 1, E8 ¼ 0 with z ¼ 48:6667. Note primal
degeneracy! Arbitrarily choose S3 and E5.
S3 + E5  1, or,
equivalently,
6y1 + 12y2  E9 ¼
113
7 y1 ¼ 3:3750, y2 ¼ 7:7292, E1 ¼ 2, E2 ¼ 8:4792, S3 ¼ 0:625,
E4 ¼ 0:75, E5 ¼ 0:375, E6 ¼ 0:875, E7 ¼ 0:625, E8 ¼ 0, E9 ¼ 0
with z ¼ 48:7708.
E8 + E9  1, or,
equivalently,
44y1 + 72y2  E10 ¼
706
8 y1 ¼ 3:5, y2 ¼ 7:6667, E1 ¼ 1:5, E2 ¼ 8:6667, S3 ¼ 0:5, E4 ¼ 1,
E5 ¼ 0:5, E6 ¼ 1:5, E7 ¼ 1:5, E8 ¼ 1, E9 ¼ 0E10 ¼ 0 with z ¼
48:8333.
...........
Optimal all-integer solution: y1 ¼ 3, y2 ¼ 8, E1 ¼ 1, E2 ¼ 8, S3 ¼
1 with z ¼ 49.
5.3 Solution Methods for Integer Programming Problems 205P : Max z ¼ 7y1 þ 7y2
s:t: 6y1 þ 7y2  34
10y1 þ 6y2  43
y1, y2  0 and integer:
(a) Produce the solution trees for branching, starting with y1 and y2, respectively.
(b) What would the optimal solution be, if the integrality requirement for y2 had
been dropped?
(c) What are the additional constraints that were introduced between the root of the
tree and the last integer solution found in the tree, assuming that branching starts
with y1?
(d) What would have happened, if the left child that resulted from the first branching
in either tree had an objective value of 34.8?
Solution:
(a) The solution that starts branching on y1 is shown in Fig. 5.12.
The solution tree that starts branching on y2 is shown in Fig. 5.13.
The problem has actually three alternative optimal solutions. Note that in
both trees, branching 4 is necessary to complete the solution tree. The optimal
solutions are: y1 ¼ 3 and y2 ¼ 2, y1 ¼ 2 and y2 ¼ 3, as well as y1 ¼ 1 and
y2 ¼ 4, all with a value of the objective function of z ¼ 35.
Fig. 5.12 Branching on y1 first
206 5 Integer Linear Programming(b) In this case we would have had to start branching with y1, so that Fig. 5.12
applies. The optimal solution would be the right child of the root of the tree, i.e.,
solution y2, which has y1 ¼ 3 and y2 ¼ 2:1667 with an objective value of z ¼
36:1667.
(c) The additional constraints are y1  3, y2  2, and y1  3.
(d) In both trees, we would never have branched from the left child of the first
branching.
Problem 6 (choosing the correct branch-and-bound tree): Consider the fol￾lowing all-integer programming problem:
P : Max z ¼ 2y1 þ y2
s:t:  3y1 þ 15y2  45
3y1  4y2  9
y1, y2  0 and integer
Four solution trees have been developed by four different individuals, each
claiming that their tree is correct. The trees are shown in Fig. 5.14a–d. However,
only one of the trees is correct. Which one? For each solution tree, write one sentence
that explains why this is or is not the correct tree.
Fig. 5.13 Branching on y2 first
5.3 Solution Methods for Integer Programming Problems 207Fig. 5.14 Four different solution trees
208 5 Integer Linear ProgrammingFig. 5.14 (continued)
5.3 Solution Methods for Integer Programming Problems 209Solution:
The solution tree in Fig. 5.14a is false. The right child of the root of the tree is not
feasible. If the solution there had been correct, branching should have continued
at that node.
The solution tree in Fig. 5.14b is false. The branching on the second level should be
y2  4 (as is), but the branch to the right child should be y2  5, not y2  4.
The solution tree in Fig. 5.14c is false. The second branching leading to the left child
has the objective value increase from z ¼ 224
/5 to z ¼ 231
/3, which cannot happen
in a maximization problem.
The solution tree in Fig. 5.14d is correct.
Problem 7 (heuristics: Greedy and Swap method): Consider the following
integer programming problem.
P : Max z ¼ 21y1 þ 11y2 þ 65y3 þ 58y4 þ 122y5
s:t: 21y1 þ 10y2 þ 42y3 þ 37y4 þ 64y5  640
y2  2
y3  1
y4  1
y4  2
y5  2
y1, y2, y3, y4, y5  0 and integer:
Use the Greedy algorithm and a Swap interchange to find a solution.
Solution: Ordering the variables with respect to their objective function contri￾bution per resource unit, the order is y5, followed by y4, y3, y2, and y1. Before we start
increasing the values of the variables, we need to set the variables to their minimum
values, i.e., y3 ¼ 1 and y4 ¼ 1, so that we do not have to worry about lower bounds
anymore. This leaves us with 640  42  37 ¼ 561 resource units.
We now start the allocation with y5. The upper bound is 2, so we set y5 ¼ 2, which
leaves us with 561  2(64) ¼ 433 resource units. The next best variable is y4. As its
upper bound equals 2 and its value already equals 1, we can only increase y4 by
1. This leaves 433  1(37) ¼ 396 resource units. The next valuable variable is y3. It
does not have an upper bound, so that we increase its value as much as the remaining
resource units allow. We have 396 units left, each unit of y3 requires 42 units, so that
the largest value of y3 ¼ 9. Increasing y3 by that value leaves us 396  9(42) ¼ 18
resource units left. The next most valuable variable is y2, whose upper bound is
2. However, the remaining resource units are only good for an increase of 1. This
leaves us 8 resource units, which are not sufficient for any other increase. In
summary, we have the solution y ¼ [0, 1, 10, 2, 2], for which the objective value
z ¼ 1021 may be calculated.
In the Swap procedure we will decrease the value of a variable by one, thus freeing
some resources, which we then try to use by increasing the value of some other
variable. For instance, decreasing the value of y2 by one frees 10 units for a total of
18, which is not sufficient to increase any other variable by an integer amount.
210 5 Integer Linear ProgrammingReducing the variable y3 by one frees up 42 units, so that 42 + 8 ¼ 50 resource
units are now available. Note that it also reduces the objective value by 65. Those
resource units may be used to increase the value of y1 by 2, which increases the
objective value by 42, not enough to make up for the loss of 65. Alternatively, we
may increase y4 by one, which increases the objective value by 58, also not sufficient
to make up for the loss.
We may now try to reduce the value of y4 by 1, freeing 37 resource units for a
total of 45. Note that the objective value decreases by 58 in the process. The resource
may now be used to increase y3 by one unit, which increases the value of the
objective function by 65. This represents a net gain of +7, so that we make this
change. The new solution is now y ¼ [0, 1, 11, 1, 2] with an objective value of
z ¼ 1028. Three resource units remain available.
The process would continue here. We terminate the procedure at this point. It so
happens that the solution found here is optimal.
Problem 8 (Lagrangean relaxation): Consider a maximization assignment
problem with coefficients as shown in the matrix
C ¼
924
315
687
2
6
4
3
7
5,
and assume that the following complicating constraint must also be satisfied:
5x11 þ x12 þ 5x13 þ 2x21 þ 4x23 þ 3x31 þ 6x32 þ 5x33  9
(a) Solve the assignment problem (e.g., as a linear programming problem), ignoring
the complicating constraint. Is the solution unique? Show that it violates the
complicating constraint.
(b) Dualize the complicating constraint and solve the resulting Lagrangean relaxa￾tion as an assignment problem, using a value of bu ¼ 1 for the dual variable.
(c) Same as (b) above, but with bu ¼ 2.
(d) Same as (b) above, but with bu ¼ 1:5.
(e) Compare and discuss the results in (b), (c), and (d) above.
Solution:
(a) Solving the assignment problem without the complicating constraint, i.e., the
reduced problem Pred, we find the unique optimal solution ex11 ¼ ex23 ¼ ex32 ¼ 1
and exij ¼ 0 otherwise. The associated maximal value of the objective function
ignoring the complicating constraint is ez ¼ 22. This solution will yield a left￾hand side value of 15 in the complicating constraint, and since the right-hand
side value of this constraint is 9, the constraint is violated.
5.3 Solution Methods for Integer Programming Problems 211(b) Dualizing the complicating constraint with a value bu ¼ 1 for the dual variable
transfers the problem into a regular assignment problem with maximizing
objective and the coefficient matrix
C ¼
4 1 1
11 1
32 2
2
6
4
3
7
5:
The solution to this problem is the same is that shown in (a), i.e., bx11 ¼ bx23 ¼
bx32 ¼ 1 and bxij ¼ 0 otherwise (alternatively bx11 ¼ bx22 ¼ bx33 ¼ 1 and bxij ¼ 0
otherwise) with objective function value bzLagr ¼ 16. The complicating constraint
is again violated. The alternative solution also violates the complicating
constraint.
(c) With bu ¼ 2, we obtain
C ¼
1 0 6
1 1 3
0 4 3
2
6
4
3
7
5,
which has optimal solutions bx12 ¼ bx23 ¼ bx31 ¼ 1 and bxij ¼ 0 otherwise, as well as
bx11 ¼ bx22 ¼ bx33 ¼ 1 and bxij ¼ 0 otherwise and bzLagr ¼ 15. Now the left-hand side
of the complicating constraint equals 8 in the first solution and 10 in the second,
so that in case of the first solution, this constraint is satisfied. The first solution has
a value of the objective function of the original problem of bz ¼ 13.
(d) With bu ¼ 1:5, we obtain
C ¼
1:5 0:5 3:5
0 1 1
1:5 1 0:5
2
6
4
3
7
5,
which has the unique optimal solution bx11 ¼ bx22 ¼ bx33 ¼ 1 and bxij ¼ 0 otherwise,
which has an optimal value of the objective function bzLagr ¼ 17. Now the left￾hand side of the complicating constraint equals 10, so that this constraint is again
violated. The value of the objective function of the original problem is bz ¼ 17.
212 5 Integer Linear Programming(e) With bu ¼ 1, as in (b), the complicating constraint is violated, indicating that the
value of bu should be increased. With bu ¼ 2 as in (c), the complicating constraint
is satisfied, demonstrating that bu is sufficiently large. Finding an intermediate
value such as bu ¼ 1:5 as in (d), the complicating constraint is again violated,
showing that the value of 1.5 is too small for the Lagrangean multiplier.
Enumerating all solutions of the original assignment problem without additional
constraints and then checking which solutions satisfy the complicating con￾straint, we can see that bu ¼ 2 actually results in the unique optimal solution to
the original problem with the complicating constraint, i.e., x12 ¼ x23 ¼ x31 ¼ 1
and xij ¼ 0 with z ¼ 13. Note that the relationship bzLagr  bz developed in Sect.
4.1 is not violated, even though we obtained a z-value of 22 in (a) which is larger
than the objective function values in (b), (c), and (d). This is because bzLagr < bz
for solutions that are infeasible, which is the case in (a), (b), and (d). A summary
of the results is presented in Table 5.23.
Table 5.23 Summary of results for problem 10
Problem Dual variable bu Optimal solution
Objective
function
value
zLagr z Feasible for P?
(a) Pred (PLagr) 0 ex11 ¼ ex23 ¼ ex32 ¼ 1 22 22 No
(b) PLagr 1 bx11 ¼ bx23 ¼ bx32 ¼ 1 16 22 No
PLagr 1 bx11 ¼ bx22 ¼ bx33 ¼ 1 16 17 No
(c) PLagr 2 bx12 ¼ bx23 ¼ bx31 ¼ 1 15 13 Yes
PLagr 2 bx11 ¼ bx22 ¼ bx33 ¼ 1 15 17 No
(d) PLagr 1.5 bx11 ¼ bx22 ¼ bx33 ¼ 1 15.5 17 No
– P – x12 ¼ x23 ¼ x31 ¼ 1 – 13 Yes
5.3 Solution Methods for Integer Programming Problems 213Network Models 6
Graph theory, the subject at the root of this chapter, dates back to 1736, when the
Swiss mathematician Leonhard Euler considered the now famed “Königsberg bridge
problem.” At that time, there were seven bridges across the River Pregel that ran
through the city of Königsberg on the Baltic Sea, and Euler wondered whether or not
it would be possible to start somewhere in the city, walk across each of the bridges
exactly once, and return to where he came from. (It was not.) We will return to
Euler’s problem in Sect. 6.5. Two hundred years later in 1936, the Hungarian
mathematician Denès König wrote the seminal book “The Theory of Finite and
Infinite Graphs,” that laid the foundations of modern graph theory. The subject was
advanced by operations researchers in the 1950s, most prominently by L.R. Ford and
D.R. Fulkerson to deal with path and network flows. For a more comprehensive
treatment of the topics covered in this chapter, readers are referred to the pertinent
literature, e.g. Eiselt and Sandblom (2000), or Murty (2006).
6.1 Definitions and Conventions
The models discussed in this section are optimization problems on a structure
commonly known as a graph. A graph (for simplicity, we will refer to graphs also
as networks, even though many graph-theorists will disagree) consists of nodes
(sometimes referred to as vertices) and arcs (or edges). Many authors refer to
undirected connections as edges and directed connections as arcs. A node and an
arc leading to or from it are said to be incident. The graph in Fig. 6.1 is an example
with the nodes n1, n2, n3, n4, and n5 represented by circles and the arcs represented by
directed or undirected lines. Arcs are written as either aij or (ni, nj), whatever is more
convenient. Similarly in an undirected graph, an edge that connects the nodes ni and
nj is either written (ni, nj) or eij. A graph that contains only undirected edges is called
an undirected graph, one with only directed arcs is a directed graph (frequently
referred to as a network), and a graph that includes directed and undirected arcs is
called a mixed graph.
# The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
H. A. Eiselt, C.-L. Sandblom, Operations Research, Springer Texts in Business and
Economics, https://doi.org/10.1007/978-3-030-97162-5_6
215Graphs can be stored in the form of an adjacency matrix A ¼ (aij), in which aij
equals the value of one, if an edge between nodes ni and nj or an arc from node ni to
node nj exists, and 0 otherwise. For the graph in Fig. 6.1, we have the adjacency
matrix
A ¼
01110
00100
11000
00101
01010
2
6
6
6
6
6
6
4
3
7
7
7
7
7
7
5
:
If the graph is undirected, its adjacency matrix will be symmetric (i.e., aij ¼ aji for
all i and j), while this is not the case in directed or mixed graphs (such as the one in
our example). It is also possible to indicate not only that a connection exists, but
some other feature about this connection, e.g. its distance, cost, or other measures. In
such a case, we replace the “1” for existing arcs or edges by the value assigned to the
arc or edge. Depending on the meaning of the values, such a matrix could then be a
distance matrix, cost matrix, or similar. It is typically assumed that all the values
associated with edges and arcs are integer. If this is not originally the case, we can
use appropriate rounding or scaling techniques.
A path is defined as a sequence of nodes and arcs that starts at a node and ends at
some node (possibly, but not necessarily, where it started). In case a path includes
directed arcs, they must be traversed in the direction indicated by its arrow. In the
graph in Fig. 6.1, the sequence n5—a52—n2—a23—n3 is a path. A circuit in a graph
is defined as a path that begins and starts at the same node. In the graph in Fig. 6.1,
the sequence n1—a14—n4—a43—n3—a31—n1 is a circuit. A node nj is said to be
reachable from another node ni, if there exists at least one path from ni to nj. A
connected graph is an undirected graph, in which each node is reachable from each
other node. The graph in Fig. 6.1 is connected. A directed graph is strongly
connected, if there exists at least one path from each node ni to each other node nj.
A tree (or tree graph) is defined as a connected undirected graph, with the
property that the removal of any arc or edge from it will render the graph discon￾nected. In that sense, a tree graph is the minimalist connected structure. An example
Fig. 6.1 Mixed graph
216 6 Network Modelsfor a tree is shown in Fig. 6.2. Also note that there exists exactly one path between
any pair of nodes in a tree.
Another specific type of graph is a so-called bipartite graph. It is characterized by
the fact that its set of nodes is subdivided into two mutually exclusive and collec￾tively exhaustive subsets. Call these two subsets O and D for reasons to become clear
below. Arcs or edges exist in a bipartite graph exclusively between the sets O and D,
i.e. each arc or edge has one end in O and the other in D. An example is shown in
Fig. 6.3.
We have encountered such graphs in linear programming when discussing the
transportation and assignment problems (Sect. 2.2.5) in this book. This also explains
the choice of name of the sets: We are shipping from origin O to destination D.
Any other problem-specific notation will be introduced whenever it is needed.
We should mention that while all problems discussed in this chapter can be
formulated as integer programming problems and solved with any of the pertinent
solvers, such a process is typically inefficient and may hide internal structures that
allow the user to gain insight into the problem. Typically, specific network-based
algorithms are very efficient and outperform general-purpose integer programming
solvers when applied to the particular class of problems they were designed to solve.
However, such specific methods depend on the network structure. Even a single
additional constraint may destroy the particular structure and will no longer allow
using the special algorithm, so that we have to return to standard integer program￾ming formulations.
Fig. 6.2 Tree
Fig. 6.3 Bipartite graph
6.1 Definitions and Conventions 2176.2 Social Network Analysis
Social network analysis is a relatively young branch of network analysis. It is
concerned about the assessment of and relations between individuals in a group
and their importance. In order to start our discussion, suppose that each individual in
a group is represented by a node and the interactions between the individuals are
shown as a directed or undirected arcs. These interactions can indicate friendship
(which must be measured by some means), the fact that two individuals had any kind
of contact with each other in a specific time frame, or any other relation. To further
explain the concept, consider the graph in Fig. 6.4.
If the graph were to represent the members of a class in school and the edges
denote existing friendly relationships, we could consider the student who is
represented by node n7 more of a loner with just a single friend, while the student
behind node n2 appears to be rather popular. Another important application is related
to terrorist networks. The goal in their analysis is to find the leaders, lieutenants, and
foot soldiers. One of the analyses will attempt to determine which of the elements are
the key players in this group. In order to do so, it is necessary to outline measures that
express the degree of centrality of a node in the graph. One guiding principle in this
quest could be P. Bonacich’s assertion that “Power comes from being connected to
those who are powerless.”
The simplest measure of centrality is the so-called degree centrality. The degree
d(ni) of a node ni in an undirected graph is defined as the number of edges incident to
that node. A high degree of a node indicates that the element is connected to many
people, so that it may be considered important. In our example, the degrees of the
nodes are 3, 5, 4, 3, 3, 2, 1, 4, 3, and 4, respectively, so that we may conclude that the
individual behind node n2 is a central figure in the network.
Another measure is betweenness centrality. This measure requires the determina￾tion of the shortest paths between all nodes. An efficient algorithm for this task will
be provided in Sect. 6.4. In our example in which there are no weights associated
with the edges in the graph that could indicate the strength of the link between
elements (such as the degree of friendship, the number of phone calls per week
Fig. 6.4 A social network
218 6 Network Modelsbetween the individuals, or a similar measure), we consider the distance between any
two nodes simply as the number of arcs or edges on the shortest path between the
nodes. In other words, if Mary wants to contact Robert and this is only possible if she
talks to Patricia first, who then talks to Ben, who, in turn, talks to Robert (which
sounds a bit like Chinese whispers), then we will say that the shortest path between
Mary and Robert is 3, as 3 contacts (edges) are necessary to transmit the message.
In our example, let D ¼ (dij) denote the matrix of shortest paths, in which the
element dij denotes the number of edges on the shortest path from ni to nj.
D ¼
0112232122
1011122212
1102221221
2120123122
2121013222
3222103221
2213330332
1221223011
2122223101
2212212110
2
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
4
3
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
5
:
While the matrix D above shows the lengths of the shortest paths, the matrix
E indicates where these paths are by listing the nodes on the shortest path(s).
Multiple shortest paths are separated by a comma, e.g., between nodes n1 and n6
there are three shortest paths, one via nodes n2 and n5, one via nodes n3 and n10, and
one via nodes n8 and n10.
E¼
  2,8 2 25,310,810 3  2,8 3,8
    5 3 1,4,9  3,9
  2 2 10  1,10 2,10 
2,8  2   5 23  2,8 8
2  2   23 4 2 6
25,310,810 5 10 5   310 10 10 
3 3  23 23 310  13,310 23,310 3
 1,4,9 1,10  4 10 13,310  
2,8  2,10 2,8 2 10 23,310  
3,8 3,9  8 6  3  
2
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
4
3
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
5
In order to determine the importance of a node as a detour node, we determine the
number of times that a node is a detour node on a shortest path. In case there are
multiple shortest paths, say, r shortest paths, each detour node is counted as 1/r. In
our example, n2 and n8 are counted ½ each for the shortest path between n1 and n4,
6.2 Social Network Analysis 219the detour nodes n2, n3, n5, and n8 are counted 1=3 each as they each appear once on
one of the three shortest paths between n1 and n6, while n10 is counted 2=3 as it
appears on two of the shortest paths between n1 and n6. Finally, n10 is counted as
1, as it appears on the unique shortest path between n6 and n3. Adding up the number
of times a node appears as a detour node results in counts of
12=3, 172=3, 182=3, 22=3, 42=3, 2, 0, 62=3, 12=3, and 121=3 , respectively. This indicates
that node n3 is the node that has the highest betweenness centrality, followed by
node n2. These two elements take a central place in the network.
Yet another measure of centrality is closeness centrality. This measure is based on
the lengths of shortest paths from a node to all other nodes. Note that again, we need
to determine shortest paths for this measure. Formally, closeness centrality is defined
as the inverse value of the sum of lengths of shortest paths from the node under
consideration to all other nodes. In our example, we can simply add the values in
each of the columns of the distance matrix D to obtain the sum of lengths of shortest
paths, which are 16, 13, 14, 16, 16, 18, 22, 15, 16, and 14. The inverse values are the
closeness centrality. It appears that node n7 is performing worst on this measure
(which is also visible in the graph in Fig. 6.4), while n2 performs best, albeit not by a
wide margin.
Other, somewhat more advanced, concepts have been devised, such as eigenvalue
centrality or one of its modifications, called Page Rank (named after one of the
co-founders of Google), which, among other uses, is used by search engines to
determine the ranks of files as they appear on the screen.
6.3 Network Flow Problems
In its simplest form, a network flow problem can be described as follows. It starts
with a graph, in which an unlimited (or, at least, sufficiently large) quantity of a
homogeneous good is available at a node called the source, while as many units as
possible are to be shipped from the source through the network to another node,
named the sink, where these units are in demand. (Some authors call this an O – D
flow, which refers to the flow from origin to destination.) Units can be shipped
through as many nodes as necessary, as long as they do not exceed the capacity,
i.e. an upper bound on the flow specified at each of the arcs and/or the nodes in the
network. Typically, we assume that there are no losses anywhere in the network.
Once the problem has been solved, the optimal flow pattern indicates how many
units are shipped on which arcs, and the optimal flow value indicates how many flow
units are shipped through the network.
One of the many examples deals with the evacuation of people from buildings,
convention halls, or even cities. As an example, consider the situation shown in
Fig. 6.5.
The floor plan depicts a large exhibition hall with the squares denoting kiosks. An
escape plan is needed for the auditorium that has only one entrance labeled by ns at
the bottom left of Fig. 6.5, and one exit on the right labeled by nt. People are assumed
to enter the hall through ns, from where they have to be evacuated through the exhibit
220 6 Network Modelshall to the door at nt and on to the outside. The walkways along the walls of the
exhibit hall are fairly wide, and each of their segments (from one corner to another)
was shown to let up to 60 people per minute through in an emergency situation. The
walkways in the center are narrower, so that only 40 people per minute can pass
along each segment. The steps on some of the outside segments shown by lines
across the walkway, present additional obstacles, so that these segments only allow
30 people per minute to pass through them. The question is now how many people
can pass through the exhibition hall in each minute. Given the capacity of the
auditorium, it can then be computed how long an evacuation of the auditorium
through the exhibit hall (from ns to nt) will take. The graph in Fig. 6.6 shows an
Fig. 6.5 Floor plan
Fig. 6.6 Arc capacities for evacuation
6.3 Network Flow Problems 221image of the situation with the appropriate arc capacities. The quest is now to find a
pattern with the largest possible flow from ns to nt.
A number of additional applications of flow problems can be found in Eiselt and
Sandblom (2000).
In order to formulate the problem, we first need to define variables. Here, the
continuous flow variables are xij, defined as the flow along the arc aij from ni directly
to node nj. Consider the example of Fig. 6.6. Since all edges are undirected, we need
to define two variables for each edge, one for each direction. Given that there are
17 edges, we will need 34 variables. (Some of the variables can be ignored, though:
we do not have to consider flows into the source, as the entire purpose is to get as
much flow out of there as possible. In other words, we do not need to define x1s and
x5s. Similarly, we do not need to consider flows out of the sink.) The objective will
then maximize the flow through the network. This flow can be expressed as the
number of flow units that are sent out of the source. In the example of Fig. 6.6, this is
xs1 + xs5. Alternatively, we can consider the number of flow units that are sent into
the sink, i.e., x6t + x9t.
As far as constraints are concerned, we have to consider two types. The first set of
constraints will have to ensure that no flow is lost or created at any of the nodes. Such
constraints are commonly called conservation equations, ( flow) balancing
equations, or Kirchhoff node equations in reference to their counterpart in electrical
networks. They are formulated by requiring that the number of units that flow into a
node equals the number of units that flow out of a node. In the above example, the
conservation equation for the node n3 is x23 + x43 + x63 ¼ x32 + x34 + x36. Conserva￾tion equations have to be formulated for each node except for the source and the sink.
The second set of constraints formulates the capacity constraints that require that the
flow along each segment does not exceed the upper limit. Again, in this example we
have xs1  60, xs5  60, x12  60, x21  60, and so forth. Adding the nonnegativity
constraints, a mathematical formulation has been obtained and the problem could be
solved with any off-the-shelf linear programming software. Whenever available,
specialized algorithms are, however, much faster, as they can exploit the special
structure of the problem.
Ford and Fulkerson were the first to describe a method to solve the maximal flow
problem. Their technique belongs to the large class of the so-called labeling
methods. Labeling methods have been developed for many network models, and
they all have in common that they are very efficient. The idea of Ford and
Fulkerson’s incremental method is to (incrementally) increase the flow in forward
arcs, and to decrease the flow in backward arcs. Both steps are necessary in order to
reach an optimal solution. In order to illustrate, consider a “forward arc,” in which
we would like to increase the flow. If the capacity of such an arc is, say, 7, and the
present flow in the arc is 4, then we can increase the flow on this arc by the present
slack of 7–4 ¼ 3 units. On the other hand, if a “backward arc” has a capacity of 7 and
a present flow of 4, we can decrease the flow on this arc by no more than its present
flow, i.e., by 4 units. This is the fundamental ideal of the labeling technique. Starting
with the source, the method attempts to label nodes along arcs. The first part of a
node’s label indicates the neighboring node it was labeled from, while the second
222 6 Network Modelspart of the label indicates the possible flow change on the arc along which the
labeling took place.
The objective value—the second part of the label—is determined by the objective
value achieved so far plus whatever contribution to the objective function has to be
accounted for in this step. (This is a standard procedure used in what is called
dynamic programming.) More specifically, labeling a node nj from a node ni along
an arc aij means that the incremental flow that can be squeezed from the source to nj
equals the minimum of what incremental flow can be sent from the source to ni plus
whatever slack capacity we have along the arc aij.
Once all nodes that can be labeled have been labeled, the iteration comes to an
end in one of two states. We either have been able to label the sink, in which case a
breakthrough has occurred, or we were not able to label the sink, which is referred to
as a nonbreakthrough. If a breakthrough has occurred, we are able to increase the
flow through a network, while in case of a nonbreakthrough, the current flow is
maximal, and the procedure terminates.
The algorithm is best explained by means of a numerical.
Example Consider the network in Fig. 6.7, whose numbers next to the arcs indicate
the capacity of the arc.
Suppose that the source ns is now labeled with (ns, 1), indicating that we start at
the node ns, and, given the present zero flow through the network, we can change the
flow by any amount we choose. At this point we could label either n1 and/or n2.
Suppose we choose n1. Since we label the node n1 from ns, the arc (ns, n1) is a
forward arc and we attempt to increase its flow. The capacity of the arc is 5, while its
present flow is zero, meaning that we can increase the flow in this arc by 5 units. This
is indicated in the label of n1, which is then (ns, 5). Now the nodes ns and n1 are
labeled, and the process continues. At this point, we have a large number of choices:
we can now either label n2 from the source ns, or label n2, n3, or nt from n1. Any
technique that labels all nodes from one node before moving on is referred to as
Fig. 6.7 Network for maximal flow example
6.3 Network Flow Problems 223breadth-first-search, while a labeling strategy that attempts to move on as deeply
into the network as possible is called depth-first-search. Suppose that we choose to
label n2 from n1. Again, we are moving with the direction of the arrow, so that this is
another forward labeling step. The label of the node n2 will then be (n1, min {5,
2–0}) ¼ (n1, 2). The reason is that while we could increase the flow from the source
on some path (we do not have to know at this point which path) to n1 by 5 units, we
can only ship two more units from n1 to n2. At this point, n2 has been labeled and one
of the many choices to continue labeling is to label the sink nt from n2. The label of
the sink nt is then (n2, min {2, 3–0}) ¼ (n2, 2). Now the sink has been labeled, and
we have achieved a breakthrough.
Whenever a breakthrough occurs, we are able to increase the flow through the
network by at least one unit. To do so, we now have to retrieve the path on which the
flow change is possible. This is where the first part of the labels comes in. In a
backward recursion, we start at the sink nt. Its label indicates that its predecessor is
n2. Now the label of n2 shows that its predecessor was n1, whose predecessor, in turn,
was ns. This means that we have successfully retrieved the path ns—n1—n2—nt.
This is the path on which the flow will be increased by 2 units, which is the second
part of the label of the sink. The resulting flow pattern is shown in Fig. 6.8, where the
arcs have two values: its capacity and its present flow.
We now delete all labels except the label of the source and start anew. We can
again label n1 from the source ns, this time the label of n1 is (ns, min {1, 5–2}) ¼ (ns,
3). From n1, it is possible to label the sink, whose label is then (n1, min {3,
4–0}) ¼ (n1, 3). Again, we have obtained a breakthrough and the flow can be
increased by 3 units. The path on which this increase takes place is determined by
following the labels backward from the sinks, which results in ns—n1—nt. The
resulting flow pattern is shown in Fig. 6.9.
Again, resetting the labels, the process begins again. At this point, it is no longer
possible to label the node n1 from the source, as the flow has reached the capacity.
However, we can still label the node n2 with (ns, min {1, 8–0}) ¼ (ns, 8). From n2,
we can label the sink nt with the label (n2, min {8, 3–2}) ¼ (n2, 1), and we have
Fig. 6.8 First flow pattern
224 6 Network Modelsachieved another breakthrough, on which the flow can be changed by one unit. The
path can be retrieved as ns—n2—nt. The flow pattern is then shown in Fig. 6.10.
The next iteration again starts by labeling the node n2 from the sink, which is
again our only choice. The label of the node n2 is now (ns, min {1, 8–1}) ¼ (ns, 7).
We can now label n3 from n2, so that the label on node n3 is (n2, min {7, 3–0}) ¼ (n2,
3). From n3, we can then label the sink with (n3, min {3, 5–0}) ¼ (n3, 3), and another
breakthrough has occurred. The path on which the flow is changed can be retrieved
as ns—n2—n3—nt, and on all arcs along that path the flow is increased by 3 units.
The resulting flow pattern is then shown in Fig. 6.11.
Notice that so far, we have only used the forward labeling in the incremental
method. The next iteration commences by resetting the labels and restarting the
process. Given that the source is labeled as usual with (ns, 1), the only choice is now
to label the node n2 with (ns, min{1, 8–4}) ¼ (ns, 4). From the node n2, no forward
labeling is possible. However, we can follow the arc from n1 to n2 against the
direction of the arc in a backward labeling step. This results in node n1 receiving
the label (n2, min {4, 2}) ¼ (n2, 2). From node n1, we can then either label nt or n3.
We arbitrarily choose n3, label it with (n1, min {2, 2–0}) ¼ (n1, 2), and from n3, we
Fig. 6.9 Second flow pattern
Fig. 6.10 Third flow pattern
6.3 Network Flow Problems 225can then label the sink nt with (n3, min {2, 5–3}) ¼ (n3, 2). Another breakthrough has
occurred, and the flow through the network can be increased by 2 units. The flow
along which the flow will be changed is retrieved through backward recursion as
ns—n2—n1—n3—nt, where the arc from n1 to n2 is used in reverse direction. The
new flow pattern is determined by increasing the flow in all forward arcs on that path,
while decreasing the flow in the solitary backward arc on the path. The resulting flow
pattern is shown in Fig. 6.12.
In the next step, we start again by labeling the node n2 from the source, which
receives the label (ns, min {1, 8–6}) ¼ (ns, 2). At this point, further progress is
blocked. The only unlabeled node adjacent to the source is n1, and the arc (ns, n1) is
filled to capacity. From the node n2, further (forward) flow to n3 and nt cannot be
sent, as both arc flows are at capacity. Also, labeling node n1 from n2 is not possible,
as the arc flow on that (backward) arc is already at the lower bound of zero. At this
point we have labeled all nodes that can be labeled, and we were not able to label the
sink. This is a nonbreakthrough. This indicates that the present flow pattern is indeed
a maximal flow with a total flow of 11 units, the number of flow units that leave the
source and, since no flow is lost along the way, the number of units that arrive at the
Fig. 6.11 Fourth flow pattern
Fig. 6.12 Fifth and optimal flow pattern
226 6 Network Modelssink. We would like to point out that this maximal flow pattern is not unique: sending
one less unit from n1 to n3, and on to nt and shipping it instead directly from n1 to nt
results in a different flow pattern with the same flow value.
Assessing the situation, we find that we now have the set Ns ¼ {ns, n2} of labeled
nodes, and its complement Nt ¼ {n1, n3, nt} of unlabeled nodes. Observe that the
flows of all arcs that lead from a node in Ns to a node in Nt (here: the arcs (ns, n1), (n2,
n3), and (n2, nt)) are at capacity, while the flows of all arcs leading from a node in Nt
to a node in Ns (here: the arc (n1, n2)) are at the zero level. All arcs that lead from a
node in Ns to a node in Nt are said to be included in the minimal cut C. In our
example, C ¼ {(ns, n1), (n2, n3), (n2, nt)}. Adding the capacities (not flows) of all arcs
in the minimal cut results in the value (or capacity) of the minimal cut, which, in or
example, equals 5 + 3 + 3 ¼ 11. This leads to the famous
Theorem (Ford and Fulkerson) The value of a maximal flow equals the value/
capacity of a minimal cut.
The minimal cut constitutes a bottleneck in the network. If we want to increase
the capacities of some arcs in the network so as to be able to increase the flow
through the network, we have to increase the capacities of arcs that are in the
minimal cut(s). Note that the minimal cut is not necessarily unique. As an example,
consider the network in Fig. 6.13 with capacities next to the arcs. The broken lines
refer to the cuts C1 ¼ {(ns, n1), (ns, n2)}, C2 ¼ {(ns, n1), (n2, nt)}, and C3 ¼ {(n1, nt),
(n2, nt)}. All cuts have a capacity equal to 5. Note that in case of multiple cuts, the
method described above will find only the minimal cut that is closest to the source.
A variety of extensions of the maximal flow problem exists. One of the most
popular generalizations is the min-cost feasible flow problem. Again, the idea is very
simple. In a network with a designated source and sink, each arc aij has a lower
bound λij and an upper bound κij on the flow. (Recall that in the above max flow
problem all lower bounds were assumed to be zero.) In addition, it is assumed to cost
Fig. 6.13 Cuts in a network
6.3 Network Flow Problems 227cij dollars to send one unit of flow on the arc aij. The problem is now to find a flow
from source to sink that respects all lower and upper bounds on the flows and that
minimizes the total shipping costs.
As an example, consider the graph in Fig. 6.14, where the numbers next to the
arcs symbolize the lower bounds λij, the upper bounds κij, and the unit costs cij,
respectively.
The cost-minimizing flow problem has three types of constraints: the usual
conservation equations, and the lower and upper bounds on each arc flow, respec￾tively. This specific problem can be formulated as follows:
P : Min z ¼ 3xs1 þ 6xs2 þ 2x12 þ 2x13 þ 5x1t þ 6x24 þ 7x2t þ 1x32 þ 3x34 þ 6x4t
s:t: xs1  x12  x13  x1t ¼ 0
xs2 þ x12 þ x32  x24  x2t ¼ 0
x13  x32  x34 ¼ 0
x34 þ x24  x4t ¼ 0
xs1  3 x13  4
x32  1 x32  5
x1t  1 x1t  4
x2t  2 x34  5
x4t  1 x24  7
xs1  7 x2t  6
xs2  5 x4t  8
x12  2
xij  0 for all arcs i, j:
Fig. 6.14 Network for min-cost feasible flow problem
228 6 Network ModelsThe solution can be obtained by any standard optimization package or specialized
algorithm. The optimal flow pattern is shown in Fig. 6.15, the total flow from source
to sink is 4, and the associated minimal total transportation costs are 47.
Further extensions are possible. One such extension has a required flow value f .
A typical network modification that accomplishes this requirement will simply add a
return arc (nt, ns) with zero costs and lower and upper bound equal to f. The original
flow problem can then be reformulated as a circulation, in which the conservation
equations have to hold for all nodes, including the source and the sink. Working with
the mathematical formulation, all we have to do is add a single constraint, which
requires that the flow value equals f. If f ¼ 7 in the above example, we either add the
constraint xs1 + xs2 ¼ 7, or, alternatively, x1t + x2t + x4t ¼ 7 and resolve. (In the above
numerical example, the new solution will be the same as that shown in Fig. 6.15,
except that an additional 3 flow units are shipped from ns to n1 and on to nt.)
It is now also possible to demonstrate how to cast the standard transportation
problem (see Sect. 2.2.5) into the mold of cost-minimal network flow problems. In
terms of the network, we will make the following modifications. In addition to the
existing origins and destinations, we first create an artificial source ns and artificial
sink nt. We then connect the source with all origins and connect all destinations with
the sink. (This type of problem reformulation can be used for all problems with
multiple sources and sinks, provided that the goods are homogeneous and do not
need to be distinguished.) The lower bounds of all (source, origin) connections are
zero, while the upper bounds equal the supplies available at the respective origin.
Similarly, all (destination, sink) arcs have a zero lower bound and an upper bound
that equals the demand at the respective destination. Both types of arcs have zero
costs. The existing arcs that connect the origins and the destinations have zero lower
bounds and arbitrarily large upper bounds (except in cases, in which capacities need
to be considered), and carry the costs specified for the original problem. Note that so
far, the zero flow would be optimal, as none of the arcs requires a flow greater than
zero, and any flow from source to sink costs money. In order to force flow through
the network, we connect the sink with the source by means of an artificial arc (nt, ns)
that has zero costs, and an upper and lower bound that are both equal to the minimum
Fig. 6.15 Optimal flow pattern
6.3 Network Flow Problems 229of the total supply and the total demand at the sources and destinations, respectively.
This way, the (sink, source) arc forces as many flow units through the network as are
in demand or are needed, whatever is less.
Another possible extension includes capacity constraints at the nodes. Two
approaches are possible. The first uses the given network and modifies it so that it
includes node capacities. This can be accomplished by “splitting” all of the nodes
with node capacities. In particular, a node ni with node capacity κi is then replaced by
an “in-node” n0
i into which all arcs lead that led into the original node ni, an “out￾node” n00
i , out of which all arcs lead, that lead out of the original node ni. Finally, the
two new nodes n0
i and n00
i are connected by an arc (n0
i
, n00
i ), whose arc capacity is the
original node capacity κi. What we have done in this approach is simply to replace
the node capacity by an arc capacity.
An alternative approach simply uses a mathematical programming formulation.
The number of flow units that flow through a node equals the number of units that
enter (and, as nothing is lost, leave) a node, and an appropriate additional constraint
is added. In the min-cost flow example of Fig. 6.14 a capacity of, say, 5 units through
node n2 can be written as xs2 + x12 + x32  5, or, equivalently, by using the outflow,
as x24 + x2t  5.
6.4 Shortest Path Problems
Similar to the maximal flow problem discussed in the previous section, shortest path
problems are easily described. Given a network with a prespecified source and sink
node as well as arc values cij that denote the cost (or distance, fuel, or any other
disutility) of traveling from node ni directly to node nj along arc aij, the task is to find
the shortest path from the source to the sink. The literature typically distinguishes
between one-to-one shortest path problems (those that search the shortest path
between source and sink), one-to-all shortest path problems (in which the task is
to find the shortest paths between the source and all other nodes in the network), and
the all-to-all shortest path problems (where the task is to determine the shortest paths
between all pairs of nodes). Clearly, it would be possible—albeit inefficient—to use
an algorithm for the one-to-one shortest path problem and apply it repeatedly so as to
solve the one-to-all and all-to-all shortest path problems.
This section first describes a way to reformulate the one-to-one shortest path
problem, so that it fits into the mold of the cost-min feasible flow problem. It then
describes the workings of an all-to-all shortest path algorithm that is not only very
efficient, but also needed in areas such as location models, where all shortest paths
have to be known before any location algorithm can even start.
First, we will discuss how to reformulate a shortest path problem as a cost￾minimizing flow problem. The idea is to force one flow unit through the network,
and let the optimizer find the cost-minimal, i.e., shortest, path. This is done by having
lower bounds of zero and upper bounds of one for all arcs, coupled with the actual
costs or distances specified for all arcs. So far, the optimal solution would be the zero
230 6 Network Modelsflow, as it is feasible, and no cheaper solution can exist (at least not as long as the arc
distances are nonnegative). We then add the circulatory arc (nt, ns) with lower and
upper bound equal to one. This forces a single unit through the network and will
result in the desired solution. As an example, the graph transformation is shown in
Fig. 6.16, where Fig. 6.16a shows the original graph with the distances or costs next
to the arcs, while Fig. 6.16b has the lower bounds, the upper bounds, and the cost/
distances at the arcs.
As far as the mathematical formulation is concerned, we can use the usual
min-cost objective function, coupled with two sets of constraints. The first single
constraint ensures that exactly one unit leaves the source. The second set of
constraints contains the usual conservation equations, which ensure that the flow
unit that leaves the source has only one place to go: the sink. In the example of
Fig. 6.16, the formulation would be as follows:
P : Min z ¼ 3xs1 þ 7xs2 þ 5x13 þ 2x12 þ 2x23 þ 1x3t þ 4x2t
s:t: xs1 þ xs2 ¼ 1
xs1  x12  x13 ¼ 0
xs2 þ x12  x23  x2t ¼ 0
x13 þ x23  x3t ¼ 0
xij  0 for all i, j:
Alternatively, the first constraint xs1 + xs2 ¼ 1 could be replaced by the constraint
x2t + x3t ¼ 1, representing the flow into the sink.
Shortest path problems have many real-world applications. In addition to the
obvious applications, in which the shortest path in a road network is to be found
(e.g., for GPS-based navigation systems), shortest path problems occur in scenarios
that seemingly have nothing to do with shortest paths. As an example, assume that a
process has been “discretized,” i.e. is subdivided into a finite number of states that
describe the system at that point in time. Each node of the network symbolizes a state
of the system, and an arc indicates a possible transition from one state to another. The
Fig. 6.16 Transformation of a shortest path problem into a min-cost flow problem. (a) Original
graph, (b) Transformed graph
6.4 Shortest Path Problems 231arc values show the amount of resources, such as time, money, or fuel, that a
transition takes.
A good example of such a problem deals with the problem of getting an aircraft
from a standstill position to a certain speed and altitude in the fasted possible way.
This problem is particularly relevant for fighter aircraft.
As a numerical example, consider the situation shown in Fig. 6.17. The two
numbers in each node indicate the state the aircraft is in: the first component is the
ground speed in 100 mph, and the second number shows the aircraft’s altitude in
1000 ft. Initially, the aircraft is at (0, 0), i.e. standing still on the ground. Each node
describes a state the aircraft can be in with respect to speed and altitude, and the arcs
between these states indicate the possible transitions from one state to another.
The values next to the arcs show the time (in seconds) that is required to make the
transition from one node to another. For example, it takes five seconds to bring the
aircraft from the standstill position at (0, 0) to (1, 0), i.e. a speed of 100 mph at zero
altitude (meaning on the runway). Suppose that it is desired to bring the aircraft from
a standstill position to a speed of 500 mph and an altitude of 4000 ft. as quickly as
possible.
At each state in this example, the pilot has three options: either stay at the same
altitude and speed up, remain at the same speed and climb, or speed up and climb
simultaneously. The shortest path in the above example includes the nodes (0, 0),
(1, 0), (2, 0), (2, 1), (2, 2), (3, 2), (4, 3), and (5, 4). In other words, the instructions to
the pilot would state to bring the aircraft from a standstill position to a speed of
200 mph, then remain at that speed and climb 2000 ft., then stay at that altitude and
speed up to 300 mph, and then accelerate and climb simultaneously to the desired
speed of 500 mph and altitude of 4000 ft. This way, it will take 39 sec, the length of
the shortest path, to reach the desired state.
Fig. 6.17 Graph for fighter aircraft problem
232 6 Network ModelsOne of the most popular special-purpose techniques for the determination of
shortest paths from one node to all other nodes is Dijkstra’s technique that was first
published in the late 1950s. It is a highly efficient method that belongs to the class of
the so-called label setting techniques (as opposed to other label-correcting
techniques). The main idea is to label the nodes ns, n1, ..., with labels L(ns),
L(n1), ..., each consisting of two parts. The first is the immediate predecessor of
the node on the shortest path known so far, and the second part is the length of the
shortest path known so far. Throughout the procedure, we distinguish between nodes
that have a temporary label and those with a permanent label. The label of a
permanently labeled node indicates the actual length of the shortest path from the
source to this node as well as the immediate predecessor on that path, while a
temporary label comprises the length of the presently shortest known path and the
node’s immediate predecessor on it. In each step of the algorithm, one node with a
temporary label is chosen and its label made permanent. The upper bounds on the
estimates of the shortest paths to all direct successors of this node are revised, and the
process is repeated until the labels of all nodes are made permanent. It is important to
realize that Dijkstra’s method is only applicable to networks with nonnegative arc
lengths.
To initialize the method, assume that the source ns is labeled L(ns) ¼ (ns, 0), while
all other nodes nj are labeled L(nj) ¼ (nj, 1). In the beginning, all nodes are assumed
to be temporary. The method now proceeds as follows. We choose the temporarily
labeled node whose (second part of the) label is minimal among all temporarily
labeled nodes. Ties are broken arbitrarily. The label of this node is then made
permanent. Suppose this node is ni. All of this node’s direct successors are then
investigated by comparing their second part of the label with the label of ni plus the
arc length of the arc aij. If the present label of nj is smaller, we leave it unchanged; if
it is larger, we replace it by setting L(nj) ¼ (ni, L(ni) + cij), i.e., by the label of ni plus
the length of the arc that connects ni and nj.
As an example of the procedure, consider the network in Fig. 6.18.
As indicated above, in the initialization step (“Step 0”), we label the source as
L(ns) ¼ (ns, 0) and all other nodes nj with L(nj) ¼ (nj, 1), and let all node labels be
Fig. 6.18 Example for Dijkstra’s technique
6.4 Shortest Path Problems 233temporary. The node with the lowest temporary label is the sink, so that it is chosen,
and its label is made permanent. All computations discussed here are summarized in
Table 5.1, where the first time a label has been made permanent, it receives a “*” and
due to its permanent status, it is not listed again below. Choosing ns to receive a
permanent label means that the labels of its direct successors n1, n2, and n3 may have
to be revised (the labels of all other nodes remain unchanged). Their present labels
are compared with the label from ns, which is the source’s label (here: 0) plus the
length of the arc from ns to the node in question. For n1, the comparison is between
1 and 0 + 5, which is 5, so that n1 is now labeled from ns, which is indicated in its
new temporary label L(n1) ¼ (ns, 5). Similarly, the labels of the nodes n2 and n3 are
(ns, 4) and (ns, 2), respectively.
We are now at the end of Step 1 in Table 6.1, where we choose the node with the
lowest temporary label. In this example, the node is n3, as it has the smallest label
with “2.” We now make this label permanent, indicate this by a star, and revise the
labels of its successors n1, n2, and n5 in Step 2. The present label of the node n1
indicates that a path of length 5 is already known from the source. If we were to label
n1 from n3, its label would be 2 + 2 ¼ 4, which is shorter, so that the new label of
L(n1) ¼ (n3, 4). For n2, we find that the presently shortest known path is of length
4 (its present label), while labeling the node from n3 would lead us to a path of length
of 2 + 6 ¼ 8. Since this new path is longer, we ignore it and leave the label of n2
unchanged. Finally in this step, the present label of n5 indicates a path of length 1 is
known, which is compared to the label the node would receive if labeled from n3,
which is 2 + 8 ¼ 10. This new label is shorter, so that node n5 receives the new label
L(n5) ¼ (n3, 10).
This process continues until all nodes have been permanently labeled. The results
are shown in Table 6.1.
The results in Table 6.1 can now be used to determine the tree of shortest paths
rooted at ns. This is done by choosing all permanent labels and connecting the node
with its direct predecessor as specified in the label. In our example, the node nt has n5
as its direct predecessor, so we introduce the arc a5t. The node n5 has n4 in its label,
so we introduce the arc a45, and so forth. The resulting arborescence (a directed tree)
is shown in Fig. 6.19.
The numbers next to the nodes are the lengths of the shortest paths from the
source ns to all nodes in the network.
Table 6.1 Permanent and temporary labels during the Dijkstra method
Step # L(ns) L(n1) L(n2) L(n3) L(n4) L(n5) L(nt)
0 (ns, 0)* (n1, 1) (n2, 1) (n3, 1) (n4, 1) (n5, 1) (nt, 1)
1 (ns, 5) (ns, 4) (ns, 2)* (n4, 1) (n5, 1) (nt, 1)
2 (n3, 4)* (ns, 4) (n4, 1) (n3, 10) (nt, 1)
3 (ns, 4)* (n1, 9) (n3, 10) (nt, 1)
4 (n2, 5)* (n3, 10) (nt, 1)
5 (n4, 7)* (n4, 9)
6 (n5, 8)*
234 6 Network ModelsSometimes (as, for instance, in location models), it is required to determine the
paths between all pairs of nodes. Clearly, Dijkstra’s technique could be used by
considering one node as a source, determine the arborescence of all shortest paths
rooted at that node, and then repeat the process with all nodes as roots. This is
somewhat tedious, and there is a very efficient technique, called the Floyd–Warshall
method, that performs this task directly. All it requires are some matrix operations.
The method starts and works with the direct distance matrix C0
, which includes all
node-to-node distances of the original problem. An iterative step in iteration k can
then be described as follows. First define row k and column k as the key row and
column. We then compare the shortest presently known distance between node ni
and node nj with a detour that uses node nk. The shorter of the two distances is then
used as the new shortest known distance. This process is repeated for all pairs of
nodes. Formally, creating matrix Ck + 1 from matrix Ck
, we use the relation
ckþ1
ij ¼ min ck
ij, ck
ik þ ck
kj n o:
Starting with the original matrix C0
, we compute the matrices C1
, C2
, ..., Cn.
The matrix Cn (given that the graph has n nodes) is then the matrix of shortest paths.
We will illustrate this procedure by means of an example. The graph and its
distances of the example are shown in Fig. 6.20.
The direct distance matrix of the mixed graph in the example is
Fig. 6.19 Arborescence for shortest paths
6.4 Shortest Path Problems 235Note that in case of undirected graphs, the direct distance matrix is symmetric,
which is not the case here. In the first iteration k ¼ 1, we will consider the node n1 as
a possible detour node. This is done by using the first row and column as the key row
and column. This row and column is copied without a change into the next matrix.
For all other elements, we compare the present distance with the one that uses n1 as a
detour node. As an example, the lowest known distance from n2 to n3 that is
presently known is c23 ¼ 3. This is compared with the detour that uses node
1, i.e., the path that uses the arcs (n2, n1) and (n1, n3), whose lengths are 2 and
7, respectively. In other words, we now compare 3 with 2 + 7 ¼ 9. Since the original
length is shorter, we keep it and continue with the next pair of nodes. Next, we
compare the distances from n2 to n4, without and with detour via n1. We find that
c24 ¼ 7 < 2+9 ¼ c21 + c14, so that again, we make no change. We continue this way
and find no changes, until we reach the connection from n3 to n4. At present,
c34 ¼ 1, i.e., there is no direct connection. We compare this distance with the
detour that uses n1, i.e. the arcs (n3, n1) and (n1, n4). Those distances are 7 and 9, so
that we are now able to reach n4 from n3 on a path of length 7 + 9 ¼ 16. This turns out
to be the only change from matrix C0 to matrix C1
, which is indicated in C1 by an
asterisk.
Fig. 6.20 Example for the
Floyd–Warshall method
236 6 Network ModelsStarting now with C1
, the process is repeated by using the second row and column
as key row (column) and comparing all known distances in C1 with the detour that
uses n2 as detour node. Again, we copy the key row and column without any changes
to C2
. Here are some of the computations. For the connection from n1 to n3, we
presently have a length of 7, which we compare with the detour via node n2, which
has a length of 2 + 3 ¼ 5, so that the new distance is shorter. Similarly, the distance
from n1 to n4 without the detour via n2 is 9, with the detour it is 2 + 7 ¼ 9, a tie. The
distance from n1 to n5 without the detour is 1, with the detour it is 2 + 4 ¼ 6. The
results are shown in the matrix C2
, again with all changes indicated by an asterisk.
The remaining iterations are shown without further comment.
6.4 Shortest Path Problems 237The matrix C6 now includes the lengths of the shortest paths between all pairs of
nodes. We should note that it is also possible to construct a second set of matrices
parallel to the computations made above, so as to keep track not only of the lengths
of the shortest paths, but also the paths themselves. This is, however, beyond the
scope of this volume. Interested readers are referred to books such as Eiselt and
Sandblom (2000).
6.5 Spanning Tree Problems
Similar to the models in the previous sections in this chapter, the problem behind
spanning trees is easily explained. Suppose there is an undirected graph that includes
the potential edges that connect the nodes of the graph. The values associated with
the edges indicate their costs. The problem is now to choose some of the edges and
include them in the solution, so that the total costs are minimized, while the network
remains connected. This may result in a graph, in which the path from one node to
another leads through many intermediate nodes.
It is apparent that what we are looking for a tree, as any graph with fewer edges
than a tree has will no longer be connected, while any graph that has more edges than
a tree will include at least one cycle, which is unnecessary.
Problems of this nature occur whenever it is very expensive to establish edges.
Typical examples are road networks, networks of power lines, or networks of sewer
tunnels. The example shown in Fig. 6.21 shows all possible connections that may be
established, coupled with their respective costs.
The task is now to find a subset of existing edges (the edges that belong to
connections that will actually be built), so as to minimize the total amount of money
necessary to connect all nodes with each other. The resulting connected subgraph of
238 6 Network Modelsthe original graph with n nodes is a minimal spanning tree of the given graph. Given
that the result will be a tree, one can show that the optimal solution will include
exactly (n  1) edges.
A variety of solution methods exist for this type of problem. Here, we describe the
Kruskal technique, which was first published in 1956. Actually, the method is
nothing but a Greedy technique (see, e.g., Sect. 5.3.3 and Chap. 17). However,
while the Greedy heuristic finds only approximate solutions in general, it is
guaranteed to find an optimal solution for this problem. It can be described as
follows. We first sort the edges in order of nondecreasing arc values, where ties
are broken arbitrarily. Starting with the edge that has the lowest arc value, we
introduce one arc at a time, provided it does not form a cycle with the already
existing edges. The procedure continues until (n  1) arcs are included in the
solution.
In the example of Fig. 6.21, we order the edges, which results in Table 6.2. The
table shows the edges and their costs in nondecreasing order.
Table 6.2 indicates which edges are introduced and which are not. In order to
visualize the process, consider Fig. 6.22. Note that after edge a13,14 is introduced, we
have introduced 14 edges, and the process terminates.
6.6 Routing Problems
Routing problems are among the most frequently used network models in practice.
We distinguish between two classes of routing models: arc routing and node routing
models. The first example of arc routing was provided by Euler and his “Königsberg
Fig. 6.21 Example for the minimal spanning tree problem
6.6 Routing Problems 239Bridge Problem” described at the beginning of this chapter. The idea common to all
arc routing problems is to find a path in a given graph, so that each arc is used on the
path exactly or at least once. Similarly, in node routing problems, the idea is to find a
path that starts at some node and returns to it, while using each node exactly or at
least once in the process. Combinations of the two classes are vehicle routing
problems, which belong to the most difficult routing problems.
The best-known arc routing problem is the Chinese Postman Problem. The name
is due to Meigu Guan, who in the course of the “cultural” revolution in China was
assigned to the position of a postal worker in the early 1960s. There, he considered
the problem of a letter carrier, who would pick up the mail at some point (a node in
the street network) and deliver it to the individual households by walking along each
street at least once. The objective of the model is to minimize total distance walked,
and the constraints ensure that mail is delivered to the houses on all streets of the
network. While some versions of the model are easy to solve, others remain difficult.
There are many important and popular applications of Chinese Postman Problems,
including street cleaning and snow removal. Clearly, those problems are more
Table 6.2 Edges of Fig. 6.21 in order of their value
Arc a48 a6,12 a23 a78 a8,14 a12 a34 a6,11 a9,14 a24
Cost 1 1 2 2 2 3 3 3 3 4
Inserted? Yes Yes Yes Yes Yes Yes Yes Yes Yes No
Arc a67 a38 a49 a5,14 a11,12 a26 a7,12 a8,12 a9,15 a16
Cost 4 5 5 5 5 6 6 6 6 7
Inserted? Yes No No Yes No No No No Yes No
Arc a10,15 a27 a13,14
Cost 7 8 8
Inserted? Yes No Yes
Fig. 6.22 Minimal spanning tree
240 6 Network Modelsdifficult, as they will include hierarchies of streets, e.g. highways are typically
plowed after a snowstorm before small neighborhood streets.
Similar to arc routing, the field of node routing has a long history. It starts with
Hamilton’s “trip around the world” developed in 1856, a game in which players have
to find a tour that visits all desirable places (the nodes in a graph) exactly once. The
best-known version of node routing is the famed traveling salesman problem, surely
one of the most popular models in all of operations research. The story (not a real
application, but a scenario that gave the model its name) is that a traveling salesman
attempts to sell his goods in a number of cities in his region. He must visit each city
exactly once and return to the place he started from. In order to have as much time as
possible with the customers, the objective is to minimize the total travel time
(or distance) of the tour. Applications of traveling salesman problems abound,
many of them seemingly unrelated. One such example is the drilling of holes into
sheet metal with the use of an automated drill press. Drilling the hole takes the same
amount of time regardless of the sequence, in which the holes are drilled, so that the
objective is to minimize the amount of time it takes to move the metal into the
position, in which the next hole is to be drilled. This is nothing but a traveling
salesman problem.
6.6.1 Arc Routing Problems
This section will deal with existence conditions for Euler tours, an extension of the
Chinese Postman Problem, in directed and undirected graphs. We continue describ￾ing a technique that allows us to find such a tour, given that it exists. Finally, should
an Euler tour not exist, we demonstrate how the Chinese Postman Problem can be
solved, given that the underlying graph is directed or undirected (but not mixed).
In order to formalize our discussion, we first need to define some graph
properties. For simplicity, we will consider only undirected and directed graphs in
this section; we leave the discussion of mixed graphs to the advanced literature. We
first need to define some properties of graph in addition to those already discussed at
the beginning of this chapter. For undirected graphs, recall that we defined the
degree of a node as the number of edges incident to it. For instance, node n1 in
Fig. 6.2 in Sect. 6.1 has a degree of 3, node n2 has a degree of 2, and each of the
nodes n3, n4, and n5 has a degree of 1. In the case of directed graphs, we define the
indegree of a node as the number of arcs leading into that node, while the outdegree
of a node measures the number of arcs leading out of that node. We then call a graph
Eulerian or an Euler graph, if it has a circuit that uses each arc or edge exactly once.
A circuit with this property is called an Euler tour and sometimes referred to as
“unicursal.”
At this point, there are three issues that have to be resolved. Firstly, we need a tool
to determine whether or not any given graph is Eulerian. Secondly, if this is the case,
then we need an algorithm that actually determines an Euler tour. Thirdly, and
finally, in case a graph is not Eulerian, we will have to find a circuit that uses all
edges at least once, so that the total length of the tour is minimized. This is the
6.6 Routing Problems 241Chinese Postman Problem. The remainder of this section will discuss these three
issues for undirected and directed graphs.
First, consider undirected graphs. There is a very simply existence condition that
indicates whether or not any given graph is Eulerian. It states that a graph is Eulerian
if and only if it is connected and all of its nodes have even degrees. We will justify
this condition later after describing a method that allows us to find an Euler circuit in
case one exists.
First, we need to define a bridge as an edge, whose removal from the graph will
disconnect the network into two connected components, each containing at least one
edge. Next, we remove this edge from the graph and repeat the step. In order to
construct an Euler circuit, we start at any arbitrary node. This method was first
described in the late 1880s and is sometimes called Fleury’s algorithm in honor of its
inventor.
In order to explain the procedure, consider the following:
Example Consider the graph in Fig. 6.23.
It is readily apparent that the graph in the figure is Eulerian as all degrees of its
nodes are even (the nodes n1, n3, and n6 have a degree of 2, while the nodes n2, n4,
and n5 have degrees of 4). Suppose that we start our tour at node n1. From here, we
can either move to n2 or n4, arbitrarily choose n4. Once that move is made, we can
remove the edge e14 from the graph, as we have used it and we cannot use it again on
our tour. From n4, we can now either move to n2, n5, or n6, as none of these
connections represents a bridge. Arbitrarily choose n6, from where we can only
move to n5. Our choices at this point are e52, e53, and e54, and since none of them
represents a bridge, we arbitrarily choose e52 and move to n2. Given that we have
Fig. 6.23 Example for
Fleury’s algorithm
242 6 Network Modelsremoved all edges that were used on the tour so far, we obtain the graph shown in
Fig. 6.24.
The node n2 has three connections left, but e21 represents a bridge—meaning that
if we were to cross it, we would be stuck at n1, unable to return and move along the
remaining edges. We thus take either of the other two edges, say, e23. From n3 we
continue on to n5, on to n4, n2, and we then finally return to n1 and complete the
circuit.
We can use this process to explain the existence condition for Eulerian circuits
that we stated earlier. When we move out of the starting node and remove the first
edge after having used it, the degree of the starting node decreases by one. From that
point on, we enter all nodes on the tour once and leave it again (maybe repeating it
multiple times as in the above example for nodes n4, n5, and n2), so that its degree
decreases by 2 each time we enter and leave a node. This means that passing through
a node and removing the edges by which we enter and leave the node decreases its
degree by an even number. Finally, when we complete the tour and enter the starting
node for the last time, we remove the edge by which we enter it, thus reducing its
degree by one. In summary, the degrees of all nodes have now been reduced to zero
by an even number. This justifies the existence condition. (By the way, in case we do
not require the tour to begin and end at the same node, the existence condition for a
tour is easily modified—all we have to do is require that the graph is connected and
that all but two of its nodes have even degrees. The nodes with odd degrees will then
be the starting and the ending points of the tour.)
Consider now directed graphs. Again, we first state the condition that guarantees
the existence of a Eulerian tour in the graph. For such a tour to exist, it is required
Fig. 6.24 Example with
edges removed
6.6 Routing Problems 243that the graph is strongly connected and that the indegree and outdegree are equal for
each node in the network. As an example, consider the graph in Fig. 6.25.
At node n6 the in- and outdegrees equal 1, at nodes n1, n2, and n3, in- and
outdegrees equal 2, and at nodes n4 and n5, in- and outdegrees equal 3. Furthermore,
between each pair of nodes there is a path in either direction, so that the graph is
strongly connected. This means that an Euler circuit exists in this graph. We can
determine it again with Fleury’s algorithm, minding the rule never to enter a
subgraph that we cannot leave again. One such tour, starting at, say, n4, will be
(n4, n5, n2, n4, n5, n4, n2, n1) (at this point we cannot move to n6, as this would lead us
to n4 without having covered all edges), so that we continue to (n3, n5, n3, n1, n6, n4),
which completes the tour.
So far, we have discussed the first two issues related to arc routing—demonstrate
the existence of Eulerian tours in undirected and directed graphs (or the lack thereof),
and, if such tours exist, how to actually find them. Assume now that a Eulerian tour
does not exist in the given graph. This will require that we use some of the edges or
arcs more than once. Given that each edge or arc has now a nonnegative length
associated with it, our task is now to add copies of the arcs or edges to the graph, so
that the augmented graph is Eulerian and an Euler tour can be found. This task is to
be accomplished so as to minimize the total length of the tour (which is identical to
stating that we minimize the sum of the arc or edge values of all arcs and edges that
have been added to the graph).
In order to explain the formulation of the problem, consider the following
numerical example shown in Fig. 6.26, in which the numbers next to the edges
indicate their lengths.
Fig. 6.25 Directed graph
with an Euler circuit
244 6 Network ModelsClearly, the graph in Fig. 6.26 is not Eulerian, as the degrees of nodes n1, n3, n5,
and n6 are odd. The degrees of these nodes must each be increased by one or any
other odd number, so as to make the graph Eulerian. However, since we are trying to
find a cost-minimal expansion of the given graph, we only need to increase the
degree by one each. This is done by adding edges of the shortest paths that connect
nodes with odd degree. In order to formulate the problem, we need to define zero-one
variables xij, which assume a value of one, if a new connection between nodes ni and
nj is introduced, and zero otherwise. In order to determine the distance between the
pairs of nodes with odd degrees, we need to find the shortest paths between all of
them. For this purpose, we can use any of the techniques described in Sect. 6.3.
The objective function is then to minimize the sum of distances of all connections
introduced in addition to the existing edges (which must be traversed anyway). The
constraints require that the number of edges that are added to the original graph to
make up the augmented graph equals one for each node with an odd degree.
To solve the numerical problem in our example, we first determine the shortest
paths between all pairs of nodes with odd degrees. Here, the distance matrix for the
nodes n1, n3, n5, and n6 is
C ¼
0464
4057
6506
4760
2
6
6
6
4
3
7
7
7
5:
Defining variables xij, which assume a value of one, if a copy of the shortest path
between ni and nj is to be added to the original non-Eulerian graph, and zero
otherwise, we can then formulate our numerical example as
Fig. 6.26 Graph with no
Eulerian tour
6.6 Routing Problems 245P : Min z ¼ 4x13 þ 6x15 þ 4x16 þ 5x35 þ 7x36 þ 6x56
s:t: x13 þ x15 þ x16 ¼ 1
x13 þ x35 þ x36 ¼ 1
x15 þ x35 þ x56 ¼ 1
x16 þ x36 þ x56 ¼ 1
x13, x15, x16, x35, x36, x56 ¼ 0 or 1:
Note that the formulation of this type of problem is nothing but an assignment
problem (see Sect. 2.2.5) with
• the elements on the main diagonal being ineligible for assignments, and
• a symmetry condition xij ¼ xji for all i and j.
Solving the problem results in x16 ¼ x35 ¼ 1 and xij ¼ 0 otherwise. In other
words, the connection between n1 to n6 (i.e., the edges e14 and e46) as well as the
connection between n3 and n5 (the edge e35) has to be included in the solution. The
resulting augmented graph is shown in Fig. 6.27.
Note that now all nodes in the augmented graph have even degree, so that the
graph satisfies the Euler condition. Fleury’s algorithm can now be used to find an
Euler tour. Arbitrarily starting at n1, one such tour is (n1, n2, n4, n1, n4, n6, n5, n4, n6,
n3, n5, n3, n1), or, equivalently, (e12, e24, e41, e14, e46, e65, e54, e46, e63, e35, e53, e31).
Finally, consider the Chinese Postman Problem in directed graphs. In order to
explain the formulation, consider the following numerical
Example Consider the directed graph shown in Fig. 6.28, whose numbers next to
the arcs denote the time (or distance or cost) required moving along it.
Fig. 6.27 Augmented graph
for shortest path example
246 6 Network ModelsIn this graph, only at node n1 in- and outdegrees are equal. Node n2 has an
indegree of 2 and an outdegree of 3, node n3 has an indegree of 2 and an outdegree of
1, node n4 has an indegree of 1 and an outdegree of 2, node n5 has an indegree of
4 and an outdegree of 2, and node n6 has an indegree of 1 and an outdegree of 2. It is
evident that a Eulerian tour does not exist in this graph. In order to solve the Chinese
Postman Problem, we first define sets of nodes N and N+
, so that N includes all
nodes, whose outdegrees are higher than their indegrees, while N+ includes all nodes
whose indegrees are higher than their respective outdegrees. In this example, we
have N+ ¼ {n3, n5} and N ¼ {n2, n4, n6}. We now determine which connections
have to be added in order to make the graph Eulerian. This requires that we define
variables xij for all ni 2 N+ and nj 2 N, which indicate how many copies of the
connection from ni to nj must be added so as to make the graph Eulerian. For
instance, if x36 ¼ 2, this would indicate that we must introduce two additional copies
of the connection from n3 to n6 in the network. However, this does not mean that a
copy of the arc a36 should be introduced (which, in this case, would refer to an arc
that does not even exist). Instead, we need to determine the shortest paths from all
nodes in N+ to all nodes in N, which can be accomplished by any of the pertinent
algorithms discussed earlier in this chapter.
Associated with each variable xij is the cost that is incurred if we actually
introduce the connection from node ni to node nj. This cost is nothing but the length
of the shortest path from ni to nj. In this example, the shortest paths from the nodes in
N+
, i.e. n3 and n5, to the nodes in N, viz., n2, n4, and n6, are collected in the distance
matrix
C ¼ 538
427  :
The objective function will then minimize the total length of the additional arcs
that are added to the original graph, so that the resulting augmented graph is
Eulerian. The constraints ensure that the sum of additional arcs that lead out of
each node in N+ equals the difference between the indegree and the outdegree of that
Fig. 6.28 Graph for the Chinese Postman Problem
6.6 Routing Problems 247node. Similarly, for each node in N we require that the additional arcs leading into
that node equal the difference of its outdegree and its indegree. The formulation of
the problem in our example is then
P : Min z ¼ 5x32 þ 3x34 þ 8x36 þ 4x52 þ 2x54 þ 7x56
s:t: x32 þ x34 þ x36 ¼ 1
x52 þ x54 þ x56 ¼ 2
x32 þ x52 ¼ 1
x34 þ x54 ¼ 1
x36 þ x56 ¼ 1
x32, x34, x36, x52, x54, x56  0 and integer:
It is apparent that the formulation is nothing but a transportation problem, see
Sect. 2.2.5. Our problem has multiple optimal solutions. One such solution is
x36 ¼ x52 ¼ x54 ¼ 1 , and xij ¼ 0 otherwise. The value x36 ¼ 1 means that one
copy of all arcs on the shortest path from n3 to n6 is included in the augmented
graph, i.e. the arcs a35, a52, and a26. Similarly, the value x52 ¼ 1 indicates that all
edges on the shortest path from n5 to n2 are included (which is a52, the direct
connection), and x54 ¼ 1 means that all arcs on the shortest path from n5 to n4 are
included (which is only the single arc a54). The augmented graph with the additional
arcs shown as broken lines is shown in Fig. 6.29.
It is also worth pointing out that an alternative optimal solution has x32 ¼ x54 ¼
x56 ¼ 1, and xij ¼ 0 otherwise. Although the solution to the transportation problem
is completely different, it turns out that this solution adds the exact same arcs to the
original graph, so that again the augmented graph of Fig. 6.29 results.
In either case, the augmented graph is now Eulerian and a tour can be found with
Fleury’s algorithm. Arbitrarily starting at n1, one such tour is (n1, n2, n3, n5, n4, n3,
n5, n4, n5, n2, n6, n5, n2, n5, n2, n6, n1), or, equivalently, (a12, a23, a35, a54, a43, a35,
Fig. 6.29 Augmented graph for the Chinese Postman Problem
248 6 Network Modelsa54, a45, a52, a26, a65, a52, a25, a52, a26, a61). The total length of the tour is found by
adding up the lengths of all arcs, it is 52.
6.6.2 Node Routing Problems
In this section, we first formulate a traveling salesman problem. We then show ways
to deal with some of the difficult constraints. Finally, we describe a simple heuristic
method that allows us finding hopefully good, but not necessarily optimal solutions
quickly.
At first glance, formulating a traveling salesman problem appears easy. We need
to formulate zero-one variables xij that assume a value of one, if the arc aij is part of
the tour, and 0 otherwise. The objective function is then simply to minimize the sum
of arc values, each multiplied with their respective binary variable. As far as
constraints go, we have to ensure that the traveling salesman tour enters each node
exactly once and that it leaves each node exactly once. In order to explain the
formulation, consider as an example the graph in Fig. 6.30.
Fig. 6.30 Graph for the
traveling salesman problem
6.6 Routing Problems 249The formulation as described above is then as follows:
Min z ¼ 2x12 þ 5x16 þ 3x21 þ 8x25 þ 4x26 þ 7x32 þ 7x34 þ 5x36 þ 4x35
þ1x43 þ 6x45 þ 5x53 þ 2x54 þ 3x61 þ 4x63 þ 3x65
s:t: x12 þ x16 ¼ 1 x21 þ x61 ¼ 1
x21 þ x25 þ x26 ¼ 1 x12 þ x32 ¼ 1
x32 þ x34 þ x35 þ x36 ¼ 1 x43 þ x53 þ x63 ¼ 1
x43 þ x45 ¼ 1 x34 þ x54 ¼ 1
x53 þ x54 ¼ 1 x25 þ x35 þ x45 þ x65 ¼ 1
x61 þ x63 þ x65 ¼ 1 x16 þ x26 þ x36 ¼ 1
xij ¼ 0 or 1 for all i, j:
The first six constraints in the above formulation force the outflow of each node to
be equal to one (meaning that the traveling salesman tour will leave each node
exactly once), while the second set of six constraints requires that the inflow into
each node equals one (meaning that the tour enters each node exactly once). If the
original graph would have direct connections between all pairs of nodes, the above
formulation would actually be identical to that of an assignment problem. We know
that for this type of problem, the zero-one constraints are satisfied without us
requiring them, so that the problem can be solved as a standard linear programming
problem. The result we find includes the following nonzero variables: x12 ¼ x21 ¼
x36 ¼ x43 ¼ x54 ¼ x65 ¼ 1 , and all other variables equal zero. The value of the
objective function for this solution equals z ¼ 16.
It is easily apparent that this solution does satisfy the constraints but is not what
we were looking for: rather than one tour, the solution includes two subtours (n1—
n2—n1) and (n3—n6—n5—n4—n3). What we will have to add are the so-called
subtour elimination constraints. There are different ways of doing this, but the most
efficient sets of such constraints require a huge number of constraints. To be precise,
for a graph of n nodes, there are 2n such subtour elimination constraints. As a result,
the modeler will refrain from including all of these constraints from the beginning,
but rather solve the problem without them. Then, if the solution has no subtours, we
are done. Otherwise, a single relevant subtour elimination constraint is introduced,
the problem is solved again, and the process is repeated until a single tour emerges.
This is the process we follow here.
The idea is now this. First, we select some subtour, define Ns as the set of nodes in
the chosen subtour, and let Ns denote the complement of this set. Note that the
present solution has no arc leading out of Ns to any node in Ns. Therefore, we define
a constraint that requires at least one arc in the solution to lead out of a node in Ns to a
node in Ns. In our example, choose the subtour (n1 – n2 – n1), so that Ns ¼ {n1, n2}
250 6 Network Modelsand Ns ¼ f g n3, n4, n5, n6 . The set {a16, a25, a26} includes all arcs that lead from Ns to
Ns, so that we can formulate the constraint
x16 þ x25 þ x26  1:
We now solve the problem again with this additional constraint. Note that due to
the additional constraint the “assignment structure” of the problem is lost, so that it is
necessary to explicitly include the zero-one requirements for all variables, which
makes the problem considerably more difficult. The optimal solution of the problem
is x12 ¼ x26 ¼ x35 ¼ x43 ¼ x54 ¼ x61 ¼ 1 with an objective value of z ¼ 16 (the
same as before, so apparently, there were alternative optimal solutions to the
problem in the first step). This means that our tour is (n1—n2—n6—n1) and (n3—
n5—n4—n3), meaning that we have successfully eliminated the previous subtour,
but now have a solution with another subtour, so that another subtour elimination
constraint must be added.
Given the subtour (n1—n2—n6—n1), our sets are Ns ¼ {n1, n2, n6} and Ns ¼
f g n3, n4, n5 , so that the set of arcs from Ns to Ns is {a25, a63, a65}. The additional
constraint can then be written as
x25 þ x63 þ x65  1:
Solving the problem again results in the solution x16 ¼ x21 ¼ x32 ¼ x43 ¼ x54 ¼
x65 ¼ 1 with a value of the objective function z ¼ 21. This solution includes the tour
(n1—n6—n5—n4—n3—n2—n1), which no longer includes a subtour. This is the
optimal solution.
While this procedure may be feasible for small and medium-sized problems, it is
not suitable for large-scale applications. Here, we may resort to heuristic algorithms.
In the simplest case, we may use the Greedy algorithm (also referred to as the nearest
neighbor method) to find a tour. In this application, we would start the Greedy
algorithm with some node, find the nearest neighbor (provided it does not result in a
subtour), move on to the next neighbor, again avoiding subtours, and so forth. Note
that the number of degrees of freedom is constantly decreasing while we make
choices. (Not that there is anything new in that: whenever you make a choice such as
spending money on some item, you will have less choices, i.e., money, for future
decisions.)
In order to explain the procedure, consider the distance matrix
D ¼
047326
304968
540834
683025
172304
874940
2
6
6
6
6
6
6
6
6
4
3
7
7
7
7
7
7
7
7
5
:
6.6 Routing Problems 251Arbitrarily starting with the node n1, the nearest neighbor, i.e., the smallest
element in the first row of D (other than the element d11 ¼ 0, which would just
lead from n1 back to itself), is the connection to n5 at a distance of 2. From n5, i.e., in
row 5 of the matrix, the nearest neighbor is n1 at a distance of 1. However, this
connection would create a subtour, so we look for the next-shortest distance. It is the
link from n5 to n3 with a distance of 2. From n3, the nearest neighbor is n5 with a
distance of 3, but going back to n5 would create a subtour. The next-shortest
distances are those to n2 and n6, both with a distance of 4. Any tie-breaking rule
can be used, here, we choose n2. From n2, the nearest neighbor is n1, which cannot be
chosen, as it would create a subtour. The next-nearest neighbor is n3, which cannot
be chosen for the same reason. The next-nearest neighbor is n5, which is also not
eligible. The next-nearest neighbor is n6 at a distance of 8, which must be chosen.
Note how the lack of degrees of freedom forces us to choose undesirable links at the
later stages of the algorithm. Actually, at this point there is no degree of freedom left,
and we have to continue on to n4, the last remaining node, and from there return to
n1. The two distances are 9 and 6, respectively, so that the entire tour is of length l ¼
31. In summary, the tour is n1—n5—n3—n2—n6—n4—n1. Clearly, this procedure
will always result in a traveling salesman tour. However, we can show that the tour
found in this manner can be arbitrarily bad.
In a “Phase 2” procedure, we can now attempt to improve the solution found
earlier. One way to do this is a “pairwise exchange” or “swap” method. It very
simply exchanges two (usually, but not necessarily, adjacent) nodes on the tour. In
the above tour, we may try to avoid the long distance from n6 to n4 by switching the
order, in which these two nodes are visited on the tour. Doing so results in the tour
n1—n5—n3—n2—n4—n6—n1, which has length l ¼ 30, which is better than the
present solution, so that this new tour becomes the starting point for further
improvements.
Starting with the new tour, we may now attempt to avoid the direct connection
from n2 to n4, which is also a very long leg of the tour. Switching the order of these
two nodes results in the tour n1—n5—n3—n4—n2—n6—n1, which is of length l ¼
36. This is higher than the previous (best known) solution, so that the switch is not
made. This swap process can be continued, until no swap change is able to further
decrease the length of the tour. Again, this being a heuristic method, an optimal
solution may not be found.
It is sometimes useful to use what is known as a multistart procedure. This means
that rather than starting with some random node (n1 in the above example), finding a
solution and then trying to improve it, we may try out some or all different nodes as
potential starting points. With each such node, we obtain a tour. We would then
choose the best tour, and try to find improvements from there.
In the above example, we could use the Greedy algorithm to find tours starting
with each of the six nodes. In addition to the tour that starts with n1, which has
already been determined above, we obtain the tours n2—n1—n5—n3—n6—n4—n2
of length l ¼ 28, n3—n5—n1—n4—n6—n2—n3 of length l ¼ 23, n4—n5—n1—n2—
n3—n6—n4 of length l ¼ 24, n5—n1—n4—n3—n2—n6—n5 of length l ¼ 23, and
n6—n3—n5—n1—n4—n2—n6 of length l ¼ 27. The tours that start with n3 and n5
252 6 Network Modelsare best, and the swap process would start with them. Any of these heuristic
procedures is computationally cheap and, once some tour has been obtained, the
process can be terminated at any point in time.
Exercises
Problem 1 (maximal flow algorithm, minimal cut): Consider the network in
Fig. 6.31, in which the numbers next to the directed arcs denote the capacity of the
arcs.
Use the method by Ford and Fulkerson to determine a maximal flow. Show the
flow pattern. What is the value of the maximal flow? Which arcs are in the minimal
cut you have found?
Solution: Starting with a flow of zero at all arcs, we first increase the flow on the
following paths:
path (ns, n1, nt): 6 units,
path (ns, n2, n4, nt): 6 units,
path (ns, n3, n5, nt): 6 units, and
path (ns, n5, nt): 3 units.
The flow pattern is then shown in the network in Fig. 6.32, where the two
numbers next to the arcs indicate the arc’s capacity and its present flow, respectively.
The next step consists of an increase of the flow on the following two paths:
path (ns, n3, n2, n1, nt): 2 units, and
path (ns, n2, n1, n4, nt): 1 unit.
The resulting flow pattern is shown in Fig. 6.33.
Fig. 6.31 Network for Problem 1
6.6 Routing Problems 253At this point, there are no more degrees of freedom, and labeling can only be done
on the path (ns, n5, n3 (backward labeling at this point), n2, n1, n4, nt). The flow on
this path can be changed by one unit, resulting in the flow pattern shown in the
network in Fig. 6.34.
Starting with the pattern in Fig. 6.34, we can label the nodes ns, n3, and n5. At this
point, a nonbreakthrough occurs, and we can conclude that the flow pattern in
Fig. 6.34 is maximal. The corresponding flow value is 25. The minimal cut includes
the capacities of all arcs that lead from labeled to unlabeled nodes. Here the minimal
cut includes the arcs (ns, n1), (ns, n2), (n3, n2), and (n5, nt). Note that the Ford and
Fulkerson method only finds the cut that is closest to the source. In this example,
another minimal cut exists with arcs (ns, n1), (n2, n1), (n2, n4), and (n5, nt).
Fig. 6.32 First flow pattern for Problem 1
Fig. 6.33 Second flow pattern for Problem 1
254 6 Network ModelsProblem 2 (formulation of a feasible flow problem with node constraints):
Consider the network shown in Fig. 6.35, where the numbers next to the arcs consist
of the lower and upper bound on the flow in the arc, while the double-digit numbers
next to the arcs indicate the per-unit cost of the flow through the arc.
In addition, we want to ensure that a total of 5 units flow from the source to the
sink and exactly two units flow through the node n3. Formulate a cost-minimizing
feasible flow model for this problem.
Fig. 6.34 Third and optimal flow pattern for Problem 1
Fig. 6.35 Network for Problem 2
6.6 Routing Problems 255Solution: Defining variables xij as the flow from node i to node j on the arc
connecting the two nodes, we can formulate the problem as follows:
P : Min z ¼ 20xst þ 50xs1 þ 40xs2 þ 20x12 þ 50x13 þ 60x1t þ 20x21 þ 50x23
þ40x2t þ 30x3t
s:t: xst þ xs1 þ xs2 ¼ 5 forcing 5 units through the network ð Þ
xs1 þ x21  x12  x13  x1t ¼ 0 conservation equation for node ð Þ n1
xs2 þ x12  x21  x23  x2t ¼ 0 conservation equation for node ð Þ n2
x13 þ x23  x3t ¼ 0 conservation equation for node ð Þ n3
x13 þ x23 ¼ 2 or; equivalently; x3t

¼ 2 : forces a flow of 2 through node n3

xst  4
xs1  3
xs2  4
x12  2
x13  6
x1t  6
x21  3
x23  7
x2t  6
x3t  4
9
>>>>>>>>>>>>>>>>>>>=
>>>>>>>>>>>>>>>>>>>;
ð Þ upper limits on arc flows
xs1  1
x1t  2
)
ð Þ lower limits on arc flows
xij  0 for all i, j:
The optimal flow pattern is shown in Fig. 6.36, where the numbers next to the arcs
are the arc flows. The total cost for the flow pattern is $480.
Changing the requirement regarding the throughput of node n3 from “exactly
2 units” to “at most 2 units” results in a flow of 3 units on the arc from ns to nt, and
2 units on the path from ns to n1 and nt, i.e. zero flow through node n3. The total costs
of this solution are significantly less, at $280.
Problem 3 (shortest path, Dijkstra method): Consider the network in Fig. 6.37
and determine the shortest paths from ns to all other nodes.
Solution: Table 6.3 shows the labeling process.
The tree with the shortest distances is shown in Fig. 6.38. Note that in this
example, the tree is a simple path.
Problem 4 (shortest path, Floyd–Warshall method): Consider the network
shown in Fig. 6.39.
256 6 Network ModelsFig. 6.36 Optimal flow pattern for Problem 2
Fig. 6.37 Network for Problem 3
Table 6.3 Labels of the nodes in the shortest path example
Step # L(ns) L(n1) L(n2) L(n3) L(n4) L(nt)
0 (ns, 0)* (n1, 1) (n2, 1) (n3, 1) (n4, 1) (nt, 1)
1 (ns, 6) (ns, 2)* (n3, 1) (ns, 10) (nt, 1)
2 (n2, 5)* (n2, 9) (ns, 10) (nt, 1)
3 (n1, 8)* (ns, 10) (n1, 14)
4 (n3, 10)* (n1, 14), or (n3, 14)
5 (n4, 13)*
6.6 Routing Problems 257Use the Floyd–Warshall algorithm to determine the shortest paths between all
pairs of nodes.
Solution: The direct distance matrix is
0 2 1 6 4
20332
1 3089
6380 1
42970
2
6
6
6
6
6
6
4
3
7
7
7
7
7
7
5
:
The first two steps result in the following approximations (where the elements
with a “*” indicate changes in the step that led to the new solution):
Fig. 6.38 Optimal solution
for Problem 3
Fig. 6.39 Network for
Problem 4
258 6 Network ModelsAfter this, there are no further changes, so that the last matrix is indeed the
shortest path matrix.
Problem 5 (minimal spanning tree): A manufacturing firm wants to connect its
nine plants by rail. The cost of establishing the line segments is shown in the
symmetric table below, where a number in row i and column j indicates the fixed
costs to connect plant i with plant j.
0 23 64 15 39 66 18 84 93
23 0 35 27 56 63 29 91 37
64 35 0 96 62 45 33 72 48
15 27 96 0 38 22 74 61 88
39 56 62 38 0 42 95 54 67
66 63 45 22 42 0 25 43 47
18 29 33 74 95 25 0 51 32
84 91 72 61 54 43 51 0 26
93 37 48 88 67 47 32 26 0
2
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
4
3
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
5
Solution: Putting the costs in nondecreasing order (ties are broken arbitrarily)
results in the sequence 15, 18, 22, 23, 25, 26, 27, 29, 32, 33, 35, 37, 38, 39, 42, 43,
45, 47, 48, 51, 54, 56, 61, 62, 63, 64, 66, 67, 72, 74, 84, 88, 91, 93, 95, and 96.
Starting at the beginning, we introduce connections (1, 4), (1, 7), (4, 6), and (1, 2)
with costs of 15, 18, 22, and 23. The next cheapest connection is (6, 7) with costs of
25. However, plants 6 and 7 are already connected. The next connection is (8, 9) with
costs of 26, which we introduce. The next connection on the list is (2, 4) with costs of
27. Plants 2 and 4 are already connected, so that we reject this connection and
continue. The next connection is (2, 7) with costs of 29, which is also rejected. The
next connection is (7, 9) with costs of 32, which is introduced. This is followed by
connection (3, 7) with costs of 33, which is introduced. This is followed by the
connections (2, 3) with costs of 35 and (2, 9) with costs of 37; both of which are
rejected. The connection (4, 5) with costs 38 connects two previously unconnected
nodes, and thus it is introduced. At this point, eight arcs have been introduced and all
plants are connected. Thus, an optimal solution has been determined. Its costs
are 207.
Problem 6 (Chinese Postman Problem in undirected graphs): A local post
office plans the route of its letter carrier. The region has eight street junctions and
6.6 Routing Problems 259twelve street segments, which the carrier has to serve. Figure 6.40 shows the streets
and the junctions as well as the distances (in hundreds of feet).
(a) Is it possible for the post office to find a tour for the postman, so that the letter
carrier has to traverse each street exactly once?
(b) In case a tour as sought under (a) does not exist, determine a tour that minimizes
the total distance that the letter carrier has to walk. What is the length of such
a tour?
(c) The agreement with the letter carriers’ union stipulates that the post office has to
pay 20¢ per ft. and year to the letter carrier. It has now become possible to hire
an outside firm for the service. However, there are two individuals who agree to
do the job, but only if they can split the tour among themselves in a specific way,
as each has a preference for certain neighborhoods. In particular, Joe, the first
potential carrier, would like to cover the streets e12, e15, e23, e26, e34, e37, and e48,
while Jim, our second carrier, prefers to deliver the mail along the remaining
streets. How much can the post office offer them, if they do not want to pay more
than they do for a single carrier under the present system?
Solution:
(a) The question is whether or not the graph in Fig. 6.40 is Eulerian. Since the
degrees of the nodes are 3, 3, 4, 3, 2, 5, 2, and 2, respectively, not all degrees are
even numbers, so that such a tour does not exist.
(b) The nodes with odd degrees are n1, n2, n4, and n6. The distances between these
nodes and the actual paths are shown in Table 6.4.
Fig. 6.40 Network for Problem 6
Table 6.4 Shortest paths and path lengths for Problem 6
n1 n2 n4 n6
n1 – 2 (e12) 7(e12, e23, e34) 3(e12, e26)
n2 2 (e21) – 5 (e23, e34) 1(e26)
n4 7 (e43, e32, e21) 5(e43, e32) – 5 (e48, e86)
n6 3 (e62, e21) 1(e62) 5(e68, e84) –
260 6 Network ModelsSolving the symmetric assignment problem, we find the unique optimal solution
x12 ¼ x21 ¼ x46 ¼ x64 ¼ 1 and xij ¼ 0 otherwise. The value assignment x12 ¼ 1
means that we add one copy of the shortest path from n1 to n2 to the graph in
Fig. 6.40 (meaning one copy of the edge e12), and the assignment x46 ¼ 1 means
that we add one copy of the shortest path from n4 to n6 to the graph, i.e. copies of
the edges e48 and e86. The resulting graph (with the added edges shown as
broken lines) is depicted in Fig. 6.41.
One of the many possible tours in the graph is n1—n2—n3—n4—n8—n6—
n2—n1—n5—n6—n3—n7—n4—n8—n6—n1 with a total length of
53, i.e. 5300 ft.
(c) At present, the length of the tour is 5300 ft. as determined under (b), meaning
that the costs to the post office are $1060 per year. Given Joe and Jim’s offer, we
first determine their respective tours. The streets they want to cover are shown in
Figs. 6.42a, b, respectively, where the solid lines indicate the streets they desire
to cover, while the broken lines show the streets they have to walk more
than once.
The total lengths of their tours are 3900 ft. and 3800 ft. for a total of 7700 ft.
Denoting by p the money the post office can pay them per foot covered, the break￾Fig. 6.41 Network with added edges for Problem 6
Fig. 6.42 Solution for Problem 6 (c). (a) Joe’s preference, (b) Jim’s preference
6.6 Routing Problems 261even point is obtained for 1060 ¼ 7700p or p ¼ 13.7662¢. This will result in
payments of $536.88 to Joe and 523.12 to Jim.
Problem 7 (Chinese Postman Problem in directed graphs): City Council plans
the snow plowing strategies for next winter. The graph in Fig. 6.43 shows the street
network, which includes only one-way streets. The numbers next to the arcs denote
the amount of time (in minutes) that is required to plow the streets.
(a) Without doing any computations, what is the shortest possible time in which the
streets may be plowed? Is it actually possible to plow the streets in this amount
of time? Explain.
(b) If the answer in (a) is negative, find the shortest tour in which all streets can be
plowed.
(c) Suppose that each minute of a snowplow costs $5. A private contractor has now
offered to plow streets a57 and a58. What is the maximal amount of money that
the municipality can pay the contractor, so that they do not pay more in total
than if they do the job themselves?
Solution:
(a) The shortest possible plowing time is the sum of all of the arc values of the graph
in Fig. 6.43. In our example, this shortest possible time is 58 min. However, this
time can only be realized if there were a tour, in which the plow traverses each
street exactly once, i.e., if an Euler tour exists. Given that the indegree does not
equal the outdegree for each node, such a tour does not exist, so that 58 min can
only be used as a lower bound on the shortest amount of time, in which all
streets can be plowed.
(b) Given that the original graph does not include an Euler tour, we have to determine
the augmented graph. In our example, the indegrees of the nodes are 1, 3, 1, 2,
1, 3, 3, and 2, while the outdegrees are 2, 2, 3, 2, 3, 1, 2, and 1, so that the sets are
N+ ¼ {n2, n6, n7, n8} N ¼ {n`1, n3, n5}. The shortest paths and their lengths for all
pairs from a node in N+ to a node in N are shown in Table 6.5.
Fig. 6.43 Street network for Problem 7
262 6 Network ModelsThe pertinent transportation problem can then be written as follows:
P : Min z ¼ 3x21 þ 10x23 þ 7x25 þ 13x61 þ 5x63 þ 17x65 þ 11x71 þ 3x73
þ15x75 þ 16x81 þ 8x83 þ 20x85
s:t: x21 þ x23 þ x25 ¼ 1
x61 þ x63 þ x65 ¼ 2
x71 þ x73 þ x75 ¼ 1
x81 þ x83 þ x85 ¼ 1
x21 þ x61 þ x71 þ x81 ¼ 1
x23 þ x63 þ x73 þ x83 ¼ 2
x25 þ x65 þ x75 þ x85 ¼ 2
xij  0 and integer for all i, j.
Solving the problem, we find many alternative optimal solutions. One of them has
x21 ¼ 1, x63 ¼ 2, x75 ¼ 1, and x85 ¼ 1, while all other xij ¼ 0 for all other
variables. This means that one copy of the arcs on the shortest path from n2 to n1
is added (the single arc a21), two copies of all arcs on the shortest path from n6 to
n3 are added (the arcs a67 and a73), one copy of all arcs on the shortest path from
n7 to n5 are added (the arcs a74, a42, a21, and a15), and one copy of all arcs on the
shortest path from n8 to n5 are added (the arcs a87, a74, a42, a21, and a15). The
resulting graph is shown in Fig. 6.44.
This graph is now Eulerian, and we can easily determine an Euler tour, such as n1,
n2, n1, n5, n6, n7, n4, n8, n7, n3, n6, n7, n4, n2, n1, n5, n7, n3, n4, n2, n1, n5, n8, n7, n4,
n2, n6, n7, n3, n2, and n1, and the length of the tour is 106.
(c) Given the length of the tour of 106, the costs for the snow plow are
106(5) ¼ $530. Tentatively deleting the arcs a57 and a58 results in the network
shown in Fig. 6.45.
n1 n3
n2 3 10
n6 13 5
Table 6.5 Shortest paths and path lengths for Problem 7
n1 n3 n5
n2 3 (a21) 10 (a26, a67, a73) 7(a21, a15)
n6 13 (a67, a74, a42, a21) 5(a67, a73) 17 (a67, a74, a42, a21, a15)
n7 11 (a74, a42, a21) 3(a73) 15 (a74, a42, a21, a15)
n8 16 (a87, a74, a42, a21) 8(a87, a73) 20 (a87, a74, a42, a21, a15)
6.6 Routing Problems 263The problem can again be formulated as a transportation problem with cost matrix
and the shortest paths happen to be the same as under (b). The “supplies” (i.e., the
indegrees minus the outdegrees of the two source nodes) are 1 and 2, while the
“demands” (i.e., the outdegrees minus the indegrees of the two sink nodes) are 1 and
2. The optimal solution includes x21 ¼ 1 and x63 ¼ 2. The augmented Eulerian graph
is shown in Fig. 6.46.
The length of the tour is 60, so that the costs are $300. This means savings of
530–300 ¼ $230, which is the maximal amount that the municipality can spend for
the plowing of the two additional arcs. It is apparent that the two arcs the contractors
offered to plow are causing significant costs if the municipality does the plowing.
This raises the issue of the apportionment of costs.
Problem 8 (heuristics applied to a traveling salesman problem): A pharma￾ceutical company uses glass containers to store their chemicals. Over time, they
Fig. 6.45 Network for Problem 7 (c)
Fig. 6.44 Augmented graph for Problem 7
264 6 Network Modelsreuse the containers, but if they do, they are required to clean them. The cleaning
costs depend on what was stored in the container before, and what will be stored
next. The cleaning costs of the six chemicals A, B, C, D, E, and F are shown in the
following matrix:
065354
207938
551642
423213
964307
238160
2
6
6
6
6
6
6
6
6
4
3
7
7
7
7
7
7
7
7
5
:
For example, if chemical C is stored in a glass that was used for F before, then the
cleaning costs are 8 (from F to C, i.e., element (F, C), in the sixth row and third
column).
(a) Assume that presently, chemical D is stored in a container. Use the Greedy
algorithm to determine a sequence that has each of the six chemicals stored in a
container exactly once. (Ties are broken arbitrarily.) Clearly specify the
sequence that results from the application of the Greedy algorithm. What are
the costs associated with the sequence?
(b) Use the pairwise exchange method to improve the solution determined under
(a). Examine all adjacent pairs until an exchange results in an improvement.
Make this improvement and stop (even though additional improvements may be
possible).
Solution:
(a) Starting with D, we obtain the sequence D—E—C—F—A—B—D. The length
of the tour (here: the cost) is l ¼ 1+4+2+2+6+9 ¼ 24.
Fig. 6.46 Augmented network for Problem 7 (c)
6.6 Routing Problems 265(b) In this application, the starting point at D is fixed. The following swap steps can
be made:
Swap E and C. Result: D—C—E—F—A—B—D, length l ¼ 31, reject.
Swap C and F. Result: D—E—F—C—A—B—D, length l ¼ 36, reject.
Swap F and A. Result: D – E – C – A – F – B – D, length l ¼ 26, reject.
Swap A and B. Result: D – E – C – F – B – A – D, length l ¼ 15, accept.
This solution becomes the new benchmark from which the swap steps continue.
If there is no fixed starting point, it would make sense to start swapping with the most
expensive direct connection, in our case B – D.
References
Eiselt HA, Sandblom C-L (2000) Integer programming and network models. Springer, Berlin
Murty K (2006). http://www-personal.umich.edu/~murty/books/network_programming/
266 6 Network ModelsLocation Models 7
This chapter introduces the basic ideas of location models. We first provide a short
introduction to the subject and enumerate some of its major components. This is
followed by a detailed discussion of the major classes of location models. For an
in-depth treatment of the models described in this chapter, we refer readers to Eiselt
and Sandblom (2004), Eiselt and Marianov (2011), Daskin (2013) and Laporte
et al. (2019).
7.1 The Major Elements of Location Problems
The origins of location theory are shrouded in history. The first to discuss location
models (and here, we use the term in the widest possible sense) were
mathematicians. One of the famous location-based puzzles (regarding the point in
the triangle from which the sum of distances to the triangles’ vertices is minimal)
were investigated and solved by Torricelli and Fermat in the seventeenth century.
The geographer von Thünen wrote about his famed “von Thünen circles” regarding
the location of economic activities around a central place, and the German geogra￾pher Weber wrote a treatise concerning location models early in the twentieth
century. Hakimi (1964) introduced location models to the field of operations
research. We will encounter his famous theorem below. Since then, thousands of
contributions have been made by researchers from fields as diverse as mathematics,
computer science, geography, business administration, and economics.
While many of us have a pretty good idea what location problems are, let us take a
step back and examine its major components. The three main components of every
location model are space, customers, and facilities. Supplies exist at the facilities,
demand occurs at the customer sites, and the goods are “somehow” transported from
the facilities to the customers. Let us now look at these components in some detail.
We distinguish between two major classes of location models, those that occur in
the plane (or sometimes in three-dimensional space), and those that occur in trans￾portation networks. While not altogether correct, location models in the plane tend to
# The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
H. A. Eiselt, C.-L. Sandblom, Operations Research, Springer Texts in Business and
Economics, https://doi.org/10.1007/978-3-030-97162-5_7
267look at the problem from a macro point of view, while those models in transportation
networks investigate the scenario from a micro perspective. Location problems in the
plane are called continuous location models (as the facilities that are to be located
can be sited anywhere in the space under consideration) in contrast to discrete
location models, many of which occur in networks. In discrete location models,
facilities can be located only at a finite number of points.
These two classes of problems are also different from the way the variables are
defined: determining the location of the variables in continuous models in the
two-dimensional plane requires the definition of variables (x1, y1), (x2, y2), ... that
symbolize the coordinates of the facilities 1, 2, ... . On the other hand, in network
models, we need a variable yj for each potential location that will assume a value of
one, if we actually do locate at site j, and 0 if we do not. This places continuous
location models into the field of linear or nonlinear optimization, while network
location models are typically formulated and solved as integer programming
problems.
As far as parameters go, we assume that the locations of our customers are
known, and so are their demands, which are commonly referred to as weights.
Formally, customer i (or, equivalently, customers at demand point i) are assumed
to have a weight of wi. In the 2-dimensional (Euclidean) plane, customer i is assumed
to be located at a point with coordinates (ai, bi), while in a network, customer i can be
found at node ni. Note that the points at which customers are located typically
represent either census units, towns, or other customer agglomerations.
Assume now that transportation takes place in the plane. While there exist many
different metrics and gauges, most authors use either rectilinear (or Manhattan)
distances, Euclidean or straight-line distances, or, sometimes, squared Euclidean
distances. Suppose that a customer is located at a point with coordinates (a, b), while
the facility is located at (x, y). The rectilinear or Manhattan distance between the
customer and the facility is then defined as |a – x|+|b – y|, i.e., it is assumed that all
movements take place parallel to the axes of the system of coordinates. The
Euclidean distance between the customer and the facility is defined as ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
ð Þ a  x 2 þ ð Þ b  y
2
q
, and the squared Euclidean distance between these two
points is (a  x)
2 + (b  y)
2
. As an example, the Manhattan, Euclidean, and squared
Euclidean distances between the points (2, 3) and (7, 1) are 7, ffiffiffiffiffi
29 p  5:3852, and
29, respectively.
In network models, it is common practice to assume that movements take place
on the shortest path between the customer and the facility. This means that the
shortest path algorithms (see Sect. 6.3) will typically have to be applied when
building a location model. In general, we will use the term “distance,” by which
we may mean the actual mileage between points or any other disutility such as time
or costs.
Other components of location problems include the following:
• The number of facilities (which may be fixed by the decision maker or may be
endogenous to the model).
268 7 Location Models• The magnitude of the demand (which may be fixed, e.g., for essential goods, or
may depend on the proximity of the facility to the customer).
• The way customers are assigned to facilities (they either choose themselves, as is
the case in the context of retail facilities, which are customer choice models, or
firms allocate customers to their facilities in allocation models, as is the case in
deliveries from warehouses).
• The type of deterministic or probabilistic parameters we are dealing with.
• Single-level or hierarchical models (hierarchical models often exist in the context
of health care, with levels such as doctor, local clinic, and regional hospital),
where a higher-level facility can accomplish all tasks a lower-level facility can. A
clearly defined referral system is crucial in multi-level models.
• Competitive or noncompetitive models (most competitive scenarios use game
theory as a tool and have firms compete in location, prices, and quantities).
One main component of any location model is the objective function pursued by
the decision maker. Until the mid-1970s, location models assumed that the facilities
to be located by the decision maker would be “attractive” to the customers in some
sense. That way, proximity to the customers was the main part of the objective.
Many facilities, however, do not fall into that mold. Consider, for instance, a grocery
store, a facility that customers normally would consider desirable to have in their
proximity. However, as noisy deliveries are made very early in the morning, the
same facility may very well be considered (at least partially) undesirable. Clearly,
power plants and landfills will be considered undesirable by most residents in the
vicinity. The NIMBY syndrome (“not in my back yard”) attests to that. While a
customer would attempt to “pull” a desirable facility towards himself, he will attempt
to “push” away an undesirable facility. In addition to push and pull models, there are
also “balancing” models, in which the decision maker attempts to locate facilities so
as to balance the interactions between individual facilities and the customers. This is
often the case in the location of automobile dealerships, motels of a chain, and fast￾food franchises, which are located so as to avoid cannibalizing demand from its own
(other) facilities.
Among the pull objectives, three classes have received most of the attention. The
first of these classes of models deals with covering objectives. The main idea is to
locate facilities, so that as many customers as possible are covered, i.e., within a
given distance of any of the facilities. Such objectives are often used for the location
of emergency equipment, i.e., ambulances, police stations, and fire stations. The
other two classes are center and median locations. In both of them, the decision
maker locates facilities so as to minimize a measure of distance. In center problems,
the planner will attempt to locate facilities, so that the largest distance to customers is
as small as possible. This objective was justified (in some sense) by Rawls’s “Theory
of Justice,” according to which the quality of a solution is determined by the lowest
quality of any of its components. While such objectives may be justified in the
context of reliability systems, it takes an extreme view and is strongly biased towards
outliers. Median problems, on the other hand, are probably the largest class in
location models. Their objective is the minimization of the weighted total distance.
7.1 The Major Elements of Location Problems 269In other words, the distance between a customer and his closest facility is deter￾mined, which is then multiplied by the magnitude of the customer’s demand.
Considering the distance as a proxy for the cost of shipping one truckload from
the facility to the customer (or vice versa), the customer’s demand then denotes the
number of truckloads to be shipped. The sum of all of these weighted distances is
then a proxy expression for the costs of the transportation of the goods to all
customers. Similarly, from a customer’s point of view, a median solution minimizes
the average customer—facility distance, thus benefitting customers as well.
Applications of location problems range from the location of schools to
warehouses and church camps, equipment for the removal of oil spills, warning
sirens in towns in case of floods, hurricanes, or tsunamis, bottling plants, landfills,
newspaper transfer points, sewage treatment plants, and many other facilities. Other
applications include nonphysical spaces. An example are brand positioning models,
in which each dimension of the space represents a feature that is deemed relevant to
customers. The features of each product will then determine its location in this
feature space. Similarly, each (potential) customer can be represented in the same
space by his ideal point, i.e., the point that has the features most preferred by the
customer. Assuming that each customer purchases the brand closest to his ideal
point, it is then possible to determine the estimated sales of each product. Then it is
also possible to relocate, i.e., redesign, a brand so as to maximize the amount of
demand it captures. The model has also been used to study the positions of political
candidates, who attempt to maximize their share of the vote.
7.2 Covering Problems
The idea behind covering models is to locate facilities that provide some service
required by customers. If customers are positioned within a certain predefined
critical distance d from any of the facilities, then they are considered served or
“covered.” Two objectives for the location of facilities are to either cover all
customers in the network with the smallest number of facilities or, alternatively, to
cover as many customers as possible with a given number of facilities. Typical
examples of applications of covering models are found when emergency facilities
are to be located. Whether the facilities in question are fire stations, ambulances,
police cruisers, or any similar “facilities,” the objective is to maximize protection.
However, measuring protection is very difficult. In order to find an expression for
“protection,” we would need to know the value of responding to an emergency from
different distances or times. We can safely assume that an increase in the time to
respond to an area for fire protection will mean that the fire has a greater chance to
spread and it may reduce the chance to save property or a life if one is in jeopardy. A
similar argument applies to other types of protection. Unfortunately, measurements
of the value of protection are nearly impossible to make. In the case of fire
protection, standards of service have been suggested by the Insurance Services
Office that, when met, virtually guarantee an adequate level of protection or, at
least, the lowest premiums for fire protection. For fire services, the most commonly
270 7 Location Modelsused surrogate measure of performance is whether service can be provided to areas
within a prespecified distance or time. Here, we restrict ourselves to covering
problems on networks.
7.2.1 The Location Set Covering Problem
One of the first models that was developed to site emergency service facilities that
incorporates a maximum service distance standard is the Location Set Covering
Problem (LSCP) introduced in the early 1970s. The objective of the LSCP is to
locate the smallest number of facilities such that each demand node is “covered” by
one or more facilities. A demand node is said to be covered, if there is a facility
within a prespecified distance from the demand node. In the context of the location
of a fire hall, the decision maker could specify that a building is covered
(or sufficiently protected), if it is within 5 miles (or 7 min or some similar measure)
of the fire hall.
If we assume that the cost to purchase land and build a facility is roughly the same
for all nodes (or is small in comparison to maintaining each of the needed fire crews),
then the objective of using the least number of facilities is equivalent to minimizing
the cost of providing service to all demand.
Throughout this chapter, we will use the subscript i to denote customers, while the
subscript j is employed to denote facilities. For simplicity, we also assume that all
customers and facilities will be located at the nodes of the network. We can then
define dij as the shortest distance (or time, cost, or any other disutility) on a path
between the demand node ni and the facility site at node nj. In addition, let the service
standard d denote the maximal service distance or time as specified by the decision
maker. We can then define a coverage matrix C d
  ¼ cij   , so that cij ¼ 1, if
customers at node ni can be covered from a facility located at node nj, and 0 other￾wise. As an example, consider the network in Fig. 7.1.
The symmetric matrix of shortest distances is D.
n1 n2 n3 n4 n5 n6 n7 n8 n9
n1 0 5 11 14 6 12 14 20 18
n2 5 0 6 9 7 11 15 19 13
n3 11 6 0 9 11 5 16 13 7
n4 14 9 9 0 8 4 16 12 16
D: n5 6 7 11 8 0 6 8 14 17
n6 12 11 5 4 6 0 14 8 12
n7 14 15 16 16 8 14 0 7 9
n8 20 19 13 12 14 8 7 0 16
n9 18 13 7 16 17 12 9 16 0
Given a covering distance of d ¼ 10, the covering matrix C(10) is then.
7.2 Covering Problems 271n1 n2 n3 n4 n5 n6 n7 n8 n9
n1 1100100 0 0
n2 1111100 0 0
n3 0111010 0 1
n4 0111110 0 0 
C(10): n5 1101111 0 0
n6 0011110 1 0
n7 0000101 1 1
n8 0000011 1 0
n9 0 0 1 0 0 0 1 0 1
The problem can then be formulated as
LSCP : Min z ¼ y1 þ y2 þ y3 þ y4 þ y5 þ y6 þ y7 þ y8 þ y9
s:t: y1 þ y2 þ y5  1
y1 þ y2 þ y3 þ y4 þ y5  1
y2 þ y3 þ y4 þ y6 þ y9  1
y2 þ y3 þ y4 þ y5 þ y6  1
y1 þ y2 þ y4 þ y5 þ y6 þ y7  1
y3 þ y4 þ y5 þ y6 þ y8  1
y5 þ y7 þ y8 þ y9  1
y6 þ y7 þ y8  1
y3 þ y7 þ y9  1
y1, y2, ... , y9 ¼ 0 or 1:
Fig. 7.1 Network example
272 7 Location ModelsThe objective function minimizes the number of facilities that are located, and the
constraints ensure that at least one facility is located within reach of every customer.
We first solve the problem as a linear programming problem, as often the solution
is integer without us requiring it, a property sometimes referred to as “integer
friendly.” In this example, the solution of the linear programming problem is y1 ¼
0, y2 ¼ 1=2, y3 ¼ 1=4, y4 ¼ 0, y5 ¼ 1=2, y6 ¼ 1=4, y7 ¼ 3=4, and y8 ¼ y9 ¼ 0. The value
of the objective function for this solution is z ¼ 2¼. Including zero-one conditions
for all variables results in the optimal solution y5 ¼ y6 ¼ y7 ¼ 1 and all other
variables zero, i.e., facilities should be located at the nodes n5, n6, and n7. However,
this solution is not unique: other optimal solutions with three facilities locate them at
nodes n2, n3, and n7, or at n3, n5 and n8, or at n2, n3 and n8, at n2, n5, n7, or at n2, n6,
and n7.
In order to further explore the problem, assume now that the distance standard is
changed to d ¼ 9. The resulting covering matrix C(9) is actually identical to that
found for d ¼ 10, which is not surprising, as there is no distance of magnitude 10 in
the matrix of shortest distances. Suppose now that d ¼ 8 . The coverage matrix
C(8) is then
n1 n2 n3 n4 n5 n6 n7 n8 n9
n1 1100100 0 0
n2 1110100 0 0
n3 0110010 0 1
n4 0001110 0 0 
C(8): n5 1101111 0 0
n6 0011110 1 0
n7 0000101 1 0
n8 0000011 1 0
n9 0 0 1 0 0 0 0 0 1
Solving the problem, we obtain a solution that locates facilities at n3, n5, and at
one of n6, n7, or n8. In other words, even with a distance standard of d ¼ 8, three
facilities are sufficient to cover all customers.
If the distance standard is reduced further to d ¼ 7, four facilities are necessary to
cover all customers. These facilities will have to be located at n2, n3, n6, and n7, or at
n3, n5, n6, n7, or at n2, n3, n6, n8, or at n3, n5, n6, n8. A further reduction of the distance
to d ¼ 6 results in five facilities being required to cover all customers. The facilities
will be located at n6, n7, n8, n9, and either n1 or n2. In case the distance standard is set
to d ¼ 5, we need six facilities to cover all customers. The facilities will be located at
n1, n5, n6, n7, n8, and n9.
For a distance standard of d ¼ 4 , we need no less than eight facilities. In
particular, there will be a facility at each node, except for either node n4 or n6.
Once the distance standard is D < 4, there must be a facility at each node.
On the other hand, increasing the distance standard from the original d ¼ 10 to
d ¼ 11, we can determine that only two facilities are needed to cover all customers.
These facilities will be located at n5 and n7.
7.2 Covering Problems 273Finally, for any distance standard d  14, only a single facility is needed to cover
all customers. This facility will have to be located at the node n6. Note that this
solution is unique.
The results are summarized in Fig. 7.2, which has the distance standard on the
ordinate and the number of facilities required to cover all nodes at the abscissa.
One observation we can make in these computations is that there typically are
multiple optimal solutions for location set covering problems. Furthermore, we
would like to mention that there exist rules that may allow users to reduce the size
of the problem, see, e.g., Eiselt and Sandblom (2004). However, given today’s
computational power, such rules are of limited usefulness.
7.2.2 The Maximal Covering Location Problem
In contrast to the location set covering problem discussed above, the Maximal
Covering Location Problem (MCLP) does not attempt the task to cover all
customers. Given a fixed number of p facilities, the task is to locate these facilities
so as to cover the largest possible number of customers (or the largest total demand).
In addition to the parameters defined in the previous section, we also need wi, which
denotes the number of customers (or the magnitude of the demand) at node ni.
As an illustration, consider again the example of the previous section with the
assumption that exactly two facilities are to be located. Furthermore, let the covering
distance be d ¼ 10. The weights (i.e., the number of customers at a node) are given
as follows:
n1 n2 n3 n4 n5 n6 n7 n8 n9
120 160 100 110 130 140 190 220 200
In order to formulate the problem, we not only need the binary location variables
yj, but we also need coverage variables xi, i ¼ 1, ..., n. A coverage variable xi
assumes a value of one, if a customer at node ni is covered by at least one facility, and
Fig. 7.2 Number of facilities versus distance standard
274 7 Location Modelszero otherwise. The main reason for these additional variables is to avoid double
counting. The formulation of our problem is then
MCLP : Max z ¼ 120x1 þ 160x2 þ 100x3 þ 110x4 þ 130x5 þ 140x6
þ190x7 þ 220x8 þ 200x9
s:t: y1 þ y2 þ y3 þ y4 þ y5 þ y6 þ y7 þ y8 þ y9 ¼ 2
x1  y1 þ y2 þ y5
x2  y1 þ y2 þ y3 þ y4 þ y5
x3  y2 þ y3 þ y4 þ y6 þ y9
x4  y2 þ y3 þ y4 þ y5 þ y6
x5  y1 þ y2 þ y4 þ y5 þ y6 þ y7
x6  y3 þ y4 þ y5 þ y6 þ y8
x7  y5 þ y7 þ y8 þ y9
x8  y6 þ y7 þ y8
x9  y3 þ y7 þ y9
y j ¼ 0 or1 for all j and
xi ¼ 0 or 1 for all i:
Each term in the objective function will count all customers at a node being
covered, if and only if the node is covered by a facility. The main reason for the use
of the covering variables xi is to ensure that customers who are covered by more than
one facility will not be counted more than once. The first constraint ensures that
exactly two facilities will be located. The remaining constraints each define the
coverage of a node. In particular, a node is considered covered, if there is at least one
facility within covering distance. In this example, consider the node n1. This node
could potentially be covered from a facility at node n1, n2, or n5. Thus, its covering
constraint is x1  y1 + y2 + y5, which ensures that if there is no facility at either n1, n2,
or n5, then the right-hand side value of the constraint equals zero, which forces the
variable x1 to assume a value of zero as well. On the other hand, if there exists at least
one variable at any one of the three nodes, the right-hand side value of the inequality
is at least one, which renders it redundant, as x1 is defined as a zero-one variable
anyway. However, while x1 could assume a value of either zero or one in such a case,
the objective function includes the term 120x1 which is part of what is to be
maximized. This pushes the value of x1 to as large a value as possible, so that it
will assume a value of “1” whenever possible.
The optimal solution of the problem is y5 ¼ y7 ¼ 1, meaning that we will locate
facilities at the nodes n5 and n7. All coverage variables except x3 equal one, and the
value of the objective function equals z ¼ 1270. In case of very large networks, it
may be difficult to solve the problem exactly, so, rather than attempting to find an
optimal solution, we may resort to a heuristic algorithm. The heuristic we use below
is of the Greedy type in that it locates one facility at a time and in each step it does so
by locating the next facility, so as to maximize the number of additional customers
that are covered in that step. Being a heuristic, the myopic Greedy procedure may
therefore not necessarily find an optimal solution.
7.2 Covering Problems 275The number of customers that are covered by a single facility located at one of the
nodes can then be determined by multiplying the coverage matrix by the vector of
weights from the left, i.e., by computing wC d
 . The jth component of the resulting
vector indicates the number of customers that will be covered if a facility were to be
located at the node nj. In our example, we obtain
½  120, 160, 100, 110, 130, 140, 190, 220, 200
110010000
111110000
011101001
011111000
110111100
001111010
000010111
000001110
001000101
2
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
4
3
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
5
¼ ½  410, 620, 710, 640, 850, 700, 740, 550, 490 :
The facility that serves most customers is one that is located at n5, from where it
covers 850 customers, so we locate a facility there. Note that this first facility is
located optimally. In order to facilitate the computations, we can most easily avoid
double counting by deleting the rows that belong to nodes that are already covered
by a facility at n5. In our example, these are the nodes n1, n2, n4, n5, n6, n7. Deleting
column n5 as well (as we have already located a facility there), the reduced coverage
matrix is
n1 n2 n3 n4 n6 n7 n8 n9
n3 01111001
n8 00001110
n9 00100101
Multiplying this matrix with the remaining vector of weights [100, 220, 200]
from the left, we obtain [0, 100, 300, 100, 320, 420, 220, 300], whose unique
maximum is in the position that belongs to n7, and we locate a facility at that site.
That way, we cover an additional 420 customers, and we can now delete the rows
that belong to nodes n8 and n9 as well as the column of n7. Since we have now
exhausted our resources, having located two facilities as required, customers at n3
will remain unserved and we have succeeded covering a total of 850 + 420 ¼ 1270
customers by locating the two facilities at nodes n5 and n7. This, as we happen to
know, is actually the optimal solution.
Typically, a construction heuristic such as Greedy will be followed by an
improvement heuristic. One such improvement heuristic is the swap-interchange
improvement heuristic. The idea is simple: in each step, one facility is removed from
its present location and relocated to a site, at which there presently is no facility. If
such a move increases coverage, it is accepted, otherwise, the search for better
solution continues until some stop criterion is satisfied.
276 7 Location ModelsWe will illustrate this procedure by using again the example shown above.
However, since the Greedy heuristic already found an optimal solution, we start
our demonstration with some nonoptimal solution. Suppose that we have located
facilities at nodes n3 and n6. The two facilities cover customers at the nodes n2, n3,
n4, n6, and n9 and n3, n4, n5, n6, and n8, respectively, so that all customers except
those at nodes n1 and n7 are covered.
We first try to remove a facility from its present location at n3 and move it to n1
instead. Such a move means that we lose coverage of customers at n2 and n9 for a
combined loss of 360 demand units, while the facility at the new location will cover
customers at n1, n2, and n5 (of whom customers at n5 were already covered by the
facility at n6), so that we gain 280 new customers. This indicates that our move
results in a net loss of 80 customers, so that the move is rejected.
Instead, we may try to relocate a facility from n3 to n2. A similar argument will
reveal the same loss of 80 customers, so this move is also rejected. Yet another
possibility is to relocate the facility from n3 to n4. This move results in a net loss of
200 customers, so again, the move is rejected.
Consider now the relocation of a facility from n3 to n5. Again, the loss of
customers due to the removal of a facility from n3 equals 360 customers, while the
relocation of the facility at n5 will cover the previously uncovered customers at n1,
n2, and n7 for a gain of 470 customers, so that there is a total net gain of
110 customers. This means that the move is accepted, so that the new solution has
facilities located at n5 and n6. This process is repeated until no further improvements
are possible. Again, the swap method is a heuristic and as such is not guaranteed to
find an optimal solution.
We would like to conclude this section by mentioning a number of possible
extensions of the basic covering models discussed above. One possibility to deal
with outliers (whose coverage is very expensive) is to attempt to cover at least a
certain proportion of the population within a prespecified distance or time. For
instance, one could attempt to locate fire stations that have 90% of the potential
customers within 8 min of the station.
Another important issue concerns congestion. Especially when resources are
scarce (e.g., ambulances), an important issue is what happens if a service call arrives
while the unit is presently busy. A possible way to deal with such situations is the
introduction of backup coverage. In other words, the decision maker may try not just
to cover each potential customer once but try to cover a certain proportion of
customers more than once. The obvious question then concerns the tradeoff between
primary coverage for some people versus backup coverage for others.
7.3 Center Problems
Another classical problem type in location analysis comprises the so-called center
problems. The basic idea of all center problems is that they locate facilities so as to
minimize the longest distance between any customer and his closest facility. One of
the problems associated with the concept of centers is their exclusive focus on the
customer with the longest facility–customer distance. This can lead to highly
7.3 Center Problems 277undesirable outcomes. In case of problems that involve multiple facilities, the center
objective minimizes the maximal distance between any customer and its nearest
facility.
Throughout this section, we assume again that all demand is clustered at the
nodes of the network (or at given points in the plane). We can then distinguish
between node centers, which are facility locations for which only the nodes are
considered, and absolute centers, in which case facilities may be located anywhere
on the network (including on arcs and not necessarily at nodes) or in the plane.
The need for this distinction is apparent by considering a graph with only a single
arc, at whose edges we have the nodes n1 and n2. Either node would serve as a single
node center (whose critical distance, i.e., the distance from the facility to the farthest
customer) equals the length of the arc. However, the point on the graph that
minimizes the maximal distance is at the center of the arc, from where the longest
distance to either customer is only half the length of the arc.
7.3.1 1-Center Problems
The simplest center problem is the 1-node center problem on a network. The
problem is to find a facility location, so as to minimize the maximal distance between
the facility and any of the customers. As an illustration, consider again the graph in
Fig. 7.1 in the previous section of this chapter.
Recall that the matrix of shortest distances was
n1 n2 n3 n4 n5 n6 n7 n8 n9
n1 0 5 11 14 6 12 14 20 18
n2 5 0 6 9 7 11 15 19 13
n3 11 6 0 9 11 5 16 13 7
n4 14 9 9 0 8 4 16 12 16
n5 6 7 11 8 0 6 8 14 17
n6 12 11 5 4 6 0 14 8 12
n7 14 15 16 16 8 14 0 7 9
n8 20 19 13 12 14 8 7 0 16
n9 18 13 7 16 17 12 9 16 0
Again, the rows symbolize the customers, while the columns stand for the
facilities. If we were to position a facility at node n1, then the distances between
the customers and the facility would be found in the first column. The longest such
distance is then the largest number in the first column, in our case 20.
A simple brute-force enumeration procedure now suggests itself for the determi￾nation of a 1-node center on graphs: tentatively locate a facility at a node, determine
the distance to the farthest customers, repeat for all potential facility locations, and
choose the location with the shortest distance. In other words, determine all column
maxima in the matrix, and chose the 1-center facility location as the column with the
minimal such distance. This will determine an optimal solution. In our example, the
278 7 Location Modelscolumn maxima are [20, 19, 16, 16, 17, 14, 16, 20, 18], so that the 1-node center is
located at node n6 and the longest facility–customer distance is 14.
Consider now the 1-absolute center problem in the plane with Manhattan
distances. This problem has a very simple closed-form solution. Assume that the
n customers are located at the points P1, P2, ..., Pn with coordinates (a1, b1), (a2, b2),
..., (an, bn), and that the facility will be located at a point with the coordinates (x, y).
Note that regardless of the number of customers, the problem has only two variables,
viz., x and y.
In order to solve the problem, we need to define five auxiliary variables. They are
α1 ¼ max
i f g ai þ bi
α2 ¼ max
i f g ai þ bi
α3 ¼ max
i f g ai  bi
α4 ¼ max
i f g ai  bi
α5 ¼ max f g ð Þ α1 þ α4 ,ð Þ α2 þ α3 :
A 1-absolute center is then located at (x, y) with coordinates.
x ¼ 1=2ð Þ α3  α4 and y ¼ 1=2ð Þ α3  α4 þ α5 with the longest distance z ¼ 1=2α5,
and, alternatively,
x ¼ 1=2ð Þ α1  α2 and y ¼ 1=2ð Þ α1 þ α2  α5 with longest distance z ¼ 1=2α5 , as
well as on every point on the line segment between these two solutions. For a simple
proof, see, e.g., Eiselt and Sandblom (2004).
As a numerical example, consider a problem with six customers, who are located
at the points P1: (0, 0), P2: (0, 3), P3: (1, 6), P4: (4, 5), P5: (4, 2), and P6: (5, 0). We
can then calculate
α1 ¼ max 0f g þ 0, 0 þ 3, 1 þ 6, 4 þ 5, 4 þ 2, 5 þ 0 ¼ 9,
α2 ¼ max f g 0 þ 0, 0 þ 3, 1 þ 6, 4 þ 5, 4 þ 2, 5 þ 0 ¼ 5,
α3 ¼ max 0f g  0, 0  3, 1  6, 4  5, 4  2, 5  0 ¼ 5,
α4 ¼ max f g 0  0, 0  3, 1  6, 4  5, 4  2, 5  0 ¼ 0, and
α5 ¼ max 9 f g ð Þ þ 0 , 5ð Þ þ 5 ¼ 10:
The 1-absolute center facility location is then at x ¼ 21=2 and y ¼ 21=2 , and,
alternatively, x ¼ 2, y ¼ 2, both with z ¼ 5.
The Manhattan distances between the individual customers and the facility are
5, 3, 5, 4, 2, and 5, respectively, in case the facility is located at (2½, 2½), and 4, 3,
5, 5, 2, 5, respectively, in case the facility is located at (2, 2). It is apparent that the
longest customer-facility distance equals 5 in both cases.
In case Euclidean distances are used, the 1-center in the plane has an interesting
geometric interpretation: it is at the center of the smallest enclosing circle. In Fig. 7.3
customers are shown as dots, while the center of the smallest enclosing circle is
shown as a star.
7.3 Center Problems 2797.3.2 p-Center Problems
This subsection considers node p-center problems in networks. The objective of p￾center problems is to locate a given number p of facilities, so that the longest distance
between any customer and his closest facility is as small as possible. While, from a
theoretical point of view, p-center problems are difficult, a simple bisection method
can be employed to solve p-center problems as a sequence of covering problems.
The method commences with an upper bound D, a lower bound D, and an initial
guess of the covering distance d between the lower and the upper bound. We then
determine the smallest number of facilities required to cover all nodes given d. The
model to do so is, of course, the location set covering problem which we dealt with in
the previous section. Suppose that the number of facilities required to cover all
customers is p d
 . If p d
  > p, then D has to be revised upward; if p d
   p, then D
might be decreased. This process continues until a covering distance is found that
cannot be reduced any further without requiring additional facilities.
The initial interval D, D   must be sufficiently large to include the optimal
solution. An obvious choice of the lower bound is D ¼ 0 , while D ¼
ð Þ n  1 max dij   with the maximum taken over all edge lengths dij in the network,
is an upper bound on the length of the longest path in the network. The reason is that
no path in the network can have more than (n  1) edges (otherwise it will include a
cycle), and the length of each edge is no more than that of the longest edge. Once
lower and upper bounds on the covering distance are determined, we apply a
bisection search. This process can be described as follows.
First, we bisect the present interval D, D   and choose a covering distance d in the
center of this interval. In particular, we choose d≔ ½ D þ D 	 
   . We then solve a
location set covering problem with the service standard d. The resulting number of
facilities required to cover all nodes is denoted by p d
 . If p d
   p, then we reduce
the upper bound of the interval D, D   to D≔d, otherwise we move up the lower part
of the interval to D≔d þ 1. Whenever the lower and upper bounds of the interval are
equal, we have found the optimal solution, in which case the last-solved covering
problem provides the location pattern of the facilities. As long as D 6¼ D , we
Fig. 7.3 Smallest enclosing
circle for given customer
locations
280 7 Location Modelsdetermine a new covering distance d on the basis of the interval D, D   with a new
lower or upper bound. Then a new covering distance d is computed as the center
point of this interval, and the procedure is repeated.
As a numerical illustration, consider the network in Fig. 7.4 and assume that p ¼ 3
facilities are to be located.
We initialize the computations with a lower bound of D ¼ 0 and an upper bound
of D ¼ 7 9ð Þ¼ 63. Bisecting the interval results in the trial value of d ¼ 31, for
which the set covering problem has a solution of p d
  ¼ pð Þ¼ 30 1, i.e., a single
facility, located anywhere, will cover all the nodes. As p(31) ¼ 1 < 3 ¼ p, we set
D≔31.
The results of the subsequent iterations are summarized in Table 7.1.
7.4 Median Problems
As opposed to the center problems with their minimax objective discussed in the
previous section, this section is devoted to median problems which have minisum
objectives. In other words, they will locate facilities, so as to minimize the sum of
distances to the customers. This feature makes this type of objective amenable to
applications in the public and the private sector. Consider, for instance, the location
of a public facility such as a library. The municipal planner will attempt to make the
library as accessible as possible to all of its potential patrons. This may be done by
minimizing the average distance between the library and its customers. It is not
difficult to demonstrate that as long as the magnitude of the demand remains
constant, minimizing the sum of facility-customer distances is the same as
minimizing the average facility-customer distance.
7.4.1 Minisum Problems in the Plane
Throughout this section, we assume that the task is to locate a single new facility
anywhere in the plane. The n customers are assumed to be located at points P1, P2,
Fig. 7.4 Network for
3-center problem
7.4 Median Problems 281..., Pn with coordinates (ai, bi) and their demand is denoted by the weight wi. The
task at hand is now to locate a facility, whose coordinates are the variables (x, y).
Note that regardless of the number of customers in the problem, the model has no
more than two variables. As discussed above, the objective is to minimize the
weighted sum of customer-facility distances, a proxy of the total cost of the trans￾portation. This type of problem (in the plane) is frequently referred to as a Weber
problem in reference to the work by the German economist-turned-sociologist
Weber (1868–1958) that culminated in the publication of his book in 1909.
The simplest problem occurs when rectilinear (Manhattan) distances are used.
The objective function is then separable, meaning that it is possible to optimize one
variable at a time without affecting the other. It turns out that the actual distances are
irrelevant, it is only important how the customers are located in relation to each
other, but not how far from each other. Actually, the procedure is very simple. We
first scan the customers from left to right (or right to left) along the abscissa and add
their weights, until the sum of weights for the first time matches or exceeds half of
the total weight. We then repeat the process by scanning the facilities and adding
their weights from top to bottom (or bottom to top), until again the reach or exceed
half the total weight for the first time. The combination of the two coordinates is the
location that minimizes the sum of weighted rectilinear distances.
In order to illustrate the procedure, consider twelve customers P1, P2, ..., P12,
whose locations are (a1, b1), (a2, b2), ..., (a12, b12), respectively, and their weights
are w1, w2, ..., w12, respectively. The numerical values of the coordinates and
weights are shown in Table 7.2.
The arrangement of customers can be visualized in Fig. 7.5.
First scan the abscissa. From left to right, customers can be rearranged as P2, then
P1 and P3, then P7, followed by P4 and P6, then P5 and P8, and so forth. Adding up
weights in that order results in 20 + (20 + 10) + 40 + (10 + 30) + (50 + 60) + 60 + ...
+ 80. Given that the total weight, i.e., the total demand of all customers, equals
500, we need to add up these values until, for the first time, they reach or exceed the
value of 500/2 ¼ 250. This happens for customer P12 with a total of 300, at the value
x ¼ 7. (The same result is achieved if we add up values starting from the right). This
procedure is now repeated for the ordinate by adding up weights from either bottom
to top or top to bottom. From the bottom, we add 20 + (10 + 60) + (20 + 50) + (10 +
Table 7.1 Bisection search to find a p-center solution in the example
Iteration # D D d p d
  Location
1 0 63 31 1 Anywhere
2 0 31 15 1 Anywhere
3 0 15 7 2 e.g., (n1,n8), or (n2,n7), or (n3, n6)
4 07 3 5 n2, n3, n4, then n7, and one of n1 and n6
5 47 5 2 (n1, n5), or (n1, n8)
6 4 5 4 3 e.g., n1 and n5, and one of n3 and n8
7 4 4 4 Stop, optimal.
282 7 Location Models50) + 30, at which point we have reached the value of 250. This occurs for customer
P6 at y ¼ 6, so that this is the optimal y-coordinate.
However, since the critical value of 250 was achieved exactly, there are alterna￾tive optimal solutions. If we had added up weights from top to bottom, we would
have achieved the value of 250 for customer P7 at the coordinate y ¼ 7, which is also
optimal. As a matter of fact, not only are the two points (x, y) ¼ (7, 6) and (7, 7)
optimal, all points with x ¼ 7 and y anywhere between 6 and 7 are optimal. The value
of the objective function can then be determined by computing the customer-facility
distances for all customers, multiplying them by the appropriate weights, and adding
them up. In our example, for the optimal location at (7, 6) we obtain the total
weighted distance 20(10) + 20(8) + 10(6) + 10(7) + 50(4) + 30(3) + 40(5) + 60
(5) + 70(4) + 80(7) + 50(3) + 60(4) ¼ 2510.
Consider now the same example as before, but with the weights of P5 and P12
interchanged, i.e., w5 ¼ 60 and w12 ¼ 50. Now the optimized location is between
5 and 7 on the abscissa, and between 6 and 7 on the ordinate, so that all locations in
the rectangle with the corners at (5, 6), (5, 7), (7, 7), and (7, 6) are optimal.
Another model works with the same scenario and also minimizes the sum of
weighted distances. However, it uses squared Euclidean distances instead of recti￾linear distances. Again, the objective function turns out to be separable, so that we
are able to optimize for our variables x and y separately. Since these variables are
Table 7.2 Locations and weights of customers in the example
Point Pj P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12
ai 2 1 2 4 5 4 3 5 8 10 9 7
bi 1 4 5 2 4 6 7 9 9 10 5 2
wi 20 20 10 10 50 30 40 60 70 80 50 60
Fig. 7.5 Location of customers
7.4 Median Problems 283continuous, we can take partial derivatives, set them equal to zero, and solve for the
variables x and y. This process results in an optimal solution in which the x￾coordinate of the facility equals the sum of weighted x-coordinates of the customers
divided by the total weight, and similar for the y-coordinates. The solution obtained
by using squared Euclidean distances is the center-of-gravity of the given points. As
a numerical illustration, consider again the above example with data shown in
Table 7.2. Here, we obtain the optimal facility location at x ¼ 2 20 ð Þþ1 20 ð Þþ⋯þ7 60 ð Þ
20þ20⋯þ60 ¼
3140=500 ¼ 6:28. The optimal y-coordinate is determined in a similar fashion as
y ¼ 3170=500 ¼ 6:34. Note the difference between this solution and that obtained
by using rectilinear distances: despite the strong pull of the customers with large
weights in the northeast corner, the solution that uses squared Euclidean distances
locates the facility more towards the west than the solution that uses rectilinear
distances. This is due to the fact that by squaring distances, long distances receive a
heavy emphasis, and the minimization function will try to avoid them. In that sense,
a minisum objective with squared Euclidean distances will find a solution that
includes features of the usual minisum, but also those of the minimax objective.
Using Euclidean distances, we can apply a similar approach. However, the
objective function is no longer separable. Setting partial derivatives to zero, we
obtain the optimality conditions
x ¼
Pn
i¼1
wiai ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
ð Þ aix 2
þð Þ biy 2 p
Pn
i¼1
wi ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
ð Þ aix 2
þð Þ biy 2 p
, and y ¼
Pn
i¼1
wibi ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
ð Þ aix 2
þð Þ biy 2 p
Pn
i¼1
wi ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
ð Þ aix 2
þð Þ biy 2 p
:
Notice that in these two relations, the variables x and y appear on the left-hand
sides as well as on the right-hand sides. A “trick” first devised by Weiszfeld in 1937
is to use these relations in an iterative procedure. It starts with a guess (x, y), inserts it
on the right-hand side of the equations, computes new values for x and y on the left￾hand side, uses these values on the right-hand side to compute new values on the left,
and so forth.
As a starting point, it may be a good idea to use the center-of-gravity, or the
solution given rectilinear distance. In order to demonstrate the Weiszfeld method,
consider the following numerical
Example There are three customers at the points P1, P2, and P3 with coordinates
(0, 0), (3, 0), and (0, 5), respectively. The demands of the customers are w1 ¼ 30,
w2 ¼ 20, and w3 ¼ 40. As a starting point, we use (x, y) ¼ (5, 5), even though this is
quite an unreasonable initial point. The reason is that the optimal solution will
always be located in the triangle formed by the customers, and our starting point is
not. We have chosen this point to demonstrate that normally—there are
counterexamples, though—this technique converges very quickly. Before starting,
note that using rectilinear distances in this example results in an optimal facility
284 7 Location Modelslocation at (0, 0), while squared Euclidean distances will locate the facility at (2=3,
22=9).
Given a starting point with coordinates x ¼ 5 and y ¼ 5, we compute the next trial
point at
x ¼
0 30 ð Þ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
ð Þ 05 2
þð Þ 05 2 p þ 3 20 ð Þ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
ð Þ 35 2
þð Þ 05 2 p þ 0 40 ð Þ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
ð Þ 05 2
þð Þ 55 2 p
30 ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
ð Þ 05 2
þð Þ 05 2 p þ 20 ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
ð Þ 35 2
þð Þ 05 2 p þ 40 ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
ð Þ 05 2
þð Þ 55 2 p  :6982,
and
y ¼
0 30 ð Þ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
ð Þ 05 2
þð Þ 05 2 p þ 0 20 ð Þ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
ð Þ 35 2
þð Þ 05 2 p þ 5 40 ð Þ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
ð Þ 05 2
þð Þ 55 2 p
30 ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
ð Þ 05 2
þð Þ 05 2 p þ 20 ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
ð Þ 35 2
þð Þ 05 2 p þ 40 ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
ð Þ 05 2
þð Þ 55 2 p  2:5068:
Given the new trial point (x, y) ¼ (0.6982, 2.5068)—which, incidentally, is quite
similar to the center-of-gravity determined earlier—we can then compute the next
trial point in similar fashion. It turns out to be (x, y) ¼ (0.5366, 2.3512), which is
quite close to the previous solution. This procedure terminates whenever some stop
criterion is satisfied, e.g., there is only a small change in the solution or the value of
the objective function.
A good tool that allows us to envisage the forces that determine the solution is the
Varignon frame named after the French mathematician Varignon, 1654–1722.
Imagine a board, in which holes have been drilled at the points at which customers
are located. One string is fed through each hole and all strings are tied together in one
knot. Below the board, weights are attached to the strings, such that the weights are
proportional to the demand of the respective customer. Given only gravity and in the
absence of friction, the knot will then settle at the optimal point. A graph of the
Varignon frame is shown in Fig. 7.6.
7.4.2 Minisum Problems in Networks
Consider now the location of a single new facility on a network. The facility location
that minimizes the sum of weighted customer–facility distances is usually referred to
as a 1-median. An obvious extension of the 1-median problem is the p-median
problem that locates p facilities so as to minimize the sum of distances between all
customers and their respective closest facilities. Given n customers in the network,
we assume that p < n; otherwise, the optimal solution locates a facility at each
customer site.
The main result we use is due to Hakimi (1964), who proved the following
7.4 Median Problems 285Theorem 6.1 At least one optimal solution of a p-median problem is located at
a node.
Given this result, we no longer have to search for an optimal solution anywhere
on an arc but can restrict ourselves to the nodes of the network. In that sense, this
theorem is reminiscent of Dantzig’s corner point theorem for linear programming
that also restricted the set of possible optimal locations from an infinite to a finite set.
First consider the case, in which the decision maker’s task is to determine a
1-median in a network. We assume that the matrix of shortest paths has already been
determined. Assuming that the matrix D ¼ (dij) includes the lengths of the shortest
paths between all pairs of nodes and given the vector w ¼ (wi) of customer demands,
we now use Hakimi’s theorem and compute the costs for all potential facility
locations at the nodes of the network. This can be accomplished by vector-matrix
multiplication. In other words, determine the vector wD, which includes in its j-th
element the total transportation cost (i.e., weighted distance) from all customers to a
facility located at node j. As an illustration consider the following
Example The graph in Fig. 7.7 includes the single-digit node-to-node (direct)
distances next to its edges and double-digit weights next to its nodes.
The symmetric matrix of shortest path distances is then
Fig. 7.6 Varignon frame
286 7 Location Modelsn1 n2 n3 n4 n5
n1 0535 5
D: n2 5 0 8 10 10
n3 3802 2
n4 5 10 2 0 4
n5 5 10 2 4 0
while the vector of demands is w ¼ [60, 40, 20, 50, 30]. Multiplying the vector
and the matrix results in wD ¼ [660; 1260; 660; 860; 940]. This means that if we
were to locate a facility at, say node n4, then our total transportation costs would be
860. Choosing the minimum in this cost vector reveals that we should either locate at
node n1 or at node n3; in both cases, the total transportation costs are 660. In order to
choose between the two alternatives, the decision maker may use secondary criteria.
Having shown how to locate a single facility, we now demonstrate how to locate a
given number p of facilities.
Arguably, p-median problems are among the most researched location models in
practice. Since they are difficult to solve, most large-scale problems will be solved
by heuristic algorithms. This section will describe one construction heuristic and two
improvement heuristics.
As usual, the Greedy heuristic works in sequential fashion. In the first stage, it
computes the weighted transportation costs for tentative locations at all nodes, and
then permanently locates the first facility at the node that allows the transportation at
the lowest cost. This is exactly the same step as that in the exact method that locates a
single facility, i.e., the 1-median on a network.
Suppose now that a number less than p of facilities have already been perma￾nently located, and the Greedy heuristic attempts to locate an additional facility. This
is accomplished by tentatively locating the new facility at one (presently unoccu￾pied) node at a time. Suppose we tentatively locate the new facility at node nj. The
method will then compute the shortest distance between each customer at node ni
and the closest facility that either exists already or is proposed (at nj). These distances
will be collected in the column vector dj. The weighted distance is then computed by
multiplying the weights and these distances, i.e., wdj. This number expresses the
Fig. 7.7 Network for p￾median example
7.4 Median Problems 287additional total transportation costs that are incurred if we locate the new facility at
node nj. This process is repeated for all possible tentative locations, and the mini￾mum is chosen. The location of the new facility is then made permanent, and the
process continues until the desired number of facilities has been located.
We will illustrate this procedure by a numerical.
Example Consider again the network in Fig. 7.7 and assume that the task is to
locate p ¼ 3 facilities, so as to minimize the total transportation costs. The matrix of
shortest distances D was already determined in the previous section, and so were the
vector of weights w and the cost vector wD ¼ [660; 1260; 660; 860; 940]. Again, we
choose to locate at either node n1 or at node n3. Arbitrarily choose node n1.
We now tentatively locate the second facility at the nodes n2, n3, n4, and n5. For
the trial location at the node n2, we now have facilities at n1 and n2, so that the
distances between a customer and the closest of our facilities are found by taking the
minima of columns 1 and 2 in the distance matrix. Here, we obtain d2 ¼ [0, 0, 3,
5, 5]T
, so that wd2 ¼ 460. For the trial location at n3, the shortest customer–facility
distances are found by computing the minimum among columns 1 and 3 in the
distance matrix D, resulting in d3 ¼ [0, 5, 0, 2, 2]T
, so that wd3 ¼ 360. Similarly, we
compute d4 ¼ [0, 5, 2, 0, 4]T with wd4 ¼ 360, and d5 ¼ [0, 5, 2, 4, 0]T with
wd5 ¼ 440. Among those trials, the lowest location costs are found at either node n3
or at node n4. Again, we arbitrarily break the tie and choose a facility location at n3.
We now have facilities permanently located at the nodes n1 and n3. In this
iteration, we tentatively locate facilities at n2, n4, and n5, one at a time. For a trial
location at n2, we have locations at n1, n2, and n3, so that we determine the shortest
customer-facility distances by computing the minima in the first three columns. This
results in d2 ¼ [0, 0, 0, 2, 2]T and wd2 ¼ 160. Similarly, the trial facilities at n4 and n5
result in distances and total transportation costs of d4 ¼ [0, 5, 0, 0, 2]T with
wd4 ¼ 260 and d5 ¼ [0, 5, 0, 2, 0]T with wd5 ¼ 300. The lowest costs are found
for the tentative location at n2, which is now made permanent. Since we have now
located all available facilities, the process terminates with facilities located at n1, n2,
and n3. The total transportation costs are then 160.
If we had broken the tie for the first facility in the same way and located at n1, but
chose n4 for the second facility, we would have ended up with facilities located at n1,
n2, and n4 for total costs of 140. On the other hand, if we had broken the tie for the
location of the first facility in favor of n3, we would have ended up with facilities
located at n1, n2, and n3 with total transportation costs of 160, the same location
pattern as that determined earlier. Notice that none of the tie-breaking rules has been
proved superior.
As usual, any construction heuristic should be followed by an application of an
improvement heuristic. In this section, we will describe two techniques. The first
heuristic we apply is the so-called location-allocation heuristic. Simply put, it
alternates between location and allocation steps. In particular, the technique is
initialized with any solution. The better the solution the procedure starts with, the
better the solution may be expected to turn out (although not necessarily so).
288 7 Location ModelsSuppose now that any facility location with the required p facilities has been
determined by eyeballing, an application of the greedy method, or any other
technique. The first step of the improvement heuristic is then the allocation phase.
In it, we simply assign each customer to its closest facility. This results in p clusters,
each with a single facility. In the location phase, we then consider one cluster at a
time, remove the facility from it, and determine an optimal facility location in it. This
new facility location may or may not be equal to the previous location of the facility.
This process is repeated for all clusters.
If there has been any change regarding the facility locations in the last step, the
procedure is repeated, until there are no further changes.
Example Consider again the above example and suppose that facilities have been
located at n3, n4, and n5. Allocating each customer to its closest facility results in
clusters {n1, n2, n3} for the facility at n3, {n4} for the facility at n4, and {n5} for
the facility at n5. The transportation costs for this initial location pattern are 500. The
location phase of the heuristic method begins by considering the first cluster. The
weights of the customers at n1, n2, and n3 are w* ¼ [60, 40, 20], and the partial
distance matrix for the three nodes is
n1 n2 n3
D*= n1 053
n2 508
n3 380
Computing the total costs for all three potential facility locations in this cluster
(i.e., determining a new single facility in this cluster as shown earlier in this section),
we obtain 260, 460, and 500, so that we choose the node n1 as the new facility
location in this cluster. The other two clusters include only one node each, so that
relocation is not possible. Thus the new location pattern includes facilities at the
nodes n1, n4, and n5 (which replaces the previous locations at n3, n4, and n5.)
Given these facility locations, we again assign customers to their closest open
facilities. The new clusters have customers {n1, n2} assigned to the facility at n1, {n3,
n4} assigned to the facility at n4, and {n5} assigned to the facility at n5. Note that
alternatively, the node n3 could have been assigned to the facility at node n5 instead.
The first cluster has weights w* ¼ [w1, w2] ¼ (60, 40) and with a distance matrix of
D ¼ 0 5
5 0   , we obtain costs of w*D* ¼ [200, 300], so that the facility will
remain at n1 with a partial cost of 200. The weight vector and distance matrix for the
second cluster are w* ¼ [w3, w4] ¼ [20, 50], and the distance matrix is D ¼
0 2
2 0  , so that the transportation costs for the two potential facility locations in
this cluster are w*D* ¼ [100, 40], meaning that the facility is again located at node
n4 with a partial cost of 40. The last cluster only includes a single node, so that the
7.4 Median Problems 289facility is located at n5 in that cluster with partial costs of 0. The total costs of this
location arrangement are then 200 + 40 + 0 ¼ 240.
These locations are the same as those in the previous iteration, indicating that the
location—allocation algorithm has converged. The solution has facilities located at
n1, n4, and n5 with total transportation costs of 240. Notice again that this solution is
not optimal.
The second heuristic we describe in this chapter is the so-called vertex substitu￾tion method, a technique that employs a simple “swap” step. Again, start with any
location arrangement of the required p facilities. In each iteration, the method
tentatively moves one of the facilities from its present location to an unassigned
location. If the swap reduces the total transportation costs, we have a new solution,
and the process continues. Otherwise, we continue with another pair of locations.
The process terminates, if no swap reduces the costs any further.
Example Consider again the above example and initialize the method with the
facilities again located at the nodes n3, n4, and n5. Moving a facility from n3 to n1
results in a cost reduction from 500 to 240, so the move is made, and the new
solution has facilities located at n1, n4, and n5. Moving a facility from n1 to n2 raises
the costs to 340, so the move is not made. Moving a facility from n1 to n3 increases
the costs to 500, and again, the move is not made. The move of a facility from n4 to
n2 leaves the cost at 240, so a tie-breaking rule must be used. Here, we keep the
facilities at their present locations. Moving a facility from n4 to n3 increases the cost
to 300, so the move is not made. Moving a facility from n5 to n2 decreases the costs
to 160, so we make the move and have a new location arrangement with facilities
located at nodes n1, n2, and n4. The procedure is repeated until no further
improvements are possible.
7.5 Other Location Problems
This section is designed to introduce some additional types of location models that
have been discussed in the literature. The first such model deals with undesirable
facilities. While it appears apparent that facilities such as sewage treatment plants,
landfills, power plants, or prisons are undesirable to have in the neighborhood of a
residential area, things are more subtle than that. As a matter of fact, just about all
facilities have desirable and undesirable features. Take, for instance, a hospital. Few
people will argue that it would be great to have such a facility nearby, the wailing
sirens of ambulances that can and will be turned on at any time of day or night will
surely be considered a nuisance. A similar argument applies to facilities such as
prisons: whereas few people would like to have them nearby (other than maybe to
visit relatives), the many employees who work in these facilities would not appreci￾ate having them at a great distance from their home.
Modeling of the location of undesirable facilities can be done in two different
ways. We either use the standard cost-minimizing objective and define “forbidden
regions” in which facilities cannot be located, or we use an objective function that
290 7 Location Modelspushes the facilities away from the customers, rather than pulling them towards them
as cost-minimizing objectives do. There are a number of problems related to both
approaches. Using forbidden regions in networks is easy: if it is not desirable or
permitted to locate a facility at a node, we simply do not define a location variable yj
for that node (or, equivalently, set the location variable to zero). The process is much
more complicated in the plane, where the forbidden regions must be defined as
system of linear or nonlinear inequalities. Using a “push” objective, i.e., an objective
that pushes facilities away from customers or population as opposed to pulling
facilities towards them, is not a straightforward process, either: first of all, if we
optimize on an unconstrained set, such an objective would attempt to push the
facilities towards infinity, which is obviously not reasonable. This means that it
will be required to define a feasible set, in which all facilities must be located. Again,
the objective that maximizes the weighted sum between customers and facilities will
tend to have the facilities locate at the border of the feasible set. A good example of
this behavior is the location of nuclear power plants in France, many of which have
been sited along the Rhine River, i.e., the border with Germany. Another problem is
that simply exchanging the “Min” for a “Max” in the objective function will not
suffice. The reason is that the usual cost-minimizing objective will automatically
assign a customer to his nearest facility, which is reasonable in almost all relevant
contexts. Similarly, a maximization objective, the objective will automatically
assign a customer to his farthest facility, which does not make sense in most cases:
customers are most affected by the nearest undesirable facility (as its effects will be
most pronounced on the customer), rather than the farthest facility. In order to ensure
that the effects of the closest facility are measured and counted towards the objective
(e.g., the overall pollution level on the population), additional constraints are
required. They result in fairly large formulations for even small problems, thus
often necessitating the use of heuristic algorithms.
Another strand of work deals with location models that have “equity” objectives.
The idea is to locate facilities, so as to make them as equally accessible to all
(potential) customers. Models of this nature have a variety of difficulties associated
with them. First of all, it is not obvious what “equity” is. Dictionaries will define it as
“fairness,” a concept just as vague. The location models in this category all deal with
equality, rather than equity. More precisely, they attempt to locate facilities, so as to
make the shortest customer-facility distances (i.e., the usual assignments) as equal as
possible. Many measures for equality have been described: the range (i.e., the
difference between the shortest and the longest of any customer-facility distances),
the variance of these distances, the Lorenz curve (a tool that economists use to
display income disparities), and the related Gini index. It is important to realize that
“equity” objectives should always be coupled with an efficiency objective, as
otherwise, they tend to result in degenerate solutions. As an example, consider two
customers located at the ends of a line segment, and a third customer who is just
below the center of that line segment. The optimal solution for any single-facility
location problem with equity objective has the facility located at the center of the
circle, on whose circumference all three customers lie. This point can be very far
away from the customers. Worse yet, as the facility moves closer to the customers
7.5 Other Location Problems 291(a move from which all customers would benefit, as they are all closer to the facility),
the solution is less equal and thus an equality objective will not make it. This is
clearly undesirable and can only be avoided if some efficiency objective is consid￾ered as well. The situation is shown in Fig. 7.8, where P1 is the facility location that
is equidistant to the customers C1, C2, and C3 (the solid black lines), while the
facility location P2 is closer to each of the customers, but no longer equidistant from
them (the broken lines).
Another active area of research concerns the location of hubs. Hubs are an
essential concept in a number of industries, most prominently the airline industry.
Typically, airline flights between an origin and a destination are routed through one
or two hubs. The inconvenience of having to change planes once or twice is
acceptable due to the fact that without hubs, many origin-destination pairs would
not permit any flights at all between them due to low traffic volume. The flight
volumes to the hubs (or concentrators) are, however, often sufficient to justify flights
to them. In addition, flights between (remote) origins or destinations and more
central hubs are typically done by small commuter planes, whose costs per
passenger-mile are typically considerably higher than the costs of larger airplane
that operate between hubs. This is the reason for inter-hub discounts, i.e., lower costs
between hubs. Hub location problems are difficult to solve exactly. The major reason
for the difficulties is the size of the problem. Typically, the formulation will use
binary variables of the type yiklj, which assume a value of one, if the traffic from
origin i to destination j is routed through hubs k and l. For example, in a network with
one hundred nodes, each of which is an origin, a destination, and a potential hub,
there would be a hundred million zero-one variables. Even considering today’s
powerful computing equipment, this is a very large problem.
Competitive location models were introduced by Hotelling in 1929. A mathemat￾ical statistician by trade, he considered the simplest competitive location problem
one can think of: two profit-maximizing duopolists locate one facility each on a line
segment. They offer a homogeneous good and compete in locations and prices.
Hotelling concluded that a situation, in which the duopolists would cluster together
at the center of the market, is stable. It took 50 years to prove him wrong, and it is
known today that there exists no stable solution in his original version of the
problem. Today, we consider two versions of competitive location models. In the
first class of problems, the main quest is to find, as Hotelling did, stable solutions.
Fig. 7.8 Problem with
equality objective
292 7 Location ModelsThese are called Nash equilibria. A Nash equilibrium is said to exist, if none of the
participants in the game has an incentive to unilaterally move out of the current
situation, which in this case means to change his location or price. The second class
of problem concerns von Stackelberg solutions. The economist von Stackelberg
considered two groups of participants: leaders and followers. The leader will choose
his actions (here: his location) first, knowing that a follower will locate later. Note
that the leader’s planning will require assumptions concerning the follower’s
objectives and perception. This is typically summarized in a reaction function,
which delineates the follower’s reaction to each of the leader’s courses of action.
In contrast, once the leader has made his decision, the follower only has to observe
what the leader has done and make his own decisions accordingly. It is apparent that
the follower’s problem is a conditional optimization problem (finding an optional
location for the follower’s facilities, given the leader’s facility locations), while the
leader’s problem is very difficult, as even the determination of the reaction function
requires the solution of a zero-one integer programming problem for each of the
leader’s possible courses of action.
Finally, consider extensive facilities and facility layout problems. In both areas,
the facilities can no longer be represented as points on a map, so that the sizes of the
facilities are no longer negligible in relation to the space they are located
in. Problems of this nature are much more difficult than “simple” location problems.
The main reason is that the shape of the facilities must now also be considered.
Typical examples of layout problems are the arrangements of workstations in an
office, the placement of rooms in a hospital (operating rooms, supply rooms,
recovery rooms, etc.), and the allocation of spaces in a shopping mall to stores.
The best-known facility layout model is the quadratic assignment problem. Gener￾ally speaking, the purpose of this problem is to assign items to empty slots.
Depending on the specific application, this may mean work functions to stations,
offices to empty rooms, or drill bits to slots on a drill. One way to formulate quadratic
assignment problems is to define binary variables yijkl, which assume a value of one,
if item i is assigned to slot j and item k is assigned to slot l, and zero otherwise.
(Notice the similarity to hub problems discussed above.) The advantage of this
formulation is its linear objective function, while the disadvantage is the very large
number of variables. Another formulation uses double-subscripted binary variables
yij, which equal one, if item i is assigned to slot j, and zero otherwise. This
formulation has much fewer variables, but its disadvantage is its quadratic objective
function (hence the name of the formulation). To this day, exact solutions for
quadratic assignment problems with more than about 30 items and slots remain
elusive.
Exercises
Problem 1 (a location set covering problem): An administrative district includes
18 small villages. One of the functions of the district officer is to ensure that each
community is reasonably well served in case of a fire. It was established that no
village should be farther than 8 min from its closest fire hall. The graph with the
villages and the distances (in minutes) between them is shown in the Fig. 7.9. All
villages must be covered.
7.5 Other Location Problems 293How many facilities will be needed to cover all customers? The covering matrix
is shown in Table 7.3.
Solution:
The formulation is
P : Min z ¼ y1 þ y2 þ y3 þ y4 þ y5 þ y6 þ y7 þ y8 þ y9 þ y10 þ y11 þ y12
þy13 þ y14 þ y15 þ y16 þ y17 þ y18
s:t: y1 þ y2 þ y3 þ y7 þ y8 þ y13  1
y1 þ y2 þ y3 þ y7 þ y8 þ y9  1
y1 þ y2 þ y3 þ y4 þ y8 þ y9 þ y10  1
y3 þ y4 þ y5 þ y9 þ y10 þ y11  1
y4 þ y5 þ y6 þ y10 þ y11 þ y17  1
y5 þ y6 þ y11 þ y12  1
y1 þ y2 þ y7 þ y8 þ y13 þ y14  1
y1 þ y2 þ y3 þ y7 þ y8 þ y9 þ y13 þ y14  1
y2 þ y3 þ y4 þ y8 þ y9 þ y10 þ y11 þ y15  1
y3 þ y4 þ y5 þ y9 þ y10 þ y11 þ y15 þ y16 þ y17  1
y4 þ y5 þ y6 þ y9 þ y10 þ y11 þ y12 þ y16 þ y17 þ y18  1
y6 þ y11 þ y12 þ y18  1
y1 þ y7 þ y8 þ y13 þ y14 þ y15  1
y7 þ y8 þ y13 þ y14 þ y15 þ y16  1
y9 þ y10 þ y13 þ y14 þ y15 þ y16 þ y17  1
y10 þ y11 þ y14 þ y15 þ y16 þ y17  1
y5 þ y10 þ y11 þ y15 þ y16 þ y17 þ y18  1
y11 þ y12 þ y17 þ y18  1
y j ¼ 0 or 1 for all j
The optimal solution locates facilities at nodes (n8, n9, n11), (n8, n10, n11), (n8, n10,
n12), or (n8, n11, n13). In all cases, three facilities are sufficient to cover all customers
as required.
2 6 4 6 4 
6 4 5 3 7 
6 2 2 5 6 
5 4 3 4 7 3 
2 7 8 6 2 4 
n1 n2 n3 n4 n5 n6 
n7 n8 n9 n10 n11 n12 
n13 n14 n15 n16 n17 n18 
Fig. 7.9 Graph for Problem 1
294 7 Location ModelsTable 7.3 Coverage matrix tor Problem 1
n1 n2 n3 n4 n5 n6 n7 n8 n9 n10 n11 n12 n13 n14 n15 n16 n17 n18
n1 1110001100 0 0 100000
n2 1110001110 0 0 000000
n3 1111000111 0 0 000000
n4 0011100011 1 0 000000
n5 0001110001 1 0 000010
n6 0000110000 1 1 000000
n7 1100001100 0 0 110000
n8 1110001110 0 0 110000
n9 0111000111 1 0 001000
n10 0011100011 1 0 001110
n11 0001110011 1 1 000111
n12 0000010000 1 1 000001
n13 1000001100 0 0 111000
n14 0000001100 0 0 111100
n15 0000000011 0 0 111110
n16 0000000001 1 0 011110
n17 0000100001 1 0 001111
n18 0000000000 1 1 000011
7.5 Other Location Problems 295Problem 2 (a maximal covering location problem, heuristics): A small rural
district has the task of locating health posts in the country to serve remove villages. A
village is said to be covered, if it is within ten miles of the health post. The district
administration can pay for no more than two health posts. The geographical layout of
the villages is shown in Fig. 7.10, in which the single-digit numbers indicate the
distances between the villages, and the double-digit numbers are the populations of
the villages.
The covering matrix is then as follows:
n1 n2 n3 n4 n5 n6 n7 n8 n9 n10 n11 n12
n1 1110110000 0 0
n2 1110111000 0 0
n3 1111001100 1 1
n4 0011001100 0 0
n5 1100110010 0 0
n6 1100111111 1 0
n7 0111011101 1 1
n8 0011011100 1 1
n9 0000110011 0 0
n10 0000011011 1 1
n11 0010011101 1 1
n12 0010001101 1 1
(a) Use the Greedy heuristic to locate the two health posts, so as to maximize the
benefit of the health posts to the people, i.e., cover as many people as possible.
(b) Demonstrate the Swap technique by exchanging the facility that was located first
with two other facilities, one at a time. (Choose the facilities with the smallest
subscripts). What are the new coverages, and would you make either of the
swaps permanent?
3 5 6 
4 7 3 
5 6 4 
5 6 4 8 
6 9 2 5 
n1 n2 n3 n4 
n5 n6 n7 n8 
n9 n10 n11 n12 
20 60 80 50 
80 20 60 50 
90 50 40 90 
Fig. 7.10 Village network
for Problem 2
296 7 Location ModelsSolution: (a) Premultiplying the coverage matrix by the vector of weights, we
obtain the number of covered customers if a facility were to be located at node nj as:
n1 n2 n3 n4 n5 n6 n7 n8 n9 n10 n11 n12
260 320 450 240 270 470 500 390 240 350 390 370
"max
Therefore, we locate the first facility at n7. In the next step, we determine:
n1 n2 n3 n4 n5 n6 n7 n8 n9 n10 n11 n12
100 100 20 0 190 190 – 0 170 90 0 0
"max "max
The second facility is then located at either n5 or n6. Consequently, the facilities
should be located at n5 and n7 or at n6 and n7. The total coverage is 690.
(b) Temporarily swap n7 and n1. The new solution has then facilities at n1, n5
(or n1, n6) and it will cover 350 (or 550) customers, so that there will be no
permanent swap.
Temporarily swap n7 and n2, and the new solution has facilities at n2 and n5 (or n2
and n6). It covers 410 (or 550) customers, so again, there will be no permanent swap.
Problem 3 (1-median and 1-center on a network): Industrial customers have
contracted demands for heat pumps. These units are to be delivered from the
warehouse of a central supplier to the companies. The supplier is now attempting
to locate the warehouse, so as to minimize the transportation cost of the pumps to its
customers. The demand is fairly constant throughout the year. The delivery is per
pickup truck, one heat pump at a time, resulting in a linear cost function. Figure 7.11
shows the supplier’s customers, their double-digit demands, and the single-digit
distances between the customers.
A consultant of the supplier had suggested locating the warehouse on the link
between the nodes n2 and n3 at a distance of 5.4 from n2. They have based their
argument on the large weights of the adjacent nodes n2 and n3 that provide a strong
pull to locate the warehouse there.
(a) Without any calculations, do you agree with the consultant’s recommendation?
(b) Find a location on the network that minimizes the total delivery cost. How much
more expensive was the consultant’s recommendation?
(c) Ignore now the weights at the nodes and assume that the same graph were to be
used by some planner to locate a vertex 1-center. Where would this center be
located?
7.5 Other Location Problems 297Solution:
(a) No. This is a 1-median problem and there is more to a solution than just weight.
(b)
w ¼ ½  40, 60, 90, 30, 20 , D ¼
0 3 12 7 4
30982
12 9 0 5 11
78506
4 2 11 6 0
2
6
6
6
6
6
6
4
3
7
7
7
7
7
7
5
:
Then wD ¼ [1550; 1210; 1390; 1330; 1450], so that node n2 is the optimal location
and the total transportation costs are 1210. The consultant’s recommendation is
[40, 60, 90, 30, 20] [8.4, 5.4, 3.6, 8.6, 7.4]T ¼ 1390. The costs are about 15%
higher than at optimum.
(c) The question is to determine a 1-node center. The column maxima are [12, 9, 12,
8, 11] with 8 being the minimum, so that node 4 will be the optimal location.
Problem 4 (1-node center in the plane with Manhattan distances): A rural
district is planning the location of its fire station. The station will have to serve seven
villages. The coordinates of the villages are (0, 0), (6, 1), (2, 3), (8, 3), (0, 4), (4, 5),
and (3, 7). As the people in the district are fierce defenders of property rights, the
roads were constructed parallel to the rectangular fields.
(a) What is the optimal minimax location of the fire station?
(b) Suppose that it takes the fire truck and its crew 3 min to get ready after an
emergency call, 2 min to drive one distance unit, and another 3 min to get the
hoses and pumps going. Given the solution determined under (a), how much
time elapses between the phone call and the beginning of the firefighting action
at the site of the most distant customer? How long does it take on average?
3 
7 
5 
9 
4 2 
6 
40 60 
30 90 
20 
n1 n2 
n4 n3 
n5 
Fig. 7.11 Network for
Problem 3
298 7 Location ModelsSolution:
(a)
α1 ¼ max 0, 7, 5, 11, 4, 9, 10 f g ¼ 11,
α2 ¼ max 0, f g 5, 1, 5, 4, 1, 4 ¼ 4,
α3 ¼ max 0, 5, f g 1, 5, 4, 1, 4 ¼ 5, and
α4 ¼ max 0, f g 7, 5, 11, 4, 9, 10 ¼ 0, so that
α5 ¼ max 11, 9 f g ¼ 11:
Hence the optimal coordinates of the facility are
x ¼ 1=2ð Þ¼ 5 21=2 and y ¼ 1=2ð Þ¼ 6 3, as well as
x ¼ 1=2ð Þ¼ 7 31=2 and y ¼ 1=2ð Þ¼ 4 2,
both with z ¼ 1=2ð Þ¼ 11 51=2.
(b) Worst case: 6 + 2(5½) ¼ 17 min. Average case ¼ 15 min in case of the first, and
14.14 min in case of the second optimal solution.
Problem 5 (p-node center in a graph): Consider again Fig. 7.11 and ignore the
weights at the nodes. For p ¼ 2, determine the vertex p-center with the bisection
search algorithm.
Solution:
D ¼ ð Þ n  1 max dij   ¼ 4 9ð Þ¼ 36, D ¼ 0:
d ¼ 18. Set cover: p(18) ¼ 1, locate anywhere. Set D ¼ 18.
d ¼ 9. Set cover: p(9) ¼ 1 with facility at node n2 or n4. Set D ¼ 9.
d ¼ 4. Set cover: p(4) ¼ 3 with facilities at nodes n4, n3, and one of n1, n2, and n5.
Set D ¼ 5
d ¼ 7. Set cover: p(7) ¼ 2, with facilities at, e.g., n1 and n3, or n3 and n5.
Set D ¼ 7
d ¼ 6. Set cover: p(6) ¼ 2, with facilities at, e.g., n1 and either n3 or n4.
Set D ¼ 6
d ¼ 5. Set cover: p(5) ¼ 2 with facilities at e.g., n1 and either n3 or n4. Set D ¼ 5.
Now D ¼ 5 ¼ D, Stop.
Problem 6 (1-median problem in the plane with Manhattan distances):
Customers are located at (0, 0), (7, 0), (6, 3), (5, 2), (5, 7), (3, 4), (6, 6), and (2, 8),
respectively, with weights 20, 40, 30, 50, 10, 60, 70, and 80, respectively.
Given Manhattan distances, determine the point that minimizes the sum of
distances to a single new facility. What are the total weighted costs from the new
facility to the customers?
7.5 Other Location Problems 299Solution: The sum of weights equals 360, so that the location of the facility
should be at (5, 4), with distances to the customers of 9, 6, 2, 2, 3, 2, 3, and 7, so that
the weighted sum (total costs) equals 1500.
Problem 7 (center-of-gravity and Weiszfeld method): Customers are located at
(0, 0), (6, 0), and (10, 5) with weights 4, 7, and 2, respectively.
(a) Calculate the center-of-gravity, i.e., the optimal solution of the minisum location
problem with squared Euclidean distances.
(b) Start with an initial guess of (1, 1) and perform one iteration with Weiszfeld’s
method to determine the 1-median.
Solution: (a) center-of-gravity is at (62/13, 10/13)  (4.77, 0.77).
(b)
x ¼
0ffiffi
2
p þ 42ffiffiffiffi
26 p þ 20ffiffiffiffi
97 p
4ffiffi
2
p þ 7ffiffiffiffi
26 p þ 2ffiffiffiffi
97 p ¼ 10:2676
4:4043 ¼ 2:3313, y ¼
0ffiffi
2
p þ 0ffiffiffiffi
26 p þ 10ffiffiffiffi
97 p
4ffiffi
2
p þ 7ffiffiffiffi
26 p þ 2ffiffiffiffi
97 p ¼ 1:0153
4:4043 ¼ 0:2305:
Problem 8 (p-median with Greedy and vertex substitution heuristics): Con￾sider the undirected graph shown in Fig. 7.12.
Assume that the customers n1, n2, ..., n8 have demands of 38, 25, 13, 18, 15, 21,
32, and 40, respectively. Suppose now that the coverage distance is d ¼ 4.
(a) Set up the capture table and apply the Greedy heuristic to locate three facilities.
Where should the facilities be located, and what is the total capture?
(b) Use the vertex substitution heuristic to improve the solution.
Solution:
(a)
Potential facility location n1 n2 n3 n4 n5 n6 n7 n8
Capture 38 124 88 88 61 61 128 72
Locate one facility at n7. The next captures are then
Potential facility location n1 n2 n3 n4 n5 n6 n8
Capture 38 36 0 0 36 36 0
6 3 1 
5 
3 
4 4 
6 
2 2 7 
3 
4 
n1 n2 n3 n4
n5 n6 n7 n8
Fig. 7.12 Graph for
Problem 8
300 7 Location ModelsLocate one facility at n1. The next captures are then.
Potential facility location n2 n3 n4 n5 n6 n8
Capture 36 0 0 36 36 0
Locate one facility at either n2, n5 or at n6. Hence the solution locates facilities at
either (n7, n1, n2), at (n7, n1, n5), or at (n7, n1, n6) with a total capture of 202. The
facilities capture everything, so there can be no better solution.
(b) Given the solution determined in (a), an improvement is not possible since all
customers are covered by the 3 facilities. Using the vertex substitution procedure for
practice anyway, starting with facilities located at, say, (n1, n5, n6), we presently
capture customers at nodes n1, n2, n5, and n6, i.e., a total of 99 customers. Exchang￾ing n1 and n3, the new solution has facilities at (n3, n5, n6) and we capture customers
at nodes n2, n3, n4, n5, and n6, i.e., a total of 92 customers. This solution is worse than
its predecessor and it is thus rejected. Now exchange n5 and n8. The new solution
locates facilities at n1, n6, and n8. These facilities capture customers at nodes n1, n2,
n5, n6, n7, and n8, i.e., a total of 171 customers. This is an improvement, and it thus
becomes our new solution. The process then continues from here.
Problem 9 (p-median in a network with Greedy, location-allocation heuris￾tic): Consider the graph in Fig. 7.13.
(a) Determine the 2-median by using the Greedy heuristic.
(b) Improve the solution found under (a) by the location-allocation heuristic.
Solution:
(a) The weights are w ¼ [60, 20, 40, 70, 40, 10] and the distance matrix is
D ¼
02 6 8 54
2 0 4 10 7 3
6 4 0 8 11 2
8 10 8 0 3 7
5 7 11 3 0 9
43 2 7 90
2
6
6
6
6
6
6
6
6
4
3
7
7
7
7
7
7
7
7
5
:
Locate the first facility: wD ¼ [1080, 1290, 1460, 1190, 1180, 1230], so
that the first facility is located at n1 at a minimal total cost of 1080.
For the second facility, we obtain costs of [, 950, 820, 440, 530, 810], so
that the second facility will be located at n4 for a total z-value of 440.
(b) Allocation: To the facility at n1, we allocated customers at nodes n1, n2, n3,
and n6, and to the facility at n4, we allocate the customers at n4 and n5. The
optimization on the two problems then results in cost vectors [320, 310, 460, 380]
with the minimum occurring at n2, and [120, 210] with the minimum at n4. The next
step then allocates to the facility at n2 the customers at n1, n2, n3, and n5, to the
facility at n4, the customers at n4 and n5, which is the same as before. Hence the
7.5 Other Location Problems 301method has converged with an optimal solution with facilities at n2 and n4 with a
total cost of 310 + 120 ¼ 430.
References
Daskin MS (2013) Network and discrete location: models, algorithms, and applications, 2nd edn.
Wiley, Hoboken, NJ
Eiselt HA, Marianov V (eds) (2011) Foundations of location analysis. Springer, New York
Eiselt HA, Sandblom C-L (2004) Decision analysis, location models, and scheduling problems.
Springer, Berlin
Hakimi S (1964) Optimum location of switching centers and the absolute centers and medians of a
graph. Oper Res 12:450–459
Laporte G, Nickel S, Saldanha da Gama F (eds) (2019) Location science, 2nd edn. Springer Nature,
Cham, Switzerland
2 4 
8 
3 
5 
4 2 
3 
7 
20 
60 
40 70 
10 40 
n1
n2
n3
n5 n4
n6
Fig. 7.13 Graph for
Problem 9
302 7 Location ModelsProject Networks 8
Back in the days when projects were dealt with by a single individual or a group of
workers working sequentially, there was no need for project networks. As an
example, consider the construction of a house. Somewhat simplistically, assume
that a single individual wants to build a log cabin. He will first dig a hole in the
ground for the foundation, then pour the cement, then lay the logs one by one, and so
forth. Each job is completely finished before the next task begins. This is a sequential
plan, and there is very little that can be done as far as planning is concerned.
Consider, however, some of the issues that have arisen as a result of the division
of labor. Nowadays, the plumbers can work at the same time the electrician does, but
not before the walls have been established, which is also required for the roof to be
put up. Given these interdependencies, planning is necessary in case time is an issue.
Clearly, while it is possible that, say, electricians and plumbers can work in the
building at the same time, it is not necessary to use this parallelism: we can still have
the two contractors work one after the other if we so wish. The project will take
longer, but it is possible.
Project networks were designed by a number of firms in the 1950s. The so-called
critical path method (or CPM for short) was developed by du Pont de Nemours and
the Remington Rand Univac corporations for construction projects. At roughly the
same time, the US Navy, in conjunction with the Lockheed Aircraft Corporation and
the consulting firm of Booz, Allen, and Hamilton devised the Program Evaluation
and Review Technique (PERT) for their Polaris missile program. Even though CPM
and PERT have completely independent backgrounds, today, we can consider them
very close brothers: their underlying ideas are identical, the resulting networks are
identical, and the only difference is that CPM is a deterministic technique, PERT is
(partially) stochastic.
This chapter is organized as follows. The first section will introduce the elements
of the critical path method, demonstrate its graphical representation, and describe
basic planning with the critical path method. The remaining sections of this chapter
deal with extensions of the basic concept: the second section allows the acceleration
of the project (a process that introduces costs into the model), the third section allows
# The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
H. A. Eiselt, C.-L. Sandblom, Operations Research, Springer Texts in Business and
Economics, https://doi.org/10.1007/978-3-030-97162-5_8
303resources to be used (which, with the obvious limitations, results in an optimization
model that allocates the scarce resources), and the final section of this chapter
discusses the probabilistic PERT method.
8.1 The Critical Path Method
Before getting into details, the planner will have to decide on what level the planning
will take place. We may be interested in planning on the macro level in order to get
the “bigger picture.” As an example, when building a house, we may have “founda￾tion work” as a single task, another one is “electrical work,” another is “plumbing,”
another is “roofing,” and so forth. Zooming in closer, we may look at each such task
as a project in itself. For instance, plumbing may include subtasks such as the
installation of pipes, connections to outside lines or septic systems, etc. Zooming
in even further, the installation of lavatories may be further subdivided into the
mounting of the washbasin, the connection of the faucet to the pipes, and so forth.
We can construct project networks on each of these levels.
Once the level has been decided upon, the task in its entirety—the building of the
house, design and planting of a public or private garden, an individual’s university
studies—will be referred to as a project. Each project can now be subdivided into
individual tasks or activities. Planting a bed of grape hyacinths, installing the
flashing on the roof, or taking a specific course in a university program are typical
examples of such activities. Associated with each activity is a duration that indicates
how long it takes to complete the activity. In CPM, the durations of all activities are
assumed to be known with certainty (which is the only distinction to PERT, where
the durations have underlying probability distributions).
In addition to the activities and their durations, we also need to have precedence
relations. Such relations indicate which activities must be completely finished before
another activity can take place. For example, in order to start the activity “drive with
the car to grandmother,” the activities “gas up the car,” “pack the gifts (wine and
cheese) for grandmother,” and “lock the house” must be completely finished. A
complete set of precedence relations will specify the (immediate) predecessors of
each activity.
In the analysis in this section, we are only concerned about time. There is no
optimization that takes place here, the goal is to determine the earliest time by which
the project can be completed. And while making these calculations, we can also find
out when each task can and must be started and finished. That allows us to determine
bottleneck activities in the project, whose delay will delay the entire project. In
subsequent sections, we will include other components in the basic network, includ￾ing money and other resources.
One of the huge advantages of project networks and one of the reasons for their
popularity among users is the ease with which they can be understood and
visualized. Following the old adage that “a manager would much rather live with a
problem he cannot solve than with a solution he cannot understand,” many managers
have adopted project networks as a standard tool in their toolkit. This was helped
304 8 Project Networkstremendously by a change in the representation that was made some time during the
1990s. Traditionally, each activity was represented by an arc in a network, while
nodes represented events (which are no more than the beginning and/or the end of
activities). Such a representation is referred to as an activity-on-arc (AOA) represen￾tation. While AOA networks have some advantages, they are rather difficult to
construct and may require a lot of artificial activities, called dummy activities. A
more modern way to visualize relations between activities is the activity-on-node
(AON) representation which we will use in this chapter. In it, each node represents
an activity, while the arcs represent the precedence relations. In addition to the given
activities, each project network contains two artificial nodes ns and nt, which are the
starting node (the source) and the terminal node (the sink), which symbolize the
beginning and the end of the project. The project start has arcs leading out of it to all
activities that do not have any predecessors, while all nodes that represent activities
without successors will have arcs leading directly to the terminal node “Project
End.” For all other activities j, we introduce an arc from node i to node j, if activity
i is an immediate predecessor of j.
As a numerical illustration, consider the precedence relations shown in Table 8.1.
The AON network that includes all of the activities and precedence relations is
shown in Fig. 8.1. Like all AON project networks, it has a unique start ns, a unique
end nt, all arcs are directed, and no cycles can possibly exist, as they would require
an activity to be completely finished before it can actually start, which is obviously
impossible.
As an example for (nested) precedence relations, consider activity F. It is clear
from the project network that the activities C, D, and E must be completely finished
Table 8.1 Precedence
relations for sample
network
Activity Immediate predecessor Duration (in weeks)
A – 5
B A 3
C A, B 7
D B 4
E B, C 6
F C, D, E 4
G D 2
H F, G 9
I F, G 6
J I 2
Fig. 8.1 Activity-on-node
project network
8.1 The Critical Path Method 305before activity F can commence. However, activity C requires that A and B are
completely finished, activity D requires that B is finished, and activity E requires that
B and C are completely finished. So, in reality activity F cannot commence before
the activities A, B, C, D, and E are all finished. This does not present a difficulty, but
users should be aware of these implicit precedence relations. Incidentally, while it is
not necessary to specify that activity C is a direct predecessor of F since E must
precede F anyway, and C precedes E, its inclusion does not introduce an error.
We are now interested in finding the earliest possible time at which the entire
project can be completed. For computational convenience, we assume that the
project start occurs at time t ¼ 0 and the end of the project occurs at time T, which
will be determined in the process. Consider now one of the many (14 in this case, to
be precise) paths of the network that lead from ns to nt, say ns – A – C – E – F – H –
nt. Starting at the end and moving back towards the beginning one step at a time, we
notice that nt can only finish if H is completely finished, which, in turn, requires that
F is completely done, which, in turn, requires that E is fully done, etc. The length of
any path from source to sink is defined as the sum of durations of all of its activities.
In this example, the path we are presently looking at has the length of
5+7+6+4+9 ¼ 31. This also means that while there are other precedence
relations, the ones on this path result in a project duration of 31, which is thus a lower
bound on the project duration. From this, we can conclude that the project duration is
equal to the length of the longest path in the network.
While it would certainly be possible to determine the longest path in a network by
enumeration (even though there will be a lot of paths), a different type of computa￾tion is preferred as it will provide decision makers with additional and very valuable
information. This procedure will be described in the following.
The first part of the procedure uses what is commonly known as a forward pass, a
forward sweep, or a forward recursion. In the forward recursion, we compute the
earliest possible starting times (ES) of all activities, as well as their earliest possible
finishing times (EF). We start labeling the nodes in the forward pass with the source
node. The earliest starting time of the source node is arbitrarily set to ES(ns) ¼ 0. For
any node ni the earliest possible finishing time EF equals the earliest possible starting
time ES plus the duration of the activity ni, which we will denote by di. For the
source, this results in EF(ns) ¼ 0. In order to continue labeling (which in the forward
pass means assigning values ES and EF to a node), we will use the following
Rule 1: In the forward pass, a node can only be labeled if all of its predecessors
have been labeled.
If more than one node satisfies the condition in Rule 1, labeling continues with
any of the nodes that satisfy the condition. The label ES of a node is then the
maximum among all EF labels of its direct predecessors. The reason for this
calculation is easily explained. Suppose that an activity has three direct predecessors
with the earliest possible finishing times of 4, 7, and 9, respectively. It is apparent
that we cannot start before 4, as none of the predecessors has been finished. Between
time 4 and 7, the first predecessor is finished, but we will have to wait until all
predecessors have been finished, which is the case at time 9.
306 8 Project NetworksApplying this rule to our example means that, once ns has been labeled, we can
now label node A. Node A has only one predecessor, so that ES(A) ¼ EF(ns) ¼ 0.
Given that, we can calculate EF(A) ¼ ES(A) + dA ¼ 0+5 ¼ 5. Now that node A is
labeled, we can continue labeling with nodes B and C. Arbitrarily choose node B.
Activity B also has only one predecessor, which is A. Consequently, ES(B) ¼
EF (A) ¼ 5, and EF(B) ¼ ES(B) + dB ¼ 5+3 ¼ 8. Now that activity B has been
labeled, we can continue labeling node C. As activity C has activities A and B as
predecessors, we have ES(C) ¼ max {(EF(A), EF(B)} ¼ max {5, 8} ¼ 8, so that
EF(C) ¼ ES(C) + dC ¼ 8+7 ¼ 15. Labeling can now continue with nodes D and E.
The results of the forward labeling phase are summarized in Table 8.2.
At this point, the sink nt has been labeled, and the ES and EF labels of the sink
(which are necessary equal, as the artificial activity nt has zero duration), indicate the
minimal total duration of the project. In other words, we now know that the project
can be completed at the earliest in T ¼ 34 weeks. This terminates the forward sweep.
In the backward sweep, we will compute the latest allowable finishing times LF
and the latest allowable starting times LS of all activities (in that order). By “latest
allowable,” we refer to the time we can start or finish an activity, given that the
project has to be completed by time T (in our example T ¼ 34). While the forward
sweep started with the source being labeled first, the backward sweep commences by
labeling the sink. Now we consider a node labeled, if we have computed its LF and
LS values. As the project duration T is now known, we will label the sink node
LF(nt) ¼ LS(nt) ¼ T. The rule for labeling other nodes is now.
Rule 2: In the backward pass, a node can only be labeled if all of its successors
have been labeled.
With the sink as the only labeled node, only the activities H and J can be labeled.
Arbitrarily choose node H. This node has only one successor, so that LF(H) ¼
LS (nt) ¼ 34. Given that activity H must be finished no later than 34 and its duration
is dH ¼ 9, we determine that LS(H) ¼ LF(H)  dH ¼ 34  9 ¼ 25. The process for
node J is similar, and we obtain LF(J) ¼ LS(nt) ¼ 34, and LS(J) ¼ LF (J)  dJ ¼
34  2 ¼ 32. At this point, the nodes nt, H and J are labeled, so that we can continue
labeling only with node I (as node G has node I as a successor, which is not yet
labeled). Node I has again only one successor, so that its label can easily be
calculated as LF(I) ¼ LS(J) ¼ 32 and LS(I) ¼ LF(I)  dI ¼ 32  6 ¼ 26. At this
point, we can continue labeling the nodes G and F. Arbitrarily choose F. Node F has
activities H and I as successors. In order to determine its latest finishing time,
consider this. As we have just computed, its two successors H and I cannot start
any later than 25 and 26, respectively. If activity F were to finish any later than, say,
Table 8.2 The earliest possible starting and finishing times
Activity ns ABC D E F G H I J nt
Earliest possible
starting time ES
0 0 5 8 8 15 21 12 25 25 31 34
Earliest possible
finishing time EF
0 5 8 15 12 21 25 14 34 31 33 34
8.1 The Critical Path Method 30725½, activity H could not start on time. In other words, in order to avoid delaying
the entire project it is necessary that all activities can start on time, so that
LF (F) ¼ min {LS(H), LS(I)} ¼ min {25, 26} ¼ 25. The resulting latest allowable
starting time is then LS(F) ¼ LF(F)  dF ¼ 25  4 ¼ 21. The next node to be labeled
is G. Since its successors are I and H, we have LF(G) ¼ min {LS(I), LS (H)} ¼
min {26, 25} ¼ 25, and LS(G) ¼ LF(G)  dG ¼ 25  2 ¼ 23. The procedure
continues in this fashion until the source is labeled. The labels of all nodes are shown
in Table 8.3.
A good test for correctness is to examine LF(ns) in the backward sweep, this value
must be zero. If it is not, an error has been made. However, the converse is not true:
even if LF(ns) ¼ 0, it does not mean that the backward recursion has been done
correctly. Furthermore, we realize that for every activity we have LF – LS ¼ EF –
ES.
Now that all earliest possible and latest allowable starting and finishing times
have been computed, we are able to determine which of the activities are critical
when scheduled and which are not. As an example, consider activity G. So far, we
have determined that the earliest possible time that we can schedule the activity is at
ES(G) ¼ 12, while the latest possible finishing time of G is LF(G) ¼ 25. This gives
us a time window of 25–12 ¼ 13 weeks during which the activity has to be
scheduled. That is not a problem, since the duration of the activity is only two
weeks, so that there is plenty of leeway. An expression of the magnitude of this
leeway is the total float of the activity G, which we will abbreviate here as TF(G).
The total float of an activity is the magnitude of the time window for its schedule
minus the duration of the activity. Formally for activity G, we have TF(G) ¼
LF (G)  ES(G)  dG ¼ 25  12  2 ¼ 11. Decision makers can use the
information provided by the total float as the amount of time by which the duration
of an activity can increase without delaying the entire project. Thus, activities with a
large float are safe in that their duration can increase significantly without delaying
the entire project. On the other hand, an activity with a large float indicates that there
might be too many resources allocated to this activity. Redirecting some of these
resources elsewhere may result in possible decreases of the durations of other
activities.
As another example, consider activity H. Its time window ranges from ES(H) ¼ 25
to LF(H) ¼ 34. With its duration of dH ¼ 9, we can compute the activity’s total float
as TF(H) ¼ LF(H)  ES(H)  dH ¼ 34  25  9 ¼ 0. This indicates that there is
absolutely no leeway when scheduling activity H. Activities that have no leeway in
the schedule are referred to as critical activities. A critical activity has the property
Table 8.3 The latest allowable starting and finishing times
Activity ns ABC D E F G H I J nt
Latest allowable starting
time LS
0 0 5 8 17 15 21 23 25 26 32 34
Latest allowable
finishing time LF
0 5 8 15 21 21 25 25 34 32 34 34
308 8 Project Networksthat as soon as its duration increases, the entire project will be delayed, regardless of
how small the time increase actually is.
Critical activities are very similar to those resources in optimization problems
whose constraints are satisfied as equations at optimum and thus represent
bottlenecks in the problem. On the other hand, noncritical activities in project
networks can be compared to resources or constraints that have positive slacks or
excess variables at optimum. Note that for the ease of computations, we can calculate
the total float also as TF ¼ LF  EF ¼ LS  ES.
In order to have all information available at a glance, it is useful to draw the
network in a slightly different way. Rather than representing each node by a circle
with its name in it, we suggest representing a node by a box with nine different fields,
as shown in Fig. 8.2. We will refer to the different fields by the geographic directions
they are found in, e.g., the field in the north, the southwest, etc. The center of the
node is reserved for the name of the activity (here activity D). The fields in the north
and in the south both show the duration dD of the activity.
Recall that during the forward sweep, we compute the earliest possible starting
time ES(D) and the earliest possible finishing time EF(D) of the activity under
consideration. They are found in the northwest and the northeastern fields, respec￾tively. In the backward sweep, we determine the latest allowable starting time LS(D)
and the latest allowable finishing time LF(D) of the activity. This information is put
into the fields in the southwest and southeast, respectively. Finally, the field in the
east will include the total float TF, which is computed after the forward and
backward passes have been completed. The field in the west will remain empty for
now. It is designed for the resource consumption of the activity, which is not
considered in the basic model.
The three fields in the top row of each node read from left to right symbolize the
relation ES + d ¼ EF, while the three fields at the bottom of a node read from right to
left show the relation LF - d ¼ LS. This is why we have chosen to include the
duration of the activity twice, once in the field in the north and again in the south.
Rather than using tables for the display of the information, we can work directly
on the graph, which is much easier, as it provides all required information at a
glance. The project network for our example is shown in Fig. 8.3.
Having all of this information at hand, we are now able to determine what gave
the method its name, viz., the critical path. Formally, the critical path is a path from
Fig. 8.2 Node representation
by a box
8.1 The Critical Path Method 309Fig. 8.3 Project network, for
example, in text
310 8 Project Networksthe source ns to the sink nt that includes only critical activities, such that node j can
directly succeed node i on the critical path, only if ES( j) ¼ LF(i). The critical path in
our example is shown in bold lines and it includes the activities ns –
A  B  C  E  F  H  nt. Clearly, the length of that path, obtained by adding
its activity durations, equals T ¼ 34. Note that the critical path does not include the
link from node B to node E, even though E directly follows B, but ES
(E) ¼ 15 > 8 ¼ LF(B). This clearly indicates that it is not sufficient to simply
connect all neighboring nodes that have zero total float. Furthermore, it may happen
that a project network has multiple critical paths. The next section will have
examples of that case.
We conclude this section by summarizing the procedure that determines the
critical path:
1. Use the forward pass to calculate the earliest possible starting and finishing times
of all activities.
2. Use the backward pass to calculate the latest allowable starting and finishing
times of all activities.
3. Calculate the total floats of all activities.
4. Determine the critical path.
While the above procedure is highly visual, it is also possible to formulate the
problem as an integer programming problem and solve it with any of the pertinent
methods. In order to do so, define zero-one variables xij, which assume a value of
1, if the activities ni and nj (in that order) are located on the critical path, and 0 if they
are not. Recall that the critical path is the longest path in the network, we can then
formulate the objective function as the sum of the products of the durations dj and the
variables xij, while the constraints are the usual node equations, along with a
constraint that requires that exactly one unit leaves the source.
In our example, we obtain the following formulation:
P : Max z ¼ 5xsA þ 3xAB þ 7xAC þ 7xBC þ 4xBD þ 6xBE þ 6xCE þ 4xCF þ 4xDFþ
2xDG þ 4xEF þ 9xFH þ 6xFI þ 9xGH þ 6xGI þ 2xIJ
s:t: xsA ¼ 1 xCF þ xDF þ xEF ¼ xFH þ xFI
xsA ¼ xAB þ xAC xDG ¼ xGH þ xGI
xAB ¼ xBC þ xBD þ xBE xGH þ xFH ¼ xHt
xAC þ xBC ¼ xCE þ xCF xGI þ xFI ¼ xIJ
xBD ¼ xDG þ xDF xIJ ¼ xJt
xBE þ xCE ¼ xEF xij ¼ 0 or 1 for all i, j:
8.1 The Critical Path Method 311The solution indicates that xsA ¼ xAB ¼ xBC ¼ xCE ¼ xEF ¼ xFH ¼ xHt ¼ 1 and
xij ¼ 0 otherwise. This is the same critical path found earlier with the graphical
method.
8.2 Project Acceleration
So far, we have considered time as the only criterion in project networks. Also note
that the technique described in the previous section did not involve any optimization,
all we have done is determined when the project can be finished by finding the length
of the longest path and which of the activities are bottlenecks in the system. In this
section, we will return to the basic model, but allow the possibility to accelerate, at a
cost, individual activities, so as to be able to finish the project earlier. The result will
be a list that shows possible finishing times of the project and the amounts that will
have to be paid to reach them. This will enable the planner to decide what combina￾tion of money spent and project duration best fits the specific situation.
In order to describe the situation, consider a single activity. As before, the activity
will have what we now call a normal duration. Since we will engage in a marginal
analysis, the cost of the activity at its normal duration is immaterial (we will have to
engage in the activity in any case), and the only costs we consider are those that are
incurred due to the acceleration of the activity. Suppose that the normal duration of
our activity is 7 hrs. It is now possible to use more resources (e.g., more manpower,
more tools, contracting out part of the activity, or any similar measure) to accelerate
this activity. Suppose that it costs $20 to reduce the duration of the activity to 6 hrs.
Using more resources still, additional money can reduce the duration of the activity
further. For simplicity, we assume that the cost function of the acceleration is linear,
meaning that reducing the duration by another hour to 5 hrs costs another $20 for a
total of $40. Note that normally the cost function is superlinear, meaning that
reducing the duration by 1 hr costs, say $x, reducing it by another hour costs more
than $x, yet another reduction is more expensive still, and so forth.
It is quite apparent that the reduction has some limitations, below which we
cannot reduce the duration of the activity any further. The shortest activity duration
of an activity that can be achieved is customarily referred to as crash time, and the
process of acceleration is sometimes called crashing. Our task is now to determine
which activities should be accelerated or crashed, so as to achieve the desired result
at the lowest possible cost.
As an illustration of the concept, consider the numerical example shown in
Fig. 8.4.
The project has four activities A, B, C, and D, whose normal times, crash times,
and unit acceleration costs are shown next to the nodes. For example, activity
D normally takes 8 hrs (there are no costs incurred at this duration), but we can
reduce the duration down to 7, 6, or 5 hrs. Each hour of acceleration costs $200. Note
that activity B cannot be accelerated.
312 8 Project NetworksIn the following, we first describe a procedure based on this network, as it
provides some insight so as to what happens during the process.
Figure 8.5 shows the project network under consideration along with the normal
durations of the activities (in hrs) and the critical path, which is shown in bold arcs.
(Note that we normally have to use the forward sweep/backward sweep procedure in
each step, but since the project network here is so small, we may enumerate the four
paths AC, AD, BC, and BD, determine their respective lengths and choose the
longest path; it is the critical path). The present duration of the project is 13 hrs.
We now have to determine which activities to accelerate. Recall that the project
duration is determined by the length of the longest path in the network. This means
that as long as we are not accelerating an activity on the longest, i.e., the critical path,
the project duration will not be reduced. This leads to the important realization that
we must accelerate an activity on the critical path. And, among those activities, we
will choose the one that minimizes our marginal, i.e., additional costs. In our
example, we have a choice between either accelerating activity A at a cost of
$600, or activity D at a cost of $200. Since it is less expensive to accelerate activity
Fig. 8.4 Project network for
acceleration example
Fig. 8.5 Critical path with
normal activity durations
8.2 Project Acceleration 313D, we reduce its duration by a single hr to 7 hrs. We now have a new network (even
though the network structure has not changed and never will during the
computations, only one activity duration has changed), and we have to determine
the critical path and the project duration anew. The result is shown in Fig. 8.6.
We notice that there are now two critical, i.e., longest paths in the network. They
are ns–A–C–nt and ns–A–D–nt, both having a length of 12. Accelerating the project
further will pose some additional difficulties. In order to demonstrate these
problems, suppose that we were to again accelerate activity D. This would cause
the path ns–A–D–nt to be only 11 hrs long, while the path ns–A–C–nt would still be
12 hrs long and as such, would now be the only critical path. In other words, we
would have spent another $200 and still would have to wait 12 hrs to finish the
project. This means that we have to refine the rule somewhat to indicate which
activities must be accelerated in order to speed up the project. In fact, we will have to
accelerate a set of activities, so that at least one of the activities in this set is on each
of the critical paths. In our small network, it is possible to examine the network and
enumerate the possibilities. In the network in Fig. 7.6, we can either accelerate
activity A (at a cost of $600), or the activities C and D (at a cost of
$300 + $200 ¼ $500). Before making the actual decision, we have to ascertain
that all of these accelerations are actually possible, i.e., that the present durations are
all above the crash times. In our case, the activities A, C, and D have present
durations of 5, 7, and 7, while their crash times are 3, 4, and 5, so that all activities
can actually be accelerated. Since the cheapest option is to accelerate activities C and
D, we accelerate each of these activities by one unit each. Based on the new activity
durations, we also determine the new critical path(s). The results are shown in
Fig. 8.7.
If any further acceleration is required, the options are the same as before.
Checking the possibility to accelerate, we find that the present activity durations of
the nodes on the critical path A, C, and D are 5, 6, and 6, while their crash times are
3, 4, and 5, so durations of all of these activities can be reduced further. The least
expensive option again involves accelerating activities C and D by 1 hr each,
resulting in the situation shown in Fig. 8.8.
Fig. 8.6 Critical paths with
activity D accelerated
314 8 Project NetworksWhile the critical paths are still the same, the situation has changed. Activity
D has now reached its crash time and no longer can be accelerated. This means that
the only way to accelerate both critical paths simultaneously is to speed up activity
A at a cost of $600. The resulting situation is shown in Fig. 8.9.
Fig. 8.7 Critical paths with
activities C and D accelerated
Fig. 8.8 Critical paths with
activities C and D accelerated
Fig. 8.9 Critical paths with
activity A accelerated
8.2 Project Acceleration 315Notice that at this point in time, all four paths from the source to the sink are
critical. This means that the next acceleration will have to ensure that at least one
activity on each path in the network is accelerated. However, in this case, at least one
path, viz., the path ns–B–D–nt can no longer be accelerated at all: both activities
B and D are at their respective crash times. This means that the shortest possible
project duration is T ¼ 9 and it can be achieved at a cost of $1800. The results
achieved in the above computations are shown in Table 8.4.
This is where the decision maker comes in. He can now determine what it costs to
accelerate the project and whether or not it is worth it. This is a good example of
what operations research does best: prepare alternative decisions (rather than actu￾ally make them).
A few concluding comments are in order. It became clear that as the process
moved forward, more and more paths became critical, implying that more and more
activities had to be accelerated in order to reduce the project duration further. This
does, of course, imply increasing costs from each unit of acceleration. This was to be
expected, of course: at first, it is easy to accelerate a project, but as the timeframe
becomes tighter and tighter, the costs skyrocket.
If it appears too cumbersome to go through the entire process, a quick idea of how
short the project duration can actually be is to crash all activities and determine the
critical path on that basis. While this will certainly result in the shortest possible
project duration, it is usually not necessary to crash all activities to reach the same
overall duration. A good illustration is the above example. The optimized durations
of 4, 5, 4, and 5 of the activities A, B, C, and D were sufficient to reduce the project
duration to 9, while the crash durations of the activities are 3, 4, 4, and 5. Their use
would result in the same overall project duration of 9.
Finally, we will delineate a linear programming formulation that finds the results
for arbitrary project durations. To formulate the problem, define variables dj and tj
with dj denoting the actual duration of activity j, and tj symbolizing tj as the earliest
possible finishing times of node/activity j for all activities including the source and
the sink. The objective then is the sum of products of the unit acceleration costs and
the number of units by which an activity has been accelerated, viz., the difference
between normal time and the actual time dj. Then there are four types of constraints.
First, there is the single constraint tt equals the desired project duration. Secondly, all
activities without predecessors have their variables ti set equal to their actual
Table 8.4 Summary of the project acceleration process
Accelerate
activity Total acceleration costs ($) Critical path Project duration (hrs)
– 0 A–D 13
D 0 + 200 ¼ 200 A–C and A–D 12
C and D 200 + 500 ¼ 700 A–C and A–D 11
C and D 700 + 500 ¼ 1200 A–C and A–D 10
C and D 1200 + 600 ¼ 1800 A–C, A–D, B–C, and A–D 9
316 8 Project Networksdurations. The third set of constraints has a constraint tj  ti + dj for each activity
j (including the sink t), for which a predecessor i has been specified. The fourth and
last set of constraints requires that the activity durations each are between crash time
and normal time. In our example, we formulate:
P : Min z ¼ 600 5ð Þþ  dA 300 7ð Þþ  dC 200 8ð Þ  dD
s:t: tt ¼ 13
tA ¼ dA
tB ¼ 4
tC  tA þ dC
tC  tB þ dC
tD  tA þ dD
tD  tB þ dD
tt  tC
tt  tD
dA  3
dA  5
dC  4
dC  7
dD  5
dD  8
d j, t j  0 for all j:
By successively changing the value of tt in the first constraint, we can (re-)create
the results in Table 8.4.
8.3 Project Planning with Resources
So far, our discussion has focused on time planning. In the process, we have assumed
that sufficient resources are available to perform the activities in the time specified
for the individual activities. In this section, we will extend the basic model by adding
a resource requirement. For simplicity, we will use only a single resource, such as
manpower, backhoes, machinery, or any other resource relevant to the project. For
simplicity, we will refer to the resource as employees throughout this section. When
we associate a resource requirement of, say, 30 units to an activity, we mean that
30 employees are required throughout the duration of the activity. As an illustration,
consider again the project network in Fig. 8.3 in Sect. 8.1. Furthermore, assume that
the resource consumptions of the individual activities are as shown in Table 8.5.
In order to schedule the activities of the project network, it is useful to employ a
so-called Gantt chart. In essence, it is a horizontal bar chart, which features the
individual activities on the ordinate, while the abscissa is a time axis. Clearly, the
activities on the critical path are scheduled from their earliest possible (or, equally,
Table 8.5 Resource consumption of the activities in the example
Activity ABCDEFGHI J
Employees required 10 20 40 20 25 30 10 25 20 25
8.3 Project Planning with Resources 317latest allowable) start times, so that they form a non-overlapping sequence of bars
that has no gaps. The bold bars in Fig. 8.10 belong to critical activities and their
position in the graph cannot be changed.
The matter is different with the noncritical activities, which have some leeway for
their schedule. Suppose now that we use a heuristic method for scheduling them,
which includes a rule that states that all noncritical activities should be scheduled as
early as possible. The regular bars that belong to the activities D, G, I, and J in
Fig. 8.10 show how these activities are scheduled. This schedule now has very clear
resource implications. From time t ¼ 0 to t ¼ 5, we only perform activity A, so that
we need 10 employees. From t ¼ 5 to t ¼ 8, we perform only activity B, which
requires 20 employees. Starting at t ¼ 8 to time t ¼ 12, we perform the activities
C and D simultaneously. This requires 40 + 20 ¼ 60 employees. At time t ¼ 12,
activity D is finished, while C is still going on. However, at t ¼ 12, activity G is also
scheduled, so that 40 employees for C and 10 employees for activity G are needed.
This process continues until the project is finished. The resource requirements are
shown in the resource requirement graph in Fig. 8.11.
It is apparent that the resource requirement is very low in the beginning, then
peaks, drops and increases again towards the end. If we were to be able to employ
only casual labor that we need to pay only when needed, then the total resource
requirement is the shaded area in Fig. 8.11. Calculating the size of the area from the
beginning, we have 10 employees needed for 5 weeks, 20 employees needed for
3 weeks, 60 employees needed for 4 weeks, and so forth, for a total of 1155
employee weeks. Assuming that we pay employees $15 per hour for 8 hrs a day
and 5 days a week, each employee will cost us $600 (plus fringe benefits, which we
Fig. 8.10 Gantt chart
318 8 Project Networksignore here for simplicity). This means that the resource costs will be $693,000 for
the entire project. Note that these costs for casual labor will remain the same,
regardless of how we schedule.
The situation changes dramatically if we have to hire all needed employees for the
entire duration of the project. As the highest manpower requirement at any point in
time is 60, this is the smallest number of permanent employees required for the
duration of the project. Employing 60 employees for the total of 34 weeks at a cost of
$600 per week costs $1,224,000, more than 76% more than the costs for casual
labor. This is caused by the fact that more than 43% of the time the employees are
paid, they are actually idle.
This calls for a different schedule whose maximal resource requirement is as low
as possible. This type of objective is of the minimax type, where we search to
minimize the maximum resource required at any point in time. Rather than using the
heuristic that schedules all activities as early as possible (not a bad choice in general,
as it allows for some noncritical activities to increase in duration without
jeopardizing the finishing of the project on time), we will use another heuristic that
schedules all activities as late as possible. The Gantt diagram that belongs to that
schedule and the associated resource consumption graph are shown in Fig. 8.12.
It turns out that while this schedule uses the exact same number of employee
weeks of casual labor—1155—the highest employee requirement at any one point is
only 50. This means that the costs for employees working throughout the project is
$1,020,000, which is 16.67% less than with the “earliest possible” schedule. The idle
time here is still 32%, though.
Other heuristics exist for the scheduling of noncritical activities. Depending on
the problem, they may be able to reduce the number of required resources further.
The problem can also be solved by exact methods, but the integer programming
problem that must be formulated for that purpose is quite difficult. For details, see,
e.g., Eiselt and Sandblom (2004).
Fig. 8.11 Resource requirement graph
8.3 Project Planning with Resources 3198.4 The PERT Method
All project planning models discussed so far have in common that they are deter￾ministic. More specifically, they have assumed that all components of the network—
the activities, their durations, and the precedence structure—are known with cer￾tainty. This section will change that. In particular, we assume here that the activity
durations are no longer known with certainty. It is important to realize that this is
only one component that can be probabilistic: fully stochastic networks are dealt
with by very sophisticated project network tools such as GERT (graphical review
and evaluation technique), which are beyond the scope of this book.
The PERT method discussed in this section assumes that the duration of the
activities are random variables with known underlying probability distributions. We
do assume that the durations of activities are independent of each other. This is a
fairly strong assumption, not justified in cases such as construction, where
occurrences such as bad weather will affect many of the activities to take longer
than they normally would. As usual in all of operations research, it is necessary to
check the assumptions carefully, and if the assumptions do not fit the scenario under
consideration, don’t use the model.
Traditionally, it has been assumed that the duration of a single activity follows
Euler’s beta distribution. This assumption has been much criticized in the literature.
However, we can derive the same formulas without making such a strict and
Fig. 8.12 Gantt chart and resource requirement graph for alternative schedule
320 8 Project Networkscontroversial assumption. Our assumption is that the activity durations follow some
symmetric bell-shaped distribution. For symmetric bell-shaped distributions, the
empirical rule in statistics is known to apply; see Appendix C. It states that about
all observations are within three standard deviations about the mean, while about
two-thirds of all observations are within one standard deviation about the mean. This
leaves 1/6 of the total mass for each of the two tails of the distribution. This situation
is shown in Fig. 8.13.
We can then define three time estimates for the duration of each activity: a most
likely time estimate tm (the mode of the distribution), a pessimistic estimate tp, and an
optimistic estimate to. The most likely time tm, set at μ, is associated with the central
part of the distribution, the pessimistic estimate tp, set at μ + 1σ, belongs to the right
tail of the distribution, and the optimistic estimate to, set at μ  1σ, is on the left tail
of the distributions. Their weights are 2/3, 1/6, and 1/6 as shown in Fig. 8.13. Based
on these estimates, we can then compute a (weighted) mean for the duration of an
activity as t ¼  1
6 to þ 2
3 tm þ 1
6 tp ¼ toþ4tmþtp
6 , which is exactly what was obtained by
using the much stronger assumption of the beta distribution. Similarly, with
tp ¼ μ + 3σ and to ¼ μ – 3σ, we can determine the variance of the activity duration,
which is then σ2 ¼ 1
36 tp  to
 2
.
Armed with this information, we can then determine the mean duration and its
variance for all of the given activities from the three time estimates, specified for
each of them. Given the mean activity durations, we will use them in exactly the
same way we dealt with time estimates in the critical path method. Going through the
regular procedure—forward sweep, backward sweep, computation of floats, critical
path—we can determine the critical path on the basis of the mean durations of the
activities. In addition to the usual information about project duration, critical and
noncritical activities, and sensitivity analyses on the basis of floats, we can use the
variances of the durations on the critical path to make probability statements. In
particular, we can provide the decision maker with an estimate concerning the
probability with which the project can be completed within a certain time.
In order to explain the concept, consider a numerical example, whose numerical
information is provided in Table 8.6. The graph for this project is the same as that for
the examples in Sects. 8.1 and 8.3, but the time estimates are obviously different.
Our first task is to calculate the mean activity durations for the ten activities. They
are 6, 3, 7, 5, 6, 4, 2, 9, 6, and 2, respectively. Those time estimates are then used in
μ–3σ μ–1σ μ μ+1σ μ+3σ 
3
2
6
1
6
1
Fig. 8.13 Empirical rule for
activity durations
8.4 The PERT Method 321the standard procedure discussed in the first section of this chapter. The results are
shown in Fig. 8.14.
Consider now the critical path ns  A  B – C – E – F – H  nt. The mean project
duration of μ ¼ 35 has already been computed in the procedure. We now have to
calculate the variance on this path. The variances of the individual activities on the
path (in order of their appearance) are 4
36 , 0, 16
36 , 36
36 , 0, and 36
36 . The sum of these
variances equals σ2 ¼ 92
36, so that the standard deviation equals σ ¼
ffiffiffiffi
92
36 q
ffi 1:5986.
Furthermore, invoking the central limit theorem (probably not justifiably so, as the
number of arcs on the critical path is fairly small), the project duration is approxi￾mately normal with mean μ and standard deviation σ.
Given this information, we are now able to provide the decision maker with some
rough estimate about the probability with which the project can be finished within a
prespecified time frame T. Suppose that the decision maker wants to know what the
probability is that the project is finished within T ¼ 36 hrs. In order to be able to use
the standard normal distribution (see Appendix C), we calculate the z-score as z ¼ Tμ
σ ¼ 3635
1:5986 ffi 0:6255, we find that the probability P(X  36) ¼ 73.42%. When
calculating these probabilities, it is always useful to draw the normal distribution
function and indicate which area we are looking for. The area relevant to this
question is shown in Fig. 8.15a, where it constitutes the shaded area plus the entire
mass to the left of the mean which, by definition, equals 0.5.
Similarly, we could compute the probability that the project takes more than
37 hrs. Such information may be needed by the decision maker, as the late comple￾tion of the project may carry a penalty with it. The z-score for the completion time of
37 isz ¼ 3735
1:5986 ffi 1:2511, from which we obtain a probability of P(X  37) ¼ 10.54%.
The area of interest under the normal distribution function is shown in Fig. 8.15b.
Finally, we compute the probability that the project is completed between 33 and
36 hrs. The area of interest is shown in Fig. 8.15c, and, due to the symmetry of the
normal distribution, it can be computed as P(33  X  36) ¼ P(33  X  35)
+ P(35  X  36) ¼ P(35  X  37) + P(35  X  36) ¼ .3946 + .2342 ¼ .6288. In
Table 8.6 Numerical information for example
Activity Immediate predecessor
Time estimates (in hrs)
Optimistic Most likely Pessimistic
A – 5 67
B A 3 33
C A, B 5 79
D B 3 4 11
E B, C 3 69
F C, D, E 4 44
G D 1 23
H F, G 4 10 10
I F, G 4 68
J I 2 22
322 8 Project NetworksFig. 8.14 Project network,
for example, in text
8.4 The PERT Method 323other words, chances are about 63% that our project will be completed within the
specified time window.
A few comments are in order. First of all, much care should be taken with the
probability statements. They have been derived with a lot of assumptions, so that it
makes absolutely no sense to report them to the decision maker with two, four, or
even more digits to the right of the decimal point, implying great accuracy. These
probabilities are rough estimates, and this should be emphasized throughout.
Secondly, what we have used is what is called the time-critical path. In other
words, we have determined the critical path on the basis of estimated activity
durations only, and then computed the probabilities. This does not necessarily
provide the planner with the true critical path. As an example, consider two paths
in some network, one with a mean duration μ ¼ 100 and a standard deviation of
σ ¼ 10, while a second, obviously noncritical, path exists in the network with μ ¼ 99
and σ ¼ 100. Note that the noncritical path is shorter, but has a much higher standard
deviation. The probability to finish the project within T ¼ 110 hrs, computed on the
basis of the critical path as we do, is then 84.13%. However, computing the same
probability on the basis of the noncritical path is only 54.38%, meaning that the
former result (that we would obtain with our procedure), grossly overestimates the
likelihood to finish the project within the specified time frame. This problem persists
in a somewhat different guise even in our example: all probability statements assume
that the critical path remains critical. Note, however, that the noncritical activities
I and J in our example have a very small float, making them almost critical. If their
durations were to increase by fairly small, insignificant amounts, they would become
critical, and their standard deviations, which have been completely ignored so far,
would suddenly have to be counted. This is yet another reason to treat the probability
statements we have calculated with the utmost caution.
Exercises
Problem 1 (acceleration of a project): Consider a project network with four
activities, their normal durations, their shortest possible durations (which can be
achieved at extra cost), and the acceleration cost per time unit. Details are shown in
Table 8.7.
Fig. 8.15 Probabilities for various events
324 8 Project Networks(a) Draw the project network.
(b) Use the forward recursion, the backward recursion, calculate the floats, and
determine the critical path(s). What is the duration of the project?
(c) Accelerate the project by one day. Clearly indicate which (combinations of)
activities could be accelerated in order to speed up the project, and which
activity or activities should be accelerated. What are the associated costs?
(d) What if the decision maker needs some further project acceleration? What is the
shortest project duration, and what are the associated costs?
Solution:
(a), (b) See Fig. 8.16.
The project duration is 9 days.
(c) Given the two critical paths A–B and A–C–D, respectively, we can either
accelerate activity A (cost: $80) or the activities B and C (cost: $30 + $50 ¼ $80), or
the activities B and D (cost: $30 + $40 ¼ $70). Given its lowest cost, we accelerate
activities B and D for a total cost of $70. Now the activity durations are 4, 4, 3, and
1, respectively, and the project duration is T ¼ 8. Note that at this point, the activity
duration of activity D has reached crash time and can no longer be accelerated. Also,
note that the system of critical paths has not changed.
Table 8.7 Details of the network in Problem 1
Activity
Immediate
predecessor(s)
Normal duration
(in days)
Shortest possible
duration (in days)
Unit cost of
acceleration ($)
A – 4 3 80
B A 5 3 30
C A 3 2 50
D A, C 2 1 40
Fig. 8.16 Project network for Problem 1
8.4 The PERT Method 325(d) After the first acceleration in (c), we can either accelerate A (cost $80) or
activities B and C (cost $80). Arbitrarily choosing activity A, the activity durations
are then 3, 4, 3, and 1, respectively, total acceleration costs are $150, and the project
length is T ¼ 7. The two critical paths are still the only critical paths in the network.
At this point, we can only accelerate activities B and C, as A and D are both at their
respective crash times. Accelerating B and C costs $80. This leads to a project
duration of T ¼ 6, which cannot be accelerated any further. The total acceleration
cost to get to this point is $230.
Problem 2 (scheduling with resources, Gantt chart and resource consump￾tion graph): A project has been subdivided into five activities. Their immediate
predecessors, activity durations, and resource consumption (e.g., the number of
employees required) are shown in Table 8.8.
(a) Draw the project network.
(b) Calculate all earliest and latest starting and finishing times as well as the total
floats of all activities. What is the critical path? What is its duration?
(c) Draw the Gantt chart and the associated resource consumption graph, assuming
that all activities are scheduled as early as possible. What is the largest resource
consumption at any point in time?
Solution: (a) See Fig. 8.17.
(b) The critical path includes the activities A, C, D, and E. Its length is 15.
(c) See Fig. 8.18.
The largest resource consumption at any point in time is 80.
Problem 3 (PERT network): A project has been subdivided into five activities.
Their immediate predecessors and the estimated activity durations (optimistic, most
likely, and pessimistic) are shown in Table 8.9.
(a) Draw the project network. Based on the mean activity durations, calculate all
earliest and latest starting and finishing times, and the total floats of all activities.
What is the critical path? What is its duration?
(b) Calculate the variance and standard deviation on the critical path.
(c) Calculate the probability that the project will be finished between 11 and
15 days.
(d) What is the probability that the project will be finished within 14 days? What is
the probability that the project will be finished in exactly 14 days?
Table 8.8 Details about the network in Problem 2
Activity Immediate Predecessor Duration (in days) Resource consumption
A – 3 40
B – 7 40
C A 5 30
D A, B, C 2 60
E B, D 5 70
326 8 Project NetworksSolution: (a) See Fig. 8.19.
The critical path is ns–B–C–D–E–nt. Its duration is 14 days.
(b) σ2 ¼ 1
36 ½ ¼ 16 þ 4 þ 0 þ 400 420
36 , σ ¼ 3.4157.
(c) P(11  X  15) ¼ P(11  X  14) + P(14  X  15). Finding z-scores z1 and
z2 for the two ranges results in z1 ¼ 1411
3:4157 ¼ :8783, leading to 31.01%, and
z2 ¼ 1514
3:4157 ¼ :2928, leading to 11.52%, for a total of 42.53%.
(d) P(X  14) ¼ 0.5. P(X ¼ 14) ¼ 0.
Problem 4 (Gantt chart, resource requirement graph, and PERT network):
A project has been subdivided into five activities. Their immediate predecessors and
the estimated activity durations (optimistic, most likely, and pessimistic) are shown
in Table 8.10.
(a) Draw the project network. On the basis of the expected durations, calculate all
earliest and latest starting and finishing times, and the total floats of all activities.
What is the critical path? What is its duration?
(b) Calculate the variance and standard deviation on the critical path.
(c) Calculate the probability that the project will be finished between 14 and
18 days.
3 5 8
C 0
D
A 0
3 5 8
8 2 10
8 2 10
0
B
0 7 7
1 7 8
1
0 3 3
0 0 0
0 0 0
n 0 s
15 0 15
15 0 15
n 0 t
15 5 15
10 5 15
E 0
0 3 3
Fig. 8.17 Project network for Problem 2
8.4 The PERT Method 327(d) What would happen to the result under (c), if the time estimates of activity
D were to be revised to 3, 6, and 9? Explain in one short sentence.
(e) Consider the starting and finishing times calculated in (a) as well as resource
requirements of 20, 50, 30, 40, and 60, respectively. On that basis, draw a Gantt
diagram given that all activities are scheduled as early as possible. What is the
highest resource requirement at any one time during the project?
(f) Repeat question (e) for the “Latest possible” scheduling rule.
Solution: (a) See Fig. 8.20.
The unique critical path is ns–A–E–nt, and its length is 16.
Fig. 8.18 Gantt chart and resource requirement graph for Problem 2
Table 8.9 Details about the network in Problem 3
Activity Immediate Predecessor Estimated duration (in days)
A – 1, 2, 3
B – 1, 3, 5
C A, B 3, 4, 5
D A, C 2, 2, 2
E C, D 1, 2, 21
328 8 Project Networks022
A 1
1 2 3
729
D 0
7 2 9
347
0
0
3 3
033
C 0
B
9
0
5 14
9 5 14
E
3 4 7
0 0 0
0 0 0
n 0 s
14 0 14
14 0 14
n 0 t
Fig. 8.19 Project network for Problem 3
Table 8.10 Details about the network in Problem 4
Activity Immediate Predecessor Estimated duration (in days)
A – 2, 6, 16
B – 2, 2, 8
C A 6, 8, 10
D B 5, 6, 7
E A, B 9, 9, 9
077
A 0
0 7 7
7 8 15
C 1
8 8 16
7 9 16
0
4
3 3
4 3 7
E 0
B
3
7
6 9
10 6 16
D
7 9 16
0 0 0
0 0 0
n 0 s
16 0 16
16 0 16
n 0 t
Fig. 8.20 Project network for Problem 4
8.4 The PERT Method 329(b) and (c) σ2 ¼ 1
36 ð Þ 196 þ 0 5:4444 and σ  2.3333. Then z ¼ 1816
2:3333 ¼ :8571,
and P(14  X  18) ¼ 2(.3043) ¼ 60.86%.
(d) The change does not affect the mean of activity D, only its variation, and since
activity D was not on the critical path before and will not be on it after the
modification, the results will not change.
(e) See Fig. 8.21.
The highest resource requirement at any point in time during the project is 130.
(f) See Fig. 8.22.
Problem 5 (Alternative linear programming formulation): Consider again the
CPM network in Fig. 8.3. Define variables xj as the earliest possible finishing time of
activity j (i.e., EF(nj)) and formulate the problem with constraints for all arcs (i, j) in
the network that read xj  xi + dj.
Solution: The formulation of the problem is
Fig. 8.21 Gantt chart and resource requirement graph for Problem 4 (e)
330 8 Project NetworksP : Min xt
s:t: xs ¼ 0
xA  xs þ 5
xB  xA þ 3
xC  xA þ 7
xC  xB þ 7
xD  xB þ 4
xE  xB þ 6
xE  xC þ 6
xF  xC þ 4
xF  xD þ 4
xF  xE þ 4
xG  xD þ 2
xH  xF þ 9
xH  xG þ 9
xI  xF þ 6
xI  xG þ 6
xJ  xI þ 2
xt  xH
xt  xJ
x j  0 for all j:
Fig. 8.22 Gantt chart and resource requirement graph for Problem 4 (f)
8.4 The PERT Method 331Solving the problem, we identify the excess variables with a value of zero and
positive shadow prices. Here, they belong to constraints 1, 2, 3, 5, 8, 11, 13, and 18.
These constraints belong to arcs in the network, which, if put together, result in the
critical path s – A – B – C – E – F – H – t.
Reference
Eiselt HA, Sandblom C-L (2004) Decision analysis, location models, and scheduling problems.
Springer, Berlin
332 8 Project NetworksMachine Scheduling 9
The subject of this chapter is the allocation of jobs (or tasks) to processors
(or machines). These terms should be understood in the widest possible sense: in
the case of a doctor treating patients, the doctor is the processor and the patients
represent the tasks; in the case of a tax audit, the auditor is the processor (or the
“machine”), while the individual cases are the tasks to be processed. This allocation
is to be made so as to optimize some objective. We will discuss a number of these
objectives below.
This problem is somewhat reminiscent of the production scheduling problem
introduced in Chap. 2, and the project planning problems discussed in Chap. 8. The
main feature of the machine scheduling problems in this chapter is that they include a
sequencing component, which determines the order in which tasks are processed on
a machine, and a scheduling component that determines at what time the processing
of a task begins and ends on a machine. Often, the term scheduling is meant to
include both the sequencing and scheduling parts of the allocation problem.
This chapter deals exclusively with deterministic scheduling problems, i.e.,
situations in which all parameters are assumed to be known with certainty. This is
not to mean that these are the only relevant scheduling problems: on the contrary, in
many important real-world applications, some of the parameters involved in the
problem are uncertain. However, the structure of probabilistic problems is no
different from that of deterministic problems (other than the fact that probabilistic
problems are typically much more difficult than their deterministic counterparts),
and what we attempt to convey in this chapter is a general idea of scheduling models,
their applications, and degree of difficulty.
The first section of this chapter introduces the basic concepts of scheduling
models. The remaining three sections of this chapter deal with different scheduling
scenarios with increasing degree of difficulty.
# The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
H. A. Eiselt, C.-L. Sandblom, Operations Research, Springer Texts in Business and
Economics, https://doi.org/10.1007/978-3-030-97162-5_9
3339.1 Basic Concepts of Machine Scheduling
As mentioned above, each machine scheduling problem features n tasks (jobs) T1,
T2, ..., Tn that are to be performed on m machines (processors) P1, P2, ..., Pm.
Processing a task on a machine will take a certain (and known) amount of time,
which is referred to as the processing time. In particular, we define pij as the
processing time of task Tj on machine Pi for all combinations of tasks and machines.
In case there is only a single machine, or all machines have the same processing time
for a job, we simplify the notation to pj.
We can then distinguish between three broad categories of models. The first
category assumes that there is only a single machine, we refer to this as single￾machine scheduling. The second category has multiple machines working in paral￾lel, all able to perform the same function. This type of model is called a parallel
machine scheduling problem. There are two subcategories of parallel machine
scheduling. In the first, parallel machines are identical (meaning that all machines
take the same amount of time to process any given task), or they are unrelated, which
indicates that it takes different amounts of time to process any given task on different
machines.
The third broad category of models includes dedicated machine scheduling
models. The main characteristic of these models is that not all machines have the
capability to process all jobs, and not all jobs need to be processed on all machines.
In particular, we distinguish between three subcategories. The first category is an
open shop. In an open shop, each task must be processed by all machines, but there is
no specific order in which the processing will take place. The second category
comprises flow shops, in which each job is to be processed on all machines, and
each task is processed by the machines in the same specified order. Finally, there are
job shops, in which each job needs to be processed on a specific set of machines, and
the processing order is also job specific.
In addition to the processing time pij introduced above, we may also have a
number of additional parameters. The first such parameter is the ready time (also
referred to as arrival time or release time) rj, which indicates the time at which task
Tj is ready for processing on the machines. In the simplest case, rj ¼ 0 for all tasks Tj,
meaning that all jobs are ready when the scheduling process starts. The second
additional parameter is the due time dj, which is the time at which task Tj should have
finished all processing. This could be a time specified in a contract or some other
delivery time that was agreed upon. If the task is not completed by that time, then
there may be a penalty for the delay.
Once all jobs have been scheduled on the machines, we can use a number of
different properties inherent in any given schedule. The first is the completion time cj
of job Tj, which is the time at which task Tj is actually completely finished. The
second property is the flow time fj of task Tj. The flow time is formally defined as
fj ¼ cj  rj, and it can be thought of as the time that a job is in the system, either
waiting to be processed or being processed. (This time is reminiscent as the “time in
the system” Ws in the analysis of waiting lines; see Chap. 15). Finally, there is the
lateness of a task Tj, which is defined as ‘j ¼ cj  dj, and tardiness, which is
334 9 Machine Schedulingexpressed as tj ¼ max {‘j, 0}. The lateness of a task is the time that elapses after the
due date and the actual completion date of a task. For late jobs, lateness and tardiness
are the same. For jobs that are completed before their due date, the lateness becomes
negative, while their tardiness equals zero.
There are many different criteria that may be used to optimize scheduling
systems. Three of the most popular such criteria are introduced here. First, there is
the makespan (or schedule length) Cmax. It is formally defined as Cmax ¼ max
j c j
 ,
and it expresses the time at which the last of the tasks has been completed. Such a
measure is meaningful if a project can only be considered completed, if all of its
individual tasks have been completed (similar to project networks; see Chap. 8). A
typical example is the processing of machine parts that have been ordered by a
customer. All of them will be shipped in a box, and the box can only be released for
transportation, once all of the individual machine parts are included.
The second criterion is the mean flow time F. The mean flow time is formally
defined as the unweighted average F ¼ 1
n f 1 þ f 2 þ ... þ f n ð Þ. (As a matter of fact,
by virtue of the definition of flow time, it is easy to demonstrate that the mean flow
time differs from the mean completion time 1
n ð Þ c1 þ c2 þ ... þ cn only by the
constant 1
n ð Þ r1 þ r2 þ ... þ rn .
) The mean flow time refers to the average time that
a job is in the system either waiting to be processed or being processed. The mean
flow time is a meaningful measure in instances such as a maintenance or repair
system, in which a machine is not available if it is waiting for repair or being
repaired.
The third and last criterion in this context is the maximal lateness Lmax. The
maximal lateness is defined as Lmax ¼ max
j
ℓ j
  , and it expresses the longest
lateness among any of the jobs. This criterion is applicable in case some lateness
is unavoidable or deemed acceptable, but the decision maker attempts to ensure that
very long delays beyond the due date are avoided.
Before discussing any details of specific scheduling models, we should point out
some general features inherent in scheduling problems. In this type of model, there is
a fairly fine line that separates models that are rather easy to solve (some of which are
represented in this chapter), while others, seemingly straightforward extensions, are
very difficult. In those cases, exact methods that find optimal solutions will take a
very long time, which may—depending on the individual circumstances—render
them impractical. In such cases, decision makers will resort to heuristic methods that
have the advantage of providing (hopefully good) solutions quickly, but the obvious
disadvantage of not necessarily resulting in an optimal solution. If a difficult problem
has to be solved in real time, the use of a heuristic is imperative; if some time is
available, an exact algorithm may be employed.
9.1 Basic Concepts of Machine Scheduling 3359.2 Single-Machine Scheduling Models
The models in this section deal with the simplest of scheduling problems: there is
only a single machine on which tasks are to be processed. Before investigating the
solutions that result from the use of the three criteria presented in the introduction
above, we may introduce another wrinkle in this seemingly primitive scenario. In
particular, we may consider modes that allow the preemption of a task, while others
do not. In case preemption is permitted, this means that each task has associated with
it a priority, and if a task with a higher priority becomes available at a time when a
task with a lower priority is being processed, then processing on the lower-level task
stops, and the higher-level task is processed first. Examples of preemption abound:
consider the case of a surgeon who is dealing with a broken leg as another patient
with a heart attack arrives. Rather than referring to the usual “first come, first served”
rule (inviting juicy lawsuits), most surgeons would probably stabilize the broken leg
and deal with the heart patient first. Similar preemptions are found for police officers,
who would interrupt a routine investigation to attend to a robbery in progress, or a
plumber, who will interrupt the installation of a water pump in a residence to attend
to a broken main. In this section, we restrict ourselves to cases, in which preemption
is not permitted.
Minimizing the makespan in case of a single machine is not meaningful, as each
sequence of tasks will result in the same value of Cmax. More specifically, Cmax
equals the sum of processing times of all tasks.
The objective that minimizes the mean flow time F is not as straightforward.
However, it is not difficult either, as it has been shown that the simple Shortest
Processing Time (SPT) algorithm solves the problem optimally. The algorithm can
be summarized by a simple rule. Note that we assume that all release times are zero.
SPT Algorithm: Schedule the task with the shortest processing time first. Delete the task and
repeat the procedure until all tasks have been scheduled.
Rather than illustrate the SPT algorithm by an example, we will first introduce a
minor extension of the rule. In particular, suppose that the decision maker has not
only processing times pj to consider, but there are also weights wj associated with the
tasks. Typically, a weight could symbolize the cost of a task as it is processed on a
machine, given that different tasks are associated with different costs. The objective
is then to minimize the average weighted flow time, defined for task Tj as wjfj. The
weighted generalization of the SPT algorithm can then be stated as
WSPT Algorithm (Smith’s Ratio Rule): Schedule the task with the shortest weighted
processing time pj/wj first. Delete the task and repeat the procedure until all tasks have
been scheduled.
As a numerical illustration, consider:
336 9 Machine SchedulingExample 1 There are seven machines in a manufacturing unit. Scheduled mainte￾nance has to be performed on these machines once in a while. The repairman has
identified the processing times required for the maintenance. Costs are incurred for
downtime, regardless of whether the machine waits for service or is being served.
These costs differ between the machines. The estimated processing times of the
machines, the downtime costs, and the weighted processing times are summarized in
Table 9.1.
Applying the WSPT algorithm, we first schedule task T4 (which has the lowest
weighted processing time of 55=9), followed by T3 with the next lowest weighted
processing time of 62=3, followed by T6, T2, T5, T7, and T1. The schedule is shown in
the Gantt chart (named after the American engineer Henry L. Gantt (1861–1919; see
also Sect. 8.3), who developed these charts in 1917) in Fig. 9.1.
The tasks T4, T3, T6, ..., T1 now have idle times of 0, 50, 90, 150, 175, 220, and
255, respectively. Adding the processing times to the idle times results in 50, 90,
150, 175, 220, 255, and 285, respectively. Multiplying these by the individual
per-minute costs and adding them up results in a total of $4930.
Consider now the objective that minimizes maximal lateness Lmax. Again, this
problem turns out to be easy from a computational point of view. A simple method
was developed in the mid-1950s by Jackson, which is now commonly referred to as
the earliest due date algorithm (or EDD algorithm or Jackson’s rule). It finds an
optimal solution and can be stated as follows. Again, the release times of all tasks are
zero.
EDD Algorithm: Schedule the task with the earliest due date first. Delete the task and repeat
the procedure until all tasks have been scheduled.
We will explain this rule by means of
Example 2 The accounting department of a large firm processes bookkeeping jobs
for various divisions of the firm. At present, they have identified eleven tasks, which
Table 9.1 Processing times and downtime cost for Example 1
Job # T1 T2 T3 T4 T5 T6 T7
Service time (minutes) 30 25 40 50 45 60 35
Downtime cost ($ per minute) 2 3 6 9 4 83
Weighted processing time pj/wj 15 81=3 62=3 55=9 11¼ 7½ 112=3
Fig. 9.1 Gantt chart for Example 1
9.2 Single-Machine Scheduling Models 337are to be completed by a single team, one after another. The processing times and the
due dates for the individual jobs are shown in Table 9.2.
The EDD rule starts by scheduling task T6 first (its due date is 10, the earliest of all
due dates), followed by T2, T1, and so forth. The Gantt chart for the schedule
obtained by the EDD rule is shown in Fig. 9.2 (where the tie between tasks T8 and
T9 that have the same due dates is broken arbitrarily).
Simple inspection reveals that the tasks T6, T2, T1, and T8 are finished before the
due date. Task T9 is late by 7 hrs, T3 is late by 9 hrs, T7 is late by 1 hr, and T5, T11, T4,
and T10 are again finished before their due dates. This means that the maximal
lateness occurs for job T3, so that Lmax ¼ 9. Had we broken the tie in favor of T9
rather than T8, task T9 would have been completed at time 34 (4 hrs late), and task T8
would have been finished at time 37 (7 hrs late). Otherwise, the schedule would have
been identical to that shown in Fig. 9.2, with Lmax ¼ 9 being still defined by job T3.
The discussion in this chapter may leave readers with the impression that single￾machine scheduling problems are easy. This is, however, not the case. Consider
again Example 2, but as an objective, we now use the total tardiness rather than the
maximal tardiness. In other words, our objective is now Lsum ¼ t1 + t2 + ... + t11
rather than Lmax ¼ min {‘1, ‘2, ..., ‘11}. We can find an optimal solution to the
problem by formulating an integer programming problem. In order to do so, we first
use variables t1, t2, ..., t11 for the tardiness of the individual tasks. We then define
variables x1, x2, ..., x11 for the actual starting times of the eleven tasks. Furthermore,
we need to define zero-one variables yij as
yij ¼ 1, if job Ti directly precedes job T j
0, otherwise 
,
where the subscripts i and j each can assume any value between 1 and 11, except that
i 6¼ j, resulting in 112 – 11 ¼ 110 zero-one variables yij in addition to the eleven
continuous variables t1, ..., t11 and the eleven continuous variables x1, ..., x11 for a
total of 132 variables.
Table 9.2 Data for Example 2
Job # T1 T2 T3 T4 T5 T6 T7 T8 T9 T10 T11
Processing time (hrs) 6 9 4 11 7 5 5 3 14 8 4
Due dates 25 15 32 70 55 10 45 30 30 80 58
Fig. 9.2 Gantt chart for Example 2
338 9 Machine SchedulingThe objective simply minimizes the sum of tardiness t1 + t2 + ... + t11. As far as
constraints are concerned, we first must define tardiness, which is done by stating
that tardiness of a job can be expressed as its starting time plus its processing time
(resulting in its finishing time) minus the due date, assuming that the resulting figure
is positive. As an example, consider job T1, whose tardiness can be written as
t1 ¼ x1 + p1  d1, provided that t1  0; otherwise, t1 is set to zero. This can be
achieved by writing x1 + p1  d1  t1, as whatever value the left-hand side of this
inequality assumes, the objective function will ensure that the value of t1 is chosen as
small as possible. This constraint is written for each of the eleven tasks separately.
We then have to ensure that a job must be completely finished before the next job
in line can begin being processed. Suppose that we have scheduled job T1 directly
before job T2. The pertinent constraint will then be x1 + p1  x2 + M(1  y12), where
M >> 0 is a suitably chosen large number. This constraint can be explained as
follows. The left-hand side expresses the finishing time of job T1, while the right￾hand side shows the starting time of job T2 plus M, if T1 is not scheduled before T2
and 0 if it is. In other words, if T1 is scheduled before T2, then this constraint requires
that the starting time of T2 is at least as large as the finishing time of T1 (i.e., T2
cannot start before T1 is finished), while in case that T1 is not scheduled before T2,
then the right-hand side of the inequality is very large, ensuring that it is satisfied
regardless of the values of the variables. The problem can then be written as follows:
P : Min z ¼ t1 þ t2 þ ... þ t11
s:t: x1 þ p1  d1  t1
⋮
x11 þ p11  d11  t11
x1 þ p1  x2  M 1  y12 ð Þ
⋮
x1 þ p1  x11  M 1  y1,11  
x2 þ p2  x1  M 1  y21 ð Þ
⋮
x11 þ p11  x10  M 1  y11,10  
x1, x2, ... , x1  0
t1, t2, ... , t11  0
y11, y12, ... , y1,11, y21, y22, ... y11,11 ¼ 0 or 1:
Note that the size of the integer programming problem for a scheduling model
with n tasks includes n2 – n zero-one variables, 2n continuous variables, and n2
structural constraints plus nonnegativity and zero-one conditions. It is apparent that
the formulation becomes unwieldy for large numbers of tasks, thus possibly requir￾ing the use of heuristic algorithms.
9.2 Single-Machine Scheduling Models 3399.3 Parallel Machine Scheduling Models
All scheduling models in this section have in common that the tasks can now be
processed on more than one machine. In general, we assume that a given number
m of machines are available. We further assume that these machines are identical in
the sense that not only can all machines process each of the tasks, but it takes the
same amount of time to process a task, regardless of the machine it is processed on.
First consider the objective of minimizing makespan Cmax. It can be demonstrated
that this problem is very difficult from a computational point of view, even for just
two machines. This means that we typically have to resort to heuristics to solve the
problem (except in cases, in which there is ample time to find exact solutions). The
most popular heuristic method for this type of problem is the longest processing time
first (or LPT) algorithm. This heuristic method belongs to the class of list scheduling
methods. All list scheduling methods first produce a priority list of tasks, starting
with the job that is assigned the highest priority. Using this list and starting with the
task that has the highest priority, jobs are then assigned one at a time to the first
available machine. In particular, the longest processing time first algorithm can be
described by the following rule:
LPT Algorithm: Put the tasks in order of nonincreasing processing times. Starting at the top,
assign the first available task to the first available machine. Repeat the procedure until all
tasks have been scheduled.
Example 3 In order to demonstrate the way the algorithm works, consider again
Example 1 above, but assume that now we have three equally qualified servicemen
available to perform the maintenance tasks. Putting the seven tasks in order of their
processing time, starting with the longest, we obtain the sequence T6, T4, T5, T3, T7,
T1, and T2 with processing times of 60, 50, 45, 40, 35, 30, and 25 min. In the
beginning, all three machines are available, so we first assign the longest task T6 to
machine P1. (Note that this machine will become available again at time 60, when T6
is completely processed.)The next task in line is T4, which is assigned to machine P2,
which is available now. (This machine will become available again at time 50, when
T4 is completely processed.) The next task in line is T5 and it is assigned to machine
P3. This machine will become available again at time 45. The next task to be
scheduled is now T3. The three machines become available again at 60, 50, and
45, so that assigning T3 to the next available machine means that it is scheduled on
P3. This process continues until all jobs are scheduled. The actual schedule is shown
in the Gantt chart in Fig. 9.3, where the shaded areas indicate the idle time on the
machines.
Note that the schedule length is Cmax ¼ 110. It is worth mentioning that this
schedule is not optimal, which is not surprising, since the LPT algorithm, which it
was determined with, is not an exact algorithm but a heuristic. Incidentally, the
optimal solution schedules T6 and T7 on machine P1, jobs T4 and T5 are processed on
340 9 Machine Schedulingmachine P2, and tasks T3, T1, and T2 are processed on P3. This schedule has no idle
time at all and all machines finish at time 95. Note that the optimality of a schedule
does not necessarily require that there is no idle time. On the other hand, if there is no
idle time on any of the machines, the schedule must obviously be optimal. However,
it is clear that if for instance, there are three machines and one of the jobs has a
processing time longer than one-third of the total processing times of all the jobs,
then every possible schedule must include some idle time on some machine.
Recall that the LPT algorithm is a heuristic algorithm and as such it is not
guaranteed to find an optimal solution, i.e., a solution that truly minimizes the
schedule length, or makespan, Cmax. Instead, we may try to find an estimate of
how far from an optimal solution the heuristic solution found by the LPT algorithm
might be. Such an estimate is conveniently defined in terms of a performance ratio
R ¼ Cmax(heuristic)/Cmax(optimum), where Cmax(heuristic) is the schedule length
obtained by the heuristic method and Cmax(optimum) is the true minimal schedule
length of the particular problem at hand. Since Cmax is a minimization objective,
Cmax(optimum)  Cmax(heuristic), so that R  1, and the smaller the value of R, the
closer the obtained schedule will be to the true minimum. It has been proved that the
performance ratio RLPT for the LPT algorithm applied to an n-job, m-machine
problem satisfies the inequality
RLPT  4
3  1
3m :
In Example 3 above, the 3-machine problem has a boundary value of
RLPT ¼ 1.2222, while the actual solution found deviates by 110/95 ¼ 1.1579, i.e.,
by 15.79% from the optimal solution.
For a two-machine (m ¼ 2) problem, the bound translates into a worst-case
performance bound of RLPT ¼ 4
3  1
3 2ð Þ ¼ 7=6  1:167, meaning that in the worst
case, the LPT algorithm will find a schedule that is 16.7% longer than that of the
optimal solution. The fact that this bound is actually tight (i.e., can be reached) is
shown in the following:
Example 4 Let a two-machine, five-job scheduling problem have the processing
times 3, 3, 2, 2, and 2, respectively. Applying the LPT heuristic to this problem
results in the schedule shown in Fig. 9.4a, whereas an optimal schedule for the same
problem is shown in Fig. 9.4b. With Cmax ¼ 7 for the LPT schedule shown in (a), and
Fig. 9.3 Gantt chart for Example 3
9.3 Parallel Machine Scheduling Models 341Cmax ¼ 6 for the optimal schedule shown in (b), we see that the worst-case bound of
RLPT ¼ 7/6 is actually achieved.
Having shown that the worst-case scenario can occur for the performance ratio
bound of RLPT ¼ 4
3  1
3m, we can look at the positive side and conclude that for a
two-machine problem an LPT schedule can never be poorer than 16.7% above the
optimal value Cmax. For a three-machine problem, this becomes slightly worse with
RLPT ¼ 4
3  1
3 3ð Þ ¼ 11=9  1:222, i.e., Cmax will never be more than 22% longer than
its optimal value (see Example 3 above). For four machines, we obtain 25%, and for
five machines 26.7%; for a large number of machines, the value approaches 331=3%.
When scheduling tasks on several parallel processors, it may sometimes be
possible and advantageous to allow preemption. In such a case, each task assigned
to a processor may be preempted, i.e., stopped, at any time and restarted later, at no
additional cost, on any (possibly another) processor. It turns out that preemption
makes the problem of minimizing the schedule length Cmax on parallel machines
easy. The so-called Wrap-Around Rule of McNaughton finds an optimal schedule
when preemption is permitted. More specifically, assume that there are m parallel
processors on which n tasks are to be performed with processing times pj, j ¼ 1,
2, ..., n. Clearly, no schedule exists with a makespan Cmax shorter than the longest of
the processing times max 1jn p j
 . It is also apparent that Cmax cannot be shorter than
the mean processing time of all jobs, i.e., 1
m
Pn
j¼1
p j. Therefore, we must have
Cmax  max max
1jn p j
 ,
1
m
Xn
j¼1
p j
( ):
The Wrap-Around Rule of McNaughton will actually achieve this lower bound.
The algorithm can be described as follows. Recall that there are m parallel machines,
n jobs with processing times p1, p2, ..., pn, and the objective is to determine a
schedule that minimizes the schedule length Cmax, while allowing the preemption
of jobs.
McNaughton’s Wrap-Around Rule: First sequence the tasks in some arbitrary order, thus
obtaining a sequence of length p1 + p2 + ... + pn time units. Then compute
Fig. 9.4 Nonoptimal (a) and optimal (b) schedule for Example 4
342 9 Machine SchedulingC
max≔ max max
1jn p j
 ,
1
m
Xn
j¼1
p j
( )
and break the time sequence at the points iC
max, i ¼ 1, 2, ... , m  1. Now schedule all
tasks in the interval ð Þ i  1 C
max; iC
max   on processor Pi, i ¼ 1, 2, ..., m, noting that
preempted tasks may be at the beginning and/or the end of each processor schedule. Finally,
any task that is preempted and accordingly processed on two different processors Pi and Pi+1
will have to be processed first on Pi+1, then preempted, and finished on Pi.
The Wrap-Around Rule can be shown to find a schedule whose length equals
C
max. It is clear that there will be no idle time on any of the processors, thanks to the
option of preempting tasks, unless there is any job with a processing time longer than
1
m
Pn
j¼1
p j.
Example 5 Consider the processing times for the eleven tasks in Example
2, but ignore the due dates. If the tasks are to be processed on two
processors (teams in our example) with a minimal schedule length, we first compute
C
max ¼ max max
1j11 p j
 , ½ P
11
j¼1
p j
( ) ¼ max p9 f g¼ , ½ð Þ 76 max 14, 38 f g¼ 38 .
The Wrap-Around Rule will then produce the optimal schedule displayed in Fig. 9.5.
In a practical application of this optimal schedule, job T6 would start on processor
P2 at time t ¼ 0 and preempted at t ¼ 4; it would then continue being processed on P1
at t ¼ 37 and finished at t ¼ 38.
Assume now that three processors (or teams) were available, and that the
processing time of job T9 increases from p9 ¼ 14 to p9 ¼ 34. We then obtain
C
max ¼ max 34, f g 1=3ð Þ 96 ¼ max 34, 32 f g ¼ 34. Some idle time is now inevitable
due to the long processing time p9. Using the Wrap-Around Rule, we obtain the
schedule displayed in Fig. 9.6.
For the preempted jobs in this optimal schedule, job T5 will commence being
processed at t ¼ 0 on P2, preempted at t ¼ 3, and then continued on P1 at t ¼ 30, until
it is finished at t ¼ 34. Job T9 will start being processed on P3 at t ¼ 0 and being
processed completely without being preempted, until it is finished at t ¼ 34 on P3.
Fig. 9.5 Optimal schedule for Example 5
9.3 Parallel Machine Scheduling Models 343Job T10 is being processed on P2 from t ¼ 16 to 24, immediately followed by T11
from t ¼ 24 to 28, after which P2 is idle until the end of the schedule at t ¼ 34.
Consider now the second of our objectives, which is to minimize mean flow time
F. It has been shown for the case of identical parallel processors, preemption is not
profitable. Therefore, we will consider only nonpreemptive schedules. The problem
can then be solved to optimality by means of a fairly simple technique. It assumes
that all tasks are ready at the beginning of the process, meaning that the mean flow
time reduces to the mean completion time.
Recall that there are n jobs and m machines. We can then formulate the following
method:
An algorithm for minimizing mean flow time: First sort the jobs in order of nondecreasing
processing time (ties are broken arbitrarily). Renumber them as T0
1, T0
2, ... , T0
n. Then assign
tasks T0
1, T0
1þm, T0
1þ2m, ... to machine P1, tasks T0
2, T0
2þm, T0
2þ2m, ... to machine P2, tasks
T0
3, T0
3þm, T0
3þ2m, ... to machine P3, and so forth. The tasks are processed in the order that
they are assigned in.
In order to illustrate this algorithm, consider again Example 1 above.
Recall that the reordered sequence of tasks is T0
1, T0
2, T0
3, T0
4, T0
5, T0
6, T0
7
  ¼
ð Þ T2, T1, T7, T3, T5, T4, T6 with the processing times (25, 30, 35, 40, 45, 50, 60).
Given that we have three machines, we assign to machine P1 the tasks T0
1, T0
4, and T0
7
(or, renumbering them again, T2, T3, and T6), machine P2 is assigned the jobs T0
2 and
T0
5 (i.e., T1 and T5), and machine P3 will process jobs T0
3 and T0
6, i.e., T7 and T4. The
resulting optimal schedule is shown in Fig. 9.7.
The mean completion time of the solution in Fig. 9.7 is F ¼ 1
7 
½ ¼ 25 þ 65 þ 125 þ 30 þ 75 þ 35 þ 85 440=7 ffi 62:8571 . The seemingly
Fig. 9.6 Optimal schedule with three processors
Fig. 9.7 Optimal schedule for minimizing mean flow time
344 9 Machine Schedulingstraightforward extension to the model with nonidentical ready times renders the
problem very difficult from a computational point of view. This is yet another
indication how fine a line there is between (sometimes very) easy and (sometimes
very) difficult problems.
Finally, the minimization of the maximal lateness in case of parallel machines
turns out to be difficult, and we leave its discussion to specialized books, such as
Eiselt and Sandblom (2004).
9.4 Dedicated Machine Scheduling Models
This section deals with different types of dedicated machine scheduling models. The
first such model includes an open shop. Recall that in an open shop, each task must
be processed on each of a number of different machines, performing different
operations. The sequence of machines, in which the jobs are processed, is immate￾rial. Here, we will deal only with the case of two machines, which happens to be
easy, while problems with three or more machines are difficult. Minimizing the
schedule length (makespan) Cmax is easy. Optimal schedules can be found by means
of the Longest Alternate Processing Time (LAPT) algorithm. It can be described as
follows:
LAPT Algorithm: Whenever a machine becomes idle, schedule the task on it that has the
longest processing time on the other machine, provided the task has not yet been processed
on that machine and is available at that time. If a task is not available, the task with the next
longest processing time on the other machine is scheduled. Ties are broken arbitrarily.
In order to illustrate the method, consider:
Example 6 In an automotive assembly shop, each semi-finished product goes
through two phases, assembly of individual components, and checking of the
components. The sequence in which these tasks are performed is immaterial. The
times (in minutes) that it takes to assemble and check the six products are shown in
Table 9.3.
Using the LAPT algorithm, we begin by scheduling a task on machine P1. The
task with the longest processing time on P2 is T3, so this job is scheduled first on P1.
The next step is to schedule a task on P2 which is now (we are still at time zero) idle.
The task with the longest processing time on P1 is again T3, which is not available at
this time, so that we schedule the task with the next longest processing time on P1
next. This is either T1 or T4. Arbitrarily choose T1. With tasks T3 and T1 being
scheduled on the two machines, T1 is the first task whose processing is finished at
Table 9.3 Data for
Example 6
Job # T1 T2 T3 T4 T5 T6
Processing time on P1 30 15 40 30 10 25
Processing time on P2 35 20 40 20 5 30
9.4 Dedicated Machine Scheduling Models 345time 35, and machine P2 becomes idle again. At this time, the available task with the
next longest processing time on P1 is T4, which is then scheduled next on P2. The
process continues in this fashion, and the resulting optimal schedule is shown in
Fig. 9.8, where the shaded areas toward the end again indicate idle times on the
machines. The total schedule length is Cmax ¼ 155 min.
There are no simple extensions of this model that are computationally easy. The
other two objectives (i.e., those that minimize mean completion time and maximal
lateness) are both very difficult from a computational point of view, even for two
machines. For their discussion, we refer to the advanced literature on the subject.
The second dedicated machine scheduling model is a flow shop model, i.e., a
model in which each task has to be processed by all machines, but in the same,
prespecified order. The schedule in Fig. 9.8 does not satisfy this condition; note that
for instance job T3 is processed on P1 first and later on P2, while task T1 is processed
on P2 first and then on P1. Similar to the case of open shops, there are very few cases
that are easy to solve. Among them is the case of two machines, for which the
makespan is to be minimized. The solution algorithm is the famed Johnson’s rule
that was first described in the early 1950s. Assuming that all jobs have to be
processed on P1 first and then on P2, and the processing time of task Tj is p1j on
machine P1 and p2j on machine P2, the algorithm can be summarized as follows:
Johnson’s Algorithm: For all jobs whose processing time on P1 is the same or less than their
processing time on P2 (i.e., p1j  p2j), determine the subschedule S1 with the tasks in
nondecreasing order of their p1j values. For all other jobs (i.e., tasks for which p1j > p2j),
determine the subschedule S2 with all tasks in nonincreasing order of their p2j values. The
sequence of jobs is then (S1, S2).
Example 7 As a numerical example for Johnson’s rule, consider again the scenario
in Example 6, but with the proviso that each job has to be assembled first and then it
is checked, i.e., it is processed on machine P1 first before it can be processed on P2.
The tasks for which the condition p1j  p2j holds are T1, T2, T3, and T6, while the jobs
with p1j > p2j are T4 and T5. Putting the former four tasks in nondecreasing order of
the processing times on P1 results in the subsequence S1 ¼ (T2, T6, T1, T3), while
the latter two jobs in nonincreasing order of their processing time on P2 are put in the
sequence S2 ¼ (T4, T5). The resulting sequence is (T2, T6, T1, T3, T4, T5), and the
corresponding schedule is shown in the Gantt chart in Fig. 9.9.
Fig. 9.8 Optimal schedule for Example 6
346 9 Machine SchedulingIt is apparent that the schedule length is Cmax ¼ 175. Note the increase in the
schedule length in comparison to the same example, in which the sequence of
processing in the machines is not fixed (i.e., the open shop) shown in Fig. 9.8.
Given that a flow shop is more restrictive than an open shop (it has the additional
constraint that all tasks have to be performed in the same order on the machines), the
increase of the schedule length from 155 to 175 min is not surprising.
Simple extensions of the problem to more than two machines as well as the
application of the mean flow time and the minimization of tardiness are computa￾tionally much more difficult. Again, we refer readers to the specialized literature.
The last model in this chapter deals with a job shop. Recall that by definition of a
job shop, not all tasks need to be performed on all machines, and the sequence in
which a job is processed on the machines is job specific. Again, due to the inherent
complexity of the problem at hand, we will restrict ourselves to the problem with two
machines and the objective that minimizes the makespan. For this type of problem,
Jackson described an exact algorithm in 1955. Note that Jackson’s method uses
Johnson’s (flow shop) algorithm as a subroutine. The method can be described as
follows:
Jackson’s Job Shop Algorithm: Subdivide the set of jobs into four subcategories:
J1 includes all jobs that require processing only on machine P1,
J2 includes all jobs that require processing only on machine P2,
J12 is the set of jobs that require processing on machine P1
first and then on P2, and
J21 is the set of jobs that need to be processed on P2 first
and then on P1.
Apply Johnson’s rule to the jobs in the set J12, resulting in the sequence S12. Then apply
Johnson’s rule to the jobs in the set J21, but with the processing times p1j and p2j exchanged.
The result is the subsequence S21. All jobs in the two sets J1 and J2 are sequenced in arbitrary
order (e.g., with those jobs that have smaller subscripts scheduled first). We denote their
sequences by S1 and S2
, respectively. The jobs are then sequenced as follows: the job order
on machine P1 is (S12, S1
, S21), while the job order on machine P2 is (S21, S2
, S12).
Example 8 As a numerical illustration, we will use a modification of Example
6. The pertinent information is found in Table 9.4.
First defining the sets, we have J1 ¼ {T4}, J2 ¼ {T3}, J12 ¼ {T2, T5}, and
J21 ¼ {T1, T6}. Since J1 and J2 include only a single task each, their respective
subsequences are S1 ¼ (T4) and S2 ¼ (T3). Applying Johnson’s algorithm to the tasks
Fig. 9.9 Optimal schedule for Example 7
9.4 Dedicated Machine Scheduling Models 347in the set J12, we obtain the sequence S12 ¼ (T2, T5). Consider now the set J21.
Applying Johnson’s rule to the two tasks in the set, viz., T1 and T6, with their
processing times p1j and p2j switched, we obtain the subsequence S21 ¼ (T1, T6).
Following Jackson’s job shop algorithm, the overall sequence on machine P1 is then
(S12, S1
, S21) ¼ (T2, T5, T4, T1, T6), while the overall sequence on machine P2 is (S21,
S2
, S12) ¼ (T1, T6, T3, T2, T5). The appropriate Gantt chart is shown in Fig. 9.10. It
turns out that the overall schedule length Cmax ¼ 130 min. We also note that
processor P1 is idle for 20 min at the end of the schedule.
Exercises
Problem 1 (single- and parallel machine scheduling): A machine scheduling
problem has ten jobs with given processing times of 3, 1, 4, 1, 5, 9, 2, 6, 5, and
3 hrs, respectively.
(a) Assume that the jobs are to be performed on a single machine and that the
objective is to minimize mean flow time, find an optimal schedule, and draw the
corresponding Gantt chart.
(b) Assume that due dates for the ten jobs are given as 14, 4, 10, 19, 40, 32, 13,
31, 7, and 26 hrs, respectively. Use the earliest due date algorithm to find a
schedule that minimizes the maximal lateness Lmax. Show the corresponding
Gantt chart. What is the minimal value of Lmax, and for which job does it occur?
Which jobs are late?
(c) Assume that there are two parallel machines to process the jobs. Trying to
minimize the schedule length, schedule the jobs using the LPT algorithm.
Display the corresponding Gantt chart. What is the performance ratio?
(d) Given two parallel machines as under (c), what is the schedule that minimizes
the mean flow time? Display the corresponding Gantt chart.
(e) Reconsider questions (c) and (d) given that there are now three rather than two
parallel machines.
Table 9.4 Data for Example 8
Job # T1 T2 T3 T4 T5 T6
Processing time p1j 30 15 – 30 10 25
Processing time p2j 35 20 40 – 5 30
Processing sequence P2, P1 P1, P2 P2 P1 P1, P2 P2, P1
Fig. 9.10 Optimal schedule for Example 8
348 9 Machine SchedulingSolution:
(a) Assuming ready times rj ¼ 0, the mean flow time F is minimized by the shortest
processing time (SPT) algorithm. The sequence of jobs is T2, T4, T7, T1, T10, T3,
T5, T9, T8, and T6. The corresponding Gantt chart is shown in Fig. 9.11.
The minimal mean flow time for this schedule is then F ¼ 1
10 
ð Þ¼ 1 þ 2 þ 4 þ 7 þ ... þ 30 þ 39 15. Since T1 and T10 both have processing
times of 3 hrs each, they may be swapped in the optimal schedule, thus creating
alternative optimal solutions. A similar argument applies to the pairs T2 and T4,
and T5 and T9.
(b) Using the EDD algorithm, we obtain the optimal schedule shown in Fig. 9.12.
It turns out that task T1 is late by 1 hr and task T6 is late by 2 hrs. All other tasks
are finished on time (but T3 only just). The lateness Lmax therefore equals 2 hrs,
obtained for T6.
(c) With two machines P1 and P2, the longest processing time LPT algorithm
minimizes the makespan Cmax.
Fig. 9.11 Optimal schedule for Problem 1 (a)
Fig. 9.12 Optimal schedule for Problem 1 (b)
Fig. 9.13 Optimal schedule for Problem 1 (c)
9.4 Dedicated Machine Scheduling Models 349The makespan is 20 hrs. In the schedule shown in Fig. 9.13, one of the machines
is idle for 1 hr.
(d) With two machines, find a schedule that minimizes the mean flow time. The
Gantt chart is shown in Fig. 9.14.
The schedule length is 22 hrs, and the mean flow time is F ¼ 1
10 
ð Þ¼ 1 þ 1 þ 3 þ 4 þ ... þ 17 þ 22 8:6 , and one processor is idle for
5 hrs. Since the schedule is optimal, the performance ratio equals 1.
(e) With three machines, we use the LPT rule as a heuristic to minimize Cmax. The
Gantt chart is shown in Fig. 9.15.
The schedule has a makespan of 13 hrs. Since there is no idle time, the schedule
must be optimal. The schedule is shown in Fig. 9.16.
The schedule length is 18 hrs, and the mean flow time is F ¼ 6.6. Note that
there is significant idle time.
Problem 2: (Parallel machine scheduling with preemption): Consider Exam￾ple 5 in Sect. 9.3.
(a) Using the original data, apply the Wrap-Around Rule to find a schedule that
minimizes makespan, assuming that preemption is allowed and that there are
three, rather than two, processors.
(b) Consider again the example as under (a). Using the increased processing time of
job T9, solve the problem with two, rather than three, processors.
Fig. 9.14 Optimal schedule for Problem 1 (d)
Fig. 9.15 Optimal schedule
for Problem 1 (e)
Fig. 9.16 Minimizing mean flow time for problem 1 (c)
350 9 Machine SchedulingSolution:
(a) First, we compute C
max ¼ max max
1j11 p j
 , 1=3
P
11
j¼1
p j
( ) ¼
max p9 f g , 1=3ð Þ 76 ¼ max 14, 25 f g 1=3 ¼ 251=3 . Using the Wrap-Around Rule
we then obtain the schedule displayed in the Gantt chart in Fig. 9.17.
An implementation of this schedule would start T4 on P2, preempt it at t ¼ 42=3,
and then resume T4 on P1 at time t ¼ 19. Task T9 then starts at P3 at t ¼ 0 and is
then preempted at t ¼ 131=3, after which it resumes at t ¼ 242=3 on P2.
(b) We now find C
max ¼ max 34, f g¼ ½ð Þ 96 48 , and the Wrap-Around Rule
generates the schedule shown in Fig. 9.18.
For a practical implementation of this schedule, task T8 will be started on
processor P2 at time t ¼ 0, will be preempted at t ¼ 2, and then resumed on
P1 at time t ¼ 47.
Problem 3 (open shop and flow shop scheduling): In a hospital laboratory,
there are two machines testing patient blood samples. Table 9.5 shows the number of
minutes required on each machine to process the samples.
(a) Assume that the order of processing the blood samples on the two machines is
arbitrary. Schedule the testing on the two machines so as to minimize the
Fig. 9.17 Schedule for Problem 2 (a)
Fig. 9.18 Schedule for Problem 2 (b)
Table 9.5 Data for
Problem 3 Blood sample T1 T2 T3 T4 T5
Processing time on P1 89793
Processing time on P2 23846
9.4 Dedicated Machine Scheduling Models 351schedule length. Display the optimal schedule in a Gantt chart. Indicate the
schedule length as well as the idle time.
(b) Assume now that all blood samples must be processed on P1 before they can be
processed on P2. Redo part (a) with these new assumptions.
(c) Are the optimal schedules in (a) and (b) unique? Discuss. There is no need to
display Gantt charts.
Solution:
(a) The LAPT algorithm is used to obtain the optimal schedule shown in Fig. 9.19.
The minimal schedule length is Cmax ¼ 36 min. There is no idle time on P1,
while P2 is idle at the end for 13 min. This is an open shop model.
(b) Johnson’s rule is used to obtain the optimal schedule shown in Fig. 9.20.
The minimal schedule length is now Cmax ¼ 38 min. Processor P1 has an idle
time of 2 min at the very end of the schedule, whereas P2 has five separate idle
time periods, totaling 15 min. This is a flow shop model.
(c) In (a), tasks T2 and T4 could swap positions in the schedule of P1 (and on P2 for
that matter) without consequences regarding the schedule length. There are
several other changes that would not destroy optimality. In (b), tasks T2 and
T4 could also swap positions on P1, necessitating modifications on processor P2.
Reference
Eiselt HA, Sandblom C-L (2004) Decision analysis, location models, and scheduling problems.
Springer, Berlin
Fig. 9.19 Schedule for Problem 3 (a)
Fig. 9.20 Schedule for Problem 3 (b)
352 9 Machine SchedulingDecision Analysis 10
Everywhere in the world, at each moment, millions of people make their own
decentralized decisions: when to get up in the morning, what tie to wear, what to
eat for lunch or dinner, what to do in the evening (go to the theater or watch
television), where to vacation, and many more. Similarly, firms will decide which
mode of transportation to use when routing their products to customers, where to
locate regional distribution centers, what new product lines to develop, etc. This
chapter first introduces the main elements of decision analysis, and then offers some
visualizations of decision analysis problem. This is followed by a discussion of some
simple decision rules, sensitivity analyses, and a discussion of the value of informa￾tion. The chapter wraps up the discussion by some thoughts on utility theory.
10.1 Introduction to Decision Analysis
In order to put decision making into a general framework, we must first distinguish
between the three major elements of decision analysis: the finite set of decisions
available to the decision maker (whom we will think of as “us”), the states of nature,
which are the decisions available to our opponent (more about that below), and the
outcome that results from the combination of our decision and that of our opponent.
Typically, the outcome is given in monetary terms, and it is usually referred to as the
payoff.
The type of problem under consideration in this chapter is not a philosophical
investigation into the decision-making process; instead, it refers to a very specific
scenario that is prevalent in decision-making circumstances. In particular, we
assume that there are a finite number of decisions at our disposal. Among these
choices, the decision maker’s task is to choose exactly one. This type of situation is
often referred to as a selection problem. Returning to the capital budgeting decision
in the introduction to integer programming in Chap. 5, we can define a binary
variable yj for each decision, such that the variable assumes a value of one if we
# The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
H. A. Eiselt, C.-L. Sandblom, Operations Research, Springer Texts in Business and
Economics, https://doi.org/10.1007/978-3-030-97162-5_10
353make decision j, and 0 otherwise. A selection problem with n possible decisions will
then feature the constraint y1 + y2 + ... + yn ¼ 1.
However, in contrast to problems that can simply be formulated as integer
programming problems, there are two possible extensions that typically arise. The
first extension involves the evaluation of a decision on more than a single criterion.
As an example, consider a department store that considers changing the layout of its
store. There are, of course, the costs of such a decision. But there is more: there is the
changed customer flow that may result in higher exposure of some goods to
customers and resulting potential higher sales of these products (the main reason
for a change in layout), the (temporary) confusion of customers who may refrain
from purchasing at the store, the potential retraining of employees if the layout
change was in conjunction with new products added to the goods available at the
store, and so forth. Selection problems of this type, in which multiple attributes of
decisions are considered, are called multiattribute decision-making problems
(or MADM). Problems of this type are discussed in Chap. 11. On the other hand,
suppose that we only consider a single criterion, but the outcome of our decision is
no longer certain. For instance, consider a major public project, such as the con￾struction of a new airport or a new sewage treatment plant for a major city. There are
the uncertainties of future inflation rates, the potential of strikes, possible future
changes of interest rates, major weather disruptions, and a myriad of other unfore￾seen events that can and most likely will change the finishing time of the project and
its cost. This is the standard scenario in decision analysis or, as it is frequently called,
games against nature. The name stems from game theory and can be explained as
follows. Consider a standard two-person game with two players, one decision maker
(us), and the other being our opponent (nature). Each of the two players has a number
of possible actions at his disposal: the decision maker has the decisions, while nature
controls the “states of nature,” nature’s equivalent of the decision maker’s decision
choices. This is the scenario examined in this chapter.
There is, however, a fundamental asymmetry in games against nature. First of all,
the combination of the decision maker’s choice and nature’s state of nature will
determine the outcome for the decision maker (nature will face no outcome).
Secondly, the decision maker will examine the possible outcomes of his decisions
before choosing one, while nature does not consider the outcomes, but chooses her
strategies according to some probability distribution. This is the reason why the
decision maker is usually called “intelligent” (we prefer to think of it as “rational”),
while nature is referred as a “random player.”
As an illustration, consider the following numerical example with three decisions
d1, d2, and d3, and four states of nature s1, s2, s3, and s4. The payoffs are shown in
Table 10.1.
Table 10.1 Payoffs s1 s2 s3 s4
d1 3 24 6
d2 2 0 4 1
d3 52 0 3
354 10 Decision AnalysisThe decision maker could argue that d1 is best, as it provides a reasonable payoff
for three states of nature, while its largest possible loss is 2 and as such not as bad
as the losses that can occur with the other two decisions. If the decision maker were
to choose d1, while nature would randomly choose s3, then the decision maker would
obtain a payoff of 4.
It is important to realize that nature’s decision is either made simultaneously with
that of the decision maker without cooperation, or, equivalently, the decision maker
chooses first, followed by nature’s choice. As in all game-theoretic situations, it is
crucial to specify which player knows what and when. This also marks the distinc￾tion concerning the level of certainty the decision maker has about nature’s choice.
As usual, consider the extremes first. If the decision maker knows with certainty
what nature is going to do, we face a decision problem under certainty. Given the
scenario of selection problems, this means that the decision maker knows which
column of the payoff matrix will apply. In the example of Table 10.1, suppose that
the decision maker knows that nature will choose s1. This means that the
consequences of the decisions are known with certainty: choosing d1 will result in
a payoff of 3, choosing d2 will result in a payoff of 2, and choosing decision d3 will
yield a payoff of 5. Clearly, the decision maker’s payoff is maximized by the payoff
of 5, which we arrive at by choosing d3. This is the optimal solution, and the problem
is solved. (Before we continue with different levels of knowledge, note that the
decision maker has jurisdiction only over his choices, e.g., d3, but he cannot choose
the payoff directly.)
On the other extreme is uncertainty. In decision making under uncertainty, the
decision maker has absolutely no idea about nature’s choice. Uncertainty is quite
rare; it may occur in the performance of new and untried products, behavior of
customers in new and untested markets, and other situations, in which no informa￾tion is provided. As in all decision-making situations, if the level of input into the
problem is low, the output will have to make do with simplistic rules. This is
precisely what happens in this situation as will be seen below.
Clearly, there is much territory between certainty and uncertainty. One milestone
in between is decision making under risk. In decision making under risk, the decision
maker is assumed to know the probability distribution used by nature. Examples
would include past weather observations for farmers, predictions concerning cus￾tomer behavior based on similar situations, and so forth. Rules for decision making
under risk are described below.
10.2 Visualizations of Decision Problems
Before getting into specific rules for different situations, we would like to describe
some ways to visualize decision-making scenarios. Different visualizations on
different levels are available. For the macro view, there are influence diagrams.
The idea is to show the basic interdependencies between decision and outcome,
while ignoring details. Their biggest strength is clarity, which is achieved by their
concentration of fundamental relations and their resulting small size. On the other
10.2 Visualizations of Decision Problems 355hand, decision trees are used for the micro view. They include specific decision rules
at each step of the way. Consequently, they tend to be large and cumbersome to
deal with.
In order to demonstrate influence diagrams, we first make a list with three
columns. The first column includes the decision maker’s possible decisions, the
second column represents the random events that somehow influence the decisions
and the outcomes, and the last column includes the consequences that are a result of
the decisions and the random events. As an illustration, consider a department store
that contemplates adding an electronics department to its services. In case the
introduction is accepted by the public, management considers relocating that depart￾ment into a separate building. The aforementioned listing is shown in Table 10.2.
In order to visualize the problem, we can create an influence diagram, in which
our decisions are shown by rectangles, random events are shown as circles, and
consequences are shown by triangles. We then add directed arcs, so that an arc from
some node i to another node j exists, if it is believed that i influences j. The influence
diagram for our problem could look as shown in Fig. 10.1.
The broken arcs in the figure are somewhat tenuous: they indicate the belief that
local acceptance of an electronics department or store is influenced by the existence
of an electronics department in our department store and our competitors’ reaction to
our introduction of the department.
On the other hand, we could zoom in and outline our decisions, our competitors’
decisions, and economic conditions in a decision tree. Decision trees have decisions
and events listed next to the arcs. A square node indicates that we will make a
decision, a round node means that a decision is made not within our control (i.e., a
random event), and a triangular node denotes the end of this branch of the tree.
Normally, there will be an indication next to a triangular node what the payoff is to
us at that point. For now, we will concentrate on the structure of the tree and return to
the numerical aspects of decision trees in a detailed discussion in Sect. 10.5. A
possible decision tree for our sample problem is shown in Fig. 10.2.
The figure of the decision process indicates that after we have decided to expand,
our competitor(s) could either not react, also expand their stores accordingly, or
build a superstore. While these are not random events, they are shown here as such as
these decisions are not within our control. In case our competitors do not react, our
profit will depend on the state of the economy, which is shown in the Figure simply
as “up” or “down.” (This exceedingly simplistic notion has been used so as to save
space―it should have become clear by now that decision trees tend to get very large
even if the players do not have very many options.) In case our competitor(s) expand,
we can either do not react ourselves or build the planned new building. Each of our
decisions will be followed by a random event, at which nature decides which turn the
economy takes. Finally, if the competitors have decided to build a superstore, we
Table 10.2 Decision, events, and consequences for the influence diagram
Decision Random event Consequence
Add electronics department General economic conditions Profit
Relocate department into a separate building Local acceptance of services
356 10 Decision Analysismay either withdraw due to limited options to raise capital or build the planned
addition. Again, each decision is followed by the economy going up or down.
10.3 Decision Rules Under Uncertainty and Risk
Our discussion below will be based on a numerical example with the payoff matrix
s1 s2 s3
d1 2 2 5
d2 0 1 7
d3 21 1
d4 2 3 4
Before performing any calculations, it is useful to first examine the payoff matrix
regarding dominances. A decision i (indicating a row) dominates a decision
k (a different row) if all elements in row i are at least as large as the corresponding
payoffs in row k. In other words, for each state of nature, decision i is at least as good
as decision k. If this is the case, then decision k can be deleted from consideration.
Fig. 10.1 Influence diagram
10.3 Decision Rules Under Uncertainty and Risk 357The determination whether or not dominances exist in a problem requires pairwise
comparisons. In our example, let us first compare decisions d1 and d2. Given the first
state of nature s1, decision d1 results in a payoff of 2, while d2 nets us only 0, so d1 is
preferred. However, given s2, decision d1 results in a loss of 2, while d2 results in a
loss of only 1, so that d2 is preferred. This means that neither decision is consistently
better than the other. Comparing d2 and d3 also results in no dominance (d3 is
preferred in case s1 or s2 occurs, while d2 is preferred to d3 in case s3 comes up).
However, the picture changes when comparing d1 and d4. Here, it is apparent that d1
and d4 are equally good given s1, while d1 is better than d4 in case of s2 and s3, so that
d1 dominates d4; thus, d4 can be deleted. The examination would have to continue
comparing the remaining pairs of decisions.
Given m decisions, ½m(m  1) pairs of decisions will have to be compared. If any
dominances are missed by accident, no harm is done: the model will be a bit bigger
Fig. 10.2 Decision tree for sample problem
358 10 Decision Analysisthan it needs to be, but no “reasonable” rule will choose a dominated decision. Note
that we cannot apply the concept of dominance to the states of nature (i.e., the
columns of the payoff matrix). The reason is that the concept of dominances is based
on a comparison of the payoffs, i.e., a rational decision maker, and nature is no such
player.
Consider now a decision-making problem under uncertainty. One simple decision
rule will attempt to guard against the worst case. It has a variety of names, Wald’s
rule named after its inventor, the pessimistic rule based on the mindset of the
decision maker, and the maximin rule based on the way the rule works. First of
all, we will determine the anticipated payoffs associated with our decisions. In our
example, a pessimist choosing d1 would assume that the worst case applies and the
payoff will be 2. Similarly, decision d2 would result in a payoff of 1, and so forth.
The vector of anticipated payoffs would then be a ¼ [2, 1, 1, 3]T
. Formally,
these are the row minima of the payoff matrix. Choosing the best among these
decisions will then be d3 as it leads to the maximum payoff among the anticipated
payoffs. This is the reason for calling the rule a “maximin” rule.
While the rule surely protects against the worst case, it has a number of
shortcomings. The most predominant problem with it is its exclusive focus on the
worst case. For instance, if someone had to pick up his multimillion-dollar winnings
from the lottery office, the worst-case rule would suggest that he not do that: while
walking or driving to the office, he might get hit by a truck and die, and this worst
case has to be avoided. Since it is unknown how likely (or, in this case, how
unlikely) such an incident would be, a decision made on the basis of Wald’s rule
would try to prevent it.
Another, very similar, rule is the optimist’s rule. An optimist’s anticipated payoff
would include the best possible payoff for each decision, i.e., the row maxima. In our
example they are a ¼ [5, 7, 2, 4]. The optimist would then choose the decision with
the highest anticipated payoff, leading to a maximax rule. In our illustration, d2
would be the optimist’s choice. This Pollyanna-inspired rule suffers from the same
limitations as Wald’s rule, except that it has replaced guarding against the worst case
by anticipation of the best case.
A third rule for making decisions under uncertainty was independently developed
by Savage and Niehans. It is called the minimax regret criterion, and it has become
the basis of what is often referred to as robust optimization. The idea is to compare
for each state of nature the payoff the decision maker gains with his decision and the
best possible payoff that could have been obtained given the same state of nature. In
our numerical example, we compute the regret of decision d2 given the second state
of nature s2. The payoff to the decision maker is 1. However, if the decision maker
had just known in advance which state of nature would occur (the second), he could
have his best response d3, which would have led to a payoff of 1, the highest payoff
given s2. The difference between the actual payoff and the best possible payoff under
that state of nature gives a regret of 1  (1) ¼ 2. These regrets are calculated for all
pairs of decisions and states of nature, and they form the regret matrix. In our
example, the regret matrix is
10.3 Decision Rules Under Uncertainty and Risk 359R ¼
032
220
006
043
2
6
6
6
4
3
7
7
7
5
:
Note that regrets are never negative. Furthermore, there is always at least one zero
in each column of the regret matrix (belonging to the element that determines the
column maximum). The decision maker can then apply any rule on the regret matrix
rather than the original payoff matrix. If we were to use the pessimist’s rule onto the
regret matrix R, we would anticipate regrets of r ¼ [3, 2, 6, 4]. Note that these are the
row maxima, not the row minima as used above. The reason is that a payoff matrix
includes payoffs that the decision maker would like to maximize, whereas the regret
matrix features regrets that, similar to costs, the decision maker would like to
minimize. Among the anticipated regrets, the decision maker will then choose the
lowest, which in our example is d2, a decision leading to an anticipated regret of
2. Applied to the regret matrix, Wald’s rule is a minimax rule in contrast to the
maximin version that is used in case a payoff matrix is given.
Consider now decision making under risk. In this scenario, we can associate with
each state of nature a probability pj, which has been determined by past observations.
Bayes’s rule is then used to compute the expected values and choose the decision
that leads to the maximum expected payoff, making Bayes’s criterion a weighted
maximum rule. In our example, suppose that the probabilities of the three states of
nature have been determined as p ¼ [0.5, 0.3, 0.2]. The expected payoffs (or expected
monetary values EMV) are then
EMVð Þ¼ d1 2ð Þ :5 2ð Þþ :3 5ð Þ¼ :2 1:4,
EMVð Þ¼ d2 0ð Þ :5 1ð Þþ :3 7ð Þ¼ :2 1:1,
EMVð Þ¼ d3 2ð Þþ :5 1ð Þþ :3 1ð Þ¼ :2 1:5, and
EMVð Þ¼ d4 2ð Þ :5 3ð Þþ :3 4ð Þ¼ :2 0:9:
Thus, the anticipated payoffs are 1.4, 1.1, 1.5, and 0.9, so that the decision maker
will choose d3, a decision that has the highest expected payoff of 1.5. In case of a tie,
it is possible to use secondary criteria.
Bayes’s rule has been popularized by what is known as the Newspaper Boy
Problem. The decision of the newspaper boy concerns the number of newspapers he
will purchase in the presence of demand uncertainty. The probabilities of the demand
are based on past experience. If the boy buys too many papers on a slow day, he will
have papers left over, which will have to be disposed of for some low salvage value.
On the other hand, if he purchases too few and the paper turns out to have some
interesting stories, he will not have enough papers to sell, so that not only does he
lose business today, but may irritate customers who may purchase their papers
elsewhere in the future. These opportunity costs also have to be included in the
model.
360 10 Decision AnalysisAs an illustration, consider the following numerical:
Example: A newspaper boy knows that he can sell either 10, 20, 30, or
40 newspapers on any given day (barring days with major headlines, such as
assassinations, wars, or the latest replacements of body parts of some actress). The
boy will purchase a newspaper for 20¢ and then sell for 90¢. The salvage value of an
unsold newspaper is 5¢, while the opportunity cost for newspapers has been
estimated to be 15¢ for each newspaper that could have been sold but was not,
due to the lack of supply. The payoff matrix for the newsboy problem is then:
where the decisions d1, d2, d3, and d4 refer to the newspaper boy buying 10, 20,
30, and 40 newspapers, while the states of nature s1, s2, s3, and s4 refer to a demand
of 10, 20, 30, and 40, respectively. (Note that it is generally not necessary that the set
of decisions equals the set of possible states of nature.) All entries on the main
diagonal refer to cases, in which the sale equals the number of newspapers that were
purchased, while all entries above the main diagonal involve some unsatisfied
demand with its opportunity costs, while the entries below the main diagonal involve
a surplus of newspapers, so that salvage values have to be applied. Given
probabilities of p ¼ [0.6, 0.2, 0.1, 0.1] for the four states of nature, the four decisions
have expected payoffs of $5.95, $8.45, $8.95, and $8.45, so that the newspaper boy
should buy 30 newspapers and expect an average daily payoff of $8.95.
Applications of this type occur in many circumstances, in which we are dealing
with perishable goods. An excellent example is the airline industry. Seats in an
airplane are a perishable commodity, as, once the airplane door closes, an empty seat
is worthless. The airline’s decision problem is then to choose among their aircraft the
type that best fits the expected demand on each of the routes.
Let us now return to our discussion of different approaches to decision making. In
decision making under risk, there is also the possibility to use target values. The idea
with this approach is to choose the decision that provides the highest probability that
the payoff does not fall short of a predetermined target value. To illustrate, use again
the example introduced earlier in this section. Suppose that a target value T ¼ 1 has
been chosen. Decision d1 will then achieve this target value only if nature chooses
either s1 or s3 as her strategy, which will happen with a probability of 0.5 and 0.2,
respectively. This means that when using d1, there is a probability of 0.7 that the
target value is reached or exceeded.
When using decision d2, the target value T ¼ 1 will be achieved only if s3 comes
up, which happens with a probability of 0.2. Similarly, decision d3 reaches the target
value in case nature plays s2 or s3, so that the probability of a payoff of at least
10.3 Decision Rules Under Uncertainty and Risk 361T equals 0.5, and for d4 the probability is 0.7. The decision maker will then choose
the decision that maximizes the probability of getting at least T ¼ 1, which is done by
choosing either d1 or d4. Note that one of those optimal choices is the dominated
decision d4. This is possible; however, a dominated decision can never be the unique
optimum for any “reasonable” decision rule.
In order to derive a simple decision tool, we plot the probability that a decision
achieves at least a prespecified target value T against the full range of target values T.
For our example, Fig. 10.3 provides the graph in question. In particular, the solid line
shows the achievements of decision d1, the broken line is for d2, and the dotted line
shows the results for d3. For clarity, we ignore the dominated decision d4.
Given the graph in Fig. 10.3, we can very simply determine which decision has
the highest probability of reaching a target value. For instance, if the target value
were T ¼ 1.5, then decisions d2 and d3 both have a probability of “1” to achieve
this value, while decision d1 has only a probability of 0.7 of achieving this payoff. In
other words, we are interested in the “upper envelope,” i.e., the highest of all of the
functions. Given that, we can determine which function is highest and summarized it
in the following decision rule:
If T < 2, any decision will achieve the target.
If T 2 [2, 1], d2 and d3 are best. Both decisions will reach the target with a
probability of 1.
If T 2 [1, 1], d3 is best. It reaches the target with a probability of 1.
Fig. 10.3 Probability of achieving target value T
362 10 Decision AnalysisIf T 2 [1, 2], d1 is best. It reaches the target with a probability of 0.7.
If T 2 [2, 5], d1 and d2 are best. Both achieve the target with a probability of 0.2.
If T 2 [5, 7], d2 is best. It achieves the target with a probability of 0.2.
If T > 7, none of the decisions will be able to reach the target.
A loose summary will indicate that d3 is best for low target values, d1 is best for
intermediate target values, and d2 is best for high target values. This is, of course, not
surprising: decision d3 has no extremes on the low end, while d2 does have an
extreme possible payoff on the high end.
10.4 Sensitivity Analyses
This section examines two types of sensitivity analyses. The first (simpler) case
assumes that a payoff, i.e., one element of the payoff matrix, is no longer known with
certainty. The idea is to develop simple decision rules that provide guidance to the
decision maker in case the payoff changes. Again, we will base our arguments on the
example introduced at the beginning of this chapter, which is shown again here for
convenience.
s1 s2 s3
d1 2 2 5
d2 0 1 7
d3 211
d4 2 3 4
p 0.5 0.3 0.2
Suppose now that there is some uncertainty concerning the payoff of the second
decision in case of the third state of nature. Given that we expect the actual payoff to
be somewhere between 5 and 10, we can then write the payoff as a23 ¼ 7 + ε with an
unknown ε 2 [2, 3]. The expected payoffs are then
EMV ¼
1:4
1:1 þ :2ε
1:5
:9
2
6
6
6
4
3
7
7
7
5:
Here, we can ignore decisions d1 and d4 as, regardless of the value of ε, decision
d3 is better than those. This leaves us with the comparison between d2 and d3.
Figure 10.4 plots the expected monetary values of the two decisions as a function of
the change ε.
The two payoff curves EMV(d2) and EMV(d3) intersect at ε ¼ 2. To the left of
ε ¼ 2, the payoff is higher for decision d3, while to the right of ε ¼ 2, the payoff with
d2 is higher. This leads us to the following decision rule:
10.4 Sensitivity Analyses 363If ε  2 (or, alternatively, a23  9), then decision d2 is best.
If ε  2 (or, alternatively, a23  9), decision d3 is best.
Next, consider the possibility that there is some uncertainty surrounding a
probability estimate. This case is somewhat more difficult conceptually, as the
increase or decrease of a single probability will necessarily imply that other
probabilities change as well, based on the simple fact that the sum of probabilities
equals one. Back in our numerical example with the original payoffs, we are now no
longer certain about the probability of s1.
As an example, we may assume that an increase of p1 by some unknown value ε
may reduce the probabilities of all other states of nature by the same amount, and
similar for a decrease of p1. If this assumption were reasonable, we then have
probabilities [p1 + ε, p2  ½ε, p3  ½ε], or, with the values of p1, p2, and p3, we
have [0.5 + ε, 0.3  ½ε, 0.2  ½ε]. Given these probabilities, we can then again
compute the expected monetary values, which are
EMVð Þ¼ ε
1:4 þ :5ε
1:1  3ε
1:5 þ 1ε
:9 þ 1:5ε
2
6
6
6
4
3
7
7
7
5:
Suppose now that it has estimated that p1 will assume a value somewhere between
0.3 and 0.6. In other words, starting with its present value of p1 ¼ 0.5, the change ε 2
[0.2, +0.1]. We can now again plot the expected monetary values of the decisions
as functions of ε. This is shown in Fig. 10.5.
For any value of ε we are interested in the highest expected payoff, i.e., in the
point on the highest curve (something called the upper envelope, shown here by the
broken line). We observe in Fig. 10.5 that the decisions d1 and d4 are dominated by
d3, leaving d2 and d3 as the only decisions of interest. The two functions intersect
where their payoffs are equal, i.e., at the point at which 1.1  3ε ¼ 1.5 + 1ε. Solving
for ε, we obtain ε ¼ 0.1 This leads to the following decision rule:
Fig. 10.4 EMV as a function of a payoff change
364 10 Decision AnalysisIf ε  0.1 (or, alternatively, p1  0.4), then decision d2 is best.
If ε  0.1 (or, alternatively, p1  0.4), then decision d3 is best.
Note that it is not generally true that decision rules such as this consist of only two
parts. It is possible that any number of the existing decisions may be best for some
range of changes.
It is, of course, possible that a change of p1 does not affect the remaining
probabilities equally. For instance, it could be estimated that an increase of p1 by
some unknown value ε will decrease p2 by 2=3ε, while p3 will decrease by 1=3ε. The
expected payoffs are then
1:4 þ
5
3 ε
1:1  5
3 ε
1:5 þ ε
:9 þ
8
3 ε
2
6
6
6
6
6
6
6
4
3
7
7
7
7
7
7
7
5
:
The payoff functions (as functions of ε) are shown in Fig. 10.6.
As above, we assume that ε 2 [0.2, +0.1]. A similar analysis to that provided
above reveals that decision d3 dominates d1 and d4, leaving the decision maker
with d2 and d3. Again, we are interested in the upper envelope, shown here by the
Fig. 10.5 EMV as a function
of a probability change
Fig. 10.6 EMV as a function
of alternative probability
change
10.4 Sensitivity Analyses 365broken line. The expected monetary values of the two decisions are equal if
1.1  5/3ε ¼ 1.5 + ε, i.e., for ε ¼ 0.15. This leads to the following decision rule:
If ε  0.15 (or, equivalently p1  0.35), then decision d2 is optimal.
If ε  0.15 (or, equivalently, p1  0.35), then decision d3 is optimal.
10.5 Decision Trees and the Value of Information
In this section, we will again consider decision-making problems under risk. In
addition to the decision rules discussed in the previous section, we will determine the
value of information that goes beyond the probabilities for the states of nature that
we continue to assume to be known. We will commence our discussion with an
extreme case known as the expected value of perfect information (EVPI). Clearly, in
reality no information is perfect, but this value provides an upper bound for the value
of any information, as no information can be worth more than perfect information.
Since it is easy to compute, it provides the decision maker with a ballpark figure. In
simple words, the EVPI is the difference of the payoff with perfect information and
the best we can do without any information beyond what is included in the standard
setting. As an illustration, consider again our example. Recall that with the
probabilities of 0.5, 0.3, and 0.2 of the three states of nature, the highest expected
monetary value was EMV* ¼ 1.5, which was achieved by choosing decision d3,
where we use an asterisk to indicate optimality. This is the best the decision maker
can do without additional information. Consider now perfect information. It means
that the decision maker will know in advance which state of nature will occur. It is
important to realize that this does not mean that the decision maker can change the
probabilities of the states of nature—all we assume is that the decision maker knows
which state of nature occurs before he makes his own decision.
In our numerical example, the best response to the first state of nature s1 is to use
d1, d3, or d4; each of these responses will result in a payoff of 2 to the decision maker.
Similarly, if the decision maker knows that s2 occurs, his best response is to choose
d3, which results in a payoff of 1. Finally, if nature chooses s3 and the decision maker
knows about it beforehand, the best response is d2, netting 7. The payoff matrix A is
shown again below with the starred element indicating those payoffs that result from
the decision maker’s best response to nature’s action.
A ¼
2 2 5
0 1 7
2 1 1
2 3 4
2
6
6
6
4
3
7
7
7
5
Given the known probability distribution p ¼ [0.5, 0.3, 0.2], we can state that
50% of the time, nature chooses s1 and the decision maker obtains a payoff of 2 (the
first column), 30% of the time, nature chooses s2 and the decision maker’s best
366 10 Decision Analysisreaction results in a payoff of 1, and finally, 10% of the time nature chooses s3 and
the decision maker’s response is d2, resulting in a payoff of 7. Hence, the expected
payoff with perfect information is EPPI ¼ 2(0.5) + 1(0.3) + 7(0.2) ¼ 2.7. The
expected value of perfect information is thenEVPI¼EPPIEMV*¼2.71.5¼1.2.
As indicated above, this is an upper bound on the amount of money that the decision
maker should be prepared to pay for any type of additional information.
Consider now a situation, in which the information provided to the decision
maker is imperfect. Imperfect information is usually provided by indicators. As an
example, consider the price of an individual stock. It is usually not possible to get
any direct information, so that we must rely on a proxy, such as demand for products
of firms in that industry or manufacturers’ receipts. Clearly, such proxies are only of
value if there is a link between them and the state of nature we want to forecast. For
instance, it would be meaningless to forecast the probability of sales of a new camera
by using the demand for potatoes. The stronger the links between a proxy and the
state of nature it is used to forecast, the closer we will be to perfect information. The
strength of the link between indicator and the state of nature is typically provided by
a table of conditional probabilities. For our example, assume there are two indicators
I1 and I2, whose links to the three states of nature are shown in Table 10.3.
In other words, given that the first state of nature s1 will actually occur, indicator
I1 has a 60% chance of coming up, and similar for the other values. It is apparent that
the probabilities in each column add to one. (In the extreme case, there would be
three indicators and the probabilities P(I1|s1) ¼ P(I2|s2) ¼ P(I3|s3) ¼ 1 and all other
conditional probabilities equal to zero. Then the three indicators are sure predictors
of the states of nature, and we have again perfect information. This is the limiting
case.)
Before performing any computation, we will first depict the structure of the
decision-making process in the form of a decision tree. The general structure of
such a tree is shown in Fig. 10.7.
In Fig. 10.7, we start on the left with the square node (the root of the tree),
indicating that we have to make a decision first. Our decision is whether or not to
solicit additional information. The lower branch shows that no additional indication
is sought, and it will end up with what we have already done earlier when applying
Bayes’s rule. Often, this part of the tree is deleted and its outcome at the end of the
first branch is shown as EMV* (¼ 1.5 in our example).
Consider now the upper branch that indicates that we ask for additional informa￾tion. This will be followed by a random event according to which some indicator
comes up. This is shown by the circular node, followed by branches for all indicators
I1, ..., Ip. Once we have received an indicator (meaning we now have additional
information), we must make one of our decisions d1, ..., dm. This is shown again as a
decision node, followed by arcs, one for each of our decisions. Finally, once we have
Table 10.3 Conditional
probabilities P(I|s) s1 s2 s3
I1 0.6 0.9 0.2
I2 0.4 0.1 0.8
10.5 Decision Trees and the Value of Information 367made a decision, a random event, i.e., one of the states of nature, will occur. The end
of the sequence of decisions and events is marked by a triangle, next to which we
will place the outcome for that particular scenario. It is important to realize that each
endpoint marked with a triangle actually symbolizes a scenario that includes the
entire sequence of decisions and events from the root of the tree to that triangle. For
instance, the topmost triangle on the right of the tree indicates that we asked for
additional information, received the indicator I1, made decision d1, and then state of
nature s1 occurred.
Once the structure of the tree has been determined, we need to put some numbers
into the tree. As already mentioned above, the payoffs are taken directly from the
payoff matrix and put next to the triangles on the right side of the decision tree. What
are now needed are probabilities. More specifically, we need two types of
probabilities. The first types are associated with the random events that determine
which of the indicators come up. They are called indicator probabilities P(Ik). So far,
we do not have these probabilities. The second type of probabilities is associated
with the states of nature occurring at the very end of the decision tree, just before the
Fig. 10.7 Decision tree involving additional information
368 10 Decision Analysispayoffs are due. At first glance, it would appear that the probabilities that we used in
Bayes’s rule (so-called prior probabilities) P(s) should be used. This is, however,
not the case. The reason is that a state of nature occurs after an indicator has come
up. And the whole point of indicators is that they are not independent from the states
of nature.
So what we need are the so-called posterior probabilities P(s|I). In other words,
these are conditional probabilities that specify the likelihood that a state of nature
occurs, given that an indicator has come up earlier. And while it may appear that we
are given the posterior probabilities in a table such as Table 10.3, this is not the case:
while both are conditional probabilities, Table 10.3 includes probabilities of the type
P(I|s); posterior probabilities are P(s|I). In other words, the probabilities will have to
be inverted, which is done by what is known as Bayes’s theorem. This theorem
(or rather conversion rule) is explained in Appendix C of this book. We will use it
here in a very convenient computational scheme shown below. And a by-product of
the conversion is the set of indicator probabilities that we also need to put numbers
on our decision tree.
The computational scheme that determines indicator probabilities and posterior
probabilities deals with each of the indicators separately. Consider again our numer￾ical example and use only the first indicator I1. Table 10.4 shows the computational
scheme we use as it applies to the indicator I1. The first column lists the states of
nature, and the second column includes their prior probabilities. The third column
includes the conditional variables that relate to the indicator under consideration
(here I1) and all states of nature. In other words, the third column is nothing but the
first row in Table 10.3. Following Bayes’s rule, we then multiply the elements in the
second and third columns and put them in the fourth column. Their sum, shown at
the bottom of column four, is then the indicator probability for the indicator under
consideration, here P(I1). Finally, the posterior probabilities in the last column are
obtained by dividing each element of column four by the indicator probability, i.e.,
P(si|I1) ¼ P(I1|si)P(si)/P(I1).
This procedure provides us with both the indicator probabilities and the posterior
probabilities needed to complete the numerical information required in the decision
tree. Similar computations are then performed for the second indicator. The results
are shown in Table 10.5. Note that the sum of indicator probabilities must equal one.
The same applies to the posterior probabilities in the rightmost columns of
Tables 10.4 and 10.5.
The decision tree with all numerical information is then shown in Fig. 10.8.
At this point, all numerical information is available, and we need to discuss a
technique that determines the optimal course of action.
Table 10.4 Computation
of P(I1) and P(s|I1)
s P(s) P(I1|s) P(I1|s)P(s) P(s|I1)
s1 0.5 0.6 0.30 0.4918
s2 0.3 0.9 0.27 0.4426
s3 0.2 0.2 0.04 0.0656
P(I1) ¼ 0.61
10.5 Decision Trees and the Value of Information 369The general idea to deal with decision trees is to use a recursive procedure. This
procedure starts from the leaves of the tree and works back to the root. There are two
rules in doing so:
Rule 1: Moving back into an event node, we take expected values.
Rule 2: Moving back into a decision node, we choose the decision with the highest
expected payoff.
Table 10.5 Computation
of P(I2) and P(s|I2)
s P(s) P(I2|s) P(I2|s)P(s) P(s|I2)
s1 0.5 0.4 0.20 0.5128
s2 0.3 0.1 0.03 0.0769
s3 0.2 0.8 0.16 0.4103
P(I2) ¼0.39
Fig. 10.8 Decision tree with numerical information included
370 10 Decision AnalysisApplying the two rules to our example, we start in the northeast corner of
Fig. 10.8. Following the top three branches with the payoffs of 2, 2, and 5 at
their respective ends, backward, we reach a decision node. This means that we use
the posterior probabilities of the branches to calculate expected payoffs at the event
node at the beginning of the branches. In this case, we compute 2(0.4918)
 2 (0.4426) + 5(0.0656) ¼ 0.4264. A similar process is used for the other branches.
The next step starts from the (total of six) event nodes whose expected payoffs
have just been computed. Starting again from the top with the three event nodes
labeled with 0.4264, 0.0166, and 1.4918, we will follow the arcs that lead into these
nodes one step backward. Since this step leads into a decision node, we will choose
the decision that results in the highest expected payoff. In this case, the highest
payoff is 1.4918 and the decision that leads to this payoff is d3. Similarly, comparing
the expected payoff in the lower part of the diagram (2.9233, 2.7952, and 1.5128),
the highest expected payoff is 2.9233 and the decision leading to this payoff is d1.
The arcs of the two decisions chosen in this step are in bold.
We now have two decision nodes labeled with 1.4918 and 2.9233, respectively.
Moving back another step means going into an event node, a task performed by
computing the expected payoff using the indicator probabilities 0.61 and 0.39. This
results in the expected payoff with imperfect information EPII ¼ 2.05. Similar to the
definition of the expected value of perfect information (the difference between the
expected payoff with and without this information), we can now define the expected
value of imperfect information, most frequently called expected value of sample
information EVSI. The EVSI is defined as the difference between the expected payoff
to the decision maker with and without sample information. Formally, we obtain
EVSI ¼ EPII  EMV* ¼ 2.05  1.5 ¼ 0.55. This is the highest amount that we
should be prepared to pay for this information. In other words, if the sample
information costs no more than 0.55, we should purchase it and proceed with the
upper part of the tree. Otherwise, we should not buy it and follow the “no survey”
part of the tree.
The final step consists of the computation of the efficiency E. The efficiency
measures how close the sample information is in comparison with perfect informa￾tion, i.e., E ¼ EVSI/EVPI. In our example, we obtain E ¼ 0.55/1.2 ¼ 0.4583.
Loosely speaking, this means that the sample information is about 45% perfect.
Note that the value of sample information depends on the strength of the link
between the indicators and the state of nature. If the indicators are very strong, the
efficiency will be close to one; if they are very weak, it will be close (or equal to)
zero. Since EVSI is never negative and it can never exceed the value of EVPI, the
efficiency is always a number between 0 and 1. Loosely speaking, it indicates the
value of sample information as a proportion of perfection.
Consider now the same example with two different indicators, whose strengths
are shown in Table 10.6.
The indicator probabilities and posterior probabilities for this example are calcu￾lated and shown in Tables 10.7 and 10.8 for the indicators I1 and I2, respectively.
10.5 Decision Trees and the Value of Information 371Constructing a decision tree similar to that in Fig. 10.8 and performing the
backward recursion results in EPII ¼ 2.12, so that EVSI ¼ 2.12  1.5 ¼ 0.62 and
E ¼ 0.62/1.2 ¼ 0.5167, slightly more efficient than in the original example.
In case the indicators are totally random, we would expect the value of this
information to be zero. It can be readily seen that this is indeed the case. To illustrate
this, consider again the above example and suppose now that there are three
indicators. The conditional probabilities P(Ik|s) all equal 1=3 . This results in all
indicator probabilities equaling 1=3 as well, while the posterior probabilities equal
the prior probabilities. Inserting them into the decision tree, we find that in the first
step of the backward recursion, we obtain the same expected payoffs we would as if
Bayes’s rule were used and, in the next step when making the decision, we will
choose the best Bayesian decision with a payoff of EMV* in each of the three cases.
The next step will multiply this value by the indicator variables by 1=3, resulting again
in EMV*, so that EPII ¼ EMV* and EVSI ¼ 0.
An interesting case occurs in the other extreme. It is apparent that if we were to
use a forecasting institute whose forecast is always correct the value of this informa￾tion equals the value of perfect information. On the other hand, imagine an institute
whose advice is always wrong. At first glance, it would appear that the value of such
advice equals zero. This is, however, not correct. As a matter of fact, such informa￾tion is also perfect, as we can rely on it: whenever they say one thing, we know that
the opposite applies. It is the consistency between what is predicted and what
actually happens that really counts.
Table 10.6 Conditional
probabilities P(I|s) s1 s2 s3
I1 0.9 0.6 0.2
I2 0.1 0.4 0.8
Table 10.7 Computation
of P(I1) and P(s|I1)
s P(s) P(I1|s) P(I1|s) P(s) P(s|I1)
s1 0.5 0.9 0.45 0.6716
s2 0.3 0.6 0.18 0.2687
s3 0.2 0.2 0.04 0.0597
P(I1) ¼ 0.67
Table 10.8 Computation
of P(I2) and P(s|I2)
s P(s) P(I2|s) P(I2|s) P(s) P(s|I2)
s1 0.5 0.1 0.05 0.1515
s2 0.3 0.4 0.12 0.3636
s3 0.2 0.8 0.16 0.4848
P(I2) ¼0.33
372 10 Decision Analysis10.6 Utility Theory
Utilities have been used for a long time by economists. Particularly noteworthy are
the analyses by the psychologists Kahneman and Tversky in the 1970s. The main
idea is to express the usefulness of a product or a service to the individual decision
maker. In order to illustrate the concept, consider the following argument. If the
expected value were to apply, then a decision maker would be indifferent to the
choice of a certain $50,000 gift and a lottery that pays $100,000 with a 50% chance
and a zero payoff with a 50% chance. The expected value in both cases is the same:
$50,000. However, most decision makers would prefer the certain $50,000.
Let us then take this argument a step further. Suppose we were to offer the
aforementioned lottery—a $100,000 payoff with a 50% chance and a zero payoff
with a 50% chance—to a decision maker and inquire what amount of money
received with certainty he were to consider equivalent to playing the lottery. This
value is called the certainty equivalent. The certainty equivalent is typically deter￾mined by a string of questions that narrow down the value. For instance, we would
describe the lottery to the decision maker and offer, say, $45,000 for certain. Would
he take the $45,000? If so, we renege on our offer and offer only $40,000 instead.
This process continues until the certainty equivalent is found. For many people, the
certainty equivalent is quite low; some go as low as $20,000. This shows a behav￾ioral trait referred to as risk aversion. As a rule, if a decision maker’s certainty
equivalent is less than the expected value of the lottery, the decision maker is risk
averse. If the certainty equivalent is higher than the expected value of the lottery, the
decision maker is risk seeking (gamblers are a typical example), while if a decision
maker’s certainty equivalent equals the expected value of a lottery, he is called risk
neutral. The graph in Fig. 10.9 plots a dollar value against the decision maker’s
certainty equivalent of the lottery.
Fig. 10.9 Certainty equivalent as a function of probabilities in the lottery
10.6 Utility Theory 373Risk aversion is the raison d’être for insurance companies that, if taken over all
those insured by them, take in more in premiums (certain cost) than they pay out
(uncertain benefits to their members). Real-life utility functions tend to be convex for
smaller dollar values and concave for larger values. The reason is that having or not
having a small amount makes very little difference to most people, while owning a
very large amount will change their life, e.g., enable them to retire early. This is, of
course, how lotteries make a living.
Note that it is important to realize when asking these questions that many times
people who claim they would rather take a risk and gamble than accept a fairly small
certainty equivalent would change their mind once the actual amount of cash is put in
front of them, taking the decision out of a purely “theoretical” realm into the hands￾on practical world. Also note that the certainty equivalents are specific to a decision
maker and are not transferable, as individuals differ in their acceptance of risk.
Another way to plot a utility function is this. Suppose that an individual owns
nothing at all, so that the first (few) dollar(s) enable him to survive, so they have a
very high value, i.e., a high utility u. Subsequent dollars, while very welcome, have
less and less utility attached to them; i.e., they are said to have decreasing marginal
values. In the end—to go to an extreme—the difference between winning ten or
11 million dollars in the lottery is negligible. Hence, utility functions tend to be
concave as shown in Fig. 10.10.
Once the certainty equivalents have been determined, they can be used to replace
the actual payoffs, so that the problem is then to maximize the expected utility. This
concept is also able to deal with cases, in which information concerning the
likelihood of states of nature is known but may be ignored by the decision maker.
As an example, consider the case of a patient, whose physician has the choice of a
number of drugs. Assume that one drug may be able to reduce the pain somewhat
without known side effects, while another may not only eliminate the pain, but also
the cause―with the possibility of major side effects that include death. If the latter
event has only a tiny probability of occurring, the expected “payoff” to the patient
may be such that the second and more effective may be chosen. However, the
physician may choose to either ignore the probabilities and use Wald’s rule so as
to minimize the worst-case damage and choose the former less effective drug, or,
similarly, may assign a very high negative “payoff” to the possibility of major side
Fig. 10.10 Concave utility
function
374 10 Decision Analysiseffects, resulting also in the former drug being chosen. Assigning very high costs
(typically shown as M >> 0) to options is a technique that is also used in optimiza￾tion under the name penalty costs, whenever options or situations are to be avoided,
while still using objectives that maximize the sum of benefits.
Exercises
Problem 1 (influence diagram): Consider the following situation. Jill lives pres￾ently in Missoula, Montana, where she has a fairly boring job. She has heard that
there are many opportunities in Denver, Colorado, and she plans to go there,
possibly resettle there, and buy a small house for herself. She plans to use her annual
vacation to look things over in Denver.
Develop an influence diagram and a decision tree for the problem.
Solution: The list of potential events and the two graphs below indicate just some
possible set of interdependencies of the decisions and events.
Decision Random event Consequence
Travel to Denver and look for a job
Resettle in Denver
Purchase a house in Denver
Economic situation
Get job offer
Salary (or net worth)
The influence diagram for this list is shown in Fig. 10.11.
A possible decision tree that describes the sequence of events is shown in
Fig. 10.12.
Problem 2 (single-stage decision making under uncertainty and risk): Con￾sider the payoff matrix of a game against nature.
s1 s2 s3 s4 s5 s6
d1 5 27 1 0 6
d2 60 3 58 1
d3 14 00 1 0
d4 4 2 3 56 0
(a) Explain in one short sentence the concept of a dominated decision. Are there any
dominated decisions in this example?
(b) How would an optimist decide?
(c) How would a pessimist decide?
(d) Find an optimal strategy using the minimax regret criterion.
(e) Suppose that the decision maker has been informed that the states of nature
occur with probabilities of 0.3, 0.2, 0.1, 0.1, 0.05, 0.25. How would a risk￾neutral decision maker decide?
(f) Construct the graph that plots the probabilities that a decision achieves a target
value T for all target values between 10 and 10 for all decisions. What is the
optimal decision for T ¼ 3½? For T ¼ 5½?
10.6 Utility Theory 375Solution:
(a) A decision dominates another if it is better or the same for all states of nature. In
this example, decision d2 dominates d4.
(b) An optimist anticipates payoffs of 7, 8, 4, and 6, so that d2 would be chosen.
(c) A pessimist anticipates payoffs of 6, 5, 1, 5, so that d3 would be chosen.
(d) The regret matrix is R ¼
160087
044600
507191
264621
2
6
6
6
4
3
7
7
7
5
with anticipated regrets of
8, 6, 9, 6, so that the (pessimistic) choice is d2 or d4 with regret of 6.
(e) The anticipated payoffs in Bayes’s model are 0.4, 2.25, 1.05, and 0.9, so that the
decision maker will choose d2.
(f) In Fig. 10.13, we show d1as a solid line, d2 as a broken line, and d3 as a dotted
line; the dominated decision d4 is deleted for clarity.
For T ¼ 3½, decision d1 is best, followed by d2 and d3. For T ¼ 5½, d2 is best and
then d1 and d2.
Problem 3 (sensitivity analysis): Consider a decision problem with three
decisions and four states of nature. The payoff matrix is as follows:
Fig. 10.11 Influence
diagram for Problem 1
376 10 Decision Analysis2 0 2 4
11 2 3
2 3 2 1
2
6
4
3
7
5:
Furthermore, suppose that the probabilities of the four states of nature are 0.4, 0.1,
0.3, and 0.2.
Fig. 10.12 Decision tree for Problem 1
Fig. 10.13 Probability of
achieving a target value for
Problem 2
10.6 Utility Theory 377(a) Perform a sensitivity analysis on a34, the payoff that results from decision d3
coupled with the fourth state of nature. It has been estimated at a34 2 [0, 4].
(b) Back to the original situation, perform a sensitivity analysis on p2. It is assumed
that (i) for each unit of increase of p2, the probability p1 decreases twice as much
as each of p3 and p4, and (ii) that p2 may decrease by as much as 0.05, while it
may increase by at most 0.2.
Solution:
(a) Given the payoff a34 ¼ 1 + ε, we obtain expected monetary values of 1.0, 0.5,
and 0.7 + 2ε for the three decisions. Given that a34 2 [0, 4], or, equivalently, ε 2
[1, 3], Fig. 10.14 shows the expected monetary values of the three decisions as
functions of ε and the upper envelope in the form of the broken line.
This leads to the following decision rule:
If a34  2.5, (or, equivalently, ε  1.5), choose d1.
If a34  2.5 (or, equivalently, ε  1.5), choose d3.
(b) The updated probabilities are p ¼ [0.4  ½ε, 0.1 + ε, 0.3  ¼ε, 0.2  ¼ε],
and the resulting expected monetary values are EMV(ε) ¼ 1.0  1.5ε, 0.5 + 0.75ε,
and .7 + 2.25ε for the three decisions. Figure 10.15 shows the expected monetary
values of the three decisions as functions of ε and the upper envelope is shown by the
broken line.
This results in the following decision rule:
If p2  0.18 (or, equivalently, ε  0.08), then choose d1.
If p2  0.18 (or, equivalently ε  0.08), then choose d3.
Problem 4 (expected value of perfect and sample information): The Canadian
McMoose chain of fast-food outlets is deciding how to keep up with the changing
tastes of its customer base. They have narrowed down their choices to the following
three decisions: d1 ¼ completely redecorate the existing franchises, d2 ¼ rebuild the
Fig. 10.14 EMV as a
function of ε, for Problem
3 (a)
378 10 Decision Analysisoutlets, and d3 ¼ modify the existing decor slightly to emphasize the “mooseyness”
of the outlet. The chain faces different states of the economy s1, s2, and s3, indicating
a level state, a slight upturn, and a significant upturn. The payoffs for all
combinations of decisions and states of the economy are shown in the following
matrix:
s1 s2 s3
d1 5 42
d2 42 9
d3 3 81
The probabilities for the three states of the economy have been determined as 0.3,
0.2, and 0.5.
(a) Determine the expected payoffs for the three decisions and choose the most
preferred decision on that basis.
(b) What is the expected value of perfect information?
(c) The McMoose management considers hiring a research institute to obtain more
detailed information about the state of the economy. They use two indicators I1
and I2 for their forecast. These two indicators are linked to the state of the
economy as shown in the following table of conditional probabilities P(I|s):
s1 s2 s3
I1 0.5 0.1 0.8
I2 0.5 0.9 0.2
Construct the decision tree for this problem and determine the expected value of
sample (imperfect) information. If the research institute charges 1.4 for their
services, should they be hired? Explain in one very short sentence.
Fig. 10.15 EMV as a
function of ε, for Problem
3 (b)
10.6 Utility Theory 379(d) What is the efficiency of the sample information?
Solution:
(a) The expected payoffs are 3.3, 3.7, and 3.0, so that they would choose d2 and
get 3.7.
(b) EVPI ¼ 7.6  3.7 ¼ 3.9.
(c)
For I1:
s P(s) P(I1|s) P(I1|s)P(s) P(s|I1)
s1 0.3 0.5 0.15 0.2632
s2 0.2 0.1 0.02 0.0351
s3 0.5 0.8 0.40 0.7018
P(I1) ¼ 0.57
For I2:
s P(s) P(I2|s) P(I2|s)P(s) P(s|I2)
s1 0.3 0.5 0.15 0.3488
s2 0.2 0.9 0.18 0.4186
s3 0.5 0.2 0.10 0.2326
P(I2) ¼ 0.43
The decision tree for the problem is then shown in Fig. 10.16.
EVSI ¼ 5.0301  3.7 ¼ 1.3301, which is less than the amount requested by the
institute, so do not hire them.
(d) The efficiency is E ¼ 1.3301/3.9 ¼ 0.3411.
Problem 5 (expected value of perfect and sample information): The
Australian automobile manufacturer Australomobil must decide whether or not to
manufacture the transmissions of their “Pithecus” model in-house (d1) or to subcon￾tract them out (d2). The company faces different levels of demand for the Pithecus
that are defined as s1, s2, s3, and s4. The payoffs for all combinations of decisions and
levels of demand are shown in the following table:
s1 s2 s3 s4
d1 4 1 2 4
d2 42 1 3
(a) Are there any dominances in the payoff matrix? Explain in one short sentence.
(b) How would a pessimist decide? What is the optimal decision under the regret
criterion? In both cases, what are the anticipated payoffs?
(c) Given prior probabilities of 0.2, 0.3, 0.4, and 0.1 for the states of nature, what is
the optimal decision with Bayes’s criterion? What is the expected payoff?
(d) What is the expected value of perfect information?
380 10 Decision Analysis(e) The Australomobil management considers hiring a research institute to obtain
more detailed information about the future level of demand. They use two
indicators I1 and I2 for their forecast. These two indicators are linked to the
level of demand as shown in the following table of conditional probabilities P(I|s):
s1 s2 s3 s4
I1 0.4 0.8 0.9 0.5
I2 0.6 0.2 0.1 0.5
Fig. 10.16 Decision tree for Problem 4
10.6 Utility Theory 381Construct the decision tree for this problem and determine the expected value of
sample (imperfect) information. If the research institute charges 0.7 for their
services, should they be hired? Explain in one very short sentence.
(f) What is the efficiency of the sample information?
Solution:
(a) There are no dominances. For s1, d1 is better than d2, but for s2, d2 is preferred
over d1.
(b) A pessimist will anticipate payoffs of 2 and  4, respectively. He will
choose d1 and anticipate a payoff of 2. For the regret criterion, we set up the regret
matrix
R ¼ 0330
8001  ,
so that the (anticipated) maximal regrets are 3 and 8. The decision maker will then
choose d1 and anticipate a regret of 3.
(c) The expected payoffs are 0.1 and 0.5, so that they would choose d2 and
get 0.5.
(d) EVPI ¼ 2.2 - 0.5 ¼ 1.7.
(e)
s P(s) P(I1|s) P(I1|s)P(s) P(s|I1)
s1 0.2 0.4 0.08 0.1096
s2 0.3 0.8 0.24 0.3288
s3 0.4 0.9 0.36 0.4932
s4 0.1 0.5 0.05 0.0685
P(I1) ¼ 0.73
s P(s) P(I2|s) P(I2|s)P(s) P(s|I2)
s1 0.2 0.6 0.12 0.4444
s2 0.3 0.2 0.06 0.2222
s3 0.4 0.1 0.04 0.1481
s4 0.1 0.5 0.05 0.1852
P(I2) ¼ 0.27
The decision tree associated with this problem is shown in Fig. 10.17.
382 10 Decision AnalysisEVSI ¼ 1.21  0.5 ¼ 0.71, which is slightly more than the amount requested by the
institute, so they should hire them.
(f) The efficiency is E ¼ 0.71/1.7 ¼ 0.4176.
Fig. 10.17 Decision tree for Problem 5
10.6 Utility Theory 383Multiattribute Decision Making 11
The models and methods in this chapter all consider problems, in which the
consequences of a decision are no longer one-dimensional, while, in contrast to
the scenario in decision analysis, the outcomes are deterministic. In other words, if
we were to, say, change the composition of a soft drink we manufacture or change
the features on a cell phone, we do not just deal with profit as a result of this decision,
but face changing customer acceptance (and demand) for the product, different costs,
changing market share, customer satisfaction, and other factors, all of which will
influence short- and long-term viability of the firm. The models discussed in this
chapter are similar to those in the chapter on multiobjective optimization, except that
the models in this chapter are discrete: we face only a finite (and usually fairly small)
number of choices.
11.1 The General Model and a Generic Solution Method
Throughout this section, we assume that the decision maker has a finite number of
possible decisions d1, d2, ..., dm that are to be evaluated on a number of criteria c1,
c2, ..., cn. All decisions are then evaluated on all criteria in a matrix (not unlike the
payoff matrices in decision analysis). Without loss of generality, we assume that all
criteria are of the utility type, i.e., more is always better. In case some of the criteria
originally denote costs or distances, i.e., criteria for which less is better, we may
convert them to utilities by multiplying them by some negative value or subtracting
them from some large number.
In order to demonstrate some of the basic concepts, consider the problem of
choosing a house. The decision maker has identified some of the criteria most
important to him, which include the size of the house, the size of the lot, the taxes
charged most recently, the price of the house, and the distance between the house and
his place of work. Furthermore, the decision maker has chosen seven possible
choices from a multiple listing service that provides some basic information
# The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
H. A. Eiselt, C.-L. Sandblom, Operations Research, Springer Texts in Business and
Economics, https://doi.org/10.1007/978-3-030-97162-5_11
385(which is how these actual data have been determined). The information is
summarized in Table 11.1.
Often, decision makers will also consider additional criteria, such as the number
of bedrooms, bathrooms, and garages. All of these properties have two garages, four
or five bedrooms, and 2½ to 3 baths, so that we have eliminated those criteria from
further consideration. In general, decision makers should restrict themselves to, say,
no more than six criteria, so as to avoid clutter and correlation. This limitation has
been suggested by psychological research.
Clearly, there is usually a correspondence between the size of the house and the
number of rooms in it. When considering correlated criteria, it is typically a good
idea to include only one of those criteria and thus avoid considering and counting
them multiple times.
Along similar lines, both asking price for the property and taxes contribute to the
financial burden to be borne by the buyer. We can combine the two by computing the
annual or monthly cost of each of the properties. In order to do so, we first assess the
amount of money the decision maker can provide in cash. Assume that this amount is
$100,000. The remaining part of the cost of the house will then have to be financed,
which we assume can be done at the (somewhat favorable) rate of 3% p.a. Formally,
we can write the monthly cost of the projects as (cost) ¼ 0.0025
(price  100,000) + 0.08333 (taxes), where the coefficient of the taxes simply
converts the annual tax to a monthly cost. The revised evaluations of the projects
are then shown in Table 11.2.
Table 11.1 Data for the sample problem
Decision
Size
(sq ft)
Lot size
(sq ft)
Taxes
($)
Asking price
($)
Distance to work
(mi)
d1 2154 8820 3829 308,500 3.8
d2 2600 14,445 7264 669,000 3.7
d3 2500 15,791 3480 284,900 6.0
d4 2300 10,496 5601 419,000 3.5
d5 3250 24,412 3275 595,000 10.6
d6 3600 7.4 acres 4719 699,900 7.5
d7 1982 10,678 6253 624,500 4.8
Table 11.2 Revised evaluations for sample problems
Decision Size (sq ft) Lot size (sq ft) Monthly cost ($) Distance to work (mi)
d1 2154 8820 840.33 3.8
d2 2600 14,445 2.027.83 3.7
d3 2500 15,791 752.25 6.0
d4 2300 10,496 1264.25 3.5
d5 3250 24,412 1510.42 10.6
d6 3600 7.4 acres 1893.00 7.5
d7 1982 10,678 1832.33 4.8
386 11 Multiattribute Decision MakingAt this point, the decision maker will have to decide which of the projects are
actually financially feasible, considering his income. Assume that the decision maker
has decided that a monthly financial load beyond $1600 is not feasible. (This, by the
way, will include potential increases of the interest rate in future years, when our
decision maker has not yet paid back a significant amount on the principal.) This will
eliminate options 2, 6, and 7. The reduced evaluation matrix is then shown in
Table 11.3.
The next step is to convert the evaluations in Table 11.3 to utilities. This is an
important and necessary step, as some of the criteria are considered better, if the
numerical evaluations are larger (within certain limits, this applies to the first two
criteria), while other criteria are considered superior, if their evaluations are smaller
(the latter two criteria in our example). Furthermore, most people would agree that
for the size of the house, the simple “bigger is better” rule does not necessarily apply:
while a house with 1000 sq. ft. is preferred to one with 600 sq. ft. by most people, a
house of the size 50,000 sq. ft. is not necessarily considered preferable (forget about
affordable) to one with 3000 sq. ft.
This decision maker does not wish to purchase a house of less than 1200 sq. ft.,
his utility increases linearly between 1200 and 2400 sq. ft. from a utility of 0 to a
utility of 0.8, and from there it increases to a utility of 1, which it reaches at 3000
sq. ft. Beyond that, the utility stays at that level. Denoting by x the size of the house
and by u(x) the utility of the size, we can write
u xð Þ¼
0, if x  1, 200
0:8 þ 0:0006667x, if x 2 1, 200; 2, 400
1
3, 000 x, if x 2 2, 400; 3, 000
1, if x > 3, 000:
8
>>>><
>>>>:
A similar argument applies to the size of the lot. Here, our decision maker will not
consider very small lots of 5000 sq. ft. or less, and his utility u( y) for a lot of size y is
summarized in the step function
Table 11.3 Reduced evaluation matrix for sample problem
Decision Size (sq ft) Lot size (sq ft) Monthly cost ($) Distance to work (mi)
d1 2154 8820 840.33 3.8
d3 2500 15,791 752.25 6.0
d4 2300 10,496 1264.25 3.5
d5 3250 24,412 1510.42 10.6
11.1 The General Model and a Generic Solution Method 387u zð Þ¼
0, if x 2 ½  0; 5, 000
0:4, if x 2 5, 000; 10, 000
0:7, if x 2 10, 000; 20, 000
0:9, if x 2 20, 000; 43, 560
1:0, if x 2 1 acre; 30 acres
0:9, if x > 30 acres:
8
>>>>>>>><
>>>>>>>>:
Similarly, the utility of the cost of the property is captured by the utility function
u(z) ¼ 1  0.0001z  0.000000328z
2
, given a cost of z, and for a distance w between
a property and the decision maker’s place of work, the utility is u-
(w) ¼ 1  .0001w  .0001w2  .0001w3
. The utility matrix for the four remaining
properties is then shown in Table 11.4.
This is the table we will work on with the generic procedure. The procedure itself
is easily described: Assign a weight to each criterion, then aggregate the utility by
using these weights (typically the weighted sum), and then choose the highest of the
resulting weighted average utilities. This “simple weighted average” process is just
about identical to Bayes’s rule in decision analysis that computes expected values,
except that the weights in this process are no probabilities.
In this example, assume that the decision maker has decided that the weights of
the four criteria are 0.25, 0.1, 0.5, and 0.15, respectively, with the obvious emphasis
on the cost of the property. The average utilities of the four decisions are then
0.6901, 0.7941, 0.5771, and 0.5206, respectively, making the second choice (the
decision d3) the clear winner.
As usual, since decision makers cannot be expected to specify exact values of the
weights, we perform sensitivity analyses by changing the weights somewhat and
determine if and how the decisions fare in this modified scenario. Suppose now that
the decision maker were to put less weight on the cost and more on the size of the
house and the lot, e.g., by using weights 0.35, 0.15, 0.35, 0.15. The weighted
average utilities are then 0.6710, 0.8015, 0.6331, and 0.6505, so that the second
decision (d3) is still the best by a long shot, indicating the stability of the decision.
One possibility to visualize different utilities in multiattribute problems are spider
plots. To prepare for the graph, the criteria have to be either all of the maximization
or all of the minimization variety. In a spider plot, each criterion is assigned one
dimension. Given m criteria, the axes are graphed with 360/m degrees between them.
In our example, this is 360/4 ¼ 90 degrees. For each criterion, we then plot points on
Table 11.4 Utility matrix for sample problem
Decision Size Lot size Financial load Distance to work
d1 0.6361 0.4 0.6843 0.9927
d3 0.8333 0.7 0.7392 0.9742
d4 0.7334 0.7 0.3493 0.9941
d5 1.0000 0.9 0.1007 0.8686
388 11 Multiattribute Decision Makingthe axes depending on the attributes. Finally, the points of one criterion on neigh￾boring axes are connected. For the utility matrix of our example in Table 11.4, the
spider plot is shown in Fig. 11.1, where the decisions d1, d3, d4, and d5 are shown as
dotted, dashed, dashed-dotted, and solid lines, respectively.
Spider plots allow an easy assessment of different decisions. For instance, if the
area of one decision completely covers the area of another decision, the latter
decision is dominated. In our example d4 is almost dominated by d3. It also
visualizes the strengths of individual decisions. For instance, decision d5 in our
example has stronger mass in the top part of the plot, i.e., it performs very well when
it comes to the size of the house, whereas it is weak at the bottom, i.e., when it comes
to the financial load.
Fig. 11.1 Spider plot
11.1 The General Model and a Generic Solution Method 38911.2 TOPSIS
The TOPSIS (Technique for Order of Preference by Similarity to Ideal Solution) was
first described by Hwang and Yoon (1981). It evaluates decisions on the basis of the
distances between their attributes and some ideal points: the closer a decision’s
attributes are to the chosen ideal point, the better a decision is. In other words, we
consider the distance as a disutility that we attempt to minimize. Even at this level, it
is apparent that at least two user-defined measures will influence the solution: the
choice of an ideal point and the yardstick used to measure the distance between the
attributes of a decision and the ideal point.
We may explain the concept by means of a simple example. A student is in the
process to choose one among four institutions of higher learning for his studies. In
five topics, the choices are the “Business Undergraduate Necessary Knowledge”
(BUNK), the “Online University of Indeterminate Junior Academics” (OUIJA), the
Commonwealth Undergraduate Teaching Education (CUTE), and the “Business
Undergraduate Liberal Learning” (BULL). Given these choices (for simplicity, we
will refer to them as d1, d2, d3, and d4), the student wants to choose one of these
institutions to learn about technical issues related to business. The attributes of the
four institutions regarding the five programs are shown in Table 11.5, where all
scores are expressed on a scale between 0 and 100, where 100 means that the course
perfectly suits our student’s interests, while 0 means that our student is not interested
in that academy’s offering at all.
Suppose now that the student wants to learn all about optimization, decision
analysis, and logistics, some background statistics, and some basic finance. Given
that, he has determined his ideal point to be [60, 100, 90, 80, 20], and that deviations
of an attribute are measured by Euclidean distances. The negative deviations
(we consider only these, as a positive deviation from a desirable goal carries no
penalty) are squared, summed up, and then the square root is taken. For decision d1,
we obtain
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
ð Þ0 2þð Þ 10060 2þð Þ 9040 2þð Þ 8030 2þ02
r ¼81:24: Similarly,
for the other three decisions, we obtain disutilities/penalties of 45.83, 88.88, and
45.83, respectively, so that decisions d2 and d4 are considered equally good and best
among the available decisions. At this point, the decision maker could either use
secondary criteria or apply different distances: weighted Euclidean distances (square
root of the weighted sum of deviations), (weighted) Manhattan distances, or others.
For details on distance functions, readers are referred to Chap. 7. Taking unweighted
Table 11.5 Strengths of the four institutions regarding the topics
Decision Statistics Optimization Decision analysis Logistics Finance
d1 70 60 40 30 90
d2 50 60 70 80 20
d3 70 80 20 30 10
d4 40 70 70 60 20
390 11 Multiattribute Decision MakingManhattan distances (the sum of positive deviations from the goal) results in
deviations of 140, 70, 150, and 90, respectively, so that the second decision performs
best, and our protagonist will attend OUIJA.
11.3 The Analytic Hierarchy Process
Similar to the generic procedure shown in the previous section, the analytic hierar￾chy process is a tool that also allows decision makers to choose among decision
alternatives that are to be evaluated on multiple criteria. The technique was devel￾oped by Thomas Saaty in the late 1970s (Saaty 1980), based on the premise that
decision makers are not really able to make choices among more than two
alternatives at a time (which is what we had to do in the generic method, when
assigning utilities to all decisions). This principle of pairwise comparisons has been
advocated for a very long time. While convenient for the user, it may (and usually
does) create inconsistencies that will have to be dealt with.
The analytic hierarchy process (or AHP, as it is sometimes called) allows a variety
of levels of objects that are arranged in a hierarchy. The exact way a problem is
structured will, of course, depend on the specific situation. In the simplest case, we
have the overall objective on the highest level, followed by the individual criteria
that are used to evaluate the individual decision on the second level, which, in turn,
are located on the third level.
Formally, for each criterion k, we will set up a matrix Ck
, whose entry in row i and
column j will indicate how much the decision maker prefers decision di over decision
dj by just considering criterion ck. Given m possible decisions, each of the matrices
Ck has m rows and m columns.
In case quantitative evaluations of the decisions on the criterion are available,
their ratios will be used. For instance, if one decision produces sales of 120 and
another decision results in expected profits of 80, then the first decision is preferred
over the second decision 120/80 ¼ 1.5 times. This value will then be used in the
matrix of pairwise comparisons. For qualitative data, the author of the original study
suggested to use the values shown in Table 11.6.
Note that the numerical values associated with the verbal statements are rather
arbitrary; in our opinion they appear extreme. For instance, if a decision maker were
Table 11.6 Quantitative values for preferences
Numerical value
of preference of di over dj Interpretation
1 The decisions di and dj are considered equal
3 Decision di is preferred over decision dj
5 Decision di is strongly preferred over decision dj
7 Decision di is very strongly preferred over decision dj
9 Decision di is extremely preferred over decision dj
11.3 The Analytic Hierarchy Process 391to “strongly” prefer decision di over decision dj, it would appear that considering di
being twice as important as dj expresses this fact rather well and there is no need of
using a factor of 5 as suggested by Saaty.
Pairwise comparisons between decisions are performed for all of the q given
criteria. This results in matrices C1
, C2
, ..., Cq
. In addition, a matrix C must be set
up, which includes the pairwise comparisons between the individual criteria. For
instance, if the decision maker considers the criterion “customer satisfaction” twice
as important as the criterion “sales,” then the value of “2” will be included in this
matrix. We assume that the matrix is pairwise consistent, in that if a decision di is
considered to be x times as good as decision dj, then we imply that decision dj is
considered to be 1/x times as good as decision di.
The task at hand is now to describe a technique that will help the decision maker
to make a decision on the basis of these evaluations. The first step in one version of
the analytic hierarchy process, called the normalization technique, is to create new
matrices Ce1
, Ce2
, ... , Ceq
and Ce from each of the existing matrices C1
, C2
, ..., Cq and
C by dividing each of its elements by its respective column sum. This is nothing but
a normalization procedure, which results in matrices, all of whose column sums
equal one. We then construct a utility matrix U that has as many rows as we have
decisions (i.e., m rows) and as many columns as we have criteria (i.e., q columns).
The first column of matrix U contains the averages of the rows of matrix Ce1
, the
second column of U contains the averages of the rows of the matrix Ce2
, and so forth.
Similarly, we obtain the vector of weights w, whose elements are the averages of the
rows of the matrix Ce . We are now ready to compute the vector of values of the
decisions v(d) ¼ Uw. In other words, the values of decision d1 are obtained by
multiplying the first row of the utility matrix with the vector of weights w, the
expected value of the second decision is obtained by multiplying the second row of
U with w, and so forth.
This procedure is best explained by an
Example: The manager of a resort faces the task of defining its target group. He
will either cater to families with children (with the appropriate kids’ facilities), or to
couples who want to experience nature (with guided walks and “soft” adventure
trips), to young individuals (with rock climbing and white-water rafting), or to
seniors (with appropriate leisure facilities for entertainment such as Bingo). These
four decisions d1, d2, d3, and d4 are mutually exclusive and collectively exhaustive.
The decisions are to be evaluated on two criteria c1 and c2. The two criteria are
market share (and with it the potential for long-term viability) and profit (short-term
benefit); they are deemed about equally important. The decision maker has been
asked to compare the individual strategies by pairwise comparisons and has come up
with the following results.
392 11 Multiattribute Decision MakingC1 ¼
1:0000 1:2000 2:0000 :9000
:8333 1:0000 2:0000 :8000
:5000 :5000 1:0000 :5000
1:1111 1:2500 2:0000 1:0000
2
6
6
6
4
3
7
7
7
5
and
C2 ¼
1:0000 :3000 :5000 1:0000
3:3333 1:0000 1:6000 3:0000
2:0000 :6250 1:0000 2:0000
1:0000 :3333 :5000 1:0000
2
6
6
6
4
3
7
7
7
5
, as well as
C ¼ 1 1
1 1  :
For instance, as far as criterion c1 is concerned, the decision maker considers
decision d1 twice as valuable as decision d3 (as shown by the value of “2” in the first
row and the third column of matrix C1
). On the other hand, decision d1 is considered
only half as good as decision d3 when looking at the second criterion. This is shown
in row 1, column 3, of matrix C2
, which shows a value of “0.5.”
The column sums of the matrix C1 are 3.4444, 3.95, 7, and 3.2, respectively, the
column sums of the matrix C2 are 7.3333, 2.2583, 3.6, and 7, respectively, and the
column sums of the matrix C are both 2. Dividing the elements of the matrices C1
,
C2
, and C by their respective column sums, we obtain the normalized matrices
Ce1
, Ce2
, and Ce shown below.
Ce1
¼
:2903 :3038 :2857 :2813
:2419 :2532 :2857 :2500
:1452 :1266 :1429 :1563
:3226 :3165 :2857 :3125
2
6
6
6
4
3
7
7
7
5
and
Ce2
¼
:1364 :1328 :1389 :1429
:4545 4428 :4444 :4286
:2727 :2768 :2778 :2857
:1364 :1476 :1389 :1429
2
6
6
6
4
3
7
7
7
5
, as well as
Ce ¼ ½ ½
½ ½  .
The row averages of the matrices Ce1
and Ce2
are then collected in the columns of
the utility matrix U ¼
:2903 :1378
:2577 :4426
:1428 :2783
:3093 :1415
2
6
6
6
4
3
7
7
7
5
, while the row averages of the matrix Ce are
11.3 The Analytic Hierarchy Process 393collected in the weight vector w ¼ :5
:5
 . Multiplying the rows of the matrix U by
the weight vector w results in the values that are associated with the four decisions
vð Þ¼ d
:2141
:3502
:2106
:2254
2
6
6
6
4
3
7
7
7
5:
It is apparent that the decision maker is very much in favor of decision d2; i.e., the
firm will be best off if they cater to couples with a love for nature. The three
remaining decisions have much lower scores.
The next question then concerns the validity of the scores provided by the
decision maker. One measure to evaluate them is the consistency of the decision
maker’s rating. While pairwise consistency is, by assumption, always satisfied, more
complex consistency is easily violated. For instance, suppose that the decision maker
considers a decision d2 three times as good as decision d5, which, in turn, he
considers to be twice as good as decision d7. This implies that the decision maker
believes that decision d2 is six times as good as decision d7. However, many decision
makers will not use this value but, say, argue that d2 is five times as good as d7, thus
introducing inconsistency. Small degrees of inconsistency are tolerable and will, in
most cases, not influence the decision. However, it is important to flag such
instances. The only remedy, though, will be for the decision maker to rethink the
problem and attempt to be more consistent in his evaluations.
There is a variety of measures that may be used to evaluate the degree of
consistency. While the originator of the analytic hierarchy process devised a
so-called consistency index, we will use a simple measure from elementary statistics.
We first observe that if the decision maker’s evaluations are completely consistent,
then all matrices Ce1
, Ce2
, ... , Ceq
and Ce will have identical elements in each of the
rows. Typically, this is not the case. Smaller variations will be tolerable, but if large
deviations show, the problem should be returned to the decision maker for reevalua￾tion. The magnitude of the variation can be evaluated by the coefficient of variation.
In order to do so, we first calculate the mean and the variance (and standard
deviation) of the elements in each row of the matrices Ce1
, Ce2
, ... , Ceq
and Ce, and
then divide the standard deviation by the mean. The resulting coefficients of varia￾tion are then examined. If their magnitude is excessive, e.g., larger than, say, ten
percent, the evaluations in that matrix should be returned to the decision maker for
inspection.
In order to explain the concept, we return to the above
Example: First consider the matrix Ce1
. The rows of the matrix have means of
0.2903, 0.2577, 0.14275, and 0.3093, while its standard deviations are 0.008432,
0.016682, 0.010612, 0.014106, so that the coefficients of variation are 0.029049,
0.064734, 0.074340, and 0.045606, respectively. In other words, none of the
394 11 Multiattribute Decision Makingcoefficients of variation are larger than 7.5%, making the decision maker’s
evaluations with respect to the first criterion acceptable.
Considering now the second criterion and its associated normalized matrix Ce2
,
we find its coefficients of variation as 0.02671, 0.02086, 0.01692, and 0.02997,
respectively, with a maximum variation of 3% from the mean. This is rather
consistent, and no corrective action is recommended.
Finally, the matrix Ce is completely consistent, as its elements are identical within
each row.
Exercises
Problem 1 (Generic method, simple weighted average): Consider the problem
of purchasing a vehicle. The decision maker has narrowed down the problem to three
choices and three criteria. The evaluations of the vehicles are summarized in
Table 11.7.
It can reasonably be assumed that the decision maker has utility functions
u xð Þ¼ 1  2, 778
1010 x2
for a purchase price of x, and
u yð Þ¼ 1  50y2
for a gas mileage of y. For the qualitative scores regarding the comfort, the
decision maker uses the so-called 5-point Likert scale: terrible—poor—medium—
good—excellent in increments of 0.25.
(a) Set up the utility matrix.
(b) Given that gas mileage and comfort combined are deemed equally important
and that combined, they are assessed as important as the purchase price,
determine the decision maker’s weights of the criteria.
(c) What are the values (utilities) of the individual decisions and how would you
advise the decision maker?
(d) Assume now that the decision maker decided to sell the car in 3 years’ time, after
having driven for 20,000 miles per year, which would cost him $4 per gallon.
For simplicity, we will ignore interest and inflation. The resale values of the
three vehicles have been estimated at 40%, 60%, and 70% of their present
purchase price. The decision maker will then only consider two criteria, viz.,
the cost of operating the vehicle for 3 years and the comfort the vehicle provides.
The utility of the costs is expressed as u zð Þ¼ 1  2e40
z , where z denotes the cost
Table 11.7 Evaluations for Problem 1
Price ($) Gas mileage (mpg) Comfort
Chrysler Prowler 50,000 12 Medium
Ford Ranger 20,000 25 Poor
Cadillac DeVille 60,000 18 Excellent
11.3 The Analytic Hierarchy Process 395(in $1000) of the vehicle during the 3 years. The decision maker considers costs
three times as important as comfort. Set up the modified utility matrix and use
the generic method to make a recommendation to the decision maker.
Solution:
(a) Given the utility functions, the utility matrix
U ¼
:3055 :6528 :5
:8889 :92 :25
0 :8457 1
2
6
4
3
7
5:
(b) w1 ¼ .5, w2 ¼ .25, w3 ¼ .25.
(c) v(d) ¼ (.44095, .73695, .461425) and the analyst would recommend that the
decision maker purchase the Ford Ranger.
(d) The loss of value of the three vehicles is 50,000  20,000 ¼ $30,000,
20,000  12,000 ¼ $8000, and 60,000  42,000 ¼ $18,000, respectively.
The gas consumptions of the three vehicles during the 3 years are 1
12 60, 000 ¼
5, 000, 1
25 60, 000 ¼ 2, 400, and 1
18 60, 000 ¼ 3, 333:33 gallons, respectively, for
costs of $20,000, $9600, and $13,333.33, respectively. The combined costs for
the three vehicles are then $50,000, $17,600, and $31,333.33, so that the utilities
with the function provided above are .1649, .7061, and .4767. The evaluation
matrix with columns for cost and comfort is then
:1013 :5
:7939 :25
:4420 :1
2
6
4
3
7
5:
The weights specified by the decision maker are w1 ¼ .75 and w2 ¼ .25, so that the
weighted average utilities of the three automobiles are .2010, .6579, and .3565,
which gives a large advantage to the Ford Ranger.
Problem 2 (TOPSIS): A family needs to decide where to go for breakfast. There
are four acceptable restaurants, the “Grill & Chill,” the “Prime Roster,” the “Sim￾mering Heights,” and the “Head Cookie” (denoted by d1, ..., d4). The important
components of the breakfast are the potatoes, the eggs, the bacon, the toast, and the
value for money. Based on past experience, the family has put together evaluations
of the five individual components, which are shown as normalized utilities in
Table 11.8.
The family has decided that the best possible point is [1, 1, ..., 1], and they
believe that deviations from the ideal point are measured well by weighted Euclidean
distances, given that they have decided that while the deviations of the quality of
396 11 Multiattribute Decision Makingpotatoes, eggs, and bacon from the ideal are about equally important, they are three
times as important as the deviation of toast from the ideal, and half as important as
the deviation of the value for money from the ideal point. Where should the
family eat?
Solution: We determine that the weights are 1 for toast, 3 for potatoes, eggs, and
bacon each, and 6 for the value. The weighted deviations from the ideal point for the
four restaurants are
d1 : Deviation
¼
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
3 1ð Þ  :9 2 þ 3 1ð Þ  :8 2 þ 3 1ð Þ  :6 2 þ 1 1ð Þ  :5 2 þ 6 1ð Þ  :6 2
r 
¼ 1, 3565,
d2 : Deviation
¼
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
3 1ð Þ  :6 2 þ 3 1ð Þ  :5 2 þ 3 1ð Þ  :4 2 þ 1 1ð Þ  :3 2 þ 6 1ð Þ  1 2
r 
¼ 1:6733,
d3 : Deviation
¼
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
3 1ð Þ  :8 2 þ 3 1ð Þ  :9 2 þ 3 1ð Þ  :5 2 þ 1 1ð Þ  :4 2 þ 6 1ð Þ  :5 2
r 
¼ 1:6613,
and
d4 : Deviation
¼
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
3 1ð Þ  :6 2 þ 3 1ð Þ  :7 2 þ 3 1ð Þ  :7 2 þ 1 1ð Þ  :8 2 þ 6 1ð Þ  :5 2
r  ¼ 1:6:
Other than the “Grill & Chill” (the first restaurant), the other eateries are fairly
close in their evaluations. Formally d4, i.e., the “Head Cookie,” would be chosen, but
as the assessments are so close, we would consider secondary objectives, which
include important considerations that have not been included in this model.
Table 11.8 Utilities for the breakfast example
Decision Potatoes Eggs Bacon Toast Value for money
d1 .9 .8 .6 .5 .6
d2 .6 .5 .4 .3 1.0
d3 .8 .9 .5 .4 .5
d4 .6 .7 .7 .8 .5
11.3 The Analytic Hierarchy Process 397Problem 3 (AHP, normalization technique): An investor may invest his avail￾able funds in blue-chip stocks, real estate, and bonds. These investments are to be
evaluated on two criteria, viz., short-term profits and long-term viability, with the
former being evaluated to be 1.5 times as important as the latter. Pairwise preference
statements have been recorded as follows:
Criterion 1: C1 ¼
1 33
1=312
1=3 ½ 1
2
6
4
3
7
5 , criterion 2: C2 ¼
1 ½ 1
5
2 1 ½
52 1
2
6
6
4
3
7
7
5
, and the
relation between the two criteria is expressed in the matrix C ¼ 1 3
2
2=3 1
2
4
3
5:
(a) Use the analytic hierarchy method to determine a ranking of the three decisions.
(b) Examine the consistency in the matrices C1 and C2
, using a quantitative
criterion. What are your conclusions?
Solution:
(a) The normalized matrices are
Ce1
¼
:6000 :6667 :5000
:2000 :2222 :3333
:2000 :1111 :1667
2
6
6
4
3
7
7
5, Ce2
¼
:1250 :1429 :1176
:2500 :2857 :2941
:6250 :5714 :5882
2
6
6
4
3
7
7
5
and
Ce ¼ :6 :6
:4 :4
" #:
The row averages of these normalized matrices are the columns of the utility
matrix U and the weight vector w, respectively, so that
U ¼
:5889 :1285
:2518 :2766
:1593 :5949
2
6
4
3
7
5
and w ¼ :6
:4
 :
We then obtain v(d) ¼ [.4047, .2617, .3335], so that we recommend the first
decision (investment in blue-chip stocks) to the decision maker.
(b) As far as consistency is concerned, we compute coefficients of variation of
.20156, .2316, and .2302 for matrix Ce1
, i.e., an average variation of more than
20%. This would give us sufficient reason to return the evaluation matrix C1 to
the decision maker for reevaluation.
398 11 Multiattribute Decision MakingConsider now the matrix Ce2
. The three rows have coefficients of variation of
.1432, .1197, and .0652 with an average of 10.94%, which appears fairly high,
but more or less acceptable. The matrix C has zero variability, and it is
consistent.
References
Hwang CL, Yoon K (1981) Multiple attribute decision making: methods and applications. Springer,
New York
Saaty TL (1980) The analytic hierarchy process: planning, priority setting, resources allocation.
McGraw-Hill, New York
References 399Inventory Models 12
Worldwide, companies hold billions of dollars in inventories. The main reason is to
create a buffer that balances the differences between the inflow and outflow of
goods. Inventories can be thought of as water tanks: there may be a constant inflow
of water that is pumped into the tank by a pump, while the outflow is low at night and
high in the morning (when people get up, take a shower, etc.); it then decreases
significantly until the demand again increases in the evening (when people come
home, do laundry, etc.), just to fall off again for the night. Other, popular, examples
include grocery stores whose inventories consist of various foodstuffs awaiting sale
to its customers. Here, the delivery of the goods is in bulk whenever a delivery truck
arrives, while the demand is unknown and erratic. In the case of hospitals, they have
in stock medical supplies, bed linen, and blood plasma. Again, the demand for these
items is uncertain and may differ widely from 1 day to the next.
All these instances have a few basic features in common. They have a supply, a
demand, and some costs to obtain, keep, and dispose of inventories. The next section
introduces a number of parameters and variables that are typically found in inventory
models. Section 12.2 describes a basic inventory model, and the subsequent sections
deal with a variety of extensions of the basic model.
12.1 Basic Concepts in Inventory Planning
For many organizations, inventories represent a major capital cost, in some cases the
dominant cost, so that the management of this capital becomes of the utmost
importance. When considering the inventories, we need to distinguish different
classes of items that are kept in stock. In practice, it turns out that about 10% of
the items that are kept in stock usually account for something in the order of 60% of
the value of all inventories. Such items are therefore of prime concern to the
company, and the stock of these items will need close attention. These most
important items are usually referred to as “A items” in the ABC classification system
developed by the General Electric Company in the 1950s. The items next in line are
# The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
H. A. Eiselt, C.-L. Sandblom, Operations Research, Springer Texts in Business and
Economics, https://doi.org/10.1007/978-3-030-97162-5_12
401the B items, which are of intermediate importance. They typically represent 30% of
the items, corresponding to about 30% of the total inventory value. Clearly, B items
do require some attention, but obviously less than A items. Finally, the bottom 60%
of the items are the C items. They usually represent maybe 10% of the monetary
value of the total inventory. The control of C items in inventory planning is less
crucial than that of the A and B items. The models in this chapter are mostly aimed at
A items.
The importance can be seen that as of 2021, the total business inventories in the
USA were evaluated at 2 trillion dollars. Given the economic importance of the
management of inventories, a considerable body of knowledge has developed as a
specialty of operations research. We may mention just-in-time (JIT) systems that
attempt to keep inventory levels in a production system at an absolute minimum and
put to work in Toyota’s so-called kanban system. There is also material
requirements planning (MRP) aimed at using the estimated demand for a final
product in order to determine the need for materials and components that are part
of a final product. Multi-echelon and supply-chain management systems also con￾sider similar aspects of production-inventory control systems. Such topics are
beyond the scope of this text, in which we can only cover some basic inventory
models.
Throughout this chapter, we will deal with inventory models that concern just a
single item. Consider an item for which the demand per period (typically a year, but
other time frames can easily be accommodated) is known or estimated to be D units.
Unless otherwise specified, the parameter D is assumed to be constant over time.
The number of items in stock is depleted over time by the demand. On the other
hand, the stock is also increased from time to time by additions caused by deliveries,
referred to as orders. Typically, replenishments are assumed to be instantaneous
(such as the arrival of goods by the truckload), resulting in sudden jumps in the
inventory level, whereas deliveries to satisfy demand are typically assumed to be
gradual. The order quantity is denoted by Q. In the models presented in this book,
Q turns out to be constant over time, given that the parameters of the model do not
change.
Related to the order quantity is also the lead time tL. The lead time is defined as
the time that elapses between the placement of an order and the moment that the
shipment actually arrives and is available on the shelf.
Figure 12.1 may illustrate the general inventory and ordering process. The
inventory level drops gradually from some point in time until it reaches a time t0.
At that point, a new shipment is received. The order and the resulting shipment are of
magnitude Q1. The inventory either shoots up immediately as shown here or
increases gradually. The moment the new shipment has arrived, customers continue
to take units out of the warehouse, so that the inventory level drops down again. At
the point t1 another shipment, this time of magnitude Q2, arrives. Again, the
inventory level drops down until it reaches zero at time t2, and after that, we
encounter shortages. At point t3, a new shipment of magnitude Q3 units comes
in. The first units are used to satisfy the backorders, and beyond that, the inventory is
built up again, and the next cycle begins.
402 12 Inventory ModelsThe cost for carrying one unit of the good in inventory for one period is called the
unit carrying or holding cost. We will denote this cost by the parameter ch.
Occasionally, the carrying cost ch will be specified as a proportion of the inventory
value rather than per unit of inventory. This will be clearly indicated whenever it
applies. The total carrying cost is therefore related to the inventory actually on hand
and will be zero for those time periods when the inventory level equals zero.
Carrying costs not only include the rental of warehouses, heating, cooling, lighting,
security, insurance, obsolescence, etc., but, as probably the most important compo￾nent, the cost of tied-up capital. Take, for instance, a car dealership with, say, 50 new
vehicles on the lot, each priced a modest $30,000. With an interest rate of 5%, the
annual holding costs are $75,000, equivalent to no less than 2½ vehicles. Similarly,
imagine that the inventory comprises, say, cell phones. Given that new models are
introduced once a year, the value of the inventory will drop dramatically a year later,
as the market for old cell phones is severely limited.
In contrast, the unit ordering costs co are defined as the cost of placing an order
and having it delivered. Ordering costs include administrative costs as well as
transportation costs. Ordering costs are usually considered to be independent of
the size of the order, but since transportation costs are involved, one may question
this assumption. However, we may argue that transportation costs may not be
directly related to the size of the order: if the order is delivered by container or
truck, the rate charged is usually not much dependent on whether the container is
one-quarter or three-quarters full; a similar argument applied to truckloads. Addi￾tionally, if the transportation costs were to relate to each item, then—assuming that
the demand must be satisfied—the total ordering cost for a period with a given
demand D would be the same, regardless of whether there are many small or few
large orders. Being a constant, this factor would then not affect the optimization.
Inventory 
Time
Q3
Q2
Q1
t
0 t
1 t
2 t
3
level
Fig. 12.1 The general inventory and ordering process
12.1 Basic Concepts in Inventory Planning 403Other cost types exist as well. For instance, in case of backorders, we allow
shortages, assuming that the sales are not lost, but will be fulfilled, once a new order
comes in. We need to note that these shortages are planned and not due to an
unexpected discrepancy between expected and real sales. The unit shortage costs
cs are defined as the cost of being short one unit for one time period (typically, but
not necessarily, a year). As such the unit shortage costs are an exact counterpart of
holding costs: while unit holding costs express the costs of having one unit in stock
for 1 year, shortage costs measure the cost of not having one unit in stock for a year.
12.2 The Economic Order Quantity (EOQ) Model
The most basic inventory model, the economic order quantity or EOQ model, has an
interesting history. Although the EOQ formula was first published in 1913 by Ford
Whitman Harris, it has been known under other names such as Camp’s formula and
Wilson’s lot size formula in the 1920s and 1930s. A good historical account is
provided in an article by Erlenkotter (1990), indicating deliberate attempts in the past
to deny Ford Whitman Harris the credit of originating the EOQ.
The assumptions of the basic economic order quantity are:
• The inventory consists of one single unperishable good held in one location.
• The demand rate for the item is constant over time and demand must be satisfied
exactly.
• Units are ordered from a single supplier in the same amount each time. For now,
we assume that replenishment is instantaneous, even though that is not really
necessary as we will show later.
• There are no quantity discounts.
• Stockouts are not allowed and the demand must be satisfied completely.
• The planning horizon is infinite, and all model parameters are stationary, i.e., they
do not change over time.
It is apparent that the problem has two components: how much to order and when.
The question of when to order is simply resolved in case of instantaneous
replenishments: since the inventory level is not allowed to become negative, and
since it does not make sense for a replenishment to occur while the inventory level is
positive, it must be optimal to place an order (and immediately receive the ship￾ment), when the inventory level reaches zero. Instead of looking at the time when to
reorder, we use the inventory level as a proxy for the time clock.
Since the order size was assumed to be the same each time, the inventory levels
over time will have the characteristic sawtooth pattern shown in Fig. 12.2, where the
time between two consecutive replenishment times is referred to as the inventory
cycle length tc, while the order quantity is Q.
We now wish to minimize the total inventory-related costs TC per period (year),
which is the sum of ordering costs and holding costs for the period, as well as the
purchasing costs.
404 12 Inventory ModelsFirst, consider the purchasing costs. Assuming that the annual demand is D and
the per-unit cost of an item are p, the total purchasing costs are pD. Assuming that
the demand must be completely satisfied and there are no price discounts, the
purchasing costs are actually a constant, so that they do not influence the optimiza￾tion and can therefore be left out of the analysis.
Next, there are the ordering costs. If we were to place N orders, each of size Q,
within the planning period, then the total amount ordered is NQ, which must equal
the demand D: since stockouts are not allowed, NQ cannot be less than D, and with
NQ > D we would be carrying more inventory than needed, incurring unnecessary
costs. With NQ ¼ D, we find that N ¼ D/Q equals the number of orders per period.
Incidentally, since tc is the length of one inventory cycle, of which there are N in one
period, tcN ¼ 1 period, so that tc ¼ 1/N ¼ Q/D. Recalling that the cost for one order is
co, we conclude that total annual ordering costs are coN ¼ coD/Q.
Since holding costs are charged per unit in inventory (the vertical axis in the
diagram in Fig. 12.2), we will need to compute the total area under the sawtooth
curve, which can be seen to be ½Q. In other words, the average inventory level is
½Q, implying that the total annual holding costs are ch(½Q). Hence, the total
inventory costs per period are
TC ¼ coD=Q þ 1=2chQ:
This expression makes intuitive sense: the larger the order quantity Q, the lower
the total ordering costs, since larger, but fewer, orders are being placed. However,
the holding costs will be higher, since more inventory is being kept in stock
(on average). This is illustrated in Fig. 12.3, where the abscissa measures the order
quantity and the ordinate measures the costs. The broken line (a rational function of
type “constant divided by the order quantity Q”) represents the ordering costs, while
the linear function (the broken line) represents the inventory holding costs. The solid
line is then the sum of the ordering and holding costs. If we were to include the
inventory level 
 time
tc
Q 
Fig. 12.2 Sawtooth inventory pattern
12.2 The Economic Order Quantity (EOQ) Model 405constant purchasing costs as well, the resulting total cost curve would simply move
up, but do not change in shape.
To find the value Q* that minimizes total inventory cost TC, we find the deriva￾tive with respect to Q, which results in
TC0 ¼ coD=Q2 þ ½ch:
Setting this expression equal to zero and solving for the variable Q results in the
optimal order quantity
Q ¼
ffiffiffiffiffiffiffiffiffiffi
2Dco
ch
r
:
(Technically, we also have to check the second derivative in order to ensure that
the optimal order quantity results in minimal, rather than maximal, inventory costs. It
does.) The expression for Q* is referred to as the economic order quantity EOQ.
Inserting this order quantity into the cost function, we obtain the total inventory costs
at optimum, which are
TC Q ð Þ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
½Dcoch
p þ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
½Dcoch
p ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffi
2Dcoch
p :
Note that it just so happens that holding and ordering costs are always equal at
optimum regardless of the value of the parameters D, co, and ch; however, the
solution Q* is not optimal simply because holding and ordering costs are equal.
Given Q*, we can then determine the related variables N* ¼ D/Q* and the
optimal inventory cycle length t

c ¼ 1=N ¼ Q=D.
costs 
order quantity Q
Fig. 12.3 Ordering, holding, and total inventory costs
406 12 Inventory ModelsExample: A retail store faces a demand of D ¼ 800 car battery chargers per year.
The cost of placing an order for the chargers is co ¼ $100 and the holding cost is
ch ¼ $4 per charger per year. The economic order quantity is therefore Q ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
2 800 ð Þð Þ 100
4
q
¼ 200 chargers, and the total cost is TC ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
2 800 ð Þð Þ 100 ð Þ4 p ¼ $800.
The optimal number of orders to be placed throughout the year is N* ¼
D/Q* ¼ 800/200 ¼ 4 orders, and the optimal inventory cycle length is t

c ¼
Q=D ¼ 200=800 ¼ 1=4 year ¼ 3 months . The total ordering costs are then
4(100) ¼ $400, which is half of the total cost; the other half is made up by the total
holding costs.
A useful feature of the economic order quantity is its insensitivity to errors in the
input data (i.e., the parameters). In our example, assume that the annual demand was
erroneously estimated to be 920 chargers, instead of the correct amount of
800 chargers, i.e., a 15% overstatement. Using the EOQ formula, we obtain the
nonoptimal and erroneous value Q* from the expression Q ¼
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
2 920 ð Þð Þ 100
4
q
ffi 214:5,
which is an overstatement by slightly more than 7%, only about half of the original
relative error. One can show that due to the square root of the formula, relative input
data errors result in relative EOQ errors of only about half the size, for reasonable
errors (say, 30% or less). Checking the total inventory costs that result from the
wrong data, we find (using the true value of D ¼ 800 and the erroneous order
quantity Q* ¼ 214.5) that TC(Q* ¼ 214.5) ¼ 100 800 ð Þ
214:5 þ 1=2 4ð Þð Þ¼ 214:5 $801:96,
which deviates only very marginally from the true value of $800.
12.3 The Economic Order Quantity with Positive Lead Time
We will now carry our discussion further and extend the model to the situation where
a positive lead time tL elapses from the moment an order is placed until the quantity
ordered has arrived and been added to the inventory.
It is apparent that the decision when to order will in no way affect the decision
how much to order. In other words, the optimum order quantity Q* still applies.
Instead of looking at the clock time for the point at which to place an order, we will
observe the inventory level and determine the order time in terms of the reorder
point R: when the on-hand inventory level decreases to the level R, an order is
placed, which will arrive after a delay of tL time units; at this point, the inventory
level, for optimal performance, should have reached zero. Putting a reorder decision
on the basis of the quantity still at hand rather than on the basis of a specific date is
much more flexible, particularly in practical situations, in which demand is not
continuous.
In our analysis, we will consider two different cases, depending on the length of
the lead time tL, which differ on the basis of the relation between lead time and
cycle time.
Case 1: tL  t

c , i.e., the lead time is less than or equal to the optimal inventory
cycle length. The demand during the lead time is then tLD, and it follows that if an
12.3 The Economic Order Quantity with Positive Lead Time 407order of size Q* is placed when the inventory level reaches R ¼ tLD, then the
replenishment will arrive exactly at the time when the inventory on hand has been
depleted, which is neither too soon, nor too late. This situation is depicted in
Fig. 12.4.
Case 2: tL > t

c , i.e., the lead time is greater than the optimal inventory cycle
length. The demand during the lead time is still tLD, but since tL > t

c, it follows that
tLD > t

cD ¼ Q, which is the highest level of inventory on hand that we will ever
reach. The arrival of an order will therefore occur during a subsequent inventory
cycle and not during the cycle in which it was ordered. This situation is illustrated in
Fig. 12.5, where t

c < tL < 2t

c .
Fig. 12.4 Lead time less than or equal to optimal inventory cycle length
Fig. 12.5 Lead time greater than optimal inventory cycle length
408 12 Inventory ModelsIn general, there could be several replenishments occurring during the lead time.
This number is actually tL
t
c
j k (the “floor of the number”), i.e., the ratio tL
t
c
j k rounded
down to the nearest integer. In Fig. 12.5, the floor equals 1, so that the replenishment
arrives at the end of the inventory cycle following after the one during which it was
ordered. In general, we obtain the relation
R ¼ tLD  tL
t
c
  Q
:
It can easily be demonstrated that this expression will cover both cases above.
Since for tL < t

c (as in Case 1), tL
t
c
j k ¼ 0, so that R* ¼ tLD, which is the expression
derived for that case.
Example: Consider the battery charger example above with D ¼ 800, co ¼ $100,
and ch ¼ $4, for which we have obtained an optimal order quantity of Q* ¼ 200 at an
annual cost of TC* ¼ 800. Given now a lead time of 2 months, i.e., 1/6 of a year, we
have tL ¼ 1=6 < 1=4 ¼ t

c, so that Case 1 applies. Here, we find the optimal reorder
point as
R ¼ tLD ¼ 1=6 800 ð Þ¼ 133 1
3 units:
On the other hand, with a lead time of tL ¼ 4 months, i.e., 1/3 of a year, we have
tL ¼ 1=3 > 1=4 ¼ t

c , so that Case 2 applies, and we find that
R ¼ 1=3 800 ð Þ 1=3
1=4
 ð Þ¼ 200 66 2
3 units:
As mentioned earlier, these shipments will not arrive in the inventory cycle they
are ordered in, but in the next cycle.
A practical way to implement the reordering in Case 1 above where tL  t

c is the
two-bin system. Upon replenishment, the order quantity Q* is physically separated
and put into two storage bins. The first bin has a capacity of Q*  R* units, while the
second bin holds R* units. The demand for items in stock is then satisfied exclusively
from the first bin until it is empty. At that time, only the R* items in the second bin
remain and the inventory manager will reorder the item. From this point onward, all
subsequent demand is now satisfied from the second bin. Given the assumptions
made in this section, the second bin will be depleted exactly at the time when the
next shipment arrives and the process is repeated.
A similar argument allows the two-bin system to be implemented in Case 2 with
tL > t

c. Supermarkets use a “virtual” version of the two-bin system by electronically
monitoring inventory levels by counting items that pass through the checkout
counters. Orders are then automatically triggered when the appropriate reorder
point has been reached.
12.3 The Economic Order Quantity with Positive Lead Time 40912.4 The Economic Order Quantity with Backorders
Assume now that we allow backorders, so that the inventory level may become
negative, in the sense that unsatisfied demand is recorded or “backordered,” to be
satisfied immediately upon replenishment of the inventory. That way, there will be
no lost sales. Such planned shortages are considered to incur a shortage cost per unit
and per period. These unit shortage costs will be denoted by cs. These costs consist of
the inconvenience to the customer of the unsatisfied demand and they will be
difficult to estimate in practice. The costs could also include special handling costs
that are incurred due to the preferential delivery to the customer of the backordered
units when they become available. As stated before, unit shortage costs are very
similar to unit holding costs: while unit holding costs express the costs to have one
unit of the good in stock for a year, unit shortage costs express the costs of not having
one unit of the good in stock for a year.
Assuming a repetitive situation, the net inventory level will be as in Fig. 12.6,
where S denotes the amount of the maximal planned shortage. Note in the figure that
the maximal inventory directly after replenishment is no longer Q as in the standard
model, but Q  S, as the stockouts are satisfied first before new inventory is built up.
In Fig. 12.6, the parameter t1 denotes the length of time during an inventory cycle
during which the inventory level is nonnegative, i.e., when there is no stockout. On
the other hand, the parameter t2 denotes the length of time during which there is no
stock at hand. Clearly, t1 + t2 ¼ tc. Using the geometric relationship of the two
similar (shaded) triangles in Fig. 12.6, we find that t1/t2 ¼ (Q  S)/S. We will use this
expression below.
We will now determine optimal levels of the order size Q and the optimal largest
shortage level S simultaneously by minimizing total inventory costs. These costs
now include not only ordering and holding costs as before, but also shortage
costs. As in the standard economic order quantity model, the annual ordering costs
Fig. 12.6 Inventory with planned shortages
410 12 Inventory Modelsare coD/Q. As far as the carrying costs are concerned, we find that the average
inventory level is obtained by averaging the inventory level during the time that no
stockouts occur. This weighted average is ½(Q  S) during the time t1, while the
inventory level during the time stockouts occur is zero for the duration t2. After some
calculations, this leads to inventory holding costs of ch ¼ ð Þ QS 2
2Q .
We can now deal with the average shortage in a similar fashion. The average
annual shortage is ½S during the time t2, which is when we have shortages. This
leads to total shortage costs of cs S2
2Q. The total inventory costs are therefore
TC Qð Þ¼ , S co
D
Q þ ch
ð Þ Q  S 2
2Q þ cs
S2
2Q :
Using partial derivatives, we can show that the total inventory costs are
minimized for
Q ¼
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
2Dco
ch
ch þ cs
cs
r
and S ¼
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
2Dco
cs
ch
ch þ cs
r
¼ ch
ch þ cs
Q
:
Example: Using the basic EOQ example above with its parameters D ¼ 800,
co ¼ $100, and ch ¼ $4 to which we add unit shortage costs of cs ¼ $6 per unit and
year, we can then determine the optimal order quantity as Q ¼
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
2 800 ð Þð Þ 100
4
4þ6
6
q

258:20 units: . The optimal shortage can then be determined as S ¼ ch
chþcs Q 
103:28. The total costs are then TC(Q*, S*) ¼ 309.84 + 185.90 + 123.94 ¼ $619.68.
Note that while this model includes more cost items than the basic EOQ model,
the same demand and cost parameters cost less in this model. The reason is that this
model allows the decision maker an added possibility, viz., to run planned shortages.
Nobody says that we have to have shortages (this model certainly allows not having
any), but here it is cost-effective to plan some shortages. As a matter of fact, the
relation between the unit shortage cost and the unit holding cost will determine the
magnitude of the planned shortage: if cs/ch is large, then shortages are expensive and
the solution will include only minor shortages. If, on the other hand, cs/ch is small,
then shortages are comparably cheap, and the model will prescribe large shortages.
To push that argument even further, suppose that shortage costs would increase
beyond all reasonable limits, i.e., cs ! 1. Then the “correction factor” chþcs
cs in
the order quantity root will approach 1, so that the order quantity Q* will assume the
same value as in the basic EOQ model. At the same time, the magnitude of the
planned shortage will tend to zero, as the shortage costs appear only in the denomi￾nator of the formula. Thus, it becomes clear that the basic model is just a special case
of the model with shortages, given that shortage costs are infinitely high. This result
is nothing but an application of the usual principle of penalty costs: if there is
something that we do not want, assign a very high penalty to it, and as a result,
the optimizer will not include the very expensive option in the solution. Another
extreme case exists if cs approaches zero. In this case, S* ¼ Q*, and Q* will assume
12.4 The Economic Order Quantity with Backorders 411the largest possible value, which is the demand for all future periods. The important
part is that in this case, there will be no inventory at all.
12.5 The Economic Order Quantity with Quantity Discounts
So far we have assumed that the unit purchasing cost p is constant and independent
of the order size Q. Recall that in the original economic order quantity model, the
costs actually were the sum of ordering, holding, and purchasing costs, viz.,
TC Qð Þ¼ , p coD=Q þ ½chQ þ pD:
However, we argued, for a fixed price p, the purchasing costs during the period
under consideration are pD, which is a constant, which does not influence the
solution, so that we could (and did) ignore the purchasing costs. In practice,
however, many suppliers will offer incentives for purchases of larger quantities in
the form of lower unit costs. The basic economic order quantity model described in
Sect. 12.2 can easily be modified to take such quantity discounts into consideration.
For simplicity, we will restrict ourselves to the standard model with no shortages
allowed and three price levels: the original non-discounted price and two discount
levels. It is straightforward to extend the model to any number of discount levels.
Before we proceed, we have to make a minor modification. Recall that the main
component of the holding costs is the cost for tied-up capital. Given that we paid a
fixed price for the good so far, this cost could simply be expressed as a dollar amount
for each unit in stock. Since we are now paying a price that is no longer fixed but
does depend on the discount level that we choose, the unit holding costs have to be
redefined. This is most easily done by defining ch as a proportion of the unit
purchasing price p. Given that, the economic order quantity is redefined as
Q ¼
ffiffiffiffiffiffiffiffiffiffi
2Dco
chp
r
:
We will denote the given (non-discounted) price level as p0, the price with the
small discount as p1, and the price given the large discount as p2. Clearly, p0> p1> p2.
(In general, we can have as many price levels as desired.) The rationale behind this
scenario is simple. For the regular price of p0, we can obtain any quantity we desire.
In order to convince our supplier to sell the goods for us at the lower price of p1, we
have to purchase at least a certain quantity of goods. This quantity will be called Q1.
Going one step further, we can ask our supplier to let us have the goods even cheaper
at a price of p2, which he may agree to, but only if we order at least Q2 units with the
obvious condition that Q2 > Q1.
At this point, we have a cost function for each of the price levels. This situation is
shown in Fig. 12.7.
The three cost functions are shown as TC( p0), TC( p1), and TC( p2), respectively.
The dots on the cost functions denote their respective optimal points, at which the
412 12 Inventory Modelscosts are minimized. The minimal quantity levels that allow for the discounts are also
shown.
Consider now the actual costs that we incur as the order quantity gradually
increases. For very low order quantities, we must be on the highest cost curve, as
we do not qualify for a discount. As we increase the value of Q, the costs decrease
and reach their minimum at the dot on the TC( p0) curve. As Q increases further, the
costs increase as well until we reach Q1. At this point, we do qualify for the small
discount, so that our actual costs jump down onto the cost curve TC( p1). As this
point happens to be to the right of the minimum on this function, the costs increase as
Q increases. This process continues until we reach Q2, the value that allows us to
obtain the second (larger) discount. Again, the costs drop at this point onto the third
and lowest cost function TC( p2). Increasing the value of Q further increases the total
costs.
The piecewise nonlinear cost function is shown as a bold line in Fig. 12.7. In
order to determine the order quantity with the overall minimal costs, we have to
examine each cost curve separately. More specifically, we determine the optimal
order quantity at each price level, and then compare them and choose the option with
the lowest costs.
First consider the highest cost curve without a price discount. We simply deter￾mine the point of lowest cost with the EOQ and record the associated cost. This is the
optimal solution given the option of paying the regular price p0.
We then continue to examine the costs incurred when paying the price p1. Again,
we determine the optimal quantity at this price by solving the economic order
quantity with this price. (Note that with decreasing prices, the optimal order quantity
increases slightly, as the expression in the denominator chp decreases.) We then have
to determine whether or not this quantity permits us to obtain the discount. If so, we
have found our optimal order quantity at this level. If this is not the case, we have to
move out of the optimum, but, as the function is increasing the farther we move out
of the optimum, just as much as required to qualify for the discount.
Fig. 12.7 Inventory cost functions for different price levels
12.5 The Economic Order Quantity with Quantity Discounts 413In our illustration in Fig. 12.7, the optimal order quantity is less than Q1, the
lowest quantity that qualifies for the price p1. We thus increase the order quantity to
Q1 and determine the costs at that point. This is the optimal order quantity given the
price p1.
This process is repeated for all discount levels. Once this has been accomplished,
we simply compare the best-known costs at each price level and choose the overall
minimum. This is our optimal solution; in Fig. 12.7 it is Q2.
This process can be illustrated by the following numerical:
Example: A company faces an annual demand for 10,000 footballs. The purchas￾ing costs are $2 per football, the holding costs are 5% of the purchasing price per
football and year, and the costs of placing one order are $80. The supplier now offers
a ½% discount in case the company orders at least 6000 units. As an alternative, the
supplier also offers a 1% discount, if the company orders at least 15,000 units.
Consider all alternatives, compute the total costs in each case and make a
recommendation.
The parameters of the problem include D ¼ 10,000, ch ¼ 5% of p, and co ¼ $80.
Case 1: No discount, so that p0 ¼ $2. Then ch ¼ $0.10, and we use the EOQ to
compute the order quantity as Q* ¼ 4000 with costs of
TC* ¼ 200 + 200 + 20,000 ¼ $20,400.
Case 2: Small discount, so that p1 ¼ $1.99. Then ch ¼ $0.0995, and the solution
of the EOQ is Q* ¼ 4010.038. This quantity does not qualify for the discount, so that
we have to move out of the optimum just as much as necessary to qualify for the
discount. Hence, we set Q:¼ 6000, for which we then obtain costs of
TC(6000) ¼ 133.33 + 298.50 + 19,900 ¼ $20,331.83.
Case 3: Bigger discount, so that p2 ¼ $1.98. Then ch ¼ $0.099, and the solution
of the EOQ is Q* ¼ 4020.15. This quantity does not qualify for the discount, so that
we have to move out of the optimum just as much as necessary to qualify for the
discount. Hence, we set Q: ¼ 15,000, for which we then obtain costs of
TC(15,000) ¼ 53.33 + 742.50 + 19,800 ¼ $20,595.83.
Comparing the three options, Case 2 offers the lowest total costs, so that we
should order 6000 footballs, obtain a ½% discount, and incur total costs of
$20,331.83.
12.6 The Production Lot Size Model
As an alternative to ordering models of the type discussed so far in this chapter, we
may produce the desired items ourselves. In such a case, the items will not arrive in
one bulk as they do in case of orders, but they arrive one piece at a time from our
machines. Assume for the time being that we cannot regulate the speed with which
our machines produce the units: we either turn the machine on, in which they churn
out r units per day (this is our production rate), or we turn off the machine, in which
case we make nothing. Recall that our annual demand was assumed to be of
magnitude D, from which we can easily compute the daily demand d, which is D/
365, D/360, or D/250 (working days), depending on the decision maker’s
414 12 Inventory Modelsspecifications. Before engaging in any computations, it is necessary to determine
whether or not the system has any feasible solutions. The simple regularity condition
is that r  d. If this condition is not satisfied, then it will not be possible to satisfy the
total demand, and we have to find ways that allow us to do so. The following
arguments assume that the regularity condition is satisfied.
Batch or intermittent production as described above occurs in many vertically
integrated companies, where the ordered items are produced internally. A production
run can then be considered an order, with the production run size corresponding to
the order size Q, and the production setup costs corresponding to the ordering costs
co.
Using an argument similar to that in Sect. 12.2, we note that D/Q is the number of
setups or production runs per period, so that the total setup costs are co(D/Q). As far
as the carrying costs are concerned, we will consider the production phase tr (the
phase during with production and demand occur) and the demand phase td (the phase
during which production does not occur, while demand occurs as usual) separately.
In the production phase, inventory accumulates at the rate of (r  d). We notice that
the duration of the production phase is tr ¼ Q/r, so that the maximal level of
inventory at the end of each production run will be (r  d)Q/r. During the demand
phase, the inventory, which starts with a level of (r  d)Q/r, decreases to zero in a
linear fashion at a rate of d, so that the slope of the function in Fig. 12.8 during that
phase is –d. Note that the main difference between this model and the basic EOQ
model is the gradual increase of the inventory level in this case. The average
inventory level during the entire cycle of duration tc ¼ tr + td is then ½ (r  d)Q/r.
Therefore, the total carrying costs per period are ½ch(r  d)Q/r. As a result, the total
production- and inventory-related costs are
time 
inventory 
on hand 
maximum 
inventory 
level 
tr td
Fig. 12.8 Inventory levels in the production lot size model
12.6 The Production Lot Size Model 415TC ¼ coD=Q þ ½chð Þ r  d Q=r:
Following the same procedure applied to the standard economic order quantity in
Sect. 12.2, we find the derivative with respect to the single variable Q, which results
in TC0 ¼ coD/Q2 + ½ch(r  d)/r. Setting the derivative equal to zero results in the
unique optimal lot size of
Q ¼
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
2Dco
ch
r
r  d
r
:
(Again, we have to check the second derivative so as to ensure that the lot size
actually minimizes the costs. It does.)
We can now illustrate the economic lot size by means of a numerical.
Example: A bottling plant faces an annual demand of 200,000 bottles of a certain
type. It can produce these bottles at a rate of 1000 bottles per day during each of the
300 working days in a year. Setup costs for a production run are $1000, and each
bottle has a carrying cost of 10¢ per bottle and year.
We first check whether or not the regularity conditions holds. Here, we have
r ¼ 1000 > 666.67 ¼ 200,000/300 ¼ d, so that the condition is indeed satisfied.
Thus, we can compute the optimal quantity made during one production run as
Q ¼
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
2 200, 000 ð Þð Þ 1, 000
0:10
1, 000
1, 000666:67 q
ffi 109, 545 bottles. The corresponding costs are
TC(Q*) ¼ $3651.50.
An interesting observation concerns the relation between the optimal lot size
developed above and the basic economic order quantity. Assuming that we can
equate the production setup cost and the unit order cost in the two respective models,
we find that the optimal value of Q in the lot size model is never smaller than the
order quantity in the economic order quantity, and that the costs in the lot sizing
model are never larger than those of the EOQ. As a matter of fact, the economic order
quantity can be seen as a special case of the lot sizing model with an infinite
production capacity (as exemplified by the fact that an order in the EOQ arrives
with infinite speed). Increasing r to arbitrarily high values has the expression r/
(r  d) approach one, so that the lot size formula reduces to the standard economic
order quantity. Applying this argument to our numerical example, we find that the
economic order quantity with the same parameters as those used in the example
equals Q* ¼ 63,245.55, a policy that costs TC(Q*) ¼ $6324.56.
Along similar lines, it is also interesting to note that more capable machines, i.e.,
those with higher production rates, incur higher costs. As a matter of fact, the
machine with the “optimal” production rate has r ¼ d, so that inventories are
unnecessary, as customers satisfy their demand at the same rate the machine
produces the goods. This is yet another example for optimal solution that fitting
the production to the demand results in solutions with the lowest cost (for another
example, see the “technology choice” example in the linear programming
formulations).
416 12 Inventory ModelsIn conclusion, just like the economic order quantity, the production lot size model
has the attractive property of being robust, i.e., quite insensitive to changes of the
parameters (input data).
12.7 The Economic Order Quantity with Stochastic Lead Time
Demand
So far in this chapter we have assumed a deterministic environment, in which all
relevant data are known with certainty, and in which the consequences of our actions
are completely predictable. We will now extend our analysis to situations involving
uncertainty and begin with the simple but important case, in which the demand
during the lead time is a random variable. However, we assume that the demand
during the lead time follows a discrete probability distribution that is known to
us. The random behavior of the demand may cause undesired and unplanned
stockouts and surpluses. While the demand is irregular throughout, we are only
concerned about the irregularity that occurs between the time that we have placed an
order (i.e., after the reorder point has been reached), and the time that the next
shipment arrives.
Whenever an unplanned surplus occurs due to the irregularity of the demand, we
have in fact carried more inventory than was actually needed, thus incurring unnec￾essary carrying costs, while in case of an unplanned stockout, there will be a penalty
cost cp charged for each unit we are out of stock. Note that cp is assumed to be
independent of the length of time that we are out of stock. This is in contrast to the
shortage costs cs for the backorder model of Sect. 12.4, where the shortage cost was
defined per quantity unit and per time unit. The difference between cp and cs in the
backorder model and this model is that the shortages in Sect. 12.4 were planned
deliberately, while the shortages in this section occur because there is a higher-than￾expected demand during the lead time tL.
Formally, we define cp as the penalty cost per unit and stockout. Furthermore, we
have D ¼ E Dð Þ as the expected value of demand per year, the lead time demand dL
(a random variable), and the expected value of dL is dL ¼ E dð ÞL . The (discrete)
probability distribution of the lead time demand dL is p(dL), while F(dL) is the
cumulative probability distribution of dL. We will restrict our discussion to the
case, in which dL is a discrete random variable. Furthermore, in this simple model,
the length of the lead time tL is still deterministic, i.e., fixed and known to the
decision maker. We also assume that R  dL  0; i.e., on average, there is still a
positive inventory level when replenishment occurs. If this condition were not to be
required, we would, on average, run out of stock at the end of each cycle. Therefore,
we may regard the quantity R  dL as the amount of stock that is kept at all times.
For this reason, this quantity is usually referred to as the expected safety stock or
buffer stock.
12.7 The Economic Order Quantity with Stochastic Lead Time Demand 41712.7.1 A Model that Optimizes the Reorder Point
The objective in this section is to minimize the sum of the carrying costs for the
expected safety stock plus the expected penalty costs for stockouts. This sum will be
denoted by TC1(R, Q), since it depends on the reorder point R as well as on the order
quantity Q. To start, we will simply use the order quantity QEOQ, which was obtained
independently of the reorder point by way of the economic order quantity. This can
be justified because of the robustness of the economic order quantity formula. We
then obtain the partial cost function
TC1 R, QEOQ   ¼ ch R  dL
  þ cp
D
QEOQ   X
dL>R
ð Þ dL  R p dð ÞL ,
where QEOQ ¼
ffiffiffiffiffiffiffi
2Dco
ch
q
, and where the first part of the relation is the cost for carrying
the safety stock. The summation in the second part of relation is taken over all
instances, in which shortages occur, so that we compute the expected shortage level.
Differentiating TC1(R, QEOQ) with respect to R and setting the resulting expression
to zero yields the condition for the optimal reorder point R*, which is
P dL > R ½ ¼ chQEOQ
cpD :
As P[dL > R
] ¼ 1  P[dL  R
] ¼ 1  F(R
), we obtain
F R ð Þ¼ 1  chQEOQ
cpD :
Since we have assumed that dL is a discrete random variable, its cumulative
distribution function F will be a step function that assumes only discrete values in the
interval [0, 1]. Therefore, it is unlikely that the right-hand side of the above equation
will equal one of these discrete values. As a way out of this dilemma, we let R*
denote the smallest value that satisfies the inequality
F R ð Þ 1  chQEOQ
cpD :
Note that we only have to consider the possible values of dL for R*.
In order to illustrate the above discussion, consider the following numerical
Example: Consider again the battery charger example of Sect. 12.2 with a
demand of D ¼ 800, ordering costs of co ¼ $100, and holding costs of ch ¼ $4
per charger per year. Furthermore, assume that the penalty costs are cp ¼ $5 per
418 12 Inventory Modelscharger and stockout. Suppose that the expected annual demand is D ¼ 800 .
Suppose that the demand during lead time has the following probability distribution.
dL (units) p(dL) F(dL)
70 0.1 0.1
75 0.2 0.3
80 0.2 0.5
85 0.3 0.8
90 0.2 1.0
The economic order quantity in this example equals QEOQ ¼ 200 units, so that
1  chQEOQ
cpD ¼ 1  4 200 ð Þ
5 800 ð Þ ¼ 0:8, and since the smallest value of dL with F(dL)  0.8
equals 85, we have R ¼ 85. As the expected demand dL ¼ E dð Þ¼ L
P
x
xpLð Þ¼ x
81:5, the expected safety stock will equal R  dL ¼ 85  81:5 ¼ 3:5 units: The
carrying cost for the expected safety stock is then ch R  dL
  ¼ 4 85 ð Þ¼  81:5
$14 , and the expected penalty cost is cp D
QEOQ 	 
 P
dL>R
dL  R ð Þp dð Þ¼ L 5 800
200 	
ð Þ 90  85 ð Þ¼ 0:2 $20 . Note that stockouts occur only if dL > R ¼ 85, which
happens only in case dL ¼ 90, an occurrence that has a probability of 0.2.
12.7.2 A Stochastic Model with Simultaneous Computation of Order
Quantity and Reorder Point
We can now refine the above model and determine the order quantity Q and the
reorder point R simultaneously. For that purpose, we consider the expected total cost
of ordering, carrying, and penalty, i.e.,
TC2ð Þ¼ Q, R co
D
Q þ ch ½Q þ R  dL
  þ cp
D
Q
  X
dL>R
ð Þ dL  R p dð ÞL :
Using partial differentiation with respect to Q and to R and setting the result to
zero yields
Q ¼
ffiffiffiffiffiffi
2D
ch
r
co þ cp
X
dL>R
dL  R ð Þp dð ÞL
 !and
F R ð Þ 1  chQ
cpD :
Again, it is understood that R is taken to be the smallest value that satisfies the
inequality. The above two relations should be solved simultaneously, which is
difficult, since Q* and R* appear in both. Instead, we will use an iterative procedure
12.7 The Economic Order Quantity with Stochastic Lead Time Demand 419that shuttles between these two relations. It commences with an order quantity Q*,
uses the second of the two relations to determine a reorder point R*, uses this reorder
point in the first relation to compute a revised value of Q*, and so forth until the
process converges and the numbers do not change anymore. As an aside, note again
that if the penalty costs get very large, the order quantity reduces again to the
standard economic order quantity of the basic model.
This process will be illustrated in the following numerical:
Example: Consider again the situation of the example in the previous section with
a demand of D ¼ 800, ordering costs of co ¼ $100, holding costs of ch ¼ $4 per
charger per year, penalty costs cp ¼ $5 per charger and stockout, and the above
probability distribution of the demand.
Again, we obtain Q ¼ 200, so that R* ¼ 85, just as in the previous procedure.
Using the modified economic order quantity, we then find a revised value of Q* as
Q ¼
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
2 800 ð Þ
4 ½  100 þ 5 5ð Þð Þ 0:2
r
 204:94 units:
Using this revised order quantity in the latter of the two relations, we find that
F R ð Þ 1  ð Þ4 ð Þ 204:94
5 800 ð Þ  :795, so that R* ¼ 85 again, and thus the procedure
terminates.
Comparing the results for Q* and R* of the simple model in the previous
subsection and the refined approach in this subsection, we notice that in both cases
the reorder point is R* ¼ 85 units, whereas the order quantity is Q* ¼ 200 units in
the simple model (obtained by using the standard economic order quantity), while it
is not very different at 204.95  205 units in the refined model. Again, this
demonstrates the robustness of the economic order quantity formula.
12.8 Extensions of the Basic Inventory Models
This section offers an outlook on some inventory policies. Following the standard
terminology (which is in some conflict of the symbols that we have used so far), we
define s as the reorder level (what we have referred to so far as the reorder point, i.e.,
the inventory level at which an order is placed), R as the intervals at which the
inventory level is checked, and S as the inventory level we have directly after a
replenishment.
We now distinguish between periodic and continuous review systems. In a
periodic review system, we check the inventory levels at regular intervals R (e.g.,
hourly, daily, or weekly), while in a continuous review system, we continuously
watch the inventory level.
An order-point, order-quantity, or (s, Q) policy involves continuous review (i.e.,
R ¼ 0) at which time an order of a given magnitude Q is placed whenever the
inventory reaches a prespecified reorder level s. An example of an (s, Q) policy is the
two-bin system described in Sect. 12.3.
420 12 Inventory ModelsAn order-point, order-up-to-level, or (s, S) policy is another continuous review
policy. Its key is an inventory level S that is specified by the inventory manager. This
is an inventory level to be attained directly after a shipment is received. So, once the
reorder point s is reached, an order of size S - s is placed, which then increases the
inventory level to S. This may be a reasonable policy in case the demand is irregular,
so that at the time that an order is placed the inventory level may suddenly dip below
s, at which time the regular order quantity may then not be sufficiently large.
A periodic review, order-up-to-level, replenishment cycle, or (R, S) policy is a
periodic review policy. At each review instant (which occurs at intervals of length
R time units) an order is placed of a size that raises the inventory level to S.
In addition, there are hybrid policies such as the (R, s, S) policy, where at review
time nothing is done, if the inventory level is above s, whereas if it is at or below the
level s, an order is placed to increase the inventory level to the magnitude S.
Each of the above policies has its own advantages and drawbacks, and it depends
on the practical situation at hand which one is the most appropriate choice. Typi￾cally, continuous review policies such as the (s, Q) and (s, S) policies are suitable for
the A items in the ABC classification introduced at the beginning of this chapter. For
B and C items, the cost of continuous review of the inventory level may not be
justified, so that periodic review policies may make more sense. For C items, the
review interval length R may be set, so that the review is done less frequently for
these items of minor value. Modern computerized inventory control systems are of
great help with any inventory system.
Exercises
Problem 1 (EOQ with positive lead time, shortages, production lot size): A
retailer faces an annual demand for 2,400,000 shirts with the “Mumbo Jumbo
Man-Savior of the Universe” logo. It costs $450 to place a single order and the
costs for keeping a single shirt in stock for an entire year are 60¢.
(a) How many units should the retailer order each time an order is placed and what
are the associated costs?
(b) Given the result under (a), how many orders should be placed and what is the
time between two consecutive orders (given a 360-day year)? What is the
reorder point if the lead time were 20 days?
(c) Assume now that it is possible to allow shortages, given that the portion of the
demand that cannot immediately be satisfied will be satisfied immediately after
the next shipment arrives. It has been estimated that the associated loss of
goodwill equals costs of 80¢ per shirt and year. Compute the order quantity,
the maximum shortage, and the associated costs.
(d) What would happen to the results in (c) if the unit shortage costs were to increase
by, say, 10¢? Explain in one short sentence, indicating the reason why.
Calculations are not required.
12.8 Extensions of the Basic Inventory Models 421(e) Suppose now that the retailer were to purchase the equipment to make the shirts
in-house. The machine is capable of making 10,000 shirts per day (again based
on a 360-day year). What is the number of shirts made in each production run?
What are the total costs?
(f) How would the results under (e) change if the capacity of the machine under
(e) were not 10,000 units per day but only 6000? Explain in one short sentence.
Solution:
(a) D ¼ 2,400,000, Co ¼ 450, Ch ¼ 0.6, so that Q* ¼ 60,000 and TC* ¼ $36,000.
(b) Then N* ¼ 40 and t

c ¼ 1=40 years ½ ¼ 9 days ½  . Reorder point R ¼
20 2, 400, 000 ð Þ =360 20
9
 60, 000 ¼ 13, 333:33.
(c) Q* ¼ 79,372.54 and S* ¼ 34,016.80. Costs TC ¼ (holding costs) + (ordering
costs) + (shortage costs) ¼ 7775.27 + 13,606.72 + 5831.45 ¼ $27,213.44.
(d) If cs increases, shortages become more expensive, so that the order quantity Q*
and the maximum shortage S* both decrease, while the total costs will increase.
(e) The regularity condition is satisfied. Q* ¼ 103,923.05 and
TC* ¼ 10,392.30 + 10,392.30 ¼ $20,784.60.
(f) The regularity condition is violated, i.e., the machine capacity is insufficient to
satisfy the demand.
Problem 2 (EOQ, positive lead time, shortages, production lot sizing): A
retailer faces an annual demand for 4000,000 pairs of sneakers with the “King
Bong” logo. It costs $100 to place a single order and the costs for keeping a single
pair of sneakers in stock for an entire year are 50¢.
(a) How many pairs of sneakers should the retailer order each time an order is
placed and what are the associated costs?
(b) Given the result under (a), how many orders should be placed and what is the
time between two consecutive orders (given a 250-day year)? What is the
reorder point if the lead time were 8 days?
(c) Assume now that it is possible to allow shortages, given that the portion of the
demand that cannot immediately be satisfied will be satisfied immediately after
the next shipment arrives. It has been estimated that the associated loss of
goodwill equals costs of 20¢ per pair of sneakers and year. Compute the order
quantity, the maximum shortage, and the associated costs.
(d) What would happen to the results in (c) if the unit shortage costs were to
decrease by, say, 5¢? Explain in one short sentence, indicating the reason
why. Calculations are not required.
(e) Suppose now that the retailer were to purchase the equipment to make the
sneakers in-house. The machine is capable of making 20,000 pairs per day
(again based on a 250-day year). How many pairs of sneakers are made in each
production run? What are the total costs?
422 12 Inventory Models(f) How would the results under (e) change if the capacity of the machine under
(e) were not 20,000 units per day but 25,000? Explain in one short sentence. No
calculations are necessary.
Solution:
(a) D ¼ 4000,000, Co ¼ 100, Ch ¼ 0.5, so that Q* ¼ 40,000 and TC* ¼ $20,000.
(b) Then N* ¼ 100 and t

c ¼ 1=100 years ½ ¼ 2:5 days ½  . Reorder point R ¼
8 4, 000, 000 ð Þ =250 8
2:5
 40, 000 ¼ 8, 000
(c) Q* ¼ 74,833.15 and S* ¼ 53,452.25. Costs TC ¼ (holding costs) + (ordering
costs) + (shortage costs) ¼ 1527.21 + 5345.22 + 3818.02 ¼ $10,690.45.
(d) If cs decreases, shortages become even cheaper, so that the order quantity Q*
and the maximum shortage S* both increase, while the total costs will decrease.
(e) The regularity condition is satisfied. Q* ¼ 89,442.72 and
TC* ¼ 4472.14 + 4472.14 ¼ $8944.28.
(f) The order quantity will decrease and the inventory-related costs will increase.
Problem 3 (quantity discounts): The annual demand for a product is 9000, and
the unit price of the product is $5. The holding costs are estimated to be 10% of the
price of the product, and the costs of placing a single order are $1000.
(a) Calculate the optimal order quantity and the associated costs.
(b) The supplier now offers a discounted price of $4.80 if we order at least
8000 units at a time. Should we take the offer? What are the associated costs?
(c) Our supplier adds another offer: a price of $4.75, if we purchase at least
18,000 units (so that we have to order only every other year). Should we take
this offer?
Solution:
(a) With a unit price of p0 ¼ $5, we obtain Q ¼
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
2 9, 000 ð Þð Þ 1, 000
ð Þ :1 ð Þ5
q
¼ 6, 000 with
TC* ¼ 1500 + 1500 + 45,000 ¼ $48,000.
(b) The discounted price of p1 ¼ $4.80 leads to an optimal quantity of
Q* ¼ 6123.72. This quantity does not qualify for a discount, so that we set Q:
¼Q1¼8000. The associated costs areTC(Q1)¼1125 + 1920 + 43,200¼$46,245.
As the total costs are lower at this price level, we should take the discount and
order 8000 units at a time, which will cost us $46,245.
(c) The deeply discounted price of p2 ¼ $4.75 leads to an optimal order quantity of
Q* ¼ 6155.87, still not enough to qualify for the discounted price level. As a
result, we must increase the order quantity to the smallest level that
allows the discount, i.e., Q ¼ Q2 ¼ 18,000. At this level, the total costs are
TC(Q2) ¼ 500 + 4275 + 42,750 ¼ $47,525, less than paying the full price, but
not as good as the smaller discount determined under (b). Hence, the overall best
option is to order 8000 units each time we place an order, a policy that will cost
us $46,245 in each cycle.
12.8 Extensions of the Basic Inventory Models 423Problem 4 (EOQ with stochastic lead time demand): Let the annual demand
for a certain item be 1000 units in the planning period. The holding costs are $5 per
unit, and the cost of placing an order is $36 per order. The penalty cost for stockouts
is $3 per unit and stockout. There is a demand for 55 units during the lead time with a
probability of 20%. The demand is 60 with a probability of 0.4, it is 65 with a
probability of 0.30, and it is 70 with a probability of 0.10.
(a) Calculate the expected demand during lead time.
(b) Determine the economic order quantity and the resulting reorder point.
(c) Find the order quantity and the reorder point by simultaneous computation.
(d) What is the buffer stock?
(e) What are the minimal expected ordering, holding, and penalty costs?
Solution:
(a) E(dL) ¼ (55)(0.2) + (60)(0.4) + (65)(0.3) + (70)(0.1) ¼ 61.5.
(b) QEOQ ¼
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
2 1, 000 ð Þ36
5
q
¼ 120 units, and with F R ð Þ 1  5 120 ð Þ
3 1, 000 ð Þ ¼ 1  0:2 ¼
0:8, so that R* ¼ 65.
(c) Revising Q ¼
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
2 1, 000 ð Þ
5 ½  36 þ 3 5ð Þ0:1
q
 122:47 units, F R ð Þ 1 
5 122 ð Þ :47
3 1, 000 ð Þ  0:7959, so that R* ¼ 65 units.
(d) The buffer stock is R  dL ¼ 65  61:5 ¼ 3:5 units:
(e) Costs TC2 ¼ (ordering costs) + (holding costs) + (penalty
costs) ¼ 293.95 + 323.68 + 12.25 ¼ $629.87.
Reference
Erlenkotter D (1990) Ford Whitman Harris and the economic order quantity model. Oper Res 38:
937–946
424 12 Inventory ModelsStochastic Processes and Markov Chains 13
Some of the previous chapters have dealt with random events in an ad hoc fashion.
This chapter will deal with such events in a systematic way. In general, in stochastic
processes, events occur over time. Time can be dealt with either in continuous
fashion, or in discrete fashion. In the continuous case, we may look at the speed of
an automobile at any given point in time or at the inventory level of a product in a
supermarket at any time. In the discrete case, speed or inventory level are observed
only during specific points in time, e.g., each minute, once a week or at similar
intervals. In this chapter, we only deal with discrete-time models. The following
three sections will introduce some of the basic ideas of stochastic processes and
Markov chains.
13.1 Basic Ideas and Concepts
Consider first the random events that take place. Using the same examples as above,
there is an infinite number of different speeds that a vehicle could be moving at,
while the demand for a product may be very large but is hardly infinite. Some types
of events are much more restrictive: as an example, consider a light bulb. It will
always be in exactly one of two “states of nature,” in that it either works, or it does
not. This is referred to as the state space, i.e., the number of different states the
“system” can possibly be in. As already hinted at, the individual states are similar to
the states of nature in decision analysis, see Chap. 10 in this volume. This chapter
deals only with processes that have a finite state space.
Each event in this discrete-time, finite state space process is then a random
variable Xt that depends on the time t at which it is observed. As an illustrative
example, consider a used car. The vehicle is in one of four states: it either runs well
(state s1), it runs with minor problems (state s2), it runs with major problems (state
s3), or it fails altogether (state s4). At any point in time, the vehicle is in exactly one
of these four states. It stands to reason that the state that the vehicle is in 1 year does
depend on the state the car was in the year before. More specifically, we can define
# The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
H. A. Eiselt, C.-L. Sandblom, Operations Research, Springer Texts in Business and
Economics, https://doi.org/10.1007/978-3-030-97162-5_13
425transition probabilities pij, which indicate the probability that the vehicle is in state j,
given that it was in state i in the previous year. (We assume that no repairs are
performed.) It is apparent that the transition probabilities are conditional
probabilities of the type pij ¼ P(Xt+1 ¼ j| Xt ¼ i), or in simple words, the probability
that the random variable is in state sj in year t + 1, given that it was in state si in year t.
As a numerical illustration, consider the matrix P ¼ ( pij) of transition probabilities
P ¼
0:90 0:06 0:03 0:01
0 0:85 0:10 0:05
0 00:75 0:25
0 0 01:00
2
6
6
6
4
3
7
7
7
5:
For example, if the vehicle has been running well in the previous year, then there
is a 90% chance that it will be running smoothly in this year as well, as shown by the
element p11. Or, if the vehicle experiences minor problems this year, then there is a
5% chance that it will fail altogether next year, as shown by the element p24. We
should point out that a similar example is known in the literature as the “bad debt”
example, in which the states of a debt are “paid in full,” “in good standing,” “in
arrears,” and “gone to collection.”
A few features of this particular transition matrix are noteworthy. First of all, note
that all elements below the main diagonal (i.e., in the lower left corner) are zero. This
simply means that, given that no repairs are performed, the vehicle will never
improve. Secondly, we can observe that the sum of probabilities in each row of
the transition matrix equals one. This is always the case, as the transition
probabilities are conditional probabilities, and as such, given that we are in a row
at time t, we must choose a successor state for time t + 1. Thirdly, notice that once the
car is in state s4 (i.e., the vehicle fails), then it will never get out of it again. Thus, the
state s4 is referred to as an absorbing state. The example presented here is a
stochastic process with the Markovian property that holds, if the present state of
the process depends only on the state of the system immediately prior to this and the
transition probabilities, meaning that the system does not have a (long) memory. As
a historical aside, Andrey Andreyevitch Markov, 1856–1922, was a Russian mathe￾matician who made important contributions to the field of probability and statistics.
A nice visual representation is the transition diagram. In this diagram, the nodes
represent the states of the process, and the arcs represent transitions with a positive
Fig. 13.1 Transition diagram for automobile example
426 13 Stochastic Processes and Markov Chainsprobability. The transition diagram for our automobile example is shown in
Fig. 13.1.
Note that absorbing states have only arcs leading into them, but not out, at least
not to other states. Here, we also assume that the process is stationary, meaning that
the transition probabilities do not change over time.
An obvious question to be asked is what happens, if we go through more than one
transition. In other words, the transition probabilities tell us what the likelihood is to
be in one state one period after the process starts. But what about two or more
periods? This is the question we discuss in the next paragraphs. In order to facilitate
the discussion, consider a simple example involving stock prices. In particular, we
only consider an upward movement of the price and a downward movement. The
transition probabilities are shown in the matrix
P ¼ 0:2 0:8
0:7 0:3
 :
In other words, if the stock went up today, then there is a 20% chance that it will
go up again tomorrow, while there is an 80% chance that the stock’s price will
decline. Similarly, if the price of the stock decreased today, then there is a 70%
chance that it will increase tomorrow and a 30% chance that it will decrease
tomorrow. When analyzing changes after multiple periods, we could use a time￾state graph as shown in Fig. 13.2, where the nodes st
i indicate the state of nature i at
the end of period t, the arcs denote the possible transitions, and their values are the
transition probabilities. As the transition probabilities are stationary, an arc from,
say, st
i to s
tþ1
j will have the same value as, say an arc from s
tþ2
i to s
tþ3
j .
Considering Fig. 13.2, assume now that we are presently in an upswing of the
stock price, i.e., in state s0
1. To find the probability that there will be an increase in
stock prices 2 days later, we will have to examine all paths from the present state s0
1
to the state s2
1. Here, there are exactly two paths: the first path leads from s0
1 to s1
1 and
on to s2
1, while the second path leads from s0
1 first to s1
2 and then to s2
1. Note that the
first path considers a price increase on the first day, while the second has a decrease
on the first day. The probability is then calculated as the sums of probabilities of all
paths from s0
1 to s2
1 , which, in turn, are computed as the product of all transition
probabilities along the path. In this example, the probability on the former path s0
1, s1
1,
Fig. 13.2 Time—state graph
for stock price example
13.1 Basic Ideas and Concepts 427s2
1 is (.2)(.2) ¼ .04 (meaning that there is only a 4% chance of two price increases in a
row), while the latter path s0
1, s1
2, s2
1 has a probability of (.8)(.7) ¼ .56, meaning that
the probability of a price decrease on day one, followed by a price increase on day
two has a probability of 56%. This means that we obtain the transition probability
that that state changes from a price increase on day t to a price increase on day t + 2 as
0.04 + 0.56 ¼ 0.60. Similarly, we compute the remaining probabilities, resulting in
the matrix P2
, which indicates the transition probabilities from day t to day t + 2. It is
P2 ¼ 0:6 0:4
0:35 0:65  :
This procedure can be repeated for any number of days. While this is possible, the
procedure is extremely awkward. We can achieve the same results by simple matrix
multiplication. In particular, we can obtain P2 ¼ PP, P3 ¼ P2
P ¼ PPP, and so forth,
so that we obtain Pr ¼ PPP...P, i.e., the matrix P multiplied itself r times. For
instance, in the stock example, we obtain
P3 ¼ P2
P ¼ 0:6 0:4
0:35 0:65   0:2 0:8
0:7 0:3
  ¼ 0:4 0:6
0:525 0:475  :
This means that given that there has been an increase in prices on day one, then
there is a 40% chance that there will be a rise in prices again 2 days later, and similar
for the other elements of the matrix.
We may also be interested in the mean first passage time rij, which is the expected
number of transitions needed to reach state j from state i for the first time. For i ¼ j,
i.e., elements rii, we talk of the mean recurrence time, which is then the expected
number of transitions occurring after leaving state i, until we return to this state for
the first time. For the car example above the element r23 would denote the expected
number of transitions required to reach state 3 (“major problems”) for the first time
after being in state 2 (“minor problems”). For the stock price example, the element
r11 denotes the mean number of days between two consecutive days with upward
price moves.
To compute the mean first passage times rij, assume that the current state is “i.”
From this state, we will reach state j in a single step with probability pij. Moving from
state i to state k with k 6¼ j in a single step, and from there, an average of rkjsteps from
state k to state j. Therefore
rij ¼ pijð Þþ 1 pi1 1 þ r1j
  þ pi2 1 þ r2j
  þ ... þ pi,j1 1 þ r j1,j
 
þpi,jþ1 1 þ r jþ1,j
  þ þ ... þ pim 1 þ rmj  ,
assuming that there are a total of m states. However, since pi1 + pi2 + ... + pim ¼ 1,
we can simplify the above expression to
rij ¼ 1 þ pi1r1j þ ... þ pi,j1r j1,j þ pi,jþ1r jþ1,j þ ... þ pimrmj:
As a numerical example, consider the stock price example, for which we obtain
428 13 Stochastic Processes and Markov Chainsr11 ¼ 1 þ p12r21,
r12 ¼ 1 þ p11r12 or, equivalently r12 ¼ 1= 1  p11 ð Þ ð Þ ,
r21 ¼ 1 þ p22r21 or, equivalently,r21 ¼ 1= 1  p22 ð Þ ð Þ , and
r22 ¼ 1 þ p21r12:
It follows that r11 ¼ 1 + p12/(1 – p22) and r22 ¼ 1 + p21/(1 – p11). For example, the
mean number of days between two consecutive days with price increases will be
r11 ¼ 1 + 0.8/(1 – 0.3) ¼ 15/7  2.14 days. Furthermore, after a day with price
increases, the expected number of days until a price decrease occurs for the first time
is r12 ¼ 1/(1 – p11) ¼ 1 – (1 – 0.2) ¼ 5/4 ¼ 1.25 days.
For the car example, we obtain 16 equations for the mean first passage times rij,
making them a bit harder to compute. Actually, with the particular feature of
deteriorating conditions leading to the absorbing state of “fail,” r44 ¼ 1, and
r41 ¼ r42 ¼ r43 ¼ 1.
So far, we have only dealt with conditional probabilities, i.e., given that a certain
state of nature prevails in the beginning, we computed the probability that some
(other) state of nature occurs after a given number of periods. Below, we start the
process with an initial probability distribution that assigns an unconditional proba￾bility u0
i to each state of nature si. These probabilities are then collected in the initial
probability row vector u0 ¼ u0
1, u0
2, ... , u0
m
  , assuming that there is a total of
m states of nature. The unconditional probabilities that describe the likelihood that
the system is in one of the states of nature after 1 week is u1
, which can be computed
by simple vector-matrix multiplication as u1 ¼ u0
P. In general, we have
ur ¼ ur1
P ¼ u0
Pr
:
As an illustration, consider again the car example. Recall that its single-stage
transition matrix was
P ¼
0:90 0:06 0:03 0:01
0 0:85 0:10 0:05
0 00:75 0:25
0 0 01:00
2
6
6
6
4
3
7
7
7
5
with the states of nature being s1: running well, s2: running with minor problems, s3:
running with major problems, and s4: not running. Suppose now that the car is
initially running with minor problems, i.e., u0 ¼ [0, 1, 0, 0]. After 1 year, we have the
probabilities
13.1 Basic Ideas and Concepts 429u1 ¼ u0
P ¼ ½  0, 1, 0, 0
0:90 0:06 0:03 0:01
0 0:85 0:10 0:05
0 00:75 0:25
0 0 01:00
2
6
6
6
4
3
7
7
7
5 ¼ ½  0, 0:85, 0:10, 0:05 ,
i.e., there is no possibility that the vehicle with perform perfectly, there is an 85%
chance that it will continue to run with minor problems, there is a 10% chance that
the problems are now major, and there is a 5% chance that the vehicle will fail
altogether. For the second year, we obtain
u2 ¼ u1
P ¼ ½  0, 0:85, 0:10, 0:05
0:90 0:06 0:03 0:01
0 0:85 0:10 0:05
0 00:75 0:25
0 0 01:00
2
6
6
6
4
3
7
7
7
5
¼ [0, 0.7225, 0.16, 0.1175], indicating that there is now a 72.25% chance that the
minor problems will persist, a 16% chance that the problems are now major, and an
11.75% chance that the vehicle will fail. Repeating the calculations, we can compute
probabilities in this way for any future year.
13.2 Steady-State Solutions
Assuming that the process converges, we will call the resulting solution a steady￾state solution. Clearly, a steady-state solution is an ideal concept that is not very
likely going to be realized in practice: take, for instance, the used car example. While
we may keep the vehicle for a long time, the time will be finite, while a steady state, a
state that no longer depends on the initial conditions, is typically reached only after
an infinite number of transitions. Still, the steady state is an important concept that
will tell us to what a process converges, provided that it converges at all. A sufficient
condition for the existence of a steady state is given if each state can be reached from
each other state on a path that has positive probability.
We showed in the previous section that un ¼ u0
P ¼ un-1P, and if n tends to
infinity, we obtain u1 ¼ u1P. To distinguish the steady-state solutions from all
others, it is customary to replace u1 by the row vector π, so that the steady-state
solutions will satisfy π ¼ πP. In addition, we have to ensure that the sum of all
elements in π equals 1, as all components of π are probabilities that are mutually
exclusive and collectively exhaustive.
In the used car example, the system of simultaneous linear equations is
430 13 Stochastic Processes and Markov Chainsπ1 ¼ 0:9π1
π2 ¼ 0:06π1 þ 0:85π2
π3 ¼ 0:03π1 þ 0:10π2 þ 0:75π3
π4 ¼ 0:01π1 þ 0:05π2 þ 0:25π3 þ π4
π1 þ π2 þ π3 þ π4 ¼ 1:
The first equation requires that π1 ¼ 0, inserting this result in the second equation
leads to π2 ¼ 0, and using this result in the third equation leads to π3 ¼ 0. The fourth
equation then reduces to the tautological identity π4 ¼ π4, but the last equation then
helps to solve the system with π4 ¼ 1. This is the obvious result mentioned earlier in
this section: whatever state the vehicle is in now, eventually it will be in a failed state.
The stock example provides another illustration of the concept. It is easily seen
that a steady state is certain to exist. In order to determine the (unconditional) steady￾state probabilities, we solve the system
½ ¼ π1, π2 ½  π1, π2
0:2 0:8
0:7 0:3
 ,
which can be written as
π1 ¼ 0:2π1 þ 0:7π2
π2 ¼ 0:8π1 þ 0:3π2, coupled with
π1 þ π2 ¼ 1:
The system has steady-state probabilities π1 ¼ 7=15 and π2 ¼ 8=15 , or
π ¼ [0.4667, 0.5333]. In other words, in the long run, we can expect stock prices
to rise about 47% of the time, while we can expect them to drop about 53% of
the time.
One can show that for the mean recurrence time rii, the relationship rii ¼ 1/πi
holds. For our stock price example, π1 ¼ 7/15 means that r11 ¼ 15/7 days, in
agreement with what we calculated in Sect. 13.1. Furthermore, r22 ¼ 1/π2 ¼
15/8 ¼ 1.875 days will be the mean time between two consecutive days of price
decreases.
13.3 Decision Making with Markov Chains
This section demonstrates how we can use the results obtained in the previous two
sections in the context of decision making. To explain, consider again the used car
example of the previous sections. Suppose now that there are three automobiles for
sale. They are of the same make and model and the transition probabilities in the
matrix P above apply to all of them. One vehicle is in perfect running order, and it
sells for $5000. The second vehicle runs with minor problems, and it sells for $4000,
13.3 Decision Making with Markov Chains 431while the third vehicle has major problems and it costs $2000. After 2 years we want
to sell the vehicle again. Its price then (as it does now) will depend on its state. In
particular, it has been estimated that a vehicle in perfect condition will sell for $3500,
one with minor problems sells for $2500, and a car with major problems sells for
$500. We assume that there is no difference between the cars in maintenance costs
and that no repairs are made (which is not really realistic). Which car should we
purchase?
We can address the problem by considering each option, one at a time. If we
purchase the vehicle that is in perfect condition, then we decide that u0 ¼ [1, 0, 0, 0].
After 1 year, we obtain u1 ¼ [0.9, 0.06, 0.03, 0.01] and after 2 years we have
u2 ¼ [0.81, 0.105, 0.0555, 0.0295]. In other words, when we attempt to sell the
vehicle, it will still be in perfect shape with a probability of 81%, it will have minor
problems with a probability of 10.5%, and so forth. The expected price we can sell
the vehicle for is then 3, 500(0.81) + 2, 500(0.105) + 500(0.0555) + 0(0.0295) ¼
$3, 125.25, resulting in a loss of 5000  3125.25 ¼ $1874.75.
The other two vehicles are dealt with similarly. If we purchase the car with minor
flaws, we decide to choose u0 ¼ [0, 1, 0, 0] and obtain the probability vectors
u1 ¼ [0, 0.85, 0.10, 0.05] after 1 year and u2 ¼ [0, 0.7225, 0.16, 0.1175] after 2 years
of ownership, so that the expected price of the car at the time of sale is $1886.25,
which means a loss of $2113.75.
Finally, consider the car with major flaws. Deciding to purchase it in the begin￾ning means to set u0 ¼ [0, 0, 1, 0], so that by the end of year 1, we have u1 ¼ [0,
0, 0.75, 0.25], and by the end of the second year (or, equivalently, at the beginning of
year 3), we have probabilities u2 ¼ [0, 0, 0.5625, 0.4375], so that the expected price
of sale is $281.25 for an expected loss of $1718.75. Given our assumptions, all
options result in losses, but our best bet would be to purchase the car with major
flaws.
Consider now the possibility of either purchasing a warranty repair policy for
$100 per month or pay the $1800 repair bill, whenever state s4 occurs. In either case,
whenever the system enters state s4, (the car fails), it stays in this state for one period
during which it is repaired and, at the end of this period, returns to s1 (runs without
problems). The transition matrix then changes to
P ¼
0:90 0:06 0:03 0:01
0 0:85 0:10 0:05
0 00:75 0:25
1000
2
6
6
6
4
3
7
7
7
5:
As opposed to the previous case without repairs, this transition matrix does not
have an absorbing state. Finding the steady-state probabilities for this case, we
obtain π ¼ ½ ¼ π1, π2, π3, π4
50
89 , 20
89 , 14
89 , 5
89    ½  :5618, :2247, :1573, :0562 . As
above, we can purchase a vehicle in state s1, s2, and s3 for $5000, $4000, and
$2000, respectively, while we now add that a failed vehicle can be purchased for
$1000. (Earlier, it made no sense to purchase a vehicle in state s4, as there were no
432 13 Stochastic Processes and Markov Chainsrepairs.) This means that in the long run, the annual expected cost of repairing the
vehicle is 0.0562(1800) ¼ $101.16 if we pay ourselves, or $100 for the policy,
which makes the purchase of the policy slightly superior.
Consider now again the case, in which four vehicles of the same make and model
are offered, one in each of the four states. The purchase prices and the repair costs are
the same as above. The idea is to purchase the vehicle at the end of year 0 (or,
equivalently, the beginning of year 1) and sell it again at the end of year 3 (the
beginning of year 4). The price for which the vehicle can be sold at that time has been
estimated to be $2500 if it is running perfectly (state s1), $1500 if it runs with minor
problems (state s2), $800 in case it has major problems (state s3), and $100 if it fails
(state s4). Analyzing this case necessitates the use of solution trees rather than the
computation of the states u0
, u1
, u2
, and u3
. The reason is that we can, for instance,
purchase a vehicle with major flaws today and sell it at the end of year 3 with major
flaws. However, it may happen that this vehicle simply stayed in this state, or it
failed, we repaired it and it was perfect for 1 year, and then it degenerated again to
the state with major problems. The two cases have the same initial state and the same
state at the end of the planning period, but they incur very different costs.
In particular, we will need four probability trees, one for each vehicle we can
purchase. For reasons of limitations of space, we display only the solution tree for
the vehicle that is initially running with major problems. The different states are
abbreviated as P (perfect running condition, state s1) Mi (state s2, minor problems),
Ma (state s3, major problems) and F (state s4, fail). All possible sequences of states
that may occur are shown in Fig. 13.3. The two numbers next to the last set of nodes
when we sell the vehicle denote the probability that this string of events occurs, and
the costs occurred throughout the time that we own the vehicle. The latter includes
the purchase price plus repairs, if any, minus the price we obtain when we sell the
car. As an example, consider the string of events that can be described as
P  F  P  Mi. In other words, the car runs perfectly when we purchase it, it
then fails in the next year, after which it runs perfectly again, and it exhibits minor
problems after that. The probability for such a sequence of events is (0.01)(1)
(0.06) ¼ 0.0006, and the costs include the purchase price of $5000, the repair bill
of $1800, and the sales price of $1500, resulting in overall costs of $5300. The
expected costs for purchasing a vehicle in perfect condition and keeping it for 3 years
are then $2888.42.
We can then perform a similar analysis with a vehicle that is initially in state s2
(running with minor problems). The tree is a bit smaller, and the expected costs are
$3650.21. Purchasing a car that has major problems will cost $1375.92, while
buying a vehicle that fails and must be repaired right away will have expected
costs of $564.15, making this the preferred strategy of the buyer.
Exercises
Problem 1 (brand switching, computation of steady-state probabilities): Three
major motel chains compete for the lucrative economy market: Sunny 6, Cloudy
7, and Rainy 8. It has been observed that customers who stay in one of the motels
usually patronize the same chain again, except if they perceive the service to be poor.
13.3 Decision Making with Markov Chains 433In particular, in case of Sunny 6, 10 percent of the time, the service is perceived to be
poor, in which case customers change to one of the other two chains with equal
probability. In case of Cloudy 7, service is perceived to be satisfactory 80 percent of
the time; if it is not, customers switch to one of the other two chains with equal
probability. Finally, 90 percent of the time service at Rainy 8 is deemed to be ok; if it
is not, customers always switch to Sunny 6.
(a) Set up the transition matrix P.
(b) In the long run, what percentage of customers patronizes the three motels? What
are the mean recurrence times for the three motels?
(c) Management of the Cloudy 7 chain perceives that its customer loyalty is not as
good as one could wish. By using of better management techniques, they may be
able to reduce the proportion of poor service to ten percent (again, in case of
poor service, customers switch to the other chains with equal likelihood). What
are the new steady-state proportions?
(d) Given that one percent of business is worth $500,000, what is the maximum
amount that the management of Cloudy 7 should be prepared to pay to imple￾ment the new management techniques that result in the improved service?
Fig. 13.3 Probability tree for vehicle with major problems
434 13 Stochastic Processes and Markov ChainsSolution:
(a) The transition matrix is
P ¼
:9 :05 :05
:1 :8 :1
:1 0 :9
2
6
4
3
7
5:
(b) The steady-state probabilities are π ¼ 4=8, 1=8, 3=8 ½ ¼ ½  0:5000, 0:1250, 0:3750 .
With rii ¼ 1/πi we conclude that r11 ¼ 2, r22 ¼ 8, and r33 ¼ 8/3  2.67
transitions. Therefore, a customer staying at a Sunny 6 motel will on average
first return to a Sunny 6 motel two transitions later.
(c) The revised transition matrix is
P ¼
:9 :05 :05
:05 :9 :05
:1 0 :9
2
6
4
3
7
5:
The steady-state probabilities are π ¼ 4=9, 2=9, 3=9 ½ ¼ ½  0:4444, 0:2222, 0:3333 .
(d) The market share of the Cloudy 7 chain has increased by 9.7222%, which is
worth $4,861,100, which is the maximal amount the management of Cloudy
7 should pay for the change.
Problem 2 (criminal recidivism, value of a policy change): Consider a city of
500,000 people, whose inhabitants have been classified by the authorities as either
“not criminal,” “misdemeanor,” or “felon.” Long-term studies indicate that the
transition of an individual from one state to another from 1 year to the next is
shown in the transition matrix
P ¼
0:95 0:04 0:01
0:50 0:40 0:10
0:10 0:30 0:60
2
6
4
3
7
5:
City council contemplates a new program that costs $70,000,000 and changes the
transition probabilities to
P0 ¼
0:95 0:04 0:01
0:70 0:25 0:05
0:20 0:40 0:40
2
6
4
3
7
5:
Assume that an individual in the “misdemeanor” category costs about $1000
annually, while the costs of a felon are $10,000 each year.
13.3 Decision Making with Markov Chains 435(a) What is the probability that a felon does not commit any crime in the next
2 years with and without the crime prevention initiative?
(b) What is the probability that an individual in the “misdemeanor” category does
not commit any crime in the next 3 years with and without the crime prevention
initiative?
(c) What are the steady-state probabilities with and without the initiative? Compare
the long-term costs with and without the initiative.
(d) What are the mean recurrence times for the three categories with and without the
incentive? How many years, on average, does it take for an individual to move
from the “felon” to the “misdemeanor” category, without the crime prevention
initiative?
Solution:
(a) The probability of the string “felon”—“no crime”—“no crime” is (0.1)
(0.95) ¼ 0.095 without the initiative and (0.2)(0.95) ¼ 0.19 with it.
(b) (b) The probability of the string “misdemeanor”—“no crime”—“no crime”—
“no crime” is (0.5)(0.95)(0.95) ¼ 0.45125 without and (0.7)(0.95)
(0.95) ¼ 0.63175 with it.
(c) The steady-state probabilities are π ¼ ½ ¼ π1, π2, π3 210=239, 19=239, 10=239 ½ 
½  0:87866, 0:07950, 0:04184 without the initiative and ½ ¼ π1, π2, π3
860=935, 56=935, 19=935 ½  ½  0:91979, 0:05989, 0:02032 with it. Without the initia￾tive, the expected costs per individual are $497.908, while they are $263.102
with it. For the city of 500,000, this means costs of $248,954,000 without the
initiative and $131,551,000 with it, a savings of $117,403,000. Since the
savings exceed the costs of $70,000,000, the initiative should be introduced.
(d) Using the values for the steady-state probabilities computed in (c) and the fact
that rii ¼ 1/πi, we obtain mean recurrence times for the three categories without
the incentive as r11 ¼ 239/210  1.138, r22 ¼ 239/19  12.58, and r33 ¼ 239/
10 ¼ 23.9 years. Considering the lifespan of individuals, mean recurrence times
in excess of twenty years may be less meaningful. If the crime prevention
program goes ahead, the mean recurrence times will change to r11 ¼ 935/
860  1.087, r22 ¼ 935/56  16.70, and r33 ¼ 935/19  49.21 years. Again,
caution must be advised when interpreting the practical aspects of long mean
recurrence times.
To compute the expected number of years to move from the “felon” category to the
“misdemeanor” category without the initiative, we need to compute r32. This can be
accomplished by solving the following system of two equations in two unknowns:
r12 ¼ 1 þ p11r12 þ p13r32, and
r32 ¼ 1 þ p21r12 þ p33r32:
436 13 Stochastic Processes and Markov ChainsSolving this system with the probabilities shown in the matrix P (i.e., p11 ¼ 0.95,
p13 ¼ 0.01, p31 ¼ 0.10, and p33 ¼ 0.6), we obtain r12 ¼ 410/19  21.58 years and
r32 ¼ 150/19  7.89 years, which will be the expected time for an individual to move
from the “felon” to the “misdemeanor” category.
13.3 Decision Making with Markov Chains 437Reliability Models 14
In the previous chapter on stochastic processes, we considered a system, which could
be in different states in any one time period, and in which the transition from one
state to another can occur randomly. Reliability theory in its simplest form includes a
number of independent components, each of which is in exactly one of two states: it
functions properly, or it does not. Our task is to calculate the probability that the
entire system works. The system under consideration may describe a life-changing
or life-saving scenario. Consider, for instance, a passenger elevator in a building. Its
proper functioning may determine whether or not the passenger live to tell of their
experience. On the other hand, if a hardware or software component of a personal
computer crashes, it could probably result in some inconvenience to the user but
would not be fatal. In reality, deadly accidents related to elevator malfunctions are
quite rare: worldwide, there are about 27 elevator deaths per year. On the other hand,
computer crashes are an almost everyday occurrence.
Given the tremendous differences between the consequences in these two, admit￾tedly extreme, examples, the setup of the two systems will differ: one will include
extensively tested components with high reliability and/or backup systems in case of
a failure, while computer systems, and sometimes emergency use medications, are
put on the market as beta versions that have not gone through rigorous testing.
Reliability theory will determine the probability that a system functions well,
given the probabilities of all of its components. The system may be small with only a
few components, or highly complex with thousands or even millions of components.
Each of its components may actually be a system in itself with many subcomponents,
which, in turn, may consist of sub-subcomponents and so forth. This is the reason
why we refer to a hierarchy of systems, and it is up to the modeler to choose the level,
at which a reliability analysis of a system and its components is to be undertaken.
The first section of this chapter studies how the reliability of a system depends on
the configuration and reliability of its components. The second section introduces
time aspects and investigates how the breakdown of components and the entire
systems occur over time from a probabilistic view. Section 14.3 extends the proba￾bilistic analysis by defining a so-called hazard function, which is subsequently used
# The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
H. A. Eiselt, C.-L. Sandblom, Operations Research, Springer Texts in Business and
Economics, https://doi.org/10.1007/978-3-030-97162-5_14
439to compute the reliability over time for different probability distributions of compo￾nent lifetime. The remaining two sections consider redundancy and standby systems,
as well as how reliability can be estimated.
Modern reliability theory has its roots in statistical quality control, developed in
the 1930s by W. Shewhart, H.F. Dodge and H.G. Romig, all at Bell Labs at the time.
Another line of thought, dating to the 1950s, is due to W. Weibull, whose distribu￾tion bearing his name is used in connection with the hazard function. For a full
account of reliability, see, e.g., the text by E.A. Elsayed (2020).
14.1 Fundamentals of Reliability Systems
The proper functioning of a system will depend on the functioning and arrangement
of its components. The reliability R of a system refers to the probability that the
system will function properly over some given period of time. Assuming that the
system consists of n components, each with an individual reliability pj, j ¼ 1, ..., n,
the system reliability depends on the pj values as well as the system structure
(or configuration). A common way to display the structure is by way of a network
representation called a block diagram. We are located on the left side of the diagram
and have a message that is to be delivered through the network to the right side. For
each component, we know the probability that the message is processed correctly.
The task is now to send the message through the network, so that it arrives on the
right side of the system. For the transmission of the message, there are some rules.
Whenever there is a fork, the message will be duplicated and sent along all forks.
Whenever a message enters a component, the probability that it leaves the compo￾nent in readable form equals the probability of the component to function. The
probability that at least one message arrives at the end of the process on the right side
of the diagram is then the reliability of the system.
One of the simplest such structures is the series structure, in which case the
system functions if and only if all of its components function. A typical example
concerns a vehicle. Suppose that we consider the motor (1), the brakes (2), and the
steering system (3), with individual reliabilities of p1 ¼ 0.8, p2 ¼ 0.9, p3 ¼ 0.9. A
block diagram of the series structure is shown in Fig. 14.1.
Statistically, the requirement that (component 1 must function) and (component
2 must function) and ... is expressed as the product of the individual probabilities,
here R ¼ p1p2p3 ¼ (0.8)(0.9)(0.9) ¼ 0.6480, a fairly dismal performance character￾istic. Note that the series in the block diagram expresses solely the fact that all
components must function for the entire system to work properly. The interpretation
is similar to that of a project network, in which tasks shown in series must all be
Fig. 14.1 The series structure of the example
440 14 Reliability Modelsperformed (where in the reliability block diagrams the sequence does not play
a role).
On the other hand, individual components may be backups or alternatives of each
other. For instance, we may use three cables 1, 2, and 3 in a cable car that is pulled by
a wire rope, such as the one in San Francisco. Suppose now that the reliability of
each cable is p1 ¼ 0.7, p2 ¼ 0.6, and p2 ¼ 0.9, respectively. The overall system
functions if at least one of the system functions. This is ensured in all cases except if
all systems fail. This means that the failure probability, the complement of reliabil￾ity, is 1  R, which is defined as the probability that all components fail, i.e., (1  p1)
(1  p2)(1  p3) ¼ (0.3)(0.4)(0.1) ¼ 0.012, so that the reliability of the system is
R ¼ 0.988 or 98.8%. In a block diagram, this situation is displayed in a parallel
structure as shown in Fig. 14.2.
For the general serial structure, we can easily show that R  min
1jn
p j (“a chain is
no stronger than its weakest link”), so that if any pj ¼ 0, then R ¼ 0 as well. On the
other hand, for the general parallel structure, we find that R  max
1jn
p j
, so that if any
pj ¼ 1, then R ¼ 1.
Clearly, few systems in engineering are pure series or pure parallel systems. As an
example, consider a mixed system with three components 1, 2, and 3, which
functions if either components 1 and 2 both function, and/or component 3 functions.
Assuming that the reliabilities of the three components are 0.5, 0.9, and 0.7, this
situation is shown in Fig. 14.3.
The probability that components 1 and 2 are both working is (0.5)(0.9) ¼ 0.45.
Now, the probability that the system functions equals [(Prob that 1 works) and (Prob
that 2 works)] or [Prob that 3 works]. Thus 1  R ¼ (1  0.45)(1  0.7) ¼ 0.165, or
simply R ¼ 83.5%.
A general parallel-series system contains m parallel paths, each consisting of
n components arranged in series, as shown in Fig. 14.4.
Fig. 14.2 The parallel
structure of the example
Fig. 14.3 A mixed system
14.1 Fundamentals of Reliability Systems 441The system shown in Fig. 14.4 is actually quite general. In case the parallel paths
have different lengths, we simply introduce fictitious “dummy” components, with
reliability of 1, so that each of the parallel paths has the same length.
The reliability of a parallel-series system is calculated by first determining the
reliability of each series subsystem (the product of the individual reliabilities) and
then the unreliability 1  R as the product of the unreliabilities of each series
subsystem.
As an example of parallel-series systems, consider an oceanographic research
vessel operating in a remote location at sea without any satellite, VHF, or cell phone
coverage. The results of its scientific measurements need to be transmitted to its
onshore base location by radio, a so-called single-sided band (SSB) shortwave. Its
messages consist of seven consecutive data components, each of which has only a
90% probability of being transmitted correctly; additionally, all seven pieces of data
need to be successfully radioed for the message to be received by the base station. At
the base station, any incoming defective message will be automatically filtered,
ensuring that only correctly received information is accepted. Our task is now to
determine how many times a message needs to be sent so as to reach a 95%
probability that a correct message is received at the base station. Alternatively, we
are interested in our course of action in case a 99% reliability is required.
We can model this situation as a parallel-series reliability system with m parallel
paths, each indicating a full transmission of the entire message. Each such path
consists of seven data components. With n ¼ 7 for each path and m parallel paths, the
reliability of any single path (i.e., the reliability that one complete message is
transmitted properly) equals (0.9)7 ¼ 0.4782969. Then the unreliability of the entire
system given that the entire message is transmitted four times equals
1  R ¼ (0.4782969)4 ¼ 0.0523, so that with four full messages transmitted, the
probability that at least one complete message is received properly is R ¼ 92.59%.
For m ¼ 5, we obtain a reliability of 0.9614, so that we need to transmit the full
message at least five times in order to obtain a probability of more than 95% that the
message is received properly. Increasing m further, we determine that it takes
8 transmissions of the message to achieve a reliability of R ¼ 99.45%, the smallest
number of transmissions that achieves at least 99% reliability.
Fig. 14.4 A general parallel-series system
442 14 Reliability ModelsAnother type of organizational system is a series-parallel system. These types of
arrangements are essentially parallel subsystems arranged in series. A simple exam￾ple is shown in Fig. 14.5, for which we calculate the reliability of the system as
follows. The probability that both parallel components 2 and 3 do not work is (1 –
0.9)(1 – 0.7) ¼ 0.03, so that the reliability of the parallel segment is 0.97. The
reliability is then (0.5)(0.97) ¼ 0.485.
A general series-parallel system contains n subsystems, arranged in series fash￾ion, each of which is a parallel system of m components as shown in Fig. 14.6.
Again, the system in Fig. 14.6 is quite general. It can easily accommodate
different numbers of parallel components. In such a case, we simply add fictitious
“dummy” components with zero reliability, so that all parallel subsystems have the
same number of components.
Yet another type of reliability systems are k-out-of-n systems, which function if
and only if at least k out of the n system components function. Special cases are n￾out-of-n systems, which are nothing more but series systems, while 1-out-of-n
systems are simply parallel systems. As an example, for a 2-out-of-3 system with
component reliabilities p1 ¼ 0.7, p2 ¼ 0.8, and p3 ¼ 0.9, respectively, we can
compute the system reliability R by observing that no more than one component can
fail, and we therefore consider the four cases: (1) none of the components fails,
(2) only the first component fails, (3) only the second component fails, and (4) only
the third component fails. The reliability of the system is then
Fig. 14.5 Example of a
series-parallel system
Fig. 14.6 A general series-parallel system
14.1 Fundamentals of Reliability Systems 443R ¼ ð Þ 0:7 ð Þ 0:8 ð Þþ 0:9 ð Þ 0:3 ð Þ 0:8 ð Þþ 0:9 ð Þ 0:7 ð Þ 0:2 ð Þþ 0:9 ð Þ 0:7 ð Þ 0:8 ð Þ 0:1
¼ 0:902:
Given the complexity of this and similar systems, we may resort to a brute-force
enumeration method, often referred to as a Boolean truth method. It simply
enumerates all possibilities the n components of the system can be in, which,
given that each component either works properly or fails, is 2n
, which already
reveals the weakness of the technique: even for a small system with only, say,
n ¼ 10 components, there are already 210 ¼ 1024 different scenarios.
As an example, consider a large aircraft with eight engines, four under each wing
(such as the B-52 Stratofortress). Suppose that on some long flight, each of the
independently functioning engines has a 98% chance of working properly. i.e.,
p ¼ 0.98. For the flight mission to be successful, at least three engines on each of
the two wings need to be working during the entire flight. The question is now what
the probability for a successful mission is.
In order to solve the problem, we first enumerate all cases, in which the entire
system works properly. First, there is the case in which all eight components work
and that is just a single instance. The probability of this happening is
(0.98)8 ¼ 0.85076302. Secondly, there are those cases in which exactly one engine
fails. There are eight such cases, each of which has a probability of (0.02)
(0.98)7 ¼ 0.01736251 for a total of 0.13890009. Finally, there are all those cases,
in which there are two failures altogether, one on each side. There are four possible
cases of a single failure on each side coupled with four cases with a single failure on
the other side of the plane for a total of 16 cases. Each such case has a probability of
(0.98)6
(0.02)2 ¼ 0.00035434 for a total of 0.00566939. The 25 cases under consid￾eration have thus a total probability of 99.533%. Even though this probability
appears quite high as it is, the probability that each engine functions properly is
much higher than 98% in reality, thus having a dramatically higher probability of a
mission being successful.
Finally, there are systems that do not fall into any categories we have covered so
far. One such example is shown in Fig. 14.7 which shows a five-component system.
Again, while applying the Boolean truth method may be tedious, it may be
possible to economize the calculations by some ad-hoc measures. In the example
Fig. 14.7 A 5-component system
444 14 Reliability Modelsof Fig. 14.7, we may exclude scenarios in which components 1 and 2 are both not
functioning properly, as they yield a zero contribution to the overall reliability as do
states, in which either components 1, 3, and 5, or components 4 and 5 both do
not work.
There are methods that can handle complex systems more efficiently. They
involve paths and cuts in the block diagrams akin to the techniques covered in
Chap. 6 on network models, but they are beyond the scope of this introductory text.
14.2 Time Aspects of Reliability
In the previous section, we dealt with the possibility of failure of components over a
given time span of operation. Clearly, a component is more likely to break down
over a longer time period than during a shorter one, and in this section, we will
discuss how time affects reliability. Specifically, observe a component j and observe
its performance over time. Building on our discussion of stochastic processes in the
previous chapter, we can model the behavior of the component as a life-and-death
process, i.e., a stochastic process with only two states: life, i.e., working, and death,
i.e., not working, and with the only transitions being those from life to life, life to
death, and death to death. Associated with this life-and-death process is the random
variable that measures the time that elapses between some point t in time (typically
t ¼ 0) at which the component is operating (alive) and the time at which it fails
(dies). We will refer to this random variable as the time-to-failure (TTF), lifetime, or
simply life, and denote it by Xj; similarly for the system life X. We will also involve
the cumulative life distributions
F jðÞ¼ t FX j
ð Þt ≔P X j  t   and
F tðÞ¼ FXð Þt ≔P Xð Þ  t :
These denote the respective probabilities that the component and system survive
until time t, so that Fj(0) ¼ F(0) ¼ 0. We will also make use of their respective
probability density functions f jðÞ¼ t dF jð Þt
dt (so that F jðÞ¼ t Rt
0
f jð Þx dx), and f tðÞ¼
dF tð Þ
dt (so that F tðÞ¼ Rt
0
f xð Þdx). It follows that pj(t) ¼ P(component j still functions at
time t) ¼ P(Xj > t) ¼ 1  P(Xj  t) ¼ 1  Fj(t), and R(t) ¼ P (the system still
functions at time t) ¼ P(X > t) ¼ 1  P(X  t) ¼ 1  F(t). This is equivalent to
stating that the reliability pj(t) of component j (or the reliability R(t) of the system),
expresses the probability that it will function properly over the time period
[0, t]. Clearly, the reliability will therefore depend on the length of the time period
[0, t], as well as the life distribution of Xj (or X). Specifically, for a series system, we
now find the reliability R(t) over the time span [0, t] as 1  F(t) ¼ R(t) ¼ p1(t)p2(t)...
pn(t) ¼ (1  F1(t))(1  F2(t))...(1  Fn(t)), and therefore F(t) ¼ 1  (1  F1(t))
(1  F2(t))...(1  Fn(t)).
14.2 Time Aspects of Reliability 445Similarly, for a parallel system we obtain F(t) ¼ 1  R(t) ¼ (1  p1(t))
(1  p2(t))...(1  pn(t)) ¼ F1(t)F2(t)...Fn(t). As an example of a lifetime distribu￾tion, we will use the exponential distribution X with the parameter λ, which has the
density function f(t) ¼ λe
λt and cumulative distribution function F(t) ¼ 1  e
λt
(see Appendix C). We write X ~ Exp(λ). This distribution is applicable to electronic
devices, LED lights, and similar objects, which do not suffer from mechanical or any
other kind of wear as opposed to those that deteriorate and work less and less well.
As a numerical example, consider a series system that consists of two components
with lifetime distributions X1 ~ Exp(λ1) and X2 ~ Exp(λ2), respectively. For the
system lifetime X we then obtain the cumulative distribution F tðÞ¼ 1 
ð Þ 1  F1ð Þt ð Þ¼ 1  F2ð Þt 1  eλ1t
eπ2t ¼ 1  eð Þ λ1þλ2 t
, so that X ~ Exp(λ1 + λ2).
In general, for an n-component series system with components that are Xj~Exp(λj),
j ¼ 1, ..., n, respectively, we will similarly obtain the system life X  Exp Pn
j¼1
λ j
 !.
Since the expected value of an exponential distribution Exp(λ) is E Xð Þ¼ 1=λ (see
Appendix C), we find that the mean lifetime of a system with Exp(λj) components is
E Xð Þ¼ Pn
j¼1
λ j
 !1
. As an example, if a two-component series system has expo￾nential lifetime components of 2 and 3 months, respectively (i.e., 1
λ1 ¼ 2 and 1
λ2 ¼ 3),
then the system mean lifetime will be 1
λ1þλ2 ¼ 1
1
2þ1
3
¼ 1:2 months.
Turning to parallel systems, assume that a two-component system has
components with lifetimes X1 ~ Exp(λ1) and X2 ~ Exp(λ2), respectively. Then we
find F tðÞ¼ F1ð Þt F2ðÞ¼ t 1  eλ1t   1  eλ2t   ¼ 1  eλ1t  eλ2t þ eð Þ λ1þλ2 t ,
so that for the density function f(t), we obtain f tðÞ¼ F0
ðÞ¼ t λ1eλ1t þ λ2eλ2t 
ð Þ λ1 þ λ2 eð Þ λ1þλ2 t , from which we conclude that E Xð Þ¼ R
1
0
xf xð Þdx ¼
R
1
0
xλ1eλ1x
dx þ R
1
0
xλ2eλ2x
dx  R
1
0
ð Þ λ1 þ λ2 eð Þ λ1þλ2 x
dx ¼ 1
λ1 þ 1
λ2  1
λ1þλ2 . In gen￾eral, for an n-component parallel system with component lifetimes Xj ~ Exp(λj),
j ¼ 1, ..., n, we similarly find
F tðÞ¼ F1ð Þt F2ð Þt ... FnðÞ¼ t 1  eλ1t   1  eλ3t   ... 1  eλnt  ,
from which the density function f(t) and then the system mean lifetime E(X) can be
determined. As an illustration, consider again the previous example with two
components and their 2- and 3-month lifespan, but assume now that the structure
is parallel rather than in series. The system mean life is then calculated as E Xð Þ¼
1
λ1 þ 1
λ2  1
λ1þλ2 ¼ 3:8 months. This is considerably longer than the 1.2 months in case
of the series system.
As another example, we consider the mixed structure displayed in Fig. 14.3 of the
previous section, but with exponentially distributed exponent lives, whose means are
5, 6, and 7 months, respectively, as shown in Fig. 14.8.
446 14 Reliability ModelsFor this system, we will determine the distribution of the system lifetime and
its expected value. We will also calculate the probability that the system life is no
longer than 5 months, at least 4 months, and between 6 and 8 months, respec￾tively. From the example in Fig. 14.3, we know that 1  R(t) ¼ [1  p1(t)p2(t)]
[1  p3(t)], so that F tðÞ¼ ½  1  ð Þ 1  F1ð Þt ð Þ 1  F2ð Þt F3ðÞ¼ t 1  eλ1t
eλ2t  
1  eλ3t   ¼ 1  eλ3t  eð Þ λ1þλ2 t þ eð Þ λ1þλ2þλ3 t and therefore f tðÞ¼
λ3eλ3t þ ð Þ λ1 þ λ2 eð Þ λ1þλ2 t  ð Þ λ1 þ λ2 þ λ3 eð Þ λ1þλ2þλ3 t . This allows us to con￾clude that the mean system life is E Xð Þ¼ 1
λ3 þ 1
λ1þλ2  1
λ1þλ2þλ3 ¼ 1
1
7
þ 1
1
5þ1
6

1 1
5þ1
6þ1
7
 7:76 months . Furthermore, P Xð Þ¼  5 Fð Þ¼ 5 1  e5=7  e 1=5þ1= ð Þ6 5 þ
e 1=5þ1=6þ1= ð Þ7 5  0:42885 , i.e., 42.9%. Also, P Xð Þ¼  4 1  Fð Þ¼ 4 e4=7 þ
e22=15  e217=105  66:5%. Finally, P(6  X  8) ¼ F(8)  F(6)  13.30%.
We end this section with some notation and some useful mathematical results.
The expected system life E(X) is sometimes also called mean time-to-failure
(MTTF), i.e., MTTF ¼ E Xð Þ¼ R
1
0
xf xð Þdx and it is possible to demonstrate that
MTTF ¼ R
1
0
R xð Þdx . Furthermore, if systems or components are replaced as soon
as they fail, we also speak of system or component mean time between failures
MTBF. In the above example, MTTF  7.765 months. Another reliability measure is
the mean residual life MRL(t), which is the expected remaining life of a system or
component that has survived until time t  0, i.e., MRL(t) ¼ E(X  t | X  t). It is
possible to show that MRL tðÞ¼ 1
R tð Þ
R
1
0
xf xð Þdx  t. Using the concept of reliability
R(t) rather than the lifetime X, it is straightforward to obtain alternative expressions
for MTTF.
14.3 Failure Rates and the Hazard Function
While the previous section investigated the issue of how long a reliability system
might be working, we here examine the risk of a breakdown in a functioning system.
More specifically, assume that a system has survived until a given time t, the
question is then what its probability is to fail within Δt time units, i.e., within the
time interval [t, t + Δt], where Δt is sufficiently small. Given a random variable
Fig. 14.8 A mixed structure with exponential component lives
14.3 Failure Rates and the Hazard Function 447X that denotes system life, the conditional probability that the system life is some￾where in the interval [t, t + Δt] is then P tð Þ¼ < X  t þ ΔtjX > t P tð Þ <XtþΔt
P Xð Þ >t . Since
P(X > t) ¼ R(t) and P(t < X  t + Δt)  f(t)Δt for small values of Δt, the conditional
probability in question is approximated by f tð Þ
R tð Þ Δt. We define h tð Þ≔ f tð Þ
R tð Þ as the hazard
rate, hazard function, or (instantaneous) failure rate of the system. In other words,
h(t)Δt is the conditional probability that a system that has survived until time t, will
fail during the interval [t, t + Δt]. The hazard rate is of major importance when
dealing with the breakdown risk of systems. We can, of course, also consider the
hazard rate hj(t) of individual components with probability density function fj(t) and
reliability Rj(t), as h jð Þt ≔ f jð Þt
R jð Þt . Since h tðÞ¼ f tð Þ
R tð Þ ¼ F0
ð Þt
1F tð Þ, we see that the hazard
function h(t) is completely determined by the lifetime distribution F(t). Conversely,
one can show that F(t) is completely determined by h(t), so that either F(t) or h(t) can
be used to describe any system.
Example: For a system life X that is exponentially distributed with the parameter
λ, the hazard rate becomes h tðÞ¼ f tð Þ
1F tð Þ ¼ λeλt
1 1eλt ð Þ , so that the hazard rate is
constant over time and equals the parameter λ. Conversely, given a constant hazard
rate λ, we can infer that the random variable X is exponentially distributed with the
parameter λ. Therefore, an exponentially distributed system life has a constant
failure rate and is the only distribution with this property.
In general, reliability systems will have a time-dependent failure rate h(t), which
typically has a “bathtub” shape similar to that shown in Fig. 14.9. As the
Figure shows, the failure rate is initially declining. This phase is often referred to
as “early failure period,” “break-in period,” “infant mortality,” “debugging,” “shake￾down’ or “burn-in period,” just to name a few. What then follows is a period with
more or less constant failure rate for a mature system. In some mechanical systems of
poor quality, this period may be missing altogether with wear-out weaknesses
occurring right after the break-in period. Finally, the failure rate increases in the
wear-out period due to ageing. In some electronic systems, the wear-out period is
missing altogether: as an example, LED lightbulbs are typically rated
25,000–40,000 hrs (or 12.5–20 years, given an 8 hrs burn time per day 5 days of the
Fig. 14.9 Bathtub shape
failure rate
448 14 Reliability Modelsweek), before a catastrophic event, i.e., a failure occurs. Note though, that over time,
dimming and color shifts do occur.
As discussed above, the exponential distribution is the only distribution with a
constant failure rate. Furthermore, it is the only distribution that is “memoryless,”
i.e., the remaining lifetime of an item of any age is the same as a remaining lifetime
of a new item. Using terminology from the previous section, we can state that
MRL(t) ¼ MTTF for all t  0, which can also be expressed as “old items are as
good as new ones” or “items don’t age.”
Lifetime distributions with a nonconstant failure rate are sometimes classified as
having a decreasing failure rate (DFR) or increasing failure rate (IFR), depending
on the case. In this context, the Weibull distribution, a generalization of the expo￾nential distribution, is quite useful. Its cumulative distribution is F tðÞ¼ 1  eð Þ λt α
,
where α and λ are given positive parameters (see Definition C7), so that its hazard
rate is h(t) ¼ αt(λt)
α  1
. One can show that for the Weibull distribution, we have
MTTF ¼ 1
λ Γð Þ 1 þ 1=α , where Γ denotes the gamma function (see Definition C8).
Figure 14.10 displays the hazard rate for the Weibull distribution, given a number of
different values of the parameter α. Note that for α ¼ 1, we obtain the exponential
distribution.
For the special case of α ¼ 2, we obtain F tðÞ¼ 1  eð Þ λt 2
, which is the so-called
Rayleigh distribution with its linearly increasing hazard rate h(t) ¼ 2λ2
t. This
distribution has applications in fields as diverse as ballistics and oceanography.
As an example, a component with a failure rate of h(t) ¼ t has a Rayleigh
distribution with α ¼ 2 and 2λ2 ¼ 1, so that λ ¼ 1= ffiffiffi
2
p . Its lifetime distribution is
then F tðÞ¼ 1  e½t
2
and its MTTF ¼ ffiffiffi
2
p Γð Þ¼ 1 þ ½ ffiffiffiffiffiffiffiffi
π=2 p  1:2533.
The gamma distribution Γ(α, λ) is another common lifetime distribution with
shape and scale parameters α and λ. Its probability density functions is f tðÞ¼
λ
Γð Þ α ð Þ λt α1
eλt (see Definition C9). For n ¼ α with an integer value for n, we obtain
the Erlang distribution and for α ¼ 1, the exponential distribution Exp(λ) ¼ Γ(1, λ).
One can show that the sum of n independent Exp(λ) variables is Γ(n, λ)-distributed, a
very useful result in replacement theory.
Example: A component of a reliability system has an exponential lifetime with a
mean of 8 days. As soon as the component fails, it is replaced by a new component
Fig. 14.10 Hazard rates for the Weibull distribution
14.3 Failure Rates and the Hazard Function 449that is identical to the failed piece, and we are now interested in the distribution, the
expected value and standard deviation of the time until the nth component fails. We
can answer this question, realizing that with Exp(λ) individual component lives, the
system life will be Γ(n, λ) ¼ Γ(n, 1/8). Therefore, E(X) ¼ n/λ ¼ 8n days and
σð Þ¼ X 8 ffiffiffi
n p . With this information, we can furthermore compute the probability
such that the probability that the second component fails within 10 days. With
X ~ Γ(2, 1/8), f tðÞ¼ λ
Γð Þ2 λteλt ¼ λ2
teλt ¼ t
64 e1
8t and by integration, we obtain
P Xð Þ¼ < 10 R 10
0 f tð Þdt ¼ 1  9
4 e5
4  0:3554 , i.e., there is a 35.5% risk that the
second component fails within 10 days.
14.4 Redundancy and Standby Systems
When designing a system, one important consideration regarding the system’s
reliability is the course of action to be taken in case the system fails. One way to
deal with failure is the introduction of redundancy. Loosely speaking a component is
referred to as redundant, if its failure does not cause the entire system to fail.
Redundancy works in parallel systems and in “k out of n” systems with k < n. On
the other hand, a series system has no redundancy since any component failure will
inevitably cause the entire system to fail.
In general, there are two types of redundancy:
• Active redundancy, where all redundant components are in service, and
• Inactive redundancy or nonactive standby, which occurs in three different
versions:
1. Hot standby, where the redundant component is in working mode and has the
same failure rate as if it had been in service
2. Warm standby, where the redundant component is working at a reduced level
and with a failure rate less than if it had been in full service, and
3. Cold standby, where the redundant component is idle with a zero failure rate.
Figure 14.11 illustrates the difference between active and inactive redundancy in
reliability systems.
As an example of a standby arrangement, consider a factory that uses electricity in
a chemical production process, where the redundant component is an electric power
generator. In case the process is sensitive, the power generator may very well be
Active Inactive 
Fig. 14.11 Active and inactive redundancy systems
450 14 Reliability Modelsrunning continuously in hot standby mode. If the process can tolerate a brief
interruption of power, a warm standby with the generator at idling speed may be
sufficient. In a cold standby mode, the generator would be started only when in need
of service.
Another classification of redundant system components distinguishes between
repairable and non-repairable systems. For instance, repairable components include
power generators, vehicle brake systems, or airplane jet engines. On the other hand,
non-repairable components include telecommunications satellites in orbit, printed
electronic circuits, and lightbulbs.
14.5 Estimating Reliability
Given a system of components that are arranged in some given configuration, we
may wish to estimate the reliability, the failure rate, or the lifetime distribution of the
system or of any of its individual components. Such estimations are done by
so-called reliability testing. This may, for instance, be required in the process of
acceptance sampling of lots of components that are supposed to meet specifications
expressed in terms of reliability, MTTF, or similar measures. For lifetime testing of
components, we would take a random sample of n components from a given lot, test
them, and record their life (failure) times t1, t2, ..., tn. The life testing can be done by
operational life testing (OLT) by using typical operating conditions, accelerated life
testing (ALT) by employing operating conditions, in which failures occur sooner
than in the actual use of the component, or highly accelerated life testing (HALT) at
very high stress levels. Accelerated testing may be needed, if results need to be
obtained quickly, or if the components that are being tested have a very long
lifespan, such as the aforementioned LED light bulbs. Using the collected data,
such as the reliability, failure rate, or lifetime distribution, parameters of the tested
components can then be statistically estimated.
For practical purposes, the life testing process may have to be stopped before all
of the n sampled units have failed. This is referred to as truncation, which occurs
either at a prespecified time T or after a prespecified number r of components have
failed. With t1, t2, ..., tr denoting the respective failure times of the units, we have
t1  t2  ...  tr  T. If the units have an exponentially distributed lifetime Exp(λ),
one can show that the mean lifetime can be estimated by the expression 1
r 
t1 þ t2 þ ... t ð Þ r þ T nð Þ  r .
Example: Nine components with identically distributed exponential lives are
tested. Failures occur at 27.2, 29.5, 32.6, 36.8, 36.8, 37.9, and 38.3 hrs, respectively,
at which time the test is terminated. The component mean life is then estimated by
1
7 ð Þ¼ 27:2 þ 29:5 þ 32:6 þ 36:8 þ 36:8 þ 37:9 þ 38:3 þ 38:3 9ð Þ  7 45:1 hrs—a
surprisingly high figure. The reliability of an individual component at time t is
therefore R tðÞ¼ et
=45:1 , so that at time t ¼ 40, a component has the reliability
Rð Þ¼ 40 e40=45:1 ¼ 41:2%.
14.5 Estimating Reliability 451Expressions can be obtained for estimating the parameters of the Weibull and
Gamma distributions as well, but they are considerably more complicated and
outside the scope of this book.
Exercises
Problem 1 (System reliability) A certain system has the reliability structure shown
in Fig. 14.12.
Here, we have p1 ¼ p4 ¼ p5 ¼ 0.9 and p2 ¼ p3 ¼ p6 ¼ 0.8.
(a) Calculate the system reliability.
(b) Suppose that one of the six components is to be removed (i.e., its reliability is
downgraded to 0). Evaluate the system reliability for each of the six possible,
resulting configurations and choose the best.
(c) Suppose that one of the six components is to have its reliability upgraded to 1.0.
Evaluate the system reliability for each of the possible six resulting
configurations and choose the best.
Solutions
(a) We find R ¼ 1  {1  p1(1  (1  p2)(1  p3))}{1  (1  (1  p4)(1  p5))
p6} ¼ 0.971712 or 97.2%.
(b) Performing the required computations, the results are summarized in Table 14.1.
Therefore, the best options are to let p4 ¼ 0 or p5 ¼ 0, resulting in a maximal
reliability of R ¼ 0.96192, i.e., 96.2%, which is only a slight deterioration from the
97.2% found under (a).
Fig. 14.12 A reliability structure for Problem 1
452 14 Reliability Models(c) Performing the required computations, the results are summarized in Table 14.2.
The best option is p6 ¼ 1 and it yields the maximum reliability R ¼ 0.99864, i.e.,
99.9%, a significant improvement over the reliability of 97.2% found in (a).
Problem 2 (System lifetime)
A radio communications satellite is powered by seven solar power cells. The
satellite will function if and only if at least four of the seven cells function, i.e., have
not yet broken down. (In reliability terms, such systems are referred to as a “four out
of seven system.) The cells break down independently of each other and over time.
Once a cell has broken down, it will remain inoperative. The probability that a cell
breaks down in any given year is 4%. Denote by Xn the number of cells still working
after n years and by Yn the number of cells that have broken down after n years.
(a) What are the distributions of X1 and Y1?
(b) What are the expected values and standard deviations of X1 and Y1?
(c) What is the probability that the satellite still functions after one year?
(d) What are the distributions of X2 and Y2?
(e) Calculate the probability that the satellite still functions after 2 years.
Solutions
(a) We find that X1 is binomially distributed with Bin(7, 0.96) and that Y1 is
Bin(7, 0.04), noting that Y1 ¼ 7  X1.
(b) E(X1) ¼ 6.72, σð Þ¼ X1
ffiffiffiffiffiffiffiffiffiffiffiffiffi
0:2688 p  0:518, E Yð Þ¼ 1 0:28, σð Þ¼ Y1
ffiffiffiffiffiffiffiffiffiffiffiffiffiffi
0:2688 p 
0:518.
(c) The satellite reliability over a one-year time span is R ¼ P(X1  4) ¼
P(Y1  3)  0.999919 or 99.99%.
Table 14.1 System reli￾ability with one component
removed
j, pj ¼ 0 R
1 0.792
2 0.94176
3 0.94176
4 0.96192
5 0.96192
6 0.864
Table 14.2 System reli￾ability with one component
that cannot fail
j, pj ¼ 1 R
1 0.99168
2 0.9792
3 0.9792
4 0.9728
5 0.9728
6 0.99864
14.5 Estimating Reliability 453(d) X2 is Bin(7, 0.962
) ¼ Bin(7, 0.9216), and Y2 is Bin(7, 1  0.962
) ¼ Bin(7, 0.0784)
where we note that Y2 ¼ 7  X2.
(e) The satellite reliability over a 2-year life span is R ¼ P(X2  4) ¼
P(Y2  3)  0.99891 or 99.89%.
Problem 3 (System lifetime) A reliability system consists of two identical
components arranged in series. The components have exponentially distributed
lifetimes with distribution parameter λ.
(a) Determine the lifetime density function as well as the average system life.
(b) Assume now that the two components are arranged in parallel instead of in
series. Determine the system lifetime density functions as well as the average
system life.
(c) What is the ratio of the average system lives for the configurations in (a) and
(b) above?
(d) For the case with three identical components, determine the system lifetime
density function using the series as well as the parallel arrangement.
Solution:
(a) Since the component lives X1 and X2 are both Exp(λ), we find
F1(t) ¼ F2(t) ¼ 1  e
λt
. Therefore, for the series system lifetime Xser we obtain
Fser(t) ¼ 1  (1  F1(t))(1  F2(t)) ¼ 1  e
2λt
, so that fser(t) ¼ 2λe
–2λt
.
Consequently, E(Xser) ¼ 1/2λ.
(b) For the parallel system lifetime Xpar, we find Fpar(t) ¼ F1(t)F2(t) ¼ (1  e
λt
)
2
,
so that fpar(t) ¼ 2(1  e
λt
)λe
λt
¼ 2λe
λt
 2λe
2λt
, and therefore E(Xpar) ¼ 2/
λ  1/2λ ¼ 3/2λ.
(c) The ratio becomes E Xpar  =E Xð Þ¼ ser 3=2λ
1=2λ ¼ 3, which shows that the parallel
system has an average life three times as long as the series system.
(d) For a 3-component series system, we calculate Fser(t) ¼ 1  (1  F1(t))
(1  F2(t))(1  F3(t)) ¼ 1  e
3λt and consequently, fser(t) 3λe
3λt and
E(Xser) ¼ 1/3λ. For the 3-component parallel systems, we obtain
Fpar(t) ¼ F1(t)F2(t)F3(t) ¼ (1  e
λt
)
3 and therefore fpar(t) ¼ 3(1  e
λt
)
2
λe
λt
.
Although it was not asked for in the question, one can show that E(Xpar) ¼
11/6λ, so that the ratio is 11/2 for the 3-component case. Actually, for the
n-component case, the ratio is n 1 þ 1
2 þ 1
3 þ ... þ 1
n
 .
Problem 4 (System reliability, lifetime, and hazard function) A system that
consists of four identical components will function if and only if at least three of the
four components function.
(a) Express the system reliability R as a function of the component reliability p.
(b) Assume that the components have exponentially distributed lifetimes with some
distribution parameter λ. Determine the system lifetime density function as well
454 14 Reliability Modelsas the system average life and compare it to the individual components’
average life.
(c) Based on the results found under (b), what is the system hazard rate function?
Determine if the system is DFR, IFR, or neither.
Solutions
(a) The system reliability is given by R ¼ 4p3
(1  p) + p4 ¼ 4p3  3p4
.
(b) With Fi(t) ¼ 1  e
λt
, we obtain pi(t) ¼ 1  Fi(t) ¼ e
λt
, and therefore
R(t) ¼ 1  F(t) ¼ 4pi(t)
3  3pi(t)
4 ¼ 4e
3λt  3e
4λt
, and f(t) ¼ F0
(t) ¼
12λ(e
3λt  e
4λt
). For the system average life E(X) we find that E Xð Þ¼
ð Þ4 1
3λ
   ð Þ3 1
4λ
  ¼ 7
12λ, i.e., slightly more than half the individual component
average life 1/λ.
(c) The hazard rate is h tðÞ¼ f tð Þ
R tð Þ ¼ 12λ e3λt
e4λt ð Þ
4e3λt
3e4λt ¼ 12λ 1eλt ð Þ
43eλt . Differentiating, we
determine that h0
ðÞ¼ t 12λ2eλt
43eλt ð Þ2 > 0. Hence, the system has an increasing hazard
rate, i.e., it is IFR.
Reference
Elsayed EA (2020) Reliability engineering, 3rd edn. Wiley, Hoboken, NJ
Reference 455Waiting Line Models 15
Waiting line or queuing systems are pervasive. Many of us remember the long lineups
in front of stores in the Soviet Union and Vietnam, and we have all experienced lineups
in banks and supermarkets, but there are many more instances with waiting lines:
think, for instance, about traffic lights, where drivers line up and wait, files that wait for
processing in the inbox at a clerk’s workstation, or telephone calls that are put in a
queue. Queuing systems were first examined by Agner Krarup Erlang (1878–1929).
Erlang was a Danish mathematician, who worked for the Copenhagen Telephone
Company. One of the questions he faced during this time was to determine the number
of telephone circuits that are required to provide an acceptable level of service.
Phone switchboard operator
# The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
H. A. Eiselt, C.-L. Sandblom, Operations Research, Springer Texts in Business and
Economics, https://doi.org/10.1007/978-3-030-97162-5_15
45715.1 Basic Queuing Models
In order to avoid discussing only special cases, we will formalize queuing systems as
follows. All entities that are in need of service of some kind will be referred to as
customers, while the service is performed at service stations. The process can then be
thought of as follows. Customers are in a calling population. Once they are in need
of service (here, for simplicity, we will consider only a single type of service
customers are interested in), they will approach the service system, where they
will line up. When it is the customer’s turn, he will be served, after which the
customer will leave the service system and rejoin the calling population. The
structure of this process can be visualized in Fig. 15.1.
The service system will require some further specifications. First of all, each
service system has only a single waiting line. A service system typically consists of a
number of c parallel service stations, each assumed to perform the same type of
service. Parallel service stations are usually referred to as channels. In some
instances, one channel consists of a series of service stations: imagine entering a
building, where a potential customer first has to be cleared by security, then being
directed to a general secretary, from where the service continues to the department
director’s secretary, and finally on to the department director. At each station, the
customer may be asked to leave the system, e.g., for not clearing security, the
unavailability of a service station, and for other reasons. Multi-phase systems can
be very complex and will not be discussed here.
In order to categorize queuing systems, Kendall (1918–2007) devised a taxon￾omy in 1953 that, in a variety of versions, some of which were added by Lee and
Taha in 1968, is the standard to this day. The original system consists of three
descriptors, and it has been extended to at most six components. In order to be as
Fig. 15.1 Queuing system
458 15 Waiting Line Modelsgeneral as possible, we introduce the complete six-component system first, but then
use only the more compact 3-component system later on. The notation is
A/S/C K/N/D,
where each letter is a placeholder for one component of waiting lines. These
components are now described in some detail. (We should note, though, that some
publications arrange the last three components in a different order.)
The symbol A describes the arrival process. We will use the symbol “M,” if arrivals
are random and follow a so-called Poisson process (where the “M” stands for
“Markovian” or “memoryless”). The Poisson distribution (see Appendix C in this
book) describes such a process. In the ubiquitous Poisson process, events such as
customer arrivals, occur randomly over time. Specifically, during a time interval of
length t, the number of events will follow a Poisson distribution Po(λt), where λ is
the parameter of the process. Furthermore, the time between two consecutive events
then follows an exponential distribution with the same parameter λ, i.e., Exp(λ).
Other popular processes include “D” or deterministic arrivals (actually: constant
arrivals) in which the time between arrivals is constant and known with certainty, as
is the case on an assembly line), “G,” where arrivals follow some general distribu￾tion, for which only some key parameters are known, e.g., mean and variance, or
“GI,” which symbolizes a general independent distribution.
The second component “S” symbolizes the service process. Again, the letter “M”
symbolizes Markovian or memoryless, i.e., exponential service time, while “D” “G,”
and “GI” symbolize deterministic/constant, general, and general independent service
rates, respectively.
The letter C indicates the number of parallel service stations that are alternatives
of each other.
The letter K in the extended version of the taxonomy describes the number of
customers that can be accommodated in the entire system. This includes the number
of customers who wait as well as those who can be served. In case this descriptor is
not used, it is assumed that K is infinite. While no real-life system has infinite
capacity, assuming an infinite K simplifies the analysis tremendously and is usually
very close to the true results as soon as K exceeds 30 or 40.
The symbol N denotes the size of the calling population. As in the case of the
capacity of the system, N will be assumed to approach infinity if not otherwise
specified. Again, for reasonably large values of N, we may assume its value to be
sufficiently close to infinity so as to simplify the computations.
Finally, the symbol D denotes the queuing discipline. Typical disciplines are
FCFS or FIFO (first come, first served or first in, first out), LCFS or LIFO (last come,
first served or last in, first out), or SIRO (service in random order). An important
category are priority queues, in which some customers receive preferential treatment.
The most prominent example of priority queues occurs in health care, where more
serious cases, as classified by the triage nurse, will be treated first. In case no queuing
discipline is specified, it is assumed that the FCFS discipline applies.
15.1 Basic Queuing Models 459In general, it will be useful to think about waiting lines as buffers between arrival
and service. A good image would be one of a water tank. While a waiting line
(similar to an inventory) grows with the inflow, it shrinks whenever service is
provided and customers leave the system.
In queuing theory, we distinguish between transient states and steady states.
While a steady state occurs if the system has been running for a very long time,
transient states are still, at least to some degree, dependent on the initial state of the
system. A simple example is the opening of a store in the morning. Initially, no one is
in the system and waiting times will be short. However, as the store operates for
some time, the system becomes more congested and is no longer dependent on the
opening conditions.
The key task of queuing theory is to compute measures of interest from some key
input factors or queue characteristics. The main characteristics of a queue are those
described in the taxonomy. Measures of interest include average waiting times, the
probability that a newly arriving customer will have to wait, the average length of a
queue, and others. In order to formalize our discussion, we use the following
conventions about notation (which, incidentally, are almost all standard in the
pertinent literature):
λ denotes the mean arrival rate, measured in [customers/hour]. It is the average
number of customers who actually arrive at the system in the specified amount
of time.
μ is the mean service rate, measured in [customers/hour]. It is the average number of
customers who can be served by a single service station.
It is worth noting that while λ expresses an actual observable fact, μ indicates a
capability. The actual number of customers served does not only depend on the
service station’s capabilities, but also on the number of customers who desire
service.
The inverse values of λ and μ also have important interpretations. Suppose that a
service station can deal with μ ¼ 12 customers per hour, then the inverse value is
1/μ ¼ 1/12 [hours/customer] ¼ 5 [minutes/customer]. This is the average service
time. The inverse value of the arrival rate can be interpreted similarly. If the average
arrival rate is λ ¼ 10 [customers/hour], then 1/λ ¼ 1/10 [hours/customer] ¼
6 [minutes/customer], meaning that on average, 6 min elapse between two succes￾sive arrivals. This is referred to as the (average) interarrival time.
It is apparent that, in case of a single service station, the arrival rate cannot exceed
the service rate. If it would, then there are more arrivals than can be handled by the
service station, so that—given infinite patience of the customers without balking
(leaving a queue immediately upon arrival because of the expected long waiting
time) or reneging (leaving the queue after a while as the anticipated future waiting
time appears too long)—the waiting line will grow towards infinity. This gives rise
to a regularity condition. In order to express it in a compact form, we define the
traffic intensity (sometimes also referred to as utilization rate) ρ ¼ λ/μ. A feasible
system (i.e., a system that has a steady state) must then have ρ < c, where c denotes
460 15 Waiting Line Modelsthe number of parallel service stations. As an example, consider a system with a
single service station that faces an arrival rate of λ ¼ 24 customers per hour. For it to
be feasible, the service rate must be μ > 24, or, equivalently, the average service time
cannot exceed 150 sec.
On the output side are the measures that we are interested in and that we can
compute. They include:
Pn: the probability that there are n customers in the system (with the important
special case of n ¼ 0 and P0, i.e., the probability that the system is idle)
Ws: the average waiting time per customer
Wq: the average time a customer spends in the queue
Ls: the average number of customers in the system, and
Lq: the average number of customers in the queue.
Before stating formulas for these measures, there are some general relations that
hold in queuing, regardless of the specific system under consideration. The first such
relation is
Ws ¼ Wq þ 1=μ: ð15:1Þ
Simply stated, the relation expresses that the total time a customer spends in the
system equals the waiting time plus the service time. Another relation is known as
Little’s formula (based on the work by J.D.C. Little who published the formula in
1961), which states that
L• ¼ λW • , ð15:2Þ
where the “•” stand for either the subscript “s” or the subscript “q”. This formula
provides a convenient way to compute the number of customers in the queue and in
the system from the average time customers spends in the queue or in the system,
respectively (or vice versa).
The simplest queuing model is the M/M/1 model, in which the number of
customer arrivals are random and follow a Poisson process (making the interarrival
times exponentially distributed), and the service time is exponential (so that the
service rate again follows a Poisson distribution). The key queuing formulas for this
model are shown in Table 15.1.
As an illustration, consider the following
Example: Customers arrive at the counter of a bank at a rate of 30 per hour.
Arrivals are random and service time is exponential, so that we are dealing with an
M/M/1 model. The clerk’s average service time is 80 sec. Putting the parameters in
their required form, we glean λ ¼ 30 and μ ¼ 45 from this information. As ρ ¼
Table 15.1 Steady-state formulas for the M/M/1 queuing model
P0 ¼ 1  ρ Ls ¼ λ
μλ Ws ¼ 1
μλ
Pn ¼ P0ρn
Lq ¼ ρλ
μλ ¼ ρ2
1ρ Wq ¼ ρ
μλ
15.1 Basic Queuing Models 461λ=μ ¼ 30=45 ¼ 2=3 < 1, the system does have a steady state. The probability that the
bank teller is idle is P0 ¼ 1  ρ ¼ 1/3. The probability that at least two customers are
waiting equals the probability that there are at least three customers in the bank or,
formally, P3 þ P4 þ P5 þ ⋯ ¼ 1  P0  P1  P2 ¼ 1  1=3  1=3ð Þ 2=3
1 
1=3ð Þ 2=3
2 ffi 8
27  :2963 or slightly less than one-third. On average, there are
Lq ¼ 1.3333 customers waiting in line and the average time a customer spends in
the system is Ws ¼ 1/15 [hr] ¼ 4 min.
An interesting case arises when the decision maker specifies the service level and
determines bounds for the capabilities of the servers. Suppose that in an M/M/1
system with λ ¼ 20, the decision maker specifies that the probability that there are
three or more customers in the system should not exceed 95%. The probability of
three or more customers in the system is again 1  P0  P1  P2 ¼ 1  (1  ρ)
 ρ(1  ρ)  ρ2
(1  ρ) ¼ 1  1 + ρ  ρ + ρ2  ρ2 + ρ3 ¼ ρ3
. As this probability
should not exceed 95%, we obtain the condition ρ3 ¼ λ3
μ3  :95 , or, as λ ¼ 20,
μ3 > 8000/0.95 or μ > 20.3449. The reason that it is sufficient that the service rate
barely exceeds the arrival rate is that the probability of three or more customers in the
system is very small.
Suppose now that the service rate is no longer random but that it follows some
general distribution. All we know about this distribution is that the mean service time
is 1/μ and the variance of the distribution equals σ2
. This means that we are dealing
with an M/G/1 model, for which some rather elegant formulas are available. Again,
as in all single channel systems in a steady state, P0 ¼ 1  ρ. Furthermore, the
Pollaczek-Khintchine formula developed in 1930 is
Lq ¼ λ2
σ2 þ ρ2
2 1ð Þ  ρ : ð15:3Þ
The values of Ls, Wq, and Ws can then be computed based on the general relations
(15.1) and (15.2). Before demonstrating this model on a numerical example, note
that the M/M/1 model is a special case of the M/G/1 model with σ2 ¼ 1/μ2
. Replacing
σ2 in the above expression results in Lq ¼ ρ2
1ρ
, which is the standard formula of the
M/M/1 model. Similarly, we observe that in the case of the M/D/1 model, i.e., the
queuing model with deterministic service time, the variance σ2 ¼ 0, so that the
Pollaczek–Khintchine formula reduces in this case to Lq ¼ ρ2
2 1ð Þ ρ
. Observe that the
number of customers waiting in the case of the deterministic model is exactly half of
that of the standard model with exponential service time. In other words, the
performance of the queuing system can be improved quite dramatically by reducing
the variance of the service time.
Example: Arrivals of customers at a single service desk follow a Poisson distri￾bution, while the service time follows a general distribution. There is an average of
λ ¼ 15 arrivals, while the service time is 3 min on average with a standard deviation
of 6 min. This means that 1/μ ¼ 1/20 [hrs] and σ2 ¼ 1/(10)2 ¼ 1/100, so that the
average number of customers waiting in line is then Lq ¼ 5.625 and the average
462 15 Waiting Line Modelswaiting time is Wq ¼ 0.375 hr ¼ 22.5 min. If the service time were exponential, we
obtain the standard M/M/1 system and the performance measures Lq ¼ 2.25 and
Wq ¼ 9 min, while the deterministic model has Lq ¼ 1.125 and Wq ¼ 4.5 min.
Another well-known result involves the so-called Kingman equation. It was
described by the British mathematician Sir John Kingman in 1961, and it is an
approximation for the expected waiting time in the very general G/G/1 queue.
Defining CVλ and CVμ as the coefficient of variation of the interarrival times and
service times, respectively (recall that the coefficient of variation is the standard
deviation divided by the mean), we can write the expected waiting time as
Wq  ρ
1  ρ
CV2
λ þ CV2
μ
2
1
μ :
If we were to apply this general formula to the special case of exponential
interarrival and service time distributions, we obtain means of 1/λ and 1/μ, respec￾tively, as well as standard deviations of 1/λ and 1/μ, respectively, so that
CVλ ¼ CVμ ¼ 1. Inserting these parameters into Kingman’s equation, we obtain
Wq  ρ
μλ , which is exactly the formula for the M/M/1 case. As an illustration,
consider again the above example with λ ¼ 15, μ ¼ 20, and σ2 ¼ 1/100, so that
CVμ ¼ ½. Assuming that CVλ ¼ 2, we obtain Wq ¼ 0.1875 hr or 11¼ min.
Consider now the case of multiple service stations. Here, we have to make the
distinction between one multi-station service center and a number of parallel single￾service centers. The general rule is that each service center has only a single waiting
line. As an example, consider what we may refer to as a “bank system” and a
“supermarket system.” In a bank system, there is a single queue and thus a single
multi-server system. In contrast, a supermarket features multiple waiting lines, hence
we deal with multiple single-server systems.
Let us deal with multiple single-server systems first, as they are a straightforward
extension of the concepts discussed earlier. The usual assumptions include no
balking (i.e., customers in need of service will join the queue regardless of its length)
and no jockeying (i.e., customers do not change queues if they perceive that another
queue may result in shorter waiting times). As an example, suppose again that
arrivals follow a Poisson distribution with a mean arrival rate of λ ¼ 45 customers
per hour, while service time is exponentially distributed with an average service time
of 1/μ ¼ 2 min (or, equivalently, a service rate of μ ¼ 30 customers per hour). Before
performing any computations, feasibility requires that ρ ¼ λ/μ ¼ 45/30 ¼ 1½ does
not exceed the number of service stations c, requiring at least two service stations in
this instance. Suppose now that c ¼ 3 service stations are available. As a result, we
now deal with three separate single-service systems, or, in terms of the taxonomy,
3  M/M/1 systems. Consider now the customers. While an average of λ ¼ 45
customers are in need of service, each of the three systems receives only about
one-third of this number, as customers may be assumed to randomly choose the
system they want to be served by. Hence, for each of the three systems, we have an
individual arrival rate of λ0 ¼ 15. The value λ0 will then replace λ in all of the
15.1 Basic Queuing Models 463formulas. In our example, feasibility is guaranteed as ρ'
¼ λ'
/μ ¼ 15/30 ¼ ½ < 1 for
each of the systems. We can then apply the usual steady-state formulas for M/M/1
models shown in Table 15.1. For instance, the average time a customer spends in the
system is Ws ¼ 1
μλ0 ¼ 1=ð Þ¼ 30  15 1=15 hrs ¼ 4 minutes. On the other hand, the
average number of customers waiting is Lq ¼ ρ2
1ρ ¼ ð Þ ½ 2
=ð Þ¼ 1  ½ ½, meaning
that on average half a customer is waiting in line in each of the three subsystems. In
other words, on average in all of our three service systems combined, 1.5 customers
will be waiting for service.
Next, consider a single M/M/3 system such as the one we encounter in a bank with
three tellers. Some of the relevant formulas are summarized in Table 15.2, where the
values of the waiting times W• can be computed by using Little’s formula (15.2).
In our example, first compute the probability that there are no customers in the
system. Here, P0 ¼ 1
1þ3
2þ9
8 ½ þ ð Þ 3=2 3
3! 13=2 ð Þ 3
¼ 4
19 . This allows us to compute Lq ¼
9/38 ffi .2368, Ls ¼ 9/38 + 3/2 ffi 1.7368 and, applying Little’s formula (15.2), we
obtain Wq ¼ Lq/λ ¼ 0.3158 min and Ws ¼ Ls/λ ¼ 2.3158 min.
The differences between the two models arise from the fact that in the former
model with separate queues, it is possible that at least one customer is still waiting,
while a service station is idle. This is based on the aforementioned assumption that
jockeying is prohibited.
A simple extension is a model with self-service. It is easily derived from the
standard M/M/c model by letting the number of service stations c tend to infinity.
Such a model will not include any waiting time, so that Lq ¼ Wq ¼ 0 and Ws ¼ 1/μ,
so that Ls ¼ ρ. It can also be shown that in this case, Pn ¼ eρ
n! ρn for any n  0.
For more complex queuing models, it is useful to have either tables or figures at
hand that provide the decision maker with a quick idea of what to expect without
engaging in complex computations. Entire books with queuing graphs and queuing
tables exist. Figure 15.2 shows a typical example of such queuing graphs. This
particular graph depicts the situation of a c  M/M/1 graph, where the abscissa
shows the utilization rate ρ, while the ordinate is related to the total number of
customers in all systems cLs. The steep solid line relates to the case of a single
facility (i.e., c ¼ 1), the broken, broken-and-dotted, broken-and-twice-dotted, and
dotted lines are for c ¼ 2, 3, 5, and 10 service stations, respectively.
Finally, we would like to have a look at multiple-server problems from a slightly
different angle. Suppose that the decision maker specifies a certain service level and
inquires how many service stations are needed in order to provide a prespecified
level of service. Questions of this nature frequently arise in the public service, e.g.,
Table 15.2 Formulas for
M/M/c systems
P0 ¼ 1
Pc1
i¼0
ρi
i!
 þ ρc
c! 1ρ ð Þc Pn ¼
ρn
n!
P0, if0  n  c
ρn
c!cnc
 P0, if n > c
8
>><
>>:
Lq ¼ ρcþ1
ð Þ c1 !ð Þ cρ 2 P0 Ls ¼ Lq + ρ
464 15 Waiting Line Modelshow many police officers are needed so as to adequately protect an area. As an
illustration, consider the following.
Example: Suppose that an average of 20 customers per hour randomly arrive at a
hospital. The system uses a subsystem for each doctor on duty, and there will be an
unknown number of c doctors available. Given that each doctor can deal with an
average of ten patients per hour, hospital administrators want to ensure that the
average waiting time for any patient does not exceed 10 min. What is the smallest
number of doctors that allows this?
Solution: The system described above is a set of c parallel M/M/1 systems, and the
individual arrival rate at each of these systems is then λ0 ¼ λ/c. Following the results
in Table 15.1, the average waiting time is defined as Wq ¼ ρ
μλ, which, with ρ0 ¼
λ0
μ ¼ λ
μc ¼ ρ=c, can be rewritten as Wq ¼ ρ=c
μλ=c ¼ 2=c
1020=c
. The condition is now that
the average waiting time is no higher than 1/6 [hr], which can be written as
2=c
10  20=c  1=6:
This inequality can be rewritten as 2
10c20  1
6, so that c  3.2. In other words, at
least four doctors are needed to provide the desired service. It may also be interesting
to note that with three doctors, the average waiting time is 12 min, while four doctors
result in an average waiting time of only 6 min.
Finally, we would like to point out that some queuing calculators are readily
available on the internet. One such example is the site https://www.supositorio.com/
rcalc/rcalclite.htm that calculates waiting times, probabilities and queue lengths for
M/M/c models.
Fig. 15.2 Customers in the
system as a function of the
utilization rate, for various
numbers of servers
15.1 Basic Queuing Models 46515.2 Optimization in Queuing
While queuing models are primarily designed to compute performance measures,
they can also be applied in the context of optimization. As an example, consider a
retail establishment. The owner of the store has to decide how many clerks to employ
for the cash registers at the checkout counter. Clearly, increasing the number of
clerks will increase the costs. However, at the same time, more clerks will result in
less waiting time for customers, which, in turn, results in less ill will, lost sales, and
other customer behavior detrimental to sales. One of the main problems applying
these models is the quantification of the loss due to customer ill will.
The example of a tool crib is much easier to justify. A tool crib is a place in which
expensive tools are kept that are not in constant use by the workers. Due to cost
considerations, it would not be feasible to provide each worker with one tool, so that
a service desk is established, where workers can sign out the tool whenever it is
needed. The costs of the system include the costs of the tool crib clerks as well as the
costs for the lost time of the workers. If c is the number of clerks and $c and $w denote
the hourly wage of a clerk and a worker, respectively, and L0
s denoting the number of
workers in each of the c service stations, then the costs can then be written as
C ¼ ð Þþ cost of clerks cost of worker’
s lost time   ¼ c $c þ $wL0
s
  ¼
¼ c $c þ $w
λ
cμ  λ
 :
The idea is now to determine the optimal number of clerks so as to minimize the
overall costs. As an illustration, consider the following
Example: The demand for a specialized tool occurs randomly at a rate of about
100 times per hour. Whenever the need arises, workers walk over to the tool crib,
sign out the tool, use it, and then return it to the tool crib. All clerks are equally
efficient with a service time of 3 min. For simplicity, we assume that the organization
of the signing out follows c  M/M/1 systems. Assume that the hourly wage of a
clerk is $c ¼ 10, while a worker’s lost hour costs $w ¼ 25.
Solution: With the given parameters of λ ¼ 100 and μ ¼ 20 (and thus ρ ¼ 5), we
first note that due to the feasibility condition ρ  c, we will need at least six clerks.
The optimal number of clerks can then be determined as follows. Consider the two
cost curves that determined the total costs. The costs for the clerks increase linearly
with the number of clerks. On the other hand, the cost for the workers’ lost time
decreases hyperbolically with an increasing number of clerks. This is shown in
Fig. 15.3. Note that while only integer values of c are relevant, we display the costs
for all real values of c so as to better show the shapes of the curves. Also note the
similarity between this cost curve and those in inventory management (Fig. 15.3).
For the smallest possible number of clerks that still has a steady state (e.g., c ¼ 6),
waiting time for the workers will be very long, resulting in high costs. As the number
of clerks increases, waiting times decrease and with them the costs. The reduction of
workers’ waiting times due to adding clerks to the system is very significant at first,
but less and less so as the number of clerks decreases. Eventually, the benefit of an
466 15 Waiting Line Modelsadditional clerk is outweighed by his costs, so that the total costs start increasing
again. This suggests a brute force search procedure, in which the total costs are
computed for c ¼ 6, 7, 8, ... clerks until the costs that were initially decreasing, start
increasing again. At that point, the optimal number of clerks has been found.
Detailed computations are shown below.
c ¼ 6: λ ¼ 100/6 ¼ 16.6667, ρ ¼ 16.6667/20 ¼ 0.8333, Ls ¼ 5 in each system,
Cost ¼ 6(10) + 5(6)(25) ¼ 810.
c ¼ 7: λ ¼ 100/7 ¼ 14.2857, ρ ¼ 0.7143, Ls ¼ 2.5 in each system,
Cost ¼ 7(10) + 2.5(7)(25) ¼ 507.5.
c ¼ 8: λ ¼ 100/8 ¼ 12.5, ρ ¼ 0.6250, Ls ¼ 1.6667 in each system,
Cost ¼ 8(10) + 1.6667(8)(25) ¼ 413.34.
c ¼ 9: λ ¼ 100/9 ¼ 11.1111, ρ ¼ 0.5556, Ls ¼ 1.25 in each system,
Cost ¼ 9(10) + 1.25(9)(25) ¼ 371.25.
c ¼ 10: λ ¼ 100/10 ¼ 10, ρ ¼ 0.5, Ls ¼ 1 in each system,
Cost ¼ 10(10) + 1(10)(25) ¼ 350.
c ¼ 11:λ ¼ 100/11 ¼ 9.0909, ρ ¼ 0.4545, Ls ¼ 0.8333 in each system,
Cost ¼ 11(10) + (0.8333)7(25) ¼ 339.16.
c ¼ 12:λ ¼ 100/12 ¼ 8.3333, ρ ¼ 0.4167, Ls ¼ 0.7143 in each system,
Cost ¼ 12(10) + (0.7143)12(25) ¼ 334.29.
c ¼ 13: λ ¼ 100/13 ¼ 7.6923, ρ ¼ 0.3846, Ls ¼ 0.625 in each system,
Cost ¼ 13(10) + 0.625(13)(25) ¼ 333.13.
Fig. 15.3 Cost curves for the tool crib example
15.2 Optimization in Queuing 467c ¼ 14: λ ¼ 100/14 ¼ 7.1429, ρ ¼ 0.3571, Ls ¼ 0.5556 in each system,
Cost ¼ 14(10) + 0.5556(14)25 ¼ 334.43.
At this point, the costs start increasing again, so that it is optimal to have c ¼ 13
parallel service stations.
An alternative procedure for the determination of the optimal number of clerks is
this. For now, assume that the variable c is continuous. We can then set up the
general cost function
Cost ¼ $cc þ $wLsc with Ls ¼ λ
cμ  λ :
Evaluating the derivative dCost
dc and setting it equal to zero results in the optimality
condition
c	 ¼ ρ 
 ρ
ffiffiffiffiffi
$w
$c
r
:
Since c* is in all likelihood not integer, we need to examine the costs of its two
neighboring integer values and choose the better one of the two. In our example,
c* ¼ 2.9 or 12.9. Since c  6, we examine c ¼ 12 and c ¼ 13, and find again c ¼ 13
as the optimal number of clerks.
If we were to repeat the problem with a single waiting line and M/M/c systems, we
will start again with c ¼ 6 clerks, which has 7.9376 customers in the system on
average, costing us 6(10) + (25)(7.9376) ¼ $258.44, dramatically less than the
multiple-queue system considered above. The system with c ¼ 7 clerks costs
$215.26, while the system with c ¼ 8 clerks will cost $211.97. We then find that
with c ¼ 9 clerks, the costs are $217.52, so that in this scenario, 8 clerks are optimal.
Another possibility to incorporate optimization in queuing systems occurs, when
retraining of clerks is considered. The basic setting is similar to that of the tool crib
above (with the firm paying for service as well as wasted time). The retraining time
for the clerks includes the actual (recurrent) retraining as well as costs for the time
that the clerk is absent during training, at which time the position must be staffed by
other clerks. It is typical that the costs to increase a clerk’s service rate increase at an
increasing rate. A numerical illustration is provided in the following.
Example: Customers arrive at a system at a rate of λ ¼ 30 per hour. Keeping them
waiting is estimated to cost $20 per hour. At present, the service rate is μ ¼ 40, but
with some additional training, this rate can be increased up to 60. Training to achieve
a service rate of μ 2 [40, 60] costs 2(μ  40)2
. For simplicity, we assume that an
M/M/1 system is used. To what service rate should the clerk be trained so as to
minimize the training and the service costs?
Solution: The cost function under consideration includes again two components.
They are the training costs of the clerk and the cost for waiting customers. Following
the results in Table 15.1, the average waiting costs in an M/M/1 system are Lq ¼
ρλ
μλ ¼ λ2
μ μð Þ λ .
Then the cost function is then
468 15 Waiting Line ModelsC ¼ ð Þþ retraining costs ð Þ¼ waiting costs 2ð Þ μ  40 2 þ 20 900
μ μð Þ  30 :
The graph in Fig. 15.4 shows the total costs in this example. In particular, the
function reaches a minimum at μ ¼ 41.12358 with costs C ¼ $41.8742.
Total costs for a number of other service rates have also been computed. They are
shown in Table 15.3.
In other words, leaving the clerk essentially un(re-)trained will cost $45, less than
10% off optimum, whereas training the clerk up to capacity will cost 18 times in total
as much as leaving him untrained.
Exercises
Problem 1 (optimization of the number of channels and the service rate):
Customers arrive at a retail outlet a rate of 12 per hour. The total time that customers
spend in the store contributes to their dissatisfaction. A wasted customer hour has
been estimated to cost $20. A clerk at the checkout counter typically earns $8 and
can serve up to 10 customers per hour.
(a) What is the cost-minimal number of checkout counters?
(b) Suppose now that an alternative to the system under (a) is to employ two clerks
who have been retrained. Their retraining enables them to serve up to
15 customers per hour, and they will earn $10 per hour. Is it worth considering
this option?
Fig. 15.4 Cost for the clerk training example
Table 15.3 Queuing costs
for differently trained clerks μ 40 41 42 50 60
C 45 41.9113 43.7143 218 810
15.2 Optimization in Queuing 469Solution:
(a) c ¼ 2, λ0 ¼ 6 each, ρ ¼ 6/10 ¼ 0.6, Ls ¼ 1.5, so that TC ¼ 20(2)(1.5) + 2(8) ¼ 76
c ¼ 3, λ0 ¼ 4 each, ρ ¼ 4/10 ¼ 0.4, Ls ¼ 2/3, so that TC ¼ 20(3)(2/3) + 3(8) ¼ 64
c ¼ 4, λ0 ¼ 5 each, ρ ¼ 3/10 ¼ 0.3, Ls ¼ 0.4286, so that TC ¼ 20(4)(0.4286) + 4(8)
¼ 66.29
This implies that the optimal solution is to have 3 clerks. At optimum, the system
will cost $64 per hour.
(b) With μ ¼ 15, we obtain ρ ¼ 0.4 and Ls ¼ 2/3. Then TC ¼ 20 2ð Þ 2=3 ð Þþ
2 10 ð Þ¼ $46:67, which is cheaper than the 3-clerk option in (a).
Problem 2 (optimization of the number of channels and sensitivity analysis):
Joe plans to open his own gas station “Joe’s Place.” He has planned to open from
7 a.m. to 11 p.m. He estimates that 15 customers will arrive each hour during the day
to fill up their tanks. Doing so takes typically 4 min plus 1 min for paying the bill. Joe
now has to decide how many pumps to install. He has read in the industry magazine
“Full of Gas” that each hour that a customer waits in line costs $15 in terms of loss of
goodwill (i.e., patronizing a different gas station in the future, buying smokes and
other emergency items elsewhere, etc.). Also, he has determined that installing a
pump costs $100 per day.
(a) Determine the optimal number of pumps Joe should install.
(b) Joe has also heard that there may be a possible gasoline shortage—or at least the
perception of one—in the near future. Joe read that in the past, this meant that
customers do not really change their driving habits, but fill up their tanks twice
as often. Would that change his plans?
Solution:
(a) Arrival rate per hour λ ¼ 15, service time 1/μ ¼ 4+1 ¼ 5 min, or μ ¼ 12
customers per hour. Thus, we need at least c ¼ 2 pumps for a steady state to
exist. The daily (16 hrs) costs are then:
c ¼ 2: λ0 ¼ 7.5 each, ρ ¼ 0.625, Lq ¼ 1.0417, so that TC(c ¼ 2) ¼ 2(100) +
16(2)15(1.0417) ¼ 700,
c ¼ 3: λ0 ¼ 5 each, ρ ¼ 0.4167, Lq ¼ 0.2976, so that TC(c ¼ 3) ¼ 3(100) +
16(3)15(0.2976) ¼ 514,
c ¼ 4: λ0 ¼ 3.75 each, ρ ¼ 0.3125, Lq ¼ 0.1420, so that TC(c ¼ 4) ¼ 4(100) +
16(4)15(0.1420) ¼ 536,
so that it is optimal to install c ¼ 3 pumps.
(b) Arrival rate per hour λ ¼ 30, service time 1/μ ¼ 2+1 ¼ 3 min (as the fill up time
is now only 2 min, since the customers fill up when the tank is half full), or
μ ¼ 20 customers per hour. Again, at least c ¼ 2 pumps are needed.
470 15 Waiting Line Modelsc ¼ 2: λ0 ¼ 15 each, ρ ¼ 0.75, Lq ¼ 2.25,
so that TC(c ¼ 2) ¼ 2(100) + 16(2)15(2.25) ¼ 1280,
c ¼ 3: λ0 ¼ 10 each, ρ ¼ 0.5, Lq ¼ 0.5,
so that TC(c ¼ 3) ¼ 3(100) + 16(3)15(0.5) ¼ 660,
c ¼ 4: λ0 ¼ 7.5 each, ρ ¼ 0.375, Lq ¼ 0.225,
so that TC(c ¼ 4) ¼ 4(100) + 16(4)15(0.225) ¼ 562,
c ¼ 5: λ0 ¼ 6 each, ρ ¼ 0.3, Lq ¼ 0.1286,
so that TC(c ¼ 5) ¼ 5(100) + 16(5)15(0.1286) ¼ 592.59.
Under these circumstances, it would be best for Joe to have c ¼ 4 pumps. This
represents a 9.34% cost increase over the case without the perception of a shortage.
Problem 3 (comparing queuing systems with fast and slow service):
Customers arrive at a retail outlet at a rate of 30 customers per hour. The total
time that customers spend in the store contributes to their dissatisfaction. A wasted
customer hour has been estimated to cost $10. Management now has two options:
either employ one fully trained fast clerk who is able to serve up to 50 customers per
hour, or two less trained slower clerks, who can handle up to 30 customers per hour
each. Each of the two clerks would have his own waiting line (the supermarket
system). Each of the slow clerks earns $6 per hour, while the fast clerk is fully aware
of his capability and asks for $16 per hour.
(a) Should we hire the two slower clerks or the one fast clerk?
(b) A new applicant for the job offers his services. The company tried him out and it
turned out that he is able to handle no less than 75 customers per hour. Based on
the result under (a), what is the maximal amount that we would pay him?
Solution:
(a) The arrival rate is λ ¼ 30. The fast clerk offers μ ¼ 50, so that ρ ¼ 30/50 ¼ 0.6
and Ls ¼ λ/(μ  λ) ¼ 30/20 ¼ 1.5. The hourly costs are then (cost for
clerk) + (costs for customers) ¼ 16 + 1.5(10) ¼ $31.
In case of the two clerks, there are two M/M/1 systems, each with an individual
arrival rate of λ0 ¼ 15. With a service rate of μ ¼ 30 each, we obtain ρ ¼
15/30 ¼ 0.5 each, so that Ls ¼ 15/(30  15) ¼ 1 each. The hourly costs are then
(costs for two clerks) + 2(costs for customers in each system) ¼ 2(6) + 2(1)
(10) ¼ 32. As a result, we should hire the fast clerk, even though he charges
more than the two slow clerks together and can handle fewer customers than the
two slower clerks combined.
(b) Given a service rate of μ ¼ 75, we obtain ρ ¼ 0.4 and Ls ¼ 2/3. With an
unknown wage w, this results in costs of w þ 2=3ð Þ¼ 10 62=3 þ w. This amount
should not exceed the costs of the best-known solution (a single fast clerk with
hourly costs of $31), so that the bound on the superfast clerk’s wage is 62=3
+ w  31 or w  $24.33.
15.2 Optimization in Queuing 471Simulation 16
Simulation is one of the major devices in an operations researcher’s toolkit, and there
is little doubt that it is among the most flexible and commonly used techniques. In
the words of Budnick et al. (1988),
Simulation is primarily concerned with experimentally predicting the behavior of a real
system for the purpose of designing the system or modifying behavior.
In other words, simulation is a tool that builds a model of a real operation that is to
be investigated, and then feeds the system with externally generated data. We
generally distinguish between deterministic and stochastic simulation. The differ￾ence is that the data that are fed into the system are either deterministic or stochastic.
As an example, a deterministic simulation may attempt to evaluate the effectiveness
of different advertising campaigns. Starting with present sales, assume that we know
the effects of different strategies as well as their costs. We can then simulate the
outcomes of different strategies, with different assumptions, etc., all assuming that
we know with certainty what happens. This chapter deals only with stochastic
simulation, which, in the above example, includes uncertain events, e.g., how
customers react to different types of advertising and how repeated ads affect
purchasing behavior. Stochastic simulation is sometimes also referred to as Monte
Carlo simulation in reference to the Monte Carlo Casinos and the (hopefully)
random outcome of their games of chance.
Another distinction is between continuous and discrete-event simulation. Contin￾uous simulation deals with processes that are continuous and that are modeled as
continuous. Typical examples include the growth of plants, movement of vehicles,
and temperatures. In contrast, discrete-event simulation (the only kind of simulation
discussed in this chapter) has a finite number of points of time, during which events
occur. This could be the demand for a product during a specific day, the number of
times a website is visited, or the number of incidents of a specific disease at a regional
hospital. Discrete-event simulation can, however, also discretize events, i.e., consider
the continuous growth of plants, which are observed, say, every 2 hrs.
# The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
H. A. Eiselt, C.-L. Sandblom, Operations Research, Springer Texts in Business and
Economics, https://doi.org/10.1007/978-3-030-97162-5_16
47316.1 Introduction to Simulation
The main reason for a researcher to resort to simulation is twofold. First of all,
simulation is probably the most flexible tool imaginable. Take queuing as an
example. While it is very difficult to incorporate reneging, jumping queues, and
other types of customer behavior in the usual analytical models (see, e.g., Chap. 15
of this volume), this presents no problem for simulation. Similarly, recall that the
queuing formulas that have been derived refer to steady-state solutions. A system
may have to run for a very long time to reach a steady state, assuming that one exists.
As a result, a modeler may be more interested in transient states, which are easily
available in a simulation.
The second reason is that simulation is very cheap. Building a model that
simulates the opening of a new restaurant will most certainly be a lot less expensive
than trying it out. Even if costs are no subject, the time frame can be compressed in a
simulation. For instance, if we were to observe the demand structure of a product, a
long time would be required, so that results would probably be available when the
product has become technologically obsolete anyway.
The main steps of a discrete-event simulation include
1. Building the model
2. Assigning numbers to uncertain events according to their likelihoods
3. Generating uncertain events
4. Applying predetermined policies
5. Evaluating the results including verifying the model.
The generation of random numbers will be explained in some detail in the next
section. The generation of uncertain events and the application of policies to them
use an accounting procedure that is demonstrated on a queuing example and an
inventory system in Sect. 16.3.
Before starting to discuss the generation of random numbers, we would like to
discuss their assignment to random events. This is best explained by way of an
example. Suppose that the owner of a store has observed the demand for a specific
item and has determined that there is a 10% chance that the demand is 20, a 30%
chance that the demand is 35, a 50% chance that the demand is 50, and a 10% chance
that the demand is 60. The task is now to assign random numbers to these random
events, so that the likelihood of choosing a random number that is assigned to an
event equals the observed probability of the event. In our example, we could use
single-digit random numbers. If we generate uniformly distributed random numbers,
then all digits are equally likely to come up; i.e., the probability of each digit’s
appearance is 0.1. We could then assign the digit 3 to the demand of 20, the digits
0, 5, and 8 to a demand of 35, the digits 1, 2, 6, 7, and 9 to a demand of 50, and the
digit 2 to the demand of 60. Since each digit has a 10% chance of appearing,
randomly generated events will have the different demands come up with the
observed probabilities. Alternatively, we could make the assignments of
double-digit random numbers (in a somewhat more orderly fashion) as shown in
474 16 SimulationTable 16.1. Again, the numbers assigned to the discrete events reflect the observed
probabilities.
For the assignment shown in Table 16.1, if the random numbers 15, 27, and
81 are drawn, they refer to demands of 35, 35, and 50, respectively. It is apparent that
assigning different random numbers to random events—even while preserving their
probabilities—will result in different demands being generated. In order to overcome
the effects that are due to the specific assignment, the process should be repeated
very often. For example, if the decision maker is interested in the demands for a
product during a 12-month period, we would not generate demands for 1 year, but
for thousands of years, so that differences due to different random number
assignments will vanish.
Given the size (more so than the complexity) of the task, it is little surprise that all
simulations are computer-based. While it is possible to write simulations in any
all-purpose programming language such as Python, C/C++, Java, or R, special
simulation languages have been around since the 1960s. Among them are commer￾cial simulation languages such as AnyLogic, Arena, or FlexSim, and open-source
programs such as Simula, SimPy, and others.
Whenever a simulation has been performed, the validation of the results is
mandatory. Often this means checking the computer code and performing statistical
tests. However, the validation of some of the behavioral assumptions and the
structure of the model are at least as important. For instance, if we assume that
customers make a special trip to a gas station, this assumption has to be validated.
While this is a task that is typically performed before the simulation takes place, it is
sometimes necessary to validate an assumption after the fact. As an example,
consider a fast-food chain that attempts to locate a new branch. In addition to
behavioral studies before the simulation, it may be very useful to apply the model
with all of its assumption to an already existing branch and see whether or not it
recreates a known situation. If it does not, the discrepancies will allow the modeler to
pinpoint the aspects of the model, in which erroneous assumptions may have
been made.
16.2 Random Numbers and Their Generation
Random numbers have been around for a long time. Among the earlier systematic
efforts to generate random numbers is the work by statistician L.H.C. Tippet, who
produced the first random number tables that were based on census numbers. In the
mid-1950s, the Rand Corporation published a tome A Million Random Numbers.
Today, we distinguish between true random numbers and pseudo-random numbers.
Table 16.1 Assignment of random numbers to demands
Demand 20 35 50 60
Probability .1 .3 .5 .1
Numbers assigned to event 01–10 11–40 41–90 91–00
16.2 Random Numbers and Their Generation 475Roughly speaking, true random numbers are generated by way of a random process,
while pseudo-random numbers are machine generated by means of a deterministic
process. An obvious way to generate true random numbers is to roll dice. Assuming
that we have a usual six-sided die which is not loaded or skewed, each side has a
chance of 1/6 of coming up; i.e., the probability of each number is 16 2
3 %. It is not
difficult to devise differently shaped dice that have ten sides, once for each possible
digit. Note that for the time being we only deal with uniformly distributed random
numbers, i.e., those in which all possible numbers have the same chance of
appearing. If two-digit random numbers are sought, use multiple dice, roll them
all, and add their numbers. Note, however, that care must be taken: taking, for
instance, two standard six-sided dice, rolling them and adding up their numbers, will
not result in uniformly distributed results. As an example, the outcome of “2” is only
possible, if both dice show a “1,” which has a probability of 1/36. On the other hand,
an outcome of “8” has a probability of 5/36, as it can be realized from 2 and 6, 3 and
5, 4 and 4, 5 and 3, and 6 and 2.
However, changing the numbers on the faces enables us to use the same process
to generate random numbers that have an equal probability of appearing. If the first
die has the numbers 0, 1, 2, 3, 4 ,and 5 on its side, the second has 0, 6, 12, 18, 24, 30,
and 36 on its sides, the third has 0, 36, 72, 108, 144, and 180, and the fourth die has
numbers 0, 216, 432, 648, 864, and 1080, then the number that results from adding
the face values of the four dice is a uniformly distributed random number between
0 and 1295.
Generating random numbers by way of rolling dice may be fun, but it certainly is
not a viable method for industrial applications. This is when we resort to machine￾generated sequences of random numbers. The basic idea of all of these generators is
the same: given a seed, i.e., an initial user-determined value, we put this number into
a “black box,” which uses our input and its internal parameters to generate another
number, which is used twofold, as the first random number, and also as the next input
into the black box, which uses it to generate the next random number.
Among the first random number generators is the Midsquare Method, which is
said to date back to Nobel Prize laureate John von Neumann. The idea is to start with
a seed, square it, retain the middle digits as random number and next input, and
continue in this fashion. The main reasoning behind choosing the center part of a
number and delete its first last parts is this. The last digit(s) of a number is/are not
necessarily random. For instance, if the last digit is 0, 1, 5, or 6, the square of the
number, regardless of what the other digits of the number are, will have a last digit of
0, 1, 5, or 6 as well. Similarly, if the last digit is an even number, then the square of
the number will also have an even last digit. As far as the leading digit is concerned,
there is much less of a chance of getting an 8 or a 9 than getting a smaller first digit.
As an example of the Midsquare Method, consider the seed x0 ¼ 107364 and
assume that we are interested in five-digit pseudo-random numbers. Squaring this
number results in 115 27028 496, so that our first random number is x1 ¼ 27028, the
center of the number shown by the appropriate spacing. Using x1 as input and
squaring it results in 73 05127 84, so that x2 ¼ 05127. Squaring x2 results in
476 16 Simulation262861 29 and x3 ¼ 62861. The process continues with x4 ¼ 51505, x5 ¼ 52765, and
so forth.
The Midsquare Method is plagued by a multitude of problems, though. Take, for
instance, the seed x0 ¼ 41 and generate a sequence of random numbers by deleting
the first and last digit after squaring. This process results in the sequence x1 ¼ 68,
x2 ¼ 62, x3 ¼ 84, x4 ¼ 05, x5 ¼ 02, and x6 ¼ 00, at which time the series has
degenerated and will never generate anything but zeroes.
A much better choice is one of the so-called Linear Congruence Methods. They
work with the function
xi ¼ ð Þ a þ bxi1 mod c,
where a, b, and c are integer parameters, while xi is the ith random number as
usual. The “mod” function returns the remainder as the result of the division. As an
example, consider a number of examples. In case 17 mod 5, we divide 17 by
5, which equals 3 and a remainder of 2, thus 17 mod 5  2. Similarly, consider
31 mod 9. Dividing 31 by 9 equals 3 and a remainder of 4, so 31 mod 9  4.
Suppose now that we use the parameters a ¼ 17, b ¼ 3, c ¼ 101, and x0 ¼ 53. We
can then compute
x1 ¼ ½  17 þ 3 53 ð Þ mod 101 ¼ 75,
x2 ¼ ½  17 þ 3 75 ð Þ mod 101 ¼ 40,
x3 ¼ ½  17 þ 3 40 ð Þ mod 101 ¼ 36,
x4 ¼ ½  17 þ 3 36 ð Þ mod 101 ¼ 24,
x5 ¼ ½  17 þ 3 24 ð Þ mod 101 ¼ 89,
and so forth. It is apparent that the largest number that can be generated in this
example will be c  1 ¼ 100. This means that after at most 100 generated numbers,
the sequence generated with our parameters will reach a number that has been
generated before. And, since the parameters have not changed, the same sequence
will be generated over and over again. The number of different random numbers that
can be generated before the sequence repeats itself is called the cycle length or the
period of the generator. Typically, the idea is to choose the parameters, so that the
period is as long as possible. However, that is not the only criterion for a good set of
random numbers. Consider a generator with a ¼ b ¼ 1, c ¼ 10, and a seed x0 ¼ 0.
The generator determines x1 ¼ 1, x2 ¼ 2, x3 ¼ 3, and so forth, until we obtain x9 ¼ 9,
and x10 ¼ x0 ¼ 0. Thus, the cycle length equals 10, but the sequence looks anything
but random.
Another obvious criterion a sequence of uniformly distributed random numbers
has to satisfy is that each digit will come up about 10% of the time. The above
sequence 0, 1, 2, ..., 9, 0, ... does exactly that. However, the conditional probability
of, say, a 5 coming up directly following a 2 is zero, while the probability of a
3 directly following a 2 is 1. This is an obvious test that this particular sequence fails.
And that makes it a pseudo-random number. In contrast, remember the roll of a
single die. Suppose that a 2 came up on one roll, and a 5 on the next. We keep on
16.2 Random Numbers and Their Generation 477rolling, until another 2 comes up. What is then the probability that the next roll will
show a 5? With a perfect die, it will be 1/6. With any pseudo-random number, it will
be 1, as whenever the same number comes up again, we are in a cycle, which repeats
itself.
There is a variety of other tests that random number generators have to pass in
order to be reasonable. The parameter c is particularly critical, and it is often chosen
as a large prime number. Overall, each sequence of numbers generated in this way
has a finite period.
Part of the importance of random numbers is not so much that they are used for
simulations, but they are also crucial for Internet security by way of encryptions, and
Internet gambling. Given the amount of money involved in these ventures, much is
at stake. And the general idea is that random numbers can only truly be generated by
a process that involves random elements. Some fairly unusual methods have been
included, among others seeds that depend on the number of particles emitted from
radioactive elements, atmospheric noise from resistors, and a patented method, a
seed based on random movements observed in lava lamps. Another simple technique
is to use digits of the value of the irrational number π, which have been computed to
billions of digits.
So far, we have discussed random numbers that are uniformly distributed. This is
not always desirable. However, fairly simple procedures can be employed to trans￾form uniformly distributed random numbers into random numbers that follow other
distributions. The easiest case is to determine random variables that are uniformly
distributed on [0, 1[. If numbers with k digits to the right of the decimal point are
sought, then each k-digit random number needs to be divided by the largest k-digit
number that can be generated, viz., 10k  1. Random numbers that are uniformly
distributed on [0, 1[ are typically the basis for the generation of random numbers that
follow other distributions.
Consider now Poisson-distributed random numbers. The cumulative density
function F(x) of a random variable x that follows a (discrete) Poisson distribution
with parameter λ can be found in many works with mathematical tables, e.g., Råde
and Westergren (2004). Table 16.2 shows the cumulative functional values for the
Poisson distribution with parameter λ ¼ 2.5.
We can now generate Poisson-distributed random numbers by starting with
uniformly distributed random numbers. If such a random number falls into an
interval F(x1) and F(x2), then the Poisson-distributed random number is x2. As a
numerical example, consider the following uniformly distributed random numbers:
Table 16.2 Cumulative distribution values for a Poisson distribution with λ ¼ 2.5
x 012345
F(x) .0821 .2873 .5438 .7576 .8912 .9580
x 6 7 8 9 10
F(x) .9858 .9958 .9989 .9997 .9999
478 16 Simulation0.0537 0.7406 0.5926 0.8807 0.6603 0.7126 0.8016 0.7973 0.9584 0.6570
0.8457
The first random number is between 0 and F(x ¼ 0), so that a random number of
0 results. The next random number 0.7406 is in the interval [.5438, .7576], so that
the next random number equals the x-value of the upper end of the interval, viz.,
3. Similarly, the next uniformly distributed random number .5926 is also located in
the interval [.5438, .7576], so that again the x-value of the upper bound x ¼ 3 results
as the next Poisson-distributed random number. Continuing in a similar fashion
results in the ten random numbers 0, 3, 3, 4, 3, 3, 4, 4, 6, 3, and 4.
Consider now random numbers that follow an exponential distribution with
parameter λ. Denoting again uniformly distributed random numbers in [0, 1[ by ui,
we can then compute exponentially distributed random numbers xi by using the
formula xi ¼ ln ui/λ. Given again the above 11 uniformly distributed random
numbers, we can compute exponentially distributed random numbers with parameter
λ ¼ 2.5 as
1.1697, 1.2012, 0.2093, 0.0508, 0.1660, 0.1355, 0.0885, 0.0906, 0.0170,
0.1680, and 0.0670.
On the other hand, random variables that follow a standard normal distribution
can be generated from uniformly distributed numbers in the [0, 1[ interval in pairs.
Letting ui and ui + 1 denote the ith pair of uniformly distributed random variables, we
can obtain a pair of related standard normally distributed random variables xi, xi + 1
by using the relations
xi ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
2 ln ui
p sin 2ð Þ πuiþ1 and xiþ1 ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
2 ln ui
p cos 2ð Þ πuiþ1 :
The sequence of standard normally distributed random numbers derived from the
first ten uniformly distributed random numbers is
0.1962, 2.4104, 0.0986, 1.0278, 0.0711, 0.9083, 0.0581, 0.6625, 0.0298,
and 0.2908.
Random numbers that follow other distributions can be computed as well. In the
next section, we will demonstrate how these random numbers can be used to model
practical situations.
16.3 Examples of Simulations
This section presents two numerical examples of simple simulations. While they
focus only on very specific aspects, they should be able to convey some of the
fundamental ideas used in real-world simulations.
16.3 Examples of Simulations 47916.3.1 Simulation of a Waiting Line System
The main goal of simulation is to evaluate existing solutions and policies. In other
words, we start with a policy (or a solution), and test how this policy or solution will
fare in an uncertain environment. And this uncertain environment is recreated by
generating scenarios, given the states of nature and their probabilities that have been
observed.
This may best be explained by a few examples. First consider a waiting line
system. For simplicity, we will work with a single-channel system. Customers arrive
at the service station, so that the interarrival times are uniformly distributed on
the integers between 4 and 9. The service times are also uniformly distributed, but
on the integers between 5 and 7 (i.e., a service time of 5 min has a probability of 1/3,
the same as a service time of 6 min and one of 7 min. The purpose is now to evaluate
the performance of the system. The criteria used for that purpose can be manifold.
On the customers’ side, we could use the probability that a customer will have to
wait, the average waiting time, and the average number of customers in the system
(really a proxy for the congestion of the system). On the server’s side, we could be
interested in the average idle time during a workday, and, of course, the cost of the
system.
First, we will have to assign events to random numbers. For simplicity, we will
use single digits for the interarrival times. A random digit of 4 means an interarrival
time of 4 min, a random number of 5 means an interarrival time of 5 min, and so
forth. In case a random digit 1, 2, 3, or 0 comes up, we will reject the digit and move
on to the next random number.
A similar procedure will be used for service times. Here, the random numbers
5, 6, and 7 denote the actual service times in minutes, all other random numbers will
be rejected. A set of uniformly distributed random numbers is shown in Table 16.3.
Tables of random numbers can be read from left to right and top to bottom, or right to
left bottom up, or in any other more or less organized way. Here, we scan them row
by row from left to right, starting with the first row.
The scenarios for the first 15 customers are generated in Table 16.4. The first
column of the Table 16.4 is the customer number. Column 2 lists the interarrival
times that are generated with the random numbers from Table 16.3. The first two
digits are 2 and 0, and neither of them are assigned to actual interarrival times (only
digits between 4 and 9 are), so that these digits are rejected. The next two digits are
4 and 9. They are assigned to interarrival times of 4 and 9, which are now the
interarrival times of the first two customers. The next four digits are 9, 1, 3, and
5. Here, 1 and 3 are unassigned and are rejected, so that only the digits 9 and 5 are
usable; they are the interarrival times of the next two customers. This process
Table 16.3 Random digits
2049 9135 6601 5112 5266 6728 2188 3846 3734 4017
7087 2825 8667 8831 1617 7239 9622 1622 0409 5822
6187 0189 5748 0380 8820 3606 7316 4297 2160 8973
480 16 SimulationTable 16.4 Simulation of a queuing system
Customer
#
Interarrival time
(random number)
Arrival
time
Service finished/
customer leaves
Customer
wait time
Station
idle time
Start
service
at
Service time
(random
number)
Cumulative
wait time
Cumulative
idle time
1 4 9:04 9:11 0 4 9:04 7 0 4
2 9 9:13 9:20 0 2 9:13 7 0 6
3 9 9:22 9:27 0 2 9:22 5 0 8
4 5 9:27 9:33 0 0 9:27 6 0 8
5 6 9:33 9:39 0 0 9:33 6 0 8
6 6 9:39 9:46 0 0 9:39 7 0 8
7 5 9:44 9:52 2 0 9:46 6 2 8
8 5 9:49 9:59 3 0 9:52 7 5 8
9 6 9:35 10:06 4 0 9:59 7 9 8
10 6 10:01 10:12 3 0 10:06 6 14 8
11 6 10:07 10:18 3 0 10:12 6 19 8
12 7 10:14 10:23 4 0 10:18 5 23 8
13 8 10:22 10:29 1 0 10:23 6 24 8
14 8 10:30 10:37 0 1 10:30 7 24 9
15 8 10:38 10:43 0 1 10:38 5 24 10
16.3 Examples of Simulations 481continues, until we have assigned interarrival times for the first 15 customers. So far,
we have used random numbers of the first seven of the ten blocks of four digits each
in the first row of Table 16.3.
We now use the same process to generate the service times for the first
15 customers. Starting with the second row of random numbers in Table 16.3
(alternatively, we could have simply continued where we left off with the interarrival
times and started with the eighth block in the first row), we use only random numbers
between 5 and 7 as service times, while rejecting all other digits. In the first block, we
keep the first and fourth digit (both 7 sec), while rejecting the second and third digit
(0 and 8), as they are unassigned. In the second block, the first three digits are
unassigned, leaving on the fourth digit (with a value of 5). The third block has the
first digit unassigned, but the remaining three digits (6, 6, and 7) are usable. The
fourth block has all digits unassigned, and so forth. The service times for the first
15 customers are displayed in column 8 of Table 16.4.
We are now ready to perform the simulation. We start with customer 1, meaning
the first row of Table 16.4. Given that the system starts operating at, say, 9 a.m., and
the interarrival time is 4 min, the first customer will arrive at 9:04 (column 3). Since
no other customer is presently in the system, service for customer 1 will start
immediately (column 7). This means that there was no waiting time (column 5),
while there was a 4 minute idle time before this customer’s arrival (column 6). Given
the service time of 7 min (column 8), service on customer 1 will be finished at 9:11
(column 4). For simplicity, we compute the aggregate waiting times (column 9) and
the aggregate idle times (column 10) for each customer, so that they are easily
available for our evaluations later on.
The computations for customer 2 are similar. Given the interarrival time of 9 min,
this customer will arrive 9 min after customer 1 arrived, meaning at 9:13. Since
customer 1 has left the service station at 9:11, there was a 2-minute idle time (column
6), but no waiting time for customer 2 (column 5). As a result, service begins
immediately at 9:13 (column 7). Given that the service takes 7 min (column 8),
customer 2 is done and leaves at 9:20.
This process continues in the same fashion until customer 6 leaves. Consider now
customer 7. When the customer arrives at 9:44, customer 6 is still in the system, so
that customer 7 will have to wait. More specifically, since customer is finished at 9:
46, customer 7 will wait for 2 min (which is recorded in column 5). Service for
customer 7 starts at 9:46 (reported in column 7). Also note that there is no idle time
for the system, which is shown in column 6. Customer 7 will leave the system after
6 minute service time (column 8) at 9:52 (column 4). The process continues in a
similar fashion for the remaining eight customers.
This is also the point, at which other behavioral aspects could easily be
incorporated. Take, for instance, the possibility that a customer reneges, i.e.,
considers the lineup too long (as a proxy for the expected waiting time) and decides
not to wait but go elsewhere instead or return at some other time. While such a
behavior would be rather difficult to incorporate in an analytical method, it is easy to
do so in a simulation. Here, if customer 7 will only enter the system, if there is no
other customer in the system, we would record at this time that one potential
482 16 Simulationcustomer was not served and continue with the next customer. The number of
customers who leave the system unserved is another criterion in the evaluation of
the performance of the system.
Suppose this very small simulation is to be evaluated. We first observe that there
is some idle time in the beginning, while later on the system is very busy. This is
hardly surprising: after all, the average interarrival time is 6.5 min, while the average
service time is 6 min, leading to an average traffic intensity of ρ ¼ .9231, a clear sign
of a very busy system.
We also observe that the total waiting time for all the 15 customers that we have
observed (column 9) is 24 min. In other words, there is an average waiting time of
24/15 ¼ 1.6 min per customer. This information will have to be evaluated by the
decision maker, who will have to decide whether or not this time is too long. Note
again that the waiting times are not even distributed throughout the day, but get
longer later in the day.
Another issue is the idle time of the system. The system has been operating from
its opening at 9:00 until the last customer left at 10:43, i.e., for a total of 103 min. The
total idle time (row 15 of column 10) is 10 min. This means that the total idle time is
10/103  10% of the time. Again, this time is clustered during the early stages of the
operation.
These, and potentially other, criteria can then be used by the decision maker to
evaluate and, if deemed necessary, improve the performance of the system by using
appropriate measures.
16.3.2 Simulation of an Inventory System
Another popular area, in which Monte Carlo simulation is applied, deals with
inventory management. Again, we first have to observe and quantify all system
parameters, and then decide on a policy, meaning assign values to the variables
under our jurisdiction. In this example, we distribute a single product, for which
there is a daily demand that is uniformly distributed between 0 and 99 units. The lead
time is assumed to be 3 days. Note that in more realistic applications, the lead time
will also be a random variable. Furthermore, we assume that we have an opening
inventory of 200 units. The unit costs are $100 to place an order and receive the
shipment, the daily holding costs are 10¢ per unit that is held overnight, and the
shortage costs are 30¢ for each unit that is not available and for which a customer has
to wait overnight. Unit holding costs and shortage costs are linear in time, meaning
that if a customer has to wait for 5 days to obtain the desired good, the shortage costs
will be 5(0.3) ¼ $1.50. The present inventory policy does not place a new order, if
another order is still outstanding, even if the inventory level is below the reorder
point. Also, in case of shortages, we assume that units are backordered and there are
no lost sales. Other than that, management has formulated the following.
Policy 1: Place an order of size Q ¼ 300, whenever the inventory level observed
at the end of a day falls below the reorder point R ¼ 150.
16.3 Examples of Simulations 483In order to perform the simulation, we first need random numbers. Table 16.5
provides the needed numbers, and we will assign a double-digit random number to a
demand of the same magnitude; i.e., a random number 67 will symbolize a demand
of 67.
Table 16.6 shows the simulation for 10 days. Some details of the computations
will be elaborated upon below.
Before opening on Day 1, our inventory is 200 units as stated in the assumptions.
The demand on this day (the first two random digits from the list) is 89, so that by the
end of the day, our inventory level has decreased to 111. Comparing this value to our
reorder point, we realize that the present inventory level has fallen below the reorder
point of R ¼ 150, so that we place an order. Given the (deterministic) lead time of
3 days, the shipment that relates to this order will arrive on Day 4 before we open the
store. The costs on that day are $100 for placing an order, 111(0.10) ¼ $11.10 in
terms of inventory holding costs (the 111 units to be carried over to Day 2 multiplied
by the unit holding costs), and zero shortage costs, as no shortages were
encountered.
Day 2 is dealt with similarly. Note that even though by the end of the day, only
20 units remain in stock which is much less than the reorder point, a new order will
not be placed according to the policy that prohibits placing a new order if another
order is still outstanding. During Day 3, we encounter a shortage. Similar to carrying
costs, shortages are assessed for the number of units we are short by the end of the
day (here: 52 units short at 30¢ each for $15.60).
This brings us to the morning of Day 4, at which time a shipment with 300 new
units comes in. This shipment belongs to the order placed on the evening of Day
1. From this shipment, the demand of all customers whose items were on backorder
will be satisfied, before the regular demand takes over.
Table 16.5 Random numbers
89 91 72 26 10 83 90 30 76 40
Table 16.6 Simulation for Inventory Policy 1
Day
#
Inventory level before
opening Demand
Inventory level after
closing Costs: Co, Ch, Cs
1 200 89 111 ! order 100, 11.10, 0
2 111 91 20 0, 2.00, 0
3 20 72 52 0, 0, 15.60
4 52 + 300 ¼ 248 26 222 0, 22.20, 0
5 222 10 212 0, 21.20, 0
6 212 83 129 ! order 100, 12.90, 0
7 129 90 39 0, 3.90, 0
8 39 30 9 0, 0.90, 0
9 9 + 300 ¼ 309 76 233 0, 23.30, 0
10 233 40 193 0, 19.30, 0
484 16 SimulationThat way, we simulate the process for 10 days. At this point, we can evaluate
Policy 1. We note that the ordering costs are $200, the carrying costs are $116.80,
and the shortage costs are $15.60, for total inventory costs of $332.40. Other
characteristics of the system can also be evaluated, for instance the service level,
which may be expressed as the proportion of the demand that can be satisfied
immediately rather than having to be backordered. For simplicity, we will concen￾trate on cost considerations. In the simulation of Policy 1, we note that the ordering
costs dominate, while shortage costs are very low. This may lead to a revised policy,
in which we place larger orders. In particular, we formulate:
Policy 2: Place an order of size Q ¼ 600, whenever the inventory level observed
at the end of a day falls below the reorder point R ¼ 150.
Using the same random numbers (and thus the same demand throughout the
10 days), the workings of this policy are shown in Table 16.7. Since the calculations
are very similar to those in Table 16.6, we just produce the results without further
comments.
The individual total costs associated with Policy 2 are Co ¼ 100, Ch ¼ 266.80,
and Cs ¼ 15.60 for total inventory costs of $382.40, a 15% increase over Policy
1. We note that the holding costs are now very high, while shortage costs are still at
their previous low level. A (hopefully improved) new policy may be defined as
having a reorder point of, say, R ¼ 100, and an order quantity of Q ¼ 450. We leave
further experimentation to the reader.
In general, we would like to emphasize that the decision rule that is to be
evaluated with simulation does not have to be a fixed rule that is determined once
in the beginning and then left unchanged throughout the process. Instead, the rule
can include periodic updates. For instance, a diet planner could optimize the food
plan for a senior citizen’s home and, once one or more of the parameters change,
reoptimize and implement the new plan. Similarly, we could periodically update the
reorder point and order quantity in an inventory system.
In our example, we could formulate the following policy that features dynamic
readjustments of order quantity and reorder point:
Table 16.7 Simulation for Inventory Policy 2
Day
#
Inventory level before
opening Demand
Inventory level after
closing Costs: Co, Ch, Cs
1 200 89 111 ! order 100, 11.10, 0
2 111 91 20 0, 2.00, 0
3 20 72 52 0, 0, 15.60
4 52 + 600 ¼ 548 26 522 0, 52.20, 0
5 522 10 512 0, 51.20, 0
6 512 83 429 0, 42.90, 0
7 429 90 339 0, 33.90, 0
8 339 30 309 0, 30.90, 0
9 309 76 233 0, 23.30, 0
10 233 40 193 0, 19.30, 0
16.3 Examples of Simulations 485Policy 3: (1) Starting with an order quantity of Q ¼ 300, readjust the order quantity
whenever an order is placed, so that it is 3 times the average daily demand since
the last time an order was placed (i.e., 3 times the average daily demand in the last
cycle).
(2) Starting with a reorder point of R ¼ 150, update the reorder point whenever an
order comes in, so that R: ¼ R  ½ (shortage/inventory level just before the new
order comes in). In other words, if there is a shortage just before the arrival of the
new order, the new reorder point equals the previous value of R plus half the
shortage. If, on the other hand, there is still some inventory left, half of that
amount is subtracted from the previous reorder point to obtain the new value of R.
This somewhat more elaborate policy requires some additional explanations.
Again, we will use the same random numbers and thus the same demand as in the
previous policies. Initially, we have a reorder point of R ¼ 150 and an order quantity
of Q ¼ 300 as in Policy 1. As in Policy 1, the demand on Day 1 equals 89, so that our
inventory level has fallen to 111 by the end of the day. The order quantity is now
recalculated as three times the average daily demand since the last order was placed.
Since this is the first order that we place, the average is computed for the time
between the beginning of the simulation and the end of Day 1. Since only one daily
demand has occurred, the order quantity is computed as Q ¼ 3(89) ¼ 267. Again, the
shipment that relates to this order will arrive in the morning of Day 4.
The computations are the same as for Policy 1 until the morning of Day 4 when
the shipment arrives. After deducting the backordered demand, we recomputed the
reorder point. Since we had a shortage of 52 units before the shipment arrived, the
new reorder point is R ¼ 150 + ½(52) ¼ 176.
The process continues again until Day 6. In the evening of Day 6, the inventory
level has fallen below the new reorder point of R ¼ 176, and we place another order,
which will arrive in the morning of Day 9. The order quantity is now computed on
the basis of the average demand since the last order was placed. Here, the previous
order was placed on Day 1, and the demand since then was 91 units on Day
2, 72 units on Day 3, 26 units on Day 4, 10 units on Day 5, and 83 units on Day
6 for an average of 282/5 ¼ 56.4 units. According to the policy, the order quantity is
three times this amount, i.e., 169.2 units, which we round to the nearest integer, so
that Q ¼ 169.
From here, the inventory system continues without interruption until Day 9, when
the shipment arrives that was ordered at the end of Day 6. After satisfying the
demand with the backordered items, we still have 145 units in stock. Since there was
a shortage of 24 units just before opening on Day 9, the new reorder point is
calculated as the previous reorder point of 176 plus half of the latest shortage for
R ¼ 176+ ½(24) ¼ 188. Note that even though the opening inventory on Day 9 is
below the reorder point, the policy allows orders to be placed only by the end of the
day, which is done here at the end of Day 9. Since the last order on Day 6, the daily
demand has been 90, 30, and 76 for an average of 196=3 ¼ 65 1
3, so that the new
486 16 Simulationorder quantity is computed as Q ¼ 3 65 1
3
  ¼ 196. The first 10 days of this simula￾tion are shown in Table 16.8.
In this policy, the total ordering costs are $300, the carrying costs are $69.90, and
the shortage costs are $22.80 for a grand total of 392.70. Since these costs are about
18% higher than those for Policy 1, further refinements are needed. However, it is
worth pointing out that the time frame of 10 days is far too short to make any real
recommendations. It was chosen here merely for illustrative purposes.
The above two examples were chosen for this book, as they deal with subject
matter that was introduced in earlier chapters, and because they are very intuitive.
Even these simple scenarios could be extended in a variety of directions so as to
become quite involved. Still the basic ideas remain the same regardless of the
complexity of the model.
Exercises
Problem 1 (simulation of a replacement problem): The lighting director of a
theater is worried about the maintenance and replacement of five floodlights. They
fail according to the number of weeks they have been installed and used. The
probability that a bulb still functions after it has been used for t weeks (i.e., it is
presently in its (t + 1)-st week of use) is denoted by P(Y|t). The numerical values are
shown in Table 16.9. In addition, the single-digit random numbers associated with
the survival events are shown in the last row of the table.
Table 16.8 Simulation for Inventory Policy 3
Day
#
Inventory level before
opening Demand
Inventory level after
closing
Costs: Co, Ch,
Cs
1 200 (R ¼ 150) 89 111 ! order
Q ¼ 267
100, 11.10, 0
2 111 91 20 0, 2.00, 0
3 20 72 52 0, 0,, 15.60
4 52 + 267 ¼ 215 (R ¼ 176) 26 189 0, 18.90, 0
5 189 10 179 0, 17.90, 0
6 179 83 96 ! order
Q ¼ 169
100, 9.60, 0
7 96 90 6 0, 0.60, 0
8 6 30 24 0, 0, 7.20
9 24 + 169 ¼ 145 (R ¼ 188) 76 69 ! order
Q ¼ 196
100, 6.90, 0
10 69 40 29 0, 2.90, 0
Table 16.9 Conditional survival probabilities and associated random digits
P(Y|0) P(Y|1) P(Y|2) P(Y|3) P(Y|  4)
Probability 0.9 0.7 0.5 0.3 0.2
Random numbers 1–9 1–7 1–5 1–3 1–2
16.3 Examples of Simulations 487Whenever a bulb fails, it has to be replaced immediately, as “the show must go
on.” Changing a bulb individually is expensive, as scaffolding must be put up. The
entire process costs $350. On the other hand, changing all five bulbs once regardless
of whether they still work or not costs $800.
The director ponders two replacement policies: either replace the bulbs only when
they actually fail, or, alternatively, in addition to failures during the week which have
to be attended to immediately, change all bulbs every 3 weeks regardless of whether
they still work or not. It should be pointed out that even if multiple bulbs fail during
the same week, they may do so at different times, so that this case has to be treated
and paid for as individual failures.
Solution: Use the following random numbers to generate specific instances of
survival and failure:
83638 51597 70322 35984 03933 30948 36142 72865 63348 28024
Tables 16.10 and 16.11 then display for each light its age in a given week, the
random number, and an indication, if a bulb works during any given week (W), or if
it fails (F). Consider, for instance, Light 4. In week 1, its age is 0 as the bulb is new.
According to Table 16.9, the random digits associated with not failing are 1–9, and
since the random digit is 3, it will symbolize proper functioning. This means that in
the beginning of week 2, Light 4 is of age 2. The next random digit is 9, which, for a
bulb in week 2, means failure. This means that during week 2, bulb 4 will be replaced
and its age in week 3 will again be 0. The process continues in this fashion for all
bulbs. The result of this replacement policy is that 16 bulb replacements are
necessary for a total cost of $5600.
The simulation for the second policy is shown in Table 16.11. Here, we use the
same random digits as before. As an explanation of the numbers in the table,
consider Light 1. It works during weeks 1 and 2, but fails during week 3, when it
has to be replaced during the week. At the beginning of week 4 (indicated by a “*” in
the leftmost column), a group replacement is made, at what time the bulb of Light
1 is replaced again, even though it was just replaced individually during the
previous week.
It turns out that this policy requires only 12 individual replacements for a total of
$4200, plus three total replacements for $2400 for a grand total of $6600. This is
more than the costs of individual replacement alone, making the former strategy
preferable.
Problem 2 (evaluation of investment strategies via simulation of stock
prices): The manager of an investment company manages a specific fund. There
are $1,000,000 available for investment and three stocks, currently priced at $17,
$59, and $103 per share, respectively, are considered for that fund. Planning is made
on a weekly basis and any amounts that are not invested will be kept in a short-term
money market account that pays 0.01% per week.
The manager considers three investment strategies. The first strategy would be to
keep all of the money in the short-term account. This is the benchmark strategy. The
second strategy will invest 50% of the available money at the end of any week that
488 16 SimulationTable 16.10 Simulation given only individual replacements
Light 1 Light 2 Light 3 Light 4 Light 5
Week Age # W/F Age # W/F Age # W/F Age # W/F Age # W/F
1 08
W 0 3
W 0 6
W 0 3
W 0 8
W
2 15
W 1 1
W 1 5
W 1 9
F 1 7
W
3 27
F 2 0
F 2 3
W 0 2
W 2 2
W
4 03
W 0 5
W 3 9
F 1 8
F 3 4
F
5 10
F 1 3
W 0 9
W 0 3
W 0 3
W
6 03
W 2 0
F 1 9
F 1 4
W 1 8
F
7 13
W 0 6
W 0 1
W 2 4
W 0 2
W
8 27
F 1 2
W 1 8
F 3 6
F 1 5
W
9 06
W 2 3
W 0 3
W 0 4
W 2 8
F
10 1 2
W 3 8
F 1 0
F 1 2
W 0 4
W
11 2 0 0 2 1
16.3 Examples of Simulations 489Table 16.11 Simulation given individual replacements in addition to triweekly group replacements
Light 1 Light 2 Light 3 Light 4 Light 5
Week Age # W/F Age # W/F Age # W/F Age # W/F Age # W/F
1 08
W 0 3
W 0 6
W 0 3
W 0 8
W
2 15
W 1 1
W 1 5
W 1 9
F 1 7
W
3 27
F 2 0
F 2 3
W 0 2
W 2 2
W
4* 0 3
W 0 5
W 0 9
W 0 8
W 0 4
W
3 10
F 1 3
W 1 9
F 1 3
W 1 3
W
6 03
W 2 0
F 0 9
W 2 4
W 2 8
F
7* 0 3
W 0 6
W 0 1
W 0 4
W 0 2
W
8 17
W 1 2
W 1 8
F 2 6
F 1 5
W
9 26
F 2 3
W 0 3
W 1 4
W 2 8
F
10* 0 2
W 0 8
W 0 0
F 0 2
W 0 4
W
11 1 1 0 1 1
490 16 Simulationhas seen at least a 2% increase in value, and will sell at the end of any week that has
seen a decline in stock price, provided that a gain can be realized. Otherwise, the
stock is held until a gain can be made. The third strategy is to invest 50% of the
available money in a stock whenever its price declines, and it will be sold as soon as
a gain can be realized. The manager will not purchase new shares of a stock that is
still held.
Stock prices are thought to follow two overlapping trends. On the one hand, their
value will be determined by the “state of the economy” (measured by the relative
value of the currency, unemployment figures, manufacturers’ receipts, and similar
factors) and, on the other hand, by stock-specific factors. The change of the state of
the economy is denoted by Δ, while the changes of the standings of the three
industries are Δ1, Δ2, and Δ3, respectively. The stock prices are then thought to be
the sum of Δ and Δj for stock j, j ¼ 1, 2, 3. The probabilities of these changes are
denoted by P(Δ), P(Δ1), P(Δ2), and P(Δ3). These probabilities have been observed
and are displayed in Table 16.12 for changes in the overall economy, Table 16.13 for
the specific case of the first industry, and Table 16.14 for the second and third
industry. In each table, double-digit random numbers # are listed that are associated
with the individual changes Δ and Δj. In addition, we will use the random numbers
shown in Table 16.15. They are read row by row from left to right. The task is to
evaluate the different investment strategies of the investment manager.
Solution: We first use the random numbers to simulate the states of the economy
Δ, followed by the simulation of the states of the three different industries Δ1, Δ2,
Table 16.12 Probabilities
P(Δ) and random numbers
#
Δ +2% +1% 0 1% 2%
P(Δ) 0.05 0.15 0.60 0.15 0.05
# 01–05 06–20 21–80 81–95 96–00
Table 16.13 Probabilities
P(Δ1) and random numbers
#
Δ1 +3% +1% 0 1% 3%
P(Δ) 0.03 0.15 0.70 0.10 0.02
# 01–02 03–12 13–82 83–97 98–00
Table 16.14 Probabilities
P(Δ2) and P(Δ3) and ran￾dom numbers #
Δ2, Δ3 +3% +1% 0 1% 4%
P(Δ2), PΔ3) 0.04 0.12 0.60 0.21 0.03
# 01–04 05  16 17–76 77–97 98–00
Table 16.15 Random
numbers
6959915528 3270108890 4539882224 7576293709
1302727211 7576371930 9295108634 6867423785
16.3 Examples of Simulations 491and Δ3. This allows us to compute the stock prices. All of these computations are
shown in Table 16.16.
Strategy 1: Leaving the entire amount of $1,000,000 in the short-term account for
10 weeks will net us $1,001,000.45, or a 0.1% gain.
Strategy 2: In week 1, none of the stocks have increased by at least 2%, so that we
keep our entire amount in the short-term account. By the end of week 2, we have
$1,000,200.01. Since Stock 2 increases by 3% in week 2, we purchase it with half of
the available money, i.e., $500,100. At the price of $61.38 a share, we obtain
8147.6051 shares. The first time we can realize a gain is at the end of week 8, at
which point we sell the shares at $61.99 a share for a total of $505,070.04. This
money is kept in the short-term account for 2 weeks, resulting in a payoff at the end
of week 10 in $505,171.06. The remaining $500,100 that were not invested in week
2 will remain in the short-term account, resulting in $504,114.83 for a total of
$1,009,285.89 or an increase of 0.929%.
Strategy 3: By the end of week 1, Stock 3 has declined in value, which leads the
investor to invest half of the available money in that stock. The $1,000,000 has
appreciated due to its investment in the short-term account for 1 week, so that
1,000,100 are available, half of which ($500,050) are invested in Stock 3. Each
share costs $101.97, so that 4903.8933 shares are purchased. Since the shares will
never exceed that value again during the 10 weeks, they will not be sold.
The remaining $500,050 are left in the short-term account for 2 weeks until the
end of Week 3, when they have appreciated to $500,150.02. As Stocks 1 and
2 decreased in value in Week 3, half of the available amount is invested in each.
(Note, by the way, that Stock 3 decreased in Week 2, but since we still hold shares of
that stock, we do not invest in it again.) The sum of $250,075.01 is invested in Stock
1, which costs $16.66 per share, so that we obtain 15,010.5048 shares. We hold them
until the end of Week 8, when their price increases to $16.83, which gives us
$252,626.80.
Back to the end of Week 3, we invested $250,075.01 in Stock 2 at $60.77 a share,
so that we obtain 4115.1063 shares. We sell these shares at the end of Week 5 for
$61.38 each, resulting in $252,585.22. We hold this money in the short-term account
until the end of Week 8, when the investment in Stock 1 is liquidated. By that time,
we have $505,212.02. Since none of the stocks declined in Week 8, we hold the
amount in the short-term account for a week, resulting in $505,262.54. We are now
at the end of Week 9. During Week 9, we observed all stocks declining. As we still
hold Stock 3, we cannot invest in it, so that we invest the entire remaining money in
Stocks 1 and 2 in equal parts. For the 252,631.27 invested in Stock 1, we obtain
15,163.9418 shares, while for the same amount invested in Stock 2, we obtain
4116.5271 shares of Stock 2.
Since none of the stock prices increase during Week 10, the account by the end of
the planning period consists of 15,163.9418 shares of Stock 1, 4116.5271 shares of
Stock 2, and 4903.8933 shares of Stock 3. The total value of the portfolio is thus
$982,989.70, for a loss of 1.7%.
Comparing the three strategies, it appears that the second investment strategy
is best.
492 16 SimulationTable 16.16 Simulation of stock prices
Week
State of economy Stock 1 Stock 2 Stock 3
#
Δ
#
Δ1 Total Price #
Δ2 Total Price #
Δ3 Total Price
0 17.00 59.00 103.00
1 69
0 45
0 0 17.00 13 +1%
l% 59.59 92
1%
1% 101.97
2 39
0 39
0 0 17.00 02 +3%
3% 61.38 95
1%
1% 100.95
3 91
1% 88 –1%
2% 16.66 72
0 1% 60.77 10 +1%
0 100.95
4 55
0 22
0 0 16.66 72
0 0 60.77 86
1%
1% 99.94
5 28
0 24
0 0 16.66 11 +1%
1% 61.38 34
0 0 99.94
6 32
0 75
0 0 16.66 75
0 0 61.38 68
0 0 99.94
7 70
0 76
0 0 16.66 76
0 0 61.38 67
0 0 99.94
8 10 +1% 29
0 +1% 16.83 37
0 +1% 61.99 42
0 +1% 100.94
9 88
1% 37
0 1% 16.66 19
0 1% 61.37 37
0 1% 99.93
10 90
1% 09 +1%
0 16.66 30
0 1% 60.76 85
1%
2% 97.93
16.3 Examples of Simulations 493References
Budnick FS, McLeavey D, Mojena R (1988) Principles of operations research for management, 2nd
edn. Richard D. Irwin, Homewood, IL
Råde L, Westergren B (2004) Beta: mathematics handbook, 5th edn. Springer, Berlin
494 16 SimulationHeuristic Methods 17
In this book, you will read about or even directly encounter a number of solution
algorithms. All of these algorithms fall into two broad categories: exact algorithms
(sometimes also somewhat misleadingly referred to as optimal algorithms) and
heuristic methods usually simply called heuristics. Exact algorithms have the obvi￾ous advantage of providing the best possible solution there is, given the user-defined
constraints, whereas heuristics do not. Some heuristics do have error bounds, some
actually proven, while others are empirical, i.e., they state that a certain heuristic
usually (typically on average) finds solutions that have a certain quality. On the other
hand, there is computing speed. Some models are such that it takes an exact
algorithm exceedingly long to find the optimal solution. Is this relevant? Well, it
depends. If the task at hand is to, say, locate a landfill for millions of dollars, you will
not care if it takes a laptop 2 or 3 weeks to run, so that it can find a solution that may
potentially save hundreds of thousands of dollars. There are limits to this argument,
of course: if it takes years or even longer to find a solution, most problems have
either solved themselves or have become irrelevant by that time. So, this is not
acceptable.
In order to make the case for heuristics, consider the situation of automated
guided vehicles (AGVs). Suppose you have a number of individual workstations
on the shop floor, each of which processes a given piece from a semi-finished
product to the finished good. For that purpose, it will be necessary to move pieces,
on which work has been finished at one station to the next station. Note that it is not
necessarily the case that all goods are running through the same sequence of jobs, not
all tasks have to be performed at all workstations, and different levels of customiza￾tion are possible. The movement of goods may be accomplished by automated
guided vehicles that receive a message from a workstation whenever a piece is
ready for pickup, and the machine knows where the piece has to go next. However,
temporarily there may be more pieces to be transported than the machine can handle,
so that it will put the tasks in a list. Whenever it is ready, it will work on that list,
depending on where it is at any point in time, how far it is to the destination for that
particular transportation job, how many workstations will be idle if they have to wait
# The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
H. A. Eiselt, C.-L. Sandblom, Operations Research, Springer Texts in Business and
Economics, https://doi.org/10.1007/978-3-030-97162-5_17
495for the next job, and many other considerations. It is apparent that solutions to that
problem have to be found in real time, i.e., immediately. This is where heuristic
algorithms come in.
Heuristic algorithms typically have two phases. The first phase is the construction
phase, in which a solution is established. This phase starts with nothing, and by the
end of the phase, we have a solution. Phase 1 should be followed by Phase 2, which
is an improvement phase, in which the method uses simple modifications of the
present solution that improve the quality of the solution. The best-known heuristics
are the Greedy Method (a construction method) and the Swap (or pairwise exchange)
method, which is an improvement method.
In order to illustrate the Greedy technique, consider the following example.
Suppose that a hiker is lost in the woods. In order to be visible from the air for a
rescue mission, he decides to climb as high as possible. Since he has neither a map,
nor an idea of where he really is (and it is very foggy, so that visibility is very limited,
making his vision myopic), he can only determine the shape of the land in close
vicinity. At any point, he can examine the terrain to his North, Northwest, West,
Southwest, South, Southeast, East, and Northeast. If there are higher points in any of
these directions, he will go into that direction that features the highest nearby point
(i.e., the best possible improvement, hence the name “Greedy”). If the points in all
eight directions are lower than where he is right now, he will conclude that he is
standing on top of a hill and stay there, assuming that he has arrived.
As an example, consider the situation shown in Fig. 17.1. In order to simplify
matters, we have determined the altitudes of the points shown as big black dots and
have them displayed again in Fig. 17.2.
Suppose now that the hiker is presently at the Northeasterly point in Fig. 17.2 at
an altitude of 2160 ft. There are only three surrounding points, those to the West, the
Southwest, and the South, with altitudes of 2210, 2230, and 2050, respectively. The
highest of these is the point to the Southwest, which is then where the hiker walks
to. This point now has eight neighbors, the highest of which is located directly to the
West of the hiker’s present position at an altitude of 2340. Consequently, the hiker
relocates to this point. Repeating the procedure, the hiker notices that all points in the
vicinity of his present location are lower. So, he concludes that he is standing on top
of a hill. From the topographic map, we know that he is, but we also know that this is
not the highest hill in the area of interest.
Formally, a point all of whose neighbors are lower (higher) is called a local
maximum (local minimum). If a local maximum (local minimum) is also the highest
(lowest) point overall, it is referred to as a global maximum (global minimum). As an
illustration, consider the function y ¼ sin x + 0.05x
2
. For values of x between 10
and +10, the function is shown in Fig. 17.3.
The function has four minima on the domain shown, they are at x ¼ 7.0689
(with y ¼ 1.7911), at x ¼ 1.4755 (with y ¼ 0.8879), at x ¼ 4.2711 (with
y ¼ 0.0079), and at x ¼ 9.6788 (with y ¼ 4.4327). (Myopic) heuristics (as well as
derivatives) will readily find local optima, but not necessarily global optima. The
reason for this is apparent: if any myopic method arrives at, say, the rightmost of the
three local minima in Fig. 17.3, how is the method to know whether or not there are
496 17 Heuristic Methodsbetter points located to the left? If a graph such as this were available, it is easy to see,
but typically it is not, as almost all problems are multidimensional, and thus cannot
be graphed in three or fewer dimensions.
Consider now again the possible movements of the hiker from all possible
starting points. Figure 17.4 shows the best move from each of the grid points,
given that, in contrast to our previous discussion, the hiker can move only parallel
to the axes. The figure shows that the hiker will end up at one of three local maxima,
shown in the figure in red. In particular, starting anywhere in the northeast (above the
red line), the hiker will finish at the local maximum with altitude 2340 ft. Starting
anywhere in the middle (the red box that includes only four grid points), the hiker
will terminate his walk at the local maximum at altitude 2460 ft. Only if the hiker
starts his walk in the south or southwest will he end the hike at the global maximum
point with altitude 2550 ft. The areas bordered by the red lines are also referred to as
the catchment areas of the local maxima.
A similar situation applies to the function in Fig. 17.3. Starting a Greedy
minimization method at any point to the left of x ¼ 5.2671 (a local maximum),
we will end up at the local minimum at x ¼ 7.0689. Starting at any point between
x ¼ 5.2671 and x ¼ 1.7463 will end at the global minimum at x ¼ 1.4276, and
starting anywhere to the right of x ¼ 1.7463 will lead to the local minimum at
x ¼ 4.2711.
Fig. 17.1 Map for the lost hiker example # Department of Natural Resources Canada. All rights
reserved
17 Heuristic Methods 497Fig. 17.2 Altitudes in feet at grid points in our example
f(x)
5.5
5
4.5
4
3.5
3
2.5
2
1.5
1
0.5
123 4 567 89
X
−0.5
−9 −8 −7 −6 −5 −4 −3 −2 −1
−1
−1.5
Fig. 17.3 Function with three local minima
498 17 Heuristic MethodsThis immediately suggests a technique that is called a multistart method. The idea
is simply to apply a Greedy technique starting at a number of different points,
compare the results, and choose the best (highest or lowest, depending on whether
a minimum or a maximum is sought).
The hiker’s plight described above might now be considered contrived and of
little general interest, if it were not for the fact that each point on the map may
represent a course of action (determined by the values of its coordinates), and the
contour lines may represent their profit. So, the search for the highest hill has now
become a search for the point of maximal profit, which is of considerable general
interest.
Next, we will discuss an improvement method. The Swap method is easy to
describe. Given a solution that has been obtained “somehow,” it takes two
components and exchanges them. Depending on the specific application, this may
mean exchange their sequence, their inclusion/exclusion status, or whatever the
problem commands. The method then computes the change of the value of the
objective function. If the value has improved (i.e., has increased in case of a
maximization problem or decreased in case of a minimization problem), the
modified solution becomes the new starting point, and the previous solution is
discarded. This step is repeated until no further Swap step can improve the solution
any further.
As an example, consider the map in Fig. 17.5 and assume that the task at hand is
to determine the shortest route from Memphis, Tennessee, to Reno, Nevada. The
traveler has outlined the tour in layers, so that a drive from a city in one layer to a city
in the next is about a day’s drive.
The distances are as follows: From Memphis to Omaha, Wichita, Oklahoma City,
and Dallas, we have 650, 555, 460, and 460 miles, respectively. From Omaha to
Denver and Albuquerque, there are 540 and 860 miles. The distances from Wichita
Fig. 17.4 Movements of hiker
17 Heuristic Methods 499to Denver and Albuquerque are 510 and 585 miles, from Oklahoma City to Denver
and Albuquerque are 630 and 550 miles, and from Dallas to Denver and
Albuquerque are 780 and 640 miles, respectively. The distances from Denver to
Salt Lake City and Phoenix are 505 and 835 miles, while from Albuquerque, there
are 610 and 460 miles to Salt Lake City and Phoenix, respectively. Finally, the
distance from Salt Lake City to Reno is 525 miles and from Phoenix to Reno there
are 735 miles.
In order to obtain some solution, we use the Greedy algorithm, starting at the
origin of our trip in Memphis. From here, Greedy will choose the nearest neighbor,
which is either Dallas or Oklahoma City, both 460 miles away. We arbitrarily choose
Dallas. The nearest neighbor of Dallas is Albuquerque, which is 640 miles away
(as opposed to Denver, which is 780 miles from Dallas). From Albuquerque, we take
the closest connection to Phoenix (460 miles), and from there we have no choice but
take the last long trip to Reno (735 miles). The trip leads us on the route Memphis–
Dallas–Albuquerque–Phoenix–Reno, and its total length is 2295 miles.
At this point, we start to swap. One possibility is to swap Phoenix and Salt Lake
City. This means that we have to add the connections from Albuquerque to Salt Lake
City (610 miles) and from Salt Lake City to Reno (525 miles) and subtract the
connections from Albuquerque to Phoenix (460 miles) and from Phoenix to Reno
(735 miles), for net savings of 60 miles. This is an improvement, and so our new
route is Memphis–Dallas–Albuquerque–Salt Lake City–Reno, and its total length is
2235 miles.
Fig. 17.5 Map of the US for the shortest route example
500 17 Heuristic MethodsWe can now use the new solution and try other Swap moves. For instance, we
could attempt to swap Albuquerque and Denver. The net change of such a swap
move is +35 miles, so we will not make this change. Another possibility is to swap
Dallas and Oklahoma City. The net change is 90 miles, so we make the change and
obtain the route Memphis–Oklahoma City–Albuquerque–Salt Lake City–Reno,
whose length is 2145 miles.
At this time, we may examine again the pair Albuquerque and Denver. This swap
did not improve the solution the last time we tried it, but since then the solution has
changed. In fact, swapping the two cities at this point results in a net change of
+630 + 505  550  610 ¼ 25, so that the change is made. This results in the new
route Memphis–Oklahoma City–Denver–Salt Lake City–Reno, which is
2120 miles long.
We may now try to exchange Oklahoma City and Wichita, which leads to a net
change of +555 + 510  460  630 ¼ 25, for another reduction in terms of the
total distance, which is now 2095 miles. The route leads from Memphis–Oklahoma
City–Denver–Salt Lake City–Reno. At this point, we may try to further reduce the
length of the tour, which is no longer possible with swap moves. As a matter of fact
(unbeknownst to us when we are just using heuristics), the tour is actually optimal.
Our final example of a heuristic method deals with a much-studied field called bin
packing. We have an unspecified number of bins, all of which are of the same
prespecified length. We also have a number of rods that are to be placed into the bins.
The problem is one-dimensional, in that the bins and the rods have the same height
and width, so that only the length of the bins and the rods that are placed into them
will decide whether or not they actually fit. For instance, if the bin is 20 ft and there
are one 6 ft rod, one 3 ft rod, and one 5 ft rod, then these three rods will occupy 14 ft
of the bin and leave 6 ft unoccupied. The task at hand is now to put the existing rods
into the smallest number of bins possible.
Despite its apparent simplicity, the problem has been proven to be very difficult
from a computational point of view. A Greedy-like heuristic is the so-called First Fit
(FF) Algorithm. In order to implement the method, we first assume that a sufficiently
large number of bins is available. These bins are numbered 1, 2, ... . The First Fit
Algorithm can be described as follows:
FF Algorithm: Put the next rod into the bin with the smallest number into which it
will fit.
As an example, suppose that all bins are 19 ft long and we have six 11 ft rods, six
6 ft rods, and twelve 4 ft rods. In this type of situation, we can actually compute a
very simple bound for the number of bins that will be needed. Here, the total length
of the rods is 6(11) + 6(6) + 12(4) ¼ 150 ft. Given that each bin is 19 ft long, we will
need at least 150/19 ffi 7.89 bins. Since the number of bins must be integer, we will
need at least 8 bins. This also means that if we were to find a solution to the problem
that requires 8 bins, this solution must be optimal.
Apply now the First Fit Algorithm. Assigning the rods in order of their lengths
(i.e., the 11 ft rods first, then the 6 ft rods, and finally the 4 ft rods), we notice that
17 Heuristic Methods 501only a single 11 ft rod fits into each bin. This means that we have to put each of the
11 ft rods into one bin each, so that we now have dealt with all 11 ft rods and have
used parts of six bins. Next, we assign the six 6 ft rods. Since each of them fits into
one of the already partially used bins, we now have six bins with one 11 ft and one
6 ft rod each, leaving 2 ft of free space in each of the six bins. This is not sufficient for
any of the remaining 4 ft bins, so that we have to use additional bins. We can place
four 4 ft rods in each bin, leaving an empty (and unusable for us) space of 3 ft each,
which requires another three bins. We now have assigned all rods to the bins. This
solution requires a total of nine bins. There is no apparent pairwise exchange (swap)
step able to improve the solution.
On the other hand, if we were to put one 11 ft and two 4 ft rods into each of six
bins, this would leave no empty space at all. The remaining six 6 ft rods can be put
into two bins. Having again assigned all rods, this solution requires only eight bins
and, given the bound computed above, must be optimal.
While the bin packing example is somewhat artificial, the mathematical structure
has important applications. Among them are employees (the bins with a capacity of
40 hrs a week) and tasks of different lengths (the rods), so that the bin packing
problem is then to minimize the number of employees necessary to perform a given
set of tasks.
A variety of other heuristics exists for this problem. An excellent (albeit difficult)
pertinent reference is the book by Garey and Johnson (1979). An interesting
extension of the problem makes available the different rods over time. This is
reminiscent of ready times in machine scheduling. The solution obtained in such a
case will be no better than the one found in the case in which all rods are available at
the beginning of the process (simply because the problem that makes rods only
available over time is more restrictive than the problem discussed here.).
The efficiency of the heuristics and their performance may be evaluated in
different ways. An obvious evaluation is to use simulation (see Chap. 13 of this
book). This will enable users to specify an average or expected error bound of the
algorithm. For instance, in the above example, the heuristic method uses 9 instead of
the optimal 8 bins, i.e., 12.5% more than optimal.
However, in some cases it is possible to determine theoretical error bounds, i.e.,
bounds that cannot be violated. For instance, it has been shown that the solution
found by the First Fit Heuristic cannot be worse than about 70% higher than the
optimal solution. One problem associated with the theoretical bounds is that while
they represent a reliable, provable property, they tend to be very high.
Many other heuristics have been presented in the literature. Many improvement
algorithms are neighborhood searches, whose main distinguishing feature is that
they start with a given solution and search for better solutions in the neighborhood of
the present solution. The Swap Method described above belongs to this class.
Another very successful heuristic in this class is tabu search. The idea of this method
is to get out of a local optimum by temporarily allowing the current solution to
deteriorate. In order to avoid cycling between solutions that are better and those that
are worse, a list of prohibited moves (a tabu list) is set up that is updated as the
algorithm progresses. This procedure allows to “get over the hump” from the present
502 17 Heuristic Methodspoint to other solutions that are hopefully better than the best solution known at this
point. For example, in Fig. 17.3, if the best-known minimum is x ¼ 7.0689, we
may allow worse solutions (i.e., those with higher functional values) in our move to
the left. This may allow us to find the global minimum at x ¼ 1.4755.
Other techniques are based on observations made in the technical or the natural
world. Examples are simulated annealing, a technique modeled after the way molten
metal cools. Similar to tabu search, it allows moves to solutions worse than the best
presently known solution. Such moves are allowed with a certain probability that
decreases during the course of the algorithm. The formulas ensure that the probabil￾ity to accept a move to a very bad solution is very small. Other methods follow some
behavioral patterns of ant colonies or bees.
Exercise
Problem (Bin packing problem with Greedy and SWAP): A firm has ten tasks
that need to be completed. Their durations are 10, 15, 7, 18, 22, 16, 19, 13, 23, and
11 hrs, respectively. The jobs cannot be split, i.e., the worker who started the job has
to finish it. A large number of potential workers is available, all receive the same
wage. Their weekly capacity is 40 hrs.
(a) Determine lower and upper bounds for the number of workers needed to
perform all tasks.
(b) Use the First Fit Algorithm to allocate the jobs to employees, so that the number
of employees needed to perform all tasks is minimized.
(c) Starting with the solution obtained under (a), use the SWAP technique to
improve the solution.
(d) Repeat (a) and (b), given that the tasks are renumbered, so that tasks with a
longer duration are scheduled first.
Solution
(a) Since the total number of hours required to perform all tasks is 154 and each
worker has a capacity of 40 hrs, we would require a minimum of 4 workers
(sufficient if it were possible to split jobs). This is a lower bound. A simple upper
bound follows the consideration that each worker performs exactly one of the
tasks, which requires ten workers, which provides an upper bound.
(b) Given the order to the tasks shown above, the first three tasks are allocated to
Worker 1 (who is then occupied for 10 + 15 + 7 ¼ 32 hrs), Tasks 4 and 5 are
assigned to Worker 2 (who now works 18 + 22 ¼ 40 hrs), Tasks 6 and 7 are
assigned to Worker 3 (who works 16 + 19 ¼ 35 hrs), Tasks 8 and 9 are assigned
to Worker 4 (who works 13 + 23 ¼ 36 hrs), and Task 10 is assigned to Worker
5, who works only 11 hrs.
(c) and (d) In the SWAP step, we may attempt to reassign jobs, so as to reduce one
worker’s load to no more than 29 hrs (i.e., 11 hrs of idle time), so that he can take
over the light load of Worker 5, who will then no longer be needed. For instance,
Worker 1’s longest task may be exchanged for the shortest task of one of the
17 Heuristic Methods 503other workers, ensuring that the step results in feasible schedule for both
workers. In our demonstration, we will not touch the workload of Worker
2, which is exactly 40 hrs, which cannot be improved upon. First attempt:
Swap jobs between Workers 1 and 3. Since the shortest task of Worker 3 is
longer than the longest task of Worker 1, no exchange can result in a shorter
workload of Worker 1. Second attempt: Workers 1 and 4 swap tasks 2 and 8, so
that Worker 1 now works on Tasks 1, 3, and 8 (for 30 hrs), while Worker 4 now
performs Tasks 2 and 9 for a total of 38 hrs. A further reduction by simple SWAP
steps is not possible. Note that an optimal solution allocates Tasks 2, 3, and 4 to
Worker 1 (for 15 + 7 + 18 ¼ 40 hrs), Tasks 1, 7, and 10 to Worker 2 (for
10 + 19 + 11 ¼ 40 hrs), Tasks 6 and 9 to Worker 3 (for 16 + 23 ¼ 39 hrs), and
Tasks 5 and 8 to Worker 4 (for 22 + 13 ¼ 35 hrs). If the tasks had been
rearranged in the order of Tasks 2, 3, 4, 1, 7, 10, 6, 9, 5, 8, this exact solution
would have been obtained. This suggests a multistart procedure with different
permutations of the sequence of tasks.
Reference
Garey MR, Johnson DS (1979) Computers and intractability: a guide to the theory of
NP-completeness. W.H. Freeman & Company, San Francisco, CA
504 17 Heuristic MethodsAppendices
Appendix A: Vectors and Matrices
This appendix is intended to provide the reader with some basic refresher regarding
some basic operations that involve matrices and vectors and the solution of systems
of simultaneous linear equations. It is not designed to replace a text, but as a mere
quick reference for some material used in this book.
Definition A.1 An [m  n]-dimensional matrix
A ¼ aij   ¼
a11 a12 ... a1n
a21 a22 ⋯ a2n
⋮⋮⋱⋮
am1 am2 ⋯ amn
2
6
6
6
4
3
7
7
7
5
is a two-dimensional array of elements aij arranged in m horizontal rows and
n vertical columns, so that the element aij is positioned in row i and column j. If
m ¼ n, the matrix is said to be square, if m ¼ 1, it is called a row vector, if n ¼ 1, it is
a column vector, and if m ¼ n ¼ 1, it is a scalar.
It is common practice to denote scalars by italicized letters, vectors by lowercase
bold letters, and matrices by capitalized letters in boldface.
Definition A.2 Given an [m  n]-dimensional matrix A and an [n  p]-dimensional
matrix B, the product C ¼ AB is an [m  p]-dimensional matrix C ¼ (cij), such that
cij ¼ ai1b1j þ ai2b2j þ ... þ ainbnj, for i ¼ 1, ... , m and j ¼ 1, ... , n:
Example A.1: With a ¼ 3, ffiffiffi
2
p , 6   and
# The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
H. A. Eiselt, C.-L. Sandblom, Operations Research, Springer Texts in Business and
Economics, https://doi.org/10.1007/978-3-030-97162-5
505B ¼
0 5 π
7 6 2 ffiffiffi
3
p 11 1
2
6
4
3
7
5,
we obtain aB ¼ 7 ffiffiffi
2
p  6 ffiffiffi
3
p , 81  6 ffiffiffi
2
p , 3π þ 2 ffiffiffi
2
p  6  , and
B2 ¼ BB ¼
35 þ π ffiffiffi
3
p 11π  30 10 þ π
42 þ 2 ffiffiffi
3
p 49 10 þ 7π
77 þ ffiffiffi
3
p 55 þ 5 ffiffiffi
3
p 21 þ π ffiffiffi
3
p
2
6
4
3
7
5:
Definition A.3 Given an [m  n]-dimensional matrix A, the transpose AT ¼ aT
ij  
is the [n  m]-dimensional matrix with aT
ij ¼ aji for i ¼ 1, ..., m and j ¼ 1, ..., n.
Example A.2: Use the vector a and the matrix B of Example A.1, we then obtain
aT ¼
3
ffiffiffi
2
p
6
2
6
4
3
7
5
and BT ¼
0 7 ffiffiffi
3
p
5 6 11
π 2 1
2
6
4
3
7
5:
Appendix B: Systems of Simultaneous Linear Equations
Definition B.1 A mathematical relation is written as
LHS R RHS,
where LHS denotes the left-hand side, R is the relation, and RHS is the right-hand
side of the relation.
Typically (but not necessarily), LHS is a function f(x1, x2, ..., xn) of n variables x1,
x2, ..., xn, R is a relation of type <, , ¼ , >, or 6¼, and RHS is a scalar.
Definition B.2 A relation f(x1, x2, ..., xn) R b is said to be linear, if the function f can
be written as f(x1, x2, ..., xn) ¼ a1x1 + a2x2 + ... + anxn. We will refer to f(x1, x2, ...,
xn) as LHS, while b is the RHS.
Example B.1: The relation 2x1  0:7x2 þ ffiffiffiffiffiffi
11 p x3  59 is linear, whereas the
relations 2 ffiffiffiffi
x1
p  0:7x2 þ ffiffiffiffiffi
11 p x3  59 , 2x1  0:7x1x2 þ ffiffiffiffiffi
11 p x3  59 , and 2x1 
0:7x2 þ ffiffiffiffiffi
11 p x3
3  59 are not, due to the appearance of the square root ffiffiffiffi
x1
p , the
product x1x2, and the cubic x3
3, respectively.
We will first deal with the case when the relation R is an equation. Assume now
that we have a system of m linear equations with n unknowns, and that we want to
find a solution, i.e., an assignment of values to the variables, that satisfies all
equations. It is now possible to prove the following
506 AppendicesTheorem B.1 Consider a system of m simultaneous linear equations in n variables:
a11x1 þ a12x2 þ ... þ a1nxn ¼ b1
a21x1 þ a22x2 þ ... þ a2nxn ¼ b2
⋮ ⋮⋮
am1x1 þ am2x2 þ ... þ amnxn ¼ bm
The system has either no solution, exactly one solution, or an infinite number of
solutions.
Example B.2: Consider the system
2x1 þ 3x2 ¼ 7
4x1 þ 6x2 ¼ 10:
It is known that if we simultaneously multiply right-hand side and left-hand side
of an equation, we do not change its content. Multiplying the first equation by 2, we
obtain 4x1 + 6x2 ¼ 14. Now the left-hand side of this equation and that of the second
equation are equal, but its right-hand side differs, indicating that there is an inherent
contradiction in the system. Thus, it is no surprise that the system has no solution.
On the other hand, consider the system
2x1 þ 3x2 ¼ 7
4x1 þ 6x2 ¼ 14:
Multiplying both sides of the first equation by 2 results in the second equation. In
other words, the two relations have exactly the same informational content. This
means that we really have only a single equation, whereas we need one equation to
specify the value of each unknown. Hence this system has an infinite number of
solutions x1 and x2 ¼ 1=3ð Þ 7  2x1 .
Consider now the case that has exactly one solution. Here, we are not concerned
with conditions in which a system has exactly one solution, but our focus is on how
to actually obtain such a solution, given that it exists. There are many different
versions of the Gaussian elimination technique (named after the German mathema￾tician Carl Friedrich Gauss, 1777–1855). In order to illustrate the technique, con￾sider the system of simultaneous linear equations
2x1 þ 3x2  5x3 ¼ 1 ðIÞ
x1  2x2 þ 4x3 ¼ 3 ðIIÞ
4x1 þ x2 þ 6x3 ¼ 2: ðIIIÞ
The idea is to eliminate one variable, say x3, from all equations but one. The
system has then one equation in all (here: three) variables, whereas two equations
include only the remaining variables (here: x1 and x2). Among these remaining
variables, we now choose another variable to be eliminated (here: x2), and the
Appendices 507procedure is repeated. In the end, we have a single equation in just one variable,
which is then replaced by its value in all of the remaining equations, resulting in
another system, in which again one of the equations is just a function of a single
variable, which is replaced by its value everywhere, and so forth, until the system is
solved.
Applying this idea to our example, we first eliminate the variable x3 from Eq. (II)
by multiplying (I) by 4 and multiplying (II) by 5, adding them, and then replacing
Eq. (II) by 4  (I)+5  (II). The revised system can then be written as
2x1 þ 3x2  5x3 ¼ 1 ð ÞI
13x1 þ 2x2 ¼ 11 II0 ð Þ¼ 4  ð Þþ I 5  ð Þ II
4x1 þ x2 þ 6x3 ¼ 2: ð Þ III
Next, we eliminate x3 from Eq. (III) and replace Eq. (III) by 6  (I)+5  (III).
This results in the system
2x1 þ 3x2  5x3 ¼ 1 ð ÞI
13x1 þ 2x2 ¼ 11 II0 ð Þ
32x1 þ 23x2 ¼ 16 III0 ð Þ¼ 6  ð Þþ I 5  ð Þ III
Since the Eqs. (II') and (III') now contain only the two variables x1 and x2, we can
eliminate x2 from Eq. (III') by replacing Eq. (III') by 23  (II')  2  (III'). This
process results in
2x1 þ 3x2  5x3 ¼ 1 ð ÞI
13x1 þ 2x2 ¼ 11 II0 ð Þ
235x1 ¼ 285: III00 ð Þ¼ 23  II0 ð Þ 2  III0 ð Þ
We say that the system is now in triangular form, due to the pattern of coefficients
on the left-hand side. This allows us to obtain the values of the variables in a
recursive procedure. First we can determine the value of x1 from Eq. (III''). Clearly,
x1 ¼  285
235 ¼  57
47. Inserting the value of x1 into Eq. (II') allows us to solve for the
variable x2. In particular, we have 13  57
47   þ 2x2 ¼ 11, which results in x2 ¼ 112
47 .
Finally, inserting the values of x1 and x2 into Eq. (I), we can solve for x3. The relation
reads 2  57
47   þ 3 112
47    5x3 ¼ 1, which results in x3 ¼ 35
47 . The system has now
been completely solved. The solution is ½ ¼  x1, x2, x3
57
47, 112
47 , 35
47   ffi
½  1:2128, 2:3830, 0:7447 . Inserting the values of the unknowns into the original
Eqs. (I), (II), and (III), we can verify that the solution is indeed correct. In analogous
fashion, we can determine the solution of any system of linear equations. For further
details, see any pertinent introductory text on linear algebra or the short summaries in
Eiselt and Sandblom (2007) or (2004).
508 AppendicesAppendix C: Probability and Statistics
While most chapters in this book deal with deterministic models, some include
probabilistic models, in which concepts of probability are needed. This appendix is
intended to briefly cover those probabilistic concepts needed in this book. It is by no
means intended to replace a thorough knowledge of statistics.
Definition C.1 The probability of an event (or outcome) of a random experiment is
a number p between 0 and 1, which measures the likelihood that the event will occur.
A value of p close to 0 indicates that the event is unlikely to happen, while a value
of p close to 1 means that the event is very likely. We can interpret the probability as
the proportion of times that the event or outcome will occur, if the experiment is
repeated a large number of times.
Suppose now that the events are mutually exclusive (i.e., the events are distinct
and do not overlap) and collectively exhaustive (meaning that exactly one of them
will occur). We then obtain the following result
Theorem C.1 If p1, p2, ..., pn denote the probabilities of the mutually exclusive and
collectively exhaustive outcomes of an experiment, then p1 + p2 + ⋯ + pn ¼ 1.
Example C.1: Consider an experiment that involves tossing a fair coin three
times. suppose that each outcome is either head or tail, standing on edge does not
occur. Denote H for “head” and T for tail, the outcome “first tail, then head, then tail”
is written as THT. Then there are eight possible outcomes: HHH, HHT, HTH, HTT,
THH, THT, TTH, and TTT. Given a fair coin, all outcomes are equally likely, and
since p1 + p2 + ⋯ + p8 ¼ 1, we obtain the result that each outcome has a probability
of P HHH ð Þ¼ P HHT ð Þ¼ ⋯ ¼ P TTT ð Þ¼ 1
8.
The probability of a composite event that consists of several outcomes is the sum
of probabilities of the outcome of the event. For instance, the event “obtain exactly
one tail in three flips of a fair coin” refers to the event {HHT, HTH, THH}, so that the
probability of such an event is Pð Þ¼ f g HHT, HTH, THH 1
8 þ 1
8 þ 1
8 ¼ 3
8. The event
space is defined as the set of all possible events of an experiment.
Definition C.2 A random variable X is a function defined on the event space of an
experiment.
Example C.2: Given the above coin tossing experiment, let X denote the number
of heads that come up in the three tosses. There are four possibilities: X ¼ 0 (which
occurs only in the event {TTT}, so that the probability of this event is P Xð Þ ¼ 0 ¼ 1
8
,
X ¼ 1 (which happens if one of the events {HTT, THT, TTH} occurs, so that the
probability P Xð Þ¼ ¼ 1 3
8
), X ¼ 2 (an event that occurs if one of {HHT, HTH, THH}
occurs, so that P Xð Þ¼ ¼ 2 3
8 ), and finally X ¼ 3 (which occurs only if {HHH}
happens, so that P Xð Þ¼ ¼ 3 1
8
). Again, the probabilities of all possible events add up
to 1.
In general, we distinguish between two different types of random variables,
discrete and continuous random variables.
Appendices 509Definition C.3 A random variable X is called discrete, if it can assume only one of
(countably many) values a1, a2, ... . The function P(aj) ¼ P(X ¼ aj) ¼ p(aj) is called
the discrete probability distribution (function) of X. The function F(aj) ¼ P(X  aj)
is called the cumulative probability distribution (function) of X.
Definition C.4 A discrete random variable X with a probability distribution func￾tion p xð Þ¼ P Xð Þ¼ ¼ x n
x
	 
px
ð Þ 1  p nx ¼ n!
x!ð Þ nx ! px
ð Þ 1  p nx
, x ¼ 0, 1, ..., n is
said to be binomially distributed with parameters n (integer) and p, where 0  p 
1. We write X  Bin(n, p).
Example C.3: Let X be the random variable that denotes the number of heads in
three tosses of a fair coin. Then X  Bin(0.5, 3) and Table C.1 displays the
probability function as well as the cumulative distribution functions.
We note that F(aj) is an increasing function of aj that eventually reaches the value
of 1 for the largest value of aj (in case of finitely many outcomes, and that converges
towards 1 for infinitely many outcomes aj. Since P and F are probabilities, we must
have 0  P(aj)  1 and 0  F(aj)  1 for all events aj.
Definition C.5 A discrete random variable X with a probability distribution func￾tion p xð Þ¼ P Xð Þ¼ ¼ x λx
x! eλ , x ¼ 0, 1, 2, ... is called Poisson-distributed with
parameter λ. In case a random variable follows this distribution, we will write X ~
Po(λ), where Po stands for Poisson.
The Poisson distribution is named in honor of Siméon Denis Poisson, a French
mathematician, 1781–1840. This distribution function is of major importance in
queuing (Chap. 15 of this book) and will be extensively used in that context.
Example C.4: If the random variable X ~ Po(λ ¼ 1.3), then P Xð Þ¼ ¼ 2 pð Þ¼ 2
ð Þ 1:3 2
2! e1:3 ¼ 0:2303, and F(X  1) ¼ p(0) + p(1) ¼ 0.6268.
Definition C.6 A random variable X is called continuous, if there exists a function
f(x), such that the cumulative distribution function F(x) for X can be written as
F xð Þ¼ Rt
1
f tð Þdt. The function f(x) is called the (probability) density function of X.
Table C.1 Probability dis￾tribution for the coin toss
example
X PF
0 1
8
1
8
1 3
8
4
8
2 3
8
7
8
3 1
8
8
8
510 AppendicesDefinition C.7 A continuous random variable X with cumulative distribution func￾tion F xð Þ¼ 1  eð Þ λx α
for x > 0, where α and λ are given positive parameters is said
to have a Weibull distribution.
For α ¼ 1, the Weibull distribution reduces to the exponential distribution Exp(λ).
Definition C.8 The gamma function Γ(x) is given by
Γð Þ¼ x
Z 1
0
et
t
x1
dt for x > 0:
One can show that Γ(1) ¼ 1 and xΓ(x) ¼ Γ(x + 1), so that Γ(n) ¼ (n  1)!, n ¼
1, 2, ... and also that Γð Þ¼ ½ ffiffiffi
π p .
Definition C.9 A continuous random variable X with density function f xð Þ¼
λ
Γ αð Þ ð Þ λx α1
eλx for x > 0, where α and λ are given positive parameters, is said to
follow a gamma distribution, and we write X  Γ(α, λ).
For α ¼ n integer, we obtain the so-called Erlang distribution Γ(n, λ), and if
additionally α ¼ 1, we obtain the exponential distribution Exp(λ) ¼ Γ(1, λ).
Definition C.10 A continuous random variable X with density function f xð Þ¼
1
σ ffiffiffiffi
2π p e
ð Þ xμ 2
2σ2 is said to follow a normal distribution (or, alternatively, Gaussian
distribution) with parameters μ and σ > 0. In such a case, we write X ~ N(μ, σ).
The distribution N(0, 1), i.e., the normal distribution with μ ¼ 0 and σ ¼ 1 is called
the standard normal distribution and is usually denoted by Z. Its density function is
called the normal curve (or bell curve), given by the function f xð Þ¼ 1ffiffiffiffi
2π p e½x2
.
Definition C.11 The expected value (or mean or expectation) E(X) or μ of a discrete
random variable X is given by E(X) ¼ μ ¼ a1P(X ¼ a1) + a2P(X ¼ a2) + ⋯, meaning
that we multiply each outcome by its associated probability and add them up. For a
continuous random variable X, the expected value is given by E Xð Þ¼ μ ¼
R
1
1
tf tð Þdt.
There are discrete and continuous random variables for which there exists no
expected value.
Example C.5: For the random variable X of Examples C2 and C3, where
X denotes the number of heads in three tosses of a fair coin, we obtain the expected
value E Xð Þ¼ 0 1
8
  þ 1 3
8
  þ 2 3
8
  þ 3 1
8
  ¼ 1½.
Theorem C.2 If X ~ Bin(n, p), then E(X) ¼ np, if X ~ Po(λ), then E(X) ¼ λ, if X has
a Weibull distribution, then E Xð Þ¼ 1
λ Γ 1 þ 1
α
 , if X ~ Exp(λ), then E Xð Þ¼ 1
λ, if
X~Γ(α, λ), then E Xð Þ¼ α
λ, and if X ~ N(μ, σ), then E(X) ¼ μ.
Appendices 511Definition C.12 The variance V(X) ¼ σ2 of a random variable X with mean μ is
defined as E((X  μ)
2
). We call σ ¼ ffiffiffiffiffiffiffiffiffiffi
V Xð Þ p the standard deviation of X. The
expression σμ ¼ σ/μ is referred to as the coefficient of variation of X.
There are discrete and continuous variables for which the mean exists, but the
variance does not. One can show that V(X) ¼ E(X2
)  μ2 and that V(X)  0 for all
random variables X, as long as E(X 2
), E(X), and V(X) exist.
Example C.6: For the random variable X of Examples C2 and C3, where
X denotes the number of heads in three tosses of a fair coin, we obtain the variance
V(X) ¼ E(X2
)  μ2 ¼ 3  2.25 ¼ 0.75. The standard deviation σ ¼ ffiffiffiffiffiffiffiffiffiffi
V Xð Þ p ¼ ffiffiffiffiffiffi
:75 p 	 :8660. The coefficient of variation σμ ¼ ffiffiffiffiffiffi
:75 p =1:5 	 0:5774.
Theorem C.3 If the random variable X ~ Bin(n, p), then V(X) ¼ np(1–p), if X ~
Po(λ), then V(X) ¼ λ, if X ~ Exp(λ), then V Xð Þ¼ 1
λ2 , if X~Γ(α, λ), then V Xð Þ¼ α
λ2 ,
and if X ~ N(μ, σ), then V(X) ¼ σ2
.
Theorem C.4 If the random variable X~N(μ, σ), then Z ¼ [(X  μ)/σ]~N(0, 1).
Theorem C.5 For any random variable X and for any given numbers a < b, we have
P(a < X  b) ¼ F(b)  F(a). For any continuous random variable X, P(a  X  b) ¼
P(a  X < b) ¼ P(a < X < b) ¼ F(b)  F(a) as well as P(X ¼ a) ¼ P(X ¼ b) ¼ 0.
Example C.7: Let the random variable X ~ N(2.3, 0.9) and compute the probabil￾ity P(1.7  X  2.6). We find that Pð Þ¼ 1:7  X  2:6 P 1:72:3
0:9  X2:3
0:9  2:62:3
0:90  
¼ Pð Þ¼ 0:6667  Z  0:3333 Fð Þ 0:3333 Fð Þ 0:6667 . The function F xð Þ¼
Rx
1
1ffiffiffiffi
2π p e½t
2
dt is called area under the normal curve and it is tabulated at the end
of this Appendix for values x  0. By virtue of symmetry of the normal curve, we
find that F(x) ¼ 1  F(x), therefore F(0.3333)  F(0.6667) ¼ F(0.3333)-
 1 + F(0.6667) ¼ 0.6306 + 1  0.7475 ¼ 0.3781 by reading the table and
interpolating as necessary.
It has been observed in practice that empirical data that are bell-shaped and
symmetric share some common properties. In particular, the empirical rule holds,
according to which about 68% of all observations lie within one standard deviation
about the mean, about 95% of the observations are within two standard deviations
about the mean, and virtually all observations are within three standard deviations
about the mean. It is worth mentioning that this is not a provable property, but an
observed rule that often occurs in practice for distributions of the aforementioned
shape.
Making now stronger assumptions about the distribution, in particular that it is
not only bell-shaped and symmetric but normal, we are able to confirm the assertion
of the empirical rule. In particular, for a standard normal variable Z ~ N(0, 1) we have
F(0) ¼ 0.5, F(1) ¼ 0.8413, F(2) ¼ 0.9773, and F(3) ¼ 0.9987. Then
512 AppendicesP(1  Z  1) ¼ 2(0.8413 – 0.5) ¼ 0.6826, P(2  Z  2) ¼ 2(0.9773 –
0.5) ¼ 0.9546, and P(3  Z  3) ¼ 2(0.9987 – 0.5) ¼ 0.9974.
We will now consider events A, B, C, ..., which are sets of outcomes of
experiments.
Definition C.13 Given the events A and B, the union A [ B is the event consisting of
outcomes that are in A or in B or both. In contrast, the intersection A \ B is the event
that consists of outcomes that are in both A and B.
Theorem C.6 (The addition law for probabilities): For any events A and B, we have
P(A [ B) ¼ P(A) + P(B)  P(A \ B).
Example C.8: Recall Example C4, in which X ~ Po(λ ¼ 1.3). Furthermore, let
A ¼ {X ¼ 0 or 1} and B ¼ {1 or 2}. Then A [ B ¼ {X ¼ 0, 1, or 2}, while
A \ B ¼ {X ¼ 1}. We then find P(A) ¼ p(0) + p(1) ¼ 0.6268, P(B) ¼ p(1) + p(2)-
¼ 0.5846, P(A [ B)p(0) + p(1) + p(2) ¼ 0.8571, P(A \ B) ¼ p(1) ¼ 0.3543. In
accordance with the theorem, we find that
P(A) + P(B)  P(A \ B) ¼ 0.6268 + 0.5846  0.8571 ¼ P(A [ B).
Definition C.14 The sets A and B are said to be collectively exhaustive, if their
union A [ B includes all possible outcomes of an experiment. The two sets are called
mutually exclusive, if their intersection is empty. The complement A of a set
A (sometimes also written as ØA) is the set of all possible outcomes not in A.
As far as probabilities are concerned, P A
  ¼ 1  P Að Þ.
The addition law for probabilities can be generalized. For instance, for mutually
exclusive sets A1, A2, ... Am, we obtain P(A1 [ A2 [ ... [ Am) ¼ P(A1) +
P(A2) + ... + P(Am). To establish a multiplication law for probabilities, we need
the following
Definition C.15 For any events A and B with P(B) 6¼ 0, the conditional probability
of A given B is
P Að Þ¼ jB P Að Þ \ B
P Bð Þ :
Example C.9: Consider an experiment, in which the random variable X denotes
that at most two heads come up in three tosses of a fair coin, i.e., X  2, and define
B as an event that sees at least one head in three tosses of a fair coin, i.e., X  1. Then
A \ B ¼ {X ¼ 1 or 2}, so that P Að Þ¼ \ B 3
8 þ 3
8 ¼ 3
4 and P Bð Þ¼ 3
8 þ 3
8 þ 1
4 ¼ 7
8 .
Therefore, P Að Þ¼ jB 3=4
7=8 ¼ 6
7 	 0:8571.
Theorem C.7 (The multiplication law for probabilities): For any events A and
B, P(A \ B) ¼ P(A| B)P(B).
Note that if P(B) ¼ 0, then P(A|B) is not defined, but in this case, the right-hand
side is interpreted as being zero.
Appendices 513Definition C.16 The events A and B are said to be statistically independent, if
P(A \ B) ¼ P(A)P(B).
Theorem C.8 (Bayes’s theorem): Let the events A1, A2, ...Am be mutually exclu￾sive and collectively exhaustive. Then for any event B with P(B) 6¼ 0, we have
P Ai ð Þ¼ jB P Bð Þ jAi P Að Þi
P Bð Þ jA1 P Að Þþ1 P Bð Þ jA2 P Að Þþ2 ...þP Bð Þ jAm P Að Þ m , i ¼ 1, ... , m
The theorem of Bayes (Thomas Bayes, English clergyman, 1702–1761) will be
used in Chap. 10 of this book. In the context of Bayes’s theorem, the unconditional
probabilities P(Ai) are called prior probabilities, while the conditional probabilities
P(Ai|B) are referred to as posterior probabilities.
Area Under the Normal Curve
x
x 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09
.00 .5000 .5040 .5080 .5120 .5160 .5199 .5239 .5279 .5319 .5359
.10 .5398 .5438 .5478 .5517 .5557 .5596 .5636 .5675 .5714 .5754
.20 .5793 .5832 .5871 .5910 .5948 .5987 .6026 .6064 .6103 .6141
.30 .6179 .6217 .6255 .6293 .6331 .6368 .6406 .6443 .6480 .6517
.40 .6554 .6591 .6628 .6664 .6700 .6736 .6772 .6808 .6844 .6879
.50 .6915 .6950 .6985 .7019 .7054 .7088 .7123 .7157 .7190 .7224
.60 .7258 .7291 .7324 .7357 .7389 .7422 .7454 .7486 .7518 .7549
.70 .7580 .7612 .7642 .7673 .7704 .7734 .7764 .7794 .7823 .7852
.80 .7881 .7910 .7939 .7967 .7996 .8023 .8051 .8079 .8106 .8133
.90 .8159 .8186 .8212 .8238 .8264 .8289 .8315 .8340 .8365 .8389
1.0 .8413 .8438 .8461 .8485 .8508 .8531 .8554 .8577 .8599 .8621
1.1 .8643 .8665 .8686 .8708 .8729 .8749 .8770 .8790 .8810 .8830
1.2 .8849 .8869 .8888 .8907 .8925 .8944 .8962 .8980 .8997 .9015
1.3 .9032 .9049 .9066 .9082 .9099 .9115 .9131 .9147 .9162 .9177
1.4 .9192 .9207 .9222 .9236 .9251 .9265 .9279 .9292 .9306 .9319
1.5 .9332 .9345 .9357 .9370 .9382 .9394 .9406 .9418 .9430 .9441
1.6 .9452 .9463 .9474 .9485 .9495 .9505 .9515 .9525 .9535 .9545
1.7 .9554 .9564 .9573 .9582 .9591 .9599 .9608 .9616 .9625 .9633
1.8 .9641 .9649 .9656 .9664 .9671 .9678 .9686 .9693 .9700 .9706
1.9 .9713 .9719 .9726 .9732 .9738 .9744 .9750 .9756 .9762 .9767
2.0 .9773 .9778 .9783 .9788 .9793 .9798 .9803 .9808 .9812 .9817
2.1 .9821 .9826 .9830 .9834 .9838 .9842 .9846 .9850 .9854 .9857
2.2 .9861 .9865 .9868 .9871 .9875 .9878 .9881 .9884 .9887 .9890
2.3 .9893 .9896 .9898 .9901 .9904 .9906 .9909 .9911 .9913 .9916
2.4 .9918 .9920 .9922 .9925 .9927 .9929 .9931 .9932 .9934 .9936
(continued)
514 Appendices(continued)
2.5 .9938 .9940 .9941 .9943 .9945 .9946 .9948 .9949 .9951 .9952
2.6 .9953 .9955 .9956 .9957 .9959 .9960 .9961 .9962 .9963 .9964
2.7 .9965 .9966 .9967 .9968 .9969 .9970 .9971 .9972 .9973 .9974
2.8 .9974 .9975 .9976 .9977 .9977 .9978 .9978 .9980 .9980 .9981
2.9 .9981 .9982 .9983 .9983 .9984 .9984 .9985 .9985 .9986 .9986
3.0 .9987 .9987 .9987 .9988 .9988 .9989 .9989 .9989 .9990 .9990
3.5 .999767
4.0 .9999683
4.5 .99999660
5.0 .999999713
References
Eiselt HA, Sandblom C-L (2004) Decision analysis, location models, and scheduling problems.
Springer, Berlin
Eiselt HA, Sandblom C-L (2007) Linear programming and its applications. Springer, Berlin
Appendices 515Index
A
ABC classification, 401, 421
Activity
critical, 304–318, 321, 324–327, 330
duration, 311–314, 317, 320, 321, 325–327
Activity-on-arc (AOA) representation, 305
Activity-on-node (AON) representation, 305
Adjacency matrix, 216
AHP, see Analytic Hierarchy Process (AHP)
Algorithms, exact and heuristic, 172
All-integer linear programming (AILP),
162–166
Allocation problem, 28–34, 333
Analytic hierarchy process, 391–399
Anticipated payoffs, 359, 360, 376, 380
AOA representation, see Activity-on-arc
(AOA) representation
AON representation, see Activity-on-node
(AON) representation
Arc, 215–217, 221–223, 225–234, 239–250,
255, 256, 262, 263, 278, 286, 305, 356,
427
Arrival rate, 460–463, 465, 470, 471
Arrival time, 334, 481
Aspiration level, 134
Assignment problems, 15, 38–45, 67, 197–199,
211–213, 217, 250, 261, 293
B
Backorders, 149, 402, 404, 410–412, 417, 484
Backward pass, sweep, or recursion, 224, 226,
306–309, 311, 313, 321, 325, 372
Basis point, 99
Bayes’s rule, 360, 367, 369, 372, 388
Bayes’s theorem, 369, 514
Bin packing, 179, 501–503
Bisection search, 280, 282, 299
Blending problems, 34–38
Block diagram, 440, 441, 445
Boolean truth method, 444
Bottleneck, 28, 103, 107, 111, 227, 304, 309,
312
Branch-and-bound methods, 186, 188–194
Branching, 189, 191–193, 206, 207, 210
Breadth-first-search, 224
Break-even analysis, 9
Breakthrough, 161, 223–226, 254
Bridge (of a graph), 2, 215, 240, 242
Budget constraint, 4, 19, 31, 82, 133–135, 199,
203
C
Calling population, 458, 459
Capital budgeting, 171, 353
Catchment areas, 497
Center, absolute
node, 278, 279
Center-of-gravity, 284, 285, 300
Center problems, 269, 277–281
Centrality, closeness
betweenness, 218, 220
degree, 218
eigenvalue, 220
Certainty equivalent, 373, 374
Changes
objective function coefficients, 94, 95, 97,
100, 108, 113
right-hand side values, 82, 94, 97, 98, 100,
101, 103, 104, 108, 164
structural and parameter, 91, 93, 94
Channel, 458, 462, 469, 470, 480
Chinese postman problem, 240–242, 246–248,
259, 262
Circuit, 216, 241–244, 451, 457
# The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
H. A. Eiselt, C.-L. Sandblom, Operations Research, Springer Texts in Business and
Economics, https://doi.org/10.1007/978-3-030-97162-5
517Coefficient of variation, 394, 395, 398, 399,
463, 512
Column
dominated, 359
Competitive location, 292
Complementary slackness conditions, 119, 122
Completion time, 322, 334, 335, 344, 346
Congruence methods, 477
Conservation equations, 222, 228, 229, 231,
256
Consistency index, 394
Constraint method, 131, 133, 134, 140
Constraints
addition and deletion of, 91
binding, 64, 73, 74, 78, 86–88, 93, 100, 101,
103–105, 123, 176, 177
complicating, 198, 199, 211–213
conditional, 168, 172, 174, 182, 200, 203
exclusion, 169, 173, 175
linking, 173, 174, 176
Construction phase, 496
Contour lines, 75, 84, 499
Convex hull, 183, 184
Convexity, 79
Corner point
theorem, 77, 80, 84, 163, 165, 183, 286
Cost, holding/carrying
ordering, 149, 403–407, 410, 412, 415,
418–420, 424, 485, 487
penalty, 375, 411, 417–420, 424
purchasing, 54, 404–406, 412
shortage, 404, 410, 411, 417, 421–423,
483–485, 487
Covering matrix, 271, 273, 294, 296
Covering problems, 270–277, 280, 281, 293
CPM, see Critical path method (CPM)
Crashing, 312
Crash time, 312, 314, 316, 317, 325, 326
Critical activity, 308–311
Critical path, 303–317, 321, 322, 324–328, 330,
332
Critical path method (CPM)
project acceleration, 312–317
Cut, minimal, 227, 253, 254
Cutting plane method, 183–188, 204
Cutting stock problems, 52–70
Cycle length, 404, 406–408, 477
D
Dantzig cut, 184–186
Decision analysis, 353–383, 385, 388, 390,
425
Decision trees, 356, 358, 366–372, 375, 377,
379–383
Decomposition principle, 8
Degeneracy
dual, 84–85, 88, 89, 185, 188
primal, 85, 86, 89, 90, 99, 104, 185, 205
Degree (of a node), 218, 241, 245
Depth-first search, 224
Destination, 38–43, 113–115, 217, 220, 229,
230, 292, 495
Diet problem, 22–28, 109, 110, 125, 172–174,
176
Dijkstra method, 234, 256
Discrete event simulation, 473
Distribution
binomial, 453, 510
Erlang, 449, 511
exponential, 446, 449, 459, 479, 511
gamma, 449, 452, 511
normal, 322, 479, 511
Poisson, 459, 461–463, 478–479, 510
Rayleigh, 449
Weibull, 440, 449, 511
Divisibility, 16, 17, 143
Dominance, 357–359, 380, 382
Duality, 114–123, 152, 166, 167, 197–199,
211, 212
Dual problem
relations to primal problem, 114–116, 119,
122
setting up, 121
Dual variables, 114, 117–119, 143, 166, 168,
198, 199, 211–213
Due time, 334
Dynamic programming, 3, 223
E
Earliest due date (EDD) algorithm, 337, 349
Earliest possible finishing times (EF), 306, 307,
309, 311, 330
Earliest possible starting times (ES), 306–309,
311, 317
Economic order quantity (EOQ), 149, 404–416,
421, 422, 424
EDD algorithm, see Earliest due date (EDD)
algorithm
Edge, 140, 215–219, 222, 238–246, 248, 261,
278, 280, 286, 509
EF, see Earliest possible finishing times (EF)
Efficiency, 38, 125, 180, 291, 292, 371, 380,
382, 383, 502
Either-or constraints, 168
518 IndexEmpirical rule, 321, 512
Employee scheduling, 49–52
EMV, see Expected monetary values (EMV)
EOQ, see Economic order quantity (EOQ)
EPII, see Expected payoff with imperfect
information (EPII)
EPPI, see Expected payoff with perfect
information (EPPI)
Error bound, 495, 502
ES, see Earliest possible starting times (ES)
Euclidean distance, 268, 279, 283–285, 300,
390, 396
Euler graph, 241
EVPI, see Expected value of perfect
information (EVPI)
EVSI, see Expected value of imperfect
(or sample) information (EVSI)
Excess variable, 90, 102–104, 106, 110, 111,
114, 117, 119, 122, 134, 185, 186, 205,
309, 332
Expected monetary values (EMV), 360,
363–367, 371, 372, 378, 379
Expected payoff with imperfect information
(EPII), 371, 372
Expected payoff with perfect information
(EPPI)
“equity” objectives, 291
Expected value, definition, 371
Expected value of imperfect (or sample)
information (EVSI), 371, 372, 380, 383
Expected value of perfect information (EVPI),
366, 367, 371, 380, 382
Extreme points
number of, 125
F
Facilities, extensive
undesirable, 269, 290
Failure rate, decreasing, increasing, 449
Feasibility, 6–9, 79, 82, 130, 197, 463, 464, 466
Feasible
set, 28, 72, 73, 76–81, 84–86, 88, 89, 94,
98–100, 120, 130, 132, 133, 138, 140,
150, 154, 155, 159, 162–165, 167,
183–186, 188–191, 291
solution, nonexistence of, 80, 81, 100
Feasible direction methods, 79
First come, first served (FCFS), 459
First fit (FF) algorithm, 501
First passage time, 428, 429
Fixed charges, 175–178
Fleury’s algorithm, 242, 244, 246, 248
Flow, pattern and value
balancing equations, 222
mean, 335, 336, 344, 347–350
time, 54, 334–336
Floyd–Warshall method, 235, 236, 256, 258
Forward pass, sweep, or recursion, 306
Frontier, nondominated, 130, 131, 138, 139, 141
G
Games against nature, 354
Gantt chart, 2, 317, 318, 320, 326–328, 330,
331, 337, 338, 340, 341, 346, 348–352
Gaussian elimination, 507
GERT, see Graphical evaluation and review
technique (GERT)
Gini index, 291
Global optimum, 496
Goal programming, 126, 133–142, 181
Graph, augmented
bipartite, 217
connected, 216
directed, undirected, and mixed, 215
Graphical review and evaluation technique
(GERT), 320
Graphical solution method, 18, 70–80, 83, 84,
86–88, 95, 96, 138
Greedy method, 194, 210, 289, 496
Guillotine cuts, 56
H
Halfplane, 72, 128
Halfspace, 71, 72, 81, 98, 127
Hazard function, 439, 440, 447–450, 454
Hazard rate, 448, 449, 455
Heuristic methods, 6, 168, 183, 194–213, 249,
252, 289, 318, 335, 340, 341, 495–504
Hub location, 292
100% rule, 97, 100, 101
Hyperplane, 71, 72, 74–76, 85, 86, 90, 98, 99
I
Implementability, 7
Improvement cone, 128, 130, 137, 138, 140,
141
Improvement phase, 496
Incremental technique, 79
Indegree, 241, 244, 247, 248, 262, 264
Influence diagrams, 355–357, 375, 376
Institute for Operations Research and
Management Science (INFORMS), 1
Index 519Integer programming, 2, 17, 161–163, 165,
166, 170–179, 183–213, 217, 268, 293,
311, 319, 338, 339, 353
Integrality gap, absolute and relative, 166
Interarrival time, 460, 461, 463, 480–483
Interchange heuristic, 195
Interval constraints, 82
Inventory models, 43, 45–49, 59, 148,
401–424
Investment allocation, 30, 32
Iso-profit lines, 75–79, 127
J
Jackson’s job shop algorithm, 347, 348
Jackson’s rule, 337
Johnson’s algorithm, 347
Just-in-time (JIT) inventory system, 402
K
Kanban systems, 402
Kendall’s notation, 458
Kingman equation, 463
Kirchhoff node equations, 222
Knapsack problems, 122, 170–172, 194, 199
Königsberg bridge problem, 2, 215, 239, 240
K-out-of-n systems, 443
Kruskal’s method, 239
L
Label correcting methods, 233
Labeling methods, 222
Label setting methods, 233
Lagrangean multipliers, 151, 167, 168, 197,
213
Lagrangean relaxation, 166, 167, 197–199, 211
Land use problem, 174, 175
LAPT algorithm, see Longest alternate
processing time (LAPT) algorithm
Last in, first out (LIFO), 459
Lateness, 334, 335, 337, 338, 345, 346, 348,
349
Latest allowable finish time (LF), 307–309, 311
Latest allowable start time (LS), 307–309
Layout problems, 293
Lead time
stochastic, 417–420
LF, see Latest allowable finish time (LF)
Linear congruence methods, 477
Linearity, 16, 17, 39
Linear programming
relaxation, 165, 166, 171, 183–189, 191,
192, 204
List scheduling methods, 340
Little’s formula, 461, 464
Local optimum, 502
Location
continuous, 268
discrete, 268
Location-allocation heuristic, 288, 301
Location models, 230, 235, 267–302
Location set covering problem (LSCP), 271,
272
Logical variables, 161, 168, 171, 173
Longest alternate processing time (LAPT)
algorithm, 345, 352
Longest processing time first (LPT) algorithm,
340–342, 348–350
Lorenz curve, 291
LPT algorithm, see Longest processing time
first (LPT) algorithm
LS, see Latest allowable start time (LS)
LSCP, see Location set covering problem
(LSCP)
M
Machine scheduling
single, parallel, dedicate, 334, 336–352
Makespan, 335, 336, 340–342, 345–347, 349,
350
Management science, vii, 1, 3, 4
Manhattan distance, 268, 279, 282, 298, 299,
390, 391
Markov chains, 425–437
Markovian property, 426
Material requirements planning (MRP), 402
Matrix, 39, 40, 43, 65, 67, 211, 212, 216, 219,
220, 235–238, 245, 247, 251, 252, 258,
259, 264, 265, 271, 273, 276, 278,
286–289, 294–297, 301, 355, 357,
359–361, 363, 366, 368, 375, 376, 379,
380, 382, 385, 387–389, 391–396, 398,
399, 426–429, 431, 432, 434, 435, 437,
505, 506
Maximal covering location problem (MCLP),
274, 275
Maximal flow problem, 222, 227, 230
Maximal lateness, 335, 337, 338, 345, 346, 348
Maximax rule, 359
Maximin rule, 359
MCLP, see Maximal covering location problem
(MCLP)
McNaughton’s algorithm, 342
520 IndexMean recurrence time, 428, 431, 434, 436
Mean time between failures (MTBF), 447
Mean time to failure (MTTF), 447, 449, 451
Median problems, 269, 281–290, 299
Midsquare method, 476, 477
Min cost feasible flow problem, 227, 228, 231
Min-cut max flow theorem, 253
Minimax, 359
Mixed-integer linear programming (MILP), 162
Model, deterministic and stochastic, 473
Modeling process, 1, 10–13, 28, 91, 92, 136
MOdified DIstribution (MODI) method, 40
Multicriteria decision making (MCDM)
problems, 126
Multiobjective programming, 125–142
Multistart procedure, 252, 504
N
Nash equilibria, 293
Nearest neighbor method, 251, 252, 500
Neighborhood search, 502
Network
flows, 215, 220–230
Newspaper boy problem, 360
NIMBY, 269
Nodes
active (in branch-and-bound), 188
reachable, 216
Nonbreakthrough, 223, 226, 254
Nondominated frontier, 130, 131, 138, 139, 141
Normal duration, 312, 324, 325
Normalization technique, 392, 398
O
Objective
function, gradient of, 75–77, 83, 84, 89,
94–96, 98, 127, 138
Operational life testing, 451
Operations research
definition, 1
elements, 4–10
journals, 3
Opportunity costs, 101, 102, 104, 110,
117–119, 360, 361
Optimality, 3, 6, 7, 9, 41, 125, 126, 149–151,
166, 204, 284, 341, 344, 352, 366, 368
Optimality gap, 166
Optimal solution
alternative, 84, 88, 89, 92, 96, 104, 206,
248, 251, 263, 349
economic analysis of, 22
unbounded, 83–84, 92–95, 104, 119, 167
unique, 78, 95, 96, 122, 149, 155–159, 198,
211–213, 261
Optimist’s rule, 359
Order point (s, S) policy, 420
Origin, 38–43, 72, 75, 76, 84, 99, 113–115,
147, 217, 220, 229, 267, 292, 500
Outdegree, 241, 244, 247, 248, 262, 264
Overshipments, 42
P
Page rank, 220
Pairwise comparisons, 358, 391, 392
Pairwise exchange, 196, 252, 265, 496
Parallel-series system, 441
Parallel structure, 441
Parameters, 5–7, 9, 16–18, 24, 39, 46, 59, 90,
91, 93, 94, 144, 149, 199, 268, 269, 274,
333, 334, 401–404, 406, 407, 410, 411,
414, 416, 417, 446, 448, 449, 451, 452,
454, 459, 461, 463, 466, 476–479, 483,
485, 510, 511
Path, 79, 84, 191, 215–220, 224–226, 230–238,
240, 244–248, 253, 254, 256–264, 268,
271, 280, 286, 303–317, 321, 322,
324–328, 330, 332, 427, 428, 430, 441,
442, 445
Payoff table, 380
Penalty term, 152, 198
Performance ratio, 341, 342, 348, 350
Periodic review (R, S) policy, 421
PERT, see Program evaluation and review
technique (PERT)
Pessimistic rule, 359
Phase, construction and improvement, 496
Poisson process, 459
Political districting, 181–183
Pollaczek–Khintchine formula, 462
Polytope, 73
Postoptimality analyses, 90–114, 119
Precedence relations, 304–306
Preemption, 336, 342, 344, 350
Probabilistic, 16, 269, 304, 320, 333, 439, 509
Probability
conditional, 367, 369, 372, 379, 381, 426,
429, 448, 477, 513, 514
definition, 16
distribution, 304, 320, 354, 355, 366, 417,
419, 420, 429, 440, 510
indicator, 368, 369, 371, 372
posterior, 369, 371, 372, 514
prior, 369, 372, 380, 514
Index 521Processing time, 5, 20–22, 93, 176, 179, 334,
336–351
Process, stationary, 427
Production-inventory models, 43, 45–49, 59
Production-lot size model, 414–417
Production planning, 6, 9, 20–22, 56, 60, 105,
106
Program evaluation and review technique
(PERT), 303, 304, 320–332
Project networks, 303–332, 335, 340
Q
Quantity discounts, 404, 412–414, 423
R
Random numbers, true and pseudo, 475–477
Random variables, 320, 417, 418, 426, 445,
447, 448, 478, 479, 483, 509–513
Reachable (in a graph), 216
Ready time, 334, 345, 349, 502
Rectilinear distance, 282, 284
Reduced costs, 102
Redundancy, 85, 86, 440, 450–451
Regret criterion, 359, 375, 382
Regret matrix, 359, 360, 376
Regularity condition, 415, 416, 422, 423, 460
Release time, 334, 336, 337
Reliability testing, 451
Reorder point, 409, 417–424, 483–486
Reshipments, 42
Resource requirement graph, 317–320, 327,
328, 330, 331
Resources, scarce, 1, 28, 30–33, 58, 277, 304
Review systems, continuous and periodic, 420
Risk
aversion, 4, 373, 374
neutral, 373, 375
seeking, 373
Robust optimization, 359
Root (of a tree), 189, 192, 193, 207, 210, 367,
368
Routing, arc and node, 239–266
Row, dominated, 359
S
Satisficing, 134
Schedule length, 335, 340–343, 345–348, 350,
352
Seed, 476–478
Selection problem, 353–355
Sensitivity analyses
graphical, 90–101
Series-parallel system, 443
Service
rate, 149, 460–463, 468, 469, 471
station, 149, 458–461, 463, 464, 466, 468,
480, 482
time, 337, 459, 461–463, 466, 470, 480–483
Service in random order (SIRO), 459
Shadow prices, 102–104, 106, 108, 110–114,
117, 120, 332
Shop, open, flow, job, 334
Shortest path problems, 230–238
Shortest processing time (SPT) algorithm, 336,
349
Simplex method, 3, 15, 16, 79, 84, 86, 131
Simulated annealing, 503
Simulation, deterministic and stochastic, 473
Simultaneous linear equations, 73, 74, 77–80,
152, 430, 505–508
Sink, 220, 222–231, 234, 255, 264, 305–307,
311, 315, 316
Smith’s ratio rule, 336
Solution
definition, 6
efficient, 130, 131
nondominated, 130–133, 140
noninferior, 130, 141
pareto-optimal, 126, 130
tree, 188–194, 206–208, 210, 433
Source, 10, 220, 222–231, 233–235, 254, 255,
264, 305–308, 311, 315, 316, 475
Spanning tree
minimal, 239, 240, 259
Spider plot, 288, 289
SPT algorithm, see Shortest processing time
(SPT) algorithm
Standard deviation, 321, 322, 324, 326, 327,
394, 450, 453, 462, 463, 512
Standby systems, 450
State, absorbing
steady, 460
transient, 460
State space, 425
Statistical independence, 514
Steady state solution, 430–431, 474
Stochastic processes, 425–437, 439, 445
Subtour elimination constraints, 250, 251
Supply chain management, 402
Swap heuristic, 195
System, k-out-of-n
lifetime, 445, 446
parallel, 443
522 Indexparallel-series, 442
series, 443
series-parallel, 443
standby, 450
T
Tabu search, 502, 503
Tardiness, 334, 335, 338, 339, 347
Target value, 134, 361–363, 375, 377
Testing
(highly) accelerated life, 451
operational life, 451
reliability, 451
Time-state graph, 427
Time-to-failure (TTF), 445, 447
Tool crib, 466–468
TOPSIS, 390–391, 396
Total float, 308, 309, 311, 326, 327
Tradeoffs, 125, 126, 135, 277
Traffic intensity, 460, 483
Transient states, 460, 474
Transition probabilities, 426–428, 431, 435
Transportation problem, 38, 39, 41, 43, 44, 49,
65, 113, 183, 248, 263, 264
(un-)balanced, 39
Traveling salesman problem, 45, 241, 249, 264
Tree (graph), 216–217
Truncation, 451
Two-bin system, 409, 420
U
Uncertainty, 354, 355, 357–364, 375, 417
Undesirable facilities, 269, 290, 291
Utility theory, 353, 373–383
Utilization rate, see Traffic intensity
V
Variables
addition and deletion of, 91
deviational, 134–136, 181
slack, 19, 102, 103, 119, 122, 134, 186, 188
surplus, 102
Variance, 180, 291, 321, 322, 326, 327, 394,
459, 462, 512
Varignon frame, 285, 286
Vector, 75, 126–133, 137, 139, 140, 276,
286–289, 297, 301, 359, 392, 394, 398,
429, 430, 432, 505–506
Vector optimization, 126–133, 137, 139, 140
Vertex (graph), 81, 130, 290, 297, 299–301
Vertex substitution method, 300–301
von Stackelberg solutions, 293
W
Wald’s rule, 359, 360, 374
Weighted shortest processing time (WSPT)
algorithm, 336, 337
Weighting method, 131–133, 135, 140
Weiszfeld method, 284, 300
Workload balancing, 179–181
Wrap-Around Rule, 342, 343, 350, 351
WSPT algorithm, see Weighted shortest
processing time (WSPT) algorithm
Index 523
