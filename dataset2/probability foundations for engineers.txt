This textbook will continue to be the best suitable textbook written specifically for a 
first course on probability theory and designed for industrial engineering and oper￾ations management students. The book offers theory in an accessible manner and 
includes numerous practical examples based on engineering applications.
Probability Foundations for Engineers, Second Edition, continues to focus spe￾cifically on probability rather than probability and statistics. It offers a conversational 
presentation rather than a theorem or proof and includes examples based on engi￾neering applications as it highlights Excel computations. This new edition presents a 
review of set theory and updates all descriptions, such as events versus outcomes, so 
that they are more understandable. Additional new material includes distributions 
such as beta and lognormal, a section on counting principles for defining probabili￾ties, a section on mixture distributions and a pair of distribution summary tables.
Intended for undergraduate engineering students, this new edition textbook offers 
a foundational knowledge of probability. It is also useful to engineers already in the 
field who want to learn more about probability concepts. An updated solutions man￾ual is available for qualified textbook adoptions.
Probability Foundations 
for EngineersProbability 
Foundations for 
Engineers
Second Edition
Joel A. NachlasSecond edition published 2023
by CRC Press
6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742
and by CRC Press
4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN
CRC Press is an imprint of Taylor & Francis Group, LLC
© 2023 Joel A. Nachlas
First edition published by CRC Press 2012
Reasonable efforts have been made to publish reliable data and information, but the author and publisher 
cannot assume responsibility for the validity of all materials or the consequences of their use. The authors 
and publishers have attempted to trace the copyright holders of all material reproduced in this publication 
and apologize to copyright holders if permission to publish in this form has not been obtained. If any 
copyright material has not been acknowledged please write and let us know so we may rectify in any 
future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, 
transmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter 
invented, including photocopying, microfilming, and recording, or in any information storage or retrieval 
system, without written permission from the publishers.
For permission to photocopy or use material electronically from this work, access www.copyright.com
or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 01923, 
978-750-8400. For works that are not available on CCC please contact mpkbookspermissions@tandf.
co.uk
Trademark notice: Product or corporate names may be trademarks or registered trademarks and are used 
only for identification and explanation without intent to infringe.
Library of Congress Cataloging‑in‑Publication Data
Names: Nachlas, Joel A., author. 
Title: Probability foundations for engineers / Joel A. Nachlas. 
Description: Second edition. | Boca Raton : CRC Press, 2023. | Includes index. 
Identifiers: LCCN 2022047193 (print) | LCCN 2022047194 (ebook) | ISBN 
9781032278483 (hbk) | ISBN 9781032278506 (pbk) | ISBN 9781003294382 (ebk) 
Subjects: LCSH: Engineering--Statistical methods. | Probabilities. | 
BISAC: BUSINESS & ECONOMICS / Operations Research. | MATHEMATICS / Probability 
& Statistics / Bayesian Analysis. | TECHNOLOGY & ENGINEERING / Operations Research. 
Classification: LCC TA340 .N28 2023 (print) | LCC TA340 (ebook) | DDC 
519.2--dc23/eng/20221006 
LC record available at https://lccn.loc.gov/2022047193
LC ebook record available at https://lccn.loc.gov/2022047194
ISBN: 978-1-032-27848-3 (hbk)
ISBN: 978-1-032-27850-6 (pbk)
ISBN: 978-1-003-29438-2 (ebk)
DOI: 10.1201/9781003294382
Typeset in Times
by SPi Technologies India Pvt Ltd (Straive)Dedication
Dedicated to the memory of
Dr. Marvin M. Nachlas,
a talented scientist, a sensitive physician 
and a loving father.vii
Contents
Preface to the Second Edition...................................................................................xi
Author .....................................................................................................................xiii
Chapter 1 Introduction ..........................................................................................1
1.1 Historical Perspectives...............................................................1
1.2 Formal Systems.........................................................................2
1.3 Intuition .....................................................................................3
Exercises...............................................................................................3
Chapter 2 A Brief Review of Set Theory..............................................................5
2.1 Introduction ...............................................................................5
2.2 Definitions .................................................................................5
2.3 Set Operations ...........................................................................6
2.4 Venn Diagrams ..........................................................................8
2.5 Dimensionality ........................................................................10
2.6 Conclusion...............................................................................10
Exercises.............................................................................................10
Chapter 3 Probability Basics...............................................................................15
3.1 Random Experiments, Outcomes and Events..........................15
3.2 Probability ...............................................................................16
3.3 Probability Axioms..................................................................17
3.4 Conditional Probability ...........................................................20
3.5 Independence...........................................................................25
Exercises.............................................................................................27
Chapter 4 Random Variables and Distributions..................................................33
4.1 Random Variables....................................................................33
4.2 Distributions ............................................................................35
4.2.1 Probability Mass Functions........................................38
4.2.2 Probability Density Functions....................................40
4.2.3 Survivor Functions .....................................................41
4.3 Discrete Distribution Functions...............................................43
4.3.1 The Bernoulli Distribution .........................................43
4.3.2 The Binomial Distribution..........................................44
4.3.3 The Multinomial Distribution ....................................47
4.3.4 The Hypergeometric Distribution...............................48
4.3.5 The Poisson Distribution............................................49
4.3.6 The Geometric Distribution........................................50viii Contents
4.3.7 The Negative Binomial Distribution ..........................51
4.4 Continuous Distribution Functions..........................................53
4.4.1 The Exponential Distribution .....................................54
4.4.2 The Gamma Distribution............................................56
4.4.3 The Weibull Distribution ............................................57
4.4.4 The Beta Distribution .................................................58
4.4.5 The Normal Distribution ............................................59
4.4.6 The Lognormal Distribution.......................................64
4.4.7 The Uniform Distribution...........................................66
4.5 Conditional Probability ...........................................................67
4.6 Residual Life Distributions......................................................68
4.7 Hazard Functions.....................................................................69
4.8 Mixture Distributions ..............................................................71
4.9 Independent Random Variables...............................................72
Exercises.............................................................................................72
Note ....................................................................................................80
Chapter 5 Joint, Marginal and Conditional Distributions...................................81
5.1 The Idea of Joint Random Variables........................................81
5.2 The Discrete Case....................................................................82
5.2.1 Marginal Probability Functions..................................84
5.2.2 Conditional Probability Functions .............................85
5.3 The Continuous Case...............................................................87
5.3.1 Marginal Probability Functions..................................89
5.3.2 Conditional Probability Functions .............................91
5.4 Independence...........................................................................94
5.5 Bivariate and Multivariate Normal Distributions....................97
5.6 Bivariate and Multivariate Exponential Distributions...........104
Exercises...........................................................................................108
Chapter 6 Expectation and Functions of Random Variables.............................113
6.1 Expectation............................................................................113
6.2 Three Properties of Expectation ............................................116
6.3 Expectation and Random Vectors..........................................118
6.4 Conditional Expectation ........................................................122
6.5 General Functions of Random Variables...............................127
6.5.1 One-Dimensional Functions.....................................127
6.5.2 Multidimensional Functions.....................................129
6.6 Expectation and Functions of Multiple Random Variables...132
6.7 Sums of Independent Random Variables...............................132
Exercises...........................................................................................138Contents ix
Chapter 7 Moment Generating Functions.........................................................145
7.1 Construction of the Moment Generating Function................145
7.2 Convolutions..........................................................................148
7.3 Joint Moment Generating Functions.....................................150
7.4 Conditional Moment Generating Functions..........................156
Exercises...........................................................................................158
Chapter 8 Approximations and Limiting Behavior...........................................161
8.1 Distribution-Free Approximations.........................................161
8.2 Normal and Poisson Approximations....................................163
8.3 Laws of Large Numbers and the Central Limit Theorem......166
Exercises...........................................................................................167
Index ......................................................................................................................169xi
Preface to the Second Edition
As was the case for the initial edition, this book is intended for undergraduate (proba￾bly sophomore level) engineering students. Depending upon the vision of the faculty, 
the text should be useful for students in electrical and mechanical engineering as well 
as those studying industrial engineering. It is specifically intended to present prob￾ability theory to students in an accessible manner. The book was first motivated by 
the persistent pattern of failure of students entering our random processes course to 
bring an understanding of basic probability with them from the prerequisite course. 
This motivation was reinforced by more recent success with the prerequisite course 
when it was organized in the manner used to construct this text.
Essentially, everyone understands and deals with probability every day in their 
normal lives. There are innumerable examples of this. Nevertheless, for some reason, 
when engineering students who have good math skills are presented with the math￾ematics of probability theory, there is a disconnect somewhere. It may not be fair to 
assert that the students arrived to the second course unprepared because of the previ￾ous emphasis on theorem-proof type mathematical presentation, but the evidence 
does support this view. In any case, in constructing this text, I have carefully avoided 
a theorem-proof type of presentation. All of the theory is included but I have tried to 
present it in a conversational rather than a formal manner. In fact, I have relied heav￾ily on the assumption that undergraduate engineering students have solid mastery of 
calculus. The math is not emphasized so much as it is used.
Another point of emphasis in the preparation of the text is that there are no balls 
in urns examples or problems. Gambling problems related to cards and dice are used 
but balls in urns have been avoided. At the same time, to the extent possible, the 
examples used are based on engineering applications – often in inventory, service 
operations, reliability or quality contexts.
In preparing this second edition of the text, my first priority was to significantly 
expand the problem sets at the end of each chapter. I believe that there are now nearly 
double the number of exercises than contained in the initial edition. Naturally, I also 
attempted to clarify some sections for which that seemed warranted. I also added a 
considerable volume of distribution related material. Specifically, I added one dis￾crete and two continuous univariate distributions. I also added a complete section 
dealing with bivariate and multivariate exponential distributions. I feel that these 
additions make the text a very complete introductory treatment of probability and 
I sincerely hope that the faculty and students who use the text will find it so.
In developing the contents of this book, I respected the fact that there is a second 
and probably more other courses that should follow it. I have therefore focused on the 
fundamentals of probability theory. I have avoided advancing into stochastic pro￾cesses, the gambler’s ruin problem, matching problems and stopping rule analyses. 
The intent is here is to provide a comprehensive and understandable treatment of the 
fundamentals. Once the students have mastered these, we can lead them forward to 
stochastic processes, simulation modeling and statistics.xii Preface to the Second Edition
Clearly, there are many views concerning how the fundamentals of probability 
should be organized. I have attempted to create coherent sections of the topic and to 
present them in an organized sequential manner. Necessarily, the text starts with set 
theory and moves to probability axioms. I then treat single-dimensional random vari￾ables and their distributions followed by multidimensional random vectors and their 
distributions and then conditional distributions. In the process, I intentionally post￾pone the discussion of expectation and moments until later in the text. I conclude 
with a short treatment of approximations and the three key limit theorems. In my 
view, this makes for a dense but manageable one semester course that should prepare 
the students for the continued study of probability. A solutions manual will also be 
available on the instructor’s hub of the Routledge website (www.routledge.
com/9781032278483). Considering the dominant role probability has in engineering 
practice and in our lives, I believe that this is an effective way to introduce the rigor 
of the subject to those who will use it.xiii
Author
Joel Nachlas is an associate professor Emeritus of the faculty of Industrial and 
Systems Engineering at Virginia Tech. He served on that faculty from 1974 until 
2016. During his tenure in that department, he served as the coordinator for the 
department’s Operations Research faculty and curricula and was also the coordi￾nator of the department’s international program. The foci of his research were the 
application of probability theory to reliability analysis and maintenance planning and 
of statistical methods to quality control. He earned the B.E.S. from Johns Hopkins 
University in 1970 and the M.S. and Ph.D. from the University of Pittsburgh in 1974 
and 1976, respectively. All three of his degrees are in Industrial Engineering with 
a concentration in Operations Research. He has received numerous awards for his 
research including the 1991 P. K. McElroy Award and the 2004 Golomski Award. 
He continues to serve as the editor of the Proceedings of the Annual Reliability and 
Maintainability Symposium. He was a member of INFORMS and IISE and he is a 
fellow of the Society of Reliability Engineers and a fellow of the American Society 
for Quality. During his tenure at VT, he served for 20 years as a visiting professor at 
Ecole Polytechnique de Nice in France. While on the VT faculty, he served for 35 
seasons as head coach of the Virginia Tech men’s lacrosse team and was selected in 
2001 as the US Lacrosse MDIA national coach of the year.DOI: 10.1201/9781003294382-1 1
Introduction 1
Most people have an intuitive feel for probability. Many people play card games – 
either for fun or for profit – and most start playing card games as children. People 
also talk about weather in terms of probability. It is common to speak of the chances 
of side effects associated with medications and the chances of automobile accidents 
or of contracting communicable diseases. These are just a few examples of the ways 
in which probability is a part of our lives that we seem to understand well.
Paradoxically, most people confronted with the study of the mathematical repre￾sentation and analysis of probability find this effort challenging or worse. The ques￾tion becomes one of translating our intuition concerning probability into an 
understanding of the mathematical structure of the subject. The answer is far from 
clear. This text represents an attempt to support the transition from intuition to math￾ematical rigor. The vehicle for promoting the transition is explanation and example 
rather than theorem and proof. As we proceed, the readers are encouraged to reflect 
on the experiences they have had with practical realizations of probability and the 
relationship of those experiences to the topics described here.
1.1 HISTORICAL PERSPECTIVES
Several authors have recounted the evolution of probability theory. They indicate 
that interest in probability started with gambling – perhaps in prehistoric times – 
and that probability analysis has been used – and sometimes abused – consistently 
in an intuitive manner until sometime in the 17th century. At that time, scientific 
study in numerous disciplines and especially in mathematics advanced dramatically. 
An element of the awakening of scientific inquiry was the exploration of random 
phenomena.
The two principal contributors to the definition of probability as a subject of sci￾entific study were the French mathematicians Pierre de Fermat and Blaise Pascal. It 
is fair to say that they were the progenitors of the mathematical study of probability. 
Their efforts took place in the 17th century and it is interesting to note that the word 
probability is derived from the French “probabilite” which is in turn derived from the 
Latin version. Subsequently, important contributions were made to mathematical 
theory of probability by Bernoulli, Huygens and DeMoivre. In general, these math￾ematicians focused on discrete problems many of which were motivated by questions 
about gambling or equivalent processes.
The structure of modern probability theory is probably attributable to Richard von 
Mises and Andrei Kolmogorov, whose principal works were published in the 1930s. 
Their advances were based on Cantor’s study of infinite sets and the formalization of 
Lebesque integration. The reader is encouraged to explore the descriptions of the 2 Probability Foundations for Engineers
work of these mathematicians. Perhaps the most important points to extract from this 
short historical description are the facts that the origins of probability analysis are 
attempts to understand real human experiences and that the scientific formalism of 
probability theory is relatively recent.
1.2 FORMAL SYSTEMS
Probability theory is a formal system as are most mathematical domains. To put this 
in perspective, we might consider that mathematics as a whole is a formal system and 
that mathematics is a very broad domain of study within which there are reasonably 
self-contained subdomains. A global definition of a “formal system” is that it is a 
coherent set of elements such as a vocabulary along with a syntax or rules for com￾bining the elements. A computer programming language such as C++ or HTML is a 
formal system as are (1) a language such as English or Chinese, (2) chemical notation 
and (3) the philosophical rules for dialog.
For discussion purposes, we observe that geometry is formal system and is also a 
subdomain of mathematics. It is the part of mathematics concerned with describing 
spatial relationships. Note that geometry starts with a set of axioms which are rules 
that are accepted without proof. The elements of geometry are angles, points, lines 
and planes. There are rules for using the axioms to construct understandings about 
the elements. It is also interesting to recognize that there are multiple types of geom￾etry of which the contrast between Euclidean (which relates to our earth) and non￾Euclidean (as applied to Relativity Theory) is intriguing.
In a mathematical sense, probability is also a formal system. It is the system that 
is used to describe random phenomena. It is based on a set of axioms and includes a 
set of rules for using the axioms to obtain understanding about the elements which 
are the realizations of the random phenomena.
What then is a random phenomenon? Each person has their own definition. The 
one assumed here is that a random phenomenon is one in which repeated application 
of the same stimulus yields different and unpredictable responses. For example, if 
one repeatedly rolls a fair six-sided die, one may observe several different responses 
and none of the responses can be predicted in advance.
Within the context of the definition of a random phenomenon, we should pause 
and distinguish between “statistics” and probability. In this text, the subject is prob￾ability which is a model for future experiences. We discuss what we will observe if 
we roll a die or if we produce a unit of product or if we monitor an inventory level or 
a stock price. Statistics is the use of analytical methods to interpret and make deci￾sions using historical information. It may involve the interpretation of observations 
from a random phenomenon, but it may also involve descriptions of non-random 
processes. The statements that parking lot P4 contains 240 places, that the US has 
had 44 presidents and that a series of 8 coin tosses yielded 5 heads are all statistics. 
They describe past experiences. Many people confuse the two terms. We are studying 
probability.Introduction 3
1.3 INTUITION
This chapter began with the comment that probability started as an intuitive evalua￾tion of future experiences. As you now undertake to study probability, consider the 
following questions:
1. What do you think are the chances that you would see a blackjack hand?
2. What is the probability that your car will survive until you graduate?
3. What is the probability that one of your classmates will die this year?
4. What is the probability that a tornado will damage your campus this year?
5. For an arbitrary consumer product that you purchase this year, what is the 
probability that it is defective?
What does your intuition suggest concerning these questions? Assuming you will use 
mathematical logic rather than intuition to answer these questions, what models do 
you think will help? It is not necessary to answer this question now. It is worthwhile 
to remember these questions and the choice of model as your study proceeds.
EXERCISES
1. Describe an experience you have had with probability, possibly in a game or bet￾ting context. Indicate how you analyzed the probabilities involved.
2. How should we interpret the fact that a weather forecast indicates a 60% chance 
of rain today and it does not rain?
3. Identify four events or activities that involve you today and are subject to 
probability.
4. Suggest four engineering applications in which probability is an important 
element.DOI: 10.1201/9781003294382-2 5
A Brief Review of Set 
Theory 2
2.1 INTRODUCTION
The starting point for our study of probability is a review of the basic concepts of 
the mathematical domain called set theory. The reason we start with set theory is 
that it provides a vehicle for organizing the elements of our probability models. As 
implied in the name, set theory is a structured language for discussing “sets”. The 
initial formal definition of set theory was provided by George Cantor in 1874. The 
objective of Cantor’s work and that of other mathematicians working with set theory 
was to obtain an understanding of infinity. The difficulty of this idea precipitated 
considerable debate among mathematicians and ultimately led to the definition of the 
axiomatic system that we will use.
This chapter is called a review of set theory because many students who undertake 
the study of probability have already encountered set theory in earlier math courses. 
For those who are meeting set theory here for the first time, the descriptions provided 
below should be sufficient. If not, many supplementary resources are available in the 
library and on the web.
A set is simply a collection of entities in which we are interested. The collection of 
interest might be all of the Ford sedans registered in Oregon this year, the people in 
Pennsylvania receiving liver transplants this month, the red face cards in a standard 
deck of poker cards, the engine bearings produced in a particular plant today, the dura￾tions of internet sessions, the hardness of cutting tools or the equity stocks included in 
your investment portfolio. This list is intended to illustrate the fact that the idea of a set 
is general. It can be applied to any collection of things that we would like to discuss or 
analyze. The collection may include a finite number of members (elements) or an 
infinite number of members. The important aspect of a set is that it be clearly defined.
2.2 DEFINITIONS
It is conventional to represent a set by a capital letter. For example, the set of Chevrolet 
Malibus registered in Florida could be represented as:
M = { | x x is a Chevrolet Malibu with Florida license tags}
Note that the capital M has been used to represent the set and that x has been used 
to represent an element (or member) of the set. The vertical line is read as “such 
that”. Thus, the above set definition should be read as:
M is the set of members, x, such that x is a Chevrolet Malibu with Florida tags.6 Probability Foundations for Engineers
Note further that braces “{ }” are used to specify the members of a set. If we wish to 
analyze features of any group of items, the definition of the corresponding set must 
make the identities of the elements clear.
For most applications, we anticipate that a set will have subsets; that is, sets may 
contain groups of members that are subject to more specific identification and can 
thus be organized into sets. For example, define the sets B and W as:
B x   { | x M and is blue}
W x   { | x M and is white}
where the symbol ∈ (epsilon) is read as “is an element of” or “is in”. Thus, the set 
B is the set of elements of M that are blue (the set of blue Chevrolet Malibus regis￾tered in Florida). We can see that the sets B and W are contained in the set M and we 
represent this as:
B M ⊂ ⊂ and W M
which are read as B is contained in M and W is contained in M. Equivalently, we could 
simply say that B is a subset of M and W is a subset of M. In terms of notation, we 
may also wish to represent cases in which a subset might actually correspond to the 
entire set of which it is a part. For two sets, say X and Y, we would represent this as:
X Y ⊆
This is read as X is a subset of Y. The distinction between this algebraic statement and 
the ones provided for B and W is that it would be more correct in those earlier cases to 
say B is a proper subset of M and W is a proper subset of M. This means that the subset 
B does not exhaust M and similarly for W. The conceptual parallel to the distinction 
in membership statements B M ⊂ and X Y ⊆ is the numerical distinction we make 
between a b < and a b ≤ . In the first case, equality is precluded while in the second 
case equality is possible. In fact, observe that an implication of this notation is that:
if X Y ⊆ and Y ⊆ X then X Y =
Thus, if two sets simultaneously contain each other, they must be identical.
Regardless of the context within which we define sets, there are two sets that 
are  fundamental to our definitions and our analysis. These are the “universe” of 
elements – the set of all of the possible elements we might discuss – and the “null” 
set (or empty set) which contains no elements at all. In a probability context, we will 
also refer to the “universe” as the “sample space” and will denote it by an upper case 
omega, Ω. We represent the empty set by the symbol ∅.
2.3 SET OPERATIONS
Since set theory is a formal mathematical system, it includes algebraic operators. 
Specifically, there are rules for combining and separating the elements of sets into 
other sets. The operations are called union and intersection. The idea of a union of A Brief Review of Set Theory 7
sets is conceptually parallel to the idea of addition in arithmetic. For two sets A and 
B of a universe Ω, the union is defined as:
A  B x{ | x A   or x B} (2.1)
Thus, the union of two sets is a set of elements that is in at least one of the sets. 
For the example of Chevrolet Malibus registered in Florida B W∪ is the set of those 
cars each of which is either blue or white.
Conceptually, an intersection is similar to arithmetic multiplication. The 
intersection of two sets is a set of elements that are in both sets; that is:
A  B x{ | x A   and x B} (2.2)
In the case of the example of Chevrolet Malibus registered in Florida, let:
T   { | x x M and is a two door model}
F x   { | x M and is a four door model}
Then, B T ∩ represents the set of those cars each of which is blue and has two 
doors. Notice that
T   F 
which is to say that the sets T and F have no common elements. Their intersection is 
the empty set. We say that these sets are disjoint or mutually exclusive.
The operators union and intersection permit us to describe sets and combinations 
of sets conveniently and efficiently. Fortunately, these operators have the desirable 
properties that one often seeks in an algebraic operator. Specifically, they are com￾mutative, associative and distributive. These relations are defined as:
commutative A  B B  A and A  B B  A
associative ( ) A B  C A  ( ) B C  and ( ) A B  C A  ( ) B C 
distributive A  ( ) B C   ( ) A B   ( ) A C and A  ( ) B C   ( ( A B  A C)
One further relationship that we define is the complement of a set. This is the set 
of elements that are not in the set. In this text, we will denote the complement of the 
set A by Ac
 and the algebraic definitions is:
A x x x A c   { |  and  }. (2.3)
Here, Ac
 is the set of elements of the universe that are not in the set A. Note that 
the definition of the complement of a set permits us to state that:
A A A A c c     and  .8 Probability Foundations for Engineers
Note also that the operations of union and intersection along with the definitions 
of the universe, the empty set and the complement of a set are sufficient for us to 
describe and analyze sets in any way we feel is informative. This includes what we 
might call the difference in sets. Suppose we have the sets:
A    1, , 234, ,5 4 and B    ,5 6,,, 7 8 .
If we wish to identify the set of elements of A that are not in B, we take:
A Bc     1 2, ,3 .
Comparable constructions permit us to describe nearly any meaningful set.
2.4 VENN DIAGRAMS
There is a convenient graphical representation that is often used for sets. This is the 
Venn diagram. An example of the Venn diagram for our sets of Chevrolet automo￾biles in Florida is (Figure 2.1).
Notice that the sets W and B are disjoint as are the sets T and F. Notice also that 
the sets we have discussed so far do not exhaust the sample space as there are other 
elements of M. Finally, observe the fact that the sets represented do not contain all of 
the elements of the sample space. For example there are elements of M that are 
neither blue nor white and are neither two nor four door models – green station 
wagons. In addition, if it were meaningful to do so, we could recognize that the set 
M can be discussed as a subset of the larger set that includes the other types of cars 
that are registered in Florida.
With our definitions of sets, subsets and operations on sets, we can construct many 
meaningful and useful statements about sets. One example is:
A A c c
   (2.4)
Another is:
E F E E F c       (2.5)
FIGURE 2.1 Example Venn diagram.
M
B
W
T
FA Brief Review of Set Theory 9
Two other useful and widely used relationships are known as DeMorgan’s Laws:
A B A B c c c      (2.6)
and
A B A B c c c      (2.7)
In order to appreciate these two relationships, consider the Venn diagram that 
represents them. Notice that the complement of the union of A and B is the set of ele￾ments that are simultaneously not in A and not in B. Similarly, the complement of the 
intersection of A and B is the set of elements of the universe that are not simultane￾ously elements of both A and B (Figure 2.2).
Another useful relationship that we can see from a Venn diagram is that a set A can 
always be represented as the union of the two sets  A B   and A Bc   ; that is:
A A B A Bc         (2.8)
Observe that the two sets  A B   and A Bc    are disjoint and the identity 
applies even if one of the intersections is empty.
It is going to be useful to extend this idea to multiple sets so consider a collection 
of sets say E1 2 E En , ,…, that is defined so that the sets are pair-wise disjoint and in 
total they exhaust the set, A, of which they are all subsets; that is:
E E i j i i j    , ,  (2.9)
and
E A i
i
n
=
=1
 (2.10)
We say that the set of sets Ej
 forms a partition of the set A. We can then use the 
partition to state that:
A     A E1 2     A E     A En (2.11)
FIGURE 2.2 Venn diagrams for DeMorgan’s Laws.
A B A B10 Probability Foundations for Engineers
If we think again of the set of Chevrolet Malibus registered in Florida, M, the sets 
B and W along with the sets representing Florida registered Malibus of each other 
available color form a partition of M.
2.5 DIMENSIONALITY
The final aspect of sets we must consider is their size. The size of a set is defined as 
the number of elements that are members of the set. Size is referred to as the car￾dinality of the set. For a set, B, we represent the cardinality by ||B|| and generally 
we wish to know whether the cardinality of a set is countable or uncountable. As 
the terms imply, a set is countable if one could match the elements of the set with 
some or all of the natural numbers – one could count them. If only some of the set 
of natural numbers are needed, the set is a finite set. If all of the natural numbers 
are needed, then the set is countably infinite. In both of these cases, the elements of 
the set are said to be “discrete”. On the other hand, if a set has uncountably infinite 
cardinality, the number of elements is infinite and is much greater than the number 
of natural numbers.
An example of a countably infinite set is the set of planets in the universe. It is 
possible to try to count them but this would likely be a never ending process.
The most common example of an uncountably infinite set is the set of real numbers 
within any arbitrary interval, say [0, 1]. To see that this is the case, enumerate and 
count any sequence of values between zero and one, say (0.0, 0.01, 0.02, 0.03, …). 
As you do this, note that between any two of your enumerated numbers, many 
additional values (infinitely many in fact) can be identified.
2.6 CONCLUSION
The definitions and relationships included in this chapter are the basic constituents of 
set theory. There are more detailed and more extensive discussions of set theory than 
the one provided here. However, the description in this chapter has been formulated 
to support our study of probability in the chapters that follow.
While set theory has many domains of application, it is fundamental to the 
construction of probability theory. The reader is encouraged to fully master the 
concepts of this chapter prior to moving on to the study of probability.
EXERCISES
1. Identify (at least two) sets of states of the US in at least two different ways.
2. For the sets you identified in exercise 1, identify subsets.
3. A random experiment consists of measuring the weight in kilograms of the car￾bon dioxide emitted by a coal fired power plant during a 4 hour period. Identify 
the sample space for this experiment.
4. An experiment consists of selecting an acre of land in the Jefferson National 
Forest at random and counting the number of Cardinal nests in that parcel of land. 
Identify the sample space for this experiment.A Brief Review of Set Theory 11
5. A random experiment consists of measuring the time required for customers 
to complete a purchase on a merchant’s web site. Identify the sample space for 
this experiment.
6. If an experiment consists of tossing a fair six-sided die, identify the sample 
space for the experiment and a partition of that sample space that includes at 
least 3 non-empty sets.
7. An experiment consists to tossing two fair four-sided dice and taking the sum 
of the numbers they show. Identify the sample space for this experiment and 
define a partition of the sample space containing at least 3 non-empty events.
8. Two six-sided dice are tossed. Let A represent the set of tosses for which the 
sum of the dice is even, B be the set of tosses for which at least one die shows 
a 3 and C be the set of tosses for which the sum of the dice is 7. Identify the 
elements of:
a. A∩ B
b. B C c ∩
c. A∩C
d. A B C c c c ∩ ∩
9. Two fair six-sided dice are tossed. Let A be the set of tosses for which the sum 
of the dice is less than 7, B be the set of tosses for which at least one die shows 
a 3 and C be the set of tosses for which the sum of the dice exceeds 4. Identify 
the elements of:
a. A Cc ∩
b. A B C c ∩ ∩
c. B Cc ∩
d. A C c c ∩
e. A    B C
10. Two fair six-sided dice are tossed. Let A be the event that the sum of the dice is 
less than 7, B be the event that at least one die shows a 3 and C be the event that 
the sum of the dice exceeds 4. Identify the elements of:
a. A Cc ∩
b. A B C c ∩ ∩
c. B Cc ∩
d. A C c c ∩
e. A    B C
11. Suppose a sample space is defined by    { | x x 0 2  0}. If the 
events  A   { | x x 8 1   2 1 },B x{ | 0 1   x C 5 7 },   { | x x  10} and 
D x   { |11 x  17}, describe the following events and draw them on the real line.
A∪ B A∩ B A Dc ∩
B C ∪ B C c c ∩ C D ∪
B D c ∩ A∩C A∪ ∪B C12 Probability Foundations for Engineers
12. Suppose an experiment consists of observing the speed of traffic flow on I695 
(the capital beltway) between the Falls Church exit and the exit for the Dulles 
airport toll road. Provide a reasonable definition of the sample space for this 
experiment. Interpret the events A   { | x x 0 4   mph m },B x{ | 20   x 35 ph}
and C x   { | 50 x  65mph}.
13. Define T = { | x x is a prime number less than12} { S x = | x is an odd number less
than12}. Identify the elements of S and T and the set T ∩ S
14. For three events A, a B C nd of a sample space, Ω, state expressions for the events:
a. Either A or B occurs
b. Only A or B occurs
c. None of them occur
d. C occurs but A and B do not occur
15. Let E, F and G be three sets. State expressions for:
a. only F occurs
b. exactly two of the sets occur
c. at least one of the sets occurs
d. E and G occur but F does not occur
e. none of the events occur
16. Let A, B and C be three sets. Prove that C A B C A C B c c c             17. Prove that R S R R S c       18. Let a universe, Ω, be the set of cards in a standard poker deck. Identify a parti￾tion of Ω.
19. Prove DeMorgan’s first law that A B A B c c c      .
20. Prove that for two sets, A and B, A ⊆ B implies that A  B A
21. In some communication circuits, a 3 component voting routine is used to deter￾mine if a message has transmitted accurately. For a particular message, let Yi
represent the event that voter i indicates accurate transmission. Express each of 
the following events in words:
Y2 3 ∪Y Y Y Y c 1 2 ∪ ∪ 3 Y Y c
 1 3   Y Y Y c
 1 2   3 
Y1 2 ∩Y Y Y c
2 3 ∩ Y Y Y c
 1 2   3  Y Y c c
1 2 ∩
22. A system is comprised of 5 components each of which is either functioning or 
failed. The components are represented by the vector x x 1 234 x x x5  , , , ,  where 
xi = 1 if component i is functioning and xi = 0 if it is failed. An experiment con￾sists of observing the states of the 5 components. (a) How many outcomes are 
there in the sample space for this experiment? (b) Suppose the system will func￾tion if components 1 and 2 are working or if components 3 and 4 are working 
or if components 1, 3 and 5 are working. Let E1 be the event that the system is 
functioning. Identify all of the outcomes in E1. (c) Let E2 be the event that com￾ponents 4 and 5 are failed. Identify the outcomes included in E2. (d) Identify the 
outcomes that are included in E1 2 ∩ E .A Brief Review of Set Theory 13
23. Suppose a sample space is defined by    { | x x 20. . 0 4   8 5, } x  . For the 
events A   { | x x 24. . 1 3   2 6},B x{ | 29. . 0 3   x C 6 3},   { | x x 38. . 2 4  2 5}
and D x   { | 41. . 0 4 x  8 5}, identify the elements of the events:
24. Identify a simpler expression for:
a. E F       F G
b. E F E F E F c c           DOI: 10.1201/9781003294382-3 15
Probability Basics 3
3.1 RANDOM EXPERIMENTS, OUTCOMES AND EVENTS
The discussion of concepts from set theory is preliminary to our investigation of 
probability. We will use relationships among sets to build our probability models. 
The starting point for the study of probability is the idea of a random experiment. A 
random experiment is just the process of observing a phenomenon in which we have 
an interest. As implied in the term random experiment, the phenomenon of interest is 
presumed to be one that is subject to randomness.
A reasonable first question is what do we mean by the term randomness? In a 
general context, the term randomness is defined here as the property of an experiment 
that repeated application of the same stimulus yields multiple responses such that on 
any observation the response that will be obtained is not predictable. A simple 
example of such a process is the tossing of a coin. Two observations are possible and 
prior to any toss, its result cannot be predicted.
Within the context of engineering applications, the experiment may be checking 
an in-process inventory level, measuring the wear on a cutting tool, counting the 
number of incoming calls to a call center, observing the value of an investment, 
measuring the thermal flux in a reactor, checking chemical catalyst concentrations in 
a tank, determining the void fraction in a composite form, counting the number of 
positive diagnostic medical tests, observing the life length of a component or 
equipment unit or any of very many other processes that we treat. The common 
features of those experiments are that they are meaningful to us and they are display 
random variation in their responses to the stimuli we apply.
A random experiment necessarily can result in any of a number of observations. 
The possible observations that might occur are called outcomes. It is important to be 
careful here. This is where we start to use set theory. The result of any random 
experiment is an outcome but we usually discuss the observation, the outcome, in 
terms of events.
An event is a set of outcomes. While our specific experimental observation will be 
an outcome, we can only define a probability model for our experiment using sets of 
outcomes. Our terminology convention is to call those sets events. We will discuss 
the relationships between outcomes and events further soon. For now, we should first 
recall from our discussion of set theory that we call the set of all possible outcomes 
of an experiment the sample space and will use Ω to represent it. Here are two 
examples.16 Probability Foundations for Engineers
3.2 PROBABILITY
Keep in mind the fact that our objective is to define a predictive mathematical model 
of an experiment consisting of observing a phenomenon or process of interest. 
Example 3.1
When an integrated circuit is manufactured, it can have three types of flaws:
a. a short due to a lack of material on a circuit path
b. a bridge due to the deposition of excessive material across two circuit 
paths
c. a crack or pit in its external coating
If we select a recently produced circuit at random and test it, the result of this 
process will be random. The set of possible observations is:
    no flaw, , short bridge c, rack
Example 3.2
The tread depth on a certain newly manufactured radial pattern automotive tires 
varies within the range of 7.5 mm to 8.5 mm. If we select a recently produced tire 
and measure its tread depth, the set of possible observations is:
   { | x x 7 5. .  8 5}
An observation we should make concerning these two examples is that the 
sample space may be comprised of a finite (or countable) number of elements or 
it may be comprised of an uncountably infinite number of elements. When the 
number of elements of the sample space is countable (as in Example 3.1), it is 
usually the case that we consider the elements “discrete” and as a result, we may 
define an event to correspond to a single outcome. On the other hand, when the 
number of elements is uncountably infinite (as in Example 3.2), we define events 
in terms of sets of outcomes. For Example 3.1, we could define the events:
A B    no flaw s , ,    hort bridge c , , C D    rack and s    hort bridge c, c rak
Other definitions are also possible but these illustrate the fact that an event may 
correspond to one or to more than one outcome. Note also the events need not 
be mutually exclusive.
In the case of Example 3.2, we would not – in fact cannot – define events com￾prised of single outcomes. Instead, we define events as intervals such as:
E   { | x x 7 8. . 0 7   99}, F x{ |7 9. . 0 8   x G 25} { and   x x | . 8 10 8  . } 40
The key point here is that events are sets. Thus, all of the things we said in 
Chapter 2 about sets apply to events. We may therefore model our random experi￾ment in terms of the events corresponding to sets of possible observations.Probability Basics 17
The definition of the sample space and its events provides the structural basis (the 
skeleton) for our model. We next attach our predictive measure – probability – to our 
structure.
In its most general sense, probability is simply a single-valued mathematical 
function that we define on a sample space. There are rules for how we make the defi￾nition but these are reasonably unrestrictive. Two key rules are (1) that we define our 
probability functions on the events of the sample space rather than on outcomes and 
(2) that the domain of the probability function is the entire sample space and the 
range is the real interval [0, 1].
Before proceeding with this idea further, we observe that people (probability spe￾cialists and philosophers most of all) have argued about how to assign probabilities to 
events and how to interpret probability measures for a long time. These debates con￾tinue and are often quite intense. Fortunately for engineers who wish to apply probabil￾ity, the formal mathematical system we will create and study here is internally consistent 
and “correct” for any (and all) of the different philosophical interpretations.
As discussed previously, just as geometry is our mathematical language for 
describing spatial relationships, probability is our mathematical language for describ￾ing randomness. The philosophical explanations of the origins of randomness differ 
substantially but the usefulness of the mathematics transcends those distinctions.
3.3 PROBABILITY AXIOMS
For the purposes of our study, probability is the measure that indicates the chance 
that a random experiment will yield a specific event. We have considerable latitude 
in how we specify that measure. In fact, the function we define may be organized in 
nearly any way we wish provided it conforms to three basic axioms. Another way to 
view this idea is that we will take three basic and very general axioms and using those 
axioms, we will construct the entire formal system that we call probability. Note the 
parallel to geometry and its basis in Peano’s Postulates.
The three axioms of probability are:
1. For every event E, 0   1   Pr E 
2. Pr  1
3. If A A1 2   , , is a disjoint collection of events, then Pr Pr
j
j
j
A A  j 

 


  
It may seem surprising but these three axioms are all that we need to develop all of 
probability theory. Starting with them, we can construct many useful results that we 
then use to model the physical phenomena that we wish to study.
Here are a few examples of the results that we can construct.
Example 3.3
For any event E, Pr P E E r c 
 
  1  
Since E Ec    and E Ec   
Pr P E E r c    
      1 and Pr P E E r P E E r c c    
     
 
18 Probability Foundations for Engineers
Example 3.4
The above result implies that Pr    0
Example 3.5
For two events E1 and E2 having E1 2 ⊆ E it must be the case that Pr P   E E 1 2  r 
Suppose E1 2 ⊆ E
Then it must be the case that E E E Ec 2 1     2 1  and clearly E E Ec 1 2     1  
Thus Pr P E E r Pr E Ec   2 1     2 1 
 
 so Pr P   E E 2 1  r 
Example 3.6
For any two events E1 and E2, Pr  E E 1 2    Pr  E E 1 2 Pr     E E 1 2 
Note that E E E E E c 1 1     2 1     2 and E E E E c  1 2       1 2   so Pr  E1 
Pr E E Pr E E c 1 2  1 2 
 
    
Similarly E E E E E c 2 2     1 1     2 and E E E E c  2 1       1 2   so Pr  E2 
Pr E E Pr E E c 2 1  1 2 
 
    
Thus Pr[E E Pr Pr E E Pr E E Pr E E c c 1 2       1 2    2 1    2  1 2  
However, the union may be decomposed as
E E E E E E E E E E c c 1 2       1 2      1 2       2 1     1 2
So Pr P   E E 1 2   r P   E E 121  r P     r  E E2
Example 3.7
For three events E1, E2, E3, we have:
Pr Pr Pr Pr Pr
Pr Pr
E E E E E E E E
E E E
1 2 3 1 2 3 1 2
1 3 2
                
        E E 3 1   Pr  E E 2 3 
Next, return to the application of probability to events rather than outcomes. 
Consider the example of tire tread depths. What are the possible outcomes? How 
many possible outcomes are there? How can we define a probability function on 
the set of outcomes in a manner that assures that axioms 1 and 2 hold?
The answer is that this cannot be done because of the fact that Ω is uncountably 
infinite. However, if we form events as subsets of the sample space, we can define 
the probability function. Hence, this is how we proceed. Consider two examples 
of a discrete sample space.Probability Basics 19
Example 3.8
Consider that we have two fair four-sided dice (pyramids on which the faces are 
numbered). For each die, what are the possible outcomes?
    1 2, ,3 4,
For the sample space corresponding to the tossing of only one die, consider the 
events:
a. we roll an even number A x = { | xis even}
b. we roll a number larger than 2 B x     x 2
c. we roll a number less than or equal to 3 C x   { | x 3}
What are the elements of the sets A, B, and C?
A B    2 4, ,    3 4, ,C    1 2, ,3 .
What are A B ∪ , A B ∩ , B C ∩ , A B ∪ ∪C?
A B     2 3, ,4 4 , , A B     B C    3 ,A B  C  
What are the probabilities of occurrence for these sets?
Pr ,   A B    3 Pr  A B   , Pr ,   B C   Pr  A B C  4
1
4
1
4 1
Example 3.9
Suppose we roll both of the four-sided dice and take the sum of their outcomes. 
What is the sample space?
    2 3, ,4 5, ,6 7, ,8
If we define the events, E, F, G and H as:
a. E – The sum is odd.
b. F – The sum is between 4 and 7.
c. G – The sum exceeds 5.
d. H – The sum is 4.
What are the elements of these sets?
E   { | x x isodd}, F x{ | 4 7   x G},    x x 5 4 , H   
What are E F ∪ , E F ∩ , E ∩G, E H∩ , E H∪ ?
E   F E   3 4, ,5 6, ,7 5 , ,   F E  7 7 , ,   G E     H E   , , H    3 4, , 5 720 Probability Foundations for Engineers
3.4 CONDITIONAL PROBABILITY
There are experiments in which we may have some partial information or for which 
knowledge concerning one event can influence the probability of another event. For 
example, if my sample space is the set of all of the male students that attend a particu￾lar university and I select one of these students at random and you tell me the student 
is a member of the school’s basketball team, then the probability that the student is 
taller than 200 cm differs from that quantity for the general student population. The 
use of information to determine probability values is referred to as the use of condi￾tional probabilities.
Another example is that if I select a motorist on an interstate highway at random 
and note that the motorist is talking on a cell phone while driving, the probability that 
the motorist is younger than 35 years of age is greater than if the motorist is not talk￾ing on a cell phone.
One final example using my dice is that you will adjust the probability that the 
sum of the numbers on the two four-sided dice exceeds 5 if I tell you that the sum is 
even. In general, we compute:
Pr[ ] X X . 1 2 5 6
16     0 375
What are the probabilities associated with these events?
Pr ,   E F    7 Pr  E F   , Pr ,   E G   Pr  E H   , Pr  E H  8
3
8
1
8 0 11
16
The above examples serve to illustrate that the assignment of probability mea￾sures to events in a discrete sample space can be relatively simple and is usually 
quite logical. Clearly, the process can be more complicated when modeling an 
industrial phenomenon such as an inventory but it should still be logical.
The extension to a continuous sample space is direct but we must again be 
guided by logical descriptions of the process of interest. For the above example 
of tire tread depth, we would require a sensible rule for assigning probabilities 
to events. One such rule that might be reasonable would be that the probability 
of an event be proportional to its length. If that were the rule, then the following 
example makes sense:
Example 3.10
E   { | x x 7 8. . 0 7  99} Pr . .
. .   E  . 
  7 99 7 80
8 5 7 5
0 19
F   { | x x 7 9. . 0 8  25} Pr . .
. .   F  . 
  8 25 7 90
8 5 7 5
0 35
G   { | x x 8 1. . 0 8  40} Pr . .
. .   G  . 
  8 4 8 10
8 5 7 5
0 30
F   G x{ |8 1. . 0 8   x 25} Pr . .
. .   F G  . 
  8 25 8 10
8 5 7 5
0 15
E   F x{ |7 8. . 0 8   x 25} Pr . .
. .   E F   . 
  8 25 7 80
8 5 7 5
0 45Probability Basics 21
However:
Pr[ | X X X X ] . 1 2 5 1 2 4
8     is even  0 5
Similarly, if we are given the fact that the first die shows a 2, we have:
Pr[ | X X X ] . 1 2 5 2 1 1
4      0 25
The general relationship for conditional probabilities is explained as follows. Let 
A and B be events defined on a sample space Ω. The conditional probability of event 
A given the occurrence of event B is denoted by Pr[ | A B] and is computed as:
Pr[ | ]
Pr
Pr
A B
A B
B    
 
(3.1)
provided PrB .    0
An appropriate view of a conditional probability is that the available knowledge 
reduces the set of possible outcomes from the full sample space to a subset of it – the 
event B. As a result, that knowledge alters the probabilities associated with our obser￾vations. Looking at a Venn diagram emphasizes the point. If we know that B has 
occurred, then we know that elements of the sample space that are in the complement 
of B have not occurred. Hence, the event B contains all of the possible observations 
and the set A∩ B contains the observations from event A that are possible.
Example 3.11
Consider the sum of the numbers showing on the four-sided dice again. Enumerate 
the elements of the sample space as:
X X 1 2 \ 1 2 3 4
1 2 3 4 5
2 3 4 5 6
3 4 5 6 7
4 5 6 7 8
FIGURE 3.1 Venn diagram illustrating conditional probability.
A B22 Probability Foundations for Engineers
Now look at the results stated above:
Pr[X X 1 2 5| ] X X 1 2 4 . 8     is even  0 5
Pr[X X 1 2 5 2 | ] X1 1 . 4      0 25
To construct these probabilities using set notation, let E     x X1 2 X 5 . 
We can see that the set E      2 4, ,  3 3, ,  3 4, ,  4 2, ,  4 3, ,  4 4, . Next, let 
A x   { | X X 1 2 is even} so that A      11, ,  1 3, ,  2 2, ,  2 4, ,  3 1, ,  3 3, ,  4 2, ,  4 4, . 
Then A E       2 4, ,  3 3, ,  4 2, ,  4 4, so Pr  A E   4
16 and Pr  A  8
16 and 
the result follows.
Similarly, let B x = = { | X1 2} so B      2 1, ,  2 2, ,  2 3, ,  2 4, and B E       2 4, . 
Now Pr  B  1
4 and Pr  B E   1
16 so again the result follows.
While they may initially appear complicated, conditional probabilities often 
simplify the definition of probability models and the calculation of probabilities. 
In addition, conditional probability leads us to other useful relationships – specifi￾cally Bayes Rule and the Law of Total Probability.
Start again with the basic conditional probability relationship.
Pr[ | ] Pr
Pr
A B
A B
B    
 
Assume that both of the events A and B are non-empty. Then, it must also be 
the case that:
Pr[ | ] Pr
Pr
B A
A B
A    
 
Therefore, we can conclude that:
Pr |   A B Pr  B A   Pr  B B  Pr  | P A A r 
We sometimes call this relationship “unconditioning”. More important, we can 
manipulate it to yield:
Pr[ | ] Pr | Pr
Pr
A B
B A A
B     
  (3.2)
This is the simplest realization of Bayes’ Rule which was initially formulated by 
Sir Thomas Bayes during the 18th century and was first published in 1763, two 
years after his death. Sir Bayes was investigating incidence rates of infectious dis￾eases. To obtain the fully expanded expression of Bayes’ Rule, we first construct 
the Law of Total Probability.Probability Basics 23
Example 3.12
Let C x   { | X X 1 2  4} and let D1 1 = = { | x X 1}, D2 1 = = { | x X 2}, D3 1 = = { | x X 3}, 
D4 1 = = { | x X 4}. Then:
Pr P   C D 1 1  r |   C D   1 . 4 0 25 Pr .   D1  0 25
Pr P   C D 2 2  r |   C D   1 . 4 0 25 Pr .   D2  0 25
Pr P   C D 3 3  r |   C D   1 . 4 0 25 Pr .   D3  0 25
Pr P   C D 4 4  r |   C D  0 Pr .   D4  0 25
From these, we compute:
Pr .   C   3
16 0 1875
To state the Law of Total Probability in words, we might say that the probability 
of an event may be computed as the sum of the probabilities of its intersections 
with the events that comprise a partition of the sample space.
Recall that for any two events, say C and D:
C C D C D C D C D c c         and         
so
Pr P C Cr P D C r Dc         
 

Now, the “unconditioning” relationship allows us to state that:
Pr P   C D  r[C D| ]Pr  D and Pr P C D r | C D Pr D c c c   
 
    
so
Pr P C Cr |D D Pr Pr C D| Pr D c c         
 
 
 
 (3.3)
This is the simplest form of the Law of Total Probability – simplest in the sense 
that only D and Dc are used to partition the sample space. If, rather than using 
only D and its complement, we extend to a partition of the sample space, say 
D1 2 , , D D … n then:
Pr P C Cr P D Cr |D D Pr
i
n
i
i
n
        i i    
  1 1
(3.4)
This is the general statement of the Law of Total Probability. Take a look at it and 
use the example of the four-sided dice to try it out.24 Probability Foundations for Engineers
Using the Law of Total Probability, we can extend the earlier conditional prob￾ability statement that:
Pr[ | ] Pr[ | ]Pr
Pr
A B
B A A
B   
 
To the form
Pr[ | ] Pr[ | Pr ]
Pr
Pr[ | Pr ]
Pr[ | Pr
A B
B A A
B
B A A
B A
j
j j j j
i
n
i
  
    
 
1
 Ai]
(3.5)
This is the general form of Bayes’ Rule. It has many applications and forms a 
basis for many of the questions and analyses we pursue in probability.
Example 3.13
An inventory system contains four types of products. Customers order one unit of 
a product at a time: 20% of customers order the first type of product, 30% the sec￾ond type, 15% the third type, and 35% the fourth type. Due to the policy used to 
manage the inventory system, the supplier is out of the first type of product 6% of 
the time, the second 2% of the time, the third 12% of the time, and the fourth 1% 
of the time. When a customer orders a product that the inventory system does not 
have, the order cannot be filled, and the customer takes their business elsewhere. 
What is the probability that an order cannot be filled?
Let Ti
 denote the event that the customer orders product type i, i = 1, 2, 3, 4.
Let S denote the event that the order cannot be filled.
Pr S S Pr( |T T )Pr . . . . . .
j
   j j           


1
4
0 06 0 2 0 02 0 3 0 12 0 15
0. . 01  0 35 0  .0395
Suppose a customer order cannot be filled. What is the probability that the 
customer ordered the second type of product?
Pr( | ) Pr( | )Pr
Pr
. .
. T S . S T T
S 2
2 2 0 02 0 3
0 0395  0 1519  
     
Example 3.14
A cell phone manufacturer purchases display screens from two different suppliers 
– 40% of screens are from Reliable Video and 60% are from New Age Technology. 
While both suppliers are trying to meet the same longevity requirements, it has Probability Basics 25
3.5 INDEPENDENCE
The next topic of this chapter is that of independence. This is a difficult topic that 
many people find confusing. The basic idea is that the probability of occurrence of an 
event either is or is not influenced by the occurrence of another event. If the chance 
of occurrence of an event is affected by the chance of occurrence of another event, 
the two events are dependent and if the probability of occurrence is not affected, the 
events are independent. Formally:
Two events A and B defined on a sample space, are said to be independent if
PrA B  Pr A B Pr        (3.6)
Events that are not independent are dependent.
Using the simple form of Bayes’ Rule that was developed previously that
Pr[ | ]
Pr[ | ]Pr
Pr
A B
B A A
B   
 
implies that for independent events Pr[ | A B] P  rA. To see this, take
Pr[ | ]
Pr
Pr
Pr Pr
Pr
A B Pr
A B
B
A B
B  A   
 
    
 
  
The fact that for independent events Pr[ | A B] P  rA is intuitively appealing as 
it clearly indicates that knowledge of the occurrence of the event B has no influence 
on the probability of event A.
been found that the screens from Reliable Video have a 1 year survival rate of 
82% while those from New Age Technology have a 94% one year survival rate. 
(a) What fraction of the company’s phones will have screens that survive one 
year? (b) If a one year old phone is selected at random and found to have a failed 
video display, what is the probability that the screen was purchased from New 
Age Technology?
Pr Pr | Pr Pr | Pr
. . .
  E E    RV   RV    E NAT   NAT
   0 40 0  82    0 60 0  . . 94  0 892
Pr[ | ] Pr[ | ]Pr
Pr
. .
. NAT E . E NAT NAT
E
c
c
  
       0 06 0 6
0 108
0 33326 Probability Foundations for Engineers
Because it is not always obvious whether two events are independent, it is impor￾tant that independence be tested using either of the conditions:
PrA B    PrA B  Pr  PrA B| P   rA or
This is illustrated in Example 3.15.
Example 3.15
Two fair six-sided dice are rolled. Define the events:
A x   { | X X 1 2 is odd}
B x = { | X1is odd}
C x   { | X X 1 2  5}
D = = { | x X1 3}
Are events A and B independent? Are events C and D independent?
Pr P   A B    1 r P   A B r       4
1
2
1
2
1
4
so events A and B are independent.
Pr P   A B r P      10     r  A B   36
1
6
5
108
2
36
so events C and D are dependent. Try other combinations of these events 
yourself.
A note of caution is that many people confuse independence and the concept 
of mutually exclusive events. These are distinct ideas that should be distinguished 
and the difference must be recognized. One observation that may help with keep￾ing the ideas separate is that since independent events have Pr P   A B   r P   A B r 
and mutually exclusive events have Pr  A B   0 we must conclude that mutually 
exclusive events with non-zero probabilities of occurrence are dependent.
Next, observe that the definition of independence allows us to obtain some 
interesting relationships immediately. For example, if A and B are independent, 
then so are A and the complement of B; that is, since A and B are independent:
Pr P   A B   r P   A B r 
Now we know that:
A A B A Bc        
so:
Pr Pr Pr
Pr Pr Pr
A A B A B
A B A B
c
c
        
 

       
 
Probability Basics 27
EXERCISES
1. A random experiment consists of measuring the weight of the carbon dioxide 
emitted by a coal fired power plant during a 4 hour period. Identify the sample 
space for this experiment.
2. An experiment consists of selecting an acre of land in the Jefferson National 
Forest at random and counting the number of Cardinal nests in that parcel of land. 
Identify the sample space for this experiment.
3. An experiment consists of measuring the speed of randomly selected southbound 
vehicles as they pass the Connecticut Avenue exit on Interstate Highway 695 (the 
capital beltway). Identify the sample space for this experiment.
4. Suppose two events A and B are mutually exclusive and that PrA .    0 3 and 
PrB .    0 5. What are the probabilities that (a) either event occurs, (b) A occurs 
but B does not, and (c) both A and B occur?
5. A university bookstore accepts the Mastercard and Visa credit cards. 42% of the 
stores customers carry a Mastercard and 33% of customers carry Visa. If 14% of 
the store’s customers carry both cards, what percentage of the store’s customers 
carry a credit card the store accepts?
6. 92% of college students have laptop computers while 68% have iPod type ste￾reo devices. If 20% of college students own neither of these types of electronic 
devices, what is the probability that a student selected at random will own both of 
the two devices?
Pr P A B r P A B r Pr P A B r c c    
           
 
 1
In addition, if events A, B and C are independent, then A is independent of all 
events obtained using B and C, such as intersections and unions.
One final result that we should obtain for conditional probabilities and inde￾pendence is the multiplication rule. Consider a set of events   E E 1 2 , ,En and 
take their intersection. Using conditioning, we compute the probability of the 
intersection as:
Pr Pr | Pr
Pr |
E E E E E E E E E E
E E
n n n n
n
1 2 1 2 1 1 2 1
1
            

 
           

E E   E E E E   E E E
E E
n n n n
n
2 1 1 1 2 2 1 2 2
1
Pr | Pr
Pr  |   E E 2 1  n n   Pr  E E 1 1 | P   E E 2 2  n  r |   E E 2 1 Pr  E1
Then, if the events are independent, this reduces to:
Pr P E E E E n r
i
n
1 2 i
1
       
This useful relationship assures that we can compute the probability of occur￾rence for intersections of events and assures that for independent events, the cal￾culation is relatively easy.28 Probability Foundations for Engineers
7. For two events A and B of a sample space, what is the value of 
PrA B     PrA B?
8. 88% of the engineering students at a particular university have TI 89 calculators 
while 65% have tablet-type pcs. If 6% of that student group owns neither of 
these types of electronic devices, what is the probability that a student selected 
at random will own both of the two devices?
9. 65% of undergraduate students have automobiles while 42% have bicycles. If 
27% of college students own neither a car nor a bike, what is the probability that 
a student selected at random will own both a car and a bike?
10. Two fair six-sided dice are rolled. Let A be the event that the sum of the numbers 
on the dice is odd and let B be the event that the first die shows an odd number. 
Compute PrA B  .
PrA B    PrA B   Pr    PrA B     1
2
1
2
1
4
3
4
11. A small community organization consists of 20 families, of which 4 have one 
child, 8 have two children, 5 have three children, 2 have four children and 1 has 
five children.
a. If one of the families is chosen at random, what is the probability that it 
has two children?
b. If one of the children is chosen at random, what is the probability this 
child comes from a family having two children?
12. Let U and W be two events of a sample space. Show that the probability that 
exactly one of these events occurs is PrU W   Pr    PrU W  2 .
13. For two sets, S and T, in a sample space show that S T ⊂ implies that T S c c ⊂ .
14. An electronic device is comprised of 3 components each of which has a prob￾ability of 0.90 of proper function, independent of the status of the other compo￾nents. The device will function if 2 or more of the components function properly. 
What is the probability that the device functions properly?
15. A couple has one female child and is expecting a second birth soon. Given that 
the first child was a girl, what is the probability that both children will be girls.
16. Two fair six-sided dice are rolled. Let A be the event that the sum of the numbers 
on the dice is odd and let B be the event that the first die shows an odd number. 
Compute PrA B  .
17. Two fair six-sided dice are rolled. Let A be the event that the sum of the numbers 
on the dice is less than or equal to 6 and let B be the event that the first die shows 
an even number. Compute PrA B   and Pr A B c    

.
18. A pair of six-sided dice is tossed. What is the probability that the second die 
shows a higher value than the first die?
19. Two fair six-sided dice are rolled. Let A be the event that the sum of the numbers 
on the dice is less than or equal to 8 and let B be the event that the first die shows 
an even number. Compute PrA B   and Pr A B c    

.Probability Basics 29
20. Suppose that for two events A and B, PrA .    0 75 and PrB .    0 60. Show that 
PrA B  .    0 35. The general form of this relationship is called Bonferroni’s 
inequality.
21. Two cards are drawn at random from a standard poker deck. What is the prob￾ability they have the same value?
22. If two fair six-sided dice are rolled, what is the conditional probability that the 
first die lands on 4 given that the sum of their numbers is 8.
23. A random experiment consists of tossing two fair six-sided dice and record￾ing the sum of the two numbers observed. For this experiment compute 
Pr[ | X X X ] 1 1   5 7  2 9 and Pr[ | 7 9 5]   X X 1 2   X1 .
24. A random experiment consists of tossing two fair four-sided dice and record￾ing the higher of the two numbers observed. (a) Identify the sample space for 
the experiment. (b) State the probability of occurrence for each element of the 
sample space. (c) Compute the probability that the smaller observed value was a 
2 given that the larger value was a 3.
25. A four-digit binary number, n, that is to be transmitted across a network is 
formed by randomly selecting the sequence of constituent digits as zeros and 
ones, each having equal probability; that is, n = n n1 234 n n where each ni has an 
equal probability of being a one or a zero. Let A be the event that n ≤ 1010 and 
B be the event that n is even. How many values of n are possible? Determine 
PrA and Pr[ | A B].
26. An experiment consists of rolling a fair four-sided die until a 3 occurs. What is 
the probability that the number of rolls exceeds 6? What is the probability that 
the number of rolls is no greater than 5 given that the number exceeds 1?
27. If it is known that in a series of coin tosses, the 4th head occurred on the 12th trial, 
what is the probability that the 3rd head occurred on the 9th trial?
28. An experiment consists of rolling a fair six-sided die until a 4 occurs. What is the 
probability that the number of rolls exceeds 9? What is the probability that the 
number of rolls is no greater than 8 given that the number exceeds 2?
29. Suppose it is found from experimentation that an unfair six-sided die displays the 
conditional probability for the first occurrence of a 5 that Pr[ | k k   4 7] .  0 13. 
What is the value of the success probability p that a 5 occurs on any trial?
30. In reliability analysis, a parallel system functions successfully as long as at least 
one of the identical parallel components is functioning. Suppose a parallel sys￾tem of 3 components, each having reliability of 0.80, is functioning, what is the 
conditional probability that component number one is functioning? What is the 
conditional probability that it is failed?
31. Suppose we have 4 different unfair coins having probability of heads equal to 
0.62, 0.56, 0.52 and 0.70, respectively. If one of the coins is selected at random 
and flipped with the result that it shows heads, what is the conditional probabil￾ity that it was the third coin that was used?
32. The machining center at a production facility has three lathes of differing ages 
and thus precision. The oldest, Machine A, produces finished units of product of 
which 88% are good, 8% are blemished and 4% unusable. Machine B produces 
92% good, 6% blemished and 2% unusable while the newest machine, Machine 30 Probability Foundations for Engineers
C, turns out product that is 96% good, 3% blemished and 1% unusable. If 
Machine A produces 25% of the company’s output while Machine B turns out 
1/3 of the output, what fraction of the company’s product is good? What per￾centage is blemished? If a unit of product is selected at random and found to be 
blemished, what is the probability that it was produced on machine B?
33. 64% of the fire alarms in a building were manufactured by Acme and the rest 
were manufactured by Emca. Fire alarms are tested every 3 months and the 
test will give a false indication of failure with probability 0.04. The test will 
also give a false indication of proper function with probability 0.08. The Acme 
alarms have a failure probability of 0.18 while those from Emca have a failure 
probability of 0.15. If a test indicates that a particular alarm is failed, what is the 
probability that it was manufactured by Acme?
34. The machining center at a production facility has three lathes of differing ages 
and thus precision. The oldest, Machine A, produces finished units of product 
of which 88% are good, 8% are blemished and 4% unusable. Machine B pro￾duces 93% good, 5% blemished and 2% unusable while the newest machine, 
Machine C, turns out product that is 96% good, 3% blemished and 1% unusable. 
If Machine A produces 25% of the company’s output while Machine B turns out 
1/3 of the output, what fraction of the company’s product is good? What percent￾age is blemished?
35. A pouch contains two normal quarters (25 cent coins) and one quarter that has 
heads on both sides. If one of the 3 coins is selected at random and is flipped, what 
is the probability the result is heads? If the result of the toss is heads, what is the 
probability that the coin tossed is a normal quarter (with tails on the other side)?
36. Diagnostic tests for gluten allergy have a false negative probability of 0.025 and 
a false positive probability of 0.045. If 12% of college students have a gluten 
allergy, what is the probability that a student who receives a positive diagnosis 
actually has the allergy?
37. The test device for ABS braking system control circuits correctly recognizes 
internal short circuits 97% of the time and incorrectly indicates the existence of 
a short in 2% of good control circuits. If the control circuit production process 
actually generates control circuits having internal shorts at a rate of 2.5%, what it 
the probability that a control circuit selected at random and identified as having 
an internal short actually has one? What is the probability that a control circuit 
identified as having a short is actually good?
38. An optical imaging device is used to inspect engine bearings for surface flaws as 
they leave a machining process. The imaging device has a 98% success rate in 
recognizing flaws and also has a 4% error rate of indicating that flaw exists in an 
unflawed bearing. If the machining process actually generates flawed bearings 
at a rate of 1.25%, what it the probability that a bearing selected at random and 
identified by the imaging machine as flawed is actually flawed.
39. Units of a manufactured product that are to be inspected are known to be subject 
to three types of defects. 6% of the units have surface finish flaws, 4% have 
improperly fixed ground wires and 3% have base mounts that are not level. It 
is known that ¼% of the units have all three defect types and ½% have surface 
finish flaws and improperly fixed ground wires. In addition, we know that ½% Probability Basics 31
have the combination of only surface finish flaws and base mounts that are not 
level, and 1½% have only the unlevel base mount defect.
a. What is the probability that a unit selected at random will have both an 
improperly fixed ground wire and a base mount that is not level?
b. What is the probability that a unit selected at random will have only the 
improperly fixed ground wire defect?
c. What is the probability that a unit selected at random will have only the 
surface flaw defect?
d. What is the probability that a unit selected at random will be defect free?
40. In reliability analysis, a parallel system functions successfully as long as at least 
one of the identical parallel components is functioning. Suppose a parallel sys￾tem of 3 components, each having reliability of 0.80, is functioning, what is the 
conditional probability that component number one is functioning? What is the 
conditional probability that it is failed?
41. A firm that produces batteries for cell phones has 3 factories, say A, B and C, 
and these factories produce 15%, 35% and 50% of their product respectively. If 
the factories have defect rates of 1%, 5% and 2% respectively and a randomly 
selected battery is found to be defective, what is the probability that that the bat￾tery tested was produced at factory B?
42. In the design of automobile dashboards, warning lights have replaced gauges. 
The warning lights are intended to alert drivers of impending engine problems. 
Let T represent the event that an engine problem occurs and L represent the 
event that a warning light turns on. Suppose Pr[ | L T] = p and Pr[ | L T ] r c c = . 
Assuming that PrT .    0 01, use Bayes Rule to express Pr[ | T L] and Pr[ | T L ]
c
in terms of p and r. If the dashboard design displays p = 0.992 and r = 0.975
what are the resulting values for Pr[ | T L] and Pr[ | T L ]
c ?
43. The diagnosis of students believed to have mononucleosis proceeds as follows. 
An initial test is performed and the test has a false positive probability of 0.025 
and a false negative probability of 0.04. Students for whom the test result is nega￾tive are given a second test for which the false positive probability is 0.10 and the 
false negative probability is 0.01. The results for the two tests are independent. 
What is the probability that a student who does not have the disease receives a 
positive diagnosis? If 1.5% of the students on campus have the disease, what is 
the probability that a student who receives a positive diagnosis actually has the 
disease?
44. Two fair six-sided dice are rolled. Let A be the event that the sum of the numbers 
on the dice is odd and let B be the event that at least one of the dice shows a 1 or 
a 5. Determine whether these events are independent. What is Pr[ | A B]?
45. Two fair six-sided dice are rolled. Let A be the event that the sum of the numbers 
on the dice is odd and let B be the event that at least one of the dice shows a 1 or 
a 5. Determine whether these events are independent. What is Pr[ | A B]?
46. Show why mutually exclusive events each having non-zero probability of occur￾rence are not independent.DOI: 10.1201/9781003294382-4 33
Random Variables 
and Distributions 4
We have seen how the possible outcomes of a random experiment may be described 
using sets and how a probability measure can be applied to the events that may be 
observed. In order to provide a general method for representing experimental obser￾vations in a quantitative manner, we will define here the idea of a random variable. 
If we consider the observations of blue or white cars, we see that these do not allow 
us to do much analytically. The idea of the random variable is that we should assign 
a number to correspond to our observations so that we can perform analyses on the 
results of our experiments.
4.1 RANDOM VARIABLES
Usually, in engineering applications, we are interested in quantifying the results of 
a random experiment. We do this by representing experimental results with one or 
more abstract variables and then assigning values to the variables to correspond to 
the possible experimental outcomes. Once we associate a numerical value with each 
possible outcome of a random experiment, then we will have a quantitative repre￾sentation of our observations. Begin with a formal definition of a random variable.
A random variable is a real-valued function defined on a sample space.
Since a random variable is a function, its domain is the sample space and a set of real 
numbers is the range of the random variable.
It is as simple and as complicated as that. We define a mapping from each element 
of the sample space to a real number. We may do this in nearly any way we wish. 
Clearly, we will usually want to define “well behaved” functions – ones that are sin￾gle valued, perhaps invertible, perhaps differentiable – but while the form of the 
function may be important to the application, this is not required by the theory.
Example 4.1
When we roll a six-sided die, the set of observations we may actually make are 
the number of spots on the side of the cube that lands facing up. A logical map￾ping is:
one spot →1
two spots →2
three spots →3
four spots →4
five spots →5
six spots →634 Probability Foundations for Engineers
Example 4.2
Referring back to the example of the depth of a tire tread in the previous chapters, 
we had the sample space:
W = £ { | x x 7 5. . £ 8 5}
Clearly, an appropriate definition of the random variable is to map the depth read￾ings to the same numerical value. A reasonable alternative would be to map the 
readings to the difference between the observed depth and 7.5 mm; that is:
y = -x 7 5.
Example 4.3
When we flip a coin, the outcomes are H and T. We can map H to 1 and T to 0 or 
we could just as well map H to 10 and T to 5.
However, if there were a reason to do so, we could define the mapping as:
one spot →– 3
two spots →– 5
three spots → 0
four spots → 2
five spots → 5
six spots → 9
Both of these mappings are acceptable as definitions of the random variable 
representing our random experiment of tossing the die. Yet another admissible 
mapping is:
one spot →– 5
two spots →– 1.5
three spots → 5
four spots → 2.2
five spots → 8
six spots → 6.5
Clearly, the logic for such an assignment would be difficult to recognize, but the 
mapping is consistent with the definition and is thus acceptable.
It is reasonable to expect that definitions of random variables will be based on 
a logical interpretation of the elements of the sample space and the experiment 
for which they are intended.Random Variables and Distributions 35
4.2 DISTRIBUTIONS
Once we have defined a random variable, we would like to assign a probability mea￾sure to it. So the question is how do we obtain probabilities for random variables?
The answer is that we associate with the random variables the same probabilities 
that apply to the events to which they correspond. Suppose that for a particular exper￾iment, we have an event A comprised of outcomes ai
 as:
A = ¼ { } a a 1 2 an , , ,
For the experiment, we would presumably have been able to define PréëAùû. Now, 
suppose we define the random variable:
x x i i = ( ) a
Then for X x = { } ( ) a x 1 2 ( ) a x ¼ ( ) an , , , , we assign PréX A Pr ë ùû = éë ùû. It is particu￾larly important to note that we define the probabilities for random variables in terms 
of probabilities of events rather than outcomes.
Example 4.4
If we measure the ambient temperature in New York City at 8:00 am, we might 
map the thermometer reading to a Celsius scale or a Fahrenheit or even a Kelvin 
scale.
Random variables are typically denoted by italicized, capital letters. Specific 
values taken on by a random variable are typically denoted by italicized, lower￾case letters; that is, we might say that the name of our random variable is say Y
that represents the number of spots we observe when tossing a die. When we 
make the observation, we say Y = y which is to say that Y takes value y on a spe￾cific trial.
In an Industrial Engineering context, there are many types of phenomena that 
we may wish to study using a random experiment. Therefore, there are many pos￾sible definitions of random variables. In general:
A random variable that can take on at most a countable number of values 
is said to be a discrete random variable. A random variable that can take 
on an uncountable number of values is said to be a continuous random 
variable.
To summarize, we create a quantitative representation of the results of our obser￾vation of a random phenomenon by mapping the elements of the sample space 
to a set of real numbers. We have considerable latitude in how we do this so the 
resulting random variable can have whatever behaviors we believe will be use￾ful. Logically, it is usually but not always the case that discrete sample spaces are 
mapped to discrete random variables and continuous sample spaces are mapped 
to continuous random variables. A final point is that the definition of the random 
variable should be completely clear in the sense that the range of the variable 
should be clearly specified. For example, we must indicate when negative values 
are permitted.36 Probability Foundations for Engineers
Example 4.5
For the six-sided die, let X be the number of spots showing. Then, for the event B, 
corresponding to an even number of spots:
Pr P éX B = r . ë ùû 246 or or = [ ] = 0 5
and for the event C, corresponding to having the number of spots exceed 2:
Pr P [ ] X > 2 0 = r . [ ] C = 667
Example 4.6
In Chapter 3 (Example 3.10) for the case of the depth of tire tread, we computed 
Pr . [ ] F = 0 35 where F = £ { | x x 7 9. . 0 8 < 25}. It would be reasonable to define the 
random variable corresponding to the depth of the tread to be the same number; 
that is:
d = d x( ) = x
and then we have Pr . [ ] 7 90 8 £ £ d . . 10 = 0 35.
The guiding principle here is that we associate probabilities with random vari￾ables by transferring the probabilities of the events for which the sets of random 
variables are the images to those sets of random variables. It is important to realize 
that by transferring probabilities from events to sets of random variables, we main￾tain consistency with the three axioms of probability. Among the consequences 
of being consistent with the probability axioms is the fact that the relationship:
For events E E 1 2 and having E E 1 2 Í ,Pr P [ ] E E 1 2 £ r[ ] (4.1)
is transferred to the random variables. This implies that if we order the random 
variable in an increasing sequence, then as we include more and more of the val￾ues of the random variable in a set, the probability is non-decreasing. This assures 
that we can transfer the probability measure for both discrete and continuous 
random variables.
Specifically, once the random variables are defined and are expressed as real 
valued quantities, we organize their probabilities into distribution functions, and 
this enables us to manipulate the probabilities efficiently. Formally, the definition 
of a distribution function is:
FX ( ) x X = £ Pr[ ] x (4.2)
Notice the notation. The name of the random variable is an italicized capital 
letter. Specific values of the random variable are represented by italicized lower 
case letters.Random Variables and Distributions 37
The distribution function is often called the cumulative distribution function
(c.d.f.) because it represents the accumulation of probability over the increasing 
values of the random variable. We can confirm the fact that the definition of the 
distribution function is consistent with the three probability axioms and as a result 
we will see that it is always the case that:
0 £ F x X ( ) £ 1
and that the function is “right continuous”.
Mathematically, the c.d.f. has four important properties that assure its applica￾tion to random phenomena. These are:
1. The distribution function is non-decreasing. That is, for a b < , it must be 
the case that FX X ( ) a F £ ( ) b .
2. limx
F x X ®¥ ( ) = 1
3. limx
F x X ®-¥ ( ) = 0
4. F is right continuous. A decreasing sequence that converges to x, has 
limy x
F y X XF x
¯ ( ) = ( )
These properties hold for both discrete and continuous random variables. In both 
cases, the function is defined with respect to the entire real line. The realizations 
of these conditions are slightly different for discrete and continuous variables so 
we treat the two cases a little differently. Consider first an example of the discrete 
case.
Example 4.7
Suppose our experiment is comprised of tossing a coin three times and counting 
the number of heads. For this experiment, the sample space is:
W = { } HHH, , HHT HTH H, , TT THH T, , HT TTH,TTT
Suppose we assign our random variable as:
X ( ) TTT = 0
X H( ) TT = X T( ) HT = X ( ) TTH = 1
X ( ) HHT = X H( ) TH = X T( ) HH = 2
X ( ) HHH = 338 Probability Foundations for Engineers
4.2.1 Probability Mass Functions
For a discrete random variable, we can use the distribution function to obtain prob￾abilities for particular values of X. The resulting function is called a “probability 
mass function”.
For the above example, the pmf is:
PréX F = X . ë ùû 0 0 = ( ) = 0 125
PréX F = X X F . ë ùû1 1 = ( ) - ( ) 0 0 = 375
PréX F = X X F . ë ùû 2 2 = ( ) - ( ) 1 0 = 375
PréX F = X X F . ë ùû 3 3 = ( ) - ( ) 2 0 = 125
Now, we assign the probabilities to this random variable as:
FX 0 1
8 ( ) = = 0.125 FX 1 4
8 ( ) = = 0 5.
FX 2 7
8 ( ) = = 0.875 FX 3 8
8 ( ) = = 1 0.
Drawing a graph of this function, we obtain the image shown in Figure 4.1. 
Take particular note of the fact that the plot is for the distribution function. This 
is the base function for assigning probabilities to random variables. Note that the 
function is right continuous as it forms a step function by making an increase at 
discrete points, is non-decreasing and reaches the value of 1.0 when the full range 
of the random variable is attained. Not fully shown is the fact that the function has 
a value of zero in the negative domain and the value of 1.0 may be considered to 
continue for all values beyond X = 3.
x
Fx (x)
1 2 3
1/8
1/2
7/8
1
FIGURE 4.1 Example discrete distribution function.Random Variables and Distributions 39
In general, when we have the distribution function for a discrete random variable, 
we obtain the probability mass function as:
fX X ( ) x X = = é x F x FX x ë ùû Pr = ( ) - - ( ) 1 (4.3)
Note that a consequence of this construction is that:
F x X f j
j
x
( ) = X ( ) =
å0
(4.4)
Equations 4.3 and 4.4 formally express the facts that probabilities for individual 
values of a discrete random variable are obtained from the distribution function and 
that these probabilities can be added up to recover accumulated values for the distri￾bution function.
It is appropriate at this point to summarize the construction of a probability mea￾sure for a discrete random variable. For a random experiment that produces one of 
only a countable number of possible outcomes, we define a mapping from each out￾come to a real number. We call that number a realization of a random variable because 
it represents the observation we make in our random experiment. We then assign 
probabilities to the real numbers by transferring the probabilities on events – sets of 
outcomes – to the corresponding sets of images of the outcomes that comprise those 
events. This provides an analytical structure that we can use to study and describe 
possible observations of the experiment.
Example 4.8
In an automated warehouse, shelf space is allocated according to the sizes of 
the products to be stored. For a particular realization of this system, the stored 
items are cases of paint (in cans) and their daily arrival sequences to the ware￾house are determined by orders received. A case of 1 pint cans contains 12 cans 
and requires ¼ of a bin shelf while a case of 1 gallon cans contains 6 cans and 
requires ½ of a bin shelf. A 5 gallon can is handled as a single unit and requires 
40% of a bin shelf and a 10 gallon can is handled individually and requires 60% 
of a bin shelf. Our experiment consists of observing the next arriving product to 
be stored. Let:
W = { } w w1 2 , ,w w3 4 ,
where a w w 1 2 = = case of1 1 pint cans a , , case of gallon cans a w3 = 5galloncan
and w4 = a10 gallon can.
Suppose we have determined that Pr . [ ] w w 1 2 = 0 20 0 , Pr . [ ] = = 60, Pr . [ ] w3 0 15
and Pr . [ ] w4 = 0 05. Next, define our random variable as X j ( ) wj = . This definition 
allows us to state that:40 Probability Foundations for Engineers
4.2.2 Probability Density Functions
In the case of a continuous random variable, we again construct the distribution func￾tion as the base of our probability analysis. We do this in the same manner as we 
use for discrete random variables – we transfer probabilities associated with events 
to their sets of images. The distribution function usually looks like the curve in 
Figure 4.2. The notation for the distribution function is the same as it was in the 
discrete case. It is:
F x X ( ) = £ éX x ë ùû Pr . (4.5)
We obtain probabilities for images of specific events using the derivative of the 
distribution function. The derivative of the distribution function is called the proba￾bility density function (p.d.f.) and is denoted by:
f x
d
dx X X ( ) = F x( ) (4.6)
It is reasonable to think of the probability density function as the rate at which the 
distribution function is accumulating probability. As with all derivatives, it is a rate 
function.
Pr . [ ] X X = 1 0 = = 20, Pr . [ ] 2 0 = = 60, Pr . [ ] X X 3 0 = = 15 Pr[ ] 4 0 = . , 05
Pr . [ ] X X £ 3 0 = < 95, Pr[ 3 0 ] . = ³ 80, Pr . [ ] X X 2 0 = £ 80andPr[ ] 2 3 £ = 0 7. . 5
Clearly, we could construct many other probability statements.
500 1000 1500 2000 x
0.2
0.4
0.6
0.8
1.0
F x
FIGURE 4.2 An example continuous distribution function.Random Variables and Distributions 41
4.2.3  Survivor Functions
Once the distribution function on a random variable has been defined, probabilities 
can be computed for the complements of sample space events, and this is often use￾ful. In general, for a distribution function F x X ( ) the probability for the event that is 
the complement to the one corresponding to X x £ is called the survivor function (or 
the reliability function) and is represented by:
F x X X ( ) = > éX x X x F x ë ùû = - é £ ë ùû Pr 1 1 Pr = - ( ) (4.7)
Example 4.9
The ambient temperature at 8:00 am in New York city falls in the interval 
- £ 40 £ 120   F T F. Define X T( ) = T so that a plot of FX ( ) x might look like the graph 
in Figure 4.2. Then the p.d.f. (probability density function) for the temperature, 
evaluated at 75
F would be:
f d
dx X X ( ) 75 = F x( )|x=75
Note that we can obtain the probabilities associated with any specific event as:
Pr[x T X x T FX X x T F x T f u du
x T
x T
1 2 2 1 X
1
2
( ) £ £ ( ) = ( ) ( ) - ( ) ( ) = ( )
( )
( )
ò
So the probability that we observe the temperature to be in the interval 
75 85   F T £ £ F is given by:
Pr[x X 75 x F 85 X X x F 85 x F 75 X X 85 F f 75 X
75
85
( ) £ £ ( ) = ( ) ( ) - ( ) ( ) = ( ) - ( ) = ò ( ) u du
One very important point here is that we must always be careful to assure that 
we define our probability functions so that:
0 £ F x X ( ) £ 1
Usually, this means that we must be careful with our definition of the random 
variable as we define the probabilities on events so that:
Pr[ ] W = 142 Probability Foundations for Engineers
This definition applies to both discrete and continuous random variables. The 
realization of Equation (4.7) for the discrete random variable is:
F x X f j f j
j x
x
X
j x
x
( ) = ( ) = - X ( ) = + =
å å 1
1
max
min
(4.8)
and for a continuous random variable, it is:
F x X f u du f u du
x
X
x
( ) = ( ) = - X ( )
¥
-¥
ò ò 1 (4.9)
Since there is no probability mass associated with an individual value, x, having x
as a limit on both integrals does not introduce any error in the computation.
Example 4.10
In Example 4.8, the probability of large format containers is:
FX ( ) 2 3 = = X X ù + 4 0 20 û éë é = ë ù
û Pr Pr = .
Example 4.11
In Example 4.9, the probability that the morning temperature in New York exceeds 
95 degrees is:
FX X 95 f u du
95
120
( ) = ( ) ò
For both discrete and continuous random variables, the above definitions of the 
distribution function and the corresponding probability mass function or prob￾ability density function are general. In engineering applications, specific realiza￾tions of these functions are used to model observed behavior. In some cases, 
the form of the distribution function that is used is obtained empirically – from 
direct observation. More often, the model is defined using one of a collection of 
previously identified distributions that have been found to represent particular 
phenomena well. In the sections that follow, these models and their usual appli￾cations are enumerated.Random Variables and Distributions 43
4.3 DISCRETE DISTRIBUTION FUNCTIONS
As illustrated in Example 4.7, a discrete distribution function may be constructed 
on the basis of a logical understanding of a process or on the basis of observations 
of the random experiment for which the distribution is to be used. In constructing 
the distribution, one has considerable latitude but must conform to the axioms of 
probability. To emphasize this point, suppose it has been observed that a particular 
process displays:
Pr , Pr . , Pr . Y , = c Y c Y c éë ùû = = éë ùû = = éë ùû 0 = 10 1 1 25
10 2 1 75
10 and 
Pr Y . = c éë ùû 3 = 10
Since the probabilities must sum to 1.0, we conclude that c = 2 and:
F y
y
y
y
y
y
Y ( ) =
-¥ < <
£ <
£ <
£ <
£ < ¥
ì
í
ï
ï
ï
î
ï
ï
ï
0 0
0 1
1 2
2 3
3
0 2
0 45
0 80
1 00
.
.
.
.
Experience over time has led us to recognize that it is often unnecessary to construct 
an empirical distribution function for a discrete random variable because many phe￾nomena display recognizable behavior patterns. Those patterns are sufficiently com￾mon that specific forms of discrete distribution functions have been defined to model 
them. In addition, these general models appear to be typical of particular types of 
phenomena. We examine the most commonly used models here. These are the:
1. Bernoulli
2. Binomial
3. Multinomial
4. Hypergeometric
5. Poisson
6. Geometric
7. Negative binomial
A point to keep in mind as we discuss these models is that they are actually families 
of distributions. Each specific realization of one of the models is obtained by select￾ing its parameter values.
4.3.1  The Bernoulli Distribution
Taking these in turn, consider first the Bernoulli random variable. It is used as a model 
for sample spaces having binary outcomes. The classic example is coin tossing, but 
it also applies to product inspections and may other physical systems. Experiments 44 Probability Foundations for Engineers
having only two possible outcomes are often called Bernoulli trials. The idea is that 
the sample space has exactly two possible outcomes: W = { } w w1 2 , .
In the case of coin tossing, W = { } H T, and we can let H and T represent both the 
outcomes and the corresponding events. If we map the sample space to the random 
variable X as X H( ) = 1 0 , X T( ) = and if we assign the probabilities as 
PréH p, Pr T q p ë ùû = éë ùû = = 1- , then the p.m.f. is:
fX x p p p q x x x x x ( ) = - ( ) = = - - 1 0 1 1 1 , ,
What then is the distribution function?
F p X ( ) 0 1 = - ( ) = q
FX ( ) 1 1 =
Note that specifying the possible values of the random variable is part of the pro￾cess of defining the distribution or p.m.f. In many cases, the possible values of the 
random variable are obvious and can be omitted but as a general rule, one should be 
careful to assure that the definition is clear.
Note further that a Bernoulli distribution has a single parameter, p. Selecting a 
value for p defines which realization of the family of distributions is to be used to 
model the experiment of interest.
Example 4.12
Suppose we test a fire alarm every three months. If the probability that the alarm 
is faulty is p = 0 0. 8, we define the random variable:
X = ì
í
î
1
0
if the alarm is faulty
if the alarm is functioning
and we find:
fX ( ) 0 0 = .92 FX ( ) 0 0 = .92
fX ( )1 0 = .08 FX ( )1 1 =
4.3.2  The Binomial Distribution
The binomial distribution provides a model for the number of events that occur in a 
fixed number of Bernoulli trials. We can use it to represent the number of heads in 
20 coin tosses, the number of defective parts found in the inspection of the output 
of a production process or any other situation where there are two possible catego￾ries of observations and we wish to know how many are in one of the categories. Random Variables and Distributions 45
It is common practice to refer to the category of interest as a “success” even if that 
corresponds to finding a defective unit of product. Essentially, a binomial random 
variable can be viewed as a sum of Bernoulli random variables. This will be verified 
formally later in this chapter.
Now to construct, the p.m.f., consider that we plan to toss a particular coin n = 20 
times and to count the number of times the coin lands showing heads. What is the 
probability that nine heads occur? Let PréH p, Pr T q p ë ùû = éë ùû = = 1- so:
Pr 9 1 9 11 é Heads ë ùû = - Cp ( ) p
where the binomial coefficient C represents the number of ways the 9 heads can be 
dispersed among the 20 tosses; that is, the two sequences:
{ } 0, , 010,,, 0 1,,, 000,,, 1 1 0,,,, 111 0 0, ,,, 1 1 0
and{ } 0 1,,, 1 00000 ,,,, ,,,, 1 1 1 0,,, 111,,, 0 010, ,0
both have 9 heads. How many such sequences are there? The answer is 
20
9
æ
è
ç ö
ø
÷ which 
is read 20 things taken 9 at a time or equivalently, the “combination of 20 items, 9 at 
a time”. This notation is interpreted as:
20
9
20
9 11
æ
è
ç ö
ø
÷ = !
! !
or in general:
n
k
n
k n k
æ
è
ç ö
ø
÷ = ( ) -
!
! ! (4.10)
It represents the number of distinct ways that n items can be arranged with k being 
of one class and (n–k) being of the other class.
The origin of the binomial coefficient is binomial theorem which states that:
a b n
j
a b n
j
n
j n j ( + ) = æ
è
ç ö
ø
÷
=
-
å0
(4.11)
The reader is encouraged to confirm this relation for n = 3 and n = 4.
Combining our ideas to this point, we find that for our coin tossing example:
fX 9 p p
20
9
1 9 11 ( ) = æ
è
ç ö
ø
÷ ( ) -46 Probability Foundations for Engineers
and so for the general case, the p.m.f. for the binomial distribution is:
f x n
x
p p n
x n x X p p x n x n x x n x ( ) = æ
è
ç ö
ø
÷ ( ) - = ( ) - ( ) - £ £ - - 1 1 0 !
! ! , (4.12)
The corresponding distribution function is:
F x n
j X p p
j
x
j n j ( ) = æ
è
ç ö
ø
÷ ( ) -
=
-
å0
1 (4.13)
As a standard notation, many people refer to the binomial c.d.f. as B(x,n,p) and to 
the p.m.f. as b(x,n,p). Using this notation:
F x X B x n p b j n p f j
j
x
j
x
( ) = ( ) = ( ) = X ( ) = =
, , å å , ,
0 0
(4.14)
To complete our discussion of the coin example, suppose the coin in question is 
“fair” (e.g. p = 0.5), what is the probability of 9 heads in 20 tosses?
fX 9 p p
20
9
1 0 16 9 11 ( ) = æ
è
ç ö
ø
÷ ( ) - = .
In addition, recognizing that a distribution function must apply to every possible 
value of a discrete random variable, one may compute the probabilities shown in 
Table 4.1. Note that the symmetry around x = 10 in the values of the p.m.f. is typical 
of binomial distributions.
Clearly, the binomial distribution is useful in analyzing coin tosses but it is also 
useful in engineering applications. The earliest application of the binomial distribu￾tion in Industrial Engineering was probably in the applications to quality control. In 
the inspection of units of product, the binomial distribution provides a useful model 
TABLE 4.1
Full Distribution and Mass Function Values for x Heads in 20 Coin Tosses
x FX ( ) x f x X ( ) x FX ( ) x f x X ( ) x FX ( ) x f x X ( )
0 ~0 ~0 7 0.1316 0.0739 14 0.9793 0.0370
1 ~0 ~0 8 0.2517 0.1201 15 0.9941 0.0148
2 0.0002 0.0002 9 0.4119 0.1602 16 0.9987 0.0046
3 0.0013 0.0011 10 0.5881 0.1762 17 0.9998 0.0011
4 0.0059 0.0046 11 0.7483 0.1602 18 ~1.0000 0.0002
5 0.0207 0.0148 12 0.8684 0.1201 19 ~1 ~0
6 0.0577 0.0370 13 0.9423 0.0739 20 ~1 ~0Random Variables and Distributions 47
for the number of defective (or non-conforming) units that will be found in an inspec￾tion sample. More recently, it has been used to represent the number of patients in a 
cohort who respond positively to a particular treatment, for the number of customers 
who select a particular brand of a product, for the number of airline passengers who 
might present risks of destructive acts and for many other similar situations.
Example 4.13
Suppose that an injection molding process for automotive interior door panels 
tends to generate defective panels at a rate of about 0.8%. What is the probability 
that a sample of 125 panels will contain 2 or fewer defective panels?
For this case, p = 0.008 so F
j X
j
j j 2
125
0 008 0 992 0 92
0
2
125 ( ) = æ
è
ç ö
ø
÷( ) ( ) =
=
-
å . . . .
In closing, we should note that the binomial distribution is called a two parameter 
distribution because it is defined by the two parameters n and p.
4.3.3  The Multinomial Distribution
The multinomial distribution provides a model for observations that are subject to 
more than two classifications. It can be viewed as a generalization or as an extension 
of the binomial distribution. One familiar example is when we are rolling a die and 
recording how many times each face comes up. In this case there are six classes of 
outcomes, so our observation space is the vector x x 1 234 x x x x 5 6 { } , , , , , . The p.m.f. for 
this random variable is:
f x x x x x x n
x x x x x x X p p p p x x x x 1 2 3 4 5 6
1 234 5 6
1 2 3 4 1 234 , , , , , !
! ! ! ! ! ! ( ) = p p
x n i x n
x x
i
i
i
5 6
1
6
5 6
0
,
£ £ , , " =
=
å (4.15)
and the c.d.f. is obtained as an appropriate summation. Observe that we could start 
with this model and simplify the analysis to the question of observations of one of the 
xk and the result is the binomial distribution.
Example 4.14
Batches of integrated circuits that are produced for assembly in cell phones and in 
portable computers are subject to two key types of defects. They may have a short 
circuit due to material bridging between circuit paths or a short circuit due to an 
absence of conductive material in a circuit path. If we inspect a sample of these 
components, we would classify each unit as w1 2 = = non-defective b ,w ridging short48 Probability Foundations for Engineers
4.3.4  The Hypergeometric Distribution
Models of sampling processes such as those used to describe quality inspections or 
disease incidence are usually binomial. Implicit in those binomial models is the view 
that the sample is small relative to the size of product or patient population so the 
probability of observing a “success” is constant. Often this is a reasonable view and 
the binomial model is appropriate. However, there are cases in which the population 
being observed is small or in which the observation process itself alters event prob￾abilities. In these cases, the hypergeometric distribution provides a more accurate 
model of the experiment.
Example 4.15
Consider a college probability course having an enrollment of 125 students of 
which 20 have not previously taken the prerequisite course in differential calcu￾lus. (They are taking it concurrently with the probability course.) If 10 students are 
asked to respond to a survey concerning their prerequisite courses, what is the 
probability that 4 of the students without the prerequisite course will be identified?
To answer this question, note that there are 125
10
æ
è
ç ö
ø
÷ possible survey samples that 
might be selected. Note further that there are 20
4
æ
è
ç ö
ø
÷ possible sets of 4 students 
without the prerequisite course and there are 105
6
æ
è
ç ö
ø
÷ possible sets of 6 students 
having the prerequisite course. Therefore, the probability that the survey includes 
X = 4 students without the prerequisite course is:
P X[ ] = =
æ
è
ç ö
ø
÷
æ
è
ç ö
ø
÷
æ
è
ç ö
ø
÷
4 =
105
6
20
4
125
10
0.044
or w3 = material void short. Let xk represent the number of units in a sample of 
n = 50 chips that belong in category k, then our probability model is:
Pr , , , , !
! ! !
X x X x X x f x x x n
x x x X p p p x x x 1 1 2 2 3 3 1 2 3
1 2 3
1 2 3 1 2 3 [ ] = = = = ( ) =
For P p = ( ) 1 2 , , p p3 = ( ) 0. , 975 0. , 015 0. , 010 and x x 1 2 = = 48, ,1 1 x3 = :
f n
x x x X p p p x x x 48 11 50
48 1 1
0 975 0 01 1 2 3
1 2 3
48 1 2 3 , , !
! ! !
!
!!! ( ) = = ( ) . . ( ) 5 0( ) . . 010 = 0 109Random Variables and Distributions 49
Example 4.16
Suppose a small production lot of a product contains 40 units of which 5 are 
defective. What is the probability that a sample of 8 units contains 1 that is 
defective? Observe that when the sample is taken, the chance that the first unit 
inspected is defective is 1 in 5 and if it is defective, the chance that the next one 
is also defective is 1 in 4. The chances for the observations change as the process 
proceeds. Thus:
P X[ ] = =
æ
è
ç ö
ø
÷
æ
è
ç ö
ø
÷
æ
è
ç ö
ø
÷
1 =
35
7
5
1
40
8
0.437
The general form for the Hypergeometric distribution is:
f x P X x
N m
n x
m
x
N
n
X ( ) = = [ ] =
-
-
æ
è
ç ö
ø
÷
æ
è
ç ö
ø
÷
æ
è
ç ö
ø
÷
(4.16)
where N is the size of population, n is the size of the sample and m is the number 
of population members which fall in the “success” category. Thus, for this three 
parameter distribution, the fact that observation probabilities change requires 
that the numbers of elements in the population must be used to compute event 
probabilities.
4.3.5  The Poisson Distribution
One of the most commonly used and important probability models in Industrial 
Engineering and in many other domains is the Poisson distribution. The reason this 
distribution is so important is that there are very many phenomena which appear 
to be well represented by it. Customer arrivals, employee and student absenteeism 
counts, defects per square area in metals and in fabrics, bird nesting patterns, meteor 
showers, and many other types of occurrences seem to display frequency behavior 
that is consistent with the Poisson model.
For the Poisson, the general form of the p.m.f. is:
f x e
t
x X x t
x
( ) = ( ) £ < ¥ -l l
! , 0 (4.17)
The associated distribution function is:
F x e
t
j X
j
x
t
j
( ) = ( )
=
-
å0
l l
! (4.18)50 Probability Foundations for Engineers
For this distribution, we say that the parameter λ represents the rate for the pro￾cess. The Poisson is a one parameter distribution and that parameter is λ.
Example 4.17
Incoming calls to a call center arrive at a rate of l = 4 / min . ute What is the prob￾ability that the number of calls in any minute exceeds 6? What is the probability 
that the number of calls in 5 minutes is fewer than 16?
To answer the first question, we take t = 1 and compute the survivor function:
F X X F
e j
X X
j
j
6 6 1 0 6 1 0 6
1 0
4
1 0
0
6
4
( ) = > = - [ ] £ = - ( )
= - ( ) =
=
-
å
Pr[ ] . Pr .
. ! . - = 0. . 889 0 111
To answer the second question, we take t = 5 and compute:
F e j X
j
j
16
20
0 221
0
16
20 ( ) = ( ) =
=
-
å ! .
Note that in the general case, the quantity t need not necessarily represent time. It 
is often used to represent time but any meaningful measure that is consistent with 
the definition of is λ is acceptable. In addition, there are cases in which a single 
“time” unit is assumed. In those cases, we have the simplified forms:
f x e x F x e j X
x
X
j
x j
( ) = ( ) = -
=
-
å l l l l
! ! and
0
where the range of the random variable remains 0 £ < x ¥.
4.3.6  The Geometric Distribution
The geometric distribution provides a model for how many Bernoulli trials will 
be required before the first time a particular event (a success) occurs. One familiar 
example is the number of rolls of a six-sided die before a 4 occurs. For a fair die, the 
probability that any roll yields a 4 is p = 1
6 so the probability that the number of rolls 
until the first time a 4 is observed equals k must be:
fK k K k p p k k
( ) = = éë ùû = - ( ) £ < ¥ -
Pr 1 1 , 1
(4.19)
which is to say, the first k -1 rolls must yield an observation other than a 4 and the 
final roll must yield a 4. Thus, we can compute fK 6 5
6
1
6 0 067 5
( ) = ( ) ( ) = . .Random Variables and Distributions 51
Of course, we can also compute values of the distribution function. By the usual 
definition:
F k K K k p p
j
k
j
( ) = £ éë ùû = - ( ) =
-
Pr å1
1
1 (4.20)
With a little effort we can simplify this summation as follows:
F k K p p p p p p
j
k
j
j
k
j
r
r
r k
( ) = - ( ) = - ( ) = - ( ) -
=
-
- =
-
-
=
¥
=
¥
å å å å 1
1
1 0
1
1
0
1 1 1 1
1
1 1
1 1
1
0
( ) - æ
è
ç
ç
ö
ø
÷
÷
= - - ( ) - - ( ) ( ) - æ
è
ç
ç
ö
ø
÷
÷
=
- =
¥
-
å
p
p
p
p p
p
r
k
r k
r k
p
p
p
p
p
p
p
p k k k
- - ( ) - - ( )
æ
è
ç
ç
ö
ø
÷
÷ = - ( ) - æ
è
ç ö
ø
1 ÷ = - ( ) - 1
1 1
1 1 1 1 1
Now that we see the result, we recognize that it is conceptually reasonable. The 
first 4 (or any event of interest) will occur on or before trial k provided it is not the 
case that the event is not observed on any of the k trials.
Example 4.18
In the development of single shot weapons such as tactical missiles, a “test ana￾lyze and fix (TAAF)” regime is used to find system faults. A sequence of firings 
is performed until a missile fails to fire properly. The device is then subjected to 
diagnosis to determine and fix the cause of the failure. If this procedure is imple￾mented for a population of missiles for which the failure probability is p = 0. , 035
what is the probability that a failure occurs on the 6th firing and what is the prob￾ability that the number of firings required to find a failure exceeds 12.
fK 6 0 965 0 035 0 029 5
( ) = ( ) . . ( ) = .
and
FK 12 0 965 0 652 12 ( ) = ( ) . . = .
4.3.7  The Negative Binomial Distribution
The negative binomial distribution may be viewed as a generalization of the geo￾metric distribution. It represents the probabilities for the number of Bernoulli trials 
required to obtain a fixed number of successes; that is, rather than consider only the 
number of trials until the first success, we can consider the number of trials until any 
selected number of success has occurred.52 Probability Foundations for Engineers
If we are counting tosses of a six-sided die and observing 4’s, the negative bino￾mial can be used to compute the probability that the third 4 occurs on any, say the 12th
toss. We can construct the p.m.f. for the negative binomial distribution logically. How 
can we have the 3rd 4 occur on the 12th toss? The answer is that the first 11 tosses must 
include two 4’s in any feasible way and then the final toss must yield a 4. Now the 
probability of two 4’s in 11 tosses is given by the binomial as b( ) 2 1, , 1 p and the prob￾ability that the final toss yields a 4 is p. Therefore, the negative binomial probability 
that the 3rd success occurs on the 12th trial is:
fK 12 b p 12 3 2 pb 11 p p p p
11
2
1
11
2
1 2 9
( ) = ( ) = ( ) = æ
è
ç ö
ø
÷ ( ) -
= æ
è
ç ö
ø
÷
- , , , ,
p p 3 9 3 9
1 11
2 9
1
6
5
6 ( ) - = ( ) ( ) = 0 049 !
! ! .
In general, we will observe the xth success on the kth trial if we observe x -1 suc￾cesses in k -1 trials and then a success on the final trial; that is:
f k b k x p pb x k p
k
x K p p x k x k x ( ) = ( ) = - ( ) - = -
-
æ
è
ç ö
ø
÷ ( ) - £ < - - 1 1 1
1
1 , , , , 1 , ¥ (4.21)
The distribution function is:
F k B k x p b j x p
j
x K p p
j x
k
j x
k
x j
( ) = ( ) = ( ) = -
-
æ
è
ç ö
ø
÷ ( ) - -
=
-
=
å å 1 1 1
1 , , , , 1 -x
(4.22)
and is interpreted as the probability that the xth success occurs on or before the kth
trial.
Note the correspondence and the difference between the negative binomial distri￾bution and the binomial distribution. For the binomial distribution, the number of 
trials is fixed and the random variable is the number of successes while for the nega￾tive binomial distribution, the number of successes is fixed and the random variable 
is the number of trials.
Because of their dual relationship, we can express the c.d.f. for the negative bino￾mial distribution in terms of that for the binomial distribution. The relationship is:
B k x p B x k p - ( ) = - ( ) - 1 , , 1 1, , (4.23)
This equation reflects the fact that the xth success occurs on or before the kth trial 
only if the number of successes in k trials is x or greater (not x -1 or fewer).Random Variables and Distributions 53
Example 4.19
In testing a new software product, a manufacturer undertakes to update the prod￾uct when the third fault is found by a member of the population of test users. If 
each user has a probability of p = 0 0. 4 of encountering a fault, what is the prob￾ability that 20 tests are performed before an update is made. What is the prob￾ability that more than 40 tests are required to find four faults?
f b K 20 20 3 0 04
19
2
0 04 0 96 0 0055 1 3 17
( ) = ( ) = æ
è
ç ö
ø
÷( ) ( ) = - , , . . . .
FK 40 1 4 B B 0 3 0 04 2 40 0 04 0 786 1 ( ) = - ( ) = ( ) = - , , . , , . .
4.4 CONTINUOUS DISTRIBUTION FUNCTIONS
As in the case of discrete random variables, it may be appropriate to construct an 
empirical distribution for the probabilities that describe an experiment. Once again, 
it is important to comply with the axioms of probability and this usually means assur￾ing that the probability model integrates to one.
Example 4.20
Suppose the random variable X has the probability density function:
f x X ( ) = - c x ( ) 1 0 £ £ x 1 2 ,
What is the value of the constant c? We obtain the answer by constructing the 
distribution function and evaluating it at the maximum value of the random vari￾able. Thus:
FX x f u u c u u c u u c x x
x
X
x x
( ) = ( ) = - ( ) = - æ
è
ç ö
ø
÷ = - æ
è
ç ö
ø ò ò
0 0
2 3
0
3 1 1
3
1
3
d d ÷
F c c X 1 1 c c 3
2
3
3
2 ( ) = = - = so =
Before leaving Example 4.20, note that it is often important to provide a more 
complete definition of a density or distribution function than was given in the 
example. A more appropriate specification of the density function would have 
been:54 Probability Foundations for Engineers
4.4.1  The Exponential Distribution
The times between arriving customers to a service provider and the length of equip￾ment operating intervals prior to failure are two of the many phenomena that seem 
to be well represented by the exponential distribution. The defining feature of this 
distribution is that short durations are more likely than long ones. The c.d.f. for the 
exponential distribution is:
F t T e t t ( ) = - < < ¥ - 1 0 l , (4.24)
and the corresponding density function (p.d.f.) is:
fT t e t ( ) = - l l (4.25)
Figure 4.3 shows a plot of the exponential density for the case in which l = 0 0. . 1
Observe that the plot implies greater probabilities for small values than for larger 
ones. One can verify this by noting that
Pré50 £ £ 60 60 50 0.058 ë ùû T F = T T ( ) - F ( ) =
while
Pré250 £ £ 260 260 250 0.008 ë ùû T F = T T ( ) - F ( ) =
f x
x
c x x
x
X ( ) =
-¥ < <
( ) - £ £
< < ¥
ì
í
ï
î
ï
0 0
1 0 1
0 1
2
Except where the simple definition is completely clear, the more formal defini￾tion should be used.
Again as in the case of the discrete random variables, observation of our natural 
environment and of our manufacturing processes causes us to see that the proba￾bilities of occurrence for events in these domains often appear to display a recog￾nizable pattern. There are several of these commonly occurring patterns that can 
be described in the form of a continuous distribution function. While there are 
relatively many available models, the seven most commonly used models are the:
1. Exponential
2. Gamma
3. Weibull
4. Beta
5. Normal
6. Lognormal
7. UniformRandom Variables and Distributions 55
Clearly, the random variable for the exponential distribution is always non￾negative. The single parameter, l, is called the rate parameter as it characterizes the 
rate at which arrivals occur. The survivor function represents the probability of a 
longer wait for an arrival or, in the case of equipment failure times, the survivor func￾tion represents the probability that failure does not occur prior to a particular time. 
This is often called the reliability of the equipment.
Example 4.21
An insurance call center receives calls at a rate of 4 per minute. What is the prob￾ability that the time between two calls is (a) less than 5 seconds (b) greater than 30 
seconds and (c) between 10 and 20 seconds? The rate of 4/min. means l = 4 so:
Pr sec. min. . . éT e £ = ë ù
û = - = - 5 1
12 1 0 283 0 333
Pr[ sec. . min.] . . T e > = = = - 30 0 5 0 135 2 0
Pr sec min sec. min.
. .
10 1
6 20 1
3
1
3
1
6 0 736 0 48
é = £ £ = ë ù
û
= ( ) - ( ) = -
T
F F T T 6 0 = .250
Example 4.22
An electronic component has a failure rate of l = - 10 4 / . hr What is its 20000 hour 
reliability?
FT 20000 1 0 e 865 2 ( ) = - = - .
100 200 300 400 500
0.002
0.004
0.006
0.008
0.010
FIGURE 4.3 Representative exponential density function.56 Probability Foundations for Engineers
4.4.2  The Gamma Distribution
The gamma distribution is defined for a non-negative random variable. Often, that 
variable is interpreted as time but other applications also exist. The algebraic form of 
the p.d.f. for the gamma distribution is:
f t
t T e t t ( ) = ( ) < < ¥
- l -
G
a a l
a
1
, 0 (4.26)
where G( ) a is the gamma function evaluated at value of the parameter a. The reader 
is reminded that the gamma function is the definite integral defined as:
G z t e dt z t ( ) =
¥
- - ò
0
1
and that for the special case in which the argument is an integer:
G( )z z = - ( ) 1 !
In general, the parameters of the gamma distribution are non-negative but are not 
necessarily integer. The c.d.f. for the gamma distribution cannot be expressed in 
closed form unless the parameter a is an integer. In that case, the c.d.f. is:
F t e
t
k
e
t
k T t
k
k
t
k
k
( ) = - ( )
( ) = ( )
( ) -
=
-
-
=
¥
1 å å 0
1
l
a
l
a
l l
G G (4.27)
For cases in which the parameter a is not an integer, values of the distribution 
function are computed by numerical integration. The value of the gamma function is 
then calculated using a numerical approximation. The best available numerical 
approximation is given in Abramowitz and Stegun1 as follows. For 0 £ £z 1,
G( ) a a +1 1 = = + + 1 2 a a + + a a + + a e ( ) a 2 3 3 4 4 5 " " 5 ! c c c c c
where:
c1 = -0.5748646 c2 = 0.9512363 c3 = -0.6998588
c4 = 0.4245549 c5 = -0.1010678
and the resulting error is bounded by e a( ) < ´ - 5 10 5
. As an example, observe that
G( ) 4 2. . 6 4 = ( ) 26 ( ) 3 2. . 6 2( ) 26 G( ) 1 2. . 6 3 = ( ) 1 386 ( ) 0. . 904 = 28 384
Thus, for a gamma distribution having a = 4 2. 6 and l = 0. , 025Random Variables and Distributions 57
F t T t e t t ( ) = = ( ) = ( ) = ò - - 325 1 1
28 384
7 977 0 281
0
125
1
G a
la a l d . . .
where the value of the integral was obtained numerically.
The gamma distribution is often a good representative model for various types of 
queuing systems, equipment failure processes and environmental pollutant accumu￾lations. It has also used to represent error rates in large software products and infec￾tion rates for communicable diseases.
Example 4.23
Suppose the random variable T represents the time to failure for an auto￾matic guided vehicle and that it has a gamma distribution with parameters 
a = = 3 0 , . l 75 / . yr Compute Pr . é2 5yr £ £ T y 7 5. . r ë ùû
Pr . . . . .
!
. . 2 5 7 5 7 5 2 5
7 5 2 7 5 [ ] £ £ = ( ) - ( ) = ( ) -
=
¥
-
=
¥
T F F å å k T T e
k
k
a k
l
a
l 5
2 5 7 5
2 5
2 5
0
1
7 5
0
1
l
l l
l
l
a
l
a
( )
= ( ) - ( )
-
-
=
-
-
=
-
å å
k
k
k
k
k
k e
e k e
!
.
!
.
.
. .
k
e
!
. . . . . . = + ( )( ) + ( ) ( )( ) æ
è
ç
ç
ç
ö
ø
÷
÷
÷
- -( ) 2 5 ( ) 0 75
2
1
2 5 0 75
1
2 5 0 75
2 e
e
-( )( )
-
+ ( )( ) + ( ) ( )( ) æ
è
ç
ç
ç
ö
ø
÷
÷
÷
=
7 5 0 75
2
1
1
7 5 0 75
1
7 5 0 75
2
. . . . . .
. . . . . . 875
2
5 625
2
1 1 875
1 875
2
1 5 625
5 625
2 + + ( ) æ
è
ç
ç
ö
ø
÷
÷
- + + ( ) æ
è
ç - e ç
ö
ø
÷
÷
= - 0. . 710 0 081= 0.630
4.4.3  The Weibull Distribution
During the first half of the 20th century, a Swedish engineer named Waloodi Weibull 
was studying the breaking behavior of metal tensile specimens. Professor Weibull 
found that the failure behavior followed a pattern that he described using the form:
F t T e t
t
( ) = - £ < ¥ - -
-
æ
è
ç ö
ø
÷
1
d
q d
b
, d (4.28)
It is reasonable to observe that three parameters should permit one to model most 
types of behavior. The three parameters are non-negative as is the random variable. 
Of the three parameters, δ is called the minimum life parameter. It represents the 
force value below which no failures occurred. It thus represents an axis offset which 
can often be ignored. If we exclude that parameter from the model, we obtain the 
more widely used two-parameter form of the distribution:
F t T e t t
( ) = - < < ¥ -( ) 1 0 q
b
, (4.29)58 Probability Foundations for Engineers
In this form, the parameter β is called the shape parameter and θ is called the loca￾tion parameter. As the names imply, the shape and location parameters determine the 
shape and location of the distribution. In the case of the Weibull, we see an informa￾tive example of the effect of the designation of the location parameter. Notice that the 
value of the distribution function at the value of the random variable t = q is 
F t T ( ) = = - e = - q 1 0 632 1 . regardless of the value of the parameter β. Thus, the value 
of θ determines the range over which the distribution varies. This is illustrated in 
Figure 4.4. Increasing values of the location parameter move the 63.2% point of the 
distribution to the right which implies a wider range of values over which the random 
variable may occur.
Example 4.24
Suppose the life lengths of certain memory chips are well modeled by the Weibull 
distribution having b = 2 2. 5 and q = 18000hrs. What fraction of the chip popula￾tion will fail by 10000 hours and what is the probability that a chip survives more 
than 25000 hours?
FT 10000 1 1 e e 1 e
10000
18000 0 556 0 2
2 25
2 25
( ) = - = - = - -æ
è
ç ö
ø
÷ -( ) -
.
. . . 66 = 0.234
FT 25000 e e e 0 1
25000
18000 1 389 2 094
2 25
2 25
( ) = = = = -æ
è
ç ö
ø
÷ -( ) -
.
. . . . 23
4.4.4  The Beta Distribution
The Beta distribution is defined for a random variable that takes on values between 
0 and 1. For this reason, it is most often used to model realizations of probabilities 
such as the value of p of the binomial distribution. Other applications are models of 
500 1000 1500 2000 2500 3000
0.2
0.4
0.6
0.8
1.0
FIGURE 4.4 Weibull c.d.f. when θ = 1000 and when θ = 1500.Random Variables and Distributions 59
proportions in surveys and elections, and variations in life expectancy probabilities 
of disease patients. The p.d.f. for the beta distribution is:
f x
x x X ( ) = x ( ) - < <
- a b - 1 1
1 , 0 1 (4.30)
where B a b a b
a b ( ) , = ( ) ( )
( ) +
G G
G which is basically a normalizing constant that assures 
that the p.d.f. integrates to 1.0. Note the similarity of the numerator of Equation 4.30 
to the corresponding terms of the binomial p.d.f.
The two parameters of the Beta distribution are both shape parameters so the dis￾tribution has great flexibility in the behaviors it can represent. Setting a < b yields a 
distribution for which small values of the random variable are prevalent. Reversing 
the inequality produces the reverse result. Many other forms are also possible.
Example 4.25
Suppose we wish to model the probability that a particular political candidate will 
win a specific election. We might perform an opinion survey and on the basis of 
the survey results, we might set a = 2 and b = 2. Using these values, we find that
FX ( ) 0 5. . = ³ P X[ ] 0 5 = 0.9375
which means that the probability the candidate will win the election is quite high.
4.4.5  The Normal Distribution
It would be difficult to overstate the importance of the normal distribution which 
some people call the Gaussian distribution. There are great number of a natural 
and man-made phenomena that display behavior that conforms to the normal dis￾tribution model. In fact, it is likely the reader has already encountered the normal 
distribution – perhaps under the label of the “bell-shaped curve” or some similar 
descriptor. Physical dimensions of manufactured products often vary in a manner 
that is well described by the normal distribution. The same is true of the quantities 
such as modulus of elasticity, density, viscosity and other “physical constants” that 
are not really constant. Physical characteristics such as the heights and weights of 
populations of animals and humans also display normal distribution type dispersion. 
Most students are probably familiar with teachers who distribute their test scores and 
course grades according to a normal distribution and have probably encountered dis￾cussions of intelligence test scores which are also thought to vary according to a nor￾mal distribution. There are many other examples, many of which are familiar to us.
In general, the normal distribution allows the random variable to vary over both 
the positive and the negative domains. The distribution is specified using two param￾eters, μ and σ where μ is the location parameter that specifies where the center of the 
distribution is and σ is the scale parameter which provides an indication of the range 60 Probability Foundations for Engineers
of dispersion of the distribution. A representative image of the normal density func￾tion is shown in Figure 4.5. The algebraic statement of the normal density is:
f x
e X x
x
( ) = - ¥ < < ¥
-( ) -m
s
ps
2
2 2
2 2 , (4.31)
and it is worth repeating that the range of the random variable is -¥ < < x ¥.
Unfortunately, the normal density cannot be integrated in closed form to provide an 
algebraic statement of the distribution function. Consequently, computation of prob￾abilities for normal random variables is usually accomplished in one of four ways.
One way to obtain probabilities for normal random variables is by numerical inte￾gration. One may simply program a numerical integration algorithm or else use one 
that is commercially available. A second approach is to use an existing mathematical 
software product which has the normal distribution incorporated as a built in func￾tion. Most math software products have it.
The most common way to obtain probabilities for the normal distribution is to 
convert the normal random variable to a “standard normal variable” and then use a 
table of the c.d.f. for the standard normal distribution. The transformation of a nor￾mal random variable to a standard normal random variable is:
z
x = - m
s
(4.32)
where it is a notational convention to use z to represent a standard normal variable. 
It is also a convention that the standard normal density and distribution functions be 
represented using the Greek letter phi as:
f
p
z
e z
( ) =
- 2
2
2 (4.33)
10 10 20 30
0.02
0.04
0.06
0.08
0.10
FIGURE 4.5 Normal density when μ = 1000 and when σ = 4.Random Variables and Distributions 61
for the density function and:
F z w dw
z
( ) = ( ) -¥
ò
f (4.34)
for the distribution function. Observe that the algebraic form for the density function 
is simplified by transformation of the variables in Equation 4.31. This is because the 
distribution on the transform variable z has parameters μ = 0 and σ = 1. Values of the 
c.d.f. of Equation 4.34 are included in a widely used table that is reproduced here in 
Table 4.2. The use of the standard normal table is illustrated in the following example.
TABLE 4.2
Cumulative Probabilities for the Standard Normal Distribution
z 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09
0.0 0.5000 0.5040 0.5080 0.5120 0.5160 0.5199 0.5239 0.5279 0.5319 0.5359
0.1 0.5398 0.5438 0.5478 0.5517 0.5557 0.5596 0.5636 0.5675 0.5714 0.5753
0.2 0.5793 0.5832 0.5871 0.5910 0.5948 0.5987 0.6026 0.6064 0.6103 0.6141
0.3 0.6179 0.6217 0.6255 0.6293 0.6331 0.6368 0.6406 0.6443 0.6480 0.6517
0.4 0.6554 0.6591 0.6628 0.6664 0.6700 0.6736 0.6772 0.6808 0.6844 0.6879
0.5 0.6915 0.6950 0.6985 0.7019 0.7054 0.7088 0.7123 0.7157 0.7190 0.7224
0.6 0.7257 0.7291 0.7324 0.7357 0.7389 0.7422 0.7454 0.7486 0.7517 0.7549
0.7 0.7580 0.7611 0.7642 0.7673 0.7704 0.7734 0.7764 0.7794 0.7823 0.7852
0.8 0.7881 0.7910 0.7939 0.7967 0.7995 0.8023 0.8051 0.8079 0.8106 0.8133
0.9 0.8159 0.8186 0.8212 0.8238 0.8264 0.8289 0.8315 0.8340 0.8365 0.8389
1.0 0.8413 0.8438 0.8461 0.8485 0.8508 0.8531 0.8554 0.8577 0.8599 0.86214
1.1 0.8643 0.8665 0.8686 0.8708 0.8728 0.8749 0.8770 0.8790 0.8810 0.8830
1.2 0.8849 0.8869 0.8888 0.8907 0.8925 0.8944 0.8962 0.8980 0.8997 0.9015
1.3 0.9032 0.9049 0.9066 0.9082 0.9099 0.9115 0.9131 0.9147 0.9162 0.9177
1.4 0.9192 0.9207 0.9222 0.9236 0.9251 0.9265 0.9279 0.9292 0.9306 0.9319
1.5 0.9332 0.9345 0.9357 0.9370 0.9382 0.9394 0.9406 0.9418 0.9429 0.9441
1.6 0.9452 0.9463 0.9474 0.9484 0.9495 0.9505 0.9515 0.9525 0.9535 0.9545
1.7 0.9554 0.9564 0.9573 0.9582 0.9591 0.9599 0.9608 0.9616 0.9625 0.9633
1.8 0.9641 0.9648 0.9656 0.9664 0.9671 0.9678 0.9686 0.9693 0.9699 0.9706
1.9 0.9712 0.9719 0.9726 0.9732 0.9738 0.9744 0.9750 0.9756 0.9761 0.9767
2.0 0.9773 0.9778 0.9783 0.9788 0.9793 0.9798 0.9803 0.9808 0.9812 0.9817
2.1 0.9821 0.9826 0.9830 0.9834 0.9838 0.9842 0.9846 0.9850 0.9854 0.9857
2.2 0.9861 0.9864 0.9868 0.9871 0.9875 0.9878 0.9881 0.9884 0.9887 0.9890
2.3 0.9893 0.9896 0.9898 0.9901 0.9904 0.9906 0.9909 0.9911 0.9913 0.9916
2.4 0.9918 0.9920 0.9922 0.9925 0.9927 0.9929 0.9931 0.9932 0.9934 0.9936
2.5 0.9938 0.9940 0.9941 0.9943 0.9945 0.9946 0.9948 0.9949 0.9951 0.9952
2.6 0.9953 0.9955 0.9956 0.9957 0.9959 0.9960 0.9961 0.9962 0.9963 0.9964
2.7 0.9965 0.9966 0.9967 0.9968 0.9969 0.9970 0.9971 0.9972 0.9973 0.9974
2.8 0.9974 0.9975 0.9976 0.9977 0.9977 0.9978 0.9979 0.9979 0.9980 0.9981
2.9 0.9981 0.9982 0.9983 0.9983 0.9984 0.9984 0.9985 0.9985 0.9986 0.9986
3.0 0.9987 0.9987 0.9987 0.9988 0.9988 0.9989 0.9989 0.9989 0.9990 0.9990
3.1 0.9990 0.9991 0.9991 0.9991 0.9992 0.9992 0.9992 0.9992 0.9993 0.9993
3.2 0.9993 0.9993 0.9994 0.9994 0.9994 0.9994 0.9994 0.9995 0.9995 0.9995
3.3 0.9995 0.9995 0.9996 0.9996 0.9996 0.9996 0.9996 0.9996 0.9996 0.9997
3.4 0.9997 0.9997 0.9997 0.9997 0.9997 0.9997 0.9997 0.9997 0.9997 0.999862 Probability Foundations for Engineers
Example 4.26
The diameters of baseball cores are well modeled by a normal distribution having 
m = 25mm and s = 0 8. . 0mm In processing, the cores are passed through a 26 mm 
screen and the cores trapped above the screen are machined down provided they 
have a diameter that is no larger than 27 mm. Cores with diameters larger than 
27 mm are discarded. What fraction of the population of cores is trapped by the 
screen? What fraction of the population of cores is machined?
Let x represent the dimension of interest, the diameter of the cores of baseballs. 
Then, the corresponding standard normal variable is:
z x = - 25
0 8.
and the cores trapped by the screen are those that have diameters that exceed 
26 mm so:
Pr Pr . . Pr .
. .
[ ] X Z > = > Z - = é
ë
ê ù
û
ú = - [ ] £
= - ( ) =
26 26 25
0 8
1 25 1 1 25
1 1 F 25 1 0 0 - = . . 8944 0 1056
In this calculation, we first find that z = 1 2. 5 is the value of the standard normal 
variate that corresponds to a core diameter of 26 mm. We then go to Table 4.2
where the leftmost column gives the values of z to a level of tenths. The column 
headings extend the values of z to hundredths so we observe that in the row 
labeled 1.2, the column headed 0.05 will correspond to z = 1 2. 5 and the table 
entry of 0.8944 is therefore the value of F( ) 1 2. . 5 Subtracting that value from 
1yoelds the probability we seek.
To determine the fraction of the population that is machined to meet the spec, 
we compute:
Pr Pr . . .
. . .
26 27 1 25 27 25
0 8
2 5
2 5 1 25 0 9
[ ] < < = < < - = é
ë
ê ù
û
ú
= ( ) - ( ) =
X Z
F F 938 - = 0. . 8944 0 0994
The observant reader will have noted that all of the z values that index the table 
are positive. However, negative values of z occur often. How is this handled? The 
symmetry of the normal density allows us to use the identity that:
F F ( ) -z z = -1 ( ) (4.35)Random Variables and Distributions 63
Example 4.28 is intended to illustrate the fact that we can determine “quantiles” 
for the normal distribution by reversing the process we used to obtain probabilities; 
that is, the example is really asking for the normal variates that have 1% of the popu￾lation in the tails of the density. Essentially, for any tail probability, γ, we can say that:
zg = F ( ) -1 g (4.36)
and
x z g g = + m s (4.37)
Example 4.27
Suppose the baseball cores that pass through the 26 mm screen are subsequently 
passed across a 24.5 mm screen and any cores that are not trapped are discarded 
because they are too small. What fraction of the population of cores is discarded 
because they are too small?
Pr . Pr .
. .
. . .
[ ] X Z < = < - = - é
ë
ê ù
û
ú
= -( ) = -
24 5 24 5 25
0 8
0 625
F F 0 625 1 0 ( ) 0 625 1 = - . . 0 0 7341= 0.2659
The rationale for Equation 4.35 and its application in Example 4.27 is that the 
area under the normal density (the integral) to the right of any value z is equal to 
the area under the normal density to the left of -z. Consequently, including only 
the positive coordinates in the table is sufficient to specify the entire distribution 
function.
A further comment is that the probability stated in Example 4.27 for F( ) 0.625
was computed by linear interpolation between F( ) 0 6. 2 and F( ) 0 6. . 3 This is a 
reasonable approach to obtaining values relative to a finer mesh.
Example 4.28
Specifications for the length of a machined component are 1. . 8 0 ± 16mm.
Assuming that component length is well modeled by a normal distribution, what 
value of s will assure that at least 98% of the population falls within the specs?
Pr . . Pr . . . . Pr . 1 64 1 96 1 64 1 80 1 96 1 80 0 16 0 [ ] £ £ = - £ £ - ù =û éë X z - £ z £ s s s
.
. . . .
16
0 16 0 16 0 16 1 0 1
s
s s s
é
ë
ê ù
û
ú
= æ
è
ç ö
ø
÷ - -æ
è
ç ö
ø
÷ = æ
è
ç ö
ø F F F F ÷ - - 6 2 0 16 1 0 98 s s
æ
è
ç ö
ø
÷ æ
è
ç ö
ø
÷ = æ
è
ç ö
ø F ÷ - = . .
so F 0 16 0 99 . . s
æ
è
ç ö
ø
÷ = and 0 16 0 99 2 326 . . .
s = = z so s = = 0 16
2 326
0 069 .
. .64 Probability Foundations for Engineers
The standard normal variate that corresponds to a c.d.f. value of γ is the γ quantile 
of the standard normal distribution and inverting Expression 4.32 yields the corre￾sponding variate of the original normal distribution. In the case of Example 4.28, the 
question is made a little more interesting by asking for σ, but the solution is still 
driven by the identity of the quantile.
A final approach that can be used to compute normal distribution probabilities is 
by numerical approximation. The approximation is defined for standard normal vari￾ates and was published by Abramwoitz and Stegun1. To compute approximate values 
for cumulative probabilities of the standard normal distribution, we calculate:
F( )z d » - ( ) + +z d z d + + z d z d + + z d z z + ( ) -
1 1
2
1 1 2 2 3 3 4 4 5 5 6 6 16
e (4.38)
where:
d1 = 0.04986(73) d3 = 0.00327(76) d5 = 0.00004(89)
d2 = 0.02114(10) d4 = 0.00003(80) d6 = 0.00000(54)
and the calculation yields a result with an error term of e ( )z < ´ - 1 5 10 7 . . 
Abramowitz and Stegun also provide an approximation algorithm for quantiles of 
0.05 or less. The algorithm is:
z t
c c t c t
e t e t e t 1 0 1 2 2
1 2 2 3 3 1 - » - + +
+ + + g + e g( ) (4.39)
where:
c0 = 2.515517 c1 = 0.802853 c2 = 0.010328
e1 = 1.432788 e2 = 0.189269 e3 = 0.001308
and:
t = æ
è
ç ö
ø
÷ ln 1
2 g
This approximation yields results for which e( ) g < ´ - 4 5 10 4 . .
4.4.6  The Lognormal Distribution
The lognormal distribution represents the behavior of a random variable for which 
the logarithm is normally distributed. Like the normal distribution, the lognormal dis￾tribution has the advantage that it can represent a random variable that extends over 
a very wide range. It is often preferred because it applies to random variables such 
as equipment repair times that cannot take negative values. It has been extensively 
used for economic modeling and has also been applied to representing equipment life Random Variables and Distributions 65
times and repair times, the dispersion in particle sizes of aerosols and the response of 
some biological materials to various stimuli.
The formulation of the lognormal model is a bit intricate. For a random variable, 
say X, we say that X has a lognormal distribution if Y = ln X has a normal distribu￾tion. Therefore, the probabilities on X conform to:
PréX x £ Pr Y x ln ë ùû = £ éë ùû
and the lognormal c.d.f. is:
F x
x X x Y
Y
( ) = æ ( ) -
è
ç
ç
ö
ø
÷
÷ F < < ¥ ln , m
s
0 (4.40)
Taking the derivative, the lognormal p.d.f is:
f x
e
x X
x
Y
Y
Y
( ) =
-( ) ln( )-m
s
ps
2
2 2
2 2
(4.41)
Working with the logarithmic transform implies that there are two scales of mea￾sure that are relevant to the analysis of any problem. Specifically, the parameters of 
the lognormal are mY and sY while the corresponding descriptive parameters on X are 
mX and s X .The two parameter sets are related by:
m s m s m s s
X X e e e e Y Y Y Y Y = = - ( ) +
2 2 2 2 2 2 2 and (4.42)
and
m m
s
s
s
m Y X Y Y X
X
= - = +
æ
è
ç ö
ø
ln ln ÷
2 2 2
2 2
and 1 (4.43)
Example 4.29
A common application of the lognormal distribution is modeling the daily change 
in the price of a stock. For the ratio of successive days prices X P
P k k
k
= -1
 is lognor￾mal. If mY = 0 0. 5 and sY = 0 8. , 0 what is the probability of a price increase greater 
than 10% and what is the probability of a price decrease of 10% or more.
A price increase of 10% corresponds to Xk = 1 1. so using Equation 4.40
FX 1 1 1
1 1 0 05
0 8 . 1 0 0566 0 478
ln . .
. ( ) = - . . æ ( ) -
è
ç
ç
ö
ø
÷
÷ F F = - ( ) =66 Probability Foundations for Engineers
4.4.7  The Uniform Distribution
The uniform distribution provides a model of cases in which the probability associ￾ated with a set of values of a random variable is proportional to the proportion of the 
sample space corresponding to the set of values. The most common application of the 
uniform distribution is in computer simulation experiments.
In the general case, we allow the random variable, say X, to fall in a range [a, b] 
and the distribution function is defined as:
F x x a
b a X ( ) = a x b -
- , £ £ (4.44)
The corresponding density function is:
f x
b a X ( ) = -
1 (4.45)
We can see that the proportion in Expression (4.44) is the fraction of the range of 
the random variable in the set { } a X £ £ x .
Under the general definition of the uniform distribution, the parameters a and b 
may be positive or negative as long as a b < . In practice, negative values are rarely 
used and in fact, the most common use of the distribution model has a = 0 and b = 1.
and a price decrease of 10% corresponds to Xk = 0 9. so
FX 0 9
0 9 0 05
0 8 . 0 1942 0 423
ln . .
. ( ) = . . æ ( ) -
è
ç
ç
ö
ø
÷
÷ F F= -( ) =
Example 4.30
The preventive maintenance service time for a particular aircraft engine, T, has a 
lognormal distribution with mX = 4 hours hours and s X = 1 2. hours. What is the 
probability that a service performed today will be completed within 5 hours? 
What is the probability it will take more than 6 hours?
For mX = 4 hours and s X = 1 2. hours: Equations 4.43 yield mY = 1.343 and 
sY
2 = 0.086 so sY = 0.294 and
Pr ln .
. [ ] T £ = . . æ -
è
ç ö
ø 5 ÷ = ( ) = 5 1 343
0 294
F F 0 907 0 818
Pr[ ] ln .
. T > = - . . æ -
è
ç ö
ø 6 1 ÷ = - ( ) = 6 1 343
0 294
F F 1 1 528 0 063Random Variables and Distributions 67
Example 4.31
For a uniformly distributed random variable on the range [0.5, 2.5], what are 
Pr . [ ] 1 3 £ £ X 1 9. and Pr[X > 2 1. ]?
Since a = 0 5. and b = 2 5.
Pr . . . . . .
. .
. .
. . 1 3 1 9 1 9 1 3 1 9 0 5
2 5 0 2
1 3 0 5
2 5 0 [ ] £ £ = ( ) - ( ) = -
- - -
- X FX XF
2
1 9 1 3
2 0 = 0 3 - = . .
. .
and
Pr[ . ] . . .
. . X F > = - X ( ) = - . -
- 2 1 1 2 1 1 = 2 1 0 5
2 5 0 5
0 2
4.5 CONDITIONAL PROBABILITY
The concept of conditional probability described in Chapter 3 extends directly to the 
random variables that are images of events and thus to their distribution and density 
or probability mass functions. The condition stated in Expression 3.1 that
Pr
Pr
Pr
A B
A B
B [ ] | = éë Ç ùû
éë ùû
is meaningful for probability calculations based on distribution functions and is again 
a reflection of a reduction of the magnitude of the sample space resulting from avail￾able information. For a discrete random variable having p.m.f. fX ( ) x , the knowledge 
that X a ³ can be incorporated in our analysis by the application of the conditioning 
Expression (3.1) with the result that we obtain fX X| ³a ( | x X ³ = a X ) Pr[ = ³ x X| ] a . In 
this case, Expression (3.1) implies:
f x X a X x X a
X x
X a
f x
F a X X a
X
X
| Pr
Pr
Pr ³ ( ) ³ = = [ ] ³ = , é = ë ùû
é ³ ë ùû
= ( )
( ) - | | 1
a x £ £ xmax
In the case of a continuous random variable T having density function fT ( )t , the 
corresponding construction is:
F t T
F t F
F T T t T T
T
| ³ ( ) | , ³ = ( ) - ( )
- ( ) t t £ £ ¥ t
t
t
168 Probability Foundations for Engineers
Example 4.33
Suppose that we know that a continuous random variable T represents the age 
at failure of a component and has an exponential distribution with parameter 
l = 0. / 005 day.. If we also know that the component will be replaced preven￾tively at an age of t = 24days, what is the probability that the component fails 
before 10 days given replacement will occur at 24 days and what is the prob￾ability that the component will survive for at least 18 days given that replacement 
will occur at 24 days?
F T
F
F
e
e T T
T
T
|
.
. £ |
-( )( )
-( )( ( ) £ = ( )
( ) = -
- t 10 24
10
24
1
1
0 025 10
0 025 24) = = 0 221
0 451
0 490 .
. .
F T
F F
F
e e T T
T T
T
|
. .
£ |
-( )( ) -(
( ) £ = ( ) - ( )
( ) = - t 18 24
24 18
24
0 025 10 0 025)( )
-( )( ) - =
24
0 025 24 1
0 197 e . .
The point of this discussion is that it is often meaningful to define or ana￾lyze conditional probabilities and the fact that the conditioning information is 
expressed in terms of a random variable and its distribution function does not alter 
its sense or limit its applicability.
4.6 RESIDUAL LIFE DISTRIBUTIONS
An important set of models is derived from our understanding of conditional prob￾ability and is suggested by Example 4.30 and 4.32. These models are usually called 
residual life distributions. They are often used reliability analysis but also in actuarial 
studies and disease modeling. The residual life models represent the probability of 
values of a random variable conditioned on the value exceeding a fixed quantity. 
Example 4.32
Suppose X is a Poisson random variable with l = 12 and we know that for a cer￾tain application X ³ 8. We compute Pr[X X = ³ 11| ] 8 as:
f x X
f
F X X
X
X
| ³ ( | ³ =) . ( )
( ) 8 8 = 11
7
0 126
and Pr[X X £ ³ 16| ] 8 as:
F X
F F
F X X
X X
X
| ³ ( ) | . ³ = ( ) - ( )
( ) 8 16 8 = 16 7
7
0 889
To construct an example for a continuous random variable and expand our 
experiences, reverse the direction of the inequality.Random Variables and Distributions 69
In an actuarial context, the model might be used to represent the survival rate of 
people whose age exceeds a specific value – for example, people who receive Social 
Security. The general expression for this residual life distribution is:
F t T
F t F
F T T t T T
T
| ³ ( | ³ =) , ( ) - ( )
- ( ) t t £ £ ¥ t
t
t
1 (4.46)
The corresponding survival probability is:
F t T
F t
F
F t
F T T
T
T
T
T
| ³ ( | ³ =) ( )
- ( ) = ( )
( ) t t
1 t t
These expressions apply regardless of the distribution model that is used to 
describe the life length variable, T.
Example 4.34
Suppose the life length of a particular radial tire is modeled by a Weibull distri￾bution having b = 2 8. 0 and q = 27500 miles. What is the probability that a tire 
having an age of 36000 miles will survive beyond 45000 miles?
F T
F
F T T
T
T
| | .
. ³ ( ) ³ = ( )
( ) 36000 45000 36000 = = 45000
36000
0 0189
0 1193
0.158
Similar calculations for equipment following service are widely used. 
Government planning for Social Security and Medicare benefits is based on simi￾lar computations. Life insurance premiums are also computed on the basis of 
residual life distribution models.
4.7 HAZARD FUNCTIONS
In the case of continuous random variables, we define an additional descriptor of the 
behavior of the distribution function. The hazard function most widely used in reli￾ability analysis but it has other applications as well. It is defined as the conditional 
failure density given survival to any point in time. An equivalent definition is that the 
hazard function is the instantaneous rate of failure given survival to any point in time.
The algebraic construction of the hazard function follows from our understanding 
of conditional probabilities and derivatives. Representing the life length (failure age) 
of a device by T, we can say:
Pr Pr[ ]
Pr[ ]
Pr
Pr[
t T t t T t t T t t T t
T t
t T t t
é £ £ + ë ù
û = £ £ + Ç >
>
= é £ £ + ë ùû
D D
D
T t
F t t F t
F t
T T
> T
= ( ) + - ( )
] ( )
D70 Probability Foundations for Engineers
Then, dividing by Dt and taking the limit yields:
z t
F t t F t
tF t
f t
F t T t
T T
T
T
T
( ) = ( ) + - ( )
( ) = ( ) ® ( ) lim
D
D
0 D (4.47)
The appropriate interpretation of this expression is that the density indicates the 
rate at which the distribution is growing and for the hazard function, must be scaled 
by the proportion of the distribution function that remains.
Example 4.35
If a population of microelectronic capacitors displays life length behavior that is 
well modeled by a Weibull distribution having b q = = 1 8. . and h 15000 rs com￾pute and interpret the value of the hazard function at 7800 and 15600 hours.
For the Weibull distribution:
f t t e F t e so z t
t T
t
T
t
( ) = ( ) = T ( ) =
- -( ) -( ) - b
q
b
q
b
b
q q
b
b
1 1 b b
and
Therefore:
FT T 7800 0 735 z 7800 7 1 10 5 ( ) = ( ) = ´ - . . and
FT T 15600 0 342 z 15600 12 4 10 5 ( ) = ( ) = ´ - . . and
so at 7800 hours, 73.5% of the population will be surviving and those survi￾vors are failing at a rate of 7 1 10 5 . ´ - per hour, while a year later at 15600 hours 
of age, only 34.2% of the population is surviving and those are failing at a rate of 
12 4 10 5 . ´ - per hour.
Example 4.36
Suppose the exponential distribution (with parameter l = 0 0. / 4 day) forms a rep￾resentative model of the life lengths of a certain population of flies. If 1000 flies 
hatch at a location today, what proportion of the original population and what 
proportion of the survivors will die on days two and three of their lives?
For the exponential, zT ( )t = l and FT t e t ( ) = - - 1 l so the proportion of the survi￾vors that start each day that die that day is 4%. However, the average number of 
flies that die on day 2 is 38 and on day 3 is 37. The reason for the difference is 
that on day 2, 4% of the 960 day one survivors die while on day 3, 4% of the 922 
day two survivors die.
An interpretation of the hazard function that makes sense in an actuarial or 
a reliability context is that the hazard function gives the rate at which surviving 
units are failing. In a reliability or actuarial context, it is quite common to refer to Random Variables and Distributions 71
4.8 MIXTURE DISTRIBUTIONS
It sometimes occurs that a particular distribution provides a good model for the 
behavior of a random phenomenon but the parameter of the distribution is not con￾stant. A  classic example of this instability has occurred in semiconductor manu￾facturing. The occurrence of defective chips on a wafer is usually well represented 
using a Poisson distribution. However, the rate for the Poisson model appears to vary 
among production machines. Thus product yield models are usually based on a mix￾ture of Poisson distributions. This is not the only domain in which mixtures appear 
but using this case as a conceptual model, consider that the overall defect frequency 
in a population of computer chips is determined using a model in which the Poisson 
parameter is a random variable. Thus, for the random variable X corresponding to the 
number of defective chips:
fX X ( ) x f = ( ) x g ( ) d ò
l
l
l l l
min
max
| L (4.49)
where gL ( ) l is a probability density describing the variation in the Poisson param￾eter. In general, for any random variable X having distribution function F x X ( ) with 
a distribution in terms of the behavior of its hazard function. One may reasonably 
say that a particular device displays an increasing hazard function so its life length 
should be modeled by a distribution having this characteristic.
One reason for specifying a distribution model in terms of its hazard function 
is that the hazard function completely identifies its distribution function. The cor￾respondence between the two functions is one to one. In fact, starting with the 
definition of the hazard function in Expression (4.46):
z t
f t
F t
f t
F t T
T
T
T
T
( ) = ( )
( ) = ( )
1- ( )
we can rearrange the terms to obtain the (relatively simple) non-homogeneous 
differential equation:
f t dF t
dt T z t F t T
( ) = T T
( ) = ( )( ) 1- ( )
for which, the solution is:
FT t e
z u du
t
T
( ) = - ò - ( )
1 0 (4.48)
Thus, every distribution function defined for a continuous non-negative random 
variable can be described in terms of its hazard function in the form given in 
Expression (4.46).72 Probability Foundations for Engineers
randomly varying parameters for which GQ ( ) q represents the parameter variation, 
the mixture distribution defined by:
F x X X ( ) = F x g d ( ) ò
q
( |q q ) Q q (4.50)
provides a useful probability model of the behavior of the variable.
4.9 INDEPENDENT RANDOM VARIABLES
Before leaving the discussion of random variables and their probabilities, consider 
the transfer of the idea of independence from events of a sample space to the cor￾responding variables. Recall that in Chapter 3, two events, say A and B, are said to 
be independent if:
PréA B Ç Pr A B Pr ë ùû = éë ùû éë ùû
or equivalently:
Pr[ ] A B| = PréA . ë ùû
Now, if there are random variables that are defined as images of the independent 
events, it must be the case that those random variables are independent as the prob￾abilities have been transferred directly. Suppose the set X contains the images of the 
elements of A and the set Y contains the images of the set B. Then, necessarily:
PréX Y| | Pr X Y Pr X Y Pr . ë ùû = éë ùû éë ùû = éë ùû and
That is, X and Y are independent.
EXERCISES
1. For the sample space described in Exercise 2 of Chapter 3, define a random vari￾able that can be used to represent observations.
2. For the sample space described in Exercise 1 of Chapter 3, define a random vari￾able that can be used to represent observations.
3. For the sample space described in Exercise 3 of Chapter 3, define a random vari￾able that can be used to represent observations.
4. Suppose an experiment consists of observing the point spreads that occur in 
within conference ACC football games and noting the largest of those values. 
Define a random variable that can be used to represent those observations.
5. Suppose an experiment consists of observing the number of customers per day 
to a fast food hamburger restaurant. Define a random variable that can be used to 
represent these observations.Random Variables and Distributions 73
6. At a local pizza restaurant, the hourly demand, D, for a particular type of pizza 
has the following probability mass function:
f d
d
d
d
d
d
d
d
D ( ) =
=
=
=
=
=
=
=
ì
í
ï
0 08 0
0 12 1
0 18 2
0 24 3
0 16 4
0 12 5
0 10 6
.
.
.
.
.
.
.
ï
ï
ï
î
ï
ï
ï
ï
What are the values for F F D D ( ) 4 2( ) é1 4 £ £ D 4 2 ë ùû é ³ ë ù
û , , Pr , P and D r ? D
7. Consider a random variable for which the distribution function is given by:
F y
Y
Y
Y
Y
Y
Y
Y ( ) =
£ <
£ <
£ <
£ <
£ <
£
0 08
0 20
0 42
0 1
1 2
2 3
0 68
0 86
1 00
3 4
4 5
5
.
.
.
.
.
. < ¥
ì
í
ï
ï
ïï
î
ï
ï
ï
ï
Determine the values for fY Y ( ) 3 3 F Y ( ) é2 4 £ £ 3 1 ë ùû é ³ ë ù
û , , Pr , P and Y r . Y
8. Suppose two fair six-sided dice are rolled. What are the possible values that 
could arise for:
a. The larger of the two numbers observed.
b. The smaller of the two numbers observed.
c. The magnitude of the difference between the two numbers observed.
d. The sum of the two numbers observed.
e. What are the probabilities associated with each of these random variables?
9. A discrete random variable, Y, has the following probability mass function:
f y
y
y
y
y
y
y
y
Y ( ) =
=
=
=
=
=
=
=
0 03 0
0 09 1
0 14 2
0 18 3
0 23 4
0 15 5
0 09 6
0 0
.
.
.
.
.
.
.
. 7 7
0 02 8
y
y
=
=
ì
í
ï
ï
ï
ï
ï
ï
î
ï
ï
ï
ï
ï
ï .
What are the values for F F Y Y ( ) 3 5( ) é2 6 £ £ Y Y 5 2 Y ë ùû é ³ ë ù
û , , Pr and Pr ?74 Probability Foundations for Engineers
10. A random variable Y has the following p.m.f.:
f y
y
y
y
y
y
y
c
c
c
c
c
Y ( ) =
=
=
=
=
=
=
ì
í
ï
ï
ïï
î
ï
ï
ï
ï
0
0 8
1 2
0 6
0 4
0
1
2
3
4
5
.
.
.
.
Determine the values for c, F F Y Y ( ) 3 2( ) é ³ 3 1 ë ù
û , P and Y r . Y
11. Consider a random variable for which the distribution function is given by:
F y
Y
Y
Y
Y
Y
Y
Y ( ) =
£ <
£ <
£ <
£ <
£ <
£
0 06 0 1
0 18 1 2
0 36 2 3
0 64 3 4
0 80 4 5
0 92 5
.
.
.
.
.
. <
£ < ¥
ì
í
ï
ï
ï
ï
î
ï
ï
ï
ï
6
1 0. 0 6 Y
Determine the values for fY Y ( ) 3 4 F Y ( ) é3 5 £ £ 4 2 ë ùû é ³ ë ù
û , , Pr , P and Y r . Y
12. If a random variable has the probability density defined as:
fX ( ) x c = - ( ) 1 0 x x £ £ 1 2 ,
a. Determine the value of c.
b. What is Pré0 5. . £ £ 0 85 ë ù X û?
13. If a random variable has the probability density defined as:
fX ( ) x c = - ( ) 4 2 x x 0 2 £ £ x 2 ,
a. Determine the value of c.
b. What is Pré0 5. . £ £ 1 2 ë ù X û?
14. Demand at a local microbrewery for its premium beer in gallons per week is a 
random variable with density function:
fX ( ) x x = - 4 1( ) 0 1 £ £ x 3
,
How much beer must the brewer produce per week in order to have a stock out 
probability in any week of 0.05?Random Variables and Distributions 75
15. An inventory system contains 4 types of products. Customers order one unit of 
product at a time. 20% of customers order product A while 30% order product 
B, 15% order product C and 35% order product D. The frequency with which 
the suppliers of the products are out of stock is 6% for product A, 2% for product 
B, 12% for product C and 1% for product D. When a customer orders a product 
that the inventory does not have, the sale is lost. What is the probability that an 
order cannot be filled? If an order is not filled, what is the probability that the 
requested product was product B?
16. What is the probability that a binomial random variable, X, having parameters 
n = 50 and p = 0.025 takes a value greater than 2? What is the probability that X
exceeds 4 if it is known that X exceeds 2?
17. What is the probability that a binomial random variable, X, having parameters 
n = 80 and p = 0.015 takes a value greater than 2? What is the probability that 
X £ 4 if it is known that X exceeds 2?
18. A particular engine bearing plant has three manufacturing lines. For line A, the 
proportion of output bearings that are defective is 0.01 while for line B the pro￾portion is 0.016 and for line C, it is 0.025. Line A produces 30% of the plant’s 
output while line B produces 45% and line C produces 25% of the output. If a 
sample of n = 100 bearings is inspected and found to contain one defective bear￾ing, what is the probability that the sample was taken from the output of line B?
19. What is the probability that a binomial random variable, Y, having parameters n
= 40 and p = 0.05 takes a value between 2 and 4 inclusive?
20. If ½ % of the output wheel-well manifolds from an injection molding process 
are defective, what is the probability that a sample of 80 units will include 2 or 
more defective manifolds?
21. The firm that manufactures patriot missiles purchases the guidance circuits from 
three different suppliers. Supplier A provides 30% of the guidance circuits and 
those circuits have a fault probability of pA = 0 0. 2 while the circuits from sup￾plier B, which provides 25% of those purchased, have a fault probability of 
pB = 0. . 025 The guidance circuits purchased from supplier C have pC = 0 0. . 1 If 
a batch of 200 missiles is fired during a particular strategic offensive and 3 of the 
missiles fail to track to target, what is the probability that the batch of missiles 
contained guidance circuits obtained from supplier B?
22. What is the probability that 24 tosses of a fair die yields 4 observations of each 
face of the die.
23. What is the probability that 24 tosses of a fair die yields X = ( ) 3 6, ,2 4, ,5 4, ?
24. For the integrated circuits in Example 4.14, what is the probability that all 50 
chips are non-defective and what is the probability that W = ( ) 46, , 2 2 ?
25. Suppose a sample of 8 cards is selected from a standard deck of cards. What is 
the probability one of the cards is an ace and what is the probability that 3 of the 
cards are “face cards”, (jack, queen or king)?
26. Suppose a small production lot of a product contains 75 units of which 4 are defec￾tive. What is the probability that a sample of 6 units contains 1 that is defective?
27. A high school graduating class in a small town has 92 students of whom 49 are 
female and 43 are male. If a sample of 10 of the students is selected at random, 
what is the probability that 6 of the students selected are female.76 Probability Foundations for Engineers
28. The number of patients arriving to a local pharmacy for flu shots during 
November is a Poisson random variable with parameter of l = 2 8. / hour. What 
is the probability that the number of arriving patients in any hour exceeds 3?
29. The number of incoming calls to a mail order call center is a Poisson random 
variable with parameter of l = 1 8. / minute. What is the probability that the num￾ber of arriving calls in any minute exceeds 3? What is the probability that the 
number of arriving calls exceeds 3 during any 2 minute interval?
30. Suppose it is known that the number of accidents occurring per day on Main 
Street is a Poisson random variable with parameter l = 4 / day.
a. What is the probability that the number of accidents on any day is 4 or more?
b. Given that the number of accidents today is at least one, what is the con￾ditional probability of 4 or more accidents today?
31. The number of calls to a particular university internet site is well modeled by a 
Poisson distribution with l = 1 2. / min. What is the probability that the number 
of calls during any minute will exceed 3 if it is known that the number of calls 
exceeds 1.
32. The number of students arriving to a campus health center each hour is well 
modeled by a Poisson distribution having l = 4 5. / hour. What is the probability 
that no students arrive during any one hour interval? What is the probability that 
more than 3 students arrive during a one hour interval?
33. For a Poisson random variable, X, having l = 2 4. / 8 hr, what is Pr[ ] X X > 6 3 ?
34. What is the probability that more than 12 tosses of a fair die are required to 
obtain the first 6?
35. Bob and Joe have each purchased an unbalanced six-sided die at a novelty shop. 
Bob’s die has PréX = . ë ùû 4 0 = 10 while Joe’s has PréX = . . ë ùû 4 0 = 20 If one of these 
dice is selected at random and rolled until the first 4 occurs and that happens to 
be the 7th roll, what is the probability that the die is Bob’s? How does this prob￾ability change if the first 4 occurs on the 3rd roll?
36. An experiment consists of rolling a fair six-sided die until a 4 occurs. What is the 
probability that the number of rolls exceeds 9? What is the probability that the 
number of rolls is no greater than 8 given that the number exceeds 2?
37. An experiment consists of repeatedly drawing a card from a standard poker deck 
and replacing it until a queen is found. What is the probability that the number 
of trials performed until the queen is found is less than or equal to 10?
38. An experiment consists of repeatedly tossing a pair of fair six-sided dice until 
they show a sum of 7. What is the probability that the experiment ends within 8 
tosses?
39. Suppose it has been found through a series of experiments that an unfair coin has 
a probability of showing heads within the first 5 times it is tossed of 0.75. What 
is the value of p, the probability heads on any single toss?
40. Consider a game of “craps” in which a player’s first roll of two fair six-sided 
dice sets up the game. If the player’s first roll is a 4, 5, 6, 8, 9, or 10, that number 
becomes the “point” and the player wins if she rolls the point again before roll￾ing a 7. If a 7 appears first, she loses. Suppose Ellen initially rolls a 5.Random Variables and Distributions 77
a. What is the probability she wins on the fifth subsequent roll?
b. What is the probability she wins within eight subsequent rolls?
41. If an experiment consists of tossing a fair die, what is the probability that a 3 
occurs for the fourth time on the twentieth toss?
42. A fair six-sided die is rolled until the third time a five is obtained. What is the 
probability that the number of rolls exceeds 20? What is the probability that the 
number of rolls exceeds 20 given that it exceeds 12? What is the expected num￾ber of rolls?
43. If it is known that in a series of coin tosses, the 4th head occurred on the 12th trial, 
what is the probability that the 3rd head occurred on the 9th trial?
44. If a bearing inspection process is continued until the third defective bearing is 
found and the defect proportion for the bearing population is p = 0. , 015 use the 
binomial distribution model to state (do not compute) the probability that the 
process terminates on or before the 64th inspected bearing.
PréN B £ , , . , B , . ë ùû = ( ) = - ( ) - 64 64 3 0 015 1 2 64 0 015 1
45. A local electricity utility company inspects residential meters at random. 4½% 
of the meters register electricity consumption inaccurately. Inspections con￾tinue until 4 inaccurate meters are found at which time an order for replacement 
meters is made. What is the probability that the number of inspections before an 
order is made exceeds 20? What is the probability that the number of inspections 
exceeds 20 given that the first 10 meters inspected are accurate?
46. If it is known that in a series of tosses of an unfair coin having Préhead . ë ùû = = p 0 40, 
the 5h head occurred on the 14th trial, what is the probability that the 3rd head 
occurred on the 8th trial?
47. The time between arrivals of customers to a concert ticket sales web site is well 
modeled by an exponential distribution with parameter l = 0 2. / 5 min. What is 
the probability that the time until the next customer arrival customer exceeds 
7.5 minutes? What is the probability that the time until the next customer arrival 
exceeds 7.5 minutes given that it is no greater than 10 minutes?
48. If the life lengths of a population of integrated circuits are well modeled by an 
exponential distribution having l = . / 0004 hour.what is the probability that a 
randomly selected member of that population operates without failure for more 
than one year (5000 hrs.)?
49. The delivery lead time for a component used by an assembly plant in its product 
is well modeled by an exponential distribution having l = 0. / 167 day. What is 
the probability that a particular component batch arrives 8 or more days after is 
ordered?
50. The life lengths of a certain population of batteries is well modeled by an expo￾nential distribution having l = 0. / 004 hr. What is the probability that a 200 hour 
old battery from this population will survive for an additional 300 hours?
51. The random variable T has a gamma distribution with parameters 
a = = 3 0 , . l 75 / . hr Compute Pré2 5. . £ £ 7 5 . ë ùû T78 Probability Foundations for Engineers
52. The time required to locate a series of bugs in a word processing software prod￾uct is well modeled by a gamma distribution having parameters a = 5 0. and 
l = 0. / 065 day. What is the probability that the discovery process will require 
more than 90 days?
53. Use the numerical approximation for the gamma function to compute the value 
of Gé3 7. . 2 ë ùû
54. For the gamma distribution having a = 3 7. 2 and l = 0. , 0084 compute FT ( ) 280
and FT ( ) 400 .
55. Life lengths of certain automotive tires are well modeled by a gamma distribu￾tion having a = 2 8. 5 and l = ´ - 7 14 10 5 . / mile. If the manufacturer of the tires 
offers a free replacement warranty of 18,000 miles on the tires, what fraction of 
the tire population will have to be replaced?
56. The life lengths of a population of ABS control circuits is thought to be well 
modeled by a Gamma distribution having a = 3 and l = . / 0004 hour. What is the 
probability that one of these circuits that is selected at random and found to have 
already operated for 2000 hours will fail within the next 6000 hours.
57. What is the probability that a gamma random variable, T, having a = 2 0. and 
l = 0. / 008 hr. takes a value greater than 400 when it is known that T exceeds 
200?
58. If the life lengths of memory chips are well modeled by the Weibull distribution 
having b = 1 8. and q = 20000 hrs, what fraction of the memory chip population 
will survive beyond 27000 hours?
59. The life lengths of certain memory chips is well modeled by the Weibull distri￾bution having b = 2 2. 5 and q = 18000 hrs.
a. What is the probability that a chip survives more than 25000 hours?
b. What is the value of the hazard function at 25000 hours?
60. The life length of a photocopier roller bearing displays a Weibull distribution 
having b = 3 2. and q = 12000 cycles. What fraction of the population will fail by 
8000 cycles? By 18000 cycles?
61. An electronics manufacturer purchases 30% of its cell phone batteries from a 
supplier that claims those batteries have a Weibull life length with parameters 
b = 1 4. 0 and q = 25000 hrs. The manufacturer produces the remaining 70% of 
its cell phone batteries in its own plant and those batteries have a Weibull life 
distribution with parameters b = 1 8. 0 and q = 30000 hrs. If a your cell phone has 
required one-year (8760 hours) warranty replacement for 12% of the battery, 
what is the probability that your phone contained a batteries from the supplier?
62. The life lengths of a population of satellite communications receivers are well 
modeled by a Weibull distribution having b = 1 2. 5 and q = 48000 hrs. What is 
the probability that one of these receivers that has operated for 40000 hours will 
continue to operate for an additional 30000 hours?
63. For a chemical process, the concentration of an impurity varies with the ambi￾ent temperature and this variation is well modeled by a beta distribution having 
a = 2 0. and b = 198.0. What is the probability that on any day, the impurity 
concentration exceeds 2.5%?Random Variables and Distributions 79
64. The infection rate of a communicable disease appears to vary with population 
density and to be well modeled by a beta distribution having a = 10.0 and b = 8 0. . 
What is the probability of observing a rate no greater than 60%? If a particular 
city is known to have an infection rate greater than 40%, what is the probability 
that city experiences a rate greater than 60%?
65. The annual snowfall in Buffalo, NY is normally distributed with m = ² 120 and 
s = ² 10. . 4 What is the probability that Buffalo will have more than 140″ in any 
year? What is the probability that this year’s total will be between 110″ and 130″?
66. A normal random variable, X, with m = 45 takes a value less than or equal to 38.5 
with probability 0.125. What is the value of s for the distribution?
67. A normal random variable, Y, with s = 7 5. takes a value greater than or equal to 
264.4 with probability 0.230. What is the value of m for the distribution?
68. The thickness, T, of personal computer chassis spacers is well modeled by a 
normal distribution having m = 0 4. mm and s = 0 0. . 4 mm If a spacer is selected 
at random, what is:
a. Pré0 3. . 3 0 £ £ 45 ë ùû T ?
b. PréT ³ . ë ùû 0 32 ?
c. PréT ³ . ? ë ùû 0 50
d. Pré0. . 375 £ £ 0 50 0. ? 32 ë ù
û T T
69. The heights of male college students in Ohio, X, is normally distributed with 
m = 175cm and s = 10. . 2 cm If one of these students is selected at random, what is:
a. Pré160. . 0 £ £ 180 0 ë ù X û?
b. PréX £ . ë ùû 150 0 ?
c. PréX ³ . ? ë ùû 195 0
70. Breakfast cereal manufacturers are subject to fines if more than 1% of their 
24 oz. cereal boxes actually contain less than 23.92 oz. of cereal. If cereal box 
fill volume displays a normal distribution with s = 0. . 045 oz at what value, m,
should the mean fill volume be set on the filler machine in order to avoid being 
at risk of a fine?
71. Use the numerical approximation of Expression (4.38) to compute PréX £ . ë ùû 57 5
for a normal distribution having m = 50.0 and s = 4 2. . Compare your result to 
the value you obtain from Table 4.2.
72. Solve Problem 65 using the numerical approximation in Expression (4.39).
73. Specifications for the length of a machined component are 2. . 4 0 ± 20 cm.
Assuming that component length is well modeled by a normal distribution, what 
value of s will assure that at least 98% of the population falls within the specs?
74. The diameters of baseball cores are well modeled by a normal distribution hav￾ing m = 25mmand s = 0 8. . 0 mm In processing, the cores are passed through a 
26 mm screen and the cores trapped above the screen are machined down pro￾vided they have a diameter that is no larger than 27 mm. Cores with diameters 
larger than 27 mm are discarded. What fraction of the cores that are trapped 
by the screen is machined down? What fraction of the population of cores is 
discarded?80 Probability Foundations for Engineers
75. What is the probability that a normal random variable having m = 24.50 and 
s = 0.625 takes a value between 24.25 and 25.10 inclusive?
76. For a normal random variable Y having mY = 15.0 and sY = 0 2. , 4 compute 
Pré14. . 64 £ £ 14 84 ë ùû Y and Pr[ . 14 64 £ £ Y Y 14. | 84 £ 15. ] 2 .
77. The sizes (diameters), D, of raindrops are considered to be lognormal in dis￾tribution. Meteorologists distinguish between various types of rain and indi￾cate that for “stratified rain” (as opposed to “convective rain”), they find that 
mD = 2 1. m2 m and s D = 0 6. . 4mm What fraction of the raindrops in a stratified 
rainfall exceed 3.0 mm in diameter and what fraction has a diameter no greater 
than 1.5 mm?
78. The life lengths of a certain population of halogen light bulbs are well mod￾eled by a lognormal distribution. For that population mT = 40000 hours and 
sT = 2500 hours. If the bulb manufacturer offers a 36000 hour warranty, what 
fraction of the bulb population will be replaced in order to honor the warranty?
79. If a random variable has a uniform distribution over the range 10 £ £ X 20 what is 
the probability that the random variable takes a value in the range é13. , 75 17.25 ë ùû?
80. A random variable, Y, has a uniform distribution over the range é2 4, . ë ùû Compute 
Pré2 2. . 5 3 £ £ 25 ë ùû Y and Pr[ . 2 25 3 £ £ Y Y . | 25 £ 3 5. ].
81. If a random variable has the probability density defined as:
fX ( ) x x = £ 2 0 , x £ 1
identify the corresponding hazard function.
82. Show that the Weibull hazard function is decreasing when b < 1, increasing 
when b > 1 and constant when b = 1.
83. The life lengths of certain memory chips is well modeled by the Weibull distri￾bution having b = 2 2. 5 and q = 18000 hrs. What is the value of the hazard func￾tion at 25000 hours?
84. Compute the value of the hazard function at T = 200 hrs. and at T = 400 hrs. for 
a gamma distribution having a = 2 0. and l = 0. / 0072 hr.
85. Suppose the distribution on a non-negative random variable, X, which takes on 
values in the interval [0, 2] has the hazard function zX ( ) x x = +1 2 . Identify the 
distribution function on X and PréX £ . . ë ùû 1 20
86. Consider a random variable X having a uniform distribution over the interval 
[a, b]. Construct that hazard function for the distribution on X.
87. Suppose the distribution on a non-negative random variable, T has the hazard 
function zT ( )t t = 0. . 0002 Identify the distribution function on T and PréT ³ . ë ùû 150
88. Construct the hazard function for a gamma distribution having a = 3 0. and 
l = 0 0. / 5 hr. Compute the value of the hazard function at T = 50 hrs.
89. The distribution on a random variable, T, has hazard function zT ( )t t = + 1 2.
where 0 £ <t ¥. Compute PréT £ . . ë ùû 0 8
NOTE
1 Abramowitz, M., Stegun, I.A. (1965), Handbook of Mathematical Functions, New York, 
Dover Publications.DOI: 10.1201/9781003294382-5 81
There is no reason to limit the definition of a sample space to a single dimension. 
There are random phenomena that are best described in terms of a vector valued out￾come. In terms of engineering applications, it is easy to imagine that (1) the quality 
of a manufactured part would be measured by several dimensions such as thickness, 
height and length, or (2) the demand for consumer electronic products from an inven￾tory would include several types of devices or several distinct models or (3) the mix 
of patient needs in a hospital emergency room would imply consumption of various 
quantities of several resources. In a possibly more familiar application, the cards 
obtained in a poker hand might also be well described in multivariate terms. Thus, in 
each of these example cases, the measured quantities might be best represented as a 
random vector.
5.1 THE IDEA OF JOINT RANDOM VARIABLES
If we encounter a sample space for which the outcomes have multivariate measures, 
it will probably be logical to define the representative random variable using the 
same number of dimensions. The result will be a multivariate random variable – a 
random vector. Consider some examples – in two dimensions.
Example 5.1
Suppose we toss two fair dice and map the number of spots facing up on each 
die to the corresponding number. Then, our random vector would be X X    1 2 ,X
where Xi
 is the number observed on die i.
Joint, Marginal 
and Conditional 
Distributions
5
Example 5.2
At a regional credit card call center, customers call in either to request a credit 
limit increase or to check on their existing balance. If Y1 represents the number of 
customers who call to request a credit limit increase during a four hour interval 
and Y2 represents the number of customers who call to check their account bal￾ance, then the random vector Y Y    1 2 ,Y models the sample space of incoming 
calls.82 Probability Foundations for Engineers
Example 5.3
If the location of a hole punched in a work piece varies in two dimensions and is 
evaluated relative to its horizontal and vertical alignment, then the random vector 
X Y,  provides a representation of hole position quality.
Example 5.4
If the life of an automotive tire is defined in terms of distance traveled and days of 
use, then the random vector D U,  represents tire age accumulation. Clearly, ran￾dom vectors are common and can include any number of dimensions. In addition, 
the quantities that comprise the random vectors may be discrete or continuous.
As in the case of univariate random variables, we map the probabilities of 
events of the sample space to the sets of random variables that are the images of 
those events. We again form the mapping so that we have a distribution function 
for the random vector. The general representation for the distribution function for 
a random vector is:
FX X X r x x x X x X x Xr r x 1 2 , ,, r   1 2 , ,, P   r ,   1 1 2 2  ,  (5.1)
and we call this function the “joint distribution function” (or joint cumulative dis￾tribution function or joint CDF) on the random vector X X     1 2 X Xr , , , . In order 
to examine the general form in detail, consider a two-dimensional random vector 
X Y,  for which the realization of expression (5.1) is:
FX Y,   x y, P   r ,   X x Y y  (5.2)
While it is not essential to do so, we can separate our discussion of the discrete 
and continuous cases.
5.2 THE DISCRETE CASE
For purposes of discussion, suppose the random vector represents our observation of 
the toss of two distinguishable dice, where the number on the first die is mapped to X
and the number on the second die is mapped to Y. Our first observation is that we can 
readily construct both the joint distribution function and the corresponding joint prob￾ability mass function for the random vector. The joint probability mass function is:
fX Y,   x y, ,   1 x y 36
and the joint distribution function is enumerated in Table 5.1.Joint, Marginal and Conditional Distributions 83
In general, the joint distribution function is parallel to the univariate distribution 
functions described in Chapter 4. As with the functions in Chapter 4:
F x X Y y f i j
i
x
j
y
, ,   , ,  X Y  .
 
0 0
(5.3)
However, disassembling expression (5.3) to obtain some of its components must 
be done carefully as some relationships are a bit counterintuitive. Perhaps the most 
important of those relationships is that:
fX Y, ,   x y, ,  F x X Y   y F  X Y,   x y 1 1 ,  (5.4)
Instead, algebraic manipulation of expression (5.3) leads to:
fX Y x y F x X Y y FX Y x y f i y
i
x
X Y
j
y
, , , ,   , ,        , ,     




1 1   0
1
0
1
f x X Y j ,   , . (5.5)
and this expression generalizes to:
Pr[ , ] , ,
,
, ,
,
x X x y Y y F x y F x y
F x y F
X Y X Y
X Y X
1 2 1 2 2 2 2 1
1 2
         
    ,Y x y,   1 1 (5.6)
At the same time, examination of Table 5.1 indicates that some expected relation￾ships do apply. In particular, note that it is always the case that:
F x X Y, ,   , , y F  X Y   x y   PrX x,Y y    1 (5.7)
TABLE 5.1
Joint Distribution Function for Two Fair Dice
X\Y 1 2 3 4 5 6
1 1
36
1
18
1
12
1
9
5
36
1
6
2 1
18
1
9
1
6
2
9
5
18
1
3
3 1
12
1
6
1
4
1
3
5
12
1
2
4 1
9
2
9
1
3
4
9
5
9
2
3
5 5
36
5
18
5
12
5
9
25
36
5
6
6 1
6
1
3
1
2
2
3
5
6
184 Probability Foundations for Engineers
and
F x X Y, ,   , , y F  X Y   x y   PrX x, . Y y    1 (5.8)
5.2.1 Marginal Probability Functions
Given the definition of a joint distribution function, one may obtain a marginal 
distribution on one of the random variables by evaluating the joint distribution func￾tion at the maximum value(s) of the other random variable(s); that is:
F x X X    F x , m Y   y ax , (5.9)
and
F y Y X    F x , m Y   ax, . y (5.10)
The marginal distributions are proper univariate distributions and conform to the 
relationships discussed in Chapter 4. Observe F x X X    F x ,Y   ,6 in the rightmost 
column of Table 5.1 and F y Y X    F y ,Y   6, in the bottom row of the table. Similarly, 
note that:
fX X   x X    x F x FX X x F Y X x y F x Y y   Pr        1 1  , m   , , ax   , m   ax
and the corresponding expression holds for the random variable Y. The corollary 
results for this expression are that the marginal probability mass functions can be 
constructed as:
fX x f x y
y
   X Y,   , (5.11)
and
fY y f x y
x
   X Y,   , . (5.12)
Thus, the joint and marginal probability measures are intertwined and one can 
usually be obtained from the other. Consider an example.
Example 5.5
Suppose that the random vector Y Y    1 2 ,Y described in Example 5.2 has joint 
probability mass function:
f y y e
y y y Y Y y y y
y y y
1 2
1 2 1
1 2
17
1 2 1
1 2 2
11 6 , , 0 0
! !    , ,        
 Joint, Marginal and Conditional Distributions 85
5.2.2  Conditional Probability Functions
The joint distribution function also provides a basis for constructing conditional 
distribution functions and their corresponding conditional probability mass 
functions. The concept of the conditional functions is exactly the same as described 
in Chapter 3 and stated in expression (3.1). If one has partial information about a 
random experiment, the probabilities of the observations of that experiment may be 
adjusted to reflect that knowledge. In Chapter 3, we found that for some experiments, 
knowledge that an event B occurred implied a revision in the probability that another 
event, say A, occurs. In this case, we found that:
Pr
Pr
Pr
A B
A B
B        
 
(5.13)
The definition of conditional probability functions on a random vector again 
requires the application of the probabilities of the events of the sample space to the 
images of those events. Once this mapping is defined, the probability associated with 
the intersection will usually be a joint probability measure and the probability of the 
condition will often be a marginal probability.
Applying expression (5.11) to this joint p.m.f. yields the marginals:
f y f y j e
y j y
e
Y
j
Y Y
j y
y j y
1 1 2
1
1 1
1
0
1
17
1 1
11 6         




  

  , , ! !
17
1 0 1
17
1
6 11
1
11 6 11 11 1
1
y 1 1 1
j y
j y y y
y j y
e
y e e
! ! ! ! y  
   
     
and
f y f i y e
i y i
e
Y
i
y
Y Y
i
y i y i
2
2
1 2
2 2
2
0
2
0
17
2
1
116         

 
 

  , , ! !
7
2 0
2
2 0
2 2 2 2
116 2 116
y
y
i y i
y
i
e
i
y i y i
i
y
i y i
!
!
! ! 


 
      

 

 
17
2
17y2
y !
We should recognize these marginal probability mass functions as Poisson so 
we know their corresponding marginal distribution functions. Values for the joint 
distribution are obtained using expression (5.3). For example, the reader may con￾firm that:
FY Y1 2 ,   y y 1 2   5 1 , . 3 0  03386 Probability Foundations for Engineers
Example 5.6
For the two dice having the joint distribution enumerated in Table 5.1, we can 
compute:
Pr ,
Pr ,
Pr
Pr ,
Pr
X Y Y
X Y Y
Y
X Y
               

  
    
3 2 4
3 2 4
4
3 2

Y
F
F
X Y
   Y
  
    4
3 2
4
1
6
2
3
1
4
, ,
or
Pr , , Pr , ,
Pr ,
P
X Y X Y
X Y X Y
X Y                  

   

3 2 4 4
3 2 4 4
4 4 
r ,
Pr ,
,
,
,
,
X Y
X Y
F
F
X Y
X Y
   
      
    3 2
4 4
3 2
4 4
1
6
4
9
3
8
or
Pr ,
Pr ,
Pr ,
Pr ,
X X Y
X X Y
X Y
X Y
               

   
  
3 4 4
3 4 4
4 4
3 4

 
      
    Pr ,
,
,
,
X Y ,
F
F
X Y
4 4 X Y
3 4
4 4
1
3
4
9
3
4
Example 5.7
For the random vector described in Example 5.2 with the joint probability mass 
function stated in Example 5.5, we can construct the conditional probability mass 
functions as:
f y y f y y
f y
e
y y y Y Y
Y Y
Y
y y y
2 1
1 2
1
1 2 1
2 1
1 2
1
17
1 2 1
11 6
      
    
 
, , !      
  !
!
e !
y
e
y y y
y y
11
1
6
11 2 1
6
1
2 1
and:
f y y f y y
f y
e
y y y Y Y
Y Y
Y
y y y
1 2
1 2
2
1 2 1
1 2
1 2
2
17
1 2 1
11 6
      
    
 
, , ! 
     

 




!
!
!
! !
e
y
y
y y y
y
y
y
y y y
y
17
2
2
1 2 1
2
1
17
11 6
17
11
17
2
1 2 1
2


 

 

 


y y 1 2y1
6
17Joint, Marginal and Conditional Distributions 87
5.3 THE CONTINUOUS CASE
As in the discrete case, a treatment of joint distribution functions for continuous ran￾dom vectors begins with Expression (5.2). Then, the extension of Expression (5.3) to 
continuous variables implies the use of integrals rather than summations:
F x X Y y X x Y y f u v dvdu
x y
, ,   , P   r ,   X Y ,      
  (5.14)
Here again, it is appropriate to note that the joint distribution may reasonably 
apply to a random vector having more than two dimensions.
One of the advantages of the continuous model is that the joint distribution func￾tion is often (not always) differentiable. In those cases in which the joint distribution 
can be differentiated, the joint probability density function is obtained as:
f x y
x y XY   , ,  F x XY y

    2
(5.15)
so:
f y y e
y Y Y
y
2 1
2
2 1
6 2
2
2 6
2         
 
!
and:
f y e Y Y2 1 4 2 6
2 1 0 0446
6 2
      

! .
and also:
f y y
y Y Y
y y
1 2
1 1
1 2
1
4
4
4 11
17
6
17       

 




 

 

 



and:
f y Y Y1 2 2 4
4
2
11
17
6
17 2 0 313
2 2
      

 




 

 

 

  .
Examples 5.6 and 5.7 illustrate the fact that the conditional probabilities may 
be analyzed using either the conditional distribution function or the conditional 
probability mass function. The choice depends upon the application. The exam￾ples are also intended to emphasize the use of the basic conditioning relationship 
given in Expression (5.13). As with many of the analyses treated in this text, it is 
very often worthwhile to base a computation on a return to an initial definition.88 Probability Foundations for Engineers
Thus, it is often possible to move between the distribution and density functions 
as necessary. As noted in the case of the discrete joint distributions, one should be 
cautious with difference computations. For the continuous random vectors:
Pr , , a X b c Y d , f u v dvdu
a
b
c
d
       X Y    (5.16)
but it is not the case that:
Pra X   b c, P   Y d r , X b Y d Pr X a, . Y c              
Instead, one can compute:
Pr , Pr , Pr ,
Pr ,
a X b c Y d X b Y d X a Y c
X a c Y d
                  
              Pr a X b Y, . c
but it is often easier to use Expression (5.16). Alternately, Expression (5.6) may 
prove to be manageable.
Example 5.8
Suppose the following joint density function has been defined to model the 
response of a new material to a magnetic field:
f x XY y e x y y x y   , ,     ,     2 0 0
The joint distribution function is constructed as:
FXY x y e dvdu e e dvdu e e
x
u
y
u v
x
u
u
y
v
x
u v
u   ,               
0 0 0
2 2 2 y
x
u u y
x
u y u u y u
du
  e e  e du e     e du e    e 
           2 2 2 1
2 0 0
2 2

 


       
0
2 1 2 2
x
y x x y e e e
and we observe that differentiation returns the joint density function:
f x y
x y
F x y
y x XY XY e e e y x x y   , ,  
     


      

 


 
    2 2 1 2 2
         
y e e e x x y x y 2 2 2 2Joint, Marginal and Conditional Distributions 89
Example 5.9
The diameter of an automotive side rail spot weld, X, and its compressive strength, 
Y, have been modeled using a bivariate version of a uniform density function as:
f x XY   , , y x   . .   , y 
1
200
0 44 0 64cm 1000 2000N
For this model:
F x y x y XY , .       0 44   1000
200
so:
FXY 0 5 1400
0 50 0 44 1400 1000
200 . , 0 12 . .    .       
and more interesting:
Pr . . , ,
.
.
0 56 0 64 1600 2000 ,
0 56
0 64
1600
2000
           X Y f u X Y v dvdu  0 1. 6
For this last computation, one might consider the probability to be the joint distri￾bution equivalent of the univariate survivor function; that is, we might label this 
probability as:
FX Y x y x X x y Y y f u v dvdu
x
x
y
y
, m , Pr , ax max , X Y ,
max max
             
This convention is not universally agreed but is used in this text.
5.3.1 Marginal Probability Functions
As in the case of the discrete variables, the joint distribution function may be used 
to obtain a marginal distribution on one of the random variables by evaluating the 
joint distribution function at the maximum value(s) of the other random variable(s). 
Expressions (5.9) and (5.10) apply equally to continuous random vectors. Their real￾izations may be stated as:
F x X X F x Y f u v dvdu
x
       X Y   

  , , , , (5.17)90 Probability Foundations for Engineers
and
F y Y X F y Y f u v dvdu
y
      X Y   


  , , , , (5.18)
The derivative relationships described in Chapter 4 also apply so the marginal 
probability density functions can be obtained as derivatives of the marginal distri￾bution functions but may also be constructed from the joint probability density func￾tion; that is:
f x
d
dx X X    F x   f x XY   v dv


 , (5.19)
and:
f y
d
dy Y Y    F y   f u XY   y du


 , (5.20)
Example 5.10
For the joint distribution function of Example 5.8, the marginal distributions may 
be constructed as:
FX X x F Y x e x          , 1 2
FY X y F Y y y e e y y           , 1 2 2
Then, by differentiation, the marginal densities are:
f x d
dx X XF x e x        2 2
f y d
dy Y YF y e e y y          2 2 2
On the other hand, integration of the joint density function yields:
f x X f x y dy e dy e e
x
XY
x
x y x y
x
x           
 
     
  , 2 2 2 2
f y Y f x y dx e dx e e e
y
XY
y
x y x y y y y                    
0 0
0
2 , 2 2 2 2
and integrating these functions will yield the marginal distribution functions.Joint, Marginal and Conditional Distributions 91
5.3.2  Conditional Probability Functions
The construction of conditional probability statements for continuous random vari￾ables again follows from Expression (3.1). We divide the probability of the image of 
the intersection of the relevant events by the probability of the image of the known 
condition. As in the case of discrete random variables, the probability associated with 
the intersection will often be a joint probability measure and the probability of the 
condition will often be a marginal probability.
Example 5.11
For the joint distribution function of Example 5.8 having the marginal distributions 
and densities constructed in Example 5.10, the conditional density functions may 
be constructed as:
f x y
f x y
f y
e
e e
e
e X Y
X Y
Y
x y
y y
x
y       
     
 
 


, , 2
2 2 1 2
and
f y x f x y
f x
e
e Y X e X Y
X
x y
x
y x
      
   
 

,     , 2
2 2
The corresponding conditional distribution functions should be obtained by 
integration.
F x y f u y du e e du e
e X Y
x
X Y y
x
u x
y             
   
 

0 0
1
1
1
1
and
FY X y x f v x dv e dv e
x
y
Y X
x
y
v x y x
                      1
There are three important points that are illustrated by Example 5.11. The first 
of these points is that a conditional probability function is really a function of the 
stated condition. For example, f x X Y    y really is a function of Y. If a value of 5 is 
specified for Y, the function is distinctly different that if the specified value is 3.
The second of the important points is that the conditional density functions are 
proper univariate density functions and the conditional distribution functions are 
proper univariate distribution functions. These functions conform to the descrip￾tions provided for univariate probability functions in Chapter 4.
The third important point is that the definitions of the conditional densities in 
Example 5.11 are incomplete as the range of the random variables should have 
been specified and was not given. Unless the range of the random variable is very 
obvious, it should be stated. In the case of Example 5.11, it should have been 
noted that f x X Y    y applies for 0 ≤ ≤ x y and f y Y X    x applies for x y   .92 Probability Foundations for Engineers
Example 5.12
For the three-dimensional joint density function:
f x X Y, ,Z   , , y z   2 0   x y z x , ,   1 0   y z 1 0,   1
there are three marginal joint densities:
f x X Y, ,   , , y f  X Y ,Z   x y,z dz x     y zdz     x y  
0
1
0
1
2
f x X Z, ,   , , z f  X Y ,Z   x y,z dy x     y zdy     x z  
0
1
0
1
2 2 1
and
f y Y Z, ,   , , z f  X Y ,Z   x y,z dx x     y zdx     y z  
0
1
0
1
2 2 1
in addition to the three univariate marginal densities:
f x X X    f x Y Z   y z dzdy     x y zdzdy x     y dy    0
1
0
1
0
1
0
1
0
1
, , , , 2  x 1
2
f y Y X    f x Y Z   y z dzdx     x y zdzdx x     y dx    0
1
0
1
0
1
0
1
0
1
, , , , 2   y
1
2
and
f z Z X    f x Y Z   y z dxdy     x y zdx y     zdy    0
1
0
1
0
1
0
1
0
1
, , , , 2 2 1  2z.
The corresponding distribution functions are:
F x y f x y dydx x y dydx xy y X Y
x y
X Y
x y x
, ,   , ,         


    0 0 0 0 0
2
2


   dx x y xy 2 2
2
The use of two-dimensional random vectors has been useful in illustrating the 
construction of marginal and conditional probability functions. However, higher 
dimension random vectors are also possible and for such vectors, our definitions 
can be extended but this should be demonstrated. Consider an example.Joint, Marginal and Conditional Distributions 93
Example 5.13
For the three-dimensional joint density function of Example 5.12, there are very 
many conditional probability functions that can be constructed. Here is a partial 
list:
FX Y, , Z X x y z f Y Z x y z FX Z, , Y X x z y f Z Y x z y FY , , , , , , , ,                 Z X    y z, ,   x fY Z, X   y z, , x
FX Y  , , Z X   x y  , , z f Y Z   x y, , z FY X  , , Z Y   y x  , , z f X Z   y x, , z FZ  X Y, ,   z x  , , y fX Y Z   x y, , z
FX Y    x y  , , f x X Y   y FX Z    x z  , , f x X Z   z FY Z    y z  ,f y Y Z  z  , , F y Y X x f   Y X y x,    
FZ X    z x  , , f z Z X   x FZ Y    z y  , . f z Z Y   y
Taking a few of these as representative:
f x z y
f x y z
f y
x y z
y
x y z
y X Z Y
X Y Z
Y
,
, , , , ,       
     

   

2
1
2
4
2 1
f x y z
f x y z
f y z
x y z
y z
x y X Y Z
X Y Z
Y Z
  ,
, ,
,
, , ,
,     
     
    2   
2 1
2
  2 1 y 
F x z f x z dzdx x zdzdx x z X Z
x z
X Z
x z x
, ,   , ,               0 0 0 0 0
2 1 2 1
2 2 2
2 2
dx x x z    
F y z f y z dzdy y zdzdy y z Y Z
y z
Y Z
y z y
, ,   , ,               0 0 0 0 0
2 1 2 1
2 2 2
2 2 dy y y z    
F x f x dx x dx x x X
x
X
x
       

 

  
 
0 0
2 1
2 2
F y f y dy y dy y y Y
y
Y
y
       

 

  
 
0 0
2 1
2 2
FZ z f x dz zdz z
z
Z
z
        
0 0
2 2
and finally:
FX Y Z x y z f x y z dzdydx x y
x y z
X Y Z
x y z
, , , ,   , ,    , ,       000 000
2 zdzdydx x y xy  z  

 


2 2 2
2 .94 Probability Foundations for Engineers
5.4 INDEPENDENCE
The concept of independence of random variables follows directly from that of inde￾pendence of events. The algebraic test is also the same. The concept of independence 
is that knowledge of the occurrence of a random event (or the random variable that is 
its image) does not restrict the chance of occurrence of another event. In Chapter 3, 
we expressed this idea in Equation (3.6) as:
PrA B  Pr A B Pr       
and we used this condition to obtain the equivalent statement that for independent 
events:
PrA B Pr A .     
The extension of this concept to random variables and random vectors is direct. 
Two random variables, X and Y, are independent if any of the following conditions 
are satisfied:
F x X Y,   , y F  X Y   x F   y (5.21)
f y x f x y
f x
x y
x
x y
x Y X
X Y
X
      
   

   

, ,
1
2
2
2 1
with the corresponding distribution functions:
F x z y x xy z
y X Z, , Y X , . F x Z Y , . z y .          
      
2 2 2
2 1 so 0 4 0 6 0 25 0.086
F x y z x xy
y X Y  , , Z X     , .  F x Y Z y z . , . . 
        
2 2
2 1 so 0 4 0 25 0 6 0 240
F y x y xy
x Y X       F y Y X x 
     
2 2
2 1 so 0 6. . 0 4 0.467
The above examples illustrate the fact that joint distributions that are defined on 
random vectors allow for a wide variety of analyses. Regardless of the dimen￾sionality of the random vector, marginal densities and distributions of any smaller 
dimensionality can be determined. In addition, conditional densities and distribu￾tions can also be constructed as needed. Related to these constructions is the fact 
that while the joint probability functions can be used to construct the marginal 
probability functions, in general, the reverse is not the case. Unless the constitu￾ent random variables are independent, as discussed next, the identities of the 
marginal probability functions are not sufficient to identify the joint functions.Joint, Marginal and Conditional Distributions 95
fX Y,   x y,  f x X Y   f y  (5.22)
F x X Y    y F  X   x (5.23)
fX Y   x y  f x X   (5.24)
The reason each of these conditions is sufficient is that they are equivalent. 
Furthermore, the conditions apply equally to discrete and continuous random vari￾ables. This is because the concept of independence applies equally to discrete and 
continuous random variables.
Note that it is also possible for subsets of the constituents of a random vector to be 
independent. For a random vector X X    1 X X234 X X5 , , , , it is conceivable that 
X X 1 2 and could be independent of X X3 4 X5 , and while at the same time X X 1 2 and are 
dependent and the set X X3 4 X5 , and are also dependent. In that case, we would find 
that:
F x X X X X X X x x x x F x X X x F X X x x 1, , 234 , , 5 1 1 234 5 1 , , 2 3 2 3 4 5 ,   , , , ,    , , 4 5   , x
and
F x X X X X X X x x x x F x X x 1 2 , , 3 4 , , 5 1 1 2 3 4 5 1 2 2 , , , , .       
Example 5.14
For the distinguishable dice having the probabilities enumerated in Table 
5.1, X and Y are independent. We can see that for each of the table entries 
FX Y,   x y, .  F x X Y  F y  For example:
FX Y, 3 4, F x X YF y 1
3
1
2
2
3          

 


and
FX Y, 4 2, . F x X YF y 2
9
2
3
1
3          

 


Also, we can see that:
f f
f X Y
X Y
Y
 3 4
3 4
4
1
36
1
6
1
6     
    , , .96 Probability Foundations for Engineers
Example 5.15
For the discrete joint density function in Example 5.5, we found that:
f y e
y
f y e
y Y
y
Y
y
2
2
1
1
2
17
2
1
11
1
17 11      
 
! ! and
so we can see that:
f y y e
y y y
f y f y e Y Y
y y y
1 2 Y Y
1 2 1
1 2 1 2
17
1 2 1
1 2
1 11 6 , , ! !            
   1
1
17
2
28
1 2
11 17 11 17 y y 1 2 y y 1 2
y
e
y
e
! ! y y! !
 

Also, we found in Example 5.7 that:
f y y e
y y Y Y
y y
2 1
2 1
2 1
6
2 1
6
       
 
!
which is not the same as:
f y e
y Y
y
2
2
2
17
2
17   

! .
Thus Y1 and Y2 are not independent.
Example 5.16
For the joint density function in Example 5.8, we found in Example 5.10 that:
f x X e f y e e x Y y y           2 2 2 2 2 and
The product of these functions is:
2 2 2 4 2 2 2 2 2 e e e e e e f x y x y y x y x y x y XY                       ,
Example 5.17
For the three-dimensional joint density function of Example 5.12, the random 
variables X and Y are dependent but the pair is independent of Z and each is indi￾vidually independent of Z. To see these relationships, observe that:
f x X Y  f y   x y x y f x X Y y


 

  

 

        1
2
1
2 , ,Joint, Marginal and Conditional Distributions 97
5.5 BIVARIATE AND MULTIVARIATE NORMAL DISTRIBUTIONS
A particularly useful realization of joint distributions is the multivariate normal. 
As with the univariate normal distribution discussed in Chapter 4, the multivariate 
normal provides an accurate model for many naturally occurring phenomena. One 
obvious example is the quality of a manufactured product that must conform simul￾taneously to several engineering specifications. The quality measures for those speci￾fications are likely to be dependent and thus to vary jointly.
The algebraic statement of the multivariate normal density is similar to but more 
complicated than the single-dimensional model. It will be easier to understand the 
multivariate distribution if we start with the bivariate case. For the two-dimensional 
distribution, a graph of the density is shown in Figure 5.1. Note that the “bell” shape 
f x X Y, ,Z X   , , y z   2 2   x y z f  ,Y Z   x y, f z      x y   z
f x X Z,   ,z x     z x   z fX Z x f z 

 
 2 1        1
2
2
While Expression (5.22) is used in Example 5.17, note that any of the other 
Expressions (5.21), (5.23) and (5.24) yield the same conclusions.
The concept of independence is particularly important for two reasons. As 
should be apparent from the construction, using multiplicative computations for 
independent random variables can simplify some calculations. The second and 
less obvious reason is that dependence implies a reduction in the size of the 
portion of the sample space that must be considered. For some modeling and 
calculation situations, the use of conditioning can greatly simplify the analysis of 
a phenomenon.
FIGURE 5.1 Bivariate normal density with   x y   12, , 18   x y   1 5. , 2 2. ,  0 6. .98 Probability Foundations for Engineers
is preserved but because of the dependence of the variables, the density is not com￾pletely symmetrical – looks a bit like a ski cap.
To specify the density function requires five parameters. The form of the density 
function is:
fX Y x y e
x y
x x y x
x
x
x
y
,   ,  

  
 

 

   
1
2 1 2
1
2 1
2 2
2
  


  


 y
y
y
y 
 



















2
(5.25)
Notice the similarity of this function to the univariate normal density. As in the 
case of the single variable model, the dispersion in each random variable is character￾ized by the two parameters   and . For two variables, this implies a total of four 
parameters. The fifth parameter, ρ, provides a measure of the strength of the depen￾dence between the variables. It is called the correlation coefficient. Thus, there are a 
total of five parameters necessary to uniquely specify a two-dimensional normal 
distribution.
Calculation of bivariate normal probabilities is difficult. There are numerical inte￾gration algorithms available but it is recommended that one use a standard mathe￾matical software package to compute the probabilities. Each of the existing packages 
has a routine for performing the calculation efficiently.
Example 5.18
For the bivariate normal distribution shown in Figure 5.1, the use of a standard 
mathematical software package yields:
FX Y,   10. , 4 14 8. .  0 039
FX Y,   12. , 6 16 5. .  0 227
FX Y,   13. , 8 21 3. .  0 850
and
FX Y, ,   1261 . , 6 5. .   1 0 F F X Y   1261 . , 6 5. .   X Y, ,   12 6 1 , ,   FX Y 6.5
1 0 0 227 0 655 0 248 0 324
 
  . .   . .  .
A particularly interesting feature of the bivariate normal distribution is that the 
marginal and conditional distributions are normal. As we know, for the joint den￾sity on the random vector X Y, , the marginal density on X is obtained as:
f x X X    f x Y   y dy


 , ,Joint, Marginal and Conditional Distributions 99
Starting with the form of the density given in Expression (5.25), we can substi￾tute for y as:
v y y
y
  

so
dv dy
y
 
and then completing the square on v in the exponent as:
1
2 1
2 1
2 2
2
2
2
  
 

 

   








  

 




  



x v x v x x
x
x
x
x
x

     

 

 1
2 1 2
2

 
 v x x
x
yields the integral:
f x X e
x
x v x x
x
x
x
    
   

 

 
  
  

 
 1
2 1 2
1
2
1
2 1
2
2
 

 
 
 

2
dv
Since x is not a variable of integration, we can factor out the first term of the inte￾grand to obtain:
f x X e e
x
x v x x
x
x
x
   
  

 



 
  
  

 1
2
1
1
1
2
2
1
2 1
2
2
 

 
 
  


2
dv
and making the substitution:
u v x x
x
    

 

 1
1 2 
 

implies that:
du dv  1 2 
so the integral reduces to:



  1
2
1
2
2
 e du
u100 Probability Foundations for Engineers
Therefore:
f x X e x
x x
x      

 
 1 
2
1
2
2
 

 (5.26)
which we recognize as the normal density. The same analysis for the marginal on 
Y yields:
f y Y e y
y y
y
  
  





 1 
2
1
2
2
 

 (5.27)
Once we have the marginal densities, we can construct the conditional densi￾ties. These are:
f y x f x y
f x
e
Y X
X Y
X
x y
x x
x
      
   

  
 

 


, ,
1
2 1 2
1
2 1 2
  



2 2
2
1
1 2
2
   
 




















 
 







 
x y y
x
x
x
x
y
y
y
y
x
x e


 



  
     

 




  
2
2 2
2
1
2 1 2
1
2 1
  
 
 
 
y
y x
e y
y y
x x
which is a univariate normal density having   
 y x y  y
x
     x  x and 
y x   y 2 2 2     1 and:
f x y
f x y
f y X Y e X Y
Y x
y x
x
x
      
  
  

  
 
, , 1
2 1 2
1
2 1 2 2
  
 
 
y   y y


 

 
2
which is a univariate normal density having   
 x y x  x
y
     y  y and 
 x y   x 2 2 2     1 .
Having seen the bivariate model, we can advance to normal distributions on 
random vectors of higher dimensionality. In order to do this, note that the expo￾nent in expression (5.25) is actually a quadratic form; that is:
1
2 1
2 2
2 2
  
 

 

    
 












 

  





x x x y y
x
x
x
y
y
y
y


     



 

  x y M x
y x y
x
y
 

 , 1
where M−1
 is a symmetric matrix comprised of the parameters  x y , a   nd as:
M x x y
x y y
       
     











 1
2 2 2
2 2 2
1
1 1
1
1
1
 

  

    


.Joint, Marginal and Conditional Distributions 101
For this case, the matrix M−1
 is obtained from the covariance matrix:
M x x y
x y y
 







  
  
2
2
and as in the univariate case, the variance terms appear in the denominators. 
To extend this form to the multivariate case, represent the random vector as 
X X     1 2 X Xr , , , for which the mean vector is       1 2 , ,  , r and the covari￾ance matrix, M is symmetric and positive definite. The elements of the covariance 
matrix are σ X Xi j ,
2 which is the covariance of the two variables. Then, the general 
form for the r-variate normal density is:
f x M X e r
X M X T
  
 
        1  1
2
2
1
2
2
1

 
(5.28)
where X X         1 1, , X X 2 2     , r r  and X T
    is the transpose of 
X   .
As in the case of the bivariate normal, the computation of probabilities is best 
performed using an available mathematical software package. In addition, all of 
the conceivable marginal and conditional densities based on the r-variate normal 
are normal densities. In each case, the construction follows exactly that used for 
the bivariate case above.
For the single-dimensional cases, the forms for the marginal densities are identi￾cal to expressions (5.26) and (5.27) above. To obtain an “s”-dimensional marginal 
from the r-variate model, we partition the random vector so that the “s” compo￾nents of X are listed first; that is, we rearrange the random vector to be:
X X         1 1, ,   X X s s   , , s s   1 1  ,Xr r  
with variables “s+1” to “r” to be integrated out. Naturally, the covariance matrix 
must also be rearranged. Let Ms represent the reorganized form of the matrix M; 
that is:
M
S R
R T s T  

 


where the sub-matrix S has dimension s x s and the sub-matrix T has dimen￾sion r–s x r–s. Then, the marginal density on the s-variate random vector 
X X s s      1 2 , , X X, is:
f x S X X X s e s
X S X
s
s s s s
T
1 2
1 1 1
2
2
1
2
2 , ,,
       
  
 


 
(5.29)
This is simply an s-variate normal.
For the conditional densities, the analysis is a little more involved as both the 
mean vector and the inverse of the covariance matrix must be adjusted to reflect 
the conditioning. We can use the bivariate case as a model for this analysis. 102 Probability Foundations for Engineers
The objective is to obtain the conditional density f x X X 1 2 , , , , X Xs s 1 ,X s r xr s    .      To do 
this, we start with a partition of the inverse of the reordered covariance matrix Ms
and label that inverse as Q. Represent the partitioned inverse as:
Q M Q U
U V s
s
T   

 

 1 1
1
With this definition, the inverse of the covariance matrix for the conditional 
density is:
Qs UV UT − − 1 1 1
To see that this is the form of the matrix inverse, consider the multiplication of 
Q and Ms. Since:
M Q I
I
I
S R
R T
Q U
U V
SQ s
s s
r s r s T
s
T
  s
  
  

 

  

 




 

  1 1
1
0
0
 
 








RU SU RV
R Q TU R U TV
T
T s T T
1 1
1 1
It must be the case that:
SQ RU I s T   1 s s 
and:
SU R 1   V 0
so:
R  SUV 1 1
and substituting gives:
SQ SUV U I s T   s s  1  1 1 .
Multiplying on both sides of the equality by S−1
 yields:
S Q U s V U   T   1 1 1 1 .
The interpretation of this form of the covariance matrix is that it represents the 
covariance of the random vector relative to the condition stated for the known 
variables. Note the correspondence to the bivariate case.
Next, for the mean vector, the dimensionality will be 1× s, so the vector will be 
obtained as:
  s      T s
T
s s s s r r
T       RT   x x   x   1 2     1 , , 1 1, , 2 2 .
Given these constructions, the conditional density is:
f x x Q UV U X X X X X s r s e ss T
s
X
s s r
s
1 2 1
1 1 1
1
2
2
1
2
2 , ,  , , , 
  
    
   

  s ss T s s
T
   Q U V U   X   1 1 1
(5.30)Joint, Marginal and Conditional Distributions 103
Example 5.19
Suppose we have a three-dimensional random vector X X     1 2 , , X X3 for which 
the mean vector is 15, , 9 12 covariance matrix is:
M 










3 0 2
0 2 1
2 1 2
and that we wish to construct the marginal density f x X X x 1 3 ,   1 3 , . First, we con￾struct the reordered covariance matrix, Ms. as:
M
S R
R T s T 










 

 


3 2 0
2 2 1
0 1 2
with
S  S 

 

 










 3 2
2 2
1 1
1 3
2
1 and
Since     15, , 9 12 , we take 2    15,12 and obtain:
f x X X x e x x x x
1 3
1 1 2 1 1 3 3 3
1 3
1
2 1
2 2 3
2 0 5
2 , , .     
 
          

      

 


           
  


3 2
1 2
0 707 1 3 3
2
1
2 15 2 15 12 3
2 1 . e x x x x 2 2   

 


.
It should also be clear that:
f x X e x
2
2 2
2
1
4 1 10
4       

Finally, to determine the conditional density f x X X X x x 1 3 ,  2   1 3 , ,   2 8 we start by 
constructing:
Q M Q U
U V s
ss
T     

 











1 1
1
3 4 2
4 6 3
2 3 2
and this implies that V UT   2 2 , , 1   3 and
Qs  



 


3 4
4 6104 Probability Foundations for Engineers
5.6 BIVARIATE AND MULTIVARIATE EXPONENTIAL DISTRIBUTIONS
Another multivariate probability model that has been found to be useful in engineer￾ing applications is the multivariate exponential distribution. As in the case of the 
multivariate normal distribution, the bivariate and multivariate exponential distribu￾tions provide tractable and useful models for representing multidimensional random 
phenomena. Also as in the case of the normal distributions, the marginal and condi￾tional distributions obtained from the multivariate exponential distribution maintain 
the distribution form (exponential). To appreciate the exponential models, start again 
with the bivariate case.
The most efficient way to define the bivariate exponential distribution is in terms 
of its survival function. In fact, this feature of the distribution is a reason to study it. 
Note particularly how this form simplifies the definition of the marginal distribu￾tions. Consider the random vector X Y,  in which both variables are non-negative; 
that is X Y     0 0 , ,     , . Then, the bivariate exponential distribution may be 
defined by:
F x X Y y X x Y y e x y x y
,
max ,   , P   r[ , ]       1 2 3   (5.31)
where 1 2   0 0 , ,  3  0 If 3  0, then X and Y are independent and 
F x X Y,   , y F  X Y   x F   y . When they are not independent, the marginal distributions 
for X and Y are defined by:
F x X X F x Y X x e x             , , P 0 r[ ]   1 3 (5.32)
In addition, T 1 0 5. and x2 2    8 9 1. Therefore:
Qs UV UT   



 

  


 

     



 

   1 1 1
3 4
4 6
2
3
1
2 2 3
3 4
4 6 ,
2 3
3 9
2
1 1
1 3
2





















and:
s
T  

 

  

 

     

 


15
12
0
1
1
2 1
15
12.5
so:
f x X X X x x e x x x
1 3 2
1 2 1 3
1 3 2
1
2 15 2 15 12
8 0 707
2 ,
.
, .        
        

5 3
2 3 12 5 2      

 

 x .
In summary, normal distributions of any meaningful dimensionality can be used 
to model a physical phenomenon. Those models are reasonably well behaved and 
conform to the marginal and conditional probability relationships as directly as 
do the other distributions we have examined.Joint, Marginal and Conditional Distributions 105
and
F y Y X F y Y Y y e y             , 0, Pr[ ]   2 3 (5.33)
Clearly, these marginal distributions are simply univariate exponential distributions.
The multivariate exponential distribution and its related functions are consider￾ably easier to analyze that the multivariate normal distribution. There is one impor￾tant caveat to that statement. Note that the bivariate exponential distribution is mostly 
continuous, but it has a singularity when X Y = . This singularity also occurs on a 
pairwise basis for the multivariate distribution. The singularity makes the construc￾tion of the density function a little intricate.
To define the bivariate exponential density requires that we analyze the distribu￾tion over three regions. Observe that the distribution may be restated as:
F x X Y,   , , y G 1 2   x y  G x  , y
where
G x y e e x y x y x y
1 3
1 2 3
1 2 3 1 2 3 , max , max ,      
               
  

and
G x y e x y
2 3
1 2 3
1 2 3 , max ,     
       
  
  
Then   
        
    
2 1 1 2 3
2 1 3
G x y
x y
F x y x y
F x y x
X Y
X Y
, ,
,
,
,
  
  
if
if 



 y
 and dG x x
dx
F x X Y x 2
3
, , ,
     
define the joint density on X Y, ; that is:
f x y
F x y x y
X Y F x y x y
X Y
, X Y
,
, ,
,
   ,
     
     
  
 

1 2 3
2 1 3
3
if
 if
F x X Y,   , x x  y




 if
(5.34)
Given the above construction of the bivariate exponential density function, it 
should be apparent that probability calculations for the bivariate exponential are 
much easier than those for the bivariate normal. In most cases, direct integration will 
be manageable. For example, if x y < , then:
F x X Y y F x y dxdy e
y x
X Y
y x
, ,   , ,             
0 0
1 2 3
0 0
1 2 3 1        x y
y x
x y
y
dxdy
e e dxdy
   
            
 
     
2 3
1 2 3
0 0
1 2 3
0
2      
     
    
    
    
  
3
0
1 2 3 1
1 2 3 1 1
e e dxdy
e e
y
x
x
x y
106 Probability Foundations for Engineers
Example 5.20
Suppose the realizations of a random vector X Y,  are well modeled by a bivari￾ate exponential distribution having 1 2   0. , 0007   0. . 0003and 3  0 0001. For 
this vector:
FX Y x y X Y e e x y
,
.   , P   r[ , ]           450 750   1 2 3 0 0007 450 0. . . 0004 750 0 62 0 54      e
Also:
F e e
e e
X Y x y
,
.
450,750 1 1
1 1
1 2 3
0 0007 450
       
    
    
 
  
         0 0004 750 0 27 0 026 0 07 . . . .
A case that may be more interesting is:
Pr[ , ]
, , , , ,
450 900 750 1250
900 1250 900 750
   
      
X Y
F F X Y X Y FX Y 450 1250 450 750
1 1 0 0007 900 0 0004 125
, , ,
. .
    
        
F
e e
X Y
0 0 0008 900
0 0003 750 0 0007 450
1
1 1 1
            
 
   
e
e e
.
. .    

  e 0 0004 1250 0 07
0 044
. .
.
The construction of the conditional distributions is also complicated by the 
singularity at X Y = . We can obtain the conditional distributions using limiting 
arguments. Consider the conditional distribution on Y given X. Let IX denote the 
interval u x, . For X in IX
F x y I X x Y y I
I X Y X X
X
, , Pr[ , ]
Pr       
 
so
FY X y x F x y I u x       X Y   X 
lim , ,
and taking the limit yields:
F y x
y
e y x
e y x
e
Y X
y
y
y y x
    

  
  



  
0 0
1 0
1
1
2
2
2 3
1
1 3
1


 

 
  











   1 3
y x
(5.35)Joint, Marginal and Conditional Distributions 107
The corresponding construction for the distribution on X given Y yields:
F x y
x
e x y
e x y
e
X Y
x
x
x x y
    

  
  



  
0 0
1 0
1
1
1
1
1 3
2
2 3
2


 

 
  











   2 3
x y
(5.36)
Example 5.21
Suppose again that the realizations of a random vector X Y,  are well modeled by a 
bivariate exponential distribution having 1 2   0. , 0007   0. . 0003 and 3  0 0001.
Suppose further that X = 500.0. Then:
F y X
y
e y
e Y X
y
y
     

  



500 0
0 0
1 0 500 0
1 0 0007
0
0 0003
0 0003
.
.
.
.
.
. .
.
. .
. .
0008
500 0
1 0 0007
0 0008
500 0
0 0003 0 0001
y
e y
y y x

 



    






so FY X   y X   300  500. . 0 0  086 while FY X   y X   1000. . 0  500 0 0  . . 3834
Observe that the conditional distribution has a jump at Y X = . For this example case, 
the jump is about 0.108. To see this, note that FY X   y X   499. . 99  500 0 0  .1393
and FY X   y X   500. . 01 500 0 0  . . 2469
Using the bivariate case as a template, observe that the trivariate exponential 
model on a random vector X Y, ,Z can be stated as:
F x y z X x Y y Z z
e
X Y Z
x y z x y
, ,
max ,
  , ,   Pr[ ,   , ]
     1 2     3 12 1   3 2 max ,   x z   3max ,   y z  123max ,   x y,z (5.37)
and that the corresponding forms apply to a random vector of any dimension of 
interest.
Finally, it is noted that like the univariate exponential distribution, the bivariate 
and multivariate exponential distribution display the memory-less property – the 
property that future survival given survival to a fixed point has the same probability 
as a new device. Exploring this feature of the bivariate exponential distribution may 
be approached in more than one way. Perhaps the most direct and understandable 
way is to consider a two component system in which the component life lengths 
are not independent and system longevity is well modeled by a bivariate exponen￾tial distribution in which λ1 represents the rate of failure for component 1, λ2 repre￾sents the rate of failure for component 2, and λ3 represents the rate of simultaneous 
failure for the two components. Then, suppose the system has survived to time “t” 108 Probability Foundations for Engineers
EXERCISES
1. Two fair six-sided dice are rolled. Let X represent the sum of the numbers 
observed on the two dice and let Y represent the magnitude of the differ￾ence between the two numbers. Construct the joint probability mass function 
fX Y,   x y, .
2. The joint p.m.f. for the discrete random variables M and N is:
f m n e
m n m MN m n n
m n m
, ! !    , ,        
  7
4 3 0 0
Compute Pr1 2   , . 4 6     M Y 
3. The joint p.m.f. for the discrete random variables M and N is:
f m n e
m n m MN m n n
m n m
, ! !    , ,        
  7
4 3 0 0
Compute PrM N   ,   3 5 and Pr1 3   , . 4 5     M N 
4. The joint p.m.f. for the discrete random variables M and N is:
f m n e
m n m MN m n n
m n m
, ! !    , ,        
  7
4 3 0 0
Construct the conditional probability mass functions fM N   m n and fN M   n m
and then compute fM N    3 6  , fN M   6 3 and fN M   6 3 , . n  10
5. The joint density on the random variables X and Y is:
fXY x y e x y y x y   , ,     ,     2 0 0
Compute Pr1 6   , . 4 1   0   X Y 
so both components have that age. Then, the continued survival of the system is 
represented as:
Pr[ , , ] ( , , ) X s ,  1 2 t Y   s t   X t   Y t   F s X Y 1 2 t s   t X t Y  t
The memory-less property occurs if:
FX Y s t s t F s X Y s FX Y t t , , ,   1 2   , ,    1 2   ,
and applying the definition of Expression (5.31) shows that this is the case so 
survival for a further s s 1 2 ,  has the same probability as surviving for that interval 
starting at the origin.Joint, Marginal and Conditional Distributions 109
6. The joint probability density function for the random variables X and Y is:
f x y
x XY   , ,   cy   x y ,  
5
0 1 1 5
Determine the value of c. Are X and Y independent?
7. Consider the joint density function f x y x xy XY   , ,   x y , 

 

     6
7 2
0 1 0 2 2
Compute PrX Y   . , . .   0 6 0 8
8. The joint density on the random variables X and Y is:
fXY x y e x y y x y   , ,     ,     2 0 0
Compute Pr0 2. . 5 0   55, . 0 60 0   . . 95   X Y 
9. The joint density on X and Y is fXY x y xe x y x y   , ,   , .      1 0 0 Compute 
PrX Y   . , . .   1 5 2 0
10. The joint density of X and Y is fXY   x y,   6 2 xy  x y  where 0 < < x y 1 0, . < < 1
Compute PrX Y   . , X  . .   0 4 0 3
11. Consider the joint density function fXY x y e y x x x   , ,     , .    4 0 0 2
Compute PrY  . .   1 6
12. Let X be selected at random from the set {1,2,3,4,5} and let Y be selected from 
the set {1,2,…X}. Identify the joint probability mass function of (X, Y) and com￾pute the conditional p.m.f. on Y given X = 4 and the conditional p.m.f. on X given 
Y = 3.
13. For the joint p.m.f. constructed in Problem 1, identify the marginal p.m.f. on Y.
14. The joint p.m.f. for the discrete random variables M and N is:
f m n e
m n m MN m n n
m n m
, ! !    , ,        
  7
4 3 0 0
Determine the marginal p.m.f. on M.
15. The joint density function on X and Y is fXY   x y, ,     x y 0 1   x y ,0 1  
Determine the marginal density functions for the two variables.
16. For the joint density function:
f x y
x X Y,   , ,   y x   , x 
1 0 0 1
Determine the marginal densities on X and Y.
17. For the discrete p.m.f. analyzed in Problems 2, 3 and 4, identify fN M   n m and 
compute PrN M   .   10  6
18. The joint density on the random variables X and Y is:
f x y
e e
y
x y
x y y
, ,
/
        
 
0 0110 Probability Foundations for Engineers
Find the conditional probability density function on X given Y and compute 
PrX Y   . .   3 6  4
19. The joint probability density function for the random variables X and Y is:
f x y
xe
y XY x y y
y
, . , ,
.
     
 0 02 0 0
0 01
2
Determine fX Y   x y and compute Pr[ . X Y   1 4  2 5. ].
20. For the joint density function f x y x xy XY   , ,   x y , 

 

     6
7 2
0 1 0 2 2
Compute Pr[ . 0 75 1   Y X . . 6 0   25]?
21. The joint density on the random variables X and Y is:
fXY x y e x y y x y   , ,     ,     2 0 0
Compute Pr3 4. .   6 2  2 .   Y X
22. The joint density on the random variables X and Y is:
fXY x y e x y y x y   , ,     ,     2 0 0
Construct the marginal densities on the two variables and the conditional density 
fX Y   x y . Then compute PrX Y   . . .   0 40 0  75
23. The joint density function on X and Y is fXY x y xe x y x y   , ,    , .        1 0 0
Construct the conditional densities fX Y   x y and fY X   y x and the conditional 
distribution functions F x X Y    y and F y Y X    x .
24. For the joint density defined in Problem 23, construct the joint distri￾bution function F x XY   , y and compute the values for FXY   1 5. ,0 6. ,
Pr0 7. . 5 1   5 0, .4 0   .6   x y and Pr0 7. . 5 1   5 0, .4 0   . . 6 1  2   x y  y
25. If the random variables X and Y have joint distribution fXY x y xe x y   ,      1 where 
0   x y   , , 0   for what value of y is PrY y   X . . ?    1 5  0 625
26. The random variables X and Y have joint distribution fXY x y xe x y   ,      1
where 0   x y   , . 0   Compute PrY X   . . , . Y    0 75 0  8 0 25 and 
PrY X   . . .   0 75 0  8
27. The joint probability density function for the random variables X and Y is:
fXY x y xe x y x y   , ,    ,        1 0 0
Compute Pr[ . Y Y   1 8  1 2. , X  1 5. ].
28. For the joint density function given in Problem 7, are the variables X and Y
independent?
29. For the joint density function given in Problem 15, are X and Y independent?
30. Show that fX Y   x y  f x X   implies independence of X and Y.Joint, Marginal and Conditional Distributions 111
31. The joint density on X and Y is fXY   x y, ,   2 0 x y   , . 0 1 y  Construct the 
conditional density fX Y   x y being sure to identify the range of variation on X
and compute the value of F y X Y   0 3. .   0 5 .
32. The joint density on X and Y is fXY   x y, ,   2 0 x y   , . 0 1 y 
Construct the conditional density fY X   y x and compute the value of 
PrY Y   . . , . X  .   0 64 0  40 0 24
33. The joint density of X and Y is defined as:
fXY x y y x e y x y y y   , ,         ,  
1 
4
0
Determine fX Y   x y  X Y     and Pr . 1 4
34. For the joint density function:
fXY x y c y x e y x y y y   , ,         ,   2 2  0
Determine the value of c and construct the marginal density functions on X and 
Y.
35. For the joint density function:
fXY x y e x y x y   , ,    ,        0 0
determine PrX Y    and PrX a  .  
36. The joint density on the random vector (X,Y,Z) is fXYZ   x y, ,z x   2  y z where 
the ranges for the variables are 0 ≤ ≤ x y 1 0, , ≤ ≤ 1 0 ≤ ≤z 1. Construct an alge￾braic expression for the conditional density fXZY   x z, .  y
37. For a random vector (X,Y) having a joint normal distribution with 
  x y   4 5. , 2 5.  x y   0 2. , 5 0  .10 and   0 4. , use a mathematical soft￾ware system to compute Pr4 1. . 5 4   65, . 2 55 2   . . 75   X Y  Also compute 
Pr4 1. . 5 4   65  2 5. . 5   X Y  
38. The height and weight of male college students displays a bivariate nor￾mal distribution with H  175cm and  H  9 cm for height, W  81 kg and 
 W  11 kg for weight and   0 6. . 8 Compute Pr[ . H W   195 0 9 cm  1 0. ] kg
which applies to male basketball players and PrW H   .   72 kg  170 0 cm
which applies to male sprinters.
39. Consider the three-dimensional normal density specified in Example 5.19. 
Determine the joint marginal density on X X 2 1    X2 , and indicate why these 
two variables are independent.
40. For a bivariate normal density, suppose the quadratic form is:
1
3
6 1 2 2 4 4 2 2 2    x x   y y  x y     
what are the values of   X Y , ,  X Y , ?  X Y, 2 2 2 and112 Probability Foundations for Engineers
41. For the normal distribution on the random vector X X    1, , X X234 ,X having 
mean vector      1 0, , 2 3, and covariance matrix:
M  













2010
0 5 0 3
1010
0 3 0 2
Identify the marginal joint density on the vector X X1 2  ,  and the conditional 
joint density
X X1 4 X X 2 3  , ,  
42. For the bivariate exponential distribution on X Y,  and having 
1   0. , 002   2 3 0. . 001 and  0 0007 compute Pr[ , X Y   Pr ],   750  500 and 
Pr[ , X Y > > 600 300].
43. For the bivariate exponential distribution on X Y,  and having 
1 2   0. , 004  0.007 and 3  0.002 compute PrX Y     625  500 , 
Pr[ ] X Y   325  500 and determine the approximate size of the probability 
jump in F x X Y     y 500 at X Y = .DOI: 10.1201/9781003294382-6 113
Once a random variable has been defined, there can be many reasons for defining 
function of that variable. One reasonable example would be a conversion of a tem￾perature measurement from a Celsius scale to a Fahrenheit scale. Another would be 
the conversion from sales volume to revenue. In general, since a random variable 
is a function for which the range is the real line, the construction of another func￾tion – a function of the random variable – should be a reasonable thing to do and 
should conform to usual algebraic behaviors. The corresponding distribution func￾tion for the functional variable can be constructed at the same time. This is an impor￾tant extension of the probability concepts treated in this text so it is included in this 
chapter. However, the analysis of general functions of random variables and random 
vectors is placed later in the chapter because there is a particular function – called 
expectation – that is central to many probability analyses so it is presented first.
6.1 EXPECTATION
The expectation of a random variable, X, is defined as a weighted sum of the possible 
values that X may take. The weighting is the corresponding probability measure. For 
a discrete random variable, X, the expectation or expected value of X is denoted by 
E X and is computed as:
E X x X x
x
        Pr . (6.1)
The corresponding definition for a continuous random variable, say Y, is com￾puted as:
E Y yf y dy
y
   Y    . (6.2)
Note that in both cases the possible values of the random variable are multiplied 
by their corresponding probability measure and that these products are accumulated 
over the range of the random variable.
It is important to note that the expected value of a random variable is a descriptor 
of the distribution on that variable. In fact, it is the first moment about the origin of 
the p.m.f. or p.d.f., (whichever term applies) and it is often referred to as the mean of 
the distribution. The expected value does, in fact, correspond to the center of gravity 
Expectation and 
Functions of 
Random Variables
6114 Probability Foundations for Engineers
of the probability measure. Thus, the expected value of a random variable – its 
mean – is an indication of the center of the distribution.
Keeping in mind the fact that the p.d.f. (or p.m.f.) is a function, we realize that it 
has higher order moments than just the first. The moments jointly characterize all of 
the features of the function. In the study of probability, we often consider higher 
moments of a distribution – particularly the second moment. In general, the kth 
moment of a distribution is defined as:
E X x X x k
x
k 
 
       Pr (6.3)
for a discrete random variable and:
E Y y f y dy k
y
k Y 
 
     (6.4)
for one that is continuous.
The emphasis on the second moment arises from the fact that it is used to compute 
the “variance” of a distribution. The variance is a measure of the dispersion in the 
probability measure and is computed as:
Var X E    X X E X  
      2 2 2 (6.5)
Note that the variance is also the first moment about the mean; that is, we could 
define the variance as:
VarX E    X E X   
 

2
.
Using the properties of expectation that are discussed later, this expression can be 
shown to equal Expression (6.5). First, consider some realizations of the mean and 
variance.
All distribution functions have moments and each of the commonly used distribu￾tions that have been discussed in this text has a mean and a variance. Usually, using 
Expressions (6.3) or (6.4), the determination of the values for the moments is not too 
difficult. Consider some examples.
Example 6.1
For the binomial distribution, we have:
E X x n
x p q x n
x n x p q n
x x
n
x n x
x
n
x n x
x
n
   

 

       




   0 1 1
!
! !
!
1
1
1 1 0
1
1
    
   
      

 

  
! !
!
! !
n x p q
np n
x n x p q np
x n x
x
n
x n xExpectation and Functions of Random Variables 115
where the final step depends on the fact that the summation represents all of the 
probability for a binomial random variable having range zero to n −1 and is thus 
equal to one. In the case of the variance, we start with:
E X x n
x p q x n
x n x p q
x
n
x n x
x
n
x n x
x
n
2
0
2
1
2
1

 
  

 

     





  !
! ! 

     
       
    

 

xn
x n x p q
np x n
x n x
x n x
x
n
!
! !
!
!
1
1 1 1
1 1 0
1

      
      
 
 

 


!
!
! !
p q
np x n
x n x p q
x n x
x
n
x n x
x
1
1 0
1
1
1
1 1
1 

 
 



  
     








   
0
1
1
1 1
1
1
1
1
n
x n x
x
n
n
x n x p q
np n
!
! !
!
! !
!
x n x p q
np n p n
x
x n x
x
n
      








      

 
 


2
1
1
2
1
2 0
2
2
1 1 1
1 1
2
     







       
       
 
! ! n x p q np n p
np n p np
x n x
  np  q n   p npq 2 2
Then, applying expression (6.5) yields:
Var  X E  X E X n p npq np npq  
          2 2 2 2 2 .
Example 6.2
For the geometric distribution, we have:
E K kpq p kq p
d
dq q p
d
dq q
k
k
k
k
k
k
k
k         


 









    1
1
0
1
0 0




 


 

 
   p   d
dq q
p
q
p
p p
1
1
1
1
1 2 2
and for the variance, we start with:
E K k pq p k k k q pq k k q
k
k
k
k
k
2 k
1
2 1
0
2 1
0
2  1  
           









   p kq
pq
d
dq q
p
pq
d
dq q
p
k
k
k
k
k
k








      









0
1
0
2
2
2
2
0
1 1
 


 

  
           pq
d
dq q p
pq
q p
p p
p p
p
p
2
2 3 3 2
1
1
1 2
1
1 2 1 1 2 .116 Probability Foundations for Engineers
Example 6.3
For the exponential distribution, we have:
E T tf t T dt te dt te e dt te e t t t t            
 
 

  
   0 0 0
1 

     t

 

 

0
1

and
E T t e dt t e te dt t e t
e 2 t t t t t
0
2 2
0
2 2 2 2 
 
        

 

  
  

    

  

  
0
2
2
0
2
2 2 2


  


    

 

 
e dt
t e
t
e e
t
t t t .
Using expression (6.5), we obtain:
Var  T E  T E T  
     2 2
2 2 2
2 1 1
   .
Notice an important feature of the moments of a distribution. They are expressed 
in terms of the parameters of the distribution. Thus, a binomial distribution hav￾ing parameters n = 80 and p = 0 0. 5 has mean X  4 and variance  X
2  3 8. 0. 
Similarly, an exponential distribution with parameter   0. / 001 hr has a mean 
value of E  T  1000. h0 rs.
6.2 THREE PROPERTIES OF EXPECTATION
The analysis and use of expected values is extensive. It is therefore useful to be aware 
of three properties of expectation. The first of these is that expectation is a “linear 
operator”. By this, we mean that:
E X Y E X E Y k m k m      
 
 (6.6)
We can see this for the simplest case as:
E X Y x y f x y dydx xf x y dydx yf
x y
X Y
x y
X Y
x y
             X    , , , , ,
, ,
,
, ,
Y
x
X Y
y
X Y
x y dydx
xf x y dx yf x y dydx E X E Y
 
              .
Using Expression (6.5), we obtain:
Var K E K E K p
p p
q
p    
 
        2 2
2 2 2
2 1 .Expectation and Functions of Random Variables 117
This construction also confirms that the expected value of a sum of random vari￾ables equals the sum of their expected values regardless of whether or not the vari￾ables are independent. Thus, expectation has the property of being linear.
A further implication of the linearity of the expectation is that for constants, say 
a and b:
E aX  b aE X b       (6.7)
and this behavior also extends to higher order moments as:
E aX b aE X b k k        .
The assertion that the first moment about the mean equals Expression (6.5) is 
based on the linearity of the expectation; that is, starting with:
VarX E    X E X   
 

2
We can perform the squaring operation and distribute the expectation across the 
resulting sum. Then:
Var X E X E X E X XE X E X
E X
        
 
        
 

  
2 2 2
2
2
   
 
   
        

 

 
2
2
2
2 2
E XE X E E X
E X E X E X E X E X2 2 
 
    E X.
This construction also illustrates the fact that an expected value, E X, is a con￾stant rather than a random variable.
A second property of expectation is that the variance “operator” is almost linear. 
In fact, the variance is not linear but we say it is almost linear because:
Var  aX  b a  Var X  2 (6.8)
Notice that a constant has no variance. It is constant and does not vary.
The third property of expectation is that it applies to functions of random variables 
as well as to the random variables. We will examine functions of random variables 
later in this chapter. For now, suppose that we have defined a function, say g X , on 
the random variable X. We can obtain the expected value of that function by applying 
the definition of expectation directly; that is, for a discrete random variable:
E g X g x X x
x
    
          Pr (6.9)118 Probability Foundations for Engineers
and for the case in which the random variable is continuous:
E g X g x f x dx
x
  X 
 
       . (6.10)
Essentially, the expectation of the function is the weighted sum of the values the 
function can assume where the weights are the associated probability measures.
Example 6.4
Suppose we roll a fair six-sided die and receive a payment equal to three times 
the number shown by the die. What is our expected gain? For this experiment, the 
payout function is:
g x   3x
so the expected payout is:
E g X   
                    1
6
3 1
6
6 1
6
9 1
6
12 1
6
15 1
6
18 10.5
For the more advanced reader, can you show that Var g X   
  26. ? 25
6.3 EXPECTATION AND RANDOM VECTORS
The definition of expectation extends directly to random vectors and the interpreta￾tion is the same. The expected values correspond to the moments of the distributions 
and thus characterize the distributions. In order to develop the expectation relations 
for random vectors, start with the two-dimensional case.
Suppose we have the random vector X Y,  and its associated distribution function 
F x X Y,   , y . As we observed above, finding the values for the individual expectations 
corresponds to constructing the marginal densities (or p.m.f.) and using those to 
obtain moments. Thus, it is the expectations of multiplicative forms that have not yet 
been treated. For a discrete random vector, the expected value function is:
E XY xy X x Y y
x y
          Pr , (6.11)
and the corresponding form for a continuous random vector is:
E XY xyf xy dydx
x y
   X Y    , . (6.12)
These definitions conform to the pattern set for the single-dimensional case. 
Consider two examples.Expectation and Functions of Random Variables 119
Example 6.5
For the discrete bivariate density defined in Example 5.5, we construct the 
expected value of the random vector Y Y1 2 ,  as:
E YY y y e
y y y e
y y
y y y y
y
1 2
0 0
1 2
17
1 2 1
17
2 1
2 1 2 1 11 6    11    



  
 ! ! 2 1
2 1 2 1
2
1
2
1 0
1 1
1 2 1
17
1
11 6
1
11


 
  



       

y
y y y
e
y
y y y y
y
! !
    
  
     

 
   1
1
1 11 6
2 1 2
1 0
1
2 1
1 2 1 1
2 1 2 1
y
y y
y y y y
y y y y
!
!
! !
11
1
1
1
11 6 17
1
2
2 1 0
1
2
1
1
2 1
2
1 2 e y
y
y
y y y
y
 y y y


 

      




 

 !
1
2
2
2
11
17
1
11
1 1 17
17
1
2 1
2
17
1 0
2
  
  
    


 

 



e y
y
e y
y
y
y
y
!
2
2
2
2
1
2
17
1 0
2 1
2 1 0 1
11
1 17
1


 
 
 
  
        
    
y e y
y y
y
y ! !
17
1
11
1 17
1
11
2
2
2
1
2
1 0
17 2 1
2
1
y
y
y
y
e y
y e

 




 
  
     
    
!
!
7
1 0
1
2
17 17
2
2 17
1
11 17 11 198 0
y
y
y e e
 
 

  
       
! .
Example 6.6
For the continuous random vector X Y,  suppose:
f x XY y e x y y x y   , ,     ,     2 0 0
Then, the expected value of the product is:
E XY xyf x y dxdy xye dxdy ye x
y
XY
y
x y y
y
      
 
 


    0 0 0 0 0 0
, 2 2 e dxdy
ye xe e dy ye ye y e dy
x
y x x y y y y


  

               2 2
0
0
0
2 2 2
        

 

        2 1
2
1
4
1
2
1
2
1
4
2 2 2 2 2 2
0
ye e ye e y e ye e y y y y y y y

   

 
 2 1   1
4
1
4
1
When we consider higher moments, the general definitions described above 
apply but there is a special case that we should examine. This is the covariance. 
First note that the higher moments are given by:
E X Y x y X x Y y m n
x y
m n 
 
    Pr  ,  (6.13)120 Probability Foundations for Engineers
Example 6.7
For the discrete bivariate density defined in Example 5.5, we found in Example 
6.5 that E  YY1 2  198.0 and we found in Example 5.5 that:
f y e
y
f y e
y Y
y
Y
y
1
1
2
2
1
11
1
2
17
2
11 17      
 
! ! and
so we can see that E  Y1  11.0 and E  Y2  17.0 and:
cov Y Y1 2 , .   E Y1 2 Y E Y E 1 2 Y 198 0 11 0. . 17 0 11.          
       0
Also, Var  Y1  11.0 and Var  Y2  17.0 so:
YY
Y Y
Var Y Var Y 1 2
1 2
1 2
11 0
11 0 17 0  0 804  
    


    cov , .
. . .
in the discrete case and:
E X Y x y f xy dydx m n
x y
m n X Y 
 
     , . (6.14)
in the continuous case. It is important to recognize that these higher moments 
exist but they are rarely used in engineering applications so we will not pursue 
them further here. Instead, consider the covariance of two random variables.
Recall that the variance is not a distribution moment but is a function of dis￾tribution moments that describes dispersion in probability. The covariance is a 
comparable measure and is defined as:
cov  X Y,   E X   E X    Y E    Y E XY E X E Y  
         (6.15)
The covariance provides information about the dispersion in probabilities and 
the mutual behavior of the two variables. The covariance may be either positive 
or negative. A positive covariance indicates that X tends to increase (decrease) as 
Y increases (decreases). A negative covariance indicates that X tends to decrease 
(increase) as Y increases (decreases). In fact, we say that the two variables may be 
correlated and their correlation is:
 
  XY XY
X Y
X Y
Var X Var Y   
     cov , 2
(6.16)
The correlation, ρXY, between X and Y will lie in the interval (–1, 1) and its sign 
will be determined by the numerator – the covariance – as the denominator is 
positive by definition.Expectation and Functions of Random Variables 121
Example 6.8
For the continuous density of Example 6.6, we obtained E  XY  1 0. and in 
Example 5.10, we found that:
f x X e f y e e x Y y y           2 2 2 2 2 and
so we can compute the expectations E   X  0 5. 0 and E  Y  1 5. 0 and:
cov  X Y, .  E X  Y E    X E   Y   1 0   0 5. . 0 1  50  0 2. 5
Also, Var  X  0 2. 5 and Var  Y  1 7. 5 so:
XY
XY
Var X Var Y   
    
    cov .
. . . 0 25
0 25 1 75
0 378
Now that the relationships of covariance and correlation have been defined, the 
reader is encouraged to review the discussion of the multivariate normal distribu￾tion of section 5.5 of this text. The covariance and correlation terms presented 
there are exactly the same as those described above. A further example is pro￾vided by the bivariate exponential distribution.
Example 6.9
For the bivariate exponential distribution described in Section 5.6, the covariance 
and correlation measures conform to the relationships described above; that is:
E  XY   


 

 




 

 1 1 1
  1 2   3 1   3 2 3
and since the corresponding marginal distributions have
E  X E  Y
    
1 1
  1 3   2 3
and
cov  X Y,           

      
3
1 2 3 1 3 2 3
and


     X Y,     
3
1 2 3
because
Var  X V  ar Y
  
  
  
1 1
1 3
2
2 3
2     and .122 Probability Foundations for Engineers
6.4 CONDITIONAL EXPECTATION
As we found in Chapter 5, conditional probabilities can be defined using the set 
relationships of Chapter 2 and these models can be useful and informative. We also 
noted that conditional distributions are proper distributions in the same sense that 
univariate and joint distributions are. A logical next observation is that conditional 
distributions have moments. In fact, the definitions for expected values and for vari￾ances apply directly to conditional distributions.
A particularly important feature of covariance is that independent random vari￾ables have a covariance of zero. This is reasonably apparent when we recognize 
the fact that independent random variables, say X and Y have:
E  XY  E X E Y .
Finally, it is appropriate to note three properties of the covariance. These are:
cov  X Y, c  ov  Y X,
cov  X X,  Var X 
cov  aX, , bY  abCov X  Y
Each of these follows directly from the definition of Expression (6.15).
The extension to higher dimensions of the expectation relationships defined 
here for two dimensions is direct. For a random vector, X X     1 2 , , X X, n
Expressions (6.11) and (6.12) need only be expanded to the dimension of 
the vector and the same applies to expressions (6.13) and (6.14). Covariance 
should be structured in matrix form as in the case of the multivariate normal 
distribution. The global variance of a multivariate distribution is rarely consid￾ered interesting but may be computed as:
Var X E X E X
i
n
      i i   






 1
(6.17)
Example 6.10
For the discrete bivariate density defined in Example 5.5, we found in Example 
5.7 that:
f y y e
y y
f y y
y
y Y Y
y y
2 1 Y Y
2 1
2 1 1 2
6
2 1
1 2
2
1
6
| | ( | ) !  ( | )     

 


 
and 11
17
6
17
1 2 1


 

 

 


y y yExpectation and Functions of Random Variables 123
Example 6.11
For the continuous density of Example 6.6, we found the conditional densities in 
Example 5.12 to be:
f x y e
e X Y f y x e
x
y Y X
y x
| | ( | ) (  | )  


   
1 and
The application of Expressions (6.4) and (6.6) to these density functions yields 
the conditional means and conditional variances:
E X Y x e
e dx e ye
e
y x
y
y y
y [ | ]       


 

0 1
1
1
Var X Y E X Y E X Y y e e ye y e
e
y y y y
     
 
      

   

2 2 2 2 2 2 2 1 2 2
1 [ | ] y
The application of Expressions (6.3) and (6.5) to these density functions yields 
the conditional means and conditional variances:
E Y Y y
y
y y
y y y y
[ | 1 2]
0
1
2
1 1
2 1 2 1
11
17
6
17
11
1  

 




 

 

 

 


 7 2


 

y
Var Y Y 1 2 E Y1 Y E Y Y y 2 2 2 1 2 2
11
17
6
17   | 
       

 




 

  
Note that the conditional distribution on Y1 given Y2 is a binomial so the mean 
and variance are easily identified. For these expressions, if we take Y2 = 4, we 
obtain E[ | Y Y 1 2 = = 4 2 ] .588 and Var[ | Y Y 1 2 = = 4 0 ] .913. Also:
E Y Y y e
y y
y y y e
y y
y y
y y
[ | ] ! 2 1 2
6
2 1 0
2 1 1
6
2 1
2 1
2 1
6         

  
 
 
  6
6
2 1
2 1
2 1
2 1
2 1
0
2 1
6
2 1
1
y y
y y
y y
y y
y y
y y e
y y
y

 
  
 
  
        
!
! 0
6
2 1
1
6 6 2 1          e
y y
y
y y
!
Var Y Y 2 1 E Y2 Y E Y Y 2 1 2   | |   2 1 | 6  
    
Again, observe that the conditional distribution on Y2 given Y1 is a Poisson so the 
mean and variance are easily identified. For these expressions, if we take Y1 = 10, 
we obtain E[ | Y Y 2 1 = = 10] 16 and Var[ | Y Y 2 1 = = 10] 6.124 Probability Foundations for Engineers
Example 6.12
For the Poisson random variable having   12.0:
E X X x f x
F F
xe
x F x
X
X X x
x
X x
[ | ] !
   
          


 

8   7
1
1 7
1
1 7 8 8
 
0 0
7
1
7
1
1 7 1
 




 

 







     
xe
x
xe
x
F e x
x
x
x
X x
x
 

 
 
! !
 







      ! . . . . 1
0 910
12 0 0 55 12 576
If we take Y = 6, we obtain E[ | X Y = = 6 0 ] .985 and Var[ | X Y = = 6 0 ] .913. Also:
E Y X ye dy x
x
y x [ | ]   

     1
Var  Y X| |  E Y X E Y X|  
     2 2 1
so taking X = 8 yields E[ | Y X] = 9.
A further observation concerning conditional expectation is that it applies to 
univariate distributions just as well as to joint distributions. If we have a discrete 
random variable for which the p.m.f. is f x X  , the mean of a corresponding con￾ditional p.m.f. is obtained as:
E X X a xf x X a x f x
F a x a
x
X X a
x a
x
X
X
[ | ] ( | ) .
max max
  |    
   


  1
The corresponding construction for the variance is:
Var  X X| |  a E   X X a E X X| . a  
     2 2
In the case of a continuous random variable, say T, the same logic yields:
E T T tf t T dt t
f t
F T T dt T
T
[ | ] ( | )   |    
      




0 0
and
Var  T T| |    E T T E [ | T T ].  
      2 2
Of course, other conditions are managed in the corresponding fashion.Expectation and Functions of Random Variables 125
E X X x f x
F F
x e
x
x
x
X
X X x
x
x
[ | ] !
2
8
2
0
2
0
7 2
8
7
1
1 7
   
      



 

     e
x
F e x
x
x
X x
x











        







  



  
!
!
1
1 7 1
1
0
2
1
7
. . . . 910   156 0 3  478  167 514
Var  X X| |    E X X E X X| . .  
 8 8     8 9  366 2 2
Example 6.13
For the exponential failure distribution of having   0.025,
E T T t
f t
F
dt e te d T
T
t [ | ] . .    
        
 24
24
1
1 0
24
0 025 24 0
0
24
  t
e te e
e e
e
t t    


 

 
 
 
 
 

1
1
1
1 24 1
1 0 6 0
24
0 6 0 6
. 0
. .
|  

 . . 6  10 807
E T T t
f t
F
dt e t e dt
e
T
T
t [ | ] .
2
0
24
2
0 60
0
24
2 24
24
1
1
1
1
   
   
 
  
  

  

   

 

 
  
0 6
2
2
0
24 2
0 6 2
2 2 2
2 24 48 2
.
.
t e
t
e e
e   t t t
 
  


 


   1
163 942 0 6 e . .
Var  T T| |    E T T E [ | T T ] . .  
 24 24   24  47 148 2 2
One final aspect of conditional expectation is that the use of conditioning may 
make it easier to obtain expected values. In particular, we may have a joint dis￾tribution for which the construction of the marginal probability measure on one 
of the variables is quite difficult so direct calculation of the mean and variance 
for that variable is correspondingly complicated. In such a case, we may exploit 
the fact that:
E Y E E Y X E Y x f x dx
x
     X 
 
       | | (6.18)
or
E Y E E Y X E Y X x X x
x
       
 | |     Pr   (6.19)
depending on whether the variables are continuous or discrete.126 Probability Foundations for Engineers
Example 6.14
For the joint density:
f x XY y e x y y x y   , ,     ,     2 0 0
we have found that:
f x X e f y x e x Y X
y x          2 2 and | ( | )
Using these:
E Y X yf y x dy ye dy e ye dy
e ye e
x
Y X
x
x y x
x
y
x y y
[ | ] ( | )   | 
  
 



 
  
          |x x x x e xe e x 1
and:
E   Y x    f x X  dx  E X   


0
1 1 3
2
as E   X  1
2.
Example 6.15
In Example 6.10, we constructed:
E Y Y y
y
y y
y y y y
[ | 1 2]
0
1
2
1 1
2 1 2 1
11
17
6
17
11
1  

 




 

 

 

 


 7 2


 

y
Thus, we can see that:
E Y y f y E Y
y
1 Y
0
2 2 2
2
2
11
17
11
17
11
17    

 

    

 

    

 

 

   17  11
These examples illustrate the fact that it can be much easier to use conditioning 
to perform probability calculations than it is to make the calculations directly. The 
reason for this is that the conditional probabilities are based on some knowledge 
that sometimes simplifies their calculations.
This completes the enumeration of the various aspects of the functions of ran￾dom variables that correspond to expectation. It is now time to move on to gen￾eral functions of random variables and random vectors. This discussion, in turn, 
will be followed by an examination of the special class of functions in which we 
sum independent random variables.Expectation and Functions of Random Variables 127
6.5 GENERAL FUNCTIONS OF RANDOM VARIABLES
6.5.1 One-Dimensional Functions
The reason for defining a function of a random variable may be as simple as a wish to 
change temperature scales. As we know, for a temperature measured on the Celsius 
scale, X, the conversion to the Fahrenheit scale is defined by the function:
Y   1 8. . X 32
For functions of this nature, the transfer of the probabilities is straight forward. 
We can readily state that:
PrY X    Pr . P     r X   
 140 1 8 32 140 52
In fact, for most linear functions, the computation of probabilities follows this 
pattern. Simply substituting the function in the probability statement and simplifying 
the expression to isolate the random variable should yield the probability.
If the functional relationship is non-linear, the computation should follow directly 
from the form of the function. In the case of an exponential function such as:
Y eaX =
we obtain probabilities on Y as:
Pr Y y Pr e y Pr aX ln y X Pr ln . a
y aX       
 
       

 


1
While the computations of the probabilities are straight forward as shown above, 
we can also construct the distribution function and density of probability mass func￾tion. The general rule for doing this is as follows:
a. For a random variable X, having probability density function or probabil￾ity mass function fX   x and a quantity Y which is defined as a function 
of X as Y  g X , we first find the inverse of the function, say g Y    1 and 
take the derivative of the inverse function, d
dy g y    1 .
b. Then:
f y
d
dy Y X    g y  f g    y   1 1 (6.20)
This is to say, we take the absolute value of the derivative of the inverse function 
and multiply it by the density function on X evaluated at the value which is the 
inverse of the functional value for which the density is desired. Consider some 
examples.128 Probability Foundations for Engineers
Example 6.16
Let FX x e x      1  so f x X e x      
Suppose Y g    X c  X
Then:
g y y
c
d
dy g y
d
dy
y
c c
        

 

  1 1 1 and
Therefore:
f y d
dy g y f g y c Y X e c
y
             1 1  
And:
F y f y dy c Y e dy e
y
Y
y
c
y c
y
           
0 0
1   
Numerically, suppose   0.0001 and c = 15. Then:
FX   x e      e      4000 1 1 0 330 0.0001 4000 0 4. .
FY   y e      e  

 

   60000 1 1 0 330
0 0001
15 60000 0 4
.
. .
Example 6.17
Let FX x e x
      1 

 so f x x X e x
  
   




1 
 and suppose again that Y g    X c  X. 
Then the inverse function and its derivative are the same as in the previous exam￾ple and so:
f y d
dy g y f g y
y
c e c Y X
y
c
         


 


  











 1 1
1












y
c e
y
c  

 


 
1
and
FY y f y dy e
y
Y
y
c         

 


0
1 

because dy c = dx. Numerically, for     1 6. , 1500.0 and c = 3 1. , 5Expectation and Functions of Random Variables 129
Example 6.18
Let F x x a
b a X    
 so f x
b a X    
1
Suppose Y g    X X  ln
Then:
g y e d
dy g y e   y y       1 1 and
Therefore:
f y d
dy g y f g y e
b a Y X
y
            
  1 1
and:
F y f y dy e
b a dy e
b a Y
y
Y
y y y
        
   0 0
1
Numerically, suppose b = 5 and a = 1.
FX   x   
 2 6  2 6 1 0
5 0 1 0 . 0 40 . .
. . .
FY   y   ln2 6. . 0 9555  0 4. 0
6.5.2  Multidimensional Functions
The extension to constructing functions of random vectors is actually the generaliza￾tion of the method described above for a single-dimensional random variable. The 
same motivation applies; that is, engineering applications may involve computing 
a function of a random vector in order to obtain a meaningful system performance 
measure. One example of this is the computation of a consumer demand function for 
a set of inter-related or competing products.
For a random vector, X X     1 2 X Xn , , , , we might define the functions Yi i  g X 
to obtain the function vector Y     Y Y1 2 Yn , , , . The functions g X i  must be differen￾tiable and the system of equations must be “invertible” in the sense that we can 
FX   x e      e     800 1 1 0 306 8001500 0 366
1 6.
. .
FY   y e      e     


 

  2520 1 1 0 306
2520 3 15 1500 0 336
1 6
. .
.
.130 Probability Foundations for Engineers
obtain the set of “inverse” functions X h i i    Y . When the functions are so defined, 
we can obtain the joint density on the random vector Y as:
fY n   y y 1 2 , ,y f  X n   h y 1 2  , , h y  h y  J x  1 (6.21)
where J   x is the Jacobian matrix of partial derivatives of the functions g X i . For 
clarification, consider the two-dimensional case. Suppose we have a bivariate prob￾ability density, fX x x 1 2   , , that X X    1 2 X Y    Y Y1 2 , , , and we have well behaved 
functions Y1 1  g X  and Y2 2  g X  so that the Jacobian matrix:
J x x
g
x
g
x
g
x
g
x
1 2
1
1
1
2
2
1
2
2
  , 


















exists. Assume further that the functions X h 1 1    Y and X h 2 2    Y can be con￾structed. In this case, expression (6.2) can be applied directly to obtain fY y y 1 2   , .
Example 6.19
Suppose the random variables X1 and X2 are independent and Gamma distributed 
with:
f x x e f x x e X
x
X
x
1
1 1 1
2
2 2 2
1 1
1
1
2 2 1
2
         
    



     
  and
and that:
Y g x x X X Y g x x X
X X 1 1 1 2 1 2 2 2 1 2 1
1 2
            , , and
For these functions:
J x x
g
x
g
x
g
x
x
x x
g
x
x
x x
1 2
1
1
1
2
2
1
2
1 2
2
2
2
1
1 2
1 1
  , 

  
 

 
  

  
  











 2
and
J x x x x 1 2
1 2
1   ,  

We construct the functions h1 1   y y, 2 and h2 1   y y, 2 as:
x y x y x
x y x
x
y 2 1 1 2 1 x y y x y y 1 1 1
1
1
   1 1 2 2 1 2 1     and s   o and    Expectation and Functions of Random Variables 131
Example 6.20
Suppose the random variables X1 and X2 have the joint density function:
f x x x x X X x x 1 2 1 2
1
2 2
2 1 2
1 ,   , ,   1 1 , 
and that y1 1 = x x2 and y x
x 2 1
2
= . What is the density on Y ? For these functions:
J x x
g
x x g
x x
g
x x
g
x
x
x
1 2
1
1 2 1
2 1
2
1 2
2
2
1
2
2
1   , 

  
 

  
  












and
J x x x
x
x
x 1 2 1
2
2
1
2
2
2 2   ,   
We construct the functions h1 1   y y, 2 and h2 1   y y, 2 as:
x y y x y
y 1 1 2 2 1
2
= and =
and then, using Expression (6.21), we obtain:
f y y
y y Y Y1 2 1 2
1
3
2 2
3
2
1
2 ,   , 
In summary, there are many engineering applications in which we wish to com￾pute a function of a random variable or random vector. The translation from the 
random quantity to a system performance measure proceeds as described above. In 
addition, once the density function or probability mass function for the performance 
measure is obtained, we may compute moments for the performance measure.
Now, as we know:
f x x x x e X X
x x
1 2
1 2 1 2 1 2
1 2 1
1 2 1
1 2
,   ,     
       
 
    
 
Therefore:
f y y
y y y y e Y Y
y y y y
1 2
1 2 1 2 1 2 1 2
1 2
1 2
1
1 2
1 1 1 ,   ,                       
    
   
       
 
 
 

 
     
1 2
1 2
1
2 2 1 2
1 1 2
1 2 1 2
y y 1 y ey1132 Probability Foundations for Engineers
6.6 EXPECTATION AND FUNCTIONS OF MULTIPLE RANDOM 
VARIABLES
Once the distribution on a function of one or more random variables has been con￾structed, the mean and variance (as well as other moments) of that distribution can 
be obtained in the usual manner. However, for some functions, the mean and variance 
can be determined directly from those of the variables in the function. Equation 6.7 
states that:
E aX  b aE X b      
and this relationship extends to sums and differences of random variables; that is, for 
the sum Z   aX bY C:
E Z aE X bE Y c         
Similarly, using the definition of variance, we find that:
Var Z E Z E Z E aX bY c E aX bY c
E
   
 
         
 
      

2 2 2 2
a X abXY acX b Y bcY c
a E X abE X E Y
2 2 2 2 2
2 2
2 2 2
2
       

                  
 

 
 
   
2 2 2 2 2
2 2 2
acE X b E Y bcE Y E c
a E  X E X  
 
               
  
b E Y E Y ab E XY E X E Y
Var X
2 2 2 2
      Va  r Y 2 C ab o var X Y,
It is true that this type of analysis is not applicable to all functions. It usually 
applies well to linear functions and many products. When it does not apply, one must 
use the general definitions of mean and variance. Note particularly that when it does 
apply, the variables in the function need not be independent.
6.7 SUMS OF INDEPENDENT RANDOM VARIABLES
In the analysis of random quantities, a functional form that occurs quite often is 
the sum. We may wish to compute multiple interval product demand as a sum of 
the demand in individual intervals. We may wish to compute the buildup of com￾ponent dimensional tolerances. We may wish to calculate overall facility usage by 
a diverse set of hospital patients or we may wish to compute the aggregate value of 
a stock portfolio. While it is not always the case that the constituent quantities are 
independent, it is often the case that the quantities of interest are independent. When 
the random variables included in a sum are independent, calculation of the sum will 
conform to the rules described here.
To describe the rules of addition, it is sufficient to consider two random variables, 
say X and Y, as sums of greater numbers of variables can always be performed on a 
pairwise basis. Suppose the distributions F x X   and F y Y   are known and our Expectation and Functions of Random Variables 133
objective is to identify F z Z   where Z   X Y. Starting with the discrete case, observe 
that in order for Z to take any specific value, it must be the case that X have a value 
no greater than Z and Y have the value Z – X. In addition, all such combinations of X
and Y should be considered. Algebraically, this means:
F z Z Z z X Y z f i j f j
i
z
j
z
             X Y      
Pr Pr 0 0
(6.22)
Note that the expression exactly represents probabilities corresponding to the 
above verbal description. Note further that the product calculation is only appropriate 
if the variables are independent. Two equivalent forms to Expression (6.22) which 
are reminiscent of the conditioning construction are:
F z Z F z j f j
j
z
    X Y     
0
(6.23)
and
F z Z f i F j i
i
z
   X Y     

0
. (6.24)
For the p.m.f., the construction is the same:
fZ z f z j f j
j
z
    X Y     
0
(6.25)
or equivalently:
fZ z f i f j i
i
z
   X Y     

0
. (6.26)
Here are some general application examples.
Example 6.21
Suppose the two random variables have binomial distributions so that:
f x n
x p p f y m
y X p p x n x
Y y m y    

 

       

 

     
1 1 1 1 and 2 2
Then:
f z f z j f j n
z j p p m
j Z
j
z
X Y
j
z
z j n z j
         


 

    
 
     0 0
1 1 1

 

     p p j m j
2 1 2134 Probability Foundations for Engineers
Example 6.22
Suppose the two random variables have Poisson distributions so that:
f x e x f y e y X
x
Y
y
          1 2
1 2
! ! and
Then:
f z f z j f j e
z j
e j Z
j
z
X Y
j
z z j j
           


 

  
  
  0 0
1 2
1 2  
! !
!
!
! ! !


 


     
 


     e
z
z
z j j
e
z j
z
z j j    
    1 2 1 2
0
1 2   1 2
z
which is a Poisson p.m.f.
In the special case in which the event probabilities are the same, so p1 2 = = p p
f z n
z j p p m
j Z p p p q
j
z
z j n z j j m j z n    


 

    

 

    

   
0
1 1  

 
 


 




 


  

 


m z
j
z
z n m z
n
z j
m
j
n m
z p q
0
which is a binomial p.m.f.
Example 6.23
Suppose the two random variables have geometric distributions so that:
f n N q p f m q p n M m        
1 1 1 2 1 and 2
For K   N M
f k K f k j f j q p q p p p
j
k
N M
j
k
k j j
j
k
         




  

  1
1
1
1
1
1 1 2
1 2 1 2
1

  

1
1
1
2
1 q q k j j
In the special case in which the event probabilities are the same, so p1 2 = = p p
f k p q k p q
k K p q
j
k
k k k          

 




  

2
1
1
2 2 2 2 2 1
1
1
which is a negative binomial p.m.f.Expectation and Functions of Random Variables 135
These examples illustrate the facts that the probabilities for the sums can be 
computed directly and in some cases, the identity of the distribution is preserved. 
Here is a more specific example.
Example 6.24
Suppose the one day demand for a particular digital camera has the p.m.f.:
d 0 1 2 3
Pr(D = d) 0.1 0.4 0.3 0.2
What are the p.m.f. and the c.d.f. on the two day demand?
The p.m.f. is:
d2 0 1 2 3 4 5 6
Pr(D2 = d2) 0.01 0.08 0.22 0.28 0.25 0.12 0.04
The c.d.f. is
d2 0 1 2 3 4 5 6
Pr(D2 = d2) 0.01 0.09 0.31 0.59 0.84 0.96 1.00
The rules for the distribution on the sum of two continuous random variables are 
similar to those for discrete random variables. Essentially, the sums are replaced 
by integrals. The concept is the same that all combinations of the two variables 
should be included. Therefore:
FZ z Z z X Y z f z y f y dydx
z z
             X Y     
  Pr Pr . (6.27)
Equivalent forms are:
FZ z F z y f y dy
z
    X Y     
 (6.28)
and:
FZ z f x F z x dx
z
   X Y     

 . (6.29)
The corresponding expressions for the probability density function are:
f z Z f z y f y dy
z
    X Y     
 (6.30)136 Probability Foundations for Engineers
Example 6.25
Suppose the two random variables have exponential distributions so that:
f x X e f y e x Y y             1 2 1 2 and
Then:
f z Z f z y f y dy e e dy e
z
X Y
z
z y y z          

       0
1 2 1 2 1 2 1       
0
1 2
2 1 0
1 2
2 1
1
2 1
z
y
z
y z
e dy
e e e

   

   
  







 
 

 
 
 
  
      








      
 
 
 
 
 
1
2 1 1 1 2
2 1
1 2
2
z
z e z z e e
For the special case in which both distributions have the same value for the rate 
parameter, so that 1 2    ,
f z Z f z y f y dy e e dy e dy
z
X Y
z
z y y z
z
                   
0 0
2
0
2        e y ze z z   z     0
2
which is a gamma density function.
and
f z Z f x f z x dx
z
   X Y     

 . (6.31)
Here are some general examples.
Example 6.26
Suppose the two random variables have gamma distributions so that:
f x x e f y y e X
x
Y
y
         
    



     
1
1
1
2 1
2
1 1 1 2 2 2
  and
Then:
f z f z y f y dy z y e Z
z
X Y
z z y
           
        
0 0
1
1
1
2 1 1 1  2

    

y e dy
z y y e
y
z
 
    

 
 
2 2
1 2 1 2 1
1
2
1 2
1 2 0
1 1
 
  
 
        

 
z y y
z z
dy
e z y y e
   
             

       
 
2
1 2 1 1 1 2 2 2
1 2 0
1 1
 
  1 y
dyExpectation and Functions of Random Variables 137
For the special case in which both distributions have the same value for the rate 
parameter, so that
f z f z y f y dy z y e y Z
z
X Y
z z y
           
        
0 0
1
1
1 1
 2 2

     

 
      
 
        
1
2
1 2 0
1 1 1 2 1 2
e dy
z y y e
y
z
z y

     


 

 
y
z z
dy
e z y y dy
z
z
       

   
 

 
 
    
 
 
1 2 1 2
1 2
1
1 2 0
1 1
2
 
2
1 2 1 2
1 2 1 2
2 1 2 0
1 1

   
 
      

 
 

    
   
e z y y dy
z
z
z
z z
 
   
   
 

 

 

 

  2
1 2 0
1 2 1 1 e z y
z
y
z dy
z z   
   
Now, let w y
z = so dw dy
z = and:
f z z e Z w w dw
z
           

     
 

      

1 2 1 2 1 2
1
1 2 0
1
1 1 1
 
1 2 1 2 1 1 2 1 2
1 2
1 2
1 2
   
   
   
   
       
 
 
 
z e  z z
 
 

 
  
1
1 2
e z
  
which is a gamma density function.
Finally, here is a specific example.
Example 6.27
Suppose a machining station at a manufacturing facility takes an exponentially 
distributed time to process a single piece. The distribution has parameter   4 / hr. 
Suppose further that the cost of holding in-process inventory is proportional to the 
length of time a workpiece waits for processing. If a particular workpiece arrives 
to the inventory and finds four pieces ahead of it with the first one just entering the 
machining station, what is the distribution on the time until processing is started 
on the arriving piece?
For this problem, the total waiting time is T T T  1 2 T T 3 4 T . We know that all 
of the times have exponential distributions with a common rate parameter so fol￾lowing Example 6.24, if X T  1 2 T and Y T  3 4 T then:
f x X f x t f t dt e e dt e
x
T T
x
x t t x                  
0
2 2 2
0
2 2
1 2
2 2      
0
2 2 2 0
2
x
x x x dt e t xe         138 Probability Foundations for Engineers
EXERCISES
1. A discrete random variable, X, has the following probability mass function:
f x
x
x
x
x
x
X   














0 08 0
0 24 1
0 30 2
0 20 3
0 18 4
.
.
.
.
.
What are the values for E X and VarX?
2. For the p.m.f. associated with the sum of the numbers observed when two fair 
six-sided dice are rolled, compute E X and VarX.
3. At a local pizza restaurant, the hourly demand, D, for a particular type of pizza 
has the following probability mass function:
f d
d
d
d
d
d
d
d
D   










0 08 0
0 12 1
0 18 2
0 24 3
0 16 4
0 12 5
0 10 6
.
.
.
.
.
.
.








Compute E D and VarD
f y Y f y t f t dt e e dt e
y
T T
y
y t t y                  
0
4 4 4
0
4 2
3 4
4 4      
0
4 2 4 0
2
y
y y y dt e t ye         
Next, we note that T X T   Y both of which have gamma distributions with the 
same parameters so we apply the result of Example 6.25 to obtain:
f t t e t e t e T
t t t
T          
    
 
  1 2   1 2 1 
1 2
4 3 4 3 4 4
4
128
  3
Of course, the result of Example 6.25 implied this result so we could have 
simply stated it at the start. The stepwise construction is interesting because of the 
transition in density function forms.Expectation and Functions of Random Variables 139
4. A random variable Y has the following p.m.f.:
f y
y
c y
c y
c y
c y
c y
Y   
















0 0
0 8 1
1 2 2
3
0 6 4
0 4 4
.
.
.
.
Compute E Y  and VarY . (Exercise 8, Ch. 4)
5. A random variable Y has the following cumulative distribution function:
F y
y
y
y
y
y
y
Y   
 
 
 
 








0 1 
0 12
0 27
0 63
0 88
1
1 2
2 3
3 4
4 5
5
.
.
.
 . 

Compute E Y  and VarY .
6. A random variable Y has the following cumulative distribution function:
F y
y
y
y
y
y
Y   
  
 
 
 
 
0 0
0 10 0 0 5
0 35 0 5 1
0 70 1 1 5
0 90 1 5 2
1
. .
. .
. .
. .
.00 2   









 y
Compute E Y  and VarY .
7. A discrete random variable, X, has the following probability mass function:
f x
x
x
x
x
x
X   














0 08 0
0 24 1
0 30 2
0 20 3
0 18 4
.
.
.
.
.
What are the values for E X V arX E  X X Var X X  
 
 
 , , 1 1 and ?
8. If a manufacturing process generates units of product of which 1.5% are defec￾tive and we take an inspection sample of n = 80 copies of the product, what are 
the mean and variance for the number of defective units found in the sample? 
Explain why it is reasonable to have a non-integer expected value.140 Probability Foundations for Engineers
9. The density function on the random variable X is:
fX   x x   1 5 0 75 0 x x   2 2 . . ,
Compute E X and VarX.
10. Suppose a fair six-sided die is tossed until Tk, the kth time a 3 occurs and that the 
tosses are then continued until Tk+4, the k+4th time a 3 occurs. State expressions 
for E T T k k     4  and VarT T k k     4  and compute their values when k = 5.
11. For the bearing inspection process described in Exercise 27 of Chapter 4, what 
are the mean and variance for the number of inspections performed?
12. If a random variable has the probability density defined as:
fX   x x   1 5  1 0   x 1 2 . ,
compute E X and VarX.
13. Compute E T  and VarT  for the exponential variable having   0.024.
14. The density function on a random variable X is f x x X     x  2 , 0 2.
a. Determine F x X  .
b. Compute E X.
c. Compute VarX.
d. Determine F X X ( . 1 25 0 | . ≥ 75).
15. The random variable X has the density f x
x X     1 , . 1 2 X  71828. Compute 
the mean and variance for the distribution on X.
16. A random variable X has a normal distribution and it is known that 
PrX  . .   17 1 0  692 while Pr[ . X X   17 4 1 | . 7 1] .  0 795. Determine the val￾ues of µX and σ X for the distribution.
17. A random variable X has E X  2 and VarX  4 2. .
a. Compute E   X  
 
 1 2
.
b. Compute Var2 3 X   .
18. Suppose a random variable has density function:
f x
ax bx x
elsewhere X        


2 0 1
0
If E X  0 6. , what are the values of a and b?
19. A random variable X has E X  4 4. and VarX  1 6. .
a. Compute E 2 3 X X 2      

.
b. Compute Var
3 1 X
4
 

 

.Expectation and Functions of Random Variables 141
20. For a binomial random variable, X, having n = 75 and p = 0 0. 2 compute 
E   X  
 
 3 2
.
21. A random variable X has E X  24.5 and VarX  3 6. . Compute 
E 2 2 X 2 1 5 2
. .     
 

 and Var2 4 X   .
22. A random variable X has E X  2 and VarX  4 2. .
a. Compute E   X  
 
 1 2
.
b. Compute Var2 3 X   .
23. The density function on a random variable X is f x x X     x  2 , . 0 2
a. Determine fX ( | x X ≥ 0 9. ).
b. Compute E[ | X X ≥ 0 9. ]
24. The random variable X has the probability density defined as:
fX   x x   1 5 0 75 0 x x   2 2 . . , .
Determine the density on the random variable Y   0 5. X 1. State the range of 
the variable Y.
25. If the random variables X and Y are independent and have the same probability 
distribution, determine E   X Y 
 

2
.
26. For the joint p.m.f. on the discrete random variables M and N:
f m n e
m n m MN m n n
m n m
, ! !    , ,        
  7
4 3 0 0
Determine E MN.
27. The random variables X and Y have the joint density function:
fX   x x   y x , , 0 1   0 1  y .
Compute E[X] and Var[X].
28. The joint probability density function for the random variables X and Y is:
fXY   x y, ,  x y 0 1   x y ,0 1  
Compute CovarXY  .
29. Consider the joint density function:
f x y x xy XY   , ,   x y , 

 

     6
7 2
0 1 0 2 2
Compute CovarXY  .142 Probability Foundations for Engineers
30. The joint density on X and Y is fXY   x y, ,   2 0 x y   ,0 1 y  .
Compute Covar , X Y  .
31. Consider the joint density function fXY x y xe x y x y   , ,    ,        1 0 0
Identify fY X| ( | y x) and construct an algebraic expression for E[ | Y X] and for 
Var[ | Y X].
32. For the joint p.m.f. on the discrete random variables M and N:
f m n e
m n m MN m n n
m n m
, ! !    , ,        
  7
4 3 0 0
Determine
E M N   Var M n E   N M   Var N M   
 12 , , 12 4 4 and .
33. The joint density on the random variables X and Y is:
f x y
e e
y
x y
x y y
, ,
/
        
 
0 0
Find the expected value of X given that Y = y.
34. The joint density on the random variables X and Y is:
fXY x y xe x y x y   , ,    ,        1 0 0
Compute E[ | Y X = 5].
35. The joint probability density function for the random variables X and Y is:
f x y
xe
y XY x y y
y
, . , ,
.
     
 0 02 0 0
0 01
2
Determine Pr[ . X Y   . , E X Y . ]   1 4 2 5  2 5 and Var[ | X Y = 2 5. ]
36. For the p.m.f. of Problem 1, determine the values for E X X 1  
 and VarX X 1  
.
37. The joint probability mass function for the random variables X and Y is:
f x y xy XY   , ,   x y , ,  , , , 30
1 2 1 234
Compute PrX Y   , , E X , E Y       2 2 and Covar , X Y  .
38. If X and Y are independent Normal random variables having 
  X X   24 0 4 0 8   Y Y   0 1 2 2 2 . , . , . . and , compute E X Y Var X Y       and .
39. The density function on a random variable X is f x x X     x  2 , 0 2. Compute
E X X 0 5.  
Expectation and Functions of Random Variables 143
40. The joint density function on X and Y is fXY   x y, ,     x y 0 1   x y ,0 1  
Compute CovarX Y,  .
41. Consider the joint density function f x y
y XY e e x y
x
y y   , ,    ,     1  0 0
a. Identify F x X Y| ( | y).
b. Compute Pr[ | X Y   1 2. ] 5 .
c. Compute E[ | X Y = 2 5. ].
42. The independent random variables X and Y have the densities:
fX Y   x x    f y    y  1
4
0 4 1
2 , , and 0 2
Determine the density on Z   X Y.
43. The random variables X and Y have joint density 
fXY x y e x y x y   , ,    ,        0 0 .
Compute Pr[ ] X Y < .
44. Suppose X and Y are independent exponential random variables. Identify the 
distribution function on Z = hX/Y and construct an expression for Pr[ ] X Y < .
45. The joint probability density function for the random variables X and Y is:
f x y
x XY   , ,   cy   x y ,  
5
0 1 1 5
Compute PrX Y  . .   3 5
46. The joint density function on X and Y is fXY   x y, ,     x y 0 1   x y ,0 1   .
Compute PrX Y  .   0 8 .
47. Suppose X and Y are independent and identically distributed uniform random 
variables over the range (0,1) and that we define the variables U = X+Y and V = 
X/Y. Determine the joint density on U and V.
48. Suppose X and Y are independent and identically distributed exponential random 
variables over the range (0,1) and that we define the variables U = X+Y and V = 
X/(X+Y). Determine the joint density on U and V.
49. For the random variables analyzed in Problem 26, compute E[N–M] and 
Var[N–M].
50. For the random variables analyzed in Problem 29, compute E[X+Y] and 
Var[X+Y].
51. For the random variables analyzed in Problem 30, compute E[X+Y] and 
Var[X+Y].
52. John tosses a fair six-sided die 12 times. Let X represent the number of times 
that a 4 occurs in those 12 tosses. Mary then tosses the same die 8 times. Let Y
represent the number of times that a 2 occurs in those 8 tosses and let Z = X+Y. 
Compute Pr[Z = 4].144 Probability Foundations for Engineers
53. Friday sales of a popular beverage is a Poisson random variable with param￾eter fri  5 8. while the corresponding Saturday sales volumes are Poisson with 
parameter sat  6 4. . Identify the distribution on Z, the volume of two day sales, 
and compute PrZ    12 .
54. A single pump is to be used sequentially to drain two liquid reserves. Let X
represent the time to drain the first reserve and Y represent the time to drain the 
second reserve. Assuming:
fX x e f y e x Y y         0 4 0 4 0 4 0 4 . . . . and
Compute Pr[X+Y>4.5].
55. Suppose X and Y are independent random variables with the exponential 
distributions:
fX x e f y e x Y y         2 2 4 2 2 4 and . .
For Z   X Y determine fZ  z and compute Pr[ . Z > 4 8].
56. Suppose X, Y and Z are independent exponential random variables with com￾mon parameter   1 5. 0. Compute the probability that the sum of these random 
variables does not exceed 4.2.
57. Suppose X and Y are normally distributed random variables having 
  X Y   24 0 20 0  X  4 0 2 . , . , . , and Y
2  2 2. 5. Compute PrX Y  .   47 5 .DOI: 10.1201/9781003294382-7 145
The distribution functions for most random variables permit the construction of a 
transform function that can make various analyses easier. The transform function is 
nearly the same as the Laplace transform and is known as the moment generating 
function.
The moment generating function for a distribution contains all of the information 
about the distribution and can be used to “generate the moments” of the distribution 
as well as for other analyses some of which are described in this chapter.
Probability distributions can be fully characterized by their moments so it is often 
useful to be able to generate the distribution moments. In addition, the distributions 
on sums of random variables and on other functions of random variables can be iden￾tified using moment generating functions.
7.1 CONSTRUCTION OF THE MOMENT GENERATING 
FUNCTION
Regardless of whether a random variable is discrete or continuous, the moment gen￾erating function (m.g.f.) for its distribution is defined as an expectation. Specifically:
MX E e x      
 
 (7.1)
In this construction, the variable θ is the transform variable and is real valued.
Given our understanding of expectation, we know that the application of this defi￾nition to a discrete random variable implies:
MX E e e X x x
x
x       
 
       Pr (7.2)
and the corresponding form for a continuous random variable is:
MX E e e f x dx x
x
x  X      
 
     (7.3)
Then, once the moment generating function is constructed, the moments of the 
distribution are obtained as successive derivatives of the m.g.f.; that is:
E X d
d
M k k
k X 
 
     
  0
(7.4)
Moment Generating 
Functions 7146 Probability Foundations for Engineers
so the kth moment is computed as the kth derivative of the m.g.f. evaluated at   0.
This relationship applies to both discrete and continuous random variables.
Example 7.1
A binomial distribution having parameters n and p has moment generating 
function:
M e n
x p q n
x X pe q q pe
x
n
x x n x
x
n x n x        

 

  

 

   




  0 0
 n
The mean and variance of the distribution may be obtained as:
E X d
d
M d
d X q pe npe q pe np n n                

 
  


 
 0 0
1
0
E X d
d
M d
d npe q pe
npe q pe
X
n
n
2 2
2
0
1
0
1

 
       
    





  
 

  n n p e q pe
np n n p np n p np
n
     
       


1
1
2 2 2
0
2 2 2 2
 

so as we know:
Var  X E  X E X np n p np n p np np npq  
           2 2 2 2 2 2 2 2 .
Example 7.2
An exponential distribution having parameter λ has moment generating function:
MX E e e e dx e dx x x x x    
 
        
 
    



     
0 0
The mean and variance of the distribution may be obtained as:
E X d
d
M d
d    X     
      




 

      0 0
2
0
1
E X d
d
M d
d X 2 2
2
0
2
0
3
0
2
2 2 
 
    
   
      




 

     Moment Generating Functions 147
so:
Var  X E  X E X  
     2 2
2 2 2
2 1 1
   .
The moment generating function can be constructed for empirical distributions 
as well as for the standard distribution families. Consider the two-day demand 
distribution constructed in Example 6.23. For that distribution, the m.g.f. is:
M e X x
e e e e
X
x
x  
   
     
     
 Pr
0 0. . 1 0 08 0 2. . 2 0 28 0 2. . 5 0 1 2 3 4 2 0 04 5 6 e e    .
Then, the mean and variance are obtained in the same manner as for other 
distributions.
E X d
d     e e   e e   e e 

     0 01 0 08 0 22 0 28 0 25 0 12 0 04 2 3 4 5 6 . . . . . . . 

     

 
       


0
2 3 4 5 6
0
0 0. . 8 0 e e 44 0 8. . 4 1 e e0 0. . 60e e 0 24  3 2.
E X d
d e e e e e e 2 2 3 4 5 6  0 08 0 44 0 84 1 0 0 60 0 24  
        

      . . . . . . 
     



        
0
2 3 4 5 6
0
0 0. . 8 0 e e 88 2 5. . 4 4 e e0 3. . 0 1 e e 44 11.94
so:
Var  X E  X E X  
         2 2 2 11. . 94 3 2 1 7. . 0
Next, the fact that the normal distribution is so widely used makes it worthwhile 
to include the construction of its moment generating function here. Recall that the 
density function for the normal distribution is:
f x X e
x
      1
2 2
2
2
2



Applying the definition of the m.g.f.:
MX e e dx e dx x
x x x
  


  
    


  


  
  1
2
1
2 2
2
2
2
2
2
2
2148 Probability Foundations for Engineers
7.2 CONVOLUTIONS
There are numerous applications of the moment generating functions, but the most 
widely implemented is the determination of the distribution on the sum of inde￾pendent random variables. Recall, that some of the sums were constructed directly 
in Chapter 6. Unfortunately, not all sums of independent random variables can 
be analyzed directly as was done in Chapter 6. For example, suppose X and Y are 
Working with the exponent, we express it using a common denominator and 
then complete the square:
2 2
2
2
2
2 2
2 2 2
2
2 2 2
2
2 2 2
  

  

  
x x x x x x
x x
          
             

    


2 2 2 4 2 4
2
2 2 2 2 4
2
2
2
2
2
  
         
   
x
x 

  

 

  
2 2
2
2 2 4
2
2 2
2
2 2
2
2
2
2
2
2
    

        
 x
Returning this form of the exponent to the m.g.f. expression:
MX e dx e
x
 
 

     
   

 
      


  1
2 2
2
2
2
2
2
2 2
2
( 2 2 2 2
1
2 2
2
2
2 2
2
2 2

 

  
e dx
e
x

   


(
because the integral corresponds to an integral over the entire range of a normal 
random variable having mean equal to    2
. Note that taking the first two 
derivatives returns the moments with which we are familiar.
E X d
d
M d
d    X     e e   







 





   
  

  

0
2
0
2 2
2 2 2 2


0

E X d
d
M d
d e
e
X 2 2
2
0
2 2
0
2 2
2 2
2 2

 
       
 






  
 

  

  
     

  
  

2 2 2
0
2 2
2 2
eMoment Generating Functions 149
independent random variables with normal distributions. The computation rules of 
Chapter 6 imply that for Z = X+Y:
f z f z y f y dy e e Z
z
X Y
z z y
x
y x
x
y
        
 
      
 




2
2
2
2
2 2
2
2
2
2


y
y
dy

and evaluating this integral is very difficult. On the other hand, the distribution on the 
sum of independent random variables is known to have moment generating function 
comprised of the product of the moment generating functions of the variables in the 
sum.
In general as well as for the case of two normal variables:
MZ X      M M   Y    (7.5)
so for the two normal variables:
MZ e e e e X X Y Y X Y
X Y
Z
           
  
  
             

2 2 2 2 2 2 2
2 2 2
2 2
2
 Z
which we recognize as the m.g.f. for a normal random variable, in this case Z, having:
  z X     Y Z and     X Y 2 2 2
.
The process of constructing the distribution on the sum of independent random 
variables is called taking the convolution of the variables. Thus, we would say that Z
is the convolution of X and Y.
As indicated in Chapter 6, sums of more than two random variables can be accu￾mulated pairwise. However, using the moment generating functions, the distribution 
on the sum of several independent random variables can be identified directly. For the 
set of independent random variables, say X X1 2 Xn , ,…, , the random variable 
Y   X X 1 2    Xn has a distribution for which the m.g.f. is:
MY M
i
n
Xi        1
(7.6)
Thus, in principle, the construction of the distribution on the sum is straightfor￾ward. This is because in constructing the moment generating function, we are per￾forming a transform operation on the probability function. Thus, we can recapture 
the probability function by inverting the transform. In point of fact, the inversion can 
be quite difficult. However, the situation is not dire.
Moment generating functions have the characteristic that they are unique. If we 
have a moment generating function with the format:
MZ e Z Z
    
   
2 2
2150 Probability Foundations for Engineers
it must be the case that the random variable Z has a normal distribution with the indi￾cated parameters. Similarly, if we find a random variable with the moment generating 
function:
MX q pe n
       
then it must be the case that the random variable X has a binomial distribution. Similar 
statements apply to each of the standard distributions described in this text so it is 
only in the analysis of other probability distributions that we must actually perform 
the inversion operation on the m.g.f. For those cases, most modern mathematical 
software packages now have inversion algorithms that in the worst case yield the 
distribution in numerical form.
7.3 JOINT MOMENT GENERATING FUNCTIONS
In Chapter 5, we observed that joint probability distributions can be used to model 
the behaviors of systems that are described in terms of random vectors. It is reason￾able to expect that those joint distributions also have moment generating functions 
and, in fact, they do. In general, for a random vector X X     1 2 X Xn , ,. , having joint 
distribution function F x X X X n x x 1 2 , , n 1 2    , ,, the joint m.g.f. is defined as:
M E e
e
X X X n X X X
x x x
n
n n
n
1 2
1 1 2 2
1 2
, , 1 2  , , ,        
 

  
     


 1 1 2 2
1 2 1 2 1 x x x X X X n n n n
n f x x x dx dx       , ,   , ,, ,, (7.7)
As before, successive derivatives of the moment generating function yield the 
moments of the distribution. However, the relationships can be a bit complicated. In 
the case of a two-dimensional vector:

     
    

 n
m n m X Y n m m M E X Y
 
  

2 1
1 2 0
0
1
2
, , (7.8)
and the extension to higher dimensionality follows the pattern implied in Expression 
(7.8). Thus, for a random vector X X     1 2 X Xn , ,. , having joint distribution func￾tion F x X X X n x x 1 2 , , n 1 2    , ,,
E X X X d
d
M r r n
s r r r s
r r n
s r r r
n
n 1 2
1 2
1 2 1 2 1  1 2  1 2 1
  
  



 
     X X 1 2 , , X n n 1 2 0      , ,, |   (7.9)
Consider two-dimensional discrete and continuous examples.Moment Generating Functions 151
Example 7.3
Suppose the following empirical p.m.f. describes a process of interest to us:
fX,Y(1,1) = 1/9 fX,Y (1,2) = 1/6 fX,Y (1,3) = 1/18
fX,Y (2,1) = 1/18 f(2,2) = 1/9 fX,Y (2,3) = 1/9
fX,Y (3,1) = 1/9 fX,Y (3,2) = 1/9 fX,Y (3,3) = 1/6
For this joint distribution, the joint moment generating function is:
M E e e f x y
e
X Y X Y
x y
X Y ,  , , XY    

1 2
1
3
1
3
1 2 1 2
1 1
9
   
 
   


 


    

   

        
 
2 1 2 1 2 1 2 1 2
1 2
1
6
1
18
1
18
1
9
1
9
2 3 2 2 2
2 3
e e e e
e   
1   
9
1
9
1
6
3 3 1 2 1 2 2 3 1 2 3 e e e      
The first joint moment is:
E  XY  M e X Y e e 
           2
2 1
1 2
1 2 3
9
2
6
3
18
2
1
1 2 1 2 1 2
 
        , , 8
4
9
6
9
3
9
6
9
9
6
2
2 2 2 3 3 3 2 3
1 2
1 2 1 2 1 2 1 2
e
e e e e e
 
        

         1 2 3
Evaluating at 1 2    0

         

2
2 1
1 2 0
0
1
2
1
9
2
6
3
18
2
18
4
9
6
9
3
9
6
9
9
  6
  

MX Y, , | 78
18
13
3 
Example 7.4
For the joint density:
f x XY y e x y y x y   , ,     ,     2 0 0
The joint moment generating function is:
MX Y E e e e dxdy e X Y
y
x y x y ,  ,     1 2
0 0 0
1 2 1 2 1     2 2  
   

  


        

       

  



 


 
 

2 1
2 1
0
1
0
1
1
1 2 1
1
y
y
x
y x
e dxdy
e e
0 1 0
2 1 1
1
1 2 1
y
y y dy  e e dy    

       
   152 Probability Foundations for Engineers
Example 7.5
The moment generating function for a bivariate normal distribution is constructed 
using the same analytical steps that were employed for the univariate distribution. 
The result is:
M E e e X Y X Y
x y x x
x
,  ,  
 



1 2
1
2 1
1 2
1 2 2
   
 
  



     
 
 

 

    
 

















	
	
	

2 2
2
2 2 1
 





  
x y y
x y
x
x
y
y
y
y
dydx
e x y x x y y
                 1 2 1
2 2 1 2 2
2 2 2
2
Clearly, differentiation will return the distribution moments of which the most 
interesting are:
E XY
MX Y    x y x y
  
 
  

2 1 2
2 1 0
0
1
2
,  ,
      

and
cov  XY  E X  Y E    X E  Y    x y  x y     x y   x y
    
 

  

        
   
 2
1
2
1
1
1
1 0
1 2
1 2
1
2 2 1
2

 
  

e e dy
e
y y
y 1
2
2
1
1
1
1
2
2
2 1
2
0
1 2 2 1
2 1
 


 


     


 

 
     
 
   
  e y 
1
1 1 1
1 1 1
2
1
1
1 2 2
 2 1 2
          
           








 

  
  
  2 1 2   2 1 2
2 1 1
2
          1 1 1 
         
Here again:
E   XY  MX Y

     

  
       

2
2 1
1 2 0
0
2
2
2 1
1
2
2 1
1 1  
 


  

, ,
1
2 1 1 1 4 1 2 1
2
2 2
0
0
2 1 2
2
2 2
1
2
    

                 



    


      
           
        


1
1 1 1
2 1 1 4 1 2 1
1
2 1 2
2 3
0
0
1
2

   

 
       1 1
4 12
8
1 3Moment Generating Functions 153
Example 7.6
The bivariate exponential distribution has a moment generating function that is 
easier to construct than that for the bivariate normal because it may be obtained 
by direct integration over the three regions that are meaningful for that distribu￾tion, X Y < , X Y > and X Y = . Thus:
MX Y E e e F x y dx x y
x y
x y , ,  , ,    X Y     1 2 2 1 3 1 2 1 2    
 
       


 dy
e F x y dxdy e F x x
x y
x y X Y x y s

        



        1 2 1 2 1 2 3
0
, , , 3  
   




 
     

dx
e e dxdy
e
y
x y x y
y
0
2 1 3
0 0
1 2 1 3 2
1
    

  
x y x y x e dxdy e e     

                        2 1 2 3 1 2 1 2 3
1 2 3
0
3  

   

    

   
 
 

x
y
y
x
dx
e e dxdy
0
2 1 3
0
2 3
2 2 1 3 1   
 
    
           

     
  e e dxdy e y
y
      x       2 3 2 1 1 1 2 3 1 2
0
1
0
3 
               

x
X Y
dx
M ,  ,            
  1 2
1 2 3 1 2 1 3 2 3 3 1 2
  1 2 3    1 2    1 3    1 2      3 2 
Here again, differentiation will return the distribution moments.
Next, recall that joint distributions can be analyzed to identify marginal prob￾ability functions. We know that in general:
FX X x F X X x 1 1  1 1   , , 2  n   , , ,
FX X x F X X x 2 1   2 2   , , 2  n   , , , ,
and so on. In terms of two dimensions, this is:
FX X x F X x 1 1  1 1   , 2   ,
FX X x F X x 2 1   2 2   , 2   ,
The corresponding result for the joint moment generating functions is that:
MX X     1 1  M M ,Y Y   , , 0 0 and     2 2  MXY   (7.9)
In other words, we can obtain the m.g.f. for a marginal distribution by evalu￾ating the joint m.g.f. with the complementary transform variables set to zero. 
This applies to joint marginal probability functions as well as to one-dimensional 
marginals.154 Probability Foundations for Engineers
Example 7.7
For the empirical distribution presented in Example 7.3, the marginal probability 
mass functions are:
fX(1) = 1/3 fX(2) = 5/18 fX(3) = 7/18
fY(1) = 5/18 fY(2) = 7/18 fY(3) = 1/3
Using the joint m.g.f. from Example 7.3, we find that:
MX X   M e Y e e    1 1 2 3 0 1
3
5
18
7
18
1 1 1    ,   ,   
and
MY X   M e Y e e    2 2 2 3 0 5
18
7
18
1
3
2 2 2    ,   ,   
Example 7.8
For the continuous distribution analyzed in Example 7.4, the marginal probability 
density functions are:
f x X f x y dy e dy e
x
XY
x
x y x       
 
  
  , 2 2 2
f y Y f x y dx e dx e e
y
XY
y
x y y y                  
0 0
, 2 2 1
Evaluating the joint m.g.f., we obtain:
MX X   M Y
 1 1
1
0 2
2        , ,
and
MY X   M Y
  2 2
2 2
2 0 2
1 1      
       , ,
Note also that:
E   X M  X Y

      
            


 

   

1
1 2 0
0
2
2 1 2
1 2
2
2 1
1 1 1
, , 2
0
0
1
2
1
2 



Moment Generating Functions 155
Example 7.9
For the bivariate normal distribution of Example 7.5:
MX Y e x y x x y y
,  ,            
1 2
2
2 1 2 1
2 2 1 2 2
2 2
      
MX X M e Y Y M MX Y e x x y
         
1 1 2 0 0 2 2
1 1
2 2
2
             
, , , , and
 2
2 2
2
y
Example 7.10
For the bivariate exponential distribution of Example 7.6:
MX X   M M Y Y MX Y
 
  
 
 1 1
1 3
1 3 1
2 2
2
     0 0    
    , , , , and         
   

  
3
2 3 2
The point of these calculations is that the joint moment generating function can 
be used to construct moments of the joint distribution or moments of any marginal 
distribution. The process is always the same. Take the appropriate derivative of 
the m.g.f. and evaluate the result with the values of the corresponding transform 
variables set to zero.
E X MX Y 2 2
1
2 1 2 0
0
2 1 2
2
1
2
4 1 2
1 2

 
  
          
    


 
  
 

, ,
  

1 2
2 2
0
0
1
2
1
2     



E   Y M  X Y

            
      


 
 
  

2
1 2 0
0
1 2
2 1
2
1
2 1 2 1
1 1
, ,
      


1
3
2 2
2 2
0
0
2
1
 

E Y MX Y 2 2
2
2 1 2 0
0
2 1 2 1
2
1
4 1 2 6 2

 
  
  
          



 
   


, ,
4 6 2 4
1 2
7
2
2 1 2
2
3
1 2
3
0
0
2
1
  
   

    
        
156 Probability Foundations for Engineers
7.4 CONDITIONAL MOMENT GENERATING FUNCTIONS
It should not be surprising to find that conditional distributions also have moment 
generating functions and conditional moments. Since conditional probability dis￾tributions are proper distributions, the rules for constructing a conditional m.g.f. 
are those stated in Expressions (7.2) and (7.3). The computation of the conditional 
moments is performed using Expression (7.4). Here are some examples.
Example 7.11
For the joint probability mass function:
f y y e
y y y Y Y y y y
y y y
1 2
1 2 1
1 2
17
1 2 1
1 2 2
11 6 , , 0 0
! !    , ,        
 
We found in Example 5.7 that:
f y y e
y y
f y y
y
y Y Y
y y
2 1 Y Y
2 1
2 1 1 2
6
2 1
1 2
2
1
6
| | ( | ) !  ( | )     

 


 
and 11
17
6
17
1 2 1


 

 

 


y y y
The joint m.g.f. is:
M e e
y y y Y Y
y y
y
y y y y y
1 2
2 1
2
1 1 2 2
1 2 1
1 2
0 0
17
1 2
11 6 , , !
        


    1
17
0 2 0
2
1 2 1 2
2 2
1
2 1 1 2 1 11 6
    
 





! !   !
! e e
y
y e
y y y y
y
y
y y y y  

          





 
!
! e e
y e e e e
y
y y
y
17
0 2
17
2 0
2 2
1 2
2
2 1
6 11
 6 11 
  y
e e
y e
2
2 1
2
6 11 17
!       
and the conditional moment generating functions are:
M y e
y
y Y Y
y
y
y
y y
1 2
1
2
1 1
1
1 2
0
2
1
11
17
6
17 | ( |  )   

 




 

 

 

 
 2 1
1
2
1
1 2 1
0
2
1
11
17
6
17
6
17
1



 

 




 

 

 
    
y
y
y y y y y
y e 1
17
1
2
e
y
 

 


and
M y e e
y y Y Y e
y y
y y y y
y y
2 1
2 1
2 2
2 1
2 1
2 1
2 1
6
2 1
6 6 | ( | ) !
       

    
 
 0 2 1
6 6 6 2 2 1
2 2 1
 
    
    e
y y e
y y
e y

 
!Moment Generating Functions 157
Example 7.12
For the joint probability density function:
f x XY y e x y y x y   , ,     ,     2 0 0
we found that:
f y x e f x y e
e Y X
y x
X Y
x
y | | ( | ) (   | )    

    
 and
1
Therefore:
M e f x dx e e
e dx e X Y x X Y x x
y y | |    1
0 0
1 1
1
1
1          

    

  
   
0
1
1
1 0 1
1
1
1 1
1
1 1

   
   




     
       
 
e dx
e
e e
x
x
y y


   
MY X e f y dy e e dy e e d y Y X y y x x y
| |    
2
0 0 0
2 2 1 2       
 
   

       y
e e e x y x
    
    
1
2
0
2
2
1 1

  |
and

       
      

 

  
1
1
0 1
2
0 1
1
1
1 1
1
1
M e e X Y y y |

   
   
 


   2
2
0 2
2
0 2 2
1
M e Y X e
x x |
Therefore:
E Y Y d
d M y d
d Y Y e
y
[ | ] ( | ) 1 2 |
1
1 2
0 1 0
1 2
1
1
2
1
6
17
11
17    

 

   

 


  

 

 


y e e y
y
2
1
0
2
6
17
11
17
11
17
11
17
1
2
1
1
 

and
E Y Y d
d M y d
d Y Y e e y e y [ | ] ( | ) 2 1 |
2
2 1
0 2
6 6
0
2 1
2
2 2 1
2
2     6

 
 

 


 
1 6 6
0 1
2 2 1
2
    6  

e y e y  
158 Probability Foundations for Engineers
EXERCISES
1. Construct the moment generating function for a Bernoulli distribution.
2. Construct the moment generating function for a Poisson distribution.
3. At a local pizza restaurant, the hourly demand, D, for a particular type of pizza 
has the following probability mass function:
f d
d
d
d
d
d
d
d
D   










0 08 0
0 12 1
0 18 2
0 24 3
0 16 4
0 12 5
0 10 6
.
.
.
.
.
.
.








Construct the moment generating function for the distribution on D and use it to 
obtain E[D] and Var[D].
4. If a random variable has the probability density defined as:
fX   x x   0 5. , 1 3 
Identify the moment generating function for the distribution.
5. The random variable X has the probability density defined as:
fX   x x   1 5 0 75 0 x x   2 2 . . ,
Construct the moment generating function for this density.

       
        
2
1
2 1 0
1
1 1 3 0
2
1 1
2
 1


M  
e e X Y y y | | |

   
     
2
2
2 2 0
2
2 2 3 0
2
1
2



M  
e Y X e
x x | | |
Var X Y e e
e
e
e
e y y
y
y
y
[ | ]    


 

     
  
 
  


 2
1
1
1
2 1 1
1
1 2
1
2
2    y 2
Var Y X e e e e x x x x [ | ]   2 2    
2 2
The conditional distributions constructed in Chapter 5 for the bivariate normal 
and bivariate exponential distributions follow the same pattern as that shown 
in the above example so their moment generating functions and their moments 
are readily obtained. Recall also that, as shown in Example 6.13, conditional 
expectations provide a convenient basis for computing the expectations for the 
marginal distributions.Moment Generating Functions 159
6. The random variable X has the probability density defined as 
fX   x x   2 1 , 0 1   x
Identify the moment generating function for the distribution on X.
7. A random variable has density function:
fX   x x  1 1  x  1 2
,
Construct the moment generating function for this density.
8. The distribution for a random variable has moment generating function 
MX  e
      1    2 1 8 8
. Compute the mean and variance of the distribution.
9. Construct the moment generating function for a geometric distribution.
10. Construct the moment generating function for a negative binomial distribution.
11. Suppose the random variable Y has moment generating function 
MY  e
       

 

 1
2
1
10
. What is Var[Y]?
12. Construct the moment generating function for a sum of two independent Poisson 
random variables. State the probability density function for the sum.
13. Construct the moment generating function for a sum of n independent Poisson 
random variables. State the probability density function for the sum.
14. Construct the moment generating function for a Gamma distribution. Take the 
derivatives of the moment generating function to obtain the mean and variance 
of the distribution.
15. Suppose the random variable X has moment generating function MX    and that 
Y   aX b. Use MX    to construct the moment generating function for Y.
16. Suppose the time between incoming calls to a call center is well modeled by an 
exponential distribution having   0. . 025 Let Ti
 time between the ith and the 
i-1st call, and define:
S T k
i
k
 i

1
What are:
a. The moment generating function on T4?
b. The moment generating function on S6?
c. E S6 
17. For the density function given in Problem 4, construct the conditional density 
on X given that X ≥ 1 5. and compute the conditional moment generating for the 
conditional density.
18. Construct the moment generating function for a sum of two independent Gamma 
random variables each having a distinct shape parameter αi and both having a 
common scale parameter λ.160 Probability Foundations for Engineers
19. Take the derivatives of the moment generating function for the normal distribu￾tion and obtain the first two distribution moments.
20. Use the moment generating functions to identify the distribution on the sum of 
n distinct and independent normal random variables. What is the distribution if 
  Xi  i and  Xi    i ?
21. The moment generating function for the random variable X isMX e e
 
   2 5.   1
and that for the random variable Y is MY  e
       0 80 0 20 20
. . . Compute 
PrX Y    3 and E XY  .
22. The joint p.m.f. for the discrete random variables X and Y is:
f x y
e
x y x XY x y y
x y x
, ! !    , ,        
  7
4 3 0 0
Construct the joint moment generating function MXY  1 2   , for this p.m.f. Then use 
the moment generating function to obtain MX Y     1 2 , , M E     X E, ,   Y E  XY ,
VarX and VarY .
23. The joint probability mass function for the discrete random variables X and Y is:
f x y
x y XY   , ,  x y , , , , , ,      1
30
1 2 1 234 5
Construct the joint moment generating function and use it to obtain 
E X E Y     and .
24. The joint probability density function for the random variables X and Y is:
fXY x y e x y y x y   , ,     ,     2 0 0
Construct the joint moment generating function MXY  1 2   , for this density and 
identify the corresponding marginal moment generating functions.
25. For the joint p.m.f. of Problem 13, construct MX Y| |     1 2 and MY X   and use 
them to compute E X Y   E Y X   
 12 and . 2
26. The joint probability density function for the random variables X and Y is:
fXY x y e x y y x y   , ,     ,     2 0 0
Construct the conditional density fY X| ( | y x) and the corresponding conditional 
moment generating function MY X|   2 . Then use the conditional moment gener￾ating function to obtain E[ | Y X = 7 5. ].
27. The joint probability density function for the random variables X and Y is:
fXY x y e x y x x   , ,        4 0 0 2
Compute PrY  .   2 Then construct the conditional moment generating func￾tion on Y given X = 2.DOI: 10.1201/9781003294382-8 161
Approximations and 
Limiting Behavior 8
There are situations in which the choice of probability model is not clear or even 
if we have an idea of which model to use, we simply wish to make an estimate of 
a probability without performing the complete computations. In addition, there are 
cases in which it is convenient to use one probability distribution to approximate 
probabilities for another distribution. Ultimately, these methods lead us to two key 
limit theorems that have wide applicability and important implications. These topics 
are treated in this chapter.
8.1 DISTRIBUTION-FREE APPROXIMATIONS
There are two key approximations we can use to get a feel for some probabilities 
without resorting to computations using the actual distributions. These two approxi￾mations are known as Markov’s inequality and Chebyshev’s inequality.
Markov’s inequality applies only to non-negative random variables but may be 
used for both discrete and continuous random variables. It states that for any positive 
value, say c:
Pr X c E X
c        (8.1)
Example 8.1
Suppose a hospital emergency room has experienced patient arrival patterns 
that are well modeled by a Poisson distribution having   4 / hr. and is using 
this model to establish staffing policies. What are the chances that more than 7 
patients will arrive during any one hour period? Using Markov’s inequality:
Pr  X  7  4
7
Example 8.2
Suppose the time between incoming calls to a telephone call center is exponential 
with parameter   0 5. / min. This means that the expected time between incoming 
calls is 2 minutes. What is the probability that the time between two calls exceeds 
6 minutes?
Pr .   T  6   2
6 0 333162 Probability Foundations for Engineers
Example 8.3
For the hospital emergency room of Example 8.1, Chebychev’s inequality indicates 
that:
Pr X k       
  7 4 3    4
9
2
2
and the chances that 9 patients arrive during any hour are:
Pr . X k       
  9 4 5    4  25 0 16 2
2
Example 8.4
For the call center of Example 8.2, Chebychev’s inequality indicates that:
Pr . X k       
  6 2 4    4  16 0 25 2
2
and the chances of an 8-minute wait between calls are:
Pr . X k       
  8 2 6    4  36 0 111 2
2
Example 8.5
For a binomial random variable with n   50 p n 0 08 4   0 3 pq  68 2 , . so   . , . . 
Then:
Pr Pr .   X X      .  
 10 6   3 68
36  0 102
Pr Pr .   X X      .  
 20 16   3 68
256  0 014
One appropriate interpretation of Markov’s inequality is that for most distribu￾tions, it is unlikely to experience a value of the random variable that is far from the 
mean of the distribution. Values close to the mean are more likely to occur. This is 
stated more directly in Chebychev’s inequality which is:
Pr X k k     
   2
2 (8.2)
This relationship applies to all random variables and specifically states that the 
probability of observing a value of a random variable that is “far” from it mean is 
inversely proportional to the square of the value of the distance.Approximations and Limiting Behavior 163
8.2 NORMAL AND POISSON APPROXIMATIONS
For naturally occurring phenomena, it has been observed that either the Poisson dis￾tribution or the normal distribution is the most appropriate probability model in a 
surprisingly large number of cases. In the case of the Poisson, it seems that many 
time (or space) indexed natural phenomena display the feature that relatively few 
occurrences are observed despite the fact that there are very many opportunities for 
the phenomenon to occur. In the case of the normal distribution, it seems that many 
phenomena display concentration of observations about the mean with near symme￾try as the values move away from the mean.
Whatever the reason, the result is that the normal distribution and the Poisson 
distribution both provide reasonable approximations to some probabilities. As a first 
case, consider the Poisson approximation to the binomial distribution. For a binomial 
distribution having parameters n and p, the values of the distribution are computed by 
as the sum of terms as:
Pr X x n
x
p q
i
x
x n x      

 




0
and in the absence of a mathematical software package, this calculation can be tax￾ing. However, if n is relatively large and p is relatively small so that   np is of 
moderate size, the required probability can be approximated by the corresponding 
Poisson c.d.f. for which convenient tables exist.
Example 8.6
For a normal random variable having   64.0 and   1 5. , 0
Pr Pr .   X X      .  
 61 3   2 25
9  0 25
Pr Pr .   X X      .  
 69 5   2 25
25  0 09
The Markov inequality and the Chebychev’s inequality provide a quick approxi￾mation to probabilities without actual reference to the distribution function. They 
are actually more important because of their implication for limiting behaviors of 
probabilities. The limits will be discussed in Section 8.3 but this is preceded by the 
examination of some additional approximations.
Example 8.7
For a binomial random variable with n   80and s p 0 0. . 4 3 o  2, calculation of
Pr ,   X B  6 6    60, . 0 04 0  .959164 Probability Foundations for Engineers
Example 8.8
For a binomial random variable with n   100 and s p 0 0. . 6 6 o   0, calculation 
of both
Pr ,   X b  7 7    100, . 0 06 0  .141
and
Pr ,   X B  7 7    100, . 0 06 0  .748
are involved but referring to a table of cumulative Poisson probabilities, we 
find that:
Pr ,   X P  7 7       6 0. ,   P   6 6. . 0 0   744 0. . 606  0 138
and
Pr ,   X P  7 7      6 0. .  0 744
The normal distribution may be also be used to approximate binomial prob￾abilities and can sometimes be used to approximate Poisson probabilities. In 
both cases, we use the mean and variance of the distribution in the standard 
normal formulation. Thus, for the binomial:
Pr P X x r Z x np
npq     
 







(8.3)
The corresponding construction for the Poisson distribution is:
Pr P X x r Z x       

 



 (8.4)
In the application of these approximations, there is another consideration 
that is important. Normal probabilities are defined for a continuous random 
variable and both the binomial and the Poisson random variables are discrete. 
Consequently, it is often appropriate to compute the approximating probabilities 
for a value X x = as Pr .   x X   0 5  x 0 5. and for cumulative probabilities that 
X x ≤ as Pr .   X x   0 5 . This is known as the continuous to discrete correction.
is taxing but referring to a table of cumulative Poisson probabilities, we find 
that:
Pr ,   X B  6 6    60, . 0 04 6   P   , .  3 2  0.955Approximations and Limiting Behavior 165
Example 8.9
For a binomial random variable with n = = 100 and s p n 0 0. . 3 3 o a p = 0 nd
npq = 2 9. ,1 we find:
Pr ,   X B  5 5    100, . 0 03 0  .919
and:
Pr Pr .
.   X Z    Pr Z . .  

 

 5      5 5 3
2 91
1 466 0 928
Example 8.10
In an automated machining process, output work pieces are accumulated into 
batches of 800 parts. If the machining process generates 2.5% defective pieces, 
what is the probability that a batch has fewer than 12 defective parts?
The number of defects in a batch should have a binomial distribution so the 
appropriate computation is:
Pr ,   X B  12    12 800, . 0 025  0.037
The normal approximation to this probability is:
Pr Pr .
. Pr .
.   X Z    Z .  

 

    

 

 12  12 5 20
19 5
12 5 20
19 5
0 045.
Example 8.11
In a manufacturing assembly line, work stoppages occur due to machinery jams 
according to a Poisson distribution having parameter   10 / . hr What is the prob￾ability that 15 or more stoppages occur during any one hour period?
The Poisson probability of this event is:
Pr ,   X P  15  1 1  4 8   . . 0 1   0 0. . 917  0 083
The normal approximation to this probability is:
Pr Pr .   X Z     Pr Z . . . .  

 

 15 1         14 5 10
10
1 1 423 1 0 0 923 0 077
To close this discussion, it is noted that the normal distribution can often be used to 
approximate most other distributions. In each case, the quality of the approxima￾tion can be poor but under certain circumstances can be quite good. For example, 
when the shape parameter of the gamma distribution is large, the normal approxi￾mates the gamma distribution reasonably well. When the shape parameter of the 
Weibull distribution is between about 2.8 and 3.5, the normal distribution approxi￾mates the Weibull distribution well.166 Probability Foundations for Engineers
8.3 LAWS OF LARGE NUMBERS AND THE CENTRAL LIMIT 
THEOREM
An important foundation for much of our ongoing study of probability is the limit￾ing behavior of sequences of random variables and sequences of probabilities. For 
these limits, there are three key results that underpin many of the results we will see 
and use.
All three, or the results to be defined here, are based on the idea mentioned at the 
start of this chapter that random variables display a “regularity” in that they tend not 
to be too far from their expected values. Within that context, suppose we observe a 
sequence of independent random variables, say Xi
, having a common probability dis￾tribution or equivalently, suppose we obtain a sequence of observations of a particu￾lar random variable, X. Then for the sequence, successively compute the values of:
Y S
n n n X n
i
n
  i


1
1
The first of our limiting results is known as the weak law of large numbers
where the term “law” may be taken to be synonymous with distribution. Although it 
was developed before Chebychev’s inequality, the result can be shown to follow logi￾cally from that inequality and is that as long as the variables Xi
 have a finite expected 
value,     E Xi, then:
lim Pr n
Yn 
    
    0 (8.5)
In words, the probability that the weighted sum (or average), Yn, will differ from 
the mean goes to zero as the number of observations increases.
The reader is encouraged to experiment with this behavior. It is suggested that the 
reader take a fair six-sided die and roll it repeatedly while recording the result of each 
roll. Compute Yn as you proceed and observe its behavior.
The second of our limiting results is comparable but stronger because it is made 
without recourse to a probability. The strong law of large numbers is that as long as 
the random variables have a finite expected value     E Xi, then:
lim
n Yn    (8.6)
The third and most widely used of our limiting results is known as the central 
limit theorem. This result is that as long as the random variables in the sequence 
have both a finite expected value     E Xi and a finite variance σ2
, then:
lim Pr n
n
y y Y
n
y e dy 

  










  
 
1
2
2
2 (8.7)Approximations and Limiting Behavior 167
which is to say, the distribution on the weighted sum, Yn, converges to a standard 
normal having expected value µ and variance σ2
n . Note that this result applies 
regardless of the identity of the distribution on the observations, Xi
. It is a very 
strong result that suggests why the descriptors of so many natural phenomena dis￾play the “bell shape”. The result also provides an indication of why the normal 
distribution often yields reasonable approximations to probabilities from other 
distributions.
A final result is that the central limit theorem also applies to a sequence of inde￾pendent random vectors. If we observe a sequence of random vectors 
X X i i     , , X X i i,r , , , 1 2 that are mutually independent and have a common distribution 
with mean vector       1 2 , ,  , r all of which elements are finite and covariance 
matrix Σ in which all elements are finite, then the distribution on the vector of com￾ponent wise sums Yn n     Y Y , , n n Y ,r , , , 1 2 computed as
Y S
n n n i X n i
j
n
, j i ,   ,


1
1
converges to a multivariate normal with mean vector µ and covariance matrix 1
n
Σ.
EXERCISES
1. For a manufacturing process that generates units of product of which 1.25% 
are defective, use Markov’s inequality to compute a limit on the probability of 
observing more than 4 defective units in an inspection sample of 100 units.
2. The number of calls arriving to a credit card service call center is Poisson with 
parameter   6 / min. Use Markov’s inequality to compute a limit on the chance 
that more than 12 customer calls arrive within a one minute interval.
3. The number of computers sold per week by the university book store is a random 
variable with an expected value of 12. Use Markov’s inequality to compute a 
limit on the probability that the sales volume in any week will be (a) 18 or more 
or (b) 24 or more.
4. The time to failure for a high intensity lamp is a gamma random variable having 
parameters    2 5. ,  0 0. . 2 Use Markov’s inequality to compute a limit on the 
probability that a lamp will survive (a) more than 150 hours or (b) more than 200 
hours.
5. Repeat Problem 1 using Chebychev’s inequality.
6. Repeat Problem 2 using Chebychev’s inequality.
7. Assuming the variance in weekly computer sales is 4, repeat Problem 3 using 
Chebychev’s inequality.
8. For male college students in the US, the mean height is 180 cm (6′) and the vari￾ance is 25 cm2. Use Chebychev’s inequality to compute a limit on the probability 
of observing a student taller than 205 cm (6′10″).
9. Use the Poisson distribution to approximate the probability for Problem 1.
10. Use the normal distribution to approximate the probability for Problem 1.168 Probability Foundations for Engineers
11. Use the Poisson distribution to approximate the cumulative binomial probabil￾ity of observing four or fewer defective parts in a sample of 200 units from a 
production lot having a defect rate of 1.5%. Then use the normal distribution to 
approximate the same probability.
12. Use the normal distribution to compute an approximation to the Poisson prob￾ability of Problem 2.
13. A gambler playing roulette and betting simply on red or black has a win prob￾ability of 0.474 because of the zero and double zero. What do the weak law of 
large numbers and the strong law of large numbers imply about the long term 
winnings of the gambler?
14. In reliability analysis, one often considers the use of a sequence of identi￾cal components where each component has an exponential life length and is 
replaced upon failure. If a the members of a particular component family have 
an exponential life length with parameter   0 0. / 1 hr, Use the strong law of 
large numbers to compute the average rate of failure based on a sequence of n 
component life lengths.
15. Some modern financial analysts suggest that the daily change in the price (value) 
of a company’s stock is a normally distributed random variable having a mean 
of zero and variance σ2
 where that variance represents a measure of risk. If the 
analysts were correct, what does the weak law of large numbers indicate about 
the long term change in stock prices?
16. If a gambler plays a game in which she wins $4 with probability 0.4, wins $2 
with probability 0.1, wins zero with probability 0.2 and loses $6 with probability 
0.3, use the strong law of large numbers to compute the likely value of the gam￾bler’s winnings after 100 plays of the game.
17. Use the central limit theorem to compute the probability that the gambler of 
Problem 14 wins more than $95 over the 100 plays of the game.
18. In an automated machining process, a cutting tool has an operational life length 
that is well modeled as a gamma random variable with    4 5. ,  0. / 0075 cycle.
Worn out tools are replaced instantaneously so that processing can continue 
uninterrupted. If the available supply of cutting tools is 25, use the central limit 
theorem to estimate the probability that the available tools will be sufficient to 
complete one week’s work which requires 15400 cycles.169
Index
A
Approximations and limiting behavior, 
161–167
central limit theorem, 166
Chebyshev’s inequality, 161–162
continuous to discrete correction, 164
Distribution free approximations, 161–163
Markov’s inequality, 161–162
multivariate normal distribution, 97–104
normal and Poisson approximations, 
163–165
strong las of large numbers, 166
weak law of large numbers, 166
B
Bayes rule, 22–23
Bernoulli distribution, 43
Beta Distribution, 58–59
Binomial distribution, 44–47
binomial coefficient, 45
Poisson approximation, 163–164
Bivariate exponential distribution, 104–108
Bivariate and multivariate Normal distributions, 
97–104
Bonferroni’s inequality, 29
C
Cdf, 35–38
Central limit theorem, law of large numbers, 
166
strong las of large numbers, 166
weak law of large numbers, 166
Chebyshev’s inequality, 161–162
Conditional probability, 20, 67–68
Bayes’ rule, 22–23
Law of total probability, 22–25
Unconditioning, 22
Continuous random variable, 35
Convolutions, 148–150
Covariance, 118–122
Cumulative distribution function, 35–38
binomial distribution, 44–47
exponential distribution, 54–55
gamma distribution, 56–57
joint distribution, 81–94
multivariate normal, 97–104
Poisson distribution, 49–50
standard normal distribution, 60
Weibull distribution, 57–58
D
DeMorgan’s laws, 9
Dependent events, 26
Discrete random variable, 35
Disjoint sets, 9
Distribution function, 35–38
Bernoulli distribution, 43
Beta distribution, 58–59
binomial coefficient, 45
Binomial distribution, 44–47
Bivariate and multivariate exponential 
distributions, 104–108
Bivariate and multivariate Normal 
distributions, 97–104
Continuous distribution functions, 53
Cumulative distribution functions, 37
cumulative probabilities for standard normal 
distribution, 61
Discrete distribution functions, 43
exponential distribution, 54–55
gamma distribution, 56–57
geometric distribution, 50–51
hypergeometric distribution, 48–49
lognormal distribution, 64–66
multinomial distribution, 47–48
negative binomial distribution, 51–53
normal distribution, 59–64
normal distribution quantiles, 64
Poisson distribution, 49–50
standard normal distribution, 60
uniform distribution, 66–67
Weibull distribution, 57–58
E
Event, 15–16
definition of, 15–16
independence, 26
Expectation and functions of random variables, 
113–118
Expectation, properties of, 116–117
expectation and random vectors, 118–122
expectation, conditional, 122–126
expectation of functions of random variables, 132
higher order moments, 114–116
Exponential distribution, 54–55
F
Failure probability, 30
Finite set, 10170 Index
Formal system, 2
Geometry, 2
global definition, 2
random phenomenon, 2
functions of random variables, one dimensional, 
127–129
functions of random variables, multi-dimensional, 
129–131
Jacobian matrix, 130
rules of addition, 133–138
G
Gamma density function, 57
Gamma distribution, 56–57
Gamma function, 57
Geometric distribution, 50–51
H
Hazard functions, 69–71
algebraic construction, 71
non-homogeneous differential equation, 71
Higher order moments, 114–116
History of probability, 1
Hypergeometric distribution, 48–49
I
Independence, 25–27
dependent events, 26
independent events, 25
multiplication rule, 26–27
J
Jacobian matrix, 130
Joint, marginal, and conditional distributions, 81
bivariate and multivariate exponential 
distributions, 104–108
bivariate and multivariate Normal 
distributions, 97–104
joint probability mass functions, 82–84
joint discrete distribution function, 83
Joint random variables, 81–94
Conditional continuous distribution functions, 
91–94
Conditional density functions, 87–89
Conditional discrete distribution functions, 
85–87
Continuous joint random variables, 
87–94
Discrete joint random variables, 82–85
Independence, 94–97
Marginal probability density functions, 
89–90
Marginal probability mass functions, 84–85
L
Law of Large Numbers, 166–167
central limit theorem, 166
strong law of large numbers, 166
weak law of large numbers, 166
Law of total probability, 22–25
Linear operator, 116
Location parameter, 58
M
Marginal probability mass function, 84–85
Marginal probability density function, 89–90
Markov’s inequality, 161–162
Minimum life parameter, 57
Mixture distribution, 71–72
Moment generating functions, 145–148
moment generating functions, conditional, 155–158
moment generating functions, joint, 150–155
Multi-dimensional functions of random variables, 
129–131
Multinomial distribution, 47–48
Multiplication rule, 26–27
Mutually exclusive sets, 7
N
Negative binomial distribution, 51–53
Normal distribution, 59–64
algebraic statement, 60
Bivariate and multivariate, 97–104
covariance matrix, 100–101
marginal and conditional densities, 99–101
Normal and Poisson approximations, 163–165
binomial distribution, 163
continuous to discrete correction, 164
Null set, 7
O
One dimensional functions, 127–129
Outcomes, 15–16
P
p.d.f., 40–41
p.m.f., 38–40
Poisson distribution, 49–50
Probability axioms, 17
probability definition, 17
probability density function, 40–41
probability mass functions, 38–40
Q
Quantiles, normal distribution, 64Index 171
R
Random experiment, 15
Events, 15
Outcomes, 15
probability axioms, 17
Random variables, 33–35
Continuous, 35
Definition, 33
Discrete, 35
Independent, 72
Random variables and distributions, 35–38
algebraic construction, 35
interpretation, 37
Random variables, expectation and functions of, 
113–118
conditional density, 67–68
conditional expectation, 122–126
covariance, 118–122
expectation, 116–118
expectation and functions of multiple random 
variables, 132
expectation and random vectors, 118–122
general functions of random variables, 129–131
inverse function, 130
rules of addition, 133–138
sums of independent random variables
Residual life distribution, 67–68
S
Sample space, 15
Partition, 9
Subsets, 6
universe as, 7
Venn disgram, 8
Sets, 5–10
Cardinality, 10
Disjoint, 9
Events, 15
mutually exclusive, 7
null, 7
Set theory, 5–10
associative relations, 7
cardinality of a set, 10
commutative relations, 7
Complements, 7
countable, 10
countably infinite set, 10
Definitions, 5
DeMorgan’s laws, 9
dimensionality, 10
distributive relations, 7
Elements, 5
finite set, 10
intersection, 7
Membership, 6
null set, 7
set operations, 6
Set partition, 9
set representation, 5
subsets, 6
sample space, 7
uncountably infinite, 10
union, 7
Venn diagrams, 8
Shape parameter, 58
Strong law of large numbers, 166
Sums of random variables, 132–138
Survivor function, 41–42
T
Test analyze and Fix (TAAF) routine, 51
Transfer of probabilities, 35–38
U
Unconditioning relationship, 22
Uniform distribution, 66–67
Univariate survivor function, 41–42
Universe, 6
V
Variance, 114–116
definition of, 114
of distribution, computation, 115–116
global, 122
normal distribution, 60
Venn diagram, 8–10
DeMorgan’s laws, 9
Vocabulary, as formal system
W
Weak law of large numbers, 166
Weibull distribution, 57–58
