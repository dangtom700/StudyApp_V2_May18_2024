R Data Analysis without 
Programming
The new edition of this innovative book, R Data Analysis without Programming, prepares readers to quickly analyze data 
and interpret statistical results using R. Professor Gerbing has developed lessR, a ground-breaking method for alleviating 
the challenges of R programming. The lessR package extends R, removing the need for programming. This edition expands 
upon the first edition’s introduction to R through lessR, which enables the readers to learn how to organize data for analy￾sis, read the data into R, and generate output without performing numerous functions and programming exercises first. 
With lessR, readers can select the necessary procedure and change the relevant variables with simple function calls. The 
text reviews and explains basic statistical procedures with the lessR enhancements added to the standard R environment. 
Using lessR, data analysis with R becomes immediately accessible to the novice user and easier to use for the experienced 
user.
Highlights along with content new to this edition include:
•	 Explanation and Interpretation of all data analysis techniques; much more than a computer manual, this book 
shows the reader how to explain and interpret the results.
•	 Introduces the concepts and commands reviewed in each chapter.
•	 Clear, relaxed writing style more effectively communicates the underlying concepts than more stilted academic 
writing.
•	 Extensive margin notes highlight, define, illustrate, and cross-reference the key concepts. When readers encounter 
a term previously discussed, the margin notes identify the page number for the initial introduction.
•	 Scenarios that highlight the use of a specific analysis followed by the corresponding R/lessR input, output, and an 
interpretation of the results.
•	 Numerous examples of output from psychology, business, education, and other social sciences, that demonstrate 
the analysis and how to interpret results.
•	 Two data sets are analyzed multiple times in the book, providing continuity throughout.
•	 Comprehensive: A wide range of data analysis techniques are presented throughout the book.
•	 Integration with machine learning as regression analysis is presented from both the traditional perspective and 
from the modern machine learning perspective.
•	 End of chapter worked problems help readers test their understanding of the concepts.
•	 A website at www.lessRstats.com that features the data sets referenced in both standard text and SPSS formats so 
readers can practice using R/lessR by working through the text examples and worked problems, R/lessR videos to 
help readers better understand the program, and more.
This book is ideal for graduate and undergraduate courses in statistics beyond the introductory course, research methods, 
and/or any data analysis course, taught in departments of psychology, business, education, and other social and health sci￾ences; this book is also appreciated by researchers doing data analysis. Prerequisites include basic statistical knowledge, 
though the concepts are explained from the beginning in the book. Previous knowledge of R is not assumed.
David Gerbing is Professor in the Applied Data Science program in the School of Business, Portland State University. He has 
published extensively in a wide range of social, behavioral, and methodological journals, and contributed basic methodologi￾cal concepts to develop the two-step method for the analysis called structural equation modeling.Taylor & Francis 
Taylor & Francis Group 
http://taylorandfrancis.com R Data Analysis without 
Programming
Explanation and Interpretation
Second Edition
David W. Gerbing
Portland State UniversityDesigned cover image: David W. Gerbing
Second edition published 2023
by Routledge 
605 Third Avenue, New York, NY 10158
and by Routledge
4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN
Routledge is an imprint of the Taylor & Francis Group, an informa business
© 2023 David W. Gerbing
The right of David W. Gerbing to be identified as authors of this work has been asserted in accordance with sections 77 and 78 of the Copyright, 
Designs and Patents Act 1988.
All rights reserved. No part of this book may be reprinted or reproduced or utilised in any form or by any electronic, mechanical, or other means, now 
known or hereafter invented, including photocopying and recording, or in any information storage or retrieval system, without permission in writing 
from the publishers.
Trademark notice: Product or corporate names may be trademarks or registered trademarks, and are used only for identification and explanation without 
intent to infringe.
First edition published by Routledge 2014
ISBN: 978-1-032-24402-0 (hbk)
ISBN: 978-1-032-24403-7 (pbk)
ISBN: 978-1-003-27841-2 (ebk)
DOI: 10.4324/9781003278412
Publisher’s note: This book has been prepared from camera-ready copy provided by the author.To the wonderful woman who remains my wife
even through the second edition
Rachel Maculan Sodré
Eu te amoContents
List of Figures xiii
List of Tables xviii
Preface xix
1 R for Data Analysis 1
1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.1.1 Data Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.1.2 R with lessR . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2 Prepare R for Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.2.1 Download R . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.2.2 Download RStudio . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.2.3 R in the Cloud . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
1.2.4 Start R . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
1.2.5 Extend R . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
1.2.6 Access lessR . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.2.7 Get Help . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.2.8 R Functions for Analysis . . . . . . . . . . . . . . . . . . . . . . 10
1.2.9 Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
1.3 Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
1.3.1 Data Example I: Employee Data . . . . . . . . . . . . . . . . . 13
1.3.2 Data Example II: Machiavellianism . . . . . . . . . . . . . . . . 16
1.3.3 Create a Data File . . . . . . . . . . . . . . . . . . . . . . . . . 18
1.4 Analysis Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
2 Read and Write Data 21
2.1 Quick Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.2 Types of Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
2.2.1 Variables as a Concept . . . . . . . . . . . . . . . . . . . . . . . 22
2.2.2 Variables in the Computer . . . . . . . . . . . . . . . . . . . . . 24
2.3 Read Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
2.3.1 Access the Data . . . . . . . . . . . . . . . . . . . . . . . . . . 25
2.3.2 Output . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.3.3 Missing Values . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
2.3.4 Row Names . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
2.4 More Data Formats . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
2.4.1 lessR Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
2.4.2 SPSS, SAS, and Stata Data . . . . . . . . . . . . . . . . . . . . 30
viCONTENTS vii
2.4.3 Fixed-Width Data . . . . . . . . . . . . . . . . . . . . . . . . . 31
2.4.4 More Options . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
2.5 Variable Labels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
2.5.1 Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
2.5.2 Variable Labels File . . . . . . . . . . . . . . . . . . . . . . . . 34
2.5.3 Variable Labels with R Functions . . . . . . . . . . . . . . . . . 35
2.6 Write Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
2.6.1 Choose an Output Format . . . . . . . . . . . . . . . . . . . . . 36
2.6.2 Write a Data Frame to a File . . . . . . . . . . . . . . . . . . . 37
2.7 Analysis Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
3 Manage Data 41
3.1 Quick Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
3.2 Categorical Variables as Factors . . . . . . . . . . . . . . . . . . . . . . 43
3.2.1 Order Levels . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
3.2.2 Value Labels . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
3.2.3 Add Levels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
3.3 Transform Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
3.3.1 Arithmetic Operators . . . . . . . . . . . . . . . . . . . . . . . 46
3.3.2 Mathematical Functions . . . . . . . . . . . . . . . . . . . . . . 47
3.4 Recode Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
3.4.1 Reverse Score Items . . . . . . . . . . . . . . . . . . . . . . . . 49
3.4.2 Missing Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
3.5 Sort Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
3.5.1 Sort by Variables . . . . . . . . . . . . . . . . . . . . . . . . . . 51
3.5.2 Sort by Other Criteria . . . . . . . . . . . . . . . . . . . . . . . 52
3.6 Subset Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
3.6.1 Select Rows and/or Columns . . . . . . . . . . . . . . . . . . . 53
3.6.2 Randomly Select Rows . . . . . . . . . . . . . . . . . . . . . . . 55
3.7 Revise Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
3.7.1 Change an Individual Data Value . . . . . . . . . . . . . . . . . 56
3.7.2 Change a Variable Name . . . . . . . . . . . . . . . . . . . . . 57
3.8 Merge Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
3.8.1 Inner Join . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
3.8.2 Outer and Full Joins . . . . . . . . . . . . . . . . . . . . . . . . 58
3.8.3 Add Rows to a Data Frame . . . . . . . . . . . . . . . . . . . . 60
3.9 Analysis Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
4 Categorical Variables 62
4.1 Quick Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
4.2 One Categorical Variable . . . . . . . . . . . . . . . . . . . . . . . . . 63
4.2.1 Bar Chart . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
4.2.2 Pie Chart . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
4.2.3 Customization . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
4.2.4 Bar Chart from the Summary Table . . . . . . . . . . . . . . . 67
4.2.5 Bar Chart of Deviation Scores . . . . . . . . . . . . . . . . . . 68
4.2.6 Stack the Bars across Multiple Variables . . . . . . . . . . . . . 69
4.2.7 Generalize Beyond the One Sample . . . . . . . . . . . . . . . . 70viii CONTENTS
4.3 Two Categorical Variables . . . . . . . . . . . . . . . . . . . . . . . . . 73
4.3.1 Bar Chart from Joint Frequencies . . . . . . . . . . . . . . . . 73
4.3.2 100% Stacked Bar Chart . . . . . . . . . . . . . . . . . . . . . . 75
4.3.3 Description with Summary Tables . . . . . . . . . . . . . . . . 77
4.3.4 Inferential Analysis . . . . . . . . . . . . . . . . . . . . . . . . . 78
4.4 Analysis Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
5 Continuous Variables 82
5.1 Quick Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
5.2 Histogram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
5.2.1 Bins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
5.2.2 Default Histogram . . . . . . . . . . . . . . . . . . . . . . . . . 84
5.2.3 Customize the Bins . . . . . . . . . . . . . . . . . . . . . . . . . 86
5.2.4 Smooth the Bins . . . . . . . . . . . . . . . . . . . . . . . . . . 87
5.2.5 Bandwidth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
5.2.6 Cumulative Histogram . . . . . . . . . . . . . . . . . . . . . . . 89
5.2.7 Histograms for All . . . . . . . . . . . . . . . . . . . . . . . . . 89
5.3 Histogram Alternatives . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
5.3.1 Box Plot and Outliers . . . . . . . . . . . . . . . . . . . . . . . 90
5.3.2 Violin-Box-Scatter Plot . . . . . . . . . . . . . . . . . . . . . . 91
5.4 Visualize Data over Time . . . . . . . . . . . . . . . . . . . . . . . . . 92
5.4.1 Run Chart . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
5.4.2 Time series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
5.5 Analysis Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
6 Statistics 99
6.1 Quick Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
6.2 Types of Summary Statistics . . . . . . . . . . . . . . . . . . . . . . . 100
6.2.1 Parametric Statistics . . . . . . . . . . . . . . . . . . . . . . . . 100
6.2.2 Order Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
6.2.3 Obtain the Statistics . . . . . . . . . . . . . . . . . . . . . . . . 107
6.2.4 Data Aggregation . . . . . . . . . . . . . . . . . . . . . . . . . 109
6.3 Evaluate a Single Mean . . . . . . . . . . . . . . . . . . . . . . . . . . 110
6.3.1 Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
6.3.2 Basis of Inference . . . . . . . . . . . . . . . . . . . . . . . . . . 112
6.3.3 Application . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
6.3.4 One-Tailed vs. Two-Tailed Tests . . . . . . . . . . . . . . . . . 120
6.4 Evaluate a Proportion . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
6.5 Analysis Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
7 Compare Two Samples 127
7.1 Quick Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
7.2 Independent-Samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
7.2.1 Research Design for Independent-Samples . . . . . . . . . . . . 128
7.2.2 Example 1: Two Existing Groups . . . . . . . . . . . . . . . . . 129
7.2.3 Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
7.2.4 Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
7.2.5 Nonparametric Alternative . . . . . . . . . . . . . . . . . . . . 135CONTENTS ix
7.2.6 Example 2: Two Experimental Groups . . . . . . . . . . . . . . 137
7.3 Dependent Samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
7.3.1 Dependent-Samples t-test . . . . . . . . . . . . . . . . . . . . . 141
7.3.2 Nonparametric Comparison . . . . . . . . . . . . . . . . . . . . 145
7.4 Multiple Proportions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
7.5 Analysis Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
8 Compare Multiple Samples 151
8.1 Quick Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
8.2 Experimental Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
8.3 One-Way Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
8.3.1 Variability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
8.3.2 Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
8.3.3 Data and Input . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
8.3.4 Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
8.3.5 Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
8.3.6 Search for Outliers . . . . . . . . . . . . . . . . . . . . . . . . . 159
8.3.7 Nonparametric Alternative . . . . . . . . . . . . . . . . . . . . 160
8.4 Randomized Block Design . . . . . . . . . . . . . . . . . . . . . . . . . 161
8.4.1 Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
8.4.2 Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
8.4.3 Input . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
8.4.4 Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
8.4.5 Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
8.4.6 Other Output . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
8.4.7 Nonparametric Alternative . . . . . . . . . . . . . . . . . . . . 168
8.4.8 Advantage of Blocking . . . . . . . . . . . . . . . . . . . . . . . 168
8.5 Analysis Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
9 Factorial Designs 171
9.1 Quick Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
9.2 Two-Way Factorial Design . . . . . . . . . . . . . . . . . . . . . . . . . 172
9.2.1 Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172
9.2.2 Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
9.2.3 Input . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
9.2.4 Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
9.2.5 Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
9.3 More Advanced Designs . . . . . . . . . . . . . . . . . . . . . . . . . . 179
9.3.1 Randomized Block Factorial Design . . . . . . . . . . . . . . . 180
9.3.2 Split-Plot Factorial Design . . . . . . . . . . . . . . . . . . . . 184
9.3.3 Unbalanced Designs . . . . . . . . . . . . . . . . . . . . . . . . 188
9.4 Analysis Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
10 Correlation 193
10.1 Quick Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
10.2 Relation of Two Numeric Variables . . . . . . . . . . . . . . . . . . . . 194
10.2.1 Scatterplot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
10.2.2 Correlation Coefficient . . . . . . . . . . . . . . . . . . . . . . . 196x CONTENTS
10.2.3 Two Unrelated Variables . . . . . . . . . . . . . . . . . . . . . . 197
10.2.4 Two Variables Positively Related . . . . . . . . . . . . . . . . . 199
10.2.5 Scatterplot Classification Variable . . . . . . . . . . . . . . . . 202
10.2.6 Bubble Plot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
10.3 Correlation Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
10.3.1 All Numeric Variables . . . . . . . . . . . . . . . . . . . . . . . 206
10.3.2 List of Variables . . . . . . . . . . . . . . . . . . . . . . . . . . 206
10.3.3 Missing Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
10.3.4 Visualizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
10.3.5 Save the Correlations . . . . . . . . . . . . . . . . . . . . . . . 210
10.3.6 Cluster Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . 210
10.4 Nonparametric Correlation Coefficients . . . . . . . . . . . . . . . . . . 213
10.5 Analysis Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
11 Regression Analysis 217
11.1 Quick Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
11.2 Regression Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
11.2.1 Supervised Machine Learning . . . . . . . . . . . . . . . . . . . 218
11.2.2 Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220
11.3 Model Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
11.3.1 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
11.3.2 Standardization . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
11.3.3 Inference for the Slope . . . . . . . . . . . . . . . . . . . . . . . 224
11.4 Model Fit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
11.4.1 Residuals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
11.4.2 Fit Indices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
11.5 Prediction Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232
11.5.1 Prediction Error . . . . . . . . . . . . . . . . . . . . . . . . . . 232
11.5.2 Predict from Existing Data . . . . . . . . . . . . . . . . . . . . 234
11.5.3 Predict from New Data . . . . . . . . . . . . . . . . . . . . . . 235
11.6 Outliers and Diagnostics . . . . . . . . . . . . . . . . . . . . . . . . . . 236
11.6.1 Bivariate Outliers . . . . . . . . . . . . . . . . . . . . . . . . . 236
11.6.2 Case-Deletion Statistics . . . . . . . . . . . . . . . . . . . . . . 238
11.6.3 Predictive Residuals . . . . . . . . . . . . . . . . . . . . . . . . 240
11.7 Model Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
11.7.1 Properties of the Residuals . . . . . . . . . . . . . . . . . . . . 241
11.7.2 Curvilinear Relationships . . . . . . . . . . . . . . . . . . . . . 243
11.8 Analysis Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
12 Multiple Regression 250
12.1 Quick Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250
12.2 Multiple Regression Model . . . . . . . . . . . . . . . . . . . . . . . . . 251
12.2.1 Multiple Predictor Variables . . . . . . . . . . . . . . . . . . . 251
12.2.2 Partial Slope Coefficients . . . . . . . . . . . . . . . . . . . . . 252
12.3 Model Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
12.3.1 Total Effects . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
12.3.2 Net Effects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
12.4 Model Fit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258CONTENTS xi
12.4.1 Fit Indices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258
12.4.2 Outliers and Assumptions . . . . . . . . . . . . . . . . . . . . . 259
12.5 Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260
12.5.1 Predictive Precision . . . . . . . . . . . . . . . . . . . . . . . . 260
12.5.2 Training vs. Testing Data . . . . . . . . . . . . . . . . . . . . . 261
12.5.3 Data Splitting . . . . . . . . . . . . . . . . . . . . . . . . . . . 262
12.6 Model Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
12.6.1 Collinearity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
12.6.2 Best Subsets . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
12.6.3 Nested Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
12.7 Analysis of Covariance . . . . . . . . . . . . . . . . . . . . . . . . . . . 268
12.7.1 Covariates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
12.7.2 Homogeneity of Regression . . . . . . . . . . . . . . . . . . . . 270
12.7.3 Group Differences . . . . . . . . . . . . . . . . . . . . . . . . . 271
12.7.4 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272
12.7.5 More Advanced Designs . . . . . . . . . . . . . . . . . . . . . . 273
12.8 Analysis Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274
13 Categorical Regression Variables 277
13.1 Quick Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
13.2 Indicator Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278
13.2.1 Dummy Variables . . . . . . . . . . . . . . . . . . . . . . . . . 278
13.2.2 Dummy Variable Regression . . . . . . . . . . . . . . . . . . . . 279
13.2.3 General Linear Model . . . . . . . . . . . . . . . . . . . . . . . 280
13.3 Custom Indicator Variables . . . . . . . . . . . . . . . . . . . . . . . . 281
13.3.1 Contrast Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . 282
13.3.2 Effects Coding Regression . . . . . . . . . . . . . . . . . . . . . 283
13.4 Binary Logistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . 285
13.4.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
13.4.2 Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
13.4.3 Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287
13.4.4 Odds Ratio . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
13.4.5 Fit Indices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290
13.4.6 Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290
13.4.7 Outliers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295
13.4.8 Multiple Predictors . . . . . . . . . . . . . . . . . . . . . . . . . 295
13.5 Analysis Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300
14 Causality 302
14.1 Quick Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302
14.2 Correlation is not Causation . . . . . . . . . . . . . . . . . . . . . . . . 303
14.2.1 Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
14.2.2 Real Life Consequences . . . . . . . . . . . . . . . . . . . . . . 305
14.3 Moderation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306
14.3.1 The Concept . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306
14.3.2 Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306
14.3.3 Manual Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . 310
14.4 Mediation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310xii FIGURES
14.4.1 The Concept . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310
14.4.2 Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312
14.4.3 The Indirect Effect . . . . . . . . . . . . . . . . . . . . . . . . . 315
14.5 Path Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317
14.6 Analysis Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319
15 Item and Factor Analysis 321
15.1 Quick Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
15.2 Overview of Factor Analysis . . . . . . . . . . . . . . . . . . . . . . . . 322
15.2.1 Latent Variables . . . . . . . . . . . . . . . . . . . . . . . . . . 322
15.2.2 Measurement Models . . . . . . . . . . . . . . . . . . . . . . . . 323
15.3 Exploratory Factor Analysis . . . . . . . . . . . . . . . . . . . . . . . . 324
15.3.1 Extraction then Rotation . . . . . . . . . . . . . . . . . . . . . 324
15.3.2 Exploratory Analysis of Mach IV Items . . . . . . . . . . . . . 326
15.4 Confirmatory Factor Analysis . . . . . . . . . . . . . . . . . . . . . . . 330
15.4.1 Covariance Structure . . . . . . . . . . . . . . . . . . . . . . . . 331
15.4.2 Analysis of a Population Model . . . . . . . . . . . . . . . . . . 334
15.4.3 Proportionality . . . . . . . . . . . . . . . . . . . . . . . . . . . 337
15.5 Confirmatory Analysis of Mach IV Items . . . . . . . . . . . . . . . . . 338
15.5.1 Analysis of Model from Exploratory Analysis . . . . . . . . . . 338
15.5.2 Revised Model . . . . . . . . . . . . . . . . . . . . . . . . . . . 339
15.5.3 Scale Reliability . . . . . . . . . . . . . . . . . . . . . . . . . . 342
15.5.4 Total Score Correlations . . . . . . . . . . . . . . . . . . . . . . 343
15.5.5 Beyond the Basics . . . . . . . . . . . . . . . . . . . . . . . . . 344
15.6 Analysis Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 347
References 349
Index 352List of Figures
1.1 Three default RStudio windowpanes and optional R Source pane for
R script. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.2 Run an R data analysis from the RStudio Source windowpane. . . . . 7
1.3 The lessR vignettes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
1.4 General procedure for functions that process data, either data analysis
or data modification. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
1.5 Variable names in the first row and the first eight rows of data from
the employee data table as stored in an Excel worksheet. . . . . . . . 13
1.6 (a) Choose the data validation option or circle invalid data, (b) Excel
data validation options to specify the acceptable data format in the
designated cells. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
1.7 Gender, Height and Weight of eight motorcyclists. . . . . . . . . . . . 20
2.1 Variable labels for Variables m01 to m04, the first four Mach IV items
stored in an Excel worksheet. . . . . . . . . . . . . . . . . . . . . . . . 35
2.2 First six rows of data for the Excel output file written by Write(). . . 38
2.3 Gender, Weight and Height of eight motorcyclists. . . . . . . . . . . . 39
3.1 Sequence of steps in a data analysis project. . . . . . . . . . . . . . . . 41
4.1 Default grayscale bar chart for a single categorical variable. . . . . . . 64
4.2 Default grayscale pie chart in the form of a ring chart for categorical
variable Dept. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
4.3 Horizontal bar chart with custom interior and exterior colors. . . . . . 66
4.4 Worksheet data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
4.5 Bar chart of the deviation score of Salary for each department from
the mean Salary of all the departments. . . . . . . . . . . . . . . . . . 68
4.6 Stacked single-bar bar charts for the 20 items of the Mach IV scale. . 69
4.7 Default grayscale bar charts for two categorical variables, stacked (left)
and unstacked (right). . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
4.8 Worksheet data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
4.9 Default grayscale bar charts for two categorical variables, stacked (left)
and 100% stacked (right). . . . . . . . . . . . . . . . . . . . . . . . . . 76
4.10 General syntax of a call to pivot(). . . . . . . . . . . . . . . . . . . . 77
5.1 Example of a bin defined over the range of data values from $50,000
to $60,000. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
5.2 The assignment of data value $57,358 to its corresponding bin. . . . . 84
5.3 Default grayscale histogram. . . . . . . . . . . . . . . . . . . . . . . . 85
5.4 Histogram with specified bins and default starting value. . . . . . . . 86
xiiixiv FIGURES
5.5 Histogram with specified bin widths and starting point for the bins. . 87
5.6 Histogram with superimposed general density curve and customized
histogram. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
5.7 Cumulative and regular histogram with specified bins. . . . . . . . . . 89
5.8 Generalized box plot. . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
5.9 Default grayscale lessR box plot. . . . . . . . . . . . . . . . . . . . . 91
5.10 Integrated violin, box, and scatterplot, called here the VBS plot. . . . 92
5.11 Distribution of the mean student ratings over the 20 terms. . . . . . . 93
5.12 Run chart of the mean rating each term for a course. . . . . . . . . . 94
5.13 Apple stock price. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
5.14 Stock price of three companies on one panel. . . . . . . . . . . . . . . 96
5.15 Trellis plot of stock price of three companies on three distinct panels. 96
6.1 The mean as the balance point of a distribution. . . . . . . . . . . . . 101
6.2 Two distributions of n = 7 with the same range, 180, and mean, 100,
but different standard deviations, 53.5 (A) vs 85.1 (B). . . . . . . . . 101
6.3 Perfect normal distribution of Y with standard scores Z showing rela￾tionship to the population standard deviation, σ, and the population
mean, µ. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
6.4 Three distributions with varying amounts of positive skew assessed by
G1. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
6.5 Histograms of Height and Weight from the BodyMeas data set. . . . . 109
6.6 Histogram for m07, the 7th Mach IV item, not reverse scored. . . . . . 111
6.7 Standard error of the sample mean as a function of sample size, for
an arbitrary standard deviation of the data equal to 10. . . . . . . . . 114
6.8 Two hypothetical normal sampling distributions of m with the same
population mean, µ, but different standard errors (deviations). . . . . 114
6.9 Density plot from ttest() of m07, the 7th item on the Mach IV scale,
with sample mean and hypothesized mean and two effect sizes, not
reverse scored. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
7.1 Density curves for men and women on Mach IV subscale Deceit. . . . 132
7.2 Density curves for distributed practice vs. massed practice on test Score.140
7.3 Density plot of the differences of weight loss, in pounds. . . . . . . . . 144
8.1 Scatterplot and means for each group of task completion Time data. . 155
8.2 Tukey family-wise confidence intervals for each of three pairwise com￾parisons. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
8.3 Plot of the four data values for each person. . . . . . . . . . . . . . . . 164
8.4 Scatter plot of the data with the values fitted by the model. . . . . . . 167
9.1 Grayscale plot of the three cell means of task completion Time, each
for Dosage and Difficulty. . . . . . . . . . . . . . . . . . . . . . . . . . 174
9.2 A split-plot design with one between-groups factor (2 levels) and one
within-groups factor (4 levels) with 7 different participants for each of
the two levels. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
9.3 Cell means of response latency for two Food levels across four different
Supplements. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188FIGURES xv
10.1 Default grayscale scatterplot of Years Employed with Annual Salary. . 195
10.2 Scatterplot of Years Employed and Annual Salary and a loess curve
with a dark background. . . . . . . . . . . . . . . . . . . . . . . . . . . 195
10.3 Scatterplot of mean-deviated Years and Salary, centered at <0,0>. . . 196
10.4 The scatterplot with the 0.95 data ellipse for 250 values of variables X
and Y that exhibit almost no relationship, correlating only r = 0.016
in this sample. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
10.5 Strong linear relationship demonstrated with the scatterplot of Years
Employed and Annual Salary for r = 0.85 with the 0.95 data ellipse. . 200
10.6 Scatterplot of Years and Salary with different plot symbols for men
and women and respective least-squares fit lines. . . . . . . . . . . . . 202
10.7 A Trellis scatterplot of Years and Salary plotted on separate panels
for each value of Gender. . . . . . . . . . . . . . . . . . . . . . . . . . 203
10.8 Scatterplot in the form of a bubble plot of Mach IV items m06 and m07.204
10.9 Annotated correlation matrix of six Mach IV items from Correlation().207
10.10 Scatterplot matrix of six Mach IV items. . . . . . . . . . . . . . . . . 209
10.11 Heat map of the correlation matrix of six Mach IV items. . . . . . . . 209
10.12 Hierarchical cluster dendrogram for the first 16 Mach IV items, anno￾tated to suggest a cluster structure. . . . . . . . . . . . . . . . . . . . 211
10.13 Heat map of reordered correlation matrix for the first 16 Mach IV
items, annotated to suggest a cluster structure. . . . . . . . . . . . . . 212
10.14 Pearson and Spearman correlation coefficients for a perfect exponential
distribution (left) and the same distribution with added noise (right). 214
11.1 Predict from information entered into a prediction equation, the model.219
11.2 Two linear functions with different slopes, b1 = 2 (left) and b1 = 0.5
(right). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
11.3 Scatterplot with best-fitting least-squares regression line from the brief
output form of the Regression() function, reg_brief(). . . . . . . . 223
11.4 Logic of the confidence interval based on the hypothetical distribution
of b1 across repeated samples, where this one obtained b1 is less than β1.226
11.5 Scatterplot with data value <10, 92681>, fitted value <10, 75206>,
and associated residual, 17475, for Xi = 10. . . . . . . . . . . . . . . . 228
11.6 Scatterplot with the null regression line and highlighted residuals. . . 230
11.7 Default Regression() scatterplot of Salary from Years experience
plus the regression line, the prediction intervals of Salary (light gray),
and the confidence intervals of the fitted value (dark gray). . . . . . . 234
11.8 Regression line with the created outlier removed (dashed) compared
to the original regression line with all the points (solid). . . . . . . . . 237
11.9 Scatterplot of fitted values and residuals. . . . . . . . . . . . . . . . . 241
11.10 Display of severe heterogeneity. . . . . . . . . . . . . . . . . . . . . . . 242
11.11 Distribution of the residuals. . . . . . . . . . . . . . . . . . . . . . . . 243
11.12 Typical non-linear functional relationships. . . . . . . . . . . . . . . . 244
11.13 Linear fit to a quadratic relationship. . . . . . . . . . . . . . . . . . . 244
11.14 Quadratic fit function for Y as a function of X. . . . . . . . . . . . . . 245
11.15 Fit to the linearized data (left) and residuals (right). . . . . . . . . . . 246xvi FIGURES
12.1 Scatterplot matrix of response Reading ability with predictors Verbal
aptitude, days Absent, and family Income. . . . . . . . . . . . . . . . 256
12.2 Diagnostic plot. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
12.3 Tension between overfitting and underfitting a model: A too simple
model never fits well, but an overly complex model only fits well on
training data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
12.4 Analysis of covariance regression lines for of Salary in terms of Years
for each level of Gender based on the assumption of parallel lines. . . 270
12.5 Regression lines separately computed for Salary and Years at each
level of Gender as the relationships exist in the data. . . . . . . . . . . 271
13.1 Scatter plot and regression line from Regression() of the categorical
variable Gender with Salary. . . . . . . . . . . . . . . . . . . . . . . . 280
13.2 Least-squares regression fit and scatter plot of Hand circumference
and binary response variable Gender scored 0 and 1. . . . . . . . . . . 286
13.3 The sigmoid curve for the probability of Male for various Hand sizes
imposed over the scatter plot of Hand circumference in inches and
Gender. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291
13.4 Classification status overlaid on the scatterplot of Gender with Hand
size. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
14.1 Fire data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
14.2 Path diagram that specifies Severity of the Fire causing the Number
of Fire Trucks at the fire as well as the total Fire Damage. . . . . . . 304
14.3 Fire data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304
14.4 Path diagram that specifies hormone replacement therapy directly
diminishes coronary heart disease. . . . . . . . . . . . . . . . . . . . . 305
14.5 Path diagram that specifies hormone replacement therapy increases
coronary heart disease. . . . . . . . . . . . . . . . . . . . . . . . . . . 305
14.6 Interaction plot from Regression() for Task Anxiety as a function of
Task Importance for three levels of the moderator academic self-efficacy.309
14.7 Path diagram of the direct influence of predictor X on response Y. . . 311
14.8 Partial mediation path diagram for predictor X, response Y, and
mediator M. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311
14.9 Path diagram of full mediation of predictor X on response Y via
mediator M. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312
14.10 Path diagram of predictor VAC on response DVB without mediation. 314
14.11 Analysis of mediation model consistent with data from Jessor and
Jessor (1977). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315
15.1 For the 20-item Mach IV correlation matrix, scree plots for the succes￾sive eigenvalues (left) and for the difference of successive eigenvalues
(right). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
15.2 Specified population two-factor multiple indicator measurement model,
three items per factor. . . . . . . . . . . . . . . . . . . . . . . . . . . . 331
15.3 Heat map of 6-variable correlation matrix with communalities in the
diagonal. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335FIGURES xvii
15.4 Annotated output correlation matrix with communalities in the diago￾nal of all measured and latent variables in the analysis. . . . . . . . . 336
15.5 Annotated proportionality matrix. . . . . . . . . . . . . . . . . . . . . 338
15.6 Annotated proportionality coefficients for the items that define the
first two factors in the confirmatory factor analysis of Mach IV. . . . 340List of Tables
1.1 The Christie and Geiss (1970) Mach IV scale. . . . . . . . . . . . . . . 16
2.1 Median read speed of three file formats over 100 trials of each Read()
function call, with the file size in MB. . . . . . . . . . . . . . . . . . . 37
3.1 Some R mathematical functions applied to the variable x. . . . . . . . 47
3.2 Logical operators. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
6.1 A distribution of values, Y , with mY = 6 and deviation scores, Y − mY .101
6.2 Conceptual expression of the variance, s
2 and the standard deviation, s.102
6.3 Computation of the sample mean, mY = 6, sample variance, s
2 = 6.5,
and sample standard deviation, sY = 2.55. . . . . . . . . . . . . . . . . 103
9.1 Subset of maze running data organized by block. . . . . . . . . . . . . 181
11.1 Synonymous names for the response and predictor variables, respec￾tively, in a regression analysis. . . . . . . . . . . . . . . . . . . . . . . . 219
13.1 Examples of categorical variables and one associated level (category). . 278
13.2 Data table of four people for categorical variable Gender and its two
corresponding indicator variables, here named GenderM and GenderW.278
13.3 Probabilities of being a Man, fitted, and of being a Woman, 1 -
fitted. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
14.1 The three variables in the mediation analysis with higher ATD scores
indicating greater intolerance of deviant behavior. . . . . . . . . . . . 312
15.1 Respective correlations of Item X1 and also Item X2 across all six
items in the model and their corresponding ratios, proportions. . . . . 337
xviiiPreface
Content Overview
Chapter 1 introduces R and then lessR and shows how to download them from
the worldwide network of R servers. The chapter then provides an example of data
analysis, shows how to structure data for analysis, and discusses general issues for
using R and lessR. Chapter 2 shows how to read the data from a computer file into R.
Then the data often must be modified before analysis begins, the topic of Chapter 3.
This editing can include changing individual data values, assigning missing data codes,
transforming, recoding, and sorting the data values for a variable, and sub-setting
and merging data tables.
Chapters 4 and 5 show how to obtain the most basic data analyses, counting how
often each data value or groups of similar data values occurred. Chapter 4 explains
bar charts and related techniques for counting the values of a categorical variable, one
with non-numeric data values. Also included is the analysis of two or more variables
in the form of bar charts for two variables and the associated cross-tabulation tables.
Chapter 5 does the same for numeric variables, which include the histogram and
related analyses, the scatter plot for one variable, the box plot, the density plot, and
the time plot, plus the associated summary statistics.
Chapter 6 explains the analysis of a single sample’s mean and other statistics. Chapter
7 focuses on the mean difference across two samples, or the direct analysis of differences
across two related samples. These analyses are based on the t-test of the mean, the
independent samples t-test, and the dependent samples t-test. The non-parametric
alternatives are also provided. Chapter 8 extends this material to compare many
groups with the analysis of variance, ANOVA. The designs considered are the one-way
independent groups and the randomized blocks analysis, and their associated post-hoc
analyses.
Chapter 9 introduces factorial designs, the two-way ANOVA, and the concept of inter￾action. Also illustrated are the randomized block factorial design and the split-plot
factorial. Effect sizes are an integral part of each discussion. The analysis of a two-way
unbalanced design is also presented.
Chapter 10 presents correlational analysis and scatterplots, and the correlation matrix
and hierarchical cluster analysis for organizing the variables that form the correlation
matrix. Chapters 11 through 14 introduce various aspects of regression analysis. The
subject of Chapter 11 is regression analysis of a linear model of a response variable
with a single predictor variable. The discussion includes estimation of the model,
inferential evaluation of the estimated coefficients, evaluation of fit, outliers and
influential observations, and prediction intervals. The chapter ends with a discussion
xixxx PREFACE
of assumptions and curvilinear relationships. Chapter 12 extends this discussion to
multiple regression, the analysis of models with multiple predictor variables, including
the analysis of covariance ANCOVA. Chapter 13 extends the analysis to models with
categorical predictor variables or a binary categorical response variable, the topic
of logistic regression. Chapter 14 shows some aspects of using regression models
to investigate causality, moderation regression models, and models to analyze the
indirect effects of mediation, extending to an introduction to path analysis.
The topic of Chapter 15 is factor analysis, both its exploratory and confirmatory
versions. The primary examples are item analysis, the analysis of items that form
a scale, such as from an attitude survey, and the corresponding scale reliabilities.
The concepts of factor extraction and factor rotation are presented for exploratory
factor analysis. Within this context, a linkage between exploratory and confirmatory
factor analysis for item analysis is provided, as well as a discussion of the covariance
structure that underlies the confirmatory analysis. Analysis of scale development
from published data on Machiavellianism appears throughout the chapter, with the
final outcome of the sub-scales derived with confirmatory factor analysis.
Personal Acknowledgments
I thank Jason T. Newsom, a colleague and Professor at the Department of Psychology,
Portland State University. Already a successful author at Routledge/Taylor &
Francis, Jason recommended me to the Routledge team for the first edition. From
that introduction, this book developed. Jason also provided two different forums for
presenting my work on lessR at Portland State University, his informal seminars
on quantitative topics for faculty and graduate students, and his annual (pre-Covid)
June workshops on quantitative topics.
Also, I thank Carlos Martin-Vide, of Rovira i Virgili University, Tarragona, Spain, for
inviting me to participate in several sessions of the International School on Big Data.
Presenting the seminars provided further incentive to develop lessR, and served as a
source of stimulation for perceptive feedback from students and faculty. In particular,
I am grateful for the input from Paul Bliese, of the University of South Carolina,
Columbia, South Carolina, USA, who recommended using the lattice package from
Deepayan Sarkar as a basis for Trellis graphics upon which lessR now relies.
My students at Portland State University also deserve recognition for their role in the
development of lessR and this book. During the last 14 years many undergraduate
and graduate students have used and contributed much feedback to the project from
the beginning of its development through the current version.
Finally, I would like to thank the three reviewers who provided comprehensive,
insightful reviews to the first edition: J. Patrick Gray, University of Wisconsin
– Milwaukee, Agnieszka Kwapisz, Montana State University, and Bertolt Meyer,
University of Zurich, Switzerland. The quality of their reviews illustrates how
whatever one person does is facilitated by the thoughtful critiques of others who are
also knowledgeable in the subject. Their reviews shaped the format of this book.Chapter 1
R for Data Analysis
1.1 Introduction
1.1.1 Data Analysis
We ask many questions for which we seek answers. Some of our inquiries are about
how things work in our world. On average, do men make more than women managers
at a particular company? Which of the two pain relievers is the most effective? What
type of career should the counselor suggest based on the client’s replies to a job
aptitude survey? Do people trust each other in general? This book explains how to
do and interpret analyses for answering questions such as these. empirical:
Information based on
observations acquired
from our five senses.
We seek empirical answers to these questions, answers based on our observations
of the world around us: What we see, hear, touch, taste, and smell. Encode these
observations as data, the measurements of different people, organizations, places,
things, and events.
data: Measurements
of different people or
whatever the unit of
analysis.
Data is the bedrock, the foundation, of our scientific knowledge
of the world and the lives that we live.
The natural variation of measurements motivates their analysis. Two different people
have different heights, place differing amounts of trust in others, have different blood
pressures, and earn different salaries. Height, trustfulness, blood pressure, and income
are some of the many variables amenable to measurement. GPA and number of credits
completed provide additional examples of measured variables for college students. data analysis:
Application of
statistical methods
to transform data
into usable
information.
What to do with all the data that result from these varying measurements? Data anal￾ysis applies statistical methods to transform data into usable information, sometimes
vast amounts of data. This derived information then informs conclusions regarding
the people, organizations, places, or whatever is the topic of interest.
data science:
Application of a wide
range of data
analytic procedures
to realize insight and
prediction.
The emerging
field of data science, now a growth engine for jobs, demonstrates how essential data
analysis is becoming in our modern world.
There are many different forms of data analysis that focus on understanding the
values of a single variable and how different variables relate to each other. Yet a small
number of general categories describe the purpose of many analyses. You may not at
this moment understand what each of the following types of analyses does, but you
will find examples and explanations of these analyses throughout this book.
12 CHAPTER 1. R FOR DATA ANALYSIS
Data Analysis. General categories of data analysis
• Data visualizations (also referred to as computer graphics)
• Statistics
◦ Descriptive statistics that summarize characteristics of the sample data
◦ Inferential statistics that generalize results from the sample to the popula￾tion from which the sample was obtained
• Supervised machine learning, predictive models constructed from the relation￾ships among variables
◦ Relate variables that are observed directly or are simple transformations,
such as converting yards to meters
◦ Relate variables that are latent, not observed, their existence inferred
from patterns of relationships among the observed variables
• Causal inference to establish patterns of causality
lessR: Analyze data
with only simple
instructions, usually
the name of the
analysis and the
variable(s) analyzed.
This book demonstrates how to analyze data and interpret the results using the
computer software system R, enhanced with a set of data analysis procedures known as
lessR. Use this software to perform a specific analysis with a single, simple instruction.
Anyone with access to a computer can now easily analyze data using free software.
1.1.2 R with lessR
lessR Package
From the beginning of widely available software for data analysis in the 1970s until
the early 21st century, data analysis was generally done with expensive, proprietary
statistical software running on costly computers. The cost and availability of statistical
applications for data analysis become increasingly less relevant as computer capability
and the R system’s popularity increase. In terms of pure statistical power to analyze
data, R compares favorably to the most expensive commercial applications available,
delivering all analyses that nearly any analyst might desire. The price is precisely $0.00.
R is free for you on any computer that runs Windows, Macintosh, or Linux/Unix.
The bad news? R’s capabilities and price are great, but mastering a relatively difficult
system with a steep learning curve may require much time – time not spent analyzing
data or even hanging out with friends. R is a proper programming language that
provides enormous flexibility for processing and analyzing data. Sometimes you write
a little code, and sometimes much code to analyze your data. Standard R is mainly
for those who enjoy, or at least endure, reading manuals, programming, and then
debugging the resulting code. Get your code working right, and you have harnessed
the power. Or, you might find yourself staring at some cryptic error message that
you are clueless about how to resolve.
Fortunately, R is designed so that anyone can contribute additional functionality
beyond the hundreds of functions already downloaded with R. Your author developed
one such extension to R, the lessR package (Gerbing, 2021, 2022). Compared to R,
lessR requires much less code to perform basic data analysis. Only simple instructions,1.1. INTRODUCTION 3
more precisely, function calls, are needed. function: Procedure
for a specific task
such as a data
analysis.
All statistical software, including R and
Excel, performs a particular data analysis by invoking a specific function. Each call
to a lessR function contains the minimum necessary information to proceed: The
name of the analysis, and the name of the variable(s) to be analyzed, sometimes
referencing additional options.
lessR Analysis
data table,
Section 1.3.1, p. 13
Organize your data as a rectangular table. Each column contains the name of a
variable in the first row followed by the corresponding data values. Store your data
in one of many different formats, including Excel worksheet files, OpenDocument
Spreadsheet files (ODS), and standard text files. The same lessR function call for
reading data also reads data stored in formats unique to specific data analysis software
systems, including R and data from the commercial systems: SPSS, Stata, and SAS.
Download R,
Section 1.2.1, p. 5
install.packages()
function,
Section 1.2.5, p. 8
Download R and install the lessR package according to the instructions on the
following pages. Begin each R session by entering the following two instructions.
Input. To begin each data analysis session: access lessR, read data into R
library("lessR")
d <- Read("")
library() function,
Section 1.1, p. 9
The R function library() accesses the lessR functions stored in your R library,
created when you installed R.
Read() function,
Section 2.3.1, p. 25
Next, transfer your data from a data file on your
computer or the web into a working R session. Enter the lessR function Read() with
the empty quotes, "", to browse your computer’s file system to locate your data file.
Read from the web,
Section 1.3.1, p. 14
Or, between the two quotes when calling Read(), locate your data with a specific
pathname, web address (URL), or name of a lessR data file. When located, Read()
reads the data from that file into a data table within R, what R calls a data frame,
typically named d as in this example. data frame: A data
table within an R
session, ready for
analysis.
After the calls to the library() and Read() functions, data analysis begins. The
following examples of lessR instructions (function calls) are essential to almost every
data analysis endeavor. bar chart example,
Section 4.1, p. 64
histogram example,
Section 5.3, p. 85
scatterplot example,
Section 10.1, p. 195
For the variables of interest in the lessR default data frame
named d, create a color-coordinated bar chart of one variable (Dept), a bar chart of
two variables (Dept and Gender), a histogram (of Salary), and a scatterplot of two
variables (Years employed with Salary).
Input. Histogram, bar chart, and scatter plot function calls
BarChart(Dept)
BarChart(Dept, by=Gender)
Histogram(Salary)
Plot(Years, Salary)
These analyses also compute and display each visualization’s relevant statistics, such
as cross-tabulation tables, inferential tests, and correlation coefficients.
Other core data analytic procedures – t-tests, analysis of variance and covariance,
regression analysis, and factor/item analysis – are also straightforward to perform4 CHAPTER 1. R FOR DATA ANALYSIS
with lessR. You may not yet know the meaning of these analyses but the lessR
instructions to produce them are straightforward. Simplicity is the theme. Data
analysis should involve minimal effort learning how to code. Instead, focus on the
meaning and interpretation of the results. Learn data analysis, not computer coding.
Creating a histogram with the statistics directly from an Excel worksheet is much
faster, easier, and more comprehensive with lessR than with Excel. This conclusion
holds throughout the material presented in this book: Data analysis with lessR is
simultaneously more accessible and more comprehensive than analysis with Excel.
Moreover, “Excel worksheets exhibit a fundamental flaw, the confounding of the data
with the instructions to process that data” (Gerbing, 2021, p. 251). By separating the
instructions to process the data distinct from the data itself, data analysis generally
becomes more straightforward, much easier to develop, and easier to debug.
If you are familiar with menu-based commercial systems such as SPSS, you recognize
the equivalents of SPSS procedures but without the menu pull-downs. Multiple
windows and menu options are replaced by a single reproducible, brief instruction.
Like Excel, SPSS costs money, whereas R is free and runs identically on all standard
computers. There is no longer any need to pay for data analytics software.
regression analysis,
Chapter 11, p. 217,
Chapter 12, p. 250,
Chapter 13, p. 277
And no need for everyone to repeat the same programming to achieve a basic set of
standard, indispensable analyses. A comprehensive regression analysis with standard
R begins with about a dozen separate R statements and programming multiple lines of
R code to organize the results. Replace the many R function calls, as well as the extra
programming to organize the output, by a single call to the lessR Regression()
function. The lessR procedures, such as for regression, rely upon R’s statistical and
visualization capabilities while also providing internal programming to obtain and
organize the results for you. Let lessR do the extra programming for you.
Two primary objectives underlie the lessR project to minimize the needed program￾ming to use R for data analysis.
◦ A data analysis procedure should produce needed text and visualization output
without additional instructions or information other than the procedure’s name,
the name of the analysis procedure, and the relevant variable name or names.
◦ If changes to the default output are desired, such as choosing a new background
color for a visualization, then scan a list of the available options in the manual
to understand how to provide all the information needed to proceed. Instead of
lines of code, simply add one or more options to the instruction that generates
the analysis.
Let’s get started!
1.2 Prepare R for Analysis the cloud:
Computer servers,
usually in locations
apart from the user,
that run applications
accessed via a web
browser.
Download and install R on your computer, or run in the cloud for free or minimal
cost with any device that runs a web browser such as a Chromebook or an iPad. The1.2. PREPARE R FOR ANALYSIS 5
choice is yours. R works the same on your computer or in the cloud.
1.2.1 Download R
CRAN: World-wide
network of servers
and information for
the R system.
The best way to learn R is to start using R, available on many Internet servers
around the world. These servers and the information on them comprise CRAN, the
Comprehensive R Archive Network. Obtain the latest version of R at:
Web Address. Download R
http://cloud.r-project.org
Select an operating system near the top of the resulting web page.
• Download R for Linux
• Download R for MacOS
• Download R for Windows
Windows: On the next web page, on the first line, click base. On the subsequent
page, on the first line again, click the Download R for Windows link that includes
the version number.
Mac: About 15 or so lines down the page, where x.y.z is the version number such as
4.2.2, click either the R-x.y.z-arm64.pkg link for the Apple M-series CPU version
or, further down the web page, the R-x.y.z.pkg link for the older Intel CPU version.
If you do not know the type of CPU in your Macintosh computer, on the Apple
menu at the top-left of the screen, choose the About this Mac option, then locate
the information for Chip.
Linux: On the subsequent web page, choose your distribution. Or, for a Debian
version of Linux, or Debian based versions such as Ubuntu and Mint, instead download
R from the usual software repository available with the Debian package system.
After downloading R, follow the instructions from the installer app. For each prompt
from the installer, choose the default. During the installation process the following
question may appear.
Would you like to use a personal library instead?
If asked, usually respond with a y, for yes, so that you have permission to access the
files created for and needed by R.
1.2.2 Download RStudio
RStudio:
Feature-rich
environment for
running R.
A favored option runs R from within an app called RStudio because of the additional
features that RStudio provides. You can download now, or later, or never, but
RStudio provides compelling advantages. Obtain Rstudio from the following link.
Web Address. Download RStudio for running R
https://posit.co/download/rstudio-desktop/6 CHAPTER 1. R FOR DATA ANALYSIS
Within the RStudio environment you are running the same R app. As shown in
Figure 1.1, RStudio presents several windowpanes, all resizable to customize for a
specific analysis. If desired, change the RStudio default color theme. From the Tools
menu, choose Global Options... and then Appearance. Your author prefers the
iPlastic theme, but there are many choices.
Figure 1.1: Three default RStudio windowpanes and optional R Source pane for R script.
console: The
window for entering
R commands and for
text output.
The primary windowpane is the R console, the same display available from running R
by itself. RStudio directs data visualizations into a second windowpane labeled Plots.
Depending on the chosen tab, display other information, such as your file directory
with the Files tab. A third windowpane displays your data from the Environment
tab or your history of entered R instructions from the History tab.
The first three windowpanes appear by default. A potential fourth windowpane,
shown in Listing 1.1, provides for text files of R code, ready for analysis. Request
this fourth windowpane within RStudio by creating a new R script file.
Input. Open a new R script file in RStudio
File menu –> New File –> R Script
reproducibility:
Analyses can be
re-run in the future
to reproduce
previously obtained
results.
An analysis of saved R instructions is reproducible. You can then save the R script file
for later access. You, or someone else in your organization, can then repeat or extend
the analysis without having to re-enter the R instructions.
command prompt:
The > symbol, which
signals that R awaits
an instruction.
By itself, or within RStudio, R processes instructions at the R console’s command
prompt, illustrated in Figure 1.1. RStudio enhances this process by running stored
instructions from the command prompt. Enter R instructions into the script window,
select one or more instructions, and press the Run button at the top-right of the
windowpane. RStudio will then copy the selected information to the command prompt
and run the instructions as if you had directly entered them into the console. Or,
click on the Compile Report button and send the input and output to an HTML file1.2. PREPARE R FOR ANALYSIS 7
for reading with a web browser or a Word document (a pdf option is also available,
but requires LaTeX software installed).
Figure 1.2 shows the two function calls required to access lessR and retrieve the
data, as well as the single function call required to generate a bar chart of the variable
Dept from that data.
first two functions,
Section 1.1.2, p. 3
Find the variable in the lessR data file called Employee, read
into the default d data table.
Figure 1.2: Run an R data analysis from the RStudio Source windowpane. text editor: A
simplified word
processor for editing
The RStudio Source windowpane is a text editor that saves information as a standard a text file.
text file.
1
text file: A file that
consists only of
letters, digits, and
punctuation plus a
few control codes.
Editing R script files is not limited to RStudio. Any text editor such as the
popular vim editor (used by your author to edit R instructions and write text such as
this book) can edit text files and, therefore, R script files accessed with RStudio.
The Source windowpane provides for saving your R instructions over time to gradually
build a collection of instructions to perform a variety of analyses. Reproduce the
output of any one analysis or modify to obtain a related analysis. Saving your
instructions in a separate file allows you to examine the logic of your underlying
computations more straightforwardly than scattering those instructions over multiple
cells and perhaps multiple worksheets with an app such as Excel.
1.2.3 R in the Cloud
An important company in the R ecosystem, Posit (formerly RStudio, Inc.), offers a
free cloud account for running R at rstudio.cloud. However, the account is free
only for the first 25 hours per month of connect time and 1 hour of execution time.
The monthly accumulated time limit accrues against the time the computer requires
to perform the data analysis computations and the time a cloud project is open. Wait
to log into your account until you are ready to enter the instructions needed to do an
analysis, and then log out of your account when an analysis is complete. Restricting
the time you access the cloud account to the time entering commands and analysis
may allow completion of data analysis projects within the free time limits.
install lessR,
Section 1.2.5, p. 8
R and RStudio are pre-installed on a cloud account, so you only need to install
lessR. Start a new project by clicking on the New Project button for each analysis.
Running in the cloud, R will not read data files directly from your computer. Instead,
upload a data file to the cloud with the Upload button under the Files tab on the
bottom-right RStudio windowpane.
1Word processors, such as MS Word or the free and MS Word compatible LibreOffice Write,
generally save their files in a non-text format with many hidden codes such as for formatting and
paging the document. They also tend to change aspects of the file, such as MS Word’s propensity to
change straight quotes to curly quotes, which do not work with R.8 CHAPTER 1. R FOR DATA ANALYSIS
1.2.4 Start R
Windows and Macintosh users begin an R session the same as any application, of
which there are several possibilities. For example, if you chose to have the R icon
placed on your desktop during the installation process, double-click that icon to start
R. A new R session opens a window called the console. Or, if running within RStudio,
the R console opens in a windowpane, illustrated in Figure 1.1.
An R session is interactive. The last line of the information from the R console in
Figure 1.1 contains only a >, the R command prompt. Enter each R instruction in
response to this prompt. Press the Enter/Return key, and R immediately processes
that instruction. The entire session consists of sequential function calls, each entered
in response to another command prompt, followed by any subsequent R output.
Sometimes the entered function call needs to be completed when Enter/Return is
pressed, such as missing the closing parenthesis. R responds with the continuation
prompt, +. Enter the missing information to continue as usual, or press ESC to cancel.
continuation
prompt: The + sign,
which indicates the
entered instruction is
incomplete.
A function call previously entered at the command line can be re-run or edited
without re-entering. One way to re-run previous instructions is to push the up-arrow
key, ↑, to retrieve the instructions. Or, click the History tab in the upper-right
RStudio windowpane. Then select one or more instructions and click on either the
To Source button to copy the instructions to the Source or script window, or click
on To Console to copy to the command prompt, ready to re-run.
1.2.5 Extend R
package: A set of
related R functions.
R organizes its many functions into groups of related functions called packages. The
initially installed version of R includes six different packages, including the stat
package, the graphics package, and the base package for various utilities. These
packages are installed as part of R on your computer system and are automatically
loaded into memory when an R session begins.
contributed
package: An R
package provided by
the user community.
Many more functions that considerably extend the functionality of R are available
from contributed packages, such as lessR, written by users in the R community.
To access the functions within a contributed package, download the package from
the CRAN servers. install.packages()
function, R:
Download a package
from the R servers.
To download the lessR package, start up R and invoke the
install.packages() function at the R command prompt, >.
Input. Install lessR and related packages
install.packages("lessR")
The lessR functions rely upon R functions as well as functions from other contributed
packages. R downloads these additional packages with lessR.
Sometimes one or more of these dependent packages is in the process of being updated
by its developer, which means that the package is not yet in a form ready to run on
Windows or Macintosh computers, what is called a compiled version. When installing
lessR, R may prompt with the following question.1.2. PREPARE R FOR ANALYSIS 9
Do you want to install from sources the packages which need compilation?
(Yes/no/cancel)
Generally, answer no. Avoid compilation since your computer likely is not equipped
with the needed tools. By answering no you download the slightly older Windows or
Macintosh ready-to-go (binary) versions of the packages, where the compilation has
already been done for you.
update.packages()
function, R: Update
the versions of the
installed R packages.
Use the update.packages() function to update one or more packages.
Input. Update all installed packages
update.packages()
RStudio also provides the option of updating from its Tools menu. Select the Check
for Package Updates... option.
1.2.6 Access lessR
install R, p. 8
After installation, begin each new R session with the library() function shown in
Listing 1.1 to access the over 60 lessR functions for data analysis, plus the included
data sets.
library() function,
R: Activate the
functions of the
specified package. Input. First entry for a new R session after lessR is installed
library("lessR")
lessR 4.2.5 feedback: gerbing@pdx.edu
--------------------------------------------------------------
> d <- Read("") Read text, Excel, SPSS, SAS, or R data file
d is default data frame, data= in analysis routines optional
Learn about reading, writing, and manipulating data, graphics,
testing means and proportions, regression, factor analysis,
customization, and descriptive statistics from pivot tables.
Enter: browseVignettes("lessR")
Listing 1.1: Access lessR with the R library() function.
To learn more about what to do next, get some help.
1.2.7 Get Help
browseVignettes()
function, R: Present a
list of documents
that illustrate
different data
analyses.
R provides two ways to obtain help. A great way to learn more about R functions
accesses the R function browseVignettes(). A vignette provides explanations and
examples to illustrate various data analyses. To access a package’s vignettes, enter the
package name enclosed in quotes in the function call. Figure 1.3 applies the function
to lessR, which organizes the vignettes by theme: Data, Visualize, Models, Factor
Analysis, and Customize. Click on an HTML link to view the primary information,
the corresponding vignette displayed as a web page. Also, click on a source link to
view the input that created the HTML, or a code link to extract the R code from the
vignette.10 CHAPTER 1. R FOR DATA ANALYSIS
Input. Access the lessR vignettes
browseVignettes("lessR")
Figure 1.3: The lessR vignettes.
Another help system provides the details of a specific function. Use the R function
help() by referencing a specific function by its name.
help() function, R:
Help to obtain the
user manual for a
specified function.
Or, use the abbreviation ?
followed by the function name. The output is the manual for the specified function.
Input. Access the manual for a specified function
?BarChart
Each required manual follows a specific structure, though somewhat geekier than a
vignette. The resulting explanation includes a definition of all the parameter options
for the function, a more detailed discussion of how to use the function, and examples
of using the function. However, if the package offers vignettes, they generally provide
more detailed explanations with output.
The next step explains how to invoke the various R, lessR, and other functions.
1.2.8 R Functions for Analysis
Function Parameters
Functions do all the work in R.
parameters of a
function:
Information input
into the calculations
of the function.
The values of a function’s parameters contain the
information provided to the function for analysis. Specify any parameter values
between the parentheses of the function call. If no parameter values are needed, still
include the parentheses after the function name but with no content, ().
In our bar chart example, the one parameter value passed to BarChart() is the
variable name, Dept. function manual,
Section 1.2.7, p. 10
Because it is listed first in the function call, and is also the
position of the first parameter in the definition of the function found in its manual,
the specified value does not require the parameter name. The parameter name that
specifies the first variable to analyze with BarChart() is x, so the following two
function calls are equivalent.1.2. PREPARE R FOR ANALYSIS 11
Input. Do not need the parameter name for the variable analyzed if listed first
BarChart(Dept)
BarChart(x=Dept) default value:
Assumed value for a
function’s parameter
that can be explicitly
changed.
Many function parameters are defined with default values, values assumed but can
be overridden. For example, lessR functions such as BarChart() reference the name
of the relevant data frame, R’s name for the data table, with the data parameter.
data frame,
Section 1.1.2, p. 3
Unlike R functions, lessR provides a default value for the input data frame, d, that
contains the variable(s) to analyze.
data parameter:
The name of the
data frame that
contains the
variables for analysis,
with default name d.
Input. Access the default d data frame with or without the data parameter
BarChart(Dept)
BarChart(Dept, data=d)
You only need to invoke the data parameter when the input data frame is not d. If
all the parameters in a function call have default values, then no information needs
to be passed to the function, just empty parentheses in the function call. For any
parameter without a default value, necessarily specify a value for that parameter.
For the primary lessR visualization functions, lessR also provides an interactive
display. You point-and-click to choose the data file and select the variable(s) of
interest. lessR then displays the visualization and associated statistics. Point-and￾click to choose different parameter values, such as changing the color of the bars or
points. Not even a need to enter the simple lessR function calls!
Use the interact() function to initiate an interactive display. Pass no values to the
function to display a list of available displays, shown in Listing 1.2.
interact()
Run interact() with a one of the following app names, such as:
interact("BarChart")
Valid names (enclose in quotes):
"BarChart", "Histogram", "PieChart", "ScatterPlot", "Trellis"
Listing 1.2: Available interactive displays of parameter values for data visualization.
When finished, lessR writes the function calls for you to generate the same output,
complete with explanatory comments.
Function Types utility function:
Inform or set
characteristics of the
data processing
environment.
Find at least three different types of functions within the R system. Utility functions
either provide information or set characteristics of the overall environment to facilitate
data processing. An example of a utility function is head(), which lists the variable
names and the first six lines of data from a specified data frame. head() function,
Section 1.3, p. 15
A data analysis function applies statistical methods to process the data to obtain the
requested analysis, the function’s output, as illustrated in Figure 1.4.12 CHAPTER 1. R FOR DATA ANALYSIS
Function Input Output Possibilities
transform
input into output
text: console
graphics: window or file
object: e.g., data table
data values
for one or more
variables
to process data
Figure 1.4: General procedure for functions that process data, either data analysis or data
modification.
An example of a data analysis function is BarChart().
data analysis
function: Procedure
to access and analyze
data.
Many lessR functions, such as
BarChart(), direct their resulting text output to the R console and any visualizations
to the plot window.
Another way to process data is to create or modify the data in preparation for
data modification subsequent data analysis.
function: Procedure
to create or modify
data.
One example of a data modification function is Read(),
which reads data from an external data file and then usually directs the data into
an R data frame. Other examples of data modification are sorting the data values
or transforming them, such as taking their logarithms. The modified data can then
either be directed back to overwrite the original data table, d, or to a new data table,
leaving the original d unmodified.
1.2.9 Vectors
vector: List of
variables or constants
defined as one entity.
A common task when calling a data analysis function specifies either a list of variables
or a list of constants of either numbers or character strings. Such a list is called a
vector.
fill parameter, lessR,
Section 4.2.3, p. 66
Suppose you wish to fill the bars of a bar chart with two alternating colors,
"darkred" and "darkblue". The fill parameter manually specifies colors for the
lessR data visualization functions that fill the interior of an object, bars or points.
Input. Bar chart with bars of alternating color for data in data frame d
BarChart(Dept, fill=c("darkred", "darkblue"))
c() function, R:
Combine a list of
values to define a
vector.
Separate the individual values in a list with commas, embedding the entire set in the
combine c() function. This function delineates a list of values, a vector, from the rest
of the information that surrounds the vector. In the above example, the value for the
parameter fill is a vector of two values, separated from the rest of the information
in the function call by the c() function.
Another example is the list of the first eight integers specified with the c() function.
c(1,2,3,4,5,6,7,8)
: notation: Specify a
sequential set of
integers.
If the items in the list are sequential integers, the : notation can be used instead.
1:8
Or, combine the types of expressions. The c() function surrounds the individual
components of the vector with commas to separate the components.1.3. DATA 13
c(1:4,5,6,7:8)
Enter any of the three preceding expressions at the command prompt to obtain the
following output.
[1] 1 2 3 4 5 6 7 8
The [1] indicates that the first value in the output begins on the first line, the only
line of output in this example.
When referring to character constants, all analysis apps such as R and Excel enclose
these constants within quotes. Here create a character vector of three letters.
c("c", "a", "t")
list of variables,
Section 10.3.2, p. 206
The concept of a vector applies not only to a sequence of numbers or characters as
illustrated here but also to lists of variables.
1.3 Data
Data analysis begins, naturally enough, with data. Organize the data for analysis into
a rectangular table, the form required by R and virtually every other data analysis
system such as Python or SPSS. This section presents two sets of data analyzed
throughout this book, and a discussion of how to create your own data table.
1.3.1 Data Example I: Employee Data
A company’s human resources department recorded the following for each employee:
Name, Years of employment, Gender, Department employed, annual Salary, Job
Satisfaction, and Health Plan. data file: A file on a
computer system
that contains data,
usually organized
into a table of rows
and columns.
Consider a data file with measurements organized into
a table for 37 employees, available on the web.
http://lessRstats.com/data/employee.xlsx
Figure 1.5 displays the first nine lines of the Excel version of this data file.
Figure 1.5: Variable names in the first row and the first eight rows of data from the employee
data table as stored in an Excel worksheet.14 CHAPTER 1. R FOR DATA ANALYSIS
Organize the Data
The rows of a data table are of the unit of the analysis, the object of study.
unit of analysis:
The class of people,
organizations, things
or places from which
measurements are
obtained.
In this
example, the unit of analysis is a person, an employee at a specific company. Other
potential units of analysis include organizations, places, events, or things in general.
The worksheet, Excel or similar, stores either a variable name or a data value in a
cell, the intersection of a row and a column. A data value is the value of a variable
data value: The for a specific instance of the unit of the analysis, here a specific person.
value of a single
measurement or
classification.
For example,
the person listed in the first row of data, the second row of the worksheet, is Darnell
Ritchie. Darnell’s annual Salary is $53,788.26. The data value for the variable Dept
is ADMN, which indicates employment in general administration. The data value
for the Years employed at the company is not available for the second person, James
Wu, so the corresponding cell is empty.
variable: Attribute
that varies from unit
to unit (e.g., different
people.)
Data analysis depends upon the concept of a variable. A variable is a characteristic
of an object or event with different values for different people, organizations, etc.
Each variable name is concise, usually less than 10 or so characters, and serves as the
reference for the data values of the variable in any subsequent data analysis.
The first row of the data table usually contains the variable names. The data values
for each variable are within the same column. However, the data values of the first
column in this particular data table are not for a variable but are instead unique ID
values that identify the employee for each row of data. For example, the first row of
data in Figure 1.5 consists of the data values for employee Darnell Ritchie.
case (observation,
instance): Data
values for a single
unit such as a person,
organization, thing
or region.
The data values in a single row of the data table in Figure 1.5 are the data for a
single person. Unfortunately, there is no standard notation for a row of data. Various
authors use terms such as case, observation, instance, example, and sample.
The form of the data table in Figure 1.5 is the wide format.
data table, wide:
Data values for each
case are in a row and
for each variable in a
column.
We explore another data
format in Chapter 8, but the wide format is more commonly encountered in data
analysis. All the data values for one case (e.g., person) are in one row, and all the
data values for a variable are in one column.
What does a data table in Excel have to do with R? If R did not exist, the data file
would still be an Excel worksheet. The data table has nothing intrinsically to do with
R. However, R can analyze the data in the worksheet.
Read() function,
lessR, Section 2.3,
p. 25
Read the data into R using the
lessR function Read(), such as into the R data frame named d.
data frame,
Section 1.2.8, p. 11 Input. Read a data file from the web into the d data frame
d <- Read("http://lessRstats.com/data/employee.xlsx")
When read into the d data frame, the data continues to exist as an Excel worksheet
file but now also exists within the working R session.
View the Data
One reason for the popularity of worksheet apps such as Excel is that the data are
always visible. Working within the command-line environment of R makes it possible1.3. DATA 15
never to view the data. However, not viewing your data is a mistake. Always be
aware of your data, what it looks like, the variable names, the number of cases (rows),
the number of variables (columns) of the data table, etc.
Viewing your data from within R is easy, accessed with simple function calls from the
command line, or, in RStudio, from the Environment tab in the upper-right window
pane.
head() function, R:
View the variable
names and first
several rows of the
specified data table
within R.
Listing 1.3 shows how easy it is to view a useful excerpt of your data in R by
using the head() function. The name of the relevant data frame is the first parameter
value.
head(d)
Years Gender Dept Salary JobSat Plan Pre Post
Ritchie, Darnell 7 M ADMN 53788.26 med 1 82 92
Wu, James NA M SALE 94494.58 low 1 62 74
Downs, Deborah 7 W FINC 57139.90 high 2 90 86
Hoang, Binh 15 M SALE 111074.86 low 3 96 97
Jones, Alissa 5 W <NA> 53772.58 <NA> 1 65 62
Afshari, Anbar 6 W ADMN 69441.93 high 2 100 100
Listing 1.3: The R function head() displays the variable names and first six lines of data,
here for data frame d.
The head() function is your friend as you proceed with a data analysis. Access the
function regularly to see what you are modifying and analyzing.
tail() function, R:
View the variable
names and last
several rows of the
specified data table
within R.
The corresponding
function tail() displays the variable names and the last six rows of the data table.
To see the entire data set, enter only the name of the data frame, here d.
names() function, R:
View the variable
names of the
specified data table
within R.
The R
function names() displays the variable names in the specified data table.
To retrieve the dimensions of your data frame, R provides three functions. The
function dim( ) displays the number of rows and then the number of columns. Or,
obtain each separately with nrow() and ncol(). Listing 1.4 illustrates these functions
applied to the data frame d. Specify the data frame for each function call.
> dim(d)
[1] 37 8
> nrow(d)
[1] 37
> ncol(d)
[1] 8
Listing 1.4: Three R functions to display the dimensions of the data frame, here d.
Always know your data when working in R or any other data analysis system. What
you thought was there is not always there. You cannot avoid seeing your data with a
worksheet, but only because worksheets mix up the instructions to process the data
with the data. R overcomes this worksheet deficiency while simultaneously offering
simple ways to show your data, allowing you to reap the benefits of decoupling the
data from the instructions for processing the data.16 CHAPTER 1. R FOR DATA ANALYSIS
1.3.2 Data Example II: Machiavellianism
The data values described in this section are the responses to an attitude survey
from Hunter, Gerbing, and Boster (1982). The topic is Machiavellianism, an essential
component of the Dark Triad (Paulhus & Williams, 2002) personality configuration.
Christie and Geis (1970) introduced the concept of Machiavellianism and its mea￾surement to psychological researchers. Their Mach IV scale, a set of 20 items for
assessing endorsement of Machiavellian beliefs, appears in Table 1.1.
Mach IV Item
1. Never tell anyone the real reason you did something unless it is useful to do so.
2. The best way to handle people is to tell them what they want to hear.
3. One should take action only when sure it is morally right.
4. Most people are basically good and kind.
5. It is safest to assume that all people have a vicious streak and it will come out
when they are given a chance.
6. Honesty is the best policy in all cases.
7. There is no excuse for lying to someone else.
8. Generally speaking, people won’t work hard unless they’re forced to do so.
9. All in all, it is better to be humble and honest than to be important and dishonest.
10. When you ask someone to do something for you, it is best to give the real reasons
for wanting it rather than giving reasons which carry more weight.
11. Most people who get ahead in the world lead clean, moral lives.
12. Anyone who completely trusts anyone else is asking for trouble.
13. The biggest difference between most criminals and other people is that the criminals
are stupid enough to get caught.
14. Most people are brave.
15. It is wise to flatter important people.
16. It is possible to be good in all respects.
17. Barnum was wrong when he said that there’s a sucker born every minute.
18. It is hard to get ahead without cutting corners here and there.
19. People suffering from incurable diseases should have the choice of being put
painlessly to death.
20. Most people forget more easily the death of a parent than the loss of their property.
Table 1.1: The Christie and Geiss (1970) Mach IV scale.
Machiavellianism:
Beliefs and values
that endorse the
cynical manipulation
of others.
To understand Machiavellianism, consider two different people with two radically
different perspectives on life and human nature. One person believes in the inherent
goodness of people, in living an ethical life, treating others with respect, trust, and
honesty. The other person believes that a primary goal of life is to achieve as much
power and material possessions as possible, at any means possible, that the “end
justifies the means”, to do whatever it takes. To “get ahead” may include lying and
cheating. This second person tends to believe that other people are like themselves
or are naive and gullible, almost willing subjects for their manipulation.
This second perspective aligns with the writings of Niccolo Machiavelli from the
1500’s, who wrote extensively on the need for political leaders to do whatever is
necessary to defend their political power and success, even if their methods are morally
contemptible. Being successful for good or bad requires cunning sophistication and
tactics to achieve the desired goal. His most famous writing, The Prince (1902/1513),1.3. DATA 17
has become a manual of sorts for those seeking to implement Machiavelli’s advice.
The concept applies to both political power, in terms of government and organizations,
and personal power, in terms of interpersonal relationships.
As with thousands of other such scales devised by psychologists and social scientists,
assess each item on a Likert scale.
Likert scale:
Measurement of
usually 4 to 7 scale
points along the
Disagree-Agree
continuum.
The respondent considers the extent of agreement
for each item along a continuum of Disagreement/Agreement, assessed by a small
number of categories that vary from Disagree to Agree, usually 5, 6, or 7 categories.
An odd number of categories allows for a neutral point in the middle, separating the
Disagreement side from the Agreement side. In contrast, an even number forces the
respondent to choose one side or the other, even if favored slightly. The Mach IV
data were collected with each of the 20 items responded to on the following 6-point
Likert scale.
Strongly Disagree Slightly Slightly Agree Strongly
Disagree Disagree Agree Agree
Hunter et al. (1982) administered the Mach IV items to college students as part of
a longer attitude survey. All items on the survey were randomized for presentation
on the resulting questionnaire. Measurements were collected for 351 respondents.
The responses to each item on the 6-pt scale were numerically coded. A Strongly
Disagree was coded as a 0, stepping through the integers for each successive category
until reaching Strongly Agree, coded as a 5.
These data provide the basis for showing how to read and analyze survey data in
lessR, referenced throughout this book. Find the data file at the following location
on the web.
http://lessRstats.com/data/Mach4.fwd
read fixed width data,
Listing 1.5 shows the first six rows of the data file. How to read the data into R is Section 2.4.3, p. 33
covered in the next chapter.
0100004150541540000401324
0127001440330440111244310
0134121054405341400202401
0222105240444520001115440
0264123230222533101312320
0282022332323141312223321
Listing 1.5: First six rows of data for the 351 row Mach IV data file.
For each respondent, a four-digit ID number occupies the first four columns of each
row. The data values are sorted by the values of this ID field. The 5th column is
Gender, encoded with 0 for Man and 1 for Woman. The last 20 columns are the
responses to the 20 Mach IV items, each response a single digit ranging from 0 to 5.
Accordingly, there are 25 columns of data.
The Mach IV data fixed-width data file is available on the web.18 CHAPTER 1. R FOR DATA ANALYSIS
http://lessRstats.com/data/Mach4.fwd
The Mach IV data set is also included as part of lessR with the name Mach4, ready
for analysis. The internal version does not include the four-digit ID number.
1.3.3 Create a Data File
Worksheet Apps
Data values for analysis are usually not directly entered into the data analysis
application, such as R. Data typically originate instead from some other application or
source, perhaps a file saved in the form of a worksheet. Typical worksheet applications
that open and save worksheet files in the Excel format, xlsx, include:
a. Microsoft Excel: Costs money and runs on Windows, Macintosh, and Android
and iOS devices LibreOffice,
libreoffice.org b. LibreOffice Calc: Free of cost, open source, runs on Windows, Macintosh, and
Linux
c. Apple Numbers: Free of cost, runs only on Apple devices, Macintosh and iOS
The lessR function Read() reads data from a worksheet in the Excel or OpenDocu￾ment Spreadsheet (ODS) format, so all three of the above choices of a worksheet app
work equally well to store data for analysis in R. All three applications save in Excel
format and Excel and LibreOffice can save in ODS format.
Perhaps your IT department provided the data in worksheet form, or some app
collected the data such as from an online survey. Or, maybe you entered your data
into a worksheet. To enter data directly into a worksheet, open the corresponding
worksheet app, then enter the variable names across the cells in the first row, as
in Figure 1.5. Enter the corresponding data row by row, aligning each column to
contain the data values for a single variable.
The worksheet is a convenient container that matches the tabular structure of data. A
worksheet wonderfully organizes the data into the proper format of rows and columns,
providing an excellent means to enter and view the data. Do not underestimate the
importance of the phrase “wonderfully organizes”. Typically, much of the work for
data analysis consists of organizing and cleaning the data for subsequent analysis.
Read() function:
Section 2.3.1, p. 25 A worksheet application and R are complementary tools for data analysis. With the
lessR functions Read() and Write(), easily exchange data stored in a worksheet
with R.
Write function,
Section 2.6.2, p. 37 For example, it is generally much faster and less work with more flexibility and
more extensive results, to read Excel worksheet data into R to compute a histogram
rather than doing the histogram from within Excel.
histogram,
Section 5.2, p. 83
Data Validation
The data values within each column of a data table should all share the same format
and the values should lie within an acceptable range. For example, the data values
for the Salary column in Figure 1.5 are all positive numbers stored in the currency
format. It is not meaningful to include a value such as −3000.82 for a Salary, or1.3. DATA 19
values of abc, or >100,000. Nor is it meaningful to represent Gender with multiple
encodings such as "W", "w", "Woman", and "woman". Instead, consistently code each
level of a categorical variable with only one encoding.
In data analytics, messy, incorrectly recorded data such as inconsistently coded data
values are all too prevalent. Someone has to clean and better organize such data
before analysis can begin. Data validation is a valuable tool for improving the quality
of the initial organization of the data.
data validation:
Verify that all data
values within the
same column have
the same format and
conform to the same
specifications.
Activate the data validation procedure when
entering data values into a spreadsheet to ensure that the data values in each column
are formatted consistently.
Data validation can, for example, verify that all data values in a column are between
0 and 100 inclusive, that all zip codes have five characters, and that all names have
only alphabetical characters. If a data value entered into a worksheet does not meet
the specified valid attributes, it is rejected and cannot be inserted into the relevant
cell.
To activate data validation in Excel, go to the Data ribbon, choose the Validate
pull-down menu, and then the Data Validation... option. Select Circle Invalid
Data as shown in Figure 1.6a. The resulting dialog box, illustrated in Figure 1.6b,
provides several options for specifying the specific type of data that Excel can accept
into the column of designated cells.
Figure 1.6: (a) Choose the data validation option or circle invalid data, (b) Excel data
validation options to specify the acceptable data format in the designated cells.
What if an erroneous data value was entered into a cell before data validation was
used? Excel can flag invalid numbers by creating a red circle around the cells in
question. Although R data analysis has far more possibilities than Excel data analysis,
Excel or similar spreadsheets are ideal for storing small to medium-sized data files.
R and worksheets work well together. Data can be cleaned in R, but it can also
be cleaned in a worksheet with data validation, then re-saved and read into R for
analysis.
In LibreOffice Calc, activate data validation by selecting the relevant column of data
values. From the Data menu, choose the Validity... option . Choose the resulting
Criteria tab and specify the desired property of valid data values in the specified
cell range.20 CHAPTER 1. R FOR DATA ANALYSIS
1.4 Analysis Problems
1. Access. Get and access R and lessR.
a. Download the latest version of R for your computer.
b. Start an R session and download lessR.
c. Verify that lessR is downloaded and properly working by displaying the infor￾mation lessR writes to the console when successfully loaded.
2. Vectors. Create a vector of . . .
a. Consecutive integers from 10 to 16.
b. The numbers -5, 0, 5.
c. The letters d, o, and g.
3. Data. Consider the data in Figure 1.7, randomly selected from a data file of the
body measurements of thousands of motorcyclists.
Figure 1.7: Gender, Height and Weight of eight motorcyclists.
a. Enter this data table into a worksheet.
b. Indicate any missing data.
c. List each variable and classify each as continuous or categorical. If categorical,
state if nominal or ordinal and specify the categories.
d. Read the data into R. Display the output.
e. Display the entire contents of the data frame into which you read the data.
f. How does the way in which the data values for each variable are encoded within
the computer match your assignment as continuous or categorical. Explain
why the computer encoding is consistent with the continuous and categorical
distinction.
g. Which value in R is encoded as NA. Why?Chapter 2
Read and Write Data
2.1 Quick Start
How to access lessR functions was addressed in Section 1.2.5. After R and lessR are
installed, load the package into memory with library("lessR") each time R is run.
data table,
Section 1.3.1, p. 13
Store data in a rectangular data table with variables in columns and people or
whatever the unit of analysis in rows. Usual, though not necessary, to place the
variable names in the first row of the data file. Store the data in various formats,
including Excel and OpenDocument Spreadsheet formats, plain text, and R and SPSS
native formats.
Read the data file into an R data table, called a data frame, to analyze the data.
data frame,
Section 2.3, p. 25 Usually read into a data frame named d, the default data name for the lessR data
analysis functions. One call to the lessR function Read() reads the data from one
of many formats and then displays relevant information regarding the data. In the
call to Read(), enclose the reference to the data’s location within quotes, "". If the
quotes are empty, Read() has you browse your file directory to locate the data file.
d <- Read("")
If the data file exists on the web, enclose the full web address (URL) within the
quotes, including the http://, as the first value passed to Read(). Or, include the
path name, the file location and name, within the quotes.
d <- Read("http://lessRstats.com/data/employee.csv")
Optionally, include row names by identifying a column of the data file that consists
of unique row ID’s with the row_names parameter, such as the following if the ID’s
are in the first column. row_names
parameter,
Section 2.3.4, p. 29 d <- Read("", row_names=1)
By default, missing data values may be literally missing, that is, no data value is
present for a specific cell in the data table. Or, if a missing value is identified by one
or more codes, such as -99 or "XX", invoke the missing parameter.
missing parameter,
Section 2.3.3, p. 27
2122 CHAPTER 2. READ AND WRITE DATA
d <- Read("", missing=c(-99,"XX"))
If you use a comma as the decimal separator instead of a period and a semicolon
instead of a comma for the value separator, invoke the Read2() function to read a
Read2() function, csv data file.
Section 2.4.4, p. 33
d <- Read2("")
Read variable labels from a separate text or Excel file to provide more informative
output than shorter variable names. Read the variable labels into the l data frame,
variable labels, the required name as of this writing.
Section 2.5.2, p. 34
l <- Read("", var_labels=TRUE)
Variable labels are optional but enhance the interpretability of the output.
2.2 Types of Variables
Data analysis revolves around the concept of a variable. Data analysis is the analysis
of the values of one or more variables, done today using the computer to perform
the computations. Yet people were doing data analysis well before computers were
invented. The meaning of a variable exists apart from the computer. Accordingly, we
define variables at two different levels: conceptual and computational. We need to
understand the meaning of our variables, and we also need to know how the data
values for a variable are represented digitally in computer storage. This distinction
applies to all data analysis software, including R, Excel, Python, SPSS, and others.
2.2.1 Variables as a Concept
Consider the conceptual meaning of variables. Data analysis distinguishes between two
types of variables, categorical and continuous. Variables are analyzed in different ways
depending on their type, so it is important to know which variables are continuous
and which are categorical before beginning any research. These variable types can
then be subdivided further, as described below.
Continuous Variables
continuous
variable: A variable
with numerical
values.
The values for continuous variables are ordered along a quantitative continuum, the
abstraction of the infinitely dense real number line. Choose any two values and find
an unlimited number of numeric values lie between them. Examples of continuous
variables for a person are Age, Salary, and extent of Agreement with an opinion
about some political issue; for a car, MPG and Weight; and for a light bulb, Mean
Number of Hours until Failure and Electrical Consumption per Hour (kilowatt hours).
A continuous variable is sometimes called a quantitative variable.
Distinguish between a continuous variable’s actual values and the data values that
emerge from measuring those values. Measurement categorizes data values into2.2. TYPES OF VARIABLES 23
specific groups. The value of the variable as it exists always differs from the value of
its measurement, the data value. Nothing, for example, weighs exactly 2 pounds, 2.01
pounds, or even 2.0000000001 pounds. The real weight may theoretically be stated as
an indefinitely large number of decimal digits. In contrast, indicate a measurement to
a specific level of precision, such as, for weight, to the nearest pound, ounce, or gram.
Measurement groups all similar weights together, approximating the true weight to
the nearest pound or whatever.
ratio data:
Numerical scale with
a fixed zero point
and equal intervals.
Interpret the data values measured on a numeric scale for a continuous variable
according to one of two types. Ratio data follow a numeric scale with the usual
properties that are assigned to numbers. There is a fixed zero point and values on
either side of zero scale proportionality. In particular, two different values can be
compared by their ratios: 20 is twice as much as 10. Equal intervals of measurement
separate values that are equal distance from each other. For example, the distance
between 21 and 22 represents the same underlying difference for 22 and 23.
A weaker numerical scale applies to interval data, which maintains the equal interval
property of ratio data, but does not have a fixed, natural zero point.
interval data:
Numerical scale
without a fixed zero
point but equal
intervals.
The classic
example of two alternative interval scales compares Fahrenheit and Celsius temper￾atures. Each has a different value of zero in terms of the actual magnitude of the
temperature. Because the value of zero is arbitrary in either scale, ratio comparisons
are not valid. For example, 20◦F is not twice as warm as 10◦F.
Categorical Variables
categorical
variable: A variable
with a relatively
small number of
unique values, called
categories or levels.
The primary type of variable other than continuous is the categorical variable.
levels: Values of a
categorical variable.
The
values of a categorical variable form a relatively small number of categories called levels.
Each level represents a distinct group. For example, the values of the categorical
variable Gender define groups of Men, Women, and Other. Other categorical variables
are Cola Preference, State of Residence, or Football Jersey Number. Yes, the number
on the jersey consists of numeric digits, but those digits are labels, not subject
to arithmetic operations such as computing an average. A categorical variable is
sometimes also called a qualitative variable or a grouping variable.
The values of categorical variables are labeled rather than measured. labeling: The
process of classifying
an observation into a
group described by a
label.
We do not
measure the state of the USA in which a person resides, but rather assign a person to
that state based on self-report or an examination of public records. The classification
into a group assigns a label, such as Oregon or Texas.
One type of categorical data are classifications into discrete, unordered categories. nominal data:
Data values grouped
into unordered
categories.
The
resulting data values are called nominal data. Data for Gender, State of Residence,
and Phone Manufacturer are examples of nominal data.
Or, the measured value of a continuous variable is so imprecise that, instead of a
numerical scale, only a few categories exist in which the measured values can be
placed. Suppose that persons admitted to the emergency room are swiftly placed into
one of only three severity categories: mild, moderate, or severe. This simple rating
scale recognizes that some injuries are more severe than others, but the severity is
classified into one of only three categories. The underlying variable for Injury Severity24 CHAPTER 2. READ AND WRITE DATA
is continuous. This underlying progression of severity is assumed, but not equal
intervals of severity that separate the levels. The rater’s interpretation of Moderate
Severity of Injury may be closer to Mild Severity than Severe Severity of Injury.
ordinal data:
Ordered categories,
rankings.
Data values grouped into ordered non-numeric categories, rankings, are ordinal data.
As another example, suppose the top three sprinters are ranked in order of finish
in the 100 meter dash: 1st, 2nd, and 3rd. The finish times represent a continuous
variable, but simply ranking contestants by order of their finish does not convey if
the race was extremely close or if the winner finished well ahead of their nearest
competitor.
2.2.2 Variables in the Computer
The data analyst has a conceptual understanding of how a variable’s data values are
structured. Are they continuous or categorical? Are they numeric or non-numeric?
What is their valid response range? The data values for the variables are analyzed on
the computer, so how the data values are conceptually defined should align with how
the computer stores them, such as within an R data frame.
After reading the data values into any data analysis app, before data analysis begins,
verify that the data were read correctly and represented correctly in the resulting
data frame. Many things can go wrong. Perhaps errors occurred as the data values
were entered into the data file. Maybe the data values were not correctly read into
the data analysis app, such as R. Perhaps there is too much missing data to permit
meaningful analysis.
data storage type:
How the data values
of a variable are
physically stored in
the computer.
The data storage type is the computer’s digital representation of a data value in its
memory. The storage type should match the conceptual definition of the variable. The
common R data storage types for numeric variables are type integer, for numbers
without decimal digits, and double, for numbers with decimal digits, each represented
in computer memory with a long storage unit called double precision. Dates and
times can also be directly represented Date variables, 1
Section 5.4.2, p. 94
Different data storage types may be used for categorical variables, both in the data
file that holds the data and in the data frame that R uses to store the data during a
data processing session. For example, the data values of a categorical variable can
be integers with a different integer assigned to each level, such as 0 for Man, 1 for
Woman, and 2 for Other. Or, the values could be stored as non-numeric characters,
what R calls type character, such as M for Man, W for Woman, and O for Other.
factor: R data
storage type for
categorical variables.
Regardless of type integer or character as read into an R data frame, transform
a categorical variable to a variable of type factor before analysis begins. The
transformation provides information beyond that contained in the data, such as
meaningfully ordering the levels of the categorical variable. Data of type factor are
internally stored as integers but displayed as descriptive labels.
factor function,
Section 3.2, p. 43
1Another data type sometimes encountered is logical, a variable with only two possible values,
TRUE and FALSE. When reading data, however, this data type is not typically encountered. The two
data values for a logical variable are usually encoded with character codes or the integers 0 and 1
instead of TRUE and FALSE.2.3. READ DATA 25
2.3 Read Data
The data table exists in multiple distinct formats and locations as a data analysis
project proceeds through various stages. The data can be saved on the computer
indefinitely, including in worksheets such as Excel or LibreOffice Calc, the universally
accessible text data file comma separated values or csv format, and data files from
numerous commercial programs such as SPSS. For analysis, read the data table into
an active R session, where the data table is called a data frame.
data frame: A data
table within an R
session, ready for
analysis.
Once inside R, the
data frame can be written back to a computer file in R format or exported to a
worksheet format such as Excel, or to csv. Transfer the data table between these
several formats with ease.
Write() function,
Section 2.6.2, p. 37
Read() function,
lessR: Read data
from a file into an R
data frame.
Read the data file with the lessR function Read(). Optionally reference most lessR
functions with an abbreviated name, such as rd() for the function Read(). Use the
full name to explicitly indicate the task performed by the function, or, an abbreviation
to minimize keystrokes. Also, R pays attention to capitalization, so be careful to also
do so. Some R functions begin with the characters read with a lower case r, which
refer to different functions than the lessR function Read() with the uppercase R.
2.3.1 Access the Data
Read() recognizes Excel2 worksheet files by the .xlsx filetype, Open Document
Spreadsheet3 files by the .ods file type, and csv text files by the .csv file type.
Identify data files previously created from within R by the .rda file type for R data,
and data files from the SPSS4 data analysis system by the usual .sav file type. format parameter:
Explicitly indicate
the file format
without relying upon
the file type.
Or,
manually set parameter format to override the data file’s file type.
Browse for the Data File
One option with Read() is to locate your data file by browsing for it on your computer
system and then reading the relevant data into an R data frame. browse for a file:
Navigate the file
system to locate the
file.
To browse for a
file, your operating system automatically opens a window that allows you to locate
the file within a given directory (folder) on your computer or network. Then, either
double-click on the file name or click on the file name and then click on the Open
button. To instruct R to browse for your data, call the Read() function with just
empty quotation marks "", potentially the only supplied parameter value.
Input. Browse for the data file’s location, then read into R
d <- Read("")
Once the file is identified, the Read() function reads the data from the file into a
designated internal R storage container for a data table, the data frame. Specify any
2
Read() relies upon the read.xlsx() function from the openxlsx package (Schauberger & Walker,
2021) to read Excel files.
3
Read() relies upon the read_ods() function from the readODS package (Schutten, Chan, Leeper,
& Foster, 2020) to read Calc files.
4
Read() relies upon the read_spss() function from haven package (Wickham, Miller, & Smith,
2022) to read SPSS files, similarly, read_SAS() and read_stata().26 CHAPTER 2. READ AND WRITE DATA
valid R name for the data frame, though if working with only one data frame, d is
assignment typically an excellent choice as it is the default name for the lessR functions.
operator: <-,
Assigns the value of
an expression on the
right side to a named
object on the left
side.
Assign
the data read into R to the d data table with the assignment operator, <-, a “less
than sign” followed by a “minus sign”. The <- mimics an arrowhead to indicate the
direction of the assignment, here for the data read from a data file into the data table
within R called d.
Specify the Data File
The alternative to browsing locates the data file with the file’s path name on your
computer or network file system or a web address (URL). List this name as the first
argument within quotes in the call to Read(). Here employee.csv is a csv data file
stored in the data directory at the lessR website. The following function call reads
data from a file on the web.
Input. Read data from a computer file on the web into R
d <- Read("http://lessRstats.com/data/employee.csv")
Another possibility reads a data file directly from within lessR itself, included in the
lessR installation.
Input. Read data from a lessR data file
d <- Read("Employee")
data parameter,
Section 1.2.8, p. 11
Because d is the default name for lessR data analysis files, d does not need to be
explicitly designated by a lessR data analysis function (by providing a value for the
data parameter). All of the many, many R data analysis routines are also available to
analyze the data in the d data frame. These procedures require the explicit statement
of the name d, such as preceding a variable name with the data frame name and a
$, or use the data parameter to identify the data frame. The standard R functions
make no assumption regarding the data frame name.
2.3.2 Output
quiet parameter,
Section 2.3.2, p. 27
Unless parameter quiet is set to TRUE, Read() displays a summary of the data frame
it just read into R. The first part of the output presents a dictionary of variable types
for the variables that have been read into R, illustrated in Listing 2.1.
Data Types
------------------------------------------------------------
character: Non-numeric data values
integer: Numeric data values, integers only
double: Numeric data values with decimal digits
------------------------------------------------------------
Listing 2.1: Dictionary of types of data read into R.2.3. READ DATA 27
The output from Read() is from an internal call to the brief version of the lessR
function details().
details() function,
lessR: Provide many
details about a data Manually call this function at any time after a data table has table.
been read into R with the specified data frame name, either the full function or db()
for details brief. The default data frame, as usual, is d. Read() then displays the
variable names, as shown in the first named column of Listing 2.2.
variable name:
The reference for a
variable in a data
analysis.
Pay particular attention to the variable names, including the pattern of capitalization,
because it is by its name the variable is referenced in any later data analysis. Also
displayed are the variable types that reference how the data values are stored internally,
the number of data values read, and the number of missing values. The displayed
number of unique data values helps identify categorical variables, which have a
relatively small number of unique values.
Variable Missing Unique
Name Type Values Values Values First and last values
------------------------------------------------------------------------------
1 Years integer 36 1 16 7 NA 7 ... 1 2 10
2 Gender character 37 0 2 M M W ... W W M
3 Dept character 36 1 5 ADMN SALE FINC ...SALE FINC
4 Salary double 37 0 37 53788.26 ... 56508.32 57562.36
5 JobSat character 35 2 3 med low high ... low high
6 Plan integer 37 0 3 1 1 2 ... 2 2 1
7 Pre integer 37 0 27 82 62 90 ... 83 59 80
8 Post integer 37 0 22 92 74 86 ... 90 71 87
Listing 2.2: The Read() summary of the variables read into the data frame d with the
variable names listed in the first column.
To help validate that the data table was read correctly, Read() lists some of the first
and last values of each variable. Match some of these data values against the contents
of the file from which the data were read. Before beginning the analysis, make sure
that you are analyzing the data you intend to analyze.
This console output of Read() is generally useful. quiet parameter:
Suppress text output
from an analysis
procedure to the
console.
However, if this information is not
needed, such as when a data set is re-read at a later time for re-analysis, then the
console output can be suppressed by setting parameter quiet to TRUE.
Input. Suppress console output
d <- Read("", quiet=TRUE)
This parameter applies to all the lessR functions that provide text feedback at the
console. style() function,
Section 4.2.3, p. 65
The value of quiet and other parameters can also be set for the entire R
session with the lessR function style().
2.3.3 Missing Values
R’s Missing Data Code
A data table with missing data values is common in the real world of data analysis.
For example, if you give people a list of items on an attitude survey, some people28 CHAPTER 2. READ AND WRITE DATA
may not answer all of the items. R’s missing data code for numeric data is NA.
NA: R’s code for
missing data, not
available. For
non-numeric data, R displays missing data as <NA>. The NA means “Not Available”.
As shown in Listing 2.3, the data value of Years for the second row of data, the data
for James Wu, is missing.
> head(d, n=2)
Years Gender Dept Salary JobSat Plan Pre Post
Ritchie, Darnell 7 M ADMN 53788.26 med 1 82 92
Wu, James NA M SALE 94494.58 low 1 62 74
Listing 2.3: Missing data of variable Years for James Wu, second line of data, NA.
By default, Read() interprets a data value as missing when it is actually missing, such
as a blank cell in a worksheet. Read()’s variable summary in Listing 2.2 provides the
number of missing data values for each variable under the column Missing Values.
Find the number of non-missing values under the previous column, Values. The sum
of these two numbers for any variable is the total number of rows of data, here 37.
For example, the variable JobSat has two missing data values and 35 recorded values.
case, observation,
Section 1.3.1, p. 14 The full details() report, shown in Listing 2.4, includes a missing data analysis for
each row of data, also referred to as a case, observation, example, instance, or sample.
Add the data frame name to the function call if not d.
Input. More detailed analysis of data with full details() function
details()
Missing Data Analysis
----------------------------------------------------
n.miss Observation
1 Wu, James
2 Jones, Alissa
1 Korhalkar, Jessica
Total number of cells in data table: 296
Total number of cells with the value missing: 4
----------------------------------------------------
Listing 2.4: Number of missing data values for each row of data.
From Listing 2.4, we see that the employee with the most missing data is Alissa
Jones. An advantage of identifying the names of each person as an ID field is that the
identifier for each row is the person’s ID, here their name. Otherwise, the identifier
would be the row number in this output.
row names,
Figure 2.3.4, p. 29
User-defined Missing Data Codes
Some data files contain user-defined missing data codes of data values that would not
naturally occur. Examples include -99 for a numeric variable that only has positive
values, or "XX" for a non-numeric variable.
missing parameter:
Designated data
value to indicate
missing data.
Read the data into R and interpret these
codes to represent missing data. Use the Read() parameter missing to inform R of
the values that define a missing value.2.3. READ DATA 29
Input. Read data with specified character and numeric missing data codes
d <- Read("", missing=c("XX",-99))
c() function,
When presenting a list of multiple values with the values in the list separated by Section 1.2.9, p. 12
commas, use the combine function c() to group the values together. This example
results in every "XX" and every -99 in the data file read into R replaced with an NA in
the resulting R data frame, here d.
2.3.4 Row Names
By default, R numbers each row of the data frame with a row number. Or, R labels
each row of data with a unique, non-numeric label. Refer back to the Employee data
table.
Employee data table,
The names in the first column are not values to be analyzed but rather ID Figure 1.5, p. 13
values that uniquely identify each row. Many R analyses identify data by its row
identifier, such as labeling points in a scatterplot.
The Read() function provides some guidance regarding the implementation of row
names, as shown in Listing 2.5. If Read() detects in the data file a column of
non-numeric data with unique values, that column is noted as a potential ID column.
For the column Name, each row of data is unique. Are these values
a unique ID for each row? To define as a row name, re-read the data file
with the following setting added to your Read() statement: row_names=1
Listing 2.5: A note from Read() suggesting a possible ID column.
There is no necessary reason to read a column of unique values as row names, but
that assignment can be helpful for labeling output from data analysis functions.
Scenario. Read the data and identify a column of data as the row names
One column in the data table of interest consists of unique names, that is, each
name uniquely identifies a specific row. Read the data into R and identify this
column as row names to identify specific data values in the R output.
rows_names
parameter: Column
number of an ID field
in a data table.
The row_names parameter in the call to Read() indicates the column number that
contains the ID’s. The following example applies when the row names in the data file
are in the first column. Listing 2.6 shows the output of the head() function with the
parameter n set to 2 to display just the first two lines of the d data frame.
Input. Specify the row name when reading data
d <- Read("http://lessRstats.com/data/employee.csv", row_names=1)
> head(d, n=2)
Years Gender Dept Salary JobSat Plan Pre Post
Ritchie, Darnell 7 M ADMN 53788.26 med 1 82 92
Wu, James NA M SALE 94494.58 low 1 62 74
Listing 2.6: Initial output of Read(), with the first column now row names.30 CHAPTER 2. READ AND WRITE DATA
Specifying the row names, R now appropriately treats the first column of information
as a row name rather than a variable. One meaningful constraint is that the row
names must be unique for each row of data. If not, the read is not successful. Two or
more rows of data should not be linked to the same ID.
2.4 More Data Formats
Worksheet files, such as from Microsoft Excel or LibreOffice Calc, or csv formatted
files with variable names in the first row, provide a straightforward means of preparing
data for entry into R. However, data can be presented in other file formats as well.
2.4.1 lessR Data
lessR provide functions that simplify R data analysis. lessR also includes data sets so
that data is always available on any computer on which lessR has been downloaded.
Read these data files by specifying the name of data set, such as Employee.
Employee data set,
Section 1.3.1, p. 13
Each lessR data file name begins with the prefix data, followed by the descriptive
name that identifies the file. When referencing the data with Read(), the prefix can
be deleted. Consider the Employee data set dataEmployee.
Input. Read data downloaded with lessR
d <- Read("Employee")
The same format applies to other internal lessR data sets, such as the Machiavellian￾ism data set Mach4, BodyMeas, Cars93, Jackets, Learn, Reading, and StockPrice.
To display information about each data set, request its manual by inserting a ?data
in front of each name.
?dataMach4
help() function:
Section 1.2.7, p. 10 The ? is an abbreviation that calls the R function help().
2.4.2 SPSS, SAS, and Stata Data
Based on the haven package (Wickham et al., 2022), Read() can read data files
written from the statistical packages SPSS, SAS, and Stata in their native format.
.sav, .zsav filetype:
Native SPSS data file.
.sas7bat filetype:
Native SAS data file.
.dta filetype: Native
Stata data file.
If the data file type is one of the defining file types, Read() automatically detects
this attribute and automatically sets the format parameter to "SPSS", "SAS", or
format parameter, "Stata".
Section 2.3.1, p. 25
Or, manually set parameter format to override the data file’s file type.
R factor, Section 3.2,
p. 43
Read() does additional processing when reading SPSS files. Similar to an R factor,
SPSS has a variable type that attaches a value label, a character string label, to a data
value. Consider a Likert response scale where the respondent indicates either Strongly
Disagree, Somewhat Disagree, Somewhat Agree, or Strongly Agree. The response
saved in the data file is an integer from 0 to 3. In the SPSS analysis, the original2.4. MORE DATA FORMATS 31
response category, the value label, is associated with each integer on subsequent
analysis output.
When read into an R data frame with haven, the variable with value labels is converted
to integers, but the labels are not retained. Instead, Read() automatically creates
two variables for each SPSS variable with value labels. The values of the first variable
are the original numeric values. The second variable is the R factor version of the
scale that contains the value labels, named with the original variable adding a _f
suffix.
For example, consider reading an SPSS data set with two four-point Likert items,
Item1 and Item2, each with integer values from 0 to 3, inclusive, and with associated
value labels that provide the meaning of the integer values, as in Listing 2.7.
Input. Read a SPSS .sav data file that includes value labels
d <- Read("")
> head(d, n=2)
Item1 Item1_f Item2 Item2_f
1 2 somewhat agree 3 strongly agree
3 2 somewhat agree 0 strongly disagree
Listing 2.7: First four rows of a data frame read from browsing for an SPSS file with two
variables, Item1 and Item2, that each have value labels.
From the given SPSS file, Read() read the integer variables Item1 and Item2 with
the value labels. The integer variables were retained with the same name. The newly
created R factor variables, Item1_f and Item2_f, preserve the value labels. interval data,
Section 2.2.1, p. 23
ordinal data,
Section 2.2.1, p. 24
Analyze
the integer versions of the variables, such as computing a mean if interpreting the
data as interval quality, or analyze the labels interpreted as ordinal quality, such as
creating a bar chart.
Find example SPSS files at the following web address.
http://lessRstats.com/data/SPSS/
Information easily flows into and out of R in a variety of formats.
2.4.3 Fixed-Width Data
A text file is a universal format, accessible to virtually all computer applications. text file, p. 7
csv data format,
Section 2.3.1, p. 25
A
csv file, where a comma or other specified character delimit adjacent data values, is
a text file, a file of just plain alphabetical characters and the digits, and common
punctuation characters such as a comma. Another kind of text file is a fwd file, the
fixed width format, where the data values for each variable conform to a specified
fixed number of columns in the data file.
fixed width format:
A text data file with
data values for each
variable in specified
columns.
Mach IV data,
Listing 1.5, p. 17
The previously introduced example of a fwd formatted text file is the Mach IV data
set. This file contains 25 digits for each row, the data for a single respondent. These
data values for each row consist of a 4-digit ID, a Gender column, and then 20 digits,32 CHAPTER 2. READ AND WRITE DATA
the responses of each respondent to the 20 items on the Mach IV scale. Unlike a csv
file, no delimiter separates the adjacent data values. There also are no variable names
in the first row of the data table because the names do not fit into the allocated
column or columns for the corresponding data values.
Scenario. Read fixed width formatted data into R
The Mach IV data consists of responses to each of the 20 Mach IV items plus
Gender, stored in a fixed width format, one column per response. Read this data
table into R.
widths parameter:
Specify the widths of
the columns for the
data values in each
row of data.
Specify the widths of the fields for fwd formatted data with the R widths parameter.
As with most of the parameters for standard R read functions, such as read.table(),
they also apply to the lessR function Read(). We also rely upon other functions to
reduce the work needed to accomplish reading the fixed-width Mach IV data.
Consider the R function, rep(), for repetition. rep() function, R:
Create a string of
characters by
repeating a specified
set of characters.
There are 20 columns for each row of
the Mach IV responses, each response of width 1. To read the Mach IV data, define a
vector of 20 1’s to indicate the 20 column widths, a task best accomplished by rep()
with two arguments: the number to be repeated, 1, and the number of repetitions,
20, as shown in Listing 2.8. Processing rep(1,20), R generates 20 1’s as if they were
entered manually, one after the other.
rep(1,20)
[1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
Listing 2.8: Illustration of the R function rep().
to() function, lessR:
Name a sequential
set of variables with
consecutive numbers
prefixed by a given
character string.
A second function to assist reading the item responses on a multi-item scale such as
the Mach IV is the lessR function to(). This function simplifies naming a sequence
of consecutive items. We may name the first variable, or item, m01, the second item
m02, and so forth until m20. The to() function specifies this sequence without needing
to enter the name of each item, as shown in Listing 2.9. To use to(), specify the
prefix of each item within quotes and then the number of items. A third parameter
value, the initial value of the vector, is otherwise assumed to be one.
to("m",20)
[1] "m01" "m02" "m03" "m04" "m05" "m06" "m07" "m08" "m09" "m10" "m11"
[12] "m12" "m13" "m14" "m15" "m16" "m17" "m18" "m19" "m20"
Listing 2.9: Illustration of the lessR function to() to name a sequence of consecutive items.
With a fwd formatted text file, there are no variable names in the first row. How to
specify the variable names? col.names
parameter: Specify
the variable names.
Use the R col.names parameter to name the variables in
the order of occurrence in the data file. As always, when presenting a list of multiple
values with the values in the list separated by commas, use the combine function c()
to group or combine the values together into a vector. c() function,
Section 1.2.9, p. 122.4. MORE DATA FORMATS 33
The first argument listed in this example is the widths parameter. The four column
ID takes up the first four columns, Gender is located in the next column, and then
one column each for the variables, the 20 Mach IV items, m01 to m20.
Input. Read fixed width formatted data with numbered variable names
d <- Read("", widths=c(4,1,rep(1,20)),
col.names=c("ID", "Gender", to("m",20)))
To specify the location of the file directly, such as a web address, list this information
as the first argument passed to Read() within the quotes. A call to Read() always
begins with the reference to the file to be read, empty quotes if browsing for the file.
2.4.4 More Options
Decimal Comma
Another Read() parameter based directly on the options provided by R changes the
character that indicates the decimal digits in a number. decimal separator:
The character in a
numeric string that
indicates where
decimal digits begin.
Most English speaking
countries, plus all of North America and China, use a period for the decimal separator,
called the decimal point. Another tradition, favored by Europe, Russia and all of
South America, uses a comma for the same purpose, and then, perhaps a semi-colon
to delimit adjacent data values instead of a comma.
Read2() function,
lessR: Read data
from a text file with
a comma for a
decimal point and a
semicolon for the
delimiter.
Use the dec parameter to specify
the character that indicates decimal digits, possibly in combination with the sep
parameter, or specify the Read2() function.
d <- Read("", sep=";", dec=",") or d <- Read2("")
When reading data with Read2(), the display of the numbers in the R output still
includes the decimal point as a period. OutDec parameter:
Specify the character
for decimal point on
R output.
To change the display on the output, invoke
the OutDec parameter for the R options function.
options(OutDec=",")
options() function,
R: View and set
system-wide options.
There are many other parameters that can also be set with the options() function.
Reference the help file with ?options to view these options.
Skip Beginning Lines of Data
Sometimes a data file begins with comment lines that describe the purpose of the
data and each of the variables contained in it. To read the actual data, skip the first
specified number of lines in the data file with the skip parameter.
skip parameter:
Skip the specified
number of lines at
the beginning of a
data file.
Input. Skip the first six first lines of an Excel, ODS or text data file
d <- Read("", skip=6)
In this example, the reading of the data begins on Line 7 of the data file, which, for
a worksheet or csv formatted file, usually would be the variable names, with data
values beginning on Line 8.34 CHAPTER 2. READ AND WRITE DATA
2.5 Variable Labels
2.5.1 Definition
To analyze the data values of a variable, refer to the variable by its name, usually
around 10 characters or less. Long variable names are more difficult to type, and
the spelling of a variable name must be exact, including any capitalization, for the
variable to be properly referenced. Shorter variable names simplify this reference to
the corresponding variables. Also, the output is more concise with shorter names.
The names and values of many more variables can be listed in columns across a
computer screen or printed with shorter variable names.
The disadvantage is that a short name may not adequately describe the meaning of
Mach IV scale, the variable. Consider the sixth item on the Mach IV scale.
Table 1.1, p. 16
Honesty is the best policy in all cases.
The variable name, m06, is concise while conveying the item’s position on the Mach IV
scale. The downside is that there is no indication of the item’s content in this name.
A variable label is a longer description that expresses the variable’s meaning more
thoroughly than its shorter name. A suitable variable label for a survey item is the
item’s content.
variable label: A
description of a
variable’s meaning. Reference variables with their shorter variable names in subsequent
function calls, but provide the more descriptive variable labels on the output to assist
interpreting the results.
Unlike standard R functions, lessR data analysis functions provide for variable labels.
Scenario. Read the variable labels into R
Increase the interpretability of the visualizations and text output of each data
analysis procedure by displaying variable labels.
When using lessR functions, the presence of variable labels trigger their automatic
use on the text or visualization output. For example, the label on the horizontal
axis of the histogram for variable m06 is ordinarily the variable name, m06. With a
variable label present, the axis label is the corresponding variable name appended to
the variable label, here the item’s content.
2.5.2 Variable Labels File
As of this writing, lessR analysis functions only recognize variable labels read into
the l data frame, the letter “el” for labels. The label data consists of two columns,
the variable name and the corresponding variable label. The file format can be csv
or Excel. For a csv file, if there is a comma in the variable label, enclose the entire
label within quotes.
The example in Figure 2.1 shows how the Mach IV items and their labels are entered
into Excel, one column for the variable names and one column for the corresponding
variable labels. The organization of the information in this labels file is flexible. There
is no need to list the variables in any particular order, nor do all variables in the
corresponding data file need have a variable label in the label file.2.6. WRITE DATA 35
Figure 2.1: Variable labels for Variables m01 to m04, the first four Mach IV items stored in
an Excel worksheet.
Read the variable labels into the l data frame with the lessR function Read(). Set
the Read() parameter var_labels to TRUE.
var_labels
parameter: Indicates
to read a variable
labels file.
To read the accompanying variable labels,
specify the file’s location as usual, with the first parameter value passed to Read(),
from. Empty quotes as the first parameter value prompts the user to browse the file
directory for the file’s location.
Read variable labels into the l data frame
l <- Read("", var_labels=TRUE)
To read the file directly from the specified location, provide a full path name or web
address (URL) within the quotes as the value for the first parameter value.
2.5.3 Variable Labels with R Functions
If variable labels are present in the l data frame, R functions can also access the labels
with the lessR function label(). The argument of the function is the corresponding
variable name.
label() function,
lessR: Use variable
labels with R
functions.
For example, the parameter to specify the x-axis for an R visualization
is xlab. The function for the R histogram function is hist(). Construct the histogram
for variable m06, the sixth item on the Mach IV scale. Here, the label() function
accesses the corresponding variable label in the data frame for display on the graph
title.
Unlike lessR functions, many R functions cannot reference the variables directly by
their name, also requiring the data frame’s name that contains the data. To use the
R function hist() to display a histogram, precede the variable name with the data
frame’s name that contains the variable and a $ sign, as follows.
hist() function: R
hist(d$m06, breaks=seq(-.5,5.5,1), xlab=label(m06)) histogram.
The variable label defined in the l data frame for variable m06 is displayed as the
label for the horizontal or x-axis of the resulting histogram.
Histogram() function,
In general, however, Section 5.2, p. 83
the lessR histogram function Histogram() provides more pleasing aesthetics, more
information regarding the underlying distribution, improved error diagnostics, and
directly references the variable by name.
2.6 Write Data
The lessR function Read() reads data from an external file into a designated R data
frame. The complementary lessR function Write() writes a designated R data frame36 CHAPTER 2. READ AND WRITE DATA
to an external file in a chosen format: Excel or ODS worksheets, csv text files, or R
or SPSS native app formatted files.
2.6.1 Choose an Output Format
One consideration when deciding on the format in which to write the contents of
a R data frame is the ease with which the data can be read back into R or other
systems for further analysis. Another consideration is the size of the resulting data
file. Different formats can result in significantly different speeds and file sizes.
The native R formatted file precisely mirrors the corresponding internal data frame,
retaining the R native data types. Another advantage of saving the complete contents
of a data frame is that re-reading a previously saved data frame in the form of a
native R data file can be much faster than standard alternatives such as Excel or .csv
files. For small files the time difference is negligible but for large files quite noticeable.
Plus, the saved R data frame format is more compact than an equivalent text file.
For text files, such as .csv files, without prior information R spends a relatively large
amount of time interpreting the data type contained within each column.5 Modern
computers are fast, but the time difference is noticeable, particularly for large data
sets. The larger the file, the greater the benefit from reading a native R data file.
To illustrate the relative speed and size of reading data files in text, Excel, and
native R formats, a large data frame of 100,000 records was constructed, illustrated
in Listing 2.10.
X1 X2 X3 X4 Xcat
1 -0.731469700793799 -0.914745627436465 -1.254735624142831 0.346012751795612 No
2 -0.322977792151102 1.596093486939910 -1.147535547043352 -0.922125823858507 No
3 -1.302878615460409 -0.362561318485318 -2.114138412521319 0.846034172639783 Yes
Listing 2.10: First three rows of a data frame with 100,000 rows of four numerical variables
and one categorical variable.
The data frame contains four numerical variables with 15 significant digits each and
a Yes/No categorical variable. The default number of displayed significant digits was
increased from 7 to 15 with the R function call option(digits=15) to display all the
digits.
The data frame was written in csv, Excel, and native R file formats. The different
formatted files were each successively read 100 times.6
. Table 2.1 shows the median
read times in seconds for each of the three applications of Read() and the respective
file sizes.
5The read time of a text file can be dramatically reduced by specifying the parameter colClasses,
for which you must provide the type of variable in each column. In this example, the median speed
was, 0.086 secs, faster than native R. With colClasses, R does not need to decipher the storage type
for each column. See ?read.csv for more information.
6The tests were conducted on an M1 2020 MacBook Air with 8GB RAM. The function
microbenchmark(), from the package of the same name (Mersmann, 2021), measured the read
times.2.6. WRITE DATA 37
Type Format Seconds MB
Text .csv 0.203 7.8
Excel .xlsx 0.960 5.6
R .rda 0.142 3.1
Table 2.1: Median read speed of three file formats over 100 trials of each Read() function
call, with the file size in MB.
Reading the large Excel data file was 6.75 times longer than reading the equivalent R
file. The sizes of the text and Excel files were, respectively, 2.5 and 1.8 times larger
than the native R file. The native R format is the best among the three alternatives.
When doing data analysis on all but small files, read the data from a text or worksheet
file, then save the data by writing the data frame in R format.
The .rda file also offers the advantage of precisely copying all aspects of the data
frame as it exists in R. After reading the data, it is often worthwhile to perform one or
more transformations of the existing data.
data editing,
Chapter 3, p. 41
factor variable,
Section 3.2, p. 43
For example, create a categorical variable
of type factor. Saving the updated data frame retains this representation of the data
values. Re-reading a text file or worksheet file requires to re-do the transformations.
2.6.2 Write a Data Frame to a File
Data flows into R for analysis. Data flows out of R just as easily.
Scenario. Write the contents of a data frame to a file
Data were read into an R data frame and edited, with several transformations
applied to the data. Write the revised data frame to a file for reading back into R
for future analysis.
Write the contents of the specified data frame to an external file with Write().
File Format Write() function,
lessR: Write
contents of a data
frame to an external
data file.
The first parameter value of Write(), from, is the name of the data frame from which
to write to the output file. The second parameter value, to, is the name of the output
file within quotes. If the filetype is not provided, Write() adds the appropriate
character string to the file name. The third parameter value format specifies the
format of the output file.
format parameter:
Specify the format
for which to write
the file. Input. Write the data frame d to filename MyData with all available formats
Write(d, "MyData", format="csv")
Write(d, "MyData", format="Excel")
Write(d, "MyData", format="ODS")
Write(d, "MyData", format="R")
Write(d, "MyData", format="SPSS")
The value of csv is the default value for format, so no need to include in the function38 CHAPTER 2. READ AND WRITE DATA
call except for clarity. wrt_x() function:
Abbreviation for
writing a data frame
to an Excel file.
wrt_r() function:
Abbreviation for
writing a data frame
to a native R file.
The Excel and R statements have abbreviations to further
simplify their use: wrt_x() and wrt_r(), respectively.
Options for writing an Excel file are parameters ExcelTable and ExcelColWidth.
These parameters are defined by the function from the openxlsx package (Schauberger
& Walker, 2021) used by lessR to do Excel reads and writes. By default, ExcelTable
is FALSE, but if set to TRUE, the Excel file is written as a specialized worksheet, an
Excel table. Setting the ExcelColWidth parameter to FALSE turns off the automatic
setting of column widths. Freed from setting column widths, the Excel file is created
in less time, a noticeable time reduction for large files.
File Location
current working
directory: The
default location of
where R reads and
writes files.
Where does R write the data file? Unless you provide a path name to specify a
directory or folder, the answer is the current working directory. On Windows, the
default location is your Documents folder. On Macintosh and Linux the default
location is the top level of your home folder. If you are working within an RStudio
project, the default location is the folder that contains the project.
The call to Write() in Listing 2.11 writes the contents of the d data frame, the lessR
BodyMeas data, to the file called BodyMeas.xlsx in the designated folder.
Input. Write the data frame d to filename BodyMeas as an Excel file
d <- Read("BodyMeas")
Write(d, "BodyMeas", format="Excel")
The d data values written at the current working directory
BodyMeas.xlsx in: /Users/gerbing/
Listing 2.11: The Write() function for creating an Excel file of the d data frame.
The file type, .xlsx for an Excel file, is automatically appended to the file name
according to the chosen format. Moreover, unlike the R functions for writing data,
Write() returns the path to the newly created file.
Figure 2.2 shows the first six lines of data for the output Excel file from Listing 2.11.
Figure 2.2: First six rows of data for the Excel output file written by Write().
To identify the current working directory to write the output files, use the R function
getwd(), for get working directory. The value returned by the function is the
pathname for the current working directory.
getwd() function, R:
Identify working
directory where files
are written.2.7. ANALYSIS PROBLEMS 39
2.7 Analysis Problems
1. Read data. Consider the data in Figure 2.3, randomly selected from a data file of
the body measurements of thousands of motorcyclists.
Figure 2.3: Gender, Weight and Height of eight motorcyclists.
a. Enter these values into a worksheet and create a csv file of these data. (See
problem #4 for Chapter 1.)
b. Read the data into R.
c. Confirm that the data were read correctly.
d. List the data within R.
e. Write the data to an R native data file.
2. Variable labels.
a. In a worksheet application, construct a variable labels file for the data in
Figure 2.3. For example, include the units of measurement along with the
variable name.
b. Convert the labels file to a csv file.
c. Read the labels file into R.
d. List the labels within R.
W 150 66
W 138 66
M 240 74
M 178 71
W 130 64
M 200
W 140 70
M 220 77
3. Fixed width format. Suppose the data from Figure 2.3 were stored
in the fwd format.
a. Enter the fixed-width data into a worksheet or csv file and then
read into R. (Hint: To read the data, treat the blank space in front
of each number as part of its field width.)
b. Create the data file on your computer system.
c. Read the data and confirm that the data were read correctly.
d. Write a csv version of the data file, without the imputed row names,
1 through 8.
4. Missing data. For the motorcyclist data in Figure 2.3, define a missing numeric
code as -999.
a. Modify the data file to include this missing data code.40 CHAPTER 2. READ AND WRITE DATA
b. Read the data into R and interpret the missing data accordingly.
c. Verify that the data was read correctly by listing the resulting data frame.
5. The following web address (URL) specifies a file in fixed width format with 351
rows of data, the data for the Hunter, Gerbing and Boster (1982) analysis.
http://lessRstats.com/data/Mach4Plus.fwd
The codebook for the data follows.
ID, 4 columns
Gender, 0 for Man, 1 for Woman, 1 column
Mach IV, 20 items, 1 column each
Dogmatism, 20 items, 1 column each
Self-esteem, 10 items, 1 column each
Internal locus of control, 8 items, 1 column each
External locus of control, Powerful others, 8 items, 1 column each
External locus of control, Chance, 8 items, 1 column each
a. Read the data into an R data frame.
b. How many rows of data are in the data file?
c. Is there any missing data? Why or why not?
d. List the variable names and the first 6 rows of the data frame.Chapter 3
Manage Data
3.1 Quick Start
Install R and lessR as explained in Section 1.2. At the beginning of each R session,
access the lessR functions with library("lessR"). From Chapter 2, read your
data into an R data frame such as d with d <- Read("") to browse for the data file.
Or, locate your data file with a path name or web address between the quotes.
A data analysis project proceeds through the steps shown in Figure 3.1.
Read
Data
Gather
Data
Analyze
Data
Manage
Data
Figure 3.1: Sequence of steps in a data analysis project.
This chapter focuses on the third step, managing and editing the data. If you have
data ready for analysis, you can skip this chapter for the moment. However, in the
typical data analysis project, about 3/4 or more of the work prepares the data for
analysis. Data does not always arrive in pristine form, often requiring additional
transformations and re-organization before analysis can begin.
Categorical vs
continuous variables,
Section 2.2.1, p. 22
Data analysis revolves around the distinction between continuous and categorical
variables. After reading the data into any data analysis app, categorical variables,
which have a limited number of unique values, are usually represented as type
character or type integer.
factor, Section 3.2,
p. 43
Before analysis begins with R, categorical variables of
either type should generally be transformed to the data type specifically for categorical
variables, the R factor.
Consider the categorical variable Gender in the d data frame, which is coded as a
type integer variable with values 0, 1, and 2. The following example uses the R
factor() function to attach the name of each Gender category to the corresponding
integer coding. These value labels then display on the analysis output.
d$Gender <- factor(d$Gender, levels=c(0,1,2),
labels=c("Man","Woman","Other")))
The levels parameter refers to the values of the variable as the occur in the data
file, initially as read into R. The labels parameter refers to the value labels attached
to the data values.
4142 CHAPTER 3. MANAGE DATA
By default, R displays the levels of a categorical variable of type character in
alphabetical order. To display an order other than alphabetical, use the factor()
function to inform R of the correct order. Here, convert the variable JobSat from type
character to type factor. Change the ordering of the levels from the alphabetical
ordering that begins with "high" to the natural ordering based on their meaning,
ordinal variable, beginning with "low".
Section 2.2.1, p. 24
In addition to changing the ordering, define the variable as an
ordinal variable with the ordered parameter.
ordered parameter,
Section 3.2.1, p. 44 d$JobSat <- d$JobSat=factor(JobSat,
levels=c("low", "med", "high"), ordered=TRUE))
Another possibility transforms the values of a continuous variable according to an
arithmetic operation. For example, create a new variable in the d data frame called
transform a variable, Salary.root as the square root of the existing values of Salary.
Section 3.3.1, p. 46
d$Salary.root <- sqrt(d$Salary))
Up to around half of the items on a survey may be written so that agreement with
the item expresses disagreement with the underlying measured attitude. Reverse
score these categorical variables coded on a 0 to 5 integer scale with lessR recode().
Scoring of each item is then consistent with the remaining items.
recode() function,
Section 3.4.1, p. 49 d <- recode(m03, old=0:5, new=5:0)
To sort the rows of data in a data frame, use the lessR function sort_by(). In this
example, first, do an ascending sort by Gender, and then within each Gender do a
descending sort according to Salary.
sort_by() function,
Section 3.5, p. 51 d <- sort_by(c(Gender, Salary), direction=c("+", "-"))
Ascending sorts are the default.
A data frame is a two-dimensional object that stores data values. To locate a cell
that contains a data value in a data frame, specify its row and column. Identify and
perhaps edit individual data values with the R function Extract indicated not by
the usual function notation, but opposing square brackets, [, and ]. The general
expression follows for a data frame named d.
Extract: General expression for identifying a data value in data frame d
d[row, col]
Specify the row according to a unique value, its row number or its row name with the
row.name() function. Specify the column either by its position in the data frame or
by the corresponding variable name within quotes. The output of Read() provides
the column numbers as part of its output to facilitate this reference.
The following example identifies the row for James Wu and then sets the number of
Years worked to 10.3.2. CATEGORICAL VARIABLES AS FACTORS 43
d[row.names(d) == "Wu, James", "Years"] <- 10
The same general Extract expression works for subsetting a data frame, pulling
out selected rows and columns. The function is easier to use in conjunction with
the lessR function .(), which simplifies the expressions used to indicate rows and
columns. Again, expressions can be used or enter the row or column numbers directly.
d2 <- d[.(Gender=="M" & Years<10), .(Years, Salary)]
Here create a new data frame d2 with only men who have worked in the company for
less than 10 years, and only retain two variables, the number of years worked and
salary.
The opposite of subsetting merges different data frames into one. The example here
uses the R function merge() to perform an inner join, which retains rows of data with
a common ID in both input data frames. Here merge by the data frame row names,
but any variable common to both data sets would work.
merge() function,
Section 3.8.1, p. 57 d <- merge(dataH1, dataH2, by="row.names")
Append data frames with rbind() if both data frames contain the same variables
and have the same structure.
rbind() function,
vertical,
Section 3.8.3, p. 60
d <- rbind(dataV1, dataV2)
3.2 Categorical Variables as Factors
variable type,
Section 2.2.1, p. 23 The values of a categorical variable are discrete categories called levels.
level: A value of a
categorical variable.
Represent
categories as numeric digits, usually integers, or by alphabetical characters. For
example, represent Gender numerically as 0 for Man, 1 for Woman, and 2 for Other.
Or encode Gender with M, W, and O. Although the mnemonic coding with alphabetical
characters better communicates meaning and prevents mistakes such as computing
the mean of a column of 0’s, 1’s, and 2’s, both representations of categorical variables
are common in data analysis.
The issue is that reading the data into any data analysis app, such as R, requires
more information for the analysis of the categorical variables than the data provides.
Three general questions require answers before data analysis begins.
◦ For integer data values, what value labels provide meaning to the numerical
levels, such as a 1 means Strongly Disagree?
◦ For non-numeric data values, how to order the levels, such as Low, Medium,
and High?
◦ For either non-numeric or integer levels, how to display response categories
that did not occur, such as for a survey item to which no one chose Strongly
Disagree?44 CHAPTER 3. MANAGE DATA
factor: A variable
with values of a
categorical variable
stored as integers
that display as labels.
R defines a variable type that represents categorical variables, the factor, to address
these issues. After reading the data into R, usually convert each categorical variable,
integer or non-numeric, to an R factor. The primary exception to a factor conversion
is when reading data such as street addresses representing unique values for each
row of data. When the values are unique, they can remain as the same variable type
assigned to them when read into R, typically type character.
factor() function, R:
Convert a character
or integer variable
to a factor.
factors() function,
lessR: Convert one
or more character
or integer variables
to factors.
Convert a categorical variable to a factor with the R function factor(). Or, use the
lessR function factors() to convert multiple variables with a single function call. To
use either of these functions, apply one or both main parameters: levels and labels.
levels parameter:
Actual or potential
levels of a categorical
variable as existing in
the data frame listed
in the desired order.
The levels parameter identifies the actual and possibly potential data values in the
data frame, the levels of the categorical variable. Also, specify the desired ordering of
the variables with this parameter.
labels parameter:
Created value labels
for the levels of a
categorical variable.
The labels parameter defines the created value
labels, entered in the exact order as the corresponding levels. Whatever the actual
(or potential) data values, integer or character strings as read, conversion to a factor
variable associates them with customized labels when displaying the analysis output.
3.2.1 Order Levels
When ordering the levels of a categorical variable, distinguish the order displayed on
the output from the ordering fundamental to the definition of the variable.
Order the Levels for Display
Given a set of non-numeric levels, without additional information R or any other data
anlaytic system must arbitrarily order the levels. Alphabetical ordering is the default.
Consider the variable JobSat with values low", "med", and "high". On subsequent
output such as a bar chart, with alphabetical ordering the level "high" precedes "low
with the level "med at the end. Obtain the desired ordering with the following use of
factor() for the variable JobSat from the Employee data table. Specify actual or
potential levels in the data with the levels parameter.
Employee data set,
Section 1.3.1, p. 13
Input. Order levels of a non-numeric variable
d$JobSat <- factor(d$JobSat, levels=c("low", "med", "high"))
In this example, the labels parameter was not used because the existing data values
are accepted. However, labels could still relabel the existing levels, for example,
“med” as “medium” in the subsequent output of data analysis functions.
ordered parameter Define an Ordinal Variable
for factor()
function: Define a
categorical variable
as ordinal.
Ordering the levels with the levels parameter specifies the order in which they are
displayed on output. Setting the factor() function ordered parameter to TRUE goes
further than specifying their presentation order to indicate that the factor variable is
nominal data, ordinal.
Section 2.2.1, p. 23
ordinal data,
Section 2.2.1, p. 24
This more fundamental distinction references two types of categorical data:
the unordered categories of nominal data and the ordered categories of ordinal data.
Although the levels of JobSat are not coded on a numeric scale, they are ordered3.2. CATEGORICAL VARIABLES AS FACTORS 45
because they reflect different locations along a more refined continuum of JobSat.
Order the levels as: low < med < high.
Data analysis routines can use this additional information inherent in ordinal data.
To define the variable as ordinal, specify the ordered=TRUE parameter with the
information provided to factor() or factors().
ordered parameter:
Define ordinal data.
Input. Define the order of an ordinal variable
d$JobSat <- factor(d$JobSat, levels=c("low", "med", "high"),
ordered=TRUE)
A variable’s conceptual definition should align with the structure of the data values
stored in the computer, which should then inform subsequent data analysis functions. BarChart() function,
For example, when presented with an ordinal variable for analysis, the lessR function Section 4.2.1, p. 64
BarChart() shows the proper order of the categories and then displays the bars in a
graded shade of the same hue, from light to dark, to indicate the underlying ordering.
3.2.2 Value Labels
read Mach IV survey
data, Section 1.3.2,
p. 16
Consider the survey data for the Mach IV on a 6-pt Likert scale from Strongly
Disagree (0) to Strongly Agree (5). If willing to interpret the numerical responses
as an interval scale, obtain some analyses of these data (e.g., factor analysis) using
the original numerical version of the responses as read from the data file.
value label:
Descriptive label
attached to a level of
a categorical
variable.
Perform
other analyses such as bar charts with meaningful value labels, plotting the factor
version of the variables. In this situation, leave the original numerical scored items
unchanged with their factor transformations defined as new variables.
factors() function,
lessR: Apply the R
factor() function to
multiple variables.
The lessR factors() function simplifies the creation of factor variables, particularly
for converting a set of multiple variables. Define the set as either a sequence of
consecutive variables in the data frame or a vector of individually specified variables.
The first parameter of the function call is the x parameter, which determines the
names of one or more variables to convert to factors. Save the output back into the
data frame for which the factors are created, here d.
With six categories of responses for the Mach4 data, here separately define the
categories as a single character vector of the six labels, called Cats for categories.
Input. Simultaneously define multiple variables as ordered factors
Cats <- c("Strongly Disagree", "Disagree", "Slightly Disagree",
"Slightly Agree", "Agree", "Strongly Agree")
d <- factors(m01:m20, levels=0:5, labels=Cats, ordered=TRUE)
By default, the factors() parameter new is set to FALSE, which informs factors()
to replace the original integer coded variables with the new factor variables. The
previous function call replaces all 20 integer-scored variables with 20 factors labeled
according to the Responses labels. Set new to TRUE to leave the original variables46 CHAPTER 3. MANAGE DATA
unmodified and instead append the newly created factor variables to the original data
frame. The newly created variables are by default named according to the original
variables with the added suffix _f.
The previous example that defined all 20 integer items on the Mach IV scale as factors
accomplishes this goal. A bar chart of any of the 20 items displays all six possible
categories, regardless if some categories had no responses. This adjustment correctly
identifies all response scale alternatives and compares bar charts for different items
defined with the same size categories.
3.2.3 Add Levels
Suppose no one assessing their attitude on a survey item responds with “Strongly
Disagree”, coded as a 0. In subsequent analysis, such as a bar chart, R can only provide
the frequencies for the data values that do occur without additional information. The
preferred approach informs R of all possible responses for display.
To solve this issue, define a factor for each variable to include all possible responses,
such as items with the same set of Likert scale responses. These factors can define
response alternatives for potential responses that could exist but do not exist in
the data. Visualization of responses for all the items are then of the same response
categories, with 0 frequencies reported for response alternatives with no responses.
3.3 Transform Data
A standard procedure in data analysis transforms the values of a variable into different
values according to a specified formula. A transformation may create a new variable
or overwrite an existing variable. These transformations are the topic of this section.
For example, the values of Salary may be expressed in dollars but for purposes of
display, such as data visualizations, express the values in thousands of dollars. The
value of $64,000 becomes $64. Or, express a variable measured in hours in terms of
minutes. Or, analyze the logarithm of a variable instead of the original measurements.
There are multiple functions for doing transformations, such as base R transform().
The most direct way to define a transformation is to enter the equation that relates
the variables. Identify the data frame that contains each variable by preceding the
variable name with the data frame name followed by a $. R applies the equation to
each row of the data table, equivalent to Excel’s Fill Down formula operation.
3.3.1 Arithmetic Operators
Consider again the Employee data. Employee data set,
Figure 1.5, p. 13
Define a new variable, Months, the number of
months employed at the company, shown in Listing 3.1. Enter the equation that
defines Months from Years directly into the R console, including the data frame
name followed by $ as part of each variable specification. The usual arithmetic
operators apply: +, −, ∗ and /, for addition, subtraction, multiplication, and division,3.3. TRANSFORM DATA 47
respectively. R uses the caret symbol, ^, for exponentiation.
Input. Define a new variable with arithmetic
d$Months <- d$Years*12
> head(d, n=4)
Years Gender Dept Salary JobSat Plan Pre Post Months
Ritchie, Darnell 7 M ADMN 53788.26 med 1 82 92 84
Wu, James NA M SALE 94494.58 low 1 62 74 NA
Downs, Deborah 7 W FINC 57139.90 high 2 90 86 84
Hoang, Binh 15 M SALE 111074.86 low 3 96 97 180
Listing 3.1: Create the transformed variable Months.
The new variable Months is created in the d data frame, positioned as the last variable
in the data frame. If the variable name on the left side of the assignment operator,
<-, has the same name as the variable on the right side, the transformed variable
replaces the existing variable. If the name of the transformed variable is different, a
new variable is created.
3.3.2 Mathematical Functions
Transformations can also be defined from mathematical functions such as standardizing
the values of a variable or calculating the logarithms of the values. Common functions
for transformations are listed in Table 3.1.
operation usage
round to n decimal digits round(x, n)
standard or z-score scale(x)
natural logarithm log(x)
square root sqrt(x)
absolute value abs(x)
standardize scale(x)
cos cosine(x)
sin sin(x)
tan tan(x)
Table 3.1: Some R mathematical functions applied to the variable x.
Consider standardization of a distribution’s values, which yields a distribution with a
specified mean and a standard deviation. standard score or
z-score: Distance
from a data value, yi,
from the mean, my,
in terms of standard
deviations.
Define a standard score as follows.
zi =
yi − my
sy
The mean and standard deviation of the standardized distribution are 0 and 1,
respectively, yielding z-scores. With a common mean and standard deviation, the
transformed variables are more comparable. The standardized distribution of z-scores
expresses each data value in terms of how many standard deviations the data value is
from its own mean.48 CHAPTER 3. MANAGE DATA
Illustrate standardization with the responses to the 20-item Mach IV scale. As shown
in Chapter 15, there are some advantages to considering subsets of Mach IV, such as
an Honesty subscale that consists of Items m06, m07, m09 and m10.
Honesty subscale,
Section 15.7, p. 341 An analyst wishes
to standardize the items before summing them, so that the responses to the different
items are more comparable before the summation.
Standardize with the R function scale().
scale() function, R:
Standardize the
values of a variable.
Instead of entering the transformation
equations directly, the following single call to the R function transform() standardizes
transform() all four variables.
function, R:
Transform the values
of a variable
according to an
arithmetic operation.
Specify the data frame once, as the first parameter value. The
standardized variables were named to begin with the letter z, shown in Listing 3.2.
Input. Standardize four variables: m06, m07, m09, m10
d <- transform(d, z06=scale(m06), z07=scale(m07),
z09=scale(m09), z10=scale(m10))
> head(d, n=4)
Gender m01 m02 ... m19 m20 z06 z07 z09 z10
1 0 0 4 ... 2 4 1.30348450 0.8317862 0.6707109 0.007512965
2 0 0 1 ... 1 0 -0.05013402 0.1528165 -0.1948021 0.007512965
3 1 2 1 ... 0 1 0.62667524 -1.8840925 -1.0603151 0.007512965
4 1 0 5 ... 4 0 0.62667524 0.8317862 0.6707109 -1.750520786
Listing 3.2: Newly created standardized variables.
Approximately 95% of normally distributed values are within two standard deviations
normal curve, of the mean.
Section 6.3, p. 104
Because a z-score indicates how many standard deviations the original
value is from its mean, most z-scores from normal distributions are between −2 and 2.
Distributions of responses on a 0 to 5 scale for each item are not necessarily normal,
but a general rule is that most z-scores look like the values in Listing 3.2. There are
usually few such values larger than 2 or especially 3 and smaller than −2 or −3.
With the z-scores defined, calculate the new scale score on the Honesty subscale as
the average of the four scores for each of the 351 respondents.
d$Honesty = (d$z06 + d$z07 + d$z09 + d$z10) / 4
The transformations only apply to the specified data frame, d. Once the R session
ends, so does the data frame. To access the transformed data for future R sessions,
write the data frame to your computer’s file system as a native R data file, with file
type .rda.
Write() function,
Section 2.6.1, p. 36
Save R code,
Section 1.2.2, p. 7
Input: Write an R data file Mach4New.rda to the file system
wrt_r("Mach4New")
In a subsequent new analysis session, use Read() to locate the new file and read into
R for additional analysis.3.4. RECODE DATA 49
3.4 Recode Data
Transformations change the values of a variable according to a specified formula, such
as standardization or taking the logarithm. These transformations can be applied to
continuous variables with many different values. Categorical variables, or continuous
variables with a small number of possible data values, can have their values changed
by converting each existing value to a new value, what is called a recode.
recode: Map each
value of a categorical
variable into a new
value.
3.4.1 Reverse Score Items
A common recode application relates to computing a scale score over multiple items
from a survey questionnaire. Mach IV scale,
Table 1.1, p. 16 Consider the 20-item Mach IV scale. Some of the
items are written so that an Agree response indicates a Machiavellian attitude, a
pro-attitude item. The 15th item on the scale provides an example.
15. It is wise to flatter important people.
Other items are written so that Agreement with the item indicates the opposite of a
Machiavellian attitude, a reversed-attitude item. An example here is the 3rd Mach IV
item.
3. One should take action only when sure it is morally right.
To encourage the respondent to read and comprehend the meaning of each item, up
to around half of the items on a scale are written such that agreement indicates the
opposite of the attitude of interest. Otherwise, as applied to the Mach IV scale, a
Machiavellian individual would respond Agree or Strongly Agree to each of the 20
items, perhaps then reading each item less critically.
To calculate a total score on the scale, all the item responses for a respondent must be
scored so that summing over them consistently indicates endorsement for the attitude
of interest. reverse score:
Reverse the scoring
of an item so that
high values indicate
Disagreement.
In the case of the Mach IV data, code a reverse-scored item such that a 0
indicates Strongly Agree, whereas for a pro-attitude item a 0 indicates a Strongly
Disagree. Every Strongly Agree response is initially coded as a 5. The reverse
scoring adjusts the coding of the relevant items before the analysis of the items.
Scenario. Reverse score Likert data
The responses to an attitude survey are coded as integers 0 through 5, which
corresponds to the response categories of Strongly Disagree through Strongly
Agree. However, about half of the items are written so that disagreement with the
item indicates agreement with the attitude of interest. Reverse score these items
so that, for example, a response of 0 that corresponds to Strongly Disagree is
transformed to a 5 to indicate Strongly Agree. recode() function,
lessR: Recode the
values of designated
variables. To implement this reverse scoring, use the lessR function recode(). The first
parameter is a list of one or more variables, such as items that are to be recoded.
old parameter: List
of existing values. The old parameter specifies the list of existing values. The new parameter specifies
the corresponding list of recoded values listed in the same order as the values in the
old list.
new parameter List
The optional parameter is new_vars, the name of a newly created variable of new values.50 CHAPTER 3. MANAGE DATA
to contain the recoded values. If omitted, then the recoded values are written over
the original values of the variable of interest.
new_vars
parameter: Name of
the new variable.
If the variable to be recoded is not in
the d data frame, then specify the data frame with the data parameter.
data parameter:
Data frame with the
variable to recode.
In the case of the Mach IV data with responses to a 6-point Likert scale, the recoded
values are written over the original values. The responses here are coded from 0 to
5. As always, for a vector of multiple values with the values in the list separated
by commas, use the combine function c() to combine the values together. Write
this vector as c(0,1,2,3,4,5), or, in the abbreviated form using the equivalent
c() function, expression 0:5.
Section 1.2.9, p. 12
The reverse scoring flips this coding with 5:0, so a 5 goes to a 0, a 4
goes to a 1, and so forth. This recoding for the reversed-attitude item m03 follows.
Input. Reverse score a Likert item
d <- recode(m03, old=0:5, new=5:0)
list of variables, The variable m03 could be replaced with multiple variables to be recoded, a vector.
Section 10.3.2, p. 206
The recode() function lists the first several values of the variable or variables to be
recoded. Next, recode() displays the implemented recode specification, as shown in
Listing 3.3. Also provided are the total number of rows of data to be recoded, and a
note that the values of the original variable are replaced with the new values.
Recoding Specification
----------------------
0 --> 5
1 --> 4
2 --> 3
3 --> 2
4 --> 1
5 --> 0
Number of cases (rows) to recode: 351
Replace existing values of each specified variable,
no value for option: new.var
Listing 3.3: The mapping of existing values to recoded values.
To recode multiple items, list them as a vector individually, or as a range of consecutive
items in the data frame with the colon notation. The complete list of recoded Mach IV
items follows.
d <- recode(c(m03,m04,m06,m07,m09,m10,m11,m14,m16,m17,m19),
old=0:5, new=5:0)
Because Items m09, m10, and m11 occur consecutively in the data frame, this expression
could be shortened by using the : notation to specify the sequence of these three
items, replacing m09,m10,m11 with m09:m11.3.5. SORT DATA 51
3.4.2 Missing Data
Missing data in the data file indicates data values literally not present. missing parameter,
Section 2.3.3, p. 27
Missing data
codes can also be applied. The missing parameter of the Read() function informs
R of missing data codes such as -99, which although physically present in the data,
indicate that the corresponding value is missing. It is also possible to assign these
codes after the data values are read.
After the data have been read, optionally assign new codes to data that have been
previously defined as missing, or, redefine specific existing values as missing. To
define missing data after the data values are read, use recode() with the value of
"missing" for either the old or new specifications. The recode() function can assign
missing data from a given data value or values, or it can assign missing data to a
specific data value or values.
Employee data table,
Figure 1.5, p. 13
For example, returning to the Employee data set, suppose the value of 1 for HealthPlan
indicates a missing data value. If this coding was not specified when the data values
were read, then assign the missing value with recode().
d <- recode(Years, old=1, new="missing")
Or suppose values originally coded as missing now should be assigned some other
code. Here, assign missing values for the variables Years and Salary a value of 99.
d <- recode(c(Years, Salary), old="missing", new=99)
Note that these assignments cannot be done with variables that are factors. Applying
recode() to a factor removes the factor attribute, so that the variable is no longer a
factor. factor function,
Section 3.2, p. 43
Manipulating the values of a factor should be instead accomplished with the
factor() function.
3.5 Sort Data
One way to organize data sorts the rows of the data frame by some criterion based
on the data values.
3.5.1 Sort by Variables
sort_by() function,
lessR: Sort the rows
of a data frame by
specified variables.
To sort the rows of data in the specified data frame, use the lessR function sort_by().
Specify the data frame as the first parameter in the function call. For example, sort
the data in data frame d by Gender.
d <- sort_by(d, Gender)
To sort by multiple criteria, list the multiple variables as a vector in the order of
their sort. When multiple variables are specified, the second variable is sorted within52 CHAPTER 3. MANAGE DATA
each level of the first variable, and so forth. Consider a sort first by Gender and then,
within each level of Gender, sort by Salary. Unless otherwise specified the variables
c() function, are sorted in the default ascending order from smallest to largest values.
Section 1.2.9, p. 12
Because
there is more than one specified variable, that is, a list of variables, use the combine
function, c(), to combine the multiple variables into a single vector, a list of values.
d <- sort_by(d, c(Gender, Salary))
direction
parameter: Specify
the direction of the
sort for a variable.
To sort at least one of the specified variables in descending order use the direction
parameter for each variable to be sorted. To use this option, list the order of the sort
for each variable. A "+" indicates an ascending sort, from smallest to largest values.
A "-" indicates a descending sort, from largest to smallest values. To sort by Gender
in ascending order, and then Salary in descending order within each level of Gender,
enter the following.
d <- sort_by(d, c(Gender, Salary), direction=c("+", "-"))
The output of this sort function call includes Listing 3.4.
Sort Specification
Gender --> ascending
Salary --> descending
Listing 3.4: Sort specification for sorting Gender in ascending order followed by Salary in
descending order within each level of Gender.
Listing 3.5 displays the first rows of sorted data. All four rows are data from women,
and the salaries are listed in descending order beginning with the highest women’s
salary of $112,563.38.
> head(d, n=4)
Years Gender Dept Salary JobSat Plan Pre Post
Correll, Trevon 21 M SALE 134419.23 low 1 97 94
Hoang, Binh 15 M SALE 111074.86 low 3 96 97
Capelle, Adam 24 M ADMN 108138.43 med 2 83 81
Knox, Michael 18 M MKTG 99062.66 med 3 81 84
Listing 3.5: Employee data sorted by Gender in ascending order and then Salary in de￾scending order.
3.5.2 Sort by Other Criteria
In R the row names of a data frame are conceptually distinct from the variables.
Unlike the variables, the row names are not subject to statistical analysis, such as
computing a mean. Instead, their purpose identifies each unique row and to appear
on the output to facilitate interpretation, such as to label individual points in a graph.3.6. SUBSET DATA 53
row.names sort:
Set the by parameter
to row.names.
The sort_by() function provides for sorting by row names, the name of the employees
in the Employee data set. Specify row.names as the criterion by which to sort.
d <- sort_by(d, row.names)
The value of each row name occurs only once. When the unique row names are sorted,
then sorting by values of the variables within each row makes no sense. If row.names
is specified as the sort criterion then no variables are specified. The direction of the
sort, however, can be specified. The default is ascending. If a descending sort by row
names is desired, include the direction="-" parameter setting.
Input. Sort the data frame by row names in descending order
d <- sort_by(row.names, direction="-")
The sort_by() function also has a value for the by parameter to randomly shuffle
the rows of data. To do so, specify random as the criterion for the sort. random sort: Set
the by parameter to
random. d <- sort_by(d, random)
This parameter is useful if the data have been previously sorted by some criterion
that is no longer relevant.
3.6 Subset Data
3.6.1 Select Rows and/or Columns
An analysis may include only part of the original data table. The lessR data analysis
functions have a parameter rows that selects only some of the rows of data in the
data frame for analysis, such as only the data for Women. Or, maybe the goal is to
identify and list some subset of the data frame, the procedure discussed here.
Extract operator:
for data frame d,
d[rows,cols].
The most general subsetting of a data frame by rows and/or columns is with the
base R Extract[] function, indicated by the data frame name preceding matched
square brackets instead of the usual matched parentheses. First indicate the rows to
be selected, then the columns.
Input. General format of the Extract operator for the d data frame with .()
d[.(rows), .(columns)]
An additional option in the preceding function call is the lessR function .(), which
wraps each expression for the rows and cols. With .(), for the row specification,
the variable names do not need to be preceded with the name of the data frame. For
the columns, quotes are not required around the variable names, and data ranges can
be specified with a colon, :.54 CHAPTER 3. MANAGE DATA
To specify the columns, enter either the column indices of the selected columns or a
list of variable names. To specify the rows, enter either:
◦ the row names with the row.names() function of the selected rows, or the row
indices such as 1, 2, etc.
◦ a logical statement that identifies the selected rows, using expressions from
Table 3.2.
operator meaning operator meaning operator meaning
== is equal to > is greater than & and
!= is not equal to < is less than | or
Table 3.2: Logical operators.
For this example, choose the rows of the d data frame for which Gender is equal to
"W" with a Post test score greater than 91. Choose the variables in the data frame
starting with Years through Salary, and the Post test score. c() function,
Section 1.2.9, p. 12
Listing 3.6 presents this
subsetting of rows with a logical expression and selecting specified columns.
d[.(Gender=="W" & Post>91), .(Years:Salary, Post)]
Years Gender Dept Salary Post
Afshari, Anbar 6 W ADMN 69441.93 100
Kimball, Claire 8 W MKTG 61356.69 92
Saechao, Suzanne 8 W SALE 55545.25 100
Listing 3.6: Data for selected variables only for women with Post scores greater than 91.
The : in the previous expression indicates a variable range, starting with the variable
Years and proceeding through the data frame until the variable Salary is reached.
Instead of listing the output at the console, assign the output to a new data frame,
with a name of your choosing, such as s in the following example.
s <- d[.(Gender=="W" & Post>91), .(Years:Salary, Post)
details() function,
Section 2.3.2, p. 27
Specifying column indices requires less typing than entering variable names. The
output of lessR Read(), also obtained with the details_brief() or db() function,
lists the characteristics of the variables including each variable’s column number in
the specified data frame. Viewing the output of Read() for the employee data reveals
that the Years occupies the first column, Salary in the fourth column, and Post in
the eighth column. Read employee data,
Section 2.2, p. 27 Rewrite the previous subsetting statements with column indices
to obtain the same results as in Listing 3.6.
d[.(Gender=="W" & Post>91), .(1:4,8)]
If subsetting columns but retaining all the rows, then do not specify a value for the
rows, but instead provide no row specification, indicated with an empty comma.3.6. SUBSET DATA 55
Listing 3.7 presents an example of including all rows of data in the subset output,
only subsetting columns, but displays only 3 of the 37 rows.
d[, .(Gender:Salary)]
Gender Dept Salary
Ritchie, Darnell M ADMN 53788.26
Wu, James M SALE 94494.58
...
Cassinelli, Anastis M FINC 57562.36
Listing 3.7: Call to Extract[] with an empty first parameter value, for the rows, and an
excerpt of output.
A row of data can also be located with either the R row.names() function or the
value of a variable with a unique value for each row.
row.names()
function: Identify a
single row of the
data frame.
Here use row.names() to locate
the data for employee Scott Fulton. Specify the name of the data frame, such as d, in
the call to row.names(). If the subset retains all the columns and just subsets rows,
then omit the column specification but still retain the comma to indicate the two
dimensions of the data frame. Listing 3.8 displays the row of data for Scott Fulton.
d[.(row.names(d)=="Fulton, Scott"), ]
Years Gender Dept Salary JobSat Plan Pre Post
Fulton, Scott 13 M SALE 87785.51 low 1 72 73
Listing 3.8: Locate a row of data based on the row ID, in this case the employee’s name.
A row selection is a logical statement. To negate a row selection, to take all rows that
do not satisfy the specified condition, add a ! to the beginning of the statement. To
exclude the specified variables, place a -, in front of the column reference.
.(!(Gender=="W" & Post>91)), -.(Years:Salary, Post)]
For negation with Extract, the ! goes within the .(). The - goes before the .().
3.6.2 Randomly Select Rows
Random Selection
To randomly subset rows of a data frame, specify random() either with an integer
value to indicate the number of rows to retain, or as a proportion to indicate the
proportion of rows to retain.
random() function,
lessR: Called within
.() to specify the
number or
proportion of
randomly selected
rows to retain.
Listing 3.9 shows one row of randomly selected data.
d[.(random(1)),]
Years Gender Dept Salary JobSat Plan Pre Post
Gvakharia, Kimberly 3 W SALE 49868.68 med 2 83 79
Listing 3.9: Randomly select one row of data from the d data frame.56 CHAPTER 3. MANAGE DATA
Training and Testing Data
training/testing data,
Section 12.5.3, p. 262
Discussed in detail later, an important aspect of analysis of reasonably large data sets
is to divide the data set into two subsets. The larger subset, usually about 75% of
the data, is used to train a predictive machine learning model. The remaining subset
of the data is used to test the model. The lessR function Regression() provides
for this separation if requested. The lessR function train_test() accomplishes this
separation apart of any one analysis.
train_test()
function, lessR:
Split a data frame
into training and
testing components. The first, and only needed, parameter value passed to the function is the name of the
data frame. To access the created data for later analysis, save the output to an R
object, which is an R data structure called a list. In this example, save the output
to the list called out.
d <- Read("Employee")
out <- train_test(d)
> names(out)
[1] "train" "test"
Listing 3.10: Randomly select 75% of the d data frame for the training data, train, and
the remaining 25% for the testing data, test.
Refer to each element of the list with the name of the output object, the list
followed by a $ followed by the object name. The analysis in Listing 3.10 generated
p_train parameter: two components, train and test, referenced by out$train and out$test.
Specify the
percentage of the
data as training data.
The
parameter p_train specifies the proportion of data in the training data set, with a
default value of 0.75.
response parameter:
Specify the response
variable to create the
training and testing
data.
Or, also separate the data for the response variable, the variable for which to predict
its values, from the remaining data. To do that separation, specify a variable name
for parameter response. The result is four created components: train_x, train_y,
test_x, and test_y.
3.7 Revise Data
3.7.1 Change an Individual Data Value
On occasion, after the data have been gathered and read into R, individual data
values need revision. For example, several of the values may have been incorrectly
entered. Revise the data for subsequent analysis.
Unfortunately, the lessR .() function with the Extract operator cannot appear on
the left side of an assignment statement. Instead, use Extract without the assistance
of .(), so the variable name needs to be included in quotes. And any reference to
variable names in the specification of the row, of which there are none in this example,
needs to be preceded by the name of the data frame and a $. See the example in
Listing 3.11.3.8. MERGE DATA 57
d[row.names(d)=="Fulton, Scott", "Salary"] <- 100000
> d[row.names(d)=="Fulton, Scott",]
Years Gender Dept Salary JobSat Plan Pre Post
Fulton, Scott 13 M SALE 100000 low 1 72 73
Listing 3.11: Change Scott Fulton’s Salary to $100,000.
If the row number and column number of the data cell to be modified are known, they
can referred to directly in the reference to Extract. For example, because Salary is
the fourth variable in the d data frame, the reference to "Salary" in the previous
expression could be replaced by a 4.
3.7.2 Change a Variable Name
rename() function,
lessR: Rename one
or more variables.
The lessR function rename() easily allows changing the name of a variable. To use
the function, list the name of the data frame, the existing variable name, and the
new name, in that order. Listing 3.12 presents an example.
d <- rename(d, Salary, AnnualSalary)
> names(d)
[1] "Years" "Gender" "Dept" "AnnualSalary" "JobSat" "Plan"
[7] "Pre" "Post"
Listing 3.12: Change the name of variable Salary to AnnualSalary in the d data frame. names() function, R:
List the names of the
variables in the
specified data frame.
The R function names() lists the names of the specified data frame, here d.
3.8 Merge Data merge: Combine
two data frames into
a single data frame. A merge of two data sets combines both data frames into one. The two data frames
generally contain different variables, but usually for the same people or whatever the
unit of analysis. The two data frames share an ID column, a common variable or row
identifier that uniquely identifies each row of data, the primary key.
primary key:
Column of data
values in the data
table that uniquely
identifies each row.
3.8.1 Inner Join
Employee data set,
Section 1.3.1, p. 13
Suppose that the Employee data set exists as two different data frames. One source
of employee data provides the data values for the first four variables: Years, Gender,
Dept and Salary. The second source provides the values for JobSat and Plan. Merge
these data sets into a single data frame for subsequent analysis. Because the Employee
data set is a single, unified data table, for purposes of the example, subset into two
separate data frames. To keep the amount of data manageable for this example, only
data from the first several rows of the Employee data set are included.
The two created data frames, Emp1a and Emp1b, are shown in Listing 3.13. One
employee, Alissa Jones, is only in Emp1a, and Darnell Ritchie is only in Emp1b.58 CHAPTER 3. MANAGE DATA
> Emp1a <- d[2:5, .(Years:Salary)] > Emp1b <- d[1:4, .(JobSat, Plan)]
> Emp1a > Emp1b
Years Gender Dept Salary JobSat Plan
Wu, James NA M SALE 94494.58 Ritchie, Darnell med 1
Downs, Deborah 7 W FINC 57139.90 Wu, James low 1
Hoang, Binh 15 M SALE 111074.86 Downs, Deborah high 2
Jones, Alissa 5 W <NA> 53772.58 Hoang, Binh low 3
Listing 3.13: Two data frames from the Employee data set with three employees in common.
merge() function, R:
Merge two data
frames into a single
data frame.
The R function merge() merges the data frames Emp1a and Emp1b to create the
primary data frame of interest. The first two parameters are x and y, the two input
data frames to merge. by parameter:
Specify the value of
parameter by by
which to join two
data frames.
The third parameter, by, provides the name of the ID column,
the primary key, with unique values for each row shared across the data sets. The
primary key could be row names or a variable with unique values. If the primary
key fields in the two data frames share common values but the variables are named
differently, specify two different parameters, by.x and by.y.
inner join merge:
Retain only those
rows of data for
which the primary
key matches in both
tables.
This first example of a merge is an inner join based on each employee’s name. To
appear in the inner joined merged data frame, the employee must have data in both
tables. Three employees have the same row name, their name, in the two input data
frames and so appear in the merged data set. On the contrary, data for neither Alissa
Jones nor Darnell Ritchie appear in the merged data frame in Listing 3.14.
d <- merge(Emp1a, Emp1b, by="row.names")
> d
Row.names Years Gender Dept Salary JobSat Plan
1 Downs, Deborah 7 W FINC 57139.90 high 2
2 Hoang, Binh 15 M SALE 111074.86 low 3
3 Wu, James NA M SALE 94494.58 low 1
Listing 3.14: The inner-joined merged data frame.
In the example in Listing 3.14, the primary key is the column of row names, specified
with "row.names" as the value of the by parameter. The merged data frame is saved
into the d data frame, available for further analysis.
When creating the merged data frame, instead of returning a data frame with the
same structure as the input data frames, merge() created a new variable named
Row.names in place of actual row names. If desired, move the values for Row.names
back to actual row names. First, use row.names() to set the row names of the merged
data frame, then subset the data frame by retaining all the rows but removing the
Row.names variable, as shown in Listing 3.15.
subset removing
columns,
Section 3.6.1, p. 55
3.8.2 Outer and Full Joins
The inner join merge only includes rows of data with a shared primary key in both
input data sets. Other possibilities for merging include the option of retaining at least
some data records from one or both of the input data frames when there a records
with primary key values unique to only one of the input data frames.3.8. MERGE DATA 59
row.names(d) <- d$Row.names
d <- d[, -.(Row.names)]
> d
Years Gender Dept Salary JobSat Plan
Downs, Deborah 7 W FINC 57139.90 high 2
Hoang, Binh 15 M SALE 111074.86 low 3
Wu, James NA M SALE 94494.58 low 1
Listing 3.15: Merged data frame with row names re-established.
left outer join
merge: All rows of
data from the first
data table, x, are
retained and only
rows from the second
data table, y, are
retained for which
the primary key
matches in both
tables.
The left-outer join retains all the rows of data from the first entered data frame, the
value of the x parameter, but requires a match of the primary key from the second
entered data frame, the value of the y parameter. If the primary key for a row of
data from the y data frame does not match a primary key value from the x data
frame, then that row of data is not retained in the merged data frame.
Listing 3.16 illustrates the left-outer join. To specify to retain all rows of data from
the x data frame, the first entered data frame, set the parameter all.x to TRUE.
merge(Emp1a, Emp1b, by="row.names", all.x=TRUE)
Row.names Years Gender Dept Salary JobSat Plan
1 Downs, Deborah 7 W FINC 57139.90 high 2
2 Hoang, Binh 15 M SALE 111074.86 low 3
3 Jones, Alissa 5 W <NA> 53772.58 <NA> NA
4 Wu, James NA M SALE 94494.58 low 1
Listing 3.16: The left-outer joined merged data frame.
Alissa Jones was represented in the x data frame, Emp1a, but not the second entered
data frame, the y data frame, Emp1b. Accordingly, in the merged data frame, Alissa
Jones has missing data for the variables from the y data frame, JobSat and Plan. right-outer join
merge: All rows of
data from the second
data table, y, are
retained and only
rows from the first
data table, x, are
retained for which
the primary key
matches in both
tables.
Another merge possibility mirrors the logic of the left-outer join. The right-outer
join retains all rows of data from the y data frame in the merge. Accomplish the
right-outer join by setting parameter all.y to TRUE. The only retained records from
the x data frame contain a primary key value that matches a corresponding value in
the y data frame.
full join: All rows of
data are retained
from both input data
frames.
The full join retains all rows of data from both input data frames, including all
rows of data in which an employee is in one input data frame but not the other. If
there is not a matching value of the primary key in the other data frame, then the
values of the corresponding variables are necessarily set to missing. Referring back
to Listing 3.13, Darnell Ritchie is not in the x data frame, Emp1a, and so will have
missing data for the variables in the merged data frame: Years, Gender, Dept, and
Salary. Similarly, Alissa Jones is not in the y data frame, Emp1b, and so will have
missing data on the corresponding variables JobSat and Plan.60 CHAPTER 3. MANAGE DATA
all parameter: Listing 3.17 illustrates the full join merge with five rows of merged data.
Specify a full join of
two input data
frames, including all
data.
To accomplish,
either set both all.x and all.y to TRUE or set the single parameter all to TRUE.
merge(Emp1a, Emp1b, by="row.names", all=TRUE)
Row.names Years Gender Dept Salary JobSat Plan
1 Downs, Deborah 7 W FINC 57139.90 high 2
2 Hoang, Binh 15 M SALE 111074.86 low 3
3 Jones, Alissa 5 W <NA> 53772.58 <NA> NA
4 Ritchie, Darnell NA <NA> <NA> NA med 1
5 Wu, James NA M SALE 94494.58 low 1
Listing 3.17: The full joined merged data frame.
Five unique primary key values across the two input data frames shows in Listing 3.13
results in five rows of retained data in the full join merge.
3.8.3 Add Rows to a Data Frame
Another type of merge adds rows of data to an existing data frame. Suppose the
data frame d with three rows of data from Listing 3.15 is the data frame of interest.
Suppose that additional data has been entered into a worksheet such as with Excel
and now should be appended to the existing data frame. In practice, read this
additional data into R. However, for this example create a data frame by arbitrarily
selecting the 11th row of data from the Employee data frame, shown in Listing 3.18
dNew <- d[11, .(Years:Plan)]
dNew
Years Gender Dept Salary JobSat Plan
Saechao, Suzanne 8 W SALE 55545.25 med 1
Listing 3.18: A created new data frame.
rbind() function, R:
Vertically merge two
data frames.
The new data, for Suzanne Saechao, resides in the dNew data frame, and has the
same structure and variables as the d data frame in Listing 3.15. To append to the d
data frame, use the R function rbind() for row bind. List the primary reference data
frame first, followed by the data frame to be appended. Listing 3.19 illustrates.
rbind(d, dNew)
Years Gender Dept Salary JobSat Plan
Downs, Deborah 7 W FINC 57139.90 high 2
Hoang, Binh 15 M SALE 111074.86 low 3
Wu, James NA M SALE 94494.58 low 1
Saechao, Suzanne 8 W SALE 55545.25 med 1
Listing 3.19: Add the data for Suzanne Saechao to the original merged data frame with
three rows of data.
R also provides an analogous function called cbind() for appending columns to a
data frame without a common ID field.
cbind() function, R:
Horizontally merge
two data frames.3.9. ANALYSIS PROBLEMS 61
3.9 Analysis Problems
1. The three values of HealthPlan coded in the data file – 1, 2 and 3 – correspond to
three health plans, respectively named GoodHealth, GetWell and BestCare.
a. Is HealthPlan a continuous or categorical variable? Why?
b. How is HealthPlan stored within the R data frame?
c. Provide the R statement that transforms HealthPlan to a factor.
The Cars93 data set contains much information on 93 1993 car models. One variable
is Source with two values, 0 for a foreign car and 1 for a car manufactured in the
USA.
?dataCars93 for
d <- Read("Cars93") more information.
2. The variable Type is stored in the data file with non-numeric values.
a. How is the data stored within the corresponding R data frame when the data
file is read into R?
b. Order the values of Type appropriately.
3. In the data file the variable Airbag is integer coded with values of 0, 1 and 2, which
correspond to "none", "driver", "driver+". The meaning of "driver" is driver
only, and "driver+" means driver and passenger air bags, so there is an ordered
progression across the three levels of Airbag.
a. What is the formal name for this type of variable?
b. Create the appropriate representation of these data in the R data frame.
4. Examine the values of horsepower, HP.
a. Sort the data by HP.
b. List the 10 most powerful cars in terms of horsepower.
5. The various modifications of the Cars93 data set in the previous problems prepare
the data for subsequent analysis, but the changes are only temporary to the R session
in which the changes are made. Chapter 2 presents two strategies for making these
changes available in a subsequent R session.
a. Discuss one strategy.
b. Discuss another strategy.Chapter 4
Categorical Variables
4.1 Quick Start
Install R and lessR as explained in Section 1.2. At the beginning of each R session,
access the lessR functions with library("lessR"). From Chapter 2, read your
data into an R data frame such as d with d <- Read("") to browse for the data file.
Or, locate your data file with a path name or web address between the quotes.
Some basic analyses are among the first analyses performed after reading the data:
variable types, Describe the distribution of values for one or more categorical variables.
Section 2.2, p. 22
The functions
described in this chapter process data from variables with relatively few unique values,
regardless of whether the variable is read as type character, integer, double, or
some other data type. Ideally, define each categorical variable in the data frame as
an R factor with the factor() function before analysis begins, though it is usually
possible though not optimal, to proceed without doing so.
factor() function,
Section 3.2, p. 43
Several lessR functions describe the sample values of a categorical variable, here
illustrated for a variable named Y.
Bar chart, p. 64
Pie chart, p. 65
Summary statistics,
p. 64
BarChart(Y)
PieChart(Y)
pivot()
BarChart(Y) and PieChart(Y) visualize the distribution of a categorical variable
and provide the related statistics. pivot() describes the values of any variable,
continuous or categorical, potentially across groups defined by categorical variables.
All variables in the data frame can be analyzed one at a time with a single function
call. As with all lessR functions that process data, the default data frame name
is d. If the argument to the function is blank, BarChart() analyzes all categorical
variables in the d data frame.
BarChart()
The resulting bar charts are written to their respective pdf files, the names of which
the standard console output provides.
CountAll(), abbreviated ca(), does the complete summary analysis selectively
624.2. ONE CATEGORICAL VARIABLE 63
invoking either BarChart() for a categorical variable or Histogram() for a continuous
variable, and also provides the corresponding summary statistics from pivot().
CountAll()
function: A bar chart
or histogram, with
summary statistics,
for each variable in a
data frame.
CountAll()
For a data frame different from d, enter the name of the data frame as the first
parameter value.
stat parameter for
analyzing statistics
such as means across
the groups,
Section 4.5, p. 68
The BarChart() function is not limited to analyzing the count of each value of a
categorical variable. Summary statistics of a numerical variable can also be analyzed
across the groups. Provide a numerical variable Y as the second parameter value.
Then include the stat parameter set at the value of the desired statistic, such as
"mean".
BarChart(X, Y, stat="mean")
The BarChart() function for two categorical variables X and Y delivers the cor￾responding bar chart. It also provides the corresponding joint frequencies, the
cross-tabulation table, which is the numerical basis of the bar chart. In addition, the
associated chi-square test of independence of the two variables follows the sample
cross-tabulation.
bar chart for two
variables, Section 4.3,
p. 73 BarChart(X, by=Y))
100% stacked bar
chart, Section 4.3.2,
p. 75
The 100% stacked bar chart is also available with the stack100 parameter.
For categorical variables X and Y, a call to pivot() yields additional summary
statistics according to the various ways of calculating the cell proportions.
Two variable
summary of
categorical variables,
Section 4.3.3, p. 77
pivot(d, mean, X, by=Y)
4.2 One Categorical Variable BarChart()
function, lessR:
Create a bar chart
and summary
statistics.
PieChart()
function, lessR:
Create a pie chart
and summary
statistics.
The lessR functions for the visualization and analysis of a categorical variable are
BarChart() and PieChart(), with respective abbreviations of bc() and pc().
Employee data table,
Figure 1.5, p. 13
Here, illustrate the BarChart() and PieChart() functions with the Employee data.
d <- Read("Employee")
This data set contains both numerical variables, integers and decimal numbers, and
categorical variables, some coded as integers and some as character strings.64 CHAPTER 4. CATEGORICAL VARIABLES
4.2.1 Bar Chart
Scenario. How many employees work in each department?
Record the department in which an employee works as the values of the variable
Dept, composed of five categories such as acct for Accounting and sale for Sales.
Count the number in each department.
This book is printed in grayscale, so all visualizations are grayscale, obtained with
the lessR function style() with the first parameter value, theme, set to "gray".
This setting is shown here but implied throughout the rest of the book. The default
theme is "colors", which displays the bars in different hues all at the same level of
brightness. The input and output from the call to BarChart() appears in Figure 4.1.
style("gray")
BarChart(Dept)
Figure 4.1: Default grayscale bar chart for a single categorical variable.
variable labels,
Section 2.5, p. 34 Because the variable labels were already read into R, the variable label for Dept by
default appears as the label for the x-axis. main: Plot title.
xlab: x-axis label.
ylab: y-axis label.
As is true of standard R visualizations,
including for lessR functions, add a title with the main parameter, such as main="My
Title". Customize the x-axis (horizontal) label with parameter xlab, and the y-axis
label (vertical) with parameter ylab.
The statistics from BarChart() in Listing 4.1 include the frequencies for each category,
here the five categories of Dept. Also provided are the overall sample size and the
corresponding proportions. By default, R lists the order of the categories alphabetically.
Change this ordering with the R factor() function. factor() function,
Section 3.2.1, p. 44
--- Dept, Department Employed ---
ACCT ADMN FINC MKTG SALE Total
Frequencies: 5 6 4 6 15 36
Proportions: 0.139 0.167 0.111 0.167 0.417 1.000
Listing 4.1: Summary statistics from BarChart().4.2. ONE CATEGORICAL VARIABLE 65
Interpretation. One categorical variable bar chart
Far more people work in the Sales department, 15, than in any other of the four
remaining departments. The corresponding sample proportion is 0.417, so 41.7% of
all the employees in this sample work in Sales.
4.2.2 Pie Chart pie chart:
Frequency plot of
categorical variable
data values according
to the size of the
slices of a pie.
The pie chart is an alternative to the one-variable bar chart. Each pie slice reflects
a frequency of occurrence, analogous to a bar in a bar chart. The lessR function
PieChart() by default generates a ring chart, a type of pie chart with no hole in the
middle. Figure 4.2 shows an example of the default ring chart. ring chart or
doughnut chart:
Pie chart with a hole
in the middle.
bar chart of the same
data, Figure 4.1,
p. 64
PieChart(Dept)
Figure 4.2: Default grayscale pie chart in the form of a ring chart for categorical variable
Dept.
To create a traditional pie chart with no hole in the middle, set the parameter hole=0.
The values of hole range from 0 (no hole) to up to 1 (narrow ring).
Experiment interactively with various parameter settings of the pie chart for your
data, including the size of the hole, with the lessR function interact().
interact(),
Section 9.2, p. 172
Input. Interactive experimentation with colors and other parameters
interact("PieChart")
When finished interactively designing your pie chart, save the created code and
reapply in the future as needed.
4.2.3 Customization
style() function,
lessR: Customize
subsequent lessR
data visualizations.
Customize the bar chart one of two ways. The lessR function style() provides
control over almost every property of a visualization, including resetting the overall66 CHAPTER 4. CATEGORICAL VARIABLES
theme for subsequent visualizations. To reset the theme, pass a theme name as
the first parameter value in quotes. Some of the many possibilities include "gray",
"darkred", and "dodgerblue". Setting the sub_theme parameter to "black" creates
each subsequent visualization with a black background.
lessR provides the following two functions to obtain more information about cus￾tomizing the visualizations.
Input. Show all available color themes and all other modifiable parameters
style(show=TRUE)
showColors()
function, lessR:
Show all R named
colors as a pdf file.
Input. Create a pdf file that shows all available named colors
showColors()
The following three parameters provide more possibilities for customizing a single
visualization.
◦ fill: Color of the interior of the object, such as bars or pie slices
◦ color: Color of the edges of the object as viewed from the exterior
◦ trans: Transparency level of the interior color, from 0 (none) to 1 (transparent)
c() function:
Section 1.2.9, p. 12 To obtain multiple colors, specify a list of the colors as a vector with the c() function.
Then set the vector as the value of the fill parameter. Two colors provide alternating
colored bars. Generally choose pleasing color combinations such as "seagreen" and
"tan" for the respective alternating fill colors of the bars.
In this example, choose two alternating shades of gray on a scale from 0 for black and
100 for white. Also, choose "black" for the borders with the color parameter, a
transparency level of 0.5 with trans, and display horizontally with horiz, producing
the bar chart of Dept in Figure 4.3.
BarChart(Dept, horiz=TRUE, fill=c("gray30", "gray70"),
color="black", trans=0.5)
Figure 4.3: Horizontal bar chart with custom interior and exterior colors.4.2. ONE CATEGORICAL VARIABLE 67
Find more examples in the Bar Chart vignette. Experiment interactively with
different parameter settings of the bar chart for your data with the lessR function
interact().
browse vignettes,
Section 1.3, p. 10
interact(),
Section 9.2, p. 172
Input. Interactive experimentation with colors and other parameters
interact("BarChart")
Various parameters are listed for which you can adjust different values to analyze your
data. Analyze your data from your desired parameter values. getColors()
function, lessR:
Create color palettes
for data visualization
functions.
Options to customize
visualizations include sequential color ranges such as "reds", "blues" or "emeralds",
implicitly made available with the lessR function getColors(), and available from
within interact(). When finished interactively designing your bar chart, save the
created code and reapply in the future as needed.
To explicitly customize colors with getColors(), adjust the h, c, and l parameters
for, hue, chroma (saturation) and luminance (brightness), respectively. Specify the
colors such as for the fill and color parameters for the visualization functions.
browseVignettes(),
Section 1.2.7, p. 9
For more detailed explanation of lessR visualization parameters, see the Customize
vignette. For a general discussion of visualization with R using other packages, such
as the widely-used ggplot2 (Wickham, 2016, 2022) package, see Gerbing (2020).
4.2.4 Bar Chart from the Summary Table
A bar chart plots numerical values against a set of categories. The height of each bar
for a category is proportional to the corresponding numerical variable. Any bar chart
function from any data analysis app constructs the bar chart from this summary data
table of categories and associated numerical values.
summary data
table: Summary
table that pairs each
category with a
numerical value, one
row for each unique
category.
By default, BarChart() constructs the summary table with the numerical variable
defined as the count of occurrence of each category in the data. However, these counts
are just one of many possibilities for the numerical variable. BarChart() can also
read this summary table directly. This flexibility allows converting any such table to
a bar chart, for any set of categories and any numerical variable.
Scenario. Generate a bar chart directly from the counts
The frequencies of occurrence of each value of the categorical variable Dept are
available, but not the raw data from which the counts were obtained. Enter these
counts into R and then generate the bar chart.
Figure 4.4: Worksheet
data.
Enter the table of categories and counts into an Excel file in
this example, shown in Figure 4.4 with two variables, Dept
and Count. Directly read the file’s contents into R, such
as with the Read() function, usually into the d data frame.
To inform BarChart() that the values read are a table
of categories and numerical values such as counts, invoke
the y parameter, the second parameter in the definition of
BarChart(), the name of the numerical variable.68 CHAPTER 4. CATEGORICAL VARIABLES
Input. Bar chart from the summary table of categories and numbers
BarChart(Dept, Count)
This call to BarChart() generates the identical bar chart in Figure 4.1. Both bar
charts are created from the same table of categories and numeric values. In the first
case, BarChart() computed the count for each category, and in the latter case, the
table of counts was previously computed, which BarChart() then read directly.
4.2.5 Bar Chart of Deviation Scores
Passing only the names of a single categorical variable and a numerical variable signals
to BarChart() that the input data frame is a summary data table with the name of
each category associated with its corresponding numerical value. Add the parameter
stat to instead process the original data table of measurements according to one
of various statistics. Examples include "sum", "mean", "sd", "min", "median", and
"max". Straightforward, then, to plot the mean Salary for each Dept.
Perhaps more interesting than each category mean of the numerical variable of
interest is the difference between each category mean from the mean of all the
categories. Looking at these differences or deviations highlights the differences among
the categories. mean deviation
scores, Section 6.1,
p. 101
To plot the mean deviations, select the parameter value "dev". The
parameter sort set to "-" sorts the categories descending from the largest positive
to the largest negative deviation to systematize the differences.
BarChart(Dept, Salary, stat="dev", sort="-")
Figure 4.5: Bar chart of the deviation score of Salary for each department from the mean
Salary of all the departments.
For example, the mean Salary of all five categories is $72,233.55. The mean Salary
for ADMIN is $81,277.117. The resulting deviation:1
$81, 277.117 − $72, 233.55 = $9043.567
1Note that the mean Salary across the categories is not the mean Salary of all the data because
different categories have different numbers of members.4.2. ONE CATEGORICAL VARIABLE 69
The value of $9043.567 is plotted for ADMIN in Figure 4.5.
Interpretation. Bar chart of deviation of each category from mean Salary
The average salary varies greatly between departments. Administration employees
earn about $9000 more than the overall average, while Accounting employees earn
about $10,400 less than the overall average. Marketing employees, on average, earn
just under $2000 less than overall average Salary.
4.2.6 Stack the Bars across Multiple Variables
Mach IV scale,
Section 1.3.2, p. 16
The 20-item Mach IV scale for the measurement of Machiavellian tendencies was
previously introduced. How to display the summary of the resulting set of responses
across all 20 Mach IV 6-point Likert items? One possibility arranges the bars for the
responses of a single categorical variable, an item, into a single bar. Then stack the
set of bars for multiple variables into a single column.
To accomplish this display of stacked bars, in place of a single variable, provide
BarChart() a vector of categorical variables to plot. grayscale, Section 4.1,
p. 64
Figure 4.6 shows the grayscale
20 stacked bars of the responses to all 20 items for all 351 respondents.
BarChart(m01:m20)
Figure 4.6: Stacked single-bar bar charts for the 20 items of the Mach IV scale. Mach4 factor
variables,
Section 3.2.2, p. 45
Before calling BarChart(), the Mach4 data set responses were read and converted
to factor variables to report the value labels instead of the corresponding integer
coding. define a variable list
with :, Section 1.2.9,
p. 12
The expression m01:m20 informs R to locate in the relevant data frame, here
d, the variable m01 and then include all variables from the position of m01 to m20. By
default, BarChart() sorts the items by the means of their responses, which can be
turned off with the parameter setting sort="0".70 CHAPTER 4. CATEGORICAL VARIABLES
Interpretation. Stacked bars of Mach IV responses
The level of agreement with the 20 Mach IV items considerably varies. Item m09
has the most agreement, with 57% of the respondents strongly agreeing with the
item and 24% agreeing. In contrast, Item m20 has 59% of the respondents strongly
disagreeing and 19% disagreeing. Several items, such as m08 and m17, have almost
equal expressions of agreement across the six response categories.
4.2.7 Generalize Beyond the One Sample
In the Employee data set sample, more people are in Sales than in any other category.
However, results are unstable from sample to sample, notoriously so for small samples.
sample fluctuations
applied to the sample
mean, Section 6.3.2,
p. 112
Would this same qualitative pattern of differences likely occur in a second sample of
37 different employees from the same population of employees from which the first
sample was obtained? Information to address this question, usually the question
of interest, is provided by BarChart() and PieChart() and also via the text only
output from Prop_test(), abbreviated prop().
The inferential analysis aims to generalize results beyond the sample to the population
as a whole. This inferential analysis addresses whether the sample results are a true
reflection of the underlying population values without having to take additional
samples. Would the conclusions from one set of sample results persist over repeated
samples, or are they a sampling fluke observed in this sample, but not the next? For
this example, maybe in another sample, Marketing occurs more frequently than Sales.
Chi-Square Test of Equal Proportions
chi-square
goodness-of-fit
test: Inferential test
to evaluate if the
values of a
categorical variable
match a distribution,
here equal
proportions.
The large differences between the obtained sample probabilities suggest that perhaps
the cell probabilities are not equal in the population. The sample probability of
randomly selecting an employee in Sales from this data set is 0.417, much larger
than the sample probability of 0.111 for Finance. Test this proposition with the
corresponding chi-squared goodness-of-fit test.
null hypothesis of
equal group
probabilities:
Membership in each
group is equally
likely.
To test for equal population sizes,
begin with the null hypothesis of equal probabilities of group membership.
Null hypothesis, H0:
Population probabilities of group membership are equal in the population
from which the data were obtained.
Even if the null hypothesis is true, the sample frequencies and associated probabilities
rarely equal the exact value expected under the assumption of equality. If the
assumption of equal probabilities is true, then the sample cell frequencies should be
reasonably close to their expected values, based on equal population proportions.
Simply examining the sample proportions and observing how discrepant they are from
each other cannot address the equality of the corresponding population proportions.
This assessment requires the application of formal probability theory in the form of a
p-value, the probability of the sample result IF the null hypothesis is actually true.
The p-value is a conditional probability, the probability of an event if the condition
that something else is true.
p-value: Given a
true null hypothesis,
the probability of
obtaining a sample
result as far or
farther from the null
value.4.2. ONE CATEGORICAL VARIABLE 71
To use this p-value to quantify the definition of “reasonable,” turn to the chi-square
statistic, χ
2
. This statistic compares the obtained frequencies with what was expected
from the null hypothesis, summed over all the cells. In the following equation, fo is
the observed frequency and fe is the expected frequency for each cell.
χ
2 =
X
 
fo − fe)
2
fe
!
The χ
2
statistic compares the obtained value to the theoretical distribution of what
would be expected should the null hypothesis of equal probabilities be correct.
inferential statistics,
Section 6.3.2, p. 112
If the sample proportions all equal each other, the χ
2 value would be zero. The more
discrepant the sample proportions from each other, the larger is χ
2
. If χ
2
is too large,
the probability of the result assuming the null hypothesis, the p-value, is too small.
For a small p-value, reject the null hypothesis as unlikely.
The accepted value for what defines a low probability, the alpha level or α, is commonly
set at 0.05. alpha level: The
definition of a
sufficiently low
probability to define
an unusual event.
Define any probability value below α = 0.05 as sufficiently unusual given
the underlying assumption of the null hypothesis. The low probability of such an
outcome, given the assumption, leads the analyst to conclude that the null hypothesis
is likely not true.
p-value, Section 6.3.3,
p. 117
Scenario. Inferential analysis of the proportions of Departmental membership
In this particular sample, the department with the most membership is Sales,
with a proportion of 41.7% of all employees. Does this pattern likely exist in the
population? Or is this sample result a sampling fluke with the reality that the
probabilities of employment in all departments are equal?
The inferential result from BarChart() follows. Evaluate the null hypothesis from the
chi-square test shown in Listing 4.2 based on the sample proportions from Listing 4.1.
chi-square test of
equal
probabilities:
Evaluate if the
probabilities of each
category are equal.
Chi-squared test of null hypothesis of equal probabilities
Chisq = 10.94444, df = 4, p-value = 0.0272
Listing 4.2: Chi-square test from BarChart().
What is the probability of obtaining a chi-statistic as large as 10.944 or larger,
assuming that the true probabilities are all equal?
Equal probabilities test: p-value = 0.027 < α = 0.05, reject H0
If the assumption of equal group probabilities is valid, then a sample with group
probabilities this discrepant from equality to yield an obtained chi-square of 10.944 is
only 0.028. We conclude that the null hypothesis of equal population probabilities of
departmental membership is likely false: Some Departments have a greater proportion
of the company’s employees than others. Informally we may conclude that Sales has
the most employees because that pattern matches the sample result. Still, a precise
evaluation of this later statement requires yet another inferential analysis beyond the
current discussion.72 CHAPTER 4. CATEGORICAL VARIABLES
Interpretation. Inferential analysis of the equality of proportions
The proportions of the company’s employees in each department are unlikely to
be equal. Apparently, the much larger number of people working in Sales, 42%,
generalizes to the population. More people work in Sales than the remaining
departments.
Note that the p-value does not provide the probability that the null hypothesis of
equal population probabilities of group membership is true. conditional
probability:
Probability of an
event presuming that
another event has
occurred.
Instead, as a conditional
probability, the p-value informs us that IF the population probabilities are equal, the
sample result is a low probability event. We conclude that the null hypothesis is
unlikely. This conclusion, however, is qualitative with the word “unlikely” not precisely
quantified with a numerical probability. The p-value is a precise quantitative result.
Unfortunately, the probability that the null hypothesis is true remains unknown.
More Flexibility for the Chi-Square Test
BarChart() and PieChart() provide the goodness-of-fit test to the uniform distribu￾tion, the evaluation of the equality of the population proportions. lessR also provides
a more general function for the analysis of proportions, Prop_test(), abbreviated
prop().
Prop_test()
function, lessR:
Function for the
inferential analysis of
proportions.
This function provides a variety of inferential tests without visualizations,
provides for directly inputting the frequencies, and provides more detail regarding
the analysis.
Listing 4.3 illustrates the input and descriptive output of Prop_test() for the analysis
of Dept, the value of the first parameter variable. The χ
2
inferential analysis is
identical to that of Listing 4.2.
Prop_test(Dept)
ACCT ADMN FINC MKTG SALE
--------- ------- ------- ------- ------- ------
observed 5 6 4 6 15
expected 7.200 7.200 7.200 7.200 7.200
residual -0.820 -0.447 -1.193 -0.447 2.907
stdn res -0.917 -0.500 -1.333 -0.500 3.250
Listing 4.3: Descriptive statistics reported by Prop_test() for the analysis of the equality
of population proportions.
In addition to the cell frequencies, the expected cell frequencies from the null hypoth￾esis of equal proportions are displayed. From these values, also displayed are the
difference between obtained and expected, the residuals. The standardized residuals
would vary from −2.58 to 2.58 for about 99% of the values in a population consistent
with the null hypothesis. Here the large standardized residual of level SALE equal to
3.25 is consistent with the rejected null hypothesis of equality. More people work in
sales than is expected given the null hypothesis.
Obtain the same analysis by entering the frequencies directly into Prop_test().
Create a vector that contains the cell frequencies. Enter this vector as the value of
parameter n_tot. Optionally, to facilitate reading the output, name the vector with4.3. TWO CATEGORICAL VARIABLES 73
the levels of the categorical variable. Listing 4.4 illustrates the process.
Input. Enter frequencies directly to analyze equality of proportions
x <- c(5,6,4,6,15)
names(x) <- c("ACCT", "ADMN", "FINC", "MKTG", "SALE")
Prop_test(n_tot=x)
Listing 4.4: Enter frequencies directly into Prop_test() for the analysis of equal population
proportions.
The inferential analysis is identical to that of Listing 4.2.
4.3 Two Categorical Variables
4.3.1 Bar Chart from Joint Frequencies
The BarChart() function for two categorical variables calculates and displays the
associated joint frequencies and then constructs the bar chart from those frequencies.
Scenario. Create a bar chart for two variables
What is the relation, if any, between two categorical variables, the department of
employment and job satisfaction?
To answer the question of interest, create the two variable bar chart. The primary
variable plotted is always the first value passed to BarChart(). by parameter:
Specify a second
variable.
If there is a second
categorical variable to plot, specify with the by parameter.
The output lists the levels of each categorical variable by default in alphabetical order.
For Gender, a nominal variable, that means the ordering of the values is arbitrary, so
the arbitrary, alphabetical ordering as Man and then Woman works. On the contrary,
JobSat is an ordinal variable with values ordered from "low" to "medium" to "high".
order levels of a
factor, Section 3.2.1,
p. 44
The desired ordering is not alphabetical, so explicitly specify the ordering of the levels
of JobSat with the base R factor() function to specify the desired ordering.
Input. Define the order of JobSat levels
d$JobSat <- factor(d$JobSat, levels=c("low","med","high")
An additional specification would formally defines JobSat as an ordinal variable by
setting the R parameter ordered to TRUE.
With JobSat values properly ordered, create the two variable bar chart, of which
Figure 4.7 shows two versions: stacked bars on the left, and unstacked bars on the
right. Stacked bars is the default. To unstack, set parameter besides to TRUE.
The stacked bar chart is ideal for presenting the percentage of each variable category
for each bar on the x-axis. Each bar has the same height as if there were no by
variable, but it is now divided into the relative proportions of each by category.
The unstacked version best compares each by variable category on the x-axis to the
remaining by categories.74 CHAPTER 4. CATEGORICAL VARIABLES
BarChart(Gender, by=JobSat)
BarChart(Gender, by=JobSat, beside=TRUE)
Figure 4.7: Default grayscale bar charts for two categorical variables, stacked (left) and
unstacked (right).
Experiment interactively with different parameter settings of the two variable bar
chart for your chosen data with the lessR function interact(). Point-and-click to
select the x-axis variable and the by-variable. Then select the desired parameter
settings to move beyond the default settings. interact(),
Section 9.2, p. 172
Input. Interactive experimentation with colors and other parameters
interact("BarChart")
When finished interactively designing your bar chart, save the created code and
reapply in the future as needed.
Some of the text output of BarChart() for two categorical variables follows.
variable labels,
Section 2.5, p. 34
First
reported are the variable names in the analysis and, if present, the accompanying
variable labels.
Gender, Man or Woman
by
JobSat, Degree of JobSat with Work Environment
joint frequency:
Frequency of
occurrence of the
combination of two
values of a
categorical variable. The focus of analysis for the relation of two categorical variables is their joint frequency,
the count of how many times two specific values in the same row of the data file,
one for each of the variables, occur together. All of the joint frequencies for each
pair of values are presented in the cross-tabulation table, shown in Listing 4.5. The
two-variable bar chart visualizes the joint frequencies.
cross-tabulation
table: Table of joint
frequencies.
Gender
JobSat F M Sum
low 2 11 13
med 7 4 11
high 8 3 11
Sum 17 18 35
Listing 4.5: Cross-tabulation table, the joint and marginal frequencies from BarChart().4.3. TWO CATEGORICAL VARIABLES 75
The cross-tabulation table from BarChart() also contains the marginal frequencies.
marginal
frequency: Row or
column sum of a
cross-tabulation
table.
In Listing 4.5 the marginal frequencies appear in the row or column labeled Sum.
The number in the bottom right corner of Listing 4.5, 35, is the sum of the row and
column sums, the grand total.
grand total: The
total number of
observations in the
table.
In this data set there are data for 37 employees, so two
employees have at least one data value missing for either Gender or JobSat or both.
Interpretation. Two categorical variable bar chart
The number of men and women employed is almost equal, with one more man than
woman among the 35 employees for whom both Gender and JobSat variables are
present in the data. The largest response category is men with low job satisfaction,
11 or 31% of all employees. The next largest response category is women with high
job satisfaction, 8 or 23% of all employees. The smallest response category is a
woman with low job satisfaction, only 2 or 6% of all employees.
Figure 4.8: Worksheet data.
A web page or newspaper article may display this
summary table of coordinates without access to the
original data. From the table of joint frequencies,
create the bar chart. Figure 4.8 shows the input
data as the joint frequencies obtained from the
earlier analysis of categorical variables JobSat and
Gender. The numerical variable could represent
any numerical entity but here represents the joint
frequency under variable Count, the y-variable, the
second variable listed in the function call.
Input. Bar chart of two categorical variables from summary data table
BarChart(Gender, Count, by=JobSat)
The preceding call to BarChart() results in the same visualization in Figure 4.7.
4.3.2 100% Stacked Bar Chart
In general, there are different numbers of observations in each category or level on
the x-axis. In this example, the numbers of men and women are almost the same, but
the numbers can vary dramatically in other situations. To compensate for unequal
sample sizes, the 100% stacked bar chart compares the percentage of the distribution
of the by variable within each level of the x-variable.
Scenario. 100% stacked bar chart of jacket sales
A motorcycle clothing manufacturer sells jackets in three different thicknesses: Lite,
Medium, or Thick. One venue for selling is the motorcycle rally that features
a specific motorcycle company. The clothing manufacturer must decide on the
product mix to bring to a rally. What percentage of jacket types do customers
purchase at a BMW or Honda motorcycle show?
To compare the product mix across different motorcycle brands, levels of the x
variable, is difficult for different sample sizes. Instead, the interest is the percentage76 CHAPTER 4. CATEGORICAL VARIABLES
of observations within each level of the x-variable, each motorcycle brand. The
100% stacked version of the bar chart facilitates the comparison of the by variable
value across the different levels of the x variable by gathering the by-values, such as
percentages, computed only for a single bar.
Each bar’s height for this analysis accounts for the full 100% of all elements within
each category, x. To create the 100% version, set the stack100 parameter to TRUE.
Figure 4.9 shows both the traditional and the 100% stacked bar charts for past sales
of rallies for two motorcycle brands. The data are from the lessR file Jackets.
d <- Read("Jackets")
BarChart(Bike, by=Jacket)
BarChart(Bike, by=Jacket, stack100=TRUE)
Figure 4.9: Default grayscale bar charts for two categorical variables, stacked (left) and
100% stacked (right).
As with the traditional bar chart of two categorical variables, the analysis begins with
the table of joint frequencies, also displayed by BarChart() as shown in Listing 4.6.
Bike
Jacket BMW Honda Sum
Lite 89 283 372
Med 135 207 342
Thick 194 117 311
Sum 418 607 1025
Listing 4.6: Joint and marginal frequencies of variables Bike and Jacket.
From the joint frequencies, to construct the 100% stacked bar chart calculate each cell
proportion not as divided by the overall sum but instead the corresponding column
sum, the column marginal sum. For example, the proportion of just BMW riders
who buy a Lite jacket:
89
418
≈ 0.2129
The BarChart() output in Listing 4.7 includes these corresponding numerical values,
the summary data table from which to construct the visualization.
The displayed bar chart lists the percentage of each cell by default. BarChart()’s
text output shows the proportions as a table and provides an extra decimal digit by
default.
digits_d parameter,
lessR: Number of
decimal digits to
display in the output.
The digits_d parameter specifies the desired number of decimal digits.4.3. TWO CATEGORICAL VARIABLES 77
Bike
Jacket BMW Honda
Lite 0.213 0.466
Med 0.323 0.341
Thick 0.464 0.193
Sum 1.000 1.000
Listing 4.7: Cell proportions within each column that are plotted in the 100% stacked bar
chart.
From Listing 4.7 we can see how the probabilities of different levels of Jacket purchases
change for BMW versus Honda riders. conditional
probability:
Probability of one
event assuming the
occurrence of another
event.
These proportions represent sample conditional
probabilities. The probability for each level of Jacket depends upon, is conditioned
upon, the respective bike. For example, if the bike is a BMW, the rider has a 0.213
probability of purchasing a Lite jacket. But, if the bike is a Honda, the conditional
probability increases to 0.466.
Interpretation. 100% bar chart for clothing sales of two motorcycle brands
46% of BMW riders prefer the thick jacket, and only 21% prefer the light jacket.
The percentages for Honda riders are reversed. 47% of Honda riders prefer the light
jacket, and only 19% want the thick jacket. To bring inventory to a rally, multiply
the corresponding percentages for the chosen bike by the anticipated number of
sales.
The different product mixes likely follow from the riding style. The BMW bike is more
of a sport bike than the Honda bikes. The thicker jacket provides more protection in
case of a spill.
4.3.3 Description with Summary Tables
Sometimes the visualization such as a bar chart may not be of interest, only the
frequency distribution of a single variable or the joint frequency distribution of two
variables. Or, the summary statistics of interest may extend beyond the analyses
from BarChart().
pivot() function,
lessR: Compute
summary statistics.
The dedicated lessR function for computing descriptive statistics is pivot(), which
outputs the results in the form of a table, a summary or pivot table. Figure 4.10
shows the syntax for pivot() parameters data, compute, variable, and by.
Figure 4.10: General syntax of a call to pivot().
There can be more than one statistic to compute, more than one variable for which
to compute the statistics, or more than a single grouping variable indicated with the
by parameter. Specify the multiple units as a vector, such as with the c() function.
c() function, R:
Section 1.2.9, p. 12 In Figure 4.10 find two by grouping variables, input as a vector with c().78 CHAPTER 4. CATEGORICAL VARIABLES
One data analysis technique partitions the data into groups, such as Men and Woman,
aggregation: then computes one or more summary statistics separately for each group.
Partition data into
groups based on one
or more categorical
variables, then
compute summary
statistics for each
group.
The statistic
can be computed over a continuous variable, discussed in the next chapter. The
relevant statistic for a categorical variable is the count or proportion for each level or
combination of levels of the frequency or joint frequency distribution for one or more
additional categorical variables. The process of computing a statistic over groups is
called aggregation. The resulting table of statistics is called pivot table.
pivot table: The
table of statistics
that results from a
data aggregation.
BarChart() implicitly performs the data aggregation to visualize the resulting pivot
table of frequencies or joint frequencies. With pivot(), set the value of the compute
parameter to table to calculate a frequency table of categorical variables. Listing 4.8
shows the analysis to obtain the frequencies and corresponding proportions for a
single categorical variable, here Dept in the d data frame.
pivot(d, table, Dept)
Dept Freq Prop
1 ACCT 5 0.135
2 ADMN 6 0.162
3 FINC 4 0.108
4 MKTG 6 0.162
5 SALE 15 0.405
6 <NA> 1 0.027
Listing 4.8: Frequency distribution of Dept from pivot().
Listing 4.9 shows the analysis to obtain the frequencies and corresponding proportions
for a single categorical variable, here Dept across levels of Gender, the value of the
by parameter. Both variables reside in the d data frame.
pivot(d, table, Dept, by=Gender)
Gender Dept_n Dept_na ACCT ADMN FINC MKTG SALE
1 M 18 0 2 2 3 1 10
2 W 18 1 3 4 1 5 5
Listing 4.9: Joint frequency distribution of Dept and Gender from pivot().
Many by variables can be included in the analysis, potentially yielding 3-way, 4-way,
and beyond joint frequency tables. However, these summary tables describe the given
data but do not indicate how stable the data would be over repeated sampling. That
is the purpose of inferential statistics.
4.3.4 Inferential Analysis
As with the inferential analysis for one categorical variable, the inferential analysis for
two categorical variables is also based on the chi-squared statistic, χ
2
.
independent
events: Occurrence
of one event
unrelated to the
probability of the
occurrence of
another.
The chi-square
test of independence evaluates the independence of two categorical variables with the
null hypothesis that the variables are unrelated or independent.
chi-square test of
independence:
Test to evaluate if
two categorical
variables are related.
Is the tendency to be4.3. TWO CATEGORICAL VARIABLES 79
classified in a particular category of the first variable related to a specific classification
in the second category? If so, the variables are related.
Scenario. Evaluate the relationship between two categorical variables
The data values of Gender in this data set have two categories, Man and Woman.
Do men or women tend to have more or less job satisfaction at the company of
interest? JobSat here is assessed with responses of 1, 2 and 3, so both variables are
categorical.
Employee data table,
Figure 1.5, p. 13
Test the relation between Gender and JobSat from the Employee data set. If the
variables are unrelated, that is, independent, then knowing a person’s Gender conveys
no information regarding the perceived level of JobSat. If the variables are related,
then one Gender has a stronger tendency to be satisfied.
null hypothesis,
H0: Assumption of
no relationship
between the
variables.
The assumption on which the test is based is the null hypothesis, denoted as H0.
H0 : Gender and endoresement of Deceit are independent
The alternative to the null hypothesis follows.
H1 : Gender and endoresement of Deceit are related
Before performing the inferential analysis of the hypothesis test, first describe the
sample. As seen from the bar chart and the corresponding frequency table, men tend
to be more dissatisfied in this sample, and women tend to be more satisfied. The
more interesting question: To what extent does the sample relationship generalize to
the entire population from which the sample was obtained?
chi-square statistic,
χ
2
, Section 4.2.7,
p. 71
The BarChart() function for two variables yields the chi-square test for independence.
The chi-square test of independence is an inferential test, which appears in Listing 4.10.
Chi-square Analysis
-------------------
Number of cases in analysis: 35
Number of variables: 2
Test of independence: Chisq = 9.300699, df = 2, p-value = 0.0096
Listing 4.10: Inferential chi-square test of the null hypothesis of no relation between Gender
and JobSat. inferential statistics,
Section 6.3.2, p. 112
The obtained chi-square value is χ
2 = 9.30. If the cell proportions reflect no rela￾tionship between the two variables, then the chi-square statistic would be precisely
zero. In actual data, however, there is sampling error.
sampling error:
Impact of the
randomness inherent
in any one sample.
Even when the null hypothesis
of independence is true, the chi-square statistic is virtually always larger than zero.
The question is how much larger than zero is reasonable if the null hypothesis of no
relationship is true?
p-value: If the null
hypothesis is true,
the probability of
obtaining a result as
deviant or more than
the obtained result.
Assess the size of the chi-square statistic in terms of its probability of getting a
chi-square value as larger or larger than what was obtained in this sample, assuming
no relation between Gender and JobSat. The p-value provides the answer.
conditional
probability,
Section 4.2.7, p. 72
If the null
hypothesis is true, the p-value is the conditional probability of what was obtained,
which is then compared to usual cutoff value that defines a low probability, the alpha
level, α = 0.05.
alpha level,
Section 4.2.7, p. 7180 CHAPTER 4. CATEGORICAL VARIABLES
If the p-value is larger than α, then the probability is sufficiently high that the
obtained result is considered consistent with the null hypothesis. If the p-value is
smaller than α, an unlikely event occurred assuming that no relation actually exists,
so reject the hypothesis of no relation as implausible.
test of the null hypothesis of no relationship:
p-value = 0.0096 < α = 0.05, so reject H0
If the null hypothesis of no relation between Gender and JobSat is true, then the low
p-value of 0.0096 indicates an unlikely event.
Interpretation. Test of independence
Gender and Satisfaction are related such that men are more dissatisfied with this
work environment than women, not just in this sample, but in the population as a
whole. The sample results generalize.
Note, however, that although the p-value is precisely computed, it is a conditional
probability in that it specifies the probability of the results IF the null hypothesis is
true. From the low p-value we conclude that the variables are likely related, but the
probability of this relationship is not known. The p-value is quantitative, but the
conclusion that the null is unlikely is qualitative. We do not know the probability
that the null hypothesis of no relation is true.
Prop_test(), analyze
a single variable,
Section 4.2.7, p. 72
As with the analysis of the distribution of a single categorical variable, Prop_test()
also provides an analysis of independence from the joint frequency table of two
categorical variables.
Input. Inferential analysis of joint frequencies
Prop_test(Dept, by=Gender)
As with the analysis of a single variable, the output is the same as from BarChart()
though enhanced with a display of the actual and expected frequencies under the null
hypothesis, and the associate residuals and standardized residuals.
4.4 Analysis Problems
The psychology department at a public university was interested in understanding
more about their in-state and out-of-state students. The origin of their current
students was classified as in-state, out-of-state-USA, and international. Also available
is each student’s gender and choice of major of either psychology as a social science
or as a biological science.
The data are available at http://lessRstats.com/data/psych.csv
1. One categorical variable.
a. Show with statistics and a bar chart how many students fit each of the three
classifications regarding origin.4.4. ANALYSIS PROBLEMS 81
b. Do the chi-square test of equal proportions. What is the conclusion?
2. Two categorical variables.
a. Is there a relation between gender and origin of student? Show with descriptive
statistics.
b. Compare the regular two-variable bar chart with the 100% stacked bar chart.
Which is more useful for the comparison? Why?
c. Analyze with the chi-square test of independence. What is the conclusion?
3. Two categorical variables.
a. Is there a relation between major and origin of student? Show with descriptive
statistics and a bar chart.
b. Analyze with the chi-square test of independence. What is the conclusion?
4. Two categorical variables.
a. Is there a relation between major and gender? Show with descriptive statistics
and a bar chart.
b. Analyze with the chi-square test of independence. What is the conclusion?Chapter 5
Continuous Variables
5.1 Quick Start
Install R and lessR as explained in Section 1.2. At the beginning of each R session,
access the lessR functions with library("lessR"). From Chapter 2, read your
data into an R data frame such as d with d <- Read("") to browse for the data file.
Or, locate your data file with a path name or web address between the quotes.
The data table, represented within R as the data frame, contains the data values for
one or more variables. continuous variables,
Section 2.2.1, p. 22
Each variable is characterized by either continuous values or
non-numerical categories. This chapter describes some basic analyses for continuous
variables that are routinely among the first analyses performed after reading the
data. These analyses consist of visualizing a continuous variable’s data values and
describing characteristics of that distribution with statistics.
For a single continuous variable, create the histogram. Or, superimpose over the
histogram a density plot that smooths out the jagged edges of the histogram. Histogram(), p. 83
Density (smoothed
frequency) curve,
p. 87
Histogram(Y)
Histogram(Y, density=TRUE)
Use Plot() for a single variable to create a superimposed violin/box/scatter plot of
VBS plot, p. 91 the variable Y in this example, here called a VBS plot.
Plot(Y)
Or, create each plot separately with the aliases Boxplot(), ViolinPlot(), and
ScatterPlot().
Box plot, p. 90
Violin plot, p. 91
Scatter plot, p. 93
BoxPlot(Y)
ViolinPlot(Y)
ScatterPlot(Y)
An alternative to the aliases is to specify the vbs_plot parameter as part of the call
to Plot() and provide some combination of the three letters "v", "b", and "s" to
specify which plots to create. The default value specifies all three plots, "vbs".
825.2. HISTOGRAM 83
To analyze all the variables in the data frame, substitute a data frame name for the
variable name in the associated function call, or leave blank to indicate the default
data frame d. Histogram() and BoxPlot() analyze all the variables according to
their respective data type.
Histogram()
BoxPlot()
Each resulting visualization is written to its own window if running R by itself or is
own windowpane if running R withing RStudio.
CountAll()
function: A bar chart
or histogram, with
summary statistics,
for all variables in a
data frame.
A related function is CountAll(), abbreviated ca(). This function does the complete
summary analysis, selectively invoking either BarChart() for each categorical variable
or Histogram() for each continuous variable, which also provides the summary
statistics from pivot(). CountAll() applies to the entire data set, providing a
complete visualization and statistical summary of all the variables in the data frame.
CountAll()
CountAll() only applies to data frames.
Plot() can also visualize data collected over time, either in the form of a run chart
or time series chart. To plot the data values in sequential order as stored in the data
frame, here the default d data frame, set the value of parameter run to TRUE. run chart
visualization,
Section 5.4.1, p. 93 Plot(Y, run=TRUE)
Or, plot a time series chart. In this example, date is a variable of type Date, such as
consecutive months. Y is the value to plot at each time point. time-series
visualization,
Section 5.4.2, p. 94 Plot(date, Y)
5.2 Histogram
The histogram is the standard data visualization for displaying the frequencies of the
data values for a continuous variable such as Age, Salary, MPG, Gallons, or Height.
Measure a continuous variable on a numerical scale. Measure the data values to a
specified level of precision, such as the nearest inch or the nearest 1/4-inch. From
that data build the histogram and compute the corresponding summary statistics.
5.2.1 Bins
In contrast to the relatively few unique values of a categorical value, a continuous
variable has many potential unique values. How many potential values? Generally
there are too many unique data values to plot each individually on a single visualization.
Consider annual Salary, where each single value for a company’s employees to the
nearest penny must be considered, from a range such as about $35,000 to $200,000.84 CHAPTER 5. CONTINUOUS VARIABLES
Because of so many potential data values, for many data sets most potential values
Employee data, never occur in the data.
Section 1.3.1, p. 13
In this example of 37 employees from the Employee data set,
all 37 salaries are unique. Any given value such as $67,392.84 would likely not occur
unless the sample size was extremely large.
bins, classes:
Sequence of adjacent
intervals, each
generally of the same
size.
A bar chart of these data values could be constructed but it would consist of a few
1’s scattered across the range of potential values. What to do with all the unique
data values that cannot be meaningful plotted as individual values? Partition the
range of values into bins, sometimes called classes. Each bin contains similar data
values, as shown in Figure 5.1.
$50,000 $60,000 Annual Salary
Lower
Cutpoint
Upper
Cutpoint
$55,000
Midpoint
cutpoints: Bin Figure 5.1: Example of a bin defined over the range of data values from $50,000 to $60,000.
lower and upper
boundaries. Define each bin by its lower and upper boundary, its cutpoints.
bin width: Distance
between cutpoints.
The width of each
bin, or bin width, follows from the values of the cutpoints. The single value that most
effectively summarizes the values of the bin is its midpoint, which is not necessarily a
midpoint: Single data value. In Figure 5.1, bin width is $10,000 and the midpoint is $55,000.
summary of all
values within the bin. To evaluate the frequencies of a continuous variable, first define the bins and then
place each data value into its respective bin. Figure 5.2 illustrates assigning a Salary
of $57,358 to the bin $50,000 to $60,000.
$40000 $50000
$57358
$30000 $60000 $70000 $80000 $90000 $100000 $110000 $120000
Figure 5.2: The assignment of data value $57,358 to its corresponding bin.
Consistently assign values exactly equal to a cutpoint to either the adjacent lower
bin or the adjacent higher bin. Each computer application that generates histograms
defaults to one or the other choice.
5.2.2 Default Histogram
Construct the histogram by placing the range of data values and bins on the horizontal
axis. Then construct a bar over each bin proportional to its frequency. Unlike a bar
graph of a categorical variable, indicate the underlying continuity of the numerical
scale with adjacent bins that share a common border.
Scenario. Construct a histogram
The variable of interest is Salary from the Employee data set. To display the
pattern of how often different salaries occur in this company, place similar values
of Salary into the same bin. Then plot the resulting histogram.
Read the data, and in this case also the variable labels, into R. Read the data into
the d data frame, and the labels into the l data frame. Both data files are part of
lessR so just refer to their corresponding names.
variable labels,
Section 2.5.2, p. 345.2. HISTOGRAM 85
d <- Read("Employee")
l <- Read("Employee_lbl")
Histogram()
function, lessR:
Obtain a histogram,
summary statistics
and a table of
frequencies.
Apply the lessR histogram function Histogram(), abbreviated hs(), to Salary in
Figure 5.3. As with all visualizations in this book, to obtain grayscale, first set the
first parameter of the lessR function style(), theme, to "gray".
Histogram(Salary)
Figure 5.3: Default grayscale histogram.
Find the corresponding frequency distribution in Listing 5.1.
frequency
distribution: The
values of a variable,
bins for a histogram,
and corresponding
counts. Bin Width: 10000
Number of Bins: 10
--------------------------------------------------------
Bin Midpoint Count Prop Cumul.c Cumul.p
--------------------------------------------------------
30000 > 40000 35000 4 0.11 4 0.11
40000 > 50000 45000 8 0.22 12 0.33
50000 > 60000 55000 8 0.22 20 0.55
60000 > 70000 65000 5 0.14 25 0.69
70000 > 80000 75000 3 0.08 28 0.77
80000 > 90000 85000 5 0.14 33 0.91
90000 > 100000 95000 1 0.03 34 0.94
100000 > 110000 105000 1 0.03 35 0.97
110000 > 120000 115000 1 0.03 36 1.00
120000 > 130000 125000 1 0.03 37 1.03
--------------------------------------------------------
Listing 5.1: Frequency distribution from the Histogram() function.
The frequency table identifies the bins and their midpoints, as well as the counts, the
proportions, cumulative counts and cumulative proportions. The > symbol for each
bin indicates that all the values in the bin are larger than the first specified value,
and include all values up to and including the second specified value. Histogram()
also displays summary statistics and an outlier analysis, discussed later.
histogram summary
statistics, Section 6.1,
p. 107
outlier, Section 5.3.1,
p. 9086 CHAPTER 5. CONTINUOUS VARIABLES
Interpretation. Histogram of Salary
The histogram and equivalent frequency distribution of Salary show that for these
37 employees, the most frequently occurring salaries are from $40,000 to $60,000,
with relatively few salaries over $100,000.
Experiment interactively with different parameter settings of the histogram for your
data with the lessR function interact().
interact(),
Section 9.2, p. 172
Input. Interactive experimentation with colors and other parameters
interact("Histogram")
When finished interactively designing your histogram, save the code and reapply in
the future as needed.
5.2.3 Customize the Bins
undersmoothed: A
histogram with a bin
width too small for
the given sample size.
Default histograms generally are OK but can usually be improved. The default
histogram in Figure 5.3 is a little undersmoothed, a little too jagged. The frequency
for the bin for the range from beyond 80000 up to and including 90000 is lower than
for the previous bin and the successive bin. These jagged bin ups and downs are
likely an arbitrary outcome of this particular sample and not a true property of the
underlying process of assigning salaries. sampling error,
Section 6.3.2, p. 112
Gather another sample, and the same jagged
pattern would likely not be repeated, a consequence of the random variation across
samples called sampling error.
Adjust the bin widths to avoid undersmoothing. To compensate for this undersmooth￾ing, increase the bin size for the histogram of Salary from $10,000 to $15,000. Set
the widths of the bins with the bin_width parameter, as in Figure 5.4.
bin_width
parameter: Set the
width of histogram
bins.
Histogram(Salary, bin_width=15000)
Figure 5.4: Histogram with specified bins and default starting value.
Set the starting point of the bins with the bin_start parameter.
bin_start
parameter: Set the
starting point of
histogram bins.
An often helpful
adjustment sets the starting value of the first bin to a lower value. In this example,
set the bin_start parameter to move the beginning of the first bin from $40,0005.2. HISTOGRAM 87
to $35,000. As Figure 5.5 shows, particularly with small samples the shape of the
histogram changes with changes to the bin width and the starting point.
Histogram(Salary, bin_width=15000, bin_start=35000)
Figure 5.5: Histogram with specified bin widths and starting point for the bins.
This revised histogram in Figure 5.5 improves the previous versions in Figures 5.3
and 5.4. The beginning bin now more accurately portrays employees with the
lowest salaries. Also, the histogram represents a smooth progression from increasing
frequencies to a peak and then a progression of lowering frequencies. The true
distribution may assume many forms, but most distributions are either increasing
in frequency as the values of the variable increase, or decreasing in frequency, or, as
shown here, increasing and then decreasing.
Tweaking the histogram bins from the given default is standard practice, especially
for histograms created from relatively small sample sizes. There is no right or wrong
or best histogram, but some histograms likely represent the underlying distribution
more effectively than others. Vary the Histogram() parameters bin_start and
bin_width to often better display the characteristics of the underlying distribution
for the available data.
5.2.4 Smooth the Bins
density plot:
Smoothed out
histogram, such as a
normal curve.
A density plot is a kind of idealized histogram in which the bin width diminishes to
zero. The density plot smooths out the jagged edges of the histogram. This smooth
curve better represents the shape of the underlying distribution, which likely is not
characterized by the sharp edges of rectangles but rather a smooth continuity. The
most well-known example of this smooth density curve is the normal curve.
A density curve can be estimated whenever a histogram is created. Instead of plotting
the frequency of a histogram bin on the y-axis, plot the value of the density at each
value on the x-axis.
Scenario. Obtain a density plot of a continuous variable
From sample data values of Salary from the Employee data set, estimate the
smooth curve, the density curve, that approximates the true shape of the underlying
distribution of Salary without the jagged edges of histogram bins.88 CHAPTER 5. CONTINUOUS VARIABLES
Histogram() To obtain a density plot, set the Histogram() function’s density parameter to TRUE.
function with
density set to TRUE:
Plot density
distribution.
The density parameter instructs Histogram() to superimpose a general density
curve over the histogram by default. Set the type parameter to "normal" to plot
only the normal curve. To view both curves simultaneously, set type to "both".
normal densities:
The smoothed
histogram in the
form of a normal
curve.
We have already created the improved histogram, so better to apply the densities to
that histogram, as shown in Figure 5.6.
Histogram(Salary, bin_start=25000, bin_width=15000, density=TRUE)
Figure 5.6: Histogram with superimposed general density curve and customized histogram.
This visualization succinctly summarizes the distribution of Salary for those 37
employees with a smooth curve.
Interpretation. Density curve of Salary data
There is a concentration of lower salaries around $60,000 per year. Some salaries
are lower, though there is a gradual lessoning of employees enjoying increasingly
higher salaries. A small number of employees have much higher relative salaries.
bandwidth: Extent 5.2.5 Bandwidth
of diminishing
influence of nearby
values to calculate
the position of a
point on a density
curve.
Also reported is the bandwidth used to construct the estimated density curve, which
is $9229, shown in Listing 5.2.
Density bandwidth for general curve: 9529.045
For a smoother curve, increase bandwidth with option: bw
Listing 5.2: Bandwidth setting from Histogram() with density set to TRUE.
The bandwidth parameter, bw, specifies the influence that near-by data values have on
the location of the current point on the density curve.
bw, bandwidth
parameter: Set the
bandwidth for
estimating the
density curve.
For estimating each point on
the curve, the surrounding data values form a set of diminishing weights. Data values
close to the given data value yield much influence on the location of the point on the
density curve. Data values far from the given data value have little if any impact on
the location of the given position on the estimated curve for the given data value.
To further smooth the visualization, increase the bandwidth parameter, bw, from the
value displayed in the output. A larger bw indicates that a wider range of surrounding5.2. HISTOGRAM 89
data values contribute to the corresponding point on the density curve. Reduce its
value for a less smooth curve. For example, increasing the bandwidth for the density
visualization in Figure 5.6 from the default value of 9529 to 12,000 smoothes out
some of the small undulations in the original visualization. The revised density plot
is not only smoother, but also flatter as a result of the increased band width.
5.2.6 Cumulative Histogram cumulative
frequency: Sum of
frequencies for all
values up to and
including the
specified value.
In place of ordinary frequencies, another Histogram() parameter plots the cumulative
frequencies or cumulative proportions. The resulting histogram is a cumulative
histogram.
cumulative
histogram:
Histogram of
cumulative
frequencies.
Histogram() offers two visualizations of the cumulative histogram: alone
or with the regular histogram. Figure 5.7 presents an example of both histograms on
the same visualization with the same bins from the previous examples.
Histogram(Salary, bin_start=25000, bin_width=15000, cumul="both")
Figure 5.7: Cumulative and regular histogram with specified bins.
Frequencies are never negative, so as the values of the variable increase, the cumulative
distribution always increases in value or stays the same.
The histograms in Figure 5.7 demonstrate that most of the salaries are in the lower
range of the complete distribution of salaries. According to the text output, identical
to that provided by the regular histogram analysis, the cumulative proportion of all
the data values up through the bin $100,000 to $115,000 is 0.97. Only one value
remains past this bin, for the salary of $124,419.23.
5.2.7 Histograms for All
The most basic and usually first analysis of the variables in a data frame examines
their distributions, both for continuous and categorical variables. The numeric
summaries of a distribution are the summary statistics, which differ for continuous
and categorical variables.
Scenario. Histograms and summary statistics for all numerical variables
For all numerical variables in a data frame, provide the histogram and text output
that includes the essential summary statistics.90 CHAPTER 5. CONTINUOUS VARIABLES
If the first value in the call to Histogram() is a data frame, all the continuous
variables in the data frame are analyzed. If there is no value passed to Histogram(),
then the data frame d is assumed.
Histogram()
With this option, for each continuous variable, Histogram() also provides relevant
summary statistics.
Countall(), abbreviated ca(), obtains a histogram for all continuous variables and
bar charts for all categorical variables, each with the relevant statistics. Best to
declare all categorical variables in the analysis as R factors before data analysis begins. factor() function,
Section 2.2, p. 22 If the variable is a factor, then CountAll() always analyzes as a categorical variable.
5.3 Histogram Alternatives
5.3.1 Box Plot and Outliers
The histogram is the traditional graphical representation of a continuous variable’s
distribution, but there are other options, such as the box plot. IQR, Section 6.2.2,
p. 107
Developed by John
Tukey (1977), the “box” in a box plot is based on the interquartile range or IQR,
the positive difference between what is essentially the first and third quartiles, the
range of data that contains the middle 50% of all the data values. The box’s width
is the IQR, with a line through the median and perpendicular lines extending out
from the edges. Tukey did not literally use the 1st and 3rd quartiles, but rather
an approximation called “hinges”, presumably because they are easier to compute
than quartiles, an important consideration with pre-computer technology. For our
purposes, as shown in Figure 5.8, we consider the box plot based on the quartiles,
which are nearly, if not exactly, equal to the hinges.
Figure 5.8: Generalized box plot.
Scenario. Create the box plot of Salary
The employee data set contains the salaries of 37 employees. What is the pattern
of the distribution of these salaries? To visualize this distribution, generate the box
plot of these salaries and identify any potential and actual outliers.
outlier: A value far
from most of the
remaining data
values.
An outlier is a value considerably different from most remaining values of the dis￾tribution. Outliers always should be identified for any variable because their values5.3. HISTOGRAM ALTERNATIVES 91
could represent a coding error. Or, an outlier could represent the outcome of a
process different from the process that generated all or most of the other values of the
distribution. If so, then mixing all the values into a single analysis may be accurate
numerically but may not represent any real-world process.
potential outlier:
An extreme value
that may be an
outlier. The boxplot is especially useful to identify outliers. The inventor of the box plot,
Tukey (1977), identified two types of outliers, based on the IQR. The potential outlier
lies between 1.5 IQR’s and 3.0 IQR’s from the edges of the box.
actual outlier: An
extreme value that
According to this likely is an outlier.
definition, an actual outlier lies more than 3.0 IQR’s from either box’s edge. whisker: A line
from a box’s edge
that extends to the
most extreme data
value that is not a
potential outlier.
Points
past the whisker are likely outliers. There are many ways to define an outlier, but
Tukey’s definition appears to work well in practice.
BoxPlot() function,
lessR: Create a box
plot.
Obtain the lessR boxplot with BoxPlot(), abbreviated bx(). The box plot of Salary
in Figure 5.9 shows one potential outlier beyond the right whisker.
grayscale, Section 4.1,
p. 64
BoxPlot(Salary)
Figure 5.9: Default grayscale lessR box plot.
The plot also demonstrates some right-tailed skew. The right side of the box after
the median bar is longer than the corresponding left side and the right whisker is
longer than the left whisker. variable labels,
Section 2.5, p. 34
Variable labels provide the label automatically for the
horizontal or x-axis.
For the default multi-color plots, more extreme outliers are labeled in a more vivid
red. For the grayscale plot in Figure 5.9, potential and actual outliers are displayed
with a square and a diamond, respectively. According to this definition, the largest
value in this distribution is considered a potential outlier.
5.3.2 Violin-Box-Scatter Plot
scatterplot: Plot
each point to
represent one or
more data values, the
values aligned on the
corresponding axes.
Other visualizations can be combined with the box plot. The one-variable scatterplot
plots a point for every data value. violin plot: Plot of
a distribution as a
density plot and its
mirror image.
The violin plot is an updated version of the boxplot
that replaces the box with a density plot and its mirror image. The violin plot
transforms the straight edges of the box into information that shows the concentration
of data values along the horizontal axis.
Obtain the combined violin plot, box plot and scatter plot in one visualization by
passing a single, numeric variable to the lessR function Plot().
Plot() function,
lessR: Plot
scatterplots and
augmented
scatterplots.
This combined plot
shows the highlighted potential outlier, the overall shape of the distribution, the
quartiles, and the specific values upon which the box plot is based. VBS plot:
Superimposed violin,
box, and scatter plot.
Here name the
combine plot the VBS plot, shown in Figure 5.10.92 CHAPTER 5. CONTINUOUS VARIABLES
Plot(Salary)
Figure 5.10: Integrated violin, box, and scatterplot, called here the VBS plot.
The following text output from Plot() reveals the value of the potential outlier from
Listing 5.3.
--- Salary, Annual Salary (USD) ---
Present: 37
Missing: 0
Total : 37
...
Outlier: 124419.2
Listing 5.3: Outlier identification.
Both Plot() and Histogram() numerically identify the outliers. Plot() also provides
visual identification to highlight the importance of identifying outliers. For the
default color theme, "colors", the outliers are plotted in two different shades of red,
depending on the severity of the outlier. As shown in shown in Figure 5.10, for a
grayscale plot, color theme of "gray" set with the style() function, outliers are
plotted as diamonds.
5.4 Visualize Data over Time
The previous analyses in this chapter applied to the distribution of values for a variable.
The values were ordered from the smallest to the largest, with the frequencies of
similar values noted. Here we focus on the order of the values in the data table, usually
because the order reflects the time that each value was generated. The emphasis on
time leads to a consideration of two different kinds of data.
Cross-sectional data values are measurements of the same variable at about the same
time over different people, or whatever is the unit of analysis.
cross-sectional
data: Data for a
variable collected at
about the same time.
Administering an
attitude survey one time to many different people generates cross-sectional data.
Analyzing the frequencies of the data values ordered from smallest to largest is a
primary analysis for these data. The visualizations include the histogram, one-variable
scatterplot, box plot, violin plot, and density plot.5.4. VISUALIZE DATA OVER TIME 93
Longitudinal data values are measurements of the same variable with variation over
different time periods.
longitudinal data:
Data for a variable
collected at different
times.
Data collected over time can, and usually should be, analyzed
with visualizations such as histograms. Longitudinal data, however, also present the
additional dimension of time for analysis. The analysis of longitudinal data includes
their visualization over time, such as with the lessR function Plot().
Plot() function,
lessR: Plot points
over time with a line
segment that joins
consecutive points.
The variable’s
values appear on the vertical or y-axis. A unit of time defines the horizontal or x-axis.
The visualization shows the performance of the ongoing process over time.
5.4.1 Run Chart
run chart: Values
plotted and identified
in the order that
they occur.
One version of a time oriented plot is the run chart, in which the horizontal axis lists
the ordinal position of each value, counting from 1 to the last value.
Scenario. Evaluate teaching performance
Consider the student ratings of overall teaching performance for a professor about
to be evaluated for tenure. The professor taught the same course once a term for
the last five years, four quarters a year. The ratings are on a 10-point scale, where
the most favorable rating possible is a 10.
Read the ratings data into R dataframe d as follows.
d <- Read("http://lessRstats.com/data/Ratings.csv")
First, the Dean examines the scatter plot of the mean ratings from these 20 classes,
without a consideration of time. The variable name is Rating, passed to Plot() as
the first parameter value, resulting in Figure 5.11.
Plot(Rating, vbs_plot="s")
Figure 5.11: Distribution of the mean student ratings over the 20 terms.
To obtain only the scatterplot and not the corresponding violin and box plot, specify
the value of "s" for the parameter vbs_plot.
The Dean concludes that the ratings are generally favorable, though some terms the
students were relatively displeased. Are those less well-received terms earlier in the
professor’s employment, or are they more recent? Has the professor’s performance in
terms of student perception been increasing or diminishing? The run chart provides
the visual answer to this question, shown in Figure 5.12. To facilitate comparison of
values at different time periods, if the run chart does not exhibit pronounced trends
up or down, Plot() adds a horizontal dotted line at the median. Compare the ratings
against the plotted median line.94 CHAPTER 5. CONTINUOUS VARIABLES
Plot(Rating, run=TRUE)
Figure 5.12: Run chart of the mean rating each term for a course.
Use the center_line parameter to specify the reference line with values "off",
"mean", "zero", or the default of "median".
center_line
parameter: Specify
the type of center
line drawn, if any. By default, Plot() displays both the
points for each individual data value and the line segment that connects each adjacent
pair of points. To display only the line, set the size parameter to 0.
type parameter:
Specify to plot line
segments (l), points
(p), or the default
both (b).
The default
plotted point is a filled circle. Access alternative shapes that R provides with the shape
parameter according to numeric codes or some character codes such as "diamond".
Enter ?points to view all the possibilities. For example, shape=23 provides diamonds
as the plotted points. shape parameter
Specify the type of
point to plot. Interpretation. Run chart of ratings
The professor’s ratings exhibit a mild trend upwards, demonstrating some improve￾ment over time. The worst ratings occurred toward the beginning of the professor’s
employment. For example, the worst mean rating, close to a 7.0, occurred for the
sixth term. The best ratings occurred for Terms 15 and 19.
Taking into account the time dimension of the ratings adds more information than
simply considering the distribution of values by their magnitude, as with a histogram
or VBS plot. The plots work together to provide complementary information. The
size of the values and how the values are distributed by size are usefulinformation.
The distribution of values over time is also useful for time-oriented data.
5.4.2 Time series
Similar to a run chart, a time series chart also plots the values of a variable over
time. The distinction is that the horizontal axis for the time series chart is labeled
with the times or dates when the data values were generated. Consider the following
three sets of monthly stock market data downloaded from finance.yahoo.com for
companies Apple, IBM, and Intel. Locate the data in the lessR data file StockPrice.
d <- Read("StockPrice")
Find an excerpt of the stock price data in Listing 5.4.5.4. VISUALIZE DATA OVER TIME 95
date Company Price
1 1985-01-01 Apple 0.101050
2 1985-02-01 Apple 0.086241
...
451 1985-01-01 IBM 12.718853
452 1985-02-01 IBM 12.497342
...
901 1985-01-01 Intel 0.379217
902 1985-02-01 Intel 0.345303
...
Listing 5.4: Excerpt from the StockPrice data included with lessR.
Order the data values for the time series chart from the earliest to the last data value.
sort_by() function,
Section 3.5, p. 51
If ordered in the opposite direction, the lessR function sort_by() can re-order the
entire data frame by the variables of interest.
Plot Time Series
The categorical variable Company has three unique values. To plot the data for just
one company, one possibility creates a new, smaller data frame by subsetting the rows
of the data frame to just those of a specific company. subset to a new data
frame, Section 3.6,
p. 53
The lessR analysis functions
also provide for another possibility: Subset the data just for a specific analysis as
indicated by another parameter in a function call. Instead of creating and storing
a separate data frame, set the rows parameter to (Company=="Apple") where the
== indicates a logical test of equality. The result is Figure 5.13, which plots Apple’s
stock price from 1985 through the middle of 2022.
Input. Select only rows of data in variable Company that equal "Apple"
Plot(Date, Price, rows=(Company=="Apple"))
Figure 5.13: Apple stock price.
by parameter,
Plot(): Specify a
categorical variable
for which to plot the
points for different
levels all on the same
panel.
Or, plot the stock market price for all three companies on the same panel, that is,
the same set of coordinate axes. Indicate to plot the data for multiple values of a
categorical variable on the same panel with the by parameter. Figure 5.14 shows this
visualization of the stock price for all three companies represented in the data: Apple,
IBM, and Intel.96 CHAPTER 5. CONTINUOUS VARIABLES
Plot(Date, Price, by=Company)
Figure 5.14: Stock price of three companies on one panel.
Cleveland (1993) developed a multiple-panel plot named a Trellis plot because of
its similarity to a garden trellis for growing plants vertically. Trellis plots are also
by1 parameter, referred to as facet plots (Wickham, 2016).
Plot(): Specify a
categorical variable
for which to plot the
points for different
levels on different
panels, as a Trellis
plot.
To plot the three time series on separate
panels, use the by1 parameter in place of by to identify the grouping variable, shown
in Figure 5.15.
Plot(Date, Price, by1=Company)
Figure 5.15: Trellis plot of stock price of three companies on three distinct panels.
Date Formats
Reading from an Excel formatted file works well as R automatically converts the
date variable to the R variable type of Date. However, if reading from a csv file, the
date variable as read will be of type character and must be converted to the Date
type with the R as.Date() function. R initially reads all characters in a csv file as
type character, then R converts the variables into different types. The many date
formats, however, preclude a straightforward, automatic solution.
R has a default date format, the ISO 8601 international standard: a four-digit year, a
hyphen, a two-digit month, a hyphen, and then a two-digit day. For example, express
the first day of Sept, 2022 as "2022-09-01". Dates expressed as character strings
in this format, or with forward slashes instead of hyphens, need no explicit format
parameter in the call to as.Date(). All other date representations embedded in a5.5. ANALYSIS PROBLEMS 97
character string require the format parameter for R to transform to an R date.
Listing 5.5 presents a variety of character string expressions of a single date, Sept
1, 2022. The output of each of these function calls output to the same date,
"2022-09-01".
as.Date("2022-09-01")
as.Date("2022.09.01", format="\%Y.\%m.\%d")
as.Date("09/01/2022", format="\%m/\%d/\%Y")
as.Date("9/1/22", format="\%m/\%d/\%y")
as.Date("September 1, 2022", format="\%B \%d, \%Y")
as.Date("Sep 1, 2022", format="\%b \%d, \%Y")
as.Date("20220901", format="\%Y\%m\%d")
Listing 5.5: Some alternate expressions of dates in character strings that decode to
"2022-09-01" with a call to as.Date() given the proper format.
For example, if the date is read as the character string "09/01/2022" into the variable
my_date, then convert with the following.
d$my_date <- as.Date(d$my_date, format="%m/%d/%Y")
To view the complete set of format codes, enter: ?strftime.
5.5 Analysis Problems
1. Consider the Cars93 data set, available from within the lessR package.
?dataCars93 for
d <- Read("Cars93") more information.
One of the variables in this data set is MPGcity.
a. What is the sample mean and standard deviation of city MPG?
b. Provide the histogram of city MPG.
c. Provide the density curve of city MPG.
d. Provide a box plot of city MPG. Are there are any outliers and potential
outliers?
e. Provide a scatter plot (dot plot) of city MPG.
f. Describe the distribution of city MPG.
2. Consider the Mach4 data set, available from within the lessR package.
?Mach4 for more
d <- Read("Mach4") information.
Now consider the ninth item, named m09, scored on a 6-point Likert scale from 0 for
Strongly Disagree to 5 for Strongly Agree.
9. All in all, it is better to be humble and honest than to be important and dishonest.98 CHAPTER 5. CONTINUOUS VARIABLES
a. What is the sample mean and standard deviation of m09?
b. Provide the histogram of m09.
c. Provide the density curve of m09.
d. Provide a box plot of m09. Are there are any outliers and potential outliers?
e. Provide a scatter plot (or dot plot) of m09.
f. Describe the distribution of m09.
3. R provides several functions for simulating data. The function rnorm generates n
simulated data values randomly sampled from a normal distribution with a specified
population mean and population standard deviation.
rnorm(n= , mean= , sd= )
When using the function, fill in the three blanks for the three respective values.
a. Generate 20 randomly sampled values from a normal distribution with µ = 50
and σ = 10.
b. Generate another 20 values and store the into a data vector for later analysis
with the R assignment statement. Call the data vector Y.
Y <- rnorm( ... )
c. List the simulated data values with Y.
d. Display their histogram. Describe the result.
e. Compare the sample mean and sample standard deviation to their population
counterparts. Are the sample values equal to the corresponding population
values? Why or why not?
f. Display their run chart. Describe the result.Chapter 6
Statistics
6.1 Quick Start
Install R and lessR as explained in Section 1.2. At the beginning of each R session,
access the lessR functions with library("lessR"). From Chapter 2, read your
data into an R data frame such as d with d <- Read("") to browse for the data file.
Or, locate your data file with a path name or web address between the quotes.
This chapter first presents a variety of statistics for describing sample data. parametric statistics,
Section 6.2.1, p. 100 These
statistics include parametric statistics – mean, standard deviation, skewness and
kurtosis – and statistics based on the ordering of the data – quantiles such as the
median and the interquartile range.
order statistics,
Beyond describing data, analysis typically involves Section 6.2.2, p. 106
inferential statistics, generalizing the results of the data sampled from the population
to the entire population. This chapter shows how to estimate the value of an unknown
population mean given a sample from that population, as well as a special case of the
mean, the proportion.
pivot() provides summary statistics to describe both numerical and categorical vari￾ables. For a continuous variable, pivot() provides a variety of statistical summaries.
Here, compute the mean and standard deviation of variable Y.
pivot(d, c(mean, sd), Y)
To obtain the summary statistics for a variable from pivot() at each level of a
categorical variable, use the by parameter. Statistics by category,
Section 6.2.4, p. 109
In this example, compute the mean and
standard deviation of Y for each level of Gender, such as Man, Woman, and Other.
pivot(d, c(mean, sd), Y, by=Gender)
one-sample t-test,
Section 6.3, p. 110
Evaluate the mean of a single sample with the inferential statistics of a confidence
interval and a hypothesis test to compare the sample mean to some pre-existing
reference value. For example, is it reasonable to consider that the average Salary for
a given company is $75,000?
ttest(Salary, mu=75000)
99100 CHAPTER 6. STATISTICS
What applies to a mean or mean difference applies to proportions, which are special
cases of means when the data for a variable have only two possibilities, scored as
0 and 1.
one proportion,
Section 6.4, p. 121 One possibility evaluates a single sample proportion against a specified
population value, such as the following to evaluate the proportion of women in the
group against the alternative of a population proportion of 0.4.
Prop_test(Gender, success="W", pi=0.4, alternative="greater")
Proportions can also be analyzed from previously computed frequencies or proportions
if the original data are unavailable.
6.2 Types of Summary Statistics
After organizing and cleaning the data to prepare for analysis, the first analyses
include computing summary statistics.
summary
statistics or
descriptive
statistics: Describe
characteristics of
sample data values.
A statistic that summarizes or describes
characteristics of a distribution of sample data values is a summary statistic. Another
phrase for the same concept is a descriptive statistic.
sample size
statistics: Number
of data values
present, n, and
absent (missing), na.
Three different types of summary statistics describe a distribution: Sample size
information, parametric statistics, and order statistics. The sample size information
is the number of data values and the number of missing data values. The other types
of statistics are described next. Generally report all three types of statistics for each
variable of interest plus a visualization, such as a bar chart for a categorical variable
or a histogram for a continuous variable.
parametric 6.2.1 Parametric Statistics
statistics: Apply
only to data values
of interval or ratio
quality.
Parametric statistics apply to variables with at least interval quality data values.
Interval data have equal intervals of magnitude between any two values one unit
apart. For example, the distance between 5
◦C and 6
◦C is the same magnitude of
temperature as between 25◦C and 26◦C. Numbers that represent a lower quality of
measurement only specify a rank ordering of the data values.
interval data,
Section 2.2.1, p. 23
Arithmetic Mean mean, sample, m, or
population, µ:
Arithmetic average,
sum of a set of
numerical values
divided by the
number of values.
Define the mean as the sum of all data values divided by the total number of non￾missing data values. Informally, the sample mean is the average, or more precisely,
the arithmetic mean. Use m to denote the sample mean. To denote the sample
mean of variable Y , subscript accordingly, mY .
Σ notation: Sum a
set of numbers, such
as a column in a
data table.
Use Σ to indicate a sum of a list of
numerical data values and Yi to indicate a specific data value of variable Y.
Sample mean: mY =
P Yi
n
The corresponding letter in the Greek alphabet, mu or µ, denotes the typically
unknown population mean of the entire population from which the data was sampled.
The mean is one of several potential indicators of the middle of the distribution, an
essential concept to describe and understand a distribution of data values. The mean6.2. TYPES OF SUMMARY STATISTICS 101
represents the middle of a distribution as the balance point of a distribution of data
values.
deviation score:
Distance a data value
lies from the mean. The concept of balance point follows from the concept of a deviation score,
the distance of a data value, Yi
, from the mean the data values, mY .
balance point of a
distribution of values:
Value that balances
the positive
deviations from the
mean against the
negative deviations.
Deviation score: Yi − mY
To illustrate, consider a small distribution of n = 5 data values (scores) with a mean
of 6, and their corresponding deviation scores in Table 6.1. For example, the first
data value, Y1 = 4, lies 2 units below mY = 6, for a deviation score of 4 − 6 = −2.
Y 4 4 5 7 10
Y − mY -2 -2 -1 1 4
Table 6.1: A distribution of values, Y , with mY = 6 and deviation scores, Y − mY .
Build a physical model with a fulcrum and a platform with a number line centered on
the fulcrum at 0. Place a 1 lb weight on the platform at the value of each deviation
score. The result in Figure 6.1 is a perfectly balanced platform.
-5 -4 -3 -2 -1 0 1 2 3 4 5
Figure 6.1: The mean as the balance point of a distribution.
The force on the platform from the negative deviation scores precisely balances the
force from the positive deviation scores. All the deviation scores sum to 0, where Σ
indicates the sum of all the values:
Sum of the deviation scores: Σ(Yi − mY ) = 0
The mean indicates the middle of a distribution, defining “middle” as the value that
balances the negative deviations against the positive deviations. As shown next, the
concept of a deviation score is the basis for defining other statistics and is essential
to the concept of a normal curve. Indeed, much of statistical analysis, including most
of the topics in this book, follows from the analysis of deviation scores.
Standard Deviation standard
deviation purpose:
Indicator of
variability
The concept. The sample standard deviation indicates the extent of variability of a
distribution of data values about its mean as shown in Figure 6.2.
A :
0 50 100 150 200
10 190
range = max – min = 190 – 10 = 180
B :
0 50 100 150 200
Figure 6.2: Two distributions of n = 7 with the same range, 180, and mean, 100, but
different standard deviations, 53.5 (A) vs 85.1 (B).102 CHAPTER 6. STATISTICS
If the standard deviation is large, the values tend to spread out much about the mean.
range: Distance
between the
maximum and
minimum values of a
distribution.
As shown in Figure 6.2, the standard deviation can differ across two distributions
with the same mean and the same range, the difference between the maximum and
minimum values. Though both distributions have a minimum value of 10 and a
maximum value of 190, for Distribution A, the remaining values cluster around the
mean. On the contrary, for Distribution B, the remaining values are more dispersed,
closer to the minimum or maximum values. The resulting standard deviations are
53.5 versus 85.1, respectively.
How to define the standard deviation to indicate variability from deviation scores
about the mean? The deviation scores sum to zero, so the mean deviation score is
always zero. To derive a variability index based on the mean requires an additional
operation to remove their minus signs, such as squaring them. sum of squares of
variable Y , SSY:
Sum of squared
deviation scores.
After squaring each
deviation score, sum them, the sum of the squared deviation scores, or, more simply,
the sum of squares. Denote the sum of squares for variable Y as SSY, the core
concept for describing the variability of a continuous variable. Table 6.2 step-by￾step constructs the expression for the sample standard deviation, s, or explicitly for
variable Y , sY , beginning with these steps to obtain SSY.
Symbol Expression Meaning
Yi a data value
Yi − mY deviation of the data value from the mean
(Yi − mY )
2
squared deviation of the data value from the mean
SSY Σ(Yi − mY )
2
sum of all squared deviations from the mean
MSY or s
2 Σ(Yi − mY )
2
n − 1
mean of squared deviations from the mean based on df
s
r
Σ(Yi − mY )
2
n − 1
square root of the mean squared deviations based on df
Table 6.2: Conceptual expression of the variance, s
2 and the standard deviation, s.
From the sum of squares, SSY, compute its mean, the mean squared deviation of
Y , MSY. variance or mean
square, s
2
: Mean
squared deviation
score, MSY.
The mean square is the variance statistic, which indicates variability. The
variance is smaller for less variable, less dispersed distributions of data values and
larger for more variable, more dispersed distributions.
However, the variance is expressed in squared units, squared meters instead of meters,
squared dollars instead of dollars. To derive a more meaningful descriptive statistic,
un-square the mean by taking the square root to return to the original metric.
standard
deviation, sample,
s, or population, σ:
Square root of the
mean deviation score.
The
square root of the mean squared deviation about the mean is the standard deviation,
which represents the size of the typical deviation score. For example, referring back
to Table 6.1, the five deviation scores are −2, −2, −1, 1, 4. Their standard deviation
is not 100, or 50, or even 10, but instead 2.55, a single value representative of
the five deviation scores. Denote the sample standard deviation as s. Denote the
corresponding, and usually unknown, value of the entire population as the Greek
version of the letter s, sigma or σ.6.2. TYPES OF SUMMARY STATISTICS 103
However, when we take the mean of the squared deviations, we divide by one less
than the number of values, n − 1, the degrees of freedom.
degrees of
freedom, df : For
the standard
deviation, number of
data values free to
vary after knowing
the sample mean,
n − 1.
Why? We first need
the mean to then calculate the standard deviation. We would calculate the mean
squared deviation score with the usual sample size, n, if we had the population mean.
However, we lack all population values and so rely on sample estimates. Without the
population mean, we first pass through the data to calculate the sample mean, mY .
Then, we make a second pass through the same data to calculate the deviation score
for each data value from that sample mean. However, once we know mY after the
first pass, if we know the value of any n − 1 of the data values, we can calculate the
remaining n
th data value, its value determined by the other n − 1 values and mY .
That second pass through the sample data with mY computed from the first pass
effectively reduces the sample size, n, by 1, resulting in n − 1, the degrees of freedom.
To illustrate the computation of s, expand Table 6.1 beyond deviation scores to
include the sum of squared deviations, SSY, which leads to the variance and the
standard deviation. As always, Σ refers to a sum and √ refers to the square root.
Because all data values in the sample are needed before knowing the value of mY , the
degrees of freedom, df, for the mean is the sample size, n. As discussed, the degrees
of freedom for the standard deviation is n − 1. The result is Table 6.3, a map of how
to manually calculate the standard deviation, such as with a worksheet app.
Σ df Σ
df
√
Y 4 4 5 7 10 30 5 6
Y − mY -2 -2 -1 1 4 0 4 0 0
(Y − mY )
2 4 4 1 1 16 26 4 6.5 2.55
Table 6.3: Computation of the sample mean, mY = 6, sample variance, s
2 = 6.5, and
sample standard deviation, sY = 2.55.
Fortunately, modern computers, even inexpensive laptops, typically perform the
standard deviation calculations in fractions of a second, even for large numbers of
data values. For example, the most inexpensive 2020 M1 MacBook Air computes
the standard deviation with the R function sd() of 1,000,000 simulated normal
distribution data values in only 0.0031 seconds.1 Even a low-end 2018 Windows
laptop required only 0.0046 seconds. Those speeds are not just fast, they are blazing. normal curve:
Bell-shaped
distribution that
describes so many
real-life distributions
and theoretical
distributions of
sample values over
multiple samples.
Relation to the normal curve. Based on squared deviations, the standard deviation
relates to normally distributed values. A specific, perfect (population) normal curve
depends on the population values that define its location, the mean µ, and variability
or width, the standard deviation σ. The mathematical formula for a perfect normal
curve provides the point on the curve for a given value of Y :
f(Y ) = 1
σ
√
2π
e
− (Y − µ)
2
2σ
2
One notable characteristic of that formula is that it involves the mathematical
constants π and e of interest in so many fields of mathematics. More to our purposes,
1The timing was accomplished with the microbenchmark() function as part of the package with
the same name.104 CHAPTER 6. STATISTICS
the equation for the normal curve includes the squared deviation score for the value
of Y . Squared deviation scores are also the basis of a normal distribution!
This inter-connectedness of the standard deviation with the normal distribution leads
to the deep, essential mathematical relationship shown in Figure 6.3. About 68.3% of
all the data values are within one standard deviation of the mean for any distribution
of normally distributed data. About 95.5% of all data values are within 2 standard
standardization, deviations of the mean.
Section 3.3.2, p. 47
The cutoff for precisely 95% of a normal distribution of value
is 1.96 standard deviation on either side of the mean, µ, a standard or z-score of 1.96.
“Normal” really does mean normal in the sense that not too many values are “weird”.
Figure 6.3: Perfect normal distribution of Y with standard scores Z showing relationship
to the population standard deviation, σ, and the population mean, µ.
For example, height is normally distributed. The mean height is about 65.5" for
USA women, with a standard deviation of about 2.5". Two standard deviations equal
(2)(2.5) or 5, so a little more than 95% of all USA women have a height between:
65.5" − 5 = 60.5" and 65" + 5 = 70.5"
This relationship between the standard deviation and the normal distribution lies at
the core of much statistical analysis.
There are as many normal curves as values for the population mean, µ, and standard
deviation, σ, an indefinitely large number of potential values for each. The power of
the relationship between the standard deviation and a normal distribution shown in
Figure 6.3 extends to all normal curves. The relationship applies for all values of µ
and σ. A normal curve with σ = 1 is considerably “skinnier” than a normal curve
with σ = 10, but the same normal curve probabilities apply to both distributions.
Skewness and Kurtosis
The mean describes the center of a distribution of a continuous variable as its
balance point of deviation scores. The standard deviation describes variability about
that mean in terms of squared deviation scores, the 2nd power. Other distribution
properties also follow from the deviation score, their 3rd , and the 4th power.
Skewness indicates the lack of symmetry of a distribution of values. skewness: Indicator
of (lack of)
distribution
symmetry.
For a symmetric
distribution, such as the normal curve, the right side of the distribution is a mirror
image of the distribution’s left side. A symmetric distribution has a skewness of 0.
Negative skewness values indicate that the distribution tends to have a tail on the
left side and positive skewness values indicate a tail on the right side.6.2. TYPES OF SUMMARY STATISTICS 105
Different formulas exist that operationalize skewness differently than the mean and
standard deviation. The formula typically found in data analysis apps, including
Excel and SPSS, is G1 or Fisher’s skewness. Fisher’s skewness:
Average of the cube
of the standardized
values of a
distribution.
Just as the computation of the standard
deviation, s, is adjusted for sample size, n−1, the computation of sample skew is also
adjusted by a sample size coefficient, the ratio of n to the product of (n − 1)(n − 2).
As with s, this adjustment becomes less influential as sample sizes increases.
G1 =
n
(n − 1)(n − 2)
X
Yi − mY
s
3
The concept of skew for G1 includes the deviation score as G1 is the average of the
cube of the standardized values, corrected for degrees of freedom.
standardized score or
z-score, Section 3.3.2,
p. 47
Figure 6.4 applies G1 to three different distributions.
Figure 6.4: Three distributions with varying amounts of positive skew assessed by G1.
The first distribution is almost perfectly symmetric, with G1 close to 0. The middle
distribution indicates moderate skew to the right with G1 = 0.74. The third distri￾bution is considerably skewed to the right, with G1 = 2.56. Values of skew close
to 0 indicate little, if any, skew. As an informal heuristic, values of skew between
−1 to − 0.5 and 0.5 to 1 are generally considered moderate skew. Values larger than
2 or smaller than −2 indicate considerable skew.
kurtosis: Indicator
of distribution
peakedness.
Kurtosis indicates a distribution’s peakedness relative to the normal curve. Skewness
focuses on extreme values in one tail of the distribution, whereas kurtosis focuses on
extreme values in both tails. Different formulas scale kurtosis differently. For the
values reported by lessR, a kurtosis of 0 is consistent with a normal distribution.
Sometimes this scaling is referred to as excess kurtosis.
excess kurtosis:
The kurtosis statistic
is scaled so that 0 is
the reference for a
normal distribution. In the population, define kurtosis as the average of the standardized values raised to
the fourth power. standardization,
Section 3.3.2, p. 47 population average of: 
Yi − µ
σ
4
Because of the need to correct the corresponding sample formula for computing
kurtosis, there is even more complexity than the corresponding formula for skewness,
with little value for repeating the computational details here.
A positive kurtosis distinct from 0 indicates a peaked distribution with some of the
values more spread to extreme values than with the normal. More values will lie more
than two standard deviations from the mean than with a normal distribution. The
distribution with positive kurtosis is “skinny” because the extreme values result in
most of the data appearing in a narrow range about the mean. A negative kurtosis106 CHAPTER 6. STATISTICS
value not too close to 0 indicates that the values are less concentrated around the
mean, resulting in a flatter distribution than the normal. These distributions have
fewer extreme values than the normal.
6.2.2 Order Statistics
order statistic:
Specify position in
an ordered set of
data values.
As indicated, the parametric statistics just discussed require that the data values
be at least interval quality. Order statistics, a class of non-parametric statistics,
only require that the data values are ordinal and so apply to a broader range of
distributions of data values than parametric statistics. An order statistic specifies
some characteristic of the position of a specific value within that distribution, which
may or may not be an actual data value. Compute an order statistic after sorting
the values of a variable from the smallest value to the largest.
median: The value
midway in an
ordered distribution.
A frequently reported order statistic is the median, a value midway between the
smallest and largest values of the sorted distribution. A distribution with an odd
number of values has a data value in the middle. For a distribution with an even
number of values, the median is generally not a data value but the mean of the two
values closest to the middle.
Order statistics are resistant to outliers relative to parameter statistics such as the
mean. An extreme value can have a disproportionate influence on the mean and no
impact on an order statistic. To illustrate, return to the sorted values of Distribution A
from Figure 6.2:
10, 80, 90, 100, 110, 120, 190
The mean and the median of these seven values are 100, with three values below
100 and three values above. Suppose the maximum value of the distribution was
1900 instead of 190, changing a symmetric distribution to a strong right-skewed
distribution.
10, 80, 90, 100, 110, 120, 1900
The median remains unchanged, completely insensitive to the value so discrepant
from the remaining values. However, the mean increases from 100 to 359. In this
new distribution, 359 does not represent any value in the distribution, but it is the
one balance point as previously defined.
balance point of a
distribution,
Section 6.1, p. 101
Report both the mean and median of a variable. The mean equals the median for a
symmetric distribution such as the normal curve. A median shifted far to the right
or left of the mean indicates a skewed distribution. Knowing each of these values
and their comparison provides helpful information for understanding the essential
characteristics of the distribution.
quartile: A value
that separates the
values of an ordered
distribution into four
equal parts.
The median requires one split of the sorted data, but any number of splits can be
made. For example, quartiles split the ordered distribution into four equal parts:
1
st , 2nd , 3rd , and 4th quarters.
The median is the second quartile. The first quartile cuts off the bottom 25% of the
distribution, and the third quartile cuts off the bottom 75%. Extend the concept
further, dividing the sorted distribution into 5, 10, or 100 equal parts with quintiles,6.2. TYPES OF SUMMARY STATISTICS 107
deciles, or percentiles, respectively. The general term for quartiles, quintiles, etc., is
quantiles.
quantile: A position
in a sorted
distribution that
splits the distribution
in values < and >
than the position.
IQR: Interquartile
range, difference
between 1st and 3rd
quartiles.
The most common order statistic for expressing variability is the interquartile range
or IQR , the difference between the 3rd and 1st quartiles of a distribution. The IQR
specifies the range of the middle 50% of the data values, centered on the median. The
IQR is valuable as a statistic, but its most common application is the visualization of
a box plot for a distribution, presented later in the chapter.
box plot,
6.2.3 Obtain the Statistics Section 5.3.1, p. 90
Analyses of a variable should include both data visualizations and essential summary
statistics.
Scenario. Obtain summary statistics
Obtain summary statistics for variables Weight and Height in the BodyMeas data set,
both parametric and non-parametric, that numerically describe key characteristics
of the distribution of the variables across the 340 people in the data set.
Statistics from Histogram()
One source of summary statistics for a continuous variable is the lessR histogram
function. The summary statistics displayed by Histogram() include: Total sample
size, the number of missing values, the mean, standard deviation, minimum, median,
and maximum.
d <- Read("BodyMeas")
Histogram(Weight)
--- Weight ---
n miss mean sd min mdn max
340 0 182.09 52.86 102.00 175.00 485.00
Listing 6.1: Summary statistics from Histogram() for Weight.
These summary or descriptive statistics are the basics, but many other statistics also
can be reported.
Statistics from pivot()
pivot() function,
lessR: Calculate
summary statistics.
The specialized lessR function for summary statistics is pivot(), which provides
more flexibility and computed statistics than those available from Histogram().
Specify the parameters of the function call according to the following pattern: data
frame, statistics to compute, and variable.
pivot(d, c(mean, sd), Y)108 CHAPTER 6. STATISTICS
If the parameter values are entered in this order, then the parameter names do not
need to be entered. Multiple statistics, multiple variables, and multiple grouping
variables can be analyzed, though only two of the three parameters can have multiple
values per function call. Specify a list of values for a parameter as a vector with the
R function c(). The data set parameter, included first in the function call, has no
default name for the input data frame and so must always be included in the function
all to pivot().
pivot() can access any R function that transforms a column of numerical data
values into a single number, including the many R provided statistical functions and
also user-defined functions. The exception is that for continuous variables pivot()
accommodates the R function quantile() that returns as many quantiles as requested.
The default number is 4, which yields quartiles. The quantile() function displays
output in percentiles. For example, the 25th percentile is the first quartile, and the
100th percentile is the maximum value of the distribution.
The following example in Listing 6.2 shows the calculation of multiple summary
c(), Section 1.2.9, statistics for two variables, Height and Weight, including the quartiles.
p. 12
pivot(d, c(mean, median, sd, IQR, skew, kurtosis, quantile),
c(Height, Weight)
n na mean median sd IQR skew kurtosis p0 p25 p50 p75 p100
Height 340 0 68.409 68 4.101 7.0 0.097 -0.835 59 65 68 72.0 78
Weight 340 0 182.085 175 52.863 70.5 1.290 3.243 102 140 175 210.5 485
Listing 6.2: Compute multiple parametric and order summary statistics for Height in inches
and Weight in lbs.
The median for each variable appears twice in Listing 6.2, under the column median
and also in the column for the 50th percentile, the column labeled p50. Find the
minimum and maximum values for each variable under the p0 and p100 columns, for
the 0th and 100th percentile, respectively.
Much information can be discerned from the output in Listing 6.2. Regarding Height,
the mean, 68.409, and median of 68 are almost equal, so skew is close to 0, at 0.097.
The quartiles are approximately evenly spread about the median. We conclude that
IQR, Section 6.2.2, the Height is a symmetric distribution with values from 59" to 78".
p. 107
The IQR indicates
that 50% of the values lie within a range of 7" about the median.
The distribution of Weight, on the contrary, is skewed. The mean, 182.085, is
considerably greater than the median of 175, indicating a right-skew that “pulls”
the mean to be larger than the median. Moreover, the skewness coefficient is 1.29,
considerably larger than 0 and larger than 1, indicating skew in the right-tail of the
distribution. Further evidence of positive skew is that the maximum value of Weight,
485, is much larger than the 3rd quartile, 210.5.
q_num parameter:
Number of quantiles
to display.
To display quantiles other than the default quartiles, specify another value of the
q_num parameter, such as 5 for quintiles or 10 for deciles. Listing 6.3 displays the
deciles of both Height and Weight.6.2. TYPES OF SUMMARY STATISTICS 109
pivot(d, quantile, c(Height, Weight), q_num=10)
_n _na p0 p10 p20 p30 p40 p50 p60 p70 p80 p90 p100
Height 340 0 59 63 65 66 67 68 70 71 72.0 74 78
Weight 340 0 102 125 137 145 160 175 190 200 224.2 250 485
Listing 6.3: Deciles for Height in inches and Weight in lbs.
The deciles provide more evidence of the right-tailed skew for the distribution of
Weight. The distance from the 10th percentile, 125, to the median, 175, is 50 lbs.
The distance from the median to the 90th percentile, 250, is 75 lbs. The values of
Weight are more spread out to the right-side tail.
The visualizations of the distributions in Figure 6.5 of Height and Weight reinforce
our previous conclusions from the descriptive statistics.
Figure 6.5: Histograms of Height and Weight from the BodyMeas data set.
Weight does exhibit a right-skew with some hefty people represented in the data set.
Instead of a peak at the mean, the distribution of Height tends to be flatter than
the normal distribution, more bunched up around the middle. The negative value of
kurtosis for Height, −0.835, quantifies this lack of peakedness.
6.2.4 Data Aggregation
Categorical variables in a data set can group rows of data, such as separately
identifying rows of data for men and for women. The summary statistics may be of
interest for a numerical variable computed separately for each group. Aggregation
splits the sample rows of data into groups and then separately computes statistics for
each group.
aggregation:
Partition data into
groups to separately
compute statistics for
each group.
The result of the aggregation is a summary table of statistics for each
group computed over one or more specified variables. Excel refers to such a summary
table as a pivot table, the motivation for the name of the lessR function pivot().
Scenario. How does Height and Weight vary by Gender?
Display the summary statistics of Height and Weight for the two Genders in the
data set.
by parameter, lessR:
Indicate a categorical
variable from which
to analyze the
different levels.
To aggregate data with pivot(), use the by parameter to reference one or more
categorical variables to define the groups, as shown in Listing 6.4, which references
one categorical variable, Gender.110 CHAPTER 6. STATISTICS
pivot(d, mean, c(Height, Weight), by=Gender)
Gender Height_n Height_na Height_mean Weight_n Weight_na Weight_mean
1 M 170 0 71.353 170 0 215.759
2 W 170 0 65.465 170 0 148.412
Listing 6.4: Sample size information, and the mean Height in inches and Weight in lbs.
For this sample of motorcycle riders, on average, the men are over 71" tall versus
about 65 1
2
" for women. Regarding Weight, men average about 215 3
4
lbs versus about
148.4 lbs for women. These differences are real – in this sample. The question of
greater interest for most analyses is if these sample differences generalize to the entire
compare two groups, population of men and women, in this case, motorcycle riders.
Section 7.2, p. 128
Investigate more
formally the potential gender differences with inferential analyses.
Employee data table,
Figure 1.5, p. 13
Another example of pivot() returns to the employee data set. Listing 6.5 shows the
summary statistics of Salary as they vary across the five different departments.
pivot(d, c(mean, sd, median, IQR), Salary, by=Dept)
Dept Salary_n Salary_na Salary_mean Salary_sd Salary_md Salary_IQR
1 ACCT 5 0 61792.78 12774.61 69547.60 21379.23
2 ADMN 6 0 81277.12 27585.15 71058.60 36120.57
3 FINC 4 0 69010.68 17852.50 61937.62 16034.81
4 MKTG 6 0 70257.13 19869.81 61658.99 26085.69
5 SALE 15 0 78830.07 23476.84 77714.85 28810.28
6 <NA> 1 0 53772.58 NA 53772.58 0.00
Listing 6.5: Brief version of summary statistics of Salary for each level of Dept.
Observe each summary statistic and compare the summary statistics across the
different groups, one group for each value of the categorical variable. Of course, these
observations apply only to this particular data set for the 37 employees. Evaluate
the population value of a single mean with a one-sample t-test. one-sample t-test,
Section 6.3, p. 110 Do an inferential
comparison of two or more means to analyze the more relevant population mean
comparisons. two-sample t-test,
Section 7.2, p. 128 Invoke a t-test of the mean difference to compare two group means
and, to compare multiple group means such as in Listing 6.5, a one-way analysis of
variance, one-way ANOVA.
one-way ANOVA,
Section 8.3, p. 153
6.3 Evaluate a Single Mean
research design:
The procedures for
collecting and
analyzing one or
more samples of data.
Organize your analysis according to a research design that specifies how to obtain the
measurements of people or whatever is the unit of analysis. For the design considered
here, analyze the data values from a single sample based on the measurements of a
single variable of interest. One characteristic of interest of this sample is its mean, m,
more informally, the average. What is the average score on the class midterm? What
is the average score on an anxiety scale administered to clients about to undergo a
specific type of therapy? What proportion of voters intend to vote for a particular
candidate?6.3. EVALUATE A SINGLE MEAN 111
6.3.1 Description
Mach IV scale,
Listing 1.5, p. 17
Consider the Mach IV scale responses for Item m07, numerically encoded from 0 to 5,
with a 0 indicating Strongly Disagree and a 5 Strongly Agree.
There is no excuse for lying to someone else.
Histogram() function,
Section 5.2, p. 83
variable labels,
Section 2.5, p. 34
Begin the analysis of Item m07 by describing the sample results using the Histogram()
function. First, read the data from the internal lessR data set Mach4 and the
corresponding file of variable labels, the 20 Mach4 items.
MachIV data,
Section 1.3.2, p. 16 d <- Read("Mach4")
l <- Read("Mach4_lbl", var_labels=TRUE)
Because of the small number of scale points assessing the underlying continuous
variable, the default histogram is problematic. The default bin width is only 0.5.
Instead, set the parameter bin_start at −.05 and bin_width parameter to 1 to
center each bin over its respective scale value. Figure 6.6 shows the resulting histogram
of the responses of the 351 respondents.
Histogram(m07, bin_start=-0.5, bin_width=1)
Figure 6.6: Histogram for m07, the 7th Mach IV item, not reverse scored.
The histogram indicates that the responses vary across the possible range of values
from 0 to 5. Values below 2.5 indicate Disagreement, and values above 2.5 indicate
Agreement. Summary statistics from Histogram() in Listing 6.6 show a sample mean
of 2.77, close to but larger than the midpoint of 2.5. sample mean,
Section 6.2.1, p. 100
--- m07: There is no excuse for lying to someone else ---
n miss mean sd min mdn max
351 0 2.77 1.47 0.00 3.00 5.00
Listing 6.6: Summary statistics for m07, the 7th Mach IV item.
For this sample of responses to m07, the mean of m = 2.77 is 0.27 units larger than
the neutral point of 2.5. But does this result apply beyond this specific sample?112 CHAPTER 6. STATISTICS
6.3.2 Basis of Inference
The primary conclusions of a research project generally focus on inferential statistics,
that is, an understanding of the population as a whole and not just an arbitrary
sample from that population. Does the one sample outcome of a sample mean greater
than 2.5 generalize to the entire population? Would the mean of subsequent samples
from the same population generally be in the Agree region? Is the true mean response
to Item m07 in the Agreement region?
Problem of Sampling Variability
Flip a fair coin ten times. Count the number of Heads in the sample of ten flips.
Score a Head a 1 and a Tail a 0 so that the mean of the ten flips is the proportion of
heads. You might get six heads from those 10 flips, which leads to a sample mean of
m = 6/10 = 0.6. Flip the same coin 10 more times, and you might get four heads,
m = 4/10 = 0.4. Which sample mean is correct? Trick question. Each mean correctly
describes the sample from which it is computed, but neither result is the correct
population value we seek to understand regarding the coin.
The true or expected number of heads from a fair coin is best described in terms
of what occurs over many, many flips, the long-run average. What is true, what is
stable, is the average computed over the entire population of coin flips, the population
mean.
population mean:
The usually unknown
value of the mean of
variable over the
entire population
from which a sample
is derived.
Coin flipping is an excellent pedagogical illustration of stability and variation
across samples because the process is well understood, and the long-run average for a
fair coin across an indefinitely large number of flips is known – not the usual situation
in data analysis.
µY : The population
mean of variable Y .
Designate the population mean with µ. For a fair coin, we know the value of µ
because half of the outcomes over an indefinitely large number of flips are heads,
µ = 0.5. However, the result of any one sample generally only approximates the
population value, m ≈ 0.5. For example, the probability of getting six heads when a
fair coin is flipped 10 times is 0.205, more than 1/5. The probability of seven heads
is 0.117, more than 1/10. In the presence of this sampling fluctuation, the true mean
of µ = 0.5 remains stable for every single flip of the coin and for every sample mean
from a collection of those coin flips – but the sample results vary.
What applies to coin flips applies to data analysis in general. Every data set is a
sample from a larger population. Generally only collect one sample, the largest one
you can get. However, IF you took another sample, just as with coin flips, you would
get a different value from the same statistical computation, such as a different sample
mean. For the Machiavellian statistics reported in Listing 6.6, m = 2.77. However,
IF we had a second sample even of the same size, we would have two different values
of m. And neither sample mean would equal the true, population mean.
Sample results are not of primary interest because the sample values are unique to
that specific sample.
sampling error: A
sample statistic, such
as the sample mean,
m, generally yields a
different value for
each sample from
which it is computed.
The random variation of sample results from sample to sample
is called sampling error. Every sample value and every sample mean computed from
multiple values reflects the true underlying mean, µ, plus the impact of random
sampling error.6.3. EVALUATE A SINGLE MEAN 113
Sampling fluctuations motivate the need for statistical inference. In the presence of
this random variability across samples, data analysis is the pursuit of corresponding
underlying stable population values for the variable of interest that persist across
samples. inferential
statistics: Analysis
of sample data that
provides information
regarding the true
population values.
The problem is that we only have access to a somewhat arbitrary sample
which is not a perfect representation of the whole population. We can only compute
sample statistics, such as the sample mean, m. Fortunately, we have inferential
statistics that use data from the sample to estimate the corresponding population
value, such as the mean indicated by the Greek letter µ for the variable of interest.
Distribution of the Sample Mean
If we cannot gain direct knowledge of population values such as µ for a variable of
interest Y , how can we learn about the values of these values? To gain this knowledge,
we need to know how the corresponding statistic such as m behaves over multiple
samples, as if we computed the sample mean, m, for each of many samples. We
typically only have one sample but what if we had access to many?
Because the value of a statistic changes over repeated samples, a sample statistic
becomes a variable when computed over multiple samples.
sampling
distribution:
Usually hypothetical
distribution of a
statistic computed
over multiple
samples.
As with any distribution
of values, the distribution of m has its own shape, location (mean), and variability
(standard deviation). The distribution of the statistic over repeated samples, its
sampling distribution, is the key to statistical inference.
The more the statistic fluctuates over repeated samples, the larger its standard
deviation, and the more error likely results for the estimation of the true mean, µ,
from any one sample.
standard deviation,
Section 6.2.1, p. 101 If, over many samples, all the values of m are close to one
another, then any one sample value is likely close to the true mean, µ. On the
contrary, if the values of m are distributed over an extensive range of values, then
any one value of m that we observe may be close or far away from the true value µ.
However, we typically only have one sample. With only one sample, how can we know
the sampling distribution of a statistic to estimate how close our one sample result
likely is to its corresponding true, population value? The mathematicians provide
one answer.
bootstrap samples, an
alternative
estimation of a
sampling
distribution,
Section 14.4.3, p. 315
The magic of one of their formulas estimates how much the statistic
varies over repeated samples from the information contained in only a single sample!
standard error:
Standard deviation
of a statistic over
usually hypothetical
repeated samples.
From a sample of size n for the variable of interest Y , compute the standard deviation
of the data, s. From this information, we can estimate the standard deviation of the
statistic m over multiple samples. We call the standard deviation of a statistic its
standard error, sm, because it indicates the amount of error in estimating µ.
Estimated standard error of m: sm =
s
√
n
To estimate the standard error of a sample mean, m, we need from our one sample
the sample standard deviation, s, and the sample size, n.
The massive implication of this simple formula is that the standard error (deviation)
of the sample mean, m, is less than then standard deviation of the data, Y . More
precisely, the larger the sample size, n, the smaller the variation of m, but with stark
diminishing returns, shown in Figure 6.7.114 CHAPTER 6. STATISTICS
0 200 400 600 800 1000
0
2
4
6
8
10
n: Sample Size
Standard Error of the Sample Mean
for a Data Standard Deviation = 10
Figure 6.7: Standard error of the sample mean as a function of sample size, for an arbitrary
standard deviation of the data equal to 10.
Double a small sample size to obtain a huge decrease in the standard error. Double a
large sample size and the decrease becomes much smaller. The cost of increasing a
sample size from 50 to 100 is likely worthwhile but perhaps not from 950 to 1000.
The shape of the distribution of m can also generally be known.
central limit
theorem: The
sample mean is
approximately
normally distributed
unless a small sample
is taken from a
non-normal, skewed
population.
Remarkably,
regardless of the shape of the distribution of Y , if the sample size is greater than
about 20 or 30, the central limit theorem ensures a normally distributed sample mean
across many hypothetical repeated samples from the same population, each sample
of the same size. If the population data values of the main variable Y are not too
skewed, normality is obtained with sample sizes less than 20. If Y is normal itself,
then the sampling distribution is normal no matter how small the sample.
In Figure 6.8, the value of m for any one single sample from the flatter, more variable
sampling distribution of m is more likely to be farther from the true mean µ than is
a single sample from the taller, less variable distribution.
μ
Distribution of m with a
smaller standard error
Distribution of m with a
larger standard error
Figure 6.8: Two hypothetical normal sampling distributions of m with the same population
mean, µ, but different standard errors (deviations).
The population mean of the sample means is the same population mean of the data.
Both share the same µ. The distribution of values of m centers over µ as does the
distribution of data values. Understanding that a sample statistic becomes a variable
with repeated sampling is key to understanding the logic of inferential statistics.6.3. EVALUATE A SINGLE MEAN 115
Knowing the standard deviation of the sampling distribution, and knowing that it is
normally distributed, allows us to use normal or related t-distribution probabilities
to compute ranges of likely values.
normal curve
probabilities,
Section 6.3, p. 104
inferential
analysis: Gain
information about a
population
parameter with the
hypothesis test and
confidence interval.
From this information, inferential statistics allow
us to generalize from the one sample of data that we collect from the population to
the entire population. What cannot be known directly is estimated. We calculate
the exact value of sample statistics, but we estimate the value of the corresponding
population parameters.
6.3.3 Application
Using the lessR function ttest(), abbreviated tt(), describe the sample and perform
an inferential analysis of the population mean, µ. Use the mu parameter to specify
the reference value for the hypothesis test.
mu parameter,
ttest(): Null
hypothesis value. If mu is not specified, the 95% confidence
interval is calculated without the accompanying hypothesis test.
Objective
Apply inferential statistics of the Machiavellianism data for Item m07 with the two
classical inferential analyses: hypothesis test and confidence interval.
Scenario. Assess level of agreement with Mach IV item m07
Do people, on average, agree or disagree with, that there is no excuse for lying to
someone else?
In this analysis, the population mean value of interest is 2.5, the divide between
Disagreement and Agreement on the response scale scored from 0 to 5 for Item m07.
The population value of interest provides the basis of the corresponding hypothesis
test. For purposes of the test, assume that the true value of µ is 2.5. If so, then the
sample mean m should be reasonably close to 2.5. The hypothesis test evaluates how
close m is to the assumed value of 2.5, the baseline null value in this analysis.
Null hypothesis, H0 : µ = 2.5, with alternative that µ 6= 2.5
Listing 6.7 shows the function call to perform the analysis.
ttest(m07, mu=2.5)
Response Variable: m07, There is no excuse for lying to someone else
Listing 6.7: Input for the one-sample t-test of a mean and first line of output.
The first part of the output of ttest() provides the summary statistics of the variable
of interest, Item m07, already reported in the Histogram() output in Listing 6.6,
including m = 2.77 and s = 1.47. Now proceed to investigate if the true mean of the
response to Item m07 differs from 2.5.
Sampling Distribution
central limit theorem,
Section 6.3.2, p. 114
Before doing the inferential tests, explore the normality assumption of the distribution
of the sample mean, m, based upon the central limit theorem. In this example, the116 CHAPTER 6. STATISTICS
sample size is 351, well beyond the threshold of 30. Accordingly, ttest() informs us
that tests of normality are not needed, as shown in Listing 6.8.
------ Normality Assumption ------
Sample mean assumed normal because n > 30, so no test needed.
Listing 6.8: Consideration of the normality of the response to m07, the 7th Mach IV item.
The beginning of the inferential analysis follows in Listing 6.9, of which the first two
lines are preliminary information for the analysis.
------ Infer ------
t-cutoff for 95% range of variation: tcut = 1.967
Standard Error of Mean: SE = 0.079
Listing 6.9: Preliminary information for statistical inference, used both for the hypothesis
test and the confidence interval.
standard error of the
mean, Section 6.3.2,
p. 113
The standard error of the sample mean, as shown in Listing 6.9, follows.
sm =
s
√
n
=
1.47
√
351
= 0.079
The sample mean, m, has a normal distribution, its width set by its standard error,
sm. Its standardized value, the corresponding z-value from which normal curve
probabilities are computed, is also normal. For hypothesis testing of the mean, we
assume the null hypothesized value of µ, denoted µ0, is the correct value. Locate the
position of the z-value within its sampling distribution as follows.
z =
m − µ0
σm
The difficulty here is that computing normal curve probabilities based on the location
of m requires that we know the actual population standard error, σm, which requires
knowing the population standard deviation, σ.
In practice, we estimate the population standard deviation, σ, using the sample
standard deviation, s, to calculate the estimated standard error, sm. That substitution
changes the statistic from the z-value to the t-value.
t-value or t-statistic,
of the mean: Number
of estimated
standard errors of
the sample mean
from the
hypothesized mean. t =
m − µ0
sm
The t-value, or t-statistic, expresses how many estimated standard errors the one
sample mean is from the hypothesized value. t-distribution:
Distribution of a
t-value over usually
hypothetical
repeated samples for
a given sample size.
Using s adds another source of error
to our probability calculation. Computing a t-value requires to calculate both m
and s. Over many repeated samples for the given sample size, the collection of
t-statistics forms a t-distribution. Account for this change by moving from the normal6.3. EVALUATE A SINGLE MEAN 117
distribution to the related family of t-distributions according to sample size, n, still
bell-shaped but a bit more dispersed than the corresponding normal distribution.
The 2.5% upper-tail cutoff of a normally distributed z-distribution is 1.96 standard
deviations. The corresponding cutoff for a t-distribution depends on the sample size,
but is generally around 2, somewhat larger for small sample sizes. Its lower limit is
1.96, approximated by larger sample sizes that generally provide a better estimate
of σ. The t-cutoff specifies the range of sampling variability of many t-values over
repeated samples, the t-distribution for the given sample size.
t-cutoff: Positive
and negative values
that define the range
of sampling
variability.
Hypothesis Test
Listing 6.9 reports the t-cutoff of the specified confidence level for a default value of
95%. Change the default with parameter conf_level.
conf_level
parameter, ttest():
The confidence level.
Here the positive value of the
cutoff for the two-tailed test is 1.967, which means that 95% of the sample t-statistics
would vary between −1.967 and 1.967 for this sample size if the null hypothesis of
µ = 2.5 is correct.
If µ = 2.5 is true, m should be close to 2.5. How many estimated standard errors is
the sample mean, m, from the hypothesized value of 2.5?
t-value =
m − 2.5
sm
=
2.77 − 2.5
0.079
= 3.497
According to the obtained t-value, the corresponding sample mean of 2.77 is a
considerable 3.497 estimated standard errors from 2.5, well beyond the value of the
95% t-cutoff of 1.967. Listing 6.10 shows the resulting hypothesis test.
Hypothesized Value H0: mu = 2.5
Hypothesis Test of Mean: t-value = 3.497, df = 350, p-value = 0.001
Listing 6.10: Hypothesis test that the true mean is 2.5 for m07, the 7th Mach IV item.
Given the degrees of freedom of df = 350, one less than the sample size of 351, for
t-value = 3.497 the corresponding p-value is 0.001.
p-value: Conditional
probability of
obtaining a sample
statistic as deviant or
more deviant from
the null hypothesized
value assuming a
true null.
The p-value is the probability,
assuming the null hypothesis is true, of obtaining a sample mean 0.27 units, 3.497
standard errors away from the hypothesized mean of 2.5, here in either direction. If
the 2.5 is the true mean, the result of a sample mean of 2.77 is quite unlikely.
How low can the p-value go before the null hypothesis is ruled out as unlikely and
thus rejected? The definition of “unlikely” is the given value of α, usually 0.05, but
sometimes 0.01 or 0.10.
statistical
decision: The
rejection or not of
the null hypothesis.
The statistical decision follows.
Difference of µ from 2.5: p-value = 0.001 < α = 0.05, so reject H0
statistically
significant
difference: A likely
difference has been
detected between the
true mean and the
hypothesized mean.
Reject the null hypothesis as unlikely. A statistically significant difference of the
population value, µ, from 2.5 has been detected.
Unfortunately, the probability of the truth of the null hypothesis is not known. What
is known is the conditional probability, the low p-value that only equals 0.001. If the
null hypothesis is true, then an unlikely event occurred. The sample mean, m, is too118 CHAPTER 6. STATISTICS
far from the hypothesized value of µ, rendering that value unlikely.
conditional
probability,
Section 4.2.7, p. 72
We do not know
the more useful probability of the truth of the null hypothesis. Compute the p-value
to as many decimal digits as desired, but our understanding of the likelihood of the
null hypothesis is qualitative. We conclude that the null hypothesis is unlikely but
cannot be more precise.
Confidence Interval
Given our conclusion that the true mean is not 2.5, what is it? The inherent
randomness of the sampling process prevents a precise answer. What is possible is
to provide an interval that likely includes the true mean, µ.
confidence
interval: Range of
values that likely
contains the
population value of
interest.
The confidence interval
is the range of plausible values for the value of the true mean, µ, at the specified
level of confidence, 95%. Calculate the confidence interval as the sample mean plus
and minus the margin of error, E, which is about two standard errors. Listing 6.11
reports the interval.
Margin of Error for 95% Confidence Level: 0.155
95% Confidence Interval for Mean: 2.620 to 2.930
Listing 6.11: Confidence interval for m07, the 7th Mach IV item.
confidence interval
logic (based on
another statistic, b,
but the logic is
unchanged),
Section 11.4, p. 226
Calculate E as about two standard errors, according to the value of the t-cutoff, to
accommodate 95% of the estimated sampling distribution of m. For this analysis:
E = (1.967)(0.079) = 0.155
The confidence interval follows.
95% CI: m ± E = 2.77 ± 0.155 = 2.62 to 2.93
The 95% confidence interval ranges from 2.62 to 2.93. All plausible values of the
population mean on a scale from 0 to 5 of endorsing honesty are above the dividing
point of 2.5. The true average response for m07 is likely in the Agreement region.
The information from the confidence interval is consistent with the hypothesis test.
The confidence interval indicates that the true mean response on the six-point scale
from 0 to 5 for m07 is likely between 2.62 and 2.93. All values in this range are
larger than the null value of 2.5. Consistent with this information, the hypothesis
test indicates that the mean value of 2.5 is unlikely, a value outside of the confidence
interval.
effect size: The Effect Size
magnitude of the
difference between
the sample and
hypothesized results.
Also of interest is the descriptive statistic of effect size, the detected magnitude of
the change in the sample from the hypothesized value. The p-value indicates if a
difference between population and the corresponding hypothesized values has been
detected but provides no information regarding the extent of this difference. To report
a significant p-value less than α, without reporting an effect size or the corresponding
confidence interval, informs the reader that there is a difference but with no indication
as to its size or practical importance.6.3. EVALUATE A SINGLE MEAN 119
The ttest() function reports effect size in two different metrics. First, consider the
units of measurement of the variable of interest. The plausible difference between
the sample mean and the hypothesized mean spans the lower and upper bounds of
the accompanying confidence interval. The confidence interval can be considered
to estimate the range of possible effect sizes in the metric for which the variable is
measured.
Another indicator of effect size is in standardized units, the sample distance of
sample and hypothesized means divided by the standard deviation of the data. This
standardized indicator is Cohen’s d after Jacob Cohen (1969). Mach IV data,
Listing 1.5, p. 17
Standardization can
be beneficial when the original measurement unit is arbitrary, such as for Likert
responses, including the items on the Mach IV scale.
The function ttest() reports the distance of the sample mean from the hypothesized
mean in the units of measurement and the standardized units. Listing 6.12 reports
these two effect size metrics.
------ Effect Size ------
Distance of sample mean from hypothesized: 0.275
Standardized Distance, Cohen’s d: 0.187
Listing 6.12: Effect size for the distance of the sample mean of m07, the 7th Mach IV item,
from the hypothesized value of 2.5.
The raw distance of the sample from the hypothesized mean is 0.27 units on the
six-point scale. The corresponding standardized value is 0.19, that is, the sample
mean is 0.19 standard deviations above the hypothesized mean of 2.5. To visualize,
ttest() provides the smoothed plot of the distribution, a density plot, shown in
Figure 6.9.
density plot,
Section 5.2.4, p. 87
Figure 6.9: Density plot from ttest() of m07, the 7th item on the Mach IV scale, with
sample mean and hypothesized mean and two effect sizes, not reverse scored.
The plot includes vertical lines that indicate the distribution’s sample mean and the120 CHAPTER 6. STATISTICS
hypothesized mean. The bottom horizontal axis displays the original metric in which
Cohen’s d for the the variable is measured.
mean: Standardized
difference of m from
m0.
The top horizontal axis displays Cohen’s d, the standardized
difference of the obtained mean, m, from the hypothesized value, m0.
Interpretation. m07 MachIV item
The Mach IV m07 population mean appears to be in the Agree region. On average,
respondents believe that lying to others is wrong. The distribution of responses,
however, shows a great deal of variation, with many respondents falling into the
Disagree region. Furthermore, the mean is only slightly higher than the neutral
value of 2.5. The mean of m07 by itself does not provide a compelling summary of
the data values.
The analysis indicates a population mean different from 2.5. However, the reasonably
large sample size of n = 351 contributed to identifying the significant difference
though not a large, practical difference.
Inference from Summary Statistics
Another ttest() option performs the t-test directly from the three summary statistics
from which the computations of the test proceed. Just specify values for n, m and s:
The sample size, the sample mean and the sample standard deviation, respectively.
Input. t-test of a mean from summary statistics
ttest(n=351, m=2.77, s=1.47, mu=2.5)
Invoking this statement results in exactly the same output as shown previously, except
the portions of the output that depend directly on the availability of the data. The
density curves cannot be analyzed, nor is a test for normality of the data performed.
6.3.4 One-Tailed vs. Two-Tailed Tests
Specifying the null and alternative hypotheses depends on the direction of any
potential deviations from the specified null value.
two-tailed test:
Rejection region
consists of both +
and − deviations
from the null value.
For the two-tailed test, deviations
in either direction from the null value are of interest. The corresponding rejection
region consists of values much larger than the null value and values that are much
smaller. The two-tailed test is the default analysis, specified by the default value
"two_sided" for the alternative parameter.
one-tailed test:
Rejection region lies
only on one side of
the null value.
The one-tailed test, with the rejection region in only one tail, has the advantage of
enhanced power when the direction of the difference is correctly predicted, which
means a larger chance of detecting an actual difference from the null hypothesized
value.
power of a
hypothesis test:
Probability of
correctly rejecting
the null hypothesis
when it is false.
The one-tailed test might achieve a p-value less than 0.05, when the two-tailed
version might not. However, performing the one-tailed test restricts the interpretation
to a mean significantly different from the null hypothesized value in only the one
specified direction from the null value.
Choosing between a one-tailed and two-tailed test is not a question of whether the
researcher prefers a large positive or negative deviation; it’s a question of what the
researcher is willing to interpret. In most situations, the researcher would prefer6.4. EVALUATE A PROPORTION 121
to interpret a large deviation from the hypothesized value, positive or negative.
Accordingly, in most situations, the two-tailed test is the appropriate test.
An example of an appropriate one-tailed test is a consumer protection agency eval￾uating a vehicle manufacturer’s gas mileage claim. The agency is unconcerned if
the average gasoline mileage exceeds the promise. Instead, the agency is primarily
concerned with variations that are much less than the declared distance. alternative
parameter Specify a
one- or two-tailed
test.
If you only
wish to evaluate deviations in one direction, use a one-tailed test with rejection on
only one side of the null value. Choose between setting the alternative parameter
to either "less" or "greater" to specify the corresponding alternative hypothesis.
1- vs 2-tailed tests
for proportions,
Section 6.4, p. 124
Find a related discussion and a worked example of one- vs. two-tailed hypothesis
tests in the following discussion of proportions.
6.4 Evaluate a Proportion
The analysis of proportions consists of two primary types.
◦ Compare the obtained proportions across the values of one or more categorical
variables for a single sample. The analysis is a test of goodness-of-fit
applied to a single variable. Or, evaluate a potential relationship between two
categorical variables, a test of independence.
◦ Focus on a single value of a categorical variable, termed a success when it
occurs. Analyze the resulting proportion of occurrence for a single sample, or
for a test of homogeneity, compare proportions of successes across distinct
data samples for a single variable.
The lessR function Prop_test(), abbreviated prop(), provides either type of analy￾sis. To use, enter either the original data from which to compute the frequencies and
then the sample proportions or enter already computed frequencies. For the analysis
of multiple categorical variables across two levels of one of the variables, the test of
homogeneity and the test of independence yield identical statistical results.
goodness-of-fit test,
Section 4.2.7, p. 72
independence test,
Section 4.3.4, p. 80
We have already discussed the tests of goodness-of-fit to the uniform distribution and
the test of independence in Chapter 4. The analysis of proportions for a single level of
the categorical variable is the focus here. Define the occurrence of a designated value
of the parameter variable as a success. Define all other values of the variable as
failures. Of course, success or failure in this context does not necessarily mean good
or bad, desired or undesired, but instead, a designated value that either occurred or
did not.
Single mean analysis,
Section 6.3.3, p. 115
An analysis previously discussed in this chapter is of a single mean, descriptive
statistics followed by the test of a hypothesized value and corresponding confidence
interval. The same logic applies to the proportion. Denote the sample proportion of
successes as p, and the corresponding population proportion with the Greek letter π
(nothing to do with the π for computing the area of a circle).122 CHAPTER 6. STATISTICS
Two-tailed Test
For a given hypothesized value of the population proportion, π0, state the null
hypothesis as:
Null hypothesis for a two-tailed test, H0 : π = π0
An event that either occurs or does not occur is a binomial event.
binomial event:
An event that has
only two outcomes,
occur (success) or
not occur (failure).
For the present
application, either the specified value of the variable occurs for the following sample
(row of data) or it does not. Calculate the probability of a binomial event with the
exact binomial test. Before computers, binomial probabilities were approximated
with the normal-curve, but now the computationally demanding exact test can be
performed instead.
independent
events: The
outcome of one event
does not change the
probability of the
outcome of a second
event.
The exact binomial test requires that trials are independent. An
outcome of any one trial does not depend on the values of previous outcomes. For
example, a head on one coin flip does not influence the outcome of a head or tail on
the next coin flip.
Do the test of a single proportion2 with the lessR function Prop_test(). Specify
the first parameter variable as the variable of interest, then provide the value of
the variable that defines a success, the value for the parameter success.
Employee data table,
Figure 1.5, p. 13
Scenario. Assess the proportion of women employees relative to 0.4
The proportion of women employees at a company two years ago was 0.4 or 40%.
Are current employee levels of women employees larger than a proportion of 0.4
beyond sampling fluctuations?
Use the lessR Employee data set for the analysis, here read into the d data frame.
d <- Read("Employee")
To test the hypothesis, set the value of parameter pi to 0.4. The second parameter in
the definition of Prop_test() is success, so when listed second in the function call
the parameter name is not required. The parameter name is listed in this example
for clarity in the call to Prop_test() shown in Listing 6.13.
Prop_test(Gender, success="W", pi=0.4)
--- Exact binomial test of a proportion
variable: Gender
success: W
Listing 6.13: Input for the Prop_test() analysis of a proportion with the variable and its
value that defines success.
Consistent with the analyses presented by the lessR functions, the first set of output
describes the sample data, shown in Listing 6.14.6.4. EVALUATE A PROPORTION 123
------ Describe ------
Number of missing values: 0
Number of successes: 19
Number of failures: 18
Number of trials: 37
Sample proportion: 0.514
Listing 6.14: Description of the sample data.
The sample proportion of current women employees is p = 0.514, clearly larger than
the previously obtained value, the null hypothesized value of π = 0.4. sampling fluctuations,
Section 6.3.2, p. 112
But sampling
fluctuations must always be considered, so generalize beyond the given sample to
the population. Flip a fair coin just ten times, and the true proportion of 0.5 would
be not be obtained more than 75% of the time. Obtaining a value of 8 heads is
reasonably inconsistent with the true probability of a fair coin with π = 0.5, though
such a result does not prove or disprove that π = 0.5, or any other value.
Following the same logic, is the sample value of p = 0.514 demonstrably larger than
π = 0.4? How consistent is the outcome of p = 0.514 with π = 0.4? Address that
question with the hypothesis test and confidence interval of inferential statistics shown
in Listing 6.15.
------ Infer ------
Two-sided hypothesis test for null of 0.4, p-value: 0.180
95% Confidence interval: 0.344 to 0.681
Listing 6.15: Inference for the hypothesis test of π0 = 0.4.
The result is that obtaining a sample proportion of p = 0.514 is not that discrepant
from the null reference value of 0.4. The probability of a discrepancy this large in
either direction of 0.4 is only 0.180.
p-value = 0.180 > α = 0.05, so do not reject H0
Of course, failing to reject the null hypothesis does not prove that the null value,
here π = 0.4, is correct. Instead, the null could still be false, and the hypothesis
test failed to detect a true difference that does exist. This possibility is indicated by
the confidence interval, which includes values below 0.4, but a larger range of values
greater than 0.4. Because π = 0.4 lies within the confidence interval, it is a plausible
value, but so are all other values that lie within the interval.
Interpretation. Inferential analysis of a proportion
There is no population difference in the current proportion of employed women in
the company, 0.514, compared to the previous result of 0.4. However, given that
the confidence interval includes values greater than 0.4, the difference is plausible
but not supported by this analysis.
2For the test of a single proportion, Prop_test() relies upon the R function binom.test().124 CHAPTER 6. STATISTICS
One-tailed Test
1- vs 2-tailed tests
for means,
Section 6.3.4, p. 120
The previous analysis was a two-tailed hypothesis test, which recognizes a difference
from the hypothesized value in either direction, smaller or larger. The advantage
of this test is that a smaller or larger deviation from the hypothesized value can be
interpreted. If the proportion of women increased or decreased from the hypothesized
value of 0.4, that result could be interpreted either way. However, in this example,
the interest focuses on a potential increase in the proportion of women employees.
Some analysts argue that the hypothesis test should be one-tailed in this situation.
Locate the rejection entirely in the upper tail of the sampling distribution on which
the test is based. The null hypothesis then becomes everything else, all values less
than the stated value of π0.
Null hypothesis for an upper one-tail test, H0 : π ≤ π0
Alternative to the null for an upper one-tail test, H0 : π > π0
For this one-tailed test, only differences in the obtained sample proportion much
larger than the hypothesized proportion lead to rejection of the null hypothesis. In
the current example, if the proportion of woman employees fell below 0.4 for this
one-tailed test, the result cannot be stated that the population proportion decreased.
If the null hypothesis cannot be rejected, the only permissible interpretation from
that one-tailed test is that the proportion of female employees did not increase.
In most situations, there is interest in interpreting a result deviant from the null in
either direction regardless of the preference for one direction or the other. Even if the
analyst wishes to evaluate a potential increase in the proportion of women employees,
there would likely be just as much or more interest to interpret the contrary result as
a decrease in that proportion. If so, the two-tailed test is the better option.
Here perform the one-tailed test. Conducting this one-tailed test assumes that the
analyst began the analysis with this test focused solely on variations larger than the
initially hypothesized population proportion of 0.4. Conducting a two-tailed test and
then, if significance is not detected, switching to a one-tailed test to achieve greater
power to detect a difference, is never appropriate.
To conduct the analysis with the rejection located only in the upper-tail, set the
parameter alternative to the value of "greater". Similarly, set the parameter to
"less" for a lower-tail one-tail test. Listing 6.16 shows the input and the output for
the inferential analysis of the upper-tail rejection region.
Prop_test(Gender, success="W", pi=0.4, alternative="greater")
------ Infer ------
Alternative hypothesis: Population proportion is greater than 0.4
Hypothesis test for null of 0.4, p-value: 0.108
95% Confidence interval: 0.368 to 1.000
Listing 6.16: Inferential analysis for a one-tailed test of a proportion.6.5. ANALYSIS PROBLEMS 125
The output for inference includes an additional line than from Listing 6.15 that indi￾cates the rejection region is only for values much larger than than 0.4. Consequently,
the increased power of the test results in a smaller p-value, 0.108, though in this
instance significance is still not achieved as the p-value remains above the criterion
of α = 0.05. The obtained sample proportion, p = 0.514, is reasonable given the
hypothesized value of 0.4. The p-value is not sufficiently unusual to reject the null
hypothesis and conclude that the true population proportion of women employed is
larger than 0.4.
Analysis from Frequencies
Direct analysis of the proportions in terms of the obtained frequencies is also possible
with Prop_test(), which accepts as input either data or the related frequencies
previously calculated from the data. In many situations, only the frequencies or
related proportions are available. In this example from Listing 6.14, we observed 19
successes, 19 women employees out of a total of 37 employees. Enter this information
directly into Prop_test() with the parameters n_succ and n_tot, respectively.
Prop_test(n_succ=19, n_tot=37, pi=.4)
The output for the descriptive and inferential statistics is identical to that of List￾ings 6.14 and 6.15 except that the number of missing values is not reported as the
original data is not available.
6.5 Analysis Problems
Consider the Cars93 data set, available from within the lessR package.
?Cars93 for more
d <- Read("Cars93") information.
Variables in this data set include MPGcity, MPGhiway, HP, Cylinders, and Source
1. Calculate the following summary statistics.
a. Mean, standard deviation, and skew for MPGhiway.
b. Mean, median, standard deviation, and IQR for MPGhiway and MPGcity
c. Quartiles for HP and Weight
d. Mean for HP and Weight classified by Cylinders, and discuss the frequency of
the categories
e. Mean for HP and Weight classified by Cylinders and Source, and discuss the
frequency of the categories
2. Is it reasonable that the true mean city MPG is 24?
a. Describe the sample mean in relation to the hypothesized mean in terms of
MPG.126 CHAPTER 6. STATISTICS
b. How many standard errors is the sample mean from the hypothesized mean.
c. Does the t-value exceed the t-cutoff?
d. What is the relation of the p-value and the null hypothesized value?
e. What is the confidence interval? Does it include 0?
f. Is the confidence interval consistent or not with the hypothesis test? Why?
g. From a statistical perspective, is 24 MPG reasonable? Why or why not?
h. Interpret verbally, without statistical jargon.
3. Is it reasonable that the true mean highway MPG is 30?
a. Describe the sample mean in relation to the hypothesized mean in terms of
MPG.
b. How many standard errors is the sample mean from the hypothesized mean.
c. Does the t-value exceed the t-cutoff?
d. What is the relation of the p-value and the null hypothesized value?
e. What is the confidence interval? Does it include 0?
f. Is the confidence interval consistent or not with the hypothesis test? Why?
g. From a statistical perspective, is 24 MPG reasonable? Why or why not?
h. Interpret verbally, without statistical jargon.
4. During the past five years, 38% of the applicants to a math graduate program have
been female. After an extensive advertising campaign targeting females, the school
administration wondered if the percent of female applicants had changed. Of the 126
applicants to next year’s class, 52 are female.
a. Has the percentage of female applicants changed?
b. Construct the 95% confidence interval around the sample proportion.
c. Does this interval contain the value of .38?
5. A candidate for political office wants to know the percentage of the electorate
planning to vote for her. The obtained sample of size of a poll of 284 likely voters
yielded 148 votes for this candidate.
a. Run the hypothesis that 50% of all of the voters intend to vote for her?
b. Construct the 95% confidence interval around the sample proportion.
c. Does the confidence interval contain the value of .50? Relate to the hypothesis
test.Chapter 7
Compare Two Samples
7.1 Quick Start
Install R and lessR as explained in Section 1.2. At the beginning of each R session,
access the lessR functions with library("lessR"). From Chapter 2, read your
data into an R data frame such as d with d <- Read("") to browse for the data file.
Or, locate your data file with a path name or web address between the quotes.
This chapter presents analyses that reveal potential differences of a continuous variable,
the response variable, across two different samples, each for a different group. The
comparison can be of the respective means, distribution shape, or directly in terms of
the differences of matched data points across the two samples.
Do average salaries differ for men and women at a given company? The analysis is first
applied to observational data, such as comparing gender, and then to experimental
data.
independent-groups
t-test, Section 7.2,
p. 128
The statistical procedure for the comparison of means across groups, such as
average salaries for men and women, is the independent-groups t-test.
ttest(Salary ∼ Gender)
The tilde, ∼ , in the above expression indicates the specification of a model, the
expression of one variable, such as Salary, in terms of one or more other variables,
such as Gender. In particular, this expression is an instance of the general linear
model, one of the more important concepts in all of statistics.
general linear model,
Section 13.2.3, p. 280
Wilcoxon rank sum
test, Section 7.2.5,
p. 135
The nonparametric alternative to the independent-groups t-test is the Wilcoxon rank
sum test. This test requires no assumptions regarding the shape of the underlying
distributions, such as normality.
with(d, wilcox.test(Salary ∼ Gender, conf.int=TRUE))
Instead of comparing two group means, the dependent-groups t-test directly compares
each data value in one sample with a matched data value in the corresponding sample. dependent-groups
t-test, Section 7.3.1,
p. 141
This comparison is only possible if the analyst is able to match the data values across
samples for the analysis of the differences between the corresponding matched scores.
Invoke this analysis with the ttest() parameter paired set to TRUE.
127128 CHAPTER 7. COMPARE TWO SAMPLES
ttest(Before, After, paired=TRUE)
Wilcoxon signed rank
test, Section 7.3.2,
p. 145
The nonparametric alternative to the dependent-groups t-test is the Wilcoxon signed
rank test.
with(d, wilcox.test(Diff, conf.int=TRUE))
multiple proportions,
Section 7.4, p. 146 A special case of the mean is the proportion, in which the data are binary with
data values 0 or 1. The analysis presented here examines the differences between
two or more proportions to evaluate if the corresponding population proportions are
different from each other. For example, does the proportion of the purchase of thick
motorcycle jackets vary across the type of motorcycle ridden by the buyer?
Prop_test(Jacket, success="Thick", by=Bike)
If the original data are not available, the analysis of proportions can also be accom￾plished from previously computed frequencies or proportions.
7.2 Independent-Samples
A standard data analysis compares the means of two separate samples, potentially
from two different populations. Do math majors have more or less average anxiety
than psychology majors? Does average blood pressure differ between vegetarians and
meat-eaters? To answer questions such as these, collect samples of data from each of
the two groups. Although the goal is to have sample sizes at least somewhat similar,
the number of participants in each sample does not have to be identical.
independent- or
between-samples
design: Data values
in each sample are
unrelated to data
values in the other
samples.
The group comparisons here are based on an independent-samples (or groups) design.
Any one data value in one sample has no connection to a data value in the other
sample. Another name for this design is a between-subjects or between-samples design.
Later we generalize to a technique for comparing the means from more than two
samples and also work with samples in which the data values are connected.
Compare two or
more group means,
Section 8.3, p. 153 7.2.1 Research Design for Independent-Samples
Mach IV scale,
Listing 1.5, p. 17
Do men and women differ in their Machiavellian tendencies? Measuring an important
component of the Dark Triad personality constellation (Paulhus & Williams, 2002),
the Mach IV scale (Christie & Geis, 1970) consists of 20 items. We partially answer
that question with samples of men and women to compare aspects of their Machi￾avellian scores. The claim is not that, based on gender differences, all men are more
Machiavellian than all women or vice versa. Instead, the question is whether the
men’s average Machiavellian score is higher or lower than the women’s average score.
Ultimately, extrapolate sample results to the population means from which the
samples were drawn. In any particular set of samples, finding that men had a higher
average Machiavellian score than women or vice versa is not of general relevance.7.2. INDEPENDENT-SAMPLES 129
Instead, does the sample mean of one group being larger than the other sample mean
generalize to the population means of all men and women from the corresponding
populations? inferential statistics,
Section 6.3.2, p. 112 Apply inferential statistics to discover relationships and patterns that
extend beyond the random fluctuations of specific samples.
7.2.2 Example 1: Two Existing Groups
Begin the analysis by reading the data and the corresponding variable labels, available
from within lessR.
Mach IV data,
Listing 1.5, p. 17
variable labels,
Section 2.5, p. 34
Input. Read data into the d data frame and variable labels into the l data
frame
d <- Read("Mach4")
l <- Read("Mach4_lbl")
Prepare the Data
Mach IV Deceit scale,
Listing 15.7, p. 341
Consider the Deceit subscale on the Mach IV scale: Items 6, 7, 9 and 10. Each item
on the scale is written such that agreement implies the endorsement of honesty, yet a
Machiavellian would endorse a deceitful perspective. In the context of the analysis of
Machiavellianism, reverse score the four Deceit items. recode() function,
Section 3.4.1, p. 49
The lessR recode() function
reverse scores the item responses, converting a 0 to a 5, a 1 to a 4, etc. Then calculate
a Deceit score for each respondent as the average of the four constituent items. The
R function transform() creates the new variable in the d data frame as the average
of the four, recoded Deceit items.
transform() function,
Section 3.3, p. 46
Input. Reverse score four items and compute their average
d <- recode(c(m06,m07,m09,m10), old=0:5, new=5:0)
d <- transform(d, Deceit = (m06 + m07 + m09 + m10) / 4)
The variable Deceit is transformed to retain the original range of values for each of its
component items: potentially from 0 to 5. In the data, Deceit varies from 0 to 4.25.
The values of Gender in this data table are encoded as integers, with 0 for Man and 1
for Woman. For clarity when reading the output, invoke the R function factor() to
redefine Gender as an R factor with labels M and W.
factor() function,
Section 3.2, p. 43
Input. Convert integer coded Gender to an R factor
d$Gender <- factor(d$Gender, levels=c(0,1), labels=c("M","W"))
Analysis
Scenario. Compare the means of two distinct groups, Men and Women
Do men and women tend to differ regarding their endorsement of Deceit? Compare
the responses of men and women on the Mach IV subscale Deceit.
This comparison between the means of the two groups involves two different variables.
Compare the response variable across the groups, here each person’s Deceit score.
The grouping variable, Gender, consists of two values, M and W.
response variable:
The continuous
variable compared
across two groups.
grouping variable:
The categorical
variable with two
unique values that
defines the groups.130 CHAPTER 7. COMPARE TWO SAMPLES
The analysis is the independent-groups t-test, accomplished with the lessR function
ttest(), or its abbreviation, tt(). A form of the test is also available that provides
a briefer output, tt_brief().
ttest() function,
lessR: Analyze the
mean difference
between two groups.
To invoke ttest(), shown in Listing 7.1, use the
standard R formula that defines a model: Begin with the response variable name,
followed by a tilde, ∼ , and then the name of the variable to relate to the response
variable, here, the name of the grouping variable with exactly two unique values.
Input. Independent-groups t-test for the mean difference
ttest(Deceit ∼ Gender)
Compare Deceit across Gender levels M and W
Listing 7.1: Analyze endorsement of Deceit across the two groups of Gender.
alternative
parameter,
Section 6.3.4, p. 121
By default, the test is two-tailed to evaluate differences in either direction. To
change to a one-tailed test, change the default value of the alternative parameter
alternative of "two_sided" to either "less" or "greater".
parameter: Specify a
one- or two-tailed
test.
The function ttest() organizes its
output by the general topics of Description and Inference.
7.2.3 Description
Sample Statistics
Listing 7.2 describes these two data samples for Men and Women.
------ Describe ------
Deceit for Gender M: n.miss = 0, n = 119, mean = 1.588, sd = 0.922
Deceit for Gender W: n.miss = 0, n = 232, mean = 1.430, sd = 0.970
Mean Difference of Deceit: 0.158
Weighted Average Standard Deviation: 0.954
Listing 7.2: Descriptive statistics for the comparison of the average of Deceit across Men
and Women.
It is from these descriptive statistics that the inferential test proceeds: The sample
size, mean and standard deviation for each group, and the number of missing data
values, if any. On average, in this particular data set, men scored 0.158 units higher
within-group or than women on a scale that can vary from 0 to 5.
pooled standard
deviation: Square
root of the weighted
average of the
variances across the
groups.
The Weighted Average Standard Deviation from Listing 7.2 is also known as the
within-group or pooled standard deviation, based on the respective variances of the
two groups. The variance is the square of the standard deviation. The variances are
weighted by their respective degrees of freedom so that the variance from the larger
sample is given more weight. Weight by the df = nj − 1 for each group, where nj is
the sample size of the j
th group.
s
2
w =
df1(s
2
1
) + df2(s
2
2
)
df1 + df2
=
(119 − 1)(0.92212
) + (232 − 1)(0.96992
)
(119 − 1) + (232 − 1) = 0.91017.2. INDEPENDENT-SAMPLES 131
The square root of the resulting average is the within-group standard deviation.
sw =
q
s
2
w =
√
0.9191 = 0.954
Note that sw = 0.954 is intermediate to the two sample standard deviations, s1 = 0.922
and s2 = 0.970.
Effect Size effect size
Section 6.3.3, p. 118
The concept of effect size was presented for a single sample analysis. Here extend the
concept of effect size as a descriptive statistic to describe the separation of the means
of the two samples.
effect size of the
mean difference: The
magnitude of the
difference between
the sample means for
a null value of 0.
Again, the p-value can detect a difference between the population
means but does not provide information regarding the extent of the difference.
The sample mean difference provides the sample estimate of effect size in terms of
the original measurement units. The confidence interval provides the corresponding
estimate in terms of the population. As with the single sample analysis, the mean
difference can be standardized, expressing the difference that generalizes beyond the
original units of measurement, in units of the standard deviation. normal curve
probabilities,
Section 6.3, p. 104
The standardized
metric becomes meaningful in reference to the normal curve which provides a baseline
for evaluating differences in terms of standard deviation.
standardized
mean difference:
Standardized effect
size for the mean
difference.
This effect size statistic is the standardized mean difference, or smd, also called
Cohen’s d (Cohen, 1969). To standardize the mean difference, divide it by the
standard deviation of the data. Of course, there are two samples, one for Men and
one for Women, so there are two standard deviations to describe the data. Combine
these two estimates as the within-group standard deviation, sw.
Given the sample mean difference, m1 − m2 and sw, compute smd.
within-group
standard deviation
s
2
w, Section 7.2,
p. 130
smd =
m1 − m2
sw
=
1.588 − 1.430
0.954
= 0.1659
Listing 7.3 shows the effect size output from ttest() that expresses the Gender effect
on the endorsement of practicing Deceit.
------ Effect Size ------
--- Assume equal population variances of Deceit for each Gender
Standardized Mean Difference of Deceit, Cohen’s d: 0.166
Listing 7.3: Effect size for the mean difference.
The value of smd for this sample is small, less than 0.17 standard deviation. Visualize
the effect size of the overlapping distributions of endorsement of Deceit for men and
women. This ttest() visualization in Figure 7.1 directly demonstrates the small
extent of the overlap of the two distributions, with just a small shift for higher scores
on Deceit for men.132 CHAPTER 7. COMPARE TWO SAMPLES
Figure 7.1: Density curves for men and women on Mach IV subscale Deceit.
7.2.4 Inference
Before conducting the inferential analysis of the equality of group population means,
first verify its assumptions.
Assumptions
The t-test for the inferential generalization of the observed mean difference to the
populations from which the two samples were obtained makes several assumptions,
evaluated in Listing 7.4.
------ Assumptions ------
Null hypothesis, for each group, is a normal distribution of Deceit.
Group M: Sample mean is normal because n > 30, so no test needed.
Group W: Sample mean is normal because n > 30, so no test needed.
Null hypothesis is equal variances of Deceit, i.e., homogeneous.
Variance Ratio test: F = 0.941/0.850 = 1.106, df = 231;118,
p-value = 0.542
Levene’s test, Brown-Forsythe: t = 0.869, df = 349, p-value = 0.385
Listing 7.4: Evaluation of underling assumptions of the independent-groups t-test of the
mean difference.
The sample mean of each sample must be normally distributed over the usually
hypothetical multiple samples, a condition generally satisfied by the central limit
assess normality theorem if the sample sizes are 30 or more.
assumptions,
Section 7.12, p. 140
If this sample size is small for one or
both samples, then a test of normality can be run for each distribution, with the null
hypothesis that the respective distribution is normal. However, these hypothesis tests7.2. INDEPENDENT-SAMPLES 133
can perform poorly, and the t-test is typically robust to violations of assumptions.
Use these tests as heuristic guides instead of interpreting literally.
The t-test evaluates if the population means are equal. However, the standard t-test
assumes that the population variances of the response variable, Deceit, are identical
for the two groups. This assumption is only crucial once at least one of the sample
sizes is very small, around 20 or less. There is no precise test for this assumption
because the existing hypothesis tests need to work better in small samples, which is
the only case in which the assumption must be properly examined.
variance ratio,
Brown-Forsythe
tests: Evaluate
equal population
variances.
Two hypothesis tests of the equality of variances for the two groups are provided.
The first is the variance ratio test, literally the ratio of the two sample variances, the
squared standard deviations. The variance ratio is an F-value. The second test is
the Brown-Forsythe version of Levene’s test. To perform this test, deviate each data
value from its respective group median. Then perform a t-test on the two sets of
deviated data values. A significant difference indicates unequal population variances.
p-value: Given
equal population
group means, the
probability of a
result as or more
deviant than the
sample result.
From Listing 7.4 the p-value of both equal variance hypothesis tests is larger than
α = 0.05. Accordingly, the null hypothesis of equal variances cannot be rejected,
and, while not proved, is shown to be consistent with the data. So proceed with the
remainder of the analysis.
Population Mean Difference
Base the inferential test on the null hypothesis, which specifies the equality of the
population group means. Express the null hypothesis as zero difference between the
corresponding population means. The alternative to the null hypothesis is that the
population means are not equal. Represent the unknown population mean of Deceit
for men with µM and µW for women.
H0 : µM − µW = 0
Listing 7.5 presents the classic t-test for the inferential analysis of the mean difference
assuming equal variances of Deceit endorsement for Men and Women.
------ Infer ------
--- Assume equal population variances of Deceit for each Gender
t-cutoff for 95% range of variation: tcut = 1.967
Standard Error of Mean Difference: SE = 0.108
Hypothesis Test of 0 Mean Diff: t-value = 1.471, df = 349, p-value = 0.142
Margin of Error for 95% Confidence Level: 0.212
95% Confidence Interval for Mean Difference: -0.053 to 0.370
Listing 7.5: Inference for the classic t-test of the mean difference, hypothesis test and
confidence interval.
Even if the null hypothesis of equal population means is true, the presence of sampling
ensures that the sample means of the response variable across the groups differ. In134 CHAPTER 7. COMPARE TWO SAMPLES
terms of this example, Men and Women may have the same average level of the
endorsement of Deceit but their sample means of Deceit endorsement for each group
will differ. The null hypothesis of equality of population means becomes untenable
only when the sample means differ too much. Hence, the need for inferential analysis
to define how much is “too” much.
The degrees of freedom for this test, df = 349, is the sample size, 351, minus two,
one for each group mean. The inferential analysis follows from the standard error
of the mean difference, the standard deviation of the sample mean difference over
many hypothetical samples from the populations for the two different groups. In
this example, the standard error of the mean difference is 0.108, considerably less
than the standard deviation of the data for the response variable for the two samples,
0.922 and 0.970.
t-value of the mean
difference: Estimated
standard errors that
separate the sample
mean difference from
the hypothesized
mean difference,
usually 0.
The observed sample mean difference of mM − mW = 0.158, shown in Listing 7.3, is
1.471 estimated standard errors from the null hypothesized mean difference of 0.
t-value =
(mM − mW ) − 0
smM−mW
=
1.588 − 1.430
0.106
= 1.471
The t-value of 1.471 corresponds to a p-value of 0.142. If the null hypothesis of no
population mean difference is true, then the probability of obtaining a sample mean
difference that deviant or more from zero, in either direction, is the p-value of 0.142.
confidence
interval of the mean
difference: Range of
plausible values of
the population mean
difference at a
specified level of
confidence.
The 95% confidence interval of the mean difference contains the plausible values
of the population mean difference. The interval is the sample mean difference plus
and minus the margin of error, E = 0.208, which itself is about two standard errors,
se = 0.106, to contain the range of 95% variation.
95% CI: (mM − mW ) ± E = (1.588 − 1.430) ± 0.208 = −0.053 to 0.370
The interval ranges from −0.053 to 0.370, which crosses the boundary value of 0, so
no difference is a plausible value.
conf_level
parameter, ttest():
Specify the
confidence level.
The default value for the confidence level is 0.95,
the default value of the parameter conf_level.
A parallel analysis does not assume equal variances of the response variable across
the groups. Listing 7.6 presents this alternate analysis.
--- Do not assume equal population variances of Deceit for each Gender
t-cutoff: tcut = 1.970
Standard Error of Mean Difference: SE = 0.106
Hypothesis Test of 0 Mean Diff: t-value = 1.496, df = 248.978, p-value = 0.136
Margin of Error for 95% Confidence Level: 0.208
95% Confidence Interval for Mean Difference: -0.050 to 0.367
Listing 7.6: Inference for the t-test that does not assume equal variances.
Here, separately calculate the variance of the responses in each group, from which to
calculate the estimated standard error of the mean difference. In general, it is better7.2. INDEPENDENT-SAMPLES 135
to make fewer assumptions. The problem, however, of using the separate variances is
that the distribution of the resulting test statistic, the t-value defined as the difference
between sample means divided by the estimated standard error, only approximates
the t-distribution. To compensate, a rather formidable expression approximates the
degrees of freedom, generally with non-integer values, here df = 248.978.
Which t-test version is preferred? The classic t-test, which assumes equal variances,
is used by the majority of researchers. Precise guidelines are missing, although an
exception would be if at least one sample is tiny and/or the group variances are
significantly different from one another. In that case, not assuming equal variances
may be the preferable option. In most applications, the two forms of the mean
difference test yield the same basic result, as is true in this example.
Conclusion
no significant
difference: The
conclusion is no
difference detected,
not that there is no
difference.
For this data set, there is a slight tendency for men, on average, to endorse Deceit
more than women, but the result is not statistically significant nor substantively
large.
Interpretation.
No true difference in endorsement of Deceit is detected for men and women. If the
difference does exist, it is likely to be small, of little or no practical importance.
The average self-reported levels of endorsement of Deceit in men and women are
essentially the same.
7.2.5 Nonparametric Alternative
Parametric tests such as the t-test involve estimating parameters, such as from the
assumption of a normal distribution.
nonparametric
test: Inference
without strong
distributional
assumptions.
A nonparametric test of two samples does not
compare parameter estimates but compares the entire distributions from which the
samples are obtained. A parametric test relies upon reasonably strong assumptions
regarding the underlying distributions, such as the assumption of normality. A
primary advantage of a nonparametric test is that it relaxes those assumptions.
Wilcoxon rank
sum test:
Nonparametric test
from two
independent-samples
that compares the
corresponding
population
distributions.
The two-sample Wilcoxon rank sum test1
is a nonparametric test for two independent￾groups that is analogous to the independent-groups t-test. The null hypothesis of the
two-sample Wilcoxon rank sum test is that the two corresponding populations have
the same continuous distribution. The alternative hypothesis is that one distribution
shifts to the right of the other distribution. The alternative hypothesis is that the
values of one distribution are systematically larger or smaller than the value of the
other distribution, though there still could be considerable overlap.
For the two-sample Wilcoxon rank sum test, the assumption of independent random
samples is required, but the more stringent requirement of normality is dropped.
Instead, even if the distributions are skewed, the assumption is that they have the
same shape. If the two distributions have the same shape but one is shifted to
1The Mann-Whitney U-test, a related nonparametric test that compares two independent-samples,
is identical to this test. Both tests produce identical results.136 CHAPTER 7. COMPARE TWO SAMPLES
the right or left of the other, their medians differ. This conclusion is true for any
cutpoint in the distribution other than the median, such as the third quartile. The
test is concerned with the relative placement of the distributions to one another
rather than with the medians themselves. The mean is not an adequate indicator for
nonparametric tests for skewed distributions due to the disproportionate influence of
skew on its value.
As is typical of nonparametric group comparisons, the Wilcoxon rank sum test is based
solely on the order of the cases from the two samples, the ranks. The transformation
to ranks retains only the ordering of the data values, with no further consideration of
their numerical values and a diminished influence of extreme values of the distributions.
Conduct the test by ordering all the values of both groups together. Each data value
in this combined distribution has a rank, beginning with 1 for the smallest data value.
The sum of all the ranks for the smaller of the two samples is the Wilcoxon rank-sum
test statistic, W.
wilcox.test()
function, R: Wilcoxon
rank sum test.
The R function for the Wilcoxon rank sum test is wilcox.test().
conf.int parameter,
R: Set TRUE to
obtain the confidence
interval.
alternative parameter
for a one-tailed test,
Section 6.3.4, p. 121
confidence level,
Section 7.2.4, p. 134
The default
values for the parameter alternative is "two_sided". The confidence interval itself,
however, must be explicitly requested with the parameter conf.int set to TRUE. The
default confidence level is 95%, or set with the conf.level parameter.
This is an R function, not modified through the lens of lessR. To inform R that the
variables are in the d data frame, either place a d$ in front of each variable name, or
use the with() function. The output of the two-sided independent-groups Wilcox
rank sum test with the requested 95% confidence interval for the comparison of Men
and Women regarding Deceit appears in Listing 7.7.
Input. R Wilcoxon test to compare two distributions
with(d, wilcox.test(Deceit ∼ Gender, conf.int=TRUE))
Wilcoxon rank sum test with continuity correction
data: Deceit by Gender
W = 15327, p-value = 0.08963
alternative hypothesis: true location shift is not equal to 0
95 percent confidence interval:
-0.00001932667 0.49991749068
sample estimates:
difference in location
0.2499937
Listing 7.7: R output from the two-sample Wilcoxon rank sum test.
Calculating the p-value, assuming the null hypothesis is true, requires knowledge of
the distribution of the test statistic, the rank sum W over many usually hypothetical
repeated samples. Express probabilities from this distribution as the p-value. Assume
that the null hypothesis of no difference in the distributions of Males and Females for
Deceit is true. In that case, the obtained p-value of 0.090 indicates that the result
was not sufficiently improbable to reject the null.
When requested with conf.int=TRUE, the output of wilcox.test() also includes7.2. INDEPENDENT-SAMPLES 137
an estimate of the difference in location of the two distributions, as well as the
corresponding confidence interval. The “location” does not refer to the median of
the original data values but rather to the following transformation. Calculate the
difference between all possible pairs of the data values in the two groups. Then rank
order this list of differences. The median of interest is the median of this list of sorted
differences, about which the confidence interval is constructed.
The qualitative pattern of the results of the parametric independent-groups t-test
and the nonparametric Wilcoxon test are similar.
Interpretation.
No difference in endorsement of Deceit was detected comparing men and women.
The location difference from the Wilcoxon test of 0.250 is somewhat larger than
the sample mean difference from the t-test of 0.158, but in the same direction. The
confidence intervals range from −0.050 to 0.367 for the mean difference and from
−0.00002 to 0.50 for the location parameter in the Wilcoxon test. The mean difference
and the location parameter are different statistics from different analyses, so they are
not expected to provide the same numerical result, especially for skewed distributions.
7.2.6 Example 2: Two Experimental Groups
The Experiment
Consider two strategies for allocating study time when preparing for a test. With
distributed practice, the learner divides their study time into multiple short study
sessions. The alternative is mass practice with only one or two significantly longer
study sessions, colloquially known as “cramming” for an exam. Many research
investigations have found that distributed practice is the best study strategy for
effective, long-term learning. To some extent, mass practice may succeed in generating
rote learning, which is rapidly forgotten, but little else.
random
assignment:
Randomly assign
participants to
experimental groups.
A researcher conducted the following experiment to compare the two learning strategies
to establish a baseline response before beginning a more extensive series of studies on
distributed vs. massed practice. Participants learned facts and information about a
geographical region they likely knew little about before the study began, a remote
area in Indonesia. Half of the participants were randomly assigned to the distributed
practice condition, in which they studied in four 30-minute sessions. The other
participants were randomly assigned to the mass practice condition in which their
study time consisted of a single two-hour session.
manipulation:
Impose a treatment
condition on the
participants.
Random assignment aims to ensure no systematic differences between two (or more)
groups of participants on any variable. The two groups of participants should be
equivalent on all variables so that any differences between the groups are due only
to random sampling error. After obtaining equivalence, administer the treatment.
The researcher purposefully manipulates the local environment differently for the two
groups of people. Students in one group get distributed practice while those in the
other group undergo massed practice. A study that assigns participants randomly to
the groups, followed by manipulating the environment, is an experiment.
experiment:
Random assignment
to different groups
followed by
manipulation.138 CHAPTER 7. COMPARE TWO SAMPLES
treatment
variable: Variable
manipulated to yield
a different value for
each group.
The variable manipulated across the defined groups, Number of Study Sessions, is
the treatment variable or independent variable. The response variable or dependent
variable in an experimental study is measured and compared across the groups. This
response variable: study’s response variable is the test Score to assess learning.
Measured variable
compared across
experimental groups.
experimental
control: Two or
more groups differ
only on the value of
the treatment
variable.
Random assignment followed by manipulation establishes experimental control. Be￾cause the two (or more) groups differ, on average, only on the treatment variable,
explain any difference in the response variable across the two groups by the different
levels of the treatment. Is the difference in amount learned in the randomly assigned
distributed and massed practice groups large enough to achieve statistical significance?
If so, attribute this difference in response to the level of the treatment variable.
The values of other potentially influential variables, called confounding variables,
cannot be accounted for without control.
confounding
variable: Variable
such as a common
cause that causes
changes in the
response variable
that is not the
variable of interest.
When not controlled, differences in the
response variable across the two groups may be due to differences in the treatment
variable, the confounding variable, or a combination of both. Assume, for example,
that the distributed practice group was only made up of men and that the study
sessions for this group were only held in the early morning. Assume the massed
practice group was only made up of women and that the study sessions were only
held late at night. The potential confounding variables in this study imply that a
difference in learning between the two groups could be due to Type of Study, Gender,
or Time of day. The reason for any disparity of learning would be unknown.
confounding variable,
Section 14.2.1, p. 304
Experimental control is the most effective procedure to eliminate confounding variables,
thus allowing for the possibility of demonstrating a causal relation, cause and effect.
causal relation: A
change in the value
of one variable leads
to a change in the
value of another
variable, with all
other variables held
constant.
With random assignment and manipulation, only one variable likely differentiates the
experimental groups: the treatment variable. Then, if the response variable’s values
systematically tend to differ between the groups, such as by the group means, the
treatment variable would appear to cause the difference.
Input
Scenario. Analyze the experimental results of Study Type on Learning
Randomly assign participants to either a distributed or massed practice study
session. When the sessions are completed, administer a test to assess learning.
Examine any potential differences in the mean amount of material learned.
Read the data from lessR data file Learn into data frame d, shown in in Listing 7.8.
d <- Read("Learn")
> d
StudyType Score
1 Many 87
2 One 74
...
34 Many 93
Listing 7.8: First and last rows of data for the Learn data set.7.2. INDEPENDENT-SAMPLES 139
Assess the amount of learning by the response variable, test Score, for each value of
the grouping variable, StudyType. Specify the analysis with the ttest() function
according to the R model specification, shown in Listing 7.9. model specification,
Section 7.2.2, p. 130
Input. Independent-groups t-test for the mean-difference
ttest(Score ∼ StudyType)
Compare Score across StudyType levels Many and One
Listing 7.9: Analyze Score across the two groups defined by categorical variable StudyType.
According to the model, the value of the grouping variable, StudyType, a treatment
variable in this context, contributes to the value of the response variable, Score. The
variation within each group of students is considered random variation about each
respective group mean.
Description
Listing 7.10 shows the descriptive statistics section of the output. For this set of
samples, the students in the distributed study sessions had higher average test scores
than those who only had one study session. The respective test scores are 87.58%
and 81.27%.
------ Describe ------
Score for StudyType Many: n.miss = 0, n = 19, mean = 87.579, sd = 8.099
Score for StudyType One: n.miss = 0, n = 15, mean = 81.267, sd = 8.746
Mean Difference of Score: 6.312
Weighted Average Standard Deviation: 8.388
Listing 7.10: Descriptive statistics for the test score for each type of study sessions.
The effect size in Listing 7.11 indicates that the mean test score for the distributed
practice condition is 0.75 standard deviations larger than the mean for the massed
practice condition. This discrepancy represents a large effect, according to Cohen
(1969), indicating a prominent separation of the distributions.
------ Effect Size ------
--- Assume equal population variances of Score for each StudyType
Standardized Mean Difference of Score, Cohen’s d: 0.753
Listing 7.11: Effect size for amount learned depending on type of study, assuming equal
population variances of test scores.
Figure 7.2, default output of ttest(), visualizes these two distributions, which shows
the extent of their separation and overlap. The distribution of the distributed practice140 CHAPTER 7. COMPARE TWO SAMPLES
test scores shifts 3/4 of a standard deviation to the right of the distribution of massed
practice test scores. The separation is reasonably large, at least for these data.
Figure 7.2: Density curves for distributed practice vs. massed practice on test Score.
Inference
The question of interest, of course, is if this sample difference generalizes to the
population. Does the effect exist beyond the extent of erratic sampling error?
assumptions of the
t-test, Section 7.4,
p. 132
To proceed with the inferential analysis of the mean difference first requires verifying
its assumptions.
◦ Normality of the sample mean difference over hypothetical repeated samples
◦ Equality of population variances of the response variable in both groups
Listing 7.12 provides the evidence that both assumptions are tenable given the high
p-values of the corresponding tests.
------ Assumptions ------
Null hypothesis, for each group, is a normal distribution of Score.
Group Many Shapiro-Wilk normality test: W = 0.960, p-value = 0.567
Group One Shapiro-Wilk normality test: W = 0.898, p-value = 0.090
Null hypothesis is equal variances of Score, i.e., homogeneous.
Variance Ratio test: F = 76.50/65.59 = 1.17, df = 14;18, p-value = 0.747
Levene’s test, Brown-Forsythe: t = -0.347, df = 32, p-value = 0.731
Listing 7.12: Evaluation of normality and equal variance assumptions.
Neither the null hypothesis of the normality of Scores for each group, nor the null
hypothesis of equal variances, is rejected. In each case, p-value > α = 0.05.
The inferential analysis of the mean difference appears in Listing 7.13.7.3. DEPENDENT SAMPLES 141
------ Infer ------
--- Assume equal population variances of Score for each StudyType
t-cutoff for 95% range of variation: tcut = 2.037
Standard Error of Mean Difference: SE = 2.897
Hypothesis Test of 0 Mean Diff: t-value = 2.179, df = 32, p-value = 0.037
Margin of Error for 95% Confidence Level: 5.901
95% Confidence Interval for Mean Difference: 0.411 to 12.214
Listing 7.13: Inference for amount learned depending on type of study.
Given the large t-value of 2.179, the analysis rejects the null hypothesis of equal
population means.
Type of Learning Effect: p-value = 0.039 < α = 0.05, so reject H0
If the value of the true mean difference is not 0, what is it? According to the confidence
interval, we are 95% confident that the true mean difference of StudyType on test
Score is larger than 0, somewhere between 0.33 to 12.29.
In summary, according to the 95% confidence interval, the true average test score
for distributed practice is from 0.33% to 12.29% larger than for the massed practice
condition. The standardized mean difference for the sample data indicates a separation
of 0.75 standard deviations.
Interpretation. Learning with distributed vs. massed practice
There is a distinction in learning between distributed and massed practice. The
difference can be attributed directly to the type of study because the data were
collected in the context of an experiment, with random assignment to treatment
conditions followed by the experimental manipulation. When learning new material,
distributed practice appears to be more effective than massed practice, with an
average of 0.33% to 12.29% higher test scores at a 95% level of confidence.
7.3 Dependent Samples
The independent-groups t-test discussed in the previous section compares the means
of samples from two different groups. The samples are drawn independently, with no
connection between any particular data value in one sample and a particular data
value in another. The analysis is solely based on the two means, two sample sizes,
and two sample standard deviations.
7.3.1 Dependent-Samples t-test
block of matched
data values: Match a
data value to a data
value in another
sample.
The alternative to an independent-groups analysis collects the data in blocks of
matched data values. A block could be the data values of the same person before142 CHAPTER 7. COMPARE TWO SAMPLES
and after training, or a block could consist of happiness measures from two different
people, a husband, and a wife, in a study of marital happiness. A dependent- or
dependent or within-samples design organizes data into these blocks.
within-samples
design: Two or more
samples are
organized in blocks of
matched data values.
The samples, then, are
necessarily the same size.
A dependent-samples design applies to treatments such as therapy for depression
or a program for weight loss. For weight loss, measure weight before the treatment
program begins. Administer the program, then re-assess the weight of each participant.
The two samples are two sets of measurements with each measurement from the first
sample linked to the same person’s measurement in the second sample.
As with the independent-groups design, there are still two sets of measurements with
two sample means and two sample standard deviations. The independent-groups
dependent-groups t-test could still analyze the mean difference for these two samples of data.
t-test: A t-test of
the difference scores
of the matched data
values.
Instead,
the t-test for the dependent-groups design directly compares the matched scores,
usually by subtracting one from the other. The analysis is not of the two means, but
the mean of a single created variable, the set of differences of the matched scores. For
the pre- and post-measurements of a treatment program, directly analyze the change
in each person’s score, person-by-person.
Analysis of direct differences can produce a more powerful test comparing the sample
power of a test: The means.
ability to detect a
real population
difference.
A test with higher power is more likely to detect an existing population
difference. When the variable of interest, such as Weight, varies much among
participants, the dependent-samples test is more powerful. Some people start out
heavier than others. In an independent-groups analysis, these weight changes across
people increase the variability within each sample of measured weights, concealing
the test’s ability to discern between group averages.
The dependent-groups approach eliminates this variation among the participants’
various beginning weights from the direct evaluation of the comparison of beginning
and ending weight. The emphasis switches from a comparison of mean weights for
the entire groups to a direct assessment of weight loss (or gain) for each person in
the analysis.
Scenario. Evaluate the effectiveness of a weight loss program
Ten people who desired to lose weight participated in a weight loss program with
measurements of their weight before and after the program. Was the program
effective, and, if so, how effective?
row.names
parameter,
Section 2.3.4, p. 29
Read the data from the lessR data file WeightLoss into the d data frame.
d <- Read("WeightLoss")
View all the data by entering the name of the data frame, d, depicted in Listing 7.14.
The first column, the row name, contains the participant’s names. Each participant
contributes two data values that constitute a block of data values, a Before and After
weight. These data values are listed in the row of data for each person. For example,
the block of data values for M. Saechao is 220 and 206.7.3. DEPENDENT SAMPLES 143
> d
Before After
Saechao, M. 220 206
Smith, D. 187 189
...
Jones, S. 298 291
Langston, M. 174 164
Listing 7.14: Weight before and after a weight loss program, in pounds, for the first few
and last few blocks of data.
One-sample t-test,
Section 6.3.3, p. 118
paired=TRUE
parameter: Specify a
dependent-groups
t-test.
The inferential test begins with the null hypothesis, which states that, on average,
there is no change in weight from before and after the program. One way to accomplish
the analysis is to calculate these differences and then perform the usual one-sample
t-test on the difference scores. This same analysis can also be done directly by listing
the two variables, Before and After, and then setting the parameter paired to TRUE
in the call to ttest().
Specify the test as a two-tailed test even though the advocates of the weight loss
program prefer to see a rejection of the null hypothesis in the direction of weight loss. one-tailed test
example,
Section 6.3.4, p. 120 Given this desire for a specific outcome, some authors recommend a one-tailed test,
here with the value of alternative set to "greater". However, setting a one-tailed
test rules out any interpretation of results in the opposite direction. If the null
hypothesis is not rejected, then the only conclusion is that the population mean
difference is not greater than 0, even if the program had the opposite result than
intended and actually leads to weight gain.
Yes, it would seem strange for a weight loss program to result in weight gain, but
sometimes the best intentions lead to unexpected results. Accordingly, unless there
is a genuine lack of interest in a result that lies in the tail opposite the tail of a
one-tailed rejection region, two-tailed tests are generally preferable.
The descriptive statistics in Listing 7.15 indicate that the mean weight loss for these
10 participants is 8.20 pounds.
Input. Dependent-groups t-test
ttest(Before, After, paired=TRUE)
------ Describe ------
Difference: n.miss = 0, n = 10, mean = -8.200, sd = 7.671
Listing 7.15: Descriptive statistics of the differences of a dependent-groups t-test.
The normality assumption information appears in Listing 7.16. The null hypothesis
is a normal population of difference scores from which these 10 scores were sampled.
This null hypothesis was not rejected as the resulting p-value of 0.573 is larger than
α = 0.05. As previously indicated, this test may not perform well in small samples,
but at least the result is consistent with the assumption of normality.144 CHAPTER 7. COMPARE TWO SAMPLES
------ Normality Assumption ------
Null hypothesis is a normal distribution of Difference.
Shapiro-Wilk normality test: W = 0.939, p-value = 0.541
Listing 7.16: Evaluation of the normality of the differences of a dependent-groups t-test.
Listing 7.17 presents the inferential analysis of the difference scores.
------ Infer ------
t-cutoff for 95% range of variation: tcut = 2.262
Standard Error of Mean: SE = 2.426
Hypothesized Value H0: mu = 0
Hypothesis Test of Mean: t-value = -3.380, df = 9, p-value = 0.008
Margin of Error for 95% Confidence Level: 5.488
95% Confidence Interval for Mean: -13.688 to -2.712
Listing 7.17: Inference of 0 population mean of the differences of dependent-groups.
Given the large magnitude of the t-value, -3.380, the hypothesis test rejects 0 as the
value of the population mean of the differences.
Weight Loss Effect: p-value = 0.008 < α = 0.05, so reject H0
The mean value of the differences is likely not zero, so what is it? There is no precise
answer, but the 95% confidence interval estimates the plausible range of values for
the average weight loss as somewhere between 2.71 to 13.69 pounds.
Figure 7.3 visualizes the distribution of the difference scores.
Figure 7.3: Density plot of the differences of weight loss, in pounds.7.3. DEPENDENT SAMPLES 145
Also illustrated is the displacement of the sample mean difference from 0, both in
terms of the weight loss of 8.2 lbs. and in terms of Cohen’s d, the number of standard
deviations of the data that separates the sample mean of the differences from the
hypothesized value of 0. In this analysis, d = 1.069 is more than 1 standard deviation
from zero, a relatively large distance.
Interpretation. Effectiveness of the weight loss program
The weight-loss program appears to be successful. Most, but not all, participants
lose weight, with an average weight loss of 2.7 to 13.7 pounds at a 95% confidence
level.
7.3.2 Nonparametric Comparison
The dependent-groups t-test directly analyzes the difference scores of matched data
values with a one-sample test, and so does the nonparametric alternative, the Wilcoxon
signed rank test. To illustrate, return to the weight loss data in Listing 7.14, and the
corresponding difference scores. The Wilcoxon test compares the positive differences,
in this case, weight loss, with the negative differences, here, weight gain.
Order the absolute values of the differences from smallest to largest. If weight loss
predominates, the weight loss values should be larger than the weight gain values.
The test assesses the position of these weight losses in the overall distribution of the
ordered absolute value of differences scores. The basis of the test is the Wilcoxon
signed rank statistic, the sum of the ranks of the positive differences.
Specify the Wilcoxon signed rank test to analyze a single variable with the same
wilcox.test() function used to compare two independent-samples. Wilcoxon rank sum
test, Section 7.2.5,
p. 135
The difference
of weight before and after the treatment program is the basis of the test. Use the
with() function to inform R that the variable is the d data frame.
The default value for the confidence level is 95%. Explicitly request the interval
by setting conf.int=TRUE. To change the confidence level, explicitly change the
default value of conf.level=0.95.
alternative
parameter: for a
one-tailed test,
Section 6.3.4, p. 121
Also, by default, the test is two-tailed to evaluate
differences in either direction, regardless of the desired direction. To change to a
one-tailed test, change the default value of alternative from "two.sided" to either
"less" or "greater".
The null hypothesis states that there is no systematic difference between the matched
data values for the two samples. The other possibility is that there is a systematic
difference. For the default two-tailed test, the difference could be in either direction.
paired parameter:
Set to TRUE to
inform R to calculate
and analyze
differences across
samples of data.
Specify the two variables that define the two matched samples, and include the
paired parameter set to TRUE, which informs R to analyze the difference between the
corresponding data values. The analysis implicitly calculates the difference between
the matched data values. The following example is for the Wilcoxon signed rank test
with an implied Difference score.
The output follows in Listing 7.18.146 CHAPTER 7. COMPARE TWO SAMPLES
with(d, wilcox.test(Before, After, paired=TRUE, conf.int=TRUE))
Wilcoxon signed rank test
data: Difference
V = 53, p-value = 0.005859
alternative hypothesis: true location is not equal to 0
95 percent confidence interval:
3 14
sample estimates:
(pseudo)median
7.5
Listing 7.18: Output from the R function wilcox.test() for analysis of a dependent-groups
comparison.
Reject the null hypothesis of no difference of Weight in the Before and After samples.
Weight Loss Effect: p-value ≈ 0.006 < α = 0.05
The sample estimate is the (pseudo) median, similar to, but not precisely equal to, the
sample median. The 95% confidence interval ranges from 3 to 14 (pseudo) medians.
These values are close to the lower and upper bounds of the confidence interval of
the true mean of the difference scores, which are 2.71 pounds and 13.69 pounds.
These results follow from the two-tailed test, interpreting the mean of the differences
in a positive or negative direction.
Interpretation. Effectiveness of the weight loss program
The conclusion from both the parametric t-test and the nonparametric wilcox.test
dependent-samples analyses are the same. The weight loss program appears to
facilitate weight loss, somewhere between about 3 and 14 pounds at a 95% level of
confidence.
1- vs. 2-tailed tests,
Section 6.3.4, p. 120
Some researchers prefer a one-tailed test, predicting an outcome in a specified direction,
though, as noted, the one-tailed test does not permit interpretation of a result in the
opposite direction. For this example, a positive mean of the differences indicates a
successful weight loss program.
7.4 Multiple Proportions
mean difference
analysis, Section 7.2,
p. 128
The difference between proportions is a topic related to the analysis of the difference
of two means. This section’s example is a comparison of two population proportions
from two groups, BMW and Honda motorcycle riders, which is analogous to analyzing
the difference between two means. This same analysis, however, can be extended to
more than two proportions simply by entering data from more than two groups.
Scenario. Do jacket preferences differ between BMW and Honda riders?
Is the proportion BMW riders who prefer thicker jackets different than the propor￾tion for Honda riders?7.4. MULTIPLE PROPORTIONS 147
Analysis from Data
Jackets data set,
Section 4.3.2, p. 75
Consider the previously referenced lessR data set Jackets. The data consist of sales
of a motorcycle clothing company. Each of the 1025 rows of data contains the name
of a motorcycle, BMW or Honda, and the thickness of a purchased jacket: Lite, Med,
or Thick. The respective variables are Bike and Jacket.
d <- Read("Jackets")
> head(d, n=4)
Bike Jacket
1 BMW Lite
2 Honda Lite
3 Honda Lite
4 Honda Med
Listing 7.19: Data set Jackets, which contains 1025 rows of data.
The question of interest regards the purchase of thick jackets. In preparation for a
motorcycle trade show, the vendor must bring the correct inventory. BMW riders
ride sportier bikes than many Honda riders, so perhaps they buy thicker jackets.
Begin with the null hypothesis of equality for the two population proportions, where
π denotes the population proportion of riders wearing thick jackets.
Null hypothesis for a two-tailed test, H0 : πBMW − πHonda = 0
To analyze with Prop_test(), use the same general syntax as for analyzing a single
proportion.
Prop_test() for a
single proportion,
Section 6.16, p. 124 Indicate the presence of more than a single group with the by parameter
set to Bike. The null hypothesis is equality of population proportions. The variable
of interest is Jacket, with success defined as the value Thick. The input and initial
output appear in Listing 7.20.
Prop_test(Jacket, success="Thick", by=Bike)
<<< 2-sample test for equality of proportions without continuity correction
variable: Jacket
success: Thick
by: Bike
Listing 7.20: Analysis of the equality of two population proportions.
Listing 7.21 describes the data with the sample proportion p.
--- Description
BMW Honda
----------- ------ ------
n_Thick 194 117
n_total 418 607
proportion 0.464 0.193
Listing 7.21: Description of sample data for the group proportions.148 CHAPTER 7. COMPARE TWO SAMPLES
The discrepancy of group proportions is obvious from the sample data. The sample
proportion of BMW riders who purchase Thick jackets is pBMW = 0.464, in contrast to
the sample proportion of Honda riders of pHonda = 0.193. However, for completeness,
also consider the inferential analysis in Listing 7.22, the test of homogeneous (identical)
population proportions of purchasing thick jackets across the two groups of riders.
--- Inference
Chi-square statistic: 86.245
Degrees of freedom: 1
Hypothesis test of equal population proportions: p-value = 0.000
Listing 7.22: Test of homogeneity for the analysis of equality of population proportions.
As expected, the null hypothesis is rejected with a p-value approximately equal to 0
within three decimal digits.
p-value = 0.000 < α = 0.05, so do reject H0
The disparity of the sample proportions in some sense obviates the need for an
inferential analysis but always recommended to pursue the full analysis. The key
in this analysis is the fairly large sample size of ntotal = 1025, so even moderate
differences between sample proportions would demonstrate a significant difference.
The larger the sample size, the more likely to properly detect a difference that actually
exists.
Interpretation. Test of the equality of proportions
A difference between the proportions of the preference for purchase of Thick jackets
for BMW and Honda motorcycle riders was confirmed. BMW riders tend to prefer
thicker jackets than Honda riders.
Analysis from Frequencies
If only the frequencies are available the analysis with Prop_test() can still proceed.
The parameter n_succ specifies the number of successes for each group, so is a vector
of multiple values. Similarly, the n_tot parameter is a vector of the total number
frequencies, of observations in each group.
Section 6.4, p. 125
The syntax is the same as for the test of a single
proportion, but with two or more more groups the parameter values become vectors.
Prop_test(n_succ=c(194,117), n_tot=c(418, 607),
group_names=c("BMW", "Honda"))
Inputting either the data or the previously computed group frequencies from the
data into the Prop_test() function, include as many groups as exist in the data.
This analogy is extended to the analysis of differences between multiple means in the
following chapter.7.5. ANALYSIS PROBLEMS 149
7.5 Analysis Problems
Answer the following questions in terms of the hypothesis test, confidence interval
and effect size.
1. Consider the Employee data set, available from within the lessR package.
?Employee for
d <- Read("Employee") more information.
Two of the variables in this data set are Salary and Gender for employees at a specific
company. Are mean Salaries the same for men and women at this company?
a. Answer with a parametric procedure. Evaluate the assumptions.
b. Answer with a a nonparametric procedure. Evaluate the assumptions.
2. Consider the Cars93 data set, available from within the lessR package.
?Cars93 for more
d <- Read("Cars93") information.
Some of the variables in this data set are MPGcity, MPGhiway, and Manual, a binary
variable with a value of 1 for a manual transmission and a 0 for an automatic.
a. Is city fuel mileage the same, on average, for cars with manual transmissions
and cars with automatic transmissions? Answer with a parametric and a
nonparametric procedure. Evaluate the assumptions of the tests.
b. Is there a difference in fuel mileage for city and highway driving? Answer with
a parametric and a nonparametric procedure. Evaluate the assumptions of the
tests.
3. A record company executive wants to know if the average play length of rock and
roll singles differs from that of country and western singles. To answer this question,
she randomly selects 10 country and western singles along with 9 rock and roll singles
and records the following play times.
C&W 3.80 3.30 3.43 3.30 3.03 4.18 3.18 3.83 3.22 3.38
R&R 3.88 4.13 4.11 3.98 3.98 3.93 3.92 3.98 4.67
Does average play length differ?
a. Describe the sample characteristics for the two sets of play times.
b. Do and interpret the hypothesis test of the mean difference.
c. Do and interpret the confidence interval of the mean difference.
d. Are the two inferential procedures consistent? Why or why not?
4. A student accepted to two grad programs could not decide which to choose based
on the merits of each program. Other considerations are non-program based, such as150 CHAPTER 7. COMPARE TWO SAMPLES
the annual snowfall in each city in which the universities are located. The amount of
snowfall in each city for the last 10 years is displayed below.
City A 38.2 33.4 30.2 34.9 42.1 41.1 29.6 39.6 42.0 60.5
City B 30.4 43.3 48.8 38.9 32.6 60.7 60.1 46.8 60.0 47.4
Does average snowfall differ?
a. Describe the sample characteristics for the two sets of snowfall.
b. Do and interpret the hypothesis test of the mean difference.
c. Do and interpret the confidence interval of the mean difference.
d. Are the two inferential procedures consistent? Why or why not?
5. Seven employees assembled parts using the old method one at a time. After
thorough training in the new method, the employees then assembled the same type
of part using the new method. The data for each employee are presented below,
measured to the nearest hundredth of a minute. Each employee contributes a block
of matched data.
Employee 1 2 3 4 5 6 7
Procedure A 17.37 14.59 16.94 24.28 16.16 21.11 22.19
Procedure B 10.37 12.19 15.14 16.08 17.06 18.61 21.49
Is one assembly process faster than the other?
a. Describe the sample characteristics for the two processes.
b. Do and interpret the hypothesis test of the mean difference.
c. Do and interpret the confidence interval of the mean difference.
d. Are the two inferential procedures consistent? Why or why not?
6. After being given some dice, Jim wanted to check that each die was fair, that is,
that the probability of obtaining any one of the six sides of each die was the same.
The following data were recorded for one of the dice.
Side 1 2 3 4 5 6
Freq 8 7 12 13 9 15
Is the die fair?Chapter 8
Compare Multiple Samples
8.1 Quick Start
Install R and lessR as explained in Section 1.2. At the beginning of each R session,
access the lessR functions with library("lessR"). From Chapter 2, read your
data into an R data frame such as d with d <- Read("") to browse for the data file.
Or, locate your data file with a path name or web address between the quotes.
experiment,
Section 7.2.6, p. 137
This chapter compares the means of a response variable across two or more groups.
Define the groups according to different values of the specified grouping variable. If
the data is from an experiment, refer to each grouping variable as a treatment variable,
factor, or independent variable. Refer to the response variable as the dependent or
outcome variable.
This chapter generalizes the t-tests for independent-groups and dependent-groups
from the last chapter. The t-test applies only to the analysis of two samples of a
single grouping variable. The analysis of variance or ANOVA is the corresponding
analysis for two or more samples (groups) of a single variable and also allows for the
analysis of multiple treatment variables.
one-way ANOVA,
Section 8.3, p. 153
Use the lessR function ANOVA() for the between-groups, that is, independent-groups,
of a single grouping variable. Refer to the analysis as a one-way ANOVA of the
response variable. In the following descriptions, Y represents the response variable
and X is a treatment variable.
ANOVA(Y ∼ X)
nonparametric
one-way analysis,
Section 8.3.7, p. 160
ANOVA is a parametric procedure that compares means under the assumption of
normality. Nonparametric analyses make less restrictive assumptions. Use the R
function kruskal.test() for the nonparametric alternative to one-way ANOVA, the
Kruskal-Wallis rank sum test.
with(d, kruskal.test(Y ∼ X))
The dependent-groups or within-groups t-test defines a block of matched responses
for two samples. ANOVA generalize the dependent-groups t-test to as many samples
151152 CHAPTER 8. COMPARE MULTIPLE SAMPLES
as specified. randomized block
ANOVA, Section 8.4,
p. 161
The more general ANOVA version is called a randomized block ANOVA,
accessible with ANOVA(). In the following function calls, Blocks is the blocking variable
of matched responses for each row of data.
ANOVA(Y ∼ X + Blocks)
nonparametric
randomized blocks
analysis,
Section 8.4.7, p. 168
The nonparametric alternative to the randomized blocks ANOVA is the Friedman rank
sum test, accomplished with the R function friedman.test().
with(d, friedman.test(Y ∼ X|Blocks))
All of these specifications of ANOVA analyses are examples of the general linear model.
Specify one variable, the response variable, in terms of one or more predictor variables,
the variables on the right side of the tilde, ∼ .
general linear model,
Section 13.2.3, p. 280
8.2 Experimental Design
independent-groups
t-test, Section 7.2,
p. 128
dependent-groups
t-test, Section 7.3,
p. 141
The independent-groups and dependent-groups t-tests compare the response variable
of interest across two data samples. These analyses and the data structures that
support them represent a fundamental aspect of analyzing experimental data. The
groups, the samples, in these two designs are either independent or organized into
blocks of dependent data. Also, refer to these two sampling procedures as between￾experiment, groups and within-groups, or between-samples and within-samples.
Section 7.2.6, p. 137
As previously
defined, each group, or sample, represents a value of the independent variable, called
a treatment variable, in the context of an experiment. The variable compared across
the groups is the response variable or dependent variable.
experimental
design: The
organization of the
participants in an
experimental study
related to one or
more treatment
variables and the
associated statistical
analysis of the
responses.
Structuring the data into two independent-groups or two dependent-groups presents
only the most basic possibilities for the research project design. What if the research
project consists of more than two groups for the treatment variable? What if the
project includes multiple treatment variables? Many types of designs are possible
that generalize well beyond the two samples amenable to analysis with the t-tests.
See Kirk (2013, pp. 45-47) for a comprehensive list of designs, followed by a detailed
explanation of each design.
We limit the consideration of designs in this book to the most common designs in
the scientific literature based on distinguishing between-groups from within-groups
designs. We refer to these different types of designs by the number of between￾groups treatment variables and the number of within-groups treatment variables
with the following terminology. A B followed by a number indicates the number of
between-groups variables and a W followed by a number indicates the number of
within-groups variables. For example, refer to the design of a study analyzed by an
independent-groups t-test as B1W0. Similarly, refer to the within-groups design for
the dependent-groups t-test as B0W1.
Analysis of variance, ANOVA, generalizes the t-test to analyze as many groups as
specified for the treatment variable of interest and also allows for multiple treatment8.3. ONE-WAY DESIGN 153
variables. The material in the following section generalizes the t-test of the mean
difference to an ANOVA capable of comparing the means of two or more groups.
More generally, each categorical variable that defines a set of groups is a grouping
variable. In the context of an experiment, refer to the grouping variable as the
treatment variable. However, just examining the data and its structure cannot convey
if the data were collected as an experiment with randomization and manipulation of
the values of the grouping variable or the values were pre-existing, such as with Gender.
ANOVA analyzes the structured data regardless of the context in which the data was
collected, more generally organized by grouping variables, or, for experiments, by
treatment variables.
8.3 One-Way Design
One-way design:
Two or more
independent-groups
of participants for
one grouping
variable.
For the one-way ANOVA design, as with the t-test of the mean difference, a single
grouping variable defines the groups. The data values recorded for one group are
unrelated to those recorded for another group. The different groups (samples) of
people, organizations, laboratory rats, or whatever constitute the unit of analysis
with no cross-group connections.
One-way design
classification: B1W0
Refer to such a design as a between-groups design or
independent-groups design, with no within-groups variables. As with data structured
to be appropriate for the independent-groups t-test, denote this design as B1W0, except
now the grouping variable can have more than two levels.
8.3.1 Variability
Comparing two means is straightforward: Subtract one sample mean from the other.
How to compare three, or four, or ten means and express this comparison as a single
number? The answer is variability. If all samples are from the same population, then
only sampling error differentiates the sample means.
sampling error:
Random fluctuations
from sample to
sample that result in
different values of
sample statistics.
An example of sampling error is
flip a fair coin 10 times and obtain six heads, and then the next 10 flips of the same
coin yields four heads. The coin is the same, but samples differ.
Random samples from the same population yield different values of the sample mean.
If sampling error is the sole reason for the different values of the sample mean in
different samples, then the means should be relatively close to each other. On the
contrary, if the means are from different populations, then sampling error differentiates
them, but so do the different corresponding population means. Samples coming from
populations of different means generally yield sample means that tend to be more
dispersed than sample means from samples from the same population.
standard deviation,
Section 6.2.1, p. 101
Assess differences among group means according to their variability. The standard
deviation, and its square, variance, directly assesses the amount of dispersion of
a set of numbers about their respective mean. If there are differences among the
corresponding population means, then the standard deviation and variance of the
sample means are generally larger than if not. For mathematical reasons the “analysis
of variance” focuses on the variance of the sample means instead of their standard
deviation.154 CHAPTER 8. COMPARE MULTIPLE SAMPLES
8.3.2 Example
Apply the one-way ANOVA to a set of measurements of the response variable assessed
over multiple independent samples.
Scenario. Examine the effects of Dosage on task completion Time
Does arousal impact the ability to complete a task? To investigate, 24 laboratory
rats were randomly and equally divided into three groups of eight, and then given
one of three dosages of an arousal inducing drug: 0, 5, and 10 milligrams. Following
the dosage, each rat completed a maze to obtain a food reward. The response
(dependent) variable is the Time in seconds to complete the maze.
This study is an experiment, so the grouping variable in this analysis is a treatment
variable, Dosage.
experiment and
related terms,
Section 7.2.6, p. 137
The control group does not experience the treatment, so no drug
dosage received. The experimental groups receive a treatment, a drug dose of 5 or 10
mg. The response variable is task completion Time, analyzed across the groups. The
one-way analysis of variance compares the means from each of the three samples.
8.3.3 Data and Input
Read function,
Section 2.3.1, p. 25
Read the data from the lessR data file Anova_1way, illustrated in Listing 8.1.
d <- Read("Anova_1way")
> d
Dosage Time
1 00mg 25.6
2 00mg 25.8
...
11 05mg 24.8
12 05mg 24.0
...
23 10mg 13.1
24 10mg 11.9
Listing 8.1: Dosage data excerpt.
The grouping variable, Dosage, has three unique values, so the t-test of the mean
difference is not applicable. Instead, analyze the impact of Dosage on task completion
Time with the lessR function ANOVA().
ANOVA()
function: Includes
analysis for one-way
ANOVA.
The syntax is the standard R model specifica￾tion, so is the same as for ttest(). List the response variable first, followed by the
tilde, ∼ , and then the grouping variable that defines the groups.
Input. One-way ANOVA
ANOVA(Time ∼ Dosage)
The output begins with summary statistics that describe the sample.8.3. ONE-WAY DESIGN 155
8.3.4 Description
Summary Statistics
The first part of the ANOVA() output describes the model, the variables in the model,
the levels of the treatment variable, and the number of rows of data. The descriptive
statistics appear next in tabular form, shown in Listing 8.2. The grand mean is the
mean of all of the data.
grand mean: Mean
of all of the data.
DESCRIPTIVE STATISTICS
n mean sd min max
00mg 8 24.26 2.69 18.40 26.50
05mg 8 23.41 2.85 19.00 28.20
10mg 8 17.82 3.96 11.90 24.20
Grand Mean: 21.833
Listing 8.2: Descriptive statistics for each group of task completion Time data.
grayscale
visualizations,
Section 4.1, p. 64
Figure 8.1 provides the gray-scale version of the scatter plot from ANOVA() of the
individual data values and the group means as diamonds.
Figure 8.1: Scatterplot and means for each group of task completion Time data.
A horizontal line drawn through each plotted group mean enhances the comparison of
the means. The mean of task completion Time for the group with the largest amount
of dosage, 10 mg, is 17.82, quite a bit smaller than the remaining two means.
The output in Listing 8.2 and Figure 8.1 can also be generated apart from the ANOVA()
function. The lessR function Plot() with the treatment variable listed first, followed
by the response variable, provides the same scatterplot.
Plot(Dosage, Time)
The lessR function pivot() provides the specified summary statistics for Time
organized by Dosage group.156 CHAPTER 8. COMPARE MULTIPLE SAMPLES
pivot(d, c(mean, sd, min, max), Time, by=Dosage)
Effect Size
The p-value in relation to α indicates if an effect is detected, yes or no. Effect size
statistics assess the magnitude of the effect of the treatment variable on the response
variable. Listing 8.3 presents the effect sizes of Dosage on task completion Time.
effect size,
Section 6.3.3, p. 118
-- Association and Effect Size
R Squared: 0.47
R Sq Adjusted: 0.42
Omega Squared: 0.41
Cohen’s f: 0.84
Listing 8.3: Indices for the magnitude of the effect.
The ANOVA() function reports two classes of these effect size statistics. Indices of the
first type are association indices, a type of correlation coefficient that assesses the
extent of the relation between the treatment and response variables. The closer to
1.0, the stronger the association.
The association statistics of primary interest, which estimate the magnitude of the
association in the population, are R squared adjusted, R2
adj = 0.42, and omega
squared, ω
2 = 0.41. These two coefficients are more conservative than the unadjusted
R2
statistic, R2 = 0.47, which has an upward bias, particularly in smaller samples
(Albers & Lakens, 2018). Some software packages report a statistic called eta-squared,
η
2
, which may appear in place of ω
2
. However, η
2
, like R2
, tends to overestimate the
true effect size.
Focus on the values of R2
adj and ω
2
to estimate the association effect size. Values
above 0.3 are typically considered to indicate a moderate effect, with values above 0.6
indicating a relatively strong effect. The obtained values of R2
adj = 0.42 and ω
2 = 0.41
indicate a moderate level of association between arousal and task completion time.
The second type of effect size statistics directly assess the size of the group differences.
An example is Cohen’s f. This statistic is analogous to the standardized mean
difference, Cohen’s d, the number of standard deviations that separate the group
means.
Cohen’s d,
Section 7.2.3, p. 131
To indicate the separation among multiple means, Cohen’s f provides the
average difference of each group mean from the overall mean of the data, standardized
in terms of the standard deviation of the data in which each data value is deviated
from its own cell mean. Cohen (1988) suggests that for most contexts a value of
f = 0.40 or larger indicates what is generally considered a large effect, larger than
0.25 a medium effect, and larger than 0.10 a small effect.
Interpretation. Effect size
With Cohen’s f equal to 0.84, arousal appears to substantially impact task comple￾tion time for the data analyzed.8.3. ONE-WAY DESIGN 157
The analysis of effect size indicates an effect of arousal on task completion time, so
perhaps this effect found in the sample generalizes to the population. That potential
generalization is analyzed next with inferential statistics.
8.3.5 Inference
Evaluate the Overall Effect
Do these descriptive results generalize to the population as a whole? Inferential statis￾tics answer that question, beginning with the ANOVA summary table in Listing 8.4.
null hypothesis
one-way ANOVA:
All group population
means are equal to
each other.
The null hypothesis tested in this inferential analysis is that the population mean
task completion Time is the same for all three drug dosages.
H0 : µ00mg = µ05mg = µ10mg
The alternative to the null hypothesis is that at least two of the group means are not
equal.
H1: At least two of the population group means are not equal
This alternative to the null hypothesis states that as a group the three group means
are not all equal, at least one group mean is not equal to the others. The results
of the hypothesis test are presented in the ANOVA Summary Table, shown for this
analysis in Listing 8.4.
-- Summary Table
df Sum Sq Mean Sq F-value p-value
Dosage 2 195.69 97.85 9.46 0.0012
Residuals 21 217.16 10.34
Listing 8.4: One-way ANOVA summary table. F-value: Test
statistic for an
ANOVA null
hypothesis. The F-value is the test statistic for the null hypothesis that all groups have equal
population means, in this case, the mean population task completion Time. variance, mean
square, Section 6.2,
p. 102
degrees of freedom,
Section 6.2.1, p. 103
A
variance is the ratio of the sum of squared deviations from a mean to the degrees of
freedom, n − 1. These two values are specified in the first two columns of an analysis
of variance summary table, df and Sum Sq, for the various sources of variation in the
study. The third column, Mean Sq estimates each corresponding variance, which is
the mean square for each source.
The analysis of variance compares the two variance estimates. For this one-way
analysis, one estimate follows from the variability of the sample mean completion
Time for each Dosage, MSDosage = 97.85. As is true of any sample variance, this
variance is nudged greater than 0 by sampling error. However, any differences between
population group means also contribute to the variability among the sample means. residual in one-way
ANOVA: Deviation
of a data value from
its group mean.
The other variance estimate is of the residuals, the combined variation of each data
value about its own group mean for all the groups, MSResiduals = 10.34. Everyone
within each group is treated the same, so this baseline residual variance presumably
reflects only pure sampling error. The F-value compares the two variance estimates,158 CHAPTER 8. COMPARE MULTIPLE SAMPLES
or mean squares, by their ratio. Indicate the numerator and denominator degrees of
freedom as: 2,21.
F2,21 =
MSDosage
MSResiduals
=
97.85
10.34
= 9.46
If the null hypothesis is true, the two variance estimates approximately equal each
other, resulting in an expected ratio of about 1. The larger the discrepancy of
population group means, the larger the numerator, so the more the F-ratio exceeds
the value of 1. Moreover, assuming the null hypothesis is true, the distribution over
hypothetical repeated samples of this statistic is known. The p-value, the probability
of an F-value as high or higher than the obtained F = 9.462, can be calculated.
Dosage Effect:
p-value = 0.0012 < α = 0.05, so reject H0
Reject the null hypothesis of no population mean differences across groups.
Interpretation. Overall evaluation of arousal on task performance
At least two of the Task Time group means differ from each other. Arousal, as
induced by the administered drug, affects task completion Time.
Post-hoc Multiple Comparisons
Reject the null hypothesis of three equal population group means of task completion
omnibus F-test: Time.
Assessment of a null
hypothesis over all
the treatment levels.
This test from the ANOVA summary table in Listing 8.4 is the omnibus F-test,
which uses the F-value to simultaneously assess the equality of means for all the levels
of the treatment variable. If the omnibus F-test is significant, the next question
follows: Which specific means are not equal to each other?
To identify the population means that differ from the rest, do not run a series of
t-tests between all feasible pairs of means at α = 0.05. Even if all of the population
means are equal, the sample means are not. Some sample means will be larger by
chance than others, skewing the chances that the largest sample mean is judged
significantly different from the smallest sample mean, even if all population means
are equal. Instead, adjust the probabilities of the sequence of tests with what are
called post-hoc tests, reported in Listing 8.5.
post-hoc test: A
test following an
omnibus F-test to
isolate a pattern of
differences between
population means.
TUKEY MULTIPLE COMPARISONS OF MEANS
Family-wise Confidence Level: 0.95
------------------------------------
diff lwr upr p adj
05mg-00mg -0.85 -4.90 3.20 0.86
10mg-00mg -6.44 -10.49 -2.38 0.00
10mg-05mg -5.59 -9.64 -1.53 0.01
Listing 8.5: All pairwise Tukey multiple comparisons.
Post-hoc tests address this “taking advantage of chance” possibility from running
multiple group comparisons. The post-hoc test accounts for the additional possibility8.3. ONE-WAY DESIGN 159
of finding significant results simply because there are many tests to perform simulta￾neously. The tests reported in Listing 8.5 are Tukey’s multiple mean comparisons,
which simultaneously maintain α = 0.05 for the entire family of tests.
The column labeled p adj, in Listing 8.5, lists the p-value adjusted for the three
simultaneous tests. Two of three comparisons in the p adj column fall below α = 0.05,
those for 10mg vs. 00mg and 10mg vs. 05mg. Time to complete the task differs for
the group with the largest dosage from each of the two groups with the least dosage.
However, the two group means with the least dosage could not be separated from
each other.
Figure 8.2 presents a visualization of Listing 8.5 with the corresponding confidence
intervals held at the family-wise confidence level of 95%.
−10 −8 −6 −4 −2 0 2
10mg−05mg
10mg−00mg
05mg−00mg
95% family−wise confidence level
Differences in mean levels of Dosage
Figure 8.2: Tukey family-wise confidence intervals for each of three pairwise comparisons.
The two intervals, which do not cross 0, are the two intervals with a p-value < α = 0.05.
The hypothesis test and the corresponding confidence intervals demonstrate pairwise
differences between the highest drug dosage level and the remaining two levels.
Interpretation. Post-hoc analysis of mean differences
Arousal affects task completion Time, but only for a sufficient level of arousal, here
induced by 10 mg of the drug.
8.3.6 Search for Outliers
Every analysis should investigate the possibility of outliers in the data. These values
can represent errors in data entry and so should be correct or deleted. They can
also represent data from processes different than which generated the majority of the
data, and so should be analyzed separately.
Each cell mean summarizes the data for the corresponding treatment level. The best
prediction of a data value, its fitted value, for a given treatment condition is its cell
mean. ANOVA() generates a table of the residuals from each respective cell mean and
standardized residuals, the first three rows appearing in Listing 8.6. By default, the
rows of data and residuals are sorted by the magnitude of the standardized residuals
and only the first 20 are listed. residual
Section 8.3.5, p. 157 A residual in a one-way ANOVA is the difference
between a data value and its corresponding cell mean.160 CHAPTER 8. COMPARE MULTIPLE SAMPLES
Fitted Values, Residuals, Standardized Residuals
-----------------------------------------
17 10mg 24.20 17.82 6.38 2.12
24 10mg 11.90 17.82 -5.92 -1.97
7 00mg 18.40 24.26 -5.86 -1.95
...
Listing 8.6: First three rows of the table of residuals and related values.
For any normal distribution, about 95% of the data values are within two standard
deviations of the mean. Assuming this normality for the residuals, each residual
divided by the standard deviation of the residuals, the corresponding standardized
residual, should generally not be much larger than 2. Any standardized value larger
than 3 is immediately suspect as a potential outlier. The largest standardized residual
from an examination of all the residuals is only 2.12.
Interpretation. Outlier analysis
The data are judged to be free of outliers.
The default settings that control the display of the rows of data and other values
res_rows can also be modified.
parameter: The
number of rows of
data to be displayed
for the residuals
analysis.
The res_rows parameter can change the default of 20 rows
displayed to any value up to the number of rows of data, specified by the value of
"all". To turn this parameter off, specify a value of 0. The res_sort parameter
can change the sort criterion from the default value of "zresid". Other values are
"fitted" for the values perfectly consistent with the estimated model, and "off" to
res_sort parameter: leave the rows of data in their original order.
The sort criterion for
the residuals analysis.
8.3.7 Nonparametric Alternative
The standard nonparametric comparison of two or more independent samples is the
Kruskal-Wallis rank sum test. Typical of nonparametric tests, the analysis is of
the ranks of the data. The analysis first converts ratio or ordinal data to ranks.
Data already in the form of ranks, ordinal data, is analyzed directly. Because the
test is nonparametric, the analysis makes no assumptions regarding the underlying
distribution shapes, such as normality. So, unlike traditional ANOVA, data can be
ordinal and non-normal, either condition which violates the assumptions underlying
ANOVA.
All the data for all the samples are first ranked as one large sample, and then the
ranks are summed for the data values within each group. The more discrepant these
sums, or the corresponding mean ranks, the more likely that the responses from one
or more samples tend to be larger than the responses in other samples. The ANOVA
compares the group means to each other, whereas the Kruskal-Wallis rank sum test
compares the mean ranks.
dosage study,
Section 8.3.2, p. 154
To illustrate, return to the Dosage data. The nonparametric null hypothesis is that
the distributions perfectly overlap. The alternative hypothesis is that at least one
of the distributions of the data values is shifted to the right or left of at least one8.4. RANDOMIZED BLOCK DESIGN 161
other distribution. Test the hypothesis from the analysis provided by the R function
kruskal.test().
kruskal.test()
function, R:
Non-parametric
alternative to the
one-way ANOVA
Input. Independent-samples Kruskal-Wallis rank sum test
with(d, kruskal.test(Time ∼ Dosage))
As the test is from an analysis of ranks, the null hypothesis is that the mean of the
ranks across the samples are from identical population distributions. Instead of the
exact p-value, the test uses a chi-square approximation, which requires minimum
sample sizes of about five. The output appears in Listing 8.7.
Kruskal-Wallis rank sum test
data: Time by Dosage
Kruskal-Wallis chi-squared = 9.6204, df = 2, p-value = 0.008146
Listing 8.7: Nonparametric Kruskal-Wallis rank sum test analysis.
To evaluate if Dosage affects Task Time, compare the resulting p-value to α = 0.05.
Dosage Effect: p-value ≈ 0.008 < α = 0.05, so reject H0
Reject the null hypothesis of no difference between the mean ranks of task completion
time for differing levels of arousal.
ANOVA conclusion,
Section 8.4, p. 157 Interpretation. Kruskal-Wallis analysis of arousal and task completion time
The conclusion that dosage of the stimulating drug decreases task completion time
remains the same as the conclusion from the ANOVA. Higher levels of arousal lead
to faster completion times.
The Kruskal-Wallis rank sum test and one-way ANOVA generally provide the same
conclusion or either rejecting or not rejecting the null hypothesis. However, should
the results differ, the more conservative Kruskal-Wallis test with fewer underlying
assumptions would generally be preferred.
8.4 Randomized Block Design
dependent samples
Section 7.3.1, p. 141
The dependent-samples t-test analyzes data organized into blocks for two levels. The
dependent-samples design with a single treatment variable is a randomized block
design or a within-groups design, which can specify more than two levels of the
blocking variable. Because there are no between-group variables and one within-group
variable, denote the design as B0W1.
randomized block
design: Dependent
samples for one
treatment variable
with blocks of
matched data values,
denoted as B0W1.
If the same experimental unit, such as the same
person, provides a measurement for each block of data, refer to the design more
specifically as a repeated measures design.
repeated
measures design:
Randomized blocks
design with the same
experimental unit
across each block.162 CHAPTER 8. COMPARE MULTIPLE SAMPLES
8.4.1 Example
Scenario. Evaluate the effectiveness of four different pre-workout supplements
Pre-workout supplements are supposed to increase the effectiveness of weight
training. Evaluate four different supplements for their effectiveness. Use the bench
press as the exercise routine to assess effectiveness.
Eight people with differing amounts of muscle strength took one of four different
pre-workout supplements and then did a bench press of 125 lbs as many times as
possible. Each person did four workouts at four different times, with a different
supplement before each workout. The experimenter randomized the presentation
order of the supplements to each person to avoid any artifacts from the presentation
order.
8.4.2 Data
Read() function,
Section 2.3.1, p. 25 First, read the data from the lessR data file Anova_rb. Listing 8.8 presents the
corresponding R data frame.
d <- Read("Anova_rb")
> d
Person sup1 sup2 sup3 sup4
1 p1 2 4 4 3
2 p2 2 5 4 6
3 p3 8 6 7 9
4 p4 4 3 5 7
5 p5 2 1 2 3
6 p6 5 5 6 8
7 p7 2 3 2 4
repeated Listing 8.8: Workout data.
measures: A block
of data that consists
of multiple
measurements on the
same unit of analysis.
The data values in Listing 8.8 display a format for data organization often encountered
in the analysis of experimental data. Each row contains multiple data values for that
person, a block of dependent, matched data values. In this example, the dependent
data values are repeated measures. Listing all the data for a single unit in one row is
convenient and compact. The data table with multiple data values per row is called
wide-form data.
wide-form data:
Groups of data
values arranged with
multiple data values
of the same variable
per row.
However, similar to requirements in other data analysis packages, the standard R
routine for analysis of variance, aov() requires the input data table organized with
one data value per row. Because lessR ANOVA() relies upon aov(), ANOVA() requires
the same data organization. long-form data:
One data value per
row of data with the
variables properly
identified.
This organization of a data table with one data value
per row is called the long-form data, which contrasts to the wide-form shown in
Listing 8.8. If there is only one data value per unit, then the wide and long forms of
a data table are the same.
These four columns of supplement data form a single variable, Supplement, in the
long form. Reshaping the wide-form data in Listing 8.8 into a long-form data table8.4. RANDOMIZED BLOCK DESIGN 163
transforms the four columns of supplements into two columns. We name the treatment
variable, Supplement, which identifies the supplement – sup1, sup2, sup3, or sup4 –
associated with the corresponding data value. Supplement is the grouping variable
that identifies the groups. The second variable is the response variable, the number
of bench press repetitions, which we name Reps.
The lessR function reshape_long() reshapes a data table from wide-form to long￾form.1
reshape_long()
function, lessR:
Convert a wide-form
data table to
long-form.
There is no assumption of a name for the input data frame, such as d.
transform
parameter: Specify
the columns in the
wide-form data table
to convert to
long-form.
Instead,
enter the data frame’s name to be reshaped as the first parameter value in the function
call, then invoke the second parameter, transform, to specify the wide-form columns
that should be transformed. Optionally, use the parameter ID to name and create
an ID field in the long-form data table that links the data value in a row with the
person from which the data was measured. However, the wide-form data in Listing 8.8
already contains an ID field, Person, so ID is not needed.
The original, wide-form version of the data in Listing 8.8 provides no information
regarding the long-form name of the response variable, nor is there a name for the
grouping variable, the treatment variable in the context of an experiment. By default,
reshape_long() names these variables Response and Group in the reshaped, long￾form data table. Or, use the response parameter to name the response variable and
the group parameter to name the grouping variable.
response parameter:
Optional name of the
response variable.
group parameter:
Optional name of the
grouping or The result is the data frame treatment variable. d re-organized as in Listing 8.9.
Input. Reshape wide-form data to long-form data
d <- reshape_long(d, transform=sup1:sup4,
group="Supplement", response="Reps")
> d
Person Supplement Reps
1 p1 sup1 2
2 p2 sup1 2
...
27 p6 sup4 8
28 p7 sup4 4
Listing 8.9: First and last rows of reshaped data for the randomized block design.
Each column heading in the original, wide-form version in Listing 8.8 becomes a value
of the grouping variable in the long-form. The reshaped long-form data is now ready
for analysis.
8.4.3 Input
ANOVA() can analyze a one-treatment variable randomized block design with one
blocking variable. Specify the response variable before the tilde, ∼ , then the treatment
variable, followed by a plus sign, +, and then the blocking variable.
ANOVA() function,
lessR: Includes
analysis for the
randomized blocks
ANOVA. 1
reshape_long() calls the standard R function reshape() with more sensible, understandable
parameter names and the ability to input a range of columns to transform to long form.164 CHAPTER 8. COMPARE MULTIPLE SAMPLES
Input. Randomized Blocks ANOVA
ANOVA(Reps ∼ Supplement + Person)
balanced design:
Each combination of
levels of the
independent
variables has the
same number of
responses.
The first part of the output lists the response variable, the treatment and blocking
variables, the number of rows of data, and indicates if the design is balanced. A
balanced design has the same number of responses for each combination of the levels of
the treatment variables. This design has two treatment variables. Each combination
of Supplement and Person levels has exactly one response.
unbalanced design,
Section 9.3.3, p. 188
A more sophisticated
analysis is required if the design is not balanced.
8.4.4 Description
Summary Statistics
The lessR function ANOVA() generated Figure 8.3, a visualization of the same data in
Listing 8.8 from which the analysis proceeds. The data table and its visualization
show that the fourth workout supplement incentivized the most repetitions. The fifth
person has the least muscle strength, and the third person is the strongest performer.
2
4
6
8
Supplement
Reps
sup1 sup2 sup3 sup4
Person
p3
p6
p4
p2
p7
p1
p5
Figure 8.3: Plot of the four data values for each person.
Find the means of the treatment and blocking variables in Listing 8.10.
Marginal Means
--------------
Supplement
sup1 sup2 sup3 sup4
3.57 3.86 4.29 5.71
Person
p1 p2 p3 p4 p5 p6 p7
3.25 4.25 7.50 4.75 2.00 6.00 2.75
Listing 8.10: Sample means for the four different supplements and seven people.8.4. RANDOMIZED BLOCK DESIGN 165
The output also includes the grand mean, not shown, which is 4.357 reps.
grand mean,
Section 8.3.4, p. 155 The mean
number of repetitions for the fourth workout supplement, 5.71, considerably surpasses
the remaining three means in this data set. Is this difference, and the other differences,
large enough to reject the null hypothesis, H0, of no differences in the population?
Effect Size
To evaluate the impact of type of supplement on performance, we first proceed with
the analysis of effect size in Listing 8.11, followed by a formal evaluation of the
null hypothesis. As always, accompany a significance test with an estimate of the
corresponding effect sizes. effect size,
Section 8.3.4, p. 156
The effect sizes in Listing 8.11 are for both the treatment
effect and the blocking variable.
Effect Size
-----------
Partial Omega Squared for Supplement: 0.38
Partial Intraclass Correlation for Person: 0.79
Cohen’s f for Supplement: 0.78
Cohen’s f for Person: 1.91
Listing 8.11: Effect sizes for the randomized block design.
The effect for differences in muscle strength for the different participants is larger
than for type of Supplement. People do differ in terms of strength, so blocking the
participants according to their strength is worthwhile. Both the correlation based
measure for type of Supplement, the omega squared of 0.38, and also Cohen’s f of
−.78, are, however, also large. The correlation based effect size for Person is not
omega squared but the Intraclass Correlation coefficient. The Person variable is
randomly sampled instead of set at fixed levels as are the four chosen supplements,
and so requires a different index of effect size.
8.4.5 Inference
Evaluate the Overall Effect
The study evaluates the effectiveness of the workout supplements increasing repetitions
of the bench press. Type of Supplement is the treatment variable for the design. The
omnibus null hypothesis is an identical population mean number of repetitions for all
four supplements.
H0 : µsup1 = µsup2 = µsup3 = µsup4
The alternative to the null hypothesis is that at least two of the population group
means are not equal.
Also, the participants’ initial levels of muscle strength are presumably different, so
their abilities to perform the bench press differ. For Person to serve as a blocking
variable, the Person effect should be significant, participants should differ on their
initial abilities. Still, that relationship is secondary to the primary interest of the166 CHAPTER 8. COMPARE MULTIPLE SAMPLES
effectiveness of the workout supplements.
Listing 8.12 presents the central output of an analysis of variance, the summary table
that partitions the sum of squares across the various effects, and also provides the
omnibus F-statistic and corresponding p-value for each testable effect, for Supplement
F-statistic, and for Person.
Section 8.3.5, p. 158
Each F-statistic is the ratio of the mean square for that effect, from
the Mean Sq column, to the Residuals mean square, 0.94.
Analysis of Variance
--------------------
df Sum Sq Mean Sq F-value p-value
Supplement 3 19.00 6.33 6.71 0.0031
Person 6 88.43 14.74 15.61 0.0000
Residuals 18 17.00 0.94
Listing 8.12: ANOVA summary table for the randomized block design.
The primary p-value of interest is for Supplement. Reject the null hypothesis of equal
group means for the repetitions response variable.
Supplement Effect: p-value = 0.0031 < α = 0.05, so reject H0
We conclude that at least some of the supplements differ in their effectiveness.
The p-value for the blocking variable, Person, is also less than α = 0.05. The
participants differ regarding performance on the bench press, which justifies blocking
participants by their overall strength level. This result is consistent with the strong
effect size for Person, Cohen’s f = 1.91.
advantage of blocking,
Section 8.4.8, p. 168
Post-hoc Multiple Comparisons
The omnibus F-value for Supplement is significant, and the value for Cohen’s f of 0.78
indicates a relatively large effect. The supplements differ in regard to effectiveness, and
the differences are relatively large. What specific mean differences can be isolated that
Tukey multiple contributed to these results?
comparisons,
Section 8.3.5, p. 159
Listing 8.13 presents the Tukey multiple comparisons
and their associated p-values for each Supplement.
Factor: Supplement
-----------
diff lwr upr p adj
sup2-sup1 0.29 -1.18 1.75 0.95
sup3-sup1 0.71 -0.75 2.18 0.53
sup4-sup1 2.14 0.67 3.61 0.00
sup3-sup2 0.43 -1.04 1.90 0.84
sup4-sup2 1.86 0.39 3.33 0.01
sup4-sup3 1.43 -0.04 2.90 0.06
Listing 8.13: Tukey’s multiple family-wise comparisons at the family-wide confidence level
of 0.95 for the treatment variable in the randomized block design.
Two comparisons are significant, with p-value < α = 0.05. These comparisons are
between the fourth workout supplement and the first and second supplements, with8.4. RANDOMIZED BLOCK DESIGN 167
the mean repetitions larger for the fourth workout supplement. A third comparison
between the fourth and third supplements just misses significance, at p-value = 0.06.
The primary finding of this analysis is that there are differences among the supplements
to promote more weight lifted during a workout. Still, these differences are largely
due to the increased effectiveness of the fourth supplement. The analysis did not
differentiate the efficacy of the first three supplements from each other.
8.4.6 Other Output
Although not shown here, the ANOVA() function also displays the residuals and
standardized residuals for the randomized block design, similar to the output shown
in Listing 8.6. residuals,
Section 8.3.6, p. 160
These values may indicate that a data value is a potential outlier. In
this analysis, one residual for the first person working out after taking the fourth
workout supplement was larger than 2.0 in magnitude, −2.06. Just one residual of
this modest size indicates the lack of outliers in the data. Use the res_rows and
res_sort parameters to control the display of the residuals.
res_rows, row_sort
parameters,
Section 8.3.6, p. 160
fitted value: Data
value predicted from
a combination of the
values of the
treatment and
blocking variables.
The ANOVA() function also provides a scatter plot with the fitted values, Listing 8.4.
A fitted value applies to each data value in the analysis and all values within the same
cell have the same fitted value. It is the data value predicted for each combination of
the treatment conditions, Supplement type and Person.
Figure 8.4: Scatter plot of the data with the values fitted by the model.
As can be seen from Listing 8.4, the progression of repetitions steadily increases from
the first to the fourth workout supplement. The omnibus F-test showed that not all
the supplements are equal in terms of their mean effectiveness. The Tukey multiple
comparisons showed that only the fourth workout supplement could be differentiated
from the remaining supplements.
Interpretation. Evaluation of pre-workout supplements
The fourth supplement is the most effective to stimulate more intense, more energetic
workouts.168 CHAPTER 8. COMPARE MULTIPLE SAMPLES
8.4.7 Nonparametric Alternative
The nonparametric alternative to the randomized blocks ANOVA is the Friedman
rank sum test. As with the other nonparametric tests, the Friedman test is based on
the ranks of the data values instead of the data values themselves. This test for the
randomized blocks design ranks the data values within each block of data across the
different levels of the treatment variable.
workout supplement
data, Listing 8.8 For the workout supplement data, each of seven participants generates data values for
each of four supplements. The Friedman test ranks the data values for each person,
here the number of repetitions of the bench press of 125 lbs after taking each workout
supplement. Then rankings of each workout supplement are compared across all
the people, with the mean rank of each workout supplement over the eight people
calculated. The null hypothesis is that the distribution of repetitions for each workout
supplement is the same. The alternative hypothesis is that at least one distribution
differs from the others.
friedman.test()
function, R:
Nonparametric
analysis of a
randomized blocks
design.
R implements this test with the function friedman.test(). The syntax is slightly
different from the ANOVA() syntax. The vertical bar, |, replaces the plus sign, +. List
the treatment variable first. The blocking variable follows the vertical bar.
Input. Nonparametric Friedman rank sum test
with(d, friedman.test(Reps ∼ Supplement|Person))
The output follows in Listing 8.14.
Friedman rank sum test
data: Reps and Supplement and Person
Friedman chi-squared = 9.9545, df = 3, p-value = 0.01896
Listing 8.14: Output of the R Friedman rank sum test.
The test statistic for the Friedman rank sum test is chi-square, χ
2
. Under the
null hypothesis the value of this statistic over repeated samples is known, so the
corresponding probability of any result given the assumption of the null is also known.
Supplement Effect: p-value = 0.0190 < α = 0.05, so reject H0
The p-value is less than α = 0.05, so reject the null hypothesis of no difference
between the number of bench press repetitions for the four different supplements.
The conclusion from this omnibus test is that at least one workout supplement has a
different distribution of error values than the rest, consistent with the result of the
parametric randomized blocks ANOVA.
8.4.8 Advantage of Blocking
Both the one-way ANOVA and the randomized block ANOVA have one treatment
variable. Both analyses aim to evaluate the differences among group means for this8.4. RANDOMIZED BLOCK DESIGN 169
treatment variable. The distinction is that the randomized block design analyzes
the data organized by blocks across the treatment levels and so introduces a second
grouping variable into the design.
The advantage of this analysis is evident from Listing 8.15. This analysis is of the
same data for the type of pre-workout Supplement related to the ability to do the
bench press, but here with the one-way, independent-groups ANOVA, which ignores
the blocking structure.
Analysis of Variance
--------------------
df Sum Sq Mean Sq F-value p-value
Supplement 3 19.00 6.33 1.44 0.2554
Residuals 24 105.43 4.39
Listing 8.15: Evaluate effectiveness of type of Supplement with a one-way ANOVA, without
consideration of the distinct blocks of data.
The result is an F-value not much larger than 1, consistent with the null hypothesis.
F3,24 =
MSSupplement
MSResiduals
=
6.33
4.39
= 1.44
From the F-value the p-value follows.
Supplement Effect:
p-value = 0.2554 < α = 0.05, so do not reject H0
No difference in effectiveness is detected among the supplements.
Why the distinction between the outcome of the one-way ANOVA and the randomized
block ANOVA? In Listing 8.15 the sum of the squared residuals is 105.43. This is
the level of error variation against what the variation of Supplement across the four
different group means is evaluated. Compare this value with some sums of squares
from the corresponding randomized blocks ANOVA in Listing 8.12.
SSResiduals, 1-way = SSPerson, RndBlck + SSResiduals, RndBlck
105.43 = 88.43 + 17.00
In the one-way ANOVA all the variation within each cell about the corresponding
cell mean is defined as error variance, unaccounted for variation. The randomized
blocks analysis partitions out some of that variance from the error variance, assigning
the source of the variation to the blocking variable, here Person. When there are
noticeable individual differences among the participants, the original unaccounted for
variation dramatically decreases. In this example, the differences in muscle strength
among the participants is a source of variation removed from the error term by the
randomized blocks analysis.
The result is that the smaller residual variability leads to a more robust, powerful test,
more likely to detect a difference that exists, such as in this analysis. A general theme
in data analysis is that if there is a known source of variation, specify that source in170 CHAPTER 8. COMPARE MULTIPLE SAMPLES
the analysis. Include Persons as a blocking variable to obtain a more powerful test
regarding the treatment variable of interest, type of Supplement.
8.5 Analysis Problems
1. Consider the Employee data set, available from an internal lessR read.
?dataEmployee for
more information. d <- Read("Employee")
Some of the variables in this data set are Salary, Gender and Dept for employees at
a specific company. Dept is a categorical variable with five levels: ACCT, ADMN,
FINC, MKTG and SALE.
a. Are there differences among the average Salary across the five departments?
Answer in terms of statistical significance, effect size and confidence intervals
among pairwise differences.
b. Is this an experiment? Why or why not?
c. The previous chapter analyzed the difference in average Salary for men and
women with an independent-groups t-test. Do so with a one-way ANOVA.
Compare the p-values from the t-value from the t-test and the F-value from
the ANOVA.
OIL1 OIL2 OIL3
27.2 27.8 30.4
23.8 28.4 28.0
27.2 34.9 33.5
20.5 26.3 23.1
2. The vehicle maintenance manager who oversees a large com￾pany fleet of cars wants to save his company’s money by max￾imizing gas mileage for the fleet. His concern is the causal
influence of three different types of Engine Oil on Miles per
Gallon (MPG) of gasoline. The manager collected four different
measures of MPG for each oil type, all by the same driver over a period of time.
a. Enter the data into a worksheet such as Excel, or use a text editor and create
as a csv file. Read the data into R. Display the resulting data frame.
b. Use reshape_long() to convert the data to long form, one data value per row,
for a total of 12 rows. Display the resulting data frame.
c. Do the three different engine oils (OIL1, OIL2, and OIL3) lead to different
gasoline mileage?
d. How could this study be expanded to be more general in its conclusions?
3. Participants in a weight loss program had their weight monitored at the beginning
of the program, and at the end of the first, second and third months. The data are
on the web.
d <- Read("http://lessRstats.com/data/WeightLoss4.csv")
a. Is the weight loss program effective? Answer with both a parametric and
nonparametric procedure, and also effect size.
b. Describe the pattern of weight loss over the three months of the study.Chapter 9
Factorial Designs
9.1 Quick Start
Install R and lessR as explained in Section 1.2. At the beginning of each R session,
access the lessR functions with library("lessR"). From Chapter 2, read your
data into an R data frame such as d with d <- Read("") to browse for the data file.
Or, locate your data file with a path name or web address between the quotes.
experiment,
Section 7.2.6, p. 137
This chapter compares the means of a response variable across two or more groups
for two different grouping variables, both variables of interest, each with potentially
several levels. If the data is from an experiment, refer to each grouping variable as a
treatment variable, factor, or independent variable. Refer to the response variable as
the dependent or outcome variable. The groups may be independently sampled, or
have matched data values across the groups.
two-way ANOVA,
Section 9.2, p. 172
The two-way factorial design specifies that each between-groups sample is a combina￾tion of the levels of the two treatment variables. The corresponding ANOVA provides
an analysis of the effect of each treatment variable separately, plus their combined
interaction. ANOVA() also analyzes data from this factorial design. Y continues to
represent the response variable. X1 and X2 are the treatment variables.
ANOVA(Y ∼ X1 * X2)
randomized block
factorial design,
Section 9.3.1, p. 180
The randomized block factorial design generalizes the randomized block design to
two or more within-group treatment variables. Each effect is analyzed separately by
the corresponding ANOVA, as well the effect of the interaction of the two treatment
variables on the response variable. Analyze data from this design directly from the
standard R ANOVA function, aov(), plus related functions.
The split-plot factorial design combines the concepts of independent-groups or between￾groups, with dependent-groups or within-groups, randomized block design.
split-plot factorial
design, Section 9.3.2,
p. 184 One
treatment variable is between-groups and the other is within-groups. Again, analyze
with standard R functions such as aov().
general linear model,
Section 13.2.3, p. 280
All of these specifications of ANOVA analyses are examples of the general linear model.
Specify one variable, the response variable, in terms of one or more predictor variables,
the variables on the right side of the tilde, ∼ .
171172 CHAPTER 9. FACTORIAL DESIGNS
9.2 Two-Way Factorial Design
independent-groups
t-test, Section 7.2.1,
p. 128
The independent-groups t-test compares the group means for two different groups
according to two different values of the grouping variable, the treatment or independent
one-way ANOVA, variable in the context of an experiment.
Section 8.3, p. 153
The one-way ANOVA generalizes this analysis
to a design with a single grouping or treatment variable but more generally applies
to the analysis of more than two groups. The design introduced here generalizes the
factorial design: A analysis to two different treatment variables.
design with two or
more treatment
variables.
This more comprehensive design, a
factorial design, allows the simultaneous study of the effects of two different variables
on the response variable.
Two-way factorial
design classification:
B2W0
With two treatment variables, factors,1 or independent variables, ANOVA analyzes
each variable’s impact on the response variable. With two between-groups variables
and no within-groups variable, refer to the two-way factorial design considered in
this section as B2W0. main effect: Effect
of a treatment
variable on the
response variable.
The effect of each treatment variable or factor in the context
of a factorial design is a main effect. A main effect occurs when the value of the
response variable varies according to the levels of a treatment variable. For example,
suppose Dosage has a main effect on task completion Time. Suppose further that
more arousal leads to improved completion Time, so that average completion Time
decreases as Dosage increases.
In addition to the main effects, the analysis of two independent or treatment variables
in the design introduces a third effect to evaluate, the interaction.
interaction effect:
The effect of one
factor is different at
different levels of
another factor.
Consider two
treatment variables or factors, peanut butter and jelly, each of which has two levels,
present and absent. There are four possible treatment combinations: plain bread,
peanut butter sandwich, jelly sandwich, and jelly and peanut butter sandwich. The
response variable is the subjective taste of the sandwich. Plain bread has a taste value
of one and is therefore undesirable. Peanut butter improves the flavor of ordinary
bread by a particular amount, say a main effect of 10 units. Jelly enhances the flavor
of plain bread by another factor, say a main effect of 8 units.
However, peanut butter and jelly combined deliver a much better taste than the
isolated effects of peanut butter and jelly, 10 + 8 = 18 units. Instead of 18 units of
tastiness, peanut butter and jelly tastes like, say, 29 units. Peanut butter and jelly
interact. The interaction is the extra contribution of the two factors simultaneously
beyond their individual, additive contributions. Given an interaction, interpretation of
either relevant factor’s main effect is problematic because each factor’s effect depends
on the other factor’s level. In the peanut butter sandwich example, whatever effect
peanut butter has on taste differs if jelly is present or not.
9.2.1 Example
To what extent does the level of arousal and task difficulty impact the ability to
complete a task?
1The term factor does not in this context refer to the R factor data type but exists a conceptual
term to describe the grouping or independent variables in the design.9.2. TWO-WAY FACTORIAL DESIGN 173
Scenario. Examine the effects of Dosage on task completion Time
To study this question, 48 laboratory rats were randomly and equally divided into
six groups according to the two levels of task difficulty, and three dosages of an
arousal inducing drug: 0, 5, or 10 milligrams. The response (dependent) variable is
the Time in seconds to complete either the easy or the hard maze.
cell: A specific
combination of the
values of the
grouping variables in
a study.
Because there are two levels of task difficulty and three levels of dosage, the 48
laboratory rats were randomly assigned to the 6 combinations of Dosage and Difficulty,
resulting in eight rats for each cell.
9.2.2 Data
Read() function,
Section 2.3.1, p. 25
Find the data in the lessR data file Anova_2way. The first two and last two rows of
this data table appear in Listing 9.1, for the variables Difficulty, Dosage and Time.
d <- Read("Anova_2way")
> d
Difficulty Dosage Time
1 Easy 00mg 25.6
2 Easy 00mg 25.8
...
47 Hard 10mg 39.3
48 Hard 10mg 43.0
Listing 9.1: Beginning and last rows of data for the two-way ANOVA.
one-way ANOVA,
The data values for the Easy task Difficulty are the same as for the previously Section 8.3, p. 153
illustrated one-way ANOVA. The data for this two-way ANOVA adds a Hard task Difficulty
level, doubling the number of data values from 24 to 48.
By coincidence, the levels of Difficulty, Easy and Hard, are in the correct order for the
default R alphabetical ordering, E before H. The levels of Dosage are in their correct
order because the labels begin with the numerical amount of the dosage. order the categories
of a variable,
Section 3.2.1, p. 44
However, in
other situations explicitly specify the order of the values of a categorical grouping
variable with the R factor function.
Each rat in the study has only one data value, Time to complete the maze. long-form data table,
Section 8.4.2, p. 162
The
long-form and wide-form of this data table are the same. All the data for each
participant appears in one row, but there is only one data value per participant, so
the analysis is ready to proceed.
9.2.3 Input
Based on a set of R functions, the lessR function ANOVA(), abbreviated av(), pro￾vides the simultaneous analysis of both treatment variables. Randomized blocks
design, Section 8.4,
p. 161
The two-way factorial
design and the previously introduced randomized blocks design have two independent
variables or factors. Both of the independent variables in the two-way factorial design
are treatment variables, whereas the randomized blocks design evaluates a blocking174 CHAPTER 9. FACTORIAL DESIGNS
factor and one treatment variable. The blocking factor is generally not of substantive
interest but instead is included to help minimize the error variability in the analysis
of the one treatment variable.
The call to ANOVA() for a factorial design replaces the + in the model specification
of a randomized block design with an asterisk, *. Following standard R syntax, the
ANOVA() asterisk instructs R to include an interaction term in the analysis. function,
lessR: Includes
analysis for the
two-way factorial
ANOVA.
Input. Analysis of a two-way factorial design
ANOVA(Time ∼ Dosage * Difficulty)
The first part of the output from ANOVA(), not listed here, describes the variables and
data in the analysis. Also assessed is if the design is balanced. A balanced design
has all treatment level combinations or cells with the same number of participants in
each cell.
unbalanced design,
Section 9.3.3, p. 188 If the design is not balanced, adjustments to the analysis are required.
9.2.4 Description
Summary Statistics
cell mean: The
mean of the response
variable for one cell
in the design.
The analysis of variance compares the means of the cells in the design. The means
for the individual cells in the design are the cell means, shown in Listing 9.2.
-- Cell Means
Dosage
Difficulty 00mg 05mg 10mg
Easy 24.26 23.41 17.82
Hard 34.73 31.71 39.35
Listing 9.2: Cell means, one for each of the six treatment combinations in the experimental
design.
The ANOVA() function also visualizes cell means, shown in Figure 9.1.
grayscale
visualizations,
Section 4.1, p. 64
Figure 9.1: Grayscale plot of the three cell means of task completion Time, each for Dosage
and Difficulty.9.2. TWO-WAY FACTORIAL DESIGN 175
The means for each treatment level averaged across the treatment levels of the
other treatment variable are the marginal means. ANOVA() lists the marginal means
separately, as in Listing 9.3.
marginal mean:
Mean of the response
variable for a level of
one treatment
variable calculated.
The mean of all the data is the grand mean, 28.548,
which is also reported.
-- Marginal Means
Dosage
00mg 05mg 10mg
29.49 27.56 28.59
Difficulty
Easy Hard
21.83 35.26
Listing 9.3: The marginal means and grand mean of completion Time.
The marginal means in Listing 9.3 indicate, for these data, a substantial difference
in task completion Time depending on the Difficulty level. The average completion
Time for the Hard Difficulty level is 35.26 sec, which drops to 21.83 sec for the Easy
Difficulty level. In contrast, the Dosage means exhibit little difference, varying only
from a high of 29.49 sec to a low of 27.56 sec. However, the patterning of the cell
means in Listing 9.2 and Figure 9.1 clarifies this pattern. Rats who ran the Easy
maze improved their completion Times as Dosage of the arousal drug increased. This
pattern differs much for rats who ran the Hard maze. The highest Dosage, and so the
highest arousal, inhibited completion Time, yielding the largest cell mean, 39.35 secs.
This pattern indicates an interaction between assigned Difficulty and Dosage of
training. The effect of one treatment variable on the response variable depends on
the level of the other treatment variable. An interaction implies no general effect
of Dosage in isolation of task Difficulty. Without an interaction the two curves in
Figure 9.1 would be parallel. Instead, the lines that connect the 10 mg cell mean
to the 5 mg cell mean for each level of Difficulty move in the opposite direction. If
corroborated by the inferential analysis of the corresponding population values, the
effect of the Dosage of the arousal drug on task completion Time depends on the
Difficulty of the task.
Also provided in Listing 9.4 are the standard deviations of the response variable for
all six cells.
-- Cell Standard Deviations
Dosage
Difficulty 00mg 05mg 10mg
Easy 2.69 2.85 3.96
Hard 4.93 4.81 4.39
Listing 9.4: Sample standard deviations for each cell in the design.176 CHAPTER 9. FACTORIAL DESIGNS
The analysis of variance, like the classic t-test, assumes equality of the cell population
variances. Even so, the sample variances and standard deviations will not be equal.
The analysis is reasonably robust against violations of this assumption, but the
standard deviations should be relatively consistent, a condition matched by these
standard deviations.
Effect Size
A question apart from the hypothesis test is the estimation of the effect size, how
strong is the effect in the sample? If the researcher hypothesized an effect, the best
result is to obtain statistical significance rejecting the null hypothesis and also show
that the effect is strong. Neither condition, however, implies the other, so always
effect size, investigate both. Listing 9.5 shows the effect size output from ANOVA().
Section 8.3.4, p. 156
-- Association and Effect Size
Partial Omega Squared for Dosage: -0.00
Partial Omega Squared for Difficulty: 0.73
Partial Omega Squared for Dosage_&_Difficulty: 0.32
Cohen’s f for Difficulty: 1.66
Cohen’s f for Dosage_&_Difficulty: 0.69
Listing 9.5: Effect sizes that describe the sample data.
The estimated association of Dosage with completion Time is zero to within two
decimal digits. Although the population value of omega squared, ω
2
, varies between 0
and 1, the sample estimate from a small population can result in negative value close
to 0, as is with the omega squared estimate for Dosage. The estimated association
of task Difficulty with completion Time is large, ω
2 = 0.73. The corresponding
association for the interaction effect is moderate, ω
2 = 0.32.
There is no effect size statistic, Cohen’s f, reported for Dosage because the correspond￾ing omega squared value is less than 0. The value of f for Difficulty is substantial,
1.66, well beyond a generally large effect size of f = 0.40 suggested by Cohen (1988).
The corresponding value for the interaction of f = 0.69 is also large, though much
smaller than the Difficulty effect.
Interpretation. Effect size for Dosage and Time
In this sample, the relationship between Dosage and completion Time directly is
non-existent. However, at least in the sample data, task Difficulty and Dosage
interact, and Difficulty by itself exhibits a strong effect on completion Time.
Next, explore the potential generalization of these effects to the entire population.
9.2.5 Inference
Evaluate the Overall Effect
There are several questions of interest in this study, each formally stated as a null
hypothesis. One hypothesis is the population equality of the mean completion time9.2. TWO-WAY FACTORIAL DESIGN 177
for each Dosage.
main effect for Dosage H0 : µ00mg = µ05mg = µ10mg
Another null hypothesis is the population equality of the mean response Time for
each task Difficulty.
main effect for task Difficulty H0 : µEasy = µHard
The null hypothesis for the interaction of Dosage and Difficulty states that there is
no interaction, the effects of one treatment variable on completion time remain the
same at each level of the other treatment variable.
interaction effect H0: Effect of each treatment variable is the
same at each level of the other treatment variable
However, Dosage and Difficulty interact if the effect of Difficulty differs depending on
the Dosage. Neither the main effect for Difficulty nor Dosage applies in the presence
of an interaction. With an interaction, there is no consistent main effect but rather a
different effect for a level of one variable at different levels of the other variable.
Do the patterns observed in the sample data generalize to the population as a whole?
Listing 9.6 shows the inferential tests based on the traditional analysis of variance
summary table. The two main effects are Dosage and Difficulty. The interaction
effect of the two factors is indicated by Dosage:Difficulty.
-- Summary Table
df Sum Sq Mean Sq F-value p-value
Dosage 2 29.88 14.94 0.92 0.4076
Difficulty 1 2164.11 2164.11 132.85 0.0000
Dosage:Difficulty 2 402.61 201.30 12.36 0.0001
Residuals 42 684.17 16.29
Listing 9.6: ANOVA summary table of a two-way factorial design.
The three tests are each based on the p-value of the corresponding test statistic, the
F-value. F-value,
Section 8.3.5, p. 158
Each F-value is the ratio of the mean square for the effect, from the Mean
Sq column, to the mean square for the residuals, MSResiduals = 16.29.
The interaction of Difficulty and Dosage on completion Time is significant.
Dosage Effect x Difficulty:
p-value = 0.0001 < α = 0.05, so reject H0
The conclusion is that Difficulty and Dosage jointly affect task completion Time.
With the significant interaction, the interpretation of the main effects becomes less
clear. Neither Difficulty nor Dosage has a general effect on the task completion Time.
Instead, any effect of one of the treatment variables (factors) that would exist depends
on the level of the other treatment variable.178 CHAPTER 9. FACTORIAL DESIGNS
The interaction entirely obscures the Dosage main effect, which is not significant.
Dosage Effect: p-value = 0.4076 > α = 0.05, so do not reject H0
The interaction shows that contrary to the outcome of the test of the marginal Dosage
means, Dosage does have an effect, just that the effect differs depending on the
level of Difficulty. In the one-way ANOVA, which examined the effect of Dosage on
completion Time, but only considered the Easy Task, the higher the Dosage the
quicker the average completion Time. The two-way ANOVA adds the Difficult Task,
in which case the largest Dosage of the arousal drug diminishes performance, resulting
in the slowest completion Time. The high Dosage cell mean for the Difficult task is
39.35 secs, the largest of the six cell means.
The main effect for task Difficulty is significant, even in the presence of the interaction
of Difficulty with Dosage.
Difficulty Effect: p-value = 0.0000 < α = 0.05, so do reject H0
Reject the null hypothesis that mean completion time is the same at all levels
of Difficulty. However, the main effect cannot be properly interpreted given that
Difficulty interacts with the Dosage as it impacts completion time.
Interpretation. Main effects and interaction
The Hard Difficulty task requires much more time to complete, on average, than the
Low Difficulty task. Even so, this effect is modulated by the significant interaction
effect. High Dosage of the arousal inducing drug has the opposite effect depending
on the task Difficulty. The Easy task facilitates performance when highly aroused,
and the Difficult task diminishes performance.
The significance of the interaction effect is the primary finding of this analysis. The
interpretation of the main effect of each treatment variable is either obscured, as
with Dosage, or at least modulated, as with Difficulty. Yes, the more Difficult task
takes longer to complete, but a fuller understanding of this effect requires explicitly
recognizing the resulting interaction.
Post-Hoc Multiple Comparisons
The large separation between the cell means of the Easy and Difficulty conditions,
and the strong interaction of Dosage and Difficulty, imply that most of the pairwise
comparisons of the cell means are significant. The lack of significance of a main
effect for Dosage, however, would imply that marginal means for Dosage are not
Multiple comparisons distinguishable from each other.
of main effects,
Listing 8.5, p. 158;
Listing 8.13, p. 166
The Tukey post-hoc comparisons verify these results
at the family-wise significance level of α = 0.05.
The Tukey multiple comparisons are presented for the two main effects. All three
pairwise comparisons for the three marginal means of Dosage are not significant,
and the Hard-Easy marginal mean comparison is significant. Express the pairwise
comparisons for the interaction effect, the latter of which are reported in Listing 9.7,
in terms of the cell means.9.3. MORE ADVANCED DESIGNS 179
Cell Means
----------
diff lwr upr p adj
05mg:Easy-00mg:Easy -0.85 -6.87 5.17 1.00
10mg:Easy-00mg:Easy -6.44 -12.46 -0.41 0.03
00mg:Hard-00mg:Easy 10.46 4.44 16.49 0.00
05mg:Hard-00mg:Easy 7.45 1.43 13.47 0.01
10mg:Hard-00mg:Easy 15.09 9.06 21.11 0.00
10mg:Easy-05mg:Easy -5.59 -11.61 0.44 0.08
00mg:Hard-05mg:Easy 11.31 5.29 17.34 0.00
05mg:Hard-05mg:Easy 8.30 2.28 14.32 0.00
10mg:Hard-05mg:Easy 15.94 9.91 21.96 0.00
00mg:Hard-10mg:Easy 16.90 10.88 22.92 0.00
05mg:Hard-10mg:Easy 13.89 7.86 19.91 0.00
10mg:Hard-10mg:Easy 21.52 15.50 27.55 0.00
05mg:Hard-00mg:Hard -3.01 -9.04 3.01 0.67
10mg:Hard-00mg:Hard 4.62 -1.40 10.65 0.22
10mg:Hard-05mg:Hard 7.64 1.61 13.66 0.01
Listing 9.7: Multiple comparison of cell means.
Only three cell mean comparisons are not significant. The two lowest Dosage levels
at both the Easy and Hard Difficulty levels cannot be differentiated. Also, there is
no distinction between the lowest and medium Dosage levels at the Hard Difficulty
level. Understanding these results emphasizes the importance of understanding and
interpreting an interaction effect in a two-way ANOVA design.
Interpretation. Post-hoc analysis
Collapsed across Difficulty levels, there is no Dosage effect, yet Dosage has a
substantial relationship to task completion Time. This effect, however, is only
evident from the corresponding interaction.
9.3 More Advanced Designs
The lessR ANOVA() function analyzes the three designs previously illustrated in this
and the previous chapter: One-way ANOVA, randomized blocks ANOVA, and two-way
ANOVA. These designs and subsequent data analyses are among the most common in
the analysis of experimental data, but many other designs are encountered, some of
them considerably more complex. Kirk (2013) categorizes, explains, and illustrates the
basic and more complex designs, which requires more than 1000 pages of explanation,
clearly beyond the scope of this book.
unbalanced designs,
Section 9.3.3, p. 188
Data analysis from designs more sophisticated than the three shown in this chapter
necessitates the use of the basic R functions on which lessR is based, as well as
functions from additional packages. One excellent reference in the application of R
to more advanced designs is Faraway (2004). When using R functions or functions
from other packages directly, realize that unlike the lessR functions, the standard R
functions have no default assumption regarding the data frame to be analyzed, which180 CHAPTER 9. FACTORIAL DESIGNS
for lessR is d. Most of these functions access the data frame with the same data
parameter as with the lessR functions.
Second, a complete analysis usually involves several to many standard R functions,
and perhaps subsequent programming as well. The primary analysis of variance R
function for balanced designs, aov(), only specifies the initial analysis. Individual
components of the analysis, such as the ANOVA summary table, must be explicitly
referenced, here with the R function summary(). By contrast the lessR functions
weave these needed functions together and provide the needed ancillary programming.
The lessR result is a more complete output from a single function call.
randomized block 9.3.1 Randomized Block Factorial Design
factorial design:
Two or more within
(dependent) groups
design, classified as
B0W2 if two
within-group factors.
The first more advanced design considered here is what Kirk (2013, p. 459), calls
a randomized block factorial design, also known as a within-groups, within-samples
or within-subjects factorial design. Because there are no between-group variables
and two within-group variables, refer to this design as B0W2.
randomized block
design Section 8.4,
p. 161
The randomized block
design already presented yields a single data value for each block at each treatment
level of the one treatment variable. The randomized block factorial design applies
the same concept to more than a single treatment variable. In the factorial version,
each block yields a single data value for each combination of levels of the treatment
advantage of blocks variables.
Section 8.4.8, p. 168
Grouping experimental participants by relevant blocks can lead to a more
powerful hypothesis test for discerning the effects of interest.
As with the randomized block design, the data values for the block can consist of
repeated measures of the same experimental unit such as a person, laboratory rat,
organization, thing, place, or event. Or, the block can consist of different units
matched on some relevant characteristic. This example applies the latter approach
to a reconsideration of the previously presented two-way between-subjects factorial
design and the corresponding analysis of variance.
two-way ANOVA,
Section 9.2, p. 172 The previous example of the two-way ANOVA examined the effect on task completion
Time in response to one of three levels of Dosage of an arousal-producing drug and
one of two levels of task Difficulty. The task is running rats through a maze to
obtain a food reward. Refer to the design as between-samples, between-groups, or
between-subjects. A different rat provides each measurement of completion Time in
one cell, one of 3x2 = 6 unique combinations of Dosage and Difficulty. The study has
8 replications per cell, 8 different rats exposed to the same treatment conditions.
Now consider the within-subjects version of the same data. Suppose the researcher
had identified a potential source of systematic variation: individual differences among
rats in their ability to run a maze. A trial maze served as a sort of a pre-test from
which to sort the rats into blocks according to their completion times. The rats in the
first block of 6 rats ran the trial maze the fastest, and the last block the slowest. The
rats within each block were randomly assigned to each of the 6 treatment combinations
of Dosage and Difficulty. Each rat in a block only experiences one of the 6 cells.
There are still 48 different rats in the study, but each block of six matched rats now
provides a score on each of the six treatment combinations, shown in Table 9.1.9.3. MORE ADVANCED DESIGNS 181
Dosage mg00 mg05 mg10 mg00 mg05 mg10
Difficulty Easy Easy Easy Hard Hard Hard
13.1 34.2 25.4 39.3
11.9 30.1 27.4 43.0
24.2 40.5 40.3 46.7
Blck1 1
lck2 2
...
8.4 19.0
B 3.0 20.8
Blck8 25.6 23.4
Table 9.1: Subset of maze running data organized by block.
The advantage of this randomized blocks factorial design over the two-way between￾groups factorial ANOVA is the same as for the randomized blocks design with one
treatment variable.
blocking advantage,
Inclusion of comparable, homogeneous participant blocks that do Section 8.4.8, p. 168
differ in performance splits that corresponding sum of squares from the sum of squares
error terms. A previous source of random error variation has been identified and
removed as a systematic source of variation. The error sum of squares is then reduced,
resulting in a more powerful test, more capable of detecting existing inequalities in
population means of the treatment variables.
The organization of the data in Table 9.1 is a within-subjects design because similar
performing rats provide the data for each block of data values. long-form data table,
Section 8.4.2, p. 162
To analyze, read
the lessR data file Anova_rbf. The data are ready for analysis, organized in the
long-form with one data value per line. The data are the same as from the 2-way
ANOVA in Listing 9.1 except for the addition of the Block variable that groups rats on
maze running ability. The first and last rows of this data table appear in Listing 9.8.
Read() function,
Section 2.3.1, p. 25 d <- Read("Anova_rbf")
> d
Difficulty Dosage Block Time
1 Easy mg00 Blck1 18.4
2 Easy mg05 Blck1 19.0
3 Easy mg10 Blck1 13.1
4 Hard mg00 Blck1 34.2
5 Hard mg05 Blck1 25.4
6 Hard mg10 Blck1 39.3
7 Easy mg00 Blck2 23.0
...
47 Hard mg05 Blck8 40.3
48 Hard mg10 Blck8 46.7
Listing 9.8: Subset of data for randomized block factorial design.
Analysis with separate error terms. The analysis proceeds from the standard R func￾tion aov(). The specification of the randomized block factorial design with aov()
partly resembles the specification of the two-way factorial design with ANOVA(). Both
designs incorporate a complete factorial structure, in this example all six treatment
combinations of three levels of Dosage and two Difficulty levels, with task completion
Time as the response variable. Include the following component to specify that
factorial design.
Time ∼ (Dosage*Difficulty)182 CHAPTER 9. FACTORIAL DESIGNS
Distinguish the between-groups from the within-groups designs with the blocking
structure of the randomized block factorial design. The distinction for the randomized
block factorial design is adding an Error term.
random effect: A
treatment variable
with levels that are
randomly sampled
instead of fixed by
the researcher in
advance of data
collection.
The Error term indicates random
effects, sources of random variation used to test the effects of interest, here Drug and
Dosage. Indicate this blocking structure with the customized Error term.
Error(Block/(Dosage*Difficulty))
This notation customizes the error term for each effect to be tested. The first term is
Block, which represents the differences among the blocks. Block represents a random
effect because the levels of Block were not set by the researcher before the study began.
Instead, the rats in the study were randomly sampled from the larger population.
The remaining three terms are Dosage, Difficulty, and their interaction. These three
effects are within the blocks2
, as indicated by the notation after the forward slash, /.
The standard R approach to linear models estimates the model with functions such
as aov() and then stores the results into an object, here named fit.
Input. Two treatment randomized block factorial design
fit <- aov(Time ∼ (Dosage*Difficulty) +
Error(Block/(Dosage*Difficulty)), data=d)
The summary() function applied to the output of aov() provides the core analysis,
the ANOVA summary table in Listing 9.9.
summary(fit)
Error: Block
Df Sum Sq Mean Sq F value Pr(>F)
Residuals 7 316.6 45.23
Error: Block:Dosage
Df Sum Sq Mean Sq F value Pr(>F)
Dosage 2 29.88 14.938 3.013 0.0816
Residuals 14 69.40 4.957
Error: Block:Difficulty
Df Sum Sq Mean Sq F value Pr(>F)
Difficulty 1 2164.1 2164.1 116.8 0.0000128
Residuals 7 129.7 18.5
Error: Block:Dosage:Difficulty
Df Sum Sq Mean Sq F value Pr(>F)
Dosage:Difficulty 2 402.6 201.30 16.73 0.000195
Residuals 14 168.5 12.04
Listing 9.9: Randomized blocks factorial design summary table from R.
Each error source of variation in the ANOVA summary table is labeled Residuals.
Compute the F-value for each effect as the ratio of its mean square compared to its
2To be more precise, the effects that follow the / are nested within the effect specified before it.9.3. MORE ADVANCED DESIGNS 183
own residuals mean square. F-value,
Section 8.3.5, p. 158 The primary result from the two-way ANOVA remains.
The interaction of Dosage of the arousal drug and task Difficulty is significant.
Dosage x Difficulty Effect:
p-value = 0.0002 < α = 0.05, reject H0
cell means plot
example, Section 9.3,
p. 175
The meaning of this interaction was previously explored in the cell means plot for
these data. Similarly, task Difficulty is significant, and Dosage is not significant.
Difficulty Effect: p-value = 0.0000 < α = 0.05, reject H0
Dosage Effect: p-value = 0.0816 > α = 0.05, do not reject H0
For the two-way between-groups design for the analysis of these data, the Residuals
Mean Sq from Listing 9.6 is 16.29. With blocking, the Residuals Mean Sq for Dosage
decreases to 4.96. The result is that the p-value of 0.0816 almost attains significance.
Compare that p-value to that obtained with the between-groups analysis of 0.4076.
Analysis with combined error terms. The previous analysis of the randomized block
factorial design was specified to evaluate each of the three effects, two main effects
and the interaction, with its own error term, the interaction of Block with the
corresponding effect. Some authors, such as Kirk (2013, p. 462), combine the residual
terms for each of the three effects. This composite error term serves as a single
baseline from which to evaluate each effect. The idea behind this combined term is
that because each error term solely reflects random error, their combination offers a
single measure of the level of random error in the study with a larger number of data
values from which to estimate the error.
To obtain an analysis with this combined error term applicable to the assessment of
all three effects in this study, specify a new, simpler error term in the call to aov(),
shown in Listing 9.10. The revised Error specification in the call to aov() specifies
only a Block error term.
Input. Randomized block factorial design with combined error term
fit <- aov(Time ∼ (Dosage*Difficulty) + Error(Block), data=d)
summary(fit)
Error: Block
Df Sum Sq Mean Sq F value Pr(>F)
Residuals 7 316.6 45.23
Error: Within
Df Sum Sq Mean Sq F value Pr(>F)
Dosage 2 29.9 14.9 1.422 0.2550
Difficulty 1 2164.1 2164.1 206.056 0.0000
Dosage:Difficulty 2 402.6 201.3 19.167 0.0000
Residuals 35 367.6 10.5
Listing 9.10: Analysis of the randomized block factorial design with a single within-subjects
error term direct from the R function aov().184 CHAPTER 9. FACTORIAL DESIGNS
The df and SumSq for this Residuals effect are the sum of the corresponding three
Residuals terms from Listing 9.9.
Residuals Sum Sq Combined: 367.6 = 69.4 + 129.7 + 168.5
This simpler error term combines the remaining sources of error variation into a
generic Within error term.
9.3.2 Split-Plot Factorial Design
split-plot factorial
design: Includes
between-groups
treatment variables
and within-groups
treatment variables,
classified as B1W1 if
one of each.
A second more advanced design is the split-plot factorial design. The most straightfor￾ward design is illustrated here, with two treatment variables. One treatment variable
is between-groups, with different sets of unmatched participants. The other treatment
variable is within-groups, the defining feature of a randomized blocks design. Each
participant in the study forms a block of matched scores on the response variable
across this within-groups treatment variable. Accordingly, classify this split-plot
factorial design as a B1W1 design.
The two-way split-plot factorial combines the one-way between-groups design with the
randomized blocks design.
one-way
between-groups
design, Section 8.3,
p. 153
The randomized blocks design has a data value for each
block of responses for each level of the treatment variable. The split-plot factorial
replicates this structure for two or more treatment conditions for the between-groups
treatment variable. Each level of the between-groups variable has its own set of
randomized blocks.
randomized blocks
design, Section 8.4,
p. 161
To illustrate, consider the previously provided randomized-blocks design. Each
of the seven participants took one of four pre-workout supplements before bench
pressing 125 pounds as many times as possible, providing 28 different measurements.
That design is randomized-blocks because each individual received all four levels
of supplement, a within-groups treatment variable. To generalize this design to
the split-plot factorial, assume that all participants in the previous trial had a
regulated, very nutritious breakfast exactly two hours before taking the pre-workout
Supplement, precisely one-half hour before starting the workout. Then, another group
of participants with the same randomized group structure across the four supplement
levels had a low nutrition breakfast.
wide- vs. long-form
data, Section 8.4.2,
p. 162
Figure 9.2 shows the structure of the data for this split-plot design in the wide-form
format for better understanding of the design. A total of 7x2 = 14 participants
were recruited for the study and then randomly assigned to one of two Food groups:
randomized blocks Hi or Low nutrition.
data, Section 8.4.2,
p. 162
The first group of 7 participants has been described in the
data file Anova_rb. Each participant in the second group, with the less nutritious
breakfast Food, also took all four Supplements in a randomized order, one for each
workout, again for a total of 28 data values. The type of Supplement continues to be a
within-groups treatment variable, whereas Food quality emerges as a between-groups
treatment variable in this design. Participants in the Low Food Quality group are
not the same as those in the Hi Food Quality group.9.3. MORE ADVANCED DESIGNS 185
Person sup1 sup2 sup3 sup4
p1 2 4 4 3
p2 2 5 4 6
...
p7 2 3 2 4
Person sup1 sup2 sup3 sup4
p8 2 2 3 3
p9 1 4 3 4
...
p14 3 2 2 4
Food
Supplement
Hi
Low
within-groups
blocks
between￾groups
Figure 9.2: A split-plot design with one between-groups factor (2 levels) and one within￾groups factor (4 levels) with 7 different participants for each of the two levels.
Find the data for the split-plot analysis in the file Anova_sp stored with lessR. The
data are organized in a long-form data table, ready for analysis, with one data value
per line, resulting in 7 x 4 x 2 = 56 rows of data.
Read() function,
Section 2.3.1, p. 25 d <- Read("Anova_sp")
Listing 9.11 shows representative rows of this data table in long form. The blocking
variable is named Person, with four rows of data per Person.
d
Person Food Supplement Reps
1 p1 Hi sup1 2
2 p1 Hi sup2 4
...
28 p7 Hi sup4 4
29 p1 Low sup1 2
...
55 p7 Low sup3 2
56 p7 Low sup4 4
Listing 9.11: Data for split-plot factorial design.
Obtain descriptive statistics for the data table with the lessR function pivot().
pivot() function
applied to continuous
variables,
Section 6.2.3, p. 107
The
summary statistics for Food quality appear in Listing 9.12.
pivot(d, c(mean, sd, min, median, max), Reps, by=Food)
Food Reps_n Reps_na Reps_mean Reps_sd Reps_min Reps_md Reps_max
1 Hi 28 0 4.357 2.147 1 4 9
2 Low 28 0 3.679 2.038 1 3 8
Listing 9.12: Marginal means and other statistics for the two levels of Food quality.186 CHAPTER 9. FACTORIAL DESIGNS
The mean repetitions of the bench press for the Hi quality Food participants is 4.36,
which is 0.68 repetitions more, on average, than for the Low quality Food participants.
Listing 9.13 presents the marginal means and other statistics for the four supplements.
pivot(d, c(mean, sd, min, median, max), Reps, by=Supplement)
Supplement Reps_n Reps_na Reps_mean Reps_sd Reps_min Reps_md Reps_max
1 sup1 14 0 3.214 2.082 1 2.0 8
2 sup2 14 0 3.357 1.550 1 3.0 6
3 sup3 14 0 4.143 1.956 2 4.0 7
4 sup4 14 0 5.357 2.240 3 4.5 9
Listing 9.13: Summary statistics for the four pre-workout Supplements.
At least for the data values found in this sample, the differences among the means
are distinct. The fourth Supplement leads to the most repetitions of the bench press
with an average of 5.36 bench presses.
Does the pattern of differences observed in the sample apply generalize to the
population? Use the R function aov() for the inferential analysis for this split-plot
factorial design. Specify the factorial design with Food*Supplement, which specifies
each main effect and their interaction. Follow the same pattern as for the randomized
blocks factorial design and specify an Error term with the blocking variable, Person,
followed by a /, followed by the within-groups variable, Supplement.
Input. Split-plot factorial design
fit <- aov(Reps ∼ (Food*Supplement) +
Error(Person/Supplement), data=d)
summary(fit)
Listing 9.14 shows the summary table obtained from aov() and summary().
F-value,
Section 8.3.5, p. 158
Error: Person
Df Sum Sq Mean Sq F value Pr(>F)
Residuals 6 156.6 26.1
Error: Person:Supplement
Df Sum Sq Mean Sq F value Pr(>F)
Supplement 3 40.48 13.494 9.203 0.000657
Residuals 18 26.39 1.466
Error: Within
Df Sum Sq Mean Sq F value Pr(>F)
Food 1 6.446 6.446 12.74 0.00155
Food:Supplement 3 0.911 0.304 0.60 0.62126
Residuals 24 12.143 0.506
Listing 9.14: Split-plot factorial design summary table direct from the R function aov().
As always with ANOVA, evaluate each effect according to its F-value, the test statistic
computed as the ratio of the mean square of each effect to the corresponding residual9.3. MORE ADVANCED DESIGNS 187
mean square. The analysis includes the block effect, Person, the between-groups effect
for Food, the within-groups effects for Supplement, and the interaction of Supplement
with Food. From these results, the Food effect on Reps, the number of repetitions of
the bench press, is significant.
Food: p-value ≈ 0.002 < α = 0.05, so reject H0
The more nutritious breakfast does facilitate better performance. From the marginal
means in Listing 9.12, the observed advantage of the more nutritious breakfast is
sufficient to generalize to the population.
The type of Supplement also is significant.
Supplement: p-value ≈ 0.001 < α = 0.05, so reject H0
As in the analysis of the randomized blocks design, there are differences among the
Supplements in terms of facilitating the bench press.
There is no detected interaction between Food and Supplement.
Food x Supplement:
p-value = 0.621 > α = 0.05, so do not reject H0
Whatever the effect of Supplement for the Hi nutrition breakfast, the same effect is
present for the Low nutrition breakfast. To view the mean and standard deviation
for each combination of Food and Supplement, specify both as by variables in the
call to pivot(), shown in Listing 9.15.
pivot(d, c(mean, sd), Reps, by=c(Supplement, Food))
Supplement Food Reps_n Reps_na Reps_mean Reps_sd
1 sup1 Hi 7 0 3.571 2.299
2 sup2 Hi 7 0 3.857 1.676
3 sup3 Hi 7 0 4.286 1.890
4 sup4 Hi 7 0 5.714 2.430
5 sup1 Low 7 0 2.857 1.952
6 sup2 Low 7 0 2.857 1.345
7 sup3 Low 7 0 4.000 2.160
8 sup4 Low 7 0 5.000 2.160
Listing 9.15: Means and other statistics for all combinations of Food quality and Supple￾ments.
The lack of interaction is demonstrated by the same patterning of the means for
Supplement in each Food group. The 4th Supplement has the highest mean for each
Food group. The second Supplement has the lowest mean.
Or, visualize the cell means with the R interaction.plot() function, depicted in
Figure 9.3, which reveals approximately parallel lines with a downward (negative)
slope.188 CHAPTER 9. FACTORIAL DESIGNS
with(d, interaction.plot(Food, Supplement, Reps))
3.0 4.0
Reps
5.0
Food
mean of
Hi Low
Supplement
sup4
sup3
sup1
sup2
Figure 9.3: Cell means of response latency for two Food levels across four different Supple￾ments.
Regardless of the type of Supplement, there is a shift downward in performance
from the Hi to the Low nutrition Food groups. Eating a better breakfast facilitates
performance in the gym, but the effects of the different Supplements remain the same
regardless of the quality of breakfast.
9.3.3 Unbalanced Designs
When each cell of the design contains the same number of observations, the analysis of
factorial designs is straightforward. Membership in one specific treatment condition
for one independent or treatment variable provides no information about membership
example, maze in the second independent variable’s treatment condition.
completion time
Section 8.3.2, p. 154
In the previous example,
knowing that a rat is in the 5 mg Dosage condition implies no knowledge about
whether that same rat is in the Easy or Hard Difficulty condition. Because of this
independence, the sum of squares for each effect, each source of variation, may be
defined without ambiguity and computed independently from all other effects in the
design.
Confounding of Effects
When the number of observations across cells in a factorial design varies, the issue
becomes more complicated. Given participation in one treatment condition, the same
person (rat, corporation, or whatever) is more likely to be in the corresponding cell
with the largest sample size in the second treatment condition. unbalanced
factorial design:
Unequal numbers of
experimental units in
different cells.
The factorial design
is unbalanced due to mismatched cell sizes. In an imbalanced design, the treatments
or independent variables are correlated.
Return to our maze running Time example. If the design is unbalanced, knowing
the Dosage treatment level allows for a more informed prediction as to the Difficulty
treatment level. Membership in a certain Dosage condition correlates with membership
in a specific Difficulty condition. At the most extreme, if everyone in a given Dosage
level was in the same Difficulty level, the levels of the variables would have a perfect9.3. MORE ADVANCED DESIGNS 189
correlation. In this extreme unbalanced situation, know the value of one of the
independent variables and the value of the other is determined.
Because the treatment variables are not independent of each other due to different cell
sizes, the corresponding sums of squares for the two treatment variables overlap. total effect,
Section 12.2.2, p. 253
The
assessment of the magnitude of an effect with overlapping effects is analogous to the
concept of a total effect defined in the multiple regression chapter. In an unbalanced
design, the SS for an effect on the response variable reflects both the effect of interest
and the impact of the associated effects. Sources of variability are muddled. The SS
must be untangled in some way to identify the impact of an effect of interest isolated
from the other effects.
Choose between implementing one of two different approaches to unbalanced designs
to isolate the source of variability from the confounded SS: Type II or Type III sums
of squares (SS). Langsrud (2003, p. 167) concluded that “there are very compelling
reasons to prefer Type II over Type III, and we advocate Type II as a preferable
default choice than Type III.” Without re-iterating the specifics here that support this
choice, lessR provides the Type II sum of squares for the two-way factorial design.
The Type II SS removes the influence of the other factors (independent or treatment
variables) before assessing the variability of the response variable attributed to the
effect of interest.
partial slope
coefficient,
Section 12.2.2, p. 252 The net effect of removing the effects of associated variables is
equivalent to the partial slope coefficient in multiple regression.
net effect,
Section 12.2.2, p. 253 If the design is unbalanced, the sum of squares for an effect depends on the order of
entry of the independent or treatment variables into the model when performing the
ANOVA, such as with R’s function aov(). The SS for the first entered variable shows
the treatment’s overall effect, including the impact of that variable on the response
variable and its correlated variables in the design. The SS for the second entered
variable, the second main effect, represents the impact of that variable after removing
the impact of the first variable. To obtain the Type II sum of squares for a treatment
variable in an unbalanced factorial design with two independent variables, run the
usual ANOVA with that variable specified second in the model specification. This
procedure requires to run the ANOVA twice so that each treatment variable appears
as the second variable in the analysis.
The resulting ANOVA summary table with the SS for each treatment defined from
listing the variable second does not include the sums of squares confounded between
the main effects. Their variability has been removed from the analysis. However, any
SS from the interaction effect(s) is confounded with the main effects in the Type II
design. If there is no interaction between the treatment variables in the population,
the Type II approach is always preferable. Type III SS can technically evaluate
main effects in the presence of an interaction. However, main effects, regardless of
statistical significance, are not interpretable in the presence of an interaction.
Example of an Unbalanced Design
equal cell data
example, Section 9.1,
p. 173
To illustrate, create a data table with unequal cell sizes. Begin with the equal cell
size version previously presented for the two-way factorial design. These data for
laboratory rats are for assessing the impact of the Dosage of a drug – 0, 5, or 10 mg190 CHAPTER 9. FACTORIAL DESIGNS
– and the Difficulty of a maze – Easy or Hard – on the needed Time to complete a
task, solving a maze to obtain a food reward. There are eight rats assigned to each
of the 3x2 = 6 groups, the treatment conditions.
First, create the unequal cell data by deleting specified rows from the original built-in
lessR data set Anova_2way. Listing 9.16 shows the resulting pivot table of cell counts.
Setting the parameter show_n to FALSE is not necessary but does eliminate some
unneeded columns for this purpose. The new cell sizes vary from 5 to 8.
d <- Read("Anova_2way")
d <- d[-c(2,3,11,22:25,47,48),]
pivot(d, table, Difficulty, by=Dosage, show_n=FALSE)
Dosage Easy Hard
1 00mg 6 7
2 05mg 7 8
3 10mg 5 6
Listing 9.16: Create and tabulate unequal cell sizes that vary from five to eight, indicating
an unbalanced design.
The ANOVA summary table from this unbalanced design is shown Listing 9.17. The
lessR function ANOVA() automatically detects the unequal cell sizes and then provides
the Type II sum of squares.
ANOVA(Time ∼ Dosage * Difficulty)
-- Summary Table from Type II Sums of Squares
df Sum Sq Mean Sq F-value p-value
Dosage 2 30.69 15.34 0.93 0.4060
Difficulty 1 1409.05 1409.05 85.08 0.0000
Dosage:Difficulty 2 200.76 100.38 6.06 0.0057
Residuals 33 546.55 16.56
Listing 9.17: ANOVA summary table of Type II sum of squares for an unbalanced two-way
factorial design.
Within the R ecosystem, the car package (Fox & Weisberg, 2019) provides a Type II
sum of squares analysis for more general ANOVA designs. Implementation requires
two steps.
least-squares analysis,
Section 11, p. 217 First, apply the standard R function lm() to do the least-squares regression
analysis of the model, which also requires identifying the input data frame with the
data parameter. Save the output into an R object, here called lm.out. Then apply
the car function Anova() to the output of lm() and specify the value of 2 for the
parameter Type.
Anova() function,
car: Analysis of
unbalanced ANOVA
designs. lm.out <- lm(Time ∼ Dosage * Difficulty, data=d)
Anova(lm.out, Type=2)
Other than formatting differences, the output of this two-way factorial ANOVA from
the car function Anova() is identical to the output from the lessR function ANOVA()9.4. ANALYSIS PROBLEMS 191
in Listing 9.17. However, for more general, unbalanced ANOVA designs, use Anova()
for the analysis. Also available is the lmer() function from the lme4 package.
9.4 Analysis Problems
1. Two drugs were evaluated for the relief of anxiety. Each drug was administered in
three different dosages. Participants in the study were randomly assigned to one of
the six treatment combinations or cells. The data are on the web.
d <- Read("http://lessRstats.com/data/Anxiety.csv")
a. Is this an experiment? Why or why not?
b. Show the marginal means for each main effect. Describe.
c. Is one drug more effective than the other? Answer with an analysis of statistical
significance and also effect size of the main effects and interaction.
2. Two different variables were studied for their impact on learning in a university
math course. One comparison is for traditional in-classroom learning vs. online
learning. The other comparison is working in teams vs ˙working alone. To evaluate,
four different sections of a course were offered in either later morning or early afternoon,
Tuesday/Thursday or Monday/Wednesday, so that all four sections were offered about
at the same time of day. At the end of the term, all students were administered the
same Final, each student taking the Final individually.
d <- Read("http://lessRstats.com/data/Final.csv")
a. Is this an experiment? Why or why not?
b. Show the marginal means for each main effect. Describe.
c. Is one teaching style more effective than the other? Answer with an analysis of
statistical significance and also effect size of the main effects and interaction.
3. A food manufacture is interested in the effect of sugar and salt on the perceived
taste of a food product.
Sugar None None Add Add
Salt None Add None Add
Taster1 2 4 7 9
Taster2 1 3 4 7
Taster3 2 5 8 8
Taster4 3 3 5 10
Taster5 2 6 6 9
To evaluate these effects and their potential inter￾action, six people were recruited as tasters. Salt
was either added to the product in a pre-determined
amount or it was not. Same for sugar.
Each taster tasted all four combinations of the prod￾uct, rating the taste on a scale from 0 to 10. The
four combinations of salt/sugar were administered in
random order to each taster. Each tasting occurred in 15 minute intervals with a
neutral food and water after each tasting to blunt the carry-over effect of one tasting
on the next.192 CHAPTER 9. FACTORIAL DESIGNS
a. Enter the data into a worksheet such as Excel, or use a text editor and create
as a csv file. Read the data into R. Display the resulting data frame.
b. Use reshape_long() to convert the data to long form, one data value per row,
for a total of 20 rows. Display the resulting data frame.
c. Compute and display the marginal means for Sugar, Salt, and Taster. Describe.
d. Do the ANOVA. Describe each effect and if statistically significant.
D Typist t1 t2
d1 Typist1 98 134
d1 Typist2 69 74
d1 Typist3 83 107
d2 Typist4 55 76
d2 Typist5 69 86
d2 Typist6 30 41
d3 Typist7 44 54
d3 Typist8 42 48
d3 Typist9 57 41
4. Does time of day impact the effectiveness of typ￾ing speed? How much does the difficulty of the
manuscript impact typing speed?
To answer these questions, some typists were given
comparable manuscripts to type, each at a different
Time: at the beginning of the work day, t1=9am and
toward the end of the work day, t2=4pm. Because
typing speed varies much for different people, the data
for each typist were organized into blocks of two data
values, one for each time of day. The manuscripts were presented in random order to
the participants.
In addition to time of day, the participants were randomly assigned to three different
levels of Difficulty, three participants per level. The three levels consisted of (d1) 7th
grade reading level, (d2) 12th grade reading level, and (d3) technical material.
a. Enter the data into a worksheet such as Excel, or use a text editor and create
as a csv file. Read the data into R. Display the resulting data frame.
b. Use reshape_long() to convert the data to long form, one data value per row,
for a total of 18 rows. Display the resulting data frame.
c. Display the marginal means, standard deviations, minimum and maximum
values for Difficulty and for Time. Describe the pattern in the data.
d. Analyze the data with a split-plot factorial ANOVA. Show the summary table
for the inferential analysis.
e. Verbally identify the effects and if each is significant or not.
f. Include the interaction plot and interpret, in conjunction with the summary
table.
g. Summarize what is learned from the analysis regarding typing speed as it relates
to manuscript difficulty and time of day.Chapter 10
Correlation
10.1 Quick Start
Install R and lessR as explained in Section 1.2. At the beginning of each R session,
access the lessR functions with library("lessR"). From Chapter 2, read your
data into an R data frame such as d with d <- Read("") to browse for the data file.
Or, locate your data file with a path name or web address between the quotes.
A central question of much research: How do variables relate to each other? Previous
chapters explored the relation between a continuous response variable and one or two
categorical grouping variables that each defined two or more samples of data. For
example, what is the relation between the response variable of annual Salary and
the grouping variable Gender which defines, for a given data set, two groups, Male
and Female? This chapter generalizes these relationships to two continuous, numeric
variables. The focus remains on the relationship between variables.
scatterplot,
Section 10.2.1, p. 194
Obtain the scatterplot and correlation of two variables with the lessR function
Plot(), such as for variables X and Y.
Plot(X,Y)
correlation matrix,
Section 10.3, p.205
The lessR function Correlation() provides the correlation matrix for the rela￾tionships of multiple variables with each other. Correlation() also visualizes the
correlations with the scatterplot matrix of the variables and a heat map. With no
specified variable list, the correlation matrix is computed for all numeric variables in
the specified data frame, with d the default.
R <- Correlation()
nonparametric
correlations,
Section 10.4, p.213
Nonparametric correlation coefficients are available with the method parameter, for
either Spearman or Kendall correlations. Specify these options for two variables or
an entire correlation matrix.
Plot(X, Y, method="spearman")
Correlation(method="kendall")
193194 CHAPTER 10. CORRELATION
10.2 Relation of Two Numeric Variables relationship of
numeric variables:
As the value of one
variable increases,
the values of the
other tend to either
increase or decrease.
Do the values of two variables tend to change together or separately? Two continuous
variables are related if, as the values of one variable increase, the values of the other
variable tend to either systematically increase or systematically decrease. Relation￾ships can be positive or negative. For a positive relationship, both variables tend to
increase together. The more Years worked at the company, the higher, on average, is
the person’s salary. Whereas for a negative relationship, the values of the variables
tend in opposite directions. The more a student is absent from class, on average, the
lower the student’s grade.
+ relation: The
values of both
variables tend to
increase together.
− relation: As the
values of one variable
increases, the values
of the other variable
tend to decrease.
10.2.1 Scatterplot
To illustrate the positive relationship of Salary and Years consider once again the
Employee data set.
Scenario. Assess the relation between annual Salary and Years of experience
What is the relationship, if any, between the number of Years employed at a
company of interest and Salary?
Employee data table,
Figure 1.5, p. 13
The employee data with the variable labels are available from within the lessR
package. Here read the data into the d data frame and the optional variable labels
into the required l data frame.
d <- Read("Employee")
l <- Read("Employee_lbl")
Plot() function,
lessR: Draw a
scatterplot for one or
two variables.
The lessR function Plot() plots each pair of data values as a single point with the
two coordinates equal to the corresponding two data values, shown in Figure 10.1.
The visual expression of the values of one or more selected variables for each row of
data is a scatterplot.
scatterplot: Graph
with one axis per
variable, one plotted
point for each
selected set of data
values per row of
data.
The two-variable scatterplot includes two variables plotted with
two axes, one for each variable. The scatterplot in Figure 10.1 shows that working
more Years tends to be associated with a higher Salary. Each plotted point represents
one employee’s data values for Years and Salary. There are 36 employees with data
values for both variables, so the scatterplot consists of 36 points.
color themes,
transparency,
Section 4.2.3, p. 65
Consider another version of the Figure 10.1 scatterplot. The version in Figure 10.2
illustrates the inverted gray scale color theme, optimized for projection onto a screen
such as for a slide show. Use the lessR function style() to choose the color theme
with the theme and sub_theme() parameters.
The fit parameter provides many possibilities for plotting a curve or a line through
the scatterplot. fit possible values,
Section 11.7.2, p. 243
The best-fitting loess curve fit through the points in Figure 10.2
summarizes the relationship between the two variables. The "loess" fit line is a non￾linear curve that follows the pattern of the data as the values of the variable x increase.
The name loess curve abbreviates locally estimated scatterplot smoothing (Cleveland,
1979). The position of each point on the curve is computed only from nearby points10.2. RELATION OF TWO NUMERIC VARIABLES 195
Plot(Years, Salary)
Figure 10.1: Default grayscale scatterplot of Years Employed with Annual Salary.
Input. Plot the best-fit nonlinear line with a black background
style("gray", sub_theme="black")
Plot(Years, Salary, fit="loess", plot_errors=TRUE)
Figure 10.2: Scatterplot of Years Employed and Annual Salary and a loess curve with a
dark background.
in the scatterplot. The result is a nonparametric procedure for summarizing the
relationship between the two variables with a nonlinear curve. For this company,
Salary does not increase much each year for the beginning levels of employment but
increases noticeably after five or so years.
The plot_errors parameter, when set to TRUE, visualizes the deviation of each
plotted point from the curve fit through the scatterplot.
plot_errors
parameter: Visualize
deviations from the
fitted line or curve
through the
scatterplot.
Experiment interactively
with different parameter settings such as fit and plot_errors for your data with
the lessR function interact(). Point-and-click to select the two variables, any196 CHAPTER 10. CORRELATION
interact(), combination of continuous and categorical variables.
Section 9.2, p. 172
Input. Interactive experimentation with colors and other parameters
interact("ScatterPlot")
When finished interactively designing your scatterplot, save the code and reapply in
the future as needed.
linear 10.2.2 Correlation Coefficient
relationship: A
straight line
summarizes the
relationship between
the variables.
The logic of the correlation coefficient as an indicator of the strength of a linear or
straight-line relationship begins with mean deviating both variables.
mean deviations,
Section 6.1, p. 101
Because the
mean of a set of deviation scores is 0, the center the scatterplot of mean deviated
variables is the origin, <0,0>. Figure 10.3 illustrates the mean-deviated scatterplot
for two variables with a strong positive relationship, Years employed and Salary.
Figure 10.3: Scatterplot of mean-deviated Years and Salary, centered at <0,0>.
Next, compute the cross-products, the product of each pair of coordinates. cross-product:
Product of the two
values that define a
point in the
scatterplot.
The mean￾deviated cross-products follow a pattern. Points in the quadrant in the upper-right of
Figure 10.3 have only + cross-products, as is true of points in the lower-left quadrant.
A scatterplot that exhibits a strong, positive linear relationship will have most of its
points in those two shaded quadrants, as is true of Years and Salary. The reverse
pattern is true of two variables negatively related, with most of its cross-products
negative in sign. Two variables that are not linearly related have about the same
number of positive and negative cross products.
The sum of the cross-products for all the data points indicates the strength of the
relationship. A large + sum indicates a + relationship, a large - sum indicates a -
relationship. A sum of cross-products approximately 0 indicates no linear relationship
because the + cross-products cancel the - cross-products in their sum.
However, the sum is confounded by sample size, the larger the sample, the larger the
sum for two related variables. To get a statistic that is not biased toward sample
size, divide the sum by degrees of freedom to get a mean. The result is the sample10.2. RELATION OF TWO NUMERIC VARIABLES 197
covariance of x and y, sxy: The average sum of the cross-products of the corresponding
deviation scores.
covariance, sxy:
Sum of the
cross-products
divided by n − 1.
sxy =
P(xi − mx)(yi − my)
n − 1
Notice the similarity between the definition of the covariance of two variables and
the variance of one variable. variance of a
variable, Section 6.2,
p. 102
The variance, the average squared deviation score, is
the covariance of a variable with itself.
The covariance can be a useful indicator of the strength of a relationship between two
variables. The stronger the relationship, the larger the magnitude of the covariance, in
either the + or - direction. However, the covariance is unit-dependent. The covariance
of height and weight when height is measured in inches is much smaller than the
same covariance when height is measured in centimeters as there are 2.54 centimeters
per inch.
Unless the units are of fundamental importance, remove the influence of the units by
converting the deviation scores to standard scores. To do so, divide each deviation
score by the standard deviation of the variable. Pearson
product-moment
correlation
coefficient, rxy: A
value from from −1
to 0 to 1 that reflects
the extent of the
linear relationship
between two
variables.
The result is the Pearson product￾moment correlation coefficient, rxy. This correlation is impervious to a change in
units, and can be expressed in terms of covariance of the two variables and standard
deviation of each variable.
rxy =
P zxzy
n − 1
=
sxy
sxsy
A correlation of +1 denotes a perfect positive association with all points falling on
a straight line. “Perfect” means that if the value of X is known, the relationship
provides the precise value of Y. A correlation of −1 indicates a perfect negative
relationship. A correlation of 0 indicates no relationship between the two variables.
There are several types of correlation coefficients. The correlation coefficient defined
above that gauges the magnitude of a linear relationship is the Pearson product￾moment coefficient, indicated in the sample with r and in the population with
the corresponding Greek letter, ρ. Plot() by default provides the sample Pearson
correlation coefficient and its inferential analysis. Correlation()
function, lessR:
Provides analysis of
the correlation
coefficient without
the scatterplot.
To obtain only text output, without
the scatterplot, use the lessR function Correlation(), abbreviated cr(). The
Correlation() function also provides the sample covariance, which expresses the
variables in their original measurement units.
10.2.3 Two Unrelated Variables
Generate the Data
Consider a scatterplot of two uncorrelated variables. Generate the data by simulating
random sampling from a normal distribution. Create simulated data values for
variables X and Y that are randomly and separately sampled, so there is no correlation
between X and Y in the population, ρ = 0.
rnorm() function, R:
Simulate data
randomly sampled
from a specified
normal population.
Generate the sample values with the R
function rnorm() that simulates sampling from a normal population with the specified
sample size, mean, µ, and standard deviation, σ.198 CHAPTER 10. CORRELATION
Input. Generate randomly sampled normal data values for X and Y
X <- rnorm(n=250, mean=50, sd=10)
Y <- rnorm(n=250, mean=50, sd=10)
The sample correlation of X and Y deviates from its corresponding population value
of 0 due to sampling error. For a sample of 250 paired data values the resulting
sample correlation between the variables X and Y is close to but not precisely equal
to 0. Because sampling error differs from sample to sample, a different set of data
values is generated for each repetition of generating a new set of random data by new
function calls to rnorm().
user’s workspace:
Area to store data
frames and other
created objects.
The creation of the variables X and Y illustrates another property of the R environment.
The (simulated) data values for these variables do not exist in a data frame such
as d. Instead they exist in a workspace called the user’s workspace, or sometimes
the global environment. Use this space for creating variables and other objects. The
ls() function: List user’s workspace is also where data frames are stored.
the contents of a
workspace.
To view the contents of this
workspace at any time during an R session, use the list function ls() without any
arguments, shown in Listing 10.1.
ls()
[1] "X" "Y"
Listing 10.1: List the objects in the global environment, the workspace.
Or, in RStudio, to view the objects click the Environment tab in the upper-right
window pane.
By default, most lessR functions that process data can also access variables stored
directly in the user workspace instead of within a data frame in that workspace.
These functions automatically scan for the specified variables in both the specified or
default data frame and the user’s workspace. For example, Plot() can by default
access any variable in d or variables in the user’s workspace.
Visualization
95% data ellipse:
Contains, on average
across multiple
samples, 95% of the
points in a sample
scatterplot of two
normally distributed
variables.
Consider the 95% data ellipse drawn around the points in the scatterplot. The
ellipse provides a visual summary of the extent of the relationship between the two
variables. The narrower the ellipse, the stronger the relationship. For a perfect linear
relationship, the ellipse collapses to a line. For two uncorrelated variables measured
on the same scale, the ellipse expands outward to a circle (if the axes of the two
variables are similarly calibrated).
Figure 10.4 shows the scatterplot, data ellipse, and fit line for two unrelated variables.
The obtained correlation in this sample is r = 0.016, differing from the population
value of ρ = 0 only by sampling error. The 95% data ellipse over the scatterplot of
variables X and Y in Figure 10.4 is almost a circle, indicating that the variables X
and Y have no relationship. As a result, the best-fitting line through the scatterplot
is nearly horizontal. The lack of a linear relationship between the variables indicates10.2. RELATION OF TWO NUMERIC VARIABLES 199
that for a specific value of X, the corresponding value of Y is as likely to be larger
than its mean near 50 as smaller than its mean. Increasing the value of X leads to no
predictability regarding the corresponding value of Y.
Plot(X, Y, ellipse=0.95, fit="lm")
Figure 10.4: The scatterplot with the 0.95 data ellipse for 250 values of variables X and Y
that exhibit almost no relationship, correlating only r = 0.016 in this sample. ellipse=0.95
parameter: Draw the
95% data ellipse
around the values in
a scatterplot.
The lessR Plot() function accesses the ellipse function directly from its ellipse
parameter, which defaults to 0.95 or TRUE to draw the 0.95 ellipse.1 Specify one
or more confidence levels between 0 and 1, including a vector of values for multiple
ellipses.
enhance parameter,
Plot(): Add
multiple features to a
scatterplot including
the 95% ellipse,
best-fitting line, and
outlier detection.
Or, set the parameter enhance to TRUE to visualize the 95% ellipse and also
the best-fitting line, outlier detection and best-fitting line without the outliers, and
the means of each variable plotted as straight lines.
outlier, Section 5.3.1,
p. 90
10.2.4 Two Variables Positively Related
Visualization
This example is of a scatterplot, 95% data ellipse, and fit line for a strong positive
relationship, again the Years and Salary scatterplot from Figures 10.1 and 10.2
Compared to the almost circular ellipse in Figure 10.4, the ellipse in Figure 10.5 is
much narrower.
This narrow ellipse is the geometric representation of a large correlation, which for
Years and Salary is r = 0.852, as reported in Listing 10.2. Accordingly, the fit line
demonstrates a considerable slope.
1The function that draws this ellipse is from Murdoch and Chow’s ellipse package (Murdoch &
Chow, 2022), which is automatically downloaded when lessR is downloaded.200 CHAPTER 10. CORRELATION
Plot(Years, Salary, ellipse=0.95)
Figure 10.5: Strong linear relationship demonstrated with the scatterplot of Years Employed
and Annual Salary for r = 0.85 with the 0.95 data ellipse.
Interpretation. Scatterplot
The number of Years employed is strongly related, though not perfectly, to the
value of Salary.
As the value of Years increases, the value of Salary tends to correspondingly increase.
Statistics
As is true of virtually any data analysis, the goal extends beyond a description of the
sample to information regarding the entire population from which the sample was
obtained. Plot() and Correlation() compute and display both sets of statistics.
The analysis begins with the descriptive statistics reported in Listing 10.2.
--- Pearson’s product-moment correlation ---
Number of paired values with neither missing, n = 36
Number of cases (rows of data) deleted: 1
Sample Covariance: s = 106451.209
Sample Correlation: r = 0.852
Listing 10.2: Background information and sample estimate of the correlation coefficient
from Plot() and Correlation().
The sample correlation to three decimal digits is r = 0.852, but what is the population
value of interest, ρ? As usual, address this question with a hypothesis test and/or
confidence interval. The null hypothesis follows.
Null hypothesis: ρ = 0, no linear relationship between Years and Salary10.2. RELATION OF TWO NUMERIC VARIABLES 201
Again, as usual, the standard error serves as the basis for the hypothesis test and con￾fidence interval. When ρ = 0, appropriate for the construction of the hypothesis test
under the assumption of no correlation, the standard error of the sample correlation
coefficient follows.
sr =
s
1 − r
2
n − 2
=
s
1 − (0.852)2
36 − 2
= 0.0897
Base the hypothesis test on the t-value, the distance in estimated standard errors the
sample statistic, r, is from the hypothesized value:
tr =
r − 0
sr
=
0.852 − 0
0.0897
= 9.501
What is the true value of the correlation coefficient? The confidence interval for the
population value of the correlation provides the answer as best can be ascertained
from the available information. However, the standard error for the confidence interval
depends on the more general case in which ρ 6= 0. This computation is somewhat
complex, its details not presented here. This standard error, sZ, is computed based
on what is known as the Fisher’s Z transform.
Listing 10.3 shows the relevant inferential statistics output from either Plot() or
Correlation().
Hypothesis Test of 0 Correlation: t = 9.501, df = 34, p-value = 0.000
95% Confidence Interval for Correlation: 0.727 to 0.923
Listing 10.3: Inferential analysis of the correlation coefficient from Plot() and
Correlation().
The hypothesis test shown in Listing 10.3 is a two-tailed test in which substantially
large differences in either the positive or negative direction from 0 lead to rejection of
0 as the population value.
Years and Salary Relationship:
p-value = 0.000 < α = 0.05, reject H0
The rejection of the null hypothesis implies that variables Years and Salary are related.
From Listing 10.3, we also obtain the 95% confidence interval.
Interpretation. Correlation
We are 95% confident that the true correlation between Years and Salary is between
0.727 and 0.923, a rather substantial positive relationship.
Specify one-tailed hypothesis tests in the R environment with the alternative
parameter. alternative
parameter,
Section 6.3.4, p. 121
The values for the alternative parameter that specify a one-tailed test
are "less" and "greater". The advantage of a one-tailed test is that a difference in
the direction of the rejection region is easier to detect than the two-tailed alternative.
The disadvantage of the one-tailed test is that if a large difference in the opposite202 CHAPTER 10. CORRELATION
direction occurs, then the null hypothesis cannot be rejected, so the difference, even if
substantial, is not detected. Usually the researcher would find a substantial deviation
from the null in either direction to be of interest, regardless if the direction of the
deviation is predicted or desired, so usually do a two-tailed test.
10.2.5 Scatterplot Classification Variable by parameter,
Plot(): Specify a
categorical variable
for which to plot the
points of the same
level in the same,
unique style on the
same panel.
Points in a scatterplot can also be plotted with different colors and/or plotting symbol
according to different values of a third variable, a categorical variable with a small
number of number of categories or levels. Specify a categorical variable with the by
parameter. The default plot uses a different color, or shade of gray for grayscale, to
plot the points for different values of by.
In this example, in addition to different shades of gray, view the scatterplot of Years
and Salary with different plot symbols for men and women, specified by the parameter
shape.
shape parameter:
Specify the shape of
a plotted point.
c() function:
Section 1.2.9, p. 12
Using the R function c(), indicate the values of shape as a vector of two
values, one for each Gender that exists in the data. Specify the fit lines each based on
a linear model, setting parameter fit to "lm" to obtain the best-fitting line through
each set of plotted points.
Find the resulting scatterplot, created using the optional shape parameter, in Fig￾ure 10.6.
Input. A scatterplot with Gender as a classification variable
Plot(Years, Salary, by=Gender,
shape=c("triup", "tridown"), fit="lm")
Figure 10.6: Scatterplot of Years and Salary with different plot symbols for men and women
and respective least-squares fit lines.
Plot symbols: To
view all symbols for
plotting points,
access the points
manual, enter:
?points.
Valid values of the plot symbol include "circle", "square", "diamond", "triup",
and "tridown", and all upper- and lower-case letters of the alphabet. The order of
the specified symbols reflects the order that the levels of the classification variable
are defined. By default R defines the order of the levels alphabetically, though the
order can be changed with the factor() function. factor() function,
Section 3.2, p. 4310.2. RELATION OF TWO NUMERIC VARIABLES 203
Another possibility for creating different plots for different levels of a classification
variable plots the scatterplot for each group on a different panel, a Trellis plot
(Cleveland, 1993).
Trellis plot: Plot
points for each level
of a categorical
variable on different
panels.
by1 parameter,
Plot(): Specify a
categorical variable
for which to plot the
points for different
levels as a Trellis
plot.
The distinction for lessR input invokes either the by parameter
to plot the points for the different levels on the same panel, or the by1 parameter
to plot on different panels as a Trellis plot. The resulting Trellis plot appears in
Figure 10.7.
Plot(Years, Salary, by1=Gender, fit="lm")
Figure 10.7: A Trellis scatterplot of Years and Salary plotted on separate panels for each
value of Gender.
By default, the panels are displayed in a single column. To create a Trellis plot with
the panels displayed in a single row, set the parameter n_row to 1. Specify the
desired number of rows or columns with n_row or n_col, respectively.
Viewing Figures 10.6 or 10.7 reveals distinctions between the distributions of men’s
and women’s salaries.
Interpretation.
At this company three of the highest four salaries are by men and three of the
lowest four salaries are by women. The scatterplot also reveals, however, that
women tend to be concentrated at the lower end of the number of Years employed.
The eight employees with the least Years of employment are all women.
Experiment interactively with different parameter settings of the Trellis visualization
for your data with the lessR function interact(). Choose your data file, then
point-and-click to select the continuous variable and the categorical by variable. interact(),
Section 9.2, p. 172
Input. Interactive experimentation with colors and other parameters
interact("Trellis")
When finished interactively designing your Trellis plot, save the code and reapply in
the future as needed.
10.2.6 Bubble Plot
Likert data are the responses to attitude items such as on the Disagree/Agree
continuum, usually 4 to 7 different scale points, coded as integer values.
Likert data,
A traditional Section 1.3.2, p. 17204 CHAPTER 10. CORRELATION
scatterplot of two variables assessed on a Likert scale is not usually informative
because of the small number of potential unique data values. A scatterplot of two five￾point Likert scale items yields only 25 unique combinations of data values. Without
some transparency of the plotted points there is no way to tell if a plotted point,
such as for a 3 on the first item and a 2 on the second item, represents one example
of data values or many. Setting a transparency level with the trans parameter can
help but with enough data most of the 25 positions for plotted points in this example
bubble plot may still be filled. : The
size of each plotted
point in a scatterplot
reflects another value
such as frequency.
The lessR solution to this problem automatically provides a form of a scatterplot
called a bubble plot if there are less than 10 unique values for each variable. The
larger the joint frequency for a pair values from the two variables, the larger the size
of the corresponding plotted point, that is, bubble. To illustrate, we return to the
Machiavellianism data set.
Mach IV data,
Listing 1.5, p. 17
Scenario. Visualize the relationship between Mach IV Items 6 and 7
6. Honesty is the best policy in all cases.
7. There is no excuse for lying to someone else.
Explore their relationship with a bubble plot.
Read the Machiavellianism data plus the corresponding items as variable labels
directly from within lessR.
d <- Read("Mach4")
l <- Read("Mach4_lbl")
loess line,
Section 10.2, p. 195 Obtain the scatterplot in Figure 10.8 with the Plot() function with the loess line.
Input. Bubble plot of Likert data with loess fit line
Plot(m06, m07, fit="loess")
Figure 10.8: Scatterplot in the form of a bubble plot of Mach IV items m06 and m07.10.3. CORRELATION MATRIX 205
Because there are only six unique values for each variable, which is less than the thresh￾old of ten, Plot() automatically displays the bubble plot version of the scatterplot. bubbles parameter:
Plot bubbles instead
of points to create a
bubble plot.
Or, set parameter bubbles to TRUE to activate a bubble plot.
The linear relationship of m06 and m07 is apparent from Figure 10.8 as the largest
bubbles lie along the increasing diagonal axis. Consistent with this pattern, the
loess fit line is approximately linear along this diagonal. The largest bubble is for
the Agree response to each item, coded as a 4. For these 351 respondents, just one
possible pair of data values did not occur: Strongly Disagree coded as a 0 for m06
paired with Agree coded as a 4 for m07.
Interpretation. Relation of m06 and m07 from the Mach IV scale
The relationship between m06 and m07 is approximately linear. Most respondents
tended to agree with both items with a substantial minority of respondents who
disagreed with both items.
Figure 10.8 visualizes the Likert response data. Also of interest are the actual joint
frequencies, the count of each possible pair of data values. Cross-tabulation
table, Section 4.3,
p. 73
Plot() also displays the
cross-tabulation table of joint frequencies from which the scatterplot was created.
m06
m07 0 1 2 3 4 5 Sum
0 4 3 2 3 3 2 17
1 7 24 7 6 18 2 64
2 4 14 30 13 24 2 87
3 2 1 10 16 12 2 43
4 0 3 13 5 56 16 93
5 1 2 1 1 8 34 47
Sum 18 47 63 44 121 58 351
Listing 10.4: Cross-tabulation table for Mach IV items m06 and m07.
pivot() applied to
categorical data,
Section 4.3.3, p. 77
This same cross-tabulation table can also be obtained from pivot().
table_margins
parameter, pivot():
Display the row and
column sums, the
table margins.
To also obtain
the marginal sums, set the parameter table_margins to TRUE.
Input. Get cross-tabulation table for m06 and m07.
pivot(d, table, m06, m07)
The advantage of pivot() is that it is a general function for simultaneously computing
descriptive statistics across all of the data or aggregated according to groups defined
by one or more categorical variables.
10.3 Correlation Matrix
The focus of this chapter has been on the linear relationship of two variables with
each other, as shown by their scatterplot and Pearson correlation coefficient. However,
many variables of interest often exist, such as the items on an attitude survey. In
this situation, the focus shifts from just two variables to the relations among all the
variables of interest.206 CHAPTER 10. CORRELATION
10.3.1 All Numeric Variables
We begin with the correlation coefficients of all the pairwise combinations of the
variables. correlation
matrix: Symmetric
table of correlations
of a set of variables.
The same lessR function Correlation(), abbreviated cr(), that calcu￾lates a single correlation coefficient between two variables also calculates a correlation
matrix, a table of correlation coefficients between all pairs of the specified variables.
Correlation() also visualizes all possible scatterplots between pairs of variables. In
the function call to Correlation(), indicate multiple variables, a vector, instead of
two separate variables. Or only specify variables to analyze some numeric variables
in the corresponding data frame.
Input. Correlation matrix from all numerical variables in d
Correlation()
When there is no specified list of variables, the Correlation() function searches the
input data frame for non-numerical variables, notes their existence, and then excludes
them from further analysis.
10.3.2 List of Variables
If only some numerical variables in the input data frame are included in the correlation
analysis, specify a vector of the included variables by listing multiple variables found
in the input data frame. A vector can be written several different ways. As always in
R, if commas delineate any of the variables, enclose the list with the c() function.
c() function,
Section 1.2.9, p. 12
For example, specify the four variables on the Mach IV Deceit subscale followed by
Mach IV subscales the two items of the Flattery subscale. ,
Section 15.7, p. 341
c(m06, m07, m09, m10, m15, m02)
Specify a vector of contiguous variables in the data frame with the : notation instead
of listing each name individually. Here, specify the first 10 Mach IV items.
m01:m10
Or, combine the c() function and the : notation to specify a vector.
c(m01:m05, m10, m13, m15:m18)
Pass any of the preceding vectors to the Correlation() function as a single parameter
value. In the following example, calculate the correlation matrix of the items of the
Deceit and Flattery subscales.
The input and an annotated version of the corresponding output correlation matrix
appears in Figure 10.9. The matrix consists of the main diagonal and the lower and
upper triangles. default data frame,
Section 1.2.8, p. 11 Access the data from the default data frame d, so no need for the
parameter data. This example contains no missing data, a topic discussed later.10.3. CORRELATION MATRIX 207
Input. Calculate the correlation matrix from the specified variables
Correlation(c(m06, m07, m09, m10, m15, m02))
Figure 10.9: Annotated correlation matrix of six Mach IV items from Correlation().
The correlation coefficient is symmetrical. The correlation of X with Y is the same as
Y with X. So each correlation appears twice in the matrix, once in the lower triangle
and once in the upper triangle. For example, the correlation of m06 and m07, 0.52,
appears twice in the top-left of the matrix. Each item correlates with itself a perfect
1.0, the value that appears in the main diagonal.
10.3.3 Missing Data
The Machiavellian data set has no missing data. To illustrate how the Correlation()
function addresses missing data, remove one value from the data set in the first row
of data for variable m06. edit individual data
value, Section 3.7.1,
p. 56
The result is shown in Listing 10.5, which displays the NA
value for m06 that indicates a missing data value in an R data frame.
d[1, "m06"] <- NA
head(d, n=1)
Gender m01 m02 m03 m04 m05 m06 m07 m08 m09 m10 m11 m12 m13 m14 ... m20
1 0 0 4 1 5 0 NA 4 1 5 4 0 0 0 0 ... 4
Listing 10.5: First row of of the d data frame of the Mach4 data set from head().
pairwise deletion:
Compute the
correlation coefficient
from all non-missing
data for each pair of
variables.
The default method for addressing missing data is pairwise deletion. Compute each
correlation coefficient from the complete paired data values for the two corresponding
variables. Listing 10.6 shows the sample sizes with parameter show_n=TRUE.
show_n parameter:
Control the display
of the table of
sample sizes for the
correlations
computed with
pairwise deletion.
Correlation(c(m06, m07, m09, m10, m15, m02), show_n=TRUE)
Missing data deletion: pairwise
m06 m07 m09 m10 m15 m02
m06 350 350 350 350 350 350
m07 350 351 351 351 351 351
m09 350 351 351 351 351 351
m10 350 351 351 351 351 351
m15 350 351 351 351 351 351
m02 350 351 351 351 351 351
Listing 10.6: Sample size matrix for the pairwise computed correlations.208 CHAPTER 10. CORRELATION
When show_n is set to TRUE, Correlation() displays the sample size instead of the
correlations.
In all 351 rows of data in this example, the data table had only one missing value
for one variable, m06. This row of data is subsequently removed from the calculation
of m06’s correlation with all other variables. The sample size for all m06 correlations
with other variables lowers from 351 to 350.
listwise deletion: Listwise deletion is another method for addressing missing data.
One missing data
value in a row leads
to the deletion of the
entire row of data.
If a row of data has
any missing data values, then that entire row of data is deleted from the analysis.
Specify listwise deletion with the miss parameter set to "listwise", of which the
default value is "pairwise". Find the input and part of the output in Listing 10.7.
Input. Correlation matrix with listwise deletion
R <- Correlation(c(m06, m07, m09, m10, m15, m02),
miss="listwise")
Missing data deletion: listwise
Sample size after deleted rows: 350
Listing 10.7: Specify listwise deletion with Correlation().
All correlation coefficients are calculated for listwise deletion with the same sample
size. The value reported in Figure 10.7 is 350, the data that remain after deleting
the first row of data from the analysis due to its one missing value.
Pairwise deletion is generally preferred over listwise deletion because of the loss
of additional data from the listwise procedure. However, because the pattern of
missing data differs for each pair of variables, the pairwise sample size on which each
correlation is based also varies. A potential problem with pairwise deletion is that
some correlations may be calculated from an extensively diminished sample size, so
always examine the sample size matrix.
sp_matrix, 10.3.4 Visualizations
parameter: Set to
FALSE to turn off the
scatterplot matrix.
heat_map,
parameter: Set to
FALSE to turn off
the heat map.
Correlation() creates two visualizations of the correlation matrix, a scatterplot
matrix and a heat map. The visualizations are displayed by default but can be turned
off by setting respective parameters sp_matrix and heat_map to FALSE.
Scatterplot Matrix
scatterplot
matrix: Table of
scatterplots, one for
each correlation.
The scatterplot matrix displays the scatterplot of each pair of variables within the set
of specified variables. Correlation()’s scatterplot matrix leverages the symmetry
of the correlation coefficient to display the scatterplots in the lower triangle of the
matrix and the corresponding correlation coefficients in the upper triangle.
scatterplot matrix,
Section 12.1, p. 256 The six variables represented in the scatterplot matrix shown in Figure 10.10 each
have a small number of unique values that limits the configuration of plotted points.
Each scatterplot also contains the line of best fit, which is helpful to gauge the extent
of the relationship. For example, the scatterplot of the two most highly correlated10.3. CORRELATION MATRIX 209
items, m06 and m07, contains a line of pronounced slope.
Figure 10.10: Scatterplot matrix of six Mach IV items.
Heat Map
heat map:
Visualization of a
matrix with each
number replaced by
a colored square.
The heat map, illustrated in Figure 10.11, visually portrays the correlation matrix
by replacing each correlation coefficient with a colored square. The larger is the
correlation, the darker the color.
m06
m07
m09
m10
m15
m02
m02
m15
m10
m09
m07
m06
Figure 10.11: Heat map of the correlation matrix of six Mach IV items.
The diagonal elements of the heat map, which represent the correlations of each item
with itself, are treated differently. To provide more color separation for off-diagonal
elements, the diagonal elements of the heat map are set to 0.
The largest correlation in the matrix, 0.52 between Items m06 and m07, is represented
by the two darkest colored squares, at the top-left of the heat map. The lowest
correlations in the matrix are between the items in the two different sub-domains.
These correlations are represented by white or very light gray colored squares in the
second-to-last and last rows of the matrix and the second-to-last and last columns.
The differentiation of the two different sub-domains of Mach IV items is clearly visible
in the Figure 10.11.210 CHAPTER 10. CORRELATION
Specify a title for the heat map with the usual R visualization parameter main.
main parameter, R:
Heat map title.
Depending on the size of the variable names, the bottom and the right margins of
the heat map might be too narrow to accommodate the full names. To widen the
margins, use the bottom and right parameters, such as bottom=5 to specify five
lines for the bottom margin.
bottom, right
parameters: Number
of lines for each
margin.
As with all lessR visualizations, the scatterplot matrix
and heat map can be written to a pdf file instead of displayed in a visualization
window. To do so, invoke the usual lessR parameter pdf set to TRUE, and if desired,
the accompanying size specifications with width and height.
10.3.5 Save the Correlations
factor analysis,
Section 15, p. 321
Saving the correlations allows them to be entered into other analyses, such as factor
analysis or reordering the variables to better detect patterns of similar correlations
with the lessR function corReorder(), discussed in the following section. reorder the
correlation matrix,
Section 15.5.2, p. 339
To save
the computed correlations and other information, direct the output of Correlation()
to an object with the generic name for a correlation matrix, R.
Input. Save into an R Object
R <- Correlation(c(m06, m07, m09, m10, m15, m02)) list: General R
structure for storing
different kinds of
information, such as
a data frame with
variables of different
types.
The output object, R in this example, is an R object called a list, which consists of
several components. View the names of the components with the R function names().
names() function, R:
Display the names of
all variables in the
specified data frame.
names(R)
[1] "out_background" "out_missing" "out_cor" "R"
Listing 10.8: Contents of the output object from Correlation().
A matrix is a simpler version of a data frame with the constraint that all columns in
the matrix must be of the same data type. The matrix R is an R square matrix with
matrix: A storage the default name for the lessR factor analysis and item analysis procedures.
container in which all
the entries are of the
same data type.
All the
entries of matrix R are real numbers, that is, numeric with decimal digits. By default,
the number of digits for each calculated correlation coefficient is two, a value changed
with the usual lessR parameter digits_d.
digits_d parameter,
Section 4.3.2, p. 76
10.3.6 Cluster Analysis
The correlation matrix may reveal patterns of how the variables relate to each other.
However, uncovering these patterns usually involves reordering the variables by their
similarity, such as by the magnitude of their correlation. Consider a self-report survey
with multiple sets of items organized into subscales. Items within a subscale should
have a higher correlation with one another than with other items. However, if the
items on the survey are randomly ordered, this structure may be obscured from the
display of the randomly ordered item correlations. cluster analysis:
Arrange a set of
variables by their
similarity. One technique to reveal the underlying structure is cluster analysis, which reorders
the correlation matrix so that similar items and variables appear adjacent to each10.3. CORRELATION MATRIX 211
other in the matrix. 2 The measure of similarity of two variables applied here is
the correlation coefficient.
corReorder()
function: Reorder
the variables in a
correlation matrix,
such as with a
hierarchical cluster
analysis.
Accomplish this correlation matrix reordering with the
lessR function corReorder(), abbreviated reord(). The function provides several
methods for reordering the correlation matrix. The primary parameter to pass to
corReorder() is the name of the saved correlation object, such as the default R.
corReorder()
function, lessR
Listing 15.5.2, p. 339
corReorder(R)
hierarchical
cluster analysis:
Display a hierarchy
of clusters from the
two most general
clusters to the most
specific clusters.
By default, corReorder() re-arranges the order of the variables in the input corre￾lation matrix according to a hierarchical cluster analysis. This clustering technique
groups the variables into a hierarchy of clusters, which results in a linear ordering of
the variables, the items in this example.
agglomerative:
Bottom-up method
that begins with each
variable as its own
cluster, combining
until all variables are
in the same, single
cluster.
The default hierarchical cluster analysis is agglomerative. This bottom-up method
proceeds by initially designating each variable as its own cluster. Then combine
the two most similar clusters into a single cluster, repeating the process until all
the variables form a single cluster. Although many ways exist to define what is
meant by the closest cluster, the default method by the R hierarchical clustering
function hclust() generally works well on correlation matrices. The lessR function
corReorder() depends on hclust() for the analysis.
dendrogram:
Visualization of
relationships as a
binary tree, from
first split at the top,
continuing down to
the leaves, the
variables.
Mach IV data,
Listing 1.5, p. 17
Figure 10.12: Hierarchical cluster dendrogram for the
first 16 Mach IV items, annotated to
suggest a cluster structure.
The hierarchical cluster analy￾sis organizes the variables into
a binary tree called a dendro￾gram, illustrated in annotated
Figure 10.12 for the first 16
Mach IV items. The dendro￾gram visualizes both the similar￾ity of the adjacent clusters and
the sequence in which they were
formed. Once the dendrogram
is formed, the top of the dendro￾gram divides all variables into
the two most dissimilar groups.
The first split of the 16 Mach IV
items defines two different sets
of content.
Mach IV subscales,
Listing 15.7, p. 341
Items in the first
branch, from m02 to m13, consist largely of items that describe Flattery and Distrust.
Items from the second branch, from m03 to m14, largely describe Deceit and Cynicism.
Each split defines new, smaller clusters. The second level of the tree splits all the
elements of each of the first two different groups, the two most dissimilar groups,
leaving four clusters. These binary splits continue until only the original elements
remain, the leaves (items) of the tree, with no more splitting possible.
To directly visualize this re-ordering of the items according to their correlations,
corReorder() also displays the annotated reordered heat map, shown in Figure 10.13.
2The other primary technique for detecting patterns of variable correlations is factor analysis, the
topic of Chapter 15.212 CHAPTER 10. CORRELATION
Figure 10.13: Heat map of reordered correlation matrix for the first 16 Mach IV items,
annotated to suggest a cluster structure.
The dendrogram and the heat map both indicate the derived linear ordering of the
items. From that information, choose the number of clusters that best represent the
structure of the variable correlations, perhaps retaining only some of the variables.
The strongest indication is for m09, m10, m06, and m07 forming a cluster of Deceit
items. The remaining items from that same first split of the tree are m11, m16, m04,
and m14, which may be described as endorsing Cynicism. For the other first split,
m02 and m15 describe the endorsement of Flattery. Most remaining items from the
split that tend to cluster, m01, m12, m05, and m13, describe Distrust.
order parameter:
Specify the method
for re-ordering the
variables in a
correlation matrix.
corReorder() also provides for a manual reordering of the correlation matrix, includ￾ing variable deletion, which allows manual fine-tuning of the initial results from an
algorithm, or experimentation to view the results of a specific recording.
vars parameter:
Specify a vector of
variables from which
to reorder the matrix.
Indicate the
specified ordering by providing a vector of ordered variable names to the parameter
vars, which also sets the order parameter to "manual".
For example, the heat maps in Figure 10.11 reveal the strongest correlation, between
m07 and m06, so perhaps place those two items and two other highly related items,
m09 and m10, at the beginning of the heat map. Items m03 and m08 appear relatively
isolated. The following code accomplishes the re-ordering, and leaving m03 and m08
out of the list deletes them from the heat map.
Input. Manual reordering of the correlation matrix
corReorder(R, order="manual",
vars=c(m09,m10,m06,m07,m11,m16,m04,m14,m02,m15,m01,m12,m05,m13))
corReorder() also implements Hunter’s (1973) linear ordering procedure when order
is set to "chain". The procedure is simple but surprisingly effective for correlation
and related matrices. The first variable is that with the largest sum of squared
correlations across all the variables, the variable that can be considered to be the10.4. NONPARAMETRIC CORRELATION COEFFICIENTS 213
most related to all the variables in the set. Then the variable that has the highest
correlation with the first variable is listed second, and so forth. When applied to the
Mach IV data, this simple procedure identified the sub-groups about as well as the
more formal implementations.
10.4 Nonparametric Correlation Coefficients
two-samples,
Chapter 7, p. 127
many-samples,
Chapter 8, p. 151
Many nonparametric approaches, as demonstrated in the chapters on group differ￾ences, analyze the ranked ordinal version of the data instead of the original data.
Analysis of ranked data yields a statistic more resistant to outliers than the corre￾sponding parametric statistic, and without assuming underlying normality. method="spearman"
parameter, R:
Spearman
correlation.
The same
principles apply to the two nonparametric correlation coefficients available within
R, the Spearman and Kendall coefficients. To compute one of these coefficients, set
the method parameter to "spearman" or "kendall" to the calls to the Plot() and
Correlation() functions.
method="kendall"
parameter, R:
Spearman Correlation Kendall correlation.
The Spearman correlation coefficient has its own specialized computational formula,
but conceptually it applies the standard Pearson correlation formula directly to the
ranked data. The application of the standard Pearson formula equals or approximately
equals the specialized formula depending if tied ranks exist in the data.
Spearman
correlation, rs:
Pearson correlation
of ranks.
Pearson correlation
coefficient,
Section 10.2.2, p. 196
Obtain a
perfect Spearman correlation3 when both sets of ranked data align perfectly, which
occurs when each person has the same rank on each of the two variables.
Because the data are expressed as ranks, any transformation that preserves the ranks
of the values of one of the variables leaves the Spearman correlation unchanged. As
such, the Spearman correlation also applies to nonlinear relationships. Squaring each
value of a variable, for example, does not change their order.
monotonic
relationship: As X
increases, Y
consistently increases
or decreases, but not
both.
The Spearman coefficient does not require linearity, so it is a more general index
of the relationship between two variables. A linear relationship is both linear and
monotonic, with a consistently positive or negative trend. A Spearman coefficient
only requires monotonicity, consistent increasing or decreasing.
Figure 10.14 compares the Pearson and Spearman correlation coefficients for an
exponential relationship. For the perfect exponential distribution on the left side of
Figure 10.14, there is perfect positive monotonicity, each value of Y increases as X
increases. With consistent ordering, the Spearman correlation achieves the maximum
value of rs = 1.0. The Pearson correlation only addresses linearity, so is less than 1.0
for the exponential distribution, r = 0.74.
However, when random error is added to the distribution on the right, the monotonicity
is no longer perfect. For this second distribution, some values of Y are less than the
previous value of Y whereas most values of Y are larger than the previous value. The
3Some authors refer to the Spearman correlation as rho, ρ, which violates the distinction between
Roman letters for sample statistics and Greek letters for corresponding population parameters. As
such, here we use rs to refer to a sample Spearman correlation.214 CHAPTER 10. CORRELATION
Pearson correlation changes little, r = 0.74, but the Spearman correlation diminishes
to rs = 0.75.
2 4 6 8
0
2000
4000
6000
8000
x
y
Pearson r = 0.74
Spearman r = 1.00
Pearson r = 0.74
Spearman r = 1.00
2 4 6 8
0
2000
4000
6000
8000
x
y
Pearson r = 0.73
Spearman r = 0.75
Figure 10.14: Pearson and Spearman correlation coefficients for a perfect exponential
distribution (left) and the same distribution with added noise (right).
Employee data set,
Section 1.3.1, p. 13
Next, apply the Spearman correlation to the relationship between Years employed
and Salary from the Employee data set shown in Listing 10.9.
Input. Calculate the Spearman correlation coefficient
Correlation(Years, Salary, method="spearman")
Sample Correlation of Years and Salary: Spearman correlation = 0.800
Alternative Hypothesis: True Spearman correlation is not equal to 0
S-value: 1553.770, p-value: 0.000
Listing 10.9: Excerpt of the analysis of the Spearman correlation for Years and Salary.
The output includes both the descriptive correlation coefficient in the sample, rs =
0.80, and a hypothesis test that the population value is zero, or, more precisely, that
the two variables are not monotonically related. The hypothesis test follows.
H0: No population monotonic relationship between Years and Salary.
Reject the null hypothesis for Years employed and Salary.
Test of Spearman population correlation of 0:
p-value = 0.000 < α = 0.05, reject H0
Interpretation. Spearman correlation
Years employed and Salary are monotonically related. As the number of Years
employed increases, Salary also tends to increase.
The S-value in the output shown in Listing 10.9 is part of the specialized formula
for the computation of rs. It is the sum of the squared difference of the ranks, row
by row. The important consideration for the analysis is the p-value in regards to the
hypothesis test.10.5. ANALYSIS PROBLEMS 215
Kendall Correlation
concordant pair of
data values: The two
values for each
variable change in
the same direction.
The Kendall correlation coefficient, rk, provides another means by which to assess
monotonicity. The coefficient is based on a direct analysis of what are called concordant
pairs. Consider any two pairs of data values, Xi
, Yi and Xj , Yj . If Xi−Xj and Yi−Yj
have the same sign then the pair of data values is called concordant. Similarly, if
Xi −Xj and Yi −Yj have the opposite sign, the pair of data values is called discordant.
discordant pair of
data values: The
values for each
variable change in
opposite directions.
If the corresponding value of Y always increases as the value of X increases, all pairs
of data values are concordant. Similarly, if Y always decreases as X increases, all
pairs of data values are discordant.
The numerator of the Kendall correlation coefficient is the number of concordant
pairs minus the number of discordant pairs of data values. Kendall
correlation: Based
on number of
concordant and
discordant pairs of
data values.
To normalize this result so
that the resulting coefficient lies between −1 and 1, divide this value by the number
of all possible pairs, n(n − 1)/2, where n is the sample size. Achieve the maximum
value +1 if all n(n − 1)/2 pairs are concordant, and achieve the minimum value −1 if
all pairs are discordant.
Consider again the relationship between Years employed and Salary, in Listing 10.10.
Correlation(Years, Salary, method="kendall")
Number of paired values with neither missing, n = 36
Number of cases (rows of data) deleted: 1
Sample Kendall Correlation: 0.635
Hypothesis Test of 0 Kendall correlation: t = 5.333, p-value = 0.000
Listing 10.10: Excerpt of the analysis of the Kendall correlation for Years and Salary.
The hypothesis test is of the following null hypothesis.
H0: No population monotonic relationship between Years and Salary.
Given the low p-value, the probability of such a large correlation assuming the null
hypothesis of no relation, reject the null hypothesis for Years employed and Salary.
Test of Kendall population correlation of 0:
p-value = 0.000 < α = 0.05, reject H0
The Pearson, Spearman, and Kendal correlation all provide the same conclusion:
Years employed and Salary are positively related.
10.5 Analysis Problems
1. Refer to the Cars93 data set, which is part of lessR.
?dataCars93 for
d <- Read("Cars93") more information.
a. Obtain the scatterplot and correlation for MPGcity and MPGhiway. Interpret.216 CHAPTER 10. CORRELATION
b. Calculate the correlation matrix and scatterplot matrix for the three prices for
each car: MinPrice, MidPrice and MaxPrice. Interpret.
c. From the correlation matrix of all numeric variables, which five variables are
most correlated with MPGcity?
d. What is the problem with the scatterplot matrix of all numeric variables?
2. Refer to the Employee data set, which is part of lessR.
?dataEmployee for
more information. d <- Read("Employee")
a. Obtain the scatterplot and correlation for Pre, the test score before instruction
on the relevant topic, and Salary. Interpret.
b. Calculate the correlation matrix and scatterplot matrix for the following three
variables: Salary, Pre, Post. Interpret.
c. Describe the pattern of missing data.
d. Describe the structure of the correlation matrix. Which variables are correlated,
which are not?
e. What is the problem with the scatterplot matrix of all numeric variables?
3. Compare the usual Pearson correlation with the corresponding nonparametric
Spearman and Kendall correlations.
a. Create a data vector X of 25 values of simulated data values from a random
normal distribution with a mean of 0 and a standard deviation of 1. Create a
second data vector, X3, which consists of the cubed values of X.
b. Display the scatterplot of X and X3. Comment. Is it linear?
c. Calculate the Pearson correlation coefficient of X and X3, as well as the Spear￾man and Kendall correlation coefficients.
d. Compare and account for the values of the three correlation coefficients.Chapter 11
Regression Analysis
11.1 Quick Start
Install R and lessR as explained in Section 1.2. At the beginning of each R session,
access the lessR functions with library("lessR"). From Chapter 2, read your
data into an R data frame such as d with d <- Read("") to browse for the data file.
Or, locate your data file with a path name or web address between the quotes.
Chapters 7 and 8 introduced the concept of a model, which expresses a response
variable in terms of the values of one or more input variables, traditionally called
predictor variables, or in the language of machine learning, features. The models
in these chapters accepted categorical input variables that defined different groups
of data. Chapter 10 compared two continuous, that is, numerical, variables with
each other. The current chapter on regression analysis generalizes and integrates the
concepts presented in these earlier chapters to include continuous predictor variables
that also predict a continuous response variable or target.
Regression analysis is an essential data analysis method for identifying relationships
among variables, and it is perhaps the most important statistical tool for data analysis.
This chapter focuses on one predictor variable. Models with many predictor variables
are reserved for the next chapter.
Regression analysis develops predictive models. Machine learning is rapidly finding
applications in academics and business, a growth engine for jobs. logistic regression,
Section 13.4, p. 285
Regression analysis,
paired with logistic regression analysis for predicting the value of a categorical
variable, discussed in a subsequent chapter, are essential supervised machine learning
methodologies. supervised machine
learning,
Section 11.2.1, p. 218
These methods also serve as the foundation for learning more esoteric
machine learning technologies such as support vector machines, random forests, and
neural networks.
one-predictor
regression analysis,
Section 11.3, p.221
To do a regression analysis, use the lessR function Regression(), here for response
variable Y and predictor variable X.
Regression(Y ∼ X)
Particularly when there is no intrinsic meaning to the units in which the variables
are measured, standardize the variables in the regression equation before model
217218 CHAPTER 11. REGRESSION ANALYSIS
standardize, estimation. Set the new_scale parameter to "z" for z-score.
Section 11.3.2, p. 224
Regression(Y ∼ X, new_scale="z")
By default, Regression() provides a prediction interval for each input data value.
However, prediction generally involves new values of the predictor variable, values
not contained in the original data from which the regression model is estimated. To
obtain predictions for any specified values of the predictor variable and associated
95% prediction intervals, use the X1_new parameter.
prediction for new
values of the
predictor variable,
Section 11.5.3, p. 235
In this example predictions are
obtained for values of the predictor variable of 10 or 25.
Regression(Y ∼ X, X1_new=c(10, 25))
By relying upon almost two dozen standard R functions, the lessR function Regression()
provides a comprehensive regression analysis – without the otherwise needed program￾ming. The different sections of Regression() output should generally be included
in any analysis: estimation of the model, residuals and model fit, outliers, collinearity
analysis, and prediction intervals for both existing and new data values. Obtain all
of these analyses with a single, simple function call. To manage the resulting output,
the default settings provide useful information regarding these different aspects of
regression, but can be changed as needed.
11.2 Regression Models
supervised 11.2.1 Supervised Machine Learning
machine learning:
Predict unknown
values of a variable
from values of
related variables.
The general class of statistical procedures for predicting the unknown value of a
variable from the values of related variables is today known as supervised machine
learning. Regression analysis is the classic form of machine learning, developed
in the 19th century, well before the term “machine learning” emerged to describe
modern prediction methodologies enabled by massive computer processing.
general linear model,
Section 13.2.3, p. 280
Regression
analysis is a general statistical procedure that subsumes the classical procedures
previously introduced in this book via the general linear model and simultaneously
serves as the gateway to modern machine learning.
A machine? The word machine in this context is a trendy, almost cute, but effective
reference to programmed instructions running on a computer. Today, computers
perform all statistical computations. Given its staggeringly massive superiority in data
processing speed, the machine uncovers relationship patterns much more effectively
than people. That is why we seek the machine’s help.
Prediction Equation
model: Represents
reality in the form of
equations to express
the relations among
variables. To predict, supervised machine learning techniques such as regression analysis analyze
one or more equations known as the model.
predictor variable,
feature: Variable(s)
from which the
prediction is made.
The values of the variables used to make
the prediction are referred to as predictor variables, or features in machine learning
terminology. To calculate the prediction, enter the values of these variables into the11.2. REGRESSION MODELS 219
prediction equation. response variable,
target: Variable
with the value to
predict.
The predicted variable traditionally has been known by terms
such as the response variable, or, in the language of machine learning, the target.
Figure 11.1 illustrates this process.
values of predictor variables, features predicted value of response variable, target
In Information Prediction Prediction
Equation
Out
Figure 11.1: Predict from information entered into a prediction equation, the model.
The information input into the analysis consists of the values of the equation’s
predictor variables. The output is the predicted value and an interval around the
predicted value that will likely contain the actual value when it occurs as the outcome
of some future event.
Use X as the generic name for a predictor variable, and Y for the response variable.
Variables in these predictive models are referred to by a variety of names in different
contexts, some of which appear in Table 11.1.
Y variable X variable
response predictor
dependent independent
outcome explanatory
criterion antecedent
target feature
Table 11.1: Synonymous names for the response and predictor variables, respectively, in a
regression analysis.
Even with all the available synonyms for the Y and X variables in the model from
traditional regression analysis, researchers who developed recent versions of machine
learning algorithms decided to introduce new names, target and feature, to refer to
exactly the some concepts referred to by the more traditional names. Certainly more
sophisticated algorithms for prediction such as neural networks have recently been
developed, but the purpose of all supervised machine learning is to build equations
for prediction that model the relationships among the variables.
Explanation
explanation:
Understand how
changes in the X
variables relate to
corresponding
changes in the y
variable.
A second machine learning goal beyond prediction is explanation, to explain the
relation of each of the predictor variables to the response variable. Understanding
the structure of how prediction occurs assists in building better predictive models.
How to choose new, effective predictor variables to add to a model? How to know
when to delete an ineffective predictor variable from a model? Answer questions such
as these by understanding the underlying relationships among the variables.
For theory testing, the analysis may explore a hypothesized relationship between the
variables in the model. In a business context, management may want to know how
the predictions are obtained and regulators may demand such an explanation. The
explanatory aspect of modeling attempts to answer the Why and What questions.220 CHAPTER 11. REGRESSION ANALYSIS
What experimental conditions best fostered learning? Why did shipping costs rise?
What attributes of a product enhance consumer satisfaction?
11.2.2 Functions
Regression analysis is the analysis of the mathematical concept of a function, in which
the value of one (or more) variables determines the value of another variable.
function: The value
of one or more
variables determines
the value of another
variable.
The
following discussion of functions is generally not a discussion of statistical techniques
such as regression analysis but is instead presents concepts upon which regression
analysis depends.
Linearity
linear function:
Weighted sum of one
or more variables.
An equation expresses a functional relationship. The simplest set of useful functions
are linear functions. Express a linear function as a constant plus the weighted sum
of one or more variables generically referred to as X. To compute the value of the
linear function, multiply the value of each X variable by its associated weight and
add the constant term. The general form of a linear function for a single X variable
follows, with constant b0 and multiplicative weight b1.
Yˆ : Value computed
from a function, the
prediction when
applied to new data.
The notation Yˆ indicates the
function’s value computed from the equation.
Yˆ = b0 + b1X
The values b0 and multiplicative weight b1 are constants that, when they assume
specific values, define a specific linear equation. parameters of a
linear function of one
variable: b0 and b1.
These constants are the parameters
of the equation. Arbitrarily consider b0 = 0 and weight b1 = 2 to define:
Yˆ = 0 + 2X = 2X
Plug in a given value of X, compute the corresponding value of Yˆ .
data value symbol:
Xi for a data value
of variable X.
To distinguish a
specific value of a variable from the variable name, subscript the variable with an
index such as i. When applied to data analysis, i represents any given row of data
in the data table, the i
th row. For a given value of variable X, arbitrarily presume
Xi = 12. Compute the value of the corresponding Yˆ
i
, which, for data analysis,
predicts the value Yˆ
i from the specific value Xi
.
Yˆ
i = 0 + 2Xi = 0 + 2(12) = 0 + 24 = 24
Given the value Xi = 12, the function precisely determines the corresponding value
of Yˆ = 24. These two values plot as a single point in the coordinate plane defined by
the X and Yˆ axes as <12, 24>. A line can be plotted from only two points.
Slope
Y -intercept: The
constant value in a
linear equation.
When X = 0, the value of Yˆ = b0, where the line crosses the Yˆ -axis. Accordingly,
b0 is the Y -intercept. The slope of the plotted line is b1, the slope coefficient, which
determines the direction and steepness of the line.
slope coefficient
definition: The
multiplier coefficient
of a predictor
variable X in a linear
equation.
Regression analysis is the analysis of linear functions so the meaning of the slope
coefficient b1 is a central concept. The slope coefficient is key to explaining the11.3. MODEL ESTIMATION 221
relationships among the variables. According to its meaning first encountered in
middle school algebra, increase X by 1 unit and Yˆ changes by the value of the slope
b1, positive or negative.
slope coefficient
meaning: For each
increase in X of 1
unit, Yˆ increases by
the slope coefficient.
The larger the magnitude of the slope, the steeper the line.
A positive value indicates that as X gets larger, so does Yˆ . For a negative slope, the
relationship is inverse, as X gets larger, Yˆ get smaller. Figure 11.2 illustrates the
relation of two values of a positive slope to the value of Yˆ .
0 1 2 3
0
2
4
6
X
Y
1
2
0 1 2 3
0
2
4
6
X
Y
1
1/2
^
^
Figure 11.2: Two linear functions with different slopes, b1 = 2 (left) and b1 = 0.5 (right).
When b1 = 2, an increase in X of 1 unit, such as moving from 1 to 2 along the
horizontal axis, yields an increase of 2 for Yˆ , from 2 to 4 in Figure 11.2. However,
when b1 = 1/2, moving X from 1 to 2 only increases Yˆ from 1/2 to 1. Accordingly, a
line with a slope of 2 is much steeper than a line with a slope of 1/2.
A linear function is called “linear” because its visualization is a straight surface, no
curves. For a single X variable plotted against Yˆ the linear surface is a straight line,
as shown by the two examples in Figure 11.2.
11.3 Model Estimation regression
analysis: Estimate
the weights of a
linear function from
the data.
How does the concept of a function relate to regression analysis? The prediction
equation provided by a regression analysis is a linear function. Regression analysis
estimates the values of b0 and b1 that define this linear function. The machine learns
the values of these two linear parameters. The estimation algorithm, how the machine
learns, examines all pairs of <X,Y> data values, then chooses an optimal set of values
for these two parameters.
data point: Data
values for a single
example a row of
data in the data
table for the relevant
variables.
Each example or sample, a paired data value, plots as a point in the corresponding
visualization, the scatterplot of two variables.
visualize a
correlation of two
variables,
Section 10.2.1, p. 194
Geometrically, refer to each set of
paired data values as a data point. The machine learns the relationship between
the response variable and predictor variable by computing the sample values b0 and
b1 for the corresponding linear function. Visualize this functional relationship as a
regression line plotted through the scatterplot. regression line:
Visualization of the
linear model
estimated by a
regression analysis.
11.3.1 Analysis
Consider the annual Salary earned working at a job. What are some variables that
account for – explain – an employee’s Salary? An example follows.222 CHAPTER 11. REGRESSION ANALYSIS
Scenario. Build a model to predict annual Salary from Years employment
The more Years worked at a specific company, the larger tends to be the employee’s
annual Salary. How much does the annual Salary increase, on average, for each
Year of employment? What information regarding Salary can potential employees
t-test, Section 7.2, be given for any specified number of Years employment?
p. 128
R model
specification:
Response variable, a
tilde, then one or
more predictor
variables, separated
by + signs.
The model specification follows the same general form as the ttest() and ANOVA()
functions, special cases of linear models limited to categorical variables as predictors.
Place the response variable on the left side of the equation, followed by a tilde, ∼ ,
and then the predictor variables on the equation’s right side, with multiple predictor
variables separated by + signs, among other possibilities.
Regression()
function, lessR:
Comprehensive
regression analysis.
To instruct the machine to learn the relationship between a predictor variable (feature)
and the response variable (target), call the lessR function Regression(), abbreviated
reg(). Obtain a briefer analysis from reg_brief().
reg_brief()
function, lessR:
Specify a brief form
of the regression
output.
This example is of variables from the lessR Employee data set, read into the default
lessR data frame d.
Input. Read the Employee data set into data frame d
d <- Read("Employee")
The response variable is Salary with one predictor variable, Years.
Input. Regression analysis with one predictor variable
reg(Salary ∼ Years)
The output begins with a general summary of the variables in the model. Following
this background information, the first analysis displays the learned (estimated) values
b0 and b1, shown in Listing 11.1 in the Estimate column.
-- Estimated Model for Salary
Estimate Std Err t-value p-value Lower 95% Upper 95%
(Intercept) 42710.90 3746.75 11.399 0.000 35096.58 50325.22
Years 3249.55 342.03 9.501 0.000 2554.46 3944.64
Listing 11.1: Estimated regression coefficients and inferential statistics, hypothesis tests
and confidence intervals.
What did the machine learn? The estimated values from Listing 11.1 of the two
regression coefficients are b0 = $42710.90 and b1 = $3249.55. These learned values
specify the equation of the sample regression line. The notation Yˆ indicates the value
of the regression function for the specified value of X.
Model estimated from data: Yˆ
Salary = 42710.90 + 3249.55XY ears
Estimating this model provides for the analysis goals of prediction and understanding
relationships.
fitted value: The
value of the response
variable, Yˆi,
calculated with the
regression model for
a value Xi.
From this equation, calculate the value of Salary consistent with the
model for a given value of Years, the fitted value.11.3. MODEL ESTIMATION 223
fitted value for X=10: Yˆ
Salary = 42710.90 + 3249.55(10) = 75206.42
The fitted value Yˆ = $75206 follows from the estimated regression model given the
value of X of 10.
Express the relation of Years and Salary with the corresponding slope coefficient,
b1 = $3249.55. The meaning of the slope coefficient follows directly from the
mathematics of a linear equation. slope coefficient
meaning: Average
change in Y from a
unit change in X.
The slope coefficient is the average change in the
response variable Y for a one-unit increase in the value of the predictor variable
X. Because the data plot as a scatterplot, not as a straight line, they calculated
change in Y is the average change, not the exact change for each sample or instance.
Referencing only this particular sample, for each additional Year employed at the
company, Salary increases an average of $3249.55.
The equation of the line is the function that best summarizes the relation between
the response variable Y and predictor variable X for the estimated values of b0 and
b1. model: An equation
to explain the value
of a response variable
in terms of the values
of predictor
variables.
This equation is a model. Indicate the transformed data values from the model as
the variable Yˆ . Distinguish between the measured data values of the variable Y and
the variable that consists of the transformed data values according to the regression
line, Yˆ .
The Regression() function displays the scatterplot of the response and predictor
variables, Years and Salary, with the included regression line. The scatterplot in
Figure 11.3 is from the brief version of the output from reg_brief(). Enhanced scatterplot,
Section 11.7, p. 234
The enhanced
scatterplot from the full version of Regression() provides additional information.
5 10 15 20
60000
80000
100000
120000
Years
Salary
Figure 11.3: Scatterplot with best-fitting least-squares regression line from the brief output
form of the Regression() function, reg_brief().
Associate each value of Years employed, Xi
, with both its paired data value of Salary,
Yi
, and its corresponding fitted value of Salary, Yˆ
i
. This value of Y fit by the regression
model, Yˆ
i
, lies on the regression line. Figure 11.3 depicts the visualization of the
relationship between the measured variables X and Y and the computed variable Yˆ .224 CHAPTER 11. REGRESSION ANALYSIS
11.3.2 Standardization
Estimate the model using the data values for each of the model’s variables. Particularly
when the variables have no natural unit, such as responses to a Likert scale attitude
standardization, survey, the variables may be standardized before the model is estimated.
Section 3.3.2, p. 47
This
transformation expresses each value in terms of its distance in standard deviations
from the mean of the corresponding variable. Instead of the original measurement
new_scale scale, the unit of the standardized variable becomes the variable’s standard deviation.
parameter: Set to
"z" to standardize
the predictor
variables before
model estimation.
Standardization likely would not be applied to the Years employed and Salary data
in this section because the units of the variables in the model are meaningful and
understood, Years employment and $ in USD. When desired, however, standardize
the predictor variables simply by setting the new_scale parameter to "z". To also
scale_response standardize the response variable, set the parameter scale_response to TRUE.
parameter: Set to
TRUE to standardize
the response variable
before estimating the
model.
11.3.3 Inference for the Slope
Focus on the Population
We analyze data to understand the corresponding population relationships rather than
describe the relationships found in a single, arbitrary sample. Infer these population
values from the data values sampled from that population rather than merely describe
the sample with summary statistics. Replace each computed sample statistic of
interest in any data analysis with the estimate of the matching population value, a
range of values that likely contains the population value.
The unknown, presumably stable population values of the regression coefficients are
written as β0 and β1, the values of primary interest. In the Salary example, express
the population model that relates Years, Y, and Salary, X, as:
population model: Yˆ = β0 + β1X1
Each sample of employees yields a set of paired data values for X and Y for each
employee. Although typically only one sample is observed, the values of the estimated
regression coefficients b0 and b1 would fluctuate across hypothetical samples that
would be gathered by repeating the sampling process. These obtained sample values
would be centered around their respective population values, such as β1 for the slope
coefficient. On average, half of the sample values of b1 would be less than β1, and
half would be larger.
inferential analysis,
Section 6.3.2, 112 As previously discussed, traditional statistical inference consists of two techniques:
Hypothesis test and confidence interval. Both concepts follow from the standard error
of the corresponding statistic. standard error:
Standard deviation
of the sample value
of a statistic over
usually hypothetical
repeated samples.
A standard error, here sb, is a standard deviation,
not of the data but of a statistic, here b, over many usually hypothetical samples.
Typically only observe one sample, but the mathematicians derived the formulas to
estimate the standard error as if we had multiple samples. The estimated standard
error of a regression coefficient indicates how much the corresponding estimate would
fluctuate from sample to sample if multiple samples of the same size were taken. bootstrap alternative
to computing the
standard error,
Section 14.4.3, p. 315
A small standard error implies that any one sample slope coefficient, b1, is likely11.3. MODEL ESTIMATION 225
close to the unknown β1. Any even moderate deviation of the sample value from the
hypothesized value renders the hypothesized value unlikely.
Hypothesis Test
Evaluate the null hypothesis of no average population change in Salary for a 1-year
increase in Years employed, a population slope coefficient of 0. The alternative to the
null hypothesis is that there a change in average Salary for a 1-year increase in Years
employed.
Null Hypothesis, H0 : β1 = 0
Alternative to the Null Hypothesis, H1 : β1 6= 0
t-value, Section 6.3.3,
p.117, Section 7.2.4,
p.134
For purposes of the test, not necessarily what is believed, assume the null hypothesis
is true. If true, the b1 will likely not exactly equal 0, but it should be close. How close
is b1 likely to be to the hypothesized value of β1 = 0 if 0 is the correct population
value?
The standard error of b1 is the gauge for assessing closeness. As previously defined
for the mean and mean difference, the t-value is the number of estimated standard
errors the sample statistic lies from the corresponding hypothesized value, 0.
one-sample t-test,
Section 6.3, p. 110
The
corresponding t-value is the estimate of b1 minus 0 divided by its estimated standard
error. From the corresponding standard error, construct the associated t-test, also
reported in Listing 11.1.
tb1 =
b1 − 0
se
=
3249.55 − 0
342.03
= 9.501
IF the assumption of a population value of 0 is true, given the distribution of the
t-statistic over repeated samples, any value beyond about 2 standard errors away
from 0 is unlikely. p-value, Section 6.3.3,
p.117
Given the assumption that β1 = 0, the probability of a sample
slope coefficient more than a whopping 9 1/2 standard errors from 0 is extremely
small. The accompanying probability IF the null value of β1 = 0 is true, the p-value,
is 0.000 to three decimal digits (and well beyond).
Effect of Years on Salary: p-value = 0.000, so reject H0
Interpretation. Hypothesis test of the slope coefficient
We conclude that a change in the Years employed does, on average, lead to an
increase in Salary.
Note that the interpretation of the analysis has nothing to do with sample statistics,
which, by definition, only describe what occurred in one particular sample. The
interpretation is about what we know regarding the population as a whole.
Confidence Interval
Given that the relationship exists, β1 > 0, the natural next question is what is the
extent of the relationship? In the sample, each additional Year was worth a $3249.55
average increase in Salary. But what about the population?226 CHAPTER 11. REGRESSION ANALYSIS
regression estimates,
Section 11.1, p. 222
We cannot know the exact value of β1. Fortunately, however, we can provide an
interval that likely includes β1.
confidence
interval: Range of
plausible values at a
specified probability
that likely contains
the true population
value, such as for β1.
The confidence interval provides the estimated extent
of the average salary change in the population for the slope coefficient found in
Figure 11.1.
To construct the interval, begin with a consequence of the sampling variability of
b1, the margin of error, E, usually based on the 95% range of sampling variability.
To conceptually demonstrate that 95% range of sampling variability, approximate
the value of the t-cutoff with 2, or, better, for this sample size of n = 37 use the
exact value of 2.030 from the lessR function call prob_tcut(df=35). The value of
the t-distribution that cuts off the upper 2.5% and corresponding lower 2.5% of the
distribution, t0.25 = 2.030, leaves 95% of the distribution of b1 in between.
t-cutoff,
Section 6.3.3, p.117
From the values of t0.25 and the standard error, sb, the estimated standard deviation
of b1, obtain the margin of error for the confidence interval.
probability
interval: Range of
values of a statistic
about the true
population value,
such as for β1.
E = t0.25(sb) = 2.030(342.03) = 694.321
95% of the sample values b1 over (hypothetical) repeated sampling would fall within
about two standard errors, E, on either side of the true, but unknown, value, β1. This
range defines the probability interval of the sample slope coefficient, b1. Figure 11.4
illustrates the process for an arbitrarily obtained b1 that happens to be less than β1.
ß
Probability Interval
hypothetical
distribution
of many b's
95% of b's
b
the one
obtained b
Figure 11.4: Logic of the confidence interval based on the hypothetical distribution of b1
across repeated samples, where this one obtained b1 is less than β1.
In practice, we do not know if our one obtained b1 is smaller than or larger than
β1, but we do know we have a 95% probability of being within the margin of error
of β1. The elegance of the logic displayed in Figure 11.4 is that to construct the
confidence interval, the true value of β1 need not be known, and, in practice, is not
known. We do know that if the (probability) interval around β1 contains the sample
value b1, then that same interval around b1, the confidence interval, contains β1,
as shown in Figure 11.4. So, 95% of intervals for samples of the same size, each
constructed around a different sample value of b1, will contain β1. That is the logic of
the confidence interval and the meaning of 95% confidence for our one actual sample.
The downside of the confidence interval is that risk cannot be avoided. In practice
we only observe a single sample, and we do not know if that sample is one of those
occasional samples that does not contain β1. The confidence interval for 5% of11.4. MODEL FIT 227
the samples fails to contain the unknown population value, and we have no way of
knowing from one sample if that sample is part of that 5%.
The computer computes the confidence interval but useful to understand conceptually
how it is derived. Begin with the margin of error.
b1 ± E = 3249.55 ± 694.321 ≈ 2554.46 to 3944.64
Listing 11.1 and the above calculation reveals that there is 95% confidence that the
interval between $2554 and $3945 likely contains the true population value β1. This
range is the extent of our knowledge regarding the true impact of the number of
Years employed on Salary. Given that interval, interpret it to obtain its meaning.
Interpretation. Confidence interval of the slope coefficient
Years of employment and salary are positively related. For each additional year
worked at the company, on average, annual salary increases somewhere between
$2554 and $3945.
Note that the interpretation of the hypothesis test and the confidence interval are
entirely oriented toward the unknown population value, β1. We can describe the
relationship in the sample with b1, but the meaning of our results focuses on the
population values.
As always, the hypothesis test and the confidence interval are consistent with each
other. In this example, the hypothesis test rejects 0 as an implausible value. The
confidence interval provides the range of plausible values, which does not contain 0.
correlation is not
causation,
Section 14.2.1, p. 303
In terms of causation, again, the fact that a change in Years employed is associated
with a change in Salary does not mean that change in Years directly causes the
change in Salary. There could be common variables that correlate with both Years
and Salary that would create the true causal impact.
11.4 Model Fit
11.4.1 Residuals
Each point in the scatterplot represents a set of paired data values, Xi and Yi
.
However, the points generally do not to lie on the regression line that summarizes
the relationship between variables X and Y . The vertical distance that separates
each plotted point from the regression line is the key to understanding how regression
analysis works. This distance from the ith point on the regression line to the
corresponding value of the response variable is the ith residual, denoted ei
. residual: Given the
value Xi, the
difference between
the data value Yi and
the fitted value, Yˆi.
Residual of ith value of X, Xi: ei = Yi − Yˆ
i
For example, consider the data in the Employee data set for the 16th row of data,
the data for Laura Kralik, who has worked for the company for 10 years and earns
Y16 = $92681.19.
fitted value for X=10,
Figure 11.3.1, p. 222 As previously computed, the fitted value of Salary for 10 years of228 CHAPTER 11. REGRESSION ANALYSIS
employment is Yˆ
16 = $75206.42. From these values the 16th residual can be calculated,
that is, for i = 16.
Residual Example: e16 = Y16 − Yˆ
16 = 92681.19 − 75206.42 = 17474.77
Laura’s actual Salary in this example is $17474.77 larger than the value of Salary
fitted by the model, as shown in Figure 11.5.
5 10 15 20
60000
80000
100000
120000
Years
Salary
<75,206>
<92,681>
d a t a v a l u e for
Xi=10
fitted value for Xi=10
residual =
17,475 {
Figure 11.5: Scatterplot with data value <10, 92681>, fitted value <10, 75206>, and
associated residual, 17475, for Xi = 10.
The residual provides the information from which the machine learns the estimates of
the regression coefficients. least-squares
estimation: Choose
the regression
coefficients that
minimize the sum of
squared residuals.
The machine learns by applying least-squares estimation
for regression analysis. The estimated coefficients b0 = $42710.90 and b1 = $3249.55
are the only two numbers out of all conceivable sets of paired values that minimize
the squared residuals for that unique data set.
least-squares estimation: Choose b0 and b1 to minimize P e
2
i
By deriving values b0 and b1 from the data, the machine learned how Years of
employment relates to Salary for employees in this one data set.
Calculate the residual for each pair of data values in the data table. Then square
each residual and sum the squared values. The result is the smallest possible sum of
squared residuals. In the Regression() output this value appears in the Analysis
of Variance section of the output in the Sum Sq column, shown in Listing 11.2.
-- Analysis of Variance
df Sum Sq Mean Sq F-value p-value
Model 1 12107157290.292 12107157290.292 90.265 0.000
Residuals 34 4560399502.217 134129397.124
Salary 35 16667556792.508 476215908.357
Listing 11.2: Analysis of variance output for the regression model, which includes the sum
of the squared residuals.11.4. MODEL FIT 229
Read the sum of the squared residuals directly from the output.
Sum of squared residuals (errors), SSE: P e
2
i = 4, 560, 399, 502.217
The resulting value is considerable but the coefficient scales according to the unit of
analysis with Salary expressed in units of only $1 with the values of Salary in the tens
of thousands of dollars. Consider a transformed variable of Salary in thousands of
dollars, created by dividing all values of Salary by 1000. Analysis of that transformed
variable would produce the same relative result, but the absolute size of the values in
Listing 11.2 would be much smaller.
11.4.2 Fit Indices
Place a best-fitting line through any scatterplot. Unfortunately, the best-fitting line
is not always an appropriate line for predicting unknown values of the variable of
interest. The points in the data scatterplot might tightly cluster about the sloped line.
Or, the best-fitting line that minimizes the sum of squared errors, SSE, could indicate
a poor fit, with the scatterplot points spread widely about the line, displaying large
residuals. “Best” is not necessarily “acceptable”.
Residual Variability
One reason the SSE does not directly serve as an indicator of fit is that it does not
compare across different analyses on different data. The reason is basic: The more
rows of data, which is desired, the higher the SSE for any one analysis. Instead,
compute their standard deviation by taking the mean and then the square root to
relate to normal curve probabilities.
se =
s P e
2
i
n − k
The degrees of freedom is n − k, where k is the number of estimated coefficients, here
two.
degrees of freedom,
Estimate b0 and b1, then make a second pass through the same data with those Section 6.2.1, p. 103
estimates to compute the SSE. Knowing n − 2 data values, and knowing b0 and b1,
determines the remaining two data values, not free to vary.
This standard deviation of the residuals, se, directly indicates the amount of scatter
about the regression line, the fit of the model to the data. A small standard deviation
is an indicator of a good fit with not much scatter about the regression line. A large
value is not so good. This standard deviation of residuals is $11,581 as reported
in the Model Fit section of the Regression() function, shown in Listing 11.3.
The se provides information regarding how close the data values are to the corre￾sponding values fitted by the model. Assuming the residuals are normally distributed,
a range of two standard deviations on either side of their mean, which is zero, contains
about 95% of the values of the distribution. The Regression() function also reports
the size of this 95% range of residual variation. Most of the values about the
regression line vary across the considerable span of over $47000.230 CHAPTER 11. REGRESSION ANALYSIS
-- Model Fit
Standard deviation of Salary: 21,799.53
Standard deviation of residuals: 11,581.42 for 34 degrees of freedom
95% range of residual variation: 47,072.57 = 2 * (2.032 * 11,581.42)
R-squared: 0.726 Adjusted R-squared: 0.718 PRESS R-squared: 0.681
Null hypothesis of all 0 population slope coefficients:
F-statistic: 90.265 df: 1 and 34 p-value: 0.000
Listing 11.3: Fit indices reported in the Model Fit section of the Regression() output.
R2
The other primary fit statistic for regression analysis is R2
, 0.726 from Listing 11.3. R2
compares the scatter about the regression for two different models, the current model
null model: compared to the null model.
Regression model
with no predictor
variables.
The null model is without X or any other variable as a
predictor variable. Fortunately, prediction is still possible even without a predictor
variable. If no other information regarding the variable Y is available from which to
predict, then predict its mean. For the null model, for all values of X: Yˆ = mY .
The null regression line is illustrated in Figure 11.6. Use Plot() to visualize the
scatterplot of Years and Salary. Set the "null" line for the fit parameter and set
plot_errors to TRUE to visualize the residuals. Compare Figure 11.5 with Figure 11.6
to view how much the actual model reduces the size of the residuals compared to the
null model.
Plot(Years, Salary, fit="null", plot_errors=TRUE)
Figure 11.6: Scatterplot with the null regression line and highlighted residuals.
How much better does the proposed model fit with X than the null model’s baseline
fit? R2 assesses how much having X in the model reduces the scatter about the fitted
values Yˆ . Compare this scatter to the scatter that results without X, the scatter
about mY as the sum of squared residuals about mY . Read the sum of the squared11.4. MODEL FIT 231
residuals directly from the output shown in Listing 11.2, where Y is Salary in this
example.
Sum of squared residuals about mY , SSY: P (Yi − mY )
2 = 16667556792.508
The sum of squared residuals about mY describes the total variability of Y, the basis
for the variance and its square root, the standard deviation, sY . So also refer to R2
as the percent of variance accounted for in the response variable, Y, by the predictor
variable.
Assess variation as the sum of squares about a regression line. SSE indicates the
corresponding size of the squared residuals about the line for the model of interest.
In contrast, SSY indicates variation about the mean of Y, the fitted values for the
null model. Define R2 as the ratio of the two sums of squares, subtracted from 1 so
that higher values indicate an improved fit.
R
2 = 1 −
SSE
SSY = 1 −
Σ(Yi − Yˆ
i)
2
Σ(Yi − mY )
2
To visualize this expression for R2
, compare the scatter about the regression line in
Figures 11.3 or 11.5 with the scatter about the mean in Figure 11.6.
The value of R2 varies from 0 to 1. A value of 0 indicates that the least-squares
regression line of interest does not reduce the sum of the squared errors from the value
obtained with the null model. R2 = 0 indicates that features X contribute nothing to
the fit of the model beyond the baseline set by the null model, so SSE = SSY.
Worst fit: R
2 = 1 −
SSY
SSY = 1 − 1 = 0
On the contrary, a value of R2 = 1 indicates a perfectly fitting model. For a perfect
fit the value of SSE is 0.
Best fit: R
2 = 1 −
SSE
SSY = 1 − 0 = 1
Values of R2 are generally considered reasonably high if above 0.5 or 0.6. Many
published analyses achieve only an R2
in the 0.3 range or somewhat below, so 0.3 is
considered an acceptable fit in some situations. The corresponding hypothesis test
is of the null hypothesis, H0, that the population R2
is zero. This value is usually
significant, as is the case in this example.
Test of population R2 = 0: p-value < α = 0.05, so reject H0
The sample R2 = 0.726 is large. Further, reject the null hypothesis of a zero value.
Using Years employed to predict Salary does a much better job than using Salary by
itself, using mean Salary as the predicted value for all values of X.
Unfortunately, R2 necessarily increases as the number of predictor variables increases,
regardless of the contribution of a new predictor to the model. This increase is more
pronounced the smaller the sample. This bias increases to the extent that if the
number of predictors equals the sample size then always R2 = 1.0.232 CHAPTER 11. REGRESSION ANALYSIS
To compensate for this sample size effect, the adjusted version of R2
, R2
adj , adjusts for
the sample size compared to the number of predictor variables. To adjust, divide each
of the two sums of squares in the definition of the statistic by their corresponding
degrees of freedom. The result is that R2
adj provides a downward adjustment and a
more realistic assessment of the comparison of the proposed model to the null model.
In very small samples the value of R2
adj may be considerably reduced from the value
of R2
. In larger samples R2
adj will still be smaller, but usually not much smaller.
R
2
P RESS,
Section 12.5.2, p. 261 Find yet another and more helpful R2
type statistic on the Regression() output
called R2
P RESS. That statistic is discussed in a later section.
11.5 Prediction Intervals
11.5.1 Prediction Error
One of the two primary purposes of regression analysis is to calculate the predicted
value, Yˆ
i
, for the response variable or target Y from a value of the predictor variable
or feature Xi
.
fitted value
calculation,
Section 11.3.1, p. 222 In general, Yˆ
i
is the value fit by the model for a given value of Xi
.
Plug the value of Xi
into the estimated model, and out pops Yˆ
i
.
training data,
Section 11.6.3, p. 240
However, when Yˆ
i
is calculated from the data from which the model was estimated, the value of the
response variable, Yi
, is already known, so there is nothing to predict. Only when
calculated from new data is the fitted value, Yˆ
i
, a true predicted value. The true
value, Yi
, will not be known until some later time when the accuracy of the prediction
can be assessed.
As is true of any statistical result, including a predicted value, the presence of error
confounds the result. Unfortunately, two forms of error underlie a prediction. First,
as discussed in the previous section, consider the training data, the data on which the
model trained, the original data. Define the residual from the model for any given
value of the predictor variable, Xi
, the distance from the fitted value of the response
variable Y from its actual value, Yi − Yˆ
i
.
training error:
The residual, the
difference of the
corresponding actual
value and fitted value
from the original
data.
These residuals, the distance to the line from each plotted data value scattered about
the regression line, indicate a lack of model fit. The model does not account for all
variation in the response variable, even on the data in which it trained. The residuals
from the training sample are training errors. The residuals from a true prediction
are prediction errors. Prediction errors are calculated the same as training errors, the
difference from the actual and fitted values, but they are conceptually distinct: Same
computation, different meaning.
prediction error:
Difference between a
prediction on new
data and the actual
value later obtained.
Statistical estimates, such as the parameters b0 and b1 of the regression line, necessarily
involve sampling error, the random variation of a statistical estimate from sample to
sampling error, sample.
Section 6.3.2, p. 112
For each new sample of paired data values for the response and predictor
variables, obtain a different set of estimates for the regression coefficients b0 and b1,
a different regression line. As with any sample estimate, regression lines vary from
sample to sample.11.5. PREDICTION INTERVALS 233
Consider any single value of the predictor variable, the generic ith value, Xi
. If a
regression line randomly fluctuates from usually hypothetical sample to sample, then
so does the corresponding point on the line, its fitted value, Yˆ
i
. For each new sample,
given a value, Xi
, obtain a different fitted value, Yˆ
i
. Express this sampling variation
of Yˆ on the regression line for a given value of Xi as the standard deviation, sYˆ
i
,
across the hypothetical random samples from the same population. 95% confidence
interval of the
expected mean of Y
for a given value of
X, Xi: Range of
values that with 95%
confidence contains
the expected mean,
the point on the
regression line, Yˆ .
The 95% range of
fluctuation of these fitted values, Yˆ
i
, defines the 95% confidence interval of the point
on the regression line, the fitted value.
The standard deviation of the residuals, se, summarizes the uncertainty imposed by
training error, the fit of the model to the one sample of data from which the model
was estimated. Unfortunately, the regression model was not optimized on new data
beyond the original sample for which to make predictions. For the corresponding
point on the regression line for Xi
, the standard deviation of prediction for a given Xi
,
spredi
, cumulates both the training error, se, and the sampling error from applying
the model to new data, sYˆ
i
. Following is the equation for spredi
.
spredi =
q
s
2
e + s
2
Yˆ
i
The standard deviation of prediction for Xi
, spredi
, is necessarily larger than the
standard deviation of the residuals, se. Large samples mitigate the influence of
sampling error. The smaller the sample, the more pronounced the effect of the
sampling error on the size of the prediction interval for a single example or row of
data, the ith row.
95% prediction
interval: Range of
values that with 95%
confidence contains
the predicted value.
To apply the concept of prediction error, obtain the prediction interval for a given
predicted value. Begin with the margin of error for a 95% interval, based on the t.025
cutoff that defines the tails of the t-distribution, a number close to 2.
Margin of prediction error: Ei = (t.025)(spredi
)
There is a 95% confidence that the actual value of Y later obtained will be contained
within the prediction interval ±Ei on either side of the fitted value, Yˆ
i
.
95% prediction interval for ith row of data: Yˆ
i ± Ei
Unlike the standard deviation of the residuals, se, the size of the prediction intervals
is not the same for all values of predictor variable X. The closer the value of X to its
mean, the smaller the interval. Because the regression line fluctuates across samples,
values at the extremes of the line vary more than do values in its middle, similar to a
teeter-totter where sitting on the end provides more up and down motion than sitting
further inward. As the sample regression line would randomly differ from sample
to sample, the values of Yˆ would vary more the further away from the mean of X.
Figure 11.7 illustrates this “teeter-totter” effect.
This scatterplot, in Figure 11.7, contains the regression line, and the confidence
intervals that reflect the variability of the regression line. The two wider, slightly
curved lines define the prediction intervals, the lower and upper bound of the interval
for each value of the predictor variable, Years.234 CHAPTER 11. REGRESSION ANALYSIS
Figure 11.7: Default Regression() scatterplot of Salary from Years experience plus the
regression line, the prediction intervals of Salary (light gray), and the confidence
intervals of the fitted value (dark gray).
11.5.2 Predict from Existing Data
Corresponding to the visualization in Figure 11.7, Regression() displays the pre￾dicted value on the regression line accompanied by the standard error of prediction,
spredi
, and the corresponding lower and upper bounds of the intervals, as shown in
Listing 11.4.
digits_d parameter,
Section 4.3.2, p. 76 To save space, set digits_d=2 in the function call to Regression().
-- Data, Predicted, Standard Error of Prediction, 95% Prediction Intervals
Years Salary pred s_pred pi.lwr pi.upr width
Hamide, Bita 1 51036.85 45960.45 12086.68 21397.36 70523.54 49126.18
Singh, Niral 2 61055.44 49210.00 12010.08 24802.58 73617.43 48814.85
Korhalkar, Jessica 2 72502.50 49210.00 12010.08 24802.58 73617.43 48814.85
...
Saechao, Suzanne 8 55545.25 68707.32 11750.78 44826.85 92587.78 47760.92
Tian, Fang 9 71084.02 71956.87 11741.93 48094.40 95819.34 47724.94
Stanley, Grayson 9 69624.87 71956.87 11741.93 48094.40 95819.34 47724.94
...
Correll, Trevon 21 134419.23 110951.50 12394.63 85762.58 136140.42 50377.84
Capelle, Adam 24 108138.43 120700.15 12760.47 94767.76 146632.55 51864.80
Listing 11.4: Predicted value in the pred column and associated prediction interval in the
pi.lwr and pi.upr columns.
By default, Regression() sorts the intervals from the smallest lower bound of the
prediction intervals to the largest lower bound. To avoid voluminous output, only
representative prediction intervals are provided, intervals for the lowest values of
the lower bound of the interval, middle values of the lower bound, and intervals
for the largest values.
pred_rows
parameter: Number
of displayed
prediction intervals
for first, middle, and
last intervals, or set
to "all".
Prediction intervals for all rows of data are displayed for a
sufficiently small sample, less than 25, or if the pred_rows parameter is set to "all".
Set parameter pred_sort to "off" to not sort the rows. pred_sort="off"
parameter: Do not
sort rows of data by
the prediction
interval lower bounds.
The confidence intervals in Listing 11.4, displayed in the darker shade of gray, reflect
the sampling error, the variation of the corresponding point on the regression line11.5. PREDICTION INTERVALS 235
from sample to sample. Larger sampling errors of the regression line contribute to
larger prediction errors of individual data values. Assuming normality, the 95% range
of the residuals provide the extent of the training error, a value already reported as
$47073 in Listing 11.3. Because the prediction intervals reflect both modeling and
sampling error, they are larger than the corresponding 95% range of the residuals.
From Figure 11.7, the smallest prediction interval is $47725 wide, from $38094 to
$85819 for 9 years of employment. The largest prediction interval is $51865 wide,
from $84768 to $136633 for 24 years of employment.
Precise prediction is not easy to achieve, often obtaining wider prediction intervals
than desired. A larger sample does reduce the impact of the sampling error, reducing
the size of the intervals. multiple regression,
Section 12, p. 250
Still, the effect of the training error can only diminish by
improving the model, such as adding new predictor variables.
11.5.3 Predict from New Data
The prediction intervals provided by the Regression() function are for each row of
the data table, the existing values of the predictor and response variable. As noted,
prediction occurs from new predictor variable values, which generally do not equal
the existing values. We need to obtain prediction intervals for specified new values.
Scenario. Obtain predictions for new values of the predictor variable
The data include employees who have worked at the company for each of 1 to 10
years. However, the interval from 10 to 16 years contains gaps, such as for 12 and
16 years. To provide to a prospective employee, generate a list of predictions for all
integer values of Years worked from 10 to 16.
The Regression() function provides the parameter X1_new for listing specified values
of the first (and here only) predictor variable to obtain a prediction of the response
variable and associated interval.
X1_new parameter:
Obtain predictions
for specified values of
the (one) predictor
variable.
The X1 refers to the first predictor variable, the
only predictor variable in this analysis. Specify the range of values of the predictor
variable as 10:16.
c() function,
Section 1.2.9, p. 12
Or invoke the c() function to specify a more customized list of
variables.
Find the resulting predictions and associated 95% prediction intervals in Listing 11.5.
Regression(Salary ∼ Years, X1_new=10:16)
Years Salary pred s_pred pi.lwr pi.upr width
10 75206.42 11743.04 51341.70 99071.14 47729.44
11 78455.97 11754.10 54568.77 102343.18 47774.41
12 81705.53 11775.09 57775.66 105635.39 47859.73
13 84955.08 11805.96 60962.48 108947.68 47985.20
14 88204.63 11846.63 64129.39 112279.87 48150.49
15 91454.18 11896.99 67276.59 115631.78 48355.19
16 94703.74 11956.93 70404.33 119003.15 48598.82
Listing 11.5: Specified predictions and prediction intervals for Salary for values of Years
employed from 10 to 16.236 CHAPTER 11. REGRESSION ANALYSIS
Apart from the prediction intervals output, the remainder of the Regression() output
is identical to that obtained without the X1_new parameter. The prediction intervals
section analyzes the new, specified values of the predictor variables. The value for
the response variable in this section is blank because it is not yet known.
11.6 Outliers and Diagnostics
Before accepting a model as a valid portrayal of the relationships among the variables,
first examine some conditions and assumptions. One such consideration is to identify
outliers. For a single variable, an outlier is the value of the variables far from most of
the other data values. Identifying and then understanding why an example, a row of
data, is an outlier in a regression analysis is needed to both estimate a valid model
and perhaps also to obtain information for how to improve the model.
outlier, Section 5.3.1,
p. 90
Outliers are interesting data points that should be explored further to understand why
they occurred and why the model failed to adequately account for their data values.
In some situations the explanation is simple and useful: a data entry error that is
identified and can be fixed. Other situations require a more thorough understanding
of how the outlier became part of the data set for analysis.
11.6.1 Bivariate Outliers
bivariate outlier:
An outlier with
respect to the
distribution of paired
data values.
Define an bivariate outlier in terms of both variables in the model. The bivariate
outlier lies outside the pattern of most of the points in the two-dimensional scatterplot.
This patterning is an ellipse for two normally distributed variables.
One essential reason to identify outliers is purely statistical. Outliers exert a dispro￾portionate influence on model estimation, the estimated values of b0 and b1. These
estimated values are chosen to minimize the sum of the squared errors or residuals.
Yet the outliers have the largest residuals, and, necessarily then, the largest squared
residuals. The larger the squared residual for a data point, the larger the influence
on the minimization of the squared residuals for all the data.
We create an outlier to demonstrate the effect of outliers on the estimated model
Employee data table, coefficients, b0 and b1.
Figure 1.5, p. 13
Estimate the following regression model for the 37 employees
in the Employee data set to explain Salary in terms of Years of employment.
model estimated from data: Yˆ
Salary = 32710.90 + 3249.55XY ears
Now change the value of Salary for just one person, Trevon Correll, the person who
edit a data value, has worked the longest at the company, 21 years, and has the highest Salary.
Section 3.7.1, p. 56
Input. Set value at specified row and column of data frame d
d["Correll, Trevon", "Salary"] <- 40000
How do the estimated coefficients change if that one Salary is lowered from $124,419.23
to $40,000? To visually explore with Plot(), both with the original regression line11.6. OUTLIERS AND DIAGNOSTICS 237
and with the outlier removed, specify a value of MD_cut of 7 to isolate all but the
largest outlier.
Mahalanobis
distance: Distance
of a point from the
center of the
distribution given
correlated variables
and unequal
standard deviations.
MD is an abbreviation for Mahalanobis distance, which assesses the
distance of a point from the center of the distribution. The value of 7 specifies
to define a point as an outlier that lies well outside of the 95% confidence ellipse
defined by a multivariate normal distribution.
95% confidence
ellipse,
Section 10.2.3, p. 198
The resulting scatterplot appears in
Figure 11.8, and also labels any outlier with its row name.
Plot(Years, Salary, fit="lm", MD_cut=7)
Figure 11.8: Regression line with the created outlier removed (dashed) compared to the
original regression line with all the points (solid).
The result of changing the value of one Salary is a decrease in the estimated slope
coefficient of $854.91. The shift in one data value decreased the impact of each
additional Year on Salary from an average of about $3250 down to $2395.
In practice, an estimated regression model may not sufficiently reflect an underlying
process if an outlier generated by a different process is included in the analysis. For
instance, Trevon Correll from the revised Employee data set worked at the company
for 21 years and yet makes much less money than others with similar Years experience.
Perhaps Trevon may be the only part-time employee in the analysis, so Trevon’s
Salary is dramatically smaller than the value fitted by the model. Always try to
understand what process lead to the deviation of the data values of an outlier.
If a distinguishing aspect of an outlier data point can be identified, deleting a data
point from the data set is a valid, appropriate response with the qualification that the
resulting model does not generalize to that condition. For example, dropping Trevon
from the data set for predicting Salary would limit the generalization of the resulting
model to full time employees. The mechanisms for determining salary for part-time
and full-time employees vary markedly and likely should be modeled separately. Or,
perhaps further experimentation would be beneficial by including additional variables
in the model such as the number of hours worked per week. The optimal response
to the identification of an outlier moves beyond statistics to a consideration of the
underlying dynamics of the how the outlier values were created.238 CHAPTER 11. REGRESSION ANALYSIS
11.6.2 Case-Deletion Statistics
Some data values for the variables in the model impact the estimation of the resulting
model more than other data values. These data values are typically outliers. To
assess these differential impacts, identify the most impactful data points.
influence, Effect of
a specific set of the
values of the
predictor variables
and the response
variable on the
estimated model.
A data
point with disproportionate influence, an influential data point, should always be
inspected, and its role in the analysis should be assessed.
An influential data point might be a data coding or entry error that would have
affected the entire analysis if it had remained. Or, the data could be entered correctly,
but a process distinct from the process that generated the remaining data values may
havee generated the outlier. Another possibility is that an influential data point may
simply indicate an unlikely event from the same process as all the data values. For
example, flip a fair coin 10 times and obtain 8 heads.
For diagnostic purposes, various influence indicators are available (Belsley, Kuh, &
Welsch, 1980). A substantial residual indicates the presence of an influential data
point. How to define “large”? Standardize the residual so that, assuming normality,
values greater than 2.5 or 3 indicate a potential, influential data point. Unfortunately,
residual standardization with an influential data point is more problematic because
that data contributed to model fit. If a data point is influential, then the regression
estimates must be adjusted more than usual to achieve this reduction.
case-deletion
statistic: A statistic
for a data point
calculated without
the point in the
analysis.
To remedy this problem of an influential point contributing to the estimation of
the model, and so lowering the size of its residual as part of the minimization of
least-squares of the residuals, consider case-deletion statistics. To avoid confounding
the residual with the data point that helped minimize that residual, delete the data
point and then re-compute the regression. Then calculate the residual of the deleted
data point from this new model. The result is an influence statistic based on a
residual for a data point that does not contribute to the estimation of the model.
Fortunately, formulas exist to make these adjustments without an actual physical
Studentized re-computation of the entire model.
residual, A
standardized residual
calculated from the
model estimated with
the corresponding
data point deleted
from the data.
The standardization that adjusts for the value of the predictor with case-deletion is
the Studentized residual. This version of the residual has the additional advantage of
following the t-distribution with degrees of freedom of n−k−1, where n is the sample
size and k is the number of predictor variables in the model. The t-distribution
provides a standard for evaluating the size of the Studentized residual. Except in
very small samples, regardless of the original scale of measurement of the response
variable Y, values larger than 2 or smaller than −2 should be infrequent, and values
larger than 2.5 or 3 or smaller than −2.5 or −3 should be rare.
Other case-deletion statistics directly assess the influence of a data point on the
estimated regression model. One index, Cook’s Distance or D, summarizes the overall
influence of a data point on all the estimated regression coefficients.
Cook’s distance,
D, Summary of the
distance between the
regression coefficients
calculated with a
specific data point
included and then
deleted.
Data points with
larger D values than the rest of the data are those that have unusual influence.
How to identify an unreasonably large value of Di for the ith example, row of data?
One suggestion is any Di > 1 likely has a disproportionate influence, but there is no11.6. OUTLIERS AND DIAGNOSTICS 239
consensus regarding a specific cut-off value. Two of the better techniques compare a
Di (a) to other influence indices, and (b) to other values of D. For example, large
values of the Studentized residual and D provide more convincing evidence of an
influential example than just D alone. Additional evidence for influence is if one or
a few examples have a very large values of D compared to the next largest values.
Also, smaller data sets provide more opportunity for examples with larger residuals
to have a correspondingly greater impact on the estimation of the model.
The influence of a data point can also be assessed according to its impact on the
fitted value.
DFFITS: Scaled
change in the fitted
value for a specific
data point when the
point is deleted.
DFFITS represents the number of standard errors that the fitted value
for a data point shifts when it is not present in the sample of data used to estimate
the model. Large values of DFFITS indicate influential data points. A general cutoff
to consider is 2, or, a recommended size-adjusted cutoff is 2
p
(k + 1)/n. However,
perhaps a more helpful approach isolates those data points with large DFFITS values
relative to most other data points. As with all influential points, the analyst should
attempt an understanding of why these data points are so influential.
Regression() labels these three case-deletion influence indices as rstudent, dffits,
and cooks. Only those data points with relatively large values on these indices are of
interest, so to conserve space, these indices by default are listed only for the 20 data
points with the largest value of Cook’s Distance. Listed for each such row of data are
the row name, the data values, the fitted value, the residual, and the three indices,
all sorted by Cook’s distance, as shown in Listing 11.6.
-- Data, Fitted, Residual, Studentized Residual, Dffits, Cook’s Distance
Years Salary fitted resid rstdnt dffits cooks
Correll, Trevon 21 134419.230 110951.498 23467.732 2.330 0.961 0.409
Capelle, Adam 24 108138.430 120700.155 -12561.725 -1.233 -0.643 0.204
James, Leslie 18 122563.380 101202.840 21360.540 2.022 0.645 0.191
Korhalkar, Jessica 2 72502.500 49210.002 23292.498 2.208 0.630 0.178
Hoang, Binh 15 111074.860 91454.183 19620.677 1.799 0.435 0.089
Billing, Susan 4 72675.260 55709.107 16966.153 1.535 0.364 0.064
Listing 11.6: Residuals and influence indices sorted by Cook’s Distance.
Trevon Correll appears to be an influential example. According to the residual, Trevon
earns a Salary $23467.73 larger than the value fitted from the model, $110951.50.
Trevon has the largest Cook’s Distance, DT C = 0.409, which is more than twice as
high as the next highest value of 0.204. This example also has the highest Studentized
residual of 2.330 and the largest DFFITS of 0.961. Further examination beyond the
regression analysis may account for this considerably larger Salary than is accounted
for by the model. Worth investigating is if the data are correct. If so, what additional
information could explain the salary larger than explained by the model? If discovered,
perhaps this additional information could be incorporated into a revised model. res_rows
parameter: The
number of rows of
data to be displayed
for the residuals
analysis.
The default settings that control the display of the rows of data and other values can
be modified. The res_rows parameter can change the default of 20 rows displayed
to any value up to the number of rows of data, specified by the value of "all". To
turn this parameter off, specify a value of 0. The res_sort parameter can change240 CHAPTER 11. REGRESSION ANALYSIS
the sort criterion from the default value of "cooks". Other values are "rstudent",
"dffits", and "off" to leave the rows of data in their original order.
res_sort parameter:
The sort criterion for
the residuals analysis.
11.6.3 Predictive Residuals
se, Section 11.4.2,
p. 229
Regrettably, the size of the R2 fit statistic only reflects the reduction in the sum
of squared residuals over the null model for the model estimated from the data on
which the model trained. R2
, as well as the related fit indices, the standard deviation
of residuals, se, and the version of R2 adjusted for sample size, R2
adj , only describe
sample properties.
training data:
Sample of data from
which the regression
model is estimated. These indices do not indicate model performance in new samples
because there is no accounting for sampling error. A high R2 or R2
adj , or a low se, is
desirable but insufficient to indicate a useful model for prediction. R
2
adj , Section 11.4.2,
p. 232
R
2
, Section 11.4.2,
p. 230
How can R2 be modified to apply to new data when only the training data is available?
To generalize to new data, calculate the usual R2
statistic as a case-deletion statistic.
Compute the residual for each row of data from a model estimated without that
specific row, the predictive residual.
predictive
residual: Residual
calculated for a row
of data from a model
estimated without
the row of data.
The predictive residuals represent residuals from
true prediction. The fitted value from which each residual is calculated is from a
model with no knowledge of the value to be fitted, a true predicted value.
R
2
P RESS:
Case-deletion
statistic computed as
R
2 with predictive
residuals instead of
all the training data
residuals.
Apply the same form of the R2
equation to these predictive residuals separately
computed for each row. The corresponding R2
statistic calculated with the predictive
residuals is PRESS R2
, for PRedictive Sum of Squares.
R
2
P RESS = 1 −
SSP RE
SSY
The expression SSP RE replaces SSE, the traditional sum of squares about the regres￾sion line, in the expression for R2
.
The more conservative R2
P RESS describes fit as the reduction in the sum of squared
residuals from the null model for predicting to new (e.g., testing) data. overfitting,
Section 12.5.2, p. 261
To evaluate
overfitting, fit that is too good, compare the predictive version R2
P RESS to the
descriptive statistic R2
that applies only to the training data. If R2
P RESS is much
lower, then the model overfits the training data, taking advantage of chance, random
fluctuations of the data that contribute to fit that do not generalize to new data.
To illustrate, return to Listing 11.3 that contains various indices for model fit output
by Regression() for regressing Salary on Years employed. Listing 11.7 reproduces
just the line of information regarding the family of R2
statistics.
R-squared: 0.726 Adjusted R-squared: 0.718 PRESS R-squared: 0.681
Listing 11.7: R2 fit indices.
The model’s fit to the training data is high, R2 = 0.726. As usual, adjusting for
the sample size effect yields a somewhat lower value, R2
adj = 0.718. Again, as usual,
R2
P RESS = 0.681 is the lowest of the thee indicators. This lower value indicates some
overfitting, so a generalization to new data will not quite obtain the same level of11.7. MODEL ASSUMPTIONS 241
fit as with the original, training data. Still, in this analysis, the drop from R2
to
R2
P RESS is reasonably small, 0.045, and, in absolute terms, R2
P RESS is quite high
with a value over 0.6. The model of explaining variation of Salary depending on Years
employed fits the data reasonably well.
In summary, the evaluation of model fit requires testing data, data beyond the
data from which the model was estimated. Case-deletion statistics provide a means
by which to simulate testing data with only the training data available. As such,
case-deletion statistics become a desirable component of a regression analysis.
11.7 Model Assumptions
As with any statistical procedure, the validity of a regression analysis requires
satisfying its underlying assumptions.
11.7.1 Properties of the Residuals
The assumptions of a regression analysis focus on the properties of the residuals, which
ideally only reflect random error. Any systematic content of the residual variable
violates one or more of the assumptions. If so, the model is too simple, so explicitly
revise the model to account for this systematic information instead of relegating it to
the error term. multiple regression,
Chapter 12, p. 250
Often this correction includes adding one or more predictor variables
(multiple regression), accounting for a nonlinear relationship, or using an estimation
procedure other than least-squares.
Figure 11.9: Scatterplot of fitted values and residuals.
Assumption 1. For each fit￾ted value, Yˆ
i
, the average
residual value should be zero.
The residuals should be ran￾domly distributed above and be￾low each fitted value, approxi￾mately evenly distributed across
positive and negative values.
Regression() provides a resid￾ual scatterplot with the fitted
values to assist in evaluating this
assumption. This visualization
in Figure 11.9 contains a dotted
horizontal line drawn through
the origin to facilitate this com￾parison. If the residuals for in￾dividual values of Yˆ
i are not evenly balanced about the horizontal zero line, the
relationship between response and predictor variables is likely not linear.
Assumption 2. Another assumption of least-squares regression is a constant popu￾lation standard deviation of the estimation errors at all values of X, se. The value
of se should apply to the variability of Y about the regression line for each value of242 CHAPTER 11. REGRESSION ANALYSIS
X. The value of Y should be no more or less difficult to predict for different values
of X. Any difference in the standard deviation of residuals for different values of X
should be attributable only to sampling error.
heteroscedasticity:
Standard deviation
of residuals differs
depending on the
value of the predictor
variable.
The violation of this equal variances
assumption is heteroscedasticity.
Figure 11.10: Display of severe heterogeneity.
The residuals should span about the same distance for each value of Yˆ . Often
the pattern exhibited by heteroscedasticity is a gradually increasing or decreasing
variability as X gets larger or smaller. Figure 11.10 also provides the basis of an
informal evaluation of this equal variances assumption, here with increasing variability
as the value of X increases.
When heteroscedasticity occurs, the estimated regression coefficients are accurately
estimated (not biased). However, without a constant value of se across the values
of X, the corresponding standard errors of the regression coefficients and associated
confidence intervals are incorrect.
Assumption 3. The residuals do not correlate with any other variable, including each
other. The correlation of successive residuals typically applies to the analysis of
time-oriented data in which each predictor variable in the regression equation is an
earlier time period. The regression of the one time period on the values of previous
time periods is called a time series regression. Usually name the time variable t.
Yˆ
t+1 = btYt + bt−1Yt−1 + bt−2Yt−2 + . . .
Yˆ
t+1 is the prediction for the next time period, Yt
is the current value, and, Yt−1 is
the value from the last time period.
time-series
regression: Express
the target variable as
a function of its
values at earlier time
points.
Each time period has its own residual, how far the prediction for that time period’s
value departed from the actual value.
autocorrelation:
Successive residuals
over time correlate.
The issue is that time series regressions often
have correlated adjacent residuals, a condition called autocorrelation. “Adjacent” can
refer to immediately adjacent or lagged over multiple time periods.
The reason for autocorrelation in time series data is that successive values may follow
a specific pattern, such as gradually increasing or gradually decreasing. In data such
as these, if one predicted value of Y is an underestimate, the next time value is11.7. MODEL ASSUMPTIONS 243
also likely to yield an underestimate. Knowledge of the sign of one residual yields
predictive information about the sign of the next residual, indicating autocorrelation.
For example, sales of swimwear peaks in Spring and Summer and decreases in Fall and
Winter. The residuals around a regression line over time would reflect this seasonality,
systematically decreasing and increasing depending on the time of year. Analysis
of time-oriented data typically requires more sophisticated procedures than simply
fitting a regression line to the data, procedures not covered here. The consideration
of time-oriented data involves many complexities that require a detailed consideration
beyond the information provided here.
Assumption 4: The estimation errors are normally distributed. This assumption is
not needed for the estimation procedure but is required for the hypothesis tests and
confidence intervals of the regression coefficients. When the assumption is violated,
the estimates are not biased, but their standard errors are not correctly estimated.
Regression() provides a density plot and histogram of the residuals to facilitate this
evaluation, which appears in Figure 11.11.
Figure 11.11: Distribution of the residuals.
The general density curve and the best-fitting normal are plotted over the histogram.
The residuals appear to be at least approximately normal, satisfying the assumption.
11.7.2 Curvilinear Relationships
Regression analysis requires the straight-line relationships of linearity. A non-linear
relationship is curvilinear. How to use linear regression to analyze a curvilinear
relation between predictor or feature X and response or target Y?
Curvilinear Examples
Fortunately, we have a strategy to cope with a non-linear relationship. To explore
non-linear relationships, Plot() provides the fit parameter to fit various non-linear
functions to the scatterplot. First, identify the type of non-linearity. Figure 11.12
depicts often encountered non-linear relations that can be fit with Plot().244 CHAPTER 11. REGRESSION ANALYSIS
Figure 11.12: Typical non-linear functional relationships.
In addition to the visualization, Plot() provides the mean squared error, MSE, for
the specified value of fit to aid comparing different functions.
Relationships fit to the data include "lm" for linear, "null" for the null model, the
mean of Y. fit="null" example,
Section 11.4.2, p. 230 Transformations of Y are provided for
y"exp" for exponential growth and decay, "log" for logarithmic, and "quad" for
quadratic increasing or decreasing. The logarithmic transformations can also be
applied to the predictor variable X instead of Y with value "xlog" and to both
X and Y with "xylog". The "loess" option fits the non-linear loess curve to the
data unconstrained by a functional relationship. fit="loess"
example,
Section 10.2, p. 195
To visualize the training errors, the
residuals, set the plot_errors parameter to TRUE.
Another possible value of fit is "power" for the generalization of the quadratic
relationship to any power function, accompanied by the fit_power parameter to
specify the power. Setting fit_power to 2 is equivalent to specifying "quad" for
the value of "fit". As evident from Figure 11.12, the quadratic function represents
non-linear growth, but at a lower rate than the explosive growth of the exponential
function. Choosing values of fit_power greater than 2 can allow for faster growth
than quadratic, but slower than exponential. Choosing values less than 1 can result
in a shape that more resembles the logarithmic function.
Figure 11.13 illustrates with a scatterplot of a quadratic relationship between variables
X and Y with an attempted linear fit.
Plot(X,Y, fit="lm")
reg(Y ∼ X)
Figure 11.13: Linear fit to a quadratic relationship.
The first visualization (left) fits a linear model to the data. The second plot (right) is11.7. MODEL ASSUMPTIONS 245
from Regression() that analyzes that linear relationship, showing the plot of the
residuals against the fitted values. Assumption 1,
Section 11.7.1, p. 241
Clearly, Assumption 1 is violated, the assumption
that the residuals should be randomly distributed above and below the fitted value.
Instead, the residuals are structured. The first set of residuals are positive, the middle
set are negative, and then, again, the final set are positive.
Curvilinear to Linear and Back
Address non-linearity in linear regression analysis with the following steps.
1. View the scatterplot to identify the type of non-linearity.
2. Transform the non-linear relationship to linear.
3. Analyze the linear relationship.
4. Transform the model back into the original non-linear metric.
The following example of these four steps analyzes data from the web.
http://lessRstats.com/data/quad.csv
Step 1. Use the fit parameter to try different fit functions with Plot(). The
quadratic function provides a good fit, as shown in Figure 11.14.
Plot(X,Y, fit="quad")
Figure 11.14: Quadratic fit function for Y as a function of X.
linearization:
Transform non-linear
data to be linear.
Step 2. Given the specified form of the non-linearity with fit, Plot() linearizes the
data. If the non-linearity is quadratic, to cancel the power of two to relate Y to X,
Plot() takes the square root of Y.
Step 3. Plot() then does the standard linear regression analysis with the linearized
data, shown in Figure 11.8.
To illustrate the linearity, what Plot() does automatically was done manually, the
linear transformation and the regression. Figure 11.15 depicts the results.
data transformation,
Section 3.3, p. 46
As Figure 11.15 demonstrates, taking the square root of quadratic Y results in more
linear data. There remains, however, a slight tendency for data values of X in the
middle range to still be a little more negative than would be expected by a pure246 CHAPTER 11. REGRESSION ANALYSIS
Regression of linearized data by transforming the data values with sqrt()
Need back transformation square of regression model to compute predicted values
Line: b0 = 4.8385 b1 = 0.9072 Fit: MSE = 4,748 Rsq = 0.984
Listing 11.8: Plot() output for the regression of the linearized data.
d$Y.sqrt <- sqrt(d$Y)
reg(Y.sqrt ∼ X)
Figure 11.15: Fit to the linearized data (left) and residuals (right).
randomization process. Trying different values for the power other than 2 resulted
in the best fit with fit set to "power" and fit_power set to 2.4. For that setting,
fit improved, which is also demonstrated in the patterning of the residuals closer to
random, though that precise value of 2.4 likely does not generalize to new data.
Step 4. Of course, this obtained regression model will compute predicted values
of the square root of the variable of interest, Y, not Y itself. back
transformation:
Transform the values
back to the original
metric with the
inverse of the original
transformation.
Given the linearized
model, return to the original metric of the data for response Y by performing the
back transformation. To return to the original metric, apply the inverse function of
the original transformation. In this example, undo the square root transformation by
squaring the solution from the linearized data.
Yˆ = (4.8385 + 0.9072 ∗ X)
2
With this equation, fitted values for Y, Yˆ , in the original metric of the quadratic
relationship can be computed. The best fitting quadratic function in Figure 11.14
plots this quadratic function. To obtain these fitted values Yˆ from this quadratic
equation, specify the same value of the parameter fit to the Regression() function
from which the non-linear model was obtained. With this procedure, linear regression
can analyze non-linear models.
The same procedure described above for quadratic functions applies to exponential
and logarithmic functions. If an exponential function is fitted to the data, then
Plot() linearizes by taking the logarithm, does the regression, then does the back
transformation of exponentiating the resulting regression model and plots the re￾sulting best-fitting curve. If fitting a logarithmic function, Plot() linearizes with
exponentiation and does the back transformation by taking the logarithm of the11.8. ANALYSIS PROBLEMS 247
resulting regression equation.
The "quad" option for the fit parameter applies to only the increasing or decreasing
side of a quadratic function. data transformation,
Section 3.3, p. 46
For a full quadratic equation that visualizes as a parabola,
create a new variable, the square of predictor X, X2
, then add the squared predictor
term directly to the model to yield two predictor variables.
d$X2 <- d$X^2
reg(Y ~ X + X2)
Listing 11.9: Create a regression model with two predictors.
This equation for Y then contains two predictor variables, X and the square of X,
X2. multiple regression,
Chapter 12, p. 250
More than a single predictor variable in a regression model is the subject of the
following chapter, the analysis of multiple regression models.
11.8 Analysis Problems
The following template guides almost any single-predictor regression analysis, for
homework problems or real-world analysis.
Note that hand calculations are not needed to answer any of these questions. The
computer does that work. Illustrate the concept with the formulas, but read the
needed numeric values from the output.
Regression Template for a Single-Predictor Analysis
Input
a. Show the R/lessR instruction to obtain the analysis. Identify the response
variable or target and the predictor variables or features.
Scatterplot
b. Show the scatterplot.
Relevance: Does the predictor variables relate to the response variable?
Estimated Model
c. Write the estimated regression model.
d. Specify and interpret the sample slope coefficient.
e. Show the calculation of the fitted/predicted score for the given values of predictor
variables X.
f. Show the associated residual, its calculation, and interpret for the given values
of predictor variables X and response variable Y.248 CHAPTER 11. REGRESSION ANALYSIS
Hypothesis Test: Applied to one predictor only
g. Specify the null hypothesis and its alternative for the hypothesis test of the slope
coefficient.
h. Specify the calculation of the t-statistic by applying the relevant numbers from
this specific analysis.
i. Include and apply the definition of the p-value with the relevant numbers for
this specific analysis.
j. Specify the basis for the statistical decision for the hypothesis test and the
resulting statistical conclusion.
k. Hypothesis Test: Interpretation, as an executive summary you would report to
management.
Confidence Interval: Applied to one predictor only
l. Specify the value that the confidence interval estimates (1st predictor).
m. Apply the definition of the margin of error for its computation by applying the
relevant numbers of this analysis with 2 for the t-cutoff (1st predictor).
n. Show the computations of the confidence interval illustrated with the specific
numbers from this analysis.
o. Confidence Interval: Interpretation, as an executive summary you would report
to management.
p. Demonstrate the consistency of the confidence interval and hypothesis test using
the specific numbers for this analysis for both results.
Model Fit
q. Evaluate fit with the standard deviation of residuals.
r. Evaluate fit with R2 and R2
P RESS, including their comparison.
s. Show any potential outliers and explain why they are outliers.
Assumptions
t. For each fitted value, Yˆ
i
, the average residual value should be zero.
u. Constant population standard deviation of the estimation errors at all values of
X.
v. [For values of the predictor variable collected over time.] The residuals
do not correlate with any other variable, including each other.
w. The estimation errors are normally distributed.
Prediction Intervals
x. For the 95% prediction interval of response variable Y for the given values of
predictor variables X, show the interval including its calculation (can approximate
with the t-cutoff of 2.
y. Interpret the prediction interval.11.8. ANALYSIS PROBLEMS 249
Conclusion
z. What decision do you recommend based on these findings to those who assigned
you to do the analysis?
1. Consider the BodyMeas data set of people who bought motorcycle clothing.
?dataBodyMeas
d <- Read("BodyMeas") for more information.
Build a model to predict Weight from Height.
Part I
Answer the questions for the single-predictor regression template.
For e., f., and x., use the data for Row #13, Weight=173, Height=68.
Part II
a. Drop this row of data for the outlier from the data table.
b. Re-estimate the model. Is the model reasonably similar or qualitatively different
from the model estimated with the outlier?
c. How does Cook’s Distance for the outlier from Part I relate to the estimated
slope coefficient from both models, with and without the outlier?
2. The Cars93 data set contains information on 93 1993 car models.
?dataCars93 for
d <- Read("Cars93") more information.
Build a model to predict MPGhiway from the horsepower, HP, of the car’s engine.
Answer the questions for the single-predictor regression template.
For e., f., and x., use the data for the Metro, HP=55, MPGhiway=50.Chapter 12
Multiple Regression
12.1 Quick Start
Install R and lessR as explained in Section 1.2. At the beginning of each R session,
access the lessR functions with library("lessR"). From Chapter 2, read your
data into an R data frame such as d with d <- Read("") to browse for the data file.
Or, locate your data file with a path name or web address between the quotes.
The previous chapter introduced the concept of regression analysis of models with a
single-predictor variable. This chapter introduces multiple regression models, those
with multiple predictor variables. Usually obtain more success with more than a
single-predictor, typically though not necessarily, with diminishing returns after six
or seven predictor variables. Almost all applied regression analysis models involve
multiple predictor variables, the features.
To do a multiple regression analysis again use the lessR function Regression(). For
more than one predictor variable separate each pair of predictor variables with a plus
sign, +. The model here is for response variable Y and predictor variables X1 and X2.
multiple regression,
Section 12.2, p.251
Regression(Y ∼ X1 + X2)
To obtain predictions and prediction intervals for any specified combination of values
of the predictor variables and associated 95% prediction intervals, use the X1_new
parameter to specify the values of the first predictor variable, X2_new for the second,
and so forth up to X6_new. Note that the parameter names such as X1_new are
hard-coded. Apply the same parameter names regardless of the actual names of the
prediction for new predictor variables submitted to the model.
values of the
predictor variable,
Section 12.5.1, p. 260 Regression(Y ∼ X1 + X2, X1_new=c(10, 25), X2_new=c(2,4,6))
This example provides predictions for all combinations of values for the first predictor
variable of 10 or 25 with the second predictor variable of 2, 4 and 6.
One-way ANOVA,
Section 8, p. 151
The analysis of variance (ANOVA) is the analysis of mean differences across two or more
groups on one or more variables, more generally grouping variables, or in the context
ANCOVA, Section 8, of an experiment, treatment or independent variables.
p. 151
Analysis of covariance (ANCOVA)
25012.2. MULTIPLE REGRESSION MODEL 251
extends the ANOVA model to regression with the categorical variable. Here generically
refer to the categorical variable as X1cat and continuous variable as X2cont, the
covariate. Look for differences among the X1cat groups controlling for differences
on the X2cont variable. The input is the usual input for a lessR regression analysis.
Regression() recognizes the ANCOVA model and does the corresponding analysis.
Regression(Y ∼ X1cat + X2cont)
12.2 Multiple Regression Model
12.2.1 Multiple Predictor Variables
The multiple regression model contains two or more predictor variables, features,
to account for the values of the response variable or target, generically labeled Y.
Express the fitted value of Y as a linear function of the set of m predictor variables,
generically labeled X, with corresponding regression coefficients b0, b1, . . . , bm.
Yˆ = b0 + b1X1 + b2X2 + . . . + bmXm
The form of the model remains the same as with the single-predictor models from
the previous chapter, just more predictor variables. residuals,
Section 11.4, p. 227 Define the residual for the
i
th data point as before: ei = Yi − Yˆ
i
. The estimation procedure is the same
least-squared residual minimization procedure applied to a single-predictor model.
least-squares
estimation,
Section 11.4.1, p. 228
The lessR function Regression() includes the same output for the analysis of all
regression models: model estimates, model fit, residuals, and prediction intervals.
Some additional considerations that result from modeling multiple predictor variables
also appear in the output.
Why add more predictor variables to the model?
purpose of regression
analysis,
The analysis of a regression model Section 11.2.1, p. 218
has two primary purposes: purpose of
regression
analysis: Predict
unknown response
values and relate the
response variable to
the predictors.
◦ Yˆ : Predict unknown values of Y, given the value of each of the predictor
variables
◦ bj : Relate how the values of the response variable change as the values of the
predictor variables change
These objectives remain the same for multiple regression though generally more
effectively achieved with multiple predictor variables.
To benefit from adding new predictors, each new predictor variable should bring
useful, alternative information to the model. new predictor
variable criteria:
The predictor
provides new,
relevant information.
Select predictor variables that meet the
following two conditions.
◦ Relevant Information: A proposed predictor variable correlates with the
response variable, the variable to be predicted.
◦ New Information: A proposed predictor variable is relatively uncorrelated
with the predictor variables already in the model.
Adding predictor variables or features to the model that do not correlate with
the response or target, or correlate too strongly with existing predictors, will not252 CHAPTER 12. MULTIPLE REGRESSION
meaningfully increase model fit or predictive accuracy. However, a model with
multiple predictor variables that correlate nearly 1.0 with the response variable and
0.0 with each other is an unattainable dream. A more reasonable informal guideline
is for each predictor to correlate at least 0.5 with the response variable and no more
than 0.3 with each other. Although relatively successful models may still fall short of
this more modest pattern of associations, models will tend to be more successful if at
least this correlational pattern is achieved.
12.2.2 Partial Slope Coefficients
The previous chapter introduced the regression model of single-predictor X and
response Y, with the estimated slope coefficient b1. Adding more predictor variables
to the single-predictor model changes the meaning of each slope coefficient, and
generally its size as well. partial slope
coefficient bj :
Average change in Y
for each unit increase
in Xj with values of
all other predictors
held constant.
Refer to a multiple regression slope coefficient for the
j
th predictor, bj , as a partial slope coefficient. As with the slope coefficient from a
one-predictor model, the partial slope coefficient bj indicates the average change in Y
with a 1-unit increase in X, but with a caveat. Interpret bj as the average change
in Y with a 1-unit increase in Xj , holding constant the values of all other predictor
variables in the model. This key concept for data analysis and scientific research in
general is discussed next.
Total Effects
fire truck example, Change the value of Variable X to assess the extent of change in Variable Y.
Section 14.2.1, p. 303
An
example that provides more detail in a later discussion of causality considers the
following fact: The more fire trucks at a fire, on average, the more damage from the
fire. A regression model that estimates the corresponding slope coefficient quantifies
how much, on average, the damage increases for each additional fire truck. Because
the relationship is real, the corresponding slope coefficient is larger than zero.
Of course, fire trucks do not cause damage. The causal relationship is that the more
severe the fire, the more fire trucks the fire department sends to the fire. The number
of fire trucks and the amount of damage share a common cause, the severity of the
fire. The relationship between fire trucks and fire damage entirely depends on the
variation of fire severity. There is no relationship between fire trucks and fire damage
for a set of fires all of the same severity.
The slope coefficient in a simple (1-predictor) model represents the effect of a 1-unit
increase in the value of the predictor variable X on the average change in the response
variable Y. The extent of this change follows from two distinct sources. First, the
slope coefficient reflects the direct causal impact X has on Y, where a change in X
directly results in a commensurate change in Y. Study hard the day before and the
morning of a test and do better on the test.
However, X likely correlates with many variables other than just Y. The number
of fire trucks at the fire, X, correlates not just with fire damage, Y, but also with
fire severity. When the value of X changes, the values of these correlated variables
change to the extent of their correlation with X. indirect effect,
Section 14.8, p. 311 Changes in other variables have12.2. MULTIPLE REGRESSION MODEL 253
an indirect impact on Y. The slope coefficient in a one-predictor model includes X’s
direct influence on Y as well as all cumulative indirect effects on Y of variables related
to X.
total effect: Effect
of a change in the
value of the predictor
variable and all
related variables on
the average change of
the response variable.
The subsequent average change in the value of the response variable, the total effect
of X on Y, reflects the cumulative impact of all of these changes. An increase in the
number of fire trucks is generally accompanied by an increase in fire severity. The
slope coefficient for X, particularly for single-predictor models, is generally not causal,
but correlational. The slope coefficient for the single-predictor model, indicates the
joint impact of X and all of its correlates on Y.
correlation not
causality,
Section 14.2.1, p. 304
The challenge with establishing the direct causal impact of X on Y is that changing
the value of X alters all variables that are associated with X. confounding variable,
Section 7.2.6, p. 138,
Section 14.2.1, p. 304
These variables that
correlate with the variable of interest are called confounding variables or lurking
variables in this context. The presence of confounding variables is a major impediment
to determining how much one variable causally influences another variable. The direct
impact of a change in X alone on Y cannot be separated from the impact of these
confounding variables, the total effect, without additional information.
Net Effects with Statistical Control
control: Eliminate
or reduce the impact
of confounding
variables to assess
the causal impact of
one variable on
another.
Control for the presence of confounding variables by removing their impact. The
most effective technique for assessing cause and effect is experimental control, a key
innovation of the scientific method.
experimental control,
Section 7.2.6, p. 137
Divide the study participants into groups at
random to eliminate any differences between groups on any variable. Then treat each
group differently. If the values of the response variable differ across the groups beyond
random sampling error, then the different treatment each group received is the likely
cause of these differences. Experimental control removes the effects of confounding
variables in the analysis of cause and effect. statistical control:
Multiple regression
partially controls the
values of potentially
confounding
variables.
When experimental control with random assignment is not possible, statistical control
is the next best alternative for isolating direct causal influences. Add more variables
to the model to remove more potential extraneous influences on the value of Y. With
multiple regression, the average change in Y that results from a change in Xj more
closely resembles the direct causal impact of Xj on Y, a net effect.
net effect: Effect of
one variable on the
response variable
with the values of
related variables held
constant.
Statistical control
isolates the net effect, disentangled from the total effect. Statistical control removes
the influence of these other predictor variables in the model on the predictor of
interest from the estimated slope coefficient bj .
With both the number of fire trucks and fire severity as predictor variables in the
model to predict fire damage, the intensity of the fire is held constant relative to the
number of fire trucks. For fires of the same severity, there is no relationship between
number of fire trucks and damage. The partial slope coefficient for number of fire
trucks vanishes.
However, the extent of a net effect is relative to the set of predictor variables
that define the model. Suppose the model includes all variables correlated with
Xj that impact Y, what may be called the ultimate net effect. In this multiple
regression model, bj expresses the direct causal impact of Xj on Y. Regrettably,254 CHAPTER 12. MULTIPLE REGRESSION
not all potential confounding variables are likely to be identified, measured, and
incorporated into the regression model. Keeping the other variables constant is
an attempt, usually only partially successful, to achieve the equivalence of groups
achieved with experimental control. However, statistical control, while typically not
as effective as experimental control in eliminating confounding variables, is often a
practical alternative in situations where experimental control is impossible.
Which interpretation, net effect or total effect, is preferred? The answer depends on
the question. If the focus is the actual change in the response variable for a change in
the predictor variable, then a single-predictor regression model is appropriate. If the
goal is to understand the direct causal impact of a predictor variable on the response
variable, include any confounding influences in the model as additional predictor
variables.
12.3 Model Estimation
Specifics of a multiple regression analysis and how it relates to one-predictor analyses
are discussed in terms of the following example.
Scenario. Identify variables that relate to reading achievement
An educational researcher investigates the conditions that contribute to reading
success. As part of this (simulated) research project, at the end of the school year,
100 students in the same grade were administered a reading ability test with a
percentage scale from 0 to 100. Verbal aptitude was measured with a separate test,
again on a scale of 0 to 100. Also measured was the number of days Absent from
school during the school year and family Income in $1000’s.
The data are in the file called Reading, downloaded as part of lessR.
d <- Read("Reading")
12.3.1 Total Effects
Separately regressing the measure of Reading ability on each of the remaining variables
one at a time demonstrates a statistically significant relation between Reading ability
and each predictor variable.
Yˆ = 38.407 + 0.523XV erbal, for H0 : βV erbal = 0, p-value = 0.000
Yˆ = 83.477 − 2.833XAbsent, for H0 : βAbsent = 0, p-value = 0.000
Yˆ = 62.282 + 0.170XIncome, for H0 : βIncome = 0, p-value = 0.000
The total effect for each predictor variable is significant, with the corresponding
p-value less than α = 0.05.
A change in each predictor variable leads to a change in Reading ability as expressed
by the total effect for each predictor. In this sample of 100 students, an increase in12.3. MODEL ESTIMATION 255
1 point on the Verbal aptitude test yields, on average, an increase of a little more
than 1-half point on the Reading test. Each additional day of being Absent yields an
average decrease of more than 2.8 points on the Reading test. Each additional $1000
of family income results in an average increase of 0.17 points on the Reading test.
Interpretation.
Verbal ability, and family income are both positively related to reading ability, the
more ability and the more income, on average, the higher reading ability. The days
absent relationship is negative. The more days absent, on average, the less reading
ability.
The relationships between all three predictors and reading ability, in terms of total
effects, are real, though not necessarily causal.
12.3.2 Net Effects
Each of these three one-predictor analyses do not provide any control on the estimation
of model slope coefficients. When one of these predictor variables varies, so do all the
variables correlated with the predictor variable, including the values of the other two
predictor variables in the study.
Scenario. Statistically control for the effects of other predictor variables
Assess these relationships regarding Reading ability with the imposition of statistical
control. Use multiple regression estimates that convey the net effect of each predictor
on Reading ability, relative to the other predictor variables.
Only with a control that holds the values of all other correlated variables with each
predictor variable constant can the underlying direct impact of a predictor variable
on the response variable be estimated. Unfortunately, all variables that correlate
with each predictor will likely not appear in the model. Nonetheless, each net effect
estimated by a partial slope coefficient indicates the impact of the corresponding
predictor on the Reading ability controlling for the remaining two predictor variables.
Input. Multiple regression analysis
Regression(Reading ∼ Verbal + Absent + Income)
Before beginning the formal model analysis, develop some intuition by viewing
the scatterplot matrix from Regression(). To demonstrate relevance, Reading
ability should strongly relate to the predictor variables. A useful guideline is that all
correlations in the first row should be more than 0.5, which means that all scatterplots
in the first column should have demonstrably sloped regression lines. Furthermore, all
of the correlations between the predictors should be relatively modest to emphasize
uniqueness, such as below 0.3.
scatterplot matrix,
Section 10.10, p. 209
Figure 12.1 shows the scatterplot matrix from Regression() for the four model
variables, which includes both the pair-wise scatterplots with best-fit lines and the
corresponding correlation coefficients. Relations with the response variable Reading
ability appear in the first row and column.256 CHAPTER 12. MULTIPLE REGRESSION
Figure 12.1: Scatterplot matrix of response Reading ability with predictors Verbal aptitude,
days Absent, and family Income.
Days Absent is the most relevant predictor, correlating r = −0.65 with the response
variable, so will likely be retained in the formal model analysis. The weakest variable
is Income, which has the lowest correlation with Reading, r = 0.43. In terms of
uniqueness, the three predictor variables have much overlap, all correlating over a
magnitude of 0.3. Another disadvantage of Income in this model is that it correlates
rather strongly with days absent, r = 0.58. The formal analysis may result in dropping
Income from the model as it is the least relevant and the least unique.
With the context gathered from visualizing the scatterplot matrix, find the resulting
partial slope coefficients and associated hypothesis tests and confidence intervals in
Listing 12.1.
-- Estimated Model for Reading
Estimate Std Err t-value p-value Lower 95% Upper 95%
(Intercept) 64.437 8.354 7.713 0.000 47.854 81.021
Verbal 0.204 0.096 2.120 0.037 0.013 0.395
Absent -2.043 0.489 -4.176 0.000 -3.013 -1.072
Income 0.033 0.037 0.912 0.364 -0.039 0.106
Listing 12.1: Estimated model coefficients, hypothesis tests and confidence intervals for the
three predictor model of Reading ability.
The estimated multiple regression model follows.
YˆReading = 64.437 + 0.204XV erbal − 2.043XAbsent + 0.033XIncome
The results of the estimated net effects with multiple regression differ from the total
effects obtained from the three separate one-predictor regressions. Consider Income,
which has an estimated net effect coefficient considerably less than the corresponding
estimated total effect. The total effect is significant, but the partial slope coefficient12.3. MODEL ESTIMATION 257
for Income, the net effect, is not statistically detectable from zero.
Net effect of Income on Reading:
p-value = 0.364 > α = 0.05 so do not reject H0 : βIncome = 0
95% confident that βIncome within −0.039 to 0 to 0.106
The null hypothesis specifies no effect of income on reading ability for students who
have the same verbal aptitude and are absent the same number of days of school, The
hypothesis of no effect cannot be rejected, and the corresponding confidence interval
of likely values of βincome includes zero.
Interpretation. Impact of family income on reading ability
The net effect of income cannot be distinguished from zero. The impact of income
on reading ability is not detectable from zero for students with the same number of
days absent and with the same verbal aptitude.
The net effect for Verbal aptitude also drops considerably from the total effect, less
than half, from 0.523 to 0.204. The effect, however, does remain significant.
Net effect of Verbal aptitude on Reading:
p-value = 0.037 < α = 0.05 so reject H0 : βV erbal = 0
95% confident that βV erbal within 0.013 to 0.395
Reject the null hypothesis of no effect of Verbal aptitude on reading ability holding the
values of family Income and days Absent, βV erbal 6= 0. Consistent with the hypothesis
test, the 95% confidence interval of plausible values for βV erbal does not include zero,
so zero is not a plausible value.
Interpretation. Impact of verbal aptitude on reading ability
When the effects of the variables absent and income are controlled, held constant,
a 1-point increase in the verbal attitude test score results in an average increase of
reading ability of 0.2 points.
Controlling for Verbal aptitude and family Income only slightly mitigates the effect
of days Absent on Reading ability. The estimate of the total effect is −2.833. The
net effect estimate is −2.043.
Net effect of days Absent on Reading:
p-value = 0.000 < α = 0.05 so reject H0 : βAbsent = 0
95% confident that βAbsent within −3.013 to −1.072
The null hypothesis of no relationship is rejected with the p-value approximately
equal to within three significant digits. The confidence interval for βAbsent consistently
spans negative values.
Interpretation. Impact of days absent on reading ability
The effect of days absent applies to all students with the same verbal aptitude and
family income: Each additional day absent averages somewhere between a little
over a 1% to 3% drop in reading ability as assessed by the reading test.258 CHAPTER 12. MULTIPLE REGRESSION
Days Absent probably directly contributes to less Reading ability because of missed
classroom instruction and missed practice. The variable is also likely a proxy for a
more abstract concept, general Motivation. The motivated student is more likely
not to miss class and is more interested in learning, attending class, and paying
attention. The researcher would test this additional hypothesis in a later study that
also included measures of Motivation beyond just class attendance.
12.4 Model Fit
The estimated model coefficients cannot be properly interpreted without several other
considerations satisfied. One consideration is the size of the residuals as related to
the model fit. The relevant output appears in Listing 12.2.
12.4.1 Fit Indices
-- Model Fit
Standard deviation of Reading: 13.504
Standard deviation of residuals: 10.192 for 96 degrees of freedom
95% range of residual variation: 40.463 = 2 * (1.985 * 10.192)
R-squared: 0.448 Adjusted R-squared: 0.430 PRESS R-squared: 0.399
Null hypothesis of all 0 population slope coefficients:
F-statistic: 25.934 df: 3 and 96 p-value: 0.000
Listing 12.2: Model fit indices derived from the size of the residuals.
According to the test of all 0 population slope coefficients, the population
R2
is apparently larger than 0 because its p-value is equal to 0 within three decimal
digits. The sample value of R2 = 0.448 is moderate and compares favorably with the
value typically obtained in the social and behavioral sciences.
Each additional predictor variable that provides relevant, new information leads to
a better fit according to a decreased sum of the squared residuals.
se, Section 11.4.2,
p. 229
This decrease
indicates improved fit with a lower standard deviation of the residuals, se, and a
larger R2
. However, particularly with multiple predictors, R2
adj is preferred over R2
.
Unlike R2
, R2
adj accounts for the number of predictor variables in the model relative
to the overall sample size.
R
2
adj , Section 11.4.2,
p. 232 Adding predictor variables that do not contribute much to
the overall reduction in the sum of squared residuals may result in a lower value of
R2
adj . For generalizing to new data, R2
P RESS is preferred and is the most conservative
estimate of the three.
R
2
P RESS,
Section 12.5.2, p. 261
The difficulty achieving precise prediction is not atypical. The 95% range of variation
of data values about the regression line based on the sample residuals spans over 40
points on a 100-point scale for Reading ability. Actual prediction applying the model
on new data yields even larger intervals.12.4. MODEL FIT 259
12.4.2 Outliers and Assumptions
Regression() provides two forms of information regarding outliers, which can have
disproportionate influence on the model’s coefficient estimates: a table displayed at
the R console and a visualization. In terms of influential data points and outliers
there do not appear to be any in this data set.
Cook’s Distance, D,
Listing 12.3 shows the five examples, rows of data, sorted by Cook’s Distance. Section 11.6.2, p. 238
-- Data, Fitted, Residual, Studentized Residual, Dffits, Cook’s Distance
Verbal Absent Income Reading fitted resid rstdnt dffits cooks
47 49 0 82 98 77.167 20.833 2.193 0.719 0.124
18 64 8 89 81 64.117 16.883 1.736 0.480 0.056
63 63 2 127 57 77.440 -20.440 -2.091 -0.468 0.053
93 70 0 17 66 79.271 -13.271 -1.380 -0.462 0.053
95 77 5 51 49 71.622 -22.622 -2.304 -0.408 0.040
Listing 12.3: Outlier and influence statistics for the five examples with the largest Cook’s
Distance.
The example with the largest Cook’s Distance is Row #47, D47 = 0.124, which is
more than twice as large as the next highest value, D18 = 0.056. However, influential
outliers also typically have a large Studentized residual, equal to 2.193, not particularly
large. Still, this sample, row of data, is potentially and outlier and its origin should
probably be investigated further.
Figure 12.2 shows the plot of the residuals and fitted values.
Figure 12.2: Diagnostic plot.
Figure 12.2 does not indicate any noticeable patterns. Still, from the Regression()
output the largest R-student value in magnitude is 2.35 and the largest Cook’s
Distance is only 0.12. In terms of assumptions, the plot of the distribution of the
residuals demonstrates at least an approximate normality. In terms of fit, influence,
and assumptions the interpretation of the estimated coefficients appears valid.260 CHAPTER 12. MULTIPLE REGRESSION
12.5 Prediction
12.5.1 Predictive Precision
For the data used to estimate the model, there is no prediction because these values are
already known. Moreover, the known data values shaped the estimated model, so the
model is biased toward those known values. Use new data to predict unknown values
of the response variable. Calculate predicted values and accompanying prediction
intervals for new data with any specified predictor values, not just those that happen
to be in the original data. prediction from new
values, Section 11.5.3,
p. 235
As with prediction from the single-predictor model, the
standard error of prediction, spredi
, for a given value of Xi
, reflects both training
error and sampling error, and so is larger than the standard deviation of the residuals,
se, which only reflects training error, also called modeling error.
X1_new,
X2_new
parameters: Obtain
predictions for
specified values of
two predictor
variables.
Illustrate the predictions for specified values of the predictor variables for the Reading
model with its two significant predictors: Verbal test score and days Absent. To
compute predictions and 95% prediction intervals for new, specified data values for
a model with two predictor variables, use the X1_new and X2_new parameters for
the Regression() function. Use X1_new to specify one or more values for the first
predictor variable, and X2_new for the second predictor variable listed in the function
call. These values may be specified up to the arbitrary cutoff of six predictor variables,
up to X6_new.
The output from Regression() is identical to the previous two-predictor variable
analysis except for the section that provides the predicted values shown in Listing 12.4.
The analysis provides all combinations of the specified values of the two predictor
variables from which to predict, here for Verbal scores of 71 and 89 and Absent days
of 3 and 10.
Input. Prediction from new data
reg(Reading ∼ Verbal + Absent, X1_new=c(71,89), X2_new=c(3,10))
-- Data, Predicted, Standard Error of Prediction, 95% Prediction Intervals
Verbal Absent Reading pred s_pred pi.lwr pi.upr width
3 71.000 10.000 59.463 10.684 38.258 80.669 42.411
4 89.000 10.000 63.143 11.143 41.028 85.259 44.231
1 71.000 3.000 75.251 10.235 54.938 95.565 40.627
2 89.000 3.000 78.931 10.401 58.287 99.575 41.288
Listing 12.4: Predicted values and prediction intervals of Reading ability from the Verbal
aptitude test score and days Absent for specified values of new data.
The values for the response variable Reading are unknown, so there are no values in
the output. The fitted values are consistent with the two predictor-variable model.
The highest expected Reading score of 78.931 in Listing 12.4 corresponds to the
highest Verbal score, 89, and the fewest days absent, 3. Unfortunately, the widths of
the prediction intervals are around 42, so the obtained value, YReading, for any one
student may vary far from the predicted value, YˆReading.12.5. PREDICTION 261
12.5.2 Training vs. Testing Data
The regression analysis estimating the model coefficients from the data is the machine
learning to relate the predictor variables or features to the response variable or target.
The learning algorithm optimizes fit from the data on which it trains to build the
model. Optimal for what? The answer: Optimal for prediction on the training data.
However, what we care about is prediction from new data for which we do not know
the value of the target.
Optimal fit to the training data results in a problem central to regression analysis
and machine learning in general: The obtained fit to the data can be too good, with
too much error minimized. Too little error? Fit that is too good? Yes, when fit is
assessed according to the data from which the machine has learned the pattern of
relationships among the variables, the resulting model might fit too well. sampling error,
Section 6.3.2, p. 112
At least
some of what the machine learned may be specific to that one data set. All data
values reflect random variation in the form of sampling error. Because predictions
are made from new data, a useful model must generalize beyond the training data to
new data.
non-linearity,
Section 11.7.2, p. 243
For small samples, the issue of sampling error is especially acute. With enough
complexity, any model can fit any data set perfectly. With perfect fit, for each
row of data the fitted responses exactly equal the actual response value. Adding
enough predictor variables and non-linear transformations of those variables, such as
quadratic or exponential transformations, can yield perfect fit: Perfect, but worthless
fit if the model does not generalize to new data.
overfit model: The
model fits the
training data too
well, not generalizing
Modeling random error is called overfitting to new data. . Overfitted models can fit the data on
which they trained very well and then fail miserably attempting to generalize to new
data.
test data: Data
sample for model
evaluation not seen
during the machine
learning process.
The resolution to the overfitting problem evaluates the model on new data,
the testing data.
underfitting: A too
simple model fails to
capture the true
underlying patterns
in the data.
The opposite problem of overfitting is, no surprise, underfitting. The
underfit model is too simple, missing patterns that do exist. The underfit model fails
to detect useful information in the data that would properly generalize to new data.
Figure 12.3 visualizes the balance between the competing influences of under- and
over-fitting for a range of model complexity.
Error on Training Data
Error on Test Data
Underfitting Overfitting
Predictive Error
Model Complexity
Too Simple Too Complex Ideal Model
Complexity
Figure 12.3: Tension between overfitting and underfitting a model: A too simple model
never fits well, but an overly complex model only fits well on training data.262 CHAPTER 12. MULTIPLE REGRESSION
When additional model revisions are only a response to random data fluctuations,
fit to the training data model increases as the model’s generalizability to predicting
with new data decreases. A successful machine learning model balances the dynamic
tension between overfitting and underfitting to achieve satisfactory prediction accuracy.
Too much model complexity absorbs quirky aspects of the training data. Too little
model complexity fails to account for the relations among the variables as they exist.
Ideally, the model trains on one set of data, fits well, and then fits almost as well on
another data set. Ideally, the model satisfies the range of model complexity labeled
“Ideal Model Complexity” in Figure 12.3.
bias-variance
trade-off: Extract
all and only
generalizable
information from the
predictor variables
(features) to account
for the response
variable (target).
The machine learning community refers to the interplay between under- and over￾fitting as the bias-variance trade-off. “Bias” refers to the property of the underfit
model that fails to consider all needed information. “Variance” refers to the variability
across different data sets of a fitted value for a given set of predictor values that
results from overfitting models. Ideally, the model has zero bias and zero variance. In
practice, there is some of each so that the analyst searches for the best compromise.
The trade-off is that decreasing either bias or variance tends to increase the other.
12.5.3 Data Splitting
Evaluating the model on new data is the process of cross-validation.
cross-validation:
Evaluate the fit of a
model on new data,
such as the testing
data.
What to do if a
new data set is not available for testing the model? If fortunate enough to have more
than a small data set, one solution, before model estimation, is to split the available
data splitting: data into two sections. Use about 70% or 75% of your data for training.
Randomly split the
data into a hold-out
sample for testing
and an estimation
(training) sample.
Hold-out the
rest of the data from the estimation phase of the analysis, and then test the model
on this hold-out sample, the testing data.
How important is data splitting? Cassie Kozyrkov (2019), the Chief Decision Scientist
at Google as of this writing, refers to data splitting as nothing less than “The most
powerful idea in data science.” Data splitting allows the researcher much flexibility in
model development, allowing for the separation of insight, model formulation, from
model evaluation. Play with your data. Develop insights. Be inspired. Build a model.
Revise the model to fit better. Revise the model again. But always evaluate on new
data. Did you discover something real, or were you only playing with one data set
and nothing more? When applied to testing data, fit will generally be reduced, but
only minimally for a successful model.
Following model respecification, the inference statistics for generalizing the model
beyond the current data set, p-values and confidence intervals for the slope coefficients
and for the model itself, are only meaningfully estimated when the model is applied
to new data. Once the model is respecified from an initial analysis on the training
data and then re-run on that same data, the p-values and confidence intervals are
reduced to heuristics instead of formal probability statements. The heuristics may
be useful for providing feedback for model development but the formal testing of a
model always requires fresh data.
An especially effective data splitting technique for model evaluation on new data
generalizes the cross-validation to several, k, splits. For a moderate-size sample, a12.5. PREDICTION 263
typical value of k is 5, or, for smaller samples, 3. k-fold
cross-validation:
Randomly split the
data into a hold-out
sample for testing
and an estimation
(training) sample k
different ways.
For k = 5, partition the data into
five different sets of about equal size. Each such set is called a fold, the process k-fold
cross-validation.
Use the first fold as the hold-out testing set. Train the first model on folds two
through five. Evaluate the trained model by predicting the values of y on the first fold,
the test data for this iteration. Then, define the second fold as the hold-out sample,
and use the first fold, and folds three through five as the training data. Evaluate the
model by applying the estimated model to the second fold. Repeat this process for
all five folds. The best single fit statistic is the average of a chosen fit statistic, such
as R2
, from applying the model to the test data sets over the k different analyses.
Perform a k-fold cross validation with Regression() by setting the parameter kfold
to the number of folds. In practice, a reasonably large data set is needed to have a
stable set of estimated parameter values and enough data left over for the hold-out
samples. More than the size n = 100 in this example would be preferred. Figure 12.5
illustrates this process with the Read data set for three folds with a 2/3 to 1/3 data
split for each fold.
reg(Reading ∼ Verbal + Absent + Income, kfold=3)
Model from Training Data Applied to Testing Data
------------------------- -------------------------
fold n se MSE Rsq n sp MSE Rsq
1 | 67 10.521 110.698 0.430 | 33 10.424 108.656 0.419
2 | 66 10.143 102.881 0.438 | 34 11.400 129.960 0.406
3 | 67 9.988 99.762 0.490 | 33 11.347 128.751 0.342
------------------------- -------------------------
Mean 10.217 104.447 0.453 11.057 122.456 0.389
Listing 12.5: Three-fold cross-validation of three separately estimated regression analysis
machine learning models showing the best assessment of fit, average R2 = 0.389
for the testing data.
The standard deviation of the residuals, se, is computed from the training data.
prediction error,
Section 11.5.1, p. 233
Applying the previously estimated model to new data changes the application of the
standard deviation to the standard deviation of prediction, spredi
, for a given Xi
, the
extent of the variation of the residuals from prediction on new data.
The three fit indices computed by Regression() – standard deviation of the residuals,
mean squared error, MSE or variance, and the family of R2
indices – all follow the
general pattern of decreased fit in the testing data. Both se and MSE increase from
training to testing data, and R2 decreases from an average of R2 = 0.453 from the
training data to an average of R2 = 0.389, the assessment of fit from this analysis.
The large variability of R2 across the three folds follows from the small sample sizes
of the testing data folds, 33 or 34.
R
2
P RESS,
Cross-validation of machine learning models is a crucial and expected aspect of Section 12.5.2, p. 261
validating any predictive model. However, unlike other machine learning training
procedures, with regression analysis we also have the R2
P RESS case-deletion statistic,
essentially a (n − 1)-fold cross-validation. Each row of data is held out to compute264 CHAPTER 12. MULTIPLE REGRESSION
the corresponding predictive residual. Both R2
P RESS and a cross-validated R2
reflect
the shrinkage of moving from training data to testing data. So it is not surprising
that R2
P RESS usually approximates the average R2
from a k-fold analysis. Compare
R2
P RESS = 0.399 from Listing 12.2 to the average R2 = 0.389 obtained in the 3-fold
cross-validation reported in Listing 12.5.
12.6 Model Selection
A multiple regression analysis begins with a model that includes two or more predictor
variables, or features. However, some predictors are more valuable than others. Which
combination of predictors produces the best model? Subsequent analysis assists in
identifying the most beneficial predictors to improve predictive accuracy and to better
explain the values of the response variable, the target in the language of machine
language.
model selection:
Choose the minimal
set of predictors that
achieves satisfactory
fit relative to all
possible models. Model selection is deciding which predictor variables to select in a regression model
from a larger initial set of variables. feature
engineering: Select
and transform
features, predictors,
to optimize model fit
and explanation.
More generally, model selection is an essential
component of feature engineering. Predictor variables are selected and also potentially
transformed to enhance model fit and predictive efficiency, often with multiple analyses
evaluating different models.
12.6.1 Collinearity
Ideally, each predictor variable relates highly to the response variable but not much
to the remaining predictor variables, providing relevant but relatively unique informa￾tion. Two or more highly related predictor variables are redundant, called collinear
predictors.
collinearity: Two
or more highly
related variables.
Little gain in predictive efficiency results from adding a new predictor
variable to the model that substantially correlates with an existing predictor variable.
Collinear predictors can detract from model estimation in addition to not enhancing
predictability. A partial slope coefficient estimates the net effect of a unit change of
the predictor variable on the response variable, with the value of all other predictor
variables held constant. When the names of two or more predictor variables simply
provide different labels for essentially the same concept, their separate impacts are
difficult to separate, inflating the standard errors of the estimates. Because of the
large standard errors for the individual estimate coefficients, obtaining statistical
significance of each affected coefficient is difficult without a considerably large data
set. An important effect included twice in the model could potentially be ignored
due to this lack of significance. Best to remove a collinear predictor from the model,
or perhaps combine the collinear features by replacing them with their average.
A large correlation between two predictor variables can indicate collinearity. More
generally, two or more predictor variables are collinear when the variables linearly
relate to each other. To evaluate the collinearity of a predictor, the j
th predictor,
Xj create a separate regression model in which Xj becomes the response variable,
and all remaining predictors are the predictor variables. For example, for three
predictor variables, to assess the collinearity of the second predictor, compute R2
for12.6. MODEL SELECTION 265
the following model.
X2 = b0 + b1(X1) + b2(X3)
If Xj is collinear with one or more of the remaining predictors, then its values can
be predicted by the remaining predictor variables. The resulting R2
j
of a collinear
predictor would be large, such as over the somewhat arbitrary but conventional
guideline of 0.8.
collinearity
indicators of a
predictor variable:
Tolerance and VIF,
each based on the R
2
of regressing the
predictor on all
others.
From this value of R2
j
for the j
th predictor, define two equivalent indices of collinearity:
Tolerance and the Variance Inflation Factor or VIF.
Tolerance(bj ) = 1 − R
2
j and VIF(bj ) = 1
Tolerance =
1
1 − R2
j
Tolerance of the j
th predictor variable is R2
j
subtracted from 1. An undesirable R2
j
larger than 0.8 is the same threshold of collinearity for tolerance less than 0.2. VIF
is the reciprocal of tolerance, so the same threshold is stipulated when VIF is larger
than 1/0.2 = 5. The minimum value of VIF is 1, which indicates no collinearity.
Values larger than 1 indicate the extent of inflation of the variance, the square of
the standard error of the variable’s slope coefficient. A VIF of 2 shows, for example,
that the variance for the standard error has doubled over the state of no collinearity,
making detection of a significant slope coefficient more difficult.
Regression() reports the values of Tolerance and VIF for each predictor variable,
shown in Listing 12.6.
-- Collinearity
Tolerance VIF
Verbal 0.600 1.667
Absent 0.463 2.158
Income 0.667 1.500
Listing 12.6: Collinearity indicators for each of the three predictor variables.
All the values of Tolerance are well above 0.2 and all the values of VIF are then
necessarily below 5. Collinearity is not an issue for this analysis.
12.6.2 Best Subsets
Model selection can proceed from several available procedures. best subsets: Lists
the fit of all (or most)
possible models from
the submitted model
for model selection.
One useful approach
examines all or most possible subsets of the predictor variables to define all (or most)
possible models. To search for the best possible subsets, the procedure computes
a fit index such as R2
adj for every possible combination of predictor variables in
the model, or most combinations for models with a large number of predictors.1
R
2
adj , Section 11.4.2,
p. 232
The Regression() function displays the modified output by default, as shown in
Listing 12.7.
1The procedure that provides best subsets analysis is the function leaps() written by Lumley
and Miller (2020) from the package of the same name, automatically downloaded and installed with
the lessR package.266 CHAPTER 12. MULTIPLE REGRESSION
-- Best Subset Regression Models
Verbal Absent Income R2adj X’s
1 1 0 0.431 2
1 1 1 0.430 3
0 1 0 0.411 1
0 1 1 0.410 2
1 0 1 0.334 2
1 0 0 0.277 1
0 0 1 0.177 1
Listing 12.7: All possible regression models evaluated in terms of R2
adj .
Each row of the output in Listing 12.7 is the analysis of a different model. A
predictor variable column with a 1 indicates that the considered model contains the
corresponding variable. A 0 indicates the variable is excluded from the model. The
R2adj column lists the model’s R2
adj , and the X’s column lists the number of predictor
variables in the corresponding model.
parsimony: Model
selection optimizes fit
and simplicity.
The goal of model selection is parsimony, to select the model that balances the two
competing criteria of the best possible fit against the smallest number of predictor
training sample, variables.
Section 11.6.3, p. 240
If a complex model with many predictor variables only slightly outperforms
a simpler model in training data, choose the simpler model. Only when meaningful
gains in fit are obtained, and this improved fit extends to new situations with new
overfitting, data, should models become more complex.
Section 12.5.2, p. 261
Overfitting a complex model to training
data is all too easy. The resulting useless improved fit applies only to the sample data
used to estimate the model, with little or no increased predictive power for new data.
Each additional predictor variable at least slightly increases the value of R2
from the
previous model without that predictor. However, each additional predictor variable
removes one more degree of freedom from the computation of the mean squared error,
MSE. R2
adj balances the improvement in fit from a new predictor variable against the
loss of an additional degree of freedom from the model. Unlike R2
, add a weak, new
predictor variable to the model and R2
adj may drop in value.
This decrease occurs in this example in which a two-predictor model of Verbal and
Absent has an R2
adj = 0.431. The fit index drops slightly to 0.430 for a model with
all three predictors. Income does not contribute to model fit. Dropping Income from
the model is also consistent with the non-significant result for its slope coefficient.
After dropping Income from the model, should the model contain Verbal aptitude
and Days Absent (R2
adj = 0.431) or just days Absent (R2
adj = 0.411)? Balancing fit
and number of predictors, perhaps the model with only Absent as a predictor variable
provides the best trade-off. The fit only drops from R2
adj = 0.431 with two predictors
to 0.411 with one, a small decrease. Still, including Verbal aptitude may contribute
to the interpretation of the model, helping to answer questions of interest. Including
or deleting predictor variables is partly a matter of judgement that depends both on
testing data, statistical and interpretative considerations.
Section 12.5.2, p. 261
And, as always, test the chosen model
on new data, the testing data, to evaluate fit.12.6. MODEL SELECTION 267
12.6.3 Nested Models
A formal test of the contribution of one or more predictor variables to a model is
an hypothesis test of a nested model, with deleted variables, compared to the full
model with all of the predictor variables. nested model:
Model with predictor
variables that are a
subset of the more
complete model.
The test of the nested model against the
full model examines the contribution of the variables deleted to obtain the nested
model. If the analysis detects no difference between the models, the deleted variables
did not significantly contribute to the model.
Scenario. Test the effectiveness of specified predictor variables
Create a nested model with the specified predictor variables deleted from the full
model. Then compare the two models.
The lessR function Nest() accomplishes the test of the difference between the models.
Nest() function,
lessR: Compare a
nested model with
the corresponding
full model.
To use the function, list the response variable, the subset of the predictor variables
for the nested model, and then the larger set of predictor variables in the full model.
The Nest function directly compares the residual sum of squares of the two models.
The process is similar to the concept underlying R2
that compares the residual sum
of squares of the full model to the null model. Here the result is more general in that
the nested model is not as severe as the null model, but still not as complete as the
full model. null model,
Section 11.4.2, p. 230
Nest() analyzes the full model first and then analyzes the data for the nested model
with the same data analyzed by the full model. The full model contains more variables,
and some of these additional variables not in the nested model may have missing
data. If the full data table was submitted to two separate analyses, any rows of data
that only had missing values on the variables not present in the nested model would
be deleted from the full model analysis, but not from the nested model analysis. The
comparison between models is only valid when the same data are analyzed for both
models.
Another strategy uses the R function na.omit() to purge all cases from the data
frame that contain at least one missing data value. na.omit() function,
R: Remove all rows of
data from the
specified data frame
with any missing
data values.
d <- na.omit(d)
However, this strategy is too broad because data values may be missing for variables
in the data table not present in either of the models. The result is the deletion of too
much data, leaving an analysis based on a smaller sample size than necessary.
Invoke the Nest() function to compare a nested model to the full model. The result
appears in Listing 12.8.
From this analysis, dropping both Verbal and Income yields a model that is not
statistically significantly different from just a 1-predictor model.
Effect of both Absent and Income: p-value = 0.074 > α = 0.05,
so do not reject H0 : βV erbal = βIncome = 0268 CHAPTER 12. MULTIPLE REGRESSION
Input. Test a nested regression model against the full model
Nest(Reading, c(Absent), c(Verbal,Absent,Income))
Reduced Model: Reading ~ Absent
Full Model : Reading ~ Absent + Verbal + Income
Analysis of Variance
df Sum Sq Mean Sq F-value p-value
Tested 2 556.36 278.18 2.68 0.074
Residual 96 9972.65 103.88
Listing 12.8: The comparison of a nested model to the more general full model.
The p-value, however, just misses the boundary of α = 0.05. This situation is
“backwards” from the typical test of a null hypothesis that detects a difference by
rejecting the null hypothesis. In this situation, not rejecting the null hypothesis leads
to a conclusion to delete variables. A lack of power of the sensitivity of the test to
find a difference that truly exists would lead to the conclusion there is a difference.
Given the relatively small sample size of 100, a larger sample may, but not necessarily,
lead to a significant difference.
Compared to a model including one or both of the other predictor variables, we
conclude that the days Absent in this sample of size 100 is probably sufficient for
predicting Reading ability. This conclusion, however, is speculative. A model with
both days Absent and Verbal ability is arguably the best model for understanding
cross-validation: the relationships, and maybe the causes, of Reading ability or lack thereof.
Re-estimate a model
on new data to verify
that the original
estimates generalize
to a new sample.
As always, whenever a model is re-specified and then re-analyzed with the same data
from which it was originally estimated, the training data, cross-validate the new
model on a new sample of data, the test data. The smaller the sample size of the
training data, the greater the need to apply the model to a new data sample, the test
data. Models re-specified on small data sets are at risk of overfitting the model to
the data, modeling random sampling error instead of components that apply to the
population as a whole.
12.7 Analysis of Covariance
Salary and Gender:
Example,
Section 13.2.2, p. 279
In Chapter 13, for the analysis of Gender and Salary of the Employee data set, a
relationship was established: Men at the company, on average, earned $14316.86 more
than women.
Employee data set,
Section 1.3.1, p. 13
However, to demonstrate a relationship does not explain the reason for
the relationship. To repeat one of the key principles of data analysis: Correlation
does not establish causation.
total effect,
Section 12.2.2, p. 252 The non-zero slope coefficient from Gender predicting Salary is a total effect, which
results, in part, from any causal impact of the predictor variable of interest, Gender,
on the target variable of interest, Salary. However, the total effect also results from
the indirect impact on Salary of all variables that correlate with Gender.12.7. ANALYSIS OF COVARIANCE 269
12.7.1 Covariates
random assignment,
Section 7.2.6, p. 137
Best to assess mean differences on the response variable across groups with ran￾dom assignment of the participants to the various groups, the treatment conditions
when conducting an experiment. experiment,
Section 7.2.6, p. 137
Random assignment eliminates the influence of
confounding variables for the assessment of the direct causal impact of a variable
on the response variable. Unfortunately, random assignment is not possible in many
situations, such as with the study of Gender.
confounding variable,
Section 7.2.6, p. 138,
Section 14.2.1, p. 304
We cannot know all possible confounding
variables related to Gender, but we try to identify the most important.
A categorical predictor variable defines the groups. The analysis of the differences
of group means on the response variable is a form of regression analysis called the
one-way analysis of variance, ANOVA.
ANOVA one-way,
Section 8.3, p. 153
ANOVA as a form
of regression analysis,
Section 13.2.3, p. 280
This example adds the number of Years worked
at the company as an additional predictor variable or feature.
covariate:
Continuous variable
added to a regression
model with one or
more categorical
variables.
A continuous variable
that may be a potential confounder, called the covariate, may impact the relation
of the grouping variable on the response variable. The differences among the group
means remain the primary focus of the analysis, but now the group effect may be
enhanced or diminished with the included covariate, better isolating the relative net
effect of the impact of Gender on Salary.
net effect,
Section 12.2.2, p. 253
The multiple regression model in Listing 12.9 evaluates the net effect of categorical
variable Gender on Salary relative to the covariate, Years worked. The analysis is
called the analysis of covariance, or ANCOVA.
ANCOVA:
Multiple regression
model to evaluate
differences among
groups controlling for
the values of
continuous variable.
Salary is measured in USD, so round
the corresponding output to only two decimal digits with the digits_d parameter,
except for the t- and p-values, always reported with three decimal digits.
Regression(Salary ∼ Years + Gender, digits_d=2)
Estimate Std Err t-value p-value Lower 95% Upper 95%
(Intercept) 37933.13 5496.41 6.901 0.000 26750.61 49115.66
Years 3467.77 386.92 8.962 0.000 2680.57 4254.97
GenderW 5170.61 4373.92 1.182 0.246 -3728.19 14069.41
Listing 12.9: Estimated model of net effects of Years experience and Gender.
factor, Section 3.2,
p. 43
As shown in Chapter 13, for a two-level categorical variable, unless explicitly redefined
as an R variable of type factor, the R convention defines a new variable by scoring
the first alphabetically listed level, here M for a man, as a 0, and the remaining level,
here W for a woman, as a 1. The variable is called a dummy variable.
Gender as a dummy
variable, Section 13.2,
p. 278 R names the
corresponding variable GenderW. Entering Gender and Years into the model results
in predictor variables GenderW and Years.
The estimated model from the Regression() output in Listing 12.9 follows.
Yˆ
Salary = $37933.13 + $3467.77XY ears + $5170.61XGenderW
The ANCOVA analyzes the impacts of Gender on Salary holding the value of the
covariate, Years experience, constant, as if everyone worked at the company for the
same number of years.270 CHAPTER 12. MULTIPLE REGRESSION
12.7.2 Homogeneity of Regression
How much does Gender impact Salary as assessed by the ANCOVA with the covariate,
Years of experience? The answer follows from the value for the dummy variable that
represents each value Gender. Write the estimated model separately for each level.
When XGenderW = 0 indicates a man, and when XGenderW = 1, a woman, as shown
in the Regression() output in Listing 12.10.
-- Assume parallel lines, no interaction of Gender with Years
Level M: Y^_Salary = 37933.132 + 3467.771(X_Years)
Level W: Y^_Salary = 43103.742 + 3467.771(X_Years)
Listing 12.10: Analysis of covariance regression models of Salary and Years for different
levels of categorical variable XGenderW , 0 and 1.
The model at each level of the categorical variable, Gender, differs only by the
y-intercept. ANCOVA necessarily imposes the constraint that regression lines for each
level of the categorical variable share a common slope. This imposed constraint is the
assumption called homogeneity of regression.
homogeneity of
regression: Same
slope between
response Y and the
covariate for each
category.
Regression() visualizes the presumed
parallel regression lines for the levels of Gender, shown in Figure 12.4.
Figure 12.4: Analysis of covariance regression lines for of Salary in terms of Years for each
level of Gender based on the assumption of parallel lines.
How to evaluate the homogeneity of regression? The constraint of parallel lines
specifies that the effect of Years experience on Salary is the same for each Gender, no
interaction.
interaction,
Section 9.2, p. 172 Listing 12.11 reports the null hypothesis test of no interaction.
-- Test of Interaction
Years:Gender df: 1 df resid: 32 SS: 300494856.573 F: 2.36 p-value: 0.134
Listing 12.11: Hypothesis test of the interaction of Gender with Years experience.
This analysis supports the null hypothesis of the homogeneity of regression. The
interaction is not significant, p-value = 0.134 > α = 0.05.
This interaction hypothesis test is a basic ANCOVA analysis. Unfortunately, for a12.7. ANALYSIS OF COVARIANCE 271
large sample, practically any divergence from pure parallelism results in a significant
interaction, whereas small samples have inadequate power to detect an existing
interaction. Visualizing independent regression lines for each level of the categorical
variable augments the hypothesis test’s binary outcome with an informal assessment.
Figure 12.5 shows this visualization from the lessR Plot() function. Plot the data
for each Gender level separately according to the by parameter. Specify the linear
model fit lines with parameter fit set at "lm".
Plot(Years, Salary, by=Gender, fit="lm")
Figure 12.5: Regression lines separately computed for Salary and Years at each level of
Gender as the relationships exist in the data.
Figure 12.4 visualizes the regression line for each level of Gender in this particular
sample with the ANCOVA assumption of no interaction of the covariate with the levels
of the categorical variable. Figure 12.5 shows the regression lines as they describe
the data without this constraint. Although the lines cross, indicating at least some
interaction in the sample, their respective slopes are not so discrepant from each
other, particularly considering the small sample size.
12.7.3 Group Differences
In this sample, the slope coefficient for GenderW is positive, b1 = $5170.61. At similar
levels of experience, in this sample women have a higher average salary than men. As
always, does this sample result generalize to the population from which the sample
was obtained?
Both ANOVA and ANCOVA detect differences in population group means. ANCOVA further
considers the influence of at least one covariate in assessing these differences. ANOVA summary
table, Section 8.4,
p. 157
For
either analysis, the ANOVA summary table provides the needed information for this
assessment.
unbalanced design,
Section 9.3.3, p. 188
However, the inclusion of both a categorical and a continuous variable in the analysis
of covariance renders the design inherently unbalanced. As with the standard ANOVA
for an unbalanced design, calculate the Type II sum of squares for the summary
table. Regression() automatically accomplishes the Type II sum of squares for an
analysis of covariance design with a single categorical variable and a single covariate.
Listing 12.12 shows the obtained summary table.272 CHAPTER 12. MULTIPLE REGRESSION
-- Analysis of Variance from Type II Sums of Squares
df Sum Sq Mean Sq F-value p-value
Years 1 10649548806.21 10649548806.21 80.33 0.000
Gender 1 185275866.97 185275866.97 1.40 0.246
Residuals 33 4375123635.25 132579504.10
Listing 12.12: Analysis of covariance summary table with Type II sums of squares.
The Gender dummy variable, GenderW, scores a W as a 1 and a M as a 0. dummy variable,
Section 13.2.1, p. 278 The
sample partial slope coefficient of $5150.61 indicates that for this specific sample of
people who worked at the company for the same number of Years, there is a small
positive increase in average salaries for women. As always, however, evaluate the
sample result in terms of its possible generalization to the population.
The analysis tests the null hypothesis of equal population means for Salary across
the levels of the categorical variable, Men and Women.
Null Hypothesis, H0 : µM = µW
The summary table for the Gender effect reveals a p-value of 0.246.
Net effect of Gender on Salary:
p-value = 0.246 > α = 0.05 for H0 : βGender = 0, do not reject H0
The null hypothesis of no relationship between Gender and Salary, holding constant
the Years employed, the net effect of Gender, cannot be rejected. The Gender partial
slope coefficient is not statistically distinguishable from zero. The small, observed
sample effect is likely due to random sampling error and so fails to generalize to the
population. Instead, interpret the small positive sample result as a random statistical
artifact.
In this sample, each additional year of employment leads to an average Salary increase
of $3468 regardless of the value of Gender. There is a 95% confidence that the true
average increase is somewhere from $2681 to $4255. This effect of Years employed on
Salary is statistically significant.
Net effect of Years on Salary:
p-value = 0.000 > α = 0.05 for H0 : βY ears = 0, reject H0
A positive relationship between Years of experience and Salary has been detected.
12.7.4 Conclusion
The multiple regression model necessarily assesses the net effects of the predictor
variables instead of their total effects. The predicted Salary for men and then for
women based on Gender are their respective sample average Salaries. The respective
men’s and women’s average Salary is mM = $71, 147.46 and mF = $56, 830.60, a
difference of $14316.86.12.7. ANALYSIS OF COVARIANCE 273
Controlling for the number of Years worked, however, this Gender effect vanishes.
This result supports the claim of a direct causal impact of Years experience on the
determination of Salary. The lack of significance for the net effect of Gender indicates
no detected difference in Salary for men and women for any group of employees
who have worked at the company for the same number of years. However, given the
lack of randomization from a proper experiment, additional potential unmeasured
confounding influences cannot be ruled out.
pivot() function,
Section 6.2.3, p. 107
Investigating further, compare the mean number of years worked for men and women,
using the lessR function pivot() with output in Listing 12.13.
pivot(d, mean, Years, by=Gender)
Gender Years_n Years_na Years_mean
1 M 17 1 12.235
2 W 19 0 6.842
Listing 12.13: Obtain the means of Years worked for men and women as a pivot table.
For some reason, women in this company have worked fewer years on average than
men. The two sample averages for Years employed are mM = 12.24 and mF = 6.84.
independent groups
t-test, Section 7.2,
p. 128
The corresponding independent groups t-test with the ttest() function shows that
the difference in average Years worked is significant with a p-value equal to 0.003.
Will this model remain applicable as women gain more experience working at the
company? The reason for this pattern of Years experience and Gender is not clear from
the data and should be explored further. Perhaps women in the past did not apply
for available jobs in equal numbers with men. Perhaps in the past the maternity leave
and support was insufficient and has since been upgraded. Or, perhaps management
used to be dominated by chauvinists who did not hire women, but now all the old
guys are retired, dead, or in jail.
Interpretation. Relation of gender to salary
Women in this workplace are currently free from overt salary discrimination. How￾ever, management should re-examine revised data in a few years to verify if the
gender discrepancy in pay diminishes as women gain more work experience. Manage￾ment should also understand why a greater percentage of women are now working
at the company. Leslie James, the company’s second-highest-paid employee, is a
woman, so perhaps this result indicates a future trend.
Achieving statistical control with the partial slope coefficients from a multiple regres￾sion enhances our understanding of the underlying causality by isolating the relative
net effects.
12.7.5 More Advanced Designs
As of this writing, the lessR function Regression() computes the Type II sum of
squares only for the most straightforward ANCOVA design of a single grouping variable
and a single covariate. With multiple grouping variables or covariates, although the
estimates for the regression coefficients of the resulting model, fit, and other statistics274 CHAPTER 12. MULTIPLE REGRESSION
are correct, the resulting ANOVA output needs to properly reflect the unbalanced design
for assessing mean differences across the groups. The output includes a message that
Anova() warns against relying upon the reported sums of squares for more advanced designs. from the
car package,
Section 9.3.3, p. 190
To obtain the unbalanced ANOVA results more generally, use the Anova() function
from the car package as previously discussed.
12.8 Analysis Problems
The following template guides almost any regression analysis, for homework problems
or real-world analysis.
Note that hand calculations are not needed to answer any of these questions. The
computer does that work. Illustrate the concept with the formulas, but read the
needed numeric values from the output.
Multiple Regression Template for Analysis
Input
a. Show the R/lessR instruction to obtain the analysis. Identify the response
variable or target and the predictor variables or features.
Scatterplot/Correlation Matrix
b. Show the scatterplot, if a single numerical predictor variable, or the scatterplot
matrix and correlation coefficients of the relationship of each of the variables in
the model with each other.
i. Relevance: Do the predictor variables relate to the response variable?
Explain.
ii. Uniqueness: [If multiple predictor variables]
Is collinearity a problem? Explain.
iii. Model Selection: [If multiple predictor variables]
What is the most likely candidate for the final model? Explain.
Estimated Model
c. Write the estimated regression model.
d. Specify and interpret the sample slope coefficient.
e. Show the calculation of the fitted/predicted score for the given values of predictor
variables X.
f. Show the associated residual, its calculation, and interpret for the given values
of predictor variables X and response variable Y.
Hypothesis Test: Applied to one predictor only
g. Specify the null hypothesis and its alternative for the hypothesis test of the slope
coefficient.12.8. ANALYSIS PROBLEMS 275
h. Specify the calculation of the t-statistic by applying the relevant numbers from
this specific analysis.
i. Include and apply the definition of the p-value with the relevant numbers for
this specific analysis.
j. Specify the basis for the statistical decision for the hypothesis test and the
resulting statistical conclusion.
k. Hypothesis Test: Interpretation, as an executive summary you would report to
management.
Confidence Interval: Applied to one predictor only
l. Specify the value that the confidence interval estimates (1st predictor).
m. Apply the definition of the margin of error for its computation by applying the
relevant numbers of this analysis with 2 for the t-cutoff (1st predictor).
n. Show the computations of the confidence interval illustrated with the specific
numbers from this analysis.
o. Confidence Interval: Interpretation, as an executive summary you would report
to management.
p. Demonstrate the consistency of the confidence interval and hypothesis test using
the specific numbers for this analysis for both results.
Model Fit
q. Evaluate fit with the standard deviation of residuals.
r. Evaluate fit with R2 and R2
P RESS, including their comparison.
s. Show any potential outliers and explain why they are outliers.
Assumptions
t. For each fitted value, Yˆ
i
, the average residual value should be zero.
u. Constant population standard deviation of the estimation errors at all values of
X.
v. [For values of the predictor variable collected over time.] The residuals
do not correlate with any other variable, including each other.
w. The estimation errors are normally distributed.
Prediction Intervals
x. For the 95% prediction interval of response variable Y for the given values of
predictor variables X, show the interval including its calculation (can approximate
with the t-cutoff of 2.
y. Interpret the prediction interval.276 CHAPTER 12. MULTIPLE REGRESSION
Model Selection
aa. Consider all the predictor variables simultaneously. Based on the p-values of
the slope coefficients, are any of these much less useful predictor variables for
predicting the response variable (target)? Why or why not?
ab. Any collinearity problems? Why or why not?
ac. Based on the best subset analysis, which model do you recommend? Why?
Conclusion
ad. What decision do you recommend based on these findings to those who assigned
you to do the analysis?
Problems
1. Use the BodyMeas data set to model Weight as a function of Height, Waist, Hips,
Chest, Hand circumference and Shoe size.
?dataBodyMeas
for more information. d <- Read("BodyMeas")
Do the multiple regression analysis according to the template. Do the hypothesis test
and confidence interval for the first predictor variable, Height.
For e., f., and x., use Row 17 of the data table with input values of the predictor vari￾ables: Height=61, Waist=32, Hips=45, Chest=44, Hand=9, Shoe=11, Weight=146.
Note that hand calculations are not needed. The computer does that work. Illustrate
the concept with the formulas, but read the needed numeric values from the output.
2. Use the house_data data set to model the selling price of a home in King County,
WA as a function of the size of the living area, size of the lot, year constructed, and
the grade of construction quality.
Find a description of the 2.34 MB data set and the data at:
https://www.kaggle.com/datasets/arathipraj/house-data
Download the data by clicking on the download icon several inches down the web
page. The data set downloads as house_data.csv.
Do the multiple regression analysis according to the template. Do the hypothesis test
and confidence interval for the first predictor variable, sqft_living.
For e., f., and x., use Row 19470 of the data table with input values of the predictor
variables: sqf_living=2090, sqf_lot=7290, yr_built=1966, and grade=8.Chapter 13
Categorical Regression Variables
13.1 Quick Start
Install R and lessR as explained in Section 1.2. At the beginning of each R session,
access the lessR functions with library("lessR"). From Chapter 2, read your
data into an R data frame such as d with d <- Read("") to browse for the data file.
Or, locate your data file with a path name or web address between the quotes.
Predictor variables, features, can also be categorical. However, the values of categorical
variables do not have numerical properties even if originally coded as integers. How
can non-numeric variables be predictor variables in regression models? The answer
is to convert the non-numerical variables to numerical variables called indicator
variables, a clever trick explored in this chapter.
indicator variables,
Section 13.2, p. 278
factor() function,
Section 3.2.1, p. 44
If any of the predictor variables in the model are categorical and encoded either as
character strings or as R factors, then R automatically does the conversion to numerical
variables so that the regression analysis proceeds by default. If the categorical variable
is integer coded, then usually need to convert the variable to a factor for the automatic
conversion to work. The analyst can also customize the conversion of the categorical
variable to numerical variables.
Prediction also includes predicting the values of a categorical variable, which requires
methods beyond the least-squares of traditional regression analysis. This chapter
also introduces the prediction of the values of a binary response variable or target.
Applying machine learning vocabulary, a categorical response variable is a label, which
refers to the name of each category.
If the response variable is categorical use logistic regression. The discussion here
pertains to the most common application of logistic regression, a binary response
variable.
logistic regression,
Logit(Ybinary ∼ X1 + X2) Section 13.4, p. 285
Ybinary is a generic reference to variable with only two values.
277278 CHAPTER 13. CATEGORICAL REGRESSION VARIABLES
13.2 Indicator Variables
The previous examples of regression analysis involved continuous predictor variables.
To do regression analysis with a categorical predictor variable, convert the levels, the
categories, to numeric variables called indicator variables.
indicator variable:
Variable with only
two unique values
that indicates group
membership.
A new variable can be
created for each level of a categorical variable.
13.2.1 Dummy Variables
The values of indicator variables correspond to the levels of a categorical variable
that define groups. A categorical variable defines at least two groups. Table 13.1
presents some examples.
Variable A Corresponding Group
Gender Woman
Blood Type O
Political Party Green Party
College Major Psychology
dummy variable: Table 13.1: Examples of categorical variables and one associated level (category).
Indicator variable
with only two unique
values, 0 or 1, that
indicates group
membership.
A common type of indicator variable is a dummy variable, a variable with only two
unique values, 0 or 1. Define a dummy variable for each group or level of a categorical
variable. Assign a value of 1 to the value of the dummy variable if the instance or
sample is a member of that group, and a value of 0 if it is not. dummy variable in
ANCOVA,
Section 12.7, p. 268 To illustrate, in a given data set Gender has two levels, M and W, for man and
woman, respectively. Create two dummy variables for Gender, here named GenderM
and GenderW as illustrated in Table 13.2. For GenderM, score the dummy variable a
1 if the person is a man and 0 if otherwise. For GenderW, score the indicator variable
a 1 if the person is a woman and 0 otherwise.
Gender GenderM GenderW
W 0 1
W 0 1
M 1 0
W 0 1
Table 13.2: Data table of four people for categorical variable Gender and its two correspond￾ing indicator variables, here named GenderM and GenderW.
The same logic applies to as many values of Gender are recorded in a specific data
set. For each level, define a dummy variable. Assign a value of 1 if a person matches
that level and a 0 otherwise.
The variable Gender in this data set has k = 2 levels. To know the value of either
dummy variable for Gender is to know the value of the other dummy variable. If the
value of GenderW for a person is a 1, then the value for the remaining dummy variable,
GenderM, necessarily is a 0, and vice versa. The two dummy variables together13.2. INDICATOR VARIABLES 279
are redundant, perfectly collinear. collinear predictor
variable,
Section 12.6.1, p. 264
To avoid this redundancy, when constructing a
regression model, only specify one dummy variable for a variable with two unique
values.
In general, for any type of indicator variable, for k levels of a categorical variable, only
k − 1 indicator variables describe the values of the categorical variable. Accomplish
the regression analysis with just these k − 1 indicator variables. In this example,
perform the regression with one dummy variable to represent Gender with either one
of its two dummy variables.
The slope coefficient in a regression model for the Gender dummy variable in the
analysis stays true to the general definition of a slope coefficient: The average change
in the response variable when the predictor variable is increased by one unit, holding
the values of any other predictors in the model constant. In the case of a dummy
variable, change the value from 0 to 1, which in the example in Table 13.2, changes its
value from a Man (0) to a Woman (1). This slope coefficient represents the average
change in salary for women in comparison to men.
13.2.2 Dummy Variable Regression
The core least-squares R regression function is lm() for “linear model”. The lessR
Regression() function relies upon lm() for the estimates of the coefficients and
other information for the proposed model. The lm() function automatically creates
the needed dummy variable(s) from a categorical predictor variable. If the values of
the categorical variable are integers, first encode the variable as a factor variable to
instruct R to treat the variable as categorical. factor variable,
Section 2.2, p. 22
In this example, the variable Gender from the Employee data set is the predictor
variable in the call to Regression() with values M and W.
Employee data table,
Figure 1.5, p. 13
Input. Regression analysis with a categorical variable
d <- Read("Employee")
Regression(Salary ∼ Gender)
R names the resulting indicator variable that it creates GenderW, a juxtaposition
of the name of the categorical variable and the name of the category or level W. factor variable,
Section 3.2, p. 43
By
default, R orders the levels of a categorical variable alphabetically, so M is the first
level of Gender and W is the second. Another R convention is that the regression
analysis retains the last k − 1 indicator variables, dropping the first indicator variable.
In this example, GenderW is retained as the dummy variable and the redundant
GenderM is dropped.
The estimated regression coefficients appear in Listing 13.1.
Estimate Std Err t-value p-value Lower 95% Upper 95%
(Intercept) 81147.458 4914.071 16.513 0.000 71171.364 91123.552
GenderW -14316.860 6857.494 -2.088 0.044 -28238.314 -395.406
Listing 13.1: Estimated model for Salary regressed on Gender.280 CHAPTER 13. CATEGORICAL REGRESSION VARIABLES
From the output in Listing 13.1, write the estimated linear model.
Yˆ
Salary = $81, 147.46 − $14, 316.86(XGenderW )
Both coefficients are significant, with p-values less than 0.05. The y-intercept, as
always, is the value of y when x = 0, here the value of Salary when the dummy
variable in the analysis, GenderW, is 0. So the intercept is the average value of Salary
for the first level of Gender, M, b0 = mM = $81, 147.46.
The average change in Salary for a 1-unit increase in the indicator variable, moving
from Man to Woman, is $14, 317 less than men per year.
mW − mM = $66, 830.60 − $81, 147.46 = −$14, 316.86
Regression() automatically creates the scatterplot and regression line illustrated in
Figure 13.1. To highlight the mean difference, Regression() draws horizontal lines
through the group means.
Figure 13.1: Scatter plot and regression line from Regression() of the categorical variable
Gender with Salary.
The inclusion of categorical predictor variables in a regression model relates to the
analyses of group comparisons regardless of the number of groups. reference group of
a dummy variable:
The level of a
categorical variable
that has no
corresponding
indicator variable in
the created contrast
matrix.
By default, when
creating a dummy variable, R defines the first level of the categorical variable as the
reference group. In general, the slope coefficients in the resulting regression analysis
are defined in terms of how different the mean of the dummy variable group is from
the mean of the reference group. For two groups, such as Man and Woman, there is
only one comparison.
13.2.3 General Linear Model
Dummy variables in regression analysis lead to the comparison of means across
groups. These comparisons can be accomplished with at least three different statistical
procedures. The p-value from a t-test of the mean difference is identical to the p-value
from a t-test of the sample slope coefficient estimated by a least-squares regression
ttest() function, analysis, as are the corresponding confidence intervals.
Section 7.2, p. 128
These results also generalize
to analysis of variance. All three analyses are applications of the general linear model13.3. CUSTOM INDICATOR VARIABLES 281
that expresses a predictive mode of response (target) as a function of one or more
predictor variables (features), which are categorical.
Input. Three equivalent analyses of a mean difference
ttest(Salary ∼ Gender)
ANOVA(Salary ∼ Gender)
Regression(Salary ∼ Gender)
Each of these three analyses of the same linear model yields identical results for the
hypothesis test of the mean difference of the Salary for Men and Women. A summary
of the output for all three analyses follows where µM and µW are the respective
population means of Salary for men and women.
Gender effect on Salary:
t-test H0 : µM − µF = 0, p-value = 0.044 < α = 0.05
ANOVA H0 : µM = µF , p-value = 0.044 < α = 0.05
Regression H0 : βGender = 0, p-value = 0.044 < α = 0.05
Each analysis detects the difference between the average Salary of men and women.
The difference is statistically significant at the typical α = 0.05 threshold. At this
company, men do have a higher average salary than women.
t-test between
groups, Section 7.2.1,
p. 128
Each subsequent analysis of a linear model listed above is more general. The t-test
compares the means of the response variable across two groups with the t-value. The
one-way ANOVA procedure compares two or more means by a ratio of two variances,
an F-value.
ANOVA one-way
between groups,
Section 8.3, p. 153
Of course, ANOVA can analyze the impact of multiple categorical variables
on the response variable. The regression procedure is the most general.
ANOVA factorial,
Section 9, p. 171
Regression
analysis can evaluate the response variable for changes in the value of any combination
of categorical and continuous predictor variables.
This analysis established the difference between average men’s and women’s salaries
at the company. total effect,
Section 12.2.2, p. 253
This slope coefficient, however, is a total effect. How much average
Salary changes between the two values of Gender does not separate the direct effect of
Gender on Salary from potential indirect effects. The correlation between Salary and
Gender could result to some extent from a causal relation of other variables to Salary
from which Gender is correlated. correlation and
causation,
Section 14.2, p. 303
Without additional information, simply showing
a difference does not explain why there is a difference, regardless of the statistical
procedure used to show that difference. Correlation is not causation.
13.3 Custom Indicator Variables
dummy variables,
Section 13.2.1, p. 278
As previously discussed, R automatically converts the k levels of a categorical variable
entered into regression analysis as predictor variables in the form of k − 1 dummy
variables. The corresponding slope coefficients indicate the difference between the
mean of that group and the reference group mean. However, dummy variables are
just one type of indicator variable, with each value either 0 or 1.282 CHAPTER 13. CATEGORICAL REGRESSION VARIABLES
13.3.1 Contrast Matrix
Indicator variables are not limited to the values of 0 and 1 for dummy variables.
Different values of indicator variables allow for comparisons that go beyond comparing
two means with each other. Instead, compare different sets of means with each other
according to combinations of the predictor variables. For example, compare the
average of the means of the first two groups versus the mean of the third group. To
define these more general comparisons, the values of indicator variables may assume
different integers and even real numbers, those with decimal digits.
contrast matrix:
Table with each
column an indicator
variable and each
row a level of the
categorical variable
to transform to an
indicator variable.
Construct indicator variables according to a table called a contrast matrix. Each
column of the contrast matrix represents an indicator variable and each row represents
a level of the corresponding categorical variable. The contrast matrix specifies how
to transform the levels of a categorical variable into indicator variables, which occurs
either by default as in the case of dummy variables or by explicit definition when
doing a regression analysis. contrasts()
function: Create a
contrast matrix from
the levels of a
categorical variable.
The R function contrasts() displays the contrast matrix for a categorical variable
defined as a factor. To illustrate, read the lessR data set "Anova_1way" into the d
data frame with grouping variable drug Dosage and response variable Time. Anova_1way data,
Section 8.1, p. 154
factor variable,
Section 2.2.2, p. 24
Then,
with the R factor() function, convert Dosage with values of 00mg, 05mg, and 10mg
from type character as read into R into a variable of type factor. Retain the same
levels as the category labels and the same order, so the call to factor() does not
need the parameters levels and labels.
Input. Read data into data frame d and create a factor variable
d <- Read("Anova_1way")
d$Dosage = factor(d$Dosage)
R constructs a default contrast matrix for a factor variable to create dummy variables,
displayed in Listing 13.2. This is the contrast matrix automatically applied to create
the dummy variables for a regression analysis of a categorical variable. For the three
levels of Dosage, two dummy variables are defined, one each for levels 05mg and
10mg.
contrasts(d$Dosage)
05mg 10mg
00mg 0 0
05mg 1 0
10mg 0 1
Listing 13.2: Default contrast matrix that defines two indicators variables with dummy
coding for the three levels of categorical variable, Dosage.
The reference group of 00mg has no corresponding dummy variable, so any row of
data with the level of 00mg is scored 0 on both dummy variables. A data value for
the variable Dosage in the 05mg group gets scored a 1 on the corresponding dummy
variable and a 0 on the dummy variable for the 10mg group. Similarly, a data value13.3. CUSTOM INDICATOR VARIABLES 283
in the 10mg group scored a 0 on the 05mg dummy variable and a 1 on the 10mg
variable.
13.3.2 Effects Coding Regression
Instead of defining another level of the categorical variable as the reference group,
reference the mean of the response variable across all rows of data. Each slope
coefficient indicates how much the corresponding group is above or below the overall
mean.
effects coding
result: Each
estimated slope
coefficient is the
difference from the
overall mean of the
response variable.
The indicator variables consistent with these group contrasts implement effects
coding. Change the definition of the indicator variables by changing the specification
of the contrast matrix.
contr.sum()
function, R: Compute
a contrast matrix for
effects coding.
The R contr.sum() function computes the effects coding contrast matrix, here for
factor variable Dosage. The required parameter is n, the number of levels of the
corresponding categorical variable.
contrasts(d$Dosage) <- contr.sum(n=3)
Unfortunately, contr.sum() does not properly name the created indicator variables,
so enter their names with the R function colnames(). As always, for a categorical
variable with k levels, create k − 1 indicator variables, two for the three levels of
Dosage. Unlike the creation of dummy variables, by default, contr.sum() does not
create an indicator variable for the last level of the grouping variable Dosage, so only
include the first two levels of Dosage as names. The first two levels, again by default,
are alphabetically ordered in this example.
colnames(contrasts(d$Dosage)) <- c("00mg", "05mg")
Effects coding requires that the values for each indicator variable sum to 0. The
values that define the column for each indicator variable balance each 1 with a −1 for
the values of the last group. Listing 13.3 shows the redefined effects coding contrast
matrix.
contrasts(d$Dosage)
00mg 05mg
1 1 0
2 0 1
3 -1 -1
Listing 13.3: Contrast matrix for effects coding for a categorical variable, Dosage, with
three levels.
With the contrast matrix for Dosage redefined to specify effects coding, that coding
is automatically applied to the subsequent regression analysis in place of the default
dummy coding. The estimated coefficients and their inferential analysis are shown in
Listing 13.4. The intercept term, b0 = 21.833 is the reference for the interpretation of
the slope coefficients, the grand mean of all the response Time data across all three284 CHAPTER 13. CATEGORICAL REGRESSION VARIABLES
groups. Interpret the two slope coefficients, one for each indicator variable, in relation
to this grand mean.
reg(Time ∼ Dosage))
-- Estimated Model for Time
Estimate Std Err t-value p-value Lower 95% Upper 95%
(Intercept) 21.833 0.656 33.262 0.000 20.468 23.198
Dosage00mg 2.429 0.928 2.617 0.016 0.499 4.360
Dosage05mg 1.579 0.928 1.701 0.104 -0.351 3.510
Listing 13.4: Estimated regression model for Time with effects coding of the categorical
variable, Dosage.
To understand the interpretation of the slope coefficients, view the means in List￾ing 13.5.
pivot(d, mean, Time, by=Dosage)
Dosage Time_n Time_na Time_mean
1 00mg 8 0 24.262
2 05mg 8 0 23.413
3 10mg 8 0 17.825
Listing 13.5: Group means for Time to complete the maze depending on Dosage.
The Time to complete the maze for the 10mg group is the fastest, increasing by 2.429
seconds over the average score for all groups, b0 = 21.833, for a group completion
Time mean of 24.262. The 5mg Dosage average group score is 1.579 seconds above
the mean of 21.833, or 23.412 secs. Both means are the same as obtained with the
ANOVA, Section 8.2, traditional ANOVA presented in Listing 8.2.
p. 155
For this example, since the 00mg dosage is the control group, it may be useful to
manually customize the contrast matrix to compare the 05mg and 10mg dosages to
the 00mg dosage group with a dummy coding. This and related analyses are left as
an exercise at the end of the chapter.
Why use effects coding? For a categorical variable with only two values (categories,
groups, labels), there is a single indicator variable for that categorical variable. If the
indicator variable is dummy coded in the analysis of only two levels of a categorical
predictor variable, the resulting slope coefficient, mB − mA, is a direct comparison
of the two group means, a natural comparison. For more than two groups, however,
the reference group of the grand mean is sometimes more meaningful than using
another level of the categorical variable as the reference. To move beyond dummy
coded indicator variables with R requires moving beyond the R default.
general linear model, A regression analysis specifies a general linear model.
Section 13.2.3, p. 280
The model that follows from
the effects coding or dummy coding analysis produces the identical ANOVA summary
ANOVA table from table as we obtained for the standard ANOVA.
previous example,
Section 8.4, p. 157
Listing 13.6 shows the ANOVA summary
table from the regression analysis.13.4. BINARY LOGISTIC REGRESSION 285
-- Analysis of Variance
df Sum Sq Mean Sq F-value p-value
Model 2 195.691 97.845 9.462 0.001
Residuals 21 217.162 10.341
Time 23 412.853 17.950
Listing 13.6: ANOVA table from Regression() from the previous one-way ANOVA example.
The expression of a regression model as a general linear model is an essential concept
in data analysis. The advantage in terms of practice is that the flexibility of defining
contrasts provides for designs that can implement many different types of contrasts
beyond the two demonstrated in this chapter. ANCOVA,
Section 12.7, p. 268
Moreover, the general linear model
implementation permits a mixture of categorical and continuous variables (covariates)
in more general, and more complex, ANCOVA designs.
13.4 Binary Logistic Regression
The previous section included categorical predictor variables in a multiple regression
model. binary variable,
Section 13.2.1, p. 278
This section features a model with a categorical response variable with two
categories. Examples of such binary or yes/no variables include data sets with only
two values of Gender (man, woman), Cigarette Use (smoke, do not smoke), Begin
Therapy (begin, do not begin), and Home Ownership (own, do not own).
label: Value of a
categorical response
variable.
In the language of machine learning, to identify the value of a categorical response
variable is to label the value. Analyze a categorical response variable with logistic
regression, specifically binary logistic regression for a binary categorical variable. Build
the model to predict, that is, to label, the object of interest from the information
available from one or more predictor variables, the features.
13.4.1 Motivation
Consider a (real) data table of various body measurements for 340 motorcyclists
fitted for motorcycle clothing. Two genders are represented in this data set, 170 men
and 170 women, with values M and W. The data include measurements for Weight
to the nearest pound, Height, Waist, Hips, and Chest to the nearest inch, Hand
circumference to the nearest quarter of an inch, and Shoe size to the nearest half
size. The data table is part of lessR, accessed via its name dataBodyMeas or the
abbreviation BodyMeas with the lessR function Read(). read lessR data,
Section 2.4.1, p. 30
d <- Read("BodyMeas")
Why do we need a different type of regression analysis for a categorical variable?
least-squares
estimation,
Section 11.4, p. 227
To illustrate, here apply traditional least-squares regression to, in this dataset, the
binary variable Gender with values M and W. When doing logistic regression with
lessR, the response variable can be coded as a character string variable or as a factor
with two values. Of course, for least-squares regression, the response variable must be286 CHAPTER 13. CATEGORICAL REGRESSION VARIABLES
numeric. Accordingly, recode the values of Gender to numeric values 0 and 1. When
interpreting the analysis, it can be more convenient to have a + slope coefficient. The
average Salary for a man is higher in this data set, so recode a W to a 0 and an M to a
recode characters to 1.
0,1, Section 3.4, p. 49
Submit a model with a binary response variable with this encoding to a traditional
least-squares regression program such as Regression().
Regress the response variable Gender on Hand circumference in inches.1 The results
of this conventional regression analysis are problematic as shown in Figure 13.2.
6 7 8 9 10 11
−0.5
0.0
0.5
1.0
1.5
Hand
Gender (0=W, 1=M)
Figure 13.2: Least-squares regression fit and scatter plot of Hand circumference and binary
response variable Gender scored 0 and 1.
The response variable Gender has only two values, 0 and 1, yet the fitted values from
the resulting estimated regression line are continuous. What does a fitted value of
0.75 mean? Or how about an out of range value such as YˆGender = −0.38?
Another problem is that the least-squares estimation procedure applied to a binary
response variable necessarily violates some assumptions. regression
assumptions,
Section 11.7, p. 241
First, from the mathematics
of the variance of a binary variable, the residuals from a binary response variable
cannot have the same variance for different values of the predictor variables. Also,
the residuals cannot be normally distributed because each residual is calculated with
subtraction from only either 0 or 1.
Residuals for a binary response: Yˆ
i = Yi − 0 or Yˆ
i = Yi − 1
Instead of continuously distributed across their range according to a normal distri￾bution, the residuals cluster only around these two values. To address these issues,
Figure 13.3 depicts the updated fitted curve for logistic regression.
13.4.2 Logic
The correct modeling of a binary response variable yields the probability that each
example or instance belongs to the reference group. Given a person’s Hand size,
1Define variable G01 as the numerical 0,1 coding of Gender. To control the scaling of the y-axis,
Plot() creates Figure 13.2 according to:
Plot(Hand, G01, fit="lm", fit_se=0, jitter_y=0, scale_y=c(-0.5,1.5,4),
ylab="Gender (0=W, 1=M)")13.4. BINARY LOGISTIC REGRESSION 287
what is the probability that the person is a Man? These probabilities, of course, vary
continuously from 0 to 1.
The path to the probability of a label begins with the concept of its odds. odds of an event:
Ratio of the
probability of the
event occurring to
not occurring.
odds =
p
1 − p
The data for half of the people in the BodyMeas data table are men and half are
women. The probability of randomly selecting a row of data for a Man is 0.5. The
odds of this selection are 0.5/0.5 = 1, often expressed as 1 to 1. If 10% of the rows
of data were from men, then the odds for this selection would be 0.1/0.9 ≈ 0.11 or
0.11 to 1. Similarly, for a probability of 0.9 the associated odds are 0.9/0.1 = 9 or
9 to 1. The greater the probability of the event, the greater its odds. The smallest
probabilities yield odds close to 0, and the closer the probability is to 1, the larger
the odds.
We wish to model the response variable over all possible values, yet odds can only be
positive numbers. However, the logarithm of the odds varies from negative to positive
infinity. The odds of an event with a probability of 0.5 are 1. The corresponding
logarithm of 0 is the boundary between an event with either less or more than a
0.5 probability. A probability close to zero yields a very large negative value. A
probability closer to 1 yields a substantial positive value.
logit: The logarithm
of the odds for the
occurrence of an
event.
This logarithm of the odds is the logit transformation, the response variable for the
logistic regression. With some algebra applied to the original model with binary Y
as the response variable, express the logit transformation as the response variable
for a linear function of the predictor variables. The notation log() is the natural
logarithm of the resulting expression.
logit(p) = log
p
1 − p

= b0 + b1X1 + b2X2 + . . . bmXm
Each partial slope coefficient from this logistic model, bj , specifies how the logarithm
of the odds change for a 1-unit increase in each of the predictor variables, with the
values of the other predictor variables held constant. The estimated model expresses
the odds for obtaining the value of the response variable of 1.
maximum
likelihood solution:
Choose estimated
values for the model
that maximize the
likelihood of the data
for that model.
According to the previously discussed least-squares principle, there is no mathematical
solution for these estimates. Fortunately, there is a method for estimating these
coefficients known as maximum likelihood, which selects parameter estimates that
maximize the conditional probability of the data for any given set of parameters.
These conditional probabilities are likelihoods. From one set of initial estimates, the
likelihood is computed. The estimates are then incrementally adjusted to produce a
higher likelihood for each iteration. A logistic regression output includes the number
of iterations performed before finding the best solution from the initial estimates.
13.4.3 Estimation
To illustrate, apply logistic regression to the BodyMeas data, real data collected for
motorcycle riders ordering motorcycle clothing online.288 CHAPTER 13. CATEGORICAL REGRESSION VARIABLES
Scenario. Predict Gender from body measurements
A customer’s Gender may be omitted from an online order for clothing. How well
can Gender be predicted from available body measurements?
Because Gender has only two values in these data, the regression model to label a
customer’s Gender should be binary logistic regression. Do this regression with the
lessR function Logit(). First, try a logistic regression model with only a single
predictor, here Hand circumference in inches. For ease of interpretation, Logit()
orders the levels of a single predictor model so that the slope for the predictor is
positive. Regardless of the value labels of the response variable, R does the analysis
with the numerical values for the response variable of 0 and 1. Men have larger hand
sizes, on average, so to maintain a positive slope, Logit() codes Woman as a 0 and
a Man as 1.
The syntax of Logit() is the same as for Regression(), the general R syntax
specification of a model.
Input. Logistic regression with a single predictor
Logit(Gender ∼ Hand)
The first part of the text output to the R console appears in Listing 13.7.
Estimated Model for the Logit of Reference Group Membership
Estimate Std Err z-value p-value Lower 95% Upper 95%
(Intercept) -26.9237 2.7515 -9.785 0.000 -32.3166 -21.5308
Hand 3.2023 0.3269 9.794 0.000 2.5615 3.8431
Listing 13.7: Estimated coefficients from logistic regression analysis.
The result has the identical form of the least-squares regression analyses previously
presented. Accompanying each estimate of the regression model is its standard error.
From the estimate and its standard error, construct the hypothesis test that the
corresponding population value is 0, and associated 95% confidence interval.
Evaluate each estimate with the null hypothesis of a zero population value for
the corresponding slope coefficient. The sample estimate from the logit model is
bHand = 3.202.
Effect of Hand size: p-value < α = 0.05, so reject H0 : βHand = 0
The slope coefficient for a 1-unit increase in Hand circumference indicates the expected
change in the logistic function is 3.20. But what does this expected change mean?
The response variable for this analysis is the logit, the logarithm of the odds, not
usually what one would describe as the variable of interest, so the resulting output
variable should be converted back to the odds ratio.13.4. BINARY LOGISTIC REGRESSION 289
13.4.4 Odds Ratio
odds ratio: The
ratio of change in the
odds that the binary
response variable
equals 1 as the value
of the predictor
variable is increased
1 unit.
Fortunately, a simple expression permits a straightforward interpretation of how
the change in the value of X impacts the odds for the value of 1 for the response
variable, here Man. The algebra applies the exponential function to each side of the
model, accomplished in R with the exp() function. The exponential function converts
a subtraction to a division. Compare the change of the odds from changing the
value of the predictor variable as a ratio. The result is the odds ratio.
exp() function:
Exponentiation.
For example,
exponentiate the estimated slope coefficient for Hand with exp(3.2023), which yields
24.59.
− relationship
between X and Y:
Odds ratio is less
than 1.0.
An odds ratio of 1.0 indicates no relationship between predictor and response variables.
An odds ratio of .5 indicates that a value of 1 for the response variable is half as likely
with an increase of the predictor variable by 1 unit, an inverse relationship. As the
predictor value increases, the value of the response variable decreases. + relationship
between X and Y:
Odds ratio is greater
than 1.0.
Values of the
odds ratio over 1.0 indicate a positive relationship of the predictor to the probability
that the value of the response variable is 1.
The Logit() function automatically displays the odds ratios and the corresponding
95% confidence intervals, shown in Listing 13.8.
Odds Ratio Lower 95% Upper 95%
Hand 24.5883 12.9547 46.6690
Listing 13.8: The estimated odds ratio for each coefficient and associated 95% confidence
interval.
The odds ratio in Listing 13.8 is considerably larger than 1.0, so Hand circumference
positively relates to being a Man. The odds are for a value of the response variable
equal to 1, that is, in reference to labeling an example of a Man from the sample
of 340 people. Specifically, these odds are almost 25 times as likely, 24.59, for each
additional inch of Hand circumference that the person is a Man. In the population,
this odds ratio, with 95% confidence, is between 13.53 and 48.93.
Measuring hand size in inches results in a range of sizes between 6 and 12. Each
inch spans a considerable portion of the range from 6 to 12 inches. The result is a
dramatic increase of the odds that a random selection of people from a sample all
with the same increased Hand size yields a Man. The odds ratio is so much larger
than 1.0 because of the unit of measurement, inches, as a percentage of hand size.
To illustrate, convert the Hand measurements from inches to millimeters by multiplying
each Hand measurement by 2.54 and re-run the analysis.
d$Hand.mm <- d$Hand.mm = 2.540*d$Hand)
Logit(Gender ∼ Hand.mm)
A millimeter is much smaller than an inch, so the size of the resulting odds ratio
decreases dramatically, from 24.59 to 3.53. The odds of selecting a Man for each
increase of 1 mm in Hand circumference increase, but the increase is smaller than290 CHAPTER 13. CATEGORICAL REGRESSION VARIABLES
the larger inch Different units for the two analyses. Regardless of the scale, inches or
mm, both analyses describe the same relationship of Gender and Hand Size.
13.4.5 Fit Indices
For a maximum likelihood solution such as for logistic regression, there is no multiple
regression least-squares minimization to obtain these statistics, nor an R2 fit index.
However, there is still the concept of a target variable and the fitted value of the
target by the model, the difference the modeling error. The primary direct indices
of model fit based on these errors are the null and residual deviance. Listing 13.9
reports these fit indices for this logistic regression analysis, upon which other indices
can also be derived (Tjur, 2009).
Null deviance: 471.340 on 339 degrees of freedom
Residual deviance: 220.664 on 338 degrees of freedom
AIC: 224.6641
Number of iterations to convergence: 6
Listing 13.9: Fit indices and number of iterations until convergence.
null model,
Section 11.4.2, p. 230
As with the null model from least-squares analysis, the null model contains only an
intercept term, without predictor variables. To evaluate fit, compare null model fit
to the fit of the proposed model with the given predictor variables (features). The
residual deviance informs how well the response variable can be predicted by a model
with the predictor variables. In this example, there is a substantial reduction of
deviance from 471.340 for the null model to 220.664 for the model of interest.
Also shown in Listing 13.9 is the number of iterations the algorithm required to
achieve an optimal maximum likelihood solution.
iteration: One of a
series of steps in
which the parameter
estimates are revised
to improve the fit of
the model to the
data.
The algorithm begins with initial
estimates, and the continually refines those estimates over a series of steps called
iterations. The goal is for the iterations to eventually converge to a solution in which
the model estimates change very little from iteration to iteration.
13.4.6 Classification
Ultimately, practical success of the model is the correct labeling of the examples
rather than by the model fit indices. Does the model correctly label the response
category given the information from one or more predictor variables or features? How
do we evaluate the success of the classification?
Sigmoid Curve
Regardless of the factor label, such as M or W, or Man or Woman, the logistic
regression proceeds with the indicator variable version of the response variable, scored
as a 0 or a 1. Base the classification on the probability of membership in the reference
group, the group scored as a 1 in the logistic regression analysis.
reference group:
The group or
category with a label
of 1 where the
variable has two
values, 0 or 1.
Choose a cutoff
threshold such as the default value of 0.5. If the probability of obtaining a 1 for the13.4. BINARY LOGISTIC REGRESSION 291
value of the response variable is greater than or equal to 0.5, assign the case (example,
instance) to the reference group. If the probability is less than 0.5, assign the case to
the group with the response variable equal to 0.
Instead of the straight line of linear regression, the corresponding output of a logistic
regression is the probability of classifying each example in the reference group, P(Y=1).
logit, Section 13.4.2,
p. 287 To obtain this probability, apply a little algebra to the logistic regression model, the
logit or logarithm of the odds ratio of the binary response variable. Express the
logit as a linear function of the predictor variable, then solve for the probability of
reference group membership. sigmoid function:
Provides P(Y=1)
given the values of
the predictor
variable(s).
The result is the sigmoid function:
P(Y = 1) = 1
e−(b0+b1Xi)
The plotted probabilities follow the sigmoid curve for the obtained values of b0 and
b1, the output of Logit() in Figure 13.3 for a single predictor variable.
Figure 13.3: The sigmoid curve for the probability of Male for various Hand sizes imposed
over the scatter plot of Hand circumference in inches and Gender.
Any value for the sigmoid function on the x-axis from negative infinity to positive
infinity gets mapped into a probability value from 0 to 1. linear regression fit,
Figure 13.2, p. 286
Because Figure 13.3 contains two visualizations, there are two y-axes that correspond
to two different variables. The right-side vertical or y-axis labels the two values of
the response variable for the corresponding scatterplot of the data for Hand size and
Gender. The left-side vertical axis spans the values of the sigmoid function from 0
to 1 to represent the probability of being a Man related to Hand size, in which all
intermediate values between 0 and 1 are applicable.
Corresponding to each probability threshold for labeling an example as a 0 (W) or
1 (M) is the cutoff for the value of the predictor variable, the feature, Hand size.
Logit() provides both values, shown in Listing 13.10.
Probability threshold for predicting M: 0.5
Corresponding cutoff threshold for Hand: 8.408
Listing 13.10: Probability and Hand size thresholds for classification according to Gender.292 CHAPTER 13. CATEGORICAL REGRESSION VARIABLES
The classifier labels all people with a hand size larger than 8.41 inches as M. All
people with a smaller hand size are labeled as W. Of course, perfect classification
is not possible, but how do we evaluate the efficacy of the classification that we do
have?
Classification Fit Indices
True +: Correctly
label as member of
the reference group.
True -: Correctly
label as not a
member of the
reference group.
One way to correctly label the person (example or instance) labels someone in the
reference group as a member of that group, a True +. Or, correctly label someone
not in the reference group, a True -. An incorrect classification mistakenly labels
someone who is not a member of the reference group as a member, the False +, such
as incorrectly labeling a women with a larger hand size as a man.
False +: Incorrectly
label as a member of
the reference group.
False -: Incorrectly
label as not a
member of the
reference group.
Or, mistakenly
label a reference group member as not a member, the False -, such as labeling a man
with a small hand size as a woman. Read False + as falsely labeling an instance as
positive. Similarly, False - means falsely labeling an instance as negative.
Of course, in this context, “positive” and “negative” are not value judgments but
rather neutral notation for whether or not a person is a member of the reference
group. Because the average Hand size for men is larger than that of women, the slope
coefficient of Hand size is positive in this example. Aside from the potential ease of
interpretation of a + or - coefficient, there is no compelling reason to designate one
Gender as the reference group over the other.
These four outputs reflect what the model correctly labels and what it mistakenly
labels. To assess prediction effectiveness, as always, compare reality, Yi
, with what
the model claims is reality, Yˆ
i
. For a binary classification analysis, the difference
between reality and the model’s prediction reduces to the 2x2 table in Figure 13.4
that tallies the counts for each of the four outcomes.
Figure 13.4: Classification status overlaid on the scatterplot of Gender with Hand size.
For the probability cutoff threshold of 0.5, according to the logistic curve, the
corresponding Hand size is a little larger than 8.41. Women with a Hand size larger
than this value are mis-classified as men, and men with hand sizes less than this value
are mis-classified as women. The corresponding table of classifications is a confusion
matrix.
Confusion matrix
: Table that lists the
numbers of the four
outcomes of a binary
classification.
From this table, multiple classification fit indices are constructed.
How do the correct classifications compare to the mis-classifications? For the BodyMeas13.4. BINARY LOGISTIC REGRESSION 293
data set both men and women are equally present, so the baseline rate of correct
predictions, from what could be called the null model, is 50%. Randomly guessing
Man or Woman to label the value of Gender results in 50% correctly labeling each
person’s Gender.
null model,
Section 11.4.2, p. 230 Applying our model with Hand circumference to predict Gender
increases the percentage of correct predictions from 50% to 88.2%, as shown in
Listing 13.11.
Probability threshold for predicting M: 0.5
Corresponding cutoff threshold for Hand: 8.408
Baseline Predicted
---------------------------------------------------
Total %Tot 0 1 %Correct
---------------------------------------------------
1 170 50.0 17 153 90.0
Gender 0 170 50.0 147 23 86.5
---------------------------------------------------
Total 340 88.2
Listing 13.11: The confusion matrix from Logit() of Hand size predicting Gender with a
probability threshold of 0.5.
Listing 13.11 indicates that the model correctly labeled Gender for 88.2% of all
examples, with slightly more Men correctly labeled, 90.0%, than Women, 86.5%.
accuracy:
Percentage of correct
classifications.
This
classification fit index is called accuracy.
accuracy =
True positives + True negatives
all outcomes
Of course, this fit index is assessed on the training data. testing data,
Section 12.5.3, p. 262
To properly assess, accuracy
must be evaluated on new data, the testing data, such as part of the data set held
out of the estimation (learning) phase of the analysis for a separate assessment.
Accuracy places the same emphasis on both types of errors, False + and False -.
Treating both types of errors equally is reasonable when the cost of each error is the
same. When labeling Gender to provide good fitting clothing to fulfill an online order,
the same emphasis on cost of the errors is reasonable. However, the impact of the
two types of errors differs in many situations.
Consider a critical manufactured part for an airplane with the reference group defined
as parts that fail. A failed part likely leads to a crashed airplane, so essential to
avoid False -’s, a part labeled by the model as successful (Yˆ = 0) when in fact
the part is defective (Y = 1). If the concern is to minimize False -’s, focus on the
sensitivity classification fit index, the proportion of actual +’s correctly labeled. With
the number of False -’s in the denominator, sensitivity can span a range from 0 with
all positive instances classified as negatives, to 1 with no False -’s.
sensitivity:
Proportion of actual
positive occurrences
correctly labeled as
positive.
sensitivity =
true positives
true positives + False negatives =
true positives
actual positives
=
153
153 + 17
= 90.00
Another example is the detection of spam emails, where spam specifies the reference
group. Spam is frustrating, but it is more important not to miss legitimate emails294 CHAPTER 13. CATEGORICAL REGRESSION VARIABLES
or mistake a good email (Y = 0)for spam (Yˆ = 1), a False +. If the concern is to
minimize False +’s, focus on precision, the proportion of predicted positives correctly
labeled. With False +’s in the denominator, the fewer False +’s, the higher the
precision, a value that can range from 0 to 1. precision:
Proportion of
correctly labeled
outcomes among the
outcomes labeled as
positive.
precision =
true positives
true positives + False positives =
true positives
predicted positives
=
153
153 + 23
= 86.93
Listing 13.12 presents the Logit() output for each fit index that follows from the
classification table.
Accuracy: 88.24
Sensitivity: 90.00
Precision: 86.93
Listing 13.12: Three primary fit indices to evaluate classification efficacy.
Unfortunately, modifying the classification algorithm to reduce the occurrences of
one form of binary classification mistake necessarilyincreases the occurrences of the
other type of error.
The Probability Threshold
The previous example used the default probability threshold of 0.5 for classification.
Change this threshold with the lessR parameter prob_cut. For example, lowering
the threshold to 0.4 for predicting a man from hand size increases the number of
predictions for a man from the same data set. Some of these instances now predicted
to be a man will, in actuality, be female, so with men as the reference group the
number of False positives will increase.
To illustrate, decrease the probability threshold from its default value of 0.5 to 0.2.
Listing 13.13 shows the resulting confusion matrix from Logit().
Logit(Gender ∼ Hand, prob_cut=0.2)
Probability threshold for predicting M: 0.2
Corresponding cutoff threshold for Hand: 7.975
Baseline Predicted
---------------------------------------------------
Total %Tot 0 1 %Correct
---------------------------------------------------
1 170 50.0 2 168 98.8
Gender 0 170 50.0 87 83 51.2
---------------------------------------------------
Total 340 75.0
Listing 13.13: Confusion matrix that results from a probability threshold of 0.2 for labeling
an instance as a man.
Lowering the probability threshold from 0.5 to 0.2 lowers the corresponding threshold
for Hand size from 8.408 to 7.975 inches. As a result, the number of False -’s,13.4. BINARY LOGISTIC REGRESSION 295
dramatically decreases from 17 to 2. However, there is a simultaneous large increase
in the number of False +’s, from 23 to 83. Avoiding so many more False -’s increases
sensitivity from 92.35 to 98.82. The downside is the corresponding increase in False +’s,
which decreases precision from 87.22 to 66.93. testing data,
Section 12.5.3, p. 262
The choice of probability threshold
must carefully balance the numbers of the two types of errors, and should also be
evaluated on testing data.
13.4.7 Outliers
outlier analysis,
Section 11.6, p. 236
The Logit() function provides the same outlier analysis from the least-squares
regression analyses from Regression(). The scatter plot of the data in Figure 13.3
reveals that the most deviant data values are for Men with a Hand circumference of 7
inches, and for Women with a value of 9.5 inches. influence statistics,
Section 11.6.2, p. 238
These are also the four values that
have an R-Studentized residual larger in magnitude than 2, shown in Listing 13.14.
They also have the largest values of Cook’s Distance.
Hand Gender fitted residual rstudent dffits cooks
125 7.0 M 0.0109 0.9891 3.045 0.1930 0.11740
253 7.0 M 0.0109 0.9891 3.045 0.1930 0.11740
162 9.5 W 0.9706 -0.9706 -2.684 -0.2256 0.07555
313 9.5 W 0.9706 -0.9706 -2.684 -0.2256 0.07555
Listing 13.14: The four cases with the magnitude of the R-Student residual larger than 2
and also the largest values of Cook’s Distance.
The information in Listing 13.14 lists the most extreme misclassifications. If a re￾examination would be possible, double-check the assignment of Gender in these four
cases. The possibility of a data coding error should be explored. If the data are
accurate, model complexity is the next issue to address. Would adding additional
predictor variables increase classification success?
13.4.8 Multiple Predictors
The possibility for improved model fit and understanding of the relationships among
the model’s variables applies to all multiple regression models regardless of their
estimation algorithm, least-squares, maximum likelihood, or whatever. Choose
predictor variables or features that are not correlated much with one another but each
associated with the response variable. The resulting model will more often correctly
label the response variable and provide a more complete knowledge of the underlying
relationships.
Full Model
Consider a variety of physical measurements that serve as the information for a model
to distinguish between a man and woman. The model specifies seven predictors to
account for Gender. As with the single predictor model of Gender from Hand size,
the goal is to predict missing information from an online customer order. In this
application, if Gender is correctly labeled then the sizing of the garments proceeds
without incurring additional expense and hassle.296 CHAPTER 13. CATEGORICAL REGRESSION VARIABLES
To maintain consistency with the previous one-predictor analysis, the reference group
in this multiple predictor analysis is explicitly stated as "M" with the ref_group
parameter. Not necessary, but the one-predictor analysis automatically made "M" the
reference group to maintain a positive slope coefficient for ease of interpretation. With
multiple predictors this criteria is not meaningful so the analysis relies upon the R
default, which is to make "W" the reference group because W follows M alphabetically.
Input. Logistic multiple regression
Logit(Gender ∼ Height + Weight + Waist + Hips + Chest + Hand + Shoe,
ref_group="M")
The estimated regression coefficients from the logistic regression model, similar to
those in Listing 13.7, provide the information needed to compute the odds ratios.
Listing 13.15 provides these odds ratios for the logistic multiple regression. The
interpretations of these values are similar to that for the 1-predictor model already
considered except interpret each coefficient with the values of all remaining predictor
variables held constant.
Odds Ratio Lower 95% Upper 95%
(Intercept) 0.0000 0.0000 0.0000
Height 1.1734 0.9512 1.4474
Weight 1.0544 1.0066 1.1044
Waist 1.1671 0.9268 1.4697
Hips 0.5109 0.3857 0.6767
Chest 1.1691 0.9260 1.4759
Hand 9.2634 3.5583 24.1158
Shoe 2.0938 1.2614 3.4755
Listing 13.15: Odds ratios and associated confidence intervals.
As seen from the output in Listing 13.7, the Logit() function provides the estimated
coefficients and hypothesis tests of each regression coefficient. Although not shown
here, in this analysis the partial slope coefficients for three variables were not sig￾nificantly different from 0, that is, the corresponding p-values were all larger than
α = 0.05. These three variables are Height, Waist and Chest.
This lack of significance for these variables can also be gleaned from the 95% confidence
intervals of the odds ratios reported in Listing 13.15. The confidence intervals of
the odds ratios for these three variables with non-significant coefficients all cross
the neutral value of 1.0. For example, the lower bound of this interval for Height is
0.9512. For each 1-inch increase in Height, the odds of a random selection of a Man
for this plausible value decrease by 0.9512, with the values for all other variables held
constant. Yet the upper bound of the same confidence interval is 1.4474, which means
that the same odds increase by 44.7%. Because the same confidence interval contains
values below 1.0 and above 1.0, the corresponding relationship between Height and
Gender=1 for a Man cannot be shown to exist, with the values of all other variables
held constant.13.4. BINARY LOGISTIC REGRESSION 297
Classification
Adding more predictor variables to a model can enhance the model’s ability to
successfully label the response variable. The classification table for this seven-predictor
model appears in Listing 13.16.
Probability threshold for predicting M: 0.5
Baseline Predicted
---------------------------------------------------
Total %Tot 0 1 %Correct
---------------------------------------------------
1 170 50.0 11 159 93.5
Gender 0 170 50.0 157 13 92.4
---------------------------------------------------
Total 340 92.9
Listing 13.16: Confusion matrix labeling Gender based on 7 predictors.
For the enhanced model, the percentage of correct classifications with the 1-predictor
model to the seven-predictor model has increased from 88.2% to 92.9%. Sensitivity
increased from 90.0% to 93.5%. Precision showed the most dramatic increase, from
86.9% to 92.4%. The increase in precision reflects the considerable decrease in False +’s
with the enhanced model, from 23 down to 13. By considering body measurements
beyond Hand size, this enhanced model much less falsely labels Women as Men.
Reduced Model
Adding six more variables to the original logistic model, which only had Hand
circumference as a predictor variable, improved predictive efficiency. Are all seven
predictor variables necessary? The logistic multiple regression indicated that three
variables had non-significant partial slope coefficients. This lack of significance
indicates that these three coefficients may not contribute to the model correctly
labeling Gender. More formally evaluate their contribution by comparing the fit
of the reduced or nested model with four predictors to the full model with seven
predictors. Nest() function for
the analysis of nested
models,
Section 12.6.3, p. 267
As with least-squares regression, use the lessR function Nest() to conduct this
hypothesis test. The null hypothesis is that all deleted variables have zero partial
slope coefficients. By default, the function assumes a least-squares solution. To
specify that logit regression is the applicable procedure, invoke the method parameter,
set to "logit".
method parameter:
Set to "logit" to
indicate a logit
analysis for
comparing models. Input. Compare nested models
Nest(Gender, c(Weight, Hips, Hand, Shoe),
c(Height, Weight, Waist, Hips, Chest, Hand, Shoe), method="logit")
Listing 13.17 depicts the comparison of the two models with Nest(). The Deviance
index is the analogy for a maximum likelihood solution to the sum of the squared
residuals for a least-squares solution.298 CHAPTER 13. CATEGORICAL REGRESSION VARIABLES
Model 1: Gender ~ Weight + Hips + Hand + Shoe
Model 2: Gender ~ Height + Weight + Waist + Hips + Chest + Hand + Shoe
Resid.df Resid.Dev df Deviance p-value
1 335 117.67
2 332 111.17 3 6.5019 0.0900
Listing 13.17: Direct comparison of a nested model to the full model with logistic regression.
Here the assessment is of the reduction of Deviance from the nested model to the full
model, a reduction of 6.5019. The result is not significant. Adding the three predictor
variables to the nested four-predictor model does not significantly reduce Deviance.
Effect of Height, Waist and Chest: p-value = 0.090 > α = 0.05,
so do not reject H0 : βHeight = βW aist = βChest = 0
The classification table further supports the conclusion of the viability of the four￾predictor model. Rerunning the model only with these four predictors, the classi￾fication table is the same as that from the seven-predictor model in Listing 13.16
except for one more False -. With Weight, Hips, Hand circumference, and Shoe size
already in the model, the addition of Height, Waist, and Chest hardly improves model
fit. Accuracy decreased only from 92.9% to 92.5%, with similar small reductions in
sensitivity and precision.
The odds ratios for these four predictors in Listing 13.18 are not much changed from
those from the seven predictor model presented in Listing 13.15.
Odds Ratio Lower 95% Upper 95%
(Intercept) 0.0000 0.0000 0.0046
Weight 1.0990 1.0628 1.1365
Hips 0.5301 0.4178 0.6726
Hand 8.5671 3.5768 20.5201
Shoe 2.0647 1.3557 3.1445
Listing 13.18: Odds ratios and confidence intervals for the four-predictor model.
For example, a 1-inch increase in Hand size increases the odds of labeling a customer
as a Man over 8.5 times more likely, which the values of all other predictor variables
held constant.
testing data,
Section 12.5.2, p. 261 Once again, the usual warning: Model fit must be confirmed with new, testing data.
Otherwise, findings may be dependent on distinctive anomalies of the specific data
set on which the model trained.
Predictions
Given the acceptance of the four predictor model, now apply the model to predict
Gender from new data.13.4. BINARY LOGISTIC REGRESSION 299
Scenario. Predict Gender from new data with the logistic regression model
Predict the unknown Gender of a customer from the known body measurements of
Weight - 148 lbs, Hips - 42 in, and Shoe size - 7.5. Suppose the customer has a
medium size glove but hand size is unknown. Use the three Hand circumference
values that fit the medium size glove: 8, 8.5, and 9.
Analyze the data with lessR Logit() using the same parameters X1_new, X2_new,
and so forth as with the least-squares regression models using lessR Regression().
prediction in
least-squares models,
Section 12.5.1, p. 260
Input. Logistic regression prediction from new data
Logit(Gender ∼ Weight + Hips + Hand + Shoe,
X1_new=148, X2_new=42, X3_new=c(8,8.5,9), X4_new=7.5)
The predicted Gender label for the three different sets of Hand measurements for the
given Weight, Hips, and Shoe size appear in Listing 13.19. The logistic regression
labels the person as a Woman.
Weight Hips Hand Shoe Ynew label fitted std.err
1 148 42 8.0 7.5 0 0.01554 0.009104
2 148 42 8.5 7.5 0 0.04416 0.023563
3 148 42 9.0 7.5 0 0.11911 0.063687
Listing 13.19: Probabilities of a being a Man for Hand sizes of 8.0, 8.5, and 9.0 for the
same Weight, Hips, and shoe size.
Because Man is the reference group for the response variable of Gender, the prob￾abilities under "fitted" in Listing 13.19 are the probabilities of being a Man. To
obtain the probabilities of being a Woman, manually subtract each of the values
under fitted from 1, shown in Table 13.3.
fitted 1 - fitted
0.01554 0.98446
0.04416 0.95584
0.11911 0.88089
Table 13.3: Probabilities of being a Man, fitted, and of being a Woman, 1 - fitted.
Interpretation.
For a person who weighs 148 lbs., 42 inch hips, and a shoe size of 7.5: For hand
sizes that fit a medium size glove – 8, 8.5, and 9 inches – the respective probabilities
of begin a woman are 0.984, 0.956, and 0.881. Proceed on the assumption the
customer is a woman.
Summary
In conclusion, realize excellent differentiation between men and women simply by
knowing the circumference of the Hand, with an 88.2% correct classification of Gender
based on this information alone. Increase the accuracy of this classification by also300 CHAPTER 13. CATEGORICAL REGRESSION VARIABLES
including the predictor variables of Weight, Hips, and Shoe size. Based on these
four body measurements, this sample of 170 men and 170 women achieves an overall
correct classification percentage to Gender of 92.6%. testing data,
Section 12.5.3, p. 262
As always, the model’s test
accuracy should always be validated on a new set of data, the testing data.
13.5 Analysis Problems
1. For the Anova_1way data set, three groups of rats were administered different
dosages of an arousal drug: 00mg, 05mg, and 10mg. The predictor variable is Dosage
and the response variable is the Time required to run a maze.
?dataAnova_1way
for more information.
d <- Read("Anova_1way")
Part I
a. Define Dosage as a factor variable with the 00mg listed last.
b. Run an effects coding regression analysis that compares the 05mg and 10mg
group means to the overall mean.
c. Interpret the slope coefficients.
Part II
a. Define Dosage as a factor variable with the 00mg listed first.
b. Run an dummy coding regression analysis that compares the 05mg and 10mg
group means to the overall mean.
c. Interpret the slope coefficients.
Part III
a. Which of the above two regression analyses would you prefer? Why?
2. The Cars93 data set contains much information on 93 1993 car models. One variable
is Source, which has two values, 0 for a foreign car and 1 for a car manufactured in
the USA.
?dataCars93 for
more information. d <- Read("Cars93")
a. Use the variable Length to account for the Source of the automobile. Does
Length successfully differentiate between foreign an USA manufactured cars in
terms of classification indices?
b. If the odds ratio is interpretable provide the interpretation for Length.
c. For the 1-variable Length predictor variable model what is the predicted Source
for a car with a Length of 60.5 inches?
d. How much improvement in prediction is there with the following 8 predictor
variables: Length, MPGhiway, HP, Weight, Engine, Length, MidPrice, and
Wheelbase?13.5. ANALYSIS PROBLEMS 301
e. Do model selection by eliminating those predictor variables that do not mean￾ingfully contribute to accurate labeling. Explain why the deleted variables were
deleted from the model.
f. From this revised model, interpret the largest odds ratio.
g. From this revised model, how does fit compare to the full model with 8 predictor
variables?Chapter 14
Causality
14.1 Quick Start
Install R and lessR as explained in Section 1.2. At the beginning of each R session,
access the lessR functions with library("lessR"). From Chapter 2, read your
data into an R data frame such as d with d <- Read("") to browse for the data file.
Or, locate your data file with a path name or web address between the quotes.
Regression analysis provides for models that relate the response or target to one or
more predictors or features. The resulting slope coefficients relate changes in the
predictors to changes in the response. The key understanding explored in this chapter
is that these related changes indicate a relationship between the variables but not
necessarily a causal relationship. This chapter investigates analyses to move beyond
the verification of a relationship, a correlation or slope coefficient, to the analysis of
causality.
interaction,
Section 9.2, p. 172
A factorial ANOVA allows for the possibility that the independent variables in the
model interact.
moderator variable,
Section 14.3, p. 306
ANOVA is regression analysis with categorical predictor variables, so
no surprise that the concept of interaction also applies to the more general regression
models. Applied to regression, the focus usually is how predictor variable X impacts
response variable Y according to the level of what is called moderator variable W. To
do the moderator analysis, indicate the moderator variable with parameter mod.
Regression(Y ∼ X + W, mod=W)
mediation,
Section 14.4, p. 310 Predictor variable X can impact response Y directly or indirectly. The indirect effect
is causally mediated through at least one other variable, mediator variable M. Here
we turn to the mediation() function in the MBESS package for the analysis. The
following analyzes the impact of predictor variable X via mediator variable M on
response variable Y.
mediation(x=d[,"X"], mediator=d[,"M"], dv=d[,"Y"])
A more general expression of causal models is the analysis of a network of causal
path analysis, relations.
Section 14.5, p. 317
Mediation is one example but the network can be much more complex.
Here the function sem() from the lavaan package is introduced for a form of causal
30214.2. CORRELATION IS NOT CAUSATION 303
analysis called path analysis. The application of path analysis in this chapter is for
the mediation model.
14.2 Correlation is not Causation
Causality? Causal inference is a rather formidable topic for which an in-depth
discussion would occupy at least an entire book. Fortunately for us, some well￾recommended books on this topic have been written (Pearl & MacKenzie, 2018;
Cunningham, 2021), books that anyone who wishes to pursue a deeper understanding
of this essential concept should explore.
Understanding causation requires understanding the relationships between the vari￾ables of interest. causal relationship,
Section 14.4.1, p. 310
The regression analysis of relationships yields a mathematical
function that describes the relationship between two or more variables. The model
relates the change in X to the average associated change in Y using the slope coefficient
b1. This coefficient, however, does not always indicate a causal relationship. The
causal interpretation is appealing because, by definition, changing the value of X
changes the value of Y. Furthermore, X’s impact on Y can be causal: Improve your
grades by studying more. However, as is always the case, the relationship between
two variables does not imply causation.
14.2.1 Example
Consider the relation between the number of fire trucks at a fire and damage in
dollars from the fire. The correlation between these two variables is real: The more
severe the fire, the more fire trucks tend to be at the fire. correlation
coefficient,
Section 10.2.2, p. 196
For the hypothetical data
in Figure 14.1, their correlation is high, r = 0.74. We can build a model in which the
number of fire trucks successfully predicts the amount of fire damage.
Figure 14.1: Fire data.
From this small, hypothetical data set, a regression anal￾ysis that expresses damage in terms of the number of fire
trucks yields the following model.
Yˆ
damage = 7617 + 55287(Xtrucks)
The larger the value of Xtrucks, the number of fire trucks
entered into that function, the more the predicted fire
damage. More precisely, on average, an additional $55,287
of damage occurred for each additional fire truck. The
value of b1 = 55287 is large enough to significantly differ
from 0, p-value = 0.014 < α = 0.05. From this correct information, a foolish researcher
might conclude that fire trucks cause damage from fires.
How to explain the high correlation of r = 0.74 between the number of fire trucks
and fire damage? Fire trucks do not cause fire damage. Instead, another variable to
consider is the fire’s severity. The more severe the fire, the more fire trucks, and the
more damage. Number of fire trucks and fire damage are correlated precisely because
they share a common cause without either variable directly causing the other. The304 CHAPTER 14. CAUSALITY
slope coefficient, b1 = 55287 for relating fire damage from the number of fire trucks,
is correct given the data. total vs. net effect,
Section 12.2.2, p. 252 However, this b1 is a total effect, which does not indicate a
direct causal relationship between the two variables. Correlation is not causation.
path diagram:
Visualization of a
causal flow between
variables with
connecting arrows
indicating causality.
To visualize a proposed causal flow of this alternative causal structure, use a path
diagram. The path diagram connects variables with arrows to indicate causality, each
arrow labeled with a + or - according to the direction of the relationship. Figure 14.2
depicts the path diagram explaining the correlation between fire trucks and damage,
path diagrams, which is the causal structure from which the hypothetical data were generated.
Section 14.4.1, p. 310,
Section 15.5.2, p. 340
Number of
Fire Trucks
Fire
Damage
+ +
Severity
of the Fire
Figure 14.2: Path diagram that specifies Severity of the Fire causing the Number of Fire
Trucks at the fire as well as the total Fire Damage.
confounding,
lurking variable:
Common cause of
two variables
responsible for their
correlation.
The silly researcher who attributes causality to b1 = $55287 in this example ignores
the common cause.
confounding variable,
Section 7.2.6, p. 138
In this situation, the severity of the fire is called a confounding
variable or a lurking variable. This example pertaining to fires was chosen because
of the obvious relation of the severity of the fire to the correlation of the number of
fire trucks and fire damage. No one would reasonably interpret the slope b1 from
the regression of amount of fire damage on the number of fire trucks at the fire as
representing causality. However, in other situations the pattern of causality is not so
obvious.
Regression analyzes the correlations between variables. Those correlations may, or
may not, reflect causality. Instead, the assessment of causality requires additional
context and information. Without this additional information, never assume that a
slope coefficient, b, indicates a causal relationship.
Figure 14.3: Fire data.
Now consider multiple regression of the hypothetical data,
first listed in Figure 14.1 for two variables, but here listed
in Figure 14.3 with the additional variable fire severity.
What happens to the relationship of two variables that
correlate only because of a common cause, and then that
common cause does not vary? The co-variation between
the two variables that share only that common cause
disappears when the values of the common cause variable
are held constant.
sampling error,
Section 6.3.2, p. 112
If there were 10 fires all at the same level of severity, any co-variation between number
of fire trucks and fire damage would be random sampling error. Multiple regression
provides this “holding the values constant” analysis. The estimated multiple regression
model of the data in Figure 14.3, with Xseverity included, eliminates the extraneous
relationship between fire trucks and damage.
Yˆ
damage = 39635 + 525(Xtrucks) + 49500(Xseverity)14.2. CORRELATION IS NOT CAUSATION 305
From btrucks = $55287 in the single predictor model, the slope coefficient for Xtrucks
diminishes to almost 0 in the model with Xseverity. In the multiple regression model,
btrucks = $525 is not statistically distinguishable from 0, p-value = 0.976. Instead,
the partial regression slope coefficient, bseverity, becomes the significant predictor of
damage by the fire, p-value = 0.005.
14.2.2 Real Life Consequences
Consider a miss-attributed causal inference from medicine designed to help people but
accomplished the opposite. experimental control,
Section 7.2.6, p. 137
Many observational studies, those without experimental
controls to remove the potential impact of confounding variables, demonstrated
that women taking hormone replacement therapy, HRT, also had a lower than
average incidence of coronary heart disease, CHD. A series of observational studies
demonstrated a well-established, genuine inverse relationship between HRT and CHD:
as HRT goes up, CHD tends to go down. A casual explanation was offered to explain
this correlation: HRT protects against CHD, illustrated in Figure 14.4.
HRT – HCHRDT
-
Figure 14.4: Path diagram that specifies hormone replacement therapy directly diminishes
coronary heart disease.
This understanding of causality guided medical practice. Women were sometimes
prescribed HRT with one goal of lowering CHD.
However, as we saw with the fire truck example, simply observing a relationship does
not imply a direct causal relationship. On the contrary, subsequent experimental
research revealed that HRT increased the likelihood of experiencing CHD. Instead of
assisting, HRT causes a small but discernible increase in the risk of CHD (Lawlor,
Smith, & Ebrahim, 2004).
How is it possible that the causal relationship is the reverse of the observed correlation?
The answer is the lurking variable, socioeconomic status, SES. Women undertaking
HRT were more likely from higher socioeconomic groups. These higher SES women
tended to have better diets and exercise, which, in turn, reduced CHD. The use of
HRT and a decrease in CHD were both joint effects of a common cause, the benefits
of a higher SES. Figure 14.5 depicts this revised causal structure.
HRT HCHRDT
SES
+ Diet Exercise
–
–
–
+
+
+
Figure 14.5: Path diagram that specifies hormone replacement therapy increases coronary
heart disease.306 CHAPTER 14. CAUSALITY
The lurking variable, SES, and its indirect impact on CHD masked the harmful effect
of HRT on CHD. The correlation between HRT and CHD is positive, but the causal
impact is negative. Correlation is not causation, and in this analysis, the correlation
of HRT and CHD is the opposite of the causal impact of HRT on CHD.
14.3 Moderation
14.3.1 The Concept
interaction,
Section 9.2, p. 172
Moderation is the concept of interaction from factorial ANOVA designs extended to more
general regression models with continuous predictor variables.
moderation: Effect
of predictor X on
response Y depends
on the value of a
third variable,
moderator W.
With moderation, the
predictor variable or feature impacts the response variable or target, but the extent of
that impact depends on the value of a third variable, the moderator. The moderator
variable changes the relationship between the predictor and response variables, either
its strength and/or its direction. The value of the moderator may enhance the impact
of the predictor, diminish its impact, or even reverse the direction.
How to represent interaction between continuous variables? For predictor X and
response Y, moderator W becomes another predictor variable in the model. However,
the primary interest remains the influence of X on Y. Moderator W is of interest just
because it influences the relationship between X and Y. Define the interaction term
in the model as the product of the interacting variables, XW.
yˆ = b0 + b1X + bW W + bXW (XW)
If, referring to the corresponding population value, βXW = 0, then the interaction
term drops from the model. If the interaction term has a significant slope coefficient,
then a moderation effect is detected. After a little algebra, the following expression
rewrites the regression model to isolate the predictor variable X to show the direct
effect of W on the impact of X on Y.
yˆ = (b0 + bW W) + (b1 + bXW W) ∗ X
This expression shows that different values of W lead to different values of the slope
coefficient for X in this expression, (b1 + bXW W). For a non-zero value of bXW , W
moderates the impact of X on Y. For example, suppose the value of W is 0.
W = 0, the slope coefficient of X is b1 + bXW (0) = b1
And the impact of X on Y if the value of W is 1?
W = 1, the slope coefficient of X is b1 + bXW (1) = b1 + bXW
A different value of moderator variable W leads to a different slope coefficient for X.
14.3.2 Example
Refer to a study of test anxiety (Nie, Lau, & Liau, 2011). The study examined the
relationship of anxiety regarding test performance to the importance of a test in terms14.3. MODERATION 307
of the consequences of good or bad test performance. Anxious students worry about
their test performance. The more important the test, the higher tends to be test
anxiety. However, one’s personal belief to possess the ability to succeed, academic
self-efficacy, moderates that relationship.
Data were simulated such that the analysis replicated the general, though not identical,
findings of the actual study (Nie et al., 2011).
http://lessRstats.com/data/testanxiety.csv
Test anxiety (TA) measurements were simulated on an 8-point scale from 1 to 8
with a score of 8 representing maximum test anxiety. Test importance was assessed
on a 15-pt scale from 1 to 15, with 15 indicating perceived maximum importance.
Academic self-efficacy (ASE) was assessed on a 7-point scale from 1 to 7 with a score
of 7 indicating the maximum ASE.
mod parameter:
Specify a potential
moderator variable.
Accomplish the moderation analysis with the lessR function Regression(), abbrevi￾ated reg(), by identifying the potential moderator variable with the mod parameter.
Listing 14.1 shows the input to perform the moderation analysis and the initial
output that verifies the moderation analysis. By identifying a predictor variable as a
potential moderator with mod, Regression() automatically creates the interaction
term.
reg(TA ∼ TI + ASE, mod=ASE)
moderator variable: ASE
variable moderated: TI
Listing 14.1: Input for reg() to evaluate ASE as a potential moderator of the influence of
predictor variable TI on response TA.
As of this writing, the automatic moderation analysis applies only to regression
models with two predictor variables, one of them the potential moderator.
To improve the interpretability of a potential interaction, researchers typically trans￾form the predictor variables entered into the regression model. A common transfor￾mation is the mean deviation of the values of each predictor variable. The result
for predictor variables X and W are distributions of values centered with a mean
of 0. Or, proceed one step further, standardizing each predictor variable, dividing
the mean deviation of each value by the variable’s standard deviation. The result
is a distribution with a mean of 0 and a standard deviation of 1. For example, the
researchers who studied text anxiety (Nie et al., 2011) standardized the predictor
variables test importance and academic self-efficacy before the analysis.
By default, specifying a moderator variable with mod instructs Regression() to au￾tomatically center each predictor variable. The Regression() controlling parameter
for the transformation is mod_transf. The default value is "center", implemented
in this analysis. Another permissible value is "z" for standardize. To not transform
the variables with the Regression() function call, specify "none".308 CHAPTER 14. CAUSALITY
Listing 14.2 shows the estimated model of the simulated data.
Estimate Std Err t-value p-value Lower 95% Upper 95%
(Intercept) 4.735 0.014 348.716 0.000 4.708 4.761
TI 0.015 0.007 2.263 0.024 0.002 0.028
ASE -0.884 0.020 -44.919 0.000 -0.923 -0.846
ASE.TI -0.101 0.008 -11.974 0.000 -0.117 -0.084
Listing 14.2: Estimated model for TA as a function of TI, ASE, and their interaction.
The value of the fit index R2 R is 0.493, indicating reasonably good fit. 2
P RESS,
Section 12.5.2, p. 261
Given the large
sample size, the value of R2
P RESS drops only to 0.491, indicating that the model
generalizes to new data as well.
Is academic self-efficacy a moderator for the impact of perceived test importance on
test anxiety? The slope coefficient for the interaction term, ASE.TI, significantly
differs from 0, so the answer is yes.
ASE.TI slope: p-value = 0.000 < α = 0.05, reject H0 : βASE.T I = 0
TI also displays a significant slope coefficient.
TI slope: p-value = 0.024 < α = 0.05, reject H0 : βT I = 0
However, given that ASE and TI interact, ASE moderates the impact of TI on TA. To
know the impact of TI on TA for a specific person, we need to know the corresponding
ANOVA interaction, level of ASE and the form of the interaction.
Section 9.2.5, p. 177
As discussed in the context of a factorial
ANOVA design, an interaction generally precludes the interpretation of the effects of the
individual variables that contribute to the interaction. The estimated slope coefficient
for test importance does not in general represent the impact of test importance on
test anxiety. Instead, the test importance slope coefficient averages the different
values of slope coefficients for different values of academic self-efficacy.
To explore the interaction of TI and ASE, choose representative values of ASE for
which to assess the slope coefficient for TI. If ASE was categorical, Regression()
would choose the different levels of ASE, such as Male, Female, and Other. However,
ASE is continuous, so Regression() selects the mean of the moderator and one
standard deviation above and below the mean. Regression() then plots the relation
of test importance to test anxiety for each of the chosen levels.
Regression() provides the output in Listing 14.3 for the moderator variable. The
predictor variables were centered over zero, so mASE = 0.
ASE: Mean = -0.000 SD = 0.701
Mean + 1SD = 0.701
Mean - 1SD = -0.701
Listing 14.3: Values for the potential moderator ASE: m, and m ± s.
The Regression() function substitutes the three constant values of ASE into the
estimated regression model to compute and display the corresponding three estimated14.3. MODERATION 309
regression equations. Listing 14.4 lists the resultant regression models.
ASE = -0.701: TA = 5.354 + 0.086(TI)
ASE = 0.000: TA = 4.735 + 0.015(TI)
ASE = 0.701: TA = 4.115 - 0.055(TI)
Listing 14.4: The three regression equations for three separate values of the moderator
variable ASE.
If ASE was not a moderator variable, there would be no interaction in the estimated
regression equation between the predictor variables TI and ASE. Without interaction,
the slopes indicating the impact of TI on TA would differ only by sampling error. If
ASE had an effect on TA, the intercepts would differ, but the common slope would
show the same impact of TI on TA regardless of the ASE level.
Instead, at least for this specific data set, the three different slopes from the three
equations in Listing 14.3 illustrate the meaning of the significant moderation effect of
ASE. Figure 14.6 visualizes this moderation effect of academic self-efficacy, plotting
the different slopes of test importance for the three different self-efficacy values.
Figure 14.6: Interaction plot from Regression() for Task Anxiety as a function of Task
Importance for three levels of the moderator academic self-efficacy.
When ASE= 0.701, one standard deviation above the mean, then bT I = −0.055.
For each additional point on the measurement scale of test importance, test anxiety
decreases by an average of .055 units. Yet, when ASE= −.701, the value of the slope
not only changes, its sign also changes, bT I = 0.086. When academic self-efficacy is
one standard deviation below its mean, for each additional unit of test importance,
test anxiety increases by an average of 0.86 units.
Interpretation. Moderation
For larger values of academic self-efficacy, as the perceived importance of the task
increases, test anxiety decreases. However, for lower values of academic self-efficacy,
test anxiety tends to increase as Task Importance increases. Academic self-efficacy
moderates the impact of test importance on test anxiety.310 CHAPTER 14. CAUSALITY
14.3.3 Manual Analysis
As indicated, the mod parameter applies only to models with two predictor variables,
X, the primary variable of interest, and M, the potential moderator. To study
moderation with more complex models, manually create the needed variables for the
analysis. Center or standardize the predictor variables, create the interaction term or
terms, then submit the enhanced model with Regression().
First, either center each predictor by mean deviation or by standardization with the
R function scale(). The following examples apply to a predictor variable X from
which a mean deviated variable is created, X_dev, and also a standardized variable,
X_z, both in the d data frame. Do the same for the moderator variable, generically
labeled W.
The scale() parameters, center and scale, are set to TRUE by default, though both
parameters are listed in the following example for clarity. Parameter center does
the mean deviation and scale does the standardization.
Input. Mean deviate or standardize predictor variable X
d$X_dev <- scale(d$X, center=TRUE, scale=FALSE)
d$X_z <- scale(d$X, center=TRUE, scale=TRUE)
Once the predictor variables have been transformed, multiply the transformed values
create a transformed to create the interaction variable.
variable, Section 3.3,
p. 46
In the following example, the centered predictor
variables X and W together create the interaction term X_int.
Input. Create the interaction term
d$X_int <- d$X_dev * d$W_dev
Analyze the model with the response variable, the transformed predictor variables,
and the created interaction term(s). As predictor variables are added to the model,
more complex interaction terms could be introduced, such as three-way interactions.
overfitted model,
Section 12.5.2, p. 261
However, these more complex models are found less often in the published research
literature, and their complexity can facilitate overfitting.
14.4 Mediation
14.4.1 The Concept
A mediator variable and a moderator variable both influence how predictor X influences
response Y. However, how the effect manifests itself differs for each. To propose
a variable as a mediator is to convey a causal hypothesis that X at least partially
influences the variable Y indirectly through the mediator variable, M. A change in
the value of predictor X affects the value of mediator M, which in turn affects the
value of response Y.
path diagram,
Section 14.2, p. 304,
Section 15.2, p. 331
To develop the concept of a mediator variable, consider the previously introduced
concept of a path diagram, a visualization of a hypothesized causal ordering of14.4. MEDIATION 311
multiple variables. To avoid a more complex system of subscripting, and to retain
compatibility with many published articles on mediation and standard computer
output, the following path diagrams follow the convention of using a, b, and c for the
slope coefficients instead of a b with subscripts (e.g., Preacher & Hayes, 2008). The
letter d specifies the intercept term.
correlation not
causality,
Section 14.2.1, p. 303
A relationship, such as assessed by a slope coefficient, does not prove a causal
relationship. The reverse, however, is true: A causal structure implies a pattern
of correlations among the variables. Moreover, that correlational structure can be
empirically investigated. In the most basic scenario, suppose Variable Y has only one
cause, Variable X. The following regression equation is consistent with that proposed
causal model. Change the value of X, and the value of Y necessarily changes.
Yˆ = dY.X + cX
The path diagram that corresponds to this causal model of Y appears in Figure 14.7.
total effect,
Section 12.2.2, p. 252
In this model, without any other causal antecedents, the single slope coefficient c
assesses the total effect of X on Y.
Predictor X Response Y
c
Figure 14.7: Path diagram of the direct influence of predictor X on response Y.
If X causes Y according to this causal model, then X and Y correlate. The correspond￾ing regression model should show a significant slope coefficient, c, for a reasonable
sample size. If c cannot be differentiated from the population value of 0, the regression
analysis does not support the presumption of causality.
mediator variable,
M: Variable that
causally intervenes
between variables X
and Y.
Figure 14.8 shows a more complex causal model. A mediator variable, M, casually
intervenes between X and Y, in addition to the direct effect of X on Y.
Predictor X Response Y
a
c'
b
Mediator M
Figure 14.8: Partial mediation path diagram for predictor X, response Y, and mediator M. indirect effect:
Effect of X on Y
mediated by at least
one mediator M.
X directly impacts M, and then M directly impacts Y, the indirect effect effect of X
on Y. X both directly and indirectly influences Y.
partial mediation:
X impacts Y directly
and indirectly via M.
The total causal effect of X on Y
is the cumulation of both direct and indirect effects. This model of a causal chain
demonstrates partial mediation.
To estimate the slope coefficients a, b, and c
0
specified in Figure 14.8 requires two
regression equations. Variable Y now depends on both X and M, so requires a multiple
regression model that estimates slope coefficients b and c
0
. Variable M relies only
upon X, in which the corresponding regression model estimates slope coefficient a.
Yˆ = dY.MX + bM + c
0X
Mˆ = dM.X + aX312 CHAPTER 14. CAUSALITY
The causal antecedents of X are not specified and so are not modeled. Variable X
only serves as a predictor variable in this proposed causal network.
full or complete
mediation: X
impacts Y only
indirectly, via the
relation via M.
If there is no direct effect of X on Y, the general model for a single mediator reduces
to the model in Figure 14.9, called full mediation, or complete mediation.
Predictor X Response Y
a b Mediator M
Figure 14.9: Path diagram of full mediation of predictor X on response Y via mediator M.
The only impact of X on Y in the full model is the indirect effect realized via the
mediator M. The full mediation model is presented here for completeness. The
empirical analysis is usually of the partial mediation model. If direct effect c
0
in
Figure 14.8 is not differentiated from 0, the more general partial mediation model
reduces to full mediation.
14.4.2 Example
The example is from on an analysis (Jessor & Jessor, 1977) of the socialization
of problem behavior in youth. Preacher and Kelley (2011) applied the analysis of
some data from this work regarding deviant behavior to mediation analysis. What
influences result in deviant behavior? The three variables, described in Table 14.1
were measured as scale scores computed from self-report responses to sets of attitude
items.
Type Name Description Sample Items
response Y DVB Deviant behavior threaten teacher out of anger
predictor X VAC Achievement values like good grades, value the honor roll
mediator M ATD Attitude toward deviance wrong to break into a locked place
wrong to beat someone up
Table 14.1: The three variables in the mediation analysis with higher ATD scores indicating
greater intolerance of deviant behavior.
Preacher and Kelley (2011, p. 96) reported for the original data the sample size
of 432, the means of the three variables, their variances, and their co-variances
(unstandardized correlations). Given this information, the following analyses were of
simulated data that match these sample characteristics.1 Accordingly, the following
regression results from these simulated data exactly match the results reported in
their paper to within the four digits of precision of their reported results.
One accessible, straightforward approach to do mediation analysis is with the
mediation() function in Kelley’s package MBESS (Kelley, 2022, 2007). From a
single function call, obtain the three regression models for assessing partial mediation,
the model of only Y and X and the two regression equations that result from the
1Multivariate normal data values for the three variables were generated by the mvrnorm() function
from the MASS package, with the parameter empirical set to TRUE to exactly reproduce the input
means and covariances.14.4. MEDIATION 313
mediation model. The output includes the estimated intercept and slope coefficients,
their inferential analysis, and model fit.
The estimated separate regression models are identical to what would be obtained by
running any standard regression function such as lessR Regression(). outliers in regression
analysis,
Section 11.6, p. 236
The output
presented here is from mediation() but note that the outlier issue does not vanish
just because three different regression models are analyzed. Running Regression()
separately for each of the models provides more information, including an analysis of
outliers.
install.packages()
function,
Section 1.2.5, p. 8
Download the MBESS package once with install.packages().
library() function,
Section 1.1, p. 9
Run the library()
function for every new R session to access MBESS functions. Everything after the
number sign, #, is an optional comment, ignored by the computer.
install.packages("MBESS") # run only once
library("MBESS")
The function mediation() requires a minimum of three parameters to identify the
relevant variables: x for the predictor X, mediator for the mediator M, and dv for
the response Y, here referred to as the dependent variable. If specified in that order,
the parameter names are not required but included in the following function call for
clarification.
mediation(x=d[,"VAC"], mediator=d[,"ATD"], dv=d[,"DVB"])
The only “quirk” of using mediation() is that the variable names are not specified
directly. data frame subset,
Section 3.6, p. 53
Instead, indicate the variables in the analysis with subset specifications of
the containing data frame. As previously discussed, the lessR function .() in the
subsetting removes the need for quotes around the variable names, but with only a
single variable to specify for each subset either approach is equally convenient.
Listing 14.5 shows the analysis of the model without mediation.
$Y.on.X$Regression.Table
Estimate Std. Error t value p(>|t|) Low Conf Limit Up Conf Limit
Intercept.Y_X 1.92358392 0.069807102 27.5557 0.000 1.78637833 2.06078952
c (Regressor) -0.03831818 0.009544282 -4.0148 0.000 -0.05707743 -0.01955893
Listing 14.5: Slightly modified output from mediation() for the model without mediation.
To conserve space, the original output is slightly modified to report the p-values to
three decimal digits instead of in scientific notation and to round the t-values to four
digits.
The direct effect, c, absorbs all of the total effect of the causal impact of X (VAC)
on Y (DVB) in this model. The analysis of this model, which does not include
mediation, yields a statistically significant effect of VAX on DVB. The slope coefficient,
c = −0.0383, is more than −4.015 standard errors from the null value of 0, yielding
an improbable result if the null hypothesis of no relation is true.314 CHAPTER 14. CAUSALITY
Figure 14.10 shows the path diagram of the model without mediation. Display this
estimate visually, indicating the statistical significance of slope c with an asterisk, *.
VAC (X) DVB (Y)
-0.383*
Figure 14.10: Path diagram of predictor VAC on response DVB without mediation.
Interpretation. Model with no mediation
Higher achievement relates to a lower deviation behavior score. The more achieve￾ment is valued, the lower the average endorsement of deviant behavior.
Achievement values and endorsement of deviant behavior are related, the interpreta￾tion is correct. However, is the relation causal? Does a high value of VAC directly tend
to lower the DVB score? One answer to that question follows from the adaptation of
the partial mediation model in Figure 14.8 to include the attitude toward deviance,
Variable ATD.
Listing 14.6 displays the multiple regression output for response DVB in the mediation
model, again slightly modified from the mediation() output to conserve space.
$Y.on.X.and.M$Regression.Table
Estimate Std.Error t value p(>|t|) Low Conf Limit Up Conf Limit
Intercept.Y_XM 2.28995 0.0703512 32.5503 0.000 2.15167704 2.428229041
c.prime (Regressor) -0.01024 0.0088361 -1.1588 0.247 -0.02760642 0.007128509
b (Mediator) -0.09628 0.0088205 -10.9154 0.000 -0.11361555 -0.078942044
Listing 14.6: Slightly modified output from mediation() for the model of deviant behavior
(DVB) with mediation.
When the attitude toward deviancy (ATD) mediates the extent of values placed on
achievement (VAC), the direct effect of ATD on deviant behavior (DVB) cannot
be statistically distinguished from 0. The slope coefficient c
0 = −0.010 has a corre￾sponding p-value of 0.247, so the null hypothesis of a 0 population slope coefficient
cannot be rejected. However, the slope for the mediator variable ATD is statistically
significant, b = −0.096 has a p-value of 0 to within three decimal digits.
To establish the mediation effect, the mediator variable must also be related to the
predictor variable. Listing 14.7 shows the output of ATD regressed on VAC.
$M.on.X$Regression.Table
Estimate Std. Error t value p(>|t|) Low Conf Limit Up Conf Limit
Intercept.M_X 3.805294 0.33803308 11.2571 0.000 3.1408912 4.4696967
a (Regressor) 0.291645 0.04621712 6.3103 0.000 0.2008054 0.3824845
Listing 14.7: Slightly modified output from mediation() for the relation of the mediator
to the predictor.
The slope coefficient from regressing ATD on VAC is statistically significant, a = 0.292,
with a p-value of 0 to within three decimal digits.
Both the causal links assessed by the slope coefficients from the predictor to the
mediator and from the mediator to the response are significant. The mediation effect14.4. MEDIATION 315
exists and, moreover, the partial mediation model reduces to full mediation as the
direct link from VAC to DVB vanishes. Figure 14.11 shows the estimated mediation
model with the estimates for slope coefficients a, b, and c
0
. Statistically significant
coefficients at the α = 0.05 level are again indicated with an asterisk, *.
VAC (X) DVB (Y)
.292*
-.010
-.096*
ATD (M)
Figure 14.11: Analysis of mediation model consistent with data from Jessor and Jessor
(1977).
Interpretation. Model with mediation
The influence of the valuing of achievement relates to a lower deviation behavior
score due to the indirect influence via the mediator of attitude toward deviant
behavior. The more achievement is valued, on average the more the attitude toward
deviant behavior becomes less accepting. Less acceptance of deviant behavior then
leads to lower the average endorsement of deviant behavior.
14.4.3 The Indirect Effect
The standard regression analysis implied by the causal models indicates that VAC
impacts DVB almost exclusively via the mediation of ATD. What is the extent of
this indirect effect? Calculate the sample indirect effect as the product of the two
estimated slope coefficients along the path of the mediating variable, a and b.
Indirect effect of X on Y = ab
For this analysis, calculate the indirect effect as:
Indirect effect: ab = 0.291645 ∗ −0.09628 = −0.02808 ≈ −0.028
The sample value of the indirect effect is straightforward to obtain. However, as always,
the question of interest is not the sample value but the corresponding population
value. How to estimate? The procedure from classical statistical inference, developed
in pre-computer times, relies upon the central limit theorem to assume a normal
distribution of the statistic of interest.
With multiple variables involved, rely upon the assumption of multivariate normality,
a multi-dimensional normal distribution. The challenge is that the assumption of
multivariate normality only sometimes applies to the estimated indirect and related
effects except in very large samples (Shrout & Bolger, 2002). How, then, to estimate
the corresponding confidence interval?
sampling distribution,
Section 6.3.2, p. 112
Classical statistical inference relies upon mathematical formulas that estimate what
would happen if drawing many hypothetical samples and computing the statistic of
interest over those samples. The behavior of hypothetical samples is inferred from
mathematical formulas dependent on a set of assumptions given the sample estimates.
Only one obtained sample is usually obtained.316 CHAPTER 14. CAUSALITY
bootstrap sample:
A sample drawn from
the original sample of
data of the same size
with replacement.
A more recently developed computer intensive alternative, bootstrapping, treats the
data sample as a proxy for the population and then repeatedly re-samples from that
population with replacement. Directly compute the statistic of interest for each
obtained sample. By sampling with replacement, for any one sample any data value
in the proxy population could be sampled more than once or not at all so that the
samples generally differ from each other. Each re-sampled data set is the same size
as the original data sample.
Bootstrapping provides usually thousands of samples to compute the standard devia￾tion of a statistic, its standard error. From the empirically estimated standard error,
compute the confidence interval of the statistic, such as the mean. Bootstrapping
provides the confidence interval without relying on assumptions such as the statistic’s
normal distribution over hypothetical repeated samples. The bootstrap samples are
not hypothetical, they are real, all from the original sample defined as the proxy
population.
To apply bootstrap sampling to estimating the confidence interval of the indirect
effect ab, estimate the partial mediation model of Figure 14.8 for each re-sample.
From each estimated model for each re-sample, compute ab. Sort the distribution of
at least a thousand obtained values of ab and define the 95% confidence interval as
the range that includes 95% of the obtained ab values centered on the mean of the
distribution. Unlike confidence intervals calculated with the assumption of normality,
bootstrap estimated confidence intervals may be asymmetrical.
Invoke bootstrap estimation with the bootstrap parameter for the mediation()
function, shown in Listing 14.8.
mediation(x=d[,"VAC"], mediator=d[,"ATD"], dv=d[,"DVB"],
bootstrap=TRUE)
Estimate CI.Lower_Percentile CI.Upper_Percentile
Indirect.Effect -0.02807922 -0.038226608 -0.01832460
Listing 14.8: Bootstrap estimate of the indirect effect confidence interval from mediation()
with 10000 re-samples.
The default number of bootstrap samples is 10,000, specified with the parameter B.
The default confidence level is 95%, specified with the conf.level parameter.
The confidence intervals in Listing 14.8 are called percentile intervals, computed
according to the previous description. The mediation() function, by default, also
reports what are called bias-corrected intervals, which are generally close to the
percentile intervals. In this example, the two sets of confidence intervals are identical
to four decimal digits, so only the percentile intervals are reported.
Multiple possible expressions of the importance of the indirect effect of X on Y are
reported by mediation(), such as the proportion of total effect due to the indirect
effect. Calculate the total effect as the sum of the direct and indirect effects.
Total Effect of X on Y = direct effect + indirect effect = c’ + ab14.5. PATH ANALYSIS 317
Total effect: c’ + ab = −0.01024 + −0.02808 = −0.03832 ≈ −0.038
The proportion of the total effect attributable to the mediating indirect effect is their
ratio.
Proportion of total effect:
indirect effect
total effect =
−0.02808
−0.03832
≈ 0.733
The mediation() function with the requested parameter bootstrap also calculates
this proportion of total effect for the indirect effect as well as its bootstrap estimated
confidence interval, shown in Listing 14.9.
Estimate CI.Lower_Percentile CI.Upper_Percentile
Ratio.of.Indirect.to.Total.Effect 0.732791 0.469284922 1.30263329
Listing 14.9: Proportion of the indirect effect of the total effect of ATD on DVB from
mediation().
In this sample, more than 73% of the total effect of ATD on DVB is attributable to
the indirect effect mediated by ATD, a value that in the population is likely to lie
between 0.469 and 1.303 with 95% confidence.
Interpretation.
The influence of valuing achievement relates to a lower deviation behavior score
due to the indirect influence via the mediator of attitude toward deviant behavior.
The more achievement is valued, the more the attitude toward deviant behavior
tends to become less accepting. Less acceptance of deviant behavior leads to lower
average endorsement of deviant behavior.
14.5 Path Analysis
A multiple regression model generalizes the one-predictor variable model to include
multiple predictor variables to account for the variability of the response variable
Y.
path model:
Network of regression
equations that
specify causal flows. A path model generalizes further by specifying a network of regression models,
multiple variables linked by a hypothesized causal network. A simple example is the
mediation path model shown in Figure 14.8.
A straightforward, comprehensive R package for path analysis and related analyses is
lavaan (Rosseel, 2012; Rosseel, Jorgensen, & Rockwood, 2022), an acronym for latent
variable analysis. latent variable
analysis,
Section 15.4, p. 330
The mediation model analyzed with the mediation() function in
the previous section is analyzed here with lavaan. With sufficiently large bootstrap
samples, the mediation analysis output from the packages mediation and lavaan
provides identical results to beyond three decimal digits.
install.packages()
function,
Section 1.2.5, p. 8
Download the lavaan package once with install.packages().
library() function,
Section 1.1, p. 9
Run the library()
function for every new R session to access lavaan functions. Everything after the
number sign, #, indicates an optional comment, ignored by the computer.318 CHAPTER 14. CAUSALITY
install.packages("lavaan") # run only once
library("lavaan")
Begin the path analysis by specifying the path model. Define an equation for each path
in the path diagram, such as in Figure 14.8.
path coefficient:
The slope in a linear
equation that relates
one variable to its
hypothesized causal
antecedent.
Corresponding to the slope coefficients
in regression analysis are the path coefficients in path analysis.
The response variable DVB is hypothesized to have two causal antecedents, predictor
VAC and moderator ATD. In the model definition include two separate equations for
the two separate paths. As with standard R functions, use the tilde, ∼ , to indicate
an equation of the model. Name the path coefficients – a, b, and c in this example –
in each defining equation.
Customized effects can also be defined as the sum or product of the estimated
parameters from the model equations. In this analysis, our interest centers on the
indirect effect of VAC on DVB and, secondarily, the total effect. Name the indirect
effect ab, the product of the coefficient a and b as defined in the model. Name the
total effect total. Use the expression := to define these new parameter values.
Specify the model and specified effects as a single character string, enclosed in single
or double quotes, ' or ". Here name the character string mediate.
Input. Step 1: Define the partial mediation path model and any effects
mediate <- "
ATD ∼ a*VAC # model
DVB ∼ b*ATD
DVB ∼ c'*VAC
ab := a*b # effects
total := c'+ (a*b)
"
The three lines that form the path model in the preceding specification indicate the
same structure as the partial mediation model in Figure 14.8. These lines are the first
three lines, which begin with ATD, DVB, and DVB, respectively. Each of these lines
specifies a regression equation that corresponds to one of the arrows in Figure 14.8.
Also defined are the indirect effect and total effects implied by the model with the :=
operator.
sem() function,
lavaan: Do structural
equation modeling
such as path analysis.
Conduct the path analysis with the lavaan function sem(), for structural equation
modeling, the general term for the analysis of these and more general models. The
first parameter is the name of the character string that defines the model for analysis.
Specify the data frame with the standard parameter data. To request the bootstrap
standard errors, set the parameter se to "boot". Request the number of bootstrap
samples with the parameter bootstrap, usually set to a number in the multiple
thousands such as 5000 or 10000. Save the analysis of sem() to an output object,
here named fit.14.6. ANALYSIS PROBLEMS 319
Input. Step 2: Analyze the path model
fit <- sem(mediate, data=d, se="boot", bootstrap=10000)
Extract the information of interest from the fit object with the summary() function.
To request the confidence intervals, set the ci parameter to TRUE. The input and
output are shown in Listing 14.10.
Input. Step 3: Extract the relevant output
summary(fit, ci=TRUE)
Regressions:
Estimate Std.Err z-value P(>|z|) ci.lower ci.upper
ATD ~
VAC (a) 0.292 0.039 7.423 0.000 0.206 0.358
DVB ~
ATD (b) -0.096 0.008 -11.833 0.000 -0.114 -0.077
VAC (c) -0.010 0.009 -1.155 0.248 -0.031 0.010
Listing 14.10: Path analysis output from sem() for the path coefficients.
The output also includes the analysis of the specified effects, shown in Listing 14.11,
for the indirect and total effects. Both the descriptive statistic of the estimated value
of the effect and its associated confidence interval are provided.
Defined Parameters:
Estimate Std.Err z-value P(>|z|) ci.lower ci.upper
indirect -0.028 0.005 -5.591 0.000 -0.038 -0.019
total -0.038 0.009 -4.262 0.000 -0.056 -0.021
Listing 14.11: Path analysis output from sem() for the indirect and total effects.
The advantage of the lavaan approach for analyzing causal relations is that the
models can easily scale beyond the simple mediation model. Just add more equations
to the specification of the path model. Moreover, the analysis of more complex path
models includes a test of fit of the entire model. For the simple mediation model,
there is no test of fit because the model is fully saturated. The simple mediation
model includes all possible paths, so the fit of the overall set of equations, the full
model, is necessarily perfect. To obtain fit indices for path models, set the parameter
fit.measures to TRUE, when calling the summary() function.
structural
equation
modeling: Analysis
of a path model with
latent (unobserved)
variables.
In this section the measurements for the variables in the model are observed directly.
However, these models can also include latent, unobserved variables that are estimated
with measurements but not directly observed. The complete analysis of a path model
of latent variables is called structural equation modeling.
latent variables,
Chapter 15, p. 321
These latent variables are
the topic of an entire chapter.
14.6 Analysis Problems
1. Analyze the data at http://lessRsats.com/data/moderate1.xlsx with vari￾ables Y, X, and M.320 CHAPTER 14. CAUSALITY
a. Analyze the moderation model with W as the moderator, X as the predictor
variable, and Y as the response variable.
b. Does the analysis of the statistical output support W as a moderator? Why or
why not?
c. Support your analysis with a visualization that either supports moderation or
does not.
d. Interpret the results.
2. Analyze the data at http://lessRsats.com/data/moderate2.xlsx with vari￾ables Y, X, and W.
a. Analyze the moderation model with W as the moderator, X as the predictor
variable, and Y as the response variable.
b. Does the analysis of the statistical output support W as a moderator? Why or
why not?
c. Support your analysis with a visualization that either supports moderation or
does not.
d. Interpret the results.
3. Analyze the data at http://lessRsats.com/data/mediate1.xlsx with variables
Y, X, and M.
a. Analyze the mediation model with M as the moderator, X as the predictor
variable, and Y as the response variable.
b. What is the indirect effect?
c. Is the indirect effect statistically significant?
d. Compare the direct effect with and without the mediation model. Is there a
substantial difference in the value of the coefficient?
e. Run the path analysis of the partial mediation model with lavaan.
f. Interpret the results.
4. Analyze the data at http://lessRsats.com/data/mediate2.xlsx with variables
Y, X, and M.
a. Analyze the mediation model with M as the moderator, X as the predictor
variable, and Y as the response variable.
b. What is the indirect effect?
c. Is the indirect effect statistically significant?
d. Compare the direct effect with and without the mediation model. Is there a
substantial difference in the value of the coefficient?
e. Run the path analysis of the partial mediation model with lavaan.
f. Interpret the results.Chapter 15
Item and Factor Analysis
15.1 Quick Start
Install R and lessR as explained in Section 1.2. At the beginning of each R session,
access the lessR functions with library("lessR"). From Chapter 2, read your
data into an R data frame such as d with d <- Read("") to browse for the data file.
Or, locate your data file with a path name or web address between the quotes.
Factor analysis provides a relatively small number of abstract variables called factors
that account for the relations among a usually much larger set of measured variables.
A primary application of factor analysis in social science research is for attitude
surveys in which many items are designed to measure a much smaller number of
attitudes. The factor analysis computes and relates the factors to the measured
variables, the items, so each factor corresponds to an attitude of interest on the
attitude survey.
The factor analysis analyzes the correlations among the measured variables. Prepare
the data for the factor analysis by reading the responses to the measured variables,
such as attitude items, and then calculate the corresponding correlation matrix,
storing the result in R, the default correlation matrix name.
d <- Read()
R <- Correlation()
Or, just read a stored correlation matrix directly, here browsing for the data file that
contains the correlation matrix.
R <- corRead("")
scree() function,
Section 15.3.2, p. 327
The first consideration is an estimate of the smallest number of factors that reasonably
well account for the correlations among the measured variables. Use the scree plot to
help determine the number of factors. The plot is of the eigenvalues of the correlation
matrix plotted in descending order using the lessR function scree().
321322 CHAPTER 15. ITEM AND FACTOR ANALYSIS
scree()
Run the exploratory factor analysis with the lessR function efa(), which requires
the n_factors parameter to specify the number of factors to extract from the input
correlation matrix. efa() function,
Section 15.3.2, p. 328 Here, specify two factors. The extraction method is maximum
likelihood. The default rotation is the oblique promax method, with the orthogonal
varimax rotation also available.
efa(n_factors=2)
The factors from the exploratory analysis are often used to define and/or validate
multi-item measurement scales. Accordingly, the output of efa() also includes the
code to copy and paste into R for the subsequent confirmatory factor analysis.
cfa() function,
Section 15.4.2, p. 335
To evaluate the unidimensionality of a set of, for example, multi-item attitude scales,
evaluate the corresponding multiple indicator measurement model or MIMM with
a confirmatory factor analysis. Specify the variables that define each factor in the
call to the lessR function cfa(). This confirmatory factor analysis also provides the
reliability of each derived scale with Coefficient Alpha and Coefficient Omega.
15.2 Overview of Factor Analysis
15.2.1 Latent Variables
A guiding principle of science is parsimony, an explanation of relatively complex
parsimony: The phenomena with as few basic principles as possible.
simplest model to
account for the most
phenomena.
A second guiding principle is that
scientific explanations may involve abstract concepts that are not directly observable,
such as gravity. Instead, infer the existence of these concepts from measurements and
observations of the world in which we live.
latent variable:
Unobserved, abstract
variable.
Factor analysis is a statistical methodology consistent with these scientific principles.
Use factor analysis to build a model of the variables of interest expressed in terms
of a relatively small number of underlying abstractions called latent variables. A
classic example of a latent variable in social science research is an attitude such as
factor: The name of Machiavellianism or a personality trait such as Extroversion/Introversion.
a latent variable in
factor analysis.
In factor
analysis, operationalize these latent variables as factors, abstractions not directly
observed but which can be empirically inferred and validated from the correlations
between the corresponding measured variables.
Many studies in social science research use surveys and questionnaires that consist of
multiple items. Although the items can be presented in a randomized order, they
typically measure each attitude or personality characteristic of interest by a set of
items called a multi-item scale. Sum each persons responses to each item on such
item analysis: a scale to estimate the value of the underlying latent variable for that person.
Analysis of the
relations of items
and the
corresponding scales.
In
this context, the measured variables are the items on the attitude survey, and factor
analysis becomes the primary tool for item analysis. Item analysis analyzes the15.2. OVERVIEW OF FACTOR ANALYSIS 323
relations between the items and scales to guide the construction of the multi-item
scales. Each derived scale typically corresponds to a factor from the factor analysis.
One of the central issues in factor analysis is the number of factors needed to account
for the relations among the measured variables. Usually, the researcher begins with
a one-to-one correspondence between the factors and the attitudes of interest. For
example, consider the social construct of Machiavellianism, an essential component of
the Dark Triad (Paulhus & Williams, 2002), as operationalized by the Christie and
Geis (1970) 20-item Mach IV scale. Mach IV scale,
Table 1.1, p. 16
Suppose the Mach IV scale assesses the construct
of Machiavellianism. If so, a factor would presumably emerge from a factor analysis
indicating a measurement model that links each Mach IV item with this underlying
factor. This analysis would provide evidence for a unified Machiavellianism construct.
15.2.2 Measurement Models
measurement
model: Relates the
observed measures to
the latent variables.
The model that relates each measured variable to the factors is a measurement model,
a concept first delineated by Charles Spearman (1904) in one of the most influential
papers in all of the social sciences. The measurement model that specifies a single
Machiavellian factor links each item on the Mach IV scale to this one factor. unidimensional
measurement model:
The observed
measures have only
one latent variable in
common.
A
one-factor model, called a unidimensional model, postulates just a single dimension,
or factor, to underlie the 20 Mach IV items.
The specification of the unidimensional measurement model expresses each scale item
in terms of two attributes. The first attribute is the underlying factor itself, the
extent that the response to the item is attributable to what all the other items share.
By definition, content not shared with any other item is unique to that specific item.
regression model,
Section 11.3, p. 221
For the unidimensional model of Machiavellianism, write the model for each of the 20
Mach IV items as the following latent variable regression model.
mi = λiF + ui
This model accounts for the response to any item, the ith item, for a given respondent
in terms of two components.
1. The underlying attribute shared with all the other scale items, the factor F,
which here presumably is Machiavellianism
2. Plus some contribution, ui
, unique to that item
The complete unidimensional measurement model for the Mach IV items is the set of
20 such equations, one equation for each item.
For example, consider the sixth item on the scale, “Honesty is the best policy in all
cases”. The associated model for this item follows.
m06 = λ06F + u06
This regression coefficient, λ, is the weight of the measured variable, the item such
as m06, on the underlying factor, the common attribute that underlies all the items.
This weight is called a factor pattern coefficient, the result of a regression analysis on
a latent variable or factor, F.
factor pattern
coefficient: Model
coefficient that
relates the measured
item to a factor.
The factor analysis of this complete 20-equation model324 CHAPTER 15. ITEM AND FACTOR ANALYSIS
estimates λi for each item, which assesses how strongly the responses to the item
directly depend on the one shared common factor.
The relation between the factors and measured variables in the measurement model
is a causal specification. Each factor partially accounts for, or causes, the response
to the item. The causal impact depends on the item’s quality in terms of its clarity
and to the extent that it reflects the Machiavellian attitude. For example, someone
with a high Machiavellian attitude would respond Agree or Strongly Agree to items
consistent with Machiavellianism.
The analysis also portrays the extent that an item’s responses depend on the unique
qualities of the item, the uniqueness.
uniqueness:
Variance of an item
not shared with any
other items.
The uniqueness term is the underlying contri￾bution of the response to the item not shared with the other items on the scale. By
definition, the unique component of each item is uncorrelated with the remaining
random response items.
error: Component
of a response that is
attributable to
unexplained
randomness.
The uniqueness component consists, in part, of a random response component.
Each administration of the item potentially changes the random response error, like
flipping the same coin 10 times and getting 6 heads, then flipping the same coin
again and getting 4 heads on 10 flips. Random response error is relatively large for
ambiguous items. The respondent may infer one meaning on reading a vague item
and another meaning from another reading. The resulting response to the item is
more of a random response than dependent on the underlying attitude. systematic
response error:
Stable component of
a response that is
attributable to the
wrong content.
invalidity:
Measurement of an
unintended
component, the
result of systematic
error.
The uniqueness component also consists of systematic error, the contribution to
invalidity regarding the measurement of the underlying attitude. The item may
measure something stable but something other than the attitude of interest. Consider
the item “I like chocolate ice cream”. Presumably, this item analyzed as the 21st
Mach IV item would result in its corresponding responses demonstrating a low value
of λ and a high uniqueness component. The responses to this ice cream item likely
have little to do with Machiavellianism, so the responses would not be related in any
meaningful sense with the responses to the actual Mach IV items. The factor analysis
would quantify this lack of relationship with an estimated value of λ close to zero.
The factor analysis differentiates the attributes of each response shared with the
remaining items, the factor, from the uniqueness of each item. By definition, items
correlate only to the extent that one or more common attributes generate the item
responses. In particular, the measurement model, which regresses the measured
variables, the items, onto the factors, imposes the correlational structure of the
measured variables. The factor analysis reverses this process, inferring the underlying
factor structure from the correlations among the variables of interest.
15.3 Exploratory Factor Analysis
15.3.1 Extraction then Rotation
The two primary types of factor analysis are confirmatory and exploratory, the latter
the subject of this section. Both models express each measured variable as a function
of the underlying factors and the corresponding uniqueness term. The distinction is15.3. EXPLORATORY FACTOR ANALYSIS 325
that exploratory factor analysis, introduced by Thurstone (1931), specifies only the
number of factors to begin the analysis. Exploratory factor analysis, as opposed to
confirmatory factor analysis, expresses each measured variable as a function of all
the factors indicated in the study.
For example, suppose two specified factors account for the correlational structure of
six items. Then the measurement model estimated by the corresponding exploratory
factor analysis defines six equations, each with two factors. Relate each measured
variable to each underlying factor with a pattern coefficient, a λ coefficient.
X1 = λ11F1 + λ12F2 + u1
X2 = λ21F1 + λ22F2 + u2
X3 = λ31F1 + λ32F2 + u3
X4 = λ41F1 + λ42F2 + u4
X5 = λ51F1 + λ52F2 + u5
X6 = λ61F1 + λ62F2 + u6
Use the analysis to uncover the structure in which the response for each measured
variable, such as an item, is causally dependent on a single factor and is not directly
dependent on any other factor in the system. For each measured variable, the
exploratory factor analysis estimates the value of each λ for each factor.
simple structure:
A factor solution in
which each item
primarily relates to a
single factor.
Although
each measured variable directly relates to every factor at some level, in the desired
solution, each item has one relatively large value of λ, and the rest of the λ’s are
reasonably close to zero in value. This style of solution is a simple structure solution.
underidentified
model: The model
has no unique
solution from the
available data.
The measurement model for the exploratory factor analysis is underidentified, with no
unique solution for each estimated value in the model. The lack of a unique solution
is the same situation from high school algebra for a system of equations with more
unknowns than equations. An underidentified model has too many unknowns for
the information obtained from the given equations, which leads to many possible
solutions all consistent with the measurement model.
factor extraction:
First set of factors in
an exploratory
analysis.
Because of under-identification, exploratory factor analysis of a set of variables
described by multiple factors consists of two different analyses. First, obtain the
initial solution, the factor extraction. The criterion for factor extraction defines the
first factor that relates the most strongly as possible to all of the observed variables.
The problem with the initial solution is that there is usually a strong general factor.
Most items tend to be highly related to this general factor and less so with the other
factors. Then the second extracted factor is the factor most related to all of the items
after extraction of the first factor. The result of the initial extraction of the specified
number of factors is the opposite of simple structure’s desired goal.
factor rotation:
Second set of factors
in an exploratory
analysis, which are
interpreted.
orthogonal
rotation:
Uncorrelated factors
from the rotation.
oblique rotation:
Correlated factors
from the rotation.
The factors from the initial factor extraction solution are uncorrelated. When the
subsequent rotated solution is obtained, the rotated factors can remain uncorrelated
orthogonal. The initial factors can also be rotated into a correlated solution, resulting
in oblique factors. However, the attitudes usually correlate with the usual attitude326 CHAPTER 15. ITEM AND FACTOR ANALYSIS
survey that measures multiple attitudes.
To study several multi-item attitude scales, typically compare the scales present on
the attitude survey with scales defined by the factor analysis. Define a derived scale
for each extracted factor, and then place each item on the scale for which it is most
related to the corresponding factor. To derive scales of related items, the choice of
orthogonal or oblique factor is not critical as both do reasonably well at this task
(Gerbing & Hamilton, 1996).
15.3.2 Exploratory Analysis of Mach IV Items
Get and Prepare the Data
The data for our example of factor analysis are the responses to the Mach IV scale
Mach IV scale, to measure Machiavellianism.
Section 1.1, p. 16
The lessR package includes the data, so read by
specifying the name of the data set in quotes as the first parameter value.
d <- Read("Mach4")
item reflection:
Agreement with the
item indicates the
opposite of
agreement with the
scale.
Before conducting a factor analysis, code the items on a scale in the same direction.
Many attitude scales are written to encourage the respondent to carefully read each
item by writing some of the items in the opposite direction from the others, called
item reflection. For the Mach IV scale, agreement with nine of the 20 items indicates
high Machiavellianism. However, 11 of the 20 items are written so that agreement
with the item indicates low Machiavellianism. Use the lessR function recode() to
reverse score, reverse score these 11 items before computing the item correlations.
Section 3.4.1, p. 49
d <- recode(c(m03,m04,m06,m07,m09,m10,m11,m14,m16,m17,m19),
old=0:5, new=5:0)
The item correlations in the form of a 20x20 correlation matrix are required for
factor analysis. Obtain the correlations using the lessR function Correlation(),
correlation matrix, abbreviated cr(), which by default sets the data parameter to d.
Section 10.3, p. 205
R <- Correlation(m01:m20)
Scree Plot
In contrast to confirmatory factor analysis, exploratory analysis does not require
the specification of a complete measurement model. Exploratory factor analysis
only requires that the number of factors that account for item correlations first be
specified. Probably the most helpful test for the optimal number of factors is the
scree plot, based on what are called eigenvalues from the initial factor extraction of
the item correlation matrix. The eigenvalues are a rescaling of the extracted factors,
one eigenvalue per factor. The first extracted factor has the largest eigenvalue, the
second extracted factor the second largest, and so forth.
A “scree” refers to a geological description of a cliff in which rock and dirt have
slid down the face of the cliff, gathering at the bottom.
scree plot: Plot of
successive
eigenvalues from the
correlation matrix to
identify the number
of factors.
The scree plot plots the15.3. EXPLORATORY FACTOR ANALYSIS 327
eigenvalues sequentially in descending order. There are as many eigenvalues as items
in the input correlation matrix. However, when plotted, the size of these eigenvalues
tends to resemble a scree. The scree plot separates the essential factors, on the cliff,
from the scree, the rubble piled up at the bottom.
Scenario. Estimate the number of factors of a correlation matrix
Analyze a correlation matrix to suggest the number of factors that should be
specified for an exploratory factor analysis.
scree() function,
lessR: Obtain a
scree plot and plot of
eigenvalue
differences.
The lessR function corScree(), abbreviated scree(), provides a scree plot of the
eigenvalues of the input correlation matrix. To generate the scree plot for the default
correlation matrix R, no argument to the function is needed.
Input. Scree and scree difference plot
scree()
The scree plot for the 20-item Mach IV correlation matrix appears in the left side of
Figure 15.1. Where does the “cliff” end and the “scree” begin? The answer based
on this scree plot is around the 4th eigenvalue, after which the “scree” begins to
accumulate. At this point the slope becomes much less steep.
Figure 15.1: For the 20-item Mach IV correlation matrix, scree plots for the successive
eigenvalues (left) and for the difference of successive eigenvalues (right).
To view this change in slope between the “cliff” and the “scree” more directly, your
author prefers to evaluate the plot of the difference of the successive eigenvalues, to
plot the successive changes directly. Accordingly, the scree() function also provides
this plot, shown in the right side of Figure 15.1. After the first four eigenvalues, the
rate of change is essentially 0, which plots as a flat line. This flat line of the plotted
differences of the eigenvalues represents the scree.
However, a purely statistical criterion for the number of factors is, at best, a guideline
for the most useful number. The final decision regarding the number of factors
also depends on the interpretability of the resulting exploratory factor analysis. For
example, if a five-factor solution is more interpretable than a four-factor solution,
that former solution is preferred. Perhaps only a small number of items represent
a crucial concept corresponding to a factor. Hence, the corresponding factor has a328 CHAPTER 15. ITEM AND FACTOR ANALYSIS
relatively small eigenvalue, but still include the factor in the results because of its
substantive importance.
The Analysis
The scree plot, and its associated plot of successive differences, provide a useful
starting point for specifying the number of factors that underlie the correlation
matrix, a required starting point for the exploratory factor analysis. Next, proceed
with the exploratory factor analysis.
Scenario. Exploratory factor analysis
Given a correlation matrix of the measured variables, such as the items, extract
the specified number of factors and then rotate to a correlated factor solution.
efa() function,
lessR: Exploratory
factor analysis.
The lessR exploratory factor analysis routine is corEFA(), abbreviated efa(). The
required n_factors parameter specifies the number of factors. Of the many factor
extraction methods, the R factor analysis function factanal(), upon which efa()
Mach IV correlations, relies, does a maximum likelihood extraction.
R, Section 15.3.2,
p. 326
Here, proceed with the four-factor
maximum likelihood exploratory analysis of the default correlation matrix R.
Input. Exploratory factor analysis for a four-factor model
efa(n_factors=4)
promax rotation:
Oblique factor
solution. The default efa() rotational method is promax, a rotational method that yields corre￾lated, that is, oblique, factors. The underlying latent variables of interest, such as the
attitudes of interest, correspond to the extracted factors. The alternative rotates the
initially extracted factors to an uncorrelated or orthogonal solution. The orthogonal
rotation method R provides is varimax.
varimax rotation:
Orthogonal factor
solution.
rotate parameter:
Can specify a
varimax rotation.
To specify, include rotate="varimax" in
the call to efa().
The primary output of the exploratory factor four-factor solution appears in List￾ing 15.1. R labels the output Loadings, an ambiguous term. Here, the term refers to
the pattern coefficients, the λ’s of the underlying measurement model. Loadings with
a value close to zero, by default values between −0.2 and 0.2, are replaced by blanks
to highlight the more important coefficients. Change the minimum value displayed
by specifying a value other than 0.2 for the min_loading parameter.
min_loading
parameter: Specify
the minimum loading
to display on the
output.
By default
the items are sorted by their highest loading across all the factors, though just the
items with loadings of 0.5 or greater are listed first. To suppress sorting, specify the
sort parameter: If following in the call to efa(), sort=FALSE.
FALSE then no
sorting of the items
by factor loading.
The four factors in this solution define a four-dimensional space, a coordinate system
with four axes, each item plotted in that space. In most social science research,
however, the interpretation of this loading matrix is usually not based on the factors
as abstract dimensions but rather on the multi-item scales defined from this loading
matrix. Listing 15.1 shows, for example, that Items m06, m07, m10, m03, and m09 all
load primarily on the first rotated factor. The interpretation of the underlying factor,
and the corresponding scale of items, follows from the content of these items. The
following section explores scale construction from the factor analysis.15.3. EXPLORATORY FACTOR ANALYSIS 329
Loadings:
Factor1 Factor2 Factor3 Factor4
m06 0.828 -0.290
m07 0.712
m10 0.539
m05 0.649
m13 0.543 -0.226
m18 0.555 -0.253
m14 -0.402 0.991 -0.401
m11 0.299 0.309 -0.609
m01 0.490
m02 0.319
m03 0.422 -0.318
m04 0.426
m08 0.236 0.202
m09 0.323
m12 0.434 0.230
m15 0.207 0.203 0.214
m16 0.274 -0.455
m17 0.267
m19
m20 0.237 0.282
Listing 15.1: Rotated factor loadings of the four-factor extraction of the Mach IV item
correlations.
Listing 15.2 shows the relations of the four factors across all the measured variables.
Here, SS loadings refer to the sum of the squared factor loadings across all of the
items for each factor.
Factor1 Factor2 Factor3 Factor4
SS loadings 1.933 2.038 1.825 1.099
Proportion Var 0.097 0.102 0.091 0.055
Cumulative Var 0.097 0.199 0.290 0.345
Listing 15.2: Sum of the squared loadings of each factor across the measured variables and
the proportion of the total variance accounted for by each factor.
There can be as many factors as there are items, so the total variance to account
for is 20. The proportion of variance accounted for by the sum of squared loadings,
Proportion Var, for the first factor follows.
Proportion of Variance for Factor #1: 1.933/20 = 0.097
The sum of squared loadings for the first three factors all approximate 2, whereas
there is a drop-off for the fourth factor, a value that only approximates 1. However, as
mentioned previously, do not interpret these indices too literally. Perhaps the items
with the content that tap into the fourth factor are simply fewer than the number
of items that tap into the other factors. The issue, again, cannot be reduced solely
to statistics. Instead, focus on the interpretability and usefulness of the factors for
explaining Machiavellianism in this example. Listings 15.1 and 15.2 are the primary
output of the exploratory factor analysis. From this information, interpret the330 CHAPTER 15. ITEM AND FACTOR ANALYSIS
meaning of the extracted factors, usually in the form of the corresponding multi-item
scales defined based on the pattern of factor loadings.
Applying exploratory factor analysis to define and/or revise a set of multi-item scales
is straightforward. Define a scale for each extracted factor and then place each item
on the scale for which it has the highest factor loading, perhaps only retaining those
items that load on the factor at some minimum value, such as 0.4. To facilitate the
scale development, the last part of the efa() output defines the corresponding scales
and then constructs the code to run the confirmatory analysis, the topic of the next
section. Copy and paste this code into R to analyze the resulting scales.
15.4 Confirmatory Factor Analysis
Exploratory factor analysis defines the factors as dimensions that set up a coordinate
system in multi-dimensional space. However, a set of coordinate axes is not a set
of scales. The construction of the scales from these factors is an ad-hoc procedure
without formal statistical evaluation of the dimensionality of the scales composed
of individual items. Fortunately, exploratory factor analysis can suggest multiple
indicator measurement models (Gerbing & Hamilton, 1996), but a suggestion is not
an evaluation of fit.
confirmatory
factor analysis:
Analysis of a
specified
measurement model.
Following the initial work of Spearman (1904) and others toward the beginning
of the 20th century, the re-emergence of confirmatory factor analysis during the
latter part of the 20th century for social science research has once again provided
access to tools for formally testing the dimensionality of the resulting scales. For
confirmatory factor analysis, specify the measurement model before the analysis.
The analysis estimates the parameters of the measurement model, the values of the
pattern coefficients, the λs, and the factor correlations. These estimates derive the
imposed correlation matrix of the measured variables and compare them to the actual
item correlations. Empirically confirm the measurement model to the extent that the
imposed correlations match the existing correlations.
The general process of factor analysis based research for attitude surveys follows a
standard pattern.
1. Gather the responses to the items on the survey.
2. Calculate the correlations among the items.
3. Do an exploratory factor analysis to get a sense of the underlying scales as
responded to by the respondents.
4. Specify and then perform a confirmatory analysis of the correlations to estimate
the model parameters such as the λ’s, preferably on new data to be able to
perform a true statistical test of model validity.
5. Evaluate the fit of the estimated model by comparing the item correlations
implied by the model to the actual item correlations.
An explanation of the meaning of these imposed correlations follows.15.4. CONFIRMATORY FACTOR ANALYSIS 331
15.4.1 Covariance Structure
A confirmatory factor analysis estimates a specified measurement model. Here apply
confirmatory factor analysis to a correlation matrix with the correct measurement
model known in advance of the analysis. We know the correct model because we define
our measurement model and then derive the item correlations computed not from
data but from the corresponding correlation pattern implied by this measurement
model. covariance
structure:
Covariances, such as
correlations, of the
variables implied by
the measurement
model.
We analyze the resulting correlations imposed by that model, the model’s
covariance structure.
1 This model is devoid of psychological content, but serves as a
helpful exercise to illustrate the conceptual basis of confirmatory factor analysis.
Our model includes six measured variables and two factors, with three variables linked
to one factor and the other three variables linked to the other factor. This model
contains multiple factors, but each item directly indicates only a single factor, an
example of a multiple indicator measurement model or MIMM. MIMM: A
measurement model
with each item
directly linked to
only one factor.
A MIMM is generally
the most useful type of measurement model for scale development because the items
on an attitude survey are usually partitioned into multi-item scales. Each item
measures a single attitude, modeled as a factor within this framework.
Define this MIMM as the six equations that link each measured item to one of the
two underlying factors. Also, specify the correlations among the factors. Specify
a moderate population factor correlation of 0.3, and a sequence of the population
pattern coefficients, the λs, that step from 0.8 to 0.6 to 0.4 for the three indicators of
each factor. Here, we specify our own population model, so we use the symbol for
the population correlation, ρ.
X1 = .8F1 + u1 X4 = .8F2 + u4
X2 = .6F1 + u2 X5 = .6F2 + u5
X3 = .4F1 + u3 X6 = .4F2 + u6
ρF1,F2 = 0.3
path diagram:
Visualization of a
causal flow between
variables with
connecting one-way
arrows indicating
causality.
path diagram,
Section 14.2.1, p. 304,
Section 14.4.1, p. 310
This model can alternatively be expressed as a path diagram, shown in Figure 15.2.
F1 F2
0.3
0.8 0.4
0.6
0.8 0.4
0.6
x2
x3 x1
x5
x6 x4
Figure 15.2: Specified population two-factor multiple indicator measurement model, three
items per factor.
1A correlation is a specific type of covariance in which the variables are standardized, with
variances of 1.00.332 CHAPTER 15. ITEM AND FACTOR ANALYSIS
The lines with arrowheads only at one end represent a causal structure, which
specifies that the response to each observed variable is due to the underlying shared
factor and an implied uniqueness term. The curved line with double-headed arrows
represents a correlation with no underlying causality specification. Factors F1 and
F2 are correlated, but this model provides no causal explanation as to why they are
correlated.
Now the critical question. How does a measurement model dictate or impose the
correlations among the measured variables, here the correlation matrix for variables
X1 through X6? Matrix algebra provides the general result. Fortunately, for the
multiple indicator measurement model, which posits only one factor underlying each
item, we can provide these equations without resorting to matrices.
External Consistency
external
consistency:
Covariance structure
of items on a
unidimensional scale
with respect to other
items.
The product rule for external consistency (Spearman, 1914; Gerbing & Anderson,
1988) determines the correlation of two items from different factors defined by a
MIMM. The extent of the item correlation depends not only the strength of the
relationship of each item with its own factor but also on the factor correlations. For
example, if the factors have a low correlation, then items from these two different
factors also have a low correlation.
The corresponding product rule provides the quantitative relationship for two items,
one from Factor F1 and another from Factor F2.
product rule for external consistency: ρXiXj = λi(ρF1,F2
)λj
For example, the correlation of Items X1 and X5 consistent with the underlying
measurement model in Figure 15.2 follows.
ρX1X5 = λ1(ρF1,F2
)λ5 = (0.8)(0.3)(0.6) = 0.144
Internal Consistency
A special case of external consistency applies to the correlations of the items of the
internal same factor, expressed as a factor-factor correlation of 1.0.
consistency:
Covariance structure
of items on a
unidimensional scale.
The product rule for
internal consistency specifies that the correlation of two items that both measure the
same factor depends on their corresponding pattern coefficients, the λ’s. Express the
rule for Items of the same factor, Xi and Xj .
product rule for internal consistency: ρXiXj = λi(1.0)λj = λiλj
One expects that higher item correlations occur for two items highly related to their
common factor. Show this pattern quantitatively by applying the product rule for
internal consistency, here for Items X1 and X2.
ρX1X2 = λ1λ2 = (0.8)(0.6) = 0.48
Similarly, apply the product rule to the second and third items.
ρX2X3 = λ2λ3 = (0.6)(0.4) = 0.2415.4. CONFIRMATORY FACTOR ANALYSIS 333
Apply the product rules to all pairs of the six items to generate the correlation matrix
perfectly consistent with this measurement model in Figure 15.2.
corRead() function,
lessR: Read a
correlation matrix
into R.
The matrix is
stored on the web. The lessR function corRead(), abbreviated rd_cor(), reads a
correlation matrix into R as shown in Listing 15.3.
correlation matrix,
Section 10.2.1, p. 194
Store the correlations in a data
structure called R, the default name for the input correlation matrix for the lessR
correlational analysis functions.
Input. Read a correlation matrix
R <- corRead("http://lessRstats.com/data/MIMMperfect.cor")
R
X1 X2 X3 X4 X5 X6
X1 1.000 0.480 0.320 0.192 0.144 0.096
X2 0.480 1.000 0.240 0.144 0.108 0.072
X3 0.320 0.240 1.000 0.096 0.072 0.048
X4 0.192 0.144 0.096 1.000 0.480 0.320
X5 0.144 0.108 0.072 0.480 1.000 0.240
X6 0.096 0.072 0.048 0.320 0.240 1.000
Listing 15.3: Population correlation matrix, the covariance structure, of the six observed
variables imposed by their multiple indicator measurement model.
Although correlation does not establish causation, a causal pattern does imply
correlation. The relation of the two underlying factors as specified by the measurement
model of the six items in Figure 15.2 constrains the population correlations of the
items to those in Listing 15.3.
Communalities
We have considered the correlations of two items from the same and different factors
consistent with the underlying measurement model. What about each item’s correla￾tion with the diagonal elements in the correlation matrix, such as in Listing 15.3?
Each item correlates with itself perfectly, the values of the diagonal elements are all
1’s. These diagonal elements are fundamentally different from and larger than the
off-diagonal elements.
The basis of this distinction between diagonal and off-diagonal correlations is that
the correlation of two different items directly depends only on the strength of their
relationships with the underlying factor or factors. However, an item’s correlation
with itself depends on both the influence of its underlying factor and uniqueness
component. The relationship between the pattern coefficient λ and the variance of
the item uniqueness attribute for the ith item follows.
ρXiXi = 1 = λ
2
i + σ
2
ui
communality:
Proportion of an
item’s variance due
only to the common
factors.
An item’s communality is the proportion of the correlation of an item with itself that
is due only to the shared factors. For a unidimensional model of the type specified
here, this communality, here λ
2
i
, is due only to the one underlying factor.
The expression of an item’s communality follows from a special case of the product
rule for internal consistency, applied to the same item twice.334 CHAPTER 15. ITEM AND FACTOR ANALYSIS
communality of ith item: λiλi = λ
2
i
To solve for this communality, extract this common contribution to the full variation
of the item, here a value of 1.0. This extraction of the common influence of the factors
on item variability, the ability to distinguish between an item’s communality and
uniqueness component, is the solution for the factor pattern coefficient, λi
. The factor
analysis, then, transforms the observed correlation matrix with 1’s in the diagonal to
the correlation matrix with communalities in the diagonal.
iteration: A The communalities are computed step-by-step, from one iteration to the next.
computed solution,
the results of which
are entered back into
the model to
compute another
solution.
Com￾pute the communalities by starting with the 1’s in the diagonal of the input correlation
matrix. Then estimate the corresponding pattern coefficients and factor-factor corre￾lations with one of the many available factor analysis estimation procedures. Next,
square the resulting estimated pattern coefficients according to the product rule for
internal consistency. Insert those values into the diagonal. Repeat this process until
the solution changes little from iteration to iteration. The result is the final estimated
parameters of the model.
15.4.2 Analysis of a Population Model
Now we have the needed information to proceed to the confirmatory factor analysis
of the correlation matrix in Listing 15.3. The covariance structure rules imposed by
a multiple-indicator measurement model presented in Figure 15.2 generated these
correlations.
Scenario. Recover the measurement model from the imposed correlations
Does the confirmatory factor analysis of the constructed correlations return the
same parameter values, the pattern coefficients and the factor correlations, from
which the imposed correlations were constructed?
calculate correlation A factor analysis begins from the correlations between each pair of variables.
matrix, Section 10.3,
p.205
One
possibility uses the lessR function Correlation() to calculate the correlations from
the original data frame that contains the raw data. Or, read the already computed
correlations from an external file, the only choice available in this example because
the correlations were computed directly from the product rules imposed by the model.
This is not a standard data analysis to evaluate a posited structure that may or
may not fit the data. Instead, this conceptual exercise examines the validity of the
confirmatory factor analysis procedure, an attempt to recover a known structure from
its implied correlations, its covariance structure.
cfa() function,
lessR: Confirmatory
factor analysis of a
multiple indicator
measurement model.
The lessR confirmatory factor analysis program corCFA(), abbreviated cfa(), an￾alyzes a MIMM. The cor in the full name for the procedure indicates that the
information entered into the procedure for analysis is not the direct responses to
the items, but their correlations. The required parameters to cfa() are the lists of
items that define each factor. By default, cfa() analyzes the correlations in the R
correlation matrix.
lavaan package,
Section 14.5, p. 318,
Section 15.14, p. 345
The input to the lessR function cfa() follows the form of the functions in the lavann
package for specifying models. To do a confirmatory factor analysis, begin by defining15.4. CONFIRMATORY FACTOR ANALYSIS 335
the measurement model as a character string with a chosen name. Here name the
character string Perfect. Within the character string, use the operator =∼ to define a
latent variable in terms of its observable components, choosing names for each latent
variable. Here name the factors generically, F1 and F2.
correlation matrix,
Pass the character string that defines the model to the cfa() function as its first Section 15.3.2, p. 326
parameter. matrix R,
Section 15.3, p. 333
The default correlation matrix, R, has already been read. Otherwise, pass
the name of the matrix as the second parameter, or includes its parameter name, also
R, and specify in any position in the function call.
Input. Confirmatory factor analysis of a two factor MIMM
Perfect <- "
F1 =∼ X1 + X2 + X3
F2 =∼ X4 + X5 + X6
"
cfa(Perfect)
heat map,
Section 10.3.4, p. 209 The output of cfa() is primarily text, but also includes one visualization, the heat
map of the item-correlation matrix with communalities in the diagonal. heat map:
Visualization of item
correlations that
replaces each
correlation coefficient
with a colored
square.
The heat
map displays a patch of color or grayscale for each correlation coefficient, as shown
in Figure 15.3. The darker shades of gray indicate the items that more strongly
correlate.
Figure 15.3: Heat map of 6-variable correlation matrix with communalities in the diagonal.
The pattern of the two three-item blocks on the heat map, in the top-left and bottom￾right corners, is the “hierarchical ordering” of unidimensional correlations initially
observed by Spearman (1904) well over a century ago. Items within the same factor
correlate less to the extent that are more weakly related to the underlying factor.
Items within each factor correlate more highly than items in different factors, as
shown by the lighter shades of gray in the bottom-left corner and top-right corner of
Figure 15.3.
The primary output of the confirmatory factor analysis, in Listing 15.4, begins
with the estimated pattern coefficient for each measured variable, the λ, and its
corresponding uniqueness.336 CHAPTER 15. ITEM AND FACTOR ANALYSIS
Indicator Analysis
-------------------------------------------------------------------------
Fac Indi Pat Unique Factors with which an indicator correlates too
tor cator tern ness highly, and other indicator diagnostics.
-------------------------------------------------------------------------
F1 X1 0.800 0.360
F1 X2 0.600 0.640
F1 X3 0.400 0.840
F2 X4 0.800 0.360
F2 X5 0.600 0.640
F2 X6 0.400 0.840
Listing 15.4: Estimated factor pattern coefficients, uniquenesses and diagnostics.
The confirmatory factor analysis perfectly recovered the underlying true measurement
model. The λ’s computed by the factor analysis, listed under the heading of Pattern,
are the exact values used to generate the covariance structure, the item correlation
matrix. There is also space for diagnostic information in the output to help understand
aspects of the model that do not fit the data, the model misspecifications.
misspecification:
A specified model
that does not match
reality.
In this
example, this area is blank because the input to the analysis, the covariance structure
in the form of a correlation matrix, fits the estimated model perfectly, as designed.
The following output, in Figure 15.4, is the complete item-factor correlation matrix
with communalities in the diagonal of the item correlations. Here, observe all of
the item-factor correlations, the correlations of the items, the measured variables,
with the factors, the latent variables.
item-factor
correlation: A
correlation of an
item with a factor.
We see, for example, that the first item, X1,
correlates 0.8 with its own factor, F1, and 0.24 with the second factor, F2. The factor
correlation of an item on its own factor for an MIMM is just the corresponding pattern
coefficient, λ.
Figure 15.4: Annotated output correlation matrix with communalities in the diagonal of
all measured and latent variables in the analysis.
The factor correlation matrix appears in the bottom right of the complete matrix
in Figure 15.4. The correlation of the two factors, F1 and F2, is 0.3. This value
for the factor correlation specified in the constructed model has also been recovered
by the confirmatory factor analysis of the implied population covariance structure,
validating the analysis for the estimation and evaluation of measurement models for
data analysis.
For an analysis of many variables, sometimes only the item-factor and factor-factor
correlations are needed. To display just those correlations on the cfa() output, set15.4. CONFIRMATORY FACTOR ANALYSIS 337
the parameter item_cor to FALSE in the function call.
item_cor
parameter: If FALSE,
then the item
correlations are not
How well does the model fit the correlations of the measured variables? Examine displayed.
the residuals in Listing 15.5. Each residual is the difference between the corre￾sponding item correlation and its value imposed by the estimated multiple indicator
measurement model.
residual: Difference
between the actual
correlation and the
imposed by the
model.
Residuals
--------------------
X1 X2 X3 X4 X5 X6
X1 0 0 0 0 0 0
X2 0 0 0 0 0 0
X3 0 0 0 0 0 0
X4 0 0 0 0 0 0
X5 0 0 0 0 0 0
X6 0 0 0 0 0 0
Listing 15.5: Residuals, the difference between observed correlations and those specified by
the estimated model.
The analysis perfectly recovered the structure of the underlying model. All residuals
are zero, as they should be, because the correlation matrix was generated to be
perfectly consistent with the underlying model.
15.4.3 Proportionality
external consistency,
Section 15.4.1, p. 332
The product rule for external consistency implies that any two items of the same factor
correlate proportionality with all other items, including items from other factors and
communalities. To illustrate, return to Figure 15.4, which lists the item correlations
with communalities in the diagonal. Consider the first two items, X1 and X2, their
correlations with all other items, and the corresponding ratios, shown in Table 15.1.
Item X1 X2 Ratio
X1 0.640 0.480 1.333
X2 0.480 0.360 1.333
X3 0.320 0.240 1.333
X4 0.192 0.144 1.333
X5 0.144 0.108 1.333
X6 0.096 0.072 1.333
Table 15.1: Respective correlations of Item X1 and also Item X2 across all six items in the
model and their corresponding ratios, proportions.
How can these proportionality constants help diagnose and suggest measurement
models consistent with the data?
Scenario. Obtain the matrix of proportionalities
A heuristic aid to specify a multiple indicator measurement model consistent with
the data is a matrix of proportionality coefficients, which provides, for each pair of
items, the average proportionality of their correlations with all other items.338 CHAPTER 15. ITEM AND FACTOR ANALYSIS
The lessR function corProp(), abbreviated prop(), provides the matrix of average
proportionalities (Hunter, 1973). Again, the default input correlation matrix is that
contained in the R output from Correlation(). Here, the output of prop() is not
saved to a data structure with the assignment operator <-, so the output instead is
directed to the console as text output.
Input. Proportionality coefficients
prop()
Figure 15.5: Annotated proportionality matrix.
As seen with these perfect correlations according to their fit with the underlying
model, the proportionalities of items that measure the same factor are all 1.00.
perfect model,
Section 15.4.2, p. 334 Recovering the postulated model from the implied covariance structure is a conceptual
exercise. This situation has no model specification error because the model is perfectly
specified. There is also no sampling error because the implied population correlations
were input into the analysis. In the real world of data analysis, sampling error
ensures that the model parameters are not perfectly recovered even for a well-specified
model. If multiple samples were taken, each sample would generate different estimates
because each sample of data is different from every other sample, and no sample
perfectly represents the population.
The process of a confirmatory factor analysis with actual data follows.
15.5 Confirmatory Analysis of Mach IV Items
15.5.1 Analysis of Model from Exploratory Analysis
Mach IV data,
Listing 1.5, p. 17
We return to the analysis of the responses to the 20 items on the Mach IV scale.
Scenario. Evaluate the implied Mach IV measurement model
A set of four Mach IV subscales was constructed based on an exploratory factor
analysis of all 20 Mach IV items. Now evaluate the dimensionality of these scales
with a confirmatory factor analysis of the resulting multiple indicator measurement
model.
The confirmatory factor analysis specification provided by the four-factor exploratory
analysis from the efa() function follows. To run this analysis, copy the instructions
for the cfa() function call at the end of the efa() output in Listing 15.6, and then
paste back into R. Before running, rename the factors for clarity.15.5. CONFIRMATORY ANALYSIS OF MACH IV ITEMS 339
MeasModel <- "
F1 =~ m01 + m02 + m03 + m04 + m05
F2 =~ m06 + m07 + m08 + m09 + m10 + m11
F3 =~ m12 + m13 + m14 + m15
F4 =~ m17 + m18 + m19 + m20
"
Listing 15.6: Confirmatory factor analysis code from the exploratory factor analysis.
The formal analysis of the corresponding MIMM indicates that the model does not
fit well. For example, 10 of the 19 items have loadings on their own factor at 0.4 or
less, and 7 of the 19 items in the model correlate more with another factor than they
do their own. The size of the average absolute value of all the residuals is also large,
0.051. The model direct from the exploratory analysis needs refinement
15.5.2 Revised Model
How to respecify the corresponding measurement model to improve fit? One strategy
deletes some of the poor fitting items from the model, such as those with low λ values.
Another approach recognizes that two items of the same underlying factor have, in
the population, the same constant of proportionality across all other variables, both
items that also measure the same factor, and items from other factors, and, indeed,
any other variable from anywhere.
item proportionality
of correlations,
Figure 15.5, p. 338
Scenario. Obtain and reorder proportionality coefficients
Convert the correlation matrix of Mach IV items to a matrix of proportionality
coefficients. Then re-order the matrix of proportionality coefficients so that similar
items tend to appear next to each other in the matrix. Identify a MIMM that
complements the guidance provided by the exploratory factor analysis.
To obtain these re-ordered proportionality coefficients, invoke the prop() function as
before. Now save the new matrix into another R object, here called P, so that the
proportionalities can be re-ordered with the lessR function corReorder(), abbrevi￾ated reord().
corReorder()
function, lessR:
Reorder the variables
that define a
correlation matrix.
The output from reord() here is not directed to a data structure, so
R directs the output to the console for viewing as text output.
corReorder()
function, lessR
Listing 10.3.6, p. 211
Input. Reordered proportionalities
P <- prop()
corReorder(P)
The algorithm lists variables with parallel profiles of correlation coefficients next to
each other. The reord() function identifies the variable in the matrix most strongly
related to all the other variables, the variable with the highest sum of squared
coefficients. This variable is placed first in the re-ordered matrix. Then the second
variable is the variable that has the highest coefficient with the first variable. The
third variable has the highest coefficient with the second, and so on.
Figure 15.6 lists the proportionality coefficients for the first eight items listed from
the output. The gray regions in the figure highlight how this technique isolated340 CHAPTER 15. ITEM AND FACTOR ANALYSIS
two groups of items such that the proportionality coefficients within each group are
relatively larger than those coefficients between the groups.
Figure 15.6: Annotated proportionality coefficients for the items that define the first two
factors in the confirmatory factor analysis of Mach IV.
To construct a best-fitting model, we need to integrate the information from the
exploratory analysis, the re-ordered proportionality coefficients, and the meaning of
the items. The two groups of items delineated in Figure 15.6 are a subset of the final
model (Hunter et al., 1982). The corresponding call to the lessR function cfa() of
the specified measurement model follows. The analysis is of the default correlation
matrix R data structure.
Mach IV correlations,
R, Section 15.3.2,
p. 326
Input. Confirmatory factor analysis of final four-factor Machiavellianism model
MeasModel <- "
Deceit =∼ m06 + m07 + m09 + m10
Flattery =∼ m15 + m02
Cynicism =∼ m04 + m11 + m16
Distrust =∼ m01 + m05 + m12 + m13
"
cfa(MeasModel)
The average residual of the confirmatory factor analysis of this four-factor model is
just 0.036. So the model has reasonable fit, but what does it mean? Factor and item
analysis are statistical techniques. Ultimately, however, the results need to make
conceptual sense, they need to be interpretable. An understanding of content should
guide the entire process of scale construction.
Scenario. List the content of the items for each derived unidimensional scale
Display the content for each of the four scales that correspond to the four factor
multiple indicator measurement model.
read variable labels,
Section 2.5, p. 34
Read variable labels into R with the var_labels parameter of the lessR function
Read(), setting the var_labels parameter to TRUE. Here, the variable label for each
item is its content. The cfa() function lists the items in each group, and automatically
displays the content of any variable labels.
The preceding call to cfa() displays the item content that identifies the four specified
distinct content domains of Machiavellianism verified as distinct content and separate
factors. The content for the final Mach IV measurement model, as output by the cfa()
function, appears in Listing 15.7 for Deceit, 15.8 for Flattery, 15.9 for Immorality,
and 15.10 for Cynicism.15.5. CONFIRMATORY ANALYSIS OF MACH IV ITEMS 341
Deceit: m06 m07 m09 m10
------------------------------
m06: Honesty is the best policy in all cases.
m07: There is no excuse for lying to someone else.
m09: All in all, it is better to be humble and honest than to be important
and dishonest.
m10: When you ask someone to do something for you, it is best to give the
real reasons for wanting it rather than giving reasons which carry
more weight.
Listing 15.7: Deceit Machiavellian subdomain.
Flattery: m15 m02
------------------------------
m15: It is wise to flatter important people.
m02: The best way to handle people is to tell them what they want to hear.
Listing 15.8: Flattery Machiavellian subdomain.
Cynicism: m04 m11 m16
------------------------------
m04: Most people are basically good and kind.
m11: Most people who get ahead in the world lead clean, moral lives.
m16: It is possible to be good in all respects.
Listing 15.9: Immorality Machiavellian subdomain.
Distrust: m01 m05 m12 m13
------------------------------
m01: Never tell anyone the real reason you did something unless it is
useful to do so.
m05: It is safest to assume that all people have a vicious streak and it
will come out when they are given a chance.
m12: Anyone who completely trusts anyone else is asking for trouble.
m13: The biggest difference between most criminals and other people is that
the criminals are stupid enough to get caught.
Listing 15.10: Cynicism Machiavellian subdomain.
The four subscales are conceptually distinct regarding their content, which leads to
correlations among the factors less than 1. The factor correlations are part of the
standard output of cfa() but can also be retrieved as a separate data structure. Save
the output of cfa() to a list structure of a chosen name. This structure contains
a component called ff_cor.
names() function, R:
Specify the names of
the components of a
data structure.
To view all the components of this list, invoke the R
function names(), passing the name of the list as its parameter value.
MeasModel,
Listing 15.11 reproduces the extracted factor correlation matrix from the cfa() Section 15.5.2, p. 340
output using the previously defined measurement model. The matrix can be input
into additional analyses, so many decimal digits are preserved. round() function, R:
Round output to the
specified number of
digits.
To more meaningfully
display with only two decimal digits, invoke the R function round().342 CHAPTER 15. ITEM AND FACTOR ANALYSIS
Input. Extract the factor-factor correlation matrix
out <- cfa(MeasModel)
round(out$ff_cor, 2)
Deceit Flattery Cynicism Distrust
Deceit 1.00 0.50 0.64 0.44
Flattery 0.50 1.00 0.16 0.51
Cynicism 0.64 0.16 1.00 0.61
Distrust 0.44 0.51 0.61 1.00
Listing 15.11: Factor correlations of Mach IV subscales.
The underlying factors correlate moderately but not excessively. This delineation of
unidimensional Machiavellian subscales advances our understanding of the meaning
of this Dark Triad social construct. By itself, uncovering these subscales does
not invalidate Machiavellianism as a generic concept. These results do, however,
demonstrate that Machiavellianism is best understood in terms of its constituent
facets.
15.5.3 Scale Reliability
reliability:
Consistency of
repeated measures.
An issue aside from the dimensionality of the subscales is the reliability of their
respective scale scores. Reliability is a fundamental concept of measurement, the
consistency of the measured values from measurement to measurement. For example,
suppose you weigh yourself one morning. When you step on the bathroom scale, it
reads 158 pounds. Then you step off the scale, wait a few seconds for it to reset to
zero, and then step back on the same scale. It now reads 167 pounds. Your weight has
not changed, but you received two entirely different results. The measuring device,
your bathroom scale for measuring weight, is unreliable.
What is the purpose of measuring attitudes with multi-item scales instead of a single
item? One reason is that the multi-item scale score, the composite score, is generally
more reliable than any of the individual items that define the scale. A primary reason
for the use of scale scores in psychological measurement is the relatively large amount
of error in individual responses. Each item by itself may be relatively unreliable.
These composite scores provide a level of analysis intermediate to the responses to
individual components, the items, and the factors from a factor analysis.
Measurements at two different points in time provide the data to assess the reliability
of the bathroom scale. The analogy for social measurement would be to administer an
attitude survey, wait for enough time to pass so that the responses are not memorized,
yet the underlying attitudes have not changed, and then re-administer the survey.
test-retest
reliability:
Correlation of
repeated measures. Administering a measuring instrument at two different times to assess the consistency
of responses forms the basis for test-retest reliability.
The opportunity to administer a survey multiple times may not exist. How, then,
to assess consistency over multiple measures at one time period? Assess consistency
over multiple items on the same scale to obtain the internal-consistency reliability.
internal￾consistency
reliability: Based
on the correlation of
items on the same
scale administered at
the same time.15.5. CONFIRMATORY ANALYSIS OF MACH IV ITEMS 343
The most well-known index of internal-consistency reliability is Coefficient Alpha,
introduced by Cronbach (1951) and sometimes called Cronbach’s Alpha. Coefficient
Omega was later developed and is more appropriate to the analysis of scales analyzed
with factor analysis.
communalities,
Section 15.4.1, p. 333
The factor analysis is accomplished with communalities in
the diagonal to partition out the unique variance of each item, which then permits
the calculation of Coefficient Omega. Coefficient Alpha ignores the communalities,
assuming that all items on the scale have the same factor pattern coefficient (Flora,
2020, for example).
Both coefficients, Alpha and Omega, are reported by cfa(), shown in Listing 15.12.
Scale Alpha Omega
----------------------
Deceit 0.691 0.701
Flattery 0.400 0.400
Cynicism 0.402 0.421
Distrust 0.515 0.517
Listing 15.12: Reliability of the four Mach IV subscales.
The more appropriate Omega reliabilities are somewhat larger than the corresponding
Alpha coefficients. However, all four scales have low reliabilities, too low to identify
individuals based on their scores on any of the subscales. A general heuristic is that
the reliability of a scale should be at least 0.7 or 0.8. The scale with the highest
reliability, 0.701, just scrapes by the minimum threshold. To improve the reliability of
each scale, increase the number of items on the scale that assess the same underlying
attitude.
Of course, the Mach IV scale was written as a single scale rather than four separate
scales, which accounts for the poor scale reliabilities. By increasing the number
of items on each scale, future research could improve the dependability of these
sub-scales. This further development would enhance the practical application of the
dimensionality analysis of Mach IV for the reliable measurement of each dimension.
15.5.4 Total Score Correlations
Compare the correlations of factors with the items and other factors to the observed
correlations calculated directly from the responses with no factor analysis involved.
total score: The
sum or average of all
the items on
multi-item scale.
Refer to the sum or average of the item responses on a scale for each person as the
total, composite, or scale score.
item-total
correlation:
Correlation of an
item with its own
scale score.
The correlation of an item with its own scale score is
an item-total correlation.
Item-total correlations have an upward bias. By definition, the item response is part
of the total score and so must necessarily correlate at least to some extent with the
total score, even if the item correlates zero with all the other items on the multi-item
scale. These item-total correlations are usually more useful when the item correlates
not with the entire total score but with the total score calculated without the unique
contribution of the item of interest, that is, with the underlying factor.344 CHAPTER 15. ITEM AND FACTOR ANALYSIS
There is also a downward bias in the item-total and the total-total correlations
measurement error, compared to their corresponding factor correlations.
Section 15.2.2, p. 323
The response to each item for
each individual consists, in part, of random measurement error, which detracts from
the true size of the correlation of that total score with other variables. The presence
of this error leads to an underestimate of the true magnitude of the relationship.
The corresponding item-factor and factor-factor correlations estimate the relations
without measurement error, as if the items are measured with perfect reliability. The
estimated correlations among the factors are larger than the observed correlations
contaminated with measurement error. correction for
attenuation: A
correlation with
measurement error
removed.
This tendency for correlations that involve
factors to increase is called correction for attenuation.
Usually, the focus is the factor analysis with communalities in the diagonal. The
preceding analysis achieved a solution for the communality estimates within the
default 25 iterations. However, the observed item-total and total-total correlations
can also be obtained from cfa().
iter parameter:
Maximum number of
iterations for the
communality
estimates.
To do so, do not iterate for communalities by
setting the parameter iter to 0. The result is an analysis with 1’s in the diagonal of
the correlation matrix for the items, the observed variables in general. It is as if the
total scale scores were computed by summing the relevant item scores and then the
correlations computed between them, shown in Listing 15.13.
MeasModel,
Section 15.5.2, p. 340
Input. Extract the total score correlation matrix
out <- cfa(MeasModel, iter=0)
round(out$ff_cor, 2)
Deceit Flattery Cynicism Distrust
Deceit 1.00 0.42 0.60 0.42
Flattery 0.42 1.00 0.13 0.42
Cynicism 0.60 0.13 1.00 0.56
Distrust 0.42 0.42 0.56 1.00
Listing 15.13: Observed correlations of the Mach IV subscale total scores.
Compare the observed, total score correlation matrix in Listing 15.13 with the
correlations of the underlying factors in Listing 15.11.
factor-factor
correlation matrix,
Section 15.11, p. 342
The correlations corrected for
attenuation are larger. For example, the estimated correlation of the Deceit factor
with the Flattery factor is 0.50. The observed correlation of their respective total
scores reduces to 0.42.
15.5.5 Beyond the Basics
The confirmatory factor analysis function used for the previous analyses is the
lessR function cfa().
iterated centroid
estimation: Factor
analysis based on
centroid factors
modified with
iteration for
communalities.
The cfa() estimation procedure implements iterated centroid
estimation or ICE, enhanced and translated to the R language from the original
Fortran code in John Hunter’s program PACKAGE (Hunter & Cohen, 1969). ICE is
computationally straightforward enough that factor analysis was accessible in the
pre-computer era of statistics, dating back to the early part of the 20th century. centroid factor:
Derived from the
total score of a set of
items.
A
centroid factor is based on a total score, in this case, the sum of the corresponding
items that define the factor. This relationship to the item analysis’s total score
exemplifies the conceptual similarity between factor analysis and item analysis.15.5. CONFIRMATORY ANALYSIS OF MACH IV ITEMS 345
The ICE procedure for confirmatory factor analysis is limited to multiple indicator
measurement models, defined as measurement models in which the measured variables
are partitioned into groups, with each group of items postulated as a unidimensional
set. The construction of multi-item scales, where each item on the scale measures the
same shared or common factor, defines a measurement model that applies to much
social science research. However, other models are also applicable in other situations,
such as the evaluation of multi-trait and multi-method measurement.
Confirmatory factor analysis of a MIMM is a specific implementation of the more
general concept of structural equation modeling. More general confirmatory factor
analysis models, and more general models beyond confirmatory factor analysis, require
structural equation modeling or SEM software.
lavaan path analysis,
Section 14.5, p. 317
One way to analyze more general
structural equation models and the more specific confirmatory factor analysis models
is with the R structural equation modeling package lavaan (Rosseel et al., 2022;
Rosseel, 2012). lavaan website,
http://lavaan.ugent.be
The lavaan package functions use more advanced estimation procedures than ICE,
such as full information maximum likelihood. And, statistical tests of fit are generally
available for the estimation procedures implemented in these packages. ICE, however,
does surprisingly well at estimation precision, almost as well as full information
maximum likelihood (Gerbing & Hamilton, 1994). Important for initial model
specification, ICE does not spread misspecification errors throughout the system,
instead localizing such errors within the misspecified factor.
Formal statistical tests are helpful but are only formally interpretable when a respeci￾fied model is tested on new data. Re-building a model on the same data means that
the statistical tests become heuristic aids to assessing fit, such as the residual analysis
provided by cfa(), and not formal statistical tests. Confirmatory factor analysis that
implents the ICE procedure is a practical, easy to use procedure applicable to the
construction of multiple indicator measurement models.
However, the statistical elegance of the algorithms embedded in the lavaan package
becomes the natural next step in the analysis once the general framework of the
measurement model is understood (Anderson & Gerbing, 1988). To facilitate the
transition to analysis with lavaan, the lessR function cfa() creates the lavaan
code needed for its confirmatory factor analysis, shown in Listing 15.14.
MeasModel <- "
F1 =~ X1 + X2 + X3
F2 =~ X4 + X5 + X6
"
library(lavaan)
fit <- lavaan::cfa(MeasModel, data=d, std.ov=TRUE, std.lv=TRUE)
summary(fit, fit.measures=TRUE)
Listing 15.14: lessR generated lavaan code from lessR cfa() for analysis of the same
model analyzed by lessR cfa().
Both lessR and lavaan refer to their respective functions for confirmatory factor
analysis as cfa(). To refer to functions loaded into the same R session with the same346 CHAPTER 15. ITEM AND FACTOR ANALYSIS
name from different packages, precede the function name with the package name
followed by two colons, ::. To distinguish the two cfa() functions in the same R
session, refer to lessR::cfa() and lavaan::cfa().
A confirmatory factor analysis always proceeds from a correlation (or more general
covariance) matrix. If the correlations between the variables in the model have already
been computed, they can be entered directly into the model’s analysis instead of the
original data. Accordingly, the lessR::cfa() generation of the lavaan::cfa() code
also includes this possibility, shown in Listing 15.15.
To access the correlation matrix directly without the data
replace the parameter data with the parameters
sample.cov and sample.nobs
These names refer to the name of the input correlation matrix and the sample size.
Listing 15.15: Directions from lessR::cfa() to revise the lavaan input to enter the
correlation matrix directly into lavaan::cfa().
Listing 15.16 shows the lavaan solution for the properly recovered factor pattern
coefficients, the same values recovered by the ICE procedure shown in Listing 15.4.
The lavaan output, however, also includes the corresponding standard errors and
z-test of the hypothesis that the population λ coefficients are greater than 0.
Latent Variables:
Estimate Std.Err z-value P(>|z|)
F1 =~
X1 0.800 0.047 16.962 0.000
X2 0.600 0.041 14.518 0.000
X3 0.400 0.037 10.738 0.000
F2 =~
X4 0.800 0.047 16.962 0.000
X5 0.600 0.041 14.518 0.000
X6 0.400 0.037 10.738 0.000
Listing 15.16: lavaan solution for the factor pattern coefficients.
Listing 15.17 shows the recovery of the one factor correlation in the model of ρ = 0.3.
Covariances:
Estimate Std.Err z-value P(>|z|)
F1 ~~
F2 0.300 0.043 7.044 0.000
Listing 15.17: lavaan solution for the factor correlation.
Also provided by lavaan::cfa() are statistical based fit indices, shown in List￾ing 15.18, the Comparative Fit Index and the Tucker-Lewis Index. Generally, these
indices should be above 0.90 to indicate sufficient fit (Anderson & Gerbing, 1988).
In this example, the fit indices approximately equal 1, indicating the perfect fit from
submitting the population correlations as input for the model analysis.15.6. ANALYSIS PROBLEMS 347
User Model versus Baseline Model:
Comparative Fit Index (CFI) 1.000
Tucker-Lewis Index (TLI) 1.019
Listing 15.18: Model fit indices.
Unlike the simpler ICE estimation procedure from lessR::cfa(), the corresponding
more sophisticated lavaan procedure provides the standard error for each estimated
coefficient, which then provides for the statistical test that the population version of the
estimated coefficient is different from 0. For initial model construction, lessR::cfa()
is recommended. For more refined model development and validation, tested on new
data, lavaan::cfa() is recommended. training vs. testing
data, Section 12.5.2,
p. 261
Of course, for standard errors and model
testing, in general, to be statistically valid, the testing of a final model should occur
on new data different from the training data, the data on which the model was
estimated.
Once initially developed, extend the confirmatory model with a full structural equation
model, replacing the unanalysed factor correlations with a causal structure. This
two-step procedure of first developing the measurement model, followed by the
superimposed structural model is now the generally accepted method for pursuing
this analysis. This procedure was explicated in your author’s dissertation (Gerbing,
1979) and further articulated by Anderson and Gerbing (1988).
15.6 Analysis Problems
The following Read() function call reads the attitude data analyzed in Hunter,
Gerbing and Boster (1982). The ID takes up the first four columns, then Gender
coded as 0 for Male and 1 for Female in one column. Then 74 columns of responses,
one to each of 74 attitude items. All the responses are already reverse scored where
appropriate.
d <- Read("http://lessRstats.com/data/Mach4Plus.fwd",
widths=c(4,1,rep(1,74)),
col.names=c("ID", "Gender", to("m",20), to("d",20), to("e",10),
to("i",8), to("p",8), to("c",8)))
The 74 columns of responses to the attitude scales consist of, in this order, the Christie
and Geis (1970) 20-item Mach IV scale, the Rokeach (1960) 20-item dogmatism scale,
the Rosenberg (1965) 10-item self-esteem scale, an 8-item internal locus of control
scale, an 8-item powerful others external locus of control scale, and an 8-item chance
external locus of control scale from Levenson (1976).
1. Rotter (1966) proposed the concept of Locus of Control. Those with Internal
Locus of Control perceive themselves to be in control of their destiny. Those with an
External Locus of Control perceive themselves to have their destiny determined by
forces outside of their own control. Levenson (1976) proposed to expand the concept348 CHAPTER 15. ITEM AND FACTOR ANALYSIS
of External Locus of Control to two different concepts, control by Powerful Others
and Chance. Here, we investigate the structure of Locus of Control.
a. Read the Mach4Plus data file into R (Section 2.4.3).
b. Create a subset of the data file that retains just the 24 Locus of Control items
(Section 3.6).
c. Compute the correlation matrix of the 24 Locus of Control items (Section 10.3).
d. Examine the heat map of the correlation matrix. How many groups of items
appear to be on the basis of the strength of their correlation? Why?
e. Run the confirmatory factor analysis on the three factor structure that corre￾sponds to the three Locus of Control concepts. How well does the model fit?
Answer in terms of the residuals and pattern coefficients.
f. Revise the three-factor model to improve fit. Interpret this solution, including
the correlation between the two External Locus of Control factors.
g. For the 24x24 item correlation matrix, obtain the scree plot of the eigenvalues
and the plot of the differences of successive eigenvalues. What is the smallest
number of factors that appear to reasonably well account for the correlations
among the observed variables? Why?
h. Extract the specified number of factors from the scree plot, and, secondarily,
the heat map. Interpret the meaning of the factors.
i. Run the corresponding confirmatory factor analysis for this number of factors
and interpret.
j. Compare the two competing measurement models. What is your conclusion
regarding the measurement model that underlies these Locus of Control items?
2. Consider the Rosenberg (1965) Self-Esteem scale. Does this scale measure one
dimension of self-esteem, or are multiple aspects of self-esteem assessed with the
scale?
a. Read the Mach4Plus data file into R (Section 2.4.3).
b. Create a subset of the data file that retains just the 10 self-esteem items
(Section 3.6).
c. Compute the correlation matrix of the 10 self-esteem items (Section 10.3).
d. Examine the heat map of the correlation matrix. How many groups of items
appear to be on the basis of the strength of their correlation? Why?
e. Run the confirmatory factor analysis on the one factor structure. How well does
the model fit? Answer in terms of the residuals and pattern coefficients.
f. What revisions, if any, would you make to this model?
g. Interpret the analysis and provide your conclusion regarding the number of
dimensions measured by this self-esteem scale.REFERENCES 349
References
Albers, C., & Lakens, D. (2018, January). When power analyses based on pilot data
are biased: Inaccurate effect size estimators and follow-up bias. Journal of
Experimental Social Psychologie, 74 , 187–195. doi: 10.1016/j.jesp.2017.09.004
Anderson, J. C., & Gerbing, D. W. (1988, May). Structural equation models in
practice: A review and recommended two-step approach. Psychological Bulletin,
103 , 411-423.
Belsley, D. A., Kuh, E., & Welsch, R. H. (1980). Regression diagnostics. New York:
Wiley.
Christie, R., & Geis, F. (1970). Studies in Machiavellianism. New York: Academic
Press.
Cleveland, W. S. (1979). Robust locally weighted regression and smoothing scat￾terplots. Journal of the American Statistical Association, 74 (368), 829-836.
Retrieved from https://www.tandfonline.com/doi/abs/10.1080/01621459
.1979.10481038 doi: 10.1080/01621459.1979.10481038
Cleveland, W. S. (1993). Visualizing data. Hobart Press.
Cohen, J. (1969). Statistical power analysis for the behavioral sciences. New York:
Academic Press.
Cohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.).
Hillsdale, NJ: Lawrence Erlbaum.
Cronbach, L. J. (1951). Coefficient alpha and the internal structure of tests. Psy￾chometrika, 16 (3), 297–334.
Cunningham, S. (2021). Causal inference: The mixtape. New Haven: Yale University
Press. Retrieved from https://mixtape.scunning.com
Faraway, J. J. (2004). Linear models with R. Chapman and Hall.
Flora, D. B. (2020). Your coefficient alpha is probably wrong, but which coefficient
omega is right? a tutorial on using r to obtain better reliability estimates.
Advances in Methods and Practices in Psychological Science, 3 (4), 484-501. doi:
https://doi.org/10.1177/2515245920951747
Fox, J., & Weisberg, S. (2019). An R companion to applied regression (Third
ed.). Thousand Oaks CA: Sage. Retrieved from https://socialsciences
.mcmaster.ca/jfox/Books/Companion/
Gerbing, D. W. (1979). Parameter estimation and model construction for recur￾sive causal models with unidimensional measurement (Unpublished doctoral
dissertation). Michigan State University.
Gerbing, D. W. (2020). R visualizations: Derive meaning from data. CRC Press.
Gerbing, D. W. (2021). Enhancement of the command-line environment for
use in the introductory statistics course and beyond. Journal of Statis￾tics and Data Science Education, 29 (3), 251-266. Retrieved from https://
www.tandfonline.com/doi/abs/10.1080/26939169.2021.1999871 doi: 10
.1080/26939169.2021.1999871
Gerbing, D. W. (2022). lessR: Less code, more results [Computer software manual].
Retrieved from https://cran.r-project.org/package=lessR (R package
version 4.2.4)350 REFERENCES
Gerbing, D. W., & Anderson, J. C. (1988, May). An updated paradigm for scale
development incorporating unidimensionality and its assessment. Journal of
Marketing Research, 25 (2), 186-192.
Gerbing, D. W., & Hamilton, J. G. (1994). The surprising viability of a simple
alternate estimation procedure for construction of large-scale structural equa￾tion measurement models. Structural Equation Modeling: A Multidisciplinary
Journal, 1 (2), 103-115.
Gerbing, D. W., & Hamilton, J. G. (1996). Viability of exploratory factor analysis as
a precursor to confirmatory factor analysis. Structural Equation Modeling: A
Multidisciplinary Journal, 3 , 62-72.
Hunter, J. E. (1973). Methods of reordering the correlation matrix to facilitate
visual inspection and preliminary cluster analysis. Journal of Educational
Measurement, 10 , 51-61.
Hunter, J. E., & Cohen, S. H. (1969, 697-700). Package: A system of computer
routines for the analysis of correlational data. Educational and Psychological
Measurement, 29 .
Hunter, J. E., Gerbing, D. W., & Boster, F. J. (1982). Machiavellian beliefs and
personality: Construct invalidity of the Machiavellian dimension. Journal of
Personality and Social Psychology, 43 (6), 1293-1305.
Jessor, R., & Jessor, S. L. (1977). Problem behavior and psychosocial development: A
longitudinal study of youth. New York, NY: Academic Press.
Kelley, K. (2007). Methods for the behavioral, educational, and social sciences: An
R package. Behavior Research Methods, 39 , 979-984.
Kelley, K. (2022). MBESS [Computer software manual]. Retrieved from https://
CRAN.R-project.org/package=MBESS (R package version 4.9.0)
Kirk, R. E. (2013). Experimental design (4th ed.). Sage.
Kozyrkov, C. (2019). The most powerful idea in data science. Retrieved 2022-05-
28, from https://towardsdatascience.com/the-most-powerful-idea-in
-data-science-78b9cd451e72
Langsrud, Ø. (2003). ANOVA for unbalanced data: Use Type II instead of Type III
sums of squares. Statistics and Computing, 13 , 163-167.
Lawlor, D. A., Smith, G. D., & Ebrahim, S. (2004). Commentary: The hormone
replacement-coronary heart disease conundrum: Is this the death of observa￾tional epidemiology? International Journal of Epidemiology, 33 (3), 464-467.
Levenson, H. (1976). Multidimensional locus of control in sociopolitical activists of
conservative and liberal ideologies. Journal of Personality and Social Psychology,
33 , 199-208.
Lumley, T., & Miller, A. (2020). leaps: Regression subset selection [Computer software
manual]. Retrieved from https://CRAN.R-project.org/package=leaps (R
package version 3.1)
Machiavelli, N. (1902/1513). The prince (W. K. M. (Translator), Ed.). Alfred A.
Knopf, Inc.
Mersmann, O. (2021). microbenchmark: Accurate timing functions [Computer
software manual]. Retrieved from https://CRAN.R-project.org/package=
microbenchmark (R package version 1.4.9)
Murdoch, D., & Chow, E. D. (2022). ellipse: Functions for drawing ellipses and
ellipse-like confidence regions [Computer software manual]. Retrieved from
https://CRAN.R-project.org/package=ellipse (R package version 0.4.3)REFERENCES 351
Nie, Y., Lau, S., & Liau, A. K. (2011). Role of academic self-efficacy in moderating
the relation between task importance and test anxiety. Learning and Individual
Differences, 21 (6), 736 741.
Paulhus, D. L., & Williams, K. M. (2002). The dark triad of personality: Narcissism,
machiavellianism, and psychopathy. Journal of Research in Personality, 36 (6),
556-563. doi: https://doi.org/10.1016/S0092-6566(02)00505-6
Pearl, J., & MacKenzie, D. (2018). The book of why: The new science of cause and
effect. NY: Basic Books.
Preacher, K. J., & Hayes, A. F. (2008). Asymptotic and resampling strategies for
assessing and comparing indirect effects in multiple mediator models. Behavior
Research Methods, 40 (3), 879-891. doi: https://doi.org/10.3758/BRM.40.3.879
Preacher, K. J., & Kelley, K. (2011). Effect size measures for mediation models: Quan￾titative strategies for communicating indirect effects. Psychological Methods,
16 (2), 93 115. doi: https://doi.org/10.1037/a0022658
Rokeach, M. (1960). The open and closed mind. New York: Basic Books.
Rosenberg, M. (1965). Society and the adolescent self-image. Princeton, NJ: Princeton
University Press.
Rosseel, Y. (2012). lavaan: An R package for structural equation modeling. Journal
of Statistical Software, 48 (2), 1–36.
Rosseel, Y., Jorgensen, T. D., & Rockwood, N. (2022). lavaan: Latent variable analysis
[Computer software manual]. Retrieved from https://cran.r-project.org/
web/packages/lavaan (R package version 0.6-12)
Rotter, J. (1966). Generalized expectancies for internal versus external control of
reinforcement. Psychological Monographs, 80 (1), Whole No. 609.
Schauberger, P., & Walker, A. (2021). openxlsx: Read, write and edit xlsx files
[Computer software manual]. Retrieved from https://CRAN.R-project.org/
package=openxlsx (R package version 4.2.5)
Schutten, G.-J., Chan, C.-H., Leeper, T. J., & Foster, J. (2020). readods: Read
and write ODS files [Computer software manual]. Retrieved from https://
CRAN.R-project.org/package=readODS (R package version 1.7.0)
Shrout, P. E., & Bolger, N. (2002). Mediation in experimental and nonexperimental
studies: New procedures and recommendations. Psychological Methods, 7 (4),
422-445.
Spearman, C. (1904). “General intelligence,” objectively determined and measured.
The American Journal of Psychology, 15 , 201-292.
Spearman, C. (1914). Theory of two factors. Psychological Review, 21 , 105-115.
Thurstone, L. L. (1931). Multiple factor analysis. Psychological Review, 38 , 406-427.
Tjur, T. (2009). Coefficients of determination in logistic regression models—a new
proposal: The coefficient of discrimination. The American Statistician, 63 (4),
366-372.
Tukey, J. W. (1977). Exploratory data analysis. Addison-Wesley.
Wickham, H. (2016). ggplot2: Elegant graphics for data analysis. Springer-Verlag
New York.
Wickham, H. (2022). Create elegant data visualisations using the grammar of graphics
[Computer software manual]. (R package version 3.3.6)
Wickham, H., Miller, E., & Smith, D. (2022). haven: Import and export ’spss’,
’stata’ and ’sas’ files [Computer software manual]. Retrieved from https://
CRAN.R-project.org/package=haven (R package version 2.5.0)Index
accuracy, 293
aggregation, 78, 109
alpha criterion, α, 71
alpha level, 79
analysis
descriptive, 100
inference, 115
ANCOVA, 269
ANOVA, 152
assignment operator, <-, 26
autocorrelation, 242
back transformation, 246
balance point, 101
balanced, 164
best possible subsets, 265
between-subjects, 128
bias-variance trade-off, 262
bin, 84
bin: midpoint, 84
binomial event, 122
blocks, 141
bootstrapping, 316
box plot
whisker, 91
Brown-Forsythe test, 133
browse, 25
bubble plot, 204
case, 14
case-deletion statistics, 238
causal relation, 138
cell, 173
central limit theorem, 114
centroid factor, 344
class, 84
cloud, 4
cluster analysis, 210
hierarchical
agglomerative, 211
Cohen’s d, 120
collinear, 264
command prompt, 6
communality, 333
concordant, 215
confidence interval, 118, 134, 226
fitted value, 233
confirmatory factor analysis, 330
confusion matrix, 292
console, 6, 8
contrast matrix, 282
control, 253
experimental, 138, 253
statistical, 253
Cook’s Distance, 238
correction for attenuation, 344
correlation
item-total, 343
Kendall, 215
Spearman, 213
correlation coefficient, 197
correlation matrix, 206
covariance structure, 331
covariate, 269
CRAN, 5
cross-product, 196
cross-tabulation, 74
cross-validate, 268
cross-validation, 262
current working directory, 38
cutpoint, 84
data, 1
analysis, 1
cross-sectional, 92
file, 13
interval, 23
long format, 162
longitudinal, 92
nominal, 23
ordinal, 24
ratio, 23
testing, 261
data format
csv, 22, 25
fwd, 31
352INDEX 353
data frame, 3, 25
data point, 221
data science, 1
data splitting, 262
data storage type, 24
data table
wide format, 14
data validation, 19
data value, 14
decimal separator, 33
default, 11
degrees of freedom, 103
dendrogram, 211
density
bandwidth, 88
plot, 87
dependent-samples, 142
design, 152
randomized block, 161
randomized block factorial, 180
split-plot factorial, 184
two-way factorial, 172
deviation score, 101
DFFITS, 239
discordant, 215
distribution t, 117
dummy variable, 269
effect
direct, 304, 305
indirect, 253, 302, 311
net, 253
total, 253, 304
effect size, 118, 131
effects coding, 283
empirical, 1
error
modeling, 232
prediction, 232
events
independent, 122
exact binomial test, 122
experiment, 137, 154
factor, 44
factor extraction, 325
factor pattern, 323
factorial design, 172
factors, 322
false negative, 292
false positive, 292
feature, 218
feature engineering, 264
Fisher’s skewness, 105
fitted value, 167, 222, 223
fold, 263
frequency
cumulative, 89
distribution, 85
grand total, 75
joint, 74
marginal, 75
function, 3, 220
Anova() car, 190
BarChart() lessR, 64
BoxPlot() lessR, 91
c() R, 12, 29, 32, 50, 52
cfa() lavaan, 345
cfa() lessR, 334
corRead(), lessR, 333
Correlation() lessR, 197
corReorder() lessR, 210, 339
CountAll() lessR, 62, 83
efa() lessR, 328
exp() R, 289
factanal() R, 328
factors() lessR, 44
getwd() R, 38
head() R, 15
install.packages() R, 8
library() R, 9
ls() R, 198
mediation() MBESS, 312
mvrnorm() MASS, 312
na.omit() R, 267
names() R, 15, 341
options() R, 33
PieChart() lessR, 65
Plot() lessR, 91, 93, 194
prob_tcut() lessR, 226
prop() lessR, 338
Prop_test() lessR, 121, 122, 125, 147
Read() lessR, 25
recode() lessR, 49, 129
rep() R, 32
reshape_long() lessR, 163354 INDEX
rnorm() R, 197
round() R, 341
row.name() R, 55
rows() lessR, 55
scale() R, 310
scree() lessR, 327
sem() lavaan, 318
sort_by() lessR, 51
style() lessR, 64
tail() R, 15
to() lessR, 32
transform() R, 129
ttest() lessR, 115, 120, 130, 143
update.packages() R, 9
function form
exponential, 244
linear, 220
logarithmic, 244
power, 244
function type
data analysis, 11
data modification, 12
utility, 11
function wilcox.test()i R, 136
global environment, 198
group
control, 154
experimental, 154
heat map, 209, 335
heteroscedasticity, 242
hierarchical cluster analysis, 211
histogram
cumulative, 89
undersmoothed, 86
hypothesis test
one-tailed, 120, 121
two-tailed, 120
independent events, 78
independent-samples, 128
influence, 238
interaction, 172
interquartile range, 90
IQR, 107
item analysis, 322
item reflection, 326
item-factor correlation, 336
iterated centroid estimation, 344
iteration, 290, 334
k-fold cross-validation, 263
kurtosis, 105
excess, 105
label, 23, 285
least-squares estimation, 228
level, 43
levels, 23
Likert scale, 17
linearize, 245
listwise deletion, 208
loess curve, 194
logit, 287
Machiavellianism, 16
machine learning, 218
Mahalanobis distance, 237
main effect, 172
manipulation, 137
margin of error, 226
matrix, 210
maximum likelihood estimation, 287
mean, 100
cell, 174
grand, 155
marginal, 175
population, 112
mean square, 102
measurement model, 323
median, 106
mediation
complete, 312
full, 312
partial, 311
mediator variable, 311
merge, 57
full, 59
inner join, 58
left-outer, 59
right-outer, 59
MIMM, 331
missing data code, NA, 28
misspecification, 336
model, 218, 223
model selection, 264
monotonic, 213INDEX 355
multiple regression, 251
nested model, 267
nonparametric, 135
normal distribution, 103
null hypothesis, 70, 79, 157
null model, 230, 293
oblique, 325
observation, 14
odds, 287
odds ratio, 289
omnibus F-test, 158
one-tailed test, 201
order statistic, 106
orthogonal, 325
outlier, 90, 236
actual, 91
bivariate, 236
potential, 91
overfitting, 261
p-value, 70, 79, 117
package, 8
contributed, 8
ellipse, 199
pairwise deletion, 207
parameter, 10
bin_start, 86
lessR, 203
all R, 60
all.x R, 59
all.y R, 59
alternative R, 120, 124, 130, 201
B MASS, 316
besides lessR, 73
bin_start lessR, 111
bin_width lessR, 86, 111
bootstrap MASS, 316
bottom lessR, 210
by lessR, 73, 77, 109, 147, 202, 203,
271
by1 lessR, 96, 203
center R, 310
center_line lessR, 94
ci lavaan, 319
col.names R, 32
colClasses R, 36
color lessR, 66
compute lessR, 77
conf.level MASS, 316
conf_level lessR, 117, 134
data lavaan, 318
data lessR, 26, 77, 190
dec R, 33
digits_d lessR, 76, 210, 269
direction lessR, 52
dv MASS, 313
ellipse lessR, 199
empirical MASS, 312
ExcelColWidth openxlsx, 38
ExcelTable openxlsx, 38
fill lessR, 66
fit lessR, 194, 230, 243, 245, 271
fit.measures lavaan, 319
format lessR, 25, 30, 37
format R, 97
from lessR, 37
group lessR, 163
hole lessR, 65
ID lessR, 163
item_cor lessR, 337
iter lessR, 344
kfold lessR, 263
labels R, 44
levels R, 44
main lessR, 210
main R, 64
mediator MASS, 313
method R, 213, 297
min_loading lessR, 328
miss R, 208
missing lessR, 21, 28, 51
mod lessR, 307
mod_transf lessR, 307
mu lessR, 115
n R, 283
n_col lessR, 203
n_factors lessR, 328
n_row lessR, 203
n_succ lessR, 125, 148
n_tot lessR, 72, 125, 148
new lessR, 45
new_scale lessR, 224
new_vars lessR, 49
ordered R, 44, 45
OutDec R, 33356 INDEX
p_train lessR, 56
paired R, 127, 143, 145
pdf lessR, 210
pi lessR, 122
plot_errors lessR, 244
pred_rows lessR, 234
prob_cut lessR, 294
q_num lessR, 108
quiet lessR, 27
random lessR, 53
res_rows lessR, 160, 167, 239
res_sort lessR, 160, 239
response lessR, 56, 163
right lessR, 210
row.names R, 53
row_names lessR, 21, 29
rows lessR, 53, 95
scale R, 310
scale_response lessR, 224
se lavaan, 318
sepR, 33
shape lessR, 94, 202
show_n lessR, 190
skip R, 33
sort lessR, 68, 328
stack100 lessR, 76
stat lessR, 68
sub_theme lessR, 66, 194
success lessR, 121
theme lessR, 64, 194
to lessR, 37
trans lessR, 204
transform lessR, 163
Type car, 190
type lessR, 94
var_labels lessR, 35, 340
variable lessR, 72, 77, 121
vbs_plot lessR, 82
widths R, 32
x MASS, 313
X1_new lessR, 235, 260, 299
X2_new lessR, 260, 299
xlab R, 64
y lessR, 67
ylab R, 64
parameters
linear model, 220
parametric statistic, 100
parsimony, 266, 322
partial slope coefficient, 252
path coefficients, 318
path diagram, 304, 310, 331
path model, 317
pie chart, 65
pivot table, 78, 109
post-hoc, 158
power, 120, 142
precision, 294
prediction interval, 233
predictive residual, 240
predictor variable, 218
primary key, 57
probability
conditional, 72, 77
probability interval, 226
product rule
external consistency, 332
internal consistency, 332
prompt
command, 8
continuation, 8
quantile, 107
quantitative variable, 22
random assignment, 137
random effect, 182
range, 102
recode, 49
reference group, 280, 290
regression
explain, 219
time series, 242
regression line, 221
relation, 194
relationship
negative, 194
positive, 194
reliability, 342
internal consistency, 342
test-retest, 342
repeated measures design, 161
reproducibility, 6
research design, 110
residual, 157, 159, 227, 337
responseINDEX 357
invalid, 324
random, 324
response variable, 154
ring chart, 65
rotation
promax, 328
varimax, 328
run chart, 93
sampling distribution, 113
sampling error, 79, 112, 153
scatterplot, 91, 194
scatterplot matrix, 208
score
composite, 343
scale, 343
total, 343
scree plot, 326
sensitivity, 293
sigmoid curve, 291
significant difference, 117
simple structure, 325
skewness, 104
slope
coefficient, 220
meaning, 223
standard deviation, 101, 102
within-group, 130
standard error
mean, 113
mean difference, 134
regression coefficient, 224
standardization, 47
standardized mean difference, 131
statistical decision, 117
statistics
inferential, 113
structural equation modeling, 319
Studentized residual, 238
sum of squares, 102
summary data table, 67
summary statistic, 100
systematic error, 324
t-cutoff, 117
target, 219
test statistic
F-value, 157
t-value, 116, 134
chi-square test, 70, 78
text editor, 7
text file, 7, 31
tolerance, 265
treatment variable, 152, 154
Trellis plot, 203
true negative, 292
true positive, 292
unbalanced design, 188
underfitting, 261
underidentified, 325
unidimensional, 323
uniqueness, 324
unit of the analysis, 14
user’s workspace, 198
variable, 14
categorical, 23
confounding, 138, 253, 304
continuous, 22
dependent, 152
dummy, 278
grouping, 23, 129, 153
independent, 152
indicator, 278
latent, 322
lurking, 304
qualitative variable, 23
response, 129, 138, 152, 219
treatment, 138
variable label, 34
variable name, 27
variance, 102
variance ratio test, 133
VBS, 91
vector, 12
VIF, 265
violin plot, 91
wide-form data, 162
Wilcoxon rank sum test, 135
Y-intercept, 220
z-scores, 47
