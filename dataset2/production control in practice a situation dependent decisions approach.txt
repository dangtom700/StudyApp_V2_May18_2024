Production Control in PracticeProduction Control in Practice
A Situation-Dependent Decisions Approach
Henny Van Ooijen
Corné DirneAuthors
Prof. Dr. Henny Van Ooijen
Eindhoven University of Technology
Tielseweg 5
4116 EB Buren
The Netherlands
Prof. Dr. Corné Dirne
Fontys University of Applied Sciences
Rondom 1
5612 AP Eindhoven
The Netherlands
Cover Image: © akinbostanci/Getty
Images
All books published by WILEY-VCH are carefully
produced. Nevertheless, authors, editors, and
publisher do not warrant the information
contained in these books, including this book,
to be free of errors. Readers are advised to keep
in mind that statements, data, illustrations,
procedural details or other items may
inadvertently be inaccurate.
Library of Congress Card No.:
applied for
British Library Cataloguing-in-Publication Data
A catalogue record for this book is available
from the British Library.
Bibliographic information published by
the Deutsche Nationalbibliothek
The Deutsche Nationalbibliothek lists
this publication in the Deutsche
Nationalbibliografie; detailed bibliographic
data are available on the Internet at
<http://dnb.d-nb.de>.
© 2024 WILEY-VCH GmbH, Boschstraße 12,
69469 Weinheim, Germany
All rights reserved (including those of
translation into other languages). No part of
this book may be reproduced in any form – by
photoprinting, microfilm, or any other
means – nor transmitted or translated into a
machine language without written permission
from the publishers. Registered names,
trademarks, etc. used in this book, even when
not specifically marked as such, are not to be
considered unprotected by law.
Print ISBN: 978-3-527-35344-6
ePDF ISBN: 978-3-527-84590-3
ePub ISBN: 978-3-527-84589-7
oBook ISBN: 978-3-527-84588-0
Typesetting Straive, Chennai, Indiav
Contents
Preface xi
Part I Production Control in General 1
1 Production Control – A Logistic Control Function 3
1.1 Logistics 3
1.2 Logistics Planning and Control 6
1.3 Logistic Concepts in Production 7
1.4 Terminology for Production Control 10
1.4.1 Concepts Used in Production Control 10
1.4.2 Complexity, Uncertainty, and Flexibility 12
References 13
2 Horizontal and Vertical Decomposition 15
2.1 Horizontal Decomposition 16
2.2 Vertical Decomposition 22
2.3 Types of Release Triggers 25
2.3.1 Just-in-Time Versus Just-in-Case 25
2.3.2 Push Versus Pull in Logistics 28
2.4 An Example of Decomposition 30
References 32
3 Planning and Control in Production Units 33
3.1 Production Control in General 33
3.2 Basic Forms of Production 35
3.2.1 Process-Wise Production 35
3.2.2 Mass Assembly/Flow Production 36
3.2.3 (Repetitive) Small Series Production (Also Called Job-Shop) 36
3.2.4 (Repetitive) Project-Wise Production 37
3.2.5 Throughput Time Production Units 37
References 39vi Contents
4 Framework for Logistic Planning and Control in Production
Systems 41
4.1 General Framework 41
4.2 Position of this Book 45
References 46
Part II Planning and Control of Decoupling Points 47
5 Decoupling Point Control 49
5.1 Decoupling Point Control – An Introduction 49
5.2 Performance Measures for Decoupling Point Control 53
5.3 Demand and Forecasting 58
5.3.1 Demand Pattern 59
5.3.2 Forecasting Methods 60
5.3.2.1 Time Series-Related Forecasting for Stationary Demand 63
5.3.2.2 Time Series-Related Forecasting for Demand with a Trend 67
5.4 Order Size 71
5.4.1 Optimal Batch Size in Case of Fixed Order Size 72
5.4.2 Relaxation of Assumptions 75
5.4.2.1 Known or Predicted Demand Variation 75
5.4.2.2 Quantity Discount 78
5.4.2.3 Minimum Order Quantity 79
5.4.2.4 No Variable Order-Related Costs 79
5.4.2.5 Interdependencies of Order Sizes – Not BOM Related 80
5.4.2.6 Interdependencies of Order Sizes – BOM Related 81
5.4.3 Single Period Problem 83
Appendix 5.A The Wagner-Whitin Algorithm 84
Appendix 5.B Example Impact Advanced and Optimal Approach for
Determining Batch Sizes 87
Appendix 5.C Newsvendor Problem 87
References 90
6 Reorder Point Decoupling Point Control Systems 93
6.1 General Discussion of Reorder Point Systems 93
6.2 When to Order? 96
6.2.1 Continuous Review 97
6.2.2 Periodic Review 99
6.2.3 The Reorder Level – Continuous Review 100
6.2.4 The Reorder Level – Periodic Review 107
6.3 How Much to Order? 109
6.3.1 Fixed Amount 109
6.3.2 Maximum Level 109Contents vii
6.3.2.1 (s, S) 109
6.3.2.2 (R, s, S) 110
Appendix 6.A Table of the One-Sided Standard Normal Distribution 110
Appendix 6.B Table Standard Normal Loss Function 112
Appendix 6.C Reorder Level Determination in Case of a General
Distribution 113
6.C.1 Discrete Demand 113
6.C.2 Continuous Demand 115
6.C.3 Determining the Reorder Level 116
References 116
7 MRP Decoupling Point Control Systems 117
7.1 General Discussion of MRP Systems 117
7.1.1 Material Requirements Planning (MRP-I) 117
7.1.2 Manufacturing Resources Planning (MRP-II) 119
7.1.2.1 Engine 119
7.1.2.2 Front End 120
7.1.2.3 Back End 121
7.2 When to Order 122
7.3 How Much to Order? 125
7.4 Discussion on MRP-Related Issues 128
7.4.1 Dealing with Uncertainty 128
7.4.2 Bill-of-Materials Versus Bill-of-Distribution 130
Appendix 7.A MRP Formulas 132
7.A.1 Rescheduling Assumption 132
References 133
8 Systems Using Echelon Stock (ESC, LRP) 135
8.1 General Discussion of Systems Using Global Norms 135
8.1.1 Discussion on ROP and MRP 136
8.1.2 Echelon Stock Control Systems 137
8.1.3 Line Requirements Planning 138
8.2 When and How Much to Order? 139
8.2.1 When and How Much to Order in Echelon Stock Systems? 139
8.2.2 When and How Much to Order in Line Requirements Planning
Systems? 139
8.3 Discussion on Echelon Stock Systems 142
References 143
9 Choosing an Appropriate DPC System 145
9.1 General Considerations 145
9.2 Advantages/Disadvantages of the Different DPC Systems 146
9.2.1 Bullwhip Effect 147
9.3 Which Decoupling Point Control System to Use? 150
References 157viii Contents
Part III Production Unit Control 159
10 General Discussion of Production Control Decisions 163
10.1 Priority Control 164
10.2 Capacity Allocation 165
10.3 Work Order Release/Work Order Detail Planning (Scheduling) 166
References 168
11 Production Control for Deterministic, Static Production
Situations (Scheduling) 169
11.1 Sequencing Orders Without Delivery Date (Throughput Time
Oriented) 170
11.1.1 Work Orders with One Operation and Work Centers with One
Machine 171
11.1.1.1 Relation Between Work-in-Process and Throughput Time 171
11.1.1.2 Minimization of the Average Throughput Time 171
11.1.1.3 Minimization of Weighted Average Throughput Time 171
11.1.2 Work Orders with One Operation and Work Centers with Parallel,
Identical Machines 172
11.1.2.1 Minimizing the Makespan 172
11.1.2.2 Minimizing the Average Throughput Time 172
11.1.3 Work Orders with Multiple Operations and Work Centers with One
Machine 173
11.1.3.1 Minimizing the Makespan for a Flow Shop with Two Operations 174
11.1.3.2 Minimizing the Makespan for a Flow Shop with More Than Two
Operations 176
11.2 Sequencing Orders with a Delivery Date (Reliability Oriented) 178
11.2.1 Minimizing the Average Lateness 179
11.2.2 Minimizing the Maximum Tardiness 179
11.2.3 Minimizing the Number of Tardy Orders (NT) 179
11.2.4 Minimizing the Average Tardiness 181
11.3 Relaxing Assumptions 183
11.3.1 Orders with Sequence-Dependent Set-Up Times 183
11.3.2 Sequencing Orders with Different Routings 184
References 185
12 Flow Process Production 187
12.1 General Description 187
12.2 Main Control Attention Points of Flow Process Production 189
12.2.1 General 189
12.2.2 Cycle Time Determination 190
12.2.2.1 A Stable Level of Demand 191
12.2.2.2 Variable Demand 194
12.2.2.3 Different Cycles on One Production Line 196Contents ix
12.3 Production Control Decisions for Flow Process Production in MTS
Situations 196
12.3.1 Sequencing 196
12.3.2 Capacity Allocation 197
12.3.3 Work Order Release 197
12.4 Production Control Decisions for Flow Process Production in MTO
Situations 197
12.4.1 Sequencing and Work Order Release 198
12.4.2 Capacity Allocation 200
12.5 Application 200
References 204
13 Mass Assembly Production 205
13.1 General Description 205
13.2 Main Control Attention Points of Mass Assembly Production 207
13.2.1 Pure Flow Production 208
13.2.2 Variants of Pure Flow Production 209
13.2.2.1 Different Processing Times 209
13.2.2.2 Variable Processing Times 211
13.2.2.3 Different Products (Needing Different Materials and/or Resources) 211
13.2.2.4 Disturbances at the Work Centers 212
13.2.2.5 No Availability of Efficient Technology 212
13.2.2.6 A Variety of Routings (Some Operations Are Skipped) 213
13.2.3 Quantitative Models for Analyzing the Effect of Buffers 214
13.2.3.1 Two Stations Without Failures 215
13.2.3.2 More Than Two Stations Without Failures 216
13.2.3.3 Two Stations with (Time-Dependent) Failures 216
13.2.3.4 More Than Two Stations with (Time-Dependent) Failures 218
13.2.4 Cross Training 219
13.3 Production Control Decisions for Mass Assembly Production 220
13.3.1 Sequencing 220
13.3.2 Capacity Allocation 220
13.3.3 Work Order Release 221
13.4 Application 222
References 224
14 Small Series Production 227
14.1 General Description 227
14.2 Main Control Attention Points of Small Series Production 229
14.2.1 Fundamental Results from Queueing Theory 230
14.2.2 Throughput Time-Related Aspects 236
14.2.2.1 Production Layout 236
14.2.2.2 Measures Based Upon Insights from Queuing Theory 236
14.2.2.3 Customer Differentiation 238
14.2.3 Lead Time Reliability Related Aspects 239x Contents
14.2.3.1 Due Date Determination Rules 240
14.2.3.2 The Effect of the Value of the Slack on the Delivery Reliability 246
14.2.3.3 Internal Versus External Due Date 248
14.3 Production Control Decisions for Small Series Production 248
14.3.1 Throughput Time 249
14.3.1.1 Sequencing 249
14.3.1.2 Capacity Allocation 252
14.3.1.3 Work Order Release/Work Order Detail Planning 256
14.3.2 Lead Time Reliability 259
14.3.2.1 Sequencing 259
14.3.2.2 Capacity Allocation 262
14.3.2.3 Work Order Release 263
14.4 Application 263
Appendix 14.A Short-Term Capacity Adjustment 265
Appendix 14.B Flexible Batching 267
Appendix 14.C The Effect of Workload Control in Case There Is a Relationship
Between Productivity and Workload 268
References 271
15 (Repetitive) Project-Based Production 273
15.1 General Description 273
15.2 Main Control Attention Points of Project-Based Production 275
15.2.1 Construction of a Network 276
15.2.1.1 Terminology 276
15.2.1.2 Duration of the Activities 279
15.2.1.3 Critical Path and Project Duration in Case Activity Times Are
Deterministic 279
15.2.1.4 Slack 281
15.2.1.5 Uncertainty in Project Duration Due to Stochastic Activity Times 282
15.2.1.6 Realistic Estimates of the Activity Times 284
15.2.1.7 Activity on Node Networks 284
15.3 Production Control Decisions for Project-Based Production 286
15.3.1 Sequencing 286
15.3.2 Capacity Allocation (and Scheduling) 286
15.3.2.1 Resource Loading 286
15.3.2.2 Resource Leveling 286
15.3.2.3 (Constrained Resource) Scheduling 287
15.3.3 Work Order Release/Project Scheduling 289
15.3.3.1 Work Order Scheduling 289
15.3.3.2 Work Order Release 290
15.4 Application 291
References 295
Index 297xi
Preface
In the 1980s–1990s of the past century Bertrand, Wortmann, and Wijngaard pub￾lished the book “Productiebeheersing en Materials Management” (in Dutch), which
was unique in its approach to the discussion of production control (see further on).
This book is no longer edited, but from a lot of sources, we heard that the material,
and the way it is discussed, are still very interesting and relevant. Therefore, we
decided to “upgrade” this book. It is extended, updated, and written in English.
Moreover, we tried to make it as practical as possible. Therefore, we will not discuss
the latest, most sophisticated research concerning the different decision functions
since these are often difficult to understand, or to implement, for practitioners.
Moreover, here holds: the first blow is half the battle, that is, with only simple
methods already a lot (cost) benefits are obtained. The target audience is students
at a bachelor level and practitioners with some experience working in the field of
(production) logistics.
In our daily life, we can observe lots of transformation processes. We can group
them into:
(a) Transformation of form or content
(b) Transformation of time
(c) Transformation of place
In this book, we will discuss the transformation of the form (production) and the
transformation of time (inventory) with an emphasis on operational control. We will
concentrate on transformation processes that take place in production organizations
or professional service organizations (insurance companies, banks, etc.). In these
organizations, we deal with the production of tangible or intangible products and
the stocking of raw materials, intermediate components, and products produced.
We will mainly use terminology from industry, but the concepts can easily (with
some creativity) be translated into professional service organizations.
We decompose the complex production control problem in supply chains into
several less complex subcontrol problems. This leads to a decoupling point control
(material coordination) problem and one or more production unit control (capacity
coordination) problems.
We distinguish several characteristic control situations, and for each of them,
we discuss the control aspects where we will use the same format for discussion forxii Preface
all the goods flow and production situations: first a general discussion of the control
situation, next a general discussion of the relevant decision functions and then a
detailed discussion of each decision function for the control situation we consider.
This is one of the unique points of this book. The fact that we distinguish several
characteristic situations and discuss for each of them the relevant decisions led to
the inclusion of “situation-dependent” in the subtitle.
The books we know on production control discuss this from a technical point of
view (LP, queuing theory, dynamic programming, etc.) or a functional point of view
(aggregate planning, materials requirement planning, etc.). The kinds of production
systems considered are limited, often only transfer lines (or flow lines) and assembly
systems are discussed. This book is different from other books on Production Plan￾ning or Production Control in the sense that especially the (basic) control decisions
are discussed for the different characteristic Decoupling Point Control and Produc￾tion Unit Control situations that we distinguish. These control decisions are not
discussed, for instance, from a mathematical point of view, but are based on the main
decisions that have to be taken in the situation considered. This is another unique
point of this book. That is why we added a “decisions approach” in the subtitle.
The book consists of three parts: production control in general, decoupling point
control for the flow of goods between units, and production unit control.
In the first part, we will discuss general aspects of production planning. We will
discuss the terminology used in this book and the way we look at production con￾trol. For the latter, we will discuss our typology of production situations found in
practice leading to several characteristic production situations, and we discuss how
we decompose the complex production control problem in supply chains.
In the second part, we will discuss the different coordination and material supply
control mechanisms that are relevant to supply chains. We first start by discussing
some general aspects and then give a typology of the different control mechanisms.
Moreover, we discuss the decisions that, in general, have to be taken. Next, each
control mechanism is discussed in more detail in a structured way.
In the third and last part, we discuss each of the production situations from our
typology and the way production should be controlled. We do this in a structured
way taking in Part I distinguished control decisions as a starting point. At the end of
each section, we illustrate the way of production control using a (stylized) practical
example.
Much of the material on which the book is based is relatively old, but since most of
the decision functions are already extensively discussed and/or exactly solved, new
research on these functions does not necessarily bring new insights. A few decision
functions (like for instance the work order release function) don’t have unique “solu￾tions” and are therefore still the subject of research. Where relevant, we extended the
discussion of control decisions with newly developed material. Again, the main con￾tribution of this book is the structured way in which the different production control
decisions in the different characteristic situations are discussed.Preface xiii
The book is meant for a broad range of readers: bachelor students from universities
of applied sciences, academic bachelor students, and practitioners. Parts of the book
discuss in more detail the determination of results or technicalities of methods; these
have a blue background and are especially meant for academic bachelor students.
We are grateful to Will Bertrand, Hans Wortmann, and Jacob Wijngaard for paving
the road for us with their book, for the lessons we learned from them during the time
we were colleagues, and for their permission to use (part of) their material.
Henny Van Ooijen
Buren, The Netherlands
Corné Dirne
Eindhoven, The Netherlands1
Part I
Production Control in General
In this part, we will discuss the subject of this book and we will give some basic
concepts and terminology. Production Control can be discussed from a different
number of viewpoints: quality, economics, technology, logistics, etc. In this book,
we will take the viewpoint of logistics. We start in Chapter 1 with a discussion of the
logistic aspects of planning and control, both in the general sense and in the context
of production. In Chapter 2, the complicated control problem will be decomposed
in two ways: a vertical decomposition and a horizontal decomposition, leading to
several (hierarchical) control problems that are much easier to handle. Moreover, in
this section, we will also discuss the different release triggers. Next, in Chapter 3,
we discuss planning and control aspects in Production Units, and subsequently, in
Chapter 4, a general framework for (logistic) planning and control in productions
and the position this book (using this framework) will be highlighted.
Content
1. Production Control – A Logistic Control Function
1.1 Logistics
1.2 Logistics Planning and Control
1.3 Logistic Concepts in Production
1.4 Terminology for Production Control
1.4.1 Concepts Used in Production Control
1.4.2 Complexity, Uncertainty, and flexibility
References
2. Horizontal and Vertical decomposition
2.1 Horizontal Decomposition
2.2 Vertical Decomposition
2.3 Types of Release Triggers
2.3.1 Just-in-Time Versus Just-in-Case
2.3.2 Push Versus Pull in Logistics
2.4 An Example of Decomposition
References
3. Planning and Control in Production Units
3.1 Production Control in General
3.2 Basic Forms of Production
Production Control in Practice: A Situation-Dependent Decisions Approach, First Edition.
Henny Van Ooijen and Corné Dirne.
© 2024 WILEY-VCH GmbH. Published 2024 by WILEY-VCH GmbH.2 Part I Production Control in General
3.2.1 Process-Wise Production
3.2.2 Mass Assembly/Flow Production
3.2.3 (Repetitive) Small Series Production (Also Called Job-Shop)
3.2.4 (Repetitive) Project-Wise Production
3.2.5 Throughput Time Production Units
References
4. Framework for Logistic Planning and Control in Production Systems
4.1 General Framework
4.2 Position of this Book
References3
1
Production Control – A Logistic Control Function
According to an earlier definition of American Production and Inventory Control
Society (APICS), production control is defined as:
[....] the task of predicting, planning and scheduling work, taking into
account manpower, materials availability and other capacity restrictions, and
cost to achieve proper quality and quantity at the time it is needed, and then
following up the schedule to see that the plan is carried out, using whatever
systems have proven satisfactory for the purpose.
(MacKay and Wiers (2004))
As such, production control can be regarded as a logistic planning and control
(LPC) function within a production environment. Therefore, we will first discuss
logistics in the general sense in Section 1.1. Next, in Section 1.2, we will concentrate
on basic decision elements in planning and control for logistics. Then in Section
1.3, we will discuss some specific characteristics of LPC in production, followed in
Section 1.4 by an introduction of basic terminology.
1.1 Logistics
The term “Logistics” originates from the logistics on the battlefields, i.e. those activ￾ities that take care of the supply and removal of troops, equipment, and materials
to and from the battlefields (see, for instance, https://www.merriam-webster.com/
dictionary/logistics). The basic function of logistics is to make sure that the
transformation process can perform its function effectively and efficiently by provid￾ing that process with the proper information, materials, and resources (“capacity”).
In Figure 1.1, the material flow is shown as a double-lined arrow, going from left
(input of materials) to right (output of finished products); information is shown as
a single line, whereas for resources triple-lined arrows are used.
The idea of “logistics” may be applied to any type of transformation process. The
transformation process can be a production process, turning the incoming materials
Production Control in Practice: A Situation-Dependent Decisions Approach, First Edition.
Henny Van Ooijen and Corné Dirne.
© 2024 WILEY-VCH GmbH. Published 2024 by WILEY-VCH GmbH.4 1 Production Control – A Logistic Control Function
Logistics
Supply and removal of
material, capacity and information
to/from a transformation process
Transformation process
Figure 1.1 Logistics.
(“raw materials”) into outgoing products, using machines controlled by operators
(capacity resources) and specifications (“information”) determined by engineering.
However, a transformation process can also be a maintenance process where a
machine that went down (incoming material) is repaired, possibly using spare parts
(also incoming materials). The repair can be done by a mechanic using tools and
possibly other machines (capacity resources), based on maintenance instructions
(=information).
The output of a transformation process doesn’t have to be tangible. Also, in
professional service organizations like banks or insurance companies transforma￾tion processes take place: in general not regarding the transformation of form,
but the transformation of information which leads to intangible output. Within
a production context, an engineering process is an example of a transformation
process with intangible output. The incoming “material” would be information (so
nonphysical). That information is turned into product and process specifications
by engineers (capacity resources). Supporting information will be used, such as
standard solutions or background information stored in databases. In hospitals,
patients are the incoming “materials.” Doctors, nurses, operating theaters, beds,
and labs are the capacity resources used to turn sick patients into outgoing
ex-patients (hopefully cured …). Finally, transportation processes can be regarded
as transformation processes, the transformation being the change of location
of the goods transported. Then clearly the goods to transport are the materials,
using transportation documents while trucks, drivers, trains, etc. are the capacity
resources.
Examples of transformation processes are given in Figure 1.2.
The logistic function aims to make sure that:
– the objects on which the transformation is performed, are available (objects such
as materials, assets to be maintained, and patients);
– the resources required to perform the transformation are available (capacity
resources such as machines, tools, operators, and transportation resources);
– the supporting information is available (like instructions for the transformation).
Objects, information, and resources often are physical by nature, but that’s not
necessarily true for all of them (cf. the example of engineering). For instance,
software can be regarded as a resource required for a particular transformation,
or particular documents may be available digitally before a process may start. It’s
not only the availability of objects, information, and resources that matter but also1.1 Logistics 5
Figure 1.2 Examples of transformation processes. Source: https://depositphotos.com/.
the removal of these items after the transformation has taken place. Making sure
the output of the process (the “products”) is made available for the next step is an
important issue in logistics. Moreover, also getting the resources back in time and
having them available for other processes is an important logistical task, either
because these resources may not yet be at the place of the next process they will
be used for, or because the resource cannot be used directly for a new process
and will be unavailable during a certain period (e.g. because the resource needs
“re-conditioning”). Sometimes even the carrier of information has to be returned to
be available next time.
It will be clear that logistics is a very broad term. In many instances, publications,
etc., it is often interpreted in relation to warehousing and/or transportation. In this
book, we will concentrate on transformation processes that take place in production
organizations (transformation of form), and thus logistics has to be interpreted with
regard to physical production processes. Therefore, we will mainly use terminology
from industry in this book.6 1 Production Control – A Logistic Control Function
1.2 Logistics Planning and Control
Logistic planning and control is all about making decisions on the availability and
the supply of the materials, information, and capacity resources at which place and
in what quantity to get the transformation process going. The two parts of logistics
planning and control are:
– Planning: determining which jobs (“orders”) should be done and setting targets
on when and who should be doing what, etc.
– Control: starting the actual jobs, monitoring their progress, and if necessary
intervening. This is also known as the control cycle (see Figure 1.3).
We will see later that usually more than one plan is made. These plans may all
differ in time horizon (e.g. a plan for the next shift versus a plan for next year), system
boundaries (one workplace versus an entire factory), and units used (“truck ZF20/13
with options X, U, and Z, planned to be produced on time 10:15” versus “120 trucks
on day 15”).
If we look in more detail at LPC, we can distinguish the following essential
decisions (see Figure 1.4):
– Actual planning: setting targets for the transformation process considered, like
due dates (when should the process be done) and efficiency targets.
– Acceptance of a job offer: a job may not be acceptable from a logistic point of view
because the targets set are not possible to meet (like a too-tight due date), the mate￾rials are not (all) available, or the capacity resources required are not available (at
least not within the required time frame). Only accepted jobs should be considered
for release.
– Release of a job chosen from all the waiting accepted jobs: this requires some kind
of prioritizing of the waiting jobs. Together with the release of the job, the mate￾rials used, information needed, and the capacity resources required should be
released (if that is not done yet). Jobs that have been released are called “work
in progress” (WIP), and the materials connected to that job are usually stored in
a buffer at the workstations.
– Progress monitoring of the released jobs: during the progress of the transforma￾tion process, jobs may run behind schedule (or ahead of schedule). Depending on
the reaction time available and the measures that might be taken, an intervention
might be considered. Such an intervention can be an adjustment of the number of
Compare Decide
Norm,
plan
Difference
Status Decision
Legend:
Material flow
Information flow
Measurement
Action
Figure 1.3 Control cycle.1.3 Logistic Concepts in Production 7
Acceptance Release Progress monitoring
Planning
Release
Transformation
process
WIP
Figure 1.4 Basic decision elements of logistic planning and control.
capacity resources available (like hiring extra temporary operators) or a reschedul￾ing of the due dates. Also, a feedback link to the release of new jobs might be
considered. It might even be a change in the job specifications, for instance, in the
number of products to be produced. Monitoring requires some kind of progress
measurement.
Releasing new jobs, information, materials, and capacity resources (or not) and
intervening in the progress of jobs that have already been released (or removing
capacity and materials from the process) is the most direct way the logistic func￾tion may influence the logistic performance of the transformation process. In other
words, LPC combines “jobs,” “materials,” “information,” and “resources” to allow
the transformation process to start (and finish).
1.3 Logistic Concepts in Production
Logistic Planning and Control (LPC) in the case of production processes ( production
planning and control) is the main subject of this book. Before we explain the position
of LPC in a production context, it is worthwhile to describe a production process as
an aspect of a production system as In ’Veld has introduced in his System Approach
(Veeke et al. 2010). Any production system can be described in terms of the PCOI
aspects (van Assen 2016; Dijkstra et al. 1997; Ribbers and Verstegen 1992):
– Process: the actual activities of the system, including the interrelations between
these activities (like material flows) and the resources used to perform the
activities.
– Control: the planning and control of the activities of the process, usually in terms
of quality, timeliness, and costs.
– Organization: the division of tasks, responsibilities, and competencies in the
system among people and functions (“who does what”).8 1 Production Control – A Logistic Control Function
– Information: the provision and gathering of information to, in, and from the
system. This information is needed for all three other aspects of the system: to
support and monitor the process, to assist in decision-making and distribute
decisions made, and to exchange facts, knowledge, and statuses between people
and functions.
Any production system operates in an environment. Crucial parts of the environ￾ment of a production system are the consumer markets buying/using the products
(“outputs”). These markets are the “reason for being” for the production system.
The output is related to the performance the system delivers. The choices made
on what to achieve in such a context from a business perspective, translated into
targets for the production system, may be considered the goals of the system as
defined by van Assen (2016). Other markets, then the consumer markets, that
the production system has to take into account, especially from a logistic point
of view, are the supplier markets providing materials and resources (“inputs”).
We will call this view on production systems the MO/PCOI-view, which is short
for “Markets-Output-Process-Control-Organization-Information” (see Figure 1.5).
Using this model on production systems, LPC can be considered as an aspect system
of the “control” part of the production system. It focuses on the planning and
control of the timeliness of the process. The logistic performance is usually expressed
in terms of having the right amount of the right products at the right place at the right
time. It consists of two basic elements:
– Delivery reliability: the degree to which the agreed delivery specifications (in time,
place, and quantity) are met.
– Delivery time: the time required to deliver the required items (so: how fast is
delivery).
Promising a short delivery time is only wise if we’re pretty sure that we’ll be able
to keep this promise and thus meet the due date. Otherwise, it might be better to
enlarge the promised delivery time somewhat. So having a short promised delivery
time usually only leads to satisfied customers if the promised delivery time is met.
nI formation
Control
Organization
Process
Output:
product
service
Market￾based
goals
Figure 1.5 MO/PCOI-view on production
systems. Source: van Assen (2016).1.3 Logistic Concepts in Production 9
On-time delivery
Low prices
Flexibility (volume, product, ...)
Low stocks
High capacity utilization
Short lead times
Material availability
Products/materials
Process resources
Process activities
Decoupling points
Production planning
Order decisions
Progress monitoring
Jobs and tasks (formal)
Behaviour (informal)
info.systems (computer- /paper-based)
Communication
Process Control Organization Information
Internal goals
and performance
External goals
and performance
Business
strategy
Customer
requirements
Logistic concept
Business strategy
Figure 1.6 Logistic concept. Source: Adapted from Ribbers and Verstegen (1992).
Besides delivery performance, often the following two aspects are considered as
well when determining the total logistic performance:
– Flexibility of delivery: the degree to which agreements made (like due dates) can
be changed afterward without loss in delivery reliability or extra costs.
– Logistic costs: all costs associated with the supply of materials and capacity
resources, such as inventory cost, cost of storage, and ordering cost.
In real-life situations, measuring only the delivery performance as mentioned
above doesn’t give an accurate and complete image. Usually, the extra costs caused
by these decisions are considered. We will discuss the specific logistic costs later.
This view on logistics is also known as the “logistic concept” (see Figure 1.6). The
main focus of this book is the “control” part of this concept.
At this point of the discussion, it is important to understand that the logistic
performance of a production process is “only” a part of the total performance of
that process. Usually, the performance of a transformation process is based on three
considerations:
– the quality of the product and process (Quality).
– the logistic performance (Delivery).
– the efforts that are taken to do so (Costs).
In this book, the focus is on logistic performance, including that part of the costs
that are logistic related. The logistic performance of any production system is always
the result of the choices made in the design of that system. As explained, these
choices concern all four PCOI aspects of a production system. In this book, we will
limit the discussion to the choices made concerning the control aspect of the system.
Studying LPC of a production system requires an understanding of all aspects of the10 1 Production Control – A Logistic Control Function
system involved. In other words, in any real-life situation, the following logic can be
followed to understand the actual situation at hand:
– describe the processes, including materials, information, and resources used;
– describe the LPC structure, including all planning and control decisions;
– describe the division of tasks and responsibilities;
– describe the supporting IT systems, including the data available.
1.4 Terminology for Production Control
As already said, we will concentrate on transformation processes that take place
in production organizations (transformation of the form) and thus logistics has to
be interpreted with regard to physical production processes. In this section, we will
define some crucial concepts used in production control (Section 1.4.1) and discuss
some general characteristics of a production situation (Section 1.4.2).
1.4.1 Concepts Used in Production Control
If we consider a physical material transformation process, the transformation steps
can be, for instance, bending, sawing, drilling, casting, welding, etc. which are called
operations and are performed at work centers consisting of one or more (more or less)
identical machines. The sequence in which the different operations are performed
often is called routing. The routings of different products can be quite different in
some production departments, whereas in other production departments, they are
identical. If we have a nonphysical process, the sequence in which the different
operations like for instance application, classification, calculation, and sending a
mail are performed is called workflow. A schematic representation of a production
process is given in Figure 1.7.
A job is a task or combination of tasks that has to be executed at a certain work
center for a certain order.
Work center
1
Work center
2
Work center
3
Work center
4
Work center Raw 5 materials
End
products
Figure 1.7 A schematic example of a production process.1.4 Terminology for Production Control 11
An order is a general term that may refer to such diverse items as a purchase order,
shop order, customer order, planned order, or schedule. In this book, we interpret it
as a document that contains all the necessary information to produce a series of
a (semifinished) product in the production department. Often several jobs have to
be executed for one order. Releasing an order means that all the necessary materi￾als, information, and/or tools have been collected and that a department can start
working on the first job of that order.
The time necessary for an operation is called the processing time. Often an order
occupies a resource longer than the processing time. For instance, at the beginning
of the operation the order has to be administrated, the resource has to be set up and
at the end of the operation, it might be that the product needs to be cooled down.
All that time, the resource cannot be used for another order and we will call this
“extended” processing time the service time.
We will call the actual time between the arrival of the order (at a work center) and
the completion time of this order (at the work center) the (work center) throughput
time, whereas the planned time between arrival (at the work center) and completion
(at the work center), often needed for planning purposes, is called (work center)
lead time. The lead time determines the Due Date, and the throughput time deter￾mines the Completion Date. The difference between these two dates, Completion
Date – Due Date, is called the lateness; max(0, lateness) is called the tardiness, and
-min(0, lateness) is called the earliness.
Remark: Often cycle time or lead time is used instead of throughput time. This
can be confusing since cycle time is often used in certain industries (like process
industries) with quite another meaning. In this book, we will use these words as
described above and thus lead time is used for the planned time and throughput
time for the actual time.
Delivery time is the time between the acceptance of a customer order and the
delivery of this order to the customer. The transformation process is driven by work
orders that are derived from customer orders, where a customer can also be the
next stock point or department. Depending on the characteristics of the resources,
customer orders might be merged into one work order or split into several work
orders.
In Figure 1.7 we see several triangles before the operations. These triangles
represent waiting lines (that lead to waiting times) that may occur since to perform
the operation a decision is required or resources are required that are limited
available. For instance, if at a certain work center, we need a drilling machine and
we only have one drilling machine we can only start with a newly arriving order
if the drilling machine is idle, otherwise this order has to wait. This leads to the
situation that the time between the arrival of the order at a work center and the
completion time of the order at this work center, which is called the (work center)
throughput time, is larger than the processing time. In many instances, the waiting
times are much larger than the operation times which implies that the throughput
time mainly consists of waiting time.12 1 Production Control – A Logistic Control Function
1.4.2 Complexity, Uncertainty, and Flexibility
Production control in general can be very complex. Therefore, for developing a
(specific) control concept, it is important to know how the production situation can
be described in terms of:
a) complexity
b) uncertainty
c) flexibility
Ad a.: Complexity is among others caused by the variety of the products, variety in
demand, variety in operations, variety in routings, variety in number of operations
per routing, etc. High complexity requires a lot of coordination and therefore one
of the main points for a concept for production control is that it should be directed
to reduce the complexity. This can be done by decomposition: divide the total
production control problem into several subproblems each with its own objective
and decision-making autonomy. An example of this is the decomposition between
production unit control and decoupling point control (also called goods flow
control), which will be discussed later on. Other examples are the decomposition
between control at an aggregate level and detail level and the decomposition
between Sales and Production.
Ad b.: Uncertainty is caused by unpredictability and dynamics. We can make a
distinction between uncertainty at the demand side and uncertainty at the process
side. Uncertainty at the demand side can be caused by the kind of customers (end
user; dealer; …), the kind of product (consumer product; professional product;
…), etc., and uncertainty at the process side can be caused by the reliability of the
machines, fluctuation in processing times, reliability of the suppliers, quality of
the materials/ components, etc. These uncertainties influence the desired control
concept for a certain production situation. For instance, if there are long-lasting
machine breakdowns, the control is quite different than in case there are more or
less frequent variations in the processing times.
Ad c.: Flexibility is important to counteract disturbances. Forms of flexibility are:
– multi-skilled operators
– machines that have small setup times and that easily can be changed
– commonality (using the same components in several different configurations)
– short lead times of components
– overcapacity
– outsourcing
– inventories (makes it possible to react quickly to changes in for instance demand)
– overtime, etc.
If there is a lot of flexibility, the effect of uncertainties can easily be downplayed so
they don’t have a large effect on the desired control concept. Making the (potential)
flexibility effective might involve substantial coordination, which might affect the
desired control concept.References 13
References
van Assen, M.F. (2016). Operational Excellence. (in Dutch). Koninklijke Boom uitgevers.
Dijkstra, L., Dirne, C.W.G.M., Govers, C.P.M. et al. (1997). Samenwerking in
ontwikkeling: productontwikkeling door uitbesteder én toeleverancier. (in Dutch).
Kluwer Bedrijfsinformatie.
MacKay, K. and Wiers, V.C.S. (2004). Practical Production Control: A Survival guide for
Planners and Schedulers. J Ross Publishing and APICS.
Ribbers, A.M.A. and Verstegen, M.F.G.M. (1992). Toegepaste logistiek. (in Dutch).
Kluwer.
Veeke, H.P.M., Ottjes, J.A., and Lodewijks, G. (2010). The Delft Systems
Approach – Analysis and Design of Industrial Systems. Springer-Verlag London Ltd.15
2
Horizontal and Vertical Decomposition
Production planning problems are in general too complex to resolve as a single
problem. If it is possible to make a monolithic mathematical model of the problem,
then this in general will be very complex and very large. Moreover, we then have
a centralized model which (implicitly) assumes that there is central control and a
high-level owner of the model/problem in the organization. However:
– Models never capture the complete richness of the production situation.
– Detailed data and figures (needed to solve some of the short-term problems),
which might give a false sense of security, do not mean much to higher-level
managers.
– Central planning takes away control from lower levels, leading to a situation
where opportunities to control are not aligned with responsibilities.
As discussed in the previous chapter, one way to cope with the complexity of a
problem is to decompose it into several less complex subproblems. In this chapter,
we will discuss two ways of decomposing the problem, i.e. horizontal and vertical
decomposition. For the production control problem, one way to do this is to distin￾guish several production units (PUs) along the supply chain. To reduce the control
complexity, these units should be self-supporting, that is all the operations that are
necessary for the transformation of the incoming material/components of that unit
into outgoing material/components can be performed by that unit. This we will
call horizontal decomposition and will be discussed in Section 2.1. Another way of
decomposing the control problem is by aggregation or detailing decisions (in terms
of time, units, etc.). This we will call vertical decomposition and will be discussed in
Section 2.2. Decoupling point control (DPC) initiates orders to be released to the PUs;
production unit control (PUC) takes care of the execution of these orders once they
are released. The release function is at the border of DPC and PUC; therefore, we
will discuss the basic logic of triggers for production (“release triggers”) in Section
2.3. Finally, in Section 2.4, we will give an example of a decomposition.
Production Control in Practice: A Situation-Dependent Decisions Approach, First Edition.
Henny Van Ooijen and Corné Dirne.
© 2024 WILEY-VCH GmbH. Published 2024 by WILEY-VCH GmbH.16 2 Horizontal and Vertical Decomposition
2.1 Horizontal Decomposition
For any company in a supply chain, the logistic function can be split up into the
following three sub-functions (see Figure 2.1):
– Supply logistics: responsible for the timely supply of raw materials for the produc￾tion process.
– Production logistics: responsible for the timely production of the products.
– Distribution logistics: responsible for the timely delivery of the products to the
customer.
Sometimes the term “material management” is used for the logistic issues
on materials in supply and production as distinguished from the logistic issues
concerning the capacity resources. In other words, material management:
– includes both supply and production.
– focuses on the materials needed for the processes.
Usually, issues dealing with materials (“goods flow”) indeed differ from the
logistic issues dealing with resources. For instance, in many cases, the capacity
resources for production like machines and operators are already available when
a customer order arrives; the supply of these resources has been taken care of in
the past (machines were bought in the past, and operators may have a long-term
contract). For materials that might be different in most cases, the materials required
will be bought after or just a short moment before the customer places the order.
That is why “supply logistics” often only refers to the supply of materials, not to the
supply of resources.
Jobs are defined in detail by creating (production) orders. Materials only start
flowing when an order for a job requiring those materials is released. In most pro￾duction processes, more than one release decision is taken (see Figure 2.2). At the
start of the process, a release decision is taken to release the order to the process
Supply Production Distribution
Input Output
Figure 2.1 Three logistic sub-functions.
Process
Days? Weeks? Months?
Figure 2.2 Release and sequencing decisions in a process.2.1 Horizontal Decomposition 17
(“order release”). All the required order information is gathered, and a check is done
on whether the materials used in the process are available. The orders define the
number of products to be created, the materials to be used, the jobs to be performed,
etc. An example is shown in Figure 2.3. Usually, targets are also set, like the due date
of the order (the moment the order should be finished). When the right moment in
time has arrived, the order will be released to be processed. What would be “the right
moment” depends heavily on the process and circumstances at hand; we will discuss
different order release situations later in this book.
The order release decision determines which orders should be completed by the
process and the need for capacity from the workstations for the next period. Usu￾ally, at the order release, each order will get a planned due date, i.e. the moment the
order is supposed to be finished by this process. That due date can, for instance, be
used to prioritize jobs at individual workstations. With the order release, it is pos￾sible to change the priorities of orders based on, for instance, the urgencies in the
total set of orders. Also, the choice can be made not to release the order to this pro￾cess but to choose an order based on the availability of capacity of the first work
center of the order (as in “outsourcing”). The order release decision acts as a buffer
between the process at hand and possibly previous processes. It decouples the pro￾cess from the environment. That is why the position where this decision is taken
is also called a decoupling point. Decoupling points subdivide larger processes into
several subprocesses (see Figure 2.4); they limit the “size” of orders, i.e. the number
of workstations to be visited by the order (also known as the length of the routing).
That makes it possible to change the content of an order and the priorities before
releasing the order for the next subprocess (“order 2” and “order 3” in Figure 2.4).
At each decoupling point, two types of checks should be done:
– Is the order indeed an order to be done by the next subprocess, and are all require￾ments met for the job to be done? If so, the order can be accepted (“acceptance”).
– Is the workload of the next subprocess not too high, and/or are all materials and
capacity resources required available within the intended lead time? If so, the
order actually can be released (“release”).
Within the subprocesses, the logistic planning and control will be easier (assuming
that these processes are not confronted with impossible demands from the “out￾side world”). After all, it’s only this subprocess one must consider, and not also all
the other subprocesses. This is called “horizontal decomposition.” The subsystems
performing these subprocesses are called (production) units.
One specific decoupling point is the customer order decoupling point (CODP)
(Hoekstra and Romme 1992) or order penetration point (Sharman 1984; Olhager
2003). That decoupling point specifies which part of the process is triggered by a
real (customer) order and which part is triggered by planning considerations (see
Figure 2.5).
For instance:
– For processes where products are bought in a retail shop or supermarket, the
CODP is positioned at point 1.Production order
Order number P005489 Creation date
Item number CMA25 Item rts-module 
v02
Bill of 
materials
BOM-CMA25-05 24 March 2005
Batch size 100
Operation 
number Operation Resource Planned 
start date
Planned 
end data
Actual 
start date
Actual 
end date
Tool 
plan Operation plan Remarks
10 CNC sheet NC21 24 March 2013 24 March 2014 NC-A25-002 OP-NC-A25-02
20 Benchwork B040 24 March 2018 24 March 2019 B-A25-011 OP-B-A25-04
30 Assembly A12 24 March 2021 24 March 2022 A-A25-008 OP-A-A25-05
40 Testing T03 24 March 2026 24 March 2027 T-A25-003 OP-T-A25-05
Figure 2.3 Example of a production order. Source: https://timsaxblog.wordpress.com/2016/10/27/production-order-documentation-in-ax-2012/ /
WordPress.2.1 Horizontal Decomposition 19
Decoupling points
Order 1 Order 2 Order 3
Subprocess
1
Release
decision!
Release
decision!
Release
decision!
Release
decision!
Subprocess
2
Subprocess
3
Figure 2.4 Decoupling points.
Distribute to stock
Make to stock
Assemble to order
: Customer Order Decoupling Point (CODP)
Make to order
Purchase to order
1
3
4
5
2
Products at
customer
Finished
products
Materials
in stock
Subassemblies Products in
central warehouse
Materials 
at suppliers
Customer order based
Planning based
Figure 2.5 Customer order decoupling point.
– For processes where products are bought at webshops like Amazon.com or
Bol.com, the CODP is positioned at point 2.
– If products are assembled from standard parts, but according to customer specifi￾cations (like cars), then the CODP is positioned at point 3.
– For processes where products are made completely on customer order (so these
products may be customer-specific), but using raw materials that are in stock,
the CODP is positioned at point 4. That might be the case where the supply of
these raw materials may take a long time (because of the location of production of
these materials or because of the scarcity of the materials). Or it might be worth￾while to buy more raw materials than needed for one customer order because of
price reductions at high volumes.
– It’s also possible to have processes where everything, including the supply of
materials, is done on customer order (point 5, “purchase to order”).
A CODP is a specific decoupling point: at this point, a distinction can be made
between the part of the process that’s done planning-based and the part that’s
done customer order-based. The advantage of having a CODP “downstream”20 2 Horizontal and Vertical Decomposition
(to the right in Figure 2.5) is that the customer order delivery time can be very
short since only limited activities need to be performed after the order has been
placed. It’s like buying a product in a retail shop; the delivery time would be
practically zero. All processes in front of the CODP are done planning-based, giving
opportunities for optimizing the usage of capacity resources, and thus increasing
efficiency (Olhager 2003). Clearly, this can only be achieved with known products
and predictable demand (little flexibility in case the demand changes). Having the
CODP “upstream” (to the left in Figure 2.5), the advantage might be that processes
are only really performed when there is a customer demand (so no products
have to be stored waiting for a customer to arrive), and that a distinction can be
made between common parts (used in all products) and customer-specific parts
(like options). Since processes only start when there is a customer order, there is
considerable flexibility concerning the products that can be demanded. Depending
on the demand characteristics, the disadvantage of an “upstream” CODP might be
a lower efficiency. So placing the CODP is a trade-off between efficiency, flexibility,
and delivery performance.
The CODP may be different for different products and even for the same prod￾uct in different markets. For instance, if the required delivery time for a product is
very short, the CODP of a product might be positioned at the end of the process
(“downstream”). However, if a particular customer requires a similar product but
with more options, or accepts a much longer delivery time if the prices are lower,
the CODP can be positioned more “upstream” for that customer. The more “up￾stream” the CODP is positioned, the lower the risk of producing products that in
the end are not required by customers. In cases where idle capacity has to be pre￾vented as much as possible, a kind of mix of make to stock (MTS) and make to order
(MTO) can be used. For an overview of these so-called hybrid MTS/MTO systems,
we refer to Peeters and Van Ooijen (2020). If a mix of MTS/MTO is used for one
single product, we have a so-called floating decoupling point, that is there is no
fixed position. Virtual-build-to-order (VBTO) and i-VBTO systems are two systems
where the decoupling point is floating and that can serve customers from anywhere
in the inventory (work-in-process and finished goods inventory) showing an inter￾esting potential of adaptation to the variability of the demand (see, e.g. Meredith and
Akinc (2007) and Calle et al. (2016)).
As already said, one way to reduce the complexity of production control is to create
PUs in the supply chain by introducing decoupling points (see Figure 2.4). A PU is
defined as a part of the organization that transforms raw materials and/or intermedi￾ate products into intermediate and/or end products in a self-supporting way, that is,
all the operations that are necessary for a certain transformation can be performed
by the PU where the transformation is performed. With the PU, realistic agreements
can be made on the acceptance of orders and the delivery date of the accepted orders.
The PU is responsible for the timely delivery of the accepted orders. If for some rea￾son some operations can’t be performed by the corresponding PU and have to be
subcontracted, then (very) reliable agreements must be made on the lead time of
the subcontracted operation(s); if not, then the PU can’t be held responsible for low
delivery performance.2.1 Horizontal Decomposition 21
A production system in general thus consists of several decoupled PUs. These PUs
are decoupled by what we will call controlled stock points(which does not necessarily
imply that there is inventory). Arguments for decoupling can be:
● Non-synchronized processes between successive steps (due to differences in
speed, setup, and uncertainty)
● Differences in batches or batch sizes
● Differences in commonality
● Differences in available (process) information (for instance, the yield)
● Reduction of complexity
Some typical spots in production systems can be regarded as important candidates:
– The spot in a process where uncertainty changes significantly: before this point,
things like time required or the number of good products produced are uncertain;
behind this point, the process is much more clear. That would be a good spot to
have a decoupling point, making it possible to plan the downstream process in
more detail and to buffer for uncertainties in the upstream process. This buffering
against uncertainties would be done at the decoupling point in a controlled stock
point.
– The point where capacity requirements and process characteristics change signif￾icantly. That’s most evident in cases where the type of capacity resources to be
used upstream are totally different from the type of capacities used downstream.
Like complex, time-consuming machining operations for parts production versus
versatile manual assembly lines.
– The point where the geographical location of parts or resources should change.
That might require a special transportation order and some special attention
from DPC.
– If a large amount of time has elapsed since the last decoupling point, circum￾stances may have changed significantly in this time frame, resulting in a change
in demand, priorities, or availability of resources. A decoupling point gives the
possibility to reevaluate earlier decisions.
– If one of the capacity resources of the process is a bottleneck, it might be a wise
strategy to make sure that several jobs are available for the bottleneck, thus making
sure that the capacity of the bottleneck is used properly.
Remark: A production unit is not necessarily an organizational department; it is
based on logistical arguments. Often we see that departments where different
production techniques are used are also different organizational departments.
Organizational departments are most of the time decoupled from each other by
a buffer of semi-products placed between them. From a logistical point of view,
this might not be wise. For instance, if one of the departments has a complicated
process where the quality of the products depends on the status of a certain
machine and in the succeeding department these products are tested (requires22 2 Horizontal and Vertical Decomposition
other techniques than producing), it might be wise not to use a (buffer) stock
between these two departments but to do the testing immediately after produc￾tion. In this way, we know how many good products we have, and it prevents
“bad” products from being made for a long time. So, from a logistical point of
view, there is one production unit, where we have two organizational depart￾ments (or units). On the other hand, it might be true that from a logistical point
of view, one organizational department can be better divided into two decou￾pled production units, for instance, because a distinction can be made in the
logistic characteristics of the products that flow through these units.
2.2 Vertical Decomposition
All the decoupling points between the subprocesses must be coordinated in such a
way that the output of the process is as planned! This means that not only lead times
must be considered but also the capacity available in the subprocesses (or units).
DPC releases orders for a unit, expecting the order to be completed by the agreed
due date. The coordination of all order release decisions (in Figure 2.6 depicted by
green diabolos) aiming at meeting the customer requirements is the crucial function
of DPC. The assumption made is that the units will meet the performance of the
orders released as agreed upon. Defining decoupling points, leading to a subdivision
into smaller subprocesses, gives the possibility to make a distinction between two
types of planning and control (see Figure 2.6), which makes the production control
problem easier.
– Decoupling Point Control (DPC)
This is all about getting, in the end, the order completed according to the deliv￾ery requirements. It is the function that has contact with the market and based
on the market information coordinates production with sales (or demand) and
coordinates the outputs of the PUs. It controls in consultation with the PU the
release of work orders to the PUs using a model of the PU based on the operational
characteristics of that unit like lead times, capacity restrictions, etc. (often there is
no explicit order release decision, and the work orders are released immediately
Decoupling point control
Unit
control
Unit
control
Unit
control
Figure 2.6 Decoupling point control versus unit control.2.2 Vertical Decomposition 23
after their creation). From the perspective of DPC, the PUs are black boxes. At
this control level, we have to deal with the coordination of flows and production
levels in aggregate terms (capacity management) and the detailed coordination
of individual product items (material coordination). The latter mainly takes place
via the control of the stock in the decoupling points. Moreover, DPC is responsi￾ble for determining the mid- or long-term amount of the resources in the different
PUs (based on their market knowledge). This is out of scope in this book since our
focus is on the operational level.
– Production Unit Control (PUC)
The task of PUC is to meet the due dates given, assuming that DPC will keep
the demand at a manageable rate. To be more precise, the workload should not
exceed certain limits. It does so by giving priorities to orders and allocating capac￾ity resources (like machines and people) to operations to be performed. Keeping
order throughput times within limits is crucial for PUC. But at the same time,
efficiency considerations are important as well in order not to spend too much
capacity (and thus costs) on executing the jobs.
DPC can make realistic agreements with PUC on the acceptance of orders and the
delivery dates of the accepted orders. The PU is responsible for the timely deliv￾ery of accepted orders. If for some reason some operations can’t be performed by
the corresponding PU and have to be subcontracted, then with the subcontractor
(very) reliable agreements must be made on the lead time of the subcontracted
operation(s); if not, then the head of the PU can’t be held responsible in case the
PU has a low delivery performance.
Remark: In many textbooks, the term goods flow control is used instead of
decoupling point control. We prefer the term decoupling point control since
goods flow control might suggest that it also controls the flow of goods within
a PU.
By using decoupling points, two layers of planning and control are introduced: one
layer is the “decoupling point level,” and the second layer is the “unit level.” This
is called the “vertical decomposition” of the logistic planning and control problem.
With these layers of planning and control, three basic order release decisions are
created (see Figure 2.7):
– the acceptance of a (customer) order to be produced;
– the release of a (production) order to a (production) unit;
– the choice of the next production order to be taken in production at a workstation
(sequencing).
For the latter two decisions, detailed planning might be made before the release.
If such a plan exists and the process runs according to plan, then deciding on which
job should be started on which workstation would mean simply following the plan.
It would be an ideal situation if you were able to create such a plan: you can optimize
all performance aspects in one decision (the all-covering plan), making sure once the24 2 Horizontal and Vertical Decomposition
Unit 1 Unit 2 Unit 3
Decoupling point control Decoupling point control
1
2 2 2
3 3 3 3 3 3 3 3 3
1: (Customer) Order acceptance
2: (Production) Order release
3: (Production) Order sequencing
2
Figure 2.7 Three basic release decisions.
plan is made, everything and everyone will work exactly as planned. All priorities
at all workstations in the process can be determined right at the moment of release
of the order. Unfortunately, in many cases the situation on a production floor is far
too complicated, variable, and uncertain for such a plan to be created, let alone be
optimal. There are too many uncertainties at the moment of order release. What
new orders will arrive shortly? Is the production process really that predictable? Too
often, circumstances will change after the decision is made, requiring some type of
reaction: operators get sick (fewer resources), jobs may take more time than origi￾nally estimated, materials appear to be unavailable at the moment a job is supposed
to start, machines break down, etc. In other words, often the production processes
requiring logistic planning and control are not deterministic. If you’re lucky, they
are stochastic (= seem to follow a known probability distribution). And sometimes
you do not even know how to describe that variability; all you know is that things
will change, and processes are just “very variable.” That’s why slack is included in
the planning, why in most cases we need some type of monitoring to determine the
actual situation, and why it might be wise to use priority rules as will be discussed
in Part III.
Just in front of every decoupling point, we can choose to create and keep a stock
of products. By doing so, the availability of materials at order release is not a big
issue anymore. The disadvantage would be that these so-called controlled stock points
will cost money. The stock points are controlled because it is the intention to have
a certain amount of stock at that point in the process just in case that stock might
be needed shortly, and materials only “leave” the stock point if an order that needs
these materials is released. Please note that these stock points differ from the stock
points (waiting lines) at workstations since these stocks are only there as long as jobs
are waiting at the workstation. As soon as all jobs are done at the workstations, the
buffer will be empty (see also Figure 2.8).
The vertical decomposition discussed so far focuses on order-based release
decisions and material flows. In most situations prior to these order-based deci￾sions, other decisions must have been made earlier. These involve decisions on
capacity reservations at suppliers for future demand, long-term contracts with
customers, decisions on investments in machine capacity, long-term related deci￾sions on human resources, etc. These decisions do not use detailed order-related
information but are based on more aggregated data. For instance, the choice of2.3 Types of Release Triggers 25
Figure 2.8 Controlled
stock versus buffer.
Buffer (=queue) : waiting for capacity!
Decoupling (=controlled stock point) : waiting for order!
Order
Capacity
Inv ent ory
extending the machine capacity can be based on the expected total production
volume per year instead of detailed information on the exact number of individual
products to be sold next week. In other words, aggregation is done in terms of the
type of units used (material or product categories instead of specific materials or
products) and in terms of time (months or years instead of hours, days, or weeks).
These decisions are part of what is called “aggregate planning” (see, for instance,
Nahmias and Olsen (2015)) and are taken at a decision layer different from the
DPC. You can say that these decisions are at the third vertical layer, as will be shown
in the decision framework in Chapter 4.
2.3 Types of Release Triggers
We will describe two important distinctions in situations and triggers, i.e. the
Just-in-Time (JIT) situation versus the Just-in-Case (JIC) situation (Section 2.3.1),
and the differences in types of triggers (pull, push, or plan-based triggers;
Section 2.3.2).
2.3.1 Just-in-Time Versus Just-in-Case
In production logistics, two important different situations for supplying materials
can be distinguished (see Figure 2.9): supply “on order” or supply “on stock.” In case
the supply is done “on order,” the process of supplying (whether it is an external
supplier shipping the items or an internal process producing components) is only
started if a customer order is received or is expected to be received (i.e. predicted).
In other words, the process is only performed if an actual customer requirement is
assumed. In that case, no stock is required. That is a big advantage of this situation:
no costs need to be made for storing the products and materials, no risks are taken on
having leftovers not needed anymore by customers, and no extensive prefinancing26 2 Horizontal and Vertical Decomposition
Lead time L
Lead time L
On order
On stock
I want ! Delivery time
Delivery time
Process
Process
Figure 2.9 On order versus on stock.
Time
Order size
Required
time
Throughput time TPT 
Order
time
D(t): demand at time “t”
t-TPT t
Figure 2.10 “Just-in-Time.”
is needed for materials and activities. Especially if the customer is prepared to wait
the entire process lead time for delivery, this situation is advantageous for the sup￾plier of the products. As shown in Figure 2.10, assuming the demand at time “t”
(written as D(t)) is known TPT time units before (TPT being the throughput time of
the supplying process), the planning of supply becomes easy: simply start an order
of size D(t) at moment t-TPT. The moment the supply process supplies the prod￾uct, the customer requires exactly that amount of product. This is the ideal “JIT”
situation.
Unfortunately in most real-life situations, things are less ideal. First of all, often
the throughput time of the supplying process is not always known exactly. Due to
uncertainties in the time required for the process and to limitations on available
resources used in the supply process, the throughput time of supply will differ from
time to time. Figure 2.11 shows a more realistic distribution of the throughput time.
If that is the case, then throughput time is not one number, but it is a distribution
with probabilities. Assuming we know the distribution of the throughput time, we
can use the average throughput time (TPT) as lead time. That means that we would
start with the supply of the order at the moment t-TPT. However, if we do that in2.3 Types of Release Triggers 27
Throughput time TPT 
Probability
planned lead time Lt
Figure 2.11 Throughput time distribution.
Throughput time TPT
For instance: 95% delivery reliability
Lead time
Probability
Average
Based on:
- Required certainty
- Possibility and cost 
of storage
Figure 2.12 Throughput time distribution and (planned) lead time.
many cases the supply would arrive too late (see Figure 2.12). The probability of the
throughput time being longer than the average throughput time is far too high to
make the average throughput time a smart lead time. Instead, it would be wise to
use a longer lead time, e.g. a lead time that would have a probability of 95% (or even
higher) to cover the throughput time.
Similar to throughput time, in many cases also demand is not exactly known.
If demand is not based on customer orders but on forecasts, then such a forecast
is needed at the moment we need to release an order for supply. We will denote
that forecast as E(D(Lt)) (expected demand on t+Lt, with Lt being the planned lead
time at time t). Again, depending on the distribution of the demand, the forecast
will be more or less accurate, and thus we may order more than E(D(Lt)) to cover
uncertainty in the demand (Qt >E(D(Lt)).28 2 Horizontal and Vertical Decomposition
Time
Order size
Planned lead time Lt
E(D( Lt
))
t + Lt t
Qt
Figure 2.13 “On Order” situation in real-life situations.
To summarize: in the “on order” situation, the idea of supplying just in time
requires the release of supply orders on t at the size of Qt >E(D(Lt)), with Lt being
the lead time at time t (see Figure 2.13).
As indicated in Figure 2.9, the second situation for supplying products is the
“on-stock” situation. In that situation, the idea is to supply the product to the
customer from stock. At the moment the customer requires the product, it can be
delivered immediately. As a result, the customer order delivery time is equal to just
the lead time of “handing over the product.” That lead time would not be zero but
usually is not much larger than that. In other words, products are kept in stock
just in case a customer might come and ask for them. That is why we would call
this situation “JIC,” as opposed to “JIT.” In this situation forecasts of the demand
are not used for supply to the customer, but for reordering products to refill stock.
This is the situation for instance at supermarkets. But also in case the products are
commodities or demand for a product is high enough it might be wise to supply
on stock.
Remark: Please note that the customer order decoupling points, as shown in
Figure 2.5, indicate which parts of the production processes are done "on order"
and which parts are done "on stock." We will see that the impact of producing "on
order" versus "on stock" is big, not only on the issue of storing products but also
on the characteristics of the production processes themselves and the logistic
planning and control of those processes.
2.3.2 Push Versus Pull in Logistics
The moment of choice and release of jobs waiting for the start of a process can be
based on different considerations. In logistics, three basic different principles can be
used (see Figure 2.14):2.3 Types of Release Triggers 29
Process
Pull
Process
Push
Process
Plan based
Plan
Queue
Queue
Queue
Figure 2.14 Push versus pull versus plan-based.
– Pull:
The moment of choice can be pull-based. Then the choice of a new job from the
waiting jobs is only taken if a customer wants the job to be done (i.e. needs the
products to be produced). The customer – whether a real external customer or
a next workstation – triggers the production of products. As a result, when no
request from a customer is received, no jobs will be performed, even if resources
are available and materials are waiting to be processed. The advantage is that no
products are being produced without actual demand for those products. And thus
there is no risk of excessive and obsolete stock of products. The production is done
“on order,” as indicated in Section 2.3.1. The disadvantage is that when a customer
wants a product, some delivery time is needed. Moreover, in case of no demand,
capacity resources are not required for this process. If there is no alternative usage
for these resources, they can become idle.
– Push:
In a push situation, jobs are performed based on the availability of materials and
orders (and resources). The arrival of a new job in the queue might be the trigger
for the next step (assuming the availability of resources at that time), even if the
products are not requested yet by a customer. In such a case, these products will
be added to stock (indicated as production “on stock” in Section 2.3.1).
The advantage of this situation is that the resource capacity is used to the full
extent. The moment resources become available, the next job will be chosen30 2 Horizontal and Vertical Decomposition
from the jobs already waiting. However, push production might lead to very high
stock-keeping costs, and if the result would be a stock of unwanted products
while other products that are wanted are not available on stock, the costs would
even be higher.
– Plan-Based:
In the case of plan-based production, the trigger for choosing a new job from
the waiting queue is not based on direct customer demand, nor on the actual
availability of materials or waiting jobs, but on a predetermined plan. Only if the
plan states that a new job should be started, the next job will be chosen. Usu￾ally, such a plan also specifies which job should be chosen. The logic used in the
plan can be anything. There might be some push-based logic behind it. It can be a
pull-based logic or any other logic. The idea behind plan-based production would
be that it might be possible to create a detailed plan optimizing the performance
of the production system. This might be possible in some situations, but unfor￾tunately creating such a plan is a very complex if not impossible task in most
situations. The impact of this way of working depends heavily on the quality of
the plan used.
2.4 An Example of Decomposition
Let’s take a look at an example of decomposition. Consider a company producing
sunroofs for campers. For the motorized version of the sunroof, the production
process is organized as follows:
– The sunroof is assembled in an assembly line where all parts (fasteners, glass,
frame, and engine) are supplied. Assembly consists of the following processing
steps:
• positioning of the frame;
• applying a sealing strip;
• fastening the roof engine;
• assembling 2 glass plates into the frame.
• testing the roof.
– The engine to open the roof is purchased. This engine is specific to the type of
camper. It is delivered directly to the assembly line (no intermediate storage).
– Fasteners, sealing strips, and packaging materials are in stock within the company
and are standard for all camper types.
– Two internal departments supply the assembly line with components:
• Glass is purchased in large sheets. The glass plates are made of these sheets
in batches. Batch sizes are determined by the expected customer orders (so on
forecast).
• The frame parts themselves are standard for all camper types and are purchased
in stock.
Every size is produced only once each week. Each sunroof requires two glass
plates. To produce a glass plate, the glass sheets are cut and prepared. After prepar￾ing, the glass plate must cure (dry) for several hours. The specific frame for camper2.4 An Example of Decomposition 31
Frame 
parts
Glass 
sheets
Fasteners
Strips
Packing 
material
Frames
Glass 
plates
PU
framing
Production unit
glass plate
S
S
CODP
CODP
CODP
CODP
CODP
CODP
S
S
S
S
Ready
to
ship
Engine
Production unit
assembly and packing 
Figure 2.15 Logistic structure of the sunroof example.
type x is assembled from different frame parts on customer order. Mounting of the
frame takes one hour, and all variants are made at the pre-assembly department
every day.
After assembly, the sunroof is packaged and prepared (and temporarily stored)
for shipment. The logistic structure for this situation is given in Figure 2.15. The
figure shows the PUs and all decoupling points, including the CODPs. As shown,
the CODPs are positioned differently for the different parts. The CODP for the
engine lies at the supplier since engines are bought on customer order. Most other
materials are bought in stock to be available when assembly starts. These materials
are not specific to customer orders. Glass plates are produced on stock, not only
because each plate can be used for more than one camper type, but also because
of the batch sizes used in production and the processing times required. The PUs
shown in Figure 2.15 comprise one or more detailed processes. These processes
are shown in more detail in Figure 2.16. These processes will only start when
orders are released. Some processes are planned in detail beforehand (like the
production of glass plates), and others simply follow a FIFO sequence (assembly).
The PU framing follows a different sequencing logic, based on the set of customer
orders and the required change-over times between different frames. DPC accepts
customer orders based on the available capacity in the PU assembly. It also makes
sure that all materials (and resources) are available when orders are released to the
individual PUs.32 2 Horizontal and Vertical Decomposition
Frame 
parts
Glass 
sheets
Fasteners
Strips
Packing 
material
Frames
Glass 
plates
Mounting 
frame Assembly Fasten 
engine
Apply 
strip
Position
frame
Packing Prepare 
shipment
Test
Cut Prepare Dry
S
S
S
S
S
S
Ready
to
ship
Engine
Figure 2.16 Detailed logistic structure of the sunroof example.
This example already shows that the five different positions of the CODP, as shown
in Figure 2.16, do not cover 100% of the real-life situations. These five positions
should be considered ideal types; reality will show many more variations than just
these five.
References
Calle, M., González-R, P.L., Leon, J.M. et al. (2016). Integrated management of
inventory and production systems based on floating decoupling point and real-time
information: a simulation based analysis. International Journal of Production
Economics 181: 48–57.
Hoekstra, S.J. and Romme, J. (1992). Integrated Logistic Structures: Developing Customer
Oriented Goods Flow. London, UK: McGraw-Hill.
Meredith, J.R. and Akinc, U. (2007). Characterizing and structuring a new
make-to-forecast production strategy. Journal of Operations Management 25 (3):
623–642.
Nahmias, S. and Olsen, T.L. (2015). Production and Operations Analysis, 7e. Waveland
Press, Inc.
Olhager, J. (2003). Strategic positioning of the order penetration point. International
Journal of Production Economics 85: 319–329.
Peeters, K. and Van Ooijen, H.P.G. (2020). Hybrid make-to-stock and make-to-order
systems: a taxonomic review. International Journal of Production Research 58 (15):
4659–4688.
Sharman, G. (1984). The rediscovery of logistics. Harvard Business Review 62 (5): 71–80.33
3
Planning and Control in Production Units
In Chapter 2, the concept of decoupling points has been introduced, thus making
it possible to distinguish decoupling point control from production unit control. In
this chapter, we will take a closer look at what happens inside the production units
in Section 3.1. Moreover, in Section 3.2, we will discuss the basic forms of production
that we distinguish, which will be the basis for our discussion in Part III.
3.1 Production Control in General
It will be evident that production units can differ a lot and therefore a uniform way of
production control does not exist. The control of production heavily depends upon
the characteristics of its production environment. Each production environment dif￾fers from other production environments, and it is relevant to know the internal and
external characteristics of the production unit to develop a good way of production
control. It is probably impossible to find a depleting typology; however, two main fac￾tors can be distinguished that are relevant from a production control point of view
(see e.g. Wild (1979)) and these can be used for a typology:
– Material complexity:
With material complexity, we especially mean the kind of and/or the number of
different materials/components that are necessary for an order. By kind of we
mean the complexity of processing the material, which often is related to the
specifics of the resources needed. It is even more complex if the products produced
by the department have different structures.
Material complexity is connected to the Bill of Materials (BOM) of the products
produced in the production unit. An example of a BOM is shown in Figure 3.1.
The more different types of parts and materials are used inside the unit, the more
difficult it becomes to make sure that all materials are available at the workstation
at the moment that these are needed. Also: the more different BOMs a production
unit has to handle, the more difficult the material coordination is going to be. In
other words, material complexity is determined by two things:
• the number of layers and different items a BOM exists of,
• and the number of different BOMs the unit has to handle.
Production Control in Practice: A Situation-Dependent Decisions Approach, First Edition.
Henny Van Ooijen and Corné Dirne.
© 2024 WILEY-VCH GmbH. Published 2024 by WILEY-VCH GmbH.34 3 Planning and Control in Production Units
Bike (1)
Handlebars (1)
Spokes (25) Tire rims (1) Aluminum
tubing (3 feet) Paint (0.5 gal)
Frame assembly
(1) Seat (1)
Wheel (2) Frame (1)
Figure 3.1 Example Bill of Materials.
– Capacity complexity:
Capacity complexity is caused by the fact that products, in order to be produced,
may have to visit several workstations in the production unit. The set and
sequence of workstations visited by a production order is called the routing of
that order. If all orders follow the same routing, planning and control is easier
(for instance in line production) than when there are all kinds of routings. If the
routings in a production unit differ a lot, planning and control can become really
complicated. Sometimes routings contain loops, i.e. some workstations may be
visited more than once (see for example Figure 3.2). That makes planning the
work even more difficult. The longer the routing, the more loops a routing has,
and the more variation there is in the routings for a production unit, the more
difficult planning and control becomes. This needs a lot of coordination. Also, if
several specialized resources are required good coordination is required for the
efficient use of these resources. In these situations, we often see few repeat orders
Work
center 1
Work
center 2
Work
center 3
Work
center 4
Work
Raw center 5
materials
End
products
Figure 3.2 High capacity complexity.3.2 Basic Forms of Production 35
for each product. If there are many repeat orders, there is less product uncertainty
and therefore it can be economically beneficial to invest in integrating production
steps and product-specific resources.
3.2 Basic Forms of Production
The just discussed material complexity and capacity complexity lead to the four fol￾lowing basic forms of production (as shown in Figure 3.3):
– Process-Wise Production
– Mass Assembly/Flow Production
– Small Series Production
– Project Wise Production
3.2.1 Process-Wise Production
According to American Production and Inventory Control Society (APICS), process
manufacturing or process-wise production is production that adds value by mixing,
separating, forming, and/or performing chemical reactions. It may be done in either
batch or continuous mode (see also Fransoo and Rutten (1993)). In a process-based
production unit, the complexity of planning and controlling the jobs is not linked
to capacity complexity or material complexity. Routings are relatively simple: only
a few operations are required, usually in a similar sequence. The materials used are
also limited in variety, as is the number of layers in the BOM. Since process-wise
production departments are specialized in the production of a limited number of
products with a high demand per product or product family, this (economically)
allows for specialized production equipment and production personnel. Often there
are large set-up times between (families of) products and therefore the biggest con￾trol issue in these production departments is the sequence of jobs to be produced
(so to control the time that is used for set-up). Due to the specialization, all products
visit the work centers in the same sequence, so there is little uncertainty about the
routings. Examples of these kinds of production are the production of corrugated
Capacity
complexity
Low
High
Material complexity
Low High
Mass assembly/
flow production
Process-wise
production
Project-wise
production
Job shop
production
Figure 3.3 Basic forms of production.36 3 Planning and Control in Production Units
cardboard packaging, the production of steel tubes, the production of paint, and the
production of glass bottles.
Characteristic for flow process production is that the products are made of a (very)
restricted number of raw materials and that there is a strongly divergent flow of
materials, that is, there are many more end products than raw materials.
3.2.2 Mass Assembly/Flow Production
According to APICS, flow production is a form of manufacturing organization in
which machines and operators handle a standard, usually uninterrupted, material
flow. The operators generally perform the same operations for each production run.
A flow shop is often referred to as a mass production shop or is said to have a contin￾uous manufacturing layout. The plant layout (arrangement of machines, benches,
assembly lines, etc.) is designed to facilitate a product “flow.” Each product, though
variable in material specifications, uses the same flow pattern throughout the shop.
Production is set at a given rate, and the products are generally manufactured in
bulk. Synonyms are flow line, flow manufacturing, and flow plant. The production
unit is focused on the assembly of (a limited range of) complex products (such as
cars, printers, or printed circuit boards). Each product consists of a large number
of raw materials and/or components, and these BOMs can be different for different
products. However the assembly process is similar for all products and the demand
for the products is usually rather high, making it beneficial to create assembly lines.
In other words, the routings of the assembly jobs are equal or very similar, so there
is little uncertainty about the routings.
3.2.3 (Repetitive) Small Series Production (Also Called Job-Shop)
According to APICS, a job shop is an organization in which similar equipment is
organized by function. Each job follows a distinct routing through the shop. It is a
type of manufacturing process used to produce items to each customer’s specifica￾tions. Production operations are designed to handle a wide range of product designs
and are performed at fixed plant locations using general-purpose equipment. In a
job shop, only one or a few different basic components or raw materials are being
used, such as metal sheets, metal tubes, or bars to produce many products with uni￾versal/flexible, broadly employable capacities. So there is a medium/high capacity
complexity and a low/medium material complexity. Examples are the production of
mechanical and electrical components and subassemblies for the automotive indus￾try, and the furniture industry. Oftentimes there is a low number of repeat orders
for individual products, and thus it doesn’t make sense (economically) to invest in
developing specialized equipment and therefore the capacities (machines, person￾nel, and toolboxes) used in these kinds of production situations are not specifically
meant to produce certain products but can be used for a broad spectrum of products.
The broad spectrum of products that can be made and the low number of repeat
orders have as a consequence that many routings can be distinguished. This leads3.2 Basic Forms of Production 37
to, in the short term, much uncertainty about the required capacity and the priority
of the orders. Often many orders are worked on simultaneously.
In the ultimate situation, each product is unique and then we have an infinite
number of routings. In that case, we speak of a one-of-a-kind production situation.
Since each order is unique and thus has its own routing, processing times, and/or
materials, the (statistics of) the (general) production characteristics of the orders
are unknown. There is no useful historical demand and future demand is often
unknown. A possibility for the (logistic) control of the production in these kinds
of production situations might be to use data from more or less similar products
that have been produced in the past. We will concentrate on the most common basic
production situations and therefore one-of-a-kind is outside our scope.
3.2.4 (Repetitive) Project-Wise Production
According to APICS, project manufacturing or project-wise production is a type
of manufacturing process used for large, often unique, items or structures that
require a custom design capability (engineer-to-order). This type of process is highly
flexible and can cope with a broad range of product designs and design changes.
Project manufacturing usually uses a fixed-position type layout. These types of
production environments often produce end products that have a very low number
of repeat orders (for instance luxurious yachts). The products are assembled from a
large amount of raw materials and components, and the process steps needed are
plenty, using different types of broadly employable resources and having a diversity
of sequential interrelations. In the ultimate case, the real project shop, we have
unique products. The products are complex concerning material requirements
and/or required capacities. This leads to much uncertainty about both the demand
for capacity and the demand for material. Examples are building companies,
companies that produce baggage handling systems, engineering departments
of engineer-to-order companies, and assembly departments for large complex
objects (for instance packaging machines or wafer steppers). Although there is a
relatively low number of orders that are worked on, each customer order consists
of several production orders and procurement orders that must be coordinated,
which complicates the control. In many of these situations, the products are not
moved toward the resources, but resources are moved when needed to the location
of the product. This is called “dock production.” The completion of jobs in such a
production unit is like completing a project.
3.2.5 Throughput Time Production Units
Given the differences between the four indicated types of production situations, hav￾ing decoupling points between different types of production units makes more sense.
The differences in the required focus of logistic planning and control in the differ￾ent production situations due to differences in complexity and uncertainty make it
difficult if not almost impossible to combine different types in one unit.38 3 Planning and Control in Production Units
Setup/change-over Processing item 1 Processing item 2 Processing item 3 Processing item n
Setup time Proc.time item 1 Batching time
item 1
Proc.time item 2 Batching time
item 2; part 2
Batching time
item 2; part 1
Proc.time item 3 Batching time
item 3; part 2
Batching time
item 3; part 1
Batching time Proc.time item n
item n
Batch operation time
Figure 3.4 Batch operation time.
One of the important characteristics of a production unit from a logistic point of
view is the throughput time of jobs through the unit. Product throughput times usu￾ally consist of several components:
– transportation time (from one workstation to the next station);
– queuing time (waiting time of a job before the start of a new process step);
– setup time required to set up the workstation for the product to be produced (also
known as changeover time);
– processing time of the product, including possibly loading and unloading of the
product;
– batching time (waiting time of a product on the processing of other products from
the same batch, either before the start of the processing of the individual product
or after this processing has finished).
The sum of setup times, processing times, and batching times is known as the
batch operation time (see Figure 3.4). The takt time of a process is the average time
between two subsequent departures of completed products. Takt time refers to the
rhythm of the process. It is often used in situations where the variation of these
inter-departure times is limited as in production lines. In literature, the term “takt
time” is also used to indicate the required inter-departure time based on the demand
for the products. We will refer to that time as the required takt time versus the actual
takt time (which refers to the actual average inter-departure time). The takt of the
process can then be regarded as the number of products produced per time. It’s a syn￾onym for the output rate of the process. Usually, throughput times, inter-departure
times, and operation times vary. The degree to which they vary can be expressed in
standard statistical terms such as variance, standard deviation, and variation coeffi￾cient. Variation usually complicates the logistic planning and control. But to what
level depends on what is known about that variation. In case all relevant data (times)
are known, the planning and control problem becomes a deterministic problem. OneReferences 39
of the major advantages would be that optimization of certain performance mea￾sures might become an option. However, in many real-life situations, not all times
are known beforehand. If an assumption can be made on the probability distribu￾tion of those times, the planning and control problem becomes a stochastic problem.
In these situations, optimizing the performance in a mathematical sense becomes
difficult. In most of these situations, a satisfactory performance in most situations is
the best that can be achieved. In case nothing is known about the times, not even
the probability distributions, improving logistic planning and control of a process
becomes very difficult. In these cases, the only option is to increase the flexibility
and/or freedom of choice as much as possible since immediate reaction to actual
status changes can be required.
References
Fransoo, J.C. and Rutten, W. (1993). A typology of production control situations in
process industries. IJOPM 14 (12): 47–57.
Wild, R. (1979). Production and Operations Management, Principles and Techniques.
London: Holt, Rinehart and Winston.41
4
Framework for Logistic Planning and Control
in Production Systems
To position our book, we will discuss a general framework for production planning
and control (Logistic Planning and Control in Production Systems) that is relevant
from a logistic point of view in Section 4.1. Next, in Section 4.2. we will discuss the
position of this book concerning production planning and control.
4.1 General Framework
As explained in Chapter 1, logistic planning and control consists on the one hand of
planning to set the targets and initiate the release of orders, and on the other hand of
control to monitor and, if necessary, react to actual order progress. In a real-life con￾text, usually several plans are made, each with a different time horizon, frequency,
time buckets, and units used. The reason for that is that these plans are required
for decisions that have different horizons and levels of detail as well. The concept
of vertical decomposition as introduced in Chapter 2 allows us to take decisions
with a different time horizon. Decisions on for instance the allocation of a capacity
resource on a particular job in most situations are not taken in one single decision
but actually are the consequence of several decisions taken with different horizons
and purposes. In Figure 4.1, an example is given of four different logistic plans con￾cerning the operator capacity for a production process. Each plan has its own merits
and function in supporting a particular decision. The plans are interrelated in the
sense that the higher-level aggregated plans create the options and boundaries for
the lower-level more detailed plans. These higher-level plans are called aggregated
because the units and time buckets that are being used in the plans are less detailed.
In the example, twice a year a plan is made with a horizon of two years on the
required number of operators per quarter. The operator capacity is expressed in the
number of full-time equivalents (FTEs) per type of operator (category, like “making”
versus “assembly”). Based on the expected demand for those quarters, decisions are
taken on hiring or firing operators. The time horizon is needed because for this com￾pany the time required to hire or fire operators would be several months. In other
words, the required reaction time is about a quarter. Each quarter a more detailed
plan is created to plan holidays of the operators and possibly contract flex workers.
Production Control in Practice: A Situation-Dependent Decisions Approach, First Edition.
Henny Van Ooijen and Corné Dirne.
© 2024 WILEY-VCH GmbH. Published 2024 by WILEY-VCH GmbH.42 4 Framework for Logistic Planning and Control in Production Systems
Decision Frequency Unit Horizon Time
bucket
Yearly
planning
1×/year fte/category
(full time
equivalent)
2 years Quarter
Quarterly
planning
4×/year hrs/category 2 quarters Week
Weekly
planning
1×/week hrs/cat/job 2 weeks Day
Daily
planning
1×/day min/cat/job 1 day Minute
Hiring/firing/
contracting
Holiday
planning/
contracting
Reservations,
coordination,
private plan
Order release
Figure 4.1 Vertical decomposition extended for operator capacity.
The horizon of that plan is two quarters (for instance to coordinate the holiday peri￾ods of the operators). The units used are more detailed, i.e. the number of hours of
operator capacity per category per week. On a weekly base, operators are assigned to
jobs that need to be produced. This weekly plan shows whether there might be issues
that need to be addressed and provides the operators with more detailed planning so
they can make arrangements, e.g. in case they need to work overtime. Finally, each
day a detailed schedule is made for each operator showing when they should start
with which job on a particular day.
The example shown in Figure 4.1 focuses on the logistic planning of operator
capacity. A similar structure of plans can be made for machine capacity. In that struc￾ture of machine plans, the issues of maintenance planning and investment in new
machinery should be included. This structure of plans for machine capacity is an
important element of what is known as “asset management.” A similar structure
of plans can be required for materials as well. For instance, usually, the contract￾ing of suppliers is done at a much earlier stage than the actual ordering of new
supplies. Also providing forecasts on volume changes with a longer time horizon
(e.g. on a half-yearly base with a horizon of one year) might be wise, especially in
the case of suppliers that provide crucial components and are long-term contacts.
Having plans with a longer time horizon allows the supplier to plan his resources
and materials. Sometimes some critical components have such a long supply lead
time that already at a very early stage these components need to be ordered from
suppliers.
To summarize: because of the reaction times required, it might be necessary to
make already some logistic decisions even though no production order has been
specified yet. This is true not only for resources but often also for materials. Usu￾ally, these types of decisions require a lot of preparation time and have a longer time
horizon than the actual allocation of a particular operator on a particular production
order on a specific day. The frequency of these “higher level” decisions is lower, the
units to plan are more aggregated (= less detailed), the time horizon used in the plan
is longer and the time buckets used are larger. These higher-level plans are called4.1 General Framework 43
MRP-II
Material requirements planning
(MRP)
Order release
Cap. req.
planning (CRP)
(aggregate) Production planning
Aggregate
resource
planning
Product-market
planning
Master production schedule
(MPS)
Demand
management
Rough-cut cap.
planning
Shop floor control
(SFC)
Procurement
Follow-up
Figure 4.2 MRP II framework. Source: Vollmann et al. (1988)/McGraw-Hill Education.
aggregate plans. On a detailed level, individual capacity resources are allocated to
specific production orders in a much smaller time bucket.
In logistic planning and control for production, a well-known framework of
plans using the ideas of aggregate and detail plans is the MRP-II Framework (see
Figure 4.2). MRP-II is short for “Manufacturing Resource Planning” (e.g. Vollmann
et al. (1988)) where MRP-I is short for “Materials Requirements Planning” (only
considers the materials and not the resources). In the MRP-II Framework, we
can recognize both aggregate plans (usually with longer time horizons and more
aggregate units and time buckets) and more detailed plans. Also, we can see that
both resources and materials are planned. Details on MRP will be discussed later in
this book.
Bertrand, Wortmann, and Wijngaard (“BWW”) present in their book on produc￾tion planning and control (Bertrand et al. 1998) another framework using the ideas of
a structure of plans. This framework is based on production control decisions and not
on functions, responsibilities, or competencies of employees or departments. Their
framework is shown in Figure 4.3. They include feedback information on actual sta￾tuses for allowing control cycles on different levels of the framework. The logistic
parameters that are input for aggregate planning and control are linked with the
non-logistic decisions taken in a business. They refer to what in Chapter 2 has been
described as the logistic performance in production. The planning and control of
the workload takes a central position in their framework. Progress of work and pos￾sible backorders are taken into account. A crucial element of the BWW framework44 4 Framework for Logistic Planning and Control in Production Systems
Aggregate
planning and
control
Workload
planning
Material
coordination
Workload
acceptance
Order
release
Order
sequencing
Capacity
assignment
Material/Order
assignment
Production unit
Logistic
parameters
(aggregate) Information on
orders and market potential
(aggregate)
Sales plan
(aggregate)
Procurement plan
Capacity plan Plan on production
volume and stock
Procurement
orders
Capacity
assignment
Order
progress
Delivery dates
(detailed) Information on
orders and forecasts
Product and
material
availability
Aggregate information on
stocks and resources
Production unit level Chain level
Work order priorities
Figure 4.3 BWW framework for logistic planning and control of production. Source:
Bertrand et al. (1998)/Educatieve Partners Nederland.
is that order release decisions should be based on workload considerations for each
separate production unit. All decisions shown in the framework from order release
and workload acceptance downwards to the material and work assignment are taken
on production unit level. All other decisions are taken on factory level (or decoupling
point level). This way the framework aligns with the concepts of decoupling points
as introduced in Chapter 2 (see also Figure 2.7).
Workload acceptance is used to align decisions on orders with decisions on
capacity resources. It is the translation of the requirements of jobs on capacity
from resources. The basic idea would be that the future flow of work orders for the
production unit is brought into line with the available production capacity and/or
vice versa. Workload planning and acceptance ensures that the expected capacity
utilization rate of the available capacity does not exceed the restrictions laid down
in the logistics parameters of the production unit. These decision-making functions
are therefore mainly conditional for the production unit to be able to meet its
targets.
The number of orders offered to a production unit can vary greatly from period
to period. It will often be tried to keep these variations under control by calculating
the consequences of the potential orders for the workload and capacity planning.
In this way, potential large overstaffing and understaffing of capacity can be identi￾fied early and solved through rescheduling of potential or planned orders (moving4.2 Position of this Book 45
forward and/or to the next level). Another way to solve the problem is to create
additional capacity on time (hiring temporary staff, attracting replenishment work)
or outsource part of the orders. To the extent possible in the given market situation,
these measures lead to greater regularity in the number of orders, and thus to a more
even workload, which in turn eases the control of the lead time. Lead time control is
an essential condition for achieving high delivery reliability. So we see that delivery
reliability starts with good capacity planning and good workload planning.
Crucial for any decision in this framework is the time frame needed between deci￾sion making and the actual result obtained (the so-called required reaction time of the
decision at hand). In case of long reaction times, decisions on workloads, capacity,
and materials need to be taken far ahead, possibly based on planned orders and/or
forecasts. The actual orders at a certain moment in time are likely to differ from
these planned orders and forecasts. In these situations, extra slack may be planned
in the availability of materials and capacity or specific reservations may be made. But
even if that is the case, at the time the actual orders are considered to be accepted
or released we still have to check whether the intended delivery dates are indeed
feasible. If not, what delivery date would be more in line with the actual situation at
hand?
4.2 Position of this Book
We will use the concepts introduced in the BWW framework throughout this book
to discuss specific situations and solutions for logistic planning and control of pro￾duction processes.
The framework presented in the previous chapter might be helpful in understand￾ing which planning and control functions we can expect and what decisions made
in the situation at hand relate to what type of decision from the framework (thus
showing what decisions might be missing). Different levels can be distinguished:
The first level in this framework represents aggregate control. Aggregate control
uses market information (aggregate information on market potential, already
accepted orders, quotations, …) and production information (availability of
resources, availability of critical raw materials, production delays/days ahead of
schedule, inventories that are aimed as anticipation of possible [temporarily] future
capacity shortages, …). This results in several plans including a production plan
(and corresponding inventory changes) and a planned availability and use of the
resource plan. The production plan and the capacity plan are time phased.
The second level concerns detailed coordination: material coordination and capac￾ity coordination or workload planning/utilization planning; the latter can be sup￾ported by rough cut capacity planning as in MRP-II.
– Material coordination
Material coordination controls the flow of goods throughout the whole company.
It starts at the delivery side by making delivery agreements with customers
or making agreements with the sales department about the rules they have to
use for determining the delivery times. In a make-to-stock situation, material46 4 Framework for Logistic Planning and Control in Production Systems
coordination determines norms for stock levels. Moreover, material coordination
determines the planned production for the different departments in volume and
timing (via planned orders). So, the result of the activities of material coordination
is agreements with customers, planned orders (checked for the availability of
materials), and their priority and supply orders. Determining planned orders and
their priority is the function of Decoupling Point Control. This mainly takes place
via the control of the decoupling stock points and will be discussed in the second
part of this book. In particular, we discuss when to order and how much to order
for a different number of characteristics decoupling point control situations that
we distinguish.
– Utilization planning/Workload planning
The utilization plan is obtained by dividing the planned production by the planned
capacity, and this information can be fed back to material coordination.
The third level of decisions can be found on the border between chain level and
production unit level: work order acceptance and work order release.
– Work order acceptance
The actual release of the planned orders often only can take place if the availability
of the resources has been checked, which is done by the decision function work
order acceptance (or workload control). For the planning not only the aforemen￾tioned utilization plan is considered but also the actual availability of the resources
and the production delays and ahead of schedules are considered.
– Work order release
The (work) order release function combines the material aspect and the capacity
aspect and results in the amount of work in process of the production unit. As
a consequence, the production unit control decisions are restricted by the work
order release.
The fourth level is the control of production units. The control of the production
units (see also Figure 4.3), especially work order detail planning, capacity/resource
allocation, priority control, and more detail on work order release will be the subject
of Part III.
References
Bertrand, J.W.M., Wortmann, J.C., and Wijngaard, J. (1998). Produktiebeheersing en
material management. (in Dutch), 2e. Houten: Educatieve Partners Nederland B.V.
Vollmann, T.E., Berry, W.L., and Whybark, D.C. (1988). Manufacturing Planning and
Control Systems, 2e. Irwin.47
Part II
Planning and Control of Decoupling Points
In this second part of the book, we will explore the functions of and decisions made at
Decoupling Points. In Chapter 5, we will introduce the concept of Decoupling Points,
and we will show that using these would help to decompose the overall Produc￾tion Control problem into interrelated but manageable smaller decision problems.
Four basic approaches for Decoupling Point Control are introduced, i.e. Re-Order
Point systems, Material Requirements Planning, Echelon Stock Control, and Line
Requirements Planning. Chapter 5 also covers the relationship between demand and
production, on the one side by translating demand and forecasts into production or
supply batches, and on the other by defining demand-related performance indica￾tors for Production Control. Chapters 6, 7, and 8 discuss the four basic approaches
for Decoupling Control (with Chapter 8 combining the two approaches using the
concept of echelon stock). In Chapter 9, we elaborate on the characteristics of these
four approaches and discuss under what circumstances which approach and with
what parameters could be used best.
Content
5. Decoupling Point Control
5.1 Decoupling Point Control – An Introduction
5.2 Performance Measures for Decoupling Point Control
5.3 Demand and Forecasting
5.3.1 Demand Pattern
5.3.2 Forecasting Methods
5.3.2.1 Time Series-Related Forecasting for Stationary Demand
5.3.2.2 Time Series-Related Forecasting for Demand with a Trend
5.4 Order Size
5.4.1 Optimal Batch Size in Case of Fixed Order Size
5.4.2 Relaxation of Assumptions
5.4.2.1 Known or Predicted Demand Variation
5.4.2.2 Quantity Discount
5.4.2.3 Minimum Order Quantity
5.4.2.4 No Variable Order-Related Costs
Production Control in Practice: A Situation-Dependent Decisions Approach, First Edition.
Henny Van Ooijen and Corné Dirne.
© 2024 WILEY-VCH GmbH. Published 2024 by WILEY-VCH GmbH.48 Part II Planning and Control of Decoupling Points
5.4.2.5 Interdependencies of Order Sizes – Not BOM Related
5.4.2.6 Interdependencies of Order Sizes – BOM Related
5.4.3 Single Period Problem
Appendix 5.A The Wagner-Whitin Algorithm
Appendix 5.B Example Impact Advanced and Optimal Approach for Determining Batch Sizes
Appendix 5.C Newsvendor Problem
References
6. Reorder Point Decoupling Point Control Systems
6.1 General Discussion of Reorder Point systems
6.2 When to Order?
6.2.1 Continuous Review
6.2.2 Periodic Review
6.2.3 The Reorder Level – Continuous Review
6.2.4 The Reorder Level – Periodic Review
6.3 How Much to Order?
6.3.1 Fixed Amount
6.3.2 Maximum Level
6.3.2.1 (s, S)
6.3.2.2 (R, s, S)
Appendix 6.A Table of the One-Sided Standard Normal Distribution
Appendix 6.B Table Standard Normal Loss Function
Appendix 6.C Reorder Level Determination in Case of a General Distribution
6.C.1 Discrete Demand
6.C.2 Continuous Demand
6.C.3 Determining the Reorder Level
References
7. MRP Decoupling Point Control Systems
7.1 General Discussion of MRP Systems
7.1.1 Material Requirements Planning (MRP-I)
7.1.2 Manufacturing Resource Planning (MRP-II)
7.1.2.1 Engine
7.1.2.2 Front End
7.1.2.3 Back End
7.2 When to Order?
7.3 How Much to Order?
7.4 Discussion on MRP-Related Issues
7.4.1 Dealing with Uncertainty
7.4.2 Bill-of-Materials Versus Bill-of-Distribution
Appendix 7.A MRP formulas
7.A.1 Rescheduling Assumption
References
8. Systems Using Echelon Stocks (ESC, LRP)
8.1 General Discussion of Systems Using Global Norms
8.1.1 Discussion on ROP and MRP
8.2.2 Echelon Stock Control Systems
8.2.3 Line Requirements Planning
8.2 When and How Much to Order?
8.2.1 When and How Much to Order in Echelon Stock Systems?
8.2.2 When and How Much to Order in Line Requirements Planning Systems?
8.3 Discussion on Echelon Stock Systems
References
9. Choosing an Appropriate DPC System
9.1 General Considerations
9.2 Advantages/Disadvantages of the Different DPC Systems
9.2.1 Bullwhip Effect
9.3 Which Decoupling Point Control System to Use?
References49
5
Decoupling Point Control
In this chapter, we will discuss several aspects that are relevant regarding the control
of decoupling points in supply chains. We will start in Section 5.1 with an elabo￾ration of decoupling point control (DPC) and give a classification of the different
control situations that can be distinguished on this level. Next, we will discuss two
important aspects of decoupling point control: in Section 5.2, we will discuss dif￾ferent performance measures, followed in Section 5.3 by a discussion of demand
characteristics.
5.1 Decoupling Point Control – An Introduction
In Part I of this book we have introduced the concept of DPC versus production unit
control (PUC). By doing so, two layers of planning and control are distinguished:
– DPC: planning and controlling the decoupling points in terms of planned receipts,
possible stock levels, and order releases, considering the usually aggregated situ￾ation of the units involved.
– PUC: planning and control of released (production) orders aimed at meeting the
planned due dates of these orders (at the next decoupling point) while considering
constraints and costs of the production unit and its resources.
Two decisions determine the height of the stock levels at the decoupling point:
(1) The release of an order to the production unit: This decision results in a decrease
in the amount of stock in the decoupling point and should be made based on the
actual condition of the production unit and the demand for items produced by
that unit. What determines the “condition” of the production unit depends on
the type of unit at hand: for job shops, other characteristics are more important
than for assembly shops. To make this work, certain agreements must be made,
and preconditions need to be met based both on the options of the unit and on
the options of the planner releasing orders. An example of such an agreement
might be: as long as the work in progress does not exceed a certain limit, a 95%
Production Control in Practice: A Situation-Dependent Decisions Approach, First Edition.
Henny Van Ooijen and Corné Dirne.
© 2024 WILEY-VCH GmbH. Published 2024 by WILEY-VCH GmbH.50 5 Decoupling Point Control
Lead time L
When?
How much?
Lead time L
On order
On stock
Delivery time
Delivery time
I want !
Figure 5.1 “On Order” versus “On Stock.”
probability can be guaranteed that the lead time for the items to be produced
will not exceed a certain time frame. This will be explained in more detail in the
part where we discuss the control of production units.
(2) The ordering of items (“supply”) from previous production units or suppliers:
This decision increases the amount of stock (inventory position) at the decou￾pling point. When the stock level is too low, or when the policy is to keep no stock
at all and there is a requirement for the items, an order for one or more items is
created and sent to the production unit or supplier providing those items (thus
creating demand at that unit). So, two ordering policies can be distinguished (see
Figure 5.1):
– In the case of an “on stock policy,” at every decoupling point, a certain amount
of stock is kept, available in case a new order using the item stored needs to
be released to the production unit (“make to stock”). That is why these stock
points are called “controlled stock points”: the stock is there for a reason, and
the stock levels are controlled explicitly. As explained, the stock is kept in these
stock points just in case that stock might be needed shortly. Just as supermar￾kets do. Determining how much stock would be needed and at what moment
in time is also known as inventory planning and control.
– In the case of an “on order policy,” no parts are kept in stock, waiting for
an order to arrive. Instead, based on expected or actual demand, an order is
sent to the previous production unit or supplier for one or more items. This
is also known as “production on demand” (“make to order”). Or when an
actual order is not received yet, but such an order is expected: “production on
forecast.”
Remark: In the rest of this book, we will use the term supplier, which can be an
external supplier as well as an internal supplier (production unit).5.1 Decoupling Point Control – An Introduction 51
Time
t t + Lt
Planned lead time
Order size
E(D(Lt
))
Q t
L t
On order On stock
Delivery time
supplier
L
Demand Ok?
Norm level
Q t
t
Figure 5.2 When and how much to order.
In both situations, two decisions are taken related to ordering: when to order and
how much to order. For the two decisions at ordering (“when” and “how much”),
three basic variables are important (see Figure 5.2):
– D(t1, t2): The demand during a period from t1 till t2.
In many cases, often the real demand is not known in advance. Instead, some
forecast is used or a mix of actual and forecasted demand. We will call that demand
the expected demand (written shortly as E(D(t1,t2)) or even shorter: E(D(t)) for
expected demand during period t).
– Qt: The number of items ordered at time t.
That amount can be based directly on the expected demand for some period in the
future but may also be based on other considerations like the costs involved.
– Lt: The lead time of the supply when ordered at moment t (also known as the
delivery time).
In real-life situations, at least three different types of lead times or delivery times
can be distinguished:
– Promised delivery time: that is the delivery time promised by the supplier (can be
a production unit).
– Planned delivery time: that is the delivery time used by the customer to deter￾mine the planned moment the items ordered would be available. That can be the
promised delivery time, but if the customer expects the supplier to be unreliable,
the planned delivery time might be different (to include some safety).
– Actual delivery time: that is the real delivery time it took the supplier to deliver a
certain order (in other words, the throughput time of the supply).
When we use Lt, we will assume that the planned delivery time is meant. And as
explained before, we will use the term lead time instead of delivery time, assuming
the supply process to be a black box with a certain lead time. As already said, D(t)
and the actual delivery usually are not known exactly beforehand. In many situa￾tions, the best we know about these variables are probability distributions (such a
situation is also known as a stochastic situation). Sometimes, we know the exact52 5 Decoupling Point Control
values of D(t) and the delivery time, although such a situation does not happen
that often. These so-called deterministic situations do help to understand some of
the basic characteristics of ordering policies, and understanding them might help in
understanding the more stochastic situations.
Although each decoupling point situation has its own characteristics and thus
its own reordering method, we can distinguish several characteristic reordering
methods. These are determined by the question of whether we have independent
demand or dependent demand (should we coordinate the order releases at the
different decoupling points, that is, should we release the orders at each decoupling
point in mutual dependence), and the question of whether we only should use local
decoupling point information or we also should use information from downstream
decoupling points (which we will call global).
If we have dependent demand, material requirements are directly coupled with
planned or released production orders for higher-level products. The orders for the
different decoupling points are released in mutual dependence. Production is con￾trolled by production orders derived from a (master) production schedule, and there￾fore, we can say that the DPC systems are program-controlled. These systems are
time-phased, that is, (a) demand (forecast) is given for several future periods (also
called time buckets).
If we have independent demand, material requirements are not directly coupled
with planned or released production orders for higher-level products. Orders at each
decoupling point are released independently based on the stock position of an item;
the DPC systems in these situations are stock-controlled. Only historical informa￾tion is used to control the decoupling points (parameter setting). This system is
non-time-phased, also called instantaneous by Verwijmeren et al. (1996).
The other important distinction between situations for DPC would be the distinc￾tion between a focus on local stock and a focus on global (or echelon) stock. In the
case of a local focus, only the stock available at the decoupling point at hand is taken
into consideration, and thus, there is no coordination of the stock points. When the
stock already present downstream is considered, i.e. if the decision is based on what
is already in the pipeline downstream plus what is at hand, then the DPC applies the
concept of echelon stock. These DPC systems make a more sophisticated (integrated)
use of the information on stocks and work in process in the system.
The two questions about order focus and stock focus lead to four characteristic
reordering methods (see also Figure 5.3):
– Re-order point methods (also known as statistical inventory control methods);
– Echelon stock control (also called base stock control) methods;
– Material requirements planning;
– Line requirements planning.
As explained, for each control method, two choices must be made when ordering
(see also Figure 5.2):
– when to order (so choosing the moment t);
– how much to order (Qt).5.2 Performance Measures for Decoupling Point Control 53
ROP
MRP LRP
Stock controlled ESC
Program controlled
Local stock info Global stock info
Order
releases
Stock info used
Figure 5.3 Four basic methods of reordering.
These decisions will be discussed for the different characteristic DPC
situations in the next chapters. We will first discuss the two situations focusing on
local performance considerations, i.e. re-order point (ROP) techniques (Chapter 6),
and materials requirement planning (MRP, Chapter 7). Then, in Chapter 8, the two
other situations will be discussed, i.e. the situations where the overall performance
is considered by using global stock information, i.e. Echelon stock control and line
requirements planning (ESC respectively LRP). Before we go into the details of
these methods, we will first take a closer look at the performance measures that can
be used to evaluate the performance of the decisions taken at DPC (Section 5.2) and
the characteristics of the demand that needs to be fulfilled (Section 5.3).
5.2 Performance Measures for Decoupling Point Control
In many approaches in scientific research, the main performance measure consid￾ered when discussing inventory control is profit maximization or cost minimization.
Using such a performance measure makes it possible to focus on a single perfor￾mance indicator (i.e. profit or costs). However, that approach requires that issues like
back-ordering or loss of sales (due to insufficient availability of the items required)
can be expressed in terms of economic value. In other words, in currencies like €
or $. In real life, translating events like back ordering and loss of sales into economic
value can be really difficult. Often, issues like loss of image and trust are connected
to these events and translating these types of issues into economic values is not obvi￾ous. That is why, in many situations, it would be preferable to not only consider the
costs or profits that can be related directly to the issue at hand but also to include the
service level provided as a separate performance measure. In other words, in most
situations, it would be preferable to consider the combination of some service level
measurement and costs (or profits) as a way of expressing the performance required.54 5 Decoupling Point Control
That creates a problem that is not focusing on a single performance indicator but on
a set of at least two. It would be up to management to make the choice on which
combination of service level and costs would be preferable.
Remark: Van Houtum and Zijm (2000) show that there is a one-to-one relation￾ship between cost models and service models, and more specifically, between
the choice of the penalty cost parameters on the one hand and the choice of
certain service measures on the other hand.
The indicators used to determine the service level performance of DPC depend on
the point of view taken. In Silver et al. (1998), at least ten performance measures are
discussed. However, assuming an on-stock policy, in real life, the following service
level definitions are used frequently from the supplier point of view (Silver et al.
1998; Diks 1997; van Donselaar and Broekmeulen 2019; Durlinger 2013):
– The cycle service level (P1):
This performance indicator gives the fraction (or percentage) of order cycles
without stock-outs. An order cycle is the time between two consecutive ordering
moments. This is called the P1-service level. So, for instance, a P1-level of 0.95
means that in 100 order cycles, on average five times, a stock-out will occur. In
general, if the expected demand in a year would be E(D(yr)), and the average
quantity ordered in an order would be Q, then the average number of orders in a
year would be:
E(D(yr))∕Q (5.1)
With a service level of P1, the average number of times a stock-out would occur
in a year would be:
(E(D(yr))∕Q) ⋅ (1 − P1) (5.2)
If E(D(yr)) is 1000 and Q equals 100, we would have, on average, 10-order cycles
in a year. With P1 = 0.95 that would result in an average of 0.5 stock-out in a year
(or one stock-out in two years).
– The fill rate (P2)
This service level is defined as the fraction of the total demand for an item that can
be fulfilled directly from stock. This is also known as the P2-service level. Usually,
this service level is less strict than the reorder cycle service level. As a result, the
amount of stock required to reach a similar fraction (e.g. 0.95) is lower. This can be
explained as follows: in the case of the P2-service level, in all order cycles where no
stock out occurs, the service level would be 100% (i.e. a fraction of 1.0). In the other
order cycles, not all demands can be fulfilled completely from stock, but some
parts still will be. So, in these order cycles, the service level would be somewhere
between 0.0 and 1.0. In the “P1-situation,” the service level in the order cycles with
stock-outs would be considered as 0. In Table 5.1, a numerical example is given
to illustrate this difference. Only when variation in demand during lead time is
very high might the opposite be true: then the number of times a shortage might5.2 Performance Measures for Decoupling Point Control 55
Table 5.1 Example difference P1 and P2.
Reorder cycle 1 2 3 4 5 6 Total
Demand 70 80 60 70 65 75 420
Delivered from stock 70 73 60 70 65 72 410
OK Not OK OK OK OK Not OK
P1 = 4/6 = 0.667
P2 = 410/420 = 0.976
occur in a year might be very low, but when it happens, the amount of the shortage
might be very high (King 2011). If we denote the expected number of items that
cannot be delivered directly from stock in a reorder cycle as E(BO) (from “back
ordered” in the reorder cycle), realizing that the average demand in an order cycle
would equal the average amount ordered (=Q), we would get:
P2 = (Q − E(BO))∕Q (5.3)
Whereas the P1 definition gives an indication for the supplier how often emer￾gency actions may have to be taken to fulfill the demand, the P2 definition is more
relevant from a customer perspective, indicating what part of the demand is likely
to be fulfilled from stock.
– The ready rate (P3):
The ready rate is the fraction of the time stock is available for a particular item. In
other words, the fraction of time the stock level is not 0. This service level defini￾tion, referred to as P3, is used in cases where the continuous availability of an item
is crucial and possibly legally required, for instance, for safety reasons or in a mil￾itary context. It is also used in cases where the actual demand is not known, only
the actual sales, like in retail. After all, if demand is not known, the determination
of P1 or P2 is not possible.
From a customer perspective, performance measures should somehow indicate
the fraction of demand that can be fulfilled as promised, i.e. either from stock
(in the “on stock” situation) or according to the agreed delivery time (in the “on
order” situation). Durlinger calls this the external service level, as opposed to the
above-mentioned internal service levels (Durlinger 2013). Two elements should be
included from a customer perspective:
– the quantity promised should be delivered (“in full”);
– the delivery should be done at the moment promised (“on time”).
The combination of these two is also referred to as OTIF: “on time in full.”
As explained above, in the “on stock” situation, the P2-service level definition
already follows the customer perspective to some extent. However, that indicator
does not consider the split up of demand over customer orders. In other words,56 5 Decoupling Point Control
it does not consider the fact that a customer may demand more than one item at the
same time (so “batches”), leaving alone the option that a customer order may con￾sist of several order lines, each specifying several units of a different type of product.
The following service level definitions do take this “demand-batching” in customer
orders into account (and are applicable in both the “on order” situation and in the
“on stock” situation):
– type 1 (CSL1):
Fraction of order lines delivered on time (as compared to the promised moment
of delivery) and complete. This measure is also known as “CLIP”: confirmed
line item performance. A variant of this performance measure is the “RLIP”:
the requested line item performance. The difference between the two is obvious:
the CLIP is based on the promises made by the supplier, while the RLIP is
purely based on the demand of the customer. If all requests are confirmed by the
supplier, these measures are identical.
– type 2 (CSL2):
Fraction of customer orders delivered on time and complete. This means that
every order-line of the order must be delivered on time and complete, so this ser￾vice level definition is stricter than the previous one.
– type 3 (CSL3):
Fraction of the total number of products ordered in a customer order that is deliv￾ered on time. This is also known as “CVP”: confirmed volume performance.
The example shown in Figure 5.4 illustrates the differences between these three
types of performance measures in an “on-stock” situation. The CLIP (SL1) in this
example is 0.70 (7 of the 10 order lines are delivered from stock), the CSL2 is 0.25
(only one out of four orders is delivered completely from stock), and the CVP is 0.81
(average of the fill rates of the four orders).
Order (in sequence of priority) Delivered from stock
Stock Item ABCDABCD
6150305010
6235003300
6304030402
6401030103
6500210021
30 Total 8 10 5 7 8 8 3 6
Total 30 25 Total
Nr. order lines 2 3 2 3 10
Nr. order lines delivered from stock 2 2 1 2 7
CSL1 1.00 0.67 0.50 0.67 0.70
CSL2 1 0 0 0 0.25
CSL3 1.00 0.80 0.60 0.86 0.81
Figure 5.4 Example of customer service levels in an “on-stock” situation.5.2 Performance Measures for Decoupling Point Control 57
Apart from the service level related to the availability of items when required, a
crucial aspect of the performance of DPC is the costs involved. The costs that can be
influenced by the decisions made by DPC are the following:
– Costs related to keeping stock:
Keeping stock usually creates costs. We might need a warehouse with equipment.
People might be needed in such a warehouse for receiving, storing, and picking
items. The items stored may need some insurance to cover the risks. These costs
need to be financed, which might lead to interest costs. We will refer to these costs
as the stock holding costs (Chold).
– Costs related to reordering:
Every time one or more items are reordered from an upstream unit (or external
supplier), some extra costs may be induced. For instance, the items ordered need
to be packed and transported. Or we might have costs related to taking care of the
required administration. These costs are called the ordering costs (Cord).
– Costs related to the value1 of the items reordered:
If items are bought from external suppliers, their price might be determined
by the number of items ordered. That would be the case if a quantity dis￾count is given by the supplier for larger orders or if the price might depend
on the urgency of the order and/or whether or not the order would fit a
previously given forecast. Even the moment of ordering might influence the
price of the items ordered, for instance, if the price depends on the situa￾tion in the market or if the item ordered is at the end of its life cycle. Items
supplied by internal units also have a certain value. That value depends
on the price paid for the raw materials but also includes the other variable
costs required to produce the item. We will refer to this cost element as the
item value (Cval).
A general performance indicator frequently used related to these costs is the inven￾tory turnover ratio. This ratio is defined as (Jacobs and Chase 2018):
ITR = costs of goods sold∕average inventory value (5.4)
This indicator shows how well the inventory is being used. If the ITR is really low
(<1.0), then we might wonder whether the inventory planning and control policies
are still okay. It might be an indication of obsolete stock. Inventory value usually is
an important part of the assets of the company on its balance sheet.
The balance between service level and costs involved is a difficult one and involves
the comparison of different points of view and interests. A higher stock level usu￾ally results in a higher service level, but also in higher stock holding costs. A similar
relation might exist between service level and frequency of ordering: more frequent
ordering (possibly even emergency orders) might increase the service level, but also
1 Note that determining the value of an item is not always that easy. It requires a more detailed
analysis of the all costs related to the item. We refer to Silver et al. (1998) and Muntslag (1993) for a
discussion more in depth on this issue.58 5 Decoupling Point Control
result in higher ordering costs and even higher item prices. Trying to express all
performance measures in the same unit (like in €) would make it possible to opti￾mize that balance, but in practice is usually not possible. It simply involves too many
different types of considerations. The choice of such a balance is not something
that is purely a matter of logistic decision-making. It involves company-wide and
market-related decisions. Therefore, the boundaries should be set by management;
DPC must then operate within these boundaries. The boundaries should be realistic
for the situation at hand. A practical way of dealing with this dilemma is to show sev￾eral “what if”-scenarios with different settings for costs and service levels, and then
let management decide. The equations and relationships described in this chapter
can be used to create these “what-if” situations.
At the end of this chapter on performance indicators, just before we introduce
the variables and logic to consider for DPC when focusing on these indicators, it is
important to stress the limitation of mathematical approaches to these problems.
As described by de Kok (2018), it is only possible to derive straightforward mathe￾matical results if there is no correlation between demand and supply characteristics.
The assumption made is that the characteristics of demand and supply are random
draws from some distribution (thus, with no correlation between these draws). This
ignores human intervention and market conditions in real-life situations.
In general, human interventions create a correlation between supply and
demand: if demand is high, lead times are longer; if demand is high, demand
is aligned with supply by negotiating a later delivery date than requested.
We […] cannot model all response options. On top of that, taking into
account the correlation between stochastic demand and stochastic supply is
mathematically inhibitive in most cases.
(de Kok 2018).
However, these models do help with understanding what actions and when
actions might be needed to cope with the situation at hand since the models
show what would happen if no intervention took place. As an example, in de
Kok (2018), a real-life 98% service level might be achieved by using a model
based on an 80% service level and an intervention mechanism with 90% success
(98% = 80%+(90%× 20%)).
5.3 Demand and Forecasting
In this section, we will study the volume of parts to be processed through the produc￾tion system and the translation of that volume into order sizes. In other words, we
will study D(t) and Qt in more detail. The first subsection will discuss the demand
pattern, including the predictability of the demand. The second subsection will dis￾cuss some of the more common time-series forecasting models. In the third subsec￾tion, we will discuss the determination of Qt based on some assumptions made. In
the last section, we will discuss the impact of these assumptions.5.3 Demand and Forecasting 59
5.3.1 Demand Pattern
Crucial for DPC is the behavior and predictability of the demand at a certain
decoupling point. If demand stems from a production unit downstream, the
planning of that unit might be used as a good source for determining the actual and
expected demand in the near future. Such demand can be regarded as dependent
demand since the demand is determined by the demand for items higher in the bill
of materials and/or planning of downstream entities. On the other hand, if there is
no downstream production unit and demand comes from external customers, there
is no (internal) dependency, and usually, demand can be regarded as independent
demand. Only if (more or less detailed) predictions are obtained from the customers
may demand still be to some extent dependent. This would be the case if some close
cooperation with the customer(s) is established, like in a main supplier arrangement
or with detailed long-term contracts. But in most situations, that won’t be the case,
and thus demand should be regarded as independent.
As explained in Silver et al. (1998), the demand in time can be regarded as consist￾ing of five different components:
– level: gives the basic level or scale of the demand (the “height” of the demand
curve);
– trend: the possible growth or decline over time;
– season: creating a returning pattern of high and low seasons;
– business cycle: impact of economic status (for instance, a trend or a step in the
level);
– randomness: any remaining issues not covered by the previous four components.
Figure 5.5 illustrates four of these components; the business cycle can take
many different forms. Typically, mathematical models used for forecasting include
regression models, models using simple moving averages, more complex models
Demand
Demand
Demand
Demand
Time
Time Time
Time
Level Trend
Season
Randomness
Figure 5.5 Four components of demand.60 5 Decoupling Point Control
covering exponential smoothing, and heuristics. We will discuss these models in the
next section.
Clearly, what a planner of replenishment orders at a decoupling point would like
to know is:
D(t): demand during a period t
However, in the case of independent demand, the only information that might be
used is the forecasted demand. In other words:
F(t) (or E(D(t)): forecast for demand during period t
Usually, that means that some error would be made when referring to the demand
at the moment of reordering new items. That error is the forecast error:
e(t) = F(t) − D(t) (5.5)
A common measure of the performance of the forecast is the mean squared error
(Silver et al. 1998):
MSE = ∑e(t)
2∕n (5.6)
Then, we can calculate the standard deviation of the forecast error with:
𝜎(e(t)) = √MSE (5.7)
In case demand is stable over time and thus only has two components, i.e. a
level-component and a randomness-component, and assuming that the forecast is
equal to the average demand, we get (see Figure 5.6):
𝜎(e(t)) = 𝜎(D(t)) (5.8)
More generally, the standard deviation of the forecast error is related to any
remaining uncertainty in demand after a forecast is determined. We will see later
that this is an important result for determining safety stocks.
5.3.2 Forecasting Methods
Two classes of forecasting methods can be distinguished if we look at the basis of the
forecast:
– “Subjective” forecasting: these types of forecasting methods use expert opinions or
expectations from relevant stakeholders to predict what may happen in the future;
– “Objective” forecasting: these methods use quantitative methods to forecast.
Average Std dev
Demand 5 6 3 7 4 10 5 3 9 8 6.00 2.45
Forecast 6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 Average Std dev
Error –1.00 0.00 –3.00 1.00 –2.00 4.00 –1.00 –3.00 3.00 2.00 0.00 2.45
Figure 5.6 Example average and standard deviation forecast error in case of stable
demand.5.3 Demand and Forecasting 61
The first type of forecasting method uses the insights of people to predict what
may happen in the (near) future. The quality of the forecast then depends to a large
extent on the relevance of the opinions of the people involved. Qualitative methods
do have some big advantages:
– non-quantitative experiences and insights of experts are used;
– any changes that may occur in the near future may be considered (if noticed by
any of the experts), even if these changes are not noticeable yet in the available
figures;
– the forecast is not an outcome of some abstract mathematical formula, but, indeed,
is “owned” by everybody involved in the procedure.
But these methods also have some disadvantages:
– the forecast tends to be subjective instead of objective, possibly resulting in an
“overweight” for the opinion of the expert with the most power or the smoothest
discussion skills;
– the forecast is likely to be not very accurate and usually can only be used with a
large confidence interval;
– the time required, and the effort of people involved may be large as compared to
quantitative methods using computers and stored data.
In this section, the focus will be on the use of quantitative models for forecasting
demand. Quantitative methods used for forecasting can be split up into two types:
– causal models
– time series models
Both types of models use one or more variables (= the forecasting variables) to pre￾dict another variable (=the forecasted variable). Whatever model we use, usually the
forecasted value is not exactly the same as the real value of the forecasted variable. In
other words, usually, a forecast is wrong! The use of more data might help to reduce
the mistakes made. The more data used, the better the forecast is likely to be. And
also the use of more recent data might be helpful. After all, information on demand
from 10 years ago is not likely to be as valuable as information from last month.
In the case of causal models, the assumption is made that there is a relation
between one variable and another. For instance, we may assume a relation between
weather conditions (e.g. temperature) and the demand for ice cream. Sometimes,
the number of variables used to obtain a forecast is not just one but more. For
instance, not only the expected temperature, but also the fact that people might have
days off because of a public holiday. In terms of mathematics, the forecast using a
causal model is based on one or more variables (x1 … xn) predicting the variable to
be forecasted (y). In other words, the assumption is that the forecasted variable y
depends on the values of the forecasting variables x1 till xn. In mathematical terms:
y ≈ f(x1, x2,… , xn) (5.9)
If this assumption is true, then knowing the x1 till xn, we can estimate y by using
this formula. This is written as:
̂y = f(x1, x2,… , xn) (5.10)
where yˆ is the estimate for y.62 5 Decoupling Point Control
A simple causal model is a model where y is assumed to follow the following logic:
y ≈ a + b ⋅ x (5.11)
An example might be the case where the number of hours required to assess a
written exam depends on:
– the estimated time to prepare the assessment (the “a”)
– the estimated time required to assess each individual exam (the “b”)
– the actual number of exams to be assessed (the “x”).
Often, more complex linear regression models use more than just one “x”; these
models are called multiple linear regression models (for obvious reasons). We will
not study these models in this section, nor will we study non-linear models.
For the single linear regression model, have a look at Figure 5.7. Let’s assume that
we know from the past n values for x (x1 till xn) and for these values also the corre￾sponding values for y (yi till yn). The dots in the figure give these real values of pairs
of xi and yi in the past. The line seems to be a reasonable forecasting model for these
pairs to be used in the future. If this line indeed gives the relation between the xi
’s
and the yi
’s in a good way, then the line may be used to predict the future yz if we
already know the xz. That is the basic principle of a causal model.
In the case of time series, the forecast is based on the historical values of the variable
forecasted. So, if a forecast is needed for the demand for a product in the next period,
the historical values of that demand are being used to predict future demand. In
mathematical terms:
D(t) ≈ f(D(t − 1), D(t − 2),… , D(t − n)) (5.12)
and as a forecast:
Ft = f(Dt−1, Dt−2,… , Dt−n) (5.13)
The model shown in Figure 5.7 can apply to a time series model if the “x” can be
regarded as “time.”
Y
X
Real values
Forecast using model
a : Intercept
b : Slope
Forecast error
Figure 5.7 Single linear regression.5.3 Demand and Forecasting 63
5.3.2.1 Time Series-Related Forecasting for Stationary Demand
In this section, we will focus on forecasting models based on time series, assuming
a linear relation between time and demand. The assumption, in that case, would be
that the actual demand behaves like:
D(t) = a + b ⋅ t + 𝜀(t) (5.14)
with:
a: a constant (the “starting level” of demand, i.e. the intercept from Figure 5.7)
b: the (constant) increase in demand in every time unit (slope of Figure 5.7)
t: period
𝜀: some (small) random variable
We will distinguish two situations: demand is stationary in time, or demand shows
a (linear) trend in time. In the case of stationary demand, the assumption is made
that b =0. In other words, the “level” of the demand doesn’t change much in time.
Demand is assumed to behave as follows:
D(t) = a + 𝜀(t) (5.15)
A typical demand pattern for stationary demand is shown in Figure 5.8. We dis￾tinguish three basic forecasting methods for this type of demand:
– moving average;
– weighted moving average;
– exponential smoothing.
Moving Average In a moving average model, the forecast for the next period is based
on the average of the actual values of the past n-periods. The assumption is that
these last n periods give a good view of what will happen next period. In Figure 5.9,
Figure 5.8 Example
demand pattern stationary
demand.
0
5
10
15
20
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
t
D(t)
For instance (N = 4): 
t 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
Dt 17 14 10 11 12 13 18 12 14 16 13 12 19 19 14 15 13 16 11 17
N = 4? 13.00 11.75 11.50 13.50 13.75 14.25 15.00 13.75 13.75 15.00 15.75 16.00 16.75 15.25 14.50 13.75
Ft = 1
4 (Dt–1 + Dt–2 + Dt–3 + Dt–4)
Ft
Figure 5.9 Example moving average.64 5 Decoupling Point Control
an example of such a model is given using n =4. In the table, Dt shows the actual
demand in the period considered. We can see, for example, that in period 1 the
demand was 17. In period 2, the demand was 14, etc. These values were used at
the beginning of period 5 to predict the new demand in that period. The formula
used is the following:
Ft = (Dt−1 + Dt−2 +···+ Dt−n)∕n (5.16)
In this example for n =4: Ft =(Dt−1 + Dt−2 + Dt−3 + Dt−4)/4
So: F5 =(D4 + D3 + D2 + D1)/4 = (17+14+10+11)/4 = 13.00
For each period, a new forecast is made, using the same formula. So, with each
“move” in periods, a new average is calculated using the last four realized values.
That’s why this model is called the “moving average” model.
Weighted Moving Average The weighted moving average is quite like the regular mov￾ing average model presented above. The only difference is that in this model, not all
values of the past are given equal weight for calculating the forecast. The assumption
is made that, for instance the most recent values may have a more predictive value
than the values further in the past. This is shown in the example of Figure 5.10.
The figure shows the same set of values for Dt in the past. And again 4 periods are
being used to determine the new forecast (n =4). However, in this example, the most
recent demand is given a weight of 0.40 instead of 0.25 (=1/4), as in the example of
Figure 5.9. The demand before that is given a weight of 0.3. The demand of the three
periods ago is given a weight of 0.20. And finally, the demand of 4 periods ago is only
given a weight of 0.10. Obviously, the sum of the weights should be 1.00 (assuming
no increase or decrease in the level of D). The formula to be used is:
Ft =
N
∑−1
i=0
(wt−1−i ⋅ Dt−1−i
) (5.17)
with: ∑N−1
i=0 (wt−1−i
) = 1.
Using this logic, we get:
F5 = (0.40 ⋅ D4 + 0.30 ⋅ D3 + 0.20 ⋅ D2 + 0.10 ⋅ D1) = 11.90
Another way of looking at this is that the normal moving average model is a spe￾cial kind of weighted moving average model. In the normal moving average, all the
weights used for the n-values from the past are the same (and equal 1/n).
For instance (N = 4): :wt–1 = 0.4; ;wt–2 = 0.3; wt–3 = 0.2 ; wt–4 = 0.1 
t 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
Dt 17 14 10 11 12 13 18 12 14 16 13 12 19 19 14 15 13 16 11 17
Ft 11.90 11.50 12.00 14.60 14.00 14.10 14.80 14.00 13.30 15.40 17.00 16.30 15.90 14.40 14.70 13.30
N = 4, 
weighted
Most impact Leastimpact
Figure 5.10 Weighted moving average.5.3 Demand and Forecasting 65
Exponential Smoothing The third time series analysis model we consider is the model
using exponential smoothing. In this forecasting model for each new forecast, a com￾bination of the actual value of the last period is used together with the previous
forecast for that value. The model looks like this:
Ft = Ft−1 + 𝛼 ⋅ (Dt−1 − Ft−1) = 𝛼 ⋅ Dt−1 + (1 − 𝛼) ⋅ Ft−1 (5.18)
with:
Ft: the new forecast for Dt
Dt−1: the actual demand in the last period (t −1)
Ft−1: the forecast made for Dt−1 in the previous period
𝛼: the so-called “smoothing-constant” (between 0 and 1).
The larger the 𝛼 is chosen (pronounce this Greek letter as “alpha”), the more
weight is given to the difference between the last actual demand and the previous
forecast. The model corrects to a certain extent (depending on the smoothing con￾stant) the forecasting error made before. In Figure 5.11, the same set of data as used
before is used to illustrate this model. In the example given 𝛼 is set at 0.1. As a result,
the new forecast for D3 is:
F3 = F2 + 0.1 ⋅ (D2 − F2) = 17.0 − 0.1 ⋅ (14.0 − 17.0) = 16.7
If we look closer at the forecasting model, we can conclude that exponential
smoothing is a special type of weighted moving average:
– The most recent demand is included with a weight of 𝛼.
– The demand in the period before that gets a weight of 𝛼⋅(1−𝛼).
– The demand three periods earlier gets a weight of 𝛼⋅(1−𝛼)
2.
– etc.
Forecasting Error The big question is: which of these three-time series models should
we use to forecast? The answer to that question depends on the expected forecast
errors! In other words, simply try these three models and determine, based on the
values of the past, which model would give the smallest error. As explained in Section
5.3.1, calculating a forecasting error is fairly simple: it’s the difference between the
forecast and the actual value of the demand (see formula (5.5)). For each period
where a forecast is made for and an actual demand is known, we can determine the
forecast error. To determine the quality of the forecasting model, often one of the
For instance α = 0.10; assumption: F1 = D1
t 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
Dt 17 14 10 11 12 13 18 12 14 16 13 12 19 19 14 15 13 16 11 17
α = 0.10 Ft 17.0 17.0 16.7 16.0 15.5 15.2 15.0 15.3 14.9 14.8 15.0 14.8 14.5 14.9 15.3 15.2 15.2 15.0 15.1 14.7
Figure 5.11 Exponential smoothing.66 5 Decoupling Point Control
t 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
Dt 17 14 10 11 12 13 18 12 14 16 13 12 19 19 14 15 13 16 11 17
N = 4 Ft 13.00 11.75 11.50 13.50 13.75 14.25 15.00 13.75 13.75 15.00 15.75 16.00 16.75 15.25 14.50 13.75
et 1.00 –1.25 –6.50 1.50 –0.25 –1.75 2.00 1.75 –5.25 –4.00 1.75 1.00 3.75 –0.75 3.50 –3.25
|et
| 1.00 1.25 6.50 1.50 0.25 1.75 2.00 1.75 5.25 4.00 1.75 1.00 3.75 –0.75 3.50 3.25
et
2 1.00 1.56 42.25 2.25 0.06 3.06 4.00 3.06 27.56 16.00 3.06 1.00 14.06 0.56 12.25 10.56
Mean error = Σ (et
)/16 = –0.42
Mean absolute deviation = Σ |et
|/16 = 2.45 
Mean squared error = Σ (et
)
22/16 = 8.89 
Figure 5.12 Quality measures for moving average with n =4.
following three calculations is used (see Figure 5.12):
– the mean error (or “ME”):
ME = 1
N
t−
∑N+1
t
e(t) (5.19)
– the mean absolute deviation (or “MAD”):
MAD = 1
N
t−
∑N+1
t
∣e(t)∣ (5.20)
(in other words, the mean of the absolute values of all the forecast errors);
– the mean squared error (MSE):
MSE = 1
N
t−
∑N+1
t
e2
(t) (5.21)
(so: using the mean of the squared forecast errors).
In Figure 5.12, these three measures for the quality of the forecast are derived from
the example given on the moving average. The errors made by the forecasting model
can be systematic or non-systematic. In case of a systematic error, the forecast tends
to be either larger than the actual demand or smaller. In the case of non-systematic
error, the average error would be around zero. That doesn’t mean that all errors are
zero. But the average error will be. In fact, in the case of a non-systematic error, the
distribution of the errors is likely to look like the distribution shown in Figure 5.13.
Clearly, this distribution also has a standard deviation: the standard deviation of the
errors. The larger this deviation would be, the worse the performance of the model.
So, the standard deviation of the forecasting error can also be regarded as a perfor￾mance measure of the forecasting model. A good approximation for determining
that standard deviation would be the following (Silver et al. 1998):
𝜎(e(t)) ≈ 1.25 × MAD (5.22)
In literature, the forecast model having a systematic or non-systematic error is also
known as a biased resp. unbiased forecasting model.5.3 Demand and Forecasting 67
0.5
0.4
0.3
0.2
0.1
0
–3 –2 –1 0
z
f (z)
123
Figure 5.13 Likely distribution of non-systematic forecasting errors: Normal distribution.
5.3.2.2 Time Series-Related Forecasting for Demand with a Trend
In case that demand follows a steady increase (or decrease), the demand pattern is
said to follow a “trend.” The basic model of such a demand is already given in the for￾mula (5.14); Figure 5.14 shows the typical demand pattern in time. If we would use
the model of moving average for this demand pattern, we would make a systematic
mistake. This is shown in Figure 5.15 (with N =6).
The reason is that the forecast lags the increase in real demand. After all, all
demands used in the model are historical demands and, thus, by definition,
0
5
10
15
20
25
30
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
a ≈ 7
b ≈ 1
t
D(t)
Figure 5.14 Demand pattern in case of a trend.
t 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
Dt 8 9 11 10 12 13 18 12 14 16 19 20 19 22 24 20 21 25 26 21
N = 6 Ft 10.5 12.2 12.7 13.2 14.2 15.3 16.5 16.7 18.3 20.0 20.7 21.0 21.8 23.0
N = 6: average (et) = –2.93
Figure 5.15 Forecasting error when using moving average in case of demand with trend.68 5 Decoupling Point Control
0
5
10
15
20
25
30
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
D(t)
F(t)
t
D(t)
Figure 5.16 Actual demand versus forecast based on moving average with n =6.
lower than the new demand will be (in case of a positive trend). This is shown in
Figure 5.16.
Corrected Moving Average We need another type of model that includes the idea of
increasing demand. Now, take a closer look at Figure 5.16. There seems to be a
constant systematic error in the size of the average forecasting error. Why not sys￾tematically use that to correct the forecast?
In other words, why not use the following forecasting model:
F′
(t)=(D(t − 1) + D(t − 2)+···+ D(t − n))∕n
e′
(t − 1) = F′
(t − 1) − D(t − 1)
F(t) = F′
(t) − e′
(t − 1) (5.23)
This model uses a three-step-approach:
– first, calculate the simple forecast using the moving average (without correction;
denoted as F′
(t))2;
– then determine the forecast error made in the previous period based on the simple
forecast from that period (denoted as e′
(t −1));
– finally, determine the real forecast F(t), including the correction by subtracting
the error from the simple forecast.
That definitely would improve the quality of the forecast.
Linear Regression Another good estimate is based on a forecasting model using the
“line” shown in Figure 5.16. A good estimate for such a line can be made using single
linear regression, as shown in Figure 5.7. In other words, we need an estimate for
the “a” (the so-called intercept) and “b” (the slope of the red “forecasting” line). It
can be shown that a good estimate for b is:
b = r ⋅
s(D)
s(t) (5.24)
2 The simple forecast is not the real forecast; to indicate the difference between the two an accent
is used for this simple forecast.5.3 Demand and Forecasting 69
where
r: so-called correlation coefficient;
s(D): standard deviation of the demands in the sample;
s(t): standard deviation of the periods in the sample.
The correlation coefficient indicates how strongly the demand correlates with
time. It can be calculated as follows:
r =
∑N
i=1(ti − t) ⋅ (Di − D)
s(t) ⋅ s(D) ⋅ (N − 1) (5.25)
with D and t being the average of the known demands resp. the t’s.
Knowing the b makes it easy to determine a:
a = D − b ⋅ t (5.26)
The correlation coefficient indicates the degree to which the two variables used
(in our case, the “t” and the “D(t)”) correlate. A high correlation (either positive or
negative) means that a change in one will result in a clear change in the other. The
maximum value of “r” is 1; the minimum value is −1. Figure 5.17 shows how this
correlation works.
In Figure 5.18, some illustrations are given on the actual meaning of the correla￾tion coefficient. Figure 5.19 shows the application of these formulas in the example
Strong Weak Strong Weak
–0.5
Positive correlation
–1.0
Negative correlation
0.0 0.5
Figure 5.17 Range of correlation coefficient (shows strength and direction of correlation).
r
2 = 1.000 r
2 = 0.991 r
2 = 0.904
r
2 = 0.821 r
2 = 0.493 r
2 = 0.0526
Figure 5.18 Correlation coefficient for several examples.70 5 Decoupling Point Control
ti 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
Di 8 9 11 10 12 13 18 12 14 16 19 20 19 22 24 20 21 25 26 21
–9.50 –8.50 –7.50 –6.50 –5.50 –4.50 –3.50 –2.50 –1.50 –0.50 0.50 1.50 2.50 3.50 4.50 5.50 6.50 7.50 8.50 9.50
–9 –8 –6 –7 –5 –4 1 –5 –3 –1 2 3 2 5 7 3 4 8 9 4
r = 580/(5.92 ∙ 5.54 ∙ (20–1)) = 0.9307
s(t) = 5.92; 
s(D) So:
s(t)
t = 10.5
D = 17.0 s(D) = 5.54; s(t) . s(D) . (N – 1) 
∑N
i = 1 r = 
b = 0.9307 ∙ (5.54/5.92) = 0.87 b = r .
a = D – b . a = 17.0–(0.87 ∙ 10.5) = 7.84 t 
Ft = 7.84 + 0.87∙t
85.5 68.0 45.0 45.5 27.5 18.0 -3.5 12.5 4.5 0.5 1.0 4.5 5.0 17.5 31.5 16.5 26.0 60.0 76.5 38.0
(t
i – t) . (Di – D)
t
i – t
(t
i – t)(Di – D)
Di – D
Figure 5.19 Example showing linear regression as a forecast for demand with a trend.
given. Linear regression seems to be an adequate forecasting model in case of
demand with a trend. Much better than the models used in the previous chapters.
However, this model also has some serious drawbacks in real-life situations. The
major disadvantages of linear regression are:
– For every new period, a new forecasting model must be created because a new data
element has been added (i.e. the demand from the last period). In other words, all
calculations must be done again.
– All demand data from history must be stored to be capable of using them in the
future.
– The data from far back are given the same weight in the model as the more
recent data.
Double Exponential Smoothing Another model using trends in demand can be used
as well. This model is known as the "double exponential smoothing" model. The
idea of the model is to use exponential smoothing to determine at each new period
the new base level “a” and possibly another exponential smoothing model to deter￾mine the slope “b” of the line. To be more precise, after each period the base level
a(t) is determined using exponential smoothing and the new step in demand b(t)
is determined using also an exponential smoothing model (possibly with different
parameters). This is shown in Figure 5.20. Figure 5.21 shows what these two expo￾nential smoothing models might look like. The method used is the method from
Holt/Winters (Winters 1960). They have checked these models and have proven that
the models work fine. The two basic formulas used for resp. a(t) and b(t) are:
F(a(t)) = 𝛼 ⋅ D(t − 1)+(1 − 𝛼) ⋅ F(t − 1)
F(b(t)) = 𝛽 ⋅ (F(a(t)) − F(a(t − 1))) + (1 − 𝛽) ⋅ F(b(t − 1)) (5.27)
And then:
F(t) = F(a(t)) + F(b(t)) (5.28)5.4 Order Size 71
Figure 5.20 Base level a(t) and step
b(t) are to be forecasted using
exponential smoothing.
Time
D(t)
t
D(t–2)
t–2 t–1
D(t–1) b(t) ?
a(t) ?
F(t)
F(a(t)) 7.00 8.00 9.00 10.10 11.01 12.01 13.00 14.40 15.13 15.92 16.80 17.87 18.96 19.86 20.95 22.16 22.88 23.57 24.54 25.52 0.1 =alpha
F(b(t)) 1.00 1.00 1.00 1.02 1.00 1.00 1.00 1.08 1.01 0.97 0.95 0.97 1.00 0.98 1.00 1.04 0.98 0.92 0.93 0.94 0.2 =beta
F(t) 8.00 9.00 10.00 11.12 12.01 13.00 14.00 15.48 16.14 16.89 17.75 18.84 19.96 20.84 21.95 23.20 23.85 24.49 25.47 26.46
t 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
D(t) 8 9 11 10 12 13 18 12 14 16 19 20 19 22 24 20 21 25 26 21
Assumption!
Figure 5.21 Example using double exponential.
These models are used in the example shown in Figure 5.21 for 𝛼 =0.1 and 𝛽 =0.2.
The assumption is made that Fa(1) =7 and Fb(1) =1 (these are the so-called starting
values to be used), and thus that the first forecast in period 1 was accurate:
F(1) = Fa(1) + Fb(1) = 8.00 (= D(1))
Then the forecast for period 2 becomes:
D(t − 1) = 8; F(t − 1) = 8
So:
Fa(2) = 0.1 × 8+0.9 × 8 = 8.00
Fb(2) = 0.2 × (8.00−7.00)+0.8 × 1.00 = 1.00
and thus: F(2) = 8.00+1.00 = 9.00
The advantages of double exponential smoothing are:
– the amount of data to be stored is much less because you only need to store the
demand and forecasts of the previous period (not more);
– demand from further back gets a much “lower” weight than recent demand.
One big disadvantage is that the model is less easy to understand and explain to
planners who must use the results.
5.4 Order Size
In this section, we will discuss the main issues when considering the size of an order,
assuming the decision has been made to order. In the first subsection, we will discuss
the way to determine the order size assuming that always one order size will be used.
In the next subsection, we will relax some of the assumptions made.72 5 Decoupling Point Control
5.4.1 Optimal Batch Size in Case of Fixed Order Size
Let’s assume that every time an order is released, a fixed order size is used. In other
words:
Qt = Q (5.29)
Assuming Qt has no impact on the value of the part ordered, the costs involved
in the choice of Q can be categorized as order-related costs (extra costs made every
time an order is released; noted as Cord) and inventory-related costs (costs related
to the holding of parts in storage; noted as Chold). Usually, order-related costs
consist of:
– administrative costs: these costs are related to the fact that a new order needs to
be created, requiring human labor and other costs like for printing and/or mailing
the order;
– in the case of an external supplier: costs for shipment that are order-related, not
order size-related, for instance, order-related transportation and administrative
costs charged by the supplier delivering the part ordered;
– in the case of an internal production unit: costs for setting up equipment
(machines) for the part ordered, like the costs for the time spent on setting up the
machine (e.g. variable labor costs) or opportunity costs for non-production (see
also Corbey 1995).
Holding costs usually consist of the following:
– interest cost, related to the price paid for the goods stored (or even to the “cost
of capital” or return on investment of the capital of a company as suggested by
Durlinger 2013 and de Kok 2018);
– costs related to the storage of the goods (warehousing, conditioning, etc.);
– costs related to the risk of decreasing the value of the goods stored, e.g. because of
damage or the goods becoming obsolete.
Assuming no more information is available on the demand in the near future but
the expected average, the stock pattern in time would look as shown in Figure 5.22.
The optimal order quantity then is:
Qopt =
√
2 ⋅ D(yr) ⋅ Cord
Chold
(5.30)
and the total costs are equal to:
TCopt = √2 ⋅ D(yr) ⋅ Cord ⋅ Chold (5.31)
This formula gives the cost-optimal batch size and is known as the economical
order quantity (EOQ) (e.g. Bertrand et al. 1998, originally: Harris 1913).5.4 Order Size 73
Q
Stock
Time
Order cycle
½Q
Order costs: 
• Demand/period: D
• Order size: Q 
• Order cost/order: O
• So: (D/Q)·O
Holding costs:
• Holding cost/unit/period: H
• Average units in store: ½·Q
• So: ½·Q·H
Figure 5.22 Stock pattern in case of fixed Q and regular D.
Derivation: Optimal Q with the EOQ Rule
The average total order-related costs in a year when using Q as order size
would be:
(D(yr)∕Q) ⋅ Cord (5.32)
with D(yr): average demand during a year.
In this formula, D(yr)/Q represents the number of times on average orders are
released. In other words, the average number of (re-)order cycles in a year. For
instance, if demand in a year on average is 100, and Q is 20, on average five
orders would be released in a year.
The average stock level caused by this order size of Q would be Q/2, resulting
in holding costs per year:
(Q∕2) ⋅ Chold (5.33)
with Chold: cost of holding one item in stock for a year.
The average total costs in a year influenced by the choice of Q would then be:
TC = (D(yr)∕Q) ⋅ Cord + (Q∕2) ⋅ Chold (5.34)
To find the Q that minimizes the total costs, we need to take the derivative
of the total costs and look for the value of Q where this derivative equals 0. In
other words:
dTC
dQ = −D(yr) ⋅ Cord
Q2 +
Chold
2 = 0, resulting in
Qopt =
√
2 ⋅ D(yr) ⋅ Cord
Chold
(5.35)74 5 Decoupling Point Control
Remark: The EOQ rule assumes an almost infinite production rate: what we order
will be delivered exactly after L time units, independent of the order size. In
many situations, we have a finite production rate, and we have to take into
account that the time it takes to deliver the order depends on the order size.
In those situations, we have to use the economic production quantity (EPQ) rule.
If we assume that during production, the sale of production units is possible
and that we have a constant production rate P during production (P > D). To find
the optimal production quantity Q per cycle, we can use the total costs per year,
given unit variable costs c, and a certain Q:
TRC(Q) = D
Q
[
Corder + Q ⋅ c + chold
I
max
2
Q
D
]
= D
Q
[
Corder + Q ⋅ c + Chold
Q
2
(
1 − D
P
)]
= Corder ⋅ D
Q + D ⋅ c + Chold
Q
2
(
1 − D
P
)
By taking the derivative, we can find the Q* that gives the minimal costs:
Q∗ =
√√√√√
2CorderD
Chold (
1 − D
P
).
It is interesting to note that in case Qopt is chosen, the order-related costs are exactly
equal to the inventory-related costs. In other words:
(D(yr)∕Qopt) ⋅ Cord = (Qopt∕2) ⋅ Chold = 1/2
√(2 ⋅ D(yr) ⋅ Cord ⋅ Chold) (5.36)
This is illustrated in Figure 5.23 for an example situation.
An important question for real-life situations is how sensitive the total affected
costs (TC) are for the choice of Q. In general, it can be shown that when Q′ = a⋅Qopt
0
500
1000
1500
2000
2500
3000
3500
4000
10 15 20 25 30 35
€
Q
OrderCosts HoldCosts TotCosts
Figure 5.23 Cost curves for D(yr) = 1000, Cord =€30, and Chold =€100.5.4 Order Size 75
is used instead of Qopt, the increase in total affected costs will be (Durlinger 2013):
TC′ = a2 + 1
2a ⋅ TC (5.37)
We will illustrate the sensitivity by using an example.
Example 5.1
Let’s assume that for a particular part, the average demand in a year would be
1200. The order costs Cord are € 25.00 per order; the holding costs Chold are
€ 1.50 per item per year. Then, using the formula (5.30), we conclude that the
cost-optimal order size would be 200 (check that for yourself!). The total affected
costs for that batch size would be € 300.00 (again: check that for yourself by
using Eq. (5.31). What would happen to the total affected costs if we would
increase the order size by 10%? So Q =220 instead of 200. Then – again using
formula (5.31) – the total costs would only increase to € 301 (order costs would
decrease to € 136; holding costs would increase to € 165)!
The main reason for this relatively low sensitivity is the fact that the curve for the
total costs is rather flat near Qopt (see Figure 5.23). That lower sensitivity makes it
possible to choose an order size that is more practicable, e.g. when considering pack￾age sizes or storage room sizes. The idea would be to first determine the cost-optimal
order size Qopt, and then choose a Q based on a practical value nearby Qopt. The low
sensitivity also helps when the cost parameters are not easy to determine. If the mis￾takes made when determining those parameters are limited, the resulting Qopt will
not be a bad choice.
5.4.2 Relaxation of Assumptions
In Section 5.4.1, we (implicity) used several assumptions. These will be discussed in
this section.
5.4.2.1 Known or Predicted Demand Variation
One of the major assumptions made in the determination of the cost-optimal order
size is the fact that we do not know much more about the expected demand than the
average demand. However, sometimes, we do know more than that. Sometimes we
know that demand is not constant over time and are also able to predict the demand
for the near future. In that situation, it might be worthwhile to change the Q ordered
when demand in the near future differs from the average demand (to avoid unnec￾essary remaining stock at the end of an order cycle).
A simple basic approach in this situation is to estimate the optimal number of
periods of demand the order size should cover based on the cost-optimal order size.
That optimal number of periods is called the “economic replenishment interval”;
the logic of ordering according to the demand during a fixed number of periods is76 5 Decoupling Point Control
known as “period order quantity (POQ).” Knowing Qopt means also knowing this
POQopt:
POQopt = Qopt∕D(yr)
=
√
2 ⋅ Cord
D(yr) ⋅ Chold
,rounded to the nearest integer greater than zero
(5.38)
Another heuristic used in practice is the so-called part period balancing (PPB)
method. The basic criterion is to determine the number of periods T covered by the
replenishment such that the total carrying costs are as close as possible to the setup
costs (as is the case when applying the EOQ).
Example 5.2
Part period balancing
Suppose we have Cord =€ 50 and Chold =€ 0.50 and that the demand for the
next 10 periods is as follows:
13, 58, 18, 115, 160, 145, 75, 48, 56, 24
If, at the beginning of the first period, we order for only the first period, the
carrying costs are € 0.
If, at the beginning of the first period, we order for two periods, the carrying
costs are:
58 * 0.50 = € 29, which is smaller than the setup costs (€ 50).
If, at the beginning of the first period, we order for three periods, the carrying
costs are:
29+2 * 18 * 0.50 = € 47, which is smaller than the setup costs (€ 50).
If, at the beginning of the first period, we order for four periods, the carrying
costs are:
47+3 * 115 * 0.50 = € 219.50, which is larger than the setup costs (€ 50).
€ 47 is closer to € 50 than € 219.50; thus, a T value of 3 is used for the first
replenishment of 89 (13+58+18) items.
If we repeat this procedure, we get replenishments of size 89, 275 (€ 80 is
closer to € 50 than € 0), 220, and 128 at the beginning of periods 1, 4, 6, and
8, resp.
This situation of determining the order quantity in case of known varying demand
is also known as the problem of Wagner and Whitin (1958). They developed an algo￾rithm by using dynamic programming that guarantees a cost-optimal solution for
order releases in case of known fluctuating demand. The basic idea of the algorithm
is simple:
– only release orders when the inventory level is zero, and there is a new demand;
– include in the new release the demand of one or more periods in the future such
that the holding costs for items required in any of these future periods do not
exceed the ordering costs for a new release.5.4 Order Size 77
In Silver et al. (1998), the exact details of the algorithm are explained. They also
describe a heuristic algorithm known as the Silver-Meal-algorithm (or least period
cost heuristic) that approaches the results from the Wagner-Whitin algorithm with
less computational effort, using the idea that when reordering the demand of the
next future period should only be included if that results into a lower average affected
costs per period (= (ordering costs plus total holding costs)/number of periods).
The Silver-Meal Algorithm
The property used in the heuristic is that the total relevant costs per time unit
are minimized for the duration of the reorder period (the number of periods of
demand covered by the batch). These relevant costs consist of the ordering costs
and the holding costs of the batch. The assumption is made that the batch size
chosen would always be the sum of the demands during a certain period (and
not more), which makes sense if the demand in a period should always be met
(no backorders allowed). The heuristic calculates the costs per time unit for a
batch size covering the demand over an increasing time horizon T (starting with
T =1) and continues to increase T as long as:
TC(DT)∕T ≥ TC(DT+1)∕(T + 1)
with TC(DT )/T: average total relevant costs per time unit for periods 1 until T
(period 1 being the next period at a decision point), and the total relevant costs
consisting of the ordering costs and the holding costs with DT the demand for T
periods.
This heuristic does not guarantee an optimal solution, because it might stop
at a local minimum (as explained in Silver et al. 1998). But in general, it will
provide a good, if not optimal, solution. The following numerical example shows
the heuristic. Suppose the forecasts for the sales of the book Production Control
in Practice for the next 10 months are as given in Table 5.2 (in units of 100
books).
Table 5.2 Input of the example.
Period 1 2 3 4 5 6 7 8 9 10 Total
Demand 25 30 40 30 20 15 30 55 65 30 360
Ordcosts 45 /Order
Holdcosts 0.5 /Period
Using the algorithm results in: TC(25) = orcosts = 45; TC(55) = ordcost+1
period holdcosts of 30 units = 45+0.5 * 30 = 60; TC(95) = ordcost+1 period
holdcosts of 30 units+2 periods holdcosts of 40 units = 45+15+2 * 0.5
* 40 = 100, etc. See Table 5.3.78 5 Decoupling Point Control
Table 5.3 Results of using the Silver-Meal heuristic for the example with the input as
in Table 5.2.
T 1 2 3 4 5 6 7 8 9 10
Q 25 55 95 125 145 160 190 245 310 340
TC(Q) 45 60 100 145 185 222,5 312,5 505 765 900 min:
TC(Q)/T 45.00 30.00 33.33 36.25 37.00 37.08 44.64 63.13 85.00 90.00 30.00
Stop? 0 0 1 1 1 1 1 1 1 1
T 12345678
Q 40 70 90 105 135 190 255 285
TC(Q) 45 60 80 102,5 162,5 300 495 600 min:
TC(Q)/T 45.00 30.00 26.67 25.63 32.50 50.00 70.71 75.00 25.63
Stop? 0 0 0 0 1 1 1 1
T 1234
Q 30 85 150 180
TC(Q) 45 72.5 137.5 182.5 min:
TC(Q)/T 45.00 36.25 45.83 45.63 36.25
Stop? 0 0 1 1
In the example shown in the table, the batch size of the first release would be
55, covering the demand in the first two periods. The next release is in period
6 with a batch size of 125, covering the demand in periods 3, 4, 5, and 6. Note
that the heuristic stops as soon as the TC/T increases with an increase in T.
In Appendix 5.A, we give a description of the Wagner-Whitin algorithm and the
solution this gives for the example just used. The Silver-Meal heuristic gives, in this
example, the same order sizes with the same costs as the Wagner-Whitin algorithm
(optimal solution).
In Bertrand et al. (1998), it is argued that often, in these types of situations, the
simpler solutions using a standard order cycle do not perform much worse than
the more complex algorithm-based approaches. The conclusion is drawn that for
single-product models, even though not all assumptions might be true in a particular
situation, the relatively simple EOQ model might still be used as a good approxima￾tion of the optimal solution. The main reason is, again, the relatively limited impact
of deviations from Qopt on the total costs.
5.4.2.2 Quantity Discount
Note that in TC = (D(yr)/Q)⋅Cord +(Q/2)⋅Chold, the total costs of the items
ordered in the period considered are not mentioned at all. These total item costs5.4 Order Size 79
clearly would be:
D(yr) ⋅ Cval (5.39)
with Cval: cost price (“value”) of one item.
The reason is that the assumption is made that these costs are not affected by the
choice of Q! These costs would be affected if the supplier would give a discount if a
certain Q is ordered (like if you order 100 parts or more than you will get a discount
of 5% on the item costs).
Assuming i different possible prices, i.e. depending on the quantity ordered a price
of Cval,i
, the approach for this situation would be:
– determine for each possible price the Qopt,i using
Qopt =
√
2 ⋅ D(yr) ⋅ Cord
Chold
(5.40)
– determine the range for Q in which a particular price i is valid (i.e. determine Qmin,i
and Qmax,i
);
– choose for each possible price i the optimal valid QVopt,i
, i.e.:
– if Qopt,i > Qmin,i and Qopt,i < Qmax,i then choose QVopt,i = Qopt,i
;
– if Qopt,i ≤ Qmin,i then choose QVopt,i = Qmin,i
;
– if Qopt,i ≥ Qmax,i then choose QVopt,i = Qmax,i
;
– in the end, choose the QVopt,i for which the following total costs are minimal:
TCi = D(yr) ⋅ Cval,i + (D(yr)∕QVopt,i
) ⋅ Cord + (QVopt,i
∕2) ⋅ Chold (5.41)
Note that if the holding costs (Chold) are dependent on the value of the part con￾sidered (often the assumption is made that Chold = ihold Cval with ihold being an inter￾est rate to determine the holding costs), then a different Cval,i will also result in a
different Chold,i
.
5.4.2.3 Minimum Order Quantity
Sometimes, the supplier of an item requires a minimum order quantity (MOQ), i.e.
only orders that have at least the quantity specified as MOQ are accepted. If that is
the case, we can use a similar reasoning as shown above:
– if Qopt,I > Qmin,I then choose QVopt,i = Qopt,I;
– if Qopt,I ≤ Qmin,I then choose QVopt,i = Qmin,I;
In other words, in this situation, the formula for Qopt can still be used, but an
additional check is needed to determine QVopt.
5.4.2.4 No Variable Order-Related Costs
In some situations, there are no real variable order-related costs. For instance, in case
the order quantity refers to a production batch size and the set-up activities for that
batch are to be done by operators with a fixed contract. Then, the costs related to the
set-up time required are fixed, not variable, if the set-up activities can be carried out
in regular production time. In such a case, no extra costs are being made in case of a80 5 Decoupling Point Control
new set-up. That would mean that the order-related costs (Cord) are 0, and thus, Qopt
would be “as small as possible,” i.e. 1.
This is only true if the order-related activities indeed can be done in regular
production time without influencing the other activities of the operators. In other
words, increasing the number of changeovers on machines doesn’t mean that some
other work of the operators cannot be done anymore in regular time. After all,
if that would be the case, especially if the operators are not able to produce the
same number of products in regular time, then the set-up costs cannot be ignored
anymore: the costs would be either costs related to, for instance, work overtime or
to lost opportunities for sales (lost margins as “opportunity costs”).
One way of achieving low order-related costs in production would be to focus on
the reduction of set-up times. That is the approach followed by Lean-production
initiatives (see e.g. Theissens 2016).
5.4.2.5 Interdependencies of Order Sizes – Not BOM Related
In the previous section, we have determined Qopt for one specific product, assum￾ing no relationship with the Q’s for other products. That is not always realistic. For
instance, the combination of Q’s chosen for different products might lead to prob￾lems in the warehouse because of a lack of space. Or the combination of Q’s might
result in a violation of an overall budget constraint on the total inventory. In general,
the following restriction might be relevant (Durlinger 2013):
∑N
i=1
mi ⋅ Qi ≤ M (5.42)
with:
M some maximum (e.g. in space or budget)
mi the “contribution” of each product i to that relevant characteristic.
This problem can be solved by using the Lagrange multiplier approach (see Silver
et al. 1998). However, in many cases, the easier approach to multiply the calculated
Qopt with mi
/M does not give a bad result either, especially not if mi
/Chold,i is more
or less equal for all i (see Durlinger 2013, based on Nahmias 1989). In case M rep￾resents a common batch size that should be ordered, as in a transportation batch,
for instance, in case of container transport or full truckloads, a good approxima￾tion of the optimal solution would be to divide that batch size over all items at the
moment of replenishment such as to maximize the time to the next replenishment.
This means that M should be divided over the individual items such that the result￾ing available stocks for each item should cover the same expected period of demand
(Silver et al. 1998). Table 5.4 illustrates a numerical example showing this principle.
The example contains four products (a, b, c, and d), and the available stock at the
moment of ordering is given in the row “I start.” The assumption is that the table
shows the inventory positions at the moment of ordering, with a total of 188. The
batch order size is 900 (that is the M), so the decision to be made is how to divide
that 900 over the four products. What is known is that the sum of the inventories after
ordering will be 188+900 = 1088. The idea would be to divide that total inventory5.4 Order Size 81
Table 5.4 Example situation with a maximum batch Q (propD is the proportion of the
total demand).
Product abcd Tot Batch Q
I start 11 45 57 75 188 900
I end 218 435 326 109 1088
propD 0.2 0.4 0.3 0.1
Q 207 390 269 34 900
over the four products in such a way that all inventories would cover the same period
of demand. That is done by dividing the total inventory according to the proportions
of demand for the individual products (shown in the row “propD”). The difference
between Iend and Istart for each product then gives the amount to be ordered. For
product a the end stock should be 0.2 × 1088 = 218, so the amount to be ordered is
218−11 = 207.
5.4.2.6 Interdependencies of Order Sizes – BOM Related
Frequently, the item considered is part of a higher-level item in the Bill of Material
(also known as a “child” – “parent” – relationship). In that case, it would be con￾venient to make sure that the Qc chosen for child-part c is a multiple of the Qp of
parent p, which it goes into. In other words:
Qc = n ⋅ Qp (5.43)
with n =1, 2, …
If we choose a Qc in that way, it never happens that items are “leftover,” i.e. that
even though some items c are still available, they cannot be used because their num￾ber is too low to be used for one batch of p. For instance, if Qp =5 and Qc =7, then
from one batch of c two items will be left over after one batch of p was being pro￾duced. These two items cannot be used to produce a new batch of p; we will have to
wait until a new batch of c has arrived.
Several approaches can be followed to determine Qp and Qc. A simple approach is
the following one. If a parent item only has one child item, then a practical approach
might be to consider both the EOQ of the parent and the EOQ of the child and
choose a convenient value for Qp and Qc close to their individual Qopt while fulfill￾ing equation Qc = n⋅Qp. If the values chosen are not “far” from their individual Qopt,
then the total costs will still be near-optimal due to the flatness of the total costs’
curves near the optimum.
The more advanced approaches use the logic of the resulting inventory levels for
child and parent, as shown in Figure 5.24. The Echelon stock shown in the figure
is the total number of child items in child stock plus the number of child items in
parent items that are not sold yet.
In Appendix 5.B, a numerical example of the possible consequences of advanced
and optimal approaches can be found.82 5 Decoupling Point Control
Qp Qc
Qc / D
Qp / D
Time
Inventory
Average inventory
– Components:
(n–1)/2 × Qp
– Product:
Qp/2
– Echelon:
Qc/2 = nQp/2
Echelon
stock
Figure 5.24 Inventory levels for child item if Qc = n⋅Qp.
(1) Advanced approach
A more advanced approach would be to optimize the choice of “n” once the
“local” EOQ for the parent has been determined. In other words, use the original
EOQ approach to determine Qp, and then choose “n” in such a way that the total
costs for the child item are minimized. As is shown in Figure 5.24, the average
inventory level for the child item in this situation would be (n−1)⋅Qp/2. So:
TCc =
( D
n ⋅ Qp
)
⋅ Cord,c +
(
(n − 1) ⋅
Qp
2
)
⋅ Chold,c (5.44)
The optimal “n” can be determined by calculating d(TCc)/d(n) = 0, using the
EOQ for Qp. This gives:
nopt =
√Cord,c
Cord,p
⋅
Chold,p
Chold,c
(5.45)
Very likely, in most cases, nopt will not be an integer. But we do need to find
an integer “n” (see Eq. 5.43). Two options can be considered in that situation:
– the largest integer [nopt]< nopt;
– the integer [nopt]+1.
Clearly, if nopt <1 then n =1 should be chosen. But if nopt would be equal
to, for instance, 4.55, then we should consider n =4 and n =5. For both possi￾ble integers, the total local costs for the child item can be calculated using the
Eq. (5.44)
The final choice would be the “n” where these costs would be the lowest.
(2) Optimal approach
In Silver et al. (1998) a procedure is described to determine the exact opti￾mal Qp and Qc for such a situation. The basis for that procedure is the total5.4 Order Size 83
costs involved:
TC(n, Qp)=(D∕Qp) ⋅ (cord,p + (cord,c∕n))
+ (Qp∕2) ⋅ ((n − 1) ⋅ chold,c + chold,p) (5.46)
This total cost equation covers the entire costs over the supply chain con￾sidered, i.e. both the cost of ordering and keeping inventory of the parent item
and the inventory and ordering costs for the child item. In other words, instead
of sub-optimizing first the Q for the parent item and then relating the batch
size for the child item to that locally optimized Qp (as shown in the “advanced
approach”), all costs are considered together to find the optimal choice for Qp
and n, assuming a relation between Qc and Qp as given in Eq. (5.43).
Again, the first step to take is to determine nopt as shown in Eq. (5.22). Like
the procedure described in the advanced approach, the two integers nearest to
nopt should be determined ([nopt] and [nopt]+1). For each of the two possible
integers (n =[nopt] and n =[nopt]+1): calculate:
Qp =
√√√√√
2
(
Cord,p +
Cord,c
n
)
D
(n − 1)Chold,c + Chold,p
(5.47)
Then, choose the n with the lowest total costs according to the equation
TC(n, Qp)=(D∕Qp) ⋅ (cord,p + (cord,c∕n))
+ (Qp∕2) ⋅ ((n − 1) ⋅ chold,c + chold,p)
5.4.3 Single Period Problem
In some situations, we only need to order for one single period, and each new period,
we again need to make a new choice. At the end of the period, any leftovers are use￾less (or can only be used at some expense); if we order less than the actual demand,
we will lose possible profit. This might be the situation for style goods or perishable
products (newspapers, magazines, fresh products, etc.). In these situations, demand
is uncertain, but we may know the probability distribution of that demand (e.g. based
on historical data). The question is how much to order to obtain the highest profit,
given shortage costs cu in case we can’t deliver (lost sales, demand is higher than the
available stock) and leftover costs co, in case demand is less than the available stock.
This is known in the literature as the newsvendor problem.
In Appendix 5.C, we show that the optimal Q (assuming a discrete demand) is the
smallest Q that meets the following equation:
P(D ≤ Q) ≥
cu
cu + co
(5.48)
with
cu: shortage costs (= price − variable costs+possible extra penalty)
co: leftover costs (= variable costs − possible remaining revenue)84 5 Decoupling Point Control
Example 5.3
In Table 5.5, an example is given. In this example:
– the demand per period follows the distribution given in the columns “D,”
“prob,” and “cum.prob” (respectively the demand value, the probability of
that value, and the cumulative probability);
– the values for the price of the product when sold is “p original”; the price that
can be obtained at the end of the period for the “leftovers,” is “p remain”;
the variable costs per product are “v”;
– for each possible Q (ranging from 10 to 80), the expected number of “short￾ages” and “leftovers” is determined, using the probability distribution of the
demands (see E(under) and E(over));
– for each Q, based on the expected number of “shortages” and “leftovers,” the
total costs are determined, using the cu and co.
In the example, porig = 15, premain = 3, and v = 10.
So cu = 15 − 10 = 5 and co = 10 − 3 = 7.
Then the criterion to use becomes:
p(D ≤ Q) ≥ 5∕(5 + 7) = 0.42
The first Q where this criterion holds is Q = 30, which indeed gives the lowest
total costs (57.5+31.5 = 89).
Appendix 5.A The Wagner-Whitin Algorithm
The Wagner-Whitin algorithm for determining batch sizes is guaranteed optimal for
the following situation:
● Demand in the (near) future is known (for a finite horizon of N-periods).
● As with MRP, the requirements for a certain period should be available at the
beginning of the period.
● The item costs are not affected by the batch size (no quantity discounts).
● There are no benefits to considering batch sizes of several items together.
● Delivery times are known.
● No back ordering allowed.
● Batches are delivered at once (not split up).
The algorithm is based on dynamic programming, following a procedure for solv￾ing subsequent mathematical equations. The algorithm considers batch sizes based
on the demand for just one period, or the demand for two periods, or for three
periods, etc. The main property used in the algorithm is that there is an upper limit
for the batch size since, as the holding costs for demand in a period are getting higher
than the ordering costs for a new batch, it would be wise to start a new batch. The
algorithm is explained in more detail by Silver et al. (1998).Table 5.5 Example newsvendor problem.
Q 10 20 30 40 50 60 70 80
p original 15 D Prob. cum.prob
E
(under)
E
(over)
E
(under)
E
(over)
E
(under)
E
(over)
E
(under)
E
(over)
E
(under)
E
(over)
E
(under)
E
(over)
E
(under)
E
(over)
E
(under)
E
(over)
V 10 10 0.1 0.10 0 0 0 10 0 20 0 30 0 40 0 50 0 60 0 70
p remain 3 20 0.25 0.35 10 0 0 0 0 10 0 20 0 30 0 40 0 50 0 60
B 0 30 0.15 0.50 20 0 10 0 0 0 0 10 0 20 0 30 0 40 0 50
40 0.2 0.70 30 0 20 0 10 0 0 0 0 10 0 20 0 30 0 40
50 0.1 0.80 40 0 30 0 20 0 10 0 0 0 0 10 0 20 0 30
60 0.1 0.90 50 0 40 0 30 0 20 0 10 0 0 0 0 10 0 20
70 0.05 0.95 60 0 50 0 40 0 30 0 20 0 10 0 0 0 0 10
80 0.05 1.00 70 0 60 0 50 0 40 0 30 0 20 0 10 0 0 0
cu 5 Exp. 27 0 18 1 11.5 4.5 6.5 9.5 3.5 16.5 1.5 24.5 0.5 33.5 0 43
co 7 cost 135 0 90 7 57.5 31.5 32.5 66.5 17.5 115.5 7.5 171.5 2.5 234.5 0 301
crit 0.42 p(x ≤ Q) 0.10 0.35 0.50 0.70 0.80 0.90 0.95 1.0086 5 Decoupling Point Control
Table 5.A.1 Input data for the example.
Period P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 Total
Demand 25 30 40 30 20 15 30 55 65 30 360
Ordcosts 45 /Order
Holdcosts 0.5 /Period
The algorithm is hardly used in practice, probably because of its complex nature
(making it difficult for practitioners to understand) and the fact that proposed batch
sizes all change in the future if the projected demands change, making it difficult
to apply, for instance, MRP-based schedules. We will give a short description of
the heuristic using the following example: consider the problem as used in the
Silver-Meal algorithm in Section 5.4.2 with the forecasts for the demand for the
next 10 periods (units of 100s) as given in Table 5.A.1.
Let us define F(t) as the total costs of the best replenishment strategy for the
demand in periods 1, 2, …, t.
F(1) is the total cost of replenishment of size 25 at the beginning of period 1, thus
simply the ordcosts 45.
For F(2), we need to consider two options:
Option 1, O1: replenish 25 at the start of period 1 and 30 at the start of
period 2, Costs = costs of best strategy from the start to the end of period
1+costs of replenishment at the start of period 2 for the demand in period
2 = F(1) +ordcosts = 45+45 = 90.
Option 2, O2: replenish at the start of period 1 enough to cover the demand for period
1 and period 2. Costs = set up costs for period 1 replenishment (for period 1 and
period 20)+carrying costs for the demand in period 2) = 45+0.5 * 30 = 60.
For F(3), we have three options:
Option, O1: Cover the demand to the end of period 2 (25+30) with the
best possible strategy and replenish 60 at the start of period 3. Costs:
F(2)+ordcosts = 60+45 = 105.
Option 2, O2: Cover the demand to the end of period 1 with the best possible strategy
and replenish 90 at the start of period 2. Costs: F(1) +ordcosts+1 month carrying
costs for the replenishment for period 3 = 45+45+1 * 0.5 * 60 = 120.
Option 3, O3: Cover the demand to the end of period 3 at the start of period 1. Costs:
ordcosts+1 month carrying costs for the demand in period 2+2 month carrying
costs for the demand in period 3 = 45+1 * 0.5 * 30+2 * 0.5 * 60 = 90.
Continuing in this way, we can construct Table 5.A.2:
Optimal first order size: 25+30 = 55 (for the first two periods)
Optimal order strategy: 55 (period 1); 105 (period 3); 85 (period 7); 95 (period 9)5.C Newsvendor Problem 87
Table 5.A.2 Replenishment costs for the different strategies.
P1 P2 P3 P4 P5 P6 P7 P8 P9 P10
O1 45 90 105 145 165 185 207.5 245 280 322.5
O2 60 110 120 155 172.5 200 235 277.5 295
O3 100 140 140 170 202.5 255 300 307.5
O4 145 170 162.5 215 285 335.5 345
O5 170 200 222.5 325 415 412.5
O6 222 275 360 487.5 490
O7 312.5 440 555 577.5
O8 505 667.5 660
O9 784 787.5
O10 900
Appendix 5.B Example Impact Advanced and Optimal
Approach for Determining Batch Sizes
Figure 5.B.1 shows a numerical example of the possible consequences of advanced
and optimal approaches. The first part of the table shown in this figure shows the
calculation of Qp and Qc if no relation between this batch size is considered. Please
note that the calculated total costs in that situation are incorrect since the calculation
used doesn’t take into account the fact that there is an issue of “left-overs” because
Qc does not meet Qc = n⋅Qp.
In the second part of the table, the advanced approach is used, and the third part of
the table shows the optimal approach. If the parent item has more than just one child
item, the issue becomes more complicated. Assuming two different child items are
needed to produce the parent item (see Figure 5.B.2), then it would be wise to have a
relation not only between the Q’s from each child item with the parent item but also
between these two child items. If Qc1, on top of being a multiple of Qp, would not
be a multiple of Qc2 (or vice versa), then again, some items would be “leftover.” For
instance: if Qp =5, Qc1 =10, and Qc2 =15, then the extra five items in Qc2 compared
to Qc1 can only be used after a new batch of c1 has arrived. Of course, that situation
would be worse if Qc1 or Qc2 (or both) would not be a multiple of Qp as well.
Appendix 5.C Newsvendor Problem
In some situations, we only need to order for one single period, for instance, if we
have style goods or perishable products (newspapers, magazines, fresh products,
etc.). Often, there is just one production unit, and we have to deal with a relatively
short selling season and an interaction of service and cost-related parameters, often
in a situation with uncertain demand. Demand (X) is uncertain; however, we assume88 5 Decoupling Point Control
Child Parent
Cord 20 10
Chold 3 5
D 100 100
Independent Qc Qp TCp TCc TC
EOQ 37 20 100 110 210
37
Advanced n opt
1.8257
42 TCp TCc TC
approach [n opt] 1 100 100 200
[n opt] + 1 2 100 80 180
[n opt] + 2 3 100 93 193
[n opt] + 3 4 100 115 215
Optimal n opt
1.8257
42 Qc Qp TC
approach [n opt] 1 35 35 173
[n opt] + 1 2 44 22 179
[n opt] + 2 3 51 17 192
[n opt] + 3 4 60 15 205
Figure 5.B.1 Example impact advanced and optimal approach for Qc and Qp.
Parent p
Child c1 Child c2
1× 1×
Figure 5.B.2 BOM for the example.
that we have a forecast of the distribution. The question is how much to order, or
to produce, to obtain the highest profit, given underage costs cu in case we can’t
deliver (lost sales, demand is higher than the available stock) and overage costs co,
in case demand is less than the available stock (salvage costs). This is comparable
to a newsvendor who has to decide how many papers to order to minimize the
sum of lost sales and salvage costs. This is, therefore, also called the Newsvendor
Problem.
If the penalty for not satisfying demand is B, we have for the underage costs:
cu = p − v + B5.C Newsvendor Problem 89
With
p: revenue per sale
v: unit variable costs (v < P)
B: penalty costs
If the costs for having more in stock or g per unit not sold (for instance, giving a
discount in case we have leftovers), then we have for the overage costs:
co = v − g
With g: salvage costs (g < v)
To determine the optimal order quantity, we need to consider the total (expected)
costs:
Total expected costs = (expected) underage costs + (expected) overage costs
Suppose that P(X) is the probability that demand is X and F(X) is the probability
that demand is less than or equal to X.
If demand is continuous, we have:
Total expected costs = co ∫ Q
0 (Q − x)f(x)dx + cu ∫ ∞
Q (x − Q)f(x)dx
Taking the first and second derivatives, we get:
dE[C(Q)]
dQ = co ∫
Q
0
f(x)dx − cu ∫
∞
Q
f(x)dx
= coP(X ≤ Q) − cu(1 − P(X ≤ Q))
= coF(Q) − cu(1 − F(Q))
d2
E[C(Q)]
d2
(Q)
= (co + cu)f(Q)
From this, it follows that the total expected costs function is convex and that the
optimal value of Q follows from
coF(Q) − cu(1 − F(Q)) = 0
and thus the optimal value for Q, Q* is such that
F(Q∗) =
cu
cu + co
(5.C.1)
If X follows a Normal distribution with mean 𝜇 and variance 𝜎X
2 then
F(Q) = P(X ≤ Q = P
(X − 𝜇
𝜎X
≤ Q − 𝜇
𝜎X
)
= 𝛷
(Q − 𝜇
𝜎X
)
where 𝛷(x) is the cdf of the standard normal random variable N(0,1).
For Q* we then get 𝛷−1
( cu
cu+co
)
= Q∗−𝜇
𝜎X
thus Q∗ = 𝜇 + 𝛷−1
( cu
cu+co
)
𝜎X90 5 Decoupling Point Control
Example 5.C.4
A newsvendor boy has to decide on how many newspapers to purchase, knowing
that the sales price is 95 cents, the purchase price is 35 cents, the disposal price
of unsold copies is 15 cents, and the average daily sales is 12.15 with a standard
deviation of 4.90.
From the data it follows dat cu = p – v +B = 95 – 35+0 = 60, and
co = v – g = 35 – 15 = 20 which gives that for Q* must hold:
F(Q∗
) = 60
60 + 20 = 0.75
If we assume that the demand can be approximated by a normal distribution,
then
Q∗ = 12.15 + 𝛷−1(0.75)
∗ 4.90 = 12.15 + 0.68∗ 4.90 = 15.48
In case we have discrete demand and we order Q, we then have:
(Q < X): Expected underage costs = cu {1 ⋅ P(X = Q +1)+2 ⋅ P(X = Q +2) +…} =
cu
∑∞
i=Q+1(i − Q) ⋅ P(X = i)
(Q ≥ X): Expected overage costs = co {Q ⋅ P(X =0)+(Q −1) ⋅ P(X =1)+…1 ⋅ P(X =
Q −1)} = co
∑Q
i=0(Q − i) ⋅ P(X = i)
Total expected costs = cu
∑∞
i=Q+1(i − Q) ⋅ P(X = i) + co
∑Q
i=0(Q − i) ⋅ P(X = i)
We can find the optimal value of Q by calculating the total expected costs for a large
range of Q-values. The Q that gives the lowest total expected costs is then the optimal
Q. A more practical way of determining Q* is by using (5.C.1). This condition may
not be satisfied at equality (since we have jumps due to the discrete distribution).
The optimal Q is then the smallest value of Q that satisfies the inequality
P(X ≤ Q) = F(Q) ≥
cu
cu + co
References
Bertrand, J.W.M., Wortmann, J.C., and Wijngaard, J. (1998). Produktiebeheersing en
Material Management, 2ee. Houten: Educatieve Partners Nederland BV (in Dutch).
Corbey, M.R. (1995). Logistiek Management & Management Accounting: Logistieke
Flexibiliteit in Bedrijfseconomisch Perspectief. PhD thesis. Eindhoven: Eindhoven
University of Technology (in Dutch).
Diks, E.B. (1997). Controlling Divergent Multi-Echelon Systems. PhD thesis. Eindhoven:
Eindhoven University of Technology.
van Donselaar, K.H. and Broekmeulen, R.A.C.M. (2019). Stochastic Inventory Models for
a Single Item at a Single Location. Beta Working Paper Series 447. Eindhoven: Beta
Research School for Operations Management and Logistics.
Durlinger, P. (2013). Productie en Voorraadbeheer – H2: Voorraadbeheer. White Paper.
Durlinger Consultancy (in Dutch).References 91
Harris, F.W. (1913). How many parts to make at once. Factory, The Magazine of
Management 10: 135–136. 152.
van Houtum, G.J. and Zijm, W.H.M. (2000). On the relationship between cost and
service models for general inventory systems. Statistica Neerlandica 54 (2): 113–264.
Jacobs, F.R. and Chase, R.B. (2018). Operations and Supply Chain Management, 15ee.
McGraw Hill Education International Edition.
King, P.L. (2011). Crack the Code – Understanding Safety Stock and Mastering its
Equations. APICS Magazine July/August 2011.
de Kok, T. (2018). Inventory management: modeling real-life supply chains and
empirical validity. Foundations and TrendsR in Technology, Information and
Operations Management 11 (4): 343–437.
Muntslag, D.R. (1993). Managing Customer Order Driven Engineering: An
Interdisciplinary and Design Oriented Approach. PhD thesis. Eindhoven University of
Technology.
Nahmias, S. (1989). Production and Operation Analysis. Homewood: Irwin.
Silver, E.A., Pyke, D.F., and Peterson, R. (1998). Inventory Management and Production
Planning and Scheduling. New York: John Wiley & Sons.
Theissens (2016). Lean Six Sigma Black Belt: Mindset, Skill Set and Tool Set, 2ee.
LSSA B.V.
Verwijmeren, M.A.A.P., van der Vlist, P., and van Donselaar, K.H. (1996). Networked
inventory management systems: materializing supply chain management.
International Journal of Physical Distribution and Logistics Management 26 (6): 16–31.
Wagner, H.M. and Whitin, T.M. (1958). Dynamic version of the economic lot size
model. Management Science 5: 89–96.
Winters, P.R. (1960). Forecasting sales by exponentially weighted moving averages.
Management Science 6 (3): 324–342.93
6
Reorder Point Decoupling Point Control Systems
In this chapter, we will assume that stock is kept at a decoupling point just in case
demand may rise that should be fulfilled without any delay. In other words, the
delivery time promised to the customer of the items from this decoupling point is
0 since the items are assumed to be available continuously (or at least when demand
rises). We will only consider decoupling point control systems for situations where
the demand is probabilistic. If demand is deterministic, keeping stock “just in case”
makes no sense. The decoupling point control systems (where we only look at the
local stock position) in case of probabilistic demand are usually called reorder point
systems.1 We start in Section 6.1 with a general discussion of these kinds of systems
followed in Sections 6.2 and 6.3 with a discussion of when to order, respectively, how
much to order.
6.1 General Discussion of Reorder Point Systems
In these kinds of systems, there is no coordination of the various stock points (decou￾pling points), and demand at a decoupling point is independent of the demand at the
other decoupling points. The focus is on the planning and control of the inventory
at this stock point as if it is a single stock point, i.e. without looking at possible other
stock points upstream or downstream of the chain of units. Figure 6.1 illustrates this
situation. At a certain moment in time, a new replenishment order is created based
on the stock level and the norms used. The stock level required in this decoupling
point depends both on the demand expected (and the uncertainty in the demand)
and on the delivery time of the supplier of the item (also including the uncertainty
in that supply). Therefore, the basic variables to be considered in this situation are
as follows:
Qt: the size of the replenishment order at moment t.
Lt: lead time (or delivery time) of the supplier at moment t.
D(Lt): demand during the lead time at moment t.
1 Reorder point systems are also known as statistical inventory control (SIC) systems.
Production Control in Practice: A Situation-Dependent Decisions Approach, First Edition.
Henny Van Ooijen and Corné Dirne.
© 2024 WILEY-VCH GmbH. Published 2024 by WILEY-VCH GmbH.94 6 Reorder Point Decoupling Point Control Systems
Qt Ok?
D(Lt
)
Lt
Norm level
Delivery time
supplier
Demand
Figure 6.1 Basic “on stock” situation.
Order size
Stock
Time t
Planned lead time Lt
Time
E(D(Lt
))
Lt Qt
t + Lt t
Qt
Figure 6.2 Basic situation when reordering.
If D(Lt) and Lt are known beforehand (i.e. we have a complete deterministic situa￾tion with no uncertainties), then both the moment of reordering and the amount to
reorder are not difficult (see Figure 6.2): we would order exactly Lt time units before
the stock level would hit the 0 level, and we would order exactly D(Lt) (or more if
that would be wise from a cost perspective). In other words, Qt ≥ D(Lt). In fact, in
such a situation, we would not require an “on stock” policy, but we can use an “on
order” policy. If Qt = D(Lt), we would even have a perfect “just in time” situation.
The extra stock in case Qt > D(Lt) is not based on the policy to have stock avail￾able “just in case” a demand might pop up; it’s merely based on cost considerations.
Unfortunately in most situations, we do not know exactly the future demand during
the lead time of the replenishment order. We might not even be sure about the exact
lead time of that replenishment order! In that case, instead of using D(Lt), we need
to “compromise” and use the expected demand during lead time. Similarly, we may
need to use the expected lead time for replenishment. This will be written as follows:
E(D(Lt)): expected demand during the lead time at moment t.
E(Lt): expected lead time at moment t.
If the distribution of demand is stable in time (no trend or season), then it makes
sense to use the historical average of the demand to determine the expected demand
during lead time. In case of a trend or a seasonal demand, the expected demand
needs to be adapted to the situation at the moment of reordering. This also holds for
the lead times, although predicting the actual lead times of suppliers might even be6.1 General Discussion of Reorder Point Systems 95
Figure 6.3 Physical stock,
available stock, and inventory
position.
Assigned,
not delivered yet
In store,
not assigned
Ordered,
not yet in store
Technical
Inventory position 
Relevant for
inventory
control!
Pipeline
Available
more difficult because those lead times might be dependent on the actual situation
at the supplier which usually is not known.
When planning and controlling inventory in a stock point, it is important to make a
distinction between the technical stock, the available stock, and the inventory position
(see Figure 6.3).
Technical stock (Iphys) is the stock that is physically present at the location (usually
stored in the warehouse). You can see and touch it. Part of what is physically stored
may already be promised to customers or specific orders, so basically is not available
anymore for new customers or orders. It is stock that is already assigned and thus
not available anymore. That part of the physical inventory should not be taken into
consideration anymore when deciding whether to order new supplies or not. The
part of the physical stock that is free to cover new demand is called available stock
(Iavail). Items that already have been ordered before and are on their way, but have not
arrived yet, are not part of the physical stock but should be taken into consideration
when reordering. If we would neglect these items (in the pipeline), we might end
up ordering the same amount twice. The items in the pipeline together with the
available stock are called the inventory position (I). When considering ordering, the
inventory position2 is relevant, not the technical stock.
Available stock can become negative when customer orders are not fulfilled
right away. In such a case backorders may be created, i.e. items to be delivered as
soon as the unassigned physical stock becomes positive again. In this chapter, the
assumption will be made that these orders indeed will be delivered later (so are
“back-ordered”), and not that these orders will be lost (known as “lost sales”). We
refer to van Donselaar and Broekmeulen (2019) for discussions on the lost-sales
situation.
In general, we can say that in an “on stock” situation to meet a certain perfor￾mance requirement (like P1, see previous chapter), a certain number of items needs
to be in store to cover for demand during the lead time of new supplies. In an “on
stock” situation, if we wait to order until the available stock equals 0, we will not be
2 In van Donselaar and Broekmeulen (2019), the available stock is called inventory on hand, while
they refer to the economical stock as the inventory position. In de Kok (2018) the available stock is
called net stock.96 6 Reorder Point Decoupling Point Control Systems
Periodic review Continuous review
Fixed order quantity (or multiple) (R, s, Q) (s,Q)
Order up-to-Level (R, s, S) (s, S)
Figure 6.4 Classification of inventory control systems.
able to meet the possible demand at all during the time required by the supplier to
supply new items, i.e. the lead time of the supplier (see again Figure 6.2). Therefore,
a certain reorder level s (also known as reorder3 point) is required which of course
must be based on the demand during the lead time of the supplier. Crossing that level
triggers the initiation of an order. Systems using such a concept are called Reorder
point (ROP) Systems. In Silver et al. (1998) four different ROP systems have been
described to cope with the situation described above. These four systems are based
on two classification criteria, i.e.:
– Reviewing the stock level is done continuously or in review periods.
If a review period of R is used, then every R time units the stock level is checked
(like once a day or once a week); if a computer system checks the stock levels, then
basically stock levels are monitored continuously (we can say: R ≈0).
– The number of items ordered is either a fixed quantity Q or is large enough to fill
the stock to a fixed maximum level (the “order-up-to-level” S).
Based on these two classification criteria, we have four basic inventory control sys￾tems (see Figure 6.4). In all four cases, a reorder level s is used to determine whether
reordering would be required or not. If at the moment of considering reordering (i.e.
the moment of reviewing the stock level), the inventory position is below the reorder
level s, then a new order is generated. The size of the order should be large enough
to make the inventory position stock rise above the reorder level again. In case a
maximum stock level is used, that will always be the case if S > s. In case of a fixed
order quantity and lumpy demand, and Q not being very large, one order of size Q
may not be large enough to raise the stock level above the reorder level again, so a
multiple of Q should be reordered (denoted as “nQ”).
6.2 When to Order?
As we have seen in Section 6.1, there are two different policies for deciding to order:
(a) a policy where stock is monitored continuously and as soon as the stock level is
too low an order is placed.
(b) a policy where periodically stock is monitored and if at such a point the stock
level is too low an order is placed.
3 The term “re-order” refers to the fact that obviously in the past at least one order has been sent
out to the supplier to create the initial stock, and now possibly a reorder might be needed to
“re-fill” that stock.6.2 When to Order? 97
Qt
Stock
Time
s
Lt
t
Figure 6.5 Basic logic (s, Q) system.
6.2.1 Continuous Review
The simplest system that uses continuous monitoring of the stock level is the (s, Q)
system. Figure 6.5 gives the basic logic of the (s, Q) system. In this case, the inventory
is monitored continuously, for instance by an ERP system. As soon as the inventory
position drops below the reorder levels, a new order is generated of the size Q (which
in this case is big enough to get the inventory position above the level s again). The
reorder level should be based on the expected demand during the delivery time L
of the items ordered. In case the actual demand after reordering is higher than was
expected, the inventory position might drop below 0 and backorders will be created.
In other words, a negative inventory position means backorders are required. Note
that the average reorder cycle time is:
Q∕E(D) (6.1)
with: E(D) expected demand per unit of time.
Depending on the type of item at hand, demand might be expressed in a discrete
number of items required (0, 1, 2, etc.) or in a continuous expression like 4.5 l or
26.3 kg. Hadley and Whitin (1963) have shown that in the case of discrete demand,
assuming back ordering, the inventory position after any review moment follows a
discrete uniform distribution, ranging from s to s + Q−1. As a result, we get for the
average inventory position:
(s + (s + 1)+···+(s − 1 + Q))∕Q = s + (Q − 1)∕2 (6.2)
Similarly, if demand follows a continuous distribution (thus e.g. when talking
about kilograms or liters instead of the number of items), Hadley and Whitin (1963)
have shown that just after the review moment the inventory position is (continu￾ously) uniformly distributed between s and s + Q, resulting in the following average
inventory position after the moment of review:
s + Q∕2 (6.3)98 6 Reorder Point Decoupling Point Control Systems
Backorders
Q
Stock
Time
s L
Available stock
Inventory position
Re-order cycle
= Q/E
Re-order cycle
(D)
Figure 6.6 Stock in time for a (s, Q) system.
The logic behind this is shown in Figure 6.6. That figure also shows that for an
(s, Q) system with continuous demand, the maximum level of the inventory
position is4
s + Q (6.4)
If the customer batch size is greater than one unit of measure, it might happen that
at the moment of the review, the stock level is (considerably) lower than the reorder
level s. That is called the undershoot at the moment of reordering. Undershoot is the
difference between s and the inventory position level at the moment of reordering
(van Donselaar and Broekmeulen 2019).
Remark: A special case of the (s, Q) system is the multi-bin system. In a multi-bin
system, two or more bins are used to store the item considered. The bin size,
expressed in the number of items, is equal for all bins. As soon as a bin gets
empty, an order is sent to the supplier equal to the bin size.
In Figure 6.7, an example is given of such a system for the situation with two
bins (the “two-bin system”). In situation 1 of the example, both bins are filled.
When items are being used, they are taken from one of the two bins (in this
case: bin 1). This is shown in situation 2. At the moment one of the bins is empty
(situation 3), the bin is sent to the supplier to be filled again. The remaining bin
is then used to take items for use (situation 4). The idea would be that the first
bin would be returned filled again before the second bin is empty (situation 5).
This actually is a (s, Q) system with a bin quantity equal to Q and s = Q.
4 Again this is correct for continuous demand; for discrete demand, the exact formula would be
s+(Q−1).6.2 When to Order? 99
1 2
1 2
1 2
2
1 2
1
2
3 Order
to supply
Supply
4
5
Figure 6.7 Example of a two-bin system.
Part description
Qty Lead
Time
Supplier
Planner
Location
Order
Date
Due
Date
Part number
Smoke-shifter, left handed.
20
Acme Smoke-Shifter, LLC
John R.
14613
1 week
Rack 1B3
Card 1 of 2
Figure 6.8 Example Kanban card. Source: https://www.velaction.com/lean-information/
wp-content/uploads/2009/06/kanban-card-example.jpg.
Of course, variants on this basic two-bin system are possible. A three-bin sys￾tem works the same way but contains three bins instead of two. In that situation,
the bin size still equals Q, but s =2Q. In general: in n-bin systems, the bin size
equals Q, where the reorder level s =(n−1)⋅Q. Ordering can be done by sending
the empty bin to the supplier but can also be done by sending a standard order
card to the supplier. Usually, such an order card (see Figure 6.8) is attached to
the bin and taken from it at the moment of reordering.
6.2.2 Periodic Review
In case the stock level is monitored periodically, we get an (R, s, Q) system. The
difference between the (s, Q) system and the (R, s, Q) system is that in the latter case,
stock levels are not monitored continuously, but only at a certain time interval of100 6 Reorder Point Decoupling Point Control Systems
Stock
Time
s
R
L
L
Q
Q
Figure 6.9 Basic logic of the (R, s, Q) system.
R (see Figure 6.9). As a result, the stock level might be considerably lower than the
reorder level s at the moment of the review (undershoot), even in de case where the
batch size of the customer equals 1 unit of measure. Usually, the reason for using
such a review period is that the inventory checks are done by humans instead of a
computer (e.g. in the case of manually controlled spare part stock), or that there are
only every R-time units an opportunity for reordering (for instance once a week or
once a month).
6.2.3 The Reorder Level – Continuous Review
When continuously monitoring [as in an (s, Q) system], the reorder level s should be
chosen such that it covers (at least) the demand during (supply) lead time. If both
demand rate (=demand per unit of time) and lead time would be known beforehand,
e.g. because both are constant, then the choice would be easy:
s = D ⋅ L (6.5)
with:
D: the demand rate
L: the lead time of the supplier.
In that case, the performance would be perfect: P1 = P2 = P3 = 1.0 (no stockouts at
all). Figure 6.5 illustrates such a situation with a constant demand rate and lead time.
However, since the demand often fluctuates and is not known exactly in advance,
usually some stock is required to absorb these fluctuations in demand or uncertainty
in the forecast. Otherwise, the performance will be poor. In case the demand dur￾ing lead time is not really known but seems to follow a more or less symmetrical6.2 When to Order? 101
Available stock
Q
Time
s
E(D(L))
Demand during
lead time:
L
0.1%
–3σ –2σ –1σ μ 1σ 2σ 3σ
2.1% 2.1% 13.6% 13.6% 0.1%
34.1% 34.1%
Figure 6.10 Available stock in time for (s, Q) system for normal distributed demand.
distribution like the Normal distribution as shown at the top of Figure 6.10, choosing
a reorder level equal to the expected demand during lead time (ss =0) would result
in a P1 of 0.50. After all, in that case, the probability that demand would be higher
than the reorder level is 50% (see lowest dotted line for demand in Figure 6.10).
The reorder level chosen in a reorder point system must be expressed as the sum
of the expected demand during lead time plus some extra safety (known as safety
stock):
s = E(D(L)) + ss (6.6)
with E(D(L)) the expected demand during the lead time and ss the safety stock.
The safety stock is needed for the occasions that the actual demand is higher than
the expected demand. We might be able to get information on the demand during
lead time based on historical data.
Example 6.1
Consider the situation where demand during the lead time might follow a dis￾tribution as shown in Figure 6.11. The average demand during the lead time in
the example is 5 (=0.05x1+0.08x2+···+0.01x14).
The probability that demands during the lead time will be 5 or less is 0.67
(=0.05+0.08+0.20+0.20+0.14). Using the demand distribution, we can
establish:
– If we choose a reorder level equal to the average demand during the lead
time (thus: s =5), the probability of not having a stockout is 0.67. In other
words, P1 = 0.67.102 6 Reorder Point Decoupling Point Control Systems
0.05
0.08
0.20 0.20
0.14
0.10
0.06
0.05
0.04
0.03
0.02
0.01 0.01 0.01
0.00
0.05
0.10
0.15
0.20
0.25
1 2 3 4 5 6 7 8 9 10 11 12 13 14
Probability
Demand during lead time
Figure 6.11 Example distribution of demand during lead time.
– Choosing a reorder level one higher than average (so: s =6) would lead to
a probability of not having a stockout of 0.61+0.10 = 0.77. Thus P1 = 0.77.
– Similarly, if we would choose s =7 we would get P1 = 0.83. Etcetera.
– Clearly, for s =14 we would get P1 = 1.00 (no stock outs!).
Using the same example, we can determine the P2 levels by applying the equation
P2 = (Q − E(BO))∕Q
for a certain Q at different values of s by determining the expected number of back￾orders for each s. We will assume Q =20. For ease of explaining, we will start with
the maximum s and then work our way down.
– For s =14, clearly P2 = 1.0.
– For s =13, there is one option where we would get a backorder (of 1 item), i.e.
if D(L) = 14. The probability of that happening is only 0.01. Thus, the expected
number of items back ordered would be 0.01 × 1 = 0.01. For Q =20, we would get
P2 = (20−0.01)/20 = 0.9995.
– If s =12, the expected number of items back ordered would be (0.01 × 1+0.01 × 2)
=0.03. Then P2 = (20−0.03)/20 = 0.9985.
– For s =9, the expected number of items back ordered becomes (0.03x1+0.02x2+
0.01x3+0.01x4+0.01x5) = 0.11, giving P2 = 0.9905.
Figure 6.12 gives these calculations, both for P1 and P2.
The example given above also shows that P2 is dependent on Q, and P1 is not.
That dependency can be illustrated even further if we change Q. In Figure 6.13, three
different values of Q are used, using the same data as in the example. P1 is not depen￾dent on Q. However, the frequency of stockouts during a certain period is! That is
shown in the following equation
Frequency of stockouts = (E(D(yr))∕Q)(1 − P1)
.6.2 When to Order? 103
D(L) Prob ∑Prob. Average s P1 BO P2
1 0.05 0.05 5000 1 0.05 4.000 0.8000
2 0.08 0.13 2 0.13 3.050 0.8475
3 0.20 0.33 Q 3 0.33 2.180 0.8910
4 0.20 0.53 20 4 0.53 1.510 0.9245
5 0.14 0.67 5 0.67 1.040 0.9480
6 0.10 0.77 6 0.77 0.710 0.9645
7 0.06 0.83 7 0.83 0.480 0.9760
8 0.05 0.88 8 0.88 0.310 0.9845
9 0.04 0.92 9 0.92 0.190 0.9905
10 0.03 0.95 10 0.95 0.110 0.9945
11 0.02 0.97 11 0.97 0.060 0.9970
12 0.01 0.98 12 0.98 0.030 0.9985
13 0.01 0.99 13 0.99 0.010 0.9995
14 0.01 1.00 14 1.00 0.000 1.0000
Figure 6.12 P1 and P2 for the example.
Figure 6.13 Dependency of P2 on Q
in the example. Q= 10 20 25
s P2 P2 P2
1 0.6000 0.8000 0.8400
2 0.6950 0.8475 0.8780
3 0.7820 0.8910 0.9128
4 0.8490 0.9245 0.9396
5 0.8960 0.9480 0.9584
6 0.9290 0.9645 0.9716
7 0.9520 0.9760 0.9808
8 0.9690 0.9845 0.9876
9 0.9810 0.9905 0.9924
10 0.9890 0.9945 0.9956
11 0.9940 0.9970 0.9976
12 0.9970 0.9985 0.9988
13 0.9990 0.9995 0.9996
14 1.0000 1.0000 1.0000104 6 Reorder Point Decoupling Point Control Systems
E(Dyr)/Q: 20 10 8
s P1 Nr.st.outs Nr.st.outs Nr.st.outs
1 0.05 19.00 9.50 7.60
2 0.13 17.40 8.70 6.96
3 0.33 13.40 6.70 5.36
4 0.53 9.40 4.70 3.76
5 0.67 6.60 3.30 2.64
6 0.77 4.60 2.30 1.84
7 0.83 3.40 1.70 1.36
8 0.88 2.40 1.20 0.96
9 0.92 1.60 0.80 0.64
10 0.95 1.00 0.50 0.40
11 0.97 0.60 0.30 0.24
12 0.98 0.40 0.20 0.16
13 0.99 0.20 0.10 0.08
14 1.00 0.00 0.00 0.00
Figure 6.14 Dependency of the number of stockouts on P1 and Q in the example.
For instance, if the expected demand in a year for the item in the example would
be 200, the number of reorder cycles for Q =20 would be 10. Then with P1 = 0.77
(s =6), the expected number of stockouts in a year would be:
(200∕20)×(1 − 0.77) = 2.30
For Q =10, that number would be: (200/10) × (1−0.77) = 4.60
And for Q =25, it would be: (200/25) × (1−0.77) = 1.84
This is illustrated in Figure 6.14.
In case we may assume that demand follows a stable demand distribution
(“stationary demand”), we can use the historical average demand as the expected
value (E(D(L))). If demand is not stable, for instance, because of a seasonal demand
pattern or a trend going up or down, then we should not use the historical average,
but a more accurate forecast considering the expected change. In the case of
seasonal demand that might mean having different expected demand values in the
different seasons. In the case of a trend, a forecasting model using such a trend
should be considered. For these types of forecasting models, we refer to 5.3 (see
also Silver et al. 1998). The choice for ss depends on the performance we would
like to achieve, both in terms of definition (like P1, P2, or P3) and in terms of the
level required (like 0.95). In the example given above the (expected) distribution
of demand during lead time can be used to determine ss (and s). In the case of the
situation described in Figure 6.10, we can use the characteristics of the Normal
distribution. If we assume a P1 definition of service level would be used, we can
derive the following calculations:
If ss =1⋅𝜎(D(L))
with 𝜎(D(L)): standard deviation of the demand during lead time6.2 When to Order? 105
then we know that P1 = 0.8413 (see Appendix 6.A)
If ss =2⋅𝜎(D(L)) then P1 = 0.9772
In general: in the situation where demand during lead time follows a certain dis￾tribution and we want to obtain a certain P1-level, then the following calculation
would give the reorder level required:
s = E(D(L)) + zP1 ⋅ 𝜎(D(L)) (6.7)
with zP1 the number of times the standard deviation of D(L) should be covered by
the safety stock to achieve the P1-level required.
In the case demand follows a normal distribution, the z-values can be found in the
tables for the standard normal distribution (see Appendix 6.A). A similar approach
can be followed for the P2-service level (see shaded section below).
Reorder Levels and P2-Service Level in Case of Normal Distributed Demand
As explained before, the fill rate (P2) is a very common measure of service
level used to determine the reorder level when ordering “on stock.” The aver￾age demand during a reorder cycle is Q, so the number of items back ordered in
a reorder cycle would be:
(1 − P2) ⋅ Q
Assuming a safety stock of:
zP2 ⋅ 𝜎(D(L))
we need to find the value of zP2 for which that service level can be achieved. The
table “unit-loss table for standard Normal distribution” (see Appendix 6.B) shows
the number of items “lost” for a certain value of z in case the standard deviation
of demand would be 1 and the average would be 0. That number is called L(z).
In other words, the expected number of items back ordered is:
E(BO) = L(z) ⋅ 𝜎(D(L))
gives the expected number of items back ordered in a reorder cycle. So, for a
given level of P2, we get:
L(z) = Q ⋅ (1 − P2)∕𝜎(D(L))
The table shows what the zP2 value is for that calculated L(Z), and thus we
can determine the safety stock level.
And vice versa: for a certain safety stock, the P2 level obtained can be deter￾mined based on the expected number of items back ordered (E(BO)):
P2 = (Q − E(BO))∕Q = (Q − L(z) ⋅ 𝜎(D(L)))∕Q
Usually, in companies, the target fill rate P2 is determined first, and then the
safety factor is calculated because in contracts there are service level agree￾ments.106 6 Reorder Point Decoupling Point Control Systems
Example 6.2
If the Q used is 20 with a 𝜎(D(L)) = 2 and the targeted fill rate would be 0.90, we
would get:
L(z) = 20 × (1 − 0.9)∕2 = 1
then zP2 = −0.90 (check Appendix 6.B), meaning that a service level less than
the average demand during the lead time would be sufficient (“negative” safety
stock)!
For a targeted fill rate of 0.99, we would get:
L(z) = 20 × (1 − 0.99)∕2 = 0.10
then zP2 =0.90, resulting in a safety stock of 0.9 × 2 = 1.8 (rounded to 2.0).
For a given safety stock level, the resulting P2 level can be calculated as well,
using the same table. So, assuming Q =20, 𝜎(D(L)) = 2, and we would have a
safety stock of 1, we can derive:
z = 1/2 = 0.50
L(z) ≫ 0.20 (again: check Appendix 6.B)
then P2 = (20 − (0.2 × 2))/20 = 0.98
If demand doesn’t follow a normal distribution, derivation of the reorder level that
leads to a certain performance is more complicated but can be done if the first and
second moments of the demand are known (Appendix 6.C).
Reorder Levels and the P2 Service Measure in Case Demand Doesn’t Follow a
Normal Distribution
If demand doesn’t follow a normal distribution, derivation of the reorder level
that leads to a certain performance is more complicated but can be done if the
first and second moments of the demand are known.
In short: for P2 (for other performance measures we refer to van Donselaar
and Broekmeulen 2019), the following expression holds in case we have dis￾crete demand and a reorder level s (BO = back order, IP = inventory position,
D =demand, R =review period, L =horizon):
P2 = 1 − E[BO]
E[D(𝜏 + L𝜏 + R + L)] = 1 − E[BO((𝜏 + R + l) − E[BO(𝜏 + L)]
E[D(𝜏 + L𝜏 + R + L)]
= 1 − E[{D(𝜏,𝜏 + R + L) − IP(𝜏)}+] − E{[D(𝜏,𝜏 + L) − IP(𝜏)}+
E[D(𝜏 + L𝜏 + R + L)]
= 1 −
∑∞
s=−∞P(IP = k)(E[{D(𝜏,𝜏 + R + L) − s}+] − E[{D(𝜏,𝜏 + L) − s])
E[D(𝜏 + L𝜏 + R + L)]
Using the t-fold convolution of the single period probability distribution or
the fitting procedure of Adan et al. (1995), the probability density function of
the demand during t periods (t = L, R, L + R) can be approximated, and thus, for
a given k, P2 can be calculated (see Appendix 6.B).6.2 When to Order? 107
In case the distribution of demand during lead time is not known, but demand
per time unit is, then we need to determine 𝜎(D(L)) first before using Eq. (6.7). If the
demand per time unit can be regarded as an independent draw from the demand
distribution, and L can be regarded as a constant, then the following equation can
be used:
𝜎(D(L)) = 𝜎(D)⋅
√L (6.8)
If in such a case the lead time L is not deterministic (i.e. constant) but varies in
time (and thus the promised lead time from the supplier is not 100% reliable), the
following equation should be used (Ross 1983):
𝜎(D(L)) = √
(E(L) ⋅ 𝜎(D)2 + E(D)2 ⋅ 𝜎(L)2) (6.9)
with:
E(L): Expected lead time in time units
𝜎(L): Standard deviation of lead time in time units
E(D): Expected demand per time unit
𝜎(D): Standard deviation of demand per time unit.
In Silver et al. (1998), it is shown that for a non-stable demand distribution, like
when there is a trend in demand or seasonal demand, the same equations can be
used if we use the standard deviation of the forecast error instead of the historical
standard deviation in demand for 𝜎(D(L)). They also show how to derive an estimate
for that standard deviation, based on the relation between the standard deviation and
the forecast itself to be established by using a regression analysis.
In case other constraints need to be considered as well, like budget constraints or
constraints on the total available space for storage, an approach using mathemati￾cal optimization would be needed (van Donselaar and Broekmeulen 2019). Similar
approaches are needed to cover the situation of lost sales.
6.2.4 The Reorder Level – Periodic Review
The difference between a (R, s, Q) system and a (s, Q) system is the review period R
(see Figure 6.15). The previously discussed (s, Q) system can be regarded as a special
case of the (R, s, Q) system with a very small review time. In this situation, similar
formulas can be used, but the difference is that the period used to determine the
amount of demand that should be covered by the reorder level s is not L but R + L.
The reason for that is that the stock level may still be at or above s at the moment
of reviewing, but immediately after that moment drop below the level of s. This is
shown in Figure 6.15. Therefore, the equations mentioned in Section 6.2.3 to deter￾mine s should be changed into:
s = E(D(R + L)) + ss
ss = k ⋅ 𝜎(D(R + L))
𝜎(D(R + L)) = √
(E(R + L) ⋅ 𝜎(D)2 + E(D)2 ⋅ 𝜎(R + L)2) (6.10)
Remark: Inventory control systems in which ordering can only be done periodi￾cally (e.g. once every two weeks) and in (multiples of) a fixed batch size are very108 6 Reorder Point Decoupling Point Control Systems Available stock
Time
Lead time L
s
Undershoot
Figure 6.15 (R, s, Q) system when inventory drops below s just after the review moment.
common in practice but receive hardly any attention in standard textbooks. Most
standard textbooks also focus on systems with continuous demand distributions
(and sometimes predominantly on one distribution only: the normal distribu￾tion). The assumption of a continuous distribution is not appropriate if the aver￾age demand is very low (like in a spare parts environment), and the assumption
of a normal distribution is not valid if the demand uncertainty is high.
In the case of lumpy demand (i.e. demand comes in batches), it might happen that
at the moment of the review, the stock level is considerably lower than the reorder
Stock
Time
s
R L L
Q Q
Figure 6.16 Example with lumpy demand and undershoot.6.3 How Much to Order? 109
level s. That is called the undershoot at the moment of reordering. Undershoot is the
difference between s and the inventory position level at the moment of reordering
(van Donselaar and Broekmeulen 2019). A practical approach to deal with the issue
of undershoot would be to add the amount to the demand during lead time. Lumpy
demands as shown in Figure 6.16 will result in a higher standard deviation of that
demand. In the case of the example given in Figure 6.5, there is no real undershoot:
as soon as the inventory position level hits the s a reorder is created.
6.3 How Much to Order?
Also for the decision on how much to order, there are two policies:
– a fixed amount
– up to a certain fixed maximum level
6.3.1 Fixed Amount
In case we always order a fixed amount Q, the optimal order size can be determined
by using Camps formula (see also 5.30), giving:
Qopt =
√
2 ⋅ D(yr) ⋅ Cord
Chold
with
D(yr): average demand during a year
Cord: order-related costs; usually ordering costs
Chold: inventory-related costs; usually holding costs
6.3.2 Maximum Level
6.3.2.1 (s, S)
The replenishment logic used in the (s, S) system is shown in Figure 6.17. The
reordering logic is similar to the (s, Q) system. Only in this case, instead of ordering
Q, the order size is chosen such that the maximum inventory position level is
reached. That level is indicated as S. In the example shown in the figure, it is clear
why such a system may make sense in that situation: demand is very irregular. At
one moment in time, it might be very large, thus dropping considerably below the
reorder level (leading to undershoot), while at other moments demand may be very
small or none at all. Usually, the maximum level S is chosen based on practical
considerations, like the amount of storage space available. The minimum order
quantity is S-s (for discrete demand). However, considering undershoot, the order
size Qt will be:
Qt = (S − s) + Undershoot(t) (6.11)
We must realize that in an (s, S) system exactly one customer order will cause
the inventory level to drop below s. Therefore, the amount of undershoot depends
on the batch size of that customer’s order. Clearly, in case of discrete demand, the110 6 Reorder Point Decoupling Point Control Systems Available stock
Time
Lead time L
s
S (=max.stock)
Q1
Q2
Figure 6.17 Basic logic of the (s, S) system.
minimum undershoot would be 1, and the maximum undershoot would be the max￾imum batch size of customer orders. The average undershoot is equal to or less
than the average customer order batch size: it is only equal when the inventory
level would have been s at the moment the customer order arrived. Assuming that
indeed the inventory level would be equal to s would be a kind of worst-case scenario.
A more analytical approach to the (s, S) system can be found in Tijms and Groenevelt
(1984).
6.3.2.2 (R, s, S)
Having a review time usually means that the undershoot at the moment of reorder￾ing becomes larger. The result is that the variation coefficient of the demand during
(R + L) tends to be higher in this case compared to the situation where (s, S) systems
would be used. And thus, the safety stock would be higher. The order quantity is the
same as in the case of a (s, S) system.
Qt = (S − s) + Undershoot(t) (6.12)
In van Donselaar and Broekmeulen (2019), it is explained that the performance of
a (R, s, S) system in many situations is comparable to the performance of a (R, s, Q)
system.
Appendix 6.A Table of the One-Sided Standard Normal
Distribution
This table shows for the Standardized Normal Distribution the probability that a
statistic is equal to or less than z. The rows give the value of z till the first decimal;
the columns show the second decimal. Example: P(x ≤1.28) = 0.89976.A Table of the One-Sided Standard Normal Distribution 111
z 0123456789
0.0 0.5000 0.5040 0.5080 0.5120 0.5160 0.5199 0.5239 0.5279 0.5319 0.5359
0.1 0.5398 0.5438 0.5478 0.5517 0.5557 0.5596 0.5636 0.5675 0.5714 0.5753
0.2 0.5793 0.5832 0.5871 0.5910 0.5948 0.5987 0.6026 0.6064 0.6103 0.6141
0.3 0.6179 0.6217 0.6255 0.6293 0.6331 0.6368 0.6406 0.6443 0.6480 0.6517
0.4 0.6554 0.6591 0.6628 0.6664 0.6700 0.6736 0.6772 0.6808 0.6844 0.6879
0.5 0.6915 0.6950 0.6985 0.7019 0.7054 0.7088 0.7123 0.7157 0.7190 0.7224
0.6 0.7257 0.7291 0.7324 0.7357 0.7389 0.7422 0.7454 0.7486 0.7517 0.7549
0.7 0.7580 0.7611 0.7642 0.7673 0.7704 0.7734 0.7764 0.7794 0.7823 0.7852
0.8 0.7881 0.7910 0.7939 0.7967 0.7995 0.8023 0.8051 0.8078 0.8106 0.8133
0.9 0.8159 0.8186 0.8212 0.8238 0.8264 0.8289 0.8315 0.8340 0.8365 0.8389
1.0 0.8413 0.8438 0.8461 0.8485 0.8508 0.8531 0.8554 0.8577 0.8599 0.8621
1.1 0.8643 0.8665 0.8686 0.8708 0.8729 0.8749 0.8770 0.8790 0.8810 0.8830
1.2 0.8849 0.8869 0.8888 0.8907 0.8925 0.8944 0.8962 0.8980 0.8997 0.9015
1.3 0.9032 0.9049 0.9066 0.9082 0.9099 0.9115 0.9131 0.9147 0.9162 0.9177
1.4 0.9192 0.9207 0.9222 0.9236 0.9251 0.9265 0.9279 0.9292 0.9306 0.9319
1.5 0.9332 0.9345 0.9357 0.9370 0.9382 0.9394 0.9406 0.9418 0.9429 0.9441
1.6 0.9452 0.9463 0.9474 0.9484 0.9495 0.9505 0.9515 0.9525 0.9535 0.9545
1.7 0.9554 0.9564 0.9573 0.9582 0.9591 0.9599 0.9608 0.9616 0.9625 0.9633
1.8 0.9641 0.9649 0.9656 0.9664 0.9671 0.9678 0.9686 0.9693 0.9699 0.9706
1.9 0.9713 0.9719 0.9726 0.9732 0.9738 0.9744 0.9750 0.9756 0.9761 0.9767
2.0 0.9772 0.9778 0.9783 0.9788 0.9793 0.9798 0.9803 0.9808 0.9812 0.9817
2.1 0.9821 0.9826 0.9830 0.9834 0.9838 0.9842 0.9846 0.9850 0.9854 0.9857
2.2 0.9861 0.9864 0.9868 0.9871 0.9875 0.9878 0.9881 0.9884 0.9887 0.9890
2.3 0.9893 0.9896 0.9898 0.9901 0.9904 0.9906 0.9909 0.9911 0.9913 0.9916
2.4 0.9918 0.9920 0.9922 0.9925 0.9927 0.9929 0.9931 0.9932 0.9934 0.9936
2.5 0.9938 0.9940 0.9941 0.9943 0.9945 0.9946 0.9948 0.9949 0.9951 0.9952
2.6 0.9953 0.9955 0.9956 0.9957 0.9959 0.9960 0.9961 0.9962 0.9963 0.9964
2.7 0.9965 0.9966 0.9967 0.9968 0.9969 0.9970 0.9971 0.9972 0.9973 0.9974
2.8 0.9974 0.9975 0.9976 0.9977 0.9977 0.9978 0.9979 0.9979 0.9980 0.9981
2.9 0.9981 0.9982 0.9982 0.9983 0.9984 0.9984 0.9985 0.9985 0.9986 0.9986
3.0 0.9987 0.9987 0.9987 0.9988 0.9988 0.9989 0.9989 0.9989 0.9990 0.9990
3.1 0.9990 0.9991 0.9991 0.9991 0.9992 0.9992 0.9992 0.9992 0.9993 0.9993
3.2 0.9993 0.9993 0.9994 0.9994 0.9994 0.9994 0.9994 0.9995 0.9995 0.9995
3.3 0.9995 0.9995 0.9995 0.9996 0.9996 0.9996 0.9996 0.9996 0.9996 0.9997
3.4 0.9997 0.9997 0.9997 0.9997 0.9997 0.9997 0.9997 0.9997 0.9997 0.9998
3.5 0.9998 0.9998 0.9998 0.9998 0.9998 0.9998 0.9998 0.9998 0.9998 0.9998
3.6 0.9998 0.9998 0.9999 0.9999 0.9999 0.9999 0.9999 0.9999 0.9999 0.9999
3.7 0.9999 0.9999 0.9999 0.9999 0.9999 0.9999 0.9999 0.9999 0.9999 0.9999
3.8 0.9999 0.9999 0.9999 0.9999 0.9999 0.9999 0.9999 0.9999 0.9999 0.9999
3.9 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000112 6 Reorder Point Decoupling Point Control Systems
Appendix 6.B Table Standard Normal Loss Function
Table 6.B L(Z) is the standard loss function, i.e. the expected number of lost sales as a
fraction of the standard deviation.
Z L(Z) Z L(Z) Z L(Z) Z L(Z) Z L(Z) Z L(Z)
−3.00 3.000 −2.00 2.008 −1.00 1.083 0.00 0.399 1.00 0.083 2.00 0.008
−2.96 2.960 −1.96 1.969 −0.96 1.050 0.04 0.379 1.04 0.077 2.04 0.008
−2.92 2.921 −1.92 1.930 −0.92 1.017 0.08 0.360 1.08 0.071 2.08 0.007
−2.88 2.881 −1.88 1.892 −0.88 0.984 0.12 0.342 1.12 0.066 2.12 0.006
−2.84 2.841 −1.84 1.853 −0.84 0.952 0.16 0.324 1.16 0.061 2.16 0.005
−2.80 2.801 −1.80 1.814 −0.80 0.920 0.20 0.307 1.20 0.056 2.20 0.005
−2.76 2.761 −1.76 1.776 −0.76 0.889 0.24 0.290 1.24 0.052 2.24 0.004
−2.72 2.721 −1.72 1.737 −0.72 0.858 0.28 0.274 1.28 0.047 2.28 0.004
−2.68 2.681 −1.68 1.699 −0.68 0.828 0.32 0.259 1.32 0.044 2.32 0.003
−2.64 2.641 −1.64 1.661 −0.64 0.798 0.36 0.245 1.36 0.040 2.36 0.003
−2.60 2.601 −1.60 1.623 −0.60 0.769 0.40 0.230 1.40 0.037 2.40 0.003
−2.56 2.562 −1.56 1.586 −0.56 0.740 0.44 0.217 1.44 0.034 2.44 0.002
−2.52 2.522 −1.52 1.548 −0.52 0.712 0.48 0.204 1.48 0.031 2.48 0.002
−2.48 2.482 −1.48 1.511 −0.48 0.684 0.52 0.192 1.52 0.028 2.52 0.002
−2.44 2.442 −1.44 1.474 −0.44 0.657 0.56 0.180 1.56 0.026 2.56 0.002
−2.40 2.403 −1.40 1.437 −0.40 0.630 0.60 0.169 1.60 0.023 2.60 0.001
−2.36 2.363 −1.36 1.400 −0.36 0.605 0.64 0.158 1.64 0.021 2.64 0.001
−2.32 2.323 −1.32 1.364 −0.32 0.579 0.68 0.148 1.68 0.019 2.68 0.001
−2.28 2.284 −1.28 1.327 −0.28 0.554 0.72 0.138 1.72 0.017 2.72 0.001
−2.24 2.244 −1.24 1.292 −0.24 0.530 0.76 0.129 1.76 0.016 2.76 0.001
−2.20 2.205 −1.20 1.256 −0.20 0.507 0.80 0.120 1.80 0.014 2.80 0.001
−2.16 2.165 −1.16 1.221 −0.16 0.484 0.84 0.112 1.84 0.013 2.84 0.001
−2.12 2.126 −1.12 1.186 −0.12 0.462 0.88 0.104 1.88 0.012 2.88 0.001
−2.08 2.087 −1.08 1.151 −0.08 0.440 0.92 0.097 1.92 0.010 2.92 0.001
−2.04 2.048 −1.04 1.117 −0.04 0.419 0.96 0.090 1.96 0.009 2.96 0.000
L(Z) is the standard loss function, i.e. the expected number of lost sales as a fraction
of the standard deviation. In other words, the lost sales are L(Z) × 𝜎demand
For a targeted fill rate of 0.99, Q =20 and 𝜎demand = 2 we would get:
L(z) = 20 × (1 − 0.99)∕2 = 0.10
then zP2 =0.90, resulting in a safety stock of 0.9 × 2 = 1.8 (rounded to 2.0).
For a given safety stock level, the resulting P2 level can be calculated as well, using
the same table. So, assuming Q =20, 𝜎(D(L)) = 2, and we would have a safety stock6.C Reorder Level Determination in Case of a General Distribution 113
of 1, we can derive:
z = 1∕2 = 0.50
L(z) ≈ 0.20 (again: see table) and then P2 = (20 − (0.2 × 2))/20 = 0.98.
Appendix 6.C Reorder Level Determination in Case of a
General Distribution
To show how we can derive the reorder level in case the demand doesn’t follow a
normal distribution, we will demonstrate this for the (R, s, nQ) system, the P2 per￾formance measure (for other measures we refer to van Donselaar and Broekmeulen
2019) with either a discrete demand distribution (Section 6.C.1) or a continuous
demand distribution (Section 6.C.2).
6.C.1 Discrete Demand
Important is the result of Hadley and Whitin (1963), who for backorder systems.
derived the probability density function for the inventory position at an arbitrary
review period. For an (R, s, nQ) control system with back ordering, the inventory
position at an arbitrary review moment just after the potential ordering moment
follows a discrete uniform distribution on the set {s, s +1, …, s−1+ Q}. For P2, we
then get:
P2 = 1 − E[BO]
E[D(t0 + L, t0 + R + L)] = 1 − E[BO(t0 + R + L)] − E[BO(t0 + L)]
E[D(t0 + L, t0 + R + L)]
= 1 − E[{D(t0, t0 + R + L) − IP(t0)}+] − E[{D(t0, t0 + L) − IP(t0)}+]
E[D(t0 + L, t0 + R + L)]
= 1 −
∑∞
k=−∞P(IP = k)(E[{D(t0, t0 + R + L) − k}+] − E[{D(t0, t0 + L) − k}+])
E[D(t0 + L, t0 + R + L)]
= 1 −
1
Q
∑Q−1
i=0
∑∞
d=s+i+1[P(DL+R = d) − P(DL = d)]{d − s − i}
E[DR]
The only things we need to know are the probability distributions of DL, DR, and
DL+R. This can be achieved in two ways:
1. The probability function of the demand during t periods (t = L, R, L + R) is the
t-fold convolution of the probability function of the single period demand.
2. By first calculating the mean and the standard deviation of the demand during t
periods (t = L, R, L + R) and then using a two-moment fitting procedure to find a
theoretical discrete probability distribution.
Ad 1. The assumption is that, for instance from empirical data, the single period
probability function is known or can be determined. From this single-period prob￾ability distribution, we can determine the t-period probability function Dt using the114 6 Reorder Point Decoupling Point Control Systems
following equation:
P(Dt = k) = ∑
j+i=k
P(Dt−1 = j and D1 = i)
We will demonstrate this way of determining the probability distributions of DL,
DR, and DL+R by an example.
Suppose that in the past we registered demand during 50 weeks and that in 20
weeks the demand was 3 units and in the other 30 weeks the demand was 4 units.
For t =1 (single period), we then get:
P(D1 = 3) = 20∕50 = 0.4 and P(D1 = 4) = 30∕50 = 0.6
Assume that L =1 and R =2. Besides D1 we then need to know D2 and D3:
For t = 2: Since D1 can be 3 or 4 units, D2 can be 6, 7, or 8 units.
P(D2 = 6) = P(D1 = 3 and D1 = 3) = 0.4 × 0.4 = 0.16
P(D2 = 7) = P(D1 = 3 and D1 = 4)+P(D1 = 4 and D1 = 3) = 0.4 × 0.6+0.6 × 0.4 = 0.48
P(D2 = 8) = P(D1 = 4 and D1 = 4) = 0.6 × 0.6 = 0.36
For t = 3, D3 can be 9, 10, 11, or 12 units
P(D3 = 9) = P(D2 = 6 and D1 = 3) = 0.16 × 0.4 = 0.064
P(D3 = 10) = P(D2 = 6 and D1 = 4) + P(D2 = 7 and D1 = 3)
= 0.16 × 0.6 + 0.48 × 0.4 = 0.288
P(D3 = 11) = P(D2 = 7 and D1 = 4) + P(D2 = 8 and D1 = 3)
= 0.48 × 0.6 + 0.36 × 0.4 = 0.432
P(D3 = 12) = P(D2 = 8 and D1 = 4) = 0.36 × 0.6 = 0.216
Suppose Q =2 and s =10. Then, since E(DR) = 2*E(D1), E(D1) = ∑∞
d=0 d ⋅ P(D1 =
d) = 3 × 0.4 + 4 × 0.6 = 3.6 and P(D1 = d) = 0 for d =9, 10, 11, and 12, the fill rate
P2 equals:
P2 = 1 −
1
2
{P(D3 = 11) × 1 + P(D3 = 12) × 2 + P(D3 = 12) × 1}
2 × 3.6 = 0.925
Ad.2 By fitting a theoretical probability distribution on the first two moments, an
approximate probability distribution is found. An effective and simple fitting proce￾dure was developed by Adan et al. (1995).
The advantage of the fitting procedure is that only two numbers (the first two
moments) are required to fully specify the probability function, whereas the advan￾tage of the convolution method (using the full empirical distribution) is that it gives
exact results. The convolution method assumes that the empirical distribution is
known. Often, however, this distribution is based on a limited set of historical data
and is thus an estimate of the true probability function.6.C Reorder Level Determination in Case of a General Distribution 115
6.C.2 Continuous Demand
For P2, we have the following formula:
P2 = 1 − E[BO(t0 + R + L)] − E[BO(t0 + L)]
E[D(t0 + L, t0 + R + L)]
Now we need expressions for E[BO(t0 + t)] for t = R + L and t = L. We use the fact
that Hadley and Whitin (1963) showed that IP(t0)∼ u(s, s + Q), and we introduce the
continuous stochastic variable Δ = IP(t0) − s, so Δ ∼ u(o, Q). Furthermore, with f t(.)
we denote the probability density function for Dt (demand during t periods), and
with g(.) we denote the probability density function for the stochastic variable Δ.
E[BO(t0 + t)] = E
[
{D(t0 + t) − IP(t0)}+]
= E
[
(Dt − s − Δ)+]
= ∫
∞
−∞ ∫
Q
0
(x − s − Δ)+ft(x)g(Δ)dΔdx
= ∫
s
−∞ ∫
Q
0
(x − s − Δ)+ft(x)g(Δ)dΔdx
+ ∫
s+Q
s ∫
Q
0
(x − s − Δ)+ft(x)g(Δ)dΔdx
+ ∫
∞
s+Q ∫
Q
0
(x − s − Δ)+ft(x)g(Δ)dΔdx
= ∫
s
−∞ ∫
Q
0
(x − s − Δ)+ft(x)g(Δ)dΔdx
+ ∫
s+Q
s ∫
x−s
0
(x − s − Δ)+ft(x)g(Δ)dΔdx
+ ∫
s+Q
s ∫
Q
x=s
(x − s − Δ)+ft(x)g(Δ)dΔdx
+ ∫
∞
s+Q ∫
Q
0
(x − s − Δ)+ft(x)g(Δ)dΔdx
In the first integral (x −s− Δ)
+ = 0 since x≤s and in the third integral
(x −s− Δ)
+ = 0 since Δ ≥x −s, therefore both the first and the third integral are
equal to zero. Given the values for x and Δ in the second and the fourth integral, we
can replace (x −s− Δ)
+ with (x −s− Δ).
Since g(Δ) = 1/Q we get:
E[BO(t0 + t)] = 1
Q ∫
s+Q
s ∫
x−s
0
(x − s − Δ)ft(x)dΔdx
+
1
Q ∫
∞
s+Q ∫
Q
0
(x − s − Δ)ft(x)dΔdx
Integrating over Δ leads to:
E[BO(t0 + t)] = 1
Q ∫
s+Q
s
[
(x − s)Δ − 1
2
Δ2
]Δ=x−s
Δ=0
ft(x)dx
+
1
Q ∫
∞
s+Q
[
(x − s)Δ − 1
2
Δ2
]Δ=Q
Δ=0
ft(x)dx116 6 Reorder Point Decoupling Point Control Systems
= 1
Q ∫
s+Q
s
1
2
(x − s)
2
ft(x)dx
+
1
Q ∫
∞
s+Q
[
(x − s)Q − 1
2
Q2
]
ft(x)dx
= 1
Q ∫
s+Q
s
1
2
(x − s)
2ft(x)dx
+ ∫
∞
s+Q
xft(x)dx −
(
s + Q
2
)
∫
∞
s+Q
ft(x)dx
6.C.3 Determining the Reorder Level
The expressions just derived can be used directly to determine the reorder level.
Often the requirement is that a certain target fill rate should be achieved. For s start￾ing at 0, and increasing with 1 the earlier given expressions can be used to determine
P2, until P2 ≥ target fill rate. The expressions can also be used to build optimization
models with (cost) objective functions and/or certain restrictions. For this, we refer
to van Donselaar and Broekmeulen (2019).
References
Adan, I., van Eenige, M., and Resing, J. (1995). Fitting discrete distributions on the first
two moments. Probability in Engineering and Informational Sciences 9: 623–632.
van Donselaar, K.H. and Broekmeulen, R.A.C.M. (2019). Stochastic Inventory Models for
a Single Item at a Single Location, Beta Working Paper Series 447. Eindhoven: Beta
Research School for Operations Management and Logistics.
Hadley, G. and Whitin, T. (1963). Analysis of Inventory Systems. Englewood Cliffs, NJ:
Prentice-Hall.
de Kok, T. (2018). Inventory Management: Modeling Real-life Supply Chains and
Empirical Validity. Foundations and Trends in Technology, Information and
Operations Management 11 (4): 343–437.
Ross, S. (1983). Stochastic Processes. New York: Series in Probability and Mathematical
Statistics, Wiley.
Silver, E.A., Pyke, D.F., and Peterson, R. (1998). Inventory Management and Production
Planning and Scheduling. New York: John Wiley & Sons.
Tijms, H. and Groenevelt, H. (1984). Simple approximations for the reorder point in
periodic and continuous review (s, S) inventory systems with service level
constraints. European Journal of Operational Research 17 (2): 175–190.117
7
MRP Decoupling Point Control Systems
In Chapter 6, the assumption was made that the decoupling point studied was a
single stock point, and the policy was based on keeping items “on stock.” The only
information on demand used was the (historical) average demand and standard devi￾ation of demand (this information was used to determine the reorder levels). No
information on the time of demand or forecast was used, resulting in (instantaneous)
delivery of the requirements for several periods in the future. In many real-life sit￾uations, more information on item demand may be available. We might already
have received some customer orders to be delivered in the future. Or we can have
time-phased forecasts, giving more details than “just” the historical average demand
per period (there is a kind of program). Suppose we know the orders or time-phased
forecasts, we can use backward scheduling to determine the moment of release of
an order, considering the batch size considerations as discussed in Chapter 5. Mate￾rials requirements planning (MRP) is a system typically using this information. It is
used widely in most (if not all) ERP software. Section 7.1, discusses the basic prin￾ciples of such a system. In Section 7.2, we will discuss the timing of ordering, and
in Section 7.3, we will discuss the order size. Finally, in Section 7.4 we discuss how
MRP handles uncertainty and the use of MRP systems in distribution environments.
7.1 General Discussion of MRP Systems
In this section, we will discuss the general characteristics of MRP systems. The
first MRP systems (MRP-I systems), which will be discussed first, did not consider
the capacity aspects. Then we will discuss MRP systems that do consider capacity
(MRP-II systems) in Section 7.1.2.
7.1.1 Material Requirements Planning (MRP-I)
One of the most widespread systems for management of the supply of parts in a
“just-in-time” setting is materials requirements planning (also known as MRP; to
be more precise: MRP-I). The basic concept used in MRP is the idea of “dependent
Production Control in Practice: A Situation-Dependent Decisions Approach, First Edition.
Henny Van Ooijen and Corné Dirne.
© 2024 WILEY-VCH GmbH. Published 2024 by WILEY-VCH GmbH.118 7 MRP Decoupling Point Control Systems
1x 2x
Forecast or 
orders
Dependent 
demand
.....
1x 1x
Dependent 
demand
Figure 7.1 Bill of materials.
demand”: the demand for parts in a product to be produced depends on the demand
for the higher-level items in the bill of materials (BOMs) for that product. This is
illustrated in Figure 7.1. The figure shows the BOM for a bicycle. The bike consists
of an assembled (“full”) frame and two wheels. If the demand for the bike is known,
then the demand for the full frames can be derived directly from that. The demand
for the wheels can be derived from the demand for the bike, only multiplied by 2.
In other words, if the demand for the completed bike (the “parent”-item) would be
10, then we would need 10 frames and 20 wheels. Similar to the full frame, the full
frame consists of the basic frame and a saddle. So, the demand for 10 full frames
results in a demand for 10 basic frames and 10 saddles.
Based on a planned lead time to produce each item and considering the inventory
position for that item, it is easy to calculate the exact amount to be ordered at the
moment that is required to get the parts delivered just in time.
This is illustrated in Figure 7.2, where:
– m equals the number of parts j (“child”) that go into the assembly i (“parent”)
(the BOM-relation);
– Lj equals the (planned) lead time for producing part j (the time-offsetting).
The actual amount to produce for part j then depends on the batch size rule used
for that part. That is the basic logic of MRP-I. In MRP, the concept of “low-level
codes” (LLC) is used by the computer to determine whether a complete MRP
schedule can already be made for a certain item. Such a schedule can only be
made for a particular item when the requirements from all the possible parents
are known. The top-level item of the BOM gets LLC0 (in the case of the BOM
from Figure 7.1: the complete bicycle). The MRP schedules for items with these
codes should be calculated first. After the schedules of all items with LLC0 have
been determined, the schedules for items with LLC1 can be calculated. In the7.1 General Discussion of MRP Systems 119
Time
Order size
Planned lead time Lj
Qparent i
t–L t j
Dchild j = m∙Qparent, i
Figure 7.2 Logic MRP-I using time-offsetting and BOM-relation.
example, the assembled frame and the wheels have LLC1. When the schedules for
these items are known, the schedules for the items with LLC2 can be determined
(i.e. saddle, frame, etc.). This will be illustrated in more detail in Section 7.2.
7.1.2 Manufacturing Resources Planning (MRP-II)
The MRP logic as described in Section 7.1.1, is exactly what the acronym says: it’s a
method for determining material requirements. It doesn’t consider capacity restric￾tions. The assumption is that those restrictions are covered by the slack used in the
norms or can be covered by the flexibility of the unit dealing with the order to be
released. Also, due to the strong relation between the demand on the top level of
the BOM (LLC0) and the gross requirements on much lower levels in the BOM, any
change in the expected future demand may result in a lot of replanning. The system
might become pretty nervous (see Bertrand et al. (1998)). Therefore, in the 1970s,
additional concepts have been developed to deal with some of these issues, resulting
in the approach of Manufacturing Resources Planning or MRP-II (see, e.g. Vollmann
et al. (1997)). This MRP-II approach is illustrated in Figure 7.3.
7.1.2.1 Engine
As illustrated, the MRP-I logic, as explained earlier, is at the core of the approach (in
the engine). Apart from a plan for materials, a detailed capacity plan is also created
for each production unit based on the orders planned, the routings to be taken and
the expected or planned lead times and operation times for the operations in the
routings. The level of detail of that capacity plan depends on the characteristics of
the production unit (see Part III of this book). If needed, the capacity plan can alter
the material requirement plan. In many software packages that is done by creating
so-called “firm-planned orders” (FPO). Such an FPO overrules the MRP-I logic by
fixing a certain order release that cannot be changed anymore in future plans. This
is done by the MRP planner.120 7 MRP Decoupling Point Control Systems
MRP II
Resource
planning 
Rough cut
capacity
planning 
Detailed
capacity
plan 
Aggregate
production
planning 
Master
production
schedule 
Material
requirements
planning
Purchasing
at supplier
Release for
production 
Shop floor
control 
Purchasing
control
Demand
management 
“front end” “engine” “back end”
Inventory
status 
Bill of
materials 
Routings
Figure 7.3 MRP-II framework. Source: Vollmann et al. (1997)/McGraw-Hill Education.
7.1.2.2 Front End
The input for the MRP calculations is given by the Master Production Schedule.
That schedule specifies the requirements of the MPS-items (the LLC0-items) for the
relevant time horizon. In many situations, these MPS items are the end products
to be produced; sometimes, different items are chosen, for instance, in case the
customer order decoupling point is not positioned at the level of end products (see,
e.g. Vollmann et al. (1997)). The MPS is based on customer orders, forecasts, and
production orders needed to keep the final stock on a required level. A mix of these
three options is also possible. In case some of the orders in the MPS are related to a
required stock level, the MRP system is not functioning as a pure “on order”-system
anymore (although for the component-supplying units, it still looks that way). The
MPS should be checked on capacity consequences in the rough-cut capacity plan
(RCCP). Usually, only the inflexible bottleneck capacities are used for this RCCP.
Based on the insights gained from the RCCP, the MPS might be changed. In many
situations, decisions on budgets, resources available and sales targets to be achieved
should be taken long before the MPS is determined. This is often called “Sales
and Operation Planning” (Vollmann et al. 1997), “Production Planning” (Jacobs
and Chase 2018), or “Aggregate Planning” (Silver et al. 1998); in Figure 7.3, this is
referred to as “Aggregate Production Planning.” Compared to the MPS, aggregate
production planning has a much longer time horizon plus a less detailed time
scale. For instance, an MPS might have a horizon of eight weeks. The requirements
for MPS items might be specified in the number of items per week (then the time
bucket is said to be one week), while the aggregate production plan has a horizon of
one year and uses time buckets of a month. Also, the items used in the production
plan might be more aggregated by nature, for instance, “city bicycles” instead of
a much more specific item specifying one particular end product (like a specific
10-speed city bicycle for ladies in black, indicated by a specific product code).
Such a long-term aggregate production plan sets the boundaries for the MPS that7.1 General Discussion of MRP Systems 121
is determined later. It specifies on an aggregated level the number of products to
be produced and possibly stored as inventory for future demand, based on the
assumed demand and possible availability and use of resources. Different strategies
might be followed, like a chase strategy (where capacity levels are adjusted to follow
the demand patterns) or a level strategy (where the capacity is set to a fixed level
and demand is followed by using inventories); see, for instance, Jacobs and Chase
(2018), and Silver et al. (1998). The decision made usually refers to:
– the size of the regular workforce per period (WFt);
– the size of the amount of overtime per period (OTt);
– the size of the amount of work outsourced (OSt);
– the production volume per period (PVt).
If the expected demand in a period would be denoted with Dt, then the resulting
inventory at the end of a period t can be calculated with:
It = It−1 + PVt − Dt (7.1)
In case It is negative, a backlog is assumed. The production volume is a function of
the regular workforce, amount of overtime and outsourcing, and expected demand
plus possible remaining backlog for that period. Usually, the final decision is, to a
large extent, based on the costs involved. These are the costs related to, for instance:
– WFt, OTt, and OSt;
– the change in these values between period t and t −1 (like costs for training or
contract-related costs in case of flex-workers);
– inventory;
– backlogs (e.g. back-ordering or loss of sales, expediting costs);
– the control system (like costs for working overtime by planning or costs made on
extra efforts on data gathering).
For a more extensive discussion, we refer to Silver et al. (1998) and Jacobs and
Chase (2018).
7.1.2.3 Back End
Once the MRP calculations have been made, the release of orders would be the
next step. Production orders may be printed or sent digitally to the production floor.
Purchasing orders are sent to the suppliers. Often, the release of orders is the first
moment where the order flow is combined with the material flow and the flow of
resources. Considerations on when that actual release will and should be done are
discussed in Part III of this book because that release decision depends on the type
of unit that needs to complete the order. The progress of each order should be mon￾itored and if needed corrective measures should be taken; that is the main focus
of Shop Floor Control (PU control) resp. Purchasing Control. Again, the nature and
details of the logic used in PU control depend on the unit considered and will be
discussed in Part III.122 7 MRP Decoupling Point Control Systems
7.2 When to Order
In this section, we will discuss the basic calculations used in MRP-I, using the
BOM-relations and the time-offsetting. The terms used in the MRP-I schedule
usually depend on the software used. We will use the following terms to do the
calculations:
– gross requirements: the required amount of a part needed during a particular
period;
– scheduled receipts: number of parts that have been ordered in the past (a produc￾tion order or a purchasing order has been released) and thus are “on their way”
to be received during a particular period with the assumption that the order will
be received at the beginning of that period (in Visser and van Goor 2011 this is
named “on order”);
– scheduled inventory: the inventory available at the end of the indicated period con￾sidering both the expected demand and the scheduled receipts (so when no new
orders will be released);
– net requirements: the extra amount required in a period when considering the
inventory on hand and the scheduled receipts;
– planned receipts: the number of parts that, according to the MRP-I logic, should
be planned to be received at the beginning of the indicated period based on net
requirements and rules for batch sizing;
– planned inventory: the amount of stock available at the end of the indicated period
when the MRP-I suggestions are followed;
– planned release: the amount to be ordered at the beginning of the indicated period
to receive the amounts as calculated at “planned receipts.”
In Appendix 7.A the formulas, that are used for the calculations in MRP, are given.
We will use the BOM shown in Figure 7.4 to illustrate the MRP calculations in
this section. The production processes needed to produce the product are shown
in Figure 7.5.
FP
MA (1)
PX (2) PY (1)
MB (2)
PX (1) PZ (2)
PY (1)
Figure 7.4 BOM for product FP.7.2 When to Order 123
Production 
MB
Production
MA
Final 
assembly
FP
MA
MB
FP
PZ
PX
PY
LPY = 3
LPX = 2
LPZ = 2
LMA = 1
LMB = 2
LFP = 2
Figure 7.5 Production process for product FP.
The product FP is assembled from three modules and one part, i.e. one module
MA, two times module MB, and one part PY. Module MA is made from two parts PX
and one part PY; module MB is made from one part PX and two parts PZ. For each
part of the process, a planned lead time is given in Figure 7.5.
By considering the available inventory (“netting”) and the time needed to produce
the (intermediate) products (“off-setting”) the required production of a certain
(intermediate) product is determined based on the independent demand of the
end item.
Assuming an “empty” system, i.e. no items are stored anywhere, and no orders are
released, Figure 7.6 gives an overview of the basic calculations made according to
Item Period 1 2 3 4 5 6 7
FP GrossReq 20
PlanReceipts 20
PlanRelease 20
MA GrossReq 20
PlanReceipts 20
PlanRelease 20
MB GrossReq 40
PlanReceipts 40
PlanRelease 40
PX GrossReq 40 40
PlanReceipts 40 40
PlanRelease 40 40
PY GrossReq 20 20
PlanReceipts 20 20
PlanRelease 20 20
PZ GrossReq 80
PlanReceipts 80
PlanRelease 80
Figure 7.6 Basic MRP calculation for product FP (abbreviations: see Appendix 7.A).124 7 MRP Decoupling Point Control Systems
the MRP logic at the beginning of period 1 for the product with the BOM as given in
Figure 7.4. If 20 products FP are needed in period 7, then an order should be released
for those 20 products two periods earlier (LFP =2), i.e. in period 5. To start with the
assembly of these 20 products in period 5, 20 modules MA are needed, 40 modules
MB, and 20 parts PY in that same period 5 (see the dotted arrows in the figure). To
have those 20 modules A available in period 5, a planned release is required for 20
modules at the start of period 4 (since LMA =1), etc.
Note that the longest cumulative lead time determines the horizon required for
such an MRP schedule; in this case, this time horizon is six time periods (resulting
at the beginning of period 1 in a schedule for periods 1–7).
As already said, MRP uses the concept of LLC to determine whether a complete
MRP schedule can already be made for a certain item. The top-level item of the
BOM gets LLC0 (in this case, the item FP). The MRP schedules for items with
these codes should be calculated first. After the schedules of all items with LLC0
have been determined, the schedules for items with LLC1 can be calculated. In
the example, MA and MB have LLC1. When the schedules for these items are
known, the schedules for the items with LLC2 can be determined (i.e. PX, PY,
and PZ). Note that item PY also has LLC2, not LLC1: the “lowest” level in the
BOM determines the LLC for an item (which makes sense since all requirements
for the item should be determined before the complete planned release can be
calculated).
Two remarks are crucial to understand:
– The entire MRP calculation results in only one real suggestion for the planner to
do: that is to order the number of parts indicated as “planned release” for the next
period (in the example: the release of 40 PX, 20 PY, and 80 PZ at the beginning of
period 1)! There is no need to release more; all the other calculated releases are
only made to determine the release numbers for items at a lower level in the BOM
(and possibly show some issues that might occur soon).
– When comparing MRP with an ROP system, as discussed in Chapter 6, note that
the idea of using known future demand as done in MRP is not used at all in ROP:
only the actual inventory position and a fixed predetermined reorder level is con￾sidered when determining possible order release.
Figure 7.6 shows that the basic MRP logic results in a “just-in-time” delivery of the
parts and products required. Assuming the gross requirements for FP are correct,
this schedule makes sure that right at the moment the final product is needed, it is
also ready. The only reason why the parts are produced earlier than that is the lead
time required to do so. But that is not due to the MRP logic; that is because the pro￾duction and procurement processes simply require that amount of time. In example
7.6, the assumption was made that there was no inventory for any of the items nor
that there had been order releases in the past that hadn’t arrived yet. Usually, that
would be a very unlikely situation. In most cases, some inventory may be available at
the beginning of period 1. That amount of inventory is called “on hand.” Also, some
orders may already have been released in the previous period or even the period
before that may not have been finished yet. These orders are “scheduled receipts,”7.3 How Much to Order? 125
i.e. scheduled to be received soon. That means that they have been ordered at the
supplier but have not arrived yet, or production orders may have been released in
the production units shown in Figure 7.3 in the past and may not be finished yet at
the start of period 1. That changes the planned amount to release. Let’s assume the
following changes in the situation discussed above:
– MA has a starting inventory (“on hand”) of 10;
– MB has 5 on hand;
– PX has nothing on hand, but an order of 25 was released to the supplier two periods
ago and is planned to be received in period 1;
– PY has 10 on hand (and no orders are “on their way”);
– PZ has 5 on hand, plus an order of 40 is on its way and is planned to be received
in period 2.
That changes the schedule (see Figure 7.7). Note that not only do the amounts
change, but also entire orders may not be needed anymore (like no planned release
for PY in period 1!).
7.3 How Much to Order?
The last factor not included yet that will change the calculations shown is the rules
used for batch sizes. The assumption made in the schedules presented in Figures 7.6
and 7.7 is that any order size would be acceptable since the required amount is also
the amount planned to be received. This is known as the “lot-for-lot” situation (L4L):
each batch size is ok. But in many situations, a different rule might be used, for
instance, a fixed batch size “Q”: if an order is required, the order size should always
be Q. If more is needed than Q, often one out of two rules are applied:
– Minimum order quantity (MOQ): any order size equal to or larger than Q is
accepted;
– Multiple order rule (nQ-rule): any multiple of Q is accepted (so: 1 × Q, 2 × Q, etc.).
This rule is, for instance, applicable for situations where parts are packed in boxes
containing a particular number of parts; the number of parts in one box would
then equal Q.
Another rule might be to order for a fixed number of periods. These batch sizing
rules change the calculations dramatically. Let’s make some assumptions on the
batch sizes to be used in the situation from above:
– FP is produced lot-for-lot;
– MA and MB are produced in batches of 8 and 15 respectively (or multiples);
– PX has a MOQ of 25;
– PY must be ordered in batch sizes of 30 (or multiples);
– PZ has a MOQ of 40.
Figure 7.8 shows the impact of using these types of batch sizing rules. Batching
tends to increase the gross requirements for parts, even though the demand for the126 7 MRP Decoupling Point Control Systems
end product has not increased. Also, due to batching, “leftover inventory” becomes
much more common. Note that we have introduced the line “net requirements” in
the MRP table in Figure 7.8, which shows the actual number of parts required to
fulfill the gross requirements, considering the on-hand inventory and the (future)
scheduled receipts.
Item Period OnHand 1 2 3 4 5 6 7
FP GrossReq 20
SchedRec
SchedInvent 0 0 0 0 0 0 0 –20
PlanReceipts 20
PlanInvent 0 0 0 0 0 0 0
PlanRelease 20
MA GrossReq 20
SchedRec
SchedInvent 10 10 10 10 10 –10 -10 –10
PlanReceipts 10
PlanInvent 10 10 10 10 0 0 0
PlanRelease 10
MB GrossReq 40
SchedRec
SchedInvent 5 5 5 5 5 –35 –35 –35
PlanReceipts 35
PlanInvent 5 5 5 5 0 0 0
PlanRelease 35
PX GrossReq 35 20
SchedRec 25
SchedInvent 0 25 25 –10 –30 –30 –30 –30
PlanReceipts 10 20
PlanInvent 25 25 0 0 0 0 0
PlanRelease 10 20
PY GrossReq 10 20
SchedRec
SchedInvent 10 10 10 10 0 –20 –20 –20
PlanReceipts 20
PlanInvent 10 10 10 0 0 0 0
PlanRelease 20
PZ GrossReq 70
SchedRec 40
SchedInvent 5 5 45 –25 –25 –25 –25 –25
PlanReceipts 25
PlanInvent 5 45 0 0 0 0 0
PlanRelease 25
Figure 7.7 MRP schedule adjusted for inventory and scheduled receipts (abbreviations: see
Appendix 7.A).7.3 How Much to Order? 127
Item Period OnHand 1 2 3 4 5 6 7
FP GrossReq 20
L4L SchedRec
L=2 SchedInvent 0 0 0 0 0 0 0 –20
NetReq 0 0 0 0 0 0
PlanReceipt 20
20
PlanInvent 0 0 0 0 0 0 0
PlanRelease 20
MA GrossReq 20
nQ 8 SchedRec
L=1 SchedInvent 10 10 10 10 10 –10 –10 –10
NetReq 0 0 0 0 10 0 0
PlanReceipt 16
PlanInvent 10 10 10 10 6 6 6
PlanRelease 16
MB GrossReq 40
nQ 15 SchedRec
L=2 SchedInvent 5 5 5 5 5 –35 –35 –35
NetReq 0 0 0 0 35 0 0
PlanReceipt 45
PlanInvent 5 5 5 5 10 10 10
PlanRelease 45
PX GrossReq 45 32
MOQ 
25
SchedRec 25
L=2 SchedInvent 0 25 25 –20 –52 –52 –52 –52
NetReq 0 0 20 27 0 0 0
PlanReceipt 25 27
PlanInvent 25 25 5 0 0 0 0
PlanRelease 25 27
PY GrossReq 16 20
nQ 30 SchedRec
L=3 SchedInvent 10 10 10 10 –6 –26 –26 –26
NetReq 0 0 0 6 0 0 0
PlanReceipt 30
PlanInvent 10 10 10 24 4 4 4
PlanRelease 30
PZ GrossReq 90
MOQ 
40
SchedRec 40
L=2 SchedInvent 5 5 45 –45 –45 –45 –45 –45
NetReq 0 0 45 0 0 0 0
PlanReceipt 45
PlanInvent 5 45 00000
PlanRelease 45
Figure 7.8 MRP schedule adjusted for batch sizing rules (abbreviations: see Appendix 7.A).128 7 MRP Decoupling Point Control Systems
Again, the main figures important for the planner to know are the “planned
releases” for the coming period, i.e. period 1. The MRP-I-proposal in the example
would be to release (Figure 7.8):
– nothing for FP, MA, and MB;
– 25 for PX, 30 for PY, and 45 for PZ.
Remark: In the example of Figure 7.8, the on-hand inventory of MA, MB, and PY
at the start of period 1 might be based on an actual physical inventory of the
shown amount without any safety stock. However, the on-hand inventory might
also result from a higher physical inventory minus safety stock. For instance, the
situation for MA may have been:
– physical inventory: 30
– required safety stock: 20
resulting in an on-hand inventory of 10 at the start of period 1. As shown in the
example, that possible safety stock is ignored since MRP-I attempts to have that
amount of stock always available.
If the planner follows up the advice and indeed releases one order of 25 for PX and
one order of 45 for PZ, and nothing changes on the expected demands. Figure 7.9
shows the MRP schedule one period later (at the end of period 1). The orders that
have been released are now shown as “scheduled receipts.” All the documents
needed for those orders have been sent to the production units producing the orders
(or to the suppliers delivering the parts). The control of those orders has been
shifted from the MRP planner to the relevant unit under the assumption that the
order considered would be ready as planned in the MRP schedule.
7.4 Discussion on MRP-Related Issues
7.4.1 Dealing with Uncertainty
MRP-I works fine as long as all the assumptions made are not violated. The main
assumptions are:
– actual lead times do not deviate from (especially they should not exceed) the
planned lead times;
– required quantities in a particular period can be predicted accurately;
– delivered quantities do not differ unexpectedly from ordered quantities.
In other words, MRP-I assumes no uncertainty. The only way that MRP-I can deal
with uncertainties is to include slack. Slack in lead time means a planned lead time
that is longer than the average or expected lead time. Slack in quantities may mean
over-planning (ordering a larger amount than required) or safety stock (… indeed,
just in case!). The problem with safety stock is that MRP-I ignores that stock in the7.4 Discussion on MRP-Related Issues 129
Item Period OnHand 2 3 4 5 6 7 8
FP GrossReq 20
L4L SchedRec
L=2 SchedInvent 0 0 0 0 0 0 –20 –20
NetReq 0 0 0 0 0 20 0
PlanReceipt 20
PlanInvent 0 0 0 0 0 0 0
PlanRelease 20
MA GrossReq 20
nQ 8 SchedRec
L=1 SchedInvent 10 10 10 10 –10 –10 –10 –10
NetReq 0 0 0 10 0 0 0
PlanReceipt 16
PlanInvent 10 10 10 20 20 20 20
PlanRelease 16
MB GrossReq 40
nQ 15 SchedRec
L=2 SchedInvent 5 5 5 5 –35 –35 –35 –35
NetReq 0 0 0 35 0 0 0
PlanReceipt 45
PlanInvent 5 5 5 10 10 10 10
PlanRelease 45
PX GrossReq 45 32
MOQ 25 SchedRec 25
L=2 SchedInvent 0 25 5 –27 –27 –27 –27 –27
NetReq 0 0 27 0 0 0 0
PlanReceipt 27
PlanInvent 25 5 0 0 0 0 0
PlanRelease 27
PY GrossReq 16 20
nQ 30 SchedRec 30
L=3 SchedInvent 10 10 10 24 4 4 4 4
NetReq 0 0 0 0 0 0 0
PlanReceipt
PlanInvent 10 10 24 4 4 4 4
PlanRelease
PZ GrossReq 90
MOQ 40 SchedRec 40 45
L=2 SchedInvent 5 45 0 0 0 0 0 0
NetReq 0 0 0 0 0 0 0
PlanReceipt
PlanInvent 45 0 0 0 0 0 0
PlanRelease
Figure 7.9 MRP schedule one period later (abbreviations: see Appendix 7.A).130 7 MRP Decoupling Point Control Systems
• Orders planned to be released: 
– Too late for release
• Scheduled orders already released:
– Shift orders to right (“reschedule out”)
– Shift orders to left (“reschedule in”) 
Figure 7.10 Important exception messages.
calculations because it assumes it shouldn’t be used. This is known as “dead stock”
(see Visser and van Goor (2011)).
There is one other way MRP software deals with uncertainties and variations. If
after the MRP-I calculations, the software notices that the planner may have to take
a special action, it generates so-called “exception messages.” In Figure 7.10, some of
the most important exception messages are given.
If the MRP-I calculation shows that the order to be released next period is already
too late (assuming the regular lead time), then a message is given “too late for
release.” It is a signal for the planner to take extra actions, like calling the supplier
and trying to get the parts with a shorter lead time.
If the calculations show that an order that is already on its way may be postponed
to a later period (because the receipt is not needed yet in the period scheduled for
receipt), then a message is given: “reschedule out.” This might help the supplying
process of setting its priorities straight.
If the calculations show that a scheduled receipt is required earlier than originally
scheduled, the message is given: “reschedule in.” If possible, the delivery of that order
should be accelerated. All these issues emphasize the fact that in real life, an MRP
system can only really work properly if somebody is making the actual release and
rescheduling decisions.
For the MRP logic to work perfectly, the BOM must be completely correct and
updated. Otherwise, the wrong parts may be ordered at the wrong moment in the
wrong quantities. In case of situations with many different complex and changing
products keeping the BOMs up to date is an intensive job, requiring discipline in
administration and well-organized database support. This issue is addressed more
extensively in discussions on product data management and product lifecycle man￾agement.
7.4.2 Bill-of-Materials Versus Bill-of-Distribution
In distribution, a similar concept may be used, namely the bill of distribution. A bill
of distribution often looks like a BOM, but then turned around. On the top level, the
final destination points are located (e.g. retail shops). On the second level regional,
we may find Regional Distribution Centres. The lowest level in the Bill of Distribu￾tion is the starting point of the distribution network, e.g. the production location.
In a distribution context, these number of layers are called “tiers.” The network
represented in Figure 7.11 is a three-tier network. In such a situation, the logic of
MRP-I can still be used, with the main difference being that the gross requirements
for a part might be the sum of the planned releases of several parents. In Figure 7.127.4 Discussion on MRP-Related Issues 131
Central storage Regional DC’s Local situation
Figure 7.11 Distribution network with three tiers.
Period OnHand 1 2 3 4 5 6
Local1 GrossReq 20 10 30
nQ, Q=5 SchedRec
L=1 SchedInvent 10 10 10 –10 –10 –20 –50
NetReq 0 0 10 0 10 30
PlanReceipts 10 10 30
PlanInvent 10 10 0 0 0 0
PlanRelease 10 10 30
Local2 GrossReq 10 20
nQ, Q=5 SchedRec
L=2 SchedInvent 15 15 5 5 –15 –15 –15
NetReq 0 0 0 15 0 0
PlanReceipts 15
PlanInvent 15 5 5 0 0 0
PlanRelease 15
Local3 GrossReq 40
nQ, Q=5 SchedRec
L=1 SchedInvent 5 5 5 5 –35 –35 –35
NetReq 0 0 0 35 0 0
PlanReceipts 35
PlanInvent 5 5 5 0 0 0
PlanRelease 35
DC GrossReq 25 35 10 30
MOQ=2
5
SchedRec 25
L=2 SchedInvent 0 25 0 –35 –45 –75 –75
NetReq 0 0 35 10 15 0
PlanReceipts 35 25 25
PlanInvent 25 0 0 15 10 0
PlanRelease 35 25 25
Figure 7.12 Example of MRP-I in distribution context (abbreviations: see Appendix 7.A).132 7 MRP Decoupling Point Control Systems
an example is given of the logic followed by distribution requirements planning
(DRP), for the situation of a product with three local destinations (“Local1,” “Lo￾cal2,” “Local3”), served from one distribution center (“DC”). The logic used by DRP
is the same as the logic used for MRP.
Appendix 7.A MRP Formulas
GrReq(t): gross requirements during period t
SchedRec(t): scheduled receipt in (beginning of) period t
SchedInv(t): scheduled inventory at the end of period t
NetReq(t): net requirements during period t
PlanRec(t): planned receipts in (beginning of) period t
PlanInv(t): planned inventory at end of period t
PlanRel(t): planned release at the beginning of period t
Maxt: maximum period considered in the MRP schedule (“planning horizon”)
L: planned lead time
Q(t): order size determined at the beginning of period t (depends on the batch size
rule used).
7.A.1 Rescheduling Assumption
If for any t there is a SchedInv(t)<0, while at some t + n SchedInv(t + n)≥0 then
first the future schedule receipts are rescheduled to earlier periods such that this will
not happen anymore.
That prevents unnecessary starts of new orders when sufficient other orders are
already released.
Using this assumption, the formulas become:
SchedInv(t) = SchedInv(t − 1) + SchedRec(t) − GrReq(t)
NetReq(t) = max (
GrReq(t) − PlanInv(t − 1) − ∑
n={t,…,Maxt)
SchedRec(n), 0
)
PlanRec(t) = if (NetReq(t) > 0) then Q(t) else 0
PlanInv(t) = PlanInv(t − 1) + SchedRec(t) + PlanRec(t) − GrReq(t)
PlanRel(t − L) = PlanRec(t)
For the child items:
GrReq(t) = ∑
all p
(nc,p × PlanRelp(t))
with p: parent item
and nc,p: number of child items going into one parent item pReferences 133
References
Bertrand, J.W.M., Wortmann, J.C., and Wijngaard, J. (1998). Produktiebeheersing en
Material Management, 2e. Houten: Educatieve Partners Nederland BV (in Dutch).
Jacobs, F.R. and Chase, R.B. (2018). Operations and Supply Chain Management, 15e.
McGraw-Hill.
Silver, E.A., Pyke, D.F., and Peterson, R. (1998). Inventory Management and Production
Planning and Scheduling. New York: Wiley.
Visser, H.M. and van Goor, A.R. (2011). Logistics: Principles and Practice. Stenfert
Kroese.
Vollmann, B., Berry, W., and Whybark, D.C. (1997). Manufacturing Planning and
Control Systems, 4e. McGraw-Hill.135
8
Systems Using Echelon Stock (ESC, LRP)
Up to now (Chapters 6 and 7), we haven’t considered coordination of the stock
points. We only considered the local stock point when deciding upon ordering. The
situation was discussed where reordering was done with the assumption made that
decoupling point control would focus on the performance of each single stage, i.e.
for each decoupling point. In this chapter, we will discuss the situation where decou￾pling point control considers several stages (or units in a supply chain) at the same
time and mainly focuses on the performance at the last decoupling point, i.e. at the
moment of delivery to the customer. The idea for such an approach would be that
only the final delivery performance to the customer counts. We will address that sit￾uation as decoupling point control using Global Norms (instead of local norms). We
start with a discussion of some of the major issues of reorder point (ROP) systems
and material requirement planning (MRP) systems, both in terms of the positive
characteristics of these systems and in terms of drawbacks. Next, we will introduce
a system that combines the ideas of ROP systems (assuming instantaneous demand)
with the use of global norms, i.e. the echelon stock control system (ESC).1 A sim￾ilar approach is used in a system where the MRP approach is adapted to the use
of global norms; this is known as line requirements planning (LRP). In Section 8.2,
we will discuss for each of the system types when and how much to order. Finally,
in Section 8.3, we will discuss some of the prerequisites and possible drawbacks
of these systems.
8.1 General Discussion of Systems Using Global Norms
In this section, we will discuss the general characteristics of ESC and LRP systems;
however, before we do this, we will first briefly discuss ROP and MRP systems, since
this highlights the relevance of ESC Systems.
1 Echelon stock control systems are also known as base stock control systems (see, e.g. Silver et al.
1998). However, the term base stock is also used to refer to ROP systems using order-up-to levels
(especially for (s, S) systems with s = S −1). Therefore, we will use the ESC.
Production Control in Practice: A Situation-Dependent Decisions Approach, First Edition.
Henny Van Ooijen and Corné Dirne.
© 2024 WILEY-VCH GmbH. Published 2024 by WILEY-VCH GmbH.136 8 Systems Using Echelon Stock (ESC, LRP)
8.1.1 Discussion on ROP and MRP
Consider the situation shown in Figure 8.1. The supply chain shown consists of three
stages, each with a stock point and a process that delivers the item to that stock
point. These supply processes all have their lead times; the final customer receives
the finished products from stock at stage 1. For such a three-stage supply chain we
can use ROP systems for each stock point shown. That would mean, assuming an
(s, Q) system (see Section 6.2), that we would determine the Q and s for stage 1
(Q1 and s1), based on the demand for the items at that stage. Similarly, the demand
at stock point 2 would be used to determine Q2 and s2. Again, demand at stock
point 3 would be used to determine Q3 and s3. In de Kok (2018), this situation is
called single item single echelon situation (SISE). These ROP systems only use local
information (Timmer et al. 1984). In other words, at stage 2 the original demand
at stage 1 is not known or used. As a result, at stage 2 we only know the ordering
characteristics of stage 1, and these characteristics differ from the original, due to
batch sizing (Q1), time delay, and possible forecasting issues. The result is what is
known as the “bullwhip effect” an amplification of variation and uncertainty (see,
e.g. Section 9.2). A similar phenomenon takes place at stage 3, again increasing the
amplification of variation and uncertainty. The relations between items as given in
the Bill of Materials (BOM) are not used either. If at stage 1 there would be enough
stock to fulfill future demand for the coming periods, there might not be a reason to
order parts at stage 2 even though the inventory position is below the reorder level.
Also, when determining Q2, the batch size at stage 1 (Q1) is not considered. On top
of that, optimization, using service levels and safety stock, is done for each stage sep￾arately, resulting in an accumulation of slack and stock throughout this three-stage
system. However, the main performance for this supply chain is the service level at
stage 1 where the final customer is being served. For instance: P1 = 0.95 might be
the goal to be achieved (see Section 5.2), but that doesn’t mean that the same service
level would be required at stages 2 and 3. It might be true that a lower service level
at these stages (and thus less safety stock) would suffice. Still, in most cases also at
these stages, a similar high service level of P1 is used.
An alternative system to be used for the situation shown in Figure 8.1 is an MRP
system. In that case, the relations between the stages are being used by the relation
in the BOM. So, the demand at stage 2 is dependent on the demand at stage 1, and
the demand at stage 3 is dependent on the demand at stage 2. To make that work,
the BOM relation must be available and up to date (requiring a serious amount of
information processing and administrative work). Unfortunately, the MRP system
is deterministic in the sense that if safety stocks are defined, this stock is not used
in the calculations. Safety stock is considered “dead stock” as if it isn’t there. Also,
L2 L1 L3 L2
Stage
1
Stage
2
Stage
3
Figure 8.1 Three-stage linear supply chain.8.1 General Discussion of Systems Using Global Norms 137
often the safety stock levels (and the batch sizes) are determined locally, so stage
by stage, without considering demand, safety, and batch sizes at the other stages.
Moreover, MRP systems can become nervous systems requiring a lot of rescheduling
if the gross requirements used at stage 1 are to some extent based on (unreliable)
forecasts, resulting in a lot of changes (see also Timmer et al. 1984).
8.1.2 Echelon Stock Control Systems
The basic idea of an ESC system is that at any stage the inventory position at that
stage plus the inventories at downstream stages should be enough to cover the
demand during the lead times through all stages. For instance, at stage 3 from the
situation shown in Figure 8.1, the only moment reordering would be required,
is when the amount of items still available at that stage or at downstream stages
(stage 2 and 1) would not be enough to cover the demand during L1 + L2 + L3.
Where exactly these items would be located (at stage 3, at stage 2, or stage 1) is not
relevant, only if the total amount is insufficient, it would be wise to reorder. Similar
for stage 2: only if the number of stage-2 items available at stage 2 or downstream is
not sufficient to cover demand during L2 + L1 it would be good to reorder. Etcetera.
To use this idea, two new concepts should be introduced, i.e. the concept of echelon
stock and the concept of echelon lead times:
– Echelon stock refers to stock available at the stage considered plus at all stages
downstream. Like the definitions given in Chapter 2, the echelon inventory position
is the number of available items at stage j plus downstream stages PLUS the items
that have been reordered but have not arrived yet at stage j (Silver et al. 1998,
based on Clark and Scarf 1960 and Magee 1958). This echelon inventory position
increases when a new batch is reordered for the stage considered and decreases
when a customer order at stage 1 arrives and is accepted. In the example shown
in Figure 8.2, a two-stage supply chain is shown. Item A is the parent item that is
being sold to customers. It is made of item B in a production unit. Item B is bought
from a supplier. At the end stock point of this supply chain, there are three items
of A stored. We assume no customer orders at this moment. In the production
unit, two more items are being produced (so have been reordered in the past) but
have not arrived yet at that final stock point. At the stock point of B, four items
have been stored; five more items have been reordered at the supplier but have
not arrived yet. In this example:
– the echelon inventory position for A would 3+2 = 5; the available echelon
stock is 3;
Lead time: 1
Items: 2
Items:4 Items: 3
Lead time: 2
Items: 5
B A
Figure 8.2 Example Echelon stock.138 8 Systems Using Echelon Stock (ESC, LRP)
– the echelon inventory position for B would be 3+2+4+5 = 14; the available ech￾elon stock is 3+2+4 = 9.
– echelon lead time is the time required for all supply processes at the stage con￾sidered plus downstream stages. This lead time does not include waiting times at
the stock points, it only includes process lead times. There might be a difference
between the planned echelon lead time and the actual echelon lead time. In the
example of Figure 8.2, we have: (planned) echelon lead time for A is 1, and for B:
1+2 = 3.
Using these echelon stock levels and lead times, we can apply the same type
of reordering systems as we have seen for the single item, single-stage situation
(Chapter 7). So, again we have four different types of systems:
– (s, Q)
– (R, s, Q)
– (s, S)
– (R, s, S)
8.1.3 Line Requirements Planning
In an MRP system, planned orders for MPS items are “exploded” to create planned
order releases and quantities for components. This is the only information directly
available for the components; no information is available on what led to these dates
and quantities, which leads to a distortion of information concerning requirements
and inventories, resulting in left-over (remnant) stock and imbalances. A system
is out of balance if the replenishment moment of a component is not equal for all
parent products with the same component. An alternative for MRP that minimizes
the distortion of the information is LRP (e.g. see Stommels 1979; Büchel 1982; Van
Donselaar et al. 1987). LRP is primarily based on the echelon concept. The main
idea (as already discussed in the previous section) is to pass on the demand infor￾mation available at downstream stages to the upstream stages and to reorder as
soon as the echelon inventory is insufficient to fulfill the (time-phased) demand. The
logic is that it would not make much sense to add more parts downstream if the
amount already available would be sufficient to fulfill the (time-phased) demand. Of
course, this would only work if it were possible to downsize the planned release from
downstream units if not enough components are available (van Donselaar 1992). In
other words, LRP assumes either flexibility in batch sizes (and an extra check on
component availability before releasing orders) or small batch sizes (such that the
likelihood of nonavailability of components would be small). For the final stage (last
unit), the LRP schedule is identical to the MRP schedule. But it will be different for
units preceding the final unit.
In other words, MRP explodes only planned orders, based on requirements,
netting, off-setting, and lot-sizing considerations. After the explosion, the only
information directly available for the product, which uses the exploded require￾ments, is planned order release dates and quantities, but not the information that led
to these dates and quantities. The main advantage of LRP as compared with MRP is8.2 When and How Much to Order? 139
the fact that the distortion of information for requirements, buffers, and inventories
is minimal. LRP explodes inventory and requirements separately and in their basic
form; the requirements, e.g. are not transformed by lot-sizing. This contributes to
higher transparency. Requirements for the component are directly derived from
the requirements for the final products (FPs). Therefore, it is easier to see how the
requirements are built up. Moreover, changes (e.g. due to a simulation) in the FPs’
requirements are recognizable at the component level. Note that on the other hand,
this way of explosion also causes the loss of lot-sizing and scheduling information. As
a result, the main disadvantage of LRP in deterministic environments is the fact that
LRP structurally ignores restrictions like firm planned orders at downstream stages,
remnant stock, and imbalance. In general, LRP is too rough for a deterministic
environment.
8.2 When and How Much to Order?
When echelon stocks are used, the moment of ordering and the quantity ordered
will be different from the systems using non-echelon information (as discussed in
Chapter 6 (ROP) and Chapter 7 (MRP)). In this section, we will briefly discuss the
main differences.
8.2.1 When and How Much to Order in Echelon Stock Systems?
For the decision of when to order, we can use the same policies as described in
Chapter 7, however, instead of the lead time and the inventory position, we must
use the echelon lead time and the echelon inventory position. For instance, for a
two-stage supply chain as shown in Figure 8.3, assuming Li is the planned lead time
for stage i, and using an (s, Q) or (s, S) system with a P1-service level, we get:
s1 = E(D(L1)) + zP1 ⋅ 𝜎(D(L1))
s2 = E(D(L1 + L2)) + zP1 ⋅ 𝜎(D(L1 + L2)) (8.1)
How much to order can be determined using the same considerations as in the case
of a single-stage ROP system (see Sections 6.3.1 and 6.3.2).
8.2.2 When and How Much to Order in Line Requirements Planning
Systems?
The logic to determine when to order in an LRP-controlled system is similar to the
one used in an MRP system: at the moment the planned inventory turns negative, a
Figure 8.3 Two-stage linear
supply chain.
Stage 2 Stage1 
L2 L1
Q2 Q1
s2 s1140 8 Systems Using Echelon Stock (ESC, LRP)
new planned receipt is required to prevent that from happening. The only difference
is that for the calculations made echelon lead times must be used instead of (single
stage) lead times, and echelon stocks must be used instead of (local)stock.
How much to order when an order is needed depends both on the demand to
be fulfilled and the batch sizing rule used. The LRP system derives the require￾ments directly from the demand for the FP. Because ordering decisions are based on
end item demand and not on orders from the next level downstream, the demand
has less variability, and thus lower safety stocks would be needed. As explained in
Section 5.4.2, it would be wise to make sure that Q2 =n ⋅ Q1 (with n being an integer,
thus 1, 2, 3 …). The determination of Q1 and n, assuming such interdependent batch
sizes, using BOM relations, is already discussed in that section.
If the environment is deterministic, LRP differs from MRP in the way remnant
stock and imbalance are handled. Note that LRP ignores lot-size restrictions in the
explosion of requirements. Consequently, LRP implicitly assumes that the remnant
stock can always be used effectively.
To illustrate how an LRP system works, we will discuss this for the example we
also used in Section 7.3. The LRP calculations can be found in Figure 8.4. Note that
the gross requirements for every component are related to the gross requirements of
the FP, multiplied by the number of components needed for that product, and offset
for the echelon lead time. All stocks are calculated from an echelon point of view.
Also, all lead times are echelon lead times. In other words,
– the on-hand echelon stocks are:
for FP: 0
for MA: 10+1 × 0 = 10
for MB: 5+2 × 0 = 5
for PX: 0+2 × 10+5+4 × 0 = 25
for PY: 10+1 × 10+1 × 0+2 × 0 = 20
for PZ: 5+2 × 5+4 × 0 = 15
– the echelon lead times “downstream” are:
for FP: 0
for MA: 2
for MB: 2
for PX via MA: 1+2 = 3
for PX via MB: 2+2 = 4
for PY in FP: 2
for PY via MA: 1+2 = 3
for PZ: 2+2 = 4
The “echelon lead times downstream” are used to offset the gross requirements
for the component considered. By doing so, two problems are prevented:
– excess stock downstream due to batch-sizing;
– ignorance of the inventory status information from downstream stages.
Moreover, as with the ESC system described in the previous section, the echelon
inventory position will be used.8.2 When and How Much to Order? 141
Item Period OnHand 1 2 3 4 5 6 7
FP GrossReq 20
L4L SchedRec
L=2 SchedInvent 0 0 0 0 0 0 0 –20
NetReq 0 0 0 0 0 0 20
PlanReceipt 20
PlanInvent 0 0 0 0 0 0 0
PlanRelease 20
MA GrossReq 20
nQ 8 SchedRec
EL(FP)=2 Ech.SchedInvent 10 10 10 10 10 –10 –10 –10
L=1 NetReq 0 0 0 0 10 0 0
PlanReceipt 16
PlanInvent 10 10 10 10 6 6 6
PlanRelease 16
MB GrossReq 40
nQ 15 SchedRec
EL(FP)=2 Ech.SchedInvent 5 5 5 5 5 –35 –35 –35
L=2 NetReq 0 0 0 0 35 0 0
PlanReceipt 45
PlanInvent 5 5 5 5 10 10 10
PlanRelease 45
PX GrossReq 40 40
MOQ 25 SchedRec 25
EL(MA)=3 Ech.SchedInvent 25 50 50 10 –30 –30 –30 –30
EL(MB)=4 NetReq 0 0 0 30 0 0 0
L=2 PlanReceipt 30
Ech.PlanInvent 50 50 10 0 0 0 0
PlanRelease 30
PY GrossReq 20 20
nQ 30 SchedRec
EL(FP)=2 Ech.SchedInvent 20 20 20 20 0 –20 –20 –20
EL(MA)=3 NetReq 0 0 0 0 20 0 0
L=3 PlanReceipt 30
Ech.PlanInvent 20 20 20 0 10 10 10
PlanRelease 30
PZ GrossReq 80
MOQ 40 SchedRec 40
EL(MB)=4 Ech.SchedInvent 15 15 55 –25 –25 –25 –25 –25
L=2 NetReq 0 0 25 0 0 0 0
PlanReceipt 40
Ech.PlanInvent 15 55 15 15 15 15 15
PlanRelease 40
Figure 8.4 LRP schedule for example of Section 7.3.142 8 Systems Using Echelon Stock (ESC, LRP)
Remark: LRP assumes that the local inventory of PX, equal to 25 units, can be
made effective by releasing an order equal to 25 units. Of course, this depends
on the flexibility of the lot size for product MA. If the lot size of MA is flexible, this
is the best advice. If, however, the lot size of MA should always be exactly equal
to (a multiple of) eight products, then LRP is wrong. Sometimes the planning
according to LRP is too rough: LRP does not consider lot-size inflexibility.
8.3 Discussion on Echelon Stock Systems
The concept of echelon stock is strong. Indeed, it does deal with some of the issues
of ROP and MRP systems in supply chains. In case the decoupling points (stock
points) are controlled by one company, then sharing the information over the supply
chain, like the way that is done with MRP, should not be a problem. Of course, if the
decoupling points are divided over two or more independent companies, then the
issue of sharing information becomes more problematic. That is generally speaking
one of the major difficulties in achieving supply chain optimization.
If the supply chain considered is not linear, the ESC or LRP system becomes
more complicated. For instance, in the case of a divergent supply chain as shown in
Figure 8.5, having enough echelon stock at stage 2 doesn’t guarantee that both stock
points at stage 1 can be provided with enough items. If for instance many items are
distributed to stock point 1.1, then the echelon stock might suggest also at stage 2
that no order is needed, even though the possible requirement for stage 1.2 may not
be fulfilled. Consider the following simplified example, assuming a supply chain as
shown in Figure 8.5.
– echelon stock for stock point 1.1: 7; reorder level for that stock point: 6;
– echelon stock for stock point 1.2: 3; reorder level for that stock point: also 6;
– echelon stock for stock point 2:
first part: inventory position at that stock point: 2
added to that: the inventory positions of stock points 1.1 and 1.2 (7,
respectively, 3)
total: 12
– reorder level at stock point 2: 10
Stage 1
Stage 2
L2
L1.1 Q2
Q1.1
s2
s1.1
Stage 1
L1.2
Q1.2
s1.2
Figure 8.5 Example of a two-stage
divergent supply chain.References 143
In this situation, there would be no reorder from stock point 1.1 (7>6). Stock point
1.2 would reorder (3<6), with an order size of at least 3 (depending on the batch size
used). At stock point 2, no reordering would be done because the echelon stock is
sufficient to cover the reorder level (12>10). But the problem is that the number of
items available at stock point 2 is 2 at most. That number is insufficient to fulfill the
requirement from stock point 1.2. The reason for this issue is the unbalance in stock
levels in the two branches of this supply chain. One possible solution for issues like
these might be to relocate stock from one stock point to the other. However, that is
not always easy to do and requires advanced information system support.
An ESC or LRP system is still basically a material-oriented system. So, there is no
capacity planning included. For ESC systems on top of that, there is no time-phased
projection or forecast of demand being used (as is the case in LRP systems). That also
means that there is no need for frequent demand forecasting. One of the big advan￾tages of an ESC or LRP system is that reordering is always based on end demand, so
no amplification of demand will happen. That also means that the calculation of the
parameters used is based on the end demand, and not on the demand observed at
the stock point considered. This will reduce the “bullwhip effect” to a large extent.
Also, any slack already available downstream (extra stock, extra planned lead times)
is taken into consideration at upstream stock points.
References
Büchel, A. (1982). An overview of possible procedures for stochastic MRP. Engineering
Casts and Production Economics 6: 43–51.
Clark, A.J. and Scarf, H. (1960, 1960). Optimal policies for multi-echelon inventory
problem. Management Science 6: 475–490.
van Donselaar, K.H. (1992). The use of MRP and LRP in a stochastic environment.
Production Planning & Control 3 (3): 239–246.
van Donselaar, K., Jenniskens, F., and Timmer, J. (1987). Line requirements planning:
alternatief voor MRP?’. Tijdschrift voor Inkoop en Logistiek 3 (11 en 12): (in Dutch).
de Kok, T. (2018). Inventory management: modeling real-life supply chains and
empirical validity. Foundations and TrendsR in Technology, Information and
Operations Management 11 (4): 343–437.
Magee, J.F. (1958). Production Planning and Inventory Contro. McGraw-Hill.
Silver, E.A., Pyke, D.F., and Peterson, R. (1998). Inventory Management and Production
Planning and Scheduling. New York: John Wiley & Sons.
Stommels, F.J. (1979). Evaluatie van een produktieplanningssysteem voor een keten met
grote onzekerheden (samen leren leveren). (in Dutch) PhD thesis. Eindhoven:
Eindhoven University of Technology.
Timmer, J.P.J., Monhemius, W., and Bertrand, J.W.M. (1984). Production and inventory
control with the base stock system (EUT—BDK report. Dept. of Industrial Engineering
and Management Science; Vol. 12). Eindhoven: Technische Hogeschool Eindhoven.145
9
Choosing an Appropriate DPC System
In the Chapters 6–8, several methods for planning and controlling decoupling points
have been discussed. A distinction has been made between “Just-in-Case”-methods
where stock is being kept at a decoupling point just in case a demand may occur
for the item stored, and “Just-in-Time”-methods where future orders or forecasts on
demand are used to plan the release of new orders in such a way that the products
demanded are available just at the moment they are required. Also, for the different
methods discussed, several parameters have been described that should be used in
these methods (like service levels, safety stocks, and planned lead times). In this final
chapter of Part II of this book, we will discuss the choice of a method and parameters
to be used in a particular situation (see also van Donselaar 1989; Timmer et al. 1984).
9.1 General Considerations
The main consideration when choosing a method for planning and controlling stock
in decoupling points is the service level required versus the costs to obtain that
service level. In many models studied in the literature, the issue of “service level”
is represented in the models by including costs for back-ordering or loss of sales. But
in a lot of real-life situations, these costs are not easy to determine, and often, it is
not straightforward that these costs can be represented by one single cost parame￾ter. One might wonder if “only” costs would be the right performance aspect. Less
easy quantifiable issues like “reputation” and “trust” may also be relevant. As a
result, an approach often chosen in real life is to define a service level that is accept￾able both from the (external) customer perspective and from the (internal) delivery
perspective.1
Defining the required service level is not easy. Considerations are usually
market-related. Who has what position in the supply chain? How important is
the customer for the supplying company in consideration? Are there alternatives
for the customer (or the supplier)? What are the safety and legal consequences
of late deliveries? The definition and choice of service level are also dependent
1 Note that what should be considered to be “external” and “internal” depends on the supply
chain considered.
Production Control in Practice: A Situation-Dependent Decisions Approach, First Edition.
Henny Van Ooijen and Corné Dirne.
© 2024 WILEY-VCH GmbH. Published 2024 by WILEY-VCH GmbH.146 9 Choosing an Appropriate DPC System
on the strategic choices of the supplying company: what are the long-term goals
to be achieved? What internal strategic choices have been made, for instance, on
organizational structures and budgeting, that should be considered as prerequisites?
That is why, in many cases, the choice for the service level should be considered as
a general and strategic managerial decision; the methods and models explained in
this book can be used to show the consequences of these choices (and thus create
“what-if”-scenarios for managers to consider).
Assuming a certain required service level has been defined, which of the four
methods from Figure 6.4 would be applicable? All four methods can be used. In all
four situations, a high service level is obtainable. However, the consequences would
be very different in terms of the complexity and costs involved. Which method would
be best is context-dependent. The two main questions that have to be answered for
the choice of a suited control system are:
– Is it beneficial to coordinate the decoupling points?
– If applicable, should we use a program-controlled method, or is it better to use a
stock-controlled method?
In Section 9.2, we first discuss the advantages and disadvantages of the differ￾ent Decoupling Point Control methods; in Section 9.3, we will discuss under which
circumstances which method to use.
9.2 Advantages/Disadvantages of the Different DPC
Systems
Stock-controlled systems or reorder point systems (ROP/ESC) are often used in
situations where the market requires nearly zero delivery time and where the actual
customer demand is not known before production starts. Production then takes
place to stock, replenishing the stocks as actual demand materializes. Stock norms
in these systems are often (but not necessarily) based on statistical analyses of
past demand.
The advantage of a ROP method is that it is simple to use and only needs local
historical information. However, the disadvantage is that possible available informa￾tion about downstream stock points and future demand is not considered; the stock
points are not coordinated. Often, the demand depends on the demand of the next
decoupling point (dependent demand), and this information can be used, leading to
less distortion of information compared to systems with no coordination.
The disadvantages of not using available information about downstream stock
points (often we say that there is no coordination of the stock points) have been
analyzed by, amongst others, Forrester (1961), Brown (1977), and Magee (1958). It
has been shown that the major disadvantages can be reduced by basing the produc￾tion decisions in all production units on the same demand information, which is the
basis of the echeclon stock control (ESC) method. The whole chain of decoupling
points downstream is seen as one (echelon) stock point. If information is delayed,
no information on downstream future requirements is used, and no information on9.2 Advantages/Disadvantages of the Different DPC Systems 147
downstream (lot-size) inventories are used, and variations are amplified, this might
lead to the so-called bull-whip effect (see Orlicky 1975; van Aken 1978): amplification
of variations in demand at the most downstream stock point when determining the
demand for upstream stock points, either based on wrongly changed expectations
(resulting in changes in the parameters used) or based on the amplification through
batch sizes.
9.2.1 Bullwhip Effect
This happens when, for a decoupling point, changes are made in how much of a good
to order from its predecessor based on a small change in real or predicted demand
for that good. Due to not having full information on the demand shift, the decou￾pling point might increase its orders from its predecessor by an even larger extent,
and this predecessor, being even more removed, will change its production by a still
larger amount, and so on. The term is derived from a scientific concept in which
movements of a whip become similarly amplified from the origin (the hand cracking
the whip) to the endpoint (the tail of the whip). The bullwhip effect typically travels
from the end-product level up the supply chain to the raw materials level. This can
lead to excessive investment in inventory, lost revenue, declines in customer service,
delayed schedules, and even layoffs or bankruptcies.
The danger of the bullwhip effect is that it amplifies inefficiencies in a supply chain
as each step up the supply chain estimates demand more and more incorrectly. This
can lead to excessive investment in inventory, lost revenue, declines in customer
service, delayed schedules, and even layoffs or bankruptcies.
The bullwhip effect generally distorts the process in one of two ways. First is when
the original order change by retailers is due to an inaccurate demand forecast. The
size of this error tends to grow as it progresses further up the supply chain to the man￾ufacturer. The second is when a retailer has the correct information about demand,
but it leads to incorrect conclusions about information as to the reason, and details of
the retailer’s order change are lost, leading to incorrect assessments by wholesalers,
which are then magnified further up the chain.
Example 9.1
Imagine a pub where typically 10 fusts of beer are sold a week. On a particularly
warm week, 12 fusts are sold instead. Mistaking the immediate increase in sales
for a broader trend, the barman orders 15 fusts from the distributor. The distrib￾utor sees the increase and expands its purchase order with the manufacturer
to anticipate increased requests from other retailers as well. The manufacturer
increases its manufacturing run in anticipation of greater product requests in
the future.
At each stage above, demand forecasts have been increasingly distorted. If the
barman sees a return to normal beer sales when the weather returns to normal,
it will suddenly find itself with more supplies than needed. The distributor and
manufacturer will have even more excess inventory.148 9 Choosing an Appropriate DPC System
Another consequence of the lack of information is that larger logistics opera￾tions at the wholesale level take longer to change, meaning that the conditions
that caused a change in demand at the retail level may have passed by the time
a wholesaler has reacted. As changing manufacturing output takes longer still,
and information from retailers is even more delayed in getting to manufacturers,
the difficulty of reacting correctly to changes in demand increases even more.
Even if the barman had accurately assessed demand, for example, due to the
start of a local beer festival, the bullwhip effect could still occur. The distributor,
not being fully aware of local conditions, may assume this is due to a broad
increase in the demand for beer rather than specific conditions for that barman.
The manufacturer, being even more removed from the situation, would be even
less likely to understand and correctly react to the change in demand.
In the example above, the manufacturer may be stuck with a significant surplus of
products. This can lead to disruptions in the supply chain and in that manufacturer’s
business, increased costs associated with storage, transportation, spoilage, losses of
revenue, delays to shipments, and more. The distributor and the pub owner in the
example may also see similar problems.
The bullwhip effect can be difficult to identify in real-time, in part because it is
caused by a lack of communication throughout a supply chain. Frequently, it is a
phenomenon that is observed after the fact when inefficiencies have already been
created. There are many things firms in a supply chain can do to prevent, or at least
reduce the likelihood and severity of, a bullwhip effect. First and foremost, they can
ensure clear and consistent communications between companies up and down the
supply chain. This will help prevent temporary or localized shifts in supply from
being misinterpreted as broader than they are. Firms can also make sure to take
a wider viewpoint when making forecasts for demand to reduce the effect of any
temporary or limited shifts. Finally, companies can work to increase the speed at
which they can respond to shifts in demand, meaning that they can readjust more
easily if they incorrectly assess demand. This also reduces the need to overproduce
or overorder to have a buffer in case demand shifts.
As already said, it has been shown that the major disadvantages (variance ampli￾fications, etc.) can be reduced by basing the production decisions in all production
phases on the same demand information (like the ESC method does). This especially
holds for multi-stage systems with (large) fluctuations in customer demand and/or
(large) lot sizes in downstream stages, lumpy demand, large lot sizes in downstream
stages, substantial variations, or non-stationary (changing) demand.
No coordination, only using local stock information, can lead to too much safety
stock. Most control systems determine an optimal quantity slack for each decou￾pling point. In doing so often, we do not take into consideration the amount of slack
created in subsequent decoupling points. This can lead to a cumulation of safety
stock and, with that, to an amount of slack in the whole system, which is larger
than necessary. This can be avoided by concentrating all slack at one level (often
the most downstream level, but that is not necessarily always the best level). By the9.2 Advantages/Disadvantages of the Different DPC Systems 149
coordination of the decoupling points, this cumulation of slack is avoided. Because
production is controlled based on echelon stock position and echelon process lead
time, the optimal safety stocks or times (slacks) are not calculated per decoupling
point, but an optimum for a chain of decoupling points is determined. When deter￾mining the reorder level of a certain item, always the amount of safety stock (or time)
required to absorb uncertainty in a chain of decoupling points is considered.
So, the advantages of ROP/ESC are that it is relatively simple to use, no forecast is
required (stationary demand), and fast-forwarding of downstream information. The
disadvantages are that it doesn’t take into account trends and/or seasonality, doesn’t
take into account imbalance in divergent systems, and doesn’t consider batch size
restrictions that might consist downstream. Program-controlled systems like materi￾als requirement planning (MRP) are often used in situations where production takes
place to customer order or where production can take place according to a master
production schedule for finished products (or for their first-level components). This
master production schedule (MPS) gives for a certain number of periods in the future
the expected demand. Based on this schedule, time-phased-gross material require￾ments can be calculated and translated to net requirements by subtracting stocks on
hand and scheduled receipts at the various stock points. The logic used in MRP is
rather straightforward, making it understandable to all involved, and, as explained,
MRP uses information on future demand (or forecasts) if available.
On top of that, MRP uses the concept of dependent demand, which can be consid￾ered as a special way of creating forecasts. However, it assumes that data used, like
lead times for the supplying units and demands in a period, are accurate and do not
change over time. The performance of MRP decreases rapidly in a stochastic envi￾ronment; see, e.g. van Donselaar (1989); MRP is too rigid. It has been demonstrated
that in a stochastic environment:
– MRP is unable to cope with lot size flexibility; as a consequence, it can’t deal effi￾ciently with (minor) variations due to random scrap, data inaccuracy, and period
order quantity (POQ) policies.
– MRP doesn’t support the joint use of safety stocks for successive stages.
Moreover, MRP (and line requirements planning [LRP]) can’t use safety stock to
handle uncertainty, since it assumes that safety stock should always be there. Safety
stock takes away inventory from the inventory position. Uncertainty leads to a lot of
nervousness; often, the planning for a certain product in a certain period changes
(a lot) when a new MRP run is done, and the planner gets a lot of exception mes￾sages. One way of dealing with this is to freeze the planning as much as possible. The
use of MRP forces the company to use an inflexible production program (MPS). For
flexible behavior toward the market, large safety stocks at the MPS level are required.
Another way would be to use smaller lot sizes: the smaller the lot size, the less ner￾vousness we have.
Comparing ROP/ESC with MRP I, we observe that ROP/ESC has no MPS, no
time-phased material requirements calculation from an MPS, no planned orders,
and less rescheduling. ROP/ESC shows much less “nervousness” because of the
reduced “rescheduling” of orders already released, as often happens in MRP150 9 Choosing an Appropriate DPC System
systems. One reason for this is, that a better use can be made of safety stocks
as mentioned before. Another reason is that in ROP/ESC, it is not necessary to
create planned orders for items at a higher BOM level except for the items at
LLC02 before orders can be planned and released for items at a lower BOM level
(“upstream”). These orders “in-between” LLC0 and the LLC from the item con￾cerned have no direct relation. This avoids nervousness. Orders are created at the
latest allowable moment, given the echelon process time, based on demand forecast
and uncertainties known at that moment. These orders do not exist before that
moment as “planned orders,” like in MRP. That is why ROP/ESC can cope better
with stochastic demand (or stochastic components in demand) or, in any case,
simpler. This applies to the use of safety stocks as well as safety time. Therefore,
the software can be simpler and fewer computing and updating actions are needed.
Moreover, less time of planners is needed because the number of “exception
messages” can be much lower. For the same reason, the decision structure and way
of operation are simpler. Replenishment orders are released only at the moment it
is required by the echelon stock position. Collings and Whybark (1982) have proven
the importance of good insight, understanding, and training for the actual users of a
system. In that light, it is important that ESC is simpler to understand, handle, and
implement.
Similar to ESC, LRP explodes detailed information on requirements on LLC0-level
to items lower in the BOM. This detailed information enables LRP at the component
level to react sensibly to circumstances, which deviate from the assumptions made
at the parent product level. Because of this way of determining requirements on a
lower BOM level, LRP has a main advantage in that the information distortion due
to batch sizing and inventories is minimal. Since requirements for the component
are directly derived from the requirements for the final products, it is easier to see
how the requirements are built up. Changes in the final products’ requirements
are recognizable at the component level, which gives the planner a better view
of the flexibility in the system (with MRP, the only information available at the
component level are the gross requirements on the component level). The result is
a more stable plan. A drawback of LRP is that the planning logic is based on the
assumption that batch sizes are 100% flexible, i.e. that it is ok to adjust batch sizes
if the amount of components available is less than a full batch size. Note that this
drawback is limited if the component’s batch size is an integer multiple of the MPS
product’s batch size (see Wijngaard and Wortmann 1985). The main disadvantage
of LRP is the fact that LRP structurally ignores restrictions like firm planned orders
at downstream stages, remnant stock, and imbalance in divergent downstream
stages.
9.3 Which Decoupling Point Control System to Use?
If the choice already has been made either for a make-to-stock situation or a
make-to-order/make-to-forecast situation, for instance, based on the required or
2 LLC stands for “Low-Level-Code,” see Section 7.2.9.3 Which Decoupling Point Control System to Use? 151
possible delivery times, then the following considerations are helpful in the decision
of what type of system to use:
– In case of an make-to-stock situation, an ROP system would be preferred because
of its simplicity and easiness of use (and maintenance of the data), assuming that
the circumstances are appropriate for such a system. Circumstances are appro￾priate when the items concerned are cheap (thus lowering the inventory holding
costs), have stable continuous requirements, demand is, to a large extent, inde￾pendent, and the items can be ordered in small lot sizes. ROP systems are less
suited for multi-stage systems where these conditions are unmet. Instead, in these
multi-stage systems an ESC control method should be used, thus reducing, among
others, the variance amplification and the bullwhip effect.
– In an make-to-order situation, we often know the demand or have a forecast of the
demand for several future periods. If the environment is more or less determinis￾tic (the performance of MRP decreases rapidly in a stochastic environment, see,
e.g. van Donselaar 1989), the use of MRP/LRP can be beneficial since it reduces
remnant stock: why should we produce more components than required for the
end items? The more convergence there is in the chain (the more components are
required for one final product), the greater the effect of basing the control of the
components on unambiguous demand (and inventory) information. If we have
a stochastic environment (with a lot of uncertainty), MRP is less suited: as soon
as the production environment becomes uncertain or the demand forecasts used
are less reliable, the performance when using MRP decreases rapidly. To absorb
uncertainty, both safety stocks, safety time, and rescheduling are needed. The con￾cept of echelon safety stocks can be used to determine the required safety on each
level. However, the strict logic used in MRP I forces us to focus nearly entirely on
rescheduling. If we have a stochastic environment with dynamic lot sizes (variable
order quantities), LRP might be better since it prevents an overreaction, because,
in general, LRP does not take into account any lot-size restriction of the parent
product when the component is planned.
If no decision is made yet on whether to supply or produce on stock or order, three
main types of characteristics of the situation at a decoupling point can be used for
the decision on which planning and control method and what parameters to use:
– The length and reliability of the lead time of the supply or production of a part
and its predictability should have an impact on the method and parameters to be
used. Short lead times tend to give preference to methods where the supply or
production is started on an order, while longer lead times more often require some
stock to be on hand to cover that longer period. Also, more reliable lead times
require less slack (either in stock as safety stock in case of supply on stock or in
time when supply is done on order). Vice versa: in the case of unreliable lead
times, the extra time planned, or the extra amount of safety stock required might
result in a preference for another method or parameters. Depending on the level
of detail required, we can have nine categories (see Figure 9.1) or just four (only
L1, L3, L7, and L9).152 9 Choosing an Appropriate DPC System
Reliable In between Unreliable
Short L1 L2 L3
In between L4 L5 L6
Long L7 L8 L9
Lead time reliability
Lead time 
Figure 9.1 Categories based on lead time characteristics.
– The demand pattern for the item considered and its predictability should have an
impact as well on the choice of method and parameters to be used. Items with a
demand that is hard to predict should be planned and controlled differently from
items with predictable demand. For instance, in the latter case, releasing orders
based on forecasted demand instead of on available stock levels might be more
logical. Items with a high yearly volume and a frequent demand are to be treated
differently from items with a low frequency and/or a low yearly volume. In the
latter case, it might be worthwhile to consider producing on order.
It should be noted that both the time scale to be used to adequately describe
the demand pattern is context specific as well as the concept of “volume.” The
time buckets used to determine the release of orders define the time scale that
should be used. In some situations, that might mean that the demand pattern
should be described in numbers per week or even per month, while in other sit￾uations, demand per day, or even per hour, should be used. Demand per month
might be rather constant, but the demand per day may vary a lot. The variation
coefficient of the demand during the relevant time unit is a good indicator of this
“variability”. Also, “high volume demand” is context-specific. It usually refers to
the situation where the item is being demanded in reasonable amounts at least
each time bucket, whereas in the case of “low demand” the demand per time
bucket would frequently (or even most of the times) be zero. Also, in case of “high
demand” a relatively large part of the capacity available in production is required
to fulfill that demand; it may even become possible to create dedicated produc￾tion units for the items considered. In other words, the fact that 1000 products are
required in a year doesn’t necessarily mean that volume is “high” or “low”: that
can only be determined in relation to the other items required in that year and the
impact this demand has on the capacity used.
Again, depending on the level of detail required, nine categories may be chosen
(as indicated in Figure 9.2) or we can limit the number of categories to four (D1, D3,
D7, and D9).
– The costs related to the choice should be considered when choosing a method and
parameters.
Three types of costs should be included:
– stock holding costs;
– ordering costs;
– costs related to the method of planning and control.9.3 Which Decoupling Point Control System to Use? 153
Predictable In between Unpredictable
Low D1 D2 D3
In between D4 D5 D6
High D7 D8 D9
Demand predictability
Demand volume
Figure 9.2 Categories based on demand characteristics.
High stock holding costs result in a preference for supply on order, while high
ordering costs tend to increase the batch sizes ordered and thus supply on stock.
An ROP method is usually cheaper to set up and maintain than an MRP-based
method, making this latter method more relevant for more expensive items (with
irregular but predictable demand). The first two cost elements (holding and order￾ing costs) are the item-related cost elements to be considered when choosing a
method and model for planning and control. Using these two cost elements, again,
we may choose nine or just four categories (see Figure 9.3).
Using the LDC (lead-time, demand, cost) -categorization suggested above, each
item can be typified in terms of its L-category, D-category, and C-category. The
number of categories can be anywhere between 64 (= 4 × 4 × 4) and 729 (= 9 × 9 × 9).
That is a lot! In a practical situation, this can only be handled efficiently and prop￾erly if done by a computer. However, there are categorization models available with
fewer categories. One of the most known models for categorization is the ABC–XYZ
categorization. In this model, two types of categories are distinguished, i.e. the
ABC–categorization and the XYZ-categorization.
The ABC categorization (see Figure 9.4) often is based on the sales volume
(expressed in €-value). Sales volume is determined by the combination of demand
volume and price per item. The percentage of the total sales volume covered by
the sales of one item determines the categorization of that item in terms of ABC.
A-category items are the items with the largest sales volume covering most of the
total sales value in a year, e.g. 70–80%. For an item to be classified as an A-item
either the demand volume should be high, or the price of the item should be high
(or both). This results in a sort of Pareto-analysis, where 10–20% of the items cause
around 70–80% of the sales volume. As stated in Hopp and Spearman (2008):
Low In between High
Low C1 C2 C3
In between C4 C5 C6
High C7 C8 C9
Ordering costs
Holding costs
Figure 9.3 Categories based on cost characteristics.154 9 Choosing an Appropriate DPC System
90%
75%
100%
Items
(Ordened from high sales volume to low)
% of total
sales volume
A-items B-items C-items
Figure 9.4 ABC-analysis, based on sales volume.
The quantity portions of materials in Class A are usually approximately 10%,
while the value portion represents approximately 70%. This means that those
materials are the most important ones and have the biggest potential for
optimization. The quantity portion of materials in Class B is 25%, whereas
Class C contains 65% of the materials. Thus, C materials occur most often.
However, the value of 15% only is the smallest one.
The exact division of items between A, B, and C is not objectively determined but
is more or less arbitrary and depends on context-specific choices (see also Silver et al.
1998). From a cost perspective, inventory optimization for A-items may have a bigger
impact than optimization of the inventories for C-items. In other words, it may be
worthwhile to plan and control A-items more closely, and to reduce the amount of
effort put into the planning and control of C-items. In the C-item category, there may
also be items included with no demand at all in the last couple of years; if that is the
case, removing these items from inventory might be considered.
The second categorization used is the XYZ-categorization based on the variabil￾ity and predictability of demand. X-items have a constant, hardly changing demand
over time. Z-items are used only incidentally. In other words, demand varies a lot,
and often, demand in a period is “zero.” The Y-items have a demand pattern between
these two categories. The distinction between these classes can be based on the vari￾ation coefficient3 (vc) of the demand per relevant time unit, for instance:
– X-items: vc <0.1
– Y-items: 0.1≤ vc ≤0.3
– Z-items: vc >0.3
3 The variation coefficient of the demand can be determined by dividing the standarddeviation of
the demand per time unit by the average demand per time unit. The time unit to be used should be
relevant for planning purposes, so should be related to the relevant time bucket used in planning
and control.9.3 Which Decoupling Point Control System to Use? 155
1 2
0
2
4
6
8
10
12
3 4 5 6 7 8 9 10
vc = 0.1
vc = 0.3
vc = 0.8
Figure 9.5 Example of demand pattern for different values of vc.
Figure 9.6 ABC–XYZ categories.
AX BX CX
AY
AZ
BY
BZ CZ
CY
X
Y
Z
ABC
In Figure 9.5 an example is shown of a demand pattern typically connected to this
categorization of X-, Y- and Z-items, using the vc-categorization as shown above.
The ABC–XYZ-categorization results in nine possible categories of items (see
Figure 9.6).
The ABC-categorization can also be created based on demand volume (not value).
Such a categorization emphasizes the importance of the quantities sold, for instance,
because that relates to the hours that need to be spent on items (more than value
does). For the XYZ-categorization, instead of variation in demand, the predictabil￾ity of the demand might be used. Predictability refers to the possibility of forecasting
the demand, even if there is variation. Note that dependency in demand, e.g. because
of the Bill-of-Material relation between two items, may make demand predictable.
A measure for that predictability might be the mean absolute deviation [MAD; see
formula (5.20) in Section 5.3.2] divided by the average demand. Using such a classi￾fication, the general guidelines shown in Figure 9.7 can be followed.
The ABC–XYZ categorization is much less detailed than the one suggested in the
combination of Figures 9.1–9.3. The biggest difference is the inclusion of the char￾acteristics of the supply of the items in the overall categorization. Also, the ordering
costs are not included in the categorization. Inventory holding costs are only indi￾rectly included since, in many cases, the assumption is that inventory holding costs
can be determined as an interest rate x the item value; that item value is, to some156 9 Choosing an Appropriate DPC System
High turnover,
good
predictability
Average turnover,
good
predictability
Low turnover,
good
predictability
High turnover,
moderate
predictability
Average turnover,
moderate
predictability
Low turnover,
moderate
predictability
High turnover,
low/none
predictability
Average turnover,
low/none
predictability
Low turnover,
low/none
predictability
+ Predictability
Lower service level
–
Higher safety
More attention
XY Z
A
B
C
+
–Turnover
Figure 9.7 General guidelines for stock control using ABC–XYZ categories.
extent, included in the sales volume for the ABC classification. The biggest advan￾tage, of course, would be that the number of categories is limited to 9.
Having a categorization of items is just the first step in selecting the method and
models to be used. The next step would be to determine for each category of items the
method and model to plan and control its supply and to determine the values of the
parameters used in that method. Assuming the involvement of human planners,
the number of different methods used in planning and control should not be too
high. Having a large variety of different methods makes it difficult for the planner to
understand the logic of the numbers presented in the planning. In general, we can
say the following:
– A-items with a high turnover make it worthwhile to put extra effort into the con￾trol system used (e.g. using continuous monitoring) and forecasting (and influenc￾ing) demand and supply (see also Silver et al. 1998; Hopp and Spearman 2008).
– A-items with a high turnover and varying but predictable demand could best be
planned with a system using a program control-based approach.
– B-items with reasonably high demand in low variation and a predictable sup￾ply lead time may be planned program-based, but could also be planned with a
stock-based control system (with low safety stock), with the main consideration
being the effort and costs involved in controlling the stock.
– For inexpensive items with low demand (C-items) and predictable and low lead
times, the option of supplying or producing on order might be an interesting one.
In general, a simple, if possible, “automatic” system could be used for planningReferences 157
and control for the C-items. The effort required by planners to cope with these
items should be as low as possible.
– For inexpensive items with high demand a multi-bin system can be used (see
Section 6.2.1).
Note that any categorization used should be updated in time cause the character￾istics used in the categorization may change in time as well.
Remark: As discussed, based on the frequency of reviewing the inventory posi￾tion and the order size, several ROP systems can be distinguished. For choosing
between continuous or periodic review, the following observations can be used:
– Given a certain customer service level we want to achieve, a continuous
review system can do that with less (safety) inventory than a periodic review
system. This is caused by the fact that due to the review period, in every
order cycle, uncertainty is faced over a longer period (the target stock level
in a periodic system covers the demand (and the uncertainty therein) during
the review period and the lead time.
– With a periodic review system, it is easier to form groups of products (same
review moment) such that the sum of the orders meets the supplier’s eco￾nomic size (e.g. Full truck loads). This might lead to quantity discounts.
– If fixing the order size meets the supplier’s economic size (for production
and transportation processes), implementation of a continuous review system
makes sense.
– Continuous review systems have a greater responsiveness to inventory posi￾tion.
– A continuous review system might require investments in tools to make it
work (for instance, RFID equipment).
References
van Aken, J.E. (1978). On the Control of Complex Industrial Organizations. Leiden,
The Netherlands: Nijhoff.
Brown, R.S.G. (1977). Material Management Systems. Wiley.
Collings, R.S. and Whybark, D.C. (1982). Coordination manufacturing and distribution
activities. IMEDE Working Paper 1, Lausanne.
van Donselaar, K.H. (1989). MRP: wanneer wel en wanneer niet? In: Handboek
Logistiek: Logistiek Management, Material Management, fystieke dystributie:
B1570-1-B1570-13 (ed. W. Ploos van Amstel). Samsom (in Dutch).
Forrester, J.W. (1961). Industrial Dynamics. New York: Wiley.
Hopp, W.J. and Spearman, M.L. (2008). Factory Physics: Foundations of Manufacturing
Management, 3e. Burr Ridge, IL: McGraw-Hill.
Magee, J.F. (1958). Production Planning and Inventory Control. McGraw-Hill.158 9 Choosing an Appropriate DPC System
Orlicky, J. (1975). Material Requirements Planning. McGraw-Hill.
Silver, E.A., Pyke, D.F., and Peterson, R. (1998). Inventory Management and Production
Planning and Scheduling. New York: Wiley.
Timmer, J. P. J., Monhemius, W. And Bertrand, J. W. M. (1984). Production and
Inventory Control with the Base Stock System. EUT – BDK Report. Departmentt of
Industrial Engineering and Management Science; 12. Eindhoven.
Wijngaard, J. and Wortmann, J.C. (1985). MRP and inventories. European Journal of
Operations Research 20: 281–293.159
Part III
Production Unit Control
In this part, we will discuss the decision functions that are relevant for control￾ling the different characteristic Production Units that we distinguish. We start in
Chapter 10 with a general discussion of the relevant production control decisions.
Although deterministic, static production situations, where everything (orders, pro￾cessing times,…) is exactly known and nothing changes, are seldom seen in practice,
we will still discuss the control of these systems in Chapter 11. The reason for doing
this is that these types of situations already show some of the complex issues the con￾trol of production units has to deal with. On top of that, some of the lessons learned in
these situations can also be used to some extent in the more complex, realistic cases
of stochastic, dynamic situations. Then, we will discuss in more detail the important
control decisions for these stochastic, dynamic production situations: (continuous)
process-wise production in Chapter 12, mass assembly production in Chapter 13,
(repetitive) small series production in Chapter 14, and (repetitive) project shop pro￾duction in Chapter 15. At the beginning of each new chapter, we first describe the
characteristics of the production situation. Thereafter the general description of the
relevant control decisions will be discussed for the concerning production situation.
Next, we will elaborate in more detail how production control should be organized
for that production situation. After the theoretical discussion of the main production
control decisions for the production situation, we will illustrate this with a more or
less (somewhat idealized) practical example.
The main focus will be the speed and reliability of the deliveries; however, the effi￾ciency and the utilization of the resources will also be considered though with less
emphasis.
Content
10. General Discussion of Production Control Decisions
10.1 Priority Control
10.2 Capacity Allocation
10.3 Work Order Release/Work Order Planning (Scheduling)
11. Production Control for Deterministic, Static Production Systems (Scheduling)
11.1 Sequencing Orders Without Delivery Date (Throughput Time Oriented)
Production Control in Practice: A Situation-Dependent Decisions Approach, First Edition.
Henny Van Ooijen and Corné Dirne.
© 2024 WILEY-VCH GmbH. Published 2024 by WILEY-VCH GmbH.160 Part III Production Unit Control
11.1.1 Work Orders with One Operation and Work Centers with One Machine
11.1.1.1 Relation Between Work-in-Process and Throughput Time
11.1.1.2 Minimization of the Average Throughput Time
11.1.1.3 Minimization of Weighted Average Throughput Time
11.1.2 Work Orders with One Operation and Work Centers with Parallel, Identical
Machines
11.1.2.1 Minimizing the Makespan
11.1.2.2 Minimizing the Average Throughput Time
11.1.3 Work Orders with Multiple Operations and Work Centers with One Machine
11.1.3.1 Minimizing the Makespan for a Flow Shop with Two Operations
11.1.3.2 Minimizing the Makespan for a Flow Shop with More than Two
Operations
11.2 Sequencing Orders with a Delivery Date (Reliability Oriented)
11.2.1 Minimizing the Average Lateness
11.2.2 Minimizing the Maximum Tardiness
11.2.3 Minimizing the Number of Tardy Orders
11.2.4 Minimizing the Average Tardiness
11.3 Relaxing Assumptions
11.3.1 Orders with Sequence Dependent Set-Up Times
11.3.2 Sequencing Order with Different Routings
References
12. Flow Process Production
12.1 General Description
12.2 Main Control Attention Points of Flow Process Production
12.2.1 General
12.2.2 Cycle Time Determination
12.2.2.1 A Stable Level of Demand
12.2.2.2 Variable Demand
12.2.2.3 Different Cycles on One Production Line
12.3 Production Control Decisions for Flow Process Production in MTS
Situations
12.3.1 Sequencing
12.3.2 Capacity Allocation
12.3.3 Work Order Release
12.4 Production Control Decisions for Flow Process Production in MTO
Situations
12.4.1 Sequencing and Work Order Release
12.4.2 Capacity Allocation
12.5 Application
References
13. Mass Assembly Production
13.1 General Description
13.2 Main Control Attention Points of Mass Assembly Production
13.2.1 Pure Flow Production
13.2.2 Variants of Pure Flow Production
13.2.2.1 Different Processing Times
13.2.2.2 Variable Processing Times
13.2.2.3 Different Products (Needing Different Materials and/or Resources)
13.2.2.4 Disturbances at the Work Centers
13.2.2.5 No Availability of Efficient Technology
13.2.2.6 A Variety of Routings (Some Operations Are Skipped)
13.2.3 Quantitative Models for Analyzing the Effect of Buffers
13.2.3.1 Two Stations Without Failures
13.2.3.2 More Than Two Stations Without FailuresContent 161
13.2.3.3 Two Stations with (Time-Dependent) Failures
13.2.3.4 More Than Two Stations with (Time-Dependent) Failures
13.2.4 Cross Training
13.3 Production Control Decisions for Mass Assembly Production
13.3.1 Sequencing
13.3.2 Capacity Allocation
13.3.3 Work Order Release
13.4 Application
References
14. Small Series Production
14.1 General Description
14.2 Main Control Attention Points of Small Series Production
14.2.1 Fundamental Results from Queueing Theory
14.2.2 Throughput Time-Related Aspects
14.2.2.1 Production Layout
14.2.2.2 Measures Based Upon Insights from Queuing Theory
14.2.2.3 Customer Differentiation
14.2.3 Lead Time Reliability Related Aspects
14.2.3.1 Due Date Determination Rules
14.2.3.2 The Effect of the Value of the Slack on the Delivery
Reliability
14.2.3.3 Internal Versus External Due Date
14.3 Production Control Decisions for Small Series Production
14.3.1 Throughput Time
14.3.1.1 Sequencing
14.3.1.2 Capacity Allocation
14.3.1.3 Work Order Release/Work Order Detail Planning
14.3.2 Lead Time Reliability
14.3.2.1 Sequencing
14.3.2.2 Capacity Allocation
14.3.2.3 Work Order Release
14.4 Application
Appendix 14.A Short-Term Capacity Adjustment
Appendix 14.B Flexible Batching
Appendix 14.C The Effect of Workload Control in Case There Is a Relationship
Between Productivity and Workload
References
15. (Repetitive) Project-Based Production
15.1 General Description
15.2 Main Control Attention Points of Project-Based Production
15.2.1 Construction of a Network
15.2.1.1 Terminology
15.2.1.2 Duration of the Activities
15.2.1.3 Critical Path and Project Duration in Case Activity Times Are
Deterministic
15.2.1.4 Slack
15.2.1.5 Uncertainty in Project Duration Due to Stochastic Activity Times
15.2.1.6 Realistic Estimates of the Activity Times
15.2.1.7 Activity on Node Networks
15.3 Production Control Decisions for Project-Based Production
15.3.1 Sequencing
15.3.2 Capacity Allocation (and Scheduling)
15.3.2.1 Resource Loading
15.3.2.2 Resource Leveling162 Part III Production Unit Control
15.3.2.3 (Constrained Resource) Scheduling
15.3.3 Work Order Release/Project Scheduling
15.3.3.1 Work Order Scheduling
15.3.3.2 Work Order Release
15.4 Application
References163
10
General Discussion of Production Control Decisions
An important goal for a production unit (PU) is to deliver products with good quality
at the right time, with short lead times, and at the right costs. We assume that the
quality aspect is not related to the logistic decisions, so the goal for the logistic con￾trol of a PU is to obtain a short lead time with high delivery reliability at low costs
which, amongst others, are determined by the availability and the use of capacity.
For this, two things are important: the arrangement of the process of producing the
products (given the technical design of the product and process as such) and the con￾trol of the production process. The process arrangement has to do with the way the
transformation processes within the PU are organized. It is not about the technical
design of the PU as such, but given the PU, the products to be produced, and the
technical processes to be used, how the transformation processes within the PU are
organized. A well-organized PU simplifies the control necessary to obtain a good
performance of the PU.
An order (which can be for the next production department, a stock point, or a
customer) often consists of several operations to be performed at several different
work centers. These production orders can have different routings through the pro￾duction department, different processing times (even at the same work center), etc.
Moreover, the quality of the base materials or (sub-) components may be bad, the
machines may fail, the operators may not work to a standard, etc. Besides that,
orders arrive dynamically (determined by the customer) and can have different sizes
(depending on how many items a customer wants). In such situations, it is hard to
determine when an order will arrive at a certain work center, and often we have
several queues of which the composition at a certain moment in time is hard to pre￾dict. Therefore, it is hard to know in advance how much capacity we need at a certain
moment (in the future) in time, which order to work on next, and when an order will
be ready. Given the (physical) process arrangement of the PU, there are, in general,
three medium/short-term control functions (see Figure 10.1) that, given the capac￾ity, need to be considered when developing the production control for a PU to deliver
as much production orders as possible on time (that is to the required delivery date):
– priority control (sequencing)
– capacity allocation
– work order release/work order detail planning (scheduling)
Production Control in Practice: A Situation-Dependent Decisions Approach, First Edition.
Henny Van Ooijen and Corné Dirne.
© 2024 WILEY-VCH GmbH. Published 2024 by WILEY-VCH GmbH.164 10 General Discussion of Production Control Decisions
Work order release
Capacity allocation
Priority control
Work
center 1 
Work
center m
Production unit
Figure 10.1 The control functions at the operational level that are relevant for the control
of the production unit.
10.1 Priority Control
The decision to be taken at what can be called the lowest level of control is: which
order from the orders in the waiting queue to produce next or (or equivalent: which
priority rule to use)? Often, we have several orders with different due dates waiting
to be processed at a work center and the question then is: which order to take next
when a machine at this work center becomes idle?
If we have a simple (stationary, deterministic) production situation, a schedule
can be made specifying which order to produce at what time to optimize some per￾formance measure. Then the sequence in which order to produce the work orders
is known. Some examples of scheduling methods are given in Chapter 11. However,
if the production situation is stochastic and/or dynamic, making such a schedule
doesn’t make much sense since the situation at hand is likely to change, and then the
schedule made might no longer be optimal. So a new one has to be made each time
a machine breaks down, processing takes longer than planned, a new order arrives,
etc. Moreover, since most practical problems are intractable/NP-hard and the opti￾mal schedule can only be approximated, continuously making “optimal” schedules
doesn’t seem to make much sense.
For dynamic, stochastic situations it is better to use a priority rule that states how
to determine the work order that has to be used next. For instance
– Should we take the order that arrived first at the work center?
– Should we take the order with the shortest processing time?
– Should we take the order that has the earliest due date (not knowing what it will
encounter at the remaining work centers in his routing)?, etc.
In general, this decision should be based on the progress of the orders and the effi￾cient use of the resource(s). Especially if the set-up times are sequence-dependent,
if there are potential quality problems, etc., an efficiency-oriented sequence will be
very attractive. However, this is often already considered at a higher level, which
leads to the situation that the decision taker has some freedom given (earlier deter￾mined) lead time and requested lead time reliability. At a higher level determined
preconditions can be considered in the work order detail planning by giving con￾straints for the sequence in which orders are produced. If there are no constraints10.2 Capacity Allocation 165
on the sequence, then the priority rule to be used might be determined by the relative
throughput time (speed) and/or delivery reliability.
10.2 Capacity Allocation
Often operators can perform several different tasks at different work centers. Also,
operations might be performed at several work centers, resulting in the option of
scheduling these operations on a different work center (for instance an operation
scheduled by the work preparation department at a computerized numerical control
(CNC) machine can also be performed at a conventional turning lathe). This might
lead to a decrease in the throughput time of this operation (even though the actual
processing time might be longer). A decision to be taken at a level higher than the
priority level then is: how to allocate the capacity? Should we use an alternative
machine and/or at which work center do we position a multi-skilled operator at
what time and for how long?
Remark: In many organizations, we see that decisions/actions that are good
from a production control point of view (and thus for the company) are not
taken/implemented because this is not allowed by the financial department
based on (wrong) financial arguments. For instance, they argue that a CNC
machine is expensive and that therefore as many products as possible should
be produced on this machine and not on the old conventional turning lathe.
These, however, are financial accounting arguments that might be relevant
at the investment level but are irrelevant at the production level once the
investment decision has been taken. At the operational level, we must use
managerial accounting arguments, which should be based on cash flows (see
Theeuwes and Adriaansen 1994).
Given the queues at the work centers at a certain moment in time, it might be
beneficial to reallocate the operators, that is to increase the number of operators at a
certain high-loaded work center (at least if machines are available) or to use an alter￾native machine. Also, it is often possible to hire or fire a (temporary) workforce. This
leads to the situation that a production department often can increase or decrease
the capacity of a certain resource or certain resources. In this way, the department
can process a stream of orders with varying volumes or diversity more or less in
time. We must distinguish between two kinds of flexibility: volume flexibility and
mix flexibility.
Volume flexibility is the ability to vary periodically the number of produced prod￾ucts, or hours worked. It gives the possibility to follow to some extent the demand.
Volume flexibility can for instance be realized by outsourcing (part of) the produc￾tion to other companies or by hiring/firing temporary workers. For production con￾trol, only forms of volume flexibility that can be used in the short term to solve
problems about the progress of the order stream (for instance by illness or big qual￾ity problems) are relevant. This leads to a higher delivery reliability than without166 10 General Discussion of Production Control Decisions
volume flexibility. Of course, to react quickly to the demand, the department must
have a certain amount of (slack) capacity available to solve problems regarding the
progress of the orders.
Mix-flexibility determines to what extent the mix of products, given the volume,
can be varied. If all product types more or less use the same amount of capacity,
there is a lot of mix flexibility. Also, if PUs have a low degree of specialization or
operators can perform a different number of tasks, there is a lot of mix flexibility. In
case PUs are highly specialized regarding the product assortment, we often have flow
production or mass assembly which limits the flexibility of the options predesigned
in the process. If many different products are produced, this might lead to a lot of
variation in resource demand and given the fact that often fixed lead times are used
at the decoupling point level, it is important that the mix of available capacity can
be adapted to the required capacity in the short term in a simple way. If this is not
possible, the realized throughput times will vary, which might have consequences
for the delivery reliability. Mix flexibility leads to a simple form of production con￾trol; the production capabilities of the PU can be expressed in a few numbers related
to the (short-term) bottleneck resources, which leads to simple and clear procedures
for capacity planning and order acceptance. Moreover, mix-flexibility is important
for high delivery reliability, since the PU then can react to the many (daily) distur￾bances that occur in the short term.
Important both for volume flexibility and mix flexibility is that the progress of
the production can be measured both in total as per resource type, and can be com￾pared with a norm. In this way, the short-term bottlenecks can be determined, and
flexibility can be used to (partly) solve them. These short-term bottlenecks must be
distinguished from the medium-term, structural, bottlenecks which must be consid￾ered at the aggregate planning level or the work order acceptance level.
10.3 Work Order Release/Work Order Detail Planning
(Scheduling)
At the highest level, at the border of the PU control and decoupling point control,
we have the decision to make whether to release an order and if so, which order to
release. Moreover, we have the work order detail planning, converting (anticipated
or expected) demand into work orders, and assigning (operation) due dates.
When capacities are highly utilized we might try not to “overload” the capacities
and hold back the release of production orders that require these capacities as one of
the first in their routing. In these situations, the work order release function might
use some form of Workload Control to control operationally the amount of work of
orders at the shop floor in relation to the available capacity and materials, such that
the throughput time of the released orders is under control. The actual release then
starts with a check of the capacity availability, which considers the actual available
capacity and the actual number of hours lagging or leading planning. This can be at
an aggregate shop level or detailed work center level. If it turns out that the release of
new orders would lead to an increase in the average throughput time (or the number10.3 Work Order Release/Work Order Detail Planning (Scheduling) 167
of orders lagging in planning) we can decide to increase the capacity or to hold up
the release of new orders. Research shows that the latter indeed leads to a controlled
PU throughput time, however, theoretically, it can be shown that the pure effect of
this is that the total throughput time (time waiting for release+throughput time) in
general increases. This is also shown in many studies for instance by Baker (1984),
Ragatz and Mabert (1988), Brobrowski and Park (1989), Wein and Chevalier (1992),
Park and Salegna (1995) and Bertrand and Van Ooijen (1996). Even in the case where
already-known future orders can start earlier and/or arriving orders can start later,
the total average throughput time increases, and the delivery reliability decreases
(Van Ooijen 1996). However, in several practical situations where a form of Work￾load Control is applied, the effects are (very) positive. The benefits of controlling
the load in the PU by the work order release function (balancing available capacity
and capacity needed) are amongst others the fact that we can choose which orders to
delay, to subcontract orders, to contact the customer, etc. Moreover, often an increase
in productivity is observed (for instance due to an increased yield) when the number
of orders in the PU is at a certain level (see, e.g. Van Ooijen 1996). The latter leads to
a decrease in the throughput time in the PU which eventually might be more than
the delay caused by the work order release function and then the total throughput
time thus decreases compared to the situation where no work order release function
is used.
The accepted orders may differ highly regarding delivery time and batch size, or
the customer batch sizes might differ from the (preferred) production batch sizes.
For instance, in some situations, customer orders require an operation at a work
center with a capacity that is a manifold of the size of the customer order, i.e. an
oven, or where there are high set-up times. In these situations, it might be beneficial
to combine several (different) orders into a production order. If this is the case, then
before orders are released several customer orders must be combined into one pro￾duction order. It is not unusual that there are one or more bottleneck resources or
that there are special requirements regarding the time at which certain operations of
the orders to be released must be performed. In these cases, a plan of progress (sched￾ule) for the orders to be released must be made (for instance such a plan might give
for each operation the moments in time or period in which the operations should be
performed). The detailed work order plan is the basis for the control of the progress
of the orders in the PU, giving maximum efficiency, and taking into account certain
constraints. This control functions through the allocation of resources and the pri￾ority control functions. It might happen that this is not sufficient and that the work
order detail plan must be adjusted. If this happens often, the work order detail plan
probably is too detailed.
It will be evident that for each of the four types of production (units) described ear￾lier, the set of aspects that make production control complex and that need the most
attention differs. This set of aspects determines which of the medium/short-term
control functions just described are relevant for the corresponding PU.168 10 General Discussion of Production Control Decisions
In the remainder of this Part III of the book, we will discuss production con￾trol for each of the four characteristic production situations introduced in Part I.
We will start each PU-specific section with a general description of the production
process and then we will discuss the (main) relevant production control aspects in
general. Next, we will discuss the production control aspects of the corresponding
PU in more detail. Finally, we will demonstrate the principles utilizing some (theo￾retical) applications. However, we first start, in Chapter 11, with production control
for deterministic, static production situations.
References
Baker, K.R. (1984). The effects of input control in a simple scheduling model. Journal of
Operations Management 4 (2): 99–112.
Bertrand, J.W.M. and Van Ooijen, H.P.G. (1996). Integrating material coordination and
capacity load smoothing in multiproduct multi-phase production systems.
International Journal of Production Economics 46-47: 1–12.
Brobowski, P.M. and Park, P.S. (1989). Work release strategies in a dual constrained job
shop. Omega 17 (2): 177–188.
Park, P.S. and Salegna, G.J. (1995). Load smoothing with feedback in a bottleneck job
shop. International Journal of Production Research 30 (6): 1549–1568.
Ragartz, G.L. and Mabert, V.A. (1988). An evaluation of order release mechanisms in a
job shop environment. Decision Sciences 9: 167–189.
Theeuwes, J.A.M. and Adriaansen, J.K.M. (1994). Towards an integrated accounting
framework for manufacturing improvement. International Journal of Production
Economics 36: 85–96.
Van Ooijen, H.P.G. (1996). Load-Based Work-Order Release and Its Effectiveness on
Delivery Performance Improvement. Ph.D. thesis. Eindhoven University of
Technology.
Wein, L.M. and Chevalier, P.B. (1992). A broader view of the job shop scheduling
problem. Management Science 38 (7): 1018–1032.169
11
Production Control for Deterministic, Static Production
Situations (Scheduling)
Deterministic, static production situations have the following characteristics:
– Resources (often machines) are continuously available from a certain point in
time (time zero).
– The times at which the orders are ready to be processed are known.
– Order characteristics, such as processing times, sequence of the operations to be
performed, and delivery dates, are known in advance.
In deterministic situations where orders more or less continuously arrive, a cer￾tain horizon can be chosen over which a given performance measure can be opti￾mized. We then have a static, deterministic production situation. For these kinds
of problems, often the only relevant production control decision is the sequence in
which the orders have to be processed to achieve a certain performance (efficiency,
delivery performance, …). Since everything is known and fixed, this comes down
to scheduling. The research-based set of publications on this Job Shop Scheduling
Problem is large (e.g. Buchmeister et al. 2021). Most of the job shop production
control situations in real life have a dynamic, stochastic nature, but sometimes we
may consider the problem as being deterministic. Although these situations are sel￾dom in practice, they may give insight into the complexities and production control
problems in dynamic, stochastic situations. Deterministic problems are sometimes
encountered in process industries or mass assembly industries. Also, for instance,
the short-term planning of big presses, or the daily planning of transport can be con￾sidered deterministic. In this chapter, we will discuss several deterministic, static
scheduling problems.
We will use the following notation:
rj
: (Ready time) The arrival time of the order
dj
: Due date of the order (planned delivery time)
Cj
: Completion time, the time at which the order is finished (actual delivery time)
f j
: Throughput time (or Flow time) of the order: Cj −rj
Lj
: Lateness of the order: the difference between actual delivery time and planned
delivery time: Cj −dj
Production Control in Practice: A Situation-Dependent Decisions Approach, First Edition.
Henny Van Ooijen and Corné Dirne.
© 2024 WILEY-VCH GmbH. Published 2024 by WILEY-VCH GmbH.170 11 Production Control for Deterministic, Static Production Situations (Scheduling)
Tj
: Tardiness of the order: max (0, Lj
)
M: Makespan i.e. the time to complete a whole set of orders
p: Processing time
w: Waiting time
Suppose that at a certain moment, we have n orders to be processed. Then the total
number of possible sequences in which N orders can be processed on one machine
equals n factorial (n!) (= n × (n−1) × (n−2) × (n−3) … × 3 × 2 × 1). We can decide
to process the orders in random order on a machine. Such a random sequence can
be determined as follows: one order is randomly chosen (all orders same chance: 1/n
to be chosen). The chosen order is the first to be processed on the machine. Then the
next order is randomly chosen (all remaining orders equal chance: 1/(n−1)). This is
the following order to be processed on the machine. So, we go on, until all orders are
scheduled. The rule whereby a random sequence is determined following the above
selection process is called the RANDOM rule. It will be clear that with this rule,
coincidence determines how “good” the sequence is. To determine a sequence in
this way is therefore unwise in general. We will usually prefer particular sequences
over others. The question now is which of the sequences we have to choose. This
will (partly) depend on which performance criterion we use. In general, one of the
following performance criteria is used (given a set of n orders):
Average throughput time: f = 1
n
∑n
j=1
fj
Maximum throughput time: f max = max1≤j≤n f j
Makespan: M = max1≤j≤nCj
Average number in system: N
Average lateness: L = 1
n
∑n
j=1
Lj
Average tardiness: T = 1
n
∑n
j=1
Tj
Maximum tardiness: Tmax = max1≤j≤nTj
Number of tardy orders: NT = ∑n
j=1𝛿(Tj
) with 𝛿(x) = 1 if x >0 and 𝛿(x) = 0 if x =0
We will first discuss the throughput time-related performance criteria and there￾after due date-related performance criteria are discussed.
11.1 Sequencing Orders Without Delivery Date
(Throughput Time Oriented)
We can distinguish between work centers with one machine and work centers with
parallel, identical machines. For both situations, we assume that set-up times are
independent of the sequence in which orders are manufactured (so they can be con￾sidered as being part of the processing time) and that operations are not interrupted:11.1 Sequencing Orders Without Delivery Date (Throughput Time Oriented) 171
once an operation has started, it goes on until it is finished. We will start with orders
with one operation (so one workstation) and next consider orders with multiple
operations (so multiple work centers).
11.1.1 Work Orders with One Operation and Work Centers with One
Machine
11.1.1.1 Relation Between Work-in-Process and Throughput Time
Performance criteria that are not related to delivery dates are the average throughput
time, the maximum throughput time, the average number of orders in the system,
and the makespan. Minimizing the average throughput time is related to maximiz￾ing customer service. If there is only one machine at a work center, minimizing the
makespan is the same as minimizing the average throughput time.
Minimizing the average number of orders in the system corresponds to minimiz￾ing the work-in-process. There exists a linear relationship (Little’s law) between
the throughput time and the work-in-process (expressed in the number of orders).
This linear relation between average throughput time and the average number of
work-in-process also holds for parallel machine situations, dynamic situations, and
stochastic situations (see Little 1961). Little’s law states:
𝜆 ⋅ f = N (11.1)
where 𝜆 is equal to the (average) number of orders that have to be processed per unit
of time.
From this law, it follows that throughput time control can be achieved by
work-in-process control (also called workload control) and that a low average
throughput time corresponds to a low average level of work-in-process (expressed
in the number of orders on the shop floor).
11.1.1.2 Minimization of the Average Throughput Time
The average throughput time is minimized when the orders are sequenced in a
monotonic nondecreasing sequence of the processing times (p1 ≤ p2 ≤ … ≤pn).
This is also known as the shortest processing time (SPT)) rule. This can easily be
proved; see for instance Baker (1975). Some properties of the SPT rule for static
one-machine problems:
SPT minimizes f, f max, and n
SPT minimizes the average waiting time: w = f − p and p is independent of the
sequence
SPT minimizes the maximum waiting time: wmax = wlast order = f max −plast order
11.1.1.3 Minimization of Weighted Average Throughput Time
Sometimes the throughput times of different orders have different weights (are dif￾ferent valued): the materials of the different orders have different values, orders are
for different customers that value the throughput times in different ways, etc. Sup￾pose that order j has a weight of 𝜔j
. Then as a performance measure, the weighted172 11 Production Control for Deterministic, Static Production Situations (Scheduling)
average throughput time can be used for sequencing orders on a machine:
f 𝜔 =
∑n
j=1𝜔j fj
∑n
j=1𝜔j
(11.2)
Now again it is easy to prove that this weighted average throughput time is mini￾mized by sequencing the orders according to a nondecreasing weighted processing
time: pj
/𝜔j
.
11.1.2 Work Orders with One Operation and Work Centers
with Parallel, Identical Machines
Sequencing problems for work centers with parallel, identical machines have to
deal with allocating orders to machines as well as with determining a sequence per
machine. This often leads to a complicated problem since in many situations the
allocation and the sequencing decisions cannot be taken independently.
Remark: We assume that an order cannot be split into several suborders that are
allocated to different machines at the work center.
11.1.2.1 Minimizing the Makespan
In most practical situations with work centers with parallel machines, the focus is on
minimizing the makespan: the time it takes to process all orders from the order set.
To solve this problem, if it can be solved, in general, takes a lot of time. An approx￾imate approach that in many instances leads to quite a good result decomposes the
problem into two subproblems: first, the allocation problem is solved and next the
sequencing problem for each machine is solved. This approach leads to a substantial
reduction of the required time. In the first step, the workload is distributed over the
parallel machine as evenly as possible, and in the second step, an optimal sequence
for each machine in isolation is determined. If we assume that each order is pro￾cessed on only one of the machines a good working method is:
– order the orders according to nonincreasing processing time;
– allocate the orders according to this sequence to the machine that has the lowest
already allocated workload;
– the sequence in which the orders are processed at each machine is not relevant.
11.1.2.2 Minimizing the Average Throughput Time
A simple procedure to generate a good solution for this problem is:
– order the orders according to nondecreasing processing time;
– allocate the orders according to this sequence to the machine that has the lowest
already allocated workload;
– process the orders at each machine in the sequence of allocation.11.1 Sequencing Orders Without Delivery Date (Throughput Time Oriented) 173
11.1.3 Work Orders with Multiple Operations and Work Centers
with One Machine
In this section, we consider orders for which multiple operations have to be per￾formed. Each operation is performed at a different workstation and each workstation
consists of one machine.
Sequencing orders with the same sequence of operations (also called flow shop)
Assumptions:
1. At time zero a number of orders, for which m operations have to be performed at
different work centers, and their relevant characteristics are known.
2. Each work center consists of one machine.
3. For all orders, the operations have to be performed in the same sequence (same
routing).
4. Set-up times are sequence-independent and thus can be considered as part of the
processing time.
5. The m different machines are continuously available.
6. Operations cannot be interrupted, and the operations of an order cannot overlap.
Flow shop sequencing problems are much more complex than one-operation
problems. Illustrative for this is the number of possible sequences. For N orders
with one operation per order, the number of possible sequences is N!. For a flow
shop problem with m operations, the possible number of sequences is (N)!m. Even
for low values for N and m, this is a high number.
Theorem 11.1 For flow shop problems with m work centers and n orders and min￾imizing the makespan as a performance criterion, it suffices, to consider only work
plans where the sequences at the first two work centers are the same.
Proof
See Figure 11.1. Suppose a schedule does not have the same ordering at the two
machines, then somewhere in the schedule at the first machine there must be an
order K that immediately precedes an order L, where order K follows order L at the
second machine. We then can interchange the sequence of order K and order L at
machine one, without any consequence for the starting time of each of the orders on
the second machine. Therefore, this will not lead to an increase in the completion
times of the orders and thus to a decrease in the performance (it might even increase
the performance).
K L
L K
Machine 1
Machine 2
Figure 11.1 Illustration of the consequences of switching the production sequence at
machine 2.174 11 Production Control for Deterministic, Static Production Situations (Scheduling)
L K
K L
Machine (m–1)
Machine m
Figure 11.2 Illustration of the consequences of switching the production sequence at the
last but one machine.
Remark: Theorem 11.1 also holds for some other performance criteria.
Theorem 11.2 For flow shop problems with m work centers and n orders and the
makespan as a performance criterion, it suffices to consider only work plans where
the sequences at the first two and the last two machines (without loss of generality
m −1 and m) are the same.
Proof
See Figure 11.2. The first part follows from Theorem 11.1. Suppose at machine m −1
and machine m, the ordering of the orders is not the same. Then somewhere in the
schedule on machine m there must be an order L that directly follows order K where
order L precedes order K at machine m −1. We can interchange the sequence of
order K and order L at machine m, without increasing the completion time of the
last at the last machine (m).
Johnson uses this rule to develop a rule to minimize the makespan for a flow shop
with two operations.
11.1.3.1 Minimizing the Makespan for a Flow Shop with Two Operations
For minimizing the makespan for a flow shop where each order needs two oper￾ations in the same sequence at different work centers (each having one resource),
it follows from Theorem 11.1 that at both work centers the same sequence can be
used. For these kinds of flow shops, Johnson (1954) developed a rule based on the
following characteristics:
– for operations on the first machine, the makespan is always smaller than for oper￾ations on the second machine; the makespan is determined by the completion
time of the last operation on the machine;
– therefore, it is important to work uninterruptedly at the first machine (always pos￾sible) and to start at the second machine as soon as possible;
– it is important to have as less as possible idle time at the second machine; this can
be achieved by having an as high as possible workload at the second machine.
Johnson’s rule consists of three steps:
1. Determine the set of orders U for which holds {j | p1,j ≤ pj,2} (orders with a pro￾cessing time at the first machine being smaller than the processing time at the
second machine); determine the set of orders V for which holds {j | p1,j ≥ pj,2} (if
pj,1 = pj,2 it is not relevant which set contains j).11.1 Sequencing Orders Without Delivery Date (Throughput Time Oriented) 175
2. Order set U in a nondecreasing sequence of p1,j, and order V in a nonincreasing
sequence of p2,j
.
3. Optimal sequence now is: orders of (ordered) set U followed by the orders of
(ordered) set V.
Example 11.1
A job shop is composed of two workstations: A and B. Each job will be pro￾cessed by A first and then by B. Each job will pass each workstation exactly
once. Table 11.1 gives the processing times of six jobs on the two machines.
Table 11.1 The processing time of six jobs at
workstations A and B.
Job Workstation A Workstation B
14 7
22 4
31 5
46 3
54 2
67 5
Question: What is the earliest time at which these orders can be finished,
assuming no more orders arrive, and in which order have the jobs to be pro￾cessed?
Solution: The earliest time at which orders can be finished can be found by
minimizing the makespan. We can use Johnson’s algorithm in this case.
Set U: {1, 2, 3] Set V: {4, 5, 6}
Orden U nondecreasing in the processing time at workstation A: {3, 2, 1}
Orden V nonincreasing in the processing time at workstation B: {6, 4, 5}
The optimal sequence is 3, 2, 1, 6, 4, and 5. Job 3 is finished at time 1 at
workstation A and then goes to workstation 2 where it can start immediately
and is finished at time 6 (1 (arrival time)+5). At time 1, job 2 can start at station
A; it is finished at time 1+2 = 3 and then goes to station B. At station B, the job
has to wait until time 6 because then job 1 is finished at station B. Doing this for
all jobs we get the data in Table 11.2. So, the earliest time at which the orders
can be finished is 27.
Table 11.2 The completion time of each of the six orders at each workstation.
Station Job 3 2 1 6 4 5
A Start 0 1 3 7 14 20
Finish 1 3 7 14 20 24
B Start 1 6 10 17 22 25
Finish 6 10 17 22 25 27176 11 Production Control for Deterministic, Static Production Situations (Scheduling)
11.1.3.2 Minimizing the Makespan for a Flow Shop with More Than Two
Operations
For minimizing the makespan for a flow shop where each order needs three oper￾ations in the same sequence at different work centers (each having one resource),
it follows from Theorems 11.1 and 11.2 that at the three work centers the same
sequence can be used. For more than three work centers this can’t be proven, how￾ever, we will only consider sequences that are the same at all work centers. Many
approximations have been developed for minimizing the makespan of a flow shop
with three or more work centers. We will discuss the approximation developed by
Campbell et al. (1970), which is derived from Johnsons rule. The basis is making
what can be called aggregate work centers by “combining” work centers.
If we have m operations, then (m −1) – “Johnson-problems” are created in the
following way:
– Problem 1: define aggregate work center 1 being work center 1 and “aggregate”
work center 2 being work center m.
– Problem 2: define “aggregate” work center 1 by adding up the processing times of
work center 1 and work center 2 and “aggregate” work center 2 by adding up the
processing times of work center m −1 and work center m.
– Problem 3: define “aggregate” work center 1 by adding up the processing times of
work center 1, work center 2, and work center 3 and “aggregate” work center 2
by adding up the processing times of work center m −2, work center m −1, and
work center m.
– …
– Problem m −1: define “aggregate” work center 1 by adding up the processing
times of work center 1, work center 2, work center 3, … work center m −1 and
“aggregate” work center 2 by adding up the processing times of work center 2,
work center 3, work center 4, …, work center m.
Solve each of the subproblems using Johnsons rule with the aggregate work cen￾ters 1 and 2. This leads to m −1 (different) sequences. Next, for each of these m −1
sequences, the makespan of the original m-work center problem is determined. The
sequence that leads to the lowest value of the makespan is then chosen as the solu￾tion to the problem. This rule is extensively tested and turns out to be quite effective
for both small and large problems.
Example 11.2
A job shop is composed of three workstations A, B, and C. Each job will be pro￾cessed first by A then by B and finally by C. Each job will pass each workstation
exactly once. Table 11.3 gives the processing times of five jobs on the three
machines.11.1 Sequencing Orders Without Delivery Date (Throughput Time Oriented) 177
Table 11.3 The processing times of five orders at each of the
workstations A, B, and C.
Job Workstation A Workstation B Workstation C
13 4 5
21 6 4
35 3 2
42 2 6
56 4 3
Question: What is the earliest time at which these orders can be finished,
assuming no more orders arrive, and in which sequence have the jobs to be
processed?
Solution: The earliest time at which orders can be finished can be found by
minimizing the makespan. Since we have a three-machine flow shop problem
we can use the algorithm of Campbell/Dudek/Smith. In this situation, we can
make 3−1 = 2 Johnson problems:
work center A and work center C
work center A+B and work center C+B
Problem 1: Consider only machine A and machine C:
Using Johnson’s algorithm for minimizing the makespan of this problem with
the two workstations A and C leads to the following optimal sequence (see
also Table 11.4): {2, 4, 1, 5, 3}. The makespan of the original problem with this
sequence is given in Table 11.5.
Table 11.4 Processing times of the jobs at machine A and machine C.
Job 1 2 3 4 5
A 31526
C 54263
Problem 2: Aggregate machine 1 and machine 2 and aggregate machine 3 and
machine 2: Using Johnson’s algorithm for minimizing the makespan of this prob￾lem with the two aggregate workstations A+B and C+B leads to the following
optimal sequence (see Table 11.6): {4, 1, 2, 5, 3}. The makespan of the original
problem with this sequence is given in Table 11.7.178 11 Production Control for Deterministic, Static Production Situations (Scheduling)
Table 11.5 The completion time of the original problem given
sequence (2, 4, 1, 5, 3).
Job 4 1 2 5 3
A Start 0 2 5 6 12
Finish 2 5 6 12 17
B Start 2 5 9 15 19
Finish 4 9 15 19 22
C Start 4 10 15 19 22
Finish 10 15 19 22 24
Table 11.6 The aggregate processing times of the jobs at
“machine” A+B and “machine” C+B.
12 345
A +B 7 7 8 4 10
C+B 9 10 5 8 7
Table 11.7 The completion time of the original problem given
sequence (4, 1, 2, 5, 3).
Job 4 1 2 5 3
A Start 0 2 5 6 12
Finish 2 5 6 12 17
B Start 2 5 9 15 19
Finish 4 9 15 19 22
C Start 4 10 15 19 22
Finish 10 15 19 22 24
Conclusion: the smallest makespan is obtained by using the sequence
{4, 1, 2, 5, 1}.
11.2 Sequencing Orders with a Delivery Date
(Reliability Oriented)
We assume that the set-up times are sequence-independent, so they can be con￾sidered as part of the processing time. Moreover, we assume that orders have one
operation, that there is one resource per work center, and that the orders have a
delivery date. In these situations, often the throughput time as such (the speed) is
not relevant but the emphasis is on the reliability.11.2 Sequencing Orders with a Delivery Date (Reliability Oriented) 179
11.2.1 Minimizing the Average Lateness
Suppose at time r =0 (without loss of generalization) we have a set of n known
orders. The average lateness for these orders, defined as L = 1
n
∑n
j=1(Cj − dj
), can be
expressed as:
L = 1
n
∑n
j=1
(Cj − dj
) = 1
n
∑n
j=1
(fj − dj
) = 1
n
∑n
j=1
fj − 1
n
∑n
j=1
dj = f − d
(where d is the average delivery date). Since d is given (determined by the cus￾tomers), L is minimized by minimizing f , and thus by processing orders according
to the SPT sequence.
11.2.2 Minimizing the Maximum Tardiness
In some situations, it is important to circumvent (extremely) high values of tardiness.
We might prefer having multiple orders that go beyond the due date with a small
amount than one order that goes beyond the due date with a (very) large amount. If
that is the case, then the orders have to be sequenced according to the due date since
the maximum tardiness and the maximum lateness are minimized by processing the
orders in a nondecreasing due date sequence (E(arliest)D(ue)D(ate) sequence); see
Baker (1975).
11.2.3 Minimizing the Number of Tardy Orders (NT )
In other situations, we might prefer having as few as possible orders going beyond
the due date and accept that some orders therefore might be pretty late (for instance
we prefer one order being 10 hours late above 10 orders being 1 hour late). In this
case, the following holds:
If the EDD sequence leads to a situation where zero or one order is too late, then
the EDD sequence minimizes the number of orders being tardy. The EDD sequence
minimizes Tmax, so if only one order is tardy, it cannot be improved.
If the EDD sequence leads to two or more orders being late, this might not be
the best sequence concerning the number of orders tardy. In that case, we can use
Hodgons algorithm (Moore 1968):
1. Order the orders according to the due date (EDD).
2. If no order is tardy, or only one order is tardy then stop, otherwise.
3. Suppose that given the EDD sequence, the kth order is the first order that is too
late; then we know for sure that from the first k orders, at least one order must be
delivered too late (EDD minimizes Tmax).
Find among the first k orders, the order with the longest processing time and place
this order at the end of the sequence.
Go to step 2 but only look at the orders that have not been placed at the end of the
sequence; repeat until all remaining orders are on time or only one order is left.180 11 Production Control for Deterministic, Static Production Situations (Scheduling)
Example 11.3
In a certain warehouse, there are a certain day at 07.00 six orders that have to
be prepared for transportation to the customer. The preparation time for each
of these six orders is given in Table 11.8. For the preparation task, there is only
one operator.
Table 11.8 The preparation time for each of the six orders that are
waiting to be transported.
Order 1 2 3 4 5 6
Preparation time 8 12 16 10 14 18
For each order, there is a separate truck and the times at which the different
drivers would like to start loading the truck are given in Table 11.9. Each driver
that can’t start with loading at the planned start times is paid a penalty of € 500.
Table 11.9 The planned start times for loading for each of the six orders that are
waiting to be transported.
Order 1 2 3 4 5 6
Planned start time loading 07.37 07.22 07.47 07.17 07.28 07.59
Question: In which sequence should the operator prepare the orders such that
the total penalty that has to be paid is as low as possible?
Solution: This is a static, deterministic single workstation problem with one
machine (the operator responsible for preparation) where the goal is to minimize
the number of tardy orders. Therefore, we can use Hodgons algorithm.
Step 1: Order according to the EDD (due date = planned starting time
loading−07.00) and determine the first order that is delivered too late.
The first order that is delivered too late is order 2 (see Table 11.10).
Table 11.10 Determination of the first order that is delivered too late with EDD.
Order 4 2 5 1 3 6
Preparation time 10 12 14 8 16 18
Due date 14 18 26 30 46 58
Sum preparation times 10 22 36 44 60 78
Too late? No Yes Yes Yes Yes Yes
Step 2: Remove among the first two orders the order with the longest prepa￾ration time (order 2 in this case). This order is put at the end of the queue. Now11.2 Sequencing Orders with a Delivery Date (Reliability Oriented) 181
determine the first order that is being delivered after its due date with the new
sequence.
The first order that is delivered too late now, is order 1 (see Table 11.11).
Table 11.11 Determination of the first order that is delivered too late with EDD
after removing one order.
Order 4 5 1 3 6 2
Preparation time 10 14 8 16 18 12
Due date 14 26 30 46 58 18
Ready time 10 24 32 48 66 78
Tardy? No No Yes Yes Yes Yes
Step 3: Remove among the first three orders the order with the longest prepa￾ration time (order 5 in this case). This order is put at the end of the queue.
Determine the first order that is being delivered too late with the new sequence.
This is order 2 (see Table 11.12).
Table 11.12 Determination of the first order that is delivered too late with EDD
after removing again one order.
Order 4 1 3 6 2 5
Preparation time 10 8 16 18 12 14
Due date 14 30 46 58 18 26
Ready time 10 18 34 52 64 78
Tardy? No No No No Yes Yes
Since we already replaced orders 2 and 5 and the remaining orders are deliv￾ered on time, this is the optimal sequence.
11.2.4 Minimizing the Average Tardiness
An often-used performance criterion is the average tardiness: T = 1
n
∑n
j=1Tj = 1
n
∑n
j=1 max (0, Lj
) = 1
n
∑n
j=1 max (0, Cj − dj
). This is a nonlinear function and no
simple, optimal rule exists for minimizing the average tardiness. Known techniques
for this problem are “branch and bound” techniques and dynamic programming
(see Baker 1975). For large problems, these techniques take a lot of time, and
we have to be satisfied with nonoptimal solutions. One of these follows from the
procedure developed by Wilkerson and Irwin (1971). This procedure is based on
the pairwise transposition of consecutive orders:
Given a sequence S as in Figure 11.3, consider the consecutive orders i and j (i
goes before j). B is the (sorted) set of orders before i and j and A is the (unsorted)182 11 Production Control for Deterministic, Static Production Situations (Scheduling)
B A
S
0 tB t
i j n
Figure 11.3 Illustration of step 3 in the Wilkinson and Irwin procedure
set of orders after i and j. Next, we consider a sequence S′ with the same sequence
as S except for the orders i and j, which have been transposed (j goes before i). Now
the question is whether the transposition leads to a lower total tardiness; if so, then
also the average tardiness will be lower. Since for most orders, the sequence stays
the same, their contribution to the total tardiness will not change and therefore we
only have to look at the contribution of orders i and j.
Derivation: Wilkinson and Irwin’s Procedure
1) Arrange the orders according to EDD
2) Determine candidate:
Take the first order of the not yet sorted order; denote this order by i and
denote its successor by j. Determine Tij and Tji. If Tij < Tji, then i is the
next candidate for the list of sorted orders otherwise, j is the next can￾didate. We will call the candidate order, order k.
3) Let us denote the time at which all orders before the pair (i, j) are finished
by tB. Let Tij be the increase in total tardiness if the sequence (i, j) is used,
and Tji be the increase in total tardiness if the sequence (j, i) is used. Denote
the last order in the list of sorted orders g (if no order is sorted yet, then the
candidate goes to the sorted list). Compare candidate k with the last order
in the sorted list g:
Tgk = max(tB −pg + pg −dg, 0) + max(tB −pg + pg + pk −dk, 0)
Tkg = max(tB −pg + pk −dk, 0) + max(tB −pg + pk + pg −dg, 0)
If Tgk ≤ Tkg then order k is placed at the end of the sorted list (thus after g);
go to step 2.
If Tgk > Tkg order g is taken out of the sorted list and goes to the list of
not yet sorted orders; go to step 3 and compare k with the last but one
sorted order.
In this rule, we see a more general characteristic of sequencing problems:
if the delivery dates are tight, then for the average tardiness, in general, it is
advantageous to process the orders according to the SPT sequence. The less
tight the delivery dates are, the more advantageous the EDD sequence will be.
If there is no sequence where no order will be delivered on time, then T is
minimized by the SPT sequence.11.3 Relaxing Assumptions 183
If all orders have the same delivery date, then T is minimized by the SPT
sequence.
If using the EDD sequence, 0 or 1 order is delivered too late, then this sequence
minimizes T.
Remark: For many sequencing problems, there are no simple optimal solutions.
In that case, we have to work with general optimization methods like dynamic
programming and the Branch and Bound method. A disadvantage of these meth￾ods is that they even for problems of mediocre size take a lot of time to solve.
Therefore, many approximation rules have been developed for special problems
which often are a simplification of the general optimization methods. Also, many
general heuristic methods have been developed like Simulation Annealing, Tabu
Search, and Genetic Algorithms, see for instance Baker (1975), French (1982), and
van Laarhoven (1988).
11.3 Relaxing Assumptions
11.3.1 Orders with Sequence-Dependent Set-Up Times
In some instances, the set-up time for a certain order might depend on the order
that just is finished. In that case, it is not allowed to incorporate the set-up time
into the processing time. This situation might occur in industries with specialized
production equipment that can process different types of products within a product
family at high speed: chemical industry, glasswork, paper mill, etc. In these cases,
often the total time it takes to process a certain set of orders (so, the makespan M)
is considered to be the main performance criterion. This total time depends on the
sequence in which the orders are processed:
M = ∑n
j=1
pj +
∑n
j=1
sj−1,j
where sj-1,j is the setup time for order j, given order j−1 is its predecessor. Since
∑n
j=1pj is independent of the sequence, minimizing the makespan is equivalent to
minimizing the sum of the set-up times. This type of problem is known as the Trav￾elling Salesman problem and no (simple) optimal rules exist. Therefore, we have to
use general methods like for instance Dynamic Programming, Branch and Bound,
or heuristics (see also Little et al. 1963).
If the number of orders is big, then even for heuristics, the required processing
time can be problematic. Therefore, often the Shortest Setup Time rule is used,
which can be improved by using a “look-forward” procedure. In the latter case,
we not only look at the total set-up time for the (n−1) other orders, but we also
look at the total set-up time of (n−1)(n−2) combinations of the other orders.
There are many approximations for these kinds of problems and this number is still184 11 Production Control for Deterministic, Static Production Situations (Scheduling)
increasing. In general, approximations are developed for specific problems. The
quality of these approximations might be quite sensitive to the characteristics of
the problem. Using these approximations in a real situation, therefore, requires a
thorough examination of the applicability and viability.
11.3.2 Sequencing Orders with Different Routings
In the general sequencing problem, each order might have a different number of
operations to be performed at several workstations in a different sequence. For
instance, for order 1 three operations have to be performed in the sequence work
center 1, work center 4, and work center 7, whereas for order 2 four operations
have to be performed in the sequence work center 2, work center 4, work center 1,
and work center 7. In contrast to the flow shop, the sequence in which the work
centers are visited is not the same for all orders; there is no relation between the
work center number and the operations sequence number. An operation, therefore,
is characterized by the triple (i, j, k), which means that operation j of order i has
to be performed at work center k. The sequence in which the different operations
of an order have to be performed is called the routing of the order. Table 11.13
gives as an example the routings and the processing times of four orders in a
three-work center shop (if the work center consists of one machine we can also use
a machine instead of a work center). These kinds of sequencing problems are called
job shop problems. No (simple) optimal rules exist for the problems. For small-scale
problems, several branch and bound or dynamic programming-based algorithms
have been developed; for large-scale problems, we have to use heuristics.
An often-used simple rule is the priority rule. Using a priority rule, conflicts
between different orders that at a certain moment compete for an operation at a
work center are solved by calculating a priority number for each order. The order
with the lowest priority number gets priority, and its operation is performed next
Table 11.13 Example of routings and processing times for a certain job shop.
Machines
Processing times 1 2 3
Orders 1 4 3 2
24 1 4
33 2 3
41 3 3
Routing Machines
123
Orders 1 1st 2nd 3rd
2 2nd 1st 3rd
3 3rd 2nd 1st
4 3rd 1st 2ndReferences 185
at the work center. As soon as the work center becomes idle, this procedure is
repeated. A lot of (experimental) research has been done on the performance of
priority rules for job shop makespan problems and for average throughput time
problems in job shops. In general, it turns out that for makespan problems a priority
rule that gives priority to the order with the highest amount of remaining processing
time performs well and for average throughput time problems the SPT-priority rule
performs well.
Priority rules give a practical method for approximating sequencing problem solu￾tions for static job shops. However, static job shop problems usually do not exist in
practice. Production departments with different kinds of machines and orders can
have a variety of routings, and orders often arrive continuously. A consequence is
that the work in process continuously is “refreshed,” so there never is a closed set of
orders that has to be processed on a given set of machines.
References
Baker, K.R. (1975). Introduction to Sequencing and Scheduling. New York: John Wiley.
Buchmeister, B., Palcic, I., and Ojstersek, R. (2021). Job Shop Scheduling Methods
Review, DAAAM International Scientific Book: 001-020. Vienna: DAAAM
International Vienna.
Campbell, H.G., Dudek, R.A., and Smith, M.L. (1970). A heuristic algorithm f or the n
job, m machine sequencing problem. Management Science 16 (10).
French, S. (1982). Sequencing and Scheduling: An Introduction to the Mathematics of the
Job Shop. Chichester: Ellis Horwood, Halsted Press.
Johnson, S.M. (1954). Optimal two and three stage production schedules with set-up
times included. Naval Research Logistics Quarterly 1 (1).
van Laarhoven, P.J.M. (1988). Theoretical and Computational Aspects of Simulated
Annealing. PhD thesis. Erasmus University Rotterdam.
Little, J.D.C. (1961). A proof of the queueing formula: L = 𝜆W. Operations Research 9:
383–387.
Little, J.D.C., Murly, K.G., Sweeney, D.W. et al. (1963). An algorithm for the travelling
salesman problem. Operations Research 11 (6).
Moore, J.M. (1968). Sequencing n jobs on one machine to minimize the number of tardy
jobs. Management Science 17.
Wilkerson, L.J. and Irwin, J.D. (1971). An improved algorithm for scheduling
independent tasks. AIIE Transactions 3 (3).187
12
Flow Process Production
As already discussed in Part I, flow process production situations are characterized
by a (very) low material complexity and a (very) low capacity or routing complexity.
There is a wide variety of these kinds of production situations, and therefore, we will
start in Section 12.1 with a discussion of the subset of production situations we will
focus on. Moreover, we also discuss the general production unit control (PUC) prob￾lem for these kinds of process-wise production situations. In Section 12.2, we will
discuss the main control attention points, and since cycle time is a main concept for
these kinds of situations, a discussion of cycle time and its consequences for differ￾ent market environments will be given. Then, in Section 12.3, the main decisions
for the Make-To-Stock (MTS) situation and in Section 12.4, the main decisions for
the Make-To-Order (MTO) situation will be addressed. Finally, in Section 12.5, The
discussion will be completed with some applications/examples.
12.1 General Description
Some examples of flow process production situations are given in Figure 12.1.
A typology of process-wise production systems is extensively discussed in Fransoo
and Rutten (1993). The definition indicates that the type of manufacturing process
performed is one of the most important characteristics. Mixing, separating, forming,
and chemical reactions are operations that are usually performed on non-discrete
products and materials. Two types can be distinguished: flow and batch process pro￾duction. See Table 12.1 for an overview of the differences. In this chapter, we will
focus on Flow process production situations.
Examples of flow process production situations are the production of oil, steel,
glass, ice cream, canned vegetables, etc. Most continuous processes can only be
performed efficiently using large installations, which tend to be very expensive. If
large quantities are demanded, this justifies continuous production, also called flow
process production (sometimes called campaign production). If smaller quantities
are required, for instance, the production of ice cream, less complex/expensive
installations suffice.
Production Control in Practice: A Situation-Dependent Decisions Approach, First Edition.
Henny Van Ooijen and Corné Dirne.
© 2024 WILEY-VCH GmbH. Published 2024 by WILEY-VCH GmbH.188 12 Flow Process Production
Figure 12.1 Examples of continuous process-wise production. Source: Courtesy of Sappi
Maastricht.
Table 12.1 Differences between two kinds of process-wise production.
Flow processes are
characterized by
Batch/mix processes are
characterized by
– High production speed, short
throughput time
– Clear determination of capacity, one
routing for all products, no volume
flexibility
– Low product complexity
– Low added value
– Strong impact of changeover times
– Small number of production steps
– Limited number of products
– Long throughput time, much
work-in-process
– Capacity is not well-defined
(different configurations, complex
routings)
– Complex products
– High added value
– Less impact of changeover times
– Large number of production steps
– Large number of products
Flow process industries are generally found in situations with few products (or
families of products) and relatively high production volumes where a continuous
input and output of material takes place and materials are processed continuously.
As said, we will focus on Flow process production situations, which often produce
a relatively fixed assortment of products, with each product (type or family) hav￾ing a relatively high demand per unit of time. (Part of) The fabrication steps are
mostly integrated, which leads to specialized production lines with complex, dedi￾cated resources and a (relative to the demand per product (type)) high production
speed per product (type). The investment in specialized equipment can be justified
by the relatively high demand. Set-up times (or change over times) between prod￾uct types are large, and set-up times within a product type are relatively small. After
production on the specialized production line, often several product type-specific
post-processing/finishing operations have to be performed; since these kinds of oper￾ations often are more or less universal (cleaning, smoothing, painting, etc.), have a
high level of volume flexibility and even may take place in another production unit
they are not considered in this section, and we will concentrate on the specialized
production line. Since several fabrication steps are integrated, there is relatively little12.2 Main Control Attention Points of Flow Process Production 189
variety between the products. Little variety, low product complexity, and the small
number of production steps cause all products to have the same routing. The lead
time is mainly determined by the cycle time, i.e. the time between two consecutive
runs of the same product. The actual processing time per unit is small, but due to
large changeover times, which are often sequence-dependent, and the high produc￾tion speed, the production batches are large. Due to the integration of production
steps into a production line, a production line is engaged with only one production
order at a time, so control of the work-in-process is no problem.
12.2 Main Control Attention Points of Flow Process
Production
12.2.1 General
Since in (continuous) process-wise production situations, the investment in (spe￾cial) capacity is high, it is important to use the production capacity as much as
possible. In these situations, we often have to deal with long and, most of the time,
sequence-dependent set-up times. Therefore good capacity planning or utilization
planning, coping with set-up time losses, is crucial. During the time the capacity is
set up for a new product, it can’t produce products, so it is important to have as little
set-up time as possible.
In the case of more than one production line where (part of) the products can
be produced, we have to decide on the allocation of the products to the different
production lines. Do we dedicate the lines to a few products, or do we use all lines
for producing the production orders? Important questions then are:
– How to decide on this?
– If we decide to allocate certain products to certain lines, how should we do the
allocation?
The advantage of dedicated lines is that less time might get lost due to set-ups,
which go at the cost of flexibility, whereas universal lines give maximal flexibility,
which might go at the cost of production time (too much time spent on set-ups). In
case a few products can be made on more than one production line although, regard￾ing quality and efficiency, sometimes with minor differences, the question is how to
define dedicated lines (which products are allocated to what line)? The lines might
be identical, or they might have different characteristics, such as speed, set-up times,
and dimensions that it can handle. In either case, the products that are made must be
allocated to one of the lines. Many factors may play a role in the allocation of products
to processing lines: the speed with which a certain product can be made on the line,
sequence-dependent set-up times, the demand characteristics of the product (fast
movers/slow movers), technical characteristics of the line, costs, etc. If the lines have
more or less the same characteristics (quality, speed, etc.), it might be good to allo￾cate products to the production lines such that products that have more or less the
same cycle time are allocated to the same production line (this requires that for each
possible allocation, estimates of cycle times have to be determined for each product).190 12 Flow Process Production
Once this allocation (on a tactical level) has been determined, at the operational
level, it must be decided, based on the actual orders, how much to produce of a cer￾tain product at which time. Given the relatively high (family) set-up times, the main
attention point for process-wise production is the effect of the set-up times. The main
relevant decisions are:
1) Determination of the time between the production moments of a product (cycle
time).
2) Determination of the sequence and quantity in which the products at a line are
produced within a cycle.
The cycle time, i.e. the (average) time between two consecutive production
moments of a series of a certain product should be determined at the tactical level
and determines the average run lengths (how long a product in a cycle is produced)
for each product. The actual run lengths are determined at the operational level
(work order detail planning).
12.2.2 Cycle Time Determination
We assume that for each production line, the set of products that are produced on it
is already determined. Many continuous process production situations are charac￾terized by the following:
– A small difference between the material costs of the different product types.
– A low number of raw materials.
– A small difference between the production speed of the different product types.
Therefore, the main differences between the product types that are made on
a production line are the differences in demand per period and the set-up time
(costs). Since good utilization planning is crucial for (continuous) process indus￾tries, the large set-up times and the high production speed forces us to work with
large production runs. Often, the set-up times are sequence-dependent, which,
to minimize the total set-up time, leads to the use of a fixed sequence (rotation
cycle). We note that this implies that the cycle time should be fixed; after all, if
the time intervals between two production runs of a certain product type are
(highly) variable we don’t know how much to produce per run, since we have
to produce enough until the unknown start of the next run (keep in mind that
here we assume that the products have a high and more or less stable demand).
Therefore, the starting point is to create steady cycle times. Since for inventory
control systems, the total costs, which are important for decoupling point control
(DPC), are not very sensitive to deviations of the optimal order size (and thus
optimal cycle time), it makes sense to use for all types of products the same cycle
time. So, we can conclude that the use of a simple model to determine the cycle
times is justified.
We start by assuming that the (total) demand is deterministic and that all products
on a given line have the same cycle time. Later on, we will shortly discuss situa￾tions with uncertainty in demand and situations with different cycles on a given
production line.12.2 Main Control Attention Points of Flow Process Production 191
12.2.2.1 A Stable Level of Demand
To determine a cycle time, we need to know how many products we will have to
produce. Therefore, we can use the (expected) demand or, if the demand is not
known yet at the moment of determining the Cycle Time, the desired production
level. In the remainder, we will use the term demand for both situations (so demand
represents the expected demand in the case of an MTS environment and the desired
production level in an MTO environment).
Suppose we have a production line where N products are made, the production
sequence is given, each product j with its changeover time sj−1,j
, changeover costs
ej−1,j
, cycle time Cj (expressed in years) and time required to produce one product
ttj (all times expressed in the same unit of time). If the demand for product j is Dj
per year and the net capacity (available capacity minus losses due to maintenance,
repair, etc.) of the production line is NC time units per year, we have the following:
– The number of set-ups per year for product j is equal to 1/Cj
, and thus, the
total changeover time per year for product j is equal to sj−1,j
/Cj
. Thus, the total
changeover time per year, s, to fulfill the total demand per year is:
s = ∑N
j=1
sj−1,j
Cj
(12.1)
– The total capacity required per year, P, to fulfill the demand is:
P = ∑N
j=1
Dj
ttj (12.2)
So, the total required capacity per year to fulfill the demand is:
∑N
j=1
(
Dj
ttj +
sj−1,j
Cj
)
(12.3)
With a total net capacity per year equal to NC, it will be evident that the following
must hold:
∑N
j=1
(
Dj
ttj +
sj−1,j
Cj
)
≤ NC (12.4)
The cycle time has immediate consequences for the average lot sizes of the prod￾ucts and thus for the average lot size inventory. Therefore, the decision on a cycle
time is often taken in consultation with DPC, most of the time by a tradeoff of the
changeover costs (per cycle) and the average lot size inventory costs resulting from
producing in a cycle. With the given cycle times and assuming that finished inter￾mediates or products are directly transferred to the stock point, the average lot size
stock level of product j is equal to:
I
Q
j = 1
2
Dj
Cj
(1 − Dj
ttj
) (12.5)
An increase in Cj leads to a decrease in the total set-up time and thus there is more
production capacity; however, this also leads to a higher stock level and, thus, extra192 12 Flow Process Production
costs. If there are no costs associated with set-up, then the problem is to minimize
the inventory holding costs. We then have the following problem:
min
C1,C2,…..CN
∑N
j=1
hj
I
Q
j
s.t.
∑N
j=1
sj−1,j
Cj
≤ smax (12.6)
with:
hj =cost per unit time (year) for keeping one unit of product j in stock
smax = maximal available time for changeover (per year): (NC−
∑N
j=1(Dj
ttj
))
If there are costs ej−1,j related to switching from producing product j−1 to produc￾ing product j, then we have the following problem:
min
C,C2,…..CN
∑N
j=1
(ej−1,j
Cj
+ hj
I
Q
j
)
s.t.
∑N
j=1
sj−1,j
Cj
≤ smax (12.7)
This might lead to a solution with a planned idle time of the capacity (just as in
the case when the total demand is lower than the available capacity). There are two
approaches for solving (12.6) or (12.7): a detailed approach and a capacity-oriented
approach.
Detailed Approach With the detailed approach first an optimal solution for each
product is determined and next a combination of these individual solutions is tried
to be found:
– For each product, we determine the optimal batch size (and thus cycle time) given
the inventory holding costs and the set-up costs for that product. We neglect the
fact that we have a limited capacity for each product.
– With the just determined Cj
’s, we check whether (12.6) holds. If not, then we must
increase the batch sizes or decrease the demand.
If (12.6) holds then 𝜌c = ∑N
j=1
(
Dj
ttj + sj−1,j
Cj
)
∕NC is the utilization rate of the pro￾duction line.
– Now, we must determine a (fixed) sequence of the production runs, given the ear￾lier determined run lengths and cycle times. This means that we must place the
runs on the time axis, retaining the distance between two consecutive runs of each
product, such that each run is placed on the time axis and there is no overlap of
production runs.
The sequencing problem is hardly solvable at high utilization rates, only for low
utilization rates often a solution can be found. However, since most of the time a high
utilization rate is desired, this is practically not interesting. The capacities in flow
process production industries are often specialized for the industry and, therefore,12.2 Main Control Attention Points of Flow Process Production 193
often pretty expensive, which leads to a situation where the company tries to utilize
the capacity as much as possible (high utilization rate). The detailed approach is,
therefore, not a good method. Moreover, in flow process production situations, there
is often a strong correlation between the different products due to the shared and
often highly utilized capacity. This then asks for a capacity-oriented approach.
Capacity-Oriented Approach With the capacity-oriented approach, first decisions are
taken on the batch sizes (production run lengths) concerning the capacity that one
wants to utilize. Next, detailed decisions are taken on whether to produce or not
(insertion of idle time) and which product (family) to produce. We assume that the
demand per product is constant, that all products have the same cycle time (Cj =C,
for all j), and that there is a given production sequence. The total changeover time
per cycle, sF, is then given by:
sF = ∑N
j=1
sj−1,j
where sj−1,j =the changeover time when changing from producing product j−1 to
producing product j and s0,1 = sN,1
The total changeover costs per cycle C, eF, are given by:
eF = ∑N
j=1
ej−1,j
Now we have the following minimization problem:
min
C
(
∑N
j=1
1
2
hj
Dj
(1 − Dj
ttj
)C +
eF
C
)
(12.8)
With hj, the holding costs of one item of product j per unit of time.
By some simple calculations, it follows for the optimal cycle time C* (the solution
of (12.8)):
C∗ =
√ 2eF
∑N
j=1hj
Dj
(1 − Dj
ttj
)
(12.9)
A limiting condition is that the cycle time should not be so small that there is not
enough time for changeovers. From (12.4), it follows that
∑N
j=1
sj−1,j
C∗ ≤ NC −
∑N
j=1
Dj
ttj
or
C∗ ≥
∑N
j=1sj−1,j
NC − ∑N
j=1Dj
ttj
(12.10)
Given these formulae, we can, given certain desired production levels Dj
, j= 1,
…, N, determine the optimal cycle time, inventory levels, and set-up costs. Based on
this, management can decide which production levels are the most appropriate.194 12 Flow Process Production
Remark: As already said, in process-wise production, there is a strong relation
between DPC and PUC, since the (minimal) cycle time determines how much is
produced. This might lead to too many supply moments for DPC and, given the
fixed ordering costs DPC has to deal with, too high costs. If the cycle time is too
large, the inventory costs for DPC might be unacceptable. Therefore, DPC and
PUC should discuss what is a good cycle time.
(The economic cycle time is based upon holding costs for the PU, but in the
ideal situation PU doesn’t have inventory).
12.2.2.2 Variable Demand
Thus far we assumed that all products have the same cycle time and that (total)
demand per cycle is constant. However, it will seldom be the case that (total) demand
is constant: for each product, the demand per cycle has a certain average and a stan￾dard deviation. Also, the aggregate demand per cycle will have a certain standard
deviation. We can account for this (see also Section 12.5) by:
(a) Using a lower value for NC (the net capacity), in the calculations. This leads to an
increase of C*
, but we have some spare capacity, which is necessary for a stable
system.
(b) Using an extra capacity stock (besides the batch size stock and the safety stock
per product).
For situations with stochastic demand, finding the optimal cycle time (includ￾ing the optimal spare capacity) is an NP-hard problem, and no simple solutions/
approximations exist; see also Winands and Houtum (2011). A practical approach
(compared to, for instance, the approach followed by Liberopoulos et al. 2010) that
doesn’t guarantee an optimal solution but gives a good approximation and is easy to
use is the following.
With the cycle time, we “reserve” capacity (determine start dates and end dates
of the production runs for the different products) to be able to make the demand. If
there is stochasticity in demand, we must reserve “extra” capacity, which might or
might not be used (depending on the actual demand). Assume that the variance in
the demand per unit of time, 𝜎D, is known. Then, a possible approach to determine
a cycle time is the following: The variance of the demand during the cycle time C is
a function of the cycle time and can be accounted for in the (maximum) required
capacity. For instance, if demand follows a normal distribution, the required
safety stock is equal to k.
√
C.𝜎D where k depends on the required performance.
We then get:
∑N
j=1
⎛
⎜
⎜
⎝
Dj
ttj +
sj−1,j
C + ttj
k.
√
C.𝜎j
C
⎞
⎟
⎟
⎠
≤ NC
or
∑N
j=1
(CDj
ttj + sj−1,j + ttj
k
√
C𝜎j
) ≤ NC. C (12.11)12.2 Main Control Attention Points of Flow Process Production 195
Y
Cycle time C
Start production
run product 1
Maximum stop-time
run product 1 =
Start production
run product 2
Start production
run last product n
Maximum stop-time
run product n =
Start production
run product 1
Figure 12.2 Graphical representation of start times and end times of the production runs
for the different products in a production cycle C.
Substituting √
C by x we get a quadratic equation that easily can be solved to
determine the optimal C. Now we can determine for each product the start time and
maximum end time of the production run within the cycle; see Figure 12.2. In this
way, we have a fixed cycle time for all products and a policy that gives the expected
performance if we have a lost sales situation.
If we have a backorder situation, then we also need to have capacity for the
backorder items. An approximate, but not very practical solution is given in Erkip
et al. (2000). A practical approach is to use the same methodology as for the lost
sales situation, however, with some spare time per cycle. So, besides a reservation of
capacity for the stochastic in demand, we also need to reserve some capacity to pro￾duce back-ordered demand. This cannot be determined by some calculations, and
a possible way to find a good value can be by using simulation or simulation-based
optimization.
Remark: If the current stock level is between 0 and the maximum stock level S,
not all production run time has to be used, and we will have idle time. A company
might decide to produce also during the rest of the time; however, then products
are produced that are not necessary to achieve the required performance level
and thus, in fact, are useless. Just producing to get rid of the idle time doesn’t
make much sense if there is no need. It only leads to higher (and uncontrolled)
inventory (costs). What can be done, and what, however, leads to a slightly higher
average stock level as a result of production (now and then) earlier than planned,
is to start the production run of the next product when the current product level
S has been reached and thus to shift all idle time to the point where the last
production run ends. This aggregated idle time can then be used to do other
things (maybe in-house outsource some of the operators till the start of the
next cycle). There is a small disadvantage to this policy: it leads to a slightly
higher average stock level (only much lower than when producing to fill up the
idle time).
If the demands of the different products are independent, we can calculate
the variance of the total demand and determine the extra time (for all products)196 12 Flow Process Production
using this variance. This extra time is then the time we can use to decrease the
available time NC (see Section 12.5). When required, the production of a certain
product can be extended using a part of this extra time.
If the total extra (safety) time is put at the end of a cycle, the cycle time of the
different products may vary (in contrast to what we described above), however,
the total cycle time remains fixed (time necessary for average demand for all
products+extra time). However, we think these are small variations and have a
small effect on the inventory.
12.2.2.3 Different Cycles on One Production Line
In practical situations, sometimes the demand for products made on a certain line
might differ a lot. In that case, the use of a pure rotation cycle (within a cycle, each
product is made once in a fixed sequence) is not realistic: it leads to unnecessary
set-up time if every product is made in each cycle. A good approach is to classify
the products based on their own “optimal” cycle time (for instance, using the EOQ
formula). For example, one can make two groups: group A consisting of fast movers
and group B consisting of slow movers. Now, one can try to find a basic cycle time Cb,
such that each product from group A is made each cycle and products from group
B are made every two cycles. This leads to two cycle times: one cycle time for prod￾ucts from group A and one for products from group B. For more details, we refer to
Maxwell (1964) and Elmaghraby (1978).
12.3 Production Control Decisions for Flow Process
Production in MTS Situations
We assume that there is no seasonality in the demand (if there is seasonality, we
need to produce in advance, which makes control more complex but doesn’t change
the rules). Since we often must deal with large, often sequence-dependent set-up
times, the decisions at the operational level (sequencing, capacity allocation, and
work order release) are more or less interrelated. Since, in the MTO situation, the
sequencing and work order release decisions are closely related, we make a distinc￾tion between MTS and MTO. In this section, we will discuss MTS situations.
12.3.1 Sequencing
Often the changeover times are sequence-dependent. Due to the changeover
times, products are often produced in a given sequence, which is such that the
total required set-up time is minimized. Therefore, the sequence within a cycle is
determined by the minimization of the total changeover time within a cycle. When
the changeover times are sequence-dependent the production schedule is fixed, and
the next production type is then the product type that follows the product type that
just has been produced. In case the changeover times are sequence-independent,
the production sequence doesn’t need to be fixed; however, there are hardly any
benefits in using variable production sequences per cycle, and therefore we assume12.4 Production Control Decisions for Flow Process Production in MTO Situations 197
that in all situations, the production schedules are fixed. Once released, the orders
are processed at the (specially designed) resource, and interruption (for instance,
changing the sequence within the resource) is not possible.
12.3.2 Capacity Allocation
Since we have specially designed equipment, capacity allocation only plays a role at
the tactical level: on which production line, if relevant, to produce the products. On
the operational level, we don’t have capacity allocation decisions.
12.3.3 Work Order Release
Since the time for production has to be utilized as much as possible, there is a close
relation in these kinds of production situations between DPC and PUC. At the start
of a new cycle, the required production is based on the norm inventory (S) and the
actual inventory.
We use the approach as described under Variable demand. If a certain production
line capacity becomes available, we then know, given this sequence, which product
(sequence) to produce next, and the only question then is how much to produce. If
the demand distribution is known and we have a lost sales situation, then with a
certain given cycle time, we can determine the order up to level S as the sum of the
required safety stock and the average demand during the cycle time. The size of the
order to release then equals S – current stock.
Remark: In the MTS situation with back-ordering, it is relevant to bring back the
aggregate inventory to its norm. By regularly comparing the actual aggregate
inventory with the norm aggregate inventory, it can be determined how much
the next period has to be corrected in the customer order acceptance (function
of DPC). Suppose the actual aggregate inventory is 9.3 days production capacity
and the norm is ten days production capacity, then in the next period, only a total
number of customer orders can be accepted that don’t use more than 9.3 days
production capacity. The rest of the customer orders are back ordered and shifted
to the next cycle. How to correct this in the acceptance should be determined at
a higher level (involving Sales). Two extremes are:
– Divide the correction evenly over all product types
– Only use a few product types for this correction
In this way, the aggregate inventory will be rapidly at its norm and (most of)
the service measures will be under control.
12.4 Production Control Decisions for Flow Process
Production in MTO Situations
Since all kinds of orders can arrive, the release decision (the selection of the prod￾uct to be produced next and how much) should be taken as late as possible. In the198 12 Flow Process Production
MTO situation, the sequencing and work order release decisions are strongly related.
To minimize the loss of production time due to changeovers also here it is beneficial
to work with cycles to control the total changeover time. The determination of the
cycle time is more or less equivalent to the determination of the cycle time as given
in Section 12.2. To check whether the available time for changeovers is adequate to
justify the standard lead times, we proceed as follows. Suppose we have a desired
standard (total) lead time for product (type) j for the flow process operation (if more
operations have to be performed for this product, it is easy to determine from this
total lead time the lead time that is relevant for the flow process operation) which
is the time between two (possible) production runs of this product. We call this the
normative cycle time of product j, and let us assume this is Fj
. Then, the required
time for changeovers that goes with these cycle times can be determined (using the
minimum of Fj over all product types j) and compared with the available time. If the
required time for set up is not smaller than the available time for set up, then we can:
– Adjust the production levels D for some products (types)
– Increase the standard lead time of one or more products (types)
Unfortunately, this is an iterative process that can’t be formalized.
Remark: Due to variations in the required volume, the effective (or required)
cycle time might deviate from the normative cycle time. Therefore we should
use some extra time (slack) in determining the normative cycle time.
With this cycle time, the (potential) production run time of a product is equal to
the time to process all the orders that have the latest production time that is before
“now +cycle time of this product.” All the other orders can be produced in the next
production run of this product without violating the delivery date. This is done for
all products that are produced on the production line.
12.4.1 Sequencing and Work Order Release
The starting points are:
1) The fraction of capacity used for changeovers must be under control. This means
that the average production runs (measured in time) must be under control. How￾ever, the actual production run times per product might vary around this average.
2) The decision about which product to produce next must be taken as late as pos￾sible. In this way, it is possible to consider all the actual (number of) customer
orders.
Often, products are categorized into product types. Between the production runs
of two product types, often large changeover times exist, and within a product type
there are only minor changeover times. Work order release must take care of the
(large) changeover times between the product (types).
The potential run length of a certain product (type) is equal to the time that is
required to process all the orders that have the latest production time that is smaller12.4 Production Control Decisions for Flow Process Production in MTO Situations 199
than “now +the cycle time of this product (type).” Now, the release decision is based
on investigating the consequences of each decision for delivery reliability. Suppose
we are at time t and we produce product i with production run time Gi (=Dj
*Cj
)
and changeover time sj−1,j
. Then, the first moment at which we can start with the
production of another product is equal to t + Gi + si
, and the orders for products with
the latest production time before this moment will be delivered too late. We calculate
the total tardiness for these orders, and we will call this the potential tardiness when
we decide to start with the production of product i: PTi
(t). This is done for each
choice of a product to be produced next, and we choose the product that leads to the
smallest potential tardiness.
The just-described way of production run determination (production run length)
gives a lot of flexibility regarding the acceptance of orders. As long as the total vol￾ume that is asked for doesn’t change, differences in the (customer order) mix can be
accepted with the standard lead time. The production run lengths adapt automati￾cally to the changes in the relative demand for the different products.
The main sequencing decision is based on the changeover time of the product
types. Within a product type, the sequencing decision should be: produce the prod￾uct with the earliest delivery date first.
Remark:
1) In the MTO situation, we assumed that the sequencing decision and pro￾duction run length determination are delayed as long as possible since in
this way the flexibility can be used as much as possible. Therefore, at the
order acceptance level, it is not known when a certain product will be pro￾duced (the sequence is not known) and what the actual production run time
of this product will be. We will briefly discuss the consequences for order
acceptance in these kinds of situations. The delivery time for each order
must be based on a standard lead time, which might depend on the product.
The standard lead time must be so large that, given the average cycle time
of that product, this product at least can (and will) be produced during this
standard lead time. Besides, it needs to be checked if there is enough capac￾ity within the standard lead time, considering the orders that already have
been accepted. If this is not the case, the delivery time must be increased.
The standard lead time is used for the market and has to be based on the
cycle time and the resulting changeover time.
2) Customer order acceptance (a function of DPC) should accept customer
orders based on the standard lead time per product (type) and the available
capacity as a function of the time. Since the decision on the production
runs is taken as late as possible, it is not known when the already accepted
orders require the capacity. However, we know that in determining the
production run, all orders that have to be delivered before a certain time
are combined. Therefore, we can conclude that the capacity usage of a
certain order will take place before the delivery date. We construct a
capacity load profile for each line and use the cumulative net production200 12 Flow Process Production
capacity and the cumulative required capacity to determine whether a new
order can be accepted (see Bertrand et al. 1998). If not, we have to adapt
the lead time that can be promised to the customer.
12.4.2 Capacity Allocation
Also, here holds that since we have specially designed equipment, capacity alloca￾tion only plays a role at the tactical level: on which production line, if relevant, to
produce the products. On the operational level, we don’t have capacity allocation
decisions.
12.5 Application
Case 12.1 VEGPRO is a small company that produces canned vegetables. They do
this at a single line where several vegetables are canned in different can sizes. There
are two types of set-ups (changeovers): one for canning another vegetable and one
for using another can size. Five different vegetables are canned in four different can
sizes. Changing over to another vegetable involves both set-up time and set-up costs.
The latter is a consequence of cleaning the line. Changing over to another can size
only involves set-up time. Set-up time and set-up costs are sequence-independent
and are respectively 120 minutes set up time and € 10.60 set up costs when changing
over to another vegetable and 30 minutes for changing over to another can size.
Table 12.2 gives the average and standard deviation of the demand per week, per
vegetable, and per can size. The production speed of the line is 2000 cans per hour,
independent of vegetable and can size.
Since it is a pretty volatile market, VEGPRO wants to reserve 10% of its capacity to
absorb variations in demand to achieve good customer performance. Costs of inven￾tory are € 0.001 per can per year. The production line operates 20 hours per day, 5
days per week, and 50 weeks per year. Due to the relatively large set-up time and
differences in demand patterns, VEGPRO decided to produce in a fixed cycle (rota￾tion cycle) where each of the vegetables A, B, and C are produced each cycle, and
vegetables D and E are alternating produced in each cycle (so: A-B-C-D; A-B-C-E;
A-B-C-D; A-B-C-E; etc.), so ABCDABCE can be seen as two cycles. Since the total
available capacity per week is 100 hours, the net capacity, given the desired reserva￾tion of 10%, is 90 hours.
To produce the demand, they need 65 hours per week, so 25 hours per week can be
used for setting up. The total set-up time per cycle is 4* (120+4 × 30)=960 minutes=
16 hours. From this, it follows that the minimal cycle time they have to use is such
that demand still can be fulfilled Cmin =3.2 days (0.64 weeks).
If we consider ABCDABCE as one cycle, then Cmin =6.4 days = 1.28 weeks.12.5 Application 201
Table 12.2 Demand information.
Vegetable Can size type
Average demand
per week
Standard deviation
demand per week
A 1 16000 1600
2 12000 1200
3 8000 800
4 4000 400
B 1 16000 1600
2 12000 1200
3 8000 800
4 4000 400
C 1 16000 1600
2 12000 1200
3 8000 800
4 4000 400
D 1 4000 400
2 3000 300
3 2000 200
4 1000 100
E 1 4000 400
2 3000 300
3 2000 200
4 1000 100
Considering ABCD or ABCE as one cycle, the cost-optimal cycle time follows
from:
min: ∑
j∈(A,B,C)
1
2
hDj
(1 − Dj
tt)C + ∑
j∈(A,B,C)
𝜀j
∕C
+ ∑
j∈(D,E)
1
2
hDj
(1 − Dj
tt)(2C) + ∑
j∈(D,E)
𝜀j
2C
From this, it follows that
Copt =
[(𝜀A + 𝜀B + 𝜀C +
1
2
𝜀E
)
∕ (h. (
1
2
ΣDAj(1 − DAjtt) + 1
2
ΣDBj(1 − DBjtt)
+
1
2
ΣDCj(1 − DCjtt)+ΣDDj(1 − DDjtt)+ΣDEj(1 − DEjtt))]1∕2
and thus
Copt = [42.4/(0.0002 ⋅(56400+19700))]1/2 =1.67 weeks per cycle. So, Cmin, the
minimal cycle time that is required to fulfill demand, is not the cost-optimal
cycle time.202 12 Flow Process Production
Case 12.2 A competitor of VEGPRO has one production line for canning five types
of vegetables: A, B, C, D, and E. At this line, four sizes of cans can be used: 0.25-liter
cans, 0.5-liter cans, 1-liter cans, and 1.5-liter cans. When a new type of vegetable
must be canned, the line must be cleaned. The cleaning time is independent of the
sequence and equal to 96 minutes. If another type of can has to be used, the line has
to be set up for this type; the set-up times are sequence-dependent. The set-up times
are given in Table 12.3. Table 12.4 gives the processing times, which depend on the
type of can and not on the type of vegetable. The mean and the standard deviation
of the demand per week are given in Table 12.5; the demand per week for a certain
item (determined by the type of vegetable and type of can) can be considered as a
random sample from a normal distribution. The company delivers from stock and
operates 18 hours per day, 5 days per week, and 52 weeks per year.
Total capacity available: 5400 minutes per week (90 hours). The set-up time con￾sists of set-ups for cans and set-ups for vegetables. Set-ups for cans take more time
than set-ups for vegetables; therefore, it is preferable (for a minimal cycle time) to
work up all vegetables for a certain type of can and then set up for another type of
can. This leads to a total set-up time of 4 × 120+4 × 5 × 96 = 2400 minutes (40 hours)
per cycle. Per week, they need 4200 minutes (70 hours) for production (demand per
week times production speed) pure production time, which results in the fact that
we have 5400–1200 minutes per week for set-ups. Therefore, the minimal cycle time
if all products are made with the same cycle time, such that the average demand can
be fulfilled is 2400/1200 = 2 weeks.
It appears that there is a possibility to work on Saturdays for four hours, and the
company wonders whether this is sufficient, given the minimal (production) cycle
time, to cover the variation in the demand with 95% reliability. The standard devi￾ation of the required capacity per week for a certain type of canned vegetable can
Table 12.3 Set up times for the different types of cans (in
minutes).
From ↓ \To → 0.25 0.50 1.00 1.50
0.25 0 120 350 450
0.50 450 0 120 350
1.00 350 450 0 120
1.50 120 350 450 0
Table 12.4 Production speed per type of can (in cans per minute).
Type of can (in l.) 0.25 0.50 1.00 1.50
Speed 120 60 30 2012.5 Application 203
Table 12.5 Demand per item (vegetable/can) (cans per week).
Item Average demand Standard deviation demand
A1 (0.25) 28000 2000
A2 (0.50) 12600 1400
A3 (1.00) 6750 1000
A4 (1.50) 3700 600
B1 (0.25) 25200 2000
B2 (0.50) 10800 1400
B3 (1.00) 6650 1000
B4 (1.50) 4400 600
C1 (0.25) 21600 2000
C2 (0.50) 14400 1400
C3 (1.00) 6300 1000
C4 (1.50) 4375 600
D1 (0.25) 27000 2000
D2 (0.50) 9000 1400
D3 (1.00) 6400 1000
D4 (1.50) 3925 600
E1 (0.25) 23400 2000
E2 (0.50) 16200 1400
E3 (1.00) 5400 1000
E4 (1.50) 4600 600
be determined by dividing the standard deviation in the demand by the production
speed. Then, the total variance in required capacity per week can be determined by
taking the squares of these standard deviations and adding them up. This leads to a
variance in the required capacity of:
5 × ((2000/120)2 +(1400/60)2 +(1000/30)2 +(600/20)2)=14167 minutes. Thus,
the standard deviation of the required capacity per week is equal to √(14167) =
119 minutes (about 1.98 hours). To get coverage of the demand of 95%, they need
a reserve capacity of 1.45 × 1.98 = 3.26 hours. Four hours of overwork capacity on
Saturday, therefore, is sufficient.
An alternative solution to increase the production volume can be found in
increasing the cycle time such that this results in a reserve capacity per week of
3.26 hours (so determining the cycle time given the availability of the capacity
of 90 – 3.26 = 86.74 hours). If they choose this alternative, the average inventory
will increase (proportional to the cycle time), thus they must outweigh the extra
inventory costs against the reduction in set-up costs and the cost savings of
overwork.204 12 Flow Process Production
References
Bertrand, J.W.M., Wortmann, J.C., and Wijngaard, J. (1998). Produktiebeheersing en
Material Management, 2nd ed. Houten: Educatieve Partners Nederland BV (in
Dutch).
Elmaghraby, S.A. (1978). The economic lot scheduling problem (ELSP); review and
extensions. Management Science 24 (6).
Erkip, N., Güllü, R. and Kocabiyikoglu, A. (2000). A quasi-birth-and-death model to
evaluate fixed cycle time policies for stochastic multi-item production/inventory
problem. Proceedings of MSOM conference, 2000.
Fransoo, J.C. and Rutten, W.G.M.M. (1993). A typology for production control
situations in process industries. Working paper Technische Universiteit Eindhoven
(TUE/BDK/LBS/; Vol. 93-02).
Liberopoulos, G., Kozanidis, G., and Hatzikonstantinou, O. (2010). Production
scheduling of a multi-grade PET resin plant. Computers and Chemical Engineering
34: 387–400.
Maxwell, W.L. (1964). The scheduling of economic lot sizes. Naval Research Logistics
Quarterly 11 (2 and 3).
Winands, E.M.M.I. and Houtum, G.J.V. (2011). The stochastic economic lot scheduling
problem: a survey. European Journal of Operational Research 210: 1–9.205
13
Mass Assembly Production
We will start the discussion on production unit control for mass assembly production
(or flow production) in Section 13.1 with a general description of the situation. Then,
in Section 13.2, the main control issues for this situation will be addressed. Next, in
Section 13.3, we will apply the basic decisions from the decision framework to this
situation. The discussion will be completed with some examples in Section 13.4.
13.1 General Description
Some examples of mass assembly production situations are given in Figure 13.1.
One of the characteristics of a mass assembly production unit, also called a flow
production unit, is that it is a product-oriented organization, that is, the product
structure and the way the operations are structured determine the design and orga￾nization of the department. For the pure form of flow production, we have:
– for all products, the operations are performed in the same sequence;
– all operations of all products have (more or less) equal, constant processing times;
– work centers (also called stations) operate without breakdowns.
If several operations have to be performed in the same sequence for a lot of prod￾ucts, it makes sense, for reasons of efficiency (concerning capacity use as well as
material supply), to have one work center for each operation or for a limited num￾ber of operations. In case we have the pure form, we can have as many work centers
as operations without having any waiting times. When at a certain work center, the
operation is finished and the order goes to the next work center, this next work cen￾ter just becomes idle (due to the equal processing times). Therefore, it is possible to
achieve a (very) high utilization without (hardly) any waiting time. This is in con￾trast with functional-oriented or organized departments, which will be discussed in
Chapter 14, where waiting times play an important role.
In practice, one hardly sees pure flow production systems, although many systems
are close to pure flow production. Deviating from pure flow production may lead to
a need for buffers to prevent the productivity from dropping too much, however, this
Production Control in Practice: A Situation-Dependent Decisions Approach, First Edition.
Henny Van Ooijen and Corné Dirne.
© 2024 WILEY-VCH GmbH. Published 2024 by WILEY-VCH GmbH.206 13 Mass Assembly Production
Figure 13.1 Examples of mass assembly or flow production. Source: https://depositphotos
.com/ and https://pixabay.com/.
leads to an increase in the throughput time. The most important reasons that lead
to non-pure flow production are:
(a) Different processing times: in most practical systems, the processing times for
the processing steps are seldom the same. Different processing steps have to be
combined into operations that are performed at different work centers, and it is
hardly possible to get a set of combinations if the goal is to have the same pro￾cessing time for each combination of processing steps in the set of combinations:
(b) Variable processing times at the work centers (stochastic processing times
instead of deterministic): if the operations at the workstations are executed
by machines the processing times at those stations often are known and more
or less constant. However, if we have mainly manual operations, for instance,
there is often some variation in the processing time, even if the operations are
identical.
(c) Different products: in the situation where different products are made on the
same set of work centers, variation in the processing times occurs if, on sev￾eral work centers, the different products have different processing times. For
instance, assume that we have two work centers, two products, P and Q and
that at each work center, the two products have different processing times. Then,
even if the processing times are constant, a change in the production of P and Q
implies a variance in the processing times.13.2 Main Control Attention Points of Mass Assembly Production 207
(d) Disturbances at the work centers (for instance, break breakdowns of machines):
disturbances at the work centers have the same effect as varying processing
times. Examples of disturbances are:
– Operators who get ill
– Operators that take time for personnel caretaking
– Operators discussing quality problems
– Machines that break down
(e) No availability of efficient technology: With this, we mean that there is no good
technology with which a specific operation at a work center at a single station
can be performed within a time that is (rather) close to the (desired) takt time
(see Section 13.2.1). Two situations can be distinguished:
(1) The available technology for a certain operation leads to a (pretty) long pro￾cessing time compared to the processing times of the other operations; this
results, given a certain takt time, in an over-utilized station.
(2) The available technology for a certain operation leads to a (pretty) short pro￾cessing time compared to the processing times of the other operations; this
results, given a certain takt time, in an under-utilized station.
(f) Variety of routings of the products made: If the different products made do not
all need all stations, but for some products, some stations can be skipped, we call
this a flow shop.
Remark:
1. Often the term flow line or line production is used. This refers to the way the
work centers are organized (layout): the physical system. Although flow pro￾duction often is found in these environments, it is not necessarily the same.
As we will see, flow production is defined abstractly, without any reference
to the layout of the production system.
2. Many terms are used that are, more or less, equivalent: transfer line, assem￾bly line, etc. What they all have in common is that all operations follow a
certain sequence, however, some operations can be skipped.
13.2 Main Control Attention Points of Mass Assembly
Production
For flow production industries the production control emphasis is on a steady (out)
flow and good utilization of the work centers (where often several operation steps
are performed). If the line is not well-designed, unnecessary blocking (the work cen￾ter can’t produce any further because the next work center is fully occupied) or
starvation (the work center can’t produce any further because the previous work
center hasn’t delivered a new job) occurs. This magnifies the importance of the deci￾sions on the tactical level. Further, since often at the different work centers, several208 13 Mass Assembly Production
materials are needed, the control also concerns the timely availability of the raw
materials/components at the different stations (material coordination), so one of
the attention points concerning production control at the operational level is the
material availability.
At the operational control level, we further might have the decision in which
sequence the orders for the different products have to be released (multiple model,
mixed model, …) and, in case the different stations are not completely balanced
and/or there are differences concerning the skill of operators, how to assign the dif￾ferent operators to the different work centers.
From a production control point of view, the most important aspect of a flow pro￾duction department is the design (or layout) of the department. In general, two types
of production systems can be distinguished: intermittent and continuous moving
lines (or belts). If we have an intermittent line then all stations need to have fin￾ished their work at a certain time, at which the products are transferred to the next
station. If we have a continuous line, then the resources (must) move along with
the product, and after a certain time, they must return to their starting position. It
will be clear that a continuous line gives more flexibility than an intermittent line
since the stations don’t need to have finished their work at the same time. Given a
well-designed flow production department, the control mostly concerns the timely
availability of the raw materials/components at the different stations (material coor￾dination), and we hardly have other operational decisions. In a pure flow production
system, the design is straightforward. If the system is less pure, it might happen that,
due to so-called blocking and starving, stations become idle (even if there is work in
the system) or blocked, which might lead to a decrease in the output of the line. We
will start in the next section with pure flow production and then gradually introduce
more complicated aspects in the next sections.
13.2.1 Pure Flow Production
As said already, pure flow production is characterized by:
(1) an identical sequence of operations for all products;
(2) (more or less) equal, constant processing times for all operations;
(3) work centers (also called stations) that operate without breakdowns.
Without loss of generalization, we will assume, for the time being, that all prod￾ucts are identical. The central concept with flow production is takt time. This is the
time between the completion times of two succeeding products. A second important
concept is throughput time (the time a product spends in the production system).
Let us denote the takt time with TT and the throughput time with TPT. Then, if
we have m operations, the following relation between TT and TPT holds (see also
Figure 13.2): TPT = TT × m (or TT = TPT/m). The design in pure flow production
systems is straightforward and the only questions are whether to merge or not some
operations.13.2 Main Control Attention Points of Mass Assembly Production 209
Station 1 Station 2 Station m
Throughput time
Processing time Processing time Processing time
Takt time Takt time Takt time
Figure 13.2 Takt time, processing time, and throughput time in case processing times at
the workstations differ, and each work center has one resource.
13.2.2 Variants of Pure Flow Production
13.2.2.1 Different Processing Times
In case the different operations have different processing times, and there is one
work center for each operation, this might lead to waiting times between the work.
It then makes sense, at the cost of loss of a little efficiency, to combine several oper￾ations. The process of combining/clustering operations with as objective of getting
well-tuned processing times per station, such that the idleness is minimized, is called
Line Balancing, and there are several techniques to do this (see, e.g. Wild 1972). Let
us denote the processing time at station k with p(k). In this situation, the takt time
TT is equal to the maximum p(k) of all stations, and the idle time for station k per
takt is equal to TT-p(k). The throughput time, TPT, is larger than the sum of the
processing times (as with pure flow production). In case we have m work centers,
we get:
TPT = TT × m = max(p(k)) × m
Example 13.1
CREDIT is a financial institute that, amongst others, grants loans. For judging
a loan application, several tasks must be performed. The times for these tasks
and their precedence relations are given in Table 13.1. The corresponding prece￾dence relation diagram is given in Figure 13.3. Each day, there are, on average,
120 loan applications, and CREDIT wants to have them all worked up at the end
of the day.
This means that each (8 × 60)/120 = 4 minutes (the takt time) an application
must be finished. The total time for working up an application is 19 minutes (see
Table 13.1), so they need at least 19/4 = 4.75, rounded to five workstations. How
to assign the tasks to the workstations?
First, consider the tasks that don’t have an immediate predecessor; this is
task a. This task is assigned to station 1. Next, tasks b, c, and d can be assigned.
Starting with the task with the longest processing time, it is determined whether
this task can be assigned to workstation 1. Since station 1 after the assignment210 13 Mass Assembly Production
of task a has a remaining time of 2 minutes (4−2), task d can be assigned to
station 1 without exceeding the takt time of 4 minutes. Now, the remaining
time is 0.2 minutes, and no more tasks can be assigned to this station without
violating the takt time. Therefore, the next station is considered. Tasks b and c
are candidates to be assigned to this station (no immediate predecessors that
are not assigned yet). Since task b has an average processing time of 3.7 minutes
and the processing time of c is larger than 0.3 (4−3.7) minutes, we go to the
next workstation. In the first instance, only task c is the candidate, but if c is
assigned also task e can be assigned. Since tasks c and e together have a pro￾cessing time of 4 minutes, station 3 is “full,” and we go to the next workstation.
Now task f is the first candidate to be considered and since this task requires
3.90 minutes and all other remaining tasks have a processing time >0.1 minutes
this station is “full.” The remaining task g is now assigned to workstation 5.
Table 13.1 The different tasks for working up a loan application, their average
processing times, and their immediately preceding tasks.
Task
Average operation
time (min)
Immediately
preceding tasks
A. Opening and sorting of applications 2.00 No
B. Working up attached letters, making notes,
and taking care of special demands
3.70 A
C. Checking and signing form 1 2.10 A
D. Signing form 2; storage of original
application
1.80 A
E. Determining loan limit using a
standardized table
1.90 C,D
F. Control by a supervisor 3.90 B,E
G. Filling in details and sending a letter 3.60 F
Total 19.00
A
B
D
C E F G
Figure 13.3 Precedence relation diagram for handling loan applications at CREDIT.13.2 Main Control Attention Points of Mass Assembly Production 211
13.2.2.2 Variable Processing Times
If there is variation in processing time, one must take into account that sometimes
(some) stations need more or less time than the takt time; this can be done in several
ways:
– If possible, use operators from stations that hardly have the risk of exceeding the
takt time, when necessary, at the stations where the takt time is exceeded (use
multi-skilled operators).
– Use buffers between stations. For instance, if in the example given, stations 2 and
3 have variable processing times, place a buffer between them (see Figure 13.4).
This prevents station 3 from “starving” if there is a delay at station 2 or if
station 2 is blocked if there is a delay at station 3. Buffers don’t change the takt
time but increase the throughput time. In Figure 13.4, we have throughput
time= 4 × 11 +buffer time.
– Change (using other technology, for instance) the breakdown structure (the way
the product is built up), if possible. This leads to another set of processing steps
and maybe to another set of combinations of processing steps. For the example in
Figure 13.4, this might lead to workstations with processing times of 10.5 each,
which given a desired takt time of 11 leads to a slack time of 0.5 at each work
center. This might be enough to cope with a possible variation in the processing
time.
13.2.2.3 Different Products (Needing Different Materials and/or Resources)
In these situations, we have to combine different flows. There are two (very) different
ways to realize a combined flow production:
– Produce in batches: make for a (long) time product P and then switch for a (long)
time to product Q. This is also called multi-model production (Wild 1972). Since
often set-up times play an important role, this reduces the amount of time that
is spent on set-up. The disadvantage of multi-model flow production is that rela￾tively large buffers are necessary.
– Produce the products in a mixed way, for instance, P, Q, P, Q, P, Q,… or P, P, Q, P,
P, Q,… This is also called mixed model production (Wild 1972). In this way, hardly
Station 1 Station 2 Station 3 Station 4
Processing time
(=10)
Processing time
(=11)
Processing time
(=10)
Processing time
(=11)
Throughput time
Buffer time
Takt time
(=11)
Takt time
(=11)
Takt time
(=11)
Takt time
(=11)
Figure 13.4 Flow-production with buffers.212 13 Mass Assembly Production
any buffers are required, however, it requires negligible set-up times (otherwise,
too much production time is lost). The exact sequence of P’s and Q’s is determined
by the processing times of the different products; on average the work centers
should be in balance with each other.
13.2.2.4 Disturbances at the Work Centers
It is not uncommon that machines break down now and then or that processes are
disturbed for other reasons (for instance, bad material). There are different ways to
cope with disturbances. Already mentioned is the use of buffers. This helps as long
as the duration of the disturbance (expressed in the number of not-produced prod￾ucts) is less than the content of the buffer. For big disturbances, buffers are rapidly
insufficient. A second way is to use redundancy: if a certain machine often breaks
down, two machines are installed instead of one. Only one machine is operating,
and as soon as it breaks down, the other machine is used. If the breakdown most of
the time is caused by a certain part, then one can decide to install this part twice.
One of these parts is used, and as soon as this part breaks down, it is replaced by the
redundant part. Of course, redundancy leads to an increase in the costs and should
outweigh the benefits of being able to produce further.
13.2.2.5 No Availability of Efficient Technology
Over-Utilized Station If compared to the other operations a certain operation takes
a long time, pure flow production is not possible. The solution is to perform this
operation in parallel: use more identical stations at the work center where the oper￾ation with the long processing time is performed. When using more stations (the
number depending on the processing time), the takt time of the work center can be
brought in line with the takt times of the other work centers. In this way, the pre￾ceding work center can continuously “feed” this work center and this work center
continuously can “feed” its succeeding work center; see Figure 13.5 for a situation
where two stations (station 2 and station 3) are required for work center 2. Due to
Station 2
Station 1 Station 4
Station 3
Processing time
(≤takt time)
Processing time
(≤2 × takt time)
Processing time
(≤takt time)
Takt time Takt time Takt time
Processing time (≤ 2 × takt time)
Figure 13.5 Flow-production using parallel stations (Throughput time (= 4 × takt time)).13.2 Main Control Attention Points of Mass Assembly Production 213
the two stations, it looks like the processing time of work center two now equals half
the processing time required for the operations at a single station (so apparently, the
processing time of operation 2 is such that with two stations, the takt time of work
center 2 is more or less identical to the takt time of the other work centers). Compli￾cating factors for parallelization are that the internal transportation system, required
for splitting and merging, is often quite complicated and expensive and that it is not
that simple to keep the output rate of the combined parallel stations constant (so the
stations work with a constant phase difference). If, for instance, the takt time of the
line equals 10-time units and the processing time of one of the operations is 20-time
units, then the work center where this operation is performed should have two sta￾tions and every 10-time units one product should be finished. This is only possible
if at the time a station becomes available, a new product arrives, and the finished
product can go to the next station in the line. If we don’t have the same constant
processing times, this often will not be the case. Therefore, buffers are often placed
before and after a work center with parallel stations, which leads to a further devi￾ation of these kinds of production systems from the pure flow production system.
Another example might be a situation where an oven operation is required. Often,
an oven operation requires a lot of time compared to a realistic takt time. A solution
might then be to “stretch” the oven and place a moving belt inside. At the entrance,
the product is placed on the moving belt and then moves through the oven during
the required oven time.
Under-Utilized Station If compared to the other operations, a certain operation takes
too little time, it can be combined with other operations to decrease the idle time
at this operation and to diminish the substantial waiting time of products once they
are finished with this operation. This, in general, is not a problem unless, for this
operation, expensive capacity must be installed. In that case, low utilization of this
capacity is not desirable. It is not the (too) short processing time of this operation
that is a problem, but the costs per unit product. We often see that in these situa￾tions, one tries to use this capacity for more types of products, each being made on
its flow production line. A few parallel flow production lines are then interrupted
because a special machine must be used for a certain operation. This is illustrated
in Figure 13.6. In the upper part, we see three products that are made on separate
flow lines. Now suppose that the operations at stations 1.3, 2.2, and 3.2 can be exe￾cuted much cheaper if they were executed at a station with a (new) technology that
can be used by all three products. Then, we get the structure as in the lower part
of Figure 13.6, where station 4 is the (combined) station with the new technology.
However, even if station 4 has some overcapacity and can run without batch size
restrictions or any other restrictions, station 4 may lead to problems since the takt
times of the three flow lines are not synchronized. Therefore, almost always buffers
are necessary before and after station 4.
13.2.2.6 A Variety of Routings (Some Operations Are Skipped)
For some flow production units, it holds that not all operations are necessary for
all the products that are made. For instance, if there are five operations A, B, C, D,
and E, then some products might need all five operations, some products only need214 13 Mass Assembly Production
Station 1.1
Station 2.1 Station 4 Station 2.3
Station 3.1 Station 3.3 Station 3.4
Station 1.2 Station 1.4 Station 1.5
Station 3.5
Station 3.1 Station 3.2
Station 2.2 Station 2.3
Station 1.5
Station 2.1
Station 1.1 Station 1.2 Station 1.3 Station 1.4
Station 3.3 Station 3.4 Station 3.5
Figure 13.6 Combining several parallel flow production lines.
operations A, B, and D (but in this sequence), some products only need A, C, and E
(but always in this sequence), some products only need B, D, and E (but always in this
sequence), etc. In this case, not only the design of the production line is important,
but also the control of the production line, especially if the sequence in which the
stations are visited depends on the product. We can’t speak of flow production any
longer since these production situations resemble small series production situations
(with a structure in the flow of orders). The control of these kinds of production
situations is discussed in the next chapter.
13.2.3 Quantitative Models for Analyzing the Effect of Buffers
As discussed in the previous section, we often need buffers in a flow line; other￾wise, starvation (a work center becomes idle because the previous operation takes
too long) or blocking (a work center cannot be used since it is occupied by an already
finished product that can’t be sent to the next work center yet) might occur. The
speed of the flow line is at most the speed of the (on average) most utilized work cen￾ter, however, significant buffers might be necessary to reach this speed. The effect
of buffers on the speed of the line can be analyzed with many stochastic models.
Often, the situation with two stations is used as the basic situation and the exact
results for this case are then subsequently used to derive approximations for more
complex flow lines (see, for instance, Gershwin 1987; Buzacott and Shantikumar
1993). As an illustration, we will discuss two characteristic models for a flow line
with two stations with a buffer with finite capacity B in between (see Figure 13.7): a
line without failures and a line with time-dependent failures.13.2 Main Control Attention Points of Mass Assembly Production 215
B Work center 1 Work center 2
Figure 13.7 Example of a flow line with two work centers and a buffer with size B.
13.2.3.1 Two Stations Without Failures
We assume that both stations have exponential processing times per unit product,
and we furthermore assume that there is an infinite stream of work for the first
station and that no failures occur. Although for many manufacturing systems, the
processing times don’t follow an exponential distribution, the assumption of expo￾nential processing times is often used to introduce the Markovian approach for the
analysis of flow lines.1 As soon as the buffer is full, either station 1 is blocked, which
means that it holds the blocking job and remains idle, or the finished job that finds
a full buffer leaves the system (loss of customer). Blocking, or loss of customers, is
over as soon as the second station completes processing its current job. If 𝜆 and 𝜇
are the average processing rates at stations 1 and 2, and 𝜌 = 𝜆/𝜇 then we get for the
probability of having n units in the buffer, Pn:
𝜌 ≠ 1 Pn = (1 − 𝜌)𝜌n
1 − 𝜌B+2 n = 0, 1, 2,…, B, B +1
𝜌 = 1 Pn = 1
B + 2
n = 0, 1, 2,…, B, B +1.
Derivation: As said 𝜆 and 𝜇 are the average processing rates (processing
rate = 1/(average processing time)) of work centers 1 and 2, respectively
(𝜆 might be larger than 𝜇). Due to the exponential processing times, the state
of the system can be described by the number of units observed in the buffer.
Let us describe the content of the buffer plus order in process at time t by the
stochastic process Nt, t ≥0. (Nt = 0, 1, 2,…, B +1). Nt = B +1 is the state where
the first station is blocked. Now the steady-state flow balance equations of {Nt}
are given by (see also Figure 13.8):
𝜆P0 = 𝜇P1
λ λ
PB PB+1
λ λ
P0 P1
μμ μ μ
Figure 13.8 Transition diagram.
1 If the processing times at the two stations can be approximated by a mixture of generalized
Erlang distributions with two phases (two-phase MGE distributions), we more or less get the same
type of analysis (e.g. see Altiok 1996).216 13 Mass Assembly Production
(𝜆 + 𝜇)Pn = 𝜆Pn−1 + 𝜇Pn+1 1 ≤ n ≤ B
𝜇PB+1 = 𝜆PB
If we define 𝜌 = 𝜆/𝜇 we get:
P1 = 𝜌P0
Pn = 𝜌nP0 n ≤ B
and PB+1 = 𝜌PB = 𝜌B+1P0
Given that ∑B+1
n=0Pn = 1
we get for 𝜌 ≠ 1 P0 = 1 − 𝜌
1 − 𝜌B+2
and thus Pn = (1 − 𝜌)𝜌n
1 − 𝜌B+2 n = 0, 1, 2,…, B, B +1
If 𝜌 = 1 we get Pn = 1
B + 2 n = 0, 1, 2,…, B, B +1.
The blocking probability is given by PB+1.
𝜌 can be larger than or equal to 1 (𝜆 larger than or equal to 𝜇) since the number of
products in the buffer (orders in the system) is restricted by the size of the buffer.
If 𝜌 <1, then the first station is the bottleneck, and if 𝜌 >1, then the second station
is the bottleneck. Suppose that 𝜌 <1 and thus the first station is the bottleneck (if
𝜌 >1 a similar reasoning holds). Each time the first station blocks, we have a loss of
production. The exact loss of production depends on the way the flow line is set up.
Let us assume that the first station keeps on processing and that the produced item
in case of blocking (full buffer) is taken off the line. The average loss of production
per unit of time then equals 𝜆 PB+1, with 𝜆 the parameter of the exponential distri￾bution of the processing times at the first station. Using Little (1961), the average
throughput time t can be determined from
𝜆(1 − PB+1)t = n
with: n = ∑B+1
i=0 i × Pi
.
13.2.3.2 More Than Two Stations Without Failures
If there are more than two stations without failures, the results of the situation with
two stations are often used to get approximations for flow lines with more than two
stations. Altiok (1982, 1996) decomposes the system of tandem queues into individ￾ual queues, ignoring some of the interactions between the components of the system.
The procedure revises the service time distributions at each queue, decomposes the
system into separate queues, and assigns new mean arrival rates.
13.2.3.3 Two Stations with (Time-Dependent) Failures
Two common types of failures that can occur are time-dependent and state￾dependent. For an extensive discussion on the background of this, we refer to13.2 Main Control Attention Points of Mass Assembly Production 217
Production
speed
Buffer
content
Time
v1
v2
0
Figure 13.9 Effect of disturbance on the content of the buffer.
Buzacott and Hanifin (1978). In the case of time-dependent failures, it is pretty easy
to show the effect of buffers. Let us assume both stations have constant processing
times but are subject to time-dependent failures. Suppose v1 resp. v2 is the speed of
the first station respectively the second station (expressed in units per unit of time).
If both stations are working, the content of the buffer increases with speed v1 −v2.
If only station 1 is working the content of the buffer increases with speed v1 and if
only station 2 is working, the content of the buffer decreases with speed v2. As an
illustration, see Figure 13.9. If we have time-dependent failures and a failure process
at station 1 that is independent of the content of the buffer, then the net speed of the
flow line can be determined in the following way. The fraction of the time that a
station is working equals P/(P + R), where P is the average production time (time
between two failures), and R is the average failure time (including the repair time).
If both stations can fail and if we have a large buffer, then the speed of the line is
equal to the minimum (v1 × P1/(P1 + R1), v2 × P2/(P2 + R2)). If there is no buffer,
the line only works if both stations are working. Since we have time-dependent
failures and the stations are independent, the fraction of the time that both stations
are working, the uptime (UT) is equal to:
UT = P1
(P1 + R1)
×
P2
(P2 + R2)
If both stations are working, the speed equals minimum (v1,v2), so the speed equals
min(UT × 𝜈1, UT × 𝜈2).
For further illustration, we assume that only the first station is subject to
time-dependent failures and that both the time between failures and the repair time
have a negative exponential distribution with parameters 𝜆 resp. 𝜇 (see Figure 13.10
for an illustration). Then the net speed of station 1 is v1 × 𝜇/(𝜆 + 𝜇). Furthermore,
we assume that v2 ≥ v1 and that the line is balanced, that is, v2 = v1 × 𝜇/(𝜆 + 𝜇).
(For other situations, we refer to Koster 1988.) Define a cycle as a period of growth
(an increase of the buffer content) followed by a period of decline (a decrease of the218 13 Mass Assembly Production
Production
speed
Time
v2
v1
0
Figure 13.10 Illustration of the effect of failures at the first station.
buffer content). The expected loss of production during a cycle, L, is determined by
the size of growth and decline. The number of cycles per unit of time is determined
by the length of a cycle.
Given a buffer size of B, the speed of the flow line with two stations is equal to
(Koster 1988):
v(B) =
v2 × (
v2
2 + 𝜇 × v1 × B
)
v1 × v2 + 𝜇 × v1 × B
If B = ∞ then the speed of the line is equal to v2, otherwise, there is a loss of pro￾duction since occasionally station 2 becomes idle. For the loss of production during
a cycle, we then have:
L = v2 − v2 × (
v2
2 + 𝜇 × v1 × B
)
v1 × v2 + 𝜇 × v1 × B
The expected length of the cycle, T, is equal to T = 1
𝜇 + 1
𝜆 = 𝜆+𝜇
𝜆×𝜇 .
13.2.3.4 More Than Two Stations with (Time-Dependent) Failures
If there are more than two stations with failures, a line L with k stations with failures
can be approximated by a set of (k −1) two-machine lines L(i) for i =1,…, (k −1).
Two successive stations with a buffer in between in a flow line are then replaced by
one station with about the same behavior in terms of production speed and process￾ing time as the original flow line; see Figure 13.11 (station 1, station 2, and the buffer
in between are replaced by a “new” station 1′
). See, for example, Gershwin (1987),
Colledani and Gershwin (2011), and Li (2013).
Station 1 Station 2 Station 3
Station 1′ Station 3
Figure 13.11 Approximation method for flow lines with more than two stations with finite
buffers and failures.13.2 Main Control Attention Points of Mass Assembly Production 219
13.2.4 Cross Training
As already said, in real-life production situations, pure flow lines are seldom found
and we have to deal with short-term variations and imbalanced average workloads
across work centers. This can be accounted for at the design level, for instance, by
adopting the use of a U-shaped line layout (see, e.g. Miltenburg 2001). An example
is given in Figure 13.12. This is sometimes called cell production. By (re-) arrang￾ing straight production lines into a U-shaped production line, operators can move
between the two legs of the U-shaped line to perform combinations of tasks that
otherwise are not allowed (or possible).
At the operational level, one can use cross-trained workers (who can work where
they are needed). These can be used to improve efficiency: if there is variability in
processing times, this is often counteracted by buffers to keep operators as utilized
as possible, however, buffers come at a cost, and if an idle operator at a work center
can switch to another work center, the productivity is increased. Also, in the sit￾uation of imbalanced average work center workloads, cross-trained operators can
divide their efforts between work centers, which increases their utilization and the
resulting throughput of the line. We have two control-related questions:
– Will we use cross-trained operators?
– Which operators to cross-train for which skills (skill pattern)?
To answer the first question, we need to make a trade-off between buffer costs and
the costs of having cross-trained operators. Cross-trained operators are, in general,
more expensive and know and then need to be trained to keep their skills at the right
level.
Answering the second question means choosing what kind of policy one wants
to use:
– A cherry-picking policy
– A skill-chaining policy
– A bucket brigade policy
Cherry picking is the policy where an under-capacitated work center borrows
(picks) capacity from underutilized operators at the other stations. The decision
Entrance 1234
876 5
Exit
Figure 13.12 A schematic example of a U-shaped production line.220 13 Mass Assembly Production
now is what skills an operator needs to have to balance the line (e.g. Hopp et al.
2004; Goldratt and Cox 1992).
(Regular) D-skill chaining is defined as the case where each operator is
cross-trained for his base work center and the next D-1 work centers (so D
skills per worker). For instance, if we have four work centers 1, 2, 3, and 4, and
the operators are trained for the following skill sets: (1,2), (2,3), (3,4), and (4,1), we
have a regular 2-skill chain (e.g. Jordan and Graves 1995; Hopp et al. 2004). D-skill
chaining does not necessarily imply that operators are cross-trained for adjacent
stations. In general, it can be defined as each operator covering exactly D unique
work centers, each work center is served by D unique operators, and the graph with
arcs (skills) connecting operators and work centers is connected. There are many
ways to construct such a D-skill chain, but this is beyond the scope of this book (e.g.
Hopp et al. 2004).
With a Bucket brigade policy (Bartholdi and Eisenstein 1996), operators farthest
downstream in the line, upon completion of a job, move up the line and take the
job from the next operator upstream; that operator, in turn, moves upstream and
takes the job from the next operator and so on until the operator farthest upstream
takes a new job. This is an effective policy, especially if there is a (huge) difference
between the workpace (caused by differences in experience, manual dexterity, pure
discipline, etc.) of different operators on the same manual task.
13.3 Production Control Decisions for Mass Assembly
Production
13.3.1 Sequencing
Since products are sent to the next work center in the routing in the rhythm of the
takt time, we don’t have orders waiting at work centers and thus we don’t have a
sequencing decision. In case we have buffers, we have several orders waiting at a
work center. However, these buffers are meant to assure a more or less uninterrupted
outflow of the line. Since all products more or less have the same processing time
at the work centers and the same number of operations, using a sequence that dif￾fers from the sequence in which products entered the buffer negatively influences
the throughput time and the delivery reliability of the products. In other words,
FIFO-sequencing usually suffices.
13.3.2 Capacity Allocation
Once decided on the cross-training policy, we have at the operational level decisions
concerning the allocation of capacity and the supply of materials. Material supply
to individual workstations can be done either by including all materials required in
the set of parts that are moving in the line, or by supplying the materials separately
to the station when required. If the usage of the material is high enough, a multi-bin
system could be used (as explained in Section 6.2.1). In this section, we will fur￾ther focus on the allocation of capacity. Given the, at the design level determined,13.3 Production Control Decisions for Mass Assembly Production 221
cross-train policy, a relevant decision at the PU control level is: how to (dynamically)
assign cross-trained workers to different tasks over time (what operator coordina￾tion policy to use)? Using the Markov decision process (MDP) theory, it is possible
to find an optimal coordination policy (e.g. Hopp et al. 2004). However, this turns
out to be a rather complex policy, whereas real-life applications generally require an
easy-to-implement operator assignment policy. Therefore, often a heuristic assign￾ment policy is chosen. In literature, one often finds one of the following heuristics:
1. Fixed-Before-Shared Policy: assign the operator o to his (base) work center if there
are any available jobs (e.g. Gel et al. 2001).
2. Zoned Craft Policy: (only applicable if all operators are cross-trained for consec￾utive work centers). Beginning from his work center, an operator carries a job
and processes it successively until the end of his zone unless he bumps into an
idle operator downstream. Then he turns back to his (base) station and gets the
next available job, or, if there is no available job, he successively checks the down￾stream work centers for available jobs within his zone (e.g. Van Oyen et al. 2001).
3. Priority Policy: assign an operator o to the non-empty work center with the highest
priority; priorities are based on the long-run effort allocation fraction of operator
o for each work center.
4. MaxQueue Policy: assign an operator to the work center with the longest queue
(e.g. Askin and Iyer 1993).
5. Maxload Policy: assign an operator to the work center with the largest workload
(see Downey and Leonard 1992 for a variant of this policy).
6. MaxQueueGap Policy: assign an operator to the work center with the largest gap
between its queue length and that of the next station.
7. MaxGap Policy: assign an operator to the work center with the maximum gap
between its workload and that of the next station.
8. Buffer Policies: a buffer is assigned a certain threshold value. After a job at a certain
work center is completed, the operator processes a job at the next work center if
the queue length at that work center exceeds its threshold value; otherwise, he
processes a job from the current work center (e.g. Ostolaza et al. 1990; McClain
et al. 2000).
13.3.3 Work Order Release
If only one kind of a (more or less) similar product is made, then of course work
order release is straightforward. In case several different products are made on the
same line, we have the following two cases:
1. Produce in batches (especially if there are significant set-up times): P, P, P, P, P, P,
Q, Q, Q, Q, Q, Q, Q, P, P,… This is called multi-model production by Wild (1972).
Since this leads to the use of buffers, we have to find a schedule that minimizes
the total set-up time costs and buffer costs and use this schedule to determine the
next order to release.
2. Produce the products in a mixed way, for instance, P, Q, P, Q, P, Q,… or P, P, Q, P, P,
Q,… This is called mixed-model production by Wild (1972). The exact sequence222 13 Mass Assembly Production
of P’s and Q’s is determined by the processing times of the different products; on
average the work centers should be in balance with each other. This sequence
determines the next order to release.
13.4 Application
Case 13.1 A producer of animal feeds has an assortment of 65 different feeds,
which are sold in customer-specific packaging. In the production department, raw
materials are mixed to get a specific animal feed. In the packaging department, the
feed is packed in bags. There are three lines: two lines, where the feed is packed,
and one line where the packed feeds are inspected and palletized.
Line 1 is an old packaging line. At the first station of this line, consisting of one
machine, the bags are filled. The second station consists of two machines, and here,
the bags are closed and labeled. At the third and last station, which consists of one
machine, the bags are sealed and sent to the buffer for the third line. Line 2 has the
same functions as line 1, but the machines at the first and third stations are newer
(and faster). Table 13.2 gives the processing times for each of the machines at both
lines. Between the machines at line 1 and line 2, there is no buffer space.
The bags from lines 1 and 2 go to the (large) buffer of line 3. Line 3 consists of two
stations, each having one machine: an inspection station and a palletizer. Between
the stations, there is no buffer space. The processing times of these machines are
respectively 25 and 30 seconds per bag. Replacing loaded pallets with empty pallets
doesn’t influence the capacity of the line.
The packaging department operates 50 weeks per year and 40 hours per week.
If the machines don’t break down, the capacity of the packaging department,
expressed in bags per year, can be determined in the following way:
Line 1: takt time = 60 seconds; capacity = (50 × 40 × 3600)/60 = 120.000
Line 2: takt time = 50 seconds; capacity = (50 × 40 × 3600)/50 = 144.000
Line 3: takt time = 30 seconds; capacity = (50 × 40 × 3600)/30 = 240.000
Line 1 and line 2 together: 264.000 bags/year, so line 3 determines the capacity of
the department, which then is 240.000 bags/year.
The reason that other, newer, machines are used for line 2, is the fact that the
already older machines at station 1 and station 3 of line 1 break down now and
then. To repair the machines, one operator is available. Table 13.3 gives the char￾acteristics of these breakdowns, which only take place if the line is used. As said
Table 13.2 Number of machines and processing times per bag
(in seconds) for line 1 and line 2.
Station 1 2 3
Number of parallel machines 1 2 1
Processing timeline 1 50 100 60
Processing timeline 2 30 100 4013.4 Application 223
Table 13.3 Characteristics of the breakdowns at line 1 (times
given in hours).
Station 1 2 3
The average time between breakdowns 200 ∞ 100
Average repair time 26.6 0 20.0
already, there are no buffers at the line. The breakdowns of the machines at line 2
and line 3 can be neglected. Considering the breakdowns, the capacity of the pack￾aging department, expressed in bags per year, can be determined in the following
way: the efficiency of line 1: 1
1+ 26,6
200 + 20,0
100
= 0.75. This implies that the capacity of line
1 is 90.000 instead of 120.000 (without breakdowns). So now the joint output of
line 1 and line 2 determines the capacity of the department which then is equal to
144.000+90.000 = 234.000 bags/year.
Case 13.2 EMPRO produces electronic modules. The production takes place on
two machines. At the first machine, the main component is produced and at the sec￾ond machine, this component is with some externally supplied materials assembled
into the end product. Both, the production of the main component and the assem￾bly take place at machines that are specially developed for these tasks. The products
are customer-specific, with as a consequence that the production time for the main
component, as well as the assembly time, can vary a lot. EMPRO wants to produce
100.000 end products per year. Both, the machine for the main component and the
assembly machine can produce, on average, 55 products per year; production times
follow an independent and identically distributed (i.i.d.) negative exponential dis￾tribution. The machines are coupled by an automated transportation system, which
includes a buffer for temporarily storing the main component.
To ensure that the required yearly production can be guaranteed with a 50% relia￾bility, given that the line operates 40 hours per week and 50 weeks per year, EMPRO
is considering installing some buffer places. The expected output of the line is 55
modules per hour (= 55 × 2000 = 110.000 modules per year) if the second machine
can produce without interruption. However, the production stops if a module is
assembled and there are no more components in the buffer to assemble the next
product at the second machine. The supply process to the buffer is characterized by
negative exponential distributed interarrival times (determined by the production
times on the first machine), except when the buffer is full, in which case the supply
is blocked. Since the speed of supply of the buffer equals the speed of the second
machine, the % loss of production for this line in case there are Z buffer places is
equal to the probability that the buffer is empty, which is given by:
pr(0) = 1
Z + 2 (𝜌 = 1)
So, a fraction 1/(Z +2) of the maximal production is the expected loss. To be able
to produce 100.000 end products per year, it must hold that
110.000 (Z + 1)∕(Z + 2) = 100.000224 13 Mass Assembly Production
From this, it follows that Z has to be 9, thus EMPRO has to install 9 buffer places.
On average this is enough, which implies that in 50% of the cases, this is (more than)
enough and that in 50% of the cases, this is too little.
Buffer places appear to be pretty expensive, and EMPRO doubts whether it is not
beneficial to work more than 40 hours per week. With 8 buffer places, instead of
9, the expected output per hour is equal to 55 × (Z +1)/(Z +2) = 55 × 9/10 = 49.5
modules. To be able to produce 100.000 end products per year, EMPRO has to work
2000/49.5 = 40.4 hours per week instead of 40 hours per week.
References
Altiok, T. (1982). Approximate analysis of exponential tandem queues with blocking.
European Journal of Operational Research 11 (4): 390–398.
Altiok, T. (1996). Performance Analysis of Manufacturing Systems. New York: Springer.
Askin, R.G. and Iyer, A. (1993). A comparison of scheduling philosophies for
manufacturing cells. European Journal of Operational Research 69: 438–449.
Bartholdi, J.J. III, and Eisenstein, D.D. (1996). A production line that balances itself.
Operations Research 44 (1): 21–34.
Buzacott, J.A. and Hanifin, L.E. (1978). Models of automatic transfer lines with
inventory banks. AIIE Transactions 10: 197–207.
Buzacott, J.A. and Shantikumar, J.G. (1993). Stochastic Models of Manufacturing
Systems. Englewood Cliffs: Prentice Hall.
Colledani, M. and Gershwin, S.B. (2011). A decomposition method for approximate
evaluation of continuous flow multi-stage lines with general Markovian machines.
Annals of Operations Research https://doi.org/10.1007/s10479-011-0961-9.
Downey, B.S. and Leonard, M.S. (1992). Assembly line with flexible workforce.
International Journal of Production Research 30 (3): 469–483.
Gel, E.G.S., Hopp, W.J., and Van, O.M.P. (2001). Workforce Agility in Systems with
Hierarchical Cross-Training. Working paper. Tempe, AZ: Arizona State University.
Gershwin, S.B. (1987). An efficient decomposition method for the approximate
evaluation of tandem queues with finite storage space and blocking. Operations
Research 35 (2): 291–305.
Goldratt, E.M. and Cox, J. (1992). The Goal: A Process of Ongoing Improvement, 2ee.
Croton-on-Hudson, NY: North River Press Inc.
Hopp, W.J., Van Tekin, E., and Oyen, M.P. (2004). Benefits of skill chaining in serial
production lineswith cross-trained workers. Management Science 50 (1): 83–98.
Jordan, W.J. and Graves, S.C. (1995). Principles on the benefits of manufacturing
process flexibility. Management Science 41 (4): 577–594.
Koster, M.B.M.d. (1988). Capacity Oriented Analysis and Design of Production Systems.
PhD thesis. Eindhoven University of Technology.
Li, J. (2013). Continuous improvement at Toyota manufacturing plant: applications of
production systems engineering methods. International Journal of Production
Research 51 (23–24): 7235–7249.References 225
Little, J.D.C. (1961). A proof of the queueing formula: L = 𝜆W. Operations Research 9:
383–387.
McClain, J.O., Schultz, K.L., and Thomas, L.J. (2000). Management of worksharing
systems. Manufacturing and Service Operations Management.
Miltenburg, J. (2001). U-shaped production lines: a review of theory and practice.
International Journal of Production Economics 70 (3): 201–214.
Ostolaza, J., McClain, J., and Thomas, J. (1990). The use of dynamic (state dependent)
assembly-line balancing to improve throughput. Journal of Manufacturing Service
Operations Management 3: 105–133.
Van Oyen, M.P., Gel, E.G.S., and Hopp, W.J. (2001). Performance opportunity of
workforce agility in collaborative and noncollaborative work systems. IIE
Transactions 33: 761–777.
Wild, R. (1972). Mass Production Management. London: Holt, Rinehart and Whinston.227
14
Small Series Production
In the case of small series production, although many routings can be followed by
the different products to produce, a lot of (the statistics) of the orders are known,
which makes it possible to develop (general) production control rules. We will start
in Section 14.1 with the discussion on Production Unit Control for Small series
production with a general description of the situation. Then, in Section 14.2, the
main issues for this situation will be addressed and, since Queueing Theory plays
an important role in these kinds of situations, Queueing Theory will be discussed
briefly. In Section 14.3, we will apply the basic decisions from the decision frame￾work to this situation. Especially, due to the routing complexity, in these kinds of
production situations production control decisions have to consider the throughput
time as well as the lead time reliability. Therefore, we have separate sections in
Sections 14.2 and 14.3 for the throughput time and the lead time reliability. The
chapter will be completed with some examples in Section 14.4.
14.1 General Description
Some examples of repetitive small series production situations are given in
Figure 14.1.
If in a production unit, a lot of different products are made, for instance, a different
number of iron baskets, chairs, etc., which have a variety in routings (not all prod￾ucts require all operations), it is not beneficial to invest in special machines since
the assortment is too diverse. Therefore, simple, universal machines (like drilling
machines, sawing machines, etc.) are used, and the production unit is character￾ized by a functional layout and the fact that, depending on the type of product,
orders can go from each work center to several other work centers, or leave the
shop. If each product is (more or less) unique we call this production situation a
one-of-a-kind production situation; this is a special form of the small series produc￾tion situation. For the remainder of this section, we will call these production units
(one-of-a-kind or small series) a job shop. We can distinguish static, deterministic sit￾uations and dynamic, stochastic situations. Examples of issues in static, deterministic
situations have already been discussed in Chapter 11. In this chapter, we will dis￾cuss dynamic, stochastic situations. For dynamic, stochastic situations, the arrival
Production Control in Practice: A Situation-Dependent Decisions Approach, First Edition.
Henny Van Ooijen and Corné Dirne.
© 2024 WILEY-VCH GmbH. Published 2024 by WILEY-VCH GmbH.228 14 Small Series Production
Figure 14.1 Some examples of (parts of) small series production situations. Source:
Gumigasuki / Pixabay; Sasint / Adobe Stock; Pexels.
times of the orders are not (precisely) known. Other characteristics of the order
are sometimes known at the arrival time of the order, but oftentimes there is still
uncertainty about, for instance, the processing time of the order. Production orders,
or work orders, have strongly varying routing; there is no fixed sequence in which
the orders visit the work centers and different work orders can have strongly vary￾ing processing times. In this type of production unit, the order throughput time is
often a multiple (often 5–20 times) of the processing time of the order. Continuously,
orders arrive and can be released to the department for production, and continu￾ously, orders are delivered by the department. Figure 14.2 gives a schematic example
of such a situation. For these kinds of production units (job shops), the production
control problem is both a dynamic (constantly new orders arrive) and a stochastic
(for many characteristics of the order only distributions are known) control prob￾lem. Given the fact that the actual processing time depends on many factors (like for
instance the operator behavior, the status of the machine, the quality of the raw mate￾rials, etc.) this actual processing time it, in general, varies around a norm processing
time. There is often a strongly fluctuating demand for capacity for certain operations
or work centers. Orders that are produced in January might need a lot of capacity at
work center I and hardly need capacity of work center II, whereas the orders that are
produced in February need a lot of capacity at work center II and little capacity at
work center III, etc. As a consequence, one has to deal with shifting bottlenecks. For
the order acceptance and the determination of the delivery date we, therefore, have
to consider the current and the to-be-expected utilization of the work centers, which14.2 Main Control Attention Points of Small Series Production 229
Finishing
Sawing
In Out Turning
Pressing Bending
Subcon
CNC center
Figure 14.2 Schematic example of a job shop production situation.
can become a bottleneck. It is important to have enough capacity flexibility, in total
and per operation or work center, to deal with the shifting bottlenecks. The logis￾tic performance requirements for such departments concern the utilization of the
resources, the throughput time, and the lead time (or throughput time) reliability.
In the next section we will discuss the main production control aspects, both con￾cerning the throughput time as well as concerning the throughput time reliability.
Moreover, we concentrate on the characteristics of the department and the control
rules that have a big influence on the throughput time and the throughput time reli￾ability. Further, we will discuss the influence of the structural characteristics of the
departments (resources, order sizes, processing times, and routing lengths) and we
discuss the influence of capacity allocation rules and priority rules on the average
throughput time followed by a discussion of different lead time determination rules
and shop floor control rules that influence the reliability.
Remark: In some situations time is spent on searching for the next order/
materials, or rework is needed due to bad quality/damaged items, etc. Often in
such a situation the term effective processing (or service) time is used. This is
the time needed to produce one good item. In the remainder of this section,
we will use the term processing time (or service time) instead of effective
processing time (or effective service time).
14.2 Main Control Attention Points of Small Series
Production
In a job shop production unit, more or less universal machines (drilling machines,
milling machines, etc.) are organized in work centers, and from each work center,
dependent on the product, orders can go to several other work centers. Each product230 14 Small Series Production
has its routing through the production unit, so in general, we have a lot of routings.
Machines often have stochastic processing times, can break down, produce prod￾ucts with different quality, etc. and orders arrive dynamically over time. In such an
environment, it is hard to predict when an order will arrive at a certain work center,
and even if this is possible, this prediction can be useless if new orders arrive. Yet,
we will have to give a delivery date to the customer, and we will have to take care
that this delivery date is highly reliable. This leads to the fact that lead time deter￾mination and throughput time control are the main control aspects of these kinds of
production units.
An important decision for production control in job shops is the sequence in which
orders are produced on the (restricted) capacity. This sequence decision may influ￾ence the realized financial utilization of the capacities (efficiency) and the timely
delivery of the orders (delivery performance or delivery reliability).
A second important decision for production control in job shops is the alloca￾tion of capacity. Since we have universal resources (machines and/or operators),
the throughput time of an order can be influenced by the allocation of the capac￾ity: an operator at the drilling machine who also has a sawing skill, for instance, can
be asked to do some sawing operations instead of the drilling operations which he
most of the time performs. Also, there are often situations where some operations
can be performed at different work centers, for instance, turning operations can be
performed on a CNC machine but also on a conventional turning lathe.
Work order scheduling is especially important if there are resources (e.g. oven or
press) for which several customer orders (for technical reasons or reasons of utiliza￾tion for instance) have to be combined into one production order. So, this raises the
question of how to combine the different customer orders. Work order detail plan￾ning is especially important in these environments (due to the dynamic, stochastic
character) to control the flow of the work orders.
For the control of the (shop) throughput times, the last main production control
decision is the release of the work orders to the shop floor. This release both con￾cerns the timing of the release and the choice of the order that will be released. If all
machines are fully utilized it doesn’t make sense to release a new order since it then
has to wait on the shop floor. Once released to the shop floor it is out of the control
of decoupling point control (DPC) and thus priorities can’t be changed any longer
(in the case of fully loaded capacities, delivering all orders on time will often be an
impossible task; by playing with the order release DPC can determine to a certain
extent which orders to delay).
For a good understanding of the control of repetitive small series production units
some elementary knowledge of queueing theory is necessary. Therefore, before dis￾cussing production control in more detail, we give a small introduction to Queueing
Theory.
14.2.1 Fundamental Results from Queueing Theory
A queue, or waiting line, develops in case the demand for capacity temporar￾ily exceeds the supply of capacity. The supply of capacity is often described by
the concept server; a server can be busy or idle and open or closed. For the
demand for capacity, the concept job (brought forward by a customer) is used. If14.2 Main Control Attention Points of Small Series Production 231
a job arrives at a server and the server is idle and open, then the server imme￾diately starts performing the job (the customer is immediately served). Often
several tasks have to be performed before one can start processing the order,
for instance, administrative tasks, set-up tasks, etc. Also, in some situations after
being processed the job might not immediately leave the resource, for instance,
it has to cool down or be inspected. The total time that a job occupies the
resource (administration time+set up time+···+processing time+cool down
time+inspection time+···) we will call service time, say s, and after s units of
time the job is finished, and the server becomes idle. All this time, from picking
the job from the queue of waiting jobs, until the job leaves the machine, the
machine is occupied by this job. Now, it is important that, although the time for
processing the job might be only a small part of the time the job occupies the
machine, the service time has to be used in all the calculations since all this
time the machine can’t be used for another job.
If a customer arrives and all work centers are busy, the job enters the wait￾ing line or queue of waiting jobs. Queueing theory investigates the relationship
between the characteristics of the queue on the one hand and the distribution
of the service times, the arrival pattern, the utilization rate, and the sequence in
which the jobs are processed, on the other hand. Characteristics of the queue
are, amongst others, the expected waiting time of a customer in the queue, the
variance of this waiting time, the expected queue length, the probability of n
waiting jobs in the queue, etc. The distribution of the service time has consider￾able influence on the waiting time of a job; if service times are constant then the
waiting time characteristics often are significantly lower compared to strongly
varying service times. An example of strongly varying service times is those that
follow a negative exponential distribution (see Figure 14.3). This distribution is
0.5
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1.0 1.5 2.0
x
f(x)
2.5
Figure 14.3 Probability density function of a negative exponential distributed variable.232 14 Small Series Production
often used in theory; it approximates the distribution of service times at work
centers in practice often pretty good.
The arrival pattern is characterized by the time between the arrival of two
jobs (inter-arrival time). Just like with the service times, if this is constant (for
instance, by using an appointment system), then the waiting time characteristics
often are significantly lower compared to strongly varying inter-arrival times.
Also, for the inter-arrival time distribution often a negative exponential distri￾bution is used.
The utilization rate 𝜌 is the quotient of the required capacity per unit of time
and the available capacity per unit of time. The required capacity is determined
by the inter-arrival times and the service times of the jobs, and the available
capacity is determined by the number of parallel servers and their hours of busi￾ness. One of the most well-known results of queueing theory is Erlang’s formula
(see Conway et al. 1967). This formula holds for the situation with one server
where jobs are processed according to the First-Come-First-Served (FCFS) prior￾ity rule and orders have a negative exponential distributed service time and a
negative exponential distributed inter-arrival time.
Erlang’s formula:
w = 𝜌 ⋅ s
(1 − 𝜌) (14.1)
where
w =the expected waiting time
s =the expected service time
𝜌 =utilization rate
Figure 14.4 gives a graphical representation of this formula. Many results are
known from queueing theory and all results have in common that they have
a curve like the one in Figure 14.4, that is that at high utilization rates, the
expected waiting time increases more than proportional and goes asymptoti￾cally to infinity.
We will discuss the following extensions of the basic situation for which
Erlang’s formula holds:
– Generally distributed service time
– Generally distributed inter-arrival time
– Disturbances in the availability of the server
– Parallel servers
General distributed service times.
A measure for the regularity is the coefficient of variation (often denoted by c),
defined as the quotient of the standard deviation and the average of a stochastic
variable:
c = 𝜎(x)
x
(14.2)14.2 Main Control Attention Points of Small Series Production 233
0.2
0
5
10
15
20
25
30
0.4 0.6
Utilization rate
Wav/Pav
0.8 1.0
Figure 14.4 Graphical representation of Erlangs formula in case the inter-arrival times
and the processing times follow a negative exponential distribution.
If all order characteristics remain the same Erlangs formula becomes Pol￾laczek’s (M/G/1) formula:
w = 𝜌 ⋅ s
2(1 − 𝜌)
(1 + c2) (14.3)
From this formula, it follows, for instance, that if the service times are constant
(c =0), the expected waiting time is half the expected waiting time in case the
service time follows a negative exponential distribution (c =1).
General distributed inter-arrival times.
Unfortunately, no simple, exact formulae (like Erlangs or Pollazcek’s formula)
exist for these situations. Approximations can, amongst others, be found in Tijms
(1986). An often-used, reasonable accurate approximation is the one investi￾gated first by Kingman and Atiyah (1961):
w = 𝜌 ⋅ s
(1 − 𝜌)
(c2
a + c2
s
2
)
(14.4)
with
c2
a = squared coefficient of variation of the inter-arrival time = 𝜎a
2∕xa
2
c2
s = squared coefficient of the service time
Regular inter-arrival times are very effective for decreasing the expected wait￾ing time and a means to create them is the use of so-called appointment systems,
like for instance found in hospitals. For inter-arrival times that are more regular
(compared to the negative exponential distribution), the effect is often even
stronger than the effect of more regular processing times.234 14 Small Series Production
Disturbances in the availability of the server.
Up till now, we assumed that the server is continuously available. In practice,
however, there are all kinds of disturbances, which lead to the unavailability
of servers for some time. These disturbances can be seen as a class of jobs that
have priority over all other jobs (rush orders). These jobs don’t lead to output but
occupy the server. Later, we will discuss a formula for these kinds of situations.
Although disturbances in practice do not exactly behave like rush orders, the
effects of disturbances can be investigated with this formula.
Parallel servers.
If we have more servers for one queue of jobs, the waiting time decreases
drastically. A rule of thumb is that if the utilization rate is about 85% or
more, the expected waiting time is about the expected waiting time in case
there is only one server divided by the number of parallel servers. If the
inter-arrival time distribution and the service time distribution are negative
exponential, the steady-state probabilities can be computed exactly, however,
the calculations are messy and provide little additional intuition, unlike the
following closed-form approximation (see Whitt 1993) in case there are m
parallel servers:
w = 𝜌
√2(m+1)−1
m(1 − 𝜌)
s (14.5)
If the inter-arrival time and/or the service time do not have a negative expo￾nential distribution, often the following approximation is used:
w = 𝜌
√2(m+1)−1
m(1 − 𝜌)
s
(c2
a + c2
s
2
)
(14.6)
Note that if m =1 then we have formula (14.4).
Many more situations have been studied, for instance, the effect of preempting
jobs, limited waiting queues, other priority rules then FCFS, etc., however for our
purpose the above suffices.
RELATION TO PRODUCTION CONTROL
In production control literature, a server is often a machine or a man-machine
system, and a job is often called an operation. A work order mostly consists
of several operations and goes via a network of waiting lines through the pro￾duction department. From classical queueing theory, it has become evident that
given a fixed capacity, even if there are pretty small variations in the inter-arrival
times and/or processing times, a 100% utilization is not feasible without an infi￾nite length of the waiting line. Insights from classical queueing theory can be
used to get an estimate of the throughput time and to agree upon delivery dates,
as will be seen later in this chapter.14.2 Main Control Attention Points of Small Series Production 235
Order throughput time for a production unit.
Thus far, we only have discussed throughput times per work center. The
throughput time of an order for a production unit, of course, is determined by
the throughput times of the work centers where the order has to be processed,
plus transportation times, etc.
Under some specific assumptions, the average order throughput time can be
determined analytically. Let us consider a production unit with several work cen￾ters and the following characteristics:
– Orders arrive at the department according to a Poisson process.
– At each work center, there are several (identical) machines.
– The service times have a negative-exponential distribution.
– At each work center orders are processed in a sequence that is independent
of the service times of the orders and independent of the waiting lines at
the work centers in the routing of an order (examples: RANDOM, FCFS, etc.).
– Transportation times and other times are neglectable (if necessary, it is not
too complex to take these into account).
– We can describe the transition of orders from one work center to another by
a so-called transition matrix. Table 14.1 is an example of such a matrix; each
element gives the probability for a random order to go from work center i
to work center j.
Table 14.1 Transition matrix.
I II III IV V Out
In 0.5 0.2 0.3
I 0.8 0.2
II 0.1 0.9
III 0.2 0.8
IV 0.8 0.2
V 1
With the characteristics given, the probability that an order from work center
i goes to work center j is independent of the work center that the order vis￾ited before work center i. In other words, sequential transition probabilities in a
stream of orders are independent of each other. Now it can easily be proven that
the arrival pattern of the orders at work centers in such departments follows a
Poisson distribution. As a consequence, the average throughput time per work
center can be determined by using the well-known waiting time formula. The
average work order throughput time then can be determined by adding up the236 14 Small Series Production
average work center throughput times of the work centers in the routing of this
work order. Under the given circumstances, determining the order throughput
time can thus be decomposed into determining the work center throughput
times. This is the well-known decomposition theorem of Jackson (1957).
It will be evident that the assumptions for using the decomposition theorem
are rather restrictive. In general, the transition probabilities are not completely
independent and the same holds for the service times of successive operations
in an order routing. Furthermore, often priority rules are used to get more regu￾lar arrivals of orders at a work center. Disregard these practical observations, the
order arrival patterns at the work centers often show irregular behavior, due to,
for instance, machine breakdowns, rejects of products for quality reasons, etc.,
and the decomposition theorem is a good basis for estimating the upper bound
of the order throughput time. Based on Jackson’s decomposition theorem, many
models have been developed to determine, given a configuration of machines
and a stream of orders, the relationship between utilization, utilization rate, and
order throughput time (Bertrand 1985; Solberg 1981; Durlinger 1985, 1986; Kar￾markar 1987). With these models, a justified, realistic choice can be made of a
combination of batch size (determines the number of setups and the processing
time for an order), utilization rate, and throughput time.
14.2.2 Throughput Time-Related Aspects
At the tactical level, some (organizational) measures can be taken that might ease
the control of the throughput time at the operational level.
14.2.2.1 Production Layout
As Burbidge (1971) has shown, a lot can be done at the tactical level to improve the
flow in a job shop. It will be evident that improving the flow has positive conse￾quences for the throughput time. He describes methods that he developed for the
systematic planning of the change from a traditional flow system to a line flow. It
attempts to show that the change need not be a leap in the dark. It can be planned
and justified at a low cost with considerable precision.
14.2.2.2 Measures Based Upon Insights from Queuing Theory
In a complex, functional organized production department, the order throughput
time is often determined by waiting for capacity to become available. Due to strong
varying routings and processing times, there is a lot of variance in the number of
job arrivals per work center per unit of time. In general, this leads to long average
throughput times as compared to the average processing time. From the queueing
theory, it follows that the following factors are important for the throughput time:
– Utilization rate
– Average service time
– The variance of the service time14.2 Main Control Attention Points of Small Series Production 237
– Average inter-arrival time
– The variance of the inter-arrival time
We will call these structural factors, which have to be considered at the design
level. Besides these, structural factors also control factors at the operational level:
– the sequence in which jobs are processed
– the use of flexible capacity
– the release of new jobs for the department
have an influence on the order throughput time. In this section, we will discuss
the structural measures. The control factors will be discussed in Section 14.3.
The effect of the structural factors on the throughput time.
From Queueing Theory we know that several factors have a strong influence on the
throughput time. The question now is what can be done with the structural factors to
improve the relationship between the average waiting time and the average service
time (and thus leads to a decrease in the throughput time) at a single work center.
We assume that:
– Orders arrive very irregularly, and the inter-arrival time, therefore, can be approx￾imated by a negative exponential distribution.
– The distribution of the service times is known; average s, coefficient of variation
y (= 𝜎s/s).
– There is no pre-emption: orders that have been started are not interrupted by other
orders, so once started an order first has to be finished before a new order can start.
– Orders are processed according to the FCFS sequence.
– Machines (or, more generally, resources) are continuously available.
With c parallel machines and a high utilization rate (𝜌 ≥0.85), a good approxima￾tion for the average waiting is given by:
w = 𝜌s
2m(1 − 𝜌)
(1 + y2
) (14.7)
Now the relation between the average waiting time and the average service time
can be structurally improved by (see also Eq. (14.3) or (14.5)):
a. Decreasing the utilization rate 𝜌. This can be done by decreasing the required
capacity and/or by increasing the available capacity. Bear in mind that the service
time often is more than the processing time. The processing time is often given
and fixed and can’t be changed. However, maybe other factors that determine the
service time and thus the utilization can be (partly) removed which decreases the
service time and thus the required capacity. Well-known, regarding this aspect,
is decreasing the utilization rate by decreasing the set-up time using the SMED
methodology (Shingo 1985). Also, it might be that not all available machine time
is fully utilized due to the absence of operators for instance during lunch breaks;
if this is a bottleneck work center then significant throughput time reductions
might be obtained by organizing the lunch break in another way. In this way, the
available capacity can be increased.238 14 Small Series Production
b. Decreasing the standard deviation of the service time s which leads to a decrease
in y2. This can be realized for instance, by splitting long production runs into
some smaller production runs (if technically possible). This leads to a smaller s
and 𝜎s but also often leads to more setups. Whether this leads to a decrease in
the average waiting time depends on the increase caused by an increase in the
utilization rate and the decrease caused by a decrease in the average service time
and the coefficient of variation of the service time. The effect can be investigated
by using one of the formulas (14.1)–(14.6) or using the formulas of Cosmetatos
(1976).
c. Decreasing the average service time of the orderss. Since an order often concerns
the processing of several items (the so-called lot size or batch size), a reduction of
the average service time can be realized by a reduction of the lot size. The effect of
this on the waiting time is obvious: halving the average service time of an order,
for instance, leads to halving the waiting time. Smaller batch sizes, however, often
lead to an increase in the set-up time, and thus an increase in the utilization rate,
so we have to recalculate the waiting time to see whether the overall effect is
positive.
d. Increase the number of parallel machines m, not by buying more machines but
by combining machines that are (almost) similar. By combining we mean that
instead of 1 queue of orders for each separate machine of a work center, we have
1 queue of orders for the set of (similar) machines in total. In this way the capac￾ity of the machines is better utilized: in the situation, with one queue for each
machine, a machine may run idle, while at the same time, there is a lot of work
at the other machine(s).
Besides the average throughput time, also the standard deviation of the through￾put time is influenced by the above-mentioned measures. In general, we can say that
if we have relatively long average throughput times, we often also have relatively
unreliable throughput times (having a relatively large standard deviation).
14.2.2.3 Customer Differentiation
In many situations, different classes of orders can be distinguished concerning the
desired throughput time (lead time). For instance, orders for a certain group of prod￾ucts always have to have a short average throughput time, whereas, for orders for
another group of products, longer average throughput times are acceptable. Another
example is distinguishing urgent orders from non-urgent orders. Suppose we have
A categories of order, with A different urgencies concerning the average throughput
time, in such a way that orders in category 1 have priority over orders in category 2,
which have priority over orders in category 3, etc. Then for a work center with one
machine, general distributed processing times and negative exponential distributed
inter-arrival times, the average waiting time for each category j can be determined
with the following formula (see Conway et al. 1967):
E(Wj
) =
∑A
i=1E(𝜆i
)E
(
s2
i
)
2
(
1 − ∑j−1
i=1 𝜌i
) (1 − ∑j
i=1 𝜌i
) (14.8)14.2 Main Control Attention Points of Small Series Production 239
with:
E(Wj
): expected waiting time for orders in category j
E(𝜆i
): expected number of arrivals of an order of category i per unit time
E(si
2): the second moment of the service time of orders in category i; if the service
times have a negative exponential distribution the following holds:
E
(
s
2
i
)
= 2[E(si
)]2
𝜌i
: utilization rate of the machine, due to orders from category i.
In case we want the different lead times to be reliable, then another way to differen￾tiate in lead times is by using operation due dates and a due date-oriented sequencing
rule (see Section 14.2.3). Research by Bertrand and Van Ooijen (1991) shows that this
can be done in a controlled way (reliable lead times) by using different allowed wait￾ing times (slacks) for setting the operation due dates of orders of the different product
classes: for work orders which belong to the fast category we assign operation due
dates which imply that there is only a little allowance for waiting, whereas for work
orders which belong to the slow category we assign operation due dates which imply
a large allowance for waiting. This is under the restriction that the weighted aver￾age (allowed) waiting times equals the average work center waiting time. Using the
operation due date sequencing rule (giving priority to the order that has to be fin￾ished first at the concerned work center) then “forces” the individual work orders to
flow at the rates implied by the due date with a small variance in work order lateness
(see Kanet and Hayya 1982). Of course, this procedure for setting flow rate norms
only makes sense if the actual flow rates follow the flow rate norms used for setting
the due dates.
The results from a simulation study of a pure job shop with two categories of prod￾ucts, described in Bertrand and Van Ooijen (1991), show that it is possible to create in
a simple way different predictable flow rates for different categories of work orders in
production departments. The method requires that the average work order flow rate
is under control, which has been achieved via Input/Output planning. It is shown
that the system can be “forced” to realize these flow rates, and therefore different
throughput times, by using operation due date sequencing as a priority rule. Reduc￾tion of the allowed slack up to 60% for orders in one product class, indeed leads to a
reduction of the actual waiting time by 60% for orders in this product class. Important
for application in practice is the observation that the validity of the balance equation
(sum of the required waiting times equals the overall work center waiting time) is
especially strong for high utilization rates.
14.2.3 Lead Time Reliability Related Aspects
We consider the production in relation to the environment and special in relation to
the requirements concerning flexibility and reliability of the deliveries. In general,
we distinguish between lead time, being the delivery time agreed upon with the cus￾tomer (so this is normative, used for instance for planning purposes), and throughput
time, being the time between the order release date and the completion time of the240 14 Small Series Production
l
j
Order release Completion
date order
Due date
Waiting time
Set up
rj dj
Set up
Start production Start production
Figure 14.5 A schematic overview of the determination of the lead time.
order. Given the order release moment, the lead time determines the due date (or
the other way around) and the throughput time determines the completion date. If
rj is the order release moment of order j, and lj is the lead time of order j then the
due date of order j, dj
, is given by: dj = rj + lj (see also Figure 14.5) The closer the
throughput time gets to the lead time (or the completion date to the due date), the
better the lead time reliability. The latter is influenced by:
– Internal shop floor control, especially the capacity allocation and the sequencing
rules.
– The lead time (or due date) determination rule.
14.2.3.1 Due Date Determination Rules
We can distinguish the following categories of due date determination rules:
1. Customer-dependent lead times. This more or less equals a situation where lead
times are determined randomly, since it is not determined by the company and
can have any value.
2. Constant, product, and order-independent lead times; all orders get the same lead
time.
3. Order-dependent lead times. With these rules order characteristics, like the num￾ber of operations or processing times, are considered in determining lead times.
4. Workload-dependent lead times. With these rules, the status of the load on the
shop floor is considered. When there is a lot of work the lead times are larger than
when there is little work.
1. Customer-dependent lead times
In this situation, the lead time equals the lead time the customer prefers. As
an illustration Table 14.2 gives the result of this lead time determination rule,
also called RDM (random), for the FCFS priority rules (also known as FIFO:
first in first out; for other priority rules see Section 14.3.2.1). These results are
obtained from Conway (1967). In this study, the production unit consists of nine14.2 Main Control Attention Points of Small Series Production 241
Table 14.2 The effect of due date determination rules on the delivery performance using
the FCFS priority rule; std = standard deviation.
Due date
determination rule
Throughput time Lateness
Priority rule Average std Average std % tardy
RDM FCFS 74 76 −5 88 41
CON FCFS 74 76 −4 76 34
NOP FCFS 74 76 −4 34 40
TWK FCFS 74 76 −5 41 45
work centers, each work center having one machine with a negative exponential
distributed processing time with mean one, random routings (with transition
probabilities of 0.11, so on average we have about nine operations per order) and
a utilization rate of about 90%. It is further assumed that the average lead time
promised to the customer equals the average throughput time. For the RDM rule
the lead times were uniformly distributed between 0 and 157.6 (this gives a stan￾dard deviation of the lead time of 45.5). Note that with FCFS the throughput time
of an order is not influenced by its Due Date.
2. Constant lead times
In this situation, all customers get the same lead time, independent of the number
of operations and processing times. So, whether an order has 10 operations with
a total processing time of five hours, two operations with a total processing time
of 100, or one operation with a processing time of one hour, they all get the same
lead time. This has also been investigated by Conway and results can be found
in Table 14.1. Note that with the RDM lead time determination rule the standard
deviation of the lateness is (considerably) larger than when the lead times are
CONSTANT.1
3. Order-dependent lead times
With the CON lead time determination rule all orders get the same lead time
although in a real job shop (as used by Conway) there is a high variety in the
number of operations per order. It will be clear that orders with few operations
have a lot of slack, whereas for orders with many operations, the slack can be
(too) tight. In these situations, due date-oriented priority rules will have a lot of
effect on the lateness. When there is a high variety in the order characteristics
(number of operations or processing times) lead time determination rules that
consider these differences lead to more reliable lead times. A rule that considers
the number of operations (NOP) is the NOP rule:
dj = rj + 𝛽 ⋅ gj (14.9)
1 The throughput time is independent of the lead time and thus we can add the variances.242 14 Small Series Production
with
rj
: release time of order j
gj
: number of operations of order j
𝛽: the allowed waiting time+processing time used for each operation
Often the variant NOP* is used:
dj = rj +
g
∑j
i=1
pij + 𝛽 ⋅ gj (14.10)
with pij: operation time (service time) of operation i of order j.
In this case, 𝛽 is equal to the allowed waiting time per operation.
We assume here that the work centers are more or less identical since the same
𝛽 is used for all operations. However, if the utilization rates or/and the processing
times differ, it is better to use a work center specific 𝛽j
. In practice 𝛽 may get the
“value” of 1 week or a couple of working days: each operation for an order has to
be finished within a week after the order has arrived at the work center. However, it
is better (more realistic) to use the historical data to determine the average waiting
time at each work center and to use this as a base for the value of 𝛽.
A rule that considers the total processing time is the TWK (Total WorK) rule:
dj = rj + 𝛼
g
∑j
i=1
pij (14.11)
with 𝛼: the slack used for each unit of processing time (𝛼 >1).
Also, here a good rule is to use historical data as a basis for the value for 𝛼. For
instance, 𝛼 can be determined as follows:
𝛼 = average shop throughput time
∑ processing times
The effects of these rules, using the FIFO rule, can be found in Table 14.1. The
value of the parameters 𝛼 and 𝛽 is such that the average lead time equals 78.8 (thus
the average lead time is more or less equal to the average throughput time), so 𝛼
is the ratio of the average throughput time and the total processing time and 𝛽
is the average throughput time per operation. It can be observed that the use of
order-dependent lead time determination rules leads to a strong reduction of the
standard deviation of the lateness compared to the situation with constant lead
times. Roughly we can state that a reduction of 50% is obtained.
4. Workload-dependent lead times
In most practical situations the number of orders, and the characteristics of those
orders, vary a lot from period to period. This of course has consequences for the
utilization of the resources and thus for the throughput times. Many companies
use capacity planning to evaluate the consequences of the potential orders concern￾ing capacity utilization. In this way, information about possible overutilization or14.2 Main Control Attention Points of Small Series Production 243
underutilization can be obtained early and measures can be taken to prevent this.
Examples of some measures are:
– replanning of orders
– hiring/firing of temporary workers
– outsourcing
– …
This, in general, leads to a smoother order arrival pattern and thus to a smoother
utilization of the resources which results in better control of the throughput time.
Although good capacity planning is essential for good delivery performance, it often
is not enough. This is caused by the fact that capacity planning in most cases only
influences capacity utilization in the medium term: due to the time it takes to imple￾ment measures (effectuation time). Therefore we will have to check whether the
planned lead times for orders that can not be replanned still are realistic. Also, the
actual available capacity might differ from the planned available capacity. For a good
delivery performance we thus also have to take into account the short-term vari￾ations in required and/or available capacity and eventually have to adapt the lead
times for the new orders that arrive. If the workload can vary a lot, it makes sense to
take into account the actual capacity utilization or workload when determining lead
times. A well-known method is the use of GANNT charts which show the capacity
load due to already accepted, but not yet finished, orders, and are used to fit new
orders concerning the remaining capacity. If a certain lead time leads to overuti￾lization of the capacity, this lead time is not accepted. Research (Bertrand 1983)
shows that this way of working indeed leads to a low standard deviation of lateness.
Figure 14.6 shows the results of this research where the production unit consisted of
Without capacity check
With capacity check
Standard
deviation
of lateness
6
7
8
012
β
345678
9
10
11
12
13
14
15
16
Figure 14.6 The effect of a capacity check on the standard deviation of the lateness.
Source: Bertrand et al. (1998).244 14 Small Series Production
five work centers, each work center consisting of one machine, 1–10 operations per
order, a negative exponentially distributed processing time with a mean of 1, and
a utilization rate of 90%. The due dates of new orders were determined in the fol￾lowing way: starting with the first operation of the order an operation due date was
determined which was equal to the operation due date of the previous operation plus
a constant 𝛽. With a capacity check, it was checked whether enough capacity was
available till this operation’s due date to perform this operation; if not, then the oper￾ation’s due date of this operation was delayed until the moment there was enough
capacity. The operation’s due date of the last operation was the delivery date of the
order. Moreover, the FIFO priority rule was used. It can be seen that the use of the
workload information in determining the delivery date (or lead time) indeed leads
to a (much) better delivery performance (lower standard deviation of the lateness).
The rule just described is quite cumbersome. A simple way to determine
workload-dependent due dates dj
, and thus workload-dependent lead times, where
also the number of operations is taken into account, in case all work centers have
more or less the same characteristics, is the following:
dj = rj +
g
∑j
i=1
pij + 𝛾𝛽t gj (14.12)
with
𝛽t: time-dependent slack at time t, which depends on the actual workload at the
shop floor
𝛾: parameter that determines which fraction of the expected mean waiting time
derived from the actual remaining workload 𝛽t is accounted for; most of the time
𝛾 will be equal to one.
The (approximate) relation between the slack per operation, 𝛽t, and the workload
at time t is as follows:
𝛽t = nt ⋅ p0
𝜌0 ⋅ m − p0 (14.13)
with
nt: number of orders at the shop floor at time t
𝜌0: an estimation of the long-term utilization rate
m: number of work centers
p0: an estimate of the average processing time per operation
Remark: At time t the expected waiting time equals:
wt =
( n
m.𝜌t
− 1
)
pt
with
pt: average service time at time t
𝜌t: utilization rate at time t14.2 Main Control Attention Points of Small Series Production 245
For relatively high utilization rates, the utilization rate will only slightly
change with changes in the Work in Process. So for high utilization rates, the
throughput time approximately changes directly proportional to the level of
Work In Process (number of orders). Therefore we replace ρ with a constant 𝜌0,
replace p (the average service time) with an estimate p0, and then, given the
number of orders equals n and the number of parallel machines equals m we
get the average waiting time per work center (based upon Little 1961):
wt =
( n
m ⋅ 𝜌0
− 1
)
p0
At time t we measure the level of Work In Process, nt, and use this equation
to get a value for the expected average waiting time per operation. For the
time-dependent slack parameter st we now use ( nt
m⋅𝜌0
− 1
)
p0.
The effects of the NOPt
* rule are illustrated in Figure 14.7 (from Bertrand 1983;
NOP** is used instead of NOPt
*
). In this figure also the results with the NOP* rule
are shown. The dashed lines give the standard deviation of the lateness in case the
NOP* rule is used in combination with the ODD (operation due date: priority is given
to the order that has to be finished first) or ODD-SPT priority rule. In this situation,
the value of 𝛽 is given (an input variable thus) and different simulations were run
with different values for 𝛽, so observations for a certain value of 𝛽 were always within
the same simulation run. The non-dashed lines give the results for the NOPt
* rule,
where 𝛽t depends on the workload at a certain moment. In this case, 𝛽 is not an input
Standard
deviation of
lateness
0
0123456
Slack parameter β
Workload dependent β
Workload
independent (”fixed”) β
7 8
NOP*/ODD-SPT
NOP*/ODD
NOP**/ODD-SPT
NOP**/ODD
8
9
10
11
12
13
14
15
16
17
18
19
Figure 14.7 The effect of constant and workload-dependent slack on the standard
deviation of the lateness for two priority rules. Source: Bertrand et al. (1998).246 14 Small Series Production
variable but a “result”: different simulations were run and for each value of 𝛽t that
was used in the simulation the standard deviation of the lateness was observed, so
observations for a certain value of 𝛽t came from different simulation runs, and the
average gives the value of the standard deviation of lateness for this 𝛽t.
Two priority rules have been used: ODD and ODD-SPT. With the latter rule,
we distinguish two classes: a class of orders being early relative to schedule and a
class of orders being relatively late to schedule. Late orders have priority over early
orders. Within the class of early orders, short operations have priority over long
operations. Late orders are sequenced using the ODD rule. The observations from
the simulation runs showed that for the cases with the ODD rule, the average actual
waiting time was about 7 and for the ODD-SPT rule this was about 5.2. We observe
that for all 𝛽-values, the standard deviation of the lateness is smaller with the NOPt
*
rule than with the NOP* rule and thus the NOPt
* leads to higher delivery reliability.
The improvements, however, are not that big. So, if we have a stationary order
stream (constant average utilization rate in the long term) a workload-dependent
lead time determination rule, which might be time-consuming, doesn’t bring many
benefits. This changes however, if the production unit operates in an environment
where (longer) periods with a high utilization rate (thus much workload) alternate
with (longer) periods with a low utilization rate (thus little workload); for instance,
if there is seasonality. In this situation, the average throughput times will vary
strongly and then it becomes interesting to use workload-dependent lead times.
Bertrand (1983) studied this situation and this study shows the effects of using
the NOPt
* rule on the average lateness in situations with, in the medium term, a
varying average utilization rate. The NOPt
* rule performs pretty well in forecasting
the change in the average throughput time and thus succeeds in controlling the
average lateness. So, just like a GANNT chart, this rule leads to a controlled average
lateness as well as to a small standard deviation of the lateness in non-stationary
environments.
14.2.3.2 The Effect of the Value of the Slack on the Delivery Reliability
An interesting question is what the effect is of tight average lead times (small value
for a or 𝛽; see Eqs. (14.9) and (14.11)) or loose average lead times (high value for
a or 𝛽) on the delivery performance. If we use tight lead times is then the average
throughput time shorter or is the standard deviation of the lateness then smaller?
If we use loose lead times (that are easy to achieve) is the standard deviation of the
lateness then smaller? Stated otherwise: does it make sense to put the production
unit under pressure, using tight lead times, or should they be given some leeway,
by using loose lead times? Eilon and Chowdhury (1976) investigated the effect on
lateness using the TWK lead time determination rule, dj = rj + a
∑gj
i=1 pij, for differ￾ent values of the slack parameter a, in a production unit with five work centers, each
work center consisting of one machine, with a utilization rate of 90%. The number of
operations per order varied between one and five and the processing time per oper￾ation had a normal distribution with a mean of 1 and a standard deviation of 0.33.
Figure 14.8 shows the effects of different values of 𝛼 on the lateness, using three kinds
of priority rules: FIFO, SPT, and U-SPT. With the latter rule, there are two classes: if14.2 Main Control Attention Points of Small Series Production 247
Average lateness
Slack parameter α
u-SPT
u-SPT
SPT
u = 50
u = 0
SPT
FIFO
FIFO
2
–7.5
–5.0
–2.5
2.5
5.0
7.5
10.0
12.5
15.0
0
4 6 8 10
Standard
deviation
of lateness
Figure 14.8 The average and standard deviation of the lateness for different values of the
slack parameter a. Source: Bertrand et al. (1998).
the remaining slack of an order is less than a certain value U then this order belongs
to the high-priority class, otherwise, it belongs to the low-priority class. High-priority
class orders have priority over low-priority class orders and within a class, the SPT
rule is used. The lower part of Figure 14.8 shows the average lateness as a function of
the slack parameter a and the upper part shows the standard deviation of the lateness
as a function of a. It can be observed that the average lateness changes directly pro￾portional to the given slack. Moreover, it can be observed that the standard deviation
of the lateness has the smallest value for the value of a where the average lateness
equals about zero (throughput time equals average throughput time). This can also
be concluded from the research of Bertrand (1983). Like Eilon, he used a produc￾tion unit with five work centers, each work center consisting of one machine, and
a utilization rate of 90%. The processing times had a negative exponential distribu￾tion with a mean of 1 and the number of operations per order varied between one
and ten. In this study, the NOP* lead time determination rule was used in combina￾tion with two priority rules: the ODD rule and the ODD-SPT rule. The results of this
study are shown in Figure 14.7. Also, here we observe that the lowest value of the
standard deviation of the lateness is obtained with a value of the slack parameter s
that gives a slack per operation that is more or less equal to the average waiting time
per operation (7 for ODD and 5.2 for ODD-SPT).
In general, we can say that the highest delivery reliability is obtained in case the
average given lead time equals the average throughput time. It makes no sense to248 14 Small Series Production
put the production unit under pressure by using tight due dates or to give them some
leeway by using loose due dates:
THE SLACK USED MUST BE JUST ENOUGH
With just enough it is meant that the slack is equal to the average throughput time.
The latter can be obtained from historical data or using Queuing Theory.
14.2.3.3 Internal Versus External Due Date
To obtain a good performance the slack used in the lead time determination rule
must equal the average waiting time. Let us call this lead time the internal lead time.
Using this lead time leads to an average lateness of zero and thus, on average, half
of the orders will be too late, and half of the orders will be too early compared to this
internal lead time. Often the delivery performance of the production unit must be
much larger than this 50%, for instance, 90% of the orders have to be delivered on
time. This might be solved by increasing the internal lead time, however, this leads to
an increase in the standard deviation of the lateness. A better solution, therefore, is to
use an external lead time besides the internal lead time. The external lead time is then
communicated to the customer, whereas the internal due date is used for control
on the shop floor. The difference between the external lead time and the internal
lead time is the safety time we need to compensate for the unavoidable standard
deviation of lateness. In practice, we often see that this distinction between internal
and external lead time is not made, with as a consequence that (much) longer lead
times have to be used to obtain a certain delivery reliability than in the case where
we have this distinction.
14.3 Production Control Decisions for Small Series
Production
Besides structural measures taken at the tactical level like the number of machines
per work center, mean and variance reduction of utilization rates, controlling the
arrival process, and measures to make capacity more flexible, several control deci￾sions at the operational level influence the order throughput time of orders in a job
shop. As already discussed, the most important control decisions are:
– The sequence in which orders are processed at the work centers.
– Capacity allocation.
– The release of new orders and/or work order detail planning.
Different control decisions have different effects on the throughput time and deliv￾ery reliability. We start in Section 14.3.1 with a discussion of the control decisions
effects on the average throughput time (speed) and next, in Section 14.3.2 we will
discuss the control decisions effects on the lead time reliability.14.3 Production Control Decisions for Small Series Production 249
14.3.1 Throughput Time
14.3.1.1 Sequencing
In practice we often see that planners tend to make detailed plans of the sequence in
which orders have to be processed, using, for instance, GANNT charts. As already
discussed in a previous chapter, making such detailed plans for stochastic, dynamic
environments does not make much sense. Continuously new orders arrive, and
orders are finished and leave the department. Moreover, the real utilization times
often (and sometimes significantly) deviate from the planned utilization times, due
to, for instance, machine breakdowns, quality, and operator availability. For that
reason, it makes more sense to use priority rules. As already said, priority rules
are very effective compared to complex sequencing rules, even for complex, static,
and deterministic situations. This, even more, holds for stochastic and dynamic
situations. A lot of different priority rules exist and some of them are useful in
decreasing the average throughput time and some of them are useful in improving
the delivery performance (see Section 14.3.2.1).
Two well-known rules don’t affect the average throughput time or the delivery
reliability: the First In First Out (FIFO) rule and the Random rule. We will continue
with discussing rules that have a positive effect on the average throughput time: the
Shortest Processing time rule and the Work In Next Queue rule.
Shortest Processing Time Rule A well-known priority rule that given certain condi￾tions has a strong effect on the average throughput time is the “Shortest Processing
Time first” rule (SPT rule). Just like for the static one-machine problem, this rule
minimizes the average throughput time per work center. The higher the utilization
rate the stronger the effect is, but also the higher the variance of the utilization time
the stronger the effect. Moreover, the effect decreases with an increasing number
of parallel machines at the work center. In Figure 14.9 we see the effect of this rule
compared to the FIFO rule for different utilization rates for a one-machine work
center. We see that at a utilization rate of 90% the waiting time under the SPT rule is
about 40% of the waiting time under the FIFO rule, so with the SPT rule the average
waiting time is about 3.6 times the average utilization time, whereas with FIFO the
average waiting time is about 9 times the average utilization time. The effect of the
SPT rule on the average waiting time per work center is not very sensitive for small
deviations in the “ideal” sequence: the dashed line gives the ratio of the average wait￾ing time under the so-called two-class-SPT rule and the average waiting time under
the FIFO rule. Under two-class-SPT, there is a threshold value for the service times
and the orders are divided into two classes: a class with orders having a service time
larger than this threshold value and a class of orders with a service time smaller than
this threshold value. Orders in the class with small service times have priority over
the other orders and within a class, the orders are sequenced according to the FIFO
rule. This “rough” rule already leads to a strong reduction in the average waiting
time compared to the FIFO rule. This allows combining the SPT effects with the
effect of other rules.250 14 Small Series Production
0 0.1
0.1
0.5
1.0
Ratio
0.5
Utilization rate
E(w)SPT
E(w)FIFO
E(w)FIFO
E(w)2-class SPT
1.0
Figure 14.9 Ratio of the
average SPT waiting time
and the average FIFO (or
FCFS) waiting time for a
one-machine waiting line
with Poisson arrivals and
negative-exponentially
distributed utilization
times. Source: Bertrand
et al. (1998).
The effect of the SPT rule is not only valid for the average throughput time per work
center but also holds for the average shop throughput time. These effects, however,
cannot be determined analytically and therefore we have to use simulation experi￾ments to verify them (e.g. see Conway et al. 1967).
Work In Next Queue Rule For deterministic static problems with more than one work
center, we have seen that the throughput time of a package of orders can be decreased
by arranging the sequence in such a way that there is as little idle time as possible.
Especially the effectiveness of Johnson’s rule is based on this principle. In this way,
we force back one of the causes of the development of queues: an irregular arrival
pattern. A regular arrival pattern is promoted by basing the sequencing decisions at
a certain work center on the waiting lines of the subsequent work centers each order
goes to when finished at this work center. Priority should be given to the order that
goes to the work center with the smallest waiting line. In this way orders that go to
heavily occupied work centers are being slow downed and orders that go to idle or
almost idle work centers are being sped up. This priority rule is called the Work In
Next Queue (WINQ) rule. For a study of the effects, we again refer to Conway et al.
(1967). From this study, it follows that for job shops with work centers with negative
exponential distributed (extended) processing times, this rule leads to work center
waiting times that are about 40% less compared to the work center waiting times
when using the FIFO rule.
Number of Unfinished Branches (In Case of Suborders) In several situations, a produc￾tion unit needs to produce components that in the next production unit are (sub-)
assembled. An order for a certain production unit then in fact consists of several
suborders, each suborder being an order for a specific component. Since the next
production unit can only start with the (sub-) assembly if all, or at least a part of
these, components are finished, it makes sense to control the flow of each of the14.3 Production Control Decisions for Small Series Production 251
suborders of an order, such that they are completed as much as possible at the same
time. Examples of such situations:
– The production of several mechanical components that at a certain time are
required for the (sub-) assembly of a certain product.
– The preparation of several documents that at a certain moment are required to
judge an application.
– The collection of several materials in a warehouse, each material having its place,
and that as one set must be sent to the customer.
In these cases, not only the throughput time of an individual compo￾nent/document/material is relevant, but especially the throughput time of the
whole order is important. The latter is determined by the throughput time of the
sub-order with the longest throughput time. Each sub-order has a completion
waiting time which is the difference between the throughput time of the order and
the throughput time of the sub-order. Now the throughput time of an order can be
decreased in two ways:
– A reduction in sub-order throughput times.
– A reduction in completion waiting times.
The easiest way to reduce the order throughput time is to prioritize in queues the
order with the lowest number of remaining operations for instance by giving each
sub-order in the queue a priority number that is inversely proportional to the num￾ber of remaining operations and to work on the sub-order with the lowest priority
number. This way of working is very effective if there are large differences between
the routing lengths of the individual sub-orders.
A reduction in the completion waiting time can be realized by taking care that the
individual sub-orders as much as possible are finished at the same pace. It will be
evident that prioritizing in situations where an order consists of several sub-orders
can be quite complex. On the one hand, we want to take measures to reduce the
individual sub-order throughput times and on the other hand, we want to take mea￾sures to reduce the completion waiting time. Moreover, the measures that must be
taken depend on the differences in routing length. However, there is a simple rule
that indirectly induces the former-mentioned aspects. With this rule, each sub-order
of an order is given a priority number that is equal to the number of unfinished
sub-orders of that order (NUB: Number of Uncompleted Branches). With this rule,
we have the following:
– Orders with a small number of sub-orders go faster than orders with lots of
sub-orders (a kind of SPT effect).
– The more sub-orders of an order are finished the higher the priority number for
the remaining sub-orders.
The disadvantage of this rule is that orders with many sub-orders might be (very)
long in the production unit (just like an order with a large processing time under
SPT). The quantitative effects of this rule have been investigated by Conway et al.
(1967) using a simulation study of a production unit with nine work centers, random252 14 Small Series Production
routings per sub-order, on average 1-hour service time, and a utilization rate of 90%.
In this study, the following variables were measured:
f o = average order throughput time
f s = average sub-order throughput time
f c = average order completion waiting time
Tc = average conditional tardiness
nT = fraction orders too late
and the following priority rules were used:
FIFO (FCFS)
SPT
FASFS: first arrived at the system, first served
NUB/SPT: the sub-order that belongs to the order with the smallest number of
uncompleted sub-orders gets priority; in the case of ties the SPT rule is used
RSO (see Section 14.3.2.1)
Three kinds of different order streams were used: a stream with orders consisting
of two suborders; a stream with orders consisting of five sub-orders and a stream
with orders consisting of ten sub-orders. The results can be found in Table 14.3.2
– SPT has a positive effect on the average sub-order throughput time (as expected),
however, if there are many sub-orders this leads to a (very) large completion wait￾ing time and a (very) large average order throughput time.
– FASFS has no effect on the sub-order throughput time but a positive effect on the
completion waiting time.
– NUB/SPT has a positive effect on the sub-order completion time. Because orders
that are almost completed are accelerated, it also has a positive effect on the com￾pletion waiting time.
– RSO has a small positive effect on the completion waiting time and the order
throughput time, but a very positive effect on the reliability: the average con￾ditional tardiness is pretty low, which is in contrast with the other rules (only
FASFS has a small positive effect on the average conditional tardiness compared to
FIFO).
14.3.1.2 Capacity Allocation
Waiting lines develop since in the short term more work arrives than can be pro￾cessed by the available capacity. If we can always adapt the capacity, waiting lines
would not occur and sequencing rules would be of minor importance. Many pro￾duction situations have, to a certain degree, flexible capacity or this can be created
by reallocating operators or the use of alternative machines.
Relocating operators: In this way, the capacity at a certain work center can be
increased at the cost of the capacity at another work center.
Use of alternative machines: If the production units are less specialized, some oper￾ations can be performed at several machines. However, this often goes at the cost of
2 For FIFO results for five and ten sub-orders are not available.Table 14.3 The effect of different priority rules in case orders consists of several sub-orders.
Number of sub-orders
per order 2 5 10
Priority rule f o f s f c Tc nT f o f s f c Tc nT f o f s f c Tc nT
FIFO 149.3 100.7 48.6 64.4 0.58 — — — — — —————
SPT 73.8 45.0 28.8 139.6 0.11 112.0 41.4 70.6 129.9 0.14 264.4 66.1 198.3 279.4 0.38
FASFS 121.1 98.7 22.4 50.7 0.52 122.1 83.8 38.3 33.4 0.22 171.7 117.3 54.4 58.1 0.26
NUB/SPT 72.3 62.0 10.3 92.8 0.12 96.9 69.4 27.5 74.0 0.10 144.9 102.6 42.3 129.6 0.15
RSO 108.1 95.4 12.7 12.55 0.10 135.4 102.1 33.3 4.3 0.01 198.2 158.7 39.5 51.2 0.12254 14 Small Series Production
efficiency. If the alternative machine has a relatively low utilization rate, this hardly
leads to problems.
Relocating Operators The quantitative effects of the use of multi-skilled operators
on the throughput time are well-known by the research of Nelson (1970) and Fryer
(1973, 1974). In Fryer’s research, there was plenty of available machine capacity (uti￾lization around 45%) and all operators could perform all operations. Since in practice
this depends on the organization (standardization, documentation, education, etc.)
and the information provision, this research shows what maximum can be obtained.
The following conclusions can be drawn from this research:
– Immediate relocating operators when they become idle (so the transfer time is
zero) leads to about a 50% reduction of the throughput time and also a 50% reduc￾tion of the variance of the throughput time, compared to the situation where
operators are not relocated.
– The realized reduction of the throughput time strongly depends on the time it
takes to relocate the operators (the time between the observation that relocation
is needed and the moment the operator starts at the work center he is relocated to);
this in turn strongly depends on a good progress signaling (information provision
system).
So, reallocating operators when they become idle can lead to great benefits, how￾ever, it requires that there is (plenty) of work available at the other work centers and
that the reallocation doesn’t take much time.
Use of Alternative Machines Processing orders on alternative machines is often
(very) limited since most machines can only perform a (very) restricted range of
different types of operations. For instance, a sawing machine can perform a lot of
different sawing operations, but it can’t perform a drilling operation. Operators on
the other hand often can perform several (very) different operations. The effect of
the use of alternative machines on the throughput time is investigated by Wayson
(1965) using computer simulation. In his simulation model, he used the same job
shop model as used by Conway et al. (1967). For two sequencing rules, FIFO and
SPT, the results are graphically presented in Figure 14.10. The dashed line indicates,
in percentages, how often an alternative machine is used for an operation. The
horizontal axis gives the (average) number of alternative machines that can be
used for an operation that in the first instance is assigned to another machine
(preferred machine). A value of 0.3 alternative machines means that for 30% of the
orders the operation on 1 alternative machine can be performed. The allocation
to an alternative machine takes no time. On each machine, orders are processed
according to the FIFO sequence. It can be seen that the use of alternative machines
can lead to large improvements in the throughput time: already when there is one
alternative machine for each operation, the average queue length decreases from
about 8.8 to about 2.3. A bisection of the throughput time occurs when about 40%
of the operations can be performed on an alternative machine. We see that there
is a significant effect of machine flexibility on the throughput time, however, this14.3 Production Control Decisions for Small Series Production 255
Number of times an
alternative machine is used
0.0
012345678
1.1
2.2
3.3
4.4
5.5
6.6
7.7
8.8
9.9 90%
80%
70%
60%
50%
40%
30%
20%
10%
0%
Average
queue length
Number of alternative
machines
FIFO
SPT
Figure 14.10 The average queue length as a function of the number of alternative
machines. Source: Bertrand et al. (1998)/Educatieve Partners Nederland.
requires in practice flexibility concerning work preparation, machine programming,
etc., and that the controller must accept that the incidental use of a less efficient
machine does not lead to economic disadvantages.
Capacity Adjustment Another aspect that can be regarded as capacity allocation is
the use of the capacity in a flexible way. If it is possible to work with, for instance,
flexible working day lenghts contracts, adjusting the available capacity for the com￾ing period by increasing/decreasing the number of hours the operators have to work
daily might improve the average throughput time (e.g. see Van Ooijen and Bertrand
2023). A flexible working hours contract might for instance be an agreement with
operators to work at least work 4 hours per day but at most 12 hours per day. How
much time they have to work daily can be determined periodically (for instance
daily or weekly) based for instance on the (characteristics of) the actual number
of orders. Using this workforce capacity flexibly can lead to a significant decrease in
total costs (work-in-process costs+lead time costs+tardiness costs+fixed capacity
costs), which decrease can be used for the flexible working hours contract. Van Ooi￾jen and Bertrand (2023) use a very simple, myopic procedure to adjust the capacity
(see Appendix 14.A). They study a symmetric job shop and assume that a work￾force contract exists that allows the shop management to adapt the daily working
time for all machine operators in a range between 1−r and 1+ r times the regular
working time of S hours. At the end of day t −1, the set of orders in the shop and
their operation due dates are observed. Using a simulation study, they show that the
use of flexible capacity decreases the average tardiness and reduces the total costs256 14 Small Series Production
(work-in-process costs+lead time costs+tardiness costs) significantly. This reduc￾tion can be used for the flexible working hours contract.
14.3.1.3 Work Order Release/Work Order Detail Planning
Work Order Detail Planning Work order detail planning is the function that decides
on the content of one work order e.g. by combining different customer orders into
one work order and on determining operations due dates which are necessary in
case an Operation Due Date (like) sequencing rule is used on the shop floor. Here
the question arises of how to combine different customer orders into production
orders to get a good utilization (and productivity) of some resources that have high
set-up times and/or a capacity that exceeds the average size of the customer order.
In general, also the question arises whether a production order should be formed at
the order release level (and be kept as one production order throughout the whole
production), or at the resources in question and be split again into smaller (e.g.
customer-based) orders after the production at these resources has taken place. Pro￾duction order forming at the resource means that a resource is set up for a new order
only if enough customer orders are waiting at this resource. This brings the question:
what is enough? The effect of dynamic production batch sizes has been investigated
by Dirne (1994). He investigated the situation where the production batch size may
vary between an lower limit and an upper limit (see also Appendix 14.B). The lower
limit determines when one may start with the production of a certain type (the total
number of items of the available number of orders for a certain type of product has
to be at least the lower limit) and the upper limit determines when to start with the
production of another type of product (if orders for other types are available).
If started with a product with a minimal batch size, new arriving orders for this
product might be worked on as long as the total number of orders of this product
that is being processed, does not exceed the maximum batch size. He changes the
minimum batch size from 1 to the maximum batch size. He concludes that dynam￾ically determining the production batch sizes in general leads to a smaller average
throughput time and that the lowest average throughput is obtained when using
a very small minimal production batch size. It does not make much sense to use a
lower limit for the production batch size. Intuitively this can be explained as follows:
– If during some time relatively few orders arrive, a lot of capacity will not be used
and therefore it does not harm to produce in small batches.
– On the other hand, if during some time relatively a lot of orders arrive, capacity
will be scarce, and we will have to use large production batches. However, in (very)
busy times the queues are large and thus there are many orders available to form
large production batches.
By using dynamically determined production batch sizes we get a kind of
self-regulating system concerning the use of capacity.
Work Order Release Next to work order detail planning (which is only about forming
production orders and giving timestamps for each operation) one might consider14.3 Production Control Decisions for Small Series Production 257
using an explicit work order release function, which considers the situation on the
shop floor. Work order release deals with two decisions:
1. When to release a work order
2. Which order(s) to release
Ad 1. When all capacities at the shop floor have long waiting lines, it seems that
it does not make sense to release new work to the shop floor since these orders then
have to wait on the shop floor. This has been investigated for instance by Bertrand
and Wortmann (1981) (Balancing Workload), Bechte (1988) (Load Oriented Order
Release), Kingsman et al. (1989) and Hopp and Spearman (2008) (Conwip).
Ad 2. If there is an opportunity to release a work order the question is: which
order(s) to release? This can be done FIFO, randomly, based on the current workload
at the different work centers a candidate work order has to visit, etc.
With workload-controlled order release the shop floor throughput times (which
are a function of the workload limit) are smaller than with uncontrolled release.
If for some orders a (relatively) small total throughput time is important then
the planner (or Decoupling Point Controller) can benefit from this by prioritizing
these orders which results in a small buffer waiting time (time waiting for release),
and thus a smaller total throughput time for these orders. The decision on which
customer is relevant is now to be taken by the planner and not by the operators;
chasing or expediting orders is no longer necessary and the shop floor is in full
control of the shop floor manager. Orders that cannot be released and might get
a long total throughput time can be decided upon by the planner to sub-contract
(which can be very effective, see Bertrand and Sridharan 1993), to use overtime,
or to change the due date in consultation with the customer. If the capacity can
be adapted this can be very effective (Onur and Fabrycky 1987; Van Ooijen 1996)
and this can be seen as a form of workload control. In practical situations, some
benefits of using an explicit work order release function have been observed,
which mainly are because, given the status of the queue of orders waiting to be
released, the capacity is increased or the productivity increased. If these factors
are not considered it is easy to prove that the total throughput time, being the
time waiting to be released plus shop throughput time, is always larger than in
the case of immediate release. Van Ooijen (1996) shows that if there is a relation
between the number of orders at the shop floor and the effective processing time,
an explicit work order release function might help to control the number of orders
at the shop floor such that the effective processing time is as low as possible, which
positively influences the throughput time. The existence of such a relationship is
based upon the work of Schmenner (1988) and Wickens et al. (2013): when the
number of work orders on the shop floor increases, the work pressure increases
which increases the level of arousal of the shop floor personnel. This in turn
has an impact on the perception, information processing, decision-making, and
actions of the shop floor operators and its shop floor management, which all
have an impact on the performance (see Wickens et al. 2013). For individuals,
the relationship between arousal and performance is known in psychology as258 14 Small Series Production
Level of arousal
Low
Poor
Performance
Good
Optimum level of arousal
Simple task
Complex task
High
Figure 14.11 The relationship between arousal and performance. Source: Wickens et al.
(2013)/Taylor & Francis.
the Yerkes-Dodson law (see Figure 14.11, taken from Wickens et al. (2013)). The
Yerkes-Dodson law models the relationship between arousal and performance as
an inverse U-shaped curve. This implies that for a given individual and a given
type of task, there exists an optimal arousal level. This is the level where the
performance has its maximal value. Thus, work pressure is productive, up to a
certain point, beyond which performance collapses. Van Ooijen (1996) investigates
a production situation with a very simple form for the relationship between
productivity and workload (see Appendix 14.C) and shows that using work-load
controlled release for such a situation leads to better performance than immediate
release.
Some Practical Considerations In practice, we often observe that throughput times
are longer than what might be expected, given the utilization rates of the resources.
Most of the time this has one of the following causes:
– The amount of work-in-progress on the shop floor is uncontrolled: the production
unit is put under pressure by releasing many work orders in anticipation of higher
productivity. The result is often the opposite: there is a lot of work-in-progress,
many priority changes, interruptions of processes by rush orders, no trust in the
planning, etc.
– At the work order release level (sometimes at the sequencing level by the opera￾tors), large orders or combinations of small orders are prioritized. By prioritizing
large orders we get an LPT (Longest Processing Time) effect which increases the
average throughput time compared to FIFO (see Conway et al. 1967). Combin￾ing several small orders increases the need for orders (to prevent machines from
running idle). For instance, if we have 10 orders in a five-machine shop the prob￾ability of machines being idle is much lower than in case we combine these 10
orders into 1 order; in the latter situation the probability of being idle is for four14.3 Production Control Decisions for Small Series Production 259
of the five machines 100%). More orders on the shop floor increase the average
throughput time.
– In the case with long sequence-dependent set-up times, it can be observed that
“platform” waiting times occur. To obtain good production efficiency often cyclic
production programs are used. In that case, a work order must wait until the pro￾duction cycle has reached the point that the machine is set up for that work order.
– Sometimes machines are (very) susceptible to interference. This delays the arrival
of orders at the other work centers. Especially in situations where the orders have
more or less the same production sequence, this has a large effect on the through￾put time.
14.3.2 Lead Time Reliability
14.3.2.1 Sequencing
To control the lead time reliability some form of due date-oriented priority rule has to
be used. Priority rules that aim for high delivery reliability consider the delivery dates
(or due dates) of the individual orders. Due date-oriented priority rules especially
affect the standard deviation of the lateness L. In general, the average lateness L, is
equal to the average throughput time f minus the average lead time agreed upon, l:
L = f − l
Since, in general, considering the due date in setting the priority does not influ￾ence the average throughput time (unless there is a relation for instance between the
operation time and the lead time), this also doesn’t influence the average lateness.
There are different types of due date-oriented priority rules; the most well-known
are:
1. Due Date (DD): the priority number is based on the delivery time dj of order j.
For instance, suppose we have three orders: order 1 with a delivery date of
12 November, order two with a delivery date of 14 December, and order three
with a delivery date of 5 September. Then order 3 gets the highest priority,
followed by order 1, and finally, order 2. We can say that the priority number is
equal to the delivery date.
2. Operation Due Date (ODD): with this rule, besides an order delivery time, a deliv￾ery time for each of the operations is determined; this operation delivery time
then functions as a priority number. A way to determine the delivery time per
operation when the order delivery time is known is to use normative (expected)
waiting times for each operation and to determine, starting with the last opera￾tion, by backward calculations the delivery time for each of the operations:
ODDj,i = ODDj,i+1 − nwi+1 − pj,i+1
with
ODDj,last operation: DD
nwi+1: normative waiting time at the workstation where operation i +1 of the
order is executed
pj,i: processing time of the ith operation of job j260 14 Small Series Production
Release date
nw1 Pj1
ODDj,1
nwn Pjn
ODDj,n–1 ODDj,n = DDj
Figure 14.12 A graphical representation of (Operation) Due Date determination.
Sometimes the Operation Due Dates are determined by forward calculations,
using normative waiting times per operation:
dj,i = ∑i
l=1
(nwl + pj,l
)
with
nwl
: normative waiting time at the workstation where operation l of the order is
executed
pj,l: the processing time of the lth operation of job j
The Operation Due Date of the last operation is the Due Date of the order. A graph￾ical representation for order j with n operations is given in Figure 14.12.
3. Remaining Slack (RS): the priority number equals the delivery time, dj
, minus
the current time, t, minus the sum of the remaining processing times:
RSj,t = dj − t −
g
∑j
i=gj−hj,t+1
pji
with
gj
: the total number of operations of job j
hj,t: the number of remaining operations of job j at time t
t: the current time
pji: the processing time of the ith operation of job j
4. Remaining Slack per operation (RSO): the priority number equals the remaining
slack (RS) divided by the number of remaining operations of order j at time t:
Sj,t = RSj
hj,t
with
hj,t: the number of remaining operations at time t
A variant of this rule is the critical ratio, which is determined as follows:
CRj,t = dj − t
∑gj
i=gj−hj−t
(pj,i + nwj,i
)
If CRj,t is smaller than 1, the order is lagging behind schedule, if CRj,t is larger than
1, the order is ahead of schedule.14.3 Production Control Decisions for Small Series Production 261
Quantitative Results of Using Due Date-Oriented Priority Rules As already said, due
date-oriented priority rules mainly affect the standard deviation of lateness. To show
the effect of due date-oriented priority rules on the standard deviation of lateness
we compare these rules with the FIFO and the RANDOM rule. With the RANDOM
rule, the priority on the shop floor is determined by criteria that, in general, don’t
consider the delivery performance. Since this rule often can be seen in practice, it
is useful to compare the effects of this rule on the standard deviation of the late￾ness. It will be clear that the quantitative effects of due date-oriented priority rules
strongly depend on the kind of production unit (number of work centers, number of
operations per order, utilization rates, lead time determination rules, etc.). We again
will use the simulation results of Conway (1967). We start with the situation where
all customers get the same lead time (the so-called CONSTANT rule: dj = rj +l). The
FIFO priority rule leads in the experiments of Conway to an average throughput time
of 74.4 hours, where a lead time of 78.8 hours is used). Table 14.4 gives the results of
all the experiments. In this table, we observe that:
1. The standard deviation of the throughput time is equal to the standard devia￾tion of the lateness. This can be explained by the fact that for all orders the same
constant lead time l is used, thus var(L) = var( f −l) = var( f). The distribution
functions of the throughput time and the lateness are equal, except for a shift
over the lead time.
2. The more detailed the delivery date is considered in determining the priorities,
the more the standard deviation of the lateness decreases. Quantitatively:
a. With FIFO the standard deviation of the lateness is about 0.7 times the stan￾dard deviation under the RANDOM rule.
b. Using the due date rule (DD) [or the remaining slack rule (RS)] in this job shop
leads to a standard deviation of about half the standard deviation under FIFO.
c. Using more detailed information (the RSO) rule gives an extra decrease of the
standard deviation of about 15%.
d. The SPT rule performs well concerning averages; however, the standard devi￾ation of the lateness is higher than with the DD or RSO rule.
We also compared the effect of the different priority rules for the different lead
time determination rules discussed in Section 14.2.3.1. Table 14.5 gives an overview
Table 14.4 The effect of priority rules on delivery reliability in case a constant lead time is
used.
Throughput time Lateness
Priority rule Average stdev Average stdev % tardy
Random 75 104 −4 104 NA
FIFO 74 76 −4 76 34
DD 73 40 −6 40 44
RSO 74 33 −5 33 48
SPT 34 48 −45 48 11262 14 Small Series Production
Table 14.5 The effect of priority rules on delivery reliability for the different lead time
determination rules.
Due date
determination rule
Throughput time Lateness
Priority rule Average std Average std % tardy
RDM FIFO 74 76 −5 88 41
RDM DD 73 54 −6 46 49
RDM RSO 74 51 −5 41 53
RDM SPT 34 48 −45 67 20
CON FIFO 74 76 −4 76 34
CON DD 73 40 −6 40 44
CON RSO 74 33 −5 33 48
CON SPT 34 48 −45 48 11
NOP FIFO 74 76 −4 34 40
NOP DD 71 83 −8 23 27
NOP RSO 73 75 −5 15 22
NOP SPT 34 48 −44 54 6
TWK FIFO 74 76 −5 41 45
TWK DD 64 82 −16 21 18
TWK RSO 66 74 −13 15 4
TWK SPT 34 48 −45 54 5
(based on the earlier-mentioned simulation results of Conway et al. (1967). Roughly
we can state that for each lead time determination rule going from FIFO to DD to
RSO a reduction of 50% in the standard deviation of the lateness is obtained. Also, for
each priority rule going from FIFO to CON to NOP/TWK a reduction of roughly 50%
is obtained. So, both the priority rules and the lead time determination rules have,
independently of each other, their contribution. If we have a closer look at the TWK
results we see that this rule in combination with the priority rule DD or RSO leads
to a significant decrease (about 14% resp. 11%) in the average throughput time. This
can be explained by the fact that with the TWK rule, orders with high processing
time(s) get a relatively long lead time and thus in the queue waiting at a work center
have a low priority for some time, which implicitly leads to an SPT effect on the aver￾age throughput time. Roughly we can state that due date-oriented rules compared
to the FIFO rule give a reduction of about 50–60% of the standard deviation of the
lateness.
14.3.2.2 Capacity Allocation
As discussed in the section on average throughput time-related aspects, the use of a
flexible working hours contract might also lead to an improvement in the lead time
performance (see Van Ooijen and Bertrand 2023). When the workforce capacity can14.4 Application 263
be used in a flexible way the production unit can work with shorter lead times and at
the same time less tardiness. The decrease in cost can (partly) be used for the flexible
working hours contract.
14.3.2.3 Work Order Release
If the number of production orders that are released to the shop floor is limited, this
increases the reliability of the shop lead time. However, since there is still a lot of
variance in the orders waiting to be released, the total lead time reliability in general
decreases (variance in lateness increases). Only sophisticated release rules (proactive
and trying to balance the load of the different work centers) with appropriate values
for the different parameters do not lead to a significant decrease (Van Ooijen 1996).
14.4 Application
VEPA is a producer of packaging machines. Two production units can be distin￾guished: a production unit where components are produced and a production unit
where the components are assembled. In the component PU, components are pro￾duced at six work centers (I–VI). Two categories of components can be distinguished:
a category (regular) consisting of the products A and B that are frequently asked for
and a category (non-regular) of products that have irregular demand. The routings
of the products in the non-regular category are given in Table 14.6. Component A
has routing VI-II and component B has routing VI-III-IV-III. The demand for com￾ponent A is three times the demand for component B.
Work center III consists of one machine that has to be set up for each order; this
takes exactly 0.5 hours. However, this setup is tricky and therefore the quality has
to be checked. This is done in the following way: after the first product of an order,
which is always for 100 products, is produced, this product is checked. This is done
by someone from the Quality department who is not always immediately available.
On average it takes 0.5 hours before the product is checked and the machine is tuned
according to the findings. The standard deviation is pretty small and can be ignored.
If work center III is visited for the second time in the routing, the average processing
Table 14.6 The transition matrix for the routings of the products
in the non-regular category.
I II III IV V Exit
In 0.5 0.2 0.3
I 0.8 0.2
II 0.1 0.9
III 0.2 0.8
IV 0.8 0.2
V 1264 14 Small Series Production
Table 14.7 The average processing times (in minutes per product) for each of the six work
centers.
Work center I II III IV V VI
Processing time 1 min 0.5 min 0.5 min (1e oper.)
1.0 min (2e oper.)
1 min 0.75 min 3 min
time is two times the average processing time that is required the first time. Also, on
the second visit, the machine has to be set up and the quality has to be checked. This
goes in the same way as holds for the first visit. Work center III has one machine
that has to be set up for each order. This takes exactly 50 minutes.
The processing times at the machines at the work centers (each having one
machine) follow a negative exponential distribution; the averages are given in
Table 14.7. On average there are 100 orders per week; 40% is for products in the
category regular and 60% is for products in the category non-regular. The arrival
process can be characterized as Poisson and at each work center orders are pro￾cessed according to the FCFS (FIFO) priority rule. The company operates 16 hours
per day, 5 days per week, and 45 weeks per year. The average throughput time of a
product at work center III can be determined in the following way:
The number of regular orders at WC3: 20 per week (2 × 10, type B; twice, since
WC3 is used two times in the routing). The number of non-regular orders at WC3:
14.4 per week.
Average service time: (10 × 110+10 × 160+14.4 × 110)/34.4 = 124.53 minutes.
Utilization rate: (10 × 160+10 × 110+14.4 × 110)/(60 × 80) = 0.8925.
Ep2 = (10 × (502 + 1102
) + 10 × (1002 + 1602
) + 14.4
× (502 + 1102))∕34.4 = 20 705
𝜎2
3 = 20705 − (124.53)
2 = 5196.93
y2 = 5196.93∕(124.53)
2 = 0.3351
TPT3 =(0.8925 × 124.53) × (1+0.3351)/2 × (1−0.8925)+110 = 824.70 minutes.
At this moment VEPA uses the same constant due dates for all products. However,
customers of products in the category regular have complaints about the delivery
performance; sometimes these products are delivered much too early, and at other
times the products are delivered much too late. Now VEPA wonders whether the
delivery performance is improved in case they want to use the same, constant lead
time for all customers, and if yes, how they should do this and what the improvement
would be. If VEPA wants to use the same constant lead time the only instrument that
is left, is the control at the shop floor: the priority rule. The delivery performance
can be improved by using a due date-oriented priority rule such as EDD or RSO.
This leads to a large decrease in the standard deviation of lateness (about 50%) and
a decrease in earliness and tardiness.
The next question they have is whether the delivery performance can be improved
even if they want to use FCFS (FIFO). The only instrument they then have is the lead14.A Short-Term Capacity Adjustment 265
time determination rule, for instance, they can use an order-dependent lead time,
such as NOP or TWK. This leads to a large decrease in the standard deviation of
lateness (about 50%).
Based upon an analysis of the throughput times it appears that especially the aver￾age throughput time at work center IV seems to be quite large. A brainstorming
meeting results in the next three alternatives:
a. Orders for products in the category regular have priority over orders in the cate￾gory non-regular at work center IV; within each category, the FCFS (FIFO) prior￾ity rule is used.
b. The orders are classified into a category of orders with large processing times and
a category of orders with small processing times. Orders in the category with large
processing times have priority over orders in the category with small processing
times at work center IV; within each category, the FCFS (FIFO) priority rule is
used.
c. No distinction in categories but use the EDD rule instead of the FCFS (FIFO) rule.
a. Since the production characteristics of the regular products and the non-regular
products are the same, prioritizing the regular products doesn’t influence the
average throughput time.
b. We now have the 2-class SPT rule, and this leads to a decrease in the average
throughput time by about 30% to 40%.
c. Using a priority rule that doesn’t consider processing time characteristics doesn’t
influence the average throughput time.
If VEPA wants to improve the average throughput time, they have to use the 2-class
SPT rule in this case.
Appendix 14.A Short-Term Capacity Adjustment
From deterministic scheduling theory, it is known that the average throughput time
of a set of orders {j =1, …, N}, with processing times p(j) and due dates d(j), that are
processed on a single machine in the order 1, …, N, is given by (Baker 1975):
(1∕N)
∑N
1
(N − j + 1)p(j)
Furthermore, they assume that the shop’s daily working time decision is decou￾pled from the job sequencing decision. Therefore, when taking the working time
decision, the job sequence is unknown, and it is assumed that there is a random job
sequence out of all possible job sequences. The average throughput time of the jobs
in the job set, assuming a random job sequence is equal to:
(1∕N)
∑N
1
(N − j + 1)E(p) = ((N + 1)E(p))∕2
with E(p) = (1/N)
∑N
1 p(j), being the average processing time of the jobs in the job set.266 14 Small Series Production
The average remaining regular working time until the due date of the jobs in
the job set, which is equal to (1∕N)
∑N
1 (d(j) − t), should be equal to the average job
throughput time (then the average lateness is 0). Thus,
((N + 1)∕2N)
∑N
1
p(j) should be equal to (1∕N)
∑N
1
(d(j) − t) (14.A.1)
If the d(j) and N are given, this equality can be satisfied by multiplying the sum
of the job processing times with a positive factor 𝛾. Note that the processing times
are expressed in hours, and the production capacity of the machine is one hour per
hour. The average processing rate of the machine expressed in jobs per hour, given
the jobs in the job set, is equal to: N/(∑N
1 (p(j)). Suppose the processing rate of the
machine can be varied. An increase in the processing rate would imply a propor￾tional reduction in the processing times, and a decrease in the processing rate would
imply a proportional increase in the processing times. Thus, if the factor 𝛾 is inter￾preted as the processing rate of the machine, it can be used to make the average job
set throughput time equal to the average due date in the job set. The processing rate
of the machines, expressed in the number of orders per calendar day, can be varied
by varying the operator’s working time per day and the working time control rule
makes use of this property. The working time per day is set such that the average
lateness of the jobs in the job set is equal to zero. It is assumed that extension or
compression of working time is realized by varying the starting time of the working
day while keeping the ending time of the working day fixed. At the end of a working
day, the working time for the next day is set to 𝜔 hours such that; 𝜔 = S/𝛾, where S
is the regular working time per day expressed in hours.
Since, by definition:
𝛾 =
(
(1∕N)
∑N
1
d(j)
)
∕
(
((N + 1)∕2N)
∑N
1
p(j)
)
= 2∕(N + 1)
(
∑N
1
(d(j) − t)∕∑N
1
p(j)
)
it follows that:
𝜔 = S((N + 1)∕2)
(
∑N
1
p(j)
)
(14.A.2)
The previous analysis pertains to a single-machine situation. A job shop consists
of multiple work centers, and jobs require multiple operations to be executed on
the work centers. Let M denote the number of work centers and let each work cen￾ter consist of a single machine. At the arrival of a job j, operation due dates are
assigned to each operation k, of the job. The operation due times d(j,k) are expressed
as moments during regular working time. The job due date (in calendar days) is
obtained from rounding off the operation due time of the last operation of the job to
the end of the working time of the day in which this operation due date falls.
In determining the working time per day per individual machine, they use the
operation due time to calculate the slack time for each operation in that machine14.B Flexible Batching 267
queue at the end of the previous working day. In this way, working time per day can
be calculated for each machine based on the processing times and operation due
times of the jobs at the machine. The working time for day t is calculated at the end
of the working day of day t −1, according to the following procedure. At the end
of each day, Eq. (14.9) is used to determine a target working time, wt, for the next
day. If this target value satisfies (1−r)S < wt <(1+r)S, then wt is implemented. If
wt <(1−r)S, then the working time is set equal to (1−r)S; if wt >(1+r)S, then the
working time is set equal to (1+r)S.
Appendix 14.B Flexible Batching
The effect of using dynamic (production) batch sizes has been investigated by Dirne
(1994) using several simulation experiments. He used a one-machine work center
and eight product-type groups. Per product type group the orders arrive according to
a Poisson process with an inter-arrival time of ten units of time. The processing times
follow a negative exponential distribution with an average processing time of one
time unit. The service time due to processing thus needs 80% of the total production
capacity, so there is 20% capacity available for set-ups. We assume that the set-up
time between groups follows a negative exponential distribution with an average of
1.25 units of time. One of the results in Figure 14.B.1 is for the situation where a
fixed batch size is used (Qfixed); per product type one has to wait until Qfiex number
of orders for product types in this group has arrived before this batch is put into the
queue at the machine. The average completion waiting time, CW, per work order is
then equal to (Qfixed −1)/2𝜆, where 𝜆 is the average arrival rate (if Qfixed is equal to 5
the total utilization rate equals 100% and thus the throughput time goes to infinity).
Figure 14.C.1 also shows the results in case the batch size might vary between a Qmin
and a Qmax, that is: for each product type group one has to wait until Qmin orders for
product types in that group have arrived before this batch can be put into the queue
at the machine. Orders for product types within the group that arrive between the
moment the batch is put into the queue at the machine and the moment one starts
processing this batch are added to this batch as long as the total batch size doesn’t
exceed Qmax. If the batch size equals Qmax a new batch is formed. In the results in
Figure 14.B.1, the maximum batch size Qmax has been set to 20; the minimum batch
size is varied between 1 and 20. We observe that over the whole range, the dynamic
batching rule leads to a lower average throughput time compared to a fixed batch
size. The lowest average throughput time in this simulation experiment is especially
obtained with a very small minimum batch size. It doesn’t make sense to use a lower
bound for the batch size of a production batch. Intuitively this can be explained as
follows:
– If during some time relatively few orders arrive, a lot of capacity will not be used
and therefore it does not harm to produce in small batches.
– On the other hand, if during some time relatively a lot of orders arrive, capacity
will be scarce, and we will have to use large production batches. However, in (very)268 14 Small Series Production
0
0
20
40
60
80
100
120
140
160
180
200
2 4 6 8 10 12 14 16
(Minimal) batch size
Qfixed
Qmax = 20
Average throughput time
18 20
Figure 14.B.1 Average work order throughput time as a function of Qmin, Qmax, and Qfixed.
busy times the queues are large and thus there are many orders available to form
large production batches.
By using dynamically determined production batch sizes we get a kind of
self-regulating system regarding the use of capacity.
Appendix 14.C The Effect of Workload Control in Case
There Is a Relationship Between Productivity and
Workload
Suppose the relationship between workload and production performance can be
modeled by interpreting the processing time as the time that is needed to produce
one good item (manufacturing time plus handling time plus rework time plus, etc.).
We assume that the processing time of an operation is affected by the workload that
exists at the start of the operation. For instance, if the level of workload is beneath the
optimal level, the operators are less alert which leads to an increase in the time that
is needed to operate. If, on the other hand, the level of the workload level is beyond
the optimal level, operators need more time to process the information, and to make
decisions and for instance, due to the high level of arousal, might make more mis￾takes. Let us model the effect of arousal on processing time as follows. Assume that
the operation processing time consists of a minimal processing time, called the fixed
part, which is equal to the processing time under the ideal workload, and a variable
part, which, over certain a range of the workload, is proportional to the absolute
value of the difference between the actual workload and the ideal workload. This
variable part represents the impact of the workload via the arousal on operator effi￾ciency. The sum of the fixed part and the variable part will be called the effective14.C The Effect of Workload Control in Case There is a relationship Between Productivity 269
processing time. In formulae:
pj =
(
1 + 𝛼
|Wj − IW|
IW )
mj for Wj < 𝛽IW
pj = (1 + 𝛼(𝛽 − 1))mj for Wj < 𝛽IW (14.C.1)
where
pj = the effective processing time (measured in time units) of operation j
Wj = the actual workload (measured in the number of work orders) in the system
at the start of operation j
IW = the ideal workload
mj = the minimal processing time of operation j
𝛼, 𝛽 = constants
This relationship is illustrated in Figure 14.C.1. Model (14.C.1) is just a simple rep￾resentation of the effects that we may expect to occur, based on the discussions in the
previous section. In particular, it is required that over a certain range of workload val￾ues, the effective processing times increase if the workload deviates from the ideal
workload. Note that an increase in effective processing time represents a decrease
in productivity. Model (14.C.1) is a simple model that provides these characteris￾tics with only three parameters, IW, 𝛼, and 𝛽. These parameters can be used to vary
the relationship between workload and effective processing time. The proportional￾ity constant 𝛼 reflects the sensitivity of the shop’s productivity for the workload in
the shop. One relevant indication for an adequate value of 𝛼 is given by Schmenner
(1988), who reports that his research statistics suggest that halving the throughput
time is worth an additional two to three percentage points to a plant’s productivity
gain. Consequently, doubling the throughput time leads to an increase in processing
times of about 3%, which would suggest an 𝛼-value of 0.03. Another indication for an
adequate value follows from the study by Lieberman and Demeester. They conclude
that, on average, each 10% reduction in inventory leads to about a 1% gain in labor
Effective
processing
time
Workload
Minimal
processing
time
Ideal workload β * Ideal workload
Figure 14.C.1 The shape of the relationship between workload and productivity.270 14 Small Series Production
productivity. Since throughput time is directly related to inventory this suggests a
𝛼-value of about 0.10.
In production systems where the effective processing times depend on the work￾load in the system, the utilization rate of the work centers is no longer a variable
that is uniquely defined by the arrival rate and processing times of the orders in
the order stream, and the capacity available per work center. The capacity required
also depends on the development of the workload over time. Thus, situations may
occur where at some point in time the workload gets so high that, given the effec￾tive processing times, the available capacity gets smaller than the capacity required
to process the order stream. Then the system becomes unstable. It is therefore use￾ful to know under which conditions an open system, that is a system where order
arrival times do not depend on the order completion rate, remains stable. Assum￾ing the model in (14.C.1), the conditions for stability can be established as follows.
For a given combination of order stream and resource structure, we define minimal
capacity utilization as the capacity utilization that would occur if the operations of all
orders only required minimal operation processing times. Let UMi denote the mini￾mal capacity utilization of work centeri for a given combination of order stream and
resource structure. Then, given the model in (14.C.1), a necessary condition for the
system to be stable is that for all i:
(1 + 𝛼(𝛽 − 1))UMi < 1
or
UMi <
1
(1 + 𝛼(𝛽 − 1)) (14.C.2)
If for any work center i, UMi > 1
(1+𝛼(𝛽−1)) , then depending on the development of
the workload, which very much depends on the variations in order arrival time, vari￾ations in routing length, and variations in processing times, the situation may occur
where for this work center the actual capacity required is larger than 100% of avail￾able capacity. This makes the workload grow without bounds. Under this condition,
the system may become unstable. Thus, if we want to guarantee the stability of the
production system without applying workload control, we must ensure that for all
work centers condition (14.C.2) is satisfied. This guarantees stability but still can
result in a situation where over a long period the work order throughput time is very
high since the actual utilization of the work center comes close to 100%. Therefore,
if we not only want to guarantee stability but also “acceptable” work order through￾put times, for instance, throughput times that on average are equal to 10 times the
average operation processing time, we should require that for all i
(1 + 𝛼(𝛽 − 1))UMi ≤ 0.9 (14.C.3)
since 0.9 × average operation processing time is equal to the average operation
throughput time.
In case 𝛼 =0.03 and 𝛽 =3 it follows from (14.C.3) that a necessary condition for
throughput being equal to ten times the average operation time is that UMi ≤0.849.References 271
References
Baker, K.R. (1975). Introduction to Sequencing and Scheduling. New York: J. Wiley.
Bechte, W. (1988). Theory and practice of load-oriented manufacturing control.
International Journal of Production Research 26 (3): 375–395.
Bertrand, J.W.M. (1983). The effect of workload dependent due-dates on job shop
performance. Management Science 29 (7): 799–816.
Bertrand, J.W.M. (1985). Multiproduct optimal batch sizes with in-process inventories
and multi-work centres. IIE Transactions 17 (2).
Bertrand, J.W.M. and Van Ooijen, H.P.G. (1991). Flow rate flexibility in complex
production departments. International Journal of Production Research 29 (4):
713–724.
Bertrand, J.W.M. and Sridharan, V. (1993). Subcontracting policies for low volume
component manufacturing. Research Report TUE/BDK/LBS/93-07. Eindhoven
University of Technology.
Bertrand, J.W.M. and Wortmann, J.C. (1981). Production Control and Information
Systems for Component Manufacturing Shops. Amsterdam: Elsevier.
Bertrand, J.W.M., Wortmann, J.C., and Wijngaard, J. (1998). Produktiebeheersing en
Material Management, 2e. Houten: Educatieve Partners Nederland BV (in Dutch).
Burbidge, J.L. (1971). Production flow analysis. The Production Engineer April/May:
139–182.
Conway, R.W., Maxwell, W.L., and Miller, L.W. (1967). Theory of Scheduling. Reading,
MA, USA: Addison-Wesley.
Cosmetatos, G.P. (1976). Some approximate equilibrium results for the multi-server
queue (M/G/r). Operations Research Quarterly 27 (3).
Dirne, C.W.G.M. (1994). Order flow times in case of single stage production batches on
minimum and maximum batch sizes. Research report TUE/BDK/LBS/93-4.
Durlinger, P.P.J. (1985). De Selectie van Logistieke parameters (I). Technische
Bedrijfsvoering November/December: (in Dutch).
Durlinger, P.P.J. (1986). De Selectie van Logistieke Parameters (II). Technische
Bedrijfsvoering January/February: (in Dutch).
Eilon, S. and Chowdhury, I.G. (1976). Due dates in job shop scheduling. International
Journal of Production Research 14: 223–237.
Fryer, J.S. (1973). Operating policies in multi-echelon dual-constraint job shops.
Management Science 19: 1001–1012.
Fryer, J.S. (1974). Labor flexibility in multi-echelon dual-constraint job shops.
Management Science 20: 1073–1080.
Hopp, W.J. and Spearman, M.L. (2008). Factory Physics: Foundations of Manufacturing
Management, 3e. Burr Ridge, IL: McGraw-Hill.
Jackson, J.R. (1957). Networks of waiting lines. Operations Research 5 (4): 518–521.
Kanet, J.J. and Hayya, J.C. (1982). Priority of dispatching with operation due dates in a
job shop. Journal of Operations Management 2 (3): 167–175.
Karmarkar, U.S. (1987). Lot sizes, lead times and in-process inventories. Management
Science 33 (3): 409–418.272 14 Small Series Production
Kingman, J.F.C. and Atiyah (1961). The single server queue in heavy traffic.
Mathematical Proceedings of the Cambridge Philosophical Society 57 (4): 902–904.
Kingsman, B.G., Tatsiopoulos, I.P., and Hendry, C. (1989). A structural methodology for
managing manufacturing lead times in make-to-order companies. European Journal
of Operational Research 40 (2): 196–209.
Little, J.D.C. (1961). A proof of the queueing formula: L = 𝜆W. Operations Research 9:
383–387.
Nelson, R.T. (1970). A simulation of labor efficiency and centralized assignment in a
production control model. Management Science 17: 97–106.
Onur, L. and Fabrycky, W.J. (1987). An input/output control system for the dynamic job
shop. IIE Transactions March: 88–97.
Schmenner, R.W. (1988). The merit of making things fast. Sloan Management Review
Fall: 11–17.
Shingo, S. (1985). A Revolution in Manufacturing: The SMED System. Productivity Press.
Solberg, J.J. (1981). Capacity planning with a stochastic workflow model. AIEE
Transactions 13 (2).
Tijms, H.C. (1986). Stochastic Modeling and Analysis – A Computational Approach.
Chichester: Wiley.
Van Ooijen, H.P.G. (1996). Load-Based Work-Order Release and its Effectiveness on
Delivery Performance Improvement. Ph.D. Thesis, Eindhoven University of
Technology.
Van Ooijen, H.P.G. and Bertrand, J. W. M. (2023). Economic Benefits of Working Time
Flexibility in Job Shop Type Manufacturing Systems. Available at SSRN: https://ssrn
.com/abstract=4472209 or https://doi.org/10.2139/ssrn.4472209.
Wayson, R.D. (1965). The Effects of Alternative Machines on Two Priority Dispatching
Disciplines in the General Job Shop. Master Thesis, Cornell University, New York.
Whitt, W. (1993). Approximating the GI/G/m queue. Production and Operations
Management 2 (2): 114–161.
Wickens, C.D. et al. (2013). Engineering Psychology and Human Performance, 4th ed.
Psychology Press.273
15
(Repetitive) Project-Based Production
In a project-based production situation, both the material control situation and the
capacity control situation are complex. In a pure one-of-a-kind project situation, the
product to be produced is unique, and no or only a few similarities can be observed
between projects. In a repetitive project based production situation (for instance, the
assembly department of large packaging machines, wafer steppers, complex medical
devices, etc.), there is a (small) repetition rate for the production of the products (so
not every product is quite unique) and there is less variety in the orders, although
parts of the product might be customer specific. There is still high uncertainty in the
medium term about orders and capacity, and there still are complex relationships
and constrained resources. In the remainder of this chapter, we will concentrate on
the repetitive project based situation. Because we have repeat orders, there is knowl￾edge of the (statistics) of the production characteristics, and therefore, it is possible
to develop (general) production control rules. We will start in Section 15.1 with the
discussion on production unit control for (Repetitive) Project-based production sit￾uations with a general description of these kinds of situations. Then, in Section 15.2,
the main issues for these situations will be addressed. Since the main construct in
these production situations is the network, we will discuss this also in Section 15.2.
In Section 15.3, we will apply the basic decisions from the decision framework to
this situation. The chapter will be completed with some examples in Section 15.4.
15.1 General Description
In Figure 15.1, some examples of products of project-basedproduction situations are
given.
In various production situations (very) complex, more or less (comparable)
customer-specific products are produced and delivered. For instance, packaging
machines, wafer steppers, industrial kettles, ships, etc. Since these products are
less generic than the ones discussed so far in this book and the number of units
that are produced is rather low, the production of these products has the charac￾ter of a project, and therefore, we will call these kinds of production situations
in the remainder of this chapter project shops. These shops are often found in
Production Control in Practice: A Situation-Dependent Decisions Approach, First Edition.
Henny Van Ooijen and Corné Dirne.
© 2024 WILEY-VCH GmbH. Published 2024 by WILEY-VCH GmbH.274 15 (Repetitive) Project-Based Production
Figure 15.1 Some examples of products from project-based production situations.
Source: https://depositphotos.com/.
engineer-to-order production. The project shop is the department where the whole
system is assembled, configured, and tested. Routings are often complex because the
products don’t move, but the different resources (mechanical engineers, electrical
engineers, etc.) move. The complexity is caused by the fact that at different points
in time, different (highly skilled) resources might be required. Since they often also
work on other jobs, it can be quite a complex puzzle to obtain a good throughput
time and delivery reliability without having too many resources. Sales are very
often (very) sensitive to macroeconomic fluctuations and thus can vary (a lot)
from year to year. Therefore, it is impossible to make a detailed (reliable) forecast.
Each customer order can be considered as a project with a network of partly
overlapping activities like engineering and design, component manufacturing,
assembly, and installation (see Figure 15.2). The manufacturing organization often
has a functional characteristic with universal production resources.
A project shop is characterized by:
(a) More or less (comparable) unique products.
(b) Large variety in orders: small, medium, and large.
(c) High uncertainty in the medium term about orders and capacity.
(d) High rate of change in customer requirements.
(e) Order planning is based on rough estimates (milestones).
(f) Complex relationships between activities.
(g) Constrained resources.
The (repetitive) project shop is often preceded by (a) production unit(s) where
components are produced and/or pre-assembled (see Figure 15.2). Such a produc￾tion unit often has the characteristics of a job shop, and a customer order often
consists of several work orders for components that are required at the assembly
department. These work orders are called sub-orders, and in Chapter 14, we have
seen how to control the flow of these orders in the component manufacturing
department.
Engineering
and design
Process
planning
Component
manufacturing
Project
manufacturing;
assembly
Installation
at customer
Figure 15.2 Global goods flow concerning a project.15.2 Main Control Attention Points of Project-Based Production 275
15.2 Main Control Attention Points of Project-Based
Production
Most of the time several projects are being carried out at the same time, with
each project being in a different phase. Resources are shared across projects in
interactions that involve multiple parallel and sequential project activities. Often,
the scheduling of activities has to be changed in response to the changing prior￾ities of individual customers (Rahim and Baksh 2003). Altogether, complicated,
unpredictable, and changing activities result in project shops being characterized
by dynamic complexity (McCarthy 1995; Sterman 2000). Control of the schedule
and resources is challenging, and manufacturing operations require flexibility.
Therefore, the relevant decisions from a Production Unit Control perspective are:
– Allocating constrained resources.
– Multi-project scheduling (prioritizing projects).
For good coordination of the activities and/or resources, such that costly resources
(for instance, special cranes, highly skilled operators, etc.) are used efficiently, we
need to have reliable estimates of when the different activities (tasks) have to be
performed. For many project shops, it is, therefore, of utmost importance to con￾struct a project network: a network of activity and event relationships that shows
the sequential relations between the tasks in a project. This can be seen as the work
order scheduling function. The less repetitive a project shop is, the more important
such a network is since projects lack the continuity of day-to-day operations and
have more complex problems of coordination. The benefits of such a network are:
– It is a consistent framework for planning, scheduling, and monitoring the project.
– It illustrates the interdependencies and precedence relationships of all tasks.
Some well-known techniques for constructing such a network (or schedule) and
determining (activity) due dates and slacks are the program evaluation and review
technique (PERT) and the critical path method (CPM). Such a network can be used
for the former mentioned relevant production unit control decisions. Constructing
a PERT or CPM network allocates resource time among project tasks. Having the
network, we need to allocate the physical resources. This is strongly related to the
network that has been constructed because different networks can have different
needs for physical resources and especially different timings of resource needs. Let us
start with a single project. Often, there is a fixed level of the various resources and the
need for some resource(s) may vary during the project. If we can construct a network
that smoothens the use of the resource(s), it may be possible to avoid a project delay
while at the same time not allocating extra resource(s) “just to be sure” (which avoids
costs). In many situations, some activities can be performed in different modes: a
normal (standard) mode, using standard resources, or in so-called crash modes, that
is, using more resources or completely different resources, which reduces the activity
time(s). The (basis) network is often constructed assuming that the activities are
performed in a normal mode. Constructing different networks using different modes
leads to different project completion times, different costs, and different patterns of
needs of resources.276 15 (Repetitive) Project-Based Production
If we consider the physical resources, then an important control aspect is to make
sure that the required resources are available when needed (resource allocation)
given certain levels of resources and that some activities need a fixed amount of time
(can’t be crashed). So, resource or capacity allocation is an important topic in these
production environments. For this, in general, the functions of resource loading and
resource leveling are used.
In many situations (several), resources might be constrained, i.e. they cannot be
increased in the short term. We then need to use some form of constrained resource
scheduling to minimize resource usage while still achieving various due dates (or
minimizing completion times while operating with limited resources).
In case there are multiple projects allocating resources and scheduling is essentially
the same as in the case of a single project. The multi-project problem, in general,
deals with determining how to allocate a new project that is added to a set of already
accepted projects and how to set a completion time for this newly added project.
Therefore, we need an efficient dynamic multi-project scheduling system.
The just discussed production control decisions for project manufacturing pro￾duction units hold especially for pure project manufacturing production units. If the
production unit operates in a repetitive project manufacturing environment, some
aspects only have to be considered once (we can say in the design phase) and then
the control questions only concern the sequence in which the orders at the different
work centers (or in the different production phases) have to be processed and the
availability of the materials.
Concerning the supply of materials we have, besides the earlier mentioned mate￾rial supply aspects, the following. Some project manufacturing situations use special
materials, for instance, rather expensive materials (e.g. lenses for wafer steppers), or
are rather oxidation sensitive, etc. It is then important that these materials are sup￾plied as late as possible. This means that they have to be supplied at the work center,
where they are processed at a time when the previous operation is finished with
high reliability. For these situations, we need to determine work order release times
for the work centers with such materials, where the work order release time then is
the moment at which the previous operation is finished with great certainty and the
special materials have to be supplied.
15.2.1 Construction of a Network
For the sake of completeness, we will give a short description of how to construct a
network (see also, for instance, Meredith and Mantel 2000). The most well-known
techniques for project scheduling are PERT and CPM. These are more or less
comparable.
15.2.1.1 Terminology
Activity: A specific task or set of tasks that use capacity and that need time.
Event: The result of completing one or more tasks. Events don’t use capacity.
Network: The combination of all activities and events, considering precedence
restrictions; they are often drawn from the left to the right, and the arrowheads15.2 Main Control Attention Points of Project-Based Production 277
represent the flow (correct precedence relations). Before an event is realized, all
directly preceding activities have to be performed; these activities are called the
predecessors of the event.
Path: A set of connected activities between two events in the network.
Critical path: Activities, events, or paths that, when delayed, induce a delay in the
finishing of the project. The critical path of a project is the sequence of critical
activities that connect the start event and the end event of the project.
Constructing a network starts with making the so-called action plan. An action
plan is a list of activities that have to be performed, the expected time and costs it
takes to execute the activity, the predecessors of each activity, and the non-routinely
resources that are required.
The precedence relations (which predecessors an activity has) depend on the tech￾nical relations. For example, when baking a cake, one first must prepare the dough.
An activity can have:
– A successor but no predecessor (start of the network).
– A successor and a predecessor.
– A predecessor and no successor (end of the network).
Activities are connected by arrows that indicate the precedence relations.
Table 15.1 gives an example of an action plan. The corresponding network is given
in Figure 15.3. We will use this example action plan in this chapter to illustrate some
calculations that are relevant concerning production control aspects in project-based
production environments. The network in Figure 15.3 is a so-called AON (activity on
node) network. The nodes represent the activities, and the arrows show the prece￾dence relations. In an AON network, it is quite common to use a node “start” if
there is more than one activity without a predecessor and a node “end” if there is
more than one activity without a successor. Another way to represent this network
is to use an AOA (activity on arrow) network. In such a network, the arrows repre￾sent the activities, and the nodes are events. The AOA network for the action plan in
Table 15.2 is given in Figure 15.4. For constructing AOA networks, often so-called
Table 15.1 Example action plan.
Activity Predecessor Time Costs Resources
A — 4 500
B — 3 200
C A, B 2 250
D A, B 3 500
E B 4 750
F C 4 250
G D, E 2 300
H F, G 3 400
I A 5 200278 15 (Repetitive) Project-Based Production
Start
A
B
E
D
C F
G H
I
End
Figure 15.3 The AON network corresponding to the action plan given in Table 15.1.
G
C F H
E
I
B
A
2
4 5 7 8 1
3 6
D
Figure 15.4 The AOA network corresponding to the action plan given in Table 15.1.
1 2
3 4
5
A
C
B E
D G
F
Figure 15.5 The AOA network in case of
activity A precedes activity D, activities
A and B precede activity E, and activities
B and C precede activity F.
dummy activities are required to represent a certain precedence relation. Dummy
activities are activities that don’t require time and resources; they are only used to
represent technical relations. In Figure 15.4, for instance, we see two dummy activ￾ities (the dashed arrows): activities C and D can only be performed if activities A
and B are finished. A more complex example of the use of dummy activities is given
in Figure 15.5. An activity can only start if the event that immediately precedes this
activity has occurred. An event occurs if all activities immediately preceding this
event have been finished. So, event 4 occurs if activity B and activity C are finished,
and only then can activity E start. Event 1 occurs if activities A and B are finished,
and activity D can start.
The choice between the AON representation and the AOA representation is often
based on personal preference. AOA networks take more time to construct, but they
clearly indicate the events (milestones). Above that, if we use AOA representation,15.2 Main Control Attention Points of Project-Based Production 279
the length of the arrow can be used to represent the (expected) time of the activity.
Now the network is constructed, the next step is to determine the throughput time
of the project and the critical path. For the throughput time, we need to know the
expected throughput times of each of the activities. The action plan in Table 15.1
assumes that the durations of the activities are known. In reality, these are most of
the time stochastic variables and we need to know the distribution of the processing
times of each of the activities.
15.2.1.2 Duration of the Activities
Since a project is a nonrecurring activity it is impossible to use data from the past
to find the distribution of the processing times of the activities. If projects are more
or less similar, then this can be used; otherwise, one has to use the expertise of the
(project) managers. Often, based on their experience and past projects, they can give
optimistic, pessimistic, and most likely estimates of the processing times:
– Optimistic estimate: a value a such that the actual time needed to execute the
activity in (about) 99% of the cases is greater than a.
– Pessimistic estimate: a value b such that the actual time needed to execute the
activity in (about) 99% of the cases is less than b.
– Most likely estimate: a value m such that the actual time to execute the activity in
most cases is m.
Assuming that the distributions follow a Beta distribution (more flexible than the
Normal distribution and can handle extreme values like a = m or b = m; see also
Littlefield and Randolp 1987; Gallagher 1987), the expected (activity processing)
time, E(AT), and the standard deviation of the (activity processing) time, 𝜎AT, can
be calculated as follows:
E(AT) = a + 4m + b
6
𝜎AT =
√(b − a
6
)2
= b − a
6 (15.1)
Suppose that for our example, given in Table 15.1, we have values for a, b, and
m, as given in Table 15.2. For the expectation and standard deviation of the activity
processing times, we then get the values as given in Table 15.3.
15.2.1.3 Critical Path and Project Duration in Case Activity Times Are
Deterministic
Now we know the expectations and standard deviations of the activity processing
times, we can determine the time at which the project will be finished. We will illus￾trate this using the AOA network in Figure 15.6 and the data given in Table 15.3.
Let us start with the assumption that the standard deviations are all equal to zero
and that the expectations are certain. If we start at time zero, we can simultane￾ously start with activities A and B. Event 2 then occurs at time 4 and event 3 occurs
at time 3. Event 4 occurs when both event 2 and event 3 have occurred, which is at
time 4 [maximum (3, 4)] since the dummy activities take no time. Event 5 occurs at280 15 (Repetitive) Project-Based Production
Table 15.2 Optimistic, most likely, and pessimistic estimates for the duration
of the activities for the example given in Table 15.1.
Activity Predecessor
Optimistic
time a
Most likely
time m
Pessimistic
time b
A — 2 3 10
B— 1 3 5
C A, B 1 2 3
D A, B 1 2.5 7
EB 3 4 5
F C 2 3.5 8
G D, E 1 1.5 5
H F, G 2 2.5 6
I A 3 4.5 9
Table 15.3 The expectation and standard deviation of the activity processing
times for the example in Table 15.1, using the estimates in Table 15.2.
Activity Predecessor
Expected
processing time
Standard deviation
processing time
A — 4 1.33
B — 3 0.67
C A, B 2 0.33
D A, B 3 1.00
E B 4 0.33
F C 4 1.00
G D, E 2 0.67
H F, G 3 0.67
I A 5 1.00
time 6 (occurrence of event 4)+2 (duration of activity C), and event 6 occurs at time
7 [= maximum (occurrence event 4+processing time activity D, occurrence event
3+processing time activity E)]. Event 7 occurs at time 10 [= maximum (occurrence
event 5+processing time activity F, occurrence event 6+processing time activity
G)], and finally event 8, the end of the project, occurs at time 13 [= maximum (occur￾rence event 2+processing time activity I, occurrence event 7+processing time activ￾ity H)]. The project thus is finished at time 13, the occurrence of event 8. The just
calculated times are called the Earliest Occurrence Times (EOTs) or Earliest Finish
Times (EFTs). Looking at Figure 15.6 we can distinguish six paths leading to event 8:
A-I: 4+5 = 9 time units
A-C-F-H: 4+2+4+3 = 13 time units15.2 Main Control Attention Points of Project-Based Production 281
B D
EOT = 4
C F
EOT = 4
4 H
EOT = 7 G
I
A
EOT = 0 EOT = 10 EOT = 13
2
EOT = 6
5 7 8 1
EOT = 3
3 E
6
Figure 15.6 The AOA network corresponding to the action plan given in Table 15.1.
A-D-G-H: 4+3+2+3 = 12 time units
B-C-F-H: 3+2+4+3 = 12 time units
B-D-G-H: 3+3+2+3 = 11 time units
B-E-G-H: 3+4+2+3 = 12 time units
The longest path is A-C-F-H, which takes 13-time units, so 13-time units is the
shortest possible time within which the project can be finished. This is called the
critical time of the network (project), and the corresponding path is called the critical
path.
15.2.1.4 Slack
Thus far, we only looked at the events and determined the EOTs for the milestones of
the project. Let us now consider the activities. We will determine the Earliest Starting
Times (ESTs) and the Latest Starting Times (LSTs) for each of the activities. The EST
for an activity is equal to the EOT of the event that immediately precedes this activity.
The activity I cannot start before event 2 has occurred, so the EOT of activity I is 4.
An important question for the project manager is: what is the latest time at which
time activity I has to start such that it doesn’t lead to a delay in the project? We know
that the length of the critical path is 13 time units. Activity I takes 5 units of time,
so the latest moment at which this has to start is 13−5 = 8, so the LST of I equals
8. Since activity I can’t start before event 2 has occurred, the Latest Occurrence Time
(LOT, also called Latest Finishing Time, LFT) for event 2 also is 8. However, if we
look at path A-C-F-H, it appears that activity C has to start at 4, so the LOT of event
2 must be 2 [minimum (2, 4)]. For activity I, which has to start at 8, but already can
start at 4, the slack is 8−4 = 4. It will be clear that all activities on the critical path
have a slack equal to 0; these can’t be delayed without delaying the project.
To determine the slack for each activity, we have to calculate the EST of the activity,
for which we start at the beginning of the network and go along forward until the
end of the network. Then, we have to calculate the LST, for which we start at the end
of the network and go back to the beginning of the network. In calculating the slack
of an activity, we assume that none of the preceding activities on the path to the282 15 (Repetitive) Project-Based Production
Table 15.4 The ESTs, LSTs, and slacks for the activities, and the EOTs, LOTs, and slacks for
the events, for the example network given in Figure 15.3.
Activity EST LST Slack Event EOT LOT Slack
A 000 1 000
B 000 2 440
C 440 3 341
D 451 4 440
E 341 5 660
F 660 6 781
G 7 8 0 7 10 10 0
H 10 10 0 8 13 13 0
I 484
activity is delayed. So, in calculating the slack for activity I, we assume that event 2
occurs at 4.
Let us look at activity G. Activity G can start after the occurrence of event 6 and
precedes activity H. To find the LST for activity G, we subtract the time for activity
H and the time for activity G from the critical time, which leads to a LOT for activity
G of 8. The EST for activity G is equal to the EOT of event 6 and thus equal to 7.
Therefore, the slack for activity G is 8−7 = 1. To find the LOT for an event, we have
to determine the LSTs of all the paths that start at this event. In Table 15.4, we can
find for all activities the corresponding ESTs, LSTs, and slacks, and for all events the
corresponding EOTs, LOTs, and slacks.
15.2.1.5 Uncertainty in Project Duration Due to Stochastic Activity Times
In the previous section, we determined the project duration in case the activity times
are deterministic, in which situation there is no uncertainty in the project duration,
and the finishing time of the project can be determined with 100% reliability. How￾ever, in most cases, the activity times are stochastic and have a certain variance (in
the network, the activity is then often followed by two numbers: the average dura￾tion and the standard deviation of the duration; for instance, instead of A we then
see A(4, 1.33)). Now, the finishing time of the project can’t be determined with 100%
reliability, and we can only determine a certain finishing time with a certain prob￾ability, or given a desired finishing time, we can determine the probability that this
finishing time will occur.
Let us assume that in our example, we have a standard deviation of the activity
times as given in column 4 of Table 15.3. Further, let us assume that the activities
are (statistically) independent of each other; in that case, the variance in the time
of a set of activities is equal to the sum of the variances of each activity within the
set. Clearly, the important variances are the variances in the times of the activities
on the critical path. In our example, activities A, C, F, and H are on the critical path,15.2 Main Control Attention Points of Project-Based Production 283
and thus the variance of the time of the set of activities on the critical path is equal to
1.332 +0.332 +12 +0.672 =3.14+0.11+1+0.45 = 4.70. Suppose the project man￾ager has agreed upon a finishing date of 15. What is the probability that the project
will be finished within 15 days? Therefore, we will use the variable Z, defined as:
Z = (D − 𝜇)∕√
𝜎2 (15.2)
with
D: the desired finishing time
𝜇
√
:the average throughput time of the project (critical time of the project)
𝜎2: the variance of the throughput time of the critical path
Z: the standard normal deviate (number of standard deviations of a Normal
distribution)
In our example, we have:
Z = (15 − 13)∕√4.7 = 0.92
From the tables of the standard Normal distribution, it follows that with Z =0.92,
the probability that the project will be finished within 15 days is 0.8212 (assuming
that the throughput time follows a Normal distribution; which often is not a bad
assumption).
If we want to have a finishing time that has a reliability of 95%, Z has to be equal
to 1.645. Using Eq. (5.2), it then follows that:
D = 13 + 1.645 × 2.17 = 16.57
Thus, if we have a due date of the project equal to 16.57, the project will be finished
within this due date with 95% reliability.
Now, the question is whether it suffices to consider only the variance (“noise”) on
the critical path. Let us consider path A-D-G-H, which has a time that is close to the
critical time. The variance on this path is 4.44, and if we use a due date of 15, it follows
that Z =(15−12)/2.11=1.42. The probability that the project will be delayed due to a
delay on path A-D-G-H is thus less than due to a delay on the critical path. However,
this doesn’t mean that the probability that the project will be finished within the due
date of 15 is 0.8212 (based upon the critical path only). If two paths are independent,
the probability that both paths are finished in time is the product of the individual
probabilities. So due to the (almost critical) path A-D-G-H, the probability that the
project will be finished within the due date of 15 is equal to 0.8212 × 0.922 = 0.7488.
Since all paths, except path A-I, have a time that is close to the critical time, we have
to calculate the probabilities that these paths will be finished within the due date of
15, and then the probability that the project will be finished within the due date of 15
can be found by multiplying all these probabilities. Path A-I has a variance (2.77) that
compared to the slack on this path given a due date of 15 (15−9), is small enough
to ensure that the probability that this path will be finished within the due date of
15 is almost 1. Therefore, this path doesn’t influence the probability that the project
will be finished in time. Only when the standard deviation on this path would have
been pretty high (for instance 6), also this path would have been important for the
probability that the project will be finished within the due date of 15.284 15 (Repetitive) Project-Based Production
To determine the probability that the project will be finished within a given due
date, it is therefore important not only to look at the critical path but also at the
paths that have a time that is close to the critical time and/or to the paths that have
a (relative) high standard deviation.
15.2.1.6 Realistic Estimates of the Activity Times
Thus far, we assumed that the activity times can be based on optimistic, pessimistic,
and almost certain estimates, given a reliability of 99% of these times. In general,
it can be quite troublesome to get these pretty accurate estimates. Fortunately, it
appears that we can use less accurate estimates for these times unless the underly￾ing distribution is pretty asymmetric. The estimate of ET hardly changes if we use
95% (once in twenty cases, the actual time is smaller than or greater than the opti￾mistic resp. pessimistic times) or 90% (once in ten cases, the actual time is smaller
than or greater than the optimistic resp. pessimistic times); however, the formula
for calculating the standard deviation does change. The calculation of the standard
deviation (15.1) is based upon the assumption that the standard deviation of a Beta
distribution is about 1/6 of the range of the distribution. Stated otherwise, the opti￾mistic and pessimistic estimates are assumed to be equal to the −3𝜎 respectively +3𝜎
values (about the 99% values). Now, suppose we use 95% estimates a′ and b′
. We then,
in fact, shrink the range, and this range is thus no longer equal to about 6𝜎. If we look
at the standard Normal distribution, we see that 95% of the distribution lies between
a′ and ∞ if Z equals −1.645 (although we assume a Beta distribution it appears that
using the Normal distribution works well in practice). Using a Z value of 1.645 gives
that 95% of the distribution lies between −∞ and b′
, so the range between a′ and b′
represents about 2 × 1.65𝜎 =3.3𝜎 instead of 6𝜎 as used with the 99% accurate esti￾mates a and b. So, if we use the 95% accurate estimates a′ and b′
, we get as an estimate
for the variance of the activity time:
𝜎2 =
(b′ − a′
3.3 )2
(15.3)
If we use 90% accurate estimates, we get:
𝜎2 =
(b′ − a′
2.6 )2
(15.4)
From research, it follows that the errors that are made when using these estimates
for the variance of a Beta distribution are good estimates of the real variance and
that the error is less than 5%. Since 95% or 90% accurate estimates of the optimistic
and pessimistic times are much easier to give than 99% accurate estimates, it is bet￾ter to use these and to calculate the corresponding estimates for the variance instead
of using the estimate given in Eq. (15.1). The latter leads to a pretty large underesti￾mation of the variances when the estimates are used, which results in a significant
error in the probability that a project will be finished at a certain time.
15.2.1.7 Activity on Node Networks
As already said, another way to draw a network is to use the AON method. In an AON
network, the nodes represent the activities, and the arrows show the precedence
relations. In an AON network, it is quite common to use a node “start” if there is15.2 Main Control Attention Points of Project-Based Production 285
more than one activity without a predecessor and a node “end” if there is more than
one activity without a successor. Let us use our example project in Table 15.1 to
demonstrate the construction of an AON network.
We begin with the node Start and connect this node with the nodes that represent
the two activities that don’t have a predecessor: A and B. Activity C can start if activ￾ities A and B have been executed, so we have to connect both the nodes for activity A
as well as the node for activity B with the node representing activity C; this also holds
for activity D. Activity E can start as soon as activity B is executed and therefore only
the node for activity B is connected to the node representing activity E. Activity I can
start as soon as activity A is executed and therefore only the node for activity A is
connected to the node representing activity I. Activity F can start as soon as activity
C is executed, and therefore, only the node for activity C is connected to the node
representing activity F. Activity G can start as soon as activity D and activity E have
been executed, so we have to connect both the node for activity D as well as the node
for activity E with the node representing activity G. Activity H can start as soon as
activity F and activity G have been executed so we have to connect both the node
for activity F as well as the node for activity G with the node representing activity
H. Activities I and H don’t have a successor, so we now connect the nodes for these
activities with the node End. This is illustrated in Figure 15.7.
In the same way, as for the AOA network, we can now determine the critical time,
the critical path, and the slack. In the corners of the rectangle that corresponds to an
activity, we often denote the earliest start time, EST, (left upper corner), the earliest
finishing time, EFT, (right upper corner), the latest starting time, LST, (left lower
corner) and the latest finishing time, LFT, (right lower corner). The EST and EFT can
be found by starting in Start and working forward towards End, and the LST and LFT
can be found by starting in End and working backward towards Start. For instance,
if we look at activity G, then we see that the earliest time that this activity can be
finished is 9, and the latest time that it has to be finished (to not delay the project) is
10. Since the activity itself takes 2 time units, the latest time that this activity should
be started is 10−2 = 8. The slack is equal to LST−EST (or LFT−EFT) = 8−7 = 1.
Since activity G utmost has to start at 8, the LFTs of the preceding activities,
D and G, are 8.
Start 0 End 13
0 4 
A
0 4
0 3 
B
1 4
4 7 
D
5 8
4 6 
C
4 6
4 9 
I
8 13
7 9 
G
8 10
10 13 
H
10 13
6 10 
F
6 10
3 4 
E
5 8
Figure 15.7 The AON network for the example project is given in Table 15.1.286 15 (Repetitive) Project-Based Production
15.3 Production Control Decisions for Project-Based
Production
In the project-shop we often have to deal with a kind of uniqueness of the func￾tional specifications of the product. Therefore, there is often a (quite) high level
of uncertainty concerning the capacity needed. It is not unusual that during the
(sub-)assembly phase, one or more revisions of the technical specifications occur
and/or that a large difference exists between the capacity requirements of the differ￾ent jobs already released. Production control therefore mainly concerns the control
of the slack and the allocation of (scarce) specific resources.
15.3.1 Sequencing
As already said, due to the complexity of the resource requirements, it is important
to have a good network with corresponding activity due dates. Because often some
scarce resources are needed it is important to get a “doable” schedule. Therefore, the
sequence in which activities are performed by a certain resource is already given by
the activity schedule that follows from the capacity allocation function.
15.3.2 Capacity Allocation (and Scheduling)
In these kinds of production units, capacity allocation, and scheduling are often inte￾grated because there often are several scarce resources, and the delivery dates of all
projects should be achieved as well as possible. Since most activities, in general, have
some slack, we can use this slack to determine start dates (which often involves the
sequence) such that the scarce resources are used as efficient as possible. The fol￾lowing discussion holds for single projects as well as for multiple projects.
15.3.2.1 Resource Loading
Capacity allocation in project-wise production units starts with resource loading. It
describes for each resource, given a certain schedule, the required amounts during
specific periods. It is irrelevant whether we consider a single or multiple projects;
it gives a general overview of the demands of a (set of) project(s) for the different
resources of the production unit. Given an action plan, resource loading is straight￾forward (see Figure 15.8). Excess demands on a firm’s resources can be identified
and reduced.
15.3.2.2 Resource Leveling
The next step is resource leveling, which aims at minimizing the period-by-period
variations in resource load by shifting tasks within their given slack allowances
(which follow from the network earlier developed). Smoothing the usage of the
resources has several advantages:
– Less hands-on management is required.
– A just-in-time inventory policy is much easier.15.3 Production Control Decisions for Project-Based Production 287
Assume: ES
One task/mechanic
How many mechanics?
b b b b b b b b d d d d d d d d d d d d d j j j j j j j j j j j j
c c c c c
a a a a a a f f f f f f f f f f f f f f f h h h h h h h h h
e g g g g g g g g g g g g g g g g g i i i i i i
e e e e e e e e
Workload diagram:
Time
so: 4!
5 10 15 20 25 30
Latest start and latest finish
Figure 15.8 Example of translation from project plan to detailed capacity plan.
– It improves morale for resources that mainly consist of personnel.
– The associated costs also tend to be leveled.
– Leveling of employment throughout a project: costs of hiring and firing are quite
significant for most companies.
Resource leveling can be used for almost all projects whether or not resources are
constrained and can be done manually if the network is not too large and there are
only a few resources. For large networks and multiple resources, several computer
programs can efficiently handle most leveling problems. Resource leveling mini￾mizes period-by-period variations in resource loading by shifting tasks within their
slack allowances.
Remark: When leveling resources, the assumption is that no change in activity
durations and/or project duration is allowed.
15.3.2.3 (Constrained Resource) Scheduling
There are two fundamental approaches to the most general problem of minimizing
resource usage while still achieving different due dates: optimization models and
heuristics. Optimization models give the best solution but are often limited in han￾dling complex situations and large problems. Heuristic approaches employ rules of
thumb based on experiences in the past with similar problems.
There are two kinds of optimization models: mathematical programming (mainly
linear programming) and enumeration. The development of these models is contin￾uous and the size of the networks for which an optimal solution can be found ever
increases. One problem is that the characteristics of problems that can be addressed
are still a question: some methods work well on one problem and not on a similar
problem.288 15 (Repetitive) Project-Based Production
The most commonly used models used for constrained resource scheduling
are heuristics: they are feasible for attacking large, nonlinear, complex problems
and although they do not generate optimal solutions, they often give quite good
solutions. Most heuristic methods start with the network schedule and analyze the
resource usage period by period and resource by resource. If, in a certain period,
more of a certain resource is needed than available, the heuristic examines the
tasks in that period for that resource. It allocates the (scarce) resource to them
sequentially, according to some priority rule. The main difference among the
heuristics is in the priority rule they use. Most of the priority rules are simple
adaptations/variations of traditional job-shop scheduling priority rules. Some of the
most used priority rules are:
– As soon as possible: all activities are scheduled as early as possible.
– As late as possible: all activities are scheduled as late as possible without delaying
the project.
– Shortest task first: tasks are ordered according to their duration, with the shortest
task first.
– Most resources first: activities are ordered using the number of resources, with the
largest user first.
– Minimum slack first: tasks are ordered by the amount of slack, starting with the
least slack.
– Most critical followers: tasks are ordered by the number of critical activities
following them, starting with the task with the greatest number of critical
followers
– Most successors: the same as the previous rule, however now, considering all fol￾lowers and not only the critical ones.
– Arbitrary: priorities are assigned not based on task length, slack, or resource
requirements; an example might be assigning priority based on project value.
If one or more or more resources are exhausted, activities requiring these resources
must be slowed or delayed until the next period when the resource(s) can be
allocated.
The just-described heuristic(s) work from the beginning to the end. Other heuris￾tics work from the end to the beginning. Activities that just precede the project finish
are scheduled to be completed as close to their latest finish time as possible. Then,
the next-to-last tasks are considered and so on. The purpose of such an approach is
to leave as much flexibility as possible that will be difficult to schedule in the middle
and early parts of the project (based on the idea that flexibility early in the project
gives the best chance of completing early and middle activities on time, thereby
improving the chance of being on time at the ending activities). Other heuristics
are based on the branch-and-bound approach. Using a tree search, they generate a
wide variety of solutions, discarding those that are not feasible and others that are
feasible but poor solutions.15.3 Production Control Decisions for Project-Based Production 289
15.3.3 Work Order Release/Project Scheduling
15.3.3.1 Work Order Scheduling
The basics for scheduling a project are already discussed in Section 15.3. Here, we
discuss the scheduling of multiple projects. In multi-project manufacturing environ￾ments, projects arrive dynamically in time. This leads to the fact that at any moment
in time, we have several projects released to the shop floor, with each project at
its stage. The most common approach concerning control is to treat the different
projects as if they are elements of a single large project. Another approach is to con￾sider all projects as completely independent. For either approach, the conceptual
basis for scheduling and allocating resources is essentially the same; however, the
two approaches lead to different scheduling and allocation outcomes (see, e.g. Kur￾tules and Davis 1982). For dynamic multi-project scheduling system, standards are
needed by which to measure the effectiveness of the schedule. Three important mea￾sures are:
a. Schedule slippage: the time passed a project’s due date/delivery date when the
project is finished.
b. Resource utilization: smoothens out the peaks and valleys of resource usage as
much as possible.
c. In-process inventory: the amount of work waiting to be processed because there
is a shortage of some resource(s).
All these criteria can’t be optimized at the same time, and thus a trade-off is
involved. It has to be decided which criterion is the most important in a certain
situation, and then this criterion should be used to evaluate different schedules. A
study by Fendley (1968) showed that the minimum slack first rule is the best overall
priority rule and generally results in minimum project slippage, minimum resource
idle time, and minimum system occupancy time (and this minimum in-process
inventory) for the cases he studied.
Also, for work order scheduling, there are two fundamental approaches: optimiza￾tion models and heuristics. For optimization often mathematical programming is
used. Most models are based on integer programming that formulates the problem
using 0–1 variables to indicate whether or not an activity is scheduled in specific
periods. The three most common objectives are:
– Minimum total throughput time for all projects.
– Minimum total completion time for all projects.
– Minimum total lateness or lateness penalty for all projects.
Constraints often are (a combination of):
– Availability of resources.
– Precedence relationships among activities.
– Activity splitting possibilities.290 15 (Repetitive) Project-Based Production
– Project and activity due dates.
– Substitution of resources to assign to specified activities.
– Concurrent and non-concurrent activity performance requirements.
Although mathematical programming can generate optimal solutions, it has some
serious drawbacks since this approach has proved to be (extremely) difficult and
computationally expensive, except for some small problems.
Due to the difficulties with the analytical formulation and the solution for real
problems, the major efforts concerning the resource-constrained multi-project
scheduling problem have focused on heuristics. Additional (to the earlier mentioned
rules) heuristics for resource allocation have been developed that draw directly on
PERT/CPM:
– Resources scheduling method: give priority (made on a pairwise comparison) to
that activity with the minimum value of dij with dij =increase in project duration
when activity j follows activity i = max(0, EFTi −LSTj
).
Where EFT =Earliest Finish Time
LST = Latest Starting Time
– Minimum late finish time: give priority based on activity finish times in the net￾work that has been designed; the earliest late finishes are scheduled first.
– Greatest resource demand: give higher priority to activities with greater demands
on resources.
– Greatest resource utilization: give priority to that combination of activities that
results in maximum resource utilization during each scheduling period.
– Most possible jobs: give priority to that set of activities that results in the greatest
number of activities being scheduled in any period.
15.3.3.2 Work Order Release
Most of the just mentioned activities are not relevant for project manufacturing pro￾duction units in case there is at each time a single project. However, in many repet￾itive project manufacturing environments, more or less, comparable projects arrive
dynamically in time, and then the same principles as for pure multi-project envi￾ronments apply. These environments are often assembly processes where, at differ￾ent stages in the assembly process, at different moments in time, different (special￾ized) operators are required to perform the operations (mechanical, electrical, etc.).
The network is rather simple, and the main concern is the timely availability of the
required resources. Since often specialized operators are needed at some stages, and
these can be expensive, we need to have reliable estimates of the expected starting
times of the activities: if the operators are too early, they will be idle and thus lead
to extra costs, if the operators are too late, it will be harder to achieve the promised
lead time. The same, more or less, can hold for some special materials (expensive,
oxidation sensitive, etc.). To determine good work order release times for each stage,
we need to determine the stage (work center) lead times. For this, we have to make a
trade-off between the sum of the work center (stage) inventory costs (if orders arrive
too early, they have to wait) and the penalty costs in case an order is delivered too
late to the customer (see also Hendrikx 1996).15.4 Application 291
Remark: Suppose we have N work center stages, that the work center inventory
costs for work center i are hi, and that the penalty costs are p per unit of time
late. Let L⃗ denote the vector of work center (stage) lead times Li
, and Gi
(L⃗) the
difference between the planned completion date and the actual completion date
given L⃗. Assuming that only inventory costs have to be paid when an order is
finished early (and not for the time it is in the process) then we have to find L⃗
such that
∑
N
i=1
hi
E[Gi
(L⃗)
+] + pE[GN(L⃗)
−]
is minimized. In case inventory costs also have to be paid for the time the order is
in process at a work center (stage), we get a similar equation. Given the work cen￾ter (stage) throughput time distributions, there are several ways to solve these
kinds of problems (e.g. Gong et al. 1994; Elhafsi 2002).
15.4 Application
PAMA has to execute two projects. Both projects start at time 0. For project 1, the
project action plan is given in Table 15.5, in which, for each activity, the predeces￾sor(s), the duration (in weeks), and the number of required employees are given. For
project 2, the project action plan is given in Table 15.6. Using an AON representa￾tion, PAMA determines the critical path, the project duration, the early start Gantt
chart, and the slacks per activity for Project 1 (see Figures 15.9 and 15.10).
PAMA does this also for Project 2 (see Figures 15.11 and 15.12). Now, PAMA
can determine the capacity usage for both projects. Using an early start scheduling
heuristic leads to Figure 15.13. Each project gets a firm deadline. For project i, i =1,
2, the deadline is di weeks, and both projects have to be executed at the same time
with the (fixed) W employees (the permanent staff). W is an integer number. If
Table 15.5 Project action plan for project 1.
Activity
Immediate
predecessor
activities
Duration
(wk)
Required
number of
employees
A1 — 4 2
B1 — 2 5
C1 A1 3 1
D1 A1 2 2
E1 B1 3 4
F1 C1, D1 1 3
G1 B1, D1, E1 1 3
H1 E1, F1, G1 2 3292 15 (Repetitive) Project-Based Production
Table 15.6 Project action plan for project 2.
Activity
Immediate predecessor
activities
Duration
(wk)
Required number
of employees
A2 — 3 3
B2 — 3 2
C2 B2 1 2
D2 A2, C2 2 3
E2 D2 1 4
Start1 End1
0 4 
A1
0 4
0 2 
B1
2 4 2 5 
E1
4 7
4 6 
D1
5 7
4 7 
C1
4 7
6 7 
G1
7 8
8 10 
H1
8 10
7 8 
F1
7 8
Figure 15.9 AON representation of the network for project 1; the critical path is
A1-C1-F1-H1.
1 2 4 6 8 10
A1
B1
C1
D1
E1
F1
G1
H1
Figure 15.10 Total project duration for project 1 (10 days); the red lines are for the critical
path, and the dashed lines are for the slacks.15.4 Application 293
0 3 
A2
0 4
Start 2 End 2
0 3 
B2
0 3
3 4 
C2
3 4
4 6 
D2
4 6
6 7 
E2
6 7
Figure 15.11 AON representation of the network for project 2; the critical path is
B2-C2-D2-E2.
A2
B2
C2
D2
E2
123 4 5 6 7
Figure 15.12 Total project duration for project 2 (7 days).
Weeks d2 d1
Number of
operators
A2
E1
E1
B2
E2
E2
B1
C2
E1 D2
G1
G1
D1
F1
1
H1
A1
C1
12345 6 7 8 9 10 11 12
13
12
11
10
9
8
7
6
5
4
3
2
1
Figure 15.13 The required capacity profile for both projects using an early start heuristic.294 15 (Repetitive) Project-Based Production
both projects have the same deadline for d1 = d2 = d and hiring additional staff is
not possible, then (based on the required number of man-weeks per activity) the
minimal number of fixed employees W required to complete both projects within
the deadline d (W is a function of d) can be determined as follows:
Requirements project 1: 49 man weeks (=4 × 2+2 × 5+···+2 × 3)
Requirements project 2: 27 man weeks
Total requirements: 76 weeks
The lower bound for the workforce to complete both projects within the dead￾line: WLB(d) = entier(76/d). Since B1 requires five workers per week, the minimum
workforce is 5 thus, d must be greater than 15.
In reality, the deadline for project 1 is d1 =12 weeks, and the deadline for project
2 is d2 =9 weeks. The actual staff is equal to W =7 and the company can hire tem￾porary employees for €2000 per employee per week. Given these numbers, all slacks
for project 1 increase by 2 (compared to Figure 15.10) and for project 2 also by 2
(compared to Figure 15.12). PAMA wonders whether it is possible to complete both
projects within the given deadlines without using temporary employees. Based on
the precedence constraints, they can only execute activities F1, G1, and H1 in weeks
10, 11, and 12, requiring less than seven workers. The remaining activities require
64 men in 9 weeks, which is exactly one man a week less than what seven workers
can deliver. So they need at least one person per week from the temporary workers
force. They can’t complete both projects within the given deadlines without tempo￾rary workers. After some calculations by shifting, using the slacks and the overlap
shown in the GANNT charts, they find the schedule as given in Figure 15.14 that
respects both projects deadlines and uses exactly one week from a temporary worker,
which, based on the calculations, they needed at least and so the schedule is optimal
and minimizes the costs for hiring temporary employees (2200 Euro).
Weeks d2 d1
Number of
operators
7 B2
B1 E2
6 D2
G1
5
A2
4 E1
3 D1
F1 H1
2 A1 C2
1 C1
1 2 3 4 5 6 7 8 9 10 11 12 13
Figure 15.14 The schedule that respects both deadlines and minimizes temporary
workforce costs.References 295
References
ElHafsi, M. (2002). Optimal lead-times planning in serial production systems with
earliness and tardiness costs. IIE Transactions 34: 233–243.
Fendley, L.G. (1968). Towards the development of a complete multi-project scheduling
system. Journal of Industrial Engineering 19: 505–515.
Gallagher, C. (1987). A note on PERT assumptions. Management Science 33 (10): 1360.
Gong, L., de Kok, T., and Ding, J. (1994). Optimal lead-times planning in a serial
production system. Management Science 40 (5): 629–632.
Hendrikx, C.J.M. (1996). Het plannen van doorlooptijden in een serieel produktiesysteem.
Master thesis. Eindhoven University of Technology (in Dutch).
Kurtules, I. and Davis, E.W. (1982). Multi-project scheduling: analysis of project
performance. IEEE Transactions on Engineering Management 28 (2): 161–172.
Littlefield, T.K. Jr., and Randolp, P.H. (1987). An answer to Sasieni’s question on PERT
times. Management Science 33 (10): 1357–1359.
McCarthy, I. (1995). Lessons learned from organizational systematics and biological
taxonomy. Integrated Manufacturing Systems 6 (6): 37–48.
Meredith, J.R. and Mantel, S.J. (2000). Project Management: A Managerial Approach, 4e.
Wiley.
Rahim, A.R.A. and Baksh, M.S.N. (2003). The need for a new product development
framework for engineer-to-order products. European Journal of Innovation
Management 6 (3): 182–196.
Sterman, J.D. (2000). Business Dynamics: Systems Thinking and Modeling for a Complex
World. New York, NY: McGraw-Hill.297
Index
a
ABC-categorization 153, 155
activity on arrow (AOA) 277–281, 285
activity on node (AON) 277, 278,
284–285, 291–293
actual delivery time 51, 169
actual takt time 38
advanced approach 81, 83, 87
aggregate planning 25, 43, 120, 166
aggregate production planning 120
asset management 42
available stock (Iavail) 80, 83, 88, 95, 152
average tardiness 170, 181–183, 255
average throughput time, minimization of
171
b
batch operation time 38
Bertrand, Wortmann and Wijngaard
(BWW) 43
first level 45
fourth level 46
second level 45–46
third level 46
bill of materials (BOM) 33, 34, 59, 118,
130, 136
vs. bill-of-distribution 130–132
bucket brigade policy 219, 220
buffer policies 221
bullwhip effect 136, 143, 147–151
c
campaign production 187
capacity adjustment 255–256, 265–267
capacity allocation 163, 165–166, 196,
197, 200, 220–221, 240, 248, 252,
255, 262–263, 276, 286
capacity complexity 34–36
capacity planning 44, 45, 119, 120, 143,
166, 189, 242, 243, 287
causal models 61, 62
cell production 219
cherry picking 219
complexity 12, 15, 20, 21, 33–37, 146,
187, 188, 227, 274, 275, 286
confirmed line item performance 56
continuous demand 98, 108, 113,
115–116
continuous review 97–107, 157
controlled stock points 21, 24, 25, 50
correlation coefficient 69
cost-optimal cycle time 201
critical path 275, 277–285, 291–293
critical time 281, 283–285
cross training 219–220
customer order decoupling point (CODP)
17, 19, 20, 28, 31, 120
d
dead stock 130, 136
decomposition
example of 30–32
theorem 236
decouples 17
decoupling point control (DPC) 49, 150
advantages/disadvantages 146–150
Production Control in Practice: A Situation-Dependent Decisions Approach, First Edition.
Henny Van Ooijen and Corné Dirne.
© 2024 WILEY-VCH GmbH. Published 2024 by WILEY-VCH GmbH.298 Index
decoupling point control (DPC) (contd.)
assumptions 75
demand pattern 59–60
forecasting methods 60–63
make-to-order situation 150–151
make-to-stock situation 151
on order policy 50
optimal batch size 72–75
performance measures 53–58
real-life situations 51
single period problem 83–84
stock levels 49
on stock policy 50
decoupling points
horizontal decomposition 17
vertical decomposition 24
delivery time 8, 11, 20, 28–29, 45, 51–52,
84, 93, 97, 151, 167, 169, 239,
259–260
demand pattern 58–60, 63, 67, 104, 121,
152, 154, 155, 200
dependent demand 52, 59, 60, 146, 149
deterministic situations 52, 94, 169, 227,
249
discrete demand 83, 90, 97, 106, 109,
110, 113–114
double exponential smoothing model 70
D-skill chaining 220
due date (DD) 259
determination rules 240–246
oriented priority rules 259
e
earliest finish times (EFTs) 280, 290
earliest occurrence times (EOTs)
280–282
earliest starting times (ESTs) 281, 282
echelon inventory position 137–140
echelon lead times downstream 140
echelon stock control system (ESC) 135,
137
concepts 142
line requirements planning 138–139
economical order quantity (EOQ)
72–74, 76, 78, 81, 82, 196
effective processing time 228, 229, 257,
269, 270
EMPRO 223, 224
exception messages 130, 149, 150
exponential smoothing 60, 63, 65, 70–71
external lead time 248
external service level 55
f
First-Come-First-Served (FCFS) 232,
234, 235, 237, 240, 241, 249, 250,
252, 254, 257, 258, 262,
264, 265
fixed amount 109, 276
fixed-before-shared policy 221
flexibility, forms of 12
flexible batching 267–268
flow process production
application 200–203
characteristics 187
control attention points 189–196
cycle time determination 190–196
capacity-oriented approach
193–194
detailed approach 192–193
one production line 196
variable demand 194–196
definition 187
description 187–189
industries 188
MTO 197–200
capacity allocation 200
sequencing and work order release
198–200
MTS
capacity allocation 197
sequencing 196–197
work order release 197
flow shop sequencing problems 173
forecasting error 65–68
forecasting methods
causal models 61
demand with trend 67–71
qualitative methods 61Index 299
for stationary demand 63–67
types 60
full-time equivalents (FTE) 41
g
goods flow control 12, 23
h
high capacity complexity 34, 36
horizontal decomposition
CODP 19
controlled stock point 21
decoupling point 17
definition 17
logistic sub-functions 16
material management 16
order release and sequencing decisions
16
i
independent demand 52, 59, 60, 123
internal lead time 248
internal service levels 55
inventory control systems 96, 107, 190
inventory position 50, 80, 95–98, 109,
113, 118, 124, 136–140, 142, 149,
157
inventory turnover ratio 57
j
job shop problems 184, 185
Johnson’s rule 174, 250
Just-in-Case-methods 145
Just-in-Time vs. Just-in-Case 25–28
l
latest finishing time (LFT) 281, 285
latest occurrence time (LOT) 281, 282
latest starting times (LSTs) 281, 282
lead time control 45
lead time reliability
application 263–265
capacity allocation 262–263
sequencing 259–262
work order release 263
length of the routing 17
linear regression models 62, 68–70
line balancing 209
line requirements planning (LRP) 52,
53, 135, 138–143, 149
Little’s law 171
logistic planning and control (LPC) 7, 41
operator capacity 42
production 43
logistics 3
concepts 7–10
planning and control 6–7
push vs. pull in 28–30
structure sunroof 31
transformation processes 4, 5
low-level codes (LLC) 118, 124, 150
m
makespan 170–178, 183, 185
manufacturing resources planning
(MRP-II) 119–121
Markets-Output-Process-Control￾Organisation-Information 8
mass assembly
application 222–224
capacity allocation 220–221
sequencing 220
work order release 221–222
mass assembly/flow production 35, 36
mass assembly production
general description 205–207
main control attention points 207–220
non-pure flow production 206
pure flow production 208–209
no availability of efficient technology
212–213
different processing times 209–210
different products 211–212
disturbances at the work centers
212
variable processing times 211
variety of routings 213–214
master production schedule (MPS) 52,
120, 138, 149, 150
material complexity 33, 35, 36, 187300 Index
material coordination 45
material management 16
material requirements planning (MRP-I)
52, 117–119
material resource planning (MRP) 43,
45, 53, 55, 86, 117–140, 142,
149–151, 153
MaxGap Policy 221
maximum level 96, 98, 109–110
Maxload Policy 221
MaxQueueGap Policy 221
MaxQueue Policy 221
minimum order quantity (MOQ) 79,
109, 125
minimum slack first rule 289
mixed model production 211, 221
mix-flexibility 166
moving average model 63, 64
multi-model production 211, 221
multiple operations 171, 173–178, 266
multiple order rule (nQ-rule) 125
multiple projects 276, 286, 289
n
Newsvendor Problem 83, 85, 87–90
non-systematic error 66
no variable order-related costs 79–80
o
objective forecasting 60
operation due date (ODD) 166, 239, 244,
245, 255, 256, 259, 260, 266
operations, transformation steps 10
optimal approach 81, 82, 87, 88
optimal order quantity 72, 89
orders 11, 17
release 17
p
parallel servers 232, 234
part period balancing (PPB) method 76
periodic review 99–100, 107–109, 157
period order quantity (POQ) 76, 149
planned delivery time 51, 169
priority control 46, 159, 163–165, 167
priority policy 221
priority rule, SPT 185
probability density function 106, 113,
115, 231
processing time, defined 11
process-wise production systems 35, 187
production control
complexity, uncertainty, and flexibility
12
concepts 11
definition 3
production on demand 50
production orders 16, 18, 23, 34, 37, 42,
43, 49, 52, 120–122, 125, 163, 166,
167, 189, 228, 230, 256, 263
production unit control (PCU) 12, 15,
23, 33, 46, 49, 159, 187, 205, 227,
273, 275
production unit, organizational
department 21
project-based production 273
activity on node networks 284–285
application 291–294
construction of a network 276–285
control attention points 275–285
critical path and project duration
279–281
description 273–274
non-recurring activity 279
production control decisions 286–291
capacity allocation 286–288
sequencing 286
work order release 290–291
work order scheduling 289–290
stochastic activity times 282–284
project shops 37, 159, 273–275, 286
project-wise production 37, 286
promised delivery time 8, 51
P2-service level 54, 55, 105
pure project shop 274
pure rotation cycle 196
q
quantitative methods 60, 61
quantitative models 214Index 301
two stations with failures 216–218
two stations without failures 215–216
quantity discount 57, 78–79, 84, 157
queueing theory 227, 230–237
r
RANDOM rule 170, 249, 261
RDM 240, 241
relaxing assumptions 183–185
relocating operators 252, 254
remaining slack (RS) 247, 260, 261
remaining slack per operation (RSO)
260–262, 264
reorder level 96–109, 113–117, 124, 136,
142, 143, 149
reorder point systems 93–96, 101, 146
repetitive project shop 159, 273
requested line item performance 56
required reaction time 41, 45
required takt time 38
rescheduling assumption 132
routing, defined 10, 34
s
safety stock 60, 101, 105, 106, 112, 128,
136, 137, 140, 145, 148–151, 156,
194, 197
scheduled receipts 122, 124, 126, 128,
130, 132, 149
sequencing orders
with a delivery date 178–183
with different routings 184–185
without delivery date 171
service level 53–58, 104–106, 136, 139,
145, 146, 157
shortest processing time rule (SPT) 171,
179, 183, 185, 245–247, 249–251,
261, 262, 265
short-term capacity adjustment 265–267
Silver-Meal-algorithm 77–78
single item single echelon situation (SISE)
136
single linear regression model 62
single project 275, 276, 286, 290
small series production 36, 227
control attention points 229–248
decisions 248–263
description 227–229
internal vs. external due date 248
lead time reliability related aspects
239–248
throughput time
capacity allocation 252–256
related aspects 236–239
sequencing 249–252
work order release/work order detail
planning 256–259
static production situations 159,
169–185
stationary demand 63–67, 104, 149
stochastic situation 51, 52, 164, 169, 171,
227
subjective forecasting 60
sunroof 30–32
systematic error 66, 68
t
takt time 38, 207–211, 213, 220, 222
technical stock (Iphys) 95
throughput time production units 37–39
t-period probability function 113
transformation process 3–7, 9–11, 163
transition matrix 235, 263
Travelling Salesman problem 183
u
unbiased forecasting model 66
uncertainty 12, 21, 35–37, 60, 93, 100,
108, 117, 128, 130, 136, 149, 151,
157, 190, 228, 273, 274, 282–284,
286
uptime (UT) 217
utilization planning 45, 46, 189, 190
v
VEGPRO 200, 202
VEPA 263–265
vertical decomposition
controlled stock points 24
decoupling point 24302 Index
vertical decomposition (contd.)
decoupling point control 22
definition 23
for operator capacity 42
order release decisions 23
production unit control 23
volume flexibility 165, 166, 188
w
Wagner–Whitin algorithm 77, 78, 84,
86–87
weighted average throughput times
171–172
weighted moving average 63–65
work centers, defined 10
workflow 10
Work In Next Queue rule (WINQ) 249,
250
workload acceptance 44
workload control 46, 167, 171, 257,
268–270
work order acceptance 46, 166
work order detail planning 46, 163, 164,
166–168, 190, 230, 248, 256–259
work order release, production unit level
46
x
XYZ-categorization 153–155
z
zoned craft policy 221WILEY END USER LICENSE AGREEMENT
Go to www.wiley.com/go/eula to access Wiley’s ebook EULA.
