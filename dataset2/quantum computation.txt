Quantum Computation
This book presents the mathematics of quantum computation. The purpose is to introduce the 
topic of quantum computing to students in computer science, physics and mathematics, who 
have no prior knowledge of this field. 
The book is written in two parts. The primary mathematical topics required for an initial under￾standing of quantum computation are dealt with in Part I: sets, functions, complex numbers and 
other relevant mathematical structures from linear and abstract algebra. Topics are illustrated 
with examples focussing on the quantum computational aspects which will follow in more detail 
in Part II. 
Part II discusses quantum information, quantum measurement and quantum algorithms. These 
topics provide foundations upon which more advanced topics may be approached with confi￾dence. 
Features:
• A more accessible approach than most competitor texts, which move into advanced,
research-level topics too quickly for today’s students.
• Part I is comprehensive in providing all necessary mathematical underpinning, particu￾larly for those who need more opportunity to develop their mathematical competence.
• More confident students may move directly to Part II and dip back into Part I as a reference. 
• Ideal for use as an introductory text for courses in quantum computing.
• Fully worked examples illustrate the application of mathematical techniques.
• Exercises throughout develop concepts and enhance understanding.
• End-of-chapter exercises offer more practice in developing a secure foundation.
Helmut Bez holds a doctorate in quantum mechanics from Oxford University. He is a visit￾ing fellow in quantum computation in the Department of Computer Science at Loughborough 
University, England. He has authored around 50 refereed papers in international journals and a 
further 50 papers in refereed conference proceedings. He has 35 years’ teaching experience in 
computer science, latterly as a reader in geometric computation, at Loughborough University. 
He has supervised/co-supervised 18 doctoral students. 
Tony Croft was the founding director of the Mathematics Education Centre at Loughborough 
University, one of the largest groups of mathematics education researchers in the UK, with an 
international reputation for the research into and practice of the learning and teaching of math￾ematics. He is co-author of several university-level textbooks, has co-authored numerous aca￾demic papers and edited academic volumes. He jointly won the IMA Gold Medal 2016 for out￾standing contribution to the improvement of the teaching of mathematics and is a UK National 
Teaching Fellow. He is currently emeritus professor of mathematics education at Loughborough 
University (https://www.lboro.ac.uk/departments/mec/staff/academic-visitors/tony-croft/).Advances in Applied Mathematics
Series Editor: Daniel Zwillinger 
Introduction to Quantum Control and Dynamics
Domenico D’Alessandro
Handbook of Radar Signal Analysis
Bassem R. Mahafza, Scott C. Winton, Atef Z. Elsherbeni
Separation of Variables and Exact Solutions to Nonlinear PDEs
Andrei D. Polyanin, Alexei I. Zhurov
Boundary Value Problems on Time Scales, Volume I
Svetlin Georgiev, Khaled Zennir
Boundary Value Problems on Time Scales, Volume II
Svetlin Georgiev, Khaled Zennir
Observability and Mathematics
Fluid Mechanics, Solutions of Navier-Stokes Equations, and Modeling
Boris Khots
Handbook of Differential Equations, Fourth Edition
Daniel Zwillinger, Vladimir Dobrushkin
Experimental Statistics and Data Analysis for Mechanical and Aerospace Engineers
James Middleton
Advanced Engineering Mathematics with MATLAB®, Fifth Edition
Dean G. Duffy
Handbook of Fractional Calculus for Engineering and Science
Harendra Singh, H. M. Srivastava, Juan J. Nieto
Advanced Engineering Mathematics
A Second Course with MATLAB®
Dean G. Duffy
Quantum Computation
Helmut Bez and Tony Croft
Computational Mathematics
An Introduction to Numerical Analysis and Scientific Computing with Python
Dimitrios Mitsotakis
https://www.routledge.com/Advances-in-Applied-Mathematics/book-series/CRCADVAPPMTH?pd=publis
hed,forthcoming&pg=1&pp=12&so=pub&view=listQuantum Computation
Helmut Bez and Tony CroftFirst edition published 2023
by CRC Press
6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742
and by CRC Press
4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN
CRC Press is an imprint of Taylor & Francis Group, LLC
© 2023 Helmut Bez and Tony Croft 
Reasonable efforts have been made to publish reliable data and information, but the author and publisher cannot 
assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers 
have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright 
holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowl￾edged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, transmitted, or 
utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including pho￾tocopying, microfilming, and recording, or in any information storage or retrieval system, without written permission 
from the publishers.
For permission to photocopy or use material electronically from this work, access www.copyright.com or contact the 
Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 01923, 978-750-8400. For works that are 
not available on CCC please contact mpkbookspermissions@tandf.co.uk
Trademark notice: Product or corporate names may be trademarks or registered trademarks and are used only for 
identification and explanation without intent to infringe.
 Library of Congress Cataloging‑in‑Publication Data
Names: Bez, H. E., author. | Croft, Tony, 1957- author. 
Title: Quantum computation/Helmut Bez and Tony Croft. 
Description: First edition. | Boca Raton : CRC Press, [2023] | Series:
   Advances in applied mathematics | Includes bibliographical references
   and index.
Identifiers: LCCN 2022039303 (print) | LCCN 2022039304 (ebook) | ISBN
   9781032206486 (hbk) | ISBN 9781032206493 (pbk) | ISBN 9781003264569
   (ebk) 
Subjects: LCSH: Quantum computing.
Classification: LCC QA76.889 .B49 2023 (print) | LCC QA76.889 (ebook) |
   DDC 004.1--dc23/eng/20221104
LC record available at https://lccn.loc.gov/2022039303
LC ebook record available at https://lccn.loc.gov/2022039304
ISBN: 978-1-032-20648-6 (hbk)
ISBN: 978-1-032-20649-3 (pbk)
ISBN: 978-1-003-26456-9 (ebk)
DOI: 10.1201/9781003264569
Typeset in LM Roman 
by KnowledgeWorks Global Ltd.
Publisher’s note: This book has been prepared from camera-ready copy provided by the authors.For the many happy and productive years we had together,
Helmut Bez wishes to dedicate his contribution to this book to
his late wife Carys.
For countless hours of patience whilst this book has been written,
Tony Croft wishes to dedicate his contribution to his wife Kate.Contents
Preface xv
Acknowledgements xvii
Symbols xix
I Mathematical Foundations for Quantum Computation 1
1 Mathematical preliminaries 3
1.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.2 Definitions and notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.3 Venn diagrams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.4 Laws of set algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.5 Boolean algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.5.1 The exclusive-or operator (xor) . . . . . . . . . . . . . . . . . . . . . 8
1.6 Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.7 Cartesian product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
1.8 Number bases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
1.9 Modular arithmetic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
1.10 Relations, equivalence relations and equivalence classes . . . . . . . . . . . 19
1.10.1 Equivalence relations . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
1.11 Combinatorics – permutations and combinations . . . . . . . . . . . . . . . 24
1.12 End-of-chapter exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
2 Functions and their application to digital gates 27
2.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
2.2 Introductory definitions and terminology . . . . . . . . . . . . . . . . . . . 27
2.3 Some more functions f : R → R . . . . . . . . . . . . . . . . . . . . . . . . 31
2.3.1 The relative growth of functions . . . . . . . . . . . . . . . . . . . . 33
2.4 The Boolean functions f : B → B . . . . . . . . . . . . . . . . . . . . . . . 34
2.5 Functions defined on Cartesian products . . . . . . . . . . . . . . . . . . . 38
2.5.1 The Boolean functions f : B × B → B . . . . . . . . . . . . . . . . . 39
2.5.2 The Boolean functions f : B
2 → B
2
. . . . . . . . . . . . . . . . . . . 41
2.5.3 Further Boolean functions . . . . . . . . . . . . . . . . . . . . . . . . 45
2.6 Further composition of functions . . . . . . . . . . . . . . . . . . . . . . . . 47
2.7 The Cartesian product of functions . . . . . . . . . . . . . . . . . . . . . . 48
2.8 Permuting (swapping) binary digits and binary strings . . . . . . . . . . . 49
2.8.1 A classical digital circuit for swapping binary digits . . . . . . . . . 50
2.8.2 Swapping binary digits using the Feynman cnot gate . . . . . . . . . 50
2.8.3 Swapping strings of binary digits – vectorising the swap operator . . 51
2.9 Copying binary digits and binary strings . . . . . . . . . . . . . . . . . . . 52
2.9.1 Fan-out . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
viiviii Contents
2.9.2 A dupe gate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
2.9.3 The cnot gate. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
2.9.4 The Feynman double gate . . . . . . . . . . . . . . . . . . . . . . . . 54
2.9.5 The ccnot (Toffoli) gate . . . . . . . . . . . . . . . . . . . . . . . . . 54
2.9.6 Fan-out copying of binary strings . . . . . . . . . . . . . . . . . . . . 54
2.9.7 Digital string copying with the non-reversible copy and reversible
swap gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
2.9.8 Digital string copying using only reversible (the cnot and swap) gates 55
2.9.9 Digital string copying using only the cnot gate . . . . . . . . . . . . 55
2.10 Periodic functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
2.10.1 Real-valued periodic functions . . . . . . . . . . . . . . . . . . . . . 56
2.10.2 Periodic Boolean functions . . . . . . . . . . . . . . . . . . . . . . . 58
2.11 End-of-chapter exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
3 Complex numbers 63
3.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
3.2 Introduction to complex numbers and the imaginary number i . . . . . . . 63
3.3 The arithmetic of complex numbers . . . . . . . . . . . . . . . . . . . . . . 64
3.4 The set of all complex numbers as a field . . . . . . . . . . . . . . . . . . . 67
3.5 The Argand diagram and polar form of a complex number . . . . . . . . . 67
3.6 The exponential form of a complex number . . . . . . . . . . . . . . . . . . 69
3.7 The Fourier transform: an application of the exponential form of a complex
number . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
3.8 End-of-chapter exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
4 Vectors 75
4.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
4.2 Vectors: preliminary definitions . . . . . . . . . . . . . . . . . . . . . . . . . 75
4.3 Graphical representation of two- and three-dimensional vectors . . . . . . . 77
4.4 Vector arithmetic: addition, subtraction and scalar multiplication . . . . . 79
4.5 Qubits represented as vectors . . . . . . . . . . . . . . . . . . . . . . . . . . 81
4.5.1 Combining kets and functions f : B → B . . . . . . . . . . . . . . . . 82
4.6 The inner product (scalar product, dot product) . . . . . . . . . . . . . . . 83
4.6.1 The inner product in R
2 and R
3
. . . . . . . . . . . . . . . . . . . . 83
4.6.2 The inner product on C
n: Definition 1 . . . . . . . . . . . . . . . . . 86
4.6.3 The inner product on C
n: Definition 2 . . . . . . . . . . . . . . . . . 88
4.7 End-of-chapter exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
5 Matrices 93
5.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
5.2 Matrices: preliminary definitions . . . . . . . . . . . . . . . . . . . . . . . . 93
5.3 Matrix arithmetic: addition, subtraction and scalar multiplication . . . . . 95
5.4 The product of two matrices . . . . . . . . . . . . . . . . . . . . . . . . . . 97
5.5 Block multiplication of matrices . . . . . . . . . . . . . . . . . . . . . . . . 104
5.6 Matrices, inner products and ket notation . . . . . . . . . . . . . . . . . . . 107
5.7 The determinant of a square matrix . . . . . . . . . . . . . . . . . . . . . . 109
5.8 The inverse of a square matrix . . . . . . . . . . . . . . . . . . . . . . . . . 110
5.8.1 The inverse of an n × n matrix . . . . . . . . . . . . . . . . . . . . . 111
5.9 Similar matrices and diagonalisation . . . . . . . . . . . . . . . . . . . . . . 112
5.10 Orthogonal matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
5.11 Unitary matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115Contents ix
5.12 Matrices and the solution of linear simultaneous equations . . . . . . . . . 117
5.12.1 Gaussian elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
5.13 Matrix transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
5.14 Projections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
5.15 End-of-chapter exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
6 Vector spaces 133
6.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
6.2 Definition of a vector space . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
6.3 Inner product spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
6.4 Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
6.5 Linear combinations, span and LinSpan . . . . . . . . . . . . . . . . . . . . 138
6.6 Linear independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
6.7 Basis of a vector space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
6.8 Change of basis matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
6.9 Orthogonal projections onto subspaces . . . . . . . . . . . . . . . . . . . . 149
6.10 Construction of an orthogonal basis – the Gram-Schmidt process . . . . . . 150
6.11 The Cartesian product of vector spaces . . . . . . . . . . . . . . . . . . . . 152
6.12 Equivalence classes defined on vector spaces . . . . . . . . . . . . . . . . . 152
6.13 The sum of a vector and a subspace . . . . . . . . . . . . . . . . . . . . . . 153
6.14 The quotient space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
6.15 End-of-chapter exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
7 Eigenvalues and eigenvectors of a matrix 157
7.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
7.2 Preliminary definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
7.3 Calculation of eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
7.4 Calculation of eigenvectors . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
7.5 Real symmetric matrices and their eigenvalues and eigenvectors . . . . . . 166
7.6 Diagonalisation of real symmetric matrices . . . . . . . . . . . . . . . . . . 170
7.7 The spectral theorem for symmetric matrices . . . . . . . . . . . . . . . . . 173
7.8 Self-adjoint matrices and their eigenvalues and eigenvectors . . . . . . . . . 175
7.9 End-of-chapter exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
8 Group theory 177
8.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
8.2 Preliminary definitions and the axioms for a group . . . . . . . . . . . . . . 177
8.3 Permutation groups and symmetric groups . . . . . . . . . . . . . . . . . . 181
8.4 Unitary groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
8.5 Cosets, partitions and equivalence classes . . . . . . . . . . . . . . . . . . . 185
8.6 Quotient groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
8.7 End-of-chapter exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
9 Linear transformations 193
9.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
9.2 Preliminary information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
9.3 The kernel and image of a linear transformation . . . . . . . . . . . . . . . 196
9.4 Linear functionals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
9.5 Matrix representations of linear transformations . . . . . . . . . . . . . . . 198
9.6 Bilinear maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200
9.7 End-of-chapter exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202x Contents
10 Tensor product spaces 205
10.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
10.2 Preliminary discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
10.3 Calculation of tensor products . . . . . . . . . . . . . . . . . . . . . . . . . 206
10.4 Inner products and norms on the tensor product space C
2 ⊗ C
2
. . . . . . 211
10.5 Formal construction of the tensor product space . . . . . . . . . . . . . . . 212
10.5.1 The free vector space generated by U × V . . . . . . . . . . . . . . . 212
10.5.2 An equivalence relation on LinSpan(U × V ). . . . . . . . . . . . . . 214
10.5.3 Definition of the tensor product space . . . . . . . . . . . . . . . . . 215
10.6 End-of-chapter exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
11 Linear operators and their matrix representations 219
11.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
11.2 Linear operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
11.3 The matrix representation of a linear operator . . . . . . . . . . . . . . . . 222
11.4 The matrix representation of a linear operator when the underlying basis is
orthonormal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
11.5 Eigenvalues and eigenvectors of linear operators . . . . . . . . . . . . . . . 226
11.6 The adjoint and self-adjoint linear operators . . . . . . . . . . . . . . . . . 227
11.7 Unitary operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
11.8 Linear operators on tensor product spaces . . . . . . . . . . . . . . . . . . 231
11.9 End-of-chapter exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238
II Foundations of quantum-gate computation 241
12 Introduction to Part II 243
12.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
12.2 Computation and physics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
12.3 Physical systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
12.4 An overview of digital computation . . . . . . . . . . . . . . . . . . . . . . 244
12.4.1 Digital computer states . . . . . . . . . . . . . . . . . . . . . . . . . 244
12.4.2 Digital computer dynamics . . . . . . . . . . . . . . . . . . . . . . . 244
12.4.3 Digital computer ‘measurement’ . . . . . . . . . . . . . . . . . . . . 244
12.5 The emergence of quantum computation . . . . . . . . . . . . . . . . . . . 245
12.6 Observables and measurement . . . . . . . . . . . . . . . . . . . . . . . . . 245
12.6.1 Observables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
12.6.2 Measurement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
12.7 The Stern-Gerlach quantum experiment . . . . . . . . . . . . . . . . . . . . 246
13 Axioms for quantum computation 249
13.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
13.2 Quantum state spaces for computation . . . . . . . . . . . . . . . . . . . . 249
13.2.1 The 2-dimensional vector space representation of ‘spin’ and quantum
bits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250
13.2.2 The case for quantum bit (qubit) representation in C
2
. . . . . . . . 251
13.2.3 Quantum bits – or qubits . . . . . . . . . . . . . . . . . . . . . . . . 251
13.2.4 Global phase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
13.2.5 Projective spaces and the Bloch sphere . . . . . . . . . . . . . . . . . 252
13.2.6 Multi-qubit state spaces . . . . . . . . . . . . . . . . . . . . . . . . . 256
13.3 Quantum observables and measurement for computation . . . . . . . . . . 257
13.4 Quantum dynamics for computation . . . . . . . . . . . . . . . . . . . . . . 257Contents xi
13.5 Orthogonal projection in a complex inner product space . . . . . . . . . . . 258
13.6 A summary of the axioms for quantum computation . . . . . . . . . . . . . 259
13.7 Special cases of the measurement axiom . . . . . . . . . . . . . . . . . . . . 260
13.8 A formal comparison with digital computation . . . . . . . . . . . . . . . . 260
13.9 Gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
14 Quantum measurement 1 263
14.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
14.2 Measurement using Axiom 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . 263
14.2.1 Measurement of non-superimposed states in C
2
. . . . . . . . . . . . 263
14.2.2 Measurement of non-superimposed states in C
2 ⊗ C
2
. . . . . . . . . 264
14.2.3 Measurement of non-superimposed states in C
2 ⊗ C
2 ⊗ C
2
. . . . . . 265
15 Quantum information processing 1: the quantum emulation of familiar
invertible digital gates 267
15.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
15.2 On the graphical representation of quantum gates . . . . . . . . . . . . . . 267
15.3 A 1-bit/qubit gate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268
15.3.1 The digital not gate . . . . . . . . . . . . . . . . . . . . . . . . . . . 268
15.3.2 The quantum not gate, notQ, on BC2 = {|xi : x ∈ B} . . . . . . . . . 268
15.4 2-bit/qubit gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
15.4.1 The non-invertible digital cnot, or xor, gate . . . . . . . . . . . . . . 269
15.4.2 The invertible digital cnot, or Feynman FD, gate . . . . . . . . . . . 269
15.4.3 The quantum cnot, or Feynman FQ, gate on
B⊗2C2 = {|xi|yi : x, y ∈ B} . . . . . . . . . . . . . . . . . . . . . . . 270
15.5 3-bit/qubit gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271
15.5.1 The digital Toffoli (or ccnot) gate . . . . . . . . . . . . . . . . . . . . 271
15.5.2 The quantum Toffoli (or ccnot) gate on
B⊗3C2 = {|xi|yi|zi : x, y, z ∈ B} . . . . . . . . . . . . . . . . . . . . . 272
15.5.3 The digital Peres (invertible half-adder) gate . . . . . . . . . . . . . 273
15.5.4 The quantum Peres gate on B⊗3C2 = {|xi|yi|zi : x, y, z ∈ B} . . . . 273
16 Unitary extensions of the gates notQ, FQ, TQ and PQ: more general
quantum inputs 275
16.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275
16.2 A lemma on unitary operators . . . . . . . . . . . . . . . . . . . . . . . . . 275
16.3 The notQ gate on C
2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276
16.4 The Feynman FQ gate on ⊗2C
2
. . . . . . . . . . . . . . . . . . . . . . . . 277
16.5 The quantum Toffoli gate on ⊗3C
2
. . . . . . . . . . . . . . . . . . . . . . 277
16.6 The quantum Peres gate on ⊗3C
2
. . . . . . . . . . . . . . . . . . . . . . . 277
16.7 Summation expressions for the unitary extensions . . . . . . . . . . . . . . 278
16.7.1 On C
2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278
16.7.2 On C
2 ⊗ C
2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278
16.7.3 On C
2 ⊗ C
2 ⊗ C
2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279
16.7.4 Notation and closing observations . . . . . . . . . . . . . . . . . . . 279
17 Quantum information processing 2: the quantum emulation of arbitrary
Boolean functions 281
17.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281
17.2 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281
17.3 Quantum emulation of arbitrary invertible Boolean functions . . . . . . . . 281xii Contents
17.3.1 The quantum emulation of the invertible subset of F(B, B) . . . . . 281
17.3.2 The quantum emulation of the invertible subset of F(B
2
, B
2
) . . . . 282
17.3.3 Explicit forms of the emulations of Section 17.3.2 . . . . . . . . . . . 283
17.3.4 The invertible subset of F(B
n, B
n), n ≥ 3 . . . . . . . . . . . . . . . 284
17.4 Quantum emulation of arbitrary non-invertible Boolean functions . . . . . 284
17.4.1 A fundamental lemma . . . . . . . . . . . . . . . . . . . . . . . . . . 285
17.4.2 The non-invertible subset of F(B, B) . . . . . . . . . . . . . . . . . . 286
17.4.3 The non-invertible functions F(B
2
, B) . . . . . . . . . . . . . . . . . 287
17.4.4 The non-invertible subset of F(B
n, B
n) . . . . . . . . . . . . . . . . . 288
17.4.5 The general functions F(B
n, B
m) . . . . . . . . . . . . . . . . . . . . 288
17.5 Black-box representations of Boolean functions and their quantum
emulations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
18 Invertible digital circuits and their quantum emulations 293
18.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293
18.2 Invertible digital circuits . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293
18.3 Junk removal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296
18.4 Quantum emulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
19 Quantum measurement 2: general pure states, Bell states 299
19.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
19.2 The measurement of super-imposed quantum states . . . . . . . . . . . . . 299
19.3 Measuring the EPR-Bell state √
1
2
(|0i|0i + |1i|1i) . . . . . . . . . . . . . . . 300
19.4 More general measurements of 2-qubit states in the computational basis . . 302
19.5 Measuring 2-qubit states in the Bell basis . . . . . . . . . . . . . . . . . . . 303
19.5.1 Measuring the observables S
2
3 and S
2
1
. . . . . . . . . . . . . . . . . 303
20 Quantum information processing 3 305
20.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305
20.2 Quantum parallelism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305
20.3 Qubit swapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306
20.4 Quantum copying – the no-cloning theorem . . . . . . . . . . . . . . . . . . 307
20.5 Quantum teleportation 1, computational-basis measurement . . . . . . . . 308
20.6 Quantum teleportation 2, Bell-basis measurement . . . . . . . . . . . . . . 310
21 More on quantum gates and circuits: those without digital equivalents 313
21.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313
21.2 General 1-qubit quantum gates . . . . . . . . . . . . . . . . . . . . . . . . . 313
21.3 The Pauli and the √
not 1-qubit gates . . . . . . . . . . . . . . . . . . . . . 314
21.4 Further 1-qubit gates: phase-shift and Hadamard . . . . . . . . . . . . . . . 315
21.5 Universal 1-qubit circuits . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316
21.6 Some 2-qubit quantum gates . . . . . . . . . . . . . . . . . . . . . . . . . . 317
21.6.1 Tensor product gates . . . . . . . . . . . . . . . . . . . . . . . . . . . 317
21.6.2 The Hadamard-cnot circuit . . . . . . . . . . . . . . . . . . . . . . . 317
21.7 The Hadamard gate on n-qubit registers . . . . . . . . . . . . . . . . . . . 320
22 Quantum algorithms 1 323
22.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323
22.2 Preliminary lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323
22.3 Diagrammatic representation of the measurement process . . . . . . . . . . 325
22.4 An introduction to computational complexity . . . . . . . . . . . . . . . . . 325Contents xiii
22.5 Oracle-based complexity estimation . . . . . . . . . . . . . . . . . . . . . . 326
22.6 Deutsch’s algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326
22.7 The Deutsch-Jozsa algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 328
22.8 The Bernstein-Vazirani algorithm . . . . . . . . . . . . . . . . . . . . . . . 330
23 Quantum algorithms 2: Simon’s algorithm 333
23.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333
23.2 Periodicity, groups, subgroups and cosets . . . . . . . . . . . . . . . . . . . 333
23.2.1 Real-valued functions . . . . . . . . . . . . . . . . . . . . . . . . . . 333
23.2.2 A summary of the real-valued case . . . . . . . . . . . . . . . . . . . 335
23.3 Boolean functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336
23.3.1 The subgroups of group (B
3
, ⊕) . . . . . . . . . . . . . . . . . . . . . 338
23.3.2 The cosets B
3/Ks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338
23.4 The hidden subgroup problem . . . . . . . . . . . . . . . . . . . . . . . . . 338
23.5 Simon’s problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338
23.6 The complexity of digital solutions . . . . . . . . . . . . . . . . . . . . . . . 339
23.7 Simon’s quantum algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 339
A Probability 343
A.1 Definition of probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343
A.2 Discrete random variables and probability distributions . . . . . . . . . . . 344
B Trigonometric ratios and identities 347
C Coordinate systems 349
D Field axioms 351
E Solutions to selected exercises 353
References 365
Index 367Preface
In this book the elements of quantum computation are presented by comparison, where
possible, with the digital case. The path from the introduction of the axioms of quantum
computation to a discussion of algorithms that demonstrate its potential is kept short – the
intention being to move the reader, having no previous knowledge of quantum computation,
rapidly to a point where more advanced aspects may be investigated.
Quantum computation has evolved from the foundations of digital computer science
and quantum mechanics and, whilst it is possible to ‘understand’ quantum computation
without physics, the approach does require a number of ‘leaps of faith’ by the learner – and
it depends upon what one means by ‘understand’. Here an attempt is made to remove some
of the mystery from the processes of quantum computation by considering the elementary
quantum physics on which it is based. In particular, we
1. introduce the Stern-Gerlach experiment to provide support for the axioms of
quantum computation, and
2. present the measurement process in terms of the von Neumann-L¨uders postulate
of quantum mechanics, rooting measurement in quantum computation directly
to a fundamental axiom of quantum physics.
The target readership of this book comprises those wishing to acquaint themselves with
quantum computation and have some mathematical skills and some understanding of digital
computation.
There are, currently, two main approaches to quantum computation, specifically:
1. quantum-gate computing,
2. adiabatic quantum computing – or quantum annealing.
Quantum-gate computing is easier to relate to the digital case, and many of the world’s
leading computer companies are pursuing this approach. For these reasons, this text relates
to the gate model of quantum computation.
There is little that is new in this book; it comprises the work of others compiled to assist
the intended readership, i.e., advanced undergraduate and masters students in mathemat￾ics or digital computation or physics, in acquiring a sufficient understanding of quantum
computation to enable more advanced study of the topic to be undertaken.
Readers with a strong background in mathematics may wish to start their reading at
Part II, using Part I as reference material. Those knowledgeable in quantum mechanics
may wish to skip the first chapter of Part II and the material on quantum measurement.
In addition Chapter 18 of Part II, relating to junk removal from general circuits, may be
omitted on first reading.
Part I is intended to provide a detailed account of all the mathematical tools necessary
to understand Part II and enable the reader to progress further with their studies. Success￾ful students need to become very fluent in the manipulation of sets and functions, topics
described in Chapters 1 and 2. A vector space representation is required to encapsulate the
properties of quantum mechanical systems and the generally accepted choice for the number
field over which the vector space is defined is the set of complex numbers. Thus Chapter 3
xvxvi Preface
provides an extensive introduction to this topic and the essential results required later in
the book. Thereafter, Part I progresses through topics in linear algebra, including details
of vector and matrix algebra, vector spaces, inner products, linear operators, eigenvalues
and eigenvectors and tensor product spaces. Quantum computation requires extensive use
of a wide range of mathematical notation and to assist the reader navigate through this,
a comprehensive list of symbols is provided at the beginning of the book. Throughout, an
abundance of worked examples and exercises (many with solutions at the back of the book)
provide opportunities for the reader to practice and develop the required fluency. Four Ap￾pendices provide supplementary information, for reference where necessary, on probability,
trigonometric functions and identities, coordinate system and field axioms. Armed with all
the mathematical tools provided in Part I, the reader will be well-equipped to tackle topics
in quantum measurement, quantum information processing and the algorithms designed to
demonstrate the potential power of quantum computation in Part II.
Finally, we hope you learn a great deal from this book, and come to share our enthusiasm
for quantum computation.
Helmut Bez
Tony Croft
December 2022Acknowledgements
The authors gratefully acknowledge the most helpful comments and suggestions received
from the following colleagues who commented on various drafts of chapters in the book: Dr
Francis Duah, Dr James Flint, Professor Michael Grove, Dr Glynis Hoenselaers, Professor
Matthew Inglis, Professor Barbara Jaworski, Dr Ciar´an Mac an Bhaird.
xviiSymbols
Symbol Description
Sets and numbers
∈ is an element of set
|A| cardinality of set A
∩ set intersection
∪ set union
A is the complement of set A
⊂ is a proper subset of
⊆ is a subset of
⊃ is a proper superset of
⊇ is a superset of
E universal set
∅ empty set
R set of real numbers
R
+ set of positive real numbers
[0, p) interval 0 ≤ x < p, for x ∈ R
N set of natural numbers 0, 1, 2, . . .
Z set of integers . . . , −2, −1, 0, 1, 2, . . .
Q set of rational numbers p
q with
p, q ∈ Z, q 6= 0
C set of complex numbers
A × B Cartesian product of sets A and B
R
2 Cartesian product R × R
R
n Cartesian product R × R × . . . × R
C
2 Cartesian product C × C
C
n Cartesian product C × C × . . . × C
A\B the set A with all elements of set B
removed
R\0 the set R with 0 removed
B the set of binary digits {0, 1}
B
k
the Cartesian product B×B×. . .×B
{0, 1}
k
the Cartesian product B×B×. . .×B
0
n an n-tuple of zeros, (0, 0, 0, . . . , 0),
P
also (000 . . . 0)
n
i=1 xi the sum x1 + x2 + . . . + xn
δij Kronecker delta = 
1 i = j
0 i 6= j
e exponential constant, e = 2.718 . . .
≡ is identically equal to
∼ is equivalent to
Functions
f : A → B function mapping set A to set B
(f ◦ g)(x) composition f(g(x))
F(A, B) set of all functions from A to B
Complex numbers
i
2 = −1 i the imaginary unit
a + ib Cartesian form of z
z = r∠θ polar form r(cos θ + i sin θ)
z = re
iθ
exponential form
z
∗
complex conjugate of z
|z| modulus of complex number z
Re(z) real part of z
Im(z) imaginary part of z
arg(z) argument of z
Vectors
−−→OP position vector of point P
{i, j} standard orthonormal basis in R
2
{i, j, k} standard orthonormal basis in R
3
kuk norm, modulus, magnitude of vec￾tor u
| i ket
h | bra
|0i ket 0 = 
1
0

|1i ket 1 = 
0
1

u · v scalar (dot) product of vectors u
and v
hu, vi inner product of vectors u and v
hu|vi inner product of vectors u and v
u × v vector product of vectors u and v
u ⊗ v tensor product of vectors u and v
nˆ unit vector in the direction of n
xixxx Symbols
Matrices
In×n n × n identity matrix
In n × n identity matrix, In×n
AT
transpose of matrix A
A−1
inverse of matrix A
A†
conjugate transpose of matrix A
|A| detA = determinant of matrix A
diag{ } diagonal matrix
Tensors
u ⊗ v tensor product of vectors u and v
|ψi ⊗ |φitensor product of tensors |ψi and
|φi
|ψi|φi alternative to |ψi ⊗ |φi
|ψ φi alternative to |ψi ⊗ |φi
A ⊗ B tensor product of linear operators
N
A and B
n
i=1 A n-fold tensor product of the linear
operator A
BC2 {|0i, |1i}
basis states of C
2
B⊗2C2 {|00i, |01i, |10i, |11i}
basis states of C
2 ⊗ C
2 = ⊗2C
2
BC2⊗C2 same as B⊗2C2
Boolean algebra
∧ Boolean operator and
∨ Boolean operator or
⊕ Boolean operator ‘exclusive-or’
(xor)
x not x when x ∈ B
Counting and probability
n! factorial n
= n × (n − 1) × · · · × 3 × 2 × 1
nPr the number of permutations of r ob￾jects chosen from n distinct objects
nCr the number of combinations of r ob￾jects chosen from n distinct objects
￾n
r

same as nCr
Group theory
(G, ◦) The set G with binary operation ◦
satisfying the group axioms
H ≤ G H is a subgroup of G
H / G H is a normal subgroup of G
G/H The quotient or factor group
(Z/nZ, +) The additive group of integers
modulo n
(Zn, +) (alternative: The additive group of
integers modulo n)
(Z/nZ)
× The multiplicative group of inte￾gers modulo n
U(2) unitary group on C
2
Abbreviations
iff if and only if
wrt with respect tPart I
Mathematical Foundations for
Quantum Computation1
Mathematical preliminaries
1.1 Objectives
Quantum computation is multidisciplinary drawing as it does from computer science,
physics and mathematics. Even when considering solely mathematical aspects of quan￾tum computation, the student will need to use a diverse range of tools and techniques
from seemingly different branches of mathematics, a characteristic which makes the sub￾ject particularly demanding for the novice. An extensive mathematical toolkit is therefore
required for a thorough study of the subject. This toolkit is built-up gradually throughout
Part I of this book. However, right from the start, there is a need to have some exposure
to the vocabulary of mathematical structures that will follow. The primary objective of
this chapter is to provide such a vocabulary in a less than formal way, with more formal
treatments being detailed in subsequent chapters. A second objective is to describe and
give practice using some preliminary techniques, such as modular arithmetic, working with
Boolean connectives, and equivalence relations that will be important as you move through
the book.
We shall first consider ‘sets’. These are fundamental mathematical structures used as
building blocks to construct more advanced and useful mathematical entities. Essentially,
a set is a collection of objects called elements. Often these elements will be numbers. As
we progress through the book, we shall introduce different types of set elements such as
functions, vectors and matrices. Once we have defined sets of interest, they can be endowed
with additional properties and become yet more advanced mathematical structures, for
example groups, vector spaces and more. All of these objects are important in the study
of quantum computation. This chapter introduces terminology associated with sets. It de￾scribes several ways in which we can perform calculations, or algebra, on sets and details
laws which set algebra must obey. It also provides an informal introduction to some of the
important mathematical structures that will be required later. It then goes on to discuss
number systems and combinatorics. We shall see that a thorough knowledge of the binary
number system, which uses just the digits 0 and 1, and the various ways in which these
numbers can be manipulated is essential for an understanding of quantum computation.
Combinatorics is that branch of mathematics that deals with counting and forms the basis
of much of probability theory. These topics will be drawn on throughout the book.
Finally, the reader’s attention is drawn to several Appendices which review some basic
aspects of probability, essential trigonometric identities, Cartesian, plane polar and spherical
polar coordinate systems, and the field axioms.
DOI: 10.1201/9781003264569-1 34 Mathematical preliminaries
1.2 Definitions and notation
Definition 1.1 Set
A set is any collection of objects, states, letters, numbers, etc.
One way of describing a set is to name it and then list its members, or elements, enclosed
in braces { }. For example, we can define a set labelled A by
A = {a, b, c}.
The set A has three elements.
A set S, say, which has a finite number of elements is called a finite set. The number
of elements in any finite set is called its cardinality, written |S|.
The set of integers, written Z, which has an infinite number of elements, is given by
Z = {. . . , −3, −2, −1, 0, 1, 2, 3, . . .}.
Another way of describing a set is to use descriptive words, for example ‘N is the set of
counting numbers, 0, 1, 2, . . .’. We use the symbols ∈ and ∈/ to mean ‘is an element of’, or
‘is not an element of’, respectively, so for example 3 ∈ Z,
2
3
∈/ Z.
A third way of describing a set is to define a rule by which all members of a set can be
found. Consider the following notation:
A = {n : n ∈ Z and n ≥ 10}.
This is read as ‘A is the set of values n such that n is in the set Z (i.e., the set of integers),
and which are greater than or equal to 10’. Thus A = {10, 11, 12, 13, . . .}. Some sets of
numbers that are of particular interest in the study of quantum computation are
B = {0, 1} the set of binary digits
N = the set of natural numbers, 0, 1, 2, . . .
Z = the set of integers, . . . , −3, −2, −1, 0, 1, 2, 3, . . .
Q = the set of rational numbers,
p
q
, where p, q ∈ Z, q 6= 0
R = the set of all real numbers
R
+ = the set of all positive real numbers
C = the set of all complex numbers.
The set of real numbers R contains integers, rational numbers and irrational numbers (num￾bers that are not rational, such as π,
√
2). Most of these numbers are familiar from school
days. However, quantum computation requires a much larger set of numbers than is provided
by R alone. This necessitates the introduction of the set C of so-called complex numbers.
These are numbers which have both real and imaginary components. We study these in
detail in Chapter 3. Complex numbers are required to describe the state of a quantum
system.
The symbol \ is used to mean ‘remove from the set’ so that, for example, R\0 means all
real numbers except the value 0.Definitions and notation 5
The set of all elements of interest at any particular time is known as the universal set,
written herein as E. A set which has no elements is called an empty set, written ∅ or { }.
The set of real numbers R together with the operations of addition and multiplication is
an algebraic structure known as a field, a term which will be used repeatedly throughout
the book. Loosely speaking this means that the real numbers obey rules, known as field
axioms (see Appendix D), that enable us to add, subtract, multiply and divide by any
non-zero number, in the manner in which we are already very familiar. Similarly, the sets
of rational numbers, Q, and complex numbers, C, each form a field. Technicalities of a field
need not concern us further, and whenever we refer to a field generally, it may be helpful
for you to have the fields R or C in mind.
A rule by which we can take two elements of a set and combine them to produce another
element in the same set is called a binary operation. Familiar binary operations are
addition and multiplication in the set of real numbers, but there are many more, as we shall
see.
We can perform calculations on whole sets using the operations intersection, union,
exclusive-or and complement which are defined as follows:
Definition 1.2 Intersection
A ∩ B = {x : x ∈ A and x ∈ B}.
Thus x ∈ A ∩ B if x is in both A and B. If the intersection of A and B is the empty set,
i.e., A ∩ B = ∅, then A and B have no elements in common and are said to be disjoint.
Definition 1.3 Union
A ∪ B = {x : x ∈ A or x ∈ B or in both}.
Thus x ∈ A ∪ B if x is in A or B or in both of A and B. Technically this operation is called
an inclusive-or because it includes the possibility that x is in both sets.
Definition 1.4 Exclusive-or (xor)
A ⊕ B = {x : x ∈ A or x ∈ B but not in both}.
Thus x ∈ A ⊕ B if x is in just one of A or B.
Definition 1.5 Complement
A = {x : x ∈ E but x /∈ A}.
Thus the complement of A comprises all those elements of the universal set that are not in
A.
Two sets are said to be equal if they have precisely the same elements. If all the elements
of a set A are also elements of a set B, and provided A and B are not the same, we say that
A is a proper subset (or simply a subset) of B, written A ⊂ B. If we wish to allow the
possibility that A and B may have precisely the same elements, we write A ⊆ B. Likewise,
if the set A includes all elements of B, and provided A 6= B, then we say A is a proper
superset of B, written A ⊃ B. Allowing for the possibility of A and B to be the same, we
can write A ⊇ B. By convention, the empty set ∅ is a subset of any set A, that is ∅ ⊂ A. If
a set is non-empty, it is regarded as a subset of itself, i.e., A ⊆ A.6 Mathematical preliminaries
Exercises
1.1 Write down all the subsets of B = {0, 1}.
1.2 The set of all subsets of a given set A is called the power set of A. How many
elements are in the power set of B = {0, 1}?
1.3 A set has three elements. How many elements are in its power set?
1.4 The cardinality of a set is n. How many elements are in its power set?
1.5 Explain why the set of even integers is a subset of Z. Is the set of odd integers a
subset of Z?
1.3 Venn diagrams
Venn diagrams provide a graphical way of picturing sets. The sets are usually drawn as
circles from which various properties can be deduced. Figure 1.1 shows sets A and B, their
intersection A ∩ B, their union A ∪ B and the complement of A.
A ∩B A∪B
E E E
A B A B A B
E
A
A
FIGURE 1.1
Venn diagrams used to depict sets and set operations: intersection, union and complement.
1.4 Laws of set algebra
The operations ∩, ∪ and complement can be used to define new sets. It is possible to show
that the following laws hold when manipulating such operations (Table 1.1). In many cases
these laws are obvious from an inspection of an appropriate Venn diagram. The laws of
Boolean algebra, which we state shortly, are analogous to these. The further laws in Table
1.2 can be derived from those in Table 1.1.
1.5 Boolean algebra
Boolean algebra is concerned with the manipulation of Boolean variables. These are
variables that can take only one of two values. Examples include TRUE or FALSE, and UPBoolean algebra 7
TABLE 1.1
The laws of set algebra
A ∪ B = B ∪ A commutative laws
A ∩ B = B ∩ A
A ∪ (B ∪ C) = (A ∪ B) ∪ C associative laws
A ∩ (B ∩ C) = (A ∩ B) ∩ C
A ∩ (B ∪ C) = (A ∩ B) ∪ (A ∩ C) distributive laws
A ∪ (B ∩ C) = (A ∪ B) ∩ (A ∪ C)
A ∪ ∅ = A identity laws
A ∩ E = A
A ∪ A = E complement laws
A ∩ A = ∅
A = A
TABLE 1.2
Laws derivable from Table 1.1
A ∪ (A ∩ B) = A absorption law
A ∩ (A ∪ B) = A
(A ∩ B) ∪ (A ∩ B) = A minimisation laws
(A ∪ B) ∩ (A ∪ B) = A
A ∪ B = A ∩ B De Morgan’s laws
A ∩ B = A ∪ B
or DOWN, and ON or OFF. If A is a binary digit, that is it can take the values 0 or 1, then
A is a Boolean variable. Given two or more Boolean variables, A, B, C, . . . we can combine
them to produce Boolean expressions using binary operations called logical connectives.
These are defined in Chapter 2 when we explain Boolean logic gates. For now it suffices
to say that the logical connectives and (∧), or (∨) and not ( ) can be manipulated with
laws directly analogous to the laws of set algebra if we interpret ∨ as ∪, ∧ as ∩, 1 as the
universal set and 0 as the empty set. Thus we have the laws in Table 1.3. Further laws given
in Table 1.4 can be derived from those in Table 1.3. In Section 1.5.1 we discuss another
logical connective, the ‘exclusive-or’, denoted ⊕.
TABLE 1.3
The laws of Boolean algebra
A ∨ B = B ∨ A commutative laws
A ∧ B = B ∧ A
A ∨ (B ∨ C) = (A ∨ B) ∨ C associative laws
A ∧ (B ∧ C) = (A ∧ B) ∧ C
A ∧ (B ∨ C) = (A ∧ B) ∨ (A ∧ C) distributive laws
A ∨ (B ∧ C) = (A ∨ B) ∧ (A ∨ C)
A ∨ 0 = A identity laws
A ∧ 1 = A
A ∨ A = 1 complement laws
A ∧ A = 0
A = A8 Mathematical preliminaries
TABLE 1.4
Laws derivable from Table 1.3
A ∨ (A ∧ B) = A absorption law
A ∧ (A ∨ B) = A
(A ∧ B) ∨ (A ∧ B) = A minimisation laws
(A ∨ B) ∧ (A ∨ B) = A
A ∨ B = A ∧ B De Morgan’s laws
A ∧ B = A ∨ B
A ∨ 1 = 1
A ∧ 0 = 0
1.5.1 The exclusive-or operator (xor)
We now focus on the exclusive-or operator, written ⊕, which is applied to a pair of Boolean
variables. This binary operation is immensely important in defining Boolean functions and
in all quantum algorithms. Thus, familiarity with its manipulation is essential.
Definition 1.6 Exclusive-or operator ⊕
The exclusive-or operator, written ⊕ and also referred to as xor, is defined on the Boolean
variables x and y so that x ⊕ y = 1 whenever x or y (but not both) equals 1, and is zero
otherwise (Table 1.5).
TABLE 1.5
Definition of ⊕
0 ⊕ 0 = 0
0 ⊕ 1 = 1
1 ⊕ 0 = 1
1 ⊕ 1 = 0
Example 1.5.1
Show that if x ∈ B then:
(a) if x = 0, 1 ⊕ x = 1.
(b) if x = 1, 1 ⊕ x = 0.
Solution
(a) if x = 0, we see directly from Table 1.5 that 1 ⊕ x = 1 ⊕ 0 = 1.
(b) if x = 1, then 1 ⊕ x = 1 ⊕ 1 = 0, as required. Expressions of the form 1 ⊕ x will occur
frequently in the quantum algorithms in Chapter 22. It is important to note that whatever
the value of x ∈ B, 1 ⊕ x gives the other value.
Example 1.5.2
Show that if x ∈ B then x ⊕ x = 0.Groups 9
Solution
Given that x ∈ B = {0, 1} then either x = 0 or x = 1.
Suppose x = 0. Then x ⊕ x = 0 ⊕ 0 = 0.
Suppose x = 1. Then x ⊕ x = 1 ⊕ 1 = 0.
Thus, whatever x, x ⊕ x = 0.
Results such as this will be used repeatedly when simplifying Boolean expressions used in
quantum gates. For example, it follows immediately that, for x1, x2 ∈ B
(x1 ∧ x2) ⊕ (x1 ∧ x2) = 0.
Exercises
1.6 Let x ∈ B = {0, 1}. Let s ∈ B = {0, 1}.
(a) Show that x ⊕ x = 0.
(b) Show that x ⊕ 0 = x.
(c) Show that x ⊕ s ⊕ s = x.
(d) Show that x ⊕ x ⊕ s = s.
1.6 Groups
The mathematical structure known as a group pervades quantum computation. Groups
are built using a set, G say, and a binary operation, ◦ say, used to combine elements of that
set. In practice, the binary operation will often be +, × or ⊕.
Definition 1.7 Group
A group is a set G with a binary operation which assigns to each pair of elements, a, b ∈ G,
another element a ◦ b which is also in G for which the following group axioms hold:
1. given a, b ∈ G, then a ◦ b ∈ G (closure)
2. for any a, b, c ∈ G then (a ◦ b) ◦ c = a ◦ (b ◦ c) (associativity)
3. there exists an identity or unit element e ∈ G such that e ◦ a = a ◦ e = a for
all a ∈ G.
4. for any a ∈ G there exists an element a
−1 also in G such that a ◦ a
−1 =
a
−1 ◦ a = e. The element a
−1
is called the inverse of a.
We will usually denote a group by (G, ◦). On occasions the group operation will be replaced
by juxtaposition of elements, for example writing a ◦ b as simply ab. This is particularly so
(but is not restricted to the case) when the binary operation is multiplication. Note that
a
−1 does not mean the reciprocal of a nor a power of a. It is purely a notation for the
inverse element of the group.
Many familiar sets and binary operations can be readily identified as groups. Consider
the following examples.10 Mathematical preliminaries
Example 1.6.1 The group (Z, +)
Consider the set of integers Z with the operation of addition, +. Clearly, adding any two
integers results in another integer, so Z is closed under addition. Associativity is obvious:
p+ (q +r) = (p +q) +r for any p, q, r ∈ Z. The identity element is 0 because adding zero to
any integer does not change it. Finally, the inverse of any integer p is −p since p+ (−p) = 0,
and −p ∈ Z. Hence the structure (Z, +) is a group.
Definition 1.8 Commutative or Abelian Group
If (G, ◦) is a group, and additionally the operation ◦ is commutative, that is a ◦ b = b ◦ a
for all a, b ∈ G, then (G, ◦) is said to be a commutative group or an Abelian group.
Clearly, because addition of integers is commutative (Z, +) is Abelian.
Definition 1.9 Subgroup
If (G, ◦) is a group, then if a subset H of G is itself a group with the same operation it is
called a subgroup, written H ≤ G.
It is straightforward to check that the set of even integers form a subgroup of (Z, +).
Exercises
1.7 Explain why the set of odd integers does not form a subgroup of (Z, +).
The following examples illustrate some more groups. Further group theory follows in Chap￾ter 8.
Example 1.6.2 The permutation group P3
Consider the set of permutations or arrangements of three objects a, b, c. These are
listed:
abc, acb, cba, bac, cab, bca.
To study these further, we need to agree a convention to keep track of changes in the
ordering. There are several ways of doing this, but suppose we let P12 indicate the operation
‘interchange the first and second letters’. Then P12 changes abc to bac, written
P12 : abc → bac.
Similarly, P12 : cab → acb and so on.
Permutations can be applied repeatedly. The so-called composition of permutations
P23P12 will be interpreted as applying P12 first followed by applying P23. Thus
P23P12 : abc → bac → bca.
Observe that the permutation bca cannot be obtained from abc by a single, simple inter￾change of two elements, but can be achieved introducing a cyclic rotation operation so that
the first element moves to the third position, the third to the second and the second to the
first, which we shall denote by P132 (the 132 indicating 1 → 3 → 2 → 1). Thus
P132 : abc → bca.Groups 11
Similarly, the operation P123 moves the first element to the second position, the second to
the third and the third to the first:
P123 : abc → cab.
The operation labelled ‘1’ defined as 1 : abc → abc shall mean leave all elements alone. Thus
there are six operations required to achieve all possible permutations:
1, P12, P13, P23, P132, P123.
The six operations thus defined can be applied repeatedly as in the example P23P12 above.
In doing so, we shall always apply the rightmost operation first. The result of combining
operations in this way can be placed in a so-called Cayley table (Table 1.6). When reading
the table, it is important to note that the operation in the header row is applied first.
Observe, for example, that P23P12 = P132. It is straightforward to check that the set of
permutations of abc with the operation of composition as defined above satisfies all the
axioms required to form a group. The identity element of the group is 1. This group is
referred to as the permutation group P3. Permutations of more than three objects can
be used to define further permutation groups, P4, P5, . . . in a similar fashion.
TABLE 1.6
Cayley table for the permutation group P3
1 P12 P13 P23 P132 P123
1 1 P12 P13 P23 P132 P123
P12 P12 1 P132 P123 P13 P23
P13 P13 P123 1 P132 P23 P12
P23 P23 P132 P123 1 P12 P13
P132 P132 P23 P12 P13 P123 1
P123 P123 P13 P23 P12 1 P132
Exercises
1.8 Referring to Example 1.6.2 use the Cayley table to write down the inverse of each
of the elements of the group. Which elements are self-inverse ?
In the following example we shall meet another group which although superficially looks
quite different has a Cayley table with an identical structure to the permutation group of
Example 1.6.2. This leads to the concept of isomorphism which is a way of comparing
apparently dissimilar groups. Permutations can also be regarded as functions. This aspect
is discussed in Chapter 2.
Example 1.6.3 The symmetric group S3
Consider the various symmetry operations on the equilateral triangle shown in Figure 1.2
which result in the triangle being in the same position albeit with the vertex labels altered.
So, for example the operation ‘reflect the triangle in the line L1’ is denoted by S12. The
operation S123 is a clockwise rotation through 2π/3. The operation S132 is an anti-clockwise
rotation through 2π/3. We can apply these symmetry operations sequentially and the results12 Mathematical preliminaries
a
b
c b c a
b
c
a b
c
b a
c a
c b
a
1 S12 S13
S23 S132 S123
L1
L2
L1
L2
L3
L3
FIGURE 1.2
The symmetric group S3: symmetries of an equilateral triangle.
of doing this are given in Table 1.7. Note when reading the table, the operation in the header
row is applied first. Observe that the structure of the Cayley table for S3 is identical to
that of the permutation group P3 in Example 1.6.2. We say that there is an isomorphism
between P3 and S3, that is there is a one-to-one correspondence between the elements of
the two sets, which preserves the structure of the group, i.e., P3 and S3 are isomorphic.
TABLE 1.7
Cayley table for the symmetric group S3
1 S12 S13 S23 S132 S123
1 1 S12 S13 S23 S132 S123
S12 S12 1 S132 S123 S13 S23
S13 S13 S123 1 S132 S23 S12
S23 S23 S132 S123 1 S12 S13
S132 S132 S23 S12 S13 S123 1
S123 S123 S13 S23 S12 1 S132
1.7 Cartesian product
Definition 1.10 Cartesian product
The Cartesian product of two sets A and B, written A×B, is the set of all ordered pairs
of elements (a, b) such that a ∈ A and b ∈ B.
The term ‘ordered pair’ indicates that the order in which a and b appear is important, so
that generally (a, b) is distinct from (b, a) and here a is chosen from the set A, whilst b is
chosen from the set B. For example, given
A = {1, 2, 3}, B = {d, e}
then
A × B = {(1, d),(1, e),(2, d),(2, e),(3, d),(3, e)}.Cartesian product 13
If A and B are finite sets with cardinalities n and m, respectively, then the cardinality of
A × B is nm. Clearly the cardinality of A × B above is 3 × 2 = 6. The two sets involved
can be the same. Consider the following example.
Example 1.7.1 The Cartesian product B × B = B
2
For the set B = {0, 1} write down the set B × B.
Solution
The set B × B consists of all ordered pairs of elements (a, b) where a ∈ B and b ∈ B:
B × B = {(0, 0),(0, 1),(1, 0),(1, 1)}.
B × B is also written as B
2
. Observe that the cardinality of B
2
is 2 × 2 = 4.
Example 1.7.2 Ordered n-tuples
We can produce the Cartesian product of more than two sets, A1, A2, . . . , An, in a natural
way to give an ordered n-tuple:
a = (a1, a2, . . . , an)
where ai ∈ Ai
. The set B
n consists of all ordered n-tuples of the Boolean variables 0 and 1.
We shall sometimes write this set as
{0, 1}
n
elements of which include, for example, the n-tuples
(0, 1, 0, 1, . . . , 0), (1, 1, 1, 1, . . . , 0), (0, 0, 0, 0, . . . , 1).
Sometimes these n-tuples are written as binary strings, that is, strings of binary digits:
0101 . . . 0, 1111 . . . 0 and so on. An n-tuple of zeros, (0, 0, 0, . . . , 0), or 000 . . . 0 will also be
written concisely as 0n.
As a further example, the set {0, 1}
3
is given by
{0, 1}
3 = {000, 001, 010, 011, 100, 101, 110, 111}.
We shall see that n-tuples {0, 1}
n arise frequently in quantum algorithms.
Example 1.7.3 The set R × R × R = R
3
The set R×R×R, written R
3
is the set of all ordered triples (x, y, z) of real numbers. Later
we shall see that such an ordered triple can be thought of as an object called a row vector
or column vector in R
3
, that is,
(x, y, z) or


x
y
z

 .
Collections of vectors like these, when they are provided with some specific rules for addition
and multiplication by real numbers are called vector spaces over the field R, as detailed
in Chapter 6.14 Mathematical preliminaries
Example 1.7.4 The set C × C = C
2
The set C × C = C
2
is the set of all ordered pairs of complex numbers. (For those readers
unfamiliar with complex numbers, details are given in Chapter 3. A complex number takes
the form a + bi, with a, b ∈ R and the imaginary unit i is defined such that i2 = −1.)
Depending upon the context, we might choose to write elements of this set as row or
column vectors. Thus the following are both elements of C
2
:
(3 + 4i, −11 + 2i),

2 − 5i
1 + 3i 
.
Collections of vectors of complex numbers, when provided with specific rules for addition
and multiplication by complex numbers, are called vector spaces over the field C. We
shall see that such vectors are used to represent the state of a quantum system.
Example 1.7.5
A matrix (plural, matrices) is a mathematical object built from rows of row vectors, or
columns of column vectors. So

4 −3 2
2 2 −1

,


1 0 0
0 1 0
0 0 1

 ,

4 + i 7 + 2i
2 − 6i 1 − i

are examples of matrices. Sets of matrices endowed with additional rules for addition and
multiplication by a real or complex number become vector spaces. These spaces are crucial
for modelling evolution of quantum states with time, and for modelling the measurement
of observables. Matrices are studied in detail in Chapters 5 and 7.
Exercises
1.9 What is the cardinality of the set B
2
?
1.10 What is the cardinality of the set B
n?
1.8 Number bases
The decimal number system, known as base 10, uses the ten digits 0, 1, 2, . . . , 8, 9. How￾ever, applications in computer science and engineering make use of other number systems,
particularly in bases 2, 8 and 16. Such number systems are referred to as binary, octal
and hexadecimal, respectively. The binary system, with 2 as its base, uses only the two
digits 0 and 1. These are called binary digits or bits. So a bit takes its value from the set
B = {0, 1}. Consider, for example, the binary number 11012 where the subscript 2 indicates
the base. This means that as we move from the right to the left, the position of each digit
represents an increasing power of 2. So,
11012 = 1(23
) + 1(22
) + 0(21
) + 1(20
)
= 8 + 4 + 0 + 1
= 1310.Modular arithmetic 15
A digital register is a physical entity capable of representing, at any given time, a binary
number. Whereas a 1 bit digital register is capable of representing just one of the numbers
0 or 1, a 2-bit register can represent 00, 01, 10 and 11, that is any element of {0, 1}
2
.
Likewise, an n-bit register is capable of representing any one of the 2n binary numbers
000 · · · 0, . . . , 111 · · · 1 in {0, 1}
n.
Example 1.8.1
When performing calculations on quantum states, we shall frequently come across objects
of the form
|10i |101i |10011i or |11110i
where strings of binary digits are parenthesised by | i. To avoid writing lengthy strings
of zeros and ones, we sometimes express these binary strings of digits as their decimal
equivalents. With this convention, you should confirm that
|10i = |2i, |101i = |5i |10011i = |19i and |11110i = |30i.
1.9 Modular arithmetic
12
3
6
9
13 = 1 (mod 12)
1
2
4
7 5
8
10
11
FIGURE 1.3
Counting modulo 12.
Modular arithmetic is a method of counting with integers, . . . , −3, −2, −1, 0, 1, 2, 3, . . .,
where instead of this sequence continuing forever, it ‘wraps around’ once a particular value,
called the modulus, is reached. Probably the most familiar example is arithmetic modulo
12, used in an everyday clock. When counting hours beyond 12 o’clock, it is usual to start
again at 1, so that 13 looks to be the same time as 1. (Figure 1.3.) We say that 13 is
congruent to 1 modulo 12 and write this as
13 ≡ 1 (mod 12).
This is equivalent to saying that
13 − 1 is divisible by 12.
Similarly 15 ≡ 3 (mod 12) since 15 − 3 is divisible by 12. It is usual to regard 12, as 0, so
that when working in mod 12, we use the digits 0, 1, 2, . . . , 11. More generally, we have the
following definition:
Definition 1.11 Congruence
We say that a is congruent to b modulo n, written a ≡ b (mod n), if
a − b is divisible by n, or a − b = kn for some k ∈ Z.16 Mathematical preliminaries
Note that ‘is congruent to’ is a so-called equivalence relation as we shall show in Section
1.10.
Example 1.9.1 Addition modulo 2 and the exclusive-or operation ⊕.
We shall see that use of addition modulo 2 is commonplace in quantum computation.
Working in modulo 2 we use only the digits 0 and 1, and hence there is an immediate
relationship between binary arithmetic and counting modulo 2. We can visualise this in
Figure 1.4. Observe that 2 ≡ 0 (mod 2) and consequently that 1 + 1 = 0 (mod 2).
 0
1
...,-5,-3,-1,3,5,....
...,-4,-2,2,4,....
FIGURE 1.4
Counting modulo 2.
We can represent the results of addition modulo 2 in Table 1.8. The + (mod 2) operation
is particularly important in quantum computation.
TABLE 1.8
Addition modulo
2
+ or ⊕ 0 1
0 0 1
1 1 0
It is equivalent to the exclusive-or operation, denoted ⊕, on Boolean variables. Recall that
the exclusive-or takes the value 1 when either, but not both, of the operands equals 1 and
is 0 otherwise. Thus
if a, b ∈ B then a ⊕ b = a + b (mod 2).
Example 1.9.2
We can extend the addition process in Example 1.9.1:
if xi ∈ B then x1 ⊕ x2 ⊕ . . . ⊕ xk = x1 + x2 + . . . + xk (mod 2).
For example,
1 ⊕ 0 ⊕ 1 ⊕ 1 ⊕ 1 ⊕ 1 = 1 + 0 + 1 + 1 + 1 + 1
= 5
= 1(mod 2).Modular arithmetic 17
TABLE 1.9
Multiplication modulo 2
× 0 1
0 0 0
1 0 1
Example 1.9.3 Multiplication modulo 2
Multiplication modulo 2 is used in some quantum algorithms. Table 1.9 defines this. Mul￾tiplication modulo 2 is equivalent to the and operation ∧ on Boolean variables which is
defined such that
0 ∧ 0 = 0, 0 ∧ 1 = 0, 1 ∧ 0 = 0, 1 ∧ 1 = 1.
Observe that the result of calculating x ∧ y when x, y ∈ B, is 1 if and only if x = y = 1.
Example 1.9.4
Consider the expression
(0 ∧ s1) ⊕ (1 ∧ s2) where s1, s2 ∈ B.
Find the values of s1 and s2 such that this expression equals zero.
Solution
Table 1.10 shows the possible combinations of values of s1 and s2 and the results of calcu￾lating (0 ∧ s1), (1 ∧ s2) and (0 ∧ s1) ⊕ (1 ∧ s2).
TABLE 1.10
Table for Example 1.9.4
s1 s2 0 ∧ s1 1 ∧ s2 (0 ∧ s1) ⊕ (1 ∧ s2)
0 0 0 0 0
0 1 0 1 1
1 0 0 0 0
1 1 0 1 1
Observe from the table that the only combinations of s1 and s2 resulting in 0 are s1 = s2 = 0
and s1 = 1, s2 = 0. When we study quantum algorithms in Chapter 23, we shall need to
solve equations such as
(0 ∧ s1) ⊕ (1 ∧ s2) = 0.
Exercises
1.11 Consider the expression
(1 ∧ s1) ⊕ (1 ∧ s2) where s1, s2 ∈ B.
Find the values of s1 and s2 such that this expression equals zero.18 Mathematical preliminaries
Example 1.9.5 Addition modulo 2: the bit-wise sum
For x ∈ B
k and y ∈ B
k
, where x = (x1, x2, . . . , xk), y = (y1, y2, . . . , yk), we define the
bit-wise ⊕ operator as
x ⊕ y = (x1 ⊕ y1, x2 ⊕ y2, . . . , xk ⊕ yk).
This is just the exclusive-or operator applied to corresponding components. For example,
suppose
x = (1, 0, 1, 1, 1, 1)
y = (1, 1, 0, 0, 1, 1)
then
x ⊕ y = (1 ⊕ 1, 0 ⊕ 1, 1 ⊕ 0, 1 ⊕ 0, 1 ⊕ 1, 1 ⊕ 1)
= (0, 1, 1, 1, 0, 0).
This operation may be seen written vertically: consider the next example.
Example 1.9.6 Addition modulo 2: the bit-wise sum
Find 111 ⊕ 101 where ⊕ represents bit-wise sum.
Solution
Corresponding bits are added modulo 2:
111 ⊕
101
010
(Finally, note that the bit-wise sum is not the same as addition modulo 2 when carry digits
are used).
Exercises
1.12 Let x ∈ B
2 = {0, 1}
2
. Let s ∈ B
2 = {0, 1}
2
. Suppose x = (1, 1) ≡ 11. Suppose
s = (1, 0) ≡ 10. Verify the following:
(a) Show that x ⊕ x = 00,
(b) Show that x ⊕ 0 = x (here 0 ≡ 00 ≡ (0, 0)),
(c) Show that x ⊕ s ⊕ s = x,
(d) Show that x ⊕ x ⊕ s = s.
Example 1.9.7 The additive group of integers modulo 2
Consider addition modulo 2 of the integers in the set {0, 1}. Table 1.8 shows this addition
rule. It is straightforward to show that this set with the operation of addition modulo 2
satisfies the axioms for a group. This group is often written as (Z/2Z, +) or simply Z/2Z,
and is referred to as the additive group of integers modulo 2.Relations, equivalence relations and equivalence classes 19
Exercises
1.13 State the identity element of the group (Z/2Z, +).
1.14 Produce a group table for (Z/3Z, +). What is the identity element in this group?
Example 1.9.8 The multiplicative group of integers modulo 8
Table 1.11 shows a multiplication rule for elements in the set {1, 3, 5, 7} modulo 8. It is
TABLE 1.11
Cayley table for
the group
(Z/8Z)
×
× 1 3 5 7
1 1 3 5 7
3 3 1 7 5
5 5 7 1 3
7 7 5 3 1
straightforward to show that this set with the operation of multiplication modulo 8 satisfies
the rules for a group. This group is often written (Z/8Z)
× and is referred to as the mul￾tiplicative group of integers modulo 8. The elements of (Z/8Z)
× are those integers in the
interval 1 ≤ n ≤ 7 which are coprime to 8, that is, their only common factor with 8 is 1.
1.10 Relations, equivalence relations and equivalence classes
Given sets A and B, we can define relationships between elements in the two sets. Specif￾ically, we consider the ordered pair (a, b) with a ∈ A, b ∈ B, i.e., (a, b) ∈ A × B, and ask
whether or not a is related to b.
Definition 1.12 Relation
For the ordered pair (a, b) ∈ A × B, a relation R is such that either
1. a is related to b, written a R b, or
2. a is not related to b, written a 6R b.
Consider the following example.
Example 1.10.1
The sets A and B are defined as A = {2, 3, 5}, B = {3, 6, 7}. For a ∈ A and b ∈ B, we shall
say a is related to b if a is a factor of b. The statement ‘is a factor of’ is the relation, R,
say. We can then write expressions such as 3 R 6 because 3 is a factor of 6. Likewise 5 6R 3
since 5 is not a factor of 3. We can imagine this relation as shown in Figure 1.5.20 Mathematical preliminaries
2 3
5 7
3 6
A B
FIGURE 1.5
The arrows show the relation R: ‘is a factor of’, between elements of two sets.
It is also helpful to think of the relation as a rule which sends, via the arrows, (some)
elements of A, the ‘input’, to (some) elements of B, the ‘output’. Note that not all elements
of A and B are necessarily involved. We can also think of a relation as defining a subset of
the Cartesian product A × B. In this case the subset, Rˆ say, is {(2, 6),(3, 3),(3, 6)}.
Example 1.10.2
Consider a set A and the ordered pair (a1, a2), with a1, a2 ∈ A, which is an element of the
Cartesian product A×A. There are many ways that a1 and a2 may, or may not, be related.
Suppose, for example, that A is the set of integers between 1 and 4. The Cartesian product
A × A is then the set of ordered pairs
{(1, 1),(1, 2),(1, 3),(1, 4),(2, 1),(2, 2),(2, 3),(2, 4),
(3, 1),(3, 2),(3, 3),(3, 4),(4, 1),(4, 2),(4, 3),(4, 4)}.
We consider possible relationships within each ordered pair, (a1, a2). We can ask, for exam￾ple, whether a1 is a multiple of a2. So, for the element (4, 2) it is clearly the case that the
first element is a multiple of the second. The statement ‘is a multiple of’ is a relation, R,
say. Having defined the relation R, we can write 4 R 2. Clearly, for the element (1, 3), the
first element is not a multiple of the second and so 1 6R 3. We can imagine the relation R as
shown in Figure 1.6.
2
1
3
4
2
1
3
4
FIGURE 1.6
The arrows show the relation R: ‘is a multiple of’, between elements of two sets.
With any given relation, it is the case that a1 R a2 or a1 6R a2, that is, any two elements are
either related or they are not. The relation R defines a subset, Rˆ say, of A × A:
Rˆ = {(a1, a2) : a1 R a2}.
For the case above, it is straightforward to check that
Rˆ = {(1, 1),(2, 1),(2, 2),(3, 1),(3, 3),(4, 1),(4, 2),(4, 4)}
which is a subset of the Cartesian product A × A.Relations, equivalence relations and equivalence classes 21
In the study of functions (Chapter 2), relations which will become particularly relevant are
‘one-to-one’ and ‘two-to-one’ relations defined on the Cartesian product A × B, such as
those depicted in Figure 1.7. We shall see in Section 1.11 that one-to-one relations on A×A
represent arrangements or permutations of the elements of the set A.
A B A B
FIGURE 1.7
A one-to-one and a two-to-one relation.
1.10.1 Equivalence relations
A particular type of relation known as an equivalence relation is significant in quantum
computation.
Definition 1.13 Equivalence relation
A relation defined on the Cartesian product A × A is called an equivalence relation,
denoted ∼ say, if it satisfies the following:
1. a ∼ a for every a ∈ A (referred to as reflexivity)
2. if a ∼ b then b ∼ a (referred to as symmetry)
3. if a ∼ b and b ∼ c then a ∼ c, (referred to as transitivity)
Definition 1.14 Equivalence class
Given an equivalence relation, ∼, the equivalence class of any element a ∈ A, written [a],
is the set of all elements in A to which a is related:
[a] = {b ∈ A : a ∼ b}
The set of equivalence classes is denoted by A/ ∼.
Example 1.10.3 The integers modulo 4, (Z/4Z)
In this example we consider the integers modulo 4, as depicted in Figure 1.8.
Working in modulo 4 recall that we use the integers 0, 1, 2, 3 and then below 0 and
above 3 we ‘wrap around’ as indicated. Consider the congruence relation
a ≡ b (mod 4).
Recall that a ≡ b (mod 4) if a − b is divisible by 4. Inspection of Figure 1.8 readily reveals
that
6 ≡ 2 (mod 4), 17 ≡ 9 (mod 4)
and so on. Equivalently, 6 − 2 is divisible by 4, and 17 − 9 likewise. To confirm that this is
indeed an equivalence relation, note that
a ≡ a mod 4 since a − a = 0 is divisible by 4 (reflexivity)22 Mathematical preliminaries
 4
1
2
3 ...,-7,-3,5,9,13,17,....
...,-6,-2,6,10,14,18,....
...,-5,-1,7,11,15,19,....
...,-4,0,8,12,16,20,....
FIGURE 1.8
Integers modulo 4.
Also, if a − b is divisible by 4, then b − a = −(a − b) is divisible by 4 (symmetry). Finally,
if a − b and b − c are both divisible by 4, then we can write
a − b = 4k, b − c = 4`, for some k, ` ∈ Z.
Adding these two expressions:
a − c = 4(k + `)
which is divisible by 4 since k + ` ∈ Z. This confirms transitivity, and hence, the relation
‘is congruent to, mod 4’ is an equivalence relation. Observe that the relation divides the
integers into equivalence classes as is clearly illustrated in Figure 1.8.
[0] = . . . , −4, 0, 4, 8, 12, 16, . . . [1] = . . . , −3, 1, 5, 9, 13, 17, . . .
[2] = . . . , −2, 2, 6, 10, 14, 18, . . . [3] = . . . , −1, 3, 7, 11, 15, . . .
The set of equivalence classes is often written Z/4Z. Every integer is in one and only
one equivalence class. We say that the equivalence relation partitions the integers into
equivalence classes. So equivalence classes are distinct.
Example 1.10.4
If ∼ is the equivalence relation a ≡ b (mod 12), then the equivalence classes are:
[0] = {. . . , 0, 12, 24, . . .}, [1] = {. . . , 1, 13, 25, . . .},
[2] = {. . . , 2, 14, 26, . . .}, [3] = {. . . , 3, 15, 27, . . .}, and so on.
Definition 1.15 Equivalence classes and the partition of a set
Given an equivalence relation ∼ on A×A, denote the equivalence class of an element a ∈ A
by [a]. Then for any b ∈ A, either [a] ∩ [b] = ∅ or [a] = [b]. So equivalence classes are
distinct entities and the equivalence relation can be thought of as ‘dividing up’ the set into
these distinct classes, hence the notation A/ ∼.
Example 1.10.5 Equivalence classes and a projective space
Consider the xy plane, R
2
, in Figure 1.9. We can define a relation, ∼, between two points
(x, y) and (x
∗
, y∗
) in the plane by
(x, y) ∼ (x
∗
, y∗
) if the points lie on the same straight line through the origin.Relations, equivalence relations and equivalence classes 23
x
y
(x, y)
(x
∗
, y
∗
)
R
2
FIGURE 1.9
(x, y) and (x
∗
, y∗
) lie on the same line through the origin.
The relation ∼ is not an equivalence relation (see the Exercises below). We now remove the
origin and consider the set R
2 \ {(0, 0)}. It is straightforward to show that, with this restric￾tion, ∼ is an equivalence relation. The set of equivalence classes, denoted R
2 \ {(0, 0)}/ ∼,
in this context often referred to as a projective space, is the set of straight lines through
the origin. Any two points are regarded as equivalent if they lie on the same straight line
through the origin. We shall refer to the lines as rays in Chapter 13. Points on the same
line, that is on the same ray, are in the same equivalence class (Figure 1.10).
x
y
R
2
\ {(0, 0)}
FIGURE 1.10
Points • on the same line or same ray are in the same equivalence class.
Exercises
1.15 Consider the set R
2
(i.e., without removal of the origin). Show that the relation
(x, y) ∼ (x
∗
, y∗
) if the points lie on the same straight line through the origin, is
not an equivalence relation because the transitivity requirement fails.
1.16 Consider the set R
2 \ {(0, 0)} and the equivalence relation: (x, y) ∼ (x
∗
, y∗
) if the
points lie on the same straight line through the origin. Show that this is equivalent
to writing (x, y) ∼ (x
∗
, y∗
) if and only if (x
∗
, y∗
) = λ(x, y) for some λ ∈ R \ {0}.24 Mathematical preliminaries
1.11 Combinatorics – permutations and combinations
Combinatorics is that branch of mathematics concerned with counting.
Definition 1.16 Permutation
A permutation of n distinct objects is an arrangement of those objects.
For example, with two objects, a and b, there are just two permutations
ab, ba.
With three distinct objects, a, b and c, there are six permutations
abc, acb, bac, bca, cab, cba.
More generally, there are n! permutations of n distinct objects.
Example 1.11.1
Consider the set of strings of two digit binary numbers B
2 = {00, 01, 10, 11} and the one￾to-one relation shown in Figure 1.11.
00
01
10
11
00
01
10
11
B
2 B
2
FIGURE 1.11
A relation as a permutation of binary strings.
We can think of this relation as a permutation of the set of four objects {00, 01, 10, 11},
specifically the permutation 00, 01, 11, 10. We have seen that there are n! permutations of
n distinct objects and consequently there 4! = 24 different permutations of the elements of
set B
2
. Calculations such as this will be required when we meet functions defined on B
2
in
Chapter 2.
If only r of the n distinct objects are selected for arrangement, there are n!
(n − r)! permu￾tations. This is often written nPr.
For example, given the objects a, b, c above, if we select just two at a time, there are
3P2 =
3!
(3 − 2)! = 6 permutations:
ab, ba, ac, ca, bc, cb.
On the other hand, given four distinct objects, which have 4! = 24 permutations, taking
just two at a time, gives
4!
(4 − 2)! = 12 permutations.
Suppose now that not all the objects are distinct. If out of the n objects, n1 are identical
and of type 1, n2 are identical and of type 2, and so on, the number of permutations using
all n objects are n!
n1!n2! . . . nk!
.End-of-chapter exercises 25
Combinations are closely related to permutations.
Definition 1.17 Combination
A combination is a selection of r objects from n distinct objects. When making the selec￾tion, the order in which we write the objects down is of no consequence.
For example, the combination ab is the same as the combination ba. The number of combi￾nations is often written nCr, or sometimes 
n
r

and is given by n!
(n − r)!r!
.
1.12 End-of-chapter exercises
1. What is the cardinality of the power set of the set B
n?
2. By drawing Venn diagrams, verify De Morgan’s laws:
A ∩ B = A ∪ B, A ∪ B = A ∩ B.
3. Use the laws of set algebra to simplify the following expressions:
(a) (A ∩ B) ∪ (A ∩ B)
(b) A ∪ (A ∩ B).
4. Consider the set Z and the equivalence relation R : a ≡ b (mod 6). Write down
an expression for each of the equivalence classes. Confirm that for x, y ∈ Z with
x 6= y, [x] ∩ [y] = ∅ or [x] = [y].
5. Consider the set of all triangles in the plane. Two triangles are similar if their
corresponding angles are the same. Show that ‘is similar to’ is an equivalence
relation.
6. Consider the set of points in R
2 and the relation (x, y) ∼ (x
∗
, y∗
) if and only if
y − x = y
∗ − x
∗
. Show that ∼ is an equivalence relation and give a geometric
interpretation of the equivalence classes.
7. Consider the set B = {0, 1}.
(a) Performing addition modulo 2, construct an addition table.
(b) Performing multiplication modulo 2, construct a multiplication table.
(c) State the additive identity element.
(d) State the multiplicative identity element.
(e) Show that the set B = {0, 1} with the above operations + and × satisfies
the axioms of a field (see Appendix D). This is an example of a finite field
often denoted F2 or GF(2).
8. Let x ∈ B
n = {0, 1}
n. Let s ∈ B
n = {0, 1}
n.
(a) Show that x ⊕ x = 0.
(b) Show that x ⊕ 0 = x.
(c) Show that x ⊕ s ⊕ s = x.
(d) Show that x ⊕ x ⊕ s = s.26 Mathematical preliminaries
9. Let x, y ∈ B.
(a) Show that
x ⊕ y = (x ∨ y) ∧ (x ∨ y).
(b) Show that
x ⊕ y = (x ∧ y) ∨ (x ∧ y).
10. Given an ordered n-tuple a ∈ B
n where a = (a1, a2, . . . , an) and ai ∈ B, then the
Hamming weight of a is defined to be the number of 1’s in a. Show that this
is equivalent to adding the terms of a. That is
Hamming weight of a =
Xn
i=1
ai
Calculate the Hamming weight of a ∈ B
6 when a = (1, 0, 1, 1, 1, 1).2
Functions and their application to digital gates
2.1 Objectives
A function is a special type of relation between elements of two sets. The first set can
be thought of as providing the input to the function, and the second set as providing the
output. In this sense, a function is used to process data.
When we refer to a digital computer’s ‘state’ at any time t, we are referring to the digital
data in its registers. A program moves the computer through a sequence of states, these
changes being realised through the application of classical Boolean gates. Mathematically,
Boolean gates are functions which act on input data, i.e., strings of bits, to produce output
data, also strings of bits, and in doing so the state of the system evolves.
The objective of this chapter is to introduce terminology and notation associated with
functions. We show how functions can be represented with pictures, tables and mathematical
formulae. Functions defined on the set {0, 1} are of particular relevance in quantum com￾putation so these are our primary focus. We make particular reference to so-called bijective
functions and inverse functions. These play a central role in the description of reversible
digital gates and also quantum gates which are used to process quantum information.
2.2 Introductory definitions and terminology
Definition 2.1 Function, or mapping
Consider two sets A and B. A function, or mapping, f say, is a relation or a rule which
assigns to each element of A just one element of B.
Here we have labelled the function f, but other letters, including Greek characters, e.g., π,
σ, will also be used. Schematically we can represent a function as shown in the example of
Figure 2.1 in which the function f maps the element a1 to b3, a2 to b4 and so on.
a
A B
f

a
a
b
b
b
b
b
FIGURE 2.1
Two sets and the function f : A → B. Each element of A maps to just one element in B.
DOI: 10.1201/9781003264569-2 2728 Functions and their application to digital gates
We write f : A → B, and f(a1) = b3, f(a2) = b4 and so on. We refer to a1, a2, a3 as inputs
to, or arguments of, the function, and b3, b4 as outputs. The set A is called the domain
of the function. The set B is called the co-domain. The set of all the elements in B which
are mapped to by the function is called the range of the function. In this case the range,
i.e., the set of outputs, is {b3, b4}. Note that the range is not necessarily the same as the
co-domain as not all elements of B need to be mapped onto.
Definition 2.2 Onto or surjective function
If all the elements of the co-domain of a function are mapped onto by some element in the
domain, then the function is said to be onto or surjective.
Note that some elements of A might map to the same element of B as occurs in Figure 2.1.
Definition 2.3 One-to-one or injective function
If each element in the range is mapped to by just one element in the domain, then the
function is said to be one-to-one or injective. A function which is not one-to-one is
referred to as many-to-one.
Definition 2.4 Bijective function
If a function is both surjective and injective it is said to be bijective. When this happens,
there is a one-to-one correspondence between elements in A and elements in B.
Bijective functions play a central role in the description of reversible digital and quantum
gates which are used to process quantum information.
Boolean functions are particularly relevant to the study of quantum computation. These
are functions where values in both the domain and the co-domain are chosen from the set
of binary digits B = {0, 1}. Consider the following example.
Example 2.2.1 A function f : B → B
Consider a function f : B → B, an example of which is shown in Figure 2.2. The domain
of f is the set B = {0, 1}, as is the co-domain. The element 1 in the domain is mapped to
0 in the co-domain. We write f(1) = 0. The element 0 is mapped to 1. We write f(0) = 1.
Functions of this type are ubiquitous in digital computation. In Section 2.4, we will consider
several such functions from B to B.
f
0 0
1 1
B B
FIGURE 2.2
An example of a function f : B → B.
Example 2.2.2 Set permutations as functions
Suppose A = {1, 2, 3, 4, 5}. Consider the function π : A → A depicted in Figure 2.3. ObserveIntroductory definitions and terminology 29
carefully that this function is one-to-one and onto, i.e., a bijection, and its effect is to permute
the elements of the set A. We can think of the function as effecting the permutation:
12345 → 23154
Indeed we say that a permutation is a bijection from A to itself. Set permutations arise
in the study of group theory (Chapter 8). In this example π(1) = 2, π(2) = 3, π(3) = 1,
π(4) = 5, π(5) = 4.
1
2
3
4
5
1
2
3
4
5
A A
π
FIGURE 2.3
An example of a function π : A → A which permutes a set.
There are several common ways of writing down set permutations. One such way is cycle
notation which would represent this permutation as
(123)(45)
which means 1 → 2, 2 → 3, 3 → 1; 4 → 5, 5 → 4. In cycle notation an element which gets
mapped to itself is usually omitted from the list.
Exercises
2.1 Use cycle notation to represent the bijection σ(1) = 2, σ(2) = 3, σ(3) = 4,
σ(4) = 5, σ(5) = 1.
2.2 Draw a set diagram which depicts the permutation (12)(345).
Example 2.2.3 Real-valued functions and graphs
Consider the function f : R → R defined by f(x) = x
2
, or equivalently f : x → x
2
. The
domain of this function is the set of real numbers R. This set provides the input. For example
if x = 3 the output is f(3) = 32 = 9. If x = −2, the output is f(−2) = (−2)2 = 4. The
input-output pairs (−2, 4), (3, 9) can be plotted as points using Cartesian coordinates (x, y)
and by joining the points we obtain the graph of the function illustrated in Figure 2.4.
Exercises
2.3 Show that the function f : R → R, f(x) = x
2
, is neither one-to-one nor surjective.30 Functions and their application to digital gates
-4 -2 0 2 4
15
10
5
f (x) f (x) = x
2
x
(−2, 4)
(3 , 9)
FIGURE 2.4
Graph of the function f : R → R defined by f(x) = x
2
.
2.4 Show that the function f : R → R, f(x) = x
3
, is both one-to-one and surjective,
and is therefore bijective.
Definition 2.5 Composite function
Consider two functions f : A → B and g : B → C. Starting with x ∈ A, then f(x) ∈ B.
We can then use g to map f(x) to g(f(x)) as shown in Figure 2.5. The function g(f(x)),
also written (g ◦ f)(x), or simply g ◦ f, is called the composite function or composition
of f and g. In other words
(g ◦ f) : A → C, (g ◦ f)(x) = g(f(x))
A B
f g
C
f(x)
x
g(f(x))
FIGURE 2.5
The composite function g(f(x)) applies f first and then g.
Composition of functions obeys the associative law, that is, if f : A → B, g : B → C,
h : C → D then
h ◦ (g ◦ f) = (h ◦ g) ◦ f.
So, provided the order of h, g and f is maintained, it does not matter which composite
function g ◦ f or h ◦ g is evaluated first.
Definition 2.6 Inverse function
When a function is bijective there is a one-to-one correspondence between its inputs and out￾puts. Consequently we can define a function, labelled f
−1 and called the inverse function,
which reverses the rule given by f. This is depicted in Figure 2.6. Note that f
−1
(f(x)) = x,
that is (f
−1 ◦ f)(x) = x. We then say that f is invertible or reversible.Some more functions f : R → R 31
A B
f
f(x)
x
f
−1
FIGURE 2.6
The inverse function f
−1
, when such exists, reverses the process in f.
Exercises
2.5 Is the function f in Example 2.2.1 invertible ?
2.3 Some more functions f : R → R
There are several common functions for which both the domain and co-domain are the set
of real numbers R. We shall refer to these as real-valued functions. This section provides
a brief review of the functions we will need in this book.
Polynomial functions f : R → R take the form
f(x) = anx
n + an−1x
n−1 + an−2x
n−2 + . . . + a2x
2 + a1x + a0
where ai ∈ R, n ∈ N, i = 0, . . . , n. The value of n is referred to as the degree of the polyno￾mial. A polynomial of degree 0 is a constant function. The graph of a real-valued function
is obtained by plotting and joining points with coordinates (x, f (x)). It is straightforward
to produce graphs of these functions using a graphing calculator, software or one of the
many available on-line graph plotting tools. The graphs of some low degree polynomials are
shown in Figure 2.7. Note that the graph of a polynomial of degree 1, i.e., f(x) = a0 + a1x,
is a straight line. The graph of a constant function (degree 0) is a horizontal line.
An exponential function f : R → R has the form
f(x) = a
x
where a is a positive real constant. In quantum computing the value of a is commonly 2.
The graph of f(x) = 2x
, for x ≥ 0, is shown in Figure 2.8. Another commonly used value for
a is the exponential constant e = 2.718 . . . in which case f(x) = ex
is referred to as the
exponential function. Values of exponential functions can be obtained using a calculator. In
order to demonstrate exponential growth, the value of a must be greater than 1.
The logarithm to base 2 function, f : R
+ → R denoted by f(x) = log2 x, has a value
y such that x = 2y
. Values of log2 x can be found using the formula log2 x =
log10 x
log10 2
. The
values of log10 can be obtained using a calculator. The graph of log2 x is shown in Figure
2.8. Note that the domain of this function is restricted to R
+; the logarithm of 0 and of
negative numbers is not defined when using real numbers.32 Functions and their application to digital gates
-3 -1-2 1 2 3 0
-5
-10
-15
5
10
degree 1
degree 2
degree 3
degree 0
f (x)
x
FIGURE 2.7
Some typical polynomial functions of low degree.
1 2 3 4
0
5
10
15
-5
f (x) = 2x
f (x) = log 2
x
f (x)
x
FIGURE 2.8
The functions defined by f(x) = 2x and f(x) = log2 x.
The trigonometric functions, f : R → R, f(x) = sin x and f(x) = cos x, follow from their
respective trigonometric ratios (see Appendix B). (Likewise for the function f(x) = tan x
though some domain restriction is needed because the tangent function is undefined when
x = (2k + 1) π
2
, k ∈ Z.) In general, values of these functions can be obtained using a
calculator, but some common special cases are given in the Appendix.
As usual, the graphs of the trigonometric functions are obtained by calculating, plotting
and joining points with coordinates (x, f (x)). Graphs of f(x) = sin x and f(x) = cos x
are shown in Figure 2.9. There will be a need to write expressions involving trigonometric
functions in different yet equivalent forms. To do this use is made of trigonometric identities
also given in Appendix B.Some more functions f : R → R 33
1
0.5
-0.5
-1
π 2π
f (x)
x
f(x) = sin x
f(x) = cosx
FIGURE 2.9
The functions f(x) = sin x and f(x) = cos x.
Exercises
2.6 Show that the functions f(x) = sin x and f(x) = cos x shown in Figure 2.9 are
neither one-to-one nor surjective.
2.7 Using software, or otherwise, plot a graph of f(x) = cos 2x. Determine whether
this function is bijective.
2.8 Plot a graph of f(x) = cos θ
2
for 0 ≤ θ ≤ π. Deduce that if r = cos θ
2
for 0 ≤ θ ≤ π,
then 0 ≤ r ≤ 1.
2.9 Plot a graph of f(x) = sin θ
2
for 0 ≤ θ ≤ π. Deduce that if r = sin θ
2
for 0 ≤ θ ≤ π,
then 0 ≤ r ≤ 1.
2.10 Use a trigonometric identity (Appendix B) to show that if
x = sin
θ
2
cos ϕ, y = sin
θ
2
sin ϕ, z = cos
θ
2
then x
2 + y
2 + z
2 = 1.
2.3.1 The relative growth of functions
Complexity theory is concerned with the efficiency of algorithms. One way in which
efficiency is measured is by estimating the time required to perform a calculation as the
number of inputs, x, increases. Observe from Figure 2.10 that as x increases some functions
grow much faster than others. An understanding of the relative growth of these functions is
essential to an understanding of the assessment of algorithm efficiency – both in the digital
and quantum domains. It is important to recognise that exponential functions grow much
more rapidly than polynomial and logarithmic ones as x increases.
To emphasise further the importance of relative growth consider the function values
in Table 2.1 and observe how incredibly rapidly the exponential function, 2x
, grows even
compared to the rapidly growing quadratic x
2
.34 Functions and their application to digital gates
10
20
30
40
1 2 3 4 5 6
f (x)
2
x
x
x
log2
x
x
2
x log2
x
FIGURE 2.10
Comparing the relative growth of the functions log2 x, x, x log2 x, x
2 and 2x
.
TABLE 2.1
Table showing the relative growth of
common functions
x = 10 x = 100 x = 1000
log2 x 3.3 6.6 10.0
x log2 x 33.2 664.4 9965.8
x
2 100 10 000 1 000 000
2
x 1024 1030 10301
2.4 The Boolean functions f : B → B
There are just four Boolean functions f : B → B where B is the set of binary digits {0, 1}.
In this section we define the four functions and illustrate several results which will be useful
when we meet Deutsch’s quantum algorithm in Chapter 22. We denote the set containing
all four functions f : B → B by F(B, B).
Example 2.4.1 The digital not function
The function f : B → B, illustrated in Figure 2.11, maps the binary digit 1 to 0, and the
binary digit 0 to 1, that is f(1) = 0, f(0) = 1.
f
0 0
1 1
FIGURE 2.11
The digital not function f : B → B, f(x) = 1 ⊕ x.The Boolean functions f : B → B 35
There are several ways of expressing this function. We could write a mathematical
formula:
f(x) = 
0 when x = 1
1 when x = 0 .
An alternative way is to tabulate the inputs and outputs as shown in Table 2.2. It is easily
TABLE 2.2
Representing the
digital not function
f by its inputs and
outputs
input x output
1 0
0 1
seen from Figure 2.11 that this function is bijective: there is a one-to-one correspondence
between values in the domain and the co-domain. This function is referred to as a digital
not gate or a digital not function. On occasions we will write notD to distinguish it from
the quantum equivalent notQ. We will indicate the not function using an overbar so that
this function can be written f(x) = x, that is 1 = 0, and 0 = 1. Further, you should verify
that the not function f can be expressed using the exclusive-or operator as f(x) = 1 ⊕ x.
It can be represented diagrammatically as in Figure 2.12.
x not x
FIGURE 2.12
The digital not function.
Exercises
2.11 Given f : B → B, f(x) = 1⊕x, verify that f(0) = 1⊕0 = 1 and f(1) = 1⊕1 = 0.
There are just three further functions f : B → B. Two of these are represented by constant
functions, giving a constant (i.e., the same) output whatever the value of the input. One is
the identity function i(x) (or sometimes we shall write iD(x)), which produces the same
output as input. Let us label these four functions as f0, f1, f2, f3, (with f2 being the digital
not function of Example 2.4.1).
In summary
f0 : B → B, f0(x) = 0 for all x ∈ B
f1 : B → B, f1(x) = i(x) = x for all x ∈ B
f2 : B → B, f2(x) = x for all x ∈ B
f3 : B → B, f3(x) = 1 for all x ∈ B.
We can record the possible inputs and outputs of these functions in Tables 2.3 . . . 2.5. (The
digital not function, f2, has been defined already in Table 2.2.)36 Functions and their application to digital gates
TABLE 2.3
The constant
function f0(x) = 0.
input x output 0
1 0
0 0
TABLE 2.4
The identity function
f1(x) = i(x) = x.
input x output x
1 1
0 0
TABLE 2.5
The constant
function f3(x) = 1.
input x output 1
1 1
0 1
The functions f2 and f1 in Tables 2.2 and 2.4 are said to be balanced in that the function
outputs the same number of 0’s as 1’s. Functions f0 and f3 are said to be unbalanced and
are constant. Determining whether a given but unknown function f : B → B is balanced or
constant is an important challenge for quantum algorithms as we shall see in Chapter 22.
Exercises
2.12 Which of the four functions f : B → B cannot be inverted ?
The following three examples illustrate useful manipulations required in the development
of quantum algorithms that we study in Chapter 22.
Example 2.4.2
Suppose f is one of the four functions f : B → B but we know not which. The exclusive-or
operator is defined on the Boolean variables x and y so that x⊕y = 1 whenever x or y (but
not both) equals 1, and is zero otherwise.
(a) Suppose that f(0) ⊕ f(1) = 0. Show that f must be one of the two constant functions.
(b) Suppose that f(0)⊕f(1) = 1. Show that f must be one of the two balanced functions.
Solution
(a) The inputs and outputs of the four functions, f0, . . . , f3, together with the result of
calculating f(0) ⊕ f(1), are shown in Table 2.6. Recall that f(0) ⊕ f(1) is 1 whenever f(1)
or f(0) (but not both) equals 1.
TABLE 2.6
input to f f0 f1 f2 f3
1 0 1 0 1
0 0 0 1 1
f(0) ⊕ f(1) 0 1 1 0
From the table, it is clear that if f(0) ⊕ f(1) = 0, then f must be one of f0 or f3, that
is, it is a constant function.
(b) Likewise, if f(0) ⊕ f(1) = 1, then f must be one of f1 or f2, that is, it is a balanced
function.The Boolean functions f : B → B 37
Example 2.4.3
Consider the four functions f : B → B. Show that in all cases
(−1)f(0)(−1)f(0)⊕f(1) = (−1)f(1)
Solution
Table 2.6 is extended in Table 2.7 to include the rows (−1)f(0), (−1)f(1), (−1)f(0)⊕f(1)
,
(−1)f(0)(−1)f(0)⊕f(1) from which the result (−1)f(0)(−1)f(0)⊕f(1) = (−1)f(1) is clear.
TABLE 2.7
input to f f0 f1 f2 f3
1 0 1 0 1
0 0 0 1 1
f(0) ⊕ f(1) 0 1 1 0
(−1)f(0) 1 1 −1 −1
(−1)f(1) 1 −1 1 −1
(−1)f(0)⊕f(1) 1 −1 −1 1
(−1)f(0)(−1)f(0)⊕f(1) 1 −1 1 −1
Example 2.4.4
Suppose f is one of the four functions f : B → B. For x ∈ B show that whatever the output
f(x),
f(x) ⊕ 0 = f(x)
and
f(x) ⊕ 1 = f(x)
where the overline denotes the not function.
Solution
The value of f(x) is either 0 or 1. Consider Table 2.8 from which it is clear that f(x) ⊕ 0
has the same value as f(x). From Table 2.9, it is clear that f(x) ⊕ 1 is f(x).
TABLE 2.8
f(x) 0 f(x) ⊕ 0
1 0 1
0 0 0
TABLE 2.9
f(x) 1 f(x) ⊕ 1
1 1 0
0 1 138 Functions and their application to digital gates
Exercises
2.13 For f : B → B show that if x ∈ B then f(x) ⊕ f(x) is always zero.
2.14 For f : B → B show that if x ∈ B, f(x) ⊕ 0 = f(x).
2.15 Show that not all four functions f : B → B are bijective.
2.16 Write down explicit expressions for all four functions in the set F(B, B).
Example 2.4.5 The functions f : B → B with binary operation ⊕ form a group
Show that the set of functions fi
: B → B, i = 0, . . . , 3, defined as f0(x) = 0, f1(x) = x,
f2(x) = ¯x, f3(x) = 1, together with the exclusive-or operator, ⊕, form a group, where
(f ⊕ g)(x) is defined as
(f ⊕ g)(x) = f(x) ⊕ g(x).
Solution
The effect of applying the operation ⊕ to the four functions is shown in Table 2.10. For
example, consider (f2⊕f1)(x) which is defined to be f2(x)⊕f1(x). When x = 0, (f2⊕f1)(0) =
f2(0) ⊕ f1(0) = 1 ⊕ 0 = 1. Likewise, when x = 1, (f2 ⊕ f1)(1) = f2(1) ⊕ f1(1) = 0 ⊕ 1 = 1.
Thus (f2 ⊕ f1)(x) = 1 = f3(x). You should verify the other results for yourself.
TABLE 2.10
The operation ⊕
applied to the set of
functions fi
: B → B
⊕ f0 f1 f2 f3
f0 f0 f1 f2 f3
f1 f1 f0 f3 f2
f2 f2 f3 f0 f1
f3 f3 f2 f1 f0
Clearly the given set of functions is closed under the defined operation ⊕. The identity
element is f0, and each element is its own inverse (e.g., observe f2 ⊕ f2 = f0). This group
is denoted (F(B, B), ⊕) and is used in the quantisation of Boolean functions.
2.5 Functions defined on Cartesian products
Suppose the domain of a function is a set of ordered pairs, (a, b) with a ∈ A and b ∈ B, i.e.,
the domain is the Cartesian product A × B. We can define a function, f, which maps each
ordered pair in A×B to just one element in a third set C, f : A×B → C. The terminology
introduced in Section 2.2 is precisely the same. We can think of this function in several
ways. It could be tabulated. Alternatively, it could be depicted in either of the ways shown
in Figure 2.13. The co-domain C could itself be a Cartesian product, as we shall see shortly.Functions defined on Cartesian products 39
C
f
A × B A
B
C
(a1, b1)
(a2, b2)
2
c
1
c
a1 a2
b1
b2
f
1
c
2
c
FIGURE 2.13
The function f : A × B → C, defined on a Cartesian product.
2.5.1 The Boolean functions f : B × B → B
We now consider specific cases where the domain of the function is the Cartesian product
B × B = B
2 and the co-domain is B, that is f : B × B → B. We denote the set of all such
functions by F(B
2
, B). The first example makes use of the logical connective exclusive-or:
Definition 2.7 Exclusive-or, ⊕
Suppose x, y ∈ B. The binary operator xor, (exclusive-or), written ⊕, is equal to 1 when
either but not both of x and y are equal to 1, and is zero otherwise:
0 ⊕ 0 =0
0 ⊕ 1 =1
1 ⊕ 0 =1
1 ⊕ 1 =0.
Example 2.5.1 The xor gate or classical cnot function f(x, y) = x ⊕ y
Consider the function f defined, using the exclusive-or ⊕, as
f : B × B → B
f(x, y) = x ⊕ y =



0 when x = 0, y = 0
1 when x = 0, y = 1
1 when x = 1, y = 0
0 when x = 1, y = 1
This function is depicted in Figure 2.14 and tabulated in Table 2.11.
TABLE 2.11
The function f : B × B → B,
f(x, y) = x ⊕ y
input (x, y) output x ⊕ y
(0,0) 0
(0,1) 1
(1,0) 1
(1,1) 040 Functions and their application to digital gates
0
0 1
1
0
1
B × B B
f
FIGURE 2.14
A pictorial representation of the function f : B × B → B, f(x, y) = x ⊕ y.
Observe that the output of this function is 1 when either but not both of x and y is 1,
and that it is balanced. This function is an exclusive-or gate (xor) (sometimes called a
classical cnot function) represented diagrammatically as in Figure 2.15.
x
y
x ⊕ y
FIGURE 2.15
Diagrammatic representation of an xor gate or classical cnot function.
It is important for what follows to note that this function is not one-to-one. For example,
both (0, 1) and (1, 0) get mapped to 1. Thus it is not invertible because we cannot recover
the input values by knowing the output value. This has implications for reversibility which
we shall discuss shortly.
Exercises
2.17 Use Table 2.11 to show that the binary operation ⊕ is commutative, that is
x ⊕ y = y ⊕ x.
There are 24 = 16 functions in the set F(B
2
, B), that is, of the type f : B × B → B. Each
of these functions accepts two Boolean variables x and y, combining these in various ways
to produce a single Boolean variable as output. Mathematically, combination is achieved
using further logical connectives: and ∧, or ∨ and not which are defined below:
Definition 2.8 Conjunction: and, ∧
Suppose x, y ∈ B. The binary operator and, (conjunction), written ∧, is equal to 1 whenFunctions defined on Cartesian products 41
both x and y are equal to 1, and is zero otherwise:
0 ∧ 0 =0
0 ∧ 1 =0
1 ∧ 0 =0
1 ∧ 1 =1.
Definition 2.9 Disjunction: or, ∨
The binary operator or, (disjunction), written ∨, is equal to 1 when either or both x and
y are equal to 1, and is zero otherwise:
0 ∨ 0 =0
0 ∨ 1 =1
1 ∨ 0 =1
1 ∨ 1 =1.
Definition 2.10 negation: not, x
The unary operator not, (negation), written x, is equal to 1 when x = 0 and equal to 0
when x = 1:
0 = 1
1 = 0.
It is important to note that just two connectives, conjunction ∧ and negation , are sufficient
to write any binary expression in an equivalent form. Thus we say that the set {∧, } is
functionally complete or universal.
TABLE 2.12
The complete set of functions f : B × B → B
x y f0 f1 f2 f3 f4 f5 f6 f7 f8 f9 f10 f11 f12 f13 f14 f15
0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1
0 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1
1 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1
1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1
The sixteen functions in F(B
2
, B) are defined by their inputs and outputs given in Table
2.12. Each can be expressed mathematically using combinations of ∧, ∨ and as shown in
Table 2.13. Observe that f6 in the table is the xor gate already discussed in Example 2.5.1.
It is important to note that none of these functions are one-to-one. Knowing the output, it
is impossible to determine the input. Consequently it is not possible to reverse the effects
of these functions – they are not invertible. We see how to overcome this limitation shortly.
2.5.2 The Boolean functions f : B
2 → B
2
.
In the following example both the domain and the co-domain of the given function are the
Cartesian product B × B = B
2
. So the input to the function is an ordered pair of Boolean
variables, likewise the output. We saw in Example 2.5.1 that the classical Boolean cnot gate
was not invertible. In the following example, we see how this situation is remedied.42 Functions and their application to digital gates
TABLE 2.13
Functions f : B × B → B expressed
mathematically
logic description
f0(x, y) = 0
f1(x, y) = x ∧ y and
f2(x, y) = x ∧ y
f3(x, y) = x
f4(x, y) = x ∧ y
f5(x, y) = y
f6(x, y) = x ⊕ y xor
f7(x, y) = x ∨ y or
f8(x, y) = x ∨ y not or (nor)
f9(x, y) = x ⊕ y
f10(x, y) = y
f11(x, y) = x ∨ y
f12(x, y) = x
f13(x, y) = x ∨ y
f14(x, y) = x ∧ y not and (nand)
f15(x, y) = 1
Example 2.5.2 The digital Feynman cnot gate.
Consider the function FD : B
2 → B
2 defined by
FD(x, y) = (x, x ⊕ y).
known as a digital Feynman cnot gate. (Here, we are using the letter F for a Feynman
gate and the subscript D to indicate we are considering the digital version. Later we will
introduce FQ for its quantum emulation.) This function is sometimes written as
cnot : B
2 → B
2
, cnot(x, y) = (x, x ⊕ y).
It is tabulated in Table 2.14 and represented diagrammatically in Figure 2.16. An alternative
diagrammatic form – Feynman’s form – is shown in Figure 2.17. Observe from the table
that this function is bijective.
TABLE 2.14
The function FD : B
2 → B
2
,
FD(x, y) = (x, x ⊕ y)
input (x, y) output (x, x ⊕ y)
(0,0) (0,0)
(0,1) (0,1)
(1,0) (1,1)
(1,1) (1,0)
Unlike the earlier classical cnot gate, the Feynman cnot gate is reversible: we can re￾cover the input values if we know the output values, that is, the gate is invertible. You
should verify this yourself. The Feynman cnot gate can also be represented as FD(x, y) =
(f3(x, y), f6(x, y)) of Tables 2.12 and 2.13.
It is worth noting that cnot is sometimes referred to as a ‘controlled not’ function. If
the first argument of the function is 0, the second does not change; if the first argument is
1, we apply a not to the second argument.Functions defined on Cartesian products 43
x
x
y
x ⊕ y
FIGURE 2.16
Diagrammatic representation of the Feynman cnot gate.
x x
y x ⊕ y
FIGURE 2.17
Feynman’s form of the digital invertible cnot gate: input (x, y), output (x, x ⊕ y).
Example 2.5.3 The identity function on B
2
.
Consider the function i : B
2 → B
2 defined by
i(x, y) = (x, y).
Because the output of the function i(x, y) = (x, y) is the same as the input, this is referred
TABLE 2.15
The identity function
i : B
2 → B
2
, i(x, y) = (x, y)
input (x, y) output (x, y)
(0,0) (0,0)
(0,1) (0,1)
(1,0) (1,0)
(1,1) (1,1)
to as the identity function on B
2
. This function is tabulated in Table 2.15. Note that
using Tables 2.12 and 2.13, we can write i(x, y) = (f3(x, y), f5(x, y)).
Example 2.5.4
Consider the function f : B
2 → B
2 defined by
f(x, y) = (x ∨ y, x).
Note that this is the function f(x, y) = (f7, f3) from Tables 2.12 and 2.13. Show that this
function is not bijective.44 Functions and their application to digital gates
TABLE 2.16
The function f : B
2 → B
2
,
f(x, y) = (x ∨ y, x)
input (x, y) output (x ∨ y, x)
(0,0) (0,0)
(0,1) (1,0)
(1,0) (1,1)
(1,1) (1,1)
Solution
We tabulate the function as shown in Table 2.16. Observe from the table that the element
(1, 1) in the range of the function is mapped to by both (1, 0) and (1, 1) in the domain. Hence,
this function cannot be injective, and therefore, it cannot be bijective. A consequence is that
this function is not reversible: knowing the output (1, 1), we cannot recover the input.
We note that all Boolean functions f : B
2 → B
2 may be represented in the form f(x, y) =
(fi(x, y), fj (x, y)) for some fi and fj , the functions B
2 → B in Tables 2.12 and 2.13. This
gives us a total of 42 × 4
2 = 44 = 256 functions in the set F(B
2
, B
2
). However, it can
be shown that only 4! = 24 of these are invertible. The invertible ones are identified in
Proposition 2.1.
Proposition 2.1 The invertible functions f : B
2 → B
2 are the pairs:
(f3, f5), (f3, f6), (f3, f9), (f3, f10),
(f5, f6), (f5, f9), (f5, f12),
(f6, f10), (f6f12),
(f9, f10), (f9, f12),
(f10, f12)
and the same pairs in contrary order. Explicitly, the pairs are
(x, y), (x, x ⊕ y), (x, x ⊕ y), (x, y),
(y, x ⊕ y), (y, x ⊕ y), (y, x),
(x ⊕ y, y), (x ⊕ y, x),
(x ⊕ y, y), (x ⊕ y, x),
(y, x).
Proof
The 24 invertible functions gk : B
2 → B
2
, for 0 ≤ k ≤ 23 are identified by selecting from
the 256 only those which are bijective, yielding:
x y| g0 g1 g2 g3 g4 g5 g6 g7 g8 g9 g10 g11
0 0| 00 00 00 00 00 00 01 01 01 01 01 01
0 1| 01 01 10 10 11 11 00 00 10 10 11 11
1 0| 10 11 01 11 01 10 10 11 00 11 10 00
1 1| 11 10 11 01 10 01 11 10 11 00 00 10Functions defined on Cartesian products 45
x y| g12 g13 g14 g15 g16 g17 g18 g19 g20 g21 g22 g23
0 0| 10 10 10 10 10 10 11 11 11 11 11 11
0 1| 00 00 01 01 11 11 01 01 10 10 00 00
1 0| 01 11 00 11 00 01 10 00 01 00 01 10
1 1| 11 01 11 00 01 00 00 10 00 01 10 01
that is, we have
g0 = (f3, f5), g1 = (f3, f6), g2 = (f5, f3), g3 = (f6, f3), g4 = (f5, f6),
g5 = (f6, f5), g6 = (f3, f9), g7 = (f3, f10), g8 = (f5, f9), g9 = (f6, f10),
g10 = (f6, f12), g11 = (f5, f12), g12 = (f9, f3), g13 = (f10, f3), g14 = (f9, f5),
g15 = (f10, f6), g16 = (f12, f5), g17 = (f12, f6), g18 = (f10, f12), g19 = (f9, f12),
g20 = (f12, f10), g21 = (f12, f9), g22 = (f9, f10), g23 = (f10, f9).
Of all the invertible functions of the type f : B
2 → B
2
, the Feynman cnot gate of Example
2.5.2, f(x, y) = (x, x ⊕ y) is, essentially, the only one of computational significance.
Exercises
2.18 Consider the function f : B
2 → B
2
, f(x, y) = (x, y ⊕ 0). Tabulate input and
output values and show that f is bijective.
2.19 Consider the function f : B
2 → B
2
, f(x, y) = (x, y ⊕ 1). Tabulate input and
output values and show that f is bijective.
2.5.3 Further Boolean functions
Example 2.5.5 The digital Toffoli gate or ccnot gate.
Consider the three-input function TD : B
3 → B
3 defined by
TD(x, y, z) = (x, y, z ⊕ (x ∧ y))
known as a digital Toffoli gate or ccnot gate. This function is tabulated in Table 2.17
and is often represented as shown in Figure 2.18. If necessary, when producing such a table,
intermediate steps can be introduced. So, for example, to evaluate z ⊕ (x ∧ y) when the
input is (1, 1, 1) note that x ∧ y = 1 ∧ 1 = 1, and then z ⊕ (x ∧ y) = 1 ⊕ 1 = 0. Careful
inspection of the table reveals that this is a bijective function and hence is reversible.
x
y
z
x
y
z ⊕ (x ∧ y)
FIGURE 2.18
The digital Toffoli or ccnot gate.
Consider again the Toffoli gate in Example 2.5.5. By setting z = 0, we obtain
TD(x, y, 0) = (x, y, 0 ⊕ (x ∧ y)).46 Functions and their application to digital gates
TABLE 2.17
The function TD : B
3 → B
3
,
TD(x, y, z) = (x, y, z ⊕ (x ∧ y))
input (x, y, z) output (x, y, z ⊕ (x ∧ y))
(0,0,0) (0,0,0)
(0,0,1) (0,0,1)
(0,1,0) (0,1,0)
(0,1,1) (0,1,1)
(1,0,0) (1,0,0)
(1,0,1) (1,0,1)
(1,1,0) (1,1,1)
(1,1,1) (1,1,0)
We have seen earlier (Example 2.4.4) that 0 ⊕ f(x) = f(x), and it follows that
TD(x, y, 0) = (x, y, x ∧ y).
Hence, with z = 0, TD enables evaluation of the and function x ∧ y. Also, with x = y = 1
we have
TD(1, 1, z) = (1, 1, z ⊕ (1 ∧ 1))
= (1, 1, z ⊕ 1)
= (1, 1, z)
since z ⊕1 = z (see Example 2.4.4). Hence, the Toffoli gate can be used to calculate the not
function. We noted earlier that the set {∧, } is functionally complete or universal. Because
the ∧ and not functions can be calculated using carefully chosen values of the inputs, the
Toffoli function can perform any computation on a digital computer in a reversible manner.
We also have
TD(x, y, 1) = (x, y, 1 ⊕ (x ∧ y)) = (x, y, x ∧ y)
to give nand (x, y), that is f14 in Table 2.13. Further, it is straightforward to verify that
TD(x, 1, 0) = (x, 1, x), TD(1, x, 0) = (1, x, x) and TD(x, x, 0) = (x, x, x), so this gate can be
used to copy input values.
In summary it follows that the Toffoli function defines a complete (or universal) set for
the generation of invertible (or reversible) versions of the Boolean functions f : B
2 → B.
The Toffoli function is also a significant quantum operator as we shall see in Chapter 15. It
is, however, not universal for quantum computation, but from the above, it follows that a
quantum computer can implement all possible digital computations.
Example 2.5.6 The digital Peres or half-adder gate.
Consider the three-input function PD : B
3 → B
3 defined by
PD(x, y, z) = (x, y ⊕ x, z ⊕ (x ∧ y)).
This function is tabulated in Table 2.18 and often represented as in Figure 2.19. This
function is known as the digital Peres or half-adder gate. It can be checked directly
from the table that this is a bijective function. The Peres gate may be viewed as a product
of the Toffoli gate with the invertible cnot gate, as we show in Section 2.7.Further composition of functions 47
TABLE 2.18
The function PD : B
3 → B
3
,
PD(x, y, z) = (x, y ⊕ x, z ⊕ (x ∧ y))
input (x, y, z) output (x, y ⊕ x, z ⊕ (x ∧ y))
(0,0,0) (0,0,0)
(0,0,1) (0,0,1)
(0,1,0) (0,1,0)
(0,1,1) (0,1,1)
(1,0,0) (1,1,0)
(1,0,1) (1,1,1)
(1,1,0) (1,0,1)
(1,1,1) (1,0,0)
x
y
z
x
y ⊕ x
z ⊕ (x ∧ y)
FIGURE 2.19
The digital Peres half-adder.
2.6 Further composition of functions
Example 2.6.1
Consider the function F : B
2 → B
2 defined by
F(x, y) = (y, x).
Obtain an expression for the composite function F ◦ F.
Solution
F ◦ F = (F ◦ F)(x, y) = F(F(x, y))
= F(y, x)
= (x, y)
= (x, y).
Note that the result of finding F ◦ F is the identity function on B
2
so that F(x, y) = (y, x)
is its own inverse, i.e., it is self-inverse.
Example 2.6.2
Consider the functions F : B
2 → B
2 and G : B
2 → B
2 defined by
F(x, y) = (x ⊕ y, y), G(x, y) = (x ⊕ y, y).48 Functions and their application to digital gates
Obtain an expression for the composite function F ◦ G.
Solution
F ◦ G = (F ◦ G)(x, y) = F(G(x, y))
= F(x ⊕ y, y)
= ((x ⊕ y) ⊕ y, y).
Now consider just (x ⊕ y) ⊕ y. This function is tabulated in Table 2.19 and is equal to x.
Consequently, F ◦ G is given by Table 2.20 and thus, in this example, as in Example 2.6.1,
TABLE 2.19
Tabulation of the function (x ⊕ y) ⊕ y
x y x ⊕ y x ⊕ y y (x ⊕ y) ⊕ y
0 0 0 1 1 0
0 1 1 0 0 0
1 0 1 0 1 1
1 1 0 1 0 1
TABLE 2.20
Calculating the
composite function
F ◦ G
(x, y) (F ◦ G)(x, y)
(0, 0) (0, 0)
(0, 1) (0, 1)
(1, 0) (1, 0)
(1, 1) (1, 1)
F ◦ G is the identity function on B
2
. You should verify that G ◦ F is also equal to the
identity function. Observe that F(x, y) = (x ⊕ y, y) is the inverse of G(x, y) = (x ⊕ y, y)
(and vice-versa).
2.7 The Cartesian product of functions
Definition 2.11 The Cartesian product of functions
Consider the two functions f1 : A1 → B1 and f2 : A2 → B2. Note that the two functions
may be defined on different domains, A1 and A2. The Cartesian product of the two
functions is another function, f1 × f2, defined as follows:
f1 × f2 : A1 × A2 → B1 × B2
(f1 × f2)(a1, a2) = (f1(a1), f2(a2)).
Note (f1(a1), f2(a2)) ∈ B1 × B2 for all a1 ∈ A1 and a2 ∈ A2.Permuting (swapping) binary digits and binary strings 49
Example 2.7.1
We have already seen the cnot function
cnot : B
2 → B
2
, cnot(x, y) = (x, x ⊕ y)
and the identity function on B,
i : B → B, i(x) = x.
Note that the cnot and identity function are defined on different domains. Thus we can now
determine the meaning of functions such as
cnot × i : B
2 × B → B
2 × B
(cnot × i)((x, y), z) = (cnot(x, y), i(z))
= ((x, x ⊕ y), z).
Strictly, the right-hand side is a vector in B
2 × B. But since there is a natural isomorphism
from B
2 × B to B
3
, we can readily write the result as
(cnot × i)(x, y, z) = (x, x ⊕ y, z).
Example 2.7.2 The digital Peres gate revisited
We have seen the digital Toffoli and Peres gates in Examples 2.5.5, and 2.5.6.
TD : B
3 → B
3
, TD(x, y, z) = ccnot(x, y, z) = (x, y, z ⊕ (x ∧ y))
PD : B
3 → B
3
, PD(x, y, z) = (x, y ⊕ x, z ⊕ (x ∧ y)).
Observe that
PD(x, y, z) = (x, y ⊕ x, z ⊕ (x ∧ y))
= (cnot(x, y), z ⊕ (x ∧ y))
= (cnot × i)(x, y, z ⊕ (x ∧ y))
= (cnot × i)(TD(x, y, z))
= (cnot × i) ◦ TD(x, y, z).
Thus PD = (cnot × i) ◦ ccnot.
2.8 Permuting (swapping) binary digits and binary strings
The Boolean functions which we have introduced can be used to construct functions which
permute, or swap, binary digits. In this section we explore several ways in which this can
be achieved.50 Functions and their application to digital gates
2.8.1 A classical digital circuit for swapping binary digits
The function f : B
2 → B
2 defined by f(x, y) = (y, x), which swaps the binary digits x
and y, is one of the 24 invertible functions referred to in Section 2.5.2. Suppose we wish to
create a classical circuit to swap digits. The classical xor gate introduced in Example 2.5.1
is not invertible, and to produce a classical circuit for swapping digits, it is necessary to
‘signal-split’, or ‘fan-out’ at three points of the circuit – see Figure 2.20.
x
y
x
y
x ⊕ y
y
(x ⊕ y) ⊕ y
(x ⊕ y) ⊕ ((x ⊕ y) ⊕ y)
FIGURE 2.20
A classical circuit for swapping binary digits.
To verify that this circuit performs as intended work through Table 2.21 from which it
follows that (x ⊕ y) ⊕ y = x and (x ⊕ y) ⊕ ((x ⊕ y) ⊕ y) = y: the digits have been swapped.
TABLE 2.21
A classical circuit for swapping binary digits (see Figure 2.20)
x y x ⊕ y (x ⊕ y) ⊕ y = x (x ⊕ y) ⊕ ((x ⊕ y) ⊕ y) = y
0 0 0 0 0
0 1 1 0 1
1 0 1 1 0
1 1 0 1 1
2.8.2 Swapping binary digits using the Feynman cnot gate
A functionally equivalent circuit to that of Figure 2.20 may be defined using the invertible
Feynman cnot function introduced in Example 2.5.2, cnot : B
2 → B
2
, cnot(x, y) = (x, x⊕y).
We define the cnot1 and cnot2 functions by:
cnot1(x, y) = (x, y ⊕ x), cnot2(x, y) = (x ⊕ y, y)
which are shown graphically in Figure 2.21.
y ⊕ x
x x
y
x x ⊕ y
y y
FIGURE 2.21
The digital, invertible cnot1 and cnot2 gates.
We can show that
cnot2 ◦ cnot1 ◦ cnot2(x, y) = (y, x),
i.e., a digital swap function defined by explicitly invertible gates as shown in Figure 2.22.Permuting (swapping) binary digits and binary strings 51
Specifically,
cnot1 ◦ cnot2(x, y) = cnot1(cnot2(x, y))
= cnot1((x ⊕ y, y))
= (x ⊕ y, y ⊕ (x ⊕ y)).
Then,
cnot2 ◦ cnot1 ◦ cnot2(x, y) = cnot2(x ⊕ y, y ⊕ (x ⊕ y))
= ((x ⊕ y) ⊕ (y ⊕ (x ⊕ y)), y ⊕ (x ⊕ y))
= (y, x)
(using Table 2.21). For brevity we often depict bit-swapping as shown in Figure 2.23.
x
y
y
x
FIGURE 2.22
The cnot implementation of the digital 2-bit swap circuit.
x
y
y
x
×
×
FIGURE 2.23
Alternative representation of the digital 2-bit swap gate.
2.8.3 Swapping strings of binary digits – vectorising the swap operator
Suppose we define the bit-swapping function swap1,2(x, y) = (y, x), then we can ‘vectorise’
this in an obvious way to create a swap function, swapv say, for binary strings.
To swap the strings (x1, x2; y1, y2) to (y1, y2; x1, x2) we have:
swapv(x1, x2; y1, y2) = swap2,4 ◦ swap1,3(x1, x2; y1, y2)
= swap2,4(y1, x2; x1, y2)
= (y1, y2; x1, x2)
as shown in Figure 2.24.
This extends in the obvious way to swap strings of length n; see Figure 2.25.52 Functions and their application to digital gates
×
×
x1
x2 y2
y1
x1
y2 x2
y1
×
×
FIGURE 2.24
Vectorising the swap function to swap binary strings.
×
×
×
×
. . .
x1x2 . . . xn
y1y2 . . . yn x1x2 . . . xn
y1y2 . . . yn
FIGURE 2.25
A swap function for binary strings.
2.9 Copying binary digits and binary strings
In this section, we look at several ways in which single binary digits and then strings of
binary digits can be copied. Specifically, we consider signal-splitting or ‘fan-out’, a dupe
gate, the cnot gate, the Toffoli gate and the Feynman double gate.
2.9.1 Fan-out
Figure 2.26 shows a simple circuit for copying a bit. There is clearly something physical
happening at the point where the ‘signal’ is ‘split’; this is usually referred to as ‘fan-out’ and
taken as implicit (or assumed without comment) in digital computing. Clearly the circuit
transforms x ∈ B to (x, x) ∈ B
2
.
x
x
x
FIGURE 2.26
A fan-out circuit for copying, x ∈ B.
2.9.2 A dupe gate
An alternative view of fan-out, more suited to comparison with the quantum case, is to
make the signal-splitting process explicit with a ‘dupe’ gate
(i, i) : B → B
2Copying binary digits and binary strings 53
defined by:
(i, i)(x) = (i(x), i(x))
= (x, x).
Here i : B → B, i(x) = x for x ∈ B is the identity function. As a function (i, i) : B → B
2
is
not invertible. It is invertible if the range is defined to be the diagonal subset {(x, x) : x ∈ B}
of B
2
. The dupe gate (i, i), which is functionally equivalent to the fan-out circuit above, may
be represented as shown in Figure 2.27. Clearly a network of dupe gates enables multiple
copies of the input bit to be produced – see Figure 2.28.
x
x
x (i,i)
FIGURE 2.27
A dupe gate to copy a bit.
x
x (i,i)
(i,i)
(i,i)
x
x
.
.
.
FIGURE 2.28
A network of dupe gates for x → x
k where x
k = (x, x, x, . . .) = (xxx . . .).
2.9.3 The cnot gate.
The fan-out and dupe gate approaches above are non-invertible processes. At the expense
of additional, or auxiliary, inputs, digital copying can be achieved using the Feynman cnot
invertible gate of Example 2.5.2. Observe that the Feynman cnot gate may be expressed as
a function of the dupe gate and the classical, non-invertible, xor gate – see Figure 2.29. The
x
x (i,i)
y ⊕ x
y
FIGURE 2.29
The cnot gate as a function of the dupe and classical non-invertible xor gates.
function of Figure 2.29 yields (x, x) on the input of (x, 0), (since 0 ⊕ x = x), the 0 being
the auxiliary input needed to copy using an invertible-digital circuit; mathematically the54 Functions and their application to digital gates
circuit may be expressed as:
(i × xor) ◦ ((i, i) × i)(x, y) = (i × xor)(x, x, y)
= (x, y ⊕ x)
= cnot(x, y).
Hence, cnot(x, 0) = (x, x). The circuit of cnot gates shown (in Feynman notation) in Figure
2.30 yields x
k
(i.e., (xxx . . . x)) on the input of x, 0
k−1
(i.e., (x000 . . . 0)). Again we note
x
x x
0 x
0. . .
. . .
. . .
. . .
FIGURE 2.30
Copying a binary digit using the cnot gate.
the necessity for auxiliary inputs 0k−1
to implement the desired circuit using only invertible
gates.
2.9.4 The Feynman double gate
Similarly, with the Feynman double gate defined by F
∗
D(x, y, z) = (x, y ⊕ x, z ⊕ x), we have
F
∗
D(x, 0, 0) = (x, x, x)
as depicted in Figure 2.31.
x
x x
0 x
0
FIGURE 2.31
Copying a binary digit using the Feynman double gate.
2.9.5 The ccnot (Toffoli) gate
With the ccnot (Toffoli) gate, TD(x, y, z) = (x, y, z ⊕ (x ∧ y)) (Example 2.5.5) we have:
TD(x, 1, 0) = (x, 1, 0 ⊕ (x ∧ 1)) = (x, 1, x ∧ 1) = (x, 1, x).
2.9.6 Fan-out copying of binary strings
The fan-out circuit of Figure 2.32 maps the string x1x2 ∈ B
2
to the string x1x2x1x2 ∈ B
4
or, equivalently, (x1, x2) ∈ B
2
to ((x1, x2),(x1, x2)) ∈ B
2 × B
2
.Copying binary digits and binary strings 55
x1
x2
x1
x2
x1
x2
FIGURE 2.32
A fan-out Boolean copy circuit: x1, x2 ∈ {0, 1}.
2.9.7 Digital string copying with the non-reversible copy and reversible
swap gates
We can represent the process represented by Figure 2.32 with the circuit shown in Figure
2.33. The swap gate is clearly invertible and may be implemented using the cnot gate
shown earlier in Figure 2.22. Copying a 2-bit string x1, x2, in the manner of Figure 2.33,
may therefore be expressed mathematically as:
swap23 ◦ ((i, i) × (i, i))(x1, x2) = swap23((i, i)(x1),(i, i)(x2))
= swap23(x1, x1, x2, x2)
= (x1, x2, x1, x2)
and similarly for arbitrary strings x1 · · · xn of digital bits.
(i,i)
(i,i)
×
×
x1
x1
x1
x1
x1
x2
x2
x2
x2
x2
FIGURE 2.33
An alternative representation of the copy circuit of Figure 2.32.
2.9.8 Digital string copying using only reversible (the cnot and swap)
gates
An alternative network, using only invertible Boolean gates, is shown in Figure 2.34. The
input (x1, 0, x2, 0) yields (x1, x2, x1, x2). We note that the use of only reversible functions
again requires the input of auxiliary bits – y1 and y2 in this case.
2.9.9 Digital string copying using only the cnot gate
‘Vectorising’ the invertible cnot gate to copy strings we have:
cnot(x1, x2 y1, y2) = (x1, x2, y1 ⊕ x1, y2 ⊕ x2)56 Functions and their application to digital gates
x1
x2
x1
x2
y2
y1 ×
× y1 ⊕ x1
y2 ⊕ x2
FIGURE 2.34
An invertible copy gate using the cnot and swap gates – functionally equivalent to that of
Figure 2.33.
as depicted in Figure 2.35, and hence, by setting y1 = y2 = 0,
cnot(x1, x2, 0, 0) = (x1, x2, x1, x2)
sometimes written concisely as
cnot(x1x2, 00) = (x1x2, x1x2).
x1
x2
x1
x2
y2
y1 y1 ⊕ x1
y2 ⊕ x2
FIGURE 2.35
Copying a binary string using the cnot gate.
More generally
cnot(x1x2 · · · xn, 0
n
) = (x1x2 · · · xn, x1x2 · · · xn).
2.10 Periodic functions
2.10.1 Real-valued periodic functions
Definition 2.12 Periodic function
Consider a real-valued function f : R → R. If for some value p ∈ R\0, we have
f(x) = f(x + p)
for all x, then f is said to be periodic. The smallest p for which this is true is called the
period of f.Periodic functions 57
The graph of such a function has a definite pattern which is repeated at regular intervals.
Example 2.10.1
The continuous function f(x) = sin x shown in Figure 2.36 has period p = 2π. Observe that
2π is the smallest interval on the x axis over which the pattern repeats.
1
0.5
-0.5
-1
π 2π
f(x)
x
f(x) = sinx
3π 4π
FIGURE 2.36
The function f(x) = sin x with period p = 2π.
Example 2.10.2
Figure 2.37 shows a graph of a periodic function which is discontinuous (i.e., there are jumps
in the graph). The function is defined as
f(x) = x, 0 ≤ x < 1, period p = 1.
Observe (by introducing the dashed extension to the line in Figure 2.37) that in the interval
1 ≤ x < 2 the function is defined by f(x) = x − 1. Likewise, in the interval 2 ≤ x < 3, the
function is defined by f(x) = x − 2, and so on.
0 1 2
f (x)
x
-1
1
-2
-1
3
FIGURE 2.37
A discontinuous, periodic function with period p = 1.
Exercises
2.20 By sketching a graph, or otherwise, show that a one-to-one function cannot be
periodic.58 Functions and their application to digital gates
2.21 If f : R → R is a constant function, i.e., f(x) = c, for all x ∈ R, show that f is
periodic but has no period.
2.22 If f : R → R is periodic with period p show that f(x ± np) = f(x) for all n ∈ N.
2.10.2 Periodic Boolean functions
Following the case of real functions, we now consider Boolean functions which exhibit peri￾odicity.
Definition 2.13 Periodic Boolean function
The Boolean function f : B
n → B
n is periodic if there exists s ∈ B
n, s 6= 0n, such that
f(x) = f(x ⊕ s)
for all x ∈ B
n.
An understanding of periodic Boolean functions is essential for the development of some
quantum algorithms, e.g., Simon’s algorithm (see Chapter 23).
Example 2.10.3
Consider the Boolean function f : B
3 → B
3 defined in Table 2.22. Show that this function
is periodic with period s = 001.
TABLE 2.22
A Boolean function
f : B
3 → B
3
x f(x)
000 000
001 000
010 000
011 000
100 101
101 101
110 111
111 111
Solution
Table 2.23 shows the result of calculating x ⊕ s and f(x ⊕ s). For example, when x = 111,
x ⊕ s = 111 ⊕ 001 = 110, and f(110) = 111 = f(111). Observe, by carefully inspecting
the table, that for each x, f(x ⊕ s) = f(x), and hence, the function is periodic with period
s = 001.
Exercises
2.23 Consider the Boolean function f : B
3 → B
3 defined in Table 2.24.
(a) Show that this function is many-to-one (in fact four-to-one).End-of-chapter exercises 59
TABLE 2.23
x ⊕ s and f(x ⊕ s) when
s = 001
x f(x) x ⊕ s f(x ⊕ s)
000 000 001 000
001 000 000 000
010 000 011 000
011 000 010 000
100 101 101 101
101 101 100 101
110 111 111 111
111 111 110 111
TABLE 2.24
000 111
001 010
010 010
011 111
100 010
101 111
110 111
111 010
(b) Show that this function is periodic with period s = 011.
(c) Show that this function is also periodic with period s = 101. Deduce that the
period of a periodic Boolean function may not be unique.
2.11 End-of-chapter exercises
1. Explain why the relation depicted in Figure 2.38 is not a function.
FIGURE 2.38
2. Explain why the functions f : B → B depicted in Figure 2.39 are not invertible.
3. If x, y ∈ B show that x ⊕ y = x ⊕ y.
4. Tabulate the function f : B
2 → B
2
, f(x, y) = (y, x ⊕ y) and determine whether
it is bijective.
5. Tabulate the function f : B
2 → B
2
, f(x, y) = (x∧y, x ∧ y) and determine whether
it is one-to-one, two-to-one or neither.60 Functions and their application to digital gates
0 0
1 1
0 0
1 1
B B B B
f : B → B f : B → B
FIGURE 2.39
6. Show that the function f : B
2 → B, f(x, y) = (x ∧ y) ∨ (x ∧ y) is equivalent to
the function f(x, y) = x ⊕ y.
7. Determine whether the function f : B
2 → B
2
, f(x, y) = (x⊕y, x∧y) is invertible.
8. Show that the function f : B
3 → B
3
, f(x, y, z) = (x, x⊕y, z⊕(x∧y)) is invertible.
9. Show the logical equivalence expressed in De Morgan’s law: x ∧ y = x ∨ y.
10. Consider the three functions
i : B → B, i(x) = x; FD(x, y) : B
2 → B
2
, FD(x, y) = (x, x ⊕ y);
TD : B
3 → B
3
, TD(x, y, z) = (x, y, z ⊕ (x ∧ y)).
Use the definition of the Cartesian product of two functions to show that
(FD × i) ◦ TD(x, y, z) = (FD × i)(x, y, z ⊕ (x ∧ y))
= (FD(x, y), i(z ⊕ (x ∧ y))),
which is equivalent to the digital Peres gate (Example 2.5.6).
11. Consider the function f : B
3 → B
3
tabulated in Table 2.25. State the range of
this function. Explain why the co-domain is not the same as the range. Show that
this function is two-to-one.
TABLE 2.25
See Q11
000 011
001 010
010 010
011 011
100 000
101 001
110 001
111 000
12. Consider the function f : R → R. Let x1, x2 ∈ R. Suppose f(x1) = f(x2). Show
that
(a) if x1 6= x2 then f is many-to-one,
(b) if f is one-to-one then x1 = x2,
(c) if f(x1) = f(x2) implies x1 = x2 then f is one-to-one.
13. Consider f : R
2 → R
2
, f(x, y) = (x − 7y, 2x + y). Prove that f is a one-to-one
function.End-of-chapter exercises 61
14. To demonstrate that a function f : A → B is surjective, it must be possible to
choose any element b ∈ B, and then find an element a ∈ A such that f(a) = b.
(a) Show that the function f : R → R, f(x) = 3x + 5 is surjective.
(b) Show that the function f : R
+ → R, f(x) = log2 x is surjective.
15. A Boolean gate G : B
k → B
k
is said to be linear if G(a ⊕ b) = G(a) ⊕ G(b) for all
a, b ∈ B
k
.
(a) Show that the Feynman gate FD(x, y) = (x, y ⊕ x) is linear on B
2
. Hint: you
need to show FD(x1, y1) ⊕ FD(x2, y2) = FD((x1, y1) ⊕ (x2, y2)).
(b) Show that the double Feynman gate F
∗
D(x, y, z) = (x, y ⊕ x, z ⊕ x) is linear
on B
3
.
16. For x1, x2, x3 ∈ B, by tabulating the functions show that
(x1 ∧ x2) ∧ x3 = x1 ∧ (x2 ∧ x3).
Deduce the associativity of ∧ and note that we can simply write x1 ∧ x2 ∧ x3
without any ambiguity.3
Complex numbers
3.1 Objectives
The objective of this chapter is to introduce complex numbers and to develop fluency in
their manipulation. It is helpful to think of a complex number as an object of the form
a + ib where a and b are real numbers called the real and imaginary parts of the complex
number. The symbol i is referred to as the imaginary unit and has the unusual property
that i2 = −1. This is unusual in the sense that there is no real number which when squared
yields a negative result, and so i is not real. Alternatively, complex numbers can be viewed
as ordered pairs of real numbers, e.g. (a, b), upon which we impose rules for addition and
multiplication. Both of these approaches are discussed in this chapter.
A knowledge of complex numbers is essential for understanding quantum computation.
This is because the state of a quantum system is described by mathematical objects called
‘vectors’ that reside in a ‘complex vector space’. In this chapter we shall introduce com￾plex numbers as solutions of quadratic equations, before explaining how calculations are
performed with them. Three common algebraic formats in which complex numbers can
be represented are described – Cartesian, polar and exponential forms – and we explain a
graphical representation known as an Argand diagram, which helps visualisation. This chap￾ter contains essential foundations required for an understanding of complex vector spaces,
tensor product spaces and quantum algorithms which follow.
3.2 Introduction to complex numbers and the imaginary number i
Suppose we wish to solve the quadratic equation 2x
2 + 2x + 5 = 0. This equation will not
factorise using real numbers. The usual procedure would be to attempt to solve it using the
quadratic formula for solving ax2 + bx + c = 0:
x =
−b ±
√
b
2 − 4ac
2a
=
−2 ±
p
4 − 4(2)(5)
4
=
−2 ±
√
−36
4
.
Note the need to take the square root of the negative number −36. But there is no real
number whose square is −36, so it is impossible to proceed without introducing a number
which is not real – called an imaginary number – symbolised by i, with the property that
i
2 = −1, i= √
−1. With this development, we can write √
−36 as √
36×
√
−1 = 6i and write
DOI: 10.1201/9781003264569-3 6364 Complex numbers
the solutions of the quadratic equation as
x =
−2 ± 6i
4
=
−1 ± 3i
2
= −
1
2
±
3
2
i.
We can now formally write down two solutions of the quadratic equation, namely, x =
−
1
2 +
3
2
i and x = −
1
2 −
3
2
i. These numbers are called complex numbers. Observe that
each is made up of two parts, a real part, −
1
2
, and an imaginary part, ±
3
2
. In the general
case we will often use the letter z to denote a complex number.
Definition 3.1 Cartesian form of a complex number
Consider the number z given by
z = a + ib where a ∈ R, b ∈ R.
Here, a is the real part of z, written a = Re(z), and b is the imaginary part of z, written
b = Im(z). The form z = a + ib is referred to as the Cartesian form. We use the symbol
C to denote the set of all complex numbers.
Note that all real numbers are complex numbers with imaginary part b equal to zero, and
so R ⊂ C.
Exercises
3.1 Obtain the solutions of the quadratic equation x
2 + x + 1 = 0 and identify their
real and imaginary parts.
3.2 Given that i2 = −1 obtain simplified expressions for (a) i3
, (b) i4
, (c) i5
.
3.3 Given that x = 2 is a solution of the cubic equation 3x
3−11x
2+16x−12 = 0 find
the remaining two solutions. (Hint: write the cubic as (x − 2)(ax2 + bx + c) = 0.)
3.4 Find a quadratic equation which has solutions z = 5 + 2i and z = 5 − 2i.
3.5 Evaluate

i
√
2
2
,a)






i
√
2
2





.b)
3.3 The arithmetic of complex numbers
Given two (or more) complex numbers, addition (and subtraction) is performed in a natural
way by adding (or subtracting) the real and imaginary parts separately. Thus if z1 = 3 − 5i
and z2 = 8 + 4i then
z1 + z2 = (3 − 5i) + (8 + 4i) = (3 + 8) + (−5i + 4i) = 11 − i
z1 − z2 = (3 − 5i) − (8 + 4i) = (3 − 8) + (−5i − 4i) = −5 − 9i.
Likewise, multiplication by a real number is performed in a natural way:
7z1 = 7(3 − 5i) = 21 − 35i.The arithmetic of complex numbers 65
More generally, to multiply two complex numbers, we make use of the fact that i2 = −1.
Thus
z1z2 = (3 − 5i)(8 + 4i) = 24 − 40i + 12i − 20i2 = 44 − 28i.
In general, addition, subtraction and multiplication are achieved as follows: given complex
numbers z1 = a1 + b1i, z2 = a2 + b2i, we define
z1 + z2 = (a1 + b1i) + (a2 + b2i) = (a1 + a2) + (b1 + b2)i
z1 − z2 = (a1 + b1i) − (a2 + b2i) = (a1 − a2) + (b1 − b2)i
z1z2 = (a1 + b1i)(a2 + b2i) = (a1a2 − b1b2) + (a1b2 + a2b1)i.
Some authors prefer to introduce complex numbers as ordered pairs of real numbers, writing
a + ib as (a, b). Then the rules of addition, subtraction and multiplication become:
(a1, b1) + (a2, b2) = (a1 + a2, b1 + b2)
(a1, b1) − (a2, b2) = (a1 − a2, b1 − b2)
(a1, b1)(a2, b2) = (a1a2 − b1b2, a1b2 + a2b1).
Note that this approach is entirely consistent with the Cartesian form given previously.
Exercises
3.6 If z1 = 3 + 8i, z2 = −3 − 2i find
a) z1 + z2 b) 4 z1 − z2 c) z1 + 3z2
z
2 d) 1
z1z2.e)
3.7 State Re(z) and Im(z) when z = (2 + 3i)(5 − 2i).
Definition 3.2 Complex conjugate
The complex conjugate of any complex number z, which we shall write as z
∗
, is found
by changing the sign of the imaginary part. Thus
if z = a + ib then z
∗ = a − ib.
(Some authors write the conjugate of z as z but throughout this book we will write z
∗
.)
Exercises
3.8 Given z1, z2 ∈ C, show that (z1 + z2)
∗ = z
∗
1 + z
∗
2
.
3.9 Given z1, z2 ∈ C, show that (z1 − z2)
∗ = z
∗
1 − z
∗
2
.66 Complex numbers
Observe that if z = a + ib
z z∗ = (a + ib)(a − ib)
= a
2 + bai − abi − b
2
i
2
= a
2 + b
2
(since i2 = −1)
which will always be real.
For the division of complex numbers use is made of the conjugate. Consider the following
example.
Example 3.3.1
Calculate z1
z2
when z1 = 3 − 5i and z2 = 8 + 4i.
Solution
z1
z2
=
3 − 5i
8 + 4i
.
We now multiply both the numerator and denominator of this fraction by the complex
conjugate of the denominator and then simplify the result:
z1
z2
=
3 − 5i
8 + 4i
×
8 − 4i
8 − 4i
=
24 − 40i − 12i − 20
64 + 16
=
4 − 52i
80
=
1
20
−
13
20
i.
In general, division of complex numbers is achieved in the following way:
If z1 = a1 + b1i and z2 = a2 + b2i then
z1
z2
=
a1 + b1i
a2 + b2i
=
a1 + b1i
a2 + b2i
×
a2 − b2i
a2 − b2i
=
a1a2 + b1b2 + i(a2b1 − a1b2)
a
2
2 + b
2
2
.
Exercises
3.10 If z1 = 4 + 4i, z2 = 3 + 5i find (a) z1
z2
, (b) z2
z1
.
3.11 If z ∈ C show that (a) z + z
∗ = 2 Re(z), (b) z − z
∗ = 2i Im(z).
3.12 Find the real and imaginary parts of
2
4 + i
−
3
2 − i
,a)
1
i
− i,b)
1
i
2 + i
.c)The set of all complex numbers as a field 67
3.4 The set of all complex numbers as a field
The set of all complex numbers, C, together with the operations of addition and multi￾plication as defined above form a mathematical structure called a field. Strictly, in order
to verify this fact, there are several requirements known as field axioms (see Appendix
D) which must be checked. Essentially, the statement that (C, +, ×) is a field means that
we are able to add, subtract and multiply any complex number and we can divide by any
non-zero complex number and still remain in the field. This field has an additive identity
element, zero, which is the complex number with both real and imaginary parts equal to
zero: z = 0 + 0i = 0. The multiplicative identity element is the real number 1.
3.5 The Argand diagram and polar form of a complex number
The Argand diagram is a graphical representation of a complex number. Using Cartesian
xy axes, the real part of the complex number z = a + ib is plotted on the horizontal axis (x
axis) and the imaginary part on the vertical axis (y axis). The complex number z is thus
represented by the point with coordinates (a, b) as shown in Figure 3.1.
Observe that by introducing polar coordinates (r, θ) (see Appendix C), we can write
a = r cos θ, b = r sin θ so that
z = a + ib = r cos θ + ir sin θ = r(cos θ + i sin θ).
x
0
y
a
b z = a + ib
r
θ
(a, b)
FIGURE 3.1
Argand diagram showing both Cartesian and polar forms of z = a + ib.
Definition 3.3 Polar form of a complex number
The form z = r(cos θ + i sin θ) is referred to as the polar form of the complex number,
sometimes abbreviated to r∠θ.
Here r ≥ 0, a real number, is the modulus or magnitude of z, written |z|. The angle θ
is the argument of z, arg(z), also commonly referred to as the phase or amplitude of
the complex number. Usually, we choose θ to lie in the interval −π < θ ≤ π. Observe that
|z| = r =
√
a
2 + b
2 and tan θ =
b
a
.
The following theorem shows how to carry out multiplication and division of complex
numbers given in polar form:68 Complex numbers
Theorem 3.1 Multiplication and division in polar form
If z1 = r1∠θ1 = r1(cos θ1 + i sin θ1) and z2 = r2∠θ2 = r2(cos θ2 + i sin θ2) then
z1z2 = r1r2∠(θ1 + θ2)
= r1r2(cos(θ1 + θ2) + i sin(θ1 + θ2))
that is, the moduli are multiplied and the arguments are added. Further,
z1
z2
=
r1
r2
∠(θ1 − θ2)
=
r1
r2
(cos(θ1 − θ2) + i sin(θ1 − θ2))
that is, the moduli are divided and the arguments are subtracted.
Proof of these results is left as an exercise (below).
Exercises
3.13 Plot the following complex numbers on an Argand diagram and calculate the
modulus and argument of each:
z = 3 + 4i,a) z = −1 − i,b) z = 3,c)
z = 2i,d) z = π + πi.e)
3.14 Find the modulus and argument of z1 = −
√
3 + i and z2 = 4 + 4i. Hence express
z1z2 and z1
z2
in polar form.
3.15 Use the trigonometric identities:
sin(A ± B) = sin A cos B ± cos A sin B
cos(A ± B) = cos A cos B ∓ sin A sin B
to prove that if z1 = r1(cos θ1 + i sin θ1) and z2 = r2(cos θ2 + i sin θ2) then
z1z2 = r1r2(cos(θ1 + θ2) + i sin(θ1 + θ2))
and
z1
z2
=
r1
r2
(cos(θ1 − θ2) + i sin(θ1 − θ2)).
3.16 For any z ∈ C show that z z∗ = |z|
2
.
3.17 Suppose z1 = 3 + 2i and z2 = 4 + 5i.
(a) Find |z1| |z2| and |z1z2| and show that these are equal.
(b) Find




z1
z2




and |z1|
|z2|
and show that these are equal.
(c) Show that the result in part (a) is true for any pair of complex numbers.
(d) Show that the result in part (b) is true for any pair of complex numbers
provided the denominator is non-zero.
3.18 Show that each of the complex numbers
z1 =
1
√
3
, z2 = −
1
√
3
, z3 =
i
√
3
, z4 = −
i
√
3
has the same magnitude.The exponential form of a complex number 69
3.6 The exponential form of a complex number
We now derive a third form in which a complex number can be expressed – the exponential
form. The derivation requires the use of Maclaurin series which we discuss first. Maclaurin
series expansions are a way of representing functions as an infinite sum of terms involving
increasing powers of a variable x, say. Three commonly met expansions are the so-called
power series for ex
, sin x and cos x:
e
x = 1 + x +
x
2
2! +
x
3
3! +
x
4
4! +
x
5
5! + · · ·
sin x = x −
x
3
3! +
x
5
5! − · · · (x in radians)
cos x = 1 −
x
2
2! +
x
4
4! − · · · (x in radians)
which are valid for any real value of x. The derivation of these results can be found in most
Calculus textbooks. Using the expansions for cos x and sin x and replacing x with θ, we can
write
cos θ + i sin θ = (1 −
θ
2
2! +
θ
4
4! − · · ·) + i(θ −
θ
3
3! +
θ
5
5! − · · ·)
= 1 + iθ −
θ
2
2! − i
θ
3
3! +
θ
4
4! + · · ·
and from the Maclaurin series expansion of the exponential function, replacing x with iθ,
we note
e
iθ = 1 + iθ −
θ
2
2! − i
θ
3
3! +
θ
4
4! + · · ·
so that
e
iθ = cos θ + i sin θ.
This result is known as Euler’s relation. The similar relation e−iθ = cos θ − i sin θ is
straightforward to deduce. We thus have an alternative expression for the complex number
z = r(cos θ + i sin θ):
Definition 3.4 The exponential form of a complex number
z = re
iθ
is the exponential form of a complex number. Here r is a non-negative real number, the
modulus of z, i.e., |z|, and usually the angle θ is chosen so that −π < θ ≤ π.
When constructing quantum circuits it will be necessary to manipulate expressions involving
complex numbers in polar and exponential forms. Consider the following example.
Example 3.6.1
Evaluate (a) e2πi
, (b) e−πi
, (c) e π
2
i
.
Solution
(a) e2πi = cos 2π + i sin 2π = 1 + 0i = 1. Alternatively observe that e2πi has modulus 1 and
argument 2π and hence corresponds to the point on the Argand diagram with Cartesian
coordinates (1, 0).70 Complex numbers
(b) e−πi = cos(−π) + i sin(−π) = −1 + 0i = −1.
(c) e π
2
i = cos π
2 + i sin π
2 = 0 + 1i = i.
Example 3.6.2
Use the first of Euler’s relations together with the trigonometric identity cos2
θ + sin2
θ = 1
to show that
1 + e2iθ = 2(cos2
θ + i sin θ cos θ).
Solution
First note that
e
2iθ = eiθ
e
iθ
= (cos θ + i sin θ)(cos θ + i sin θ)
= cos2
θ − sin2
θ + 2i sin θ cos θ.
Then
1 + e2iθ = 1 + cos2
θ − sin2
θ + 2i sin θ cos θ
= 2 cos2
θ + 2i sin θ cos θ
= 2(cos2
θ + i sin θ cos θ)
as required.
Exercises
3.19 Evaluate (a) e−2πi
, (b) eπi
, (c) e−πi/2
, (d) e−5πi/2
.
3.20 Evaluate (a) e2πi/3
, (b) e4πi/3
, (c) e−2πi/3
, (d) e−4πi/3
.
3.21 Show that 1 − e
2iθ = 2(sin2
θ − i cos θ sin θ).
3.22 Express z = e1+iπ/2
in the form a + ib.
Theorem 3.2 Multiplication and division in exponential form
Given two complex numbers z1 = r1e
iθ1 and z2 = r2e
iθ2
, their product and quotient are
found from
z1z2 = r1r2e
i(θ1+θ2)
,
z1
z2
=
r1
r2
e
i(θ1−θ2)
.
Proof
z1z2 = (r1e
iθ1
)(r2e
iθ2
) = r1r2e
i(θ1+θ2)
z1
z2
=
r1e
iθ1
r2e
iθ2
=
r1
r2
e
i(θ1−θ2)
.
The exponential form of a complex number 71
Example 3.6.3 Multiplication by a complex number of modulus 1: global phase
Show that multiplying an arbitrary complex number z1 = r1e
iθ1 by any complex number
with modulus equal to 1, does not change its modulus.
Solution
Let z2 = 1eiθ2 be any complex number with modulus 1. Then
z2z1 = (1eiθ2
)(r1e
iθ1
) = r1e
i(θ2+θ1)
.
Observe that whilst the argument has changed, the modulus is unaltered. Multiplication
of a complex number by another with modulus 1, i.e., eiφ
say, will be important when we
introduce global phase factors in Chapter 13.
Example 3.6.4 Relative phase of two complex numbers
Consider the two complex numbers z1 = r1e
iθ1 and z2 = r2e
iθ2 shown on the Argand
diagram in Figure 3.2. The angle between the two complex numbers represented in this way
is θ1 − θ2. This quantity, or sometimes ei(θ1−θ2 ), is referred to as the relative phase of the
two complex numbers. Observe that
z1
z2
=
|z1|
|z2|
e
i(θ1−θ2)
and hence the relative phase can be expressed as
e
i(θ1−θ2) =
z1/z2
|z1|/|z2|
.
x
0
y
r
θ
θ1
2
r2
1
z1
z2
θ1 − θ2
FIGURE 3.2
The angle θ1 − θ2 between the two complex numbers is the relative phase.
Exercises
3.23 Use Euler’s relations to express cos θ and sin θ in terms of exponential functions.72 Complex numbers
3.24 Show that 1
cos θ + i sin θ
= cos θ − i sin θ.
3.25 Express 1 + e2iωt in the form a + ib.
3.26 Calculate the relative phase of z1 = 2eiπ/3 and z2 = 5eiπ/6
.
3.7 The Fourier transform: an application of the exponential form
of a complex number
The exponential form of a complex number is an essential ingredient of Fourier series, the
Fourier transform and the discrete Fourier transform. These tools are used in the study of
quantum algorithms.
Definition 3.5 Discrete Fourier transform
Suppose we have a continuous function f(t) which we sample at intervals T. We obtain a
sequence of n sample values f(0), f(T), f(2T), . . . , f((n − 1)T). Without loss of generality
assume T = 1, so that we obtain the samples
f(0), f(1), f(2), . . . , f(n − 1), that is f(j), j = 0, 1, 2, . . . , n − 1.
The discrete Fourier transform (d.f.t.) is another sequence ˆf(k), k = 0, 1, 2, . . . , n − 1,
defined by
ˆf(k) = 1
√
n
nX−1
j=0
f(j)e−2πikj/n for k = 0, 1, 2, . . . , n − 1.
There are several variants of this definition. Some authors include a factor of 1
n
rather
than √
1
n
used here. Others use a positive exponential instead of the negative one used
above. What is critical is that you know which formula is being used and that you apply it
consistently.
Example 3.7.1
Find the discrete Fourier transform of the sequence f(j) = 1, 1, 3, 1.
Solution
Here the number of terms in the sequence, n, is 4. We evaluate the d.f.t. for each value of
k in turn using ˆf(k) = 1
√
n
nX−1
j=0
f(j)e−2πikj/n
.
k = 0
ˆf(0) = 1
√
4
X
3
j=0
f(j) = 1
2
(1 + 1 + 3 + 1) = 3.
k = 1
ˆf(1) = 1
√
4
X
3
j=0
f(j)e−πij/2 =
1
2
(1 + e−πi/2 + 3e−πi + e−3πi/2
) = 1
2
(1 − i − 3 + i) = −1.The Fourier transform: an application of the exponential form of a complex number 73
k = 2
ˆf(2) = 1
√
4
X
3
j=0
f(j)e−πij =
1
2
(1 + e−πi + 3e−2πi + e−3πi
) = 1
2
(1 − 1 + 3 − 1) = 1.
k = 3
ˆf(3) = 1
√
4
X
3
j=0
f(j)e−3πij/2 =
1
2
(1 + e−3πi/2 + 3e−3πi + e−9πi/2
) = 1
2
(1 + i − 3 − i) = −1.
Finally, ˆf(k) = 3, −1, 1, −1.
Example 3.7.2 The Hadamard transform
Consider the sequence of just two terms f(0), f(1). Find its discrete Fourier transform.
Solution
From the formula for the d.f.t. with n = 2, we have
ˆf(k) = 1
√
2
X
1
j=0
f(j)e−πikj =
1
√
2
(f(0) + f(1)e−πik).
k = 0
ˆf(0) = 1
√
2
(f(0) + f(1))
k = 1
ˆf(1) = 1
√
2
(f(0) + f(1)e−πi
) = 1
√
2
(f(0) − f(1)).
Hence, the sequence f(0), f(1) is transformed by the discrete Fourier transform to the
sequence √
1
2
(f(0) +f(1)), √
1
2
(f(0)−f(1)). This particular version of the Fourier transform
is referred to as the Hadamard transform and is fundamental in quantum computation.
Exercises
3.27 By writing λ = e2πi/n show that the d.f.t. can be expressed as:
ˆf(k) = 1
√
n
nX−1
j=0
f(j)λ
−kj for k = 0, 1, 2, . . . , n − 1.
3.28 The inverse discrete Fourier transform of the sequence ˆf(k), k = 0, 1, 2, . . . , n − 1
is given by
f(j) = 1
√
n
nX−1
k=0
ˆf(k)e2πikj/n for j = 0, 1, 2, . . . , n − 1.
Calculate the inverse d.f.t of ˆf(k) = 1, 6 + i, −9, 6 − i.
3.29 Given a sequence of three terms f(0), f(1), f(2) find an explicit expression for
each term, ˆf(k), in the discrete Fourier transform.74 Complex numbers
3.8 End-of-chapter exercises
1. Express the following complex numbers in Cartesian form:
(a) 1
2 + 3i
(b) 1
x − iy
.
2. Show that 1
cos θ − i sin θ
= cos θ + i sin θ.
3. Show that for any z1, z2 ∈ C, (z1z2)
∗ = z
∗
1
z
∗
2
.
4. Suppose z1 = x1 + iy1 = r1e
iθ1
, z2 = x2 + iy2 = r2e
iθ2
. Show that
|z1 + z2|
2 = |z1|
2 + |z2|
2 + |z1| |z2|(ei(θ2−θ1) + ei(θ1−θ2)
)
and that this can also be written, using Euler’s relations, as
|z1 + z2|
2 = |z1|
2 + |z2|
2 + 2|z1| |z2| cos(θ2 − θ1).
5. Consider the complex number z = re
iθ
. Show that the effect of multiplication by
i is an anti-clockwise rotation by π
2
on the Argand diagram.
6. Show that the set of complex numbers C with the operation of addition is a group
(C, +).
7. Show that the subset of complex numbers {1, −1, i, −i} with the operation of
multiplication is a group.
8. De Moivre’s theorem: Use Euler’s relation eiθ = cos θ + i sin θ to show that
when n ∈ N
(cos θ + i sin θ)
n = cos nθ + i sin nθ.
9. If z =
e
iθ
√
2
show that |z
2
| =
1
2
.
10. By writing z in polar form as z = r(cos θ+i sin θ) and using De Moivre’s theorem
(see Question 8 above) find the three solutions of the equation z
3 = 1. (Hint:
write 1 = 1(cos 2kπ + i sin 2kπ), k = 0, 1, 2.)
11. Show that if θ =
π
2
then z =
e
iθ
√
2
=
i
√
2
.4
Vectors
4.1 Objectives
Mathematical representations of the state of a quantum system are provided by vectors.
These also hold information about the probability that the system will be in a particular
state once a measurement has been made. In this chapter we shall introduce the qubit, a
quantum bit, which is represented as a vector and show how qubits are manipulated. We
show how two or more vectors can be multiplied by scalars (real or complex numbers) and
added together to yield new vectors. Such combinations lead to the important concept of
linear superposition and pave the way for a more general treatment in Chapter 6 Vector
spaces. Finally, we introduce the ‘inner product’ of two vectors, which in turn leads to the
important concepts of norm and orthogonality.
4.2 Vectors: preliminary definitions
Definition 4.1 Row vector.
An ordered set of n real numbers, ui, i = 1, . . . , n, written in the pattern or array
u = (u1, u2, . . . , un)
is referred to as an n-dimensional row vector of real numbers.
Some authors indicate vectors using a bold font as in u. We shall avoid this practice except
in some situations where there is ambiguity. The vector u is an element of the set R
n. So,
for example, the vector given by the ordered pair of real numbers
u = (8, −19)
is an element of the Cartesian product R × R = R
2
. The ordered triple
v = (3.1, −2.8, 0)
is a row vector in R
3
.
Definition 4.2 Column vector.
An ordered set of n real numbers, vi, i = 1, . . . , n, written in the pattern or array
v =


v1
v2
.
.
.
vn


is referred to as an n-dimensional column vector of real numbers.
DOI: 10.1201/9781003264569-4 7576 Vectors
For example,
v =


3.2
−9.0
0
7.5


is a column vector in R
4
. In quantum computation the entries, or elements, in row and
column vectors will generally be complex numbers, so for example
c = (2 + i, 5 + 3i) and d =

5 − i
−9 − 2i 
are vectors in C
2
. Whether we work with row vectors or column vectors is usually determined
by the context.
When working with vectors in R
n, elements of the underlying field, in this case R, are
referred to as scalars. Likewise, when working with vectors in C
n, the term scalar means
a complex number, i.e., an element of C.
If every element of a vector in R
n or C
n is the scalar 0, we refer to it as the zero vector
0 = (0, 0, 0, . . . , 0). Note there is some potential confusion between the scalar 0 and the zero
vector 0 so the reader needs to be aware of the context and choose accordingly.
In some quantum algorithms the underlying set will be B = {0, 1}. The following two
vectors are elements of B
4 and B
3
, respectively:
x = (1, 0, 1, 1), y =


0
1
1

 .
Quantum computation uses a special notation – known as Dirac bra-ket notation – to
denote the state of a quantum system. A ket, symbolised by | i, can be regarded as a
column vector. For example, we can write d above as
|di =

5 − i
−9 − 2i 
.
Likewise, a bra, written h |, can be regarded as a row vector. For example, we can write c
above as
hc| = (2 + i, 5 + 3i).
We will have occasion to write a specific ket as a bra and care must be taken to observe the
following convention which, though strange at first, lays the groundwork for inner products
which will follow. The bra hd| corresponding to the ket |di is found by forming a row vector
using the same elements but taking the complex conjugate of each. Thus we can write
|di =

5 − i
−9 − 2i 
, hd| = (5 + i, −9 + 2i)
and say that the bra hd| is the Hermitian conjugate or conjugate transpose of the ket
|di.
In quantum computation the two column vectors, in C
2
,

1
0

and 
0
1

are used
frequently and so we introduce the following notation and terminology.
Definition 4.3 |0i and |1i
ket 0 = |0i =

1
0

ket 1 = |1i =

0
1

.Graphical representation of two- and three-dimensional vectors 77
Exercises
4.1 Write down the bra corresponding to the ket |ψi = √
1
3

i
1 + i 
.
4.2 Write down the ket corresponding to the bra hφ| = ( √
1
2
, √
1
2
).
4.3 Graphical representation of two- and three-dimensional vectors
When working with two- and three-dimensional vectors of real numbers it is often helpful
to use a graphical representation in which directed line segments are used to represent
the vectors. A directed line segment is just a segment of a straight line with an arrow to
indicate its direction. With such a representation it is straightforward to introduce the length
or magnitude of a vector, the angle between two vectors, and orthogonality which occurs
when two vectors are perpendicular. These notions readily generalise to higher dimensions
and are important in quantum computation as we shall see.
Consider the xy Cartesian coordinate system and the point, P, with coordinates (x1, y1),
shown in Figure 4.1 where it is denoted P(x1, y1).
O
P
Q
θ
x
y
z
x
y
O
(x1 , y1)
R
x1
y1
x3
y3
z3
r1
r2
r3
(x3, y3, z3)
(x2 , y2)
FIGURE 4.1
Graphical representation of position vectors in R
2 and R
3
.
It is natural to draw a directed line segment from the origin to the point P. This di￾rected line segment is referred to as the position vector, r1, of P and we can write
r1 =
−−→OP =

x1
y1

. We say that x1 and y1 are the components of vector r1 relative to the
given xy coordinate system. Likewise, the position vector of the point Q, with coordinates
(x2, y2), is r2 =
−−→OQ =

x2
y2

. The position vector of the point R(x3, y3, z3) in three￾dimensional space is then r3 =


x3
y3
z3

. Two remarks are worth making here: if we use a
different coordinate system these components may change. Also, whilst we have introduced
the graphical representation using column vectors, we could equally have used row vectors
such as r1 = (x1, y1). There is then potential for some confusion because (x1, y1) refers to78 Vectors
both the coordinates of a point and the position vector of that point. This distinction is of
no great concern in what follows.
With these graphical representations in mind we can readily introduce the ‘length’ of a
vector r of real numbers, written |r|, by making use of Pythagoras’ theorem:
length of r1 = |r1| =
q
x
2
1 + y
2
1
, length of r2 = |r2| =
q
x
2
2 + y
2
2
length of r3 = |r3| =
q
x
2
3 + y
2
3 + z
2
3
.
In the context of a more general column or row vector of real or complex numbers, u say,
we have the following via a generalisation of Pythagoras’ theorem:
Definition 4.4 Magnitude or norm of a vector
The magnitude or norm, written kuk, of a vector of real or complex numbers:
if u = (u1, u2, . . . , un), kuk =
p
|u1|
2 + |u2|
2 + . . . + |un|
2.
Note that it is necessary to take the modulus of each complex component prior to squaring
it. This ensures that the norm is a real number. We shall discuss this aspect further when
we study inner products in Section 4.6.
If kuk = 1 then u is referred to as a unit vector. A unit vector is frequently, but not
always, indicated using the hat symbol, as in ˆr. Thus far, the two- and three-dimensional
vectors of real numbers shown have been tied to the origin. In general this is not neces￾sary and so-called free vectors can be placed at any location provided their lengths and
directions are maintained.
Example 4.3.1 The standard orthonormal basis in R
2 and in R
3
Figure 4.2a shows vectors drawn from the origin to the points (1, 0) and (0, 1). We shall
denote these as i =

1
0

and j =

0
1

. Clearly i and j are unit vectors along the x
and y axes and they are mutually perpendicular or orthogonal. As we shall see, these two
vectors can be used to construct any other vector in R
2
. We refer to i and j as the standard
orthonormal basis of R
2
.
Moving to three dimensions the standard orthonormal basis in R
3
is i =


1
0
0

, j =


0
1
0

 and k =


0
0
1

 (Figure 4.2b ).
Example 4.3.2
Consider the ket |ψi =
 
√
1
2
√
1
2
!
. Show that this is a unit vector.
Solution
k |ψi k =
s
1
√
2
2
+

1
√
2
2
=
r
1
2
+
1
2
= 1
thus |ψi is a unit vector.Vector arithmetic: addition, subtraction and scalar multiplication 79
O
x
y
z
x
y
O
1
1
i
j
i
j
k
1
1
1
a) b)
(0, 1)
(1, 0)
FIGURE 4.2
a) The standard orthonormal basis in R
2
, and b) in R
3
.
Exercises
4.3 Show that the bra hφ| = (
√
3
2
,
1
2
) is a unit vector.
4.4 Show that both ket 0 and ket 1 are unit vectors.
4.5 Draw ket 0 and ket 1 as vectors in the xy Cartesian coordinate system. Show
that these vectors are orthogonal (i.e., they are perpendicular).
4.6 Find the norm of the vector (8, −1, 2).
4.7 Find the norm of the vector (i, 1 + i).
4.8 Show that the vector |ψi =
 
√
1
5
√
2
5
!
has unit norm.
4.4 Vector arithmetic: addition, subtraction and scalar
multiplication
Vectors having the same number of components, n say, can be added or subtracted in an
element-by-element fashion to yield another vector of the same size. Any vector can be
multiplied by a scalar (a real or complex number) by multiplying each element of the vector
by that number, again yielding another vector of the same size.
Definition 4.5 Addition and multiplication by a scalar
Given vectors u and v with components ui and vi, respectively, (i = 1, . . . , n), the sum
u + v is a vector with components ui + vi. Given a scalar k, the multiple ku is a vector with
components kui.
For example, if u, v ∈ C
2 and given by
u = (1 + i, 3 − 2i) and v = (7 − 2i, 8 − 3i)
then
u + v = (8 − i, 11 − 5i), 5u = (5 + 5i, 15 − 10i), 2i v = (4 + 14i, 6 + 16i).80 Vectors
This property, that adding two vectors of real or complex numbers, or multiplying a vector
by a scalar, results in other vectors of the same size and nature means that the set of
vectors is closed under these operations. Closure under addition and scalar multiplication
is an important characteristic that we shall return to in Chapter 6 Vector spaces.
Example 4.4.1
Consider the real two-dimensional vectors a =

3
4

and b =

2
1

shown in Figure 4.3.
O
A
 B
C
a =
3
4
a
b =
2
1
b
FIGURE 4.3
Vector addition.
Their sum, a+b, calculated by adding corresponding components, is 
5
5

. Geometrically,
this result can be obtained by translating b so that its tail coincides with the head of a (that
is
−→AC in Figure 4.3). The third side of the triangle thus produced, that is OC, represents
a + b. This is known as the triangle law (or sometimes the parallelogram law) of vector
addition. Now consider the difference a − b =

3
4

−

2
1

=

1
3

. Study Figure 4.3
and note that a − b is represented by the vector from the head of b to the head of a, that
is
−−→BA.
Example 4.4.2
Using vector addition, scalar multiplication and the triangle law of addition any vector in
R
2
can be constructed from the standard orthonormal basis i and j. For example

3
4

= 3 
1
0

+ 4 
0
1

.
Vectors in R
3
can, likewise, be constructed from the basis vectors i, j and k.
Given any two vectors, u, v say, (of the same size), and any two scalars, k, `, we can use
the operations of scalar multiplication and addition to give a new vector. This leads to the
concept of superposition.
Definition 4.6 Superposition
A superposition of the vectors u and v is a linear combination
w = ku + `vQubits represented as vectors 81
where k and ` are real or complex numbers. Generally, given n vectors ui, i = 1, . . . , n
having the same size, the expression
Xn
i=1
kiui
where ki ∈ C, is a linear combination. (Note the ui here are each vectors, and not the
components of a vector u.)
Superpositions of vectors ui play an essential role in quantum computation.
Exercises
4.9 Consider u, v ∈ C
n and k ∈ C. Show from the definitions of vector addition and
scalar multiplication that:
a) u + v = v + u, i.e., vector addition is commutative,
b) (u + v) + w = u + (v + w) i.e., vector addition is associative,
0u = 0 where on the left 0 is the scalar zero and on the right 0 is the zero
vector,
c)
1u = u.d)
4.10 Express the vectors √
1
2

1
1

and √
1
2

−1
1

as a superposition of the kets |0i
and |1i.
4.11 Express the vectors 
1
0

and 
0
1

as a superposition of the kets |ψi and |φi
where |ψi = √
1
2

1
1

and |φi = √
1
2

1
−1

.
4.12 Consider x, y ∈ B
4 given by
x = (1, 1, 0, 1), y = (0, 1, 1, 1).
Using arithmetic modulo 2, show that x + y = (1, 0, 1, 0). Show that this is
equivalent to bit-wise application of the exclusive-or operator ⊕.
4.5 Qubits represented as vectors
A qubit or quantum bit is a mathematical representation of the state of the simplest
quantum system. Chapter 12 discusses qubits in detail. One way in which a qubit can be
described is by a linear combination of the two column vectors 
1
0

and 
0
1

. We
make specific use of the ket notation introduced in Section 4.2 and write |0i =

1
0

and
|1i =

0
1

and their more general linear combination, |ψi say, as
|ψi = α0|0i + α1|1i82 Vectors
where α0 and α1 are any complex numbers such that |α0|
2 + |α1|
2 = 1. (The reason for this
requirement will become apparent in due course). Thus a qubit has an infinity of possible
states which are linear combinations (also known as blends, or superpositions) of the vectors
|0i and |1i.
Example 4.5.1
The vector
|ψi =
1
√
2
|0i +
1
√
2
|1i
is a superposition of the kets |0i and |1i. Note that, using the rules of scalar multiplication
and vector addition, we could write this vector as
|ψi =
1
√
2

1
0

+
1
√
2

0
1

=

√
1
2
0

+

0
√
1
2

=
 
√
1
2
√
1
2
!
.
You should verify that this ket has norm 1 (which is a consequence of requiring |α0|
2+|α1|
2 =
1, as above).
4.5.1 Combining kets and functions f : B → B
When we study quantum algorithms, we will need to be familiar with the manipulation
of kets containing functions, specifically the four functions f : B → B defined in Section
2.4. Here we prove an important result. Working through the Proof will provide excellent
practice in the required manipulations.
Lemma 4.1 If f : B → B then
|f(x)i − |1 ⊕ f(x)i = (−1)f(x)
(|0i − |1i), x ∈ B.
Proof
(a) Consider the balanced, identity function f(x) = x, x ∈ B. Clearly f(0) = 0, f(1) = 1.
So, if x = 0, then f(x) = 0 and
|f(x)i − |1 ⊕ f(x)i = |f(0)i − |1 ⊕ f(0)i
= |0i − |1 ⊕ 0i
= |0i − |1i
= (−1)0
(|0i − |1i)
= (−1)f(x)
(|0i − |1i) as required.
Likewise, if x = 1, then f(x) = 1 and
|f(x)i − |1 ⊕ f(x)i = |f(1)i − |1 ⊕ f(1)i
= |1i − |1 ⊕ 1i
= |1i − |0i
= (−1)1
(|0i − |1i)
= (−1)f(x)
(|0i − |1i) as required.
and therefore the Lemma is true when f(x) = x.The inner product (scalar product, dot product) 83
(b) Consider the balanced, not function f(x) = x, x ∈ B. Then f(0) = 1, f(1) = 0.
|f(x)i − |1 ⊕ f(x)i = |xi − |1 ⊕ xi
=

|1i − |0i if x = 0
|0i − |1i if x = 1
= (−1)f(x)
(|0i − |1i)
and again, the Lemma is true.
(c) Consider the constant function f(x) = 1, x ∈ B.
|f(x)i − |1 ⊕ f(x)i = |1i − |1 ⊕ 1i
= |1i − |0i
= (−1)f(x)
(|0i − |1i).
(d) Consider the constant function f(x) = 0, x ∈ B.
|f(x)i − |1 ⊕ f(x)i = |0i − |1 ⊕ 0i
= |0i − |1i
= (−1)f(x)
(|0i − |1i)
as required. 
4.6 The inner product (scalar product, dot product)
‘Multiplication’ of vectors is important in the study of quantum computation. There are
several ways that the product of two vectors can be defined depending upon the application
that we are interested in. We focus here on the inner product. The inner product is often
referred to as the scalar product or dot product, particularly when the elements of the
vectors are real and the vectors are two- or three-dimensional. In quantum computation the
vectors under consideration will often have complex elements.
4.6.1 The inner product in R
2 and R
3
We begin our study of inner products with vectors in R
2 and R
3
. In fact there are many
different inner products on R
2 and R
3 and we focus here on the so-called Euclidean inner
product because it possesses a helpful geometrical interpretation.
Definition 4.7 Euclidean inner product in R
2 and R
3
Given two vectors u and v we denote their Euclidean inner product, or scalar product, by
hu, vi (some authors write u · v) and define it to be
hu, vi = kukkvk cos θ
where θ is the angle between u and v (Figure 4.4).
By rearrangement, this definition provides a way of writing the cosine of the angle, and
hence the angle between the two vectors:
cos θ =
hu, vi
kukkvk
.84 Vectors
v
θ
u
FIGURE 4.4
The scalar product can be used to determine θ.
Further, observe that
hu, ui = kukkuk = kuk
2
(since cos 0 = 1) and hence the Euclidean inner product can be used to define the modulus
or norm of a vector.
Definition 4.8 The Euclidean norm
kuk =
p
hu, ui.
With hu, vi defined in this way it is possible to show that the inner product of vectors u, v, w
in R
2 and R
3 has the following properties :
1. hu, vi = hv, ui, (commutativity)
2. khu, vi = hku, vi = hu, kvi, where k ∈ R
3. h(u + v), wi = hu, wi + hv, wi, (distributivity)
4. hu, ui ≥ 0 and hu, ui = 0 if and only if u = 0 (positivity).
We now demonstrate an alternative but equivalent formula for the Euclidean inner product
in R
2
, one which we shall generalise shortly to R
n.
Theorem 4.1 Given two vectors u = (u1, u2) and v = (v1, v2), their scalar product hu, vi =
kukkvk cos θ, where θ is the angle between u and v, is given by
hu, vi = u1v1 + u2v2.
Proof
Using the cosine rule, referring to Figure 4.4,
ku − vk
2 =kuk
2+kvk
2 − 2kuk kvk cos θ
=kuk
2+kvk
2 − 2hu, vi.
Writing u = (u1, u2), v = (v1, v2) then
k(u1 − v1, u2 − v2)k
2 = k(u1, u2)k
2 + k(v1, v2)k
2 − 2hu, vi
(u1 − v1)
2 + (u2 − v2)
2 = u
2
1 + u
2
2 + v
2
1 + v
2
2 − 2hu, vi
u
2
1 + v
2
1 − 2u1v1 + u
2
2 + v
2
2 − 2u2v2 = u
2
1 + u
2
2 + v
2
1 + v
2
2 − 2hu, vi
−2u1v1 − 2u2v2 = −2hu, vi
so that
hu, vi = u1v1 + u2v2
as required. Further, observe that hu, ui = u
2
1 +u
2
2 = kuk
2
. Note that this is consistent with
the definition of norm introduced in Section 4.3. The inner product (scalar product, dot product) 85
Exercises
4.13 Calculate the Euclidean inner product of the vectors u = (4, −2) and v = (3, −3)
and hence determine the angle between u and v.
4.14 Show that the Euclidean inner product on R
3 defined by hu, vi = u · v =
kuk kvk cos θ can be expressed as hu, vi = u1v1 + u2v2 + u3v3 =
P3
i=1 uivi
.
4.15 Show that the Euclidean inner product of u = (5, −3, 2) and v = (−2, 4, 1) is
−20.
We are now in a position to generalise the previous results to vectors in R
n.
Definition 4.9 Inner product in R
n
Consider row vectors in R
n. Suppose u = (u1, u2, . . . , un) and v = (v1, v2, . . . , vn). Their
inner product, written hu, vi is defined to be
hu, vi = u1v1 + u2v2 + . . . + unvn =
Xn
i=1
uivi
the result being a scalar. We generalise the idea of the cosine of an angle between two vectors,
u, v ∈ R
n, and hence the angle between them, by defining this to be
cos θ =
hu, vi
kukkvk
.
It can be shown that this is a well-defined quantity.
It is important to point out again that there are different ways in which an inner product
on R
n can be defined. Consequently the norms of vectors and the angles between them
depend upon the choice of inner product. Strictly speaking, when referring to the norm
and angle between vectors we should state which inner product is being used. The inner
product in R
n has a number of properties: if u = (u1, u2, . . . , un), v = (v1, v2, . . . , vn),
w = (w1, w2, . . . , wn) where ui
, vi
, wi ∈ R, the inner product, hu, vi =
Xn
i=1
uivi satisfies:
1. hu, vi = hv, ui, (commutativity)
2. khu, vi = hku, vi = hu, kvi, where k ∈ R
3. h(u + v), wi = hu, wi + hv, wi, (distributivity)
4. hu, ui ≥ 0, and hu, ui = 0 if and only if u = 0 (positivity).
Properties (2) and (3) together mean that the scalar product is linear. Commutativity can
be used to show that hu,(v + w)i = hu, vi + hu, wi. It is very helpful to gain familiarity
with properties such as (2) and (3) because we draw upon similar properties to establish
the tensor product in Chapter 10.
We now turn our attention to finding the inner product of two vectors of complex
numbers. A word of warning is necessary. Conventions differ among computer scientists,
physicists, engineers and mathematicians as to how the inner product is defined. We shall
give two definitions below, starting with the one used frequently by mathematicians.86 Vectors
4.6.2 The inner product on C
n: Definition 1
Definition 4.10 The inner product on C
n
Consider two row vectors in C
n. Suppose u = (u1, u2, . . . , un) and v = (v1, v2, . . . , vn) where
ui
, vi ∈ C. We define the inner product (often referred to as the standard Hermitian
inner product) written as hu, vi to be
hu, vi = u1v
∗
1 + u2v
∗
2 + . . . + unv
∗
n =
Xn
i=1
uiv
∗
i
where ∗ denotes complex conjugate.
Observe this definition is very similar to the previous one used for vectors of real numbers
but now the operation requires taking the complex conjugate of the elements in the second
vector.
Example 4.6.1
If u = (1 + 2i, 3 − 2i) and v = (5 − i, 7 + 4i) find hu, vi, hv, ui, hu, ui and hv, vi.
Solution
hu, vi = (1 + 2i)(5 − i)∗ + (3 − 2i)(7 + 4i)∗
= (1 + 2i)(5 + i) + (3 − 2i)(7 − 4i)
= (5 − 2 + i + 10i) + (21 − 8 − 12i − 14i)
= 16 − 15i.
hv, ui = (5 − i)(1 + 2i)∗ + (7 + 4i)(3 − 2i)∗
= (5 − i)(1 − 2i) + (7 + 4i)(3 + 2i)
= (5 − 2 − i − 10i) + (21 − 8 + 12i + 14i)
= 16 + 15i.
hu, ui = (1 + 2i)(1 + 2i)∗ + (3 − 2i)(3 − 2i)∗
= (1 + 2i)(1 − 2i) + (3 − 2i)(3 + 2i)
= (1 + 4) + (9 + 4)
= 18.
hv, vi = (5 − i)(5 − i)∗ + (7 + 4i)(7 + 4i)∗
= (5 − i)(5 + i) + (7 + 4i)(7 − 4i)
= (25 + 1) + (49 + 16)
= 91.
We make two important observations about these answers. Firstly, the inner product is not
commutative, that is hu, vi 6= hv, ui. In fact observe that hu, vi = hv, ui
∗
. Secondly, the inner
product of any non-zero vector with itself is always a real, non-negative number.The inner product (scalar product, dot product) 87
We summarise several properties of this inner product below:
if u = (u1, u2, . . . , un), v = (v1, v2, . . . , vn), w = (w1, w2, . . . , wn) where ui
, vi
, wi ∈ C, the
inner product,
hu, vi =
Xn
i=1
uiv
∗
i
satisfies:
1. hu, vi = hv, ui
∗
,
2. khu, vi = hku, vi = hu, k∗vi, where k ∈ C
3. h(u + v), wi = hu, wi + hv, wi, (distributivity)
4. hu, ui ≥ 0 and hu, ui = 0 if and only if u = 0, (positivity).
Properties (2) and (3) imply that this inner product is ‘linear in the first slot’, but because
of the conjugation in (2) it is not ‘linear in the second slot’. Using properties (1) and (3) it
can be shown that the statement hu,(v + w)i = hu, vi + hu, wi is nevertheless true. We say
that the inner product is anti-linear, or sesquilinear.
As before, an important quantity derived from the inner product is the norm of u,
written kuk, defined by kuk =
p
hu, ui. Referring back to Example 4.6.1
kuk =
√
18, kvk =
√
91.
In general, the norm kuk is calculated from:
kuk
2 = hu, ui
=
Xn
i=1
uiu
∗
i
=
Xn
i=1
|ui
|
2
.
Any vector with norm equal to 1 is said to be a unit vector and any vector can be
normalised by dividing by its norm.
Definition 4.11 Orthogonal and orthonormal sets of vectors
Two vectors, u and v, are said to be orthogonal (with respect to the defined inner product)
if hu, vi = 0. A set of vectors the elements of which are both normalised and mutually
orthogonal is said to be an orthonormal set.
In the previous discussion our examples referred to row vectors. Precisely the same calcu￾lations can be performed on column vectors.
Example 4.6.2
With the previously given definition of the inner product on C
n, show that the vectors u
and v are orthogonal, where
u =


1
3
−5

 , v =


−2
4
2

 .88 Vectors
Solution
Note that these vectors have real elements and so
hu, vi =
X
3
i=1
uiv
∗
i =
X
3
i=1
uivi = (1)(−2) + (3)(4) + (−5)(2) = 0
and hence u and v are orthogonal.
We now turn to the second definition of the inner product on C
n, the one favoured by many
physicists and computer scientists and which lends itself well to bra-ket notation, as we
shall see later.
4.6.3 The inner product on C
n: Definition 2
Definition 4.12 The inner product on C
n
Consider two row vectors in C
n. Suppose u = (u1, u2, . . . , un) and v = (v1, v2, . . . , vn) where
ui
, vi ∈ C. We now define the inner product, written as hu, vi, to be
hu, vi = u
∗
1
v1 + u
∗
2
v2 + . . . + u
∗
n
vn =
Xn
i=1
u
∗
i
vi
where ∗ denotes complex conjugate.
Observe this definition is very similar to the previous one but now the complex conjugate is
taken of the elements of the first vector. Applying this definition to the vectors of Example
4.6.1 you should verify that:
hu, vi = 16 + 15i
hv, ui = 16 − 15i
hu, ui = 18 as previously
hv, vi = 91 as previously.
Observe that the results of finding hu, vi depend on the definition used. We summarise
several properties of this inner product below: if u = (u1, u2, . . . , un), v = (v1, v2, . . . , vn),
w = (w1, w2, . . . , wn) where ui
, vi
, wi ∈ C, the inner product, hu, vi =
Xn
i=1
u
∗
i
vi satisfies:
1. hu, vi = hv, ui
∗
,
2. khu, vi = hk
∗u, vi = hu, kvi, where k ∈ C
3. hu, v + wi = hu, vi + hu, wi, (distributivity)
4. hu, ui ≥ 0 and hu, ui = 0 if and only if u = 0, (positivity).
where ∗ denotes complex conjugate.
Properties (2) and (3) imply that this inner product is ‘linear in the second slot’, but
because of the conjugation in (2) it is not ‘linear in the first slot’. Using properties (1) and
(3) it can be shown that the statement h(u+v), wi = hu, wi+hv, wi is nevertheless true. As
with Definition 1, we say that this inner product is anti-linear, or sesquilinear. The norm
of a vector u is found, as previously, from kuk
2 = hu, ui.End-of-chapter exercises 89
Exercises
4.16 Calculate, using both definitions, the inner product, hz, wi, of z ∈ C
2 and w ∈ C
2
where z = (z1, z2), w = (w1, w2).
4.17 Suppose z ∈ C
2 and z = (z1, z2). Show that kzk
2 = |z1|
2 + |z2|
2
.
4.18 Calculate, using both definitions, the inner product, hz, wi, of z ∈ C and w ∈ C.
Show that kzk
2 = zz∗ = |z|
2
.
4.19 Consider x, y ∈ B
4 where x = (x1, x2, x3, x4), y = (y1, y2, y3, y4) and where
xi
, yi ∈ B. Define the inner product hx, yi to be X
4
i=1
xiyi where the multiplication
and addition operations are carried out modulo 2. Find hx, yi when:
a) x = (1, 1, 1, 0), y = (0, 1, 1, 1),
b) x = (0, 0, 1, 1), y = (1, 0, 0, 0),
c) x = (0, 0, 0, 0), y = (0, 0, 0, 0),
d) x = (1, 1, 1, 1), y = (1, 1, 1, 1).
In all cases show that the inner product is equivalent to calculating
(x1 ∧ y1) ⊕ (x2 ∧ y2) ⊕ (x3 ∧ y3) ⊕ (x4 ∧ y4).
4.20 Let ψ = α1e1 + α2e2 + . . . + αnen, where αi ∈ R and
e1 =


1
0
0
.
.
.
0


, e2 =


0
1
0
.
.
.
0


, . . . , en =


0
0
0
.
.
.
1


.
Show that:
a) he1, ψi = α1, b) he2, ψi = α2, . . . , c) hen, ψi = αn.
4.7 End-of-chapter exercises
1. Show that the vectors √
1
2

1
1

and √
1
2

1
−1

are unit vectors and are mu￾tually orthogonal.
2. Write the qubit |ψi =
q
2
3
|0i +
q
1
3
|1i as a column vector and find its norm.
3. Given u =

1 + i
−2 − 3i 
and v =

5 + 2i
1 − 2i 
, evaluate the linear combination
3u + 7v.90 Vectors
4. Given u =

1 + i
−2 − 3i 
and v =

5 + 2i
1 − 2i 
find the inner products hu, vi, hv, ui,
hu, ui, hv, vi using both Definitions 1 and 2.
5. Write the vectors u =

1
0

and v =

0
1

as linear combinations of the
vectors 
1
1

and 
1
−1

.
6. Given the vectors u =

2
−1

, v =

−5
−2

and w =

−1
1

, choose one of
the vectors and express it as a linear combination of the other two.
7. Given the vectors u =


1
1
0

, v =


1
1
1

, and w =


1
0
1

 show that it is
impossible to write any one of the vectors as a linear combination of the other
two.
8. Show that



1
√
2


1
0
0
1

 ,
1
√
2


0
1
1
0

 ,
1
√
2


1
0
0
−1

 ,
1
√
2


0
1
−1
0





is an orthonormal set (the so-called Bell basis).
9. Consider x, a ∈ B
n where x = (x1, x2, . . . , xn), a = (a1, a2, . . . , an) and where
xi
, ai ∈ B. Define the inner product hx, ai to be Xn
i=1
xiai where the multiplication
and addition operations are carried out modulo 2. Show that:
a) if x = (1, 0, 0, 0, . . . , 0) then x · a yields the value a1,
b) if x = (0, 1, 0, 0, . . . , 0) then x · a yields the value a2,
if xi = 1, xj = 0 for j = 1, . . . , n, j 6= i then x · a yields the value ai c) .
10. Let |+i = √
1
2
(|0i + |1i) and let |−i = √
1
2
(|0i − |1i) show that:
√
1
2
a) (|+i + |−i) = |0i,
√
1
2
b) (|+i − |−i) = |1i.
11. Consider u, v ∈ R
n with inner product hu, vi =
Pn
i=1 uivi
. Show that if u and v
are unit vectors the angle, θ, between them can be found from cos θ = hu, vi.
12. Let x = (x1, x2, x3) = (x1x2x3) with xi ∈ B. Let y ∈ B
3
. Complete Table 4.1
which evaluates x · y = hx, yi =
P3
i=1 xiyi where the multiplication and addition
operations are carried out modulo 2.
13. Let p, x ∈ B
n, i.e., p = (p1p2 . . . pn), x = (x1x2 . . . xn) where xi
, pi ∈ B. Define
the scalar product of p and x to be
p · x = (p1 ∧ x1) ⊕ (p2 ∧ x2) ⊕ . . . ⊕ (pn ∧ xn).
a) Evaluate p · x when p = (111) and x = (010).
b) Evaluate p · x when p = (111) and x = (110).End-of-chapter exercises 91
TABLE 4.1
Table for question 12
x y x · y
x1x2x3 000 0
x1x2x3 001 x3
x1x2x3 010
x1x2x3 011
x1x2x3 100
x1x2x3 101 x1 ⊕ x3
x1x2x3 110
x1x2x3 111 x1 ⊕ x2 ⊕ x3
14. As in Exercise 13, let p, x ∈ B
3 = B × B × B. Consider the sum
X
x∈{0,1}3
p · x
Note that X
x∈{0,1}3
indicates that the sum is taken over all possible values x in the
set {0, 1}
3 = B
3
, i.e.,
{0, 1}
3 = {000, 001, 010, 011, 100, 101, 110, 111}.
Choose any non-zero value of p ∈ B
3 and show that X
x∈{0,1}3
p · x = 4.
(In Chapter 22 we show that for p, x ∈ B
n with p 6= 0n,
2
Xn−1
x=0
p · x = 2n−1
, a result
used in some quantum algorithms.)5
Matrices
5.1 Objectives
In quantum computation, matrices ‘operate’ or act on vectors to move a system from one
quantum state to another as it evolves with time. They are also used to model the measure￾ment of quantities of interest. Of particular relevance are the so-called symmetric, Hermitian,
orthogonal and unitary matrices which are introduced in this chapter. In subsequent chap￾ters we go on to develop this material and explain the terms ‘eigenvalues’ and ‘eigenvectors’
of matrices. When a quantum system is measured, the only possible measured values are the
eigenvalues of the measurement matrices, and eigenvectors are used in the representation
of the post-measurement state of the system.
The solution of sets of linear simultaneous equations can be accomplished using matrices
and we show how this is done. Matrices can also be used to perform transformations of
points in two- and three-dimensional space through rotations and reflections. These notions
generalise to higher dimensions and are useful tools in some quantum algorithms. The
objective of this chapter is to show how calculations with matrices are performed and to
explain much of this important terminology.
5.2 Matrices: preliminary definitions
Much of what we have already discussed about row and column vectors can be extended to
potentially larger mathematical structures. If we increase the permissible numbers of rows
and/or columns, we create rectangular arrays known as matrices (singular matrix), for
example,


2 1
4 −3
7 11

 and 
3 + 2i −i 1 − i
i 0 3 − 2i 
.
Definition 5.1 Matrix
An m × n matrix is a rectangular array with m rows and n columns of real or complex
numbers. These numbers are called the elements of the matrix.
For example,

0 1
1 0 
and 
0 −i
i 0 
are both 2 × 2 matrices. We can regard a 1 × n matrix as a row vector, and an m × 1
matrix as a column vector. We will usually use an upper-case letter to denote a matrix.
DOI: 10.1201/9781003264569-5 9394 Matrices
More generally, if the matrix A has m rows and n columns we can write
A = (aij ) =


a11 a12 . . . a1n
a21 a22 . . . a2n
.
.
.
.
.
.
.
.
.
.
.
.
am1 am2 . . . amn


.
Here aij represents the real or complex element in the ith row and jth column of A.
Matrices which have particular characteristics or properties are given specific names. A
square matrix has the same number of rows as columns. A diagonal matrix is a square
matrix with zeros everywhere except possibly on the leading diagonal, that is from top￾left to bottom-right. An identity matrix is a diagonal matrix with 1’s on the leading
diagonal. For example, the matrices
C =


8 −3 1
−15 7 2
0 1 0

 , D =


4 0 0
0 2 0
0 0 11

 , and I =

1 0
0 1 
are all square. D and I are diagonal. I is an identity matrix. Generally, we use the symbol I
(or In or In×n) to represent an identity matrix. The diagonal matrix


λ1 0 0 0
0 λ2 0 0
0 0
.
.
. 0
0 0 0 λn


will be denoted diag{λ1, λ2, . . . , λn}.
Definition 5.2 Transpose of a matrix
The transpose of a matrix A, denoted with a superscript T as in AT
, is found by inter￾changing its rows and columns. Note that if A = (aij ) then AT = (aji).
Thus the first row becomes the first column of the transpose, and so on. For example, if
M =

4 + i 1 − i
5 −3 − 2i 
then MT =

4 + i 5
1 − i −3 − 2i 
.
Definition 5.3 Symmetric matrix
A symmetric matrix, A, is a square matrix which is equal to its transpose:
A = A
T
.
Thus if A = (aij ) is symmetric, then aij = aji.
Thus both
A =

7 12
12 −5

and B =

1 + i 3 + 5i
3 + 5i 2 − 2i 
are symmetric matrices. This is immediately obvious because AT = A and BT = B, but note
also that a symmetric matrix possesses symmetry about its leading diagonal. Of particular
interest in quantum computation are real symmetric matrices.
Definition 5.4 Conjugate transpose or Hermitian transpose of a matrix
The conjugate transpose, or Hermitian transpose of a matrix A, denoted by A†
, is
found by taking the complex conjugate of the transpose of A:
A
† = (A
T
)
∗
Note that this is equivalent to A† = (A∗
)
T
. Here ∗ denotes conjugate, and T denotes trans￾pose.Matrix arithmetic: addition, subtraction and scalar multiplication 95
An important set of matrices used in quantum computation are known as self-adjoint or
Hermitian matrices.
Definition 5.5 Self-adjoint matrix
A self-adjoint matrix, H say, is a square matrix which is equal to the complex conjugate
of its transpose:
H = (HT
)
∗ = (H∗
)
T = H†
.
Self-adjoint matrices play an important role in quantum computation because, as we shall
see, they have real eigenvalues which correspond to the measured values of quantum ob￾servables.
Example 5.2.1
Suppose
H =

7 7 + 2i
7 − 2i −5

.
Taking the complex conjugate of each element we find
H∗ =

7 7 − 2i
7 + 2i −5

.
Then, taking the transpose we obtain
(H∗
)
T =

7 7 + 2i
7 − 2i −5

which is H†
. Observe that H† = (H∗
)
T = H so that H is indeed self-adjoint.
Exercises
5.1 Show that the leading diagonal elements of a self-adjoint matrix are necessarily
real.
5.2 Show that a real symmetric matrix is self-adjoint.
5.3 The sum of the elements on the leading diagonal of a matrix A is known as its
trace, and denoted tr(A). If A =


7 2 1
8 2 3
9 −1 −4

 find tr(A) and tr(AT
). What
is the trace of an n × n identity matrix ?
5.3 Matrix arithmetic: addition, subtraction and scalar
multiplication
When two matrices have the same size, addition and subtraction are performed, as with
vectors, in an element by element fashion, to yield another matrix of the same size. Mul￾tiplication by a scalar is achieved by multiplying every element of a matrix by that scalar.96 Matrices
So, if
A =

4 1
5 −3

, B =

−8 2
0 −3.5

, C =

0 −i
i 0 
then
A + B =

−4 3
5 −6.5

, A − B =

12 −1
5 0.5

, 7B =

−56 14
0 −24.5

3iC =

0 3
−3 0 
.
Definition 5.6 Matrix addition and scalar multiplication
Let A and B have the same size (i.e., the same number of rows and the same number of
columns). If A = (aij ), B = (bij ) then A + B = (aij + bij ). For a scalar k, kA = k(aij ) =
(kaij ).
Note that matrix addition is associative, that is
(A + B) + C = A + (B + C)
so that we can write simply A + B + C and there is no ambiguity. This follows because
addition of real or complex numbers is associative.
Example 5.3.1 The set of m × n matrices forms a group under addition.
Observe that if we add two m × n matrices the result is another m × n matrix. So the set
is closed under addition. We have noted already that matrix addition is associative. The
m × n matrix with all elements equal to zero is the identity element for this group. (Note
that this is not the same as an identity matrix.) The inverse of any m × n matrix under
addition is simply the matrix in which all elements are the negative of those in the original
matrix. The remaining group axioms are readily verified. Thus the set of m × n matrices
forms a group under addition.
Exercises
5.4 Show that the sum and difference of two real symmetric n×n matrices is another
real symmetric matrix.
5.5 Show that if an n × n real symmetric matrix is multiplied by a real scalar the
result is another real symmetric matrix.
5.6 If A =

9 4
3 2 
find A + AT and show that this is a symmetric matrix.
5.7 A skew-symmetric matrix is one for which AT = −A. If A =

9 4
3 2 
find
A − AT and show that the result is a skew-symmetric matrix.The product of two matrices 97
5.8 If A and B are n × n matrices and k is a scalar show that
(A + B)
T = AT + BT a) (kA)
T = kAT
.b)
5.4 The product of two matrices
Matrices of an appropriate size can be multiplied to yield another matrix. Specifically, if A
and B are p×q and r×s matrices, respectively, we can form the matrix product C = AB
if and only if q = r, that is the number of columns in the first matrix is equal to the number
of rows in the second. The result, C, is then a p × s matrix. The formal definition of the
matrix product is as follows:
Definition 5.7 Matrix multiplication
Given A and B of size p × q and q × s respectively, the product C = AB is p × s and given
by
C = (cij ) where cij =
Xq
k=1
aikbkj .
The examples which follow demonstrate how this calculation is performed in practice. As
will be illustrated, matrix multiplication is, in general, not commutative, that is AB 6= BA.
However, the following properties hold in general:
Theorem 5.1 Provided that matrices P, Q and R are of compatible sizes:
1. matrix multiplication is associative: (P Q)R = P(QR).
2. multiplication is distributive over addition: P(Q + R) = P Q + P R, and sim￾ilarly
3. (P + Q)R = P R + QR.
4. A(kB) = k(AB), where k is a scalar.
Given the expression AB we say that B is pre-multiplied by A, or A is post-multiplied
by B.
Example 5.4.1
(i) Use the formula in the Definition above to calculate c21 when A =

5 3 −1
2 1 8 
and
B =


7
2
1

.
(ii) Find the matrix product C = AB.
Solution
(i) Note that A has size 2 × 3, B has size 3 × 1 and so the product can be formed and will98 Matrices
have size 2 × 1. The calculation of c21 is performed as follows:
cij =
Xq
k=1
aikbkj
c21 =
X
3
k=1
a2kbk1
= a21b11 + a22b21 + a23b31
= (2)(7) + (1)(2) + (8)(1)
= 24.
You should try calculating c11 in a similar fashion.
(ii) The whole calculation is usually set out as follows:
C = AB =

5 3 −1
2 1 8 


7
2
1

 =

(5)(7) + (3)(2) + (−1)(1)
(2)(7) + (1)(2) + (8)(1) 
=

40
24 
.
Note from Example 5.4.1 that the product BA cannot be formed because in this order the
number of columns in the first matrix is not equal to the number of rows in the second.
Even in cases when both AB and BA can be formed, in general AB 6= BA so that matrix
multiplication in general is not commutative. However, there are exceptions as we shall see.
Example 5.4.2
Find, if possible, the products AB and BA when
A =


4
−3
1

 and B =
￾
−1 2 5 
.
Solution
A has size 3×1 and B has size 1×3. Therefore, in the expression AB the number of columns
in the first matrix, 1, is the same as the number of rows in the second. The product can be
formed and will have size 3 × 3:
AB =


4
−3
1


￾
−1 2 5 
=


−4 8 20
3 −6 −15
−1 2 5

 .
In the expression BA the number of columns in the first matrix, 3, is the same as the
number of rows in the second. The product can be formed and will have size 1 × 1, that is,
a scalar.
BA =
￾
−1 2 5 


4
−3
1

 = (−1)(4) + (2)(−3) + (5)(1) = −5.
We emphasise again that, in general, AB 6= The product of two matrices 99
Example 5.4.3
Evaluate the matrix product MP where M =

1 1
1 −1

and P =

e
iθ2 0
0 eiθ1

.
Solution
MP =

1 1
1 −1
  e
iθ2 0
0 eiθ1

=

e
iθ2 e
iθ1
e
iθ2 −e
iθ1

.
Example 5.4.4
Given H = √
1
2

1 1
1 −1

, find a) H

1
0

, b) H

0
1

.
Solution
a) H

1
0

= √
1
2

1 1
1 −1
  1
0

= √
1
2

1
1

.
b) H

0
1

= √
1
2

1 1
1 −1
  0
1

= √
1
2

1
−1

.
Recall that in ket notation 
1
0

= |0i and 
0
1

= |1i. Further, in quantum computation
√
1
2

1
1

is often written |+i and √
1
2

1
−1

as |−i in which case the foregoing results
would be written
H|0i = |+i, H|1i = |−i.
Example 5.4.5
Given A =

7 −2
1 4 
, find A2 and show that A2 − 11A + 30I = 0 where 0 here is the
2 × 2 matrix with all elements zero.
Solution
A
2 =

7 −2
1 4   7 −2
1 4 
=

47 −22
11 14 
.
Then
A
2 − 11A + 30I =

47 −22
11 14 
− 11 
7 −2
1 4 
+ 30 
1 0
0 1 
=

47 −22
11 14 
−

77 −22
11 44 
+

30 0
0 30 
=

0 0
0 0 
.
as required.100 Matrices
Exercises
5.9 Suppose X =

0 1
1 0 
. Recall that |0i =

1
0

and |1i =

0
1

. Show that
a) X|0i = |1i, b) X|1i = |0i.
5.10 Suppose H = √
1
2

1 1
1 −1

and Z =

1 0
0 −1

. Show that H Z H = X
where X =

0 1
1 0 
.
5.11 If x is an n × 1 matrix (column vector) and In×n is the n × n identity matrix,
show that In×nx = x.
5.12 Let P =


p11 p12 p13
p21 p22 p23
p31 p32 p33

 and let D be the diagonal matrix diag{λ1, λ2, λ3} =


λ1 0 0
0 λ2 0
0 0 λ3

. Show that
P D =


λ1p11 λ2p12 λ3p13
λ1p21 λ2p22 λ3p23
λ1p31 λ2p32 λ3p33

 .
Observe particularly the way that the λi on the diagonal of D transfer to the
columns of P D.
Definition 5.8 Normal matrix
A square matrix A with real elements for which
A AT = A
T A
that is, A commutes with its transpose, is said to be normal.
Example 5.4.6
Show that the matrices A =

0 −5
5 0 
and B =

1 1
−1 1 
are both normal.
Solution
A AT =

0 −5
5 0   0 5
−5 0 
=

25 0
0 25 
A
T A =

0 5
−5 0   0 −5
5 0 
=

25 0
0 25 
B BT =

1 1
−1 1   1 −1
1 1 
=

2 0
0 2 
B
T B =

1 −1
1 1   1 1
−1 1 
=

2 0
0 2 
.The product of two matrices 101
We see that A AT = AT A and B BT = BT B as is required for the matrices to be normal.
More generally, for matrices with complex elements, we have the following definition:
Definition 5.9 Normal matrix
A square matrix A with complex elements for which
A A† = A
† A
that is, A commutes with its conjugate transpose, is said to be normal.
Clearly, in the case of matrices with real elements this is equivalent to A AT = AT A. All
symmetric and self-adjoint matrices are necessarily normal.
Example 5.4.7 Linear functional
Evaluate, if possible, the matrix product AB where
A = ( 3 −5 2 ) and B =


1
3
−7

 .
Solution
A has size 1×3. B has size 3×1. So the product AB can be found and will be of size 1 ×1,
that is, a single number.
AB = ( 3 −5 2 )


1
3
−7

 = 3 × 1 + (−5) × 3 + 2 × (−7) = −26.
It will become important when we study linear functionals (Chapter 9) to observe that we
can think of AB as the row vector A = (3, −5, 2) ‘operating’ on the column vector B which
follows it to produce a scalar, i.e., we can think of it, using our knowledge of functions, as
A (B), that is as a ‘function’ A with argument B.
The following theorem will be used when we study linear operators for quantum dynamics.
Theorem 5.2 The transpose of a product
Provided matrices A and B are compatible for multiplication:
(AB)
T = B
T A
T
.
Example 5.4.8 The property (AB)
T = BT AT
.
Suppose A =

3 1
2 6 
, B =

−1 4
3 8 
. Show that (AB)
T = BT AT
.
Solution
AB =

3 1
2 6   −1 4
3 8 
=

0 20
16 56 102 Matrices
and hence
(AB)
T =

0 16
20 56 
.
Also
B
T A
T =

−1 3
4 8   3 2
1 6 
=

0 16
20 56 
and hence (AB)
T = BT AT
.
The result of the previous example is true more generally, including when the transpose
operation is replaced by the conjugate transpose †.
Theorem 5.3 Provided that the matrices are compatible for multiplication
(AB)
† = B
†A
†
.
We now introduce a result known as the completeness relation or resolution of the
identity. This result is drawn upon frequently in quantum computations. Consider first
the following example.
Example 5.4.9 Matrix multiplication and the resolution of the identity
In this example we start to introduce matrix multiplication using the ket notation.
If |α1i =

1
0

and |α2i =

0
1

evaluate
(a) |α1ihα1|, (b) |α2ihα2|, (c) P2
i=1|αiihαi
|.
Solution
(a) Given |α1i =

1
0

recall that the bra corresponding to this ket is found by taking the
conjugate transpose to give hα1| =
￾
1 0 
. Then
|α1ihα1| =

1
0

￾
1 0 
=

1 0
0 0 
.
(b) Similarly,
|α2ihα2| =

0
1

￾
0 1 
=

0 0
0 1 
.
(c)
X
2
i=1
|αiihαi
| =

1 0
0 0 
+

0 0
0 1 
=

1 0
0 1 
.
We make an important observation: the final result is an identity matrix.
More generally, given a set of orthonormal n-dimensional vectors {|αii}, i = 1, . . . , n, it can
be shown that
Xn
i=1
|αiihαi
| = I.
This result is the completeness relation or the resolution of the identiThe product of two matrices 103
Example 5.4.10
Evaluate X
1
z=0
z|zihz|.
Solution
X
1
z=0
z|zihz| = 0|0ih0| + 1|1ih1|
= 0 
1
0

￾
1 0 
+ 1 
0
1

￾
0 1 
= 0 
1 0
0 0 
+ 1 
0 0
0 1 
=

0 0
0 1 
.
Example 5.4.11 Matrix multiplication and permutation matrices
Consider the effect of pre-multiplying the column vector u =


a
b
c

 by the matrix P =


0 0 1
1 0 0
0 1 0

.
P u =


0 0 1
1 0 0
0 1 0




a
b
c

 =


c
a
b

 .
Observe that the effect is to re-order or permute the elements of u. Thus P is referred to
as a permutation matrix.
Definition 5.10 Permutation matrix.
A permutation matrix P is a square matrix which has a 1 just once in every row and
column and zero everywhere else. Pre-multiplying a column vector u by P has the effect of
re-ordering or permuting the elements of u.
Exercises
5.13 Given |0i =

1
0

and |1i =

0
1

, find
X
1
x=0
a) |xihx|,
X
1
x=0
b) |xihx|.
5.14 Given A =

a11 a12
a21 a22 
, B =

b11 b12
b21 b22 
show that (AB)
T = BT AT104 Matrices
5.15 Prove that all symmetric matrices are necessarily normal.
5.16 Prove that all self-adjoint matrices are necessarily normal.
5.17 Given the orthonormal set {|α1i, |α2i} where |α1i =
 
√
1
2
√
1
2
!
, |α2i =
 
− √
1
2
√
1
2
!
,
perform the resolution of the identity to show that X
2
i=1
|αiihαi
| = I.
5.5 Block multiplication of matrices
Given two matrices, A and B, which are compatible for multiplication it is possible to par￾tition each matrix into submatrices called blocks and then perform multiplication, treating
the blocks as though they were simple matrix elements. In order to do this the number of
columns of blocks in A must equal the number of rows of blocks in B. Consider the following
example which illustrates how this is done.
Example 5.5.1
Suppose we wish to calculate the following product of two 3 × 3 matrices, P and Q:
P Q =


p11 p12 p13
p21 p22 p23
p31 p32 p33




q11 q12 q13
q21 q22 q23
q31 q32 q33

 .
Suppose we block P with vertical lines to produce three columns, and block Q with hori￾zontal lines to produce three rows, as shown.
P Q =


p11 p12 p13
p21 p22 p23
p31 p32 p33




q11 q12 q13
q21 q22 q23
q31 q32 q33

 .
Now treating each block as a single element we can write
P Q =
￾
P1 P2 P3



Q1
Q2
Q3


where P1 =


p11
p21
p31

, Q1 =
￾
q11 q12 q13 
and so on. Given the sizes are still compat￾ible for multiplication, that is 1 × 3 and 3 × 1, respectively, we find the product, which will
be 1 × 1, is given by
P Q = P1Q1 + P2Q2 + P3Q3.
It is this form of the product that will be useful when we study eigenvalues and matrix
decomposition in Chapter 7. To verify that this form does indeed give the required produBlock multiplication of matrices 105
observe that
P Q =


p11
p21
p31


￾
q11 q12 q13 
+


p12
p22
p32


￾
q21 q22 q23 
. . .
+


p13
p23
p33


￾
q31 q32 q33 
=


p11q11 p11q12 p11q13
p21q11 p21q12 p21q13
p31q11 p31q12 p31q13

 +


p12q21 p12q22 p12q23
p22q21 p22q22 p22q23
p32q21 p32q22 p32q23

 . . .
+


p13q31 p13q32 p13q33
p23q31 p23q32 p23q33
p33q31 p33q32 p33q33


=


p11q11 + p12q21 + p13q31 p11q12 + p12q22 + p13q32 p11q13 + p12q23 + p13q33
p21q11 + p22q21 + p23q31 p21q12 + p22q22 + p23q32 p21q13 + p22q23 + p23q33
p31q11 + p32q21 + p33q31 p31q12 + p32q22 + p33q32 p31q13 + p32q23 + p33q33


which is the required matrix product.
Example 5.5.2
Partition the first matrix below into three columns and the second into three rows and
perform block multiplication to find P Q.
P Q =


1 2 3
4 5 1
6 0 2




4 −1
2 3
0 7

 .
Solution
We partition the matrices as indicated:
P Q =


1 2 3
4 5 1
6 0 2




4 −1
2 3
0 7


that is,
P Q =
￾
P1 P2 P3



Q1
Q2
Q3

 .
The product is then
P Q = P1Q1 + P2Q2 + P106 Matrices
We can evaluate this as follows:
P Q =


1
4
6


￾
4 −1

+


2
5
0


￾
2 3 
+


3
1
2


￾
0 7 
=


4 −1
16 −4
24 −6

 +


4 6
10 15
0 0

 +


0 21
0 7
0 14


=


8 26
26 18
24 8

 .
Example 5.5.3
Consider P =

p11 p12
p21 p22 
.
a) State P
T
.
b) Evaluate P P T
.
c) Let p1 =

p11
p21 
and p2 =

p12
p22 
, the first and second columns of P respectively,
and use block multiplication to deduce that P P T
can be evaluated as
P P T = p1p
T
1 + p2p
T
2
.
Solution
a) P
T =

p11 p21
p12 p22 
.
b)
P P T =

p11 p12
p21 p22   p11 p21
p12 p22 
=

p
2
11 + p
2
12 p11p21 + p12p22
p21p11 + p22p12 p
2
21 + p
2
22 
.
c) Let
p1 =

p11
p21 
and p2 =

p12
p22 
,
so that
p
T
1 =
￾
p11 p21 
and p
T
2 =
￾
p12 p22 
.
Observe that in block form
P =
￾
p1 p2

and P
T =

p
T
1
p
T
2Matrices, inner products and ket notation 107
Then
P P T =
￾
p1 p2


p
T
1
p
T
2

= p1p
T
1 + p2p
T
2
=

p11
p21 
￾
p11 p21 
+

p12
p22 
￾
p12 p22 
=

p
2
11 p11p21
p21p11 p
2
21 
+

p
2
12 p12p22
p22p12 p
2
22 
=

p
2
11 + p
2
12 p11p21 + p12p22
p21p11 + p22p12 p
2
21 + p
2
22 
.
Comparing this result with the solution to part b) we deduce that
P P T = p1p
T
1 + p2p
T
2
where p1 and p2 are the first and second columns of P, respectively. This result can be
readily generalised to larger n × n matrices:
P P T = p1p
T
1 + p2p
T
2 + . . . + pnp
T
n
where the pi
, i = 1, 2, . . . , n are the columns of P.
5.6 Matrices, inner products and ket notation
We shall now consider how the inner product in C
n, is written and applied using ket
notation. We have seen that given two column vectors u and v in C
n, their inner product
is defined (Definition 2 in Section 4.6.3) as hu, vi =
Xn
i=1
u
∗
i
vi
. So, for example, if u =

1 + i
2 − 3i 
, and v =

4 + 5i
1 + 2i 
then
h

1 + i
2 − 3i 
,

4 + 5i
1 + 2i 
i = (1 + i)∗
(4 + 5i) + (2 − 3i)∗
(1 + 2i)
= (1 − i)(4 + 5i) + (2 + 3i)(1 + 2i)
= 9 + i − 4 + 7i
= 5 + 8i.
Observe that the same result can be obtained using matrix multiplication by calculating
￾
1 − i 2 + 3i 

4 + 5i
1 + 2i 
.
But the row vector here is simply the bra corresponding to the ket 
1 + i
2 − 3i 
. (Recall that
when writing a ket as a bra we take the complex conjugate of each element and then find
the transpose.) Thus, sticking with the ket notation, the inner product of |ui and |vi can
be written as
hu| |vi or more concisely hu108 Matrices
Definition 5.11 The inner product of two kets.
The inner product of two kets (column vectors) |ui and |vi is hu| |vi, also written more
concisely as hu|vi where the bra hu| is found by taking the complex conjugate transpose of
the ket |ui.
In Section 5.4, we noted that a row vector can be thought of as operating on a column
vector to produce a scalar. With this in mind we can think of the bra hu| as operating on
an arbitrary vector v (of the appropriate size) to produce a complex scalar, that is
hu|(v) : v → C.
That is, a vector v is mapped by the bra to a scalar in C.
Example 5.6.1
Calculate the inner product hψ1|ψ2i if |ψ1i =
 1+i
2
1−i
2
!
and |ψ2i =
 
√
i
2
√−i
2
!
.
Solution
hψ1|ψ2i =
￾
1−i
2
1+i
2

 
√
i
2
√−i
2
!
=

1 − i
2
  i
√
2

+

1 + i
2
  −i
√
2

=
i + 1 − i + 1
2
√
2
=
1
√
2
.
Example 5.6.2
Consider the bra hv| and kets |ui and |wi, and the combination
|ui hv| |wi.
We explore what this might mean. Suppose
|ui =

u1
u2

, hv| = ( v1 v2 ), |wi =

w1
w2

.
Then, performing |ui hv| first,
|ui hv| =

u1
u2

( v1 v2 ) = 
u1v1 u1v2
u2v1 u2v2

so that
|ui hv| |wi =

u1v1 u1v2
u2v1 u2v2
  w1
w2

=

u1v1w1 + u1v2w2
u2v1w1 + u2v2w2

.
Alternatively, performing hv| |wi first,
hv| |wi = hv|wi = ( v1 v2 )

w1
w2

= v1w1 + v2w2, a scalarThe determinant of a square matrix 109
Then, because hv|wi = v1w1 + v2w2 is a scalar, we perform scalar multiplication:
|ui hv| |wi =

u1
u2

(v1w1 + v2w2) = 
u1v1w1 + u1v2w2
u2v1w1 + u2v2w2

the same result as above.
This example serves to illustrate the associativity of matrix multiplication. Further, it
illustrates the following: we see that |ui hv| |wi can be interpreted as a matrix |ui hv| multi￾plying, or operating on, a vector |wi to produce another vector. Alternatively, it represents
the vector |ui being multiplied by the scalar hv| |wi resulting from an inner product. We
shall use this result when discussing projection matrices later in this chapter.
Exercises
5.18 Given |ui =

4
−2

, hv| =
￾
5 7 
, |wi =

3
9

, evaluate
(a) |uihv|
(b) |uihv||wi
(c) hv||wi
(d) (hv||wi)|ui.
5.7 The determinant of a square matrix
Definition 5.12 Determinant of a 2 × 2 matrix
Consider the square 2×2 matrix A =

a b
c d 
. The scalar quantity, ad−bc, derived from
this matrix is called the determinant, written det(A) or |A|, that is
|A| =




a b
c d



 = ad − bc.
Example 5.7.1
Given A =

1 5
−2 8 
its determinant is




1 5
−2 8




= (1)(8) − (5)(−2) = 8 + 10 = 18.
Given B =

4 6
2 3 
its determinant is




4 6
2 3




= (4)(3) − (6)(2) = 0110 Matrices
For larger square matrices, formulae exist for finding determinants. For example, in the case
of a 3 × 3 matrix A, we have
Definition 5.13 Determinant of a 3 × 3 matrix
Given A =


a11 a12 a13
a21 a22 a23
a31 a32 a33

, its determinant is given by
|A| = a11




a22 a23
a32 a33



 − a12




a21 a23
a31 a33




+ a13




a21 a22
a31 a32




.
Note that there are also other, equivalent, formulae for finding the determinant of a 3 × 3
matrix. The determinant of a 4 × 4 matrix is found by generalising the previous result in
an obvious way and can be found in most Linear Algebra textbooks.
Exercises
5.19 Show that the determinant of the matrix A =


2 −1 0
−1 2 −1
0 −1 2

 is 4.
5.20 For the matrix A in the previous exercise, find |AT
|. Comment upon the result.
5.21 Find




cos ωt sin ωt
− sin ωt cos ωt




.
5.22 Show that if A is a 2 × 2 or 3 × 3 matrix, then |A| = |AT
|, a result which is also
true for an arbitrary n × n matrix.
5.8 The inverse of a square matrix
Suppose A is a square matrix. If there exists another matrix B say, such that AB = BA = I,
an identity matrix, then B is said to be the inverse of A. Likewise A is the inverse of B.
It is a straightforward matter, left to the reader, to check that

4 1
3 1 
is the inverse of 
1 −1
−3 4 
.
It is usual to denote the inverse of A by A−1
. We have the following definition:
Definition 5.14 The inverse of a matrix
Given a square matrix A, its inverse, when such exists, is denoted A−1 and has the property
that
A A−1 = A
−1 A = I
where I is the identity matrix having the same size as A. Matrices which possess an inverse
are said to be non-singular or invertible.The inverse of a square matrix 111
When it exists, the inverse of a 2 × 2 matrix A =

a b
c d 
is given by
A
−1 =
1
ad − bc 
d −b
−c a 
=
1
|A|

d −b
−c a 
.
Whenever det (A) = |A| = ad − bc = 0 the inverse does not exist, that is, the matrix is
singular, because 1
0
is undefined.
Example 5.8.1 The general linear group GL(n, F).
The set of all non-singular n × n matrices with elements from a field F (usually R or C)
with the operation of matrix multiplication form a group called the general linear group,
denoted GL(n, F). Because the elements of this group are non-singular we can be assured
that each has a multiplicative inverse. The identity element for the group is the n×n identity
matrix. You should verify that all the group axioms are satisfied. The group GL(n, F) and
any subgroups are referred to as linear groups or sometimes matrix groups.
5.8.1 The inverse of an n × n matrix
The procedure for calculating the inverse of a general n × n matrix necessitates the in￾troduction of some new terminology. Suppose A = (aij ) is an n × n matrix. Consider the
submatrix obtained from A by removing its i-th row and j-th column. The determinant
of this submatrix, written Mij , is a scalar called the minor of element aij . We form the
matrix of minors by replacing each element of A with its minor. The cofactor of aij , which
we will denote by Aij , is given by Aij = (−1)i+jMij . We can then write down the matrix
of cofactors by imposing the place sign (−1)i+j on each minor.


A11 A12 A13 . . . A1n
A21 A22 A23 . . . A2n
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
An1 An2 An3 . . . Ann


.
Then the transpose of the matrix of cofactors is called the classical adjoint of A denoted
adj(A). The inverse of an n × n matrix A, when this exists, is given by
A
−1 =
1
|A|
adj(A).
Example 5.8.2
Find the inverse of A =


3 1 0
5 2 −1
1 4 −2

.
Solution
We begin by calculating the matrix of minors of each element of A. For example, the minor
of a11 (the element of A which equals 3) is found by removing its row and column and
calculating M11 =




2 −1
4 −2



 = −4 − (−4) = 0. Continuing in this fashion the matrix of112 Matrices
minors is
M =


0 −9 18
−2 −6 11
−1 −3 1

 .
Imposing the place sign (−1)i+j on each minor gives the matrix of cofactors:


0 9 18
2 −6 −11
−1 3 1

 .
The classical adjoint of A is then the transpose of the matrix of cofactors:
adj(A) =


0 2 −1
9 −6 3
18 −11 1

 .
The determinant of A is given by the formula in Definition 5.13:
|A| = 3




2 −1
4 −2



 − 1




5 −1
1 −2




+ 0




5 2
1 4




= 3(0) − 1(−9) = 9.
Then finally, the inverse of A is given by
A
−1 =
1
|A|
adj(A) = 1
9


0 2 −1
9 −6 3
18 −11 1

 .
Exercises
5.23 Calculate, if possible, the inverse of A =

3 −2
4 −2

.
5.24 Calculate, if possible, the inverse of A =


3 −1 7
2 0 1
5 −2 6

.
5.25 Calculate the values of λ for which A =

2 − λ 4
1.5 1 − λ

has no inverse.
5.26 Suppose R =

cos θ − sin θ
sin θ cos θ

.
Find RT a) .
b) Show that R RT = RT R = I, the 2 × 2 identity matrix.
Deduce that RT = R−1
c) .
5.9 Similar matrices and diagonalisation
Definition 5.15 Similar matrices
A square matrix B is said to be similar to another square matrix A if there exists an
invertible matrix P such that B = P
−1AP.Similar matrices and diagonalisation 113
Similar matrices have several properties in common. In particular they share the same
special values known as eigenvalues, a property we explore further in Chapter 7.
Example 5.9.1 Similar matrices
Given P =

3 −1
−5 2 
, by explicitly evaluating the matrix products show that B =

−10 7
−36 22 
is similar to A =

7 3
1 5 
.
Solution
Using the formula for the inverse matrix,
P
−1 =

2 1
5 3 
.
Then
P
−1AP =

2 1
5 3   7 3
1 5   3 −1
−5 2 
=

15 11
38 30   3 −1
−5 2 
=

−10 7
−36 22 
.
Hence, B =

−10 7
−36 22 
is similar to A =

7 3
1 5 
.
Exercises
5.27 If B is similar to A show that A is similar to B.
Example 5.9.2 Diagonalisation
Given P =

1 2
1 1 
, by explicitly evaluating the matrix products show that B =

5 0
0 6 
is similar to A =

7 −2
1 4 
.
Solution
Using the formula for the inverse matrix, P
−1 =

−1 2
1 −1

. Then
P
−1AP =

−1 2
1 −1
  7 −2
1 4   1 2
1 1 
=

−5 10
6 −6
  1 2
1 1 
=

5 0
0 6 
.114 Matrices
Thus A =

7 −2
1 4 
is similar to the diagonal matrix B =

5 0
0 6 
. The process we have
observed is known as diagonalisation and will be important when we study eigenvalues
and eigenvectors used in making quantum measurements.
5.10 Orthogonal matrices
Definition 5.16 Orthogonal matrix
An orthogonal matrix A, is a non-singular square matrix for which its transpose is equal
to its inverse:
A
T = A
−1
.
Equivalently, AT A = A AT = I.
An important property of any orthogonal matrix is that the columns are orthogonal and
normalised. The same is true of the rows. Orthogonal matrices are necessarily normal, that
is they commute with their transpose.
We saw in the previous section that matrices A and B are similar if there exists P
such that B = P
−1AP. We also saw that under some circumstances, which we have yet
to explain, the matrix B is diagonal. If in addition, the matrix P is orthogonal, so that
P
−1 = P
T
, then the condition for similarity becomes B = P
T AP. If such a matrix exists,
we say that A and B are orthogonally similar. If A is orthogonally similar to a diagonal
matrix, then A is orthogonally diagonalisable. We shall see that all symmetric matrices
have this property.
Example 5.10.1
Consider the matrix P =

−1/
√
2 1/
√
2
1/
√
2 1/
√
2

.
(a) Show that P is an orthogonal matrix.
(b) By evaluating P
T AP when A is the symmetric matrix 
5 −1
−1 5 
show that A is
orthogonally diagonalisable.
Solution
(a) Consider P
T P:
P
T P =

−1/
√
2 1/
√
2
1/
√
2 1/
√
2
  −1/
√
2 1/
√
2
1/
√
2 1/
√
2

=

1 0
0 1 
= I.
Likewise, P P T = I. Thus P is an orthogonal matrix. Observe that the inner product of
the two columns of P is zero. Further, the modulus of each column vector is 1. Thus the
columns are orthogonal and normalised. Likewise, the rows.Unitary matrices 115
(b) By explicitly evaluating the matrix products, we find
P
T AP =

−1/
√
2 1/
√
2
1/
√
2 1/
√
2
  5 −1
−1 5   −1/
√
2 1/
√
2
1/
√
2 1/
√
2

=

−1/
√
2 1/
√
2
1/
√
2 1/
√
2
  −6/
√
2 4/
√
2
6/
√
2 4/
√
2

=

6 0
0 4 
.
Thus the result is a diagonal matrix, and so A is orthogonally diagonalisable.
Example 5.10.2 The orthogonal group O(2) and the special orthogonal group
SO(2).
The set of all 2 × 2 orthogonal matrices, together with matrix multiplication, form a group,
denoted O(2). So, given two orthogonal matrices, their product is another orthogonal ma￾trix. The identity element of the group is the identity matrix I =

1 0
0 1 
which is readily
checked to be orthogonal. Every orthogonal matrix possesses an inverse which is also orthog￾onal. Indeed, this is its transpose. Those matrices in O(2) which have determinant equal to
1, form a subgroup denoted SO(2) referred to as the special orthogonal group.
5.11 Unitary matrices
Definition 5.17 Unitary matrix
A non-singular, complex matrix U such that U
† = U
−1
(where U
†
is the conjugate transpose
of U, i.e., (U
∗
)
T
) is said to be a unitary matrix. Thus
U U† = U
†U = I.
The rows of a unitary matrix are pairwise orthogonal so that the inner product of two
distinct rows will be zero. Likewise, the columns. The norm of each row or column is equal
to 1. Unitary matrices are necessarily normal, that is they commute with their conjugate
transpose. These properties are easily verified for the unitary matrices in the examples
which follow. Such matrices play a special role in quantum computation because quantum
dynamics is via unitary operators acting on the underlying vector space of quantum states.
Example 5.11.1 Pauli matrices.
Consider the following 2 × 2 matrices known as Pauli matrices.
σ0 =

1 0
0 1 
, σ1 =

0 1
1 0 
, σ2 =

0 −i
i 0 
, σ3 =

1 0
0 −1

.
It is a straightforward exercise to verify that each of these matrices is unitary, σ
−1
i = σ
†
i
,
i = 0, . . . , 3.116 Matrices
Example 5.11.2
Consider the matrix given by
σ =
1
2
(1 + i)(σ0 − iσ1)
where σ0 and σ1 are the Pauli matrices of Example 5.11.1. Show that this matrix is unitary,
i.e., σ
−1 = σ
†
.
Solution
σ =
1
2
(1 + i)(σ0 − iσ1)
=
1
2
(1 + i) 
1 −i
−i 1 
=
 1+i
2
1−i
2
1−i
2
1+i
2
!
.
The conjugate transpose of this matrix is
σ
† =
 1−i
2
1+i
2
1+i
2
1−i
2
!
.
Evaluating σ σ† we find
 1+i
2
1−i
2
1−i
2
1+i
2
! 1−i
2
1+i
2
1+i
2
1−i
2
!
=

1 0
0 1 
= I.
Likewise, σ
† σ = I. Thus the given matrix is unitary.
Exercises
5.28 Consider the Pauli matrices of Example 5.11.1. Show that

1
2
(1 + i)(σ0 − iσ1)
2
= σ1.
Deduce that σ1 has a square root.
We have seen previously that matrices A and B are similar if there exists P such that
B = P
−1AP. We also saw that under some circumstances, which we have yet to explain,
the matrix B is diagonal. If in addition, the matrix P is unitary, so that P
−1 = P
†
, then
the condition for similarity becomes B = P
†AP. If such a matrix exists, we say that A and
B are unitarily similar. If A is unitarily similar to a diagonal matrix, then A is unitarily
diagonalisable. We shall see that all self-adjoint matrices have this property.Matrices and the solution of linear simultaneous equations 117
Example 5.11.3 The unitary group U(2) and the special unitary group SU(2).
The set of all 2 × 2 unitary matrices, together with matrix multiplication, form a group,
denoted U(2). So, given two unitary matrices, their product is another unitary matrix. The
identity element of the group is the identity matrix I =

1 0
0 1 
which is readily checked
to be unitary. Every unitary matrix possesses an inverse which is also unitary. Indeed, this
is its conjugate transpose. Those matrices in U(2) which have determinant equal to 1, form
a subgroup denoted SU(2) referred to as the special unitary group. Elements of the
unitary group U(2) are used in the representation of 1-qubit gates (Chapter 21).
5.12 Matrices and the solution of linear simultaneous equations
Using our knowledge of matrix multiplication, a set of m linear equations in n unknowns,
x1, . . . , xn,
a11x1 + a12x2 + . . . + a1nxn = b1
a21x1 + a22x2 + . . . + a2nxn = b2
.
.
. =
.
.
.
am1x1 + am2x2 + . . . + amnxn = bm
can be written in matrix form as


a11 a12 . . . a1n
a21 a22 . . . a2n
.
.
.
.
.
.
.
.
.
.
.
.
am1 am2 . . . amn




x1
x2
.
.
.
xn


=


b1
b2
.
.
.
bm


or more succinctly as Ax = b where
A =


a11 a12 . . . a1n
a21 a22 . . . a2n
.
.
.
.
.
.
.
.
.
.
.
.
am1 am2 . . . amn


, x =


x1
x2
.
.
.
xn


, and b =


b1
b2
.
.
.
bm


.
The m × n matrix A is then called the coefficient matrix. It is important to note that
when solving any such set of linear simultaneous equations just three possibilities exist:
• there is a unique solution (i.e., unique values for each of x1, x2, . . . , xn).
• no solutions exist whatsoever, and the equations are said to be inconsistent.
• there is an infinite number of different solutions.
In the special case when A is square and non-singular, its inverse A−1
exists and we can
formally write the solution as
x = A
−1
b
by pre-multiplying Ax = b by A−1 and using the properties that A−1A = I and Ix = x,
where I is an identity matrix. Thus by finding the inverse matrix, the solution can be
determined. Recall A−1
exists if |A| 6= 0.118 Matrices
In the case when all bi are zero, i.e., b = 0, the equations are said to be homogeneous.
Note that if the equations are homogeneous, i.e., Ax = 0, and if |A| 6= 0 then x = A−10 = 0.
The only solution is the trivial one for which xi = 0 for all i.
An important consequence is that if we seek non-trivial solutions of a set of homoge￾neous equations, Ax = 0, then we will require |A| = 0 in order for such solutions to exist.
Further, if A has rank r (the rank is the number of linearly independent rows of A, see
Chapter 6) then the number of linearly independent solutions of the homogeneous system
Ax = 0 is n − r.
Example 5.12.1
Use the matrix inverse, if possible, to solve the equations
(a) x1 + 2x2 = 13
2x1 − 5x2 = 8
(b) x1 + 2x2 = 0
2x1 − 5x2 = 0
(c) x1 + 2x2 = 0
2x1 + 4x2 = 0.
Solution
(a) Note that the set of equations is not homogeneous, i.e., the set is inhomogeneous. In
matrix form, Ax = b, we have

1 2
2 −5
  x1
x2

=

13
8

.
Note |A| = −9 6= 0. Using the formula for the matrix inverse, A−1 = −
1
9

−5 −2
−2 1 
from
which x = A−1
b, that is
x =

x1
x2

= −
1
9

−5 −2
−2 1   13
8

=

9
2

.
It follows that x1 = 9, x2 = 2 is the unique solution.
(b) The equations are homogeneous. In matrix form, Ax = b, we have

1 2
2 −5
  x1
x2

=

0
0

.
The inverse of A exists (it is the same as in part (a)).
x =

x1
x2

= −
1
9

−5 −2
−2 1   0
0

=

0
0

.
Hence, only the trivial solution, x1 = x2 = 0 exists.
(c) The equations are homogeneous. Here, the matrix form, Ax = b, of the equations is

1 2
2 4   x1
x2

=

0
0

.
Observe that |A| = 0 and hence A−1 does not exist, and this method of solution is inap￾propriate. Nevertheless, solutions do exist as we shall demonstrate shortly.Matrices and the solution of linear simultaneous equations 119
In the following section, we present a systematic process for solving simultaneous linear
equations called Gaussian elimination.
5.12.1 Gaussian elimination
Gaussian elimination is a systematic method for solving sets of linear equations. To solve
the linear system Ax = b, we form the augmented matrix by appending the column
vector b to the right-hand side of the coefficient matrix to give:


a11 a12 . . . a1n b1
a21 a22 . . . a2n b2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
am1 am2 . . . amn bm


.
Next we perform any of the following so-called elementary row operations:
• interchange any two rows,
• multiply any row by a non-zero constant,
• add or subtract any row from any other.
A little thought will show that these operations are just the same ones as applied to eliminate
one of the variables when solving the sort of simultaneous equations met in a school course.
The aim is to achieve row echelon form in which
• any rows of zeros are at the bottom,
• each successive row contains more leading zeros than the previous one.
Once the augmented matrix is in row echelon form, back-substitution leads to the solution
as we demonstrate in the examples below. Using this method reveals the behaviour men￾tioned previously: there may be a unique solution, no solutions at all, or an infinite number
of solutions. This behaviour is also illustrated in the examples below. Note that there are
multiple ways of achieving echelon form leading to different yet equivalent expressions for
the solution.
Example 5.12.2 Gaussian elimination – unique solution
Use Gaussian elimination to show that the following linear system of equations has a unique
solution and determine it.
2x − y + z = −2
x + 2y + 3z = −1
2x + 2y − z = 8.
Solution
The augmented matrix is


2 −1 1 −2
1 2 3 −1
2 2 −1 8

 .120 Matrices
Leave row 1 alone. Replace row 2 with the result of subtracting row 1 from twice row 2,
denoted R2 → 2R2 − R1. Then perform the operation R3 → R3 − R1.
The result is the matrix:


2 −1 1 −2
0 5 5 0
0 3 −2 10

 .
The aim of these operations is to introduce as many leading zeros as possible.
Continuing, leave row 2 alone and perform R3 → 5R3 − 3R2 to give


2 −1 1 −2
0 5 5 0
0 0 −25 50

 .
At this point, the equations are in echelon form. Observe that as we move down the rows
each successive row contains more leading zeros than the previous one. In this example there
are no rows of zeros, but if there were then they should be at the bottom.
The final row, re-expressed as an equation is −25z = 50 from which z = −2. The process
of back-substitution then begins by substituting this value of z into the row above. This
then reads:
5y + 5z = 5y + 5(−2) = 0
5y = 10
y = 2.
Finally, the solutions already obtained for y and z are substituted into the first equation:
2x − y + z = 2x − (2) + (−2) = −2
2x = 2
x = 1.
So the unique solution is
x = 1, y = 2, z = −2.
Example 5.12.3 Gaussian elimination – inconsistent equations
Use Gaussian elimination to show that the following linear system of equations has no
solution, i.e., it is inconsistent.
2x + y − z = −3
3x + 2y + z = 6
x + y + 2z = 8.
Solution
The augmented matrix is


2 1 −1 −3
3 2 1 6
1 1 2 8

 .
We reduce this matrix to echelon form. Leave row 1 alone and then perform the operations
R2 → 2R2 − 3R1, and R3 → 2R3 − R1:


2 1 −1 −3
0 1 5 21
0 1 5 19

 .Matrices and the solution of linear simultaneous equations 121
Continuing, leave row 2 alone and perform R3 → R3 − R2 to give


2 1 −1 −3
0 1 5 21
0 0 0 −2

 .
We now have echelon form, but notice that an issue arises. The final line reads 0x+0y+0z =
−2, i.e., 0 = −2 which is clearly impossible. This signals to us that the equations are
inconsistent and there are no solutions.
Example 5.12.4 Gaussian elimination – an infinite number of solutions
Use Gaussian elimination to show that the following linear system of equations has an
infinite number of solutions and write down a general expression for them.
x + y − z = 1
3x − y + 5z = 3
7x + 2y + 3z = 7.
Solution
The augmented matrix is


1 1 −1 1
3 −1 5 3
7 2 3 7

 .
We reduce the augmented matrix to echelon form. Leave row 1 alone and perform the
operations R2 → R2 − 3R1, R3 → R3 − 7R1. This gives


1 1 −1 1
0 −4 8 0
0 −5 10 0

 .
Continuing, perform R3 → 4R3 − 5R2:


1 1 −1 1
0 −4 8 0
0 0 0 0

 .
Observe that on this occasion there is a row of zeros and this row is at the bottom of the
matrix. This row provides no information; it merely states 0 = 0, but leads to the concept
of a free variable. Rows 1 and 2 correspond to the equations
x + y − z = 1
− 4y + 8z = 0.
In the echelon form, the first row starts off with a non-zero x, the second with a non-zero
y. There is now no row that starts off with a non-zero z. It is this variable that we shall
choose as a free variable. We can choose the value of z to be anything we please, say z = µ,
where µ is our choice. We then continue the back-substitution to obtain
z = µ122 Matrices
−4y + 8z = −4y + 8µ = 0 so that y = 2µ.
x + y − z = x + 2µ − µ = 1 so that x = 1 − µ.
We can therefore formally write the solution as x = 1 − µ, y = 2µ, z = µ or often as


x
y
z

 =


1 − µ
2µ
µ

 =


1
0
0

 + µ


−1
2
1

 .
Clearly then, there is an infinite number of different solutions as µ is allowed to vary.
Example 5.12.5 Gaussian elimination – an infinite number of solutions
Use Gaussian elimination to show that the following linear system of equations has an
infinite number of solutions and write down a general expression for them.
x + 7y − z + w = 1
y − z = 3
Solution
In this example there are four unknowns and only two equations. The augmented matrix is


1 7 −1 1 1
0 1 −1 0 3
0 0 0 0 0
0 0 0 0 0


or more simply 
1 7 −1 1 1
0 1 −1 0 3 
Observe that the augmented matrix is in row echelon form. The first row starts off with a
non-zero x, the second with a non-zero y. There are no rows starting with a non-zero z or
a non-zero w. We thus create two free variables, w = λ, z = µ say. Then back-substitution
gives
y = 3 + z
= 3 + µ
x = 1 − 7y + z − w
= 1 − 7(3 + µ) + µ − λ
= −20 − 6µ − λ.
We can therefore formally write the solution as


x
y
z
w

 =


−20 − 6µ − λ
3 + µ
µ
λ

 =


−20
3
0
0


+ µ


−6
1
1
0


+ λ


−1
0
0
1


Clearly then, there is a doubly infinite number of different solutions as µ and λ are allowed
to vary.
Systems of linear equations which have an infinite number of solutions are important in theMatrices and the solution of linear simultaneous equations 123
study of eigenvectors in Chapter 7. In turn, eigenvectors are used in the representation of
the states of a quantum system (Chapter 13).
Example 5.12.6 Linear simultaneous equations with arithmetic mod 2
In some quantum algorithms it is necessary to solve linear simultaneous equations whilst
performing arithmetic modulo 2. Consider the set of linear simultaneous equations
x1 + x2 + x3 = 1
x2 + x4 = 1
x1 + x3 = 1
x2 + x3 + x4 = 1
where x1, . . . , x4 are binary variables 0 or 1. We seek a solution using arithmetic modulo 2.
Solution
We can apply Gaussian elimination in the way shown previously making use of addition
modulo 2 in which:
0 + 0 = 0, 0 + 1 = 1 + 0 = 1, 1 + 1 = 0.
The augmented matrix is


1 1 1 0 1
0 1 0 1 1
1 0 1 0 1
0 1 1 1 1

 .
We apply the row operation R3 → R3 − R1 to obtain:


1 1 1 0 1
0 1 0 1 1
0 1 0 0 0
0 1 1 1 1

 .
where we note that −1 = 1 mod 2. Then applying R3 → R3 − R2, R4 → R4 − R2 gives:


1 1 1 0 1
0 1 0 1 1
0 0 0 1 1
0 0 1 0 0

 .
Interchanging rows 3 and 4:


1 1 1 0 1
0 1 0 1 1
0 0 1 0 0
0 0 0 1 1

 .
which is now in row echelon form. Back-substitution then yields:
x4 = 1, x3 = 0
x2 = 1 − x4 = 1 − 1 = 0124 Matrices
x1 = 1 − x2 − x3 = 1 − 0 − 0 = 1.
The unique solution is then
x1 = 1, x2 = 0, x3 = 0, x4 = 1.
You should verify that this is indeed a solution by substituting into the original equations.
Example 5.12.7 Linear simultaneous equations with arithmetic mod 2 – free
variables
Consider the set of linear simultaneous equations
x1 + x3 = 1
x2 + x3 + x4 = 0
x1 + x2 + x3 = 0
x1 + x2 + x4 = 1
where x1, . . . , x4 are binary variables 0 or 1. We seek a solution using arithmetic modulo 2.
Solution
The augmented matrix is


1 0 1 0 1
0 1 1 1 0
1 1 1 0 0
1 1 0 1 1

 .
We apply the following row operations R3 → R3 − R1, R4 → R4 − R1 to obtain:


1 0 1 0 1
0 1 1 1 0
0 1 0 0 1
0 1 1 1 0

 .
Then apply R3 → R3 − R2, R4 → R4 − R2 to obtain


1 0 1 0 1
0 1 1 1 0
0 0 1 1 1
0 0 0 0 0

 .
Observe that in echelon form there is no row starting off with a non-zero x4. We choose this
to be free: x4 = µ say where µ ∈ {0, 1}. Then using back-substitution
x3 = 1 − x4 = 1 − µ
x2 + (1 − µ) + µ = 0, so that x2 = −1 = 1(mod 2)
x1 = 1 − x3 = 1 − (1 − µ) = µ.
Hence the solution is x1 = µ, x2 = 1, x3 = 1 − µ (= 1 + µ), x4 = µ, where µ ∈ {0, 1}. This
solution can also be expressed as


x1
x2
x3
x4

 =


0
1
1
0


+ µ


1
0
1
1

 .Matrices and the solution of linear simultaneous equations 125
When we study quantum algorithms in Chapters 22 and 23, we will require the solution
of a linear system of simultaneous equations with Boolean variables 0 and 1. For these
applications, we shall write addition using the exclusive-or operator ⊕, and multiplication
using the Boolean operator ∧, or ‘and’. Consider the following example.
Example 5.12.8
Solve the following equation to find s1 and s2 where s1, s2 ∈ {0, 1}.
1 ∧ s1 ⊕ 1 ∧ s2 = 0.
Solution
Observe that with the Gaussian elimination notation introduced previously, this equation
is equivalent to the echelon form
s1 + s2 = 0.
So select s2 to be a free variable, s2 = µ where µ ∈ {0, 1}. Then s1 = −µ. We could write
the solution as

s1
s2

= µ

−1
1

= µ

1
1

because −1 = 1 (mod 2). Later we shall see the solution written as a string of binary values
s1s2, so in this case there are two distinct solutions 00 (when µ = 0) and 11 (when µ = 1).
Example 5.12.9
Solve the following equation to find s1 and s2 where s1, s2 ∈ {0, 1}.
0 ∧ s1 ⊕ 1 ∧ s2 = 0.
Solution
The given equation is equivalent to writing 0s1 + 1s2 = 0. Thus s2 = 0 and s1, a free
variable, can be either 0 or 1. We can write each solution as a string of binary values, s1s2,
that is 00 and 10.
Exercises
5.29 Solve the homogeneous equations x1 + 4x2 = 0
3x1 + 12x2 = 0.
5.30 Use Gaussian elimination to solve
x + 2y − 3z + 2w = 2
2x + 5y − 8z + 6w = 5
3x + 4y − 5z + 2w = 4.126 Matrices
5.13 Matrix transformations
We have seen that a point in the xy plane with Cartesian coordinates (x, y) can be repre￾sented by a column vector 
x
y

. By pre-multiplying this vector by carefully chosen 2 × 2
matrices, the point can be relocated elsewhere in the plane. For example, the point might
be rotated through an angle θ say around the origin; it might be reflected in any chosen line
through the origin. Such an operation is referred to as a transformation and is achieved
using a transformation matrix. In this section we describe transformation matrices which
perform rotations and reflections in the plane.
Definition 5.18 Rotation matrix.
The transformation matrix W(θ) that rotates a point anticlockwise about the origin by an
angle θ is
W(θ) = 
cos θ − sin θ
sin θ cos θ

.
We now derive this transformation matrix. Consider the point P(x, y) in Figure 5.1 and
the effect of rotation through an angle θ anticlockwise about the origin to the new position
given by the point Q(x
0
, y0
).
x
0
y
r
θ1
r
θ
P(x, y)
Q(x , y )
FIGURE 5.1
Rotation in the plane, anticlockwise through angle θ.
The position vectors of P and Q are r =

x
y

and r
0 =

x
0
y
0

, respectively. Clearly
rotation around the origin preserves the length of the arm OP and hence |r| = |r
0
|. We now
derive a transformation matrix that will achieve such a rotation. Referring to Figure 5.1
observe that
cos θ1 =
x
|r|
, sin θ1 =
y
|r|
cos(θ1 + θ) = x
0
|r
0
|
, sin(θ1 + θ) = y
0
|r
0
|
.
Expanding the equations in the previous line using trigonometric identities (see Appendix
B), we have
cos θ1 cos θ − sin θ1 sin θ =
x
0
|r
0
|
, sin θ1 cos θ + cos θ1 sin θ =
y
0
|r
0
|Matrix transformations 127
so that
x
0 = |r
0
|(cos θ1 cos θ − sin θ1 sin θ), y0 = |r
0
|(sin θ1 cos θ + cos θ1 sin θ).
Noting |r
0
| = |r|, we have
x
0 = |r| cos θ1 cos θ − |r|sin θ1 sin θ, y0 = |r|sin θ1 cos θ + |r| cos θ1 sin θ.
But x = |r| cos θ1 and y = |r|sin θ1 and hence
x
0 = x cos θ − y sin θ, y0 = x sin θ + y cos θ.
Writing these equations in matrix form, we have

x
0
y
0

=

cos θ − sin θ
sin θ cos θ
  x
y

.
Let us label the matrix which achieves this rotation through θ by W(θ), that is
W(θ) = 
cos θ − sin θ
sin θ cos θ

.
It is straightforward to verify that the matrix W(θ) is an orthogonal matrix and in common
with all orthogonal matrices it preserves the length of the vector upon which it operates.
This is to be expected because the point P is being rotated about the origin and hence its
distance from the origin remains the same.
Definition 5.19 Reflection in the x axis.
The transformation matrix U that reflects a point in the x axis is
U =

1 0
0 −1

.
We now derive this transformation matrix. Consider the reflection of the point P(x, y) in
the x axis as shown in Figure 5.2. Clearly, under this operation the x coordinate does not
change, but the new y coordinate is −y. Thus Q is the point (x, −y). Labelling the matrix
x 0
y
r
r
P(x, y )
Q(x , y )= Q(x, −y)
FIGURE 5.2
Reflection in the x axis.
which achieves this transformation by U we have

x
0
y
0

= U

x
y

=

1 0
0 −1
  x
y

.128 Matrices
Transformations such as these can be performed one after the other using matrix multipli￾cation. For example
W(φ)U W(−φ)

x
y

first applies W(−φ) to the point 
x
y

which is a clockwise rotation through φ, followed by
reflection in the x axis, followed by an anticlockwise rotation through φ. This can be shown
to be equivalent to reflection in the line through the origin with gradient tan φ (Figure 5.3).
x
0
y
P (x, y )
Q(x , y )
φ
FIGURE 5.3
Reflection in the line through the origin with gradient tan φ.
5.14 Projections
We have seen that vectors in R
2 and R
3
can be visualised conveniently. Such visualisations
are helpful in defining quantities such as the length of a vector and the angle between
vectors. These concepts can then be generalised to more complicated vector spaces for
which visualisation is not feasible. Consider Figure 5.4 which shows arbitrary vectors a and
n in R
2 or R
3
. A line has been drawn from P to meet n at a right-angle. The distance OQ is
O
P
Q
a
n
θ
FIGURE 5.4
OQ is the projection of a onto n.
called the orthogonal projection of a onto n, or alternatively ‘the component of a in the
direction of n’. Observe, by simple trigonometry, that the length of OQ is |a| cos θ. Suppose
we take the inner product (here the Euclidean inner product) of a with the unit vector ˆn:
ha|nˆi = |a| |nˆ| cos θ
= |a| cos θ.Projections 129
We conclude that the orthogonal projection of a onto n is given by the scalar product ha|nˆi.
If we seek a vector in the same direction as n but with length equal to the projection OQ
then this is simply ha|nˆinˆ. In ket notation this vector is ha|nˆi |nˆi.
This concept of orthogonal projection of one vector onto another is generalised from R
2
and R
3
to vectors of complex numbers in the following example.
Example 5.14.1 Projection matrices
Consider the kets |ψi and |φi in C
n. We might depict them schematically as in Figure 5.5
although of course we are now dealing with vectors of complex numbers. Let |ψi be a unit
|ψ
|φ
FIGURE 5.5
The projection of |φi onto |ψi.
vector. Suppose we wish to project |φi onto |ψi. Consider the expression
(|ψihψ|)|φi.
Observe that |ψihψ| is a matrix which multiplies the ket |φi. But using associativity of
matrix multiplication, we can consider first hψ|φi which is the inner product of |ψi and |φi,
and of course, a complex scalar, c say.
Thus
(|ψihψ|)|φi = c|ψi.
The effect of multiplying |φi by |ψihψ| has been to project it onto |ψi. The matrix |ψihψ|
is referred to as a projection matrix.
Definition 5.20 Projection matrix.
Given a unit vector |ψi, the matrix which projects an arbitrary |φi onto |ψi is given by
|ψihψ|
and is referred to as a projection matrix.
Example 5.14.2
Find the projection matrix which projects an arbitrary |φi onto the unit vector |ψi =  √
3/2
1/2

. Find the orthogonal projection of 
1
5

onto |ψi.
Solution
The projection matrix is given by
|ψihψ| =
 √
3/2
1/2

￾ √
3/2 1/2

=

3/4
√
3/4
√
3/4 1/4

130 Matrices
Then

3/4
√
3/4
√
3/4 1/4
  1
5

=

(3 + 5√
3)/4
(5 + √
3)/4

.
5.15 End-of-chapter exercises
1. Find the values of λ for which the linear system
x + 4y = λx
2x + 3y = λy
has non-trivial solutions.
2. Solve the following equations using Gaussian elimination
2x − y + z = 2
−2x + y + z = 4
6x − 3y − 2z = −9.
3. If A =


2 1 3
4 2 1
−1 3 2

 and B =


1 −7 0
0 2 5
3 4 5

 find AT
, BT and (AB)
T
.
Deduce that (AB)
T = BT AT
.
4. Show that the Pauli matrices
σ0 =

1 0
0 1 
, σ1 =

0 1
1 0 
, σ2 =

0 −i
i 0 
, σ3 =

1 0
0 −1

are self-inverse.
5. Show that the projection matrix P =

3/4
√
3/4
√
3/4 1/4

has the property that
P
2 = P. This is a property possessed by all projection matrices.
6. Given A =

3 2
−3 −4

, find A2 and show that A2 + A − 6I = 0, the zero
matrix.
7. Suppose H =

3 1 + i
1 − i 2 
.
Find the adjoint matrix H†
.a)
b) Confirm that H is self-adjoint.
c) Show that H is normal, i.e HH† = H†H.
Show that if P =

−
1+i
2
1 + i
1 1 
, then P
−1HP = D where D is a
diagonal matrix with 1 and 4 on the leading diagonal. Deduce that H is
similar to D.
d)
8. The commutator of two matrices A and B is written [A, B] and is defined to
be AB − BA.End-of-chapter exercises 131
a) Deduce that if A and B commute, then the commutator is zero.
b) For the Pauli matrices given in Q4 above, show that [σ2, σ3] = 2iσ1.
9. Given H =

1 1
1 −1

and P =

e
iθ2 0
0 eiθ1

show that
1
2
HP H = ei
θ2+θ1
2
 
cos θ
2 −i sin θ
2
−i sin θ
2
cos θ
2
!
where θ = θ1 − θ2.
10. Find a 4×4 permutation matrix which permutes the Boolean strings 00, 01, 10, 11
to 01, 00, 11, 10.
11. Show that the matrix U =


0 0 0 1
1 0 0 0
0 1 0 0
0 0 1 0


is unitary.
12. It can be shown that all permutation matrices are orthogonal. Find a counter￾example which demonstrates that the converse is not true.
13. Given H = √
1
2

1 1
1 −1

show that
H|0i = √
1
2
a) (|0i + |1i),
H|1i = √
1
2
b) (|0i − |1i).
14. Given Z =

1 0
0 −1

, show that
a) Z|0i = |0i, b) Z|1i = −|1i,
c) Z|+i = |−i, d) Z|−i = |+i,
where |+i = √
1
2

1
1

and |−i = √
1
2

1
−1

.
15. Let A =

a11 a12
a21 a22 
, b =

b1
b2

, c =

c1
c2

, and let k be a scalar.
a) Evaluate Ab. b) Evaluate Ac.
c) Evaluate A(b + c). A(kb).d)
e) Deduce that A(b + c) = Ab + Ac. f) Deduce that A(kb) = kAb.
Results e) and f) mean that the matrix A is a ‘linear operator’, further details of
which are given in Chapter 11.6
Vector spaces
6.1 Objectives
Vector spaces are the mathematical structures in which the state vectors of quantum compu￾tation live. When endowed with an inner product, a vector space becomes an inner product
space. Of particular relevance are complex inner product spaces. An objective of this chap￾ter is to provide a formal definition of a vector space and give several examples. You will
see that, in this context, a ‘vector’ is a much more general object than vectors such as
those in R
2 and R
3
. However, the already familiar operations of vector addition and scalar
multiplication are essential ingredients of all vector spaces. We demonstrate how vectors in
a vector space can be constructed using a set of vectors called a basis and see how vectors
can be written in terms of different bases. Of particular importance are orthonormal bases
in which the basis vectors are mutually orthogonal and normalised. Finally, we introduce
cosets and explain how a quotient space is constructed.
6.2 Definition of a vector space
Definition 6.1 Vector space
Suppose F is a field (usually the set of real numbers R, or the set of complex numbers C).
Let V be a nonempty set {u, v, w, . . .} upon which we can define operations called addition
and scalar multiplication such that for any u, v ∈ V and k ∈ F, the sum u + v ∈ V and the
product ku ∈ V . Then V is called a vector space over the field F if the following axioms
are satisfied:
1. (u + v) + w = u + (v + w) - associativity of addition
2. there exists a zero element in V , or identity under addition, denoted 0,
such that u + 0 = u and 0 + u = u for all u ∈ V .
3. for each u, there exists an inverse under addition, denoted −u, also in V ,
such that u + (−u) = 0.
4. u + v = v + u - commutativity of addition
5. k(u + v) = ku + kv, for u, v ∈ V , k ∈ F - distributivity
6. for a, b ∈ F, (a + b)u = au + bu - distributivity
7. for a, b ∈ F, (ab)u = a(bu) - associativity of scalar multiplication
8. for 1 ∈ F, 1u = u.
The elements of any vector space V are hereafter referred to as vectors irrespective of
whether they ‘look like’ the familiar vectors in R
2 and R
3
. Because for any u, v ∈ V and
DOI: 10.1201/9781003264569-6 133134 Vector spaces
k ∈ F, the sum u + v ∈ V and the product ku ∈ V we say that the set V is closed under
the operations of addition and scalar multiplication. Note that axioms 1-4 mean that (V, +)
is an Abelian group.
Example 6.2.1 The vector space of n-tuples of complex numbers
The set of n-tuples of complex numbers, denoted C
n, with the usual, natural operations of
addition and multiplication of complex numbers is a vector space over C. By way of example
consider the specific case when n = 2 and the vector space is C
2 over the field C. Observe
that the column vectors u and v where
u =

1 + i
−7 + 2i 
and v =

2 + 3i
5 − 4i 
are both elements of C
2
. As we have seen previously, their sum is defined in a natural way
to be
u + v =

1 + i
−7 + 2i 
+

2 + 3i
5 − 4i 
=

3 + 4i
−2 − 2i 
which is clearly also an element of C
2
. The scalar multiple, ku, for k ∈ C, is given by
ku = k

1 + i
−7 + 2i 
=

k(1 + i)
k(−7 + 2i) 
which is clearly also an element of C
2
. So the set C
2
is closed under addition and scalar
multiplication. In this example, the zero vector is 
0
0

∈ C
2
. You should confirm for
yourself that the remaining axioms are satisfied.
We shall write vectors in C
n as both column vectors and row vectors as the need arises.
Example 6.2.2 The vector space of n-tuples of real numbers
Because all real numbers are complex numbers (with zero imaginary part), the previous
example readily interprets when the vectors are real. For example, consider the vector space
R
3 over the field R consisting of 3-tuples of real numbers. Observe that the row vectors u
and v, where
u = (9, −8, 2) and v = (−1, 2, 3),
are both elements of R
3
. Their sum is the vector (8, −6, 5) which is clearly also in R
3
. The
scalar multiple 5u = 5(9, −8, 2) = (45, −40, 10) is also in R
3
. So the set R
3
is closed under
addition and scalar multiplication. The zero element, or identity under addition is (0, 0, 0).
It is straightforward to check that the remaining axioms for a vector space hold.
We generalise the results of the previous two examples by emphasising that for real and
complex n-tuples addition and scalar multiplication are defined and performed component￾wise, i.e., if u = (u1, u2, . . . , un) and v = (v1, v2, . . . , vn) then
u + v = (u1, u2, . . . , un) + (v1, v2, . . . , vn) = (u1 + v1, u2 + v2, . . . , un + vn)
and, for k ∈ F,
ku = k(u1, u2, . . . , un) = (ku1, ku2, . . . , kun).
We often refer to an n-tuple vector such as u = (u1, u2, . . . , un) which has n elements
as an n-dimensional vector. This terminology should not be confused with the dimensionDefinition of a vector space 135
of a vector space which is not necessarily the same. It is possible, for example, to have a
two-dimensional vector space populated with three-dimensional vectors. This will become
clear when we discuss subspaces and the basis of a vector space shortly.
Example 6.2.3 The vector space B = {0, 1} with operations ⊕ and ∧
Consider the set B = {0, 1}. We define addition within this set using ⊕, that is the exclusive￾or operation or equivalently addition modulo 2. Choosing the underlying field again to be
{0, 1} (which is the finite field F2), scalar multiplication can be performed using the Boolean
and, ∧, or equivalently multiplication modulo 2. It is straightforward to show that {0, 1} is
closed under the operations ⊕ and ∧ and that all the axioms for a vector space are satisfied.
Thus far, the examples of vector spaces that we have seen look like familiar vectors in R
2
and R
3
. However, given the stated definition of a vector space, many other mathematical
objects can now be thought of as vectors.
Example 6.2.4 The vector space of m × n matrices over C
We have seen previously that two matrices of the same size can be added. We have also
seen how to multiply a matrix by a scalar. In both cases the result is another matrix of
the same size. It is straightforward to check that the set of m × n matrices with the usual
addition and scalar multiplication is a vector space. The zero element of the vector space
is an m × n matrix in which all elements are zero. You should verify that the remaining
axioms of a vector space are satisfied.
Example 6.2.5 The vector space of functions
Consider the set of all real-valued functions f : R → R. For example, the functions
f1(x) = x
2
, f2(x) = sin x, and f3(x) = ex
are all elements of this set. We can define the operation of addition on this set as follows:
for any functions f and g
(f + g) = (f + g)(x) = f(x) + g(x).
Clearly this sum is another real-valued function defined on R so the set is closed under
addition. Given f1 and f2 above, then (f1 + f2)(x) = f1(x) + f2(x) = x
2 + sin x. We define
scalar multiplication on a function f, for k ∈ R, by
(kf) = (kf)(x) = k f(x).
Clearly, the result is another real-valued function, so the set is closed under scalar multipli￾cation. For example, if k = 5 and f3(x) = ex
then
(5f3) = (5f3)(x) = 5 f3(x) = 5ex
.
The zero vector in this vector space is the zero function, z(x) = 0 because, for any function
f(x),
z(x) + f(x) = 0 + f(x) = f(x).
The inverse of the function f(x) is (−f)(x) = −f(x) because
f(x) + (−f(x)) = 0.136 Vector spaces
For example, the inverse of f2(x) = sin x is (−f2)(x) = − sin x. The remaining vector
space axioms are readily verified.
Exercises
6.1 Consider the set of all polynomials of degree 2 or less, with real coefficients. This
set includes, for example, the polynomials p1(t) = 1 + 2t−t
2
, p2(t) = 2 + 3t−7t
2
,
p3(t) = 1 + 5t, p4(t) = 6. Show that this set with the usual operations of addition
and multiplication by a real number is a vector space over R. Identify the zero
element. Determine the inverse of each of the polynomials given above.
6.3 Inner product spaces
In Chapter 4 we discussed how, given two vectors u = (u1, u2, . . . , un), v = (v1, v2, . . . , vn),
we can define their inner product hu, vi. In the case of vectors in R
2 and R
3
, we defined the
Euclidean inner product as
hu, vi =
X
2
i=1
uivi
, hu, vi =
X
3
i=1
uivi
,
respectively, and we generalised this for vectors in R
n. We then showed how an inner product
can be defined for vectors u, v ∈ C
n either as
hu, vi =
Xn
i=1
uiv
∗
i
(Definition 1)
where ∗ denotes the complex conjugate, or
hu, vi =
Xn
i=1
u
∗
i
vi (Definition 2).
We noted that the former is the inner product usually preferred by mathematicians whereas
the latter is often preferred by physicists and computer scientists. In fact, the latter is better
suited to the Dirac bra-ket notation as we shall see.
Definition 6.2 Inner product space
An inner product space is a vector space V over a field F which is endowed with an inner
product. A complex inner product space is a vector space for which the underlying field is
C.
The inner product assigns, to each pair of vectors u, v ∈ V , a scalar in the underlying field
F. Thus it is a mapping V × V → F. To qualify as an inner product, the mapping has to
satisfy requirements as listed in Chapter 4. Once an inner product has been defined on a
vector space it makes sense to refer to the norm of a vector u, as
kuk =
p
hu, ui.
Further, two vectors u, v ∈ V are orthogonal if hu, vi = 0. We shall see in Chapter 13 that
the state vectors of quantum computation are elements of a complex inner product space.Subspaces 137
Exercises
6.2 Show that if |ψi = (ψ1, ψ2, . . . , ψn)
T
, where ψi ∈ C, then
k|ψik =
vuutXn
i=1
|ψi
|
2.
6.3 Show if |ψi = (ψ1, ψ2, . . . , ψn)
T
is a unit vector then Pn
i=1|ψi
|
2 = 1.
6.4 Subspaces
Definition 6.3 Subspace
A subset U of a vector space V is called a subspace if it is a vector space in its own right.
It follows that the subspace will be closed under vector addition and scalar multiplication.
Importantly, this definition requires that the subset U contains the zero element of V .
Example 6.4.1
Consider the vector space R
2
. We can represent any (row) vector (x, y) in this space as a
directed line segment drawn from the origin to the point with coordinates (x, y). Vectors
(2, 3), (−3, 5) and several others are shown in Figure 6.1. Now consider the subset, U, of
R
2
comprising any vector of the form (x, 5x). That is,
U = {(x, y) ∈ R
2
: y = 5x}.
So this set includes (1, 5), (2, 10), (3, 15) and so on. Importantly, it includes the zero vector
(0, 0). Note that if we draw vectors from the origin to the points (x, 5x), all the resulting
x
y
-2-4 0 2 4 6 8 10 12 14
16
4
6
8
10
12
14
subset U
(-3,5)
(2, 3)
(2, 10)
(1, 5)
(3, 15)
2
FIGURE 6.1
The subset U of R
2
is a subspace.138 Vector spaces
points lie on the same straight line passing through the origin. It is straightforward to show
that for all the vectors in U, the axioms for a vector space are satisfied, and thus U is a
subspace of V . Particularly, adding any two vector in U results in another vector in U. For
example, (1, 5) + (2, 10) = (3, 15). Likewise for scalar multiples. Observe that the vector
space U is one-dimensional (a straight line) yet is populated by two-dimensional vectors.
Exercises
6.4 Consider the vector space R
3 and row vectors of the form (x, y, z).
(a) Consider the subset of R
3
comprising vectors of the form (x, y, 0) (i.e., the
xy plane). Verify that this is a subspace.
(b) Consider the subset of R
3
comprising vectors of the form (0, y, 0) (i.e., the y
axis). Verify that this is a subspace.
6.5 Consider the subset of R
2 given by
U = {(x, y) ∈ R
2
: y = 2x + 1}.
Explain why this set cannot be a subspace of R
2
.
6.6 Show that the set of all 2 × 2 symmetric matrices, with the usual operations of
addition and multiplication by a scalar, is a subspace of the vector space of all
2 × 2 matrices over R.
6.5 Linear combinations, span and LinSpan
We have seen that, given a vector space V over a field F, we can perform the operations of
addition of vectors and multiplication of a vector by a scalar.
Definition 6.4 Linear combination and Span
Given a set of vectors v1, v2, . . . , vn ∈ V and a set of scalars k1, k2, . . . , kn ∈ F, then the
vector formed by scalar multiplication and vector addition
v = k1v1 + k2v2 + . . . + knvn =
Xn
i=1
kivi
is called a linear combination of the vectors v1, v2, . . . , vn. The set of all linear combina￾tions of the vectors v1, v2, . . . , vn is called the span of the set {v1, v2, . . . , vn}.
If we select a subset of V , S say, then the set of all linear combinations of the vectors in S
is a subspace referred to as the subspace spanned by S or LinSpan(S).
Example 6.5.1 Vectors which span R
2
(a) Show that the row vector (8, 4) ∈ R
2
can be written as a linear combination of the
vectors (1, 1) and (−1, 1).Linear combinations, span and LinSpan 139
(b) Show that an arbitrary vector (x, y) ∈ R
2
can be written as a linear combination of the
vectors (1, 1) and (−1, 1). Deduce that (1, 1) and (−1, 1) together span R
2
.
Solution
(a) We form the linear combination
(8, 4) = k1(1, 1) + k2(−1, 1)
where k1, k2 ∈ R are yet to be determined. Then, equating components,
8 = k1 − k2, 4 = k1 + k2.
These simultaneous equations have solution k1 = 6, k2 = −2 thus
(8, 4) = 6(1, 1) − 2(−1, 1)
which is the required linear combination.
(b) We form the linear combination
(x, y) = k1(1, 1) + k2(−1, 1)
where k1, k2 ∈ R. Then, equating components,
x = k1 − k2, y = k1 + k2.
These simultaneous equations have solution k1 =
x+y
2
, k2 =
y−x
2
thus
(x, y) = x + y
2
(1, 1) + y − x
2
(−1, 1)
which is the required linear combination. Thus any vector, (x, y), in R
2
can be written in
this way and therefore (1, 1) and (−1, 1) span the whole of R
2
. As we shall see shortly, there
are many other pairs of vectors which also span R
2
.
Exercises
6.7 Show that the vectors (1, 0, 0), (0, 1, 0) and (0, 0, 1) span R
3
.
6.8 Show that the vectors (1, 0, 0), (0, 1, 0) and (1, 1, 0) do not span R
3
. Importantly
note that just because we have three vectors in a three-dimensional space, it does
not follow that they span the space.
Consider an m × n matrix A with real elements:
A =


a11 a12 a13 . . . a1n
a21 a22 a23 . . . a2n
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
am1 am2 am3 . . . amn


.
We can regard each row of the matrix as a distinct n-dimensional vector. We can then form
linear combinations of these row vectors.140 Vector spaces
Definition 6.5 Row space and column space of a matrix
The subspace of R
n spanned by the rows of an m × n matrix A is called the row space of
A. Similarly, we can regard each column as a distinct m-dimensional column vector and the
subspace of R
m spanned by these is called the column space of A.
Example 6.5.2 The row space of matrix A
Consider A =

2 1
1 1 
. Regarding each row as a vector, we can form linear combinations:
k1(2, 1) + k2(1, 1), k1, k2 ∈ R.
The set of all such linear combinations is the row space of A. Suppose we choose an arbitrary
row vector in R
2
, (x, y) say. It is of interest to ask whether this is an element of the row
space of A. We form
(x, y) = k1(2, 1) + k2(1, 1)
from which x = 2k1 + k2, y = k1 + k2 and solving for k1, k2 we find
k1 = x − y, k2 = 2y − x
so that
(x, y) = (x − y)(2, 1) + (2y − x)(1, 1)
e.g. (7, 4) = 3(2, 1) + 1(1, 1). Thus any vector in R
2
can be written as a linear combination
of the rows of A. Thus the row space of A is in fact R
2
. This is not always the case.
Consider the following example.
Example 6.5.3
Consider B =

4 2
2 1 
. The set of all linear combinations
k1(4, 2) + k2(2, 1), k1, k2 ∈ R
is the row space of B. Observe that this combination can be written
2k1(2, 1) + k2(2, 1) = (2k1 + k2)(2, 1)
so that any vector in the row space of B must be a multiple of (2, 1). It is not possible to
write an arbitrary vector in R
2 as a linear combination of the row vectors. The row space
of B does not span the whole of R
2
.
Definition 6.6 Null space of a matrix
Consider an m × n matrix A and m linear homogeneous equations in n unknowns, Ax = 0.
The set of solutions is a subspace of R
n called the null space of A.
We can think of the null space as the set of all vectors that map to the zero vector when
multiplied by A.Linear independence 141
Consider the following example.
Example 6.5.4 The null space of a matrix A
The set of two linear homogeneous equations in three unknowns
x + 3y + z = 0
y + z = 0
has matrix form AX = 0 where
A =

1 3 1
0 1 1 
, X =


x
y
z

 , 0 = 
0
0

.
Using Gaussian elimination (see Chapter 5) the full set of solutions can be written in terms
of the free variable t as x = 2t, y = −t, z = t, that is
X =


2t
−t
t

 = t


2
−1
1

 for any t ∈ R.
Here, the set of solutions is the subspace spanned by


2
−1
1

. This subspace is 1-
dimensional. Note that this subspace, as expected, includes the zero vector, (obtained by
taking t = 0). You should verify that any vector in this subspace satisfies the given set of
equations and think of this as all vectors in the null space being mapped to the zero vector
following multiplication by the matrix A.
Exercises
6.9 Consider the linear homogeneous equation x + y + z = 0. Show that the solution
space is two-dimensional and is spanned by


−1
1
0

 and


−1
0
1

.
6.10 Given A =

4 1
2 1 
show that the null space of A contains only the zero vector.
6.11 Find the null space of A =

1 1
1 1 
.
6.6 Linear independence
Definition 6.7 Linear independence
Suppose we have a vector space V over a field F. A set of vectors v1, v2, . . . , vn ∈ V is said
to be linearly independent if the only way that the linear combination
k1v1 + k2v2 + . . . + knvn
for k1, . . . , kn ∈ F, can equal the zero vector, is if all the ki
, i = 1, . . . , n, are zero.142 Vector spaces
Example 6.6.1
Consider the following linear combination of three vectors in R
3
:
k1(1, 0, 0) + k2(0, 1, 0) + k3(0, 0, 1)
and suppose this sum equals the zero vector (0, 0, 0). Then we see
k1(1, 0, 0) + k2(0, 1, 0) + k3(0, 0, 1) = (k1, 0, 0) + (0, k2, 0) + (0, 0, k3)
= (k1, k2, k3).
If this is to equal the zero vector, (0, 0, 0), then clearly k1 = k2 = k3 = 0, i.e., all ki must
be zero. The three given vectors are therefore linearly independent.
Example 6.6.2
Consider the following linear combination of three vectors in R
3
:
k1(1, 2, 3) + k2(−1, −3, −7) + k3(3, 5, 5)
and suppose this sum equals the zero vector (0, 0, 0). Then
k1(1, 2, 3) + k2(−1, −3, −7) + k3(3, 5, 5) = (k1, 2k1, 3k1) +
(−k2, −3k2, −7k2) + (3k3, 5k3, 5k3)
= (k1 − k2 + 3k3, 2k1 − 3k2 + 5k3, 3k1 − 7k2 + 5k3).
If this sum equals (0, 0, 0), then
k1 − k2 + 3k3 = 0
2k1 − 3k2 + 5k3 = 0
3k1 − 7k2 + 5k3 = 0.
Using Gaussian elimination, it is straightforward to verify that there is an infinite number
of non-zero solutions of these simultaneous equations e.g. k1 = −8, k2 = −2, k3 = 2. Clearly
not all the ki are zero, and hence, these vectors are not linearly independent. We say that
they are linearly dependent. Indeed this dependency is apparent when we see that the
final vector can be written as a linear combination of the other two:
(3, 5, 5) = 4(1, 2, 3) + (−1, −3, −7).
Exercises
6.12 Show that the row vectors (1, 1) and (−1, 1) are linearly independent.
6.13 Show that the R
3 vectors e1 = (2, 1, 3), e2 = (1, 0, 1), and e3 = (3, 2, 1) are
linearly independent and that they span R
3
.
6.14 Consider the vector space of 2 × 3 matrices and the set of vectors

1 2 3
0 1 7 
,

−2 −3 5
1 0 2 
,

2 1 9
2 2 9 
,

3 2 1
1 1 0 
.
Determine whether these vectors are linearly independent. If they are linearly
dependent, express one in terms of the other three.Basis of a vector space 143
6.15 Consider the vector space of polynomials of degree 2 or less. Determine whether
the set of polynomials p1(t) = 2t
2 + 3t − 1, p2(t) = t
2 − t − 1, p3(t) = t
2 + t is
linearly independent.
6.7 Basis of a vector space
Definition 6.8 Basis
Given a vector space V over a field F, suppose that there is a set of n linearly independent
vectors, E = {e1, e2, . . . , en}, that span the space. Then this set is said to be a basis for
V , and the vector space V is said to have finite dimension, n. We shall often abbreviate
‘dimension’ to ‘dim’. Note that a basis is not unique.
Because E spans the vector space V any vector v ∈ V can be written as a linear combination
of the basis vectors, thus
v = v1e1 + v2e2 + . . . + vnen
where vi ∈ F, i = 1, . . . , n. We will refer to v1, v2, . . . , vn as the coordinates of v with
respect to the basis E, sometimes writing the coordinate vector [v]E as
[v]E =


v1
v2
.
.
.
vn


.
The subscript E here indicates that the coordinate vector on the right depends upon the
chosen basis. If the same vector is written using a different basis say E = {e1, e2, . . . , en}
and v = v1e1 + v2e2 + . . . + vnen, where vi ∈ F, then
[v]E =


v1
v2
.
.
.
vn


.
It is important to recognise that a basis of a vector space is not unique and that any vector
can be written in any suitable basis we choose. Later we shall see that there is a need to
write the same vector in different bases and we shall discuss how this is done.
Example 6.7.1 The standard basis of R
3 over the field R
Consider the vector space R
3 of row vectors over the field R. The set of vectors, E =
{e1, e2, e3} where
e1 = (1, 0, 0), e2 = (0, 1, 0), e3 = (0, 0, 1)
is linearly independent and spans R
3
. E as defined here is referred to as the standard basis
for R
3
. Any vector v ∈ R
3
can be written as a linear combination of these basis vectors. For
example
v = (4, −3, 11) = 4(1, 0, 0) − 3(0, 1, 0) + 11(0, 0, 1) = 4e1 − 3e2 + 11e3.144 Vector spaces
Then the coordinate vector of v with respect to this basis is
[v]E =


4
−3
11

 .
We can regard a basis as the building blocks which can be used to generate any vector
in the space. Other sets of three vectors in R
3
can be found which are independent and
span the space, and hence, the basis is not unique. Observe that because we have used the
standard basis here, the components of the coordinate vector are simply the components of
the vector v = (4, −3, 11). This will not be the case when other bases are used, as we shall
see.
Example 6.7.2
Consider the vector space R
2 of column vectors over the field R. With the standard basis
E = {e1, e2} where e1 =

1
0

and e2 =

0
1

, the coordinate vector of v =

4
−8

is [v]E =

4
−8

. Express v in terms of the basis E = {e1, e2} where e1 =

1
1

and
e2 =

−1
1

.
Solution
Let
v =

4
−8

= k1e1 + k2e2 = k1

1
1

+ k2

−1
1

Then
k1 − k2 = 4
k1 + k2 = −8
from which k1 = −2, k2 = −6. Then v = −2e1 − 6e2 and we can write
[v]E =

−2
−6

.
This is the coordinate vector of v with respect to the basis E. In the following section we
show how change of basis can be achieved using matrices.
Definition 6.9 Orthonormal basis
Consider a vector space V and a basis for it. When the basis vectors are normalised (i.e.,
they each have norm equal to 1), and when they are pairwise mutually orthogonal, the basis
is said to be orthonormal.
If the basis vectors are not orthogonal, it is possible to carry out a procedure – the Gram￾Schmidt process – in order to produce an orthogonal basis. This is explained in Section
6.10.Basis of a vector space 145
We have already seen that if A is the m × n matrix with real elements:
A =


a11 a12 a13 . . . a1n
a21 a22 a23 . . . a2n
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
am1 am2 am3 . . . amn


we can regard each row of the matrix as a distinct n-dimensional vector. We can then form
linear combinations of these row vectors. The subspace of R
n spanned by these row vectors
is the row space of A.
Definition 6.10 Row rank and column rank of a matrix
The number of linearly independent vectors required to span the row space of a matrix A
is referred to as the row rank of A. Similarly, the number of linearly independent vectors
required to span the column space is referred to as the column rank of A.
It is possible to show that for any matrix A, the row rank and column rank are equal and
hence we can simply refer to the rank of the matrix.
Exercises
6.16 Show that the standard basis in Example 6.7.1 is orthonormal.
6.17 Show that the set {(1, 1),(1, −1)} is a basis for R
2
, that the basis is orthogonal
but not normalised. How would it be normalised ?
Example 6.7.3 The standard basis of C
2 over the field C
Consider the vector space C
2 of row vectors over the field C. Recall that this space includes
vectors such as (1 + i, −7 + 2i). The standard basis of this space is E = {e1, e2} where
e1 = (1, 0), e2 = (0, 1). Any vector z ∈ C
2
, that is z = (z1, z2) where z1, z2 ∈ C, can be
written in terms of the standard basis as
z = (z1, z2) = k1e1 + k2e2
= k1(1, 0) + k2(0, 1)
where k1, k2 ∈ C. Then, by equating respective components, k1 = z1 and k2 = z2, so the
coordinate vector in the standard basis is
[z]E =

z1
z2

.
For example, if
z = (1 + i, −7 + 2i) = (1 + i)e1 + (−7 + 2i)e2
the coordinate vector of z with respect to the standard basis E is
[z]E =

1 + i
−7 + 2i 
.146 Vector spaces
Example 6.7.4 The computational basis of C
2 over the field C
The column vectors |0i =

1
0

and |1i =

0
1

form an orthonormal basis for the
vector space C
2 of column vectors over the field C. This basis is often referred to as the
computational basis. We shall often write this basis as the set BC2 = {|0i, |1i}.
Example 6.7.5 An alternative basis of C
2 over the field C
The set E = {e1, e2} where e1 = √
1
2
(1, 1), e2 = √
1
2
(1, −1) is an orthonormal basis for C
2
.
Any row vector in C
2
, z = (z1, z2) say, can be expressed in terms of this basis as follows:
z = (z1, z2) = z1e1 + z2e2
= z1
1
√
2
(1, 1) + z2
1
√
2
(1, −1)
where z1, z2 are the coordinates of the given vector in this new basis. Thus
[z]E =

z1
z2

.
These coordinates can be found by equating respective components:
z1 =
1
√
2
z1 +
1
√
2
z2 (1)
z2 =
1
√
2
z1 −
1
√
2
z2 (2)
Forming (1) + (2) gives
z1 + z2 =
2
√
2
z1 so that z1 =
√
2
2
(z1 + z2) = 1
√
2
(z1 + z2).
Similarly, subtracting (2) from (1) gives
z1 − z2 =
2
√
2
z2 so that z2 =
√
2
2
(z1 − z2) = 1
√
2
(z1 − z2).
Then we have
(z1, z2) = 1
√
2
(z1 + z2)e1 +
1
√
2
(z1 − z2)e2
and thus the coordinate vector of z in the new basis is
[z]E =
 
√
1
2
(z1 + z2)
√
1
2
(z1 − z2)
!
.
We can also represent this transformation from the old to the new basis in matrix form as

z1
z2

=
1
√
2

1 1
1 −1
  z1
z2

.
The mapping from (z1, z2) to (z1, z2) is the discrete Fourier transform on C
2
, also known as
the Hadamard transform. This transform is fundamental in quantum computation as will
be discussed in Chapter 21.Change of basis matrices 147
6.8 Change of basis matrices
Suppose we have a basis E = {e1, e2, . . . , en} of an n-dimensional vector space V over a field
F. Suppose now we have a second basis of V , E = {e1, e2, . . . , en}. We may wish to convert
a vector with coordinates in terms of the original basis into one with coordinates in terms of
the new basis. We show how this can be achieved with multiplication by a suitable matrix.
Each of the original basis vectors must be written as a linear combination of the new ones:
e1 = p11e1 + p21e2 + . . . + pn1en
e2 = p12e1 + p22e2 + . . . + pn2en
.
.
. =
.
.
.
en = p1ne1 + p2ne2 + . . . + pnnen.
Here the pij ∈ F. The way that the subscripts in the equations above have been numbered
is deliberate; the reason for this choice will become apparent. An arbitrary vector v ∈ V
can be written in terms of the original basis as
v = c1e1 + c2e2 + . . . + cnen ci ∈ F
so that its coordinate vector with respect to the original basis is
[v]E =


c1
c2
.
.
.
cn


We can then express v in terms of the new basis E as follows:
v = c1e1 + c2e2 + . . . + cnen
= c1(p11e1 + p21e2 + . . . + pn1en)
+ c2(p12e1 + p22e2 + . . . + pn2en)
.
.
.
+ cn(p1ne1 + p2ne2 + . . . + pnnen).
Rearranging,
v = (c1p11 + c2p12 + . . . + cnp1n)e1
+ (c1p21 + c2p22 + . . . + cnp2n)e2
.
.
.
+ (c1pn1 + c2pn2 + . . . + cnpnn)en.
So that, in terms of coordinates,
[v]E =


c1p11 + c2p12 + . . . + cnp1n
c1p21 + c2p22 + . . . + cnp2n
.
.
.
c1pn1 + c2pn2 + . . . + cnpnn


=


p11 p12 . . . p1n
p21 p22 . . . p2n
.
.
.
.
.
.
.
.
.
.
.
.
pn1 pn2 . . . pnn




c1
c2
.
.
.
cn


.148 Vector spaces
The n × n matrix on the right, P say, effects the transformation from the original basis to
the new one. Therefore, in general
[v]new = P[v]original
where the columns of P are the coordinates of the original basis vectors written in terms of
the new ones.
Example 6.8.1
Consider the vector v ∈ R
2
, where in the standard basis [v]E =

4
−8

. Express v in terms
of the basis E = {e1, e2} where e1 =

1
1

and e2 =

−1
1

.
Solution
We begin by writing the original basis vectors in terms of the new ones:

1
0

= k1e1 + k2e2
= k1

1
1

+ k2

−1
1

from which k1 =
1
2
, k2 = −
1
2
so that 
1
0

=
1
2
e1 −
1
2
e2. Likewise

0
1

= k1

1
1

+ k2

−1
1

from which k1 =
1
2
, k2 =
1
2
so that 
0
1

=
1
2
e1 +
1
2
e2. Now form P in which the columns
are the coordinates of the original basis vectors written in terms of the new ones:
P =
 1
2
1
2
−
1
2
1
2
!
.
Pre-multiplying a vector written in the original basis by P gives the vector in the new basis:
[v]E = P[v]E =
 1
2
1
2
−
1
2
1
2
! 
4
−8

=

−2
−6

.
Compare this with the method of solution in Example 6.7.2.
Example 6.8.2
Consider the vector v ∈ R
2 where in the standard basis [v]E =
 
√
1
2
√
1
2
!
. Now suppose the
standard basis vectors are rotated anticlockwise through π
6
. Recall from Section 5.13 that
multiplying a column vector in R
2 by the matrix M =

cos θ − sin θ
sin θ cos θ

effects a rotationOrthogonal projections onto subspaces 149
through θ. Thus the new basis vectors are E = {e1, e2} where
e1 =
 √
3/2 −1/2
1/2
√
3/2
  1
0

=
 √
3/2
1/2

and
e2 =
 √
3/2 −1/2
1/2
√
3/2
  0
1

=

−1/2
√
3/2

.
Writing the original basis in terms of the new basis vectors:

1
0

=
√
3
2
e1 −
1
2
e2

0
1

=
1
2
e1 +
√
3
2
e2.
Therefore, the matrix which effects the change of basis from the original to the new basis is
 √
3
2
1
2
−
1
2
√
3
2
!
.
Hence, the coordinates of v where [v]E =
 
√
1
2
√
1
2
!
are given by
[v]E =
 √
3
2
1
2
−
1
2
√
3
2
! √
1
2
√
1
2
!
=


√
3+1
2
√
2
√
3−1
2
√
2

 .
6.9 Orthogonal projections onto subspaces
Consider the real vector space R
3 with the standard basis e1 = (1, 0, 0), e2 = (0, 1, 0), e3 =
(0, 0, 1) as described in Example 6.7.1. The vector (x, y, z) is shown in Figure 6.2. Imagine
shining a torch from above along the direction of the z axis. The shadow of the vector
(x, y, z) on the xy plane is referred to as the orthogonal projection onto this plane. The
‘projection line’ from the point (x, y, z) to the point (x, y, 0) is perpendicular, or orthogonal,
to the xy plane.
Now consider the subspace of R
3
spanned by just e1 = (1, 0, 0) and e2 = (0, 1, 0). This
is the xy plane. Observe that the projection onto this subspace has two components, one in
each of the directions e1 and e2. Recall from Section 5.14 that the component of an arbitrary
vector a in the direction of vector n is given by the scalar (inner) product a · nˆ, or ha, nˆi.
Denoting the projected vector by Pxy(x, y, z) we have
Pxy(x, y, z) = h(x, y, z), e1ie1 + h(x, y, z), e2ie2
= xe1 + ye2
= (x, y, 0).
More generally, if φ0, . . . , φn is an orthonormal basis for an (n+1)-dimensional inner product
space H, any subset {χ1, . . . , χk} of k elements of {φ0, . . . , φn} defines a k-dimensional150 Vector spaces
x
y
z
(x, y, z)
(x, y, 0)
e2
e3
e1
FIGURE 6.2
Orthogonal projections in R
3
.
subspace of H which we shall denote Hχ. Then, generalising the R
3
case above, for any
ψ ∈ H, the orthogonal projection, Pχψ of ψ onto the subspace Hχ is
Pχψ = hχ1, ψiχ1 + . . . + hχk, ψiχk.
Knowledge of such orthogonal projections is essential to an understanding of the measure￾ment of observables (see Chapter 13).
6.10 Construction of an orthogonal basis – the Gram-Schmidt
process
We have seen that it is possible to have a basis of a vector space V for which the basis vectors
are not orthogonal. The Gram-Schmidt process enables construction of an orthonormal
basis. We start by illustrating this process with a simple two-dimensional example and then
generalise.
Example 6.10.1 The Gram-Schmidt process
Consider the basis of row vectors in R
2 given by v1 = (1, 0), v2 = (2, 3). Firstly, you should
verify that these vectors are linearly independent, that they do form a basis for R
2 but that
they are not orthogonal.
We begin the process by normalising the given vectors if they are not already normalised:
so
vˆ1 = (1, 0), vˆ2 =
1
√
13
(2, 3).
We shall let ˆu1 and ˆu2 stand for the orthonormal basis vectors that we seek. Choose ˆu1 = ˆv1.Construction of an orthogonal basis – the Gram-Schmidt process 151
We then subtract from ˆv2 its projection in the direction of ˆu1:
vˆ2 − hvˆ2, uˆ1iuˆ1 =
1
√
13
(2, 3) − h 1
√
13
(2, 3),(1, 0)i(1, 0)
=
1
√
13
(2, 3) −
2
√
13
(1, 0)
= (0,
3
√
13
).
The result is a vector which is orthogonal to ˆu1 but which is not yet normalised. Finally,
normalising produces ˆu2 = (0, 1). We deduce that an orthonormal basis is {(1, 0),(0, 1)}.
Observe that the first vector in the new basis is equal to the first vector in the given basis.
(Note this new, orthonormal basis is not unique - consider the following example).
Example 6.10.2 The Gram-Schmidt process
Consider the basis of row vectors in R
2 given by v1 = (2, 3) and v2 = (1, 0). We shall
construct an orthonormal basis, but starting from v1 = (2, 3).
vˆ1 =
1
√
13
(2, 3), vˆ2 = (1, 0).
As before, we shall let ˆu1, and ˆu2 stand for the orthonormal basis vectors that we seek.
Choose ˆu1 = ˆv1. We then subtract from ˆv2 its projection in the direction of ˆu1:
vˆ2 − hvˆ2, uˆ1iuˆ1 = (1, 0) − h(1, 0),
1
√
13
(2, 3)i
(2, 3)
√
13
= (1, 0) −
2
√
13
(2, 3)
√
13
= (1, 0) −
2
13
(2, 3)
= ( 9
13
, −
6
13
)
The result is a vector which is orthogonal to ˆu1 but which is not yet normalised. Fi￾nally, normalising produces ˆu2 = ( √
9
117 , − √
6
117 ). We deduce that an orthonormal basis
is { √
1
13 (2, 3),( √
9
117 , − √
6
117 )}.
More generally, we have the following procedure.
Given a basis {v1, v2, . . . , vn} of an n-dimensional vector space, an orthonormal basis
{uˆ1, uˆ2, . . . , uˆn} is constructed as follows:
First, normalise each original basis vector to give {vˆ1, vˆ2, . . . , vˆn}.
Let
uˆ1 = ˆv1.
Subtract from ˆv2 its projection in the direction of ˆu1. What remains must be orthogonal to
uˆ1: so
u2 = ˆv2 − hvˆ2, uˆ1iuˆ1, uˆ2 =
u2
ku2k
.
Then subtract from ˆv3 its projections in the directions of both ˆu1 and ˆu2. What remains
must be orthogonal to both ˆu1 and ˆu2:
u3 = ˆv3 − hvˆ3, uˆ1iuˆ1 − hvˆ3, uˆ2iuˆ2, uˆ3 =
u3
ku3k
.
And continue in like fashion until ˆun is calculated and the process is complete.152 Vector spaces
Exercises
6.18 Use the Gram-Schmidt process to produce an orthonormal basis of R
3
from the
basis e1 = {1, 1, 0}, e2 = {0, 1, 1}, e3 = {2, 1, 0}.
6.11 The Cartesian product of vector spaces
Suppose we have two vector spaces, V and W, over the same field F. Recall that the
Cartesian product V × W of the sets V and W is defined to be the set of ordered pairs
(v, w), with v ∈ V , w ∈ W. For example, the product C × C is the set of ordered pairs of
complex numbers (which we have already seen in Example 6.2.1 where C × C is written
C
2
). Thus, for example,
(1 + i, 2 − 5i) ∈ C × C.
We now consider how the structure of a vector space is imposed on the Cartesian product.
Definition 6.11 The Cartesian product of vector spaces
We define the operations of addition and scalar multiplication on the set V ×W in a natural
way:
(v1, w1) + (v2, w2) = (v1 + v2, w1 + w2)
k(v1, w1) = (kv1, kw1) k ∈ F.
We emphasise here that, for example, v1 + v2 is the addition of the two vectors v1 and v2
in V and not the addition of the components of a vector v. It follows immediately that with
these rules, the set V × W is a vector space over F as should be verified.
6.12 Equivalence classes defined on vector spaces
Example 6.12.1
Consider a vector space V and a subspace U. Choose any two vectors v, w ∈ V . Define the
relation R by
wRv if w − v ∈ U.
So, vectors w and v are related if their difference lies in the subspace U. We show that R
is an equivalence relation. Firstly, note that for any w ∈ V , wRw because w − w = 0 ∈ U.
This is because U is a subspace and hence contains the zero element (reflexivity). Secondly,
note that if wRv then w − v ∈ U. But since U is a subspace, then it contain the inverse
−(w − v) = v − w. Hence, v − w ∈ U and so vRw (symmetry). Finally, note that if w1Rw2
and w2Rw3 then
w1 − w2 ∈ U, w2 − w3 ∈ U.
Hence, the sum of these is also in U:
(w1 − w2) + (w2 − w3) = w1 − w3 ∈ UThe sum of a vector and a subspace 153
and so w1Rw3 (transitivity). Thus R defines an equivalence relation on V .
Recall from Chapter 1 that the set of all elements in V equivalent to a fixed element v
is called the equivalence class of v, denoted [v] (not to be confused with coordinate vector
[v]basis which has a subscript). So using the vector space V , subspace U and the equivalence
relation R of Example 6.12.1 we can fix v ∈ V and calculate its equivalence class [v] by
writing down the set of all elements in V that are equivalent to v, that is all w for which
w − v ∈ U. By definition [v] = {w : wRv} = {w : w − v ∈ U}. For any such w, we have
w − v = u ∈ U so w = v + u.
6.13 The sum of a vector and a subspace
Definition 6.12 The sum of a vector and a subspace; cosets
Suppose v is a specific vector in V and that U is a subspace of V . We define the sum of
v and U to be the set
v + U = {v + u : u ∈ U}.
That is, v + U is the set of all vectors obtained by adding to the specific vector v all the
vectors in the subspace U.
The set of points {v + u : u ∈ U} is called an affine subset or coset.
Example 6.13.1
Referring to Example 6.4.1, suppose we form the set of vectors v + U when U = {(x, y) ∈
R
2
: y = 5x} and v = (4, 1) say. Thus this set includes the vectors
(4, 1),(5, 6),(6, 11),(7, 16), . . .
As before we can depict these vectors by drawing line segments from the origin as shown in
Figure 6.3. Observe that the endpoints of the vectors lie on a line parallel to the subspace
U. This set of points is referred to as an affine subset or coset of U. We can repeat this
process for other vectors v. The case when v = (11, 4) is illustrated. Note that this second
affine subset is a line parallel to the first, and to the subspace U.
Example 6.13.2
As in Example 6.13.1 suppose we form the set of vectors v + U when U = {(x, y) ∈ R
2
:
y = 5x} and v = (−1, −5) say. Observe that in this example v ∈ U. Write down several
vectors in this set and deduce that because v ∈ U then v + U = U. Likewise, deduce that
−v + U = U.
Solution
The subspace U contains vectors of the form k(1, 5) where k ∈ R. Vectors in the set v + U
take the form (−1, −5) + k(1, 5) and hence this set includes, for example,
(−1, −5),(0, 0),(1, 5).
This set is identical to U. We deduce that when v ∈ U, v+U = U. It follows that −v+U = U.154 Vector spaces
x
2 4 6 8 10
4
6
8
10
2
12
12
14
16
0 -4 -2
y
(4,1)
14
subspace U
(5,6)
(6,11)
an affine subset
or coset of U in V
x
2 4 6 8 10
6
8
10
2
12
12
14
16
0 -2-4
y
14
subspace U
affine subsets
or cosets of U in V
4
(6 ,11)
(12,9)
(11,4)
(5 ,6)
(4 ,1)
(a) (b)
FIGURE 6.3
(a) Subspace U and an affine subset, or coset, of U in V . (b) A second coset.
In general, for any fixed v ∈ V , the set v + U can be thought of as a set of points on a
line passing through v and parallel to the line representing the subspace U. As in Example
6.13.1, we refer to such sets as affine subsets or cosets of U in V . Note that these subsets
are not subspaces. In yet more general cases when U is any subspace of V , we say that the
affine subset v + U is parallel to U. That is, we can make use of the geometric terminology
even in more abstract cases.
Returning to the equivalence relation in Example 6.12.1, wRv if w − v ∈ U, and the
corresponding equivalence class [v],
[v] = {w : w − v ∈ U}, that is [v] = {w : w = v + u for u ∈ U}
observe that the affine subset generated by v is its equivalence class [v].
6.14 The quotient space
Definition 6.13 Let V be a vector space and U a subspace of V . The set of all affine
subsets of V that are parallel to U is called the quotient space V /U.
So we think of fixing U, and let v vary throughout V :
V /U = {v + U : v ∈ V }
that is, V /U is the set of cosets of U in V . Note the subtle distinction. Here we are fixing
the subspace U and varying v throughout V , in contrast to the earlier definition of the sum
of a vector and a subspace. It is possible to show that these affine subsets partition V into
mutually disjoint subsets. This is equivalent to saying that the set of equivalence classes [v]
for v ∈ V partitions V into mutually disjoint subsets. Every vector v lies in one and only
one of these subsets.End-of-chapter exercises 155
Addition and scalar multiplication can be defined on the set V /U in the following way,
so that this set is a vector space. Suppose we select a subspace U, and then choose any
v, w ∈ V and k ∈ F. Then
v + U ∈ V /U, w + U ∈ V /U.
Define the sum of these elements as follows:
(v + U) + (w + U) = (v + w) + U.
Define multiplication by a scalar as
k(v + U) = (kv) + U.
With these definitions, the quotient space can be shown to satisfy the axioms of a vector
space. The zero of the quotient space V /U is the subspace U itself which is the coset
corresponding to the zero element of V .
6.15 End-of-chapter exercises
1. Let |ai and |bi be orthogonal unit vectors in a real inner product space. Consider
the linear combination |ψi = α|ai + β|bi where α, β ∈ R. Show that |ψi is a unit
vector if α
2 + β
2 = 1.
2. Let |ai and |bi be orthogonal unit vectors in a complex inner product space.
Consider the linear combination |ψi = α|ai + β|bi where α, β ∈ C. Show that |ψi
is a unit vector if |α|
2 + |β|
2 = 1.
3. Consider the set, P1, of all polynomials in t of degree 0 or 1.
(a) Show that this set with the usual operations of addition and multiplication
by a scalar is a vector space over R.
(b) Show that the set {e0, e1} = {1, t} is a basis for this vector space (the stan￾dard basis).
(c) Show that the set {e0, e1} = {1, 1 − t} is an alternative basis.
(d) Express the polynomial p = 5 + 2t in terms of the alternative basis in part
(c).
4. Consider the vector space over R, P2 say, of all polynomials in t of degree 2 or
less. Show that the set {f0, f1, f2} = {1, t, t2} is a basis for this vector space.
5. Consider the following set of vectors in R
4
:
u =


1
2
0
1


, v =


2
2
0
1


, w =


1
1
1
0

 .
Determine whether the vector r =


8
11
1
5


is an element of the space spanned
by u, v and w.156 Vector spaces
6. Consider the set
n 1
√
3
(1, 1, 1),
1
√
3
(1, e
2πi/3
, e
4πi/3
),
1
√
3
(1, e
4πi/3
, e
8πi/3
)
o
.
(a) Show that the set is orthonormal.
(b) Show that any vector in C
3
can be expressed in terms of vectors from this
set and deduce that the set is a basis for C
3
.
7. Consider C
2
, the set of ordered pairs of complex numbers (z1, z2) where z1, z2 ∈ C.
With addition and scalar multiplication defined by
(z1, z2) + (w1, w2) = (z1 + w1, z2, +w2), α(z1, z2) = (αz1, αz2)
where z1, z2, w1, w2, α ∈ C, show that C
2
satisfies the axioms for a vector space
over C.
8. Consider the vector space C
2 over C. Show that the set
{(i, 0),(0, i)}
is a basis and express z = (z1, z2) in terms of this basis.
9. Consider C
2
, the set of ordered pairs of complex numbers (z1, z2) where z1, z2 ∈ C.
As a vector space over the set of real numbers R this is a four-dimensional vector
space. Show that the set
{(1, 0),(0, 1),(i, 0),(0, i)}
is a basis, and express the vector z = (3 + 4i, −7 − 2i) in terms of this basis.7
Eigenvalues and eigenvectors of a matrix
7.1 Objectives
We have already pointed out that in quantum computation vectors are used to represent the
state of a system. Matrices are used in the modelling of the measurement of observables and
to move a system from one state to another by acting on state vectors. Measured values
of observables are permitted to take on only certain special values, and these values are
related to the eigenvalues of the measurement matrix. Post-measurement the system is in
a state represented by an eigenvector or eigenvectors. The objective of this chapter is to
introduce these essential topics and to show how eigenvalues and eigenvectors are calculated.
In quantum computation it is relevant to consider cases in which the eigenvalues are all real
and the eigenvectors are orthogonal. We discuss the conditions under which this happens.
7.2 Preliminary definitions
Given a square n × n matrix A with elements from a field F (real or complex numbers), we
have seen that we can find the product of A and an n × 1 matrix (a column vector), v, to
obtain Av, which will also be an n×1 matrix (a column vector). For any given n×n matrix
A, we now consider particular sets of non-zero vectors v which have the special property
that Av = λv where λ is a scalar in the field F. That is, the result of multiplying the vector
v by A is a scalar multiple of v. (To be of any interest, we will require the vectors v to be
non-zero vectors since the equation Av = λv is always true if v = 0.)
Definition 7.1 Eigenvalues and eigenvectors of a matrix
If A is an n × n matrix, a non-zero vector v is an eigenvector of A if
Av = λv
for some scalar λ called an eigenvalue of A.
Geometrically, if we were working in R
2 or R
3
this means that multiplication by A results
in a vector which is in the same direction as v if λ > 0 and is in the opposite direction to
v if λ < 0. Such vectors are called eigenvectors of A corresponding to the eigenvalue λ.
The vector – scalar pairs v, λ are significant in the study of quantum computation because
quantum states can be expressed in terms of eigenvectors, and eigenvalues are related to
the possible measured values of observables.
DOI: 10.1201/9781003264569-7 157158 Eigenvalues and eigenvectors of a matrix
7.3 Calculation of eigenvalues
We begin by discussing how to calculate the eigenvalues of a matrix A. Recall that if I is
the n×n identity matrix then Iv = v. We can then write Av = λv as Av = λIv from which
Av − λIv = 0
or
(A − λI)v = 0.
Here 0 represents a column vector of zeros, i.e., the zero vector. This equation represents a
set of n linear homogeneous equations. To see this, consider the following example.
Example 7.3.1
If A =

1 4
2 3 
and v =

v1
v2

, write down the set of linear homogeneous equations
corresponding to (A − λI)v = 0.
Solution
A − λI =

1 4
2 3 
− λ

1 0
0 1 
=

1 4
2 3 
−

λ 0
0 λ

=

1 − λ 4
2 3 − λ

.
Then (A − λI)v = 0 becomes

1 − λ 4
2 3 − λ
  v1
v2

=

0
0

and writing out in full
(1 − λ)v1 + 4v2 = 0
2v1 + (3 − λ)v2 = 0
which is the required set of linear homogeneous equations in the unknowns v1 and v2.
Returning to the more general equation (A − λI)v = 0, non-trivial solutions exist only if
v is in the null space of A − λI. Recall from Section 6.5 that the null space is the set of
all vectors which map to the zero vector. Equivalently, from Section 5.12, A − λI must be
singular, that is
det(A − λI) = |A − λI| = 0.
Definition 7.2 Eigenvalues and the characteristic equation
The eigenvalues, λ, of a square n × n matrix A are found by solving the characteristic
equation
det (A − λI) = |A − λI| = 0.Calculation of eigenvalues 159
As we shall see in the following examples, this is a polynomial equation of degree n. It has
n roots (some of which may be complex) and hence n eigenvalues (some of which may be
repeated).
Definition 7.3 Algebraic multiplicity
The number of times a particular eigenvalue occurs in the solution of the characteristic
equation is called its algebraic multiplicity.
Definition 7.4 Spectrum
The set of eigenvalues of A, {λi}, is referred to as the spectrum of A.
Consider the following examples.
Example 7.3.2
Show that the matrix A =

1 4
2 3 
has two real distinct eigenvalues.
Solution
The characteristic equation is
|A − λI| =





1 4
2 3 
− λ

1 0
0 1 


 =




1 − λ 4
2 3 − λ




= 0,
that is
(1 − λ)(3 − λ) − 8 = λ
2 − 4λ − 5 = 0.
Factorising,
(λ − 5)(λ + 1) = 0
λ = 5, −1.
Thus there are two real eigenvalues: 5 and −1. The spectrum of A is {5, −1}.
Example 7.3.3
Show that the matrix A =

1 −1
1 1 
has two distinct complex eigenvalues.
Solution
The characteristic equation is
|A − λI| =





1 −1
1 1 
− λ

1 0
0 1 


 =




1 − λ −1
1 1 − λ




= 0,
that is
(1 − λ)(1 − λ) + 1 = λ
2 − 2λ + 2 = 0.160 Eigenvalues and eigenvectors of a matrix
Then
λ =
−(−2) ±
p
(−2)2 − 4(1)(2)
2
=
2 ±
√
−4
2
= 1 ± i.
The characteristic equation has complex roots 1 + i, 1 − i, and thus the eigenvalues are
1 + i and 1 − i. The observation that the eigenvalues are not real is important. As we
shall see, because eigenvalues are related to the measured values of observables, in quantum
computation they must be real.
As a further example, it is straightforward to verify that the matrix A =

4 1
−1 2 
has
a repeated real eigenvalue λ = 3. Thus the algebraic multiplicity of this eigenvalue is 2.
Exercises
7.1 Find the eigenvalues of 
1 0
0 0 
. Write down the spectrum and comment on
algebraic multiplicity.
7.2 If A is a real symmetric 2 × 2 matrix, prove that the eigenvalues of A are real.
7.3 Show that if D is a diagonal matrix, then the elements on the diagonal are the
eigenvalues of D.
7.4 An upper triangular matrix U is a square matrix for which all elements below
the leading diagonal are zero. Show that for such a 2 × 2 or 3 × 3 matrix, the
elements on the diagonal are the eigenvalues of U. This result generalises to any
n × n upper triangular matrix.
7.4 Calculation of eigenvectors
For each eigenvalue, λ, of the matrix A we calculate the corresponding eigenvector or eigen￾vectors, v, using the equation Av = λv. We shall see that once we have calculated an
eigenvector then any multiple of that vector is also an eigenvector: that is, eigenvectors
are only defined up to a constant multiple. Thus we can think of an eigenvector as defin￾ing a direction - the eigendirection. Often we will normalise the eigenvector so that its
magnitude is 1. We draw attention to some important characteristics of eigenvalues and
eigenvectors which are explored in the examples which follow:
1. An n×n matrix may have n distinct eigenvalues and (as a consequence) n linearly
independent eigenvectors.
2. On the other hand, some eigenvalues may be repeated, i.e., an n × n matrix may
not have n distinct eigenvalues. However, it is still possible for the matrix to
possess n linearly independent eigenvectors.
3. When some eigenvalues are repeated, it is possible that a full set of n linearly
independent eigenvectors does not exist.
4. We shall also find that in some cases, the eigenvectors are mutually orthogonal.Calculation of eigenvectors 161
Example 7.4.1 Matrix A possesses distinct eigenvalues and a full set of eigen￾vectors
In Example 7.3.2, we showed that the matrix A =

1 4
2 3 
has eigenvalues λ = 5, −1.
Find an eigenvector corresponding to each of these values. Show that these eigenvectors are
linearly independent and hence that a full set (n = 2) of eigenvectors exists. Show that the
eigenvectors are not orthogonal.
Solution
First consider λ = 5. Writing v =

v1
v2

then Av = 5v implies

1 4
2 3   v1
v2

= 5 
v1
v2

.
Writing out these equations explicitly and rearranging we obtain the homogeneous set
−4v1 + 4v2 = 0
2v1 − 2v2 = 0.
These equations can be solved using Gaussian elimination, but note that both are equivalent
to −v1 + v2 = 0, i.e., v2 = v1. Thus there is an infinite number of solutions each being a
multiple of 
1
1

. Equivalently, there is an infinite number of eigenvectors corresponding
to eigenvalue λ = 5 and each is a multiple of 
1
1

. We can explicitly show this by writing
v = µ

1
1

where µ is any real non-zero number we wish, referred to as a free variable.
Often we will require the eigenvector to be normalised, in which case v = √
1
2

1
1

=

1/
√
2
1/
√
2

.
To find the eigenvectors corresponding to λ = −1, we proceed in a similar way by solving
Av = −v. You should verify that a suitably normalised eigenvector is v =

2/
√
5
−1/
√
5

.
It is straightforward to verify that the two eigenvectors 
1/
√
2
1/
√
2

and 
2/
√
5
−1/
√
5

are
linearly independent. This means that one cannot be written as a multiple of the other.
Finally, note that taking the inner product
h

1/
√
2
1/
√
2

,

2/
√
5
−1/
√
5

i =
2
√
10
−
1
√
10
=
1
√
10
6= 0
and so the eigenvectors are not orthogonal.
Example 7.4.2 Matrix A possesses distinct eigenvalues and a full set of orthog￾onal eigenvectors
Show that the eigenvalues of the matrix A =

0 0
0 1 
are real and distinct. Find the
corresponding eigenvectors and show that these are linearly independent. Deduce that a162 Eigenvalues and eigenvectors of a matrix
full set of eigenvectors exists. Show that eigenvectors corresponding to different eigenvalues
are orthogonal.
Solution
As previously, the eigenvalues are found by solving det(A − λI) = 0, that is




−λ 0
0 1 − λ



 = λ
2 − λ = λ(λ − 1) = 0.
Thus the eigenvalues are λ = 0 and 1. For each eigenvalue in turn we solve Av = λv to
determine the eigenvectors v. For λ = 0, you should verify that a corresponding eigenvector
is 
1
0

. For λ = 1, a corresponding eigenvector is 
0
1

. The two vectors 
1
0

and

0
1

are linearly independent – clearly, one cannot be written as a multiple of the other.
Further, taking the inner product,
h

1
0

,

0
1

i = (1)(0) + (0)(1) = 0
and so the eigenvectors are orthogonal.
Note that in the previous two examples, the matrices each had two distinct eigenvalues and
their respective eigenvectors were linearly independent though not necessarily orthogonal.
This result is generalised in the following theorem which we state without proof.
Theorem 7.1 If an n × n matrix A has n distinct eigenvalues, then A has a full set of n
linearly independent eigenvectors.
Now consider the following two examples where the eigenvalues are not distinct.
Example 7.4.3 Matrix A has a repeated eigenvalue and only one independent
eigenvector
Show that the matrix A =

−3 4
−4 5 
has a repeated real eigenvalue and only one inde￾pendent eigenvector.
Solution
The eigenvalues are found from




−3 − λ 4
−4 5 − λ



 = λ
2 − 2λ + 1 = (λ − 1)2 = 0
and hence, there is a single repeated real eigenvalue λ = 1. To find any eigenvectors, we
solve

−3 4
−4 5   v1
v2

= 1 
v1
v2

that is
−4v1 + 4v2 = 0
−4v1 + 4v2 = 0Calculation of eigenvectors 163
that is v1 = v2, and so there is just one independent (normalised) eigenvector 1
√
2

1
1

.
The matrix A does not have a full set of eigenvectors.
Example 7.4.4 Matrix A has a repeated eigenvalue and two independent eigen￾vectors
Show that the matrix A =

3 0
0 3 
has a repeated real eigenvalue and two independent
orthogonal eigenvectors.
Solution
The eigenvalues are found from




3 − λ 0
0 3 − λ




= (3 − λ)
2 = 0
and hence, there is a single repeated real eigenvalue λ = 3. To find any eigenvectors, we
solve

3 0
0 3   v1
v2

= 3 
v1
v2

that is
3v1 = 3v1
3v2 = 3v2.
These two equations are satisfied by any v1 and v2. Thus there are two independent eigen￾vectors,

1
0

, and 
0
1

,
(for example). It is clear that these two eigenvectors are orthogonal. Further, it is readily
verified that any linear combination of the two eigenvectors corresponding to λ = 3 is also
an eigenvector.
Observe that in Example 7.4.3 there was a single eigenvalue and a single corresponding
eigenvector. On the other hand in Example 7.4.4 there was a single eigenvalue and two
independent eigenvectors. We now draw out some general results.
For an n × n matrix, for each eigenvalue, λ, we can calculate corresponding eigenvec￾tors. It is possible for there to be up to n independent eigenvectors associated with each
eigenvalue.
Definition 7.5 Geometric multiplicity
Suppose λ is an eigenvalue of matrix A. For each λ, the number of linearly independent
eigenvectors is called the geometric multiplicity of λ.
For a specific λ its eigenvectors, together with the zero vector, form a vector space called
the eigenspace of A corresponding to λ. When a one-dimensional eigenspace is generated
by an eigenvalue then that eigenvalue is said to be non-degenerate (e.g. both eigenvalues
in Example 7.4.1). On the other hand when the eigenspace generated by an eigenvalue
has dimension greater than 1 the eigenvalue is said to be degenerate (see Example 7.4.4164 Eigenvalues and eigenvectors of a matrix
in which there is a two-dimensional eigenspace generated by λ = 3). Any set of linearly
independent vectors which span an eigenspace is said to be a basis for that eigenspace.
Example 7.4.5 Repeated eigenvalues, degeneracy and a full set of eigenvectors
Find the eigenvalues and corresponding eigenvectors of the matrix
A =


0 0 0 0
0 1 0 0
0 0 0 0
0 0 0 1

 .
Solution
We solve |A − λI| = 0:
|A − λI| =








−λ 0 0 0
0 1 − λ 0 0
0 0 −λ 0
0 0 0 1 − λ








= −λ






1 − λ 0 0
0 −λ 0
0 0 1 − λ






= −λ(1 − λ)




−λ 0
0 1 − λ




= −λ(1 − λ)(−λ)(1 − λ)
= λ
2
(1 − λ)
2
.
Solving λ
2
(1 − λ)
2 = 0 gives the eigenvalues λ = 0, λ = 1, both with algebraic multiplicity
2.
λ = 0: we solve Av = 0v = 0. Thus


0 0 0 0
0 1 0 0
0 0 0 0
0 0 0 1




v1
v2
v3
v4

 =


0
0
0
0

 .
It follows immediately that v2 = v4 = 0 whilst v1 and v3 are free variables. Thus there are
two independent (and normalised) eigenvectors corresponding to λ = 0:


1
0
0
0

 ,


0
0
1
0


and thus the eigenvalue λ = 0 is degenerate.
λ = 1: we solve Av = v.


0 0 0 0
0 1 0 0
0 0 0 0
0 0 0 1




v1
v2
v3
v4

 =


v1
v2
v3
v4

 .Calculation of eigenvectors 165
Simplifying:


−1 0 0 0
0 0 0 0
0 0 −1 0
0 0 0 0




v1
v2
v3
v4

 =


0
0
0
0

 .
Then v1 = v3 = 0 whilst v2 and v4 are free variables. Thus there are two independent (and
normalised) eigenvectors corresponding to λ = 1:


0
1
0
0

 ,


0
0
0
1


and thus the eigenvalue λ = 1 is degenerate. Clearly the geometric multiplicity of each
eigenvalue is 2. There are thus four independent eigenvectors. We shall meet these again
later in the context of 2-qubit quantum states.
Example 7.4.6 Repeated eigenvalue
Find the eigenvalues and eigenvectors of A =


5 −1 −1
2 2 −2
1 −1 3

.
Solution
Consider Av = λv. The eigenvalues are calculated from
|A − λI =






5 − λ −1 −1
2 2 − λ −2
1 −1 3 − λ






= (5 − λ)




2 − λ −2
−1 3 − λ




+ 1




2 −2
1 3 − λ



 − 1




2 2 − λ
1 −1




= (5 − λ)(λ
2 − 5λ + 4) + 1(8 − 2λ) − 1(−4 + λ)
= −λ
3 + 10λ
2 − 32λ + 32 = 0.
The cubic equation can be solved either by drawing a graph and searching for the roots or
using online or computer software. We find the three eigenvalues are λ = 2, 4, 4. Now let
v =


v1
v2
v3


and consider again Av = λv.
λ = 2.
3v1 − v2 − v3 = 0
2v1 − 2v3 = 0
v1 − v2 + v3 = 0.
We solve these equations using Gaussian elimination. The augmented matrix is


3 −1 −1 0
2 0 −2 0
1 −1 1 0

 .166 Eigenvalues and eigenvectors of a matrix
Performing the row operations R2 → 3R2 − 2R1, and R3 → 3R3 − R1 produces


3 −1 −1 0
0 2 −4 0
0 −2 4 0

 .
Then R3 → R3 + R2 gives


3 −1 −1 0
0 2 −4 0
0 0 0 0

 .
Let v3 = µ, then by back substitution v2 = 2µ and v1 = µ. Thus the eigenvectors corre￾sponding to λ = 2 have the form v = µ


1
2
1

. Observe that λ = 2 is non-degenerate.
λ = 4. This gives rise to the following linear simultaneous equations:
v1 − v2 − v3 = 0
2v1 − 2v2 − 2v3 = 0
v1 − v2 − v3 = 0
all three of which are equivalent. We need only consider v1 − v2 − v3 = 0. Let v3 = µ and
v2 = ν from which v1 = µ + ν. We can formally write down the eigenvectors as
v = µ


1
0
1

 + ν


1
1
0

 .
Observe that λ = 4 is degenerate because its eigenspace has dimension greater than 1.
When calculating eigenvalues and eigenvectors, the foregoing examples make clear that a
variety of scenarios are possible; these are associated with multiplicity, linear independence
and orthogonality. In the study of quantum computation, the matrices with which we usually
work have properties that enable us to be quite specific about the nature of the eigenvalues
and eigenvectors. We explore these properties in the following sections.
Exercises
7.5 Find the eigenvalues and eigenvectors of A =

1 3
4 −1

.
7.6 Find the eigenvalues and eigenvectors of B =


4 −1 1
−2 4 0
−4 3 1

.
7.5 Real symmetric matrices and their eigenvalues and
eigenvectors
The eigenvalues and eigenvectors of real symmetric matrices have important properties
which are highly relevant to the study of quantum computation. We now state theseReal symmetric matrices and their eigenvalues and eigenvectors 167
properties prior to discussing and giving examples. We emphasise that attention here is
restricted to symmetric matrices with real elements. Symmetric matrices with complex ele￾ments do exist but are beyond the scope of this discussion. Given any real symmetric n × n
matrix A:
1. The eigenvalues of A are always real.
2. Eigenvectors arising from distinct eigenvalues are orthogonal.
3. Eigenvectors arising from the same eigenvalue may not be orthogonal but using
them it is possible to generate an orthogonal set.
4. The matrix A possesses an orthonormal set of n linearly independent eigenvectors
which can be used as a basis for the vector space C
n over the field C.
5. It is possible to perform a transformation on A to represent it as a diagonal matrix
D in a process called orthogonal diagonalisation. Specifically, D = P
T AP where
P is an orthogonal matrix constructed using the n orthonormal eigenvectors.
These important points are illustrated in the examples which follow this theorem.
Theorem 7.2 The eigenvalues of any 2 × 2 real symmetric matrix are real.
Proof
Consider the symmetric matrix A =

a b
b d 
where a, b, d ∈ R. Its eigenvalues are found
by solving the quadratic characteristic equation:
λ
2 − (a + d)λ + ad − b
2 = 0.
Consider the discriminant ∆ = (a + d)
2 − 4(ad − b
2
). Rearranging
∆ = a
2 + d
2 + 2ad − 4ad + 4b
2
= (a − d)
2 + 4b
2
.
As the sum of two squared real numbers, ∆ ≥ 0 and so the roots of the quadratic equation,
and hence the eigenvalues, are real. Further, for this 2 × 2 matrix, the roots and hence the
eigenvalues are equal if and only if ∆ = 0, that is a = d and b = 0. In this case the matrix
has the form A =

a 0
0 a

, that is, a multiple of the identity matrix.

Example 7.5.1 Real and distinct eigenvalues
Show that the eigenvalues of the symmetric matrix A =

2 3
3 −2

are real and find the
corresponding eigenvectors. Show that these eigenvectors are orthogonal.
Solution
The eigenvalues are found by solving |A − λI| = 0, that is




2 − λ 3
3 −2 − λ



 = λ
2 − 13 = 0.
So there are two distinct real eigenvalues λ = ±
√
13.168 Eigenvalues and eigenvectors of a matrix
For λ =
√
13 its eigenvectors are found by solving Av =
√
13v:

2 3
3 −2
  v1
v2

=
√
13 
v1
v2

(2 −
√
13)v1 + 3v2 = 0
3v1 − (2 + √
13)v2 = 0.
From which we can take an eigenvector, v
(
√
13) say, to be any multiple of  2+√
13
3
1

.
Likewise eigenvectors corresponding to λ = −
√
13, v
(−
√
13) say, are multiples of  2−
√
13
3
1

.
If we take the inner product (scalar product) of the two eigenvectors we find
hv
(
√
13), v−(
√
13)i =
 2+√
13
3
1

·
 2−
√
13
3
1

=
4 − 13
9
+ 1 = −1 + 1 = 0
and thus the eigenvectors corresponding to the two distinct eigenvalues are orthogonal.
When necessary these vectors can be normalised. Moreover, these two vectors can be used
to construct an orthogonal basis for C
2
. This means that they can be used as building
blocks for any vector in C
2
.
Example 7.5.2 A repeated eigenvalue
a) Show that the eigenvalues of the symmetric matrix A =

3 0
0 3 
are real.
b) Write down a pair of independent eigenvectors which are not orthogonal.
c) Write down a pair of independent eigenvectors which are orthogonal.
Solution
a) The eigenvalues are found by solving |A − λI| = 0, that is




3 − λ 0
0 3 − λ



 = λ
2 − 6λ + 9 = (λ − 3)2 = 0.
So λ = 3 (twice). That is, the eigenvalue λ = 3 has algebraic multiplicity 2.
b) The eigenvectors are found by solving Av = 3v:

3 0
0 3   v1
v2

= 3 
v1
v2

which reduces to simply 0v1 + 0v2 = 0 so that both v1 and v2 are free variables µ1 and µ2
say. The most general form of an eigenvector is then v =

µ1
µ2

. Suppose µ1 = 1, µ2 = 1,
then v =

1
1

is an eigenvector. Alternatively, suppose µ1 = 1, µ2 = 0, then v =

1
0

is also an eigenvector. It is easily verified that 
1
1

and 
1
0

are linearly independent.
They are not orthogonal since their inner product is 1 6= 0.Real symmetric matrices and their eigenvalues and eigenvectors 169
c) Suppose µ1 = 1, µ2 = 0, then v =

1
0

is an eigenvector. Alternatively, suppose
µ1 = 0, µ2 = 1, then v =

0
1

is also an eigenvector. These eigenvectors are clearly
independent, orthogonal and have norm 1. We observe that even when there is a single
(repeated) eigenvalue there are two independent eigenvectors and these can be chosen to be
orthonormal.
The behaviour illustrated in this example generalises to larger n×n symmetric matrices
which always have real eigenvalues and possess an orthonormal set of n linearly independent
eigenvectors. These can be used as a basis for the vector space C
n over the field C.
Example 7.5.3 Repeated eigenvalues: generating an orthonormal set of eigen￾vectors
Find the eigenvalues and corresponding eigenvectors of the real symmetric matrix
A =


5 2 2
2 5 2
2 2 5

 .
Produce an orthonormal set of three eigenvectors.
Solution
The eigenvalues are found by solving the characteristic equation:






5 − λ 2 2
2 5 − λ 2
2 2 5 − λ






= (5 − λ)




5 − λ 2
2 5 − λ



 − 2




2 2
2 5 − λ




+ 2




2 5 − λ
2 2




= (5 − λ)(λ
2 − 10λ + 21) − 2(6 − 2λ) + 2(−6 + 2λ)
= −λ
3 + 15λ
2 − 63λ + 81 = 0.
This cubic equation can be solved, for example by plotting a graph to locate the roots, or
by using a computer or an on-line package. It factorises as
(−λ + 3)(λ − 3)(λ − 9) = 0
so that the eigenvalues are λ = 3, 3 and 9.
To find the eigenvectors corresponding to λ = 3 we solve 2v1 + 2v2 + 2v3 = 0. (Note all
three equations are the same). We deduce there are two free variables, so let v3 = µ and
v2 = ν then v1 = −µ − ν. The eigenvectors corresponding to λ = 3 then take the general
form


v1
v2
v3

 = µ


−1
0
1

 + ν


−1
1
0

 .
Clearly, two independent eigenvectors are


−1
0
1

 and


−1
1
0

. With this choice, the
inner product is 1, so they are not orthogonal. An eigenvector corresponding to

λ = 9 is

1
1
1

 as is readily verified.170 Eigenvalues and eigenvectors of a matrix
We now construct an orthonormal set. Applying the Gram-Schmidt process (Chapter 6)
produces an orthogonal pair, for example:


−1
0
1

 and


−1
2
−1

 which when normalised
become


−1/
√
2
0
1/
√
2

 and


−1/
√
6
2
√
6
−1/
√
6

. It is straightforward to show that a normalised
eigenvector corresponding to λ = 9 is


1/
√
3
1/
√
3
1/
√
3

. We now have a full set of three orthonor￾mal eigenvectors. You should check that this is indeed the case.
7.6 Diagonalisation of real symmetric matrices
Suppose A is an n × n real symmetric matrix. As we have seen, symmetric matrices have
a full set of n linearly independent eigenvectors. Given any matrix that has n linearly
independent eigenvectors it is possible to perform a transformation in order to represent it
as a diagonal matrix. We now illustrate how this is achieved.
Form a new matrix, P say, whose columns are the independent eigenvectors of A. (At
this stage there is no requirement that the eigenvectors are orthonormal – just linearly
independent). Then evaluation of P
−1AP yields a diagonal matrix, D, with the eigenvalues
on the diagonal (in the same order as the eigenvectors appeared in the columns of P). Thus
P
−1AP = D.
Pre-multiplying by P and post-multiplying by P
−1 we can rewrite this equivalently as
A = P DP −1
so that the matrix A has been factorised using its eigenvectors and eigenvalues. The
transformation from A to P DP −1
is known as a similarity transformation.
Definition 7.6 A diagonalisable matrix
The n × n symmetric matrix A is diagonalisable because there exists an n × n matrix
P with the property that
P
−1AP = D, a diagonal matrix containing the eigenvalues.
Equivalently,
A = P DP −1
is a so-called matrix factorisation.
P is the matrix whose columns are the eigenvectors of A.
In the previous development there was no requirement that the eigenvectors be
orthonormal – just linearly independent. When we impose this additional requirement the
matrix P is an orthogonal matrix. Recall from Section 5.10 that such a matrix, P, has the
property that P
−1 = P
T
. Consequently the similarity transformation which diagonalises A
can be written
P
T AP = D or equivalently A = P DP T
.Diagonalisation of real symmetric matrices 171
Definition 7.7 Orthogonally diagonalisable
The n × n symmetric matrix A is orthogonally diagonalisable because there exists an
n × n orthogonal matrix P with the property that
P
T AP = D, a diagonal matrix containing the eigenvalues.
Equivalently,
A = P DP T
.
P is the matrix whose columns are the normalised eigenvectors of A.
Example 7.6.1

Find the eigenvalues and corresponding eigenvectors of the real symmetric matrix A =
4 2
2 4 
and hence diagonalise A.
Solution
The characteristic equation is λ
2 − 8λ + 12 = (λ − 6)(λ − 2) = 0 so that λ = 2, 6. Because
these are distinct their eigenvectors must be orthogonal as we now show. Eigenvectors
corresponding to λ = 2, 6, respectively, are readily shown to be:

1
−1

and 
1
1

.
These eigenvectors are linearly independent and orthogonal though not normalised. Form
P whose columns are the eigenvectors of A:
P =

1 1
−1 1 
from which P
−1 =
1
2

1 −1
1 1 
.
It is then straightforward to check that
P
−1AP =
1
2

1 −1
1 1   4 2
2 4   1 1
−1 1 
=

2 0
0 6 
the result being a diagonal matrix, D say, with the eigenvalues on the diagonal.
Now suppose we require that the eigenvectors be orthonormal before we form the matrix
P. So let the normalised eigenvectors be
 
√
1
2
− √
1
2
!
and 
√
1
2
√
1
2
!
.
Then take P to be
P =
 
√
1
2
√
1
2
− √
1
2
√
1
2
!
.
It is readily verified that, by construction, this matrix is orthogonal (that is its columns are
mutually orthogonal and normalised). You should confirm that this is the case. Thus A is
orthogonally diagonalisable. You should verify that P
T AP = D.172 Eigenvalues and eigenvectors of a matrix
Example 7.6.2
Consider the matrix A =


5 2 2
2 5 2
2 2 5

 of Example 7.5.3.
(a) Demonstrate diagonalisation
(b) Demonstrate orthogonal diagonalisation.
Solution
(a) We have seen that the eigenvalues of

A are λ = 9, 3, 3 with corresponding eigenvectors:

1
1
1

,


−1
0
1

 and


−1
2
−1

. So form
P =


1 −1 −1
1 0 2
1 1 −1

 .
You should note that this choice is not unique. We could have placed the eigenvectors in a
different order, or used different eigenvectors. You should verify that
P
−1 =
1
6


2 2 2
−3 0 3
−1 2 −1

 .
Then evaluating P
−1AP:
P
−1AP =
1
6


2 2 2
−3 0 3
−1 2 −1




5 2 2
2 5 2
2 2 5




1 −1 −1
1 0 2
1 1 −1


=
1
6


2 2 2
−3 0 3
−1 2 −1




9 −3 −3
9 0 6
9 3 −3


=
1
6


54 0 0
0 18 0
0 0 18


=


9 0 0
0 3 0
0 0 3


which is the required diagonal matrix. Note that the eigenvalues of A lie on the diagonal in
the same order as we placed the eigenvectors in P.
(b) Now form P using the orthonormal eigenvectors from Example 7.5.3, that is:
P =


1/
√
3 −1/
√
2 −1/
√
6
1/
√
3 0 2/
√
6
1/
√
3 1/
√
2 −1/
√
6

 .
By construction this is an orthogonal matrix. Therefore, its inverse equals its transpose:
P
−1 = P
T =


1/
√
3 1/
√
3 1/
√
3
−1/
√
2 0 1/
√
2
−1/
√
6 2/
√
6 −1/
√
6

 .The spectral theorem for symmetric matrices 173
Then
P
T AP =


1/
√
3 1/
√
3 1/
√
3
−1/
√
2 0 1/
√
2
−1/
√
6 2/
√
6 −1/
√
6




5 2 2
2 5 2
2 2 5




1/
√
3 −1/
√
2 −1/
√
6
1/
√
3 0 2/
√
6
1/
√
3 1/
√
2 −1/
√
6


=


1/
√
3 1/
√
3 1/
√
3
−1/
√
2 0 1/
√
2
−1/
√
6 2/
√
6 −1/
√
6




9/
√
3 −3/
√
2 −3/
√
6
9/
√
3 0 6/
√
6
9/
√
3 3/
√
2 −3/
√
6


=


9 0 0
0 3 0
0 0 3


which is the required diagonal matrix. Again note that the eigenvalues of A lie on the
diagonal in the same order as we placed the orthonormal eigenvectors in P.
7.7 The spectral theorem for symmetric matrices
We have seen that if A is an n × n real symmetric matrix, it has an orthonormal set of n
eigenvectors and can thus be orthogonally diagonalised by a matrix P say whose columns
are the eigenvectors:
P
T AP = D.
Equivalently, we have the decomposition:
A = P DP T
.
We now explore a different way in which this can be written leading to the spectral theorem.
Theorem 7.3 Spectral theorem
Given an n × n real symmetric matrix A with eigenvalues λi and orthonormal eigenvectors
pi the spectral decomposition of A is given by
A = P DP T = λ1p1p
T
1 + λ2p2p
T
2 + . . . + λnpnp
T
n =
Xn
i=1
λipip
T
i
.
Proof
The proof requires use of block multiplication of matrices and a general result both of which
were introduced in Chapter 5. Specifically, given an n × n matrix P whose columns are the
vectors p1, p2, . . . , pn, then
P P T = p1p
T
1 + p2p
T
2 + . . . + pnp
T
n
.
(See Example 5.5.3.) We calculate A = P DP T using block matrix multiplication as follows.
First consider P D. Let the columns of P be p1, p2, . . . , pn . That these are in fact column
vectors is indicated below by writing
p1
.
.
.
etc. Similarly, the rows of D are d1, d2, . . . , dn
which we indicate below by d1 · · · etc.
P D =
 
p1 p2 . . . pn
.
.
.
.
.
.
.
.
.
.
.
.
!


d1 . . . . . . . . .
d2 . . . . . . . . .
.
.
. · · · · · · · · ·
dn . . . . . . . . .


.174 Eigenvalues and eigenvectors of a matrix
Thus
P D =
￾
p1 p2 . . . pn



d1
d2
.
.
.
dn


= p1d1 + p2d2 + . . . + pndn
using block matrix multiplication. Because D is diag{λ1, λ2, . . . , λn} this simplifies to
P D =
 
λ1p1 λ2p2 . . . λnpn
.
.
.
.
.
.
.
.
.
.
.
.
!
=
￾
λ1p1 λ2p2 . . . λnpn

.
(See, for example, Exercise 5.12.) Then
A = P DP T =
￾
λ1p1 λ2p2 . . . λnpn



p
T
1
p
T
2
.
.
.
p
T
n


= λ1p1p
T
1 + λ2p2p
T
2 + . . . + λnpnp
T
n
which is often called the spectral theorem or the spectral decomposition of A. This allows
A to be expressed as a sum of terms where each term is an eigenvalue multiplied by the
matrix which is a projection on to the eigenspace corresponding to that eigenvalue.

Example 7.7.1 Spectral decomposition
In Example 7.6.1, we showed that the matrix A =

4 2
2 4 
had eigenvalues λ = 2, 6 with
corresponding eigenvectors 
1
−1

and 
1
1

. Perform the spectral decomposition.
Solution
The given eigenvectors are already orthogonal but we require them to be normalised. So let
p1 =

1/
√
2
−1/
√
2

and p2 =

1/
√
2
1/
√
2

.
Then
p1p
T
1 =

1/
√
2
−1/
√
2

￾
1/
√
2 −1/
√
2

=

1/2 −1/2
−1/2 1/2

p2p
T
2 =

1/
√
2
1/
√
2

￾
1/
√
2 1/
√
2

=

1/2 1/2
1/2 1/2

.
Then
A =
X
2
i=1
λipip
T
i = 2 
1/2 −1/2
−1/2 1/2

+ 6 
1/2 1/2
1/2 1/2

.
This is the required spectral decomposition, which is readily checked to equSelf-adjoint matrices and their eigenvalues and eigenvectors 175
7.8 Self-adjoint matrices and their eigenvalues and eigenvectors
The important results regarding symmetric matrices given in the previous section generalise
to the case of complex self-adjoint matrices. In particular, if A is an n × n complex self￾adjoint matrix all the eigenvalues of A are real, the eigenvectors corresponding to different
eigenvalues are mutually orthogonal and A possesses an orthonormal set of n eigenvectors.
The spectral theorem for self-adjoint matrices is given here without proof.
Theorem 7.4 The spectral theorem for self-adjoint matrices
Given an n × n complex self-adjoint matrix A with eigenvalues λi and orthonormal eigen￾vectors pi, the spectral decomposition of A is given by
A = P DP −1 = P DP† =
Xn
i=1
λipip
†
i
.
The matrix P whose columns are the orthonormal eigenvectors is a unitary matrix (i.e.,
P
−1 = P
†
). In quantum computing applications the final result above is sometimes written
in the equivalent form
A =
Xn
i=1
λi
|piihpi
|.
7.9 End-of-chapter exercises
1. Show that the characteristic equation of A =

4 −5
2 −3

is λ
2 −λ−2 = 0. Show
that A2 − A − 2I = 0, where 0 here denotes the zero matrix. Deduce that the
matrix A satisfies its characteristic equation (Cayley-Hamilton theorem).
2. Find the eigenvalues and eigenvectors of A =

1 0
0 −1

.
3. Find a 2 × 2 matrix 
a b
c d 
which has eigenvectors 
1
0

,

0
1

and eigen￾values 1 and 0 respectively.
4. Find a 2 × 2 matrix 
a b
c d 
which has eigenvectors 
1
0

,

0
1

and eigen￾values 0 and 1, respectively.
5. Find the eigenvalues and eigenvectors of A =

0 0
0 1 
.
6. Find a 2 × 2 matrix 
a b
c d 
which has eigenvectors 
√
1
2
√
1
2
!
,
 
√
1
2
− √
1
2
!
and
eigenvalues 1 and −1, respectively.
7. Find the eigenvalues and eigenvectors of
A =


0 0 0 0
0 0 0 0
0 0 1 0
0 0 0 1

 a)

A =


0 0 0 0
0 1 0 0
0 0 0 0
0 0 0 1


.b)176 Eigenvalues and eigenvectors of a matrix
8. The matrices S
2
1
, S
2
2 and S
2
3 defined below arise in the study of so-called Bell
states. Find their eigenvalues and eigenvectors:
S
2
1 =
1
2


1 0 0 1
0 1 1 0
0 1 1 0
1 0 0 1

 a)

S
2
2 =
1
2


1 0 0 −1
0 1 1 0
0 1 1 0
−1 0 0 1

 b)

S
2
3 =


1 0 0 0
0 0 0 0
0 0 0 0
0 0 0 1


.c)
(Hint: see Q13, 14 below).
9. Find the eigenvalues and eigenvectors of


1 0 0 0
0 0 0 0
0 0 0 0
0 0 0 0

 .
10. Find the eigenvalues and corresponding eigenvectors of P =

1 0
0 eiφ

.
11. Using the properties of the transpose and of determinants show that |A − λI| =
|AT − λI|. Deduce that A and AT have the same characteristic equation and
hence the same eigenvalues.
12. Suppose v is an eigenvector of B with eigenvalue λ. Suppose B is similar to A,
that is, there exists P such that B = P
−1AP. Show that λ is an eigenvalue of A.
13. If λ is an eigenvalue of A show that kλ is an eigenvalue of kA, where k is a scalar.
14. If v is an eigenvector of A with eigenvalue λ show that v is also an eigenvector of
kA with eigenvalue kλ.8
Group theory
8.1 Objectives
In this chapter, we shall see how a set G that is endowed with a binary operation ◦ – a
way of combining two set elements to yield another element in the same set – becomes a
mathematical structure called a group, denoted (G, ◦). To be a group, the structure must
satisfy rules known as group axioms which concern: closure and associativity under the
binary operation, and the existence of an identity element and inverse elements within
the set G. An objective of this chapter is to explain this terminology and provide several
examples of groups.
The evolution of quantum states is modelled through the action of unitary matrices on
state vectors. Recall from Chapter 5 that a unitary matrix U satisfies UU† = U
†U = I
where U
†
is the conjugate transpose of U. Because quantum computation is reversible, each
matrix U must have an inverse and clearly this role is filled by U
†
. We have already seen
that an n×n identity matrix, I, is unitary and also that matrix multiplication is associative.
It is these characteristics that mean the set of n × n unitary matrices together with the
operation of matrix multiplication is a group – the unitary group U(n). These and other
relevant aspects of group theory are explored in this chapter.
8.2 Preliminary definitions and the axioms for a group
We noted in Chapter 1 that a group is a mathematical structure, denoted (G, ◦), comprising
a set G and a binary operation ◦, called composition, which combines two elements of
the set G to produce another element also in G and for which particular rules, or group
axioms, hold. Specifically, we require closure under the binary operation, associativity of
the operation when applied to three (or more) elements, the existence of an identity element
within the set G and an inverse for each element of G.
Definition 8.1 Group
A set G with a binary operation ◦ is a group (G, ◦) provided:
1. given a, b ∈ G, then a ◦ b ∈ G (closure)
2. for any a, b, c ∈ G then (a ◦ b) ◦ c = a ◦ (b ◦ c) (associativity)
3. there exists an identity or unit element e ∈ G such that e ◦ a = a ◦ e = a for
all a ∈ G.
4. for any a ∈ G, there exists an element also in G, which we denote by a
−1
(not to be confused with a power or reciprocal), such that a ◦ a
−1 = a
−1 ◦ a = e.
The element denoted a
−1
is called the inverse of a.
DOI: 10.1201/9781003264569-8 177178 Group theory
Here, the binary operation ◦ stands for one of the many possible operations that we shall
see in the examples which follow. For example, the binary operation is frequently addition
modulo 2 or exclusive-or in which case we write (G, ⊕). On occasions, when the binary
operation in a ◦ b is clear we omit the operation altogether and write ab. The number of
elements in the set G is called the order of the group, denoted |G|. When the underlying
set has finite order (and is small), it can be illustrative to produce a composition table,
also known as a Cayley table, which shows the results of combining all the elements of
the group (see Example 8.2.1 below). If, in addition, the operation ◦ is commutative (that
is a ◦ b = b ◦ a, for all a, b ∈ G) then (G, ◦) is a commutative or Abelian group. In Section
1.6 we saw several example of groups from different branches of mathematics, for example
the permutation groups Pn, and the symmetric groups Sn. We also saw that if there is a
mapping between two groups which is one-to-one, onto and which is structure preserving,
we call this correspondence an isomorphism and say that the groups are isomorphic.
Formally, we have the following:
Definition 8.2 Group isomorphism
Given groups (G, ◦) and (G0
, ∗), if there exists a one-to-one and onto (i.e., bijective) mapping
Φ : G → G0
such that
Φ(g1 ◦ g2) = Φ(g1) ∗ Φ(g2) where g1, g2 ∈ G
then Φ is an isomorphism and the groups (G, ◦) and (G0
, ∗) are said to be isomorphic.
Definition 8.3 Subgroup
If a subset, H, of G is itself a group with the same operation it is called a subgroup of G
and we write H ≤ G.
Every group has at least two subgroups: the group itself, and the group consisting of just
the identity element {e}, a so-called trivial subgroup. Note that the identity element of the
group, e, must necessarily be in H in order that (H, ◦) is a subgroup.
Example 8.2.1
Consider the set G = {1, −1} and take as the binary operation the usual multiplication of
numbers, ×. Show that (G, ×) is a group.
Solution
A group composition table is shown in Table 8.1. In general, when reading a composition
TABLE 8.1
Composition
table for (G, ×)
× 1 −1
1 1 −1
−1 −1 1
table to find g1 ◦ g2 say, the first element, g1, is found in the left-most column. The second
element, g2, is found in the top row. Then inspection of the intersection in the table of the
corresponding row and column gives the required result. From Table 8.1, it is immediately
apparent that (G, ×) is closed: every element that results from a composition is a member
of the original set. The associativity of multiplication of integers is obvious. The identityPreliminary definitions and the axioms for a group 179
element of the group is 1 since 1×1 = 1, and 1×−1 = −1. Finally, each element of G has an
inverse, an element which multiplies it to give the identity 1. This is also apparent from the
table because the identity element occurs in each row and each column of the body of the
table. There are two further points to note from this example. Firstly, each element is its
own inverse, i.e., is self-inverse, and secondly, the composition table is symmetric about
the leading diagonal (top-left to bottom-right) which tells us that the group is commutative
or Abelian. These are not properties of groups in general.
Example 8.2.2
Consider the set B = {0, 1} and take as the binary operation addition modulo 2, ⊕. Show
that (B, ⊕) is a group.
Solution
A group composition table is shown in Table 8.2. Note that working in modulo 2, 1⊕1 = 0.
It is immediately apparent that (B, ⊕) is closed. Associativity follows because addition is
TABLE 8.2
Composition table for
(B, ⊕)
⊕ 0 1
0 0 1
1 1 0
associative. The identity element is 0. Each element has an inverse because the identity
element, 0, appears once in each row and column of the body of the table.
Example 8.2.3
For the groups (G, ×) and (B, ⊕) of Examples 8.2.1 and 8.2.2 find an isomorphism Φ : G → B
and deduce that the two groups are isomorphic.
Solution
From an inspection of the group tables, it is apparent that the structures are the same and
all that is required is a relabelling of elements. This is done through the mapping Φ. Define
Φ(1G) = 0B, Φ(−1G) = 1B
where we have inserted subscripts to be explicit about the group to which the elements
belong. This mapping is one-to-one and onto. We must also show that for any g1, g2 ∈ G,
Φ(g1 × g2) = Φ(g1) ⊕ Φ(g2).
Suppose g1 = 1G, g2 = −1G. Then
Φ(1G × −1G) = Φ(−1G) = 1B.
Also
Φ(1G) ⊕ Φ(−1G) = 0B ⊕ 1B = 1B.180 Group theory
Hence,
Φ(1G × −1G) = Φ(1G) ⊕ Φ(−1G).
Likewise, for any other choice of two elements in G, as you should verify. Hence, the two
groups are isomorphic.
Example 8.2.4
Verify that (R, +) is a commutative group.
Solution
Here we are concerned with the set of real numbers, R, and the operation of addition, +.
It is immediately clear that the operation of adding real numbers is closed: if a, b ∈ R then
a + b ∈ R. Further, addition of real numbers is associative: (a + b) + c = a + (b + c). The
identity element of this group is 0 ∈ R because 0+a = a+0 = a for any a ∈ R. The negative
of any real number is another real number and so the inverse of a ∈ R is −a, particularly
because a+ (−a) = 0. Thus (R, +) satisfies the axioms for a group. Because addition of real
numbers is commutative, it follows that (R, +) is a commutative group.
Example 8.2.5 The groups (B
n = {0, 1}
n, ⊕)
Consider the set B
n = {0, 1}
n with operation ⊕, which is bit-wise addition modulo 2. This
is a group. Consider the case n = 2, for which
B
2 = {(0, 0),(0, 1),(1, 0),(1, 1)}
which we abbreviate to
B
2 = {00, 01, 10, 11}.
Bit-wise addition modulo 2 is performed by adding corresponding binary digits and working
in modulo 2, thus, for example
1 1
1 0
0 1
.
The composition table for the group (B
2
, ⊕) is given in Table 8.3.
TABLE 8.3
Composition table for
(B
2
, ⊕)
⊕ 00 01 10 11
00 00 01 10 11
01 01 00 11 10
10 10 11 00 01
11 11 10 01 00
It is straightforward to verify that the group axioms are satisfied. The identity element
is 00 and every element is its own inverse. Further, observe that this group is Abelian.
You should convince yourself that the composition table for the group (B
3
, ⊕) is as given
in Table 8.4. Further, note that the group is Abelian and every element is its own inverse.Permutation groups and symmetric groups 181
TABLE 8.4
Composition table for (B
3
, ⊕)
⊕ 000 001 010 011 100 101 110 111
000 000 001 010 011 100 101 110 111
001 001 000 011 010 101 100 111 110
010 010 011 000 001 110 111 100 101
011 011 010 001 000 111 110 101 100
100 100 101 110 111 000 001 010 011
101 101 100 111 110 001 000 011 010
110 110 111 100 101 010 011 000 001
111 111 110 101 100 011 010 001 000
Example 8.2.6 The group of invertible Boolean functions on B
2
In Section 2.5.2, we discussed several examples of Boolean functions f : B
2 → B
2
, including
the Feynman cnot gate f(x, y) = (x, x⊕y). We noted that whilst there are 44
such functions
in total, only 4! = 24 of these are invertible. We also discussed how such functions can be
composed. It can be shown that the set of all invertible functions f : B
2 → B
2 determines
a group under functional composition.
Exercises
8.1 Explain why (R
+, +) is not a group.
8.2 Explain why (R\0, +) is not a group.
8.3 Explain why (R, ×) is not a group.
8.4 Show that ({00, 01}, ⊕) is a two element subgroup of (B
2
, ⊕). How many two
element subgroups of (B
2
, ⊕) are there ?
8.5 Show that ({000, 011}, ⊕) is a two element subgroup of (B
3
, ⊕). How many two
element subgroups of (B
3
, ⊕) are there ?
8.6 Consider the rectangle ABDC shown in Figure 8.1. Observe that L1 and L2 are
axes of symmetry and that the rectangle possesses rotational symmetry of 180◦
.
Through a suitable labelling of the symmetry operations draw up a composition
table for the symmetry group of the rectangle. This group is called the Klein￾four group.
8.7 Show that the group (B
2
, ⊕) is isomorphic to the Klein-four group.
8.3 Permutation groups and symmetric groups
We saw in Chapter 2 that a rearrangement or permutation of the elements of a set can be
thought of as a bijective function. Therefore, to a collection of permutations we can apply
the operation of function composition, ◦, which means applying one permutation followed
by another.182 Group theory
A B
C D
L1
L2
R
FIGURE 8.1
The symmetries of a rectangle.
Definition 8.4 Permutation group
A collection of permutations, with the operation of function composition ◦, which obey the
group axioms is called a permutation group.
In this section we shall use cycle notation (see Chapter 2) to represent a permutation.
Consider the following example.
Example 8.3.1
Consider the set A = {1, 2, 3} and the following set of three permutations, P = {σ0, σ1, σ2}
where
σ0 = the identity, σ1 = (123), σ2 = (132).
It may be helpful to picture the three permutations as shown in Figure 8.2. Note that there
are other permutations of the set {1, 2, 3} which are not discussed in this example.
1
2
3
A A
1
2
3
σ1
1
2
3
1
2
3
σ2
1
2
3
1
2
3
σ0
A A A A
FIGURE 8.2
Some permutations of the set A = {1, 2, 3}.
Now consider the function composition σ1 ◦ σ2:
(σ1 ◦ σ2)(1) = σ1(σ2(1)) = σ1(3) = 1
(σ1 ◦ σ2)(2) = σ1(σ2(2)) = σ1(1) = 2
(σ1 ◦ σ2)(3) = σ1(σ2(3)) = σ1(2) = 3.
Observe that (σ1 ◦ σ2)(x) = σ0(x), the identity element. Further compositions can be cal￾culated in a similar way. This gives rise to the composition table in Table 8.5 which revealsUnitary groups 183
TABLE 8.5
Composition table
for the
permutation group
◦ σ0 σ1 σ2
σ0 σ0 σ1 σ2
σ1 σ1 σ2 σ0
σ2 σ2 σ0 σ1
that the set is closed under composition. It is straightforward to verify associativity. The
identity element is σ0. Every element has an inverse (this is apparent because the iden￾tity element appears in every row and every column of the table, and thus, for example,
σ2 ◦ σ1 = σ0, so the inverse of σ1 is σ2). Thus the given set of permutations {σ0, σ1, σ2}
with the binary operation ◦ is a permutation group, in fact an Abelian permutation group.
The previous example illustrated a group whose elements were some permutations of
{1, 2, 3}. In addition to the permutations stated there, there are three further possible ones.
The six permutations together with the operation ◦ form the group of all permutations of
the three numbers which we call the symmetric group S3. Symmetric groups of higher
order are defined in an obvious way.
Definition 8.5 Symmetric group
The symmetric group Sn is the group of all permutations of n elements. The group
operation is function composition. Note |Sn| = n!.
For example, the group S5 is the group of all permutations of five elements, of which there
are 5! = 120.
Example 8.3.2
Consider again Example 8.3.1. Each of the permutations can be represented in an alternative
form using a so-called permutation matrix and matrix multiplication. Observe that


1 0 0
0 1 0
0 0 1




1
2
3

 =


1
2
3

 ,


0 1 0
0 0 1
1 0 0




1
2
3

 =


2
3
1

 ,


0 0 1
1 0 0
0 1 0




1
2
3

 =


3
1
2

 .
Labelling the three permutation matrices as M0 = I, M1 and M2, respectively, we can
produce the composition table in Table 8.6. It is straightforward to check that the group
axioms are satisfied.
8.4 Unitary groups
In quantum computation, unitary matrices play an important role in determining how the
state of a computation evolves in time. Recall that a square (n × n) matrix, U, is unitary if
U U† = U
†U = I, the n × n identity matrix,184 Group theory
TABLE 8.6
Composition table for the
group of permutation
matrices
M0 = I M1 M2
M0 M0 M1 M2
M1 M1 M2 M0
M2 M2 M0 M1
where U
†
is the conjugate transpose of U. It can be shown that the set of all n × n unitary
matrices with matrix multiplication form a group.
Definition 8.6 Unitary group
The set of all n×n unitary matrices form a group labelled U(n), called the unitary group
of degree n, where the group operation is matrix multiplication.
U(n) is a subgroup of the general linear group, GL(n, C), that is the set of all n × n
invertible matrices with complex elements. The reason why unitary matrices are particularly
important is because (a) when they act on a state vector they preserve its norm, and (b)
they are invertible. These are two essential properties of the evolution of a quantum state.
Theorem 8.1 The set of 2 × 2 unitary matrices U(2) form a group under matrix multipli￾cation.
Proof
We first prove closure. Suppose U1, U2 ∈ U(2). Then
U1U
†
1 = I, U2U
†
2 = I.
Then consider the product (U1U2)(U1U2)
†
:
(U1U2)(U1U2)
† = (U1U2)(U
†
2U
†
1
) (since (U1U2)
† = U
†
2U
†
1
)
= U1(U2 U
†
2
)U
†
1
(associativity)
= U1(I)U
†
1
= U1 U
†
1
= I.
Likewise (U1U2)
†
(U1U2) = I. Hence, U1U2 is unitary. We have shown that the product of
two unitary matrices is also a unitary matrix of the same size. The group identity element
is the 2 × 2 identity matrix, which is readily shown to be unitary. Associativity follows
immediately because matrix multiplication is associative. Finally, each U ∈ U(2) must have
an inverse element in the group, and this role is played by U
†
since U U† = U
†U = I. 
Example 8.4.1 The Pauli matrices
Consider the following set of 2 × 2 matrices known as Pauli matrices:
σ0 =

1 0
0 1 
, σ1 =

0 1
1 0 
, σ2 =

0 −i
i 0 
, σ3 =

1 0
0 −1

.Cosets, partitions and equivalence classes 185
Show that whilst these are all unitary matrices, they do not form a group (e.g. calculate
σ2σ1). (We note that by extending the set to form the 16 element set
P = {±σ0, ±iσ0, ±σ1, ±iσ1, ±σ2, ±iσ2, ±σ3, ±iσ3}
it is possible to show that (P, ◦) is a group – the Pauli group – where the binary operation
◦ represents matrix multiplication.)
Solution
Note that σ0 is the identity matrix. That each of these is a unitary matrix is easy to verify.
For example, consider σ2. First check carefully that σ
†
2 = σ2, then
σ2σ
†
2 =

0 −i
i 0   0 −i
i 0 
=

1 0
0 1 
and likewise for σ
†
2σ2. Hence, σ2 is a unitary matrix, as are the others. Now form, for
example,
σ2σ1 =

0 −i
i 0   0 1
1 0 
=

−i 0
0 i 
.
The resulting matrix is clearly not a member of the original set, and hence, the closure
axiom is not satisfied. The four given Pauli matrices do not form a group.
8.5 Cosets, partitions and equivalence classes
In this section we demonstrate how, given a group (G, ◦) and a subgroup, (H, ◦), we can
use the subgroup to divide up or partition G into distinct sets known as cosets. Elements
in these distinct sets share certain properties and as such can be regarded as equivalent in
some sense, as we shall explain. We begin by defining cosets.
Definition 8.7 Cosets
Suppose (G, ◦) is a group and (H, ◦) is a subgroup. Now choose any element a ∈ G that is
not in H. We can form the set consisting of all elements of the form a ◦ h, for h ∈ H. This
set is called a left coset of H in G, written a ◦ H or aH:
aH = {a ◦ h : h ∈ H}.
In a similar fashion, a right coset of H in G, written H ◦ a or Ha, is given by:
Ha = {h ◦ a : h ∈ H}.
By choosing a different element b ∈ G, say, we can form its coset in a similar way. We shall
find shortly that any element of G is in one and only one coset. Note that, apart from H
itself, the cosets are not subgroups. The only subgroup in this discussion is (H, ◦). Further,
the identity element of G, e say, must be in H because (H, ◦) is the only subgroup.
Finally, note that the set of all the left cosets of H in G is written G/H. The set of all
right cosets is denoted H\G.186 Group theory
Example 8.5.1
Consider the sets G = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11} and H = {0, 3, 6, 9}with the operation
of addition modulo 12. Observe H ⊂ G. It is easily verified by inspection of the composition
table in Table 8.7 that (G, + mod 12) is a group. Further, from Table 8.8, we see that the
TABLE 8.7
Composition table for (G, + mod 12)
+ 0 1 2 3 4 5 6 7 8 9 10 11
0 0 1 2 3 4 5 6 7 8 9 10 11
1 1 2 3 4 5 6 7 8 9 10 11 0
2 2 3 4 5 6 7 8 9 10 11 0 1
3 3 4 5 6 7 8 9 10 11 0 1 2
4 4 5 6 7 8 9 10 11 0 1 2 3
5 5 6 7 8 9 10 11 0 1 2 3 4
6 6 7 8 9 10 11 0 1 2 3 4 5
7 7 8 9 10 11 0 1 2 3 4 5 6
8 8 9 10 11 0 1 2 3 4 5 6 7
9 9 10 11 0 1 2 3 4 5 6 7 8
10 10 11 0 1 2 3 4 5 6 7 8 9
11 11 0 1 2 3 4 5 6 7 8 9 10
set H = {0, 3, 6, 9} with the same operation is a subgroup of (G, + mod 12), i.e H ≤ G. We
TABLE 8.8
Composition table
for (H, + mod12)
+ 0 3 6 9
0 0 3 6 9
3 3 6 9 0
6 6 9 0 3
9 9 0 3 6
now form the left cosets of H in G. We choose any element of G that is not in H (note that
were we to choose an element that is in H, its composition with any other element in H
will remain in H because H is a subgroup and is therefore closed). Choosing 1 ∈ G, 1 ∈/ H,
H = {0, 3, 6, 9}
1 + H = {1 + h : h ∈ H}
= {1, 4, 7, 10}.
This is the left coset of H in G generated by the element 1. Moving on to another element
in G that is not in H, say 2:
2 + H = {2 + h : h ∈ H}
= {2, 5, 8, 11}.
This is the left coset of H in G generated by the element 2. Every element of G has now been
considered: observe that it is either in H or one of the two additional cosets. (The subgroup
H can be regarded as the coset 0 + H). Thus a consequence of generating the cosets of H
in G is to partition the set G into distinct (not overlapping) subsets. Every element of G
is in one and only one coset. Within each coset, the elements share particular properties.
For example, all elements in H are divisible by 3. All elements in 1 + H have remainder 1
when divided by 3. All elements in 2 + H have remainder 2 when divided by 3. Thus, inCosets, partitions and equivalence classes 187
this sense, the elements within each coset are equivalent. We say they are equivalent with
respect to the left coset.
Example 8.5.2 Cosets as equivalence classes
Referring to the previous example, we can label the cosets [0], [1], [2] where we now regard
0,1 and 2 as representing all the elements in their coset. The coset [1] is thus a class of
equivalent elements – an equivalence class. Likewise [0] and [2]. That is,
[0] = {h : h ∈ H} = H
[1] = {1 + h : h ∈ H}, [2] = {2 + h : h ∈ H}.
Explicitly:
[0] = {0, 3, 6, 9}, [1] = {1, 4, 7, 10}, [2] = {2, 5, 8, 11}.
If we define the relation ∼ such that
b ∼ a if and only if b = a + h for some h ∈ H
then ∼ is an equivalence relation. For example, we see that 11 ∼ 2 because 11 = 2 + h
for some h ∈ H, specifically, 11 = 2 + 9 and 9 ∈ H, and observe that both 2 and 11 are
elements of the equivalence class [2].
More generally, for the group (G, ◦), suppose H ≤ G and that we have partitioned G into
left (or right) cosets. Then we write
b ∼ a if there exists h ∈ H such that b = a ◦ h.
This means that a and b lie in the same left coset, that is, in the same equivalence class.
Theorem 8.2 Suppose H ≤ G and that we have partitioned G into left (or right) cosets.
For a, b ∈ G, if a and b are in the same equivalence class then
a
−1
◦ b = h for some h ∈ H.
Proof
Let [a] be the equivalence class containing a, i.e.,
[a] = {a ◦ h : h ∈ H}.
If b ∈ [a], that is, b ∼ a, then b = a ◦ h for some h ∈ H. By pre-applying the inverse of a to
b = a ◦ h:
a
−1
◦ b = a
−1
◦ a ◦ h = e ◦ h = h where e is the group identity element
we deduce that
b ∼ a if a
−1
◦ b = h for some h ∈ H.

Suppose the left and right cosets of a subgroup are found to be identical. This leads to the
following definition.
Definition 8.8 Normal subgroup, H / G
Given H ≤ G, H is said to be a normal subgroup, written H / G, if for every element in
G, the left and right cosets are identical.188 Group theory
It can be shown that any subgroups of a commutative group are normal.
Example 8.5.3 Cosets arising from subgroups of (B
3
, ⊕)
Consider the group (B
3
, ⊕) of Example 8.2.5 which we noted was Abelian. For s ∈ B
3
let
Ks be the two element set {000, s}. It is straightforward to verify that (Ks, ⊕) is a normal
subgroup.
Suppose we choose s = 101 and consider the subgroup (K101, ⊕) where K101 =
{000, 101}. The (left) cosets are then
[x] = x ⊕ K101 = {x ⊕ k : k ∈ K101}.
It follows that
[000] = {000, 101}
[001] = {001, 100}
[010] = {010, 111}
[011] = {011, 110}.
We see that a consequence of generating the cosets of K101 in B
3 has been the partition of
B
3
into distinct equivalence classes. You should verify that we could have carried out the
same process for any other s ∈ B
3
.
Example 8.5.4
Consider the group (B
3
, ⊕) of Example 8.2.5 which we noted was Abelian.
a) Show that S = {000, 010, 100, 110} is a four element subgroup.
b) Find the (left) cosets of S and hence partition B
3
.
Solution
a) The composition table for (S, ⊕) is given in Table 8.9. This shows that the set S is closed
under ⊕. Associativity is inherited from the main group. The identity element is 000. Each
element has an inverse because the identity appears in each row and column. Hence (S, ⊕)
is a subgroup.
TABLE 8.9
⊕ 000 010 100 110
000 000 010 100 110
010 010 000 110 100
100 100 110 000 010
110 110 100 010 000
b) For x ∈ B
3
the cosets are given by:
[x] = x ⊕ S = {x ⊕ s : s ∈ S}.
Then
[000] = {000, 010, 100, 110} = S
[001] = {001, 011, 101, 111}.
This is the required partition of B
3
.Quotient groups 189
Exercises
8.8 Show that for (H, ◦) to be a normal subgroup of (G, ◦), for all a ∈ G and all
h ∈ H, a
−1 ◦ h ◦ a ∈ H.
8.9 For s ∈ B
3
let Ks be the two element set {000, s}. Show that the cosets of
K001 in B
3 are the equivalence classes [000] = {000, 001}, [010] = {010, 011},
[100] = {100, 101}, [110] = {110, 111}.
8.6 Quotient groups
Suppose (H, ◦) is a normal subgroup of (G, ◦). We let G/H be the set of all left cosets of
H in G. Because H is a normal subgroup, the set of left cosets is identical to the set of
right cosets and we can ignore the distinction. The set G/H and a suitably defined binary
operation form a new group called the quotient group, or factor group of G relative to H.
Definition 8.9 Quotient group or factor group
Given H / G, the set of all (left) cosets of H in G, written G/H, together with the binary
operation ◦ as defined below and inherited from the group (G, ◦) is a group (G/H, ◦) called
the quotient group or factor group.
Theorem 8.3 Given H / G and cosets [a], [b], [c] ∈ G/H. Define the binary operation ∗ on
the cosets as
[a] ∗ [b] = [a ◦ b],
that is, two cosets are composed by applying the group operation to the representatives of
the cosets and determining the coset in which the result lies. Then (G/H, ∗) is a group.
Proof
First, consider
([a] ∗ [b]) ∗ [c] = [a ◦ b] ∗ [c]
= [(a ◦ b) ◦ c]
= [a ◦ (b ◦ c)] because of associativity of (G, ◦)
= [a] ∗ [b ◦ c]
= [a] ∗ ([b] ∗ [c])
and thus the operation ∗ is associative. The identity element in the new group is the coset
[e], where e is the identity element of (G, ◦), that is the subgroup H itself, because
[e] ∗ [a] = [e ◦ a]
= [a] because e is the identity in G.
Likewise [a] ∗ [e] = [a] and so [e] is the identity element in (G/H, ∗). Finally, the inverse of
[a] ∈ G/H is the coset [a
−1
] ∈ G/H because
[a] ∗ [a
−1
] = [a ◦ a
−1
]
= [e].
190 Group theory
Because we end up using the original group operation ◦ in performing calculations with
cosets, it is usual practice to write both operations, ◦ and ∗, with the same symbol ◦ say.
Then (G/H, ◦) is known as a quotient group or factor group.
Example 8.6.1
Consider the commutative group (Z, +) and the subgroup (5Z, +) where
5Z = {0, ±5, ±10, ±15, . . .}
that is, all integer multiples of 5. That (5Z, +) is a subgroup is readily verified.
Suppose we choose the element 1 ∈ Z, noting 1 ∈/ 5Z. Forming the set consisting of all
elements of the form 1 + h for h ∈ 5Z we obtain
{. . . , −9, −4, 1, 6, 11, 16, . . .} which we denote 1 + 5Z or [1].
Likewise, choosing the element 2 ∈ Z, noting 2 ∈/ 5Z we obtain
{. . . , −8, −3, 2, 7, 12, 17, . . .} which we denote 2 + 5Z or [2].
Continuing in this fashion, we define
{. . . , −7, −2, 3, 8, 13, 18, . . .} which we denote 3 + 5Z or [3].
{. . . , −6, −1, 4, 9, 14, 19, . . .} which we denote 4 + 5Z or [4].
At this stage, every element of Z is in one and only one of the cosets (with [0] = 5Z). Thus
we have partitioned Z into distinct sets. Every element of any particular coset is considered
to be equivalent to any other in its coset and can therefore be regarded as representative
of that coset. We now have a set, Z/5Z, with 5 elements {[0], [1], [2], [3], [4]}. The set Z/5Z
with addition is the quotient group, (Z/5Z, +).
8.7 End-of-chapter exercises
1. Show that the set of complex numbers C with the usual addition of complex
numbers, +, is a group – the additive group of complex numbers (C, +).
2. Let Q\0 be the set of rational numbers without 0. Show that with the usual
multiplication, ·, of rational numbers (Q\0, ·) is a group.
3. Show that (C\0, ·) is a group, where · is the usual multiplication of complex
numbers.
4. Show that the group (B
2
, ⊕) is isomorphic to (G, ×(mod 8)) where G = {1, 3, 5, 7}
and multiplication is performed modulo 8.
5. The matrix W(θ) = 
cos θ − sin θ
sin θ cos θ

effects the rotation of a point in the xy
plane anticlockwise through an angle θ.
Show that the set of all such matrices forms a group (the special orthogonal
group SO(2)) under matrix multiplication.
a)
b) State the identity element of the group.
c) How many elements are there in this group?End-of-chapter exercises 191
6. Suppose (G, ◦) is a group and suppose H is a subset of G. Show that if H 6= ∅
and if whenever a, b ∈ H then a ◦ b
−1 ∈ H then H is a subgroup.
7. We have seen that U(2) is the unitary group – the group of 2×2 unitary matrices.
The group SU(2), the special unitary group is the subgroup of U(2) comprising
those unitary matrices with determinant equal to 1. Show that 
√
1
2
√
1
2
− √
1
2
√
1
2
!
is an element of SU(2).
8. We have noted that the four Pauli matrices {σ0, σ1, σ2, σ3} do not form a group.
Show that the 16 element set
P = {±σ0, ±iσ0, ±σ1, ±iσ1, ±σ2, ±iσ2, ±σ3, ±iσ3}
is a group – the Pauli group – with binary operation the usual matrix multipli￾cation.
9. Consider the additive group of integers modulo 5, (Z/5Z, +).
a) Produce a Cayley table for this group.
Show that by repeatedly adding the element 1, every other element in the
group can be generated.
b)
Likewise, show that by repeatedly adding the elements 2, 3 and 4 every
other element in the group can be generated.
If every element of a group can be generated in this way by a single element
of the group, then that group is said to be cyclic.
c)
Consider the set {1, −1, i, −i} with the usual multiplication of complex num￾bers. Show that this is a cyclic group and determine which elements can be
used to generate the group.
d)
10. Suppose (G, ◦) and (H, ∗) are groups and f : G → H is an arbitrary function.
Suppose further that we define the function ˆf : G × H → G × H as
ˆf(g, h) = (g, h ∗ f(g)), for g ∈ G, h ∈ H.
Suppose g1, g2 ∈ G, h1, h2 ∈ H. Use the group properties to show that if
ˆf(g1, h1) = ˆf(g2, h2)
then g1 = g2 and h1 = h2, and deduce that ˆf is a one-to-one function.
11. Consider the groups and functions defined in Q10. Show that for any (a, b) in the
co-domain of ˆf, there exists g ∈ G, h ∈ H such that ˆf(g, h) = (a, b). Deduce that
ˆf is an onto function.
12. Prove that if a Cayley table is symmetric about its leading diagonal then the
corresponding group is Abelian.9
Linear transformations
9.1 Objectives
Given two arbitrary vector spaces V and W over the same field F, a transformation is
a function which maps each vector in V to a vector in W. Linear transformations are
functions which have additional properties which mean that the vector space operations
of vector addition and scalar multiplication are preserved. The objective of this chapter
is to introduce general linear transformations and shows how they can be represented by
matrices. It introduces the terms ‘kernel’ and ‘image’ and a result known as the ‘dimension
theorem’.
These preliminaries pave the way for the study of a particular type of linear transfor￾mation known as a linear operator (see Chapter 11). Linear operators are essential to the
study of quantum computation because they represent physical observables and determine
the time evolution of a quantum system.
9.2 Preliminary information
Suppose we have two vector spaces, V and W say, over the same field, F. In quantum
computation the field F will usually be R or C. A function, or mapping, f, assigns to
each v ∈ V a unique element of W. We write f : V → W. We now consider particular
functions known as linear transformations.
Definition 9.1 Linear transformation or homomorphism
A mapping, f, is a linear transformation or homomorphism if it satisfies
f(u + v) = f(u) + f(v)
f(ku) = kf(u)
for any u, v ∈ V and k ∈ F.
This requirement means that f preserves the vector space structure: addition of vectors
and multiplication of vectors by a scalar (Note that in f(u + v) = f(u) + f(v) the addition
on the left-hand side takes place in V , whereas that on the right takes place in W. These
additions need not be the same, although it is conventional to use the same symbol +).
Definition 9.2 Isomorphism
If a linear transformation f is one-to-one and onto, it is said to be an isomorphism.
DOI: 10.1201/9781003264569-9 193194 Linear transformations
In this chapter we explore linear transformations in general. In Chapter 11 we consider the
specific and important case when the linear transformation maps the vector space V to
itself, i.e., f : V → V . Such a transformation is called a linear operator.
Example 9.2.1
Consider the vector spaces over R of row vectors in R
3 and R
2
. Let f : R
3 → R
2 be defined
by
f(u1, u2, u3) = (u1 + u3, 4u2 − u3).
a) Find f(0, 0, 0).
b) Find f(4, 5, −2).
c) Show that f is a linear transformation.
Solution
a) f(0, 0, 0) = (0, 0).
b) f(4, 5, −2) = (4 + (−2), 4(5) − (−2)) = (2, 22).
Observe that in each case a vector in R
3
is mapped to one in R
2
. Further, observe that the
zero vector in R
3
is mapped to the zero vector in R
2
. This is generally true, as we prove
below.
(c) To show that f is a linear transformation, choose two arbitrary vectors in R
3
, u =
(u1, u2, u3), v = (v1, v2, v3). Their sum is u + v = (u1 + v1, u2 + v2, u3 + v3). The images
under f are given by
f(u) = f(u1, u2, u3) = (u1 + u3, 4u2 − u3).
f(v) = f(v1, v2, v3) = (v1 + v3, 4v2 − v3).
f(u + v) = f(u1 + v1, u2 + v2, u3 + v3)
= ((u1 + v1) + (u3 + v3), 4(u2 + v2) − (u3 + v3))
= ((u1 + u3) + (v1 + v3),(4u2 − u3) + (4v2 − v3))
= (u1 + u3, 4u2 − u3) + (v1 + v3, 4v2 − v3)
= f(u) + f(v).
This proves the first of the requirements for f to be a linear transformation. Now consider
scalar multiplication:
f(ku) = f(k(u1, u2, u3)) = f(ku1, ku2, ku3)
= (ku1 + ku3, 4ku2 − ku3)
= k(u1 + u3, 4u2 − u3)
= kf(u).
which proves the second requirement. Hence, f is a linear transformation.
Theorem 9.1 If f : V → W is a linear transformation then f(0) = 0. That is f maps the
zero vector in V to the zero vector in W.
Proof
If f is a linear transformation, we know that f(ku) = kf(u). The underlying field F must
have a zero element, so choose k = 0. Then
f(0u) = 0f(u), and hence f(0) = 0
as required. Preliminary information 195
Example 9.2.2
Consider now column vectors in the vector spaces V = W = R
2 over the field R. Let
f : R
2 → R
2 be given by
f

u1
u2

=

3u1 − 2u2
u1 + 4u2

.
(a) Evaluate f

4
0

, f

−2
3

, f

0
0

.
(b) Show that f is a linear mapping.
Solution
(a) f

4
0

=

12
4

, f

−2
3

=

−12
10 
, f

0
0

=

0
0

.
(b) We must show that f(u + v) = f(u) + f(v), f(ku) = kf(u). Let
u =

u1
u2

, v =

v1
v2

,
then
u + v =

u1
u2

+

v1
v2

=

u1 + v1
u2 + v2

.
Note that this is vector addition in the domain. Mapping this sum we find
f(u + v) = 
3(u1 + v1) − 2(u2 + v2)
u1 + v1 + 4(u2 + v2)

=

3u1 + 3v1 − 2u2 − 2v2
u1 + v1 + 4u2 + 4v2

=

(3u1 − 2u2) + (3v1 − 2v2)
(u1 + 4u2) + (v1 + 4v2)

=

3u1 − 2u2
u1 + 4u2

+

3v1 − 2v2
v1 + 4v2

= f(u) + f(v) as required.
Note that now this is vector addition in the co-domain. You should use a similar approach
to check the second requirement.
Example 9.2.3 A matrix as a linear transformation.
Consider the case when V = R
n and W = R
m, over the field R, and the elements of these
vector spaces are column vectors. An m × n matrix A is a linear transformation such that
A : R
n → R
m.
Take, for example, A : R
2 → R
3
, that is m = 3, n = 2, and A =


7 1
2 −3
4 2

, then
A

x
y

=


7 1
2 −3
4 2



x
y

=


7x + y
2x − 3y
4x + 2y

 .196 Linear transformations
Observe that the matrix A maps a column vector in R
2
to a column vector in R
3
. The
transformation is linear because, in general, for matrix multiplication,
A(u + v) = Au + Av, A(ku) = kA(u)
for u, v ∈ R
n, k ∈ R. (See Theorem 5.1.)
9.3 The kernel and image of a linear transformation
Definition 9.3 Kernel of a linear transformation
Given a linear transformation f : V → W the kernel of f, written ker(f), is the set of
vectors in V which map to the zero vector in W, that is
ker(f) = {v ∈ V : f(v) = 0}.
Definition 9.4 Image of a linear transformation
The image of f, written Im(f), is the set of vectors in W which are mapped to by some
element in V , that is
Im(f) = {w ∈ W : w = f(v), for some v ∈ V }.
It is possible to show that both the kernel and the image of a linear transformation are
subspaces. If ker(f) contains only the zero vector then by definition it has dimension zero.
Example 9.3.1
Consider the linear transformation f : R
3 → R
2
, f(x, y, z) = (x + y + z, 2x − y + 3z). Find
a basis for (a) its kernel and (b) its image.
Solution
(a) The kernel consists of all element of R
3 which map to zero. If (a, b, c) ∈ ker(f) then
f(a, b, c) = (a + b + c, 2a − b + 3c) = (0, 0).
Thus
a + b + c = 0
2a − b + 3c = 0.
Solving these equations by reduction to row echelon form gives (a, b, c) = µ(−4, 1, 3) where
µ ∈ R is a free variable. Thus any multiple of (−4, 1, 3) is in the kernel of f. A basis for the
kernel is (−4, 1, 3) and thus the dimension of the kernel, also referred to as the nullity of
f, is 1.
(b) Every vector in V can be written as a linear combination of its basis vectors. To deter￾mine the image we consider how each basis vector in V is mapped to W. We shall use the
standard basis of R
3
. Then
f(1, 0, 0) = (1, 2), f(0, 1, 0) = (1, −1), f(0, 0, 1) = (1, 3).Linear functionals 197
Any vector in the image must be a combination of these vectors. Clearly this set is linearly
dependent. It is easily verified that the first two are linearly independent and can be used
as a basis for Im(f). The image is therefore the whole of R
2
, that is, it has dimension 2.
The dimension of the image of f is also known as the rank of f.
Note from the previous example in which f : R
3 → R
2
,
dim R
3 = dim ker(f) + dim Im(f).
This result is true more generally and is known as the dimension theorem which we state
without proof:
Theorem 9.2 The dimension theorem
For any linear transformation, f : V → W,
dim V = dim ker(f) + dim Im(f).
9.4 Linear functionals
We begin by noting that every field is a vector space over itself. Here we consider linear
transformations from vector spaces to their underlying fields.
Definition 9.5 Linear functional
A linear transformation from the vector space V to the underlying field F, i.e., f : V → F
is called a linear functional or linear form.
Example 9.4.1
Consider the vector space over C of column vectors in C
3
, that is vectors of the form


c1
c2
c3


where c1, c2 and c3 are complex numbers. The function f : C
3 → C defined by
f


c1
c2
c3

 = 3c1 + 7c2 − 2c3
is a linear functional. For example, if u =


1 + i
2 − 3i
8

 then
f(u) = 3(1 + i) + 7(2 − 3i) − 2(8) = 1 − 18i.
It is clear that the functional maps vectors in C
3
to the underlying field C.
The linear functional can be represented as a row vector ￾
3 7 −2

. Then, via matrix
multiplication we have
￾
3 7 −2



1 + i
2 − 3i
8

 = 1 − 18198 Linear transformations
Exercises
9.1 Show that f : R
3 → R, f(x, y, z) = 4x + 2y − 7z is a linear functional.
9.2 Determine whether or not f : R
3 → R, f(x, y, z) = 4x + 2y − 7z + 3 is a linear
functional.
9.3 Determine whether or not f : R
3 → R, f(x, y, z) = xyz is a linear functional.
9.5 Matrix representations of linear transformations
If V and W are finite dimensional vector spaces, over the same field F, with dimensions n
and m respectively then any linear transformation f : V → W can be represented by an
m × n matrix, A. The image of v ∈ V under f is then found by premultiplying v by A. In
this section we show how this is achieved.
Let V have basis {v1, v2, . . . , vn} and let W have basis {w1, w2, . . . , wm}. Then for each
basis vector vi
in V we calculate its image under f:
f(vi) = a1iw1 + a2iw2 + . . . + amiwm, i = 1, 2, . . . , n.
Here, the aji ∈ F, j = 1, 2, . . . , m, i = 1, 2, . . . , n, are the components of f(vi) with respect
to the chosen basis of W. An arbitrary vector in v ∈ V is
v = c1v1 + c2v2 + . . . + cnvn, ci ∈ F, i = 1, 2, . . . , n
and so its coordinate vector in this basis is
[v] =


c1
c2
.
.
.
cn


.
Then, because f is linear,
f(v) = f(c1v1 + c2v2 + . . . + cnvn)
= c1f(v1) + c2f(v2) + . . . + cnf(vn)
= c1(a11w1 + a21w2 + . . . + am1wm)
+ c2(a12w1 + a22w2 + . . . + am2wm)
.
.
.
+ cn(a1nw1 + a2nw2 + . . . + amnwm).
Rearranging the right-hand side:
f(v) = (c1a11 + c2a12 + . . . + cna1n)w1
+ (c1a21 + c2a22 + . . . + cna2n)w2
.
.
.
+ (c1am1 + c2am2 + . . . + cnamn)wmMatrix representations of linear transformations 199
and so the coordinate vector of the image of v under f is
[f(v)] =


c1a11 + c2a12 + . . . + cna1n
c1a21 + c2a22 + . . . + cna2n
.
.
.
c1am1 + c2am2 + . . . + cnamn


=


a11 a12 . . . a1n
a21 a22 . . . a2n
.
.
.
.
.
.
.
.
.
.
.
.
am1 am2 . . . amn




c1
c2
.
.
.
cn


.
We see that the image of v ∈ V under f is then found by premultiplying the coordinate
vector of v by A where
A =


a11 a12 . . . a1n
a21 a22 . . . a2n
.
.
.
.
.
.
.
.
.
.
.
.
am1 am2 . . . amn


.
Thus the columns of the matrix A representing the linear transformation f are precisely the
coordinates of the images of the domain’s basis vectors. Consider the following example.
Example 9.5.1
Consider the following linear transformation f : R
2 → R
3
:
f(x, y) = (3x + 4y, 2x − y, 5x + y).
Assume we are working with the standard bases in both vector spaces. We calculate the
image of the domain’s basis vectors:
f(1, 0) = (3, 2, 5) = 3(1, 0, 0) + 2(0, 1, 0) + 5(0, 0, 1).
f(0, 1) = (4, −1, 1) = 4(1, 0, 0) − 1(0, 1, 0) + 1(0, 0, 1).
Now form a matrix, A, whose columns are the coefficients in the rows of the above expres￾sions:
A =


3 4
2 −1
5 1

 .
This is the required matrix representation of the linear transformation f with respect to
the stated bases. If we now choose any vector in R
2
,

a
b

say, premultiplication by A will
give the image. That is


3 4
2 −1
5 1



a
b

=


3a + 4b
2a − b
5a + b

 .
In summary, to find the matrix representation of a linear transformation f : V → W, we
have the following:
Theorem 9.3
1. Let V have basis {v1, v2, . . . , vn} and let W have basis {w1, w2, . . . , wm}.
2. Calculate f(vi), i = 1, 2, . . . , n, expressing each in terms of the co-domain
basis vectors.200 Linear transformations
3. Form an m × n matrix, A, whose columns are the coefficients of each co￾domain basis vector obtained in 2. This is the required matrix representation.
Importantly, note that the representation depends upon the basis chosen. Different bases
will give different matrix representations.
Example 9.5.2
Consider V = R
3 with basis e1 =


1
1
0

, e2 =


0
1
1

, e3 =


0
0
1

. Consider W = R
2
with basis f1 =

1
1

, f2 =

1
−1

. Find the matrix representation of f : R
3 → R
2
,
f


x
y
z

 =

x + y + z
x − y − z

with respect to the given bases.
Solution
First find the image of the basis vectors:
f(e1) = f


1
1
0

 =

2
0

, f(e2) = f


0
1
1

 =

2
−2

,
f(e3) = f


0
0
1

 =

1
−1

,
and in terms of the given co-domain basis:
f(e1) = 1 
1
1

+ 1 
1
−1

, f(e2) = 0 
1
1

+ 2 
1
−1

,
f(e3) = 0 
1
1

+ 1 
1
−1

.
Finally, forming the matrix whose columns are the coefficients in the rows above:
A =

1 0 0
1 2 1 
.
You should check for yourself that this matrix does indeed represent the given transforma￾tion. For example, use it to find f


3
4
2

 (remember to work with the correct bases).
9.6 Bilinear maps
Definition 9.6 Bilinear map
Consider vector spaces U, V, W over the same field F. Let u, u1, u2 ∈ U, v, v1, v2 ∈ V . Then
a bilinear map, f, takes vectors from the Cartesian product U × V to W and satisfies
f : U × V → WBilinear maps 201
f(u1 + u2, v) = f(u1, v) + f(u2, v), f(u, v1 + v2) = f(u, v1) + f(u, v2),
kf(u, v) = f(ku, v) = f(u, kv).
Example 9.6.1
Let u =

u
(1)
u
(2) 
, v =

v
(1)
v
(2) 
be elements of R
2
. Consider the map f : R
2 × R
2 → R
2
defined by f(u, v) = f
 u
(1)
u
(2) 
,

v
(1)
v
(2)  =

u
(2)v
(2)
u
(1)v
(1) 
.
If u =

3
4

, v =

2
−1

find f(u, v).a)
b) Show that f is a bilinear map.
Solution
a) f(u, v) = f(

3
4

,

2
−1

) = 
(4)(−1)
(3)(2) 
=

−4
6

.
b) Let u1 =
 
u
(1)
1
u
(2)
1
!
, u2 =
 
u
(1)
2
u
(2)
2
!
. Let v =

v
(1)
v
(2) 
.
Then
u1 + u2 =
 
u
(1)
1 + u
(1)
2
u
(2)
1 + u
(2)
2
!
.
Applying the bilinear map:
f(u1 + u2, v) = f(
 
u
(1)
1 + u
(1)
2
u
(2)
1 + u
(2)
2
!
,

v
(1)
v
(2) 
) = 
(u
(2)
1 + u
(2)
2
)v
(2)
(u
(1)
1 + u
(1)
2
)v
(1) !
=
 
u
(2)
1
v
(2)
u
(1)
1
v
(1) !
+
 
u
(2)
2
v
(2)
u
(1)
2
v
(1) !
= f(u1, v) + f(u2, v).
In a similar way it is straightforward to verify the linearity in the second argument.
Further,
kf(u, v) = kf(

u
(1)
u
(2) 
,

v
(1)
v
(2) 
) = k

u
(2)v
(2)
u
(1)v
(1) 
=

ku(2)v
(2)
ku(1)v
(1) 
= f(ku, v) or alternatively f(u, kv)
as required. Hence, f is a bilinear map.
Exercises
9.4 Let u = (x1, x2), v = (y1, y2) be elements of R
2
. Consider the inner product on
R
2 defined by f : R
2 × R
2 → R, f(u, v) = hu, vi =
P2
i=1 xiyi
. Show that f is a
bilinear map.202 Linear transformations
9.7 End-of-chapter exercises
1. Consider the linear transformation f : R
3 → R
3
,
f(x, y, z) = (x, y, 0).
This is a projection mapping of a point in three-dimensional space onto the xy
plane.
a) Verify that f is indeed a linear transformation.
Find its matrix representation, P, in the standard basis (1, 0, 0), (0, 1, 0),
(0, 0, 1).
b)
A projector, P, is a linear transformation with the property that P
2 = P.
Verify that P is indeed a projector.
c)
2. Consider the linear transformation f : C
2 → C
2
,
f(z1, z2) = (1
2
z1 +
1
2
z2,
1
2
z1 +
1
2
z2).
a) Verify that f is indeed a linear transformation.
b) Find its matrix representation, P, in the standard basis (1, 0), (0, 1).
Show that P c) 2 = P and hence verify that P is indeed a projector.
3. The transformation W(θ) : R
2 → R
2
, which rotates a point in the xy plane
anticlockwise through an angle θ is given by the matrix
W(θ) = 
cos θ − sin θ
sin θ cos θ

.
Show that W is a linear transformation.
4. Let u = (x1, x2, x3), v = (y1, y2, y3) be elements of R
3
. Consider the inner product
on R
3 defined by f : R
3 × R
3 → R, f(u, v) = hu, vi =
P3
i=1 xiyi
. Show that f is
a bilinear map.
5. Suppose f : U → V , g : U → V are linear transformations over a field F. The
sum f + g is defined to be the mapping (f + g) : U → V such that
(f + g)(u) = f(u) + g(u).
Scalar multiplication kf, for k ∈ F, is defined such that
(kf)(u) = k f(u).
Show that f + g and kf are also linear transformations.
6. Suppose f : U → V is a linear transformation. Show that ker(f) and Im(f) are
subspaces of U and V , respectively.
7. Let f : R
3 → R
3
, f(x, y, z) = (2x + 3y + z, x + z, y − z).
a) Find a basis for the kernel of f.
b) Find a basis for the image of f.
c) Verify the dimension theorem.End-of-chapter exercises 203
8. Consider the transformation f : C → C, f(z) = z
∗
, the complex conjugate of z.
a) Show that if C is a vector space over R, then f is a linear transformation.
b) Show that if C is a vector space over C, then f is not a linear transformation.
This example highlights the importance of specifying the underlying field clearly.
9. Find the kernel of A − λI where A =

3 8
0 −1

and λ = 3.
10. Consider the linear operator A : R
n → R
n with matrix representation A. Show
that the eigenspace corresponding to an eigenvalue λ is the kernel of A − λI.10
Tensor product spaces
10.1 Objectives
We have already described how a qubit is represented by a vector in the two-dimensional
vector space C
2
, that is
|ψi = α0|0i + α1|1i where |α0|
2 + |α1|
2 = 1
and {|0i, |1i} is an orthonormal basis for C
2
. The study of quantum systems involving two
or more qubits necessitates the development of mathematical tools which capture the inter￾actions between the qubits. This leads to the construction of objects called tensors which
are elements of a vector space called a tensor product space. This material is essential since
tensors are required for defining the means of data representation in quantum computation.
The objective of this chapter is to define the tensor product ⊗ of two vectors and show
how this product can be calculated. This leads to the concept of an entangled state. We
show how tensor products reside in a tensor product space and define an inner product and
norm on that space.
10.2 Preliminary discussion
The tensor product is the result of combining two (or more) vectors in a particular way
designed to capture the interaction between the various parts of the individual vectors. The
results of finding tensor products reside in a vector space called the tensor product space,
the elements of which are called tensors.
The formal construction of a tensor product space is somewhat abstract and can be
difficult for the student to understand. So, initially, we develop the tools necessary for
performing calculations with tensors. Later in the chapter we give a more formal treatment
once some familiarity has been established.
Bilinear maps were defined in Chapter 9. Recall that if U, V , W are vector spaces over
the same field F, a bilinear map, f, takes vectors from the Cartesian product U × V to W,
f : U × V → W, and satisfies
f(u1 + u2, v) = f(u1, v) + f(u2, v), f(u, v1 + v2) = f(u, v1) + f(u, v2),
kf(u, v) = f(ku, v) = f(u, kv).
Observe that these properties mean that f behaves linearly in each of the two slots separately
(to see this, fix v and then fix u). We have already seen one way in which vectors can be
combined: the inner, scalar or dot product, · or h , i, in R
n . The inner product is a function
which takes two vectors, u, v ∈ R
n say, and produces a scalar c ∈ R. It is straightforward to
DOI: 10.1201/9781003264569-10 205206 Tensor product spaces
show that the (real) inner product possesses the properties required of a bilinear map from
R
n × R
n to R. (We note in passing that the inner product on vectors in C
n is not a bilinear
map). The tensor product, which we give the symbol ⊗, is constructed in such a way that
it satisfies the properties required of a bilinear map.
Definition 10.1 The tensor product as a bilinear map
Consider vector spaces U and V over the same field, F (usually C). For u ∈ U and v ∈ V
their tensor product, written u⊗v, is a bilinear map which satisfies the distributive properties
given in Table 10.1. In Table 10.1 u, u1, u2 ∈ U, v, v1, v2 ∈ V , λ ∈ F.
We use the same symbol for the tensor product space U ⊗ V in which the tensor products
u ⊗ v reside. We shall draw upon these properties extensively in our calculations. The
following section introduces the way in which tensor product calculations are performed.
10.3 Calculation of tensor products
Given vector spaces U and V of dimensions n and m, respectively, we may select vectors
u ∈ U, v ∈ V to form a tensor product u ⊗ v. This new vector is an element of the tensor
product space U ⊗ V , a vector space of dimension nm, and is referred to as a separable
tensor.
Example 10.3.1
Given that the vectors 
1
0

and 
0
1

are elements of C
2
, then

1
0

⊗

0
1

is a tensor in the tensor product space C
2 ⊗ C
2

. Recall that using ket notation, we write
1
0

= |0i and 
0
1

= |1i, so that the tensor can be written |0i ⊗ |1i. It is common
practice to omit the symbol ⊗ and write this as |0i|1i, or even more briefly as |01i. Similarly,
|1i ⊗ |0i may be written |1i|0i or |10i, and so on.
The tensor product space U ⊗ V is a vector space. Within it, tensors can be added and
multiplied by scalars to yield new tensors. Thus elements of U ⊗ V are linear combinations
of separable tensors.
Example 10.3.2
Consider the following tensors in C
2 ⊗ C
2
:
|00i, |01i, |10i, |11i.
Because they are elements of a vector space, we can perform addition and multiplication
by a scalar from the field C. Thus the following is an element of C
2 ⊗ C
2
:
α00|00i + α01|01i + α10|10i + α11|11i
where α00, α01, α10, α11 ∈ C.Calculation of tensor products 207
The properties stated in Table 10.1 can be used to rewrite expressions involving tensors.
TABLE 10.1
Required properties of ⊗
(i) (u1 + u2) ⊗ v = u1 ⊗ v + u2 ⊗ v
(ii) u ⊗ (v1 + v2) = u ⊗ v1 + u ⊗ v2
(iii) λ(u ⊗ v) = (λu) ⊗ v = u ⊗ (λv)
Example 10.3.3
Suppose u =

1
0

, v =

0
1

. The tensor (au + bv) ⊗ cu, where a, b, c ∈ C is
 
a

1
0

+ b

0
1
 !
⊗
 
c

1
0
 !
and can be written using the distributive rules as
ac 
1
0

⊗

1
0

+ bc 
0
1

⊗

1
0

.
Gaining fluency in manipulating expressions such as these is important for the study of
quantum algorithms. In ket notation we would write
(a|0i + b|1i) ⊗ c|0i = ac|0i ⊗ |0i + bc|1i ⊗ |0i = ac|00i + bc|10i.
.
Some tensors formed by taking linear combinations within the tensor product space are not
separable and are referred to as entangled. Consider the following Example.
Example 10.3.4
Consider the following element of the tensor product space C
2 ⊗ C
2
formed as a linear
combination of separable tensors:

1
0

⊗

1
0

+

0
1

⊗

0
1

. (10.1)
Show that it is impossible to write this tensor as u ⊗ v where u, v ∈ C
2 and hence that it
represents an entangled state.
Solution
Any vectors u, v ∈ C
2
can be written as linear combinations of basis vectors, that is
u = a

1
0

+ b

0
1

v = c

1
0

+ d

0
1

where a, b, c, d ∈ C. Hence,
u ⊗ v =
 
a

1
0

+ b

0
1
 !
⊗
 
c

1
0

+ d

0
1
 !208 Tensor product spaces
which, using the distributivity properties, can be written
u ⊗ v = ac 
1
0

⊗

1
0

+ ad 
1
0

⊗

0
1

+ bc 
0
1

⊗

1
0

+ bd 
0
1

⊗

0
1

.
Hence, if Equation 10.1 is to be expressed as a separable tensor, we require
ac = 1, ad = 0, bc = 0, bd = 1.
From ad = 0 it follows that either a = 0 or d = 0 but either of these options makes ac = 1 or
bd = 1 impossible. Thus the given tensor (10.1) cannot be expressed as a separable tensor.
It represents an entangled state. This particular state, written briefly as |00i + |11i, when
normalised is known as a Bell state and is prominent in quantum computation.
Exercises
10.1 The tensors |00i − |11i, |01i + |10i and |01i − |10i, once normalised, are so-called
Bell states. Show that each state is entangled.
10.2 Show that |00i + |10i is not an entangled state.
10.3 Show that |10i + |11i is not an entangled state.
10.4 Consider the state
|0i|f(0)i + |1i|f(1)i
where f(x) ∈ {0, 1}. Show that if f(0) = f(1) the state is a separable tensor (not
entangled), whereas if f(0) 6= f(1) the state is entangled.
We have seen that elements in the tensor product space U ⊗ V are linear combinations
of separable tensors u ⊗ v, with u ∈ U, v ∈ V . If {e1, e2, . . . , en} is a basis for U, and
{f1, f2, . . . , fm} is a basis for V then we know
u =
Xn
i=1
αiei
, v =
Xm
j=1
βjfj ,
for some αi
, βj in the underlying field. Thus
u ⊗ v =
Xn
i=1
αiei ⊗
Xm
j=1
βjfj .
This expression can be rewritten using the distributive rules to give
u ⊗ v =
Xn
i=1
Xm
j=1
αiβj ei ⊗ fj .
This means that any separable tensor can be written as a linear combination of the tensor
products, ei ⊗ fj , of the basis vectors, ei and fj in the original vector spaces. The numberCalculation of tensor products 209
of such products is nm. We see that the ‘building blocks’ of the separable tensors are the
nm products ei ⊗ fj and because any tensor is a linear combination of separable tensors it
follows that the set {ei ⊗ fj} over all i, j, is a basis for the tensor product space. Thus the
dimension of the tensor product space is nm and any element can be written
Xn
i=1
Xm
j=1
βi,j ei ⊗ fj where βi,j ∈ F.
Here we focus initially on the tensor product space C
2⊗C
2 because in quantum computation
it is the finite dimensional tensor product spaces C
2 ⊗C
2 ⊗. . . C
2
, (n copies), written ⊗nC
2
,
that are of primary interest. Consider the following example when n = 2.
Example 10.3.5 A basis for C
2 ⊗ C
2
.
Suppose we choose the standard basis of C
2
e1 =

1
0

, e2 =

0
1

.
For any u, v ∈ C
2 we can then write u =

u1
u2

= u1e1 +u2e2, v =

v1
v2

= v1e1 +v2e2.
Then
u ⊗ v = (u1e1 + u2e2) ⊗ (v1e1 + v2e2)
= u1v1 e1 ⊗ e1 + u1v2 e1 ⊗ e2 + u2v1 e2 ⊗ e1 + u2v2 e2 ⊗ e2.
Observe that the dimension of C
2 ⊗ C
2
is four – there must be four tensors in the basis:
{e1 ⊗ e1, e1 ⊗ e2, e2 ⊗ e1, e2 ⊗ e2}.
To facilitate computation, we introduce a matrix notation for this basis, writing the four
basis elements, ei ⊗ ej , as:
e1 ⊗ e1 =


1
0
0
0


, e1 ⊗ e2 =


0
1
0
0


, e2 ⊗ e1 =


0
0
1
0


, e2 ⊗ e2 =


0
0
0
1

 .
It then follows that we can write

u1
u2

⊗

v1
v2

= u1v1


1
0
0
0


+ u1v2


0
1
0
0


+ u2v1


0
0
1
0


+ u2v2


0
0
0
1


=


u1v1
u1v2
u2v1
u2v2

 .
In ket notation we write
u = u1|0i + u2|1i, v = v1|0i + v2|1i,210 Tensor product spaces
so that
u ⊗ v = (u1|0i + u2|1i) ⊗ (v1|0i + v2|1i)
= u1v1|0i ⊗ |0i + u1v2|0i ⊗ |1i + u2v1|1i ⊗ |0i + u2v2|1i ⊗ |1i
= u1v1|00i + u1v2|01i + u2v1|10i + u2v2|11i.
The four basis elements thus obtained are referred to as the computational basis of C
2 ⊗C
2
.
Definition 10.2 The computational basis of C
2 ⊗ C
2
In matrix notation, the four computational basis elements are
e1 ⊗ e1 =


1
0
0
0


, e1 ⊗ e2 =


0
1
0
0


, e2 ⊗ e1 =


0
0
1
0


, e2 ⊗ e2 =


0
0
0
1


and in ket notation
|00i, |01i, |10i, |11i.
We will frequently write this set of basis vectors as BC2⊗C2 or B⊗2C2 .
More generally, the tensor product space ⊗nC
2 has dimension 2n which becomes
huge as n increases. The basis will be written in a similar fashion, as B⊗nC2 =
{|00 · · · 0i, . . . , |11 · · · 1i}.
Example 10.3.6
a) Evaluate |00ih00|.
b) Evaluate |01ih01|.
c) Show that X
11
pq=00
|pqihpq| is the 4 × 4 identity matrix.
Solution
a) Recall that given the ket |00i =


1
0
0
0


the corresponding bra is ￾
1 0 0 0 
.
Then
|00ih00| =


1
0
0
0


￾
1 0 0 0 
=


1 0 0 0
0 0 0 0
0 0 0 0
0 0 0 0

 .
b)
|01ih01| =


0
1
0
0


￾
0 1 0 0 
=


0 0 0 0
0 1 0 0
0 0 0 0
0 0 0 0

 .
c) By evaluating |10ih10| and |11ih11| as in parts a) and b) it follows immediately that
X
11
pq=00
|pqihpq| =


1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1


as requirInner products and norms on the tensor product space C
2 ⊗ C
2 211
Exercises
10.5 Suppose |+i = √
1
2
(|0i + |1i) and |−i = √
1
2
(|0i − |1i).
a) Write down an expression for |+i ⊗ |+i = |++i.
b) Write down an expression for |−i ⊗ |−i = |−−i.
c) Deduce that |++i + |−−i = |00i + |11i.
10.6 Write down the computational basis B⊗3C2 in both ket notation and matrix form.
10.7 Show that
X
3
z=0
z|zihz| =


0 0 0 0
0 1 0 0
0 0 2 0
0 0 0 3

 .
(Hint: express the numbers in the kets and bras as their binary equivalents, e.g.,
for z = 2, z|zihz| = 2|10ih10|).
10.4 Inner products and norms on the tensor product space C
2⊗C
2
We have already seen that the inner product on C
2
can be defined (Definition 4.12) as
follows:
if u =

u1
u2

∈ C
2
, v =

v1
v2

∈ C
2
, then
hu, vi = u
∗
1
v1 + u
∗
2
v2.
In general, the result is a complex number (i.e., a scalar). The norm of u is obtained from
kuk
2 = hu, ui = u
∗
1u1 + u
∗
2u2.
We now consider how the inner product can be defined on a pair of tensors in C
2 ⊗ C
2
.
Definition 10.3 The inner product in the tensor product space
Suppose we have two separable tensors in C
2⊗C
2
, say u⊗v and w⊗x where u, v, w, x ∈ C
2
.
The inner product in the tensor product space C
2 ⊗ C
2
is defined to be
hu ⊗ v, w ⊗ xi = hu, wi · hv, xi.
Because each of the individual inner products on the right hand side is a complex number,
then the overall quantity on the right is the product of two complex numbers, i.e., another
complex number, again a scalar. The · here indicates the multiplication of these scalars. It
is possible to show from this definition that all the required properties of an inner product
are satisfied. The definition of the norm follows from the inner product.
Definition 10.4 The norm of a tensor product
ku ⊗ vk
2 = hu ⊗ v, u ⊗ vi = hu, ui · hv, vi = kuk
2
kv||2
.212 Tensor product spaces
Example 10.4.1
Find the inner product in C
2 ⊗ C
2 given by
D 
1 + i
2 − i

⊗

1 − i
i

,

i
2i 
⊗

3
−2i  E
.
Solution
By definition the inner product equals
D 
1 + i
2 − i

,

i
2i  E
·
D 
1 − i
i

,

3
−2i  E
which equals
￾
1 − i 2 + i 

i
2i 
·
￾
1 + i −i


3
−2i 
.
Calculating the individual inner products gives:

(1 − i)(i) + (2 + i)(2i)
·

(1 + i)(3) + (−i)(−2i)
= (−1 + 5i) · (1 + 3i) = −16 + 2i.
Whilst the inner product on the tensor product space is defined on separable tensors in
C
2 ⊗ C
2
, we can extend the definition in a linear fashion. For example,
hu ⊗ v + u
0 ⊗ v
0
, w ⊗ xi = hu ⊗ v, w ⊗ xi + hu
0 ⊗ v
0
, w ⊗ xi.
10.5 Formal construction of the tensor product space
In this section we show how the tensor product space is formally constructed. Given two
vector spaces U and V over the same field, F, we begin by introducing the concept of a
free vector space generated by the Cartesian product U × V . As we shall see this space is
huge, but the introduction of an equivalence relation and equivalence classes enable us to
group together elements with certain well-defined properties, and in turn, these equivalence
classes become the tensor products.
10.5.1 The free vector space generated by U × V
To construct the required tensor product space we begin with two finite dimensional vector
spaces U and V over the same field F. Let U and V have dimensions n and m, respectively.
Recall that the Cartesian product U ×V is the set of ordered pairs (u, v) with u ∈ U, v ∈ V .
In general, there will be an infinite number of such ordered pairs even when the underlying
vector spaces each have finite dimension. Suppose we make any selection of such ordered
pairs and form a finite linear combination of them. Such a finite linear combination would
take the form
X
i,j
αi,j (ui
, vj ) (ui ∈ U, vj ∈ V, αi,j ∈ F).
The set containing all such linear combinations is called LinSpan (U × V Formal construction of the tensor product space 213
Definition 10.5 The set LinSpan (U × V )
LinSpan (U × V ) is a set containing all finite linear combinations of ordered pairs (ui
, vj )
with ui ∈ U, vj ∈ V :
LinSpan(U × V ) = nX
i,j
αi,j (ui
, vj )
o
(αi,j ∈ F).
Example 10.5.1
Suppose we select just the three ordered pairs
(u1, v1),(u1, v5) and (u3, v2).
A linear combination of these ordered pairs is then
a1,1(u1, v1) + a1,5(u1, v5) + a3,2(u3, v2) where a1,1, a1,5, a3,2 ∈ F.
This linear combination is an element of LinSpan (U × V ).
It is crucial to realise in this discussion that the ordered pair (ui
, vj ) is regarded as a single,
‘enclosed’ entity. That is, we can’t perform any further manipulation on it by, for example,
taking the constants inside the parentheses, or by adding ordered pairs as we might do
ordinarily. Expressions such as
a1,1(u1, v1) + a1,5(u1, v5) + a3,2(u3, v2)
are sometimes referred to as ‘formal sums’, which means we write down what looks like
a sum but we cannot add or scalar multiply the components in parentheses. It might be
helpful to think of (ui
, vj ) as an impenetrable and unbreakable ‘atom’.
The set LinSpan(U × V ) can be endowed with the properties of addition and scalar
multiplication in an obvious way which turn it into a vector space called a free vector
space for which the ordered pairs (ui
, vj ) are the basis vectors.
Example 10.5.2
Suppose U = V = R. Then
U × V = R × R = {(u, v) : u, v ∈ R}.
LinSpan (R × R) is a set containing all finite linear combinations of ordered pairs of real
numbers. Consider two vectors in LinSpan(R × R), say:
u1 = 5(2, 3) + 7(6, 11) − 3(1, −1),
u2 = 2(6, 11) + 3(1, −1) + 8(9, 8).
Note that (2, 3),(6, 11),(1, −1) and (9, 8) are just some of the infinite number of basis
elements of LinSpan (R × R). Then, to illustrate scalar multiplication and vector addition,
observe that
4u1 = 20(2, 3) + 28(6, 11) − 12(1, −1),
u1 + u2 = 5(2, 3) + 9(6, 11) + 8(9, 8).
No further simplification is possible.214 Tensor product spaces
Example 10.5.3
Working in the free vector space LinSpan(R × R), simplify, if possible,
(a) (5, 2) − (3, 2) − (2, 2),
(b) 5(2, 4) − (10, 20),
(c) 12(2, 3) − 8(2, 3).
Solution
(a), (b) In neither case can the expression be simplified. Remember that in the free vector
space, each ordered pair ( , ) is treated as a distinct entity. In particular, 5(2, 4) 6= (10, 20).
(c) 12(2, 3) − 8(2, 3) = 4(2, 3).
Example 10.5.4
Consider the following vectors in the free vector space LinSpan(C
2 × C
2
). Each is a linear
combination of ordered pairs in C
2 × C
2
.
u1 = 7  1 + i
2 − i

,

3i
5 − 2i  + 2  3 − i
−8i 
,

1 + 2i
9 − 2i  .
u2 = 4  1 + i
2 − i

,

3i
5 − 2i  − 2
 3 − i
−8i 
,

1 + 2i
9 − 2i  .
They can be added to give:
u1 + u2 = 11  1 + i
2 − i

,

3i
5 − 2i 
but no further simplification is possible.
The vector space LinSpan(U × V ) thus defined has every element in U × V as a basis
element. So this space has infinite dimension. In the following sections we shall see how to
narrow it down to something more manageable and useful by grouping together elements
with certain properties.
10.5.2 An equivalence relation on LinSpan(U × V ).
We have seen that LinSpan(U × V ) is a vector space. Suppose that S is a subspace of
LinSpan(U × V ) generated by all linear combinations of the form
1. (u1 + u2, v) − (u1, v) − (u2, v),
2. (u, v1 + v2) − (u, v1) − (u, v2),
3. k(u, v) − (ku, v),
4. k(u, v) − (u, kv).
Example 10.5.5
Suppose V = W = R. Consider the vector x in LinSpan (R × R):
x = (5, 11) − (5, 7) − (5, 4).Formal construction of the tensor product space 215
This vector cannot be simplified. But observe that it is of the form given above (see 2.) and
hence x ∈ S.
Example 10.5.6
Suppose V = W = R. Consider the vector y in LinSpan (R × R):
y = 6(5, 11) − (5, 66).
This vector cannot be simplified. But observe that it is of the form given above (see 4.) and
hence x ∈ S.
Suppose that x and y are both elements of LinSpan(U ×V ). We shall say that x is equivalent
to y, written x ∼ y if x − y ∈ S, the aforementioned subspace. The relation ∼ is an
equivalence relation which induces a partition so that every vector in the set LinSpan(U ×V ),
(u, v) say, is in one, and only one, equivalence class or coset written [(u, v)]. The set of
equivalence classes themselves form another vector space written LinSpan(U ×V )/ ∼. Thus
this is the quotient space formed by factoring LinSpan(U × V ) by S.
10.5.3 Definition of the tensor product space
Definition 10.6 The tensor product space, written U ⊗ V , is defined to be the space of
equivalence classes:
U ⊗ V = LinSpan(U × V )/ ∼
with the equivalence class of (u, v) given by
[(u, v)] = {x ∈ LinSpan(U × V ) : x ∼ (u, v)}
where the equivalence relation ∼ is defined by
x ∼ y if x − y ∈ S
and S is the subspace of LinSpan(U × V ) spanned by vectors of the form
1. (u1 + u2, v) − (u1, v) − (u2, v),
2. (u, v1 + v2) − (u, v1) − (u, v2),
3. k(u, v) − (ku, v),
4. k(u, v) − (u, kv).
The above equivalence class can be written equivalently as
[(u, v)] = {x ∈ LinSpan(U × V ) : x − (u, v) ∈ S}
or as
[(u, v)] = {x ∈ LinSpan(U × V ) : x − (u, v) = s for some s ∈ S}.
The elements of the tensor product space U ⊗ V are then written as
[(u, v)] or more usually u ⊗ v.
(Note that it is usual to use the symbol ⊗ both for the tensor product of individual tensors
and in the tensor product space definition.)216 Tensor product spaces
We now have every vector in LinSpan(U × V ) assigned to its equivalence class.
u ⊗ v = [(u, v)] = {x ∈ LinSpan(U × V ) : x = (u, v) + s for some s ∈ S}
= (u, v) + S.
Thus the tensor product is written as the sum of a vector and a subspace (see Section 6.13).
Because of the way the tensor product space was constructed, it follows that the tensor
product u ⊗ v obeys the properties given in Table 10.1. We now detail why this is the case.
Property (i) of Table 10.1: (u1 + u2) ⊗ v = u1 ⊗ v + u2 ⊗ v.
By definition
(u1 + u2) ⊗ v = [(u1 + u2, v)]
= (u1 + u2, v) + S
= (u1 + u2, v) − s + S for any s ∈ S.
Suppose now we choose s to be
s = (u1 + u2, v) − (u1, v) − (u2, v)
which is clearly in S (Property 1 above). Then
(u1 + u2) ⊗ v = (u1 + u2, v) − ((u1 + u2, v) − (u1, v) − (u2, v)) + S
= (u1, v) + (u2, v) + S
= ((u1, v) + S) + ((u2, v) + S)
= [(u1, v)] + [(u2, v)]
= u1 ⊗ v + u2 ⊗ v
as required.
Property (iii) of Table 10.1: λ(u ⊗ v) = (λu) ⊗ v
By definition
λ(u ⊗ v) = λ[(u, v)]
= λ((u, v) + S)
= λ(u, v) + S
= λ(u, v) − s + S for any s ∈ S.
Choose s = λ(u, v) − (λu, v) which is clearly in S (Property 3 above). Then
λ(u ⊗ v) = λ(u, v) − (λ(u, v) − (λu, v)) + S
= (λu, v) + S
= [(λu, v)]
= (λu) ⊗ v
as required. The remaining two properties can be verified in a similar way.End-of-chapter exercises 217
10.6 End-of-chapter exercises
1. State the dimension of each of the following tensor product spaces and give their
natural bases:
(a) R
2 ⊗ R
2
,
(b) R
3 ⊗ R
2
,
(c) R
2 ⊗ R
4
.
2. Determine whether
|00i − |01i + |10i − |11i
is an entangled tensor.
3. Calculate the inner product
h

4
5

⊗

2
−3

,

1
−1

⊗

−2
2

i.
4. Calculate the inner product
h

1 − i
3 + 2i 
⊗

3i
1 + i 
,

1 + i
−3i 
⊗

2i
5 + i 
i.
5. Let
B1 =
1
√
2
(|0i ⊗ |0i + |1i ⊗ |1i, B2 =
1
√
2
(|0i ⊗ |1i + |1i ⊗ |0i,
B3 =
1
√
2
(|0i ⊗ |0i − |1i ⊗ |1i, B4 =
1
√
2
(|0i ⊗ |1i − |1i ⊗ |0i.
Evaluate:
(a) B1 + B3,
(b) B2 + B4,
(c) B1 − B3,
(d) B2 − B4.
6. Use the distributive properties of the tensor product to show that
(|0i + |1i) ⊗ (|0i ⊗ |0i + |1i ⊗ |1i)
can be expressed as
|000i + |100i + |011i + |111i.
7. If {e1, e2, . . . , en} is a basis for U, and {f1, f2, . . . , fm} is a basis for V then if
u ∈ U, v ∈ V ,
u =
Xn
i=1
αiei
, v =
Xm
j=1
βjfj ,218 Tensor product spaces
for some αi
, βj in the underlying field. Use the distributive properties of the tensor
product (Table 10.1) to show that any separable tensor can be written
u ⊗ v =
Xn
i=1
Xm
j=1
αiβj ei ⊗ fj .
8. Express X
11
pq=00
|pqi hpq| as a 4 × 4 matrix. (Note the sum is over all elements of the
set {00, 01, 10, 11}.)11
Linear operators and their matrix representations
11.1 Objectives
Linear operators play a prominent role in the study of quantum computation. The principal
objective of this chapter is to define a linear operator and demonstrate how one can be
represented by a matrix. Two particularly important types of linear operator are unitary
operators and self-adjoint operators. Unitary operators determine the time evolution, or
dynamics of a quantum system by acting on the state vectors residing in a vector space V .
We shall be interested in the way that an operator, A say, acts on a quantum state vector
|ψi to produce another state vector |φi, i.e.,
A|ψi = |φi, |ψi, |φi ∈ V.
That is, unitary operators bring about state-to-state transitions. Furthermore, self-adjoint
operators are used to model physical observables. Following the measurement of an ob￾servable on a quantum state the post-measurement state is related to the eigenvectors of
the operator and the possible measured values are its eigenvalues. We demonstrate how to
calculate these eigenvalues and eigenvectors. The relevance of self-adjoint and unitary oper￾ators to quantum computation is explored in detail when we study the axioms of quantum
computation in Chapter 13. Finally, given two linear operators, A and B say, we explain
how their tensor product A ⊗ B is calculated.
11.2 Linear operators
In Chapter 9 we saw that a linear transformation, f, is a function or mapping between
vector spaces, V and W say, over the same field F, which preserves the linear structure of
the vector space V . That is, for any u, v ∈ V and k ∈ F,
f : V → W
f(u + v) = f(u) + f(v)
f(ku) = kf(u)
Now consider the special case when both domain and co-domain are the same vector space
V over the field F which we will henceforth take to be C.
Definition 11.1 Linear operator
A linear operator, A, is a linear transformation from a vector space V , over the field C,
to itself:
A : V → V
DOI: 10.1201/9781003264569-11 219220 Linear operators and their matrix representations
with the property that
A(u + v) = A(u) + A(v)
A(ku) = kA(u)
for any u, v ∈ V and k ∈ C.
Given a basis of V we shall see that the operator A can be represented by a matrix. A
calligraphic font has been used to denote the linear operator, but occasionally we shall
abuse this notation and simply denote A by its matrix representation, A. (Some authors
write the operator as Aˆ to distinguish it from the matrix A.)
Example 11.2.1
Consider the operator A : C
2 → C
2 defined by
A

z1
z2

=

2z1 − 3z2
4z1 + z2

where z1, z2 ∈ C.
Find A

1
2

.a)
Find A

1 + i
2 − i

.b)
c) Show that A is a linear operator.
Solution
a) A

1
2

=

2(1) − 3(2)
4(1) + 2 
=

−4
6

.
b) A

1 + i
2 − i

=

2(1 + i) − 3(2 − i)
4(1 + i) + (2 − i) 
=

−4 + 5i
6 + 3i 
.
Clearly the operator A maps vectors in C
2
to other vectors in C
2
.
c) Consider two vectors in C
2
, u =

u1
u2

, v =

v1
v2

, where u1, u2, v1, v2 ∈ C. First
note that
A(u) = A

u1
u2

=

2u1 − 3u2
4u1 + u2

.
A(v) = A

v1
v2

=

2v1 − 3v2
4v1 + v2

.
Adding u and v:
u + v =

u1
u2

+

v1
v2

=

u1 + v1
u2 + v2

.Linear operators 221
Applying the operator A to u + v we obtain
A(u + v) = A

u1 + v1
u2 + v2

=

2(u1 + v1) − 3(u2 + v2)
4(u1 + v1) + (u2 + v2)

=

(2u1 − 3u2) + (2v1 − 3v2)
(4u1 + u2) + (4v1 + v2)

=

2u1 − 3u2
4u1 + u2

+

2v1 − 3v2
4v1 + v2)

= A(u) + A(v).
The first of the requirements for A to be a linear operator is therefore satisfied.
Now, if k ∈ C, ku = k

u1
u2

=

ku1
ku2

. Then
A(ku) = A

ku1
ku2

=

2ku1 − 3ku2
4ku1 + ku2

= k

2u1 − 3u2
4u1 + u2

= kA(u).
Thus the second of the requirements for A to be a linear operator is satisfied. We have
confirmed that A is a linear operator.
Exercises
11.1 Consider A : R
2 → R
2 defined by A

x1
x2

=

x1 + x2 + 1
x1 − x2

where x1, x2 ∈
R. Show that A is not a linear operator.
11.2 Show that the identity operator I : C
2 → C
2 defined by I

z1
z2

=

z1
z2

where z1, z2 ∈ C is a linear operator.
11.3 Show that the operator A : C
2 → C
2 defined by A

z1
z2

=

z1
0

is a linear
operator.
11.4 Show that the operator A : C
4 → C
4 defined by A


z1
z2
z3
z4

 =


0
z2
0
z4


is a linear
operator.
11.5 Show that the kernel of the linear operator A : R
2 → R
2 defined by A

x1
x2

=

x1 + 2x2
3x1 + 4x2

contains only the zero vector 
0
0

. What is the dimension of
ker(A) ?222 Linear operators and their matrix representations
11.6 Show that the kernel of the linear operator A : R
2 → R
2 defined by A

x1
x2

=

x1 + 2x2
2x1 + 4x2

is spanned by the vector 
−2
1

.
11.7 Consider the linear operators A : C
2 → C
2
, B : C
2 → C
2
. We define their sum by
(A + B)u = Au + Bu
for u ∈ C
2
, and multiplication by a scalar k ∈ C as
(kA)u = k(Au).
Let z1, z2 ∈ C. If A(z1, z2) = (z1 − z2, z1 + z2), B(z1, z2) = (z1 + z2, z1 − z2), find
an expression for the operators A + B and kA.
11.8 If A : C
2 → C
2
, B : C
2 → C
2
, their product AB is defined by the composition
(AB)u = (A ◦ B)u = A(Bu).
With A and B defined in the previous question:
a) find an expression for AB,
b) find an expression for BA.
Deduce that, in general, AB 6= BA.
11.3 The matrix representation of a linear operator
Consider a linear operator A : V → V . Suppose that E = {e1, e2, . . . , en} is a basis for V .
We can apply A to each basis vector and the result will be another vector in V . Hence,
A(e1) = a11e1 + a21e2 + . . . + an1en
A(e2) = a12e1 + a22e2 + . . . + an2en
.
.
. =
.
.
.
A(en) = a1ne1 + a2ne2 + . . . + annen
for some aij ∈ C, i, j = 1, 2, . . . , n. The transpose of the matrix of coefficients in the above
expressions is the matrix representation of A in the given basis. Thus
A =


a11 a12 · · · a1n
a21 a22 · · · a2n
.
.
.
.
.
.
.
.
.
.
.
.
an1 an2 · · · ann


.
Observe that each column of A is the coordinate vector of the image under A of a basis
vector. It is important to note that changing the basis will change the representation.The matrix representation of a linear operator 223
Example 11.3.1 Finding a matrix representation of a linear operator
Suppose z = (z1, z2) ∈ C
2
. Consider the linear operator defined by
A : C
2 → C
2
, A(z) = A(z1, z2) = (0, z2).
Thus A acts on a row vector in C
2
to produce another row vector in the same vector space.
Suppose we choose E = {e1, e2} where e1 = (1, 0), e2 = (0, 1) as the basis of C
2
, i.e., the
standard basis. Then, z = (z1, z2) can be expressed in terms of this basis as
z = (z1, z2) = z1(1, 0) + z2(0, 1) = z1e1 + z2e2.
Thus the coordinate vector (see Chapter 6) corresponding to z in the stated basis is then
[z]E =

z1
z2

.
Applying the operator A to each basis vector,
A(e1) = A(1, 0) = (0, 0) = 0e1 + 0e2,
A(e2) = A(0, 1) = (0, 1) = 0e1 + 1e2,
and the transpose of the matrix of coefficients gives the required matrix representation:
A =

0 0
0 1 
.
We can now use this matrix representation to carry out the transformation. Any vector z
is transformed by multiplying its coordinate vector [z]E by the matrix A. Thus
A[z]E = A

z1
z2

=

0 0
0 1   z1
z2

=

0
z2

.
The result, 
0
z2

, is the coordinate vector following the transformation. We emphasise
again that if we change the underlying basis, the matrix representation will change. Consider
the following example.
Example 11.3.2 Finding a matrix representation of a linear operator
Suppose we have the same operator as in Example 11.3.1 but this time we choose the basis
E = {e1, e2} where e1 = ( √
1
2
, √
1
2
), e2 = ( √
1
2
, − √
1
2
). We first express z = (z1, z2) in terms of
this basis: let
(z1, z2) = k1e1 + k2e2 = k1(
1
√
2
,
1
√
2
) + k2(
1
√
2
, −
1
√
2
)
Equating components
z1 =
1
√
2
k1 +
1
√
2
k2, z2 =
1
√
2
k1 −
1
√
2
k2
and solving for k1 and k2 we obtain k1 =
√
2(z1+z2)
2
, k2 =
√
2(z1−z2)
2
. Then
z = (z1, z2) =
√
2(z1 + z2)
2
e1 +
√
2(z1 − z2)
2
e2.224 Linear operators and their matrix representations
Thus the coordinate vector corresponding to z in the basis E is
[z]E =
 √
2(z1+z2)
√
2
2(z1−z2)
2
!
.
Applying the operator A to each basis vector
A(e1) = A(
1
√
2
,
1
√
2
) = (0,
1
√
2
)
which we express in terms of the basis E = {e1, e2} where e1 = ( √
1
2
, √
1
2
), e2 = ( √
1
2
, − √
1
2
):
(0,
1
√
2
) = 1
2
e1 −
1
2
e2.
Likewise,
A(e2) = A(
1
√
2
, −
1
√
2
) = (0, −
1
√
2
) = −
1
2
e1 +
1
2
e2
and the transpose of the matrix of coefficients gives the required matrix representation:
A =
 1
2 −
1
2
−
1
2
1
2
!
.
As above, the matrix representation acts on column vectors:
Az = A[z]E = A


√
2(z1+z2)
2
√
2(z1−z2)
2

 =
 1
2 −
1
2
−
1
2
1
2
! 

√
2(z1+z2)
2
√
2(z1−z2)
2

 =
 √
2
2
z2
−
√
2
2
z2
!
.
The result is the coordinate vector with respect to the basis E. We emphasise again that
changing the basis changes the matrix representation.
Example 11.3.3 The identity operator, I, on C
2
.
Consider the linear operator I : C
2 → C
2 by I(z1, z2) = (z1, z2), where z1, z2 ∈ C. The
underlying field is C. Note that the output vector is the same as the input, hence the
name identity operator. Suppose we choose as a basis for C
2
the vectors e1 = (1, 0), e2 =
(0, 1). Then it follows immediately that the matrix representation is the identity matrix
I =

1 0
0 1 
.
Exercises
11.9 Suppose z = (z1, z2) ∈ C
2 and consider the linear operator A : C
2 → C
2
, A(z) =
A(z1, z2) = (z2, z1). Find the matrix representation of A in the standard basis
and in the basis E = {e1, e2} where e1 =

√
1
2
, √
1
2

and e2 =

√
1
2
, − √
1
2

.The matrix representation of a linear operator when the underlying basis is orthonormal 225
11.4 The matrix representation of a linear operator when the
underlying basis is orthonormal
Consider again a linear operator A : V → V . Suppose this time that we choose a basis for
V , E = {e1, e2, . . . , en}, which is orthonormal so that hei
|ej i = δij with respect to the inner
product hu, vi =
Xn
i=1
u
∗
i
vi
. As before, we can apply A to each basis vector and the result
will be another vector in V . Hence,
A(e1) = a11e1 + a21e2 + . . . + an1en
A(e2) = a12e1 + a22e2 + . . . + an2en
.
.
. =
.
.
.
A(en) = a1ne1 + a2ne2 + . . . + annen.
Now consider taking the inner product of each of the above expressions with each basis
vector in turn: for example
he1|Ae1i = a11he1|e1i + a21he1|e2i + . . . + an1he1|eni = a11
he2|Ae1i = a11he2|e1i + a21he2|e2i + . . . + an1he2|eni = a21
and so on. Note hei
|ej i = δij =

0 i 6= j
1 i = j
.
Likewise,
he1|Ae2i = a12he1|e1i + a22he1|e2i + . . . + an2he1|eni = a12
he2|Ae2i = a12he2|e1i + a22he2|e2i + . . . + an2he2|eni = a22.
Observe that aij = hei
|Aej i. Hence, the matrix A is given by
A =


he1|Ae1i he1|Ae2i · · · he1|Aeni
he2|Ae1i he2|Ae2i · · · he2|Aeni
.
.
.
.
.
.
.
.
.
.
.
.
hen|Ae1i hen|Ae2i · · · hen|Aeni


.
This representation shows the advantage of choosing an orthonormal basis for V .
Exercises
11.10 Consider the linear operator A : C
2 → C
2
, A(z) = A(z1, z2) = (0, z2). Let an
orthonormal basis of C
2 be {e1, e2} where e1 = ( √
1
2
, √
1
2
), e2 = ( √
1
2
, − √
1
2
). With
the inner product on C
2 defined as hu, vi =
P2
i=1 u
∗
i
vi calculate:
a) Ae1 b) Ae2 c) he1, Ae1i d) he1, Ae2i
e) he2, Ae1i f) he2, Ae2i
and hence write down the matrix representation of A in the given orthonormal
basis.226 Linear operators and their matrix representations
11.5 Eigenvalues and eigenvectors of linear operators
Consider a linear operator A on a vector space V , and particular, non-zero, state vectors
|ψi ∈ V which satisfy
A|ψi = λ|ψi where λ ∈ C.
Definition 11.2 The eigenvalues and eigenvectors of a linear operator A
Given a vector space V over the field C and a linear operator A : V → V , then a non-zero
state vector |ψi is an eigenvector or eigenstate of A with eigenvalue λ ∈ C if
A|ψi = λ|ψi.
These properties are analogous to those of eigenvalues and eigenvectors of matrices discussed
in Chapter 7. In fact we can use the matrix representation of a linear operator to find its
eigenvalues and eigenvectors.
Example 11.5.1
Find the eigenvalues and corresponding eigenvectors of the linear operator A : C
2 → C
2
,
A(z1, z2) = (0, z2), where z1, z2 ∈ C, which has matrix representation, with respect to the
standard basis, given by
A =

0 0
0 1 
.
Solution
The eigenvalues of this matrix (and of the operator) are found by solving |A − λI| = 0, i.e.,




−λ 0
0 1 − λ



 = λ
2 − λ = λ(λ − 1) = 0
so the eigenvalues are λ = 0, 1. The corresponding (normalised) eigenvectors of A are readily
shown to be (1, 0)T and (0, 1)T
, respectively.
Exercises
11.11 Consider the linear operator A : C
2 → C
2
, A(z1, z2) = (z1, −z2).
a) Show that the matrix representation of A with respect to the standard basis
is A =

1 0
0 −1

.
b) Find the eigenvalues and corresponding eigenvectors of A.
11.12 Consider the linear operator A : C
2 → C
2
, A(z1, z2) = (z2, −z1). Choose the
orthonormal basis of C
2
to be {e1, e2} where e1 = ( √
1
2
, √
1
2
), e2 = ( √
1
2
, − √
1
2
).
Find the matrix representation of A in this basis and determine its eigenvalues
and eigenvectors.The adjoint and self-adjoint linear operators 227
11.6 The adjoint and self-adjoint linear operators
Consider the vector space C
n over the field C. Suppose A is a linear operator on this space.
Suppose further that for u, v ∈ C
n an inner product is defined by hu, vi =
Xn
i=1
u
∗
i
vi for
ui
, vi ∈ C. For every linear operator A there exists another, called the adjoint and denoted
A†
, which is defined as follows:
Definition 11.3 The adjoint operator, A†
For every linear operator A, the adjoint, A†
, is a linear operator such that
hu, Avi = hA†u, vi
for any u, v ∈ C
n.
Example 11.6.1
Consider the linear operator S : C
2 → C
2 defined by S(z1, z2) = (az1 + bz2, cz1 + dz2),
where a, b, c, d, z1, z2 ∈ C. Determine the adjoint operator S
†
.
Solution
Let the adjoint operator, S
†
, be defined by
S
†
: C
2 → C
2
, S
†
(z1, z2) = (a
0
z1 + b
0
z2, c0
z1 + d
0
z2).
Then
h(z1, z2), S(w1, w2)i = h(z1, z2),(aw1 + bw2, cw1 + dw2)i
= z
∗
1
(aw1 + bw2) + z
∗
2
(cw1 + dw2).
Also,
h(S
†
(z1, z2),(w1, w2)i = h(a
0
z1 + b
0
z2, c0
z1 + d
0
z2),(w1, w2)i
= (a
0∗z
∗
1 + b
0∗z
∗
2
)w1 + (c
0∗z
∗
1 + d
0∗z
∗
2
)w2.
For these to be equal, we require
a
0∗ = a, b0∗ = c, c0∗ = b, d0∗ = d.
Thus
S
†
(z1, z2) = (a
∗
z1 + c
∗
z2, b∗
z1 + d
∗
z2).
Example 11.6.2
Find the matrix representations, with respect to the standard basis, of the linear operators
S and S
† of Example 11.6.1. Show that the matrix S
†
is the adjoint of the matrix S as
defined in Chapter 5.228 Linear operators and their matrix representations
Solution
Applying S to each basis vector:
S(1, 0) = (a, c) = a(1, 0) + c(0, 1), S(0, 1) = (b, d) = b(1, 0) + d(0, 1).
The transpose of the matrix of coefficients gives the required matrix representation:
S =

a b
c d 
.
Likewise, applying S
†
to each basis vector:
S
†
(1, 0) = (a
∗
, b∗
) = a
∗
(1, 0) + b
∗
(0, 1), S
†
(0, 1) = (c
∗
, d∗
) = c
∗
(1, 0) + d
∗
(0, 1).
The transpose of the matrix of coefficients gives the required matrix representation:
S
† =

a
∗
c
∗
b
∗ d
∗

.
Referring to Chapter 5, we observe that S
†
is the adjoint (that is, the conjugate transpose)
of S.
Exercises
11.13 Given that S : C
2 → C
2
, S(z1, z2) = (−iz1 + (1 + i)z2, 4z1 − 2iz2) verify that the
adjoint operator S
†
is given by S
†
(z1, z2) = (iz1 + 4z2,(1 − i)z1 + 2iz2).
Linear operators A which have the property that they are equal to their adjoint, i.e A† = A,
are described as self-adjoint.
Definition 11.4 Self-adjoint operator
If a linear operator A is equal to its adjoint, i.e. A† = A, then the operator is said to be
self-adjoint. In such cases,
hu, Avi = hAu, vi.
Self-adjoint operators are important in quantum computation because they are related to
quantum measurement. This aspect is discussed in detail in Chapter 13. We note, without
proof, the following properties of a self-adjoint operator, A, on a finite-dimensional complex
inner product space:
Theorem 11.1 Given a self-adjoint operator A, on a finite-dimensional complex inner
product space,
1. the eigenvalues are real,
2. eigenvectors corresponding to distinct eigenvalues are orthogonal,
3. if the eigenvalues are all distinct then the eigenvectors form a basis of the
underlying vector space.Unitary operators 229
Note that even when some of the eigenvalues are repeated, it is possible to construct an
orthonormal basis of the underlying vector space.
Example 11.6.3
Suppose the inner product is defined by hu, vi =
X
i
u
∗
i
vi
. Show that the linear operator A
of Example 11.3.1 is self-adjoint, that is hu, Avi = hAu, vi.
Solution
We use the previous matrix representation A =

0 0
0 1 
with respect to the standard
basis E.
Let [u]E =

u1
u2

, [v]E =

v1
v2

. Then
A[u]E =

0 0
0 1   u1
u2

=

0
u2

,
A[v]E =

0 0
0 1   v1
v2

=

0
v2

.
Then
hu, Avi =
￾
u
∗
1 u
∗
2


0
v2

= u
∗
2
v2.
Similarly,
hAu, vi =
￾
0 u
∗
2


v1
v2

= u
∗
2
v2.
Thus hu, Avi = hAu, vi and so A is a self-adjoint operator. Thus the operator of Example
11.3.1 is self-adjoint and is represented in the basis {|0i, |1i} by the matrix A =

0 0
0 1 
.
It has eigenvalues 0 and 1 with eigenvectors (1, 0) and (0, 1), respectively. These facts will
be used repeatedly in the quantum computations which follow.
11.7 Unitary operators
Consider a linear operator U : V → V , and its adjoint U
†
. We have seen that the adjoint
operator U
†
satisfies
hu, Uvi = hU†u, vi.
Definition 11.5 Unitary operator
If a linear operator U is such that
U
† = U
−1
the operator is said to be unitary.
So a unitary operator has the property that its adjoint is equal to its inverse. Note that if
an operator A is both self-adjoint and unitary, then A = A† = A−1
in which case we say
that A is self-invers230 Linear operators and their matrix representations
Unitary operators play the role in quantum computation that Boolean functions play
in digital computation, that is they operate on quantum states to effect time evolution.
In quantum computation a state vector must have norm equal to 1, and thus in the time
evolution of the state this norm must be preserved. The following theorem shows that
unitary operators have this desired effect.
Theorem 11.2 Preservation of the inner product
If a unitary operator U acts on vectors u and v in a vector space V the value of the inner
product of u and v remains unchanged:
hUu, Uvi = hu, vi.
Hence, the norm of a vector is preserved under the unitary transformation.
Proof
hUu, Uvi = hU†Uu, vi using the property of the adjoint
= hU−1Uu, vi because U is unitary
= hIu, vi because U
−1U is the identity operator
= hu, vi.
We have shown that when the operator is unitary, it preserves the value of the inner product.
Recall that the norm-squared of v is given by hv, vi = kvk
2
. Hence, from the previous result,
hUv, Uvi = hv, vi
kUvk
2 = kvk
2
.
Thus when a unitary operator acts on a vector, the norm of the vector is not changed.

In quantum computation, state vectors with unit norm will transform under a unitary
operator to another state vector also with a unit norm.
Example 11.7.1
Consider the linear operator A : C
3 → C
3 with matrix representation
A =


0 1 0
0 0 1
1 0 0

 .
Show that this matrix is unitary but not self-adjoint.
Solution
The adjoint matrix is given by the conjugate transpose A† =


0 0 1
1 0 0
0 1 0

 . Observe
immediately that A 6= A†
so A is not self-adjoint. It is straightforward to verify that
AA† =


0 1 0
0 0 1
1 0 0




0 0 1
1 0 0
0 1 0

 =


1 0 0
0 1 0
0 0 1

 = I.Linear operators on tensor product spaces 231
Likewise, A†A = I, and hence, A is a unitary operator.
Example 11.7.2
The Pauli operator X : C
2 → C
2
is defined such that if z = (z1, z2) then
X (z) = X (z1, z2) = (z2, z1).
Show that this operator is unitary and self-adjoint.
Solution
The matrix representation of X in the standard basis is X =

0 1
1 0 
. Taking the con￾jugate transpose it is clear that the adjoint of X is X† =

0 1
1 0 
= X and hence X
is self-adjoint. Moreover, X−1 =
1
−1

0 −1
−1 0 
=

0 1
1 0 
= X† and hence X is
unitary.
Exercises
11.14 Find the matrix representation, with respect to the standard basis, of the linear
operator (Pauli operator) Σ2 : C
2 → C
2 defined by Σ2(z1, z2) = (−iz2, iz1). Show
that this operator is self-adjoint and unitary.
11.15 Show that the product of two unitary matrices is a unitary matrix.
11.8 Linear operators on tensor product spaces
We have already seen how a linear operator A say, acts on vectors in a vector space, for
example C
2
, to produce another vector in the same space. We now start to explore how
such operators act on vectors in the tensor product space C
2 ⊗ C
2
.
Definition 11.6 Linear operators on tensor product spaces
If A : C
2 → C
2 and B : C
2 → C
2
, then A ⊗ B : C
2 ⊗ C
2 → C
2 ⊗ C
2
is given by
(A ⊗ B)(|ψi ⊗ |φi) = A|ψi ⊗ B|φi
where |ψi, |φi ∈ C
2
.
We shall assume that the operators are self-adjoint because such operators are related to the
measurement of observables. We now demonstrate how the tensor product can be calculated
in practice. Assume the standard computational basis. Let the matrix representations of A
and B be
A =

a11 a12
a21 a22 
, B =

b11 b12
b21 b22 232 Linear operators and their matrix representations
and let |ψi =

ψ1
ψ2

, |φi =

φ1
φ2

. Then
A|ψi =

a11 a12
a21 a22   ψ1
ψ2

=

a11ψ1 + a12ψ2
a21ψ1 + a22ψ2

,
B|φi =

b11 b12
b21 b22   φ1
φ2

=

b11φ1 + b12φ2
b21φ1 + b22φ2

.
It follows that (see e.g., Example 10.3.5)
A|ψi ⊗ B|φi =

a11ψ1 + a12ψ2
a21ψ1 + a22ψ2

⊗

b11φ1 + b12φ2
b21φ1 + b22φ2

=


(a11ψ1 + a12ψ2)(b11φ1 + b12φ2)
(a11ψ1 + a12ψ2)(b21φ1 + b22φ2)
(a21ψ1 + a22ψ2)(b11φ1 + b12φ2)
(a21ψ1 + a22ψ2)(b21φ1 + b22φ2)


=


a11b11ψ1φ1 + a11b12ψ1φ2 + a12b11ψ2φ1 + a12b12ψ2φ2
a11b21ψ1φ1 + a11b22ψ1φ2 + a12b21ψ2φ1 + a12b22ψ2φ2
a21b11ψ1φ1 + a21b12ψ1φ2 + a22b11ψ2φ1 + a22b12ψ2φ2
a21b21ψ1φ1 + a21b22ψ1φ2 + a22b21ψ2φ1 + a22b22ψ2φ2


=


a11b11 a11b12 a12b11 a12b12
a11b21 a11b22 a12b21 a12b22
a21b11 a21b12 a22b11 a22b12
a21b21 a21b22 a22b21 a22b22




ψ1φ1
ψ1φ2
ψ2φ1
ψ2φ2

 .
We require this expression to equal (A ⊗ B)(|ψi ⊗ |φi). Now
|ψi ⊗ |φi =


ψ1φ1
ψ1φ2
ψ2φ1
ψ2φ2

 .
Thus
A ⊗ B must equal


a11b11 a11b12 a12b11 a12b12
a11b21 a11b22 a12b21 a12b22
a21b11 a21b12 a22b11 a22b12
a21b21 a21b22 a22b21 a22b22

 .
And so
A ⊗ B =

a11 a12
a21 a22 
⊗

b11 b12
b21 b22 
=


a11b11 a11b12 a12b11 a12b12
a11b21 a11b22 a12b21 a12b22
a21b11 a21b12 a22b11 a22b12
a21b21 a21b22 a22b21 a22b22


revealing that A ⊗ B can be calculated from
A ⊗ B =

a11 a12
a21 a22 
⊗

b11 b12
b21 b22 
=


a11 
b11 b12
b21 b22 
a12 
b11 b12
b21 b22 
a21 
b11 b12
b21 b22 
a22 
b11 b12
b21 b22 

 .Linear operators on tensor product spaces 233
Example 11.8.1
If X =

0 1
1 0 
and Y =

0 −i
i 0 
find X ⊗ Y and Y ⊗ X.
Solution
X ⊗ Y =

0 1
1 0 
⊗

0 −i
i 0 
=


0

0 −i
i 0 
1

0 −i
i 0 
1

0 −i
i 0 
0

0 −i
i 0 


=


0 0 0 −i
0 0 i 0
0 −i 0 0
i 0 0 0

 .
Y ⊗ X =

0 −i
i 0 
⊗

0 1
1 0 
=


0

0 1
1 0 
−i

0 1
1 0 
i

0 1
1 0 
0

0 1
1 0 


=


0 0 0 −i
0 0 −i 0
0 i 0 0
i 0 0 0

 .
Example 11.8.2
If H = √
1
2

1 1
1 −1

and I =

1 0
0 1 
find H ⊗ I.
Solution
H ⊗ I =
1
√
2

1 1
1 −1

⊗

1 0
0 1 
=
1
√
2


1

1 0
0 1 
1

1 0
0 1 
1

1 0
0 1 
−1

1 0
0 1 


=
1
√
2


1 0 1 0
0 1 0 1
1 0 −1 0
0 1 0 −1

 .234 Linear operators and their matrix representations
Example 11.8.3
Consider the following two linear operators:
the Pauli X operator is given by X : C
2 → C
2 which on the standard computational
basis states is defined as X |0i = |1i, X |1i = |0i.
the Pauli Z operator is given by Z : C
2 → C
2 which on the standard computational
basis states is defined as Z|0i = |0i, Z|1i = −|1i.
Find (X ⊗ Z)|ψi when |ψi = |1i|1i.
Solution
(X ⊗ Z)|ψi = (X ⊗ Z)(|1i|1i)
= X |1i ⊗ Z|1i
= |0i ⊗ (−|1i)
= −|0i|1i
= −|01i.
We illustrate how the same result can be obtained using matrices. The matrix representa￾tions (in the standard computational basis of C
2
) of X and Z are
X =

0 1
1 0 
, Z =

1 0
0 −1

.
Then
X ⊗ Z =


0 0 1 0
0 0 0 −1
1 0 0 0
0 −1 0 0

 .
Now
|ψi = |1i|1i
=

0
1

⊗

0
1

=


0
0
0
1


and so
(X ⊗ Z)|ψi =


0 0 1 0
0 0 0 −1
1 0 0 0
0 −1 0 0




0
0
0
1

 =


0
−1
0
0

 = −


0
1
0
0

 = −|01i.
Example 11.8.4
Given the Hadamard transform H = √
1
2

1 1
1 −1

,
a) calculate the matrix representation of H⊗2 = H ⊗ H.Linear operators on tensor product spaces 235
b) evaluate (H ⊗ H)(|0i ⊗ |0i) by explicitly evaluating the tensor products.
c) evaluate (H ⊗ H)(|0i ⊗ |0i) using the matrix found in part a).
Solution
a)
H ⊗ H =
1
√
2

1 1
1 −1

⊗
1
√
2

1 1
1 −1

=
1
2

1 1
1 −1

⊗

1 1
1 −1

=
1
2


1

1 1
1 −1

1

1 1
1 −1

1

1 1
1 −1

−1

1 1
1 −1



=
1
2


1 1 1 1
1 −1 1 −1
1 1 −1 −1
1 −1 −1 1

 .
b) From the definition of how the tensor product of operators acts on vectors in tensor
product spaces, we have
(H ⊗ H)(|0i ⊗ |0i) = H|0i ⊗ H|0i
=
1
√
2
(|0i + |1i) ⊗
1
√
2
(|0i + |1i)
=
1
2
(|00i + |01i + |10i + |11i)
=
1
2




1
0
0
0


+


0
1
0
0


+


0
0
1
0


+


0
0
0
1




=
1
2


1
1
1
1

 .
c) Noting that |0i ⊗ |0i =


1
0
0
0


we have
(H ⊗ H)(|0i ⊗ |0i) = 1
2


1 1 1 1
1 −1 1 −1
1 1 −1 −1
1 −1 −1 1




1
0
0
0

 =
1
2


1
1
1
1

 .
Example 11.8.5
Suppose A =

0 0
0 1 
and I =

1 0
0 1 
. Evaluate A ⊗ I ⊗ I.236 Linear operators and their matrix representations
Solution
By a natural extension of the foregoing, we have
A ⊗ I ⊗ I =

0 0
0 1 
⊗

1 0
0 1 
⊗

1 0
0 1 
=

0 0
0 1 
⊗


1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1


=


0


1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1


0


1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1


0


1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1


1


1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1




=


0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0
0 0 0 0 1 0 0 0
0 0 0 0 0 1 0 0
0 0 0 0 0 0 1 0
0 0 0 0 0 0 0 1


.
Exercises
11.16 Evaluate each of (H ⊗H)|01i, (H ⊗H)|10i, (H ⊗H)|11i, by explicitly evaluating
the tensor products and by using the matrix in Example 11.8.4.
11.17 Given x1, x2 ∈ {0, 1} and the Hadamard transform, H, show that
(H ⊗ H)(|x1i ⊗ |x2i)
can be expressed as
1
2
(X
1
y=0
(−1)y·x1
|yi ⊗X
1
y=0
(−1)y·x2
|yi
)
and then as
1
2
O
2
i=1
X
1
y=0
(−1)y·xi
|yi.
(Note On
i=1
represents the n-fold tensor product.) Using this result, confirm the
answers to Exercise 11.16.Linear operators on tensor product spaces 237
11.18 Consider again Exercise 11.17. Now let |yi, y = 0, 1, 2, 3, be the decimal represen￾tation of |00i, |01i, |10i, |11i. Let x = (x1, x2). Show that the expressions for the
tensor product (H ⊗ H)(|x1i ⊗ |x2i) given therein are equivalent to
1
2
X
3
y=0
(−1)x·y
|yi
where x · y is the scalar product Pxiyi and multiplication and addition are
performed modulo 2. (Note that when evaluating the scalar product x · y, use the
binary form for y, that is y ∈ {(0, 0),(0, 1),(1, 0),(1, 1)} .)
11.19 Let Z =

1 0
0 −1

and H = √
1
2

1 1
1 −1

,
a) find (Z ⊗ H)(|0i ⊗ |0i) b) find (Z ⊗ H)(|0i ⊗ |1i)
c) find (Z ⊗ H)(|1i ⊗ |0i) d) find (Z ⊗ H)(|1i ⊗ |1i).
Knowing the eigenbasis of self-adjoint operators of the form A ⊗ B will be essential for the
study of quantum measurement. Consider the following example.
Example 11.8.6 The eigenbasis of the operators A ⊗ I and I ⊗ A.
Suppose A and I have matrix representations
A =

0 0
0 1 
, I =

1 0
0 1 
.
Then
A ⊗ I =


0 0 0 0
0 0 0 0
0 0 1 0
0 0 0 1


, I ⊗ A =


0 0 0 0
0 1 0 0
0 0 0 0
0 0 0 1

 .

Now the eigenvalues of A⊗I are 0 and 1. The eigenvectors corresponding to 0 are |0i⊗|0i =

1
0
0
0


and |0i ⊗ |1i =


0
1
0
0


. Those corresponding to 1 are |1i ⊗ |0i =


0
0
1
0


and
|1i ⊗ |1i =


0
0
0
1

.
The eigenvalues of I ⊗ A are also 0 and 1. The eigenvectors corresponding to 0 are
|0i ⊗ |0i =


1
0
0
0


and |1i ⊗ |0i =


0
0
1
0


. Those corresponding to 1 are |0i ⊗ |1i =


0
1
0
0


and |1i ⊗ |1i =


0
0
0
1

.
Thus the eigenbasis of both operators is |00i, |01i, |10i and |11i.238 Linear operators and their matrix representations
11.9 End-of-chapter exercises
1. Show that A : R
3 → R
3 defined by A(x1, x2, x3) = (2x1 − x2 + x3, −x2 − x3, x1)
where x1, x2, x3 ∈ R is a linear operator.
2. Show that A : R
2 → R
2 defined by A(x1, x2) = (x
2
1
, x2
2
) where x1, x2 ∈ R is not
a linear operator.
3. Show that A : R → R defined by A(x) = 2x − 1 where x ∈ R is not a linear
operator.
4. Show that A : C
2 → C
2 defined by A(z1, z2) = (2z1, z2) where z1, z2 ∈ C is a
linear operator.
5. Show that |0ih0|+|1ih1|= I, the identity operator.
6. Suppose |+i = √
1
2
(|0i + |1i) and |−i = √
1
2
(|0i − |1i). Show that
|+ih+| + |−ih−|= I.
7. Given A =

0 0
0 1 
, evaluate (A ⊗ I)|11i, that is


0 0 0 0
0 0 0 0
0 0 1 0
0 0 0 1




0
0
0
1

.
8. Given A =

0 0
0 1 
, show that
(I ⊗ A)|11i =


0 0 0 0
0 1 0 0
0 0 0 0
0 0 0 1




0
0
0
1

 =


0
0
0
1

 = |11i.
Deduce that |11i is an eigenvector of I ⊗ A. State the corresponding eigenvalue.
9. Consider the linear operator A : C
2 → C
2
, A(z) = A(z1, z2) = (z1, 0). Let an
orthonormal basis of C
2 be {e1, e2} where e1 = ( √
1
2
, √
1
2
), e2 = ( √
1
2
, − √
1
2
). With
the inner product on C
2 defined as hu, vi =
P2
i=1 u
∗
i
vi calculate:
a) Ae1 b) Ae2 c) he1, Ae1i d) he1, Ae2i
e) he2, Ae1i f) he2, Ae2i
and hence write down the matrix representation of A in the given orthonormal
basis.
10. Consider the linear operator A : C
2 → C
2
, A(z) = A(z1, z2) = (z2, z1). Let an
orthonormal basis of C
2 be {e1, e2} where e1 = ( √
1
2
, √
1
2
), e2 = ( √
1
2
, − √
1
2
). With
the inner product on C
2 defined as hu, vi =
P2
i=1 u
∗
i
vi calculate:
a) Ae1 b) Ae2 c) he1, Ae1i d) he1, Ae2i
e) he2, Ae1i f) he2, Ae2i
and hence write down the matrix representation of A in the given orthonormal
basis.End-of-chapter exercises 239
11. Let A and B be Hermitian operators.
a) Show that the commutator [A, B] = AB − BA is not Hermitian.
b) Show that i[A, B] is Hermitian.
12. In Example 11.8.5 we calculated the matrix representation of A ⊗ I ⊗ I where
A =

0 0
0 1 
and I =

1 0
0 1 
. Use this representation to confirm that |111i
is an eigenvector of A ⊗ I ⊗ I with eigenvalue 1.
13. Explore whether eigenvalues and eigenvectors are basis independent.
14. Let B =

1 0
0 0 
.
a) Find B ⊗ B.
Show that |00i =


1
0
0
0

 b)

is an eigenvector of B ⊗ B with eigenvalue 1.Part II
Foundations of quantum-gate
computation12
Introduction to Part II
12.1 Objectives
This chapter overviews some physical aspects of computation in preparation, and as a
foundation, for the presentation of the formal axioms of quantum computation that follow.
In particular:
1. the concepts of state, dynamics, observables and measurement are introduced in
both the classical and quantum domains,
2. the Stern-Gerlach quantum experiment is discussed to provide a basis for the
introduction of the qubit (the quantum equivalent of the digital bit).
12.2 Computation and physics
Computation may be mathematically defined, but it is a physical process requiring ‘ma￾chines’ for its implementation. The machines may be mechanical and simple, for example
the abacus, or of a more complex mechanical nature (for example, the mechanical Pasca￾line and Babbage machines). Modern digital computers are electro-mechanical in nature,
and based on the von Neumann architecture; they are founded on the laws of (i) classical
electro-magnetism (Maxwell’s equations) and (ii) the non-relativistic classical-mechanics of
Newton, Lagrange and Hamilton. Digital computers have proved to be immensely powerful,
they continue to dominate the world of general computation, and will occupy this position
for years to come.
12.3 Physical systems
Dynamic physical systems, including both the classical and quantum cases, incorporate:
1. state spaces,
2. time-evolution functions, and
3. ‘measurement’ processes.
In the next section, we view digital computation from this perspective which we will, later,
compare directly with the quantum case.
DOI: 10.1201/9781003264569-12 243244 Introduction to Part II
12.4 An overview of digital computation
12.4.1 Digital computer states
The state space of a digital computer comprises its set of registers; and its state, at time
t, is defined to be the data in its registers at time t.
12.4.2 Digital computer dynamics
Dynamics are defined to be the changes of state in time. A program (derived from an
algorithm) for a digital computer moves the computer through a series of states for each
valid input. When the process terminates the output is available. The changes of state,
determined by the program, are realised by classical Boolean gates.
A digital machine has only a finite number of possible states; these can therefore be
listed as
s1, . . . , sk.
It follows that the dynamic path determined by a specific coding of an algorithm, with a
given input, may be represented as a sequence of, time-ordered, machine states:
s
∗
1
, s∗
2
, · · · , s∗
p
where s
∗
i ∈ {s1, . . . , sk} for each i ∈ {1, . . . , p}. Hence there are Boolean functions
f0, . . . , fp−1 such that
x
f0 −→ s
∗
1
f1 −→ s
∗
2
f2 −→ · · ·
fp−2 −→ s
∗
p−1
fp−1 −→ s
∗
p
where x is a Boolean string that incorporates the input data, and f0 is a Boolean function
that gives rise to the initial machine state, s
∗
1
, from the program and the data provided as
input.
Equivalently, we can use the notation of function composition and write:
(fp−1 ◦ · · · ◦ f2 ◦ f1 ◦ f0)(x).
12.4.3 Digital computer ‘measurement’
In digital computation extracting the output of a computation is a trivial process. For
comparison (later) with the quantum case, we extend the functional representation of a
digital computation to include an ‘output’, or ‘measurement’, process.
We have
x
f0 −→ s
∗
1
f1 −→ s
∗
2
f2 −→ · · ·
fp−2 −→ s
∗
p−1
fp−1 −→ s
∗
p
µD −→ y
where µD is the Boolean function that ‘projects’ the contents of the registers, of the final
state s
∗
p
, comprising the desired output, y, of the computation.
That is, the complete process may be written as:
y = µD ◦ (fp−1 ◦ · · · ◦ f2 ◦ f1 ◦ f0)(x) (12.1)
The function µD is not a computation, in the sense that it does not change data in the
registers of the final state s
∗
p
; rather, it projects the contents of the registers that comprise
the output data of the computation.The emergence of quantum computation 245
The function µD is trivial, and is included here only for the purposes of comparison
with the quantum case. The equivalent quantum ‘operator’, which we denote by µQ, plays
the same role as that of µD in the digital case, i.e., it determines, by projection from the
final (quantum) state, the output of the computation. The quantum operator µQ is said to
‘measure’ the final quantum state; the quantum measurement process is, unlike the digital
case, non-trivial, as we shall see.
12.5 The emergence of quantum computation
In the 1980s, the physicist Richard Feynman expressed the view that, in order to model
the quantum world adequately, vastly more powerful computers than those likely to be
implemented using digital technology would be needed – and he suggested that computers
based on quantum physics (rather than the classical physics of the digital world) might
provide solutions.
In recent years, a number of quantum approaches to computation have evolved and
their, potential, power has been demonstrated by the development of quantum algorithms
of massively greater efficiency than known digital algorithms for the same problem. Shor’s
quantum algorithm for the factorisation of integers is regarded as the most significant to
date; it demonstrates that integers can be factorised ‘efficiently’ on a quantum-mechanical
computer. Given that many current encryption algorithms depend on the inability to fac￾torise large integers efficiently on a digital computer, Shor’s algorithm threatens the current
approach to encryption that we all depend upon. However, the threat is not immediate as,
at this time, no ‘stable’ quantum computer of the ‘size’ required has been physically realised
(i.e., engineered).
12.6 Observables and measurement
12.6.1 Observables
Observables, in both classical and quantum mechanics, correspond to ‘measurable’ proper￾ties such as mass, position, momentum, energy etc. Measurements result from experiments
designed to produce values for observables. In both the classical and quantum cases, the
values measured are real numbers.
12.6.2 Measurement
In quantum mechanics the ‘sharp end’ of the measurement device is, inevitably, of the same
‘size’ or ‘scale’ as the system under measurement. It should not therefore be too surprising
that quantum measurement may disturb the state. The bull-in-the-china-shop analogy is
helpful – the state of the china is unlikely to be the same, post-inspection by the bull!
Although such large-scale analogies impart some understanding, they do not explain the
complexities and/or subtleties of the quantum measurement process. In comparing classical
and quantum measurement we make the following observations:246 Introduction to Part II
1. In classical physics, measurement processes are usually far less intrusive and may
normally be performed with minimal, or no, change of state. Where changes of
state do occur, they are predictable and not stochastic. This is unlike the quantum
case where distinct post-measurement states, with known a priori probabilities,
can occur.
2. The measurement processes, in both the classical and quantum cases, produce
real numbers as output. The classical measurement process is deterministic –
in the sense that measuring systems prepared in the same state will produce
identical measured values for all observables (i.e., mass, momentum etc.). This is
not true in quantum mechanics; the measurement of identically prepared systems
can produce distinct outcomes – again with known a priori probabilities.
The Stern-Gerlach experiment, discussed below, illustrates the issues raised in both (1) and
(2) above. Later we also use the Stern-Gerlach experiment to introduce the concept of a
‘qubit’, this being the quantum equivalent of the bit in classical digital computing.
12.7 The Stern-Gerlach quantum experiment
The full generalities of quantum mechanics are not required for quantum computation. In
particular, vector spaces of infinite dimension and the associated spectral complexities of
operators on such spaces can be avoided. Below we identify a minimal quantum physics,
specifically the Stern-Gerlach experiment, required to underpin the axioms of quantum
computation and to illustrate the processes of quantum measurement and dynamics. The
Stern-Gerlach experiment illustrates some of the essential differences between classical and
quantum mechanics. The experiment is illustrated in Figure 12.1, which shows a stream of
quantum particles, all prepared in the same way, passing through a magnetic ‘measuring’
device. As is shown, the input particle stream splits into two distinct output streams – each
comprising 50% of the particles. This is very distinct from what would be expected if the
particles were classical in nature – specifically classical particles would emerge from the
magnetic field all displaying similar behaviour – dependent upon the nature of the field.
The quantum ‘observable’ under measurement in the Stern-Gerlach experiment is intrinsic
‘quantum spin’, and the interpretation of the experiment is that the spin state of each of
the particles in the input stream is a linear ‘superposition’ of the two states that occur
post-measurement. If we denote the output spin states by |↑i and |↓i then the spin state of
the particles in the input stream is represented quantum mechanically as the superimposed
state
1
√
2
|↑i +
1
√
2
|↓i.
This experiment also illustrates a fundamental aspect of quantum measurement –
specifically that the measurement process may change the state of the particles. We have:
1. all particles in the input stream are in the ‘pure’ quantum state 1
√
2
|↑i +
1
√
2
|↓i,
2. following measurement 50% of the particles are in the ‘pure’ state |↑i and 50% in
the ‘pure’ state |↓i.
We denote the measured values distinguishing the experimental outcomes as +1 and −1
for spin-up and spin-down respectively with corresponding post-measurement states |↑i and
|↓i.The Stern-Gerlach quantum experiment 247
incoming stream
magnet
magnet
spin-up
stream
spin-down
stream
FIGURE 12.1
A Stern-Gerlach configuration.
Hence the Stern-Gerlach measurement changes the state of each of the particles in
the input stream. This is not always the case in quantum measurement processes and in
quantum computation, it is necessary to understand the conditions under which a change-of￾state occurs under measurement – and the conditions under which it doesn’t. In addition, it
is often necessary to understand precisely what the post-measurement state is. These issues
are addressed later in the statements of the axioms for quantum computation.
It should be stressed that the states above are representative of the spin states of the
incoming and outgoing streams, and should not be confused with a complete ‘wave function’
representation, which would model additional physical properties that are not required in
an introductory treatment of quantum computation.
The Stern-Gerlach experiment (see, for example, the reference [1], p252) dates back to
the early 1920s and was one of the first experiments to demonstrate the quantum properties
of physical systems.13
Axioms for quantum computation
13.1 Objectives
The purpose of Sections 13.2, 13.3 and 13.4 is to impart some physical insight into the
formal axioms of quantum computation presented in Section 13.6. Axioms are, by definition,
unprovable; their purpose being to provide a foundation for a particular theory. To avoid
the introduction of further physics, we use the Stern-Gerlach experiment to provide support
for the current approach to information representation in quantum computation. It should
be stated that other approaches to providing support for the same axioms exist. Readers
familiar with the axioms of quantum mechanics may skip much of Sections 13.2, 13.3 and
13.4 and move straight to Section 13.6. Section 13.6 also includes two important special
cases of the general measurement axiom, these being sufficient for many of the quantum
measurements performed in later chapters.
13.2 Quantum state spaces for computation
Some understanding of the state spaces, time-evolution functions and measurement pro￾cesses of quantum mechanics is required in quantum computation. In the case of digital
computing, we have seen that the state space comprises the digital registers, changes of
state are implemented by Boolean functions and measurement is a simple projection func￾tion.
In general quantum mechanics, the state spaces are vector spaces of infinite dimension.
However, a complete understanding of the generalities of quantum mechanics is not required
for quantum computation. We begin by considering the state space sub-domain required
for computation. The considerations of the time-evolution and measurement processes will
follow in Sections 13.3 and 13.4.
For quantum computation:
1. the underlying state spaces are finite dimensional (this is a significant simplifica￾tion),
2. many of the foundations (in particular, the coverage of the current text) may be
fully understood without the introduction of so-called ‘mixed’ quantum states,
it being sufficient to consider only the pure states of quantum systems. A mixed
quantum state may, for example, relate to a gas of quantum particles where the
precise state of each individual particle cannot be known and statistical estimates
are necessary. This should not be confused with pure states that are superpositions
in a given basis. For example, the input state of the particles in the Stern-Gerlach
experiment is not a mixed state; it is a superposition of pure states which is itself
DOI: 10.1201/9781003264569-13 249250 Axioms for quantum computation
pure. The interpretation is that the particle is in both states at the same time and
there is no quantum mechanical uncertainty relating to the nature of the state.
However, it is necessary to understand the structure of the state spaces of multi-particle
quantum systems; these being the tensor product of the component systems. Hence in some
ways the quantum mechanics of the computational domain are relatively simple, but in other
ways, e.g., the requirement for multi-particle state spaces, it is not the simplest quantum
sub-domain. It is within the context of the above that the axioms for quantum computation,
presented in Section 13.6, should be viewed.
13.2.1 The 2-dimensional vector space representation of ‘spin’ and
quantum bits
The interpretation of the Stern-Gerlach experiment is that the incoming stream of particles
have two internal quantum ‘spin’ states, which we denote using Dirac ket notation as |↑i
and |↓i. Given that a fifty-fifty post-measurement outcome is obtained, the incoming spin
state is
1
√
2
|↑i +
1
√
2
|↓i
or equivalently,

1
2
1
2
|↑i +

1
2
1
2
|↓i.
The amplitudes (i.e., the multipliers) of |↑i and |↓i are equal in this case because the post￾measurement states |↑i and |↓i are equally likely for all incoming particles. Generally, in
quantum mechanics, amplitudes occurring in states such as the above need not be equal.
For example, if a sixty-forty distribution of a pair of states |ai and |bi is observed post￾measurement, then the initial state of the system under measurement would be expressed
as:

3
5
1
2
|ai +

2
5
1
2
|bi.
Hence, in general, amplitudes αa and αb in pre-measurement states of the form
αa|ai + αb|bi, where αa
2 + αb
2 = 1 (13.1)
are interpreted as follows:
• αa
2
is the probability of post-measurement state |ai occurring, and
• αb
2
is the probability of post-measurement state |bi occurring.
We have seen expressions like Equation 13.1 before. If we think of |ai and |bi as vectors in
a vector space then Equation 13.1 is a linear combination of |ai and |bi, and if |ai and |bi
are orthogonal unit vectors then any linear combination αa|ai + αb|bi, with αa
2 + αb
2 = 1,
is a unit vector.
Returning to the specific case of the Stern-Gerlach experiment, it is clear that we are
dealing with particles that have two distinct internal spin states |↑i and |↓i, and that this
has some similarity with the classical bit (binary digit) of digital computation. The bit is
the basis of information representation in classical computation. Hence, if we could entrap
(i.e., locally constrain) 2-state quantum systems, their states – e.g., |↑i, |↓i – could form a
basis for the quantum-mechanical representation of information, performing the same role
as the binary digits {0, 1} of digital computation.Quantum state spaces for computation 251
Of particular interest is that, in the quantum case, linear combinations α↑|↑i + α↓|↓i,
with α
2
↑ + α
2
↓ = 1, are valid pure states distinct from |↑i and |↓i. This generalises the
classical case where {0, 1} is closed under the algebraic operations ⊕ and ∧ (under which
{0, 1} determines a vector space over the field F2 – see Chapter 1) and is partially responsible
for the increased power of quantum computation.
In summary, the above suggests the existence of a 2-dimensional vector space of quantum
states that could, possibly, be used as the basis of an approach to computation – replacing
the digital pair {0, 1} of classical computation with the unit vectors (or ‘rays’ – see Sections
13.2.2, 13.2.4 and 13.2.5) of a, suitably defined, 2-dimensional vector space. We call such
quantum equivalents of the classical bits {0, 1} quantum bits, or qubits.
13.2.2 The case for quantum bit (qubit) representation in C
2
From the discussion of the Stern-Gerlach experiment, it is clear that
1. real-number multipliers, relating to post-measurement outcome probabilities (i.e.,
the α↑ and α↓ in α↑|↑i + α↓|↓i) can occur in the representation of pure quantum
states,
2. the suggested 2-dimensional vector space representation must have an inner prod￾uct, enabling the concepts of unit and orthogonal vectors to be defined.
That real numbers can occur implies, by a theorem of Frobenius, that the fields over which
the 2-dimensional space is defined are restricted to be either R or C. If non-commutativity
of multiplication is permitted then Frobenius’ theorem admits a further alternative – i.e.,
Hamilton’s ‘quaternions’. Jauch [1], p131, discusses these alternatives (i.e., R and the quater￾nions) to the generally accepted choice of the complex number field C and provides refer￾ences to the literature on this issue. Here, from this point, we focus on the development of
quantum theory in vector spaces over C and interpret superpositions such as
α↑|↑i + α↓|↓i
to be unit vectors in C
2
. It follows that α↑ ∈ C and α↓ ∈ C are such that |α↑|
2 + |α↓|
2 = 1
and that the superpositions are unit vectors with respect to the inner product
(z1, z2) · (w1, w2) = z1w1 + z2w2
on C
2
. (You should be aware, as we noted in Chapter 4, an alternative definition of the
inner product, in which conjugation is performed on the first of the vectors in the products
on the right-hand side, is favoured by many physicists and computer scientists). The row
vectors (1, 0),(0, 1) determine an orthonormal basis of C
2 and can be realised as |↑i = (1, 0)
and |↓i = (0, 1). Alternatively, using Dirac ket notation and column vectors, the standard
basis of C
2
is often written in the form 
1
0

= |0i, and 
0
1

= |1i.
13.2.3 Quantum bits – or qubits
In quantum computation the unit vectors or, more precisely, the rays of C
2
(see below)
replace the bits {0, 1} of digital computation and are called qubits.
13.2.4 Global phase
We note, again from the Stern-Gerlach experiment, that distinct unit vectors in C
2
can
represent the same quantum spin state. Specifically, we have a representation of the incoming252 Axioms for quantum computation
particle states as
1
√
2
|↑i +
1
√
2
|↓i
with the post-measurement probabilities of ( √
1
2
)
2 =
1
2
for both possible outcomes. Accepting
that the amplitudes α↑, α↓ are, generally, complex numbers, we note that
i
√
2
|↑i +
i
√
2
|↓i
(with i2 = −1) is, therefore, also a valid representation of the state 1
√
2
|↑i +
1
√
2
|↓i because
|
i
√
2
|
2 =
1
2
. More generally we have that
e
iθ
√
2
|↑i +
e
iθ
√
2
|↓i
for all θ ∈ R are valid unit vector representations of the same state. That is, if (z1, z2) ∈ C
2
is a unit vector then so is eiθ
(z1, z2) for any θ ∈ R, and the vectors eiθ
(z1, z2) represent the
same quantum state for all θ ∈ R.
Generally, in quantum mechanics, the vectors |ψi and z|ψi, for any non-zero z ∈ C,
represent the same pure state (Mackey [2], p76). That is, pure quantum states are ‘rays’ in
the underlying vector space and global multipliers of quantum states, (i.e., the z in z|ψi)
may be ‘ignored’; equivalently all such z may be replaced by 1 without changing the physical
properties of the state.
We note that e
iθ1
√
2
|↑i+
e
iθ2
√
2
|↓i, for θ1 6= θ2, is not on the same ray in C
2 as
1
√
2
|↑i+
1
√
2
|↓i,
i.e., local phase factors cannot be ignored.
Rays in vector spaces determine projective spaces and, to assist their understanding, a
short introduction is given below. Readers may, at least initially, omit this material and
simply regard qubits as unit vectors in C
2 – this being sufficient for much of what follows in
this text. We will, primarily, represent states as unit vectors in C
2
, but occasionally, in the
sections on quantum algorithms, global phase multipliers will arise – which, for the reasons
given above, may be ignored.
13.2.5 Projective spaces and the Bloch sphere
First we consider real projective spaces, before moving on to complex projective spaces,
which are more relevant to quantum computation.
1. Real projective spaces
The projective space of R
n+1 is the n-dimensional space of lines passing through the origin
of R
n+1. The lines are the 1-dimensional subspaces of R
n+1 and are also referred to as rays.
Consider the case n = 1 for which we obtain a 1-dimensional space of 1-dimensional
subspaces (i.e., the lines in R
2
through (0, 0)). We note that if (x, y) lies on a ray in R
2
,
then (x, y) identifies the ray and (−x, −y) lies on the same ray – see Figure 13.1.
The rays do not determine equivalence classes of points in R
2 because their intersections
are not empty – as each ray passes through the origin (0, 0) ∈ R
2
. If instead we remove the
origin from R
2
(Figure 13.2) and define, on R
2 \ {(0, 0)}, the equivalence relation
(x, y) ∼ (x
∗
, y∗
) iff (x
∗
, y∗
) = λ(x, y) for some λ ∈ R \ {0}Quantum state spaces for computation 253
x
y
(x, y)
(−x,−y)
FIGURE 13.1
The projective space of 1-dimensional subspaces (or rays) in R
2
. Points (x, y) and (−x, −y)
lie on the same ray.
x
y
FIGURE 13.2
Rays in R
2 \ {0, 0}.
then the rays, i.e., the elements, [(x, y)], of
(R
2
\ {(0, 0)})/ ∼
are such that
[(x, y)] ∩ [(x
∗
, y∗
)] = ∅ if (x, y)  (x
∗
, y∗
)
and
[(x, y)] = [(x
∗
, y∗
)] if (x, y) ∼ (x
∗
, y∗
)
and determine a partition of R
2 \ {(0, 0)} – see Figures 13.3 and 13.4.
Rays may be identified in R
2 \ {(0, 0)} by
1. any vector (x, y) on the ray – see Figure 13.3 (this is not so for rays in R
2
as then each ray passes through the origin (0, 0)). There are, clearly, infinitely
many representative points for a given ray – imposing the condition that the
representative be a unit vector reduces the options.
2. a unit vector that lies on the ray – this leads to just two choices, specifically
(x, y)
p
x
2 + y
2
and (−x, −y)
p
x
2 + y
2
where (x, y) is any point on the ray.
Hence, rays in R
2 \ {(0, 0)} may be identified by either of two unit vectors.254 Axioms for quantum computation
x
y
(x, y)
(x
∗
, y∗
)
FIGURE 13.3
(x, y) ∼ (x
∗
, y∗
).
x
y
(x
∗
, y
∗
)
(x, y)
FIGURE 13.4
(x, y) 6∼ (x
∗
, y∗
).
The relevance of this to quantum computing is that, as we have seen in Section 13.2.4,
qubits are more precisely defined as elements of the ray space
(C
2
\ {(0, 0)})/ ∼
rather than unit vectors in C
2
.
In the above, ∼ is an equivalence relation similar to that defined earlier on R
2 \ {(0, 0)}.
It is therefore of interest to investigate the space (C
2 \ {(0, 0)})/ ∼ (which is known as the
Bloch sphere). We do this below.
2. A complex projective space – the Bloch sphere
The vector space C
2
is of particular significance in quantum computation. Generally, in
quantum mechanics, the wave functions |ψi and z|ψi (for z ∈ C, z 6= 0) define the same ray
in the underlying vector space – and hence the same quantum state. The Bloch sphere (in￾troduced below) is, essentially, a ‘parametrisation’ of the quantum states (C
2 \ {(0, 0)})/ ∼;
i.e., the pure qubit states.
Up to global phase the pure states of C
2
, in an orthonormal basis {|0i, |1i}, take the
form
|ψi = α|0i + β|1i
where α, β ∈ C with |α|
2 + |β|
2 = 1. Hence for some η0, η1 ∈ [0, 2π) and r0, r1 ∈ R, with
r0 > 0 and r1 > 0, we have
|ψi = r0e
iη0
|0i + r1e
iη1
|1iQuantum state spaces for computation 255
where r
2
0 + r
2
1 = 1. (See Section 3.6 on the exponential form of a complex number.) We
write r0 = cos(θ/2) and r1 = sin(θ/2); and the equivalence of |ψi and z|ψi then gives
|ψi = cos(θ/2)eiη0
|0i + sin(θ/2)eiη1
|1i
∼ cos(θ/2)|0i + sin(θ/2)ei(η1−η0)
|1i.
Writing ϕ = η1 − η0, we have
|ψi = cos(θ/2) |0i + sin(θ/2)eiϕ
|1i
= cos(θ/2) |0i + (sin(θ/2) cos ϕ + i sin(θ/2) sin ϕ) |1i
= cos(θ/2) |0i + sin(θ/2) cos ϕ |1i + i sin(θ/2) sin ϕ |1i.
The vector
(sin(θ/2) cos ϕ, sin(θ/2) sin ϕ, cos(θ/2))
lies on the unit sphere, x
2+y
2+z
2−1 = 0, of R
3
. That is, each pure qubit state corresponds
to a point on the surface of the unit sphere centred at the origin of R
3
. This ‘parametrisation’
of the pure states of C
2
is known as the Bloch sphere depicted in Figure 13.5.
y
θ
x
|ψ
z = 1, |0
z = −1, |1
ϕ
FIGURE 13.5
The Bloch sphere, x = sin θ cos ϕ, y = sin θ sin ϕ, z = cos θ.
To assist the interpretation of the Bloch sphere representation, we evaluate the positions
of a number of pure states on the surface of the sphere. For
|ψi = cos(θ/2) |0i + sin(θ/2) cos ϕ |1i + i sin(θ/2) sin ϕ |1i
we have
• for θ = 0, the north-pole of the sphere, (0, 0, 1), is representative of the vector |ψi = |0i,
• for θ = π and ϕ = 0, the south-pole, (0, 0, −1), of the sphere, is representative of the
vector |ψi = |1i,
• for θ = π/2 and ϕ = 0, the positive x-radial vector, (1, 0, 0), of the sphere, is representative
of the vector |ψi = √
1
2
(|0i + |1i),256 Axioms for quantum computation
• for θ = π/2 and ϕ = π, the negative x-radial vector, (−1, 0, 0), of the sphere, is represen￾tative of the vector |ψi = √
1
2
(|0i − |1i),
• for θ = π/2 and ϕ = π/2, the positive y-radial vector, (0, 1, 0), of the sphere, is represen￾tative of the vector |ψi = √
1
2
(|0i + i|1i),
• for θ = π/2 and ϕ = 3π/2, the negative y-radial vector, (0, −1, 0), of the sphere, is
representative of the vector |ψi = √
1
2
(|0i − i|1i).
We note, for example, that although the states |0i and |1i are orthogonal in the vector
space C
2
, they are not orthogonal when represented in R
3 on the Bloch sphere. The rep￾resentation is therefore far from being conformal or geometric. It should be regarded as a
‘parametrisation’ of the pure quantum states of C
2
.
13.2.6 Multi-qubit state spaces
Accepting that the unit vectors of C
2
form the basis of information representation in quan￾tum computation we need, following the digital case, to define n-qubit quantum registers.
Considerations of the requirements of quantum measurement and the ability to have suffi￾cient structure to model qubit-qubit interactions appropriately, lead naturally to the conclu￾sion that the underlying vector space for n-qubit registers, or systems, is the tensor product
space of the constituent qubits, that is,
⊗
nC
2 = C
2 ⊗ C
2 ⊗ · · · C
2
, n copies.
We should note that it is not easy to justify fully this choice a priori; but it will be seen as
suitable when specific concepts, such as entanglement, are introduced in the presentation
of quantum computation to follow.
It follows from the discussion of tensor product spaces in Chapter 10 that an orthonormal
basis of C
2 ⊗ C
2
is given by
{|0i ⊗ |0i, |0i ⊗ |1i, |1i ⊗ |0i, |1i ⊗ |1i}.
For brevity we often write |xi ⊗ |yi, for x, y ∈ {0, 1}, as |xi|yi or even as |xyi. So, for
example, the state |0i ⊗ |0i will be written as simply |00i and the orthonormal basis of
C
2 ⊗ C
2
is then
{|00i, |01i, |10i, |11i}.
On occasions, especially when n is large, the basis may be written using decimal equivalents
{|0i, |1i, |2i, |3i}.
Clearly, care must be taken to read the specific context under consideration.
Similarly, orthonormal bases of ⊗nC
2 may be constructed. It follows that the dimension
of the vector space ⊗nC
2
is 2n with basis
{|00 · · · 0i, . . . , |11 · · · 1i}.Quantum observables and measurement for computation 257
13.3 Quantum observables and measurement for computation
Appealing again to the Stern-Gerlach experiment, we illustrate the quantum model of the
measurement of observables on quantum states. We note that the self-adjoint operator
A : C
2 → C
2 defined by
A =

1 0
0 −1

has the two distinct eigenvalues of 1 and −1 with eigenvectors, respectively
(1, 0)T = |↑i = |0i
and
(0, 1)T = |↓i = |1i.
Hence, the spectral properties of the self-adjoint operator A are representative of the two
possible outcomes of Stern-Gerlach experiment, i.e.,
1. the real value 1 measured with post-measurement state |↑i,
2. the real value −1 measured with post-measurement state |↓i.
Specifically, the operator A models the potential outcomes of the measurement of the ‘in￾trinsic spin’ observable on the input state:
|φi =
1
√
2
|↑i +
1
√
2
|↓i.
We recall that the pure state |φi is a linear superposition of the two possible outcomes,
incorporating (in the amplitudes) the probabilities of each outcome. The interpretation
being that the particle is simultaneously in both states prior to measurement.
We stress that the self-adjoint operator A is not applied to the input state |φi, it simply
models the possible outputs of
1. the measured value (1 or −1),
2. the corresponding post-measurement state (|0i or |1i respectively)
following a measurement process.
This generalises to the following: to each physically measurable observable on a quan￾tum system there corresponds a self-adjoint operator, on the underlying vector space, the
eigenvalue-eigenvector pairs of which relate to the measurable values of the observable and
the resulting post-measurement state. The precise nature of the post-measurement state
depends on the degeneracy of the related eigenvalue - the full details of this are given in
Axiom 3 of Section 13.6 below.
13.4 Quantum dynamics for computation
The following observations may be used to support the axiom of unitary evolution of quan￾tum states:258 Axioms for quantum computation
1. given that all quantum states are represented by unit vectors, state-to-state tran￾sitions must be unitary (because linear unit vector to unit vector transformations
are unitary – see Section 11.7),
2. for readers, more familiar with general quantum mechanics, we note that the
dynamics of non-relativistic quantum systems are determined by Schr¨odinger’s
famous equation:
i~
∂|ψti
∂t = H|ψti
where H is a self-adjoint operator on the underlying vector space of states V ,
|ψti ∈ V , ~ is Planck’s constant, i = (−1) 1
2 and t is the time variable. The
operator H is representative of the energy observable. Schr¨odinger’s equation
may be integrated, to the form:
|ψti = Ut|ψ0i,
where Ut is a unitary operator on V and |ψ0i is the system state at t = 0. The
unitary operator may be expressed, as a function of the quantum Hamiltonian
H, as Ut = e− it
~ H.
From (1) and (2) it follows that unitary operators play the role, in quantum com￾putation, that Boolean functions play in digital computation; i.e., they implement
the steps in the computation from input to output.
13.5 Orthogonal projection in a complex inner product space
Before formally summarising the axioms of quantum computation, we take a moment to
discuss the concept of orthogonal projection in a complex inner product space. We begin
with, for the purposes of ease of visualisation, the real vector space R
3 with the usual inner
product of two vectors (x, y, z) and (x
∗
, y∗
, z∗
) given by
h(x, y, z),(x
∗
, y∗
, z∗
)i = xx∗ + yy∗ + zz∗
and standard orthonormal basis
i = (1, 0, 0), j = (0, 1, 0), k = (0, 0, 1).
It is clear from Figure 13.6 that the orthogonal projection of the point (x, y, z) ∈ R
3 onto
the xy-plane is (x, y, 0); i.e., the dashed ‘projection’ line from the point (x, y, z) to the point
(x, y, 0) is parallel to the z-axis and orthogonal to the xy-plane. Denoting the projected
vector by Pxy(x, y, z), mathematically we have
Pxy(x, y, z) = hi,(x, y, z)ii + hj,(x, y, z)ij.
= (x, y, 0).
We note that Pxy is a linear operator on R
3 with the property Pxy
2 = Pxy.
More generally (and in standard rather than Dirac notation): let φ0, . . . , φn be an or￾thonormal basis of an (n+1)-dimensional complex inner product space H with inner product
h,i. Any subset {χ1, . . . , χk} of k elements of {φ0, . . . , φn} clearly defines a k-dimensional
subspace of H, which we denote by Hχ.A summary of the axioms for quantum computation 259
x
y
z
(x, y, z)
(x, y, 0)
FIGURE 13.6
Orthogonal projection in R
3
.
For any ψ ∈ H the orthogonal projection, Pχψ, of ψ onto the subspace Hχ is, generalising
the R
3
case, given by
Pχψ = hχ1, ψiχ1 + · · · + hχk, ψiχk.
13.6 A summary of the axioms for quantum computation
From the above we state the following axioms, or postulates, for quantum computation.
Axiom 1 The underlying vector space of an n-qubit quantum computer is the n-fold tensor￾product space
⊗
nC
2
and the pure states are the rays of ⊗nC
2
, i.e., elements of the space ⊗nC
2/ ∼.
Axiom 2 The observables of a quantum system with underlying state space ⊗nC
2
corre￾spond to the self-adjoint operators on ⊗nC
2
.
Axiom 3 Following the measurement of an observable O on the state |ψi ∈ ⊗nC
2
, the
post-measurement state is the normalisation of the vector P|ψi, where P is the orthogonal
projection of |ψi onto the subspace of ⊗nC
2
generated by the eigenstates of O that
1. occur in the linear-superposition of |ψi, and
2. are compatible with the outcome of the measurement of O.
The probability of the outcome obtained is kP|ψik2
.
Axiom 4 Unitary operators implement the computational steps in quantum computation.
It should be noted that
1. Axiom 3 is due to von Neumann and L¨uders, and relates to the measurement of
general pure quantum states. For readers with no prior knowledge of quantum
mechanics, it is the most difficult of the axioms to understand. It is therefore
helpful to consider two special cases:260 Axioms for quantum computation
(i) Where the eigenvalues of the associated self-adjoint operator are non￾degenerate – i.e., to each eigenvalue of the operator there is only one eigendi￾rection. In this case, the measurement axiom simplifies to Axiom 3.1, see Section
13.7 below.
(ii) Where the state is an eigenstate of the observable to be measured, in which
case the measurement axiom simplifies to Axiom 3.2, see Section 13.7 below.
2. A measurement is not a computational step in a quantum computation – it is a
physical process modelled by a self-adjoint operator on the underlying quantum
state space.
3. It follows from Axiom 4 that quantum computation is reversible – this is not true
of the current implementations of digital computation.
4. Quantum measurement processes are not reversible.
13.7 Special cases of the measurement axiom
Case 1: In the case where an observable O is non-degenerate, Axiom 3 may be stated as
Axiom 3.1 Following the measurement of O on the quantum state
|χi = α0|φ0i + α1|φ1i + · · · + αn|φni; where Xn
i=0
|αi
|
2 = 1
in an (n + 1)-dimensional complex inner product space (expressed in an orthonormal basis
{|φ0i, . . . |φni} determined by O) the post-measurement state is one of the eigenvectors
{|φ0i, . . . , |φni}
of O, and the possible measured values of O are its eigenvalues
{λ0, . . . , λn}.
When λi
is measured for O, the post-measurement state is |φii and this occurs with prob￾ability |αi
|
2
.
Case 2: In the case where the state, |ψi, to be measured is an eigenstate of the observable
O (which may be degenerate) we have
Axiom 3.2 Following the measurement of O on the eigenstate |ψi, the measured value is
the eigenvalue corresponding to |ψi and the post-measurement state is |ψi.
13.8 A formal comparison with digital computation
We noted in Section 12.4.3 that digital computations may be expressed functionally as
y = µD ◦ (fp−1 ◦ · · · ◦ f2 ◦ f1 ◦ f0)(x) (D)Gates 261
and, from the axioms above for quantum computing, we may now express quantum com￾putations in the form
|ysi = (Uq−1 · · ·U2 U1 U0)|xi
where |xi and |ysi are the initial and final quantum states of the computation and
U0, . . . , Uq−1 are unitary operators. The inclusion of a quantum measurement process, µQ,
leads to
y = µQ|ysi = µQ(Uq−1 · · ·U2 U1 U0)|xi
or
y = µQ(Uq−1 · · ·U2 U1 U0)|xi (Q)
where y is the output, in real numbers, of the quantum computation.
Relation (Q) is structurally similar to relation (D) for the digital case. We should how￾ever note that the quantum measurement process, µQ, is not an operator on the quantum
state vector; it is a physical measurement process, modelled by an appropriate self-adjoint
operator on the underlying quantum state space (refer to quantum Axioms 3, 3.1 and 3.2
above).
13.9 Gates
In both digital and quantum computation, fundamental computational steps are often re￾ferred to as ‘gates’. In the digital case, gates are implemented by (non-invertible) Boolean
functions and, by Axiom 4, it follows that in the quantum case gates are implemented by
unitary operators – which are invertible. Hence digital computation is not invertible, i.e.,
the input cannot be determined from the output, but in the quantum case this is not so.
We will meet invertible digital gates later – when we consider the quantum emulation of
digital computations. However, current digital computers are not based on invertible digital
technology.
Exercises
13.1 Discuss the identity operator, which is self-adjoint, as an observable on the quan￾tum states of ⊗nC
2
.
13.2 Let A and B be self-adjoint operators on ⊗nC
2 with identical non-degenerate
eigenvectors. Show that the measurement of A followed by the measurement of
B, on any state, is equivalent, in the sense that the measured values obtained are
identical, to the measurement of B followed by the measurement of A.14
Quantum measurement 1
14.1 Objectives
In this chapter, the simplest quantum measurement situation, i.e., where the state is an
eigenvector of the observable to be measured, is considered. The states are therefore non￾superimposed and measurement Axiom 3.2 of Section 13.7 in the previous chapter is appli￾cable. We consider states in C
2
, C
2 ⊗ C
2 and C
2 ⊗ C
2 ⊗ C
2
. The generalisation to ⊗nC
2
is
then clear.
14.2 Measurement using Axiom 3.2
The applications of Axiom 3.2, discussed below, are applied in Chapters 15 and 17 to the
quantum emulation of familiar digital gates (Chapter 15) and, more generally, to arbitrary
Boolean functions (Chapter 17).
14.2.1 Measurement of non-superimposed states in C
2
Recall that a superimposed state in C
2
takes the form |ψi = α0|0i+α1|1i where α0, α1 ∈ C.
But for non-superimposed states in C
2
there are just two states to measure, i.e., |ψi = |0i
and |ψi = |1i. Consider the observable A, that is, the self-adjoint operator, with matrix
representation
A =

0 0
0 1 
.
The eigenvalues of A are 0 and 1 with eigenvectors |0i = (1, 0)T and |1i = (0, 1)T
respec￾tively. Hence, measurement Axiom 3.2 is applicable and it follows that if A is measured on
the state
|ψi = |0i
then, with probability 1, the real value of 0 will be obtained for A and the post-measurement
state |0i will result. Similarly measuring A on the state
|ψi = |1i
results, with probability 1, in the value of 1 for A and the post-measurement state |1i will
result. Hence in this simple case, quantum measurement is deterministic.
We note that because 
0 0
0 1 
=

0
1

￾
0 1 
the operator A can be expressed,
in Dirac notation, as
A = |1ih1|
DOI: 10.1201/9781003264569-14 26264 Quantum measurement 1
and we have
A|0i = |1ih1|0i
= 0|0i
and
A|1i = |1ih1|1i
= |1i
= 1|1i.
We do not concern ourselves with the physical realisation of measurement; the underlying
assumption of quantum mechanics is that if A is self-adjoint, then there is some physical
means of implementing the associated observable.
14.2.2 Measurement of non-superimposed states in C
2 ⊗ C
2
Here we measure the states |xi|yi ∈ C
2 ⊗ C
2
, i.e., states without superposition; specifically
the four states |0i|0i, |0i|1i, |1i|0i and |1i|1i of C
2 ⊗ C
2
.
Again we apply Axiom 3.2 and, later, illustrate its application to the quantum cnot
function and the Peres half-adder. To understand the quantum cnot and half-adder gates
it is necessary to measure simple 2-qubit quantum states. The measurement of ‘simple’
n-qubit states follows by generalisation of the 2-qubit case.
If A and B are self-adjoint operators on C
2
the operator A ⊗ B : C
2 ⊗ C
2 → C
2 ⊗ C
2
defined by (A ⊗ B)(|xi ⊗ |yi) = A|xi ⊗ B|yi is self-adjoint.
Two qubit states are elements of the tensor product space C
2 ⊗ C
2
. We measure an
observable A on the first qubit using the self-adjoint operator A ⊗ I on C
2 ⊗ C
2 and,
similarly, I ⊗ A, is used to measure the second qubit. Here, I denotes the identity operator
on C
2 – which is self-adjoint.
We will measure the quantum cnot output using the operators A ⊗ I and I ⊗ A where
A is as defined earlier. That is,
A =

0 0
0 1 
.
It follows that
A ⊗ I =


0 0 0 0
0 0 0 0
0 0 1 0
0 0 0 1


and
I ⊗ A =


0 0 0 0
0 1 0 0
0 0 0 0
0 0 0 1

 .
We also have
|0i ⊗ |0i =


1
0
0
0

 , |0i ⊗ |1i =


0
1
0
0

 , |1i ⊗ |0i =


0
0
1
0

 , |1i ⊗ |1i =


0
0
0
1

 .
Hence,Measurement using Axiom 3.2 265
Proposition 14.1
(i) The eigenvalues of the operator A⊗I are 0 and 1. The eigenvectors with respect to 0
are |0i⊗|0i and |0i⊗|1i and the eigenvectors with respect to 1 are |1i⊗|0i and |1i⊗|1i.
(ii) The eigenvalues of the operator I ⊗ A are 0 and 1. The eigenvectors with respect to
0 are |0i ⊗ |0i and |1i ⊗ |0i and those with respect to 1 are |0i ⊗ |1i and |1i ⊗ |1i.
Measuring the first qubit of the states |0i|0i, . . . , |1i|1i:
We measure A on the first qubit of the simple (i.e., non-superimposed) state
|ψi = |xi|yi where x, y ∈ B
using the operator A ⊗ I. From Proposition 14.1 and the quantum measurement postulate,
we have
1. If |ψi = |0i|0i then, because A ⊗ I|0i|0i = 0|0i|0i, 0 will be measured for A on
|ψi with probability 1 and the post-measurement state will be |0i|0i.
2. If |ψi = |0i|1i then, because A ⊗ I|0i|1i = 0 |0i|1i, 0 will be measured for A on
|ψi with probability 1 and the post-measurement state will be |0i|1i.
3. If |ψi = |1i|0i then, because A ⊗ I|1i|0i = 1 |1i|0i, 1 will be measured for A on
|ψi with probability 1 and the post-measurement state will be |1i|0i.
4. If |ψi = |1i|1i then, because A ⊗ I|1i|1i = 1 |1i|1i, 1 will be measured for A on
|ψi with probability 1 and the post-measurement state will be |1i|1i.
Measuring the second qubit of the states |0i|0i, . . . , |1i|1i:
Measuring A on the second qubit of |ψi = |xi|yi, for x, y ∈ B, using the operator I ⊗ A we
have
1. If |ψi = |0i|0i then, because I ⊗ A|0i|0i = 0 |0i|0i, 0 will be measured for A on
|ψi with probability 1 and the post-measurement state will be |0i|0i.
2. If |ψi = |0i|1i then, because I ⊗ A|0i|1i = 1 |0i|1i, 1 will be measured for A on
|ψi with probability 1 and the post-measurement state will be |0i|1i.
3. If |ψi = |1i|0i then, because I ⊗ A|1i|0i = 0 |1i|0i, 0 will be measured for A on
|ψi with probability 1 and the post-measurement state will be |1i|0i.
4. If |ψi = |1i|1i then because I ⊗ A|1i|1i = 1 |1i|1i, 1 will be measured for A on
|ψi with probability 1 and the post-measurement state will be |1i|1i.
As in Section14.2.1 we note that the above measurement outcomes on non-superimposed
quantum states are deterministic.
14.2.3 Measurement of non-superimposed states in C
2 ⊗ C
2 ⊗ C
2
We first note that Proposition 14.1 is easily generalised to the observables A ⊗ I ⊗ I,
I ⊗ A ⊗ I and I ⊗ I ⊗ A for measuring 3-qubit states of the form |xi|yi|zi. We leave the
generalisation of Proposition 14.1 and the completion of the below as exercises. From the
quantum measurement postulate, we have
Measuring the first qubit of the states |0i|0i|0i, . . . , |1i|1i|1i:
Measuring A on the first qubit of the simple (i.e., non-superimposed) state
|ψi = |xi|yi|zi where x, y, z ∈ B
using the operator A ⊗ I ⊗ I, we have266 Quantum measurement 1
• If |ψi = |0i|0i|0i then, because A ⊗ I ⊗ I|0i|0i|0i = 0|0i|0i|0i, 0 will be measured for A
with probability 1 and the post-measurement state will be |0i|0i|0i.
• If |ψi = |0i|0i|1i then, because A ⊗ I ⊗ I|0i|0i|1i = 0|0i|0i|1i, 0 will be measured for A
with probability 1 and the post-measurement state will be |0i|0i|1i.
.
.
.
• If |ψi = |1i|1i|1i then, because A ⊗ I ⊗ I|1i|1i|1i = 1|1i|1i|1i, 1 will be measured for A
with probability 1 and the post-measurement state will be |1i|1i|1i.
Measurement of A on the 2nd and 3rd qubits of states of the form |xi|yi|zi is left as an
exercise. The measurement of simple, i.e., non-superimposed, n-qubit states is the obvious
generalisation of the cases n = 1, 2 and 3 discussed above.
Exercises
14.1 For |ψi = |xi|yi|zi where x, y, z ∈ B use the operator A ⊗ I ⊗ I to perform
measurement of A on the first qubit. In each case, give the measured value, its
probability and the post-measurement state.
14.2 Use the operators I ⊗ A ⊗ I and I ⊗ I ⊗ A to measure the 2nd and 3rd qubits
of states of the form |ψi = |xi|yi|zi where x, y, z ∈ B. In each case, give the
measured value, its probability and the post-measurement state.15
Quantum information processing 1: the quantum
emulation of familiar invertible digital gates
15.1 Objectives
Using Section 14.2 we are now able to define quantum equivalents of a number of familiar
digital gates. It will be seen that the quantum gates, with appropriately restricted state
vector inputs, produce exactly the same numerical outputs as their digital counterparts.
It follows from the axioms of Section 13.6 that quantum computation is invertible (or
reversible). Hence, although current digital technology is not based on invertible Boolean
functions, in order to emulate digital computation quantum mechanically, it is necessary
to first construct invertible representations of Boolean functions. In this chapter we con￾sider only some of the more familiar examples – up to and including the Peres half-adder.
In Chapter 17, we show how to construct invertible representations of arbitrary Boolean
functions and discuss their quantum emulations.
15.2 On the graphical representation of quantum gates
As in the digital case, quantum-gate computation is based on fundamental gates. Having
discussed quantum states and their measurement, we can now consider the transformation of
quantum states, by ‘gates’, for computational purposes. Some quantum gates have parallels
in the digital domain – and it is these that we discuss initially. Quantum gates without
digital equivalents are discussed later.
In the graphical representation of quantum gates and circuits, the horizontal lines (or
‘wires’) indicate the time-ordered application of the unitary gates to the input qubits. There
are no physical wires. In the digital case the wires are physical – in addition to indicating
the temporal-ordering of application. Wired representations of gates are suitable and un￾ambiguous in the digital case. However, in the quantum case so-called ‘entangled states’,
which we discuss later, can occur, and additional features are required in the diagrams to
represent them.
DOI: 10.1201/9781003264569-15 267268 Quantum information processing 1
15.3 A 1-bit/qubit gate
15.3.1 The digital not gate
As was described in detail in Section 2.4, the digital not gate is a function notD : B → B
which may be specified by
Digital notD:
Input : x ∈ B
Output : 1 ⊕ x
i.e., notD(x) = 1 ⊕ x = x. Clearly, on input 0 the gate produces 1, and on input 1 the
output is 0. As noted in Chapter 2 this function is bijective and hence is invertible.
15.3.2 The quantum not gate, notQ, on BC2 = {|xi : x ∈ B}
Because the digital gate notD is invertible, a quantum equivalent, denoted notQ, is easy to
construct. Denote the basis states |0i and |1i of C
2 by BC2 = {|xi : x ∈ B}. The notQ gate
may be specified, initially, on the pair of basis vectors by
Quantum not, notQ:
Input : |xi, x ∈ B
Output (quantum) : |1 ⊕ xi
Output (post-µQ) : 1 ⊕ x
The quantum not gate, notQ, is defined, on the basis states of C
2
, by:
notQ|xi = |1 ⊕ xi, where x ∈ {0, 1}.
We have notQ|0i = |1i and notQ|1i = |0i and the gate may be represented as the ‘wired’
circuit shown in Figure 15.1.
|x notQ |1 ⊕ x
FIGURE 15.1
The 1-qubit quantum not gate, notQ, on {|0i, |1i}, i.e., x ∈ B.
Recall that the eigenvalue-eigenvector pairs of the observable A = |1ih1| are 0, |0i and 1, |1i.
Hence, measuring notQ with the observable A = |1ih1|, we see that:
A|1 ⊕ xi = |1ih1|1 ⊕ xi
=
(
|1ih1|1i = 1|1i when x = 0
|1ih1|0i = 0|0i when x = 1
i.e., following Section 14.2.1, where the measurement of the simple states |0i and |1i is
discussed, we have:
• when x = 0: 1 will be measured and the post-measurement state will be |1i,
• when x = 1: 0 will be measured and the post-measurement state will be |0i.2-bit/qubit gates 269
Hence,
• on the states |0i and |1i the output is deterministic and the quantum gate performs in
exactly the same way as its digital counterpart.
However, in the quantum case, the more general (superimposed) input states:
α0|0i + α1|1i
are valid. We discuss this more general case in Section 16.3.
15.4 2-bit/qubit gates
We consider two digital cnot or xor gates labelled Digital 1 and Digital 2. The first is not
invertible whereas the second is.
15.4.1 The non-invertible digital cnot, or xor, gate
As described in detail in Section 2.5, the digital cnot gate may be specified in the following
way:
Digital 1 (classical, non-invertible):
Input: (x, y) ∈ B
2
Output: x ⊕ y ∈ B
and is usually represented diagrammatically as shown in Figure 15.2.
x
y
x ⊕ y
FIGURE 15.2
The classical Boolean cnot gate.
We have
Input (x, y) Output x ⊕ y
(0, 0) 0
(0, 1) 1
(1, 0) 1
(1, 1) 0
The gate is clearly the Boolean function f6(x, y) = x ⊕ y (see Table 2.13); and given that
it is not invertible we seek an invertible representation from which a quantum equivalent
gate may be defined. In Chapter 17 we consider a general technique for constructing in￾vertible representations of non-invertible Boolean functions. Below we introduce Feynman’s
invertible form of f6.
15.4.2 The invertible digital cnot, or Feynman FD, gate
The Feynman gate, FD : B
2 → B
2
, first introduced in Section 2.5, is an invertible represen￾tation of the digital cnot gate and is specified by:270 Quantum information processing 1
Digital 2 (Feynman, invertible):
Input: (x, y) ∈ B
2
Output: (x, y ⊕ x) ∈ B
2
Functionally we have
FD(x, y) = (x, x ⊕ y)
and
Input (x, y) Output (x, x ⊕ y)
(0, 0) (0, 0)
(0, 1) (0, 1)
(1, 0) (1, 1)
(1, 1) (1, 0)
Observe that FD is bijective and hence is invertible. It may be represented graphically in
either of the two ways shown in Figure 15.3. It can be shown that the Feynman gate FD is
self-inverse (see Exercises).
x x
y
x
x ⊕ y
x
y
x ⊕ y
FIGURE 15.3
Two graphical representations of the Feynman invertible cnot gate.
The Feynman gate above is, essentially, the only invertible digital gate with 2-inputs and
2-outputs; it is this form of the cnot gate that may be used to define a quantum equivalent.
15.4.3 The quantum cnot, or Feynman FQ, gate on
B⊗2C2 = {|xi|yi : x, y ∈ B}
The invertible digital gate, FD, enables a quantum equivalent to be defined on the basis
states
B⊗2C2 = {|0i|0i, |0i|1i, |1i|0i, |1i|1i}
of C
2 ⊗ C
2 by:
Quantum 1:
Input: |xi ⊗ |yi ∈ B⊗2C2
Output (quantum): |xi|y ⊕ xi ∈ B⊗2C2
Output (post-measurement of 2nd qubit): y ⊕ x ∈ B
or
Quantum 2:
Input: |xi ⊗ |yi ∈ B⊗2C2
Output (quantum): |xi|y ⊕ xi ∈ B⊗2C2
Output (post-measurement of 1st and 2nd qubits): (x, y ⊕ x) ∈ B
2
.
These specifications may be seen as differing, in both the digital and quantum cases, only
in the measurement processes. For Quantum 1 only the second output qubit is measured,3-bit/qubit gates 271
whereas for Quantum 2 both the first and second qubits are measured. This gate is the
function FQ : B⊗2C2 → B⊗2C2 defined by
FQ(|xi|yi) = |xi|y ⊕ xi
and represented diagrammatically as shown in Figure 15.4.
|x
|y ⊕ x
|x
|y
FIGURE 15.4
The quantum cnot gate: input |xi ⊗ |yi, output |xi ⊗ |y ⊕ xi.
We have
Input |xi ⊗ |yi Output |xi ⊗ |y ⊕ xi µQ
|0i ⊗ |0i |0i ⊗ |0i (0, 0)
|0i ⊗ |1i |0i ⊗ |1i (0, 1)
|1i ⊗ |0i |1i ⊗ |1i (1, 1)
|1i ⊗ |1i |1i ⊗ |0i (1, 0)
In the above, µQ denotes the measurement of A = |1ih1| on qubits 1 and 2, using the
operators A ⊗ I and I ⊗ A respectively. The measurement outputs, above, follow from
the full discussion of the measurement of such output states in Section 14.2.2. Clearly the
outputs of FQ, on the domain B⊗2C2 , are precisely those of the equivalent digital gate FD.
For Quantum 2 we can write
(x, y ⊕ x) = µQ ◦ FQ(|xi|yi)
where µQ denotes a quantum measurement process.
15.5 3-bit/qubit gates
15.5.1 The digital Toffoli (or ccnot) gate
In the sense that the Feynman digital gate, FD, is an invertible form of the digital function
f6(x, y) = y ⊕x, the Toffoli gate TD : B
3 → B
3
, first introduced in Section 2.5.3 and defined
by
TD(x, y, z) = (x, y, z ⊕ (x ∧ y)),
provides an invertible representation of the digital gate f1(x, y) = x ∧ y (see Table 2.13);
i.e., we have
TD(x, y, 0) = (x, y, x ∧ y) = (x, y, f1(x, y)).
TD is often represented diagrammatically as shown in Figure 15.5. It can be shown that the
digital Toffoli gate is self-inverse (see Exercises).
With z = 0 the Toffoli gate gives the ∧ function, and with x ∧ y = 1 it gives the not
function, i.e., we have TD(1, 1, z) = (1, 1, z). We have, as a consequence, Lemma 15.1:272 Quantum information processing 1
x
y
z
x
y
z ⊕ (x ∧ y)
FIGURE 15.5
The digital Toffoli 3-bit, invertible gate.
Lemma 15.1 The Toffoli function defines a complete (or universal) set for the generation
of invertible (or reversible) versions of the Boolean functions f : B
2 → B.
Hence, the Toffoli function can perform any computation on a digital computer in a re￾versible manner. We also have
TD(x, y, 1) = (x, y, x ∧ y) = (x, y, f1(x, y))
and
TD(x, 1, 0) = (x, 1, x) or TD(1, x, 0) = (1, x, x)
i.e., the Toffoli function can copy. Further, we note TD(x, x, 0) = (x, x, x).
15.5.2 The quantum Toffoli (or ccnot) gate on
B⊗3C2 = {|xi|yi|zi : x, y, z ∈ B}
The invertible digital gate TD enables a quantum equivalent, denoted TQ, to be constructed;
we define TQ : B⊗3C2 → B⊗3C2 to be
TQ(|xi|yi|zi) = |xi|yi|z ⊕ (x ∧ y)i
and represent it diagrammatically as shown in Figure 15.6.
|x
|y
|z |z ⊕ (x ∧ y)
|x
|y
FIGURE 15.6
The quantum Toffoli 3-bit, invertible gate on B⊗3C2 .
We have
Input |xi|yi|zi Output |xi|yi|z ⊕ (x ∧ y)i µQ
|0i|0i|0i |0i|0i|0i (0, 0, 0)
|0i|0i|1i |0i|0i|1i (0, 0, 1)
|0i|1i|0i |0i|1i|0i (0, 1, 0)
|0i|1i|1i |0i|1i|1i (0, 1, 1)
|1i|0i|0i |1i|0i|0i (1, 0, 0)
|1i|0i|1i |1i|0i|1i (1, 0, 1)
|1i|1i|0i |1i|1i|1i (1, 1, 1)
|1i|1i|1i |1i|1i|0i (1, 1, 0)3-bit/qubit gates 273
where the measurement outputs of the final column follow from the discussion of the mea￾surement of such output states in Section 14.2.3. Clearly the outputs of TQ, on the domain
B⊗3C2 are precisely those of the equivalent digital gate TD. Having constructed a quantum
equivalent of the digital Toffoli gate TD we can conclude, from Lemma 15.1, that:
• all digital computations may be performed on a quantum computer.
15.5.3 The digital Peres (invertible half-adder) gate
The digital Peres gate, PD : B
3 → B
3
, defined by
PD(x, y, z) = (x, y ⊕ x, z ⊕ (x ∧ y))
is an example of a 3-input invertible Boolean gate and is an invertible representation of the
non-invertible digital half-adder, (s, c) : B
2 → B
2
, defined by:
(s, c)(x, y) = (x ⊕ y, x ∧ y).
(Here, the s and c refer to the sum and carry digits of the half-adder). Note that PD may
be represented as shown in Figure 15.7.
x
y
z
x
y ⊕ x
z ⊕ (x ∧ y)
FIGURE 15.7
Circuit for the 3-bit (invertible) Peres half-adder.
The Peres gate PD is invertible but, unlike FD and TD, it is not self-inverse. The Peres gate
may be viewed as a product of the digital Toffoli and Feynman gates. We have:
PD(x, y, z) = FD × i ◦ TD(x, y, z)
= FD × i(x, y, z ⊕ (x ∧ y))
= (FD(x, y), z ⊕ (x ∧ y))
= (x, x ⊕ y, z ⊕ (x ∧ y)).
15.5.4 The quantum Peres gate on B⊗3C2 = {|xi|yi|zi : x, y, z ∈ B}
The invertible digital half-adder, PD, enables a quantum half-adder to be constructed. We
define, on B⊗3C2 , the function
PQ(|xi|yi|zi) = |xi|y ⊕ xi|z ⊕ (x ∧ y)i
which is represented diagrammatically as in Figure 15.8.
The quantum Peres gate is the product of two operators on B⊗3C2 : the quantum Toffoli
U1 = TQ and the operator U2 = FQ ⊗ I, that is,
PQ(|xi|yi|zi) = |xi|y ⊕ xi|z ⊕ x ∧ yi
= FQ ⊗ I TQ|xi|yi|zi
= U2 U1|xi|yi|zi274 Quantum information processing 1
|x
|y
|z
|x
|y ⊕ x
|z ⊕ (x ∧ y)
FIGURE 15.8
Circuit for the quantum Peres half-adder.
and
(x, y ⊕ x, z ⊕ x ∧ y) = µQ ◦ FQ ⊗ i TQ|xi|yi|zi
= µQ ◦ U2 U1|xi|yi|zi.
which should be compared with the digital equivalent of the previous sub-section. We have:
Input |xi|yi|zi Output |xi|y ⊕ xi|z ⊕ (x ∧ y)i µQ
|0i|0i|0i |0i|0i|0i (0, 0, 0)
|0i|0i|1i |0i|0i|1i (0, 0, 1)
|0i|1i|0i |0i|1i|0i (0, 1, 0)
|0i|1i|1i |0i|1i|1i (0, 1, 1)
|1i|0i|0i |1i|1i|0i (1, 1, 0)
|1i|0i|1i |1i|1i|1i (1, 1, 1)
|1i|1i|0i |1i|0i|1i (1, 0, 1)
|1i|1i|1i |1i|0i|0i (1, 0, 0)
The real output, in the quantum case, follows by measuring the output qubits in the way
described in Section 14.2.3; clearly the outputs of PQ, on the domain B⊗3C2 , are precisely
those of the equivalent digital gate PD.
Exercises
15.1 Show that the digital Feynman gate FD is self-inverse i.e., that FD
−1 = FD.
15.2 Show that the digital Toffoli gate TD is self-inverse; i.e., we have T
−1
D = TD.
15.3 Show that the inverse of the Peres gate is:
P
−1
D (x, y, z) = (x, x ⊕ y, z ⊕ (x ∧ y)).16
Unitary extensions of the gates notQ, FQ, TQ and
PQ: more general quantum inputs
16.1 Objectives
In Chapter 15, quantum emulations of some familiar invertible digital gates were discussed;
specifically the not, cnot, Toffoli and Peres gates, each of which is significant in quantum
computation, were considered. The emulations were defined on basis elements only – i.e., on
BC2 or BC2⊗C2 or BC2⊗C2⊗C2 etc. On these restricted inputs the quantum emulations were
seen to produce outputs identical to their digital equivalents. Unlike the digital gates the
quantum emulations may be extended beyond the basis sets to the entire tensor product
spaces so defined – i.e., to C
2
, C
2 ⊗ C
2
, C
2 ⊗ C
2 ⊗ C
2
etc. Such generalised inputs to the
quantum gates lead to more general quantum outputs to which the simple deterministic
quantum measurement processes of Chapter 15 are not applicable. The quantum gates
defined on the extended domains are shown to be unitary, and are denoted in the same
way as their restrictions to the basis vectors. We first consider a sufficient condition for an
operator to be unitary; it has applications in our discussions below and in Chapter 17.
16.2 A lemma on unitary operators
Let V be an n-dimensional vector space over C with inner product h,i and let b1, . . . , bn be
an orthonormal basis of V . We have
Lemma 16.1 If T : V → V is a linear operator on V and T permutes the orthonormal
basis b1, . . . , bn of V , then T is unitary on V ; i.e., kT vk = kvk for all v ∈ V , equivalently
hT v, T vi = hv, vi for all v ∈ V .
Proof
For v = α1b1 + . . . + αnbn ∈ V we have
T v = α1T b1 + . . . + αnT bn
= α1bπ(1) + . . . + αnbπ(n)
for some π ∈ Sn.
Here Sn is the permutation group of n elements (see Section 8.3). Further
kvk
2 = kα1b1 + . . . + αnbnk
2 = hα1b1 + . . . + αnbn, α1b1 + . . . + αnbni
= |α1|
2 + . . . + |αn|
2
DOI: 10.1201/9781003264569-16 275276 Unitary extensions of the gates notQ, FQ, TQ and PQ
and
kT vk
2 = kα1bπ(1) + . . . + αnbπ(n)k
2 = hα1bπ(1) + . . . + αnbπ(n)
, α1bπ(1) + . . . + αnbπ(n)i
= |α1|
2 + . . . + |αn|
2
hence kT vk
2 = kvk
2
, as required, and T is unitary on V .

16.3 The notQ gate on C
2
On the general 1-qubit inputs α0|0i + α1|1i we have, by linear extension of notQ to the
whole of C
2
, that:
notQ(α0|0i + α1|1i) = α0notQ(|0i) + α1notQ(|1i)
= α0|1i + α1|0i,
equivalently represented as shown in Figure 16.1.
α0|0 + α1|1 notQ α0|1 + α1|0
FIGURE 16.1
The 1-qubit quantum negation gate on C
2
, where |α0|
2+|α1|
2= 1.
Recall from Section 14.2 that the eigenvalue-eigenvector pairs of the observable A = |1ih1|
are 0, |0i and 1, |1i. Measuring the quantum output of the negation gate, using the operator
|1ih1|, with this general input produces:
• 0 with probability |α1|
2 and
• 1 with probability |α0|
2
.
That is, the output of the gate notQ, following measurement, is generally non-deterministic.
The quantum negation gate on the whole of C
2 may be specified as:
Input : α0|0i + α1|1i where |α0|
2 + |α1|
2 = 1
Output(quantum) : α0|1i + α1|0i
Output(post−µQ) : 1 and post-measurement state |1i, with probability |α0|
2
, or
0 and post-measurement state |0i, with probability |α1|
2
.
As notQ permutes the basis {|0i, |1i} it follows from Lemma 16.1 that notQ is unitary on
C
2
, and its matrix, in this basis is the Pauli matrix:
σ1 =

0 1
1 0 
i.e.,

0 1
1 0   α0
α1

=

α1
α0

.
and, in particular,

0 1
1 0   1
0

=

0
1

, and 
0 1
1 0   0
1

=

1
0

,
equivalently σ1|0i = |1i and σ1|1i = |0i.The Feynman FQ gate on ⊗2C
2 277
16.4 The Feynman FQ gate on ⊗2C
2
In Chapter 15, FQ is only defined on the basis states B⊗2C2 = {|xi|yi : x, y ∈ B} of the space
C
2 ⊗ C
2
. We extend FQ to a linear operator on the whole of C
2 ⊗ C
2 by linear extension.
That is, we define FQ on the general state
|ψi = α00|0i|0i + α01|0i|1i + α10|1i|0i + α11|1i|1i
of C
2 ⊗ C
2 by
FQ|ψi ≡ α00FQ|0i|0i + α01FQ|0i|1i + α10FQ|1i|0i + α11FQ|1i|1i.
Note that FQ on the basis states has been defined in Section 15.4.3. It follows from Lemma
16.1 that FQ is unitary on ⊗2C
2
. The representation of FQ in the basis B⊗2C2 is the unitary
matrix


1 0 0 0
0 1 0 0
0 0 0 1
0 0 1 0

 .
16.5 The quantum Toffoli gate on ⊗3C
2
The extension of the quantum Toffoli gate to ⊗3C
2
is defined similarly to that of FQ. Given
the general state of C
2 ⊗ C
2 ⊗ C
2
:
|ψi = α000|0i|0i|0i + · · · + α111|1i|1i|1i
the quantum Toffoli gate is defined to be
TQ|ψi = α000TQ|0i|0i|0i + · · · + α111TQ|1i|1i|1i,
with TQ on the basis states having been defined in Section 15.5.2. The representation of FQ
in the basis B⊗3C2 is the unitary matrix:


1 0 0 0 0 0 0 0
0 1 0 0 0 0 0 0
0 0 1 0 0 0 0 0
0 0 0 1 0 0 0 0
0 0 0 0 1 0 0 0
0 0 0 0 0 1 0 0
0 0 0 0 0 0 0 1
0 0 0 0 0 0 1 0


.
16.6 The quantum Peres gate on ⊗3C
2
This gate is extended to ⊗3C
2
in the same way as the Toffoli gate above and its represen￾tation in the basis B⊗3C2 is the unitary matrix:278 Unitary extensions of the gates notQ, FQ, TQ and PQ


1 0 0 0 0 0 0 0
0 1 0 0 0 0 0 0
0 0 1 0 0 0 0 0
0 0 0 1 0 0 0 0
0 0 0 0 0 0 0 1
0 0 0 0 0 0 1 0
0 0 0 0 1 0 0 0
0 0 0 0 0 1 0 0


.
16.7 Summation expressions for the unitary extensions
16.7.1 On C
2
The unitary extensions of the identity function iQ and the notQ function may be expressed
as:
iQ =
X
1
x=0
|xihx|
and
notQ =
X
1
x=0
|xihx|
respectively. Alternatively, we may write:
iQ =
X
1
x=0
|iD(x)ihx|,
where iD(x) = x, and
notQ =
X
1
x=0
|notD(x)ihx|.
16.7.2 On C
2 ⊗ C
2
Writing the digital Feynman gate FD as
FD(x, y) = (FD(x, y)1, FD(x, y)2)
where FD(x, y)1 = x and FD(x, y)2 = y ⊕ x, the quantum Feynman gate FQ on the basis
states B⊗2C2 can be written as
FQ|xi|yi = |xi|y ⊕ xi
= |FD(x, y)1i|FD(x, y)2i.
Introducing the shorthand notation |FD(x, y)i for |FD(x, y)1i|FD(x, y)2i, the relationship
between the digital representation on B
2 and the quantum representation on B⊗2C2 may be
expressed as:
FQ|xi|yi = |FD(x, y)iSummation expressions for the unitary extensions 279
or alternatively and more succinctly,
FQ|xyi = |FD(xy)i.
Now consider the general state of C
2 ⊗ C
2
:
|ψi = α00|00i + α01|01i + α10|10i + α11|11i.
Taking the inner product with the basis state |00i we obtain, due to the orthonormality of
the basis states,
h00|ψi = α00h00|00i + α01h00|01i + α10h00|10i + α11h00|11i
= α00.
Likewise h01|ψi = α01, h10|ψi = α10, h11|ψi = α11.
Extending FQ to the general state, we obtain
FQ|ψi = α00FQ|00i + α01FQ|01i + α10FQ|10i + α11FQ|11i
= h00|ψiFQ|00i + h01|ψiFQ|01i + h10|ψiFQ|10i + h11|ψiFQ|11i
= (FQ|00ih00| + FQ|01ih01| + FQ|10ih10| + FQ|11ih11|)|ψi
= (|FD(00)ih00| + |FD(01)ih01| + |FD(10)ih10| + |FD(11)ih11|)|ψi
=
￾ X
11
xy=00
|FD(xy)ihxy|

|ψi
i.e., we may write the linearly extended gate as
FQ =
X
11
xy=00
|FD(xy)ihxy|
=
X
11
xy=00
|xi|y ⊕ xihxy|.
16.7.3 On C
2 ⊗ C
2 ⊗ C
2
For the unitary extension of the quantum Toffoli gate to C
2 ⊗ C
2 ⊗ C
2
, we have:
TQ =
X
111
xyz=000
|TD(xyz)ihxyz|
=
X
111
xyz=000
|xi|yi|z ⊕ (x ∧ y)ihxyz|.
The unitary extension of the Peres gate may be represented similarly.
16.7.4 Notation and closing observations
Moving forward it is helpful to simplify the notation for the unitarily extended operators
above to Uf where f is the invertible Boolean function being emulated. For example, we
write Ui for iQ, Unot for notQ, Ucnot for cnotQ, UT for TQ etc280 Unitary extensions of the gates notQ, FQ, TQ and PQ
We note that the general form of the unitary extensions discussed above may then be
expressed as:
Uf =
X
s∈Bn
|f(s)ihs|
=
X
s∈Bn
|f1(s)· · · fn(s)ihs|
=
X
s∈Bn
|f1(s)i · · · |fn(s)ihs|
where f : B
n → B
n is an invertible Boolean function and s ∈ B
n denotes a string of n
Boolean digits.
On a basis element |s
0
i of ⊗nC
n, where s
0 ∈ B
n, we obtain
Uf |s
0
i = |f1(s
0
)i · · · |fn(s
0
)i,
which follows from the observation that
hs|s
0
i =

1 if s
0 = s
0 if s
0 6= s.
States such as Uf |s
0
i are not superimposed and may therefore be measured using Axiom
3.2. If |ψi ∈ ⊗nC
2
is a superimposed state then Axiom 3 applies to the measurement of
Uf |ψi.
Exercises
16.1 Show that from the expressions for iQ and notQ given in Section 16.7.1, we obtain
iQ(α0|0i + α1|1i) = α0|0i + α1|1i, and
notQ(α0|0i + α1|1i) = α0|1i + α1|0i.17
Quantum information processing 2: the quantum
emulation of arbitrary Boolean functions
17.1 Objectives
Chapters 15 and 16 relate to the quantum emulation, and unitary extension, of a number of
special invertible Boolean functions. In this chapter, we turn our attention to the quantum
emulation of arbitrary Boolean functions. This extends the work of Chapters 15 and 16 in
two respects:
1. to arbitrary invertible Boolean functions, and
2. to arbitrary non-invertible Boolean functions.
Below we consider them separately beginning with the case of invertible functions, which is
similar to the treatment of the special cases. For the non-invertible functions we first define
invertible representations of them, from which quantum emulations may be constructed.
We first consider the invertible case.
17.2 Notation
We denote the set of all functions from B
n to B
m by F(B
n, B
m); i.e.,
F(B
n
, B
m) = {f : f : B
n → B
m}.
For m 6= n the functions of F(B
n, B
m) are not invertible; for m = n the set F(B
n, B
n)
comprises both invertible and non-invertible functions. For example F(B, B) comprises four
functions, two of which are invertible and two of which are not. For the special invertible
Boolean functions, considered in Chapters 15 and 16, we note that not ∈ F(B, B), cnot ∈
F(B
2
, B
2
) and the Boolean Toffoli and Peres functions are invertible elements of F(B
3
, B
3
).
17.3 Quantum emulation of arbitrary invertible Boolean functions
17.3.1 The quantum emulation of the invertible subset of F(B, B)
There are just two invertible functions in F(B, B), namely the identity function i(x) = x
and the function not(x) = x, both of which have been covered as special cases in Chapters
15 and 16 where the unitary operators Ui and Unot have been defined on C
2
.
DOI: 10.1201/9781003264569-17 281282 Quantum information processing 2
17.3.2 The quantum emulation of the invertible subset of F(B
2
, B
2
)
The following lemma, which we prove for the invertible functions of F(B
2
, B
2
), generalises
easily to the invertible functions of F(B
n, B
n) for n ≥ 3.
Lemma 17.1 If f : B
2 → B
2
is invertible then the operator Uf : C
2⊗C
2 → C
2⊗C
2 defined
by
Uf =
X
11
pq=00
|f(pq)ihpq|
is unitary. If i is the identity function on B
2
then Ui =
X
11
pq=00
|pqihpq| is the identity operator
on C
2 ⊗ C
2
.
Proof
For |xi|yi ∈ BC2⊗C2 we have:
Uf |xi|yi = |f(xy)i.
As f(xy) ∈ B
2
it follows that |f(xy)i ∈ BC2⊗C2 and because f is invertible we have |f(xy)i 6=
|f(x
0y
0
)i unless xy = x
0y
0
. Hence Uf permutes the elements of the basis BC2⊗C2 and is, by
Lemma 16.1, unitary. On the general state
|ψi = α00|00i + α01|01i + α10|10i + α11|11i
we have
Ui
|ψi = α00Ui
|00i + α01Ui
|01i + α10Ui
|10i + α11Ui
|11i = |ψi.

Under the conditions of the Lemma, in particular for f invertible, we now show that Uf
determines a quantum emulation of f in the sense that the values of f may be computed
deterministically from it. Clearly for some f1, f2 ∈ F(B
2
, B) we may write any f ∈ F(B
2
, B
2
)
as f(x, y) = (f1(x, y), f2(x, y)) and, in the compact notation, introduced in Section 16.7.4
we have, for f invertible, the unitary operator:
Uf =
X
11
pq=00
|f1(pq)i|f2(pq)ihpq|.
On the basis element |xi|yi ∈ BC2⊗C2 we obtain, using the result in Section 16.7.4:
Uf |xi|yi = |f1(xy)i|f2(xy)i
from which the values f1(xy) and f2(xy), of f, may be obtained deterministically by mea￾suring the observables A ⊗ I and I ⊗ A, where A = |1ih1|, on the first and second qubits of
the right hand side.
In the case of F(B
2
, B
2
) the invertible functions are readily identified and enumerated;
hence their quantum emulations may be determined explicitly and we do this in Section
17.3.3. The invertible functions of F(B
n, B
n), for n ≥ 3, are too numerous to list, and for
these cases we discuss their general unitary representation and provide specific examples –
see Section 17.3.4.Quantum emulation of arbitrary invertible Boolean functions 283
17.3.3 Explicit forms of the emulations of Section 17.3.2
Continuing the consideration of F(B
2
, B
2
); it is shown in Chapter 2 that there is a total of
24 invertible elements of F(B
2
, B
2
) determined by the following pairs from F(B
2
, B) (refer
to Table 2.13):
(f3, f5), (f3, f6), (f3, f9), (f3, f10),
(f5, f6), (f5, f9), (f5, f12),
(f6, f10), (f6, f12),
(f9, f10), (f9, f12),
(f10, f12)
and the same pairs in contrary order. Explicitly, the pairs are:
(x, y), (x, x ⊕ y), (x, x ⊕ y), (x, y),
(y, x ⊕ y), (y, x ⊕ y), (y, x),
(x ⊕ y, y), (x ⊕ y, x),
(x ⊕ y, y), (x ⊕ y, x),
(y, x).
Denoting the pairs by fi,j , their quantum emulations on the basis elements BC2⊗C2 are:
Ufi,j |xi|yi = |fi,j (xy)i = |fi(xy)i|fj (xy)i,
which produce the same outputs as their Boolean equivalents when the observables A ⊗ I
and I ⊗ A are measured on the first and second qubits. It follows from Lemma 17.1 that
the linear extension of Ufi,j to C
2 ⊗ C
2
is unitary.
Example 17.3.1
For f3,6(x, y) = (f3(x, y), f6(x, y)) = (x, x ⊕ y) we obtain:
Uf3,6
|xi|yi = |f3,6(xy)i
= |f3(xy)i|f6(xy)i
= |xi|x ⊕ yi
which is the quantum Feynman gate FQ discussed in Chapter 15. The matrix representation
of Uf3,6
, in the computational basis, is given in Section 16.4.
Example 17.3.2
For f6,10(x, y) = (x ⊕ y, y) we have
Uf6,10 |xi|yi = |x ⊕ yi|yi284 Quantum information processing 2
from which we obtain:
Uf6,10 |0i|0i = |01i = 0|00i + 1|01i + 0|10i + 0|11i
Uf6,10 |0i|1i = |10i = 0|00i + 0|01i + 1|10i + 0|11i
Uf6,10 |1i|0i = |11i = 0|00i + 0|01i + 0|10i + 1|11i
Uf6,10 |1i|1i = |00i = 1|00i + 0|01i + 0|10i + 0|11i.
i.e., the matrix representation of Uf6,10 , in the computational basis, is:


0 0 0 1
1 0 0 0
0 1 0 0
0 0 1 0

 .
The remaining 22 invertible Boolean functions, of type F(B
2
, B
2
), have similar quantum
emulations that may be measured in the same way.
17.3.4 The invertible subset of F(B
n, B
n), n ≥ 3
The case n = 3: we do not consider the explicit forms of all invertible functions of F(B
3
, B
3
),
but note that the Toffoli and Peres gates are examples. For a general invertible f ∈ F(B
3
, B
3
)
we define:
Uf =
X
111
pqr=000
|f(pqr)ihpqr|
from which we obtain:
Uf |xi|yi|zi = |f(xyz)i
= |f1(xyz)i|f2(xyz)i|f3(xyz)i.
Measuring the three qubits with the operators A ⊗ I ⊗ I, I ⊗ A ⊗ I and I ⊗ I ⊗ A produces
the same outputs as their digital counterparts. The linear extension of Uf defines a unitary
operator on C
2 ⊗ C
2 ⊗ C
2
.
For n > 3, Uf is constructed from f in a similar way.
17.4 Quantum emulation of arbitrary non-invertible Boolean
functions
Here we consider the quantum emulation of the arbitrary Boolean functions F(B
n, B
m).
Such functions are not, in general, invertible. For every non-invertible function, f, we deter￾mine an invertible ‘representation’ which ‘embeds’ the function f and from which the values
of f, at arbitrary domain points, are easily obtained. We will see that the fi,j functions of
Section 17.3.3 embed a subset of the functions F(B
2
, B), and for general non-invertible
functions, f, a group theory based construction produces embeddings, which we denote by
ˆf; see Lemma 17.2 and its corollary below.Quantum emulation of arbitrary non-invertible Boolean functions 285
17.4.1 A fundamental lemma
Here we construct, for any f ∈ F(B
n, B
m), an invertible representation, ˆf : B
n+m → B
n+m,
from which a unitary quantum equivalent, on ⊗n+mC
2
, is easily determined. The following
lemma and corollary are fundamental to the work of this chapter and play a role both in
invertible-digital and quantum computation.
Lemma 17.2 If G and H are groups and f : G → H is an arbitrary function, then the
function ˆf : G × H → G × H defined by:
ˆf(g, h) = (g, hf(g))
is one-to-one and onto, with inverse
ˆf
−1
(g, h) = (g, hf(g)
−1
).
Proof
1. to show that ˆf is one-to-one: we have if (g, hf(g)) = (g
∗
, h∗f(g
∗
)) then g
∗ = g
and h
∗f(g
∗
) = h
∗f(g) = hf(g), i.e., h
∗ = h.
2. to show that ˆf is onto: for arbitrary (a, b) ∈ G × H seek a pair (g, hf(g)) such
that (g, hf(g)) = (a, b). We have g = a and hf(a) = b, i.e., h = bf(a)
−1
.
3. It is easy to show that ˆf
−1
(
ˆf(g, h)) = (g, h).

Corollary 1 With G = (B
n, ⊕) and H = (B
m, ⊕) we have: if f : B
n → B
m then the
function ˆf : B
n+m → B
n+m defined by ˆf(x, y) = (x, y ⊕ f(x)) is one-to-one, onto and
self-inverse.
Proof
That ˆf is one-to-one and onto is immediate from Lemma 17.2; that it is self-inverse follows
from ˆf(
ˆf(x, y)) = (x, y ⊕ f(x) ⊕ f(x)) = (x, y).

We note that for the function ˆf of the corollary, defined for x ∈ B
n and y ∈ B
m, we obtain
for y = 0m
ˆf(x, 0
m) = (x, f(x))
i.e., the function value at x occurs in the second component of ˆf(x, 0
m) and we therefore refer
to ˆf as an invertible ‘representation of f’. It will be seen, in Chapter 18, that Corollary
1 leads to auxiliary bits/qubits being introduced in both invertible-digital and quantum
computations. In the remainder of this chapter, we consider invertible representations of:
1. the non-invertible subset of F(B, B),
2. the non-invertible functions F(B
2
, B) and
3. the non-invertible subsets of the general case F(B
n, B
m), for n > 2 and m ≥ 1,
beginning with the non-invertible subset of F(B
n, B
n).286 Quantum information processing 2
17.4.2 The non-invertible subset of F(B, B)
The non-invertible functions of F(B, B) are f0(x) = 0 and f1(x) = 1, for x ∈ B. For each of
these we apply Corollary 1 and define invertible Boolean representations of f0 and f1
ˆf0(x, y) = (x, y ⊕ f0(x))
= (x, y ⊕ 0)
= (x, y) (the identity function on B
2
),
and
ˆf1(x, y) = (x, y ⊕ f1(x))
= (x, y ⊕ 1)
= (x, y)
from which we have
ˆf0(x, 0) = (x, f0(x))
= (x, 0)
and
ˆf1(x, 0) = (x, f1(x))
= (x, 1).
Quantum emulations of f0 and f1 may then be defined on the basis elements of BC2⊗C2 by
Ufˆ0
|xi|yi = |xi|y ⊕ f0(x)i
= |xi|yi
and
Ufˆ1
|xi|yi = |xi|y ⊕ f1(x)i
= |xi|yi
which, by linear extension, determine the unitary operators (also denoted Ufˆ0
and Ufˆ1
):
Ufˆ0
=
X
11
pq=00
|pqihpq|
Ufˆ1
=
X
11
pq=00
|pqihpq|
on C
2 ⊗ C
2
. We have
Ufˆ0
|xi|0i = |xi|0i and Ufˆ1
|xi|0i = |xi|1i
which produce the same outputs as their Boolean equivalents, for x ∈ B, when measured
with the operator I ⊗ A. We note, finally, that the identity function ˆf0 on B
2
is emulated
by the identity operator Ufˆ0
on C
2 ⊗ C
2
.Quantum emulation of arbitrary non-invertible Boolean functions 287
17.4.3 The non-invertible functions F(B
2
, B)
The functions of F(B
2
, B), which we denote by f0, . . . , f15, are clearly not invertible (see
Table 2.13). For the purposes of constructing quantum emulations we consider the two
following subsets separately.
F1(B
2
, B) = {f5, f6, f9, f10}
and
F2(B
2
, B) = {f0, f1, f2, f3, f4, f7, f8, f11, f12, f13, f14, f15}.
I. The subset F1(B
2
, B) of F(B
2
, B):
Although Corollary 1 may be invoked to construct invertible representations of these func￾tions, it produces representations on B
3
. Instead representations on B
2 are possible and, as
shown below, we have already constructed suitable quantum emulations for them in Section
17.3.3.
Specifically, we have f5(x, y) = y, f6(x, y) = x ⊕ y, f9(x, y) = x ⊕ y, f10(x, y) = y, and
note that, from Section 17.3.3, the functions f3,5, f3,6, f3,9 andf3,10 are invertible and take
the form:
f3,5(x, y) = (x, f5(x, y))
f3,6(x, y) = (x, f6(x, y))
f3,9(x, y) = (x, f9(x, y))
f3,10(x, y) = (x, f10(x, y))
with the quantum emulations (again see Section 17.3.3):
Uf3,5
|xi|yi = |xi|f5(x, y)i
Uf3,6
|xi|yi = |xi|f6(x, y)i
Uf3,9
|xi|yi = |xi|f9(x, y)i
Uf3,10 |xi|yi = |xi|f10(x, y)i
on the basis states of C
2 ⊗ C
2
. Measuring the 2nd qubit, in each case provides, determinis￾tically, the value of the associated non-invertible Boolean function of F1(B
2
, B) on (x, y).
The extended operators Uf3,i , on C
2 ⊗ C
2
, may be expressed as
Uf3,i =
X
11
pq=00
|pi|fi(pq)ihp|hq|.
II. The subset F2(B
2
, B) of F(B
2
, B):
It is easy to show that the above constructions, i.e., (x, fi(x, y)), are not invertible for
the functions of F2(B
2
, B) (see Exercise 17.1 below). It follows from the explicit forms
for the invertible functions of F(B
2
, B
2
), identified in Section 17.3.3, that no invertible
representations from B
2
to B
2 are possible for the functions of F2(B
2
, B). Instead we apply
Corollary 1 and define invertible representations with domain and range B
3
; i.e., for fi ∈
F2(B
2
, B) we define
ˆfi(x, y, z) = (x, y, z ⊕ fi(x, y))288 Quantum information processing 2
and note that:
• the process, applied to f1, i.e., f1(x, y) = x ∧ y, yields the Toffoli gate:
ˆf1(x, y, z) = (x, y, z ⊕ (x ∧ y)).
which is an invertible form of the ∧ function. There exist similar representations for all
the functions of F2(B
2
, B).
From Corollary 1 we have invertible representations ˆfi
: B
3 → B
3
, of fi ∈ F2(B
2
, B), defined
by:
ˆfi(x, y, z) = (x, y, z ⊕ fi(x, y)),
and the quantum operators Ufˆi
on the basis states B⊗3C2
Ufˆi
|xi|yi|zi = |xi|yi|z ⊕ fi(x, y)i
from which we note that:
Ufˆi
|xi|yi|0i = |xi|yi|fi(x, y)i.
The function value fi(x, y) ∈ B can be obtained by measuring the 3rd qubit of Ufˆi
|xi|yi|0i
with the operator I ⊗ I ⊗ A.
The extended operator Ufˆi
, on C
2 ⊗ C
2 ⊗ C
2
, may be expressed as:
Ufˆi
=
X
111
pqr=000
|pi|qi|r ⊕ fi(pq)ihp|hq|hr|.
17.4.4 The non-invertible subset of F(B
n, B
n)
From Corollary 1 it follows that for any non-invertible f : B
n → B
n, the invertible repre￾sentation ˆf : B
2n → B
2n is defined by
ˆf(x, y) = (x, y ⊕ f(x))
≡ (x1, . . . , xn, y1 ⊕ f1(x), . . . , yn ⊕ fn(x))
where x = x1, . . . , xn ∈ B
n and y = y1, . . . , yn. For ˆf : B
2n → B
2n we have:
ˆf(x, 0) = (x, f(x)),
or
ˆf(x1, . . . , xn, 0) = (x1, . . . xn, f1(x1, . . . , xn), . . . , fn(x1, . . . , xn))
i.e., the components of f(x) comprise the n + 1, . . . , 2n components of ˆf(x, 0).
The quantum emulations of these functions correspond to the special case of m = n in
the section, immediately below, which discusses the more general functions F(B
n, B
m).
17.4.5 The general functions F(B
n, B
m)
Again, following Corollary 1 we have for any f : B
n → B
m, the invertible representation
ˆf : B
n+m → B
n+m defined by:
ˆf(x, y) = (x, y ⊕ f(x))
≡ (x1, . . . , xn, y1 ⊕ f1(x), . . . , ym ⊕ fm(x))Black-box representations of Boolean functions and their quantum emulations 289
where x = x1, . . . , xn ∈ B
n and y = y1, . . . , ym ∈ B
m. We have:
ˆf(x1, . . . , xn, 0) = (x1, . . . , xn, f1(x1, . . . , xn), . . . , fm(x1, . . . , xn))
i.e., the components of f(x) comprise the n + 1, . . . , n + m components of ˆf(x, 0). For
f : B
n → B
m we define Uf on the basis states B⊗n+mC2 by:
Ufˆ|xi|yi = |xi|y ⊕ f(x)i
where |xi ∈ ⊗nC
2
, |yi ∈ ⊗mC
2
. From this we obtain:
Ufˆ|xi|0i = |xi|f(x)i
= |x1i · · · |xni|f1(x)i|f2(x)i · · · |fm(x)i
and the Boolean values f1(x), . . . , fm(x) may be obtained by measuring the state Ufˆ|xi|0i
with the operators:
I
⊗n ⊗ A ⊗ I
⊗(m−1)
.
.
.
I
⊗n ⊗ I
⊗(m−1) ⊗ A
respectively. We note that the linear extension of Ufˆ to ⊗n+mC
2
, also denoted Ufˆ, may be
expressed as:
Ufˆ =
X
p,q
|pi|q ⊕ f(p)i|hp|hq|
where p ∈ B
n and q ∈ B
m.
17.5 Black-box representations of Boolean functions and their
quantum emulations
It is common, in algorithms, to use black-box (or ‘oracle’) representations of invertible
Boolean functions and their corresponding quantum operators. The examples in the figures
below are for:
1. invertible functions of F(B
2
, B
2
) and their quantum emulations – see Section
17.3.2 and Figures 17.1 and 17.2,
2. invertible representations of the functions F1(B
2
, B) and their quantum emula￾tions – see Section 17.4.3 and Figures 17.3 and 17.4,
3. invertible representations of the functions F2(B
2
, B) and their quantum emula￾tions – see Section 17.4.3 and Figures 17.5 and 17.6.
Oracles, in general, show what is to be computed on the input specified without details
of how the computations are done or even what f is. Often, in the design of algorithms,
we only have query access to an oracle for a function – from which we may investigate its
properties.
We note that for the quantum operators the general states of C
2 ⊗ C
2 are also valid
input.290 Quantum information processing 2
y
x
f
f1(x, y)
f2(x, y)
FIGURE 17.1
Black-box representation of an invertible function f : B
2 → B
2
, where f(x, y) =
(f1(x, y), f2(x, y)) on B
2
.
|x
|y
|f1(x, y)
Uf
|f2(x, y)
FIGURE 17.2
The black-box representation of the unitary operator Uf , on the basis states B⊗2C2 , for f
of Figure 17.1.
y
x
fi (x, y)
f3,i
x
FIGURE 17.3
Black-box representation of the non-invertible function fi ∈ F1(B
2
, B) embedded in f3,i.
|x |x
|y
Uf3,i
|fi(x, y)
FIGURE 17.4
Black-box representation of the quantum operator Uf3,i on B⊗2C2 .
y
x x
ˆf
y ⊕ fi(x)
i
FIGURE 17.5
Black-box representation of a non-invertible function fi ∈ F2(B
2
, B).
|y ⊕ f(x)
|x |x
|y
Ufˆ
i
i
FIGURE 17.6
Black-box representation of the quantum operator Ufˆi
on B⊗2C2 for fi ∈ F2(B
2
, B).Black-box representations of Boolean functions and their quantum emulations 291
Exercises
17.1 Show that the functions
(a) (x, y) → (x, x ∧ y) and
(b) (x, y) → (x, x ∨ y),
from B
2
to B
2
, are not invertible.
17.2 Show that the function
ˆf7(x, y, z) = (x, y, z ⊕ f7(x, y))
= (x, y, z ⊕ (x ∨ y))
is invertible.
17.3 If f : B
2 → B is defined by f(x1, x2) = x1 ∧ x2, show that the associated ˆf
function is the Toffoli gate.
17.4 Show that the invertible subset of F(B
n, B
n), which we can denote by G(n, B),
determines a group under the functional composition operator ◦.
17.5 Show that the functions of F(B
n, B
m) determine a group under the ⊕ operator.
17.6 If f ∈ F(B
n, B
m) and Fˆ
n,m comprises the functions ˆf, where ˆf(x, y) = (x, y ⊕
f(x)) for x ∈ B
n and y ∈ B
m, show that Fˆ
n,m determines a group under functional
composition.
17.7 Show that the groups (F(B
n, B
m), ⊕) and (Fˆ
n,m, ◦) are isomorphic.
17.8 Show that for all g, g∗ ∈ G(n, B) the quantum emulation, g → Ug of g, defined
by Ug|xi = |g(x)i, satisfies
Ugg∗ = UgUg
∗ .
Under these conditions we say that g → Ug is a representation of the group
G(n, B) in the unitary group of ⊗nC
2
.
17.9 Show that the group Fˆ
n,m has a representation in the unitary group of ⊗n+mC
2
.18
Invertible digital circuits and their
quantum emulations
18.1 Objectives
Black box representations of functions, by definition, do not provide information on how a
function, so represented, is computed. By a ‘circuit’ for the computation of a function we
mean a sequence of ‘basic’ gates that implement the function. It can be shown that, given
a classical circuit for the Boolean function ˆf, there is a quantum circuit of equal ‘efficiency’
for the implementation of the unitary operator Ufˆ and hence the quantum computation of
f. In this chapter we consider examples of invertible digital functions and their evaluation
using basic invertible Boolean gates; specifically the Toffoli and Feynman gates TD and
FD. We recall that the Toffoli gate is complete for invertible digital computation, however,
the inclusion of the Feynman gate, FD, simplifies the circuits. We proceed by example to
demonstrate the process and consequential side-effects of such implementations and their
quantum emulations. It is seen that the cost of using only Toffoli gates is that additional
input bits/qubits may be required, and the computation of intermediate results (usually
referred to as garbage or junk) may be necessary. We also consider a general circuit for
re-setting registers containing ‘junk’, this is helpful to avoid problems with downstream
computation.
18.2 Invertible digital circuits
We begin with a general Boolean function f : B
n → B
m and the general invertible repre￾sentation ˆf : B
n+m → B
n+m, of f, discussed earlier and defined by:
ˆf(x, y) = (x, y ⊕ f(x)) for x ∈ B
n
and y ∈ B
m.
We note that invertible representations need not be of this form and representations with
fewer than n + m variables may be possible (for example, the Peres gate); but what follows
is based on the ˆf representation, which works for any f. We have:
ˆf(x, 0
m) = (x1, . . . , xn, f1(x), . . . , fm(x)).
The n-input Toffoli gate is defined by:
T
(n)
(x1, x2, . . . , xn−1, y) = (x1, x2, . . . , xn−1, y ⊕ (x1 ∧ x2 ∧ . . . ∧ xn−1)).
For n = 3 we have the ‘usual’ Toffoli gate T
(3)(x1, x2, y) = (x1, x2, y ⊕ (x1 ∧ x2)) with the
circuit shown in Figure 18.1.
DOI: 10.1201/9781003264569-18 293294 Invertible digital circuits and their quantum emulations
y y ⊕ ( ∧ )
x1 x1
x2
x1 x2
x2
FIGURE 18.1
The Boolean Toffoli, or ccnot, 3-bit invertible gate.
In the 4-input case we have f : B
3 → B and ˆf : B
4 → B
4 defined by
f(x1, x2, x3) = x1 ∧ x2 ∧ x3
ˆf(x1, x2, x3, y) = (x1, x2, x3, y ⊕ (x1 ∧ x2 ∧ x3)).
Clearly, more than one 3-input Toffoli gate is necessary to implement the 4-input, invertible
Toffoli gate ˆf. It may, however, be implemented using two 3-input Toffoli gates – see Figure
18.2. We note that a workspace register, y1, is required to hold the intermediate result
x1 ∧ x2, the y2 register holds the required output of the circuit when y1 and y2 are suitably
initialised.
y1
y2
x3 x3
⊕ (y ⊕(x1∧x2) y2 ( 1
)∧x3)
⊕ ( ∧ )
x1 x1
x2
x1 x2
x2
y1
FIGURE 18.2
A realisation of the Toffoli 4-bit invertible gate using two Toffoli 3-input gates.
The circuit outputs (x1, x2, x3, x1∧x2, x1∧x2∧x3) on the input of (x1, x2, x3, 0, 0). (Figure
18.3). In addition to the function value f(x1, x2, x3) the value x1∧x2 is also output. This is a
direct consequence of computing a non-invertible function using an invertible representation.
Unwanted outputs, x1∧x2, in this case, generated by networks such as Cfˆ are often referred
to as ‘junk’.
Alternatively, to re-order the outputs, consider Figure 18.4 from which we obtain, by
setting y1 = y2 = 0, the network in Figure 18.5.Invertible digital circuits 295
x1 x2
x1∧x2 ∧x3
0
0
∧ (junk)
Cfˆ
x3 x3
x1 x1
x2 x2
FIGURE 18.3
A network Cfˆ for the Toffoli 4-bit invertible gate ˆf.
y2 ⊕ (x1 ∧ x2) (junk)
C
∗
fˆ
y1
y2
x3 x3
x1 x1
x2 x2
y1 ⊕ ((y2 ⊕ (x1 ∧ x2)) ∧ x3)
FIGURE 18.4
A modification of Cfˆ to re-order the output.
(junk)
x1 ∧ x2 ∧ x3
x1 ∧ x2
0
0
C
∗
fˆ
x3 x3
x1 x1
x2 x2
FIGURE 18.5
The network C
∗
fˆ with y1 = y2 = 0.296 Invertible digital circuits and their quantum emulations
18.3 Junk removal
Registers holding junk can be reset to their initial values – this is sometimes referred to
as ‘un-computing’ the junk. For example, in the case above, this can be done using an
additional Toffoli gate as shown in Figure 18.6. A more general structure for resetting
registers holding junk is illustrated in Figure 18.7.
0 x1 ∧ x2 ∧ x3
0
C
∗
fˆ
x3 x3
x1 x1
x2 x2
0
FIGURE 18.6
A junk-removed network for ˆf.
0
0 0
0
C
∗
fˆ
x3
x1
x2
0 x1 ∧ x2 ∧ x3
x3
x1
x2
C
∗−1
fˆ
FIGURE 18.7
An alternative network for junk removal from ˆf.Quantum emulation 297
The circuit of Figure 18.7 introduces an additional input register and further gates,
however, it is representative of a general network structure for the removal of junk in
invertible digital computation – see Figure 18.8. The inputs, other than x1, . . . , xn, to the
network above are often called ‘auxiliary’ or ‘ancilla’ inputs necessary for the ‘junk-removed’
computation of f(x) using an invertible circuit.
m ‘output’ bits 
k workspace bits 
m copy bits 
m bits 
k bits 
x1
xn
.
.
.
x1
xn
.
.
.
0
.
.
.
0
0
.
.
.
0
x1
xn
.
.
.
0
.
.
.
0
0
.
.
.
0
f1(x)
fm(x)
.
.
.
j1(x)
jk(x)
0
.
.
.
0
.
.
.
.
. Cfˆ . C
−1
fˆ
f1(x)
fm(x)
.
.
.
.
.
.
FIGURE 18.8
A general network structure for the computation of any f : B
n → B
m with junk
j1(x), . . . jk(x) removed – i.e., the workspace registers reset to 0.
18.4 Quantum emulation
Figure 18.9 shows a quantum network for the implementation of |f(x)i with junk removed.
Mathematically, for the network of Figure 18.9 we have:
(U
−1
f ⊗ I)cnot(Uf ⊗ I)|xi|0
mi|0
k
i|0
mi = (U
−1
f ⊗ I)cnot|xi|f(x)i|ji|0
mi
= (U
−1
f ⊗ I)(|xi|f(x)i|ji|f(x)i)
= |xi|0
mi|0
k
i|f(x)i.298 Invertible digital circuits and their quantum emulations
m ‘output’ qubits 
k workspace qubits 
m copy qubits 
m qubits 
k qubits 
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
|x1
|x1 |x1
|xn |xn
|0
|0
|0
|0
|xn
|0
|0
|f1(x)
|fm(x)
.
.
.
|fm(x)
|f1(x)
|jk(x)
|j1(x)
.
.
.
.
.
.
|0
|0
.
.
.
|0
|0
Ufˆ U
−1
fˆ
FIGURE 18.9
A quantum network structure for the computation of |f(x)i with junk |j1(x)i, . . . |jk(x)i
removed.
Exercises
18.1 Show that the digital gate
T
∨
D(x, y, z) = (x, y, z ⊕ (y ∨ x))
is invertible.
18.2 Design an invertible digital circuit to output the pair (x1, x2) ∈ B
2
in descending
order. (Hint: note that max(x1, x2) = x1 ∨ x2 and min(x1, x2) = x1 ∧ x2).19
Quantum measurement 2: general pure states,
Bell states
19.1 Objectives
The relatively simple examples of quantum computation (emulating digital computation)
discussed in Chapters 15, 17 and 18 involved only the simplest quantum inputs and pro￾duced, deterministically, outputs identical to their digital counterparts. The simplified state￾ment of the von Neumann-L¨uders measurement postulate, discussed in Chapter 14, was ap￾plicable to these computations. In this chapter we introduce the processing of more general,
i.e., super-imposed, quantum states. This is divergent from the digital case and, as will be
clear later, imparts quantum computation with extended power.
19.2 The measurement of super-imposed quantum states
In moving forward to consider these generalisations, it is necessary to invoke the general
von Neumann-L¨uders measurement model (i.e., Axiom 3 of Section 13.6) which we restate
below and apply it to an example with which we already have some familiarity.
Axiom 3
Following the measurement of an observable O on the state |ψi ∈ ⊗nC
2
, the post￾measurement state is the normalisation of the vector P|ψi, where P is the orthogonal
projection of |ψi onto the subspace of ⊗nC
2 generated by the eigenstates of O that
1. occur in the linear-superposition of |ψi, and
2. are compatible with the outcome of the measurement of O.
The probability of the outcome obtained is kP|ψik2
.
Example 19.2.1 An application of the general measurement postulate (Axiom
3)
Consider the quantum state
|ψi = α00|0i|0i + α01|0i|1i + α10|1i|0i + α11|1i|1i ∈ C
2 ⊗ C
2
where |α00|
2+|α01|
2+|α10|
2+|α11|
2= 1. If A is the observable with matrix

0 0
0 1 
DOI: 10.1201/9781003264569-19 299300 Quantum measurement 2: general pure states, Bell states
in the computational basis, then the vectors {|0i|0i, |0i|1i, |1i|0i, |1i|1i} are eigenvectors of
the observable A ⊗ I and they determine an orthonormal basis of C
2 ⊗ C
2
(see Example
11.8.6). The eigenvalues of A ⊗ I are 0 and 1 and they are both degenerate. Specifically, we
have
A ⊗ I|0i|0i = 0|0i|0i, A ⊗ I|0i|1i = 0|0i|1i
and
A ⊗ I|1i|0i = 1|1i|0i, A ⊗ I|1i|1i = 1|1i|1i.
Hence if 0 is measured for A ⊗ I on |ψi, we have
1.
P|ψi = α00|0i|0i + α01|0i|1i
which happens with probability kP|ψik2 = |α00|
2 + |α01|
2
, and
2. the post-measurement state
α00|0i|0i + α01|0i|1i
p
|α00|
2 + |α01|
2
.
Similarly if 1 is measured for A ⊗ I on |ψi, we have:
1.
P|ψi = α10|1i|0i + α11|1i|1i
which happens with probability kP|ψik2 = |α10|
2 + |α11|
2
, and
2. the post-measurement state
α10|1i|0i + α11|1i|1i
p
|α10|
2 + |α11|
2
.
19.3 Measuring the EPR-Bell state √
1
2
(|0i|0i + |1i|1i)
From the above we also note that if 0 is obtained when measuring the first qubit of the Bell
state
|B1i =
1
√
2
(|0i|0i + |1i|1i)
with the operator A ⊗ I, the post-measurement state is
|0i|0i
and if 1 is measured then the post-measurement state is
|1i|1i
these being the states that are
1. compatible with the measured value of the observable and
2. occur in the representation of |B1i as a superposition of the eigenstates of the
observable A ⊗ I (the von Neumann-L¨uders postulate).Measuring the EPR-Bell state √
1
2
(|0i|0i + |1i|1i) 301
This is a striking result in that the post-measurement state of the second qubit, although
nothing is actually done to it (it being ‘trivially’ measured by the identity operator), is
identical to that of the first qubit. The physical implications of this result are particularly
interesting. It is possible to design an experiment that outputs two photons moving in
opposite directions (say left and right) with the same, but unknown helicity. The helicity
of a photon is a quantum observable that takes just two values which are usually denoted
by v and h. It follows that the state of the combined photon pair is the entangled tensor:
1
√
2
(|vi|vi + |hi|hi).
If the helicity of the left-travelling photon is measured with result v, then the post￾measurement state of the pair is
|vi|vi
i.e., the right travelling photon, the state of which has not been measured (and which
may be millions of miles separated from the left-travelling photon at the time when the
helicity of the left-travelling photon is measured), has the same post-measurement state
as that of the left-travelling photon. It is as though the right-travelling photon has also
been measured. Of course, if the helicity of the left travelling photon was measured to be h
then the post-measurement state of the pair would be |hi|hi. The state √
1
2
(|vi|vi + |hi|hi)
is known as the Einstein-Podolsky-Rosen (EPR) state, these being the physicists to first
identify this counter-intuitive quantum phenomenon. This theoretically-predicted behaviour
was initially regarded as paradoxical and referred to, by Einstein, as ‘spooky action at a
distance’. In quantum mechanics the EPR and Bell states are said to be entangled – a term
defined formally in Chapter 10. More recent research, particularly due to the Irish physicist
John Bell, and experiments by various institutions have supported the view that there is
no paradox and that quantum mechanics may be ‘non-local’.
Another view: Alice and Bob work in physics laboratories in locations separated by some
distance. They each have a 2-state quantum particle and their particles are entangled; i.e.,
the state of the pair is √
1
2
(|0i|0i + |1i|1i). If Alice measures 0 for her particle then the
post-measurement state of the pair is |0i|0i; Bob will now measure 0 for his particle with
probability 1, and Alice knows this in advance without any communication between the
two. Prior to Alice’s measurement, Bob would have measured 0 with probability 1
2
and 1
with probability 1
2
for his particle.
This was regarded as paradoxical in the sense that faster-than-light ‘communication’ ap￾pears to be taking place and this is incompatible with relativity theory. However, unless Alice
tells Bob (by classical means e.g., telephone, carrier pigeon etc.) that she has measured her
particle and dis-entangled the pair, Bob wouldn’t know that his particle was dis-entangled
– and he couldn’t determine this by measurement. Because if the dis-entangled state were
|0i|0i he would measure 0 with probability 1, but he couldn’t conclude that the particles
were dis-entangled prior to his measurement – because Bob’s probability of measuring 0 for
his particle was 1
2 whilst they were entangled.302 Quantum measurement 2: general pure states, Bell states
19.4 More general measurements of 2-qubit states in the
computational basis
Again we consider measurement of the observables A ⊗ I : C
2 ⊗ C
2 → C
2 ⊗ C
2 and
I ⊗ A : C
2 ⊗ C
2 → C
2 ⊗ C
2 on the general state |ψi ∈ C
2 ⊗ C
2 defined by
|ψi = α00|0i|0i + α01|0i|1i + α10|1i|0i + α11|1i|1i ∈ C
2 ⊗ C
2
where |α00|
2 + |α01|
2+|α10|
2 + |α11|
2 = 1. The observables have the matrix form
A ⊗ I =


0 0 0 0
0 0 0 0
0 0 1 0
0 0 0 1


and
I ⊗ A =


0 0 0 0
0 1 0 0
0 0 0 0
0 0 0 1

 .
Measuring A ⊗ I on |ψi we have:
1. if 0 is measured then the post-measurement state is, as we have seen earlier, given by
|ψ1i =
α00|0i|0i + α01|0i|1i
p
|α00|
2 + |α01|
2
1.1 Measuring |ψ1i with I ⊗ A with the result 0 leads to the post-measurement state
|0i|0i
1.2 Measuring |ψ1i with I ⊗ A with the result 1 leads to the post-measurement state
|0i|1i
2. if 1 is measured for A ⊗ I then the post-measurement state is, as we have seen earlier,
given by:
|ψ2i =
α10|1i|0i + α11|1i|1i
p
|α10|
2 + |α11|
2
2.1 Measuring |ψ2i with I ⊗ A with the result 0 leads to the post-measurement state
|1i|0i
2.2 Measuring |ψ2i with I ⊗ A with the result 1 leads to the post-measurement state
|1i|1i,
i.e., we have for consecutive measurements of the observables, A ⊗ I and I ⊗ A, the four
possible outcomes:
• for measurements (0, 0) the post-measurement state will be |0i|0i,
• for measurements (0, 1) the post-measurement state will be |0i|1i,
• for measurements (1, 0) the post-measurement state will be |1i|0i,
• for measurements (1, 1) the post-measurement state will be |1i|1i.Measuring 2-qubit states in the Bell basis 303
19.5 Measuring 2-qubit states in the Bell basis
To this point we have only considered quantum measurement in ‘computational’ bases, i.e.,
{|0i, |1i} and the tensor product bases constructed therefrom. Here, we consider measure￾ment in the orthonormal (and entangled) Bell basis:
|B1i =
1
√
2
(|0i|0i + |1i|1i)
|B2i =
1
√
2
(|0i|1i + |1i|0i)
|B3i =
1
√
2
(|0i|0i − |1i|1i)
|B4i =
1
√
2
(|0i|1i − |1i|0i)
of C
2 ⊗ C
2
. For any normalised state |ψi ∈ C
2 ⊗ C
2
there exist β1, β2, β3, β4 ∈ C with
P4
i=1|βi
|
2 = 1 and
|ψi = β1|B1i + β2|B2i + β3|B3i + β4|B4i.
As we have seen, the von Neumann-L¨uders postulate relates measurement to self-adjoint op￾erators having eigenvectors corresponding to the basis chosen to represent the state – in this
case the Bell vectors |B1i, |B2i, |B3i, |B4i. The following observables S
2
1
, S2
2
, S3
3
, equivalently
self-adjoint operators on C
2 ⊗ C
2
, each have the Bell states as eigenstates.
S
2
1 =
1
2


1 0 0 1
0 1 1 0
0 1 1 0
1 0 0 1


, S2
2 =
1
2


1 0 0 −1
0 1 1 0
0 1 1 0
−1 0 0 1


, S2
3 =


1 0 0 0
0 0 0 0
0 0 0 0
0 0 0 1

 .
Lemma 19.1 The Bell states |B1i, |B2i, |B3i, |B4i are eigenvectors of each of the observ￾ables S
2
1
, S2
2 and S
2
3
. The eigenvalues are 0 and 1, for each of the observables, and the
spectral properties are as indicated in the following table:
S
2
1 S
2
2 S
2
3
|B1i 1 0 1
|B2i 1 1 0
|B3i 0 1 1
|B4i 0 0 0
Proof
Left as an exercise.
19.5.1 Measuring the observables S
2
3 and S
2
1
1. If measuring the state |ψi with the observable S
2
3
results in the outcome 1, then the
post-measurement state is:
|ψ1i =
β1|B1i + β3|B3i
p
|β1|
2 + |β3|
2
.
Then if a measurement of the observable S
2
1 on the state |ψ1i results in the outcome 1 then
the resulting state is:
|B1i.304 Quantum measurement 2: general pure states, Bell states
If a measurement of the observable S
2
1 on the state |ψ1i results in the outcome 0 then the
resulting state is:
|B3i.
2. If measuring the state |ψi with the observable S
2
3
results in the outcome 0 then the
post-measurement state is:
|ψ2i =
β2|B2i + β4|B4i
p
|β2|
2 + |β4|
2
.
Then if a measurement of the observable S
2
1 on the state |ψ2i results in the outcome 1 then
the resulting state is:
|B2i.
If a measurement of the observable S
2
1 on the state |ψ2i results in the outcome 0 then the
resulting state is:
|B4i.
That is, when measuring the observables S
2
3 and S
2
1 on the state |ψi we have:
• if (1, 1) is measured then the post-measurement state will be |B1i,
• if (1, 0) is measured then the post-measurement state will be |B3i,
• if (0, 1) is measured then the post-measurement state will be |B2i,
• if (0, 0) is measured then the post-measurement state will be |B4i.
Exercises
19.1 Following the calculations above to measure the observables S
2
3 and S
2
1 on the
state |ψi:
(a) perform the calculations to measure the observables S
2
1 and S
2
2 on the state
|ψi.
(b) perform the calculations to measure the observables S
2
2 and S
2
3 on the state
|ψi.20
Quantum information processing 3
20.1 Objectives
We have seen in earlier chapters how digital computations can be emulated quantum me￾chanically. In this chapter, we cover further aspects of processing, and highlight some signif￾icant differences that occur between the digital and quantum cases. We first introduce the
concepts of quantum parallelism and quantum swapping both of which have analogs in the
digital case. We demonstrate that copying, which is a trivial operation in digital comput￾ing, is not generally achievable in the quantum case. The quantum concept of teleportation,
which has no analog in digital computing, is also introduced.
20.2 Quantum parallelism
For f : B → B, consider the circuit for Uf |xi|yi = |xi|y ⊕ f(x)i, extended linearly to the
whole of C
2 ⊗ C
2
, with the inputs shown in Figure 20.1.
Uf
|0
1
√
2
(|0 + |1 )
Uf (
1
√
2
(|0 + |1 )|0 )
FIGURE 20.1
Quantum parallelism.
Uf (
1
√
2
(|0i + |1i)|0i) = 1
√
2
(Uf |0i|0i + Uf |1i|0i)
=
1
√
2
(|0i|f(0)i + |1i|f(1)i).
That is, the input of a single state 1
√
2
(|0i+|1i)⊗|0i ∈ C
2⊗C
2
to the ‘gate’ to evaluate f has
given rise to two evaluations, f(0) and f(1), of f. This is known as quantum parallelism
and occurs because superpositions of states are also valid states in quantum mechanics. The
two input registers hold |0i ∈ C
2 and 1
√
2
(|0i + |1i) ∈ C
2
; the latter could be generated, for
example, by the Hadamard gate (see Section 21.4). We should also note that if f(0) 6= f(1)
then the output state is entangled (see Section 10.3 on tensor products).
DOI: 10.1201/9781003264569-20 305306 Quantum information processing 3
20.3 Qubit swapping
In certain quantum algorithms, e.g., the quantum Fourier transform (which is beyond the
scope of this book), it is necessary to swap qubits. In particular, a unitary operator that
maps the state
|φi|ψi ≡ (α1|0i + β1|1i) ⊗ (α2|0i + β2|1i)
to the state
|ψi|φi ≡ (α2|0i + β2|1i) ⊗ (α1|0i + β1|1i)
is required. Much as in the digital case, we define a function S on the basis states B⊗2C2 of
C
2 ⊗ C
2 by
S|xi|yi = |yi|xi where x, y ∈ B,
the linear extension of which to C
2⊗C
2
, also denoted S and referred to as the swap operator,
is unitary and has the matrix representation:


1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1

 .
It is straightforward to show that for a general qubit pair, i.e., |φi and |ψi, as defined above,
we obtain:
S|φi|ψi = |ψi|φi,
i.e., S swaps the qubits as required. Figure 20.2 illustrates S on the basis states B⊗2C2 ;
and the linear extension of S, to C
2 ⊗ C
2
, showing the action of S in the general case is
represented in Figure 20.3.
|x
|y
×
×
|y
|x
FIGURE 20.2
The swap operator on B⊗2C2 .
×
×
α1|0 + β1|1
α2|0 + β2|1 α1|0 + β1|1
α2|0 + β2|1
FIGURE 20.3
The linearly extended swap operator on C
2 ⊗ C
2
.
Suitable generalisations of S to swap pairs of qubits in the general qubit space ⊗nC
2 are
easily defined.Quantum copying – the no-cloning theorem 307
20.4 Quantum copying – the no-cloning theorem
We compare the quantum case with the functions, introduced earlier, to copy in the digital
domain.
The quantum equivalent of the classical dupe function (i, i) : B → B
2 defined by
(i, i)(x) = (x, x) is the function U : {|0i, |1i} → C
2 ⊗ C
2 defined by
U|xi = |xi|xi for x ∈ B.
That is, U is such that
U|0i = |0i|0i
and
U|1i = |1i|1i.
Clearly U copies the states |0i and |1i. However, the following is true:
Lemma 20.1 The linear extension of U to C
2
, defined by
U(α0|0i + α1|1i) ≡ α0U|0i + α1U|1i,
does not copy the general quantum state, α0|0i + α1|1i, of C
2
.
Proof
In general α0U|0i + α1U|1i 6= (α0|0i + α1|1i) ⊗ (α0|0i + α1|1i). Equality only occurs
in the cases α0 = 1, α1 = 0 and α0 = 0, α1 = 1 for which we have U|0i = |0i|0i and
U|1i = |1i|1i.

We conclude that while the classical bit-copying function (i, i)(x) = (x, x) is easily extended
to copy a general digital register (see Figure 2.28) this is not the case for the quantum
equivalent.
We have also seen that the digital cnot function enables arbitrary Boolean strings to be
copied – see Figure 2.35. The quantum cnot operator, Ucnot|xi|yi = |xi|y ⊕ xi copies the
states |0i and |1i; i.e., we have Ucnot|0i|0i = |0i|0i and Ucnot|1i|0i = |1i|1i. However, the
linear extension of Ucnot to C
2 ⊗ C
2 gives:
Ucnot(α0|0i + α1|1i)|0i = α0Ucnot|0i|0i + α1Ucnot|1i|0i
= α0|0i|0i + α1|1i|1i
6= (α0|0i + α1|1i) ⊗ (α0|0i + α1|1i)
and the input state α0|0i + α1|1i is not duplicated. The following Lemma explains why our
attempts to clone general quantum states has not been successful.
Lemma 20.2 Let V be a finite dimensional vector space, then
1. the mapping C1 : V → V ⊗ V defined by C1|φi = |φi|φi is non-linear,
2. the mapping C2 : V ⊗ V → V ⊗ V defined by C2|φi|φ0i = |φi|φi, for any
constant |φ0i ∈ V , is non-linear.
Proof
1. We have C1(|φi + |χi) = (|φi + |χi)(|φi + |χi) = |φi|φi + |χi|χi + |φi|χi + |χi|φi
and C1|φi + C1|χi = |φi|φi + |χi|χi. Hence C1(|φi + |χi) 6= C1|φi + C1|χi and
C1 is not linear.
2. Similarly C2(|φi + |χi)|φ0i 6= C2(|φi|φ0i) + C2(|χi|φ0i).
308 Quantum information processing 3
As quantum processes are unitary (and hence linear) the Lemma tells us that a general quan￾tum cloning operator does not exist. In compensation for not being able to copy quantum
states (the no-cloning theorem), quantum theory enables us to reproduce a state wherever
we like, at the expense of losing the original. This is what has become known as quantum
teleportation. The process depends, fundamentally, on quantum entanglement. Teleporta￾tion enables us to demonstrate measurement in both the computational and Bell bases, to
achieve the same outcome.
20.5 Quantum teleportation 1, computational-basis measurement
Alice has a qubit |ψi = α|0ic + β|1ic in her possession that she wishes to convey to Bob.
For a classical state Alice could measure its properties and send them to Bob, who could
then reproduce it. However, if Alice measures the quantum state |ψi, the state may change
and a different approach is required.
The objective of teleportation is to transmit |ψi to Bob using only classical information;
i.e., bits. The teleportation process requires that Alice and Bob share an entangled state,
which is fixed in advance by agreement between them. We assume this state to be:
|φi =
1
√
2
(|0ia|0ib + |1ia|1ib)
of which each has a qubit which we distinguish by the subscripts a and b. This can be
implemented by preparing the particles together and firing them to Alice and Bob from
the same source. The subscripts a and b in the entangled state refer to Alice’s and Bob’s
particles. The combined state of the particles held by Alice is therefore
|ψi ⊗ |φi =
1
√
2
(α|0ic + β|1ic) ⊗ (|0ia|0ib + |1ia|1ib)
=
1
√
2
(α|0ic|0ia|0ib + α|0ic|1ia|1ib + β|1ic|0ia|0ib + β|1ic|1ia|1ib)
to which she applies the unitary operators cnot ⊗ I and H ⊗ I ⊗ I to obtain:
(H ⊗ I ⊗ I)(cnot ⊗ I)
1
√
2
(α|0ic|0ia|0ib + α|0ic|1ia|1ib + β|1ic|0ia|0ib + β|1ic|1ia|1ib)
= (H ⊗ I ⊗ I)
1
√
2
(α|0ic|0ia|0ib + α|0ic|1ia|1ib + β|1ic|1ia|0ib + β|1ic|0ia|1ib)
=
1
√
2
(αH(|0ic)|0ia|0ib + αH(|0ic)|1ia|1ib + βH(|1ic)|1ia|0ib + βH(|1ic)|0ia|1ib)
=
1
√
2
￾
αH(|0ic)[|0ia|0ib + |1ia|1ib] + βH(|1ic)[|1ia|0ib + |0ia|1ib]

=
1
2
￾
α(|0ic + |1ic)[|0ia|0ib + |1ia|1ib] + β(|0ic − |1ic)[|1ia|0ib + |0ia|1ib]

=
1
2
￾
α|0ic|0ia|0ib + α|0ic|1ia|1ib + α|1ic|0ia|0ib + α|1ic|1ia|1ib
+ β|0ic|1ia|0ib + β|0ic|0ia|1ib − β|1ic|1ia|0ib − β|1ic|0ia|1ib

=
1
2
(|0ic|0ia[α|0ib + β|1ib] + |0ic|1ia[α|1ib + β|0ib]
+ |1ic|0ia[α|0ib − β|1ib] + |1ic|1ia[α|1ib − β|0ib]
Quantum teleportation 1, computational-basis measurement 309
Alice now measures the observable A ⊗ I on the first qubit and the observable I ⊗ A on the
second qubit of the transformed state. Here A has the matrix representation
A =

0 0
0 1 
.
As we have seen earlier A has eigenvector |0i with eigenvalue 0 and eigenvector |1i with
eigenvalue 1.
Alice communicates, using a classical channel (e.g., text message, Morse code signal), the
results of her measurements; i.e., the two classical bits of information ((0, 0),(0, 1),(1, 0), or
(1, 1)) obtained from her measurements. It follows that if Alice’s measurements of A result
in:
• (1, 1), then the post-measurement state of Bob’s particle is α|1ib − β|0ib and he applies
the gate:
V11 =

0 1
−1 0 
to transform his particle to the one Alice wished to transmit,
• (1, 0), then the post-measurement state of Bob’s particle is α|0ib − β|1ib and he applies
the gate:
V10 =

1 0
0 −1

,
• (0, 1), then the post-measurement state of Bob’s particle is α|1ib + β|0ib and he applies
the gate:
V01 =

0 1
1 0 
,
• (0, 0), then the post-measurement state of Bob’s particle is α|0ib + β|1ib and no transfor￾mation of his particle is necessary – equivalently V00 is the 2 × 2 identity matrix.
Following the measurements Alice is left with one of the states |11i, |10i, |01i, |00i as shown
in Figure 20.4 – where vertical bars are used to denote that the state |φi is entangled.
|ψ |
Vxy
|ψ
1
√
2
(|0 a|0 b + |1 a|1 b)
x ∈ {0, 1} y ∈ {0, 1}
classical channel from
Alice to Bob
Alice T
Bob 
xy
|φ =
FIGURE 20.4
A quantum circuit to teleport the state |ψi = α|0i + β|1i from Alice to Bob, where T =
(H ⊗ I ⊗ I)(cnot ⊗ I).
For each qubit teleported, Alice sends Bob two classical bits of information. If a hacker
intercepts the two bits, they know what Bob needs to do to ‘receive’ his state. However,
this information is useless as the hacker cannot interact with Bob’s particle.310 Quantum information processing 3
20.6 Quantum teleportation 2, Bell-basis measurement
Again Alice wishes to send the qubit state |ψi = α|0ic+β|1ic to Bob. As before the subscript
c is used to distinguish it from states labelled a and b below, and we assume that Alice and
Bob share the first of the following entangled states:
|B1iab =
1
√
2
(|0ia|0ib + |1ia|1ib)
|B2iab =
1
√
2
(|0ia|1ib + |1ia|0ib)
|B3iab =
1
√
2
(|0ia|0ib − |1ia|1ib)
|B4iab =
1
√
2
(|0ia|1ib − |1ia|0ib).
Alice has one of the particles of |B1iab (labelled a), Bob has the other (labelled b). Hence
at this point Alice has two particles, i.e., c (the one she wishes to teleport to Bob) and a
(one of the entangled pair a, b). Bob has one particle, namely b. The state vector for the
compound system is therefore:
|ψi ⊗ |B1iab = (α|0ic + β|1ic) ⊗
1
√
2
(|0ia|0ib + |1ia|1ib)
and Alice will make a local measurement, in the Bell basis, on the two particles in her
possession (i.e., c and a).
To assist the interpretation of Alice’s measurements, we write her qubits as superposi￾tions of the Bell states. We have:
|ψi ⊗ |B1iab = (α|0ic + β|1ic) ⊗
1
√
2
(|0ia ⊗ |0ib + |1ia ⊗ |1ib)
=
1
√
2
[α(|0ic ⊗ |0ia) ⊗ |0ib + α(|0ic ⊗ |1ia) ⊗ |1ib
+ β(|1ic ⊗ |0ia) ⊗ |0ib + β(|1ic ⊗ |1ia) ⊗ |1ib]
and the following identities can be substituted, with subscripts c and a, for the terms in
emboldened braces
|0i ⊗ |0i =
1
√
2
(|B1i + |B3i), |0i ⊗ |1i =
1
√
2
(|B2i + |B4i),
|1i ⊗ |0i =
1
√
2
(|B2i − |B4i), |1i ⊗ |1i =
1
√
2
(|B1i − |B3i).
We obtain
|ψi ⊗ |B1iab =
1
2
|B1ica ⊗ (α|0ib + β|1ib) + 1
2
|B2ica ⊗ (α|1ib + β|0ib)
+
1
2
|B3ica ⊗ (α|0ib − β|1ib) + 1
2
|B4ica ⊗ (α|1ib − β|0ib).Quantum teleportation 2, Bell-basis measurement 311
We note that the above is just an alternative expression for |ψi ⊗ |B1iab, achieved by a
change-of-basis on Alice’s part of the system. No operations have been performed and the
three particles are still in the same total state. Note that Alice’s particles, c and a, are
now entangled in Bell states, and the entanglement initially shared by Alice’s and Bob’s
particles a and b is broken.
Teleportation takes place when Alice measures the observables S
2
1 and S
2
3 on her two
qubits, c and a, in the Bell basis
|B1ica, |B2ica, |B3ica, |B4ica.
This process is described in Section 19.5 and now applied to the first two qubits of |ψi ⊗
|B1iab. The result is that |ψi ⊗ |B1iab collapses to one of the following four states:
• |B1ica ⊗ (α|0ib + β|1ib) if Alice measures (1, 1), resulting in final state |B1i,
• |B3ica ⊗ (α|0ib − β|1ib) if Alice measures (1, 0), resulting in final state |B3i,
• |B2ica ⊗ (α|1ib + β|0ib) if Alice measures (0, 1), resulting in final state |B2i,
• |B4ica ⊗ (α|1ib − β|0ib) if Alice measures (0, 0), resulting in final state |B4i.
In the above, each of the four possible states of Bob’s qubit are related to the state Alice
wishes to teleport by the following unitary operators:

1 0
0 1 
,

1 0
0 −1

,

0 1
1 0 
,

0 1
−1 0 
.
If Bob receives:
• (1, 1), he knows his qubit is precisely the qubit Alice wished to teleport and he need do
nothing.
• (1, 0), he applies the gate
U10 =

1 0
0 −1

to his qubit, which is then the state Alice wished to teleport.
• (0, 1), he applies the gate
U01 =

0 1
1 0 
to his qubit.
• (0, 0), he applies the gate
U00 =

0 1
−1 0 
to his qubit.
Bob now has the state |ψi Alice wished to teleport to him. Alice is left with one of
|B1ica, |B2ica, |B3ica, |B4ica from which the state |ψi cannot be reconstructed.
A circuit for the teleportation process is shown in Figure 20.5. The B in the measurement
graphic indicates measurement of the first two qubits of the input state
|ψi ⊗ 1
√
2
(|0ia|0ib + |1ia|1ib)312 Quantum information processing 3
in the Bell basis. We note, in Figure 20.5, that the subscript q = 1 if xy = 11, q = 3 if
xy = 10, q = 2 if xy = 01 and q = 4 if xy = 00.
|ψ |
Uxy
|ψ
1
√
2
(|0 a|0 b + |1 a|1 b)
x ∈ {0, 1} y ∈ {0, 1}
classical channel from
Alice to Bob
Alice B B Bq
Bob 
FIGURE 20.5
A quantum circuit to teleport the state |ψi = α|0i + β|1i from Alice to Bob.21
More on quantum gates and circuits: those without
digital equivalents
21.1 Objectives
To this point only quantum gates derived from invertible digital counterparts or general
invertible Boolean functions have been considered. Here we introduce a number of quantum
gates without digital equivalents. In particular, we consider the Pauli gates, the √
not gate,
the phase-shift gate and the Hadamard gate, some of which occur in quantum algorithms
introduced later. These gates impart additional power to quantum computation.
21.2 General 1-qubit quantum gates
Definition 21.1 1-qubit gate
A 1-qubit gate is a transformation U : C
2 → C
2
for U ∈ U(2), where U(2) is the unitary
group on C
2
.
It follows from the definition that all 1-qubit gates are represented by matrices of the form
U =

u00 u01
u10 u11 
where uij ∈ C and UU† = I2, the 2 × 2 identity matrix. U maps |0i which is represented by

1
0

to u00|0i + u10|1i. Similarly |1i, which is represented by

0
1

is mapped by U to u01|0i + u11|1i. The general 1-qubit |ψi = c0|0i + c1|1i therefore trans￾forms, under U, to:
U|ψi = c0(u00|0i + u10|1i) + c1(u01|0i + u11|1i)
= (c0u00 + c1u01)|0i + (c0u10 + c1u11)|1i.
The specific 1-qubit gates, discussed below, are special cases and their actions on qubits
follow from this general expression. The matrix representations, as linear combinations of
the Pauli matrices and the identity matrix, follow from the general expressions discussed
earlier.
DOI: 10.1201/9781003264569-21 313314 More on quantum gates and circuits: those without digital equivalents
Following the digital case, quantum gates may be represented graphically. A 1-qubit gate
is depicted as shown in Figure 21.1, where x ∈ {0, 1} and the time-axis runs horizontally.
|x U U|x
FIGURE 21.1
A generic 1-qubit quantum gate.
21.3 The Pauli and the √
not 1-qubit gates
1. The Pauli X gate, or the quantum not gate:
We have met this gate earlier. The Pauli gate has a parallel in the digital case
and is included again here for two reasons:
(a) for completeness with the other two Pauli gates,
(b) the square root, written √
not, of the not gate exists as a useful quantum
gate.
The other two Pauli gates and the quantum √
not have no parallels in digital
computation. The matrix of the not is the first Pauli matrix σ1, sometimes also
denoted X, i.e.,

0 1
1 0 
= σ1.
The gate takes |0i to |1i and |1i to |0i.
2. The Pauli Y gate:

0 −i
i 0 
= σ2.
The gate takes |0i to i|1i and |1i to −i|0i.
3. The Pauli Z gate:

1 0
0 −1

= σ3.
The gate takes |0i to |0i and |1i to −|1i.
4. The √
not gate: as a further divergence from digital computation we note that
[
1
2
(1 + i)(I2 − iσ1)]2 = σ1,
and hence the quantum not gate has a square root, which is unitary and hence
a quantum gate. That is, it is an element of the group U(2), denoted by √
not.
From the above, the matrix of √
not is:
1
2
(1 + i)(I2 − iσ1) = 1
2
(1 + i) 
1 −i
−i 1 
.
The quantum √
not gate is depicted as shown in Figure 21.2.Further 1-qubit gates: phase-shift and Hadamard 315
|0
√ (1 + i)|0 + (1 − i)|1
2
|1
√ (1 − i)|0 + (1 + i)|1
2
not not
FIGURE 21.2
The quantum √
not gate.
21.4 Further 1-qubit gates: phase-shift and Hadamard
1. The phase-shift gate: a phase-shift gate fixes |0i but shifts the phase of |1i by eiφ
and may be represented by the matrix
Rφ =

1 0
0 eiφ

.
We note that Rφ may be expressed in terms of the Pauli matrices:

1 0
0 eiφ

= ei
1
2
φ

e
−i
1
2
φ 0
0 ei
1
2
φ

=
1
2
(1 + eiφ
)I2 +
1
2
(1 − e
iφ
)σ3.
and is also readily implemented physically. The gate is represented diagrammat￾ically in Figure 21.3.
e
iφ
|0 Rφ |0 |1 Rφ |1
FIGURE 21.3
The phase-shift gate.
2. Another useful, and easily implemented, gate is the Hadamard gate with matrix
H =
1
√
2

1 1
1 −1

.
H is the 1-qubit Fourier transform gate (Figure 21.4). We note that this can be
expressed in terms of the Pauli matrices as
H =
1
√
2

1 1
1 −1

=
1
√
2
(σ1 + σ3).
|0 H |1 H
|0 + |1
√
2
|0 |1
√
2
FIGURE 21.4
The Hadamard gate.316 More on quantum gates and circuits: those without digital equivalents
21.5 Universal 1-qubit circuits
A set of 1-qubit gates is said to be universal if they generate all the unitary operators of C
2
.
Equivalently, the set is universal if an arbitrary 1-qubit state |ψi may be generated from
the set, starting from |0i. That is, for any |ψi ∈ C
2
there is a circuit, Cψ, of gates from the
set which can be used to generate it, as shown in Figure 21.5.
|0 Cψ
|ψ
FIGURE 21.5
A universal 1-qubit gate.
Lemma 21.1 The Hadamard and phase-shift gates are universal as shown in Figure 21.6,
where
|ψi = cos θ|0i + eiφ
sin θ|1i
up to a global phase factor.
|0 H R2θ H R π
2 +φ
|ψ
FIGURE 21.6
The universality of H and Rφ.
Proof
It is easy to show that the circuit, which is equivalent to the product, R π
2 +φHR2θH of
unitary matrices, generates
1
2
(1 + e2θi
)|0i +
1
2
ieiφ
(1 − e
2θi
)|1i
from |0i. We have e2θi = eθi
e
θi = (cos θ + i sin θ)
2
from which we obtain
1 + e2θi = 2(cos2
θ + i sin θ cos θ),
1 − e
2θi = 2(sin2
θ − i sin θ cos θ).
A little algebra then gives
(1 + e2θi
)|0i + ieiφ
(1 − e
2θi
)|1i = eiθ
(cos θ|0i + eiφ
sin θ|1i)
and the Lemma is true.

Equivalently, the vector |0i may be moved to any position on the surface of the Bloch sphere
(see Chapter 13) using only Hadamard and phase-shift gates.Some 2-qubit quantum gates 317
21.6 Some 2-qubit quantum gates
We note that the complete set of 1-qubit gates (discussed above), acting on both qubits,
augmented by the quantum cnot gate on C
2 ⊗ C
2 determine a universal set for 2-qubit
quantum gates. However, the proof of this is beyond the scope of this text; instead we
concentrate on 2-qubit gates that occur in the quantum algorithms we present later.
21.6.1 Tensor product gates
A 1-qubit gate U ∈ U(2) defines the 2-qubit gate U ⊗ U : C
2 ⊗ C
2 → C
2 ⊗ C
2 – see
Figure 21.7. It follows that if AU is a matrix representation of U then AU ⊗ AU is a matrix
representation of U ⊗ U.
|x U U|x
|y U U|y
|x
|y
|x = |1 ⊕ x
|y = |1 ⊕ y
notQ
notQ
e.g. for U = notQ
FIGURE 21.7
2-qubit gates U ⊗ U and notQ ⊗ notQ.
More generally if U and V are distinct 1-qubit gates then gates U ⊗V and V ⊗U determine
2-qubit gates. However, not all 2-qubit gates may be expressed as tensor products of 1-qubit
gates. An example of such is the cnot gate (see Exercises).
21.6.2 The Hadamard-cnot circuit
The Hadamard-cnot circuit is the product of the quantum cnotQ gate, written here as cnot,
and the tensor product gate H ⊗ I. H is the Hadamard gate discussed above:
H|0i =
1
√
2
(|0i + |1i) and H|1i =
1
√
2
(|0i − |1i),
and cnot : C
2 ⊗ C
2 → C
2 ⊗ C
2
is the 2-qubit operator defined by:
cnot(|0i|0i) = |0i|0i
cnot(|0i|1i) = |0i|1i
cnot(|1i|0i) = |1i|1i
cnot(|1i|1i) = |1i|0i.
We consider its algebraic, graphical and matrix representations below.
1. Algebraic representation: the circuit as a product of unitary matrices (quantum gates)
on the 2-qubit tensor product space C
2 ⊗ C
2
. We have:
cnot(H ⊗ I) : C
2 ⊗ C
2 → C
2 ⊗ C
2
.318 More on quantum gates and circuits: those without digital equivalents
H and cnot are linear on their respective domains, and I denotes the identity operator on
C
2
. Hence, for example:
cnot(H ⊗ I)(|0i|0i) = cnot￾ 1
√
2
(|0i + |1i) ⊗ |0i

=
1
√
2
￾
cnot(|0i ⊗ |0i) + cnot(|1i ⊗ |0i)

=
1
√
2
￾
|0i|0i + |1i|1i

.
We recognise the state 1
√
2
￾
|0i|0i + |1i|1i

∈ C
2 ⊗ C
2
as the entangled EPR-Bell state.
In general, we have H|xi =
1
√
2
(|0i + (−1)x
|1i) and:
cnot(H ⊗ I)(|xi|yi) = cnot(H(|xi) ⊗ |yi)
= cnot(
1
√
2
(|0i + (−1)x
|1i) ⊗ |yi
=
1
√
2
cnot(|0i|yi + (−1)x
|1i|yi)
=
1
√
2
(cnot|0i|yi + (−1)x
cnot|1i|yi)
=
1
√
2
(|0i|yi + (−1)x
|1i|yi).
From the above we obtain:
cnot(H ⊗ I)(|0i|0i) = 1
√
2
(|0i|0i + |1i|1i)
cnot(H ⊗ I)(|0i|1i) = 1
√
2
(|0i|1i + |1i|0i)
cnot(H ⊗ I)(|1i|0i) = 1
√
2
(|0i|0i − |1i|1i)
cnot(H ⊗ I)(|1i|1i) = 1
√
2
(|0i|1i − |1i|0i).
2. Graphical representation: for |xi ∈ C
2 and |yi ∈ C
2
consider Figure 21.8 where, as
before, for the orthonormal basis {|0i, |1i} of C
2
, H : C
2 → C
2
is the 1-qubit Hadamard
operator
H|0i =
1
√
2
(|0i + |1i) and H|1i =
1
√
2
(|0i − |1i),
and cnot : C
2 ⊗ C
2 → C
2 ⊗ C
2
is the 2-qubit operator:
cnot(|0i|0i) = |0i|0i
cnot(|0i|1i) = |0i|1i
cnot(|1i|0i) = |1i|1i
cnot(|1i|1i) = |1iSome 2-qubit quantum gates 319
| y
| x H
t0
t1
t2
|
|
1
√
2
(|0 |y + (−1)x
|1 |y )
time 
FIGURE 21.8
The Hadamard-cnot circuit for x, y ∈ B.
For x = y = 0 we have:
|0i|0i at time t0, (unentangled input tensor)
1
√
2
(|0i + |1i)|0i at time t1, (unentangled tensor)
1
√
2
￾
|0i|0i + |1i|1i

at time t2 (entangled output tensor).
Note that the output, in this case, is an entangled tensor. On the basis elements
{|0i|0i, |0i|1i, |1i|0i, |1i|1i} of C
2 ⊗ C
2
the outputs of the gate determine an orthonormal
entangled set of basis states of C
2 ⊗C
2 – known as the EPR or Bell states (see 1. Algebraic
representation, above). These states are very significant in quantum mechanics and quantum
computation. The vertical lines on the output ‘wires’ indicate that the outputs are entangled
and the output tensor is placed between the two output ‘wires’ – as shown in Figure 21.8.
3. Matrix representation: the matrices of H ⊗ I and cnot, in the basis
{|0i|0i, |0i|1i, |1i|0i, |1i|1i} of C
2 ⊗ C
2 are respectively:
1
√
2


1 0 1 0
0 1 0 1
1 0 −1 0
0 1 0 −1


and


1 0 0 0
0 1 0 0
0 0 0 1
0 0 1 0

 .
The matrix of cnot(H ⊗ I) is therefore the product:
1
√
2


1 0 1 0
0 1 0 1
0 1 0 −1
1 0 −1 0

 .
From this we obtain:
cnot(H ⊗ I)|0i|0i =
1
√
2


1 0 1 0
0 1 0 1
0 1 0 −1
1 0 −1 0




1
0
0
0

 =


1
0
0
1

 =
1
√
2
(|0i|0i + |1i|1i)
etc320 More on quantum gates and circuits: those without digital equivalents
The input/output possibilities for gates are:
1. unentangled input and unentangled output (e.g., the Toffoli, Peres gates on
|xi|yi|zi for x, y, z ∈ B). For such, the wired input and wired output representa￾tions are fine,
2. unentangled input and entangled output (H ⊗ I followed by cnotQ, see above),
3. entangled input and entangled output (the unitary operator U|xi|yi = |xi|yi
maps |0i|0i + |1i|1i to |0i|1i + |1i|0i),
4. entangled input and unentangled output (the inverse of (2) i.e., cnotQ followed
by H ⊗ I).
21.7 The Hadamard gate on n-qubit registers
The Hadamard gate features in most known quantum algorithms, we therefore re-visit it in
its most general form. The Hadamard gate H : C
2 → C
2
, on C
2
, may be expressed as
H|xi =
1
√
2
X
y∈{0,1}
(−1)xy|yi
where |xi ∈ C
2
, |yi ∈ C
2
, and xy = x · y. We note that for x = 0 we obtain
H|0i =
1
√
2
(|0i + |1i),
and for x = 1 we have
H|1i =
1
√
2
(|0i − |1i)
as required.
More generally, we can define H⊗n :
Nn C
2 →
Nn C
2
.
Lemma 21.2 For |xi = |x1x2 . . . xni ∈ ⊗nC
2 we have
H⊗n
|xi =
On
i=1
￾ 1
√
2
X
1
yi=0
(−1)xiyi
|yii

.
Proof
H⊗n
|xi = H|x1iH|x2i · · · H|xni, (by definition)
=
1
√
2
(|0i + (−1)x1
|1i)
1
√
2
(|0i + (−1)x2
|1i). . .
1
√
2
(|0i + (−1)xn |1i)
=
On
i=1
1
√
2
(|0i + (−1)xi
|1i)
=
On
i=1
￾ 1
√
2
X
1
yi=0
(−1)xiyi
|yii

.

We also havThe Hadamard gate on n-qubit registers 321
Lemma 21.3
H⊗n
|xi =
1
2
n
2
2
Xn−1
y=0
(−1)x·y
|yi.
Proof
By induction: for n = 1 the right hand side of the above is
H|xi =
1
√
2
X
y∈{0,1}
(−1)xy|yi
which is correct for x ∈ B. Assuming the relation is true for n, we demonstrate that the
truth for n + 1 is implied and the expression is therefore generally true. By definition we
have
H⊗(n+1) = H⊗n ⊗ H.
For |xi ∈ ⊗nC
2 and |zi ∈ C
2 we have |xi|zi ∈ ⊗n+1C
2 and H⊗(n+1)|xi|zi = H⊗n|xi⊗H|zi.
For z = 0 we obtain:
H⊗(n+1)|xi|0i = H⊗n
|xi ⊗ H|0i
=
1
2
n
2
2
Xn−1
y=0
(−1)x·y
|yi
1
√
2
(|0i + |1i)
=
1
2
n+1
2
￾
2
Xn−1
y=0
(−1)x·y
|yi(|0i + |1i)

=
1
2
n+1
2
￾
2
Xn−1
y=0
(−1)x·y
|y0i + |y1i

=
1
2
n+1
2
￾
2
nX+1−1
q=0
(−1)x0·q
|qi

where q = yp for p ∈ B.
Similarly for z = 1, and the Lemma is true.

Exercises
21.1 Show that the quantum cnot gate cannot be written as a tensor product of two
1-qubit gates.
21.2 Show that
H⊗(n+1)(⊗
n
|0i |1i) = 1
√
2
n+1
X
x∈{0,1}n
|xi(|0i − |122
Quantum algorithms 1
22.1 Objectives
The quantum algorithms covered in this chapter, i.e., the Deutsch, Deutsch-Jozsa and
Bernstein-Vazirani algorithms, have been chosen for their relative simplicity and theoreti￾cal interest. They do not address problems of real practical value, their purpose being to
demonstrate, using simple examples, the great potential power of quantum computation
relative to the digital case. The most significant breakthrough in quantum computing is
Shor’s integer factorisation algorithm. Shor’s algorithm is beyond the scope of this intro￾ductory text, but its significance lies in the fact that it demonstrates the potential to factor
large integers efficiently. No efficient digital algorithms for factoring large integers exist, and
many current encryption algorithms rely on this being the case. Shor’s algorithm therefore
threatens current encryption methods, and is responsible for much of the recent accelerated
interest in quantum computation.
22.2 Preliminary lemmas
Lemma 22.1 If f : B → B, then in C
2 we have:
|f(x)i − |1 ⊕ f(x)i = (−1)f(x)
(|0i − |1i).
Proof
As we have seen (Section 2.4) there are four functions of the type f : B → B; two are
balanced and two are constant.
(i) For f balanced.
(a) if f(x) = x we have
|f(x)i − |1 ⊕ f(x)i = |xi − |1 ⊕ xi
=
(
|0i − |1i for x = 0 = f(0)
|1i − |0i for x = 1 = f(1)
= (−1)f(x)
(|0i − |1i).
DOI: 10.1201/9781003264569-22 323324 Quantum algorithms 1
(b) if f(x) = x we have
|f(x)i − |1 ⊕ f(x)i = |xi − |1 ⊕ xi
= |xi − |xi
=
(
|1i − |0i for x = 0, f(0) = 1
|0i − |1i for x = 1, f(1) = 0
= (−1)f(x)
(|0i − |1i).
(ii) For f constant.
(a) if f(x) = 1 we have
|f(x)i − |1 ⊕ f(x)i = |1i − |0i
= (−1)f(x)
(|0i − |1i).
(b) if f(x) = 0 we have
|f(x)i − |1 ⊕ f(x)i = |0i − |1i
= (−1)f(x)
(|0i − |1i)
hence the result.

Lemma 22.2 If p ∈ B
n and x ∈ B
n then for p 6= 0n we have
2
Xn−1
x=0
p · x = 2n−1
.
Proof
Clearly either p · x = 0 or p · x = 1. The Lemma states that for 2n−1 values of x we have
p · x = 1 and for the other 2n−1 values of x we have p · x = 0.
Consider the case p = 100 · · · 0 ∈ B
n: then for all x ∈ B
n of the form x = 0x
0
, where
x
0 ∈ B
n−1
is arbitrary, we have p · x = 0. There are 2n−1
strings of the form 0x
0
in B
n.
Strings x not of the form x = 0x
0
, are such that x = 1x
0
, with x
0 arbitrary. For these we
have p · x = 1. A similar argument holds for all other strings, p ∈ B
n, of Hamming weight 1.
Very similar arguments hold for strings p ∈ B
n of Hamming weights 2, 3, . . . , n. The Lemma
is therefore true.

Corollary
For p ∈ B
n and x ∈ B
n we have
2
Xn−1
x=0
(−1)p·x = 2n
for p = 0n
and
2
Xn−1
x=0
(−1)p·x = 0 for p 6= 0n
.Diagrammatic representation of the measurement process 325
Proof
For p = 0n each term of the sum is 1. For p 6= 0, the Lemma shows that half the terms are
1 and half are −1 – the sum is therefore zero.

22.3 Diagrammatic representation of the measurement process
In the graphical representations of quantum circuits and algorithms the measurement pro￾cess, which we have in algebraic expressions denoted µQ, is usually represented by a meter
– as shown in Figures 22.1 and 22.2. The real output channel producing the output λ ∈ R
is indicated in Figure 22.2 by the pair of parallel lines.
M
FIGURE 22.1
Meter representation of quantum measurement.
M
|ψ λ∈ R
FIGURE 22.2
Measurement of an observable on the state |ψi producing the output λ ∈ R.
22.4 An introduction to computational complexity
Computational complexity (or complexity theory) is, with computability, one of the great
achievements of the study of automated computation. Its roots date back to the Turing
era, and it currently intertwines classical and quantum computation in a fascinating way.
Complexity theory is a vast topic, and we consider only the minimum required to assess the
potential performance improvements offered by the quantum algorithms we discuss later.
Complexity theory is concerned with estimating, or bounding, the resources required to
compute solutions to particular problems. Both space (e.g., memory) and execution-time are
resources that may be considered. Here we are only concerned with execution-time bounds
– which are often a function of the problem ‘size’. For example, consider the problem of
sorting a set of n integers into ascending order. One would expect the time to sort a set of
1000 integers to be greater than that for sorting 10 integers. In this case the problem size326 Quantum algorithms 1
is n, the number of integers to be sorted, and we denote the execution time by t(n). That
is, we have a function t : N → R which we refer to as the time-complexity function of the
computation. In general t is an increasing function of n.
The simplest sorting algorithms are such that t is quadratic, i.e., t takes the form t(n) =
k1n
2
for some constant k1 and the execution time grows quadratically with respect to
problem size n. The most efficient (known) digital sorting algorithms have execution times
proportional to n log(n) i.e., t(n) = k2n log(n) for some constant k2. The constants are,
generally, not known and will depend on the implementation of the algorithm and on the
hardware used. This being so it is usual to say that an algorithm is O(n
2
) if t(n) = k1n
2
,
and O(n log(n)) if t(n) = k2n log(n) etc., when discussing the execution-time complexity.
The notation being used here is referred to as big-O notation. When the execution time is
independent of n, i.e., t is constant, we write O(1) for its complexity class; O(2n) problems
are said to be ‘hard’.
22.5 Oracle-based complexity estimation
The algorithms we discuss in this chapter use oracles. We recall that oracles are black-boxes,
or virtual devices, that respond immediately to queries – irrespective of the computation
that may be necessary to ‘answer’ the query. Essentially, we assume that both quantum and
classical oracles have this abstract property and that it is therefore acceptable to compare
their respective efficiencies (i.e., complexities), by counting the number of oracle queries
required, in both the classical and quantum algorithms, to solve a particular problem.
22.6 Deutsch’s algorithm
Deutsch’s algorithm determines whether a classical function f : B → B is constant or
balanced. The possibilities for f are (i) f(0) = f(1) = 0, (ii) f(0) = f(1) = 1, i.e., f is
constant, or (iii) f(0) = 1, f(1) = 0 or (iv) f(0) = 0, f(1) = 1 (i.e., f is balanced).
We note that if f(0) ⊕ f(1) = 0 then f is constant and if f(0) ⊕ f(1) = 1 then f is
balanced (see Example 2.4.2).
We begin with the two-qubit state |0i ⊗ |1i and apply the tensor product Hadamard
transform H ⊗ H to obtain:
1
2
(|0i + |1i)(|0i − |1i).
We assume we have a quantisation of the function f; i.e., an implementation of the unitary
operator Uf |xi|yi = |xi|y ⊕ f(x)i which delivers values for f(x) when queried.
We apply Uf to
1
2
(|0i + |1i)(|0i − |1i) to obtain:
Uf (
1
2
(|0i + |1i)(|0i − |1i)) = 1
2

|0i (|0 ⊕ f(0)i − |1 ⊕ f(0)i) + |1i (|0 ⊕ f(1)i − |1 ⊕ f(1)i)

=
1
2

|0i (|f(0)i − |1 ⊕ f(0)i) + |1i(|f(1)i − |1 ⊕ f(1)i)

.Deutsch’s algorithm 327
Then, from Lemma 22.1, we have
Uf (
1
2
(|0i + |1i)(|0i − |1i)) = 1
2

|0i(−1)f(0)(|0i − |1i) + |1i(−1)f(1)(|0i − |1i)

=
1
2

|0i(−1)f(0) + |1i(−1)f(1)
(|0i − |1i)
= (−1)f(0) 1
2

|0i + (−1)f(0)⊕f(1)|1i

(|0i − |1i)
= (−1)f(0) 1
√
2

|0i + (−1)f(0)⊕f(1)|1i
 1
√
2
(|0i − |1i)
=
1
√
2

|0i + (−1)f(0)⊕f(1)|1i
 1
√
2
(|0i − |1i).
(Note that global phase factors ((−1)f(0) in this case) can be ignored in quantum mechanics).
Applying H ⊗ I, where H is the Hadamard operator, transforms the state to:
1
2
[|0i + |1i + (−1)f(0)⊕f(1)|0i − (−1)f(0)⊕f(1)|1i]
1
√
2
(|0i − |1i)
which is equal to
1
2
[(1 + (−1)f(0)⊕f(1))|0i + (1 − (−1)f(0)⊕f(1))|1i]
1
√
2
(|0i − |1i).
The first qubit of the state above is not a superposition; because f(0) ⊕ f(1) is either 0 or
1 the state is therefore either |0i or |1i before measurement. In fact we have:
1
2
[(1 + (−1)f(0)⊕f(1))|0i + (1 − (−1)f(0)⊕f(1))|1i] = |f(0) ⊕ f(1)i
and the state is, therefore, equal to:
|f(0) ⊕ f(1)i
1
√
2
(|0i − |1i).
Hence, if 0 is measured for the observable:
A =

0 0
0 1 
on the first qubit, it follows that f(0) ⊕ f(1) = 0 and is f constant, and if 1 is measured
then f(0) ⊕ f(1) = 1 and f is balanced. We note that after the application of the operator
Uf the second qubit, 1
√
2
(|0i − |1i), could have been ignored.
Mathematically the algorithm may be expressed as:
µQ(H ⊗ I)(Uf (H ⊗ H))|0i ⊗ |1i,
where µQ denotes measurement of the first qubit. A circuit for the algorithm is shown in
Figure 22.3.
Complexity: digitally we compute:
if f(0) = f(1) then f constant else f balanced
i.e., two queries to an oracle for f are required. In the quantum case only one query is
required – i.e., one application of the operator Uf is sufficient to determine the nature of f.
If the evaluation of f is the primary ‘overhead’ then the quantum algorithm is 50% more
‘efficient’. The next algorithm shows that a more significant improvement in efficiency, over
a digital equivalent, is possible with a quantum algorithm.328 Quantum algorithms 1
|0
|1
H
H
H
Uf
|χ
|f(0) ⊕ f(1)
|0 |1 |ψ |χ
Uf |ψ |χ
FIGURE 22.3
A circuit for Deutsch’s algorithm, where |ψi = √
1
2
(|0i + |1i) and |χi = √
1
2
(|0i − |1i).
22.7 The Deutsch-Jozsa algorithm
The Deutsch-Jozsa algorithm generalises Deutsch’s algorithm to the functions f : B
n → B.
That is, we have an oracle for f : B
n → B, which is guaranteed to be either constant or
balanced; the purpose of the Deutsch-Jozsa algorithm is to decide which it is by querying
the oracle for f.
The initial state for the algorithm is (⊗n|0i)|1i; i.e., the first n qubits are in state |0i and
the last qubit is in state |1i. The Hadamard gate is then applied to each qubit to obtain,
using Lemma 21.3, the state
|ψ1i =
1
√
2
n+1
X
x∈{0,1}n
|xi(|0i − |1i).
The function f is available as a quantum oracle; it maps |xi|yi to |xi|y ⊕ f(x)i. Applying
the oracle to |ψ1i produces the state
|ψ2i =
1
√
2
n+1
X
x∈{0,1}n
|xi(|f(x)i − |1 ⊕ f(x)i).
Given that for each x, f(x) is either 0 or 1 the above may be written, using Lemma 22.1,
as:
|ψ2i =
1
√
2
n+1
X
x∈{0,1}n
(−1)f(x)
|xi(|0i − |1i).
At this point the last qubit, i.e., 1
√
2
(|0i − |1i), can be ignored giving:
|ψ2i =
1
√
2
n
X
x∈{0,1}n
(−1)f(x)
|xi.
We now apply a Hadamard transform to each of the remaining qubits, using Lemma 21.3,
to obtainThe Deutsch-Jozsa algorithm 329
|ψ3i =
1
2
n
X
x∈{0,1}n
(−1)f(x)
 X
y∈{0,1}n
(−1)x·y
|yi

=
1
2
n
X
y∈{0,1}n
 X
x∈{0,1}n
(−1)f(x)
(−1)x·y

|yi (∗)
where x · y = (x0 ∧ y0) ⊕ (x1 ∧ y1) ⊕ · · · ⊕ (xn−1 ∧ yn−1).
It is helpful to look at the case n = 2 in some detail; the general case is similar. For
n = 2 we have:
|ψ3i =
1
4
[(−1)f(00) + (−1)f(01) + (−1)f(10) + (−1)f(11)]|00i
+
1
4
[(−1)f(00) + (−1)f(01)(−1) + (−1)f(10) + (−1)f(11)(−1)]|01i
+
1
4
[(−1)f(00) + (−1)f(01) + (−1)f(10)(−1) + (−1)f(11)(−1)]|10i
+
1
4
[(−1)f(00) + (−1)f(01)(−1) + (−1)f(10)(−1) + (−1)f(11)]|11i.
It should be noted that, in the case where f is either constant or balanced, this is not
a superposition of quantum states; in fact, following a little arithmetic, we see that f is
constant if and only if
|ψ3i = |00i
and that f is balanced if and only if
|ψ3i = |01i, or |10i, or |11i.
Hence, at least in the case n = 2, the algorithm generates states that are not in superpo￾sition. Such states can be measured deterministically using an observable which is easily
determined to differentiate the outcomes by eigenvalue, and correct results are computed
deterministically. In particular measuring |ψ3i with the observable B ⊗ B where
B =

1 0
0 0 
,
we note that if the outcome is 1 then f is constant, and if the outcome is 0 then f is
balanced. These conclusions follow from the fact that |00i is an eigenvector of B ⊗ B with
eigenvalue 1 and that |01i, |10i and |11i are eigenvectors of B ⊗ B with eigenvalue 0.
For n > 2 the situation is similar. The coefficient of the vector |0i
⊗n
in the general case
is:
1
2
n
X
x∈{0,1}n
(−1)f(x)
which is of magnitude 1 if f is constant and 0 if f is balanced. Hence measuring the state
with the observable ⊗nB = B⊗B⊗· · ·⊗B (which has eigenvalues 1 and 0; with |00 · · · 0i as
eigenstate for 1, and all other basis elements |yi, for y ∈ {0, 1}
n, corresponding to eigenvalue
0) we conclude that if 1 is measured then f is constant and if 0 is measured it is balanced.
A circuit for this algorithm is shown in Figure 22.4.330 Quantum algorithms 1
|0
|1 H
Uf
|χ
|ψ |χ
H⊗n
|0
⊗n
|1
⊗n
H⊗n
FIGURE 22.4
A circuit for the Deutsch-Jozsa algorithm, where |ψi = √
1
2n
P
x∈{0,1}n |xi and |χi =
√
1
2
(|0i − |1i).
Complexity: the Deutsch-Jozsa algorithm may be thought to be O(n) due to the ap￾plication of the tensor product gate H⊗n. The question here is: does the tensor product
take n times longer to execute than the operator H on a single qubit? This is a quantum
mechanical issue beyond the scope of this book; however, it follows from the physics that
the n applications of H may be executed simultaneously without time penalty and the
complexity of the Deutsch-Jozsa algorithm is therefore O(1). Classical digital computation
would require 2n−1 + 1 evaluations of f to determine its nature and the classical algorithm
is therefore O(2n−1
).
22.8 The Bernstein-Vazirani algorithm
The Bernstein-Vazirani problem may be stated as follows: A function f : B
n → B is known
to be of the form f(x) = x · a for some a ∈ B
n; given an oracle for f, determine a. Here x · a
is defined to be x · a = (x1 ∧ a1) ⊕ (x2 ∧ a1) ⊕ · · · ⊕ (xn ∧ an).
A quantum algorithm follows the Deutsch-Jozsa algorithm computation as far as the
relation marked (*) in the discussion of that algorithm. At this point we have:
|ψ3i =
1
2
n
X
y∈{0,1}n
 X
x∈{0,1}n
(−1)f(x)
(−1)x·y

|yi (∗)
which has been obtained with a single call to a quantum oracle for f. For the Bernstein￾Vazirani algorithm, we have f(x) = x · a giving, in this case:
|ψ3i =
1
2
n
X
y∈{0,1}n
 X
x∈{0,1}n
(−1)x·a
(−1)x·y

|yi
=
1
2
n
2
Xn−1
y=0
|yi
2
Xn−1
x=0
(−1)x·a
(−1)x·y
.The Bernstein-Vazirani algorithm 331
We note that, using the Corollary to Lemma 22.2,
1
2
n
2
Xn−1
x=0
(−1)x·a
(−1)x·y =
1
2
n
2
Xn−1
x=0
(−1)x·(a+y)
=

1 if a = y, i.e., a + y = 0
0 if a 6= y.
Hence 1
2
n
2
Xn−1
x=0
(−1)x·a
(−1)x·y = δa,y and, from the relation (*) above, we obtain:
|ψ3i =
1
2
n
2
Xn−1
x=0
2
Xn−1
y=0
(−1)x·a
(−1)x·y
|yi
=
2
Xn−1
y=0
δa,y|yi
= |ai.
In the above δa,y is the function defined by δa,y = 1 if y = a and 0 otherwise.
Measurement 1:
We can measure |ai ∈ ⊗nC
2 qubit-by-qubit with the operators:
A ⊗ I ⊗ · · · ⊗ I measures |a1i with output a1 ∈ B
I ⊗ A ⊗ · · · ⊗ I measures |a2i with output a2 ∈ B
.
.
.
I ⊗ I ⊗ · · · ⊗ A measures |ani with output an ∈ B
and the string a = a1a2 · · · an is revealed. In the above A is represented by the matrix:
A =

0 0
0 1 
which is self-adjoint on C
2
.
Measurement 2:
Alternatively, we can measure |ai ∈ ⊗nC
2 with the observable,
2
Xn−1
z=0
z|zihz| : ⊗
nC
2 → ⊗nC
2
the outcome of which is an eigenvalue a ∈ {0, . . . , 2
n − 1}; the binary form of which is the
required string a ∈ B
n.
The matrix representation of the observable
2
Xn−1
z=0
z|zihz| : ⊗
nC
2 → ⊗nC
2
is diagonal,
having the form:


0 0 0 0 . . .
0 1 0 0
0 0 2 0
0 0 0 3
.
.
.
.
.
.
0 . . . . . . 0 2n − 1

332 Quantum algorithms 1
for which the eigenvalues are 0, 1, . . . , 2
n − 1 with eigenvectors |0 · · · 0i, . . . , |1 · · · 1i respec￾tively.
The circuit for the Bernstein-Vazirani algorithm is, essentially, the same as that for
the Deutsch-Jozsa algorithm, but with f : B
n → B defined by f(x) = x · a, rather than
f : B
n → B being either constant or balanced.
Complexity: Digitally, a single evaluation of a classical oracle for f can deliver at most
one of the values a1, . . . , an. That is, we have
f(100 · · · 0) = (1 ∧ a1) ⊕ (0 ∧ a2) ⊕ · · · ⊕ (0 ∧ an) = a1
f(010 · · · 0) = (0 ∧ a1) ⊕ (1 ∧ a2) ⊕ · · · ⊕ (0 ∧ an) = a2
.
.
.
f(00 · · · 01) = (0 ∧ a1) ⊕ (0 ∧ a2) ⊕ · · · ⊕ (1 ∧ an) = an,
and the digital algorithm is therefore O(n). The execution-time bound of the quantum
algorithm is O(1).23
Quantum algorithms 2: Simon’s algorithm
23.1 Objectives
Simon’s algorithm relates to the computation of the period of a Boolean function; some
aspects of periodicity are, therefore, covered in this chapter prior to consideration of the
algorithm.
23.2 Periodicity, groups, subgroups and cosets
23.2.1 Real-valued functions
If f : R → R and, for some P ∈ R \ 0 we have
f(x) = f(x + P)
for all x, then f is said to be periodic. The smallest P for which this is true is called the
period of f.
Observation 23.1 If f : R → R is periodic with period p then f(x) = f(x ± np) for all
n ∈ N.
Proof
If f(x + p) = f(x) for all x ∈ R then with x
∗ = x + p we have
f(x + 2p) = f(x
∗ + p)
= f(x
∗
)
= f(x + p)
= f(x) etc.

The following are true:
1. If f : R → R is constant, i.e., f(x) = c, for all x ∈ R, then f is periodic, but has
no period.
2. If f : R → R is one-to-one then f is not periodic.
3. If f : R → R is such that f(x+a) = f(x) for all x and for all a then f is constant.
We note that R is a commutative group under +. For p ∈ R, p > 0 we define
Zp = {0, ±np : n ∈ N}
= {0, ±p, ±2p, ±3p, . . .}.
DOI: 10.1201/9781003264569-23 333334 Quantum algorithms 2: Simon’s algorithm
For any given p, Zp is a subgroup of R; each Zp isomorphic to Z. Hence each Zp is a discrete
subgroup of R with infinitely many elements. We note that the group Zp should not be
confused with finite discrete groups Zk defined by addition mod k, for k ∈ {2, 3, 4, . . .}.
R/Zp denotes the coset space of R with respect to the subgroup Zp; the coset containing
x ∈ R is by definition: (see Sections 8.5 and 8.6)
x + Zp = {x, x ± p, x ± 2p, x ± 3p, . . .}
which we denote by [x]. We have:
[
0≤x<p
[x] = R
and for x 6= y, for x, y ∈ [0, p),
[x] ∩ [y] = ∅
where ∅ denotes the empty set.
Lemma 23.1 f : R → R has period 0 < p ∈ R if and only if it is constant on the cosets
R/Zp; f may, however, take the same value on distinct cosets. See Example 23.2.1.
Proof
This follows directly from the observation above and Example 23.2.1 below.

Example 23.2.1
The function f : R → R, f(x) = sin x, shown in Figure 23.1 is continuous and has period
p = 2π. Hence in this example we are concerned with the cosets of the subgroup Z2π in R.
We have:
Z2π = {0, ±2π, ±4π, . . . }
and the coset, determined by x ∈ R, is by definition:
x + Z2π = {x, x ± 2π, x ± 4π, . . .}
which is also denoted [x]. It follows that if y 6= x±2πn, for some n ∈ N, then [y]∩[x] = ∅. In
Figure 23.1 the x-values identified by the filled circles are elements of [x] and those identified
by the hollow circles are elements of [y] = [π − x]. We have [x] ∩ [y] = ∅, but we note that
the two distinct cosets are mapped to the same value by the sine function.
Example 23.2.2
The discontinuous function fd : R → R, shown in Figure 23.2 and defined by:
fd(x) =



x, 0 ≤ x < 1
x − 1, 1 ≤ x < 2
x − 2, 2 ≤ x < 3
.
.
.
.
.
.
has period p = 1. Each of the cosets R/Z1 maps to a distinct value in the range [0, 1).
Specifically the coset x + Z1 maps to x.
Conjecture: If f is continuous and periodic, with period p, then there will exist distinct
cosets in R/Zp on which the value of f is the same.Periodicity, groups, subgroups and cosets 335
1
-1
π 2π
x
sin x
0 x y 3π
FIGURE 23.1
Graph (not to scale) of the sine function, with y = π − x.
1 2
0
f
x
1
3 4
x x+ 1 x+ 2 x+ 3
x
d
FIGURE 23.2
Graph of the periodic function fd and the coset x + Z1.
23.2.2 A summary of the real-valued case
Below we summarise, in various alternative forms, the key aspects of periodicity for functions
f : R → R. This summary should be helpful in understanding the Boolean case discussed
below.
f one-to-one
f is one-to-one if f(x) = f(y) implies y = x.
f is one-to-one if f(x) = f(y) implies y − x = 0.
f periodic
f is periodic if there exists 0 6= p ∈ R such that f(x) = f(x + p) for all x ∈ R.
f is periodic if there exists 0 6= p ∈ R such that f(x) = f(y) implies y = x + p for all
x ∈ R.
f is periodic if there exists 0 6= p ∈ R such that f(x) = f(y) implies y − x = p for all
x ∈ R.
f : R → R is periodic, with 0 < p ∈ R, if and only if it is constant on the cosets R/Zp;
f may, however, take the same value on distinct cosets.
In the above p may not be unique – see the sine example, which is periodic with p ∈
{2π, 4π, 6π, . . . }, i.e., it is periodic with respect to distinct translations involving multiples
of 2π. We show later that Boolean functions may also be periodic with respect to distinct
translations.
f combined statements 1
f is periodic, or one-to-one, if f(x) = f(y) implies y − x = p for some p 6= 0 (periodic) or
x − y = 0 (one-to-one).336 Quantum algorithms 2: Simon’s algorithm
Equivalently: f is periodic, or one-to-one, if f(x) = f(y) implies y − x ∈ {0, p} for some
p 6= 0; where if y − x = p then f is periodic, and if x − y = 0 then f is one-to-one.
f combined statements 2
If f is such that f(x) = f(y) if and only if y = x + a for some a ∈ R, a 6= 0 then f has
period a and is a two-to-one function.
Equivalently: If f is such that f(x) = f(y) if and only if y −x = a for some a ∈ R, a 6= 0
then f has period a and is a two-to-one function.
23.3 Boolean functions
We note that (B
n, ⊕) is a group and, following the case of real functions, we say that
the Boolean function f : B
n → B
n is periodic if there exists s ∈ B
n, s 6= 0n, such that
f(x) = f(x ⊕ s) for all x ∈ B
n.
For s ∈ B
n, s 6= 0n, the sets Ks = {0, s} are 2-element subgroups of B
n. The cosets of
Ks in B
n are the two element sets {x, x ⊕ s} for x ∈ B
n (see Section 8.5).
As in the real case the periodicity of a function implies it is constant on the cosets –
but not necessarily taking different values on distinct cosets. That is, the following lemma
is true.
Lemma 23.2 f : B
n → B
n has period s ∈ B
n if and only if it is constant on the cosets
B
n/Ks. As in the real case, f may take the same value on distinct cosets – see Example
23.3.1.
Proof
(i) If f is periodic then there is an s ∈ B
n such that f(x) = f(x ⊕ s) for all x ∈ B;
equivalently f is constant on the sets {x, x ⊕ s} for x ∈ B
n which are the cosets of Ks in
B
n.
(ii) If f is constant on the cosets B
n/Ks then, clearly, f(x) = f(x ⊕ s) for all x ∈ B
n.

Example 23.3.1
The function
000 → 000
001 → 000
010 → 000
011 → 000
100 → 101
101 → 101
110 → 111
111 → 111
has period 001 but takes the same value, i.e., 000, on the cosets {000, 001} and {010, 011}
of B
3/K001.
Corollary
If f : B
n → B
n has period s ∈ B
n and f takes distinct values on the cosets B
n/Ks then f
is a two-to-one function.Boolean functions 337
Observation
As in the real case, a non-constant, Boolean function may be periodic with respect to distinct
translations. We show this below for functions f : B
3 → B
3 with translations s = 011 and
s
∗ = 101.
Proof
Assume f : B
n → B
n is such that:
f(x) = f(x ⊕ s) for all x
= f(x ⊕ s
∗
) for all x
then with x
∗ = x ⊕ s we obtain the further relation:
f(x) = f(x
∗
) = f(x
∗ ⊕ s
∗
)
= f(x ⊕ s ⊕ s
∗
).
Hence for x = 0n we have:
f(0n
) = f(s)
f(0n
) = f(s
∗
)
f(0n
) = f(s ⊕ s
∗
)
and for x = 1n:
f(1n
) = f(s)
f(1n
) = f(s
∗)
f(1n
) = f(s ⊕ s
∗).
Consider the case n = 3 choosing s = 011 and s
∗ = 101 we obtain the relations:
f(000) = f(011)
f(000) = f(101)
f(000) = f(110)
f(111) = f(100)
f(111) = f(010)
f(111) = f(001).
Choosing A, B ∈ B
3
, A 6= B for the values of f(000) and f(111) respectively, we obtain a
function f : B
3 → B
3 defined by:
f(x)
f(000) = A
f(001) = B (x = s ⊕ s
∗)
f(010) = B (x = s
∗)
f(011) = A (x = s)
f(100) = B (x = s)
f(101) = A (x = s
∗
)
f(110) = A (x = s ⊕ s
∗
)
f(111) = B
which is periodic in both s = 011 and s = 101. As A and B may be selected arbitrarily from
B
3
, provided A 6= B, a total of 8C2 = 28 functions, periodic in both s = 011 and s
∗ = 101,
are defined by the above.
338 Quantum algorithms 2: Simon’s algorithm
23.3.1 The subgroups of group (B
3
, ⊕)
• Subgroups with one element: {000 ∈ B}
• Subgroups with two elements: Ks = {000, s}, where s ∈ B
3 and s 6= 0
• Subgroups with four elements include:
1. {000, 001, 010, 011},
2. {000, 001, 100, 101},
3. {000, 010, 100, 110},
4. {000, 010, 101, 111},
5. {000, 011, 100, 111},
6. {000, 011, 101, 110},
.
.
.
23.3.2 The cosets B
3/Ks
For x ∈ B
3
the coset xKs is xKs = {x, x ⊕ s}.
23.4 The hidden subgroup problem
Many known quantum algorithms are special cases of the hidden subgroup problem which
we now state in a general form.
Let f : G → X be a function from the finite group G to a discrete set X that is constant
on the cosets of a subgroup K, of G, but takes a different value on each coset. (i.e., the
‘induced’ mapping fG/K : G/K → X is one-to one). Assuming we have an oracle for the
unitary operator Uf |gi|h ⊕ f(g)i, for g ∈ G and h ∈ X, and ⊕ an appropriate binary
operation on X, find K.
23.5 Simon’s problem
For this problem we have an oracle for a function f : B
n → B
n, where f is guaranteed to be
two-to-one and periodic; equivalently for some unknown s ∈ B
n, with s 6= 0n, we have, for
y 6= x, f(x) = f(y) if and only if y = x ⊕ s. The problem is to determine s by oracle query.
Simon’s problem can be expressed as a particular case of the hidden subgroup problem,
with G = (B
n, ⊕) and K = {0
n, s}, for some s ∈ B
n. The periodic two-to-one function f
is said to ‘hide’ the subgroup {0
n, s} of (B
n, ⊕). f is constant on the cosets, B
n/{0
n, s}, of
B
n and takes a distinct value on each coset. Simon’s problem may therefore be expressed
as: given an oracle for f, determine the subgroup K = {0
n, s} of (B
n, ⊕).The complexity of digital solutions 339
23.6 The complexity of digital solutions
Using ‘brute-force’ we query the oracle and test pairs (f(0n), f(b)), for b ∈ B
n \ {0
n}, for
equality. Clearly up to 2n − 1 comparisons are required to determine the period of f and
this approach is, therefore, O(2n) in execution time.
An alternative view is to investigate the probability of success when a particular number
of distinct pairs (x, y) ∈ B
n × B
n are tested for f(x) = f(y) from which the period, s, is
then s = x ⊕ y.
The ‘solution’ pairs (x, y), i.e., those for which f(x) = f(y), with y 6= x are sparse, in
the complete set of distinct pairs (x, y) ∈ B
n × B
n. Specifically, as f is two-to-one, there
will be precisely 2n−1
solution pairs, amongst the total number of distinct pairs, which is:
2
n
C2 =
2
n!
2 × (2n − 2)! =
2
n(2n − 1)
2
.
Hence, the probability of a randomly selected pair (x, y) providing a solution is
2 × 2
n−1
2
n × (2n − 1) =
1
2
n − 1
.
It follows that exponentially many pairs need to be checked for the probability of success to
be high. Given the nature of Simon’s problem there seem to be no ‘smart’ ways to determine
s and that, digitally, the problem is ‘hard’; i.e., no polynomially-bounded digital solutions
are known.
23.7 Simon’s quantum algorithm
We note that the algorithm may be expressed as shown in Figure 23.3. The input state is
|ψ0i = |0
ni ⊗ |0
ni, equivalently |0i
⊗n ⊗ |0i
⊗n. The algorithm may be expressed as follows:
defining
A = µQ{Uf [H⊗n ⊗ I(|0i
n ⊗ |0i
n
)]}
we follow this with
µQ(H⊗n ⊗ I(A))
to give
µQ(H⊗n ⊗ I(µQ{Uf [H⊗n ⊗ I(|0i
n ⊗ |0i
n
)]}))
as shown in Figure 23.3.
Following the first Hadamard transform the state is:
|ψ1i = (H⊗n
|0
n
i) ⊗ |0
n
i
=
1
2
n
2
X
x∈{0,1}n
|xi ⊗ |0
n
i
and following Uf we have
|ψ2i = Uf |ψ1i
=
1
2
n
2
X
x∈{0,1}n
|xi ⊗ |f(x)i.340 Quantum algorithms 2: Simon’s algorithm
|0
Uf
⊗n
H⊗n
|0
⊗n
|ψ3
|ψ2
|ψ1
|ψ4
|ψ0
H⊗n y ∈ B
n
with y · s = 0
FIGURE 23.3
A circuit for a single pass of Simon’s quantum computation.
At this point we follow Preskill’s approach (‘for pedagogical clarity’ – see his on-line ma￾terial) and measure the second register of the state |ψ2i. Specifically, we measure the self￾adjoint operator
O = I ⊗
2
Xn−1
z=0
z|zihz|
on the second register, |f(x)i, of the state |ψ2i. The outcome is a value, f(x
∗
), for some
x
∗ ∈ B
n, of the observable O and we can conclude that the normalised state, following the
measurement of O on |ψ2i is:
|ψ3i =
1
√
2
(|x
∗
i + |x
∗ ⊕ si) ⊗ |f(x
∗
)i
where s is the period of f.
The reasons this conclusion may be drawn are:
1. the assumption that f is both periodic and two-to-one; hence the measured value
f(x
∗
) occurs also at x
∗ ⊕ s; i.e., we have f(x
∗ ⊕ s) = f(x
∗
), where s is the period
of f,
and
2. the von Neumann-L¨uders measurement postulate, which states that the post￾measurement state is the normalised projection of |ψ2i onto the subset of states
compatible with the measured value, f(x
∗
), of O on |ψ2i.
Measuring the first register of the state 1
√
2
(|x
∗
i + |x
∗ ⊕ si) ⊗ |f(x
∗
)i would not help
to determine s; with probability 1
2 we would obtain either x
∗ or x
∗ ⊕ s but neither of
these outcomes reveals anything about s and the state collapses to either |x
∗
i ⊗ |f(x
∗
)i or
|x
∗ ⊕ si ⊗ |f(x
∗
)i.
Instead we apply the Hadamard transform, H⊗n, to the first register of |ψ3i. At this
point the second register may be dropped and we compute:
H⊗n
(
1
√
2
(|x
∗
i + |x
∗ ⊕ si) = 1
2
(n+1)/2
2
Xn−1
y=0
[(−1)x
∗
·y + (−1)(x
∗⊕s)·y
]|yi. (1)Simon’s quantum algorithm 341
We now note that
(−1)x
∗
·y + (−1)(x
∗⊕s)·y = (−1)x
∗
·y + (−1)x
∗
·y
(−1)s·y
= (−1)x
∗
·y
(1 + (−1)s·y
)
=

0 if s · y = 1
2(−1)x
∗
·y
if s · y = 0.
Hence the only non-zero terms in the sum on the right-hand side of relation (1) are those
for which s · y = 0 and we therefore have:
|ψ4i ≡ H⊗n
(
1
√
2
(|x
∗
i + |x
∗ ⊕ si)
=
1
2
(n−1)/2
X
y with y·s=0
(−1)x
∗
·y
|yi
=
X
y with y·s=0
αy|yi
where αy =
(−1)x
∗
·y
2
(n−1)/2
and |αy|
2 =
1
2
n−1
.
Measurement of the state |ψ4i with the observable
2
Xn−1
z=0
z|zihz| therefore produces a
y ∈ B
n with the property y · s = 0. Specific y values each occur with probability 1
2n−1 .
A single y satisfying y · s = 0 is insufficient to determine s, (n − 1) linearly independent
(in the Boolean sense) y values being required to enable the (n−1)×n homogeneous system
of linear equations:
y1 · s = 0
y2 · s = 0
.
.
.
yn−1 · s = 0
to be solved for s which identifies the hidden subgroup K = {0
n, s}.This step of Simon’s
algorithm is solved classically. Hence the quantum procedure is, first, repeated until a set
y1, . . . , yn−1 of linearly independent y’s are obtained. Given that the probability of obtaining
a specific y, using the quantum procedure, is low (i.e., 1
2n−1 ) it follows that with O(n)
repetitions, the probability of not having (n−1) linearly independent y values is low. Hence
the query complexity of the classical solution is exponential, i.e., O(2n), whilst that of
the quantum algorithm is linear, i.e., O(n). A wider view of complexity estimation might
include the final step of Simon’s algorithm (which is done classically) – i.e., solving the
linear equations to determine s. As this is no worse than O(n
3
), Simon’s solution remains
significantly more efficient.
Simon’s problem is of little practical interest; its purpose is to demonstrate the great
potential power of quantum computation.
Writing s = s1 · · · sn and yi = yi1 · · · yin then the equation system above may be written as


y11 y12 · · · y1n
y21 y22 · · · y2n
.
.
.
.
.
.
.
.
.
.
.
.
y(n−1)1 y(n−1)2 · · · y(n−1)n




s1
s2
.
.
.
sn


=


0
0
.
.
.
0

342 Quantum algorithms 2: Simon’s algorithm
where the ‘arithmetic’ is Boolean, i.e.,
yi
· s = (yi1 ∧ s1) ⊕ (yi2 ∧ s2) ⊕ · · · ⊕ (yin ∧ sn).
That is, ∧ replaces multiplication, in the fields R and C, and ⊕ replaces addition.
The simplest examples to illustrate the post-quantum determination of s are for n = 2.
In this case we just have one equation, y · s = 0, to solve for s, the period of f.
Example 23.7.1
Suppose y = 01 is measured, then the equation for s = s1s2 is:
01 · s1s2 = 0.
We have
01 · s1s2 = (0 ∧ s1) ⊕ (1 ∧ s2)
= 0 ⊕ s2
= 0 if and only if s2 = 0.
Given that s 6= 00, otherwise f would not be periodic, the solution is s = 10.
Example 23.7.2
Suppose y = 11 is measured, then the equation for s = s1s2 is:
11 · s1s2 = 0.
We have
11 · s1s2 = (1 ∧ s1) ⊕ (1 ∧ s2)
= s1 ⊕ s2
= 0 if and only if s1 = 1 and s2 = 1,
s = 00 again, not being allowed. Hence the solution is s = 11.A
Probability
A.1 Definition of probability
Understanding quantum computation relies on an understanding of probability. If an event,
E say, (or outcome) is certain to occur, we say that the probability of it occurring, P(E),
is 1. If it is impossible for an event to occur it has probability 0, that is P(E) = 0. We see
that in some circumstances the outcome of an event can be calculated exactly, that is, it is
deterministic. All other probabilities lie between 0 and 1. Events with probabilities nearer
to 1 are more likely to happen than not.
Definition A.1 Probability
For any event E, the probability of its occurrence, P(E), is such that
0 ≤ P(E) ≤ 1.
The result of performing an experiment is often referred to as an event. Consider the ex￾periment of tossing a coin and hoping to obtain a head, H. We may not be able to predict
precisely whether a head will occur because the outcome is stochastic or random. If the
coin is fair then the theoretical probability of obtaining a head is 1
2
. This means that on
average one in every two tosses should result in a head. So P(H) = 0.5. Likewise the prob￾ability of obtaining a tail, T, is also 1
2
, that is P(T) = 0.5. No other outcomes are possible
in such an experiment. Observe that P(H) + P(T) = 1, representing total probability: we
know that obtaining a head or a tail is certain to happen. Note that to calculate theoreti￾cal probabilities, a priori knowledge about the possible events is essential. In this case we
assumed that the coin was fair. In some circumstances we don’t have sufficient information
to calculate a theoretical probability. In such a case we can calculate an experimental
probability by performing an experiment a large number of times, N say. Then we can
observe the number of times, M say, that an event E occurs. The experimental probability
of event E is then given by
P(E) = M
N
.
For example, if we toss a biased coin 1000 times, and obtain a head on 650 occasions, the
experimental probability of obtaining a head is
P(H) = 650
1000
= 0.65.
Example A.1.1
Suppose 6000 four-digit binary numbers are generated, e.g. 1101, 1111, 0011, 0101, . . ..
Suppose that we observe the number 0000 on 423 occasions. Calculate
(a) the experimental probability of obtaining the event E: 0000,
DOI: 10.1201/9781003264569-A 343344 Probability
(b) the theoretical probability of obtaining E : 0000 assuming that all outcomes are equally
likely.
Solution
(a) The experimental probability P(E) = 423
6000 = 0.0705.
(b) There are 24 = 16 possible outcomes when generating four-digit binary numbers. If each
outcome is equally likely then P(E) = 1
16 = 0.0625. It would seem that in our experiment
the number 0000 has been generated rather more times than we would have expected if the
outcome was truly random.
Example A.1.2
When a quantum bit is measured the measured value is either 0 or 1. The probability of
obtaining 0, P(0), is sometimes labelled |α0|
2 and similarly, the probability of obtaining 1,
P(1), is sometimes labelled |α1|
2
. These are the only possible outcomes and so
P(0) + P(1) = |α0|
2 + |α1|
2 = 1.
This rather simple fact has profound implications for how quantum states are described
mathematically.
A.2 Discrete random variables and probability distributions
There are usually several possible outcomes of any given experiment. For example, if we
measure some property of a quantum system, we find that the measured value can take on
one of several possible values. Which value we actually achieve is not known a priori but
what we do know is the probability of achieving that value. Thus the measured value is a
random variable, X say.
When a random variable can assume any value in a given interval it is said to be
continuous. On the other hand, when the variable must assume a value from a set of
individually specified values it is said to be discrete.
Given a discrete random variable X, which in a single experiment can assume one of
the n values x1, x2, . . . , xn, a probability distribution tells us how the total probability
is distributed amongst the various possible values. For example, in a single throw of a fair
die the probability of getting any one of the values 1, . . . , 6 is 1
6
. Table A.1 shows this
probability distribution. Note that we write P(X = x) to indicate the probability that the
random variable X assumes the specific value x. Note also that the sum of the probabilities
TABLE A.1
A discrete probability distribution
x 1 2 3 4 5 6
P(X = x)
1
6
1
6
1
6
1
6
1
6
1
6
of all possible outcomes is 1 representing total probability.Discrete random variables and probability distributions 345
The expected value or expectation, E(X), of a discrete random variable X is found
by multiplying each possible value by its probability and then summing the results. So, if
the value xi occurs with probability pi then E(X) = Xn
i=1
pixi
.
Definition A.2 Expected value of the discrete random variable X:
Expected value = E(X) = Xn
i=1
pixi
.
If an experiment is repeated a large number of times, then the mean or average of the result￾ing measurements is approximately equal to the expected value. Again, if an experiment is
repeated a large number of times, there will be a spread of values of resulting measurements.
The standard deviation and the variance are both measures of this spread. If the value
xi occurs with probability pi
, then the variance, written σ
2
, is found from
σ
2 =
Xn
i=1
pi(xi − E(X))2
.
The standard deviation, σ, is then the square root of the variance.
Definition A.3 Variance and standard deviation of the discrete random variable
X:
variance = σ
2 =
Xn
i=1
pi(xi − E(X))2
.
The standard deviation is the square root of the variance.
Example A.2.1
A discrete random variable X can take two possible values 0 and 1 each with probability 1
2
(Table A.2). Calculate the expected value and the variance.
TABLE A.2
A discrete
probability
distribution
x 0 1
P(X = x)
1
2
1
2
Solution
The expected value is given by
E(X) = X
2
i=1
pixi =
1
2
× 0 +
1
2
× 1 =
1
2
.
The variance is given by
σ
2 =
X
2
i=1
pi(xi −
1
2
)
2 =
1
2
(0 −
1
2
)
2 +
1
2
(1 −
1
2
)
2 =
1
4
It follows that the standard deviation is σ =
1
2
.B
Trigonometric ratios and identities
The three common trigonometric ratios sine (sin), cosine (cos) and tangent (tan) are
defined with reference to a right-angled triangle ABC (Figure B.1).
sin θ =
side opposite to θ
hypotenuse =
BC
AC , cos θ =
side adjacent to θ
hypotenuse =
AB
AC
tan θ =
side opposite to θ
side adjacent to θ
=
BC
AB
A B
C
θ
FIGURE B.1
A right-angled triangle.
The trigonometric ratios of π
6
= 30◦
,
π
4
= 45◦
and π
3
= 60◦
occur frequently in calculations.
They can be calculated exactly by considering suitable right-angled triangles:
sin
π
6
=
1
2
, cos
π
6
=
√
3
2
, tan
π
6
=
1
√
3
sin
π
4
=
1
√
2
, cos
π
4
=
1
√
2
, tan
π
4
= 1
sin
π
3
=
√
3
2
, cos
π
3
=
1
2
, tan
π
3
=
√
3
Whilst these ratios are defined with respect to a right-angled triangle, and so 0 < θ < 90◦
,
the definition can be readily extended to angles of any size. Use of a calculator makes
calculation of trigonometric ratios straightforward. The trigonometric functions f : θ →
sin θ, f : θ → cos θ, f : θ → tan θ then follow immediately. The graphs of sin θ and cos θ
have been given in Chapter 2. Inspection of the graphs reveals other important properties
of the trigonometric functions, particularly
cos(−θ) = cos θ and sin(−θ) = − sin θ
so that cos θ is a so-called even function and sin θ is a so-called odd function.
Trigonometric identities provide a means to write a given expression involving
trigonometric functions in terms of other trigonometric functions. Some common identi￾ties are given in Table B.1. (Note that (cos A)
2
is usually written cos2 A. Likewise, sin2 A
means (sin A)
2
).
DOI: 10.1201/9781003264569-B 347348 Trigonometric ratios and identities
TABLE B.1
Common trigonometric identities
sin A
cos A = tan A
sin2 A + cos2 A = 1
sin(A + B) = sin A cos B + sin B cos A
sin(A − B) = sin A cos B − sin B cos A
sin 2A = 2 sin A cos A
cos(A + B) = cos A cos B − sin A sin B
cos(A − B) = cos A cos B + sin A sin B
cos 2A = cos2 A − sin2 A
sin A + sin B = 2 sin ￾ A+B
2

cos ￾ A−B
2

sin A − sin B = 2 sin ￾ A−B
2

cos ￾ A+B
2

cos A + cos B = 2 cos ￾ A+B
2

cos ￾ A−B
2

cos A − cos B = −2 sin ￾ A+B
2

sin ￾ A−B
2

Exercises
B.1 Show that 1
2 −
1
2
cos θ = sin2 θ
2
.
B.2 Show that 1
2 +
1
2
cos θ = cos2C
Coordinate systems
The coordinates of a point describe its position. When working in two spatial dimensions
the Cartesian coordinate system is the most common. The system comprises two axes
– usually referred to as the x axis and the y axis – intersecting at right angles at a point
called the origin O. Referring to Figure C.1 the horizontal distance of point P from the y
axis is the x coordinate of P, with positive values to the right of O and negative values to
the left. Likewise, the vertical distance is the y coordinate. We state that the coordinates of
the point P are (x, y), written P(x, y). An alternative description is to give the distance of
x
0
y
r
θ
x
y P(x, y)
FIGURE C.1
P has Cartesian coordinates (x, y) and polar coordinates (r, θ).
point P from the origin, r ≥ 0 say, and the angle, θ, that the arm OP makes with a reference
axis, usually the positive x axis, and usually measured anticlockwise so that −π < θ ≤ π
(Figure C.1). Then, the coordinates (r, θ) are the polar coordinates of P. Cartesian and
polar coordinates are related through the formulae:
x = r cos θ, y = r sin θ
r =
p
x
2 + y
2, tan θ =
y
x
.
When working in three spatial dimensions, the position of a point can be described by
three Cartesian coordinates, so the coordinates of P are (x, y, z) (Figure C.2). In situations
involving spherical symmetry, it is often more useful to work in spherical polar coordi￾nates (r, θ, ϕ) which are defined with reference to Figure C.3. Cartesian and spherical polar
coordinates are related through the formulae:
x = r cos ϕ sin θ, y = r sin ϕ sin θ, z = r cos θ
DOI: 10.1201/9781003264569-C 349350 Coordinate systems
x
y
z
P(x, y, z)
FIGURE C.2
The Cartesian coordinates of P are (x, y, z).
y
z
r
P
x
ϕ
θ
FIGURE C.3
The spherical polar coordinates of P are (r, θ, ϕ).D
Field axioms
Fields are fundamental mathematical structures to which we make frequent reference in this
book. Two particularly relevant fields are the set of real numbers R and the set of complex
numbers C. To be referred to as a field a set must possess two binary operations, that
is a way of combining two elements from the set, which satisfy properties known as field
axioms. These are given below.
A field F is a set with two binary operations, that we can conveniently call addition +
and multiplication ·, which satisfy the following1
:
1. The set F must be closed under + and ·. That is, for any a, b ∈ F, a + b ∈ F and
a · b ∈ F.
2. Addition is associative: for a, b, c ∈ F, a + (b + c) = (a + b) + c.
3. There exists an identity element for addition, often denoted 0 ∈ F, such that
0 + a = a + 0 = a for any a ∈ F.
4. For every element a ∈ F there is an additive inverse labelled −a such that
a + (−a) = 0.
5. Addition is commutative: for a, b ∈ F, a + b = b + a.
6. Multiplication is associative: for a, b, c ∈ F, a · (b · c) = (a · b) · c.
7. Multiplication is commutative: for a, b ∈ F, a · b = b · a.
8. There exists a multiplicative identity, often denoted by 1 ∈ F, such that 1 · a = a
for any a ∈ F.
9. For every element a ∈ F, with the exception of possibly 0, there is a multiplicative
inverse labelled a
−1
such that a · (a
−1
) = 1.
10. Multiplication is distributive over addition: for a, b, c ∈ F, a·(b+c) = (a·b)+(a·c).
Notes
1. Axioms 1-5 mean that (F, +) is an Abelian group.
2. Axioms 1, 6-9 mean that (F, ·) is an Abelian group.
1other binary operations can be substituted for + and ·, provided the field axioms are satisfied.
DOI: 10.1201/9781003264569-D 351E
Solutions to selected exercises
Chapter 1.
1.1 ∅, {0}, {1}, {0, 1} = B.
1.2 The power set of B = {∅, {0}, {1}, {0, 1}}. The cardinality is 4.
1.3 Let the set be {a, b, c}. The power set has 23 = 8 elements:
{∅, {a}, {b}, {c}, {a, b}, {a, c}, {b, c}, {a, b, c}}.
1.4 2n.
1.5 Every even integer is clearly an integer. The set of odd integers is a subset of Z.
1.7 The set of odd integers does not include the additive identity 0.
1.8 The elements which are self-inverse: 1, P12, P13, P23.
1.9 4.
1.10 2n.
1.11 s1 = s2 = 0, or s1 = s2 = 1.
1.13 The identity element is 0.
1.14 The identity element is 0.
+ 0 1 2
0 0 1 2
1 1 2 0
2 2 0 1
Chapter 1. End-of-chapter exercises.
1. 2(2n)
.
3. a) A, b) A ∪ B.
4. [0] = {. . . , 0, 6, 12, . . .}, [1] = {. . . , 1, 7, 13, . . .}, [2] = {. . . , 2, 8, 14, . . .},
[3] = {. . . , 3, 9, 15, . . .}, [4] = {. . . , 4, 10, 16, . . .}, [5] = {. . . , 5, 11, 17, . . .}.
6. If y − x = y
∗ − x
∗
then y
∗ − y = x
∗ − x and then y
∗−y
x∗−x = 1. The slope of the line joining
the two points has gradient 1. The set of equivalence classes is the set of straight lines with
gradient 1.
7. a)
+ 0 1
0 0 1
1 1 0
b)
× 0 1
0 0 0
1 0 1
c) 0. d) 1.
10. 5
DOI: 10.1201/9781003264569-E 353354 Solutions to selected exercises
Chapter 2.
2.1 (12345).
2.5 Yes
2.12 f0(x) = 0 and f3(x) = 1 cannot be inverted. They are not one-to-one.
2.16 f0(x) = 0, f1(x) = x, f2(x) = x, f3(x) = 1, for all x ∈ B.
2.18
(x, y) f(x, y) = (x, y ⊕ 0)
(0, 0) (0, 0)
(0, 1) (0, 1)
(1, 0) (1, 0)
(1, 1) (1, 1)
2.19
(x, y) f(x, y) = (x, y ⊕ 1)
(0, 0) (0, 1)
(0, 1) (0, 0)
(1, 0) (1, 1)
(1, 1) (1, 0)
Chapter 2. End-of-chapter exercises.
4. Bijective.
(x, y) f(x, y) = (y, x ⊕ y)
(0, 0) (0, 1)
(0, 1) (1, 0)
(1, 0) (0, 0)
(1, 1) (1, 1)
5. It is neither one-to-one nor two-to-one.
(x, y) f(x, y) = (x ∧ y, x ∧ y)
(0, 0) (0, 1)
(0, 1) (0, 1)
(1, 0) (0, 1)
(1, 1) (1, 0)
6. From the table (x ∧ y) ∨ (x ∧ y) is equivalent to x ⊕ y.
x y x ⊕ y x ∧ y x ∧ y (x ∧ y) ∨ (x ∧ y)
0 0 0 0 0 0
0 1 1 0 1 1
1 0 1 1 0 1
1 1 0 0 0 0
7. It is not invertible.
(x, y) f(x, y) = (x ⊕ y, x ∧ y)
(0, 0) (0, 0)
(0, 1) (1, 0)
(1, 0) (1, 0)
(1, 1) (0, 1)
8. The function is bijective.Solutions to selected exercises 355
x y z x ⊕ y x ∧ y z ⊕ (z ∧ y)
0 0 0 0 0 0
0 0 1 0 0 1
0 1 0 1 0 0
0 1 1 1 0 1
1 0 0 1 0 0
1 0 1 1 0 1
1 1 0 0 1 1
1 1 1 0 1 0
Thus
000 → 000
001 → 001
010 → 010
011 → 011
100 → 110
101 → 111
110 → 101
111 → 100
11. The range is {000, 001, 010, 011}. The range is a proper subset of the co-domain B
3
.
14. (a) Given b in the co-domain, choose a =
b−5
3
, then f(a) = b.
(b) Given b in the co-domain, choose a = 2b
, then f(a) = b.
Chapter 3.
3.1 −
1
2 ± i
√
3
2
.
3.2 (a) −i, (b) 1, (c) i.
3.3 (x − 2)(3x
2 − 5x + 6), 5
6 ± i
√
47
6
.
3.4 x
2 − 10x + 29 = 0. There are others.
3.5 a) −
1
2
, b) 1
2
.
3.6 a) 6i, b) 6 + 10i, c) 3 + 26i, d) −55 + 48i, e) 7 − 30i.
3.7 Re(z) = 16, Im(z) = 11.
3.10 16−4i
17 , 1 + i
4
.
3.12 a) −
62
85 , −
61
85 ; b) 0, −2; c) −
1
2
, −
1
2
.
3.13 a) 5, tanθ =
4
3
, θ = 53.13◦
. b) √
2, −
3π
4 = −135◦
.
c) 3, 0. d) 2, π
2
. e) √
2π,
π
4
.
3.14 2, 5π
6
; 4√
2, π
4
; z1z2 = 8√
2∠
13π
12 ;
z1
z2
=
1
2
√
2
∠
7π
12 .
3.17 (a) |z1||z2| = |z1z2| =
√
533.
(b) |
z1
z2
| =
|z1|
|z2| =
√
533
41 .
3.19 (a) 1. (b) −1. (c) −i. (d) −i.
3.20 (a) −
1
2 +
√
3
2
i. (b) −
1
2 −
√
3
2
i. (c) −
1
2 −
√
3
2
i. (d) −
1
2 +
√
3
2
i.
3.22 ei.
3.23. cos θ =
e
iθ+e−iθ
2
, sin θ =
e
iθ−e
−iθ
2i .
3.25 a = 1 + cos 2ωt, b = sin 2ωt.
3.26 eiπ/6
.
3.28 f(0) = 2, f(1) = 4, f(2) = −10, f(3) = 6.
3.29 ˆf(0) = √
1
3
(f(0) + f(1) + f(2)), ˆf(1) = √
1
3
￾
f(0) + f(1)e−2πi/3 + f(2)e−4πi/3

,
ˆf(2) = √
1
3
￾
f(0) + f(1)e−4πi/3 + f(2)e−8πi/3
356 Solutions to selected exercises
Chapter 3. End-of-chapter exercises.
1. (a) 2
13 −
3
13 i. (b) x
x2+y2 +
y
x2+y2 i
10. z = 1∠2kπ/3, k = 0, 1, 2.
Chapter 4.
4.1 hψ| = √
1
3
(−i, 1 − i).
4.2 |φi =

1/
√
2
1/
√
2

.
4.6 √
69.
4.7 √
3.
4.10 √
1
2

1
1

= √
1
2
(|0i + |1i). √
1
2

−1
1

= √
1
2
(−|0i + |1i)
4.11 
1
0

=
√
2
2
(|ψi + |φi). 
0
1

=
√
2
2
(|ψi − |φi).
4.13 u · v = 18, cos θ = 0.9487, θ = 18.4
◦
.
4.16 z1w
∗
1 + z2w
∗
2
; z
∗
1w1 + z
∗
2w2.
4.18 zw∗
(definition 1).
4.19 a) 0, b) 0, c) 0, d) 0.
Chapter 4. End-of-chapter exercises.
2.  p
p
2/3
1/3

; 1.
3. 
38 + 17i
1 − 23i 
.
4. Definition 1: hu, vi = 11 − 4i, hv, ui = 11 + 4i, hu, ui = 15, hv, vi = 34.
Definition 2: hu, vi = 11 + 4i, hv, ui = 11 − 4i, hu, ui = 15, hv, vi = 34.
5. 
1
0

=
1
2

1
1

+
1
2

1
−1

;

0
1

=
1
2

1
1

−
1
2

1
−1

.
12.
x y x · y
x1x2x3 000 0
x1x2x3 001 x3
x1x2x3 010 x2
x1x2x3 011 x2 ⊕ x3
x1x2x3 100 x1
x1x2x3 101 x1 ⊕ x3
x1x2x3 110 x1 ⊕ x2
x1x2x3 111 x1 ⊕ x2 ⊕ x3
13. a) 1, b) 0.
Chapter 5.
5.3 Tr(A) = 5. Tr(AT
) = 5. Tr(In×n) = n.
5.6 A + AT =

18 7
7 4 
which is symmetric.
5.7 A − AT =

0 1
−1 0 
which is skew-symmetric.
5.13 a) 
1 0
0 1 
, b) 
0 1
1 0 
.Solutions to selected exercises 357
5.18 (a) 
20 28
−10 −14 
, (b) 
312
−156 
, (c) 78, (d) 
312
−156 
.
5.20 |A| = |AT
| = 4.
5.21 1.
5.23 
−1 1
−2 3/2

.
5.24 1
15


−2 8 1
7 17 −11
4 −1 −2

.
5.25 λ = 4, −1.
5.26 a) RT =

cos θ sin θ
− sin θ cos θ

.
5.29 x1 = −4µ, x2 = µ.
5.30 x = 2µ − λ, y = 1 + 2λ − 2µ, z = λ, w = µ.
Chapter 5. End-of-chapter exercises.
1. λ = 5, −1.
2. x =
µ−1
2
, y = µ, z = 3, for µ ∈ R.
3. AB =


11 0 20
7 −20 15
5 21 25

, (AB)
T =


11 7 5
0 −20 21
20 15 25


7. a) H† =

3 1 + i
1 − i 2 
, c) H H† =

11 5 + 5i
5 − 5i 6 
.
8. [σ2, σ3] = 
0 2i
2i 0 
= 2iσ1.
9. HP =

e
iθ2 e
iθ1
e
iθ2 −e
iθ1

. HP H =

e
iθ2 + eiθ1 e
iθ2 − e
iθ1
e
iθ2 − e
iθ1 e
iθ2 + eiθ1

.
Further

e
iθ2 + eiθ1 e
iθ2 − e
iθ1
e
iθ2 − e
iθ1 e
iθ2 + eiθ1

=
 
e
i
θ2
2 e
i
θ2
2 + ei
θ1
2 e
i
θ1
2 e
i
θ2
2 e
i
θ2
2 − e
i
θ1
2 e
i
θ1
2
e
i
θ2
2 e
i
θ2
2 − e
i
θ1
2 e
i
θ1
2 e
i
θ2
2 e
i
θ2
2 + ei
θ1
2 e
i
θ1
2
!
= ei
θ2+θ1
2
 
e
i
θ2−θ1
2 + e−i
θ2−θ1
2 e
i
θ2−θ1
2 − e
−i
θ2−θ1
2
e
i
θ2−θ1
2 − e
−i
θ2−θ1
2 e
i
θ2−θ1
2 + e−i
θ2−θ1
2
!
= ei
θ2+θ1
2
 
e
−i
θ1−θ2
2 + ei
θ1−θ2
2 e
−i
θ1−θ2
2 − e
i
θ1−θ2
2
e
−i
θ1−θ2
2 − e
i
θ1−θ2
2 e
−i
θ1−θ2
2 + ei
θ1−θ2
2
!
= ei
θ2+θ1
2
 
e
i
θ
2 + e−i
θ
2 −(ei
θ
2 − e
−i
θ
2 )
−(ei
θ
2 − e
−i
θ
2 ) ei
θ
2 + e−i
θ
2
!
where θ = θ1 − θ2
Note 2 cos θ
2 = ei
θ
2 + e−i
θ
2 and 2i sin θ
2 = ei
θ
2 − e
−i
θ
2 . Therefore
HP H = ei
θ2+θ1
2
 
2 cos θ
2 −2i sin θ
2
−2i sin θ
2
2 cos θ
2
!
so that
1
2
HP H = ei
θ2+θ1
2
 
cos θ
2 −i sin θ
2
−i sin θ
2
cos θ
2
!
where θ = θ1 − θ2, as required.
12. e.g. 
√
1
2
√
1
2
√
1
2
− √
1
2
!
is orthogonal but is not a permutation matrix.358 Solutions to selected exercises
Chapter 6.
6.1 The zero element is the zero polynomial p(t) = 0. The inverses are −pi(t), i = 1, . . . , 4.
6.5 The zero element of R
2
is (0, 0). This element is not in U and hence U cannot be a
subspace.
6.11 The null space is µ

−1
1

, µ ∈ R.
6.14 Linearly dependent: the third is the sum of the others.
6.15 Linearly independent.
6.18 For example √
1
2
(1, 1, 0), √
1
6
(−1, 1, 2), √
1
3
(1, −1, 1).
Chapter 6. End-of-chapter exercises.
3. d) p = 7e0 − 2e1.
5.


8
11
1
5


= 3


1
2
0
1


+ 2


2
2
0
1


+


1
1
1
0


so r is in the space spanned by u, v and w.
8. (z1, z2) = −iz1(i, 0) − iz2(0, i).
9. (3 + 4i, −7 − 2i) = 3(1, 0) − 7(0, 1) + 4(i, 0) − 2(0, i).
Chapter 7.
7.1 Spectrum = {1, 0}. The algebraic multiplicity of each eigenvalue is 1.
7.5 λ =
√
13, v =
 1+√
13
4
1

; λ = −
√
13, v =
 1−
√
13
4
1

.
7.6 λ = 3, 4, 2, v =


1
2
1

,


0
1
1

,


−1
−1
1

 resp.
Chapter 7. End-of-chapter exercises.
2. λ = 1, −1, v =

1
0

,

0
1

resp.
5. λ = 1, 0, v =

0
1

,

1
0

resp.
7. a) 0,0,1,1; v =


0
1
0
0

,


1
0
0
0

,


0
0
0
1

,


0
0
1
0


resp.
b) 0,0,1,1; v =


0
0
1
0

,


1
0
0
0

,


0
0
0
1

,


0
1
0
0


resp.
8. a) 0,0,1,1; v =


−1
0
0
1

,


0
−1
1
0

,


1
0
0
1

,


0
1
1
0


resp.
b) 0,0,1,1; v =


1
0
0
1

,


0
−1
1
0

,


−1
0
0
1

,


0
1
1
0


resp.Solutions to selected exercises 359
c) 1,1,0,0; v =


0
0
0
1

,


1
0
0
0

,


0
0
1
0

,


0
1
0
0


resp.
9. 0,0,0,1; v =


0
0
1
0

,


0
1
0
0

,


0
0
0
1

,


1
0
0
0


resp.
10. λ = 1: 
1
0

. λ = eiφ
:

0
1

.
Chapter 8.
8.1 (R
+, +) does not include the identity element 0.
8.2 (R\0, +) does not include the identity element 0.
8.3 The element 0 ∈ R does not possess a multiplicative inverse.
8.4 3.
8.5 7.
8.6
I L1 L2 R
I I L1 L2 R
L1 L1 I R L2
L2 L2 R I L1
R R L2 L1 I
8.7 For example, choose Φ(00) = I, Φ(01) = L1, Φ(10) = L2, Φ(11) = R.
Chapter 8. End-of-chapter exercises.
5. b) The identity element of the group is the identity matrix I =

1 0
0 1 
.
c) The group has an infinite number of elements.
9.
a)
+ 0 1 2 3 4
0 0 1 2 3 4
1 1 2 3 4 0
2 2 3 4 0 1
3 3 4 0 1 2
4 4 0 1 2 3
d)
· 1 −1 i −i
1 1 −1 i −i
−1 −1 1 −i i
i i −i −1 1
−i −i i 1 −1
The group can be generated by both i and −i.
Chapter 9.
9.2 f is not a linear functional.
9.3 f is not a linear functional.360 Solutions to selected exercises
Chapter 9. End-of-chapter exercises.
1. P =


1 0 0
0 1 0
0 0 0

.
2. P =
 1
2
1
2
1
2
1
2
!
.
7. a) The kernel contains only the zero vector. The dimension of the kernel is zero.
b) Any three linearly independent vectors, e.g. (2, 1, 0), (0, −3, 2), (0, 0, −4).
c) dim ker(f) + dim Im(f)= 0 + 3 = 3 = dim R
3
.
9. The kernel is spanned by 
1
0

.
Chapter 10.
10.5 a) 1
2
(|00i + |10i + |01i + |11i).
b) 1
2
(|00i − |10i − |01i + |11i).
10.6 The computational basis B⊗3C2 is
|000i, |001i, |010i, . . . , |111i
which is equal to


1
0
0
0
0
0
0
0


,


0
1
0
0
0
0
0
0


,


0
0
1
0
0
0
0
0


, . . . ,


0
0
0
0
0
0
0
1


.
Chapter 10. End-of-chapter exercises.
1. a) dimension: 2 × 2 = 4.


1
0
0
0

 ,


0
1
0
0

 ,


0
0
1
0

 ,


0
0
0
1

 .
b) dimension: 3 × 2 = 6.


1
0
0
0
0
0


,


0
1
0
0
0
0


,


0
0
1
0
0
0


,


0
0
0
1
0
0


,


0
0
0
0
1
0


,


0
0
0
0
0
1


.Solutions to selected exercises 361
c) dimension: 2 × 4 = 8.


1
0
0
0
0
0
0
0


,


0
1
0
0
0
0
0
0


,


0
0
1
0
0
0
0
0


,


0
0
0
1
0
0
0
0


,


0
0
0
0
1
0
0
0


,


0
0
0
0
0
1
0
0


,


0
0
0
0
0
0
1
0


,


0
0
0
0
0
0
0
1


.
2. Not entangled: (|0i + |1i) ⊗ (|0i − |1i).
3. 10.
4. −100 − 60i.
5. a) √
2
2
(|0i ⊗ |0i) = √
2|00i.
b) √
2
2
(|0i ⊗ |1i) = √
2|01i.
c) √
2
2
(|1i ⊗ |1i) = √
2|11i.
d) √
2
2
(|1i ⊗ |0i) = √
2|10i
8.


0 1 0 0
1 0 0 0
0 0 0 1
0 0 1 0

.
Chapter 11.
11.7 (A + B)(z1, z2) = (2z1, 2z1).
(kA)(z1, z2) = k(z1 − z2, z1 + z2) = (k(z1 − z2), k(z1 + z2)).
11.8 a) (AB)(z1, z2) = (2z2, 2z1). b) (BA)(z1, z2) = (2z1, −2z2).
11.9 
0 1
1 0 
;

1 0
0 −1

.
11.10 a) (0, √
1
2
), b) (0, − √
1
2
), c) 1
2
, d) −
1
2
, e) −
1
2
, f) 1
2
.
 1
2 −
1
2
−
1
2
1
2
!
.
11.11 b) λ = 1, v =

1
0

; λ = −1, v =

0
1

.
11.12 
0 1
−1 0 
. λ = i,

1
i

; λ = −i,

1
−i

.
11.14 
0 −i
i 0 
.
11.16 (H ⊗ H)|01i =
1
2


1
−1
1
−1


, (H ⊗ H)|10i =
1
2


1
1
−1
−1


, (H ⊗ H)|11i =
1
2


1
−1
−1
1

.
11.19 a) √
1
2
(|00i + |01i), b) √
1
2
(|00i − |01i), c) − √
1
2
(|10i + |11i), d) − √
1
2
(|10i − |11i)
Chapter 11. End-of-chapter exercises.
7. |11i =


0
0
0
1

.362 Solutions to selected exercises
9. a) ( √
1
2
, 0), b) ( √
1
2
, 0), c) 1
2
, d) 1
2
, e) 1
2
, f) 1
2
.
 1
2
1
2
1
2
1
2
!
.
10. a) ( √
1
2
, √
1
2
), b) (− √
1
2
, √
1
2
), c) 1, d) 0, e) 0, f) −1. 
1 0
0 −1

.
Chapter 13.
13.1 As the only eigenvalue of the identity operator is 1 all measurements will produce this
value. We also note that all vectors of ⊗nC
2 are eigenvectors. Considering the case C
2 ⊗ C
2
and choosing any orthonormal basis |φ1i, |φ2i, |φ3i, |φ4i, we have, for any |ψi ∈ C
2 ⊗ C
2
,
that
|ψi = α1|φ1i + α2|φ2i + α3|φ3i + α4|φ4i
for some αi ∈ C. Measuring I on |ψi will produce the eigenvalue 1 with probability 1; and
the post-measurement state will be |ψi, because each of the states |φ1i, |φ2i, |φ3i, |φ4i are
eigenvectors of I with eigenvalue 1. The extension of the solution to ⊗nC
2
is clear.
13.2 Again we consider the solution in ⊗2C
2
, the extension to ⊗nC
2 being clear. Assume
|φ1i, |φ2i, |φ3i, |φ4i are eigenvectors of both A and B and assume the corresponding eigen￾values of A and B are, respectively, λ1, . . . , λ4 and µ1, . . . , µ4.
Measuring A first we will measure λk with probability |αk|
2 with post-measurement state
|φki. Measuring B on |φki produces µk with probability 1. The result is the pair of mea￾surements (λk, µk) occurring with probability |αk|
2
.
Measuring B first we will measure µq with probability |αq|
2 with post-measurement state
|φqi. Measuring A on |φqi produces λq with probability 1. The result is the pair of mea￾surements (λq, µq) occurring with probability |αq|
2
.
The possible outcomes in each case are therefore (λ1, µ1), . . . ,(λ4, µ4), with (λi
, µi) occur￾ring with probability |αi
|
2
.
Chapter 17.
17.3
ˆf(x1, x2, y) = (x1, x2, y ⊕ f(x1, x2)) = (x1, x2, y ⊕ (x1 ∧ x2)).
This is the Toffoli gate.
Chapter 18.
18.1 By evaluating T
∨
D ◦ T
∨
D, it follows that T
∨
D is self inverse.
18.2 A suitable circuit, using the digital TD, T ∨
D and swap gates, is shown in Figure E.1.
x1
x2
0
0
x1
TD x2
x1
x2
x1 ∧ x2 x1 ∨ x2 = max{x1, x2}
x1 ∧ x2 = min{x1, x2}
×
×
0
T
∨
D
FIGURE E.1
A circuit, using invertible digital gates, to compute max{x1, x2} and min{x1, x2}.Solutions to selected exercises 363
Chapter 21.
21.1 Assuming that the quantum cnot gate may be expressed as the tensor product of two
1-qubit gates implies that we have:


1 0 0 0
0 1 0 0
0 0 0 1
0 0 1 0

 =

a b
c d 
⊗

A B
C D 
.
for some a, b, c, d and A, B, C, D. Comparing the upper left 2 ×2 matrices of each side gives
B = C = 0, and this is incompatible with the equivalence of the lower 2 × 2 matrices of
each side. Hence, no such representation of the quantum cnot gate is possible.References
The material presented in this book is based largely on the work of others. The authors
have benefitted from the following formally published material on mathematics, quantum
mechanics and computation.
In addition to the references cited below, the authors have benefitted from lecture notes, and
other materials, kindly made available by teachers and researchers in quantum computing
including: Coecke (Oxford University), Hannabuss (Oxford University), Jozsa (Cambridge
University), Watrous (University of Waterloo), de Wolf (University of Amsterdam), Childs
(University of Maryland), Preskill (California institute of Technology) and Rieffel and Polak
(MIT).
[1] J.M. Jauch. Foundations of Quantum Mechanics. Addison Wesley, 1968.
[2] G.W. Mackey. Mathematical Foundations of Quantum Mechanics. Benjamin, 1963.
[3] M.A. Nielsen and I.L. Chuang. Quantum Computation and Quantum Information, 10th
Anniversary Edition. Cambridge University Press, 2010.
365Index
adjoint
classical, 111
operator, 227
adjoint operator, 227
affine subset, 153
algebraic multiplicity, 159
algorithm, 323
Bernstein-Vazirani, 330
Deutsch, 326
Deutsch-Jozsa, 328
Shor’s, 323
Simon’s, 333
anti-linear, 87, 88
Argand diagram, 67
argument of a complex number, 67
argument of a function, 28
auxiliary bit, 285
axioms of quantum computation, 249
summary, 259
back-substitution, 119
basis, 143
Bell, 90
computational, 146, 256
standard orthonormal, 78
Bell basis, 90
Bell state, 208, 299
Bell, John, 301
Bernstein-Vazirani algorithm, 330
binary, 14
binary digit, 7
binary operation, 5
binary string, 13
bit, 14
Bloch sphere, 252
Bloch, Felix, 254
block multiplication of matrices, 104
Boolean function, 28
period of, 28, 333, 336
bra, 76
cardinality, 4
Cartesian product, 12
Cartesian product of vector spaces, 152
Cayley table, 11, 178
Cayley-Hamilton theorem, 175
co-domain, 28
column rank, 145
column space, 140
commutator, 130
complement, 5
completeness relation, 102
complex conjugate, 65
complex number, 64
Cartesian form, 64
exponential form, 69
imaginary part, 64
real part, 64
complexity, 325, 327, 330, 332
complexity theory, 33
composition table, 178
computational basis, 146, 256
congruence, 15
conjugate, 65
conjugate transpose, 76
continuous random variable, 344
coordinate vector, 223
coordinates
Cartesian, 349
polar, 349
spherical polar, 349
copying, 307
copying binary strings, 52
coset, 153
cycle notation, 29
De Moivre’s theorem, 74
degeneracy, 163
degree of a polynomial, 31
determinant, 109
deterministic, 246, 263, 343
Deutsch algorithm, 326
Deutsch-Jozsa algorithm, 328
diagonalisation, 113
digital computation, 244
digital computer
367368 Index
dynamics, 244
measurement, 244
states, 244
dimension of a vector space, 143
dimension theorem, 197
Dirac bra-ket, 76
directed line segment, 77
discrete Fourier transform, 72
discrete random variable, 344
disjoint sets, 5
domain, 28
dupe
function, 52
gate, 52
dynamics, 244
eigendirection, 160
eigenspace, 163
eigenvalue, 157
degenerate, 163
eigenvalues of a linear operator, 226
eigenvector, 157
eigenvectors of a linear operator, 226
Einstein, Albert, 301
Einstein-Podolsky-Rosen (EPR) state, 301
element, 4, 93
entangled photons, 301
entangled tensor, 207
entanglement, 256
EPR state, 301
equivalence
class, 21, 187
relation, 16, 21
Euclidean inner product, 83
Euler’s relations, 69
even function, 347
event, 343
exclusive-or, 5, 8
expectation, 345
exponential function, 31
Feynman gate, 269
Feynman, Richard, 245
field, 5
axioms, 5, 351
finite, 25
finite field, 25
function, 27
bijective, 28
Boolean, 28
composition, 30
exponential, 31
injective, 28
inverse, 30
many-to-one, 28
one-to-one, 28
onto, 28
periodic, 56
polynomial, 31
self-inverse, 47
surjective, 28
two-to-one, 336, 338
functionally complete, 41
gate
digital not gate, 34, 268
digital Peres gate, 46, 273
digital Toffoli gate, 45, 271
Feynman gate, 42, 269
Hadamard, 315
invertible digital cnot, 42, 269
Pauli, 314
phase-shift, 315
quantum cnot gate, 270
quantum not gate, 268
quantum Toffoli gate, 272
gates, 261
Gaussian elimination, 119
geometric multiplicity, 163
global phase, 251
Gram-Schmidt orthogonalisation process,
144
group, 9, 12, 177
Abelian, 10, 178
axioms, 177
Cayley table, 178
commutative, 10, 178
factor group, 189
general linear, 111, 184
isomorphic, 12, 178
isomorphism, 178
Klein-four group, 181
linear, 111
order, 178
orthogonal, 115
partition, 186
permutation, 10, 11
quotient group, 189
special orthogonal, 115
subgroup, 178
coset, 185
left coset, 185Index 369
normal subgroup, 187
symmetric, 11
unitary, 184
Hadamard gate, 315
Hadamard transform, 73
half-adder, 273
Hamming weight, 26
Hermitian conjugate, 76
Hermitian matrix, 95
hexadecimal, 14
hidden subgroup, 338
homogeneous, 118
homomorphism, 193
image, 196
imaginary number, 63
imaginary part, 64
inclusive-or,
5
inner product
Euclidean, 83
in
C
n
, 86
in
R
2 and
R
3
, 83
in
R
n
, 85
inner product space, 136
intersection of sets,
5
inverse function, 30
isomorphism, 11
, 178
kernel, 196
ket, 76
linear combination, 80
linear form, 197
linear functional, 197
linear group, 111
linear independence, 141
linear operator, 194
, 219
adjoint of, 227
eigenvalues, 226
eigenvectors, 226
self-adjoint, 228
linear transformation, 193
kernel of, 196
matrix as a, 195
linearly dependent, 142
linearly independent, 141
LinSpan, 138
logical connectives,
7
Maclaurin series, 69
mapping, 27
matrices, 93
block multiplication, 104
matrix, 14
, 93
diagonal, 94
element, 93
Hermitian, 95
identity, 94
non-singular, 110
normal, 100
, 101
orthogonal, 114
permutation, 103
product, 97
self-adjoint, 95
similar, 112
singular, 111
skew-symmetric, 96
square, 94
trace, 95
transpose, 94
unitary, 115
matrix group, 111
measurement, 245
modular arithmetic, 15
modulus of a complex number, 67
multiplicity, 159
, 163
no-cloning theorem, 307
non-degenerate, 163
non-singular matrix, 110
norm, 87
normal matrix, 100
, 101
null space, 140
nullity, 196
observable, 245
, 257
octal, 14
odd function, 347
operator, 194
, 257
projection, 258
unitary, 259
oracle, 289
, 326
ordered pair, 12
orthogonal, 87
orthogonal group, 115
orthogonal matrix, 114
orthogonal projection, 258
orthonormal, 87
outcome, 343
partition, 22
, 185
Pauli gates, 314370 Index
Pauli matrices, 115
, 184
Peres gate, 273
Peres half-adder, 273
period, 56
Boolean function, 28
, 333
, 336
permutation
cycle notation, 29
permutation group
P3
, 10
permutation matrix, 103
phase-shift gate, 315
Podolsky, Boris, 301
polar coordinates, 349
polynomial function, 31
power set,
6
probability, 343
experimental, 343
theoretical, 343
total, 344
projection, 258
projection matrix, 129
projection operator, 258
projective space, 22
projective spaces, 252
complex, 254
real, 252
proper subset,
5
proper superset,
5
quantum bit, 81
, 251
quantum computation, 245
axioms, 249
measurement, 257
observables, 257
quantum gates
Hadamard, 313
, 315
Pauli, 313
, 314
phase-shift, 315
root-not, 313
, 314
tensor product gate, 313
quantum measurement, 245
quantum parallelism, 305
quantum swapping, 306
quantum time-evolution, 243
qubit, 81
, 251
quotient group, 189
random variable, 344
continuous, 344
discrete, 344
expected value, 345
range, 28
rank, 145
, 197
rays, 252
register
digital, 244
relation, 19
equivalence, 21
one-to-one, 21
two-to-one, 21
relative phase, 71
resolution of the identity, 102
Rosen, Nathan, 301
row echelon form, 119
row rank, 145
row space, 140
row vector, 75
scalar, 76
self-adjoint matrix, 95
self-adjoint operator, 228
, 257
self-inverse
function, 47
group element, 179
sesquilinear, 87
, 88
set
element of,
4
empty set,
5
finite,
4
universal set,
5
sets
disjoint,
5
similar matrix, 112
Simon’s algorithm, 333
simultaneous equations, 117
singular matrix, 111
skew-symmetric matrix, 96
special orthogonal group, 115
spectral decomposition, 173
spectral theorem
self-adjoint matrices, 175
symmetric matrices, 173
spectrum, 159
spherical polar coordinates, 349
standard deviation, 345
state of a digital computer, 244
state spaces, 243
, 249
Stern-Gerlach experiment, 246
stochastic, 343
subgroup, 178
hidden, 338
trivial, 178
subset,
5Index 371
subspace, 137
affine subset of , 153
coset of, 153
superimposed state, 246
superposition, 80
, 246
, 251
, 257
superset,
5
swapping, 49
, 306
symmetric group
S
3
, 11
teleportation, 308
tensor, 205
entangled, 205
tensor product space, 205
, 256
time-evolution, 243
Toffoli gate, 271
trace of a matrix, 95
transformation, 126
transformation matrix, 126
transpose, 94
trigonometric functions, 347
trigonometric identities, 347
trigonometric ratios, 347
two-to-one function, 336
, 338
two-to-one relation, 21
union,
5
unitarily diagonalisable, 116
unitarily similar, 116
unitary group, 183
unitary matrix, 115
unitary operator, 259
universal set, 41
variance, 345
vector
column, 13
row, 13
, 75
unit, 78
zero, 76
vector space
dimension, 143
homomorphism, 193
isomorphism, 193
subspace of a, 137
vector spaces
Cartesian product, 152
vectors
dot product, 83
inner product, 83
linearly independent, 141
orthogonal, 87
orthonormal, 87
scalar product, 83
Venn diagram,
6
von Neumann-L¨uders, 299
xor,
8
zero vector, 76
