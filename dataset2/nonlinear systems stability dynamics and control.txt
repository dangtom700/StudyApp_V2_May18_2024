Nonlinear 
Systems 
Stability, Dynamics and ControlThis page intentionally left blankNEW JERSEY • LONDON • SINGAPORE • BEIJING • SHANGHAI • HONG KONG • TAIPEI • CHENNAI • TOKYO
World Scientific
Nonlinear 
Systems 
Stability, Dynamics and Control
Guanrong Chen
City University of Hong Kong, ChinaPublished by
World Scientific Publishing Co. Pte. Ltd.
5 Toh Tuck Link, Singapore 596224
USA office: 27 Warren Street, Suite 401-402, Hackensack, NJ 07601
UK office: 57 Shelton Street, Covent Garden, London WC2H 9HE
Library of Congress Control Number: 2023028635
British Library Cataloguing-in-Publication Data
A catalogue record for this book is available from the British Library.
NONLINEAR SYSTEMS
Stability, Dynamics and Control
Copyright © 2024 by World Scientific Publishing Co. Pte. Ltd. 
All rights reserved. This book, or parts thereof, may not be reproduced in any form or by any means, 
electronic or mechanical, including photocopying, recording or any information storage and retrieval 
system now known or to be invented, without written permission from the publisher.
For photocopying of material in this volume, please pay a copying fee through the Copyright Clearance 
Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, USA. In this case permission to photocopy 
is not required from the publisher.
ISBN 978-981-127-898-3 (hardcover)
ISBN 978-981-127-899-0 (ebook for institutions)
ISBN 978-981-127-900-3 (ebook for individuals)
For any available supplementary material, please visit 
https://www.worldscientific.com/worldscibooks/10.1142/13488#t=suppl
Printed in SingaporeDedicated to the memory of
Professor Gennady Alexeyevich Leonov (1947–2018)This page intentionally left blankPreface
Nonlinear systems constitute a fundamental subject for study in systems
engineering. The subject has been extensively investigated by both non￾linear control and nonlinear dynamics communities, whereas the focus is
usually very different, on controllers design and dynamics analysis, respec￾tively. The last two decades have witnessed gradual merging of control
theory and dynamics analysis, but not to the extent of controlling nonlin￾ear dynamics such as bifurcations and chaos. This monograph is an attempt
to fill the gap to a certain extent while presenting a rather comprehensive
coverage of the nonlinear systems theory in a self-contained and hopefully
easily-readable manner.
This introductory treatise is not intended to be a research reference with
the state-of-the-art theories and techniques presented, nor as a very com￾prehensive handbook, given that there are already many available in the
market today. It is written for self-study and, in particular, as an elemen￾tary textbook that can be taught in a one-semester course at the advanced
undergraduate level or entrance level of graduate curricula focusing on non￾linear systems — both control theory and dynamics analysis.
The main contents of the book comprise systems stability (Chapters 2–
4), bifurcation and chaos dynamics (Chapter 5) and controllers design
(Chapter 6), for both continuous-time and discrete-time settings. In partic￾ular, it discusses the special topics on bifurcation control and chaos control
at the end of the last chapter.
This monograph is presented in a textbook style, in which most contents
are elementary with some classical results and popular examples taken or
modified from the existing literature, which might have also appeared in
some other introductory textbooks. Since this is not a survey, a long list of
related references is not included, yet appreciation to the various original
viiviii Nonlinear Systems
sources are indicated. Throughout the book, to keep its contents at an
elementary level, some advanced theories are presented without detailed
proofs, merely for the completeness of the relevant discussions. This kind
of contents and exercises are marked by ∗
for indication.
To this end, I would like to especially thank Dr. Yi Jiang from the
City University of Hong Kong for helping proof-read the entire manuscript,
and thank Ms. Lakshmi Narayanan from the World Scientific Publishing
Company for her support and assistance.
Guanrong Chen
Hong Kong, 2023Contents
Preface vii
1. Nonlinear Systems: Preliminaries 1
1.1 A Typical Nonlinear Dynamical Model . . . . . . . . . . . 1
1.2 Autonomous Systems and Map Iterations . . . . . . . . . 3
1.3 Dynamical Analysis on Phase Planes . . . . . . . . . . . . 7
1.3.1 Phase Plane of a Planar System . . . . . . . . . . 7
1.3.2 Analysis on Phase Planes . . . . . . . . . . . . . . 11
1.4 Qualitative Behaviors of Dynamical Systems . . . . . . . . 16
1.4.1 Qualitative Analysis of Linear Dynamics . . . . . 19
1.4.2 Qualitative Analysis of Nonlinear Dynamics . . . 23
2. Stabilities of Nonlinear Systems (I) 37
2.1 Lyapunov Stabilities . . . . . . . . . . . . . . . . . . . . . 38
2.2 Lyapunov Stability Theorems . . . . . . . . . . . . . . . . 42
2.3 LaSalle Invariance Principle . . . . . . . . . . . . . . . . . 57
2.4 Some Instability Theorems . . . . . . . . . . . . . . . . . . 60
2.5 Construction of Lyapunov Functions . . . . . . . . . . . . 64
2.6 Stability Regions: Basins of Attraction . . . . . . . . . . . 68
3. Stabilities of Nonlinear Systems (II) 79
3.1 Linear Stability of Nonlinear Systems . . . . . . . . . . . . 79
3.2 Linear Stability of Nonlinear Systems with Periodic Linearity 85
3.3 Comparison Principles . . . . . . . . . . . . . . . . . . . . 89
3.3.1 Comparison Principle on the Plane . . . . . . . . 89
3.3.2 Comparison Principle in Higher-Dimensional Spaces 92
ixx Nonlinear Systems
3.4 Orbital Stability . . . . . . . . . . . . . . . . . . . . . . . 93
3.5 Structural Stability . . . . . . . . . . . . . . . . . . . . . . 94
3.6 Total Stability: Stability under Persistent Disturbances . 96
4. Stabilities of Nonlinear Systems (III) 101
4.1 Lur’e Systems Formulated in the Frequency Domain . . . 101
4.2 Absolute Stability and Frequency-Domain Criteria . . . . 104
4.2.1 Background and Motivation . . . . . . . . . . . . 104
4.2.2 SISO Lur’e Systems . . . . . . . . . . . . . . . . . 106
4.2.3 MIMO Lur’e Systems . . . . . . . . . . . . . . . . 111
4.3 Harmonic Balance Approximation and Describing Function 112
4.4 BIBO Stability . . . . . . . . . . . . . . . . . . . . . . . . 118
4.4.1 Small Gain Theorem . . . . . . . . . . . . . . . . 119
4.4.2 Relation between BIBO and Lyapunov Stabilities 121
4.4.3 Contraction Mapping Theorem . . . . . . . . . . . 123
5. Nonlinear Dynamics: Bifurcations and Chaos 129
5.1 Typical Bifurcations . . . . . . . . . . . . . . . . . . . . . 129
5.2 Period-Doubling Bifurcation . . . . . . . . . . . . . . . . . 133
5.3 Hopf Bifurcations in 2-Dimensional Systems . . . . . . . . 135
5.3.1 Hyperbolic Systems and Normal Form Theorem . 136
5.3.2 Decoupled 2-Dimensional Systems . . . . . . . . . 139
5.3.3 Hopf Bifurcation of 2-Dimensional Systems . . . . 144
5.4 Poincar´e Maps . . . . . . . . . . . . . . . . . . . . . . . . 145
5.5 Strange Attractors and Chaos . . . . . . . . . . . . . . . . 148
5.5.1 Chaotic Lorenz System . . . . . . . . . . . . . . . 150
5.5.2 Poincar´e–Bendixson Theorem . . . . . . . . . . . 152
5.5.3 Some Other Chaotic Systems . . . . . . . . . . . . 153
5.5.4 Characterizations of Chaos . . . . . . . . . . . . . 156
5.6 Chaos in Discrete-Time Systems . . . . . . . . . . . . . . 160
6. Nonlinear Systems Control 167
6.1 Feedback Control of Nonlinear Systems . . . . . . . . . . . 167
6.1.1 Engineering Perspectives on Controllers Design . . 168
6.1.2 A General Approach to Controllers Design . . . . 170
6.2 Feedback Controllers for Nonlinear Systems . . . . . . . . 171
6.2.1 Linear Controllers for Nonlinear Systems . . . . . 172
6.2.2 Nonlinear Controllers for Nonlinear Systems . . . 175Contents xi
6.2.3 General Ideas for Nonlinear Controllers Design . . 178
6.3 More about Nonlinear Controllers Design . . . . . . . . . 179
6.3.1 An Illustrative Example . . . . . . . . . . . . . . . 179
6.3.2 Adaptive Controllers . . . . . . . . . . . . . . . . 182
6.3.3 Lyapunov Redesign of Nonlinear Controllers . . . 188
6.3.4 Sliding-Mode Control . . . . . . . . . . . . . . . . 191
6.4 Controlling Bifurcations and Chaos . . . . . . . . . . . . . 194
6.4.1 Controlling Bifurcations . . . . . . . . . . . . . . . 194
6.4.2 Controlling Chaos . . . . . . . . . . . . . . . . . . 207
Bibliography 217
Index 219xii Nonlinear Systems
Notation
Rn Space of n-dimensional real vectors
C
1 Space of continuously differential functions
A⊤ Transpose of matrix A
|| · || Euclidean norm (L2-norm)
◦ Composite operation of two maps
Re{λ} Real part of eigenvalue λ
λmax(A) Maximum eigenvalue of matrix A
λmin(A) Minimum eigenvalue of matrix A
diag{·} Diagonal matrix
trace(A) Trace of matrix A
exp{x} Exponential function e
x
inf{·} InfimumChapter 1
Nonlinear Systems: Preliminaries
A nonlinear system in the mathematical sense refers to a set of nonlinear
equations, which can be algebraic, difference, differential, integral, func￾tional, and operator equations, or a combination of some of them. A non￾linear system is used to describe a physical device or process that otherwise
cannot be well defined by a set of linear equations of any kind, although a
linear system is considered as a special case of a nonlinear system. Dynam￾ical system, on the other hand, is used as a synonym of a mathematical or
physical system, where the output behavior evolves with time and some￾times with other varying system parameters as well. The system responses,
or behaviors, of a dynamical system is referred to as system dynamics.
1.1 A Typical Nonlinear Dynamical Model
A representative mathematical model of nonlinear dynamical systems is the
pendulum equation. The study of pendula can be traced back to as early as
Christian Huygens who investigated in 1665 the perfect synchrony of two
identical pendulum clocks that he invented in 1656. He then reported his
findings to the Royal Society of The Netherlands [Huygens (1665)].
A simple and idealized pendulum consists of a volumeless ball and a
rigid and massless rod, which is connected to a pivot, as shown in Fig. 1.1.
In this figure, ℓ is the length of the rod, m is the mass of the ball, g is
the constant of gravity acceleration, θ = θ(t) is the angle of the rod with
respect to the vertical axis, and f = f(t) is the resistive force applied to the
ball. The straight down position finds the ball at rest; but if it is displaced
by an angle from the reference axis and then let go, it will swing back and
forth on a circular arc within a vertical plane to which it is confined.
For general purpose of mathematical analysis of the pendulum, a basic
12 Nonlinear Systems
mg
f

θ
θ
Fig. 1.1 Schematic diagram of a pendulum model.
assumption is that the resistive force f is proportional to the velocity along
the arc of the motion trajectory of the volumeless ball, i.e. f = κ s˙, where
κ ≥ 0 is a constant and s = s(t) = ℓ θ(t) is the ball-traveled arc length
measured from the vertical reference axis.
It follows from Newton’s second law of motion that
m s¨ = − mg sin(θ) − κ s , ˙
or
¨θ +
κ
m
˙θ +
g
ℓ
sin(θ) = 0 . (1.1)
This is the idealized and damped pendulum equation, which is nonlinear
due to the involvement of the sine function. When κ = 0, i.e. f = 0, it
becomes the undamped pendulum equation
¨θ +
g
ℓ
sin(θ) = 0 . (1.2)
To this end, by introducing two new variables,
x1 = θ and x2 = ˙θ ,
the damped pendulum equation can be rewritten in the following state￾space form:
x˙ 1 = x2 ,
x˙ 2 = −
κ
m
x2 −
g
ℓ
sin(x1). (1.3)
Here, the variables x1 = x1(t) and x2 = x2(t) are called the system states,
for they describe the physical states, namely the angular position and an￾gular velocity respectively, of the pendulum.Nonlinear Systems: Preliminaries 3
From elementary physics, it is known that the pendulum state vector
x(t) = [x1(t) x2(t)]⊤ is periodic. In general, even in a higher-dimensional
case, a state x(t) of a dynamical system is a periodic solution if it is a
solution of the system and moreover satisfies x(t + tp) = x(t) for some
constant tp > 0. The least value of such tp is called the (fundamental)
period of the periodic solution, while the solution is said to be tp-periodic.
Although conceptually straightforward and formally simple, this pen￾dulum model has many important and interesting properties. This repre￾sentative model of nonlinear systems will be frequently referred to, not only
within this chapter but also throughout the book.
1.2 Autonomous Systems and Map Iterations
The pendulum model (1.3) is called an autonomous system, in which there
is no independent (or separated) time variable t, other than a time variable
as the system states, anywhere in the model formulation. On the contrary,
the following forced pendulum is nonautonomous:
x˙ 1 = x2 ,
x˙ 2 = −
κ
m
x2 −
g
ℓ
sin(x1) + h(t), (1.4)
since t exists as a variable in the function h(t), independent of the system
states, which in this example is an external force applied to the pendulum.
One example of such a force input is h(t) = a cos(ωt), which will change the
angular acceleration of the pendulum, where a and ω are some constants.
The forced pendulum (1.4) has a time variable, t, within the external
force term h(t), which may not be shown as the time variables in the system
states x1 and x2 for brevity. However, if h(t) = a cos(θ(t)), then the forced
pendulum is considered to be autonomous because the time variable in the
force term becomes the time variable of the system state x1(t). In the latter
case, the external input should be denoted as h(x1) instead of h(t) in the
system equations.
In general, an n-dimensional autonomous system is described by
x˙ = f(x; p), x0 ∈ R
n
, t ∈ [t0, ∞), (1.5)
and a nonautonomous system is expressed as
x˙ = f(x, t; p), x0 ∈ R
n
, t ∈ [t0, ∞), (1.6)
where x = x(t) = [x1(t) · · · xn(t)]⊤ is the state vector, x0 is the initial
state with initial time t0 ≥ 0, p is a vector of system parameters, which can4 Nonlinear Systems
be varied but are independent of time, and
f =



f1
.
.
.
fn


 =



f1(x1, . . . , xn)
.
.
.
fn(x1, . . . , xn)



is called the system function or vector field.
In a well-formulated mathematical model, the system function should
satisfy some defining conditions such that the model, for example (1.5) or
(1.6), has a unique solution for each initial state x0 in a region of interest,
Ω ⊆ Rn, and for each permissible set of parameters p. According to the
elementary theory of ordinary differential equations, this is ensured if the
function f satisfies the Lipschitz condition
||f(x) − f(y)|| ≤ α ||x − y||
for all x and y in Ω that satisfy the system equation, and for some constant
α > 0, called the Lipschitz constant. Here and throughout the book, || · ||
denotes the standard Euclidean norm (the “length”) of a vector, i.e. the
L2-norm. This general setting, i.e. with the fulfillment of some necessary
defining conditions for a given mathematical model, will not be repeatedly
mentioned and described later on, for simplicity of presentation.
Sometimes, an n-dimensional continuous-time dynamical system is de￾scribed by a time-varying map,
Fc(t) : x → g
￾
x, t; p

, x0 ∈ R
n
, t ∈ [t0, ∞), (1.7)
or, by a time-invariant map,
Fc : x → g
￾
x; p

, x0 ∈ R
n
, t ∈ [t0, ∞). (1.8)
These two maps, in the continuous-time case, take a function to a function;
so by nature they are operators, which however will not be further studied
in this book.
For the discrete-time case, with similar notation, a nonlinear dynamical
system is either described by a time-varying difference equation,
xk+1 = fk
￾
xk; p

, x0 ∈ R
n
, k = 0, 1, . . . , (1.9)
where the subscript of fk signifies the dependence of f on the discrete time
variable k, or described by a time-invariant difference equation,
xk+1 = f
￾
xk; p

, x0 ∈ R
n
, k = 0, 1, . . . . (1.10)
Also, it may be described either by a time-varying map,
Fd(k) : xk → gk
￾
xk; p

, x0 ∈ R
n
, k = 0, 1, . . . , (Nonlinear Systems: Preliminaries 5
or by a time-invariant map,
Fd : xk → g
￾
xk; p

, x0 ∈ R
n
, k = 0, 1, . . . . (1.12)
These discrete-time maps, particularly the time-invariant ones, are very
important and convenient for the study of system dynamics; they will be
further discussed in the book later on.
For a system given by a time-invariant difference equation, repeatedly
iterating the system function f leads to
xk = f ◦ · · · ◦ f
| {z }
k times
￾
x0

:= f
k
￾
x0

, (1.13)
where “◦” denotes the composition operation of two functions or maps.
Similarly, for a map Fd, repeatedly iterating it backwards yields
xk = Fd
￾
xk−1

= Fd
￾
Fd(xk−2)

= · · · := F
k
d
￾
x0

, (1.14)
where k = 0, 1, 2, . . . .
Example 1.1. For the map
f(x) = p x(1 − x), p ∈ R .
one has
f
2
(x) = f
￾
p x(1 − x)

= p [p x(1 − x)]￾
1 − [p x(1 − x)] 
,
where the last equality is obtained by substituting each x in the previous
step with [px(1−x)]. For a large number n of iterations, it quickly becomes
very messy and actually impossible to write out the final explicit formula
of the composite map f
n(x).
If a function or map f is invertible, with inverse f
−1
, then one has
f
−2
(x) = ￾
f
−1
2
(x), and f
−n(x) = ￾
f
−1
n
(x), and so on. With the con￾vention that f
0 denotes the identity map, namely f
0
(x) := x, a general
composition formula for an invertible f can be obtained:
f
n
(x) = f ◦ f
n−1
(x) = f
￾
f
n−1
(x)

, n = 0, ±1, ±2, . . . . (1.15)
The derivative of a composite map can be obtained via the chain rule.
For instance, in the 1-dimensional case,
￾
f
n
′￾
x0

= f
′
￾
xn−1

· · · f
′
￾
x0

. (1.16)
This formula is convenient to use, because one does not need to explicitly
compute f
n(x), or (f
n)
′
(x). Moreover, using the chain rule, one has
￾
f
−1
′
=
1
f
′
￾
x−1
 , x−1 := f6 Nonlinear Systems
Example 1.2. For f(x) = x(1 − x) with x0 = 1/2 and n = 3, one has
f
′
(x) = 1 − 2x , x1 = f(x0) = 1/4 , and x2 = f(x1) = 3/16 ,
so that
￾
f
3
′
(1/2) = f
′
(3/16) f
′
(1/4) f
′
(1/2)
=
￾
1 − 2(3/16)￾1 − 2(1/4)￾1 − 2(1/2)
= 0 .
Finally, consider a function or map f given by either (1.13) or (1.14).
Definition 1.1. For a positive integer n, a point x
∗
is called a periodic
point of period n, or an n-periodic point, of f, if it satisfies
f
n
￾
x
∗

= x
∗ but f
k
￾
x
∗

̸= x
∗
for 0 < k < n . (1.18)
If x
∗
is of period one (n = 1), then it is also called a fixed point, or an
equilibrium point, which satisfies
f
￾
x
∗

= x
∗
. (1.19)
Moreover, a point x
∗
is said to be eventually periodic of period n if there
is an integer m > 0 such that
f
m
￾
x
∗

is a periodic point and f
m+n
￾
x
∗

= f
m
￾
x
∗

. (1.20)
Consequently,
f
n+q
￾
x
∗

= f
q
￾
x
∗

for all q ≥ m .
This justifies the name “eventually”.
Example 1.3. The map f(x) = x
3 − x has three fixed points: x
∗
1 = 0 and
x
∗
1,2 = ±
√
2, which are solutions of the equation f(x
∗
) = x
∗
. It has two
eventually fixed points of period one: x
∗
1,2 = ±1, since their first iterates
go to the fixed point 0.
Definition 1.2. For a continuous-time function or map, f, with a fixed
point x
∗
, the forward orbit of x
∗
is
Ω
+
￾
x
∗

:= 
f
k
￾
x
∗

: k ≥ 0
	
.
If f is invertible, then the backward orbit of x
∗
is
Ω
−
￾
x
∗

:= 
f
k
￾
x
∗

: k ≤ 0
	
.
The whole orbit of x
∗
, thus, is
Ω
￾
x
∗

= Ω+
￾
x
∗

∪ Ω
−
￾
x
∗

=

f
k
￾
x
∗

: k = 0Nonlinear Systems: Preliminaries 7
Definition 1.3. For a continuous-time function or map, f, a set S ⊂ Rn
is said to be forward invariant under f, if f
k
(x) ∈ S for all x ∈ S and for
all k = 0, 1, 2, . . .. Furthermore, for an invertible f, a set S ⊂ Rn is said
to be backward invariant under f, if f
k
(x) ∈ S for all x ∈ S and for all
k = 0, −1, −2, . . ..
1.3 Dynamical Analysis on Phase Planes
In this section, a general 2-dimensional nonlinear autonomous system is
considered:
x˙ = f
￾
x, y
,
y˙ = g
￾
x, y
. (1.21)
In this system, the two functions f and g together describe the vector field
of the system. Here and in the following, for 2- or 3-dimensional systems,
the state variables will be denoted as x, y, and z, instead of x1, x2, and x3,
for notational convenience.
1.3.1 Phase Plane of a Planar System
The path traveled by a solution of the continuous-time planar system (1.21),
starting from an initial state (x0, y0), is a solution trajectory or orbit of the
system, and is sometimes denoted as φt(x0, y0).
For autonomous systems, the x– ˙x coordinate plane is called the phase
plane of the system. In general, even if y ̸= ˙x, the x–y coordinate plane is
called the (generalized) phase plane. In the higher-dimensional case, it is
called the phase space of the underlying dynamical system. Moreover, the
orbit family of an autonomous system, corresponding to all possible initial
conditions, is called a solution flow in the phase space. The graphical layout
of the solution flow provides a phase portrait of the system dynamics in the
phase space, as depicted by Fig. 1.2.
The phase portrait of the damped and undamped pendulum systems,
(1.1) and (1.2), are shown in Fig. 1.3.
Examining the phase portraits shown in Fig. 1.3, a natural question
arises: how can one determine the motion direction of the orbit flow in the
phase plane as the time evolves? Clearly, a computer graphic demonstration
can provide a fairly complete answer to this question. However, a quick
sketch to show the qualitative behavior of the system dynamics is still quite
possible, as illustrated by the following two example8 Nonlinear Systems
0
Orbits
Initial points
x
y
Fig. 1.2 Phase portrait on the phase plane of a dynamical system.
−3 −
θ
θ
˙
π π π 3π
(a) damped pendulum
−π π 3π
θ
θ
˙
(b) undamped pendulum
Fig. 1.3 Phase portraits of the damped and undamped pendula.
Example 1.4. Consider the simple linear harmonic oscillator
¨θ + θ = 0 .
By defining
x = θ and y = ˙θ ,
this harmonic equation becomes
x˙ = y ,
y˙ = − x .Nonlinear Systems: Preliminaries 9
With initial conditions x(0) = 1 and y(0) = 0, this equation has solution
x(t) = cos(t) and y(t) = − sin(t).
The solution trajectory in the x–y–t space and the corresponding orbit on
the x–y phase plane are sketched in Fig. 1.4, together with some other
solutions starting from different initial conditions. This shows clearly the
direction of motion of the phase portrait.
x
t
y
0 2 4 6 8ππππ
(a) phase portrait in the x–y–t space
x
y
(b) phase portrait on the x–y phase plane
Fig. 1.4 Phase portraits of the simple harmonic equation.
Example 1.5. Consider the normalized undamped pendulum equation
¨θ + sin(θ) = 0 .
By defining
x = θ and y = ˙θ ,
this pendulum equation can be written as
x˙ = y ,
y˙ = − sin(x).10 Nonlinear Systems
With initial conditions x(0) = 1 and y(0) = 0, it has solution
x(t) = θ(t) = 2 sin−1
￾
tanh(t)

and y(t) = ˙θ(t).
The phase portrait of this undamped pendulum, along with some other
solutions starting from different initial conditions, is sketched on the x–
y phase plane shown in Fig. 1.5. This sketch also clearly indicates the
direction of motion of the solution flow. The shape of a solution trajectory
of this undamped pendulum in the x–y–t space can also be sketched, which
could be quite complex however, depending on the initial conditions.
−π π x = θ
One Period
y = θ˙
Fig. 1.5 Phase portrait of the undamped pendulum equation.
Example 1.6. Another way to understand the phase portrait of the general
undamped pendulum
x˙ = y ,
y˙ = −
g
ℓ
sin(x)
is to examine its total (kinetic and potential) energy
E =
y
2
2
+
g
ℓ
Z x
0
sin(σ) dσ =
y
2
2
+
g
ℓ
￾
1 − cos(x)

.
Figure 1.6 shows the potential energy plot, P(x), versus x = θ, and the cor￾responding phase portrait on the x–y phase plane of the damped pendulum.
It is clear that the lowest level of total energy is E = 0, which corresponds
to the angular positions x = θ = ±2nπ, n = 0, 1, . . .. As the total energy
increases, the pendulum swings up or down, with an increasing or decreas￾ing angular speed, |y| = |
˙θ|, provided that E is within its limit indicated
by E2. Within each period of oscillation, the total energy E = constant,
according to the conservation law of energy, for this idealized undamped
penduluNonlinear Systems: Preliminaries 11
x = θ
y = E = 0
E = E1
Ep
E = E2
E = E3
˙
θ
Fig. 1.6 Phase portrait of the undamped pendulum versus its total energy.
1.3.2 Analysis on Phase Planes
This subsection addresses the following question: why is it important to
study autonomous systems and their phase portraits?
The answer to this question is provided by the following several theo￾rems, which together summarize a few important and useful properties of
autonomous systems in the study of nonlinear dynamics. Although these
theorems are stated and proven for planar systems in this subsection, they
generally hold for higher-dimensional autonomous systems as well.
Theorem 1.1. A nonautonomous system can be equivalently reformulated
as an autonomous one.
Proof. Consider a general nonautonomous system,
x˙ = f
￾
x, t
, x0 ∈ R
n
.
Let the independent time variable t be a new variable by defining xn+1(t) =
t for this separated variable t of the system. Then, ˙xn+1 = 1. Consequently,
the original system can be equivalently reformulated by augmenting it as

x˙
x˙ n+1 
=

f
￾
x, xn+1
1

,
which is an autonomous system.
Obviously the price to pay for this conversion, from a nonautonomous
system to an autonomous one, is the increase of dimension. In dynamic12 Nonlinear Systems
systems analysis, this usually is acceptable since the increase is only by one,
which is not a big deal for higher-dimensional systems. Nevertheless, this
shows that, without loss of generality, one may only discuss autonomous
systems in nonlinear dynamical analysis especially in higher-dimensional
cases.
However, it is important to note that, in a nonlinear control system of
the form
x˙ = f
￾
x, u(t)

,
which will be studied in detail later in the book, the controller u(t) is a time
function and is yet to be designed, which it is not a system variable. In this
case, one should not (cannot) convert the control system to be autonomous
using this technique; otherwise, the controller loses its physical meaning
and can never be designed for the intended control tasks. This issue will
be revisited later within the context of feedback controllers design.
Theorem 1.2. If x(t) is a solution of the autonomous system x˙ = f(x),
then so is the trajectory x(t + a), for any real constant a. Moreover, these
two solutions are the same, except that they may pass the same point on
the phase plane at two different time instants.
The last statement of the theorem describes the inherent time-invariant
property of autonomous systems.
Proof. Because d
dt x(t) = f(x(t)), for any real constant τ , one has
d
dt x(t + a)



t=τ
=
d
ds x(s)



s=τ+a
= f(x(s))



s=τ+a
= f(x(t + a))



t=τ
.
Since this holds for all real τ , it implies that x(t + a) is a solution of the
equation x˙ = f(x). Moreover, the value assumed by x(t) at time instant
t = t
∗
is the same as that assumed by x(t + a) at time instant t = t
∗ − a.
Hence, these two solutions are identical, in the sense that they have the
same trajectory if they are both plotted on the same phase plane.
Example 1.7. The autonomous system ˙x(t) = x(t) has a solution x(t) =
e
t
. It is easy to verify that e
t+a
is also a solution of this system for any real
constant a. These two solutions are the same, in the sense that they have
the same trajectory if they are plotted on the x– ˙x phase plane, except that
they pass the same point at two different time instants; for instance, the
first one passes the point (x, x˙) = (1, 1) at t = 0 but the second, at t = −aNonlinear Systems: Preliminaries 13
However, a nonautonomous system may not have such a property.
Example 1.8. The nonautonomous system ˙x(t) = e
t has a solution x(t) =
e
t
. But e
t+a
is not its solution if a ̸= 0.
Note that, if one applies Theorem 1.1 to Example 1.8 and let y(t) = t,
then
x˙(t) = e
y(t)
,
y˙(t) = 1 ,
(a)
which has solution
x(t) = e
t
,
y(t) = t .
(b)
Theorem 1.1 states that (b) is a solution of (a), which does not mean that
(b) is a solution of the original equation ˙x(t) = e
t
. In fact, only the first part
of (b), i.e. x(t) = e
t
, is a solution of the original equation, and the second
part of (b) is merely used to convert the given nonautonomous system to
be an autonomous one.
Theorem 1.3. Suppose that a given autonomous system x˙(t) = f(x(t))
has a unique solution starting from an initial state x(t0) = x0. Then, there
will not be any other (different) orbit of the same system that also passes
through this same point x0 on the phase plane at any time.
Before giving a proof to this result, two remarks are in order. First,
this theorem implies that the solution flow of an autonomous system has
simple geometry, as depicted in Fig. 1.7, where different orbits starting from
different initial states do not cross each other. Second, in the phase portrait
of the (damped or undamped) pendulum (see Fig. 1.3, it may seem that
there are more than one orbit passing through the points (−π, 0) and (π, 0)
etc. However, those orbits are periodic orbits, so the principal solution of
the pendulum corresponds to those curves located between the two vertical
lines passing through the two end points x = −π and x = π, respectively.
Thus, within each 2π period, actually no self-crossing exists. It will be
seen later that all such seemingly self-crossing occur only at those special
points called stable node (sink), unstable node (source), or saddle node (see
Fig. 1.8), where the orbits either spiral into a sink, spiral out from a source,
or spiral in and out from a saddle node in different directions.14 Nonlinear Systems
y
x
y
x = f(x) ·
ξ = f(ξ) ξ ≠ x
·
x
(a) flow has no self-crossing (b) crossing is impossible
Fig. 1.7 Simple phase portrait of an autonomous system.
(a) source (b) sink (c) saddle node
Fig. 1.8 Simple phase portrait of an autonomous system.
Proof. Let x1 and x2 be two solutions of x˙ = f(x), satisfying
x1(t1) = x0 and x2(t2) = x0 , t1 ̸= t2 .
By Theorem 1.2, one has
xe2(t) := x2
￾
t − (t1 − t2)

,
which is the same solution of the given autonomous system, namely,
x2(t) = xe2(t). (a)
This solution satisfies
xe2(t1) := x2
￾
t1 − (t1 − t2)

= x2(t2) = x0 .
Therefore, by the uniqueness of the solution, x1 and xe2 are the same:
x1(t) = xe2(t), (b)
since they are both equal to x0 at the same initial time t1. Thus, (a) and
(b) together imply that x1 and x2 are identical.
Note that a nonautonomous system may not have such a property.
Example 1.9. Consider the nonautonomous system
x˙(t) = cos(tNonlinear Systems: Preliminaries 15
This system has the following solutions, among others:
x1(t) = sin(t) and x2(t) = 1 + sin(t).
These two solutions are different, for if they are plotted on the phase plane,
they show two different trajectories:
x˙ 1(t) = cos(t) = ±
q
1 − sin2
(t) = ±
q
1 − x
2
1
,
x˙ 2(t) = cos(t) = ±
q
1 − sin2
(t) = ±
p
1 − [1 + sin(t) − 1]2
= ±
p
1 − (x2 − 1)2 .
These two trajectories cross over at a point, (x1, x2) = (1/2, 1/2), as can
be seen from Fig. 1.9.
0 12 x1, x2
1 – x1
2
1
2
x1 =
1 – x1 x 2 1 = –
1 – (x2 – 1) x 2 2 = –
1 – (x2 – 1) x 2 ˙ 2 =
˙
˙
˙
x1, x2 ˙ ˙
Fig. 1.9 Two crossing trajectories of a nonautonomous system.
Theorem 1.4. A closed orbit of the autonomous system x˙ = f(x) on the
phase plane corresponds to a periodic solution of the system.
Proof. For a τ -periodic solution, x(t), one has x(t0 + τ ) = x(t0) for any
t0 ∈ R, which means that the trajectory of x(t) is closed.
On the contrary, suppose that the orbit of x(t) is closed. Let x0 be a
point in the closed orbit. Then, x0 = x(t0) for some t0, and the trajectory
of x(t) will return to x0 after some time, say τ ≥ 0; that is, x(t0+τ ) = x0 =
x(t0). Since x0 is arbitrary, and so is t0, this implies that x(t + τ ) = x(t)
for all t, meaning that x(t) is periodic with period τ .
Yet, a nonautonomous system may not have such a property.
Example 1.10. The nonautonomous system
x˙ = 2 t y ,
y˙ = −2 t x ,16 Nonlinear Systems
has solution
x(t) = α cos(t
2
) + β sin(t
2
),
y(t) = − α sin(t
2
) + β cos(t
2
),
for some constants α and β determined by initial conditions. This solution
is not periodic, but it is a circle (a closed orbit) on the x–y phase plane.
As mentioned at the beginning of this subsection, the above four theo￾rems hold for general higher-dimensional autonomous systems. Since these
properties are simple, elegant and easy to use, which a nonautonomous sys￾tem may not have, it is very natural to focus a general study of complex
dynamics on autonomous systems in various forms with any dimensions.
This motivates the following investigations.
1.4 Qualitative Behaviors of Dynamical Systems
In this section, consider a general 2-dimensional autonomous system,
x˙ = f(x, y),
y˙ = g(x, y). (1.22)
Let Γ be a periodic solution of the system which, as discussed above, has a
closed orbit on the x–y phase plane.
Definition 1.4. Γ is said to be an inner (outer) limit cycle of system
(1.22) if, in an arbitrarily small neighborhood of the inner (outer) region
of Γ, there is always (part of) a nonperiodic solution orbit of the system.
Γ is called a limit cycle, if it is both inner and outer limit cycles.
Simply put, a limit cycle is a periodic orbit of the system that corre￾sponds to a closed orbit on the phase plane and possesses certain (attracting
or repelling) limiting properties. Figure 1.10 shows some typical limit cy￾cles for the 2-dimensional system (1.22), where the attracting limit cycle is
said to be stable, while the repelling one, unstable.
Example 1.11. The simple harmonic oscillator discussed in Example 1.4
has no limit cycles. The solution flow of the system constitutes a ring of
periodic orbits, called periodic ring, as shown in Fig. 1.4. Similarly, the
undamped pendulum has no limit cycles, as shown in Fig. 1.3.Nonlinear Systems: Preliminaries 17
(a) inner limit cycle (b) outer limit cycle (c) attracting limit cycle
(d) repelling limit cycle (e) saddle limit cycle (f) saddle limit cycle
Fig. 1.10 Periodic orbits and limit cycles.
This example shows that, although a limit cycle is a periodic orbit, not
all periodic orbits are limit cycles, not even inner or outer limit cycles.
Example 1.12. A typical example of a stable limit cycle is the periodic
solution of the Rayleigh oscillator, described by
x¨ + x = p
￾
x˙ − x˙
3

, p > 0 , (1.23)
which was formulated in the 1920s to describe oscillations in some electrical
and mechanical systems. This limit cycle is shown in Fig. 1.11 for some
different values of p. These phase portraits are usually obtained either
numerically or experimentally, because they do not have simple analytic
formulas.
Example 1.13. Another typical example of a stable limit cycle is the pe￾riodic solution of the van der Pol oscillator, described by
x¨ + x = p
￾
1 − x
2

x , p > ˙ 0 , (1.24)
which was formulated around 1920 to describe oscillations in a triode cir￾cuit. This limit cycle is shown in Fig. 1.12, which is usually obtained either
numerically or experimentally, because it does not have a simple analytic
formul18 Nonlinear Systems
x
x
x˙ x˙
(a) (b)
x˙ x˙
x x
(c) (d)
Fig. 1.11 Phase portrait of the Rayleigh oscillator. (a) p = 0.01; (b) p = 0.1; (c) p = 1.0;
(d) p = 10.0.
x
x
–3
–3
–2
–1
1
2
3
˙
–2 –1 0 1 2 3
Fig. 1.12 Phase portrait of the van der Pol oscillator.Nonlinear Systems: Preliminaries 19
1.4.1 Qualitative Analysis of Linear Dynamics
For illustration, consider a 2-dimensional linear autonomous (i.e. time￾invariant) system,
x˙(t) = A x(t), x(0) = x0 , (1.25)
where A is a given 2×2 constant matrix and, for simplicity, the initial time
is set to t0 = 0.
Obviously, this system has a unique fixed point x
∗ = 0 and has a unique
solution x(t) = e
tAx0. Decompose its solution as
x(t) = e
tA x0 = M etJM−1x0 , (1.26)
where M =

v1 v2

with v1 and v2 being two linearly independent real
eigenvectors associated with the two eigenvalues of A, and J is in the Jordan
canonical form, which is one of the following three possible forms:

λ1 0
0 λ2

,

λ κ
0 λ

,

α −β
β α 
,
with λ1, λ2, λ, α, and β being real constants, and κ = 0 or 1. Note that
for the third case, its eigenvalues are complex conjugates: λ1,2 = α ± j β,
where j =
√
−1.
Thus, there are three cases to study, according to the three different
canonical forms of the Jordan matrix J shown above.
Case (i). The two constants λ1 and λ2 are different, but both real and
nonzero.
In this case, λ1 and λ2 are the eigenvalues of matrix A, associated with
two eigenvectors v1 and v2, respectively. Let
z = M−1x ,
where M =

v1 v2

. Then, the given system is transformed to
z˙ =

λ1 0
0 λ2

z , with z0 = M−1x0 := 
z10
z20 
.
Its solution is
z1(t) = z10 e
tλ1 and z2(t) = z20 e
tλ2
,
which are related by
z2(t) = c z
λ2/λ1
1
(t), with c = z20￾
z10−λ2/λ1
.
To show the phase portraits of the solution flow, there are three situa￾tions to consider: (a) λ2 < λ1 < 0; (b) λ2 < 0 < λ1; (c) 0 < λ2 < λ120 Nonlinear Systems
z1 0
z2
Fig. 1.13 Phase portrait of the transformed case (a): λ2 < λ1 < 0.
Only situation (a) is discussed in detail here. In this case, the two
eigenvalues are both negative, so that e
tλ1 → 0 and e
tλ2 → 0 as t → ∞,
but the latter tends to zero faster. The corresponding phase portrait is
shown in Fig. 1.13, where the fixed point (the origin) is a stable node.
Now, return to the original state, x = Mz. The original phase por￾trait is somewhat twisted, as shown in Fig. 1.14. Figures 1.13 and 1.14
are topologically equivalent, hence can be considered to be the same qual￾itatively. A more precise meaning of topological equivalence will be given
later in (1.29). Roughly speaking, it means that their dynamical behaviors
are qualitatively similar.
0
v2 v1
x1
x2
Fig. 1.14 Phase portrait of the original case (a): λ2 < λ1 < 0.
The other two situations, (b) and (c), can be analyzed in the same way,
where case (b) shows a saddle node and case (c), an unstable node. This
is left as an exercise to sketch.Nonlinear Systems: Preliminaries 21
Case (ii). The two constants λ1 and λ2 are nonzero complex conjugates:
λ1,2 = α ± j β, where j =
√
−1. Let
z = M−1x ,
and transform the given system to
z˙ =

α −β
β α 
z , with z0 =

z10
z20 
.
In polar coordinates,
r =
q
z
2
1 + z
2
2
and θ = tan−1

z2
z1

,
which has solution
r(t) = r0 e
α t and θ(t) = θ0 + β t ,
where r0 = (z
2
10+z
2
20)
1/2 and θ0 = tan−1
￾
z20/z10
. This solution trajectory
is visualized by Fig. 1.15, where the fixed point (the origin) in case (a) is
called a stable node, in case (b), an unstable focus, and in case (c), a center.
On the original x–y phase plane, the phase portrait has a twisted shape, as
shown in Fig. 1.16.
α 
r r r
< 0 α > 0 α = 0
(a) stable focus (b) unstable focus (c) center
Fig. 1.15 Phase portrait of the transformed Case (ii): λ1,2 = α ± j β.
x
y yy
x x
(a) stable focus (b) unstable focus (c) center
Fig. 1.16 Phase portrait of the original Case (ii): λ1,2 = α ± j β22 Nonlinear Systems
Case (iii). The two constants λ1 and λ2 are nonzero multiple real values:
λ1 = λ2 := λ. Let
z = M−1x ,
and transform the given system to
z˙ =

λ κ
0 λ

z , with z0 =

z10
z20 
.
Its solution is
z1(t) = e
λt￾
z10 + κ z20t

and z2(t) = z20 e
λt
,
which are related via
z1(t) = z2(t)

z10
z20
+
κ
λ
ln 
z2(t)
z20  .
Its phase portrait is shown in Fig. 1.17, and its corresponding phase portrait
on the x–y phase plane is similar; in particular, the linear coordinates that
transform M do not change the shape of any straight line on the two phase
planes.
λ < 0 κ = 0
λ > 0 κ = 0
z2
z1
z2
z1
(a) stable focus (b) unstable focus
λ < 0 κ = 1
λ > 0 κ = 1
z2
z1
z2
z1
(c) stable node (d) unstable node
Fig. 1.17 Phase portrait of Case (iii): λ1 = λ2 ̸= 0.
Case (iv). One, or both, of λ1,2 is zero.
In this degenerate case, the matrix A in x˙ = Ax has a nontrivial null
space, of dimension 1 or 2 respectively, so that any vector in the null spacNonlinear Systems: Preliminaries 23
of A is a fixed point. As a result, the system has a fixed or equilibrium
subspace. Specifically, these two situations are as follows:
(a) λ1 = 0 but λ2 ̸= 0
In this case, the system can be transformed to
z˙ =

0 0
0 λ2

z , with z0 =

z10
z20 
,
which has solution
z1(t) = z10 and z2(t) = z20 e
λ2t
.
The phase portrait of x = Mz is shown in Fig. 1.18, where (a) shows a
stable equilibrium subspace and (b), an unstable subspace.
x2
x1
x2
x1
(a) a stable equilibrium subspace (b) an unstable equilibrium subspace
Fig. 1.18 Phase portrait of Case (iv) (a): λ1 = 0 but λ2 ̸= 0.
(b) λ1 = λ2 = 0
In this case, the system is transformed to
z˙ =

0 1
0 0 
z , with z0 =

z10
z20 
,
which has solution
z1(t) = z10 + z20 t and z2(t) = z20 .
The phase portrait of x = Mz is shown in Fig. 1.19, which is a saddle
equilibrium subspace.
1.4.2 Qualitative Analysis of Nonlinear Dynamics
This subsection is devoted to some qualitative analysis of dynamical be￾haviors of a general nonlinear autonomous system in a neighborhood of a24 Nonlinear Systems
x1
x2
Fig. 1.19 Phase portrait of Case (iv) (b): λ1 = λ2 = 0.
fixed point (equilibrium point) of the system. Therefore, unlike the linear
systems discussed above, all results derived here are local.
Consider a general nonlinear autonomous system,
x˙ = f(x), x0 ∈ R
n
, (1.27)
where it is assumed that f ∈ C
1
, i.e. it is continuously differentiable with
respect to its arguments, and that the system has a fixed point, x
∗
.
Taylor-expanding f(x) at x
∗ yields
x˙ = f
￾
x
∗

+

∂f
∂x

x=x∗
￾
x − x
∗

+ e(x) = J
￾
x − x
∗

+ e(x),
where
J =

∂f
∂x

x=x∗
=





∂f1/∂x1 · · · ∂f1/∂xn
.
.
.
.
.
.
∂fn/∂x1 · · · ∂fn/∂xn





x=x∗
is the Jacobian, and e(x) = o(||x||) represents the residual of all higher￾order terms, which satisfies
lim
||x||→∞
||e(x)||
||x|| = 0 .
Letting y = x − x
∗
leads to
y˙ = J y + e(y),
where e(y) = o(||y||). In a small neighborhood of x
∗
, ||x − x
∗
|| is small,
so o(||y||) ≈ 0. Thus, the nonlinear autonomous system (1.27) and its
linearized system x˙ = J (x − x
∗
) have the same dynamical behaviors; par￾ticularly, the latter in a small neighborhood of x
∗
is the same as y˙ = J yNonlinear Systems: Preliminaries 25
a small neighborhood of 0. In other words, between x and y, the following
are comparable:
x˙ = f(x) versus y˙ = J y (1.28)
x
∗
is



stable node
unstable node
stable focus
unstable focus
saddle node
⇐⇒ y = 0 is



stable node
unstable node
stable focus
unstable focus
saddle node
(1.29)
In this sense, the local dynamical behaviors of the two systems in (1.28)
are said to be qualitatively the same, or topologically equivalent. A precise
mathematical definition is given as follows.
Definition 1.5. Two time-invariant system functions, f : X → Y and
g : X∗ → Y
∗
, where X, Y , X∗
, and Y
∗ are (open sets of) metric spaces,
are said to be topologically equivalent, if there is a homeomorphism, h :
Y → Y
∗
, such that h
−1
: X∗ → X and
g(x) = h
−1
◦ f ◦ h(x), x ∈ X ,
where ◦ is the composite operation of two maps.
This definition is illustrated in Fig. 1.20. Here, a homeomorphism is
an invertible continuous function whose inverse is also continuous. For
instance, for X = Y = R, the two functions f(x) = 2x
3 and g(x) = 8x
3 are
topologically equivalent. This is because one can find a homeomorphisim,
h(x) = (x)
1/3
, which yields
h
−1
◦ f ◦ h(x) = ￾
2
￾
(x)
1/3
33
= 8x
3 = g(x).
A homeomorphism preserves the system dynamics as seen from the one￾one correspondence (1.29). When both X and Y are Euclidean spaces, the
homeomorphism h may be viewed as a nonsingular coordinates transform.
For discrete-time systems (maps), such topological equivalence is also
called the topological conjugacy, and the two maps are said to be topologi￾cally conjugate if they satisfy the relationships shown in Fig. 1.20, where h
is a homeomorphism.
Theorem 1.5. If f and g are topologically conjugate, then
(i) the orbits of f are mapped to the orbits of g under h;
(ii) if x
∗
is a fixed point of f, then the eigenvalues of f
′
(x
∗
) are mapped to
the eigenvalues of g
′
(x
∗
26 Nonlinear Systems
X∗ ✲ Y
∗
g
X ✲ Y
f
✻
h−1
❄
h
Fig. 1.20 Two topologically equivalent functions or topologically conjugate maps.
Proof. First, note that the orbit of x
∗ under iterates of map f is
Ω(x
∗
) = 
. . . , f −k
(x
∗
), . . . , f −1
(x
∗
), x∗
, f(x
∗
), . . . , f k
(x
∗
)
	
.
Since f = h
−1 ◦ g ◦ h, for any given k > 0 one has
f
k
(x
∗
) = ￾
h
−1
◦ g ◦ h

◦ · · · ◦ ￾
h
−1
◦ g ◦ h

(x
∗
)
= h
−1
◦ g
k
◦ h(x
∗
).
On the other hand, since f
−1 = h
−1 ◦ g
−1 ◦ h, for any given k > 0 one has
h ◦ f
−k
(x
∗
) = g
−k
◦ h(x
∗
).
A comparison of the above two equalities shows that the orbit of x
∗ under
iterates of f is mapped by h to the orbit of h(x
∗
) under iterates of map g.
This proves part (i).
The conclusion of part (ii) follows from a direct calculation:
df
dx



x=x∗
=
dh−1
dx



x=x∗
·
dg
dx



x=h(x∗)
·
dh
dx



x=x∗
,
noting that similar matrices have the same eigenvalues.
Example 1.14. The damped pendulum system (1.1), namely,
x˙ = y ,
y˙ = −
κ
m
y −
g
ℓ
sin(x),
has two fixed points:
￾
x
∗
, y∗

=
￾
θ
∗
,
˙θ
∗

= (0, 0) and ￾
x
∗
, y∗

=
￾
θ
∗
,
˙θ
∗

= (π, 0).
It is known from the pendulum physics (see Fig. 1.21) that the first fixed
point is stable while the second, unsNonlinear Systems: Preliminaries 27
π
(a) stable fixed point (b) unstable fixed point
Fig. 1.21 Two fixed points of the damped pendulum.
According to the above analysis, the Jacobian of the damped pendulum
system is
J =

0 1
−g ℓ−1
cos(x) −κ/m 
.
There are two cases to consider at the two fixed points:
(a) x
∗ = θ
∗ = 0
In this case, the two eigenvalues of J are
λ1,2 = −
κ
2m
±
1
2
q￾
κ/m2
− 4
￾
g/ℓ
,
implying that the fixed point is stable since Re{λ1,2} < 0.
(b) x
∗ = θ
∗ = π
In this case, the two eigenvalues of J are
λ1,2 = −
κ
2m
±
1
2
q￾
κ/m2
+ 4￾
g/ℓ
,
where Re{λ1} > 0 and Re{λ2} < 0, which implies that the fixed point is a
saddle node and, hence, is unstable in one direction on the plane shown in
Fig. 1.21, along which the pendulum can swing back and forth.
Clearly, the mathematical analysis given here is consistent with the
physics of the damped pendulum as discussed before.
Example 1.15. For easy explanation of concept, consider a composite 2-
and 1-dimensional autonomous system,
x˙ =


−1 −2 0
2 −1 0
0 0 1

 x .
At the fixed point (0, 0, 0), this system has eigenvalues −1 ± 2j and 1,
implying that the origin is a saddle node, as illustrated by the phase portrait
in Fig. 128 Nonlinear Systems
Fig. 1.22 3-dimensional saddle node.
Example 1.16. Consider a simplified coupled-neuron model,
x˙ = − αx + h(β − y),
y˙ = − αy + h(β − x),
where α > 0 and β > 0 are constants, and h(u) is a continuous func￾tion satisfying h(−u) = −h(u) with h
′
(u) being two-sided monotonically
decreasing as u → ±∞. One typical case is the sigmoidal function
h(u) = 2
1 − e−au
− 1 , a > 0 .
In this coupled-neuron model, with the general function h as described, one
has the following:
(i) there is a fixed point at x
∗ = y
∗
:= λ;
(ii) if
h
′
(β − λ) = −
dh(β − y)
dy




y=λ
< α ,
then this fixed point is unique and is a stable node;
(iii) if h
′
(β − λ) > α, then there are two other fixed points, at
￾
µ, α−1h(β − µ)

and ￾
α
−1h(β − µ), µ
respectively, for the same value of µ; they are stable nodes; but the
one at (λ, λ) becomes a saddle point in this case.
Now it is noted that a fixed point at x
∗ = y
∗ = λ is equivalent to
showing that − αλ + h(β − λ) = 0, or that the straight line z = αx and the
curve z = h(β − x) has a crossing point on the x–z plane. This is obvious
from the geometry depicted in Fig. 1.23, since h is continuouNonlinear Systems: Preliminaries 29
h(u
u
)
0
h(β
β
− x
x
)
0 λ β
f f = x
x =⇒ =⇒
Fig. 1.23 Existence of a fixed point in the coupled-neuron model.
Then it is noted that λ is the unique root of equation
f(λ) := −αλ + h(β − λ) = 0
being equivalent to showing that the function f(λ) is strictly monotonic,
so that f(λ) = 0 has only one root. To verify this, observe that
f
′
(λ) = − α + h
′
(β − λ) < 0 .
where h
′
(β − λ) = −d h(β − λ)/dλ < α by assumption. This implies that
f(λ) is decreasing. Moreover,
h(−u) = − h(u) =⇒ −h
′
(−u) = − h
′
(u) =⇒ h
′
(−u) = h
′
(u).
Since h
′
(u) is two-sided monotonically decreasing as u → ±∞, so are h
′
(−u)
and
f
′
(λ) = − α + h
′
(β − λ).
Therefore, f(λ) is strictly monotonic, so f(λ) = 0 has only one root.
To determine the stability of this root, by examining the Jacobian
J


x=y=λ
=

− α h′
(β − λ)
h
′
(β − λ) − α

,
one can see that its eigenvalues
s1 = − α − h
′
(β − λ) and s2 = − α + h
′
(β − λ)
satisfy s1 < s2 < 0, since h
′
(β −λ) < α. Hence, x = y = λ is a stable node.
Finally, consider the following equation:
f(x) = − αx + h(β − x) = 0
on the x–z plane. If h
′
(β − λ) > α, then it can be verified that the curve
z = h(β − x) and the straight line z = αx have three crossing points, as
shown in Fig. 1.24.
In the above, it has already been shown that at least one crossing point
is at x = λ, where h
′
(β − λ) > α. It can be further verified that there must30 Nonlinear Systems
λ 0 λλ r
f
β
α
x
x
h(β − x)
Fig. 1.24 Three crossing points between the curve and the straight line.
be two more crossing points, one at λr > λ and the other at λℓ < λ, as
depicted in Fig. 1.24.
Indeed, for x > λ, since h
′
(β − x) is monotonically decreasing as dis￾cussed above, one has
f
′
(x) = − α + h
′
(β − x) > − α + h
′
(β − λ) > 0 ,
so that
h
′
(β − x) > α > 0 , for all x > α .
Since h
′
(β − x) is two-sided monotonically decreasing, h
′
(β − x) → −∞ as
x → ∞, there must be a point, λr, such that h
′
(β − λr) = α. This implies
that the two curves h(β − x) and αx have a crossing point λr > λ. The
existence of another crossing point, λℓ < λ, can be similarly verified.
Next, to find the two new fixed points of the system, one can set
− αx + h(β − y) = 0
to obtain
x
∗
1 = α
−1h(β − µ),
y
∗
1 = µ ,
and set
− αy + h(β − x) = 0
to obtain
x
∗
2 = µ ,
y
∗
2 = α
−1h(β − µ),
where µ is a real value. These solutions have the same Jacobian, and the
eigenvalues of the Jacobian are
s1 = − α −
p
h
′(β − x)h
′(β − y),
s2 = − α +
p
h
′(β − x)h
′(β − y),
which satisfy s1 < s2 < 0 at the above two crossing points, and satisfy
s1 < 0 < s2 at x
∗ = y
∗ = λ, where the latter is a saddle node.Nonlinear Systems: Preliminaries 31
Now, return to the general nonlinear autonomous system (1.27).
Definition 1.6. The fixed point x
∗ of the autonomous system (1.27) is
said to be hyperbolic, if all the eigenvalues of the system Jacobian J at this
fixed point have nonzero real parts.
The importance of hyperbolic fixed points of a nonlinear autonomous
system can be appreciated by the following fundamental result on the local
dynamics of the autonomous system.
Theorem 1.6 (Grobman–Hartman Theorem for Systems). Let
x
∗
be a hyperbolic fixed point of the nonlinear autonomous system (1.27).
Then, the dynamical properties of this system is qualitatively the same as
that of its linearized system, in a (small) neighborhood of x
∗
.
Here, the equivalence of the dynamics of the two systems is local, and
this theorem is not applicable to a nonautonomous system in general.
Proof. See [Robinson (1995)]: p. 158.
For discrete-time systems, there is another version of the theorem for
maps.
Theorem 1.7 (Grobman–Hartman Theorem for Maps). Let x
∗
be
a hyperbolic fixed point of the continuously differentiable map f : Rn → Rn.
Then, the dynamical properties of this map is qualitatively the same as that
of its linearized map [Df(x
∗
)] : Rn → Rn, in a (small) neighborhood of x
∗
.
Proof. Without loss of generality, assume that x
∗ = 0. Let A = [Df(0)],
and decompose its state space according to the stable eigenvalues, denoted
Es
, and unstable eigenvalues, denoted Eu
, respectively. Then, Rn = Es ⊕
Eu
. Denote As = A


Es and Au = A


Eu , defined and restricted respectively
on the two eigenspaces. By choosing appropriate coordinates, it can be
assumed that ||As|| < 1 and ||A−1
u
|| < 1 < ||Au||. Moreover, denote µ =
max{||As||, ||A−1
u
||} < 1.
In a (small) neighborhood of the fixed point x
∗ = 0, consider the expan￾sion of the map f(x) = [Df(0)]x + g(x), with the higher-order terms satis￾fying g(0) = 0 and [Dg(0)] = 0. Thus, for any small δ > 0 there is a (small)
neighborhood of the fixed point, in which supx∈Rn {||g(x)||+|| [Dg(x)] ||} <
δ. This guarantees the existence of f
−1
, which is also continuously differ￾entiable.32 Nonlinear Systems
Now, the proof is carried out by verifying the topologically conjugate
relationship shown in Fig. 1.20.
The objective is to find a homeomorphism h : Rn → Rn in the form of
h = I + k, where I is the identity map and k : Rn → Rn is a continuous
map, such that
h ◦ (A + g) = A ◦ h .
It can be verified that the topologically conjugate relationship is equivalent
to either
k = −g ◦ (A + g)
−1 + A ◦ k ◦ (A + g)
−1
or
k = A
−1
◦ g + A
−1
◦ k ◦ (A + g).
Now, in the small neighborhood of the fixed point x
∗ = 0, define a
map F(k, g) = Fs(k, g) + Fu(k, g), according to the above topologically
conjugate relationship, as follows:
Fs(k, g) = −gs ◦ (A + g)
−1 + As ◦ ks ◦ (A + g)
−1
and
Fu(k, g) = A
−1
u ◦ gu + A
−1
u ◦ ku ◦ (A + g).
It can be verified that F(k, g) is continuous in g, satisfying F(0, 0) = 0,
and
||F(k, g)|| ≤ ||g|| + µ||k|| and ||F(k, g) − F(k
′
, g)|| ≤ µ||k − k
′
|| ,
for all continuous k, k
′ ∈ Rn. Therefore, F(k, g) is also continuous in k
and furthermore F(·, g) is a uniform contraction mapping. Consequently,
F(k
′
, g) = g for some k
′
if and only if there is a k such that k
′ = k(g). It
follows that h(g) = I + k(g) and h ◦ (A + g) = A ◦ h.
It remains to show that this h = h(g) is a homeomorphism.
Consider (A+g)◦r = r◦A with r = I+k
′
. Similarly to the above, it can
be verified that for each g, correspondingly there exists a unique r = r(g),
perhaps for a smaller δ > 0. It follows from the conjugate relationships for
both h and r that
h ◦ r =

A−1 ◦ h ◦ (A + g)

◦ r
= A−1 ◦ h ◦

(A + g) ◦ r

= A−1 ◦ h ◦ r ◦ A .
On the other hand,
h ◦ r = I + k
′ + k ◦ [I + k
′
] := I + s .Nonlinear Systems: Preliminaries 33
Hence,
s = A
−1
◦ s ◦ A if and only if s = A ◦ s ◦ A
−1
.
This implies that F(s, 0) = s. Since the fixed point is unique and since
F(0, 0) = 0, one has s = 0; therefore, h ◦ r = I. Similarly, it can be shown
that r ◦ h = I. Thus, h is a homeomorphism.
The following example provides a visual illustration of Theorem 1.6.
Example 1.17. Consider a nonlinear system,
x˙ = −x ,
y˙ = x
2 + y .
Its linearized system at (0, 0) is
x˙ = −x ,
y˙ = y .
Their phase portraits are shown in Fig. 1.25.
0 0
(a) nonlinear system (b) linearized system
Fig. 1.25 Illustration of the Grobman–Hartman Theorem.
As can be seen, the two phase portraits are not exactly the same, but
they are qualitatively the same, namely topologically equivalent, in the
sense that one can be obtained from the other by smoothly bending the
flow of the solution curves.34 Nonlinear Systems
Exercises
1.1 For the following two linear systems, sketch by hand their phase
portraits and classify their fixed points:
x˙ = −3x + 4y , y˙ = −2x + 3y ,
and
x˙ = 4x − 3y , y˙ = 8x − 6y .
1.2 Consider the Duffing oscillator equation
x¨(t) + a x˙(t) + b x(t) + c x3
(t) = γ cos(ωt), (1.30)
where a, b, c are constants and γ cos(ωt) is an external force input.
By defining y(t) = ˙x(t), rewrite this equation in a state-space form.
Use a computer to plot its phase portraits for the following cases:
a = 0.4, b = −1.1, c = 1.0, ω = 1.8, and (1) γ = 0.620, (2) γ =
1.498, (3) γ = 1.800, (4) γ = 2.100, (5) γ = 2.300, (6) γ = 7.000.
Indicate the directions of the orbit flows.
1.3 Consider the Chua circuit, shown in Fig. 1.26, which consists of one
inductor (L), two capacitors (C1, C2), one linear resistor (R), and
one piecewise-linear resistor (g).
R
L
C2
vC2 vC1
g(·)
C1
iL
+ +
– –
Fig. 1.26 The Chua circuit.
The dynamical equations of the circuit are
C1 v˙C1
= R
−1
￾
vC2
− vC1
) − g(vC1

,
C2 v˙C2
= R
−1
￾
vC1
− vC2

+ iL
,
L ˙iL = −vC2
, (1.3Nonlinear Systems: Preliminaries 35
where iL
is the current through the inductor L, vC1
and vC2
are the
voltages across C1 and C2 respectively, and
g
￾
vC1

= m0vC1
+
1
2
￾
m1 − m0
 ￾ |vC1
+ 1| − |vC1
− 1|

,
with m0 < 0 and m1 < 0 being some appropriately chosen con￾stants. This piecewise linear function is shown in Fig. 1.27.
✲
✻
PPPPP
❅
❅
❅❅PPPPP
• •
−1 1
m0
m0
m1
Fig. 1.27 The piecewise linear resistance in the Chua circuit.
Verify that, by defining p = C2/C1 > 0 and q = C2R2/L > 0, with
a change of variables, x(τ ) = vC1
(t), y(τ ) = VC2
(t), z(τ ) = R iL
(t),
and τ = t/(GC2), the above circuit equations can be reformulated
into the state-space form, as
x˙ = p
￾
− x + y − f(x)

,
y˙ = x − y + z ,
z˙ = −q y , (1.32)
where f(x) = R g￾
vC1

.
For p = 10.0, q = 14.87, m0 = −0.68, m1 = −1.27, with initial
conditions (−0.1, −0.1, −0.1), use a computer to plot the circuit
orbit portrait in the x–y–z space; or, show the portrait projections
on the three principal planes: (a) the x–y plane, z–x the plane, and
(c) the z–y plane.
1.4 Consider the following nonlinear system:
x˙ = y + κ x ￾
x
2 + y
2

,
y˙ = −x + κ y ￾
x
2 + y
2

.
Show that (0, 0) is the only fixed point, and find under what condi￾tion on the constant κ, this fixed point is a stable or unstable focus.
[Hint: Polar coordinates may be convenient t36 Nonlinear Systems
1.5 For the following two nonlinear systems, determine the types and
the stabilities of their fixed points:
y¨ + y + y
3 = 0 ,
and
x˙ = −x + xy ,
y˙ = y − xy .
1.6 For each of the following systems, find the fixed points and deter￾mine their types and stabilities:
(a)
x˙ = y cos(x),
y˙ = sin(x) ;
(b)
x˙ = (x − y)
￾
x
2 + y
2 − 1

,
y˙ = (x + y)
￾
x
2 + y
2 − 1

;
(c)
x˙ = 1 − x y−1
,
y˙ = −x y−1
￾
1 − x y−1

;
(d)
x˙ = y ,
y˙ = −x −
1
3
x
3 − y .
1.7 Let f and g be two topologically equivalent maps in metric spaces
X and Y , and h be the homeomorphism satisfying g = h
−1 ◦ f ◦ h.
Verify that h
￾
g
k
(x)

= f
k
￾
h(x)

for any integer k ≥ 0.
1.8 Verify that the following two maps are not topologically equivalent
in any neighborhood of the origin: f(x) = x and g(x) = xChapter 2
Stabilities of Nonlinear Systems (I)
Stability theory plays a central role in systems engineering, especially in
the field of control systems and automation, with regard to both dynamics
and control.
Stability of a dynamical system, with or without control and disturbance
inputs, is a fundamental requirement in real-world applications. Roughly
speaking, stability means that the system outputs and its internal sig￾nals are bounded within some allowable limits (the so-called bounded-input
bounded-output stability) or, sometimes more strictly, the system outputs
tend to an equilibrium state of interest (the so-called asymptotic stability).
Conceptually, there are different kinds of stabilities, among which three
basic notions are the main concerns in nonlinear dynamics and control sys￾tems: the stability of a system with respect to its equilibria, the orbital
stability of the system output trajectory, and the structural stability of the
system itself.
The basic concept of stability emerged from the study of an equilib￾rium state of a mechanical system, dated back to as early as 1644 when
E. Torricelli studied the equilibrium of a rigid body under the natural force
of gravity. The classical stability theorem of G. Lagrange, formulated in
1788, is perhaps the most well-known result about stability of conservative
mechanical systems: for a conservative system currently at the position of
an isolated equilibrium and perhaps subject to some simple constraints, if
its potential energy has a minimum then this equilibrium position of the
system is stable.
The evolution of the fundamental concepts of system stability and tra￾jectory stability then went through a long history of development, with
many advances and fruitful results, till the celebrated Ph.D. thesis of
A. M. Lyapunov, “The General Problem of Motion Stability,” completed
3738 Nonlinear Systems
in 1892. This monograph is so fundamental that its ideas and techniques
are virtually leading all basic research and applications on stabilities of
nonlinear dynamical systems today. In fact, not only dynamical behav￾ioral analysis in modern physics but also controllers design in engineering
systems depend upon the principles of Lyapunov’s stability theories.
This chapter is devoted to a brief introduction to the basic stability
theories, criteria, and methodologies of Lyapunov, as well as a few related
important stability concepts, for general nonlinear dynamical systems (see
[Chen (1999)] for a brief summary).
2.1 Lyapunov Stabilities
Briefly, the Lyapunov stability of a system with respect to its equilibrium of
interest involves the behavior of the system outputs toward the equilibrium
state, that are wandering nearby and around the equilibrium (stability in
the sense of Lyapunov) or gradually approaching it (asymptotic stability);
the orbital stability of a system output is the resistance of the trajectory
to small perturbations; the structural stability of a system is the robust￾ness of the system structural properties against small perturbations. These
three basic types of stabilities are introduced in this section, for dynamical
systems without explicitly involving control inputs.
To start with, consider a general nonautonomous system,
x˙ = f(x, t), x(t0) = x0 ∈ R
n
, (2.1)
where, without loss of generality, it is assumed that the origin x
∗ = 0 is a
system fixed point. Lyapunov stability theory concerns various stabilities
of the system trajectories with respect to this fixed point. When another
fixed point is discussed, the new one is first shifted to zero by a change of
variables, and then the transformed system is studied in the same way.
Definition 2.1. System (2.1) is said to be stable in the sense of Lyapunov
about the fixed point x
∗ = 0 if, for any ε > 0 and any initial time t0 ≥ 0,
there exists a constant, δ = δ(ε, t0) > 0, such that
||x(t0)|| < δ =⇒ ||x(t)|| < ε for all t ≥ t0 . (2.2)
This stability is said to be uniform with respect to the initial time, if the
constant δ = δ(ε) is independent of t0 over the entire time interval [0, ∞).
This concept of stability is illustrated in Fig. 2.1.
It should be emphasized that the constant δ generally depends on both
ε and t0. Only for autonomous systems is it always independent of t0Stabilities of Nonlinear Systems (I) 39
t ε
x(t)
x(0)
δ
Fig. 2.1 Geometric meaning of stability in the sense of Lyapunov.
(thus, this stability for autonomous systems is always uniform with respect
to the initial time). It is particularly important to point out that, unlike
autonomous systems, one cannot simply fix the initial time t0 = 0 for a
nonautonomous system in a general discussion of its stability.
Example 2.1. Consider the following linear time-varying system with a
discontinuous coefficient:
x˙(t) = 1
1 − t
x(t), x(t0) = x0 .
It has an explicit solution
x(t) = x0
1 − t0
1 − t
, 0 ≤ t0 ≤ t < ∞,
which is well defined over the entire interval [0, ∞) even when t → 1 (see
Fig. 2.2). Clearly, this solution is stable in the sense of Lyapunov about
the equilibrium x
∗ = 0 over the entire time domain [t0, ∞) if and only if
t0 ≥ 1.
This example shows that the initial time, t0, indeed plays an important
role in the stability of a nonautonomous system.
At this point, it is worth noting that it is often not desirable to shift the
initial time t0 to zero by a change of the time variable, t → t−t0 for control
systems. For instance, consider a system that initially starts operation at
time zero and then is subject to an external control input at time t0 > 0.
For this system, both controlled and uncontrolled behaviors are important
in the analysis of the entire process of the system state evolution. Another
example is the case where a system has multiple singularities or equilibria
of interest, which are not allowed to change or cannot be eliminated by a
shift using a single change of variables.40 Nonlinear Systems
t
1−t0
t0
1
0 1
Fig. 2.2 Solution curve with a singularity at t0 = 1.
It should also be noted that if a system is stable in the sense of Lya￾punov about its fixed point, then starting from any bounded initial state in
the neighborhood of the fixed point, its corresponding solution trajectories
will always be bounded, which follows directly from the definition of the
stability. Note that this is always true for linear systems, including time￾varying systems. To show this, first recall that, for a linear time-varying
system,
x˙(t) = A(t) x(t), x(t0) = x0 ∈ R
n
, (2.3)
the fundamental matrix is defined by
Φ(t, t0) = exp Z t
t0
A(τ ) dτ
, (2.4)
which satisfies Φ(s, s) = I for all t0 ≤ s ≤ t < ∞. In particular, if A(t) = A
is a constant matrix then
Φ(t, t0) = e
(t−t0)A := Φ(t − t0).
Using matrix (2.4), any solution of system (2.3) can be expressed as
x(t) = Φ(t, t0) x0 . (2.5)
Proposition 2.1. System (2.3) is stable in the sense of Lyapunov about its
zero fixed point if and only if all its solution trajectories are bounded.
Proof. If the system is stable about its zero fixed point, then for any ε > 0,
there exists a δ > 0 such that ||xe0|| < δ implies that, starting from this xe0
at time t0, the system solution trajectory x(t;t0, xe0) satisfies



 x
￾
t;t0, xe0
 


 =



Φ(t, t0)xe0



 < ε .
In particular, letting xe0 = [0 · · · 0 δ/2 0 · · · 0]⊤, with δ/2 being the ith
component, yields



Φ(t, t0)xe0



 =



Φi(t, t0)



 δ/2 < ε Stabilities of Nonlinear Systems (I) 41
where Φi
is the ith column of Φ. Therefore, ||Φ(t, t0)|| < 2nεδ−1
, so that



x
￾
t;t0, x0



 =



Φ(t, t0)x0



 < 2nεδ−1
||x0|| ,
which means that the solution trajectory x(t) is bounded if x0 is so.
Conversely, if all solution trajectories of system (2.3) are bounded, then
there is a constant c > 0 such that ||Φ(t, t0)|| < c. Thus, for any given
ε > 0, ||x0|| < δ := ε/c implies that



x(t;t0)



 =



Φ(t, t0)x0



 ≤ c ||x0|| < ε .
Hence, the system is stable in the sense of Lyapunov about its zero fixed
point.
Next, a more important type of stability is introduced.
Definition 2.2. System (2.1) is said to be asymptotically stable about its
fixed point x
∗ = 0, if it is stable in the sense of Lyapunov and furthermore
there exists a constant, δ = δ(t0) > 0, such that
||x(t0)|| < δ =⇒ ||x(t)|| → 0 as t → ∞. (2.6)
The asymptotic stability is said to be uniform, if the constant δ is indepen￾dent of t0 over [0, ∞), and is said to be global if the convergence ||x|| → 0
is independent of the initial state x(t0) over the entire spatial domain on
which the system is defined (e.g. with δ = ∞). If, furthermore,
||x(t0)|| < δ =⇒ ||x(t)|| ≤ c e−σ t , t ≥ t0 , (2.7)
for two constants c, σ > 0, then the system is said to be exponentially
stable about its fixed point x
∗
.
The reason for the first requirement of being stable in the sense of
Lyapunov is to exclude those unstable transient situations like the one
shown in Example 2.1. The asymptotic stability is visualized in Fig. 2.3,
and the exponential stability, in Fig. 2.4.
t x(0) δ
Fig. 2.3 Geometric meaning of the asymptotic stability42 Nonlinear Systems
t
Fig. 2.4 Geometric meaning of the exponential stability.
Clearly, exponential stability implies asymptotic stability, and asymp￾totic stability implies the stability in the sense of Lyapunov, but the reverse
is usually not true.
Example 2.2. For illustration, a system with output x1(t) = x0 sin(t) is
stable in the sense of Lyapunov, but is not asymptotically stable, about 0;
a system with output x2(t) = x0 (1 + t − t0)
−1/2
is asymptotically stable
(so, also is stable in the sense of Lyapunov), but is not exponentially stable
about 0; however, a system with output x3(t) = x0 e
−t
is exponentially
stable (hence, is both asymptotically stable and stable in the sense of Lya￾punov). Finally, the simple example shown in Fig. 2.5 is asymptotically
stable but not uniformly.
t0 t t 1
x0 x1
Fig. 2.5 Asymptotically but not uniformly stable.
2.2 Lyapunov Stability Theorems
Most stability theorems derived in this section apply to the general nonau￾tonomous system (2.1), namely,
x˙ = f(x, t), x(t0) = x0 ∈ R
n
, (2.8)
where f : D × [0, ∞) → Rn is defined and continuously differentiable in
a neighborhood of the origin, D ⊆ Rn, with a given initial state x0 ∈ D.Stabilities of Nonlinear Systems (I) 43
Again, without loss of generality, assume that x
∗ = 0 is the system’s fixed
point of interest.
First, for the general autonomous system
x˙ = f(x), x(t0) = x0 ∈ R
n
, (2.9)
an important special case of (2.8), with a continuously differentiable f :
D → Rn, the following criterion of stability, called the first (or indirect)
method of Lyapunov, is very convenient to use.
Theorem 2.1 (First Method of Lyapunov).
[For continuous-time autonomous systems]
In system (2.9), let J =

∂f/∂x

x=x∗=0 be its Jacobian evaluated at the
zero fixed point. If all the eigenvalues of J have negative real parts, then
the system is asymptotically stable about x
∗ = 0.
Proof. This theorem is a direct consequence of the Grobman–Hartman
Theorem, i.e. Theorem 1.6 introduced in Chapter 1, and the linear systems
stability criteria.
Note that this and the following Lyapunov theorems apply to linear
systems as well, for linear systems are merely special cases of nonlinear
systems. When f(x) = A x, the linear time-invariant system x˙ = A x has
the only fixed point x
∗ = 0, and the system Jacobian is J = A. Thus, if
A has all eigenvalues with negative real parts, the system is asymptotically
stable about its fixed point. Therefore, Theorem 2.1 is consistent with the
familiar linear stability theory.
Note also that the region of asymptotic stability given by Theorem 2.1 is
local, which can be quite large for some nonlinear systems but may be very
small for some others. However, there is no general criterion for determining
the boundary of such a local stability region, which is also called the region
of attraction, when this and the following Lyapunov methods are applied.
Clearly, when it is applied to linear systems, the stability is always global.
Next, before introducing the next theorem of Lyapunov, two examples
are first discussed.
Example 2.3. Consider the damped pendulum (1.1), namely,
x˙ = y
y˙ = −
g
ℓ
sin(x) −
κ
m
y .44 Nonlinear Systems
Its Jacobian is
J =

0 1
−g ℓ−1
cos(x) −κ/m 
(x∗,y∗)=(0,0)
=

0 1
−g ℓ−1 −κ/m 
,
which has eigenvalues
λ1,2 = −
κ
2m
±
1
2
p
(κ
2/m2) − 4g/ℓ ,
both with negative real parts. Hence, the damped pendulum system is
asymptotically stable about its fixed point (x
∗
, y∗
) = (θ
∗
,
˙θ
∗
) = (0, 0).
It is very important to note that Theorem 2.1 cannot be applied to a
general nonautonomous system, because this theorem is neither necessary
nor sufficient, as shown by the following example.
Example 2.4. Consider the following linear time-varying system:
x˙(t) = 
−1 + 1.5 cos2
(t) 1 − 1.5 sin(t) cos(t)
−1 − 1.5 sin(t) cos(t) −1 + 1.5 sin2
(t)

x(t).
This system has eigenvalues λ1,2 = −0.25 ± j 0.25√
7, both with negative
real parts and being independent of the time variable t. If Theorem 2.1
is used to judge this system, the conclusion would be that the system is
asymptotically stable about its fixed point 0.
However, the solution of this system is precisely
x(t) = 
e
0.5t
cos(t) e
−t
sin(t)
−e
0.5t
sin(t) e
−t
cos(t)
  x1(t0)
x2(t0)

,
which is unstable for any initial conditions with a bounded and nonzero
value of x1(t0), no matter how small this initial value is.
This example shows that by using the Lyapunov first method alone to
determine the stability of a general time-varying system, the conclusion can
be wrong.
A rigorous approach for asymptotic stability analysis of general nonau￾tonomous systems is provided by the second method of Lyapunov. The
concept of class-K function will be useful, where K refers to its first use of
the Greek letter κ, which is defined as follows:
K =

g(t) : g(t0) = 0, g(t) > 0 if t > t0,
g(t) is continuous and nondecreasing on [t0, ∞)
	
.Stabilities of Nonlinear Systems (I) 45
Theorem 2.2 (Second Method of Lyapunov).
[For continuous-time nonautonomous systems]
System (2.8) is globally (over the entire domain D), uniformly (with re￾spect to the initial time over the entire time interval [t0, ∞) ), and asymptot￾ically stable about its zero fixed point, if there exist a scalar-valued function,
V (x, t), defined on D×[t0, ∞), and three functions α(·), β(·), γ(·) ∈ K, such
that
(i) V (0, t) = 0 for all t ≥ t0;
(ii) V (x, t) > 0 for all x ̸= 0 in D and all t ≥ t0;
(iii) α (||x||) ≤ V (x, t) ≤ β (||x||) for all t ≥ t0;
(iv) V˙ (x, t) ≤ − γ (||x||) < 0 for all t ≥ t0 , ||x|| ̸= 0.
In Theorem 2.2, the function V is called a Lyapunov function, and the
method of constructing a Lyapunov function for stability determination is
called the second (or direct) method of Lyapunov.
The role of Lyapunov function in the theorem is illustrated in Fig. 2.6,
where for simplicity only the autonomous case is visualized. In the figure,
assume that a Lyapunov function, V (x), has been found, with a bowl-shape
as shown based on conditions (i) and (ii). Then, condition (iv) is
V˙ (x) = 
∂V
∂x

x˙ < 0 , x ̸= 0 , (2.10)
where 
∂V
∂x

is the gradient of V along the trajectory x.
The operation of computing V˙ (x) is usually said to take the derivative
of V (x) along the system trajectory x.
Now, as is well known from Calculus, if the inner product of the above
gradient and the tangent vector x˙ is constantly negative, as guaranteed by
condition (2.10), then the angle between these two vectors is larger than
π/2. This means that the surface of V (x) is monotonically shrinking to
zero (as seen in Fig. 2.6). Consequently, the system trajectory x, with the
projection on the domain as shown in the figure, converges to zero as time
tends to infinity.
Proof. First, the stability in the sense of Lyapunov is established.
This requires to show that, for any ε > 0, there is a δ = δ(ε, t0) > 0,
such that
||x(t0)|| < δ =⇒ ||x(t)|| < ε for all t ≥ t0 .46 Nonlinear Systems
tangent (parallel)
gradient
x

∂V
∂

V ( )
−
tangent
0
x x
x
−
−
x1 x2
x˙
˙
Fig. 2.6 Geometric meaning of the Lyapunov function.
The given conditions (iii) and (iv) together imply that, for x ̸= 0,
0 < α(||x||) ≤ V (x, t) ≤ V
￾
x(t0), t0

for all t ≥ t0 .
Since V (x, t) is continuous and satisfies V (0, t0) = 0, there exists a δ =
δ(ε, t0) > 0 such that
||x(t0)|| < δ =⇒ V
￾
x(t0), t0

< α(ε),
because α(||x||) > 0 for x ̸= 0. Therefore, if ||x(t0)|| < δ, then
α(||x||) ≤ V
￾
x(t0), t0

≤ α(ε) for all t ≥ t0 .
Since α(·) ∈ K, this implies that ||x(t)|| < ε for all t ≥ t0.
Next, the uniform stability property is verified.
The given condition (iii), namely,
0 < α (||x||) ≤ V (x, t) ≤ β (||x||) for all x ̸= 0 and t ≥ t0 ,
implies that, for any ε > 0, there is a δ = δ(ε) > 0, independent of t0, such
that β(δ) < α(ε), as illustrated in Fig. 2.7. Therefore, for any initial state
x(t0) satisfying ||x(t0)|| < δ,
α(||x(t)||) ≤ V (x, t) ≤ V
￾
x(t0), t0

≤ β
￾
||x(t0)||
≤ β(δ) < α(ε)
for all t ≥ t0, which implies that
||x(t)|| < ε for all t ≥ t0 .
Since δ = δ(ε) is independent of t0, this stability is uniform with respect to
the initial time.
Further, the uniform asymptotic stability is veriStabilities of Nonlinear Systems (I) 47
β (δ
δ
)
α(ε
ε
)
 β ||x||
x
x
V(t, )
 || ||
|| || x
α
0
Fig. 2.7 Geometric meaning of the comparison β(δ) < α(ε).
It follows from the uniform stability, just shown above, that for any
ε > 0, there is a δ = δ(ε) > 0, independent of t0, such that
||x(t0)|| < δ =⇒ ||x(t)|| < ε for all t ≥ t0 .
It is required to show that there is a t
∗ > 0, independent of t0, such that
||x(t0)|| < δ =⇒ ||x(t)|| < ε for all t ≥ t
∗
.
To do so, let
t
∗ = t
∗
(ε, δ) = β(δ)/γ(δ).
Then, it can be verified that there is a t1 ∈ (t0, t0 + t
∗
] such that
||x(t0)|| < δ =⇒ ||x(t1)|| < δ .
If not, namely, ||x(t0)|| < δ but ||x(t)|| ≥ δ for all t ∈ (t0, t0 + t
∗
], then
condition (iv) implies that
V˙ (x, t) ≤ − γ(||x||) ≤ − γ(δ) < 0 ,
so that, by condition (iii),
V (x(t), t) = V
￾
x(t0), t0) + Z t
t0
V˙ (x(τ ), τ ) dτ
≤ V
￾
x(t0), t0

−
Z t
t0
γ(δ) dτ
= V
￾
x(t0), t0

− γ(δ)
￾
t − t0

≤ β(δ) − γ(δ)
￾
t − t0

.
Thus, it follows that, for all x(t) satisfying ||x(t)|| ≥ δ,
V (x, t)


t=t0+t
∗ ≤ β(δ) − γ(δ)t
∗ = β(δ) − β(δ) = 0 .
This contradicts the fact that V (x, t) > 0 for all x ̸= 0. Therefore, as
claimed above, there is a t1 ∈ (t0, t0 + t
∗
] such that, ||x(t0)|| < δ implies
||x(t1)|| < δ. Consequently,
||x(t0)|| < δ =⇒ ||x(t)|| < ε for all t > t0 + t
∗ ≥48 Nonlinear Systems
Since t
∗ = t
∗
(ε, δ) is independent of t0, this asymptotic stability is uniform
with respect to the initial time.
Finally, the global uniform asymptotic stability is proved.
Since α(||x||) → ∞ as ||x|| → ∞, one has
α(||x||) ≤ V (x(t), t) ≤ β(||x||) → ∞ as ||x|| → ∞ .
Hence, starting from any initial state x(t0) ∈ Rn, there is always a (large
enough) δ > 0 such that ||x(t0)|| < δ. For this δ, since α(||x||) → ∞ as
||x|| → ∞, there is always a (large enough) r > 0 such that β(δ) < α(r)
(see also Fig. 2.7). Thus,
α(||x(t)||) ≤ V (x(t), t) ≤ V
￾
x(t0), t0

≤ β(||x(t0)||) ≤ β(δ) ≤ α(r),
which implies that
||x(t)|| < r for all t ≥ t0 .
This establishes the global stability in the sense of Lyapunov.
Under all the given conditions, the uniform asymptotic stability can also
be verified in the same manner by repeating the above arguments.
Regarding the uniform negative definiteness condition (iv) of Theo￾rem 2.2, it is very important to note that it cannot be weakened to be
the simpler condition that V˙ (x, t) < 0 for all t ≥ t0. The reason is that this
weaker condition is not sufficient for nonautonomous systems in general, as
can be seen from the following example.
Example 2.5. Consider the simplest 1-dimensional linear system,
x˙ = 0 , t ≥ 0 .
Suppose that one chooses the Lyapunov function
V (x, t) = t + 2
t + 1
x
2
,
which satisfies
V˙ (x, t) = −
x
2
(t + 1)2
< 0 for all t ≥ 0 .
Then, one tends to conclude that the given system is asymptotically stable
about its zero fixed point. However, the solution of this system is x = x0,
the constant initial condition, which is stable in the sense of Lyapunov but
not asymptoticallyStabilities of Nonlinear Systems (I) 49
Now, return to Theorem 2.2. Since autonomous systems are the special
case of nonautonomous systems, this theorem applies to them as well. The
following three examples show how the theorem can be applied to the tech￾nical stability analysis of both autonomous and nonautonomous systems.
Example 2.6. Consider a general linear time-varying system,
x˙(t) = A(t) x(t),
where the system matrix A(t) is assumed to satisfy the following condition:
there are constants a and b and a uniformly positive definite and symmetric
matrix Q(t) ≥ λmin(Q)I > 0 for all t ≥ t0, where λmin(Q) is the minimum
eigenvalue of Q(t), such that the time-varying Lyapunov equation
P˙(t) + P(t)A(t) + A
⊤(t)P(t) + Q(t) = 0
has a positive definite matrix solution P(t) satisfying
0 < aI ≤ P(t) ≤ b I for all t ≥ t0 .
It can be shown that this system is globally, uniformly, and asymptoti￾cally stable about its zero fixed point. Indeed, using the Lyapunov function
V (x, t) = x
⊤(t)P(t) x(t),
and choosing the following three class-K functions:
α(τ ) = a τ 2
, β(τ ) = b τ 2
, γ(τ ) = λmin(Q) τ
2
,
one has
α(||x||) = a ||x||2 ≤ V (x, t) ≤ b ||x||2 = β(||x||),
and
V˙ (x, t) = x
⊤P˙ x + x
⊤P x˙ + x˙
⊤P x
= x
⊤

P˙ + P A + A
⊤P

x
= − x
⊤Q x
≤ − λmin(Q)||x||2
= − γ(||x||).
Moreover,
α(||x||) → ∞ as ||x|| → ∞ .
Therefore, all conditions stated in Theorem 2.2 are satisfied, so the system
is globally, uniformly, and asymptotically stable about its zero fixed point.50 Nonlinear Systems
Since a linear time-invariant system,
x˙ = A x , x0 ∈ R
n
,
is a special case of the above linear time-varying system, this time-invariant
system is asymptotically stable about its zero fixed point if and only if, for
any positive definite matrix Q, the Lyapunov equation
A
⊤P + P A + Q = 0
has a unique positive definite matrix solution P. In fact, this amounts to
noting that for this constant matrix P, one has P˙ = 0 in the Lyapunov
equation of the above example.
Example 2.7. The damped pendulum (1.1), namely,
x˙ = y
y˙ = −
g
ℓ
sin(x) −
κ
m
y
is asymptotically stable about its fixed point (x
∗
, y∗
) = (θ
∗
,
˙θ
∗
) = (0, 0).
This can be verified by using the Lyapunov function
V (x, y) = 1
2

x y 
Q

x
y

+
g
ℓ
￾
1 − cos(x)

,
with
Q := 
κ
2/(2m2
) κ/(2m)
κ/(2m) 1 
.
Indeed, V (x, y) > 0 for all −π/2 < x < π/2 and all y ∈ R. Moreover,
V˙ (x, y) = 
x y 
Q

x˙
y˙

+
g
ℓ
sin(x) ˙x
= −
g κ
2m ℓ x sin(x) −
κ
m
y
2 < 0
for all −π/2 < x < π/2 and y ∈ R. By Theorem 2.2, this damped pendulum
system is asymptotically stable about its fixed point (0, 0).
Note that, although this damped pendulum is an autonomous system,
Theorem 2.2 can still be applied, with
α(·) = 1
2
λmin(Q) (·)
2
,
β(·) = 
1
2
λmax(Q) + 2 g
L

(·)
2
,
γ(x) = min 
g
2mℓ ,
1
m

κ
￾
sin(x) + y
2

,
where κ sin(x) is a monotone function on [0, π/2Stabilities of Nonlinear Systems (I) 51
Before discussing one more example, a simple and useful result is pre￾sented.
Lemma 2.1. For an autonomous system x˙ = f(x), all the eigenvalues of
its Jacobian, J, have negative real parts if and only if the Lyapunov equation
J
⊤P + P J + Q = 0
has a unique positive definite matrix solution P for any positive definite
matrix Q. Moreover, a Lyapunov function for analyzing the system stability
can be chosen as
V (x) = x
⊤P x .
Proof. The first part of the lemma is well known in the linear systems
theory.
If such a positive definite matrix P indeed exists, then V (x) so con￾structed is a Lyapunov function, which satisfies V (0) = 0, V (x) > 0 for all
x ̸= 0, and
V˙ (x) = x
⊤P x˙ + x˙ P x
= x
⊤P f(x) + f(x) P x
= x
⊤P

J x + HOT 
+

J x + HOT ⊤
P x
= x
⊤

P J + JP 
x + 2 x
⊤P

HOT 
= − x
⊤Q x + 2 x
⊤P

HOT 
< 0 for small enough ||x|| ,
where J is the Jacobian and HOT represents the higher-order terms in the
Taylor expansion of f(x) about the zero fixed point.
Example 2.8. Consider a nonautonomous system in the special form of
x˙ = A x + g(x, t),
where A is a stable constant matrix and g is a nonlinear function satisfying
g(0, t) = 0 and ||g(x, t)|| ≤ c ||x|| for a constant c > 0 and for all t ∈ [t0, ∞).
Since A is stable, the following Lyapunov equation
P A + A
⊤P + I = 0
has a unique positive definite matrix solution, P. Then, the Lyapunov
function V (x, t) = x
⊤P x yields
V˙ (x, t) = x
⊤Px˙ + x˙
⊤P x
= x
⊤

P A + A
⊤P

x + 2 x
⊤P g(x, t)
≤ − x
⊤x + 2 λmax(P) c ||x||2
,52 Nonlinear Systems
where λmax(P) is the largest eigenvalue of P. Therefore, if the constant
c < 1/(2λmax(P)) and if the following three class-K functions are used:
α(ζ) = λmin(P) ζ
2
, β(ζ) = λmax(P) ζ
2
,
γ(ζ) = 
1 − 2c λmax(P)

ζ
2
,
then conditions (iii) and (iv) of Theorem 2.2 are satisfied. As a result, the
given system is globally, uniformly, and asymptotically stable about its zero
fixed point.
This example shows that the linear part of a weakly nonlinear nonau￾tonomous system can dominate the system stability.
Example 2.8 motivates the following investigation. Consider a nominal
nonautonomous system,
x˙ = f(x, t), x(t0) = x0 ∈ R
n
,
where f is continuously differentiable, with a zero fixed point f(0, t) = 0 for
all t ≥ t0. Suppose that a small perturbation, e(x, t), is input, yielding a
perturbed system of the form
x˙ = f(x, t) + e(x, t).
The question is: if the nominal system was asymptotically stable about
its zero fixed point, then under a small perturbation does the perturbed
system remain to be stable, at least in the sense of Lyapunov, about the
zero fixed point? If so, the nominal system is said to be totally stable
about its zero fixed point. This issue will be further addressed in the
next chapter, for which the Malkin Theorem will provide an answer: if
the nominal system is uniformly asymptotically stable about its zero fixed
point, and the perturbation is small enough, then the perturbed system is
totally stable.
At this point, it should be noted that in Theorem 2.2, the uniform sta￾bility is guaranteed by the class-K functions α, β, and γ stated in conditions
(iii) and (iv), which is necessary since the solution of a nonautonomous sys￾tem may sensitively depend on the initial time, as seen from Example 2.1.
For autonomous systems, however, these class-K functions (hence, condi￾tion (iii)) are not needed. In this case, Theorem 2.2 reduces to the following
simple form.
Theorem 2.3 (Second Method of Lyapunov).
[For continuous-time autonomous systems]
The autonomous system (2.9) is globally (over the entire domain D) and
asymptotically stable about its zero fixed point, if there exists a scalar-valued
function, V (x), defined on D, such thatStabilities of Nonlinear Systems (I) 53
Table 2.1 Lyapunov functions for autonomous and nonautonomous systems.
Autonomous Systems Nonautonomous Systems
V (0) = 0 V (0, t) = 0 for all t ≥ t0
V (x) > 0 V (x, t) > 0 for all x ̸= 0 and all t ≥ t0
0 < α(||x||) ≤ V (x, t) ≤ β(||x||) for some α, β ∈ K and all t ≥ t0
V˙ (x) < 0 V˙ (x, t) ≤ −γ(||x||) < 0 for some γ ∈ K and all t ≥ t0
(i) V (0) = 0;
(ii) V (x) > 0 for all x ̸= 0 in D;
(iii) (not needed)
(iv) V˙ (x) < 0 for all x ̸= 0 in D.
Note that if condition (iv) in Theorem 2.3 is replaced by
(iv’) V˙ (x) ≤ 0 for all x ∈ D ,
then the resulting stability is only in the sense of Lyapunov but may not
be asymptotic.
Note also that if D = Rn, namely if V (x) → ∞ as ||x|| → ∞, then the
stability is global over Rn.
A comparison of the stability conditions for autonomous and nonau￾tonomous systems stated in Theorems 2.2 and 2.3 is given in Table 2.1.
Example 2.9. Consider a linear time-invariant system,
x˙ = A x ,
with a negative definite constant matrix A.
If Theorem 2.1 is applied, one can use the Lyapunov function V (x) =
1
2
x
⊤x, which satisfies V (0) = 0, V (x) > 0 for all x ̸= 0, and
V˙ (x) = x x˙ = x
⊤A x < 0 , for all x ̸= 0 .
Therefore, the system is asymptotically stable about its zero fixed point,
consistent with the well-known linear systems theory.
Example 2.10. Consider the undamped pendulum (1.2), namely,
x˙ = y
y˙ = −
g
ℓ
sin(x),54 Nonlinear Systems
where x = θ is the angular variable defined on −π < θ < π, with the
vertical axis as its reference, and g is the gravity constant.
It is easy to verify that the system Jacobian at the zero equilibrium has
a pair of purely imaginary eigenvalues, λ1,2 = ±j
p
g/ℓ, where j =
√
−1
so that Theorem 2.1 is not applicable. However, if one uses the Lyapunov
function
V =
g
ℓ
￾
1 − cos(x)

+
1
2
y
2
,
then
V˙ (x) = g
ℓ
x˙ sin(x) + y y˙
=
g
ℓ
y sin(x) −
g
ℓ
y sin(x) = 0
for all (x, y) over the entire domain.
Thus, the conclusion is that this undamped pendulum is stable in the
sense of Lyapunov but not asymptotically, consistent with the physics of
the undamped pendulum.
Theorem 2.4 (Krasovskii Theorem).
[For continuous-time autonomous systems]
For the autonomous system (2.9), let J(x) = 
∂f/∂x

be its Jacobian,
which generally is a function of x(t). A sufficient condition for the system
to be asymptotically stable about its zero fixed point is that there exist two
real positive definite constant matrices, P and Q, such that the matrix
J
⊤(x) P + P J(x) + Q
is semi-negative definite for all x ̸= 0 in a neighborhood D of the origin,
in which 0 is the only fixed point of the system. In this case, a Lyapunov
function can be chosen as
V (x) = f
⊤(x) P f(x).
Furthermore, if D = Rn and V (x) → ∞ as ||x|| → ∞, then this asymptotic
stability is global.
Proof. First, since x
∗ = 0 is a fixed point, f(0) = 0, so
V (0) = f
⊤(0) P f(0) = 0 .
Then, for any x ̸= 0 in the neighborhood D, f(x) ̸= 0, since 0 is the only
fixed point of f in D. Therefore, V (x) = f
⊤(x)P f(x) > 0 since P is positivStabilities of Nonlinear Systems (I) 55
definite. Also,
V˙ (x) = 
∂V
∂x

x˙
=
"
f
⊤(x)

∂V
∂x
⊤
P + f
⊤(x) P

∂V
∂x

#
f(x)
= f
⊤(x)

J
⊤(x) P + P J(x)

f(x)
= f
⊤(x)

J
⊤(x)P + P J(x) + Q

f(x) − f
⊤(x) Q f(x)
< 0 for all x ̸= 0 .
Finally, the global stability follows from Theorem 2.3.
Note that the above matrix Q is used to guarantee the matrix [J
⊤(x)P +
P J(x)] to always be negative definite. The reason is that, for some x in
the neighborhood of the zero fixed point, the Jacobian J(x) may be zero,
so that J
⊤P +P J = 0 which, in turn, implies that V˙ (x) = 0 (see the proof
above). But, if Q > 0, then it is guaranteed that V˙ (x) < 0 for all x ̸= 0.
Usually, a convenient choice is Q = I, the identity matrix.
Similar stability criteria can be established for discrete-time systems.
A discrete-time system is called autonomous if its system function f does
not explicitly contain an independent discrete-time variable k; otherwise,
it will be called nonautonomous and denoted as fk, k = 1, 2, ... .
Two main results on discrete-time system stabilities are summarized as
follows.
Theorem 2.5 (First Method of Lyapunov).
[For discrete-time autonomous systems]
Let x
∗ = 0 be a fixed point of the discrete-time autonomous system
xk+1 = f(xk), (2.11)
where f : D → Rn is continuously differentiable in a neighborhood of the
origin, D ⊆ Rn, and let J =

∂f/∂xk

xk=x∗=0 be its Jacobian evaluated
at this fixed point. If all the eigenvalues of J are strictly less than one in
absolute value, then the system is asymptotically stable about its zero fixed
point.
Theorem 2.6 (Second Method of Lyapunov).
[For discrete-time nonautonomous systems]
Let x
∗ = 0 be a fixed point of the nonautonomous system
xk+1 = fk(xk), (2.12)56 Nonlinear Systems
where fk : D → Rn is continuously differentiable in a neighborhood of the
origin, D ⊆ Rn. Then the system (2.12) is globally (over the entire domain
D) and asymptotically stable about its zero fixed point, if there exists a
scalar-valued function, V (xk, k), defined on D and continuous in xk, such
that
(i) V (0, k) = 0 for all k ≥ k0;
(ii) V (xk, k) > 0 for all xk ̸= 0 in D and for all k ≥ k0;
(iii) ∆V (xk, k) := V
￾
xk, k
− V
￾
xk−1, k − 1

< 0 for all xk ̸= 0 in D and
for all k ≥ k0 + 1;
(iv) 0 < W￾
||xk||
< V ￾
xk, k
for all k ≥ k0 + 1, where W(·) is a pos￾itive continuous function defined on D, satisfying W(||0||) = 0 and
limτ→∞ W(τ ) = ∞ monotonically.
As a special case, for discrete-time autonomous systems, Theorem 2.6
reduces to the following simple form.
Theorem 2.7 (Second Method of Lyapunov).
[For discrete-time autonomous systems]
Let x
∗ = 0 be a fixed point for the autonomous system (2.11). Then,
the system is globally (over the entire domain D) and asymptotically stable
about this zero fixed point if there exists a scalar-valued function, V (xk),
defined on D and continuous in xk, such that
(i) V (0) = 0;
(ii) V (xk) > 0 for all xk ̸= 0 in D;
(iii) ∆V (xk) := V
￾
xk

− V
￾
xk−1

< 0 for all xk ̸= 0 in D;
(iv) V
￾
x

→ ∞ as ||x|| → ∞.
To this end, it is important to emphasize that all the Lyapunov theorems
stated above only offer sufficient conditions for asymptotic stability. On
one hand, if no suitable Lyapunov function can be constructed to verify
the conditions, one cannot say anything about the stability of the given
system. On the other hand, usually more than one Lyapunov function may
be constructed for the same system. Thus, for a given system, one choice
of a Lyapunov function may yield a less conservative result (e.g. with a
larger stability region) than other choices. Here, stability region refers to
the set of system initial states starting from which the system trajectory
will converge to the zero fixed point.
Nevertheless, there is a necessary condition in theory about the existence
of a Lyapunov fuStabilities of Nonlinear Systems (I) 57
Theorem 2.8 (Massera Inverse Theorem). Suppose that the au￾tonomous system (2.9) is asymptotically stable about its fixed point x
∗ and
f is continuously differentiable with respect to x for all t ∈ [t0, ∞). Then,
a Lyapunov function exists for this system.
A combination of this theorem and Theorem 2.2 provides a necessary
and sufficient condition for the asymptotic stability of a general autonomous
system. For a general nonautonomous system, however, a necessary and
sufficient condition of this type is possible but would be much more com￾plicated.
Proof. See [Massera (1956)].
2.3 LaSalle Invariance Principle
First, the concepts of limit sets and invariant sets are introduced.
Definition 2.3. For a given dynamical system, a point z in the state space
is said to be an ω-limit point of a trajectory x(t) of the system if for every
open neighborhood Uz of z, the trajectory of x(t) enters Uz at a finite time
t. The set of all ω-limit points of x(t) is called the ω-limit set of x(t), which
can be formulated as
Ω
+(x) = n
z ∈ R
n

 there exists {tk} with tk → ∞
such that x(tk) → z as k → ∞o
.
In the definition, ω is the last letter in the Greek alphabet, which is used
to indicate the limiting process t → ∞. Similarly, the α-limit point and the
α-limit set are defined for the opposite limiting process as t → −∞, where
α is the first letter of the Greek alphabet. Equivalently,
Ω
−(x) = n
z ∈ R
n

 there exists {tk} with tk → −∞
such that x(tk) → z as k → ∞o
.
Simple examples of ω-limit points and ω-limit sets are fixed points and
periodic trajectories (e.g. limit cycles), respectively, which however need
not be stable.
Definition 2.4. An ω-limit set, Ω+(x), is attracting if there exists an open
neighborhood UΩ+ of Ω+(x), such that when the trajectory of a system
state enters UΩ+ at some instant t1 ≥ t0, then this trajectory will approach58 Nonlinear Systems
Ω
+(x) arbitrarily closely as t → ∞. The basin of attraction of an attracting
point is the union of all such open neighborhoods. An ω-limit set is repelling
if the system trajectory always moves away from it.
Simple examples of attracting sets include stable fixed points and stable
limit cycles, and repelling sets include those unstable ones.
Definition 2.5. A set S ⊂ Rn is said to be forward (backward) invariant
under function (or map) f if, for all x(t) ∈ S, f(x(t)) ⊆ S for all t ≥ t0
(t ≤ t0).
As a note at this point, which will be further discussed in detail later, an
attractor is defined to be the union of all those points in an attracting set
that are invariant under the system function. In other words, an attractor
is an ω-limit set, Ω+(x), satisfying the property that all system trajectories
near Ω+(x) have Ω+(x) as their ω-limit sets.
As another note, for a given map M in the discrete-time setting, an
ω-limit set is similarly defined. Moreover, an ω-limit set is invariant under
the map if M(Ω+(xk)) = Ω+(xk) for all k; namely, starting from any point
x0 in the set, the trajectory xk = Mk
(x0) will eventually return to the
same set (but need not be the same point). Thus, an (invariant) ω-limit
set embraces fixed points and periodic trajectories.
Theorem 2.9. The ω-limit set of a finite state x of an autonomous system,
x˙ = f(x), is always invariant under f.
Proof. It amounts to showing that for any z ∈ Ω
+(x), the system trajec￾tory φt(z) ∈ Ω
+(x).
By definition of Ω+(x), there exists a sequence {tk} with tk → ∞ as
k → ∞, such that φtk
(x) → z as k → ∞. Fix t and choose tk sufficiently
large so that t + tk > 0. Then,
φt+tk
(x) = φt(φtk
(x)) → φt(z) as k → ∞,
due to the time-invariant property of solutions of an autonomous system.
Therefore, φt(z) ∈ Ω
+(x), implying that Ω+(x) is invariant under the
map f.
Now, consider once again the autonomous system (2.14) with a fixed
point x
∗ = 0.
Theorem 2.10 (LaSalle Invariance Principle). Let V (x) be a Lya￾punov function of system (2.14), defined in a neighborhood D of the fixedStabilities of Nonlinear Systems (I) 59
point x
∗ = 0. Also, let φt(x0) be a bounded solution trajectory of the sys￾tem, with its initial state x0 and all its ω-limit points being confined within
D. Moreover, let
E =

x ∈ D

 V˙ (x) = 0 	
(2.13)
and S ⊂ E be the largest invariant subset of E in the sense that if the
initial state x0 ∈ S then the entire trajectory φt(x0) ⊂ S for all t ≥ t0.
Then, for any initial state x0 ∈ D, the solution trajectory of the system
satisfies
φt(x0) → S as t → ∞.
Proof. Since V (x) is a Lyapunov function, it is non-increasing in t and is
bounded from below by zero. Hence,
V (φt(x0)) → c (t → ∞)
for a constant c depending only on x0.
Let Ω(x0) be the ω-limit set of x0. Then, Ω(x0) is an invariant set
(Theorem 2.9) and belongs to D since the latter contains all limit points of
φt(x0).
Take a limit point of φt(x0), z ∈ Ω(x0). Then, there is an increasing
subsequence {tk}, with tk → ∞ as k → ∞, such that φtk
(x0) → z as
k → ∞. Therefore, by the continuity of V , V (z) = c. Since z is arbitrarily
chosen, this holds for all z ∈ Ω(x0).
Note that, since Ω(x0) is invariant, if z ∈ Ω(x0) then φt(z) ∈ Ω(x0) for
all t ≥ t0. Hence, V (z) = c for all t ≥ t0 and for all z ∈ Ω(x0). This means
V˙ (z) = 0 for all z ∈ Ω(x0).
Consequently, since S is the largest invariant subset of E, Ω(x0) ⊆ S ⊆ E.
But φt(x0) → Ω(x0), so φt(x0) → S, as t → ∞.
This invariance principle is consistent with the Lyapunov theorems when
they are applicable to a problem. Sometimes, when V˙ = 0 over a subset
of the domain of V , a Lyapunov theorem is not easy to directly apply, but
the LaSalle invariance principle may be convenient to use.
Example 2.11. Consider the system
x˙ = − x +
1
3
x
3 + y ,
y˙ = − x .60 Nonlinear Systems
The Lyapunov function V = x
2 + y
2 yields
V˙ =
1
2
x
2

1
3
x
2 − 1

,
which is negative for x
2 < 3 but is zero for x = 0 and x
2 = 3, regardless of
the variable y. Thus, Lyapunov theorems do not seem to be applicable, at
least not directly.
However, observe that the set E defined by (2.13) for this system has
only three straight lines: x = −
√
3, x = 0, and x =
√
3. Note also that all
trajectories intersecting the line x = 0 satisfy ˙y = 0 (the second equation)
and so will not remain on the line x = 0 unless y = 0. This means that
the largest invariant subset S containing the points with x = 0 is the only
point (0, 0).
Thus, it follows from Theorem 2.10 that, starting from any initial state
located in a neighborhood of the point (0, 0) bounded within the two stripes
x = ±
√
3, say located inside the disk
D =

(x, y)| x
2 + y
2 < 3
	
,
the solution trajectory will always be attracted to the point (0, 0). This
means that the system is (locally) asymptotically stable about its zero fixed
point.
2.4 Some Instability Theorems
Once again, consider the general autonomous system (2.9), namely,
x˙ = f(x), x(t0) = x0 ∈ R
n
, (2.14)
with a fixed point x
∗ = 0. Sometimes, disproving the stability is easier
than trying to find a Lyapunov function to prove it, since in this case a
Lyapunov function can never be found. To disprove the stability, when the
system indeed is unstable, the following instability theorems are useful.
Theorem 2.11 (Linear Instability Theorem). In system (2.14), let
J =

∂f/∂x

x=x∗=0 be its Jacobian evaluated at x
∗ = 0. If at least one
of the eigenvalues of J has a positive real part, then the system is unstable
about the fixed point x
∗ = 0.
For discrete-time systems, there is a similar result: The discrete-time
autonomous system
xk+1 = f(xk), k = 0, 1, 2, . . . ,Stabilities of Nonlinear Systems (I) 61
is unstable about its fixed point x
∗ = 0 if at least one of the eigenvalues of
the system Jacobian is larger than 1 in absolute value.
The following two negative theorems can be easily extended to nonau￾tonomous systems in an obvious way.
Theorem 2.12 (General Instability Theorem). For system (2.14),
let V (x) be a positive differential function defined on a neighborhood D of
the origin, such that V (0) = 0 and in any arbitrarily small neighborhood of
0, there always exists an x0 which satisfies V (x0) > 0. If there is a closed
subset Ω ⊆ D ∩ B containing x0, where B is the unit ball of Rn, on which
V (x) > 0 and V˙ (x) > 0, then the system is unstable about its zero fixed
point.
Proof. First, observe that since V is differentiable, it is continuous and so
is bounded over any closed bounded set.
It can be shown that the trajectory x(t) of the system, starting from
x(0) = x0, must leave the subset Ω as time t evolves. To show this, note
that as long as this trajectory x(t) is inside the subset Ω, one has V (x) > 0.
This is because V (x) starts with the value c := V (x0) > 0, and V˙ (x) > 0
by assumption. Let
α = inf 
V˙ (x)| x ∈ Ω and V (x) ≥ c > 0
	
.
Then, since Ω is a closed bounded set, α > 0, so that
V (x(t)) = V (x0) + Z t
0
V˙ (x(τ )) dτ ≥ c +
Z t
0
α dt = c + α t .
This inequality shows that x(t) cannot stay in Ω forever, because V (x) is
bounded on the closed subset Ω but now V (x) ≥ c + αt → ∞ as t → ∞.
Thus, x(t) must leave the set Ω.
Next, note that x(t) cannot leave Ω through the surface V (x) = 0.
This is because V (x(t)) ≥ c > 0, which implies that x(t) has to leave Ω
through a path with the magnitude satisfying ||x|| ≥ ||x0|| (since it starts
with x(0) = x0). According to the assumption, this x0 can be arbitrarily
close to the origin, which implies that the origin is unstable: any trajectory
starting from a point in an arbitrarily small neighborhood of the origin will
eventually move away from it.
Example 2.12. Consider the system
x˙ = y + x
￾
x
2 + y
4

,
y˙ = − x + y
￾
x
2 + y
4
62 Nonlinear Systems
which has the fixed point (0, 0). The system Jacobian at the fixed point has
a pair of imaginary eigenvalues, λ1,2 = ±j, where j =
√
−1, so Theorem 2.1
is not applicable.
Note that the Lyapunov function V =
1
2
￾
x
2 + y
2

leads to
V˙ =
￾
x
2 + y
2
 ￾x
2 + y
4

> 0 , for all (x, y) ̸= (0, 0).
Hence, this system is unstable about its zero fixed point by Theorem 2.12.
A variant of Theorem 2.12 is the following.
Theorem 2.13. For system (2.14), let V (x) be a positive differential func￾tion defined on a neighborhood D of the origin, such that V (0) = 0 and in
any arbitrarily small neighborhood of 0, there always exists an x0 satisfying
V (x0) > 0. If there is a closed subset Ω ⊆ D ∩ B containing x0, where B is
the unit ball of Rn, on which V˙ (x) > aV (x) for some constant a > 0 and
for all x ∈ ω, then the system is unstable about its zero fixed point.
Example 2.13. Consider the autonomous system
x˙ = x + 2y + xy2
,
y˙ = 2x + y − x
2
y .
Let V (x, y) = x
2 − y
2
. Then, V (x, y) > 0 in the subset Ω = 
(x, y)


|x| >
|y|
	
, as shown in Fig. 2.8. Moreover,
V˙ (x, y) = 2x
2 − 2y
2 + 4x
2
y
2 = 2 V (x, y) + 4x
2
y
2 ≥ 2 V (x, y),
for all (x, y) ∈ Ω. Therefore, the system is unstable about its zero fixed
point.
x
y
Fig. 2.8 The subset in which |x| > |y|.
Theorem 2.14 (Chetaev Instability Theorem). For system (2.14),
let V (x) be a positive and continuously differentiable function defined on
D, and let Ω be a subset of D, containing the origin (i.e. 0 ∈ D ⊆ Ω).Stabilities of Nonlinear Systems (I) 63
(i) V (x) > 0 and V˙ (x) > 0 for all x ̸= 0 in D,
(ii) V (x) = 0 for all x on the boundary of Ω,
then the system is unstable about its zero fixed point.
Proof. This instability theorem can be established by an argument similar
to that given in the proof of Theorem 2.12. It is illustrated in Fig. 2.9, which
graphically shows that if the theorem conditions are satisfied, then there
is a gap within any neighborhood of the origin, so that it is possible for a
system state trajectory to escape from the neighborhood of the origin along
a path within this gap.
0
D
V = 0
V = 0
Ω
Fig. 2.9 Illustration of the Chetaev theorem.
Example 2.14. Consider the system
x˙ = x
2 + 2 y
5
,
y˙ = x y2
.
Choose the Lyapunov function
V = x
2 − y
4
,
which is positive inside the region defined by
x = y
2
and x = − y
2
.
Let D be the right-half plane and Ω be the shaded area D0 shown in
Fig. 2.10. Clearly, V = 0 on the boundary of Ω, and V > 0 with
V˙ = 2x
3 > 0 for all (x, y) ∈ D. According to Theorem 2.14, this sys￾tem is unstable about its zero fixed point.64 Nonlinear Systems
D
D
V = 0
V = 0
D0
x
y
Fig. 2.10 The defining region of a Lyapunov function.
2.5 Construction of Lyapunov Functions
As seen from the previous subsections, Lyapunov functions play the central
role in the Lyapunov stability theory, and how to find a simple working
Lyapunov function is key to the determination of the stability or instability
of a system with respect to its fixed point of interest.
There are many successful techniques for constructing a simple effective
Lyapunov function. One natural choice is to use the total energy (kinetic
energy and potential energy) of the system as the Lyapunov function, as
illustrated by the following two examples.
Example 2.15. Consider the system
x˙ = y ,
y˙ = −f(x).
Let
V (x, y) = K + P =
1
2
y
2 +
Z x
0
f(z)dz ,
where K is the kinetic energy and P is the potential energy of the system.
It can be easily verified that
V˙ = yy˙ + f(x) ˙x = −yf(x) + yf(x) = 0 .
Hence, the system is stable in the sense of Lyapunov about its zero fixed
point.
Example 2.16. Consider the dimensionless undamped pendulum (1.2) of
unit mass, namely
x˙ = y ,
y˙ = − sin(x),
where x = θ is the angular variable defined on −π < x < π.Stabilities of Nonlinear Systems (I) 65
It satisfies sin(0) = 0 and x sin(x) > 0 for −
π
2 < x < π
2
. Let
V (x, y) = K + P =
1
2
y
2 +
Z x
0
sin(z)dz
=
1
2
y
2 + (1 − cos(x)).
Since
V˙ = yy˙ + sin(x) ˙x = −y sin(x) + sin(x)y = 0 ,
the pendulum is stable in the sense of Lyapunov about its zero fixed point
for −
π
2 < x = θ < π
2
.
Another relatively simple and direct approach to constructing a Lya￾punov function for an autonomous system is to apply Theorem 2.4, in
which a Lyapunov function can be chosen as
V (x) = f
⊤(x)P f(x),
where P and Q are two real positive definite constant matrices such that
the matrix
J
⊤(x)P + P J(x) + Q
is negative semi-definite for all x ̸= 0 in a neighborhood D of the zero
fixed point. If these two matrices P and Q exist, then the given system is
asymptotically stable about the zero fixed point in D, and if furthermore
D = Rn and V (x) → ∞ as ||x|| → ∞, then this stability is global.
A limitation of this approach is that two constant matrices, P and Q,
have to exist such that the matrix-valued function J
⊤(x)P + P J(x) + Q is
negative semi-definite for all x ̸= 0 in D. This existence is usually difficult
or even impossible to prove. Therefore, more flexible methods are needed
to construct a Lyapunov function in a general case.
To introduce one general method, let V (x) be a Lyapunov function to
be constructed, and let its gradient be defined by
g
⊤(x) = 
g1(x) · · · gn(x)

=

∂V (x)
∂x1
· · ·
∂V (x)
∂xn

.
Then,
V˙ (x) = Xn
i=1
∂V (x)
∂xi
x˙ i
=

∂V (x)
∂x1
· · ·
∂V (x)
∂xn




f1(x)
.
.
.
fn(x)



= g
⊤(x)f(x).66 Nonlinear Systems
Lemma 2.2. The above g(x) is the gradient of a scalar-valued function if
and only if
∂gi
∂xj
=
∂gj
∂xi
, for all i, j = 1, . . . , n .
Proof. If g(x) is the gradient of a scalar-valued function, V (x), then
g
⊤(x) = 
g1(x) · · · gn(x)

=

∂V (x)
∂x1
· · ·
∂V (x)
∂xn

.
It follows that
∂gi
∂xj
=
∂
∂xj
∂V
∂xi
=
∂
2V
∂xj∂xi
=
∂
2V
∂xi∂xj
=
∂
∂xi
∂V
∂xj
=
∂gj
∂xi
,
for all i, j = 1, . . . , n.
Conversely, if ∂gi/∂xj = ∂gj/∂xi for all i, j = 1, . . . , n, then a double
integration gives
Z Z ∂gi
∂xj
dxidxj =
Z Z ∂gj
∂xi
dxidxj ,
or
Z
gi dxi =
Z
gj dxj := V (x),
which holds for all i and j, hence is independent of i and j, i, j = 1, . . . , n.
Thus,
∂V
∂xk
=
∂
∂xk
Z
gk dxk = gk , k = 1, . . . , n ,
implying that g(x) is the gradient of the scalar-valued function V (x).
To construct the function V (x), note that V (x) can be calculated by
the integration of its gradient:
V (x) = Z x
0
Xn
i=1
∂V (x)
∂xi
dxi =
Z x
0
Xn
i=1
gi(x) dxi
.
Then, Lemma 2.2 shows that ∂gi/∂xj = ∂gj/∂xi
, for all i, j = 1, . . . , n.
This means that the above integral is taken over a conservative field, so
that the integral can be taken over any path joining 0 to the terminal state
x. In particular, this can be carried out along all the principal axes, namely,
V (x) = Z x1
0
g1(ξ1, 0, . . . , 0) dξ1 +
Z x2
0
g2(x1, ξ2, 0, . . . , 0) dξ2
+ · · · +
Z xn
0
gn(x1, . . . , xn−1, ξn) dξn .Stabilities of Nonlinear Systems (I) 67
This formula can be used to construct a Lyapunov function, V (x), for
a given autonomous system, x˙ = f(x), in a straightforward manner, as
illustrated by the following example.
This approach to constructing Lyapunov functions is called the varia￾tional gradient method.
Example 2.17. Consider the damped pendulum system (1.1), namely,
x˙ = y
y˙ = −
g
ℓ
sin(x) −
κ
m
y ,
where x = θ is the angular variable defined on −π < x < π.
One may start with a simple assumed form,
g(x, y) = 
g1(x, y) g2(x, y)

=

α(x, y) x + β(x, y) y γ(x, y) x + δ(x, y) y

,
where α, β, γ, δ are functions of x and y, and are to be determined so as
to satisfy the condition ∂g1(x, y)/∂y = ∂g2(x, y)/∂x; that is,
β(x, y) + ∂β(x, y)
∂y y +
∂α(x, y)
∂y x = γ(x, y) + ∂γ(x, y)
∂x x +
∂δ(x, y)
∂x y . (a)
On the other hand,
V˙ (x, y) = ∂V (x, y)
∂x x˙ +
∂V (x, y)
∂y y˙
=

g1(x, y) g2(x, y)


f1(x, y)
f2(x, y)

= α(x, y) xy + β(x, y) y
2 −
κ
m
γ(x, y) xy
−
g
ℓ
γ(x, y) x sin(x) −
κ
m
δ(x, y) y
2 −
g
ℓ
δ(x, y) y sin(x).
Then, some appropriate functions α, β, γ, δ should be chosen to simplify
V˙ (x, y), so that V (x, y) can be easily found. For this purpose, one may
proceed as follows:
(i) Cancel the crossed-product terms, by letting
α(x, y) x −
κ
m
γ(x, y) x −
g
m
δ(x, y) sin(x) = 0 , (b)
so that
V˙ (x, y) = 
β(x, y) − κ m−1
δ(x, y)

y
2 − g ℓ−1
γ(x, y) x sin(x). (c)68 Nonlinear Systems
(ii) By inspection, use α = α(x) and β = γ = δ = constant in equations
(b) and (c), so that
g1(x, y) = α(x) x + β y , g2(x, y) = γ x + δ y , and β = γ ,
where the last equality follows from equation (a).
Thus, one has
V (x, y) = Z x
0
g1(ξ1, 0) dξ1 +
Z y
0
g2(x, ξ2) dξ2
=
Z x
0
α(ξ1) ξ1 dξ1 +
Z y
0
￾
γ x + δ ξ2

dξ2
=
Z x
0
 κ
m
ξ1 +
g
ℓ
δ sin(ξ1)

dξ1 +
Z y
0
￾
γ x + δ ξ2

dξ2
=
κ
2m
γ x2 +
g
ℓ
δ −
g
ℓ
δ cos(x) + γ xy +
δ
2
y
2
=
1
2
[ x y ]

(κ γ)/m γ
γ δ   x
y

+
g
ℓ
δ [1 − cos(x)] .
Now, to satisfy the requirements for a Lyapunov function, namely,
V (0, 0) = 0 and V (x, y) > 0 with V˙ (x, y) < 0 for all x ̸= 0 and y ̸= 0, one
may simply choose the constants γ and δ such that 0 < γ < κ m−1
δ < ∞.
Then, it can be verified that this condition is sufficient to guarantee that
V (x, y) so constructed is a Lyapunov function. Consequently, the damped
pendulum system is asymptotically stable about its zero fixed point in the
domain of −π < x = θ < π, consistent with the physics of the pendulum.
2.6 Stability Regions: Basins of Attraction
In the damped pendulum system (1.1), its fixed point (θ
∗
,
˙θ
∗
) = (0, 0) is
asymptotically stable, and any initial state satisfying the angular condition
−π < θ0 < π will eventually move to this fixed point at rest. In other
words, the stability region, or basin of attraction, of the zero fixed point
for this pendulum system is (−π, π).
In general, for a nonautonomous system,
x˙ = f(x, t), x(t0) = x0 ∈ R
n
, (2.15)
with a fixed point x
∗ = 0, let φt(x0) be its solution trajectory starting from
x0. Suppose that this system is asymptotically stable about its zero fixed
point. The interest here is to find out how large the region for the initial
state x0 can be, within which one can guarantee that φt(x0) → 0 as t → Stabilities of Nonlinear Systems (I) 69
Definition 2.6. For system (2.15), the region of stability, or the basin of
attraction of its stable zero fixed point, is the set
B =
n
x0 ∈ R
n


limt→∞
φt(x0) = 0 o
.
For a stable linear system, x˙ = A x, it is clear that B = Rn, the entire
state space. For a nonlinear nonautonomous system, however, finding its
exact basin of attraction is very difficult or even impossible in general.
Therefore, one turns to find a (relatively conservative) estimate of the basin,
to have a sense of “at least how large it would be”. For this purpose, if one
can find a Lyapunov function, V (x), defined on a neighborhood of the zero
fixed point, D ⊆ Rn, and find a region
Ωc =

x ∈ R
n
| V (x) ≤ c
	
where c ≥ 0 is a constant, then all system trajectories starting from inside
Ωc will tend to zero as time evolves. In other words, the basin of attraction
is at least as large as Ωc.
Here, it is important to note that one may not simply use the defining
domain D of the Lyapunov function as an estimate of the basin of attraction,
as illustrated by the following example.
Example 2.18. Consider the autonomous system
x˙ = y ,
y˙ = − x +
1
3
x
3 − y .
The Lyapunov function
V (x, y) = 3
4
x
2 −
1
12
x
4 +
1
2
xy +
1
2
y
2
satisfies
V˙ (x, y) = −
1
2
x
2

1 −
1
3
x
2

−
1
2
y
2
,
so V (x, y) is well defined on
D =
n
(x, y) ∈ R
2
: −
√
3 < x < √
3
o
.
It is clear that V (0, 0) = 0 and V (x, y) > 0 with V˙ (x, y) < 0 for all nonzero
(x, y) ∈ D. However, it can also be seen, from a computer plot shown in
Fig. 2.11, that D is not a subset of B, although they have a large overlapping
region. In this example, D cannot be used as an estimate of B.70 Nonlinear Systems
x
y
4
2
−4
−2
−4
0
−2 0 2 4
Fig. 2.11 Computer plot of the region of attraction.
Here, what is actually needed is to find a largest possible bounded sub￾set, Ωc, within the defining domain D, for the given system. This may be
accomplished by following a general procedure, as illustrated by working
through this Example 2.18 as follows:
(1) Compute the system Jacobian evaluated at the zero fixed point:
J =

∂f(x)
∂x

x=x∗=0
=

0 1
−1 −1

.
Make sure all its eigenvalues have negative real parts. Here, λ1,2 =
−0.5 ± 1.5 j , where j =
√
−1 .
(2) Choose a constant matrix Q > 0 (e.g. Q = I) and solve the Lyapunov
equation
J
⊤P + P J + Q = 0
for the positive definite matrix P. Then, find its eigenvalues:
P =

3/2 1/2
1/2 1 
, λ1,2 =
5 ±
√
13
4
.
(3) Estimate a constant c > 0 using
c < min
||x|| = r
V (x),
where r is a constant to be determined. In so doing, since x
⊤Px is a
Lyapunov function and
x
⊤P x ≥ λmin(P)||x||2
,
one has the following estimate:
c < λmin(P)||x||2 =
5 −
√
13
4
r
2
.Stabilities of Nonlinear Systems (I) 71
(4) Enlarge the estimate of the basin of attraction: Let x = ρ cos(θ) and
y = ρ sin(θ), and compute
V˙ (x) = x
⊤P x˙ + x˙
⊤P x
= 2 [ x y ]

3/2 1/2
1/2 1 
"
y
− x +
1
3
x
3 − y
#
= − x
2 +
2
3
x
3
y +
1
3
x
4 − y
2
= − ρ
2 +
1
3
ρ
4
cos3
(θ)
￾
2 sin(θ) + cos(θ)

≤ − ρ
2 +
1
3
ρ
4
|2 sin(θ) + cos(θ)|
≤ − ρ
2 +
1
3
ρ
4 × 2.2361
< 0 ,
which gives ρ
2 < 3/2.2361, so that
c <
5 −
√
13
4
r
2 ≤
5 −
√
13
4
ρ
2 =
5 −
√
13
4
·
3
2.2361
< 0.46771 .
(5) An estimate of the basin of attraction for the system is finally obtained,
as
Ωc =

x ∈ R
2
: x
⊤

3/2 1/2
1/2 1 
x ≤ 0.46771 
.
4
2
−4
−2
−4
0
−2 0 2 4
Fig. 2.12 Estimation of the region of attraction.
The resulting estimated basin of attraction, Ωc, is an ellipse as shown
in Fig. 2.12. Clearly, it is a rather conservative estimate as compared to
the true basin given by the computer plot in Fig. 2.11 or Fig. 2.1272 Nonlinear Systems
Exercises
2.1 Verify which of the following systems is: (a) stable in the sense of Lya￾punov only; (b) asymptotically but not exponentially stable; (c) ex￾ponentially stable:
x˙ = −
x
(1 + t)
2
, x˙ = −
x
1 + t
, x˙ = −
x
√
1 + t
, x˙ = − (1 + t) x .
[Hint: If necessary, these equations can be analytically solved by the
standard method of separation of variables.]
2.2 Determine whether or not the following two systems have a stable
fixed point. If so, explain whether the stability is (a) in the sense of
Lyapunov only, (b) global, (c) asymptotic, (d) uniform, or (e) expo￾nential.

x˙
y˙

=

−1 2 sin(t)
0 −(t + 1)   x
y

and 
x˙
y˙

=

−1 e
2t
0 −2
  x˙
y˙

.
2.3 ∗ Prove that a linear time-varying system ˙x = A(t)x is uniformly and
asymptotically stable if and only if it is exponentially stable. Dis￾prove this for nonlinear systems by the counterexample ˙x = −x
3 with
solution x(t) = x0
1 + 2 x
2
0
(t − t0)
1/2
.
2.4 Consider the system
x˙ = 2x − 5y + x
2 − 4xy ,
y˙ = 2x − 4y + 2x
2 − 3xy + 8y
2
.
Use the Lyapunov first method to determine the stability of its zero
fixed point. Try the Lyapunov function V (x) = 2x
2 − 6xy + 5y
2
, to
see if it provides the same solution on the stability.
2.5 Consider the nonlinear system
x¨ + 3p
1 + ˙x
2 + 2 x = 0 .
(a) Find its fixed point on the x– ˙x phase plane.
(b) Determine the stability of the fixed point.
(c) Determine the type (center, saddle, focus) of the fixed point.
2.6 Consider a time-varying damped pendulum described by
x˙ = y ,
y˙ = −
k(t)
m
y −
g
ℓ
sin(x),
where the time-varying dumping coefficient satisfies
a ≤ k(t) ≤ b and c ≤ ˙k(t) ≤ dStabilities of Nonlinear Systems (I) 73
for some constants a, b, c, d. Under what conditions on a, b, c, d are its
zero fixed point: (a) stable in the sense of Lyapunov, (b) asymptoti￾cally stable, (c) uniformly stable (if applicable), and (d) globally and
asymptotically stable?
2.7 Analyze the stability of the following dynamical system:
x¨ + 2a |x| x˙ + b x = c ,
where a > 0, b > 0, and c are constants.
2.8 Consider the Volterra–Lotka system
x˙(t) = a x(t) − b x(t)y(t),
y˙(t) = b x(t)y(t) − c y(t),
where a, b, c are constants.
(a) Clearly, (0, 0) is a fixed point of the system. Find the other one.
(b) Under what condition is (0, 0) asymptotically stable?
(c) Linearize the system about (0, 0), and then about another fixed
point, respectively. Find the solutions of the two linearized sys￾tems.
2.9 Verify that the linear time-varying system
x˙ =

− 1 − 9 cos2
(6t) + 12 sin(6t) cos(6t)

x
+

12 cos2
(6t) + 9 sin(6t) cos(6t)

y ,
y˙ =

− 12 sin2
(6t) + 9 sin(6t) cos(6t)

x
−

1 + 9 sin2
(6t) + 12 sin(6t) cos(6t)

y ,
has eigenvalues λ1 = −1 and λ2 = −10, but is not even stable about
its zero fixed point since its solution is (verify it):
x = c1e
2t
(cos(6t) + 2 sin(6t)) + c2e
−13t
(sin(6t) − 2 cos(6t)),
y = c1e
2t
(2 cos(6t) − sin(6t)) + c2e
−13t
(2 sin(6t) + cos(6t)).
2.10 Discuss the stability of the following system about its zero fixed point,
by the Lyapunov first method and then by another method of your
choice:

x˙
y˙

=
 17
4 −
3
4
e
t
199
4
e
−t −
33
4
  x
y

.
2.11 Given a linear system, x˙ = A x, with a constant matrix
A =

0 1
a b 
,74 Nonlinear Systems
where a < 0 and b < 0 are constants. Prove its stability by con￾structing a Lyapunov function of the form V (x) = x
⊤Px by using the
positive definite matrix solution P of the Lyapunov equation
A
⊤P + P A = − Q
in which Q = diag {2, 2}.
2.12 Determine the stability of the fixed point of the following system:
x˙ = −2x + f(t)y ,
y˙ = g(t)x − 2y ,
where |f(t)| ≤ 1 and |g(t)| ≤ 1 for all t ≥ 0.
2.13 Consider the nonhomogeneous system
x¨(t) + 2 ˙x(t) = − f(x(t)),
where f(·) is a nonlinear differentiable function. Find the bounds a
and b for the input, in the sense that ax(t) ≤ f(x(t)) ≤ bx(t), such
that within these bounds the system is asymptotically stable about
its zero fixed point.
2.14 Use the Lyapunov function V (x) = x
2
1 + x
2
2
to study the stability of
the zero fixed point of the system
x˙ = x
￾
λ
2 − x
2 − y
2

+ y
￾
x
2 + y
2 + λ
2

,
y˙ = − x
￾
λ
2 + x
2 + y
2

+ y
￾
λ
2 − x
2 − y
2

, ,
when (a) λ = 0 and (b) λ ̸= 0.
2.15 Determine the stability of the zero fixed points of the following sys￾tems:
x˙ = −x − y ,
y˙ = x − y
2
,
and
x˙ = −x + y
2
,
y˙ = −y .
2.16 Determine the stability of the zero fixed points of the following sys￾tems:
x˙ = x − y + xy ,
y˙ = −y − y
2
,
and
x˙ = x
2 + y
3
,
y˙ = −y + x
3
.
[Hint: Try V (x, y) = (2x − y)
2 − y
2 and V (x, y) = x − y
2/2, respec￾tiveStabilities of Nonlinear Systems (I) 75
2.17 The following linear system is obviously stable in the sense of Lya￾punov:
x˙ = y ,
y˙ = − x ,
since it is equivalent to ¨x = −x, with solution x(t) = c1 sin(t) +
c2 cos(t). However, someone shows the following reasoning to argue
that it is unstable even in the sense of Lyapunov. Explain why it is
wrong:
Let V (x, y) = (2x − y)
2 − y
2 = 4x
2 − 4xy .
Then V (x, y) > 0 if x > y (see Fig. 2.13 for a selected region Ω)
and V (x, y) = 0 on the boundaries x = y and x = 0 of Ω
with V (0, 0) = 0 and (0, 0) ∈ boundary of Ω .
However,
V˙ (x, y) = 8xx˙ − 4 ˙xy − 4xy˙ = 8xy − 4y
2 − 4x(−x)
= 4(2x − y)y + 4x
2 > 0 inside Ω .
So, the equilibrium point (0,0) is unstable in the sense of Lyapunov.
x
y
y = x
V = 0
V = 0
x = 0
0
V > ˙ 0
Ω0
Fig. 2.13 Selected region of Ω.
2.18 Apply the LaSalle invariance principle to show that the Li´enard equa￾tion
x¨ + f(x) ˙x + g(x) = 0 ,
with g(0) = 0, is globally asymptotically stable about its zero fixed
point, if the following conditions are satisfied:
f(x) > 0 , xg(x) > 0 for all x ̸= 0 , and
G(x) = Z x
0
g(z) dz → ∞ (x → ∞).
[Hint: Try V (x, x˙) = 1
2
y
2 + G(x), and note that the largest invariant
set is S = {(x, y): f(x)y
2 = 0}, where y = ˙x.]76 Nonlinear Systems
2.19 Apply the result of the previous exercise to the following systems:
x¨ + x
2x˙ + x = 0
and to the van der Pol oscillator equation
x¨ + ε (x
2 − 1) ˙x + x = 0 (ε < 0).
2.20 Consider the following autonomous system:
x˙ = y ,
y˙ = − f(x),
satisfying f(0) = 0. Let the Lyapunov function be constructed in
terms of the total system energy:
V (x, y) = 1
2
y
2 +
Z x
0
f(z)dz .
Use this Lyapunov function to discuss the stability of the system with
respect to the properties of f.
2.21 Use the variable gradient method to find a Lyapunov function for the
system
x˙ = y ,
y˙ = − 4 (x + y) − h(x + y),
where h(·) is a nonlinear function satisfying h(0) = 0 and zh(z) ≥ 0
for all |z| ≤ 1. [Hint: V (x) = 2x
2 + 2xy + y
2
is one solution.]
2.22 Estimate the basin of attraction of the zero fixed point for the following
system:
x˙ = − y ,
y˙ = x +
￾
x
2 − 1

y .
[Hint: Solve the Lyapunov equation to find the matrix P > 0 and
then try the Lyapunov function V (x) = x
⊤Px. Note also that
| cos2
θ sin θ| ≤ 0.3849 and |2 sin θ − cos θ| ≤ 2.2361.]
2.23 For the following system:
x˙ =
 1
√
2
z +
α
2

y − β x ,
y˙ =
 1
√
2
z −
α
2

x − β y ,
z˙ =
√
2
￾
1 − xy 
,
where α > 0 and β > 0, show that in a large enough neighborhood of
the origin there must be an attractor, and that this attractor is not
the origin. [Hint: Construct a Lyapunov function to argueStabilities of Nonlinear Systems (I) 77
2.24 Consider the following autonomous system:
x˙ = y ,
y˙ = − x +
￾
1 − x
2 − y
2

y .
(a) Discuss the stability of the system about its zero fixed point.
(b) Find the limit cycle of the system.
(c) Use the Lyapunov method and the LaSalle principle to show that
any system trajectory, if not starting from the origin, will converge
to the limit cycle.
[Hint: Polar coordinates may be convenient to use.This page intentionally left blankChapter 3
Stabilities of Nonlinear Systems (II)
This chapter continues the study of nonlinear system stabilities. Specifi￾cally, the linear stability of nonlinear systems with periodic linearity will
be discussed, followed by orbital stability, structural stability and total sta￾bility. Meanwhile, a useful toll of comparison principle will be introduced.
3.1 Linear Stability of Nonlinear Systems
The first method of Lyapunov provides a linear stability analysis for non￾linear autonomous systems. As can be seen from Theorems 2.1 and 2.11,
the local stability and instability issues for autonomous systems are quite
simple, which are similar to that of the familiar linear time-invariant sys￾tems. As has also been emphasized in the last chapter, the Lyapunov first
method cannot be generally applied to nonautonomous systems. Thus, one
may wonder to what extent the simple and efficient linear stability analysis
methodology can be used for studying nonlinear nonautonomous systems.
In this section, the attention is first switched back onto the general
nonautonomous system:
x˙ = f(x, t), x(t0) = x0 ∈ R
n
, (3.1)
which is assumed to have a fixed point at x
∗ = 0.
In system (3.1), a Taylor expansion of the function f about its zero fixed
point gives
x˙ = f(x, t) = J(t;t0) x + g(x, t), (3.2)
where J(t;t0) = 
∂f/∂x

x=0 is the Jacobian and g(x, t) is the residual of
the expansion, which is assumed to satisfy



g(x, t)



 ≤ a ||x||2
for all t ∈ [t0, ∞), (3.3)
7980 Nonlinear Systems
with constant a > 0. Recall from (2.5) that the solution of Eq. (3.2) is
given by
x(t) = Φ(t, t0) x0 +
Z t
t0
Φ(t, τ ) g(x(τ ), τ ) dτ , (3.4)
where Φ(t, τ ) is the fundamental matrix of the system, defined in (2.4).
Theorem 3.1 (A General Linear Stability Theorem). For the non￾linear nonautonomous system (3.2), if there are two positive constants, c
and σ, such that



Φ(t, τ )



 ≤ c e−σ(t−τ)
for all t0 ≤ τ ≤ t < ∞,
and if
lim
||x||→0
||g(x, t)||
||x|| = 0
uniformly with respect to t ∈ [t0, ∞), then there are two positive constants,
γ and δ, such that



x(t)



 ≤ c



x0



 e
−γ(t−t0)
for all ||x0|| ≤ δ and all t ∈ [t0, ∞).
This result implies that under the theorem conditions, the system is
locally, uniformly and exponentially stable about its zero fixed point. A
proof of this theorem relies on the Gronwall Inequality, which is very useful
in its own right (see, e.g. Example 3.3 below).
Lemma 3.1 (Gronwall Inequality). Let x(t), y(t), and z(t) be scalar￾valued continuous functions defined on [a, b], with z(t) > 0 and
x(t) ≤ y(t) + Z t
a
z(τ )x(τ ) dτ , for all t ∈ [a, b] .
Then, the following Gronwall Inequality:
x(t) ≤ y(t) + Z t
a
z(τ )y(τ ) exp Z t
τ
z(s)ds
dτ , (3.5)
holds for all t ∈ [a, b].
Proof. Define
r(t) = exp 
−
Z t
a
z(τ )dτ Z t
a
z(τ )x(τ )dτ , t ∈ [a, b].Stabilities of Nonlinear Systems (II) 81
Then,
r˙(t) = 
x(t) −
Z t
a
z(τ )x(τ )dτ
z(t) exp 
−
Z t
a
z(τ )dτ
,
where, by the integral inequality in the assumed condition,

x(t) −
Z t
a
z(τ )x(τ )dτ
≤ y(t).
Since r(a) = 0, integrating the above inequality from a to t gives
r(t) ≤
Z t
a
y(s)z(s) exp 
−
Z s
a
z(τ )dτ
ds .
Consequently,
Z t
a
z(τ )x(τ )dτ = exp Z t
a
z(τ )dτ
r(t)
≤
Z t
a
y(τ )z(τ ) exp Z t
a
z(s)ds −
Z τ
a
z(s)ds
dτ ,
where R t
a
z(s)ds −
R τ
a
z(s)ds =
R t
τ
z(s)ds.
Substituting this into the integral inequality in the assumed condition
yields the Gronwall inequality.
Now, a proof of Theorem 3.1 can be readily given.
Proof. Equation (3.4), or its slightly different form,
x(t) = Φ(t, t0) x0 +
Z t
t0
Φ(t, t0)Φ−1
(t0, τ ) g(x(τ ), τ ) dτ ,
along with the bound on Φ(t, t0) given in the theorem, yields
||x(t)|| ≤ c e−σ(t−t0)
||x0|| +
Z t
t0
c e−σ(t−τ)
||g(x(τ ), τ )|| dτ .
Due to the limiting condition given in the theorem, which means that g
is a higher-order function of x, one has ||g(x, t)|| ≤ β ||x|| for a constant
β > 0, for (small enough) ||x|| < β/α with α > 0. It then follows from the
Gronwall Inequality, with x(t) = ||x||e
σt therein, that
x(t) ≤ c eσt0
||x(t0)|| + β c Z t
t0
x(τ ) dτ
which holds for small enough ||x||. Using again the Gronwall Inequality,
but with constant functions y = c eσt0
||x(t0)|| and z = β c at this time,
yields
x(t) ≤ c eσt0
||x(t0)|| · e
β c(t−t0)
.82 Nonlinear Systems
Therefore,
||x(t)|| ≤ c ||x(t0)|| · e
−(σ−β c) (t−t0)
.
Thus, by choosing β < σ/c and letting γ = σ − β c > 0, and moreover
restricting ||x(t0)|| to be so small that ||x(t0)|| < δ := β/(αc), the above
inequality remains to be valid, with



x(t)



 ≤ c



x0



 e
−γ(t−t0)
which holds for all ||x0|| ≤ δ and all t ∈ [t0, ∞).
Example 3.1. The simple nonlinear autonomous system
x˙ = −a x + x
3
is exponentially stable about its zero fixed point, if a > 0, in a small
neighborhood of zero.
The stability of this system is obvious: the linear part of the system,
x˙ = −a x is exponentially stable, while the nonlinear part, x
3
, is extremely
small in |x| near the origin. Therefore, the linear stability dominates the
nonlinear instability in this system.
Note, however, that not all solutions of this system tend to the zero
fixed point, which can be verified simply by sketching the phase portrait
of its solution flow, or be understood from the fact that for large |x|, the
term x
3 will dominate the linear part, −a x. Hence, starting from an initial
point relatively far from the origin, the system trajectory will diverge.
Example 3.2. Consider the damped pendulum (1.1), now in the following
form:

˙θ
¨θ

=

0 1
−
g
ℓ −
κ
m
  θ
˙θ

+

0
g
ℓ
(θ − sin(θ)) 
.
It is easy to verify that the above constant system matrix is stable. More￾over,
lim
|θ|→0
|θ − sin(θ)|
|θ|
= 0 .
Therefore, all the conditions of Theorem 3.1 are satisfied. The system is
asymptotically stable about its zero fixed point.
If the following truncated expansion is used:



g
ℓ
￾
θ − sin(θ)



 ≈
g
ℓ




θ
3
3! + · · · ±
θ
2n+1
(2n + 1)!




≤ c
￾
|θ|
3 + · · · + |θ|
2n+Stabilities of Nonlinear Systems (II) 83
for some constant c > 0, then Theorem 3.1 is also applied to conclude that
the zero fixed point of the damped pendulum is asymptotically stable.
Note that this example shows that the use of a more accurate approxi￾mate model may not make a radical difference in asymptotic stability anal￾ysis, as for the pendulum with a small enough angle here.
Example 3.3. Consider a perturbed linear time-invariant system,
x˙(t) = 
A + ∆A(t)

x(t), x(t0) = x0 ∈ R
n
,
where the nominal matrix A is stable (i.e. all its eigenvalues have negative
real parts). If the perturbation of the system matrix, ∆A(t), is continuous
and satisfies



∆A(t)



 < c for all t0 ≤ t < ∞
for a small enough constant c > 0, then the perturbed system will remain
asymptotically stable about its zero fixed point.
To show this, however, the limiting condition stated in Theorem 3.1
cannot be verified. Thus, one has to resort to a direct application of the
Gronwall Inequality (3.5).
The stability of matrix A implies that there are positive constants α
and β such that ||Φ(t, t0)|| ≤ α e−(t−t0)β
for all t ≥ t0, so that the solution
x(t) = Φ(t, t0)x0 +
Z t
t0
Φ(t, τ )∆A(τ )x(τ ) dτ
satisfies
||x(t)|| e
β(t−t0) ≤ α||x0|| +
Z t
t0
α||∆A(τ )|| ||x(τ )|| e
β(t−τ)
dτ
≤ α||x0|| exp 
α
Z t
t0
||∆A(τ )|| dτ 
≤ α||x0|| e
α c (t−t0)
,
where the Gronwall Inequality (3.5) has been used. It then follows that
||x(t)|| ≤ α ||x0|| e
(αc−β) (t−t0)
.
If c is small enough such that αc − β < 0, then ||x(t)|| → 0 as t → ∞.
Note that the system x˙(t) = 
A + ∆A(t)

x(t) in this example can be
viewed as
x˙(t) = A x(t) + ε g(x(t), t), x(t0) = x0 ∈ R
n
,84 Nonlinear Systems
where ε > 0 is a constant and ||g|| ≤ δ uniformly for all x ∈ Rn for
all t ≥ t0, with the constant δ > 0 such that ε δ < c for a small enough
c > 0. This means that a uniformly small (even nonlinear and time-varying)
perturbation does not alter the stability of the linear part of the system.
Next, return to system (3.2). If the Jacobian J(t;t0) = J therein is
a stable constant matrix, then the following simple criterion (as a special
case of Example 2.6) is convenient to use.
Theorem 3.2. Suppose that in system (3.2), the matrix J(t;t0) = J is a
stable constant matrix and g(0, t) = 0. Let P be a positive definite matrix
solution of the Lyapunov equation
P J + J
⊤P + Q = 0 ,
where Q is a positive definite constant matrix. If
||g(x, t)|| ≤ a ||x||
for a constant a < 1
2
λmax(P) uniformly on [t0, ∞), then system (3.2) is
globally, uniformly, and asymptotically stable about its zero fixed point.
A proof of this theorem was actually given in Example 2.6, and is a
special case of the following slightly generalized result.
Theorem 3.3. In system (3.2), suppose that both the matrix J(t;t0) and
the function g(x, t) are continuous, and that there is a continuous non￾negative function γ(t) satisfying R ∞
t0
γ(τ )dτ ≤ a < ∞ and ||g(x, t)|| ≤
γ(t)||x(t)||. Under these conditions, if J(t;t0) is uniformly stable (with all
eigenvalues having a negative real part for all t ≥ t0), then the system (3.2)
is locally, uniformly, and asymptotically stable about its zero fixed point.
Proof. Note that the matrix J(t;t0) is uniformly stable. Thus, according
to Exercise 3.6, there is a constant c such that ||Φ(t, τ )|| ≤ c for all t0 ≤
τ ≤ t < ∞. For any t
∗ ≥ t0,
x(t) = Φ(t, t∗
)x(t
∗
) + Z t
t
∗
Φ(t, τ )g(x(τ ), τ ) dτ ,
where t0 ≤ t
∗ ≤ t < ∞. It follows from the Gronwall inequality (3.5) that
||x(t)|| ≤ c ||x(t
∗
)|| + c
Z t
t
∗
γ(τ )||x(τ )|| dτ
≤ c ||x(t
∗
)|| exp 
c
Z t
t
∗
γ(τ ) dτ 
≤ c ||x(t
∗
)|| exp 
c
Z ∞
t
∗
γ(τ ) dτ 
≤ c ea
||x(t
∗
)|| .Stabilities of Nonlinear Systems (II) 85
For any ε > 0, starting from an initial state satisfying ||x(t
∗
)|| < εe−a/(2c),
one has ||x(t)|| < ε/2. Then, an argument similar to that given in the
proof of Theorem 3.1 shows that ||x(t)|| < ε/2 for all t ≥ t
∗
. Therefore,
||x(t)|| < ε for all t0 ≤ t < ∞. Hence, the system is stable in the sense of
Laypunov about its zero fixed point.
Furthermore, since the matrix J(t;t0) is uniformly stable, ||Φ(t, t0)|| →
0 as t → ∞. Thus, for any ε > 0 and any bounded ||x0||, there is a t
∗ > t0
such that ||Φ(t, t0)x0|| < ε for t ≥ t
∗
, so that
||x(t)|| ≤ ||Φ(t, t0)x0|| +
Z t
t0
||Φ(t, τ )|| ||g(x(τ ), τ )|| dτ
≤ ε +
Z t
t0
c γ(τ )||x(τ )|| dτ
≤ ε exp 
c
Z ∞
t0
γ(τ ) dτ 
≤ ε eac, t∗ ≤ t < ∞.
Since ε is arbitrary and e
ac does not depend on ε and t
∗
, it follows that
||x(t)|| → 0 as t → ∞ uniformly.
3.2 Linear Stability of Nonlinear Systems with Periodic
Linearity
First, consider a linear time-varying system with a periodic coefficient ma￾trix,
x˙ = A(t) x , x(t0) = x0 ∈ R
n
, (3.6)
where A(t + tp) = A(t), with the fundamental period tp > 0. It should be
noted that a solution of this system may not be periodic, as can be seen
from the following simple example.
Example 3.4. The system
x˙(t) = [1 + sin(t)] x(t)
has a 2π-periodic coefficient, but its nonzero solution is aperiodic:
x(t) = x0 e
t−cos(t)
.
To further study system (3.6), let Φ(t, t0) be the fundamental matrix
associated with A(t), which is defined by Φ(t, τ ) = exp  R t
τ A(σ)dσ	
. Recall
that all columns of Φ(t, t0) are linearly independent solution vectors of (3.6),
and the following is satisfied, Φ(˙ t, τ ) = A(t)Φ(t, τ ) for t0 ≤ τ ≤ t < ∞.86 Nonlinear Systems
For the tp-periodic matrix A(t + tp) = A(t), it is clear that Φ(t + tp) =
Φ(t), so that Φ(t + tp, t0) is another fundamental matrix associated with
A(t). Thus, the columns of Φ(t+tp), being (linearly independent) solutions
of (3.6), are linear combinations of those in Φ(t, t0):
ϕij (t + tp) = Xn
ℓ=1
cℓj ϕiℓ ,
where Φ = [ϕij ] and {cij} are constants, or
Φ(t + tp, t0) = Φ(t, t0) C (3.7)
with matrix C = [cij ]. Since det [Φ(t + tp, t0)] = det [Φ(t, t0)] · det [C] and
det [Φ(t, t0)] ̸= 0 for all t ≥ t0, one has det [C] ̸= 0.
It should be noted that all the eigenvalues of C are independent of the
choice of the fundamental matrix Φ(t, t0). To see this, let Φ1(t, t0) and
Φ2(t, t0) be two fundamental matrices associated with A(t). Then, since
they consist of solutions of the same equation (3.6), there is a constant
matrix, D, such that
Φ1(t, t0) = Φ2(t, t0) D ,
and this D is nonsingular since both Φ1 and Φ2 are so. Thus, it follows
that
Φ1(t + tp;t0) = Φ2(t + tp;t0) D = Φ2(t, t0) CD
= Φ1(t, t0)D−1CD := Φ1(t, t0) H .
This implies that H plays the role of C in Eq. (3.7). This also implies
that H and C are similar nonsingular matrices, so they have the same
eigenvalues. Therefore, a different choice of the fundamental matrix Φ in
(3.7) only changes the form, but not the eigenvalues, of the constant matrix
C therein.
Definition 3.1. The eigenvalues of the constant matrix C in Eq. (3.7) are
called the Floquet multipliers of the system (3.6).
The importance of the Floquet multipliers can be appreciated from the
observation that Eq. (3.7) implies
Φ(t + ntp, t0) = Φ(t, t0) C
n
, n = 1, 2, . . . , (3.8)
which, in turn, implies that the long-term dynamical behavior of a solution
of system (3.6) is determined by the eigenvalues of C, namely, the Floquet
multipliers.
Theorem 3.4 (Floquet Theorem). System (3.6) has at least one
nonzero solution, x(t), and this solution satisfies
x(t + tp) = λ x(t), (3.9)
where λ is a Floquet multiplier.Stabilities of Nonlinear Systems (II) 87
Note that this solution, x(t), may not be periodic (unless λ = 1).
Proof. Let λ be an eigenvalue of C and let v be its associated eigenvector:
[C − λI] v = 0 .
Let x(t) = Φ(t, t0)v. Then, it is straightforward to verify that x(t) is a
nonzero solution of system (3.6). Moreover,
x(t + tp, t0) = Φ(t + tp, t0) v = Φ(t, t0) C v = Φ(t, t0) λ v = λ x(t),
as claimed.
The following example shows how to calculate the Floquet multipliers
for a simple system.
Example 3.5. Consider the system

x˙
y˙

=
"
1 1
0
sin(t)+cos(t)
2+sin(t)−cos(t)
# 
x
y

,
with initial time t0 = 0. This linear system can be easily solved, yielding
x(t) = a et − b
￾
2 + sin(t)

,
y(t) = b
￾
2 + sin(t) − cos(t)

,
(a)
with constants a and b determined by initial conditions. Hence, a corre￾sponding fundamental matrix is
Φ(t) = 
−2 − sin(t) e
t
2 + sin(t) − cos(t) 0 
.
Note that matrix Φ(t) is 2π-periodic up to a constant matrix multiplier,
and the matrix C in (3.7) must satisfy Φ(t+2π) = Φ(t)C here, for all t ≥ 0.
Therefore, Φ(2π) = Φ(0)C, so that
C = Φ−1
(0)Φ(2π) = 
1 0
0 e
2π

.
Thus, the Floquet multipliers of the system are given by the eigenvalues of
matrix C, which are λ1 = 1 and λ2 = e
2π
.
In this example, λ1 = 1 implies a 2π-periodic solution of the system
(see (3.9)), which corresponds to a = 0 in the solution formula (a88 Nonlinear Systems
Now, in Eq. (3.7), let
R = ln(C)/tp ,
where the principal value of the natural logarithm is taken, so C = e
tpR.
This constant matrix, R, is very useful. For example, it can be used to
transform the periodic system (3.6) to be one having a constant coefficient
matrix. Indeed, letting
x =

Φ(t, t0) e
−(t−t0)R

y
changes the system (3.6) to
y˙ = R y .
Definition 3.2. In Definition 3.1, if for a Floquet multiplier λ, there is a
real constant ρ such that
λ = e
ρ tp
,
then this ρ is called a characteristic exponent or Floquet number of the
system (3.6).
In this definition, observe that
ρ = ln(λ)/tp ,
where the principal value of the natural logarithm is taken. Since λ is an
eigenvalue of the constant matrix C, ρ is an eigenvalue of the constant
matrix R. Moreover, for the real part of ρ,
Re{ ρ } < 0 ⇐⇒ |λ| < 1 ,
which is useful for stability determination.
Now, consider a nonlinear nonautonomous system of the form
x˙ = f(x, t) = J(t) x + g(x, t), x(t0) = x0 ∈ R
n
, (3.10)
where g(0, t) = 0 and J(t) is a p-periodic matrix, with p > 0:
J(t + p) = J(t) for all t ∈ [t0, ∞).
Theorem 3.5 (Floquet Theorem). In the system (3.10), assume that
both g(x, t) and ∂g(x, t)/∂x are continuous in a bounded region D contain￾ing the origin. Assume also that
lim
||x||→0
||g(x, t)||
||x|| = 0
uniformly over [t0, ∞). If the system Floquet multipliers satisfy
| λi
| < 1 , i = 1, . . . , n , for all t ∈ [t0, ∞), (3.11)
then system (3.10) is globally, uniformly, and asymptotically stable about
its zero fixed point. In particular, this holds for the linear system (3.10)
with g = 0 therein.Stabilities of Nonlinear Systems (II) 89
Proof. First, for the linear case with g = 0, it follows from (3.9) and
condition (3.11) that
x(t0 + ntp) = λ
n
maxx(t0) → 0 as n → ∞,
where λmax = max{λi
: i = 1, . . . , n}. Then, for the general case, property
(3.8) and condition (3.11) together imply that there exist constants c > 0
and σ > 0 such that
||Φ(t0 + ntp, t0)|| ≤ c e−σ n
,
so that conditions similar to those in Theorem 3.1 are satisfied. Hence,
the proof of Theorem 3.1 can be suitably modified for proving the present
theorem.
One should compare this theorem with Theorem 3.1, which is for sys￾tems with an aperiodic coefficient matrix and so can achieve exponential
stability. For systems with a periodic coefficient matrix, however, the
best stability that one may expect is the asymptotic stability but not
the exponential one. This is due to the periodicity of the system which,
roughly, yields a stable solution converging to zero at the same rate as
c e−σt sin(ωt + ϕ).
3.3 Comparison Principles
For large-scale and interconnected nonlinear systems, or systems described
by differential inequalities rather than differential equations, the above sta￾bility criteria may not be directly applicable. In many such cases, the
comparison principle is useful.
3.3.1 Comparison Principle on the Plane
3.3.1.1 Comparison of zero points
First, recall a concept from ordinary differential equations. A one-variable
linear differential equation is said to be homogeneous if it satisfies
L(x) = 0 ,
where
L(·) = Xn
i=0
fi(t)
d
i
dti
(·),
in which fi may be constant but not all zero, i = 1, 2, . . . , n.90 Nonlinear Systems
Example 3.6. The following second-order linear differential equations are
homogeneous:
sin(t)¨x + 2 ˙x + x = 0 , x¨ + 2t x˙ + 3t
2x = 0 ,
but the following are not:
x¨ + 2 ˙x + x = sin(x), x¨ + 2t x˙ + 3t
2x = 1 .
Theorem 3.6 (Sturm Separation Theorem). Consider a second￾order homogeneous linear differential equation,
x¨ + c(t)x = 0 ,
defined on [t0, T], T ≤ ∞, where c(t) is a continuous function. Let ϕ1(t)
and ϕ2(t) be its two fundamental solutions. Then, all zeros of the nontrivial
solutions are isolated. Moreover, for any two successive zeros t1 and t2 of
ϕ1(t), solution ϕ2(t) has exactly one zero located in (t1, t2).
Proof. Assume that the zeros of a nontrivial solution is not isolated. Then,
since the zero set has a limit point, and the solution is a continuous function,
the initial value problem has a zero solution, a contradiction.
Then, assume that the nontrivial solution ϕ2(t) does not have zero in
(t1, t2), then it does not have zero on [t1, t2] since ϕ1(t) and ϕ2(t) are two
fundamental solutions, hence are linearly independent. Thus, one can define
a function
ψ(t) = ϕ1(t)
ϕ2(t)
on [t1, t2]. This function is continuous on [t1, t2], where ψ˙(t) exists on
(t1, t2), and ψ(t1) = ψ(t2) = 0. Consequently, by Rolle’s Theorem in
Calculus, there exists a point p ∈ (t1, t2) such that ψ˙(p) = 0. This implies
that the Wronskian determinant is zero, but it is impossible because ϕ1(t)
and ϕ2(t) are linearly independent.
Finally, assume that ϕ2(t) has two zeros in (t1, t2), say t1 < t3 < t4 < t2.
Then, similarly to the above argument, ϕ1(t) will have a zero in (t3, t4), but
this contradicts the fact that t1 and t2 are two successive zeros. Hence, ϕ2(t)
cannot have more than one zero in (t1, t2).Stabilities of Nonlinear Systems (II) 91
Theorem 3.7 (Sturm Comparison Theorem). Consider two second￾order homogeneous linear differential equations,
x¨ + c1(t)x = 0 ,
x¨ + c2(t)x = 0 ,
defined on [t0, T], T ≤ ∞, where the two coefficients are continuous func￾tions satisfying c1(t) ≤ c2(t) on [t0, T]. Let ϕ1(t) and ϕ2(t) be two nontrivial
solutions of the two equations, respectively. Then, between any two succes￾sive zeros t1 and t2 of ϕ1(t) = 0, there exists at least one zero of ϕ2(t)
unless c1(t) ≡ c2(t) on [t0, T].
Proof. Without loss of generality, assume that ϕ1(t) > 0 on (t1, t2); oth￾erwise, consider −ϕ1(t). Thus, one has ϕ˙
1(t1) > 0 and ϕ˙
1(t2) < 0.
Suppose, on the contrary, that ϕ2(t) does not have zero on (t1, t2) and,
likewise, without loss of generality assume that ϕ2(t) > 0 on (t1, t2).
Multiplying the first equation by ϕ1(t) and the second by ϕ2(t) and then
subtracting the resultant equations yield
ϕ¨
1ϕ2 − ϕ1ϕ¨
2 + (c1 − c2)ϕ1ϕ2 = 0 ,
which gives
d
dt (ϕ˙
1ϕ2 − ϕ1ϕ˙
2) = −(c1 − c2)ϕ1ϕ2 .
Integrating both sides from t1 to t2 leads to
(ϕ˙
1ϕ2 − ϕ1ϕ˙
2)



t2
t1
= −
Z t2
t1
[c1(t) − c2(t)]ϕ1(t)ϕ2(t)dt .
Note that the left-hand side is non-positive, while the right-hand side is
strictly positive unless c1(t) ≡ c2(t) on (t1, t2). Thus, for the case of c1(t) ̸≡
c2(t), this is a contradiction.
It is remarked that if c1(t) ≡ c2(t) then the two equations become the
same, so ϕ1(t) ≡ ϕ2(t) on (t1, t2). In this case, the theorem does not exist.
Definition 3.3. The equation d
dt ￾
p(t) ˙x

+q(t)x = 0 is said to be oscillatory
on a time interval [t0, T], T ≤ ∞, if there exists a nontrivial solution of the
equation with infinitely many zeros on the time interval.
Example 3.7. The equation ¨x + x = 0 is oscillatory on [t0, ∞). Indeed, it
has a nontrivial solution x(t) = sin(t), which has infinitely many zeros on
[t0, ∞).
In fact, this equation has a general solution x(t) = c sin(t), with a con￾stant c, and every particular solution has infinitely many zeros on [t0, ∞)92 Nonlinear Systems
It can be proved that the equation d
dt ￾
p(t) ˙x

+ q(t)x = 0 is oscillatory
on a time interval [t0, T], T ≤ ∞, if and only if all its solutions are so.
Thus, it follows that the oscillatory equation (system) d
dt ￾
p(t) ˙x

+
q(t)x = 0 is bounded, in the sense that all its solutions are bounded.
Note, however, that this system is not stable about its zero fixed point
in the sense of Lyapunov, simply because no solution trajectory will stay
nearby the zero fixed point forever.
3.3.1.2 Comparison of functions
For a more general setting than the above Sturm comparison theorem, there
is an extension as follows.
Lemma 3.2 (Comparison Lemma). Consider the 1-dimensional
nonautonomous system
x˙ = f(x, t), x(t0) = x0 ,
where f(x, t) is locally Lipschitz in x ∈ D ⊆ R and continuous in
t ∈ [t0, ∞). Let y(t) be a continuously differentiable function on [t0, ∞),
satisfying
y˙(t) ≤ f(y, t), y(t0) ≤ x(t0),
for all x ∈ D and all t ∈ [t0, ∞). Then,
y(t) ≤ x(t), for all t ∈ [t0, ∞).
It can be easily seen that this lemma would be useful for determining
the stability of one system from that of another.
3.3.2 Comparison Principle in Higher-Dimensional Spaces
Unlike the case of planar systems discussed above, comparing two entangled
trajectories in a higher-dimensional space is very difficult and, in fact, only
in some special situations such comparison is possible. Moreover, there
are various forms of comparison principle in the higher-dimensional set￾ting. Here, one particular form is presented to show some ideas beyond the
comparison principle for higher-dimensional systems.
Consider the following two n-dimensional nonautonomous systems:
x˙ i = fi(x1, x2, . . . , xn, t), xi(t0) = x
0
i
, (3.12)
where fi
is continuous in t ∈ [t0, ∞), i = 1, 2, ..., n, and
y˙i = gi(y1, y2, . . . , yn, t), yi(t0) = y
0
i
, (3.13)
where gi
is continuous in t ∈ [t0, ∞), i = 1, 2, . . . , n, with n ≥ Stabilities of Nonlinear Systems (II) 93
Theorem 3.8 (Comparison Principle for Higher-Dimensional Sys￾tems). Suppose that, for all i, j = 1, 2, . . . , n,
(i) fi(x1, x2, . . . , xn, t) ≥ gi(y1, y2, . . . , yn, t);
(ii) x
0
i ≥ y
0
i
;
(iii) ∂fi/∂xj ≥ 0, i ̸= j.
Then, yi ≥ 0 for all t ≥ t0 and i = 1, 2, . . . , n, implying that xi ≥ 0, for all
t ≥ t0 and i = 1, 2, . . . , n.
Proof. See [Wa´zewski (1950)].
3.4 Orbital Stability
The orbital stability differs from the Lyapunov stabilities in that it concerns
with the stability of a system output (or state) trajectory under small
external perturbations.
Let φt(x0) be a p-periodic solution, p > 0, of the autonomous system
x˙(t) = f(x), x(t0) = x0 ∈ R
n
, (3.14)
and let Γ represent the closed orbit of φt(x0) in the phase space, namely,
Γ = n
y | y = φt(x0), 0 ≤ t < p o
.
Definition 3.4. The p-periodic solution trajectory, φt(x0), of the au￾tonomous system (3.14) is said to be orbitally stable if, for any ε > 0,
there exits a constant δ = δ(ε) > 0 such that for any x0 satisfying
d(x0, Γ) := inf
y∈Γ
||x0 − y|| < δ ,
the solution of the system, φt(x0), satisfies
d(φt(x0), Γ) < ε , for all t ≥ t0 .
The orbital stability is visualized in Fig. 3.1.
Theorem 3.9 (Orbital Stability Theorem). Let x(t) be a p-periodic
solution of an autonomous system. Suppose that the system has Floquet
multipliers λi, with λ1 = 0 and |λi
| < 1, for i = 2, . . . , n. Then, this
periodic solution x(t) is orbitally stable.94 Nonlinear Systems
Γ
d (x0, Γ) < δ
x0
d(ϕt(x0), Γ) < ε
ϕt(x0)
Fig. 3.1 Geometric meaning of the orbital stability.
Proof. See [Goddington and Levinson (1955)]: Chapter 13.2, Theorem 2.2.
Example 3.8. For a simple example, a stable periodic solution, particu￾larly a stable fixed point of a system, is orbitally stable. This is because all
nearby trajectories approach it and, as such, it becomes a nearby orbit after
a small perturbation and so will move back toward its original position (or
stay nearby). On the contrary, unstable and saddle-node type of periodic
orbits are orbitally unstable.
3.5 Structural Stability
Consider again the autonomous system (3.14).
To proceed, recall that a function or map is called a diffeomorphism if
it is differentiable, invertible and its inverse is also differentiable.
Definition 3.5. Two system orbits are said to be topologically equivalent if
there is a diffeomorphism that transfers one orbit to another. Two systems
are said to be topologically orbitally equivalent, if their phase portraits are
topologically equivalent.
If the dynamics of a system in the phase space changes radically, for
instance by the appearance of a new fixed point or a new periodic orbit, due
to small external perturbations, then the system is structurally unstable.
To be more precise, consider the following set of functions:
S =

g(x)


||g(x)|| < ∞, ||∂g(x)/∂x|| < ∞, ∀ x ∈ R
n
	
.
Definition 3.6. If, for any g ∈ S, there exists an ε > 0 such that the orbits
of the two systems
x˙ = f(x) (3.15)
x˙ = f(x) + ε g(x) (3.16)Stabilities of Nonlinear Systems (II) 95
are topologically orbitally equivalent, then the autonomous system (3.15)
is said to be structurally stable.
Example 3.9. System ˙x = x
2
is not structurally stable in any neighbor￾hood of the origin. This is because, when the system is slightly perturbed,
to become say ˙x = x
2 + ε, where ε > 0, then the resulting system has two
equilibria, x
∗
1 =
√
ε and x
∗
2 = −
√
ε, which has more numbers of fixed points
than the original system that possesses only one, x
∗ = 0.
In contrast, ˙x = x is structurally stable in any neighborhood of the
origin, although the origin is an unstable fixed point of the system. To
verify this, consider its perturbed system
x˙ = x + ε g(x),
where g(x) is continuously differentiable and there is a constant c > 0 such
that
| g(x)| < c and

 g
′
(x)

 < c , for all x ∈ R .
To find the fixed points of the perturbed system, let
f(x) := x + ε g(x) = 0
and choose ε such that ε < 1/(2c). Then,

 εg′
(x)

 < ε c < 1/2 , for all x ∈ R ,
so that
f
′
(x) = 1 + ε g′
(x) > 1/2 , for all x ∈ R .
This implies that the curve of the function f(x) intersects the x-axis at
exactly one single point. Hence, equation
x˙ = f(x) = x + ε g(x)
has exactly one solution, x
∗
. Since g(x) is (uniformly) bounded, |g(x)| < c
for all x ∈ R. As ε → 0, one has εg(x) → 0, so that x
∗ → 0 as well. This
means that the perturbed fixed point approaches the unperturbed one.
Furthermore, it must be shown that the perturbed fixed point is always
unstable just like the unperturbed one. To do so, observe that the linearized
equation of the perturbed system is
x˙ = f(x) ≈ f
￾
xe

+ f
′
(x)


x=xe
￾
x − xe

= 0 + 
1 + ε g′
￾
xe
 ￾x − xe
96 Nonlinear Systems
where xe is the perturbed fixed point. Since
1 + ε g′
￾
xe

> 1 −

 ε g′
￾
xe
 
 > 1 −
1
2
=
1
2
> 0 ,
the eigenvalue of the Jacobian f
′
(xe) is positive, implying that the per￾turbed system is unstable about its zero fixed point. Therefore, the unper￾turbed and perturbed systems are topologically orbitally equivalent; or, in
other words, the unperturbed system is structurally stable about its zero
fixed point.
Theorem 3.10 (Peixoto Structural Stability Theorem). Consider a
2-dimensional autonomous system. Suppose that f is twice differentiable
on a compact and connected subset D bounded by a simple closed curve, Γ,
with an outward normal vector, ⃗n. Assume that f · ⃗n ̸= 0 on Γ. Then, the
system is structurally stable on D if and only if
(i) all equilibria are hyperbolic;
(ii) all periodic orbits are hyperbolic;
(iii) if x and y are hyperbolic saddles (probably, x = y), then Ws
(x)∩Wu
(y)
is empty.
Proof. See [Glendinning (1994)]: p. 92.
3.6 Total Stability: Stability under Persistent Disturbances
Consider a nonautonomous system and its perturbed version
x˙ = f(x, t), x(t0) = x0 ∈ R
n
, (3.17)
x˙ = f(x, t) + h(x, t), (3.18)
where f is continuously differentiable, with f(0, t) = 0, and h is a persistent
perturbation in the sense defined below.
Definition 3.7. A function h is a persistent perturbation if, for any ε > 0,
there are two positive constants, δ1 and δ2, such that
||h(xe, t)|| < δ1 , ∀ t ∈ [t0, ∞),
and
||xe(t0)|| < δ2
together implying ||xe(t)|| < Stabilities of Nonlinear Systems (II) 97
Definition 3.8. The zero fixed point of the unperturbed system (3.17) is
said to be totally stable, if the persistently perturbed system (3.18) remains
stable in the sense of Lyapunov.
As the next theorem states, all uniformly and asymptotically stable
systems with persistent perturbations are totally stable, namely, a stable
orbit starting from a neighborhood of another orbit will stay nearby.
Theorem 3.11 (Malkin Theorem). If the unperturbed system (3.17)
is uniformly and asymptotically stable about its zero fixed point, then it
is totally stable, namely, the persistently perturbed system (3.18) remains
stable in the sense of Lyapunov.
Proof. See [Hoppensteadt (2000)]: p. 104.
Next, consider an autonomous system with persistent perturbations:
x˙ = f(x) + h(x, t), x ∈ R
n
. (3.19)
Theorem 3.12 (Perturbed Orbital Stability Theorem). If φt(x0) is
an orbitally stable solution of the unperturbed autonomous system (3.19),
with h = 0 therein, then it is totally stable; that is, the perturbed system
remains orbitally stable under persistent perturbations.
Proof. See [Hoppensteadt (2000)]: p. 107.98 Nonlinear Systems
Exercises
3.1 Compare and comment on the stabilities of the following two systems:
x˙ = − a sin(x) and ˙x = − a

x −
x
3
3! + · · · ±
x
2n+1
(2n + 1)!
,
with constant a > 0.
3.2 Consider a linear time-varying system, x˙ = A(t) x, and let Φ(t, t0)
be its fundamental matrix. Show that this system is uniformly stable
in the sense of Lyapunov about its zero fixed point if and only if
||Φ(t, τ )|| ≤ c < ∞ for a constant c and for all t0 ≤ τ ≤ t < ∞.
3.3 Consider a perturbed linear time-invariant system, x˙ = [ A+∆A(t) ] x,
with ∆A(t) being continuous and satisfying R ∞
t0
||∆A(τ )|| dτ ≤ c < ∞.
Show that this perturbed system is stable in the sense of Lyapunov
about its zero fixed point.
3.4 ∗ Consider the following nonlinear nonautonomous system:
x˙ =

A + ∆A(t)

x + g(x, t),
which satisfies all conditions stated in Theorem 3.1. Assume, more￾over, that ∆A(t) is continuous with ∆A(t) → 0 as t → ∞. Show that
this system is asymptotically stable about its zero fixed point. [Hint:
Mimic the proof of Theorem 3.1 and Example 3.3.]
3.5 Determine the stability of the fixed point of the following system:
x˙ = −2 x + f(t) y
y˙ = g(t) x − 2 y ,
where |f(t)| ≤ 1/2 and |g(t)| ≤ 1/2 for all t ≥ 0.
3.6 For the following system:
x˙ = − sin(2t) x + (cos(2t) − 1) y
y˙ = (cos(2t) + 1) x + sin(2t) y ,
find its Floquet multipliers and Floquet numbers.
3.7 Let A(t) be a periodic matrix of period τ > 0, and let {λi} and {ρi} be
its Floquet multipliers and numbers, i = 1, . . . , n, respectively. Verify
that
Yn
i=1
λi = exp Z t+τ
t
trace [A(s)] ds
and
Xn
i=1
ρi =
1
τ
Z t+τ
t
trace [A(s)] ds .Stabilities of Nonlinear Systems (II) 99
3.8 Consider the following linear time-varying system with a periodic co￾efficient:
x˙ = (σ + cos(t)) x .
Verify that its Floquet multiplier is ρ = σ, and discuss the stability of
the system about its zero fixed point with respect to the value of this
multiplier.
3.9 Consider the predator-prey model
x˙ = − α x + β x2 − γ xy
y˙ = − δ x + ε xy ,
where all coefficients are positive constants. Discuss the stability of
this system about its zero fixed point.
3.10 Consider the Mathieu equation
x¨ +
￾
a + b cos(t)

x = 0 .
Discuss the stability of its orbit about the zero fixed point in terms of
the constants a and b.
3.11 A pendulum with mass m and weight-less string of length a hangs
from a support that is constrained to move with vertical and horizontal
displacements ζ(t) and η(t), respectively. It can be verified that the
motion equation of this pendulum is
a ¨θ + (g − ¨ζ) sin(θ) + ¨η cos(θ) = 0 .
Assume that ζ(t) = α sin(ωt) and η(t) = β sin(2ωt), where ω =
p
g/a.
Show that the linearized equation, for small amplitudes, has a solution
θ(t) = 8β
α
cos(ωt).
Discuss the stability of this solution.
3.12 Suppose that the Li´enard equation
x¨ + f(x) ˙x + g(x) = 0
has a tp-periodic solution, φt(x0). Show that if
Z tp
0
f(φτ (x0) dτ > 0 ,
then this periodic solution is orbitally stableThis page intentionally left blankChapter 4
Stabilities of Nonlinear Systems (III)
This chapter further studies the stabilities of nonlinear systems, introduc￾ing some analytic methods in the frequency domain, such as Lur’e systems
formulation, harmonic balance approximation technique, bounded-input
bounded-output stability concept and criteria, as well as the small gain
theorem and the contraction mapping principle. All these are commonly
used methodologies, very popular in engineering applications.
4.1 Lur’e Systems Formulated in the Frequency Domain
To motivate, consider a 1-dimensional nonlinear autonomous system,
(
x˙ = f(x), −∞ < t < ∞,
x(−∞) = 0 ,
where only causal signals are considered, so for simplicity the initial con￾dition can be replaced by x(0) = 0, with 0 ≤ t < ∞. Using the unit-step
function
g(t) = (
1 t ≥ 0 ,
0 t < 0 ,
the solution of this system can be written as
x(t) = Z t
−∞
f(x(τ )) dτ
=
Z ∞
−∞
g(t − τ ) f(x(τ )) dτ
= [g ∗ f(x)] (t) (convolution).
The system can be implemented in the time domain as shown in Fig. 4.1.
101102 Nonlinear Systems
g (t)
–f (·)
0 x = g ∗ f (x) +
–
Fig. 4.1 Configuration of the 1-dimensional system.
In this configuration, if an “exponential signal” of the form e
st is input
to the plant, represented by g(·), where s is a complex variable, then
g(t) ∗ e
st =
Z ∞
−∞
g(τ ) e
s(t−τ)
dτ
=
Z ∞
−∞
g(τ ) e
−sτ dτ
e
st
: = G(s) e
st
,
in which G(s) is actually the bilateral Laplace transform of g(t) and serves
as the transfer function of the linear plant represented by g(·). This is
illustrated in Fig. 4.2.
g(t)
g(t) ∗ est
G(s)
G(s) est est est
(a) in time domain (b) in frequency domain
(parameter: s) (parameter: t)
Fig. 4.2 Input–output relations of the plant g(·).
Now, to generalize, consider a higher-dimensional nonlinear autonomous
system,
(
x˙ = f(x), −∞ < t < ∞,
x(−∞) = 0 ,
where again only causal signals are considered, so the initial condition can
be replaced by x(0) = 0, with 0 ≤ t < ∞. Rewrite the system in the
following form of Lur’e system: (
x˙ = A x + B h(y)
y = C x ,
(4.1)Stabilities of Nonlinear Systems (III) 103
with initial conditions x(0) = 0 or y(0) = 0, where A, B, C are constant
matrices, in which A is chosen by the user but B and C are determined
from the given system (possibly, B = C = I), and h is a vector-valued
nonlinear function generated through the reformulation. One specific and
rather special example is
B = C = I , y = x , h = f − A ,
with A chosen to possess a special property such as stability for convenience.
Of course, there are other choices for the reformulation. The main purpose
of this reformulation is to have a linear part, Ax, for the resulting system
(4.1). In many applications, a given system may have already be given in
the Lur’e form.
After the above reformulation, taking the Laplace transform L{·} with
zero initial conditions on the Lur’e system (4.1) yields
sxb = Axb + BL

h(y)
	
,
or
xb = [sI − A]
−1BL

h(y)
	
.
where xb = L{x}. Consequently,
yb = C xb = C [sI − A]
−1BL

h(y)
	
:= G(s)L

h(y)
	
,
in which the system transfer matrix is
G(s) = C [sI − A]
−1B . (4.2)
An implementation of the Lur’e system (4.1) is shown in Fig. 4.3, where
both time-domain and frequency-domain notations are mixed for simplicity
of expressions.
The Lur’e system shown in Fig. 4.3 is a closed-loop configuration, where
the feedback loop is usually considered as a “controller.” Thus, this system
is sometimes written in the following equivalent control form:



x˙ = A x + B u
y = C x
u = h(y).
(4.3)104 Nonlinear Systems
–h (·)
0 u y +
– C [sI − A]
−1B
Fig. 4.3 Configuration of the Lur’e system.
4.2 Absolute Stability and Frequency-Domain Criteria
This section introduces the concept of absolute stability about the Lur’e
system and derives some frequency-domain criteria for this type of stability.
4.2.1 Background and Motivation
To provide some background information and motivation, consider a single￾input single-output (SISO) Lur’e system,



x˙ = A x + b u
y = c
⊤x
u = h(y).
(4.4)
Assume that h(0) = 0. Then, x
∗ = 0 is a fixed point of the system. The
transfer function of its linear part is given by G(s) = c
⊤[sI − A]
−1b. Let
Gr(j ω) = G(j ω) + G(−j ω) be the real part of G(j ω), and assume that
(i) G(s) is stable and satisfies Gr(j ω) ≥ 0;
(ii) u(t) = −g(x) y(t) with g(x) ≥ 0.
Based on condition (i), one can construct a rational function (as another
transfer function), R(s), and find a constant vector, r, such that
R(s)R(−s) = G(s) + G(−s) (4.5)
with
R(s) = r
⊤[sI − A]
−1b . (4.6)
This is known as spectral factorization in matrix theory.
Then, consider the Lyapunov equation
P A + A
⊤P = − r r⊤
with a positive definite constant matrix, P. For
P [sI − A] + [−sI − A
⊤] P = r r⊤ ,Stabilities of Nonlinear Systems (III) 105
multiplying [sI − A]
−1b to its right and b
⊤[−sI − A⊤]
−1
to its left yields
b
⊤P [−sI − A]
−1b + b
⊤[−sI − A
⊤]
−1P b = R(−s)R(s). (4.7)
Thus, one can identify c = P b by comparing (4.5) and (4.7).
To this end, the Lyapunov function V (x) = x
⊤Px yields
V˙ (x) = − x
⊤r r⊤x − 2 x
⊤c g(x) c
⊤x
= −
￾
r
⊤x
2
− 2
￾
c
⊤x
2
g(x) ≤ 0 .
This implies that system (4.4) is stable in the sense of Lyapunov about its
fixed point x = 0.
Note that this stability can be strengthened to be asymptotic if condi￾tions (i) and (ii) above are modified to be as follows:
(i)′ G(s) is stable and satisfies Gr(j ω) > 0;
(ii)′′ u(t) = −g(x) y(t) with g(x) > 0.
This is left to be further discussed below, where some even more conve￾nient stability criteria will be derived.
Now, observe that, in many cases, either condition (i) or (ii) may not
be satisfied by the given system (4.4). Nevertheless, if the function g(x)
satisfies the following weaker condition:
α ≤ g(x) < β for all x ∈ R
n
, (4.8)
for some constants α and β, then one may try to change the variables as
follows:
u = β v − α z and y = z − v ,
so that
v(t) = − g˜(x) z(t)
where
g˜(x) = g(x) − α
β − g(x)
≥ 0 ,
which satisfies condition (ii). In this case, the new transfer function becomes
Ge(s) = 1 + β G(s)
1 + α G(s)
.
If this new transfer function satisfies condition (i), then all of the above
analysis and results remain valid. This motivates the study of the stability
problem under condition (4.8) below in the rest of the sectio106 Nonlinear Systems
4.2.2 SISO Lur’e Systems
First, single-input single-output (SISO) Lur’e systems of the form (4.4) are
discussed, namely,



x˙ = A x + b u
y = c
⊤x
u = h(y).
(4.9)
Assume that h(0) = 0, so x
∗ = 0 is a fixed point of the system.
In light of condition (4.8), the following sector condition is imposed.
Sector Condition: The Lur’e system (4.9) is said to satisfy the local
(global) sector condition on the nonlinear function h(·), if there exist two
constants, α ≤ β, such that
(i) local sector condition:
α y2
(t) ≤ y(t) h(y(t)) ≤ β y2
(t), (4.10)
for all −∞ < ym ≤ y(t) ≤ yM < ∞ and t ∈ [t0, ∞);
(ii) global sector condition:
α y2
(t) ≤ y(t) h(y(t)) ≤ β y2
(t), (4.11)
for all −∞ < y(t) < ∞ and t ∈ [t0, ∞).
Here, [α, β] is called a sector for the nonlinear function h(·). Moreover,
system (4.9) is said to be absolutely stable within the sector [α, β] if the
system is globally asymptotically stable about its fixed point x
∗ = 0 for
any nonlinear function h(·) satisfying the global sector condition (ii).
The above local and global sector conditions are visualized in Figs. 4.4(a)
and 4.4(b), respectively. It is easy to see that if a system satisfies the global
sector condition then it also satisfies the local sector condition, but the
converse may not be true.
As a historical remark, Aizerman made a conjecture in 1940s that if
the Lur’e system is stable about its zero fixed point for all linear system
approximations with corresponding constant slope κ satisfying α ≤ κ ≤ β
as shown in Fig. 4.4, then the original nonlinear Lur’e system would be
stable. In 1957, Kalman modified this to be an even stronger conjecture
that if the system is stable for all linear system approximations with α0 ≤
dh(y)/dy ≤ β0 for some α0 ≤ α and β0 ≤ β, then the original nonlinear
system would be stable. It is now known that they both are false becauseStabilities of Nonlinear Systems (III) 107
0 b y 0 y
a
h(y)
h(y)
α y
β y
α y
β y
(a) (b)
Fig. 4.4 Local and global sector conditions.
some counterexamples were found lately. One simple counterexample is
given in Exercise 4.4 (see [Hahn (1967)] for another counterexample).
The following simple example shows, in some specific cases, how to
determine a sector for a given system.
Example 4.1. The linear system



x˙ = a x + b u
y = c x
u = h(y) = y
satisfies the global sector condition with α = β = 1.
In this system, if the controller is changed to be
u = h(y) = 2 y
2
,
then the system becomes nonlinear but satisfies
−2 y
2
(t) ≤ y(t) · 2 y
2
(t) ≤ 2 y
2
(t)
for all −1 ≤ y(t) ≤ 1 and t ∈ [t0, ∞). In this case, the system satisfies
the local sector condition with a = −1 and b = 1, over the sector [α, β] =
[−2, 2].
Theorem 4.1 (Popov Criterion). Suppose that the SISO Lur’e system
(4.9) satisfies the following conditions:
(i) A is stable and {A, b} is controllable;
(ii) the system satisfies the global sector condition with α = 0 therein;
(iii) for any ε > 0, there is a constant γ > 0 such that
Re 
(1 + j γ ω) G(j ω)
	
+
1
β
≥ ε for all ω ≥ 0 , (4.12)108 Nonlinear Systems
where G(s) is the transfer function defined by (4.2), and Re{·} denotes the
real part of a complex number (or function). Then, the system is globally
asymptotically stable about its fixed point x
∗ = 0 within the sector.
Proof. See [Khalil (1996)]: pp. 419–421.
The Popov criterion has the following geometric meaning. Separate the
complex function G(s) into its real and imaginary parts:
G(j ω) = Gr(ω) + j Gi(ω),
where
Gr(ω) = 1
2

G(j ω) + G(−j ω)

,
Gi(ω) = 1
2j

G(j ω) − G(−j ω)

.
Then, rewrite condition (iii) as
1
β
> − Gr(ω) + γ ω Gi(ω) for all ω ≥ 0 .
Thus, the graphical situation of the Popov criterion as shown in Fig. 4.5
implies the global asymptotic stability of the system about its zero fixed
point.
Re
H(jω)
slope 1
γ
– 1
β
0
Im
Fig. 4.5 Geometric meaning of the Popov criterion.
Example 4.2. Consider again the linear system discussed in Example 4.1.
This 1-dimensional system is controllable as long as b ̸= 0. It is also easy to
verify that the system satisfies the global sector condition, even with α = 0.
Finally, for any ε > 0, if the constant γ is chosen such that
γ >
ε
￾
a
2
b
2
c
2 + ω
2

− abc − a
2
b
2
c
2 − ω
2
ω2
Stabilities of Nonlinear Systems (III) 109
then some simple calculation shows that
Re 
1 + j γ ω
j ω − (a + bc)

+ 1 > ε for all ω ≥ 0 .
Therefore, this linear feedback control system is globally asymptotically
stable about its zero fixed point. This is consistent with the familiar linear
analysis.
Example 4.3. Consider a Lur’e system with linearity described by the
transfer function G(s) = 1/[s(s + a)
2
] and nonlinearity described by the
relay function h(·) shown in Fig. 4.6, where a and 0 < b < 1 are constants.
Clearly,
Gr(j ω) = −2a
￾
a
2 + ω2
2
and Gi(j ω) = −ω
￾
a
2 − ω
2
2
￾
a
2 + ω2
2
,
and the Popov criterion requires
1
β
>
2a
￾
a
2 + ω2
2 −
γω2
￾
a
2 − ω
2
2
￾
a
2 + ω2
2
for all ω ≥ 0, which leads to the choice of a small γ > 0 with
α = 0 and β = a
3
/2 .
b –b
h
0
1
–1
Fig. 4.6 The relay nonlinearity.
The Popov criterion has a natural connection to the linear Nyquist
criterion. A more direct generalization of the Nyquist criterion to nonlinear
systems is the following.
Theorem 4.2 (Circle Criterion). Suppose that the SISO Lur’e system
(4.9) satisfies the following conditions:
(i) matrix A has no purely imaginary eigenvalues, but has κ eigenvalues
with positive real 110 Nonlinear Systems
(ii) the system satisfies the global sector condition;
(iii) one of the following situations holds:
(a) 0 < α < β: the Nyquist plot of G(j ω) encircles the disk D
￾
−
1
α
, −
1
β

counterclockwise κ times but does not enter it;
(b) 0 = α < β: the Nyquist plot of G(j ω) stays within the open half-plane
Re{s} > −
1
β
;
(c) α < 0 < β: the Nyquist plot of G(j ω) stays within the open disk
D
￾
−
1
β
, −
1
α

;
(d) α < β < 0: the Nyquist plot of − G(j ω) encircles the disk D
￾
1
α
,
1
β

counterclockwise κ times but does not enter it.
Then, the system is globally asymptotically stable about its fixed point
x
∗ = 0.
Here, the disk D
￾
−
1
α
, −
1
β

, for the case of 0 < α < β, can be visualized
in Fig. 4.7.
– 1
β – 1
α
0
Fig. 4.7 The disk D
￾
− 1
α
, − 1
β

.
Example 4.4. Consider the Lur’e system



x˙ = a x + h(u)
y = b x + h(u)
u = k1x + k2y ,
where a < 0, b < 0, k1 and k2 are constants, and the nonlinear function
h(·) satisfies the global sector condition with α = 0 and β = ∞.
It follows from a direct calculation that
Re 
(1 + j γ ω) G(j ω)
	
+
1
β
=
(ak2 + bk2) (ab − ω
2
) + (k1 + k2)ω
2
(a + b)
(ab − ω2)
2 + ω2(a + b)Stabilities of Nonlinear Systems (III) 111
Thus, the circle criterion (4.12) is
(ak2 + bk2) ab − ω
2

(ak1 + bk2) + (k1 + k2) (a + b)

> 0
for all ω ≥ 0, which is equivalent to
ak2 + bk2 > 0 and (ak1 + bk2) + (k1 + k2) (a + b) > 0 .
4.2.3 MIMO Lur’e Systems
Consider a multi-input multi-output (MIMO) Lur’e system, as shown in
Fig. 4.3, of the form



x(s) = G(s) u(s)
y = C x
u(t) = − h
￾
y(t)

,
(4.13)
where G(s), as well as h and C, are defined in (4.2). If the system satisfies
the following Popov inequality:
Z t1
t0
y
⊤(τ )x(τ ) dτ ≥ − γ for all t1 ≥ t0 (4.14)
for a constant γ ≥ 0 independent of t, then it is said to be hyperstable.
The linear part of this MIMO system is described by the transfer matrix
G(s), which is said to be positive real if
(i) G(s) has no poles located inside the open half-plane Re{s} > 0;
(ii) poles of G(s) on the imaginary axis are simple, and the residues of
its entries (as ω → ∞) form a semi-positive definite and symmetric
matrix;
(iii) the matrix Gr(j ω) := 1
2

G(j ω)+G⊤(−j ω)

is a semi-positive definite
and symmetric matrix for all real values of ω that are not poles of G(s).
Example 4.5. The transfer matrix
G(s) = 
s/(s + 1) −1/(s + 2)
1/(s + 2) 2/(s + 1) 
is positive real, because (i) all its poles are located on the left-half plane,
(ii) the matrix formed by the residues,
Gr(j ∞) = 
1 0
0 0 
,
is semi-positive definite, and (iii) the matrix
Gr(j ω) = 
ω
2/(1 + ω
2
) j ω/(4 + ω
2
)
− j ω/(4 + ω
2
) 2/(1 + ω
2
)

is positive definite for all ω ≥ 0 (its smallest eigenvalue approaches 2 as
ω → ∞)112 Nonlinear Systems
The matrix shown in this example is sometimes referred to as strictly
positive real, which satisfies stronger conditions: (i)′ has no pole on the
imaginary axis, (ii)′
the residues of its entries form a nonzero, semi-positive
definite and symmetric matrix, and (iii)′
the matrix G(j ω) + G⊤(−j ω) is
positive definite and symmetric for all ω ≥ 0.
Theorem 4.3 (Hyperstability Theorem). The MIMO Lur’e system
(4.13) is hyperstable if and only if its transfer matrix G(s) is positive real.
Moreover, it is asymptotically hyperstable if and only if its transfer matrix
is strictly positive real.
Proof. See [Anderson and Vongpanitterd (1972)]; [Popov (1973)].
4.3 Harmonic Balance Approximation and Describing
Function
Consider again the SISO Lur’e systems (4.9), namely,



x˙ = A x + b u
y = c
⊤x
u = h(y),
(4.15)
where h(0) = 0, so x
∗ = 0 is a fixed point of the system. Assume that the
matrix A is invertible.
The interest here is the existence of periodic orbits (e.g. limit cycles)
in the system output y(t). Presumably, the system output has a periodic
orbit, which is expressed in the Fourier series form:
y(t) = X∞
k=−∞
ak e
j k ωt
,
where all ak = ¯ak and ω are to be determined.
Observe that, if y(t) is periodic, since h(·) is a time-invariant function,
h(y(t)) is also periodic with the same period ω. Therefore,
u(t) = h(y(t)) = X∞
k=−∞
ck e
j k ωt
,
where all ck = ck(a0, a±1, · · ·) are to be determined.
Observe also that in the frequency s-domain the input–output relation
of the linear plant is
Y (s) = G(s)U(s),Stabilities of Nonlinear Systems (III) 113
where the transfer matrix
G(s) = c
⊤

sI − A
−1
b :=
n(s)
d(s)
is equivalent to a differential equation of constant coefficients with zero
initial conditions:
d(p) y(t) = n(p) u(t), p := d/dt .
Since
p ej k ωt =
d
dt e
j k ωt = j k ω ej k ωt
,
one has
d(p) y(t) = X∞
k=−∞
d(j k ω) ak e
j k ωt
and
n(p) u(t) = X∞
k=−∞
n(j k ω) ck e
j k ωt
.
Consequently, the above differential equation yields
X∞
k=−∞
h
d(j k ω) ak − n(j k ω) ck
i
e
j k ωt = 0 .
It then follows from the orthogonality of the Fourier basis functions that
d(j k ω) ak − n(j k ω) ck = 0 , k = 0, ±1, ±2, . . . ,
or
G(j k ω) ck − ak = 0 , k = 0, ±1, ±2, . . . .
Here, for k = 0, G(0) is well defined since A is invertible by assumption.
Observe, furthermore, that since G(j k ω) = G(−j k ω), ak = ¯ak, and
ck = ¯ck, for all k = 0, ±1, ±2, . . . , it suffices to consider
G(j k ω) ck − ak = 0 , k = 0, 1, 2, . . . .
However, this infinite-dimensional system of nonlinear algebraic equations
is very different if not impossible to solve for the unknown ω and {ak},
while {ck} are functions of {ak}. One thus resorts to applying some kind
of approximations.
Since G(s) is strictly proper, it satisfies

 G(j k ω)

 → 0 (k → ∞),114 Nonlinear Systems
namely, G(k j ω) ≈ 0 for large values of k. So, it is reasonable to truncate
the above infinite-dimensional system. The first-order approximation, with
k = 0 and 1, gives
(
G(0) bc0 − ba0 = 0
G(j ω) bc1 − ba1 = 0 ,
where the first equation is real and the second is complex, with bci ≈ ci and
bai ≈ ai
, i = 0, 1. Because bc0 and bc1 are both functions of ba0 and ba1, the
above system of two equations has only two real unknowns, ω and ba0, and
one complex unknown, ba1. Clearly, to solve for four real unknowns from
three real equations, one more constraint is needed. It turns out that if one
only considers the following special cases of the nonlinear feedback system,
then some useful results can be obtained: Assume that
(i) the nonlinear function h(·) is odd: h(−y) = − h(y), and is time￾invariant;
(ii) if y = α sin(ωt), α ̸= 0, then the first-order harmonic of − h(y) domi￾nates the other harmonic components.
Here, assumption (i) implies that bc0 = 0, so that the first real equation
yields ba0 = 0 as well. Assumption (ii) implies that
α sin(ωt) = α
2j

e
j ωt − e
−j ωt 
,
which gives ba1 = α/(2j). Thus, the second equation above becomes
G(j ω) bc1(0, α/(2j)) − α/(2j) = 0 ,
where, by the Fourier series coefficient formulas,
bc1(0, α/(2j)) = ω
2π
Z 2π/ω
0
− h(α(ωt)) e
−j ωtdt
= j
ω
π
Z π/ω
0
h(α(ωt)) sin(ωt) dt .
Define
Ψ(α) := bc1(0, α/(2j))
α/(2j)
= −
2ω
π α Z π/ω
0
h(α sin(ωt)) sin(ωt) dt , (4.16)
which is called the describing function of the odd nonlinearity −h(·). Then,
one obtains the first-order harmonic balance equation:
G(j ω) Ψ(α) + 1 = 0 . (4.17)Stabilities of Nonlinear Systems (III) 115
If this complex equation is solvable, then one may first solve
(
Gr(j ω) Ψ(α) + 1 = 0
Gi(j ω) = 0 ,
for the two real unknowns ω and α, where Gr and Gi are the real and imag￾inary parts of G, respectively. This yields all the other expected results:
ba1 = α/(2j), bc1 = j
ω
π
Z π/ω
0
h(α(ωt)) sin(ωt) dt , ba0 = 0 , bc0 = 0 .
Consequently,
yb(t) ≈ ba−1e
−j ωt + ba1e
j ωt =
j α
2
e
−j ωt −
j α
2
e
j ωt
is the first-order approximation of a possible periodic orbit of the system
output, which is generated by an input signal of the form
ub(t) = h(yb(t)) ≈ bc−1e
−j ωt + bc1e
j ωt
.
In summary, one arrives at the following conclusion, which is helpful for
predicting possible periodic orbits (limit cycles) of the system output.
Theorem 4.4. Consider the SISO Lur’e system



x˙ = Ax + b u
y = c
⊤x
u = h(y).
Assume that the nonlinear function h(·) is odd and time-invariant, with the
property that for y(t) = α sin(ωt), α ̸= 0, only the first-order harmonic of
− h(y) is significant. Define the describing function of − h(·) by
Ψ(α) = −
2ω
π α Z π/ω
0
h(α sin(ωt)) sin(ωt) dt .
If the first-order harmonic balance equation
(
Gr(j ω) Ψ(α) + 1 = 0
Gi(j ω) = 0
has solutions ω and α, then
yb
⟨1⟩
(t) = j α
2
e
−j ωt −
j α
2
e
j ωt
is the first-order approximation of a possible periodic orbit of the system
output. If this harmonic balance equation does not have solutions, then the
system will not likely output any periodic orbits.116 Nonlinear Systems
Example 4.6. Consider an SISO system with
G(s) = 1
s(s + 1) (s + 2)
and
(a) as shown in Fig. 4.8(a):
− h(y) = sgn (y) =



1 y > 0
0 y = 0
−1 y < 0 ;
(b) as shown in Fig. 4.8(b):
− h(y) = sat (y) =



−1 y < −1
y −1 ≤ y ≤ 1
1 1 < y .
1 1
–1 –1
–1 1
sgn (y) sat (y
y
)
y
(a) (b)
Fig. 4.8 Two different functions of − h(y).
First, it can be verified that
G(j ω) = 1
j ω (j ω + 1) (j ω + 2) =
−3ω − j (2 − ω
2
)
9ω2 + ω(2 − ω2)
2
,
so that the second harmonic balance equation becomes
Gi(j ω) = − (2 − ω
2
)
9ω3 + ω(2 − ω2 )
2
= 0 ,
which has two roots: ω1 =
√
2 and ω2 = −
√
2 (this one is ignored due to
symmetry).
(a) The describing function in this case with a signum function is
Ψ(α) = 2
π α Z π
0
− h(α sin θ) sin θ dθ =
4
π α
(θ = ωt).Stabilities of Nonlinear Systems (III) 117
Hence, the first harmonic balance equation at ω =
√
2 becomes
−3
√
2
9
￾√
2
3
+
√
2

2 − (
√
2 )2
2
4
π α
+ 1 = 0 ,
which yields α = 2/(3π). The conclusion is that there is possibly a periodic
orbit of the form
yb
⟨1⟩
(t) = j α
2
e
−j ωt −
j α
2
e
j ωt =
j
3π
e
−j
√
2t −
j
3π
e
j
√
2t
,
which has amplitude 2/(3π) and frequency √
2.
(b) The describing function in this case with a saturation function is
Ψ(α) = 2
π α Z π
0
− h(α sin θ) sin θ dθ ≤ 1 for all α ,
so that the first harmonic balance equation at ω =
√
2 becomes
−3
√
2
9
￾√
2
3
+
√
2

2 − (
√
2)2
2 Ψ(α) + 1 = 0 , Ψ(α) ≤ 1 ,
which has no solution for all real (unknown) α. The conclusion is that there
does not likely exist any periodic orbit in the system output. To make sure
this is the case, usually higher-order harmonic balance approximations are
needed, so as to obtain more accurate predictions (see [Moiola and Chen
(1996)]).
When solving the equation Gr(j ω)Ψ(α) = 1 graphically, one can sketch
two curves on the complex plane, Gr(j ω) and − 1/Ψ(α), and then gradually
increase ω and α respectively to find their crossing points:
(i) if the two curves are (visually) tangent, as illustrated in Fig. 4.9(a),
then a conclusion drawn from the describing function method will not
be satisfactory in general;
(ii) if the two curves are (visually) transversal, as illustrated in Fig. 4.9(b),
then a conclusion drawn from the describing function analysis will gen￾erally be reliable.
Theorem 4.5 (Graphical Stability Criterion for Periodic Orbits).
Each intersection point of the two curves, Gr(j ω) and − 1/Ψ(α), corre￾sponds to a periodic orbit, yb
⟨1⟩
(t), in the output of system (4.15). If the
points, near the intersection and on one side of the curve − 1/Ψ(α) where
− α is increasing, are not encircled by the curve Gr(j ω), then the corre￾sponding periodic output is stable; otherwise, it is unstabl118 Nonlinear Systems
Im Im
0 Re
ω ω
0 Re
G Gr(jω) (α) r(jω
α α
Ψ ) Ψ(α)
– 1 – 1
(a) (b)
Fig. 4.9 Graphical describing function analysis.
Proof. See [Moiola and Chen (1996)]: pp. 25–26.
A word of warning is that the describing method discussed in this section
is a graphical method, which is based on numerical computation and human
visual judgement; therefore, although convenient and mostly successful in
practice, there is a chance that it leads to incorrect conclusions about the
system stability. A counterexample is given in Exercise 4.4.
4.4 BIBO Stability
A relatively simple and also relatively weak, but very practical type of
stability is discussed in this section.
This is the bounded-input bounded-output (BIBO) stability, which refers
to the property of a system that any bounded input to the system produces
a bounded output through the system.
Consider an input–output map and its configuration as shown in
Fig. 4.10(a).
Definition 1. The system S is said to be BIBO stable from the input
set U to the output set Y if, for each admissible input u ∈ U and the
corresponding output y ∈ Y , there exist two non-negative constants, bi
and bo, such that
||u||U ≤ bi =⇒ ||y||Y ≤ bo . (4.18)
Note that since all norms are equivalent in a finite-dimensional vec￾tor space, it is unnecessary to distinguish under what kind of norms for
the input and output signals the BIBO stability is defined and measured.
Moreover, it is important to note that, in the above definition, even if biStabilities of Nonlinear Systems (III) 119
is small and bo is large, the system is still considered to be BIBO stable,
which is usually good enough for many practical applications.
+
+
+
–
S
u y
S1
S2 e2
u2
u1 y1
y2
e1
(a) open-loop (b) closed-loop
Fig. 4.10 Input–output relations.
4.4.1 Small Gain Theorem
A convenient criterion for verifying the BIBO stability of a closed-loop
control system is the small gain theorem, which applies to most systems
(linear and nonlinear, continuous-time and discrete-time, deterministic and
stochastic, time-delayed, of any dimensions) in various forms, as long as
the mathematical setting is appropriately formulated to meet the condi￾tions of the theorem. The main disadvantage of this criterion is its over￾conservativity, in general.
Consider the typical closed-loop system shown in Fig. 4.10(b), where
the inputs, outputs, and internal signals are related as follows:
(
S1(e1) = e2 − u2
S2(e2) = u1 − e1 .
(4.19)
First, it is important to note that the individual BIBO stability of S1 and
S2 is not sufficient to guarantee the BIBO stability of the connected closed￾loop system. For instance, in the discrete-time setting of Fig. 4.10(b),
suppose that S1 ≡ 1 and S2 ≡ −1, with u1(k) ≡ 1 for all k = 0, 1, . . ..
Then, S1 and S2 are BIBO stable individually, but it can be easily verified
that y1(k) = k → ∞ as the discrete-time variable k evolves. Therefore, a
stronger condition restricting the interaction of S1 and S2 is necessary.
Theorem 4.6 (Small Gain Theorem). If there exists four constants,
L1, L2, M1, M2, with L1L2 < 1, such that
(
||S1(e1)|| ≤ M1 + L1||e1||
||S2(e2)|| ≤ M2 + L2||e2|| ,
(4.20)120 Nonlinear Systems
then



||e1|| ≤ (1 − L1L2)
−1

||u1|| + L2||u2|| + M2 + L2M1

||e2|| ≤ (1 − L1L2)
−1

||u2|| + L1||u1|| + M1 + L1M2

,
(4.21)
where the norms ||·|| are defined over the spaces to which the signals belong.
Consequently, (4.20) and (4.21) together imply that if the system inputs (u1
and u2) are bounded then the corresponding outputs (S1(e1) and S2(e2))
are bounded.
Note that the four constants, L1, L2, M1, M2, can be somewhat arbitrary
(e.g. either L1 or L2 can be large, and some of them can even be negative).
As long as L1L2 < 1, the BIBO stability conclusion follows. This inequality
is the key condition for the theorem to hold, which is required by the
inversion (1 − L1L2)
−1
in the bounds (4.21).
Proof. It follows from (4.19) and (4.20) that
||e1|| ≤ ||u1|| + ||S2(e2)|| ≤ ||u1|| + M2 + L2||e2|| .
Similarly, one has
||e2|| ≤ ||u2|| + ||S1(e1)|| ≤ ||u2|| + M1 + L1||e1|| .
By combining these two inequalities, one obtains
||e1|| ≤ L1L2||e1|| + ||u1|| + L2||u2|| + M2 + L2M1 ,
which, on the basis of L1L2 < 1, yields
||e1|| ≤ (1 − L1L2)
−1
￾
||u1|| + L2||u2|| + M2 + L2M1

.
The other inequality can be similarly verified.
In the special case where the input–output spaces, U and Y , are both
in the L2-space, a similar criterion based on the system passivity property
can be obtained. In this case, an inner product between two vectors in the
space will be useful, which is defined by
⟨ξ, η⟩ =
Z ∞
t0
ξ
⊤(τ )η(τ )dτ .
Theorem 4.7 (Passivity Stability Theorem). If there are four con￾stants, L1, L2, M1, M2, with L1 + L2 > 0, such that
( 

e1, S1(e1)

≥ L1||e1||2 + M1


e2, S2(e2)

≥ L2||S2(e2)||2 + M2 ,
(4.22)
then the closed-loop system (4.19) is BIBO stableStabilities of Nonlinear Systems (III) 121
Proof. See [Sastry (1999)]: pp. 155–156.
As mentioned above, the main disadvantage of this criterion is its over￾conservativeness in providing the sufficient conditions for the BIBO stabil￾ity. One resolution is to transform the system into the Lur’e configuration,
and then apply the circle or Popov criterion under the sector condition, if
it can be satisfied, which may lead to less-conservative stability conditions,
in general.
4.4.2 Relation between BIBO and Lyapunov Stabilities
There is a closed relationship between the BIBO stability of a nonlinear
feedback system and the Lyapunov stability of a related nonlinear control
system [Desoer and Vidyasagar (1975)].
Consider a nonlinear system in the form of
x˙ = A x − f(x, t), x(t0) = x0 , (4.23)
where f : Rn × R1 → Rn is a real vector-valued integrable nonlinear func￾tion, satisfying f(0, t) = 0 for all t ∈ [0, ∞) (so the system has a zero
fixed point). Assume also that the system matrix A is stable (i.e. has all
eigenvalues with negative real parts).
By adding and then subtracting the term Ax, a general nonlinear system
can always be written in this form.
Now, define
(
x(t) = u(t) −
R t
t0
e
(t−τ)A y(τ )dτ
y(t) = f(x, t),
(4.24)
with u(t) = x0e
tA. Then, system (4.23) can be implemented by a feed￾back configuration as depicted in Fig. 4.11, with error signal e = x, plant
P(·) (t) = f(·, t), and a compensator C(·) (t) = R t
t0
e
(t−τ)A(·) (τ )dτ .
+
– P
C
u y e
U = Lp V = Lp
Fig. 4.11 A nonlinear feedback system.122 Nonlinear Systems
Consider the nonlinear system (4.24) with the feedback configuration
shown in Fig. 4.11.
Theorem 4.8. Suppose that U = V = Lp([0, ∞), Rn), with 1 < p < ∞.
Then, if the feedback system shown in Fig. 4.11 is BIBO stable from U to
V , the nonlinear system (4.23) is globally asymptotically stable above the
zero fixed point, i.e. ||x(t)|| → 0 as t → ∞.
Proof. Since all eigenvalues of the constant matrix A have negative real
parts, one has



 x0e
tA 


 ≤ M e−αt
for some 0 < α, M < ∞ for all t ∈ [0, ∞), so that ||u(t)|| = ||x0e
tA|| → 0
as t → ∞. Hence, in view of the first equation of (4.24), if it can be proven
that
v(t) := Z t
0
e
(t−τ)A y(τ ) dτ → 0 (t → ∞)
in the Euclidean norm, then it will follow that
||x(t)|| = ||u(t) − v(t)|| → 0 (t → ∞).
To prove so, write
v(t) = Z t/2
0
e
(t−τ)Ay(τ )dτ +
Z t
t/2
e
(t−τ)A y(τ ) dτ
=
Z t
t/2
e
τA y(t − τ ) dτ +
Z t
t/2
e
(t−τ)A y(τ ) dτ .
Then, by the H¨older inequality, one has
||v(t)|| ≤








Z t
t/2
e
τAy(t − τ )dτ








+








Z t
t/2
e
(t−τ)Ay(τ )dτ








≤
Z t
t/2

e
τA

q
dτ1/qZ t
t/2
|y(t − τ )|
p
dτ1/p
+
Z t
t/2



e
(t−τ)A




q
dτ1/qZ t
t/2
|y(τ )|
p
dτ1/p
≤
Z ∞
t/2



e
τA

q
dτ1/qZ ∞
0
|y(τ )|
p
dτ1/p
+
Z ∞
0



e
τA



q
dτ1/qZ ∞
t/2
|y(τ )|
p
dτ1/p
.Stabilities of Nonlinear Systems (III) 123
Since all eigenvalues of A have negative real parts and since the feedback
system is BIBO stable from U to V , so that y ∈ V = Lp([0, ∞), Rn), one
has
limt→∞ Z ∞
t/2

e
τA

q
dτ = 0
and
limt→∞ Z ∞
t/2
|y(τ )|
p
dτ = 0 .
Consequently, it follows that ||v(t)|| → 0 as t → ∞.
4.4.3 Contraction Mapping Theorem
The small gain theorem discussed above by nature is a kind of contraction
mapping theorem. The contraction mapping theorem can be used to deter￾mine the BIBO stability property of a system described by a map in various
forms, provided that the system (or the map) is appropriately formulated.
The following is a typical (global) contraction mapping theorem.
Define the operator norm of the input–output map S by
||S|| := sup
x1̸=x2
||S(x2) − S(x1)||
||x2 − x1|| .
Theorem 4.9 (Contraction Mapping Theorem). If the operator norm
of the input–output map S satisfies ||S|| < 1, then the system equation
y(t) = S(y(t)) + c
has a unique solution for any constant vector c ∈ Rn. This solution satisfies
||y||∞ ≤
￾
1 − ||S|| −1
||c|| .
Moreover, the sequence of the iterations
yk+1 = S(yk), y0 ∈ R
n
, k = 0, 1, . . . ,
satisfies
||yk|| → 0 as k → ∞.
Proof. In the continuous-time setting, it is clear that
||y|| ≤ ||S(y)|| + ||c|| ≤ ||S|| · ||y|| + ||c|| ,
so that
￾
1 − ||S|| 
||y|| ≤ ||c||124 Nonlinear Systems
Since ||S|| < 1, one has
||y|| ≤ ￾
1 − ||S|| 
||c|| for all t ≥ 0 .
Therefore, taking the supremum over t ∈ [0, ∞) on both sides yields the
expected result immediately.
In the discrete-time case, let y0 ∈ Rn be arbitrarily given, so ||y0|| < ∞.
By iterations, since ||S|| < 1, one obtains
||yk|| = ||S(yk−1)|| = · · · = ||S
k
(y0)|| ≤ ||S||k
||y0|| → 0 (k → ∞)Stabilities of Nonlinear Systems (III) 125
Exercises
4.1 The following is a model of a perfectly mixed chemical reactor with
a cooling coil, in which two first-order, consecutive, irreversible, and
exothermic reactions A → B → C occur:



x˙ = − x + a(1 − x) e
z
y˙ = − y + a(1 − x) e
z − acy ez
z˙ = − (1 + β) z + ab(1 − x) e
z + abcα y ez
,
where all coefficients are constants with certain physical meanings.
Reformulate this model in the Lur’e form, namely, assuming b = 0
and c = 1, find A, h(·), and G(s).
4.2 Consider the following autonomous system:
x¨ + 2 ˙x + f(x) = 0 ,
where f(·) is a nonlinear differentiable function. Find the bounds a
and b for ax ≤ f(x) ≤ bx, such that the zero fixed point of the system
is asymptotically stable.
4.3 Consider the following system:



x˙ 1 = x2
x˙ 2 = − 2 x2 + h(y)
y = x1 ,
where h(·) is a nonlinear function belonging to a sector [0, κ] for some
κ > 0. Reformulate this system into the Lur’e form and then use
either Popov or circle criterion to discuss its absolute stability.
4.4 Verify that the following SISO Lur’e system is a counterexample to
both the Aizerman–Kalman conjecture and the describing function
method:
A =

0 1
−1 −1

, b =

0
−1

, c
⊤ = [−1 − 1] ,
and
h(y) =




1 −
e
−2
1+e−1

y |y| < 1

1 −
e
−2|y|
|y| (1+e−|y|)

y |y| ≥ 1 .
[Hint: see [Narendra and Taylor (1973)]: pp. 69–72.]126 Nonlinear Systems
4.5 Show that the transfer function
G(s) = ω
2
n
(s + a)
s
2 + 2ζωns + ω2
n
,
where ωn > 0 and 0 < ζ < 1, is strictly positive real if and only if
0 < a < 2ζωn.
4.6 Use the circle criterion to discuss the absolute stability of the following
system:
(
x˙ = − x − h(x + y)
y˙ = x − y − 2h(x + y),
where h(·) is a nonlinear differentiable function given by
− h(y) =



−a y < −b < 0
0 −b ≤ y ≤ b
a b < y .
4.7 Consider the following autonomous nonlinear system:



x˙ 1 = − a x1 + x2 − h(x1)
x˙ 2 = − x1 + x3
x˙ 3 = − a x1 + b h(x1)
y = x1 ,
where a > 0, b > 0, and h(·) is a piecewise continuous function satis￾fying h(0) = 0 and 0 < h(z) < κ z. Reformulate this system into the
Lur’e form and then apply the Popov or circle criterion to discuss the
absolute stability of the system. Is there any condition on the sector
upper bound κ? [Hint: Discuss three cases: (a) b < a2
, (b) b = a
2
,
and (c) b > a2
, respectively.]
4.8 Find the describing function for each of the following odd nonlinear
functions:
h(y) = y
5
, h(y) = y
3
|y| , h(y) = sin(y).
4.9 Consider the piecewise-linear function shown in Fig. 4.12, which con￾tains the two nonlinear functions of Fig. 4.8 as special cases. Verify
that
(a) when |α| ≤ δ, the describing function Ψ(α) = s1;
(b) when α > δ,
Ψ(α) = 2 (s1 − s2)
π
h
sin−1
(δ/α) + δα−1p
1 − (δ/α)
2
i
+ s2 ;Stabilities of Nonlinear Systems (III) 127
slope s2
slope s1
slope s2
z
h
–δ 0 δ
Fig. 4.12 A piecewise-linear function, − h(y).
(c) with s1 = 1, s2 = 0, and δ = 1, the above result reduces to that
obtained in Example 4.6 for case (b).
4.10 Consider the nonlinear feedback system shown in Fig. 4.10(b), with
S1 : e1 →
e
−t
2
sin(e1) and S2 : e2 →
e
−t
2
+ sin(e2).
Determine the upper bounds for ||e1|| and ||e2||, respectively, using
the small gain theorem.
4.11 Consider a unity feedback system, as shown in Fig. 4.10(b), with
S1 = P representing a given nonlinear plant and S2 = I (the identity
mapping). Suppose that the plant has a perturbation, so P becomes
P +∆. Derive the small gain theorem corresponding to this perturbed
closed-loop system and find the norm condition on the perturbation
term, namely, find the smallest possible upper bound on ||∆|| in terms
of the bounds given in the small gain theorem just derived, such that
under this condition the perturbed system remains BIBO stable.This page intentionally left blankChapter 5
Nonlinear Dynamics: Bifurcations
and Chaos
Nonlinear systems have complex dynamics, especially bifurcations and
chaos, in addition to periodic oscillations such as limit cycles, as briefly
introduced in Sec. 1.4.
This chapter further studies some typical nonlinear dynamics, partic￾ularly bifurcations and chaos. Basic notions of period-doubling bifurca￾tion and Hopf bifurcation are investigated, and strange attractors and
chaos are discussed. Some typical chaotic systems are analyzed, mainly
on continuous-time models but also briefly in the discrete-time setting.
Throughout the course, some useful mathematical analysis tools such as
normal form and Poincar´e map are introduced.
5.1 Typical Bifurcations
Consider a nonlinear autonomous system with a real variable parameter:
x˙ = f(x; µ), x ∈ R
n
, µ ∈ R , (5.1)
where the nonlinear function f : Rn × R → Rn is assumed to satisfy all
necessary conditions for the existence (and oftentimes also the uniqueness)
of a solution with respect to any given initial state x0 ∈ Rn and fixed
parameter µ ∈ R. Assume also that x
∗ = 0 is a fixed point, also called an
equilibrium point, of the system at µ = µ0, namely
f(0, µ0) = 0 .
The concern here is whether or not and, if so, how this equilibrium point
and its stability may change as the parameter µ is gradually varied in a
neighborhood of µ0. This study has a great impact on the understanding
129130 Nonlinear Systems
of the dynamical behaviors and stabilities of the parameterized nonlinear
system (5.1).
If there is a change of stability of the equilibrium point, as µ is varied and
passes through a critical value µ0, for instance the system is stable about
its equilibrium point of interest when µ > µ0, but becomes unstable when
µ < µ0, then the system is said to have a bifurcation at the equilibrium
point. The critical value µ = µ0 is called a bifurcation value and (0, µ0) in
the x–µ space is called a bifurcation point. A more precise definition for
the 1-dimensional case, namely
x˙ = f(x; µ), x, µ ∈ R , (5.2)
is given below.
Definition 5.1. The one-parameter family of 1-dimensional nonlinear sys￾tems (5.2) is said to have a bifurcation at an equilibrium point, (x
∗
, µ0),
if the equilibrium curve near the point x = x
∗ and near µ = µ0 is not
qualitatively the same as (i.e. not topologically equivalent to) the original
curve near x = x
∗ at µ = µ0.
For illustration, a few simple and representative examples are discussed.
Example 5.1. Consider the 1-dimensional system
x˙ = f(x; µ) = µ x − x
2
,
which has two equilibrium points: x
∗
1 = 0 and x
∗
2 = µ. As µ is varied, two
equilibrium curves will be generated, as shown in Fig. 5.1.
x = 0
0
x
x = 
µ
µ
Fig. 5.1 Equilibrium curves of the system in Example 5.1.
The stabilities of the above equilibrium point and curves are determined
as follows: first, since this is an autonomous system, one may calculate its
Jacobian, obtaining
J =
∂f
∂x




x=x∗
= µ − 2 x


x=x∗
.Nonlinear Dynamics: Bifurcations and Chaos 131
(i) At x
∗ = 0, J = µ, so when µ < 0, the system is stable about this
equilibrium point and when µ > 0, it is unstable.
(ii) At x
∗ = µ, J = −µ, so when µ < 0, the system is unstable about this
equilibrium point and when µ > 0, it is stable.
Therefore, (x
∗
, µ0) = (0, 0) is a bifurcation point. This type of bifurcation
is called the transcritical bifurcation.
Example 5.2. Consider the 1-dimensional system
x˙ = f(x; µ) = µ − x
2
,
which has one equilibrium point x
∗
0 = 0 at µ = µ0 = 0, with an equilibrium
curve (x
∗
)
2 = µ for all µ ≥ 0, which yields two branches x
∗
1 =
√µ and
x
∗
2 = −
√µ. Clearly, these two branches contain the point x
∗
0 = 0 when
µ = 0. As µ is varied over the interval (0, ∞), these two equilibrium curves
have shapes as shown in Fig. 5.2.
0
x
µ
Fig. 5.2 Equilibrium curves of the system in Example 5.2.
The stabilities of the equilibrium curves can be similarly determined, as
follows. The system Jacobian is
J =
∂f
∂x




x=x∗
= −2 x


x=x∗
.
(i) At x
∗ =
√µ, J = −2
√µ, so when µ > 0, the system is stable about
this equilibrium state.
(ii) At x
∗ = −
√µ, J = 2√µ, so when µ > 0, the system is unstable about
this equilibrium state.
Therefore, (x
∗
, µ0) = (0, 0) is a bifurcation point. This type of bifurcation
is called the saddle-node bifurcation.
Example 5.3. Consider the 1-dimensional system
x˙ = f(x; µ) = µ x − x
3
,132 Nonlinear Systems
which has one equilibrium point x
∗
0 = 0 for all µ ∈ R, with an equilibrium
curve (x
∗
)
2 = µ for all µ ≥ 0, which yields two branches x
∗
1 =
√µ and
x
∗
2 = −
√µ. These two branches contain the point x
∗
0 = 0 when µ = 0. As
µ is varied, the two equilibrium curves have the shapes as shown in Fig. 5.3.
0
x
µ
x = 0
x2 = µ
Fig. 5.3 Equilibrium curves of the system in Example 5.3.
The stabilities of the equilibrium curves can be similarly determined, as
follows. The system Jacobian is
J =
∂f
∂x




x=x∗
= µ − 3x
2



x=x∗
.
(i) At x
∗
0 = 0, J = µ, so when µ > 0, the system is unstable about this
equilibrium state, while for µ < 0, it is stable.
(ii) At (x
∗
)
2 = µ, J = −2µ, so when µ > 0, the system is stable about
both the two equilibrium branches x
∗
1 =
√µ and x
∗
2 = −
√µ.
Therefore, (x
∗
, µ0) = (0, 0) is a bifurcation point. This type of bifurcation
is called the pitchfork bifurcation, inspired by the shape of the bifurcation
diagram, as shown in Fig. 5.3.
To this end, it is important to note that not every nonlinear system
with a varying parameter has bifurcations.
Example 5.4. Consider the 1-dimensional system
x˙ = f(x; µ) = µ − x
3
,
which has one and only one equilibrium curve (x
∗
)
3 = µ, for all µ ∈ R. As
µ is varied, this equilibrium curve has the shape as visualized in Fig. 5.4.
The stabilities of the equilibrium curve can be determined by examining
the system Jacobian
J =
∂f
∂x




x=x∗
= −3x
2



x=x∗
< 0 for all x
∗ =
3√
µ ̸= 0 .Nonlinear Dynamics: Bifurcations and Chaos 133
0 t
stable
stable
x3 =
x
µ
Fig. 5.4 The equilibrium curve of the system in Example 5.4.
Clearly, (x
∗
)
3 = µ is always stable and, around the critical point (x
∗
, µ0) =
(0, 0), the stability of the system about the equilibrium curve does not
change. Therefore, there is no bifurcation in this system.
It can be observed from the above typical bifurcation examples that
(x
∗
, µ0) is a bifurcation point for system (5.2) if either
(i) there are more than one curve of equilibrium solutions of the system
passing through the point (x
∗
, µ0) on the x–µ plane, or
(ii) in the case where there is only one equilibrium curve, this curve is
located on one side of the vertical line µ = µ0 in the neighborhood of
the point (x
∗
, µ0) on the x–µ plane.
For 1-dimensional one-parametric systems, this observation is generally
correct.
5.2 Period-Doubling Bifurcation
For discrete-time systems, there is a special and interesting dynamical phe￾nomenon called period-doubling bifurcation.
A typical example is the logistic map
xk+1 = µ xk(1 − xk), k = 0, 1, 2, . . . , (5.3)
with x0 ∈ (0, 1). Let the non-negative parameter µ be gradually varied,
starting from µ = 0. Then, one can observe the following phenomena:
Case 1. 0 < µ < 1.
In this case, starting from any x0 ∈ (0, 1), one can observe that xk → 0
as k → ∞.134 Nonlinear Systems
Case 2. 1 ≤ µ < 3.
Within this range of parameter values, starting from any x0 ∈ (0, 1),
one can observe that xk approaches a steady state as k → ∞. For instance,
with µ = 2.6, the result is shown in Fig. 5.5.
0.0
5 10 15 20 k
xk
0.2
0.4
0.6
0.8
Fig. 5.5 Converging orbit of the logistic map with µ = 2.6.
Case 3. 3 ≤ µ < 3.449 · · · .
Starting from any x0 ∈ (0, 1), xk → period-2 cycles as k → ∞; namely,
after a transient, {xk} = { · · · , x(1), x(2), x(1), x(2), x(1), x(2)
, · · · }, as shown
in Fig. 5.6 for the case of µ = 3.3.
0.00
5 10 15 20 k
xk
0.25
0.50
0.75
1.00
Fig. 5.6 Period-2 orbit of the logistic map with µ = 3.3.
Case 4. 3.449 · · · ≤ µ < 4.0.
Depending on the value of µ, xk → period-2n cycle for some n > 0 as
k → ∞, as shown in Fig. 5.7 for the case of µ = 3.5.Nonlinear Dynamics: Bifurcations and Chaos 135
0.00
5 10 15 20 k
xk
0.25
0.50
0.75
1.00
Fig. 5.7 Period-22 orbit of the logistic map with µ = 3.5.
Within the range (3, 4) of the parameter µ values, some very rich bifur￾cating phenomena, called the period-doubling bifurcation, can be observed.
The period-doubling bifurcated orbit is summarized in Table 5.1 and shown
in Fig. 5.8, and particularly by the µ–xk diagram in Fig. 5.9, where very
interesting self-similarity within the period-doubling bifurcation is promi￾nent.
Table 5.1 Period-doubling bifurca￾tion of the logistic map.
Parameter Period
µ < 3.0 1 2
0
µ = 3.0 2 2
1
µ = 3.449 · · · 4 2
2
µ = 3.54409 · · · 8 2
3
µ = 3.5644 · · · 16 2
4
µ = 3.568759 · · · 32 2
5
.
.
.
.
.
.
.
.
.
µ = 3.569946 · · · ∞ 2∞
5.3 Hopf Bifurcations in 2-Dimensional Systems
Bifurcations in 2-dimensional parameterized systems can be quite compli￾cated. Some examples of the hyperbolic systems are first studied, along
with the useful normal form theorem. For the non-hyperbolic case, the
system has a very typical Hopf bifurcation, which will be subsequently dis￾cussed.136 Nonlinear Systems
0.00
5 10 15 20 k
xk
0.25
0.50
0.75
1.00
Fig. 5.8 Period-doubling of the logistic map with µ = 3.569946 · · · .
Fig. 5.9 Self-similarity of the logistic period-doubling diagram.
5.3.1 Hyperbolic Systems and Normal Form Theorem
Some examples of hyperbolic systems with rather complicated bifurcation
phenomena are first presented and discussed.
Example 5.5. Consider the 2-dimensional linear parameterized system
(
x˙ = − µ x + y
y˙ = − µ x − 3 y ,
µ ∈ R .Nonlinear Dynamics: Bifurcations and Chaos 137
The system has eigenvalues
λ1,2 = −
µ + 3
2
±
1
2
p
(µ − 1) (µ − 9).
Hence, the zero equilibrium point is hyperbolic. As µ is varied, some bifur￾cations occur as depicted in Fig. 5.10.
• • • ✲
0 1 9 µ
λ1,2
↓
bifurcation point
real real complex real
opposite signs same signs same signs
(saddle) (stable node) (stable spiral) (stable node)
Fig. 5.10 Bifurcations of the 2-dimensional parametric system in Example 5.5.
Example 5.6. Consider the controlled damped pendulum
(
x˙ = y
y˙ = − y − sin(x) + u ,
where the angular variable x = θ satisfies −π ≤ θ ≤ π and y = ˙θ. This
system has two possible equilibrium points: y
∗ = 0 and sin(x
∗
) = u. There
are three cases:
(i) u > 1: there is no equilibrium point;
(ii) u = 1: there is one equilibrium point, at (x
∗
, y∗
) = (π/2, 0);
(iii) u < 1: there are two equilibrium points.
If u = µ is considered as the system variable parameter, then the above
three cases together show that µ0 = u0 = 1 is a bifurcation value.
In order to determine the type of bifurcation at µ0, an effective way is
to transform the system to the so-called normal form, in three steps, as
follows:
Step 1. Shift (x
∗
, y∗
, µ0) to (0, 0, 0)
This can be done by a change of variables:



u = 1 + v
x = z1 + π/2
y = z2 ,138 Nonlinear Systems
which yields a new system of the form
(
z˙1 = z2
z˙2 = − z1 − cos(z1) + 1 + v .
Step 2. Find the Jacobian and its eigenvalues and eigenvectors
J =

0 1
sin(z1) −1

z
∗
1 =0
=

0 1
0 −1

and
λ1 = 0 , w1 =

1
0

; λ2 = −1 , w2 =

−1
1

.
Step 3. Find the normal form
Use the nonsingular matrix
P := 
w1 w2

=

1 −1
0 1 
to transform the new system to be in the normal form
ζ = P z ,
which is
( ˙ζ1 = − cos ￾
ζ1 + ζ2

+ 1 + v
˙ζ2 = − ζ2 − cos ￾
ζ1 + ζ2

+ 1 + v .
To this end, the following theorem applies.
Theorem 5.1 (Normal Form Theorem). Suppose that, in the normal
form
( ˙ζ1 = f(v, ζ1, ζ2)
˙ζ2 = − ζ2 + g(v, ζ1, ζ2),
(5.4)
the two nonlinear functions f and g are both nontrivial functions of v.
There are three cases:
(i) if
∂f
∂v (0, 0, 0) ̸= 0 and ∂
2f
∂ζ2
1
(0, 0, 0) ̸= 0
then there is a saddle-node bifurcation at v0 = Nonlinear Dynamics: Bifurcations and Chaos 139
(ii) if
v
∂f
∂v (0, 0, 0) ·
∂
2f
∂ζ2
1
(0, 0, 0) < 0
then there are two hyperbolic equilibria: one is a saddle node and the
other is a stable node;
(iii) if
v
∂f
∂v (0, 0, 0) ·
∂
2f
∂ζ2
1
(0, 0, 0) > 0
then there is no equilibrium point (so, no bifurcation).
Proof. See [Wiggins (1990)]: p. 216.
Note that, since nonsingular linear transforms do not change the sys￾tem qualitative behaviors (topological properties), the type of bifurcation
concluded by the theorem can be transformed back to the original system
without altering its qualitative properties and dynamical behaviors. The
only corresponding changes are the state variables and the parameter value.
Example 5.7. Return to Example 5.6 on the controlled pendulum. It can
be easily verified that
∂f
∂v (0, 0, 0) = 1 ̸= 0
∂
2f
∂ζ2
1
(0, 0, 0) = − cos(ζ1 + ζ2)



ζ1=ζ2=0
= −1 ̸= 0 .
Therefore, at v0 = 0 the system in the normal form has a saddle-node
bifurcation. After transforming back to the original system, it has a saddle￾node bifurcation at u0 = 1.
Note that one may also exchange ζ1 and ζ2 to obtain a “dual theorem”
for a “dual system” of the normal form (5.4). This may be useful in some
cases.
5.3.2 Decoupled 2-Dimensional Systems
Some 2-dimensional (i.e. planar) systems can be decoupled, so that the
bifurcation analysis becomes much easier.
Consider a 2-dimensional parameterized system,
(
x˙ = f(x, y; µ)
y˙ = g(x, y; µ),140 Nonlinear Systems
which has a bifurcation point (x
∗
, y∗
, µ0) = (0, 0, 0). Applying the Taylor
expansion at (x
∗
, y∗
) = (0, 0), one can rewrite the system as

x˙
y˙

=

a(µ)
b(µ)

+ J(µ)

x
y

+ HOT ,
where J is the Jacobian and HOT represents all the higher-order terms.
Since µ = µ0 = 0 is a bifurcation value, a(0) = b(0) = 0. Let λ1(µ)
and λ2(µ) be the eigenvalues of J(µ). Then, using a nonsingular lin￾ear transform, one can assume without loss of generality that J(µ) =
diag {λ1(µ), λ2(µ)}. It then follows that
(
x˙ = a(µ) + λ1(µ) x + HOT
y˙ = b(µ) + λ2(µ) x + HOT .
(5.5)
Theorem 5.2. Consider the decoupled 2-dimensional system (5.5).
(i) If da(µ)/dµ


µ=0 ̸= 0, then there exists a single branch of equilibria
of the system in a neighborhood of the bifurcation point (x
∗
, y∗
, µ0) =
(0, 0, 0), which is of saddle-node type.
(ii) (a) If da(µ)/dµ


µ=0 = 0 and
∂
2f
∂µ2
(0, 0, 0) ·
∂
2f
∂x2
(0, 0, 0) −

∂
2f
∂µ∂x (0, 0, 0) 2
< 0 ,
then there are two branches of equilibria, which intersect and exchange
stability at the bifurcation point, and the bifurcation is either transcrit￾ical or of pitchfork type.
(b) If da(µ)/dµ


µ=0 = 0 and
∂
2f
∂µ2
(0, 0, 0) ·
∂
2f
∂x2
(0, 0, 0) −

∂
2f
∂µ∂x (0, 0, 0) 2
> 0 ,
then the bifurcation point has an isolated point, and actually the only
solution of the system in a neighborhood of this point is the point itself.
Proof. See [Glendinning (1994)]: pp. 206–219.
It is remarked that one may exchange x ↔ y and f ↔ g to have a
“dual” theorem.
Example 5.8. Consider the following decoupled 2-dimensional system:
(
x˙ = µ − x
2
y˙ = − y .Nonlinear Dynamics: Bifurcations and Chaos 141
Observe that the second equation is linear and stable: y(t) = e
−t → 0
as t → ∞; the first one is 1-dimensional with a saddle-node bifurcation
as seen in Example 5.2. The phase portraits of this system for different
values of µ are shown in Fig. 5.11(a). This system satisfies the saddle-node
bifurcation condition stated in Theorem 5.2, since
a(µ) = µ and da(µ)
dµ



µ=0
= 1 ̸= 0 .
0
µ → 0 µ→0
µ → 0
µ < 0 µ < 0
µ > 0 µ > 0
µ = 0 µ = 0
√|µ|
(a) saddle-node bifurcation (b) pitchfork bifurcation
Fig. 5.11 Pitchfork bifurcation in a 2-dimensional system.
Example 5.9. Consider the following decoupled 2-dimensional system:
(
x˙ = − µ x − x
3
y˙ = − y .142 Nonlinear Systems
Observe that the second equation is linear and stable: y(t) = e
−t → 0
as t → ∞; and the first one is 1-dimensional with a pitchfork bifurcation
as seen in Example 5.3. This system satisfies the pitchfork bifurcation
condition stated in Theorem 5.2 above, since
∂
2f
∂µ2
(0, 0, 0) ·
∂
2f
∂x2
(0, 0, 0) −

∂
2f
∂µ∂x (0, 0, 0) 2
= −1 < 0 .
The phase portraits of this system for different values of µ are shown in
Fig. 5.11(b).
Example 5.10. Consider the Lotka–Volterra system:
(
x˙ = x (µ − x) − (x + 1) y
2
y˙ = y (x − 1).
Its equilibrium points and curves, when plotted in the (x, y, µ := z)-space,
include:
(1)



x
∗
1 = 0
y
∗
1 = 0
z
∗
1 = µ
(2)



x
∗
2 = µ
y
∗
2 = 0
z
∗
2 = µ
(3)



x
∗
3 = 1
2

y
∗
3
2
= µ − 1
z
∗
3 = µ
as shown in Fig. 5.12.
0
(1)
(3)
(2)
1
U
U
U
S
S
S
S
x
y
µ
µ
Fig. 5.12 Equilibrium points and curves of the Lotka–Voterra system.
The system Jacobian is
J =

µ − 2x − y
2 −2(x + 1)y
y x − 1

.
Case (1). The eigenvalues of J, evaluated at (x
∗
1
, y∗
1
, z∗
1
), are λ1(µ) =
µ and λ2(µ) = −1. So, there is a bifurcation point at µ0 = 0, which is
stable for µ < µ0 = 0 (S in Fig. 5.12) but is unstable for µ > µ0 = 0
(U in Fig. 5.12).Nonlinear Dynamics: Bifurcations and Chaos 143
Case (2). The eigenvalues of J, evaluated at (x
∗
2
, y∗
2
, z∗
2
), are λ1(µ) =
−µ and λ2(µ) = µ−1. So, there are two bifurcation points, at µ01 = 0
and µ02 = 1, respectively; the first one is stable for µ01 = 0 < µ < 1
(S in Fig. 5.12) but the second is unstable for µ < µ01 = 0 and
µ > µ02 = 1 (U in Fig. 5.12).
Case (3). The eigenvalues of J, evaluated at (x
∗
3
, y∗
3
, z∗
3
), are
λ1,2(y) = y
2 − 1 ±
p
(y
2 − 9)2 − 80
2
or
λ1,2(µ) = µ − 3 ±
p
(µ − 19)2 − 320
4
.
There exist bifurcation points only for µ ≥ 1, which is stable for 1 ≤
µ < 3 but is unstable for µ > 3.
It should be noted that, in Case (3) above, the two eigenvalues µ1,2(µ) are
(a) both real negative, if 1 < µ < 19 −
√
320;
(b) complex conjugate with negative real parts, if 19 −
√
320 < µ < 3;
(c) complex conjugate with positive real parts, if µ > 19 + √
320.
Also, as shown in Fig. 5.13,
(a) there is a pitchfork bifurcation at µ = 1;
(b) there is a Hopf bifurcation at µ = 3.
0 0
1
1 1 23
(3) (3)
y (µ–1) 2
 = 1
–
2
S S
S
µ µ
x y
Fig. 5.13 Two other bifurcations of the Lotka–Voterra system.144 Nonlinear Systems
5.3.3 Hopf Bifurcation of 2-Dimensional Systems
For a 2-dimensional non-hyperbolic system, the following is the main result
on Hopf bifurcation (see Fig. 5.14).
Theorem 5.3 (Poincar´e–Andronov–Hopf Theorem). Suppose that
the 2-dimensional parameterized system
(
x˙ = f(x, y; µ)
y˙ = g(x, y; µ)
(5.6)
has a zero equilibrium point, (x
∗
, y∗
) = (0, 0), and that its Jacobian has a
pair of complex conjugate eigenvalues, λ(µ) and λ¯(µ). If
d Re {λ(µ)}
d µ




µ=µ0
> 0 , β(µ) ̸= 0 ,
for some µ0, then
(i) µ = µ0 is a bifurcation point of the system;
(ii) for close enough values µ < µ0, the zero equilibrium point is asymp￾totically stable;
(iii) for close enough values µ > µ0, the zero equilibrium point is unstable;
(iv) for close enough values µ ̸= µ0, the zero equilibrium is surrounded by
a limit cycle of magnitude O(
p
|µ − µ0|).
Proof. See [Arrowsmith and Place (1990)].
Example 5.11. Consider the following 2-dimensional system:
(
x˙ = y + x
￾
µ − x
2 − y
2

y˙ = − x + y
￾
µ − x
2 − y
2

.
Its Jacobian matrix is
J =

µ − 3x
2 − y
2 1 − 2xy
−1 − 2xy µ − x
2 − 3y
2

(x∗,y∗)=(0,0)
=

µ 1
−1 µ

,
with complex conjugate eigenvalues λ1 = µ + j and λ2 = µ − j, satisfying
α(µ) = µ, β(µ) = 1 ̸= 0, and ∂α(µ)/∂µ = 1 > 0.
Theorem 5.3 implies that, as µ increases to pass the value of µ0 = 0, the
system changes its stability and a limit cycle will emerge with magnitude
O(
p
|µ − µ0|) = O(
p
|µ|).
Using the polar coordinates, the system can be rewritten as
(
ρ˙ = ρ
￾
µ − ρ
2

˙θ = − 1 .
It is easy to verify, e.g. by computer plots (see Fig. 5.14), tNonlinear Dynamics: Bifurcations and Chaos 145
(i) if µ ≤ 0 then the system orbit will spiral inward to the zero equilibrium
point (stable focus);
(ii) if µ > 0 then (0, 0) becomes unstable, and a limit cycle of radius ρ0 =
√µ will suddenly emerge as µ is changed from negative to positive
through the bifurcation point µ = µ0 = 0.
x1
x
µ
µ >
µ
2
x1
x1 x1
x1 x1
x1 x1
x2
x2
µ0
µ < µ0
µ = µ0
µ = µ0
x2
x2 x2
x x2 2
Fig. 5.14 Hopf bifurcation in a 2-dimensional system.
5.4 Poincar´e Maps
Consider a general 2-dimensional autonomous system,
(
x˙ = f(x, y)
y˙ = g(x, y)146 Nonlinear Systems
along with its phase plane. Let Γ be a curve starting from an equilibrium
point of the system with the property that it cuts each solution orbit on
the phase plane transversely, namely, not tangential to the orbit anywhere.
Consider a point, P0 = P0(x0, y0), on Γ. Suppose that an orbit passing
through Γ at P0 returns to Γ after some time, being cut by Γ again but
probably at a different point, P1 = P1(x1, y1), as illustrated in Fig. 5.15.
This new point, P1, is called the first return point and the map
MΓ: (x0, y0) → (x1, y1) (or MΓ: P0 → P1 )
is called the Poincar´e map, denoted as
(x1, y1) = MΓ(x0, y0) (or P1 = MΓ(P0) ).
P1
P
y
x
0
Γ
0
(x*, y*)
Fig. 5.15 The Poincar´e first return map.
If the orbit continues to make a second turn and cuts Γ at P2 =
P2(x2, y2), then
(x2, y2) = MΓ(x1, y1) = MΓ
￾
MΓ(x0, y0)

= M2
Γ
(x0, y0).
After n returns, one has
(xn, yn) = Mn
Γ
(x0, y0)
￾
or Pn = Mn
Γ
(P0)

,
as visualized in Fig. 5.16.
In the situation shown as in Fig. 5.16, since P
∗ = Mn
Γ
￾
P
∗

for all n, P
∗
is an equilibrium point on a periodic orbit. Moreover, as illustrated by the
figure, since
P0 → P1 → · · · → P
∗
and P
′
0 → P
′
1 → · · · → P
∗
,
this periodic orbit is a stable limit cycle. This implies that if a Poincar´e
map MΓ for a given autonomous system can be appropriately selected, and
a point satisfying P
∗ = MΓ(P
∗
) exists such that
Mn
Γ
￾
P0

→ P
∗
and Mn
Γ
￾
P
′
0

→ PNonlinear Dynamics: Bifurcations and Chaos 147
P
0
P0
P
1
P1 P∗
x
y
(x*, y*)
Fig. 5.16 The Poincar´e map with multiple returns.
where P0 and P
′
0 are located on two opposite sides of P
∗ at its located
solution curve, then it can be concluded that the system has a stable limit
cycle. The following example shows more details.
Example 5.12. Consider the following 2-dimensional parameterized sys￾tem:
(
x˙ = µ x + y − x
p
x
2 + y
2
y˙ = − x + µ y − y
p
x
2 + y
2 ,
which has an equilibrium point (x
∗
, y∗
) = (0, 0). Consider an orbit Γ: x >
0, y = 0, namely, the positive semi-x-axis.
In polar coordinates, this system is written as
(
r˙ = r (µ − r)
˙θ = − 1 ,
which has a solution
(
r = µ r0 / (r0 + (µ − r0) e
−µt)
θ = − t + θ0 ,
or, simply,
r =
µ r0
r0 + (µ − r0) e−µ(θ−θ0)
.
Note that, in polar coordinates, the first return is completed for a 2π-change
of θ and, for this example, it is −2π. With θ0 = 0 and for 0 ≤ µ ≤ r0, the
Poincar´e map is given by
r1 = MΓ(r0) = µ r0
r0 + (µ − r0) e−µ 2π
.
Therefore, for P0 = (r0, 0), one has
rn = Mn
Γ
(r0) = µ r0
r0 + (µ − r0) e−µ 2nπ
,148 Nonlinear Systems
and θn = 0 (which, actually, can be arbitrary). Thus, if r0 = µ and
θ0 = 0 are chosen, then rn → µ and θn → 0 as n → ∞. This implies that
P
∗ = (µ, 0), as shown in Fig. 5.17, where the stable limit cycle is r = µ,
which passes the point P
∗
.
0
P∗=µ
r1 r0
y
x
Γ
Fig. 5.17 Poincar´e map and the stable limit cycle in Example 5.12.
5.5 Strange Attractors and Chaos
Return to the general higher-dimensional autonomous system
x˙ = f(x), x(t0) = x0 ∈ R
n
, (5.7)
and recall the concept of ω-limit sets (or positive limit sets) introduced in
Definition 2.4: a set of points z ∈ Rn is an ω-limit set of system (5.7), if
there is a solution orbit x(t) of the system such that
||x(tn) − z|| → 0 as n → ∞ and tn → ∞.
It is clear that all stable equilibrium points and all periodic solution
orbits (e.g. limit cycles), which need not be stable, are positive limit sets;
for example, every solution orbit of the linear harmonic oscillator described
by ˙x = y and ˙y = −x is a positive limit set.
Note that the difference between a stable equilibrium point and a limit
cycle as a positive limit set is that the former does not include the trajectory
that approaches the limit but the latter does. In the former case, the system
solution orbit is called a manifold, which is formally defined as follows.
Definition 5.2. Let x
∗ be a saddle-node type of equilibrium point. The
stable manifold of x
∗
, denoted Ws
(x
∗
), is the set of points x satisfying that
the system solution orbit φt(x0) approaches x
∗ as t → ∞. An unstable
manifold of x
∗
, denoted Wu
(x
∗
), is the set of points x satisfying that the
system solution orbit φt(x0) approaches x
∗ as t → −∞.Nonlinear Dynamics: Bifurcations and Chaos 149
The concepts of stable and unstable manifolds are illustrated in
Fig. 5.18.
x∗
Fig. 5.18 Stable and unstable manifolds of a saddle-node equilibrium point.
Example 5.13. Consider a 3-dimensional linear system with real eigenval￾ues, λ1 > 0 > λ2 > λ3. A solution of the system is given by
φt(x0) = c1e
λ1tw1 + c2e
λ2tw2 + c3e
λ3tw3 ,
where wi are the corresponding eigenvectors and ci are constants deter￾mined by the initial conditions, i = 1, 2, 3.
This system has a unique ω-limit set, which is the zero equilibrium point
(the origin). The stable and unstable manifolds are determined by its eigen￾values in this linear case: the stable manifold is the hyperplane spanned by
w2 and w3, and the unstable manifold is the line curve determined by w1.
These two manifolds intersect transversally at the zero equilibrium point,
as illustrated in Fig. 5.19.
w2 w3
w1
Wu
Ws
x∗
Fig. 5.19 Stable and unstable manifolds of an equilibrium point in a 3-dimensional
linear system.
Now, return to the notion of positive limit sets that are not necessarily
single points.
Definition 5.3. Consider the general autonomous system (5.7). A set Λ
inside a region Ω ⊆ Rn is called an attractor of the system, if the positive
limit set of an arbitrary solution orbit of the system lies in Ω then it must
lie in Λ.150 Nonlinear Systems
Intuitively, by definition, an attractor is a limit set and, within some
region containing this limit set, all system solution trajectories tend to
evolve toward it.
As a couple of simple examples, a stable focus point is an attractor, for
which the region Ω ⊆ Rn is the basin of attraction (see Definition 2.4), and
the limit cycle of the van der Pol oscillator (see Fig. 1.12) is an attractor,
for which the region is Ω = R2\{0}.
Note that in a general situation, Ω in Definition 5.3 can be a set but
need not be a region. The largest possible set Ω is the basin of attraction
containing the set Λ. For example, in the special case where Λ = {0}, this
reduces to the basin of attraction of the zero equilibrium point.
Note also that an attractor can be a point, a finite set of points, a
curve, a manifold, or even a complicated set with a fractal structure, which
is referred to as a strange attractor.
5.5.1 Chaotic Lorenz System
Typically, chaotic systems have strange attractors. Before introducing the
concept and notion of chaos, a representative example is discussed.
Example 5.14. Consider the Lorenz system



x˙ = σ (y − x)
y˙ = − xz + r x − y
z˙ = xy − b z ,
(5.8)
where σ, r, and b are positive constants. It is easy to verify that this system
has three equilibrium points:
(1)



x
∗
1 = 0
y
∗
1 = 0
z
∗
1 = 0 ,
(2)



x
∗
2 =
p
b(r − 1)
y
∗
2 =
p
b(r − 1)
z
∗
2 = r − 1 ,
(3)



x
∗
3 = −
p
b(r − 1)
y
∗
3 = −
p
b(r − 1)
z
∗
3 = r − 1 .
Here, by examining the eigenvalues of the system Jacobian at the three
equilibrium points, it can be verified that the first equilibrium point is
stable if r < 1 but unstable if r > 1, while the stability of the other two
equilibrium points depends on the values of all three parameters σ, r, and b;
for example, they are both unstable if r = σ(σ+b+3)/(σ−b−1) = 24.74 · · · .
In the following discussions, select and fix σ = 10 and b =
8
3
, and
gradually change the values of r as a parameter:Nonlinear Dynamics: Bifurcations and Chaos 151
Case 1. 0 < r < 1.
In this case, (x
∗
1
, y∗
1
, z∗
1
) is a global stable focus point. The other equi￾librium points, (x
∗
2
, y∗
2
, z∗
2
) and (x
∗
3
, y∗
3
, z∗
3
), are complex conjugates, so
they cannot be displayed in the phase space.
Case 2. 1 < r < 24.74 · · · .
In this case, (x
∗
1
, y∗
1
, z∗
1
) becomes unstable, with other two real equi￾librium points (x
∗
2
, y∗
2
, z∗
2
) and (x
∗
3
, y∗
3
, z∗
3
) bifurcating out, which are
both local stable point attractors. The phase portraits when 1 < r <
13.926 · · · and 13.926 · · · < r < 24.74 · · · are shown in Figs. 5.20(a) and
5.20(b), respectively.
(a) (b)
Fig. 5.20 The two local point attractor of the Lorenz system.
Case 3. r = 24.74 · · · .
In this case, both (x
∗
2
, y∗
2
, z∗
2
) and (x
∗
3
, y∗
3
, z∗
3
) become unstable and a
Hopf bifurcation appears.
Case 4. r = 28 .
This is the most interesting situation where chaos emerges: the system
orbit is spiraling around one of the two equilibrium points, (x
∗
2
, y∗
2
, z∗
2
)
and (x
∗
3
, y∗
3
, z∗
3
), for a certain period of time (which is unpredictable be￾forehand), then suddenly jumps to the vicinity of another equilibrium
point, which it spirals around for a while (again, the time period of such
encircling is unpredictable beforehand), and then it suddenly switches
back to the first equilibrium point, · · · . This switching process contin￾ues indefinitely and infinitely, but the system orbit never converges to
nor diverges from either equilibrium point, and never exactly repeats it￾self. Thus, the system orbit spiraling around the two equilibrium points
virtually approach a strange attractor. This wandering system orbit is
said to be a chaotic orbit, while the Lorenz system is called a chaotic
system. This chaotic attractor of the Lorenz system is visualized in
Fig. 5.21.
The concept of chaos is rather difficult to precisely define in rigor￾ous mathematical terms, due to its highly complicated nature and multi￾ple complex characteristics. Nevertheless, a simple, easily-understandable,152 Nonlinear Systems
Fig. 5.21 The strange attractor of the Lorenz system.
convenient way to describe and conceptually correct, although not precisely
rigorous “definition” of chaos can be given, as follows.
Definition 5.4. A phase orbit is said to be chaotic, if it is bounded in a
region confined within the phase space but is not convergent (to a point)
and is not periodic (not even quasi-periodic).
Here, a system orbit is said to be quasi-periodic if it is not-exactly but
irregularly similar to a periodic orbit. It is different from the so-called
almost periodic orbit that has increasing regularity over time.
5.5.2 Poincar´e–Bendixson Theorem
As mentioned above, a chaotic orbit must be bounded in the phase space.
This is intuitively necessary; otherwise, the orbit diverges so it will disap￾pear from the phase space. However, boundedness is obviously not suf￾ficient; for instance, an orbit spiraling toward a stable focus is bounded
but it is certainly not chaotic. The following is an important result on the
consequence of boundedness for 2-dimensional (planar) systems.
Theorem 5.4 (Poincar´e–Bendixson Theorem). Consider a 2-
dimensional autonomous system, x˙ = f(x), x(t0) = x0 ∈ Rn, defined on
a bounded closed region Ω ⊂ Rn. Let Γ be a solution orbit of the system,
which enters region Ω and then stays inside forever. In this case, the orbit ΓNonlinear Dynamics: Bifurcations and Chaos 153
must be one of the following types: (i) it is a closed orbit; (ii) it approaches
a closed orbit; (iii) it approaches an equilibrium point.
Proof. See [Verhulst (1996)]: p. 47.
These three possibilities are illustrated in Fig. 5.22.
Ω Ω Ω
Γ Γ Γ
(i) (ii) (iii)
Fig. 5.22 Three cases of a bounded orbit on the plane.
Corollary 5.1. For a continuous-time autonomous system to have chaos,
its dimension has to be at least three.
5.5.3 Some Other Chaotic Systems
Note that the Poincar´e–Bendixson Theorem does not apply to nonau￾tonomous systems since a 2-dimensional nonautonomous system may have
chaos. Two typical examples are the van der Pol oscillator (1.24) and the
Duffing oscillator (1.30), as discussed in more detail below.
Example 5.15. The van der Pol oscillator is described by
(
x˙ = x −
1
3
x
3 − y + p + q cos(ωt)
y˙ = c (x + a − b y),
(5.9)
which is chaotic when
(p, q, a, b, cω) = (0.0, 0.74, 0.7, 0.8, 0.1, 1.0),
with the strange attractor shown in Fig. 5.23.
Example 5.16. The Duffing oscillator is described by
(
x˙ = y
y˙ = − a x − b x3 − c y + q cos(ωt),
(5.10)
which is chaotic when
(a, b, c, q, ω) = (−1.1, 1.0, 0.4, 1.8, 1.8),
with the strange attractor shown in Fig. 5.24.154 Nonlinear Systems
Fig. 5.23 The van der Pol chaotic attractor.
Note also that the Poincar´e–Bendixson Theorem does not apply to
autonomous systems of dimension three or higher since they may have
chaos. Simple examples of 3-dimensional autonomous systems that have
only quadratic nonlinearity but can generate chaos include the Lorenz sys￾tem (see Example 5.14), Chua’s circuit (see Fig. 1.26), and the R¨ossler and
Chen systems to be discussed below (see, e.g. [Chen (2020)]).
Fig. 5.24 The Duffing chaotic attractor.Nonlinear Dynamics: Bifurcations and Chaos 155
Example 5.17. The R¨ossler system



x˙ = − y − z
y˙ = x + a y
z˙ = x z − b z + c ,
(5.11)
is chaotic when (a, b, c) = (0.2, 5.7, 0.2), with the strange attractor shown
in Fig. 5.25.
Fig. 5.25 The R¨ossler chaotic attractor.
Example 5.18. The Chen system



x˙ = a (y − x)
y˙ = (c − a) x − x z + c y
z˙ = x y − b z
(5.12)
is chaotic when (a, b, c) = (35, 3, 28), with the strange attractor shown in
Fig. 5.26.
It is important to clarify that although the system equations of (5.12) are
quite similar to that of the Lorenz system (5.8), they are topologically non￾equivalent; namely, there is no diffeomorphism (e.g. nonsingular coordinates
transform) that can transfer one to another [Chen (2020)].
Note that although a strange attractor is an indication of chaos, which
may not be easy to find in general, however, there are some other charac￾teristics of chaos that can be relatively easily verified, even before a strange
attractor is found, which are introduced below.156 Nonlinear Systems
Fig. 5.26 The Chen chaotic attractor.
Note also that for higher-dimensional continuous-time autonomous sys￾tems, the Poincar´e–Bendixson Theorem may actually be further extended
so as to include at least one more possibility from the three possibilities
(i)–(iii) stated in Theorem 5.4, for example, the following:
(iv) The system orbit Γ approaches a limit set.
Here, a limit set includes a strange attractor because a strange attractor
is an ω-limit set (see Definition 2.3). Note, however, that there are some
other limit sets that are not equilibrium points, limit cycles, or strange
attractors. Also, depending on the definition of chaos, a strange attractor
can be non-chaotic. In this regard, usually, a chaotic attractor is referred
to as a strange attractor for a system that is sensitive to initial conditions,
which is discussed next.
5.5.4 Characterizations of Chaos
Regarding the possibility (iv) above, when Γ approaches a chaotic attrac￾tor, locally it must have the following features: on the one hand, it keeps
approaching a certain subset (e.g. a point), meaning that it possesses some
sort of “negative Jacobian eigenvalue” along that particular direction of
approaching, on the average; on the other hand, it keeps leaving this sub￾set (or point), meaning that it possesses a “positive Jacobian eigenvalue”
along that direction (see Fig. 5.18). Usually, when a system has a positive
Jacobian eigenvalue, it will diverge along the direction specified by the cor￾responding eigenvector. However, the global boundedness property of theNonlinear Dynamics: Bifurcations and Chaos 157
system prohibits its global divergence. It is easy to imagine that this global
boundedness is determined by the summation of all Jacobian eigenvalues,
which should be negative. Thus, a suitable combination of these attracting
and repelling features of the strange attractor leads the system orbits to
very complicated dynamical behaviors such as chaos.
5.5.4.1 Lyaponov Exponent: Continuous-Time Systems
For a nonlinear autonomous system, to quantify the average of a Jacobian
eigenvalue of the system, which is evaluated at different operating points
throughout the entire dynamical process, the concept of Lyapunov exponent
is useful.
For a 1-dimensional autonomous system,
x˙ = f(x), x(t0) = x0 ,
defined on a domain D ∈ R, if for almost all initial conditions x0 ∈ D (i.e.
except perhaps a set of measure zero), the solution x(t) = x(t; x0) of the
system behaves like
| x(t; x0)| → e
λ t for large enough t > t0 ,
where λ = λ(x0) can be evaluated by
λ(x0) = limt→∞
1
t
ln

 x(t; x0)


.
This λ(x0), if it is finite, is called the Lyapunov exponent of the system
orbit x(t; x0) starting from x0.
Obviously, the concept of Lyapunov exponent is a generalization of
eigenvalue for linear systems. It is also clear that the Lyapunov expo￾nent is sensitive to the system’s initial conditions x0 ∈ D, which will be
further discussed below in Definition 5.6.
For a higher-dimensional autonomous system,
x˙ = f(x), x(t0) = x0 ∈ R
n
,
the maximum Lyapunov exponent is defined by
λ(x0) = limt→∞
1
t
ln



 z(t; x0)




, (5.13)
where z(t) = z(t; x0) is a solution of the corresponding linearized system
(
z˙ = J(x) z
z(t0) = x0 ,
in which J(x) = ∂f(x)/∂x is the system Jacobnian with x = x(t; x0).158 Nonlinear Systems
Table 5.2 Lyapunov exponents of some typical
chaotic systems.
System Lyapunov Exponent
Chen (λ1, λ2, λ3) = (1.983, 0.000, −11.986)
Chua (λ1, λ2, λ3) = (0.230, 0.000, −1.7800)
Lorenz (λ1, λ2, λ3) = (0.897, 0.000, −14.565)
R¨ossler (λ1, λ2, λ3) = (0.130, 0.000, −14.100)
For a 3-dimensional autonomous system to have chaos, it is common
that its three Lyapunov exponents are λ1 > 0, λ2 = 0, and λ3 < 0, denoted
as (+, 0, −), with λ1 + λ3 < 0. The Lyapunov exponents for several typical
chaotic systems are listed in Table 5.2.
In general, for 3-dimensional systems, the three Lyapunov exponents
(−, −, −) correspond to an equilibrium point, (0, −, −) a periodic orbit,
(0, 0, −) a torus, and (+, 0, −) chaos as shown in Table 5.2.
It is remarked that to characterize chaos, in addition to the Lyapunov
exponent there are some other measures, such as positive entropy, fractal
dimension, and continuous power spectrum [Wiggins (1990); Hoppensteadt
(2000)], which are not further studied in this text.
5.5.4.2 Lyaponov Exponent: Discrete-Time Systems
Lyapunov exponents in discrete-time systems are quite different, although
the concept and therefore the definition are similar.
For a discrete-time “autonomous” system,
xk+1 = f
￾
xk

, x0 ∈ R
n
,
denote its Jacobian at the kth step by
Jk = Jk(x0) = ∂f
￾
xk

∂xk
,
which all depend on the initial state x0 since xk = f
k
(x0). Let
Pk = Pk(x0) := JkJk−1 · · · J2J1
be the kth product of such Jacobians, and let ei = ei(x0) be the eigenvalues
of Pk, arranged according to
|e1| ≥ |e2| ≥ · · · ≥ |en| ≥ 0Nonlinear Dynamics: Bifurcations and Chaos 159
The ith Lyapunov exponent of the system orbit {xk}, starting from x0, is
defined by
λi(x0) = lim
k→∞
1
k
ln

ei
￾
x0


, i = 1, . . . , n . (5.14)
Differing from the continuous-time setting, a discrete-time system can
be chaotic even if it is 1-dimensional and has (the only) one positive Lya￾punov exponent. The logistic map (5.3) is a typical example of this type:
xk+1 = µ xk
￾
1 − xk

, x0 ∈ (0, 1), 0 < µ ≤ 4 , (5.15)
This map is chaotic when, say, µ = 4.0 (or slightly smaller). In this case,
its only Lyapunov exponent is λ = ln 2 = 0.693 · · · . Its diagram of period￾doubling bifurcation leading to chaos is shown in Fig. 5.8.
Another example is the 2-dimensional H´enon map,
(
xk+1 = 1 − a x2
k + yk
yk+1 = b xk ,
(5.16)
which is chaotic when (a, b) = (1.4, 0.3), with Lyapunov exponents λ1 =
0.603 and λ2 = −2.34 without a zero exponent. A typical chaotic phase
portrait of the H´enon map is shown in Fig. 5.27.
1.5
1
0.5
0
–0.5
–1
–1.5
–1.5 –1 –0.5 0 0.5 1 1.5
Fig. 5.27 The H´enon chaotic attractor.
In both continuous-time and discrete-time autonomous systems, a pos￾itive maximum Lyapunov exponent is a necessary condition for chaos to
exis160 Nonlinear Systems
5.6 Chaos in Discrete-Time Systems
Chaos in discrete-time systems, or maps, has a rather precise definition. For
a 1-dimensional map, the following definition [Touhey (1997)] is convenient
to use and verify.
Definition 5.5. A map M : S → S, where S is a nonempty set in a bounded
and compact domain, is chaotic if and only if for any two nonempty open
subsets, U and V , of S, there exist a point x0 ∈ S and a period-n orbit of f,
with a non-negative integer n, such that f(x0) ∩ U ̸= ∅ and f(x0) ∩ V ̸= ∅.
This definition is equivalent to another, more common definition of dis￾crete chaos given earlier in the sense of Devaney [Devaney (1987)].
Definition 5.6. A map M : S → S, where S is a nonempty set in a
bounded and compact domain, is chaotic if
(i) it is sensitive to initial conditions;
(ii) it is topologically transitive;
(iii) it has a dense set of periodic orbits.
Here, the meaning of each condition is further interpreted as follows:
(i) “Sensitivity to initial conditions” means that starting from two arbitrar￾ily close initial points, x1 and x2, in S, the corresponding orbits, Mn(x1)
and Mn(x2), will fall far apart after a large enough number of iterations,
n. For instance, let S = [a, b] be a bounded interval in R+, and M : S → S
be a map. This property of sensitivity to initial conditions implies that
for any prescribed δ ∈ [0, b − a], as long as x1 ̸= x2 in S, no matter
how close they are, after a large enough number of iterations, n, one has



Mn(x1) − Mn(x2)



 > δ. This property is illustrated in Fig. 5.28.
0 1 2 k
x1
y1 x0
y 0
xk
k y
δ
Fig. 5.28 Sensitivity to initial conditions.
(ii) “Topological transitivity” means that for any nonempty open set Ω ⊆ S,Nonlinear Dynamics: Bifurcations and Chaos 161
no matter how small it is, an orbit of the map will sooner or later travel
into it. This property is illustrated in Fig. 5.29.
0 1 2 ∞ k
k
S
S
x1
x2
x0
range
x ∈
Fig. 5.29 Topological transitivity property.
(iii) “Dense set of periodic orbits” means that the map has infinitely many
periodic orbits of different periods, and all these periodic orbits constitute
a dense set in S. The period-doubling bifurcation diagram of the logistic
map, shown in Fig. 5.8, with µ ∈ (0, 4.0], best illustrates this property.
Example 5.19. Consider again the logistic map
xk+1 = 4 xk
￾
1 − xk

.
First, it can be verified that this map is equivalent to
xk+1 = 2 xk (mod 1), (5.17)
since they both have the same (unique) solution
xk =
1
2
￾
1 − cos(2π2
k
y0)

,
where
y0 =
1
2π
cos−1
￾
1 − 2 x0

.
Then, one can show that the map (5.17) satisfies conditions (i)–(iii) in
Definition 5.6, as verified below:
(i) System (5.17) has a Lyapunov exponent λ = ln 2 > 0. Indeed,
λ = lim
k→∞
1
k
ln

 JkJk−1 · · · J2J1

 = lim
k→∞
1
k
ln 2k = ln 2 ,
where Jk = 2 for all k = 1, 2, . . . , and the modulo operation does not
change the derivative, except at the turning points, which does not affect the
concerned derivatives. Therefore, the map is sensitive to initial conditions
because its orbit is divergi162 Nonlinear Systems
(ii) Because of the mod-1 operation, the map (5.17) is equivalent to the
following double angle map from the unit circle to itself:
∠ xk+1 = 2 ∠ xk (mod 2π), (5.18)
where ∠ is the angle, and x0 is any initial point on the unit circle, as shown
in Fig. 5.30.
0
S
(r ≡ 1)
∠ x
1
Fig. 5.30 The double angle map from the unit circle to itself.
Since the angle is doubled on each iteration, and the number 2 is not an
integer multiplier of π, it is clear that for any nonempty open arc-segment S
on the circle, sooner or later (i.e. there is an index k), the point will fall into
the arc-segment: xk ∈ S. Therefore, this double angle map is topologically
transitive, so is the map (5.17).
(iii) Since Eq. (5.18) is 2π-periodic, one has
∠ xk+1+n = 2n∠ xk ± 2mπ , m, n, k = 0, 1, 2, . . . .
Thus, by letting k → ∞, one can see that for each given pair (n, m), this
equation yields an equilibrium point, ∠ x
∗
, satisfying
∠ x
∗ = 2n∠ x
∗ ± 2mπ , m, n, k = 0, 1, 2, . . . .
This gives
∠ x
∗ =
±2mπ
2
n − 1
, m = 0, 1, 2, . . . , 2
n − 1; n = 1, 2, . . . .
Therefore, the map has infinitely many periodic orbits of different periods,
all located on the unit circle. Moreover, as n, m → ∞, one can see that all
these periodic points become dense on the unit circle, almost uniformly, so
does the map (5.17).
For a higher-dimensional map: M : S → S, where S is a bounded and
compact set in Rn with n > 1, there is a definition of chaos introduced
in [Marotto (1978)], which generalizes an earlier definition in the sense ofNonlinear Dynamics: Bifurcations and Chaos 163
Li–Yorke [Li and Yorke (1975)] from 1-dimensional to higher-dimensional
maps.
Consider an n-dimensional discrete-time autonomous system,
xk+1 = f
￾
xk

, (5.19)
where f is a vector-valued continuous nonlinear function. Let f
′
(x) and
det(f
′
(x)) be the Jacobian of f at x and its determinant, respectively, and
let Br(x) be a closed ball of radius r > 0 centered at x in Rn.
Definition 5.7. An equilibrium point x
∗ of (5.19), assuming it exists, is
said to be a snapback repeller if
(i) there exists a real number, r > 0, such that f is differentiable with all
eigenvalues of f
′
(x) exceeding unity in absolute value for all x ∈ Br(x
∗
);
(ii) there exists an x
0 ∈ Br(x
∗
), with x
0 ̸= x
∗
, such that for some posi￾tive integer m, f
m(x
0
) = x
∗ and f
m(x
0
) are differentiable at x
0 with
det((f
m)
′
(x
0
)) ̸= 0.
The concept of snapback repeller is illustrated in Fig. 5.31.
0
f
f
m
Br(x∗
x
x
∗
)
f
(x) 
1
>1 
Fig. 5.31 The snapback repeller illustrated.
Theorem 5.5 (Marotto Theorem). If system (5.19) has a snapback
repeller then the system is chaotic in the sense of Li and Yorke, namely,
(i) there exists a positive integer n such that, for every integer p ≥ n,
system (5.19) has p-periodic points164 Nonlinear Systems
(ii) there exist a scrambled set (i.e. an uncountable invariant set) S con￾taining no periodic points, such that
(a) f(S) ⊂ S,
(b) for every y ∈ S and any periodic point x of (5.19):
lim sup
k→∞

 f
k
(x) − f
k
(y)

 > 0 ,
(c) for every x, y ∈ S with x ̸= y,
lim sup
k→∞

 f
k
(x) − f
k
(y)

 > 0 ;
(iii) there exists an uncountable subset S0 of S such that, for any x, y ∈ S0,
lim inf
k→∞

 f
k
(x) − f
k
(y)

 = 0 .Nonlinear Dynamics: Bifurcations and Chaos 165
Exercises
5.1 For each of the following equations, determine the type of bifurcation
when µ is varied, and sketch the corresponding bifurcation diagram:
x˙ = 1 + µ x + x
2
, x˙ = µ x + x
2
,
x˙ = x + µ x3
, x˙ = µ − 3 x
2
.
5.2 For each of the following equations, determine the type of bifurcation
when µ is varied, and sketch the corresponding bifurcation diagram:
x˙ = µ x − x(1 − x), x˙ = x − µ x(1 − x),
x˙ = x − µ x3
, x˙ = µ x −
x
1 + x
.
5.3 Sketch all the periodic solutions of the following system, and indicate
their stabilities:
x¨ + ˙x (x
2 + ˙x
2 − 1) + x = 0 .
5.4 Determine the limit cycles and their stabilities for the following two
systems:
(
ρ˙ = ρ (1 − ρ
2
) (9 − ρ
2
)
˙θ = 1
and
(
ρ˙ = ρ (ρ − 1) (ρ
2 − 2)
˙θ = 1 .
5.5 For the following systems, discuss their bifurcations and sketch their
phase portraits as µ varies:
(
x˙ = µ x − x
2
y˙ = − y
and (
x˙ = µ x + x
3
y˙ = − y .
5.6 For the following system, discuss its bifurcation and sketch its phase
portrait as µ varies:
(
x˙ = y − 2 x
y˙ = µ + x
2 − y .
5.7 Consider the system
(
x˙ = µ x − y + x y2
y˙ = x + µ y + y
3
.
Study its Hopf bifurcation as µ is varied to pass µ0 = 0. Is this
bifurcation supercritical or subcritical?166 Nonlinear Systems
5.8 Consider the following biased van der Pol oscillator:
x¨ + µ
￾
x
2 − 1

x˙ + x = c ,
where c is a constant. Find the curve in the µ–c plane on which Hopf
bifurcations occur.
5.9 Consider the following system:



x˙ =

√
1
2
z +
α
2

y − β x
y˙ =

√
1
2
z −
α
2

x − β y
z˙ =
√
2
￾
1 − xy 
(α, β > 0).
Use a typical quadratic Lyapunov function to argue that in a large
enough neighborhood of (0, 0, 0) there must be an attractor, and this
attractor is not (0, 0, 0).
5.10 Consider the skew tent map
xk+1 =
(
x/a if 0 ≤ x ≤ a
(1 − x)/(1 − a) if a < x ≤ 1 ,
with initial state x0 ∈ (0, 1) and parameter 0.5 ≤ a < 1.
(i) Pick arbitrarily x0 and a to generate a relatively long time series
of the map (e.g. 100,000 points). Then, calculate its Lyapunov
exponent λ. Verify if your result satisfies
0 < − ln (a) ≤ λ ≤ − ln (1 − a).
(ii) Pick an arbitrary x0 with a fixed a = 0.5. Find all the equilibrium
points of the map and classify their stabilities.
(iii) Play around, with different values of x0 and a ∈ [0.5, 1), to see if
you can get period-2, period-3, period-4 orbits, and even chaoChapter 6
Nonlinear Systems Control
This chapter briefly discusses the basic ideas, principles and general meth￾ods for designing controllers for nonlinear systems, including chaotic sys￾tems.
It should be made clear at the start that this chapter is not about
specific engineering design of any kind of controller with implementation
guidelines, but rather, only some mathematical descriptions of approaches
and methodologies, mostly based on the theories and techniques developed
in the previous chapters.
6.1 Feedback Control of Nonlinear Systems
Consider a general nonlinear dynamical system,
(
x˙(t) = f(x, u, t),
y(t) = h(x, u, t),
(6.1)
where x(t) is the system state vector, y(t) the output vector, and u(t) the
control input vector. Here, in a general discussion, once again it is assumed
that all the necessary conditions on the vector-valued functions f and h are
satisfied such that the system has a unique solution in a certain bounded
region of the state space for each given initial state x0 = x(t0), with initial
time t0 ≥ 0.
Given a reference signal, r(t), which can be either a constant vector (set￾point) or a function of time (target trajectory), the problem is to design a
controller in the state-feedback form
u(t) = g(x, t), (6.2)
or, sometimes, in the output-feedback form
u(t) = g(y, t),
167168 Nonlinear Systems
where g is a nonlinear (including linear) vector-valued function, such that
the controlled system
(
x˙(t) = f
￾
x, g(x, t), t
,
y(t) = h
￾
x, g(x, t)

,
(6.3)
can be driven by the feedback control g(x, t) to track the target:
limt→tf
||y(t) − r(t)|| = 0 , (6.4)
where the terminal time tf ≤ ∞ is predetermined according to the applica￾tion, typically tf = ∞ in theory, which means that the time is long enough
in practice, and || · || is the Euclidean norm of a vector.
The special case with r(t) ≡ c, a constant vector, is sometimes referred
to as a stabilization problem and, without loss of generality, one may set
c = 0 by a change of variables in the nonlinear systems setting.
Since the second equation in system (6.1) is merely a mapping, which
can be easily handled in general, it is ignored in the following discussion
by simply setting h(·) = I, the identity mapping, so that y = x, without
further discussion on output-feedback control in this chapter.
6.1.1 Engineering Perspectives on Controllers Design
From the general mathematical form of the nonlinear controller (6.2), it
might seem that this controller can be arbitrary in any form and of any
complexity. However, it is very important to point out that in a state￾feedback controller design, particularly in finding a nonlinear controller for
a given nonlinear system, one must bear in mind that the controller should
be (much) simpler than the given system to be practical, implementable
and cost-effective.
One might easily have a false impression that a nonlinear controller
could be in any formulation since it is nonlinear anyway. For example,
to design a nonlinear controller u(t) to drive the state vector x(t) of the
nonlinear system
x˙(t) = f(x(t), t) + u(t)
to track a target trajectory x
∗
(t), one might attempt to use
u(t) = x˙
∗
(t) − f(x(t), t) + K
￾
x(t) − x
∗
(t)

,
where K has all its eigenvalues with negative real parts. This controller
leads to the tracking-error dynamical system
e˙(t) = K e(t), with e(t) = x(t) − x
∗
(t),
yielding e(t) → 0, or x(t) → x
∗
(t), as t →Nonlinear Systems Control 169
Is there anything wrong mathematically? No.
Another example is: for a given nonlinear controlled system in the
canonical form



x˙ 1(t) = x2(t)
x˙ 2(t) = x3(t)
.
.
.
x˙ n−1(t) = xn(t)
x˙ n(t) = f
￾
x1(t), · · · , xn(t)

+ u(t),
suppose that one wants to find a nonlinear controller u(t) to drive the state
vector x(t) = 
x1(t) · · · xn(t)
⊤
to a target state, xe, i.e.
x(t) → xe as t → ∞.
It is mathematically straightforward to use the controller
u(t) = − f
￾
x1(t), · · · , xn(t)

+ kc
￾
xn(t) − xen

with an arbitrary constant kc < 0. This controller yields
x˙ n(t) = kc
￾
xn(t) − xen

,
which guarantees that en(t) := xn(t) − xen → 0 as t → ∞ since ˙en(t) =
kcen(t) and kc < 0. Indeed, the resulting n-dimensional controlled system
is a completely controllable linear system, x˙ = A x + b u, with
A =





0 1 0 · · · 0
0 0 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
. 1
0 0 0 · · · 0





and b =





0
.
.
.
0
1





.
Therefore, a suitable constant control gain exists for the state-feedback
controller, u = kc xn, to achieve that x(t) → xe as t → ∞.
Is there anything wrong mathematically? No.
Just to show one more example, suppose that one would like to find a
nonlinear controller, say uk in the discrete-time setting, to drive the state
vector xk of a given nonlinear control system of the form
xk+1 = fk(xk) + uk
to a target trajectory satisfying a predetermined target dynamical form of
xk+1 = ϕk(xk), then mathematically it is very easy to use
uk = ϕk(xk) − fk(170 Nonlinear Systems
which will bring the original system state xk to the target trajectory in one
step.
Is there anything wrong mathematically? No.
To this end, all such (exaggerated) examples seem to suggest a “univer￾sal controller design methodology” that works for any given system with
any given target, and the argument is mathematically correct. However, as
all practitioners know, a fatal problem with such a “design” is that the con￾troller removes part of the given system, or is even more complicated than
the given system, which has no practical value: every term in a mathemati￾cal model of a practical system has a specific physical meaning, representing
a resistor, a spring, a neuron, etc., which is a part of the given circuit, ma￾chine or bio-system. They should be controlled but not be removed by
the designed controller. For instance, if a term R represents a resistor in
the mathematical model of a circuit device, then cancelling this R by the
controller simply means to remove the resistor from the given device.
Recall that the conventional linear PID (proportional-integral￾derivative) controllers never remove any part of the given system (usually
called a plant), and this is why they can be commonly used in the industry.
Furthermore, in practice, due to the ubiquitous noise, uncertainty, and
parameter mismatch, a precise term-cancellation is generally impossible.
As a result, the consequent analysis and implementation of the above
cancellation-based design cannot be carried out.
Therefore, a nonlinear controller design is expected to present a simplest
possible and cost-effective controller: if a linear controller can be found to do
the job, use a linear controller; otherwise, try a simple nonlinear controller
with a piecewise or a quadratic nonlinearity and so on. Also, usually full
state feedback information is not available in practice, so one should try to
design a controller using only output feedback (i.e. partial state feedback),
which means the second equation of (6.1) is essential, although this notion
will not be further discussed in this chapter. Whether or not one can
find a simple, easily implementable, low-cost, and effective controller for a
given nonlinear system requires both good theoretical knowledge and design
experience.
6.1.2 A General Approach to Controllers Design
Returning to the central theme of feedback control for the general nonlinear
dynamical system (6.1)–(6.4), a basic idea is first outlined here for tracking
control.Nonlinear Systems Control 171
Let the target trajectory (or set-point) be xe(t), and assume that it is
differentiable, so that by denoting
˙xe(t) = z(t), (6.5)
one can subtract this equation (6.5) from the first equation of (6.3), to
obtain the error dynamical system
e˙ = F(e, t), (6.6)
where e = x − xe and
F(e, t) := f
￾
x, g(x, t), t
− z(t).
If the target trajectory xe is a periodic orbit of the given system (6.1), that
is, if it satisfies
˙xe = f
￾
xe, 0, t
, (6.7)
then similarly a subtraction of (6.7) from the first equation of (6.1) gives
e˙ = F(e, t), (6.8)
where e = x − xe and
F(e, t) := f
￾
x, g(x, t), t
− f
￾
xe, 0, t
.
In either case, e.g. in the second case, which is more difficult in general,
the goal of design is to determine the controller u(t) = g(x, t) such that
limt→∞
||e(t)|| = 0 , (6.9)
which implies that tracking control is achieved:
limt→∞
||x(t) − x˜(t)|| = 0 . (6.10)
It is now clear from (6.9) and (6.10) that if zero is a fixed point of the
nonlinear system (6.8), then the original controllability problem has been
converted to an asymptotic stability problem of this fixed point. Thus, the
Lyapunov first and second methods may be applied or modified to obtain
rigorous mathematical analysis for the controller’s design. This is further
discussed in more detail below.
6.2 Feedback Controllers for Nonlinear Systems
This section discusses how a linear or nonlinear controller may be designed
for controlling a nonlinear dynamical system based on the rigorous Lya￾punov function meth172 Nonlinear Systems
6.2.1 Linear Controllers for Nonlinear Systems
Given the Lyapunov first method for nonlinear autonomous systems and, if
applicable, the linear stability theory for nonlinear nonautonomous systems
with weak nonlinearities (see Sec. 3.1), we show that a linear feedback
controller may be able to control a nonlinear dynamical system in a rigorous
way.
Take the nonlinear, actually chaotic Chua circuit shown in Fig. 1.26 as
an example. This circuit is a simple yet very interesting electrical system
that displays many typical bifurcation and chaotic phenomena.
Example 6.1. The Chua circuit is shown in Fig. 6.1 again. It consists of
one inductor L, two capacitors C1 and C2, one linear resistor R, and one
nonlinear resistor g, which is a nonlinear function of the voltage across its
two terminals: g = g
￾
VC1
(t)

.
R
L
C2
VC2 VC1
g(·)
C1
iL
+ +
– –
Fig. 6.1 The Chua circuit.
Let iL(t) be the current through the inductor L, and VC1
(t) and VC2
(t)
be the voltages across C1 and C2, respectively, as shown in the circuit
diagram. Then, it follows from Kirchhoff’s laws that
C1
d
dt VC1
(t) = 1
R

VC2
(t) − VC1
(t)

+ g
￾
VC1
(t)

,
C2
d
dt VC2
(t) = 1
R

VC1
(t) − VC2
(t)

+ iL(t),
L
d
dt iL(t) = − VC2
(t).
In the discussion of controlling the circuit behavior, it turns out to be
easier to first apply the nonlinear transformation
x1(t˜) = VC1
(t), x2(t˜) = VC2
(t), x3(t) = R iL(t), t˜= t/(C2Nonlinear Systems Control 173
to reformulate the circuit equations to the following dimensionless form:



x˙ = p [ − x + y − f(x) ]
y˙ = x − y + z
z˙ = − q y ,
(6.11)
where p > 0, q > 0, and f(x) = R g(x) is a nonlinear function of the form
f(x) = m0 x +
1
2
(m1 − m0) (|x + 1| − |x − 1|),
in which m0 < 0 and m1 < 0.
It is known that, with p = 10.0, q = 14.87, m0 = −0.68, and m1 =
−1.27, the circuit displays a chaotic attractor (see Fig. 6.2) along with a
limit cycle of large magnitude. This limit cycle is a large unstable periodic
orbit, hence unable to be shown in the simulation figure, which encompasses
the non-periodic attractor. It is generated due to the eventual passivity of
the transistors.
0.5
0
–0.5
–1
10
5
0
–5
z(t) –10 x(t)
y(t)
–6
–4
–2
0
2
4
1
Fig. 6.2 The double scroll chaotic attractor of the Chua circuit.
Now, let (¯x, y, ¯ z¯) be the unstable periodic orbit of the Chua circuit
(6.11). Then, the circuit trajectory (x, y, z) can be driven from any (finite)
state to reach this periodic orbit by a simple linear feedback control of the
form


u1
u2
u3

 = −K


x − x¯
y − y¯
z − z¯

 = −


k11 0 0
0 k22 0
0 0 k33




x − x¯
y − y¯
z − z¯

 (6.12)
with
k11 ≥ −pm1 , k22 ≥ 0 , and k33 ≥ 0 ,
where the control can be applied to the trajectory at any time.174 Nonlinear Systems
A mathematical verification is given as follows. First, one observes that
the controlled circuit is



x˙ = p
￾
− x + y − f(x)

− k11 (x − x¯),
y˙ = x − y + z − k22 (y − y¯),
z˙ = −q y − k33 (z − z¯).
(6.13)
Since the unstable periodic orbit (¯x, y, ¯ z¯) is itself a (periodic) solution of
the circuit, one has



x¯˙ = p
￾
− x¯ + ¯y − f(¯x)

,
y¯˙ = ¯x − y¯ + ¯z ,
z¯˙ = −q y , ¯
(6.14)
so that a subtraction of (6.14) from (6.13), with the new notation
X = x − x , Y ¯ = y − y , ¯ and Z = z − z , ¯
yields



X˙ = p
￾
− X + Y − fe(x, x¯)

− k11X ,
Y˙ = X − Y + Z − k22Y ,
Z˙ = −q Y − k33Z ,
(6.15)
where
fe(x, x¯) =



m0(x − x¯) x ≥ 1, x¯ ≥ 1
m0x − m1x¯ + m1 − m0 x ≥ 1, −1 ≤ x¯ ≤ 1
m0(x − x¯) + 2(m1 − m0) x ≥ 1, x¯ ≤ −1
m1x − m0x¯ − m1 + m0 −1 ≤ x ≤ 1, x¯ ≥ 1
m1(x − x¯) −1 ≤ x ≤ 1, −1 ≤ x¯ ≤ 1
m1x − m0x¯ + m1 − m0 −1 ≤ x ≤ 1, x¯ ≤ −1
m0(x − x¯) − 2(m1 − m0) x ≤ −1, x¯ ≥ 1
m0x − m1x¯ − m1 + m0 x ≤ −1, −1 ≤ x¯ ≤ 1
m0(x − x¯) x ≤ −1, x¯ ≤ −1
with m1 < m0 < 0.
Consider the following Lyapunov function for system (6.15):
V (X, Y, Z) = q
2
X2 +
p q
2
Y
2 +
p
2
Z
2
.
It is clear that V (0, 0, 0) = 0 and V (X, Y, Z) > 0 for all X, Y, Z that are not
simultaneously zero. On the other hand, since p, q > 0 and k22, k33 ≥ 0,Nonlinear Systems Control 175
follows that
V˙ = q XX˙ + p q Y Y˙ + p ZZ˙
= q X￾
− p X + p Y − p fe(x, x¯) − k11X

+p q Y ￾
X − Y + Z − k22 Y

+ p Z ￾
− q Y − k33 Z

= −p q X2 + 2 p q XY − p q Y 2 − p q Xfe(x, x¯)
−q k11 X2 − k22 p q Y 2 − k33 p Z2
= −p

q (X − Y )
2 + q k22 Y
2 + k33 Z
2

− q
￾
p Xfe(x, x¯) + k11 X2

≤ 0
for all X, Y , and Z, if
p Xfe(x, x¯) + k11 X2 ≥ 0 (6.16)
for all x and ¯x. To find the conditions under which (6.16) holds, by a careful
examination of the nine possible cases for the function ˜f(x, x˜) shown above,
the following common condition can be obtained:
k11 ≥ max 
− p m0, −p m1
	
= − p m1 , (6.17)
in which m1 < m0 < 0. This condition guarantees the inequality (6.16).
Hence, if the stated conditions are satisfied, the equilibrium point (0, 0, 0)
of the controlled circuit (6.15) is globally asymptotically stable, so that
|X| → 0 , |Y | → 0 , |Z| → 0 as t → ∞,
simultaneously. That is, starting with the feedback control at any time on
the chaotic trajectory, one has
limt→∞
|x(t) − x¯(t)| = 0 , limt→∞
|y(t) − y¯(t)| = 0 , limt→∞
|z(t) − z¯(t)| = 0 .
The tracking result is visualized in Fig. 6.3.
6.2.2 Nonlinear Controllers for Nonlinear Systems
It is not always possible to use a linear controller to control a nonlinear
dynamical system. In fact, nonlinear feedback controllers are necessary in
most cases.
To show one example of a nonlinear controller design, consider the
chaotic Duffing oscillator (5.10) again.
Example 6.2. The Duffing oscillator is described by

x˙ = y ,
y˙ = − p2 x − x
3 − p1 y + q cos(ωt),
(6.18)
where p1, p2, q, and ω are constant paramet176 Nonlinear Systems
4
2
0
y
z
x
–2
–4
4
2
0
–2
–2
0
2
4
–4
–4
Fig. 6.3 Tracking the unstable limit cycle of the Chua circuit.
With the parameters set p1 = 0.4, p2 = −1.1, q = 2.1 (or q = 1.8),
and ω = 1.8, the Duffing system has a chaotic attractor (see Fig. 5.24).
The Duffing system has some inherent unstable limit cycles, which however
do not have analytic expressions and even cannot be accurately displayed
graphically due to their instability.
For this system, suppose that for controlling its chaotic trajectory to
one of its inherent unstable periodic orbits, we design a feedback controller.
Denote by (¯x, y¯) = (¯x(t), y¯(t)) the target trajectory, one of its (un￾known) unstable periodic orbits. The goal is to control the system trajec￾tory to track this periodic trajectory, namely,
limt→∞
|x(t) − x¯(t)| = 0 and limt→∞
|y(t) − y¯(t)| = 0 . (6.19)
Denote the nonlinear feedback controller by u(t) = h(t; x, x¯), which is
to be determined. By adding the controller to the second equation of the
original system, one obtains the following controlled Duffing system:

x˙ = y ,
y˙ = − p2 x − x
3 − p1 y + q cos(ωt) + h(t; x, x¯).
(6.20)
Since the periodic orbit (¯x, y¯) is itself a solution of the original system,
subtracting (6.18), with (x, y) replaced by (¯x, y¯) therein, from system (6.20),
one obtains the tracking error system

X˙ = Y ,
Y˙ = − p2 X −
￾
x
3 − x¯
3

− p1 Y + h(t; x, x¯),
(6.21Nonlinear Systems Control 177
where
X = x − x¯ and Y = y − y . ¯
Next, observe that the controlled Duffing system (6.21) is a nonlinear,
nonautonomous system; therefore, the Lyapunov first method may not ap￾ply. In this particular case, however, the Lyapunov second method can be
applied. Indeed, a nonlinear controller h(x) can be designed as follows:
h(x) = k X + 3 ¯x
2 X + 3 ¯x X2
,
where k is a constant to be determined. Note that the Duffing system is
third-order and nonautonomous but this controller is autonomous and it
does not cancel any term of the original system. Consequently, the con￾trolled system (6.21) becomes

X˙ = Y
Y˙ = −
￾
k + p2

X − p1 Y − X3
.
(6.22)
Consider the Lyapunov function
V (X, Y ) = k + p2
2
X2 +
1
4
X4 +
1
2
Y
2
,
which satisfies
V˙ = (k + p2)XX˙ + X3X˙ + Y Y˙
= (k + p2)XY + X3Y + Y [−(k + p2)X − p1Y − X3
]
= −p1Y
2
.
It then follows from the LaSalle Invariance Principle (see Theorem 2.10)
that if Y = 0, but X ̸= 0, then (6.22) yields
Y˙ = −(k + p2)X − X3
.
It is thus clear that if k + p2 > 0, which gives a criterion for choosing
the parameter k, then (i) for X > 0, Y < ˙ 0; (ii) for X < 0, Y > ˙ 0.
Therefore, the system solution trajectory will not stay on the X-axis unless
(X, Y ) = (0, 0). This means that the zero fixed point of the controlled
Duffing system (6.22) is asymptotically stable, so that X → 0 and Y → 0
as t → ∞, namely the goal
|x − x¯| → 0 and |x˙ − x¯˙| → 0 (t → ∞)
is achieved. Numerically, tracking is achieved with the result visualized in
Fig. 6.4.
Finally, it should be emphasized that this simple example is only used
to illustrate some basic idea and approach in designing a simple nonlinear
controller for a given complex nonlinear system, which is by no means the
best possible or most effective design. In fact, in this example, tracking
to which periodic trajectory cannot be specified beforehand. Nevertheless,
sometimes it would be sufficient if the target is just a periodic trajectory,
of whatever period, for which the goal of control has been achieved178 Nonlinear Systems
0
4
2
0
–2
–4
1
0.5
0
–0.5
–1
–2 –1 1 2 0.5 1.5 1 x(t)
x
chaotic solution period-1 solution
Fig. 6.4 Tracking an unstable limit cycle of the Duffing oscillator.
6.2.3 General Ideas for Nonlinear Controllers Design
Consider a general nonlinear and nonautonomous controlled system,
x˙ = f(x, t) + u(t), (6.23)
which is assumed to possess a periodic orbit x˜ of period tp > 0: x˜(t+tp) =
x˜(t), 0 ≤ t < ∞. The goal is to design a feedback controller of the form
u(t) = K
￾
x − x˜

+ g
￾
x − x˜, t
, (6.24)
where K is a constant matrix and g is a (simple) nonlinear vector-valued
function such that the trajectory of the controlled system can track the
target periodic orbit x˜, namely,
limt→∞
||x(t) − x˜(t)|| = 0 . (6.25)
As usual, the controller (6.24) is added to the given system (6.23) to
obtain the controlled system
x˙ = f(x, t) + u = f(x, t) + K
￾
x − x˜

+ g
￾
x − x˜, t
. (6.26)
Since the target periodic orbit x˜ is itself a solution of the original system,
it satisfies
x˜˙ = f(˜x, t). (6.27)
A subtraction of (6.27) from (6.26) gives
X˙ = F(X, t) + K X + g(X, t), (6.28)
where
X = x − x˜ and F(X, t) = f(x, t) − f(x˜, t).
It is clear that F(0, t) = 0 for all t ∈ [0,Nonlinear Systems Control 179
Now, Taylor-expand the right-hand side of the controlled system (6.28)
at X = 0 (i.e. at x = x˜), and suppose that the nonlinear controller is
designed such that it satisfies g(0, t) = 0. Then, one has
X˙ = A(x˜, t)X + h(X, K, t), (6.29)
where A(x˜, t) = ∂F(X, t)/∂X


X=0 and h(X, K, t) is the rest of the Taylor
expansion, which is a function of t, K, and higher-order terms o(X).
To this end, the design is to determine both the constant control gain
matrix K and the nonlinear controller g(X, t) based on the linearized model
(6.29), such that X → 0 (i.e. x → x˜) as t → ∞.
The following criteria follow from Theorems 3.1 and 3.4.
Theorem 6.1. Suppose that, in system (6.29), h(0, K, t) = 0 and A(x˜, t) =
A is a constant matrix with all eigenvalues having negative real parts. If
lim
||X||→0
||h(X, K, t)||
||X|| = 0
uniformly with respect to t ∈ [0, ∞), where || · || is the Euclidean norm,
then the controller u(t) defined in (6.24) will drive the trajectory x of the
controlled system (6.28) to the target periodic trajectory x˜ as t → ∞.
Next, recall the concept of Floquet multipliers for periodic systems in￾troduced in Definition 3.1.
Theorem 6.2. In system (6.29), suppose that h(0, K, t) = 0 and that
h(X, K, t) and ∂h(X, K, t)/∂X are both continuous in a bounded region
with ||X|| < ∞. Assume also that
lim
||X||→0
||h(X, K, t)||
||X|| = 0 ,
uniformly with respect to t ∈ [0, ∞). If all the multipliers of the system
(6.29) satisfy
|λi
| < 1 , i = 1, . . . , n , ∀ t ∈ [0, ∞),
then the nonlinear controller (6.24) will drive the trajectory x of the original
controlled system (6.29) to the target periodic trajectory x˜ as t → ∞.
6.3 More about Nonlinear Controllers Design
6.3.1 An Illustrative Example
Consider a class of Li´enard equations of the form
x¨ + b( ˙x) + c(x) = 0 , (6.30)180 Nonlinear Systems
which is assumed to have a zero fixed point (x
∗
, x˙
∗
) = (0, 0), where b(·) and
c(·) are nonlinear functions satisfying
(i) ˙x b( ˙x) > 0 for all ˙x ̸= 0;
(ii) x c(x) > 0 for all x ̸= 0.
Examples of this type of equations include:
x¨ + ˙x
3 + x
5 = 0
x¨ +
κ
m
x˙
3 +
g
2
m ℓ2
sin(x) = 0 (pendumu: −π < x < π)
x¨ − µ
￾
1 − x
2

x˙ + x = 0 (van der Pol: µ > 0 , |x| > 1).
For this class of systems, an effective Lyapunov function is the total energy
function
V (x, x˙) = 1
2
x˙
2
(t) + Z x(t)
0
c(σ) dσ .
Under conditions (i) and (ii) above, V (x, x˙) > 0 for (x, x˙) ̸= (0, 0), and
V˙ = ˙x x¨ + c(x) ˙x
= − x b ˙ ( ˙x) − x c ˙ (x) + c(x) ˙x
= − x b ˙ ( ˙x)
< 0 for all ˙x ̸= 0 .
Clearly, the only chance for V˙ = 0 while ˙x ̸= 0 is when x ≡ 0, i.e. on the
x-axis. But this axis is not a region, so the Lyapunov instability theorems
studied in Sec. 2.4 do not apply. Besides, whenever the system orbit is
located on the x-axis, since ˙x ̸= 0, it will leave this axis immediately, and
right after that moment, V <˙ 0, which forces the orbit to move toward the
origin. This implies that the zero fixed point of the system is asymptotically
stable. If, moreover,
Z x(t)
0
c(σ) dσ → ∞ as |x| → ∞ ,
then the asymptotic stability is global.
Now, suppose that conditions (i) and (ii) above are not simultaneously
satisfied. In this case, the objective is to design a controller, u, to be added
to the right-hand side of the given system, namely
x¨ + b( ˙x) + c(x) = u , (6.31)
so as to stabilize the zero fixed pointNonlinear Systems Control 181
How can this be accomplished? It is quite natural to try a controller of
the form
u = u1( ˙x) + u2(x),
where u1(·) and u2(·) may be nonlinear, both are to be determined. The
controlled system becomes
x¨ +

b( ˙x) − u1( ˙x)

+

c(x) − u2(x)

= 0 .
Thus, it is clear that one has to find u1 and u2 such that
(i’) ˙x [b( ˙x) − u1( ˙x)] > 0 for all ˙x ̸= 0;
(ii’) x [c(x) − u2(x)] ≥ 0 for all x ̸= 0.
The following example illustrates how these conditions can be satisfied in
the design.
Example 6.3. Consider the system
x¨ − x˙
3 + x
2 = 0 ,
which does not simultaneously satisfy conditions (i) and (ii). The objective
is to design a controller u = u1( ˙x) + u2(x) that can ensure both
(
x˙

− x˙
3 − u1( ˙x)

> 0 ( ˙x ̸= 0)
x

x
2 − u2(x)

≥ 0 (x ̸= 0).
For this purpose, it might be mathematically straightforward to choose
u1( ˙x) = − 2 ˙x
3
and u2(x) = x
2 − x ,
which yields
u = u1 + u2 = − 2 ˙x
3 + x
2 − x .
However, this controller is not desirable since it is even more complicated
than the given system; it basically cancels the given nonlinearity and then
adds a stable linear portion back to the resulting system. In so doing, it
actually replaced the given system by a stable one, which is impractical as
discussed in Sec. 6.1.1.
To design a simple and easily implementable controller can sometimes
be quite technical. Nevertheless, for the purpose of illustration, a slightly
simpler design for this particular example could be
u1( ˙x) = − x˙
3 − x˙ and u2(x) = − x |x| ,
which ensures that
(
x˙

− x˙
3 − u1( ˙x)

= ˙x
2 > 0
x

x
2 − u2(x)

= x
2

x + |x|

≥ 0 ,
for all x ̸= 0 and ˙x ̸= 0.182 Nonlinear Systems
6.3.2 Adaptive Controllers
The above Lyapunov design method is also useful for controllers design for
uncertain systems, where some system parameters are unknown, therefore
have to be estimated. The designed controllers using the estimated param￾eters instead of the unknown parameters, which are not usable, are referred
to as adaptive controllers.
6.3.2.1 Observer-Based Adaptive Control
The following very simple example is used to explain the basic idea and
approach.
Example 6.4. Consider an uncertain linear control system,
x˙ + α x = u ,
where α is an unknown constant. The objective is to design a controller,
u, such that the controlled state x(t) → 0 as t → ∞.
Case 1.
If |α| ≤ α, where the upper bound constant α is known, which may be
conservative, then the linear controller
u = − 2 α x
can do the job. Indeed, the controlled system becomes
x˙ +
￾
2 α + α

x = 0 ,
which has a solution
x(t) = x0 e
−(2α+α)t → 0 (t → ∞).
Case 2.
If no such an upper bound α is known, then usually no linear controller
can be designed, since whatever linear controller u = −kx is used, the
controlled system orbit will be
x(t) = x0 e
−(k+α)t
,
which does not provide any guideline for the determination of the constant
gain k since α is unknown.
In this case, one has to resort to a different methodology. Estimation
of α on the line is often necessary, for which the observer is a useful toolNonlinear Systems Control 183
Suppose that αb is an estimate of the unknown α, which satisfies a so￾called observer equation,
˙xb = f
￾
x, αb

,
where f(·, ·) is to be determined. Use αb to design the controller and denote
u = u
￾
x, αb

.
To find the observer f(·, ·), one may consider the Lyapunov function
V
￾
x, αb

=
1
2
x
2 +
1
2
￾
αb − α
2
,
and then look for both f(·, ·) and u(·, ·) to ensure that
V˙ ≤ − c x2 < 0
for some constant c > 0. Here, one may use some class-K functions (e.g.
γ(|x|
2
) := c |x|
2
; see Sec. 2.2). Since it is desirable to have
V˙ = x x˙ +
￾
αb − α

˙αb = x (u − α x) + ￾
αb − α

˙αb ≤ − c x2
,
or
x u + αb
˙αb − α
￾
x
2 + ˙αb

≤ − c x2
,
one natural choice for the observer equation is
˙αb = − x
2
.
Its solution is
αb(t) = αb(0) −
Z t
0
x
2
(τ ) dτ ,
where αb(0) may be chosen to be zero. Thus, what is wanted becomes
V˙ = x u − ˙α x b
2 ≤ − c x2
,
which suggests the following form for the controller:
u =
￾
αb − c

x (c > 0).
Usually, it is preferred that the controller can be a negative feedback. For
this purpose, c can be any constant satisfying |αb(t)| ≤ c for all t ≥ 0.
The entire adaptive control system based on this observer design can be
implemented as shown in Fig. 6.5.
The methodology discussed above can be further extended to adaptive
control of higher-dimensional uncertain linear systems, and even nonlin￾ear systems. Only the simple single-input single-output (SISO) higher￾dimensional linear systems and 1-dimensional nonlinear systems are dis￾cussed in the following. The general multiple input multiple out (MIMO)
case is not further studied in this t184 Nonlinear Systems
c
x x u x x2 α
−
−
+
1
( )2
s + α
1
s
1 ˙ α α
˙
Fig. 6.5 Implementation of the observer-based adaptive control system.
6.3.2.2 Adaptive Control of Uncertain Linear Systems
The concept of a positive real transfer function (for SISO systems) and
matrix (for MIMO systems) is useful.
Definition 6.1. A transfer function H(s) of an SISO linear system is said
to be positive real if
Re{H(s)} > 0 for all Re{s} > 0 ,
and is said to be strictly positive real if
Re{H(s − ε)} > 0 for all Re{s} ≥ ε > 0 .
For instance, H(s) = 1/s is positive real but not strictly, and H(s) =
1/(s + α) with α > 0 is strictly positive real.
Lemma 6.1 (Kalman–Yakubovich–Popov Lemma). Consider the
SISO LTI system

x˙ = Ax + bu
y = c
⊤x .
Its transfer function
H(s) = c
⊤[sI − A]
−1b
is strictly positive real if and only if there exists positive definite and sym￾metrical matrices P and Q such that

A⊤P + P A = Q
Pb = c .
Proof. See [Khalil (1996)]: pp. 240–241.Nonlinear Systems Control 185
Theorem 6.3. Suppose that the SISO LTI system

x˙ = Ax + bu
y − y˜ = c
⊤x
has a strictly positive real transfer function, H(s), and contains an unknown
parameters vector, θ, where y˜ = ˜y(t) is a given target trajectory for tracking.
Let θb = θb(t) be an estimate of θ, generated by the following observer, with
ey := y − y . ˜
If, in the frequency domain after taking the Laplace transformations,
Ey(s) = L{ey(t)} = H(s)L
n
α θb⊤v(t)
o
for some vector-valued function v(t) and an unknown constant α with
known (positive or negative) sign, then using an observer for estimating
θ, in the form of
˙
θb = −sign{α} κ ey(t)v(t),
where κ > 0 is a constant and θb is bounded, the controller
u(t) = α θb⊤v(t)
will yield a globally bounded error trajectory, ey(t). If, furthermore, ||v|| ≤
M < ∞, then
ey(t) → 0 namely y(t) → y˜(t) (t → ∞).
Proof. The controlled system is
(
x˙ = Ax + b

α θb⊤v(t)

ey = c
⊤x .
It follows from Lemma 6.1 that, since H(s) is strictly positive real, there
exist positive definite and symmetrical matrices P and Q such that

A⊤P + P A = Q
Pb = c .
Construct a Lyapunov function
V (x, θb) = x
⊤Px +
|α|
κ
θb⊤θ . b186 Nonlinear Systems
Then, one has
V˙ = x
⊤Px˙ + x˙
⊤Px +
|α|
κ
￾˙
θb
⊤
θb+
￾
θb
⊤ ˙
θb

= x
⊤P

Ax + b α θb⊤v(t)

+

Ax + b α θb⊤v(t)
⊤
Px
+
|α|
κ

−2 sign{α} κ v
⊤θb

= x
⊤(P A + A⊤P)x + 2 x
⊤Pbαθb⊤v − 2 θb⊤α ey v
= x
⊤(−Q)x + 2 x
⊤cαθb⊤v − 2θb⊤α c
⊤x v
= −x
⊤Qx
< 0 (for all x ̸= 0).
Therefore, the system defined by



x˙ = Ax + b

α θb⊤v(t)

˙
θb = −sign{α} κ c
⊤xv(t)
is globally asymptotically stable about its zero fixed point (x
∗
, θb∗
). If,
furthermore, ||v|| ≤ M < ∞, then
ey = c
⊤x → 0 (t → ∞),
as claimed.
6.3.2.3 Adaptive Control of Uncertain Nonlinear Systems
The above methodology can also be extended to some uncertain nonlinear
systems, as shown by the following simple example.
Example 6.5. Consider an uncertain nonlinear system,
x˙ + α g(x) = u , g(0) = 0 ,
where α is an unknown constant and g(·) is a given nonlinear function. The
objective is to design a controller, u = u(t), to derive x(t) → 0 as t → ∞.
To have some mathematical idea about the controller design in the case
that α is known, one may attempt to use the controller
u = α g(x) − cx (c > 0),
which leads to ˙x+cx = 0 with a solution x(t) = x0e
−ct → 0 as t → ∞. This
controller is clearly impractical since it simply cancels the nonlinear term,
but it offers some idea for a practical design as further discussed beloNonlinear Systems Control 187
To follow the above naive idea, but now α is unknown, so a simple
controller seems impossible. In this situation, perhaps the simplest but
acceptable choice could be to design an adaptive controller of the form
u = α g b (x) − cx (c > 0),
where αb is an estimate of the unknown α. Then, the controlled system is
x˙ + eα g(x) + cx = 0 ,
where eα = α − αb is the estimation error.
Try the Lyapunov function
V (x, eα) = 1
2
x
2 +
1
2β
e
2
α (β > 0),
which gives
V˙ = xx˙ +
1
β
eαe˙α
= x(−cx − eαg(x)) + 1
β
eαe˙α
= −cx2 − eα

xg(x) −
1
β
e˙α

.
Hence, one may design an observer to estimate α, such that
˙αb = −βxg(x),
which yields
e˙α = − ˙αb = βxg(x),
so that
V˙ = −cx2 < 0 for all x ̸= 0 .
Thus, the controller is obtained as
u = αgb (x) − cx (c > 0),
where
αb(t) = αb(0) − β
Z t
0
x(τ )g(x(τ ))dτ ,
with αb(0) = 0 and an arbitrary β > 0.188 Nonlinear Systems
6.3.3 Lyapunov Redesign of Nonlinear Controllers
Consider a nominal model of nonlinear control systems,
x˙ = f(x, t) + G(x, t)u , (6.32)
where f and G are nonlinear vector- or matrix-valued functions satisfying
f(0, t) = 0 and G(0, t) = 0 for all t ≥ t0, with a state-feedback controller
u = u1(x, t)
to be designed.
Suppose that the controller stabilizes the nominal system, such that the
controlled system
x˙ = f(x, t) + G(x, t)u1(x, t)
is uniformly asymptotically stable about its zero fixed point.
In this case, there exists a Lyapunov function, V (x, t), and three class-K
functions, α(·), β(·) and γ(·), such that (see Sec. 2.2)
α(||x||) ≤ V (x) ≤ β(||x||)
and
V˙ (x, t) ≤ −γ(||x||)
for all x ∈ D, the defining domain of the system, which contains the origin.
Now, suppose that the nominal system is perturbed by some uncertainty,
in the form of an external input, leading to
x˙ = f(x, t) + G(x, t)[u + δ(x, u, t)] , (6.33)
where δ is an unknown vector-valued function, which is small such that the
perturbed system (6.33) remains uniquely solvable under the original initial
conditions. Further, suppose that, for any control input u = u1 + u2, with
u2 satisfying
||δ((x, u1 + u2, t)|| ≤ h(x, t) + c||u2||
for a non-negative continuous function h(·, t), which need not be small in
magnitude, with a constant 0 ≤ c < 1.
Then, design u2 such that the controller u = u1 + u2 can stabilize
the perturbed system (6.33). This design of the additional controller u2 is
referred to as a redesign of the first design of the controller u = u1.
To complete the redesign, start with
x˙ = f(x, t) + G(x, t)[u + δ(x, u, t)]
= f(x, t) + G(x, t)u1 + G(x, t)[u2 + δ(x, u1 + u2, t)] .Nonlinear Systems Control 189
If the perturbed system (6.33) can be stabilized then there exists a
Lyapunov function, Ve(x, t), associated with the perturbed system, which
reduces to the Lyapunov function V (x, t) for the nominal system when
δ = 0. This Lyapunov function Ve(x, t) satisfies
α(||x||) ≤ Ve(x) ≤ β(||x||),
for some class-K functions α and β that satisfy both of the above two
inequalities.
It follows that
˙
Ve =
∂Ve
∂t +
∂Ve
∂x
(f + Gu1) + ∂Ve
∂x
(u2 + δ)
=
∂V
∂t +
∂V
∂x
(f + Gu1) + ∂Ve
∂x
(u2 + δ)
≤ −γ(||x||) + ∂Ve
∂x
(u2 + δ)
for all x ∈ D. Denote w⊤ =

∂V /∂ e x

G and observe that
w⊤(u2 + δ) = w⊤u2 + w⊤δ
≤ w⊤u2 + ||w|| · ||δ||
≤ w⊤u2 + ||w|| · [h(x, t) + c||u2||] .
Then, choose
u2 ≤ −
ρ(x, t)
1 − c
·
w
||w|| ,
where h(x, t) ≤ ρ(x, t) for all x ∈ D component-wise for all t ≥ t0. It
follows that
w⊤(u2 + δ) ≤
ρ(x, t)
1 − c
||w|| + ||w||ρ(x, t) + c
1 − c
||w||ρ(x, t) ≤ 0 ,
implying that
˙
Ve(x, t) ≤ −γ(||x||).
This guarantees the uniform asymptotic stability of the perturbed system
(6.33) about its zero fixed point.
Example 6.6. Consider the nonlinear controlled pendulum equation

x˙ = y
y˙ = −
κ
m y − a sin(x) + b y
with a = g/ℓ, where the gravity g, the length ℓ and the coefficient b are all
constants.190 Nonlinear Systems
Following the above discussions to design a controller that can stabilize
this pendulum system, try
u1 = b u = a sin(x) + k1x + k2y
with k1 and k2 being chosen such that the matrix 
0 1
k1 k2 − κ/m 
is stable.
Now, suppose that the pendulum system contains some uncertainties,
so that the constants a and b are unknown hence cannot be used for the
controller.
Let the perturbed parameters be ˜a and ˜b. Then, it can be verified that
the above perturbation term δ is given by
δ(x, t) = a˜b − ab˜
b
sin(x) +
˜b − b
b
(u1 − a sin(x)),
where x = [x y]
⊤. Therefore,
||δ(x, u1 + u2)||
≤



a˜b−ab˜
b


 ||x|| +



˜b−b
b


 ||u1 − a sin(x)|| +



˜b−b
b


 ||u2||
≤



a˜b−ab˜
b


 ||x|| +



˜b−b
b


 |k1| ||x|| +



˜b−b
b


 |k2| ||y|| +



˜b−b
b


 ||u2||
:= h(x) + c||u2|| ,
where h(·) is a non-negative continuous function.
It then follows from the Lyapunov redesign method that the new feed￾back controller, u2, needs to be found by constructing a suitable Lyapunov
function. To do so, let P be a solution of the following matrix Lyapunov
equation:
P

0 1
k1 k2 − κ/m 
+ P

0 k1
11 k2 − κ/m 
P + I = 0
and calculate the scaler-valued function w(x) by
w(x) = ∂Ve
∂x
G =
∂
∂x

1
2
x
⊤Px
  0
b

= x
⊤P

0
b

.
It then follows that
u2 ≤ −
ρ(x)
1 − c
w(x)
|w(x)|
,
where ρ(x) ≥ h(x) component-wise, which can be any non-negative con￾tinuous function providing some freedom for other purposes such as robust
stability of the controlled system.Nonlinear Systems Control 191
6.3.4 Sliding-Mode Control
Sliding-mode control, also known as variable structure control, is a nonlin￾ear control method by which the motion dynamics of the controlled system
is effectively constrained to be within a subspace of the system state space.
The sliding motion is then achieved by a controller or algorithm to alter the
system dynamics along some consequent sliding-mode surfaces within the
state space, where on each sliding mode surface the system is equivalent to
a lower-order uncontrolled system that is easier to control.
Sliding-mode control is achieved in two steps: (i) (sliding-mode surface
design) predefine a sliding surface, in such a way that the desired system
dynamics can be achieved during the sliding-mode motion; (ii) (sliding￾mode controller design) design a controller to drive the closed-loop system
dynamics to reach and then be retained on the sliding surface. These are
respectively discussed in the following.
6.3.4.1 Sliding-Mode Surface Design
To introduce the main idea of sliding-mode control, consider an LTI system,
x˙ = Ax + Bu , (6.34)
where x ∈ Rn and u ∈ Rm with 1 ≤ m ≤ n. Assume that this LIT system
is controllable.
Predefine a variable vector s(t) ∈ Rm, which passes the origin of the
state space, in the following form
s(t) = Cx(t), (6.35)
where C ∈ Rm×n is a parametric matrix such that CB is nonsingular,
which is to be further determined.
Introduce an equivalent controller, ueq ∈ Rm, such that it can restrict
the system motion on the sliding-mode surface s(t) = 0 with s˙(t) = 0 for
all t ≥ t0. Thus,
(
s˙(t) = Cx˙(t) = 0
C

Ax(t) + Bueq(t)

= 0 ,
(6.36)
yielding
ueq = −[CB]
−1CAx (6.37)
and
x˙(t) = 
I − B[CB]
−1C

Ax(t). (6.38)192 Nonlinear Systems
The system (6.34) is said to remain on the sliding-mode surface if its
state vector reaches the surface and then sliding on all the connected switch￾ing surfaces in the state space.
Figure 6.6 visualizes the “reaching” phase and “sliding” phase of the
sliding-mode control in the 1-dimensional system setting.
= 0
Sliding phase
Reaching phase
x˙(t)
x(t)
s(t)
Fig. 6.6 Sliding-model control: “reaching” phase and “sliding” phase.
In the reaching phase of the sliding-mode controller, the Lyapunov sec￾ond method described by Theorem 2.2 is useful. Consider the Lyapunov
function
V =
1
2
s
⊤s ,
which satisfies
V˙ = s
⊤(t)s˙(t) < 0 , for s(t) ̸= 0 , (6.39)
if the reaching condition
s
⊤(t)s˙(t) < 0 (6.40)
is satisfied.
6.3.4.2 Sliding-Mode Controller Design
Under the (sufficient) condition (6.40), a reaching law is designed as
s˙(t) = −ε sgn[s(t)] − g(x(t)), (6.41)
where the signum function sgn[·] is defined component-wise, with a constant
control gain ε > 0 and continuous function g satisfying g(0) = 0 and
s
⊤g(s(t)) > 0 for s(t) ̸= 0 and all t ≥ t0.Nonlinear Systems Control 193
Example 6.7. Some specific reaching laws (6.41) are:
(i) s˙(t) = −ε sgn[s(t)];
(ii) s˙(t) = −ε sgn[s(t)] − ks(t), k > 0;
(iii) s˙(t) = −ε ||s(t)||k
sgn[s(t)], 0 < k < 1.
A typical sliding-mode controller is composed of two parts:
u(t) = ueq(t) + us(t), (6.42)
where ueq(t) is the equivalent controller (6.37) and
us(t) = −h[CB]
−1
sgn[s(t)] (6.43)
is a simple nonlinear controller with h > 0 being a constant control gain to
be determined.
Substituting (6.42), namely both (6.37) and (6.43), into (6.39) yields
V˙ (t) = s
⊤(t)[CA]x(t) + s
⊤[CB]u(t)
= −h||s(t)|| < 0 .
Consequently, s(t) → 0 as t → ∞.
After the system state vector reaches the sliding-mode surface s(t) = 0,
the system dynamics will stay in the sliding phase governed by (6.38).
6.3.4.3 Chattering and its Attenuation
During the sliding phase, due to possible switching time delay in the sign
change of s(t) and switch device time-lag in implementation, the system
trajectory may change its motion direction from time to time, or even fre￾quently. Such repetition of direction changes generates a “zig-zag motion”
oscillating around the predefined sliding surface, as illustrated in Fig. 6.7.
reaching phase
chaering
0
sliding phase
sliding surface
s(t) = 0
Fig. 6.7 Chattering in the sliding-mode control process.194 Nonlinear Systems
In general, the chattering will result in low control accuracy, non-smooth
motion performance, high heat losses in circuits, and high wear of mechan￾ical components, and even instability; therefore, it is very undesirable. In
the worst situation, at least in theory, there could even be the so-called
“Zeno phenomenon” in the sense that infinitely many times of switchings
occur in a finite time duration. All such chattering oscillations have to be
reduced or, if possible, completely eliminated.
One simple but effective technique to handle the chattering problem is
to use the so-called boundary-layer controller, namely, to replace the second
part of the sliding-mode controller (6.43) by
us(t) = −h[CB]
−1
sat[s(t)] , (6.44)
with the saturation function
sat[s(t)] = (
s(t)/ρ for ||s(t)|| ≤ ρ
sgn[s(t)] for ||s(t)|| > ρ ,
where the constant ρ > 0 is chosen to well control the boundaries of the
chattering motion.
It should be noted that, although the boundary-layer controller may
reduce or even eliminate the chattering motion, it may lead to losing the
system asymptotic stability, therefore this is not a perfect solution. As a
remedy or improvement, the following continuous approximation can be
considered:
us(t) = −h[CB]
−1

s(t)
||s(t)|| + ρ

, (6.45)
However, this modification is not perfect either, since it needs a high-gain
control when the system trajectory is close to the sliding surface.
Therefore, more advanced techniques are needed to perform better con￾trol, which has seen promising progress, but will not be further discussed
in this textbook.
6.4 Controlling Bifurcations and Chaos
6.4.1 Controlling Bifurcations
The notion of bifurcations in nonlinear dynamical systems, studied in
Sec. 5.1, is important and useful for many engineering applications. Var￾ious types of bifurcations occur frequently in practical systems, which are
usually harmful to the systems regarding their structures and dynamics.Nonlinear Systems Control 195
Therefore, they should be greatly reduced or even completely eliminated,
which is a special task commonly referred to as bifurcation control.
Typical examples of bifurcation control include crisis monitoring and
protection of power grids, dynamics-based practical design of power elec￾tronic devices, surge and stall prevention for jet engines, regulation of
pathological heart rhythms (e.g. fibrillation, ectopic foci), electronic and
mechanical control systems analysis and design such as thermal convec￾tion, lasers, oscillators, and so on.
This section is devoted to a study of the bifurcation control task, pre￾senting some basic ideas, design principles and analysis methods.
6.4.1.1 Controlling Discrete-Time Nonlinear Systems
In applications, sometimes it is important to delay the onset of a bifurcation
of stability to gain some time for taking actions to protect a system. For
instance, for the logistic map (system), one desirable but challenging task is
to delay the appearance of period-doubling bifurcation or the onset of chaos
as its key parameter approaches the bifurcation value, by implementing a
simple (e.g. linear) state-feedback controller without directly modifying the
parameter, which is usually unaccessible as discussed in Sec. 6.1.1.
As an example, consider the logistic map (5.3), namely
xk+1 = f
￾
xk, p
:= p xk
￾
1 − xk

.
As the parameter p is gradually increased in interval (3.0, 4.0), it shows
period-doubling leading to chaos:
period : 1 → 2 → 4 → · · · → 2
k → · · · → chaos
as shown in Fig. 5.9.
Now, consider the bifurcation control problem as follows:
Is it possible (and if so, how) to design a simple controller (a control
sequence), {uk}, such that the controlled logistic system
xk+1 = F
￾
xk, p
:= p xk
￾
1 − xk

+ uk
can achieve, for example, one of the following goals:
(i) the limiting chaotic behavior of the period-doubling bifurcation process
is delayed, or completely suppressed;
(ii) the first or second bifurcation has delayed occurrence, or some bifur￾cations are changed either in form or in stability;
(iii) the asymptotic behavior of the controlled system becomes chaotic, if
desirable, when the parameter p is currently not in the chaotic reg196 Nonlinear Systems
In this section, for some typical bifurcations, the problems of controlling
transcritical, saddle-node, pitchfork, Hopf and period-doubling bifurcations
are studied.
To prepare for these, their basic properties are first reviewed (see
[Glendinning (1994)]).
(a) Transcritical Bifurcation
Theorem 6.4. Consider the following system:
x˙ = f(x; p), f(0, 0) = ∂f
∂x (0, 0) = 0 .
If
∂f
∂p (0, 0) = 0 ,
∂
2f
∂x2
(0, 0) ̸= 0
∂
2f
∂p∂x (0, 0) −
∂
2f
∂x2
·
∂
2f
∂p2
(0, 0) > 0 ,
then the system has a transcritical bifurcation at (0, 0). Moreover,
∂
2
f
∂x2 (0, 0) > 0 → stable/unstable branches
∂
2
f
∂x2 (0, 0) < 0 → unstable/stable branches
(b) Saddle-Node Bifurcation
Theorem 6.5. Consider the following system:
x˙ = f(x; p), f(0, 0) = ∂f
∂x (0, 0) = 0 .
If
∂f
∂p (0, 0) ̸= 0 ,
∂
2f
∂x2
(0, 0) ̸= 0 ,
then the system has a saddle-note bifurcation at (0, 0). Moreover,
∂
2
f
∂x2 (0, 0) > 0 → stable/unstable branches
∂
2
f
∂x2 (0, 0) < 0 → unstable/stable branchesNonlinear Systems Control 197
(c) Pitchfork Bifurcation
Theorem 6.6. Consider the following system:
x˙ = f(x; p), f(0, 0) = ∂f
∂x (0, 0) = 0 .
If
∂f
∂p (0, 0) = ∂
2f
∂x2
(0, 0) = 0
∂
2f
∂p∂x (0, 0) ̸= 0 ,
∂
3f
∂x3
(0, 0) ̸= 0 ,
then the system has a pitchfork bifurcation at (0, 0). Moreover,
∂
3
f
∂x3 (0, 0) > 0 → stable/unstable branches
∂
3
f
∂x3 (0, 0) < 0 → unstable/stable branches
(d) Hopf Bifurcation
Recall the Hopf Bifurcation Theorem 5.3, as follows.
Theorem 6.7. Suppose that the 2-dimensional system
x˙ = f(x, y; p)
y˙ = g(x, y; p)
has a zero equilibrium, (x
∗
, y∗
) = (0, 0), and that its Jacobian has a pair of
complex conjugate eigenvalues, λ(p) and λ¯(p). If, for some p0,
d Re{λ(p)}
d p




p=p0
> 0 ,
then p = p0 is a bifurcation point, and
(i) for close enough values p < p0, the zero equilibrium is asymptotically
stable;
(ii) for close enough values p > p0, the zero equilibrium is unstable;
(iii) for close enough values p ̸= p0, the zero equilibrium is surrounded by
a limit cycle of magnitude O(
p
|p − p0|).198 Nonlinear Systems
The discrete version of the Hopf bifurcation theorem is introduced as
follows.
Consider a 2-dimensional parameterized system:
xk+1 = f(xk, yk; p),
yk+1 = g(xk, yk; p),
with a parameter p and an equilibrium point (x
∗
, y∗
), satisfying x
∗ =
f(x
∗
, y∗
; p), y
∗ = g(x
∗
, y∗
; p) simultaneously for all p.
Let J(p) be its Jacobian at this equilibrium, with a pair of complex
conjugate eigenvalues, λ2(p) = λ¯
1(p). If
|λ1(p
∗
)| = 1 and ∂|λ1(p)|
∂p




p=p∗
> 0
then the system has a Hopf bifurcation at (x
∗
, y∗
, p∗
), in a way analogous
to the continuous-time setting.
6.4.1.2 State-Feedback Control of Bifurcations
Consider a 1-dimensional discrete control system,
xk+1 = F(xk; p) := f(xk; p) + u(xk; p),
where p is a real parameter, x0 is the initial state, and u(·) is a state￾feedback controller to be designed.
Usually, u(·; p) is required not to modify the system equilibria, e.g.
u(0 ; p) = 0, in order not to alter the original system dynamics when control
is nonexistent.
Bifurcation controller design for this system is formulated as the follow￾ing routine-checking procedure.
Design Scheme
Step 0. Initiate a structure of the controller (e.g. a linear state-feedback
controller):
xk+1 = F(xk; p) := f(xk; p) + u(xk; p).
Step 1. Solve the equilibrium equation
x
∗ = F(x
∗
; p)
for a solution x
∗
(t; p); if no solution, change the structure of the controller
and try again.Nonlinear Systems Control 199
Table 6.1 Classification of bifurcations.
∂F
∂p



(x∗,p∗)
∂
2F
∂x2



(x∗,p∗)
Bifurcations
̸= 0 ̸= 0 saddle-node
= 0 ̸= 0 transcriptical
= 0 = 0 pitchfork
Table 6.2 Stability of equilibria near saddle-node bifurcation.
∂F
∂p



(x∗,p∗)
∂
2F
∂x2



(x∗,p∗)
stable equil. unstable equil. no equil.
> 0 > 0 p < p∗ (upper) p < p∗ (lower) p > p∗
> 0 < 0 p > p∗ (upper) p > p∗ (lower) p < p∗
< 0 > 0 p > p∗ (upper) p > p∗ (lower) p < p∗
< 0 < 0 p < p∗ (upper) p < p∗ (lower) p > p∗
Table 6.3 Stability of equilibria near transcritical bifurcation.
∂
2F
∂x∂p



(x∗,p∗)
− ∂
2F
∂x2
∂
2F
∂p2
∂
2F
∂x2



(x∗,p∗)
stable equil. unstable equil.
̸= 0 > 0 (lower) (upper)
̸= 0 < 0 (upper (lower)
Step 2. Determine the bifurcating parameter value p = p
∗
such that
∂F
∂x




x=x∗
p=p∗
= 1 ;
if no solution, change the structure of the controller, and then return to
Step 1.
Step 3. Determine the type of the bifurcation according to Table 6.1.
Step 4. Determine the stability of the equilibria according to Tables 6.2–
6.4.
Several controller designs for some typical 1-dimensional system bifur￾cations are specified below.200 Nonlinear Systems
Table 6.4 Stability of equilibria near pitchfork bifurcation.
∂F
∂p



(x∗,p∗)
∂
3F
∂x3



(x∗,p∗)
stable equil.
(1st branch)
unstable equil.
(1st branch)
stable equil.
(2nd branch)
unstable equil.
(2nd branch)
> 0 > 0 p < p∗ p > p∗ — p < p∗
> 0 < 0 p < p∗ p > p∗ p > p∗ —
< 0 > 0 p > p∗ p < p∗ — p > p∗
< 0 < 0 p > p∗ p < p∗ p < p∗ —
Table 6.5 Stability near period-doubling bifurcation.
ξ η period-doubling stable equil. unstable equil.
> 0 > 0 p < p∗ (stable) p > p∗ p < p∗
> 0 < 0 p > p∗ (unstable) p > p∗ p < p∗
< 0 > 0 p > p∗ (stable) p < p∗ p > p∗
< 0 < 0 p < p∗ (unstable) p < p∗ p > p∗
6.4.1.3 Controlling Period-Doubling Bifurcation
Design Scheme
Step 0. Initiate a structure of the controller.
Step 1. Solve the equilibrium equation x
∗ = F(x
∗
; p) for a solution x
∗
(t; p);
if no solution, change the structure of the controller and try again.
Step 2. Determine the bifurcating parameter p = p
∗
such that
∂F
∂x




x=x∗
p=p∗
= −1 ;
if no solution, change the controller structure and then return to Step 1.
Step 3. Determine the existence of the period-doubling bifurcation and the
stability of the equilibria according to Table 6.5, in which
ξ =

2
∂
2F
∂x∂p +
∂F
∂p
∂
2F
∂x2
 


x=x∗
p=p∗
,
η =

1
2

∂
2F
∂x2
2
+
1
3
∂
3F
2
∂x3
 



x=x∗
p=p∗
.Nonlinear Systems Control 201
Example 6.8. Consider the controlled logistic system:
xk+1 = F
￾
xk, p
= p xk
￾
1 − xk

+ uk .
The objective is to shift the original bifurcation point (x
∗
, p∗
) to a new
position, (x
o
, po
).
Following the above procedure, solving the equilibrium equation
F(x
o
, po
) = p
o x
o
(1 − x
o
) + u



x=xo
p=po
= x
o
gives
u



x=xo
p=po
= x
o − p
ox
o + p
o
(x
o
)
2
.
Then, the control condition leads to
∂F
∂x




x=xo
p=po
= p
o − 2p
ox
o +
∂u
∂x




x=xo
p=po
= 1 ,
which gives
∂u
∂x




x=xo
p=po
= 1 − p
o + 2p
ox
o
.
These two conditions together yield a simple linear controller:
u(xk; p) = ￾
1 − p
o + 2p
ox
o

xk − p
o
(x
o
)
2 =: c1 xk + c2 .
Thus, for the linearly controlled logistic map
xk+1 = F(xk; p) = p xk(1 − xk) + (c1 xk + c2),
one has
∂F
∂p




x=xo
p=po
= x
o − (x
o
)
2
,
∂
2F
∂x2




x=xo
p=po
= −2p
o
.
According to Table 6.1:
• if x
o = 0 and p
o ̸= 0, then (x
o
, po
) is a transcritical bifurcating point;
• if x
o ̸= 0 and p
o ̸= 0, then (x
o
, po
) is a saddle-node bifurcating point;
• if x
o = 0 and p
o = 0, then (x
o
, po
) is a pitchfork bifurcating point.
Note that for pitchfork bifurcation, Table 6.4 shows that ∂
3F/∂x3 ̸= 0,
so a linear controller cannot produce a pitchfork bifurcation. However, ac￾tually an additional cubic term can easily do the job, though the controller
is more complex.
On the other hand, one has
ξ = 2(1 − 2x
o
) − 2p
ox
o
(1 − x
o
), η = 2(p
o
)
2
.
Thus, a controller can be designed to shift the original period-doubling
bifurcation, starting for instance from a stable equilibrium (x
∗
, p∗
) =
(3.0, 0.6) to a new position, say (x
o
, po
) = (4.0, 0.6), as shown in Fig. 6202 Nonlinear Systems
(b) after control
(a) before control
–0.8
0
0.2
0.4
0.6
0.8
1
1.2
|y|
p
1.4
1.6
–0.7 –0.6 –0.5 –0.4 –0.3
–0.8
0
0.2
0.4
0.6
0.8
1
1.2
|y|
p
1.4
1.6
–0.7 –0.6 –0.5 –0.4 –0.3
Fig. 6.8 Shifting the period-doubling bifurcation of the logistic map.
6.4.1.4 Controlling Hopf Bifurcation
Consider a controlled system:
x˙ = f(x, y; p),
y˙ = g(x, y; p) + u(x, y; p).
The objective is to design a simple controller, u(x, y; p), which does not
change the original equilibrium (x
∗
, y∗
) but can move the original Hopf
bifurcation point (x
∗
, y∗
, p∗
) to a new position (x
o
, yo
, po
) ̸= (x
∗
, y∗
, p∗
).Nonlinear Systems Control 203
Clearly, the controller must satisfy u(x
∗
, y∗
; p) = 0 for all p, in order
not to change the position of (x
∗
, y∗
).
The controlled system Jacobian at (x
o
, yo
) is
J(p) = 
fx fy
gx + ux gy + uy

x=x∗,y=y0=0
where fx = ∂f /∂x and gy = ∂g/∂y, etc., which has eigenvalues
λ
c
1,2
(p) = 1
2
(fx + gy + uy)
±
1
2
r￾
fx + gy + uy
2
− 4
h
fx(gy + uy) − fy(gx + ux)
i
where fx := fx


x=xo, y=yo
, gy := gy


x=xo, y=yo
, etc.
To obtain a Hopf bifurcation at (x
o
, yo
; p
o
), the above analysis leads to
the following conditions:
(i) (x
o
, yo
) is an equilibrium point of the controlled system:
f(x
o
, yo
; p) = 0, g(x
o
, yo
; p) + u(x
o
, yo
; p) = 0, ∀ p ∈ R ;
(ii) the complex conjugate eigenvalues λ
c
1,2
(p) are purely imaginary at
(x
o
, yo
; p
o
):
(fx + gy + uy)



p=po
= 0 ,
fx(gy + uy) − fy(gx + ux)



p=po
> 0 ,
(fx + gy + uy)
2 − 4
h
fx(gy + uy) − fy(gx + ux)
i 


p̸=p0
< 0 ;
(iii) the crossing of the eigenlocus at the imaginary axis is transversal:
∂Re{λ
c
1
(p)}
∂p




p=po
=
∂(fx + gy + uy)
∂p




p=po
> 0 .
Example 6.9. Consider a discrete-time controlled system:
xk+1 = f(xk; p) + uk(yk; p),
yk+1 = xk+1 − xk .
The controller is designed such that uk(0; p) = 0, so it will not change
the position of the original equilibrium x
∗
.
The Jacobian of the controlled system at (x
o
, yo
) = (x
∗
, 0) is
J(p) = 
fx uy
fx − 1 0 
x=x∗,y=y0=0
204 Nonlinear Systems
where fx = ∂f /∂xk and uy = ∂uk/∂yk, which has eigenvalues
λ1,2(p) = 1
2
fx ±
1
2
q
f
2
x − 4uy(fx − 1).
Conditions for the controller are
λ1(p) = λ¯
2(p), |λ1,2(p
∗
)| = 1 ,
f
2
x ≤ 4uy(fx − 1),
∂|λ1,2(p
∗
)|
∂p > 0 .
As a specific example, the controlled logistic system
xk+1 = p xk
￾
1 − xk

+ uk(yk; p),
yk+1 = xk+1 − xk ,
yielding uk = e
p and yk = e
p
(xk − xk−1). The control performance is
visualized by a simulation shown in Fig. 6.9.
–0.8–0.5 –0.4 –0.3 –0.2 –0.1 0 0.1 xk
yk
yk
0.2 0.3
(a) before control: a stable node (b) after control: a periodic orbit
0.4 0.5 –0.5 –0.4 –0.3 –0.2 –0.1 0 0.1 xk
0.2 0.3 0.4 0.5
–0.6
–0.4
–0.2
0
0.2
0.4
0.6
–0.5
–0.4
–0.3
–0.2
–0.1
0
0.3
0.3
0.2
0.4
0.5
Fig. 6.9 Controlling the logistic map from a stable node to a periodic orbit.
Example 6.10. Consider a special form of the Lorenz system:
x˙ = −p(x − y),
y˙ = −xz − y ,
z˙ = xy − z − r ,
where p and r are parameters, which have three equilibria:
C0 = (0, 0, 0), C± = ±
√
r − 1 Nonlinear Systems Control 205
With p = 4:
• when 15.15 ≤ r ≤ 15.25, the system has two locally stable equilibria
C± and one chaotic attractor;
• when rH = 16, the system has a Hopf bifurcation, as shown in
Fig. 6.10, which shows the trajectories of the uncontrolled Lorenz system
with p = 4, initial condition (x0, y0, z0) = (10.0, −10.0, 5.7): (a) chaos for
r = 15.15; (b) stable C− for r = 13.20; (c) stable C+ for r = 15.25.
6
2
–2
–6
z
z
z
x
–10
–14
6
2
–2
–6
–10
–14
6
2
–2
–6
–10
–14
–12 –8
(a)
(b)
(c)
–4 0 4 8 12
Fig. 6.10 Trajectories of the uncontrolled Lorenz system.
Now, design a feedback controller, named “washout filter”, as follows:
x˙ = −p(x − y),
y˙ = −xz − y ,
z˙ = xy − z − r − u ,
u = −kc(y − cv) − kn(y − cv)
3
,
v˙ = y − cv ,206 Nonlinear Systems
where c = 0.5, kc = 2.5, kn = 0.09, which yields rH = 36.004 for C+ and
rH = 1.085 for C− (comparing to the original value of rH = 16).
The trajectories of the Lorenz system under control with a washout
filter are shown in Fig. 6.11, as follows:
(a) kn = 0.009, converging to C+ ;
(b) kn = 0.009, diverging to ∞;
(c) kn = 10, converging to a limit cycle;
(d) kn = 10, converging to a limit cycle;
(e) kn = 10, converging to C− ;
(f) kn = 10, converging to a limit cycle.
6
2
(a) r=30 (b) r=37.5
(c) r=37.5 (d) r=45
(e) r=1.08 (f) r=1.11
–2
–6
–10
–0.2
–0.6
–1.0
–1.4
–0.2
–0.6
–1.0
–1.4
–0.7 –0.5 –0.3 x –0.1 0.1
1.0
0.5
0.0
–0.5
–1.0
–1.5
–1.2 –0.8 –0.4 x 0.0
–1.8
5.8 5.9 6 6.1 x 6.2 6.3
–0.2
–0.6
–1.0
–1.4
–1.8
6.4 6.5 6.6 6.7 x 6.8
246
x
8 10
6
2
–2
–6
z
z
z
z
z
z
–10
246x
8 10
Fig. 6.11 Trajectories of the controlled Lorenz system.Nonlinear Systems Control 207
6.4.1.5 More about Bifurcation Control
It is worth mentioning that bifurcation control is a rich subject to study,
albeit fairly challenging, with, for instance, the following topics to be further
investigated:
• delaying the onset of a bifurcation;
• stabilizing a bifurcation solution or branch;
• changing a bifurcation value;
• enlarging the parameter range of a bifurcation;
• modifying the shape or type of a bifurcation;
• introducing a new bifurcation point;
• creating a new type of bifurcation;
• reversing a period-doubling bifurcation;
• monitoring a degenerate Hopf bifurcation;
• controlling the multiplicity, amplitude, and/or frequency of limit cycles
emerging from Hopf bifurcation;
• and, a certain combination of some of these.
6.4.2 Controlling Chaos
It was shown in Examples 6.1 and 6.2 that chaos can be controlled to pe￾riodic orbits, including equilibria as special cases. However, it should be
noted that in these examples no specific chaos properties were utilized,
where the chaotic systems were treated just like nonchaotic nonlinear sys￾tems. Therefore, using skillfully designed or even brute-force controllers it
is often possible to accomplish stabilization or tracking control tasks.
In contrast, making a nonchaotic system to be chaotic by means of
control is much harder. Since chaos can be useful in some engineering
applications, such as liquids-mixing, information encryption, secure com￾munication, weak signal detection, global optimality searching, etc., there
is a need to generate chaos purposefully when it is desirable. This reverse
task is referred to as chaotification (or, anti-control of chaos).
To do so, consider a general finite-dimensional discrete-time dynamical
system, need not be originally chaotic or unstable, in the form of
xk+1 = fk(xk), x0 ∈ R
n
, (6.46)
where, at the moment, fk(·) is only assumed to be continuously differen￾tiable, at least locally in a region of interest. This means that the given
system (6.46) can be linear or nonlinear, time-invariant or time-varying,
and stable or unstable.208 Nonlinear Systems
The objective is to design a controller in the form of a control input
sequence, {uk}, such that the output state vector sequence of the controlled
system
xk+1 = fk(xk) + uk (6.47)
behaves chaotically, in the sense of Devaney (Definition 5.6) or Li–Yorke
(Definition 5.5)
As discussed in Sec. 6.1.1, a practical design of a useful controller should
yield a simple structure, such that the goal of control (here, chaotification)
can be achieved. The following discussions will follow this basic engineer￾ing principle, thereby designing some simple and implementable chaotifiers
(anti-controllers).
6.4.2.1 Chaotification Problem Description
Consider a simple nonlinear state-feedback controller of the form
uk = ak φ(σkxk) 1, (6.48)
where 1 = [1, · · · , 1]⊤ is an all-one vector, {ak} and {σk} are two sequences
of real constants to be determined, and φ(·) is a simple unimodal function
such as a piecewise-linear sawtooth function, as shown in Fig. 6.12, or a sine
function, both of which can be easily implemented by commercial circuits.
◦
◦
✁
• ✲
✛ Bk ✛
delay
❄
0
fk(·), x0 ✲
xk
xk+1
✻ ✲
￾
￾ ❅❅
❅❅
Fig. 6.12 The state-feedback anti-control configuration.
Using this controller, without tuning any of the system parameters, the
controlled system
xk+1 = fk(xk) + ak φ(σkxk) 1 (6.4Nonlinear Systems Control 209
is expected to become chaotic, regardless of the functional form of the given
system fk(·), which is only assumed to be Lipschitz so that its Jacobian
can be uniformly bounded.
When the sawtooth function is used in (6.48), the controlled system is
equivalent to the following:
xk+1 = fk(xk) + Bk xk (mod 1), (6.50)
where {Bk} is a sequence of constant matrices and (mod 1) is the
component-wise modulo-one operation, which maps [0,1] onto itself.
It is remarked that this equivalence is not obvious, but can be verified
component-wise, namely in the 1-dimensional setting, as demonstrated by
the simple example discussed at the end of this section below.
Note also that in operation (mod 1), the magnitude 1 can be replaced
by any positive real number, such as 2π if the sine function is used in (6.48).
Thus, the controlled system (6.50) rather than (6.49) is discussed here
for notational simplicity.
To describe the problem more precisely, some new notation is intro￾duced. Let
Jj (z) = f
′
j
(z) + Bj (6.51)
be the Jacobian of the controlled system (6.50), evaluated at z, j = 0, 1, . . .,
and let
Tj = Tj (x0, . . . , xj ) := Jj (xj )· · · J0(x0).
Moreover, let µ
j
i
be the ith eigenvalue of the jth product matrix 
T
⊤
j Tj

,
i = 1, . . . , n, j = 0, 1, . . . , and recall that the ith Lyapunov exponent of the
orbit {xk} of the controlled system, starting from x0, is given by (5.14),
namely,
λi(x0) = lim
k→∞
1
2k
ℓn | µi
| , i = 1, . . . , n . (6.52)
The chaotification approach comprises two steps:
First, design the constant matrices {Bk} such that all the Lyapunov
exponents of the controlled system orbit starting from x0 are finite and
strictly positive:
0 < c ≤ λi < ∞, i = 1, . . . , n, (6.53)
where c is a pre-desired constant. To be practical in achieving this goal,
namely, for implementation purpose, it is also required that the constant
gain sequence {Bk} is uniformly bounded for all k = 0, 1, . . . . As a matter210 Nonlinear Systems
of fact, this can be guaranteed by using a very simple constant feedback
gain Bk ≡ σIn with some σ > 0 for all k = 0, 1, . . . , as demonstrated below.
Second, take the (mod 1) operation, so as to force the diverging system
orbits to return back into a bounded region, thereby generating chaos as
desired.
It is remarked that the above-described procedure of chaotification is al￾ways possible under one (and only one) natural condition that all Jacobians
Jk(xk) of the given system are uniformly bounded:
||f
′
k
(xk)|| ≤ M < ∞, for all k = 0, 1, . . . , (6.54)
where any suitable matrix norm such as the spectral norm can be used. This
restriction simply avoids that the system orbits diverge too quickly before
the controller takes effective action. This condition is not too restrictive,
since many systems such as all linear time-invariant systems satisfy it.
6.4.2.2 A General Chaotification Algorithm
Start with the initial controlled system x1 = f0(x0) + B0x0, where x0 is
given. Calculate its Jacobian J0(x0) = f
′
0
(x0) + B0 and set T0 = J0(x0).
Design a positive feedback control gain, B0 = σ0I, by choosing a constant
σ0 > 0 such that the matrix [T0T
⊤
0
] is finite and diagonally dominant.
For k = 0, 1, 2, . . . , start with the controlled system xk+1 = fk(xk) +
Bkxk, where both Bk and xk are obtained from the previous step. Perform
the following steps iteratively:
Step 1. Compute the Jacobian Jk(xk) = f
′
k
(xk) + σkI and then set Tk =
JkTk−1.
Step 2. Design a positive feedback controller by choosing a constant σk > 0
such that the matrix [TkT
⊤
k −e
2kcI] is finite and diagonally dominant, where
the constant c > 0 is the one specified in (6.53).
Step 3. Apply the (mod 1) operation to the controlled system, as shown
in (6.50), or apply a unimodal function (e.g. the piecewise-linear sawtooth
function shown in Fig. 6.12) to the controller, as formulated in (6.49).
To this end, it can be rigorously proved [Chen (2003)] that the controlled
system so designed is guaranteed to be chaotic in the sense of Devaney and
also of Li–Yorke, as further illustrated below.
A few remarks are in order.
First, a simple choice for {σk} is to use the constant σ = M +e
c
, where
c is given in (6.53) and M is given in (6.54), for all k = 0, 1, . . . .Nonlinear Systems Control 211
Second, in Step 3 above, if the sawtooth function is used, its magnitude
can be a constant, ak = a for all k = 0, 1, . . . , and this constant can be
arbitrarily small, which means that “small control” is possible; but in this
case, σ should be large, i.e. the frequency should be high.
Third, some other unimodal functions such as the sine function can also
be used at Step 3 for the controller.
It is particularly important to emphasize that this chaotification scheme
is fairly general because the given system (6.46) can be quite arbitrary: it
can be linear or nonlinear, time-invariant or time-varying, stable or un￾stable, of any finite dimension, provided that the system Jacobians are
uniformly bounded in the sense of (6.54).
It is easily noticed that the above chaotification scheme utilizes a full￾state feedback, which is often not desirable in engineering applications be￾cause full-state information is not always available, e.g. using sensors in
practice. For engineering control systems, it is more preferable to use out￾put feedback instead of state feedback, which means that only part of the
state vector is measured and used for feedback control. Therefore, it is
natural to modify the above algorithm and use output feedback instead of
full-state feedback to achieve chaotification, which is further discussed next.
6.4.2.3 A Modified Chaotification Algorithm
Recall the chaotifier (6.48), namely,
uk = ak φ(σkxk) 1, (6.55)
where 1 = [1, · · · , 1]⊤ is an all-one vector, {ak} and {σk} are two sequences
of real constants to be determined, and φ(·) is a simple unimodal function
as the piecewise-linear sawtooth function or the sine function.
Now, this full-state feedback chaotifier is modified to be
uk = bφ(xk) = −b sin(hxk), (6.56)
where b is an n × 1 constant vector and h is a 1 × n constant vector, both
are to be determined.
The chaotifier (6.48) can also be modified as
uk = aφ(xk) = a

sin x
1
k
sin x
2
k
· · · sin x
n
k
⊤
, (6.57)
where a is a constant to be determined, and xk =

x
1
k x
2
k
· · · x
n
k
⊤
is the
state vector.
The above anti-controller can also be slightly modified to be
uk = aϕ(cxk) = a

sin cx1
k
sin cx2
k
· · · sin cxn
k
⊤
, (6.58)212 Nonlinear Systems
where a and c are constants to be determined and xk =

x
1
k x
2
k
· · · x
n
k
⊤
is the state vector as above.
Note that the corresponding controlled system (6.47) using either of
these chaotifiers is chaotic in the sense of Devaney for some specifically
given systems, as demonstrated by the following example.
6.4.2.4 An Illustrative Example
To illustrate how the above-discussed state-feedback (or output-feedback)
controller can chaotify a given nonchaotic system, and how to verify that the
controlled system is chaotic, a simple 1-dimensional example is discussed.
Consider the 1-dimensional linear state-feedback control system
xk+1 = a xk + uk (mod 1) (6.59)
where the controller
uk =
￾
N + e
c

xk , (6.60)
with 0 < c ≤ |a| ≤ N < ∞.
Putting together, this controlled system is
xk+1 = a xk +
￾
N + e
c

xk =
￾
a + N + e
c

xk (mod 1) (6.61)
where ￾
a + N + e
c

> 1.
Define a map ϕ : S
1 → S
1
, where S
1
is the unit circle on the 2-
dimensional plane, by
ϕ(x) = ￾
a + N + e
c

∠ x , x ∈ S
1
, (6.62)
in which ∠ x is the angle of the point x ∈ S
1
(thus, 0 ≤ ∠ x < 2π and ϕ is
2π-periodic). Note that this map is a one-variable map since the radius of
the circle is fixed and only the angle is variable.
It can be verified that the two systems, (6.61) and (6.62), are equiva￾lent. Indeed, by multiplying 2π on both sides, it can be seen that (6.61) is
equivalent to
xk+1 =
￾
a + N + e
c

xk (mod 2π), (6.63)
or
∠ xk+1 =
￾
a + N + e
c

∠ xk , xk ∈ S
1
,
which is equivalent to (6.62) if the initial state x0 can be arbNonlinear Systems Control 213
Finally, it can be shown that the map (6.63) is equivalent to the chaotic
logistic map xk+1 = 4xk(1 − xk) by defining xk =
1
2

1 − cos(2πξyk)

in the
logistic map, where ξ := (a + N + e
c
) > 1. With this change of variables,
the logistic map becomes
1
2

1 − cos(2πξyk+1)

= 4 ·
1
2

1 − cos(2πξyk)

·
n
1 −
1
2

1 − cos(2πξyk)
o
=

1 − cos(2πξyk)

·

1 + cos(2πξyk)

,
which has the solution precisely given by formula (6.63).214 Nonlinear Systems
Exercises
6.1 Consider the pendulum model

x˙ = y
y˙ = −
κ
m y −
g
ℓ
sin(x) + u ,
where u is a variable parameter used as an adjustable constant control
input. Convert this system into the normal form.
6.2 * Consider the following controlled system:
x˙ = Ax + f(x, t) + u(x, t),
where A is a stable constant matrix and
lim
||x||→∞
||f(x, t)||
||x|| = 0 for all t ≥ t0 .
Suppose that the controller u is designed such that
||u(x, t)|| ≤ γ(t) for all t ≥ t0 and ||x|| ≤ κ ,
for some constant κ < ∞, where γ(t) is a non-negative continuous
function satisfying for any constant a > 0, that
Z t+a
t
γ(τ )dτ → 0 (t → ∞).
Show that, for the controlled system, x(t) → 0 as t → ∞.
6.3 Consider an uncertain nonlinear control system,

x˙ = y
y˙ = f(x) + u ,
where x = [x y]
⊤ and f is unknown but satisfies |f(x)| ≤ c||x|| for
some constant c > 0, and the controller
u = −x − y + v ,
with v to be determined. Verify that if v is designed as
v =
(
−c||x|| w
|w|
if c||x|| · |w| ≥ ε
−c
2
||x||2 w
ε
if c||x|| · |w| < ε ,
where w = 2x
⊤P

0
1

with P being a positive definite and symmetrical
matrix, then the uncertain system can be stabilized to zero, namely,
x → 0 and y → 0 as t → ∞.
[Hint: Consider the Lyapunov function V = x
⊤Px.]Nonlinear Systems Control 215
6.4 Consider the motion equation of a controlled single-link robot arm:
D(q)¨q + C(q, q˙) ˙q + ϕ(q) = u ,
where q = q(t) is the joint angular variable, C > 0, D > 0 and ϕ
are nonlinear matrix- or vector-valued functions, and u = u(t) is the
controller input to be designed. The objective is to control the system
output q(t) to track a target trajectory q
d = q
d
(t) as t → ∞.
Suppose that the controller u(t) is designed in three steps:
i. Assume a control input u(t) of the form
u = D0(q)v + C0(q, q˙) ˙q + ϕ(q),
where c = c(t) is a new controller to be further designed. Verify
that, by designing x = [x1 x2]
⊤ = [q q˙]
⊤, the controlled system
can be reformulated as
x˙ = Ax + B(v + ξ),
and find the explicit expressions of the two constant matrices A
and B, as well as the uncertain term ξ = ξ(x), which contains all
the other terms including perhaps also x.
ii. Introduce the error signal e = [e1 e2]
⊤ with
(
e1 = x1 − x
d
1 = q − q
d
e2 = x2 − x
d
2 = ˙q − q˙
d
.
Show that the above error-signal model can be reformulated as
e˙ = Aee + Be(v + ξe),
and find the explicit expressions of the two constant matrices Ae
and Be, as well as the uncertain term ξe = ξe(x), which contains all
uncertain terms including perhaps q
d
, ˙q
d and ¨q
d
. Is this error-signal
model stable?
iii. Suppose that the model uncertainties ∆C and ∆ϕ are both
bounded, although unknown, in the sense that |ξe| ≤ 1. Design
the controller by determining a controller
v = k
⊤e + v0 = [k1 k2]

e1
e2

+ v0
for some constant feedback control gains k1 and k2, as well as a
constant control input v0, such that the above controlled error￾signal model is stable with e(t) → 0 as t → ∞. If this is impossible,
explain what the basic limitation of a linear controller is for an
uncertain nonlinear system, and suggest a simple working nonlinear
(e.g. adaptive) controller for this robot arm system.216 Nonlinear Systems
6.5 Verify that the reaching law (6.41) satisfies the reaching condition
(6.40).
6.6 In Example 6.8, it was shown that a linear controller cannot produce a
pitchfork bifurcation. Verify that an additional cubic term to the linear
controller can do the job.
6.7 Consider the simplified pendulum model
¨θ + a θ + b sin(θ) = u ,
where 0 < a < b are given constant parameters. Verify, by both mathe￾matical analysis and physical interpretation, that the uncontrolled pen￾dulum with u = 0 does not have a Hopf bifurcation. Then, design a
linear controller of the form
u = µ(θ + ˙θ),
with µ = µ(a, b) being a suitably chosen variable control gain, such
that the controlled pendulum has a Hopf bifurcation.
6.8 * Verify that the controlled system described by the map (6.63) satisfies
Definition 5.6 for chaos:
i. Verify that, after k iterations of the map, one has ϕ
k
(x) = ξ
k ∠ x.
ii. Since ϕ is 2π-periodic, let ξ
k ∠ x = ∠ x + 2ℓπ, where ℓ is an integer
satisfying 0 ≤ ℓ ≤ ⌊ ξ
k
⌋ − 1. Verify that the periodic solutions of
the map ϕ are almost uniformly distributed on the unit circle:
∠ xk,ℓ =
2ℓπ
ξ
k − 1
, ℓ = 0, 1, . . . , ⌊ ξ
k
⌋ − 1 ,
which means that for two arbitrarily given nonempty open subsets
in S
1
, there are two large enough integers k0 and ℓ0 such that the
periodic solution {xk0,ℓ0
} of the map has at least one point in each
of these two subsets.Bibliography
Anderson, B. D. O. and Vongpanitterd, S. [1972] Network Analysis and Synthesis:
A Modern System Approach (Prentice-Hall).
Arrowsmith, D. K. and Place, C. M. [1990] An Introduction to Dynamical Systems
(Campridge Univ. Press).
Chen, G. [1999] “Stability of nonlinear systems,” in Encycropidia of E E
(W. K. Chen, ed.) (Wiley) 2004 Edition: pp. 4881–4896.
Chen, G. [2003] “Chaotification via feedback: The discrete case,” in Chaos Con￾trol (G. Chen and X. Yu, eds.) (Springer), pp. 159–177.
Chen, G. [2020] “Generalized Lorenz systems family,”
https://arxiv.org/abs/2006.04066
Coddington, E. A. and Levinson, N. [1955] Theory of Ordinary Differential Equa￾tions (McGraw-Hill).
Desoer, C. A. and Vidyasagar, M. [1975] Feedback Systems: Input-Output Prop￾erties (Academic Press).
Devaney, R. L. [1987] An Introduction to Chaotic Dynamical Systems (CRC
Press).
Glendinning, P. [1994] Stability, Instability and Chaos: An Introduction to the
Theory of Nonlinear Differential Equations (Cambridge Univ. Press).
Hahn, W. [1967] Stability of Motion (Springer-Verlag).
Hoppensteadt, F. C. [2000] Analysis and Simulation of Chaotic Systems
(Springer).
Huygens, C. [1665] “A letter to the Royal Society,” in Oeuvres Completes de
Christian Huygens, M. Nijhoff (ed.), Societe Hollandaise des Sciences, The
Hague, The Netherlands, 1893, vol. 5, p. 246 (in French).
Khalil, H. K. [1996] Nonlinear Systems (Prentice-Hall).
Li, T. Y. and Jorke, J. A. [1975] “Period three implies chaos,” Amer. Math.
Month., 82, 481–485.
Marotto, F. R. [1978] “Snap-back repellers imply chaos in R
n
,” J. Math. Anal.
Appl., 63, 199–223.
Massera, J. L. [1956] “Contributions to stability theory,” Annals of Mathematics,
64(1): 182–206.
217218 Nonlinear Systems
Moiola, J. L. and Chen, G. [1996] Hopf Bifurcation Analysis: A Frequency Domain
Approach (World Scientific).
Narendra, K. S. and Taylor, J. H. [1973] Frequency Domain Criteria for Absolute
Stability (Academic Press).
Popov, V. M. [1973] Hyperstability of Automatic Control Systems (Springer￾Verlag).
Robinson, C. [1995] Dynamical Systems: Stability, Symbolic Dynamics, and
Chaos (CRC Press).
Sastry, S. [1999] Nonlinear Systems: Analysis, Stability and Control (Springer).
Touhey, P. [1997] “Yet another definition of chaos,” Amer. Math. Month., 104,
411–414.
Verhulst, F. [1996] Nonlinear Differential Equations and Dynamical Systems
(Springer).
Wa´zewski, T. [1950] “Syst´emes des ´equations et des in´egalit´es diff´erentielles ordi￾naires aux deuxi´eme members monotones et leurs applications,” Ann. Soc.
Polon. Math., 23, 112–166 (in French).
Wiggins, S. [1990] Introduction to Applied Nonlinear Dynamical Systems and
Chaos (Springer).Index
α-limit set, 57
ω-limit set, 57, 148
absolute stability, 106
adaptive control, 182
Aizerman–Kalman conjecture, 106,
125
asymptotic stability, 41
attractor, 149
autonomous system, 3
basin of attraction, 58
BIBO stability, 118
bifurcation control, 195, 198, 200, 202
bifurcation point, 130
chaos, 151
chaos control, 207
chaos in the sense of Devaney, 160
chaos in the sense of Li–Yorke, 163
chaotic attractor, 151
chaotic orbit, 151
chaotic system, 151
chaotification, 207, 209, 210
chaotifier, 208, 211
characteristic exponent, 88
Chen system, 155
Chetaev Instability Theorem, 62
Chua circuit, 172
Circle Criterion, 109
class-K function, 44
Comparison Lemma, 92
Comparison Principle, 93
Contraction Mapping Theorem, 123
describing function, 114
diffeomorphism, 94
Duffing oscillator, 153, 175
equilibrium point, 6
equilibrium subspace, 23
equivalent controller, 191
eventually periodic, 6
first method of Lyapunov, 43, 55
fixed point, 6
Floquet multiplier, 86, 179
Floquet number, 88
Floquet Theorem, 86, 88
forward invariant set, 7
General Instability Theorem, 61
General Linear Stability Theorem, 80
global boundedness, 157
Graphical Stability Criterion, 117
Grobman–Hartman Theorem, 31
Gronwall Inequality, 80
H´enon map, 159
harmonic balance approximation, 112
harmonic balance equation, 114
harmonic oscillator, 8
219220 Nonlinear Systems
homeomorphism, 25
Hopf bifurcation, 144, 197
hyper stability, 111
hyperbolic system, 31
Kalman–Yakubovich–Popov Lemma,
184
Krasovskii Theorem, 54
LaSalle Invariance Principle, 58, 177
Li’enard equation, 99
Li´enard equation, 75
limit cycle, 16
Linear Instability Theorem, 60
Lipschitz condition, 4
logistic map, 133, 159, 161, 195, 204
Lorenz system, 150, 204
Lotka–Volterra system, 142
Lur’e system, 102, 111
Lyapunov exponent, 157
Lyapunov redesign, 188
Lyapunov stability, 38
Malkin Theorem, 97
manifold, 148
Marotto Theorem, 163
Massera Inverse Theorem, 57
Mathieu equation, 99
maximum Lyapunov exponent, 157
nonautonomous system, 3
Normal Form Theorem, 138
orbital stability, 93
Orbital Stability Theorem, 93
Passivity Stability Theorem, 120
Peixoto Structural Stability Theorem,
96
pendulum model, 1, 9, 10, 26, 43, 50,
53, 64, 67, 72, 99, 137, 139, 189,
214, 216
period-doubling bifurcation, 133
periodic motion, 3
persistent perturbation, 96
Perturbed Orbital Stability Theorem,
97
phase portrait, 7
PID controller, 170
pitchfork bifurcation, 132, 197
Poincar´e map, 146
Poincar´e–Andronov–Hopf Theorem,
144
Poincar´e–Bendixson Theorem, 152
Popov Criterion, 107
positive real function, 184
predator-prey model, 99
R¨ossler system, 155
Rayleigh oscillator, 17
region of stability, 69
saddle node, 13
saddle-node bifurcation, 131, 196
second method of Lyapunov, 45, 52,
55, 56
sector condition, 106
sensitivity to initial condition, 160
skew tent map, 166
sliding-mode chattering, 193
sliding-mode control, 191
sliding-mode reaching law, 192
sliding-mode surface, 191
Small Gain Theorem, 119
stability in the sense of Lyapunov, 39
stable node, 13
structural stability, 95
Sturm Comparison Theorem, 91
Sturm Separation Theorem, 90
topological conjugacy, 25
topological equivalence, 25
topological orbital equivalence, 94
topological transitivity, 160
total stability, 97
tracking control, 170
transcritical bifurcation, 131, 196
uncertain nonlinear system, 186
unstable node, 13Index 221
van der Pol oscillator, 17, 153, 166
variational gradient method, 67
Volterra–Lotka system, 73
Zeno phenomenon, 194
